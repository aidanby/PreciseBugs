{"buggy_code": ["/*\n * Contains the definition of registers common to all PowerPC variants.\n * If a register definition has been changed in a different PowerPC\n * variant, we will case it in #ifndef XXX ... #endif, and have the\n * number used in the Programming Environments Manual For 32-Bit\n * Implementations of the PowerPC Architecture (a.k.a. Green Book) here.\n */\n\n#ifndef _ASM_POWERPC_REG_H\n#define _ASM_POWERPC_REG_H\n#ifdef __KERNEL__\n\n#include <linux/stringify.h>\n#include <asm/cputable.h>\n\n/* Pickup Book E specific registers. */\n#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)\n#include <asm/reg_booke.h>\n#endif /* CONFIG_BOOKE || CONFIG_40x */\n\n#ifdef CONFIG_FSL_EMB_PERFMON\n#include <asm/reg_fsl_emb.h>\n#endif\n\n#ifdef CONFIG_8xx\n#include <asm/reg_8xx.h>\n#endif /* CONFIG_8xx */\n\n#define MSR_SF_LG\t63              /* Enable 64 bit mode */\n#define MSR_ISF_LG\t61              /* Interrupt 64b mode valid on 630 */\n#define MSR_HV_LG \t60              /* Hypervisor state */\n#define MSR_VEC_LG\t25\t        /* Enable AltiVec */\n#define MSR_VSX_LG\t23\t\t/* Enable VSX */\n#define MSR_POW_LG\t18\t\t/* Enable Power Management */\n#define MSR_WE_LG\t18\t\t/* Wait State Enable */\n#define MSR_TGPR_LG\t17\t\t/* TLB Update registers in use */\n#define MSR_CE_LG\t17\t\t/* Critical Interrupt Enable */\n#define MSR_ILE_LG\t16\t\t/* Interrupt Little Endian */\n#define MSR_EE_LG\t15\t\t/* External Interrupt Enable */\n#define MSR_PR_LG\t14\t\t/* Problem State / Privilege Level */\n#define MSR_FP_LG\t13\t\t/* Floating Point enable */\n#define MSR_ME_LG\t12\t\t/* Machine Check Enable */\n#define MSR_FE0_LG\t11\t\t/* Floating Exception mode 0 */\n#define MSR_SE_LG\t10\t\t/* Single Step */\n#define MSR_BE_LG\t9\t\t/* Branch Trace */\n#define MSR_DE_LG\t9 \t\t/* Debug Exception Enable */\n#define MSR_FE1_LG\t8\t\t/* Floating Exception mode 1 */\n#define MSR_IP_LG\t6\t\t/* Exception prefix 0x000/0xFFF */\n#define MSR_IR_LG\t5 \t\t/* Instruction Relocate */\n#define MSR_DR_LG\t4 \t\t/* Data Relocate */\n#define MSR_PE_LG\t3\t\t/* Protection Enable */\n#define MSR_PX_LG\t2\t\t/* Protection Exclusive Mode */\n#define MSR_PMM_LG\t2\t\t/* Performance monitor */\n#define MSR_RI_LG\t1\t\t/* Recoverable Exception */\n#define MSR_LE_LG\t0 \t\t/* Little Endian */\n\n#ifdef __ASSEMBLY__\n#define __MASK(X)\t(1<<(X))\n#else\n#define __MASK(X)\t(1UL<<(X))\n#endif\n\n#ifdef CONFIG_PPC64\n#define MSR_SF\t\t__MASK(MSR_SF_LG)\t/* Enable 64 bit mode */\n#define MSR_ISF\t\t__MASK(MSR_ISF_LG)\t/* Interrupt 64b mode valid on 630 */\n#define MSR_HV \t\t__MASK(MSR_HV_LG)\t/* Hypervisor state */\n#else\n/* so tests for these bits fail on 32-bit */\n#define MSR_SF\t\t0\n#define MSR_ISF\t\t0\n#define MSR_HV\t\t0\n#endif\n\n#define MSR_VEC\t\t__MASK(MSR_VEC_LG)\t/* Enable AltiVec */\n#define MSR_VSX\t\t__MASK(MSR_VSX_LG)\t/* Enable VSX */\n#define MSR_POW\t\t__MASK(MSR_POW_LG)\t/* Enable Power Management */\n#define MSR_WE\t\t__MASK(MSR_WE_LG)\t/* Wait State Enable */\n#define MSR_TGPR\t__MASK(MSR_TGPR_LG)\t/* TLB Update registers in use */\n#define MSR_CE\t\t__MASK(MSR_CE_LG)\t/* Critical Interrupt Enable */\n#define MSR_ILE\t\t__MASK(MSR_ILE_LG)\t/* Interrupt Little Endian */\n#define MSR_EE\t\t__MASK(MSR_EE_LG)\t/* External Interrupt Enable */\n#define MSR_PR\t\t__MASK(MSR_PR_LG)\t/* Problem State / Privilege Level */\n#define MSR_FP\t\t__MASK(MSR_FP_LG)\t/* Floating Point enable */\n#define MSR_ME\t\t__MASK(MSR_ME_LG)\t/* Machine Check Enable */\n#define MSR_FE0\t\t__MASK(MSR_FE0_LG)\t/* Floating Exception mode 0 */\n#define MSR_SE\t\t__MASK(MSR_SE_LG)\t/* Single Step */\n#define MSR_BE\t\t__MASK(MSR_BE_LG)\t/* Branch Trace */\n#define MSR_DE\t\t__MASK(MSR_DE_LG)\t/* Debug Exception Enable */\n#define MSR_FE1\t\t__MASK(MSR_FE1_LG)\t/* Floating Exception mode 1 */\n#define MSR_IP\t\t__MASK(MSR_IP_LG)\t/* Exception prefix 0x000/0xFFF */\n#define MSR_IR\t\t__MASK(MSR_IR_LG)\t/* Instruction Relocate */\n#define MSR_DR\t\t__MASK(MSR_DR_LG)\t/* Data Relocate */\n#define MSR_PE\t\t__MASK(MSR_PE_LG)\t/* Protection Enable */\n#define MSR_PX\t\t__MASK(MSR_PX_LG)\t/* Protection Exclusive Mode */\n#ifndef MSR_PMM\n#define MSR_PMM\t\t__MASK(MSR_PMM_LG)\t/* Performance monitor */\n#endif\n#define MSR_RI\t\t__MASK(MSR_RI_LG)\t/* Recoverable Exception */\n#define MSR_LE\t\t__MASK(MSR_LE_LG)\t/* Little Endian */\n\n#if defined(CONFIG_PPC_BOOK3S_64)\n/* Server variant */\n#define MSR_\t\tMSR_ME | MSR_RI | MSR_IR | MSR_DR | MSR_ISF |MSR_HV\n#define MSR_KERNEL      MSR_ | MSR_SF\n#define MSR_USER32\tMSR_ | MSR_PR | MSR_EE\n#define MSR_USER64\tMSR_USER32 | MSR_SF\n#elif defined(CONFIG_PPC_BOOK3S_32) || defined(CONFIG_8xx)\n/* Default MSR for kernel mode. */\n#define MSR_KERNEL\t(MSR_ME|MSR_RI|MSR_IR|MSR_DR)\n#define MSR_USER\t(MSR_KERNEL|MSR_PR|MSR_EE)\n#endif\n\n/* Floating Point Status and Control Register (FPSCR) Fields */\n#define FPSCR_FX\t0x80000000\t/* FPU exception summary */\n#define FPSCR_FEX\t0x40000000\t/* FPU enabled exception summary */\n#define FPSCR_VX\t0x20000000\t/* Invalid operation summary */\n#define FPSCR_OX\t0x10000000\t/* Overflow exception summary */\n#define FPSCR_UX\t0x08000000\t/* Underflow exception summary */\n#define FPSCR_ZX\t0x04000000\t/* Zero-divide exception summary */\n#define FPSCR_XX\t0x02000000\t/* Inexact exception summary */\n#define FPSCR_VXSNAN\t0x01000000\t/* Invalid op for SNaN */\n#define FPSCR_VXISI\t0x00800000\t/* Invalid op for Inv - Inv */\n#define FPSCR_VXIDI\t0x00400000\t/* Invalid op for Inv / Inv */\n#define FPSCR_VXZDZ\t0x00200000\t/* Invalid op for Zero / Zero */\n#define FPSCR_VXIMZ\t0x00100000\t/* Invalid op for Inv * Zero */\n#define FPSCR_VXVC\t0x00080000\t/* Invalid op for Compare */\n#define FPSCR_FR\t0x00040000\t/* Fraction rounded */\n#define FPSCR_FI\t0x00020000\t/* Fraction inexact */\n#define FPSCR_FPRF\t0x0001f000\t/* FPU Result Flags */\n#define FPSCR_FPCC\t0x0000f000\t/* FPU Condition Codes */\n#define FPSCR_VXSOFT\t0x00000400\t/* Invalid op for software request */\n#define FPSCR_VXSQRT\t0x00000200\t/* Invalid op for square root */\n#define FPSCR_VXCVI\t0x00000100\t/* Invalid op for integer convert */\n#define FPSCR_VE\t0x00000080\t/* Invalid op exception enable */\n#define FPSCR_OE\t0x00000040\t/* IEEE overflow exception enable */\n#define FPSCR_UE\t0x00000020\t/* IEEE underflow exception enable */\n#define FPSCR_ZE\t0x00000010\t/* IEEE zero divide exception enable */\n#define FPSCR_XE\t0x00000008\t/* FP inexact exception enable */\n#define FPSCR_NI\t0x00000004\t/* FPU non IEEE-Mode */\n#define FPSCR_RN\t0x00000003\t/* FPU rounding control */\n\n/* Bit definitions for SPEFSCR. */\n#define SPEFSCR_SOVH\t0x80000000\t/* Summary integer overflow high */\n#define SPEFSCR_OVH\t0x40000000\t/* Integer overflow high */\n#define SPEFSCR_FGH\t0x20000000\t/* Embedded FP guard bit high */\n#define SPEFSCR_FXH\t0x10000000\t/* Embedded FP sticky bit high */\n#define SPEFSCR_FINVH\t0x08000000\t/* Embedded FP invalid operation high */\n#define SPEFSCR_FDBZH\t0x04000000\t/* Embedded FP div by zero high */\n#define SPEFSCR_FUNFH\t0x02000000\t/* Embedded FP underflow high */\n#define SPEFSCR_FOVFH\t0x01000000\t/* Embedded FP overflow high */\n#define SPEFSCR_FINXS\t0x00200000\t/* Embedded FP inexact sticky */\n#define SPEFSCR_FINVS\t0x00100000\t/* Embedded FP invalid op. sticky */\n#define SPEFSCR_FDBZS\t0x00080000\t/* Embedded FP div by zero sticky */\n#define SPEFSCR_FUNFS\t0x00040000\t/* Embedded FP underflow sticky */\n#define SPEFSCR_FOVFS\t0x00020000\t/* Embedded FP overflow sticky */\n#define SPEFSCR_MODE\t0x00010000\t/* Embedded FP mode */\n#define SPEFSCR_SOV\t0x00008000\t/* Integer summary overflow */\n#define SPEFSCR_OV\t0x00004000\t/* Integer overflow */\n#define SPEFSCR_FG\t0x00002000\t/* Embedded FP guard bit */\n#define SPEFSCR_FX\t0x00001000\t/* Embedded FP sticky bit */\n#define SPEFSCR_FINV\t0x00000800\t/* Embedded FP invalid operation */\n#define SPEFSCR_FDBZ\t0x00000400\t/* Embedded FP div by zero */\n#define SPEFSCR_FUNF\t0x00000200\t/* Embedded FP underflow */\n#define SPEFSCR_FOVF\t0x00000100\t/* Embedded FP overflow */\n#define SPEFSCR_FINXE\t0x00000040\t/* Embedded FP inexact enable */\n#define SPEFSCR_FINVE\t0x00000020\t/* Embedded FP invalid op. enable */\n#define SPEFSCR_FDBZE\t0x00000010\t/* Embedded FP div by zero enable */\n#define SPEFSCR_FUNFE\t0x00000008\t/* Embedded FP underflow enable */\n#define SPEFSCR_FOVFE\t0x00000004\t/* Embedded FP overflow enable */\n#define SPEFSCR_FRMC \t0x00000003\t/* Embedded FP rounding mode control */\n\n/* Special Purpose Registers (SPRNs)*/\n#define SPRN_CTR\t0x009\t/* Count Register */\n#define SPRN_DSCR\t0x11\n#define SPRN_CTRLF\t0x088\n#define SPRN_CTRLT\t0x098\n#define   CTRL_CT\t0xc0000000\t/* current thread */\n#define   CTRL_CT0\t0x80000000\t/* thread 0 */\n#define   CTRL_CT1\t0x40000000\t/* thread 1 */\n#define   CTRL_TE\t0x00c00000\t/* thread enable */\n#define   CTRL_RUNLATCH\t0x1\n#define SPRN_DABR\t0x3F5\t/* Data Address Breakpoint Register */\n#define   DABR_TRANSLATION\t(1UL << 2)\n#define   DABR_DATA_WRITE\t(1UL << 1)\n#define   DABR_DATA_READ\t(1UL << 0)\n#define SPRN_DABR2\t0x13D\t/* e300 */\n#define SPRN_DABRX\t0x3F7\t/* Data Address Breakpoint Register Extension */\n#define   DABRX_USER\t(1UL << 0)\n#define   DABRX_KERNEL\t(1UL << 1)\n#define SPRN_DAR\t0x013\t/* Data Address Register */\n#define SPRN_DBCR\t0x136\t/* e300 Data Breakpoint Control Reg */\n#define SPRN_DSISR\t0x012\t/* Data Storage Interrupt Status Register */\n#define   DSISR_NOHPTE\t\t0x40000000\t/* no translation found */\n#define   DSISR_PROTFAULT\t0x08000000\t/* protection fault */\n#define   DSISR_ISSTORE\t\t0x02000000\t/* access was a store */\n#define   DSISR_DABRMATCH\t0x00400000\t/* hit data breakpoint */\n#define   DSISR_NOSEGMENT\t0x00200000\t/* STAB/SLB miss */\n#define SPRN_TBRL\t0x10C\t/* Time Base Read Lower Register (user, R/O) */\n#define SPRN_TBRU\t0x10D\t/* Time Base Read Upper Register (user, R/O) */\n#define SPRN_TBWL\t0x11C\t/* Time Base Lower Register (super, R/W) */\n#define SPRN_TBWU\t0x11D\t/* Time Base Upper Register (super, R/W) */\n#define SPRN_SPURR\t0x134\t/* Scaled PURR */\n#define SPRN_HIOR\t0x137\t/* 970 Hypervisor interrupt offset */\n#define SPRN_LPCR\t0x13E\t/* LPAR Control Register */\n#define SPRN_DBAT0L\t0x219\t/* Data BAT 0 Lower Register */\n#define SPRN_DBAT0U\t0x218\t/* Data BAT 0 Upper Register */\n#define SPRN_DBAT1L\t0x21B\t/* Data BAT 1 Lower Register */\n#define SPRN_DBAT1U\t0x21A\t/* Data BAT 1 Upper Register */\n#define SPRN_DBAT2L\t0x21D\t/* Data BAT 2 Lower Register */\n#define SPRN_DBAT2U\t0x21C\t/* Data BAT 2 Upper Register */\n#define SPRN_DBAT3L\t0x21F\t/* Data BAT 3 Lower Register */\n#define SPRN_DBAT3U\t0x21E\t/* Data BAT 3 Upper Register */\n#define SPRN_DBAT4L\t0x239\t/* Data BAT 4 Lower Register */\n#define SPRN_DBAT4U\t0x238\t/* Data BAT 4 Upper Register */\n#define SPRN_DBAT5L\t0x23B\t/* Data BAT 5 Lower Register */\n#define SPRN_DBAT5U\t0x23A\t/* Data BAT 5 Upper Register */\n#define SPRN_DBAT6L\t0x23D\t/* Data BAT 6 Lower Register */\n#define SPRN_DBAT6U\t0x23C\t/* Data BAT 6 Upper Register */\n#define SPRN_DBAT7L\t0x23F\t/* Data BAT 7 Lower Register */\n#define SPRN_DBAT7U\t0x23E\t/* Data BAT 7 Upper Register */\n\n#define SPRN_DEC\t0x016\t\t/* Decrement Register */\n#define SPRN_DER\t0x095\t\t/* Debug Enable Regsiter */\n#define DER_RSTE\t0x40000000\t/* Reset Interrupt */\n#define DER_CHSTPE\t0x20000000\t/* Check Stop */\n#define DER_MCIE\t0x10000000\t/* Machine Check Interrupt */\n#define DER_EXTIE\t0x02000000\t/* External Interrupt */\n#define DER_ALIE\t0x01000000\t/* Alignment Interrupt */\n#define DER_PRIE\t0x00800000\t/* Program Interrupt */\n#define DER_FPUVIE\t0x00400000\t/* FP Unavailable Interrupt */\n#define DER_DECIE\t0x00200000\t/* Decrementer Interrupt */\n#define DER_SYSIE\t0x00040000\t/* System Call Interrupt */\n#define DER_TRE\t\t0x00020000\t/* Trace Interrupt */\n#define DER_SEIE\t0x00004000\t/* FP SW Emulation Interrupt */\n#define DER_ITLBMSE\t0x00002000\t/* Imp. Spec. Instruction TLB Miss */\n#define DER_ITLBERE\t0x00001000\t/* Imp. Spec. Instruction TLB Error */\n#define DER_DTLBMSE\t0x00000800\t/* Imp. Spec. Data TLB Miss */\n#define DER_DTLBERE\t0x00000400\t/* Imp. Spec. Data TLB Error */\n#define DER_LBRKE\t0x00000008\t/* Load/Store Breakpoint Interrupt */\n#define DER_IBRKE\t0x00000004\t/* Instruction Breakpoint Interrupt */\n#define DER_EBRKE\t0x00000002\t/* External Breakpoint Interrupt */\n#define DER_DPIE\t0x00000001\t/* Dev. Port Nonmaskable Request */\n#define SPRN_DMISS\t0x3D0\t\t/* Data TLB Miss Register */\n#define SPRN_EAR\t0x11A\t\t/* External Address Register */\n#define SPRN_HASH1\t0x3D2\t\t/* Primary Hash Address Register */\n#define SPRN_HASH2\t0x3D3\t\t/* Secondary Hash Address Resgister */\n#define SPRN_HID0\t0x3F0\t\t/* Hardware Implementation Register 0 */\n#define HID0_EMCP\t(1<<31)\t\t/* Enable Machine Check pin */\n#define HID0_EBA\t(1<<29)\t\t/* Enable Bus Address Parity */\n#define HID0_EBD\t(1<<28)\t\t/* Enable Bus Data Parity */\n#define HID0_SBCLK\t(1<<27)\n#define HID0_EICE\t(1<<26)\n#define HID0_TBEN\t(1<<26)\t\t/* Timebase enable - 745x */\n#define HID0_ECLK\t(1<<25)\n#define HID0_PAR\t(1<<24)\n#define HID0_STEN\t(1<<24)\t\t/* Software table search enable - 745x */\n#define HID0_HIGH_BAT\t(1<<23)\t\t/* Enable high BATs - 7455 */\n#define HID0_DOZE\t(1<<23)\n#define HID0_NAP\t(1<<22)\n#define HID0_SLEEP\t(1<<21)\n#define HID0_DPM\t(1<<20)\n#define HID0_BHTCLR\t(1<<18)\t\t/* Clear branch history table - 7450 */\n#define HID0_XAEN\t(1<<17)\t\t/* Extended addressing enable - 7450 */\n#define HID0_NHR\t(1<<16)\t\t/* Not hard reset (software bit-7450)*/\n#define HID0_ICE\t(1<<15)\t\t/* Instruction Cache Enable */\n#define HID0_DCE\t(1<<14)\t\t/* Data Cache Enable */\n#define HID0_ILOCK\t(1<<13)\t\t/* Instruction Cache Lock */\n#define HID0_DLOCK\t(1<<12)\t\t/* Data Cache Lock */\n#define HID0_ICFI\t(1<<11)\t\t/* Instr. Cache Flash Invalidate */\n#define HID0_DCI\t(1<<10)\t\t/* Data Cache Invalidate */\n#define HID0_SPD\t(1<<9)\t\t/* Speculative disable */\n#define HID0_DAPUEN\t(1<<8)\t\t/* Debug APU enable */\n#define HID0_SGE\t(1<<7)\t\t/* Store Gathering Enable */\n#define HID0_SIED\t(1<<7)\t\t/* Serial Instr. Execution [Disable] */\n#define HID0_DCFA\t(1<<6)\t\t/* Data Cache Flush Assist */\n#define HID0_LRSTK\t(1<<4)\t\t/* Link register stack - 745x */\n#define HID0_BTIC\t(1<<5)\t\t/* Branch Target Instr Cache Enable */\n#define HID0_ABE\t(1<<3)\t\t/* Address Broadcast Enable */\n#define HID0_FOLD\t(1<<3)\t\t/* Branch Folding enable - 745x */\n#define HID0_BHTE\t(1<<2)\t\t/* Branch History Table Enable */\n#define HID0_BTCD\t(1<<1)\t\t/* Branch target cache disable */\n#define HID0_NOPDST\t(1<<1)\t\t/* No-op dst, dstt, etc. instr. */\n#define HID0_NOPTI\t(1<<0)\t\t/* No-op dcbt and dcbst instr. */\n\n#define SPRN_HID1\t0x3F1\t\t/* Hardware Implementation Register 1 */\n#ifdef CONFIG_6xx\n#define HID1_EMCP\t(1<<31)\t\t/* 7450 Machine Check Pin Enable */\n#define HID1_DFS\t(1<<22)\t\t/* 7447A Dynamic Frequency Scaling */\n#define HID1_PC0\t(1<<16)\t\t/* 7450 PLL_CFG[0] */\n#define HID1_PC1\t(1<<15)\t\t/* 7450 PLL_CFG[1] */\n#define HID1_PC2\t(1<<14)\t\t/* 7450 PLL_CFG[2] */\n#define HID1_PC3\t(1<<13)\t\t/* 7450 PLL_CFG[3] */\n#define HID1_SYNCBE\t(1<<11)\t\t/* 7450 ABE for sync, eieio */\n#define HID1_ABE\t(1<<10)\t\t/* 7450 Address Broadcast Enable */\n#define HID1_PS\t\t(1<<16)\t\t/* 750FX PLL selection */\n#endif\n#define SPRN_HID2\t0x3F8\t\t/* Hardware Implementation Register 2 */\n#define SPRN_HID2_GEKKO\t0x398\t\t/* Gekko HID2 Register */\n#define SPRN_IABR\t0x3F2\t/* Instruction Address Breakpoint Register */\n#define SPRN_IABR2\t0x3FA\t\t/* 83xx */\n#define SPRN_IBCR\t0x135\t\t/* 83xx Insn Breakpoint Control Reg */\n#define SPRN_HID4\t0x3F4\t\t/* 970 HID4 */\n#define SPRN_HID4_GEKKO\t0x3F3\t\t/* Gekko HID4 */\n#define SPRN_HID5\t0x3F6\t\t/* 970 HID5 */\n#define SPRN_HID6\t0x3F9\t/* BE HID 6 */\n#define   HID6_LB\t(0x0F<<12) /* Concurrent Large Page Modes */\n#define   HID6_DLP\t(1<<20)\t/* Disable all large page modes (4K only) */\n#define SPRN_TSC_CELL\t0x399\t/* Thread switch control on Cell */\n#define   TSC_CELL_DEC_ENABLE_0\t0x400000 /* Decrementer Interrupt */\n#define   TSC_CELL_DEC_ENABLE_1\t0x200000 /* Decrementer Interrupt */\n#define   TSC_CELL_EE_ENABLE\t0x100000 /* External Interrupt */\n#define   TSC_CELL_EE_BOOST\t0x080000 /* External Interrupt Boost */\n#define SPRN_TSC \t0x3FD\t/* Thread switch control on others */\n#define SPRN_TST \t0x3FC\t/* Thread switch timeout on others */\n#if !defined(SPRN_IAC1) && !defined(SPRN_IAC2)\n#define SPRN_IAC1\t0x3F4\t\t/* Instruction Address Compare 1 */\n#define SPRN_IAC2\t0x3F5\t\t/* Instruction Address Compare 2 */\n#endif\n#define SPRN_IBAT0L\t0x211\t\t/* Instruction BAT 0 Lower Register */\n#define SPRN_IBAT0U\t0x210\t\t/* Instruction BAT 0 Upper Register */\n#define SPRN_IBAT1L\t0x213\t\t/* Instruction BAT 1 Lower Register */\n#define SPRN_IBAT1U\t0x212\t\t/* Instruction BAT 1 Upper Register */\n#define SPRN_IBAT2L\t0x215\t\t/* Instruction BAT 2 Lower Register */\n#define SPRN_IBAT2U\t0x214\t\t/* Instruction BAT 2 Upper Register */\n#define SPRN_IBAT3L\t0x217\t\t/* Instruction BAT 3 Lower Register */\n#define SPRN_IBAT3U\t0x216\t\t/* Instruction BAT 3 Upper Register */\n#define SPRN_IBAT4L\t0x231\t\t/* Instruction BAT 4 Lower Register */\n#define SPRN_IBAT4U\t0x230\t\t/* Instruction BAT 4 Upper Register */\n#define SPRN_IBAT5L\t0x233\t\t/* Instruction BAT 5 Lower Register */\n#define SPRN_IBAT5U\t0x232\t\t/* Instruction BAT 5 Upper Register */\n#define SPRN_IBAT6L\t0x235\t\t/* Instruction BAT 6 Lower Register */\n#define SPRN_IBAT6U\t0x234\t\t/* Instruction BAT 6 Upper Register */\n#define SPRN_IBAT7L\t0x237\t\t/* Instruction BAT 7 Lower Register */\n#define SPRN_IBAT7U\t0x236\t\t/* Instruction BAT 7 Upper Register */\n#define SPRN_ICMP\t0x3D5\t\t/* Instruction TLB Compare Register */\n#define SPRN_ICTC\t0x3FB\t/* Instruction Cache Throttling Control Reg */\n#define SPRN_ICTRL\t0x3F3\t/* 1011 7450 icache and interrupt ctrl */\n#define ICTRL_EICE\t0x08000000\t/* enable icache parity errs */\n#define ICTRL_EDC\t0x04000000\t/* enable dcache parity errs */\n#define ICTRL_EICP\t0x00000100\t/* enable icache par. check */\n#define SPRN_IMISS\t0x3D4\t\t/* Instruction TLB Miss Register */\n#define SPRN_IMMR\t0x27E\t\t/* Internal Memory Map Register */\n#define SPRN_L2CR\t0x3F9\t\t/* Level 2 Cache Control Regsiter */\n#define SPRN_L2CR2\t0x3f8\n#define L2CR_L2E\t\t0x80000000\t/* L2 enable */\n#define L2CR_L2PE\t\t0x40000000\t/* L2 parity enable */\n#define L2CR_L2SIZ_MASK\t\t0x30000000\t/* L2 size mask */\n#define L2CR_L2SIZ_256KB\t0x10000000\t/* L2 size 256KB */\n#define L2CR_L2SIZ_512KB\t0x20000000\t/* L2 size 512KB */\n#define L2CR_L2SIZ_1MB\t\t0x30000000\t/* L2 size 1MB */\n#define L2CR_L2CLK_MASK\t\t0x0e000000\t/* L2 clock mask */\n#define L2CR_L2CLK_DISABLED\t0x00000000\t/* L2 clock disabled */\n#define L2CR_L2CLK_DIV1\t\t0x02000000\t/* L2 clock / 1 */\n#define L2CR_L2CLK_DIV1_5\t0x04000000\t/* L2 clock / 1.5 */\n#define L2CR_L2CLK_DIV2\t\t0x08000000\t/* L2 clock / 2 */\n#define L2CR_L2CLK_DIV2_5\t0x0a000000\t/* L2 clock / 2.5 */\n#define L2CR_L2CLK_DIV3\t\t0x0c000000\t/* L2 clock / 3 */\n#define L2CR_L2RAM_MASK\t\t0x01800000\t/* L2 RAM type mask */\n#define L2CR_L2RAM_FLOW\t\t0x00000000\t/* L2 RAM flow through */\n#define L2CR_L2RAM_PIPE\t\t0x01000000\t/* L2 RAM pipelined */\n#define L2CR_L2RAM_PIPE_LW\t0x01800000\t/* L2 RAM pipelined latewr */\n#define L2CR_L2DO\t\t0x00400000\t/* L2 data only */\n#define L2CR_L2I\t\t0x00200000\t/* L2 global invalidate */\n#define L2CR_L2CTL\t\t0x00100000\t/* L2 RAM control */\n#define L2CR_L2WT\t\t0x00080000\t/* L2 write-through */\n#define L2CR_L2TS\t\t0x00040000\t/* L2 test support */\n#define L2CR_L2OH_MASK\t\t0x00030000\t/* L2 output hold mask */\n#define L2CR_L2OH_0_5\t\t0x00000000\t/* L2 output hold 0.5 ns */\n#define L2CR_L2OH_1_0\t\t0x00010000\t/* L2 output hold 1.0 ns */\n#define L2CR_L2SL\t\t0x00008000\t/* L2 DLL slow */\n#define L2CR_L2DF\t\t0x00004000\t/* L2 differential clock */\n#define L2CR_L2BYP\t\t0x00002000\t/* L2 DLL bypass */\n#define L2CR_L2IP\t\t0x00000001\t/* L2 GI in progress */\n#define L2CR_L2IO_745x\t\t0x00100000\t/* L2 instr. only (745x) */\n#define L2CR_L2DO_745x\t\t0x00010000\t/* L2 data only (745x) */\n#define L2CR_L2REP_745x\t\t0x00001000\t/* L2 repl. algorithm (745x) */\n#define L2CR_L2HWF_745x\t\t0x00000800\t/* L2 hardware flush (745x) */\n#define SPRN_L3CR\t\t0x3FA\t/* Level 3 Cache Control Regsiter */\n#define L3CR_L3E\t\t0x80000000\t/* L3 enable */\n#define L3CR_L3PE\t\t0x40000000\t/* L3 data parity enable */\n#define L3CR_L3APE\t\t0x20000000\t/* L3 addr parity enable */\n#define L3CR_L3SIZ\t\t0x10000000\t/* L3 size */\n#define L3CR_L3CLKEN\t\t0x08000000\t/* L3 clock enable */\n#define L3CR_L3RES\t\t0x04000000\t/* L3 special reserved bit */\n#define L3CR_L3CLKDIV\t\t0x03800000\t/* L3 clock divisor */\n#define L3CR_L3IO\t\t0x00400000\t/* L3 instruction only */\n#define L3CR_L3SPO\t\t0x00040000\t/* L3 sample point override */\n#define L3CR_L3CKSP\t\t0x00030000\t/* L3 clock sample point */\n#define L3CR_L3PSP\t\t0x0000e000\t/* L3 P-clock sample point */\n#define L3CR_L3REP\t\t0x00001000\t/* L3 replacement algorithm */\n#define L3CR_L3HWF\t\t0x00000800\t/* L3 hardware flush */\n#define L3CR_L3I\t\t0x00000400\t/* L3 global invalidate */\n#define L3CR_L3RT\t\t0x00000300\t/* L3 SRAM type */\n#define L3CR_L3NIRCA\t\t0x00000080\t/* L3 non-integer ratio clock adj. */\n#define L3CR_L3DO\t\t0x00000040\t/* L3 data only mode */\n#define L3CR_PMEN\t\t0x00000004\t/* L3 private memory enable */\n#define L3CR_PMSIZ\t\t0x00000001\t/* L3 private memory size */\n\n#define SPRN_MSSCR0\t0x3f6\t/* Memory Subsystem Control Register 0 */\n#define SPRN_MSSSR0\t0x3f7\t/* Memory Subsystem Status Register 1 */\n#define SPRN_LDSTCR\t0x3f8\t/* Load/Store control register */\n#define SPRN_LDSTDB\t0x3f4\t/* */\n#define SPRN_LR\t\t0x008\t/* Link Register */\n#ifndef SPRN_PIR\n#define SPRN_PIR\t0x3FF\t/* Processor Identification Register */\n#endif\n#define SPRN_PTEHI\t0x3D5\t/* 981 7450 PTE HI word (S/W TLB load) */\n#define SPRN_PTELO\t0x3D6\t/* 982 7450 PTE LO word (S/W TLB load) */\n#define SPRN_PURR\t0x135\t/* Processor Utilization of Resources Reg */\n#define SPRN_PVR\t0x11F\t/* Processor Version Register */\n#define SPRN_RPA\t0x3D6\t/* Required Physical Address Register */\n#define SPRN_SDA\t0x3BF\t/* Sampled Data Address Register */\n#define SPRN_SDR1\t0x019\t/* MMU Hash Base Register */\n#define SPRN_ASR\t0x118   /* Address Space Register */\n#define SPRN_SIA\t0x3BB\t/* Sampled Instruction Address Register */\n#define SPRN_SPRG0\t0x110\t/* Special Purpose Register General 0 */\n#define SPRN_SPRG1\t0x111\t/* Special Purpose Register General 1 */\n#define SPRN_SPRG2\t0x112\t/* Special Purpose Register General 2 */\n#define SPRN_SPRG3\t0x113\t/* Special Purpose Register General 3 */\n#define SPRN_SPRG4\t0x114\t/* Special Purpose Register General 4 */\n#define SPRN_SPRG5\t0x115\t/* Special Purpose Register General 5 */\n#define SPRN_SPRG6\t0x116\t/* Special Purpose Register General 6 */\n#define SPRN_SPRG7\t0x117\t/* Special Purpose Register General 7 */\n#define SPRN_SRR0\t0x01A\t/* Save/Restore Register 0 */\n#define SPRN_SRR1\t0x01B\t/* Save/Restore Register 1 */\n#define   SRR1_WAKEMASK\t\t0x00380000 /* reason for wakeup */\n#define   SRR1_WAKERESET\t0x00380000 /* System reset */\n#define   SRR1_WAKESYSERR\t0x00300000 /* System error */\n#define   SRR1_WAKEEE\t\t0x00200000 /* External interrupt */\n#define   SRR1_WAKEMT\t\t0x00280000 /* mtctrl */\n#define   SRR1_WAKEDEC\t\t0x00180000 /* Decrementer interrupt */\n#define   SRR1_WAKETHERM\t0x00100000 /* Thermal management interrupt */\n#define   SRR1_PROGFPE\t\t0x00100000 /* Floating Point Enabled */\n#define   SRR1_PROGPRIV\t\t0x00040000 /* Privileged instruction */\n#define   SRR1_PROGTRAP\t\t0x00020000 /* Trap */\n#define   SRR1_PROGADDR\t\t0x00010000 /* SRR0 contains subsequent addr */\n#define SPRN_HSRR0\t0x13A\t/* Save/Restore Register 0 */\n#define SPRN_HSRR1\t0x13B\t/* Save/Restore Register 1 */\n\n#define SPRN_TBCTL\t0x35f\t/* PA6T Timebase control register */\n#define   TBCTL_FREEZE\t\t0x0000000000000000ull /* Freeze all tbs */\n#define   TBCTL_RESTART\t\t0x0000000100000000ull /* Restart all tbs */\n#define   TBCTL_UPDATE_UPPER\t0x0000000200000000ull /* Set upper 32 bits */\n#define   TBCTL_UPDATE_LOWER\t0x0000000300000000ull /* Set lower 32 bits */\n\n#ifndef SPRN_SVR\n#define SPRN_SVR\t0x11E\t/* System Version Register */\n#endif\n#define SPRN_THRM1\t0x3FC\t\t/* Thermal Management Register 1 */\n/* these bits were defined in inverted endian sense originally, ugh, confusing */\n#define THRM1_TIN\t(1 << 31)\n#define THRM1_TIV\t(1 << 30)\n#define THRM1_THRES(x)\t((x&0x7f)<<23)\n#define THRM3_SITV(x)\t((x&0x3fff)<<1)\n#define THRM1_TID\t(1<<2)\n#define THRM1_TIE\t(1<<1)\n#define THRM1_V\t\t(1<<0)\n#define SPRN_THRM2\t0x3FD\t\t/* Thermal Management Register 2 */\n#define SPRN_THRM3\t0x3FE\t\t/* Thermal Management Register 3 */\n#define THRM3_E\t\t(1<<0)\n#define SPRN_TLBMISS\t0x3D4\t\t/* 980 7450 TLB Miss Register */\n#define SPRN_UMMCR0\t0x3A8\t/* User Monitor Mode Control Register 0 */\n#define SPRN_UMMCR1\t0x3AC\t/* User Monitor Mode Control Register 0 */\n#define SPRN_UPMC1\t0x3A9\t/* User Performance Counter Register 1 */\n#define SPRN_UPMC2\t0x3AA\t/* User Performance Counter Register 2 */\n#define SPRN_UPMC3\t0x3AD\t/* User Performance Counter Register 3 */\n#define SPRN_UPMC4\t0x3AE\t/* User Performance Counter Register 4 */\n#define SPRN_USIA\t0x3AB\t/* User Sampled Instruction Address Register */\n#define SPRN_VRSAVE\t0x100\t/* Vector Register Save Register */\n#define SPRN_XER\t0x001\t/* Fixed Point Exception Register */\n\n#define SPRN_MMCR0_GEKKO 0x3B8 /* Gekko Monitor Mode Control Register 0 */\n#define SPRN_MMCR1_GEKKO 0x3BC /* Gekko Monitor Mode Control Register 1 */\n#define SPRN_PMC1_GEKKO  0x3B9 /* Gekko Performance Monitor Control 1 */\n#define SPRN_PMC2_GEKKO  0x3BA /* Gekko Performance Monitor Control 2 */\n#define SPRN_PMC3_GEKKO  0x3BD /* Gekko Performance Monitor Control 3 */\n#define SPRN_PMC4_GEKKO  0x3BE /* Gekko Performance Monitor Control 4 */\n#define SPRN_WPAR_GEKKO  0x399 /* Gekko Write Pipe Address Register */\n\n#define SPRN_SCOMC\t0x114\t/* SCOM Access Control */\n#define SPRN_SCOMD\t0x115\t/* SCOM Access DATA */\n\n/* Performance monitor SPRs */\n#ifdef CONFIG_PPC64\n#define SPRN_MMCR0\t795\n#define   MMCR0_FC\t0x80000000UL /* freeze counters */\n#define   MMCR0_FCS\t0x40000000UL /* freeze in supervisor state */\n#define   MMCR0_KERNEL_DISABLE MMCR0_FCS\n#define   MMCR0_FCP\t0x20000000UL /* freeze in problem state */\n#define   MMCR0_PROBLEM_DISABLE MMCR0_FCP\n#define   MMCR0_FCM1\t0x10000000UL /* freeze counters while MSR mark = 1 */\n#define   MMCR0_FCM0\t0x08000000UL /* freeze counters while MSR mark = 0 */\n#define   MMCR0_PMXE\t0x04000000UL /* performance monitor exception enable */\n#define   MMCR0_FCECE\t0x02000000UL /* freeze ctrs on enabled cond or event */\n#define   MMCR0_TBEE\t0x00400000UL /* time base exception enable */\n#define   MMCR0_PMC1CE\t0x00008000UL /* PMC1 count enable*/\n#define   MMCR0_PMCjCE\t0x00004000UL /* PMCj count enable*/\n#define   MMCR0_TRIGGER\t0x00002000UL /* TRIGGER enable */\n#define   MMCR0_PMAO\t0x00000080UL /* performance monitor alert has occurred, set to 0 after handling exception */\n#define   MMCR0_SHRFC\t0x00000040UL /* SHRre freeze conditions between threads */\n#define   MMCR0_FCTI\t0x00000008UL /* freeze counters in tags inactive mode */\n#define   MMCR0_FCTA\t0x00000004UL /* freeze counters in tags active mode */\n#define   MMCR0_FCWAIT\t0x00000002UL /* freeze counter in WAIT state */\n#define   MMCR0_FCHV\t0x00000001UL /* freeze conditions in hypervisor mode */\n#define SPRN_MMCR1\t798\n#define SPRN_MMCRA\t0x312\n#define   MMCRA_SDSYNC\t0x80000000UL /* SDAR synced with SIAR */\n#define   MMCRA_SDAR_DCACHE_MISS 0x40000000UL\n#define   MMCRA_SDAR_ERAT_MISS   0x20000000UL\n#define   MMCRA_SIHV\t0x10000000UL /* state of MSR HV when SIAR set */\n#define   MMCRA_SIPR\t0x08000000UL /* state of MSR PR when SIAR set */\n#define   MMCRA_SLOT\t0x07000000UL /* SLOT bits (37-39) */\n#define   MMCRA_SLOT_SHIFT\t24\n#define   MMCRA_SAMPLE_ENABLE 0x00000001UL /* enable sampling */\n#define   POWER6_MMCRA_SDSYNC 0x0000080000000000ULL\t/* SDAR/SIAR synced */\n#define   POWER6_MMCRA_SIHV   0x0000040000000000ULL\n#define   POWER6_MMCRA_SIPR   0x0000020000000000ULL\n#define   POWER6_MMCRA_THRM\t0x00000020UL\n#define   POWER6_MMCRA_OTHER\t0x0000000EUL\n#define SPRN_PMC1\t787\n#define SPRN_PMC2\t788\n#define SPRN_PMC3\t789\n#define SPRN_PMC4\t790\n#define SPRN_PMC5\t791\n#define SPRN_PMC6\t792\n#define SPRN_PMC7\t793\n#define SPRN_PMC8\t794\n#define SPRN_SIAR\t780\n#define SPRN_SDAR\t781\n\n#define SPRN_PA6T_MMCR0 795\n#define   PA6T_MMCR0_EN0\t0x0000000000000001UL\n#define   PA6T_MMCR0_EN1\t0x0000000000000002UL\n#define   PA6T_MMCR0_EN2\t0x0000000000000004UL\n#define   PA6T_MMCR0_EN3\t0x0000000000000008UL\n#define   PA6T_MMCR0_EN4\t0x0000000000000010UL\n#define   PA6T_MMCR0_EN5\t0x0000000000000020UL\n#define   PA6T_MMCR0_SUPEN\t0x0000000000000040UL\n#define   PA6T_MMCR0_PREN\t0x0000000000000080UL\n#define   PA6T_MMCR0_HYPEN\t0x0000000000000100UL\n#define   PA6T_MMCR0_FCM0\t0x0000000000000200UL\n#define   PA6T_MMCR0_FCM1\t0x0000000000000400UL\n#define   PA6T_MMCR0_INTGEN\t0x0000000000000800UL\n#define   PA6T_MMCR0_INTEN0\t0x0000000000001000UL\n#define   PA6T_MMCR0_INTEN1\t0x0000000000002000UL\n#define   PA6T_MMCR0_INTEN2\t0x0000000000004000UL\n#define   PA6T_MMCR0_INTEN3\t0x0000000000008000UL\n#define   PA6T_MMCR0_INTEN4\t0x0000000000010000UL\n#define   PA6T_MMCR0_INTEN5\t0x0000000000020000UL\n#define   PA6T_MMCR0_DISCNT\t0x0000000000040000UL\n#define   PA6T_MMCR0_UOP\t0x0000000000080000UL\n#define   PA6T_MMCR0_TRG\t0x0000000000100000UL\n#define   PA6T_MMCR0_TRGEN\t0x0000000000200000UL\n#define   PA6T_MMCR0_TRGREG\t0x0000000001600000UL\n#define   PA6T_MMCR0_SIARLOG\t0x0000000002000000UL\n#define   PA6T_MMCR0_SDARLOG\t0x0000000004000000UL\n#define   PA6T_MMCR0_PROEN\t0x0000000008000000UL\n#define   PA6T_MMCR0_PROLOG\t0x0000000010000000UL\n#define   PA6T_MMCR0_DAMEN2\t0x0000000020000000UL\n#define   PA6T_MMCR0_DAMEN3\t0x0000000040000000UL\n#define   PA6T_MMCR0_DAMEN4\t0x0000000080000000UL\n#define   PA6T_MMCR0_DAMEN5\t0x0000000100000000UL\n#define   PA6T_MMCR0_DAMSEL2\t0x0000000200000000UL\n#define   PA6T_MMCR0_DAMSEL3\t0x0000000400000000UL\n#define   PA6T_MMCR0_DAMSEL4\t0x0000000800000000UL\n#define   PA6T_MMCR0_DAMSEL5\t0x0000001000000000UL\n#define   PA6T_MMCR0_HANDDIS\t0x0000002000000000UL\n#define   PA6T_MMCR0_PCTEN\t0x0000004000000000UL\n#define   PA6T_MMCR0_SOCEN\t0x0000008000000000UL\n#define   PA6T_MMCR0_SOCMOD\t0x0000010000000000UL\n\n#define SPRN_PA6T_MMCR1 798\n#define   PA6T_MMCR1_ES2\t0x00000000000000ffUL\n#define   PA6T_MMCR1_ES3\t0x000000000000ff00UL\n#define   PA6T_MMCR1_ES4\t0x0000000000ff0000UL\n#define   PA6T_MMCR1_ES5\t0x00000000ff000000UL\n\n#define SPRN_PA6T_UPMC0 771\t/* User PerfMon Counter 0 */\n#define SPRN_PA6T_UPMC1 772\t/* ... */\n#define SPRN_PA6T_UPMC2 773\n#define SPRN_PA6T_UPMC3 774\n#define SPRN_PA6T_UPMC4 775\n#define SPRN_PA6T_UPMC5 776\n#define SPRN_PA6T_UMMCR0 779\t/* User Monitor Mode Control Register 0 */\n#define SPRN_PA6T_SIAR\t780\t/* Sampled Instruction Address */\n#define SPRN_PA6T_UMMCR1 782\t/* User Monitor Mode Control Register 1 */\n#define SPRN_PA6T_SIER\t785\t/* Sampled Instruction Event Register */\n#define SPRN_PA6T_PMC0\t787\n#define SPRN_PA6T_PMC1\t788\n#define SPRN_PA6T_PMC2\t789\n#define SPRN_PA6T_PMC3\t790\n#define SPRN_PA6T_PMC4\t791\n#define SPRN_PA6T_PMC5\t792\n#define SPRN_PA6T_TSR0\t793\t/* Timestamp Register 0 */\n#define SPRN_PA6T_TSR1\t794\t/* Timestamp Register 1 */\n#define SPRN_PA6T_TSR2\t799\t/* Timestamp Register 2 */\n#define SPRN_PA6T_TSR3\t784\t/* Timestamp Register 3 */\n\n#define SPRN_PA6T_IER\t981\t/* Icache Error Register */\n#define SPRN_PA6T_DER\t982\t/* Dcache Error Register */\n#define SPRN_PA6T_BER\t862\t/* BIU Error Address Register */\n#define SPRN_PA6T_MER\t849\t/* MMU Error Register */\n\n#define SPRN_PA6T_IMA0\t880\t/* Instruction Match Array 0 */\n#define SPRN_PA6T_IMA1\t881\t/* ... */\n#define SPRN_PA6T_IMA2\t882\n#define SPRN_PA6T_IMA3\t883\n#define SPRN_PA6T_IMA4\t884\n#define SPRN_PA6T_IMA5\t885\n#define SPRN_PA6T_IMA6\t886\n#define SPRN_PA6T_IMA7\t887\n#define SPRN_PA6T_IMA8\t888\n#define SPRN_PA6T_IMA9\t889\n#define SPRN_PA6T_BTCR\t978\t/* Breakpoint and Tagging Control Register */\n#define SPRN_PA6T_IMAAT\t979\t/* Instruction Match Array Action Table */\n#define SPRN_PA6T_PCCR\t1019\t/* Power Counter Control Register */\n#define SPRN_BKMK\t1020\t/* Cell Bookmark Register */\n#define SPRN_PA6T_RPCCR\t1021\t/* Retire PC Trace Control Register */\n\n\n#else /* 32-bit */\n#define SPRN_MMCR0\t952\t/* Monitor Mode Control Register 0 */\n#define   MMCR0_FC\t0x80000000UL /* freeze counters */\n#define   MMCR0_FCS\t0x40000000UL /* freeze in supervisor state */\n#define   MMCR0_FCP\t0x20000000UL /* freeze in problem state */\n#define   MMCR0_FCM1\t0x10000000UL /* freeze counters while MSR mark = 1 */\n#define   MMCR0_FCM0\t0x08000000UL /* freeze counters while MSR mark = 0 */\n#define   MMCR0_PMXE\t0x04000000UL /* performance monitor exception enable */\n#define   MMCR0_FCECE\t0x02000000UL /* freeze ctrs on enabled cond or event */\n#define   MMCR0_TBEE\t0x00400000UL /* time base exception enable */\n#define   MMCR0_PMC1CE\t0x00008000UL /* PMC1 count enable*/\n#define   MMCR0_PMCnCE\t0x00004000UL /* count enable for all but PMC 1*/\n#define   MMCR0_TRIGGER\t0x00002000UL /* TRIGGER enable */\n#define   MMCR0_PMC1SEL\t0x00001fc0UL /* PMC 1 Event */\n#define   MMCR0_PMC2SEL\t0x0000003fUL /* PMC 2 Event */\n\n#define SPRN_MMCR1\t956\n#define   MMCR1_PMC3SEL\t0xf8000000UL /* PMC 3 Event */\n#define   MMCR1_PMC4SEL\t0x07c00000UL /* PMC 4 Event */\n#define   MMCR1_PMC5SEL\t0x003e0000UL /* PMC 5 Event */\n#define   MMCR1_PMC6SEL 0x0001f800UL /* PMC 6 Event */\n#define SPRN_MMCR2\t944\n#define SPRN_PMC1\t953\t/* Performance Counter Register 1 */\n#define SPRN_PMC2\t954\t/* Performance Counter Register 2 */\n#define SPRN_PMC3\t957\t/* Performance Counter Register 3 */\n#define SPRN_PMC4\t958\t/* Performance Counter Register 4 */\n#define SPRN_PMC5\t945\t/* Performance Counter Register 5 */\n#define SPRN_PMC6\t946\t/* Performance Counter Register 6 */\n\n#define SPRN_SIAR\t955\t/* Sampled Instruction Address Register */\n\n/* Bit definitions for MMCR0 and PMC1 / PMC2. */\n#define MMCR0_PMC1_CYCLES\t(1 << 7)\n#define MMCR0_PMC1_ICACHEMISS\t(5 << 7)\n#define MMCR0_PMC1_DTLB\t\t(6 << 7)\n#define MMCR0_PMC2_DCACHEMISS\t0x6\n#define MMCR0_PMC2_CYCLES\t0x1\n#define MMCR0_PMC2_ITLB\t\t0x7\n#define MMCR0_PMC2_LOADMISSTIME\t0x5\n#endif\n\n/*\n * SPRG usage:\n *\n * All 64-bit:\n *\t- SPRG1 stores PACA pointer\n *\n * 64-bit server:\n *\t- SPRG0 unused (reserved for HV on Power4)\n *\t- SPRG2 scratch for exception vectors\n *\t- SPRG3 unused (user visible)\n *\n * 64-bit embedded\n *\t- SPRG0 generic exception scratch\n *\t- SPRG2 TLB exception stack\n *\t- SPRG3 unused (user visible)\n *\t- SPRG4 unused (user visible)\n *\t- SPRG6 TLB miss scratch (user visible, sorry !)\n *\t- SPRG7 critical exception scratch\n *\t- SPRG8 machine check exception scratch\n *\t- SPRG9 debug exception scratch\n *\n * All 32-bit:\n *\t- SPRG3 current thread_info pointer\n *        (virtual on BookE, physical on others)\n *\n * 32-bit classic:\n *\t- SPRG0 scratch for exception vectors\n *\t- SPRG1 scratch for exception vectors\n *\t- SPRG2 indicator that we are in RTAS\n *\t- SPRG4 (603 only) pseudo TLB LRU data\n *\n * 32-bit 40x:\n *\t- SPRG0 scratch for exception vectors\n *\t- SPRG1 scratch for exception vectors\n *\t- SPRG2 scratch for exception vectors\n *\t- SPRG4 scratch for exception vectors (not 403)\n *\t- SPRG5 scratch for exception vectors (not 403)\n *\t- SPRG6 scratch for exception vectors (not 403)\n *\t- SPRG7 scratch for exception vectors (not 403)\n *\n * 32-bit 440 and FSL BookE:\n *\t- SPRG0 scratch for exception vectors\n *\t- SPRG1 scratch for exception vectors (*)\n *\t- SPRG2 scratch for crit interrupts handler\n *\t- SPRG4 scratch for exception vectors\n *\t- SPRG5 scratch for exception vectors\n *\t- SPRG6 scratch for machine check handler\n *\t- SPRG7 scratch for exception vectors\n *\t- SPRG9 scratch for debug vectors (e500 only)\n *\n *      Additionally, BookE separates \"read\" and \"write\"\n *      of those registers. That allows to use the userspace\n *      readable variant for reads, which can avoid a fault\n *      with KVM type virtualization.\n *\n *      (*) Under KVM, the host SPRG1 is used to point to\n *      the current VCPU data structure\n *\n * 32-bit 8xx:\n *\t- SPRG0 scratch for exception vectors\n *\t- SPRG1 scratch for exception vectors\n *\t- SPRG2 apparently unused but initialized\n *\n */\n#ifdef CONFIG_PPC64\n#define SPRN_SPRG_PACA \t\tSPRN_SPRG1\n#else\n#define SPRN_SPRG_THREAD \tSPRN_SPRG3\n#endif\n\n#ifdef CONFIG_PPC_BOOK3S_64\n#define SPRN_SPRG_SCRATCH0\tSPRN_SPRG2\n#endif\n\n#ifdef CONFIG_PPC_BOOK3E_64\n#define SPRN_SPRG_MC_SCRATCH\tSPRN_SPRG8\n#define SPRN_SPRG_CRIT_SCRATCH\tSPRN_SPRG7\n#define SPRN_SPRG_DBG_SCRATCH\tSPRN_SPRG9\n#define SPRN_SPRG_TLB_EXFRAME\tSPRN_SPRG2\n#define SPRN_SPRG_TLB_SCRATCH\tSPRN_SPRG6\n#define SPRN_SPRG_GEN_SCRATCH\tSPRN_SPRG0\n#endif\n\n#ifdef CONFIG_PPC_BOOK3S_32\n#define SPRN_SPRG_SCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_SCRATCH1\tSPRN_SPRG1\n#define SPRN_SPRG_RTAS\t\tSPRN_SPRG2\n#define SPRN_SPRG_603_LRU\tSPRN_SPRG4\n#endif\n\n#ifdef CONFIG_40x\n#define SPRN_SPRG_SCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_SCRATCH1\tSPRN_SPRG1\n#define SPRN_SPRG_SCRATCH2\tSPRN_SPRG2\n#define SPRN_SPRG_SCRATCH3\tSPRN_SPRG4\n#define SPRN_SPRG_SCRATCH4\tSPRN_SPRG5\n#define SPRN_SPRG_SCRATCH5\tSPRN_SPRG6\n#define SPRN_SPRG_SCRATCH6\tSPRN_SPRG7\n#endif\n\n#ifdef CONFIG_BOOKE\n#define SPRN_SPRG_RSCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_WSCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_RSCRATCH1\tSPRN_SPRG1\n#define SPRN_SPRG_WSCRATCH1\tSPRN_SPRG1\n#define SPRN_SPRG_RSCRATCH_CRIT\tSPRN_SPRG2\n#define SPRN_SPRG_WSCRATCH_CRIT\tSPRN_SPRG2\n#define SPRN_SPRG_RSCRATCH2\tSPRN_SPRG4R\n#define SPRN_SPRG_WSCRATCH2\tSPRN_SPRG4W\n#define SPRN_SPRG_RSCRATCH3\tSPRN_SPRG5R\n#define SPRN_SPRG_WSCRATCH3\tSPRN_SPRG5W\n#define SPRN_SPRG_RSCRATCH_MC\tSPRN_SPRG6R\n#define SPRN_SPRG_WSCRATCH_MC\tSPRN_SPRG6W\n#define SPRN_SPRG_RSCRATCH4\tSPRN_SPRG7R\n#define SPRN_SPRG_WSCRATCH4\tSPRN_SPRG7W\n#ifdef CONFIG_E200\n#define SPRN_SPRG_RSCRATCH_DBG\tSPRN_SPRG6R\n#define SPRN_SPRG_WSCRATCH_DBG\tSPRN_SPRG6W\n#else\n#define SPRN_SPRG_RSCRATCH_DBG\tSPRN_SPRG9\n#define SPRN_SPRG_WSCRATCH_DBG\tSPRN_SPRG9\n#endif\n#define SPRN_SPRG_RVCPU\t\tSPRN_SPRG1\n#define SPRN_SPRG_WVCPU\t\tSPRN_SPRG1\n#endif\n\n#ifdef CONFIG_8xx\n#define SPRN_SPRG_SCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_SCRATCH1\tSPRN_SPRG1\n#endif\n\n/*\n * An mtfsf instruction with the L bit set. On CPUs that support this a\n * full 64bits of FPSCR is restored and on other CPUs the L bit is ignored.\n *\n * Until binutils gets the new form of mtfsf, hardwire the instruction.\n */\n#ifdef CONFIG_PPC64\n#define MTFSF_L(REG) \\\n\t.long (0xfc00058e | ((0xff) << 17) | ((REG) << 11) | (1 << 25))\n#else\n#define MTFSF_L(REG)\tmtfsf\t0xff, (REG)\n#endif\n\n/* Processor Version Register (PVR) field extraction */\n\n#define PVR_VER(pvr)\t(((pvr) >>  16) & 0xFFFF)\t/* Version field */\n#define PVR_REV(pvr)\t(((pvr) >>   0) & 0xFFFF)\t/* Revison field */\n\n#define __is_processor(pv)\t(PVR_VER(mfspr(SPRN_PVR)) == (pv))\n\n/*\n * IBM has further subdivided the standard PowerPC 16-bit version and\n * revision subfields of the PVR for the PowerPC 403s into the following:\n */\n\n#define PVR_FAM(pvr)\t(((pvr) >> 20) & 0xFFF)\t/* Family field */\n#define PVR_MEM(pvr)\t(((pvr) >> 16) & 0xF)\t/* Member field */\n#define PVR_CORE(pvr)\t(((pvr) >> 12) & 0xF)\t/* Core field */\n#define PVR_CFG(pvr)\t(((pvr) >>  8) & 0xF)\t/* Configuration field */\n#define PVR_MAJ(pvr)\t(((pvr) >>  4) & 0xF)\t/* Major revision field */\n#define PVR_MIN(pvr)\t(((pvr) >>  0) & 0xF)\t/* Minor revision field */\n\n/* Processor Version Numbers */\n\n#define PVR_403GA\t0x00200000\n#define PVR_403GB\t0x00200100\n#define PVR_403GC\t0x00200200\n#define PVR_403GCX\t0x00201400\n#define PVR_405GP\t0x40110000\n#define PVR_476\t\t0x11a52000\n#define PVR_STB03XXX\t0x40310000\n#define PVR_NP405H\t0x41410000\n#define PVR_NP405L\t0x41610000\n#define PVR_601\t\t0x00010000\n#define PVR_602\t\t0x00050000\n#define PVR_603\t\t0x00030000\n#define PVR_603e\t0x00060000\n#define PVR_603ev\t0x00070000\n#define PVR_603r\t0x00071000\n#define PVR_604\t\t0x00040000\n#define PVR_604e\t0x00090000\n#define PVR_604r\t0x000A0000\n#define PVR_620\t\t0x00140000\n#define PVR_740\t\t0x00080000\n#define PVR_750\t\tPVR_740\n#define PVR_740P\t0x10080000\n#define PVR_750P\tPVR_740P\n#define PVR_7400\t0x000C0000\n#define PVR_7410\t0x800C0000\n#define PVR_7450\t0x80000000\n#define PVR_8540\t0x80200000\n#define PVR_8560\t0x80200000\n/*\n * For the 8xx processors, all of them report the same PVR family for\n * the PowerPC core. The various versions of these processors must be\n * differentiated by the version number in the Communication Processor\n * Module (CPM).\n */\n#define PVR_821\t\t0x00500000\n#define PVR_823\t\tPVR_821\n#define PVR_850\t\tPVR_821\n#define PVR_860\t\tPVR_821\n#define PVR_8240\t0x00810100\n#define PVR_8245\t0x80811014\n#define PVR_8260\tPVR_8240\n\n/* 476 Simulator seems to currently have the PVR of the 602... */\n#define PVR_476_ISS\t0x00052000\n\n/* 64-bit processors */\n/* XXX the prefix should be PVR_, we'll do a global sweep to fix it one day */\n#define PV_NORTHSTAR\t0x0033\n#define PV_PULSAR\t0x0034\n#define PV_POWER4\t0x0035\n#define PV_ICESTAR\t0x0036\n#define PV_SSTAR\t0x0037\n#define PV_POWER4p\t0x0038\n#define PV_970\t\t0x0039\n#define PV_POWER5\t0x003A\n#define PV_POWER5p\t0x003B\n#define PV_970FX\t0x003C\n#define PV_630\t\t0x0040\n#define PV_630p\t0x0041\n#define PV_970MP\t0x0044\n#define PV_970GX\t0x0045\n#define PV_BE\t\t0x0070\n#define PV_PA6T\t\t0x0090\n\n/* Macros for setting and retrieving special purpose registers */\n#ifndef __ASSEMBLY__\n#define mfmsr()\t\t({unsigned long rval; \\\n\t\t\tasm volatile(\"mfmsr %0\" : \"=r\" (rval)); rval;})\n#ifdef CONFIG_PPC_BOOK3S_64\n#define __mtmsrd(v, l)\tasm volatile(\"mtmsrd %0,\" __stringify(l) \\\n\t\t\t\t     : : \"r\" (v) : \"memory\")\n#define mtmsrd(v)\t__mtmsrd((v), 0)\n#define mtmsr(v)\tmtmsrd(v)\n#else\n#define mtmsr(v)\tasm volatile(\"mtmsr %0\" : : \"r\" (v) : \"memory\")\n#endif\n\n#define mfspr(rn)\t({unsigned long rval; \\\n\t\t\tasm volatile(\"mfspr %0,\" __stringify(rn) \\\n\t\t\t\t: \"=r\" (rval)); rval;})\n#define mtspr(rn, v)\tasm volatile(\"mtspr \" __stringify(rn) \",%0\" : : \"r\" (v)\\\n\t\t\t\t     : \"memory\")\n\n#ifdef __powerpc64__\n#ifdef CONFIG_PPC_CELL\n#define mftb()\t\t({unsigned long rval;\t\t\t\t\\\n\t\t\tasm volatile(\t\t\t\t\t\\\n\t\t\t\t\"90:\tmftb %0;\\n\"\t\t\t\\\n\t\t\t\t\"97:\tcmpwi %0,0;\\n\"\t\t\t\\\n\t\t\t\t\"\tbeq- 90b;\\n\"\t\t\t\\\n\t\t\t\t\"99:\\n\"\t\t\t\t\t\\\n\t\t\t\t\".section __ftr_fixup,\\\"a\\\"\\n\"\t\t\\\n\t\t\t\t\".align 3\\n\"\t\t\t\t\\\n\t\t\t\t\"98:\\n\"\t\t\t\t\t\\\n\t\t\t\t\"\t.llong %1\\n\"\t\t\t\\\n\t\t\t\t\"\t.llong %1\\n\"\t\t\t\\\n\t\t\t\t\"\t.llong 97b-98b\\n\"\t\t\\\n\t\t\t\t\"\t.llong 99b-98b\\n\"\t\t\\\n\t\t\t\t\"\t.llong 0\\n\"\t\t\t\\\n\t\t\t\t\"\t.llong 0\\n\"\t\t\t\\\n\t\t\t\t\".previous\"\t\t\t\t\\\n\t\t\t: \"=r\" (rval) : \"i\" (CPU_FTR_CELL_TB_BUG)); rval;})\n#else\n#define mftb()\t\t({unsigned long rval;\t\\\n\t\t\tasm volatile(\"mftb %0\" : \"=r\" (rval)); rval;})\n#endif /* !CONFIG_PPC_CELL */\n\n#else /* __powerpc64__ */\n\n#define mftbl()\t\t({unsigned long rval;\t\\\n\t\t\tasm volatile(\"mftbl %0\" : \"=r\" (rval)); rval;})\n#define mftbu()\t\t({unsigned long rval;\t\\\n\t\t\tasm volatile(\"mftbu %0\" : \"=r\" (rval)); rval;})\n#endif /* !__powerpc64__ */\n\n#define mttbl(v)\tasm volatile(\"mttbl %0\":: \"r\"(v))\n#define mttbu(v)\tasm volatile(\"mttbu %0\":: \"r\"(v))\n\n#ifdef CONFIG_PPC32\n#define mfsrin(v)\t({unsigned int rval; \\\n\t\t\tasm volatile(\"mfsrin %0,%1\" : \"=r\" (rval) : \"r\" (v)); \\\n\t\t\t\t\trval;})\n#endif\n\n#define proc_trap()\tasm volatile(\"trap\")\n\n#ifdef CONFIG_PPC64\n\nextern void ppc64_runlatch_on(void);\nextern void __ppc64_runlatch_off(void);\n\n#define ppc64_runlatch_off()\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tif (cpu_has_feature(CPU_FTR_CTRL) &&\t\t\\\n\t\t    test_thread_flag(TIF_RUNLATCH))\t\t\\\n\t\t\t__ppc64_runlatch_off();\t\t\t\\\n\t} while (0)\n\nextern unsigned long scom970_read(unsigned int address);\nextern void scom970_write(unsigned int address, unsigned long value);\n\n#else\n#define ppc64_runlatch_on()\n#define ppc64_runlatch_off()\n\n#endif /* CONFIG_PPC64 */\n\n#define __get_SP()\t({unsigned long sp; \\\n\t\t\tasm volatile(\"mr %0,1\": \"=r\" (sp)); sp;})\n\nstruct pt_regs;\n\nextern void ppc_save_regs(struct pt_regs *regs);\n\n#endif /* __ASSEMBLY__ */\n#endif /* __KERNEL__ */\n#endif /* _ASM_POWERPC_REG_H */\n", "/*\n * Performance event support - powerpc architecture code\n *\n * Copyright 2008-2009 Paul Mackerras, IBM Corporation.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <asm/reg.h>\n#include <asm/pmc.h>\n#include <asm/machdep.h>\n#include <asm/firmware.h>\n#include <asm/ptrace.h>\n\nstruct cpu_hw_events {\n\tint n_events;\n\tint n_percpu;\n\tint disabled;\n\tint n_added;\n\tint n_limited;\n\tu8  pmcs_enabled;\n\tstruct perf_event *event[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int flags[MAX_HWEVENTS];\n\tunsigned long mmcr[3];\n\tstruct perf_event *limited_counter[MAX_LIMITED_HWCOUNTERS];\n\tu8  limited_hwidx[MAX_LIMITED_HWCOUNTERS];\n\tu64 alternatives[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long amasks[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long avalues[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\n\tunsigned int group_flag;\n\tint n_txn_start;\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\nstruct power_pmu *ppmu;\n\n/*\n * Normally, to ignore kernel events we set the FCS (freeze counters\n * in supervisor mode) bit in MMCR0, but if the kernel runs with the\n * hypervisor bit set in the MSR, or if we are running on a processor\n * where the hypervisor bit is forced to 1 (as on Apple G5 processors),\n * then we need to use the FCHV bit to ignore kernel events.\n */\nstatic unsigned int freeze_events_kernel = MMCR0_FCS;\n\n/*\n * 32-bit doesn't have MMCRA but does have an MMCR2,\n * and a few other names are different.\n */\n#ifdef CONFIG_PPC32\n\n#define MMCR0_FCHV\t\t0\n#define MMCR0_PMCjCE\t\tMMCR0_PMCnCE\n\n#define SPRN_MMCRA\t\tSPRN_MMCR2\n#define MMCRA_SAMPLE_ENABLE\t0\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp) { }\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_read_regs(struct pt_regs *regs) { }\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_PPC32 */\n\n/*\n * Things that are specific to 64-bit implementations.\n */\n#ifdef CONFIG_PPC64\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\n\tif ((mmcra & MMCRA_SAMPLE_ENABLE) && !(ppmu->flags & PPMU_ALT_SIPR)) {\n\t\tunsigned long slot = (mmcra & MMCRA_SLOT) >> MMCRA_SLOT_SHIFT;\n\t\tif (slot > 1)\n\t\t\treturn 4 * (slot - 1);\n\t}\n\treturn 0;\n}\n\n/*\n * The user wants a data address recorded.\n * If we're not doing instruction sampling, give them the SDAR\n * (sampled data address).  If we are doing instruction sampling, then\n * only give them the SDAR if it corresponds to the instruction\n * pointed to by SIAR; this is indicated by the [POWER6_]MMCRA_SDSYNC\n * bit in MMCRA.\n */\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tunsigned long sdsync = (ppmu->flags & PPMU_ALT_SIPR) ?\n\t\tPOWER6_MMCRA_SDSYNC : MMCRA_SDSYNC;\n\n\tif (!(mmcra & MMCRA_SAMPLE_ENABLE) || (mmcra & sdsync))\n\t\t*addrp = mfspr(SPRN_SDAR);\n}\n\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tunsigned long sihv = MMCRA_SIHV;\n\tunsigned long sipr = MMCRA_SIPR;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn 0;\t/* not a PMU interrupt */\n\n\tif (ppmu->flags & PPMU_ALT_SIPR) {\n\t\tsihv = POWER6_MMCRA_SIHV;\n\t\tsipr = POWER6_MMCRA_SIPR;\n\t}\n\n\t/* PR has priority over HV, so order below is important */\n\tif (mmcra & sipr)\n\t\treturn PERF_RECORD_MISC_USER;\n\tif ((mmcra & sihv) && (freeze_events_kernel != MMCR0_FCHV))\n\t\treturn PERF_RECORD_MISC_HYPERVISOR;\n\treturn PERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Overload regs->dsisr to store MMCRA so we only need to read it once\n * on each interrupt.\n */\nstatic inline void perf_read_regs(struct pt_regs *regs)\n{\n\tregs->dsisr = mfspr(SPRN_MMCRA);\n}\n\n/*\n * If interrupts were soft-disabled when a PMU interrupt occurs, treat\n * it as an NMI.\n */\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n\treturn !regs->softe;\n}\n\n#endif /* CONFIG_PPC64 */\n\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\nvoid perf_event_print_debug(void)\n{\n}\n\n/*\n * Read one performance monitor counter (PMC).\n */\nstatic unsigned long read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tswitch (idx) {\n\tcase 1:\n\t\tval = mfspr(SPRN_PMC1);\n\t\tbreak;\n\tcase 2:\n\t\tval = mfspr(SPRN_PMC2);\n\t\tbreak;\n\tcase 3:\n\t\tval = mfspr(SPRN_PMC3);\n\t\tbreak;\n\tcase 4:\n\t\tval = mfspr(SPRN_PMC4);\n\t\tbreak;\n\tcase 5:\n\t\tval = mfspr(SPRN_PMC5);\n\t\tbreak;\n\tcase 6:\n\t\tval = mfspr(SPRN_PMC6);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tval = mfspr(SPRN_PMC7);\n\t\tbreak;\n\tcase 8:\n\t\tval = mfspr(SPRN_PMC8);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to read PMC%d\\n\", idx);\n\t\tval = 0;\n\t}\n\treturn val;\n}\n\n/*\n * Write one PMC.\n */\nstatic void write_pmc(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 1:\n\t\tmtspr(SPRN_PMC1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtspr(SPRN_PMC2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtspr(SPRN_PMC3, val);\n\t\tbreak;\n\tcase 4:\n\t\tmtspr(SPRN_PMC4, val);\n\t\tbreak;\n\tcase 5:\n\t\tmtspr(SPRN_PMC5, val);\n\t\tbreak;\n\tcase 6:\n\t\tmtspr(SPRN_PMC6, val);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tmtspr(SPRN_PMC7, val);\n\t\tbreak;\n\tcase 8:\n\t\tmtspr(SPRN_PMC8, val);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMC%d\\n\", idx);\n\t}\n}\n\n/*\n * Check if a set of events can all go on the PMU at once.\n * If they can't, this will look at alternative codes for the events\n * and see if any combination of alternative codes is feasible.\n * The feasible set is returned in event_id[].\n */\nstatic int power_check_constraints(struct cpu_hw_events *cpuhw,\n\t\t\t\t   u64 event_id[], unsigned int cflags[],\n\t\t\t\t   int n_ev)\n{\n\tunsigned long mask, value, nv;\n\tunsigned long smasks[MAX_HWEVENTS], svalues[MAX_HWEVENTS];\n\tint n_alt[MAX_HWEVENTS], choice[MAX_HWEVENTS];\n\tint i, j;\n\tunsigned long addf = ppmu->add_fields;\n\tunsigned long tadd = ppmu->test_adder;\n\n\tif (n_ev > ppmu->n_counter)\n\t\treturn -1;\n\n\t/* First see if the events will go on as-is */\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tif ((cflags[i] & PPMU_LIMITED_PMC_REQD)\n\t\t    && !ppmu->limited_pmc_event(event_id[i])) {\n\t\t\tppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t       cpuhw->alternatives[i]);\n\t\t\tevent_id[i] = cpuhw->alternatives[i][0];\n\t\t}\n\t\tif (ppmu->get_constraint(event_id[i], &cpuhw->amasks[i][0],\n\t\t\t\t\t &cpuhw->avalues[i][0]))\n\t\t\treturn -1;\n\t}\n\tvalue = mask = 0;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tnv = (value | cpuhw->avalues[i][0]) +\n\t\t\t(value & cpuhw->avalues[i][0] & addf);\n\t\tif ((((nv + tadd) ^ value) & mask) != 0 ||\n\t\t    (((nv + tadd) ^ cpuhw->avalues[i][0]) &\n\t\t     cpuhw->amasks[i][0]) != 0)\n\t\t\tbreak;\n\t\tvalue = nv;\n\t\tmask |= cpuhw->amasks[i][0];\n\t}\n\tif (i == n_ev)\n\t\treturn 0;\t/* all OK */\n\n\t/* doesn't work, gather alternatives... */\n\tif (!ppmu->get_alternatives)\n\t\treturn -1;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tchoice[i] = 0;\n\t\tn_alt[i] = ppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t\t  cpuhw->alternatives[i]);\n\t\tfor (j = 1; j < n_alt[i]; ++j)\n\t\t\tppmu->get_constraint(cpuhw->alternatives[i][j],\n\t\t\t\t\t     &cpuhw->amasks[i][j],\n\t\t\t\t\t     &cpuhw->avalues[i][j]);\n\t}\n\n\t/* enumerate all possibilities and see if any will work */\n\ti = 0;\n\tj = -1;\n\tvalue = mask = nv = 0;\n\twhile (i < n_ev) {\n\t\tif (j >= 0) {\n\t\t\t/* we're backtracking, restore context */\n\t\t\tvalue = svalues[i];\n\t\t\tmask = smasks[i];\n\t\t\tj = choice[i];\n\t\t}\n\t\t/*\n\t\t * See if any alternative k for event_id i,\n\t\t * where k > j, will satisfy the constraints.\n\t\t */\n\t\twhile (++j < n_alt[i]) {\n\t\t\tnv = (value | cpuhw->avalues[i][j]) +\n\t\t\t\t(value & cpuhw->avalues[i][j] & addf);\n\t\t\tif ((((nv + tadd) ^ value) & mask) == 0 &&\n\t\t\t    (((nv + tadd) ^ cpuhw->avalues[i][j])\n\t\t\t     & cpuhw->amasks[i][j]) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j >= n_alt[i]) {\n\t\t\t/*\n\t\t\t * No feasible alternative, backtrack\n\t\t\t * to event_id i-1 and continue enumerating its\n\t\t\t * alternatives from where we got up to.\n\t\t\t */\n\t\t\tif (--i < 0)\n\t\t\t\treturn -1;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Found a feasible alternative for event_id i,\n\t\t\t * remember where we got up to with this event_id,\n\t\t\t * go on to the next event_id, and start with\n\t\t\t * the first alternative for it.\n\t\t\t */\n\t\t\tchoice[i] = j;\n\t\t\tsvalues[i] = value;\n\t\t\tsmasks[i] = mask;\n\t\t\tvalue = nv;\n\t\t\tmask |= cpuhw->amasks[i][j];\n\t\t\t++i;\n\t\t\tj = -1;\n\t\t}\n\t}\n\n\t/* OK, we have a feasible combination, tell the caller the solution */\n\tfor (i = 0; i < n_ev; ++i)\n\t\tevent_id[i] = cpuhw->alternatives[i][choice[i]];\n\treturn 0;\n}\n\n/*\n * Check if newly-added events have consistent settings for\n * exclude_{user,kernel,hv} with each other and any previously\n * added events.\n */\nstatic int check_excludes(struct perf_event **ctrs, unsigned int cflags[],\n\t\t\t  int n_prev, int n_new)\n{\n\tint eu = 0, ek = 0, eh = 0;\n\tint i, n, first;\n\tstruct perf_event *event;\n\n\tn = n_prev + n_new;\n\tif (n <= 1)\n\t\treturn 0;\n\n\tfirst = 1;\n\tfor (i = 0; i < n; ++i) {\n\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK) {\n\t\t\tcflags[i] &= ~PPMU_LIMITED_PMC_REQD;\n\t\t\tcontinue;\n\t\t}\n\t\tevent = ctrs[i];\n\t\tif (first) {\n\t\t\teu = event->attr.exclude_user;\n\t\t\tek = event->attr.exclude_kernel;\n\t\t\teh = event->attr.exclude_hv;\n\t\t\tfirst = 0;\n\t\t} else if (event->attr.exclude_user != eu ||\n\t\t\t   event->attr.exclude_kernel != ek ||\n\t\t\t   event->attr.exclude_hv != eh) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (eu || ek || eh)\n\t\tfor (i = 0; i < n; ++i)\n\t\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK)\n\t\t\t\tcflags[i] |= PPMU_LIMITED_PMC_REQD;\n\n\treturn 0;\n}\n\nstatic void power_pmu_read(struct perf_event *event)\n{\n\ts64 val, delta, prev;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tif (!event->hw.idx)\n\t\treturn;\n\t/*\n\t * Performance monitor interrupts come even when interrupts\n\t * are soft-disabled, as long as interrupts are hard-enabled.\n\t * Therefore we treat them like NMIs.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tbarrier();\n\t\tval = read_pmc(event->hw.idx);\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\n\n\t/* The counters are only 32 bits wide */\n\tdelta = (val - prev) & 0xfffffffful;\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &event->hw.period_left);\n}\n\n/*\n * On some machines, PMC5 and PMC6 can't be written, don't respect\n * the freeze conditions, and don't generate interrupts.  This tells\n * us if `event' is using such a PMC.\n */\nstatic int is_limited_pmc(int pmcnum)\n{\n\treturn (ppmu->flags & PPMU_LIMITED_PMC5_6)\n\t\t&& (pmcnum == 5 || pmcnum == 6);\n}\n\nstatic void freeze_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t    unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev, delta;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tif (!event->hw.idx)\n\t\t\tcontinue;\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tevent->hw.idx = 0;\n\t\tdelta = (val - prev) & 0xfffffffful;\n\t\tlocal64_add(delta, &event->count);\n\t}\n}\n\nstatic void thaw_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t  unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tevent->hw.idx = cpuhw->limited_hwidx[i];\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tperf_event_update_userpage(event);\n\t}\n}\n\n/*\n * Since limited events don't respect the freeze conditions, we\n * have to read them immediately after freezing or unfreezing the\n * other events.  We try to keep the values from the limited\n * events as consistent as possible by keeping the delay (in\n * cycles and instructions) between freezing/unfreezing and reading\n * the limited events as small and consistent as possible.\n * Therefore, if any limited events are in use, we read them\n * both, and always in the same order, to minimize variability,\n * and do it inside the same asm that writes MMCR0.\n */\nstatic void write_mmcr0(struct cpu_hw_events *cpuhw, unsigned long mmcr0)\n{\n\tunsigned long pmc5, pmc6;\n\n\tif (!cpuhw->n_limited) {\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n\t\treturn;\n\t}\n\n\t/*\n\t * Write MMCR0, then read PMC5 and PMC6 immediately.\n\t * To ensure we don't get a performance monitor interrupt\n\t * between writing MMCR0 and freezing/thawing the limited\n\t * events, we first write MMCR0 with the event overflow\n\t * interrupt enable bits turned off.\n\t */\n\tasm volatile(\"mtspr %3,%2; mfspr %0,%4; mfspr %1,%5\"\n\t\t     : \"=&r\" (pmc5), \"=&r\" (pmc6)\n\t\t     : \"r\" (mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE)),\n\t\t       \"i\" (SPRN_MMCR0),\n\t\t       \"i\" (SPRN_PMC5), \"i\" (SPRN_PMC6));\n\n\tif (mmcr0 & MMCR0_FC)\n\t\tfreeze_limited_counters(cpuhw, pmc5, pmc6);\n\telse\n\t\tthaw_limited_counters(cpuhw, pmc5, pmc6);\n\n\t/*\n\t * Write the full MMCR0 including the event overflow interrupt\n\t * enable bits, if necessary.\n\t */\n\tif (mmcr0 & (MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n}\n\n/*\n * Disable all events to prevent PMU interrupts and to allow\n * events to be added or removed.\n */\nstatic void power_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tif (!cpuhw->disabled) {\n\t\tcpuhw->disabled = 1;\n\t\tcpuhw->n_added = 0;\n\n\t\t/*\n\t\t * Check if we ever enabled the PMU on this cpu.\n\t\t */\n\t\tif (!cpuhw->pmcs_enabled) {\n\t\t\tppc_enable_pmcs();\n\t\t\tcpuhw->pmcs_enabled = 1;\n\t\t}\n\n\t\t/*\n\t\t * Disable instruction sampling if it was enabled\n\t\t */\n\t\tif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\n\t\t\tmtspr(SPRN_MMCRA,\n\t\t\t      cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\t\t\tmb();\n\t\t}\n\n\t\t/*\n\t\t * Set the 'freeze counters' bit.\n\t\t * The barrier is to make sure the mtspr has been\n\t\t * executed and the PMU has frozen the events\n\t\t * before we return.\n\t\t */\n\t\twrite_mmcr0(cpuhw, mfspr(SPRN_MMCR0) | MMCR0_FC);\n\t\tmb();\n\t}\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Re-enable all events if disable == 0.\n * If we were previously disabled and events were added, then\n * put the new config on the PMU.\n */\nstatic void power_pmu_enable(struct pmu *pmu)\n{\n\tstruct perf_event *event;\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tlong i;\n\tunsigned long val;\n\ts64 left;\n\tunsigned int hwc_index[MAX_HWEVENTS];\n\tint n_lim;\n\tint idx;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tif (!cpuhw->disabled) {\n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\n\tcpuhw->disabled = 0;\n\n\t/*\n\t * If we didn't change anything, or only removed events,\n\t * no need to recalculate MMCR* settings and reset the PMCs.\n\t * Just reenable the PMU with the current MMCR* settings\n\t * (possibly updated for removal of events).\n\t */\n\tif (!cpuhw->n_added) {\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\t\tmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\n\t\tif (cpuhw->n_events == 0)\n\t\t\tppc_set_pmu_inuse(0);\n\t\tgoto out_enable;\n\t}\n\n\t/*\n\t * Compute MMCR* values for the new set of events\n\t */\n\tif (ppmu->compute_mmcr(cpuhw->events, cpuhw->n_events, hwc_index,\n\t\t\t       cpuhw->mmcr)) {\n\t\t/* shouldn't ever get here */\n\t\tprintk(KERN_ERR \"oops compute_mmcr failed\\n\");\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Add in MMCR0 freeze bits corresponding to the\n\t * attr.exclude_* bits for the first event.\n\t * We have already checked that all events have the\n\t * same values for these bits as the first event.\n\t */\n\tevent = cpuhw->event[0];\n\tif (event->attr.exclude_user)\n\t\tcpuhw->mmcr[0] |= MMCR0_FCP;\n\tif (event->attr.exclude_kernel)\n\t\tcpuhw->mmcr[0] |= freeze_events_kernel;\n\tif (event->attr.exclude_hv)\n\t\tcpuhw->mmcr[0] |= MMCR0_FCHV;\n\n\t/*\n\t * Write the new configuration to MMCR* with the freeze\n\t * bit set and set the hardware events to their initial values.\n\t * Then unfreeze the events.\n\t */\n\tppc_set_pmu_inuse(1);\n\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\tmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\n\tmtspr(SPRN_MMCR0, (cpuhw->mmcr[0] & ~(MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\t\t\t| MMCR0_FC);\n\n\t/*\n\t * Read off any pre-existing events that need to move\n\t * to another PMC.\n\t */\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx && event->hw.idx != hwc_index[i] + 1) {\n\t\t\tpower_pmu_read(event);\n\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\tevent->hw.idx = 0;\n\t\t}\n\t}\n\n\t/*\n\t * Initialize the PMCs for all the new and moved events.\n\t */\n\tcpuhw->n_limited = n_lim = 0;\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx)\n\t\t\tcontinue;\n\t\tidx = hwc_index[i] + 1;\n\t\tif (is_limited_pmc(idx)) {\n\t\t\tcpuhw->limited_counter[n_lim] = event;\n\t\t\tcpuhw->limited_hwidx[n_lim] = idx;\n\t\t\t++n_lim;\n\t\t\tcontinue;\n\t\t}\n\t\tval = 0;\n\t\tif (event->hw.sample_period) {\n\t\t\tleft = local64_read(&event->hw.period_left);\n\t\t\tif (left < 0x80000000L)\n\t\t\t\tval = 0x80000000L - left;\n\t\t}\n\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tevent->hw.idx = idx;\n\t\tif (event->hw.state & PERF_HES_STOPPED)\n\t\t\tval = 0;\n\t\twrite_pmc(idx, val);\n\t\tperf_event_update_userpage(event);\n\t}\n\tcpuhw->n_limited = n_lim;\n\tcpuhw->mmcr[0] |= MMCR0_PMXE | MMCR0_FCECE;\n\n out_enable:\n\tmb();\n\twrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\n\n\t/*\n\t * Enable instruction sampling if necessary\n\t */\n\tif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\n\t\tmb();\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2]);\n\t}\n\n out:\n\tlocal_irq_restore(flags);\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *ctrs[], u64 *events,\n\t\t\t  unsigned int *flags)\n{\n\tint n = 0;\n\tstruct perf_event *event;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tctrs[n] = group;\n\t\tflags[n] = group->hw.event_base;\n\t\tevents[n++] = group->hw.config;\n\t}\n\tlist_for_each_entry(event, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(event) &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tctrs[n] = event;\n\t\t\tflags[n] = event->hw.event_base;\n\t\t\tevents[n++] = event->hw.config;\n\t\t}\n\t}\n\treturn n;\n}\n\n/*\n * Add a event to the PMU.\n * If all events are not already frozen, then we disable and\n * re-enable the PMU in order to get hw_perf_enable to do the\n * actual work of reconfiguring the PMU.\n */\nstatic int power_pmu_add(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tint n0;\n\tint ret = -EAGAIN;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\t/*\n\t * Add the event to the list (if there is room)\n\t * and check whether the total set is still feasible.\n\t */\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tn0 = cpuhw->n_events;\n\tif (n0 >= ppmu->n_counter)\n\t\tgoto out;\n\tcpuhw->event[n0] = event;\n\tcpuhw->events[n0] = event->hw.config;\n\tcpuhw->flags[n0] = event->hw.event_base;\n\n\tif (!(ef_flags & PERF_EF_START))\n\t\tevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be peformed\n\t * at commit time(->commit_txn) as a whole\n\t */\n\tif (cpuhw->group_flag & PERF_EVENT_TXN)\n\t\tgoto nocheck;\n\n\tif (check_excludes(cpuhw->event, cpuhw->flags, n0, 1))\n\t\tgoto out;\n\tif (power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n0 + 1))\n\t\tgoto out;\n\tevent->hw.config = cpuhw->events[n0];\n\nnocheck:\n\t++cpuhw->n_events;\n\t++cpuhw->n_added;\n\n\tret = 0;\n out:\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/*\n * Remove a event from the PMU.\n */\nstatic void power_pmu_del(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tif (event == cpuhw->event[i]) {\n\t\t\twhile (++i < cpuhw->n_events) {\n\t\t\t\tcpuhw->event[i-1] = cpuhw->event[i];\n\t\t\t\tcpuhw->events[i-1] = cpuhw->events[i];\n\t\t\t\tcpuhw->flags[i-1] = cpuhw->flags[i];\n\t\t\t}\n\t\t\t--cpuhw->n_events;\n\t\t\tppmu->disable_pmc(event->hw.idx - 1, cpuhw->mmcr);\n\t\t\tif (event->hw.idx) {\n\t\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\t\tevent->hw.idx = 0;\n\t\t\t}\n\t\t\tperf_event_update_userpage(event);\n\t\t\tbreak;\n\t\t}\n\t}\n\tfor (i = 0; i < cpuhw->n_limited; ++i)\n\t\tif (event == cpuhw->limited_counter[i])\n\t\t\tbreak;\n\tif (i < cpuhw->n_limited) {\n\t\twhile (++i < cpuhw->n_limited) {\n\t\t\tcpuhw->limited_counter[i-1] = cpuhw->limited_counter[i];\n\t\t\tcpuhw->limited_hwidx[i-1] = cpuhw->limited_hwidx[i];\n\t\t}\n\t\t--cpuhw->n_limited;\n\t}\n\tif (cpuhw->n_events == 0) {\n\t\t/* disable exceptions if no events are running */\n\t\tcpuhw->mmcr[0] &= ~(MMCR0_PMXE | MMCR0_FCECE);\n\t}\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * POWER-PMU does not support disabling individual counters, hence\n * program their cycle counter to their max value and ignore the interrupts.\n */\n\nstatic void power_pmu_start(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\ts64 left;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (!(event->hw.state & PERF_HES_STOPPED))\n\t\treturn;\n\n\tif (ef_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tevent->hw.state = 0;\n\tleft = local64_read(&event->hw.period_left);\n\twrite_pmc(event->hw.idx, left);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void power_pmu_stop(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\twrite_pmc(event->hw.idx, 0);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n */\nvoid power_pmu_start_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tperf_pmu_disable(pmu);\n\tcpuhw->group_flag |= PERF_EVENT_TXN;\n\tcpuhw->n_txn_start = cpuhw->n_events;\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nvoid power_pmu_cancel_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nint power_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i, n;\n\n\tif (!ppmu)\n\t\treturn -EAGAIN;\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tn = cpuhw->n_events;\n\tif (check_excludes(cpuhw->event, cpuhw->flags, 0, n))\n\t\treturn -EAGAIN;\n\ti = power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n);\n\tif (i < 0)\n\t\treturn -EAGAIN;\n\n\tfor (i = cpuhw->n_txn_start; i < n; ++i)\n\t\tcpuhw->event[i]->hw.config = cpuhw->events[i];\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\n/*\n * Return 1 if we might be able to put event on a limited PMC,\n * or 0 if not.\n * A event can only go on a limited PMC if it counts something\n * that a limited PMC can count, doesn't require interrupts, and\n * doesn't exclude any processor mode.\n */\nstatic int can_go_on_limited_pmc(struct perf_event *event, u64 ev,\n\t\t\t\t unsigned int flags)\n{\n\tint n;\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\n\tif (event->attr.exclude_user\n\t    || event->attr.exclude_kernel\n\t    || event->attr.exclude_hv\n\t    || event->attr.sample_period)\n\t\treturn 0;\n\n\tif (ppmu->limited_pmc_event(ev))\n\t\treturn 1;\n\n\t/*\n\t * The requested event_id isn't on a limited PMC already;\n\t * see if any alternative code goes on a limited PMC.\n\t */\n\tif (!ppmu->get_alternatives)\n\t\treturn 0;\n\n\tflags |= PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD;\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\n\treturn n > 0;\n}\n\n/*\n * Find an alternative event_id that goes on a normal PMC, if possible,\n * and return the event_id code, or 0 if there is no such alternative.\n * (Note: event_id code 0 is \"don't count\" on all machines.)\n */\nstatic u64 normal_pmc_alternative(u64 ev, unsigned long flags)\n{\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\tint n;\n\n\tflags &= ~(PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD);\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\tif (!n)\n\t\treturn 0;\n\treturn alt[0];\n}\n\n/* Number of perf_events counting hardware events */\nstatic atomic_t num_events;\n/* Used to avoid races in calling reserve/release_pmc_hardware */\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n/*\n * Release the PMU if this is the last perf_event.\n */\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (!atomic_add_unless(&num_events, -1, 1)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_dec_return(&num_events) == 0)\n\t\t\trelease_pmc_hardware();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n/*\n * Translate a generic cache event_id config to a raw event_id code.\n */\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\n{\n\tunsigned long type, op, result;\n\tint ev;\n\n\tif (!ppmu->cache_events)\n\t\treturn -EINVAL;\n\n\t/* unpack config */\n\ttype = config & 0xff;\n\top = (config >> 8) & 0xff;\n\tresult = (config >> 16) & 0xff;\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX ||\n\t    op >= PERF_COUNT_HW_CACHE_OP_MAX ||\n\t    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tev = (*ppmu->cache_events)[type][op][result];\n\tif (ev == 0)\n\t\treturn -EOPNOTSUPP;\n\tif (ev == -1)\n\t\treturn -EINVAL;\n\t*eventp = ev;\n\treturn 0;\n}\n\nstatic int power_pmu_event_init(struct perf_event *event)\n{\n\tu64 ev;\n\tunsigned long flags;\n\tstruct perf_event *ctrs[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int cflags[MAX_HWEVENTS];\n\tint n;\n\tint err;\n\tstruct cpu_hw_events *cpuhw;\n\n\tif (!ppmu)\n\t\treturn -ENOENT;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tev = event->attr.config;\n\t\tif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\n\t\t\treturn -EOPNOTSUPP;\n\t\tev = ppmu->generic_events[ev];\n\t\tbreak;\n\tcase PERF_TYPE_HW_CACHE:\n\t\terr = hw_perf_cache_event(event->attr.config, &ev);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tcase PERF_TYPE_RAW:\n\t\tev = event->attr.config;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tevent->hw.config_base = ev;\n\tevent->hw.idx = 0;\n\n\t/*\n\t * If we are not running on a hypervisor, force the\n\t * exclude_hv bit to 0 so that we don't care what\n\t * the user set it to.\n\t */\n\tif (!firmware_has_feature(FW_FEATURE_LPAR))\n\t\tevent->attr.exclude_hv = 0;\n\n\t/*\n\t * If this is a per-task event, then we can use\n\t * PM_RUN_* events interchangeably with their non RUN_*\n\t * equivalents, e.g. PM_RUN_CYC instead of PM_CYC.\n\t * XXX we should check if the task is an idle task.\n\t */\n\tflags = 0;\n\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\tflags |= PPMU_ONLY_COUNT_RUN;\n\n\t/*\n\t * If this machine has limited events, check whether this\n\t * event_id could go on a limited event.\n\t */\n\tif (ppmu->flags & PPMU_LIMITED_PMC5_6) {\n\t\tif (can_go_on_limited_pmc(event, ev, flags)) {\n\t\t\tflags |= PPMU_LIMITED_PMC_OK;\n\t\t} else if (ppmu->limited_pmc_event(ev)) {\n\t\t\t/*\n\t\t\t * The requested event_id is on a limited PMC,\n\t\t\t * but we can't use a limited PMC; see if any\n\t\t\t * alternative goes on a normal PMC.\n\t\t\t */\n\t\t\tev = normal_pmc_alternative(ev, flags);\n\t\t\tif (!ev)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/*\n\t * If this is in a group, check if it can go on with all the\n\t * other hardware events in the group.  We assume the event\n\t * hasn't been linked into its leader's sibling list at this point.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader, ppmu->n_counter - 1,\n\t\t\t\t   ctrs, events, cflags);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevents[n] = ev;\n\tctrs[n] = event;\n\tcflags[n] = flags;\n\tif (check_excludes(ctrs, cflags, n, 1))\n\t\treturn -EINVAL;\n\n\tcpuhw = &get_cpu_var(cpu_hw_events);\n\terr = power_check_constraints(cpuhw, events, cflags, n + 1);\n\tput_cpu_var(cpu_hw_events);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tevent->hw.config = events[n];\n\tevent->hw.event_base = cflags[n];\n\tevent->hw.last_period = event->hw.sample_period;\n\tlocal64_set(&event->hw.period_left, event->hw.last_period);\n\n\t/*\n\t * See if we need to reserve the PMU.\n\t * If no events are currently in use, then we have to take a\n\t * mutex to ensure that we don't race with another task doing\n\t * reserve_pmc_hardware or release_pmc_hardware.\n\t */\n\terr = 0;\n\tif (!atomic_inc_not_zero(&num_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&num_events) == 0 &&\n\t\t    reserve_pmc_hardware(perf_event_interrupt))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tatomic_inc(&num_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\tevent->destroy = hw_perf_event_destroy;\n\n\treturn err;\n}\n\nstruct pmu power_pmu = {\n\t.pmu_enable\t= power_pmu_enable,\n\t.pmu_disable\t= power_pmu_disable,\n\t.event_init\t= power_pmu_event_init,\n\t.add\t\t= power_pmu_add,\n\t.del\t\t= power_pmu_del,\n\t.start\t\t= power_pmu_start,\n\t.stop\t\t= power_pmu_stop,\n\t.read\t\t= power_pmu_read,\n\t.start_txn\t= power_pmu_start_txn,\n\t.cancel_txn\t= power_pmu_cancel_txn,\n\t.commit_txn\t= power_pmu_commit_txn,\n};\n\n/*\n * A counter has overflowed; update its count and record\n * things if requested.  Note that interrupts are hard-disabled\n * here so there is no possibility of being interrupted.\n */\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\n\t\t\t       struct pt_regs *regs, int nmi)\n{\n\tu64 period = event->hw.sample_period;\n\ts64 prev, delta, left;\n\tint record = 0;\n\n\tif (event->hw.state & PERF_HES_STOPPED) {\n\t\twrite_pmc(event->hw.idx, 0);\n\t\treturn;\n\t}\n\n\t/* we don't have to worry about interrupts here */\n\tprev = local64_read(&event->hw.prev_count);\n\tdelta = (val - prev) & 0xfffffffful;\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * See if the total period for this event has expired,\n\t * and update for the next period.\n\t */\n\tval = 0;\n\tleft = local64_read(&event->hw.period_left) - delta;\n\tif (period) {\n\t\tif (left <= 0) {\n\t\t\tleft += period;\n\t\t\tif (left <= 0)\n\t\t\t\tleft = period;\n\t\t\trecord = 1;\n\t\t\tevent->hw.last_period = event->hw.sample_period;\n\t\t}\n\t\tif (left < 0x80000000LL)\n\t\t\tval = 0x80000000LL - left;\n\t}\n\n\twrite_pmc(event->hw.idx, val);\n\tlocal64_set(&event->hw.prev_count, val);\n\tlocal64_set(&event->hw.period_left, left);\n\tperf_event_update_userpage(event);\n\n\t/*\n\t * Finally record data if requested.\n\t */\n\tif (record) {\n\t\tstruct perf_sample_data data;\n\n\t\tperf_sample_data_init(&data, ~0ULL);\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_ADDR)\n\t\t\tperf_get_data_addr(regs, &data.addr);\n\n\t\tif (perf_event_overflow(event, nmi, &data, regs))\n\t\t\tpower_pmu_stop(event, 0);\n\t}\n}\n\n/*\n * Called from generic code to get the misc flags (i.e. processor mode)\n * for an event_id.\n */\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tu32 flags = perf_get_misc_flags(regs);\n\n\tif (flags)\n\t\treturn flags;\n\treturn user_mode(regs) ? PERF_RECORD_MISC_USER :\n\t\tPERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Called from generic code to get the instruction pointer\n * for an event_id.\n */\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tunsigned long ip;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn regs->nip;\t/* not a PMU interrupt */\n\n\tip = mfspr(SPRN_SIAR) + perf_ip_adjust(regs);\n\treturn ip;\n}\n\n/*\n * Performance monitor interrupt stuff\n */\nstatic void perf_event_interrupt(struct pt_regs *regs)\n{\n\tint i;\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\tstruct perf_event *event;\n\tunsigned long val;\n\tint found = 0;\n\tint nmi;\n\n\tif (cpuhw->n_limited)\n\t\tfreeze_limited_counters(cpuhw, mfspr(SPRN_PMC5),\n\t\t\t\t\tmfspr(SPRN_PMC6));\n\n\tperf_read_regs(regs);\n\n\tnmi = perf_intr_is_nmi(regs);\n\tif (nmi)\n\t\tnmi_enter();\n\telse\n\t\tirq_enter();\n\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (!event->hw.idx || is_limited_pmc(event->hw.idx))\n\t\t\tcontinue;\n\t\tval = read_pmc(event->hw.idx);\n\t\tif ((int)val < 0) {\n\t\t\t/* event has overflowed */\n\t\t\tfound = 1;\n\t\t\trecord_and_restart(event, val, regs, nmi);\n\t\t}\n\t}\n\n\t/*\n\t * In case we didn't find and reset the event that caused\n\t * the interrupt, scan all events and reset any that are\n\t * negative, to avoid getting continual interrupts.\n\t * Any that we processed in the previous loop will not be negative.\n\t */\n\tif (!found) {\n\t\tfor (i = 0; i < ppmu->n_counter; ++i) {\n\t\t\tif (is_limited_pmc(i + 1))\n\t\t\t\tcontinue;\n\t\t\tval = read_pmc(i + 1);\n\t\t\tif ((int)val < 0)\n\t\t\t\twrite_pmc(i + 1, 0);\n\t\t}\n\t}\n\n\t/*\n\t * Reset MMCR0 to its normal value.  This will set PMXE and\n\t * clear FC (freeze counters) and PMAO (perf mon alert occurred)\n\t * and thus allow interrupts to occur again.\n\t * XXX might want to use MSR.PM to keep the events frozen until\n\t * we get back out of this interrupt.\n\t */\n\twrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\n\n\tif (nmi)\n\t\tnmi_exit();\n\telse\n\t\tirq_exit();\n}\n\nstatic void power_pmu_setup(int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tif (!ppmu)\n\t\treturn;\n\tmemset(cpuhw, 0, sizeof(*cpuhw));\n\tcpuhw->mmcr[0] = MMCR0_FC;\n}\n\nstatic int __cpuinit\npower_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)\n{\n\tunsigned int cpu = (long)hcpu;\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_UP_PREPARE:\n\t\tpower_pmu_setup(cpu);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nint register_power_pmu(struct power_pmu *pmu)\n{\n\tif (ppmu)\n\t\treturn -EBUSY;\t\t/* something's already registered */\n\n\tppmu = pmu;\n\tpr_info(\"%s performance monitor hardware support registered\\n\",\n\t\tpmu->name);\n\n#ifdef MSR_HV\n\t/*\n\t * Use FCHV to ignore kernel events if MSR.HV is set.\n\t */\n\tif (mfmsr() & MSR_HV)\n\t\tfreeze_events_kernel = MMCR0_FCHV;\n#endif /* CONFIG_PPC64 */\n\n\tperf_pmu_register(&power_pmu, \"cpu\", PERF_TYPE_RAW);\n\tperf_cpu_notifier(power_pmu_notifier);\n\n\treturn 0;\n}\n"], "fixing_code": ["/*\n * Contains the definition of registers common to all PowerPC variants.\n * If a register definition has been changed in a different PowerPC\n * variant, we will case it in #ifndef XXX ... #endif, and have the\n * number used in the Programming Environments Manual For 32-Bit\n * Implementations of the PowerPC Architecture (a.k.a. Green Book) here.\n */\n\n#ifndef _ASM_POWERPC_REG_H\n#define _ASM_POWERPC_REG_H\n#ifdef __KERNEL__\n\n#include <linux/stringify.h>\n#include <asm/cputable.h>\n\n/* Pickup Book E specific registers. */\n#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)\n#include <asm/reg_booke.h>\n#endif /* CONFIG_BOOKE || CONFIG_40x */\n\n#ifdef CONFIG_FSL_EMB_PERFMON\n#include <asm/reg_fsl_emb.h>\n#endif\n\n#ifdef CONFIG_8xx\n#include <asm/reg_8xx.h>\n#endif /* CONFIG_8xx */\n\n#define MSR_SF_LG\t63              /* Enable 64 bit mode */\n#define MSR_ISF_LG\t61              /* Interrupt 64b mode valid on 630 */\n#define MSR_HV_LG \t60              /* Hypervisor state */\n#define MSR_VEC_LG\t25\t        /* Enable AltiVec */\n#define MSR_VSX_LG\t23\t\t/* Enable VSX */\n#define MSR_POW_LG\t18\t\t/* Enable Power Management */\n#define MSR_WE_LG\t18\t\t/* Wait State Enable */\n#define MSR_TGPR_LG\t17\t\t/* TLB Update registers in use */\n#define MSR_CE_LG\t17\t\t/* Critical Interrupt Enable */\n#define MSR_ILE_LG\t16\t\t/* Interrupt Little Endian */\n#define MSR_EE_LG\t15\t\t/* External Interrupt Enable */\n#define MSR_PR_LG\t14\t\t/* Problem State / Privilege Level */\n#define MSR_FP_LG\t13\t\t/* Floating Point enable */\n#define MSR_ME_LG\t12\t\t/* Machine Check Enable */\n#define MSR_FE0_LG\t11\t\t/* Floating Exception mode 0 */\n#define MSR_SE_LG\t10\t\t/* Single Step */\n#define MSR_BE_LG\t9\t\t/* Branch Trace */\n#define MSR_DE_LG\t9 \t\t/* Debug Exception Enable */\n#define MSR_FE1_LG\t8\t\t/* Floating Exception mode 1 */\n#define MSR_IP_LG\t6\t\t/* Exception prefix 0x000/0xFFF */\n#define MSR_IR_LG\t5 \t\t/* Instruction Relocate */\n#define MSR_DR_LG\t4 \t\t/* Data Relocate */\n#define MSR_PE_LG\t3\t\t/* Protection Enable */\n#define MSR_PX_LG\t2\t\t/* Protection Exclusive Mode */\n#define MSR_PMM_LG\t2\t\t/* Performance monitor */\n#define MSR_RI_LG\t1\t\t/* Recoverable Exception */\n#define MSR_LE_LG\t0 \t\t/* Little Endian */\n\n#ifdef __ASSEMBLY__\n#define __MASK(X)\t(1<<(X))\n#else\n#define __MASK(X)\t(1UL<<(X))\n#endif\n\n#ifdef CONFIG_PPC64\n#define MSR_SF\t\t__MASK(MSR_SF_LG)\t/* Enable 64 bit mode */\n#define MSR_ISF\t\t__MASK(MSR_ISF_LG)\t/* Interrupt 64b mode valid on 630 */\n#define MSR_HV \t\t__MASK(MSR_HV_LG)\t/* Hypervisor state */\n#else\n/* so tests for these bits fail on 32-bit */\n#define MSR_SF\t\t0\n#define MSR_ISF\t\t0\n#define MSR_HV\t\t0\n#endif\n\n#define MSR_VEC\t\t__MASK(MSR_VEC_LG)\t/* Enable AltiVec */\n#define MSR_VSX\t\t__MASK(MSR_VSX_LG)\t/* Enable VSX */\n#define MSR_POW\t\t__MASK(MSR_POW_LG)\t/* Enable Power Management */\n#define MSR_WE\t\t__MASK(MSR_WE_LG)\t/* Wait State Enable */\n#define MSR_TGPR\t__MASK(MSR_TGPR_LG)\t/* TLB Update registers in use */\n#define MSR_CE\t\t__MASK(MSR_CE_LG)\t/* Critical Interrupt Enable */\n#define MSR_ILE\t\t__MASK(MSR_ILE_LG)\t/* Interrupt Little Endian */\n#define MSR_EE\t\t__MASK(MSR_EE_LG)\t/* External Interrupt Enable */\n#define MSR_PR\t\t__MASK(MSR_PR_LG)\t/* Problem State / Privilege Level */\n#define MSR_FP\t\t__MASK(MSR_FP_LG)\t/* Floating Point enable */\n#define MSR_ME\t\t__MASK(MSR_ME_LG)\t/* Machine Check Enable */\n#define MSR_FE0\t\t__MASK(MSR_FE0_LG)\t/* Floating Exception mode 0 */\n#define MSR_SE\t\t__MASK(MSR_SE_LG)\t/* Single Step */\n#define MSR_BE\t\t__MASK(MSR_BE_LG)\t/* Branch Trace */\n#define MSR_DE\t\t__MASK(MSR_DE_LG)\t/* Debug Exception Enable */\n#define MSR_FE1\t\t__MASK(MSR_FE1_LG)\t/* Floating Exception mode 1 */\n#define MSR_IP\t\t__MASK(MSR_IP_LG)\t/* Exception prefix 0x000/0xFFF */\n#define MSR_IR\t\t__MASK(MSR_IR_LG)\t/* Instruction Relocate */\n#define MSR_DR\t\t__MASK(MSR_DR_LG)\t/* Data Relocate */\n#define MSR_PE\t\t__MASK(MSR_PE_LG)\t/* Protection Enable */\n#define MSR_PX\t\t__MASK(MSR_PX_LG)\t/* Protection Exclusive Mode */\n#ifndef MSR_PMM\n#define MSR_PMM\t\t__MASK(MSR_PMM_LG)\t/* Performance monitor */\n#endif\n#define MSR_RI\t\t__MASK(MSR_RI_LG)\t/* Recoverable Exception */\n#define MSR_LE\t\t__MASK(MSR_LE_LG)\t/* Little Endian */\n\n#if defined(CONFIG_PPC_BOOK3S_64)\n/* Server variant */\n#define MSR_\t\tMSR_ME | MSR_RI | MSR_IR | MSR_DR | MSR_ISF |MSR_HV\n#define MSR_KERNEL      MSR_ | MSR_SF\n#define MSR_USER32\tMSR_ | MSR_PR | MSR_EE\n#define MSR_USER64\tMSR_USER32 | MSR_SF\n#elif defined(CONFIG_PPC_BOOK3S_32) || defined(CONFIG_8xx)\n/* Default MSR for kernel mode. */\n#define MSR_KERNEL\t(MSR_ME|MSR_RI|MSR_IR|MSR_DR)\n#define MSR_USER\t(MSR_KERNEL|MSR_PR|MSR_EE)\n#endif\n\n/* Floating Point Status and Control Register (FPSCR) Fields */\n#define FPSCR_FX\t0x80000000\t/* FPU exception summary */\n#define FPSCR_FEX\t0x40000000\t/* FPU enabled exception summary */\n#define FPSCR_VX\t0x20000000\t/* Invalid operation summary */\n#define FPSCR_OX\t0x10000000\t/* Overflow exception summary */\n#define FPSCR_UX\t0x08000000\t/* Underflow exception summary */\n#define FPSCR_ZX\t0x04000000\t/* Zero-divide exception summary */\n#define FPSCR_XX\t0x02000000\t/* Inexact exception summary */\n#define FPSCR_VXSNAN\t0x01000000\t/* Invalid op for SNaN */\n#define FPSCR_VXISI\t0x00800000\t/* Invalid op for Inv - Inv */\n#define FPSCR_VXIDI\t0x00400000\t/* Invalid op for Inv / Inv */\n#define FPSCR_VXZDZ\t0x00200000\t/* Invalid op for Zero / Zero */\n#define FPSCR_VXIMZ\t0x00100000\t/* Invalid op for Inv * Zero */\n#define FPSCR_VXVC\t0x00080000\t/* Invalid op for Compare */\n#define FPSCR_FR\t0x00040000\t/* Fraction rounded */\n#define FPSCR_FI\t0x00020000\t/* Fraction inexact */\n#define FPSCR_FPRF\t0x0001f000\t/* FPU Result Flags */\n#define FPSCR_FPCC\t0x0000f000\t/* FPU Condition Codes */\n#define FPSCR_VXSOFT\t0x00000400\t/* Invalid op for software request */\n#define FPSCR_VXSQRT\t0x00000200\t/* Invalid op for square root */\n#define FPSCR_VXCVI\t0x00000100\t/* Invalid op for integer convert */\n#define FPSCR_VE\t0x00000080\t/* Invalid op exception enable */\n#define FPSCR_OE\t0x00000040\t/* IEEE overflow exception enable */\n#define FPSCR_UE\t0x00000020\t/* IEEE underflow exception enable */\n#define FPSCR_ZE\t0x00000010\t/* IEEE zero divide exception enable */\n#define FPSCR_XE\t0x00000008\t/* FP inexact exception enable */\n#define FPSCR_NI\t0x00000004\t/* FPU non IEEE-Mode */\n#define FPSCR_RN\t0x00000003\t/* FPU rounding control */\n\n/* Bit definitions for SPEFSCR. */\n#define SPEFSCR_SOVH\t0x80000000\t/* Summary integer overflow high */\n#define SPEFSCR_OVH\t0x40000000\t/* Integer overflow high */\n#define SPEFSCR_FGH\t0x20000000\t/* Embedded FP guard bit high */\n#define SPEFSCR_FXH\t0x10000000\t/* Embedded FP sticky bit high */\n#define SPEFSCR_FINVH\t0x08000000\t/* Embedded FP invalid operation high */\n#define SPEFSCR_FDBZH\t0x04000000\t/* Embedded FP div by zero high */\n#define SPEFSCR_FUNFH\t0x02000000\t/* Embedded FP underflow high */\n#define SPEFSCR_FOVFH\t0x01000000\t/* Embedded FP overflow high */\n#define SPEFSCR_FINXS\t0x00200000\t/* Embedded FP inexact sticky */\n#define SPEFSCR_FINVS\t0x00100000\t/* Embedded FP invalid op. sticky */\n#define SPEFSCR_FDBZS\t0x00080000\t/* Embedded FP div by zero sticky */\n#define SPEFSCR_FUNFS\t0x00040000\t/* Embedded FP underflow sticky */\n#define SPEFSCR_FOVFS\t0x00020000\t/* Embedded FP overflow sticky */\n#define SPEFSCR_MODE\t0x00010000\t/* Embedded FP mode */\n#define SPEFSCR_SOV\t0x00008000\t/* Integer summary overflow */\n#define SPEFSCR_OV\t0x00004000\t/* Integer overflow */\n#define SPEFSCR_FG\t0x00002000\t/* Embedded FP guard bit */\n#define SPEFSCR_FX\t0x00001000\t/* Embedded FP sticky bit */\n#define SPEFSCR_FINV\t0x00000800\t/* Embedded FP invalid operation */\n#define SPEFSCR_FDBZ\t0x00000400\t/* Embedded FP div by zero */\n#define SPEFSCR_FUNF\t0x00000200\t/* Embedded FP underflow */\n#define SPEFSCR_FOVF\t0x00000100\t/* Embedded FP overflow */\n#define SPEFSCR_FINXE\t0x00000040\t/* Embedded FP inexact enable */\n#define SPEFSCR_FINVE\t0x00000020\t/* Embedded FP invalid op. enable */\n#define SPEFSCR_FDBZE\t0x00000010\t/* Embedded FP div by zero enable */\n#define SPEFSCR_FUNFE\t0x00000008\t/* Embedded FP underflow enable */\n#define SPEFSCR_FOVFE\t0x00000004\t/* Embedded FP overflow enable */\n#define SPEFSCR_FRMC \t0x00000003\t/* Embedded FP rounding mode control */\n\n/* Special Purpose Registers (SPRNs)*/\n#define SPRN_CTR\t0x009\t/* Count Register */\n#define SPRN_DSCR\t0x11\n#define SPRN_CTRLF\t0x088\n#define SPRN_CTRLT\t0x098\n#define   CTRL_CT\t0xc0000000\t/* current thread */\n#define   CTRL_CT0\t0x80000000\t/* thread 0 */\n#define   CTRL_CT1\t0x40000000\t/* thread 1 */\n#define   CTRL_TE\t0x00c00000\t/* thread enable */\n#define   CTRL_RUNLATCH\t0x1\n#define SPRN_DABR\t0x3F5\t/* Data Address Breakpoint Register */\n#define   DABR_TRANSLATION\t(1UL << 2)\n#define   DABR_DATA_WRITE\t(1UL << 1)\n#define   DABR_DATA_READ\t(1UL << 0)\n#define SPRN_DABR2\t0x13D\t/* e300 */\n#define SPRN_DABRX\t0x3F7\t/* Data Address Breakpoint Register Extension */\n#define   DABRX_USER\t(1UL << 0)\n#define   DABRX_KERNEL\t(1UL << 1)\n#define SPRN_DAR\t0x013\t/* Data Address Register */\n#define SPRN_DBCR\t0x136\t/* e300 Data Breakpoint Control Reg */\n#define SPRN_DSISR\t0x012\t/* Data Storage Interrupt Status Register */\n#define   DSISR_NOHPTE\t\t0x40000000\t/* no translation found */\n#define   DSISR_PROTFAULT\t0x08000000\t/* protection fault */\n#define   DSISR_ISSTORE\t\t0x02000000\t/* access was a store */\n#define   DSISR_DABRMATCH\t0x00400000\t/* hit data breakpoint */\n#define   DSISR_NOSEGMENT\t0x00200000\t/* STAB/SLB miss */\n#define SPRN_TBRL\t0x10C\t/* Time Base Read Lower Register (user, R/O) */\n#define SPRN_TBRU\t0x10D\t/* Time Base Read Upper Register (user, R/O) */\n#define SPRN_TBWL\t0x11C\t/* Time Base Lower Register (super, R/W) */\n#define SPRN_TBWU\t0x11D\t/* Time Base Upper Register (super, R/W) */\n#define SPRN_SPURR\t0x134\t/* Scaled PURR */\n#define SPRN_HIOR\t0x137\t/* 970 Hypervisor interrupt offset */\n#define SPRN_LPCR\t0x13E\t/* LPAR Control Register */\n#define SPRN_DBAT0L\t0x219\t/* Data BAT 0 Lower Register */\n#define SPRN_DBAT0U\t0x218\t/* Data BAT 0 Upper Register */\n#define SPRN_DBAT1L\t0x21B\t/* Data BAT 1 Lower Register */\n#define SPRN_DBAT1U\t0x21A\t/* Data BAT 1 Upper Register */\n#define SPRN_DBAT2L\t0x21D\t/* Data BAT 2 Lower Register */\n#define SPRN_DBAT2U\t0x21C\t/* Data BAT 2 Upper Register */\n#define SPRN_DBAT3L\t0x21F\t/* Data BAT 3 Lower Register */\n#define SPRN_DBAT3U\t0x21E\t/* Data BAT 3 Upper Register */\n#define SPRN_DBAT4L\t0x239\t/* Data BAT 4 Lower Register */\n#define SPRN_DBAT4U\t0x238\t/* Data BAT 4 Upper Register */\n#define SPRN_DBAT5L\t0x23B\t/* Data BAT 5 Lower Register */\n#define SPRN_DBAT5U\t0x23A\t/* Data BAT 5 Upper Register */\n#define SPRN_DBAT6L\t0x23D\t/* Data BAT 6 Lower Register */\n#define SPRN_DBAT6U\t0x23C\t/* Data BAT 6 Upper Register */\n#define SPRN_DBAT7L\t0x23F\t/* Data BAT 7 Lower Register */\n#define SPRN_DBAT7U\t0x23E\t/* Data BAT 7 Upper Register */\n\n#define SPRN_DEC\t0x016\t\t/* Decrement Register */\n#define SPRN_DER\t0x095\t\t/* Debug Enable Regsiter */\n#define DER_RSTE\t0x40000000\t/* Reset Interrupt */\n#define DER_CHSTPE\t0x20000000\t/* Check Stop */\n#define DER_MCIE\t0x10000000\t/* Machine Check Interrupt */\n#define DER_EXTIE\t0x02000000\t/* External Interrupt */\n#define DER_ALIE\t0x01000000\t/* Alignment Interrupt */\n#define DER_PRIE\t0x00800000\t/* Program Interrupt */\n#define DER_FPUVIE\t0x00400000\t/* FP Unavailable Interrupt */\n#define DER_DECIE\t0x00200000\t/* Decrementer Interrupt */\n#define DER_SYSIE\t0x00040000\t/* System Call Interrupt */\n#define DER_TRE\t\t0x00020000\t/* Trace Interrupt */\n#define DER_SEIE\t0x00004000\t/* FP SW Emulation Interrupt */\n#define DER_ITLBMSE\t0x00002000\t/* Imp. Spec. Instruction TLB Miss */\n#define DER_ITLBERE\t0x00001000\t/* Imp. Spec. Instruction TLB Error */\n#define DER_DTLBMSE\t0x00000800\t/* Imp. Spec. Data TLB Miss */\n#define DER_DTLBERE\t0x00000400\t/* Imp. Spec. Data TLB Error */\n#define DER_LBRKE\t0x00000008\t/* Load/Store Breakpoint Interrupt */\n#define DER_IBRKE\t0x00000004\t/* Instruction Breakpoint Interrupt */\n#define DER_EBRKE\t0x00000002\t/* External Breakpoint Interrupt */\n#define DER_DPIE\t0x00000001\t/* Dev. Port Nonmaskable Request */\n#define SPRN_DMISS\t0x3D0\t\t/* Data TLB Miss Register */\n#define SPRN_EAR\t0x11A\t\t/* External Address Register */\n#define SPRN_HASH1\t0x3D2\t\t/* Primary Hash Address Register */\n#define SPRN_HASH2\t0x3D3\t\t/* Secondary Hash Address Resgister */\n#define SPRN_HID0\t0x3F0\t\t/* Hardware Implementation Register 0 */\n#define HID0_EMCP\t(1<<31)\t\t/* Enable Machine Check pin */\n#define HID0_EBA\t(1<<29)\t\t/* Enable Bus Address Parity */\n#define HID0_EBD\t(1<<28)\t\t/* Enable Bus Data Parity */\n#define HID0_SBCLK\t(1<<27)\n#define HID0_EICE\t(1<<26)\n#define HID0_TBEN\t(1<<26)\t\t/* Timebase enable - 745x */\n#define HID0_ECLK\t(1<<25)\n#define HID0_PAR\t(1<<24)\n#define HID0_STEN\t(1<<24)\t\t/* Software table search enable - 745x */\n#define HID0_HIGH_BAT\t(1<<23)\t\t/* Enable high BATs - 7455 */\n#define HID0_DOZE\t(1<<23)\n#define HID0_NAP\t(1<<22)\n#define HID0_SLEEP\t(1<<21)\n#define HID0_DPM\t(1<<20)\n#define HID0_BHTCLR\t(1<<18)\t\t/* Clear branch history table - 7450 */\n#define HID0_XAEN\t(1<<17)\t\t/* Extended addressing enable - 7450 */\n#define HID0_NHR\t(1<<16)\t\t/* Not hard reset (software bit-7450)*/\n#define HID0_ICE\t(1<<15)\t\t/* Instruction Cache Enable */\n#define HID0_DCE\t(1<<14)\t\t/* Data Cache Enable */\n#define HID0_ILOCK\t(1<<13)\t\t/* Instruction Cache Lock */\n#define HID0_DLOCK\t(1<<12)\t\t/* Data Cache Lock */\n#define HID0_ICFI\t(1<<11)\t\t/* Instr. Cache Flash Invalidate */\n#define HID0_DCI\t(1<<10)\t\t/* Data Cache Invalidate */\n#define HID0_SPD\t(1<<9)\t\t/* Speculative disable */\n#define HID0_DAPUEN\t(1<<8)\t\t/* Debug APU enable */\n#define HID0_SGE\t(1<<7)\t\t/* Store Gathering Enable */\n#define HID0_SIED\t(1<<7)\t\t/* Serial Instr. Execution [Disable] */\n#define HID0_DCFA\t(1<<6)\t\t/* Data Cache Flush Assist */\n#define HID0_LRSTK\t(1<<4)\t\t/* Link register stack - 745x */\n#define HID0_BTIC\t(1<<5)\t\t/* Branch Target Instr Cache Enable */\n#define HID0_ABE\t(1<<3)\t\t/* Address Broadcast Enable */\n#define HID0_FOLD\t(1<<3)\t\t/* Branch Folding enable - 745x */\n#define HID0_BHTE\t(1<<2)\t\t/* Branch History Table Enable */\n#define HID0_BTCD\t(1<<1)\t\t/* Branch target cache disable */\n#define HID0_NOPDST\t(1<<1)\t\t/* No-op dst, dstt, etc. instr. */\n#define HID0_NOPTI\t(1<<0)\t\t/* No-op dcbt and dcbst instr. */\n\n#define SPRN_HID1\t0x3F1\t\t/* Hardware Implementation Register 1 */\n#ifdef CONFIG_6xx\n#define HID1_EMCP\t(1<<31)\t\t/* 7450 Machine Check Pin Enable */\n#define HID1_DFS\t(1<<22)\t\t/* 7447A Dynamic Frequency Scaling */\n#define HID1_PC0\t(1<<16)\t\t/* 7450 PLL_CFG[0] */\n#define HID1_PC1\t(1<<15)\t\t/* 7450 PLL_CFG[1] */\n#define HID1_PC2\t(1<<14)\t\t/* 7450 PLL_CFG[2] */\n#define HID1_PC3\t(1<<13)\t\t/* 7450 PLL_CFG[3] */\n#define HID1_SYNCBE\t(1<<11)\t\t/* 7450 ABE for sync, eieio */\n#define HID1_ABE\t(1<<10)\t\t/* 7450 Address Broadcast Enable */\n#define HID1_PS\t\t(1<<16)\t\t/* 750FX PLL selection */\n#endif\n#define SPRN_HID2\t0x3F8\t\t/* Hardware Implementation Register 2 */\n#define SPRN_HID2_GEKKO\t0x398\t\t/* Gekko HID2 Register */\n#define SPRN_IABR\t0x3F2\t/* Instruction Address Breakpoint Register */\n#define SPRN_IABR2\t0x3FA\t\t/* 83xx */\n#define SPRN_IBCR\t0x135\t\t/* 83xx Insn Breakpoint Control Reg */\n#define SPRN_HID4\t0x3F4\t\t/* 970 HID4 */\n#define SPRN_HID4_GEKKO\t0x3F3\t\t/* Gekko HID4 */\n#define SPRN_HID5\t0x3F6\t\t/* 970 HID5 */\n#define SPRN_HID6\t0x3F9\t/* BE HID 6 */\n#define   HID6_LB\t(0x0F<<12) /* Concurrent Large Page Modes */\n#define   HID6_DLP\t(1<<20)\t/* Disable all large page modes (4K only) */\n#define SPRN_TSC_CELL\t0x399\t/* Thread switch control on Cell */\n#define   TSC_CELL_DEC_ENABLE_0\t0x400000 /* Decrementer Interrupt */\n#define   TSC_CELL_DEC_ENABLE_1\t0x200000 /* Decrementer Interrupt */\n#define   TSC_CELL_EE_ENABLE\t0x100000 /* External Interrupt */\n#define   TSC_CELL_EE_BOOST\t0x080000 /* External Interrupt Boost */\n#define SPRN_TSC \t0x3FD\t/* Thread switch control on others */\n#define SPRN_TST \t0x3FC\t/* Thread switch timeout on others */\n#if !defined(SPRN_IAC1) && !defined(SPRN_IAC2)\n#define SPRN_IAC1\t0x3F4\t\t/* Instruction Address Compare 1 */\n#define SPRN_IAC2\t0x3F5\t\t/* Instruction Address Compare 2 */\n#endif\n#define SPRN_IBAT0L\t0x211\t\t/* Instruction BAT 0 Lower Register */\n#define SPRN_IBAT0U\t0x210\t\t/* Instruction BAT 0 Upper Register */\n#define SPRN_IBAT1L\t0x213\t\t/* Instruction BAT 1 Lower Register */\n#define SPRN_IBAT1U\t0x212\t\t/* Instruction BAT 1 Upper Register */\n#define SPRN_IBAT2L\t0x215\t\t/* Instruction BAT 2 Lower Register */\n#define SPRN_IBAT2U\t0x214\t\t/* Instruction BAT 2 Upper Register */\n#define SPRN_IBAT3L\t0x217\t\t/* Instruction BAT 3 Lower Register */\n#define SPRN_IBAT3U\t0x216\t\t/* Instruction BAT 3 Upper Register */\n#define SPRN_IBAT4L\t0x231\t\t/* Instruction BAT 4 Lower Register */\n#define SPRN_IBAT4U\t0x230\t\t/* Instruction BAT 4 Upper Register */\n#define SPRN_IBAT5L\t0x233\t\t/* Instruction BAT 5 Lower Register */\n#define SPRN_IBAT5U\t0x232\t\t/* Instruction BAT 5 Upper Register */\n#define SPRN_IBAT6L\t0x235\t\t/* Instruction BAT 6 Lower Register */\n#define SPRN_IBAT6U\t0x234\t\t/* Instruction BAT 6 Upper Register */\n#define SPRN_IBAT7L\t0x237\t\t/* Instruction BAT 7 Lower Register */\n#define SPRN_IBAT7U\t0x236\t\t/* Instruction BAT 7 Upper Register */\n#define SPRN_ICMP\t0x3D5\t\t/* Instruction TLB Compare Register */\n#define SPRN_ICTC\t0x3FB\t/* Instruction Cache Throttling Control Reg */\n#define SPRN_ICTRL\t0x3F3\t/* 1011 7450 icache and interrupt ctrl */\n#define ICTRL_EICE\t0x08000000\t/* enable icache parity errs */\n#define ICTRL_EDC\t0x04000000\t/* enable dcache parity errs */\n#define ICTRL_EICP\t0x00000100\t/* enable icache par. check */\n#define SPRN_IMISS\t0x3D4\t\t/* Instruction TLB Miss Register */\n#define SPRN_IMMR\t0x27E\t\t/* Internal Memory Map Register */\n#define SPRN_L2CR\t0x3F9\t\t/* Level 2 Cache Control Regsiter */\n#define SPRN_L2CR2\t0x3f8\n#define L2CR_L2E\t\t0x80000000\t/* L2 enable */\n#define L2CR_L2PE\t\t0x40000000\t/* L2 parity enable */\n#define L2CR_L2SIZ_MASK\t\t0x30000000\t/* L2 size mask */\n#define L2CR_L2SIZ_256KB\t0x10000000\t/* L2 size 256KB */\n#define L2CR_L2SIZ_512KB\t0x20000000\t/* L2 size 512KB */\n#define L2CR_L2SIZ_1MB\t\t0x30000000\t/* L2 size 1MB */\n#define L2CR_L2CLK_MASK\t\t0x0e000000\t/* L2 clock mask */\n#define L2CR_L2CLK_DISABLED\t0x00000000\t/* L2 clock disabled */\n#define L2CR_L2CLK_DIV1\t\t0x02000000\t/* L2 clock / 1 */\n#define L2CR_L2CLK_DIV1_5\t0x04000000\t/* L2 clock / 1.5 */\n#define L2CR_L2CLK_DIV2\t\t0x08000000\t/* L2 clock / 2 */\n#define L2CR_L2CLK_DIV2_5\t0x0a000000\t/* L2 clock / 2.5 */\n#define L2CR_L2CLK_DIV3\t\t0x0c000000\t/* L2 clock / 3 */\n#define L2CR_L2RAM_MASK\t\t0x01800000\t/* L2 RAM type mask */\n#define L2CR_L2RAM_FLOW\t\t0x00000000\t/* L2 RAM flow through */\n#define L2CR_L2RAM_PIPE\t\t0x01000000\t/* L2 RAM pipelined */\n#define L2CR_L2RAM_PIPE_LW\t0x01800000\t/* L2 RAM pipelined latewr */\n#define L2CR_L2DO\t\t0x00400000\t/* L2 data only */\n#define L2CR_L2I\t\t0x00200000\t/* L2 global invalidate */\n#define L2CR_L2CTL\t\t0x00100000\t/* L2 RAM control */\n#define L2CR_L2WT\t\t0x00080000\t/* L2 write-through */\n#define L2CR_L2TS\t\t0x00040000\t/* L2 test support */\n#define L2CR_L2OH_MASK\t\t0x00030000\t/* L2 output hold mask */\n#define L2CR_L2OH_0_5\t\t0x00000000\t/* L2 output hold 0.5 ns */\n#define L2CR_L2OH_1_0\t\t0x00010000\t/* L2 output hold 1.0 ns */\n#define L2CR_L2SL\t\t0x00008000\t/* L2 DLL slow */\n#define L2CR_L2DF\t\t0x00004000\t/* L2 differential clock */\n#define L2CR_L2BYP\t\t0x00002000\t/* L2 DLL bypass */\n#define L2CR_L2IP\t\t0x00000001\t/* L2 GI in progress */\n#define L2CR_L2IO_745x\t\t0x00100000\t/* L2 instr. only (745x) */\n#define L2CR_L2DO_745x\t\t0x00010000\t/* L2 data only (745x) */\n#define L2CR_L2REP_745x\t\t0x00001000\t/* L2 repl. algorithm (745x) */\n#define L2CR_L2HWF_745x\t\t0x00000800\t/* L2 hardware flush (745x) */\n#define SPRN_L3CR\t\t0x3FA\t/* Level 3 Cache Control Regsiter */\n#define L3CR_L3E\t\t0x80000000\t/* L3 enable */\n#define L3CR_L3PE\t\t0x40000000\t/* L3 data parity enable */\n#define L3CR_L3APE\t\t0x20000000\t/* L3 addr parity enable */\n#define L3CR_L3SIZ\t\t0x10000000\t/* L3 size */\n#define L3CR_L3CLKEN\t\t0x08000000\t/* L3 clock enable */\n#define L3CR_L3RES\t\t0x04000000\t/* L3 special reserved bit */\n#define L3CR_L3CLKDIV\t\t0x03800000\t/* L3 clock divisor */\n#define L3CR_L3IO\t\t0x00400000\t/* L3 instruction only */\n#define L3CR_L3SPO\t\t0x00040000\t/* L3 sample point override */\n#define L3CR_L3CKSP\t\t0x00030000\t/* L3 clock sample point */\n#define L3CR_L3PSP\t\t0x0000e000\t/* L3 P-clock sample point */\n#define L3CR_L3REP\t\t0x00001000\t/* L3 replacement algorithm */\n#define L3CR_L3HWF\t\t0x00000800\t/* L3 hardware flush */\n#define L3CR_L3I\t\t0x00000400\t/* L3 global invalidate */\n#define L3CR_L3RT\t\t0x00000300\t/* L3 SRAM type */\n#define L3CR_L3NIRCA\t\t0x00000080\t/* L3 non-integer ratio clock adj. */\n#define L3CR_L3DO\t\t0x00000040\t/* L3 data only mode */\n#define L3CR_PMEN\t\t0x00000004\t/* L3 private memory enable */\n#define L3CR_PMSIZ\t\t0x00000001\t/* L3 private memory size */\n\n#define SPRN_MSSCR0\t0x3f6\t/* Memory Subsystem Control Register 0 */\n#define SPRN_MSSSR0\t0x3f7\t/* Memory Subsystem Status Register 1 */\n#define SPRN_LDSTCR\t0x3f8\t/* Load/Store control register */\n#define SPRN_LDSTDB\t0x3f4\t/* */\n#define SPRN_LR\t\t0x008\t/* Link Register */\n#ifndef SPRN_PIR\n#define SPRN_PIR\t0x3FF\t/* Processor Identification Register */\n#endif\n#define SPRN_PTEHI\t0x3D5\t/* 981 7450 PTE HI word (S/W TLB load) */\n#define SPRN_PTELO\t0x3D6\t/* 982 7450 PTE LO word (S/W TLB load) */\n#define SPRN_PURR\t0x135\t/* Processor Utilization of Resources Reg */\n#define SPRN_PVR\t0x11F\t/* Processor Version Register */\n#define SPRN_RPA\t0x3D6\t/* Required Physical Address Register */\n#define SPRN_SDA\t0x3BF\t/* Sampled Data Address Register */\n#define SPRN_SDR1\t0x019\t/* MMU Hash Base Register */\n#define SPRN_ASR\t0x118   /* Address Space Register */\n#define SPRN_SIA\t0x3BB\t/* Sampled Instruction Address Register */\n#define SPRN_SPRG0\t0x110\t/* Special Purpose Register General 0 */\n#define SPRN_SPRG1\t0x111\t/* Special Purpose Register General 1 */\n#define SPRN_SPRG2\t0x112\t/* Special Purpose Register General 2 */\n#define SPRN_SPRG3\t0x113\t/* Special Purpose Register General 3 */\n#define SPRN_SPRG4\t0x114\t/* Special Purpose Register General 4 */\n#define SPRN_SPRG5\t0x115\t/* Special Purpose Register General 5 */\n#define SPRN_SPRG6\t0x116\t/* Special Purpose Register General 6 */\n#define SPRN_SPRG7\t0x117\t/* Special Purpose Register General 7 */\n#define SPRN_SRR0\t0x01A\t/* Save/Restore Register 0 */\n#define SPRN_SRR1\t0x01B\t/* Save/Restore Register 1 */\n#define   SRR1_WAKEMASK\t\t0x00380000 /* reason for wakeup */\n#define   SRR1_WAKERESET\t0x00380000 /* System reset */\n#define   SRR1_WAKESYSERR\t0x00300000 /* System error */\n#define   SRR1_WAKEEE\t\t0x00200000 /* External interrupt */\n#define   SRR1_WAKEMT\t\t0x00280000 /* mtctrl */\n#define   SRR1_WAKEDEC\t\t0x00180000 /* Decrementer interrupt */\n#define   SRR1_WAKETHERM\t0x00100000 /* Thermal management interrupt */\n#define   SRR1_PROGFPE\t\t0x00100000 /* Floating Point Enabled */\n#define   SRR1_PROGPRIV\t\t0x00040000 /* Privileged instruction */\n#define   SRR1_PROGTRAP\t\t0x00020000 /* Trap */\n#define   SRR1_PROGADDR\t\t0x00010000 /* SRR0 contains subsequent addr */\n#define SPRN_HSRR0\t0x13A\t/* Save/Restore Register 0 */\n#define SPRN_HSRR1\t0x13B\t/* Save/Restore Register 1 */\n\n#define SPRN_TBCTL\t0x35f\t/* PA6T Timebase control register */\n#define   TBCTL_FREEZE\t\t0x0000000000000000ull /* Freeze all tbs */\n#define   TBCTL_RESTART\t\t0x0000000100000000ull /* Restart all tbs */\n#define   TBCTL_UPDATE_UPPER\t0x0000000200000000ull /* Set upper 32 bits */\n#define   TBCTL_UPDATE_LOWER\t0x0000000300000000ull /* Set lower 32 bits */\n\n#ifndef SPRN_SVR\n#define SPRN_SVR\t0x11E\t/* System Version Register */\n#endif\n#define SPRN_THRM1\t0x3FC\t\t/* Thermal Management Register 1 */\n/* these bits were defined in inverted endian sense originally, ugh, confusing */\n#define THRM1_TIN\t(1 << 31)\n#define THRM1_TIV\t(1 << 30)\n#define THRM1_THRES(x)\t((x&0x7f)<<23)\n#define THRM3_SITV(x)\t((x&0x3fff)<<1)\n#define THRM1_TID\t(1<<2)\n#define THRM1_TIE\t(1<<1)\n#define THRM1_V\t\t(1<<0)\n#define SPRN_THRM2\t0x3FD\t\t/* Thermal Management Register 2 */\n#define SPRN_THRM3\t0x3FE\t\t/* Thermal Management Register 3 */\n#define THRM3_E\t\t(1<<0)\n#define SPRN_TLBMISS\t0x3D4\t\t/* 980 7450 TLB Miss Register */\n#define SPRN_UMMCR0\t0x3A8\t/* User Monitor Mode Control Register 0 */\n#define SPRN_UMMCR1\t0x3AC\t/* User Monitor Mode Control Register 0 */\n#define SPRN_UPMC1\t0x3A9\t/* User Performance Counter Register 1 */\n#define SPRN_UPMC2\t0x3AA\t/* User Performance Counter Register 2 */\n#define SPRN_UPMC3\t0x3AD\t/* User Performance Counter Register 3 */\n#define SPRN_UPMC4\t0x3AE\t/* User Performance Counter Register 4 */\n#define SPRN_USIA\t0x3AB\t/* User Sampled Instruction Address Register */\n#define SPRN_VRSAVE\t0x100\t/* Vector Register Save Register */\n#define SPRN_XER\t0x001\t/* Fixed Point Exception Register */\n\n#define SPRN_MMCR0_GEKKO 0x3B8 /* Gekko Monitor Mode Control Register 0 */\n#define SPRN_MMCR1_GEKKO 0x3BC /* Gekko Monitor Mode Control Register 1 */\n#define SPRN_PMC1_GEKKO  0x3B9 /* Gekko Performance Monitor Control 1 */\n#define SPRN_PMC2_GEKKO  0x3BA /* Gekko Performance Monitor Control 2 */\n#define SPRN_PMC3_GEKKO  0x3BD /* Gekko Performance Monitor Control 3 */\n#define SPRN_PMC4_GEKKO  0x3BE /* Gekko Performance Monitor Control 4 */\n#define SPRN_WPAR_GEKKO  0x399 /* Gekko Write Pipe Address Register */\n\n#define SPRN_SCOMC\t0x114\t/* SCOM Access Control */\n#define SPRN_SCOMD\t0x115\t/* SCOM Access DATA */\n\n/* Performance monitor SPRs */\n#ifdef CONFIG_PPC64\n#define SPRN_MMCR0\t795\n#define   MMCR0_FC\t0x80000000UL /* freeze counters */\n#define   MMCR0_FCS\t0x40000000UL /* freeze in supervisor state */\n#define   MMCR0_KERNEL_DISABLE MMCR0_FCS\n#define   MMCR0_FCP\t0x20000000UL /* freeze in problem state */\n#define   MMCR0_PROBLEM_DISABLE MMCR0_FCP\n#define   MMCR0_FCM1\t0x10000000UL /* freeze counters while MSR mark = 1 */\n#define   MMCR0_FCM0\t0x08000000UL /* freeze counters while MSR mark = 0 */\n#define   MMCR0_PMXE\t0x04000000UL /* performance monitor exception enable */\n#define   MMCR0_FCECE\t0x02000000UL /* freeze ctrs on enabled cond or event */\n#define   MMCR0_TBEE\t0x00400000UL /* time base exception enable */\n#define   MMCR0_PMC1CE\t0x00008000UL /* PMC1 count enable*/\n#define   MMCR0_PMCjCE\t0x00004000UL /* PMCj count enable*/\n#define   MMCR0_TRIGGER\t0x00002000UL /* TRIGGER enable */\n#define   MMCR0_PMAO\t0x00000080UL /* performance monitor alert has occurred, set to 0 after handling exception */\n#define   MMCR0_SHRFC\t0x00000040UL /* SHRre freeze conditions between threads */\n#define   MMCR0_FCTI\t0x00000008UL /* freeze counters in tags inactive mode */\n#define   MMCR0_FCTA\t0x00000004UL /* freeze counters in tags active mode */\n#define   MMCR0_FCWAIT\t0x00000002UL /* freeze counter in WAIT state */\n#define   MMCR0_FCHV\t0x00000001UL /* freeze conditions in hypervisor mode */\n#define SPRN_MMCR1\t798\n#define SPRN_MMCRA\t0x312\n#define   MMCRA_SDSYNC\t0x80000000UL /* SDAR synced with SIAR */\n#define   MMCRA_SDAR_DCACHE_MISS 0x40000000UL\n#define   MMCRA_SDAR_ERAT_MISS   0x20000000UL\n#define   MMCRA_SIHV\t0x10000000UL /* state of MSR HV when SIAR set */\n#define   MMCRA_SIPR\t0x08000000UL /* state of MSR PR when SIAR set */\n#define   MMCRA_SLOT\t0x07000000UL /* SLOT bits (37-39) */\n#define   MMCRA_SLOT_SHIFT\t24\n#define   MMCRA_SAMPLE_ENABLE 0x00000001UL /* enable sampling */\n#define   POWER6_MMCRA_SDSYNC 0x0000080000000000ULL\t/* SDAR/SIAR synced */\n#define   POWER6_MMCRA_SIHV   0x0000040000000000ULL\n#define   POWER6_MMCRA_SIPR   0x0000020000000000ULL\n#define   POWER6_MMCRA_THRM\t0x00000020UL\n#define   POWER6_MMCRA_OTHER\t0x0000000EUL\n#define SPRN_PMC1\t787\n#define SPRN_PMC2\t788\n#define SPRN_PMC3\t789\n#define SPRN_PMC4\t790\n#define SPRN_PMC5\t791\n#define SPRN_PMC6\t792\n#define SPRN_PMC7\t793\n#define SPRN_PMC8\t794\n#define SPRN_SIAR\t780\n#define SPRN_SDAR\t781\n\n#define SPRN_PA6T_MMCR0 795\n#define   PA6T_MMCR0_EN0\t0x0000000000000001UL\n#define   PA6T_MMCR0_EN1\t0x0000000000000002UL\n#define   PA6T_MMCR0_EN2\t0x0000000000000004UL\n#define   PA6T_MMCR0_EN3\t0x0000000000000008UL\n#define   PA6T_MMCR0_EN4\t0x0000000000000010UL\n#define   PA6T_MMCR0_EN5\t0x0000000000000020UL\n#define   PA6T_MMCR0_SUPEN\t0x0000000000000040UL\n#define   PA6T_MMCR0_PREN\t0x0000000000000080UL\n#define   PA6T_MMCR0_HYPEN\t0x0000000000000100UL\n#define   PA6T_MMCR0_FCM0\t0x0000000000000200UL\n#define   PA6T_MMCR0_FCM1\t0x0000000000000400UL\n#define   PA6T_MMCR0_INTGEN\t0x0000000000000800UL\n#define   PA6T_MMCR0_INTEN0\t0x0000000000001000UL\n#define   PA6T_MMCR0_INTEN1\t0x0000000000002000UL\n#define   PA6T_MMCR0_INTEN2\t0x0000000000004000UL\n#define   PA6T_MMCR0_INTEN3\t0x0000000000008000UL\n#define   PA6T_MMCR0_INTEN4\t0x0000000000010000UL\n#define   PA6T_MMCR0_INTEN5\t0x0000000000020000UL\n#define   PA6T_MMCR0_DISCNT\t0x0000000000040000UL\n#define   PA6T_MMCR0_UOP\t0x0000000000080000UL\n#define   PA6T_MMCR0_TRG\t0x0000000000100000UL\n#define   PA6T_MMCR0_TRGEN\t0x0000000000200000UL\n#define   PA6T_MMCR0_TRGREG\t0x0000000001600000UL\n#define   PA6T_MMCR0_SIARLOG\t0x0000000002000000UL\n#define   PA6T_MMCR0_SDARLOG\t0x0000000004000000UL\n#define   PA6T_MMCR0_PROEN\t0x0000000008000000UL\n#define   PA6T_MMCR0_PROLOG\t0x0000000010000000UL\n#define   PA6T_MMCR0_DAMEN2\t0x0000000020000000UL\n#define   PA6T_MMCR0_DAMEN3\t0x0000000040000000UL\n#define   PA6T_MMCR0_DAMEN4\t0x0000000080000000UL\n#define   PA6T_MMCR0_DAMEN5\t0x0000000100000000UL\n#define   PA6T_MMCR0_DAMSEL2\t0x0000000200000000UL\n#define   PA6T_MMCR0_DAMSEL3\t0x0000000400000000UL\n#define   PA6T_MMCR0_DAMSEL4\t0x0000000800000000UL\n#define   PA6T_MMCR0_DAMSEL5\t0x0000001000000000UL\n#define   PA6T_MMCR0_HANDDIS\t0x0000002000000000UL\n#define   PA6T_MMCR0_PCTEN\t0x0000004000000000UL\n#define   PA6T_MMCR0_SOCEN\t0x0000008000000000UL\n#define   PA6T_MMCR0_SOCMOD\t0x0000010000000000UL\n\n#define SPRN_PA6T_MMCR1 798\n#define   PA6T_MMCR1_ES2\t0x00000000000000ffUL\n#define   PA6T_MMCR1_ES3\t0x000000000000ff00UL\n#define   PA6T_MMCR1_ES4\t0x0000000000ff0000UL\n#define   PA6T_MMCR1_ES5\t0x00000000ff000000UL\n\n#define SPRN_PA6T_UPMC0 771\t/* User PerfMon Counter 0 */\n#define SPRN_PA6T_UPMC1 772\t/* ... */\n#define SPRN_PA6T_UPMC2 773\n#define SPRN_PA6T_UPMC3 774\n#define SPRN_PA6T_UPMC4 775\n#define SPRN_PA6T_UPMC5 776\n#define SPRN_PA6T_UMMCR0 779\t/* User Monitor Mode Control Register 0 */\n#define SPRN_PA6T_SIAR\t780\t/* Sampled Instruction Address */\n#define SPRN_PA6T_UMMCR1 782\t/* User Monitor Mode Control Register 1 */\n#define SPRN_PA6T_SIER\t785\t/* Sampled Instruction Event Register */\n#define SPRN_PA6T_PMC0\t787\n#define SPRN_PA6T_PMC1\t788\n#define SPRN_PA6T_PMC2\t789\n#define SPRN_PA6T_PMC3\t790\n#define SPRN_PA6T_PMC4\t791\n#define SPRN_PA6T_PMC5\t792\n#define SPRN_PA6T_TSR0\t793\t/* Timestamp Register 0 */\n#define SPRN_PA6T_TSR1\t794\t/* Timestamp Register 1 */\n#define SPRN_PA6T_TSR2\t799\t/* Timestamp Register 2 */\n#define SPRN_PA6T_TSR3\t784\t/* Timestamp Register 3 */\n\n#define SPRN_PA6T_IER\t981\t/* Icache Error Register */\n#define SPRN_PA6T_DER\t982\t/* Dcache Error Register */\n#define SPRN_PA6T_BER\t862\t/* BIU Error Address Register */\n#define SPRN_PA6T_MER\t849\t/* MMU Error Register */\n\n#define SPRN_PA6T_IMA0\t880\t/* Instruction Match Array 0 */\n#define SPRN_PA6T_IMA1\t881\t/* ... */\n#define SPRN_PA6T_IMA2\t882\n#define SPRN_PA6T_IMA3\t883\n#define SPRN_PA6T_IMA4\t884\n#define SPRN_PA6T_IMA5\t885\n#define SPRN_PA6T_IMA6\t886\n#define SPRN_PA6T_IMA7\t887\n#define SPRN_PA6T_IMA8\t888\n#define SPRN_PA6T_IMA9\t889\n#define SPRN_PA6T_BTCR\t978\t/* Breakpoint and Tagging Control Register */\n#define SPRN_PA6T_IMAAT\t979\t/* Instruction Match Array Action Table */\n#define SPRN_PA6T_PCCR\t1019\t/* Power Counter Control Register */\n#define SPRN_BKMK\t1020\t/* Cell Bookmark Register */\n#define SPRN_PA6T_RPCCR\t1021\t/* Retire PC Trace Control Register */\n\n\n#else /* 32-bit */\n#define SPRN_MMCR0\t952\t/* Monitor Mode Control Register 0 */\n#define   MMCR0_FC\t0x80000000UL /* freeze counters */\n#define   MMCR0_FCS\t0x40000000UL /* freeze in supervisor state */\n#define   MMCR0_FCP\t0x20000000UL /* freeze in problem state */\n#define   MMCR0_FCM1\t0x10000000UL /* freeze counters while MSR mark = 1 */\n#define   MMCR0_FCM0\t0x08000000UL /* freeze counters while MSR mark = 0 */\n#define   MMCR0_PMXE\t0x04000000UL /* performance monitor exception enable */\n#define   MMCR0_FCECE\t0x02000000UL /* freeze ctrs on enabled cond or event */\n#define   MMCR0_TBEE\t0x00400000UL /* time base exception enable */\n#define   MMCR0_PMC1CE\t0x00008000UL /* PMC1 count enable*/\n#define   MMCR0_PMCnCE\t0x00004000UL /* count enable for all but PMC 1*/\n#define   MMCR0_TRIGGER\t0x00002000UL /* TRIGGER enable */\n#define   MMCR0_PMC1SEL\t0x00001fc0UL /* PMC 1 Event */\n#define   MMCR0_PMC2SEL\t0x0000003fUL /* PMC 2 Event */\n\n#define SPRN_MMCR1\t956\n#define   MMCR1_PMC3SEL\t0xf8000000UL /* PMC 3 Event */\n#define   MMCR1_PMC4SEL\t0x07c00000UL /* PMC 4 Event */\n#define   MMCR1_PMC5SEL\t0x003e0000UL /* PMC 5 Event */\n#define   MMCR1_PMC6SEL 0x0001f800UL /* PMC 6 Event */\n#define SPRN_MMCR2\t944\n#define SPRN_PMC1\t953\t/* Performance Counter Register 1 */\n#define SPRN_PMC2\t954\t/* Performance Counter Register 2 */\n#define SPRN_PMC3\t957\t/* Performance Counter Register 3 */\n#define SPRN_PMC4\t958\t/* Performance Counter Register 4 */\n#define SPRN_PMC5\t945\t/* Performance Counter Register 5 */\n#define SPRN_PMC6\t946\t/* Performance Counter Register 6 */\n\n#define SPRN_SIAR\t955\t/* Sampled Instruction Address Register */\n\n/* Bit definitions for MMCR0 and PMC1 / PMC2. */\n#define MMCR0_PMC1_CYCLES\t(1 << 7)\n#define MMCR0_PMC1_ICACHEMISS\t(5 << 7)\n#define MMCR0_PMC1_DTLB\t\t(6 << 7)\n#define MMCR0_PMC2_DCACHEMISS\t0x6\n#define MMCR0_PMC2_CYCLES\t0x1\n#define MMCR0_PMC2_ITLB\t\t0x7\n#define MMCR0_PMC2_LOADMISSTIME\t0x5\n#endif\n\n/*\n * SPRG usage:\n *\n * All 64-bit:\n *\t- SPRG1 stores PACA pointer\n *\n * 64-bit server:\n *\t- SPRG0 unused (reserved for HV on Power4)\n *\t- SPRG2 scratch for exception vectors\n *\t- SPRG3 unused (user visible)\n *\n * 64-bit embedded\n *\t- SPRG0 generic exception scratch\n *\t- SPRG2 TLB exception stack\n *\t- SPRG3 unused (user visible)\n *\t- SPRG4 unused (user visible)\n *\t- SPRG6 TLB miss scratch (user visible, sorry !)\n *\t- SPRG7 critical exception scratch\n *\t- SPRG8 machine check exception scratch\n *\t- SPRG9 debug exception scratch\n *\n * All 32-bit:\n *\t- SPRG3 current thread_info pointer\n *        (virtual on BookE, physical on others)\n *\n * 32-bit classic:\n *\t- SPRG0 scratch for exception vectors\n *\t- SPRG1 scratch for exception vectors\n *\t- SPRG2 indicator that we are in RTAS\n *\t- SPRG4 (603 only) pseudo TLB LRU data\n *\n * 32-bit 40x:\n *\t- SPRG0 scratch for exception vectors\n *\t- SPRG1 scratch for exception vectors\n *\t- SPRG2 scratch for exception vectors\n *\t- SPRG4 scratch for exception vectors (not 403)\n *\t- SPRG5 scratch for exception vectors (not 403)\n *\t- SPRG6 scratch for exception vectors (not 403)\n *\t- SPRG7 scratch for exception vectors (not 403)\n *\n * 32-bit 440 and FSL BookE:\n *\t- SPRG0 scratch for exception vectors\n *\t- SPRG1 scratch for exception vectors (*)\n *\t- SPRG2 scratch for crit interrupts handler\n *\t- SPRG4 scratch for exception vectors\n *\t- SPRG5 scratch for exception vectors\n *\t- SPRG6 scratch for machine check handler\n *\t- SPRG7 scratch for exception vectors\n *\t- SPRG9 scratch for debug vectors (e500 only)\n *\n *      Additionally, BookE separates \"read\" and \"write\"\n *      of those registers. That allows to use the userspace\n *      readable variant for reads, which can avoid a fault\n *      with KVM type virtualization.\n *\n *      (*) Under KVM, the host SPRG1 is used to point to\n *      the current VCPU data structure\n *\n * 32-bit 8xx:\n *\t- SPRG0 scratch for exception vectors\n *\t- SPRG1 scratch for exception vectors\n *\t- SPRG2 apparently unused but initialized\n *\n */\n#ifdef CONFIG_PPC64\n#define SPRN_SPRG_PACA \t\tSPRN_SPRG1\n#else\n#define SPRN_SPRG_THREAD \tSPRN_SPRG3\n#endif\n\n#ifdef CONFIG_PPC_BOOK3S_64\n#define SPRN_SPRG_SCRATCH0\tSPRN_SPRG2\n#endif\n\n#ifdef CONFIG_PPC_BOOK3E_64\n#define SPRN_SPRG_MC_SCRATCH\tSPRN_SPRG8\n#define SPRN_SPRG_CRIT_SCRATCH\tSPRN_SPRG7\n#define SPRN_SPRG_DBG_SCRATCH\tSPRN_SPRG9\n#define SPRN_SPRG_TLB_EXFRAME\tSPRN_SPRG2\n#define SPRN_SPRG_TLB_SCRATCH\tSPRN_SPRG6\n#define SPRN_SPRG_GEN_SCRATCH\tSPRN_SPRG0\n#endif\n\n#ifdef CONFIG_PPC_BOOK3S_32\n#define SPRN_SPRG_SCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_SCRATCH1\tSPRN_SPRG1\n#define SPRN_SPRG_RTAS\t\tSPRN_SPRG2\n#define SPRN_SPRG_603_LRU\tSPRN_SPRG4\n#endif\n\n#ifdef CONFIG_40x\n#define SPRN_SPRG_SCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_SCRATCH1\tSPRN_SPRG1\n#define SPRN_SPRG_SCRATCH2\tSPRN_SPRG2\n#define SPRN_SPRG_SCRATCH3\tSPRN_SPRG4\n#define SPRN_SPRG_SCRATCH4\tSPRN_SPRG5\n#define SPRN_SPRG_SCRATCH5\tSPRN_SPRG6\n#define SPRN_SPRG_SCRATCH6\tSPRN_SPRG7\n#endif\n\n#ifdef CONFIG_BOOKE\n#define SPRN_SPRG_RSCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_WSCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_RSCRATCH1\tSPRN_SPRG1\n#define SPRN_SPRG_WSCRATCH1\tSPRN_SPRG1\n#define SPRN_SPRG_RSCRATCH_CRIT\tSPRN_SPRG2\n#define SPRN_SPRG_WSCRATCH_CRIT\tSPRN_SPRG2\n#define SPRN_SPRG_RSCRATCH2\tSPRN_SPRG4R\n#define SPRN_SPRG_WSCRATCH2\tSPRN_SPRG4W\n#define SPRN_SPRG_RSCRATCH3\tSPRN_SPRG5R\n#define SPRN_SPRG_WSCRATCH3\tSPRN_SPRG5W\n#define SPRN_SPRG_RSCRATCH_MC\tSPRN_SPRG6R\n#define SPRN_SPRG_WSCRATCH_MC\tSPRN_SPRG6W\n#define SPRN_SPRG_RSCRATCH4\tSPRN_SPRG7R\n#define SPRN_SPRG_WSCRATCH4\tSPRN_SPRG7W\n#ifdef CONFIG_E200\n#define SPRN_SPRG_RSCRATCH_DBG\tSPRN_SPRG6R\n#define SPRN_SPRG_WSCRATCH_DBG\tSPRN_SPRG6W\n#else\n#define SPRN_SPRG_RSCRATCH_DBG\tSPRN_SPRG9\n#define SPRN_SPRG_WSCRATCH_DBG\tSPRN_SPRG9\n#endif\n#define SPRN_SPRG_RVCPU\t\tSPRN_SPRG1\n#define SPRN_SPRG_WVCPU\t\tSPRN_SPRG1\n#endif\n\n#ifdef CONFIG_8xx\n#define SPRN_SPRG_SCRATCH0\tSPRN_SPRG0\n#define SPRN_SPRG_SCRATCH1\tSPRN_SPRG1\n#endif\n\n/*\n * An mtfsf instruction with the L bit set. On CPUs that support this a\n * full 64bits of FPSCR is restored and on other CPUs the L bit is ignored.\n *\n * Until binutils gets the new form of mtfsf, hardwire the instruction.\n */\n#ifdef CONFIG_PPC64\n#define MTFSF_L(REG) \\\n\t.long (0xfc00058e | ((0xff) << 17) | ((REG) << 11) | (1 << 25))\n#else\n#define MTFSF_L(REG)\tmtfsf\t0xff, (REG)\n#endif\n\n/* Processor Version Register (PVR) field extraction */\n\n#define PVR_VER(pvr)\t(((pvr) >>  16) & 0xFFFF)\t/* Version field */\n#define PVR_REV(pvr)\t(((pvr) >>   0) & 0xFFFF)\t/* Revison field */\n\n#define __is_processor(pv)\t(PVR_VER(mfspr(SPRN_PVR)) == (pv))\n\n/*\n * IBM has further subdivided the standard PowerPC 16-bit version and\n * revision subfields of the PVR for the PowerPC 403s into the following:\n */\n\n#define PVR_FAM(pvr)\t(((pvr) >> 20) & 0xFFF)\t/* Family field */\n#define PVR_MEM(pvr)\t(((pvr) >> 16) & 0xF)\t/* Member field */\n#define PVR_CORE(pvr)\t(((pvr) >> 12) & 0xF)\t/* Core field */\n#define PVR_CFG(pvr)\t(((pvr) >>  8) & 0xF)\t/* Configuration field */\n#define PVR_MAJ(pvr)\t(((pvr) >>  4) & 0xF)\t/* Major revision field */\n#define PVR_MIN(pvr)\t(((pvr) >>  0) & 0xF)\t/* Minor revision field */\n\n/* Processor Version Numbers */\n\n#define PVR_403GA\t0x00200000\n#define PVR_403GB\t0x00200100\n#define PVR_403GC\t0x00200200\n#define PVR_403GCX\t0x00201400\n#define PVR_405GP\t0x40110000\n#define PVR_476\t\t0x11a52000\n#define PVR_STB03XXX\t0x40310000\n#define PVR_NP405H\t0x41410000\n#define PVR_NP405L\t0x41610000\n#define PVR_601\t\t0x00010000\n#define PVR_602\t\t0x00050000\n#define PVR_603\t\t0x00030000\n#define PVR_603e\t0x00060000\n#define PVR_603ev\t0x00070000\n#define PVR_603r\t0x00071000\n#define PVR_604\t\t0x00040000\n#define PVR_604e\t0x00090000\n#define PVR_604r\t0x000A0000\n#define PVR_620\t\t0x00140000\n#define PVR_740\t\t0x00080000\n#define PVR_750\t\tPVR_740\n#define PVR_740P\t0x10080000\n#define PVR_750P\tPVR_740P\n#define PVR_7400\t0x000C0000\n#define PVR_7410\t0x800C0000\n#define PVR_7450\t0x80000000\n#define PVR_8540\t0x80200000\n#define PVR_8560\t0x80200000\n/*\n * For the 8xx processors, all of them report the same PVR family for\n * the PowerPC core. The various versions of these processors must be\n * differentiated by the version number in the Communication Processor\n * Module (CPM).\n */\n#define PVR_821\t\t0x00500000\n#define PVR_823\t\tPVR_821\n#define PVR_850\t\tPVR_821\n#define PVR_860\t\tPVR_821\n#define PVR_8240\t0x00810100\n#define PVR_8245\t0x80811014\n#define PVR_8260\tPVR_8240\n\n/* 476 Simulator seems to currently have the PVR of the 602... */\n#define PVR_476_ISS\t0x00052000\n\n/* 64-bit processors */\n/* XXX the prefix should be PVR_, we'll do a global sweep to fix it one day */\n#define PV_NORTHSTAR\t0x0033\n#define PV_PULSAR\t0x0034\n#define PV_POWER4\t0x0035\n#define PV_ICESTAR\t0x0036\n#define PV_SSTAR\t0x0037\n#define PV_POWER4p\t0x0038\n#define PV_970\t\t0x0039\n#define PV_POWER5\t0x003A\n#define PV_POWER5p\t0x003B\n#define PV_POWER7\t0x003F\n#define PV_970FX\t0x003C\n#define PV_630\t\t0x0040\n#define PV_630p\t0x0041\n#define PV_970MP\t0x0044\n#define PV_970GX\t0x0045\n#define PV_BE\t\t0x0070\n#define PV_PA6T\t\t0x0090\n\n/* Macros for setting and retrieving special purpose registers */\n#ifndef __ASSEMBLY__\n#define mfmsr()\t\t({unsigned long rval; \\\n\t\t\tasm volatile(\"mfmsr %0\" : \"=r\" (rval)); rval;})\n#ifdef CONFIG_PPC_BOOK3S_64\n#define __mtmsrd(v, l)\tasm volatile(\"mtmsrd %0,\" __stringify(l) \\\n\t\t\t\t     : : \"r\" (v) : \"memory\")\n#define mtmsrd(v)\t__mtmsrd((v), 0)\n#define mtmsr(v)\tmtmsrd(v)\n#else\n#define mtmsr(v)\tasm volatile(\"mtmsr %0\" : : \"r\" (v) : \"memory\")\n#endif\n\n#define mfspr(rn)\t({unsigned long rval; \\\n\t\t\tasm volatile(\"mfspr %0,\" __stringify(rn) \\\n\t\t\t\t: \"=r\" (rval)); rval;})\n#define mtspr(rn, v)\tasm volatile(\"mtspr \" __stringify(rn) \",%0\" : : \"r\" (v)\\\n\t\t\t\t     : \"memory\")\n\n#ifdef __powerpc64__\n#ifdef CONFIG_PPC_CELL\n#define mftb()\t\t({unsigned long rval;\t\t\t\t\\\n\t\t\tasm volatile(\t\t\t\t\t\\\n\t\t\t\t\"90:\tmftb %0;\\n\"\t\t\t\\\n\t\t\t\t\"97:\tcmpwi %0,0;\\n\"\t\t\t\\\n\t\t\t\t\"\tbeq- 90b;\\n\"\t\t\t\\\n\t\t\t\t\"99:\\n\"\t\t\t\t\t\\\n\t\t\t\t\".section __ftr_fixup,\\\"a\\\"\\n\"\t\t\\\n\t\t\t\t\".align 3\\n\"\t\t\t\t\\\n\t\t\t\t\"98:\\n\"\t\t\t\t\t\\\n\t\t\t\t\"\t.llong %1\\n\"\t\t\t\\\n\t\t\t\t\"\t.llong %1\\n\"\t\t\t\\\n\t\t\t\t\"\t.llong 97b-98b\\n\"\t\t\\\n\t\t\t\t\"\t.llong 99b-98b\\n\"\t\t\\\n\t\t\t\t\"\t.llong 0\\n\"\t\t\t\\\n\t\t\t\t\"\t.llong 0\\n\"\t\t\t\\\n\t\t\t\t\".previous\"\t\t\t\t\\\n\t\t\t: \"=r\" (rval) : \"i\" (CPU_FTR_CELL_TB_BUG)); rval;})\n#else\n#define mftb()\t\t({unsigned long rval;\t\\\n\t\t\tasm volatile(\"mftb %0\" : \"=r\" (rval)); rval;})\n#endif /* !CONFIG_PPC_CELL */\n\n#else /* __powerpc64__ */\n\n#define mftbl()\t\t({unsigned long rval;\t\\\n\t\t\tasm volatile(\"mftbl %0\" : \"=r\" (rval)); rval;})\n#define mftbu()\t\t({unsigned long rval;\t\\\n\t\t\tasm volatile(\"mftbu %0\" : \"=r\" (rval)); rval;})\n#endif /* !__powerpc64__ */\n\n#define mttbl(v)\tasm volatile(\"mttbl %0\":: \"r\"(v))\n#define mttbu(v)\tasm volatile(\"mttbu %0\":: \"r\"(v))\n\n#ifdef CONFIG_PPC32\n#define mfsrin(v)\t({unsigned int rval; \\\n\t\t\tasm volatile(\"mfsrin %0,%1\" : \"=r\" (rval) : \"r\" (v)); \\\n\t\t\t\t\trval;})\n#endif\n\n#define proc_trap()\tasm volatile(\"trap\")\n\n#ifdef CONFIG_PPC64\n\nextern void ppc64_runlatch_on(void);\nextern void __ppc64_runlatch_off(void);\n\n#define ppc64_runlatch_off()\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tif (cpu_has_feature(CPU_FTR_CTRL) &&\t\t\\\n\t\t    test_thread_flag(TIF_RUNLATCH))\t\t\\\n\t\t\t__ppc64_runlatch_off();\t\t\t\\\n\t} while (0)\n\nextern unsigned long scom970_read(unsigned int address);\nextern void scom970_write(unsigned int address, unsigned long value);\n\n#else\n#define ppc64_runlatch_on()\n#define ppc64_runlatch_off()\n\n#endif /* CONFIG_PPC64 */\n\n#define __get_SP()\t({unsigned long sp; \\\n\t\t\tasm volatile(\"mr %0,1\": \"=r\" (sp)); sp;})\n\nstruct pt_regs;\n\nextern void ppc_save_regs(struct pt_regs *regs);\n\n#endif /* __ASSEMBLY__ */\n#endif /* __KERNEL__ */\n#endif /* _ASM_POWERPC_REG_H */\n", "/*\n * Performance event support - powerpc architecture code\n *\n * Copyright 2008-2009 Paul Mackerras, IBM Corporation.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <asm/reg.h>\n#include <asm/pmc.h>\n#include <asm/machdep.h>\n#include <asm/firmware.h>\n#include <asm/ptrace.h>\n\nstruct cpu_hw_events {\n\tint n_events;\n\tint n_percpu;\n\tint disabled;\n\tint n_added;\n\tint n_limited;\n\tu8  pmcs_enabled;\n\tstruct perf_event *event[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int flags[MAX_HWEVENTS];\n\tunsigned long mmcr[3];\n\tstruct perf_event *limited_counter[MAX_LIMITED_HWCOUNTERS];\n\tu8  limited_hwidx[MAX_LIMITED_HWCOUNTERS];\n\tu64 alternatives[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long amasks[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long avalues[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\n\tunsigned int group_flag;\n\tint n_txn_start;\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\nstruct power_pmu *ppmu;\n\n/*\n * Normally, to ignore kernel events we set the FCS (freeze counters\n * in supervisor mode) bit in MMCR0, but if the kernel runs with the\n * hypervisor bit set in the MSR, or if we are running on a processor\n * where the hypervisor bit is forced to 1 (as on Apple G5 processors),\n * then we need to use the FCHV bit to ignore kernel events.\n */\nstatic unsigned int freeze_events_kernel = MMCR0_FCS;\n\n/*\n * 32-bit doesn't have MMCRA but does have an MMCR2,\n * and a few other names are different.\n */\n#ifdef CONFIG_PPC32\n\n#define MMCR0_FCHV\t\t0\n#define MMCR0_PMCjCE\t\tMMCR0_PMCnCE\n\n#define SPRN_MMCRA\t\tSPRN_MMCR2\n#define MMCRA_SAMPLE_ENABLE\t0\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp) { }\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_read_regs(struct pt_regs *regs) { }\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_PPC32 */\n\n/*\n * Things that are specific to 64-bit implementations.\n */\n#ifdef CONFIG_PPC64\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\n\tif ((mmcra & MMCRA_SAMPLE_ENABLE) && !(ppmu->flags & PPMU_ALT_SIPR)) {\n\t\tunsigned long slot = (mmcra & MMCRA_SLOT) >> MMCRA_SLOT_SHIFT;\n\t\tif (slot > 1)\n\t\t\treturn 4 * (slot - 1);\n\t}\n\treturn 0;\n}\n\n/*\n * The user wants a data address recorded.\n * If we're not doing instruction sampling, give them the SDAR\n * (sampled data address).  If we are doing instruction sampling, then\n * only give them the SDAR if it corresponds to the instruction\n * pointed to by SIAR; this is indicated by the [POWER6_]MMCRA_SDSYNC\n * bit in MMCRA.\n */\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tunsigned long sdsync = (ppmu->flags & PPMU_ALT_SIPR) ?\n\t\tPOWER6_MMCRA_SDSYNC : MMCRA_SDSYNC;\n\n\tif (!(mmcra & MMCRA_SAMPLE_ENABLE) || (mmcra & sdsync))\n\t\t*addrp = mfspr(SPRN_SDAR);\n}\n\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tunsigned long sihv = MMCRA_SIHV;\n\tunsigned long sipr = MMCRA_SIPR;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn 0;\t/* not a PMU interrupt */\n\n\tif (ppmu->flags & PPMU_ALT_SIPR) {\n\t\tsihv = POWER6_MMCRA_SIHV;\n\t\tsipr = POWER6_MMCRA_SIPR;\n\t}\n\n\t/* PR has priority over HV, so order below is important */\n\tif (mmcra & sipr)\n\t\treturn PERF_RECORD_MISC_USER;\n\tif ((mmcra & sihv) && (freeze_events_kernel != MMCR0_FCHV))\n\t\treturn PERF_RECORD_MISC_HYPERVISOR;\n\treturn PERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Overload regs->dsisr to store MMCRA so we only need to read it once\n * on each interrupt.\n */\nstatic inline void perf_read_regs(struct pt_regs *regs)\n{\n\tregs->dsisr = mfspr(SPRN_MMCRA);\n}\n\n/*\n * If interrupts were soft-disabled when a PMU interrupt occurs, treat\n * it as an NMI.\n */\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n\treturn !regs->softe;\n}\n\n#endif /* CONFIG_PPC64 */\n\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\nvoid perf_event_print_debug(void)\n{\n}\n\n/*\n * Read one performance monitor counter (PMC).\n */\nstatic unsigned long read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tswitch (idx) {\n\tcase 1:\n\t\tval = mfspr(SPRN_PMC1);\n\t\tbreak;\n\tcase 2:\n\t\tval = mfspr(SPRN_PMC2);\n\t\tbreak;\n\tcase 3:\n\t\tval = mfspr(SPRN_PMC3);\n\t\tbreak;\n\tcase 4:\n\t\tval = mfspr(SPRN_PMC4);\n\t\tbreak;\n\tcase 5:\n\t\tval = mfspr(SPRN_PMC5);\n\t\tbreak;\n\tcase 6:\n\t\tval = mfspr(SPRN_PMC6);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tval = mfspr(SPRN_PMC7);\n\t\tbreak;\n\tcase 8:\n\t\tval = mfspr(SPRN_PMC8);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to read PMC%d\\n\", idx);\n\t\tval = 0;\n\t}\n\treturn val;\n}\n\n/*\n * Write one PMC.\n */\nstatic void write_pmc(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 1:\n\t\tmtspr(SPRN_PMC1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtspr(SPRN_PMC2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtspr(SPRN_PMC3, val);\n\t\tbreak;\n\tcase 4:\n\t\tmtspr(SPRN_PMC4, val);\n\t\tbreak;\n\tcase 5:\n\t\tmtspr(SPRN_PMC5, val);\n\t\tbreak;\n\tcase 6:\n\t\tmtspr(SPRN_PMC6, val);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tmtspr(SPRN_PMC7, val);\n\t\tbreak;\n\tcase 8:\n\t\tmtspr(SPRN_PMC8, val);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMC%d\\n\", idx);\n\t}\n}\n\n/*\n * Check if a set of events can all go on the PMU at once.\n * If they can't, this will look at alternative codes for the events\n * and see if any combination of alternative codes is feasible.\n * The feasible set is returned in event_id[].\n */\nstatic int power_check_constraints(struct cpu_hw_events *cpuhw,\n\t\t\t\t   u64 event_id[], unsigned int cflags[],\n\t\t\t\t   int n_ev)\n{\n\tunsigned long mask, value, nv;\n\tunsigned long smasks[MAX_HWEVENTS], svalues[MAX_HWEVENTS];\n\tint n_alt[MAX_HWEVENTS], choice[MAX_HWEVENTS];\n\tint i, j;\n\tunsigned long addf = ppmu->add_fields;\n\tunsigned long tadd = ppmu->test_adder;\n\n\tif (n_ev > ppmu->n_counter)\n\t\treturn -1;\n\n\t/* First see if the events will go on as-is */\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tif ((cflags[i] & PPMU_LIMITED_PMC_REQD)\n\t\t    && !ppmu->limited_pmc_event(event_id[i])) {\n\t\t\tppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t       cpuhw->alternatives[i]);\n\t\t\tevent_id[i] = cpuhw->alternatives[i][0];\n\t\t}\n\t\tif (ppmu->get_constraint(event_id[i], &cpuhw->amasks[i][0],\n\t\t\t\t\t &cpuhw->avalues[i][0]))\n\t\t\treturn -1;\n\t}\n\tvalue = mask = 0;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tnv = (value | cpuhw->avalues[i][0]) +\n\t\t\t(value & cpuhw->avalues[i][0] & addf);\n\t\tif ((((nv + tadd) ^ value) & mask) != 0 ||\n\t\t    (((nv + tadd) ^ cpuhw->avalues[i][0]) &\n\t\t     cpuhw->amasks[i][0]) != 0)\n\t\t\tbreak;\n\t\tvalue = nv;\n\t\tmask |= cpuhw->amasks[i][0];\n\t}\n\tif (i == n_ev)\n\t\treturn 0;\t/* all OK */\n\n\t/* doesn't work, gather alternatives... */\n\tif (!ppmu->get_alternatives)\n\t\treturn -1;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tchoice[i] = 0;\n\t\tn_alt[i] = ppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t\t  cpuhw->alternatives[i]);\n\t\tfor (j = 1; j < n_alt[i]; ++j)\n\t\t\tppmu->get_constraint(cpuhw->alternatives[i][j],\n\t\t\t\t\t     &cpuhw->amasks[i][j],\n\t\t\t\t\t     &cpuhw->avalues[i][j]);\n\t}\n\n\t/* enumerate all possibilities and see if any will work */\n\ti = 0;\n\tj = -1;\n\tvalue = mask = nv = 0;\n\twhile (i < n_ev) {\n\t\tif (j >= 0) {\n\t\t\t/* we're backtracking, restore context */\n\t\t\tvalue = svalues[i];\n\t\t\tmask = smasks[i];\n\t\t\tj = choice[i];\n\t\t}\n\t\t/*\n\t\t * See if any alternative k for event_id i,\n\t\t * where k > j, will satisfy the constraints.\n\t\t */\n\t\twhile (++j < n_alt[i]) {\n\t\t\tnv = (value | cpuhw->avalues[i][j]) +\n\t\t\t\t(value & cpuhw->avalues[i][j] & addf);\n\t\t\tif ((((nv + tadd) ^ value) & mask) == 0 &&\n\t\t\t    (((nv + tadd) ^ cpuhw->avalues[i][j])\n\t\t\t     & cpuhw->amasks[i][j]) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j >= n_alt[i]) {\n\t\t\t/*\n\t\t\t * No feasible alternative, backtrack\n\t\t\t * to event_id i-1 and continue enumerating its\n\t\t\t * alternatives from where we got up to.\n\t\t\t */\n\t\t\tif (--i < 0)\n\t\t\t\treturn -1;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Found a feasible alternative for event_id i,\n\t\t\t * remember where we got up to with this event_id,\n\t\t\t * go on to the next event_id, and start with\n\t\t\t * the first alternative for it.\n\t\t\t */\n\t\t\tchoice[i] = j;\n\t\t\tsvalues[i] = value;\n\t\t\tsmasks[i] = mask;\n\t\t\tvalue = nv;\n\t\t\tmask |= cpuhw->amasks[i][j];\n\t\t\t++i;\n\t\t\tj = -1;\n\t\t}\n\t}\n\n\t/* OK, we have a feasible combination, tell the caller the solution */\n\tfor (i = 0; i < n_ev; ++i)\n\t\tevent_id[i] = cpuhw->alternatives[i][choice[i]];\n\treturn 0;\n}\n\n/*\n * Check if newly-added events have consistent settings for\n * exclude_{user,kernel,hv} with each other and any previously\n * added events.\n */\nstatic int check_excludes(struct perf_event **ctrs, unsigned int cflags[],\n\t\t\t  int n_prev, int n_new)\n{\n\tint eu = 0, ek = 0, eh = 0;\n\tint i, n, first;\n\tstruct perf_event *event;\n\n\tn = n_prev + n_new;\n\tif (n <= 1)\n\t\treturn 0;\n\n\tfirst = 1;\n\tfor (i = 0; i < n; ++i) {\n\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK) {\n\t\t\tcflags[i] &= ~PPMU_LIMITED_PMC_REQD;\n\t\t\tcontinue;\n\t\t}\n\t\tevent = ctrs[i];\n\t\tif (first) {\n\t\t\teu = event->attr.exclude_user;\n\t\t\tek = event->attr.exclude_kernel;\n\t\t\teh = event->attr.exclude_hv;\n\t\t\tfirst = 0;\n\t\t} else if (event->attr.exclude_user != eu ||\n\t\t\t   event->attr.exclude_kernel != ek ||\n\t\t\t   event->attr.exclude_hv != eh) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (eu || ek || eh)\n\t\tfor (i = 0; i < n; ++i)\n\t\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK)\n\t\t\t\tcflags[i] |= PPMU_LIMITED_PMC_REQD;\n\n\treturn 0;\n}\n\nstatic void power_pmu_read(struct perf_event *event)\n{\n\ts64 val, delta, prev;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tif (!event->hw.idx)\n\t\treturn;\n\t/*\n\t * Performance monitor interrupts come even when interrupts\n\t * are soft-disabled, as long as interrupts are hard-enabled.\n\t * Therefore we treat them like NMIs.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tbarrier();\n\t\tval = read_pmc(event->hw.idx);\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\n\n\t/* The counters are only 32 bits wide */\n\tdelta = (val - prev) & 0xfffffffful;\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &event->hw.period_left);\n}\n\n/*\n * On some machines, PMC5 and PMC6 can't be written, don't respect\n * the freeze conditions, and don't generate interrupts.  This tells\n * us if `event' is using such a PMC.\n */\nstatic int is_limited_pmc(int pmcnum)\n{\n\treturn (ppmu->flags & PPMU_LIMITED_PMC5_6)\n\t\t&& (pmcnum == 5 || pmcnum == 6);\n}\n\nstatic void freeze_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t    unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev, delta;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tif (!event->hw.idx)\n\t\t\tcontinue;\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tevent->hw.idx = 0;\n\t\tdelta = (val - prev) & 0xfffffffful;\n\t\tlocal64_add(delta, &event->count);\n\t}\n}\n\nstatic void thaw_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t  unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tevent->hw.idx = cpuhw->limited_hwidx[i];\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tperf_event_update_userpage(event);\n\t}\n}\n\n/*\n * Since limited events don't respect the freeze conditions, we\n * have to read them immediately after freezing or unfreezing the\n * other events.  We try to keep the values from the limited\n * events as consistent as possible by keeping the delay (in\n * cycles and instructions) between freezing/unfreezing and reading\n * the limited events as small and consistent as possible.\n * Therefore, if any limited events are in use, we read them\n * both, and always in the same order, to minimize variability,\n * and do it inside the same asm that writes MMCR0.\n */\nstatic void write_mmcr0(struct cpu_hw_events *cpuhw, unsigned long mmcr0)\n{\n\tunsigned long pmc5, pmc6;\n\n\tif (!cpuhw->n_limited) {\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n\t\treturn;\n\t}\n\n\t/*\n\t * Write MMCR0, then read PMC5 and PMC6 immediately.\n\t * To ensure we don't get a performance monitor interrupt\n\t * between writing MMCR0 and freezing/thawing the limited\n\t * events, we first write MMCR0 with the event overflow\n\t * interrupt enable bits turned off.\n\t */\n\tasm volatile(\"mtspr %3,%2; mfspr %0,%4; mfspr %1,%5\"\n\t\t     : \"=&r\" (pmc5), \"=&r\" (pmc6)\n\t\t     : \"r\" (mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE)),\n\t\t       \"i\" (SPRN_MMCR0),\n\t\t       \"i\" (SPRN_PMC5), \"i\" (SPRN_PMC6));\n\n\tif (mmcr0 & MMCR0_FC)\n\t\tfreeze_limited_counters(cpuhw, pmc5, pmc6);\n\telse\n\t\tthaw_limited_counters(cpuhw, pmc5, pmc6);\n\n\t/*\n\t * Write the full MMCR0 including the event overflow interrupt\n\t * enable bits, if necessary.\n\t */\n\tif (mmcr0 & (MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n}\n\n/*\n * Disable all events to prevent PMU interrupts and to allow\n * events to be added or removed.\n */\nstatic void power_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tif (!cpuhw->disabled) {\n\t\tcpuhw->disabled = 1;\n\t\tcpuhw->n_added = 0;\n\n\t\t/*\n\t\t * Check if we ever enabled the PMU on this cpu.\n\t\t */\n\t\tif (!cpuhw->pmcs_enabled) {\n\t\t\tppc_enable_pmcs();\n\t\t\tcpuhw->pmcs_enabled = 1;\n\t\t}\n\n\t\t/*\n\t\t * Disable instruction sampling if it was enabled\n\t\t */\n\t\tif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\n\t\t\tmtspr(SPRN_MMCRA,\n\t\t\t      cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\t\t\tmb();\n\t\t}\n\n\t\t/*\n\t\t * Set the 'freeze counters' bit.\n\t\t * The barrier is to make sure the mtspr has been\n\t\t * executed and the PMU has frozen the events\n\t\t * before we return.\n\t\t */\n\t\twrite_mmcr0(cpuhw, mfspr(SPRN_MMCR0) | MMCR0_FC);\n\t\tmb();\n\t}\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Re-enable all events if disable == 0.\n * If we were previously disabled and events were added, then\n * put the new config on the PMU.\n */\nstatic void power_pmu_enable(struct pmu *pmu)\n{\n\tstruct perf_event *event;\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tlong i;\n\tunsigned long val;\n\ts64 left;\n\tunsigned int hwc_index[MAX_HWEVENTS];\n\tint n_lim;\n\tint idx;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tif (!cpuhw->disabled) {\n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\n\tcpuhw->disabled = 0;\n\n\t/*\n\t * If we didn't change anything, or only removed events,\n\t * no need to recalculate MMCR* settings and reset the PMCs.\n\t * Just reenable the PMU with the current MMCR* settings\n\t * (possibly updated for removal of events).\n\t */\n\tif (!cpuhw->n_added) {\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\t\tmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\n\t\tif (cpuhw->n_events == 0)\n\t\t\tppc_set_pmu_inuse(0);\n\t\tgoto out_enable;\n\t}\n\n\t/*\n\t * Compute MMCR* values for the new set of events\n\t */\n\tif (ppmu->compute_mmcr(cpuhw->events, cpuhw->n_events, hwc_index,\n\t\t\t       cpuhw->mmcr)) {\n\t\t/* shouldn't ever get here */\n\t\tprintk(KERN_ERR \"oops compute_mmcr failed\\n\");\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Add in MMCR0 freeze bits corresponding to the\n\t * attr.exclude_* bits for the first event.\n\t * We have already checked that all events have the\n\t * same values for these bits as the first event.\n\t */\n\tevent = cpuhw->event[0];\n\tif (event->attr.exclude_user)\n\t\tcpuhw->mmcr[0] |= MMCR0_FCP;\n\tif (event->attr.exclude_kernel)\n\t\tcpuhw->mmcr[0] |= freeze_events_kernel;\n\tif (event->attr.exclude_hv)\n\t\tcpuhw->mmcr[0] |= MMCR0_FCHV;\n\n\t/*\n\t * Write the new configuration to MMCR* with the freeze\n\t * bit set and set the hardware events to their initial values.\n\t * Then unfreeze the events.\n\t */\n\tppc_set_pmu_inuse(1);\n\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\tmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\n\tmtspr(SPRN_MMCR0, (cpuhw->mmcr[0] & ~(MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\t\t\t| MMCR0_FC);\n\n\t/*\n\t * Read off any pre-existing events that need to move\n\t * to another PMC.\n\t */\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx && event->hw.idx != hwc_index[i] + 1) {\n\t\t\tpower_pmu_read(event);\n\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\tevent->hw.idx = 0;\n\t\t}\n\t}\n\n\t/*\n\t * Initialize the PMCs for all the new and moved events.\n\t */\n\tcpuhw->n_limited = n_lim = 0;\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx)\n\t\t\tcontinue;\n\t\tidx = hwc_index[i] + 1;\n\t\tif (is_limited_pmc(idx)) {\n\t\t\tcpuhw->limited_counter[n_lim] = event;\n\t\t\tcpuhw->limited_hwidx[n_lim] = idx;\n\t\t\t++n_lim;\n\t\t\tcontinue;\n\t\t}\n\t\tval = 0;\n\t\tif (event->hw.sample_period) {\n\t\t\tleft = local64_read(&event->hw.period_left);\n\t\t\tif (left < 0x80000000L)\n\t\t\t\tval = 0x80000000L - left;\n\t\t}\n\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tevent->hw.idx = idx;\n\t\tif (event->hw.state & PERF_HES_STOPPED)\n\t\t\tval = 0;\n\t\twrite_pmc(idx, val);\n\t\tperf_event_update_userpage(event);\n\t}\n\tcpuhw->n_limited = n_lim;\n\tcpuhw->mmcr[0] |= MMCR0_PMXE | MMCR0_FCECE;\n\n out_enable:\n\tmb();\n\twrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\n\n\t/*\n\t * Enable instruction sampling if necessary\n\t */\n\tif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\n\t\tmb();\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2]);\n\t}\n\n out:\n\tlocal_irq_restore(flags);\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *ctrs[], u64 *events,\n\t\t\t  unsigned int *flags)\n{\n\tint n = 0;\n\tstruct perf_event *event;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tctrs[n] = group;\n\t\tflags[n] = group->hw.event_base;\n\t\tevents[n++] = group->hw.config;\n\t}\n\tlist_for_each_entry(event, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(event) &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tctrs[n] = event;\n\t\t\tflags[n] = event->hw.event_base;\n\t\t\tevents[n++] = event->hw.config;\n\t\t}\n\t}\n\treturn n;\n}\n\n/*\n * Add a event to the PMU.\n * If all events are not already frozen, then we disable and\n * re-enable the PMU in order to get hw_perf_enable to do the\n * actual work of reconfiguring the PMU.\n */\nstatic int power_pmu_add(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tint n0;\n\tint ret = -EAGAIN;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\t/*\n\t * Add the event to the list (if there is room)\n\t * and check whether the total set is still feasible.\n\t */\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tn0 = cpuhw->n_events;\n\tif (n0 >= ppmu->n_counter)\n\t\tgoto out;\n\tcpuhw->event[n0] = event;\n\tcpuhw->events[n0] = event->hw.config;\n\tcpuhw->flags[n0] = event->hw.event_base;\n\n\tif (!(ef_flags & PERF_EF_START))\n\t\tevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be peformed\n\t * at commit time(->commit_txn) as a whole\n\t */\n\tif (cpuhw->group_flag & PERF_EVENT_TXN)\n\t\tgoto nocheck;\n\n\tif (check_excludes(cpuhw->event, cpuhw->flags, n0, 1))\n\t\tgoto out;\n\tif (power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n0 + 1))\n\t\tgoto out;\n\tevent->hw.config = cpuhw->events[n0];\n\nnocheck:\n\t++cpuhw->n_events;\n\t++cpuhw->n_added;\n\n\tret = 0;\n out:\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/*\n * Remove a event from the PMU.\n */\nstatic void power_pmu_del(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tif (event == cpuhw->event[i]) {\n\t\t\twhile (++i < cpuhw->n_events) {\n\t\t\t\tcpuhw->event[i-1] = cpuhw->event[i];\n\t\t\t\tcpuhw->events[i-1] = cpuhw->events[i];\n\t\t\t\tcpuhw->flags[i-1] = cpuhw->flags[i];\n\t\t\t}\n\t\t\t--cpuhw->n_events;\n\t\t\tppmu->disable_pmc(event->hw.idx - 1, cpuhw->mmcr);\n\t\t\tif (event->hw.idx) {\n\t\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\t\tevent->hw.idx = 0;\n\t\t\t}\n\t\t\tperf_event_update_userpage(event);\n\t\t\tbreak;\n\t\t}\n\t}\n\tfor (i = 0; i < cpuhw->n_limited; ++i)\n\t\tif (event == cpuhw->limited_counter[i])\n\t\t\tbreak;\n\tif (i < cpuhw->n_limited) {\n\t\twhile (++i < cpuhw->n_limited) {\n\t\t\tcpuhw->limited_counter[i-1] = cpuhw->limited_counter[i];\n\t\t\tcpuhw->limited_hwidx[i-1] = cpuhw->limited_hwidx[i];\n\t\t}\n\t\t--cpuhw->n_limited;\n\t}\n\tif (cpuhw->n_events == 0) {\n\t\t/* disable exceptions if no events are running */\n\t\tcpuhw->mmcr[0] &= ~(MMCR0_PMXE | MMCR0_FCECE);\n\t}\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * POWER-PMU does not support disabling individual counters, hence\n * program their cycle counter to their max value and ignore the interrupts.\n */\n\nstatic void power_pmu_start(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\ts64 left;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (!(event->hw.state & PERF_HES_STOPPED))\n\t\treturn;\n\n\tif (ef_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tevent->hw.state = 0;\n\tleft = local64_read(&event->hw.period_left);\n\twrite_pmc(event->hw.idx, left);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void power_pmu_stop(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\twrite_pmc(event->hw.idx, 0);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n */\nvoid power_pmu_start_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tperf_pmu_disable(pmu);\n\tcpuhw->group_flag |= PERF_EVENT_TXN;\n\tcpuhw->n_txn_start = cpuhw->n_events;\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nvoid power_pmu_cancel_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nint power_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i, n;\n\n\tif (!ppmu)\n\t\treturn -EAGAIN;\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tn = cpuhw->n_events;\n\tif (check_excludes(cpuhw->event, cpuhw->flags, 0, n))\n\t\treturn -EAGAIN;\n\ti = power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n);\n\tif (i < 0)\n\t\treturn -EAGAIN;\n\n\tfor (i = cpuhw->n_txn_start; i < n; ++i)\n\t\tcpuhw->event[i]->hw.config = cpuhw->events[i];\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\n/*\n * Return 1 if we might be able to put event on a limited PMC,\n * or 0 if not.\n * A event can only go on a limited PMC if it counts something\n * that a limited PMC can count, doesn't require interrupts, and\n * doesn't exclude any processor mode.\n */\nstatic int can_go_on_limited_pmc(struct perf_event *event, u64 ev,\n\t\t\t\t unsigned int flags)\n{\n\tint n;\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\n\tif (event->attr.exclude_user\n\t    || event->attr.exclude_kernel\n\t    || event->attr.exclude_hv\n\t    || event->attr.sample_period)\n\t\treturn 0;\n\n\tif (ppmu->limited_pmc_event(ev))\n\t\treturn 1;\n\n\t/*\n\t * The requested event_id isn't on a limited PMC already;\n\t * see if any alternative code goes on a limited PMC.\n\t */\n\tif (!ppmu->get_alternatives)\n\t\treturn 0;\n\n\tflags |= PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD;\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\n\treturn n > 0;\n}\n\n/*\n * Find an alternative event_id that goes on a normal PMC, if possible,\n * and return the event_id code, or 0 if there is no such alternative.\n * (Note: event_id code 0 is \"don't count\" on all machines.)\n */\nstatic u64 normal_pmc_alternative(u64 ev, unsigned long flags)\n{\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\tint n;\n\n\tflags &= ~(PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD);\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\tif (!n)\n\t\treturn 0;\n\treturn alt[0];\n}\n\n/* Number of perf_events counting hardware events */\nstatic atomic_t num_events;\n/* Used to avoid races in calling reserve/release_pmc_hardware */\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n/*\n * Release the PMU if this is the last perf_event.\n */\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (!atomic_add_unless(&num_events, -1, 1)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_dec_return(&num_events) == 0)\n\t\t\trelease_pmc_hardware();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n/*\n * Translate a generic cache event_id config to a raw event_id code.\n */\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\n{\n\tunsigned long type, op, result;\n\tint ev;\n\n\tif (!ppmu->cache_events)\n\t\treturn -EINVAL;\n\n\t/* unpack config */\n\ttype = config & 0xff;\n\top = (config >> 8) & 0xff;\n\tresult = (config >> 16) & 0xff;\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX ||\n\t    op >= PERF_COUNT_HW_CACHE_OP_MAX ||\n\t    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tev = (*ppmu->cache_events)[type][op][result];\n\tif (ev == 0)\n\t\treturn -EOPNOTSUPP;\n\tif (ev == -1)\n\t\treturn -EINVAL;\n\t*eventp = ev;\n\treturn 0;\n}\n\nstatic int power_pmu_event_init(struct perf_event *event)\n{\n\tu64 ev;\n\tunsigned long flags;\n\tstruct perf_event *ctrs[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int cflags[MAX_HWEVENTS];\n\tint n;\n\tint err;\n\tstruct cpu_hw_events *cpuhw;\n\n\tif (!ppmu)\n\t\treturn -ENOENT;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tev = event->attr.config;\n\t\tif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\n\t\t\treturn -EOPNOTSUPP;\n\t\tev = ppmu->generic_events[ev];\n\t\tbreak;\n\tcase PERF_TYPE_HW_CACHE:\n\t\terr = hw_perf_cache_event(event->attr.config, &ev);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tcase PERF_TYPE_RAW:\n\t\tev = event->attr.config;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tevent->hw.config_base = ev;\n\tevent->hw.idx = 0;\n\n\t/*\n\t * If we are not running on a hypervisor, force the\n\t * exclude_hv bit to 0 so that we don't care what\n\t * the user set it to.\n\t */\n\tif (!firmware_has_feature(FW_FEATURE_LPAR))\n\t\tevent->attr.exclude_hv = 0;\n\n\t/*\n\t * If this is a per-task event, then we can use\n\t * PM_RUN_* events interchangeably with their non RUN_*\n\t * equivalents, e.g. PM_RUN_CYC instead of PM_CYC.\n\t * XXX we should check if the task is an idle task.\n\t */\n\tflags = 0;\n\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\tflags |= PPMU_ONLY_COUNT_RUN;\n\n\t/*\n\t * If this machine has limited events, check whether this\n\t * event_id could go on a limited event.\n\t */\n\tif (ppmu->flags & PPMU_LIMITED_PMC5_6) {\n\t\tif (can_go_on_limited_pmc(event, ev, flags)) {\n\t\t\tflags |= PPMU_LIMITED_PMC_OK;\n\t\t} else if (ppmu->limited_pmc_event(ev)) {\n\t\t\t/*\n\t\t\t * The requested event_id is on a limited PMC,\n\t\t\t * but we can't use a limited PMC; see if any\n\t\t\t * alternative goes on a normal PMC.\n\t\t\t */\n\t\t\tev = normal_pmc_alternative(ev, flags);\n\t\t\tif (!ev)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/*\n\t * If this is in a group, check if it can go on with all the\n\t * other hardware events in the group.  We assume the event\n\t * hasn't been linked into its leader's sibling list at this point.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader, ppmu->n_counter - 1,\n\t\t\t\t   ctrs, events, cflags);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevents[n] = ev;\n\tctrs[n] = event;\n\tcflags[n] = flags;\n\tif (check_excludes(ctrs, cflags, n, 1))\n\t\treturn -EINVAL;\n\n\tcpuhw = &get_cpu_var(cpu_hw_events);\n\terr = power_check_constraints(cpuhw, events, cflags, n + 1);\n\tput_cpu_var(cpu_hw_events);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tevent->hw.config = events[n];\n\tevent->hw.event_base = cflags[n];\n\tevent->hw.last_period = event->hw.sample_period;\n\tlocal64_set(&event->hw.period_left, event->hw.last_period);\n\n\t/*\n\t * See if we need to reserve the PMU.\n\t * If no events are currently in use, then we have to take a\n\t * mutex to ensure that we don't race with another task doing\n\t * reserve_pmc_hardware or release_pmc_hardware.\n\t */\n\terr = 0;\n\tif (!atomic_inc_not_zero(&num_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&num_events) == 0 &&\n\t\t    reserve_pmc_hardware(perf_event_interrupt))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tatomic_inc(&num_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\tevent->destroy = hw_perf_event_destroy;\n\n\treturn err;\n}\n\nstruct pmu power_pmu = {\n\t.pmu_enable\t= power_pmu_enable,\n\t.pmu_disable\t= power_pmu_disable,\n\t.event_init\t= power_pmu_event_init,\n\t.add\t\t= power_pmu_add,\n\t.del\t\t= power_pmu_del,\n\t.start\t\t= power_pmu_start,\n\t.stop\t\t= power_pmu_stop,\n\t.read\t\t= power_pmu_read,\n\t.start_txn\t= power_pmu_start_txn,\n\t.cancel_txn\t= power_pmu_cancel_txn,\n\t.commit_txn\t= power_pmu_commit_txn,\n};\n\n/*\n * A counter has overflowed; update its count and record\n * things if requested.  Note that interrupts are hard-disabled\n * here so there is no possibility of being interrupted.\n */\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\n\t\t\t       struct pt_regs *regs, int nmi)\n{\n\tu64 period = event->hw.sample_period;\n\ts64 prev, delta, left;\n\tint record = 0;\n\n\tif (event->hw.state & PERF_HES_STOPPED) {\n\t\twrite_pmc(event->hw.idx, 0);\n\t\treturn;\n\t}\n\n\t/* we don't have to worry about interrupts here */\n\tprev = local64_read(&event->hw.prev_count);\n\tdelta = (val - prev) & 0xfffffffful;\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * See if the total period for this event has expired,\n\t * and update for the next period.\n\t */\n\tval = 0;\n\tleft = local64_read(&event->hw.period_left) - delta;\n\tif (period) {\n\t\tif (left <= 0) {\n\t\t\tleft += period;\n\t\t\tif (left <= 0)\n\t\t\t\tleft = period;\n\t\t\trecord = 1;\n\t\t\tevent->hw.last_period = event->hw.sample_period;\n\t\t}\n\t\tif (left < 0x80000000LL)\n\t\t\tval = 0x80000000LL - left;\n\t}\n\n\twrite_pmc(event->hw.idx, val);\n\tlocal64_set(&event->hw.prev_count, val);\n\tlocal64_set(&event->hw.period_left, left);\n\tperf_event_update_userpage(event);\n\n\t/*\n\t * Finally record data if requested.\n\t */\n\tif (record) {\n\t\tstruct perf_sample_data data;\n\n\t\tperf_sample_data_init(&data, ~0ULL);\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_ADDR)\n\t\t\tperf_get_data_addr(regs, &data.addr);\n\n\t\tif (perf_event_overflow(event, nmi, &data, regs))\n\t\t\tpower_pmu_stop(event, 0);\n\t}\n}\n\n/*\n * Called from generic code to get the misc flags (i.e. processor mode)\n * for an event_id.\n */\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tu32 flags = perf_get_misc_flags(regs);\n\n\tif (flags)\n\t\treturn flags;\n\treturn user_mode(regs) ? PERF_RECORD_MISC_USER :\n\t\tPERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Called from generic code to get the instruction pointer\n * for an event_id.\n */\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tunsigned long ip;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn regs->nip;\t/* not a PMU interrupt */\n\n\tip = mfspr(SPRN_SIAR) + perf_ip_adjust(regs);\n\treturn ip;\n}\n\nstatic bool pmc_overflow(unsigned long val)\n{\n\tif ((int)val < 0)\n\t\treturn true;\n\n\t/*\n\t * Events on POWER7 can roll back if a speculative event doesn't\n\t * eventually complete. Unfortunately in some rare cases they will\n\t * raise a performance monitor exception. We need to catch this to\n\t * ensure we reset the PMC. In all cases the PMC will be 256 or less\n\t * cycles from overflow.\n\t *\n\t * We only do this if the first pass fails to find any overflowing\n\t * PMCs because a user might set a period of less than 256 and we\n\t * don't want to mistakenly reset them.\n\t */\n\tif (__is_processor(PV_POWER7) && ((0x80000000 - val) <= 256))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Performance monitor interrupt stuff\n */\nstatic void perf_event_interrupt(struct pt_regs *regs)\n{\n\tint i;\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\tstruct perf_event *event;\n\tunsigned long val;\n\tint found = 0;\n\tint nmi;\n\n\tif (cpuhw->n_limited)\n\t\tfreeze_limited_counters(cpuhw, mfspr(SPRN_PMC5),\n\t\t\t\t\tmfspr(SPRN_PMC6));\n\n\tperf_read_regs(regs);\n\n\tnmi = perf_intr_is_nmi(regs);\n\tif (nmi)\n\t\tnmi_enter();\n\telse\n\t\tirq_enter();\n\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (!event->hw.idx || is_limited_pmc(event->hw.idx))\n\t\t\tcontinue;\n\t\tval = read_pmc(event->hw.idx);\n\t\tif ((int)val < 0) {\n\t\t\t/* event has overflowed */\n\t\t\tfound = 1;\n\t\t\trecord_and_restart(event, val, regs, nmi);\n\t\t}\n\t}\n\n\t/*\n\t * In case we didn't find and reset the event that caused\n\t * the interrupt, scan all events and reset any that are\n\t * negative, to avoid getting continual interrupts.\n\t * Any that we processed in the previous loop will not be negative.\n\t */\n\tif (!found) {\n\t\tfor (i = 0; i < ppmu->n_counter; ++i) {\n\t\t\tif (is_limited_pmc(i + 1))\n\t\t\t\tcontinue;\n\t\t\tval = read_pmc(i + 1);\n\t\t\tif (pmc_overflow(val))\n\t\t\t\twrite_pmc(i + 1, 0);\n\t\t}\n\t}\n\n\t/*\n\t * Reset MMCR0 to its normal value.  This will set PMXE and\n\t * clear FC (freeze counters) and PMAO (perf mon alert occurred)\n\t * and thus allow interrupts to occur again.\n\t * XXX might want to use MSR.PM to keep the events frozen until\n\t * we get back out of this interrupt.\n\t */\n\twrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\n\n\tif (nmi)\n\t\tnmi_exit();\n\telse\n\t\tirq_exit();\n}\n\nstatic void power_pmu_setup(int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tif (!ppmu)\n\t\treturn;\n\tmemset(cpuhw, 0, sizeof(*cpuhw));\n\tcpuhw->mmcr[0] = MMCR0_FC;\n}\n\nstatic int __cpuinit\npower_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)\n{\n\tunsigned int cpu = (long)hcpu;\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_UP_PREPARE:\n\t\tpower_pmu_setup(cpu);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nint register_power_pmu(struct power_pmu *pmu)\n{\n\tif (ppmu)\n\t\treturn -EBUSY;\t\t/* something's already registered */\n\n\tppmu = pmu;\n\tpr_info(\"%s performance monitor hardware support registered\\n\",\n\t\tpmu->name);\n\n#ifdef MSR_HV\n\t/*\n\t * Use FCHV to ignore kernel events if MSR.HV is set.\n\t */\n\tif (mfmsr() & MSR_HV)\n\t\tfreeze_events_kernel = MMCR0_FCHV;\n#endif /* CONFIG_PPC64 */\n\n\tperf_pmu_register(&power_pmu, \"cpu\", PERF_TYPE_RAW);\n\tperf_cpu_notifier(power_pmu_notifier);\n\n\treturn 0;\n}\n"], "filenames": ["arch/powerpc/include/asm/reg.h", "arch/powerpc/kernel/perf_event.c"], "buggy_code_start_loc": [882, 1271], "buggy_code_end_loc": [882, 1320], "fixing_code_start_loc": [883, 1272], "fixing_code_end_loc": [884, 1342], "type": "CWE-189", "message": "Integer overflow in the perf_event_interrupt function in arch/powerpc/kernel/perf_event.c in the Linux kernel before 2.6.39 on powerpc platforms allows local users to cause a denial of service (unhandled performance monitor exception) via vectors that trigger certain outcomes of performance events.", "other": {"cve": {"id": "CVE-2011-4611", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-17T11:00:35.367", "lastModified": "2023-02-13T03:23:59.103", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Integer overflow in the perf_event_interrupt function in arch/powerpc/kernel/perf_event.c in the Linux kernel before 2.6.39 on powerpc platforms allows local users to cause a denial of service (unhandled performance monitor exception) via vectors that trigger certain outcomes of performance events."}, {"lang": "es", "value": "Desbordamiento de entero en la funci\u00f3n perf_event_interrupt en arch/powerpc/kernel/perf_event.c en el n\u00facleo de Linux anteriores a v2.6.39 en plataformas powerpc que permite a usuarios locales causar una denegaci\u00f3n de servicio (excepci\u00f3n de no controlada del monitor de rendimiento) a trav\u00e9s de vectores que activan la salida de ciertos eventos de rendimiento."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-189"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.39", "matchCriteriaId": "176353CE-F17E-4776-AD9F-19014DA75B76"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v2.6/ChangeLog-2.6.39", "source": "secalert@redhat.com", "tags": ["Broken Link"]}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=0837e3242c73566fc1c0196b4ec61779c25ffc93", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/12/15/2", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=767914", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/0837e3242c73566fc1c0196b4ec61779c25ffc93", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0837e3242c73566fc1c0196b4ec61779c25ffc93"}}