{"buggy_code": ["\"\"\"Datasource for LXD, reads /dev/lxd/sock representation of instance data.\n\nNotes:\n * This datasource replaces previous NoCloud datasource for LXD.\n * Older LXD images may not have updates for cloud-init so NoCloud may\n   still be detected on those images.\n * Detect LXD datasource when /dev/lxd/sock is an active socket file.\n * Info on dev-lxd API: https://linuxcontainers.org/lxd/docs/master/dev-lxd\n\"\"\"\n\nimport os\nimport socket\nimport stat\nimport time\nfrom enum import Flag, auto\nfrom json.decoder import JSONDecodeError\nfrom typing import Any, Dict, List, Optional, Union, cast\n\nimport requests\nfrom requests.adapters import HTTPAdapter\n\n# Note: `urllib3` is transitively installed by `requests`.\nfrom urllib3.connection import HTTPConnection\nfrom urllib3.connectionpool import HTTPConnectionPool\n\nfrom cloudinit import log as logging\nfrom cloudinit import sources, subp, url_helper, util\nfrom cloudinit.net import find_fallback_nic\n\nLOG = logging.getLogger(__name__)\n\nLXD_SOCKET_PATH = \"/dev/lxd/sock\"\nLXD_SOCKET_API_VERSION = \"1.0\"\nLXD_URL = \"http://lxd\"\n\n# Config key mappings to alias as top-level instance data keys\nCONFIG_KEY_ALIASES = {\n    \"cloud-init.user-data\": \"user-data\",\n    \"cloud-init.network-config\": \"network-config\",\n    \"cloud-init.vendor-data\": \"vendor-data\",\n    \"user.user-data\": \"user-data\",\n    \"user.network-config\": \"network-config\",\n    \"user.vendor-data\": \"vendor-data\",\n}\n\n\ndef _get_fallback_interface_name() -> str:\n    default_name = \"eth0\"\n    if subp.which(\"systemd-detect-virt\"):\n        try:\n            virt_type, _ = subp.subp([\"systemd-detect-virt\"])\n        except subp.ProcessExecutionError as err:\n            LOG.warning(\n                \"Unable to run systemd-detect-virt: %s.\"\n                \" Rendering default network config.\",\n                err,\n            )\n            return default_name\n        if virt_type.strip() in (\n            \"kvm\",\n            \"qemu\",\n        ):  # instance.type VIRTUAL-MACHINE\n            arch = util.system_info()[\"uname\"][4]\n            if arch == \"ppc64le\":\n                return \"enp0s5\"\n            elif arch == \"s390x\":\n                return \"enc9\"\n            else:\n                return \"enp5s0\"\n    return default_name\n\n\ndef generate_network_config(\n    nics: Optional[List[str]] = None,\n) -> Dict[str, Any]:\n    \"\"\"Return network config V1 dict representing instance network config.\"\"\"\n    # TODO: The original intent of this function was to use the nics retrieved\n    # from LXD's devices endpoint to determine the primary nic and write\n    # that out to network config. However, for LXD VMs, the device name\n    # may differ from the interface name in the VM, so we'll instead rely\n    # on our fallback nic code. Once LXD's devices endpoint grows the\n    # ability to provide a MAC address, we should rely on that information\n    # rather than just the glorified guessing that we're doing here.\n    primary_nic = find_fallback_nic()\n    if primary_nic:\n        LOG.debug(\n            \"LXD datasource generating network from discovered active\"\n            \" device: %s\",\n            primary_nic,\n        )\n    else:\n        primary_nic = _get_fallback_interface_name()\n        LOG.debug(\n            \"LXD datasource generating network from systemd-detect-virt\"\n            \" platform default device: %s\",\n            primary_nic,\n        )\n\n    return {\n        \"version\": 1,\n        \"config\": [\n            {\n                \"type\": \"physical\",\n                \"name\": primary_nic,\n                \"subnets\": [{\"type\": \"dhcp\", \"control\": \"auto\"}],\n            }\n        ],\n    }\n\n\nclass SocketHTTPConnection(HTTPConnection):\n    def __init__(self, socket_path):\n        super().__init__(\"localhost\")\n        self.socket_path = socket_path\n\n    def connect(self):\n        self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        self.sock.connect(self.socket_path)\n\n\nclass SocketConnectionPool(HTTPConnectionPool):\n    def __init__(self, socket_path):\n        self.socket_path = socket_path\n        super().__init__(\"localhost\")\n\n    def _new_conn(self):\n        return SocketHTTPConnection(self.socket_path)\n\n\nclass LXDSocketAdapter(HTTPAdapter):\n    def get_connection(self, url, proxies=None):\n        return SocketConnectionPool(LXD_SOCKET_PATH)\n\n\ndef _raw_instance_data_to_dict(metadata_type: str, metadata_value) -> dict:\n    \"\"\"Convert raw instance data from str, bytes, YAML to dict\n\n    :param metadata_type: string, one of as: meta-data, vendor-data, user-data\n        network-config\n\n    :param metadata_value: str, bytes or dict representing or instance-data.\n\n    :raises: InvalidMetaDataError on invalid instance-data content.\n    \"\"\"\n    if isinstance(metadata_value, dict):\n        return metadata_value\n    if metadata_value is None:\n        return {}\n    try:\n        parsed_metadata = util.load_yaml(metadata_value)\n    except AttributeError as exc:  # not str or bytes\n        raise sources.InvalidMetaDataException(\n            \"Invalid {md_type}. Expected str, bytes or dict but found:\"\n            \" {value}\".format(md_type=metadata_type, value=metadata_value)\n        ) from exc\n    if parsed_metadata is None:\n        raise sources.InvalidMetaDataException(\n            \"Invalid {md_type} format. Expected YAML but found:\"\n            \" {value}\".format(md_type=metadata_type, value=metadata_value)\n        )\n    return parsed_metadata\n\n\nclass DataSourceLXD(sources.DataSource):\n\n    dsname = \"LXD\"\n\n    _network_config: Union[Dict, str] = sources.UNSET\n    _crawled_metadata: Union[Dict, str] = sources.UNSET\n\n    sensitive_metadata_keys = (\n        \"merged_cfg\",\n        \"user.meta-data\",\n        \"user.vendor-data\",\n        \"user.user-data\",\n    )\n\n    skip_hotplug_detect = True\n\n    def _unpickle(self, ci_pkl_version: int) -> None:\n        super()._unpickle(ci_pkl_version)\n        self.skip_hotplug_detect = True\n\n    @staticmethod\n    def ds_detect() -> bool:\n        \"\"\"Check platform environment to report if this datasource may run.\"\"\"\n        return is_platform_viable()\n\n    def _get_data(self) -> bool:\n        \"\"\"Crawl LXD socket API instance data and return True on success\"\"\"\n        self._crawled_metadata = util.log_time(\n            logfunc=LOG.debug,\n            msg=\"Crawl of metadata service\",\n            func=read_metadata,\n        )\n        self.metadata = _raw_instance_data_to_dict(\n            \"meta-data\", self._crawled_metadata.get(\"meta-data\")\n        )\n        config = self._crawled_metadata.get(\"config\", {})\n        user_metadata = config.get(\"user.meta-data\", {})\n        if user_metadata:\n            user_metadata = _raw_instance_data_to_dict(\n                \"user.meta-data\", user_metadata\n            )\n        if not isinstance(self.metadata, dict):\n            self.metadata = util.mergemanydict(\n                [util.load_yaml(self.metadata), user_metadata]\n            )\n        if \"user-data\" in self._crawled_metadata:\n            self.userdata_raw = self._crawled_metadata[\"user-data\"]\n        if \"network-config\" in self._crawled_metadata:\n            self._network_config = _raw_instance_data_to_dict(\n                \"network-config\", self._crawled_metadata[\"network-config\"]\n            )\n        if \"vendor-data\" in self._crawled_metadata:\n            self.vendordata_raw = self._crawled_metadata[\"vendor-data\"]\n        return True\n\n    def _get_subplatform(self) -> str:\n        \"\"\"Return subplatform details for this datasource\"\"\"\n        return \"LXD socket API v. {ver} ({socket})\".format(\n            ver=LXD_SOCKET_API_VERSION, socket=LXD_SOCKET_PATH\n        )\n\n    def check_instance_id(self, sys_cfg) -> str:\n        \"\"\"Return True if instance_id unchanged.\"\"\"\n        response = read_metadata(metadata_keys=MetaDataKeys.META_DATA)\n        md = response.get(\"meta-data\", {})\n        if not isinstance(md, dict):\n            md = util.load_yaml(md)\n        return md.get(\"instance-id\") == self.metadata.get(\"instance-id\")\n\n    @property\n    def network_config(self) -> dict:\n        \"\"\"Network config read from LXD socket config/user.network-config.\n\n        If none is present, then we generate fallback configuration.\n        \"\"\"\n        if self._network_config == sources.UNSET:\n            if self._crawled_metadata == sources.UNSET:\n                self._get_data()\n            if isinstance(self._crawled_metadata, dict):\n                if self._crawled_metadata.get(\"network-config\"):\n                    LOG.debug(\"LXD datasource using provided network config\")\n                    self._network_config = self._crawled_metadata[\n                        \"network-config\"\n                    ]\n                elif self._crawled_metadata.get(\"devices\"):\n                    # If no explicit network config, but we have net devices\n                    # available to us, find the primary and set it up.\n                    devices: List[str] = [\n                        k\n                        for k, v in self._crawled_metadata[\"devices\"].items()\n                        if v[\"type\"] == \"nic\"\n                    ]\n                    self._network_config = generate_network_config(devices)\n        if self._network_config == sources.UNSET:\n            # We know nothing about network, so setup fallback\n            LOG.debug(\n                \"LXD datasource generating network config using fallback.\"\n            )\n            self._network_config = generate_network_config()\n\n        return cast(dict, self._network_config)\n\n\ndef is_platform_viable() -> bool:\n    \"\"\"Return True when this platform appears to have an LXD socket.\"\"\"\n    if os.path.exists(LXD_SOCKET_PATH):\n        return stat.S_ISSOCK(os.lstat(LXD_SOCKET_PATH).st_mode)\n    return False\n\n\ndef _get_json_response(\n    session: requests.Session, url: str, do_raise: bool = True\n):\n    url_response = _do_request(session, url, do_raise)\n    if not url_response.ok:\n        LOG.debug(\n            \"Skipping %s on [HTTP:%d]:%s\",\n            url,\n            url_response.status_code,\n            url_response.text,\n        )\n        return {}\n    try:\n        return url_response.json()\n    except JSONDecodeError as exc:\n        raise sources.InvalidMetaDataException(\n            \"Unable to process LXD config at {url}.\"\n            \" Expected JSON but found: {resp}\".format(\n                url=url, resp=url_response.text\n            )\n        ) from exc\n\n\ndef _do_request(\n    session: requests.Session, url: str, do_raise: bool = True\n) -> requests.Response:\n    for retries in range(30, 0, -1):\n        response = session.get(url)\n        if 500 == response.status_code:\n            # retry every 0.1 seconds for 3 seconds in the case of 500 error\n            # tis evil, but it also works around a bug\n            time.sleep(0.1)\n            LOG.warning(\n                \"[GET] [HTTP:%d] %s, retrying %d more time(s)\",\n                response.status_code,\n                url,\n                retries,\n            )\n        else:\n            break\n    LOG.debug(\"[GET] [HTTP:%d] %s\", response.status_code, url)\n    if do_raise and not response.ok:\n        raise sources.InvalidMetaDataException(\n            \"Invalid HTTP response [{code}] from {route}: {resp}\".format(\n                code=response.status_code,\n                route=url,\n                resp=response.text,\n            )\n        )\n    return response\n\n\nclass MetaDataKeys(Flag):\n    NONE = auto()\n    CONFIG = auto()\n    DEVICES = auto()\n    META_DATA = auto()\n    ALL = CONFIG | DEVICES | META_DATA\n\n\nclass _MetaDataReader:\n    def __init__(self, api_version: str = LXD_SOCKET_API_VERSION):\n        self.api_version = api_version\n        self._version_url = url_helper.combine_url(LXD_URL, self.api_version)\n\n    def _process_config(self, session: requests.Session) -> dict:\n        \"\"\"Iterate on LXD API config items. Promoting CONFIG_KEY_ALIASES\n\n        Any CONFIG_KEY_ALIASES which affect cloud-init behavior are promoted\n        as top-level configuration keys: user-data, network-data, vendor-data.\n\n        LXD's cloud-init.* config keys override any user.* config keys.\n        Log debug messages if any user.* keys are overridden by the related\n        cloud-init.* key.\n        \"\"\"\n        config: dict = {\"config\": {}}\n        config_url = url_helper.combine_url(self._version_url, \"config\")\n        # Represent all advertized/available config routes under\n        # the dict path {LXD_SOCKET_API_VERSION: {config: {...}}.\n        config_routes = _get_json_response(session, config_url)\n\n        # Sorting keys to ensure we always process in alphabetical order.\n        # cloud-init.* keys will sort before user.* keys which is preferred\n        # precedence.\n        for config_route in sorted(config_routes):\n            config_route_url = url_helper.combine_url(LXD_URL, config_route)\n            config_route_response = _do_request(\n                session, config_route_url, do_raise=False\n            )\n            if not config_route_response.ok:\n                LOG.debug(\n                    \"Skipping %s on [HTTP:%d]:%s\",\n                    config_route_url,\n                    config_route_response.status_code,\n                    config_route_response.text,\n                )\n                continue\n\n            cfg_key = config_route.rpartition(\"/\")[-1]\n            # Leave raw data values/format unchanged to represent it in\n            # instance-data.json for cloud-init query or jinja template\n            # use.\n            config[\"config\"][cfg_key] = config_route_response.text\n            # Promote common CONFIG_KEY_ALIASES to top-level keys.\n            if cfg_key in CONFIG_KEY_ALIASES:\n                # Due to sort of config_routes, promote cloud-init.*\n                # aliases before user.*. This allows user.* keys to act as\n                # fallback config on old LXD, with new cloud-init images.\n                if CONFIG_KEY_ALIASES[cfg_key] not in config:\n                    config[\n                        CONFIG_KEY_ALIASES[cfg_key]\n                    ] = config_route_response.text\n                else:\n                    LOG.warning(\n                        \"Ignoring LXD config %s in favor of %s value.\",\n                        cfg_key,\n                        cfg_key.replace(\"user\", \"cloud-init\", 1),\n                    )\n        return config\n\n    def __call__(self, *, metadata_keys: MetaDataKeys) -> dict:\n        with requests.Session() as session:\n            session.mount(self._version_url, LXDSocketAdapter())\n            # Document API version read\n            md: dict = {\"_metadata_api_version\": self.api_version}\n            if MetaDataKeys.META_DATA in metadata_keys:\n                md_route = url_helper.combine_url(\n                    self._version_url, \"meta-data\"\n                )\n                md[\"meta-data\"] = _do_request(session, md_route).text\n            if MetaDataKeys.CONFIG in metadata_keys:\n                md.update(self._process_config(session))\n            if MetaDataKeys.DEVICES in metadata_keys:\n                url = url_helper.combine_url(self._version_url, \"devices\")\n                devices = _get_json_response(session, url, do_raise=False)\n                if devices:\n                    md[\"devices\"] = devices\n            return md\n\n\ndef read_metadata(\n    api_version: str = LXD_SOCKET_API_VERSION,\n    metadata_keys: MetaDataKeys = MetaDataKeys.ALL,\n) -> dict:\n    \"\"\"Fetch metadata from the /dev/lxd/socket routes.\n\n    Perform a number of HTTP GETs on known routes on the devlxd socket API.\n    Minimally all containers must respond to <LXD_SOCKET_API_VERSION>/meta-data\n    when the LXD configuration setting `security.devlxd` is true.\n\n    When `security.devlxd` is false, no /dev/lxd/socket file exists. This\n    datasource will return False from `is_platform_viable` in that case.\n\n    Perform a GET of <LXD_SOCKET_API_VERSION>/config` and walk all `user.*`\n    configuration keys, storing all keys and values under a dict key\n        LXD_SOCKET_API_VERSION: config {...}.\n\n    In the presence of the following optional user config keys,\n    create top level aliases:\n      - user.user-data -> user-data\n      - user.vendor-data -> vendor-data\n      - user.network-config -> network-config\n\n    :param api_version:\n        LXD API version to operated with.\n    :param metadata_keys:\n        Instance of `MetaDataKeys` indicating what keys to fetch.\n    :return:\n        A dict with the following optional keys: meta-data, user-data,\n        vendor-data, network-config, network_mode, devices.\n\n        Below <LXD_SOCKET_API_VERSION> is a dict representation of all raw\n        configuration keys and values provided to the container surfaced by\n        the socket under the /1.0/config/ route.\n    \"\"\"\n    return _MetaDataReader(api_version=api_version)(\n        metadata_keys=metadata_keys\n    )\n\n\n# Used to match classes to dependencies\ndatasources = [\n    (DataSourceLXD, (sources.DEP_FILESYSTEM,)),\n]\n\n\n# Return a list of data sources that match this set of dependencies\ndef get_datasource_list(depends):\n    return sources.list_from_depends(depends, datasources)\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    description = \"\"\"Query LXD metadata and emit a JSON object.\"\"\"\n    parser = argparse.ArgumentParser(description=description)\n    parser.parse_args()\n    print(util.json_dumps(read_metadata(metadata_keys=MetaDataKeys.ALL)))\n\n# vi: ts=4 expandtab\n", "# Author: Eric Benner <ebenner@vultr.com>\n#\n# This file is part of cloud-init. See LICENSE file for license information.\n\n# Vultr Metadata API:\n# https://www.vultr.com/metadata/\n\nimport cloudinit.sources.helpers.vultr as vultr\nfrom cloudinit import log as log\nfrom cloudinit import sources, stages, util, version\n\nLOG = log.getLogger(__name__)\nBUILTIN_DS_CONFIG = {\n    \"url\": \"http://169.254.169.254\",\n    \"retries\": 30,\n    \"timeout\": 10,\n    \"wait\": 5,\n    \"user-agent\": \"Cloud-Init/%s - OS: %s Variant: %s\"\n    % (\n        version.version_string(),\n        util.system_info()[\"system\"],\n        util.system_info()[\"variant\"],\n    ),\n}\n\n\nclass DataSourceVultr(sources.DataSource):\n\n    dsname = \"Vultr\"\n\n    def __init__(self, sys_cfg, distro, paths):\n        super(DataSourceVultr, self).__init__(sys_cfg, distro, paths)\n        self.ds_cfg = util.mergemanydict(\n            [\n                util.get_cfg_by_path(sys_cfg, [\"datasource\", \"Vultr\"], {}),\n                BUILTIN_DS_CONFIG,\n            ]\n        )\n\n    @staticmethod\n    def ds_detect():\n        return vultr.is_vultr()\n\n    # Initiate data and check if Vultr\n    def _get_data(self):\n\n        LOG.debug(\"Machine is a Vultr instance\")\n\n        # Fetch metadata\n        self.metadata = self.get_metadata()\n        self.userdata_raw = self.metadata[\"user-data\"]\n\n        # Generate config and process data\n        self.get_datasource_data(self.metadata)\n\n        # Dump some data so diagnosing failures is manageable\n        LOG.debug(\"Vultr Vendor Config:\")\n        LOG.debug(util.json_dumps(self.metadata[\"vendor-data\"]))\n        LOG.debug(\"SUBID: %s\", self.metadata[\"instance-id\"])\n        LOG.debug(\"Hostname: %s\", self.metadata[\"local-hostname\"])\n        if self.userdata_raw is not None:\n            LOG.debug(\"User-Data:\")\n            LOG.debug(self.userdata_raw)\n\n        return True\n\n    # Process metadata\n    def get_datasource_data(self, md):\n        # Generate network config\n        if \"cloud_interfaces\" in md:\n            # In the future we will just drop pre-configured\n            # network configs into the array. They need names though.\n            vultr.add_interface_names(md[\"cloud_interfaces\"])\n            self.netcfg = md[\"cloud_interfaces\"]\n        else:\n            self.netcfg = vultr.generate_network_config(md[\"interfaces\"])\n        # Grab vendordata\n        self.vendordata_raw = md[\"vendor-data\"]\n\n        # Default hostname is \"guest\" for whitelabel\n        if self.metadata[\"local-hostname\"] == \"\":\n            self.metadata[\"local-hostname\"] = \"guest\"\n\n        self.userdata_raw = md[\"user-data\"]\n        if self.userdata_raw == \"\":\n            self.userdata_raw = None\n\n    # Get the metadata by flag\n    def get_metadata(self):\n        return vultr.get_metadata(\n            self.distro,\n            self.ds_cfg[\"url\"],\n            self.ds_cfg[\"timeout\"],\n            self.ds_cfg[\"retries\"],\n            self.ds_cfg[\"wait\"],\n            self.ds_cfg[\"user-agent\"],\n            tmp_dir=self.distro.get_tmp_exec_path(),\n        )\n\n    # Compare subid as instance id\n    def check_instance_id(self, sys_cfg):\n        if not vultr.is_vultr():\n            return False\n\n        # Baremetal has no way to implement this in local\n        if vultr.is_baremetal():\n            return False\n\n        subid = vultr.get_sysinfo()[\"subid\"]\n        return sources.instance_id_matches_system_uuid(subid)\n\n    # Currently unsupported\n    @property\n    def launch_index(self):\n        return None\n\n    @property\n    def network_config(self):\n        return self.netcfg\n\n\n# Used to match classes to dependencies\ndatasources = [\n    (DataSourceVultr, (sources.DEP_FILESYSTEM,)),\n]\n\n\n# Return a list of data sources that match this set of dependencies\ndef get_datasource_list(depends):\n    return sources.list_from_depends(depends, datasources)\n\n\nif __name__ == \"__main__\":\n    import sys\n\n    if not vultr.is_vultr():\n        print(\"Machine is not a Vultr instance\")\n        sys.exit(1)\n\n    # It should probably be safe to try to detect distro via stages.Init(),\n    # which will fall back to Ubuntu if no distro config is found.\n    # this distro object is only used for dhcp fallback. Feedback from user(s)\n    # of __main__ would help determine if a better approach exists.\n    #\n    # we don't needReportEventStack, so reporter=True\n    distro = stages.Init(reporter=True).distro\n\n    md = vultr.get_metadata(\n        distro,\n        BUILTIN_DS_CONFIG[\"url\"],\n        BUILTIN_DS_CONFIG[\"timeout\"],\n        BUILTIN_DS_CONFIG[\"retries\"],\n        BUILTIN_DS_CONFIG[\"wait\"],\n        BUILTIN_DS_CONFIG[\"user-agent\"],\n    )\n    config = md[\"vendor-data\"]\n    sysinfo = vultr.get_sysinfo()\n\n    print(util.json_dumps(sysinfo))\n    print(util.json_dumps(config))\n", "# Copyright (C) 2012 Canonical Ltd.\n# Copyright (C) 2012 Hewlett-Packard Development Company, L.P.\n# Copyright (C) 2012 Yahoo! Inc.\n#\n# Author: Scott Moser <scott.moser@canonical.com>\n# Author: Juerg Haefliger <juerg.haefliger@hp.com>\n# Author: Joshua Harlow <harlowja@yahoo-inc.com>\n#\n# This file is part of cloud-init. See LICENSE file for license information.\n\nimport abc\nimport copy\nimport json\nimport os\nimport pickle\nimport re\nfrom collections import namedtuple\nfrom enum import Enum, unique\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom cloudinit import dmi, importer\nfrom cloudinit import log as logging\nfrom cloudinit import net, type_utils\nfrom cloudinit import user_data as ud\nfrom cloudinit import util\nfrom cloudinit.atomic_helper import write_json\nfrom cloudinit.distros import Distro\nfrom cloudinit.event import EventScope, EventType\nfrom cloudinit.filters import launch_index\nfrom cloudinit.helpers import Paths\nfrom cloudinit.persistence import CloudInitPickleMixin\nfrom cloudinit.reporting import events\n\nDSMODE_DISABLED = \"disabled\"\nDSMODE_LOCAL = \"local\"\nDSMODE_NETWORK = \"net\"\nDSMODE_PASS = \"pass\"\n\nVALID_DSMODES = [DSMODE_DISABLED, DSMODE_LOCAL, DSMODE_NETWORK]\n\nDEP_FILESYSTEM = \"FILESYSTEM\"\nDEP_NETWORK = \"NETWORK\"\nDS_PREFIX = \"DataSource\"\n\nEXPERIMENTAL_TEXT = (\n    \"EXPERIMENTAL: The structure and format of content scoped under the 'ds'\"\n    \" key may change in subsequent releases of cloud-init.\"\n)\n\n\nREDACT_SENSITIVE_VALUE = \"redacted for non-root user\"\n\n# Key which can be provide a cloud's official product name to cloud-init\nMETADATA_CLOUD_NAME_KEY = \"cloud-name\"\n\nUNSET = \"_unset\"\nMETADATA_UNKNOWN = \"unknown\"\n\nLOG = logging.getLogger(__name__)\n\n# CLOUD_ID_REGION_PREFIX_MAP format is:\n#  <region-match-prefix>: (<new-cloud-id>: <test_allowed_cloud_callable>)\nCLOUD_ID_REGION_PREFIX_MAP = {\n    \"cn-\": (\"aws-china\", lambda c: c == \"aws\"),  # only change aws regions\n    \"us-gov-\": (\"aws-gov\", lambda c: c == \"aws\"),  # only change aws regions\n    \"china\": (\"azure-china\", lambda c: c == \"azure\"),  # only change azure\n}\n\n\n@unique\nclass NetworkConfigSource(Enum):\n    \"\"\"\n    Represents the canonical list of network config sources that cloud-init\n    knows about.\n    \"\"\"\n\n    CMD_LINE = \"cmdline\"\n    DS = \"ds\"\n    SYSTEM_CFG = \"system_cfg\"\n    FALLBACK = \"fallback\"\n    INITRAMFS = \"initramfs\"\n\n    def __str__(self) -> str:\n        return self.value\n\n\nclass DatasourceUnpickleUserDataError(Exception):\n    \"\"\"Raised when userdata is unable to be unpickled due to python upgrades\"\"\"\n\n\nclass DataSourceNotFoundException(Exception):\n    pass\n\n\nclass InvalidMetaDataException(Exception):\n    \"\"\"Raised when metadata is broken, unavailable or disabled.\"\"\"\n\n\ndef process_instance_metadata(metadata, key_path=\"\", sensitive_keys=()):\n    \"\"\"Process all instance metadata cleaning it up for persisting as json.\n\n    Strip ci-b64 prefix and catalog any 'base64_encoded_keys' as a list\n\n    @return Dict copy of processed metadata.\n    \"\"\"\n    md_copy = copy.deepcopy(metadata)\n    base64_encoded_keys = []\n    sens_keys = []\n    for key, val in metadata.items():\n        if key_path:\n            sub_key_path = key_path + \"/\" + key\n        else:\n            sub_key_path = key\n        if key in sensitive_keys or sub_key_path in sensitive_keys:\n            sens_keys.append(sub_key_path)\n        if isinstance(val, str) and val.startswith(\"ci-b64:\"):\n            base64_encoded_keys.append(sub_key_path)\n            md_copy[key] = val.replace(\"ci-b64:\", \"\")\n        if isinstance(val, dict):\n            return_val = process_instance_metadata(\n                val, sub_key_path, sensitive_keys\n            )\n            base64_encoded_keys.extend(return_val.pop(\"base64_encoded_keys\"))\n            sens_keys.extend(return_val.pop(\"sensitive_keys\"))\n            md_copy[key] = return_val\n    md_copy[\"base64_encoded_keys\"] = sorted(base64_encoded_keys)\n    md_copy[\"sensitive_keys\"] = sorted(sens_keys)\n    return md_copy\n\n\ndef redact_sensitive_keys(metadata, redact_value=REDACT_SENSITIVE_VALUE):\n    \"\"\"Redact any sensitive keys from to provided metadata dictionary.\n\n    Replace any keys values listed in 'sensitive_keys' with redact_value.\n    \"\"\"\n    if not metadata.get(\"sensitive_keys\", []):\n        return metadata\n    md_copy = copy.deepcopy(metadata)\n    for key_path in metadata.get(\"sensitive_keys\"):\n        path_parts = key_path.split(\"/\")\n        obj = md_copy\n        for path in path_parts:\n            if isinstance(obj[path], dict) and path != path_parts[-1]:\n                obj = obj[path]\n        obj[path] = redact_value\n    return md_copy\n\n\nURLParams = namedtuple(\n    \"URLParams\",\n    [\n        \"max_wait_seconds\",\n        \"timeout_seconds\",\n        \"num_retries\",\n        \"sec_between_retries\",\n    ],\n)\n\nDataSourceHostname = namedtuple(\n    \"DataSourceHostname\",\n    [\"hostname\", \"is_default\"],\n)\n\n\nclass DataSource(CloudInitPickleMixin, metaclass=abc.ABCMeta):\n\n    dsmode = DSMODE_NETWORK\n    default_locale = \"en_US.UTF-8\"\n\n    # Datasource name needs to be set by subclasses to determine which\n    # cloud-config datasource key is loaded\n    dsname = \"_undef\"\n\n    # Cached cloud_name as determined by _get_cloud_name\n    _cloud_name = None\n\n    # Cached cloud platform api type: e.g. ec2, openstack, kvm, lxd, azure etc.\n    _platform_type = None\n\n    # More details about the cloud platform:\n    #  - metadata (http://169.254.169.254/)\n    #  - seed-dir (<dirname>)\n    _subplatform = None\n\n    # Track the discovered fallback nic for use in configuration generation.\n    _fallback_interface = None\n\n    # The network configuration sources that should be considered for this data\n    # source.  (The first source in this list that provides network\n    # configuration will be used without considering any that follow.)  This\n    # should always be a subset of the members of NetworkConfigSource with no\n    # duplicate entries.\n    network_config_sources: Tuple[NetworkConfigSource, ...] = (\n        NetworkConfigSource.CMD_LINE,\n        NetworkConfigSource.INITRAMFS,\n        NetworkConfigSource.SYSTEM_CFG,\n        NetworkConfigSource.DS,\n    )\n\n    # read_url_params\n    url_max_wait = -1  # max_wait < 0 means do not wait\n    url_timeout = 10  # timeout for each metadata url read attempt\n    url_retries = 5  # number of times to retry url upon 404\n    url_sec_between_retries = 1  # amount of seconds to wait between retries\n\n    # The datasource defines a set of supported EventTypes during which\n    # the datasource can react to changes in metadata and regenerate\n    # network configuration on metadata changes. These are defined in\n    # `supported_network_events`.\n    # The datasource also defines a set of default EventTypes that the\n    # datasource can react to. These are the event types that will be used\n    # if not overridden by the user.\n    # A datasource requiring to write network config on each system boot\n    # would call default_update_events['network'].add(EventType.BOOT).\n\n    # Default: generate network config on new instance id (first boot).\n    supported_update_events = {\n        EventScope.NETWORK: {\n            EventType.BOOT_NEW_INSTANCE,\n            EventType.BOOT,\n            EventType.BOOT_LEGACY,\n            EventType.HOTPLUG,\n        }\n    }\n    default_update_events = {\n        EventScope.NETWORK: {\n            EventType.BOOT_NEW_INSTANCE,\n        }\n    }\n\n    # N-tuple listing default values for any metadata-related class\n    # attributes cached on an instance by a process_data runs. These attribute\n    # values are reset via clear_cached_attrs during any update_metadata call.\n    cached_attr_defaults: Tuple[Tuple[str, Any], ...] = (\n        (\"ec2_metadata\", UNSET),\n        (\"network_json\", UNSET),\n        (\"metadata\", {}),\n        (\"userdata\", None),\n        (\"userdata_raw\", None),\n        (\"vendordata\", None),\n        (\"vendordata_raw\", None),\n        (\"vendordata2\", None),\n        (\"vendordata2_raw\", None),\n    )\n\n    _dirty_cache = False\n\n    # N-tuple of keypaths or keynames redact from instance-data.json for\n    # non-root users\n    sensitive_metadata_keys: Tuple[str, ...] = (\n        \"merged_cfg\",\n        \"security-credentials\",\n    )\n\n    # True on datasources that may not see hotplugged devices reflected\n    # in the updated metadata\n    skip_hotplug_detect = False\n\n    _ci_pkl_version = 1\n\n    def __init__(self, sys_cfg, distro: Distro, paths: Paths, ud_proc=None):\n        self.sys_cfg = sys_cfg\n        self.distro = distro\n        self.paths = paths\n        self.userdata = None\n        self.metadata: dict = {}\n        self.userdata_raw = None\n        self.vendordata = None\n        self.vendordata2 = None\n        self.vendordata_raw = None\n        self.vendordata2_raw = None\n\n        self.ds_cfg = util.get_cfg_by_path(\n            self.sys_cfg, (\"datasource\", self.dsname), {}\n        )\n        if not self.ds_cfg:\n            self.ds_cfg = {}\n\n        if not ud_proc:\n            self.ud_proc = ud.UserDataProcessor(self.paths)\n        else:\n            self.ud_proc = ud_proc\n\n    def _unpickle(self, ci_pkl_version: int) -> None:\n        \"\"\"Perform deserialization fixes for Paths.\"\"\"\n        if not hasattr(self, \"vendordata2\"):\n            self.vendordata2 = None\n        if not hasattr(self, \"vendordata2_raw\"):\n            self.vendordata2_raw = None\n        if not hasattr(self, \"skip_hotplug_detect\"):\n            self.skip_hotplug_detect = False\n        if hasattr(self, \"userdata\") and self.userdata is not None:\n            # If userdata stores MIME data, on < python3.6 it will be\n            # missing the 'policy' attribute that exists on >=python3.6.\n            # Calling str() on the userdata will attempt to access this\n            # policy attribute. This will raise an exception, causing\n            # the pickle load to fail, so cloud-init will discard the cache\n            try:\n                str(self.userdata)\n            except AttributeError as e:\n                LOG.debug(\n                    \"Unable to unpickle datasource: %s.\"\n                    \" Ignoring current cache.\",\n                    e,\n                )\n                raise DatasourceUnpickleUserDataError() from e\n\n    def __str__(self):\n        return type_utils.obj_name(self)\n\n    def ds_detect(self) -> bool:\n        \"\"\"Check if running on this datasource\"\"\"\n        return True\n\n    def override_ds_detect(self) -> bool:\n        \"\"\"Override if either:\n        - only a single datasource defined (nothing to fall back to)\n        - commandline argument is used (ci.ds=OpenStack)\n\n        Note: get_cmdline() is required for the general case - when ds-identify\n        does not run, _something_ needs to detect the kernel command line\n        definition.\n        \"\"\"\n        if self.dsname.lower() == parse_cmdline().lower():\n            LOG.debug(\n                \"Machine is configured by the kernel commandline to run on \"\n                \"single datasource %s.\",\n                self,\n            )\n            return True\n        elif self.sys_cfg.get(\"datasource_list\", []) in (\n            [self.dsname],\n            [self.dsname, \"None\"],\n        ):\n            LOG.debug(\n                \"Machine is configured to run on single datasource %s.\", self\n            )\n            return True\n        return False\n\n    def _check_and_get_data(self):\n        \"\"\"Overrides runtime datasource detection\"\"\"\n        if self.override_ds_detect():\n            return self._get_data()\n        elif self.ds_detect():\n            LOG.debug(\"Machine is running on %s.\", self)\n            return self._get_data()\n        else:\n            LOG.debug(\"Datasource type %s is not detected.\", self)\n            return False\n\n    def _get_standardized_metadata(self, instance_data):\n        \"\"\"Return a dictionary of standardized metadata keys.\"\"\"\n        local_hostname = self.get_hostname().hostname\n        instance_id = self.get_instance_id()\n        availability_zone = self.availability_zone\n        # In the event of upgrade from existing cloudinit, pickled datasource\n        # will not contain these new class attributes. So we need to recrawl\n        # metadata to discover that content\n        sysinfo = instance_data[\"sys_info\"]\n        return {\n            \"v1\": {\n                \"_beta_keys\": [\"subplatform\"],\n                \"availability-zone\": availability_zone,\n                \"availability_zone\": availability_zone,\n                \"cloud_id\": canonical_cloud_id(\n                    self.cloud_name, self.region, self.platform_type\n                ),\n                \"cloud-name\": self.cloud_name,\n                \"cloud_name\": self.cloud_name,\n                \"distro\": sysinfo[\"dist\"][0],\n                \"distro_version\": sysinfo[\"dist\"][1],\n                \"distro_release\": sysinfo[\"dist\"][2],\n                \"platform\": self.platform_type,\n                \"public_ssh_keys\": self.get_public_ssh_keys(),\n                \"python_version\": sysinfo[\"python\"],\n                \"instance-id\": instance_id,\n                \"instance_id\": instance_id,\n                \"kernel_release\": sysinfo[\"uname\"][2],\n                \"local-hostname\": local_hostname,\n                \"local_hostname\": local_hostname,\n                \"machine\": sysinfo[\"uname\"][4],\n                \"region\": self.region,\n                \"subplatform\": self.subplatform,\n                \"system_platform\": sysinfo[\"platform\"],\n                \"variant\": sysinfo[\"variant\"],\n            }\n        }\n\n    def clear_cached_attrs(self, attr_defaults=()):\n        \"\"\"Reset any cached metadata attributes to datasource defaults.\n\n        @param attr_defaults: Optional tuple of (attr, value) pairs to\n           set instead of cached_attr_defaults.\n        \"\"\"\n        if not self._dirty_cache:\n            return\n        if attr_defaults:\n            attr_values = attr_defaults\n        else:\n            attr_values = self.cached_attr_defaults\n\n        for attribute, value in attr_values:\n            if hasattr(self, attribute):\n                setattr(self, attribute, value)\n        if not attr_defaults:\n            self._dirty_cache = False\n\n    def get_data(self) -> bool:\n        \"\"\"Datasources implement _get_data to setup metadata and userdata_raw.\n\n        Minimally, the datasource should return a boolean True on success.\n        \"\"\"\n        self._dirty_cache = True\n        return_value = self._check_and_get_data()\n        if not return_value:\n            return return_value\n        self.persist_instance_data()\n        return return_value\n\n    def persist_instance_data(self, write_cache=True):\n        \"\"\"Process and write INSTANCE_JSON_FILE with all instance metadata.\n\n        Replace any hyphens with underscores in key names for use in template\n        processing.\n\n        :param write_cache: boolean set True to persist obj.pkl when\n            instance_link exists.\n\n        @return True on successful write, False otherwise.\n        \"\"\"\n        if write_cache and os.path.lexists(self.paths.instance_link):\n            pkl_store(self, self.paths.get_ipath_cur(\"obj_pkl\"))\n        if hasattr(self, \"_crawled_metadata\"):\n            # Any datasource with _crawled_metadata will best represent\n            # most recent, 'raw' metadata\n            crawled_metadata = copy.deepcopy(\n                getattr(self, \"_crawled_metadata\")\n            )\n            crawled_metadata.pop(\"user-data\", None)\n            crawled_metadata.pop(\"vendor-data\", None)\n            instance_data = {\"ds\": crawled_metadata}\n        else:\n            instance_data = {\"ds\": {\"meta_data\": self.metadata}}\n            if hasattr(self, \"network_json\"):\n                network_json = getattr(self, \"network_json\")\n                if network_json != UNSET:\n                    instance_data[\"ds\"][\"network_json\"] = network_json\n            if hasattr(self, \"ec2_metadata\"):\n                ec2_metadata = getattr(self, \"ec2_metadata\")\n                if ec2_metadata != UNSET:\n                    instance_data[\"ds\"][\"ec2_metadata\"] = ec2_metadata\n        instance_data[\"ds\"][\"_doc\"] = EXPERIMENTAL_TEXT\n        # Add merged cloud.cfg and sys info for jinja templates and cli query\n        instance_data[\"merged_cfg\"] = copy.deepcopy(self.sys_cfg)\n        instance_data[\"merged_cfg\"][\"_doc\"] = (\n            \"Merged cloud-init system config from /etc/cloud/cloud.cfg and\"\n            \" /etc/cloud/cloud.cfg.d/\"\n        )\n        instance_data[\"sys_info\"] = util.system_info()\n        instance_data.update(self._get_standardized_metadata(instance_data))\n        try:\n            # Process content base64encoding unserializable values\n            content = util.json_dumps(instance_data)\n            # Strip base64: prefix and set base64_encoded_keys list.\n            processed_data = process_instance_metadata(\n                json.loads(content),\n                sensitive_keys=self.sensitive_metadata_keys,\n            )\n        except TypeError as e:\n            LOG.warning(\"Error persisting instance-data.json: %s\", str(e))\n            return False\n        except UnicodeDecodeError as e:\n            LOG.warning(\"Error persisting instance-data.json: %s\", str(e))\n            return False\n        json_sensitive_file = self.paths.get_runpath(\"instance_data_sensitive\")\n        cloud_id = instance_data[\"v1\"].get(\"cloud_id\", \"none\")\n        cloud_id_file = os.path.join(self.paths.run_dir, \"cloud-id\")\n        util.write_file(f\"{cloud_id_file}-{cloud_id}\", f\"{cloud_id}\\n\")\n        # cloud-id not found, then no previous cloud-id fle\n        prev_cloud_id_file = None\n        new_cloud_id_file = f\"{cloud_id_file}-{cloud_id}\"\n        # cloud-id found, then the prev cloud-id file is source of symlink\n        if os.path.exists(cloud_id_file):\n            prev_cloud_id_file = os.path.realpath(cloud_id_file)\n\n        util.sym_link(new_cloud_id_file, cloud_id_file, force=True)\n        if prev_cloud_id_file and prev_cloud_id_file != new_cloud_id_file:\n            util.del_file(prev_cloud_id_file)\n        write_json(json_sensitive_file, processed_data, mode=0o600)\n        json_file = self.paths.get_runpath(\"instance_data\")\n        # World readable\n        write_json(json_file, redact_sensitive_keys(processed_data))\n        return True\n\n    def _get_data(self) -> bool:\n        \"\"\"Walk metadata sources, process crawled data and save attributes.\"\"\"\n        raise NotImplementedError(\n            \"Subclasses of DataSource must implement _get_data which\"\n            \" sets self.metadata, vendordata_raw and userdata_raw.\"\n        )\n\n    def get_url_params(self):\n        \"\"\"Return the Datasource's preferred url_read parameters.\n\n        Subclasses may override url_max_wait, url_timeout, url_retries.\n\n        @return: A URLParams object with max_wait_seconds, timeout_seconds,\n            num_retries.\n        \"\"\"\n        max_wait = self.url_max_wait\n        try:\n            max_wait = int(self.ds_cfg.get(\"max_wait\", self.url_max_wait))\n        except ValueError:\n            util.logexc(\n                LOG,\n                \"Config max_wait '%s' is not an int, using default '%s'\",\n                self.ds_cfg.get(\"max_wait\"),\n                max_wait,\n            )\n\n        timeout = self.url_timeout\n        try:\n            timeout = max(0, int(self.ds_cfg.get(\"timeout\", self.url_timeout)))\n        except ValueError:\n            timeout = self.url_timeout\n            util.logexc(\n                LOG,\n                \"Config timeout '%s' is not an int, using default '%s'\",\n                self.ds_cfg.get(\"timeout\"),\n                timeout,\n            )\n\n        retries = self.url_retries\n        try:\n            retries = int(self.ds_cfg.get(\"retries\", self.url_retries))\n        except Exception:\n            util.logexc(\n                LOG,\n                \"Config retries '%s' is not an int, using default '%s'\",\n                self.ds_cfg.get(\"retries\"),\n                retries,\n            )\n\n        sec_between_retries = self.url_sec_between_retries\n        try:\n            sec_between_retries = int(\n                self.ds_cfg.get(\n                    \"sec_between_retries\", self.url_sec_between_retries\n                )\n            )\n        except Exception:\n            util.logexc(\n                LOG,\n                \"Config sec_between_retries '%s' is not an int,\"\n                \" using default '%s'\",\n                self.ds_cfg.get(\"sec_between_retries\"),\n                sec_between_retries,\n            )\n\n        return URLParams(max_wait, timeout, retries, sec_between_retries)\n\n    def get_userdata(self, apply_filter=False):\n        if self.userdata is None:\n            self.userdata = self.ud_proc.process(self.get_userdata_raw())\n        if apply_filter:\n            return self._filter_xdata(self.userdata)\n        return self.userdata\n\n    def get_vendordata(self):\n        if self.vendordata is None:\n            self.vendordata = self.ud_proc.process(self.get_vendordata_raw())\n        return self.vendordata\n\n    def get_vendordata2(self):\n        if self.vendordata2 is None:\n            self.vendordata2 = self.ud_proc.process(self.get_vendordata2_raw())\n        return self.vendordata2\n\n    @property\n    def fallback_interface(self):\n        \"\"\"Determine the network interface used during local network config.\"\"\"\n        if self._fallback_interface is None:\n            self._fallback_interface = net.find_fallback_nic()\n            if self._fallback_interface is None:\n                LOG.warning(\n                    \"Did not find a fallback interface on %s.\", self.cloud_name\n                )\n        return self._fallback_interface\n\n    @property\n    def platform_type(self):\n        if not hasattr(self, \"_platform_type\"):\n            # Handle upgrade path where pickled datasource has no _platform.\n            self._platform_type = self.dsname.lower()\n        if not self._platform_type:\n            self._platform_type = self.dsname.lower()\n        return self._platform_type\n\n    @property\n    def subplatform(self):\n        \"\"\"Return a string representing subplatform details for the datasource.\n\n        This should be guidance for where the metadata is sourced.\n        Examples of this on different clouds:\n            ec2:       metadata (http://169.254.169.254)\n            openstack: configdrive (/dev/path)\n            openstack: metadata (http://169.254.169.254)\n            nocloud:   seed-dir (/seed/dir/path)\n            lxd:   nocloud (/seed/dir/path)\n        \"\"\"\n        if not hasattr(self, \"_subplatform\"):\n            # Handle upgrade path where pickled datasource has no _platform.\n            self._subplatform = self._get_subplatform()\n        if not self._subplatform:\n            self._subplatform = self._get_subplatform()\n        return self._subplatform\n\n    def _get_subplatform(self):\n        \"\"\"Subclasses should implement to return a \"slug (detail)\" string.\"\"\"\n        if hasattr(self, \"metadata_address\"):\n            return \"metadata (%s)\" % getattr(self, \"metadata_address\")\n        return METADATA_UNKNOWN\n\n    @property\n    def cloud_name(self):\n        \"\"\"Return lowercase cloud name as determined by the datasource.\n\n        Datasource can determine or define its own cloud product name in\n        metadata.\n        \"\"\"\n        if self._cloud_name:\n            return self._cloud_name\n        if self.metadata and self.metadata.get(METADATA_CLOUD_NAME_KEY):\n            cloud_name = self.metadata.get(METADATA_CLOUD_NAME_KEY)\n            if isinstance(cloud_name, str):\n                self._cloud_name = cloud_name.lower()\n            else:\n                self._cloud_name = self._get_cloud_name().lower()\n                LOG.debug(\n                    \"Ignoring metadata provided key %s: non-string type %s\",\n                    METADATA_CLOUD_NAME_KEY,\n                    type(cloud_name),\n                )\n        else:\n            self._cloud_name = self._get_cloud_name().lower()\n        return self._cloud_name\n\n    def _get_cloud_name(self):\n        \"\"\"Return the datasource name as it frequently matches cloud name.\n\n        Should be overridden in subclasses which can run on multiple\n        cloud names, such as DatasourceEc2.\n        \"\"\"\n        return self.dsname\n\n    @property\n    def launch_index(self):\n        if not self.metadata:\n            return None\n        if \"launch-index\" in self.metadata:\n            return self.metadata[\"launch-index\"]\n        return None\n\n    def _filter_xdata(self, processed_ud):\n        filters = [\n            launch_index.Filter(util.safe_int(self.launch_index)),\n        ]\n        new_ud = processed_ud\n        for f in filters:\n            new_ud = f.apply(new_ud)\n        return new_ud\n\n    @property\n    def is_disconnected(self):\n        return False\n\n    def get_userdata_raw(self):\n        return self.userdata_raw\n\n    def get_vendordata_raw(self):\n        return self.vendordata_raw\n\n    def get_vendordata2_raw(self):\n        return self.vendordata2_raw\n\n    # the data sources' config_obj is a cloud-config formated\n    # object that came to it from ways other than cloud-config\n    # because cloud-config content would be handled elsewhere\n    def get_config_obj(self):\n        return {}\n\n    def get_public_ssh_keys(self):\n        return normalize_pubkey_data(self.metadata.get(\"public-keys\"))\n\n    def publish_host_keys(self, hostkeys):\n        \"\"\"Publish the public SSH host keys (found in /etc/ssh/*.pub).\n\n        @param hostkeys: List of host key tuples (key_type, key_value),\n            where key_type is the first field in the public key file\n            (e.g. 'ssh-rsa') and key_value is the key itself\n            (e.g. 'AAAAB3NzaC1y...').\n        \"\"\"\n\n    def _remap_device(self, short_name):\n        # LP: #611137\n        # the metadata service may believe that devices are named 'sda'\n        # when the kernel named them 'vda' or 'xvda'\n        # we want to return the correct value for what will actually\n        # exist in this instance\n        mappings = {\"sd\": (\"vd\", \"xvd\", \"vtb\")}\n        for (nfrom, tlist) in mappings.items():\n            if not short_name.startswith(nfrom):\n                continue\n            for nto in tlist:\n                cand = \"/dev/%s%s\" % (nto, short_name[len(nfrom) :])\n                if os.path.exists(cand):\n                    return cand\n        return None\n\n    def device_name_to_device(self, _name):\n        # translate a 'name' to a device\n        # the primary function at this point is on ec2\n        # to consult metadata service, that has\n        #  ephemeral0: sdb\n        # and return 'sdb' for input 'ephemeral0'\n        return None\n\n    def get_locale(self):\n        \"\"\"Default locale is en_US.UTF-8, but allow distros to override\"\"\"\n        locale = self.default_locale\n        try:\n            locale = self.distro.get_locale()\n        except NotImplementedError:\n            pass\n        return locale\n\n    @property\n    def availability_zone(self):\n        top_level_az = self.metadata.get(\n            \"availability-zone\", self.metadata.get(\"availability_zone\")\n        )\n        if top_level_az:\n            return top_level_az\n        return self.metadata.get(\"placement\", {}).get(\"availability-zone\")\n\n    @property\n    def region(self):\n        return self.metadata.get(\"region\")\n\n    def get_instance_id(self):\n        if not self.metadata or \"instance-id\" not in self.metadata:\n            # Return a magic not really instance id string\n            return \"iid-datasource\"\n        return str(self.metadata[\"instance-id\"])\n\n    def get_hostname(self, fqdn=False, resolve_ip=False, metadata_only=False):\n        \"\"\"Get hostname or fqdn from the datasource. Look it up if desired.\n\n        @param fqdn: Boolean, set True to return hostname with domain.\n        @param resolve_ip: Boolean, set True to attempt to resolve an ipv4\n            address provided in local-hostname meta-data.\n        @param metadata_only: Boolean, set True to avoid looking up hostname\n            if meta-data doesn't have local-hostname present.\n\n        @return: a DataSourceHostname namedtuple\n            <hostname or qualified hostname>, <is_default> (str, bool).\n            is_default is a bool and\n            it's true only if hostname is localhost and was\n            returned by util.get_hostname() as a default.\n            This is used to differentiate with a user-defined\n            localhost hostname.\n            Optionally return (None, False) when\n            metadata_only is True and local-hostname data is not available.\n        \"\"\"\n        defdomain = \"localdomain\"\n        defhost = \"localhost\"\n        domain = defdomain\n        is_default = False\n\n        if not self.metadata or not self.metadata.get(\"local-hostname\"):\n            if metadata_only:\n                return DataSourceHostname(None, is_default)\n            # this is somewhat questionable really.\n            # the cloud datasource was asked for a hostname\n            # and didn't have one. raising error might be more appropriate\n            # but instead, basically look up the existing hostname\n            toks = []\n            hostname = util.get_hostname()\n            if hostname == \"localhost\":\n                # default hostname provided by socket.gethostname()\n                is_default = True\n            hosts_fqdn = util.get_fqdn_from_hosts(hostname)\n            if hosts_fqdn and hosts_fqdn.find(\".\") > 0:\n                toks = str(hosts_fqdn).split(\".\")\n            elif hostname and hostname.find(\".\") > 0:\n                toks = str(hostname).split(\".\")\n            elif hostname:\n                toks = [hostname, defdomain]\n            else:\n                toks = [defhost, defdomain]\n        else:\n            # if there is an ipv4 address in 'local-hostname', then\n            # make up a hostname (LP: #475354) in format ip-xx.xx.xx.xx\n            lhost = self.metadata[\"local-hostname\"]\n            if net.is_ipv4_address(lhost):\n                toks = []\n                if resolve_ip:\n                    toks = util.gethostbyaddr(lhost)\n\n                if toks:\n                    toks = str(toks).split(\".\")\n                else:\n                    toks = [\"ip-%s\" % lhost.replace(\".\", \"-\")]\n            else:\n                toks = lhost.split(\".\")\n\n        if len(toks) > 1:\n            hostname = toks[0]\n            domain = \".\".join(toks[1:])\n        else:\n            hostname = toks[0]\n\n        if fqdn and domain != defdomain:\n            hostname = \"%s.%s\" % (hostname, domain)\n\n        return DataSourceHostname(hostname, is_default)\n\n    def get_package_mirror_info(self):\n        return self.distro.get_package_mirror_info(data_source=self)\n\n    def get_supported_events(self, source_event_types: List[EventType]):\n        supported_events: Dict[EventScope, set] = {}\n        for event in source_event_types:\n            for (\n                update_scope,\n                update_events,\n            ) in self.supported_update_events.items():\n                if event in update_events:\n                    if not supported_events.get(update_scope):\n                        supported_events[update_scope] = set()\n                    supported_events[update_scope].add(event)\n        return supported_events\n\n    def update_metadata_if_supported(\n        self, source_event_types: List[EventType]\n    ) -> bool:\n        \"\"\"Refresh cached metadata if the datasource supports this event.\n\n        The datasource has a list of supported_update_events which\n        trigger refreshing all cached metadata as well as refreshing the\n        network configuration.\n\n        @param source_event_types: List of EventTypes which may trigger a\n            metadata update.\n\n        @return True if the datasource did successfully update cached metadata\n            due to source_event_type.\n        \"\"\"\n        supported_events = self.get_supported_events(source_event_types)\n        for scope, matched_events in supported_events.items():\n            LOG.debug(\n                \"Update datasource metadata and %s config due to events: %s\",\n                scope.value,\n                \", \".join([event.value for event in matched_events]),\n            )\n            # Each datasource has a cached config property which needs clearing\n            # Once cleared that config property will be regenerated from\n            # current metadata.\n            self.clear_cached_attrs(((\"_%s_config\" % scope, UNSET),))\n        if supported_events:\n            self.clear_cached_attrs()\n            result = self.get_data()\n            if result:\n                return True\n        LOG.debug(\n            \"Datasource %s not updated for events: %s\",\n            self,\n            \", \".join([event.value for event in source_event_types]),\n        )\n        return False\n\n    def check_instance_id(self, sys_cfg):\n        # quickly (local check only) if self.instance_id is still\n        return False\n\n    @staticmethod\n    def _determine_dsmode(candidates, default=None, valid=None):\n        # return the first candidate that is non None, warn if not valid\n        if default is None:\n            default = DSMODE_NETWORK\n\n        if valid is None:\n            valid = VALID_DSMODES\n\n        for candidate in candidates:\n            if candidate is None:\n                continue\n            if candidate in valid:\n                return candidate\n            else:\n                LOG.warning(\n                    \"invalid dsmode '%s', using default=%s\", candidate, default\n                )\n                return default\n\n        return default\n\n    @property\n    def network_config(self):\n        return None\n\n    def setup(self, is_new_instance):\n        \"\"\"setup(is_new_instance)\n\n        This is called before user-data and vendor-data have been processed.\n\n        Unless the datasource has set mode to 'local', then networking\n        per 'fallback' or per 'network_config' will have been written and\n        brought up the OS at this point.\n        \"\"\"\n        return\n\n    def activate(self, cfg, is_new_instance):\n        \"\"\"activate(cfg, is_new_instance)\n\n        This is called before the init_modules will be called but after\n        the user-data and vendor-data have been fully processed.\n\n        The cfg is fully up to date config, it contains a merged view of\n           system config, datasource config, user config, vendor config.\n        It should be used rather than the sys_cfg passed to __init__.\n\n        is_new_instance is a boolean indicating if this is a new instance.\n        \"\"\"\n        return\n\n\ndef normalize_pubkey_data(pubkey_data):\n    keys = []\n\n    if not pubkey_data:\n        return keys\n\n    if isinstance(pubkey_data, str):\n        return pubkey_data.splitlines()\n\n    if isinstance(pubkey_data, (list, set)):\n        return list(pubkey_data)\n\n    if isinstance(pubkey_data, (dict)):\n        for (_keyname, klist) in pubkey_data.items():\n            # lp:506332 uec metadata service responds with\n            # data that makes boto populate a string for 'klist' rather\n            # than a list.\n            if isinstance(klist, str):\n                klist = [klist]\n            if isinstance(klist, (list, set)):\n                for pkey in klist:\n                    # There is an empty string at\n                    # the end of the keylist, trim it\n                    if pkey:\n                        keys.append(pkey)\n\n    return keys\n\n\ndef find_source(\n    sys_cfg, distro, paths, ds_deps, cfg_list, pkg_list, reporter\n) -> Tuple[DataSource, str]:\n    ds_list = list_sources(cfg_list, ds_deps, pkg_list)\n    ds_names = [type_utils.obj_name(f) for f in ds_list]\n    mode = \"network\" if DEP_NETWORK in ds_deps else \"local\"\n    LOG.debug(\"Searching for %s data source in: %s\", mode, ds_names)\n\n    for name, cls in zip(ds_names, ds_list):\n        myrep = events.ReportEventStack(\n            name=\"search-%s\" % name.replace(\"DataSource\", \"\"),\n            description=\"searching for %s data from %s\" % (mode, name),\n            message=\"no %s data found from %s\" % (mode, name),\n            parent=reporter,\n        )\n        try:\n            with myrep:\n                LOG.debug(\"Seeing if we can get any data from %s\", cls)\n                s = cls(sys_cfg, distro, paths)\n                if s.update_metadata_if_supported(\n                    [EventType.BOOT_NEW_INSTANCE]\n                ):\n                    myrep.message = \"found %s data from %s\" % (mode, name)\n                    return (s, type_utils.obj_name(cls))\n        except Exception:\n            util.logexc(LOG, \"Getting data from %s failed\", cls)\n\n    msg = \"Did not find any data source, searched classes: (%s)\" % \", \".join(\n        ds_names\n    )\n    raise DataSourceNotFoundException(msg)\n\n\ndef list_sources(cfg_list, depends, pkg_list):\n    \"\"\"Return a list of classes that have the same depends as 'depends'\n    iterate through cfg_list, loading \"DataSource*\" modules\n    and calling their \"get_datasource_list\".\n    Return an ordered list of classes that match (if any)\n    \"\"\"\n    src_list = []\n    LOG.debug(\n        \"Looking for data source in: %s,\"\n        \" via packages %s that matches dependencies %s\",\n        cfg_list,\n        pkg_list,\n        depends,\n    )\n\n    for ds in cfg_list:\n        ds_name = importer.match_case_insensitive_module_name(ds)\n        m_locs, _looked_locs = importer.find_module(\n            ds_name, pkg_list, [\"get_datasource_list\"]\n        )\n        if not m_locs:\n            LOG.error(\n                \"Could not import %s. Does the DataSource exist and \"\n                \"is it importable?\",\n                ds_name,\n            )\n        for m_loc in m_locs:\n            mod = importer.import_module(m_loc)\n            lister = getattr(mod, \"get_datasource_list\")\n            matches = lister(depends)\n            if matches:\n                src_list.extend(matches)\n                break\n    return src_list\n\n\ndef instance_id_matches_system_uuid(\n    instance_id, field: str = \"system-uuid\"\n) -> bool:\n    # quickly (local check only) if self.instance_id is still valid\n    # we check kernel command line or files.\n    if not instance_id:\n        return False\n\n    dmi_value = dmi.read_dmi_data(field)\n    if not dmi_value:\n        return False\n    return instance_id.lower() == dmi_value.lower()\n\n\ndef canonical_cloud_id(cloud_name, region, platform):\n    \"\"\"Lookup the canonical cloud-id for a given cloud_name and region.\"\"\"\n    if not cloud_name:\n        cloud_name = METADATA_UNKNOWN\n    if not region:\n        region = METADATA_UNKNOWN\n    if region == METADATA_UNKNOWN:\n        if cloud_name != METADATA_UNKNOWN:\n            return cloud_name\n        return platform\n    for prefix, cloud_id_test in CLOUD_ID_REGION_PREFIX_MAP.items():\n        (cloud_id, valid_cloud) = cloud_id_test\n        if region.startswith(prefix) and valid_cloud(cloud_name):\n            return cloud_id\n    if cloud_name != METADATA_UNKNOWN:\n        return cloud_name\n    return platform\n\n\ndef convert_vendordata(data, recurse=True):\n    \"\"\"data: a loaded object (strings, arrays, dicts).\n    return something suitable for cloudinit vendordata_raw.\n\n    if data is:\n       None: return None\n       string: return string\n       list: return data\n             the list is then processed in UserDataProcessor\n       dict: return convert_vendordata(data.get('cloud-init'))\n    \"\"\"\n    if not data:\n        return None\n    if isinstance(data, str):\n        return data\n    if isinstance(data, list):\n        return copy.deepcopy(data)\n    if isinstance(data, dict):\n        if recurse is True:\n            return convert_vendordata(data.get(\"cloud-init\"), recurse=False)\n        raise ValueError(\"vendordata['cloud-init'] cannot be dict\")\n    raise ValueError(\"Unknown data type for vendordata: %s\" % type(data))\n\n\nclass BrokenMetadata(IOError):\n    pass\n\n\n# 'depends' is a list of dependencies (DEP_FILESYSTEM)\n# ds_list is a list of 2 item lists\n# ds_list = [\n#   ( class, ( depends-that-this-class-needs ) )\n# }\n# It returns a list of 'class' that matched these deps exactly\n# It mainly is a helper function for DataSourceCollections\ndef list_from_depends(depends, ds_list):\n    ret_list = []\n    depset = set(depends)\n    for (cls, deps) in ds_list:\n        if depset == set(deps):\n            ret_list.append(cls)\n    return ret_list\n\n\ndef pkl_store(obj: DataSource, fname: str) -> bool:\n    \"\"\"Use pickle to serialize Datasource to a file as a cache.\n\n    :return: True on success\n    \"\"\"\n    try:\n        pk_contents = pickle.dumps(obj)\n    except Exception:\n        util.logexc(LOG, \"Failed pickling datasource %s\", obj)\n        return False\n    try:\n        util.write_file(fname, pk_contents, omode=\"wb\", mode=0o400)\n    except Exception:\n        util.logexc(LOG, \"Failed pickling datasource to %s\", fname)\n        return False\n    return True\n\n\ndef pkl_load(fname: str) -> Optional[DataSource]:\n    \"\"\"Use pickle to deserialize a instance Datasource from a cache file.\"\"\"\n    pickle_contents = None\n    try:\n        pickle_contents = util.load_file(fname, decode=False)\n    except Exception as e:\n        if os.path.isfile(fname):\n            LOG.warning(\"failed loading pickle in %s: %s\", fname, e)\n\n    # This is allowed so just return nothing successfully loaded...\n    if not pickle_contents:\n        return None\n    try:\n        return pickle.loads(pickle_contents)\n    except DatasourceUnpickleUserDataError:\n        return None\n    except Exception:\n        util.logexc(LOG, \"Failed loading pickled blob from %s\", fname)\n        return None\n\n\ndef parse_cmdline() -> str:\n    \"\"\"Check if command line argument for this datasource was passed\n    Passing by command line overrides runtime datasource detection\n    \"\"\"\n    cmdline = util.get_cmdline()\n    ds_parse_0 = re.search(r\"ds=([a-zA-Z]+)(\\s|$|;)\", cmdline)\n    ds_parse_1 = re.search(r\"ci\\.ds=([a-zA-Z]+)(\\s|$|;)\", cmdline)\n    ds_parse_2 = re.search(r\"ci\\.datasource=([a-zA-Z]+)(\\s|$|;)\", cmdline)\n    ds = ds_parse_0 or ds_parse_1 or ds_parse_2\n    deprecated = ds_parse_1 or ds_parse_2\n    if deprecated:\n        dsname = deprecated.group(1).strip()\n        util.deprecate(\n            deprecated=(\n                f\"Defining the datasource on the commandline using \"\n                f\"ci.ds={dsname} or \"\n                f\"ci.datasource={dsname}\"\n            ),\n            deprecated_version=\"23.2\",\n            extra_message=f\"Use ds={dsname} instead\",\n        )\n    if ds and ds.group(1):\n        return ds.group(1)\n    return \"\"\n", "# Copyright (C) 2012 Canonical Ltd.\n# Copyright (C) 2012, 2013 Hewlett-Packard Development Company, L.P.\n# Copyright (C) 2012 Yahoo! Inc.\n#\n# This file is part of cloud-init. See LICENSE file for license information.\n\nimport copy\nimport os\nimport sys\nfrom collections import namedtuple\nfrom typing import Dict, Iterable, List, Optional, Set\n\nfrom cloudinit import cloud, distros, handlers, helpers, importer\nfrom cloudinit import log as logging\nfrom cloudinit import net, sources, type_utils, util\nfrom cloudinit.event import EventScope, EventType, userdata_to_events\n\n# Default handlers (used if not overridden)\nfrom cloudinit.handlers.boot_hook import BootHookPartHandler\nfrom cloudinit.handlers.cloud_config import CloudConfigPartHandler\nfrom cloudinit.handlers.jinja_template import JinjaTemplatePartHandler\nfrom cloudinit.handlers.shell_script import ShellScriptPartHandler\nfrom cloudinit.handlers.shell_script_by_frequency import (\n    ShellScriptByFreqPartHandler,\n)\nfrom cloudinit.net import cmdline\nfrom cloudinit.reporting import events\nfrom cloudinit.settings import (\n    CLOUD_CONFIG,\n    PER_ALWAYS,\n    PER_INSTANCE,\n    PER_ONCE,\n    RUN_CLOUD_CONFIG,\n)\nfrom cloudinit.sources import NetworkConfigSource\n\nLOG = logging.getLogger(__name__)\n\nNO_PREVIOUS_INSTANCE_ID = \"NO_PREVIOUS_INSTANCE_ID\"\n\n\ndef update_event_enabled(\n    datasource: sources.DataSource,\n    cfg: dict,\n    event_source_type: EventType,\n    scope: Optional[EventScope] = None,\n) -> bool:\n    \"\"\"Determine if a particular EventType is enabled.\n\n    For the `event_source_type` passed in, check whether this EventType\n    is enabled in the `updates` section of the userdata. If `updates`\n    is not enabled in userdata, check if defined as one of the\n    `default_events` on the datasource. `scope` may be used to\n    narrow the check to a particular `EventScope`.\n\n    Note that on first boot, userdata may NOT be available yet. In this\n    case, we only have the data source's `default_update_events`,\n    so an event that should be enabled in userdata may be denied.\n    \"\"\"\n    default_events: Dict[\n        EventScope, Set[EventType]\n    ] = datasource.default_update_events\n    user_events: Dict[EventScope, Set[EventType]] = userdata_to_events(\n        cfg.get(\"updates\", {})\n    )\n    # A value in the first will override a value in the second\n    allowed = util.mergemanydict(\n        [\n            copy.deepcopy(user_events),\n            copy.deepcopy(default_events),\n        ]\n    )\n    LOG.debug(\"Allowed events: %s\", allowed)\n\n    scopes: Iterable[EventScope]\n    if not scope:\n        scopes = allowed.keys()\n    else:\n        scopes = [scope]\n    scope_values = [s.value for s in scopes]\n\n    for evt_scope in scopes:\n        if event_source_type in allowed.get(evt_scope, []):\n            LOG.debug(\n                \"Event Allowed: scope=%s EventType=%s\",\n                evt_scope.value,\n                event_source_type,\n            )\n            return True\n\n    LOG.debug(\n        \"Event Denied: scopes=%s EventType=%s\", scope_values, event_source_type\n    )\n    return False\n\n\nclass Init:\n    def __init__(self, ds_deps: Optional[List[str]] = None, reporter=None):\n        if ds_deps is not None:\n            self.ds_deps = ds_deps\n        else:\n            self.ds_deps = [sources.DEP_FILESYSTEM, sources.DEP_NETWORK]\n        # Created on first use\n        self._cfg: Optional[dict] = None\n        self._paths: Optional[helpers.Paths] = None\n        self._distro: Optional[distros.Distro] = None\n        # Changed only when a fetch occurs\n        self.datasource: Optional[sources.DataSource] = None\n        self.ds_restored = False\n        self._previous_iid = None\n\n        if reporter is None:\n            reporter = events.ReportEventStack(\n                name=\"init-reporter\",\n                description=\"init-desc\",\n                reporting_enabled=False,\n            )\n        self.reporter = reporter\n\n    def _reset(self, reset_ds=False):\n        # Recreated on access\n        self._cfg = None\n        self._paths = None\n        self._distro = None\n        if reset_ds:\n            self.datasource = None\n            self.ds_restored = False\n\n    @property\n    def distro(self):\n        if not self._distro:\n            # Try to find the right class to use\n            system_config = self._extract_cfg(\"system\")\n            distro_name = system_config.pop(\"distro\", \"ubuntu\")\n            distro_cls = distros.fetch(distro_name)\n            LOG.debug(\"Using distro class %s\", distro_cls)\n            self._distro = distro_cls(distro_name, system_config, self.paths)\n            # If we have an active datasource we need to adjust\n            # said datasource and move its distro/system config\n            # from whatever it was to a new set...\n            if self.datasource is not None:\n                self.datasource.distro = self._distro\n                self.datasource.sys_cfg = self.cfg\n        return self._distro\n\n    @property\n    def cfg(self):\n        return self._extract_cfg(\"restricted\")\n\n    def _extract_cfg(self, restriction):\n        # Ensure actually read\n        self.read_cfg()\n        # Nobody gets the real config\n        ocfg = copy.deepcopy(self._cfg)\n        if restriction == \"restricted\":\n            ocfg.pop(\"system_info\", None)\n        elif restriction == \"system\":\n            ocfg = util.get_cfg_by_path(ocfg, (\"system_info\",), {})\n        elif restriction == \"paths\":\n            ocfg = util.get_cfg_by_path(ocfg, (\"system_info\", \"paths\"), {})\n        if not isinstance(ocfg, (dict)):\n            ocfg = {}\n        return ocfg\n\n    @property\n    def paths(self):\n        if not self._paths:\n            path_info = self._extract_cfg(\"paths\")\n            self._paths = helpers.Paths(path_info, self.datasource)\n        return self._paths\n\n    def _initial_subdirs(self):\n        c_dir = self.paths.cloud_dir\n        run_dir = self.paths.run_dir\n        initial_dirs = [\n            c_dir,\n            os.path.join(c_dir, \"scripts\"),\n            os.path.join(c_dir, \"scripts\", \"per-instance\"),\n            os.path.join(c_dir, \"scripts\", \"per-once\"),\n            os.path.join(c_dir, \"scripts\", \"per-boot\"),\n            os.path.join(c_dir, \"scripts\", \"vendor\"),\n            os.path.join(c_dir, \"seed\"),\n            os.path.join(c_dir, \"instances\"),\n            os.path.join(c_dir, \"handlers\"),\n            os.path.join(c_dir, \"sem\"),\n            os.path.join(c_dir, \"data\"),\n            os.path.join(run_dir, \"sem\"),\n        ]\n        return initial_dirs\n\n    def purge_cache(self, rm_instance_lnk=False):\n        rm_list = [self.paths.boot_finished]\n        if rm_instance_lnk:\n            rm_list.append(self.paths.instance_link)\n        for f in rm_list:\n            util.del_file(f)\n        return len(rm_list)\n\n    def initialize(self):\n        self._initialize_filesystem()\n\n    def _initialize_filesystem(self):\n        util.ensure_dirs(self._initial_subdirs())\n        log_file = util.get_cfg_option_str(self.cfg, \"def_log_file\")\n        if log_file:\n            util.ensure_file(log_file, mode=0o640, preserve_mode=True)\n            perms = self.cfg.get(\"syslog_fix_perms\")\n            if not perms:\n                perms = {}\n            if not isinstance(perms, list):\n                perms = [perms]\n\n            error = None\n            for perm in perms:\n                u, g = util.extract_usergroup(perm)\n                try:\n                    util.chownbyname(log_file, u, g)\n                    return\n                except OSError as e:\n                    error = e\n\n            LOG.warning(\n                \"Failed changing perms on '%s'. tried: %s. %s\",\n                log_file,\n                \",\".join(perms),\n                error,\n            )\n\n    def read_cfg(self, extra_fns=None):\n        # None check so that we don't keep on re-loading if empty\n        if self._cfg is None:\n            self._cfg = self._read_cfg(extra_fns)\n            # LOG.debug(\"Loaded 'init' config %s\", self._cfg)\n\n    def _read_cfg(self, extra_fns):\n        no_cfg_paths = helpers.Paths({}, self.datasource)\n        instance_data_file = no_cfg_paths.get_runpath(\n            \"instance_data_sensitive\"\n        )\n        merger = helpers.ConfigMerger(\n            paths=no_cfg_paths,\n            datasource=self.datasource,\n            additional_fns=extra_fns,\n            base_cfg=fetch_base_config(instance_data_file=instance_data_file),\n        )\n        return merger.cfg\n\n    def _restore_from_cache(self):\n        # We try to restore from a current link and static path\n        # by using the instance link, if purge_cache was called\n        # the file wont exist.\n        return sources.pkl_load(self.paths.get_ipath_cur(\"obj_pkl\"))\n\n    def _write_to_cache(self):\n        if self.datasource is None:\n            return False\n        if util.get_cfg_option_bool(self.cfg, \"manual_cache_clean\", False):\n            # The empty file in instance/ dir indicates manual cleaning,\n            # and can be read by ds-identify.\n            util.write_file(\n                self.paths.get_ipath_cur(\"manual_clean_marker\"),\n                omode=\"w\",\n                content=\"\",\n            )\n        return sources.pkl_store(\n            self.datasource, self.paths.get_ipath_cur(\"obj_pkl\")\n        )\n\n    def _get_datasources(self):\n        # Any config provided???\n        pkg_list = self.cfg.get(\"datasource_pkg_list\") or []\n        # Add the defaults at the end\n        for n in [\"\", type_utils.obj_name(sources)]:\n            if n not in pkg_list:\n                pkg_list.append(n)\n        cfg_list = self.cfg.get(\"datasource_list\") or []\n        return (cfg_list, pkg_list)\n\n    def _restore_from_checked_cache(self, existing):\n        if existing not in (\"check\", \"trust\"):\n            raise ValueError(\"Unexpected value for existing: %s\" % existing)\n\n        ds = self._restore_from_cache()\n        if not ds:\n            return (None, \"no cache found\")\n\n        run_iid_fn = self.paths.get_runpath(\"instance_id\")\n        if os.path.exists(run_iid_fn):\n            run_iid = util.load_file(run_iid_fn).strip()\n        else:\n            run_iid = None\n\n        if run_iid == ds.get_instance_id():\n            return (ds, \"restored from cache with run check: %s\" % ds)\n        elif existing == \"trust\":\n            return (ds, \"restored from cache: %s\" % ds)\n        else:\n            if hasattr(ds, \"check_instance_id\") and ds.check_instance_id(\n                self.cfg\n            ):\n                return (ds, \"restored from checked cache: %s\" % ds)\n            else:\n                return (None, \"cache invalid in datasource: %s\" % ds)\n\n    def _get_data_source(self, existing) -> sources.DataSource:\n        if self.datasource is not None:\n            return self.datasource\n\n        with events.ReportEventStack(\n            name=\"check-cache\",\n            description=\"attempting to read from cache [%s]\" % existing,\n            parent=self.reporter,\n        ) as myrep:\n\n            ds, desc = self._restore_from_checked_cache(existing)\n            myrep.description = desc\n            self.ds_restored = bool(ds)\n            LOG.debug(myrep.description)\n\n        if not ds:\n            util.del_file(self.paths.instance_link)\n            (cfg_list, pkg_list) = self._get_datasources()\n            # Deep copy so that user-data handlers can not modify\n            # (which will affect user-data handlers down the line...)\n            (ds, dsname) = sources.find_source(\n                self.cfg,\n                self.distro,\n                self.paths,\n                copy.deepcopy(self.ds_deps),\n                cfg_list,\n                pkg_list,\n                self.reporter,\n            )\n            LOG.info(\"Loaded datasource %s - %s\", dsname, ds)\n        self.datasource = ds\n        # Ensure we adjust our path members datasource\n        # now that we have one (thus allowing ipath to be used)\n        self._reset()\n        return ds\n\n    def _get_instance_subdirs(self):\n        return [\"handlers\", \"scripts\", \"sem\"]\n\n    def _get_ipath(self, subname=None):\n        # Force a check to see if anything\n        # actually comes back, if not\n        # then a datasource has not been assigned...\n        instance_dir = self.paths.get_ipath(subname)\n        if not instance_dir:\n            raise RuntimeError(\n                \"No instance directory is available.\"\n                \" Has a datasource been fetched??\"\n            )\n        return instance_dir\n\n    def _reflect_cur_instance(self):\n        # Remove the old symlink and attach a new one so\n        # that further reads/writes connect into the right location\n        idir = self._get_ipath()\n        util.del_file(self.paths.instance_link)\n        util.sym_link(idir, self.paths.instance_link)\n\n        # Ensures these dirs exist\n        dir_list = []\n        for d in self._get_instance_subdirs():\n            dir_list.append(os.path.join(idir, d))\n        util.ensure_dirs(dir_list)\n\n        # Write out information on what is being used for the current instance\n        # and what may have been used for a previous instance...\n        dp = self.paths.get_cpath(\"data\")\n\n        # Write what the datasource was and is..\n        ds = \"%s: %s\" % (type_utils.obj_name(self.datasource), self.datasource)\n        previous_ds = None\n        ds_fn = os.path.join(idir, \"datasource\")\n        try:\n            previous_ds = util.load_file(ds_fn).strip()\n        except Exception:\n            pass\n        if not previous_ds:\n            previous_ds = ds\n        util.write_file(ds_fn, \"%s\\n\" % ds)\n        util.write_file(\n            os.path.join(dp, \"previous-datasource\"), \"%s\\n\" % (previous_ds)\n        )\n\n        # What the instance id was and is...\n        iid = self.datasource.get_instance_id()\n        iid_fn = os.path.join(dp, \"instance-id\")\n\n        previous_iid = self.previous_iid()\n        util.write_file(iid_fn, \"%s\\n\" % iid)\n        util.write_file(self.paths.get_runpath(\"instance_id\"), \"%s\\n\" % iid)\n        util.write_file(\n            os.path.join(dp, \"previous-instance-id\"), \"%s\\n\" % (previous_iid)\n        )\n\n        self._write_to_cache()\n        # Ensure needed components are regenerated\n        # after change of instance which may cause\n        # change of configuration\n        self._reset()\n        return iid\n\n    def previous_iid(self):\n        if self._previous_iid is not None:\n            return self._previous_iid\n\n        dp = self.paths.get_cpath(\"data\")\n        iid_fn = os.path.join(dp, \"instance-id\")\n        try:\n            self._previous_iid = util.load_file(iid_fn).strip()\n        except Exception:\n            self._previous_iid = NO_PREVIOUS_INSTANCE_ID\n\n        LOG.debug(\"previous iid found to be %s\", self._previous_iid)\n        return self._previous_iid\n\n    def is_new_instance(self):\n        \"\"\"Return true if this is a new instance.\n\n        If datasource has already been initialized, this will return False,\n        even on first boot.\n        \"\"\"\n        previous = self.previous_iid()\n        ret = (\n            previous == NO_PREVIOUS_INSTANCE_ID\n            or previous != self.datasource.get_instance_id()\n        )\n        return ret\n\n    def fetch(self, existing=\"check\"):\n        return self._get_data_source(existing=existing)\n\n    def instancify(self):\n        return self._reflect_cur_instance()\n\n    def cloudify(self):\n        # Form the needed options to cloudify our members\n        return cloud.Cloud(\n            self.datasource,\n            self.paths,\n            self.cfg,\n            self.distro,\n            helpers.Runners(self.paths),\n            reporter=self.reporter,\n        )\n\n    def update(self):\n        self._store_rawdata(self.datasource.get_userdata_raw(), \"userdata\")\n        self._store_processeddata(self.datasource.get_userdata(), \"userdata\")\n        self._store_raw_vendordata(\n            self.datasource.get_vendordata_raw(), \"vendordata\"\n        )\n        self._store_processeddata(\n            self.datasource.get_vendordata(), \"vendordata\"\n        )\n        self._store_raw_vendordata(\n            self.datasource.get_vendordata2_raw(), \"vendordata2\"\n        )\n        self._store_processeddata(\n            self.datasource.get_vendordata2(), \"vendordata2\"\n        )\n\n    def setup_datasource(self):\n        with events.ReportEventStack(\n            \"setup-datasource\", \"setting up datasource\", parent=self.reporter\n        ):\n            if self.datasource is None:\n                raise RuntimeError(\"Datasource is None, cannot setup.\")\n            self.datasource.setup(is_new_instance=self.is_new_instance())\n\n    def activate_datasource(self):\n        with events.ReportEventStack(\n            \"activate-datasource\",\n            \"activating datasource\",\n            parent=self.reporter,\n        ):\n            if self.datasource is None:\n                raise RuntimeError(\"Datasource is None, cannot activate.\")\n            self.datasource.activate(\n                cfg=self.cfg, is_new_instance=self.is_new_instance()\n            )\n            self._write_to_cache()\n\n    def _store_rawdata(self, data, datasource):\n        # Raw data is bytes, not a string\n        if data is None:\n            data = b\"\"\n        util.write_file(self._get_ipath(\"%s_raw\" % datasource), data, 0o600)\n\n    def _store_raw_vendordata(self, data, datasource):\n        # Only these data types\n        if data is not None and type(data) not in [bytes, str, list]:\n            raise TypeError(\n                \"vendordata_raw is unsupported type '%s'\" % str(type(data))\n            )\n        # This data may be a list, convert it to a string if so\n        if isinstance(data, list):\n            data = util.json_dumps(data)\n        self._store_rawdata(data, datasource)\n\n    def _store_processeddata(self, processed_data, datasource):\n        # processed is a Mime message, so write as string.\n        if processed_data is None:\n            processed_data = \"\"\n        util.write_file(\n            self._get_ipath(datasource), str(processed_data), 0o600\n        )\n\n    def _default_handlers(self, opts=None) -> List[handlers.Handler]:\n        if opts is None:\n            opts = {}\n\n        opts.update(\n            {\n                \"paths\": self.paths,\n                \"datasource\": self.datasource,\n            }\n        )\n        # TODO(harlowja) Hmmm, should we dynamically import these??\n        cloudconfig_handler = CloudConfigPartHandler(**opts)\n        shellscript_handler = ShellScriptPartHandler(**opts)\n        def_handlers = [\n            cloudconfig_handler,\n            shellscript_handler,\n            ShellScriptByFreqPartHandler(PER_ALWAYS, **opts),\n            ShellScriptByFreqPartHandler(PER_INSTANCE, **opts),\n            ShellScriptByFreqPartHandler(PER_ONCE, **opts),\n            BootHookPartHandler(**opts),\n            JinjaTemplatePartHandler(\n                **opts, sub_handlers=[cloudconfig_handler, shellscript_handler]\n            ),\n        ]\n        return def_handlers\n\n    def _default_vendordata_handlers(self):\n        return self._default_handlers(\n            opts={\n                \"script_path\": \"vendor_scripts\",\n                \"cloud_config_path\": \"vendor_cloud_config\",\n            }\n        )\n\n    def _default_vendordata2_handlers(self):\n        return self._default_handlers(\n            opts={\n                \"script_path\": \"vendor_scripts\",\n                \"cloud_config_path\": \"vendor2_cloud_config\",\n            }\n        )\n\n    def _do_handlers(\n        self, data_msg, c_handlers_list, frequency, excluded=None\n    ):\n        \"\"\"\n        Generalized handlers suitable for use with either vendordata\n        or userdata\n        \"\"\"\n        if excluded is None:\n            excluded = []\n\n        cdir = self.paths.get_cpath(\"handlers\")\n        idir = self._get_ipath(\"handlers\")\n\n        # Add the path to the plugins dir to the top of our list for importing\n        # new handlers.\n        #\n        # Note(harlowja): instance dir should be read before cloud-dir\n        for d in [cdir, idir]:\n            if d and d not in sys.path:\n                sys.path.insert(0, d)\n\n        def register_handlers_in_dir(path):\n            # Attempts to register any handler modules under the given path.\n            if not path or not os.path.isdir(path):\n                return\n            potential_handlers = util.get_modules_from_dir(path)\n            for (fname, mod_name) in potential_handlers.items():\n                try:\n                    mod_locs, looked_locs = importer.find_module(\n                        mod_name, [\"\"], [\"list_types\", \"handle_part\"]\n                    )\n                    if not mod_locs:\n                        LOG.warning(\n                            \"Could not find a valid user-data handler\"\n                            \" named %s in file %s (searched %s)\",\n                            mod_name,\n                            fname,\n                            looked_locs,\n                        )\n                        continue\n                    mod = importer.import_module(mod_locs[0])\n                    mod = handlers.fixup_handler(mod)\n                    types = c_handlers.register(mod)\n                    if types:\n                        LOG.debug(\n                            \"Added custom handler for %s [%s] from %s\",\n                            types,\n                            mod,\n                            fname,\n                        )\n                except Exception:\n                    util.logexc(\n                        LOG, \"Failed to register handler from %s\", fname\n                    )\n\n        # This keeps track of all the active handlers\n        c_handlers = helpers.ContentHandlers()\n\n        # Add any handlers in the cloud-dir\n        register_handlers_in_dir(cdir)\n\n        # Register any other handlers that come from the default set. This\n        # is done after the cloud-dir handlers so that the cdir modules can\n        # take over the default user-data handler content-types.\n        for mod in c_handlers_list:\n            types = c_handlers.register(mod, overwrite=False)\n            if types:\n                LOG.debug(\"Added default handler for %s from %s\", types, mod)\n\n        # Form our cloud interface\n        data = self.cloudify()\n\n        def init_handlers():\n            # Init the handlers first\n            for (_ctype, mod) in c_handlers.items():\n                if mod in c_handlers.initialized:\n                    # Avoid initiating the same module twice (if said module\n                    # is registered to more than one content-type).\n                    continue\n                handlers.call_begin(mod, data, frequency)\n                c_handlers.initialized.append(mod)\n\n        def walk_handlers(excluded):\n            # Walk the user data\n            part_data = {\n                \"handlers\": c_handlers,\n                # Any new handlers that are encountered get writen here\n                \"handlerdir\": idir,\n                \"data\": data,\n                # The default frequency if handlers don't have one\n                \"frequency\": frequency,\n                # This will be used when new handlers are found\n                # to help write their contents to files with numbered\n                # names...\n                \"handlercount\": 0,\n                \"excluded\": excluded,\n            }\n            handlers.walk(data_msg, handlers.walker_callback, data=part_data)\n\n        def finalize_handlers():\n            # Give callbacks opportunity to finalize\n            for (_ctype, mod) in c_handlers.items():\n                if mod not in c_handlers.initialized:\n                    # Said module was never inited in the first place, so lets\n                    # not attempt to finalize those that never got called.\n                    continue\n                c_handlers.initialized.remove(mod)\n                try:\n                    handlers.call_end(mod, data, frequency)\n                except Exception:\n                    util.logexc(LOG, \"Failed to finalize handler: %s\", mod)\n\n        try:\n            init_handlers()\n            walk_handlers(excluded)\n        finally:\n            finalize_handlers()\n\n    def consume_data(self, frequency=PER_INSTANCE):\n        # Consume the userdata first, because we need want to let the part\n        # handlers run first (for merging stuff)\n        with events.ReportEventStack(\n            \"consume-user-data\",\n            \"reading and applying user-data\",\n            parent=self.reporter,\n        ):\n            if util.get_cfg_option_bool(self.cfg, \"allow_userdata\", True):\n                self._consume_userdata(frequency)\n            else:\n                LOG.debug(\"allow_userdata = False: discarding user-data\")\n\n        with events.ReportEventStack(\n            \"consume-vendor-data\",\n            \"reading and applying vendor-data\",\n            parent=self.reporter,\n        ):\n            self._consume_vendordata(\"vendordata\", frequency)\n\n        with events.ReportEventStack(\n            \"consume-vendor-data2\",\n            \"reading and applying vendor-data2\",\n            parent=self.reporter,\n        ):\n            self._consume_vendordata(\"vendordata2\", frequency)\n\n        # Perform post-consumption adjustments so that\n        # modules that run during the init stage reflect\n        # this consumed set.\n        #\n        # They will be recreated on future access...\n        self._reset()\n        # Note(harlowja): the 'active' datasource will have\n        # references to the previous config, distro, paths\n        # objects before the load of the userdata happened,\n        # this is expected.\n\n    def _consume_vendordata(self, vendor_source, frequency=PER_INSTANCE):\n        \"\"\"\n        Consume the vendordata and run the part handlers on it\n        \"\"\"\n\n        # User-data should have been consumed first.\n        # So we merge the other available cloud-configs (everything except\n        # vendor provided), and check whether or not we should consume\n        # vendor data at all. That gives user or system a chance to override.\n        if vendor_source == \"vendordata\":\n            if not self.datasource.get_vendordata_raw():\n                LOG.debug(\"no vendordata from datasource\")\n                return\n            cfg_name = \"vendor_data\"\n        elif vendor_source == \"vendordata2\":\n            if not self.datasource.get_vendordata2_raw():\n                LOG.debug(\"no vendordata2 from datasource\")\n                return\n            cfg_name = \"vendor_data2\"\n        else:\n            raise RuntimeError(\n                \"vendor_source arg must be either 'vendordata'\"\n                \" or 'vendordata2'\"\n            )\n\n        _cc_merger = helpers.ConfigMerger(\n            paths=self._paths,\n            datasource=self.datasource,\n            additional_fns=[],\n            base_cfg=self.cfg,\n            include_vendor=False,\n        )\n        vdcfg = _cc_merger.cfg.get(cfg_name, {})\n\n        if not isinstance(vdcfg, dict):\n            vdcfg = {\"enabled\": False}\n            LOG.warning(\n                \"invalid %s setting. resetting to: %s\", cfg_name, vdcfg\n            )\n\n        enabled = vdcfg.get(\"enabled\")\n        no_handlers = vdcfg.get(\"disabled_handlers\", None)\n\n        if not util.is_true(enabled):\n            LOG.debug(\"%s consumption is disabled.\", vendor_source)\n            return\n\n        if isinstance(enabled, str):\n            util.deprecate(\n                deprecated=f\"Use of string '{enabled}' for \"\n                \"'vendor_data:enabled' field\",\n                deprecated_version=\"23.1\",\n                extra_message=\"Use boolean value instead.\",\n            )\n\n        LOG.debug(\n            \"%s will be consumed. disabled_handlers=%s\",\n            vendor_source,\n            no_handlers,\n        )\n\n        # Ensure vendordata source fetched before activation (just in case.)\n\n        # c_handlers_list keeps track of all the active handlers, while\n        # excluding what the users doesn't want run, i.e. boot_hook,\n        # cloud_config, shell_script\n        if vendor_source == \"vendordata\":\n            vendor_data_msg = self.datasource.get_vendordata()\n            c_handlers_list = self._default_vendordata_handlers()\n        else:\n            vendor_data_msg = self.datasource.get_vendordata2()\n            c_handlers_list = self._default_vendordata2_handlers()\n\n        # Run the handlers\n        self._do_handlers(\n            vendor_data_msg, c_handlers_list, frequency, excluded=no_handlers\n        )\n\n    def _consume_userdata(self, frequency=PER_INSTANCE):\n        \"\"\"\n        Consume the userdata and run the part handlers\n        \"\"\"\n\n        # Ensure datasource fetched before activation (just incase)\n        user_data_msg = self.datasource.get_userdata(True)\n\n        # This keeps track of all the active handlers\n        c_handlers_list = self._default_handlers()\n\n        # Run the handlers\n        self._do_handlers(user_data_msg, c_handlers_list, frequency)\n\n    def _get_network_key_contents(self, cfg) -> dict:\n        \"\"\"\n        Network configuration can be passed as a dict under a \"network\" key, or\n        optionally at the top level. In both cases, return the config.\n        \"\"\"\n        if cfg and \"network\" in cfg:\n            return cfg[\"network\"]\n        return cfg\n\n    def _find_networking_config(self):\n        disable_file = os.path.join(\n            self.paths.get_cpath(\"data\"), \"upgraded-network\"\n        )\n        if os.path.exists(disable_file):\n            return (None, disable_file)\n\n        available_cfgs = {\n            NetworkConfigSource.CMD_LINE: cmdline.read_kernel_cmdline_config(),\n            NetworkConfigSource.INITRAMFS: cmdline.read_initramfs_config(),\n            NetworkConfigSource.DS: None,\n            NetworkConfigSource.SYSTEM_CFG: self.cfg.get(\"network\"),\n        }\n\n        if self.datasource and hasattr(self.datasource, \"network_config\"):\n            available_cfgs[\n                NetworkConfigSource.DS\n            ] = self.datasource.network_config\n\n        if self.datasource:\n            order = self.datasource.network_config_sources\n        else:\n            order = sources.DataSource.network_config_sources\n        for cfg_source in order:\n            if not isinstance(cfg_source, NetworkConfigSource):\n                LOG.warning(\n                    \"data source specifies an invalid network cfg_source: %s\",\n                    cfg_source,\n                )\n                continue\n            if cfg_source not in available_cfgs:\n                LOG.warning(\n                    \"data source specifies an unavailable network\"\n                    \" cfg_source: %s\",\n                    cfg_source,\n                )\n                continue\n            ncfg = self._get_network_key_contents(available_cfgs[cfg_source])\n            if net.is_disabled_cfg(ncfg):\n                LOG.debug(\"network config disabled by %s\", cfg_source)\n                return (None, cfg_source)\n            if ncfg:\n                return (ncfg, cfg_source)\n        if not self.cfg.get(\"network\", True):\n            LOG.warning(\"Empty network config found\")\n        return (\n            self.distro.generate_fallback_config(),\n            NetworkConfigSource.FALLBACK,\n        )\n\n    def _apply_netcfg_names(self, netcfg):\n        try:\n            LOG.debug(\"applying net config names for %s\", netcfg)\n            self.distro.networking.apply_network_config_names(netcfg)\n        except Exception as e:\n            LOG.warning(\"Failed to rename devices: %s\", e)\n\n    def _get_per_boot_network_semaphore(self):\n        return namedtuple(\"Semaphore\", \"semaphore args\")(\n            helpers.FileSemaphores(self.paths.get_runpath(\"sem\")),\n            (\"apply_network_config\", PER_ONCE),\n        )\n\n    def _network_already_configured(self) -> bool:\n        sem = self._get_per_boot_network_semaphore()\n        return sem.semaphore.has_run(*sem.args)\n\n    def apply_network_config(self, bring_up):\n        \"\"\"Apply the network config.\n\n        Find the config, determine whether to apply it, apply it via\n        the distro, and optionally bring it up\n        \"\"\"\n        netcfg, src = self._find_networking_config()\n        if netcfg is None:\n            LOG.info(\"network config is disabled by %s\", src)\n            return\n\n        def event_enabled_and_metadata_updated(event_type):\n            return update_event_enabled(\n                datasource=self.datasource,\n                cfg=self.cfg,\n                event_source_type=event_type,\n                scope=EventScope.NETWORK,\n            ) and self.datasource.update_metadata_if_supported([event_type])\n\n        def should_run_on_boot_event():\n            return (\n                not self._network_already_configured()\n                and event_enabled_and_metadata_updated(EventType.BOOT)\n            )\n\n        if (\n            self.datasource is not None\n            and not self.is_new_instance()\n            and not should_run_on_boot_event()\n            and not event_enabled_and_metadata_updated(EventType.BOOT_LEGACY)\n        ):\n            LOG.debug(\n                \"No network config applied. Neither a new instance\"\n                \" nor datasource network update allowed\"\n            )\n            # nothing new, but ensure proper names\n            self._apply_netcfg_names(netcfg)\n            return\n\n        # refresh netcfg after update\n        netcfg, src = self._find_networking_config()\n\n        # ensure all physical devices in config are present\n        self.distro.networking.wait_for_physdevs(netcfg)\n\n        # apply renames from config\n        self._apply_netcfg_names(netcfg)\n\n        # rendering config\n        LOG.info(\n            \"Applying network configuration from %s bringup=%s: %s\",\n            src,\n            bring_up,\n            netcfg,\n        )\n\n        sem = self._get_per_boot_network_semaphore()\n        try:\n            with sem.semaphore.lock(*sem.args):\n                return self.distro.apply_network_config(\n                    netcfg, bring_up=bring_up\n                )\n        except net.RendererNotFoundError as e:\n            LOG.error(\n                \"Unable to render networking. Network config is \"\n                \"likely broken: %s\",\n                e,\n            )\n            return\n        except NotImplementedError:\n            LOG.warning(\n                \"distro '%s' does not implement apply_network_config. \"\n                \"networking may not be configured properly.\",\n                self.distro,\n            )\n            return\n\n\ndef read_runtime_config():\n    return util.read_conf(RUN_CLOUD_CONFIG)\n\n\ndef fetch_base_config(*, instance_data_file=None) -> dict:\n    return util.mergemanydict(\n        [\n            # builtin config, hardcoded in settings.py.\n            util.get_builtin_cfg(),\n            # Anything in your conf.d or 'default' cloud.cfg location.\n            util.read_conf_with_confd(\n                CLOUD_CONFIG, instance_data_file=instance_data_file\n            ),\n            # runtime config. I.e., /run/cloud-init/cloud.cfg\n            read_runtime_config(),\n            # Kernel/cmdline parameters override system config\n            util.read_conf_from_cmdline(),\n        ],\n        reverse=True,\n    )\n", "# This file is part of cloud-init. See LICENSE file for license information.\n\nimport copy\nimport inspect\nimport os\nimport stat\n\nfrom cloudinit import importer, util\nfrom cloudinit.event import EventScope, EventType\nfrom cloudinit.helpers import Paths\nfrom cloudinit.sources import (\n    EXPERIMENTAL_TEXT,\n    METADATA_UNKNOWN,\n    REDACT_SENSITIVE_VALUE,\n    UNSET,\n    DataSource,\n    canonical_cloud_id,\n    pkl_load,\n    redact_sensitive_keys,\n)\nfrom cloudinit.user_data import UserDataProcessor\nfrom tests.unittests.helpers import CiTestCase, mock\n\n\nclass DataSourceTestSubclassNet(DataSource):\n\n    dsname = \"MyTestSubclass\"\n    url_max_wait = 55\n\n    def __init__(\n        self,\n        sys_cfg,\n        distro,\n        paths,\n        custom_metadata=None,\n        custom_userdata=None,\n        get_data_retval=True,\n    ):\n        super(DataSourceTestSubclassNet, self).__init__(sys_cfg, distro, paths)\n        self._custom_userdata = custom_userdata\n        self._custom_metadata = custom_metadata\n        self._get_data_retval = get_data_retval\n\n    def _get_cloud_name(self):\n        return \"SubclassCloudName\"\n\n    def _get_data(self):\n        if self._custom_metadata:\n            self.metadata = self._custom_metadata\n        else:\n            self.metadata = {\n                \"availability_zone\": \"myaz\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"region\": \"myregion\",\n            }\n        if self._custom_userdata:\n            self.userdata_raw = self._custom_userdata\n        else:\n            self.userdata_raw = \"userdata_raw\"\n        self.vendordata_raw = \"vendordata_raw\"\n        return self._get_data_retval\n\n\nclass InvalidDataSourceTestSubclassNet(DataSource):\n    pass\n\n\nclass TestDataSource(CiTestCase):\n\n    with_logs = True\n    maxDiff = None\n\n    def setUp(self):\n        super(TestDataSource, self).setUp()\n        self.sys_cfg = {\"datasource\": {\"_undef\": {\"key1\": False}}}\n        self.distro = \"distrotest\"  # generally should be a Distro object\n        self.paths = Paths({})\n        self.datasource = DataSource(self.sys_cfg, self.distro, self.paths)\n\n    def test_datasource_init(self):\n        \"\"\"DataSource initializes metadata attributes, ds_cfg and ud_proc.\"\"\"\n        self.assertEqual(self.paths, self.datasource.paths)\n        self.assertEqual(self.sys_cfg, self.datasource.sys_cfg)\n        self.assertEqual(self.distro, self.datasource.distro)\n        self.assertIsNone(self.datasource.userdata)\n        self.assertEqual({}, self.datasource.metadata)\n        self.assertIsNone(self.datasource.userdata_raw)\n        self.assertIsNone(self.datasource.vendordata)\n        self.assertIsNone(self.datasource.vendordata_raw)\n        self.assertEqual({\"key1\": False}, self.datasource.ds_cfg)\n        self.assertIsInstance(self.datasource.ud_proc, UserDataProcessor)\n\n    def test_datasource_init_gets_ds_cfg_using_dsname(self):\n        \"\"\"Init uses DataSource.dsname for sourcing ds_cfg.\"\"\"\n        sys_cfg = {\"datasource\": {\"MyTestSubclass\": {\"key2\": False}}}\n        distro = \"distrotest\"  # generally should be a Distro object\n        datasource = DataSourceTestSubclassNet(sys_cfg, distro, self.paths)\n        self.assertEqual({\"key2\": False}, datasource.ds_cfg)\n\n    def test_str_is_classname(self):\n        \"\"\"The string representation of the datasource is the classname.\"\"\"\n        self.assertEqual(\"DataSource\", str(self.datasource))\n        self.assertEqual(\n            \"DataSourceTestSubclassNet\",\n            str(DataSourceTestSubclassNet(\"\", \"\", self.paths)),\n        )\n\n    def test_datasource_get_url_params_defaults(self):\n        \"\"\"get_url_params default url config settings for the datasource.\"\"\"\n        params = self.datasource.get_url_params()\n        self.assertEqual(params.max_wait_seconds, self.datasource.url_max_wait)\n        self.assertEqual(params.timeout_seconds, self.datasource.url_timeout)\n        self.assertEqual(params.num_retries, self.datasource.url_retries)\n        self.assertEqual(\n            params.sec_between_retries, self.datasource.url_sec_between_retries\n        )\n\n    def test_datasource_get_url_params_subclassed(self):\n        \"\"\"Subclasses can override get_url_params defaults.\"\"\"\n        sys_cfg = {\"datasource\": {\"MyTestSubclass\": {\"key2\": False}}}\n        distro = \"distrotest\"  # generally should be a Distro object\n        datasource = DataSourceTestSubclassNet(sys_cfg, distro, self.paths)\n        expected = (\n            datasource.url_max_wait,\n            datasource.url_timeout,\n            datasource.url_retries,\n            datasource.url_sec_between_retries,\n        )\n        url_params = datasource.get_url_params()\n        self.assertNotEqual(self.datasource.get_url_params(), url_params)\n        self.assertEqual(expected, url_params)\n\n    def test_datasource_get_url_params_ds_config_override(self):\n        \"\"\"Datasource configuration options can override url param defaults.\"\"\"\n        sys_cfg = {\n            \"datasource\": {\n                \"MyTestSubclass\": {\n                    \"max_wait\": \"1\",\n                    \"timeout\": \"2\",\n                    \"retries\": \"3\",\n                    \"sec_between_retries\": 4,\n                }\n            }\n        }\n        datasource = DataSourceTestSubclassNet(\n            sys_cfg, self.distro, self.paths\n        )\n        expected = (1, 2, 3, 4)\n        url_params = datasource.get_url_params()\n        self.assertNotEqual(\n            (\n                datasource.url_max_wait,\n                datasource.url_timeout,\n                datasource.url_retries,\n                datasource.url_sec_between_retries,\n            ),\n            url_params,\n        )\n        self.assertEqual(expected, url_params)\n\n    def test_datasource_get_url_params_is_zero_or_greater(self):\n        \"\"\"get_url_params ignores timeouts with a value below 0.\"\"\"\n        # Set an override that is below 0 which gets ignored.\n        sys_cfg = {\"datasource\": {\"_undef\": {\"timeout\": \"-1\"}}}\n        datasource = DataSource(sys_cfg, self.distro, self.paths)\n        (\n            _max_wait,\n            timeout,\n            _retries,\n            _sec_between_retries,\n        ) = datasource.get_url_params()\n        self.assertEqual(0, timeout)\n\n    def test_datasource_get_url_uses_defaults_on_errors(self):\n        \"\"\"On invalid system config values for url_params defaults are used.\"\"\"\n        # All invalid values should be logged\n        sys_cfg = {\n            \"datasource\": {\n                \"_undef\": {\n                    \"max_wait\": \"nope\",\n                    \"timeout\": \"bug\",\n                    \"retries\": \"nonint\",\n                }\n            }\n        }\n        datasource = DataSource(sys_cfg, self.distro, self.paths)\n        url_params = datasource.get_url_params()\n        expected = (\n            datasource.url_max_wait,\n            datasource.url_timeout,\n            datasource.url_retries,\n            datasource.url_sec_between_retries,\n        )\n        self.assertEqual(expected, url_params)\n        logs = self.logs.getvalue()\n        expected_logs = [\n            \"Config max_wait 'nope' is not an int, using default '-1'\",\n            \"Config timeout 'bug' is not an int, using default '10'\",\n            \"Config retries 'nonint' is not an int, using default '5'\",\n        ]\n        for log in expected_logs:\n            self.assertIn(log, logs)\n\n    @mock.patch(\"cloudinit.sources.net.find_fallback_nic\")\n    def test_fallback_interface_is_discovered(self, m_get_fallback_nic):\n        \"\"\"The fallback_interface is discovered via find_fallback_nic.\"\"\"\n        m_get_fallback_nic.return_value = \"nic9\"\n        self.assertEqual(\"nic9\", self.datasource.fallback_interface)\n\n    @mock.patch(\"cloudinit.sources.net.find_fallback_nic\")\n    def test_fallback_interface_logs_undiscovered(self, m_get_fallback_nic):\n        \"\"\"Log a warning when fallback_interface can not discover the nic.\"\"\"\n        self.datasource._cloud_name = \"MySupahCloud\"\n        m_get_fallback_nic.return_value = None  # Couldn't discover nic\n        self.assertIsNone(self.datasource.fallback_interface)\n        self.assertEqual(\n            \"WARNING: Did not find a fallback interface on MySupahCloud.\\n\",\n            self.logs.getvalue(),\n        )\n\n    @mock.patch(\"cloudinit.sources.net.find_fallback_nic\")\n    def test_wb_fallback_interface_is_cached(self, m_get_fallback_nic):\n        \"\"\"The fallback_interface is cached and won't be rediscovered.\"\"\"\n        self.datasource._fallback_interface = \"nic10\"\n        self.assertEqual(\"nic10\", self.datasource.fallback_interface)\n        m_get_fallback_nic.assert_not_called()\n\n    def test__get_data_unimplemented(self):\n        \"\"\"Raise an error when _get_data is not implemented.\"\"\"\n        with self.assertRaises(NotImplementedError) as context_manager:\n            self.datasource.get_data()\n        self.assertIn(\n            \"Subclasses of DataSource must implement _get_data\",\n            str(context_manager.exception),\n        )\n        datasource2 = InvalidDataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, self.paths\n        )\n        with self.assertRaises(NotImplementedError) as context_manager:\n            datasource2.get_data()\n        self.assertIn(\n            \"Subclasses of DataSource must implement _get_data\",\n            str(context_manager.exception),\n        )\n\n    def test_get_data_calls_subclass__get_data(self):\n        \"\"\"Datasource.get_data uses the subclass' version of _get_data.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertTrue(datasource.get_data())\n        self.assertEqual(\n            {\n                \"availability_zone\": \"myaz\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"region\": \"myregion\",\n            },\n            datasource.metadata,\n        )\n        self.assertEqual(\"userdata_raw\", datasource.userdata_raw)\n        self.assertEqual(\"vendordata_raw\", datasource.vendordata_raw)\n\n    def test_get_hostname_strips_local_hostname_without_domain(self):\n        \"\"\"Datasource.get_hostname strips metadata local-hostname of domain.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertTrue(datasource.get_data())\n        self.assertEqual(\n            \"test-subclass-hostname\", datasource.metadata[\"local-hostname\"]\n        )\n        self.assertEqual(\n            \"test-subclass-hostname\", datasource.get_hostname().hostname\n        )\n        datasource.metadata[\"local-hostname\"] = \"hostname.my.domain.com\"\n        self.assertEqual(\"hostname\", datasource.get_hostname().hostname)\n\n    def test_get_hostname_with_fqdn_returns_local_hostname_with_domain(self):\n        \"\"\"Datasource.get_hostname with fqdn set gets qualified hostname.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertTrue(datasource.get_data())\n        datasource.metadata[\"local-hostname\"] = \"hostname.my.domain.com\"\n        self.assertEqual(\n            \"hostname.my.domain.com\",\n            datasource.get_hostname(fqdn=True).hostname,\n        )\n\n    def test_get_hostname_without_metadata_uses_system_hostname(self):\n        \"\"\"Datasource.gethostname runs util.get_hostname when no metadata.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertEqual({}, datasource.metadata)\n        mock_fqdn = \"cloudinit.sources.util.get_fqdn_from_hosts\"\n        with mock.patch(\"cloudinit.sources.util.get_hostname\") as m_gethost:\n            with mock.patch(mock_fqdn) as m_fqdn:\n                m_gethost.return_value = \"systemhostname.domain.com\"\n                m_fqdn.return_value = None  # No maching fqdn in /etc/hosts\n                self.assertEqual(\n                    \"systemhostname\", datasource.get_hostname().hostname\n                )\n                self.assertEqual(\n                    \"systemhostname.domain.com\",\n                    datasource.get_hostname(fqdn=True).hostname,\n                )\n\n    def test_get_hostname_without_metadata_returns_none(self):\n        \"\"\"Datasource.gethostname returns None when metadata_only and no MD.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertEqual({}, datasource.metadata)\n        mock_fqdn = \"cloudinit.sources.util.get_fqdn_from_hosts\"\n        with mock.patch(\"cloudinit.sources.util.get_hostname\") as m_gethost:\n            with mock.patch(mock_fqdn) as m_fqdn:\n                self.assertIsNone(\n                    datasource.get_hostname(metadata_only=True).hostname\n                )\n                self.assertIsNone(\n                    datasource.get_hostname(\n                        fqdn=True, metadata_only=True\n                    ).hostname\n                )\n        self.assertEqual([], m_gethost.call_args_list)\n        self.assertEqual([], m_fqdn.call_args_list)\n\n    def test_get_hostname_without_metadata_prefers_etc_hosts(self):\n        \"\"\"Datasource.gethostname prefers /etc/hosts to util.get_hostname.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertEqual({}, datasource.metadata)\n        mock_fqdn = \"cloudinit.sources.util.get_fqdn_from_hosts\"\n        with mock.patch(\"cloudinit.sources.util.get_hostname\") as m_gethost:\n            with mock.patch(mock_fqdn) as m_fqdn:\n                m_gethost.return_value = \"systemhostname.domain.com\"\n                m_fqdn.return_value = \"fqdnhostname.domain.com\"\n                self.assertEqual(\n                    \"fqdnhostname\", datasource.get_hostname().hostname\n                )\n                self.assertEqual(\n                    \"fqdnhostname.domain.com\",\n                    datasource.get_hostname(fqdn=True).hostname,\n                )\n\n    def test_get_data_does_not_write_instance_data_on_failure(self):\n        \"\"\"get_data does not write INSTANCE_JSON_FILE on get_data False.\"\"\"\n        tmp = self.tmp_dir()\n        paths = Paths({\"run_dir\": tmp})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n            get_data_retval=False,\n        )\n        self.assertFalse(datasource.get_data())\n        json_file = paths.get_runpath(\"instance_data\")\n        self.assertFalse(\n            os.path.exists(json_file), f\"Found unexpected file {json_file}\"\n        )\n\n    def test_get_data_writes_json_instance_data_on_success(self):\n        \"\"\"get_data writes INSTANCE_JSON_FILE to run_dir as world readable.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        sys_info = {\n            \"python\": \"3.7\",\n            \"platform\": (\n                \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n            ),\n            \"uname\": [\n                \"Linux\",\n                \"myhost\",\n                \"5.4.0-24-generic\",\n                \"SMP blah\",\n                \"x86_64\",\n            ],\n            \"variant\": \"ubuntu\",\n            \"dist\": [\"ubuntu\", \"20.04\", \"focal\"],\n        }\n        with mock.patch(\"cloudinit.util.system_info\", return_value=sys_info):\n            with mock.patch(\n                \"cloudinit.sources.canonical_cloud_id\",\n                return_value=\"canonical_cloud_id\",\n            ):\n                datasource.get_data()\n        json_file = Paths({\"run_dir\": tmp}).get_runpath(\"instance_data\")\n        content = util.load_file(json_file)\n        expected = {\n            \"base64_encoded_keys\": [],\n            \"merged_cfg\": REDACT_SENSITIVE_VALUE,\n            \"sensitive_keys\": [\"merged_cfg\"],\n            \"sys_info\": sys_info,\n            \"v1\": {\n                \"_beta_keys\": [\"subplatform\"],\n                \"availability-zone\": \"myaz\",\n                \"availability_zone\": \"myaz\",\n                \"cloud_id\": \"canonical_cloud_id\",\n                \"cloud-name\": \"subclasscloudname\",\n                \"cloud_name\": \"subclasscloudname\",\n                \"distro\": \"ubuntu\",\n                \"distro_release\": \"focal\",\n                \"distro_version\": \"20.04\",\n                \"instance-id\": \"iid-datasource\",\n                \"instance_id\": \"iid-datasource\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"local_hostname\": \"test-subclass-hostname\",\n                \"kernel_release\": \"5.4.0-24-generic\",\n                \"machine\": \"x86_64\",\n                \"platform\": \"mytestsubclass\",\n                \"public_ssh_keys\": [],\n                \"python_version\": \"3.7\",\n                \"region\": \"myregion\",\n                \"system_platform\": (\n                    \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n                ),\n                \"subplatform\": \"unknown\",\n                \"variant\": \"ubuntu\",\n            },\n            \"ds\": {\n                \"_doc\": EXPERIMENTAL_TEXT,\n                \"meta_data\": {\n                    \"availability_zone\": \"myaz\",\n                    \"local-hostname\": \"test-subclass-hostname\",\n                    \"region\": \"myregion\",\n                },\n            },\n        }\n        self.assertEqual(expected, util.load_json(content))\n        file_stat = os.stat(json_file)\n        self.assertEqual(0o644, stat.S_IMODE(file_stat.st_mode))\n        self.assertEqual(expected, util.load_json(content))\n\n    def test_get_data_writes_redacted_public_json_instance_data(self):\n        \"\"\"get_data writes redacted content to public INSTANCE_JSON_FILE.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            Paths({\"run_dir\": tmp}),\n            custom_metadata={\n                \"availability_zone\": \"myaz\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"region\": \"myregion\",\n                \"some\": {\n                    \"security-credentials\": {\n                        \"cred1\": \"sekret\",\n                        \"cred2\": \"othersekret\",\n                    }\n                },\n            },\n        )\n        self.assertCountEqual(\n            (\n                \"merged_cfg\",\n                \"security-credentials\",\n            ),\n            datasource.sensitive_metadata_keys,\n        )\n        sys_info = {\n            \"python\": \"3.7\",\n            \"platform\": (\n                \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n            ),\n            \"uname\": [\n                \"Linux\",\n                \"myhost\",\n                \"5.4.0-24-generic\",\n                \"SMP blah\",\n                \"x86_64\",\n            ],\n            \"variant\": \"ubuntu\",\n            \"dist\": [\"ubuntu\", \"20.04\", \"focal\"],\n        }\n        with mock.patch(\"cloudinit.util.system_info\", return_value=sys_info):\n            datasource.get_data()\n        json_file = Paths({\"run_dir\": tmp}).get_runpath(\"instance_data\")\n        redacted = util.load_json(util.load_file(json_file))\n        expected = {\n            \"base64_encoded_keys\": [],\n            \"merged_cfg\": REDACT_SENSITIVE_VALUE,\n            \"sensitive_keys\": [\n                \"ds/meta_data/some/security-credentials\",\n                \"merged_cfg\",\n            ],\n            \"sys_info\": sys_info,\n            \"v1\": {\n                \"_beta_keys\": [\"subplatform\"],\n                \"availability-zone\": \"myaz\",\n                \"availability_zone\": \"myaz\",\n                \"cloud-name\": \"subclasscloudname\",\n                \"cloud_name\": \"subclasscloudname\",\n                \"distro\": \"ubuntu\",\n                \"distro_release\": \"focal\",\n                \"distro_version\": \"20.04\",\n                \"instance-id\": \"iid-datasource\",\n                \"instance_id\": \"iid-datasource\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"local_hostname\": \"test-subclass-hostname\",\n                \"kernel_release\": \"5.4.0-24-generic\",\n                \"machine\": \"x86_64\",\n                \"platform\": \"mytestsubclass\",\n                \"public_ssh_keys\": [],\n                \"python_version\": \"3.7\",\n                \"region\": \"myregion\",\n                \"system_platform\": (\n                    \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n                ),\n                \"subplatform\": \"unknown\",\n                \"variant\": \"ubuntu\",\n            },\n            \"ds\": {\n                \"_doc\": EXPERIMENTAL_TEXT,\n                \"meta_data\": {\n                    \"availability_zone\": \"myaz\",\n                    \"local-hostname\": \"test-subclass-hostname\",\n                    \"region\": \"myregion\",\n                    \"some\": {\"security-credentials\": REDACT_SENSITIVE_VALUE},\n                },\n            },\n        }\n        self.assertCountEqual(expected, redacted)\n        file_stat = os.stat(json_file)\n        self.assertEqual(0o644, stat.S_IMODE(file_stat.st_mode))\n\n    def test_get_data_writes_json_instance_data_sensitive(self):\n        \"\"\"\n        get_data writes unmodified data to sensitive file as root-readonly.\n        \"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            Paths({\"run_dir\": tmp}),\n            custom_metadata={\n                \"availability_zone\": \"myaz\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"region\": \"myregion\",\n                \"some\": {\n                    \"security-credentials\": {\n                        \"cred1\": \"sekret\",\n                        \"cred2\": \"othersekret\",\n                    }\n                },\n            },\n        )\n        sys_info = {\n            \"python\": \"3.7\",\n            \"platform\": (\n                \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n            ),\n            \"uname\": [\n                \"Linux\",\n                \"myhost\",\n                \"5.4.0-24-generic\",\n                \"SMP blah\",\n                \"x86_64\",\n            ],\n            \"variant\": \"ubuntu\",\n            \"dist\": [\"ubuntu\", \"20.04\", \"focal\"],\n        }\n\n        self.assertCountEqual(\n            (\n                \"merged_cfg\",\n                \"security-credentials\",\n            ),\n            datasource.sensitive_metadata_keys,\n        )\n        with mock.patch(\"cloudinit.util.system_info\", return_value=sys_info):\n            with mock.patch(\n                \"cloudinit.sources.canonical_cloud_id\",\n                return_value=\"canonical-cloud-id\",\n            ):\n                datasource.get_data()\n        sensitive_json_file = Paths({\"run_dir\": tmp}).get_runpath(\n            \"instance_data_sensitive\"\n        )\n        content = util.load_file(sensitive_json_file)\n        expected = {\n            \"base64_encoded_keys\": [],\n            \"merged_cfg\": {\n                \"_doc\": (\n                    \"Merged cloud-init system config from \"\n                    \"/etc/cloud/cloud.cfg and /etc/cloud/cloud.cfg.d/\"\n                ),\n                \"datasource\": {\"_undef\": {\"key1\": False}},\n            },\n            \"sensitive_keys\": [\n                \"ds/meta_data/some/security-credentials\",\n                \"merged_cfg\",\n            ],\n            \"sys_info\": sys_info,\n            \"v1\": {\n                \"_beta_keys\": [\"subplatform\"],\n                \"availability-zone\": \"myaz\",\n                \"availability_zone\": \"myaz\",\n                \"cloud_id\": \"canonical-cloud-id\",\n                \"cloud-name\": \"subclasscloudname\",\n                \"cloud_name\": \"subclasscloudname\",\n                \"distro\": \"ubuntu\",\n                \"distro_release\": \"focal\",\n                \"distro_version\": \"20.04\",\n                \"instance-id\": \"iid-datasource\",\n                \"instance_id\": \"iid-datasource\",\n                \"kernel_release\": \"5.4.0-24-generic\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"local_hostname\": \"test-subclass-hostname\",\n                \"machine\": \"x86_64\",\n                \"platform\": \"mytestsubclass\",\n                \"public_ssh_keys\": [],\n                \"python_version\": \"3.7\",\n                \"region\": \"myregion\",\n                \"subplatform\": \"unknown\",\n                \"system_platform\": (\n                    \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n                ),\n                \"variant\": \"ubuntu\",\n            },\n            \"ds\": {\n                \"_doc\": EXPERIMENTAL_TEXT,\n                \"meta_data\": {\n                    \"availability_zone\": \"myaz\",\n                    \"local-hostname\": \"test-subclass-hostname\",\n                    \"region\": \"myregion\",\n                    \"some\": {\n                        \"security-credentials\": {\n                            \"cred1\": \"sekret\",\n                            \"cred2\": \"othersekret\",\n                        }\n                    },\n                },\n            },\n        }\n        self.assertCountEqual(expected, util.load_json(content))\n        file_stat = os.stat(sensitive_json_file)\n        self.assertEqual(0o600, stat.S_IMODE(file_stat.st_mode))\n        self.assertEqual(expected, util.load_json(content))\n\n    def test_get_data_handles_redacted_unserializable_content(self):\n        \"\"\"get_data warns unserializable content in INSTANCE_JSON_FILE.\"\"\"\n        tmp = self.tmp_dir()\n        paths = Paths({\"run_dir\": tmp})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n            custom_metadata={\"key1\": \"val1\", \"key2\": {\"key2.1\": self.paths}},\n        )\n        datasource.get_data()\n        json_file = paths.get_runpath(\"instance_data\")\n        content = util.load_file(json_file)\n        expected_metadata = {\n            \"key1\": \"val1\",\n            \"key2\": {\n                \"key2.1\": (\n                    \"Warning: redacted unserializable type <class\"\n                    \" 'cloudinit.helpers.Paths'>\"\n                )\n            },\n        }\n        instance_json = util.load_json(content)\n        self.assertEqual(expected_metadata, instance_json[\"ds\"][\"meta_data\"])\n\n    def test_persist_instance_data_writes_ec2_metadata_when_set(self):\n        \"\"\"When ec2_metadata class attribute is set, persist to json.\"\"\"\n        tmp = self.tmp_dir()\n        cloud_dir = os.path.join(tmp, \"cloud\")\n        util.ensure_dir(cloud_dir)\n        paths = Paths({\"run_dir\": tmp, \"cloud_dir\": cloud_dir})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n        )\n        datasource.ec2_metadata = UNSET\n        datasource.get_data()\n        json_file = paths.get_runpath(\"instance_data\")\n        instance_data = util.load_json(util.load_file(json_file))\n        self.assertNotIn(\"ec2_metadata\", instance_data[\"ds\"])\n        datasource.ec2_metadata = {\"ec2stuff\": \"is good\"}\n        datasource.persist_instance_data()\n        instance_data = util.load_json(util.load_file(json_file))\n        self.assertEqual(\n            {\"ec2stuff\": \"is good\"}, instance_data[\"ds\"][\"ec2_metadata\"]\n        )\n\n    def test_persist_instance_data_writes_canonical_cloud_id_and_symlink(self):\n        \"\"\"canonical-cloud-id class attribute is set, persist to json.\"\"\"\n        tmp = self.tmp_dir()\n        cloud_dir = os.path.join(tmp, \"cloud\")\n        util.ensure_dir(cloud_dir)\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            Paths({\"run_dir\": tmp, \"cloud_dir\": cloud_dir}),\n        )\n        cloud_id_link = os.path.join(tmp, \"cloud-id\")\n        cloud_id_file = os.path.join(tmp, \"cloud-id-my-cloud\")\n        cloud_id2_file = os.path.join(tmp, \"cloud-id-my-cloud2\")\n        for filename in (cloud_id_file, cloud_id_link, cloud_id2_file):\n            self.assertFalse(\n                os.path.exists(filename), \"Unexpected link found {filename}\"\n            )\n        with mock.patch(\n            \"cloudinit.sources.canonical_cloud_id\", return_value=\"my-cloud\"\n        ):\n            datasource.get_data()\n            self.assertEqual(\"my-cloud\\n\", util.load_file(cloud_id_link))\n            # A symlink with the generic /run/cloud-init/cloud-id\n            # link is present\n            self.assertTrue(util.is_link(cloud_id_link))\n            datasource.persist_instance_data()\n            # cloud-id<cloud-type> not deleted: no cloud-id change\n            self.assertTrue(os.path.exists(cloud_id_file))\n        # When cloud-id changes, symlink and content change\n        with mock.patch(\n            \"cloudinit.sources.canonical_cloud_id\", return_value=\"my-cloud2\"\n        ):\n            datasource.persist_instance_data()\n        self.assertEqual(\"my-cloud2\\n\", util.load_file(cloud_id2_file))\n        # Previous cloud-id-<cloud-type> file removed\n        self.assertFalse(os.path.exists(cloud_id_file))\n        # Generic link persisted which contains canonical-cloud-id as content\n        self.assertTrue(util.is_link(cloud_id_link))\n        self.assertEqual(\"my-cloud2\\n\", util.load_file(cloud_id_link))\n\n    def test_persist_instance_data_writes_network_json_when_set(self):\n        \"\"\"When network_data.json class attribute is set, persist to json.\"\"\"\n        tmp = self.tmp_dir()\n        cloud_dir = os.path.join(tmp, \"cloud\")\n        util.ensure_dir(cloud_dir)\n        paths = Paths({\"run_dir\": tmp, \"cloud_dir\": cloud_dir})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n        )\n        datasource.get_data()\n        json_file = paths.get_runpath(\"instance_data\")\n        instance_data = util.load_json(util.load_file(json_file))\n        self.assertNotIn(\"network_json\", instance_data[\"ds\"])\n        datasource.network_json = {\"network_json\": \"is good\"}\n        datasource.persist_instance_data()\n        instance_data = util.load_json(util.load_file(json_file))\n        self.assertEqual(\n            {\"network_json\": \"is good\"}, instance_data[\"ds\"][\"network_json\"]\n        )\n\n    def test_persist_instance_serializes_datasource_pickle(self):\n        \"\"\"obj.pkl is written when instance link present and write_cache.\"\"\"\n        tmp = self.tmp_dir()\n        cloud_dir = os.path.join(tmp, \"cloud\")\n        util.ensure_dir(cloud_dir)\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            Paths({\"run_dir\": tmp, \"cloud_dir\": cloud_dir}),\n        )\n        pkl_cache_file = os.path.join(cloud_dir, \"instance/obj.pkl\")\n        self.assertFalse(os.path.exists(pkl_cache_file))\n        datasource.network_json = {\"network_json\": \"is good\"}\n        # No /var/lib/cloud/instance symlink\n        datasource.persist_instance_data(write_cache=True)\n        self.assertFalse(os.path.exists(pkl_cache_file))\n\n        # Symlink /var/lib/cloud/instance but write_cache=False\n        util.sym_link(cloud_dir, os.path.join(cloud_dir, \"instance\"))\n        datasource.persist_instance_data(write_cache=False)\n        self.assertFalse(os.path.exists(pkl_cache_file))\n\n        # Symlink /var/lib/cloud/instance and write_cache=True\n        datasource.persist_instance_data(write_cache=True)\n        self.assertTrue(os.path.exists(pkl_cache_file))\n        ds = pkl_load(pkl_cache_file)\n        self.assertEqual(datasource.network_json, ds.network_json)\n\n    def test_get_data_base64encodes_unserializable_bytes(self):\n        \"\"\"On py3, get_data base64encodes any unserializable content.\"\"\"\n        tmp = self.tmp_dir()\n        paths = Paths({\"run_dir\": tmp})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n            custom_metadata={\"key1\": \"val1\", \"key2\": {\"key2.1\": b\"\\x123\"}},\n        )\n        self.assertTrue(datasource.get_data())\n        json_file = paths.get_runpath(\"instance_data\")\n        content = util.load_file(json_file)\n        instance_json = util.load_json(content)\n        self.assertCountEqual(\n            [\"ds/meta_data/key2/key2.1\"], instance_json[\"base64_encoded_keys\"]\n        )\n        self.assertEqual(\n            {\"key1\": \"val1\", \"key2\": {\"key2.1\": \"EjM=\"}},\n            instance_json[\"ds\"][\"meta_data\"],\n        )\n\n    def test_get_hostname_subclass_support(self):\n        \"\"\"Validate get_hostname signature on all subclasses of DataSource.\"\"\"\n        base_args = inspect.getfullargspec(DataSource.get_hostname)\n        # Import all DataSource subclasses so we can inspect them.\n        modules = util.get_modules_from_dir(\n            os.path.dirname(os.path.dirname(__file__))\n        )\n        for _loc, name in modules.items():\n            mod_locs, _ = importer.find_module(name, [\"cloudinit.sources\"], [])\n            if mod_locs:\n                importer.import_module(mod_locs[0])\n        for child in DataSource.__subclasses__():\n            if \"Test\" in child.dsname:\n                continue\n            self.assertEqual(\n                base_args,\n                inspect.getfullargspec(child.get_hostname),\n                \"%s does not implement DataSource.get_hostname params\" % child,\n            )\n            for grandchild in child.__subclasses__():\n                self.assertEqual(\n                    base_args,\n                    inspect.getfullargspec(grandchild.get_hostname),\n                    \"%s does not implement DataSource.get_hostname params\"\n                    % grandchild,\n                )\n\n    def test_clear_cached_attrs_resets_cached_attr_class_attributes(self):\n        \"\"\"Class attributes listed in cached_attr_defaults are reset.\"\"\"\n        count = 0\n        # Setup values for all cached class attributes\n        for attr, value in self.datasource.cached_attr_defaults:\n            setattr(self.datasource, attr, count)\n            count += 1\n        self.datasource._dirty_cache = True\n        self.datasource.clear_cached_attrs()\n        for attr, value in self.datasource.cached_attr_defaults:\n            self.assertEqual(value, getattr(self.datasource, attr))\n\n    def test_clear_cached_attrs_noops_on_clean_cache(self):\n        \"\"\"Class attributes listed in cached_attr_defaults are reset.\"\"\"\n        count = 0\n        # Setup values for all cached class attributes\n        for attr, _ in self.datasource.cached_attr_defaults:\n            setattr(self.datasource, attr, count)\n            count += 1\n        self.datasource._dirty_cache = False  # Fake clean cache\n        self.datasource.clear_cached_attrs()\n        count = 0\n        for attr, _ in self.datasource.cached_attr_defaults:\n            self.assertEqual(count, getattr(self.datasource, attr))\n            count += 1\n\n    def test_clear_cached_attrs_skips_non_attr_class_attributes(self):\n        \"\"\"Skip any cached_attr_defaults which aren't class attributes.\"\"\"\n        self.datasource._dirty_cache = True\n        self.datasource.clear_cached_attrs()\n        for attr in (\"ec2_metadata\", \"network_json\"):\n            self.assertFalse(hasattr(self.datasource, attr))\n\n    def test_clear_cached_attrs_of_custom_attrs(self):\n        \"\"\"Custom attr_values can be passed to clear_cached_attrs.\"\"\"\n        self.datasource._dirty_cache = True\n        cached_attr_name = self.datasource.cached_attr_defaults[0][0]\n        setattr(self.datasource, cached_attr_name, \"himom\")\n        self.datasource.myattr = \"orig\"\n        self.datasource.clear_cached_attrs(\n            attr_defaults=((\"myattr\", \"updated\"),)\n        )\n        self.assertEqual(\"himom\", getattr(self.datasource, cached_attr_name))\n        self.assertEqual(\"updated\", self.datasource.myattr)\n\n    @mock.patch.dict(\n        DataSource.default_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    @mock.patch.dict(\n        DataSource.supported_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_update_metadata_only_acts_on_supported_update_events(self):\n        \"\"\"update_metadata_if_supported wont get_data on unsupported events.\"\"\"\n        self.assertEqual(\n            {EventScope.NETWORK: set([EventType.BOOT_NEW_INSTANCE])},\n            self.datasource.default_update_events,\n        )\n\n        fake_get_data = mock.Mock()\n        self.datasource.get_data = fake_get_data\n        self.assertFalse(\n            self.datasource.update_metadata_if_supported(\n                source_event_types=[EventType.BOOT]\n            )\n        )\n        self.assertEqual([], fake_get_data.call_args_list)\n\n    @mock.patch.dict(\n        DataSource.supported_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_update_metadata_returns_true_on_supported_update_event(self):\n        \"\"\"update_metadata_if_supported returns get_data on supported events\"\"\"\n\n        def fake_get_data():\n            return True\n\n        self.datasource.get_data = fake_get_data\n        self.datasource._network_config = \"something\"\n        self.datasource._dirty_cache = True\n        self.assertTrue(\n            self.datasource.update_metadata_if_supported(\n                source_event_types=[\n                    EventType.BOOT,\n                    EventType.BOOT_NEW_INSTANCE,\n                ]\n            )\n        )\n        self.assertEqual(UNSET, self.datasource._network_config)\n\n        self.assertIn(\n            \"DEBUG: Update datasource metadata and network config due to\"\n            \" events: boot-new-instance\",\n            self.logs.getvalue(),\n        )\n\n\nclass TestRedactSensitiveData(CiTestCase):\n    def test_redact_sensitive_data_noop_when_no_sensitive_keys_present(self):\n        \"\"\"When sensitive_keys is absent or empty from metadata do nothing.\"\"\"\n        md = {\"my\": \"data\"}\n        self.assertEqual(\n            md, redact_sensitive_keys(md, redact_value=\"redacted\")\n        )\n        md[\"sensitive_keys\"] = []\n        self.assertEqual(\n            md, redact_sensitive_keys(md, redact_value=\"redacted\")\n        )\n\n    def test_redact_sensitive_data_redacts_exact_match_name(self):\n        \"\"\"Only exact matched sensitive_keys are redacted from metadata.\"\"\"\n        md = {\n            \"sensitive_keys\": [\"md/secure\"],\n            \"md\": {\"secure\": \"s3kr1t\", \"insecure\": \"publik\"},\n        }\n        secure_md = copy.deepcopy(md)\n        secure_md[\"md\"][\"secure\"] = \"redacted\"\n        self.assertEqual(\n            secure_md, redact_sensitive_keys(md, redact_value=\"redacted\")\n        )\n\n    def test_redact_sensitive_data_does_redacts_with_default_string(self):\n        \"\"\"When redact_value is absent, REDACT_SENSITIVE_VALUE is used.\"\"\"\n        md = {\n            \"sensitive_keys\": [\"md/secure\"],\n            \"md\": {\"secure\": \"s3kr1t\", \"insecure\": \"publik\"},\n        }\n        secure_md = copy.deepcopy(md)\n        secure_md[\"md\"][\"secure\"] = \"redacted for non-root user\"\n        self.assertEqual(secure_md, redact_sensitive_keys(md))\n\n\nclass TestCanonicalCloudID(CiTestCase):\n    def test_cloud_id_returns_platform_on_unknowns(self):\n        \"\"\"When region and cloud_name are unknown, return platform.\"\"\"\n        self.assertEqual(\n            \"platform\",\n            canonical_cloud_id(\n                cloud_name=METADATA_UNKNOWN,\n                region=METADATA_UNKNOWN,\n                platform=\"platform\",\n            ),\n        )\n\n    def test_cloud_id_returns_platform_on_none(self):\n        \"\"\"When region and cloud_name are unknown, return platform.\"\"\"\n        self.assertEqual(\n            \"platform\",\n            canonical_cloud_id(\n                cloud_name=None, region=None, platform=\"platform\"\n            ),\n        )\n\n    def test_cloud_id_returns_cloud_name_on_unknown_region(self):\n        \"\"\"When region is unknown, return cloud_name.\"\"\"\n        for region in (None, METADATA_UNKNOWN):\n            self.assertEqual(\n                \"cloudname\",\n                canonical_cloud_id(\n                    cloud_name=\"cloudname\", region=region, platform=\"platform\"\n                ),\n            )\n\n    def test_cloud_id_returns_platform_on_unknown_cloud_name(self):\n        \"\"\"When region is set but cloud_name is unknown return cloud_name.\"\"\"\n        self.assertEqual(\n            \"platform\",\n            canonical_cloud_id(\n                cloud_name=METADATA_UNKNOWN,\n                region=\"region\",\n                platform=\"platform\",\n            ),\n        )\n\n    def test_cloud_id_aws_based_on_region_and_cloud_name(self):\n        \"\"\"When cloud_name is aws, return proper cloud-id based on region.\"\"\"\n        self.assertEqual(\n            \"aws-china\",\n            canonical_cloud_id(\n                cloud_name=\"aws\", region=\"cn-north-1\", platform=\"platform\"\n            ),\n        )\n        self.assertEqual(\n            \"aws\",\n            canonical_cloud_id(\n                cloud_name=\"aws\", region=\"us-east-1\", platform=\"platform\"\n            ),\n        )\n        self.assertEqual(\n            \"aws-gov\",\n            canonical_cloud_id(\n                cloud_name=\"aws\", region=\"us-gov-1\", platform=\"platform\"\n            ),\n        )\n        self.assertEqual(  # Overrideen non-aws cloud_name is returned\n            \"!aws\",\n            canonical_cloud_id(\n                cloud_name=\"!aws\", region=\"us-gov-1\", platform=\"platform\"\n            ),\n        )\n\n    def test_cloud_id_azure_based_on_region_and_cloud_name(self):\n        \"\"\"Report cloud-id when cloud_name is azure and region is in china.\"\"\"\n        self.assertEqual(\n            \"azure-china\",\n            canonical_cloud_id(\n                cloud_name=\"azure\", region=\"chinaeast\", platform=\"platform\"\n            ),\n        )\n        self.assertEqual(\n            \"azure\",\n            canonical_cloud_id(\n                cloud_name=\"azure\", region=\"!chinaeast\", platform=\"platform\"\n            ),\n        )\n\n\n# vi: ts=4 expandtab\n", "# This file is part of cloud-init. See LICENSE file for license information.\n\n\"\"\"Tests related to cloudinit.stages module.\"\"\"\nimport os\nimport stat\n\nimport pytest\n\nfrom cloudinit import sources, stages\nfrom cloudinit.event import EventScope, EventType\nfrom cloudinit.sources import NetworkConfigSource\nfrom cloudinit.util import write_file\nfrom tests.unittests.helpers import mock\nfrom tests.unittests.util import TEST_INSTANCE_ID, FakeDataSource\n\nM_PATH = \"cloudinit.stages.\"\n\n\nclass TestInit:\n    @pytest.fixture(autouse=True)\n    def setup(self, tmpdir):\n        self.tmpdir = tmpdir\n        self.init = stages.Init()\n        self.init._cfg = {\n            \"system_info\": {\n                \"distro\": \"ubuntu\",\n                \"paths\": {\"cloud_dir\": self.tmpdir, \"run_dir\": self.tmpdir},\n            }\n        }\n        self.init.datasource = FakeDataSource(paths=self.init.paths)\n        self._real_is_new_instance = self.init.is_new_instance\n        self.init.is_new_instance = mock.Mock(return_value=True)\n\n    def test_wb__find_networking_config_disabled(self):\n        \"\"\"find_networking_config returns no config when disabled.\"\"\"\n        disable_file = os.path.join(\n            self.init.paths.get_cpath(\"data\"), \"upgraded-network\"\n        )\n        write_file(disable_file, \"\")\n        assert (None, disable_file) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"net_config\",\n        [\n            {\"config\": \"disabled\"},\n            {\"network\": {\"config\": \"disabled\"}},\n        ],\n    )\n    def test_wb__find_networking_config_disabled_by_kernel(\n        self, m_cmdline, m_initramfs, net_config, caplog\n    ):\n        \"\"\"find_networking_config returns when disabled by kernel cmdline.\"\"\"\n        m_cmdline.return_value = net_config\n        m_initramfs.return_value = {\"config\": [\"fake_initrd\"]}\n        assert (\n            None,\n            NetworkConfigSource.CMD_LINE,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert \"network config disabled by cmdline\" in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"net_config\",\n        [\n            {\"config\": \"disabled\"},\n            {\"network\": {\"config\": \"disabled\"}},\n        ],\n    )\n    def test_wb__find_networking_config_disabled_by_initrd(\n        self, m_cmdline, m_initramfs, net_config, caplog\n    ):\n        \"\"\"find_networking_config returns when disabled by kernel cmdline.\"\"\"\n        m_cmdline.return_value = {}\n        m_initramfs.return_value = net_config\n        assert (\n            None,\n            NetworkConfigSource.INITRAMFS,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert \"network config disabled by initramfs\" in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"net_config\",\n        [\n            {\"config\": \"disabled\"},\n            {\"network\": {\"config\": \"disabled\"}},\n        ],\n    )\n    def test_wb__find_networking_config_disabled_by_datasrc(\n        self, m_cmdline, m_initramfs, net_config, caplog\n    ):\n        \"\"\"find_networking_config returns when disabled by datasource cfg.\"\"\"\n        m_cmdline.return_value = {}  # Kernel doesn't disable networking\n        m_initramfs.return_value = {}  # initramfs doesn't disable networking\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": {},\n        }  # system config doesn't disable\n\n        self.init.datasource = FakeDataSource(network_config=net_config)\n        assert (\n            None,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert \"network config disabled by ds\" in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"net_config\",\n        [\n            {\"config\": \"disabled\"},\n            {\"network\": {\"config\": \"disabled\"}},\n        ],\n    )\n    def test_wb__find_networking_config_disabled_by_sysconfig(\n        self, m_cmdline, m_initramfs, net_config, caplog\n    ):\n        \"\"\"find_networking_config returns when disabled by system config.\"\"\"\n        m_cmdline.return_value = {}  # Kernel doesn't disable networking\n        m_initramfs.return_value = {}  # initramfs doesn't disable networking\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": net_config,\n        }\n        assert (\n            None,\n            NetworkConfigSource.SYSTEM_CFG,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert \"network config disabled by system_cfg\" in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test__find_networking_config_uses_datasrc_order(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config should check sources in DS defined order\"\"\"\n        # cmdline and initramfs, which would normally be preferred over other\n        # sources, disable networking; in this case, though, the DS moves them\n        # later so its own config is preferred\n        m_cmdline.return_value = {\"config\": \"disabled\"}\n        m_initramfs.return_value = {\"config\": \"disabled\"}\n\n        self.init.datasource = FakeDataSource(network_config=in_config)\n        self.init.datasource.network_config_sources = [\n            NetworkConfigSource.DS,\n            NetworkConfigSource.SYSTEM_CFG,\n            NetworkConfigSource.CMD_LINE,\n            NetworkConfigSource.INITRAMFS,\n        ]\n\n        assert (\n            out_config,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test__find_networking_config_warns_if_datasrc_uses_invalid_src(\n        self, m_cmdline, m_initramfs, in_config, out_config, caplog\n    ):\n        \"\"\"find_networking_config should check sources in DS defined order\"\"\"\n        self.init.datasource = FakeDataSource(network_config=in_config)\n        self.init.datasource.network_config_sources = [\n            \"invalid_src\",\n            NetworkConfigSource.DS,\n        ]\n\n        assert (\n            out_config,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"WARNING\"\n        assert (\n            \"data source specifies an invalid network cfg_source: invalid_src\"\n            in caplog.text\n        )\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test__find_networking_config_warns_if_datasrc_uses_unavailable_src(\n        self, m_cmdline, m_initramfs, in_config, out_config, caplog\n    ):\n        \"\"\"find_networking_config should check sources in DS defined order\"\"\"\n        self.init.datasource = FakeDataSource(network_config=in_config)\n        self.init.datasource.network_config_sources = [\n            NetworkConfigSource.FALLBACK,\n            NetworkConfigSource.DS,\n        ]\n\n        assert (\n            out_config,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"WARNING\"\n        assert (\n            \"data source specifies an unavailable network cfg_source: fallback\"\n            in caplog.text\n        )\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test_wb__find_networking_config_returns_kernel(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config returns kernel cmdline config if present.\"\"\"\n        m_cmdline.return_value = in_config\n        m_initramfs.return_value = {\"config\": [\"fake_initrd\"]}\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": {\"config\": [\"fakesys_config\"]},\n        }\n        self.init.datasource = FakeDataSource(\n            network_config={\"config\": [\"fakedatasource\"]}\n        )\n        assert (\n            out_config,\n            NetworkConfigSource.CMD_LINE,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test_wb__find_networking_config_returns_initramfs(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config returns initramfs config if present.\"\"\"\n        m_cmdline.return_value = {}\n        m_initramfs.return_value = in_config\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": {\"config\": [\"fakesys_config\"]},\n        }\n        self.init.datasource = FakeDataSource(\n            network_config={\"config\": [\"fakedatasource\"]}\n        )\n        assert (\n            out_config,\n            NetworkConfigSource.INITRAMFS,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test_wb__find_networking_config_returns_system_cfg(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config returns system config when present.\"\"\"\n        m_cmdline.return_value = {}  # No kernel network config\n        m_initramfs.return_value = {}  # no initramfs network config\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": in_config,\n        }\n        self.init.datasource = FakeDataSource(\n            network_config={\"config\": [\"fakedatasource\"]}\n        )\n        assert (\n            out_config,\n            NetworkConfigSource.SYSTEM_CFG,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test_wb__find_networking_config_returns_datasrc_cfg(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config returns datasource net config if present.\"\"\"\n        m_cmdline.return_value = {}  # No kernel network config\n        m_initramfs.return_value = {}  # no initramfs network config\n        self.init.datasource = FakeDataSource(network_config=in_config)\n        assert (\n            out_config,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    def test_wb__find_networking_config_returns_fallback(\n        self, m_cmdline, m_initramfs, caplog\n    ):\n        \"\"\"find_networking_config returns fallback config if not defined.\"\"\"\n        m_cmdline.return_value = {}  # Kernel doesn't disable networking\n        m_initramfs.return_value = {}  # no initramfs network config\n        # Neither datasource nor system_info disable or provide network\n\n        fake_cfg = {\n            \"config\": [{\"type\": \"physical\", \"name\": \"eth9\"}],\n            \"version\": 1,\n        }\n\n        def fake_generate_fallback():\n            return fake_cfg\n\n        # Monkey patch distro which gets cached on self.init\n        distro = self.init.distro\n        distro.generate_fallback_config = fake_generate_fallback\n        assert (\n            fake_cfg,\n            NetworkConfigSource.FALLBACK,\n        ) == self.init._find_networking_config()\n        assert \"network config disabled\" not in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\", return_value={})\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\", return_value={})\n    def test_warn_on_empty_network(self, m_cmdline, m_initramfs, caplog):\n        \"\"\"funky whitespace can lead to a network key that is None, which then\n        causes fallback. Test warning log on empty network key.\n        \"\"\"\n        m_cmdline.return_value = {}  # Kernel doesn't disable networking\n        m_initramfs.return_value = {}  # no initramfs network config\n        # Neither datasource nor system_info disable or provide network\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": None,\n        }\n        self.init.datasource = FakeDataSource(network_config={\"network\": None})\n\n        self.init.distro.generate_fallback_config = lambda: {}\n\n        self.init._find_networking_config()\n        assert \"Empty network config found\" in caplog.text\n\n    def test_apply_network_config_disabled(self, caplog):\n        \"\"\"Log when network is disabled by upgraded-network.\"\"\"\n        disable_file = os.path.join(\n            self.init.paths.get_cpath(\"data\"), \"upgraded-network\"\n        )\n\n        def fake_network_config():\n            return (None, disable_file)\n\n        self.init._find_networking_config = fake_network_config\n\n        self.init.apply_network_config(True)\n        assert caplog.records[0].levelname == \"INFO\"\n        assert f\"network config is disabled by {disable_file}\" in caplog.text\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    def test_apply_network_on_new_instance(self, m_ubuntu, m_macs):\n        \"\"\"Call distro apply_network_config methods on is_new_instance.\"\"\"\n        net_cfg = {\n            \"version\": 1,\n            \"config\": [\n                {\n                    \"subnets\": [{\"type\": \"dhcp\"}],\n                    \"type\": \"physical\",\n                    \"name\": \"eth9\",\n                    \"mac_address\": \"42:42:42:42:42:42\",\n                }\n            ],\n        }\n\n        def fake_network_config():\n            return net_cfg, NetworkConfigSource.FALLBACK\n\n        m_macs.return_value = {\"42:42:42:42:42:42\": \"eth9\"}\n\n        self.init._find_networking_config = fake_network_config\n\n        self.init.apply_network_config(True)\n        networking = self.init.distro.networking\n        networking.apply_network_config_names.assert_called_with(net_cfg)\n        self.init.distro.apply_network_config.assert_called_with(\n            net_cfg, bring_up=True\n        )\n\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    def test_apply_network_on_same_instance_id(self, m_ubuntu, caplog):\n        \"\"\"Only call distro.networking.apply_network_config_names on same\n        instance id.\"\"\"\n        self.init.is_new_instance = self._real_is_new_instance\n        old_instance_id = os.path.join(\n            self.init.paths.get_cpath(\"data\"), \"instance-id\"\n        )\n        write_file(old_instance_id, TEST_INSTANCE_ID)\n        net_cfg = {\n            \"version\": 1,\n            \"config\": [\n                {\n                    \"subnets\": [{\"type\": \"dhcp\"}],\n                    \"type\": \"physical\",\n                    \"name\": \"eth9\",\n                    \"mac_address\": \"42:42:42:42:42:42\",\n                }\n            ],\n        }\n\n        def fake_network_config():\n            return net_cfg, NetworkConfigSource.FALLBACK\n\n        self.init._find_networking_config = fake_network_config\n\n        self.init.apply_network_config(True)\n        networking = self.init.distro.networking\n        networking.apply_network_config_names.assert_called_with(net_cfg)\n        self.init.distro.apply_network_config.assert_not_called()\n        assert (\n            \"No network config applied. Neither a new instance nor datasource \"\n            \"network update allowed\" in caplog.text\n        )\n\n    def _apply_network_setup(self, m_macs):\n        old_instance_id = os.path.join(\n            self.init.paths.get_cpath(\"data\"), \"instance-id\"\n        )\n        write_file(old_instance_id, TEST_INSTANCE_ID)\n        net_cfg = {\n            \"version\": 1,\n            \"config\": [\n                {\n                    \"subnets\": [{\"type\": \"dhcp\"}],\n                    \"type\": \"physical\",\n                    \"name\": \"eth9\",\n                    \"mac_address\": \"42:42:42:42:42:42\",\n                }\n            ],\n        }\n\n        def fake_network_config():\n            return net_cfg, NetworkConfigSource.FALLBACK\n\n        m_macs.return_value = {\"42:42:42:42:42:42\": \"eth9\"}\n\n        self.init._find_networking_config = fake_network_config\n        self.init.datasource = FakeDataSource(paths=self.init.paths)\n        self.init.is_new_instance = mock.Mock(return_value=False)\n        return net_cfg\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    @mock.patch.dict(\n        sources.DataSource.default_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE, EventType.BOOT}},\n    )\n    def test_apply_network_allowed_when_default_boot(self, m_ubuntu, m_macs):\n        \"\"\"Apply network if datasource permits BOOT event.\"\"\"\n        net_cfg = self._apply_network_setup(m_macs)\n\n        self.init.apply_network_config(True)\n        networking = self.init.distro.networking\n        assert (\n            mock.call(net_cfg)\n            == networking.apply_network_config_names.call_args_list[-1]\n        )\n        assert (\n            mock.call(net_cfg, bring_up=True)\n            == self.init.distro.apply_network_config.call_args_list[-1]\n        )\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    @mock.patch.dict(\n        sources.DataSource.default_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_apply_network_disabled_when_no_default_boot(\n        self, m_ubuntu, m_macs, caplog\n    ):\n        \"\"\"Don't apply network if datasource has no BOOT event.\"\"\"\n        self._apply_network_setup(m_macs)\n        self.init.apply_network_config(True)\n        self.init.distro.apply_network_config.assert_not_called()\n        assert (\n            \"No network config applied. Neither a new instance nor datasource \"\n            \"network update allowed\" in caplog.text\n        )\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    @mock.patch.dict(\n        sources.DataSource.default_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_apply_network_allowed_with_userdata_overrides(\n        self, m_ubuntu, m_macs\n    ):\n        \"\"\"Apply network if userdata overrides default config\"\"\"\n        net_cfg = self._apply_network_setup(m_macs)\n        self.init._cfg = {\"updates\": {\"network\": {\"when\": [\"boot\"]}}}\n        self.init.apply_network_config(True)\n        networking = self.init.distro.networking\n        networking.apply_network_config_names.assert_called_with(net_cfg)\n        self.init.distro.apply_network_config.assert_called_with(\n            net_cfg, bring_up=True\n        )\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    @mock.patch.dict(\n        sources.DataSource.supported_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_apply_network_disabled_when_unsupported(\n        self, m_ubuntu, m_macs, caplog\n    ):\n        \"\"\"Don't apply network config if unsupported.\n\n        Shouldn't work even when specified as userdata\n        \"\"\"\n        self._apply_network_setup(m_macs)\n\n        self.init._cfg = {\"updates\": {\"network\": {\"when\": [\"boot\"]}}}\n        self.init.apply_network_config(True)\n        self.init.distro.apply_network_config.assert_not_called()\n        assert (\n            \"No network config applied. Neither a new instance nor datasource \"\n            \"network update allowed\" in caplog.text\n        )\n\n\nclass TestInit_InitializeFilesystem:\n    \"\"\"Tests for cloudinit.stages.Init._initialize_filesystem.\n\n    TODO: Expand these tests to cover all of _initialize_filesystem's behavior.\n    \"\"\"\n\n    @pytest.fixture\n    def init(self, paths):\n        \"\"\"A fixture which yields a stages.Init instance with paths and cfg set\n\n        As it is replaced with a mock, consumers of this fixture can set\n        `init._cfg` if the default empty dict configuration is not appropriate.\n        \"\"\"\n        with mock.patch(M_PATH + \"util.ensure_dirs\"):\n            init = stages.Init()\n            init._cfg = {}\n            init._paths = paths\n            yield init\n\n    @mock.patch(M_PATH + \"util.ensure_file\")\n    def test_ensure_file_not_called_if_no_log_file_configured(\n        self, m_ensure_file, init\n    ):\n        \"\"\"If no log file is configured, we should not ensure its existence.\"\"\"\n        init._cfg = {}\n\n        init._initialize_filesystem()\n\n        assert 0 == m_ensure_file.call_count\n\n    def test_log_files_existence_is_ensured_if_configured(self, init, tmpdir):\n        \"\"\"If a log file is configured, we should ensure its existence.\"\"\"\n        log_file = tmpdir.join(\"cloud-init.log\")\n        init._cfg = {\"def_log_file\": str(log_file)}\n\n        init._initialize_filesystem()\n\n        assert log_file.exists()\n        # Assert we create it 0o640  by default if it doesn't already exist\n        assert 0o640 == stat.S_IMODE(log_file.stat().mode)\n\n    def test_existing_file_permissions_are_not_modified(self, init, tmpdir):\n        \"\"\"If the log file already exists, we should not modify its permissions\n\n        See https://bugs.launchpad.net/cloud-init/+bug/1900837.\n        \"\"\"\n        # Use a mode that will never be made the default so this test will\n        # always be valid\n        mode = 0o606\n        log_file = tmpdir.join(\"cloud-init.log\")\n        log_file.ensure()\n        log_file.chmod(mode)\n        init._cfg = {\"def_log_file\": str(log_file)}\n\n        init._initialize_filesystem()\n\n        assert mode == stat.S_IMODE(log_file.stat().mode)\n"], "fixing_code": ["\"\"\"Datasource for LXD, reads /dev/lxd/sock representation of instance data.\n\nNotes:\n * This datasource replaces previous NoCloud datasource for LXD.\n * Older LXD images may not have updates for cloud-init so NoCloud may\n   still be detected on those images.\n * Detect LXD datasource when /dev/lxd/sock is an active socket file.\n * Info on dev-lxd API: https://linuxcontainers.org/lxd/docs/master/dev-lxd\n\"\"\"\n\nimport os\nimport socket\nimport stat\nimport time\nfrom enum import Flag, auto\nfrom json.decoder import JSONDecodeError\nfrom typing import Any, Dict, List, Optional, Tuple, Union, cast\n\nimport requests\nfrom requests.adapters import HTTPAdapter\n\n# Note: `urllib3` is transitively installed by `requests`.\nfrom urllib3.connection import HTTPConnection\nfrom urllib3.connectionpool import HTTPConnectionPool\n\nfrom cloudinit import log as logging\nfrom cloudinit import sources, subp, url_helper, util\nfrom cloudinit.net import find_fallback_nic\n\nLOG = logging.getLogger(__name__)\n\nLXD_SOCKET_PATH = \"/dev/lxd/sock\"\nLXD_SOCKET_API_VERSION = \"1.0\"\nLXD_URL = \"http://lxd\"\n\n# Config key mappings to alias as top-level instance data keys\nCONFIG_KEY_ALIASES = {\n    \"cloud-init.user-data\": \"user-data\",\n    \"cloud-init.network-config\": \"network-config\",\n    \"cloud-init.vendor-data\": \"vendor-data\",\n    \"user.user-data\": \"user-data\",\n    \"user.network-config\": \"network-config\",\n    \"user.vendor-data\": \"vendor-data\",\n}\n\n\ndef _get_fallback_interface_name() -> str:\n    default_name = \"eth0\"\n    if subp.which(\"systemd-detect-virt\"):\n        try:\n            virt_type, _ = subp.subp([\"systemd-detect-virt\"])\n        except subp.ProcessExecutionError as err:\n            LOG.warning(\n                \"Unable to run systemd-detect-virt: %s.\"\n                \" Rendering default network config.\",\n                err,\n            )\n            return default_name\n        if virt_type.strip() in (\n            \"kvm\",\n            \"qemu\",\n        ):  # instance.type VIRTUAL-MACHINE\n            arch = util.system_info()[\"uname\"][4]\n            if arch == \"ppc64le\":\n                return \"enp0s5\"\n            elif arch == \"s390x\":\n                return \"enc9\"\n            else:\n                return \"enp5s0\"\n    return default_name\n\n\ndef generate_network_config(\n    nics: Optional[List[str]] = None,\n) -> Dict[str, Any]:\n    \"\"\"Return network config V1 dict representing instance network config.\"\"\"\n    # TODO: The original intent of this function was to use the nics retrieved\n    # from LXD's devices endpoint to determine the primary nic and write\n    # that out to network config. However, for LXD VMs, the device name\n    # may differ from the interface name in the VM, so we'll instead rely\n    # on our fallback nic code. Once LXD's devices endpoint grows the\n    # ability to provide a MAC address, we should rely on that information\n    # rather than just the glorified guessing that we're doing here.\n    primary_nic = find_fallback_nic()\n    if primary_nic:\n        LOG.debug(\n            \"LXD datasource generating network from discovered active\"\n            \" device: %s\",\n            primary_nic,\n        )\n    else:\n        primary_nic = _get_fallback_interface_name()\n        LOG.debug(\n            \"LXD datasource generating network from systemd-detect-virt\"\n            \" platform default device: %s\",\n            primary_nic,\n        )\n\n    return {\n        \"version\": 1,\n        \"config\": [\n            {\n                \"type\": \"physical\",\n                \"name\": primary_nic,\n                \"subnets\": [{\"type\": \"dhcp\", \"control\": \"auto\"}],\n            }\n        ],\n    }\n\n\nclass SocketHTTPConnection(HTTPConnection):\n    def __init__(self, socket_path):\n        super().__init__(\"localhost\")\n        self.socket_path = socket_path\n\n    def connect(self):\n        self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        self.sock.connect(self.socket_path)\n\n\nclass SocketConnectionPool(HTTPConnectionPool):\n    def __init__(self, socket_path):\n        self.socket_path = socket_path\n        super().__init__(\"localhost\")\n\n    def _new_conn(self):\n        return SocketHTTPConnection(self.socket_path)\n\n\nclass LXDSocketAdapter(HTTPAdapter):\n    def get_connection(self, url, proxies=None):\n        return SocketConnectionPool(LXD_SOCKET_PATH)\n\n\ndef _raw_instance_data_to_dict(metadata_type: str, metadata_value) -> dict:\n    \"\"\"Convert raw instance data from str, bytes, YAML to dict\n\n    :param metadata_type: string, one of as: meta-data, vendor-data, user-data\n        network-config\n\n    :param metadata_value: str, bytes or dict representing or instance-data.\n\n    :raises: InvalidMetaDataError on invalid instance-data content.\n    \"\"\"\n    if isinstance(metadata_value, dict):\n        return metadata_value\n    if metadata_value is None:\n        return {}\n    try:\n        parsed_metadata = util.load_yaml(metadata_value)\n    except AttributeError as exc:  # not str or bytes\n        raise sources.InvalidMetaDataException(\n            \"Invalid {md_type}. Expected str, bytes or dict but found:\"\n            \" {value}\".format(md_type=metadata_type, value=metadata_value)\n        ) from exc\n    if parsed_metadata is None:\n        raise sources.InvalidMetaDataException(\n            \"Invalid {md_type} format. Expected YAML but found:\"\n            \" {value}\".format(md_type=metadata_type, value=metadata_value)\n        )\n    return parsed_metadata\n\n\nclass DataSourceLXD(sources.DataSource):\n\n    dsname = \"LXD\"\n\n    _network_config: Union[Dict, str] = sources.UNSET\n    _crawled_metadata: Union[Dict, str] = sources.UNSET\n\n    sensitive_metadata_keys: Tuple[\n        str, ...\n    ] = sources.DataSource.sensitive_metadata_keys + (\n        \"user.meta-data\",\n        \"user.vendor-data\",\n        \"user.user-data\",\n        \"cloud-init.user-data\",\n        \"cloud-init.vendor-data\",\n    )\n\n    skip_hotplug_detect = True\n\n    def _unpickle(self, ci_pkl_version: int) -> None:\n        super()._unpickle(ci_pkl_version)\n        self.skip_hotplug_detect = True\n\n    @staticmethod\n    def ds_detect() -> bool:\n        \"\"\"Check platform environment to report if this datasource may run.\"\"\"\n        return is_platform_viable()\n\n    def _get_data(self) -> bool:\n        \"\"\"Crawl LXD socket API instance data and return True on success\"\"\"\n        self._crawled_metadata = util.log_time(\n            logfunc=LOG.debug,\n            msg=\"Crawl of metadata service\",\n            func=read_metadata,\n        )\n        self.metadata = _raw_instance_data_to_dict(\n            \"meta-data\", self._crawled_metadata.get(\"meta-data\")\n        )\n        config = self._crawled_metadata.get(\"config\", {})\n        user_metadata = config.get(\"user.meta-data\", {})\n        if user_metadata:\n            user_metadata = _raw_instance_data_to_dict(\n                \"user.meta-data\", user_metadata\n            )\n        if not isinstance(self.metadata, dict):\n            self.metadata = util.mergemanydict(\n                [util.load_yaml(self.metadata), user_metadata]\n            )\n        if \"user-data\" in self._crawled_metadata:\n            self.userdata_raw = self._crawled_metadata[\"user-data\"]\n        if \"network-config\" in self._crawled_metadata:\n            self._network_config = _raw_instance_data_to_dict(\n                \"network-config\", self._crawled_metadata[\"network-config\"]\n            )\n        if \"vendor-data\" in self._crawled_metadata:\n            self.vendordata_raw = self._crawled_metadata[\"vendor-data\"]\n        return True\n\n    def _get_subplatform(self) -> str:\n        \"\"\"Return subplatform details for this datasource\"\"\"\n        return \"LXD socket API v. {ver} ({socket})\".format(\n            ver=LXD_SOCKET_API_VERSION, socket=LXD_SOCKET_PATH\n        )\n\n    def check_instance_id(self, sys_cfg) -> str:\n        \"\"\"Return True if instance_id unchanged.\"\"\"\n        response = read_metadata(metadata_keys=MetaDataKeys.META_DATA)\n        md = response.get(\"meta-data\", {})\n        if not isinstance(md, dict):\n            md = util.load_yaml(md)\n        return md.get(\"instance-id\") == self.metadata.get(\"instance-id\")\n\n    @property\n    def network_config(self) -> dict:\n        \"\"\"Network config read from LXD socket config/user.network-config.\n\n        If none is present, then we generate fallback configuration.\n        \"\"\"\n        if self._network_config == sources.UNSET:\n            if self._crawled_metadata == sources.UNSET:\n                self._get_data()\n            if isinstance(self._crawled_metadata, dict):\n                if self._crawled_metadata.get(\"network-config\"):\n                    LOG.debug(\"LXD datasource using provided network config\")\n                    self._network_config = self._crawled_metadata[\n                        \"network-config\"\n                    ]\n                elif self._crawled_metadata.get(\"devices\"):\n                    # If no explicit network config, but we have net devices\n                    # available to us, find the primary and set it up.\n                    devices: List[str] = [\n                        k\n                        for k, v in self._crawled_metadata[\"devices\"].items()\n                        if v[\"type\"] == \"nic\"\n                    ]\n                    self._network_config = generate_network_config(devices)\n        if self._network_config == sources.UNSET:\n            # We know nothing about network, so setup fallback\n            LOG.debug(\n                \"LXD datasource generating network config using fallback.\"\n            )\n            self._network_config = generate_network_config()\n\n        return cast(dict, self._network_config)\n\n\ndef is_platform_viable() -> bool:\n    \"\"\"Return True when this platform appears to have an LXD socket.\"\"\"\n    if os.path.exists(LXD_SOCKET_PATH):\n        return stat.S_ISSOCK(os.lstat(LXD_SOCKET_PATH).st_mode)\n    return False\n\n\ndef _get_json_response(\n    session: requests.Session, url: str, do_raise: bool = True\n):\n    url_response = _do_request(session, url, do_raise)\n    if not url_response.ok:\n        LOG.debug(\n            \"Skipping %s on [HTTP:%d]:%s\",\n            url,\n            url_response.status_code,\n            url_response.text,\n        )\n        return {}\n    try:\n        return url_response.json()\n    except JSONDecodeError as exc:\n        raise sources.InvalidMetaDataException(\n            \"Unable to process LXD config at {url}.\"\n            \" Expected JSON but found: {resp}\".format(\n                url=url, resp=url_response.text\n            )\n        ) from exc\n\n\ndef _do_request(\n    session: requests.Session, url: str, do_raise: bool = True\n) -> requests.Response:\n    for retries in range(30, 0, -1):\n        response = session.get(url)\n        if 500 == response.status_code:\n            # retry every 0.1 seconds for 3 seconds in the case of 500 error\n            # tis evil, but it also works around a bug\n            time.sleep(0.1)\n            LOG.warning(\n                \"[GET] [HTTP:%d] %s, retrying %d more time(s)\",\n                response.status_code,\n                url,\n                retries,\n            )\n        else:\n            break\n    LOG.debug(\"[GET] [HTTP:%d] %s\", response.status_code, url)\n    if do_raise and not response.ok:\n        raise sources.InvalidMetaDataException(\n            \"Invalid HTTP response [{code}] from {route}: {resp}\".format(\n                code=response.status_code,\n                route=url,\n                resp=response.text,\n            )\n        )\n    return response\n\n\nclass MetaDataKeys(Flag):\n    NONE = auto()\n    CONFIG = auto()\n    DEVICES = auto()\n    META_DATA = auto()\n    ALL = CONFIG | DEVICES | META_DATA\n\n\nclass _MetaDataReader:\n    def __init__(self, api_version: str = LXD_SOCKET_API_VERSION):\n        self.api_version = api_version\n        self._version_url = url_helper.combine_url(LXD_URL, self.api_version)\n\n    def _process_config(self, session: requests.Session) -> dict:\n        \"\"\"Iterate on LXD API config items. Promoting CONFIG_KEY_ALIASES\n\n        Any CONFIG_KEY_ALIASES which affect cloud-init behavior are promoted\n        as top-level configuration keys: user-data, network-data, vendor-data.\n\n        LXD's cloud-init.* config keys override any user.* config keys.\n        Log debug messages if any user.* keys are overridden by the related\n        cloud-init.* key.\n        \"\"\"\n        config: dict = {\"config\": {}}\n        config_url = url_helper.combine_url(self._version_url, \"config\")\n        # Represent all advertized/available config routes under\n        # the dict path {LXD_SOCKET_API_VERSION: {config: {...}}.\n        config_routes = _get_json_response(session, config_url)\n\n        # Sorting keys to ensure we always process in alphabetical order.\n        # cloud-init.* keys will sort before user.* keys which is preferred\n        # precedence.\n        for config_route in sorted(config_routes):\n            config_route_url = url_helper.combine_url(LXD_URL, config_route)\n            config_route_response = _do_request(\n                session, config_route_url, do_raise=False\n            )\n            if not config_route_response.ok:\n                LOG.debug(\n                    \"Skipping %s on [HTTP:%d]:%s\",\n                    config_route_url,\n                    config_route_response.status_code,\n                    config_route_response.text,\n                )\n                continue\n\n            cfg_key = config_route.rpartition(\"/\")[-1]\n            # Leave raw data values/format unchanged to represent it in\n            # instance-data.json for cloud-init query or jinja template\n            # use.\n            config[\"config\"][cfg_key] = config_route_response.text\n            # Promote common CONFIG_KEY_ALIASES to top-level keys.\n            if cfg_key in CONFIG_KEY_ALIASES:\n                # Due to sort of config_routes, promote cloud-init.*\n                # aliases before user.*. This allows user.* keys to act as\n                # fallback config on old LXD, with new cloud-init images.\n                if CONFIG_KEY_ALIASES[cfg_key] not in config:\n                    config[\n                        CONFIG_KEY_ALIASES[cfg_key]\n                    ] = config_route_response.text\n                else:\n                    LOG.warning(\n                        \"Ignoring LXD config %s in favor of %s value.\",\n                        cfg_key,\n                        cfg_key.replace(\"user\", \"cloud-init\", 1),\n                    )\n        return config\n\n    def __call__(self, *, metadata_keys: MetaDataKeys) -> dict:\n        with requests.Session() as session:\n            session.mount(self._version_url, LXDSocketAdapter())\n            # Document API version read\n            md: dict = {\"_metadata_api_version\": self.api_version}\n            if MetaDataKeys.META_DATA in metadata_keys:\n                md_route = url_helper.combine_url(\n                    self._version_url, \"meta-data\"\n                )\n                md[\"meta-data\"] = _do_request(session, md_route).text\n            if MetaDataKeys.CONFIG in metadata_keys:\n                md.update(self._process_config(session))\n            if MetaDataKeys.DEVICES in metadata_keys:\n                url = url_helper.combine_url(self._version_url, \"devices\")\n                devices = _get_json_response(session, url, do_raise=False)\n                if devices:\n                    md[\"devices\"] = devices\n            return md\n\n\ndef read_metadata(\n    api_version: str = LXD_SOCKET_API_VERSION,\n    metadata_keys: MetaDataKeys = MetaDataKeys.ALL,\n) -> dict:\n    \"\"\"Fetch metadata from the /dev/lxd/socket routes.\n\n    Perform a number of HTTP GETs on known routes on the devlxd socket API.\n    Minimally all containers must respond to <LXD_SOCKET_API_VERSION>/meta-data\n    when the LXD configuration setting `security.devlxd` is true.\n\n    When `security.devlxd` is false, no /dev/lxd/socket file exists. This\n    datasource will return False from `is_platform_viable` in that case.\n\n    Perform a GET of <LXD_SOCKET_API_VERSION>/config` and walk all `user.*`\n    configuration keys, storing all keys and values under a dict key\n        LXD_SOCKET_API_VERSION: config {...}.\n\n    In the presence of the following optional user config keys,\n    create top level aliases:\n      - user.user-data -> user-data\n      - user.vendor-data -> vendor-data\n      - user.network-config -> network-config\n\n    :param api_version:\n        LXD API version to operated with.\n    :param metadata_keys:\n        Instance of `MetaDataKeys` indicating what keys to fetch.\n    :return:\n        A dict with the following optional keys: meta-data, user-data,\n        vendor-data, network-config, network_mode, devices.\n\n        Below <LXD_SOCKET_API_VERSION> is a dict representation of all raw\n        configuration keys and values provided to the container surfaced by\n        the socket under the /1.0/config/ route.\n    \"\"\"\n    return _MetaDataReader(api_version=api_version)(\n        metadata_keys=metadata_keys\n    )\n\n\n# Used to match classes to dependencies\ndatasources = [\n    (DataSourceLXD, (sources.DEP_FILESYSTEM,)),\n]\n\n\n# Return a list of data sources that match this set of dependencies\ndef get_datasource_list(depends):\n    return sources.list_from_depends(depends, datasources)\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    description = \"\"\"Query LXD metadata and emit a JSON object.\"\"\"\n    parser = argparse.ArgumentParser(description=description)\n    parser.parse_args()\n    print(util.json_dumps(read_metadata(metadata_keys=MetaDataKeys.ALL)))\n\n# vi: ts=4 expandtab\n", "# Author: Eric Benner <ebenner@vultr.com>\n#\n# This file is part of cloud-init. See LICENSE file for license information.\n\n# Vultr Metadata API:\n# https://www.vultr.com/metadata/\n\nfrom typing import Tuple\n\nimport cloudinit.sources.helpers.vultr as vultr\nfrom cloudinit import log as log\nfrom cloudinit import sources, stages, util, version\n\nLOG = log.getLogger(__name__)\nBUILTIN_DS_CONFIG = {\n    \"url\": \"http://169.254.169.254\",\n    \"retries\": 30,\n    \"timeout\": 10,\n    \"wait\": 5,\n    \"user-agent\": \"Cloud-Init/%s - OS: %s Variant: %s\"\n    % (\n        version.version_string(),\n        util.system_info()[\"system\"],\n        util.system_info()[\"variant\"],\n    ),\n}\n\n\nclass DataSourceVultr(sources.DataSource):\n\n    dsname = \"Vultr\"\n\n    sensitive_metadata_keys: Tuple[\n        str, ...\n    ] = sources.DataSource.sensitive_metadata_keys + (\"startup-script\",)\n\n    def __init__(self, sys_cfg, distro, paths):\n        super(DataSourceVultr, self).__init__(sys_cfg, distro, paths)\n        self.ds_cfg = util.mergemanydict(\n            [\n                util.get_cfg_by_path(sys_cfg, [\"datasource\", \"Vultr\"], {}),\n                BUILTIN_DS_CONFIG,\n            ]\n        )\n\n    @staticmethod\n    def ds_detect():\n        return vultr.is_vultr()\n\n    # Initiate data and check if Vultr\n    def _get_data(self):\n\n        LOG.debug(\"Machine is a Vultr instance\")\n\n        # Fetch metadata\n        self.metadata = self.get_metadata()\n        self.userdata_raw = self.metadata[\"user-data\"]\n\n        # Generate config and process data\n        self.get_datasource_data(self.metadata)\n\n        # Dump some data so diagnosing failures is manageable\n        LOG.debug(\"SUBID: %s\", self.metadata[\"instance-id\"])\n        LOG.debug(\"Hostname: %s\", self.metadata[\"local-hostname\"])\n\n        return True\n\n    # Process metadata\n    def get_datasource_data(self, md):\n        # Generate network config\n        if \"cloud_interfaces\" in md:\n            # In the future we will just drop pre-configured\n            # network configs into the array. They need names though.\n            vultr.add_interface_names(md[\"cloud_interfaces\"])\n            self.netcfg = md[\"cloud_interfaces\"]\n        else:\n            self.netcfg = vultr.generate_network_config(md[\"interfaces\"])\n        # Grab vendordata\n        self.vendordata_raw = md[\"vendor-data\"]\n\n        # Default hostname is \"guest\" for whitelabel\n        if self.metadata[\"local-hostname\"] == \"\":\n            self.metadata[\"local-hostname\"] = \"guest\"\n\n        self.userdata_raw = md[\"user-data\"]\n        if self.userdata_raw == \"\":\n            self.userdata_raw = None\n\n    # Get the metadata by flag\n    def get_metadata(self):\n        return vultr.get_metadata(\n            self.distro,\n            self.ds_cfg[\"url\"],\n            self.ds_cfg[\"timeout\"],\n            self.ds_cfg[\"retries\"],\n            self.ds_cfg[\"wait\"],\n            self.ds_cfg[\"user-agent\"],\n            tmp_dir=self.distro.get_tmp_exec_path(),\n        )\n\n    # Compare subid as instance id\n    def check_instance_id(self, sys_cfg):\n        if not vultr.is_vultr():\n            return False\n\n        # Baremetal has no way to implement this in local\n        if vultr.is_baremetal():\n            return False\n\n        subid = vultr.get_sysinfo()[\"subid\"]\n        return sources.instance_id_matches_system_uuid(subid)\n\n    # Currently unsupported\n    @property\n    def launch_index(self):\n        return None\n\n    @property\n    def network_config(self):\n        return self.netcfg\n\n\n# Used to match classes to dependencies\ndatasources = [\n    (DataSourceVultr, (sources.DEP_FILESYSTEM,)),\n]\n\n\n# Return a list of data sources that match this set of dependencies\ndef get_datasource_list(depends):\n    return sources.list_from_depends(depends, datasources)\n\n\nif __name__ == \"__main__\":\n    import sys\n\n    if not vultr.is_vultr():\n        print(\"Machine is not a Vultr instance\")\n        sys.exit(1)\n\n    # It should probably be safe to try to detect distro via stages.Init(),\n    # which will fall back to Ubuntu if no distro config is found.\n    # this distro object is only used for dhcp fallback. Feedback from user(s)\n    # of __main__ would help determine if a better approach exists.\n    #\n    # we don't needReportEventStack, so reporter=True\n    distro = stages.Init(reporter=True).distro\n\n    md = vultr.get_metadata(\n        distro,\n        BUILTIN_DS_CONFIG[\"url\"],\n        BUILTIN_DS_CONFIG[\"timeout\"],\n        BUILTIN_DS_CONFIG[\"retries\"],\n        BUILTIN_DS_CONFIG[\"wait\"],\n        BUILTIN_DS_CONFIG[\"user-agent\"],\n    )\n    config = md[\"vendor-data\"]\n    sysinfo = vultr.get_sysinfo()\n", "# Copyright (C) 2012 Canonical Ltd.\n# Copyright (C) 2012 Hewlett-Packard Development Company, L.P.\n# Copyright (C) 2012 Yahoo! Inc.\n#\n# Author: Scott Moser <scott.moser@canonical.com>\n# Author: Juerg Haefliger <juerg.haefliger@hp.com>\n# Author: Joshua Harlow <harlowja@yahoo-inc.com>\n#\n# This file is part of cloud-init. See LICENSE file for license information.\n\nimport abc\nimport copy\nimport json\nimport os\nimport pickle\nimport re\nfrom collections import namedtuple\nfrom enum import Enum, unique\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom cloudinit import dmi, importer\nfrom cloudinit import log as logging\nfrom cloudinit import net, type_utils\nfrom cloudinit import user_data as ud\nfrom cloudinit import util\nfrom cloudinit.atomic_helper import write_json\nfrom cloudinit.distros import Distro\nfrom cloudinit.event import EventScope, EventType\nfrom cloudinit.filters import launch_index\nfrom cloudinit.helpers import Paths\nfrom cloudinit.persistence import CloudInitPickleMixin\nfrom cloudinit.reporting import events\n\nDSMODE_DISABLED = \"disabled\"\nDSMODE_LOCAL = \"local\"\nDSMODE_NETWORK = \"net\"\nDSMODE_PASS = \"pass\"\n\nVALID_DSMODES = [DSMODE_DISABLED, DSMODE_LOCAL, DSMODE_NETWORK]\n\nDEP_FILESYSTEM = \"FILESYSTEM\"\nDEP_NETWORK = \"NETWORK\"\nDS_PREFIX = \"DataSource\"\n\nEXPERIMENTAL_TEXT = (\n    \"EXPERIMENTAL: The structure and format of content scoped under the 'ds'\"\n    \" key may change in subsequent releases of cloud-init.\"\n)\n\n\nREDACT_SENSITIVE_VALUE = \"redacted for non-root user\"\n\n# Key which can be provide a cloud's official product name to cloud-init\nMETADATA_CLOUD_NAME_KEY = \"cloud-name\"\n\nUNSET = \"_unset\"\nMETADATA_UNKNOWN = \"unknown\"\n\nLOG = logging.getLogger(__name__)\n\n# CLOUD_ID_REGION_PREFIX_MAP format is:\n#  <region-match-prefix>: (<new-cloud-id>: <test_allowed_cloud_callable>)\nCLOUD_ID_REGION_PREFIX_MAP = {\n    \"cn-\": (\"aws-china\", lambda c: c == \"aws\"),  # only change aws regions\n    \"us-gov-\": (\"aws-gov\", lambda c: c == \"aws\"),  # only change aws regions\n    \"china\": (\"azure-china\", lambda c: c == \"azure\"),  # only change azure\n}\n\n\n@unique\nclass NetworkConfigSource(Enum):\n    \"\"\"\n    Represents the canonical list of network config sources that cloud-init\n    knows about.\n    \"\"\"\n\n    CMD_LINE = \"cmdline\"\n    DS = \"ds\"\n    SYSTEM_CFG = \"system_cfg\"\n    FALLBACK = \"fallback\"\n    INITRAMFS = \"initramfs\"\n\n    def __str__(self) -> str:\n        return self.value\n\n\nclass DatasourceUnpickleUserDataError(Exception):\n    \"\"\"Raised when userdata is unable to be unpickled due to python upgrades\"\"\"\n\n\nclass DataSourceNotFoundException(Exception):\n    pass\n\n\nclass InvalidMetaDataException(Exception):\n    \"\"\"Raised when metadata is broken, unavailable or disabled.\"\"\"\n\n\ndef process_instance_metadata(metadata, key_path=\"\", sensitive_keys=()):\n    \"\"\"Process all instance metadata cleaning it up for persisting as json.\n\n    Strip ci-b64 prefix and catalog any 'base64_encoded_keys' as a list\n\n    @return Dict copy of processed metadata.\n    \"\"\"\n    md_copy = copy.deepcopy(metadata)\n    base64_encoded_keys = []\n    sens_keys = []\n    for key, val in metadata.items():\n        if key_path:\n            sub_key_path = key_path + \"/\" + key\n        else:\n            sub_key_path = key\n        if (\n            key.lower() in sensitive_keys\n            or sub_key_path.lower() in sensitive_keys\n        ):\n            sens_keys.append(sub_key_path)\n        if isinstance(val, str) and val.startswith(\"ci-b64:\"):\n            base64_encoded_keys.append(sub_key_path)\n            md_copy[key] = val.replace(\"ci-b64:\", \"\")\n        if isinstance(val, dict):\n            return_val = process_instance_metadata(\n                val, sub_key_path, sensitive_keys\n            )\n            base64_encoded_keys.extend(return_val.pop(\"base64_encoded_keys\"))\n            sens_keys.extend(return_val.pop(\"sensitive_keys\"))\n            md_copy[key] = return_val\n    md_copy[\"base64_encoded_keys\"] = sorted(base64_encoded_keys)\n    md_copy[\"sensitive_keys\"] = sorted(sens_keys)\n    return md_copy\n\n\ndef redact_sensitive_keys(metadata, redact_value=REDACT_SENSITIVE_VALUE):\n    \"\"\"Redact any sensitive keys from to provided metadata dictionary.\n\n    Replace any keys values listed in 'sensitive_keys' with redact_value.\n    \"\"\"\n    # While 'sensitive_keys' should already sanitized to only include what\n    # is in metadata, it is possible keys will overlap. For example, if\n    # \"merged_cfg\" and \"merged_cfg/ds/userdata\" both match, it's possible that\n    # \"merged_cfg\" will get replaced first, meaning \"merged_cfg/ds/userdata\"\n    # no longer represents a valid key.\n    # Thus, we still need to do membership checks in this function.\n    if not metadata.get(\"sensitive_keys\", []):\n        return metadata\n    md_copy = copy.deepcopy(metadata)\n    for key_path in metadata.get(\"sensitive_keys\"):\n        path_parts = key_path.split(\"/\")\n        obj = md_copy\n        for path in path_parts:\n            if (\n                path in obj\n                and isinstance(obj[path], dict)\n                and path != path_parts[-1]\n            ):\n                obj = obj[path]\n        if path in obj:\n            obj[path] = redact_value\n    return md_copy\n\n\nURLParams = namedtuple(\n    \"URLParams\",\n    [\n        \"max_wait_seconds\",\n        \"timeout_seconds\",\n        \"num_retries\",\n        \"sec_between_retries\",\n    ],\n)\n\nDataSourceHostname = namedtuple(\n    \"DataSourceHostname\",\n    [\"hostname\", \"is_default\"],\n)\n\n\nclass DataSource(CloudInitPickleMixin, metaclass=abc.ABCMeta):\n\n    dsmode = DSMODE_NETWORK\n    default_locale = \"en_US.UTF-8\"\n\n    # Datasource name needs to be set by subclasses to determine which\n    # cloud-config datasource key is loaded\n    dsname = \"_undef\"\n\n    # Cached cloud_name as determined by _get_cloud_name\n    _cloud_name = None\n\n    # Cached cloud platform api type: e.g. ec2, openstack, kvm, lxd, azure etc.\n    _platform_type = None\n\n    # More details about the cloud platform:\n    #  - metadata (http://169.254.169.254/)\n    #  - seed-dir (<dirname>)\n    _subplatform = None\n\n    # Track the discovered fallback nic for use in configuration generation.\n    _fallback_interface = None\n\n    # The network configuration sources that should be considered for this data\n    # source.  (The first source in this list that provides network\n    # configuration will be used without considering any that follow.)  This\n    # should always be a subset of the members of NetworkConfigSource with no\n    # duplicate entries.\n    network_config_sources: Tuple[NetworkConfigSource, ...] = (\n        NetworkConfigSource.CMD_LINE,\n        NetworkConfigSource.INITRAMFS,\n        NetworkConfigSource.SYSTEM_CFG,\n        NetworkConfigSource.DS,\n    )\n\n    # read_url_params\n    url_max_wait = -1  # max_wait < 0 means do not wait\n    url_timeout = 10  # timeout for each metadata url read attempt\n    url_retries = 5  # number of times to retry url upon 404\n    url_sec_between_retries = 1  # amount of seconds to wait between retries\n\n    # The datasource defines a set of supported EventTypes during which\n    # the datasource can react to changes in metadata and regenerate\n    # network configuration on metadata changes. These are defined in\n    # `supported_network_events`.\n    # The datasource also defines a set of default EventTypes that the\n    # datasource can react to. These are the event types that will be used\n    # if not overridden by the user.\n    # A datasource requiring to write network config on each system boot\n    # would call default_update_events['network'].add(EventType.BOOT).\n\n    # Default: generate network config on new instance id (first boot).\n    supported_update_events = {\n        EventScope.NETWORK: {\n            EventType.BOOT_NEW_INSTANCE,\n            EventType.BOOT,\n            EventType.BOOT_LEGACY,\n            EventType.HOTPLUG,\n        }\n    }\n    default_update_events = {\n        EventScope.NETWORK: {\n            EventType.BOOT_NEW_INSTANCE,\n        }\n    }\n\n    # N-tuple listing default values for any metadata-related class\n    # attributes cached on an instance by a process_data runs. These attribute\n    # values are reset via clear_cached_attrs during any update_metadata call.\n    cached_attr_defaults: Tuple[Tuple[str, Any], ...] = (\n        (\"ec2_metadata\", UNSET),\n        (\"network_json\", UNSET),\n        (\"metadata\", {}),\n        (\"userdata\", None),\n        (\"userdata_raw\", None),\n        (\"vendordata\", None),\n        (\"vendordata_raw\", None),\n        (\"vendordata2\", None),\n        (\"vendordata2_raw\", None),\n    )\n\n    _dirty_cache = False\n\n    # N-tuple of keypaths or keynames redact from instance-data.json for\n    # non-root users\n    sensitive_metadata_keys: Tuple[str, ...] = (\n        \"merged_cfg\",\n        \"security-credentials\",\n        \"userdata\",\n        \"user-data\",\n        \"user_data\",\n        \"vendordata\",\n        \"vendor-data\",\n        # Provide ds/vendor_data to avoid redacting top-level\n        #  \"vendor_data\": {enabled: True}\n        \"ds/vendor_data\",\n    )\n\n    # True on datasources that may not see hotplugged devices reflected\n    # in the updated metadata\n    skip_hotplug_detect = False\n\n    _ci_pkl_version = 1\n\n    def __init__(self, sys_cfg, distro: Distro, paths: Paths, ud_proc=None):\n        self.sys_cfg = sys_cfg\n        self.distro = distro\n        self.paths = paths\n        self.userdata = None\n        self.metadata: dict = {}\n        self.userdata_raw = None\n        self.vendordata = None\n        self.vendordata2 = None\n        self.vendordata_raw = None\n        self.vendordata2_raw = None\n\n        self.ds_cfg = util.get_cfg_by_path(\n            self.sys_cfg, (\"datasource\", self.dsname), {}\n        )\n        if not self.ds_cfg:\n            self.ds_cfg = {}\n\n        if not ud_proc:\n            self.ud_proc = ud.UserDataProcessor(self.paths)\n        else:\n            self.ud_proc = ud_proc\n\n    def _unpickle(self, ci_pkl_version: int) -> None:\n        \"\"\"Perform deserialization fixes for Paths.\"\"\"\n        if not hasattr(self, \"vendordata2\"):\n            self.vendordata2 = None\n        if not hasattr(self, \"vendordata2_raw\"):\n            self.vendordata2_raw = None\n        if not hasattr(self, \"skip_hotplug_detect\"):\n            self.skip_hotplug_detect = False\n        if hasattr(self, \"userdata\") and self.userdata is not None:\n            # If userdata stores MIME data, on < python3.6 it will be\n            # missing the 'policy' attribute that exists on >=python3.6.\n            # Calling str() on the userdata will attempt to access this\n            # policy attribute. This will raise an exception, causing\n            # the pickle load to fail, so cloud-init will discard the cache\n            try:\n                str(self.userdata)\n            except AttributeError as e:\n                LOG.debug(\n                    \"Unable to unpickle datasource: %s.\"\n                    \" Ignoring current cache.\",\n                    e,\n                )\n                raise DatasourceUnpickleUserDataError() from e\n\n    def __str__(self):\n        return type_utils.obj_name(self)\n\n    def ds_detect(self) -> bool:\n        \"\"\"Check if running on this datasource\"\"\"\n        return True\n\n    def override_ds_detect(self) -> bool:\n        \"\"\"Override if either:\n        - only a single datasource defined (nothing to fall back to)\n        - commandline argument is used (ci.ds=OpenStack)\n\n        Note: get_cmdline() is required for the general case - when ds-identify\n        does not run, _something_ needs to detect the kernel command line\n        definition.\n        \"\"\"\n        if self.dsname.lower() == parse_cmdline().lower():\n            LOG.debug(\n                \"Machine is configured by the kernel commandline to run on \"\n                \"single datasource %s.\",\n                self,\n            )\n            return True\n        elif self.sys_cfg.get(\"datasource_list\", []) in (\n            [self.dsname],\n            [self.dsname, \"None\"],\n        ):\n            LOG.debug(\n                \"Machine is configured to run on single datasource %s.\", self\n            )\n            return True\n        return False\n\n    def _check_and_get_data(self):\n        \"\"\"Overrides runtime datasource detection\"\"\"\n        if self.override_ds_detect():\n            return self._get_data()\n        elif self.ds_detect():\n            LOG.debug(\"Machine is running on %s.\", self)\n            return self._get_data()\n        else:\n            LOG.debug(\"Datasource type %s is not detected.\", self)\n            return False\n\n    def _get_standardized_metadata(self, instance_data):\n        \"\"\"Return a dictionary of standardized metadata keys.\"\"\"\n        local_hostname = self.get_hostname().hostname\n        instance_id = self.get_instance_id()\n        availability_zone = self.availability_zone\n        # In the event of upgrade from existing cloudinit, pickled datasource\n        # will not contain these new class attributes. So we need to recrawl\n        # metadata to discover that content\n        sysinfo = instance_data[\"sys_info\"]\n        return {\n            \"v1\": {\n                \"_beta_keys\": [\"subplatform\"],\n                \"availability-zone\": availability_zone,\n                \"availability_zone\": availability_zone,\n                \"cloud_id\": canonical_cloud_id(\n                    self.cloud_name, self.region, self.platform_type\n                ),\n                \"cloud-name\": self.cloud_name,\n                \"cloud_name\": self.cloud_name,\n                \"distro\": sysinfo[\"dist\"][0],\n                \"distro_version\": sysinfo[\"dist\"][1],\n                \"distro_release\": sysinfo[\"dist\"][2],\n                \"platform\": self.platform_type,\n                \"public_ssh_keys\": self.get_public_ssh_keys(),\n                \"python_version\": sysinfo[\"python\"],\n                \"instance-id\": instance_id,\n                \"instance_id\": instance_id,\n                \"kernel_release\": sysinfo[\"uname\"][2],\n                \"local-hostname\": local_hostname,\n                \"local_hostname\": local_hostname,\n                \"machine\": sysinfo[\"uname\"][4],\n                \"region\": self.region,\n                \"subplatform\": self.subplatform,\n                \"system_platform\": sysinfo[\"platform\"],\n                \"variant\": sysinfo[\"variant\"],\n            }\n        }\n\n    def clear_cached_attrs(self, attr_defaults=()):\n        \"\"\"Reset any cached metadata attributes to datasource defaults.\n\n        @param attr_defaults: Optional tuple of (attr, value) pairs to\n           set instead of cached_attr_defaults.\n        \"\"\"\n        if not self._dirty_cache:\n            return\n        if attr_defaults:\n            attr_values = attr_defaults\n        else:\n            attr_values = self.cached_attr_defaults\n\n        for attribute, value in attr_values:\n            if hasattr(self, attribute):\n                setattr(self, attribute, value)\n        if not attr_defaults:\n            self._dirty_cache = False\n\n    def get_data(self) -> bool:\n        \"\"\"Datasources implement _get_data to setup metadata and userdata_raw.\n\n        Minimally, the datasource should return a boolean True on success.\n        \"\"\"\n        self._dirty_cache = True\n        return_value = self._check_and_get_data()\n        if not return_value:\n            return return_value\n        self.persist_instance_data()\n        return return_value\n\n    def persist_instance_data(self, write_cache=True):\n        \"\"\"Process and write INSTANCE_JSON_FILE with all instance metadata.\n\n        Replace any hyphens with underscores in key names for use in template\n        processing.\n\n        :param write_cache: boolean set True to persist obj.pkl when\n            instance_link exists.\n\n        @return True on successful write, False otherwise.\n        \"\"\"\n        if write_cache and os.path.lexists(self.paths.instance_link):\n            pkl_store(self, self.paths.get_ipath_cur(\"obj_pkl\"))\n        if hasattr(self, \"_crawled_metadata\"):\n            # Any datasource with _crawled_metadata will best represent\n            # most recent, 'raw' metadata\n            crawled_metadata = copy.deepcopy(\n                getattr(self, \"_crawled_metadata\")\n            )\n            crawled_metadata.pop(\"user-data\", None)\n            crawled_metadata.pop(\"vendor-data\", None)\n            instance_data = {\"ds\": crawled_metadata}\n        else:\n            instance_data = {\"ds\": {\"meta_data\": self.metadata}}\n            if hasattr(self, \"network_json\"):\n                network_json = getattr(self, \"network_json\")\n                if network_json != UNSET:\n                    instance_data[\"ds\"][\"network_json\"] = network_json\n            if hasattr(self, \"ec2_metadata\"):\n                ec2_metadata = getattr(self, \"ec2_metadata\")\n                if ec2_metadata != UNSET:\n                    instance_data[\"ds\"][\"ec2_metadata\"] = ec2_metadata\n        instance_data[\"ds\"][\"_doc\"] = EXPERIMENTAL_TEXT\n        # Add merged cloud.cfg and sys info for jinja templates and cli query\n        instance_data[\"merged_cfg\"] = copy.deepcopy(self.sys_cfg)\n        instance_data[\"merged_cfg\"][\"_doc\"] = (\n            \"Merged cloud-init system config from /etc/cloud/cloud.cfg and\"\n            \" /etc/cloud/cloud.cfg.d/\"\n        )\n        instance_data[\"sys_info\"] = util.system_info()\n        instance_data.update(self._get_standardized_metadata(instance_data))\n        try:\n            # Process content base64encoding unserializable values\n            content = util.json_dumps(instance_data)\n            # Strip base64: prefix and set base64_encoded_keys list.\n            processed_data = process_instance_metadata(\n                json.loads(content),\n                sensitive_keys=self.sensitive_metadata_keys,\n            )\n        except TypeError as e:\n            LOG.warning(\"Error persisting instance-data.json: %s\", str(e))\n            return False\n        except UnicodeDecodeError as e:\n            LOG.warning(\"Error persisting instance-data.json: %s\", str(e))\n            return False\n        json_sensitive_file = self.paths.get_runpath(\"instance_data_sensitive\")\n        cloud_id = instance_data[\"v1\"].get(\"cloud_id\", \"none\")\n        cloud_id_file = os.path.join(self.paths.run_dir, \"cloud-id\")\n        util.write_file(f\"{cloud_id_file}-{cloud_id}\", f\"{cloud_id}\\n\")\n        # cloud-id not found, then no previous cloud-id fle\n        prev_cloud_id_file = None\n        new_cloud_id_file = f\"{cloud_id_file}-{cloud_id}\"\n        # cloud-id found, then the prev cloud-id file is source of symlink\n        if os.path.exists(cloud_id_file):\n            prev_cloud_id_file = os.path.realpath(cloud_id_file)\n\n        util.sym_link(new_cloud_id_file, cloud_id_file, force=True)\n        if prev_cloud_id_file and prev_cloud_id_file != new_cloud_id_file:\n            util.del_file(prev_cloud_id_file)\n        write_json(json_sensitive_file, processed_data, mode=0o600)\n        json_file = self.paths.get_runpath(\"instance_data\")\n        # World readable\n        write_json(json_file, redact_sensitive_keys(processed_data))\n        return True\n\n    def _get_data(self) -> bool:\n        \"\"\"Walk metadata sources, process crawled data and save attributes.\"\"\"\n        raise NotImplementedError(\n            \"Subclasses of DataSource must implement _get_data which\"\n            \" sets self.metadata, vendordata_raw and userdata_raw.\"\n        )\n\n    def get_url_params(self):\n        \"\"\"Return the Datasource's preferred url_read parameters.\n\n        Subclasses may override url_max_wait, url_timeout, url_retries.\n\n        @return: A URLParams object with max_wait_seconds, timeout_seconds,\n            num_retries.\n        \"\"\"\n        max_wait = self.url_max_wait\n        try:\n            max_wait = int(self.ds_cfg.get(\"max_wait\", self.url_max_wait))\n        except ValueError:\n            util.logexc(\n                LOG,\n                \"Config max_wait '%s' is not an int, using default '%s'\",\n                self.ds_cfg.get(\"max_wait\"),\n                max_wait,\n            )\n\n        timeout = self.url_timeout\n        try:\n            timeout = max(0, int(self.ds_cfg.get(\"timeout\", self.url_timeout)))\n        except ValueError:\n            timeout = self.url_timeout\n            util.logexc(\n                LOG,\n                \"Config timeout '%s' is not an int, using default '%s'\",\n                self.ds_cfg.get(\"timeout\"),\n                timeout,\n            )\n\n        retries = self.url_retries\n        try:\n            retries = int(self.ds_cfg.get(\"retries\", self.url_retries))\n        except Exception:\n            util.logexc(\n                LOG,\n                \"Config retries '%s' is not an int, using default '%s'\",\n                self.ds_cfg.get(\"retries\"),\n                retries,\n            )\n\n        sec_between_retries = self.url_sec_between_retries\n        try:\n            sec_between_retries = int(\n                self.ds_cfg.get(\n                    \"sec_between_retries\", self.url_sec_between_retries\n                )\n            )\n        except Exception:\n            util.logexc(\n                LOG,\n                \"Config sec_between_retries '%s' is not an int,\"\n                \" using default '%s'\",\n                self.ds_cfg.get(\"sec_between_retries\"),\n                sec_between_retries,\n            )\n\n        return URLParams(max_wait, timeout, retries, sec_between_retries)\n\n    def get_userdata(self, apply_filter=False):\n        if self.userdata is None:\n            self.userdata = self.ud_proc.process(self.get_userdata_raw())\n        if apply_filter:\n            return self._filter_xdata(self.userdata)\n        return self.userdata\n\n    def get_vendordata(self):\n        if self.vendordata is None:\n            self.vendordata = self.ud_proc.process(self.get_vendordata_raw())\n        return self.vendordata\n\n    def get_vendordata2(self):\n        if self.vendordata2 is None:\n            self.vendordata2 = self.ud_proc.process(self.get_vendordata2_raw())\n        return self.vendordata2\n\n    @property\n    def fallback_interface(self):\n        \"\"\"Determine the network interface used during local network config.\"\"\"\n        if self._fallback_interface is None:\n            self._fallback_interface = net.find_fallback_nic()\n            if self._fallback_interface is None:\n                LOG.warning(\n                    \"Did not find a fallback interface on %s.\", self.cloud_name\n                )\n        return self._fallback_interface\n\n    @property\n    def platform_type(self):\n        if not hasattr(self, \"_platform_type\"):\n            # Handle upgrade path where pickled datasource has no _platform.\n            self._platform_type = self.dsname.lower()\n        if not self._platform_type:\n            self._platform_type = self.dsname.lower()\n        return self._platform_type\n\n    @property\n    def subplatform(self):\n        \"\"\"Return a string representing subplatform details for the datasource.\n\n        This should be guidance for where the metadata is sourced.\n        Examples of this on different clouds:\n            ec2:       metadata (http://169.254.169.254)\n            openstack: configdrive (/dev/path)\n            openstack: metadata (http://169.254.169.254)\n            nocloud:   seed-dir (/seed/dir/path)\n            lxd:   nocloud (/seed/dir/path)\n        \"\"\"\n        if not hasattr(self, \"_subplatform\"):\n            # Handle upgrade path where pickled datasource has no _platform.\n            self._subplatform = self._get_subplatform()\n        if not self._subplatform:\n            self._subplatform = self._get_subplatform()\n        return self._subplatform\n\n    def _get_subplatform(self):\n        \"\"\"Subclasses should implement to return a \"slug (detail)\" string.\"\"\"\n        if hasattr(self, \"metadata_address\"):\n            return \"metadata (%s)\" % getattr(self, \"metadata_address\")\n        return METADATA_UNKNOWN\n\n    @property\n    def cloud_name(self):\n        \"\"\"Return lowercase cloud name as determined by the datasource.\n\n        Datasource can determine or define its own cloud product name in\n        metadata.\n        \"\"\"\n        if self._cloud_name:\n            return self._cloud_name\n        if self.metadata and self.metadata.get(METADATA_CLOUD_NAME_KEY):\n            cloud_name = self.metadata.get(METADATA_CLOUD_NAME_KEY)\n            if isinstance(cloud_name, str):\n                self._cloud_name = cloud_name.lower()\n            else:\n                self._cloud_name = self._get_cloud_name().lower()\n                LOG.debug(\n                    \"Ignoring metadata provided key %s: non-string type %s\",\n                    METADATA_CLOUD_NAME_KEY,\n                    type(cloud_name),\n                )\n        else:\n            self._cloud_name = self._get_cloud_name().lower()\n        return self._cloud_name\n\n    def _get_cloud_name(self):\n        \"\"\"Return the datasource name as it frequently matches cloud name.\n\n        Should be overridden in subclasses which can run on multiple\n        cloud names, such as DatasourceEc2.\n        \"\"\"\n        return self.dsname\n\n    @property\n    def launch_index(self):\n        if not self.metadata:\n            return None\n        if \"launch-index\" in self.metadata:\n            return self.metadata[\"launch-index\"]\n        return None\n\n    def _filter_xdata(self, processed_ud):\n        filters = [\n            launch_index.Filter(util.safe_int(self.launch_index)),\n        ]\n        new_ud = processed_ud\n        for f in filters:\n            new_ud = f.apply(new_ud)\n        return new_ud\n\n    @property\n    def is_disconnected(self):\n        return False\n\n    def get_userdata_raw(self):\n        return self.userdata_raw\n\n    def get_vendordata_raw(self):\n        return self.vendordata_raw\n\n    def get_vendordata2_raw(self):\n        return self.vendordata2_raw\n\n    # the data sources' config_obj is a cloud-config formated\n    # object that came to it from ways other than cloud-config\n    # because cloud-config content would be handled elsewhere\n    def get_config_obj(self):\n        return {}\n\n    def get_public_ssh_keys(self):\n        return normalize_pubkey_data(self.metadata.get(\"public-keys\"))\n\n    def publish_host_keys(self, hostkeys):\n        \"\"\"Publish the public SSH host keys (found in /etc/ssh/*.pub).\n\n        @param hostkeys: List of host key tuples (key_type, key_value),\n            where key_type is the first field in the public key file\n            (e.g. 'ssh-rsa') and key_value is the key itself\n            (e.g. 'AAAAB3NzaC1y...').\n        \"\"\"\n\n    def _remap_device(self, short_name):\n        # LP: #611137\n        # the metadata service may believe that devices are named 'sda'\n        # when the kernel named them 'vda' or 'xvda'\n        # we want to return the correct value for what will actually\n        # exist in this instance\n        mappings = {\"sd\": (\"vd\", \"xvd\", \"vtb\")}\n        for (nfrom, tlist) in mappings.items():\n            if not short_name.startswith(nfrom):\n                continue\n            for nto in tlist:\n                cand = \"/dev/%s%s\" % (nto, short_name[len(nfrom) :])\n                if os.path.exists(cand):\n                    return cand\n        return None\n\n    def device_name_to_device(self, _name):\n        # translate a 'name' to a device\n        # the primary function at this point is on ec2\n        # to consult metadata service, that has\n        #  ephemeral0: sdb\n        # and return 'sdb' for input 'ephemeral0'\n        return None\n\n    def get_locale(self):\n        \"\"\"Default locale is en_US.UTF-8, but allow distros to override\"\"\"\n        locale = self.default_locale\n        try:\n            locale = self.distro.get_locale()\n        except NotImplementedError:\n            pass\n        return locale\n\n    @property\n    def availability_zone(self):\n        top_level_az = self.metadata.get(\n            \"availability-zone\", self.metadata.get(\"availability_zone\")\n        )\n        if top_level_az:\n            return top_level_az\n        return self.metadata.get(\"placement\", {}).get(\"availability-zone\")\n\n    @property\n    def region(self):\n        return self.metadata.get(\"region\")\n\n    def get_instance_id(self):\n        if not self.metadata or \"instance-id\" not in self.metadata:\n            # Return a magic not really instance id string\n            return \"iid-datasource\"\n        return str(self.metadata[\"instance-id\"])\n\n    def get_hostname(self, fqdn=False, resolve_ip=False, metadata_only=False):\n        \"\"\"Get hostname or fqdn from the datasource. Look it up if desired.\n\n        @param fqdn: Boolean, set True to return hostname with domain.\n        @param resolve_ip: Boolean, set True to attempt to resolve an ipv4\n            address provided in local-hostname meta-data.\n        @param metadata_only: Boolean, set True to avoid looking up hostname\n            if meta-data doesn't have local-hostname present.\n\n        @return: a DataSourceHostname namedtuple\n            <hostname or qualified hostname>, <is_default> (str, bool).\n            is_default is a bool and\n            it's true only if hostname is localhost and was\n            returned by util.get_hostname() as a default.\n            This is used to differentiate with a user-defined\n            localhost hostname.\n            Optionally return (None, False) when\n            metadata_only is True and local-hostname data is not available.\n        \"\"\"\n        defdomain = \"localdomain\"\n        defhost = \"localhost\"\n        domain = defdomain\n        is_default = False\n\n        if not self.metadata or not self.metadata.get(\"local-hostname\"):\n            if metadata_only:\n                return DataSourceHostname(None, is_default)\n            # this is somewhat questionable really.\n            # the cloud datasource was asked for a hostname\n            # and didn't have one. raising error might be more appropriate\n            # but instead, basically look up the existing hostname\n            toks = []\n            hostname = util.get_hostname()\n            if hostname == \"localhost\":\n                # default hostname provided by socket.gethostname()\n                is_default = True\n            hosts_fqdn = util.get_fqdn_from_hosts(hostname)\n            if hosts_fqdn and hosts_fqdn.find(\".\") > 0:\n                toks = str(hosts_fqdn).split(\".\")\n            elif hostname and hostname.find(\".\") > 0:\n                toks = str(hostname).split(\".\")\n            elif hostname:\n                toks = [hostname, defdomain]\n            else:\n                toks = [defhost, defdomain]\n        else:\n            # if there is an ipv4 address in 'local-hostname', then\n            # make up a hostname (LP: #475354) in format ip-xx.xx.xx.xx\n            lhost = self.metadata[\"local-hostname\"]\n            if net.is_ipv4_address(lhost):\n                toks = []\n                if resolve_ip:\n                    toks = util.gethostbyaddr(lhost)\n\n                if toks:\n                    toks = str(toks).split(\".\")\n                else:\n                    toks = [\"ip-%s\" % lhost.replace(\".\", \"-\")]\n            else:\n                toks = lhost.split(\".\")\n\n        if len(toks) > 1:\n            hostname = toks[0]\n            domain = \".\".join(toks[1:])\n        else:\n            hostname = toks[0]\n\n        if fqdn and domain != defdomain:\n            hostname = \"%s.%s\" % (hostname, domain)\n\n        return DataSourceHostname(hostname, is_default)\n\n    def get_package_mirror_info(self):\n        return self.distro.get_package_mirror_info(data_source=self)\n\n    def get_supported_events(self, source_event_types: List[EventType]):\n        supported_events: Dict[EventScope, set] = {}\n        for event in source_event_types:\n            for (\n                update_scope,\n                update_events,\n            ) in self.supported_update_events.items():\n                if event in update_events:\n                    if not supported_events.get(update_scope):\n                        supported_events[update_scope] = set()\n                    supported_events[update_scope].add(event)\n        return supported_events\n\n    def update_metadata_if_supported(\n        self, source_event_types: List[EventType]\n    ) -> bool:\n        \"\"\"Refresh cached metadata if the datasource supports this event.\n\n        The datasource has a list of supported_update_events which\n        trigger refreshing all cached metadata as well as refreshing the\n        network configuration.\n\n        @param source_event_types: List of EventTypes which may trigger a\n            metadata update.\n\n        @return True if the datasource did successfully update cached metadata\n            due to source_event_type.\n        \"\"\"\n        supported_events = self.get_supported_events(source_event_types)\n        for scope, matched_events in supported_events.items():\n            LOG.debug(\n                \"Update datasource metadata and %s config due to events: %s\",\n                scope.value,\n                \", \".join([event.value for event in matched_events]),\n            )\n            # Each datasource has a cached config property which needs clearing\n            # Once cleared that config property will be regenerated from\n            # current metadata.\n            self.clear_cached_attrs(((\"_%s_config\" % scope, UNSET),))\n        if supported_events:\n            self.clear_cached_attrs()\n            result = self.get_data()\n            if result:\n                return True\n        LOG.debug(\n            \"Datasource %s not updated for events: %s\",\n            self,\n            \", \".join([event.value for event in source_event_types]),\n        )\n        return False\n\n    def check_instance_id(self, sys_cfg):\n        # quickly (local check only) if self.instance_id is still\n        return False\n\n    @staticmethod\n    def _determine_dsmode(candidates, default=None, valid=None):\n        # return the first candidate that is non None, warn if not valid\n        if default is None:\n            default = DSMODE_NETWORK\n\n        if valid is None:\n            valid = VALID_DSMODES\n\n        for candidate in candidates:\n            if candidate is None:\n                continue\n            if candidate in valid:\n                return candidate\n            else:\n                LOG.warning(\n                    \"invalid dsmode '%s', using default=%s\", candidate, default\n                )\n                return default\n\n        return default\n\n    @property\n    def network_config(self):\n        return None\n\n    def setup(self, is_new_instance):\n        \"\"\"setup(is_new_instance)\n\n        This is called before user-data and vendor-data have been processed.\n\n        Unless the datasource has set mode to 'local', then networking\n        per 'fallback' or per 'network_config' will have been written and\n        brought up the OS at this point.\n        \"\"\"\n        return\n\n    def activate(self, cfg, is_new_instance):\n        \"\"\"activate(cfg, is_new_instance)\n\n        This is called before the init_modules will be called but after\n        the user-data and vendor-data have been fully processed.\n\n        The cfg is fully up to date config, it contains a merged view of\n           system config, datasource config, user config, vendor config.\n        It should be used rather than the sys_cfg passed to __init__.\n\n        is_new_instance is a boolean indicating if this is a new instance.\n        \"\"\"\n        return\n\n\ndef normalize_pubkey_data(pubkey_data):\n    keys = []\n\n    if not pubkey_data:\n        return keys\n\n    if isinstance(pubkey_data, str):\n        return pubkey_data.splitlines()\n\n    if isinstance(pubkey_data, (list, set)):\n        return list(pubkey_data)\n\n    if isinstance(pubkey_data, (dict)):\n        for (_keyname, klist) in pubkey_data.items():\n            # lp:506332 uec metadata service responds with\n            # data that makes boto populate a string for 'klist' rather\n            # than a list.\n            if isinstance(klist, str):\n                klist = [klist]\n            if isinstance(klist, (list, set)):\n                for pkey in klist:\n                    # There is an empty string at\n                    # the end of the keylist, trim it\n                    if pkey:\n                        keys.append(pkey)\n\n    return keys\n\n\ndef find_source(\n    sys_cfg, distro, paths, ds_deps, cfg_list, pkg_list, reporter\n) -> Tuple[DataSource, str]:\n    ds_list = list_sources(cfg_list, ds_deps, pkg_list)\n    ds_names = [type_utils.obj_name(f) for f in ds_list]\n    mode = \"network\" if DEP_NETWORK in ds_deps else \"local\"\n    LOG.debug(\"Searching for %s data source in: %s\", mode, ds_names)\n\n    for name, cls in zip(ds_names, ds_list):\n        myrep = events.ReportEventStack(\n            name=\"search-%s\" % name.replace(\"DataSource\", \"\"),\n            description=\"searching for %s data from %s\" % (mode, name),\n            message=\"no %s data found from %s\" % (mode, name),\n            parent=reporter,\n        )\n        try:\n            with myrep:\n                LOG.debug(\"Seeing if we can get any data from %s\", cls)\n                s = cls(sys_cfg, distro, paths)\n                if s.update_metadata_if_supported(\n                    [EventType.BOOT_NEW_INSTANCE]\n                ):\n                    myrep.message = \"found %s data from %s\" % (mode, name)\n                    return (s, type_utils.obj_name(cls))\n        except Exception:\n            util.logexc(LOG, \"Getting data from %s failed\", cls)\n\n    msg = \"Did not find any data source, searched classes: (%s)\" % \", \".join(\n        ds_names\n    )\n    raise DataSourceNotFoundException(msg)\n\n\ndef list_sources(cfg_list, depends, pkg_list):\n    \"\"\"Return a list of classes that have the same depends as 'depends'\n    iterate through cfg_list, loading \"DataSource*\" modules\n    and calling their \"get_datasource_list\".\n    Return an ordered list of classes that match (if any)\n    \"\"\"\n    src_list = []\n    LOG.debug(\n        \"Looking for data source in: %s,\"\n        \" via packages %s that matches dependencies %s\",\n        cfg_list,\n        pkg_list,\n        depends,\n    )\n\n    for ds in cfg_list:\n        ds_name = importer.match_case_insensitive_module_name(ds)\n        m_locs, _looked_locs = importer.find_module(\n            ds_name, pkg_list, [\"get_datasource_list\"]\n        )\n        if not m_locs:\n            LOG.error(\n                \"Could not import %s. Does the DataSource exist and \"\n                \"is it importable?\",\n                ds_name,\n            )\n        for m_loc in m_locs:\n            mod = importer.import_module(m_loc)\n            lister = getattr(mod, \"get_datasource_list\")\n            matches = lister(depends)\n            if matches:\n                src_list.extend(matches)\n                break\n    return src_list\n\n\ndef instance_id_matches_system_uuid(\n    instance_id, field: str = \"system-uuid\"\n) -> bool:\n    # quickly (local check only) if self.instance_id is still valid\n    # we check kernel command line or files.\n    if not instance_id:\n        return False\n\n    dmi_value = dmi.read_dmi_data(field)\n    if not dmi_value:\n        return False\n    return instance_id.lower() == dmi_value.lower()\n\n\ndef canonical_cloud_id(cloud_name, region, platform):\n    \"\"\"Lookup the canonical cloud-id for a given cloud_name and region.\"\"\"\n    if not cloud_name:\n        cloud_name = METADATA_UNKNOWN\n    if not region:\n        region = METADATA_UNKNOWN\n    if region == METADATA_UNKNOWN:\n        if cloud_name != METADATA_UNKNOWN:\n            return cloud_name\n        return platform\n    for prefix, cloud_id_test in CLOUD_ID_REGION_PREFIX_MAP.items():\n        (cloud_id, valid_cloud) = cloud_id_test\n        if region.startswith(prefix) and valid_cloud(cloud_name):\n            return cloud_id\n    if cloud_name != METADATA_UNKNOWN:\n        return cloud_name\n    return platform\n\n\ndef convert_vendordata(data, recurse=True):\n    \"\"\"data: a loaded object (strings, arrays, dicts).\n    return something suitable for cloudinit vendordata_raw.\n\n    if data is:\n       None: return None\n       string: return string\n       list: return data\n             the list is then processed in UserDataProcessor\n       dict: return convert_vendordata(data.get('cloud-init'))\n    \"\"\"\n    if not data:\n        return None\n    if isinstance(data, str):\n        return data\n    if isinstance(data, list):\n        return copy.deepcopy(data)\n    if isinstance(data, dict):\n        if recurse is True:\n            return convert_vendordata(data.get(\"cloud-init\"), recurse=False)\n        raise ValueError(\"vendordata['cloud-init'] cannot be dict\")\n    raise ValueError(\"Unknown data type for vendordata: %s\" % type(data))\n\n\nclass BrokenMetadata(IOError):\n    pass\n\n\n# 'depends' is a list of dependencies (DEP_FILESYSTEM)\n# ds_list is a list of 2 item lists\n# ds_list = [\n#   ( class, ( depends-that-this-class-needs ) )\n# }\n# It returns a list of 'class' that matched these deps exactly\n# It mainly is a helper function for DataSourceCollections\ndef list_from_depends(depends, ds_list):\n    ret_list = []\n    depset = set(depends)\n    for (cls, deps) in ds_list:\n        if depset == set(deps):\n            ret_list.append(cls)\n    return ret_list\n\n\ndef pkl_store(obj: DataSource, fname: str) -> bool:\n    \"\"\"Use pickle to serialize Datasource to a file as a cache.\n\n    :return: True on success\n    \"\"\"\n    try:\n        pk_contents = pickle.dumps(obj)\n    except Exception:\n        util.logexc(LOG, \"Failed pickling datasource %s\", obj)\n        return False\n    try:\n        util.write_file(fname, pk_contents, omode=\"wb\", mode=0o400)\n    except Exception:\n        util.logexc(LOG, \"Failed pickling datasource to %s\", fname)\n        return False\n    return True\n\n\ndef pkl_load(fname: str) -> Optional[DataSource]:\n    \"\"\"Use pickle to deserialize a instance Datasource from a cache file.\"\"\"\n    pickle_contents = None\n    try:\n        pickle_contents = util.load_file(fname, decode=False)\n    except Exception as e:\n        if os.path.isfile(fname):\n            LOG.warning(\"failed loading pickle in %s: %s\", fname, e)\n\n    # This is allowed so just return nothing successfully loaded...\n    if not pickle_contents:\n        return None\n    try:\n        return pickle.loads(pickle_contents)\n    except DatasourceUnpickleUserDataError:\n        return None\n    except Exception:\n        util.logexc(LOG, \"Failed loading pickled blob from %s\", fname)\n        return None\n\n\ndef parse_cmdline() -> str:\n    \"\"\"Check if command line argument for this datasource was passed\n    Passing by command line overrides runtime datasource detection\n    \"\"\"\n    cmdline = util.get_cmdline()\n    ds_parse_0 = re.search(r\"ds=([a-zA-Z]+)(\\s|$|;)\", cmdline)\n    ds_parse_1 = re.search(r\"ci\\.ds=([a-zA-Z]+)(\\s|$|;)\", cmdline)\n    ds_parse_2 = re.search(r\"ci\\.datasource=([a-zA-Z]+)(\\s|$|;)\", cmdline)\n    ds = ds_parse_0 or ds_parse_1 or ds_parse_2\n    deprecated = ds_parse_1 or ds_parse_2\n    if deprecated:\n        dsname = deprecated.group(1).strip()\n        util.deprecate(\n            deprecated=(\n                f\"Defining the datasource on the commandline using \"\n                f\"ci.ds={dsname} or \"\n                f\"ci.datasource={dsname}\"\n            ),\n            deprecated_version=\"23.2\",\n            extra_message=f\"Use ds={dsname} instead\",\n        )\n    if ds and ds.group(1):\n        return ds.group(1)\n    return \"\"\n", "# Copyright (C) 2012 Canonical Ltd.\n# Copyright (C) 2012, 2013 Hewlett-Packard Development Company, L.P.\n# Copyright (C) 2012 Yahoo! Inc.\n#\n# This file is part of cloud-init. See LICENSE file for license information.\n\nimport copy\nimport os\nimport sys\nfrom collections import namedtuple\nfrom typing import Dict, Iterable, List, Optional, Set\n\nfrom cloudinit import cloud, distros, handlers, helpers, importer\nfrom cloudinit import log as logging\nfrom cloudinit import net, sources, type_utils, util\nfrom cloudinit.event import EventScope, EventType, userdata_to_events\n\n# Default handlers (used if not overridden)\nfrom cloudinit.handlers.boot_hook import BootHookPartHandler\nfrom cloudinit.handlers.cloud_config import CloudConfigPartHandler\nfrom cloudinit.handlers.jinja_template import JinjaTemplatePartHandler\nfrom cloudinit.handlers.shell_script import ShellScriptPartHandler\nfrom cloudinit.handlers.shell_script_by_frequency import (\n    ShellScriptByFreqPartHandler,\n)\nfrom cloudinit.net import cmdline\nfrom cloudinit.reporting import events\nfrom cloudinit.settings import (\n    CLOUD_CONFIG,\n    PER_ALWAYS,\n    PER_INSTANCE,\n    PER_ONCE,\n    RUN_CLOUD_CONFIG,\n)\nfrom cloudinit.sources import NetworkConfigSource\n\nLOG = logging.getLogger(__name__)\n\nNO_PREVIOUS_INSTANCE_ID = \"NO_PREVIOUS_INSTANCE_ID\"\n\n\ndef update_event_enabled(\n    datasource: sources.DataSource,\n    cfg: dict,\n    event_source_type: EventType,\n    scope: Optional[EventScope] = None,\n) -> bool:\n    \"\"\"Determine if a particular EventType is enabled.\n\n    For the `event_source_type` passed in, check whether this EventType\n    is enabled in the `updates` section of the userdata. If `updates`\n    is not enabled in userdata, check if defined as one of the\n    `default_events` on the datasource. `scope` may be used to\n    narrow the check to a particular `EventScope`.\n\n    Note that on first boot, userdata may NOT be available yet. In this\n    case, we only have the data source's `default_update_events`,\n    so an event that should be enabled in userdata may be denied.\n    \"\"\"\n    default_events: Dict[\n        EventScope, Set[EventType]\n    ] = datasource.default_update_events\n    user_events: Dict[EventScope, Set[EventType]] = userdata_to_events(\n        cfg.get(\"updates\", {})\n    )\n    # A value in the first will override a value in the second\n    allowed = util.mergemanydict(\n        [\n            copy.deepcopy(user_events),\n            copy.deepcopy(default_events),\n        ]\n    )\n    LOG.debug(\"Allowed events: %s\", allowed)\n\n    scopes: Iterable[EventScope]\n    if not scope:\n        scopes = allowed.keys()\n    else:\n        scopes = [scope]\n    scope_values = [s.value for s in scopes]\n\n    for evt_scope in scopes:\n        if event_source_type in allowed.get(evt_scope, []):\n            LOG.debug(\n                \"Event Allowed: scope=%s EventType=%s\",\n                evt_scope.value,\n                event_source_type,\n            )\n            return True\n\n    LOG.debug(\n        \"Event Denied: scopes=%s EventType=%s\", scope_values, event_source_type\n    )\n    return False\n\n\nclass Init:\n    def __init__(self, ds_deps: Optional[List[str]] = None, reporter=None):\n        if ds_deps is not None:\n            self.ds_deps = ds_deps\n        else:\n            self.ds_deps = [sources.DEP_FILESYSTEM, sources.DEP_NETWORK]\n        # Created on first use\n        self._cfg: Optional[dict] = None\n        self._paths: Optional[helpers.Paths] = None\n        self._distro: Optional[distros.Distro] = None\n        # Changed only when a fetch occurs\n        self.datasource: Optional[sources.DataSource] = None\n        self.ds_restored = False\n        self._previous_iid = None\n\n        if reporter is None:\n            reporter = events.ReportEventStack(\n                name=\"init-reporter\",\n                description=\"init-desc\",\n                reporting_enabled=False,\n            )\n        self.reporter = reporter\n\n    def _reset(self, reset_ds=False):\n        # Recreated on access\n        self._cfg = None\n        self._paths = None\n        self._distro = None\n        if reset_ds:\n            self.datasource = None\n            self.ds_restored = False\n\n    @property\n    def distro(self):\n        if not self._distro:\n            # Try to find the right class to use\n            system_config = self._extract_cfg(\"system\")\n            distro_name = system_config.pop(\"distro\", \"ubuntu\")\n            distro_cls = distros.fetch(distro_name)\n            LOG.debug(\"Using distro class %s\", distro_cls)\n            self._distro = distro_cls(distro_name, system_config, self.paths)\n            # If we have an active datasource we need to adjust\n            # said datasource and move its distro/system config\n            # from whatever it was to a new set...\n            if self.datasource is not None:\n                self.datasource.distro = self._distro\n                self.datasource.sys_cfg = self.cfg\n        return self._distro\n\n    @property\n    def cfg(self):\n        return self._extract_cfg(\"restricted\")\n\n    def _extract_cfg(self, restriction):\n        # Ensure actually read\n        self.read_cfg()\n        # Nobody gets the real config\n        ocfg = copy.deepcopy(self._cfg)\n        if restriction == \"restricted\":\n            ocfg.pop(\"system_info\", None)\n        elif restriction == \"system\":\n            ocfg = util.get_cfg_by_path(ocfg, (\"system_info\",), {})\n        elif restriction == \"paths\":\n            ocfg = util.get_cfg_by_path(ocfg, (\"system_info\", \"paths\"), {})\n        if not isinstance(ocfg, (dict)):\n            ocfg = {}\n        return ocfg\n\n    @property\n    def paths(self):\n        if not self._paths:\n            path_info = self._extract_cfg(\"paths\")\n            self._paths = helpers.Paths(path_info, self.datasource)\n        return self._paths\n\n    def _initial_subdirs(self):\n        c_dir = self.paths.cloud_dir\n        run_dir = self.paths.run_dir\n        initial_dirs = [\n            c_dir,\n            os.path.join(c_dir, \"scripts\"),\n            os.path.join(c_dir, \"scripts\", \"per-instance\"),\n            os.path.join(c_dir, \"scripts\", \"per-once\"),\n            os.path.join(c_dir, \"scripts\", \"per-boot\"),\n            os.path.join(c_dir, \"scripts\", \"vendor\"),\n            os.path.join(c_dir, \"seed\"),\n            os.path.join(c_dir, \"instances\"),\n            os.path.join(c_dir, \"handlers\"),\n            os.path.join(c_dir, \"sem\"),\n            os.path.join(c_dir, \"data\"),\n            os.path.join(run_dir, \"sem\"),\n        ]\n        return initial_dirs\n\n    def purge_cache(self, rm_instance_lnk=False):\n        rm_list = [self.paths.boot_finished]\n        if rm_instance_lnk:\n            rm_list.append(self.paths.instance_link)\n        for f in rm_list:\n            util.del_file(f)\n        return len(rm_list)\n\n    def initialize(self):\n        self._initialize_filesystem()\n\n    def _initialize_filesystem(self):\n        util.ensure_dirs(self._initial_subdirs())\n        log_file = util.get_cfg_option_str(self.cfg, \"def_log_file\")\n        if log_file:\n            # At this point the log file should have already been created\n            # in the setupLogging function of log.py\n            util.ensure_file(log_file, mode=0o640, preserve_mode=False)\n            perms = self.cfg.get(\"syslog_fix_perms\")\n            if not perms:\n                perms = {}\n            if not isinstance(perms, list):\n                perms = [perms]\n\n            error = None\n            for perm in perms:\n                u, g = util.extract_usergroup(perm)\n                try:\n                    util.chownbyname(log_file, u, g)\n                    return\n                except OSError as e:\n                    error = e\n\n            LOG.warning(\n                \"Failed changing perms on '%s'. tried: %s. %s\",\n                log_file,\n                \",\".join(perms),\n                error,\n            )\n\n    def read_cfg(self, extra_fns=None):\n        # None check so that we don't keep on re-loading if empty\n        if self._cfg is None:\n            self._cfg = self._read_cfg(extra_fns)\n            # LOG.debug(\"Loaded 'init' config %s\", self._cfg)\n\n    def _read_cfg(self, extra_fns):\n        no_cfg_paths = helpers.Paths({}, self.datasource)\n        instance_data_file = no_cfg_paths.get_runpath(\n            \"instance_data_sensitive\"\n        )\n        merger = helpers.ConfigMerger(\n            paths=no_cfg_paths,\n            datasource=self.datasource,\n            additional_fns=extra_fns,\n            base_cfg=fetch_base_config(instance_data_file=instance_data_file),\n        )\n        return merger.cfg\n\n    def _restore_from_cache(self):\n        # We try to restore from a current link and static path\n        # by using the instance link, if purge_cache was called\n        # the file wont exist.\n        return sources.pkl_load(self.paths.get_ipath_cur(\"obj_pkl\"))\n\n    def _write_to_cache(self):\n        if self.datasource is None:\n            return False\n        if util.get_cfg_option_bool(self.cfg, \"manual_cache_clean\", False):\n            # The empty file in instance/ dir indicates manual cleaning,\n            # and can be read by ds-identify.\n            util.write_file(\n                self.paths.get_ipath_cur(\"manual_clean_marker\"),\n                omode=\"w\",\n                content=\"\",\n            )\n        return sources.pkl_store(\n            self.datasource, self.paths.get_ipath_cur(\"obj_pkl\")\n        )\n\n    def _get_datasources(self):\n        # Any config provided???\n        pkg_list = self.cfg.get(\"datasource_pkg_list\") or []\n        # Add the defaults at the end\n        for n in [\"\", type_utils.obj_name(sources)]:\n            if n not in pkg_list:\n                pkg_list.append(n)\n        cfg_list = self.cfg.get(\"datasource_list\") or []\n        return (cfg_list, pkg_list)\n\n    def _restore_from_checked_cache(self, existing):\n        if existing not in (\"check\", \"trust\"):\n            raise ValueError(\"Unexpected value for existing: %s\" % existing)\n\n        ds = self._restore_from_cache()\n        if not ds:\n            return (None, \"no cache found\")\n\n        run_iid_fn = self.paths.get_runpath(\"instance_id\")\n        if os.path.exists(run_iid_fn):\n            run_iid = util.load_file(run_iid_fn).strip()\n        else:\n            run_iid = None\n\n        if run_iid == ds.get_instance_id():\n            return (ds, \"restored from cache with run check: %s\" % ds)\n        elif existing == \"trust\":\n            return (ds, \"restored from cache: %s\" % ds)\n        else:\n            if hasattr(ds, \"check_instance_id\") and ds.check_instance_id(\n                self.cfg\n            ):\n                return (ds, \"restored from checked cache: %s\" % ds)\n            else:\n                return (None, \"cache invalid in datasource: %s\" % ds)\n\n    def _get_data_source(self, existing) -> sources.DataSource:\n        if self.datasource is not None:\n            return self.datasource\n\n        with events.ReportEventStack(\n            name=\"check-cache\",\n            description=\"attempting to read from cache [%s]\" % existing,\n            parent=self.reporter,\n        ) as myrep:\n\n            ds, desc = self._restore_from_checked_cache(existing)\n            myrep.description = desc\n            self.ds_restored = bool(ds)\n            LOG.debug(myrep.description)\n\n        if not ds:\n            util.del_file(self.paths.instance_link)\n            (cfg_list, pkg_list) = self._get_datasources()\n            # Deep copy so that user-data handlers can not modify\n            # (which will affect user-data handlers down the line...)\n            (ds, dsname) = sources.find_source(\n                self.cfg,\n                self.distro,\n                self.paths,\n                copy.deepcopy(self.ds_deps),\n                cfg_list,\n                pkg_list,\n                self.reporter,\n            )\n            LOG.info(\"Loaded datasource %s - %s\", dsname, ds)\n        self.datasource = ds\n        # Ensure we adjust our path members datasource\n        # now that we have one (thus allowing ipath to be used)\n        self._reset()\n        return ds\n\n    def _get_instance_subdirs(self):\n        return [\"handlers\", \"scripts\", \"sem\"]\n\n    def _get_ipath(self, subname=None):\n        # Force a check to see if anything\n        # actually comes back, if not\n        # then a datasource has not been assigned...\n        instance_dir = self.paths.get_ipath(subname)\n        if not instance_dir:\n            raise RuntimeError(\n                \"No instance directory is available.\"\n                \" Has a datasource been fetched??\"\n            )\n        return instance_dir\n\n    def _reflect_cur_instance(self):\n        # Remove the old symlink and attach a new one so\n        # that further reads/writes connect into the right location\n        idir = self._get_ipath()\n        util.del_file(self.paths.instance_link)\n        util.sym_link(idir, self.paths.instance_link)\n\n        # Ensures these dirs exist\n        dir_list = []\n        for d in self._get_instance_subdirs():\n            dir_list.append(os.path.join(idir, d))\n        util.ensure_dirs(dir_list)\n\n        # Write out information on what is being used for the current instance\n        # and what may have been used for a previous instance...\n        dp = self.paths.get_cpath(\"data\")\n\n        # Write what the datasource was and is..\n        ds = \"%s: %s\" % (type_utils.obj_name(self.datasource), self.datasource)\n        previous_ds = None\n        ds_fn = os.path.join(idir, \"datasource\")\n        try:\n            previous_ds = util.load_file(ds_fn).strip()\n        except Exception:\n            pass\n        if not previous_ds:\n            previous_ds = ds\n        util.write_file(ds_fn, \"%s\\n\" % ds)\n        util.write_file(\n            os.path.join(dp, \"previous-datasource\"), \"%s\\n\" % (previous_ds)\n        )\n\n        # What the instance id was and is...\n        iid = self.datasource.get_instance_id()\n        iid_fn = os.path.join(dp, \"instance-id\")\n\n        previous_iid = self.previous_iid()\n        util.write_file(iid_fn, \"%s\\n\" % iid)\n        util.write_file(self.paths.get_runpath(\"instance_id\"), \"%s\\n\" % iid)\n        util.write_file(\n            os.path.join(dp, \"previous-instance-id\"), \"%s\\n\" % (previous_iid)\n        )\n\n        self._write_to_cache()\n        # Ensure needed components are regenerated\n        # after change of instance which may cause\n        # change of configuration\n        self._reset()\n        return iid\n\n    def previous_iid(self):\n        if self._previous_iid is not None:\n            return self._previous_iid\n\n        dp = self.paths.get_cpath(\"data\")\n        iid_fn = os.path.join(dp, \"instance-id\")\n        try:\n            self._previous_iid = util.load_file(iid_fn).strip()\n        except Exception:\n            self._previous_iid = NO_PREVIOUS_INSTANCE_ID\n\n        LOG.debug(\"previous iid found to be %s\", self._previous_iid)\n        return self._previous_iid\n\n    def is_new_instance(self):\n        \"\"\"Return true if this is a new instance.\n\n        If datasource has already been initialized, this will return False,\n        even on first boot.\n        \"\"\"\n        previous = self.previous_iid()\n        ret = (\n            previous == NO_PREVIOUS_INSTANCE_ID\n            or previous != self.datasource.get_instance_id()\n        )\n        return ret\n\n    def fetch(self, existing=\"check\"):\n        return self._get_data_source(existing=existing)\n\n    def instancify(self):\n        return self._reflect_cur_instance()\n\n    def cloudify(self):\n        # Form the needed options to cloudify our members\n        return cloud.Cloud(\n            self.datasource,\n            self.paths,\n            self.cfg,\n            self.distro,\n            helpers.Runners(self.paths),\n            reporter=self.reporter,\n        )\n\n    def update(self):\n        self._store_rawdata(self.datasource.get_userdata_raw(), \"userdata\")\n        self._store_processeddata(self.datasource.get_userdata(), \"userdata\")\n        self._store_raw_vendordata(\n            self.datasource.get_vendordata_raw(), \"vendordata\"\n        )\n        self._store_processeddata(\n            self.datasource.get_vendordata(), \"vendordata\"\n        )\n        self._store_raw_vendordata(\n            self.datasource.get_vendordata2_raw(), \"vendordata2\"\n        )\n        self._store_processeddata(\n            self.datasource.get_vendordata2(), \"vendordata2\"\n        )\n\n    def setup_datasource(self):\n        with events.ReportEventStack(\n            \"setup-datasource\", \"setting up datasource\", parent=self.reporter\n        ):\n            if self.datasource is None:\n                raise RuntimeError(\"Datasource is None, cannot setup.\")\n            self.datasource.setup(is_new_instance=self.is_new_instance())\n\n    def activate_datasource(self):\n        with events.ReportEventStack(\n            \"activate-datasource\",\n            \"activating datasource\",\n            parent=self.reporter,\n        ):\n            if self.datasource is None:\n                raise RuntimeError(\"Datasource is None, cannot activate.\")\n            self.datasource.activate(\n                cfg=self.cfg, is_new_instance=self.is_new_instance()\n            )\n            self._write_to_cache()\n\n    def _store_rawdata(self, data, datasource):\n        # Raw data is bytes, not a string\n        if data is None:\n            data = b\"\"\n        util.write_file(self._get_ipath(\"%s_raw\" % datasource), data, 0o600)\n\n    def _store_raw_vendordata(self, data, datasource):\n        # Only these data types\n        if data is not None and type(data) not in [bytes, str, list]:\n            raise TypeError(\n                \"vendordata_raw is unsupported type '%s'\" % str(type(data))\n            )\n        # This data may be a list, convert it to a string if so\n        if isinstance(data, list):\n            data = util.json_dumps(data)\n        self._store_rawdata(data, datasource)\n\n    def _store_processeddata(self, processed_data, datasource):\n        # processed is a Mime message, so write as string.\n        if processed_data is None:\n            processed_data = \"\"\n        util.write_file(\n            self._get_ipath(datasource), str(processed_data), 0o600\n        )\n\n    def _default_handlers(self, opts=None) -> List[handlers.Handler]:\n        if opts is None:\n            opts = {}\n\n        opts.update(\n            {\n                \"paths\": self.paths,\n                \"datasource\": self.datasource,\n            }\n        )\n        # TODO(harlowja) Hmmm, should we dynamically import these??\n        cloudconfig_handler = CloudConfigPartHandler(**opts)\n        shellscript_handler = ShellScriptPartHandler(**opts)\n        def_handlers = [\n            cloudconfig_handler,\n            shellscript_handler,\n            ShellScriptByFreqPartHandler(PER_ALWAYS, **opts),\n            ShellScriptByFreqPartHandler(PER_INSTANCE, **opts),\n            ShellScriptByFreqPartHandler(PER_ONCE, **opts),\n            BootHookPartHandler(**opts),\n            JinjaTemplatePartHandler(\n                **opts, sub_handlers=[cloudconfig_handler, shellscript_handler]\n            ),\n        ]\n        return def_handlers\n\n    def _default_vendordata_handlers(self):\n        return self._default_handlers(\n            opts={\n                \"script_path\": \"vendor_scripts\",\n                \"cloud_config_path\": \"vendor_cloud_config\",\n            }\n        )\n\n    def _default_vendordata2_handlers(self):\n        return self._default_handlers(\n            opts={\n                \"script_path\": \"vendor_scripts\",\n                \"cloud_config_path\": \"vendor2_cloud_config\",\n            }\n        )\n\n    def _do_handlers(\n        self, data_msg, c_handlers_list, frequency, excluded=None\n    ):\n        \"\"\"\n        Generalized handlers suitable for use with either vendordata\n        or userdata\n        \"\"\"\n        if excluded is None:\n            excluded = []\n\n        cdir = self.paths.get_cpath(\"handlers\")\n        idir = self._get_ipath(\"handlers\")\n\n        # Add the path to the plugins dir to the top of our list for importing\n        # new handlers.\n        #\n        # Note(harlowja): instance dir should be read before cloud-dir\n        for d in [cdir, idir]:\n            if d and d not in sys.path:\n                sys.path.insert(0, d)\n\n        def register_handlers_in_dir(path):\n            # Attempts to register any handler modules under the given path.\n            if not path or not os.path.isdir(path):\n                return\n            potential_handlers = util.get_modules_from_dir(path)\n            for (fname, mod_name) in potential_handlers.items():\n                try:\n                    mod_locs, looked_locs = importer.find_module(\n                        mod_name, [\"\"], [\"list_types\", \"handle_part\"]\n                    )\n                    if not mod_locs:\n                        LOG.warning(\n                            \"Could not find a valid user-data handler\"\n                            \" named %s in file %s (searched %s)\",\n                            mod_name,\n                            fname,\n                            looked_locs,\n                        )\n                        continue\n                    mod = importer.import_module(mod_locs[0])\n                    mod = handlers.fixup_handler(mod)\n                    types = c_handlers.register(mod)\n                    if types:\n                        LOG.debug(\n                            \"Added custom handler for %s [%s] from %s\",\n                            types,\n                            mod,\n                            fname,\n                        )\n                except Exception:\n                    util.logexc(\n                        LOG, \"Failed to register handler from %s\", fname\n                    )\n\n        # This keeps track of all the active handlers\n        c_handlers = helpers.ContentHandlers()\n\n        # Add any handlers in the cloud-dir\n        register_handlers_in_dir(cdir)\n\n        # Register any other handlers that come from the default set. This\n        # is done after the cloud-dir handlers so that the cdir modules can\n        # take over the default user-data handler content-types.\n        for mod in c_handlers_list:\n            types = c_handlers.register(mod, overwrite=False)\n            if types:\n                LOG.debug(\"Added default handler for %s from %s\", types, mod)\n\n        # Form our cloud interface\n        data = self.cloudify()\n\n        def init_handlers():\n            # Init the handlers first\n            for (_ctype, mod) in c_handlers.items():\n                if mod in c_handlers.initialized:\n                    # Avoid initiating the same module twice (if said module\n                    # is registered to more than one content-type).\n                    continue\n                handlers.call_begin(mod, data, frequency)\n                c_handlers.initialized.append(mod)\n\n        def walk_handlers(excluded):\n            # Walk the user data\n            part_data = {\n                \"handlers\": c_handlers,\n                # Any new handlers that are encountered get writen here\n                \"handlerdir\": idir,\n                \"data\": data,\n                # The default frequency if handlers don't have one\n                \"frequency\": frequency,\n                # This will be used when new handlers are found\n                # to help write their contents to files with numbered\n                # names...\n                \"handlercount\": 0,\n                \"excluded\": excluded,\n            }\n            handlers.walk(data_msg, handlers.walker_callback, data=part_data)\n\n        def finalize_handlers():\n            # Give callbacks opportunity to finalize\n            for (_ctype, mod) in c_handlers.items():\n                if mod not in c_handlers.initialized:\n                    # Said module was never inited in the first place, so lets\n                    # not attempt to finalize those that never got called.\n                    continue\n                c_handlers.initialized.remove(mod)\n                try:\n                    handlers.call_end(mod, data, frequency)\n                except Exception:\n                    util.logexc(LOG, \"Failed to finalize handler: %s\", mod)\n\n        try:\n            init_handlers()\n            walk_handlers(excluded)\n        finally:\n            finalize_handlers()\n\n    def consume_data(self, frequency=PER_INSTANCE):\n        # Consume the userdata first, because we need want to let the part\n        # handlers run first (for merging stuff)\n        with events.ReportEventStack(\n            \"consume-user-data\",\n            \"reading and applying user-data\",\n            parent=self.reporter,\n        ):\n            if util.get_cfg_option_bool(self.cfg, \"allow_userdata\", True):\n                self._consume_userdata(frequency)\n            else:\n                LOG.debug(\"allow_userdata = False: discarding user-data\")\n\n        with events.ReportEventStack(\n            \"consume-vendor-data\",\n            \"reading and applying vendor-data\",\n            parent=self.reporter,\n        ):\n            self._consume_vendordata(\"vendordata\", frequency)\n\n        with events.ReportEventStack(\n            \"consume-vendor-data2\",\n            \"reading and applying vendor-data2\",\n            parent=self.reporter,\n        ):\n            self._consume_vendordata(\"vendordata2\", frequency)\n\n        # Perform post-consumption adjustments so that\n        # modules that run during the init stage reflect\n        # this consumed set.\n        #\n        # They will be recreated on future access...\n        self._reset()\n        # Note(harlowja): the 'active' datasource will have\n        # references to the previous config, distro, paths\n        # objects before the load of the userdata happened,\n        # this is expected.\n\n    def _consume_vendordata(self, vendor_source, frequency=PER_INSTANCE):\n        \"\"\"\n        Consume the vendordata and run the part handlers on it\n        \"\"\"\n\n        # User-data should have been consumed first.\n        # So we merge the other available cloud-configs (everything except\n        # vendor provided), and check whether or not we should consume\n        # vendor data at all. That gives user or system a chance to override.\n        if vendor_source == \"vendordata\":\n            if not self.datasource.get_vendordata_raw():\n                LOG.debug(\"no vendordata from datasource\")\n                return\n            cfg_name = \"vendor_data\"\n        elif vendor_source == \"vendordata2\":\n            if not self.datasource.get_vendordata2_raw():\n                LOG.debug(\"no vendordata2 from datasource\")\n                return\n            cfg_name = \"vendor_data2\"\n        else:\n            raise RuntimeError(\n                \"vendor_source arg must be either 'vendordata'\"\n                \" or 'vendordata2'\"\n            )\n\n        _cc_merger = helpers.ConfigMerger(\n            paths=self._paths,\n            datasource=self.datasource,\n            additional_fns=[],\n            base_cfg=self.cfg,\n            include_vendor=False,\n        )\n        vdcfg = _cc_merger.cfg.get(cfg_name, {})\n\n        if not isinstance(vdcfg, dict):\n            vdcfg = {\"enabled\": False}\n            LOG.warning(\n                \"invalid %s setting. resetting to: %s\", cfg_name, vdcfg\n            )\n\n        enabled = vdcfg.get(\"enabled\")\n        no_handlers = vdcfg.get(\"disabled_handlers\", None)\n\n        if not util.is_true(enabled):\n            LOG.debug(\"%s consumption is disabled.\", vendor_source)\n            return\n\n        if isinstance(enabled, str):\n            util.deprecate(\n                deprecated=f\"Use of string '{enabled}' for \"\n                \"'vendor_data:enabled' field\",\n                deprecated_version=\"23.1\",\n                extra_message=\"Use boolean value instead.\",\n            )\n\n        LOG.debug(\n            \"%s will be consumed. disabled_handlers=%s\",\n            vendor_source,\n            no_handlers,\n        )\n\n        # Ensure vendordata source fetched before activation (just in case.)\n\n        # c_handlers_list keeps track of all the active handlers, while\n        # excluding what the users doesn't want run, i.e. boot_hook,\n        # cloud_config, shell_script\n        if vendor_source == \"vendordata\":\n            vendor_data_msg = self.datasource.get_vendordata()\n            c_handlers_list = self._default_vendordata_handlers()\n        else:\n            vendor_data_msg = self.datasource.get_vendordata2()\n            c_handlers_list = self._default_vendordata2_handlers()\n\n        # Run the handlers\n        self._do_handlers(\n            vendor_data_msg, c_handlers_list, frequency, excluded=no_handlers\n        )\n\n    def _consume_userdata(self, frequency=PER_INSTANCE):\n        \"\"\"\n        Consume the userdata and run the part handlers\n        \"\"\"\n\n        # Ensure datasource fetched before activation (just incase)\n        user_data_msg = self.datasource.get_userdata(True)\n\n        # This keeps track of all the active handlers\n        c_handlers_list = self._default_handlers()\n\n        # Run the handlers\n        self._do_handlers(user_data_msg, c_handlers_list, frequency)\n\n    def _get_network_key_contents(self, cfg) -> dict:\n        \"\"\"\n        Network configuration can be passed as a dict under a \"network\" key, or\n        optionally at the top level. In both cases, return the config.\n        \"\"\"\n        if cfg and \"network\" in cfg:\n            return cfg[\"network\"]\n        return cfg\n\n    def _find_networking_config(self):\n        disable_file = os.path.join(\n            self.paths.get_cpath(\"data\"), \"upgraded-network\"\n        )\n        if os.path.exists(disable_file):\n            return (None, disable_file)\n\n        available_cfgs = {\n            NetworkConfigSource.CMD_LINE: cmdline.read_kernel_cmdline_config(),\n            NetworkConfigSource.INITRAMFS: cmdline.read_initramfs_config(),\n            NetworkConfigSource.DS: None,\n            NetworkConfigSource.SYSTEM_CFG: self.cfg.get(\"network\"),\n        }\n\n        if self.datasource and hasattr(self.datasource, \"network_config\"):\n            available_cfgs[\n                NetworkConfigSource.DS\n            ] = self.datasource.network_config\n\n        if self.datasource:\n            order = self.datasource.network_config_sources\n        else:\n            order = sources.DataSource.network_config_sources\n        for cfg_source in order:\n            if not isinstance(cfg_source, NetworkConfigSource):\n                LOG.warning(\n                    \"data source specifies an invalid network cfg_source: %s\",\n                    cfg_source,\n                )\n                continue\n            if cfg_source not in available_cfgs:\n                LOG.warning(\n                    \"data source specifies an unavailable network\"\n                    \" cfg_source: %s\",\n                    cfg_source,\n                )\n                continue\n            ncfg = self._get_network_key_contents(available_cfgs[cfg_source])\n            if net.is_disabled_cfg(ncfg):\n                LOG.debug(\"network config disabled by %s\", cfg_source)\n                return (None, cfg_source)\n            if ncfg:\n                return (ncfg, cfg_source)\n        if not self.cfg.get(\"network\", True):\n            LOG.warning(\"Empty network config found\")\n        return (\n            self.distro.generate_fallback_config(),\n            NetworkConfigSource.FALLBACK,\n        )\n\n    def _apply_netcfg_names(self, netcfg):\n        try:\n            LOG.debug(\"applying net config names for %s\", netcfg)\n            self.distro.networking.apply_network_config_names(netcfg)\n        except Exception as e:\n            LOG.warning(\"Failed to rename devices: %s\", e)\n\n    def _get_per_boot_network_semaphore(self):\n        return namedtuple(\"Semaphore\", \"semaphore args\")(\n            helpers.FileSemaphores(self.paths.get_runpath(\"sem\")),\n            (\"apply_network_config\", PER_ONCE),\n        )\n\n    def _network_already_configured(self) -> bool:\n        sem = self._get_per_boot_network_semaphore()\n        return sem.semaphore.has_run(*sem.args)\n\n    def apply_network_config(self, bring_up):\n        \"\"\"Apply the network config.\n\n        Find the config, determine whether to apply it, apply it via\n        the distro, and optionally bring it up\n        \"\"\"\n        netcfg, src = self._find_networking_config()\n        if netcfg is None:\n            LOG.info(\"network config is disabled by %s\", src)\n            return\n\n        def event_enabled_and_metadata_updated(event_type):\n            return update_event_enabled(\n                datasource=self.datasource,\n                cfg=self.cfg,\n                event_source_type=event_type,\n                scope=EventScope.NETWORK,\n            ) and self.datasource.update_metadata_if_supported([event_type])\n\n        def should_run_on_boot_event():\n            return (\n                not self._network_already_configured()\n                and event_enabled_and_metadata_updated(EventType.BOOT)\n            )\n\n        if (\n            self.datasource is not None\n            and not self.is_new_instance()\n            and not should_run_on_boot_event()\n            and not event_enabled_and_metadata_updated(EventType.BOOT_LEGACY)\n        ):\n            LOG.debug(\n                \"No network config applied. Neither a new instance\"\n                \" nor datasource network update allowed\"\n            )\n            # nothing new, but ensure proper names\n            self._apply_netcfg_names(netcfg)\n            return\n\n        # refresh netcfg after update\n        netcfg, src = self._find_networking_config()\n\n        # ensure all physical devices in config are present\n        self.distro.networking.wait_for_physdevs(netcfg)\n\n        # apply renames from config\n        self._apply_netcfg_names(netcfg)\n\n        # rendering config\n        LOG.info(\n            \"Applying network configuration from %s bringup=%s: %s\",\n            src,\n            bring_up,\n            netcfg,\n        )\n\n        sem = self._get_per_boot_network_semaphore()\n        try:\n            with sem.semaphore.lock(*sem.args):\n                return self.distro.apply_network_config(\n                    netcfg, bring_up=bring_up\n                )\n        except net.RendererNotFoundError as e:\n            LOG.error(\n                \"Unable to render networking. Network config is \"\n                \"likely broken: %s\",\n                e,\n            )\n            return\n        except NotImplementedError:\n            LOG.warning(\n                \"distro '%s' does not implement apply_network_config. \"\n                \"networking may not be configured properly.\",\n                self.distro,\n            )\n            return\n\n\ndef read_runtime_config():\n    return util.read_conf(RUN_CLOUD_CONFIG)\n\n\ndef fetch_base_config(*, instance_data_file=None) -> dict:\n    return util.mergemanydict(\n        [\n            # builtin config, hardcoded in settings.py.\n            util.get_builtin_cfg(),\n            # Anything in your conf.d or 'default' cloud.cfg location.\n            util.read_conf_with_confd(\n                CLOUD_CONFIG, instance_data_file=instance_data_file\n            ),\n            # runtime config. I.e., /run/cloud-init/cloud.cfg\n            read_runtime_config(),\n            # Kernel/cmdline parameters override system config\n            util.read_conf_from_cmdline(),\n        ],\n        reverse=True,\n    )\n", "# This file is part of cloud-init. See LICENSE file for license information.\n\nimport copy\nimport inspect\nimport os\nimport stat\n\nfrom cloudinit import importer, util\nfrom cloudinit.event import EventScope, EventType\nfrom cloudinit.helpers import Paths\nfrom cloudinit.sources import (\n    EXPERIMENTAL_TEXT,\n    METADATA_UNKNOWN,\n    REDACT_SENSITIVE_VALUE,\n    UNSET,\n    DataSource,\n    canonical_cloud_id,\n    pkl_load,\n    redact_sensitive_keys,\n)\nfrom cloudinit.user_data import UserDataProcessor\nfrom tests.unittests.helpers import CiTestCase, mock\n\n\nclass DataSourceTestSubclassNet(DataSource):\n\n    dsname = \"MyTestSubclass\"\n    url_max_wait = 55\n\n    def __init__(\n        self,\n        sys_cfg,\n        distro,\n        paths,\n        custom_metadata=None,\n        custom_userdata=None,\n        get_data_retval=True,\n    ):\n        super(DataSourceTestSubclassNet, self).__init__(sys_cfg, distro, paths)\n        self._custom_userdata = custom_userdata\n        self._custom_metadata = custom_metadata\n        self._get_data_retval = get_data_retval\n\n    def _get_cloud_name(self):\n        return \"SubclassCloudName\"\n\n    def _get_data(self):\n        if self._custom_metadata:\n            self.metadata = self._custom_metadata\n        else:\n            self.metadata = {\n                \"availability_zone\": \"myaz\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"region\": \"myregion\",\n            }\n        if self._custom_userdata:\n            self.userdata_raw = self._custom_userdata\n        else:\n            self.userdata_raw = \"userdata_raw\"\n        self.vendordata_raw = \"vendordata_raw\"\n        return self._get_data_retval\n\n\nclass InvalidDataSourceTestSubclassNet(DataSource):\n    pass\n\n\nclass TestDataSource(CiTestCase):\n\n    with_logs = True\n    maxDiff = None\n\n    def setUp(self):\n        super(TestDataSource, self).setUp()\n        self.sys_cfg = {\"datasource\": {\"_undef\": {\"key1\": False}}}\n        self.distro = \"distrotest\"  # generally should be a Distro object\n        self.paths = Paths({})\n        self.datasource = DataSource(self.sys_cfg, self.distro, self.paths)\n\n    def test_datasource_init(self):\n        \"\"\"DataSource initializes metadata attributes, ds_cfg and ud_proc.\"\"\"\n        self.assertEqual(self.paths, self.datasource.paths)\n        self.assertEqual(self.sys_cfg, self.datasource.sys_cfg)\n        self.assertEqual(self.distro, self.datasource.distro)\n        self.assertIsNone(self.datasource.userdata)\n        self.assertEqual({}, self.datasource.metadata)\n        self.assertIsNone(self.datasource.userdata_raw)\n        self.assertIsNone(self.datasource.vendordata)\n        self.assertIsNone(self.datasource.vendordata_raw)\n        self.assertEqual({\"key1\": False}, self.datasource.ds_cfg)\n        self.assertIsInstance(self.datasource.ud_proc, UserDataProcessor)\n\n    def test_datasource_init_gets_ds_cfg_using_dsname(self):\n        \"\"\"Init uses DataSource.dsname for sourcing ds_cfg.\"\"\"\n        sys_cfg = {\"datasource\": {\"MyTestSubclass\": {\"key2\": False}}}\n        distro = \"distrotest\"  # generally should be a Distro object\n        datasource = DataSourceTestSubclassNet(sys_cfg, distro, self.paths)\n        self.assertEqual({\"key2\": False}, datasource.ds_cfg)\n\n    def test_str_is_classname(self):\n        \"\"\"The string representation of the datasource is the classname.\"\"\"\n        self.assertEqual(\"DataSource\", str(self.datasource))\n        self.assertEqual(\n            \"DataSourceTestSubclassNet\",\n            str(DataSourceTestSubclassNet(\"\", \"\", self.paths)),\n        )\n\n    def test_datasource_get_url_params_defaults(self):\n        \"\"\"get_url_params default url config settings for the datasource.\"\"\"\n        params = self.datasource.get_url_params()\n        self.assertEqual(params.max_wait_seconds, self.datasource.url_max_wait)\n        self.assertEqual(params.timeout_seconds, self.datasource.url_timeout)\n        self.assertEqual(params.num_retries, self.datasource.url_retries)\n        self.assertEqual(\n            params.sec_between_retries, self.datasource.url_sec_between_retries\n        )\n\n    def test_datasource_get_url_params_subclassed(self):\n        \"\"\"Subclasses can override get_url_params defaults.\"\"\"\n        sys_cfg = {\"datasource\": {\"MyTestSubclass\": {\"key2\": False}}}\n        distro = \"distrotest\"  # generally should be a Distro object\n        datasource = DataSourceTestSubclassNet(sys_cfg, distro, self.paths)\n        expected = (\n            datasource.url_max_wait,\n            datasource.url_timeout,\n            datasource.url_retries,\n            datasource.url_sec_between_retries,\n        )\n        url_params = datasource.get_url_params()\n        self.assertNotEqual(self.datasource.get_url_params(), url_params)\n        self.assertEqual(expected, url_params)\n\n    def test_datasource_get_url_params_ds_config_override(self):\n        \"\"\"Datasource configuration options can override url param defaults.\"\"\"\n        sys_cfg = {\n            \"datasource\": {\n                \"MyTestSubclass\": {\n                    \"max_wait\": \"1\",\n                    \"timeout\": \"2\",\n                    \"retries\": \"3\",\n                    \"sec_between_retries\": 4,\n                }\n            }\n        }\n        datasource = DataSourceTestSubclassNet(\n            sys_cfg, self.distro, self.paths\n        )\n        expected = (1, 2, 3, 4)\n        url_params = datasource.get_url_params()\n        self.assertNotEqual(\n            (\n                datasource.url_max_wait,\n                datasource.url_timeout,\n                datasource.url_retries,\n                datasource.url_sec_between_retries,\n            ),\n            url_params,\n        )\n        self.assertEqual(expected, url_params)\n\n    def test_datasource_get_url_params_is_zero_or_greater(self):\n        \"\"\"get_url_params ignores timeouts with a value below 0.\"\"\"\n        # Set an override that is below 0 which gets ignored.\n        sys_cfg = {\"datasource\": {\"_undef\": {\"timeout\": \"-1\"}}}\n        datasource = DataSource(sys_cfg, self.distro, self.paths)\n        (\n            _max_wait,\n            timeout,\n            _retries,\n            _sec_between_retries,\n        ) = datasource.get_url_params()\n        self.assertEqual(0, timeout)\n\n    def test_datasource_get_url_uses_defaults_on_errors(self):\n        \"\"\"On invalid system config values for url_params defaults are used.\"\"\"\n        # All invalid values should be logged\n        sys_cfg = {\n            \"datasource\": {\n                \"_undef\": {\n                    \"max_wait\": \"nope\",\n                    \"timeout\": \"bug\",\n                    \"retries\": \"nonint\",\n                }\n            }\n        }\n        datasource = DataSource(sys_cfg, self.distro, self.paths)\n        url_params = datasource.get_url_params()\n        expected = (\n            datasource.url_max_wait,\n            datasource.url_timeout,\n            datasource.url_retries,\n            datasource.url_sec_between_retries,\n        )\n        self.assertEqual(expected, url_params)\n        logs = self.logs.getvalue()\n        expected_logs = [\n            \"Config max_wait 'nope' is not an int, using default '-1'\",\n            \"Config timeout 'bug' is not an int, using default '10'\",\n            \"Config retries 'nonint' is not an int, using default '5'\",\n        ]\n        for log in expected_logs:\n            self.assertIn(log, logs)\n\n    @mock.patch(\"cloudinit.sources.net.find_fallback_nic\")\n    def test_fallback_interface_is_discovered(self, m_get_fallback_nic):\n        \"\"\"The fallback_interface is discovered via find_fallback_nic.\"\"\"\n        m_get_fallback_nic.return_value = \"nic9\"\n        self.assertEqual(\"nic9\", self.datasource.fallback_interface)\n\n    @mock.patch(\"cloudinit.sources.net.find_fallback_nic\")\n    def test_fallback_interface_logs_undiscovered(self, m_get_fallback_nic):\n        \"\"\"Log a warning when fallback_interface can not discover the nic.\"\"\"\n        self.datasource._cloud_name = \"MySupahCloud\"\n        m_get_fallback_nic.return_value = None  # Couldn't discover nic\n        self.assertIsNone(self.datasource.fallback_interface)\n        self.assertEqual(\n            \"WARNING: Did not find a fallback interface on MySupahCloud.\\n\",\n            self.logs.getvalue(),\n        )\n\n    @mock.patch(\"cloudinit.sources.net.find_fallback_nic\")\n    def test_wb_fallback_interface_is_cached(self, m_get_fallback_nic):\n        \"\"\"The fallback_interface is cached and won't be rediscovered.\"\"\"\n        self.datasource._fallback_interface = \"nic10\"\n        self.assertEqual(\"nic10\", self.datasource.fallback_interface)\n        m_get_fallback_nic.assert_not_called()\n\n    def test__get_data_unimplemented(self):\n        \"\"\"Raise an error when _get_data is not implemented.\"\"\"\n        with self.assertRaises(NotImplementedError) as context_manager:\n            self.datasource.get_data()\n        self.assertIn(\n            \"Subclasses of DataSource must implement _get_data\",\n            str(context_manager.exception),\n        )\n        datasource2 = InvalidDataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, self.paths\n        )\n        with self.assertRaises(NotImplementedError) as context_manager:\n            datasource2.get_data()\n        self.assertIn(\n            \"Subclasses of DataSource must implement _get_data\",\n            str(context_manager.exception),\n        )\n\n    def test_get_data_calls_subclass__get_data(self):\n        \"\"\"Datasource.get_data uses the subclass' version of _get_data.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertTrue(datasource.get_data())\n        self.assertEqual(\n            {\n                \"availability_zone\": \"myaz\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"region\": \"myregion\",\n            },\n            datasource.metadata,\n        )\n        self.assertEqual(\"userdata_raw\", datasource.userdata_raw)\n        self.assertEqual(\"vendordata_raw\", datasource.vendordata_raw)\n\n    def test_get_hostname_strips_local_hostname_without_domain(self):\n        \"\"\"Datasource.get_hostname strips metadata local-hostname of domain.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertTrue(datasource.get_data())\n        self.assertEqual(\n            \"test-subclass-hostname\", datasource.metadata[\"local-hostname\"]\n        )\n        self.assertEqual(\n            \"test-subclass-hostname\", datasource.get_hostname().hostname\n        )\n        datasource.metadata[\"local-hostname\"] = \"hostname.my.domain.com\"\n        self.assertEqual(\"hostname\", datasource.get_hostname().hostname)\n\n    def test_get_hostname_with_fqdn_returns_local_hostname_with_domain(self):\n        \"\"\"Datasource.get_hostname with fqdn set gets qualified hostname.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertTrue(datasource.get_data())\n        datasource.metadata[\"local-hostname\"] = \"hostname.my.domain.com\"\n        self.assertEqual(\n            \"hostname.my.domain.com\",\n            datasource.get_hostname(fqdn=True).hostname,\n        )\n\n    def test_get_hostname_without_metadata_uses_system_hostname(self):\n        \"\"\"Datasource.gethostname runs util.get_hostname when no metadata.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertEqual({}, datasource.metadata)\n        mock_fqdn = \"cloudinit.sources.util.get_fqdn_from_hosts\"\n        with mock.patch(\"cloudinit.sources.util.get_hostname\") as m_gethost:\n            with mock.patch(mock_fqdn) as m_fqdn:\n                m_gethost.return_value = \"systemhostname.domain.com\"\n                m_fqdn.return_value = None  # No maching fqdn in /etc/hosts\n                self.assertEqual(\n                    \"systemhostname\", datasource.get_hostname().hostname\n                )\n                self.assertEqual(\n                    \"systemhostname.domain.com\",\n                    datasource.get_hostname(fqdn=True).hostname,\n                )\n\n    def test_get_hostname_without_metadata_returns_none(self):\n        \"\"\"Datasource.gethostname returns None when metadata_only and no MD.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertEqual({}, datasource.metadata)\n        mock_fqdn = \"cloudinit.sources.util.get_fqdn_from_hosts\"\n        with mock.patch(\"cloudinit.sources.util.get_hostname\") as m_gethost:\n            with mock.patch(mock_fqdn) as m_fqdn:\n                self.assertIsNone(\n                    datasource.get_hostname(metadata_only=True).hostname\n                )\n                self.assertIsNone(\n                    datasource.get_hostname(\n                        fqdn=True, metadata_only=True\n                    ).hostname\n                )\n        self.assertEqual([], m_gethost.call_args_list)\n        self.assertEqual([], m_fqdn.call_args_list)\n\n    def test_get_hostname_without_metadata_prefers_etc_hosts(self):\n        \"\"\"Datasource.gethostname prefers /etc/hosts to util.get_hostname.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        self.assertEqual({}, datasource.metadata)\n        mock_fqdn = \"cloudinit.sources.util.get_fqdn_from_hosts\"\n        with mock.patch(\"cloudinit.sources.util.get_hostname\") as m_gethost:\n            with mock.patch(mock_fqdn) as m_fqdn:\n                m_gethost.return_value = \"systemhostname.domain.com\"\n                m_fqdn.return_value = \"fqdnhostname.domain.com\"\n                self.assertEqual(\n                    \"fqdnhostname\", datasource.get_hostname().hostname\n                )\n                self.assertEqual(\n                    \"fqdnhostname.domain.com\",\n                    datasource.get_hostname(fqdn=True).hostname,\n                )\n\n    def test_get_data_does_not_write_instance_data_on_failure(self):\n        \"\"\"get_data does not write INSTANCE_JSON_FILE on get_data False.\"\"\"\n        tmp = self.tmp_dir()\n        paths = Paths({\"run_dir\": tmp})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n            get_data_retval=False,\n        )\n        self.assertFalse(datasource.get_data())\n        json_file = paths.get_runpath(\"instance_data\")\n        self.assertFalse(\n            os.path.exists(json_file), f\"Found unexpected file {json_file}\"\n        )\n\n    def test_get_data_writes_json_instance_data_on_success(self):\n        \"\"\"get_data writes INSTANCE_JSON_FILE to run_dir as world readable.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg, self.distro, Paths({\"run_dir\": tmp})\n        )\n        sys_info = {\n            \"python\": \"3.7\",\n            \"platform\": (\n                \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n            ),\n            \"uname\": [\n                \"Linux\",\n                \"myhost\",\n                \"5.4.0-24-generic\",\n                \"SMP blah\",\n                \"x86_64\",\n            ],\n            \"variant\": \"ubuntu\",\n            \"dist\": [\"ubuntu\", \"20.04\", \"focal\"],\n        }\n        with mock.patch(\"cloudinit.util.system_info\", return_value=sys_info):\n            with mock.patch(\n                \"cloudinit.sources.canonical_cloud_id\",\n                return_value=\"canonical_cloud_id\",\n            ):\n                datasource.get_data()\n        json_file = Paths({\"run_dir\": tmp}).get_runpath(\"instance_data\")\n        content = util.load_file(json_file)\n        expected = {\n            \"base64_encoded_keys\": [],\n            \"merged_cfg\": REDACT_SENSITIVE_VALUE,\n            \"sensitive_keys\": [\"merged_cfg\"],\n            \"sys_info\": sys_info,\n            \"v1\": {\n                \"_beta_keys\": [\"subplatform\"],\n                \"availability-zone\": \"myaz\",\n                \"availability_zone\": \"myaz\",\n                \"cloud_id\": \"canonical_cloud_id\",\n                \"cloud-name\": \"subclasscloudname\",\n                \"cloud_name\": \"subclasscloudname\",\n                \"distro\": \"ubuntu\",\n                \"distro_release\": \"focal\",\n                \"distro_version\": \"20.04\",\n                \"instance-id\": \"iid-datasource\",\n                \"instance_id\": \"iid-datasource\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"local_hostname\": \"test-subclass-hostname\",\n                \"kernel_release\": \"5.4.0-24-generic\",\n                \"machine\": \"x86_64\",\n                \"platform\": \"mytestsubclass\",\n                \"public_ssh_keys\": [],\n                \"python_version\": \"3.7\",\n                \"region\": \"myregion\",\n                \"system_platform\": (\n                    \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n                ),\n                \"subplatform\": \"unknown\",\n                \"variant\": \"ubuntu\",\n            },\n            \"ds\": {\n                \"_doc\": EXPERIMENTAL_TEXT,\n                \"meta_data\": {\n                    \"availability_zone\": \"myaz\",\n                    \"local-hostname\": \"test-subclass-hostname\",\n                    \"region\": \"myregion\",\n                },\n            },\n        }\n        self.assertEqual(expected, util.load_json(content))\n        file_stat = os.stat(json_file)\n        self.assertEqual(0o644, stat.S_IMODE(file_stat.st_mode))\n        self.assertEqual(expected, util.load_json(content))\n\n    def test_get_data_writes_redacted_public_json_instance_data(self):\n        \"\"\"get_data writes redacted content to public INSTANCE_JSON_FILE.\"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            Paths({\"run_dir\": tmp}),\n            custom_metadata={\n                \"availability_zone\": \"myaz\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"region\": \"myregion\",\n                \"some\": {\n                    \"security-credentials\": {\n                        \"cred1\": \"sekret\",\n                        \"cred2\": \"othersekret\",\n                    }\n                },\n                \"someother\": {\n                    \"nested\": {\n                        \"userData\": \"HIDE ME\",\n                    }\n                },\n                \"VENDOR-DAta\": \"HIDE ME TOO\",\n            },\n        )\n        self.assertCountEqual(\n            (\n                \"merged_cfg\",\n                \"security-credentials\",\n                \"userdata\",\n                \"user-data\",\n                \"user_data\",\n                \"vendordata\",\n                \"vendor-data\",\n                \"ds/vendor_data\",\n            ),\n            datasource.sensitive_metadata_keys,\n        )\n        sys_info = {\n            \"python\": \"3.7\",\n            \"platform\": (\n                \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n            ),\n            \"uname\": [\n                \"Linux\",\n                \"myhost\",\n                \"5.4.0-24-generic\",\n                \"SMP blah\",\n                \"x86_64\",\n            ],\n            \"variant\": \"ubuntu\",\n            \"dist\": [\"ubuntu\", \"20.04\", \"focal\"],\n        }\n        with mock.patch(\"cloudinit.util.system_info\", return_value=sys_info):\n            datasource.get_data()\n        json_file = Paths({\"run_dir\": tmp}).get_runpath(\"instance_data\")\n        redacted = util.load_json(util.load_file(json_file))\n        expected = {\n            \"base64_encoded_keys\": [],\n            \"merged_cfg\": REDACT_SENSITIVE_VALUE,\n            \"sensitive_keys\": [\n                \"ds/meta_data/VENDOR-DAta\",\n                \"ds/meta_data/some/security-credentials\",\n                \"ds/meta_data/someother/nested/userData\",\n                \"merged_cfg\",\n            ],\n            \"sys_info\": sys_info,\n            \"v1\": {\n                \"_beta_keys\": [\"subplatform\"],\n                \"availability-zone\": \"myaz\",\n                \"availability_zone\": \"myaz\",\n                \"cloud-name\": \"subclasscloudname\",\n                \"cloud_name\": \"subclasscloudname\",\n                \"cloud_id\": \"subclasscloudname\",\n                \"distro\": \"ubuntu\",\n                \"distro_release\": \"focal\",\n                \"distro_version\": \"20.04\",\n                \"instance-id\": \"iid-datasource\",\n                \"instance_id\": \"iid-datasource\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"local_hostname\": \"test-subclass-hostname\",\n                \"kernel_release\": \"5.4.0-24-generic\",\n                \"machine\": \"x86_64\",\n                \"platform\": \"mytestsubclass\",\n                \"public_ssh_keys\": [],\n                \"python_version\": \"3.7\",\n                \"region\": \"myregion\",\n                \"system_platform\": (\n                    \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n                ),\n                \"subplatform\": \"unknown\",\n                \"variant\": \"ubuntu\",\n            },\n            \"ds\": {\n                \"_doc\": EXPERIMENTAL_TEXT,\n                \"meta_data\": {\n                    \"VENDOR-DAta\": REDACT_SENSITIVE_VALUE,\n                    \"availability_zone\": \"myaz\",\n                    \"local-hostname\": \"test-subclass-hostname\",\n                    \"region\": \"myregion\",\n                    \"some\": {\"security-credentials\": REDACT_SENSITIVE_VALUE},\n                    \"someother\": {\n                        \"nested\": {\"userData\": REDACT_SENSITIVE_VALUE}\n                    },\n                },\n            },\n        }\n        self.assertEqual(expected, redacted)\n        file_stat = os.stat(json_file)\n        self.assertEqual(0o644, stat.S_IMODE(file_stat.st_mode))\n\n    def test_get_data_writes_json_instance_data_sensitive(self):\n        \"\"\"\n        get_data writes unmodified data to sensitive file as root-readonly.\n        \"\"\"\n        tmp = self.tmp_dir()\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            Paths({\"run_dir\": tmp}),\n            custom_metadata={\n                \"availability_zone\": \"myaz\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"region\": \"myregion\",\n                \"some\": {\n                    \"security-credentials\": {\n                        \"cred1\": \"sekret\",\n                        \"cred2\": \"othersekret\",\n                    }\n                },\n            },\n        )\n        sys_info = {\n            \"python\": \"3.7\",\n            \"platform\": (\n                \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n            ),\n            \"uname\": [\n                \"Linux\",\n                \"myhost\",\n                \"5.4.0-24-generic\",\n                \"SMP blah\",\n                \"x86_64\",\n            ],\n            \"variant\": \"ubuntu\",\n            \"dist\": [\"ubuntu\", \"20.04\", \"focal\"],\n        }\n\n        self.assertCountEqual(\n            (\n                \"merged_cfg\",\n                \"security-credentials\",\n                \"userdata\",\n                \"user-data\",\n                \"user_data\",\n                \"vendordata\",\n                \"vendor-data\",\n                \"ds/vendor_data\",\n            ),\n            datasource.sensitive_metadata_keys,\n        )\n        with mock.patch(\"cloudinit.util.system_info\", return_value=sys_info):\n            with mock.patch(\n                \"cloudinit.sources.canonical_cloud_id\",\n                return_value=\"canonical-cloud-id\",\n            ):\n                datasource.get_data()\n        sensitive_json_file = Paths({\"run_dir\": tmp}).get_runpath(\n            \"instance_data_sensitive\"\n        )\n        content = util.load_file(sensitive_json_file)\n        expected = {\n            \"base64_encoded_keys\": [],\n            \"merged_cfg\": {\n                \"_doc\": (\n                    \"Merged cloud-init system config from \"\n                    \"/etc/cloud/cloud.cfg and /etc/cloud/cloud.cfg.d/\"\n                ),\n                \"datasource\": {\"_undef\": {\"key1\": False}},\n            },\n            \"sensitive_keys\": [\n                \"ds/meta_data/some/security-credentials\",\n                \"merged_cfg\",\n            ],\n            \"sys_info\": sys_info,\n            \"v1\": {\n                \"_beta_keys\": [\"subplatform\"],\n                \"availability-zone\": \"myaz\",\n                \"availability_zone\": \"myaz\",\n                \"cloud_id\": \"canonical-cloud-id\",\n                \"cloud-name\": \"subclasscloudname\",\n                \"cloud_name\": \"subclasscloudname\",\n                \"distro\": \"ubuntu\",\n                \"distro_release\": \"focal\",\n                \"distro_version\": \"20.04\",\n                \"instance-id\": \"iid-datasource\",\n                \"instance_id\": \"iid-datasource\",\n                \"kernel_release\": \"5.4.0-24-generic\",\n                \"local-hostname\": \"test-subclass-hostname\",\n                \"local_hostname\": \"test-subclass-hostname\",\n                \"machine\": \"x86_64\",\n                \"platform\": \"mytestsubclass\",\n                \"public_ssh_keys\": [],\n                \"python_version\": \"3.7\",\n                \"region\": \"myregion\",\n                \"subplatform\": \"unknown\",\n                \"system_platform\": (\n                    \"Linux-5.4.0-24-generic-x86_64-with-Ubuntu-20.04-focal\"\n                ),\n                \"variant\": \"ubuntu\",\n            },\n            \"ds\": {\n                \"_doc\": EXPERIMENTAL_TEXT,\n                \"meta_data\": {\n                    \"availability_zone\": \"myaz\",\n                    \"local-hostname\": \"test-subclass-hostname\",\n                    \"region\": \"myregion\",\n                    \"some\": {\n                        \"security-credentials\": {\n                            \"cred1\": \"sekret\",\n                            \"cred2\": \"othersekret\",\n                        }\n                    },\n                },\n            },\n        }\n        self.assertCountEqual(expected, util.load_json(content))\n        file_stat = os.stat(sensitive_json_file)\n        self.assertEqual(0o600, stat.S_IMODE(file_stat.st_mode))\n        self.assertEqual(expected, util.load_json(content))\n\n    def test_get_data_handles_redacted_unserializable_content(self):\n        \"\"\"get_data warns unserializable content in INSTANCE_JSON_FILE.\"\"\"\n        tmp = self.tmp_dir()\n        paths = Paths({\"run_dir\": tmp})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n            custom_metadata={\"key1\": \"val1\", \"key2\": {\"key2.1\": self.paths}},\n        )\n        datasource.get_data()\n        json_file = paths.get_runpath(\"instance_data\")\n        content = util.load_file(json_file)\n        expected_metadata = {\n            \"key1\": \"val1\",\n            \"key2\": {\n                \"key2.1\": (\n                    \"Warning: redacted unserializable type <class\"\n                    \" 'cloudinit.helpers.Paths'>\"\n                )\n            },\n        }\n        instance_json = util.load_json(content)\n        self.assertEqual(expected_metadata, instance_json[\"ds\"][\"meta_data\"])\n\n    def test_persist_instance_data_writes_ec2_metadata_when_set(self):\n        \"\"\"When ec2_metadata class attribute is set, persist to json.\"\"\"\n        tmp = self.tmp_dir()\n        cloud_dir = os.path.join(tmp, \"cloud\")\n        util.ensure_dir(cloud_dir)\n        paths = Paths({\"run_dir\": tmp, \"cloud_dir\": cloud_dir})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n        )\n        datasource.ec2_metadata = UNSET\n        datasource.get_data()\n        json_file = paths.get_runpath(\"instance_data\")\n        instance_data = util.load_json(util.load_file(json_file))\n        self.assertNotIn(\"ec2_metadata\", instance_data[\"ds\"])\n        datasource.ec2_metadata = {\"ec2stuff\": \"is good\"}\n        datasource.persist_instance_data()\n        instance_data = util.load_json(util.load_file(json_file))\n        self.assertEqual(\n            {\"ec2stuff\": \"is good\"}, instance_data[\"ds\"][\"ec2_metadata\"]\n        )\n\n    def test_persist_instance_data_writes_canonical_cloud_id_and_symlink(self):\n        \"\"\"canonical-cloud-id class attribute is set, persist to json.\"\"\"\n        tmp = self.tmp_dir()\n        cloud_dir = os.path.join(tmp, \"cloud\")\n        util.ensure_dir(cloud_dir)\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            Paths({\"run_dir\": tmp, \"cloud_dir\": cloud_dir}),\n        )\n        cloud_id_link = os.path.join(tmp, \"cloud-id\")\n        cloud_id_file = os.path.join(tmp, \"cloud-id-my-cloud\")\n        cloud_id2_file = os.path.join(tmp, \"cloud-id-my-cloud2\")\n        for filename in (cloud_id_file, cloud_id_link, cloud_id2_file):\n            self.assertFalse(\n                os.path.exists(filename), \"Unexpected link found {filename}\"\n            )\n        with mock.patch(\n            \"cloudinit.sources.canonical_cloud_id\", return_value=\"my-cloud\"\n        ):\n            datasource.get_data()\n            self.assertEqual(\"my-cloud\\n\", util.load_file(cloud_id_link))\n            # A symlink with the generic /run/cloud-init/cloud-id\n            # link is present\n            self.assertTrue(util.is_link(cloud_id_link))\n            datasource.persist_instance_data()\n            # cloud-id<cloud-type> not deleted: no cloud-id change\n            self.assertTrue(os.path.exists(cloud_id_file))\n        # When cloud-id changes, symlink and content change\n        with mock.patch(\n            \"cloudinit.sources.canonical_cloud_id\", return_value=\"my-cloud2\"\n        ):\n            datasource.persist_instance_data()\n        self.assertEqual(\"my-cloud2\\n\", util.load_file(cloud_id2_file))\n        # Previous cloud-id-<cloud-type> file removed\n        self.assertFalse(os.path.exists(cloud_id_file))\n        # Generic link persisted which contains canonical-cloud-id as content\n        self.assertTrue(util.is_link(cloud_id_link))\n        self.assertEqual(\"my-cloud2\\n\", util.load_file(cloud_id_link))\n\n    def test_persist_instance_data_writes_network_json_when_set(self):\n        \"\"\"When network_data.json class attribute is set, persist to json.\"\"\"\n        tmp = self.tmp_dir()\n        cloud_dir = os.path.join(tmp, \"cloud\")\n        util.ensure_dir(cloud_dir)\n        paths = Paths({\"run_dir\": tmp, \"cloud_dir\": cloud_dir})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n        )\n        datasource.get_data()\n        json_file = paths.get_runpath(\"instance_data\")\n        instance_data = util.load_json(util.load_file(json_file))\n        self.assertNotIn(\"network_json\", instance_data[\"ds\"])\n        datasource.network_json = {\"network_json\": \"is good\"}\n        datasource.persist_instance_data()\n        instance_data = util.load_json(util.load_file(json_file))\n        self.assertEqual(\n            {\"network_json\": \"is good\"}, instance_data[\"ds\"][\"network_json\"]\n        )\n\n    def test_persist_instance_serializes_datasource_pickle(self):\n        \"\"\"obj.pkl is written when instance link present and write_cache.\"\"\"\n        tmp = self.tmp_dir()\n        cloud_dir = os.path.join(tmp, \"cloud\")\n        util.ensure_dir(cloud_dir)\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            Paths({\"run_dir\": tmp, \"cloud_dir\": cloud_dir}),\n        )\n        pkl_cache_file = os.path.join(cloud_dir, \"instance/obj.pkl\")\n        self.assertFalse(os.path.exists(pkl_cache_file))\n        datasource.network_json = {\"network_json\": \"is good\"}\n        # No /var/lib/cloud/instance symlink\n        datasource.persist_instance_data(write_cache=True)\n        self.assertFalse(os.path.exists(pkl_cache_file))\n\n        # Symlink /var/lib/cloud/instance but write_cache=False\n        util.sym_link(cloud_dir, os.path.join(cloud_dir, \"instance\"))\n        datasource.persist_instance_data(write_cache=False)\n        self.assertFalse(os.path.exists(pkl_cache_file))\n\n        # Symlink /var/lib/cloud/instance and write_cache=True\n        datasource.persist_instance_data(write_cache=True)\n        self.assertTrue(os.path.exists(pkl_cache_file))\n        ds = pkl_load(pkl_cache_file)\n        self.assertEqual(datasource.network_json, ds.network_json)\n\n    def test_get_data_base64encodes_unserializable_bytes(self):\n        \"\"\"On py3, get_data base64encodes any unserializable content.\"\"\"\n        tmp = self.tmp_dir()\n        paths = Paths({\"run_dir\": tmp})\n        datasource = DataSourceTestSubclassNet(\n            self.sys_cfg,\n            self.distro,\n            paths,\n            custom_metadata={\"key1\": \"val1\", \"key2\": {\"key2.1\": b\"\\x123\"}},\n        )\n        self.assertTrue(datasource.get_data())\n        json_file = paths.get_runpath(\"instance_data\")\n        content = util.load_file(json_file)\n        instance_json = util.load_json(content)\n        self.assertCountEqual(\n            [\"ds/meta_data/key2/key2.1\"], instance_json[\"base64_encoded_keys\"]\n        )\n        self.assertEqual(\n            {\"key1\": \"val1\", \"key2\": {\"key2.1\": \"EjM=\"}},\n            instance_json[\"ds\"][\"meta_data\"],\n        )\n\n    def test_get_hostname_subclass_support(self):\n        \"\"\"Validate get_hostname signature on all subclasses of DataSource.\"\"\"\n        base_args = inspect.getfullargspec(DataSource.get_hostname)\n        # Import all DataSource subclasses so we can inspect them.\n        modules = util.get_modules_from_dir(\n            os.path.dirname(os.path.dirname(__file__))\n        )\n        for _loc, name in modules.items():\n            mod_locs, _ = importer.find_module(name, [\"cloudinit.sources\"], [])\n            if mod_locs:\n                importer.import_module(mod_locs[0])\n        for child in DataSource.__subclasses__():\n            if \"Test\" in child.dsname:\n                continue\n            self.assertEqual(\n                base_args,\n                inspect.getfullargspec(child.get_hostname),\n                \"%s does not implement DataSource.get_hostname params\" % child,\n            )\n            for grandchild in child.__subclasses__():\n                self.assertEqual(\n                    base_args,\n                    inspect.getfullargspec(grandchild.get_hostname),\n                    \"%s does not implement DataSource.get_hostname params\"\n                    % grandchild,\n                )\n\n    def test_clear_cached_attrs_resets_cached_attr_class_attributes(self):\n        \"\"\"Class attributes listed in cached_attr_defaults are reset.\"\"\"\n        count = 0\n        # Setup values for all cached class attributes\n        for attr, value in self.datasource.cached_attr_defaults:\n            setattr(self.datasource, attr, count)\n            count += 1\n        self.datasource._dirty_cache = True\n        self.datasource.clear_cached_attrs()\n        for attr, value in self.datasource.cached_attr_defaults:\n            self.assertEqual(value, getattr(self.datasource, attr))\n\n    def test_clear_cached_attrs_noops_on_clean_cache(self):\n        \"\"\"Class attributes listed in cached_attr_defaults are reset.\"\"\"\n        count = 0\n        # Setup values for all cached class attributes\n        for attr, _ in self.datasource.cached_attr_defaults:\n            setattr(self.datasource, attr, count)\n            count += 1\n        self.datasource._dirty_cache = False  # Fake clean cache\n        self.datasource.clear_cached_attrs()\n        count = 0\n        for attr, _ in self.datasource.cached_attr_defaults:\n            self.assertEqual(count, getattr(self.datasource, attr))\n            count += 1\n\n    def test_clear_cached_attrs_skips_non_attr_class_attributes(self):\n        \"\"\"Skip any cached_attr_defaults which aren't class attributes.\"\"\"\n        self.datasource._dirty_cache = True\n        self.datasource.clear_cached_attrs()\n        for attr in (\"ec2_metadata\", \"network_json\"):\n            self.assertFalse(hasattr(self.datasource, attr))\n\n    def test_clear_cached_attrs_of_custom_attrs(self):\n        \"\"\"Custom attr_values can be passed to clear_cached_attrs.\"\"\"\n        self.datasource._dirty_cache = True\n        cached_attr_name = self.datasource.cached_attr_defaults[0][0]\n        setattr(self.datasource, cached_attr_name, \"himom\")\n        self.datasource.myattr = \"orig\"\n        self.datasource.clear_cached_attrs(\n            attr_defaults=((\"myattr\", \"updated\"),)\n        )\n        self.assertEqual(\"himom\", getattr(self.datasource, cached_attr_name))\n        self.assertEqual(\"updated\", self.datasource.myattr)\n\n    @mock.patch.dict(\n        DataSource.default_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    @mock.patch.dict(\n        DataSource.supported_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_update_metadata_only_acts_on_supported_update_events(self):\n        \"\"\"update_metadata_if_supported wont get_data on unsupported events.\"\"\"\n        self.assertEqual(\n            {EventScope.NETWORK: set([EventType.BOOT_NEW_INSTANCE])},\n            self.datasource.default_update_events,\n        )\n\n        fake_get_data = mock.Mock()\n        self.datasource.get_data = fake_get_data\n        self.assertFalse(\n            self.datasource.update_metadata_if_supported(\n                source_event_types=[EventType.BOOT]\n            )\n        )\n        self.assertEqual([], fake_get_data.call_args_list)\n\n    @mock.patch.dict(\n        DataSource.supported_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_update_metadata_returns_true_on_supported_update_event(self):\n        \"\"\"update_metadata_if_supported returns get_data on supported events\"\"\"\n\n        def fake_get_data():\n            return True\n\n        self.datasource.get_data = fake_get_data\n        self.datasource._network_config = \"something\"\n        self.datasource._dirty_cache = True\n        self.assertTrue(\n            self.datasource.update_metadata_if_supported(\n                source_event_types=[\n                    EventType.BOOT,\n                    EventType.BOOT_NEW_INSTANCE,\n                ]\n            )\n        )\n        self.assertEqual(UNSET, self.datasource._network_config)\n\n        self.assertIn(\n            \"DEBUG: Update datasource metadata and network config due to\"\n            \" events: boot-new-instance\",\n            self.logs.getvalue(),\n        )\n\n\nclass TestRedactSensitiveData(CiTestCase):\n    def test_redact_sensitive_data_noop_when_no_sensitive_keys_present(self):\n        \"\"\"When sensitive_keys is absent or empty from metadata do nothing.\"\"\"\n        md = {\"my\": \"data\"}\n        self.assertEqual(\n            md, redact_sensitive_keys(md, redact_value=\"redacted\")\n        )\n        md[\"sensitive_keys\"] = []\n        self.assertEqual(\n            md, redact_sensitive_keys(md, redact_value=\"redacted\")\n        )\n\n    def test_redact_sensitive_data_redacts_exact_match_name(self):\n        \"\"\"Only exact matched sensitive_keys are redacted from metadata.\"\"\"\n        md = {\n            \"sensitive_keys\": [\"md/secure\"],\n            \"md\": {\"secure\": \"s3kr1t\", \"insecure\": \"publik\"},\n        }\n        secure_md = copy.deepcopy(md)\n        secure_md[\"md\"][\"secure\"] = \"redacted\"\n        self.assertEqual(\n            secure_md, redact_sensitive_keys(md, redact_value=\"redacted\")\n        )\n\n    def test_redact_sensitive_data_does_redacts_with_default_string(self):\n        \"\"\"When redact_value is absent, REDACT_SENSITIVE_VALUE is used.\"\"\"\n        md = {\n            \"sensitive_keys\": [\"md/secure\"],\n            \"md\": {\"secure\": \"s3kr1t\", \"insecure\": \"publik\"},\n        }\n        secure_md = copy.deepcopy(md)\n        secure_md[\"md\"][\"secure\"] = \"redacted for non-root user\"\n        self.assertEqual(secure_md, redact_sensitive_keys(md))\n\n\nclass TestCanonicalCloudID(CiTestCase):\n    def test_cloud_id_returns_platform_on_unknowns(self):\n        \"\"\"When region and cloud_name are unknown, return platform.\"\"\"\n        self.assertEqual(\n            \"platform\",\n            canonical_cloud_id(\n                cloud_name=METADATA_UNKNOWN,\n                region=METADATA_UNKNOWN,\n                platform=\"platform\",\n            ),\n        )\n\n    def test_cloud_id_returns_platform_on_none(self):\n        \"\"\"When region and cloud_name are unknown, return platform.\"\"\"\n        self.assertEqual(\n            \"platform\",\n            canonical_cloud_id(\n                cloud_name=None, region=None, platform=\"platform\"\n            ),\n        )\n\n    def test_cloud_id_returns_cloud_name_on_unknown_region(self):\n        \"\"\"When region is unknown, return cloud_name.\"\"\"\n        for region in (None, METADATA_UNKNOWN):\n            self.assertEqual(\n                \"cloudname\",\n                canonical_cloud_id(\n                    cloud_name=\"cloudname\", region=region, platform=\"platform\"\n                ),\n            )\n\n    def test_cloud_id_returns_platform_on_unknown_cloud_name(self):\n        \"\"\"When region is set but cloud_name is unknown return cloud_name.\"\"\"\n        self.assertEqual(\n            \"platform\",\n            canonical_cloud_id(\n                cloud_name=METADATA_UNKNOWN,\n                region=\"region\",\n                platform=\"platform\",\n            ),\n        )\n\n    def test_cloud_id_aws_based_on_region_and_cloud_name(self):\n        \"\"\"When cloud_name is aws, return proper cloud-id based on region.\"\"\"\n        self.assertEqual(\n            \"aws-china\",\n            canonical_cloud_id(\n                cloud_name=\"aws\", region=\"cn-north-1\", platform=\"platform\"\n            ),\n        )\n        self.assertEqual(\n            \"aws\",\n            canonical_cloud_id(\n                cloud_name=\"aws\", region=\"us-east-1\", platform=\"platform\"\n            ),\n        )\n        self.assertEqual(\n            \"aws-gov\",\n            canonical_cloud_id(\n                cloud_name=\"aws\", region=\"us-gov-1\", platform=\"platform\"\n            ),\n        )\n        self.assertEqual(  # Overrideen non-aws cloud_name is returned\n            \"!aws\",\n            canonical_cloud_id(\n                cloud_name=\"!aws\", region=\"us-gov-1\", platform=\"platform\"\n            ),\n        )\n\n    def test_cloud_id_azure_based_on_region_and_cloud_name(self):\n        \"\"\"Report cloud-id when cloud_name is azure and region is in china.\"\"\"\n        self.assertEqual(\n            \"azure-china\",\n            canonical_cloud_id(\n                cloud_name=\"azure\", region=\"chinaeast\", platform=\"platform\"\n            ),\n        )\n        self.assertEqual(\n            \"azure\",\n            canonical_cloud_id(\n                cloud_name=\"azure\", region=\"!chinaeast\", platform=\"platform\"\n            ),\n        )\n\n\n# vi: ts=4 expandtab\n", "# This file is part of cloud-init. See LICENSE file for license information.\n\n\"\"\"Tests related to cloudinit.stages module.\"\"\"\nimport os\nimport stat\n\nimport pytest\n\nfrom cloudinit import sources, stages\nfrom cloudinit.event import EventScope, EventType\nfrom cloudinit.sources import NetworkConfigSource\nfrom cloudinit.util import write_file\nfrom tests.unittests.helpers import mock\nfrom tests.unittests.util import TEST_INSTANCE_ID, FakeDataSource\n\nM_PATH = \"cloudinit.stages.\"\n\n\nclass TestInit:\n    @pytest.fixture(autouse=True)\n    def setup(self, tmpdir):\n        self.tmpdir = tmpdir\n        self.init = stages.Init()\n        self.init._cfg = {\n            \"system_info\": {\n                \"distro\": \"ubuntu\",\n                \"paths\": {\"cloud_dir\": self.tmpdir, \"run_dir\": self.tmpdir},\n            }\n        }\n        self.init.datasource = FakeDataSource(paths=self.init.paths)\n        self._real_is_new_instance = self.init.is_new_instance\n        self.init.is_new_instance = mock.Mock(return_value=True)\n\n    def test_wb__find_networking_config_disabled(self):\n        \"\"\"find_networking_config returns no config when disabled.\"\"\"\n        disable_file = os.path.join(\n            self.init.paths.get_cpath(\"data\"), \"upgraded-network\"\n        )\n        write_file(disable_file, \"\")\n        assert (None, disable_file) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"net_config\",\n        [\n            {\"config\": \"disabled\"},\n            {\"network\": {\"config\": \"disabled\"}},\n        ],\n    )\n    def test_wb__find_networking_config_disabled_by_kernel(\n        self, m_cmdline, m_initramfs, net_config, caplog\n    ):\n        \"\"\"find_networking_config returns when disabled by kernel cmdline.\"\"\"\n        m_cmdline.return_value = net_config\n        m_initramfs.return_value = {\"config\": [\"fake_initrd\"]}\n        assert (\n            None,\n            NetworkConfigSource.CMD_LINE,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert \"network config disabled by cmdline\" in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"net_config\",\n        [\n            {\"config\": \"disabled\"},\n            {\"network\": {\"config\": \"disabled\"}},\n        ],\n    )\n    def test_wb__find_networking_config_disabled_by_initrd(\n        self, m_cmdline, m_initramfs, net_config, caplog\n    ):\n        \"\"\"find_networking_config returns when disabled by kernel cmdline.\"\"\"\n        m_cmdline.return_value = {}\n        m_initramfs.return_value = net_config\n        assert (\n            None,\n            NetworkConfigSource.INITRAMFS,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert \"network config disabled by initramfs\" in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"net_config\",\n        [\n            {\"config\": \"disabled\"},\n            {\"network\": {\"config\": \"disabled\"}},\n        ],\n    )\n    def test_wb__find_networking_config_disabled_by_datasrc(\n        self, m_cmdline, m_initramfs, net_config, caplog\n    ):\n        \"\"\"find_networking_config returns when disabled by datasource cfg.\"\"\"\n        m_cmdline.return_value = {}  # Kernel doesn't disable networking\n        m_initramfs.return_value = {}  # initramfs doesn't disable networking\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": {},\n        }  # system config doesn't disable\n\n        self.init.datasource = FakeDataSource(network_config=net_config)\n        assert (\n            None,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert \"network config disabled by ds\" in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"net_config\",\n        [\n            {\"config\": \"disabled\"},\n            {\"network\": {\"config\": \"disabled\"}},\n        ],\n    )\n    def test_wb__find_networking_config_disabled_by_sysconfig(\n        self, m_cmdline, m_initramfs, net_config, caplog\n    ):\n        \"\"\"find_networking_config returns when disabled by system config.\"\"\"\n        m_cmdline.return_value = {}  # Kernel doesn't disable networking\n        m_initramfs.return_value = {}  # initramfs doesn't disable networking\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": net_config,\n        }\n        assert (\n            None,\n            NetworkConfigSource.SYSTEM_CFG,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert \"network config disabled by system_cfg\" in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test__find_networking_config_uses_datasrc_order(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config should check sources in DS defined order\"\"\"\n        # cmdline and initramfs, which would normally be preferred over other\n        # sources, disable networking; in this case, though, the DS moves them\n        # later so its own config is preferred\n        m_cmdline.return_value = {\"config\": \"disabled\"}\n        m_initramfs.return_value = {\"config\": \"disabled\"}\n\n        self.init.datasource = FakeDataSource(network_config=in_config)\n        self.init.datasource.network_config_sources = [\n            NetworkConfigSource.DS,\n            NetworkConfigSource.SYSTEM_CFG,\n            NetworkConfigSource.CMD_LINE,\n            NetworkConfigSource.INITRAMFS,\n        ]\n\n        assert (\n            out_config,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test__find_networking_config_warns_if_datasrc_uses_invalid_src(\n        self, m_cmdline, m_initramfs, in_config, out_config, caplog\n    ):\n        \"\"\"find_networking_config should check sources in DS defined order\"\"\"\n        self.init.datasource = FakeDataSource(network_config=in_config)\n        self.init.datasource.network_config_sources = [\n            \"invalid_src\",\n            NetworkConfigSource.DS,\n        ]\n\n        assert (\n            out_config,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"WARNING\"\n        assert (\n            \"data source specifies an invalid network cfg_source: invalid_src\"\n            in caplog.text\n        )\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test__find_networking_config_warns_if_datasrc_uses_unavailable_src(\n        self, m_cmdline, m_initramfs, in_config, out_config, caplog\n    ):\n        \"\"\"find_networking_config should check sources in DS defined order\"\"\"\n        self.init.datasource = FakeDataSource(network_config=in_config)\n        self.init.datasource.network_config_sources = [\n            NetworkConfigSource.FALLBACK,\n            NetworkConfigSource.DS,\n        ]\n\n        assert (\n            out_config,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n        assert caplog.records[0].levelname == \"WARNING\"\n        assert (\n            \"data source specifies an unavailable network cfg_source: fallback\"\n            in caplog.text\n        )\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test_wb__find_networking_config_returns_kernel(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config returns kernel cmdline config if present.\"\"\"\n        m_cmdline.return_value = in_config\n        m_initramfs.return_value = {\"config\": [\"fake_initrd\"]}\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": {\"config\": [\"fakesys_config\"]},\n        }\n        self.init.datasource = FakeDataSource(\n            network_config={\"config\": [\"fakedatasource\"]}\n        )\n        assert (\n            out_config,\n            NetworkConfigSource.CMD_LINE,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test_wb__find_networking_config_returns_initramfs(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config returns initramfs config if present.\"\"\"\n        m_cmdline.return_value = {}\n        m_initramfs.return_value = in_config\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": {\"config\": [\"fakesys_config\"]},\n        }\n        self.init.datasource = FakeDataSource(\n            network_config={\"config\": [\"fakedatasource\"]}\n        )\n        assert (\n            out_config,\n            NetworkConfigSource.INITRAMFS,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test_wb__find_networking_config_returns_system_cfg(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config returns system config when present.\"\"\"\n        m_cmdline.return_value = {}  # No kernel network config\n        m_initramfs.return_value = {}  # no initramfs network config\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": in_config,\n        }\n        self.init.datasource = FakeDataSource(\n            network_config={\"config\": [\"fakedatasource\"]}\n        )\n        assert (\n            out_config,\n            NetworkConfigSource.SYSTEM_CFG,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    @pytest.mark.parametrize(\n        \"in_config,out_config\",\n        [\n            ({\"config\": {\"a\": True}}, {\"config\": {\"a\": True}}),\n            ({\"network\": {\"config\": {\"a\": True}}}, {\"config\": {\"a\": True}}),\n        ],\n    )\n    def test_wb__find_networking_config_returns_datasrc_cfg(\n        self, m_cmdline, m_initramfs, in_config, out_config\n    ):\n        \"\"\"find_networking_config returns datasource net config if present.\"\"\"\n        m_cmdline.return_value = {}  # No kernel network config\n        m_initramfs.return_value = {}  # no initramfs network config\n        self.init.datasource = FakeDataSource(network_config=in_config)\n        assert (\n            out_config,\n            NetworkConfigSource.DS,\n        ) == self.init._find_networking_config()\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\")\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\")\n    def test_wb__find_networking_config_returns_fallback(\n        self, m_cmdline, m_initramfs, caplog\n    ):\n        \"\"\"find_networking_config returns fallback config if not defined.\"\"\"\n        m_cmdline.return_value = {}  # Kernel doesn't disable networking\n        m_initramfs.return_value = {}  # no initramfs network config\n        # Neither datasource nor system_info disable or provide network\n\n        fake_cfg = {\n            \"config\": [{\"type\": \"physical\", \"name\": \"eth9\"}],\n            \"version\": 1,\n        }\n\n        def fake_generate_fallback():\n            return fake_cfg\n\n        # Monkey patch distro which gets cached on self.init\n        distro = self.init.distro\n        distro.generate_fallback_config = fake_generate_fallback\n        assert (\n            fake_cfg,\n            NetworkConfigSource.FALLBACK,\n        ) == self.init._find_networking_config()\n        assert \"network config disabled\" not in caplog.text\n\n    @mock.patch(M_PATH + \"cmdline.read_initramfs_config\", return_value={})\n    @mock.patch(M_PATH + \"cmdline.read_kernel_cmdline_config\", return_value={})\n    def test_warn_on_empty_network(self, m_cmdline, m_initramfs, caplog):\n        \"\"\"funky whitespace can lead to a network key that is None, which then\n        causes fallback. Test warning log on empty network key.\n        \"\"\"\n        m_cmdline.return_value = {}  # Kernel doesn't disable networking\n        m_initramfs.return_value = {}  # no initramfs network config\n        # Neither datasource nor system_info disable or provide network\n        self.init._cfg = {\n            \"system_info\": {\"paths\": {\"cloud_dir\": self.tmpdir}},\n            \"network\": None,\n        }\n        self.init.datasource = FakeDataSource(network_config={\"network\": None})\n\n        self.init.distro.generate_fallback_config = lambda: {}\n\n        self.init._find_networking_config()\n        assert \"Empty network config found\" in caplog.text\n\n    def test_apply_network_config_disabled(self, caplog):\n        \"\"\"Log when network is disabled by upgraded-network.\"\"\"\n        disable_file = os.path.join(\n            self.init.paths.get_cpath(\"data\"), \"upgraded-network\"\n        )\n\n        def fake_network_config():\n            return (None, disable_file)\n\n        self.init._find_networking_config = fake_network_config\n\n        self.init.apply_network_config(True)\n        assert caplog.records[0].levelname == \"INFO\"\n        assert f\"network config is disabled by {disable_file}\" in caplog.text\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    def test_apply_network_on_new_instance(self, m_ubuntu, m_macs):\n        \"\"\"Call distro apply_network_config methods on is_new_instance.\"\"\"\n        net_cfg = {\n            \"version\": 1,\n            \"config\": [\n                {\n                    \"subnets\": [{\"type\": \"dhcp\"}],\n                    \"type\": \"physical\",\n                    \"name\": \"eth9\",\n                    \"mac_address\": \"42:42:42:42:42:42\",\n                }\n            ],\n        }\n\n        def fake_network_config():\n            return net_cfg, NetworkConfigSource.FALLBACK\n\n        m_macs.return_value = {\"42:42:42:42:42:42\": \"eth9\"}\n\n        self.init._find_networking_config = fake_network_config\n\n        self.init.apply_network_config(True)\n        networking = self.init.distro.networking\n        networking.apply_network_config_names.assert_called_with(net_cfg)\n        self.init.distro.apply_network_config.assert_called_with(\n            net_cfg, bring_up=True\n        )\n\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    def test_apply_network_on_same_instance_id(self, m_ubuntu, caplog):\n        \"\"\"Only call distro.networking.apply_network_config_names on same\n        instance id.\"\"\"\n        self.init.is_new_instance = self._real_is_new_instance\n        old_instance_id = os.path.join(\n            self.init.paths.get_cpath(\"data\"), \"instance-id\"\n        )\n        write_file(old_instance_id, TEST_INSTANCE_ID)\n        net_cfg = {\n            \"version\": 1,\n            \"config\": [\n                {\n                    \"subnets\": [{\"type\": \"dhcp\"}],\n                    \"type\": \"physical\",\n                    \"name\": \"eth9\",\n                    \"mac_address\": \"42:42:42:42:42:42\",\n                }\n            ],\n        }\n\n        def fake_network_config():\n            return net_cfg, NetworkConfigSource.FALLBACK\n\n        self.init._find_networking_config = fake_network_config\n\n        self.init.apply_network_config(True)\n        networking = self.init.distro.networking\n        networking.apply_network_config_names.assert_called_with(net_cfg)\n        self.init.distro.apply_network_config.assert_not_called()\n        assert (\n            \"No network config applied. Neither a new instance nor datasource \"\n            \"network update allowed\" in caplog.text\n        )\n\n    def _apply_network_setup(self, m_macs):\n        old_instance_id = os.path.join(\n            self.init.paths.get_cpath(\"data\"), \"instance-id\"\n        )\n        write_file(old_instance_id, TEST_INSTANCE_ID)\n        net_cfg = {\n            \"version\": 1,\n            \"config\": [\n                {\n                    \"subnets\": [{\"type\": \"dhcp\"}],\n                    \"type\": \"physical\",\n                    \"name\": \"eth9\",\n                    \"mac_address\": \"42:42:42:42:42:42\",\n                }\n            ],\n        }\n\n        def fake_network_config():\n            return net_cfg, NetworkConfigSource.FALLBACK\n\n        m_macs.return_value = {\"42:42:42:42:42:42\": \"eth9\"}\n\n        self.init._find_networking_config = fake_network_config\n        self.init.datasource = FakeDataSource(paths=self.init.paths)\n        self.init.is_new_instance = mock.Mock(return_value=False)\n        return net_cfg\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    @mock.patch.dict(\n        sources.DataSource.default_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE, EventType.BOOT}},\n    )\n    def test_apply_network_allowed_when_default_boot(self, m_ubuntu, m_macs):\n        \"\"\"Apply network if datasource permits BOOT event.\"\"\"\n        net_cfg = self._apply_network_setup(m_macs)\n\n        self.init.apply_network_config(True)\n        networking = self.init.distro.networking\n        assert (\n            mock.call(net_cfg)\n            == networking.apply_network_config_names.call_args_list[-1]\n        )\n        assert (\n            mock.call(net_cfg, bring_up=True)\n            == self.init.distro.apply_network_config.call_args_list[-1]\n        )\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    @mock.patch.dict(\n        sources.DataSource.default_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_apply_network_disabled_when_no_default_boot(\n        self, m_ubuntu, m_macs, caplog\n    ):\n        \"\"\"Don't apply network if datasource has no BOOT event.\"\"\"\n        self._apply_network_setup(m_macs)\n        self.init.apply_network_config(True)\n        self.init.distro.apply_network_config.assert_not_called()\n        assert (\n            \"No network config applied. Neither a new instance nor datasource \"\n            \"network update allowed\" in caplog.text\n        )\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    @mock.patch.dict(\n        sources.DataSource.default_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_apply_network_allowed_with_userdata_overrides(\n        self, m_ubuntu, m_macs\n    ):\n        \"\"\"Apply network if userdata overrides default config\"\"\"\n        net_cfg = self._apply_network_setup(m_macs)\n        self.init._cfg = {\"updates\": {\"network\": {\"when\": [\"boot\"]}}}\n        self.init.apply_network_config(True)\n        networking = self.init.distro.networking\n        networking.apply_network_config_names.assert_called_with(net_cfg)\n        self.init.distro.apply_network_config.assert_called_with(\n            net_cfg, bring_up=True\n        )\n\n    @mock.patch(\"cloudinit.net.get_interfaces_by_mac\")\n    @mock.patch(\"cloudinit.distros.ubuntu.Distro\")\n    @mock.patch.dict(\n        sources.DataSource.supported_update_events,\n        {EventScope.NETWORK: {EventType.BOOT_NEW_INSTANCE}},\n    )\n    def test_apply_network_disabled_when_unsupported(\n        self, m_ubuntu, m_macs, caplog\n    ):\n        \"\"\"Don't apply network config if unsupported.\n\n        Shouldn't work even when specified as userdata\n        \"\"\"\n        self._apply_network_setup(m_macs)\n\n        self.init._cfg = {\"updates\": {\"network\": {\"when\": [\"boot\"]}}}\n        self.init.apply_network_config(True)\n        self.init.distro.apply_network_config.assert_not_called()\n        assert (\n            \"No network config applied. Neither a new instance nor datasource \"\n            \"network update allowed\" in caplog.text\n        )\n\n\nclass TestInit_InitializeFilesystem:\n    \"\"\"Tests for cloudinit.stages.Init._initialize_filesystem.\n\n    TODO: Expand these tests to cover all of _initialize_filesystem's behavior.\n    \"\"\"\n\n    @pytest.fixture\n    def init(self, paths):\n        \"\"\"A fixture which yields a stages.Init instance with paths and cfg set\n\n        As it is replaced with a mock, consumers of this fixture can set\n        `init._cfg` if the default empty dict configuration is not appropriate.\n        \"\"\"\n        with mock.patch(M_PATH + \"util.ensure_dirs\"):\n            init = stages.Init()\n            init._cfg = {}\n            init._paths = paths\n            yield init\n\n    @mock.patch(M_PATH + \"util.ensure_file\")\n    def test_ensure_file_not_called_if_no_log_file_configured(\n        self, m_ensure_file, init\n    ):\n        \"\"\"If no log file is configured, we should not ensure its existence.\"\"\"\n        init._cfg = {}\n\n        init._initialize_filesystem()\n\n        assert 0 == m_ensure_file.call_count\n\n    def test_log_files_existence_is_ensured_if_configured(self, init, tmpdir):\n        \"\"\"If a log file is configured, we should ensure its existence.\"\"\"\n        log_file = tmpdir.join(\"cloud-init.log\")\n        init._cfg = {\"def_log_file\": str(log_file)}\n\n        init._initialize_filesystem()\n\n        assert log_file.exists()\n        # Assert we create it 0o640  by default if it doesn't already exist\n        assert 0o640 == stat.S_IMODE(log_file.stat().mode)\n\n    def test_existing_file_permissions(self, init, tmpdir):\n        \"\"\"Test file permissions are set as expected.\n\n        CIS Hardening requires 640 permissions. These permissions are\n        currently hardcoded on every boot, but if there's ever a reason\n        to change this, we need to then ensure that they\n        are *not* set every boot.\n\n        See https://bugs.launchpad.net/cloud-init/+bug/1900837.\n        \"\"\"\n        log_file = tmpdir.join(\"cloud-init.log\")\n        log_file.ensure()\n        # Use a mode that will never be made the default so this test will\n        # always be valid\n        log_file.chmod(0o606)\n        init._cfg = {\"def_log_file\": str(log_file)}\n\n        init._initialize_filesystem()\n\n        assert 0o640 == stat.S_IMODE(log_file.stat().mode)\n"], "filenames": ["cloudinit/sources/DataSourceLXD.py", "cloudinit/sources/DataSourceVultr.py", "cloudinit/sources/__init__.py", "cloudinit/stages.py", "tests/unittests/sources/test_init.py", "tests/unittests/test_stages.py"], "buggy_code_start_loc": [17, 6, 114, 206, 460, 609], "buggy_code_end_loc": [175, 161, 252, 207, 576, 625], "fixing_code_start_loc": [17, 7, 114, 206, 461, 609], "fixing_code_end_loc": [179, 158, 275, 209, 602, 629], "type": "CWE-532", "message": "Sensitive data could be exposed in logs of cloud-init before version 23.1.2. An attacker could use this information to find hashed passwords and possibly escalate their privilege.", "other": {"cve": {"id": "CVE-2023-1786", "sourceIdentifier": "security@ubuntu.com", "published": "2023-04-26T23:15:08.690", "lastModified": "2023-05-08T18:38:50.347", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Sensitive data could be exposed in logs of cloud-init before version 23.1.2. An attacker could use this information to find hashed passwords and possibly escalate their privilege."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security@ubuntu.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-532"}]}, {"source": "security@ubuntu.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-532"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:canonical:cloud-init:*:*:*:*:*:*:*:*", "versionEndExcluding": "23.1.2", "matchCriteriaId": "3BD226D5-A4A7-42EC-B66E-0252C1706B33"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:esm:*:*:*", "matchCriteriaId": "7A5301BF-1402-4BE0-A0F8-69FBE79BC6D6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:esm:*:*:*", "matchCriteriaId": "B3293E55-5506-4587-A318-D1734F781C09"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:20.04:*:*:*:lts:*:*:*", "matchCriteriaId": "902B8056-9E37-443B-8905-8AA93E2447FB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:22.04:*:*:*:lts:*:*:*", "matchCriteriaId": "359012F1-2C63-415A-88B8-6726A87830DE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:22.10:*:*:*:-:*:*:*", "matchCriteriaId": "47842532-D2B6-44CB-ADE2-4AC8630A4D8C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:23.04:*:*:*:*:*:*:*", "matchCriteriaId": "B2E702D7-F8C0-49BF-9FFB-883017076E98"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:38:*:*:*:*:*:*:*", "matchCriteriaId": "CC559B26-5DFC-4B7A-A27C-B77DE755DFF9"}]}]}], "references": [{"url": "https://bugs.launchpad.net/cloud-init/+bug/2013967", "source": "security@ubuntu.com", "tags": ["Issue Tracking"]}, {"url": "https://github.com/canonical/cloud-init/commit/a378b7e4f47375458651c0972e7cd813f6fe0a6b", "source": "security@ubuntu.com", "tags": ["Patch"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/ATBJSXPL2IOAD2LDQRKWPLIC7QXS44GZ/", "source": "security@ubuntu.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://ubuntu.com/security/notices/USN-6042-1", "source": "security@ubuntu.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/canonical/cloud-init/commit/a378b7e4f47375458651c0972e7cd813f6fe0a6b"}}