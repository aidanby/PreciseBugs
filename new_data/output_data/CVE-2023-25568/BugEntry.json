{"buggy_code": ["package defaults\n\nimport (\n\t\"time\"\n)\n\nconst (\n\t// these requests take at _least_ two minutes at the moment.\n\tProvideTimeout  = time.Minute * 3\n\tProvSearchDelay = time.Second\n\n\t// Number of concurrent workers in decision engine that process requests to the blockstore\n\tBitswapEngineBlockstoreWorkerCount = 128\n\t// the total number of simultaneous threads sending outgoing messages\n\tBitswapTaskWorkerCount = 8\n\t// how many worker threads to start for decision engine task worker\n\tBitswapEngineTaskWorkerCount = 8\n\t// the total amount of bytes that a peer should have outstanding, it is utilized by the decision engine\n\tBitswapMaxOutstandingBytesPerPeer = 1 << 20\n\t// the number of bytes we attempt to make each outgoing bitswap message\n\tBitswapEngineTargetMessageSize = 16 * 1024\n\t// HasBlockBufferSize is the buffer size of the channel for new blocks\n\t// that need to be provided. They should get pulled over by the\n\t// provideCollector even before they are actually provided.\n\t// TODO: Does this need to be this large givent that?\n\tHasBlockBufferSize = 256\n\n\t// Maximum size of the wantlist we are willing to keep in memory.\n\tMaxQueuedWantlistEntiresPerPeer = 1024\n)\n", "package network\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\tbsmsg \"github.com/ipfs/go-libipfs/bitswap/message\"\n\t\"github.com/ipfs/go-libipfs/bitswap/network/internal\"\n\n\tcid \"github.com/ipfs/go-cid\"\n\tlogging \"github.com/ipfs/go-log\"\n\t\"github.com/libp2p/go-libp2p/core/connmgr\"\n\t\"github.com/libp2p/go-libp2p/core/host\"\n\t\"github.com/libp2p/go-libp2p/core/network\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\tpeerstore \"github.com/libp2p/go-libp2p/core/peerstore\"\n\t\"github.com/libp2p/go-libp2p/core/protocol\"\n\t\"github.com/libp2p/go-libp2p/core/routing\"\n\t\"github.com/libp2p/go-libp2p/p2p/protocol/ping\"\n\tmsgio \"github.com/libp2p/go-msgio\"\n\tma \"github.com/multiformats/go-multiaddr\"\n\t\"github.com/multiformats/go-multistream\"\n)\n\nvar log = logging.Logger(\"bitswap_network\")\n\nvar connectTimeout = time.Second * 5\n\nvar maxSendTimeout = 2 * time.Minute\nvar minSendTimeout = 10 * time.Second\nvar sendLatency = 2 * time.Second\nvar minSendRate = (100 * 1000) / 8 // 100kbit/s\n\n// NewFromIpfsHost returns a BitSwapNetwork supported by underlying IPFS host.\nfunc NewFromIpfsHost(host host.Host, r routing.ContentRouting, opts ...NetOpt) BitSwapNetwork {\n\ts := processSettings(opts...)\n\n\tbitswapNetwork := impl{\n\t\thost:    host,\n\t\trouting: r,\n\n\t\tprotocolBitswapNoVers:  s.ProtocolPrefix + ProtocolBitswapNoVers,\n\t\tprotocolBitswapOneZero: s.ProtocolPrefix + ProtocolBitswapOneZero,\n\t\tprotocolBitswapOneOne:  s.ProtocolPrefix + ProtocolBitswapOneOne,\n\t\tprotocolBitswap:        s.ProtocolPrefix + ProtocolBitswap,\n\n\t\tsupportedProtocols: s.SupportedProtocols,\n\t}\n\n\treturn &bitswapNetwork\n}\n\nfunc processSettings(opts ...NetOpt) Settings {\n\ts := Settings{SupportedProtocols: append([]protocol.ID(nil), internal.DefaultProtocols...)}\n\tfor _, opt := range opts {\n\t\topt(&s)\n\t}\n\tfor i, proto := range s.SupportedProtocols {\n\t\ts.SupportedProtocols[i] = s.ProtocolPrefix + proto\n\t}\n\treturn s\n}\n\n// impl transforms the ipfs network interface, which sends and receives\n// NetMessage objects, into the bitswap network interface.\ntype impl struct {\n\t// NOTE: Stats must be at the top of the heap allocation to ensure 64bit\n\t// alignment.\n\tstats Stats\n\n\thost          host.Host\n\trouting       routing.ContentRouting\n\tconnectEvtMgr *connectEventManager\n\n\tprotocolBitswapNoVers  protocol.ID\n\tprotocolBitswapOneZero protocol.ID\n\tprotocolBitswapOneOne  protocol.ID\n\tprotocolBitswap        protocol.ID\n\n\tsupportedProtocols []protocol.ID\n\n\t// inbound messages from the network are forwarded to the receiver\n\treceivers []Receiver\n}\n\ntype streamMessageSender struct {\n\tto        peer.ID\n\tstream    network.Stream\n\tconnected bool\n\tbsnet     *impl\n\topts      *MessageSenderOpts\n}\n\n// Open a stream to the remote peer\nfunc (s *streamMessageSender) Connect(ctx context.Context) (network.Stream, error) {\n\tif s.connected {\n\t\treturn s.stream, nil\n\t}\n\n\ttctx, cancel := context.WithTimeout(ctx, s.opts.SendTimeout)\n\tdefer cancel()\n\n\tif err := s.bsnet.ConnectTo(tctx, s.to); err != nil {\n\t\treturn nil, err\n\t}\n\n\tstream, err := s.bsnet.newStreamToPeer(tctx, s.to)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.stream = stream\n\ts.connected = true\n\treturn s.stream, nil\n}\n\n// Reset the stream\nfunc (s *streamMessageSender) Reset() error {\n\tif s.stream != nil {\n\t\terr := s.stream.Reset()\n\t\ts.connected = false\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Close the stream\nfunc (s *streamMessageSender) Close() error {\n\treturn s.stream.Close()\n}\n\n// Indicates whether the peer supports HAVE / DONT_HAVE messages\nfunc (s *streamMessageSender) SupportsHave() bool {\n\treturn s.bsnet.SupportsHave(s.stream.Protocol())\n}\n\n// Send a message to the peer, attempting multiple times\nfunc (s *streamMessageSender) SendMsg(ctx context.Context, msg bsmsg.BitSwapMessage) error {\n\treturn s.multiAttempt(ctx, func() error {\n\t\treturn s.send(ctx, msg)\n\t})\n}\n\n// Perform a function with multiple attempts, and a timeout\nfunc (s *streamMessageSender) multiAttempt(ctx context.Context, fn func() error) error {\n\t// Try to call the function repeatedly\n\tvar err error\n\tfor i := 0; i < s.opts.MaxRetries; i++ {\n\t\tif err = fn(); err == nil {\n\t\t\t// Attempt was successful\n\t\t\treturn nil\n\t\t}\n\n\t\t// Attempt failed\n\n\t\t// If the sender has been closed or the context cancelled, just bail out\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t}\n\n\t\t// Protocol is not supported, so no need to try multiple times\n\t\tif errors.Is(err, multistream.ErrNotSupported[protocol.ID]{}) {\n\t\t\ts.bsnet.connectEvtMgr.MarkUnresponsive(s.to)\n\t\t\treturn err\n\t\t}\n\n\t\t// Failed to send so reset stream and try again\n\t\t_ = s.Reset()\n\n\t\t// Failed too many times so mark the peer as unresponsive and return an error\n\t\tif i == s.opts.MaxRetries-1 {\n\t\t\ts.bsnet.connectEvtMgr.MarkUnresponsive(s.to)\n\t\t\treturn err\n\t\t}\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase <-time.After(s.opts.SendErrorBackoff):\n\t\t\t// wait a short time in case disconnect notifications are still propagating\n\t\t\tlog.Infof(\"send message to %s failed but context was not Done: %s\", s.to, err)\n\t\t}\n\t}\n\treturn err\n}\n\n// Send a message to the peer\nfunc (s *streamMessageSender) send(ctx context.Context, msg bsmsg.BitSwapMessage) error {\n\tstart := time.Now()\n\tstream, err := s.Connect(ctx)\n\tif err != nil {\n\t\tlog.Infof(\"failed to open stream to %s: %s\", s.to, err)\n\t\treturn err\n\t}\n\n\t// The send timeout includes the time required to connect\n\t// (although usually we will already have connected - we only need to\n\t// connect after a failed attempt to send)\n\ttimeout := s.opts.SendTimeout - time.Since(start)\n\tif err = s.bsnet.msgToStream(ctx, stream, msg, timeout); err != nil {\n\t\tlog.Infof(\"failed to send message to %s: %s\", s.to, err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (bsnet *impl) Self() peer.ID {\n\treturn bsnet.host.ID()\n}\n\nfunc (bsnet *impl) Ping(ctx context.Context, p peer.ID) ping.Result {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tres := <-ping.Ping(ctx, bsnet.host, p)\n\treturn res\n}\n\nfunc (bsnet *impl) Latency(p peer.ID) time.Duration {\n\treturn bsnet.host.Peerstore().LatencyEWMA(p)\n}\n\n// Indicates whether the given protocol supports HAVE / DONT_HAVE messages\nfunc (bsnet *impl) SupportsHave(proto protocol.ID) bool {\n\tswitch proto {\n\tcase bsnet.protocolBitswapOneOne, bsnet.protocolBitswapOneZero, bsnet.protocolBitswapNoVers:\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc (bsnet *impl) msgToStream(ctx context.Context, s network.Stream, msg bsmsg.BitSwapMessage, timeout time.Duration) error {\n\tdeadline := time.Now().Add(timeout)\n\tif dl, ok := ctx.Deadline(); ok && dl.Before(deadline) {\n\t\tdeadline = dl\n\t}\n\n\tif err := s.SetWriteDeadline(deadline); err != nil {\n\t\tlog.Warnf(\"error setting deadline: %s\", err)\n\t}\n\n\t// Older Bitswap versions use a slightly different wire format so we need\n\t// to convert the message to the appropriate format depending on the remote\n\t// peer's Bitswap version.\n\tswitch s.Protocol() {\n\tcase bsnet.protocolBitswapOneOne, bsnet.protocolBitswap:\n\t\tif err := msg.ToNetV1(s); err != nil {\n\t\t\tlog.Debugf(\"error: %s\", err)\n\t\t\treturn err\n\t\t}\n\tcase bsnet.protocolBitswapOneZero, bsnet.protocolBitswapNoVers:\n\t\tif err := msg.ToNetV0(s); err != nil {\n\t\t\tlog.Debugf(\"error: %s\", err)\n\t\t\treturn err\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"unrecognized protocol on remote: %s\", s.Protocol())\n\t}\n\n\tatomic.AddUint64(&bsnet.stats.MessagesSent, 1)\n\n\tif err := s.SetWriteDeadline(time.Time{}); err != nil {\n\t\tlog.Warnf(\"error resetting deadline: %s\", err)\n\t}\n\treturn nil\n}\n\nfunc (bsnet *impl) NewMessageSender(ctx context.Context, p peer.ID, opts *MessageSenderOpts) (MessageSender, error) {\n\topts = setDefaultOpts(opts)\n\n\tsender := &streamMessageSender{\n\t\tto:    p,\n\t\tbsnet: bsnet,\n\t\topts:  opts,\n\t}\n\n\terr := sender.multiAttempt(ctx, func() error {\n\t\t_, err := sender.Connect(ctx)\n\t\treturn err\n\t})\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn sender, nil\n}\n\nfunc setDefaultOpts(opts *MessageSenderOpts) *MessageSenderOpts {\n\tcopy := *opts\n\tif opts.MaxRetries == 0 {\n\t\tcopy.MaxRetries = 3\n\t}\n\tif opts.SendTimeout == 0 {\n\t\tcopy.SendTimeout = maxSendTimeout\n\t}\n\tif opts.SendErrorBackoff == 0 {\n\t\tcopy.SendErrorBackoff = 100 * time.Millisecond\n\t}\n\treturn &copy\n}\n\nfunc sendTimeout(size int) time.Duration {\n\ttimeout := sendLatency\n\ttimeout += time.Duration((uint64(time.Second) * uint64(size)) / uint64(minSendRate))\n\tif timeout > maxSendTimeout {\n\t\ttimeout = maxSendTimeout\n\t} else if timeout < minSendTimeout {\n\t\ttimeout = minSendTimeout\n\t}\n\treturn timeout\n}\n\nfunc (bsnet *impl) SendMessage(\n\tctx context.Context,\n\tp peer.ID,\n\toutgoing bsmsg.BitSwapMessage) error {\n\n\ttctx, cancel := context.WithTimeout(ctx, connectTimeout)\n\tdefer cancel()\n\n\ts, err := bsnet.newStreamToPeer(tctx, p)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttimeout := sendTimeout(outgoing.Size())\n\tif err = bsnet.msgToStream(ctx, s, outgoing, timeout); err != nil {\n\t\t_ = s.Reset()\n\t\treturn err\n\t}\n\n\treturn s.Close()\n}\n\nfunc (bsnet *impl) newStreamToPeer(ctx context.Context, p peer.ID) (network.Stream, error) {\n\treturn bsnet.host.NewStream(ctx, p, bsnet.supportedProtocols...)\n}\n\nfunc (bsnet *impl) Start(r ...Receiver) {\n\tbsnet.receivers = r\n\t{\n\t\tconnectionListeners := make([]ConnectionListener, len(r))\n\t\tfor i, v := range r {\n\t\t\tconnectionListeners[i] = v\n\t\t}\n\t\tbsnet.connectEvtMgr = newConnectEventManager(connectionListeners...)\n\t}\n\tfor _, proto := range bsnet.supportedProtocols {\n\t\tbsnet.host.SetStreamHandler(proto, bsnet.handleNewStream)\n\t}\n\tbsnet.host.Network().Notify((*netNotifiee)(bsnet))\n\tbsnet.connectEvtMgr.Start()\n\n}\n\nfunc (bsnet *impl) Stop() {\n\tbsnet.connectEvtMgr.Stop()\n\tbsnet.host.Network().StopNotify((*netNotifiee)(bsnet))\n}\n\nfunc (bsnet *impl) ConnectTo(ctx context.Context, p peer.ID) error {\n\treturn bsnet.host.Connect(ctx, peer.AddrInfo{ID: p})\n}\n\nfunc (bsnet *impl) DisconnectFrom(ctx context.Context, p peer.ID) error {\n\tpanic(\"Not implemented: DisconnectFrom() is only used by tests\")\n}\n\n// FindProvidersAsync returns a channel of providers for the given key.\nfunc (bsnet *impl) FindProvidersAsync(ctx context.Context, k cid.Cid, max int) <-chan peer.ID {\n\tout := make(chan peer.ID, max)\n\tgo func() {\n\t\tdefer close(out)\n\t\tproviders := bsnet.routing.FindProvidersAsync(ctx, k, max)\n\t\tfor info := range providers {\n\t\t\tif info.ID == bsnet.host.ID() {\n\t\t\t\tcontinue // ignore self as provider\n\t\t\t}\n\t\t\tbsnet.host.Peerstore().AddAddrs(info.ID, info.Addrs, peerstore.TempAddrTTL)\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase out <- info.ID:\n\t\t\t}\n\t\t}\n\t}()\n\treturn out\n}\n\n// Provide provides the key to the network\nfunc (bsnet *impl) Provide(ctx context.Context, k cid.Cid) error {\n\treturn bsnet.routing.Provide(ctx, k, true)\n}\n\n// handleNewStream receives a new stream from the network.\nfunc (bsnet *impl) handleNewStream(s network.Stream) {\n\tdefer s.Close()\n\n\tif len(bsnet.receivers) == 0 {\n\t\t_ = s.Reset()\n\t\treturn\n\t}\n\n\treader := msgio.NewVarintReaderSize(s, network.MessageSizeMax)\n\tfor {\n\t\treceived, err := bsmsg.FromMsgReader(reader)\n\t\tif err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\t_ = s.Reset()\n\t\t\t\tfor _, v := range bsnet.receivers {\n\t\t\t\t\tv.ReceiveError(err)\n\t\t\t\t}\n\t\t\t\tlog.Debugf(\"bitswap net handleNewStream from %s error: %s\", s.Conn().RemotePeer(), err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\tp := s.Conn().RemotePeer()\n\t\tctx := context.Background()\n\t\tlog.Debugf(\"bitswap net handleNewStream from %s\", s.Conn().RemotePeer())\n\t\tbsnet.connectEvtMgr.OnMessage(s.Conn().RemotePeer())\n\t\tatomic.AddUint64(&bsnet.stats.MessagesRecvd, 1)\n\t\tfor _, v := range bsnet.receivers {\n\t\t\tv.ReceiveMessage(ctx, p, received)\n\t\t}\n\t}\n}\n\nfunc (bsnet *impl) ConnectionManager() connmgr.ConnManager {\n\treturn bsnet.host.ConnManager()\n}\n\nfunc (bsnet *impl) Stats() Stats {\n\treturn Stats{\n\t\tMessagesRecvd: atomic.LoadUint64(&bsnet.stats.MessagesRecvd),\n\t\tMessagesSent:  atomic.LoadUint64(&bsnet.stats.MessagesSent),\n\t}\n}\n\ntype netNotifiee impl\n\nfunc (nn *netNotifiee) impl() *impl {\n\treturn (*impl)(nn)\n}\n\nfunc (nn *netNotifiee) Connected(n network.Network, v network.Conn) {\n\t// ignore transient connections\n\tif v.Stat().Transient {\n\t\treturn\n\t}\n\n\tnn.impl().connectEvtMgr.Connected(v.RemotePeer())\n}\nfunc (nn *netNotifiee) Disconnected(n network.Network, v network.Conn) {\n\t// Only record a \"disconnect\" when we actually disconnect.\n\tif n.Connectedness(v.RemotePeer()) == network.Connected {\n\t\treturn\n\t}\n\n\tnn.impl().connectEvtMgr.Disconnected(v.RemotePeer())\n}\nfunc (nn *netNotifiee) OpenedStream(n network.Network, s network.Stream) {}\nfunc (nn *netNotifiee) ClosedStream(n network.Network, v network.Stream) {}\nfunc (nn *netNotifiee) Listen(n network.Network, a ma.Multiaddr)         {}\nfunc (nn *netNotifiee) ListenClose(n network.Network, a ma.Multiaddr)    {}\n", "package bitswap\n\nimport (\n\t\"time\"\n\n\tdelay \"github.com/ipfs/go-ipfs-delay\"\n\t\"github.com/ipfs/go-libipfs/bitswap/client\"\n\t\"github.com/ipfs/go-libipfs/bitswap/server\"\n\t\"github.com/ipfs/go-libipfs/bitswap/tracer\"\n)\n\ntype option func(*Bitswap)\n\n// Option is interface{} of server.Option or client.Option or func(*Bitswap)\n// wrapped in a struct to gain strong type checking.\ntype Option struct {\n\tv interface{}\n}\n\nfunc EngineBlockstoreWorkerCount(count int) Option {\n\treturn Option{server.EngineBlockstoreWorkerCount(count)}\n}\n\nfunc EngineTaskWorkerCount(count int) Option {\n\treturn Option{server.EngineTaskWorkerCount(count)}\n}\n\nfunc MaxOutstandingBytesPerPeer(count int) Option {\n\treturn Option{server.MaxOutstandingBytesPerPeer(count)}\n}\n\nfunc MaxQueuedWantlistEntriesPerPeer(count uint) Option {\n\treturn Option{server.MaxQueuedWantlistEntriesPerPeer(count)}\n}\n\nfunc TaskWorkerCount(count int) Option {\n\treturn Option{server.TaskWorkerCount(count)}\n}\n\nfunc ProvideEnabled(enabled bool) Option {\n\treturn Option{server.ProvideEnabled(enabled)}\n}\n\nfunc SetSendDontHaves(send bool) Option {\n\treturn Option{server.SetSendDontHaves(send)}\n}\n\nfunc WithPeerBlockRequestFilter(pbrf server.PeerBlockRequestFilter) Option {\n\treturn Option{server.WithPeerBlockRequestFilter(pbrf)}\n}\n\nfunc WithScoreLedger(scoreLedger server.ScoreLedger) Option {\n\treturn Option{server.WithScoreLedger(scoreLedger)}\n}\n\nfunc WithTargetMessageSize(tms int) Option {\n\treturn Option{server.WithTargetMessageSize(tms)}\n}\n\nfunc WithTaskComparator(comparator server.TaskComparator) Option {\n\treturn Option{server.WithTaskComparator(comparator)}\n}\n\nfunc ProviderSearchDelay(newProvSearchDelay time.Duration) Option {\n\treturn Option{client.ProviderSearchDelay(newProvSearchDelay)}\n}\n\nfunc RebroadcastDelay(newRebroadcastDelay delay.D) Option {\n\treturn Option{client.RebroadcastDelay(newRebroadcastDelay)}\n}\n\nfunc SetSimulateDontHavesOnTimeout(send bool) Option {\n\treturn Option{client.SetSimulateDontHavesOnTimeout(send)}\n}\n\nfunc WithTracer(tap tracer.Tracer) Option {\n\t// Only trace the server, both receive the same messages anyway\n\treturn Option{\n\t\toption(func(bs *Bitswap) {\n\t\t\tbs.tracer = tap\n\t\t}),\n\t}\n}\n", "// Package decision implements the decision engine for the bitswap service.\npackage decision\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math/bits\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\n\t\"github.com/ipfs/go-cid\"\n\tbstore \"github.com/ipfs/go-ipfs-blockstore\"\n\twl \"github.com/ipfs/go-libipfs/bitswap/client/wantlist\"\n\t\"github.com/ipfs/go-libipfs/bitswap/internal/defaults\"\n\tbsmsg \"github.com/ipfs/go-libipfs/bitswap/message\"\n\tpb \"github.com/ipfs/go-libipfs/bitswap/message/pb\"\n\tbmetrics \"github.com/ipfs/go-libipfs/bitswap/metrics\"\n\tblocks \"github.com/ipfs/go-libipfs/blocks\"\n\tlogging \"github.com/ipfs/go-log\"\n\t\"github.com/ipfs/go-metrics-interface\"\n\t\"github.com/ipfs/go-peertaskqueue\"\n\t\"github.com/ipfs/go-peertaskqueue/peertask\"\n\t\"github.com/ipfs/go-peertaskqueue/peertracker\"\n\tprocess \"github.com/jbenet/goprocess\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n)\n\n// TODO consider taking responsibility for other types of requests. For\n// example, there could be a |cancelQueue| for all of the cancellation\n// messages that need to go out. There could also be a |wantlistQueue| for\n// the local peer's wantlists. Alternatively, these could all be bundled\n// into a single, intelligent global queue that efficiently\n// batches/combines and takes all of these into consideration.\n//\n// Right now, messages go onto the network for four reasons:\n// 1. an initial `sendwantlist` message to a provider of the first key in a\n//    request\n// 2. a periodic full sweep of `sendwantlist` messages to all providers\n// 3. upon receipt of blocks, a `cancel` message to all peers\n// 4. draining the priority queue of `blockrequests` from peers\n//\n// Presently, only `blockrequests` are handled by the decision engine.\n// However, there is an opportunity to give it more responsibility! If the\n// decision engine is given responsibility for all of the others, it can\n// intelligently decide how to combine requests efficiently.\n//\n// Some examples of what would be possible:\n//\n// * when sending out the wantlists, include `cancel` requests\n// * when handling `blockrequests`, include `sendwantlist` and `cancel` as\n//   appropriate\n// * when handling `cancel`, if we recently received a wanted block from a\n//   peer, include a partial wantlist that contains a few other high priority\n//   blocks\n//\n// In a sense, if we treat the decision engine as a black box, it could do\n// whatever it sees fit to produce desired outcomes (get wanted keys\n// quickly, maintain good relationships with peers, etc).\n\nvar log = logging.Logger(\"engine\")\n\nconst (\n\t// outboxChanBuffer must be 0 to prevent stale messages from being sent\n\toutboxChanBuffer = 0\n\t// targetMessageSize is the ideal size of the batched payload. We try to\n\t// pop this much data off the request queue, but it may be a little more\n\t// or less depending on what's in the queue.\n\tdefaultTargetMessageSize = 16 * 1024\n\t// tagFormat is the tag given to peers associated an engine\n\ttagFormat = \"bs-engine-%s-%s\"\n\n\t// queuedTagWeight is the default weight for peers that have work queued\n\t// on their behalf.\n\tqueuedTagWeight = 10\n\n\t// maxBlockSizeReplaceHasWithBlock is the maximum size of the block in\n\t// bytes up to which we will replace a want-have with a want-block\n\tmaxBlockSizeReplaceHasWithBlock = 1024\n)\n\n// Envelope contains a message for a Peer.\ntype Envelope struct {\n\t// Peer is the intended recipient.\n\tPeer peer.ID\n\n\t// Message is the payload.\n\tMessage bsmsg.BitSwapMessage\n\n\t// A callback to notify the decision queue that the task is complete\n\tSent func()\n}\n\n// PeerTagger covers the methods on the connection manager used by the decision\n// engine to tag peers\ntype PeerTagger interface {\n\tTagPeer(peer.ID, string, int)\n\tUntagPeer(p peer.ID, tag string)\n}\n\n// Assigns a specific score to a peer\ntype ScorePeerFunc func(peer.ID, int)\n\n// ScoreLedger is an external ledger dealing with peer scores.\ntype ScoreLedger interface {\n\t// Returns aggregated data communication with a given peer.\n\tGetReceipt(p peer.ID) *Receipt\n\t// Increments the sent counter for the given peer.\n\tAddToSentBytes(p peer.ID, n int)\n\t// Increments the received counter for the given peer.\n\tAddToReceivedBytes(p peer.ID, n int)\n\t// PeerConnected should be called when a new peer connects,\n\t// meaning the ledger should open accounting.\n\tPeerConnected(p peer.ID)\n\t// PeerDisconnected should be called when a peer disconnects to\n\t// clean up the accounting.\n\tPeerDisconnected(p peer.ID)\n\t// Starts the ledger sampling process.\n\tStart(scorePeer ScorePeerFunc)\n\t// Stops the sampling process.\n\tStop()\n}\n\n// Engine manages sending requested blocks to peers.\ntype Engine struct {\n\t// peerRequestQueue is a priority queue of requests received from peers.\n\t// Requests are popped from the queue, packaged up, and placed in the\n\t// outbox.\n\tpeerRequestQueue *peertaskqueue.PeerTaskQueue\n\n\t// FIXME it's a bit odd for the client and the worker to both share memory\n\t// (both modify the peerRequestQueue) and also to communicate over the\n\t// workSignal channel. consider sending requests over the channel and\n\t// allowing the worker to have exclusive access to the peerRequestQueue. In\n\t// that case, no lock would be required.\n\tworkSignal chan struct{}\n\n\t// outbox contains outgoing messages to peers. This is owned by the\n\t// taskWorker goroutine\n\toutbox chan (<-chan *Envelope)\n\n\tbsm *blockstoreManager\n\n\tpeerTagger PeerTagger\n\n\ttagQueued, tagUseful string\n\n\tlock sync.RWMutex // protects the fields immediately below\n\n\t// peerLedger saves which peers are waiting for a Cid\n\tpeerLedger *peerLedger\n\n\t// an external ledger dealing with peer scores\n\tscoreLedger ScoreLedger\n\n\tticker *time.Ticker\n\n\ttaskWorkerLock  sync.Mutex\n\ttaskWorkerCount int\n\n\ttargetMessageSize int\n\n\t// maxBlockSizeReplaceHasWithBlock is the maximum size of the block in\n\t// bytes up to which we will replace a want-have with a want-block\n\tmaxBlockSizeReplaceHasWithBlock int\n\n\tsendDontHaves bool\n\n\tself peer.ID\n\n\t// metrics gauge for total pending tasks across all workers\n\tpendingGauge metrics.Gauge\n\n\t// metrics gauge for total pending tasks across all workers\n\tactiveGauge metrics.Gauge\n\n\t// used to ensure metrics are reported each fixed number of operation\n\tmetricsLock         sync.Mutex\n\tmetricUpdateCounter int\n\n\ttaskComparator TaskComparator\n\n\tpeerBlockRequestFilter PeerBlockRequestFilter\n\n\tbstoreWorkerCount          int\n\tmaxOutstandingBytesPerPeer int\n\n\tmaxQueuedWantlistEntriesPerPeer uint\n}\n\n// TaskInfo represents the details of a request from a peer.\ntype TaskInfo struct {\n\tPeer peer.ID\n\t// The CID of the block\n\tCid cid.Cid\n\t// Tasks can be want-have or want-block\n\tIsWantBlock bool\n\t// Whether to immediately send a response if the block is not found\n\tSendDontHave bool\n\t// The size of the block corresponding to the task\n\tBlockSize int\n\t// Whether the block was found\n\tHaveBlock bool\n}\n\n// TaskComparator is used for task prioritization.\n// It should return true if task 'ta' has higher priority than task 'tb'\ntype TaskComparator func(ta, tb *TaskInfo) bool\n\n// PeerBlockRequestFilter is used to accept / deny requests for a CID coming from a PeerID\n// It should return true if the request should be fullfilled.\ntype PeerBlockRequestFilter func(p peer.ID, c cid.Cid) bool\n\ntype Option func(*Engine)\n\nfunc WithTaskComparator(comparator TaskComparator) Option {\n\treturn func(e *Engine) {\n\t\te.taskComparator = comparator\n\t}\n}\n\nfunc WithPeerBlockRequestFilter(pbrf PeerBlockRequestFilter) Option {\n\treturn func(e *Engine) {\n\t\te.peerBlockRequestFilter = pbrf\n\t}\n}\n\nfunc WithTargetMessageSize(size int) Option {\n\treturn func(e *Engine) {\n\t\te.targetMessageSize = size\n\t}\n}\n\nfunc WithScoreLedger(scoreledger ScoreLedger) Option {\n\treturn func(e *Engine) {\n\t\te.scoreLedger = scoreledger\n\t}\n}\n\n// WithBlockstoreWorkerCount sets the number of worker threads used for\n// blockstore operations in the decision engine\nfunc WithBlockstoreWorkerCount(count int) Option {\n\tif count <= 0 {\n\t\tpanic(fmt.Sprintf(\"Engine blockstore worker count is %d but must be > 0\", count))\n\t}\n\treturn func(e *Engine) {\n\t\te.bstoreWorkerCount = count\n\t}\n}\n\n// WithTaskWorkerCount sets the number of worker threads used inside the engine\nfunc WithTaskWorkerCount(count int) Option {\n\tif count <= 0 {\n\t\tpanic(fmt.Sprintf(\"Engine task worker count is %d but must be > 0\", count))\n\t}\n\treturn func(e *Engine) {\n\t\te.taskWorkerCount = count\n\t}\n}\n\n// WithMaxOutstandingBytesPerPeer describes approximately how much work we are will to have outstanding to a peer at any\n// given time. Setting it to 0 will disable any limiting.\nfunc WithMaxOutstandingBytesPerPeer(count int) Option {\n\tif count < 0 {\n\t\tpanic(fmt.Sprintf(\"max outstanding bytes per peer is %d but must be >= 0\", count))\n\t}\n\treturn func(e *Engine) {\n\t\te.maxOutstandingBytesPerPeer = count\n\t}\n}\n\n// WithMaxQueuedWantlistEntriesPerPeer limits how much individual entries each peer is allowed to send.\n// If a peer send us more than this we will truncate newest entries.\n// It defaults to DefaultMaxQueuedWantlistEntiresPerPeer.\nfunc WithMaxQueuedWantlistEntriesPerPeer(count uint) Option {\n\treturn func(e *Engine) {\n\t\te.maxQueuedWantlistEntriesPerPeer = count\n\t}\n}\n\nfunc WithSetSendDontHave(send bool) Option {\n\treturn func(e *Engine) {\n\t\te.sendDontHaves = send\n\t}\n}\n\n// wrapTaskComparator wraps a TaskComparator so it can be used as a QueueTaskComparator\nfunc wrapTaskComparator(tc TaskComparator) peertask.QueueTaskComparator {\n\treturn func(a, b *peertask.QueueTask) bool {\n\t\ttaskDataA := a.Task.Data.(*taskData)\n\t\ttaskInfoA := &TaskInfo{\n\t\t\tPeer:         a.Target,\n\t\t\tCid:          a.Task.Topic.(cid.Cid),\n\t\t\tIsWantBlock:  taskDataA.IsWantBlock,\n\t\t\tSendDontHave: taskDataA.SendDontHave,\n\t\t\tBlockSize:    taskDataA.BlockSize,\n\t\t\tHaveBlock:    taskDataA.HaveBlock,\n\t\t}\n\t\ttaskDataB := b.Task.Data.(*taskData)\n\t\ttaskInfoB := &TaskInfo{\n\t\t\tPeer:         b.Target,\n\t\t\tCid:          b.Task.Topic.(cid.Cid),\n\t\t\tIsWantBlock:  taskDataB.IsWantBlock,\n\t\t\tSendDontHave: taskDataB.SendDontHave,\n\t\t\tBlockSize:    taskDataB.BlockSize,\n\t\t\tHaveBlock:    taskDataB.HaveBlock,\n\t\t}\n\t\treturn tc(taskInfoA, taskInfoB)\n\t}\n}\n\n// NewEngine creates a new block sending engine for the given block store.\n// maxOutstandingBytesPerPeer hints to the peer task queue not to give a peer more tasks if it has some maximum\n// work already outstanding.\nfunc NewEngine(\n\tctx context.Context,\n\tbs bstore.Blockstore,\n\tpeerTagger PeerTagger,\n\tself peer.ID,\n\topts ...Option,\n) *Engine {\n\treturn newEngine(\n\t\tctx,\n\t\tbs,\n\t\tpeerTagger,\n\t\tself,\n\t\tmaxBlockSizeReplaceHasWithBlock,\n\t\topts...,\n\t)\n}\n\nfunc newEngine(\n\tctx context.Context,\n\tbs bstore.Blockstore,\n\tpeerTagger PeerTagger,\n\tself peer.ID,\n\tmaxReplaceSize int,\n\topts ...Option,\n) *Engine {\n\te := &Engine{\n\t\tscoreLedger:                     NewDefaultScoreLedger(),\n\t\tbstoreWorkerCount:               defaults.BitswapEngineBlockstoreWorkerCount,\n\t\tmaxOutstandingBytesPerPeer:      defaults.BitswapMaxOutstandingBytesPerPeer,\n\t\tpeerTagger:                      peerTagger,\n\t\toutbox:                          make(chan (<-chan *Envelope), outboxChanBuffer),\n\t\tworkSignal:                      make(chan struct{}, 1),\n\t\tticker:                          time.NewTicker(time.Millisecond * 100),\n\t\tmaxBlockSizeReplaceHasWithBlock: maxReplaceSize,\n\t\ttaskWorkerCount:                 defaults.BitswapEngineTaskWorkerCount,\n\t\tsendDontHaves:                   true,\n\t\tself:                            self,\n\t\tpeerLedger:                      newPeerLedger(),\n\t\tpendingGauge:                    bmetrics.PendingEngineGauge(ctx),\n\t\tactiveGauge:                     bmetrics.ActiveEngineGauge(ctx),\n\t\ttargetMessageSize:               defaultTargetMessageSize,\n\t\ttagQueued:                       fmt.Sprintf(tagFormat, \"queued\", uuid.New().String()),\n\t\ttagUseful:                       fmt.Sprintf(tagFormat, \"useful\", uuid.New().String()),\n\t\tmaxQueuedWantlistEntriesPerPeer: defaults.MaxQueuedWantlistEntiresPerPeer,\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(e)\n\t}\n\n\te.bsm = newBlockstoreManager(bs, e.bstoreWorkerCount, bmetrics.PendingBlocksGauge(ctx), bmetrics.ActiveBlocksGauge(ctx))\n\n\t// default peer task queue options\n\tpeerTaskQueueOpts := []peertaskqueue.Option{\n\t\tpeertaskqueue.OnPeerAddedHook(e.onPeerAdded),\n\t\tpeertaskqueue.OnPeerRemovedHook(e.onPeerRemoved),\n\t\tpeertaskqueue.TaskMerger(newTaskMerger()),\n\t\tpeertaskqueue.IgnoreFreezing(true),\n\t\tpeertaskqueue.MaxOutstandingWorkPerPeer(e.maxOutstandingBytesPerPeer),\n\t}\n\n\tif e.taskComparator != nil {\n\t\tqueueTaskComparator := wrapTaskComparator(e.taskComparator)\n\t\tpeerTaskQueueOpts = append(peerTaskQueueOpts, peertaskqueue.PeerComparator(peertracker.TaskPriorityPeerComparator(queueTaskComparator)))\n\t\tpeerTaskQueueOpts = append(peerTaskQueueOpts, peertaskqueue.TaskComparator(queueTaskComparator))\n\t}\n\n\te.peerRequestQueue = peertaskqueue.New(peerTaskQueueOpts...)\n\n\treturn e\n}\n\nfunc (e *Engine) updateMetrics() {\n\te.metricsLock.Lock()\n\tc := e.metricUpdateCounter\n\te.metricUpdateCounter++\n\te.metricsLock.Unlock()\n\n\tif c%100 == 0 {\n\t\tstats := e.peerRequestQueue.Stats()\n\t\te.activeGauge.Set(float64(stats.NumActive))\n\t\te.pendingGauge.Set(float64(stats.NumPending))\n\t}\n}\n\n// SetSendDontHaves indicates what to do when the engine receives a want-block\n// for a block that is not in the blockstore. Either\n// - Send a DONT_HAVE message\n// - Simply don't respond\n// Older versions of Bitswap did not respond, so this allows us to simulate\n// those older versions for testing.\nfunc (e *Engine) SetSendDontHaves(send bool) {\n\te.sendDontHaves = send\n}\n\n// Starts the score ledger. Before start the function checks and,\n// if it is unset, initializes the scoreLedger with the default\n// implementation.\nfunc (e *Engine) startScoreLedger(px process.Process) {\n\te.scoreLedger.Start(func(p peer.ID, score int) {\n\t\tif score == 0 {\n\t\t\te.peerTagger.UntagPeer(p, e.tagUseful)\n\t\t} else {\n\t\t\te.peerTagger.TagPeer(p, e.tagUseful, score)\n\t\t}\n\t})\n\tpx.Go(func(ppx process.Process) {\n\t\t<-ppx.Closing()\n\t\te.scoreLedger.Stop()\n\t})\n}\n\nfunc (e *Engine) startBlockstoreManager(px process.Process) {\n\te.bsm.start()\n\tpx.Go(func(ppx process.Process) {\n\t\t<-ppx.Closing()\n\t\te.bsm.stop()\n\t})\n}\n\n// Start up workers to handle requests from other nodes for the data on this node\nfunc (e *Engine) StartWorkers(ctx context.Context, px process.Process) {\n\te.startBlockstoreManager(px)\n\te.startScoreLedger(px)\n\n\te.taskWorkerLock.Lock()\n\tdefer e.taskWorkerLock.Unlock()\n\n\tfor i := 0; i < e.taskWorkerCount; i++ {\n\t\tpx.Go(func(_ process.Process) {\n\t\t\te.taskWorker(ctx)\n\t\t})\n\t}\n\n}\n\nfunc (e *Engine) onPeerAdded(p peer.ID) {\n\te.peerTagger.TagPeer(p, e.tagQueued, queuedTagWeight)\n}\n\nfunc (e *Engine) onPeerRemoved(p peer.ID) {\n\te.peerTagger.UntagPeer(p, e.tagQueued)\n}\n\n// WantlistForPeer returns the list of keys that the given peer has asked for\nfunc (e *Engine) WantlistForPeer(p peer.ID) []wl.Entry {\n\te.lock.RLock()\n\tdefer e.lock.RUnlock()\n\n\treturn e.peerLedger.WantlistForPeer(p)\n}\n\n// LedgerForPeer returns aggregated data communication with a given peer.\nfunc (e *Engine) LedgerForPeer(p peer.ID) *Receipt {\n\treturn e.scoreLedger.GetReceipt(p)\n}\n\n// Each taskWorker pulls items off the request queue up to the maximum size\n// and adds them to an envelope that is passed off to the bitswap workers,\n// which send the message to the network.\nfunc (e *Engine) taskWorker(ctx context.Context) {\n\tdefer e.taskWorkerExit()\n\tfor {\n\t\toneTimeUse := make(chan *Envelope, 1) // buffer to prevent blocking\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase e.outbox <- oneTimeUse:\n\t\t}\n\t\t// receiver is ready for an outoing envelope. let's prepare one. first,\n\t\t// we must acquire a task from the PQ...\n\t\tenvelope, err := e.nextEnvelope(ctx)\n\t\tif err != nil {\n\t\t\tclose(oneTimeUse)\n\t\t\treturn // ctx cancelled\n\t\t}\n\t\toneTimeUse <- envelope // buffered. won't block\n\t\tclose(oneTimeUse)\n\t}\n}\n\n// taskWorkerExit handles cleanup of task workers\nfunc (e *Engine) taskWorkerExit() {\n\te.taskWorkerLock.Lock()\n\tdefer e.taskWorkerLock.Unlock()\n\n\te.taskWorkerCount--\n\tif e.taskWorkerCount == 0 {\n\t\tclose(e.outbox)\n\t}\n}\n\n// nextEnvelope runs in the taskWorker goroutine. Returns an error if the\n// context is cancelled before the next Envelope can be created.\nfunc (e *Engine) nextEnvelope(ctx context.Context) (*Envelope, error) {\n\tfor {\n\t\t// Pop some tasks off the request queue\n\t\tp, nextTasks, pendingBytes := e.peerRequestQueue.PopTasks(e.targetMessageSize)\n\t\te.updateMetrics()\n\t\tfor len(nextTasks) == 0 {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn nil, ctx.Err()\n\t\t\tcase <-e.workSignal:\n\t\t\t\tp, nextTasks, pendingBytes = e.peerRequestQueue.PopTasks(e.targetMessageSize)\n\t\t\t\te.updateMetrics()\n\t\t\tcase <-e.ticker.C:\n\t\t\t\t// When a task is cancelled, the queue may be \"frozen\" for a\n\t\t\t\t// period of time. We periodically \"thaw\" the queue to make\n\t\t\t\t// sure it doesn't get stuck in a frozen state.\n\t\t\t\te.peerRequestQueue.ThawRound()\n\t\t\t\tp, nextTasks, pendingBytes = e.peerRequestQueue.PopTasks(e.targetMessageSize)\n\t\t\t\te.updateMetrics()\n\t\t\t}\n\t\t}\n\n\t\t// Create a new message\n\t\tmsg := bsmsg.New(false)\n\n\t\tlog.Debugw(\"Bitswap process tasks\", \"local\", e.self, \"taskCount\", len(nextTasks))\n\n\t\t// Amount of data in the request queue still waiting to be popped\n\t\tmsg.SetPendingBytes(int32(pendingBytes))\n\n\t\t// Split out want-blocks, want-haves and DONT_HAVEs\n\t\tblockCids := make([]cid.Cid, 0, len(nextTasks))\n\t\tblockTasks := make(map[cid.Cid]*taskData, len(nextTasks))\n\t\tfor _, t := range nextTasks {\n\t\t\tc := t.Topic.(cid.Cid)\n\t\t\ttd := t.Data.(*taskData)\n\t\t\tif td.HaveBlock {\n\t\t\t\tif td.IsWantBlock {\n\t\t\t\t\tblockCids = append(blockCids, c)\n\t\t\t\t\tblockTasks[c] = td\n\t\t\t\t} else {\n\t\t\t\t\t// Add HAVES to the message\n\t\t\t\t\tmsg.AddHave(c)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Add DONT_HAVEs to the message\n\t\t\t\tmsg.AddDontHave(c)\n\t\t\t}\n\t\t}\n\n\t\t// Fetch blocks from datastore\n\t\tblks, err := e.bsm.getBlocks(ctx, blockCids)\n\t\tif err != nil {\n\t\t\t// we're dropping the envelope but that's not an issue in practice.\n\t\t\treturn nil, err\n\t\t}\n\n\t\tfor c, t := range blockTasks {\n\t\t\tblk := blks[c]\n\t\t\t// If the block was not found (it has been removed)\n\t\t\tif blk == nil {\n\t\t\t\t// If the client requested DONT_HAVE, add DONT_HAVE to the message\n\t\t\t\tif t.SendDontHave {\n\t\t\t\t\tmsg.AddDontHave(c)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Add the block to the message\n\t\t\t\t// log.Debugf(\"  make evlp %s->%s block: %s (%d bytes)\", e.self, p, c, len(blk.RawData()))\n\t\t\t\tmsg.AddBlock(blk)\n\t\t\t}\n\t\t}\n\n\t\t// If there's nothing in the message, bail out\n\t\tif msg.Empty() {\n\t\t\te.peerRequestQueue.TasksDone(p, nextTasks...)\n\t\t\tcontinue\n\t\t}\n\n\t\tlog.Debugw(\"Bitswap engine -> msg\", \"local\", e.self, \"to\", p, \"blockCount\", len(msg.Blocks()), \"presenceCount\", len(msg.BlockPresences()), \"size\", msg.Size())\n\t\treturn &Envelope{\n\t\t\tPeer:    p,\n\t\t\tMessage: msg,\n\t\t\tSent: func() {\n\t\t\t\t// Once the message has been sent, signal the request queue so\n\t\t\t\t// it can be cleared from the queue\n\t\t\t\te.peerRequestQueue.TasksDone(p, nextTasks...)\n\n\t\t\t\t// Signal the worker to check for more work\n\t\t\t\te.signalNewWork()\n\t\t\t},\n\t\t}, nil\n\t}\n}\n\n// Outbox returns a channel of one-time use Envelope channels.\nfunc (e *Engine) Outbox() <-chan (<-chan *Envelope) {\n\treturn e.outbox\n}\n\n// Peers returns a slice of Peers with whom the local node has active sessions.\nfunc (e *Engine) Peers() []peer.ID {\n\te.lock.RLock()\n\tdefer e.lock.RUnlock()\n\n\treturn e.peerLedger.CollectPeerIDs()\n}\n\n// MessageReceived is called when a message is received from a remote peer.\n// For each item in the wantlist, add a want-have or want-block entry to the\n// request queue (this is later popped off by the workerTasks)\nfunc (e *Engine) MessageReceived(ctx context.Context, p peer.ID, m bsmsg.BitSwapMessage) {\n\tentries := m.Wantlist()\n\n\tif len(entries) > 0 {\n\t\tlog.Debugw(\"Bitswap engine <- msg\", \"local\", e.self, \"from\", p, \"entryCount\", len(entries))\n\t\tfor _, et := range entries {\n\t\t\tif !et.Cancel {\n\t\t\t\tif et.WantType == pb.Message_Wantlist_Have {\n\t\t\t\t\tlog.Debugw(\"Bitswap engine <- want-have\", \"local\", e.self, \"from\", p, \"cid\", et.Cid)\n\t\t\t\t} else {\n\t\t\t\t\tlog.Debugw(\"Bitswap engine <- want-block\", \"local\", e.self, \"from\", p, \"cid\", et.Cid)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif m.Empty() {\n\t\tlog.Infof(\"received empty message from %s\", p)\n\t}\n\n\tnewWorkExists := false\n\tdefer func() {\n\t\tif newWorkExists {\n\t\t\te.signalNewWork()\n\t\t}\n\t}()\n\n\t// Dispatch entries\n\twants, cancels := e.splitWantsCancels(entries)\n\twants, denials := e.splitWantsDenials(p, wants)\n\n\t// Get block sizes\n\twantKs := cid.NewSet()\n\tfor _, entry := range wants {\n\t\twantKs.Add(entry.Cid)\n\t}\n\tblockSizes, err := e.bsm.getBlockSizes(ctx, wantKs.Keys())\n\tif err != nil {\n\t\tlog.Info(\"aborting message processing\", err)\n\t\treturn\n\t}\n\n\te.lock.Lock()\n\n\tif m.Full() {\n\t\te.peerLedger.ClearPeerWantlist(p)\n\t}\n\n\ts := uint(e.peerLedger.WantlistSizeForPeer(p))\n\tif wouldBe := s + uint(len(wants)); wouldBe > e.maxQueuedWantlistEntriesPerPeer {\n\t\tlog.Debugw(\"wantlist overflow\", \"local\", e.self, \"remote\", p, \"would be\", wouldBe)\n\t\t// truncate wantlist to avoid overflow\n\t\tavailable, o := bits.Sub(e.maxQueuedWantlistEntriesPerPeer, s, 0)\n\t\tif o != 0 {\n\t\t\tavailable = 0\n\t\t}\n\t\twants = wants[:available]\n\t}\n\n\tfor _, entry := range wants {\n\t\te.peerLedger.Wants(p, entry.Entry)\n\t}\n\tfor _, entry := range cancels {\n\t\tlog.Debugw(\"Bitswap engine <- cancel\", \"local\", e.self, \"from\", p, \"cid\", entry.Cid)\n\t\tif e.peerLedger.CancelWant(p, entry.Cid) {\n\t\t\te.peerRequestQueue.Remove(entry.Cid, p)\n\t\t}\n\t}\n\te.lock.Unlock()\n\n\tvar activeEntries []peertask.Task\n\n\t// Cancel a block operation\n\tsendDontHave := func(entry bsmsg.Entry) {\n\t\t// Only add the task to the queue if the requester wants a DONT_HAVE\n\t\tif e.sendDontHaves && entry.SendDontHave {\n\t\t\tc := entry.Cid\n\n\t\t\tnewWorkExists = true\n\t\t\tisWantBlock := false\n\t\t\tif entry.WantType == pb.Message_Wantlist_Block {\n\t\t\t\tisWantBlock = true\n\t\t\t}\n\n\t\t\tactiveEntries = append(activeEntries, peertask.Task{\n\t\t\t\tTopic:    c,\n\t\t\t\tPriority: int(entry.Priority),\n\t\t\t\tWork:     bsmsg.BlockPresenceSize(c),\n\t\t\t\tData: &taskData{\n\t\t\t\t\tBlockSize:    0,\n\t\t\t\t\tHaveBlock:    false,\n\t\t\t\t\tIsWantBlock:  isWantBlock,\n\t\t\t\t\tSendDontHave: entry.SendDontHave,\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\t}\n\n\t// Deny access to blocks\n\tfor _, entry := range denials {\n\t\tlog.Debugw(\"Bitswap engine: block denied access\", \"local\", e.self, \"from\", p, \"cid\", entry.Cid, \"sendDontHave\", entry.SendDontHave)\n\t\tsendDontHave(entry)\n\t}\n\n\t// For each want-have / want-block\n\tfor _, entry := range wants {\n\t\tc := entry.Cid\n\t\tblockSize, found := blockSizes[entry.Cid]\n\n\t\t// If the block was not found\n\t\tif !found {\n\t\t\tlog.Debugw(\"Bitswap engine: block not found\", \"local\", e.self, \"from\", p, \"cid\", entry.Cid, \"sendDontHave\", entry.SendDontHave)\n\t\t\tsendDontHave(entry)\n\t\t} else {\n\t\t\t// The block was found, add it to the queue\n\t\t\tnewWorkExists = true\n\n\t\t\tisWantBlock := e.sendAsBlock(entry.WantType, blockSize)\n\n\t\t\tlog.Debugw(\"Bitswap engine: block found\", \"local\", e.self, \"from\", p, \"cid\", entry.Cid, \"isWantBlock\", isWantBlock)\n\n\t\t\t// entrySize is the amount of space the entry takes up in the\n\t\t\t// message we send to the recipient. If we're sending a block, the\n\t\t\t// entrySize is the size of the block. Otherwise it's the size of\n\t\t\t// a block presence entry.\n\t\t\tentrySize := blockSize\n\t\t\tif !isWantBlock {\n\t\t\t\tentrySize = bsmsg.BlockPresenceSize(c)\n\t\t\t}\n\t\t\tactiveEntries = append(activeEntries, peertask.Task{\n\t\t\t\tTopic:    c,\n\t\t\t\tPriority: int(entry.Priority),\n\t\t\t\tWork:     entrySize,\n\t\t\t\tData: &taskData{\n\t\t\t\t\tBlockSize:    blockSize,\n\t\t\t\t\tHaveBlock:    true,\n\t\t\t\t\tIsWantBlock:  isWantBlock,\n\t\t\t\t\tSendDontHave: entry.SendDontHave,\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\t}\n\n\t// Push entries onto the request queue\n\tif len(activeEntries) > 0 {\n\t\te.peerRequestQueue.PushTasksTruncated(e.maxQueuedWantlistEntriesPerPeer, p, activeEntries...)\n\t\te.updateMetrics()\n\t}\n}\n\n// Split the want-have / want-block entries from the cancel entries\nfunc (e *Engine) splitWantsCancels(es []bsmsg.Entry) ([]bsmsg.Entry, []bsmsg.Entry) {\n\twants := make([]bsmsg.Entry, 0, len(es))\n\tcancels := make([]bsmsg.Entry, 0, len(es))\n\tfor _, et := range es {\n\t\tif et.Cancel {\n\t\t\tcancels = append(cancels, et)\n\t\t} else {\n\t\t\twants = append(wants, et)\n\t\t}\n\t}\n\treturn wants, cancels\n}\n\n// Split the want-have / want-block entries from the block that will be denied access\nfunc (e *Engine) splitWantsDenials(p peer.ID, allWants []bsmsg.Entry) ([]bsmsg.Entry, []bsmsg.Entry) {\n\tif e.peerBlockRequestFilter == nil {\n\t\treturn allWants, nil\n\t}\n\n\twants := make([]bsmsg.Entry, 0, len(allWants))\n\tdenied := make([]bsmsg.Entry, 0, len(allWants))\n\n\tfor _, et := range allWants {\n\t\tif e.peerBlockRequestFilter(p, et.Cid) {\n\t\t\twants = append(wants, et)\n\t\t} else {\n\t\t\tdenied = append(denied, et)\n\t\t}\n\t}\n\n\treturn wants, denied\n}\n\n// ReceivedBlocks is called when new blocks are received from the network.\n// This function also updates the receive side of the ledger.\nfunc (e *Engine) ReceivedBlocks(from peer.ID, blks []blocks.Block) {\n\tif len(blks) == 0 {\n\t\treturn\n\t}\n\n\t// Record how many bytes were received in the ledger\n\tfor _, blk := range blks {\n\t\tlog.Debugw(\"Bitswap engine <- block\", \"local\", e.self, \"from\", from, \"cid\", blk.Cid(), \"size\", len(blk.RawData()))\n\t\te.scoreLedger.AddToReceivedBytes(from, len(blk.RawData()))\n\t}\n}\n\n// NotifyNewBlocks is called when new blocks becomes available locally, and in particular when the caller of bitswap\n// decide to store those blocks and make them available on the network.\nfunc (e *Engine) NotifyNewBlocks(blks []blocks.Block) {\n\tif len(blks) == 0 {\n\t\treturn\n\t}\n\n\t// Get the size of each block\n\tblockSizes := make(map[cid.Cid]int, len(blks))\n\tfor _, blk := range blks {\n\t\tblockSizes[blk.Cid()] = len(blk.RawData())\n\t}\n\n\t// Check each peer to see if it wants one of the blocks we received\n\tvar work bool\n\tfor _, b := range blks {\n\t\tk := b.Cid()\n\n\t\te.lock.RLock()\n\t\tpeers := e.peerLedger.Peers(k)\n\t\te.lock.RUnlock()\n\n\t\tfor _, entry := range peers {\n\t\t\twork = true\n\n\t\t\tblockSize := blockSizes[k]\n\t\t\tisWantBlock := e.sendAsBlock(entry.WantType, blockSize)\n\n\t\t\tentrySize := blockSize\n\t\t\tif !isWantBlock {\n\t\t\t\tentrySize = bsmsg.BlockPresenceSize(k)\n\t\t\t}\n\n\t\t\te.peerRequestQueue.PushTasksTruncated(e.maxQueuedWantlistEntriesPerPeer, entry.Peer, peertask.Task{\n\t\t\t\tTopic:    k,\n\t\t\t\tPriority: int(entry.Priority),\n\t\t\t\tWork:     entrySize,\n\t\t\t\tData: &taskData{\n\t\t\t\t\tBlockSize:    blockSize,\n\t\t\t\t\tHaveBlock:    true,\n\t\t\t\t\tIsWantBlock:  isWantBlock,\n\t\t\t\t\tSendDontHave: false,\n\t\t\t\t},\n\t\t\t})\n\t\t\te.updateMetrics()\n\t\t}\n\t}\n\n\tif work {\n\t\te.signalNewWork()\n\t}\n}\n\n// TODO add contents of m.WantList() to my local wantlist? NB: could introduce\n// race conditions where I send a message, but MessageSent gets handled after\n// MessageReceived. The information in the local wantlist could become\n// inconsistent. Would need to ensure that Sends and acknowledgement of the\n// send happen atomically\n\n// MessageSent is called when a message has successfully been sent out, to record\n// changes.\nfunc (e *Engine) MessageSent(p peer.ID, m bsmsg.BitSwapMessage) {\n\te.lock.Lock()\n\tdefer e.lock.Unlock()\n\n\t// Remove sent blocks from the want list for the peer\n\tfor _, block := range m.Blocks() {\n\t\te.scoreLedger.AddToSentBytes(p, len(block.RawData()))\n\t\te.peerLedger.CancelWantWithType(p, block.Cid(), pb.Message_Wantlist_Block)\n\t}\n\n\t// Remove sent block presences from the want list for the peer\n\tfor _, bp := range m.BlockPresences() {\n\t\t// Don't record sent data. We reserve that for data blocks.\n\t\tif bp.Type == pb.Message_Have {\n\t\t\te.peerLedger.CancelWantWithType(p, bp.Cid, pb.Message_Wantlist_Have)\n\t\t}\n\t}\n}\n\n// PeerConnected is called when a new peer connects, meaning we should start\n// sending blocks.\nfunc (e *Engine) PeerConnected(p peer.ID) {\n\te.lock.Lock()\n\tdefer e.lock.Unlock()\n\n\te.scoreLedger.PeerConnected(p)\n}\n\n// PeerDisconnected is called when a peer disconnects.\nfunc (e *Engine) PeerDisconnected(p peer.ID) {\n\te.peerRequestQueue.Clear(p)\n\n\te.lock.Lock()\n\tdefer e.lock.Unlock()\n\n\te.peerLedger.PeerDisconnected(p)\n\te.scoreLedger.PeerDisconnected(p)\n}\n\n// If the want is a want-have, and it's below a certain size, send the full\n// block (instead of sending a HAVE)\nfunc (e *Engine) sendAsBlock(wantType pb.Message_Wantlist_WantType, blockSize int) bool {\n\tisWantBlock := wantType == pb.Message_Wantlist_Block\n\treturn isWantBlock || blockSize <= e.maxBlockSizeReplaceHasWithBlock\n}\n\nfunc (e *Engine) numBytesSentTo(p peer.ID) uint64 {\n\treturn e.LedgerForPeer(p).Sent\n}\n\nfunc (e *Engine) numBytesReceivedFrom(p peer.ID) uint64 {\n\treturn e.LedgerForPeer(p).Recv\n}\n\nfunc (e *Engine) signalNewWork() {\n\t// Signal task generation to restart (if stopped!)\n\tselect {\n\tcase e.workSignal <- struct{}{}:\n\tdefault:\n\t}\n}\n", "package server\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ipfs/go-cid\"\n\tblockstore \"github.com/ipfs/go-ipfs-blockstore\"\n\t\"github.com/ipfs/go-libipfs/bitswap/internal/defaults\"\n\t\"github.com/ipfs/go-libipfs/bitswap/message\"\n\tpb \"github.com/ipfs/go-libipfs/bitswap/message/pb\"\n\tbmetrics \"github.com/ipfs/go-libipfs/bitswap/metrics\"\n\tbsnet \"github.com/ipfs/go-libipfs/bitswap/network\"\n\t\"github.com/ipfs/go-libipfs/bitswap/server/internal/decision\"\n\t\"github.com/ipfs/go-libipfs/bitswap/tracer\"\n\tblocks \"github.com/ipfs/go-libipfs/blocks\"\n\tlogging \"github.com/ipfs/go-log\"\n\t\"github.com/ipfs/go-metrics-interface\"\n\tprocess \"github.com/jbenet/goprocess\"\n\tprocctx \"github.com/jbenet/goprocess/context\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\t\"go.uber.org/zap\"\n)\n\nvar provideKeysBufferSize = 2048\n\nvar log = logging.Logger(\"bitswap-server\")\nvar sflog = log.Desugar()\n\nconst provideWorkerMax = 6\n\ntype Option func(*Server)\n\ntype Server struct {\n\tsentHistogram     metrics.Histogram\n\tsendTimeHistogram metrics.Histogram\n\n\t// the engine is the bit of logic that decides who to send which blocks to\n\tengine *decision.Engine\n\n\t// network delivers messages on behalf of the session\n\tnetwork bsnet.BitSwapNetwork\n\n\t// External statistics interface\n\ttracer tracer.Tracer\n\n\t// Counters for various statistics\n\tcounterLk sync.Mutex\n\tcounters  Stat\n\n\t// the total number of simultaneous threads sending outgoing messages\n\ttaskWorkerCount int\n\n\tprocess process.Process\n\n\t// newBlocks is a channel for newly added blocks to be provided to the\n\t// network.  blocks pushed down this channel get buffered and fed to the\n\t// provideKeys channel later on to avoid too much network activity\n\tnewBlocks chan cid.Cid\n\t// provideKeys directly feeds provide workers\n\tprovideKeys chan cid.Cid\n\n\t// Extra options to pass to the decision manager\n\tengineOptions []decision.Option\n\n\t// the size of channel buffer to use\n\thasBlockBufferSize int\n\t// whether or not to make provide announcements\n\tprovideEnabled bool\n}\n\nfunc New(ctx context.Context, network bsnet.BitSwapNetwork, bstore blockstore.Blockstore, options ...Option) *Server {\n\tctx, cancel := context.WithCancel(ctx)\n\n\tpx := process.WithTeardown(func() error {\n\t\treturn nil\n\t})\n\tgo func() {\n\t\t<-px.Closing() // process closes first\n\t\tcancel()\n\t}()\n\n\ts := &Server{\n\t\tsentHistogram:      bmetrics.SentHist(ctx),\n\t\tsendTimeHistogram:  bmetrics.SendTimeHist(ctx),\n\t\ttaskWorkerCount:    defaults.BitswapTaskWorkerCount,\n\t\tnetwork:            network,\n\t\tprocess:            px,\n\t\tprovideEnabled:     true,\n\t\thasBlockBufferSize: defaults.HasBlockBufferSize,\n\t\tprovideKeys:        make(chan cid.Cid, provideKeysBufferSize),\n\t}\n\ts.newBlocks = make(chan cid.Cid, s.hasBlockBufferSize)\n\n\tfor _, o := range options {\n\t\to(s)\n\t}\n\n\ts.engine = decision.NewEngine(\n\t\tctx,\n\t\tbstore,\n\t\tnetwork.ConnectionManager(),\n\t\tnetwork.Self(),\n\t\ts.engineOptions...,\n\t)\n\ts.engineOptions = nil\n\n\ts.startWorkers(ctx, px)\n\n\treturn s\n}\n\nfunc TaskWorkerCount(count int) Option {\n\tif count <= 0 {\n\t\tpanic(fmt.Sprintf(\"task worker count is %d but must be > 0\", count))\n\t}\n\treturn func(bs *Server) {\n\t\tbs.taskWorkerCount = count\n\t}\n}\n\nfunc WithTracer(tap tracer.Tracer) Option {\n\treturn func(bs *Server) {\n\t\tbs.tracer = tap\n\t}\n}\n\n// ProvideEnabled is an option for enabling/disabling provide announcements\nfunc ProvideEnabled(enabled bool) Option {\n\treturn func(bs *Server) {\n\t\tbs.provideEnabled = enabled\n\t}\n}\n\nfunc WithPeerBlockRequestFilter(pbrf decision.PeerBlockRequestFilter) Option {\n\to := decision.WithPeerBlockRequestFilter(pbrf)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// WithTaskComparator configures custom task prioritization logic.\nfunc WithTaskComparator(comparator decision.TaskComparator) Option {\n\to := decision.WithTaskComparator(comparator)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// Configures the engine to use the given score decision logic.\nfunc WithScoreLedger(scoreLedger decision.ScoreLedger) Option {\n\to := decision.WithScoreLedger(scoreLedger)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// LedgerForPeer returns aggregated data about blocks swapped and communication\n// with a given peer.\nfunc (bs *Server) LedgerForPeer(p peer.ID) *decision.Receipt {\n\treturn bs.engine.LedgerForPeer(p)\n}\n\n// EngineTaskWorkerCount sets the number of worker threads used inside the engine\nfunc EngineTaskWorkerCount(count int) Option {\n\to := decision.WithTaskWorkerCount(count)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// SetSendDontHaves indicates what to do when the engine receives a want-block\n// for a block that is not in the blockstore. Either\n// - Send a DONT_HAVE message\n// - Simply don't respond\n// This option is only used for testing.\nfunc SetSendDontHaves(send bool) Option {\n\to := decision.WithSetSendDontHave(send)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// EngineBlockstoreWorkerCount sets the number of worker threads used for\n// blockstore operations in the decision engine\nfunc EngineBlockstoreWorkerCount(count int) Option {\n\to := decision.WithBlockstoreWorkerCount(count)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\nfunc WithTargetMessageSize(tms int) Option {\n\to := decision.WithTargetMessageSize(tms)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// MaxOutstandingBytesPerPeer describes approximately how much work we are will to have outstanding to a peer at any\n// given time. Setting it to 0 will disable any limiting.\nfunc MaxOutstandingBytesPerPeer(count int) Option {\n\to := decision.WithMaxOutstandingBytesPerPeer(count)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// MaxQueuedWantlistEntriesPerPeer limits how much individual entries each peer is allowed to send.\n// If a peer send us more than this we will truncate newest entries.\n// It defaults to defaults.MaxQueuedWantlistEntiresPerPeer.\nfunc MaxQueuedWantlistEntriesPerPeer(count uint) Option {\n\to := decision.WithMaxQueuedWantlistEntriesPerPeer(count)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// HasBlockBufferSize configure how big the new blocks buffer should be.\nfunc HasBlockBufferSize(count int) Option {\n\tif count < 0 {\n\t\tpanic(\"cannot have negative buffer size\")\n\t}\n\treturn func(bs *Server) {\n\t\tbs.hasBlockBufferSize = count\n\t}\n}\n\n// WantlistForPeer returns the currently understood list of blocks requested by a\n// given peer.\nfunc (bs *Server) WantlistForPeer(p peer.ID) []cid.Cid {\n\tvar out []cid.Cid\n\tfor _, e := range bs.engine.WantlistForPeer(p) {\n\t\tout = append(out, e.Cid)\n\t}\n\treturn out\n}\n\nfunc (bs *Server) startWorkers(ctx context.Context, px process.Process) {\n\tbs.engine.StartWorkers(ctx, px)\n\n\t// Start up workers to handle requests from other nodes for the data on this node\n\tfor i := 0; i < bs.taskWorkerCount; i++ {\n\t\ti := i\n\t\tpx.Go(func(px process.Process) {\n\t\t\tbs.taskWorker(ctx, i)\n\t\t})\n\t}\n\n\tif bs.provideEnabled {\n\t\t// Start up a worker to manage sending out provides messages\n\t\tpx.Go(func(px process.Process) {\n\t\t\tbs.provideCollector(ctx)\n\t\t})\n\n\t\t// Spawn up multiple workers to handle incoming blocks\n\t\t// consider increasing number if providing blocks bottlenecks\n\t\t// file transfers\n\t\tpx.Go(bs.provideWorker)\n\t}\n}\n\nfunc (bs *Server) taskWorker(ctx context.Context, id int) {\n\tdefer log.Debug(\"bitswap task worker shutting down...\")\n\tlog := log.With(\"ID\", id)\n\tfor {\n\t\tlog.Debug(\"Bitswap.TaskWorker.Loop\")\n\t\tselect {\n\t\tcase nextEnvelope := <-bs.engine.Outbox():\n\t\t\tselect {\n\t\t\tcase envelope, ok := <-nextEnvelope:\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tstart := time.Now()\n\n\t\t\t\t// TODO: Only record message as sent if there was no error?\n\t\t\t\t// Ideally, yes. But we'd need some way to trigger a retry and/or drop\n\t\t\t\t// the peer.\n\t\t\t\tbs.engine.MessageSent(envelope.Peer, envelope.Message)\n\t\t\t\tif bs.tracer != nil {\n\t\t\t\t\tbs.tracer.MessageSent(envelope.Peer, envelope.Message)\n\t\t\t\t}\n\t\t\t\tbs.sendBlocks(ctx, envelope)\n\n\t\t\t\tdur := time.Since(start)\n\t\t\t\tbs.sendTimeHistogram.Observe(dur.Seconds())\n\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (bs *Server) logOutgoingBlocks(env *decision.Envelope) {\n\tif ce := sflog.Check(zap.DebugLevel, \"sent message\"); ce == nil {\n\t\treturn\n\t}\n\n\tself := bs.network.Self()\n\n\tfor _, blockPresence := range env.Message.BlockPresences() {\n\t\tc := blockPresence.Cid\n\t\tswitch blockPresence.Type {\n\t\tcase pb.Message_Have:\n\t\t\tlog.Debugw(\"sent message\",\n\t\t\t\t\"type\", \"HAVE\",\n\t\t\t\t\"cid\", c,\n\t\t\t\t\"local\", self,\n\t\t\t\t\"to\", env.Peer,\n\t\t\t)\n\t\tcase pb.Message_DontHave:\n\t\t\tlog.Debugw(\"sent message\",\n\t\t\t\t\"type\", \"DONT_HAVE\",\n\t\t\t\t\"cid\", c,\n\t\t\t\t\"local\", self,\n\t\t\t\t\"to\", env.Peer,\n\t\t\t)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unrecognized BlockPresence type %v\", blockPresence.Type))\n\t\t}\n\n\t}\n\tfor _, block := range env.Message.Blocks() {\n\t\tlog.Debugw(\"sent message\",\n\t\t\t\"type\", \"BLOCK\",\n\t\t\t\"cid\", block.Cid(),\n\t\t\t\"local\", self,\n\t\t\t\"to\", env.Peer,\n\t\t)\n\t}\n}\n\nfunc (bs *Server) sendBlocks(ctx context.Context, env *decision.Envelope) {\n\t// Blocks need to be sent synchronously to maintain proper backpressure\n\t// throughout the network stack\n\tdefer env.Sent()\n\n\terr := bs.network.SendMessage(ctx, env.Peer, env.Message)\n\tif err != nil {\n\t\tlog.Debugw(\"failed to send blocks message\",\n\t\t\t\"peer\", env.Peer,\n\t\t\t\"error\", err,\n\t\t)\n\t\treturn\n\t}\n\n\tbs.logOutgoingBlocks(env)\n\n\tdataSent := 0\n\tblocks := env.Message.Blocks()\n\tfor _, b := range blocks {\n\t\tdataSent += len(b.RawData())\n\t}\n\tbs.counterLk.Lock()\n\tbs.counters.BlocksSent += uint64(len(blocks))\n\tbs.counters.DataSent += uint64(dataSent)\n\tbs.counterLk.Unlock()\n\tbs.sentHistogram.Observe(float64(env.Message.Size()))\n\tlog.Debugw(\"sent message\", \"peer\", env.Peer)\n}\n\ntype Stat struct {\n\tPeers         []string\n\tProvideBufLen int\n\tBlocksSent    uint64\n\tDataSent      uint64\n}\n\n// Stat returns aggregated statistics about bitswap operations\nfunc (bs *Server) Stat() (Stat, error) {\n\tbs.counterLk.Lock()\n\ts := bs.counters\n\tbs.counterLk.Unlock()\n\ts.ProvideBufLen = len(bs.newBlocks)\n\n\tpeers := bs.engine.Peers()\n\tpeersStr := make([]string, len(peers))\n\tfor i, p := range peers {\n\t\tpeersStr[i] = p.Pretty()\n\t}\n\tsort.Strings(peersStr)\n\ts.Peers = peersStr\n\n\treturn s, nil\n}\n\n// NotifyNewBlocks announces the existence of blocks to this bitswap service. The\n// service will potentially notify its peers.\n// Bitswap itself doesn't store new blocks. It's the caller responsibility to ensure\n// that those blocks are available in the blockstore before calling this function.\nfunc (bs *Server) NotifyNewBlocks(ctx context.Context, blks ...blocks.Block) error {\n\tselect {\n\tcase <-bs.process.Closing():\n\t\treturn errors.New(\"bitswap is closed\")\n\tdefault:\n\t}\n\n\t// Send wanted blocks to decision engine\n\tbs.engine.NotifyNewBlocks(blks)\n\n\t// If the reprovider is enabled, send block to reprovider\n\tif bs.provideEnabled {\n\t\tfor _, blk := range blks {\n\t\t\tselect {\n\t\t\tcase bs.newBlocks <- blk.Cid():\n\t\t\t\t// send block off to be reprovided\n\t\t\tcase <-bs.process.Closing():\n\t\t\t\treturn bs.process.Close()\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (bs *Server) provideCollector(ctx context.Context) {\n\tdefer close(bs.provideKeys)\n\tvar toProvide []cid.Cid\n\tvar nextKey cid.Cid\n\tvar keysOut chan cid.Cid\n\n\tfor {\n\t\tselect {\n\t\tcase blkey, ok := <-bs.newBlocks:\n\t\t\tif !ok {\n\t\t\t\tlog.Debug(\"newBlocks channel closed\")\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif keysOut == nil {\n\t\t\t\tnextKey = blkey\n\t\t\t\tkeysOut = bs.provideKeys\n\t\t\t} else {\n\t\t\t\ttoProvide = append(toProvide, blkey)\n\t\t\t}\n\t\tcase keysOut <- nextKey:\n\t\t\tif len(toProvide) > 0 {\n\t\t\t\tnextKey = toProvide[0]\n\t\t\t\ttoProvide = toProvide[1:]\n\t\t\t} else {\n\t\t\t\tkeysOut = nil\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (bs *Server) provideWorker(px process.Process) {\n\t// FIXME: OnClosingContext returns a _custom_ context type.\n\t// Unfortunately, deriving a new cancelable context from this custom\n\t// type fires off a goroutine. To work around this, we create a single\n\t// cancelable context up-front and derive all sub-contexts from that.\n\t//\n\t// See: https://github.com/ipfs/go-ipfs/issues/5810\n\tctx := procctx.OnClosingContext(px)\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\n\tlimit := make(chan struct{}, provideWorkerMax)\n\n\tlimitedGoProvide := func(k cid.Cid, wid int) {\n\t\tdefer func() {\n\t\t\t// replace token when done\n\t\t\t<-limit\n\t\t}()\n\n\t\tlog.Debugw(\"Bitswap.ProvideWorker.Start\", \"ID\", wid, \"cid\", k)\n\t\tdefer log.Debugw(\"Bitswap.ProvideWorker.End\", \"ID\", wid, \"cid\", k)\n\n\t\tctx, cancel := context.WithTimeout(ctx, defaults.ProvideTimeout) // timeout ctx\n\t\tdefer cancel()\n\n\t\tif err := bs.network.Provide(ctx, k); err != nil {\n\t\t\tlog.Warn(err)\n\t\t}\n\t}\n\n\t// worker spawner, reads from bs.provideKeys until it closes, spawning a\n\t// _ratelimited_ number of workers to handle each key.\n\tfor wid := 2; ; wid++ {\n\t\tlog.Debug(\"Bitswap.ProvideWorker.Loop\")\n\n\t\tselect {\n\t\tcase <-px.Closing():\n\t\t\treturn\n\t\tcase k, ok := <-bs.provideKeys:\n\t\t\tif !ok {\n\t\t\t\tlog.Debug(\"provideKeys channel closed\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-px.Closing():\n\t\t\t\treturn\n\t\t\tcase limit <- struct{}{}:\n\t\t\t\tgo limitedGoProvide(k, wid)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (bs *Server) ReceiveMessage(ctx context.Context, p peer.ID, incoming message.BitSwapMessage) {\n\t// This call records changes to wantlists, blocks received,\n\t// and number of bytes transfered.\n\tbs.engine.MessageReceived(ctx, p, incoming)\n\t// TODO: this is bad, and could be easily abused.\n\t// Should only track *useful* messages in ledger\n\n\tif bs.tracer != nil {\n\t\tbs.tracer.MessageReceived(p, incoming)\n\t}\n}\n\n// ReceivedBlocks notify the decision engine that a peer is well behaving\n// and gave us usefull data, potentially increasing it's score and making us\n// send them more data in exchange.\nfunc (bs *Server) ReceivedBlocks(from peer.ID, blks []blocks.Block) {\n\tbs.engine.ReceivedBlocks(from, blks)\n}\n\nfunc (*Server) ReceiveError(err error) {\n\tlog.Infof(\"Bitswap Client ReceiveError: %s\", err)\n\t// TODO log the network error\n\t// TODO bubble the network error up to the parent context/error logger\n\n}\nfunc (bs *Server) PeerConnected(p peer.ID) {\n\tbs.engine.PeerConnected(p)\n}\nfunc (bs *Server) PeerDisconnected(p peer.ID) {\n\tbs.engine.PeerDisconnected(p)\n}\n\n// Close is called to shutdown the Client\nfunc (bs *Server) Close() error {\n\treturn bs.process.Close()\n}\n"], "fixing_code": ["package defaults\n\nimport (\n\t\"encoding/binary\"\n\t\"time\"\n)\n\nconst (\n\t// these requests take at _least_ two minutes at the moment.\n\tProvideTimeout  = time.Minute * 3\n\tProvSearchDelay = time.Second\n\n\t// Number of concurrent workers in decision engine that process requests to the blockstore\n\tBitswapEngineBlockstoreWorkerCount = 128\n\t// the total number of simultaneous threads sending outgoing messages\n\tBitswapTaskWorkerCount = 8\n\t// how many worker threads to start for decision engine task worker\n\tBitswapEngineTaskWorkerCount = 8\n\t// the total amount of bytes that a peer should have outstanding, it is utilized by the decision engine\n\tBitswapMaxOutstandingBytesPerPeer = 1 << 20\n\t// the number of bytes we attempt to make each outgoing bitswap message\n\tBitswapEngineTargetMessageSize = 16 * 1024\n\t// HasBlockBufferSize is the buffer size of the channel for new blocks\n\t// that need to be provided. They should get pulled over by the\n\t// provideCollector even before they are actually provided.\n\t// TODO: Does this need to be this large givent that?\n\tHasBlockBufferSize = 256\n\n\t// Maximum size of the wantlist we are willing to keep in memory.\n\tMaxQueuedWantlistEntiresPerPeer = 1024\n\n\t// Copied from github.com/ipfs/go-verifcid#maximumHashLength\n\t// FIXME: expose this in go-verifcid.\n\tMaximumHashLength = 128\n\tMaximumAllowedCid = binary.MaxVarintLen64*4 + MaximumHashLength\n)\n", "package network\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\tbsmsg \"github.com/ipfs/go-libipfs/bitswap/message\"\n\t\"github.com/ipfs/go-libipfs/bitswap/network/internal\"\n\n\tcid \"github.com/ipfs/go-cid\"\n\tlogging \"github.com/ipfs/go-log\"\n\t\"github.com/libp2p/go-libp2p/core/connmgr\"\n\t\"github.com/libp2p/go-libp2p/core/host\"\n\t\"github.com/libp2p/go-libp2p/core/network\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\tpeerstore \"github.com/libp2p/go-libp2p/core/peerstore\"\n\t\"github.com/libp2p/go-libp2p/core/protocol\"\n\t\"github.com/libp2p/go-libp2p/core/routing\"\n\t\"github.com/libp2p/go-libp2p/p2p/protocol/ping\"\n\tmsgio \"github.com/libp2p/go-msgio\"\n\tma \"github.com/multiformats/go-multiaddr\"\n\t\"github.com/multiformats/go-multistream\"\n)\n\nvar log = logging.Logger(\"bitswap_network\")\n\nvar connectTimeout = time.Second * 5\n\nvar maxSendTimeout = 2 * time.Minute\nvar minSendTimeout = 10 * time.Second\nvar sendLatency = 2 * time.Second\nvar minSendRate = (100 * 1000) / 8 // 100kbit/s\n\n// NewFromIpfsHost returns a BitSwapNetwork supported by underlying IPFS host.\nfunc NewFromIpfsHost(host host.Host, r routing.ContentRouting, opts ...NetOpt) BitSwapNetwork {\n\ts := processSettings(opts...)\n\n\tbitswapNetwork := impl{\n\t\thost:    host,\n\t\trouting: r,\n\n\t\tprotocolBitswapNoVers:  s.ProtocolPrefix + ProtocolBitswapNoVers,\n\t\tprotocolBitswapOneZero: s.ProtocolPrefix + ProtocolBitswapOneZero,\n\t\tprotocolBitswapOneOne:  s.ProtocolPrefix + ProtocolBitswapOneOne,\n\t\tprotocolBitswap:        s.ProtocolPrefix + ProtocolBitswap,\n\n\t\tsupportedProtocols: s.SupportedProtocols,\n\t}\n\n\treturn &bitswapNetwork\n}\n\nfunc processSettings(opts ...NetOpt) Settings {\n\ts := Settings{SupportedProtocols: append([]protocol.ID(nil), internal.DefaultProtocols...)}\n\tfor _, opt := range opts {\n\t\topt(&s)\n\t}\n\tfor i, proto := range s.SupportedProtocols {\n\t\ts.SupportedProtocols[i] = s.ProtocolPrefix + proto\n\t}\n\treturn s\n}\n\n// impl transforms the ipfs network interface, which sends and receives\n// NetMessage objects, into the bitswap network interface.\ntype impl struct {\n\t// NOTE: Stats must be at the top of the heap allocation to ensure 64bit\n\t// alignment.\n\tstats Stats\n\n\thost          host.Host\n\trouting       routing.ContentRouting\n\tconnectEvtMgr *connectEventManager\n\n\tprotocolBitswapNoVers  protocol.ID\n\tprotocolBitswapOneZero protocol.ID\n\tprotocolBitswapOneOne  protocol.ID\n\tprotocolBitswap        protocol.ID\n\n\tsupportedProtocols []protocol.ID\n\n\t// inbound messages from the network are forwarded to the receiver\n\treceivers []Receiver\n}\n\ntype streamMessageSender struct {\n\tto        peer.ID\n\tstream    network.Stream\n\tconnected bool\n\tbsnet     *impl\n\topts      *MessageSenderOpts\n}\n\n// Open a stream to the remote peer\nfunc (s *streamMessageSender) Connect(ctx context.Context) (network.Stream, error) {\n\tif s.connected {\n\t\treturn s.stream, nil\n\t}\n\n\ttctx, cancel := context.WithTimeout(ctx, s.opts.SendTimeout)\n\tdefer cancel()\n\n\tif err := s.bsnet.ConnectTo(tctx, s.to); err != nil {\n\t\treturn nil, err\n\t}\n\n\tstream, err := s.bsnet.newStreamToPeer(tctx, s.to)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.stream = stream\n\ts.connected = true\n\treturn s.stream, nil\n}\n\n// Reset the stream\nfunc (s *streamMessageSender) Reset() error {\n\tif s.stream != nil {\n\t\terr := s.stream.Reset()\n\t\ts.connected = false\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Close the stream\nfunc (s *streamMessageSender) Close() error {\n\treturn s.stream.Close()\n}\n\n// Indicates whether the peer supports HAVE / DONT_HAVE messages\nfunc (s *streamMessageSender) SupportsHave() bool {\n\treturn s.bsnet.SupportsHave(s.stream.Protocol())\n}\n\n// Send a message to the peer, attempting multiple times\nfunc (s *streamMessageSender) SendMsg(ctx context.Context, msg bsmsg.BitSwapMessage) error {\n\treturn s.multiAttempt(ctx, func() error {\n\t\treturn s.send(ctx, msg)\n\t})\n}\n\n// Perform a function with multiple attempts, and a timeout\nfunc (s *streamMessageSender) multiAttempt(ctx context.Context, fn func() error) error {\n\t// Try to call the function repeatedly\n\tvar err error\n\tfor i := 0; i < s.opts.MaxRetries; i++ {\n\t\tif err = fn(); err == nil {\n\t\t\t// Attempt was successful\n\t\t\treturn nil\n\t\t}\n\n\t\t// Attempt failed\n\n\t\t// If the sender has been closed or the context cancelled, just bail out\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t}\n\n\t\t// Protocol is not supported, so no need to try multiple times\n\t\tif errors.Is(err, multistream.ErrNotSupported[protocol.ID]{}) {\n\t\t\ts.bsnet.connectEvtMgr.MarkUnresponsive(s.to)\n\t\t\treturn err\n\t\t}\n\n\t\t// Failed to send so reset stream and try again\n\t\t_ = s.Reset()\n\n\t\t// Failed too many times so mark the peer as unresponsive and return an error\n\t\tif i == s.opts.MaxRetries-1 {\n\t\t\ts.bsnet.connectEvtMgr.MarkUnresponsive(s.to)\n\t\t\treturn err\n\t\t}\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase <-time.After(s.opts.SendErrorBackoff):\n\t\t\t// wait a short time in case disconnect notifications are still propagating\n\t\t\tlog.Infof(\"send message to %s failed but context was not Done: %s\", s.to, err)\n\t\t}\n\t}\n\treturn err\n}\n\n// Send a message to the peer\nfunc (s *streamMessageSender) send(ctx context.Context, msg bsmsg.BitSwapMessage) error {\n\tstart := time.Now()\n\tstream, err := s.Connect(ctx)\n\tif err != nil {\n\t\tlog.Infof(\"failed to open stream to %s: %s\", s.to, err)\n\t\treturn err\n\t}\n\n\t// The send timeout includes the time required to connect\n\t// (although usually we will already have connected - we only need to\n\t// connect after a failed attempt to send)\n\ttimeout := s.opts.SendTimeout - time.Since(start)\n\tif err = s.bsnet.msgToStream(ctx, stream, msg, timeout); err != nil {\n\t\tlog.Infof(\"failed to send message to %s: %s\", s.to, err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (bsnet *impl) Self() peer.ID {\n\treturn bsnet.host.ID()\n}\n\nfunc (bsnet *impl) Ping(ctx context.Context, p peer.ID) ping.Result {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tres := <-ping.Ping(ctx, bsnet.host, p)\n\treturn res\n}\n\nfunc (bsnet *impl) Latency(p peer.ID) time.Duration {\n\treturn bsnet.host.Peerstore().LatencyEWMA(p)\n}\n\n// Indicates whether the given protocol supports HAVE / DONT_HAVE messages\nfunc (bsnet *impl) SupportsHave(proto protocol.ID) bool {\n\tswitch proto {\n\tcase bsnet.protocolBitswapOneOne, bsnet.protocolBitswapOneZero, bsnet.protocolBitswapNoVers:\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc (bsnet *impl) msgToStream(ctx context.Context, s network.Stream, msg bsmsg.BitSwapMessage, timeout time.Duration) error {\n\tdeadline := time.Now().Add(timeout)\n\tif dl, ok := ctx.Deadline(); ok && dl.Before(deadline) {\n\t\tdeadline = dl\n\t}\n\n\tif err := s.SetWriteDeadline(deadline); err != nil {\n\t\tlog.Warnf(\"error setting deadline: %s\", err)\n\t}\n\n\t// Older Bitswap versions use a slightly different wire format so we need\n\t// to convert the message to the appropriate format depending on the remote\n\t// peer's Bitswap version.\n\tswitch s.Protocol() {\n\tcase bsnet.protocolBitswapOneOne, bsnet.protocolBitswap:\n\t\tif err := msg.ToNetV1(s); err != nil {\n\t\t\tlog.Debugf(\"error: %s\", err)\n\t\t\treturn err\n\t\t}\n\tcase bsnet.protocolBitswapOneZero, bsnet.protocolBitswapNoVers:\n\t\tif err := msg.ToNetV0(s); err != nil {\n\t\t\tlog.Debugf(\"error: %s\", err)\n\t\t\treturn err\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"unrecognized protocol on remote: %s\", s.Protocol())\n\t}\n\n\tatomic.AddUint64(&bsnet.stats.MessagesSent, 1)\n\n\tif err := s.SetWriteDeadline(time.Time{}); err != nil {\n\t\tlog.Warnf(\"error resetting deadline: %s\", err)\n\t}\n\treturn nil\n}\n\nfunc (bsnet *impl) NewMessageSender(ctx context.Context, p peer.ID, opts *MessageSenderOpts) (MessageSender, error) {\n\topts = setDefaultOpts(opts)\n\n\tsender := &streamMessageSender{\n\t\tto:    p,\n\t\tbsnet: bsnet,\n\t\topts:  opts,\n\t}\n\n\terr := sender.multiAttempt(ctx, func() error {\n\t\t_, err := sender.Connect(ctx)\n\t\treturn err\n\t})\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn sender, nil\n}\n\nfunc setDefaultOpts(opts *MessageSenderOpts) *MessageSenderOpts {\n\tcopy := *opts\n\tif opts.MaxRetries == 0 {\n\t\tcopy.MaxRetries = 3\n\t}\n\tif opts.SendTimeout == 0 {\n\t\tcopy.SendTimeout = maxSendTimeout\n\t}\n\tif opts.SendErrorBackoff == 0 {\n\t\tcopy.SendErrorBackoff = 100 * time.Millisecond\n\t}\n\treturn &copy\n}\n\nfunc sendTimeout(size int) time.Duration {\n\ttimeout := sendLatency\n\ttimeout += time.Duration((uint64(time.Second) * uint64(size)) / uint64(minSendRate))\n\tif timeout > maxSendTimeout {\n\t\ttimeout = maxSendTimeout\n\t} else if timeout < minSendTimeout {\n\t\ttimeout = minSendTimeout\n\t}\n\treturn timeout\n}\n\nfunc (bsnet *impl) SendMessage(\n\tctx context.Context,\n\tp peer.ID,\n\toutgoing bsmsg.BitSwapMessage) error {\n\n\ttctx, cancel := context.WithTimeout(ctx, connectTimeout)\n\tdefer cancel()\n\n\ts, err := bsnet.newStreamToPeer(tctx, p)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttimeout := sendTimeout(outgoing.Size())\n\tif err = bsnet.msgToStream(ctx, s, outgoing, timeout); err != nil {\n\t\t_ = s.Reset()\n\t\treturn err\n\t}\n\n\treturn s.Close()\n}\n\nfunc (bsnet *impl) newStreamToPeer(ctx context.Context, p peer.ID) (network.Stream, error) {\n\treturn bsnet.host.NewStream(ctx, p, bsnet.supportedProtocols...)\n}\n\nfunc (bsnet *impl) Start(r ...Receiver) {\n\tbsnet.receivers = r\n\t{\n\t\tconnectionListeners := make([]ConnectionListener, len(r))\n\t\tfor i, v := range r {\n\t\t\tconnectionListeners[i] = v\n\t\t}\n\t\tbsnet.connectEvtMgr = newConnectEventManager(connectionListeners...)\n\t}\n\tfor _, proto := range bsnet.supportedProtocols {\n\t\tbsnet.host.SetStreamHandler(proto, bsnet.handleNewStream)\n\t}\n\tbsnet.host.Network().Notify((*netNotifiee)(bsnet))\n\tbsnet.connectEvtMgr.Start()\n\n}\n\nfunc (bsnet *impl) Stop() {\n\tbsnet.connectEvtMgr.Stop()\n\tbsnet.host.Network().StopNotify((*netNotifiee)(bsnet))\n}\n\nfunc (bsnet *impl) ConnectTo(ctx context.Context, p peer.ID) error {\n\treturn bsnet.host.Connect(ctx, peer.AddrInfo{ID: p})\n}\n\nfunc (bsnet *impl) DisconnectFrom(ctx context.Context, p peer.ID) error {\n\treturn bsnet.host.Network().ClosePeer(p)\n}\n\n// FindProvidersAsync returns a channel of providers for the given key.\nfunc (bsnet *impl) FindProvidersAsync(ctx context.Context, k cid.Cid, max int) <-chan peer.ID {\n\tout := make(chan peer.ID, max)\n\tgo func() {\n\t\tdefer close(out)\n\t\tproviders := bsnet.routing.FindProvidersAsync(ctx, k, max)\n\t\tfor info := range providers {\n\t\t\tif info.ID == bsnet.host.ID() {\n\t\t\t\tcontinue // ignore self as provider\n\t\t\t}\n\t\t\tbsnet.host.Peerstore().AddAddrs(info.ID, info.Addrs, peerstore.TempAddrTTL)\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase out <- info.ID:\n\t\t\t}\n\t\t}\n\t}()\n\treturn out\n}\n\n// Provide provides the key to the network\nfunc (bsnet *impl) Provide(ctx context.Context, k cid.Cid) error {\n\treturn bsnet.routing.Provide(ctx, k, true)\n}\n\n// handleNewStream receives a new stream from the network.\nfunc (bsnet *impl) handleNewStream(s network.Stream) {\n\tdefer s.Close()\n\n\tif len(bsnet.receivers) == 0 {\n\t\t_ = s.Reset()\n\t\treturn\n\t}\n\n\treader := msgio.NewVarintReaderSize(s, network.MessageSizeMax)\n\tfor {\n\t\treceived, err := bsmsg.FromMsgReader(reader)\n\t\tif err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\t_ = s.Reset()\n\t\t\t\tfor _, v := range bsnet.receivers {\n\t\t\t\t\tv.ReceiveError(err)\n\t\t\t\t}\n\t\t\t\tlog.Debugf(\"bitswap net handleNewStream from %s error: %s\", s.Conn().RemotePeer(), err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\tp := s.Conn().RemotePeer()\n\t\tctx := context.Background()\n\t\tlog.Debugf(\"bitswap net handleNewStream from %s\", s.Conn().RemotePeer())\n\t\tbsnet.connectEvtMgr.OnMessage(s.Conn().RemotePeer())\n\t\tatomic.AddUint64(&bsnet.stats.MessagesRecvd, 1)\n\t\tfor _, v := range bsnet.receivers {\n\t\t\tv.ReceiveMessage(ctx, p, received)\n\t\t}\n\t}\n}\n\nfunc (bsnet *impl) ConnectionManager() connmgr.ConnManager {\n\treturn bsnet.host.ConnManager()\n}\n\nfunc (bsnet *impl) Stats() Stats {\n\treturn Stats{\n\t\tMessagesRecvd: atomic.LoadUint64(&bsnet.stats.MessagesRecvd),\n\t\tMessagesSent:  atomic.LoadUint64(&bsnet.stats.MessagesSent),\n\t}\n}\n\ntype netNotifiee impl\n\nfunc (nn *netNotifiee) impl() *impl {\n\treturn (*impl)(nn)\n}\n\nfunc (nn *netNotifiee) Connected(n network.Network, v network.Conn) {\n\t// ignore transient connections\n\tif v.Stat().Transient {\n\t\treturn\n\t}\n\n\tnn.impl().connectEvtMgr.Connected(v.RemotePeer())\n}\nfunc (nn *netNotifiee) Disconnected(n network.Network, v network.Conn) {\n\t// Only record a \"disconnect\" when we actually disconnect.\n\tif n.Connectedness(v.RemotePeer()) == network.Connected {\n\t\treturn\n\t}\n\n\tnn.impl().connectEvtMgr.Disconnected(v.RemotePeer())\n}\nfunc (nn *netNotifiee) OpenedStream(n network.Network, s network.Stream) {}\nfunc (nn *netNotifiee) ClosedStream(n network.Network, v network.Stream) {}\nfunc (nn *netNotifiee) Listen(n network.Network, a ma.Multiaddr)         {}\nfunc (nn *netNotifiee) ListenClose(n network.Network, a ma.Multiaddr)    {}\n", "package bitswap\n\nimport (\n\t\"time\"\n\n\tdelay \"github.com/ipfs/go-ipfs-delay\"\n\t\"github.com/ipfs/go-libipfs/bitswap/client\"\n\t\"github.com/ipfs/go-libipfs/bitswap/server\"\n\t\"github.com/ipfs/go-libipfs/bitswap/tracer\"\n)\n\ntype option func(*Bitswap)\n\n// Option is interface{} of server.Option or client.Option or func(*Bitswap)\n// wrapped in a struct to gain strong type checking.\ntype Option struct {\n\tv interface{}\n}\n\nfunc EngineBlockstoreWorkerCount(count int) Option {\n\treturn Option{server.EngineBlockstoreWorkerCount(count)}\n}\n\nfunc EngineTaskWorkerCount(count int) Option {\n\treturn Option{server.EngineTaskWorkerCount(count)}\n}\n\nfunc MaxOutstandingBytesPerPeer(count int) Option {\n\treturn Option{server.MaxOutstandingBytesPerPeer(count)}\n}\n\nfunc MaxQueuedWantlistEntriesPerPeer(count uint) Option {\n\treturn Option{server.MaxQueuedWantlistEntriesPerPeer(count)}\n}\n\n// MaxCidSize only affects the server.\n// If it is 0 no limit is applied.\nfunc MaxCidSize(n uint) Option {\n\treturn Option{server.MaxCidSize(n)}\n}\n\nfunc TaskWorkerCount(count int) Option {\n\treturn Option{server.TaskWorkerCount(count)}\n}\n\nfunc ProvideEnabled(enabled bool) Option {\n\treturn Option{server.ProvideEnabled(enabled)}\n}\n\nfunc SetSendDontHaves(send bool) Option {\n\treturn Option{server.SetSendDontHaves(send)}\n}\n\nfunc WithPeerBlockRequestFilter(pbrf server.PeerBlockRequestFilter) Option {\n\treturn Option{server.WithPeerBlockRequestFilter(pbrf)}\n}\n\nfunc WithScoreLedger(scoreLedger server.ScoreLedger) Option {\n\treturn Option{server.WithScoreLedger(scoreLedger)}\n}\n\nfunc WithTargetMessageSize(tms int) Option {\n\treturn Option{server.WithTargetMessageSize(tms)}\n}\n\nfunc WithTaskComparator(comparator server.TaskComparator) Option {\n\treturn Option{server.WithTaskComparator(comparator)}\n}\n\nfunc ProviderSearchDelay(newProvSearchDelay time.Duration) Option {\n\treturn Option{client.ProviderSearchDelay(newProvSearchDelay)}\n}\n\nfunc RebroadcastDelay(newRebroadcastDelay delay.D) Option {\n\treturn Option{client.RebroadcastDelay(newRebroadcastDelay)}\n}\n\nfunc SetSimulateDontHavesOnTimeout(send bool) Option {\n\treturn Option{client.SetSimulateDontHavesOnTimeout(send)}\n}\n\nfunc WithTracer(tap tracer.Tracer) Option {\n\t// Only trace the server, both receive the same messages anyway\n\treturn Option{\n\t\toption(func(bs *Bitswap) {\n\t\t\tbs.tracer = tap\n\t\t}),\n\t}\n}\n", "// Package decision implements the decision engine for the bitswap service.\npackage decision\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math/bits\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\n\t\"github.com/ipfs/go-cid\"\n\tbstore \"github.com/ipfs/go-ipfs-blockstore\"\n\twl \"github.com/ipfs/go-libipfs/bitswap/client/wantlist\"\n\t\"github.com/ipfs/go-libipfs/bitswap/internal/defaults\"\n\tbsmsg \"github.com/ipfs/go-libipfs/bitswap/message\"\n\tpb \"github.com/ipfs/go-libipfs/bitswap/message/pb\"\n\tbmetrics \"github.com/ipfs/go-libipfs/bitswap/metrics\"\n\tblocks \"github.com/ipfs/go-libipfs/blocks\"\n\tlogging \"github.com/ipfs/go-log\"\n\t\"github.com/ipfs/go-metrics-interface\"\n\t\"github.com/ipfs/go-peertaskqueue\"\n\t\"github.com/ipfs/go-peertaskqueue/peertask\"\n\t\"github.com/ipfs/go-peertaskqueue/peertracker\"\n\tprocess \"github.com/jbenet/goprocess\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\tmh \"github.com/multiformats/go-multihash\"\n)\n\n// TODO consider taking responsibility for other types of requests. For\n// example, there could be a |cancelQueue| for all of the cancellation\n// messages that need to go out. There could also be a |wantlistQueue| for\n// the local peer's wantlists. Alternatively, these could all be bundled\n// into a single, intelligent global queue that efficiently\n// batches/combines and takes all of these into consideration.\n//\n// Right now, messages go onto the network for four reasons:\n// 1. an initial `sendwantlist` message to a provider of the first key in a\n//    request\n// 2. a periodic full sweep of `sendwantlist` messages to all providers\n// 3. upon receipt of blocks, a `cancel` message to all peers\n// 4. draining the priority queue of `blockrequests` from peers\n//\n// Presently, only `blockrequests` are handled by the decision engine.\n// However, there is an opportunity to give it more responsibility! If the\n// decision engine is given responsibility for all of the others, it can\n// intelligently decide how to combine requests efficiently.\n//\n// Some examples of what would be possible:\n//\n// * when sending out the wantlists, include `cancel` requests\n// * when handling `blockrequests`, include `sendwantlist` and `cancel` as\n//   appropriate\n// * when handling `cancel`, if we recently received a wanted block from a\n//   peer, include a partial wantlist that contains a few other high priority\n//   blocks\n//\n// In a sense, if we treat the decision engine as a black box, it could do\n// whatever it sees fit to produce desired outcomes (get wanted keys\n// quickly, maintain good relationships with peers, etc).\n\nvar log = logging.Logger(\"engine\")\n\nconst (\n\t// outboxChanBuffer must be 0 to prevent stale messages from being sent\n\toutboxChanBuffer = 0\n\t// targetMessageSize is the ideal size of the batched payload. We try to\n\t// pop this much data off the request queue, but it may be a little more\n\t// or less depending on what's in the queue.\n\tdefaultTargetMessageSize = 16 * 1024\n\t// tagFormat is the tag given to peers associated an engine\n\ttagFormat = \"bs-engine-%s-%s\"\n\n\t// queuedTagWeight is the default weight for peers that have work queued\n\t// on their behalf.\n\tqueuedTagWeight = 10\n\n\t// maxBlockSizeReplaceHasWithBlock is the maximum size of the block in\n\t// bytes up to which we will replace a want-have with a want-block\n\tmaxBlockSizeReplaceHasWithBlock = 1024\n)\n\n// Envelope contains a message for a Peer.\ntype Envelope struct {\n\t// Peer is the intended recipient.\n\tPeer peer.ID\n\n\t// Message is the payload.\n\tMessage bsmsg.BitSwapMessage\n\n\t// A callback to notify the decision queue that the task is complete\n\tSent func()\n}\n\n// PeerTagger covers the methods on the connection manager used by the decision\n// engine to tag peers\ntype PeerTagger interface {\n\tTagPeer(peer.ID, string, int)\n\tUntagPeer(p peer.ID, tag string)\n}\n\n// Assigns a specific score to a peer\ntype ScorePeerFunc func(peer.ID, int)\n\n// ScoreLedger is an external ledger dealing with peer scores.\ntype ScoreLedger interface {\n\t// Returns aggregated data communication with a given peer.\n\tGetReceipt(p peer.ID) *Receipt\n\t// Increments the sent counter for the given peer.\n\tAddToSentBytes(p peer.ID, n int)\n\t// Increments the received counter for the given peer.\n\tAddToReceivedBytes(p peer.ID, n int)\n\t// PeerConnected should be called when a new peer connects,\n\t// meaning the ledger should open accounting.\n\tPeerConnected(p peer.ID)\n\t// PeerDisconnected should be called when a peer disconnects to\n\t// clean up the accounting.\n\tPeerDisconnected(p peer.ID)\n\t// Starts the ledger sampling process.\n\tStart(scorePeer ScorePeerFunc)\n\t// Stops the sampling process.\n\tStop()\n}\n\n// Engine manages sending requested blocks to peers.\ntype Engine struct {\n\t// peerRequestQueue is a priority queue of requests received from peers.\n\t// Requests are popped from the queue, packaged up, and placed in the\n\t// outbox.\n\tpeerRequestQueue *peertaskqueue.PeerTaskQueue\n\n\t// FIXME it's a bit odd for the client and the worker to both share memory\n\t// (both modify the peerRequestQueue) and also to communicate over the\n\t// workSignal channel. consider sending requests over the channel and\n\t// allowing the worker to have exclusive access to the peerRequestQueue. In\n\t// that case, no lock would be required.\n\tworkSignal chan struct{}\n\n\t// outbox contains outgoing messages to peers. This is owned by the\n\t// taskWorker goroutine\n\toutbox chan (<-chan *Envelope)\n\n\tbsm *blockstoreManager\n\n\tpeerTagger PeerTagger\n\n\ttagQueued, tagUseful string\n\n\tlock sync.RWMutex // protects the fields immediately below\n\n\t// peerLedger saves which peers are waiting for a Cid\n\tpeerLedger *peerLedger\n\n\t// an external ledger dealing with peer scores\n\tscoreLedger ScoreLedger\n\n\tticker *time.Ticker\n\n\ttaskWorkerLock  sync.Mutex\n\ttaskWorkerCount int\n\n\ttargetMessageSize int\n\n\t// maxBlockSizeReplaceHasWithBlock is the maximum size of the block in\n\t// bytes up to which we will replace a want-have with a want-block\n\tmaxBlockSizeReplaceHasWithBlock int\n\n\tsendDontHaves bool\n\n\tself peer.ID\n\n\t// metrics gauge for total pending tasks across all workers\n\tpendingGauge metrics.Gauge\n\n\t// metrics gauge for total pending tasks across all workers\n\tactiveGauge metrics.Gauge\n\n\t// used to ensure metrics are reported each fixed number of operation\n\tmetricsLock         sync.Mutex\n\tmetricUpdateCounter int\n\n\ttaskComparator TaskComparator\n\n\tpeerBlockRequestFilter PeerBlockRequestFilter\n\n\tbstoreWorkerCount          int\n\tmaxOutstandingBytesPerPeer int\n\n\tmaxQueuedWantlistEntriesPerPeer uint\n\tmaxCidSize                      uint\n}\n\n// TaskInfo represents the details of a request from a peer.\ntype TaskInfo struct {\n\tPeer peer.ID\n\t// The CID of the block\n\tCid cid.Cid\n\t// Tasks can be want-have or want-block\n\tIsWantBlock bool\n\t// Whether to immediately send a response if the block is not found\n\tSendDontHave bool\n\t// The size of the block corresponding to the task\n\tBlockSize int\n\t// Whether the block was found\n\tHaveBlock bool\n}\n\n// TaskComparator is used for task prioritization.\n// It should return true if task 'ta' has higher priority than task 'tb'\ntype TaskComparator func(ta, tb *TaskInfo) bool\n\n// PeerBlockRequestFilter is used to accept / deny requests for a CID coming from a PeerID\n// It should return true if the request should be fullfilled.\ntype PeerBlockRequestFilter func(p peer.ID, c cid.Cid) bool\n\ntype Option func(*Engine)\n\nfunc WithTaskComparator(comparator TaskComparator) Option {\n\treturn func(e *Engine) {\n\t\te.taskComparator = comparator\n\t}\n}\n\nfunc WithPeerBlockRequestFilter(pbrf PeerBlockRequestFilter) Option {\n\treturn func(e *Engine) {\n\t\te.peerBlockRequestFilter = pbrf\n\t}\n}\n\nfunc WithTargetMessageSize(size int) Option {\n\treturn func(e *Engine) {\n\t\te.targetMessageSize = size\n\t}\n}\n\nfunc WithScoreLedger(scoreledger ScoreLedger) Option {\n\treturn func(e *Engine) {\n\t\te.scoreLedger = scoreledger\n\t}\n}\n\n// WithBlockstoreWorkerCount sets the number of worker threads used for\n// blockstore operations in the decision engine\nfunc WithBlockstoreWorkerCount(count int) Option {\n\tif count <= 0 {\n\t\tpanic(fmt.Sprintf(\"Engine blockstore worker count is %d but must be > 0\", count))\n\t}\n\treturn func(e *Engine) {\n\t\te.bstoreWorkerCount = count\n\t}\n}\n\n// WithTaskWorkerCount sets the number of worker threads used inside the engine\nfunc WithTaskWorkerCount(count int) Option {\n\tif count <= 0 {\n\t\tpanic(fmt.Sprintf(\"Engine task worker count is %d but must be > 0\", count))\n\t}\n\treturn func(e *Engine) {\n\t\te.taskWorkerCount = count\n\t}\n}\n\n// WithMaxOutstandingBytesPerPeer describes approximately how much work we are will to have outstanding to a peer at any\n// given time. Setting it to 0 will disable any limiting.\nfunc WithMaxOutstandingBytesPerPeer(count int) Option {\n\tif count < 0 {\n\t\tpanic(fmt.Sprintf(\"max outstanding bytes per peer is %d but must be >= 0\", count))\n\t}\n\treturn func(e *Engine) {\n\t\te.maxOutstandingBytesPerPeer = count\n\t}\n}\n\n// WithMaxQueuedWantlistEntriesPerPeer limits how much individual entries each peer is allowed to send.\n// If a peer send us more than this we will truncate newest entries.\nfunc WithMaxQueuedWantlistEntriesPerPeer(count uint) Option {\n\treturn func(e *Engine) {\n\t\te.maxQueuedWantlistEntriesPerPeer = count\n\t}\n}\n\n// WithMaxQueuedWantlistEntriesPerPeer limits how much individual entries each peer is allowed to send.\n// If a peer send us more than this we will truncate newest entries.\nfunc WithMaxCidSize(n uint) Option {\n\treturn func(e *Engine) {\n\t\te.maxCidSize = n\n\t}\n}\n\nfunc WithSetSendDontHave(send bool) Option {\n\treturn func(e *Engine) {\n\t\te.sendDontHaves = send\n\t}\n}\n\n// wrapTaskComparator wraps a TaskComparator so it can be used as a QueueTaskComparator\nfunc wrapTaskComparator(tc TaskComparator) peertask.QueueTaskComparator {\n\treturn func(a, b *peertask.QueueTask) bool {\n\t\ttaskDataA := a.Task.Data.(*taskData)\n\t\ttaskInfoA := &TaskInfo{\n\t\t\tPeer:         a.Target,\n\t\t\tCid:          a.Task.Topic.(cid.Cid),\n\t\t\tIsWantBlock:  taskDataA.IsWantBlock,\n\t\t\tSendDontHave: taskDataA.SendDontHave,\n\t\t\tBlockSize:    taskDataA.BlockSize,\n\t\t\tHaveBlock:    taskDataA.HaveBlock,\n\t\t}\n\t\ttaskDataB := b.Task.Data.(*taskData)\n\t\ttaskInfoB := &TaskInfo{\n\t\t\tPeer:         b.Target,\n\t\t\tCid:          b.Task.Topic.(cid.Cid),\n\t\t\tIsWantBlock:  taskDataB.IsWantBlock,\n\t\t\tSendDontHave: taskDataB.SendDontHave,\n\t\t\tBlockSize:    taskDataB.BlockSize,\n\t\t\tHaveBlock:    taskDataB.HaveBlock,\n\t\t}\n\t\treturn tc(taskInfoA, taskInfoB)\n\t}\n}\n\n// NewEngine creates a new block sending engine for the given block store.\n// maxOutstandingBytesPerPeer hints to the peer task queue not to give a peer more tasks if it has some maximum\n// work already outstanding.\nfunc NewEngine(\n\tctx context.Context,\n\tbs bstore.Blockstore,\n\tpeerTagger PeerTagger,\n\tself peer.ID,\n\topts ...Option,\n) *Engine {\n\treturn newEngine(\n\t\tctx,\n\t\tbs,\n\t\tpeerTagger,\n\t\tself,\n\t\tmaxBlockSizeReplaceHasWithBlock,\n\t\topts...,\n\t)\n}\n\nfunc newEngine(\n\tctx context.Context,\n\tbs bstore.Blockstore,\n\tpeerTagger PeerTagger,\n\tself peer.ID,\n\tmaxReplaceSize int,\n\topts ...Option,\n) *Engine {\n\te := &Engine{\n\t\tscoreLedger:                     NewDefaultScoreLedger(),\n\t\tbstoreWorkerCount:               defaults.BitswapEngineBlockstoreWorkerCount,\n\t\tmaxOutstandingBytesPerPeer:      defaults.BitswapMaxOutstandingBytesPerPeer,\n\t\tpeerTagger:                      peerTagger,\n\t\toutbox:                          make(chan (<-chan *Envelope), outboxChanBuffer),\n\t\tworkSignal:                      make(chan struct{}, 1),\n\t\tticker:                          time.NewTicker(time.Millisecond * 100),\n\t\tmaxBlockSizeReplaceHasWithBlock: maxReplaceSize,\n\t\ttaskWorkerCount:                 defaults.BitswapEngineTaskWorkerCount,\n\t\tsendDontHaves:                   true,\n\t\tself:                            self,\n\t\tpeerLedger:                      newPeerLedger(),\n\t\tpendingGauge:                    bmetrics.PendingEngineGauge(ctx),\n\t\tactiveGauge:                     bmetrics.ActiveEngineGauge(ctx),\n\t\ttargetMessageSize:               defaultTargetMessageSize,\n\t\ttagQueued:                       fmt.Sprintf(tagFormat, \"queued\", uuid.New().String()),\n\t\ttagUseful:                       fmt.Sprintf(tagFormat, \"useful\", uuid.New().String()),\n\t\tmaxQueuedWantlistEntriesPerPeer: defaults.MaxQueuedWantlistEntiresPerPeer,\n\t\tmaxCidSize:                      defaults.MaximumAllowedCid,\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(e)\n\t}\n\n\te.bsm = newBlockstoreManager(bs, e.bstoreWorkerCount, bmetrics.PendingBlocksGauge(ctx), bmetrics.ActiveBlocksGauge(ctx))\n\n\t// default peer task queue options\n\tpeerTaskQueueOpts := []peertaskqueue.Option{\n\t\tpeertaskqueue.OnPeerAddedHook(e.onPeerAdded),\n\t\tpeertaskqueue.OnPeerRemovedHook(e.onPeerRemoved),\n\t\tpeertaskqueue.TaskMerger(newTaskMerger()),\n\t\tpeertaskqueue.IgnoreFreezing(true),\n\t\tpeertaskqueue.MaxOutstandingWorkPerPeer(e.maxOutstandingBytesPerPeer),\n\t}\n\n\tif e.taskComparator != nil {\n\t\tqueueTaskComparator := wrapTaskComparator(e.taskComparator)\n\t\tpeerTaskQueueOpts = append(peerTaskQueueOpts, peertaskqueue.PeerComparator(peertracker.TaskPriorityPeerComparator(queueTaskComparator)))\n\t\tpeerTaskQueueOpts = append(peerTaskQueueOpts, peertaskqueue.TaskComparator(queueTaskComparator))\n\t}\n\n\te.peerRequestQueue = peertaskqueue.New(peerTaskQueueOpts...)\n\n\treturn e\n}\n\nfunc (e *Engine) updateMetrics() {\n\te.metricsLock.Lock()\n\tc := e.metricUpdateCounter\n\te.metricUpdateCounter++\n\te.metricsLock.Unlock()\n\n\tif c%100 == 0 {\n\t\tstats := e.peerRequestQueue.Stats()\n\t\te.activeGauge.Set(float64(stats.NumActive))\n\t\te.pendingGauge.Set(float64(stats.NumPending))\n\t}\n}\n\n// SetSendDontHaves indicates what to do when the engine receives a want-block\n// for a block that is not in the blockstore. Either\n// - Send a DONT_HAVE message\n// - Simply don't respond\n// Older versions of Bitswap did not respond, so this allows us to simulate\n// those older versions for testing.\nfunc (e *Engine) SetSendDontHaves(send bool) {\n\te.sendDontHaves = send\n}\n\n// Starts the score ledger. Before start the function checks and,\n// if it is unset, initializes the scoreLedger with the default\n// implementation.\nfunc (e *Engine) startScoreLedger(px process.Process) {\n\te.scoreLedger.Start(func(p peer.ID, score int) {\n\t\tif score == 0 {\n\t\t\te.peerTagger.UntagPeer(p, e.tagUseful)\n\t\t} else {\n\t\t\te.peerTagger.TagPeer(p, e.tagUseful, score)\n\t\t}\n\t})\n\tpx.Go(func(ppx process.Process) {\n\t\t<-ppx.Closing()\n\t\te.scoreLedger.Stop()\n\t})\n}\n\nfunc (e *Engine) startBlockstoreManager(px process.Process) {\n\te.bsm.start()\n\tpx.Go(func(ppx process.Process) {\n\t\t<-ppx.Closing()\n\t\te.bsm.stop()\n\t})\n}\n\n// Start up workers to handle requests from other nodes for the data on this node\nfunc (e *Engine) StartWorkers(ctx context.Context, px process.Process) {\n\te.startBlockstoreManager(px)\n\te.startScoreLedger(px)\n\n\te.taskWorkerLock.Lock()\n\tdefer e.taskWorkerLock.Unlock()\n\n\tfor i := 0; i < e.taskWorkerCount; i++ {\n\t\tpx.Go(func(_ process.Process) {\n\t\t\te.taskWorker(ctx)\n\t\t})\n\t}\n\n}\n\nfunc (e *Engine) onPeerAdded(p peer.ID) {\n\te.peerTagger.TagPeer(p, e.tagQueued, queuedTagWeight)\n}\n\nfunc (e *Engine) onPeerRemoved(p peer.ID) {\n\te.peerTagger.UntagPeer(p, e.tagQueued)\n}\n\n// WantlistForPeer returns the list of keys that the given peer has asked for\nfunc (e *Engine) WantlistForPeer(p peer.ID) []wl.Entry {\n\te.lock.RLock()\n\tdefer e.lock.RUnlock()\n\n\treturn e.peerLedger.WantlistForPeer(p)\n}\n\n// LedgerForPeer returns aggregated data communication with a given peer.\nfunc (e *Engine) LedgerForPeer(p peer.ID) *Receipt {\n\treturn e.scoreLedger.GetReceipt(p)\n}\n\n// Each taskWorker pulls items off the request queue up to the maximum size\n// and adds them to an envelope that is passed off to the bitswap workers,\n// which send the message to the network.\nfunc (e *Engine) taskWorker(ctx context.Context) {\n\tdefer e.taskWorkerExit()\n\tfor {\n\t\toneTimeUse := make(chan *Envelope, 1) // buffer to prevent blocking\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase e.outbox <- oneTimeUse:\n\t\t}\n\t\t// receiver is ready for an outoing envelope. let's prepare one. first,\n\t\t// we must acquire a task from the PQ...\n\t\tenvelope, err := e.nextEnvelope(ctx)\n\t\tif err != nil {\n\t\t\tclose(oneTimeUse)\n\t\t\treturn // ctx cancelled\n\t\t}\n\t\toneTimeUse <- envelope // buffered. won't block\n\t\tclose(oneTimeUse)\n\t}\n}\n\n// taskWorkerExit handles cleanup of task workers\nfunc (e *Engine) taskWorkerExit() {\n\te.taskWorkerLock.Lock()\n\tdefer e.taskWorkerLock.Unlock()\n\n\te.taskWorkerCount--\n\tif e.taskWorkerCount == 0 {\n\t\tclose(e.outbox)\n\t}\n}\n\n// nextEnvelope runs in the taskWorker goroutine. Returns an error if the\n// context is cancelled before the next Envelope can be created.\nfunc (e *Engine) nextEnvelope(ctx context.Context) (*Envelope, error) {\n\tfor {\n\t\t// Pop some tasks off the request queue\n\t\tp, nextTasks, pendingBytes := e.peerRequestQueue.PopTasks(e.targetMessageSize)\n\t\te.updateMetrics()\n\t\tfor len(nextTasks) == 0 {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn nil, ctx.Err()\n\t\t\tcase <-e.workSignal:\n\t\t\t\tp, nextTasks, pendingBytes = e.peerRequestQueue.PopTasks(e.targetMessageSize)\n\t\t\t\te.updateMetrics()\n\t\t\tcase <-e.ticker.C:\n\t\t\t\t// When a task is cancelled, the queue may be \"frozen\" for a\n\t\t\t\t// period of time. We periodically \"thaw\" the queue to make\n\t\t\t\t// sure it doesn't get stuck in a frozen state.\n\t\t\t\te.peerRequestQueue.ThawRound()\n\t\t\t\tp, nextTasks, pendingBytes = e.peerRequestQueue.PopTasks(e.targetMessageSize)\n\t\t\t\te.updateMetrics()\n\t\t\t}\n\t\t}\n\n\t\t// Create a new message\n\t\tmsg := bsmsg.New(false)\n\n\t\tlog.Debugw(\"Bitswap process tasks\", \"local\", e.self, \"taskCount\", len(nextTasks))\n\n\t\t// Amount of data in the request queue still waiting to be popped\n\t\tmsg.SetPendingBytes(int32(pendingBytes))\n\n\t\t// Split out want-blocks, want-haves and DONT_HAVEs\n\t\tblockCids := make([]cid.Cid, 0, len(nextTasks))\n\t\tblockTasks := make(map[cid.Cid]*taskData, len(nextTasks))\n\t\tfor _, t := range nextTasks {\n\t\t\tc := t.Topic.(cid.Cid)\n\t\t\ttd := t.Data.(*taskData)\n\t\t\tif td.HaveBlock {\n\t\t\t\tif td.IsWantBlock {\n\t\t\t\t\tblockCids = append(blockCids, c)\n\t\t\t\t\tblockTasks[c] = td\n\t\t\t\t} else {\n\t\t\t\t\t// Add HAVES to the message\n\t\t\t\t\tmsg.AddHave(c)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Add DONT_HAVEs to the message\n\t\t\t\tmsg.AddDontHave(c)\n\t\t\t}\n\t\t}\n\n\t\t// Fetch blocks from datastore\n\t\tblks, err := e.bsm.getBlocks(ctx, blockCids)\n\t\tif err != nil {\n\t\t\t// we're dropping the envelope but that's not an issue in practice.\n\t\t\treturn nil, err\n\t\t}\n\n\t\tfor c, t := range blockTasks {\n\t\t\tblk := blks[c]\n\t\t\t// If the block was not found (it has been removed)\n\t\t\tif blk == nil {\n\t\t\t\t// If the client requested DONT_HAVE, add DONT_HAVE to the message\n\t\t\t\tif t.SendDontHave {\n\t\t\t\t\tmsg.AddDontHave(c)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Add the block to the message\n\t\t\t\t// log.Debugf(\"  make evlp %s->%s block: %s (%d bytes)\", e.self, p, c, len(blk.RawData()))\n\t\t\t\tmsg.AddBlock(blk)\n\t\t\t}\n\t\t}\n\n\t\t// If there's nothing in the message, bail out\n\t\tif msg.Empty() {\n\t\t\te.peerRequestQueue.TasksDone(p, nextTasks...)\n\t\t\tcontinue\n\t\t}\n\n\t\tlog.Debugw(\"Bitswap engine -> msg\", \"local\", e.self, \"to\", p, \"blockCount\", len(msg.Blocks()), \"presenceCount\", len(msg.BlockPresences()), \"size\", msg.Size())\n\t\treturn &Envelope{\n\t\t\tPeer:    p,\n\t\t\tMessage: msg,\n\t\t\tSent: func() {\n\t\t\t\t// Once the message has been sent, signal the request queue so\n\t\t\t\t// it can be cleared from the queue\n\t\t\t\te.peerRequestQueue.TasksDone(p, nextTasks...)\n\n\t\t\t\t// Signal the worker to check for more work\n\t\t\t\te.signalNewWork()\n\t\t\t},\n\t\t}, nil\n\t}\n}\n\n// Outbox returns a channel of one-time use Envelope channels.\nfunc (e *Engine) Outbox() <-chan (<-chan *Envelope) {\n\treturn e.outbox\n}\n\n// Peers returns a slice of Peers with whom the local node has active sessions.\nfunc (e *Engine) Peers() []peer.ID {\n\te.lock.RLock()\n\tdefer e.lock.RUnlock()\n\n\treturn e.peerLedger.CollectPeerIDs()\n}\n\n// MessageReceived is called when a message is received from a remote peer.\n// For each item in the wantlist, add a want-have or want-block entry to the\n// request queue (this is later popped off by the workerTasks)\nfunc (e *Engine) MessageReceived(ctx context.Context, p peer.ID, m bsmsg.BitSwapMessage) (mustKillConnection bool) {\n\tentries := m.Wantlist()\n\n\tif len(entries) > 0 {\n\t\tlog.Debugw(\"Bitswap engine <- msg\", \"local\", e.self, \"from\", p, \"entryCount\", len(entries))\n\t\tfor _, et := range entries {\n\t\t\tif !et.Cancel {\n\t\t\t\tif et.WantType == pb.Message_Wantlist_Have {\n\t\t\t\t\tlog.Debugw(\"Bitswap engine <- want-have\", \"local\", e.self, \"from\", p, \"cid\", et.Cid)\n\t\t\t\t} else {\n\t\t\t\t\tlog.Debugw(\"Bitswap engine <- want-block\", \"local\", e.self, \"from\", p, \"cid\", et.Cid)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif m.Empty() {\n\t\tlog.Infof(\"received empty message from %s\", p)\n\t}\n\n\tnewWorkExists := false\n\tdefer func() {\n\t\tif newWorkExists {\n\t\t\te.signalNewWork()\n\t\t}\n\t}()\n\n\t// Dispatch entries\n\twants, cancels := e.splitWantsCancels(entries)\n\twants, denials := e.splitWantsDenials(p, wants)\n\n\t// Get block sizes\n\twantKs := cid.NewSet()\n\tfor _, entry := range wants {\n\t\twantKs.Add(entry.Cid)\n\t}\n\tblockSizes, err := e.bsm.getBlockSizes(ctx, wantKs.Keys())\n\tif err != nil {\n\t\tlog.Info(\"aborting message processing\", err)\n\t\treturn\n\t}\n\n\te.lock.Lock()\n\n\tif m.Full() {\n\t\te.peerLedger.ClearPeerWantlist(p)\n\t}\n\n\ts := uint(e.peerLedger.WantlistSizeForPeer(p))\n\tif wouldBe := s + uint(len(wants)); wouldBe > e.maxQueuedWantlistEntriesPerPeer {\n\t\tlog.Debugw(\"wantlist overflow\", \"local\", e.self, \"remote\", p, \"would be\", wouldBe)\n\t\t// truncate wantlist to avoid overflow\n\t\tavailable, o := bits.Sub(e.maxQueuedWantlistEntriesPerPeer, s, 0)\n\t\tif o != 0 {\n\t\t\tavailable = 0\n\t\t}\n\t\twants = wants[:available]\n\t}\n\n\tfilteredWants := wants[:0] // shift inplace\n\n\tfor _, entry := range wants {\n\t\tif entry.Cid.Prefix().MhType == mh.IDENTITY {\n\t\t\t// This is a truely broken client, let's kill the connection.\n\t\t\te.lock.Unlock()\n\t\t\tlog.Warnw(\"peer wants an identity CID\", \"local\", e.self, \"remote\", p)\n\t\t\treturn true\n\t\t}\n\t\tif e.maxCidSize != 0 && uint(entry.Cid.ByteLen()) > e.maxCidSize {\n\t\t\t// Ignore requests about CIDs that big.\n\t\t\tcontinue\n\t\t}\n\n\t\te.peerLedger.Wants(p, entry.Entry)\n\t\tfilteredWants = append(filteredWants, entry)\n\t}\n\tclear := wants[len(filteredWants):]\n\tfor i := range clear {\n\t\tclear[i] = bsmsg.Entry{} // early GC\n\t}\n\twants = filteredWants\n\tfor _, entry := range cancels {\n\t\tif entry.Cid.Prefix().MhType == mh.IDENTITY {\n\t\t\t// This is a truely broken client, let's kill the connection.\n\t\t\te.lock.Unlock()\n\t\t\tlog.Warnw(\"peer canceled an identity CID\", \"local\", e.self, \"remote\", p)\n\t\t\treturn true\n\t\t}\n\t\tif e.maxCidSize != 0 && uint(entry.Cid.ByteLen()) > e.maxCidSize {\n\t\t\t// Ignore requests about CIDs that big.\n\t\t\tcontinue\n\t\t}\n\n\t\tlog.Debugw(\"Bitswap engine <- cancel\", \"local\", e.self, \"from\", p, \"cid\", entry.Cid)\n\t\tif e.peerLedger.CancelWant(p, entry.Cid) {\n\t\t\te.peerRequestQueue.Remove(entry.Cid, p)\n\t\t}\n\t}\n\te.lock.Unlock()\n\n\tvar activeEntries []peertask.Task\n\n\t// Cancel a block operation\n\tsendDontHave := func(entry bsmsg.Entry) {\n\t\t// Only add the task to the queue if the requester wants a DONT_HAVE\n\t\tif e.sendDontHaves && entry.SendDontHave {\n\t\t\tc := entry.Cid\n\n\t\t\tnewWorkExists = true\n\t\t\tisWantBlock := false\n\t\t\tif entry.WantType == pb.Message_Wantlist_Block {\n\t\t\t\tisWantBlock = true\n\t\t\t}\n\n\t\t\tactiveEntries = append(activeEntries, peertask.Task{\n\t\t\t\tTopic:    c,\n\t\t\t\tPriority: int(entry.Priority),\n\t\t\t\tWork:     bsmsg.BlockPresenceSize(c),\n\t\t\t\tData: &taskData{\n\t\t\t\t\tBlockSize:    0,\n\t\t\t\t\tHaveBlock:    false,\n\t\t\t\t\tIsWantBlock:  isWantBlock,\n\t\t\t\t\tSendDontHave: entry.SendDontHave,\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\t}\n\n\t// Deny access to blocks\n\tfor _, entry := range denials {\n\t\tlog.Debugw(\"Bitswap engine: block denied access\", \"local\", e.self, \"from\", p, \"cid\", entry.Cid, \"sendDontHave\", entry.SendDontHave)\n\t\tsendDontHave(entry)\n\t}\n\n\t// For each want-have / want-block\n\tfor _, entry := range wants {\n\t\tc := entry.Cid\n\t\tblockSize, found := blockSizes[entry.Cid]\n\n\t\t// If the block was not found\n\t\tif !found {\n\t\t\tlog.Debugw(\"Bitswap engine: block not found\", \"local\", e.self, \"from\", p, \"cid\", entry.Cid, \"sendDontHave\", entry.SendDontHave)\n\t\t\tsendDontHave(entry)\n\t\t} else {\n\t\t\t// The block was found, add it to the queue\n\t\t\tnewWorkExists = true\n\n\t\t\tisWantBlock := e.sendAsBlock(entry.WantType, blockSize)\n\n\t\t\tlog.Debugw(\"Bitswap engine: block found\", \"local\", e.self, \"from\", p, \"cid\", entry.Cid, \"isWantBlock\", isWantBlock)\n\n\t\t\t// entrySize is the amount of space the entry takes up in the\n\t\t\t// message we send to the recipient. If we're sending a block, the\n\t\t\t// entrySize is the size of the block. Otherwise it's the size of\n\t\t\t// a block presence entry.\n\t\t\tentrySize := blockSize\n\t\t\tif !isWantBlock {\n\t\t\t\tentrySize = bsmsg.BlockPresenceSize(c)\n\t\t\t}\n\t\t\tactiveEntries = append(activeEntries, peertask.Task{\n\t\t\t\tTopic:    c,\n\t\t\t\tPriority: int(entry.Priority),\n\t\t\t\tWork:     entrySize,\n\t\t\t\tData: &taskData{\n\t\t\t\t\tBlockSize:    blockSize,\n\t\t\t\t\tHaveBlock:    true,\n\t\t\t\t\tIsWantBlock:  isWantBlock,\n\t\t\t\t\tSendDontHave: entry.SendDontHave,\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\t}\n\n\t// Push entries onto the request queue\n\tif len(activeEntries) > 0 {\n\t\te.peerRequestQueue.PushTasksTruncated(e.maxQueuedWantlistEntriesPerPeer, p, activeEntries...)\n\t\te.updateMetrics()\n\t}\n\treturn false\n}\n\n// Split the want-have / want-block entries from the cancel entries\nfunc (e *Engine) splitWantsCancels(es []bsmsg.Entry) ([]bsmsg.Entry, []bsmsg.Entry) {\n\twants := make([]bsmsg.Entry, 0, len(es))\n\tcancels := make([]bsmsg.Entry, 0, len(es))\n\tfor _, et := range es {\n\t\tif et.Cancel {\n\t\t\tcancels = append(cancels, et)\n\t\t} else {\n\t\t\twants = append(wants, et)\n\t\t}\n\t}\n\treturn wants, cancels\n}\n\n// Split the want-have / want-block entries from the block that will be denied access\nfunc (e *Engine) splitWantsDenials(p peer.ID, allWants []bsmsg.Entry) ([]bsmsg.Entry, []bsmsg.Entry) {\n\tif e.peerBlockRequestFilter == nil {\n\t\treturn allWants, nil\n\t}\n\n\twants := make([]bsmsg.Entry, 0, len(allWants))\n\tdenied := make([]bsmsg.Entry, 0, len(allWants))\n\n\tfor _, et := range allWants {\n\t\tif e.peerBlockRequestFilter(p, et.Cid) {\n\t\t\twants = append(wants, et)\n\t\t} else {\n\t\t\tdenied = append(denied, et)\n\t\t}\n\t}\n\n\treturn wants, denied\n}\n\n// ReceivedBlocks is called when new blocks are received from the network.\n// This function also updates the receive side of the ledger.\nfunc (e *Engine) ReceivedBlocks(from peer.ID, blks []blocks.Block) {\n\tif len(blks) == 0 {\n\t\treturn\n\t}\n\n\t// Record how many bytes were received in the ledger\n\tfor _, blk := range blks {\n\t\tlog.Debugw(\"Bitswap engine <- block\", \"local\", e.self, \"from\", from, \"cid\", blk.Cid(), \"size\", len(blk.RawData()))\n\t\te.scoreLedger.AddToReceivedBytes(from, len(blk.RawData()))\n\t}\n}\n\n// NotifyNewBlocks is called when new blocks becomes available locally, and in particular when the caller of bitswap\n// decide to store those blocks and make them available on the network.\nfunc (e *Engine) NotifyNewBlocks(blks []blocks.Block) {\n\tif len(blks) == 0 {\n\t\treturn\n\t}\n\n\t// Get the size of each block\n\tblockSizes := make(map[cid.Cid]int, len(blks))\n\tfor _, blk := range blks {\n\t\tblockSizes[blk.Cid()] = len(blk.RawData())\n\t}\n\n\t// Check each peer to see if it wants one of the blocks we received\n\tvar work bool\n\tfor _, b := range blks {\n\t\tk := b.Cid()\n\n\t\te.lock.RLock()\n\t\tpeers := e.peerLedger.Peers(k)\n\t\te.lock.RUnlock()\n\n\t\tfor _, entry := range peers {\n\t\t\twork = true\n\n\t\t\tblockSize := blockSizes[k]\n\t\t\tisWantBlock := e.sendAsBlock(entry.WantType, blockSize)\n\n\t\t\tentrySize := blockSize\n\t\t\tif !isWantBlock {\n\t\t\t\tentrySize = bsmsg.BlockPresenceSize(k)\n\t\t\t}\n\n\t\t\te.peerRequestQueue.PushTasksTruncated(e.maxQueuedWantlistEntriesPerPeer, entry.Peer, peertask.Task{\n\t\t\t\tTopic:    k,\n\t\t\t\tPriority: int(entry.Priority),\n\t\t\t\tWork:     entrySize,\n\t\t\t\tData: &taskData{\n\t\t\t\t\tBlockSize:    blockSize,\n\t\t\t\t\tHaveBlock:    true,\n\t\t\t\t\tIsWantBlock:  isWantBlock,\n\t\t\t\t\tSendDontHave: false,\n\t\t\t\t},\n\t\t\t})\n\t\t\te.updateMetrics()\n\t\t}\n\t}\n\n\tif work {\n\t\te.signalNewWork()\n\t}\n}\n\n// TODO add contents of m.WantList() to my local wantlist? NB: could introduce\n// race conditions where I send a message, but MessageSent gets handled after\n// MessageReceived. The information in the local wantlist could become\n// inconsistent. Would need to ensure that Sends and acknowledgement of the\n// send happen atomically\n\n// MessageSent is called when a message has successfully been sent out, to record\n// changes.\nfunc (e *Engine) MessageSent(p peer.ID, m bsmsg.BitSwapMessage) {\n\te.lock.Lock()\n\tdefer e.lock.Unlock()\n\n\t// Remove sent blocks from the want list for the peer\n\tfor _, block := range m.Blocks() {\n\t\te.scoreLedger.AddToSentBytes(p, len(block.RawData()))\n\t\te.peerLedger.CancelWantWithType(p, block.Cid(), pb.Message_Wantlist_Block)\n\t}\n\n\t// Remove sent block presences from the want list for the peer\n\tfor _, bp := range m.BlockPresences() {\n\t\t// Don't record sent data. We reserve that for data blocks.\n\t\tif bp.Type == pb.Message_Have {\n\t\t\te.peerLedger.CancelWantWithType(p, bp.Cid, pb.Message_Wantlist_Have)\n\t\t}\n\t}\n}\n\n// PeerConnected is called when a new peer connects, meaning we should start\n// sending blocks.\nfunc (e *Engine) PeerConnected(p peer.ID) {\n\te.lock.Lock()\n\tdefer e.lock.Unlock()\n\n\te.scoreLedger.PeerConnected(p)\n}\n\n// PeerDisconnected is called when a peer disconnects.\nfunc (e *Engine) PeerDisconnected(p peer.ID) {\n\te.peerRequestQueue.Clear(p)\n\n\te.lock.Lock()\n\tdefer e.lock.Unlock()\n\n\te.peerLedger.PeerDisconnected(p)\n\te.scoreLedger.PeerDisconnected(p)\n}\n\n// If the want is a want-have, and it's below a certain size, send the full\n// block (instead of sending a HAVE)\nfunc (e *Engine) sendAsBlock(wantType pb.Message_Wantlist_WantType, blockSize int) bool {\n\tisWantBlock := wantType == pb.Message_Wantlist_Block\n\treturn isWantBlock || blockSize <= e.maxBlockSizeReplaceHasWithBlock\n}\n\nfunc (e *Engine) numBytesSentTo(p peer.ID) uint64 {\n\treturn e.LedgerForPeer(p).Sent\n}\n\nfunc (e *Engine) numBytesReceivedFrom(p peer.ID) uint64 {\n\treturn e.LedgerForPeer(p).Recv\n}\n\nfunc (e *Engine) signalNewWork() {\n\t// Signal task generation to restart (if stopped!)\n\tselect {\n\tcase e.workSignal <- struct{}{}:\n\tdefault:\n\t}\n}\n", "package server\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ipfs/go-cid\"\n\tblockstore \"github.com/ipfs/go-ipfs-blockstore\"\n\t\"github.com/ipfs/go-libipfs/bitswap/internal/defaults\"\n\t\"github.com/ipfs/go-libipfs/bitswap/message\"\n\tpb \"github.com/ipfs/go-libipfs/bitswap/message/pb\"\n\tbmetrics \"github.com/ipfs/go-libipfs/bitswap/metrics\"\n\tbsnet \"github.com/ipfs/go-libipfs/bitswap/network\"\n\t\"github.com/ipfs/go-libipfs/bitswap/server/internal/decision\"\n\t\"github.com/ipfs/go-libipfs/bitswap/tracer\"\n\tblocks \"github.com/ipfs/go-libipfs/blocks\"\n\tlogging \"github.com/ipfs/go-log\"\n\t\"github.com/ipfs/go-metrics-interface\"\n\tprocess \"github.com/jbenet/goprocess\"\n\tprocctx \"github.com/jbenet/goprocess/context\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\t\"go.uber.org/zap\"\n)\n\nvar provideKeysBufferSize = 2048\n\nvar log = logging.Logger(\"bitswap-server\")\nvar sflog = log.Desugar()\n\nconst provideWorkerMax = 6\n\ntype Option func(*Server)\n\ntype Server struct {\n\tsentHistogram     metrics.Histogram\n\tsendTimeHistogram metrics.Histogram\n\n\t// the engine is the bit of logic that decides who to send which blocks to\n\tengine *decision.Engine\n\n\t// network delivers messages on behalf of the session\n\tnetwork bsnet.BitSwapNetwork\n\n\t// External statistics interface\n\ttracer tracer.Tracer\n\n\t// Counters for various statistics\n\tcounterLk sync.Mutex\n\tcounters  Stat\n\n\t// the total number of simultaneous threads sending outgoing messages\n\ttaskWorkerCount int\n\n\tprocess process.Process\n\n\t// newBlocks is a channel for newly added blocks to be provided to the\n\t// network.  blocks pushed down this channel get buffered and fed to the\n\t// provideKeys channel later on to avoid too much network activity\n\tnewBlocks chan cid.Cid\n\t// provideKeys directly feeds provide workers\n\tprovideKeys chan cid.Cid\n\n\t// Extra options to pass to the decision manager\n\tengineOptions []decision.Option\n\n\t// the size of channel buffer to use\n\thasBlockBufferSize int\n\t// whether or not to make provide announcements\n\tprovideEnabled bool\n}\n\nfunc New(ctx context.Context, network bsnet.BitSwapNetwork, bstore blockstore.Blockstore, options ...Option) *Server {\n\tctx, cancel := context.WithCancel(ctx)\n\n\tpx := process.WithTeardown(func() error {\n\t\treturn nil\n\t})\n\tgo func() {\n\t\t<-px.Closing() // process closes first\n\t\tcancel()\n\t}()\n\n\ts := &Server{\n\t\tsentHistogram:      bmetrics.SentHist(ctx),\n\t\tsendTimeHistogram:  bmetrics.SendTimeHist(ctx),\n\t\ttaskWorkerCount:    defaults.BitswapTaskWorkerCount,\n\t\tnetwork:            network,\n\t\tprocess:            px,\n\t\tprovideEnabled:     true,\n\t\thasBlockBufferSize: defaults.HasBlockBufferSize,\n\t\tprovideKeys:        make(chan cid.Cid, provideKeysBufferSize),\n\t}\n\ts.newBlocks = make(chan cid.Cid, s.hasBlockBufferSize)\n\n\tfor _, o := range options {\n\t\to(s)\n\t}\n\n\ts.engine = decision.NewEngine(\n\t\tctx,\n\t\tbstore,\n\t\tnetwork.ConnectionManager(),\n\t\tnetwork.Self(),\n\t\ts.engineOptions...,\n\t)\n\ts.engineOptions = nil\n\n\ts.startWorkers(ctx, px)\n\n\treturn s\n}\n\nfunc TaskWorkerCount(count int) Option {\n\tif count <= 0 {\n\t\tpanic(fmt.Sprintf(\"task worker count is %d but must be > 0\", count))\n\t}\n\treturn func(bs *Server) {\n\t\tbs.taskWorkerCount = count\n\t}\n}\n\nfunc WithTracer(tap tracer.Tracer) Option {\n\treturn func(bs *Server) {\n\t\tbs.tracer = tap\n\t}\n}\n\n// ProvideEnabled is an option for enabling/disabling provide announcements\nfunc ProvideEnabled(enabled bool) Option {\n\treturn func(bs *Server) {\n\t\tbs.provideEnabled = enabled\n\t}\n}\n\nfunc WithPeerBlockRequestFilter(pbrf decision.PeerBlockRequestFilter) Option {\n\to := decision.WithPeerBlockRequestFilter(pbrf)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// WithTaskComparator configures custom task prioritization logic.\nfunc WithTaskComparator(comparator decision.TaskComparator) Option {\n\to := decision.WithTaskComparator(comparator)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// Configures the engine to use the given score decision logic.\nfunc WithScoreLedger(scoreLedger decision.ScoreLedger) Option {\n\to := decision.WithScoreLedger(scoreLedger)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// LedgerForPeer returns aggregated data about blocks swapped and communication\n// with a given peer.\nfunc (bs *Server) LedgerForPeer(p peer.ID) *decision.Receipt {\n\treturn bs.engine.LedgerForPeer(p)\n}\n\n// EngineTaskWorkerCount sets the number of worker threads used inside the engine\nfunc EngineTaskWorkerCount(count int) Option {\n\to := decision.WithTaskWorkerCount(count)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// SetSendDontHaves indicates what to do when the engine receives a want-block\n// for a block that is not in the blockstore. Either\n// - Send a DONT_HAVE message\n// - Simply don't respond\n// This option is only used for testing.\nfunc SetSendDontHaves(send bool) Option {\n\to := decision.WithSetSendDontHave(send)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// EngineBlockstoreWorkerCount sets the number of worker threads used for\n// blockstore operations in the decision engine\nfunc EngineBlockstoreWorkerCount(count int) Option {\n\to := decision.WithBlockstoreWorkerCount(count)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\nfunc WithTargetMessageSize(tms int) Option {\n\to := decision.WithTargetMessageSize(tms)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// MaxOutstandingBytesPerPeer describes approximately how much work we are will to have outstanding to a peer at any\n// given time. Setting it to 0 will disable any limiting.\nfunc MaxOutstandingBytesPerPeer(count int) Option {\n\to := decision.WithMaxOutstandingBytesPerPeer(count)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// MaxQueuedWantlistEntriesPerPeer limits how much individual entries each peer is allowed to send.\n// If a peer send us more than this we will truncate newest entries.\n// It defaults to defaults.MaxQueuedWantlistEntiresPerPeer.\nfunc MaxQueuedWantlistEntriesPerPeer(count uint) Option {\n\to := decision.WithMaxQueuedWantlistEntriesPerPeer(count)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// MaxCidSize limits how big CIDs we are willing to serve.\n// We will ignore CIDs over this limit.\n// It defaults to [defaults.MaxCidSize].\n// If it is 0 no limit is applied.\nfunc MaxCidSize(n uint) Option {\n\to := decision.WithMaxCidSize(n)\n\treturn func(bs *Server) {\n\t\tbs.engineOptions = append(bs.engineOptions, o)\n\t}\n}\n\n// HasBlockBufferSize configure how big the new blocks buffer should be.\nfunc HasBlockBufferSize(count int) Option {\n\tif count < 0 {\n\t\tpanic(\"cannot have negative buffer size\")\n\t}\n\treturn func(bs *Server) {\n\t\tbs.hasBlockBufferSize = count\n\t}\n}\n\n// WantlistForPeer returns the currently understood list of blocks requested by a\n// given peer.\nfunc (bs *Server) WantlistForPeer(p peer.ID) []cid.Cid {\n\tvar out []cid.Cid\n\tfor _, e := range bs.engine.WantlistForPeer(p) {\n\t\tout = append(out, e.Cid)\n\t}\n\treturn out\n}\n\nfunc (bs *Server) startWorkers(ctx context.Context, px process.Process) {\n\tbs.engine.StartWorkers(ctx, px)\n\n\t// Start up workers to handle requests from other nodes for the data on this node\n\tfor i := 0; i < bs.taskWorkerCount; i++ {\n\t\ti := i\n\t\tpx.Go(func(px process.Process) {\n\t\t\tbs.taskWorker(ctx, i)\n\t\t})\n\t}\n\n\tif bs.provideEnabled {\n\t\t// Start up a worker to manage sending out provides messages\n\t\tpx.Go(func(px process.Process) {\n\t\t\tbs.provideCollector(ctx)\n\t\t})\n\n\t\t// Spawn up multiple workers to handle incoming blocks\n\t\t// consider increasing number if providing blocks bottlenecks\n\t\t// file transfers\n\t\tpx.Go(bs.provideWorker)\n\t}\n}\n\nfunc (bs *Server) taskWorker(ctx context.Context, id int) {\n\tdefer log.Debug(\"bitswap task worker shutting down...\")\n\tlog := log.With(\"ID\", id)\n\tfor {\n\t\tlog.Debug(\"Bitswap.TaskWorker.Loop\")\n\t\tselect {\n\t\tcase nextEnvelope := <-bs.engine.Outbox():\n\t\t\tselect {\n\t\t\tcase envelope, ok := <-nextEnvelope:\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tstart := time.Now()\n\n\t\t\t\t// TODO: Only record message as sent if there was no error?\n\t\t\t\t// Ideally, yes. But we'd need some way to trigger a retry and/or drop\n\t\t\t\t// the peer.\n\t\t\t\tbs.engine.MessageSent(envelope.Peer, envelope.Message)\n\t\t\t\tif bs.tracer != nil {\n\t\t\t\t\tbs.tracer.MessageSent(envelope.Peer, envelope.Message)\n\t\t\t\t}\n\t\t\t\tbs.sendBlocks(ctx, envelope)\n\n\t\t\t\tdur := time.Since(start)\n\t\t\t\tbs.sendTimeHistogram.Observe(dur.Seconds())\n\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (bs *Server) logOutgoingBlocks(env *decision.Envelope) {\n\tif ce := sflog.Check(zap.DebugLevel, \"sent message\"); ce == nil {\n\t\treturn\n\t}\n\n\tself := bs.network.Self()\n\n\tfor _, blockPresence := range env.Message.BlockPresences() {\n\t\tc := blockPresence.Cid\n\t\tswitch blockPresence.Type {\n\t\tcase pb.Message_Have:\n\t\t\tlog.Debugw(\"sent message\",\n\t\t\t\t\"type\", \"HAVE\",\n\t\t\t\t\"cid\", c,\n\t\t\t\t\"local\", self,\n\t\t\t\t\"to\", env.Peer,\n\t\t\t)\n\t\tcase pb.Message_DontHave:\n\t\t\tlog.Debugw(\"sent message\",\n\t\t\t\t\"type\", \"DONT_HAVE\",\n\t\t\t\t\"cid\", c,\n\t\t\t\t\"local\", self,\n\t\t\t\t\"to\", env.Peer,\n\t\t\t)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unrecognized BlockPresence type %v\", blockPresence.Type))\n\t\t}\n\n\t}\n\tfor _, block := range env.Message.Blocks() {\n\t\tlog.Debugw(\"sent message\",\n\t\t\t\"type\", \"BLOCK\",\n\t\t\t\"cid\", block.Cid(),\n\t\t\t\"local\", self,\n\t\t\t\"to\", env.Peer,\n\t\t)\n\t}\n}\n\nfunc (bs *Server) sendBlocks(ctx context.Context, env *decision.Envelope) {\n\t// Blocks need to be sent synchronously to maintain proper backpressure\n\t// throughout the network stack\n\tdefer env.Sent()\n\n\terr := bs.network.SendMessage(ctx, env.Peer, env.Message)\n\tif err != nil {\n\t\tlog.Debugw(\"failed to send blocks message\",\n\t\t\t\"peer\", env.Peer,\n\t\t\t\"error\", err,\n\t\t)\n\t\treturn\n\t}\n\n\tbs.logOutgoingBlocks(env)\n\n\tdataSent := 0\n\tblocks := env.Message.Blocks()\n\tfor _, b := range blocks {\n\t\tdataSent += len(b.RawData())\n\t}\n\tbs.counterLk.Lock()\n\tbs.counters.BlocksSent += uint64(len(blocks))\n\tbs.counters.DataSent += uint64(dataSent)\n\tbs.counterLk.Unlock()\n\tbs.sentHistogram.Observe(float64(env.Message.Size()))\n\tlog.Debugw(\"sent message\", \"peer\", env.Peer)\n}\n\ntype Stat struct {\n\tPeers         []string\n\tProvideBufLen int\n\tBlocksSent    uint64\n\tDataSent      uint64\n}\n\n// Stat returns aggregated statistics about bitswap operations\nfunc (bs *Server) Stat() (Stat, error) {\n\tbs.counterLk.Lock()\n\ts := bs.counters\n\tbs.counterLk.Unlock()\n\ts.ProvideBufLen = len(bs.newBlocks)\n\n\tpeers := bs.engine.Peers()\n\tpeersStr := make([]string, len(peers))\n\tfor i, p := range peers {\n\t\tpeersStr[i] = p.Pretty()\n\t}\n\tsort.Strings(peersStr)\n\ts.Peers = peersStr\n\n\treturn s, nil\n}\n\n// NotifyNewBlocks announces the existence of blocks to this bitswap service. The\n// service will potentially notify its peers.\n// Bitswap itself doesn't store new blocks. It's the caller responsibility to ensure\n// that those blocks are available in the blockstore before calling this function.\nfunc (bs *Server) NotifyNewBlocks(ctx context.Context, blks ...blocks.Block) error {\n\tselect {\n\tcase <-bs.process.Closing():\n\t\treturn errors.New(\"bitswap is closed\")\n\tdefault:\n\t}\n\n\t// Send wanted blocks to decision engine\n\tbs.engine.NotifyNewBlocks(blks)\n\n\t// If the reprovider is enabled, send block to reprovider\n\tif bs.provideEnabled {\n\t\tfor _, blk := range blks {\n\t\t\tselect {\n\t\t\tcase bs.newBlocks <- blk.Cid():\n\t\t\t\t// send block off to be reprovided\n\t\t\tcase <-bs.process.Closing():\n\t\t\t\treturn bs.process.Close()\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (bs *Server) provideCollector(ctx context.Context) {\n\tdefer close(bs.provideKeys)\n\tvar toProvide []cid.Cid\n\tvar nextKey cid.Cid\n\tvar keysOut chan cid.Cid\n\n\tfor {\n\t\tselect {\n\t\tcase blkey, ok := <-bs.newBlocks:\n\t\t\tif !ok {\n\t\t\t\tlog.Debug(\"newBlocks channel closed\")\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif keysOut == nil {\n\t\t\t\tnextKey = blkey\n\t\t\t\tkeysOut = bs.provideKeys\n\t\t\t} else {\n\t\t\t\ttoProvide = append(toProvide, blkey)\n\t\t\t}\n\t\tcase keysOut <- nextKey:\n\t\t\tif len(toProvide) > 0 {\n\t\t\t\tnextKey = toProvide[0]\n\t\t\t\ttoProvide = toProvide[1:]\n\t\t\t} else {\n\t\t\t\tkeysOut = nil\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (bs *Server) provideWorker(px process.Process) {\n\t// FIXME: OnClosingContext returns a _custom_ context type.\n\t// Unfortunately, deriving a new cancelable context from this custom\n\t// type fires off a goroutine. To work around this, we create a single\n\t// cancelable context up-front and derive all sub-contexts from that.\n\t//\n\t// See: https://github.com/ipfs/go-ipfs/issues/5810\n\tctx := procctx.OnClosingContext(px)\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\n\tlimit := make(chan struct{}, provideWorkerMax)\n\n\tlimitedGoProvide := func(k cid.Cid, wid int) {\n\t\tdefer func() {\n\t\t\t// replace token when done\n\t\t\t<-limit\n\t\t}()\n\n\t\tlog.Debugw(\"Bitswap.ProvideWorker.Start\", \"ID\", wid, \"cid\", k)\n\t\tdefer log.Debugw(\"Bitswap.ProvideWorker.End\", \"ID\", wid, \"cid\", k)\n\n\t\tctx, cancel := context.WithTimeout(ctx, defaults.ProvideTimeout) // timeout ctx\n\t\tdefer cancel()\n\n\t\tif err := bs.network.Provide(ctx, k); err != nil {\n\t\t\tlog.Warn(err)\n\t\t}\n\t}\n\n\t// worker spawner, reads from bs.provideKeys until it closes, spawning a\n\t// _ratelimited_ number of workers to handle each key.\n\tfor wid := 2; ; wid++ {\n\t\tlog.Debug(\"Bitswap.ProvideWorker.Loop\")\n\n\t\tselect {\n\t\tcase <-px.Closing():\n\t\t\treturn\n\t\tcase k, ok := <-bs.provideKeys:\n\t\t\tif !ok {\n\t\t\t\tlog.Debug(\"provideKeys channel closed\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-px.Closing():\n\t\t\t\treturn\n\t\t\tcase limit <- struct{}{}:\n\t\t\t\tgo limitedGoProvide(k, wid)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (bs *Server) ReceiveMessage(ctx context.Context, p peer.ID, incoming message.BitSwapMessage) {\n\t// This call records changes to wantlists, blocks received,\n\t// and number of bytes transfered.\n\tmustKillConnection := bs.engine.MessageReceived(ctx, p, incoming)\n\tif mustKillConnection {\n\t\tbs.network.DisconnectFrom(ctx, p)\n\t}\n\t// TODO: this is bad, and could be easily abused.\n\t// Should only track *useful* messages in ledger\n\n\tif bs.tracer != nil {\n\t\tbs.tracer.MessageReceived(p, incoming)\n\t}\n}\n\n// ReceivedBlocks notify the decision engine that a peer is well behaving\n// and gave us usefull data, potentially increasing it's score and making us\n// send them more data in exchange.\nfunc (bs *Server) ReceivedBlocks(from peer.ID, blks []blocks.Block) {\n\tbs.engine.ReceivedBlocks(from, blks)\n}\n\nfunc (*Server) ReceiveError(err error) {\n\tlog.Infof(\"Bitswap Client ReceiveError: %s\", err)\n\t// TODO log the network error\n\t// TODO bubble the network error up to the parent context/error logger\n\n}\nfunc (bs *Server) PeerConnected(p peer.ID) {\n\tbs.engine.PeerConnected(p)\n}\nfunc (bs *Server) PeerDisconnected(p peer.ID) {\n\tbs.engine.PeerDisconnected(p)\n}\n\n// Close is called to shutdown the Client\nfunc (bs *Server) Close() error {\n\treturn bs.process.Close()\n}\n"], "filenames": ["bitswap/internal/defaults/defaults.go", "bitswap/network/ipfs_impl.go", "bitswap/options.go", "bitswap/server/internal/decision/engine.go", "bitswap/server/server.go"], "buggy_code_start_loc": [3, 373, 33, 27, 217], "buggy_code_end_loc": [29, 374, 33, 767, 515], "fixing_code_start_loc": [4, 373, 34, 28, 218], "fixing_code_end_loc": [36, 374, 40, 809, 529], "type": "CWE-770", "message": "Boxo, formerly known as go-libipfs, is a library for building IPFS applications and implementations. In versions 0.4.0 and 0.5.0, if an attacker is able allocate arbitrary many bytes in the Bitswap server, those allocations are lasting even if the connection is closed. This affects users accepting untrusted connections with the Bitswap server and also affects users using the old API stubs at `github.com/ipfs/go-libipfs/bitswap` because users then transitively import `github.com/ipfs/go-libipfs/bitswap/server`. Boxo versions 0.6.0 and 0.4.1 contain a patch for this issue. As a workaround, those who are using the stub object at `github.com/ipfs/go-libipfs/bitswap` not taking advantage of the features provided by the server can refactor their code to use the new split API that will allow them to run in a client only mode: `github.com/ipfs/go-libipfs/bitswap/client`.", "other": {"cve": {"id": "CVE-2023-25568", "sourceIdentifier": "security-advisories@github.com", "published": "2023-05-10T14:15:32.187", "lastModified": "2023-05-19T01:53:27.043", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Boxo, formerly known as go-libipfs, is a library for building IPFS applications and implementations. In versions 0.4.0 and 0.5.0, if an attacker is able allocate arbitrary many bytes in the Bitswap server, those allocations are lasting even if the connection is closed. This affects users accepting untrusted connections with the Bitswap server and also affects users using the old API stubs at `github.com/ipfs/go-libipfs/bitswap` because users then transitively import `github.com/ipfs/go-libipfs/bitswap/server`. Boxo versions 0.6.0 and 0.4.1 contain a patch for this issue. As a workaround, those who are using the stub object at `github.com/ipfs/go-libipfs/bitswap` not taking advantage of the features provided by the server can refactor their code to use the new split API that will allow them to run in a client only mode: `github.com/ipfs/go-libipfs/bitswap/client`."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:L/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "HIGH", "baseScore": 8.2, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 4.2}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-770"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-400"}, {"lang": "en", "value": "CWE-770"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:protocol:boxo:0.4.0:*:*:*:*:go:*:*", "matchCriteriaId": "E5D4F0B2-1C1A-4BBB-B133-EB87E61AD2B3"}, {"vulnerable": true, "criteria": "cpe:2.3:a:protocol:boxo:0.5.0:*:*:*:*:go:*:*", "matchCriteriaId": "ECB93B03-A1E1-4A87-994C-2AC70D3FF5CA"}]}]}], "references": [{"url": "https://github.com/ipfs/boxo/commit/62cbac40b96f49e39cd7fedc77ee6b56adce4916", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/ipfs/boxo/commit/9cb5cb54d40b57084d1221ba83b9e6bb3fcc3197", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/ipfs/boxo/commit/baa748b682fabb21a4c1f7628a8af348d4645974", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/ipfs/go-libipfs/security/advisories/GHSA-m974-xj4j-7qv5", "source": "security-advisories@github.com", "tags": ["Mitigation", "Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/ipfs/boxo/commit/62cbac40b96f49e39cd7fedc77ee6b56adce4916"}}