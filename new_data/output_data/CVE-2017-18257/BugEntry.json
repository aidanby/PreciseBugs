{"buggy_code": ["/*\n * fs/f2fs/data.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/buffer_head.h>\n#include <linux/mpage.h>\n#include <linux/writeback.h>\n#include <linux/backing-dev.h>\n#include <linux/pagevec.h>\n#include <linux/blkdev.h>\n#include <linux/bio.h>\n#include <linux/prefetch.h>\n#include <linux/uio.h>\n#include <linux/mm.h>\n#include <linux/memcontrol.h>\n#include <linux/cleancache.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"trace.h\"\n#include <trace/events/f2fs.h>\n\nstatic bool __is_cp_guaranteed(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode;\n\tstruct f2fs_sb_info *sbi;\n\n\tif (!mapping)\n\t\treturn false;\n\n\tinode = mapping->host;\n\tsbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_META_INO(sbi) ||\n\t\t\tinode->i_ino ==  F2FS_NODE_INO(sbi) ||\n\t\t\tS_ISDIR(inode->i_mode) ||\n\t\t\tis_cold_data(page))\n\t\treturn true;\n\treturn false;\n}\n\nstatic void f2fs_read_end_io(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(F2FS_P_SB(bio->bi_io_vec->bv_page), FAULT_IO))\n\t\tbio->bi_error = -EIO;\n#endif\n\n\tif (f2fs_bio_encrypted(bio)) {\n\t\tif (bio->bi_error) {\n\t\t\tfscrypt_release_ctx(bio->bi_private);\n\t\t} else {\n\t\t\tfscrypt_decrypt_bio_pages(bio->bi_private, bio);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tif (!bio->bi_error) {\n\t\t\tif (!PageUptodate(page))\n\t\t\t\tSetPageUptodate(page);\n\t\t} else {\n\t\t\tClearPageUptodate(page);\n\t\t\tSetPageError(page);\n\t\t}\n\t\tunlock_page(page);\n\t}\n\tbio_put(bio);\n}\n\nstatic void f2fs_write_end_io(struct bio *bio)\n{\n\tstruct f2fs_sb_info *sbi = bio->bi_private;\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\t\tenum count_type type = WB_DATA_TYPE(page);\n\n\t\tif (IS_DUMMY_WRITTEN_PAGE(page)) {\n\t\t\tset_page_private(page, (unsigned long)NULL);\n\t\t\tClearPagePrivate(page);\n\t\t\tunlock_page(page);\n\t\t\tmempool_free(page, sbi->write_io_dummy);\n\n\t\t\tif (unlikely(bio->bi_error))\n\t\t\t\tf2fs_stop_checkpoint(sbi, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfscrypt_pullback_bio_page(&page, true);\n\n\t\tif (unlikely(bio->bi_error)) {\n\t\t\tmapping_set_error(page->mapping, -EIO);\n\t\t\tf2fs_stop_checkpoint(sbi, true);\n\t\t}\n\t\tdec_page_count(sbi, type);\n\t\tclear_cold_data(page);\n\t\tend_page_writeback(page);\n\t}\n\tif (!get_pages(sbi, F2FS_WB_CP_DATA) &&\n\t\t\t\twq_has_sleeper(&sbi->cp_wait))\n\t\twake_up(&sbi->cp_wait);\n\n\tbio_put(bio);\n}\n\n/*\n * Return true, if pre_bio's bdev is same as its target device.\n */\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blk_addr, struct bio *bio)\n{\n\tstruct block_device *bdev = sbi->sb->s_bdev;\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tif (FDEV(i).start_blk <= blk_addr &&\n\t\t\t\t\tFDEV(i).end_blk >= blk_addr) {\n\t\t\tblk_addr -= FDEV(i).start_blk;\n\t\t\tbdev = FDEV(i).bdev;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (bio) {\n\t\tbio->bi_bdev = bdev;\n\t\tbio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);\n\t}\n\treturn bdev;\n}\n\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (FDEV(i).start_blk <= blkaddr && FDEV(i).end_blk >= blkaddr)\n\t\t\treturn i;\n\treturn 0;\n}\n\nstatic bool __same_bdev(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blk_addr, struct bio *bio)\n{\n\treturn f2fs_target_device(sbi, blk_addr, NULL) == bio->bi_bdev;\n}\n\n/*\n * Low-level block read/write IO operations.\n */\nstatic struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,\n\t\t\t\tint npages, bool is_read)\n{\n\tstruct bio *bio;\n\n\tbio = f2fs_bio_alloc(npages);\n\n\tf2fs_target_device(sbi, blk_addr, bio);\n\tbio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;\n\tbio->bi_private = is_read ? NULL : sbi;\n\n\treturn bio;\n}\n\nstatic inline void __submit_bio(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct bio *bio, enum page_type type)\n{\n\tif (!is_read_io(bio_op(bio))) {\n\t\tunsigned int start;\n\n\t\tif (f2fs_sb_mounted_blkzoned(sbi->sb) &&\n\t\t\tcurrent->plug && (type == DATA || type == NODE))\n\t\t\tblk_finish_plug(current->plug);\n\n\t\tif (type != DATA && type != NODE)\n\t\t\tgoto submit_io;\n\n\t\tstart = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;\n\t\tstart %= F2FS_IO_SIZE(sbi);\n\n\t\tif (start == 0)\n\t\t\tgoto submit_io;\n\n\t\t/* fill dummy pages */\n\t\tfor (; start < F2FS_IO_SIZE(sbi); start++) {\n\t\t\tstruct page *page =\n\t\t\t\tmempool_alloc(sbi->write_io_dummy,\n\t\t\t\t\tGFP_NOIO | __GFP_ZERO | __GFP_NOFAIL);\n\t\t\tf2fs_bug_on(sbi, !page);\n\n\t\t\tSetPagePrivate(page);\n\t\t\tset_page_private(page, (unsigned long)DUMMY_WRITTEN_PAGE);\n\t\t\tlock_page(page);\n\t\t\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE)\n\t\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n\t\t/*\n\t\t * In the NODE case, we lose next block address chain. So, we\n\t\t * need to do checkpoint in f2fs_sync_file.\n\t\t */\n\t\tif (type == NODE)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_CP);\n\t}\nsubmit_io:\n\tif (is_read_io(bio_op(bio)))\n\t\ttrace_f2fs_submit_read_bio(sbi->sb, type, bio);\n\telse\n\t\ttrace_f2fs_submit_write_bio(sbi->sb, type, bio);\n\tsubmit_bio(bio);\n}\n\nstatic void __submit_merged_bio(struct f2fs_bio_info *io)\n{\n\tstruct f2fs_io_info *fio = &io->fio;\n\n\tif (!io->bio)\n\t\treturn;\n\n\tbio_set_op_attrs(io->bio, fio->op, fio->op_flags);\n\n\tif (is_read_io(fio->op))\n\t\ttrace_f2fs_prepare_read_bio(io->sbi->sb, fio->type, io->bio);\n\telse\n\t\ttrace_f2fs_prepare_write_bio(io->sbi->sb, fio->type, io->bio);\n\n\t__submit_bio(io->sbi, io->bio, fio->type);\n\tio->bio = NULL;\n}\n\nstatic bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,\n\t\t\t\t\t\tstruct page *page, nid_t ino)\n{\n\tstruct bio_vec *bvec;\n\tstruct page *target;\n\tint i;\n\n\tif (!io->bio)\n\t\treturn false;\n\n\tif (!inode && !page && !ino)\n\t\treturn true;\n\n\tbio_for_each_segment_all(bvec, io->bio, i) {\n\n\t\tif (bvec->bv_page->mapping)\n\t\t\ttarget = bvec->bv_page;\n\t\telse\n\t\t\ttarget = fscrypt_control_page(bvec->bv_page);\n\n\t\tif (inode && inode == target->mapping->host)\n\t\t\treturn true;\n\t\tif (page && page == target)\n\t\t\treturn true;\n\t\tif (ino && ino == ino_of_node(target))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,\n\t\t\t\t\t\tstruct page *page, nid_t ino,\n\t\t\t\t\t\tenum page_type type)\n{\n\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\tstruct f2fs_bio_info *io = &sbi->write_io[btype];\n\tbool ret;\n\n\tdown_read(&io->io_rwsem);\n\tret = __has_merged_page(io, inode, page, ino);\n\tup_read(&io->io_rwsem);\n\treturn ret;\n}\n\nstatic void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type, int rw)\n{\n\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\tstruct f2fs_bio_info *io;\n\n\tio = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];\n\n\tdown_write(&io->io_rwsem);\n\n\tif (!__has_merged_page(io, inode, page, ino))\n\t\tgoto out;\n\n\t/* change META to META_FLUSH in the checkpoint procedure */\n\tif (type >= META_FLUSH) {\n\t\tio->fio.type = META_FLUSH;\n\t\tio->fio.op = REQ_OP_WRITE;\n\t\tio->fio.op_flags = REQ_PREFLUSH | REQ_META | REQ_PRIO;\n\t\tif (!test_opt(sbi, NOBARRIER))\n\t\t\tio->fio.op_flags |= REQ_FUA;\n\t}\n\t__submit_merged_bio(io);\nout:\n\tup_write(&io->io_rwsem);\n}\n\nvoid f2fs_submit_merged_bio(struct f2fs_sb_info *sbi, enum page_type type,\n\t\t\t\t\t\t\t\t\tint rw)\n{\n\t__f2fs_submit_merged_bio(sbi, NULL, NULL, 0, type, rw);\n}\n\nvoid f2fs_submit_merged_bio_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type, int rw)\n{\n\tif (has_merged_page(sbi, inode, page, ino, type))\n\t\t__f2fs_submit_merged_bio(sbi, inode, page, ino, type, rw);\n}\n\nvoid f2fs_flush_merged_bios(struct f2fs_sb_info *sbi)\n{\n\tf2fs_submit_merged_bio(sbi, DATA, WRITE);\n\tf2fs_submit_merged_bio(sbi, NODE, WRITE);\n\tf2fs_submit_merged_bio(sbi, META, WRITE);\n}\n\n/*\n * Fill the locked page with data located in the block address.\n * Return unlocked page.\n */\nint f2fs_submit_page_bio(struct f2fs_io_info *fio)\n{\n\tstruct bio *bio;\n\tstruct page *page = fio->encrypted_page ?\n\t\t\tfio->encrypted_page : fio->page;\n\n\ttrace_f2fs_submit_page_bio(page, fio);\n\tf2fs_trace_ios(fio, 0);\n\n\t/* Allocate a new bio */\n\tbio = __bio_alloc(fio->sbi, fio->new_blkaddr, 1, is_read_io(fio->op));\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\tbio_put(bio);\n\t\treturn -EFAULT;\n\t}\n\tbio_set_op_attrs(bio, fio->op, fio->op_flags);\n\n\t__submit_bio(fio->sbi, bio, fio->type);\n\treturn 0;\n}\n\nint f2fs_submit_page_mbio(struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tenum page_type btype = PAGE_TYPE_OF_BIO(fio->type);\n\tstruct f2fs_bio_info *io;\n\tbool is_read = is_read_io(fio->op);\n\tstruct page *bio_page;\n\tint err = 0;\n\n\tio = is_read ? &sbi->read_io : &sbi->write_io[btype];\n\n\tif (fio->old_blkaddr != NEW_ADDR)\n\t\tverify_block_addr(sbi, fio->old_blkaddr);\n\tverify_block_addr(sbi, fio->new_blkaddr);\n\n\tbio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;\n\n\tif (!is_read)\n\t\tinc_page_count(sbi, WB_DATA_TYPE(bio_page));\n\n\tdown_write(&io->io_rwsem);\n\n\tif (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||\n\t    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags) ||\n\t\t\t!__same_bdev(sbi, fio->new_blkaddr, io->bio)))\n\t\t__submit_merged_bio(io);\nalloc_new:\n\tif (io->bio == NULL) {\n\t\tif ((fio->type == DATA || fio->type == NODE) &&\n\t\t\t\tfio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {\n\t\t\terr = -EAGAIN;\n\t\t\tdec_page_count(sbi, WB_DATA_TYPE(bio_page));\n\t\t\tgoto out_fail;\n\t\t}\n\t\tio->bio = __bio_alloc(sbi, fio->new_blkaddr,\n\t\t\t\t\t\tBIO_MAX_PAGES, is_read);\n\t\tio->fio = *fio;\n\t}\n\n\tif (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) <\n\t\t\t\t\t\t\tPAGE_SIZE) {\n\t\t__submit_merged_bio(io);\n\t\tgoto alloc_new;\n\t}\n\n\tio->last_block_in_bio = fio->new_blkaddr;\n\tf2fs_trace_ios(fio, 0);\nout_fail:\n\tup_write(&io->io_rwsem);\n\ttrace_f2fs_submit_page_mbio(fio->page, fio);\n\treturn err;\n}\n\nstatic void __set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tstruct f2fs_node *rn = F2FS_NODE(dn->node_page);\n\t__le32 *addr_array;\n\n\t/* Get physical address of data block */\n\taddr_array = blkaddr_in_node(rn);\n\taddr_array[dn->ofs_in_node] = cpu_to_le32(dn->data_blkaddr);\n}\n\n/*\n * Lock ordering for the change of data block address:\n * ->data_page\n *  ->node_page\n *    update block addresses in the node page\n */\nvoid set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true);\n\t__set_data_blkaddr(dn);\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n}\n\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)\n{\n\tdn->data_blkaddr = blkaddr;\n\tset_data_blkaddr(dn);\n\tf2fs_update_extent_cache(dn);\n}\n\n/* dn->ofs_in_node will be returned with up-to-date last block pointer */\nint reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\n\tif (!count)\n\t\treturn 0;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\tif (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))\n\t\treturn -ENOSPC;\n\n\ttrace_f2fs_reserve_new_blocks(dn->inode, dn->nid,\n\t\t\t\t\t\tdn->ofs_in_node, count);\n\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true);\n\n\tfor (; count > 0; dn->ofs_in_node++) {\n\t\tblock_t blkaddr =\n\t\t\tdatablock_addr(dn->node_page, dn->ofs_in_node);\n\t\tif (blkaddr == NULL_ADDR) {\n\t\t\tdn->data_blkaddr = NEW_ADDR;\n\t\t\t__set_data_blkaddr(dn);\n\t\t\tcount--;\n\t\t}\n\t}\n\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n\treturn 0;\n}\n\n/* Should keep dn->ofs_in_node unchanged */\nint reserve_new_block(struct dnode_of_data *dn)\n{\n\tunsigned int ofs_in_node = dn->ofs_in_node;\n\tint ret;\n\n\tret = reserve_new_blocks(dn, 1);\n\tdn->ofs_in_node = ofs_in_node;\n\treturn ret;\n}\n\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tbool need_put = dn->inode_page ? false : true;\n\tint err;\n\n\terr = get_dnode_of_data(dn, index, ALLOC_NODE);\n\tif (err)\n\t\treturn err;\n\n\tif (dn->data_blkaddr == NULL_ADDR)\n\t\terr = reserve_new_block(dn);\n\tif (err || need_put)\n\t\tf2fs_put_dnode(dn);\n\treturn err;\n}\n\nint f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tstruct extent_info ei;\n\tstruct inode *inode = dn->inode;\n\n\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\tdn->data_blkaddr = ei.blk + index - ei.fofs;\n\t\treturn 0;\n\t}\n\n\treturn f2fs_reserve_block(dn, index);\n}\n\nstruct page *get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\tint op_flags, bool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct dnode_of_data dn;\n\tstruct page *page;\n\tstruct extent_info ei;\n\tint err;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = F2FS_I_SB(inode),\n\t\t.type = DATA,\n\t\t.op = REQ_OP_READ,\n\t\t.op_flags = op_flags,\n\t\t.encrypted_page = NULL,\n\t};\n\n\tif (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))\n\t\treturn read_mapping_page(mapping, index, NULL);\n\n\tpage = f2fs_grab_cache_page(mapping, index, for_write);\n\tif (!page)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\tdn.data_blkaddr = ei.blk + index - ei.fofs;\n\t\tgoto got_it;\n\t}\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\tif (err)\n\t\tgoto put_err;\n\tf2fs_put_dnode(&dn);\n\n\tif (unlikely(dn.data_blkaddr == NULL_ADDR)) {\n\t\terr = -ENOENT;\n\t\tgoto put_err;\n\t}\ngot_it:\n\tif (PageUptodate(page)) {\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\t/*\n\t * A new dentry page is allocated but not able to be written, since its\n\t * new inode page couldn't be allocated due to -ENOSPC.\n\t * In such the case, its blkaddr can be remained as NEW_ADDR.\n\t * see, f2fs_add_link -> get_new_data_page -> init_inode_metadata.\n\t */\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\tfio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;\n\tfio.page = page;\n\terr = f2fs_submit_page_bio(&fio);\n\tif (err)\n\t\tgoto put_err;\n\treturn page;\n\nput_err:\n\tf2fs_put_page(page, 1);\n\treturn ERR_PTR(err);\n}\n\nstruct page *find_data_page(struct inode *inode, pgoff_t index)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\n\tpage = find_get_page(mapping, index);\n\tif (page && PageUptodate(page))\n\t\treturn page;\n\tf2fs_put_page(page, 0);\n\n\tpage = get_read_data_page(inode, index, 0, false);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\tif (PageUptodate(page))\n\t\treturn page;\n\n\twait_on_page_locked(page);\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 0);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n/*\n * If it tries to access a hole, return an error.\n * Because, the callers, functions in dir.c and GC, should be able to know\n * whether this page exists or not.\n */\nstruct page *get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\t\tbool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\nrepeat:\n\tpage = get_read_data_page(inode, index, 0, for_write);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\t/* wait for read completion */\n\tlock_page(page);\n\tif (unlikely(page->mapping != mapping)) {\n\t\tf2fs_put_page(page, 1);\n\t\tgoto repeat;\n\t}\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n/*\n * Caller ensures that this data page is never allocated.\n * A new zero-filled data page is allocated in the page cache.\n *\n * Also, caller should grab and release a rwsem by calling f2fs_lock_op() and\n * f2fs_unlock_op().\n * Note that, ipage is set only by make_empty_dir, and if any error occur,\n * ipage should be released by this function.\n */\nstruct page *get_new_data_page(struct inode *inode,\n\t\tstruct page *ipage, pgoff_t index, bool new_i_size)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\tstruct dnode_of_data dn;\n\tint err;\n\n\tpage = f2fs_grab_cache_page(mapping, index, true);\n\tif (!page) {\n\t\t/*\n\t\t * before exiting, we should make sure ipage will be released\n\t\t * if any error occur.\n\t\t */\n\t\tf2fs_put_page(ipage, 1);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, NULL, 0);\n\terr = f2fs_reserve_block(&dn, index);\n\tif (err) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(err);\n\t}\n\tif (!ipage)\n\t\tf2fs_put_dnode(&dn);\n\n\tif (PageUptodate(page))\n\t\tgoto got_it;\n\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t} else {\n\t\tf2fs_put_page(page, 1);\n\n\t\t/* if ipage exists, blkaddr should be NEW_ADDR */\n\t\tf2fs_bug_on(F2FS_I_SB(inode), ipage);\n\t\tpage = get_lock_data_page(inode, index, true);\n\t\tif (IS_ERR(page))\n\t\t\treturn page;\n\t}\ngot_it:\n\tif (new_i_size && i_size_read(inode) <\n\t\t\t\t((loff_t)(index + 1) << PAGE_SHIFT))\n\t\tf2fs_i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));\n\treturn page;\n}\n\nstatic int __allocate_data_block(struct dnode_of_data *dn)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\tpgoff_t fofs;\n\tblkcnt_t count = 1;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\n\tdn->data_blkaddr = datablock_addr(dn->node_page, dn->ofs_in_node);\n\tif (dn->data_blkaddr == NEW_ADDR)\n\t\tgoto alloc;\n\n\tif (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))\n\t\treturn -ENOSPC;\n\nalloc:\n\tget_node_info(sbi, dn->nid, &ni);\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\n\tallocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,\n\t\t\t\t\t\t&sum, CURSEG_WARM_DATA);\n\tset_data_blkaddr(dn);\n\n\t/* update i_size */\n\tfofs = start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +\n\t\t\t\t\t\t\tdn->ofs_in_node;\n\tif (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_SHIFT))\n\t\tf2fs_i_size_write(dn->inode,\n\t\t\t\t((loff_t)(fofs + 1) << PAGE_SHIFT));\n\treturn 0;\n}\n\nstatic inline bool __force_buffered_io(struct inode *inode, int rw)\n{\n\treturn ((f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) ||\n\t\t\t(rw == WRITE && test_opt(F2FS_I_SB(inode), LFS)) ||\n\t\t\tF2FS_I_SB(inode)->s_ndevs);\n}\n\nint f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tstruct f2fs_map_blocks map;\n\tint err = 0;\n\n\tif (is_inode_flag_set(inode, FI_NO_PREALLOC))\n\t\treturn 0;\n\n\tmap.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);\n\tmap.m_len = F2FS_BYTES_TO_BLK(iocb->ki_pos + iov_iter_count(from));\n\tif (map.m_len > map.m_lblk)\n\t\tmap.m_len -= map.m_lblk;\n\telse\n\t\tmap.m_len = 0;\n\n\tmap.m_next_pgofs = NULL;\n\n\tif (iocb->ki_flags & IOCB_DIRECT) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\treturn err;\n\t\treturn f2fs_map_blocks(inode, &map, 1,\n\t\t\t__force_buffered_io(inode, WRITE) ?\n\t\t\t\tF2FS_GET_BLOCK_PRE_AIO :\n\t\t\t\tF2FS_GET_BLOCK_PRE_DIO);\n\t}\n\tif (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (!f2fs_has_inline_data(inode))\n\t\treturn f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_AIO);\n\treturn err;\n}\n\n/*\n * f2fs_map_blocks() now supported readahead/bmap/rw direct_IO with\n * f2fs_map_blocks structure.\n * If original data blocks are allocated, then give them to blockdev.\n * Otherwise,\n *     a. preallocate requested block addresses\n *     b. do not use extent cache for better performance\n *     c. give the block addresses to blockdev\n */\nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,\n\t\t\t\t\t\tint create, int flag)\n{\n\tunsigned int maxblocks = map->m_len;\n\tstruct dnode_of_data dn;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint mode = create ? ALLOC_NODE : LOOKUP_NODE;\n\tpgoff_t pgofs, end_offset, end;\n\tint err = 0, ofs = 1;\n\tunsigned int ofs_in_node, last_ofs_in_node;\n\tblkcnt_t prealloc;\n\tstruct extent_info ei;\n\tblock_t blkaddr;\n\n\tif (!maxblocks)\n\t\treturn 0;\n\n\tmap->m_len = 0;\n\tmap->m_flags = 0;\n\n\t/* it only supports block size == page size */\n\tpgofs =\t(pgoff_t)map->m_lblk;\n\tend = pgofs + maxblocks;\n\n\tif (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {\n\t\tmap->m_pblk = ei.blk + pgofs - ei.fofs;\n\t\tmap->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);\n\t\tmap->m_flags = F2FS_MAP_MAPPED;\n\t\tgoto out;\n\t}\n\nnext_dnode:\n\tif (create)\n\t\tf2fs_lock_op(sbi);\n\n\t/* When reading holes, we need its node page */\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = get_dnode_of_data(&dn, pgofs, mode);\n\tif (err) {\n\t\tif (flag == F2FS_GET_BLOCK_BMAP)\n\t\t\tmap->m_pblk = 0;\n\t\tif (err == -ENOENT) {\n\t\t\terr = 0;\n\t\t\tif (map->m_next_pgofs)\n\t\t\t\t*map->m_next_pgofs =\n\t\t\t\t\tget_next_page_offset(&dn, pgofs);\n\t\t}\n\t\tgoto unlock_out;\n\t}\n\n\tprealloc = 0;\n\tlast_ofs_in_node = ofs_in_node = dn.ofs_in_node;\n\tend_offset = ADDRS_PER_PAGE(dn.node_page, inode);\n\nnext_block:\n\tblkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {\n\t\tif (create) {\n\t\t\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag == F2FS_GET_BLOCK_PRE_AIO) {\n\t\t\t\tif (blkaddr == NULL_ADDR) {\n\t\t\t\t\tprealloc++;\n\t\t\t\t\tlast_ofs_in_node = dn.ofs_in_node;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = __allocate_data_block(&dn);\n\t\t\t\tif (!err)\n\t\t\t\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tgoto sync_out;\n\t\t\tmap->m_flags = F2FS_MAP_NEW;\n\t\t\tblkaddr = dn.data_blkaddr;\n\t\t} else {\n\t\t\tif (flag == F2FS_GET_BLOCK_BMAP) {\n\t\t\t\tmap->m_pblk = 0;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag == F2FS_GET_BLOCK_FIEMAP &&\n\t\t\t\t\t\tblkaddr == NULL_ADDR) {\n\t\t\t\tif (map->m_next_pgofs)\n\t\t\t\t\t*map->m_next_pgofs = pgofs + 1;\n\t\t\t}\n\t\t\tif (flag != F2FS_GET_BLOCK_FIEMAP ||\n\t\t\t\t\t\tblkaddr != NEW_ADDR)\n\t\t\t\tgoto sync_out;\n\t\t}\n\t}\n\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO)\n\t\tgoto skip;\n\n\tif (map->m_len == 0) {\n\t\t/* preallocated unwritten block should be mapped for fiemap. */\n\t\tif (blkaddr == NEW_ADDR)\n\t\t\tmap->m_flags |= F2FS_MAP_UNWRITTEN;\n\t\tmap->m_flags |= F2FS_MAP_MAPPED;\n\n\t\tmap->m_pblk = blkaddr;\n\t\tmap->m_len = 1;\n\t} else if ((map->m_pblk != NEW_ADDR &&\n\t\t\tblkaddr == (map->m_pblk + ofs)) ||\n\t\t\t(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||\n\t\t\tflag == F2FS_GET_BLOCK_PRE_DIO) {\n\t\tofs++;\n\t\tmap->m_len++;\n\t} else {\n\t\tgoto sync_out;\n\t}\n\nskip:\n\tdn.ofs_in_node++;\n\tpgofs++;\n\n\t/* preallocate blocks in batch for one dnode page */\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO &&\n\t\t\t(pgofs == end || dn.ofs_in_node == end_offset)) {\n\n\t\tdn.ofs_in_node = ofs_in_node;\n\t\terr = reserve_new_blocks(&dn, prealloc);\n\t\tif (err)\n\t\t\tgoto sync_out;\n\n\t\tmap->m_len += dn.ofs_in_node - ofs_in_node;\n\t\tif (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {\n\t\t\terr = -ENOSPC;\n\t\t\tgoto sync_out;\n\t\t}\n\t\tdn.ofs_in_node = end_offset;\n\t}\n\n\tif (pgofs >= end)\n\t\tgoto sync_out;\n\telse if (dn.ofs_in_node < end_offset)\n\t\tgoto next_block;\n\n\tf2fs_put_dnode(&dn);\n\n\tif (create) {\n\t\tf2fs_unlock_op(sbi);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\n\tgoto next_dnode;\n\nsync_out:\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (create) {\n\t\tf2fs_unlock_op(sbi);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\nout:\n\ttrace_f2fs_map_blocks(inode, map, err);\n\treturn err;\n}\n\nstatic int __get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\tstruct f2fs_map_blocks map;\n\tint err;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\tmap.m_next_pgofs = next_pgofs;\n\n\terr = f2fs_map_blocks(inode, &map, create, flag);\n\tif (!err) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\tbh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;\n\t\tbh->b_size = map.m_len << inode->i_blkbits;\n\t}\n\treturn err;\n}\n\nstatic int get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\t\tflag, next_pgofs);\n}\n\nstatic int get_data_block_dio(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\tF2FS_GET_BLOCK_DIO, NULL);\n}\n\nstatic int get_data_block_bmap(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\t/* Block number less than F2FS MAX BLOCKS */\n\tif (unlikely(iblock >= F2FS_I_SB(inode)->max_file_blocks))\n\t\treturn -EFBIG;\n\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\tF2FS_GET_BLOCK_BMAP, NULL);\n}\n\nstatic inline sector_t logical_to_blk(struct inode *inode, loff_t offset)\n{\n\treturn (offset >> inode->i_blkbits);\n}\n\nstatic inline loff_t blk_to_logical(struct inode *inode, sector_t blk)\n{\n\treturn (blk << inode->i_blkbits);\n}\n\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\tu64 start, u64 len)\n{\n\tstruct buffer_head map_bh;\n\tsector_t start_blk, last_blk;\n\tpgoff_t next_pgofs;\n\tu64 logical = 0, phys = 0, size = 0;\n\tu32 flags = 0;\n\tint ret = 0;\n\n\tret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC);\n\tif (ret)\n\t\treturn ret;\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);\n\t\tif (ret != -EAGAIN)\n\t\t\treturn ret;\n\t}\n\n\tinode_lock(inode);\n\n\tif (logical_to_blk(inode, len) == 0)\n\t\tlen = blk_to_logical(inode, 1);\n\n\tstart_blk = logical_to_blk(inode, start);\n\tlast_blk = logical_to_blk(inode, start + len - 1);\n\nnext:\n\tmemset(&map_bh, 0, sizeof(struct buffer_head));\n\tmap_bh.b_size = len;\n\n\tret = get_data_block(inode, start_blk, &map_bh, 0,\n\t\t\t\t\tF2FS_GET_BLOCK_FIEMAP, &next_pgofs);\n\tif (ret)\n\t\tgoto out;\n\n\t/* HOLE */\n\tif (!buffer_mapped(&map_bh)) {\n\t\tstart_blk = next_pgofs;\n\n\t\tif (blk_to_logical(inode, start_blk) < blk_to_logical(inode,\n\t\t\t\t\tF2FS_I_SB(inode)->max_file_blocks))\n\t\t\tgoto prep_next;\n\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\t}\n\n\tif (size) {\n\t\tif (f2fs_encrypted_inode(inode))\n\t\t\tflags |= FIEMAP_EXTENT_DATA_ENCRYPTED;\n\n\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\tphys, size, flags);\n\t}\n\n\tif (start_blk > last_blk || ret)\n\t\tgoto out;\n\n\tlogical = blk_to_logical(inode, start_blk);\n\tphys = blk_to_logical(inode, map_bh.b_blocknr);\n\tsize = map_bh.b_size;\n\tflags = 0;\n\tif (buffer_unwritten(&map_bh))\n\t\tflags = FIEMAP_EXTENT_UNWRITTEN;\n\n\tstart_blk += logical_to_blk(inode, size);\n\nprep_next:\n\tcond_resched();\n\tif (fatal_signal_pending(current))\n\t\tret = -EINTR;\n\telse\n\t\tgoto next;\nout:\n\tif (ret == 1)\n\t\tret = 0;\n\n\tinode_unlock(inode);\n\treturn ret;\n}\n\nstatic struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t unsigned nr_pages)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct fscrypt_ctx *ctx = NULL;\n\tstruct bio *bio;\n\n\tif (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {\n\t\tctx = fscrypt_get_ctx(inode, GFP_NOFS);\n\t\tif (IS_ERR(ctx))\n\t\t\treturn ERR_CAST(ctx);\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);\n\t}\n\n\tbio = bio_alloc(GFP_KERNEL, min_t(int, nr_pages, BIO_MAX_PAGES));\n\tif (!bio) {\n\t\tif (ctx)\n\t\t\tfscrypt_release_ctx(ctx);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio->bi_private = ctx;\n\n\treturn bio;\n}\n\n/*\n * This function was originally taken from fs/mpage.c, and customized for f2fs.\n * Major change was from block_size == page_size in f2fs by default.\n */\nstatic int f2fs_mpage_readpages(struct address_space *mapping,\n\t\t\tstruct list_head *pages, struct page *page,\n\t\t\tunsigned nr_pages)\n{\n\tstruct bio *bio = NULL;\n\tunsigned page_idx;\n\tsector_t last_block_in_bio = 0;\n\tstruct inode *inode = mapping->host;\n\tconst unsigned blkbits = inode->i_blkbits;\n\tconst unsigned blocksize = 1 << blkbits;\n\tsector_t block_in_file;\n\tsector_t last_block;\n\tsector_t last_block_in_file;\n\tsector_t block_nr;\n\tstruct f2fs_map_blocks map;\n\n\tmap.m_pblk = 0;\n\tmap.m_lblk = 0;\n\tmap.m_len = 0;\n\tmap.m_flags = 0;\n\tmap.m_next_pgofs = NULL;\n\n\tfor (page_idx = 0; nr_pages; page_idx++, nr_pages--) {\n\n\t\tprefetchw(&page->flags);\n\t\tif (pages) {\n\t\t\tpage = list_last_entry(pages, struct page, lru);\n\t\t\tlist_del(&page->lru);\n\t\t\tif (add_to_page_cache_lru(page, mapping,\n\t\t\t\t\t\t  page->index,\n\t\t\t\t\t\t  readahead_gfp_mask(mapping)))\n\t\t\t\tgoto next_page;\n\t\t}\n\n\t\tblock_in_file = (sector_t)page->index;\n\t\tlast_block = block_in_file + nr_pages;\n\t\tlast_block_in_file = (i_size_read(inode) + blocksize - 1) >>\n\t\t\t\t\t\t\t\tblkbits;\n\t\tif (last_block > last_block_in_file)\n\t\t\tlast_block = last_block_in_file;\n\n\t\t/*\n\t\t * Map blocks using the previous result first.\n\t\t */\n\t\tif ((map.m_flags & F2FS_MAP_MAPPED) &&\n\t\t\t\tblock_in_file > map.m_lblk &&\n\t\t\t\tblock_in_file < (map.m_lblk + map.m_len))\n\t\t\tgoto got_it;\n\n\t\t/*\n\t\t * Then do more f2fs_map_blocks() calls until we are\n\t\t * done with this page.\n\t\t */\n\t\tmap.m_flags = 0;\n\n\t\tif (block_in_file < last_block) {\n\t\t\tmap.m_lblk = block_in_file;\n\t\t\tmap.m_len = last_block - block_in_file;\n\n\t\t\tif (f2fs_map_blocks(inode, &map, 0,\n\t\t\t\t\t\tF2FS_GET_BLOCK_READ))\n\t\t\t\tgoto set_error_page;\n\t\t}\ngot_it:\n\t\tif ((map.m_flags & F2FS_MAP_MAPPED)) {\n\t\t\tblock_nr = map.m_pblk + block_in_file - map.m_lblk;\n\t\t\tSetPageMappedToDisk(page);\n\n\t\t\tif (!PageUptodate(page) && !cleancache_get_page(page)) {\n\t\t\t\tSetPageUptodate(page);\n\t\t\t\tgoto confused;\n\t\t\t}\n\t\t} else {\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tif (!PageUptodate(page))\n\t\t\t\tSetPageUptodate(page);\n\t\t\tunlock_page(page);\n\t\t\tgoto next_page;\n\t\t}\n\n\t\t/*\n\t\t * This page will go to BIO.  Do we need to send this\n\t\t * BIO off first?\n\t\t */\n\t\tif (bio && (last_block_in_bio != block_nr - 1 ||\n\t\t\t!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {\nsubmit_and_realloc:\n\t\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\t\tbio = NULL;\n\t\t}\n\t\tif (bio == NULL) {\n\t\t\tbio = f2fs_grab_bio(inode, block_nr, nr_pages);\n\t\t\tif (IS_ERR(bio)) {\n\t\t\t\tbio = NULL;\n\t\t\t\tgoto set_error_page;\n\t\t\t}\n\t\t\tbio_set_op_attrs(bio, REQ_OP_READ, 0);\n\t\t}\n\n\t\tif (bio_add_page(bio, page, blocksize, 0) < blocksize)\n\t\t\tgoto submit_and_realloc;\n\n\t\tlast_block_in_bio = block_nr;\n\t\tgoto next_page;\nset_error_page:\n\t\tSetPageError(page);\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tunlock_page(page);\n\t\tgoto next_page;\nconfused:\n\t\tif (bio) {\n\t\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\t\tbio = NULL;\n\t\t}\n\t\tunlock_page(page);\nnext_page:\n\t\tif (pages)\n\t\t\tput_page(page);\n\t}\n\tBUG_ON(pages && !list_empty(pages));\n\tif (bio)\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\treturn 0;\n}\n\nstatic int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page->mapping, NULL, page, 1);\n\treturn ret;\n}\n\nstatic int f2fs_read_data_pages(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct page *page = list_last_entry(pages, struct page, lru);\n\n\ttrace_f2fs_readpages(inode, page, nr_pages);\n\n\t/* If the file has inline data, skip readpages */\n\tif (f2fs_has_inline_data(inode))\n\t\treturn 0;\n\n\treturn f2fs_mpage_readpages(mapping, pages, NULL, nr_pages);\n}\n\nint do_write_data_page(struct f2fs_io_info *fio)\n{\n\tstruct page *page = fio->page;\n\tstruct inode *inode = page->mapping->host;\n\tstruct dnode_of_data dn;\n\tint err = 0;\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);\n\tif (err)\n\t\treturn err;\n\n\tfio->old_blkaddr = dn.data_blkaddr;\n\n\t/* This page is already truncated */\n\tif (fio->old_blkaddr == NULL_ADDR) {\n\t\tClearPageUptodate(page);\n\t\tgoto out_writepage;\n\t}\n\n\tif (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {\n\t\tgfp_t gfp_flags = GFP_NOFS;\n\n\t\t/* wait for GCed encrypted page writeback */\n\t\tf2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),\n\t\t\t\t\t\t\tfio->old_blkaddr);\nretry_encrypt:\n\t\tfio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,\n\t\t\t\t\t\t\tPAGE_SIZE, 0,\n\t\t\t\t\t\t\tfio->page->index,\n\t\t\t\t\t\t\tgfp_flags);\n\t\tif (IS_ERR(fio->encrypted_page)) {\n\t\t\terr = PTR_ERR(fio->encrypted_page);\n\t\t\tif (err == -ENOMEM) {\n\t\t\t\t/* flush pending ios and wait for a while */\n\t\t\t\tf2fs_flush_merged_bios(F2FS_I_SB(inode));\n\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\t\tgfp_flags |= __GFP_NOFAIL;\n\t\t\t\terr = 0;\n\t\t\t\tgoto retry_encrypt;\n\t\t\t}\n\t\t\tgoto out_writepage;\n\t\t}\n\t}\n\n\tset_page_writeback(page);\n\n\t/*\n\t * If current allocation needs SSR,\n\t * it had better in-place writes for updated data.\n\t */\n\tif (unlikely(fio->old_blkaddr != NEW_ADDR &&\n\t\t\t!is_cold_data(page) &&\n\t\t\t!IS_ATOMIC_WRITTEN_PAGE(page) &&\n\t\t\tneed_inplace_update(inode))) {\n\t\trewrite_data_page(fio);\n\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\ttrace_f2fs_do_write_data_page(page, IPU);\n\t} else {\n\t\twrite_data_page(&dn, fio);\n\t\ttrace_f2fs_do_write_data_page(page, OPU);\n\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\tif (page->index == 0)\n\t\t\tset_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);\n\t}\nout_writepage:\n\tf2fs_put_dnode(&dn);\n\treturn err;\n}\n\nstatic int f2fs_write_data_page(struct page *page,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tloff_t i_size = i_size_read(inode);\n\tconst pgoff_t end_index = ((unsigned long long) i_size)\n\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\tloff_t psize = (page->index + 1) << PAGE_SHIFT;\n\tunsigned offset = 0;\n\tbool need_balance_fs = false;\n\tint err = 0;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = wbc_to_write_flags(wbc),\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t};\n\n\ttrace_f2fs_writepage(page, DATA);\n\n\tif (page->index < end_index)\n\t\tgoto write;\n\n\t/*\n\t * If the offset is out-of-range of file size,\n\t * this page does not have to be written to disk.\n\t */\n\toffset = i_size & (PAGE_SIZE - 1);\n\tif ((page->index >= end_index + 1) || !offset)\n\t\tgoto out;\n\n\tzero_user_segment(page, offset, PAGE_SIZE);\nwrite:\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto redirty_out;\n\tif (f2fs_is_drop_cache(inode))\n\t\tgoto out;\n\t/* we should not write 0'th page having journal header */\n\tif (f2fs_is_volatile_file(inode) && (!page->index ||\n\t\t\t(!wbc->for_reclaim &&\n\t\t\tavailable_free_memory(sbi, BASE_CHECK))))\n\t\tgoto redirty_out;\n\n\t/* we should bypass data pages to proceed the kworkder jobs */\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tmapping_set_error(page->mapping, -EIO);\n\t\tgoto out;\n\t}\n\n\t/* Dentry blocks are controlled by checkpoint */\n\tif (S_ISDIR(inode->i_mode)) {\n\t\terr = do_write_data_page(&fio);\n\t\tgoto done;\n\t}\n\n\tif (!wbc->for_reclaim)\n\t\tneed_balance_fs = true;\n\telse if (has_not_enough_free_secs(sbi, 0, 0))\n\t\tgoto redirty_out;\n\n\terr = -EAGAIN;\n\tf2fs_lock_op(sbi);\n\tif (f2fs_has_inline_data(inode))\n\t\terr = f2fs_write_inline_data(inode, page);\n\tif (err == -EAGAIN)\n\t\terr = do_write_data_page(&fio);\n\tif (F2FS_I(inode)->last_disk_size < psize)\n\t\tF2FS_I(inode)->last_disk_size = psize;\n\tf2fs_unlock_op(sbi);\ndone:\n\tif (err && err != -ENOENT)\n\t\tgoto redirty_out;\n\nout:\n\tinode_dec_dirty_pages(inode);\n\tif (err)\n\t\tClearPageUptodate(page);\n\n\tif (wbc->for_reclaim) {\n\t\tf2fs_submit_merged_bio_cond(sbi, NULL, page, 0, DATA, WRITE);\n\t\tremove_dirty_inode(inode);\n\t}\n\n\tunlock_page(page);\n\tf2fs_balance_fs(sbi, need_balance_fs);\n\n\tif (unlikely(f2fs_cp_error(sbi)))\n\t\tf2fs_submit_merged_bio(sbi, DATA, WRITE);\n\n\treturn 0;\n\nredirty_out:\n\tredirty_page_for_writepage(wbc, page);\n\tif (!err)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\n\tunlock_page(page);\n\treturn err;\n}\n\n/*\n * This function was copied from write_cche_pages from mm/page-writeback.c.\n * The major change is making write step of cold data page separately from\n * warm/hot data page.\n */\nstatic int f2fs_write_cache_pages(struct address_space *mapping,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\tint ret = 0;\n\tint done = 0;\n\tstruct pagevec pvec;\n\tint nr_pages;\n\tpgoff_t uninitialized_var(writeback_index);\n\tpgoff_t index;\n\tpgoff_t end;\t\t/* Inclusive */\n\tpgoff_t done_index;\n\tint cycled;\n\tint range_whole = 0;\n\tint tag;\n\tint nwritten = 0;\n\n\tpagevec_init(&pvec, 0);\n\n\tif (wbc->range_cyclic) {\n\t\twriteback_index = mapping->writeback_index; /* prev offset */\n\t\tindex = writeback_index;\n\t\tif (index == 0)\n\t\t\tcycled = 1;\n\t\telse\n\t\t\tcycled = 0;\n\t\tend = -1;\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\t\trange_whole = 1;\n\t\tcycled = 1; /* ignore range_cyclic tests */\n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\tdone_index = index;\n\twhile (!done && (index <= end)) {\n\t\tint i;\n\n\t\tnr_pages = pagevec_lookup_tag(&pvec, mapping, &index, tag,\n\t\t\t      min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tif (page->index > end) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdone_index = page->index;\n\n\t\t\tlock_page(page);\n\n\t\t\tif (unlikely(page->mapping != mapping)) {\ncontinue_unlock:\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!PageDirty(page)) {\n\t\t\t\t/* someone wrote it for us */\n\t\t\t\tgoto continue_unlock;\n\t\t\t}\n\n\t\t\tif (PageWriteback(page)) {\n\t\t\t\tif (wbc->sync_mode != WB_SYNC_NONE)\n\t\t\t\t\tf2fs_wait_on_page_writeback(page,\n\t\t\t\t\t\t\t\tDATA, true);\n\t\t\t\telse\n\t\t\t\t\tgoto continue_unlock;\n\t\t\t}\n\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\tif (!clear_page_dirty_for_io(page))\n\t\t\t\tgoto continue_unlock;\n\n\t\t\tret = mapping->a_ops->writepage(page, wbc);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\t/*\n\t\t\t\t * keep nr_to_write, since vfs uses this to\n\t\t\t\t * get # of written pages.\n\t\t\t\t */\n\t\t\t\tif (ret == AOP_WRITEPAGE_ACTIVATE) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tret = 0;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tdone_index = page->index + 1;\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tnwritten++;\n\t\t\t}\n\n\t\t\tif (--wbc->nr_to_write <= 0 &&\n\t\t\t    wbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n\n\tif (!cycled && !done) {\n\t\tcycled = 1;\n\t\tindex = 0;\n\t\tend = writeback_index - 1;\n\t\tgoto retry;\n\t}\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\tmapping->writeback_index = done_index;\n\n\tif (nwritten)\n\t\tf2fs_submit_merged_bio_cond(F2FS_M_SB(mapping), mapping->host,\n\t\t\t\t\t\t\tNULL, 0, DATA, WRITE);\n\n\treturn ret;\n}\n\nstatic int f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t    struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct blk_plug plug;\n\tint ret;\n\n\t/* deal with chardevs and other special file */\n\tif (!mapping->a_ops->writepage)\n\t\treturn 0;\n\n\t/* skip writing if there is no dirty page in this inode */\n\tif (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)\n\t\treturn 0;\n\n\tif (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&\n\t\t\tget_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&\n\t\t\tavailable_free_memory(sbi, DIRTY_DENTS))\n\t\tgoto skip_write;\n\n\t/* skip writing during file defragment */\n\tif (is_inode_flag_set(inode, FI_DO_DEFRAG))\n\t\tgoto skip_write;\n\n\t/* during POR, we don't need to trigger writepage at all. */\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto skip_write;\n\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\n\tblk_start_plug(&plug);\n\tret = f2fs_write_cache_pages(mapping, wbc);\n\tblk_finish_plug(&plug);\n\t/*\n\t * if some pages were truncated, we cannot guarantee its mapping->host\n\t * to detect pending bios.\n\t */\n\n\tremove_dirty_inode(inode);\n\treturn ret;\n\nskip_write:\n\twbc->pages_skipped += get_dirty_pages(inode);\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\treturn 0;\n}\n\nstatic void f2fs_write_failed(struct address_space *mapping, loff_t to)\n{\n\tstruct inode *inode = mapping->host;\n\tloff_t i_size = i_size_read(inode);\n\n\tif (to > i_size) {\n\t\ttruncate_pagecache(inode, i_size);\n\t\ttruncate_blocks(inode, i_size, true);\n\t}\n}\n\nstatic int prepare_write_begin(struct f2fs_sb_info *sbi,\n\t\t\tstruct page *page, loff_t pos, unsigned len,\n\t\t\tblock_t *blk_addr, bool *node_changed)\n{\n\tstruct inode *inode = page->mapping->host;\n\tpgoff_t index = page->index;\n\tstruct dnode_of_data dn;\n\tstruct page *ipage;\n\tbool locked = false;\n\tstruct extent_info ei;\n\tint err = 0;\n\n\t/*\n\t * we already allocated all the blocks, so we don't need to get\n\t * the block addresses when there is no need to fill the page.\n\t */\n\tif (!f2fs_has_inline_data(inode) && len == PAGE_SIZE &&\n\t\t\t!is_inode_flag_set(inode, FI_NO_PREALLOC))\n\t\treturn 0;\n\n\tif (f2fs_has_inline_data(inode) ||\n\t\t\t(pos & PAGE_MASK) >= i_size_read(inode)) {\n\t\tf2fs_lock_op(sbi);\n\t\tlocked = true;\n\t}\nrestart:\n\t/* check inline_data */\n\tipage = get_node_page(sbi, inode->i_ino);\n\tif (IS_ERR(ipage)) {\n\t\terr = PTR_ERR(ipage);\n\t\tgoto unlock_out;\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, ipage, 0);\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tif (pos + len <= MAX_INLINE_DATA) {\n\t\t\tread_inline_data(page, ipage);\n\t\t\tset_inode_flag(inode, FI_DATA_EXIST);\n\t\t\tif (inode->i_nlink)\n\t\t\t\tset_inline_node(ipage);\n\t\t} else {\n\t\t\terr = f2fs_convert_inline_page(&dn, page);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tif (dn.data_blkaddr == NULL_ADDR)\n\t\t\t\terr = f2fs_get_block(&dn, index);\n\t\t}\n\t} else if (locked) {\n\t\terr = f2fs_get_block(&dn, index);\n\t} else {\n\t\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\t\tdn.data_blkaddr = ei.blk + index - ei.fofs;\n\t\t} else {\n\t\t\t/* hole case */\n\t\t\terr = get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\t\t\tif (err || dn.data_blkaddr == NULL_ADDR) {\n\t\t\t\tf2fs_put_dnode(&dn);\n\t\t\t\tf2fs_lock_op(sbi);\n\t\t\t\tlocked = true;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* convert_inline_page can make node_changed */\n\t*blk_addr = dn.data_blkaddr;\n\t*node_changed = dn.node_changed;\nout:\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (locked)\n\t\tf2fs_unlock_op(sbi);\n\treturn err;\n}\n\nstatic int f2fs_write_begin(struct file *file, struct address_space *mapping,\n\t\tloff_t pos, unsigned len, unsigned flags,\n\t\tstruct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *page = NULL;\n\tpgoff_t index = ((unsigned long long) pos) >> PAGE_SHIFT;\n\tbool need_balance = false;\n\tblock_t blkaddr = NULL_ADDR;\n\tint err = 0;\n\n\ttrace_f2fs_write_begin(inode, pos, len, flags);\n\n\t/*\n\t * We should check this at this moment to avoid deadlock on inode page\n\t * and #0 page. The locking rule for inline_data conversion should be:\n\t * lock_page(page #0) -> lock_page(inode_page)\n\t */\n\tif (index != 0) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\nrepeat:\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page) {\n\t\terr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t*pagep = page;\n\n\terr = prepare_write_begin(sbi, page, pos, len,\n\t\t\t\t\t&blkaddr, &need_balance);\n\tif (err)\n\t\tgoto fail;\n\n\tif (need_balance && has_not_enough_free_secs(sbi, 0, 0)) {\n\t\tunlock_page(page);\n\t\tf2fs_balance_fs(sbi, true);\n\t\tlock_page(page);\n\t\tif (page->mapping != mapping) {\n\t\t\t/* The page got truncated from under us */\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\n\tf2fs_wait_on_page_writeback(page, DATA, false);\n\n\t/* wait for GCed encrypted page writeback */\n\tif (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))\n\t\tf2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);\n\n\tif (len == PAGE_SIZE || PageUptodate(page))\n\t\treturn 0;\n\n\tif (!(pos & (PAGE_SIZE - 1)) && (pos + len) >= i_size_read(inode)) {\n\t\tzero_user_segment(page, len, PAGE_SIZE);\n\t\treturn 0;\n\t}\n\n\tif (blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tSetPageUptodate(page);\n\t} else {\n\t\tstruct bio *bio;\n\n\t\tbio = f2fs_grab_bio(inode, blkaddr, 1);\n\t\tif (IS_ERR(bio)) {\n\t\t\terr = PTR_ERR(bio);\n\t\t\tgoto fail;\n\t\t}\n\t\tbio->bi_opf = REQ_OP_READ;\n\t\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\t\tbio_put(bio);\n\t\t\terr = -EFAULT;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t__submit_bio(sbi, bio, DATA);\n\n\t\tlock_page(page);\n\t\tif (unlikely(page->mapping != mapping)) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(!PageUptodate(page))) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\n\nfail:\n\tf2fs_put_page(page, 1);\n\tf2fs_write_failed(mapping, pos + len);\n\treturn err;\n}\n\nstatic int f2fs_write_end(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_f2fs_write_end(inode, pos, len, copied);\n\n\t/*\n\t * This should be come from len == PAGE_SIZE, and we expect copied\n\t * should be PAGE_SIZE. Otherwise, we treat it with zero copied and\n\t * let generic_perform_write() try to copy data again through copied=0.\n\t */\n\tif (!PageUptodate(page)) {\n\t\tif (unlikely(copied != len))\n\t\t\tcopied = 0;\n\t\telse\n\t\t\tSetPageUptodate(page);\n\t}\n\tif (!copied)\n\t\tgoto unlock_out;\n\n\tset_page_dirty(page);\n\n\tif (pos + copied > i_size_read(inode))\n\t\tf2fs_i_size_write(inode, pos + copied);\nunlock_out:\n\tf2fs_put_page(page, 1);\n\tf2fs_update_time(F2FS_I_SB(inode), REQ_TIME);\n\treturn copied;\n}\n\nstatic int check_direct_IO(struct inode *inode, struct iov_iter *iter,\n\t\t\t   loff_t offset)\n{\n\tunsigned blocksize_mask = inode->i_sb->s_blocksize - 1;\n\n\tif (offset & blocksize_mask)\n\t\treturn -EINVAL;\n\n\tif (iov_iter_alignment(iter) & blocksize_mask)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct address_space *mapping = iocb->ki_filp->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tsize_t count = iov_iter_count(iter);\n\tloff_t offset = iocb->ki_pos;\n\tint rw = iov_iter_rw(iter);\n\tint err;\n\n\terr = check_direct_IO(inode, iter, offset);\n\tif (err)\n\t\treturn err;\n\n\tif (__force_buffered_io(inode, rw))\n\t\treturn 0;\n\n\ttrace_f2fs_direct_IO_enter(inode, offset, count, rw);\n\n\tdown_read(&F2FS_I(inode)->dio_rwsem[rw]);\n\terr = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);\n\tup_read(&F2FS_I(inode)->dio_rwsem[rw]);\n\n\tif (rw == WRITE) {\n\t\tif (err > 0)\n\t\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\telse if (err < 0)\n\t\t\tf2fs_write_failed(mapping, offset + count);\n\t}\n\n\ttrace_f2fs_direct_IO_exit(inode, offset, count, rw, err);\n\n\treturn err;\n}\n\nvoid f2fs_invalidate_page(struct page *page, unsigned int offset,\n\t\t\t\t\t\t\tunsigned int length)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino >= F2FS_ROOT_INO(sbi) &&\n\t\t(offset % PAGE_SIZE || length != PAGE_SIZE))\n\t\treturn;\n\n\tif (PageDirty(page)) {\n\t\tif (inode->i_ino == F2FS_META_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_META);\n\t\t} else if (inode->i_ino == F2FS_NODE_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_NODES);\n\t\t} else {\n\t\t\tinode_dec_dirty_pages(inode);\n\t\t\tremove_dirty_inode(inode);\n\t\t}\n\t}\n\n\t/* This is atomic written page, keep Private */\n\tif (IS_ATOMIC_WRITTEN_PAGE(page))\n\t\treturn;\n\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n}\n\nint f2fs_release_page(struct page *page, gfp_t wait)\n{\n\t/* If this is dirty page, keep PagePrivate */\n\tif (PageDirty(page))\n\t\treturn 0;\n\n\t/* This is atomic written page, keep Private */\n\tif (IS_ATOMIC_WRITTEN_PAGE(page))\n\t\treturn 0;\n\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n\treturn 1;\n}\n\n/*\n * This was copied from __set_page_dirty_buffers which gives higher performance\n * in very high speed storages. (e.g., pmem)\n */\nvoid f2fs_set_page_dirty_nobuffers(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tunsigned long flags;\n\n\tif (unlikely(!mapping))\n\t\treturn;\n\n\tspin_lock(&mapping->private_lock);\n\tlock_page_memcg(page);\n\tSetPageDirty(page);\n\tspin_unlock(&mapping->private_lock);\n\n\tspin_lock_irqsave(&mapping->tree_lock, flags);\n\tWARN_ON_ONCE(!PageUptodate(page));\n\taccount_page_dirtied(page, mapping);\n\tradix_tree_tag_set(&mapping->page_tree,\n\t\t\tpage_index(page), PAGECACHE_TAG_DIRTY);\n\tspin_unlock_irqrestore(&mapping->tree_lock, flags);\n\tunlock_page_memcg(page);\n\n\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\treturn;\n}\n\nstatic int f2fs_set_data_page_dirty(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\n\ttrace_f2fs_set_page_dirty(page, DATA);\n\n\tif (!PageUptodate(page))\n\t\tSetPageUptodate(page);\n\n\tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n\t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {\n\t\t\tregister_inmem_page(inode, page);\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * Previously, this page has been registered, we just\n\t\t * return here.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (!PageDirty(page)) {\n\t\tf2fs_set_page_dirty_nobuffers(page);\n\t\tupdate_dirty_page(inode, page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic sector_t f2fs_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\n\tif (f2fs_has_inline_data(inode))\n\t\treturn 0;\n\n\t/* make sure allocating whole blocks */\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\tfilemap_write_and_wait(mapping);\n\n\treturn generic_block_bmap(mapping, block, get_data_block_bmap);\n}\n\n#ifdef CONFIG_MIGRATION\n#include <linux/migrate.h>\n\nint f2fs_migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tint rc, extra_count;\n\tstruct f2fs_inode_info *fi = F2FS_I(mapping->host);\n\tbool atomic_written = IS_ATOMIC_WRITTEN_PAGE(page);\n\n\tBUG_ON(PageWriteback(page));\n\n\t/* migrating an atomic written page is safe with the inmem_lock hold */\n\tif (atomic_written && !mutex_trylock(&fi->inmem_lock))\n\t\treturn -EAGAIN;\n\n\t/*\n\t * A reference is expected if PagePrivate set when move mapping,\n\t * however F2FS breaks this for maintaining dirty page counts when\n\t * truncating pages. So here adjusting the 'extra_count' make it work.\n\t */\n\textra_count = (atomic_written ? 1 : 0) - page_has_private(page);\n\trc = migrate_page_move_mapping(mapping, newpage,\n\t\t\t\tpage, NULL, mode, extra_count);\n\tif (rc != MIGRATEPAGE_SUCCESS) {\n\t\tif (atomic_written)\n\t\t\tmutex_unlock(&fi->inmem_lock);\n\t\treturn rc;\n\t}\n\n\tif (atomic_written) {\n\t\tstruct inmem_pages *cur;\n\t\tlist_for_each_entry(cur, &fi->inmem_pages, list)\n\t\t\tif (cur->page == page) {\n\t\t\t\tcur->page = newpage;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tmutex_unlock(&fi->inmem_lock);\n\t\tput_page(page);\n\t\tget_page(newpage);\n\t}\n\n\tif (PagePrivate(page))\n\t\tSetPagePrivate(newpage);\n\tset_page_private(newpage, page_private(page));\n\n\tmigrate_page_copy(newpage, page);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n#endif\n\nconst struct address_space_operations f2fs_dblock_aops = {\n\t.readpage\t= f2fs_read_data_page,\n\t.readpages\t= f2fs_read_data_pages,\n\t.writepage\t= f2fs_write_data_page,\n\t.writepages\t= f2fs_write_data_pages,\n\t.write_begin\t= f2fs_write_begin,\n\t.write_end\t= f2fs_write_end,\n\t.set_page_dirty\t= f2fs_set_data_page_dirty,\n\t.invalidatepage\t= f2fs_invalidate_page,\n\t.releasepage\t= f2fs_release_page,\n\t.direct_IO\t= f2fs_direct_IO,\n\t.bmap\t\t= f2fs_bmap,\n#ifdef CONFIG_MIGRATION\n\t.migratepage    = f2fs_migrate_page,\n#endif\n};\n"], "fixing_code": ["/*\n * fs/f2fs/data.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/buffer_head.h>\n#include <linux/mpage.h>\n#include <linux/writeback.h>\n#include <linux/backing-dev.h>\n#include <linux/pagevec.h>\n#include <linux/blkdev.h>\n#include <linux/bio.h>\n#include <linux/prefetch.h>\n#include <linux/uio.h>\n#include <linux/mm.h>\n#include <linux/memcontrol.h>\n#include <linux/cleancache.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"trace.h\"\n#include <trace/events/f2fs.h>\n\nstatic bool __is_cp_guaranteed(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode;\n\tstruct f2fs_sb_info *sbi;\n\n\tif (!mapping)\n\t\treturn false;\n\n\tinode = mapping->host;\n\tsbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_META_INO(sbi) ||\n\t\t\tinode->i_ino ==  F2FS_NODE_INO(sbi) ||\n\t\t\tS_ISDIR(inode->i_mode) ||\n\t\t\tis_cold_data(page))\n\t\treturn true;\n\treturn false;\n}\n\nstatic void f2fs_read_end_io(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tint i;\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(F2FS_P_SB(bio->bi_io_vec->bv_page), FAULT_IO))\n\t\tbio->bi_error = -EIO;\n#endif\n\n\tif (f2fs_bio_encrypted(bio)) {\n\t\tif (bio->bi_error) {\n\t\t\tfscrypt_release_ctx(bio->bi_private);\n\t\t} else {\n\t\t\tfscrypt_decrypt_bio_pages(bio->bi_private, bio);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tif (!bio->bi_error) {\n\t\t\tif (!PageUptodate(page))\n\t\t\t\tSetPageUptodate(page);\n\t\t} else {\n\t\t\tClearPageUptodate(page);\n\t\t\tSetPageError(page);\n\t\t}\n\t\tunlock_page(page);\n\t}\n\tbio_put(bio);\n}\n\nstatic void f2fs_write_end_io(struct bio *bio)\n{\n\tstruct f2fs_sb_info *sbi = bio->bi_private;\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i) {\n\t\tstruct page *page = bvec->bv_page;\n\t\tenum count_type type = WB_DATA_TYPE(page);\n\n\t\tif (IS_DUMMY_WRITTEN_PAGE(page)) {\n\t\t\tset_page_private(page, (unsigned long)NULL);\n\t\t\tClearPagePrivate(page);\n\t\t\tunlock_page(page);\n\t\t\tmempool_free(page, sbi->write_io_dummy);\n\n\t\t\tif (unlikely(bio->bi_error))\n\t\t\t\tf2fs_stop_checkpoint(sbi, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfscrypt_pullback_bio_page(&page, true);\n\n\t\tif (unlikely(bio->bi_error)) {\n\t\t\tmapping_set_error(page->mapping, -EIO);\n\t\t\tf2fs_stop_checkpoint(sbi, true);\n\t\t}\n\t\tdec_page_count(sbi, type);\n\t\tclear_cold_data(page);\n\t\tend_page_writeback(page);\n\t}\n\tif (!get_pages(sbi, F2FS_WB_CP_DATA) &&\n\t\t\t\twq_has_sleeper(&sbi->cp_wait))\n\t\twake_up(&sbi->cp_wait);\n\n\tbio_put(bio);\n}\n\n/*\n * Return true, if pre_bio's bdev is same as its target device.\n */\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blk_addr, struct bio *bio)\n{\n\tstruct block_device *bdev = sbi->sb->s_bdev;\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tif (FDEV(i).start_blk <= blk_addr &&\n\t\t\t\t\tFDEV(i).end_blk >= blk_addr) {\n\t\t\tblk_addr -= FDEV(i).start_blk;\n\t\t\tbdev = FDEV(i).bdev;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (bio) {\n\t\tbio->bi_bdev = bdev;\n\t\tbio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);\n\t}\n\treturn bdev;\n}\n\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (FDEV(i).start_blk <= blkaddr && FDEV(i).end_blk >= blkaddr)\n\t\t\treturn i;\n\treturn 0;\n}\n\nstatic bool __same_bdev(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blk_addr, struct bio *bio)\n{\n\treturn f2fs_target_device(sbi, blk_addr, NULL) == bio->bi_bdev;\n}\n\n/*\n * Low-level block read/write IO operations.\n */\nstatic struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,\n\t\t\t\tint npages, bool is_read)\n{\n\tstruct bio *bio;\n\n\tbio = f2fs_bio_alloc(npages);\n\n\tf2fs_target_device(sbi, blk_addr, bio);\n\tbio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;\n\tbio->bi_private = is_read ? NULL : sbi;\n\n\treturn bio;\n}\n\nstatic inline void __submit_bio(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct bio *bio, enum page_type type)\n{\n\tif (!is_read_io(bio_op(bio))) {\n\t\tunsigned int start;\n\n\t\tif (f2fs_sb_mounted_blkzoned(sbi->sb) &&\n\t\t\tcurrent->plug && (type == DATA || type == NODE))\n\t\t\tblk_finish_plug(current->plug);\n\n\t\tif (type != DATA && type != NODE)\n\t\t\tgoto submit_io;\n\n\t\tstart = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;\n\t\tstart %= F2FS_IO_SIZE(sbi);\n\n\t\tif (start == 0)\n\t\t\tgoto submit_io;\n\n\t\t/* fill dummy pages */\n\t\tfor (; start < F2FS_IO_SIZE(sbi); start++) {\n\t\t\tstruct page *page =\n\t\t\t\tmempool_alloc(sbi->write_io_dummy,\n\t\t\t\t\tGFP_NOIO | __GFP_ZERO | __GFP_NOFAIL);\n\t\t\tf2fs_bug_on(sbi, !page);\n\n\t\t\tSetPagePrivate(page);\n\t\t\tset_page_private(page, (unsigned long)DUMMY_WRITTEN_PAGE);\n\t\t\tlock_page(page);\n\t\t\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE)\n\t\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n\t\t/*\n\t\t * In the NODE case, we lose next block address chain. So, we\n\t\t * need to do checkpoint in f2fs_sync_file.\n\t\t */\n\t\tif (type == NODE)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_CP);\n\t}\nsubmit_io:\n\tif (is_read_io(bio_op(bio)))\n\t\ttrace_f2fs_submit_read_bio(sbi->sb, type, bio);\n\telse\n\t\ttrace_f2fs_submit_write_bio(sbi->sb, type, bio);\n\tsubmit_bio(bio);\n}\n\nstatic void __submit_merged_bio(struct f2fs_bio_info *io)\n{\n\tstruct f2fs_io_info *fio = &io->fio;\n\n\tif (!io->bio)\n\t\treturn;\n\n\tbio_set_op_attrs(io->bio, fio->op, fio->op_flags);\n\n\tif (is_read_io(fio->op))\n\t\ttrace_f2fs_prepare_read_bio(io->sbi->sb, fio->type, io->bio);\n\telse\n\t\ttrace_f2fs_prepare_write_bio(io->sbi->sb, fio->type, io->bio);\n\n\t__submit_bio(io->sbi, io->bio, fio->type);\n\tio->bio = NULL;\n}\n\nstatic bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,\n\t\t\t\t\t\tstruct page *page, nid_t ino)\n{\n\tstruct bio_vec *bvec;\n\tstruct page *target;\n\tint i;\n\n\tif (!io->bio)\n\t\treturn false;\n\n\tif (!inode && !page && !ino)\n\t\treturn true;\n\n\tbio_for_each_segment_all(bvec, io->bio, i) {\n\n\t\tif (bvec->bv_page->mapping)\n\t\t\ttarget = bvec->bv_page;\n\t\telse\n\t\t\ttarget = fscrypt_control_page(bvec->bv_page);\n\n\t\tif (inode && inode == target->mapping->host)\n\t\t\treturn true;\n\t\tif (page && page == target)\n\t\t\treturn true;\n\t\tif (ino && ino == ino_of_node(target))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,\n\t\t\t\t\t\tstruct page *page, nid_t ino,\n\t\t\t\t\t\tenum page_type type)\n{\n\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\tstruct f2fs_bio_info *io = &sbi->write_io[btype];\n\tbool ret;\n\n\tdown_read(&io->io_rwsem);\n\tret = __has_merged_page(io, inode, page, ino);\n\tup_read(&io->io_rwsem);\n\treturn ret;\n}\n\nstatic void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type, int rw)\n{\n\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\tstruct f2fs_bio_info *io;\n\n\tio = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];\n\n\tdown_write(&io->io_rwsem);\n\n\tif (!__has_merged_page(io, inode, page, ino))\n\t\tgoto out;\n\n\t/* change META to META_FLUSH in the checkpoint procedure */\n\tif (type >= META_FLUSH) {\n\t\tio->fio.type = META_FLUSH;\n\t\tio->fio.op = REQ_OP_WRITE;\n\t\tio->fio.op_flags = REQ_PREFLUSH | REQ_META | REQ_PRIO;\n\t\tif (!test_opt(sbi, NOBARRIER))\n\t\t\tio->fio.op_flags |= REQ_FUA;\n\t}\n\t__submit_merged_bio(io);\nout:\n\tup_write(&io->io_rwsem);\n}\n\nvoid f2fs_submit_merged_bio(struct f2fs_sb_info *sbi, enum page_type type,\n\t\t\t\t\t\t\t\t\tint rw)\n{\n\t__f2fs_submit_merged_bio(sbi, NULL, NULL, 0, type, rw);\n}\n\nvoid f2fs_submit_merged_bio_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type, int rw)\n{\n\tif (has_merged_page(sbi, inode, page, ino, type))\n\t\t__f2fs_submit_merged_bio(sbi, inode, page, ino, type, rw);\n}\n\nvoid f2fs_flush_merged_bios(struct f2fs_sb_info *sbi)\n{\n\tf2fs_submit_merged_bio(sbi, DATA, WRITE);\n\tf2fs_submit_merged_bio(sbi, NODE, WRITE);\n\tf2fs_submit_merged_bio(sbi, META, WRITE);\n}\n\n/*\n * Fill the locked page with data located in the block address.\n * Return unlocked page.\n */\nint f2fs_submit_page_bio(struct f2fs_io_info *fio)\n{\n\tstruct bio *bio;\n\tstruct page *page = fio->encrypted_page ?\n\t\t\tfio->encrypted_page : fio->page;\n\n\ttrace_f2fs_submit_page_bio(page, fio);\n\tf2fs_trace_ios(fio, 0);\n\n\t/* Allocate a new bio */\n\tbio = __bio_alloc(fio->sbi, fio->new_blkaddr, 1, is_read_io(fio->op));\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\tbio_put(bio);\n\t\treturn -EFAULT;\n\t}\n\tbio_set_op_attrs(bio, fio->op, fio->op_flags);\n\n\t__submit_bio(fio->sbi, bio, fio->type);\n\treturn 0;\n}\n\nint f2fs_submit_page_mbio(struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tenum page_type btype = PAGE_TYPE_OF_BIO(fio->type);\n\tstruct f2fs_bio_info *io;\n\tbool is_read = is_read_io(fio->op);\n\tstruct page *bio_page;\n\tint err = 0;\n\n\tio = is_read ? &sbi->read_io : &sbi->write_io[btype];\n\n\tif (fio->old_blkaddr != NEW_ADDR)\n\t\tverify_block_addr(sbi, fio->old_blkaddr);\n\tverify_block_addr(sbi, fio->new_blkaddr);\n\n\tbio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;\n\n\tif (!is_read)\n\t\tinc_page_count(sbi, WB_DATA_TYPE(bio_page));\n\n\tdown_write(&io->io_rwsem);\n\n\tif (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||\n\t    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags) ||\n\t\t\t!__same_bdev(sbi, fio->new_blkaddr, io->bio)))\n\t\t__submit_merged_bio(io);\nalloc_new:\n\tif (io->bio == NULL) {\n\t\tif ((fio->type == DATA || fio->type == NODE) &&\n\t\t\t\tfio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {\n\t\t\terr = -EAGAIN;\n\t\t\tdec_page_count(sbi, WB_DATA_TYPE(bio_page));\n\t\t\tgoto out_fail;\n\t\t}\n\t\tio->bio = __bio_alloc(sbi, fio->new_blkaddr,\n\t\t\t\t\t\tBIO_MAX_PAGES, is_read);\n\t\tio->fio = *fio;\n\t}\n\n\tif (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) <\n\t\t\t\t\t\t\tPAGE_SIZE) {\n\t\t__submit_merged_bio(io);\n\t\tgoto alloc_new;\n\t}\n\n\tio->last_block_in_bio = fio->new_blkaddr;\n\tf2fs_trace_ios(fio, 0);\nout_fail:\n\tup_write(&io->io_rwsem);\n\ttrace_f2fs_submit_page_mbio(fio->page, fio);\n\treturn err;\n}\n\nstatic void __set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tstruct f2fs_node *rn = F2FS_NODE(dn->node_page);\n\t__le32 *addr_array;\n\n\t/* Get physical address of data block */\n\taddr_array = blkaddr_in_node(rn);\n\taddr_array[dn->ofs_in_node] = cpu_to_le32(dn->data_blkaddr);\n}\n\n/*\n * Lock ordering for the change of data block address:\n * ->data_page\n *  ->node_page\n *    update block addresses in the node page\n */\nvoid set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true);\n\t__set_data_blkaddr(dn);\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n}\n\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)\n{\n\tdn->data_blkaddr = blkaddr;\n\tset_data_blkaddr(dn);\n\tf2fs_update_extent_cache(dn);\n}\n\n/* dn->ofs_in_node will be returned with up-to-date last block pointer */\nint reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\n\tif (!count)\n\t\treturn 0;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\tif (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))\n\t\treturn -ENOSPC;\n\n\ttrace_f2fs_reserve_new_blocks(dn->inode, dn->nid,\n\t\t\t\t\t\tdn->ofs_in_node, count);\n\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true);\n\n\tfor (; count > 0; dn->ofs_in_node++) {\n\t\tblock_t blkaddr =\n\t\t\tdatablock_addr(dn->node_page, dn->ofs_in_node);\n\t\tif (blkaddr == NULL_ADDR) {\n\t\t\tdn->data_blkaddr = NEW_ADDR;\n\t\t\t__set_data_blkaddr(dn);\n\t\t\tcount--;\n\t\t}\n\t}\n\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n\treturn 0;\n}\n\n/* Should keep dn->ofs_in_node unchanged */\nint reserve_new_block(struct dnode_of_data *dn)\n{\n\tunsigned int ofs_in_node = dn->ofs_in_node;\n\tint ret;\n\n\tret = reserve_new_blocks(dn, 1);\n\tdn->ofs_in_node = ofs_in_node;\n\treturn ret;\n}\n\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tbool need_put = dn->inode_page ? false : true;\n\tint err;\n\n\terr = get_dnode_of_data(dn, index, ALLOC_NODE);\n\tif (err)\n\t\treturn err;\n\n\tif (dn->data_blkaddr == NULL_ADDR)\n\t\terr = reserve_new_block(dn);\n\tif (err || need_put)\n\t\tf2fs_put_dnode(dn);\n\treturn err;\n}\n\nint f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tstruct extent_info ei;\n\tstruct inode *inode = dn->inode;\n\n\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\tdn->data_blkaddr = ei.blk + index - ei.fofs;\n\t\treturn 0;\n\t}\n\n\treturn f2fs_reserve_block(dn, index);\n}\n\nstruct page *get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\tint op_flags, bool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct dnode_of_data dn;\n\tstruct page *page;\n\tstruct extent_info ei;\n\tint err;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = F2FS_I_SB(inode),\n\t\t.type = DATA,\n\t\t.op = REQ_OP_READ,\n\t\t.op_flags = op_flags,\n\t\t.encrypted_page = NULL,\n\t};\n\n\tif (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))\n\t\treturn read_mapping_page(mapping, index, NULL);\n\n\tpage = f2fs_grab_cache_page(mapping, index, for_write);\n\tif (!page)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\tdn.data_blkaddr = ei.blk + index - ei.fofs;\n\t\tgoto got_it;\n\t}\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\tif (err)\n\t\tgoto put_err;\n\tf2fs_put_dnode(&dn);\n\n\tif (unlikely(dn.data_blkaddr == NULL_ADDR)) {\n\t\terr = -ENOENT;\n\t\tgoto put_err;\n\t}\ngot_it:\n\tif (PageUptodate(page)) {\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\t/*\n\t * A new dentry page is allocated but not able to be written, since its\n\t * new inode page couldn't be allocated due to -ENOSPC.\n\t * In such the case, its blkaddr can be remained as NEW_ADDR.\n\t * see, f2fs_add_link -> get_new_data_page -> init_inode_metadata.\n\t */\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\tfio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;\n\tfio.page = page;\n\terr = f2fs_submit_page_bio(&fio);\n\tif (err)\n\t\tgoto put_err;\n\treturn page;\n\nput_err:\n\tf2fs_put_page(page, 1);\n\treturn ERR_PTR(err);\n}\n\nstruct page *find_data_page(struct inode *inode, pgoff_t index)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\n\tpage = find_get_page(mapping, index);\n\tif (page && PageUptodate(page))\n\t\treturn page;\n\tf2fs_put_page(page, 0);\n\n\tpage = get_read_data_page(inode, index, 0, false);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\tif (PageUptodate(page))\n\t\treturn page;\n\n\twait_on_page_locked(page);\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 0);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n/*\n * If it tries to access a hole, return an error.\n * Because, the callers, functions in dir.c and GC, should be able to know\n * whether this page exists or not.\n */\nstruct page *get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\t\tbool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\nrepeat:\n\tpage = get_read_data_page(inode, index, 0, for_write);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\t/* wait for read completion */\n\tlock_page(page);\n\tif (unlikely(page->mapping != mapping)) {\n\t\tf2fs_put_page(page, 1);\n\t\tgoto repeat;\n\t}\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n/*\n * Caller ensures that this data page is never allocated.\n * A new zero-filled data page is allocated in the page cache.\n *\n * Also, caller should grab and release a rwsem by calling f2fs_lock_op() and\n * f2fs_unlock_op().\n * Note that, ipage is set only by make_empty_dir, and if any error occur,\n * ipage should be released by this function.\n */\nstruct page *get_new_data_page(struct inode *inode,\n\t\tstruct page *ipage, pgoff_t index, bool new_i_size)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\tstruct dnode_of_data dn;\n\tint err;\n\n\tpage = f2fs_grab_cache_page(mapping, index, true);\n\tif (!page) {\n\t\t/*\n\t\t * before exiting, we should make sure ipage will be released\n\t\t * if any error occur.\n\t\t */\n\t\tf2fs_put_page(ipage, 1);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, NULL, 0);\n\terr = f2fs_reserve_block(&dn, index);\n\tif (err) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(err);\n\t}\n\tif (!ipage)\n\t\tf2fs_put_dnode(&dn);\n\n\tif (PageUptodate(page))\n\t\tgoto got_it;\n\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t} else {\n\t\tf2fs_put_page(page, 1);\n\n\t\t/* if ipage exists, blkaddr should be NEW_ADDR */\n\t\tf2fs_bug_on(F2FS_I_SB(inode), ipage);\n\t\tpage = get_lock_data_page(inode, index, true);\n\t\tif (IS_ERR(page))\n\t\t\treturn page;\n\t}\ngot_it:\n\tif (new_i_size && i_size_read(inode) <\n\t\t\t\t((loff_t)(index + 1) << PAGE_SHIFT))\n\t\tf2fs_i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));\n\treturn page;\n}\n\nstatic int __allocate_data_block(struct dnode_of_data *dn)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\tpgoff_t fofs;\n\tblkcnt_t count = 1;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\n\tdn->data_blkaddr = datablock_addr(dn->node_page, dn->ofs_in_node);\n\tif (dn->data_blkaddr == NEW_ADDR)\n\t\tgoto alloc;\n\n\tif (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))\n\t\treturn -ENOSPC;\n\nalloc:\n\tget_node_info(sbi, dn->nid, &ni);\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\n\tallocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,\n\t\t\t\t\t\t&sum, CURSEG_WARM_DATA);\n\tset_data_blkaddr(dn);\n\n\t/* update i_size */\n\tfofs = start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +\n\t\t\t\t\t\t\tdn->ofs_in_node;\n\tif (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_SHIFT))\n\t\tf2fs_i_size_write(dn->inode,\n\t\t\t\t((loff_t)(fofs + 1) << PAGE_SHIFT));\n\treturn 0;\n}\n\nstatic inline bool __force_buffered_io(struct inode *inode, int rw)\n{\n\treturn ((f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) ||\n\t\t\t(rw == WRITE && test_opt(F2FS_I_SB(inode), LFS)) ||\n\t\t\tF2FS_I_SB(inode)->s_ndevs);\n}\n\nint f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tstruct f2fs_map_blocks map;\n\tint err = 0;\n\n\tif (is_inode_flag_set(inode, FI_NO_PREALLOC))\n\t\treturn 0;\n\n\tmap.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);\n\tmap.m_len = F2FS_BYTES_TO_BLK(iocb->ki_pos + iov_iter_count(from));\n\tif (map.m_len > map.m_lblk)\n\t\tmap.m_len -= map.m_lblk;\n\telse\n\t\tmap.m_len = 0;\n\n\tmap.m_next_pgofs = NULL;\n\n\tif (iocb->ki_flags & IOCB_DIRECT) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\treturn err;\n\t\treturn f2fs_map_blocks(inode, &map, 1,\n\t\t\t__force_buffered_io(inode, WRITE) ?\n\t\t\t\tF2FS_GET_BLOCK_PRE_AIO :\n\t\t\t\tF2FS_GET_BLOCK_PRE_DIO);\n\t}\n\tif (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (!f2fs_has_inline_data(inode))\n\t\treturn f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_AIO);\n\treturn err;\n}\n\n/*\n * f2fs_map_blocks() now supported readahead/bmap/rw direct_IO with\n * f2fs_map_blocks structure.\n * If original data blocks are allocated, then give them to blockdev.\n * Otherwise,\n *     a. preallocate requested block addresses\n *     b. do not use extent cache for better performance\n *     c. give the block addresses to blockdev\n */\nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,\n\t\t\t\t\t\tint create, int flag)\n{\n\tunsigned int maxblocks = map->m_len;\n\tstruct dnode_of_data dn;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint mode = create ? ALLOC_NODE : LOOKUP_NODE;\n\tpgoff_t pgofs, end_offset, end;\n\tint err = 0, ofs = 1;\n\tunsigned int ofs_in_node, last_ofs_in_node;\n\tblkcnt_t prealloc;\n\tstruct extent_info ei;\n\tblock_t blkaddr;\n\n\tif (!maxblocks)\n\t\treturn 0;\n\n\tmap->m_len = 0;\n\tmap->m_flags = 0;\n\n\t/* it only supports block size == page size */\n\tpgofs =\t(pgoff_t)map->m_lblk;\n\tend = pgofs + maxblocks;\n\n\tif (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {\n\t\tmap->m_pblk = ei.blk + pgofs - ei.fofs;\n\t\tmap->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);\n\t\tmap->m_flags = F2FS_MAP_MAPPED;\n\t\tgoto out;\n\t}\n\nnext_dnode:\n\tif (create)\n\t\tf2fs_lock_op(sbi);\n\n\t/* When reading holes, we need its node page */\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = get_dnode_of_data(&dn, pgofs, mode);\n\tif (err) {\n\t\tif (flag == F2FS_GET_BLOCK_BMAP)\n\t\t\tmap->m_pblk = 0;\n\t\tif (err == -ENOENT) {\n\t\t\terr = 0;\n\t\t\tif (map->m_next_pgofs)\n\t\t\t\t*map->m_next_pgofs =\n\t\t\t\t\tget_next_page_offset(&dn, pgofs);\n\t\t}\n\t\tgoto unlock_out;\n\t}\n\n\tprealloc = 0;\n\tlast_ofs_in_node = ofs_in_node = dn.ofs_in_node;\n\tend_offset = ADDRS_PER_PAGE(dn.node_page, inode);\n\nnext_block:\n\tblkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {\n\t\tif (create) {\n\t\t\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag == F2FS_GET_BLOCK_PRE_AIO) {\n\t\t\t\tif (blkaddr == NULL_ADDR) {\n\t\t\t\t\tprealloc++;\n\t\t\t\t\tlast_ofs_in_node = dn.ofs_in_node;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = __allocate_data_block(&dn);\n\t\t\t\tif (!err)\n\t\t\t\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tgoto sync_out;\n\t\t\tmap->m_flags = F2FS_MAP_NEW;\n\t\t\tblkaddr = dn.data_blkaddr;\n\t\t} else {\n\t\t\tif (flag == F2FS_GET_BLOCK_BMAP) {\n\t\t\t\tmap->m_pblk = 0;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag == F2FS_GET_BLOCK_FIEMAP &&\n\t\t\t\t\t\tblkaddr == NULL_ADDR) {\n\t\t\t\tif (map->m_next_pgofs)\n\t\t\t\t\t*map->m_next_pgofs = pgofs + 1;\n\t\t\t}\n\t\t\tif (flag != F2FS_GET_BLOCK_FIEMAP ||\n\t\t\t\t\t\tblkaddr != NEW_ADDR)\n\t\t\t\tgoto sync_out;\n\t\t}\n\t}\n\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO)\n\t\tgoto skip;\n\n\tif (map->m_len == 0) {\n\t\t/* preallocated unwritten block should be mapped for fiemap. */\n\t\tif (blkaddr == NEW_ADDR)\n\t\t\tmap->m_flags |= F2FS_MAP_UNWRITTEN;\n\t\tmap->m_flags |= F2FS_MAP_MAPPED;\n\n\t\tmap->m_pblk = blkaddr;\n\t\tmap->m_len = 1;\n\t} else if ((map->m_pblk != NEW_ADDR &&\n\t\t\tblkaddr == (map->m_pblk + ofs)) ||\n\t\t\t(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||\n\t\t\tflag == F2FS_GET_BLOCK_PRE_DIO) {\n\t\tofs++;\n\t\tmap->m_len++;\n\t} else {\n\t\tgoto sync_out;\n\t}\n\nskip:\n\tdn.ofs_in_node++;\n\tpgofs++;\n\n\t/* preallocate blocks in batch for one dnode page */\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO &&\n\t\t\t(pgofs == end || dn.ofs_in_node == end_offset)) {\n\n\t\tdn.ofs_in_node = ofs_in_node;\n\t\terr = reserve_new_blocks(&dn, prealloc);\n\t\tif (err)\n\t\t\tgoto sync_out;\n\n\t\tmap->m_len += dn.ofs_in_node - ofs_in_node;\n\t\tif (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {\n\t\t\terr = -ENOSPC;\n\t\t\tgoto sync_out;\n\t\t}\n\t\tdn.ofs_in_node = end_offset;\n\t}\n\n\tif (pgofs >= end)\n\t\tgoto sync_out;\n\telse if (dn.ofs_in_node < end_offset)\n\t\tgoto next_block;\n\n\tf2fs_put_dnode(&dn);\n\n\tif (create) {\n\t\tf2fs_unlock_op(sbi);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\n\tgoto next_dnode;\n\nsync_out:\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (create) {\n\t\tf2fs_unlock_op(sbi);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\nout:\n\ttrace_f2fs_map_blocks(inode, map, err);\n\treturn err;\n}\n\nstatic int __get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\tstruct f2fs_map_blocks map;\n\tint err;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\tmap.m_next_pgofs = next_pgofs;\n\n\terr = f2fs_map_blocks(inode, &map, create, flag);\n\tif (!err) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\tbh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;\n\t\tbh->b_size = (u64)map.m_len << inode->i_blkbits;\n\t}\n\treturn err;\n}\n\nstatic int get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\t\tflag, next_pgofs);\n}\n\nstatic int get_data_block_dio(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\tF2FS_GET_BLOCK_DIO, NULL);\n}\n\nstatic int get_data_block_bmap(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\t/* Block number less than F2FS MAX BLOCKS */\n\tif (unlikely(iblock >= F2FS_I_SB(inode)->max_file_blocks))\n\t\treturn -EFBIG;\n\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\tF2FS_GET_BLOCK_BMAP, NULL);\n}\n\nstatic inline sector_t logical_to_blk(struct inode *inode, loff_t offset)\n{\n\treturn (offset >> inode->i_blkbits);\n}\n\nstatic inline loff_t blk_to_logical(struct inode *inode, sector_t blk)\n{\n\treturn (blk << inode->i_blkbits);\n}\n\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\tu64 start, u64 len)\n{\n\tstruct buffer_head map_bh;\n\tsector_t start_blk, last_blk;\n\tpgoff_t next_pgofs;\n\tu64 logical = 0, phys = 0, size = 0;\n\tu32 flags = 0;\n\tint ret = 0;\n\n\tret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC);\n\tif (ret)\n\t\treturn ret;\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);\n\t\tif (ret != -EAGAIN)\n\t\t\treturn ret;\n\t}\n\n\tinode_lock(inode);\n\n\tif (logical_to_blk(inode, len) == 0)\n\t\tlen = blk_to_logical(inode, 1);\n\n\tstart_blk = logical_to_blk(inode, start);\n\tlast_blk = logical_to_blk(inode, start + len - 1);\n\nnext:\n\tmemset(&map_bh, 0, sizeof(struct buffer_head));\n\tmap_bh.b_size = len;\n\n\tret = get_data_block(inode, start_blk, &map_bh, 0,\n\t\t\t\t\tF2FS_GET_BLOCK_FIEMAP, &next_pgofs);\n\tif (ret)\n\t\tgoto out;\n\n\t/* HOLE */\n\tif (!buffer_mapped(&map_bh)) {\n\t\tstart_blk = next_pgofs;\n\n\t\tif (blk_to_logical(inode, start_blk) < blk_to_logical(inode,\n\t\t\t\t\tF2FS_I_SB(inode)->max_file_blocks))\n\t\t\tgoto prep_next;\n\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\t}\n\n\tif (size) {\n\t\tif (f2fs_encrypted_inode(inode))\n\t\t\tflags |= FIEMAP_EXTENT_DATA_ENCRYPTED;\n\n\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\tphys, size, flags);\n\t}\n\n\tif (start_blk > last_blk || ret)\n\t\tgoto out;\n\n\tlogical = blk_to_logical(inode, start_blk);\n\tphys = blk_to_logical(inode, map_bh.b_blocknr);\n\tsize = map_bh.b_size;\n\tflags = 0;\n\tif (buffer_unwritten(&map_bh))\n\t\tflags = FIEMAP_EXTENT_UNWRITTEN;\n\n\tstart_blk += logical_to_blk(inode, size);\n\nprep_next:\n\tcond_resched();\n\tif (fatal_signal_pending(current))\n\t\tret = -EINTR;\n\telse\n\t\tgoto next;\nout:\n\tif (ret == 1)\n\t\tret = 0;\n\n\tinode_unlock(inode);\n\treturn ret;\n}\n\nstatic struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t unsigned nr_pages)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct fscrypt_ctx *ctx = NULL;\n\tstruct bio *bio;\n\n\tif (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {\n\t\tctx = fscrypt_get_ctx(inode, GFP_NOFS);\n\t\tif (IS_ERR(ctx))\n\t\t\treturn ERR_CAST(ctx);\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);\n\t}\n\n\tbio = bio_alloc(GFP_KERNEL, min_t(int, nr_pages, BIO_MAX_PAGES));\n\tif (!bio) {\n\t\tif (ctx)\n\t\t\tfscrypt_release_ctx(ctx);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio->bi_private = ctx;\n\n\treturn bio;\n}\n\n/*\n * This function was originally taken from fs/mpage.c, and customized for f2fs.\n * Major change was from block_size == page_size in f2fs by default.\n */\nstatic int f2fs_mpage_readpages(struct address_space *mapping,\n\t\t\tstruct list_head *pages, struct page *page,\n\t\t\tunsigned nr_pages)\n{\n\tstruct bio *bio = NULL;\n\tunsigned page_idx;\n\tsector_t last_block_in_bio = 0;\n\tstruct inode *inode = mapping->host;\n\tconst unsigned blkbits = inode->i_blkbits;\n\tconst unsigned blocksize = 1 << blkbits;\n\tsector_t block_in_file;\n\tsector_t last_block;\n\tsector_t last_block_in_file;\n\tsector_t block_nr;\n\tstruct f2fs_map_blocks map;\n\n\tmap.m_pblk = 0;\n\tmap.m_lblk = 0;\n\tmap.m_len = 0;\n\tmap.m_flags = 0;\n\tmap.m_next_pgofs = NULL;\n\n\tfor (page_idx = 0; nr_pages; page_idx++, nr_pages--) {\n\n\t\tprefetchw(&page->flags);\n\t\tif (pages) {\n\t\t\tpage = list_last_entry(pages, struct page, lru);\n\t\t\tlist_del(&page->lru);\n\t\t\tif (add_to_page_cache_lru(page, mapping,\n\t\t\t\t\t\t  page->index,\n\t\t\t\t\t\t  readahead_gfp_mask(mapping)))\n\t\t\t\tgoto next_page;\n\t\t}\n\n\t\tblock_in_file = (sector_t)page->index;\n\t\tlast_block = block_in_file + nr_pages;\n\t\tlast_block_in_file = (i_size_read(inode) + blocksize - 1) >>\n\t\t\t\t\t\t\t\tblkbits;\n\t\tif (last_block > last_block_in_file)\n\t\t\tlast_block = last_block_in_file;\n\n\t\t/*\n\t\t * Map blocks using the previous result first.\n\t\t */\n\t\tif ((map.m_flags & F2FS_MAP_MAPPED) &&\n\t\t\t\tblock_in_file > map.m_lblk &&\n\t\t\t\tblock_in_file < (map.m_lblk + map.m_len))\n\t\t\tgoto got_it;\n\n\t\t/*\n\t\t * Then do more f2fs_map_blocks() calls until we are\n\t\t * done with this page.\n\t\t */\n\t\tmap.m_flags = 0;\n\n\t\tif (block_in_file < last_block) {\n\t\t\tmap.m_lblk = block_in_file;\n\t\t\tmap.m_len = last_block - block_in_file;\n\n\t\t\tif (f2fs_map_blocks(inode, &map, 0,\n\t\t\t\t\t\tF2FS_GET_BLOCK_READ))\n\t\t\t\tgoto set_error_page;\n\t\t}\ngot_it:\n\t\tif ((map.m_flags & F2FS_MAP_MAPPED)) {\n\t\t\tblock_nr = map.m_pblk + block_in_file - map.m_lblk;\n\t\t\tSetPageMappedToDisk(page);\n\n\t\t\tif (!PageUptodate(page) && !cleancache_get_page(page)) {\n\t\t\t\tSetPageUptodate(page);\n\t\t\t\tgoto confused;\n\t\t\t}\n\t\t} else {\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tif (!PageUptodate(page))\n\t\t\t\tSetPageUptodate(page);\n\t\t\tunlock_page(page);\n\t\t\tgoto next_page;\n\t\t}\n\n\t\t/*\n\t\t * This page will go to BIO.  Do we need to send this\n\t\t * BIO off first?\n\t\t */\n\t\tif (bio && (last_block_in_bio != block_nr - 1 ||\n\t\t\t!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {\nsubmit_and_realloc:\n\t\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\t\tbio = NULL;\n\t\t}\n\t\tif (bio == NULL) {\n\t\t\tbio = f2fs_grab_bio(inode, block_nr, nr_pages);\n\t\t\tif (IS_ERR(bio)) {\n\t\t\t\tbio = NULL;\n\t\t\t\tgoto set_error_page;\n\t\t\t}\n\t\t\tbio_set_op_attrs(bio, REQ_OP_READ, 0);\n\t\t}\n\n\t\tif (bio_add_page(bio, page, blocksize, 0) < blocksize)\n\t\t\tgoto submit_and_realloc;\n\n\t\tlast_block_in_bio = block_nr;\n\t\tgoto next_page;\nset_error_page:\n\t\tSetPageError(page);\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tunlock_page(page);\n\t\tgoto next_page;\nconfused:\n\t\tif (bio) {\n\t\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\t\tbio = NULL;\n\t\t}\n\t\tunlock_page(page);\nnext_page:\n\t\tif (pages)\n\t\t\tput_page(page);\n\t}\n\tBUG_ON(pages && !list_empty(pages));\n\tif (bio)\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\treturn 0;\n}\n\nstatic int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page->mapping, NULL, page, 1);\n\treturn ret;\n}\n\nstatic int f2fs_read_data_pages(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct page *page = list_last_entry(pages, struct page, lru);\n\n\ttrace_f2fs_readpages(inode, page, nr_pages);\n\n\t/* If the file has inline data, skip readpages */\n\tif (f2fs_has_inline_data(inode))\n\t\treturn 0;\n\n\treturn f2fs_mpage_readpages(mapping, pages, NULL, nr_pages);\n}\n\nint do_write_data_page(struct f2fs_io_info *fio)\n{\n\tstruct page *page = fio->page;\n\tstruct inode *inode = page->mapping->host;\n\tstruct dnode_of_data dn;\n\tint err = 0;\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);\n\tif (err)\n\t\treturn err;\n\n\tfio->old_blkaddr = dn.data_blkaddr;\n\n\t/* This page is already truncated */\n\tif (fio->old_blkaddr == NULL_ADDR) {\n\t\tClearPageUptodate(page);\n\t\tgoto out_writepage;\n\t}\n\n\tif (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {\n\t\tgfp_t gfp_flags = GFP_NOFS;\n\n\t\t/* wait for GCed encrypted page writeback */\n\t\tf2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),\n\t\t\t\t\t\t\tfio->old_blkaddr);\nretry_encrypt:\n\t\tfio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,\n\t\t\t\t\t\t\tPAGE_SIZE, 0,\n\t\t\t\t\t\t\tfio->page->index,\n\t\t\t\t\t\t\tgfp_flags);\n\t\tif (IS_ERR(fio->encrypted_page)) {\n\t\t\terr = PTR_ERR(fio->encrypted_page);\n\t\t\tif (err == -ENOMEM) {\n\t\t\t\t/* flush pending ios and wait for a while */\n\t\t\t\tf2fs_flush_merged_bios(F2FS_I_SB(inode));\n\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\t\tgfp_flags |= __GFP_NOFAIL;\n\t\t\t\terr = 0;\n\t\t\t\tgoto retry_encrypt;\n\t\t\t}\n\t\t\tgoto out_writepage;\n\t\t}\n\t}\n\n\tset_page_writeback(page);\n\n\t/*\n\t * If current allocation needs SSR,\n\t * it had better in-place writes for updated data.\n\t */\n\tif (unlikely(fio->old_blkaddr != NEW_ADDR &&\n\t\t\t!is_cold_data(page) &&\n\t\t\t!IS_ATOMIC_WRITTEN_PAGE(page) &&\n\t\t\tneed_inplace_update(inode))) {\n\t\trewrite_data_page(fio);\n\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\ttrace_f2fs_do_write_data_page(page, IPU);\n\t} else {\n\t\twrite_data_page(&dn, fio);\n\t\ttrace_f2fs_do_write_data_page(page, OPU);\n\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\tif (page->index == 0)\n\t\t\tset_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);\n\t}\nout_writepage:\n\tf2fs_put_dnode(&dn);\n\treturn err;\n}\n\nstatic int f2fs_write_data_page(struct page *page,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tloff_t i_size = i_size_read(inode);\n\tconst pgoff_t end_index = ((unsigned long long) i_size)\n\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\tloff_t psize = (page->index + 1) << PAGE_SHIFT;\n\tunsigned offset = 0;\n\tbool need_balance_fs = false;\n\tint err = 0;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = wbc_to_write_flags(wbc),\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t};\n\n\ttrace_f2fs_writepage(page, DATA);\n\n\tif (page->index < end_index)\n\t\tgoto write;\n\n\t/*\n\t * If the offset is out-of-range of file size,\n\t * this page does not have to be written to disk.\n\t */\n\toffset = i_size & (PAGE_SIZE - 1);\n\tif ((page->index >= end_index + 1) || !offset)\n\t\tgoto out;\n\n\tzero_user_segment(page, offset, PAGE_SIZE);\nwrite:\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto redirty_out;\n\tif (f2fs_is_drop_cache(inode))\n\t\tgoto out;\n\t/* we should not write 0'th page having journal header */\n\tif (f2fs_is_volatile_file(inode) && (!page->index ||\n\t\t\t(!wbc->for_reclaim &&\n\t\t\tavailable_free_memory(sbi, BASE_CHECK))))\n\t\tgoto redirty_out;\n\n\t/* we should bypass data pages to proceed the kworkder jobs */\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tmapping_set_error(page->mapping, -EIO);\n\t\tgoto out;\n\t}\n\n\t/* Dentry blocks are controlled by checkpoint */\n\tif (S_ISDIR(inode->i_mode)) {\n\t\terr = do_write_data_page(&fio);\n\t\tgoto done;\n\t}\n\n\tif (!wbc->for_reclaim)\n\t\tneed_balance_fs = true;\n\telse if (has_not_enough_free_secs(sbi, 0, 0))\n\t\tgoto redirty_out;\n\n\terr = -EAGAIN;\n\tf2fs_lock_op(sbi);\n\tif (f2fs_has_inline_data(inode))\n\t\terr = f2fs_write_inline_data(inode, page);\n\tif (err == -EAGAIN)\n\t\terr = do_write_data_page(&fio);\n\tif (F2FS_I(inode)->last_disk_size < psize)\n\t\tF2FS_I(inode)->last_disk_size = psize;\n\tf2fs_unlock_op(sbi);\ndone:\n\tif (err && err != -ENOENT)\n\t\tgoto redirty_out;\n\nout:\n\tinode_dec_dirty_pages(inode);\n\tif (err)\n\t\tClearPageUptodate(page);\n\n\tif (wbc->for_reclaim) {\n\t\tf2fs_submit_merged_bio_cond(sbi, NULL, page, 0, DATA, WRITE);\n\t\tremove_dirty_inode(inode);\n\t}\n\n\tunlock_page(page);\n\tf2fs_balance_fs(sbi, need_balance_fs);\n\n\tif (unlikely(f2fs_cp_error(sbi)))\n\t\tf2fs_submit_merged_bio(sbi, DATA, WRITE);\n\n\treturn 0;\n\nredirty_out:\n\tredirty_page_for_writepage(wbc, page);\n\tif (!err)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\n\tunlock_page(page);\n\treturn err;\n}\n\n/*\n * This function was copied from write_cche_pages from mm/page-writeback.c.\n * The major change is making write step of cold data page separately from\n * warm/hot data page.\n */\nstatic int f2fs_write_cache_pages(struct address_space *mapping,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\tint ret = 0;\n\tint done = 0;\n\tstruct pagevec pvec;\n\tint nr_pages;\n\tpgoff_t uninitialized_var(writeback_index);\n\tpgoff_t index;\n\tpgoff_t end;\t\t/* Inclusive */\n\tpgoff_t done_index;\n\tint cycled;\n\tint range_whole = 0;\n\tint tag;\n\tint nwritten = 0;\n\n\tpagevec_init(&pvec, 0);\n\n\tif (wbc->range_cyclic) {\n\t\twriteback_index = mapping->writeback_index; /* prev offset */\n\t\tindex = writeback_index;\n\t\tif (index == 0)\n\t\t\tcycled = 1;\n\t\telse\n\t\t\tcycled = 0;\n\t\tend = -1;\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\t\trange_whole = 1;\n\t\tcycled = 1; /* ignore range_cyclic tests */\n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\tdone_index = index;\n\twhile (!done && (index <= end)) {\n\t\tint i;\n\n\t\tnr_pages = pagevec_lookup_tag(&pvec, mapping, &index, tag,\n\t\t\t      min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tif (page->index > end) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdone_index = page->index;\n\n\t\t\tlock_page(page);\n\n\t\t\tif (unlikely(page->mapping != mapping)) {\ncontinue_unlock:\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!PageDirty(page)) {\n\t\t\t\t/* someone wrote it for us */\n\t\t\t\tgoto continue_unlock;\n\t\t\t}\n\n\t\t\tif (PageWriteback(page)) {\n\t\t\t\tif (wbc->sync_mode != WB_SYNC_NONE)\n\t\t\t\t\tf2fs_wait_on_page_writeback(page,\n\t\t\t\t\t\t\t\tDATA, true);\n\t\t\t\telse\n\t\t\t\t\tgoto continue_unlock;\n\t\t\t}\n\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\tif (!clear_page_dirty_for_io(page))\n\t\t\t\tgoto continue_unlock;\n\n\t\t\tret = mapping->a_ops->writepage(page, wbc);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\t/*\n\t\t\t\t * keep nr_to_write, since vfs uses this to\n\t\t\t\t * get # of written pages.\n\t\t\t\t */\n\t\t\t\tif (ret == AOP_WRITEPAGE_ACTIVATE) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tret = 0;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tdone_index = page->index + 1;\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tnwritten++;\n\t\t\t}\n\n\t\t\tif (--wbc->nr_to_write <= 0 &&\n\t\t\t    wbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n\n\tif (!cycled && !done) {\n\t\tcycled = 1;\n\t\tindex = 0;\n\t\tend = writeback_index - 1;\n\t\tgoto retry;\n\t}\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\tmapping->writeback_index = done_index;\n\n\tif (nwritten)\n\t\tf2fs_submit_merged_bio_cond(F2FS_M_SB(mapping), mapping->host,\n\t\t\t\t\t\t\tNULL, 0, DATA, WRITE);\n\n\treturn ret;\n}\n\nstatic int f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t    struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct blk_plug plug;\n\tint ret;\n\n\t/* deal with chardevs and other special file */\n\tif (!mapping->a_ops->writepage)\n\t\treturn 0;\n\n\t/* skip writing if there is no dirty page in this inode */\n\tif (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)\n\t\treturn 0;\n\n\tif (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&\n\t\t\tget_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&\n\t\t\tavailable_free_memory(sbi, DIRTY_DENTS))\n\t\tgoto skip_write;\n\n\t/* skip writing during file defragment */\n\tif (is_inode_flag_set(inode, FI_DO_DEFRAG))\n\t\tgoto skip_write;\n\n\t/* during POR, we don't need to trigger writepage at all. */\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto skip_write;\n\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\n\tblk_start_plug(&plug);\n\tret = f2fs_write_cache_pages(mapping, wbc);\n\tblk_finish_plug(&plug);\n\t/*\n\t * if some pages were truncated, we cannot guarantee its mapping->host\n\t * to detect pending bios.\n\t */\n\n\tremove_dirty_inode(inode);\n\treturn ret;\n\nskip_write:\n\twbc->pages_skipped += get_dirty_pages(inode);\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\treturn 0;\n}\n\nstatic void f2fs_write_failed(struct address_space *mapping, loff_t to)\n{\n\tstruct inode *inode = mapping->host;\n\tloff_t i_size = i_size_read(inode);\n\n\tif (to > i_size) {\n\t\ttruncate_pagecache(inode, i_size);\n\t\ttruncate_blocks(inode, i_size, true);\n\t}\n}\n\nstatic int prepare_write_begin(struct f2fs_sb_info *sbi,\n\t\t\tstruct page *page, loff_t pos, unsigned len,\n\t\t\tblock_t *blk_addr, bool *node_changed)\n{\n\tstruct inode *inode = page->mapping->host;\n\tpgoff_t index = page->index;\n\tstruct dnode_of_data dn;\n\tstruct page *ipage;\n\tbool locked = false;\n\tstruct extent_info ei;\n\tint err = 0;\n\n\t/*\n\t * we already allocated all the blocks, so we don't need to get\n\t * the block addresses when there is no need to fill the page.\n\t */\n\tif (!f2fs_has_inline_data(inode) && len == PAGE_SIZE &&\n\t\t\t!is_inode_flag_set(inode, FI_NO_PREALLOC))\n\t\treturn 0;\n\n\tif (f2fs_has_inline_data(inode) ||\n\t\t\t(pos & PAGE_MASK) >= i_size_read(inode)) {\n\t\tf2fs_lock_op(sbi);\n\t\tlocked = true;\n\t}\nrestart:\n\t/* check inline_data */\n\tipage = get_node_page(sbi, inode->i_ino);\n\tif (IS_ERR(ipage)) {\n\t\terr = PTR_ERR(ipage);\n\t\tgoto unlock_out;\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, ipage, 0);\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tif (pos + len <= MAX_INLINE_DATA) {\n\t\t\tread_inline_data(page, ipage);\n\t\t\tset_inode_flag(inode, FI_DATA_EXIST);\n\t\t\tif (inode->i_nlink)\n\t\t\t\tset_inline_node(ipage);\n\t\t} else {\n\t\t\terr = f2fs_convert_inline_page(&dn, page);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tif (dn.data_blkaddr == NULL_ADDR)\n\t\t\t\terr = f2fs_get_block(&dn, index);\n\t\t}\n\t} else if (locked) {\n\t\terr = f2fs_get_block(&dn, index);\n\t} else {\n\t\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\t\tdn.data_blkaddr = ei.blk + index - ei.fofs;\n\t\t} else {\n\t\t\t/* hole case */\n\t\t\terr = get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\t\t\tif (err || dn.data_blkaddr == NULL_ADDR) {\n\t\t\t\tf2fs_put_dnode(&dn);\n\t\t\t\tf2fs_lock_op(sbi);\n\t\t\t\tlocked = true;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* convert_inline_page can make node_changed */\n\t*blk_addr = dn.data_blkaddr;\n\t*node_changed = dn.node_changed;\nout:\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (locked)\n\t\tf2fs_unlock_op(sbi);\n\treturn err;\n}\n\nstatic int f2fs_write_begin(struct file *file, struct address_space *mapping,\n\t\tloff_t pos, unsigned len, unsigned flags,\n\t\tstruct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *page = NULL;\n\tpgoff_t index = ((unsigned long long) pos) >> PAGE_SHIFT;\n\tbool need_balance = false;\n\tblock_t blkaddr = NULL_ADDR;\n\tint err = 0;\n\n\ttrace_f2fs_write_begin(inode, pos, len, flags);\n\n\t/*\n\t * We should check this at this moment to avoid deadlock on inode page\n\t * and #0 page. The locking rule for inline_data conversion should be:\n\t * lock_page(page #0) -> lock_page(inode_page)\n\t */\n\tif (index != 0) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\nrepeat:\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page) {\n\t\terr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t*pagep = page;\n\n\terr = prepare_write_begin(sbi, page, pos, len,\n\t\t\t\t\t&blkaddr, &need_balance);\n\tif (err)\n\t\tgoto fail;\n\n\tif (need_balance && has_not_enough_free_secs(sbi, 0, 0)) {\n\t\tunlock_page(page);\n\t\tf2fs_balance_fs(sbi, true);\n\t\tlock_page(page);\n\t\tif (page->mapping != mapping) {\n\t\t\t/* The page got truncated from under us */\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\n\tf2fs_wait_on_page_writeback(page, DATA, false);\n\n\t/* wait for GCed encrypted page writeback */\n\tif (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))\n\t\tf2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);\n\n\tif (len == PAGE_SIZE || PageUptodate(page))\n\t\treturn 0;\n\n\tif (!(pos & (PAGE_SIZE - 1)) && (pos + len) >= i_size_read(inode)) {\n\t\tzero_user_segment(page, len, PAGE_SIZE);\n\t\treturn 0;\n\t}\n\n\tif (blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tSetPageUptodate(page);\n\t} else {\n\t\tstruct bio *bio;\n\n\t\tbio = f2fs_grab_bio(inode, blkaddr, 1);\n\t\tif (IS_ERR(bio)) {\n\t\t\terr = PTR_ERR(bio);\n\t\t\tgoto fail;\n\t\t}\n\t\tbio->bi_opf = REQ_OP_READ;\n\t\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\t\tbio_put(bio);\n\t\t\terr = -EFAULT;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t__submit_bio(sbi, bio, DATA);\n\n\t\tlock_page(page);\n\t\tif (unlikely(page->mapping != mapping)) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(!PageUptodate(page))) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\n\nfail:\n\tf2fs_put_page(page, 1);\n\tf2fs_write_failed(mapping, pos + len);\n\treturn err;\n}\n\nstatic int f2fs_write_end(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_f2fs_write_end(inode, pos, len, copied);\n\n\t/*\n\t * This should be come from len == PAGE_SIZE, and we expect copied\n\t * should be PAGE_SIZE. Otherwise, we treat it with zero copied and\n\t * let generic_perform_write() try to copy data again through copied=0.\n\t */\n\tif (!PageUptodate(page)) {\n\t\tif (unlikely(copied != len))\n\t\t\tcopied = 0;\n\t\telse\n\t\t\tSetPageUptodate(page);\n\t}\n\tif (!copied)\n\t\tgoto unlock_out;\n\n\tset_page_dirty(page);\n\n\tif (pos + copied > i_size_read(inode))\n\t\tf2fs_i_size_write(inode, pos + copied);\nunlock_out:\n\tf2fs_put_page(page, 1);\n\tf2fs_update_time(F2FS_I_SB(inode), REQ_TIME);\n\treturn copied;\n}\n\nstatic int check_direct_IO(struct inode *inode, struct iov_iter *iter,\n\t\t\t   loff_t offset)\n{\n\tunsigned blocksize_mask = inode->i_sb->s_blocksize - 1;\n\n\tif (offset & blocksize_mask)\n\t\treturn -EINVAL;\n\n\tif (iov_iter_alignment(iter) & blocksize_mask)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct address_space *mapping = iocb->ki_filp->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tsize_t count = iov_iter_count(iter);\n\tloff_t offset = iocb->ki_pos;\n\tint rw = iov_iter_rw(iter);\n\tint err;\n\n\terr = check_direct_IO(inode, iter, offset);\n\tif (err)\n\t\treturn err;\n\n\tif (__force_buffered_io(inode, rw))\n\t\treturn 0;\n\n\ttrace_f2fs_direct_IO_enter(inode, offset, count, rw);\n\n\tdown_read(&F2FS_I(inode)->dio_rwsem[rw]);\n\terr = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);\n\tup_read(&F2FS_I(inode)->dio_rwsem[rw]);\n\n\tif (rw == WRITE) {\n\t\tif (err > 0)\n\t\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\telse if (err < 0)\n\t\t\tf2fs_write_failed(mapping, offset + count);\n\t}\n\n\ttrace_f2fs_direct_IO_exit(inode, offset, count, rw, err);\n\n\treturn err;\n}\n\nvoid f2fs_invalidate_page(struct page *page, unsigned int offset,\n\t\t\t\t\t\t\tunsigned int length)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino >= F2FS_ROOT_INO(sbi) &&\n\t\t(offset % PAGE_SIZE || length != PAGE_SIZE))\n\t\treturn;\n\n\tif (PageDirty(page)) {\n\t\tif (inode->i_ino == F2FS_META_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_META);\n\t\t} else if (inode->i_ino == F2FS_NODE_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_NODES);\n\t\t} else {\n\t\t\tinode_dec_dirty_pages(inode);\n\t\t\tremove_dirty_inode(inode);\n\t\t}\n\t}\n\n\t/* This is atomic written page, keep Private */\n\tif (IS_ATOMIC_WRITTEN_PAGE(page))\n\t\treturn;\n\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n}\n\nint f2fs_release_page(struct page *page, gfp_t wait)\n{\n\t/* If this is dirty page, keep PagePrivate */\n\tif (PageDirty(page))\n\t\treturn 0;\n\n\t/* This is atomic written page, keep Private */\n\tif (IS_ATOMIC_WRITTEN_PAGE(page))\n\t\treturn 0;\n\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n\treturn 1;\n}\n\n/*\n * This was copied from __set_page_dirty_buffers which gives higher performance\n * in very high speed storages. (e.g., pmem)\n */\nvoid f2fs_set_page_dirty_nobuffers(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tunsigned long flags;\n\n\tif (unlikely(!mapping))\n\t\treturn;\n\n\tspin_lock(&mapping->private_lock);\n\tlock_page_memcg(page);\n\tSetPageDirty(page);\n\tspin_unlock(&mapping->private_lock);\n\n\tspin_lock_irqsave(&mapping->tree_lock, flags);\n\tWARN_ON_ONCE(!PageUptodate(page));\n\taccount_page_dirtied(page, mapping);\n\tradix_tree_tag_set(&mapping->page_tree,\n\t\t\tpage_index(page), PAGECACHE_TAG_DIRTY);\n\tspin_unlock_irqrestore(&mapping->tree_lock, flags);\n\tunlock_page_memcg(page);\n\n\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\treturn;\n}\n\nstatic int f2fs_set_data_page_dirty(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\n\ttrace_f2fs_set_page_dirty(page, DATA);\n\n\tif (!PageUptodate(page))\n\t\tSetPageUptodate(page);\n\n\tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n\t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {\n\t\t\tregister_inmem_page(inode, page);\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * Previously, this page has been registered, we just\n\t\t * return here.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (!PageDirty(page)) {\n\t\tf2fs_set_page_dirty_nobuffers(page);\n\t\tupdate_dirty_page(inode, page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic sector_t f2fs_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\n\tif (f2fs_has_inline_data(inode))\n\t\treturn 0;\n\n\t/* make sure allocating whole blocks */\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\tfilemap_write_and_wait(mapping);\n\n\treturn generic_block_bmap(mapping, block, get_data_block_bmap);\n}\n\n#ifdef CONFIG_MIGRATION\n#include <linux/migrate.h>\n\nint f2fs_migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tint rc, extra_count;\n\tstruct f2fs_inode_info *fi = F2FS_I(mapping->host);\n\tbool atomic_written = IS_ATOMIC_WRITTEN_PAGE(page);\n\n\tBUG_ON(PageWriteback(page));\n\n\t/* migrating an atomic written page is safe with the inmem_lock hold */\n\tif (atomic_written && !mutex_trylock(&fi->inmem_lock))\n\t\treturn -EAGAIN;\n\n\t/*\n\t * A reference is expected if PagePrivate set when move mapping,\n\t * however F2FS breaks this for maintaining dirty page counts when\n\t * truncating pages. So here adjusting the 'extra_count' make it work.\n\t */\n\textra_count = (atomic_written ? 1 : 0) - page_has_private(page);\n\trc = migrate_page_move_mapping(mapping, newpage,\n\t\t\t\tpage, NULL, mode, extra_count);\n\tif (rc != MIGRATEPAGE_SUCCESS) {\n\t\tif (atomic_written)\n\t\t\tmutex_unlock(&fi->inmem_lock);\n\t\treturn rc;\n\t}\n\n\tif (atomic_written) {\n\t\tstruct inmem_pages *cur;\n\t\tlist_for_each_entry(cur, &fi->inmem_pages, list)\n\t\t\tif (cur->page == page) {\n\t\t\t\tcur->page = newpage;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tmutex_unlock(&fi->inmem_lock);\n\t\tput_page(page);\n\t\tget_page(newpage);\n\t}\n\n\tif (PagePrivate(page))\n\t\tSetPagePrivate(newpage);\n\tset_page_private(newpage, page_private(page));\n\n\tmigrate_page_copy(newpage, page);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n#endif\n\nconst struct address_space_operations f2fs_dblock_aops = {\n\t.readpage\t= f2fs_read_data_page,\n\t.readpages\t= f2fs_read_data_pages,\n\t.writepage\t= f2fs_write_data_page,\n\t.writepages\t= f2fs_write_data_pages,\n\t.write_begin\t= f2fs_write_begin,\n\t.write_end\t= f2fs_write_end,\n\t.set_page_dirty\t= f2fs_set_data_page_dirty,\n\t.invalidatepage\t= f2fs_invalidate_page,\n\t.releasepage\t= f2fs_release_page,\n\t.direct_IO\t= f2fs_direct_IO,\n\t.bmap\t\t= f2fs_bmap,\n#ifdef CONFIG_MIGRATION\n\t.migratepage    = f2fs_migrate_page,\n#endif\n};\n"], "filenames": ["fs/f2fs/data.c"], "buggy_code_start_loc": [967], "buggy_code_end_loc": [968], "fixing_code_start_loc": [967], "fixing_code_end_loc": [968], "type": "CWE-190", "message": "The __get_data_block function in fs/f2fs/data.c in the Linux kernel before 4.11 allows local users to cause a denial of service (integer overflow and loop) via crafted use of the open and fallocate system calls with an FS_IOC_FIEMAP ioctl.", "other": {"cve": {"id": "CVE-2017-18257", "sourceIdentifier": "cve@mitre.org", "published": "2018-04-04T17:29:01.520", "lastModified": "2018-07-04T01:29:02.040", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The __get_data_block function in fs/f2fs/data.c in the Linux kernel before 4.11 allows local users to cause a denial of service (integer overflow and loop) via crafted use of the open and fallocate system calls with an FS_IOC_FIEMAP ioctl."}, {"lang": "es", "value": "La funci\u00f3n __get_data_block en fs/f2fs/data.c en el kernel de Linux, en versiones anteriores a la 4.11, permite que usuarios locales provoquen una denegaci\u00f3n de servicio (desbordamiento de enteros y bucle) mediante el uso manipulado de las llamadas del sistema open y fallocate con un ioctl FS_IOC_FIEMAP."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.11", "matchCriteriaId": "68E74529-58C5-4D73-8176-DDDF25C71F22"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b86e33075ed1909d8002745b56ecf73b833db143", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/torvalds/linux/commit/b86e33075ed1909d8002745b56ecf73b833db143", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://usn.ubuntu.com/3696-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3696-2/", "source": "cve@mitre.org"}, {"url": "https://www.debian.org/security/2018/dsa-4188", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b86e33075ed1909d8002745b56ecf73b833db143"}}