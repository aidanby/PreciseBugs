{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the Interfaces handler.\n *\n * Version:\t@(#)dev.h\t1.0.10\t08/12/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tDonald J. Becker, <becker@cesdis.gsfc.nasa.gov>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tBjorn Ekwall. <bj0rn@blox.se>\n *              Pekka Riikonen <priikone@poseidon.pspt.fi>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\t\tMoved to /usr/include/linux for NET3\n */\n#ifndef _LINUX_NETDEVICE_H\n#define _LINUX_NETDEVICE_H\n\n#include <linux/if.h>\n#include <linux/if_ether.h>\n#include <linux/if_packet.h>\n#include <linux/if_link.h>\n\n#ifdef __KERNEL__\n#include <linux/pm_qos_params.h>\n#include <linux/timer.h>\n#include <linux/delay.h>\n#include <linux/mm.h>\n#include <asm/atomic.h>\n#include <asm/cache.h>\n#include <asm/byteorder.h>\n\n#include <linux/device.h>\n#include <linux/percpu.h>\n#include <linux/rculist.h>\n#include <linux/dmaengine.h>\n#include <linux/workqueue.h>\n\n#include <linux/ethtool.h>\n#include <net/net_namespace.h>\n#include <net/dsa.h>\n#ifdef CONFIG_DCB\n#include <net/dcbnl.h>\n#endif\n\nstruct vlan_group;\nstruct netpoll_info;\nstruct phy_device;\n/* 802.11 specific */\nstruct wireless_dev;\n\t\t\t\t\t/* source back-compat hooks */\n#define SET_ETHTOOL_OPS(netdev,ops) \\\n\t( (netdev)->ethtool_ops = (ops) )\n\n#define HAVE_ALLOC_NETDEV\t\t/* feature macro: alloc_xxxdev\n\t\t\t\t\t   functions are available. */\n#define HAVE_FREE_NETDEV\t\t/* free_netdev() */\n#define HAVE_NETDEV_PRIV\t\t/* netdev_priv() */\n\n/* hardware address assignment types */\n#define NET_ADDR_PERM\t\t0\t/* address is permanent (default) */\n#define NET_ADDR_RANDOM\t\t1\t/* address is generated randomly */\n#define NET_ADDR_STOLEN\t\t2\t/* address is stolen from other device */\n\n/* Backlog congestion levels */\n#define NET_RX_SUCCESS\t\t0\t/* keep 'em coming, baby */\n#define NET_RX_DROP\t\t1\t/* packet dropped */\n\n/*\n * Transmit return codes: transmit return codes originate from three different\n * namespaces:\n *\n * - qdisc return codes\n * - driver transmit return codes\n * - errno values\n *\n * Drivers are allowed to return any one of those in their hard_start_xmit()\n * function. Real network devices commonly used with qdiscs should only return\n * the driver transmit return codes though - when qdiscs are used, the actual\n * transmission happens asynchronously, so the value is not propagated to\n * higher layers. Virtual network devices transmit synchronously, in this case\n * the driver transmit return codes are consumed by dev_queue_xmit(), all\n * others are propagated to higher layers.\n */\n\n/* qdisc ->enqueue() return codes. */\n#define NET_XMIT_SUCCESS\t0x00\n#define NET_XMIT_DROP\t\t0x01\t/* skb dropped\t\t\t*/\n#define NET_XMIT_CN\t\t0x02\t/* congestion notification\t*/\n#define NET_XMIT_POLICED\t0x03\t/* skb is shot by police\t*/\n#define NET_XMIT_MASK\t\t0x0f\t/* qdisc flags in net/sch_generic.h */\n\n/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It\n * indicates that the device will soon be dropping packets, or already drops\n * some packets of the same priority; prompting us to send less aggressively. */\n#define net_xmit_eval(e)\t((e) == NET_XMIT_CN ? 0 : (e))\n#define net_xmit_errno(e)\t((e) != NET_XMIT_CN ? -ENOBUFS : 0)\n\n/* Driver transmit return codes */\n#define NETDEV_TX_MASK\t\t0xf0\n\nenum netdev_tx {\n\t__NETDEV_TX_MIN\t = INT_MIN,\t/* make sure enum is signed */\n\tNETDEV_TX_OK\t = 0x00,\t/* driver took care of packet */\n\tNETDEV_TX_BUSY\t = 0x10,\t/* driver tx path was busy*/\n\tNETDEV_TX_LOCKED = 0x20,\t/* driver tx lock was already taken */\n};\ntypedef enum netdev_tx netdev_tx_t;\n\n/*\n * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;\n * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.\n */\nstatic inline bool dev_xmit_complete(int rc)\n{\n\t/*\n\t * Positive cases with an skb consumed by a driver:\n\t * - successful transmission (rc == NETDEV_TX_OK)\n\t * - error while transmitting (rc < 0)\n\t * - error while queueing to a different device (rc & NET_XMIT_MASK)\n\t */\n\tif (likely(rc < NET_XMIT_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\n#endif\n\n#define MAX_ADDR_LEN\t32\t\t/* Largest hardware address length */\n\n#ifdef  __KERNEL__\n/*\n *\tCompute the worst case header length according to the protocols\n *\tused.\n */\n\n#if defined(CONFIG_WLAN) || defined(CONFIG_AX25) || defined(CONFIG_AX25_MODULE)\n# if defined(CONFIG_MAC80211_MESH)\n#  define LL_MAX_HEADER 128\n# else\n#  define LL_MAX_HEADER 96\n# endif\n#elif defined(CONFIG_TR) || defined(CONFIG_TR_MODULE)\n# define LL_MAX_HEADER 48\n#else\n# define LL_MAX_HEADER 32\n#endif\n\n#if !defined(CONFIG_NET_IPIP) && !defined(CONFIG_NET_IPIP_MODULE) && \\\n    !defined(CONFIG_NET_IPGRE) &&  !defined(CONFIG_NET_IPGRE_MODULE) && \\\n    !defined(CONFIG_IPV6_SIT) && !defined(CONFIG_IPV6_SIT_MODULE) && \\\n    !defined(CONFIG_IPV6_TUNNEL) && !defined(CONFIG_IPV6_TUNNEL_MODULE)\n#define MAX_HEADER LL_MAX_HEADER\n#else\n#define MAX_HEADER (LL_MAX_HEADER + 48)\n#endif\n\n/*\n *\tOld network device statistics. Fields are native words\n *\t(unsigned long) so they can be read and written atomically.\n */\n\nstruct net_device_stats {\n\tunsigned long\trx_packets;\n\tunsigned long\ttx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_bytes;\n\tunsigned long\trx_errors;\n\tunsigned long\ttx_errors;\n\tunsigned long\trx_dropped;\n\tunsigned long\ttx_dropped;\n\tunsigned long\tmulticast;\n\tunsigned long\tcollisions;\n\tunsigned long\trx_length_errors;\n\tunsigned long\trx_over_errors;\n\tunsigned long\trx_crc_errors;\n\tunsigned long\trx_frame_errors;\n\tunsigned long\trx_fifo_errors;\n\tunsigned long\trx_missed_errors;\n\tunsigned long\ttx_aborted_errors;\n\tunsigned long\ttx_carrier_errors;\n\tunsigned long\ttx_fifo_errors;\n\tunsigned long\ttx_heartbeat_errors;\n\tunsigned long\ttx_window_errors;\n\tunsigned long\trx_compressed;\n\tunsigned long\ttx_compressed;\n};\n\n#endif  /*  __KERNEL__  */\n\n\n/* Media selection options. */\nenum {\n        IF_PORT_UNKNOWN = 0,\n        IF_PORT_10BASE2,\n        IF_PORT_10BASET,\n        IF_PORT_AUI,\n        IF_PORT_100BASET,\n        IF_PORT_100BASETX,\n        IF_PORT_100BASEFX\n};\n\n#ifdef __KERNEL__\n\n#include <linux/cache.h>\n#include <linux/skbuff.h>\n\nstruct neighbour;\nstruct neigh_parms;\nstruct sk_buff;\n\nstruct netdev_hw_addr {\n\tstruct list_head\tlist;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n\tunsigned char\t\ttype;\n#define NETDEV_HW_ADDR_T_LAN\t\t1\n#define NETDEV_HW_ADDR_T_SAN\t\t2\n#define NETDEV_HW_ADDR_T_SLAVE\t\t3\n#define NETDEV_HW_ADDR_T_UNICAST\t4\n#define NETDEV_HW_ADDR_T_MULTICAST\t5\n\tbool\t\t\tsynced;\n\tbool\t\t\tglobal_use;\n\tint\t\t\trefcount;\n\tstruct rcu_head\t\trcu_head;\n};\n\nstruct netdev_hw_addr_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n#define netdev_hw_addr_list_count(l) ((l)->count)\n#define netdev_hw_addr_list_empty(l) (netdev_hw_addr_list_count(l) == 0)\n#define netdev_hw_addr_list_for_each(ha, l) \\\n\tlist_for_each_entry(ha, &(l)->list, list)\n\n#define netdev_uc_count(dev) netdev_hw_addr_list_count(&(dev)->uc)\n#define netdev_uc_empty(dev) netdev_hw_addr_list_empty(&(dev)->uc)\n#define netdev_for_each_uc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->uc)\n\n#define netdev_mc_count(dev) netdev_hw_addr_list_count(&(dev)->mc)\n#define netdev_mc_empty(dev) netdev_hw_addr_list_empty(&(dev)->mc)\n#define netdev_for_each_mc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->mc)\n\nstruct hh_cache {\n\tstruct hh_cache *hh_next;\t/* Next entry\t\t\t     */\n\tatomic_t\thh_refcnt;\t/* number of users                   */\n/*\n * We want hh_output, hh_len, hh_lock and hh_data be a in a separate\n * cache line on SMP.\n * They are mostly read, but hh_refcnt may be changed quite frequently,\n * incurring cache line ping pongs.\n */\n\t__be16\t\thh_type ____cacheline_aligned_in_smp;\n\t\t\t\t\t/* protocol identifier, f.e ETH_P_IP\n                                         *  NOTE:  For VLANs, this will be the\n                                         *  encapuslated type. --BLG\n                                         */\n\tu16\t\thh_len;\t\t/* length of header */\n\tint\t\t(*hh_output)(struct sk_buff *skb);\n\tseqlock_t\thh_lock;\n\n\t/* cached hardware header; allow for machine alignment needs.        */\n#define HH_DATA_MOD\t16\n#define HH_DATA_OFF(__len) \\\n\t(HH_DATA_MOD - (((__len - 1) & (HH_DATA_MOD - 1)) + 1))\n#define HH_DATA_ALIGN(__len) \\\n\t(((__len)+(HH_DATA_MOD-1))&~(HH_DATA_MOD - 1))\n\tunsigned long\thh_data[HH_DATA_ALIGN(LL_MAX_HEADER) / sizeof(long)];\n};\n\nstatic inline void hh_cache_put(struct hh_cache *hh)\n{\n\tif (atomic_dec_and_test(&hh->hh_refcnt))\n\t\tkfree(hh);\n}\n\n/* Reserve HH_DATA_MOD byte aligned hard_header_len, but at least that much.\n * Alternative is:\n *   dev->hard_header_len ? (dev->hard_header_len +\n *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0\n *\n * We could use other alignment values, but we must maintain the\n * relationship HH alignment <= LL alignment.\n *\n * LL_ALLOCATED_SPACE also takes into account the tailroom the device\n * may need.\n */\n#define LL_RESERVED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_RESERVED_SPACE_EXTRA(dev,extra) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(extra))&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_ALLOCATED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(dev)->needed_tailroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n\nstruct header_ops {\n\tint\t(*create) (struct sk_buff *skb, struct net_device *dev,\n\t\t\t   unsigned short type, const void *daddr,\n\t\t\t   const void *saddr, unsigned len);\n\tint\t(*parse)(const struct sk_buff *skb, unsigned char *haddr);\n\tint\t(*rebuild)(struct sk_buff *skb);\n#define HAVE_HEADER_CACHE\n\tint\t(*cache)(const struct neighbour *neigh, struct hh_cache *hh);\n\tvoid\t(*cache_update)(struct hh_cache *hh,\n\t\t\t\tconst struct net_device *dev,\n\t\t\t\tconst unsigned char *haddr);\n};\n\n/* These flag bits are private to the generic network queueing\n * layer, they may not be explicitly referenced by any other\n * code.\n */\n\nenum netdev_state_t {\n\t__LINK_STATE_START,\n\t__LINK_STATE_PRESENT,\n\t__LINK_STATE_NOCARRIER,\n\t__LINK_STATE_LINKWATCH_PENDING,\n\t__LINK_STATE_DORMANT,\n};\n\n\n/*\n * This structure holds at boot time configured netdevice settings. They\n * are then used in the device probing.\n */\nstruct netdev_boot_setup {\n\tchar name[IFNAMSIZ];\n\tstruct ifmap map;\n};\n#define NETDEV_BOOT_SETUP_MAX 8\n\nextern int __init netdev_boot_setup(char *str);\n\n/*\n * Structure for NAPI scheduling similar to tasklet but with weighting\n */\nstruct napi_struct {\n\t/* The poll_list must only be managed by the entity which\n\t * changes the state of the NAPI_STATE_SCHED bit.  This means\n\t * whoever atomically sets that bit can add this napi_struct\n\t * to the per-cpu poll_list, and whoever clears that bit\n\t * can remove from the list right before clearing the bit.\n\t */\n\tstruct list_head\tpoll_list;\n\n\tunsigned long\t\tstate;\n\tint\t\t\tweight;\n\tint\t\t\t(*poll)(struct napi_struct *, int);\n#ifdef CONFIG_NETPOLL\n\tspinlock_t\t\tpoll_lock;\n\tint\t\t\tpoll_owner;\n#endif\n\n\tunsigned int\t\tgro_count;\n\n\tstruct net_device\t*dev;\n\tstruct list_head\tdev_list;\n\tstruct sk_buff\t\t*gro_list;\n\tstruct sk_buff\t\t*skb;\n};\n\nenum {\n\tNAPI_STATE_SCHED,\t/* Poll is scheduled */\n\tNAPI_STATE_DISABLE,\t/* Disable pending */\n\tNAPI_STATE_NPSVC,\t/* Netpoll - don't dequeue from poll_list */\n};\n\nenum gro_result {\n\tGRO_MERGED,\n\tGRO_MERGED_FREE,\n\tGRO_HELD,\n\tGRO_NORMAL,\n\tGRO_DROP,\n};\ntypedef enum gro_result gro_result_t;\n\ntypedef struct sk_buff *rx_handler_func_t(struct sk_buff *skb);\n\nextern void __napi_schedule(struct napi_struct *n);\n\nstatic inline int napi_disable_pending(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nstatic inline int napi_schedule_prep(struct napi_struct *n)\n{\n\treturn !napi_disable_pending(n) &&\n\t\t!test_and_set_bit(NAPI_STATE_SCHED, &n->state);\n}\n\n/**\n *\tnapi_schedule - schedule NAPI poll\n *\t@n: napi context\n *\n * Schedule NAPI poll routine to be called if it is not already\n * running.\n */\nstatic inline void napi_schedule(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule(n);\n}\n\n/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */\nstatic inline int napi_reschedule(struct napi_struct *napi)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\t__napi_schedule(napi);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/**\n *\tnapi_complete - NAPI processing complete\n *\t@n: napi context\n *\n * Mark NAPI processing as complete.\n */\nextern void __napi_complete(struct napi_struct *n);\nextern void napi_complete(struct napi_struct *n);\n\n/**\n *\tnapi_disable - prevent NAPI from scheduling\n *\t@n: napi context\n *\n * Stop NAPI from being scheduled on this context.\n * Waits till any outstanding processing completes.\n */\nstatic inline void napi_disable(struct napi_struct *n)\n{\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\twhile (test_and_set_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: napi context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nstatic inline void napi_enable(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tsmp_mb__before_clear_bit();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n}\n\n#ifdef CONFIG_SMP\n/**\n *\tnapi_synchronize - wait until NAPI is not running\n *\t@n: napi context\n *\n * Wait until NAPI is done being scheduled on this context.\n * Waits till any outstanding processing completes but\n * does not disable future activations.\n */\nstatic inline void napi_synchronize(const struct napi_struct *n)\n{\n\twhile (test_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n}\n#else\n# define napi_synchronize(n)\tbarrier()\n#endif\n\nenum netdev_queue_state_t {\n\t__QUEUE_STATE_XOFF,\n\t__QUEUE_STATE_FROZEN,\n#define QUEUE_STATE_XOFF_OR_FROZEN ((1 << __QUEUE_STATE_XOFF)\t\t| \\\n\t\t\t\t    (1 << __QUEUE_STATE_FROZEN))\n};\n\nstruct netdev_queue {\n/*\n * read mostly part\n */\n\tstruct net_device\t*dev;\n\tstruct Qdisc\t\t*qdisc;\n\tunsigned long\t\tstate;\n\tstruct Qdisc\t\t*qdisc_sleeping;\n#ifdef CONFIG_RPS\n\tstruct kobject\t\tkobj;\n#endif\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tint\t\t\tnuma_node;\n#endif\n/*\n * write mostly part\n */\n\tspinlock_t\t\t_xmit_lock ____cacheline_aligned_in_smp;\n\tint\t\t\txmit_lock_owner;\n\t/*\n\t * please use this field instead of dev->trans_start\n\t */\n\tunsigned long\t\ttrans_start;\n} ____cacheline_aligned_in_smp;\n\nstatic inline int netdev_queue_numa_node_read(const struct netdev_queue *q)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\treturn q->numa_node;\n#else\n\treturn NUMA_NO_NODE;\n#endif\n}\n\nstatic inline void netdev_queue_numa_node_write(struct netdev_queue *q, int node)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tq->numa_node = node;\n#endif\n}\n\n#ifdef CONFIG_RPS\n/*\n * This structure holds an RPS map which can be of variable length.  The\n * map is an array of CPUs.\n */\nstruct rps_map {\n\tunsigned int len;\n\tstruct rcu_head rcu;\n\tu16 cpus[0];\n};\n#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + (_num * sizeof(u16)))\n\n/*\n * The rps_dev_flow structure contains the mapping of a flow to a CPU and the\n * tail pointer for that CPU's input queue at the time of last enqueue.\n */\nstruct rps_dev_flow {\n\tu16 cpu;\n\tu16 fill;\n\tunsigned int last_qtail;\n};\n\n/*\n * The rps_dev_flow_table structure contains a table of flow mappings.\n */\nstruct rps_dev_flow_table {\n\tunsigned int mask;\n\tstruct rcu_head rcu;\n\tstruct work_struct free_work;\n\tstruct rps_dev_flow flows[0];\n};\n#define RPS_DEV_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_dev_flow_table) + \\\n    (_num * sizeof(struct rps_dev_flow)))\n\n/*\n * The rps_sock_flow_table contains mappings of flows to the last CPU\n * on which they were processed by the application (set in recvmsg).\n */\nstruct rps_sock_flow_table {\n\tunsigned int mask;\n\tu16 ents[0];\n};\n#define\tRPS_SOCK_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_sock_flow_table) + \\\n    (_num * sizeof(u16)))\n\n#define RPS_NO_CPU 0xffff\n\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t\tu32 hash)\n{\n\tif (table && hash) {\n\t\tunsigned int cpu, index = hash & table->mask;\n\n\t\t/* We only give a hint, preemption can change cpu under us */\n\t\tcpu = raw_smp_processor_id();\n\n\t\tif (table->ents[index] != cpu)\n\t\t\ttable->ents[index] = cpu;\n\t}\n}\n\nstatic inline void rps_reset_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t       u32 hash)\n{\n\tif (table && hash)\n\t\ttable->ents[hash & table->mask] = RPS_NO_CPU;\n}\n\nextern struct rps_sock_flow_table __rcu *rps_sock_flow_table;\n\n/* This structure contains an instance of an RX queue. */\nstruct netdev_rx_queue {\n\tstruct rps_map __rcu\t\t*rps_map;\n\tstruct rps_dev_flow_table __rcu\t*rps_flow_table;\n\tstruct kobject\t\t\tkobj;\n\tstruct net_device\t\t*dev;\n} ____cacheline_aligned_in_smp;\n#endif /* CONFIG_RPS */\n\n#ifdef CONFIG_XPS\n/*\n * This structure holds an XPS map which can be of variable length.  The\n * map is an array of queues.\n */\nstruct xps_map {\n\tunsigned int len;\n\tunsigned int alloc_len;\n\tstruct rcu_head rcu;\n\tu16 queues[0];\n};\n#define XPS_MAP_SIZE(_num) (sizeof(struct xps_map) + (_num * sizeof(u16)))\n#define XPS_MIN_MAP_ALLOC ((L1_CACHE_BYTES - sizeof(struct xps_map))\t\\\n    / sizeof(u16))\n\n/*\n * This structure holds all XPS maps for device.  Maps are indexed by CPU.\n */\nstruct xps_dev_maps {\n\tstruct rcu_head rcu;\n\tstruct xps_map __rcu *cpu_map[0];\n};\n#define XPS_DEV_MAPS_SIZE (sizeof(struct xps_dev_maps) +\t\t\\\n    (nr_cpu_ids * sizeof(struct xps_map *)))\n#endif /* CONFIG_XPS */\n\n/*\n * This structure defines the management hooks for network devices.\n * The following hooks can be defined; unless noted otherwise, they are\n * optional and can be filled with a null pointer.\n *\n * int (*ndo_init)(struct net_device *dev);\n *     This function is called once when network device is registered.\n *     The network device can use this to any late stage initializaton\n *     or semantic validattion. It can fail with an error code which will\n *     be propogated back to register_netdev\n *\n * void (*ndo_uninit)(struct net_device *dev);\n *     This function is called when device is unregistered or when registration\n *     fails. It is not called if init fails.\n *\n * int (*ndo_open)(struct net_device *dev);\n *     This function is called when network device transistions to the up\n *     state.\n *\n * int (*ndo_stop)(struct net_device *dev);\n *     This function is called when network device transistions to the down\n *     state.\n *\n * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,\n *                               struct net_device *dev);\n *\tCalled when a packet needs to be transmitted.\n *\tMust return NETDEV_TX_OK , NETDEV_TX_BUSY.\n *        (can also return NETDEV_TX_LOCKED iff NETIF_F_LLTX)\n *\tRequired can not be NULL.\n *\n * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb);\n *\tCalled to decide which queue to when device supports multiple\n *\ttransmit queues.\n *\n * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);\n *\tThis function is called to allow device receiver to make\n *\tchanges to configuration when multicast or promiscious is enabled.\n *\n * void (*ndo_set_rx_mode)(struct net_device *dev);\n *\tThis function is called device changes address list filtering.\n *\n * void (*ndo_set_multicast_list)(struct net_device *dev);\n *\tThis function is called when the multicast address list changes.\n *\n * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);\n *\tThis function  is called when the Media Access Control address\n *\tneeds to be changed. If this interface is not defined, the\n *\tmac address can not be changed.\n *\n * int (*ndo_validate_addr)(struct net_device *dev);\n *\tTest if Media Access Control address is valid for the device.\n *\n * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);\n *\tCalled when a user request an ioctl which can't be handled by\n *\tthe generic interface code. If not defined ioctl's return\n *\tnot supported error code.\n *\n * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);\n *\tUsed to set network devices bus interface parameters. This interface\n *\tis retained for legacy reason, new devices should use the bus\n *\tinterface (PCI) for low level management.\n *\n * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);\n *\tCalled when a user wants to change the Maximum Transfer Unit\n *\tof a device. If not defined, any request to change MTU will\n *\twill return an error.\n *\n * void (*ndo_tx_timeout)(struct net_device *dev);\n *\tCallback uses when the transmitter has not made any progress\n *\tfor dev->watchdog ticks.\n *\n * struct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,\n *                      struct rtnl_link_stats64 *storage);\n * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n *\tCalled when a user wants to get the network device usage\n *\tstatistics. Drivers must do one of the following:\n *\t1. Define @ndo_get_stats64 to fill in a zero-initialised\n *\t   rtnl_link_stats64 structure passed by the caller.\n *\t2. Define @ndo_get_stats to update a net_device_stats structure\n *\t   (which should normally be dev->stats) and return a pointer to\n *\t   it. The structure may be changed asynchronously only if each\n *\t   field is written atomically.\n *\t3. Update dev->stats asynchronously and atomically, and define\n *\t   neither operation.\n *\n * void (*ndo_vlan_rx_register)(struct net_device *dev, struct vlan_group *grp);\n *\tIf device support VLAN receive acceleration\n *\t(ie. dev->features & NETIF_F_HW_VLAN_RX), then this function is called\n *\twhen vlan groups for the device changes.  Note: grp is NULL\n *\tif no vlan's groups are being used.\n *\n * void (*ndo_vlan_rx_add_vid)(struct net_device *dev, unsigned short vid);\n *\tIf device support VLAN filtering (dev->features & NETIF_F_HW_VLAN_FILTER)\n *\tthis function is called when a VLAN id is registered.\n *\n * void (*ndo_vlan_rx_kill_vid)(struct net_device *dev, unsigned short vid);\n *\tIf device support VLAN filtering (dev->features & NETIF_F_HW_VLAN_FILTER)\n *\tthis function is called when a VLAN id is unregistered.\n *\n * void (*ndo_poll_controller)(struct net_device *dev);\n *\n *\tSR-IOV management functions.\n * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);\n * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan, u8 qos);\n * int (*ndo_set_vf_tx_rate)(struct net_device *dev, int vf, int rate);\n * int (*ndo_get_vf_config)(struct net_device *dev,\n *\t\t\t    int vf, struct ifla_vf_info *ivf);\n * int (*ndo_set_vf_port)(struct net_device *dev, int vf,\n *\t\t\t  struct nlattr *port[]);\n * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);\n */\n#define HAVE_NET_DEVICE_OPS\nstruct net_device_ops {\n\tint\t\t\t(*ndo_init)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_uninit)(struct net_device *dev);\n\tint\t\t\t(*ndo_open)(struct net_device *dev);\n\tint\t\t\t(*ndo_stop)(struct net_device *dev);\n\tnetdev_tx_t\t\t(*ndo_start_xmit) (struct sk_buff *skb,\n\t\t\t\t\t\t   struct net_device *dev);\n\tu16\t\t\t(*ndo_select_queue)(struct net_device *dev,\n\t\t\t\t\t\t    struct sk_buff *skb);\n\tvoid\t\t\t(*ndo_change_rx_flags)(struct net_device *dev,\n\t\t\t\t\t\t       int flags);\n\tvoid\t\t\t(*ndo_set_rx_mode)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_set_multicast_list)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_mac_address)(struct net_device *dev,\n\t\t\t\t\t\t       void *addr);\n\tint\t\t\t(*ndo_validate_addr)(struct net_device *dev);\n\tint\t\t\t(*ndo_do_ioctl)(struct net_device *dev,\n\t\t\t\t\t        struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_set_config)(struct net_device *dev,\n\t\t\t\t\t          struct ifmap *map);\n\tint\t\t\t(*ndo_change_mtu)(struct net_device *dev,\n\t\t\t\t\t\t  int new_mtu);\n\tint\t\t\t(*ndo_neigh_setup)(struct net_device *dev,\n\t\t\t\t\t\t   struct neigh_parms *);\n\tvoid\t\t\t(*ndo_tx_timeout) (struct net_device *dev);\n\n\tstruct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,\n\t\t\t\t\t\t     struct rtnl_link_stats64 *storage);\n\tstruct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n\n\tvoid\t\t\t(*ndo_vlan_rx_register)(struct net_device *dev,\n\t\t\t\t\t\t        struct vlan_group *grp);\n\tvoid\t\t\t(*ndo_vlan_rx_add_vid)(struct net_device *dev,\n\t\t\t\t\t\t       unsigned short vid);\n\tvoid\t\t\t(*ndo_vlan_rx_kill_vid)(struct net_device *dev,\n\t\t\t\t\t\t        unsigned short vid);\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\tvoid                    (*ndo_poll_controller)(struct net_device *dev);\n\tint\t\t\t(*ndo_netpoll_setup)(struct net_device *dev,\n\t\t\t\t\t\t     struct netpoll_info *info);\n\tvoid\t\t\t(*ndo_netpoll_cleanup)(struct net_device *dev);\n#endif\n\tint\t\t\t(*ndo_set_vf_mac)(struct net_device *dev,\n\t\t\t\t\t\t  int queue, u8 *mac);\n\tint\t\t\t(*ndo_set_vf_vlan)(struct net_device *dev,\n\t\t\t\t\t\t   int queue, u16 vlan, u8 qos);\n\tint\t\t\t(*ndo_set_vf_tx_rate)(struct net_device *dev,\n\t\t\t\t\t\t      int vf, int rate);\n\tint\t\t\t(*ndo_get_vf_config)(struct net_device *dev,\n\t\t\t\t\t\t     int vf,\n\t\t\t\t\t\t     struct ifla_vf_info *ivf);\n\tint\t\t\t(*ndo_set_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct nlattr *port[]);\n\tint\t\t\t(*ndo_get_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, struct sk_buff *skb);\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n\tint\t\t\t(*ndo_fcoe_enable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_disable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_ddp_setup)(struct net_device *dev,\n\t\t\t\t\t\t      u16 xid,\n\t\t\t\t\t\t      struct scatterlist *sgl,\n\t\t\t\t\t\t      unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_ddp_done)(struct net_device *dev,\n\t\t\t\t\t\t     u16 xid);\n#define NETDEV_FCOE_WWNN 0\n#define NETDEV_FCOE_WWPN 1\n\tint\t\t\t(*ndo_fcoe_get_wwn)(struct net_device *dev,\n\t\t\t\t\t\t    u64 *wwn, int type);\n#endif\n};\n\n/*\n *\tThe DEVICE structure.\n *\tActually, this whole structure is a big mistake.  It mixes I/O\n *\tdata with strictly \"high-level\" data, and it has to know about\n *\talmost every data structure used in the INET module.\n *\n *\tFIXME: cleanup struct net_device such that network protocol info\n *\tmoves out.\n */\n\nstruct net_device {\n\n\t/*\n\t * This is the first field of the \"visible\" part of this structure\n\t * (i.e. as seen by users in the \"Space.c\" file).  It is the name\n\t * of the interface.\n\t */\n\tchar\t\t\tname[IFNAMSIZ];\n\n\tstruct pm_qos_request_list pm_qos_req;\n\n\t/* device name hash chain */\n\tstruct hlist_node\tname_hlist;\n\t/* snmp alias */\n\tchar \t\t\t*ifalias;\n\n\t/*\n\t *\tI/O specific fields\n\t *\tFIXME: Merge these and struct ifmap into one\n\t */\n\tunsigned long\t\tmem_end;\t/* shared mem end\t*/\n\tunsigned long\t\tmem_start;\t/* shared mem start\t*/\n\tunsigned long\t\tbase_addr;\t/* device I/O address\t*/\n\tunsigned int\t\tirq;\t\t/* device IRQ number\t*/\n\n\t/*\n\t *\tSome hardware also needs these fields, but they are not\n\t *\tpart of the usual set specified in Space.c.\n\t */\n\n\tunsigned char\t\tif_port;\t/* Selectable AUI, TP,..*/\n\tunsigned char\t\tdma;\t\t/* DMA channel\t\t*/\n\n\tunsigned long\t\tstate;\n\n\tstruct list_head\tdev_list;\n\tstruct list_head\tnapi_list;\n\tstruct list_head\tunreg_list;\n\n\t/* Net device features */\n\tunsigned long\t\tfeatures;\n#define NETIF_F_SG\t\t1\t/* Scatter/gather IO. */\n#define NETIF_F_IP_CSUM\t\t2\t/* Can checksum TCP/UDP over IPv4. */\n#define NETIF_F_NO_CSUM\t\t4\t/* Does not require checksum. F.e. loopack. */\n#define NETIF_F_HW_CSUM\t\t8\t/* Can checksum all the packets. */\n#define NETIF_F_IPV6_CSUM\t16\t/* Can checksum TCP/UDP over IPV6 */\n#define NETIF_F_HIGHDMA\t\t32\t/* Can DMA to high memory. */\n#define NETIF_F_FRAGLIST\t64\t/* Scatter/gather IO. */\n#define NETIF_F_HW_VLAN_TX\t128\t/* Transmit VLAN hw acceleration */\n#define NETIF_F_HW_VLAN_RX\t256\t/* Receive VLAN hw acceleration */\n#define NETIF_F_HW_VLAN_FILTER\t512\t/* Receive filtering on VLAN */\n#define NETIF_F_VLAN_CHALLENGED\t1024\t/* Device cannot handle VLAN packets */\n#define NETIF_F_GSO\t\t2048\t/* Enable software GSO. */\n#define NETIF_F_LLTX\t\t4096\t/* LockLess TX - deprecated. Please */\n\t\t\t\t\t/* do not use LLTX in new drivers */\n#define NETIF_F_NETNS_LOCAL\t8192\t/* Does not change network namespaces */\n#define NETIF_F_GRO\t\t16384\t/* Generic receive offload */\n#define NETIF_F_LRO\t\t32768\t/* large receive offload */\n\n/* the GSO_MASK reserves bits 16 through 23 */\n#define NETIF_F_FCOE_CRC\t(1 << 24) /* FCoE CRC32 */\n#define NETIF_F_SCTP_CSUM\t(1 << 25) /* SCTP checksum offload */\n#define NETIF_F_FCOE_MTU\t(1 << 26) /* Supports max FCoE MTU, 2158 bytes*/\n#define NETIF_F_NTUPLE\t\t(1 << 27) /* N-tuple filters supported */\n#define NETIF_F_RXHASH\t\t(1 << 28) /* Receive hashing offload */\n\n\t/* Segmentation offload features */\n#define NETIF_F_GSO_SHIFT\t16\n#define NETIF_F_GSO_MASK\t0x00ff0000\n#define NETIF_F_TSO\t\t(SKB_GSO_TCPV4 << NETIF_F_GSO_SHIFT)\n#define NETIF_F_UFO\t\t(SKB_GSO_UDP << NETIF_F_GSO_SHIFT)\n#define NETIF_F_GSO_ROBUST\t(SKB_GSO_DODGY << NETIF_F_GSO_SHIFT)\n#define NETIF_F_TSO_ECN\t\t(SKB_GSO_TCP_ECN << NETIF_F_GSO_SHIFT)\n#define NETIF_F_TSO6\t\t(SKB_GSO_TCPV6 << NETIF_F_GSO_SHIFT)\n#define NETIF_F_FSO\t\t(SKB_GSO_FCOE << NETIF_F_GSO_SHIFT)\n\n\t/* List of features with software fallbacks. */\n#define NETIF_F_GSO_SOFTWARE\t(NETIF_F_TSO | NETIF_F_TSO_ECN | \\\n\t\t\t\t NETIF_F_TSO6 | NETIF_F_UFO)\n\n\n#define NETIF_F_GEN_CSUM\t(NETIF_F_NO_CSUM | NETIF_F_HW_CSUM)\n#define NETIF_F_V4_CSUM\t\t(NETIF_F_GEN_CSUM | NETIF_F_IP_CSUM)\n#define NETIF_F_V6_CSUM\t\t(NETIF_F_GEN_CSUM | NETIF_F_IPV6_CSUM)\n#define NETIF_F_ALL_CSUM\t(NETIF_F_V4_CSUM | NETIF_F_V6_CSUM)\n\n\t/*\n\t * If one device supports one of these features, then enable them\n\t * for all in netdev_increment_features.\n\t */\n#define NETIF_F_ONE_FOR_ALL\t(NETIF_F_GSO_SOFTWARE | NETIF_F_GSO_ROBUST | \\\n\t\t\t\t NETIF_F_SG | NETIF_F_HIGHDMA |\t\t\\\n\t\t\t\t NETIF_F_FRAGLIST)\n\n\t/* Interface index. Unique device identifier\t*/\n\tint\t\t\tifindex;\n\tint\t\t\tiflink;\n\n\tstruct net_device_stats\tstats;\n\tatomic_long_t\t\trx_dropped; /* dropped packets by core network\n\t\t\t\t\t     * Do not use this in drivers.\n\t\t\t\t\t     */\n\n#ifdef CONFIG_WIRELESS_EXT\n\t/* List of functions to handle Wireless Extensions (instead of ioctl).\n\t * See <net/iw_handler.h> for details. Jean II */\n\tconst struct iw_handler_def *\twireless_handlers;\n\t/* Instance data managed by the core of Wireless Extensions. */\n\tstruct iw_public_data *\twireless_data;\n#endif\n\t/* Management operations */\n\tconst struct net_device_ops *netdev_ops;\n\tconst struct ethtool_ops *ethtool_ops;\n\n\t/* Hardware header description */\n\tconst struct header_ops *header_ops;\n\n\tunsigned int\t\tflags;\t/* interface flags (a la BSD)\t*/\n\tunsigned short\t\tgflags;\n        unsigned int            priv_flags; /* Like 'flags' but invisible to userspace. */\n\tunsigned short\t\tpadded;\t/* How much padding added by alloc_netdev() */\n\n\tunsigned char\t\toperstate; /* RFC2863 operstate */\n\tunsigned char\t\tlink_mode; /* mapping policy to operstate */\n\n\tunsigned int\t\tmtu;\t/* interface MTU value\t\t*/\n\tunsigned short\t\ttype;\t/* interface hardware type\t*/\n\tunsigned short\t\thard_header_len;\t/* hardware hdr length\t*/\n\n\t/* extra head- and tailroom the hardware may need, but not in all cases\n\t * can this be guaranteed, especially tailroom. Some cases also use\n\t * LL_MAX_HEADER instead to allocate the skb.\n\t */\n\tunsigned short\t\tneeded_headroom;\n\tunsigned short\t\tneeded_tailroom;\n\n\t/* Interface address info. */\n\tunsigned char\t\tperm_addr[MAX_ADDR_LEN]; /* permanent hw address */\n\tunsigned char\t\taddr_assign_type; /* hw address assignment type */\n\tunsigned char\t\taddr_len;\t/* hardware address length\t*/\n\tunsigned short          dev_id;\t\t/* for shared network cards */\n\n\tspinlock_t\t\taddr_list_lock;\n\tstruct netdev_hw_addr_list\tuc;\t/* Unicast mac addresses */\n\tstruct netdev_hw_addr_list\tmc;\t/* Multicast mac addresses */\n\tint\t\t\tuc_promisc;\n\tunsigned int\t\tpromiscuity;\n\tunsigned int\t\tallmulti;\n\n\n\t/* Protocol specific pointers */\n\n#if defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE)\n\tstruct vlan_group __rcu\t*vlgrp;\t\t/* VLAN group */\n#endif\n#ifdef CONFIG_NET_DSA\n\tvoid\t\t\t*dsa_ptr;\t/* dsa specific data */\n#endif\n\tvoid \t\t\t*atalk_ptr;\t/* AppleTalk link \t*/\n\tstruct in_device __rcu\t*ip_ptr;\t/* IPv4 specific data\t*/\n\tstruct dn_dev __rcu     *dn_ptr;        /* DECnet specific data */\n\tstruct inet6_dev __rcu\t*ip6_ptr;       /* IPv6 specific data */\n\tvoid\t\t\t*ec_ptr;\t/* Econet specific data\t*/\n\tvoid\t\t\t*ax25_ptr;\t/* AX.25 specific data */\n\tstruct wireless_dev\t*ieee80211_ptr;\t/* IEEE 802.11 specific data,\n\t\t\t\t\t\t   assign before registering */\n\n/*\n * Cache lines mostly used on receive path (including eth_type_trans())\n */\n\tunsigned long\t\tlast_rx;\t/* Time of last Rx\n\t\t\t\t\t\t * This should not be set in\n\t\t\t\t\t\t * drivers, unless really needed,\n\t\t\t\t\t\t * because network stack (bonding)\n\t\t\t\t\t\t * use it if/when necessary, to\n\t\t\t\t\t\t * avoid dirtying this cache line.\n\t\t\t\t\t\t */\n\n\tstruct net_device\t*master; /* Pointer to master device of a group,\n\t\t\t\t\t  * which this device is member of.\n\t\t\t\t\t  */\n\n\t/* Interface address info used in eth_type_trans() */\n\tunsigned char\t\t*dev_addr;\t/* hw address, (before bcast\n\t\t\t\t\t\t   because most packets are\n\t\t\t\t\t\t   unicast) */\n\n\tstruct netdev_hw_addr_list\tdev_addrs; /* list of device\n\t\t\t\t\t\t      hw addresses */\n\n\tunsigned char\t\tbroadcast[MAX_ADDR_LEN];\t/* hw bcast add\t*/\n\n#ifdef CONFIG_RPS\n\tstruct kset\t\t*queues_kset;\n\n\tstruct netdev_rx_queue\t*_rx;\n\n\t/* Number of RX queues allocated at register_netdev() time */\n\tunsigned int\t\tnum_rx_queues;\n\n\t/* Number of RX queues currently active in device */\n\tunsigned int\t\treal_num_rx_queues;\n#endif\n\n\trx_handler_func_t __rcu\t*rx_handler;\n\tvoid __rcu\t\t*rx_handler_data;\n\n\tstruct netdev_queue __rcu *ingress_queue;\n\n/*\n * Cache lines mostly used on transmit path\n */\n\tstruct netdev_queue\t*_tx ____cacheline_aligned_in_smp;\n\n\t/* Number of TX queues allocated at alloc_netdev_mq() time  */\n\tunsigned int\t\tnum_tx_queues;\n\n\t/* Number of TX queues currently active in device  */\n\tunsigned int\t\treal_num_tx_queues;\n\n\t/* root qdisc from userspace point of view */\n\tstruct Qdisc\t\t*qdisc;\n\n\tunsigned long\t\ttx_queue_len;\t/* Max frames per queue allowed */\n\tspinlock_t\t\ttx_global_lock;\n\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps __rcu *xps_maps;\n#endif\n\n\t/* These may be needed for future network-power-down code. */\n\n\t/*\n\t * trans_start here is expensive for high speed devices on SMP,\n\t * please use netdev_queue->trans_start instead.\n\t */\n\tunsigned long\t\ttrans_start;\t/* Time (in jiffies) of last Tx\t*/\n\n\tint\t\t\twatchdog_timeo; /* used by dev_watchdog() */\n\tstruct timer_list\twatchdog_timer;\n\n\t/* Number of references to this device */\n\tint __percpu\t\t*pcpu_refcnt;\n\n\t/* delayed register/unregister */\n\tstruct list_head\ttodo_list;\n\t/* device index hash chain */\n\tstruct hlist_node\tindex_hlist;\n\n\tstruct list_head\tlink_watch_list;\n\n\t/* register/unregister state machine */\n\tenum { NETREG_UNINITIALIZED=0,\n\t       NETREG_REGISTERED,\t/* completed register_netdevice */\n\t       NETREG_UNREGISTERING,\t/* called unregister_netdevice */\n\t       NETREG_UNREGISTERED,\t/* completed unregister todo */\n\t       NETREG_RELEASED,\t\t/* called free_netdev */\n\t       NETREG_DUMMY,\t\t/* dummy device for NAPI poll */\n\t} reg_state:16;\n\n\tenum {\n\t\tRTNL_LINK_INITIALIZED,\n\t\tRTNL_LINK_INITIALIZING,\n\t} rtnl_link_state:16;\n\n\t/* Called from unregister, can be used to call free_netdev */\n\tvoid (*destructor)(struct net_device *dev);\n\n#ifdef CONFIG_NETPOLL\n\tstruct netpoll_info\t*npinfo;\n#endif\n\n#ifdef CONFIG_NET_NS\n\t/* Network namespace this network device is inside */\n\tstruct net\t\t*nd_net;\n#endif\n\n\t/* mid-layer private */\n\tunion {\n\t\tvoid\t\t\t\t*ml_priv;\n\t\tstruct pcpu_lstats __percpu\t*lstats; /* loopback stats */\n\t\tstruct pcpu_tstats __percpu\t*tstats; /* tunnel stats */\n\t\tstruct pcpu_dstats __percpu\t*dstats; /* dummy stats */\n\t};\n\t/* GARP */\n\tstruct garp_port __rcu\t*garp_port;\n\n\t/* class/net/name entry */\n\tstruct device\t\tdev;\n\t/* space for optional device, statistics, and wireless sysfs groups */\n\tconst struct attribute_group *sysfs_groups[4];\n\n\t/* rtnetlink link ops */\n\tconst struct rtnl_link_ops *rtnl_link_ops;\n\n\t/* VLAN feature mask */\n\tunsigned long vlan_features;\n\n\t/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SIZE\t\t65536\n\tunsigned int\t\tgso_max_size;\n\n#ifdef CONFIG_DCB\n\t/* Data Center Bridging netlink ops */\n\tconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\n\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n\t/* max exchange id for FCoE LRO by ddp */\n\tunsigned int\t\tfcoe_ddp_xid;\n#endif\n\t/* n-tuple filter list attached to this device */\n\tstruct ethtool_rx_ntuple_list ethtool_ntuple_list;\n\n\t/* phy device may attach itself for hardware timestamping */\n\tstruct phy_device *phydev;\n};\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n\n#define\tNETDEV_ALIGN\t\t32\n\nstatic inline\nstruct netdev_queue *netdev_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t unsigned int index)\n{\n\treturn &dev->_tx[index];\n}\n\nstatic inline void netdev_for_each_tx_queue(struct net_device *dev,\n\t\t\t\t\t    void (*f)(struct net_device *,\n\t\t\t\t\t\t      struct netdev_queue *,\n\t\t\t\t\t\t      void *),\n\t\t\t\t\t    void *arg)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tf(dev, &dev->_tx[i], arg);\n}\n\n/*\n * Net namespace inlines\n */\nstatic inline\nstruct net *dev_net(const struct net_device *dev)\n{\n\treturn read_pnet(&dev->nd_net);\n}\n\nstatic inline\nvoid dev_net_set(struct net_device *dev, struct net *net)\n{\n#ifdef CONFIG_NET_NS\n\trelease_net(dev->nd_net);\n\tdev->nd_net = hold_net(net);\n#endif\n}\n\nstatic inline bool netdev_uses_dsa_tags(struct net_device *dev)\n{\n#ifdef CONFIG_NET_DSA_TAG_DSA\n\tif (dev->dsa_ptr != NULL)\n\t\treturn dsa_uses_dsa_tags(dev->dsa_ptr);\n#endif\n\n\treturn 0;\n}\n\n#ifndef CONFIG_NET_NS\nstatic inline void skb_set_dev(struct sk_buff *skb, struct net_device *dev)\n{\n\tskb->dev = dev;\n}\n#else /* CONFIG_NET_NS */\nvoid skb_set_dev(struct sk_buff *skb, struct net_device *dev);\n#endif\n\nstatic inline bool netdev_uses_trailer_tags(struct net_device *dev)\n{\n#ifdef CONFIG_NET_DSA_TAG_TRAILER\n\tif (dev->dsa_ptr != NULL)\n\t\treturn dsa_uses_trailer_tags(dev->dsa_ptr);\n#endif\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_priv - access network device private data\n *\t@dev: network device\n *\n * Get network device private data\n */\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\n\treturn (char *)dev + ALIGN(sizeof(struct net_device), NETDEV_ALIGN);\n}\n\n/* Set the sysfs physical device reference for the network logical device\n * if set prior to registration will cause a symlink during initialization.\n */\n#define SET_NETDEV_DEV(net, pdev)\t((net)->dev.parent = (pdev))\n\n/* Set the sysfs device type for the network logical device to allow\n * fin grained indentification of different network device types. For\n * example Ethernet, Wirelss LAN, Bluetooth, WiMAX etc.\n */\n#define SET_NETDEV_DEVTYPE(net, devtype)\t((net)->dev.type = (devtype))\n\n/**\n *\tnetif_napi_add - initialize a napi context\n *\t@dev:  network device\n *\t@napi: napi context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * netif_napi_add() must be used to initialize a napi context prior to calling\n * *any* of the other napi related functions.\n */\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight);\n\n/**\n *  netif_napi_del - remove a napi context\n *  @napi: napi context\n *\n *  netif_napi_del() removes a napi context from the network device napi list\n */\nvoid netif_napi_del(struct napi_struct *napi);\n\nstruct napi_gro_cb {\n\t/* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */\n\tvoid *frag0;\n\n\t/* Length of frag0. */\n\tunsigned int frag0_len;\n\n\t/* This indicates where we are processing relative to skb->data. */\n\tint data_offset;\n\n\t/* This is non-zero if the packet may be of the same flow. */\n\tint same_flow;\n\n\t/* This is non-zero if the packet cannot be merged with the new skb. */\n\tint flush;\n\n\t/* Number of segments aggregated. */\n\tint count;\n\n\t/* Free the skb? */\n\tint free;\n};\n\n#define NAPI_GRO_CB(skb) ((struct napi_gro_cb *)(skb)->cb)\n\nstruct packet_type {\n\t__be16\t\t\ttype;\t/* This is really htons(ether_type). */\n\tstruct net_device\t*dev;\t/* NULL is wildcarded here\t     */\n\tint\t\t\t(*func) (struct sk_buff *,\n\t\t\t\t\t struct net_device *,\n\t\t\t\t\t struct packet_type *,\n\t\t\t\t\t struct net_device *);\n\tstruct sk_buff\t\t*(*gso_segment)(struct sk_buff *skb,\n\t\t\t\t\t\tint features);\n\tint\t\t\t(*gso_send_check)(struct sk_buff *skb);\n\tstruct sk_buff\t\t**(*gro_receive)(struct sk_buff **head,\n\t\t\t\t\t       struct sk_buff *skb);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb);\n\tvoid\t\t\t*af_packet_priv;\n\tstruct list_head\tlist;\n};\n\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n\nextern rwlock_t\t\t\t\tdev_base_lock;\t\t/* Device list lock */\n\n\n#define for_each_netdev(net, d)\t\t\\\n\t\tlist_for_each_entry(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_reverse(net, d)\t\\\n\t\tlist_for_each_entry_reverse(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_rcu(net, d)\t\t\\\n\t\tlist_for_each_entry_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_safe(net, d, n)\t\\\n\t\tlist_for_each_entry_safe(d, n, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue(net, d)\t\t\\\n\t\tlist_for_each_entry_continue(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue_rcu(net, d)\t\t\\\n\tlist_for_each_entry_continue_rcu(d, &(net)->dev_base_head, dev_list)\n#define net_device_entry(lh)\tlist_entry(lh, struct net_device, dev_list)\n\nstatic inline struct net_device *next_net_device(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = dev->dev_list.next;\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *next_net_device_rcu(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = rcu_dereference(dev->dev_list.next);\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *first_net_device(struct net *net)\n{\n\treturn list_empty(&net->dev_base_head) ? NULL :\n\t\tnet_device_entry(net->dev_base_head.next);\n}\n\nextern int \t\t\tnetdev_boot_setup_check(struct net_device *dev);\nextern unsigned long\t\tnetdev_boot_base(const char *prefix, int unit);\nextern struct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t\t      const char *hwaddr);\nextern struct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type);\nextern struct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type);\nextern void\t\tdev_add_pack(struct packet_type *pt);\nextern void\t\tdev_remove_pack(struct packet_type *pt);\nextern void\t\t__dev_remove_pack(struct packet_type *pt);\n\nextern struct net_device\t*dev_get_by_flags_rcu(struct net *net, unsigned short flags,\n\t\t\t\t\t\t      unsigned short mask);\nextern struct net_device\t*dev_get_by_name(struct net *net, const char *name);\nextern struct net_device\t*dev_get_by_name_rcu(struct net *net, const char *name);\nextern struct net_device\t*__dev_get_by_name(struct net *net, const char *name);\nextern int\t\tdev_alloc_name(struct net_device *dev, const char *name);\nextern int\t\tdev_open(struct net_device *dev);\nextern int\t\tdev_close(struct net_device *dev);\nextern void\t\tdev_disable_lro(struct net_device *dev);\nextern int\t\tdev_queue_xmit(struct sk_buff *skb);\nextern int\t\tregister_netdevice(struct net_device *dev);\nextern void\t\tunregister_netdevice_queue(struct net_device *dev,\n\t\t\t\t\t\t   struct list_head *head);\nextern void\t\tunregister_netdevice_many(struct list_head *head);\nstatic inline void unregister_netdevice(struct net_device *dev)\n{\n\tunregister_netdevice_queue(dev, NULL);\n}\n\nextern int \t\tnetdev_refcnt_read(const struct net_device *dev);\nextern void\t\tfree_netdev(struct net_device *dev);\nextern void\t\tsynchronize_net(void);\nextern int \t\tregister_netdevice_notifier(struct notifier_block *nb);\nextern int\t\tunregister_netdevice_notifier(struct notifier_block *nb);\nextern int\t\tinit_dummy_netdev(struct net_device *dev);\nextern void\t\tnetdev_resync_ops(struct net_device *dev);\n\nextern int call_netdevice_notifiers(unsigned long val, struct net_device *dev);\nextern struct net_device\t*dev_get_by_index(struct net *net, int ifindex);\nextern struct net_device\t*__dev_get_by_index(struct net *net, int ifindex);\nextern struct net_device\t*dev_get_by_index_rcu(struct net *net, int ifindex);\nextern int\t\tdev_restart(struct net_device *dev);\n#ifdef CONFIG_NETPOLL_TRAP\nextern int\t\tnetpoll_trap(void);\n#endif\nextern int\t       skb_gro_receive(struct sk_buff **head,\n\t\t\t\t       struct sk_buff *skb);\nextern void\t       skb_gro_reset_offset(struct sk_buff *skb);\n\nstatic inline unsigned int skb_gro_offset(const struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline unsigned int skb_gro_len(const struct sk_buff *skb)\n{\n\treturn skb->len - NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline void skb_gro_pull(struct sk_buff *skb, unsigned int len)\n{\n\tNAPI_GRO_CB(skb)->data_offset += len;\n}\n\nstatic inline void *skb_gro_header_fast(struct sk_buff *skb,\n\t\t\t\t\tunsigned int offset)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 + offset;\n}\n\nstatic inline int skb_gro_header_hard(struct sk_buff *skb, unsigned int hlen)\n{\n\treturn NAPI_GRO_CB(skb)->frag0_len < hlen;\n}\n\nstatic inline void *skb_gro_header_slow(struct sk_buff *skb, unsigned int hlen,\n\t\t\t\t\tunsigned int offset)\n{\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\treturn pskb_may_pull(skb, hlen) ? skb->data + offset : NULL;\n}\n\nstatic inline void *skb_gro_mac_header(struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 ?: skb_mac_header(skb);\n}\n\nstatic inline void *skb_gro_network_header(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->frag0 ?: skb->data) +\n\t       skb_network_offset(skb);\n}\n\nstatic inline int dev_hard_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t  unsigned short type,\n\t\t\t\t  const void *daddr, const void *saddr,\n\t\t\t\t  unsigned len)\n{\n\tif (!dev->header_ops || !dev->header_ops->create)\n\t\treturn 0;\n\n\treturn dev->header_ops->create(skb, dev, type, daddr, saddr, len);\n}\n\nstatic inline int dev_parse_header(const struct sk_buff *skb,\n\t\t\t\t   unsigned char *haddr)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse)\n\t\treturn 0;\n\treturn dev->header_ops->parse(skb, haddr);\n}\n\ntypedef int gifconf_func_t(struct net_device * dev, char __user * bufptr, int len);\nextern int\t\tregister_gifconf(unsigned int family, gifconf_func_t * gifconf);\nstatic inline int unregister_gifconf(unsigned int family)\n{\n\treturn register_gifconf(family, NULL);\n}\n\n/*\n * Incoming packets are placed on per-cpu queues\n */\nstruct softnet_data {\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n\tstruct list_head\tpoll_list;\n\tstruct sk_buff\t\t*completion_queue;\n\tstruct sk_buff_head\tprocess_queue;\n\n\t/* stats */\n\tunsigned int\t\tprocessed;\n\tunsigned int\t\ttime_squeeze;\n\tunsigned int\t\tcpu_collision;\n\tunsigned int\t\treceived_rps;\n\n#ifdef CONFIG_RPS\n\tstruct softnet_data\t*rps_ipi_list;\n\n\t/* Elements below can be accessed between CPUs for RPS */\n\tstruct call_single_data\tcsd ____cacheline_aligned_in_smp;\n\tstruct softnet_data\t*rps_ipi_next;\n\tunsigned int\t\tcpu;\n\tunsigned int\t\tinput_queue_head;\n\tunsigned int\t\tinput_queue_tail;\n#endif\n\tunsigned\t\tdropped;\n\tstruct sk_buff_head\tinput_pkt_queue;\n\tstruct napi_struct\tbacklog;\n};\n\nstatic inline void input_queue_head_incr(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tsd->input_queue_head++;\n#endif\n}\n\nstatic inline void input_queue_tail_incr_save(struct softnet_data *sd,\n\t\t\t\t\t      unsigned int *qtail)\n{\n#ifdef CONFIG_RPS\n\t*qtail = ++sd->input_queue_tail;\n#endif\n}\n\nDECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\n\n#define HAVE_NETIF_QUEUE\n\nextern void __netif_schedule(struct Qdisc *q);\n\nstatic inline void netif_schedule_queue(struct netdev_queue *txq)\n{\n\tif (!test_bit(__QUEUE_STATE_XOFF, &txq->state))\n\t\t__netif_schedule(txq->qdisc);\n}\n\nstatic inline void netif_tx_schedule_all(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tnetif_schedule_queue(netdev_get_tx_queue(dev, i));\n}\n\nstatic inline void netif_tx_start_queue(struct netdev_queue *dev_queue)\n{\n\tclear_bit(__QUEUE_STATE_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_start_queue - allow transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n */\nstatic inline void netif_start_queue(struct net_device *dev)\n{\n\tnetif_tx_start_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_start_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nstatic inline void netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_NETPOLL_TRAP\n\tif (netpoll_trap()) {\n\t\tnetif_tx_start_queue(dev_queue);\n\t\treturn;\n\t}\n#endif\n\tif (test_and_clear_bit(__QUEUE_STATE_XOFF, &dev_queue->state))\n\t\t__netif_schedule(dev_queue->qdisc);\n}\n\n/**\n *\tnetif_wake_queue - restart transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are available.\n */\nstatic inline void netif_wake_queue(struct net_device *dev)\n{\n\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_wake_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n}\n\nstatic inline void netif_tx_stop_queue(struct netdev_queue *dev_queue)\n{\n\tif (WARN_ON(!dev_queue)) {\n\t\tprintk(KERN_INFO \"netif_stop_queue() cannot be called before \"\n\t\t       \"register_netdev()\");\n\t\treturn;\n\t}\n\tset_bit(__QUEUE_STATE_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_stop_queue - stop transmitted packets\n *\t@dev: network device\n *\n *\tStop upper layers calling the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are unavailable.\n */\nstatic inline void netif_stop_queue(struct net_device *dev)\n{\n\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\n\nstatic inline int netif_tx_queue_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn test_bit(__QUEUE_STATE_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_queue_stopped - test if transmit queue is flowblocked\n *\t@dev: network device\n *\n *\tTest if transmit queue on device is currently unable to send.\n */\nstatic inline int netif_queue_stopped(const struct net_device *dev)\n{\n\treturn netif_tx_queue_stopped(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline int netif_tx_queue_frozen_or_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_XOFF_OR_FROZEN;\n}\n\n/**\n *\tnetif_running - test if up\n *\t@dev: network device\n *\n *\tTest if the device has been brought up.\n */\nstatic inline int netif_running(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_START, &dev->state);\n}\n\n/*\n * Routines to manage the subqueues on a device.  We only need start\n * stop, and a check if it's stopped.  All other device management is\n * done at the overall netdevice level.\n * Also test the device if we're multiqueue.\n */\n\n/**\n *\tnetif_start_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Start individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_start_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_start_queue(txq);\n}\n\n/**\n *\tnetif_stop_subqueue - stop sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Stop individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_stop_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n#ifdef CONFIG_NETPOLL_TRAP\n\tif (netpoll_trap())\n\t\treturn;\n#endif\n\tnetif_tx_stop_queue(txq);\n}\n\n/**\n *\tnetif_subqueue_stopped - test status of subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Check individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline int __netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\treturn netif_tx_queue_stopped(txq);\n}\n\nstatic inline int netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\treturn __netif_subqueue_stopped(dev, skb_get_queue_mapping(skb));\n}\n\n/**\n *\tnetif_wake_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Resume individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_wake_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n#ifdef CONFIG_NETPOLL_TRAP\n\tif (netpoll_trap())\n\t\treturn;\n#endif\n\tif (test_and_clear_bit(__QUEUE_STATE_XOFF, &txq->state))\n\t\t__netif_schedule(txq->qdisc);\n}\n\n/*\n * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used\n * as a distribution range limit for the returned value.\n */\nstatic inline u16 skb_tx_hash(const struct net_device *dev,\n\t\t\t      const struct sk_buff *skb)\n{\n\treturn __skb_tx_hash(dev, skb, dev->real_num_tx_queues);\n}\n\n/**\n *\tnetif_is_multiqueue - test if device has multiple transmit queues\n *\t@dev: network device\n *\n * Check if device has multiple transmit queues\n */\nstatic inline int netif_is_multiqueue(const struct net_device *dev)\n{\n\treturn dev->num_tx_queues > 1;\n}\n\nextern int netif_set_real_num_tx_queues(struct net_device *dev,\n\t\t\t\t\tunsigned int txq);\n\n#ifdef CONFIG_RPS\nextern int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\tunsigned int rxq);\n#else\nstatic inline int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\t\tunsigned int rxq)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline int netif_copy_real_num_queues(struct net_device *to_dev,\n\t\t\t\t\t     const struct net_device *from_dev)\n{\n\tnetif_set_real_num_tx_queues(to_dev, from_dev->real_num_tx_queues);\n#ifdef CONFIG_RPS\n\treturn netif_set_real_num_rx_queues(to_dev,\n\t\t\t\t\t    from_dev->real_num_rx_queues);\n#else\n\treturn 0;\n#endif\n}\n\n/* Use this variant when it is known for sure that it\n * is executing from hardware interrupt context or with hardware interrupts\n * disabled.\n */\nextern void dev_kfree_skb_irq(struct sk_buff *skb);\n\n/* Use this variant in places where it could be invoked\n * from either hardware interrupt or other context, with hardware interrupts\n * either disabled or enabled.\n */\nextern void dev_kfree_skb_any(struct sk_buff *skb);\n\n#define HAVE_NETIF_RX 1\nextern int\t\tnetif_rx(struct sk_buff *skb);\nextern int\t\tnetif_rx_ni(struct sk_buff *skb);\n#define HAVE_NETIF_RECEIVE_SKB 1\nextern int\t\tnetif_receive_skb(struct sk_buff *skb);\nextern gro_result_t\tdev_gro_receive(struct napi_struct *napi,\n\t\t\t\t\tstruct sk_buff *skb);\nextern gro_result_t\tnapi_skb_finish(gro_result_t ret, struct sk_buff *skb);\nextern gro_result_t\tnapi_gro_receive(struct napi_struct *napi,\n\t\t\t\t\t struct sk_buff *skb);\nextern void\t\tnapi_gro_flush(struct napi_struct *napi);\nextern struct sk_buff *\tnapi_get_frags(struct napi_struct *napi);\nextern gro_result_t\tnapi_frags_finish(struct napi_struct *napi,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  gro_result_t ret);\nextern struct sk_buff *\tnapi_frags_skb(struct napi_struct *napi);\nextern gro_result_t\tnapi_gro_frags(struct napi_struct *napi);\n\nstatic inline void napi_free_frags(struct napi_struct *napi)\n{\n\tkfree_skb(napi->skb);\n\tnapi->skb = NULL;\n}\n\nextern int netdev_rx_handler_register(struct net_device *dev,\n\t\t\t\t      rx_handler_func_t *rx_handler,\n\t\t\t\t      void *rx_handler_data);\nextern void netdev_rx_handler_unregister(struct net_device *dev);\n\nextern int\t\tdev_valid_name(const char *name);\nextern int\t\tdev_ioctl(struct net *net, unsigned int cmd, void __user *);\nextern int\t\tdev_ethtool(struct net *net, struct ifreq *);\nextern unsigned\t\tdev_get_flags(const struct net_device *);\nextern int\t\t__dev_change_flags(struct net_device *, unsigned int flags);\nextern int\t\tdev_change_flags(struct net_device *, unsigned);\nextern void\t\t__dev_notify_flags(struct net_device *, unsigned int old_flags);\nextern int\t\tdev_change_name(struct net_device *, const char *);\nextern int\t\tdev_set_alias(struct net_device *, const char *, size_t);\nextern int\t\tdev_change_net_namespace(struct net_device *,\n\t\t\t\t\t\t struct net *, const char *);\nextern int\t\tdev_set_mtu(struct net_device *, int);\nextern int\t\tdev_set_mac_address(struct net_device *,\n\t\t\t\t\t    struct sockaddr *);\nextern int\t\tdev_hard_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    struct netdev_queue *txq);\nextern int\t\tdev_forward_skb(struct net_device *dev,\n\t\t\t\t\tstruct sk_buff *skb);\n\nextern int\t\tnetdev_budget;\n\n/* Called by rtnetlink.c:rtnl_unlock() */\nextern void netdev_run_todo(void);\n\n/**\n *\tdev_put - release reference to device\n *\t@dev: network device\n *\n * Release reference to device to allow it to be freed.\n */\nstatic inline void dev_put(struct net_device *dev)\n{\n\tirqsafe_cpu_dec(*dev->pcpu_refcnt);\n}\n\n/**\n *\tdev_hold - get reference to device\n *\t@dev: network device\n *\n * Hold reference to device to keep it from being freed.\n */\nstatic inline void dev_hold(struct net_device *dev)\n{\n\tirqsafe_cpu_inc(*dev->pcpu_refcnt);\n}\n\n/* Carrier loss detection, dial on demand. The functions netif_carrier_on\n * and _off may be called from IRQ context, but it is caller\n * who is responsible for serialization of these calls.\n *\n * The name carrier is inappropriate, these functions should really be\n * called netif_lowerlayer_*() because they represent the state of any\n * kind of lower layer not just hardware media.\n */\n\nextern void linkwatch_fire_event(struct net_device *dev);\nextern void linkwatch_forget_dev(struct net_device *dev);\n\n/**\n *\tnetif_carrier_ok - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline int netif_carrier_ok(const struct net_device *dev)\n{\n\treturn !test_bit(__LINK_STATE_NOCARRIER, &dev->state);\n}\n\nextern unsigned long dev_trans_start(struct net_device *dev);\n\nextern void __netdev_watchdog_up(struct net_device *dev);\n\nextern void netif_carrier_on(struct net_device *dev);\n\nextern void netif_carrier_off(struct net_device *dev);\n\nextern void netif_notify_peers(struct net_device *dev);\n\n/**\n *\tnetif_dormant_on - mark device as dormant.\n *\t@dev: network device\n *\n * Mark device as dormant (as per RFC2863).\n *\n * The dormant state indicates that the relevant interface is not\n * actually in a condition to pass packets (i.e., it is not 'up') but is\n * in a \"pending\" state, waiting for some external event.  For \"on-\n * demand\" interfaces, this new state identifies the situation where the\n * interface is waiting for events to place it in the up state.\n *\n */\nstatic inline void netif_dormant_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant_off - set device as not dormant.\n *\t@dev: network device\n *\n * Device is not in dormant state.\n */\nstatic inline void netif_dormant_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline int netif_dormant(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_DORMANT, &dev->state);\n}\n\n\n/**\n *\tnetif_oper_up - test if device is operational\n *\t@dev: network device\n *\n * Check if carrier is operational\n */\nstatic inline int netif_oper_up(const struct net_device *dev)\n{\n\treturn (dev->operstate == IF_OPER_UP ||\n\t\tdev->operstate == IF_OPER_UNKNOWN /* backward compat */);\n}\n\n/**\n *\tnetif_device_present - is device available or removed\n *\t@dev: network device\n *\n * Check if device has not been removed from system.\n */\nstatic inline int netif_device_present(struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_PRESENT, &dev->state);\n}\n\nextern void netif_device_detach(struct net_device *dev);\n\nextern void netif_device_attach(struct net_device *dev);\n\n/*\n * Network interface message level settings\n */\n#define HAVE_NETIF_MSG 1\n\nenum {\n\tNETIF_MSG_DRV\t\t= 0x0001,\n\tNETIF_MSG_PROBE\t\t= 0x0002,\n\tNETIF_MSG_LINK\t\t= 0x0004,\n\tNETIF_MSG_TIMER\t\t= 0x0008,\n\tNETIF_MSG_IFDOWN\t= 0x0010,\n\tNETIF_MSG_IFUP\t\t= 0x0020,\n\tNETIF_MSG_RX_ERR\t= 0x0040,\n\tNETIF_MSG_TX_ERR\t= 0x0080,\n\tNETIF_MSG_TX_QUEUED\t= 0x0100,\n\tNETIF_MSG_INTR\t\t= 0x0200,\n\tNETIF_MSG_TX_DONE\t= 0x0400,\n\tNETIF_MSG_RX_STATUS\t= 0x0800,\n\tNETIF_MSG_PKTDATA\t= 0x1000,\n\tNETIF_MSG_HW\t\t= 0x2000,\n\tNETIF_MSG_WOL\t\t= 0x4000,\n};\n\n#define netif_msg_drv(p)\t((p)->msg_enable & NETIF_MSG_DRV)\n#define netif_msg_probe(p)\t((p)->msg_enable & NETIF_MSG_PROBE)\n#define netif_msg_link(p)\t((p)->msg_enable & NETIF_MSG_LINK)\n#define netif_msg_timer(p)\t((p)->msg_enable & NETIF_MSG_TIMER)\n#define netif_msg_ifdown(p)\t((p)->msg_enable & NETIF_MSG_IFDOWN)\n#define netif_msg_ifup(p)\t((p)->msg_enable & NETIF_MSG_IFUP)\n#define netif_msg_rx_err(p)\t((p)->msg_enable & NETIF_MSG_RX_ERR)\n#define netif_msg_tx_err(p)\t((p)->msg_enable & NETIF_MSG_TX_ERR)\n#define netif_msg_tx_queued(p)\t((p)->msg_enable & NETIF_MSG_TX_QUEUED)\n#define netif_msg_intr(p)\t((p)->msg_enable & NETIF_MSG_INTR)\n#define netif_msg_tx_done(p)\t((p)->msg_enable & NETIF_MSG_TX_DONE)\n#define netif_msg_rx_status(p)\t((p)->msg_enable & NETIF_MSG_RX_STATUS)\n#define netif_msg_pktdata(p)\t((p)->msg_enable & NETIF_MSG_PKTDATA)\n#define netif_msg_hw(p)\t\t((p)->msg_enable & NETIF_MSG_HW)\n#define netif_msg_wol(p)\t((p)->msg_enable & NETIF_MSG_WOL)\n\nstatic inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)\n{\n\t/* use default */\n\tif (debug_value < 0 || debug_value >= (sizeof(u32) * 8))\n\t\treturn default_msg_enable_bits;\n\tif (debug_value == 0)\t/* no output */\n\t\treturn 0;\n\t/* set low N bits */\n\treturn (1 << debug_value) - 1;\n}\n\nstatic inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)\n{\n\tspin_lock(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = cpu;\n}\n\nstatic inline void __netif_tx_lock_bh(struct netdev_queue *txq)\n{\n\tspin_lock_bh(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = smp_processor_id();\n}\n\nstatic inline int __netif_tx_trylock(struct netdev_queue *txq)\n{\n\tint ok = spin_trylock(&txq->_xmit_lock);\n\tif (likely(ok))\n\t\ttxq->xmit_lock_owner = smp_processor_id();\n\treturn ok;\n}\n\nstatic inline void __netif_tx_unlock(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_unlock_bh(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock_bh(&txq->_xmit_lock);\n}\n\nstatic inline void txq_trans_update(struct netdev_queue *txq)\n{\n\tif (txq->xmit_lock_owner != -1)\n\t\ttxq->trans_start = jiffies;\n}\n\n/**\n *\tnetif_tx_lock - grab network device transmit lock\n *\t@dev: network device\n *\n * Get network device transmit lock\n */\nstatic inline void netif_tx_lock(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tspin_lock(&dev->tx_global_lock);\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* We are the only thread of execution doing a\n\t\t * freeze, but we have to grab the _xmit_lock in\n\t\t * order to synchronize with threads which are in\n\t\t * the ->hard_start_xmit() handler and already\n\t\t * checked the frozen bit.\n\t\t */\n\t\t__netif_tx_lock(txq, cpu);\n\t\tset_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic inline void netif_tx_lock_bh(struct net_device *dev)\n{\n\tlocal_bh_disable();\n\tnetif_tx_lock(dev);\n}\n\nstatic inline void netif_tx_unlock(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* No need to grab the _xmit_lock here.  If the\n\t\t * queue is not stopped for another reason, we\n\t\t * force a schedule.\n\t\t */\n\t\tclear_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\tnetif_schedule_queue(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n}\n\nstatic inline void netif_tx_unlock_bh(struct net_device *dev)\n{\n\tnetif_tx_unlock(dev);\n\tlocal_bh_enable();\n}\n\n#define HARD_TX_LOCK(dev, txq, cpu) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_lock(txq, cpu);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define HARD_TX_UNLOCK(dev, txq) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_unlock(txq);\t\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\nstatic inline void netif_tx_disable(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tlocal_bh_disable();\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t__netif_tx_lock(txq, cpu);\n\t\tnetif_tx_stop_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n\tlocal_bh_enable();\n}\n\nstatic inline void netif_addr_lock(struct net_device *dev)\n{\n\tspin_lock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_lock_bh(struct net_device *dev)\n{\n\tspin_lock_bh(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock(struct net_device *dev)\n{\n\tspin_unlock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock_bh(struct net_device *dev)\n{\n\tspin_unlock_bh(&dev->addr_list_lock);\n}\n\n/*\n * dev_addrs walker. Should be used only for read access. Call with\n * rcu_read_lock held.\n */\n#define for_each_dev_addr(dev, ha) \\\n\t\tlist_for_each_entry_rcu(ha, &dev->dev_addrs.list, list)\n\n/* These functions live elsewhere (drivers/net/net_init.c, but related) */\n\nextern void\t\tether_setup(struct net_device *dev);\n\n/* Support for loadable net-drivers */\nextern struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\t\t\t       void (*setup)(struct net_device *),\n\t\t\t\t       unsigned int txqs, unsigned int rxqs);\n#define alloc_netdev(sizeof_priv, name, setup) \\\n\talloc_netdev_mqs(sizeof_priv, name, setup, 1, 1)\n\n#define alloc_netdev_mq(sizeof_priv, name, setup, count) \\\n\talloc_netdev_mqs(sizeof_priv, name, setup, count, count)\n\nextern int\t\tregister_netdev(struct net_device *dev);\nextern void\t\tunregister_netdev(struct net_device *dev);\n\n/* General hardware address lists handling functions */\nextern int __hw_addr_add_multiple(struct netdev_hw_addr_list *to_list,\n\t\t\t\t  struct netdev_hw_addr_list *from_list,\n\t\t\t\t  int addr_len, unsigned char addr_type);\nextern void __hw_addr_del_multiple(struct netdev_hw_addr_list *to_list,\n\t\t\t\t   struct netdev_hw_addr_list *from_list,\n\t\t\t\t   int addr_len, unsigned char addr_type);\nextern int __hw_addr_sync(struct netdev_hw_addr_list *to_list,\n\t\t\t  struct netdev_hw_addr_list *from_list,\n\t\t\t  int addr_len);\nextern void __hw_addr_unsync(struct netdev_hw_addr_list *to_list,\n\t\t\t     struct netdev_hw_addr_list *from_list,\n\t\t\t     int addr_len);\nextern void __hw_addr_flush(struct netdev_hw_addr_list *list);\nextern void __hw_addr_init(struct netdev_hw_addr_list *list);\n\n/* Functions used for device addresses handling */\nextern int dev_addr_add(struct net_device *dev, unsigned char *addr,\n\t\t\tunsigned char addr_type);\nextern int dev_addr_del(struct net_device *dev, unsigned char *addr,\n\t\t\tunsigned char addr_type);\nextern int dev_addr_add_multiple(struct net_device *to_dev,\n\t\t\t\t struct net_device *from_dev,\n\t\t\t\t unsigned char addr_type);\nextern int dev_addr_del_multiple(struct net_device *to_dev,\n\t\t\t\t struct net_device *from_dev,\n\t\t\t\t unsigned char addr_type);\nextern void dev_addr_flush(struct net_device *dev);\nextern int dev_addr_init(struct net_device *dev);\n\n/* Functions used for unicast addresses handling */\nextern int dev_uc_add(struct net_device *dev, unsigned char *addr);\nextern int dev_uc_del(struct net_device *dev, unsigned char *addr);\nextern int dev_uc_sync(struct net_device *to, struct net_device *from);\nextern void dev_uc_unsync(struct net_device *to, struct net_device *from);\nextern void dev_uc_flush(struct net_device *dev);\nextern void dev_uc_init(struct net_device *dev);\n\n/* Functions used for multicast addresses handling */\nextern int dev_mc_add(struct net_device *dev, unsigned char *addr);\nextern int dev_mc_add_global(struct net_device *dev, unsigned char *addr);\nextern int dev_mc_del(struct net_device *dev, unsigned char *addr);\nextern int dev_mc_del_global(struct net_device *dev, unsigned char *addr);\nextern int dev_mc_sync(struct net_device *to, struct net_device *from);\nextern void dev_mc_unsync(struct net_device *to, struct net_device *from);\nextern void dev_mc_flush(struct net_device *dev);\nextern void dev_mc_init(struct net_device *dev);\n\n/* Functions used for secondary unicast and multicast support */\nextern void\t\tdev_set_rx_mode(struct net_device *dev);\nextern void\t\t__dev_set_rx_mode(struct net_device *dev);\nextern int\t\tdev_set_promiscuity(struct net_device *dev, int inc);\nextern int\t\tdev_set_allmulti(struct net_device *dev, int inc);\nextern void\t\tnetdev_state_change(struct net_device *dev);\nextern int\t\tnetdev_bonding_change(struct net_device *dev,\n\t\t\t\t\t      unsigned long event);\nextern void\t\tnetdev_features_change(struct net_device *dev);\n/* Load a device via the kmod */\nextern void\t\tdev_load(struct net *net, const char *name);\nextern void\t\tdev_mcast_init(void);\nextern struct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\t       struct rtnl_link_stats64 *storage);\n\nextern int\t\tnetdev_max_backlog;\nextern int\t\tnetdev_tstamp_prequeue;\nextern int\t\tweight_p;\nextern int\t\tnetdev_set_master(struct net_device *dev, struct net_device *master);\nextern int skb_checksum_help(struct sk_buff *skb);\nextern struct sk_buff *skb_gso_segment(struct sk_buff *skb, int features);\n#ifdef CONFIG_BUG\nextern void netdev_rx_csum_fault(struct net_device *dev);\n#else\nstatic inline void netdev_rx_csum_fault(struct net_device *dev)\n{\n}\n#endif\n/* rx skb timestamps */\nextern void\t\tnet_enable_timestamp(void);\nextern void\t\tnet_disable_timestamp(void);\n\n#ifdef CONFIG_PROC_FS\nextern void *dev_seq_start(struct seq_file *seq, loff_t *pos);\nextern void *dev_seq_next(struct seq_file *seq, void *v, loff_t *pos);\nextern void dev_seq_stop(struct seq_file *seq, void *v);\n#endif\n\nextern int netdev_class_create_file(struct class_attribute *class_attr);\nextern void netdev_class_remove_file(struct class_attribute *class_attr);\n\nextern struct kobj_ns_type_operations net_ns_type_operations;\n\nextern char *netdev_drivername(const struct net_device *dev, char *buffer, int len);\n\nextern void linkwatch_run_queue(void);\n\nunsigned long netdev_increment_features(unsigned long all, unsigned long one,\n\t\t\t\t\tunsigned long mask);\nunsigned long netdev_fix_features(unsigned long features, const char *name);\n\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev);\n\nint netif_skb_features(struct sk_buff *skb);\n\nstatic inline int net_gso_ok(int features, int gso_type)\n{\n\tint feature = gso_type << NETIF_F_GSO_SHIFT;\n\treturn (features & feature) == feature;\n}\n\nstatic inline int skb_gso_ok(struct sk_buff *skb, int features)\n{\n\treturn net_gso_ok(features, skb_shinfo(skb)->gso_type) &&\n\t       (!skb_has_frag_list(skb) || (features & NETIF_F_FRAGLIST));\n}\n\nstatic inline int netif_needs_gso(struct sk_buff *skb, int features)\n{\n\treturn skb_is_gso(skb) && (!skb_gso_ok(skb, features) ||\n\t\tunlikely(skb->ip_summed != CHECKSUM_PARTIAL));\n}\n\nstatic inline void netif_set_gso_max_size(struct net_device *dev,\n\t\t\t\t\t  unsigned int size)\n{\n\tdev->gso_max_size = size;\n}\n\nextern int __skb_bond_should_drop(struct sk_buff *skb,\n\t\t\t\t  struct net_device *master);\n\nstatic inline int skb_bond_should_drop(struct sk_buff *skb,\n\t\t\t\t       struct net_device *master)\n{\n\tif (master)\n\t\treturn __skb_bond_should_drop(skb, master);\n\treturn 0;\n}\n\nextern struct pernet_operations __net_initdata loopback_net_ops;\n\nstatic inline int dev_ethtool_get_settings(struct net_device *dev,\n\t\t\t\t\t   struct ethtool_cmd *cmd)\n{\n\tif (!dev->ethtool_ops || !dev->ethtool_ops->get_settings)\n\t\treturn -EOPNOTSUPP;\n\treturn dev->ethtool_ops->get_settings(dev, cmd);\n}\n\nstatic inline u32 dev_ethtool_get_rx_csum(struct net_device *dev)\n{\n\tif (!dev->ethtool_ops || !dev->ethtool_ops->get_rx_csum)\n\t\treturn 0;\n\treturn dev->ethtool_ops->get_rx_csum(dev);\n}\n\nstatic inline u32 dev_ethtool_get_flags(struct net_device *dev)\n{\n\tif (!dev->ethtool_ops || !dev->ethtool_ops->get_flags)\n\t\treturn 0;\n\treturn dev->ethtool_ops->get_flags(dev);\n}\n\n/* Logging, debugging and troubleshooting/diagnostic helpers. */\n\n/* netdev_printk helpers, similar to dev_printk */\n\nstatic inline const char *netdev_name(const struct net_device *dev)\n{\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\treturn \"(unregistered net_device)\";\n\treturn dev->name;\n}\n\nextern int netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t const char *format, ...)\n\t__attribute__ ((format (printf, 3, 4)));\nextern int netdev_emerg(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_alert(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_crit(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_err(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_warn(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_notice(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_info(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\n\n#if defined(DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n\tnetdev_printk(KERN_DEBUG, __dev, format, ##args)\n#elif defined(CONFIG_DYNAMIC_DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tdynamic_dev_dbg((__dev)->dev.parent, \"%s: \" format,\t\\\n\t\t\tnetdev_name(__dev), ##args);\t\t\\\n} while (0)\n#else\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, __dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netdev_vdbg\tnetdev_dbg\n#else\n\n#define netdev_vdbg(dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, dev, format, ##args);\t\\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n * netdev_WARN() acts like dev_printk(), but with the key difference\n * of using a WARN/WARN_ON to get the message out, including the\n * file/line information and a backtrace.\n */\n#define netdev_WARN(dev, format, args...)\t\t\t\\\n\tWARN(1, \"netdevice: %s\\n\" format, netdev_name(dev), ##args);\n\n/* netif printk helpers, similar to netdev_printk */\n\n#define netif_printk(priv, type, level, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t  \t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_printk(level, (dev), fmt, ##args);\t\\\n} while (0)\n\n#define netif_level(level, priv, type, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_##level(dev, fmt, ##args);\t\t\\\n} while (0)\n\n#define netif_emerg(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(emerg, priv, type, dev, fmt, ##args)\n#define netif_alert(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(alert, priv, type, dev, fmt, ##args)\n#define netif_crit(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(crit, priv, type, dev, fmt, ##args)\n#define netif_err(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(err, priv, type, dev, fmt, ##args)\n#define netif_warn(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(warn, priv, type, dev, fmt, ##args)\n#define netif_notice(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(notice, priv, type, dev, fmt, ##args)\n#define netif_info(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(info, priv, type, dev, fmt, ##args)\n\n#if defined(DEBUG)\n#define netif_dbg(priv, type, dev, format, args...)\t\t\\\n\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args)\n#elif defined(CONFIG_DYNAMIC_DEBUG)\n#define netif_dbg(priv, type, netdev, format, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tdynamic_dev_dbg((netdev)->dev.parent,\t\t\\\n\t\t\t\t\"%s: \" format,\t\t\t\\\n\t\t\t\tnetdev_name(netdev), ##args);\t\\\n} while (0)\n#else\n#define netif_dbg(priv, type, dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\t\\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netif_vdbg\tnetif_dbg\n#else\n#define netif_vdbg(priv, type, dev, format, args...)\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n#endif /* __KERNEL__ */\n\n#endif\t/* _LINUX_NETDEVICE_H */\n", "/*\n * \tNET3\tProtocol independent device support routines.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n * \t\tAuthors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *              \t\t\tto 2 if register_netdev gets called\n *              \t\t\tbefore net_dev_init & also removed a\n *              \t\t\tfew lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *\t\tAlan Cox\t: \tFix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *\t\tAlan Cox\t: \tFixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *              \t\t\tindefinitely on dev->refcnt\n * \t\tJ Hadi Salim\t:\t- Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <asm/uaccess.h>\n#include <asm/system.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/notifier.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <linux/rtnetlink.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/stat.h>\n#include <net/dst.h>\n#include <net/pkt_sched.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/kmod.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/wext.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <linux/pci.h>\n#include <linux/inetdevice.h>\n\n#include \"net-sysfs.h\"\n\n/* Instead of increasing this, you should create a hash table. */\n#define MAX_GRO_SKBS 8\n\n/* This should be increased if a protocol with a bigger head is added. */\n#define GRO_MAX_HEAD (MAX_HEADER + 128)\n\n/*\n *\tThe list of packet types we will receive (as opposed to discard)\n *\tand the routines to invoke.\n *\n *\tWhy 16. Because with 16 the only overlap we get on a hash of the\n *\tlow nibble of the protocol value is RARP/SNAP/X.25.\n *\n *      NOTE:  That is no longer true with the addition of VLAN tags.  Not\n *             sure which should go first, but I bet it won't make much\n *             difference if we are running VLANs.  The good news is that\n *             this protocol won't be in the list unless compiled in, so\n *             the average user (w/out VLANs) will not be adversely affected.\n *             --BLG\n *\n *\t\t0800\tIP\n *\t\t8100    802.1Q VLAN\n *\t\t0001\t802.3\n *\t\t0002\tAX.25\n *\t\t0004\t802.2\n *\t\t8035\tRARP\n *\t\t0005\tSNAP\n *\t\t0805\tX.25\n *\t\t0806\tARP\n *\t\t8137\tIPX\n *\t\t0009\tLocaltalk\n *\t\t86DD\tIPv6\n */\n\n#define PTYPE_HASH_SIZE\t(16)\n#define PTYPE_HASH_MASK\t(PTYPE_HASH_SIZE - 1)\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstatic struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstatic struct list_head ptype_all __read_mostly;\t/* Taps */\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned hash = full_name_hash(name, strnlen(name, IFNAMSIZ));\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_lock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic inline void rps_unlock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_unlock(&sd->input_pkt_queue.lock);\n#endif\n}\n\n/* Device list insertion */\nstatic int list_netdevice(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock_bh(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock_bh(&dev_base_lock);\n\treturn 0;\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\t/* Unlink dev from the device chain */\n\twrite_lock_bh(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\thlist_del_rcu(&dev->name_hlist);\n\thlist_del_rcu(&dev->index_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] =\n\t{ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE802_TR, ARPHRD_IEEE80211,\n\t ARPHRD_IEEE80211_PRISM, ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET,\n\t ARPHRD_PHONET_PIPE, ARPHRD_IEEE802154,\n\t ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] =\n\t{\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t \"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t \"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t \"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t \"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t \"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t \"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t \"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t \"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t \"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t \"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t \"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t \"_xmit_FCFABRIC\", \"_xmit_IEEE802_TR\", \"_xmit_IEEE80211\",\n\t \"_xmit_IEEE80211_PRISM\", \"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\",\n\t \"_xmit_PHONET_PIPE\", \"_xmit_IEEE802154\",\n\t \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n\n\t\tProtocol management and registration routines\n\n*******************************************************************************/\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn &ptype_all;\n\telse\n\t\treturn &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tprintk(KERN_WARNING \"dev_remove_pack: %p not found.\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n/******************************************************************************\n\n\t\t      Device Boot-time Settings Routines\n\n*******************************************************************************/\n\n/* Boot time configuration table */\nstatic struct netdev_boot_setup dev_boot_setup[NETDEV_BOOT_SETUP_MAX];\n\n/**\n *\tnetdev_boot_setup_add\t- add new setup entry\n *\t@name: name of the device\n *\t@map: configured settings for the device\n *\n *\tAdds new setup entry to the dev_boot_setup list.  The function\n *\treturns 0 on error and 1 on success.  This is a generic routine to\n *\tall netdevices.\n */\nstatic int netdev_boot_setup_add(char *name, struct ifmap *map)\n{\n\tstruct netdev_boot_setup *s;\n\tint i;\n\n\ts = dev_boot_setup;\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] == '\\0' || s[i].name[0] == ' ') {\n\t\t\tmemset(s[i].name, 0, sizeof(s[i].name));\n\t\t\tstrlcpy(s[i].name, name, IFNAMSIZ);\n\t\t\tmemcpy(&s[i].map, map, sizeof(s[i].map));\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i >= NETDEV_BOOT_SETUP_MAX ? 0 : 1;\n}\n\n/**\n *\tnetdev_boot_setup_check\t- check boot time settings\n *\t@dev: the netdevice\n *\n * \tCheck boot time settings for the device.\n *\tThe found settings are set for the device to be used\n *\tlater in the device probing.\n *\tReturns 0 if no settings found, 1 if they are.\n */\nint netdev_boot_setup_check(struct net_device *dev)\n{\n\tstruct netdev_boot_setup *s = dev_boot_setup;\n\tint i;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] != '\\0' && s[i].name[0] != ' ' &&\n\t\t    !strcmp(dev->name, s[i].name)) {\n\t\t\tdev->irq \t= s[i].map.irq;\n\t\t\tdev->base_addr \t= s[i].map.base_addr;\n\t\t\tdev->mem_start \t= s[i].map.mem_start;\n\t\t\tdev->mem_end \t= s[i].map.mem_end;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_boot_setup_check);\n\n\n/**\n *\tnetdev_boot_base\t- get address from boot time settings\n *\t@prefix: prefix for network device\n *\t@unit: id for network device\n *\n * \tCheck boot time settings for the base address of device.\n *\tThe found settings are set for the device to be used\n *\tlater in the device probing.\n *\tReturns 0 if no settings found.\n */\nunsigned long netdev_boot_base(const char *prefix, int unit)\n{\n\tconst struct netdev_boot_setup *s = dev_boot_setup;\n\tchar name[IFNAMSIZ];\n\tint i;\n\n\tsprintf(name, \"%s%d\", prefix, unit);\n\n\t/*\n\t * If device already registered then return base of 1\n\t * to indicate not to probe for this interface\n\t */\n\tif (__dev_get_by_name(&init_net, name))\n\t\treturn 1;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++)\n\t\tif (!strcmp(name, s[i].name))\n\t\t\treturn s[i].map.base_addr;\n\treturn 0;\n}\n\n/*\n * Saves at boot time configured settings for any netdevice.\n */\nint __init netdev_boot_setup(char *str)\n{\n\tint ints[5];\n\tstruct ifmap map;\n\n\tstr = get_options(str, ARRAY_SIZE(ints), ints);\n\tif (!str || !*str)\n\t\treturn 0;\n\n\t/* Save settings */\n\tmemset(&map, 0, sizeof(map));\n\tif (ints[0] > 0)\n\t\tmap.irq = ints[1];\n\tif (ints[0] > 1)\n\t\tmap.base_addr = ints[2];\n\tif (ints[0] > 2)\n\t\tmap.mem_start = ints[3];\n\tif (ints[0] > 3)\n\t\tmap.mem_end = ints[4];\n\n\t/* Add new entry to the list */\n\treturn netdev_boot_setup_add(str, &map);\n}\n\n__setup(\"netdev=\", netdev_boot_setup);\n\n/*******************************************************************************\n\n\t\t\t    Device Interface Subroutines\n\n*******************************************************************************/\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct hlist_node *p;\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry(dev, p, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n *\tdev_get_by_name_rcu\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name.\n *\tIf the name is found a pointer to the device is returned.\n * \tIf the name is not found then %NULL is returned.\n *\tThe reference counters are not incremented so the caller must be\n *\tcareful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct hlist_node *p;\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry_rcu(dev, p, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct hlist_node *p;\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, p, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct hlist_node *p;\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, p, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tfor_each_netdev(net, dev)\n\t\tif (dev->type == type)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_getfirstbyhwtype);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\tdev_get_by_flags_rcu - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trcu_read_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *dev_get_by_flags_rcu(struct net *net, unsigned short if_flags,\n\t\t\t\t    unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tret = NULL;\n\tfor_each_netdev_rcu(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_get_by_flags_rcu);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tto allow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nint dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn 0;\n\tif (strlen(name) >= IFNAMSIZ)\n\t\treturn 0;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn 0;\n\n\twhile (*name) {\n\t\tif (*name == '/' || isspace(*name))\n\t\t\treturn 0;\n\t\tname++;\n\t}\n\treturn 1;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tp = strnchr(name, IFNAMSIZ-1, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\tset_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tif (buf != name)\n\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!__dev_get_by_name(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tstruct net *net;\n\tint ret;\n\n\tBUG_ON(!dev_net(dev));\n\tnet = dev_net(dev);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_get_valid_name(struct net_device *dev, const char *name, bool fmt)\n{\n\tstruct net *net;\n\n\tBUG_ON(!dev_net(dev));\n\tnet = dev_net(dev);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (fmt && strchr(name, '%'))\n\t\treturn dev_alloc_name(dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\tif (dev->flags & IFF_UP)\n\t\treturn -EBUSY;\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0)\n\t\treturn 0;\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(dev, newname, 1);\n\tif (err < 0)\n\t\treturn err;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\treturn ret;\n\t}\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_del(&dev->name_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tprintk(KERN_ERR\n\t\t\t       \"%s: name change rollback failed: %d.\\n\",\n\t\t\t       dev->name, ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tASSERT_RTNL();\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (!len) {\n\t\tif (dev->ifalias) {\n\t\t\tkfree(dev->ifalias);\n\t\t\tdev->ifalias = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tdev->ifalias = krealloc(dev->ifalias, len + 1, GFP_KERNEL);\n\tif (!dev->ifalias)\n\t\treturn -ENOMEM;\n\n\tstrlcpy(dev->ifalias, alias, len+1);\n\treturn len;\n}\n\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tcall_netdevice_notifiers(NETDEV_CHANGE, dev);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\nint netdev_bonding_change(struct net_device *dev, unsigned long event)\n{\n\treturn call_netdevice_notifiers(event, dev);\n}\nEXPORT_SYMBOL(netdev_bonding_change);\n\n/**\n *\tdev_load \t- load a network module\n *\t@net: the applicable net namespace\n *\t@name: name of interface\n *\n *\tIf a network interface is not present and the process has suitable\n *\tprivileges this function loads the module. If module loading is not\n *\tavailable in this kernel then it becomes a nop.\n */\n\nvoid dev_load(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\trcu_read_unlock();\n\n\tif (!dev && capable(CAP_NET_ADMIN))\n\t\trequest_module(\"%s\", name);\n}\nEXPORT_SYMBOL(dev_load);\n\nstatic int __dev_open(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tIs it even present?\n\t */\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\tret = call_netdevice_notifiers(NETDEV_PRE_UP, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t *\tCall device private open method\n\t */\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\t/*\n\t *\tIf it went open OK then:\n\t */\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\t/*\n\t\t *\tSet the flags.\n\t\t */\n\t\tdev->flags |= IFF_UP;\n\n\t\t/*\n\t\t *\tEnable NET_DMA\n\t\t */\n\t\tnet_dmaengine_get();\n\n\t\t/*\n\t\t *\tInitialize multicasting status\n\t\t */\n\t\tdev_set_rx_mode(dev);\n\n\t\t/*\n\t\t *\tWakeup transmit queue engine\n\t\t */\n\t\tdev_activate(dev);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev:\tdevice to open\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev)\n{\n\tint ret;\n\n\t/*\n\t *\tIs it already up?\n\t */\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\t/*\n\t *\tOpen device\n\t */\n\tret = __dev_open(dev);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/*\n\t *\t... and announce new interface.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic int __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/*\n\t\t *\tTell people we are going down, so that they can\n\t\t *\tprepare to death, when device is still operating.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_clear_bit(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\t/*\n\t\t *\tDevice is now down.\n\t\t */\n\n\t\tdev->flags &= ~IFF_UP;\n\n\t\t/*\n\t\t *\tShutdown NET_DMA\n\t\t */\n\t\tnet_dmaengine_put();\n\t}\n\n\treturn 0;\n}\n\nstatic int __dev_close(struct net_device *dev)\n{\n\tint retval;\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\tretval = __dev_close_many(&single);\n\tlist_del(&single);\n\treturn retval;\n}\n\nint dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(tmp_list);\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_move(&dev->unreg_list, &tmp_list);\n\n\t__dev_close_many(head);\n\n\t/*\n\t * Tell people we are down\n\t */\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\t/* rollback_registered_many needs the complete original list */\n\tlist_splice(&tmp_list, head);\n\treturn 0;\n}\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nint dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\tdev_close_many(&single);\n\tlist_del(&single);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tif (dev->ethtool_ops && dev->ethtool_ops->get_flags &&\n\t    dev->ethtool_ops->set_flags) {\n\t\tu32 flags = dev->ethtool_ops->get_flags(dev);\n\t\tif (flags & ETH_FLAG_LRO) {\n\t\t\tflags &= ~ETH_FLAG_LRO;\n\t\t\tdev->ethtool_ops->set_flags(dev, flags);\n\t\t}\n\t}\n\tWARN_ON(dev->features & NETIF_F_LRO);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\n\nstatic int dev_boot_phase = 1;\n\n/*\n *\tDevice change register/unregister. These are not inline or static\n *\tas we export them to the world.\n */\n\n/**\n *\tregister_netdevice_notifier - register a network notifier block\n *\t@nb: notifier\n *\n *\tRegister a notifier to be called when network device events occur.\n *\tThe notifier passed is linked into the kernel structures and must\n *\tnot be reused until it has been unregistered. A negative errno code\n *\tis returned on a failure.\n *\n * \tWhen registered all registration and up events are replayed\n *\tto the new notifier to allow device to have a race free\n *\tview of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net_device *last;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\terr = nb->notifier_call(nb, NETDEV_REGISTER, dev);\n\t\t\terr = notifier_to_errno(err);\n\t\t\tif (err)\n\t\t\t\tgoto rollback;\n\n\t\t\tif (!(dev->flags & IFF_UP))\n\t\t\t\tcontinue;\n\n\t\t\tnb->notifier_call(nb, NETDEV_UP, dev);\n\t\t}\n\t}\n\nunlock:\n\trtnl_unlock();\n\treturn err;\n\nrollback:\n\tlast = dev;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev == last)\n\t\t\t\tbreak;\n\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tnb->notifier_call(nb, NETDEV_GOING_DOWN, dev);\n\t\t\t\tnb->notifier_call(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tnb->notifier_call(nb, NETDEV_UNREGISTER, dev);\n\t\t\tnb->notifier_call(nb, NETDEV_UNREGISTER_BATCH, dev);\n\t\t}\n\t}\n\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n *\tunregister_netdevice_notifier - unregister a network notifier block\n *\t@nb: notifier\n *\n *\tUnregister a notifier previously registered by\n *\tregister_netdevice_notifier(). The notifier is unlinked into the\n *\tkernel structures and may then be reused. A negative errno code\n *\tis returned on a failure.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn raw_notifier_call_chain(&netdev_chain, val, dev);\n}\n\n/* When > 0 there are consumers of rx skb time stamps */\nstatic atomic_t netstamp_needed = ATOMIC_INIT(0);\n\nvoid net_enable_timestamp(void)\n{\n\tatomic_inc(&netstamp_needed);\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n\tatomic_dec(&netstamp_needed);\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tif (atomic_read(&netstamp_needed))\n\t\t__net_timestamp(skb);\n\telse\n\t\tskb->tstamp.tv64 = 0;\n}\n\nstatic inline void net_timestamp_check(struct sk_buff *skb)\n{\n\tif (!skb->tstamp.tv64 && atomic_read(&netstamp_needed))\n\t\t__net_timestamp(skb);\n}\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tskb_orphan(skb);\n\tnf_reset(skb);\n\n\tif (unlikely(!(dev->flags & IFF_UP) ||\n\t\t     (skb->len > (dev->mtu + dev->hard_header_len + VLAN_HLEN)))) {\n\t\tatomic_long_inc(&dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\tskb_set_dev(skb, dev);\n\tskb->tstamp.tv64 = 0;\n\tskb->pkt_type = PACKET_HOST;\n\tskb->protocol = eth_type_trans(skb, dev);\n\treturn netif_rx(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tatomic_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nstatic void dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif ((ptype->dev == dev || !ptype->dev) &&\n\t\t    (ptype->af_packet_priv == NULL ||\n\t\t     (struct sock *)ptype->af_packet_priv != skb->sk)) {\n\t\t\tif (pt_prev) {\n\t\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\t\tpt_prev = ptype;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\t\tif (!skb2)\n\t\t\t\tbreak;\n\n\t\t\tnet_timestamp_set(skb2);\n\n\t\t\t/* skb->nh should be correctly\n\t\t\t   set by sender, so that the second statement is\n\t\t\t   just protection against buggy protocols.\n\t\t\t */\n\t\t\tskb_reset_mac_header(skb2);\n\n\t\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t\t    skb2->network_header > skb2->tail) {\n\t\t\t\tif (net_ratelimit())\n\t\t\t\t\tprintk(KERN_CRIT \"protocol %04x is \"\n\t\t\t\t\t       \"buggy, dev %s\\n\",\n\t\t\t\t\t       ntohs(skb2->protocol),\n\t\t\t\t\t       dev->name);\n\t\t\t\tskb_reset_network_header(skb2);\n\t\t\t}\n\n\t\t\tskb2->transport_header = skb2->network_header;\n\t\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\t\tpt_prev = ptype;\n\t\t}\n\t}\n\tif (pt_prev)\n\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\trcu_read_unlock();\n}\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater then real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tint rc;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (txq < dev->real_num_tx_queues)\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n\t}\n\n\tdev->real_num_tx_queues = txq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_RPS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\nstatic inline void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = &__get_cpu_var(softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nvoid dev_kfree_skb_irq(struct sk_buff *skb)\n{\n\tif (atomic_dec_and_test(&skb->users)) {\n\t\tstruct softnet_data *sd;\n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\tsd = &__get_cpu_var(softnet_data);\n\t\tskb->next = sd->completion_queue;\n\t\tsd->completion_queue = skb;\n\t\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\t\tlocal_irq_restore(flags);\n\t}\n}\nEXPORT_SYMBOL(dev_kfree_skb_irq);\n\nvoid dev_kfree_skb_any(struct sk_buff *skb)\n{\n\tif (in_irq() || irqs_disabled())\n\t\tdev_kfree_skb_irq(skb);\n\telse\n\t\tdev_kfree_skb(skb);\n}\nEXPORT_SYMBOL(dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/**\n * skb_dev_set -- assign a new device to a buffer\n * @skb: buffer for the new device\n * @dev: network device\n *\n * If an skb is owned by a device already, we have to reset\n * all data private to the namespace a device belongs to\n * before assigning it a new device.\n */\n#ifdef CONFIG_NET_NS\nvoid skb_set_dev(struct sk_buff *skb, struct net_device *dev)\n{\n\tskb_dst_drop(skb);\n\tif (skb->dev && !net_eq(dev_net(skb->dev), dev_net(dev))) {\n\t\tsecpath_reset(skb);\n\t\tnf_reset(skb);\n\t\tskb_init_secmark(skb);\n\t\tskb->mark = 0;\n\t\tskb->priority = 0;\n\t\tskb->nf_trace = 0;\n\t\tskb->ipvs_property = 0;\n#ifdef CONFIG_NET_SCHED\n\t\tskb->tc_index = 0;\n#endif\n\t}\n\tskb->dev = dev;\n}\nEXPORT_SYMBOL(skb_set_dev);\n#endif /* CONFIG_NET_NS */\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_shinfo(skb)->gso_size)) {\n\t\t/* Let GSO fix up the checksum. */\n\t\tgoto out_set_summed;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tBUG_ON(offset >= skb_headlen(skb));\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tBUG_ON(offset + sizeof(__sum16) > skb_headlen(skb));\n\n\tif (skb_cloned(skb) &&\n\t    !skb_clone_writable(skb, offset + sizeof(__sum16))) {\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\n/**\n *\tskb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n */\nstruct sk_buff *skb_gso_segment(struct sk_buff *skb, int features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EPROTONOSUPPORT);\n\tstruct packet_type *ptype;\n\t__be16 type = skb->protocol;\n\tint vlan_depth = ETH_HLEN;\n\tint err;\n\n\twhile (type == htons(ETH_P_8021Q)) {\n\t\tstruct vlan_hdr *vh;\n\n\t\tif (unlikely(!pskb_may_pull(skb, vlan_depth + VLAN_HLEN)))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tvh = (struct vlan_hdr *)(skb->data + vlan_depth);\n\t\ttype = vh->h_vlan_encapsulated_proto;\n\t\tvlan_depth += VLAN_HLEN;\n\t}\n\n\tskb_reset_mac_header(skb);\n\tskb->mac_len = skb->network_header - skb->mac_header;\n\t__skb_pull(skb, skb->mac_len);\n\n\tif (unlikely(skb->ip_summed != CHECKSUM_PARTIAL)) {\n\t\tstruct net_device *dev = skb->dev;\n\t\tstruct ethtool_drvinfo info = {};\n\n\t\tif (dev && dev->ethtool_ops && dev->ethtool_ops->get_drvinfo)\n\t\t\tdev->ethtool_ops->get_drvinfo(dev, &info);\n\n\t\tWARN(1, \"%s: caps=(0x%lx, 0x%lx) len=%d data_len=%d ip_summed=%d\\n\",\n\t\t     info.driver, dev ? dev->features : 0L,\n\t\t     skb->sk ? skb->sk->sk_route_caps : 0L,\n\t\t     skb->len, skb->data_len, skb->ip_summed);\n\n\t\tif (skb_header_cloned(skb) &&\n\t\t    (err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC)))\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype,\n\t\t\t&ptype_base[ntohs(type) & PTYPE_HASH_MASK], list) {\n\t\tif (ptype->type == type && !ptype->dev && ptype->gso_segment) {\n\t\t\tif (unlikely(skb->ip_summed != CHECKSUM_PARTIAL)) {\n\t\t\t\terr = ptype->gso_send_check(skb);\n\t\t\t\tsegs = ERR_PTR(err);\n\t\t\t\tif (err || skb_gso_ok(skb, features))\n\t\t\t\t\tbreak;\n\t\t\t\t__skb_push(skb, (skb->data -\n\t\t\t\t\t\t skb_network_header(skb)));\n\t\t\t}\n\t\t\tsegs = ptype->gso_segment(skb, features);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\n\treturn segs;\n}\nEXPORT_SYMBOL(skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev)\n{\n\tif (net_ratelimit()) {\n\t\tprintk(KERN_ERR \"%s: hw csum failure.\\n\",\n\t\t\tdev ? dev->name : \"<unknown>\");\n\t\tdump_stack();\n\t}\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* Actually, we should eliminate this check as soon as we know, that:\n * 1. IOMMU is present and allows to map all the memory.\n * 2. No high memory really exists on this machine.\n */\n\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tif (PageHighMem(skb_shinfo(skb)->frags[i].page))\n\t\t\t\treturn 1;\n\t}\n\n\tif (PCI_DMA_BUS_IS_PHYS) {\n\t\tstruct device *pdev = dev->dev.parent;\n\n\t\tif (!pdev)\n\t\t\treturn 0;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tdma_addr_t addr = page_to_phys(skb_shinfo(skb)->frags[i].page);\n\t\t\tif (!pdev->dma_mask || addr + PAGE_SIZE - 1 > *pdev->dma_mask)\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\nstruct dev_gso_cb {\n\tvoid (*destructor)(struct sk_buff *skb);\n};\n\n#define DEV_GSO_CB(skb) ((struct dev_gso_cb *)(skb)->cb)\n\nstatic void dev_gso_skb_destructor(struct sk_buff *skb)\n{\n\tstruct dev_gso_cb *cb;\n\n\tdo {\n\t\tstruct sk_buff *nskb = skb->next;\n\n\t\tskb->next = nskb->next;\n\t\tnskb->next = NULL;\n\t\tkfree_skb(nskb);\n\t} while (skb->next);\n\n\tcb = DEV_GSO_CB(skb);\n\tif (cb->destructor)\n\t\tcb->destructor(skb);\n}\n\n/**\n *\tdev_gso_segment - Perform emulated hardware segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: device features as applicable to this skb\n *\n *\tThis function segments the given skb and stores the list of segments\n *\tin skb->next.\n */\nstatic int dev_gso_segment(struct sk_buff *skb, int features)\n{\n\tstruct sk_buff *segs;\n\n\tsegs = skb_gso_segment(skb, features);\n\n\t/* Verifying header integrity only. */\n\tif (!segs)\n\t\treturn 0;\n\n\tif (IS_ERR(segs))\n\t\treturn PTR_ERR(segs);\n\n\tskb->next = segs;\n\tDEV_GSO_CB(skb)->destructor = skb->destructor;\n\tskb->destructor = dev_gso_skb_destructor;\n\n\treturn 0;\n}\n\n/*\n * Try to orphan skb early, right before transmission by the device.\n * We cannot orphan skb if tx timestamp is requested or the sk-reference\n * is needed on driver level for other reasons, e.g. see net/can/raw.c\n */\nstatic inline void skb_orphan_try(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (sk && !skb_shinfo(skb)->tx_flags) {\n\t\t/* skb_tx_hash() wont be able to get sk.\n\t\t * We copy sk_hash into skb->rxhash\n\t\t */\n\t\tif (!skb->rxhash)\n\t\t\tskb->rxhash = sk->sk_hash;\n\t\tskb_orphan(skb);\n\t}\n}\n\nstatic bool can_checksum_protocol(unsigned long features, __be16 protocol)\n{\n\treturn ((features & NETIF_F_GEN_CSUM) ||\n\t\t((features & NETIF_F_V4_CSUM) &&\n\t\t protocol == htons(ETH_P_IP)) ||\n\t\t((features & NETIF_F_V6_CSUM) &&\n\t\t protocol == htons(ETH_P_IPV6)) ||\n\t\t((features & NETIF_F_FCOE_CRC) &&\n\t\t protocol == htons(ETH_P_FCOE)));\n}\n\nstatic int harmonize_features(struct sk_buff *skb, __be16 protocol, int features)\n{\n\tif (!can_checksum_protocol(features, protocol)) {\n\t\tfeatures &= ~NETIF_F_ALL_CSUM;\n\t\tfeatures &= ~NETIF_F_SG;\n\t} else if (illegal_highdma(skb->dev, skb)) {\n\t\tfeatures &= ~NETIF_F_SG;\n\t}\n\n\treturn features;\n}\n\nint netif_skb_features(struct sk_buff *skb)\n{\n\t__be16 protocol = skb->protocol;\n\tint features = skb->dev->features;\n\n\tif (protocol == htons(ETH_P_8021Q)) {\n\t\tstruct vlan_ethhdr *veh = (struct vlan_ethhdr *)skb->data;\n\t\tprotocol = veh->h_vlan_encapsulated_proto;\n\t} else if (!vlan_tx_tag_present(skb)) {\n\t\treturn harmonize_features(skb, protocol, features);\n\t}\n\n\tfeatures &= (skb->dev->vlan_features | NETIF_F_HW_VLAN_TX);\n\n\tif (protocol != htons(ETH_P_8021Q)) {\n\t\treturn harmonize_features(skb, protocol, features);\n\t} else {\n\t\tfeatures &= NETIF_F_SG | NETIF_F_HIGHDMA | NETIF_F_FRAGLIST |\n\t\t\t\tNETIF_F_GEN_CSUM | NETIF_F_HW_VLAN_TX;\n\t\treturn harmonize_features(skb, protocol, features);\n\t}\n}\nEXPORT_SYMBOL(netif_skb_features);\n\n/*\n * Returns true if either:\n *\t1. skb has frag_list and the device doesn't support FRAGLIST, or\n *\t2. skb is fragmented and the device does not support SG, or if\n *\t   at least one of fragments is in highmem and device does not\n *\t   support DMA from it.\n */\nstatic inline int skb_needs_linearize(struct sk_buff *skb,\n\t\t\t\t      int features)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t\t\t((skb_has_frag_list(skb) &&\n\t\t\t\t!(features & NETIF_F_FRAGLIST)) ||\n\t\t\t(skb_shinfo(skb)->nr_frags &&\n\t\t\t\t!(features & NETIF_F_SG)));\n}\n\nint dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\tstruct netdev_queue *txq)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint rc = NETDEV_TX_OK;\n\n\tif (likely(!skb->next)) {\n\t\tint features;\n\n\t\t/*\n\t\t * If device doesnt need skb->dst, release it right now while\n\t\t * its hot in this cpu cache\n\t\t */\n\t\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\t\tskb_dst_drop(skb);\n\n\t\tif (!list_empty(&ptype_all))\n\t\t\tdev_queue_xmit_nit(skb, dev);\n\n\t\tskb_orphan_try(skb);\n\n\t\tfeatures = netif_skb_features(skb);\n\n\t\tif (vlan_tx_tag_present(skb) &&\n\t\t    !(features & NETIF_F_HW_VLAN_TX)) {\n\t\t\tskb = __vlan_put_tag(skb, vlan_tx_tag_get(skb));\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\n\t\t\tskb->vlan_tci = 0;\n\t\t}\n\n\t\tif (netif_needs_gso(skb, features)) {\n\t\t\tif (unlikely(dev_gso_segment(skb, features)))\n\t\t\t\tgoto out_kfree_skb;\n\t\t\tif (skb->next)\n\t\t\t\tgoto gso;\n\t\t} else {\n\t\t\tif (skb_needs_linearize(skb, features) &&\n\t\t\t    __skb_linearize(skb))\n\t\t\t\tgoto out_kfree_skb;\n\n\t\t\t/* If packet is not checksummed and device does not\n\t\t\t * support checksumming for this protocol, complete\n\t\t\t * checksumming here.\n\t\t\t */\n\t\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\tskb_checksum_start_offset(skb));\n\t\t\t\tif (!(features & NETIF_F_ALL_CSUM) &&\n\t\t\t\t     skb_checksum_help(skb))\n\t\t\t\t\tgoto out_kfree_skb;\n\t\t\t}\n\t\t}\n\n\t\trc = ops->ndo_start_xmit(skb, dev);\n\t\ttrace_net_dev_xmit(skb, rc);\n\t\tif (rc == NETDEV_TX_OK)\n\t\t\ttxq_trans_update(txq);\n\t\treturn rc;\n\t}\n\ngso:\n\tdo {\n\t\tstruct sk_buff *nskb = skb->next;\n\n\t\tskb->next = nskb->next;\n\t\tnskb->next = NULL;\n\n\t\t/*\n\t\t * If device doesnt need nskb->dst, release it right now while\n\t\t * its hot in this cpu cache\n\t\t */\n\t\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\t\tskb_dst_drop(nskb);\n\n\t\trc = ops->ndo_start_xmit(nskb, dev);\n\t\ttrace_net_dev_xmit(nskb, rc);\n\t\tif (unlikely(rc != NETDEV_TX_OK)) {\n\t\t\tif (rc & ~NETDEV_TX_MASK)\n\t\t\t\tgoto out_kfree_gso_skb;\n\t\t\tnskb->next = skb->next;\n\t\t\tskb->next = nskb;\n\t\t\treturn rc;\n\t\t}\n\t\ttxq_trans_update(txq);\n\t\tif (unlikely(netif_tx_queue_stopped(txq) && skb->next))\n\t\t\treturn NETDEV_TX_BUSY;\n\t} while (skb->next);\n\nout_kfree_gso_skb:\n\tif (likely(skb->next == NULL))\n\t\tskb->destructor = DEV_GSO_CB(skb)->destructor;\nout_kfree_skb:\n\tkfree_skb(skb);\nout:\n\treturn rc;\n}\n\nstatic u32 hashrnd __read_mostly;\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nu16 __skb_tx_hash(const struct net_device *dev, const struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues)\n{\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\thash = skb_get_rx_queue(skb);\n\t\twhile (unlikely(hash >= num_tx_queues))\n\t\t\thash -= num_tx_queues;\n\t\treturn hash;\n\t}\n\n\tif (skb->sk && skb->sk->sk_hash)\n\t\thash = skb->sk->sk_hash;\n\telse\n\t\thash = (__force u16) skb->protocol ^ skb->rxhash;\n\thash = jhash_1word(hash, hashrnd);\n\n\treturn (u16) (((u64) hash * num_tx_queues) >> 32);\n}\nEXPORT_SYMBOL(__skb_tx_hash);\n\nstatic inline u16 dev_cap_txqueue(struct net_device *dev, u16 queue_index)\n{\n\tif (unlikely(queue_index >= dev->real_num_tx_queues)) {\n\t\tif (net_ratelimit()) {\n\t\t\tpr_warning(\"%s selects TX queue %d, but \"\n\t\t\t\t\"real number of TX queues is %d\\n\",\n\t\t\t\tdev->name, queue_index, dev->real_num_tx_queues);\n\t\t}\n\t\treturn 0;\n\t}\n\treturn queue_index;\n}\n\nstatic inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\trcu_read_lock();\n\tdev_maps = rcu_dereference(dev->xps_maps);\n\tif (dev_maps) {\n\t\tmap = rcu_dereference(\n\t\t    dev_maps->cpu_map[raw_smp_processor_id()]);\n\t\tif (map) {\n\t\t\tif (map->len == 1)\n\t\t\t\tqueue_index = map->queues[0];\n\t\t\telse {\n\t\t\t\tu32 hash;\n\t\t\t\tif (skb->sk && skb->sk->sk_hash)\n\t\t\t\t\thash = skb->sk->sk_hash;\n\t\t\t\telse\n\t\t\t\t\thash = (__force u16) skb->protocol ^\n\t\t\t\t\t    skb->rxhash;\n\t\t\t\thash = jhash_1word(hash, hashrnd);\n\t\t\t\tqueue_index = map->queues[\n\t\t\t\t    ((u64)hash * map->len) >> 32];\n\t\t\t}\n\t\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\t\tqueue_index = -1;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nstatic struct netdev_queue *dev_pick_tx(struct net_device *dev,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tint queue_index;\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (dev->real_num_tx_queues == 1)\n\t\tqueue_index = 0;\n\telse if (ops->ndo_select_queue) {\n\t\tqueue_index = ops->ndo_select_queue(dev, skb);\n\t\tqueue_index = dev_cap_txqueue(dev, queue_index);\n\t} else {\n\t\tstruct sock *sk = skb->sk;\n\t\tqueue_index = sk_tx_queue_get(sk);\n\n\t\tif (queue_index < 0 || skb->ooo_okay ||\n\t\t    queue_index >= dev->real_num_tx_queues) {\n\t\t\tint old_index = queue_index;\n\n\t\t\tqueue_index = get_xps_queue(dev, skb);\n\t\t\tif (queue_index < 0)\n\t\t\t\tqueue_index = skb_tx_hash(dev, skb);\n\n\t\t\tif (queue_index != old_index && sk) {\n\t\t\t\tstruct dst_entry *dst =\n\t\t\t\t    rcu_dereference_check(sk->sk_dst_cache, 1);\n\n\t\t\t\tif (dst && skb_dst(skb) == dst)\n\t\t\t\t\tsk_tx_queue_set(sk, queue_index);\n\t\t\t}\n\t\t}\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tbool contended = qdisc_is_running(q);\n\tint rc;\n\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits __QDISC_STATE_RUNNING owner to get the lock more often\n\t * and dequeue packets faster.\n\t */\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\tkfree_skb(skb);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\t\tif (!(dev->priv_flags & IFF_XMIT_DST_RELEASE))\n\t\t\tskb_dst_force(skb);\n\n\t\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t} else\n\t\t\tqdisc_run_end(q);\n\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\tskb_dst_force(skb);\n\t\trc = qdisc_enqueue_root(skb, q);\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\nstatic DEFINE_PER_CPU(int, xmit_recursion);\n#define RECURSION_LIMIT 10\n\n/**\n *\tdev_queue_xmit - transmit a buffer\n *\t@skb: buffer to transmit\n *\n *\tQueue a buffer for transmission to a network device. The caller must\n *\thave set the device and priority and built the buffer before calling\n *\tthis function. The function can be called from an interrupt.\n *\n *\tA negative errno code is returned on a failure. A success does not\n *\tguarantee the frame will be transmitted as it may be dropped due\n *\tto congestion or traffic shaping.\n *\n * -----------------------------------------------------------------------------------\n *      I notice this method can also return errors from the queue disciplines,\n *      including NET_XMIT_DROP, which is a positive value.  So, errors can also\n *      be positive.\n *\n *      Regardless of the return value, the skb is consumed, so it is currently\n *      difficult to retry a send to this method.  (You can bump the ref count\n *      before sending to hold a reference for retry if you are careful.)\n *\n *      When calling this method, interrupts MUST be enabled.  This is because\n *      the BH enable code must have IRQs enabled so that it will not deadlock.\n *          --BLG\n */\nint dev_queue_xmit(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\ttxq = dev_pick_tx(dev, skb);\n\tq = rcu_dereference_bh(txq->qdisc);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_verd = SET_TC_AT(skb->tc_verd, AT_EGRESS);\n#endif\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t   loopback, all the sorts of tunnels...\n\n\t   Really, it is unlikely that netif_tx_lock protection is necessary\n\t   here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t   counters.)\n\t   However, it is possible, that they rely on protection\n\t   made by us here.\n\n\t   Check this and shot the lock. It is not prone from deadlocks.\n\t   Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\tif (txq->xmit_lock_owner != cpu) {\n\n\t\t\tif (__this_cpu_read(xmit_recursion) > RECURSION_LIMIT)\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_tx_queue_stopped(txq)) {\n\t\t\t\t__this_cpu_inc(xmit_recursion);\n\t\t\t\trc = dev_hard_start_xmit(skb, dev, txq);\n\t\t\t\t__this_cpu_dec(xmit_recursion);\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_CRIT \"Virtual device %s asks to \"\n\t\t\t\t       \"queue packet!\\n\", dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_CRIT \"Dead loop on virtual device \"\n\t\t\t\t       \"%s, fix it urgently!\\n\", dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tkfree_skb(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\nEXPORT_SYMBOL(dev_queue_xmit);\n\n\n/*=======================================================================\n\t\t\tReceiver routines\n  =======================================================================*/\n\nint netdev_max_backlog __read_mostly = 1000;\nint netdev_tstamp_prequeue __read_mostly = 1;\nint netdev_budget __read_mostly = 300;\nint weight_p __read_mostly = 64;            /* old backlog weight */\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n/*\n * __skb_get_rxhash: calculate a flow hash based on src/dst addresses\n * and src/dst port numbers. Returns a non-zero hash number on success\n * and 0 on failure.\n */\n__u32 __skb_get_rxhash(struct sk_buff *skb)\n{\n\tint nhoff, hash = 0, poff;\n\tstruct ipv6hdr *ip6;\n\tstruct iphdr *ip;\n\tu8 ip_proto;\n\tu32 addr1, addr2, ihl;\n\tunion {\n\t\tu32 v32;\n\t\tu16 v16[2];\n\t} ports;\n\n\tnhoff = skb_network_offset(skb);\n\n\tswitch (skb->protocol) {\n\tcase __constant_htons(ETH_P_IP):\n\t\tif (!pskb_may_pull(skb, sizeof(*ip) + nhoff))\n\t\t\tgoto done;\n\n\t\tip = (struct iphdr *) (skb->data + nhoff);\n\t\tif (ip->frag_off & htons(IP_MF | IP_OFFSET))\n\t\t\tip_proto = 0;\n\t\telse\n\t\t\tip_proto = ip->protocol;\n\t\taddr1 = (__force u32) ip->saddr;\n\t\taddr2 = (__force u32) ip->daddr;\n\t\tihl = ip->ihl;\n\t\tbreak;\n\tcase __constant_htons(ETH_P_IPV6):\n\t\tif (!pskb_may_pull(skb, sizeof(*ip6) + nhoff))\n\t\t\tgoto done;\n\n\t\tip6 = (struct ipv6hdr *) (skb->data + nhoff);\n\t\tip_proto = ip6->nexthdr;\n\t\taddr1 = (__force u32) ip6->saddr.s6_addr32[3];\n\t\taddr2 = (__force u32) ip6->daddr.s6_addr32[3];\n\t\tihl = (40 >> 2);\n\t\tbreak;\n\tdefault:\n\t\tgoto done;\n\t}\n\n\tports.v32 = 0;\n\tpoff = proto_ports_offset(ip_proto);\n\tif (poff >= 0) {\n\t\tnhoff += ihl * 4 + poff;\n\t\tif (pskb_may_pull(skb, nhoff + 4)) {\n\t\t\tports.v32 = * (__force u32 *) (skb->data + nhoff);\n\t\t\tif (ports.v16[1] < ports.v16[0])\n\t\t\t\tswap(ports.v16[0], ports.v16[1]);\n\t\t}\n\t}\n\n\t/* get a consistent hash (same value on both flow directions) */\n\tif (addr2 < addr1)\n\t\tswap(addr1, addr2);\n\n\thash = jhash_3words(addr1, addr2, ports.v32, hashrnd);\n\tif (!hash)\n\t\thash = 1;\n\ndone:\n\treturn hash;\n}\nEXPORT_SYMBOL(__skb_get_rxhash);\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tstruct netdev_rx_queue *rxqueue;\n\tstruct rps_map *map;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_sock_flow_table *sock_flow_table;\n\tint cpu = -1;\n\tu16 tcpu;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue = dev->_rx + index;\n\t} else\n\t\trxqueue = dev->_rx;\n\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (map) {\n\t\tif (map->len == 1 &&\n\t\t    !rcu_dereference_raw(rxqueue->rps_flow_table)) {\n\t\t\ttcpu = map->cpus[0];\n\t\t\tif (cpu_online(tcpu))\n\t\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t} else if (!rcu_dereference_raw(rxqueue->rps_flow_table)) {\n\t\tgoto done;\n\t}\n\n\tskb_reset_network_header(skb);\n\tif (!skb_get_rxhash(skb))\n\t\tgoto done;\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tu16 next_cpu;\n\t\tstruct rps_dev_flow *rflow;\n\n\t\trflow = &flow_table->flows[skb->rxhash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\tnext_cpu = sock_flow_table->ents[skb->rxhash &\n\t\t    sock_flow_table->mask];\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (equal to RPS_NO_CPU).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu == RPS_NO_CPU || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = rflow->cpu = next_cpu;\n\t\t\tif (tcpu != RPS_NO_CPU)\n\t\t\t\trflow->last_qtail = per_cpu(softnet_data,\n\t\t\t\t    tcpu).input_queue_head;\n\t\t}\n\t\tif (tcpu != RPS_NO_CPU && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (map) {\n\t\ttcpu = map->cpus[((u64) skb->rxhash * map->len) >> 32];\n\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int rps_ipi_queued(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *mysd = &__get_cpu_var(softnet_data);\n\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\treturn 0;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tlocal_irq_save(flags);\n\n\trps_lock(sd);\n\tif (skb_queue_len(&sd->input_pkt_queue) <= netdev_max_backlog) {\n\t\tif (skb_queue_len(&sd->input_pkt_queue)) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock(sd);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state)) {\n\t\t\tif (!rps_ipi_queued(sd))\n\t\t\t\t____napi_schedule(sd, &sd->backlog);\n\t\t}\n\t\tgoto enqueue;\n\t}\n\n\tsd->dropped++;\n\trps_unlock(sd);\n\n\tlocal_irq_restore(flags);\n\n\tatomic_long_inc(&skb->dev->rx_dropped);\n\tkfree_skb(skb);\n\treturn NET_RX_DROP;\n}\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process.  It always succeeds. The buffer\n *\tmay be dropped during processing for congestion control or by the\n *\tprotocol layers.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\n\nint netif_rx(struct sk_buff *skb)\n{\n\tint ret;\n\n\t/* if netpoll wants it, pretend we never saw it */\n\tif (netpoll_rx(skb))\n\t\treturn NET_RX_DROP;\n\n\tif (netdev_tstamp_prequeue)\n\t\tnet_timestamp_check(skb);\n\n\ttrace_netif_rx(skb);\n#ifdef CONFIG_RPS\n\t{\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\t}\n#else\n\t{\n\t\tunsigned int qtail;\n\t\tret = enqueue_to_backlog(skb, get_cpu(), &qtail);\n\t\tput_cpu();\n\t}\n#endif\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_rx);\n\nint netif_rx_ni(struct sk_buff *skb)\n{\n\tint err;\n\n\tpreempt_disable();\n\terr = netif_rx(skb);\n\tif (local_softirq_pending())\n\t\tdo_softirq();\n\tpreempt_enable();\n\n\treturn err;\n}\nEXPORT_SYMBOL(netif_rx_ni);\n\nstatic void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = &__get_cpu_var(softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(atomic_read(&skb->users));\n\t\t\ttrace_kfree_skb(skb, net_tx_action);\n\t\t\t__kfree_skb(skb);\n\t\t}\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock;\n\n\t\t\thead = head->next_sched;\n\n\t\t\troot_lock = qdisc_lock(q);\n\t\t\tif (spin_trylock(root_lock)) {\n\t\t\t\tsmp_mb__before_clear_bit();\n\t\t\t\tclear_bit(__QDISC_STATE_SCHED,\n\t\t\t\t\t  &q->state);\n\t\t\t\tqdisc_run(q);\n\t\t\t\tspin_unlock(root_lock);\n\t\t\t} else {\n\t\t\t\tif (!test_bit(__QDISC_STATE_DEACTIVATED,\n\t\t\t\t\t      &q->state)) {\n\t\t\t\t\t__netif_reschedule(q);\n\t\t\t\t} else {\n\t\t\t\t\tsmp_mb__before_clear_bit();\n\t\t\t\t\tclear_bit(__QDISC_STATE_SCHED,\n\t\t\t\t\t\t  &q->state);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n#if (defined(CONFIG_BRIDGE) || defined(CONFIG_BRIDGE_MODULE)) && \\\n    (defined(CONFIG_ATM_LANE) || defined(CONFIG_ATM_LANE_MODULE))\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\n#ifdef CONFIG_NET_CLS_ACT\n/* TODO: Maybe we should just force sch_ingress to be compiled in\n * when CONFIG_NET_CLS_ACT is? otherwise some useless instructions\n * a compare and 2 stores extra right now if we dont have it on\n * but have CONFIG_NET_CLS_ACT\n * NOTE: This doesnt stop any functionality; if you dont have\n * the ingress scheduler, you just cant add policies on ingress.\n *\n */\nstatic int ing_filter(struct sk_buff *skb, struct netdev_queue *rxq)\n{\n\tstruct net_device *dev = skb->dev;\n\tu32 ttl = G_TC_RTTL(skb->tc_verd);\n\tint result = TC_ACT_OK;\n\tstruct Qdisc *q;\n\n\tif (unlikely(MAX_RED_LOOP < ttl++)) {\n\t\tif (net_ratelimit())\n\t\t\tpr_warning( \"Redir loop detected Dropping packet (%d->%d)\\n\",\n\t\t\t       skb->skb_iif, dev->ifindex);\n\t\treturn TC_ACT_SHOT;\n\t}\n\n\tskb->tc_verd = SET_TC_RTTL(skb->tc_verd, ttl);\n\tskb->tc_verd = SET_TC_AT(skb->tc_verd, AT_INGRESS);\n\n\tq = rxq->qdisc;\n\tif (q != &noop_qdisc) {\n\t\tspin_lock(qdisc_lock(q));\n\t\tif (likely(!test_bit(__QDISC_STATE_DEACTIVATED, &q->state)))\n\t\t\tresult = qdisc_enqueue_root(skb, q);\n\t\tspin_unlock(qdisc_lock(q));\n\t}\n\n\treturn result;\n}\n\nstatic inline struct sk_buff *handle_ing(struct sk_buff *skb,\n\t\t\t\t\t struct packet_type **pt_prev,\n\t\t\t\t\t int *ret, struct net_device *orig_dev)\n{\n\tstruct netdev_queue *rxq = rcu_dereference(skb->dev->ingress_queue);\n\n\tif (!rxq || rxq->qdisc == &noop_qdisc)\n\t\tgoto out;\n\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tswitch (ing_filter(skb, rxq)) {\n\tcase TC_ACT_SHOT:\n\tcase TC_ACT_STOLEN:\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\nout:\n\tskb->tc_verd = 0;\n\treturn skb;\n}\n#endif\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive hander for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tASSERT_RTNL();\n\n\tif (dev->rx_handler)\n\t\treturn -EBUSY;\n\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive hander from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\trcu_assign_pointer(dev->rx_handler, NULL);\n\trcu_assign_pointer(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\nstatic inline void skb_bond_set_mac_by_master(struct sk_buff *skb,\n\t\t\t\t\t      struct net_device *master)\n{\n\tif (skb->pkt_type == PACKET_HOST) {\n\t\tu16 *dest = (u16 *) eth_hdr(skb)->h_dest;\n\n\t\tmemcpy(dest, master->dev_addr, ETH_ALEN);\n\t}\n}\n\n/* On bonding slaves other than the currently active slave, suppress\n * duplicates except for 802.3ad ETH_P_SLOW, alb non-mcast/bcast, and\n * ARP on active-backup slaves with arp_validate enabled.\n */\nint __skb_bond_should_drop(struct sk_buff *skb, struct net_device *master)\n{\n\tstruct net_device *dev = skb->dev;\n\n\tif (master->priv_flags & IFF_MASTER_ARPMON)\n\t\tdev->last_rx = jiffies;\n\n\tif ((master->priv_flags & IFF_MASTER_ALB) &&\n\t    (master->priv_flags & IFF_BRIDGE_PORT)) {\n\t\t/* Do address unmangle. The local destination address\n\t\t * will be always the one master has. Provides the right\n\t\t * functionality in a bridge.\n\t\t */\n\t\tskb_bond_set_mac_by_master(skb, master);\n\t}\n\n\tif (dev->priv_flags & IFF_SLAVE_INACTIVE) {\n\t\tif ((dev->priv_flags & IFF_SLAVE_NEEDARP) &&\n\t\t    skb->protocol == __cpu_to_be16(ETH_P_ARP))\n\t\t\treturn 0;\n\n\t\tif (master->priv_flags & IFF_MASTER_ALB) {\n\t\t\tif (skb->pkt_type != PACKET_BROADCAST &&\n\t\t\t    skb->pkt_type != PACKET_MULTICAST)\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (master->priv_flags & IFF_MASTER_8023AD &&\n\t\t    skb->protocol == __cpu_to_be16(ETH_P_SLOW))\n\t\t\treturn 0;\n\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__skb_bond_should_drop);\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct net_device *orig_dev;\n\tstruct net_device *master;\n\tstruct net_device *null_or_orig;\n\tstruct net_device *orig_or_bond;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tif (!netdev_tstamp_prequeue)\n\t\tnet_timestamp_check(skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\t/* if we've gotten here through NAPI, check netpoll */\n\tif (netpoll_receive_skb(skb))\n\t\treturn NET_RX_DROP;\n\n\tif (!skb->skb_iif)\n\t\tskb->skb_iif = skb->dev->ifindex;\n\n\t/*\n\t * bonding note: skbs received on inactive slaves should only\n\t * be delivered to pkt handlers that are exact matches.  Also\n\t * the deliver_no_wcard flag will be set.  If packet handlers\n\t * are sensitive to duplicate packets these skbs will need to\n\t * be dropped at the handler.\n\t */\n\tnull_or_orig = NULL;\n\torig_dev = skb->dev;\n\tmaster = ACCESS_ONCE(orig_dev->master);\n\tif (skb->deliver_no_wcard)\n\t\tnull_or_orig = orig_dev;\n\telse if (master) {\n\t\tif (skb_bond_should_drop(skb, master)) {\n\t\t\tskb->deliver_no_wcard = 1;\n\t\t\tnull_or_orig = orig_dev; /* deliver only exact match */\n\t\t} else\n\t\t\tskb->dev = master;\n\t}\n\n\t__this_cpu_inc(softnet_data.processed);\n\tskb_reset_network_header(skb);\n\tskb_reset_transport_header(skb);\n\tskb->mac_len = skb->network_header - skb->mac_header;\n\n\tpt_prev = NULL;\n\n\trcu_read_lock();\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (skb->tc_verd & TC_NCLS) {\n\t\tskb->tc_verd = CLR_TC_NCLS(skb->tc_verd);\n\t\tgoto ncls;\n\t}\n#endif\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (ptype->dev == null_or_orig || ptype->dev == skb->dev ||\n\t\t    ptype->dev == orig_dev) {\n\t\t\tif (pt_prev)\n\t\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = ptype;\n\t\t}\n\t}\n\n#ifdef CONFIG_NET_CLS_ACT\n\tskb = handle_ing(skb, &pt_prev, &ret, orig_dev);\n\tif (!skb)\n\t\tgoto out;\nncls:\n#endif\n\n\t/* Handle special case of bridge or macvlan */\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tskb = rx_handler(skb);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_hwaccel_do_receive(&skb)) {\n\t\t\tret = __netif_receive_skb(skb);\n\t\t\tgoto out;\n\t\t} else if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * Make sure frames received on VLAN interfaces stacked on\n\t * bonding interfaces still make their way to any base bonding\n\t * device that may have registered for a specific ptype.  The\n\t * handler may have to adjust skb->dev and orig_dev.\n\t */\n\torig_or_bond = orig_dev;\n\tif ((skb->dev->priv_flags & IFF_802_1Q_VLAN) &&\n\t    (vlan_dev_real_dev(skb->dev)->priv_flags & IFF_BONDING)) {\n\t\torig_or_bond = vlan_dev_real_dev(skb->dev);\n\t}\n\n\ttype = skb->protocol;\n\tlist_for_each_entry_rcu(ptype,\n\t\t\t&ptype_base[ntohs(type) & PTYPE_HASH_MASK], list) {\n\t\tif (ptype->type == type && (ptype->dev == null_or_orig ||\n\t\t     ptype->dev == skb->dev || ptype->dev == orig_dev ||\n\t\t     ptype->dev == orig_or_bond)) {\n\t\t\tif (pt_prev)\n\t\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = ptype;\n\t\t}\n\t}\n\n\tif (pt_prev) {\n\t\tret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t} else {\n\t\tatomic_long_inc(&skb->dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\tif (netdev_tstamp_prequeue)\n\t\tnet_timestamp_check(skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n#ifdef CONFIG_RPS\n\t{\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu, ret;\n\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t} else {\n\t\t\trcu_read_unlock();\n\t\t\tret = __netif_receive_skb(skb);\n\t\t}\n\n\t\treturn ret;\n\t}\n#else\n\treturn __netif_receive_skb(skb);\n#endif\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/* Network device is going away, flush any packets still pending\n * Called with irqs disabled.\n */\nstatic void flush_backlog(void *arg)\n{\n\tstruct net_device *dev = arg;\n\tstruct softnet_data *sd = &__get_cpu_var(softnet_data);\n\tstruct sk_buff *skb, *tmp;\n\n\trps_lock(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev == dev) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock(sd);\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev == dev) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n}\n\nstatic int napi_gro_complete(struct sk_buff *skb)\n{\n\tstruct packet_type *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &ptype_base[ntohs(type) & PTYPE_HASH_MASK];\n\tint err = -ENOENT;\n\n\tif (NAPI_GRO_CB(skb)->count == 1) {\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || ptype->dev || !ptype->gro_complete)\n\t\t\tcontinue;\n\n\t\terr = ptype->gro_complete(skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (err) {\n\t\tWARN_ON(&ptype->list == head);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\nout:\n\treturn netif_receive_skb(skb);\n}\n\ninline void napi_gro_flush(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb, *next;\n\n\tfor (skb = napi->gro_list; skb; skb = next) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\t\tnapi_gro_complete(skb);\n\t}\n\n\tnapi->gro_count = 0;\n\tnapi->gro_list = NULL;\n}\nEXPORT_SYMBOL(napi_gro_flush);\n\nenum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct packet_type *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &ptype_base[ntohs(type) & PTYPE_HASH_MASK];\n\tint same_flow;\n\tint mac_len;\n\tenum gro_result ret;\n\n\tif (!(skb->dev->features & NETIF_F_GRO) || netpoll_rx_on(skb))\n\t\tgoto normal;\n\n\tif (skb_is_gso(skb) || skb_has_frag_list(skb))\n\t\tgoto normal;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || ptype->dev || !ptype->gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tmac_len = skb->network_header - skb->mac_header;\n\t\tskb->mac_len = mac_len;\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = 0;\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\n\t\tpp = ptype->gro_receive(&napi->gro_list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tstruct sk_buff *nskb = *pp;\n\n\t\t*pp = nskb->next;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t\tnapi->gro_count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush || napi->gro_count >= MAX_GRO_SKBS)\n\t\tgoto normal;\n\n\tnapi->gro_count++;\n\tNAPI_GRO_CB(skb)->count = 1;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tskb->next = napi->gro_list;\n\tnapi->gro_list = skb;\n\tret = GRO_HELD;\n\npull:\n\tif (skb_headlen(skb) < skb_gro_offset(skb)) {\n\t\tint grow = skb_gro_offset(skb) - skb_headlen(skb);\n\n\t\tBUG_ON(skb->end - skb->tail < grow);\n\n\t\tmemcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);\n\n\t\tskb->tail += grow;\n\t\tskb->data_len -= grow;\n\n\t\tskb_shinfo(skb)->frags[0].page_offset += grow;\n\t\tskb_shinfo(skb)->frags[0].size -= grow;\n\n\t\tif (unlikely(!skb_shinfo(skb)->frags[0].size)) {\n\t\t\tput_page(skb_shinfo(skb)->frags[0].page);\n\t\t\tmemmove(skb_shinfo(skb)->frags,\n\t\t\t\tskb_shinfo(skb)->frags + 1,\n\t\t\t\t--skb_shinfo(skb)->nr_frags * sizeof(skb_frag_t));\n\t\t}\n\t}\n\nok:\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}\nEXPORT_SYMBOL(dev_gro_receive);\n\nstatic inline gro_result_t\n__napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff *p;\n\n\tfor (p = napi->gro_list; p; p = p->next) {\n\t\tunsigned long diffs;\n\n\t\tdiffs = (unsigned long)p->dev ^ (unsigned long)skb->dev;\n\t\tdiffs |= p->vlan_tci ^ skb->vlan_tci;\n\t\tdiffs |= compare_ether_header(skb_mac_header(p),\n\t\t\t\t\t      skb_gro_mac_header(skb));\n\t\tNAPI_GRO_CB(p)->same_flow = !diffs;\n\t\tNAPI_GRO_CB(p)->flush = 0;\n\t}\n\n\treturn dev_gro_receive(napi, skb);\n}\n\ngro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\t\tif (netif_receive_skb(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\tcase GRO_MERGED_FREE:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_HELD:\n\tcase GRO_MERGED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_skb_finish);\n\nvoid skb_gro_reset_offset(struct sk_buff *skb)\n{\n\tNAPI_GRO_CB(skb)->data_offset = 0;\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\n\tif (skb->mac_header == skb->tail &&\n\t    !PageHighMem(skb_shinfo(skb)->frags[0].page)) {\n\t\tNAPI_GRO_CB(skb)->frag0 =\n\t\t\tpage_address(skb_shinfo(skb)->frags[0].page) +\n\t\t\tskb_shinfo(skb)->frags[0].page_offset;\n\t\tNAPI_GRO_CB(skb)->frag0_len = skb_shinfo(skb)->frags[0].size;\n\t}\n}\nEXPORT_SYMBOL(skb_gro_reset_offset);\n\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tskb_gro_reset_offset(skb);\n\n\treturn napi_skb_finish(__napi_gro_receive(napi, skb), skb);\n}\nEXPORT_SYMBOL(napi_gro_receive);\n\nstatic void napi_reuse_skb(struct napi_struct *napi, struct sk_buff *skb)\n{\n\t__skb_pull(skb, skb_headlen(skb));\n\tskb_reserve(skb, NET_IP_ALIGN - skb_headroom(skb));\n\tskb->vlan_tci = 0;\n\tskb->dev = napi->dev;\n\tskb->skb_iif = 0;\n\n\tnapi->skb = skb;\n}\n\nstruct sk_buff *napi_get_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\n\tif (!skb) {\n\t\tskb = netdev_alloc_skb_ip_align(napi->dev, GRO_MAX_HEAD);\n\t\tif (skb)\n\t\t\tnapi->skb = skb;\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_get_frags);\n\ngro_result_t napi_frags_finish(struct napi_struct *napi, struct sk_buff *skb,\n\t\t\t       gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\tcase GRO_HELD:\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\n\t\tif (ret == GRO_HELD)\n\t\t\tskb_gro_pull(skb, -ETH_HLEN);\n\t\telse if (netif_receive_skb(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\tcase GRO_MERGED_FREE:\n\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_frags_finish);\n\nstruct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tstruct ethhdr *eth;\n\tunsigned int hlen;\n\tunsigned int off;\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*eth);\n\teth = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\teth = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!eth)) {\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\tskb = NULL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, sizeof(*eth));\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.  We'll fix it up properly at the end.\n\t */\n\tskb->protocol = eth->h_proto;\n\nout:\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_frags_skb);\n\ngro_result_t napi_gro_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi_frags_skb(napi);\n\n\tif (!skb)\n\t\treturn GRO_DROP;\n\n\treturn napi_frags_finish(napi, skb, __napi_gro_receive(napi, skb));\n}\nEXPORT_SYMBOL(napi_gro_frags);\n\n/*\n * net_rps_action sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\twhile (remsd) {\n\t\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\t\tif (cpu_online(remsd->cpu))\n\t\t\t\t__smp_call_function_single(remsd->cpu,\n\t\t\t\t\t\t\t   &remsd->csd, 0);\n\t\t\tremsd = next;\n\t\t}\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tint work = 0;\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\n#ifdef CONFIG_RPS\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd->rps_ipi_list) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n#endif\n\tnapi->weight = weight_p;\n\tlocal_irq_disable();\n\twhile (work < quota) {\n\t\tstruct sk_buff *skb;\n\t\tunsigned int qlen;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\tlocal_irq_enable();\n\t\t\t__netif_receive_skb(skb);\n\t\t\tlocal_irq_disable();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota) {\n\t\t\t\tlocal_irq_enable();\n\t\t\t\treturn work;\n\t\t\t}\n\t\t}\n\n\t\trps_lock(sd);\n\t\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\t\tif (qlen)\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\n\t\tif (qlen < quota - work) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set on backlog.\n\t\t\t * we can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tlist_del(&napi->poll_list);\n\t\t\tnapi->state = 0;\n\n\t\t\tquota = work + qlen;\n\t\t}\n\t\trps_unlock(sd);\n\t}\n\tlocal_irq_enable();\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(&__get_cpu_var(softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\nvoid __napi_complete(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tBUG_ON(n->gro_list);\n\n\tlist_del(&n->poll_list);\n\tsmp_mb__before_clear_bit();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n}\nEXPORT_SYMBOL(__napi_complete);\n\nvoid napi_complete(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\t/*\n\t * don't let napi dequeue from the cpu poll list\n\t * just in case its running on a different cpu\n\t */\n\tif (unlikely(test_bit(NAPI_STATE_NPSVC, &n->state)))\n\t\treturn;\n\n\tnapi_gro_flush(n);\n\tlocal_irq_save(flags);\n\t__napi_complete(n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(napi_complete);\n\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight)\n{\n\tINIT_LIST_HEAD(&napi->poll_list);\n\tnapi->gro_count = 0;\n\tnapi->gro_list = NULL;\n\tnapi->skb = NULL;\n\tnapi->poll = poll;\n\tnapi->weight = weight;\n\tlist_add(&napi->dev_list, &dev->napi_list);\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tspin_lock_init(&napi->poll_lock);\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n}\nEXPORT_SYMBOL(netif_napi_add);\n\nvoid netif_napi_del(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb, *next;\n\n\tlist_del_init(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tfor (skb = napi->gro_list; skb; skb = next) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\t\tkfree_skb(skb);\n\t}\n\n\tnapi->gro_list = NULL;\n\tnapi->gro_count = 0;\n}\nEXPORT_SYMBOL(netif_napi_del);\n\nstatic void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = &__get_cpu_var(softnet_data);\n\tunsigned long time_limit = jiffies + 2;\n\tint budget = netdev_budget;\n\tvoid *have;\n\n\tlocal_irq_disable();\n\n\twhile (!list_empty(&sd->poll_list)) {\n\t\tstruct napi_struct *n;\n\t\tint work, weight;\n\n\t\t/* If softirq window is exhuasted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 || time_after(jiffies, time_limit)))\n\t\t\tgoto softnet_break;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Even though interrupts have been re-enabled, this\n\t\t * access is safe because interrupts can only add new\n\t\t * entries to the tail of this list, and only ->poll()\n\t\t * calls can remove this head entry from the list.\n\t\t */\n\t\tn = list_first_entry(&sd->poll_list, struct napi_struct, poll_list);\n\n\t\thave = netpoll_poll_lock(n);\n\n\t\tweight = n->weight;\n\n\t\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t\t * with netpoll's poll_napi().  Only the entity which\n\t\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t\t * actually make the ->poll() call.  Therefore we avoid\n\t\t * accidently calling ->poll() when NAPI is not scheduled.\n\t\t */\n\t\twork = 0;\n\t\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\t\twork = n->poll(n, weight);\n\t\t\ttrace_napi_poll(n);\n\t\t}\n\n\t\tWARN_ON_ONCE(work > weight);\n\n\t\tbudget -= work;\n\n\t\tlocal_irq_disable();\n\n\t\t/* Drivers must not modify the NAPI state if they\n\t\t * consume the entire weight.  In such cases this code\n\t\t * still \"owns\" the NAPI instance and therefore can\n\t\t * move the instance around on the list at-will.\n\t\t */\n\t\tif (unlikely(work == weight)) {\n\t\t\tif (unlikely(napi_disable_pending(n))) {\n\t\t\t\tlocal_irq_enable();\n\t\t\t\tnapi_complete(n);\n\t\t\t\tlocal_irq_disable();\n\t\t\t} else\n\t\t\t\tlist_move_tail(&n->poll_list, &sd->poll_list);\n\t\t}\n\n\t\tnetpoll_poll_unlock(have);\n\t}\nout:\n\tnet_rps_action_and_irq_enable(sd);\n\n#ifdef CONFIG_NET_DMA\n\t/*\n\t * There may not be any more sk_buffs coming right now, so push\n\t * any pending DMA copies to hardware\n\t */\n\tdma_issue_pending_all();\n#endif\n\n\treturn;\n\nsoftnet_break:\n\tsd->time_squeeze++;\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\tgoto out;\n}\n\nstatic gifconf_func_t *gifconf_list[NPROTO];\n\n/**\n *\tregister_gifconf\t-\tregister a SIOCGIF handler\n *\t@family: Address family\n *\t@gifconf: Function handler\n *\n *\tRegister protocol dependent address dumping routines. The handler\n *\tthat is passed must not be freed or reused until it has been replaced\n *\tby another handler.\n */\nint register_gifconf(unsigned int family, gifconf_func_t *gifconf)\n{\n\tif (family >= NPROTO)\n\t\treturn -EINVAL;\n\tgifconf_list[family] = gifconf;\n\treturn 0;\n}\nEXPORT_SYMBOL(register_gifconf);\n\n\n/*\n *\tMap an interface index to its name (SIOCGIFNAME)\n */\n\n/*\n *\tWe need this ioctl for efficient implementation of the\n *\tif_indextoname() function required by the IPv6 API.  Without\n *\tit, we would have to search all the interfaces to find a\n *\tmatch.  --pb\n */\n\nstatic int dev_ifname(struct net *net, struct ifreq __user *arg)\n{\n\tstruct net_device *dev;\n\tstruct ifreq ifr;\n\n\t/*\n\t *\tFetch the caller's info block.\n\t */\n\n\tif (copy_from_user(&ifr, arg, sizeof(struct ifreq)))\n\t\treturn -EFAULT;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifr.ifr_ifindex);\n\tif (!dev) {\n\t\trcu_read_unlock();\n\t\treturn -ENODEV;\n\t}\n\n\tstrcpy(ifr.ifr_name, dev->name);\n\trcu_read_unlock();\n\n\tif (copy_to_user(arg, &ifr, sizeof(struct ifreq)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n *\tPerform a SIOCGIFCONF call. This structure will change\n *\tsize eventually, and there is nothing I can do about it.\n *\tThus we will need a 'compatibility mode'.\n */\n\nstatic int dev_ifconf(struct net *net, char __user *arg)\n{\n\tstruct ifconf ifc;\n\tstruct net_device *dev;\n\tchar __user *pos;\n\tint len;\n\tint total;\n\tint i;\n\n\t/*\n\t *\tFetch the caller's info block.\n\t */\n\n\tif (copy_from_user(&ifc, arg, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\tpos = ifc.ifc_buf;\n\tlen = ifc.ifc_len;\n\n\t/*\n\t *\tLoop over the interfaces, and write an info block for each.\n\t */\n\n\ttotal = 0;\n\tfor_each_netdev(net, dev) {\n\t\tfor (i = 0; i < NPROTO; i++) {\n\t\t\tif (gifconf_list[i]) {\n\t\t\t\tint done;\n\t\t\t\tif (!pos)\n\t\t\t\t\tdone = gifconf_list[i](dev, NULL, 0);\n\t\t\t\telse\n\t\t\t\t\tdone = gifconf_list[i](dev, pos + total,\n\t\t\t\t\t\t\t       len - total);\n\t\t\t\tif (done < 0)\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\ttotal += done;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t *\tAll done.  Write the updated control block back to the caller.\n\t */\n\tifc.ifc_len = total;\n\n\t/*\n\t * \tBoth BSD and Solaris return 0 here, so we do too.\n\t */\n\treturn copy_to_user(arg, &ifc, sizeof(struct ifconf)) ? -EFAULT : 0;\n}\n\n#ifdef CONFIG_PROC_FS\n/*\n *\tThis is invoked by the /proc filesystem handler to display a device\n *\tin detail.\n */\nvoid *dev_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct net *net = seq_file_net(seq);\n\tloff_t off;\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tif (!*pos)\n\t\treturn SEQ_START_TOKEN;\n\n\toff = 1;\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (off++ == *pos)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\n\nvoid *dev_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct net_device *dev = (v == SEQ_START_TOKEN) ?\n\t\t\t\t  first_net_device(seq_file_net(seq)) :\n\t\t\t\t  next_net_device((struct net_device *)v);\n\n\t++*pos;\n\treturn rcu_dereference(dev);\n}\n\nvoid dev_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic void dev_seq_printf_stats(struct seq_file *seq, struct net_device *dev)\n{\n\tstruct rtnl_link_stats64 temp;\n\tconst struct rtnl_link_stats64 *stats = dev_get_stats(dev, &temp);\n\n\tseq_printf(seq, \"%6s: %7llu %7llu %4llu %4llu %4llu %5llu %10llu %9llu \"\n\t\t   \"%8llu %7llu %4llu %4llu %4llu %5llu %7llu %10llu\\n\",\n\t\t   dev->name, stats->rx_bytes, stats->rx_packets,\n\t\t   stats->rx_errors,\n\t\t   stats->rx_dropped + stats->rx_missed_errors,\n\t\t   stats->rx_fifo_errors,\n\t\t   stats->rx_length_errors + stats->rx_over_errors +\n\t\t    stats->rx_crc_errors + stats->rx_frame_errors,\n\t\t   stats->rx_compressed, stats->multicast,\n\t\t   stats->tx_bytes, stats->tx_packets,\n\t\t   stats->tx_errors, stats->tx_dropped,\n\t\t   stats->tx_fifo_errors, stats->collisions,\n\t\t   stats->tx_carrier_errors +\n\t\t    stats->tx_aborted_errors +\n\t\t    stats->tx_window_errors +\n\t\t    stats->tx_heartbeat_errors,\n\t\t   stats->tx_compressed);\n}\n\n/*\n *\tCalled from the PROCfs module. This now uses the new arbitrary sized\n *\t/proc/net interface to create /proc/net/dev\n */\nstatic int dev_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"Inter-|   Receive                            \"\n\t\t\t      \"                    |  Transmit\\n\"\n\t\t\t      \" face |bytes    packets errs drop fifo frame \"\n\t\t\t      \"compressed multicast|bytes    packets errs \"\n\t\t\t      \"drop fifo colls carrier compressed\\n\");\n\telse\n\t\tdev_seq_printf_stats(seq, v);\n\treturn 0;\n}\n\nstatic struct softnet_data *softnet_get_online(loff_t *pos)\n{\n\tstruct softnet_data *sd = NULL;\n\n\twhile (*pos < nr_cpu_ids)\n\t\tif (cpu_online(*pos)) {\n\t\t\tsd = &per_cpu(softnet_data, *pos);\n\t\t\tbreak;\n\t\t} else\n\t\t\t++*pos;\n\treturn sd;\n}\n\nstatic void *softnet_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\treturn softnet_get_online(pos);\n}\n\nstatic void *softnet_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn softnet_get_online(pos);\n}\n\nstatic void softnet_seq_stop(struct seq_file *seq, void *v)\n{\n}\n\nstatic int softnet_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct softnet_data *sd = v;\n\n\tseq_printf(seq, \"%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\\n\",\n\t\t   sd->processed, sd->dropped, sd->time_squeeze, 0,\n\t\t   0, 0, 0, 0, /* was fastroute */\n\t\t   sd->cpu_collision, sd->received_rps);\n\treturn 0;\n}\n\nstatic const struct seq_operations dev_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = dev_seq_show,\n};\n\nstatic int dev_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &dev_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations dev_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open    = dev_seq_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = seq_release_net,\n};\n\nstatic const struct seq_operations softnet_seq_ops = {\n\t.start = softnet_seq_start,\n\t.next  = softnet_seq_next,\n\t.stop  = softnet_seq_stop,\n\t.show  = softnet_seq_show,\n};\n\nstatic int softnet_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &softnet_seq_ops);\n}\n\nstatic const struct file_operations softnet_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open    = softnet_seq_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = seq_release,\n};\n\nstatic void *ptype_get_idx(loff_t pos)\n{\n\tstruct packet_type *pt = NULL;\n\tloff_t i = 0;\n\tint t;\n\n\tlist_for_each_entry_rcu(pt, &ptype_all, list) {\n\t\tif (i == pos)\n\t\t\treturn pt;\n\t\t++i;\n\t}\n\n\tfor (t = 0; t < PTYPE_HASH_SIZE; t++) {\n\t\tlist_for_each_entry_rcu(pt, &ptype_base[t], list) {\n\t\t\tif (i == pos)\n\t\t\t\treturn pt;\n\t\t\t++i;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void *ptype_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\trcu_read_lock();\n\treturn *pos ? ptype_get_idx(*pos - 1) : SEQ_START_TOKEN;\n}\n\nstatic void *ptype_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct packet_type *pt;\n\tstruct list_head *nxt;\n\tint hash;\n\n\t++*pos;\n\tif (v == SEQ_START_TOKEN)\n\t\treturn ptype_get_idx(0);\n\n\tpt = v;\n\tnxt = pt->list.next;\n\tif (pt->type == htons(ETH_P_ALL)) {\n\t\tif (nxt != &ptype_all)\n\t\t\tgoto found;\n\t\thash = 0;\n\t\tnxt = ptype_base[0].next;\n\t} else\n\t\thash = ntohs(pt->type) & PTYPE_HASH_MASK;\n\n\twhile (nxt == &ptype_base[hash]) {\n\t\tif (++hash >= PTYPE_HASH_SIZE)\n\t\t\treturn NULL;\n\t\tnxt = ptype_base[hash].next;\n\t}\nfound:\n\treturn list_entry(nxt, struct packet_type, list);\n}\n\nstatic void ptype_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic int ptype_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct packet_type *pt = v;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"Type Device      Function\\n\");\n\telse if (pt->dev == NULL || dev_net(pt->dev) == seq_file_net(seq)) {\n\t\tif (pt->type == htons(ETH_P_ALL))\n\t\t\tseq_puts(seq, \"ALL \");\n\t\telse\n\t\t\tseq_printf(seq, \"%04x\", ntohs(pt->type));\n\n\t\tseq_printf(seq, \" %-8s %pF\\n\",\n\t\t\t   pt->dev ? pt->dev->name : \"\", pt->func);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations ptype_seq_ops = {\n\t.start = ptype_seq_start,\n\t.next  = ptype_seq_next,\n\t.stop  = ptype_seq_stop,\n\t.show  = ptype_seq_show,\n};\n\nstatic int ptype_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &ptype_seq_ops,\n\t\t\tsizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations ptype_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open    = ptype_seq_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = seq_release_net,\n};\n\n\nstatic int __net_init dev_proc_net_init(struct net *net)\n{\n\tint rc = -ENOMEM;\n\n\tif (!proc_net_fops_create(net, \"dev\", S_IRUGO, &dev_seq_fops))\n\t\tgoto out;\n\tif (!proc_net_fops_create(net, \"softnet_stat\", S_IRUGO, &softnet_seq_fops))\n\t\tgoto out_dev;\n\tif (!proc_net_fops_create(net, \"ptype\", S_IRUGO, &ptype_seq_fops))\n\t\tgoto out_softnet;\n\n\tif (wext_proc_init(net))\n\t\tgoto out_ptype;\n\trc = 0;\nout:\n\treturn rc;\nout_ptype:\n\tproc_net_remove(net, \"ptype\");\nout_softnet:\n\tproc_net_remove(net, \"softnet_stat\");\nout_dev:\n\tproc_net_remove(net, \"dev\");\n\tgoto out;\n}\n\nstatic void __net_exit dev_proc_net_exit(struct net *net)\n{\n\twext_proc_exit(net);\n\n\tproc_net_remove(net, \"ptype\");\n\tproc_net_remove(net, \"softnet_stat\");\n\tproc_net_remove(net, \"dev\");\n}\n\nstatic struct pernet_operations __net_initdata dev_proc_ops = {\n\t.init = dev_proc_net_init,\n\t.exit = dev_proc_net_exit,\n};\n\nstatic int __init dev_proc_init(void)\n{\n\treturn register_pernet_subsys(&dev_proc_ops);\n}\n#else\n#define dev_proc_init() 0\n#endif\t/* CONFIG_PROC_FS */\n\n\n/**\n *\tnetdev_set_master\t-\tset up master/slave pair\n *\t@slave: slave device\n *\t@master: new master device\n *\n *\tChanges the master device of the slave. Pass %NULL to break the\n *\tbonding. The caller must hold the RTNL semaphore. On a failure\n *\ta negative errno code is returned. On success the reference counts\n *\tare adjusted, %RTM_NEWLINK is sent to the routing socket and the\n *\tfunction returns zero.\n */\nint netdev_set_master(struct net_device *slave, struct net_device *master)\n{\n\tstruct net_device *old = slave->master;\n\n\tASSERT_RTNL();\n\n\tif (master) {\n\t\tif (old)\n\t\t\treturn -EBUSY;\n\t\tdev_hold(master);\n\t}\n\n\tslave->master = master;\n\n\tif (old) {\n\t\tsynchronize_net();\n\t\tdev_put(old);\n\t}\n\tif (master)\n\t\tslave->flags |= IFF_SLAVE;\n\telse\n\t\tslave->flags &= ~IFF_SLAVE;\n\n\trtmsg_ifinfo(RTM_NEWLINK, slave, IFF_SLAVE);\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_master);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif ((dev->flags & IFF_UP) && ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned short old_flags = dev->flags;\n\tuid_t uid;\n\tgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tprintk(KERN_WARNING \"%s: promiscuity touches roof, \"\n\t\t\t\t\"set promiscuity failed, promiscuity feature \"\n\t\t\t\t\"of device might be broken.\\n\", dev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tprintk(KERN_INFO \"device %s %s promiscuous mode\\n\",\n\t\t       dev->name, (dev->flags & IFF_PROMISC) ? \"entered\" :\n\t\t\t\t\t\t\t       \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(current->audit_context, GFP_ATOMIC,\n\t\t\t\tAUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t\"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\tdev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t(old_flags & IFF_PROMISC),\n\t\t\t\taudit_get_loginuid(current),\n\t\t\t\tuid, gid,\n\t\t\t\taudit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned short old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\tunsigned short old_flags = dev->flags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tprintk(KERN_WARNING \"%s: allmulti touches roof, \"\n\t\t\t\t\"set allmulti failed, allmulti feature of \"\n\t\t\t\t\"device might be broken.\\n\", dev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n\telse {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1);\n\t\t\tdev->uc_promisc = 1;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1);\n\t\t\tdev->uc_promisc = 0;\n\t\t}\n\n\t\tif (ops->ndo_set_multicast_list)\n\t\t\tops->ndo_set_multicast_list(dev);\n\t}\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned dev_get_flags(const struct net_device *dev)\n{\n\tunsigned flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tint old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\t/* Bit is different  ? */\n\t\tret = ((old_flags & IFF_UP) ? __dev_close : __dev_open)(dev);\n\n\t\tif (!ret)\n\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\t\tdev_set_promiscuity(dev, inc);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t   is important. Some (broken) drivers set IFF_PROMISC, when\n\t   IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\tdev_set_allmulti(dev, inc);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE)))\n\t\tcall_netdevice_notifiers(NETDEV_CHANGE, dev);\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned flags)\n{\n\tint ret, changes;\n\tint old_flags = dev->flags;\n\n\tret = __dev_change_flags(dev, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = old_flags ^ dev->flags;\n\tif (changes)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, changes);\n\n\t__dev_notify_flags(dev, old_flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\n/**\n *\tdev_set_mtu - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\t/*\tMTU must be positive.\t */\n\tif (new_mtu < 0)\n\t\treturn -EINVAL;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = 0;\n\tif (ops->ndo_change_mtu)\n\t\terr = ops->ndo_change_mtu(dev, new_mtu);\n\telse\n\t\tdev->mtu = new_mtu;\n\n\tif (!err && dev->flags & IFF_UP)\n\t\tcall_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (!err)\n\t\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\n/*\n *\tPerform the SIOCxIFxxx calls, inside rcu_read_lock()\n */\nstatic int dev_ifsioc_locked(struct net *net, struct ifreq *ifr, unsigned int cmd)\n{\n\tint err;\n\tstruct net_device *dev = dev_get_by_name_rcu(net, ifr->ifr_name);\n\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tswitch (cmd) {\n\tcase SIOCGIFFLAGS:\t/* Get interface flags */\n\t\tifr->ifr_flags = (short) dev_get_flags(dev);\n\t\treturn 0;\n\n\tcase SIOCGIFMETRIC:\t/* Get the metric on the interface\n\t\t\t\t   (currently unused) */\n\t\tifr->ifr_metric = 0;\n\t\treturn 0;\n\n\tcase SIOCGIFMTU:\t/* Get the MTU of a device */\n\t\tifr->ifr_mtu = dev->mtu;\n\t\treturn 0;\n\n\tcase SIOCGIFHWADDR:\n\t\tif (!dev->addr_len)\n\t\t\tmemset(ifr->ifr_hwaddr.sa_data, 0, sizeof ifr->ifr_hwaddr.sa_data);\n\t\telse\n\t\t\tmemcpy(ifr->ifr_hwaddr.sa_data, dev->dev_addr,\n\t\t\t       min(sizeof ifr->ifr_hwaddr.sa_data, (size_t) dev->addr_len));\n\t\tifr->ifr_hwaddr.sa_family = dev->type;\n\t\treturn 0;\n\n\tcase SIOCGIFSLAVE:\n\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase SIOCGIFMAP:\n\t\tifr->ifr_map.mem_start = dev->mem_start;\n\t\tifr->ifr_map.mem_end   = dev->mem_end;\n\t\tifr->ifr_map.base_addr = dev->base_addr;\n\t\tifr->ifr_map.irq       = dev->irq;\n\t\tifr->ifr_map.dma       = dev->dma;\n\t\tifr->ifr_map.port      = dev->if_port;\n\t\treturn 0;\n\n\tcase SIOCGIFINDEX:\n\t\tifr->ifr_ifindex = dev->ifindex;\n\t\treturn 0;\n\n\tcase SIOCGIFTXQLEN:\n\t\tifr->ifr_qlen = dev->tx_queue_len;\n\t\treturn 0;\n\n\tdefault:\n\t\t/* dev_ioctl() should ensure this case\n\t\t * is never reached\n\t\t */\n\t\tWARN_ON(1);\n\t\terr = -EINVAL;\n\t\tbreak;\n\n\t}\n\treturn err;\n}\n\n/*\n *\tPerform the SIOCxIFxxx calls, inside rtnl_lock()\n */\nstatic int dev_ifsioc(struct net *net, struct ifreq *ifr, unsigned int cmd)\n{\n\tint err;\n\tstruct net_device *dev = __dev_get_by_name(net, ifr->ifr_name);\n\tconst struct net_device_ops *ops;\n\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tops = dev->netdev_ops;\n\n\tswitch (cmd) {\n\tcase SIOCSIFFLAGS:\t/* Set interface flags */\n\t\treturn dev_change_flags(dev, ifr->ifr_flags);\n\n\tcase SIOCSIFMETRIC:\t/* Set the metric on the interface\n\t\t\t\t   (currently unused) */\n\t\treturn -EOPNOTSUPP;\n\n\tcase SIOCSIFMTU:\t/* Set the MTU of a device */\n\t\treturn dev_set_mtu(dev, ifr->ifr_mtu);\n\n\tcase SIOCSIFHWADDR:\n\t\treturn dev_set_mac_address(dev, &ifr->ifr_hwaddr);\n\n\tcase SIOCSIFHWBROADCAST:\n\t\tif (ifr->ifr_hwaddr.sa_family != dev->type)\n\t\t\treturn -EINVAL;\n\t\tmemcpy(dev->broadcast, ifr->ifr_hwaddr.sa_data,\n\t\t       min(sizeof ifr->ifr_hwaddr.sa_data, (size_t) dev->addr_len));\n\t\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\t\treturn 0;\n\n\tcase SIOCSIFMAP:\n\t\tif (ops->ndo_set_config) {\n\t\t\tif (!netif_device_present(dev))\n\t\t\t\treturn -ENODEV;\n\t\t\treturn ops->ndo_set_config(dev, &ifr->ifr_map);\n\t\t}\n\t\treturn -EOPNOTSUPP;\n\n\tcase SIOCADDMULTI:\n\t\tif ((!ops->ndo_set_multicast_list && !ops->ndo_set_rx_mode) ||\n\t\t    ifr->ifr_hwaddr.sa_family != AF_UNSPEC)\n\t\t\treturn -EINVAL;\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t\treturn dev_mc_add_global(dev, ifr->ifr_hwaddr.sa_data);\n\n\tcase SIOCDELMULTI:\n\t\tif ((!ops->ndo_set_multicast_list && !ops->ndo_set_rx_mode) ||\n\t\t    ifr->ifr_hwaddr.sa_family != AF_UNSPEC)\n\t\t\treturn -EINVAL;\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t\treturn dev_mc_del_global(dev, ifr->ifr_hwaddr.sa_data);\n\n\tcase SIOCSIFTXQLEN:\n\t\tif (ifr->ifr_qlen < 0)\n\t\t\treturn -EINVAL;\n\t\tdev->tx_queue_len = ifr->ifr_qlen;\n\t\treturn 0;\n\n\tcase SIOCSIFNAME:\n\t\tifr->ifr_newname[IFNAMSIZ-1] = '\\0';\n\t\treturn dev_change_name(dev, ifr->ifr_newname);\n\n\t/*\n\t *\tUnknown or private ioctl\n\t */\n\tdefault:\n\t\tif ((cmd >= SIOCDEVPRIVATE &&\n\t\t    cmd <= SIOCDEVPRIVATE + 15) ||\n\t\t    cmd == SIOCBONDENSLAVE ||\n\t\t    cmd == SIOCBONDRELEASE ||\n\t\t    cmd == SIOCBONDSETHWADDR ||\n\t\t    cmd == SIOCBONDSLAVEINFOQUERY ||\n\t\t    cmd == SIOCBONDINFOQUERY ||\n\t\t    cmd == SIOCBONDCHANGEACTIVE ||\n\t\t    cmd == SIOCGMIIPHY ||\n\t\t    cmd == SIOCGMIIREG ||\n\t\t    cmd == SIOCSMIIREG ||\n\t\t    cmd == SIOCBRADDIF ||\n\t\t    cmd == SIOCBRDELIF ||\n\t\t    cmd == SIOCSHWTSTAMP ||\n\t\t    cmd == SIOCWANDEV) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (ops->ndo_do_ioctl) {\n\t\t\t\tif (netif_device_present(dev))\n\t\t\t\t\terr = ops->ndo_do_ioctl(dev, ifr, cmd);\n\t\t\t\telse\n\t\t\t\t\terr = -ENODEV;\n\t\t\t}\n\t\t} else\n\t\t\terr = -EINVAL;\n\n\t}\n\treturn err;\n}\n\n/*\n *\tThis function handles all \"interface\"-type I/O control requests. The actual\n *\t'doing' part of this is dev_ifsioc above.\n */\n\n/**\n *\tdev_ioctl\t-\tnetwork device ioctl\n *\t@net: the applicable net namespace\n *\t@cmd: command to issue\n *\t@arg: pointer to a struct ifreq in user space\n *\n *\tIssue ioctl functions to devices. This is normally called by the\n *\tuser space syscall interfaces but can sometimes be useful for\n *\tother purposes. The return value is the return from the syscall if\n *\tpositive or a negative errno code on error.\n */\n\nint dev_ioctl(struct net *net, unsigned int cmd, void __user *arg)\n{\n\tstruct ifreq ifr;\n\tint ret;\n\tchar *colon;\n\n\t/* One special case: SIOCGIFCONF takes ifconf argument\n\t   and requires shared lock, because it sleeps writing\n\t   to user space.\n\t */\n\n\tif (cmd == SIOCGIFCONF) {\n\t\trtnl_lock();\n\t\tret = dev_ifconf(net, (char __user *) arg);\n\t\trtnl_unlock();\n\t\treturn ret;\n\t}\n\tif (cmd == SIOCGIFNAME)\n\t\treturn dev_ifname(net, (struct ifreq __user *)arg);\n\n\tif (copy_from_user(&ifr, arg, sizeof(struct ifreq)))\n\t\treturn -EFAULT;\n\n\tifr.ifr_name[IFNAMSIZ-1] = 0;\n\n\tcolon = strchr(ifr.ifr_name, ':');\n\tif (colon)\n\t\t*colon = 0;\n\n\t/*\n\t *\tSee which interface the caller is talking about.\n\t */\n\n\tswitch (cmd) {\n\t/*\n\t *\tThese ioctl calls:\n\t *\t- can be done by all.\n\t *\t- atomic and do not require locking.\n\t *\t- return a value\n\t */\n\tcase SIOCGIFFLAGS:\n\tcase SIOCGIFMETRIC:\n\tcase SIOCGIFMTU:\n\tcase SIOCGIFHWADDR:\n\tcase SIOCGIFSLAVE:\n\tcase SIOCGIFMAP:\n\tcase SIOCGIFINDEX:\n\tcase SIOCGIFTXQLEN:\n\t\tdev_load(net, ifr.ifr_name);\n\t\trcu_read_lock();\n\t\tret = dev_ifsioc_locked(net, &ifr, cmd);\n\t\trcu_read_unlock();\n\t\tif (!ret) {\n\t\t\tif (colon)\n\t\t\t\t*colon = ':';\n\t\t\tif (copy_to_user(arg, &ifr,\n\t\t\t\t\t sizeof(struct ifreq)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t\treturn ret;\n\n\tcase SIOCETHTOOL:\n\t\tdev_load(net, ifr.ifr_name);\n\t\trtnl_lock();\n\t\tret = dev_ethtool(net, &ifr);\n\t\trtnl_unlock();\n\t\tif (!ret) {\n\t\t\tif (colon)\n\t\t\t\t*colon = ':';\n\t\t\tif (copy_to_user(arg, &ifr,\n\t\t\t\t\t sizeof(struct ifreq)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t\treturn ret;\n\n\t/*\n\t *\tThese ioctl calls:\n\t *\t- require superuser power.\n\t *\t- require strict serialization.\n\t *\t- return a value\n\t */\n\tcase SIOCGMIIPHY:\n\tcase SIOCGMIIREG:\n\tcase SIOCSIFNAME:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tdev_load(net, ifr.ifr_name);\n\t\trtnl_lock();\n\t\tret = dev_ifsioc(net, &ifr, cmd);\n\t\trtnl_unlock();\n\t\tif (!ret) {\n\t\t\tif (colon)\n\t\t\t\t*colon = ':';\n\t\t\tif (copy_to_user(arg, &ifr,\n\t\t\t\t\t sizeof(struct ifreq)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t\treturn ret;\n\n\t/*\n\t *\tThese ioctl calls:\n\t *\t- require superuser power.\n\t *\t- require strict serialization.\n\t *\t- do not return a value\n\t */\n\tcase SIOCSIFFLAGS:\n\tcase SIOCSIFMETRIC:\n\tcase SIOCSIFMTU:\n\tcase SIOCSIFMAP:\n\tcase SIOCSIFHWADDR:\n\tcase SIOCSIFSLAVE:\n\tcase SIOCADDMULTI:\n\tcase SIOCDELMULTI:\n\tcase SIOCSIFHWBROADCAST:\n\tcase SIOCSIFTXQLEN:\n\tcase SIOCSMIIREG:\n\tcase SIOCBONDENSLAVE:\n\tcase SIOCBONDRELEASE:\n\tcase SIOCBONDSETHWADDR:\n\tcase SIOCBONDCHANGEACTIVE:\n\tcase SIOCBRADDIF:\n\tcase SIOCBRDELIF:\n\tcase SIOCSHWTSTAMP:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\t/* fall through */\n\tcase SIOCBONDSLAVEINFOQUERY:\n\tcase SIOCBONDINFOQUERY:\n\t\tdev_load(net, ifr.ifr_name);\n\t\trtnl_lock();\n\t\tret = dev_ifsioc(net, &ifr, cmd);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tcase SIOCGIFMEM:\n\t\t/* Get the per device memory space. We can add this but\n\t\t * currently do not support it */\n\tcase SIOCSIFMEM:\n\t\t/* Set the per device memory buffer space.\n\t\t * Not applicable in our case */\n\tcase SIOCSIFLINK:\n\t\treturn -EINVAL;\n\n\t/*\n\t *\tUnknown or private ioctl.\n\t */\n\tdefault:\n\t\tif (cmd == SIOCWANDEV ||\n\t\t    (cmd >= SIOCDEVPRIVATE &&\n\t\t     cmd <= SIOCDEVPRIVATE + 15)) {\n\t\t\tdev_load(net, ifr.ifr_name);\n\t\t\trtnl_lock();\n\t\t\tret = dev_ifsioc(net, &ifr, cmd);\n\t\t\trtnl_unlock();\n\t\t\tif (!ret && copy_to_user(arg, &ifr,\n\t\t\t\t\t\t sizeof(struct ifreq)))\n\t\t\t\tret = -EFAULT;\n\t\t\treturn ret;\n\t\t}\n\t\t/* Take care of Wireless Extensions */\n\t\tif (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST)\n\t\t\treturn wext_handle_ioctl(net, &ifr, cmd, arg);\n\t\treturn -EINVAL;\n\t}\n}\n\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tstatic int ifindex;\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nstatic LIST_HEAD(net_todo_list);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n}\n\nstatic void rollback_registered_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never \"\n\t\t\t\t \"was registered\\n\", dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tdev_close_many(head);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t   this device. They should clean all the things.\n\t\t*/\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\trtmsg_ifinfo(RTM_DELLINK, dev, ~0U);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\t/* Notifier chain MUST detach us from master device. */\n\t\tWARN_ON(dev->master);\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n\t}\n\n\t/* Process any work delayed until the end of the batch */\n\tdev = list_first_entry(head, struct net_device, unreg_list);\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER_BATCH, dev);\n\n\trcu_barrier();\n\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tdev_put(dev);\n}\n\nstatic void rollback_registered(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\trollback_registered_many(&single);\n\tlist_del(&single);\n}\n\nunsigned long netdev_fix_features(unsigned long features, const char *name)\n{\n\t/* Fix illegal SG+CSUM combinations. */\n\tif ((features & NETIF_F_SG) &&\n\t    !(features & NETIF_F_ALL_CSUM)) {\n\t\tif (name)\n\t\t\tprintk(KERN_NOTICE \"%s: Dropping NETIF_F_SG since no \"\n\t\t\t       \"checksum feature.\\n\", name);\n\t\tfeatures &= ~NETIF_F_SG;\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_SG)) {\n\t\tif (name)\n\t\t\tprintk(KERN_NOTICE \"%s: Dropping NETIF_F_TSO since no \"\n\t\t\t       \"SG feature.\\n\", name);\n\t\tfeatures &= ~NETIF_F_TSO;\n\t}\n\n\tif (features & NETIF_F_UFO) {\n\t\t/* maybe split UFO into V4 and V6? */\n\t\tif (!((features & NETIF_F_GEN_CSUM) ||\n\t\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))\n\t\t\t    == (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\t\tif (name)\n\t\t\t\tprintk(KERN_ERR \"%s: Dropping NETIF_F_UFO \"\n\t\t\t\t       \"since no checksum offload features.\\n\",\n\t\t\t\t       name);\n\t\t\tfeatures &= ~NETIF_F_UFO;\n\t\t}\n\n\t\tif (!(features & NETIF_F_SG)) {\n\t\t\tif (name)\n\t\t\t\tprintk(KERN_ERR \"%s: Dropping NETIF_F_UFO \"\n\t\t\t\t       \"since no NETIF_F_SG feature.\\n\", name);\n\t\t\tfeatures &= ~NETIF_F_UFO;\n\t\t}\n\t}\n\n\treturn features;\n}\nEXPORT_SYMBOL(netdev_fix_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (netif_carrier_ok(rootdev)) {\n\t\tif (!netif_carrier_ok(dev))\n\t\t\tnetif_carrier_on(dev);\n\t} else {\n\t\tif (netif_carrier_ok(dev))\n\t\t\tnetif_carrier_off(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\n#ifdef CONFIG_RPS\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\n\tBUG_ON(count < 1);\n\n\trx = kcalloc(count, sizeof(struct netdev_rx_queue), GFP_KERNEL);\n\tif (!rx) {\n\t\tpr_err(\"netdev: Unable to allocate %u rx queues.\\n\", count);\n\t\treturn -ENOMEM;\n\t}\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++)\n\t\trx[i].dev = dev;\n\treturn 0;\n}\n#endif\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\n\tBUG_ON(count < 1);\n\n\ttx = kcalloc(count, sizeof(struct netdev_queue), GFP_KERNEL);\n\tif (!tx) {\n\t\tpr_err(\"netdev: Unable to allocate %u tx queues.\\n\",\n\t\t       count);\n\t\treturn -ENOMEM;\n\t}\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\n/**\n *\tregister_netdevice\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tCallers must hold the rtnl semaphore. You may want\n *\tregister_netdev() instead of this.\n *\n *\tBUGS:\n *\tThe locking appears insufficient to guarantee two parallel registers\n *\twill not get the same name.\n */\n\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tdev->iflink = -1;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = dev_get_valid_name(dev, dev->name, 0);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tdev->ifindex = dev_new_index(net);\n\tif (dev->iflink == -1)\n\t\tdev->iflink = dev->ifindex;\n\n\t/* Fix illegal checksum combinations */\n\tif ((dev->features & NETIF_F_HW_CSUM) &&\n\t    (dev->features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tprintk(KERN_NOTICE \"%s: mixed HW and IP checksum settings.\\n\",\n\t\t       dev->name);\n\t\tdev->features &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\tif ((dev->features & NETIF_F_NO_CSUM) &&\n\t    (dev->features & (NETIF_F_HW_CSUM|NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tprintk(KERN_NOTICE \"%s: mixed no checksumming and other settings.\\n\",\n\t\t       dev->name);\n\t\tdev->features &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM|NETIF_F_HW_CSUM);\n\t}\n\n\tdev->features = netdev_fix_features(dev->features, dev->name);\n\n\t/* Enable software GSO if SG is supported. */\n\tif (dev->features & NETIF_F_SG)\n\t\tdev->features |= NETIF_F_GSO;\n\n\t/* Enable GRO and NETIF_F_HIGHDMA for vlans by default,\n\t * vlan_dev_init() will do the dev->features check, so these features\n\t * are enabled only if supported by underlying device.\n\t */\n\tdev->vlan_features |= (NETIF_F_GRO | NETIF_F_HIGHDMA);\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = netdev_register_kobject(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\tdev->reg_state = NETREG_REGISTERED;\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tdev_init_scheduler(dev);\n\tdev_hold(dev);\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\trollback_registered(dev);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U);\n\nout:\n\treturn ret;\n\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\trtnl_lock();\n\n\t/*\n\t * If the name is a format string the caller wants us to do a\n\t * name allocation.\n\t */\n\tif (strchr(dev->name, '%')) {\n\t\terr = dev_alloc_name(dev, dev->name);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\terr = register_netdevice(dev);\nout:\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\n/*\n * netdev_wait_allrefs - wait until all references are gone.\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic void netdev_wait_allrefs(struct net_device *dev)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tint refcnt;\n\n\tlinkwatch_forget_dev(dev);\n\n\trebroadcast_time = warning_time = jiffies;\n\trefcnt = netdev_refcnt_read(dev);\n\n\twhile (refcnt != 0) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\t\t\t/* don't resend NETDEV_UNREGISTER_BATCH, _BATCH users\n\t\t\t * should have already handle it the first time */\n\n\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t     &dev->state)) {\n\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t * pending on unregister. If this\n\t\t\t\t * happens, we simply run the queue\n\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t * for this device.\n\t\t\t\t */\n\t\t\t\tlinkwatch_run_queue();\n\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tmsleep(250);\n\n\t\trefcnt = netdev_refcnt_read(dev);\n\n\t\tif (time_after(jiffies, warning_time + 10 * HZ)) {\n\t\t\tprintk(KERN_EMERG \"unregister_netdevice: \"\n\t\t\t       \"waiting for %s to become free. Usage \"\n\t\t\t       \"count = %d\\n\",\n\t\t\t       dev->name, refcnt);\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct list_head list;\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\twhile (!list_empty(&list)) {\n\t\tstruct net_device *dev\n\t\t\t= list_first_entry(&list, struct net_device, todo_list);\n\t\tlist_del(&dev->todo_list);\n\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tprintk(KERN_ERR \"network todo '%s' but state %d\\n\",\n\t\t\t       dev->name, dev->reg_state);\n\t\t\tdump_stack();\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\n\t\ton_each_cpu(flush_backlog, dev, 1);\n\n\t\tnetdev_wait_allrefs(dev);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev));\n\t\tWARN_ON(rcu_dereference_raw(dev->ip_ptr));\n\t\tWARN_ON(rcu_dereference_raw(dev->ip6_ptr));\n\t\tWARN_ON(dev->dn_ptr);\n\n\t\tif (dev->destructor)\n\t\t\tdev->destructor(dev);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64.  They have the same\n * fields in the same order, with only the type differing.\n */\nstatic void netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t\t    const struct net_device_stats *netdev_stats)\n{\n#if BITS_PER_LONG == 64\n        BUILD_BUG_ON(sizeof(*stats64) != sizeof(*netdev_stats));\n        memcpy(stats64, netdev_stats, sizeof(*stats64));\n#else\n\tsize_t i, n = sizeof(*stats64) / sizeof(u64);\n\tconst unsigned long *src = (const unsigned long *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(sizeof(*netdev_stats) / sizeof(unsigned long) !=\n\t\t     sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = src[i];\n#endif\n}\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\tstorage->rx_dropped += atomic_long_read(&dev->rx_dropped);\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tqueue->qdisc = &noop_qdisc;\n\tqueue->qdisc_sleeping = &noop_qdisc;\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\n/**\n *\talloc_netdev_mqs - allocate network device\n *\t@sizeof_priv:\tsize of private data to allocate space for\n *\t@name:\t\tdevice name format string\n *\t@setup:\t\tcallback to initialize device\n *\t@txqs:\t\tthe number of TX subqueues to allocate\n *\t@rxqs:\t\tthe number of RX subqueues to allocate\n *\n *\tAllocates a struct net_device with private data area for driver use\n *\tand performs basic initialization.  Also allocates subquue structs\n *\tfor each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tsize_t alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device \"\n\t\t       \"with zero queues.\\n\");\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_RPS\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device \"\n\t\t       \"with zero RX queues.\\n\");\n\t\treturn NULL;\n\t}\n#endif\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kzalloc(alloc_size, GFP_KERNEL);\n\tif (!p) {\n\t\tprintk(KERN_ERR \"alloc_netdev: Unable to allocate device.\\n\");\n\t\treturn NULL;\n\t}\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_p;\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_MAX_SIZE;\n\n\tINIT_LIST_HEAD(&dev->ethtool_ntuple_list.list);\n\tdev->ethtool_ntuple_list.count = 0;\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE;\n\tsetup(dev);\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n#ifdef CONFIG_RPS\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n#endif\n\n\tstrcpy(dev->name, name);\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n\tfree_percpu(dev->pcpu_refcnt);\n\tkfree(dev->_tx);\n#ifdef CONFIG_RPS\n\tkfree(dev->_rx);\n#endif\n\nfree_p:\n\tkfree(p);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n *\tfree_netdev - free network device\n *\t@dev: device\n *\n *\tThis function does the last stage of destroying an allocated device\n * \tinterface. The reference to the device object is released.\n *\tIf this is the last reference then it will be freed.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\trelease_net(dev_net(dev));\n\n\tkfree(dev->_tx);\n#ifdef CONFIG_RPS\n\tkfree(dev->_rx);\n#endif\n\n\tkfree(rcu_dereference_raw(dev->ingress_queue));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\t/* Clear ethtool n-tuple list */\n\tethtool_ntuple_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tkfree((char *)dev - dev->padded);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\trollback_registered(dev);\n\t\t/* Finish processing unregister after unlock */\n\t\tnet_set_todo(dev);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tif (!list_empty(head)) {\n\t\trollback_registered_many(head);\n\t\tlist_for_each_entry(dev, head, unreg_list)\n\t\t\tnet_set_todo(dev);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\tdev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint dev_change_net_namespace(struct net_device *dev, struct net *net, const char *pat)\n{\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\terr = -EINVAL;\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(dev_net(dev), net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (__dev_get_by_name(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\tif (dev_get_valid_name(dev, pat, 1))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\terr = -ENODEV;\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t   this device. They should clean all the things.\n\n\t   Note that dev->reg_state stays at NETREG_REGISTERED.\n\t   This is wanted because this way 8021q and macvlan know\n\t   the device is just moving and can keep their slaves up.\n\t*/\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER_BATCH, dev);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\n\t/* If there is an ifindex conflict assign a new one */\n\tif (__dev_get_by_index(net, dev->ifindex)) {\n\t\tint iflink = (dev->iflink == dev->ifindex);\n\t\tdev->ifindex = dev_new_index(net);\n\t\tif (iflink)\n\t\t\tdev->iflink = dev->ifindex;\n\t}\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dev_change_net_namespace);\n\nstatic int dev_cpu_callback(struct notifier_block *nfb,\n\t\t\t    unsigned long action,\n\t\t\t    void *ocpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu, oldcpu = (unsigned long)ocpu;\n\tstruct softnet_data *sd, *oldsd;\n\n\tif (action != CPU_DEAD && action != CPU_DEAD_FROZEN)\n\t\treturn NOTIFY_OK;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = __skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn NOTIFY_OK;\n}\n\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nunsigned long netdev_increment_features(unsigned long all, unsigned long one,\n\t\t\t\t\tunsigned long mask)\n{\n\t/* If device needs checksumming, downgrade to it. */\n\tif (all & NETIF_F_NO_CSUM && !(one & NETIF_F_NO_CSUM))\n\t\tall ^= NETIF_F_NO_CSUM | (one & NETIF_F_ALL_CSUM);\n\telse if (mask & NETIF_F_ALL_CSUM) {\n\t\t/* If one device supports v4/v6 checksumming, set for all. */\n\t\tif (one & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM) &&\n\t\t    !(all & NETIF_F_GEN_CSUM)) {\n\t\t\tall &= ~NETIF_F_ALL_CSUM;\n\t\t\tall |= one & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);\n\t\t}\n\n\t\t/* If one device supports hw checksumming, set for all. */\n\t\tif (one & NETIF_F_GEN_CSUM && !(all & NETIF_F_GEN_CSUM)) {\n\t\t\tall &= ~NETIF_F_ALL_CSUM;\n\t\t\tall |= NETIF_F_HW_CSUM;\n\t\t}\n\t}\n\n\tone |= NETIF_F_ALL_CSUM;\n\n\tone |= all & NETIF_F_ONE_FOR_ALL;\n\tall &= one | NETIF_F_LLTX | NETIF_F_GSO | NETIF_F_UFO;\n\tall |= one & mask & NETIF_F_ONE_FOR_ALL;\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head *netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc(sizeof(*hash) * NETDEV_HASHENTRIES, GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\t@buffer: buffer for resulting name\n *\t@len: size of buffer\n *\n *\tDetermine network driver for device.\n */\nchar *netdev_drivername(const struct net_device *dev, char *buffer, int len)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\n\tif (len <= 0 || !buffer)\n\t\treturn buffer;\n\tbuffer[0] = 0;\n\n\tparent = dev->dev.parent;\n\n\tif (!parent)\n\t\treturn buffer;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\tstrlcpy(buffer, driver->name, len);\n\treturn buffer;\n}\n\nstatic int __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t   struct va_format *vaf)\n{\n\tint r;\n\n\tif (dev && dev->dev.parent)\n\t\tr = dev_printk(level, dev->dev.parent, \"%s: %pV\",\n\t\t\t       netdev_name(dev), vaf);\n\telse if (dev)\n\t\tr = printk(\"%s%s: %pV\", level, netdev_name(dev), vaf);\n\telse\n\t\tr = printk(\"%s(NULL net_device): %pV\", level, vaf);\n\n\treturn r;\n}\n\nint netdev_printk(const char *level, const struct net_device *dev,\n\t\t  const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\tint r;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\tr = __netdev_printk(level, dev, &vaf);\n\tva_end(args);\n\n\treturn r;\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nint func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tint r;\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tr = __netdev_printk(level, dev, &vaf);\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\treturn r;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\trtnl_lock();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops)\n\t\t\tcontinue;\n\n\t\t/* Push remaing network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tprintk(KERN_EMERG \"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t__func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n\trtnl_unlock();\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\trtnl_lock();\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\tlist_del(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit = default_device_exit,\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tmemset(sd, 0, sizeof(*sd));\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n\t\tsd->completion_queue = NULL;\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tsd->csd.func = rps_trigger_softirq;\n\t\tsd->csd.info = sd;\n\t\tsd->csd.flags = 0;\n\t\tsd->cpu = i;\n#endif\n\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t\tsd->backlog.gro_list = NULL;\n\t\tsd->backlog.gro_count = 0;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\thotcpu_notifier(dev_cpu_callback, 0);\n\tdst_init();\n\tdev_mcast_init();\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n\nstatic int __init initialize_hashrnd(void)\n{\n\tget_random_bytes(&hashrnd, sizeof(hashrnd));\n\treturn 0;\n}\n\nlate_initcall_sync(initialize_hashrnd);\n\n", "/*\n *\tLinux NET3:\tGRE over IP protocol decoder.\n *\n *\tAuthors: Alexey Kuznetsov (kuznet@ms2.inr.ac.ru)\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n */\n\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/in.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/if_arp.h>\n#include <linux/mroute.h>\n#include <linux/init.h>\n#include <linux/in6.h>\n#include <linux/inetdevice.h>\n#include <linux/igmp.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/etherdevice.h>\n#include <linux/if_ether.h>\n\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/protocol.h>\n#include <net/ipip.h>\n#include <net/arp.h>\n#include <net/checksum.h>\n#include <net/dsfield.h>\n#include <net/inet_ecn.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n#include <net/rtnetlink.h>\n#include <net/gre.h>\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n#include <net/ipv6.h>\n#include <net/ip6_fib.h>\n#include <net/ip6_route.h>\n#endif\n\n/*\n   Problems & solutions\n   --------------------\n\n   1. The most important issue is detecting local dead loops.\n   They would cause complete host lockup in transmit, which\n   would be \"resolved\" by stack overflow or, if queueing is enabled,\n   with infinite looping in net_bh.\n\n   We cannot track such dead loops during route installation,\n   it is infeasible task. The most general solutions would be\n   to keep skb->encapsulation counter (sort of local ttl),\n   and silently drop packet when it expires. It is a good\n   solution, but it supposes maintaing new variable in ALL\n   skb, even if no tunneling is used.\n\n   Current solution: xmit_recursion breaks dead loops. This is a percpu\n   counter, since when we enter the first ndo_xmit(), cpu migration is\n   forbidden. We force an exit if this counter reaches RECURSION_LIMIT\n\n   2. Networking dead loops would not kill routers, but would really\n   kill network. IP hop limit plays role of \"t->recursion\" in this case,\n   if we copy it from packet being encapsulated to upper header.\n   It is very good solution, but it introduces two problems:\n\n   - Routing protocols, using packets with ttl=1 (OSPF, RIP2),\n     do not work over tunnels.\n   - traceroute does not work. I planned to relay ICMP from tunnel,\n     so that this problem would be solved and traceroute output\n     would even more informative. This idea appeared to be wrong:\n     only Linux complies to rfc1812 now (yes, guys, Linux is the only\n     true router now :-)), all routers (at least, in neighbourhood of mine)\n     return only 8 bytes of payload. It is the end.\n\n   Hence, if we want that OSPF worked or traceroute said something reasonable,\n   we should search for another solution.\n\n   One of them is to parse packet trying to detect inner encapsulation\n   made by our node. It is difficult or even impossible, especially,\n   taking into account fragmentation. TO be short, tt is not solution at all.\n\n   Current solution: The solution was UNEXPECTEDLY SIMPLE.\n   We force DF flag on tunnels with preconfigured hop limit,\n   that is ALL. :-) Well, it does not remove the problem completely,\n   but exponential growth of network traffic is changed to linear\n   (branches, that exceed pmtu are pruned) and tunnel mtu\n   fastly degrades to value <68, where looping stops.\n   Yes, it is not good if there exists a router in the loop,\n   which does not force DF, even when encapsulating packets have DF set.\n   But it is not our problem! Nobody could accuse us, we made\n   all that we could make. Even if it is your gated who injected\n   fatal route to network, even if it were you who configured\n   fatal static route: you are innocent. :-)\n\n\n\n   3. Really, ipv4/ipip.c, ipv4/ip_gre.c and ipv6/sit.c contain\n   practically identical code. It would be good to glue them\n   together, but it is not very evident, how to make them modular.\n   sit is integral part of IPv6, ipip and gre are naturally modular.\n   We could extract common parts (hash table, ioctl etc)\n   to a separate module (ip_tunnel.c).\n\n   Alexey Kuznetsov.\n */\n\nstatic struct rtnl_link_ops ipgre_link_ops __read_mostly;\nstatic int ipgre_tunnel_init(struct net_device *dev);\nstatic void ipgre_tunnel_setup(struct net_device *dev);\nstatic int ipgre_tunnel_bind_dev(struct net_device *dev);\n\n/* Fallback tunnel: no source, no destination, no key, no options */\n\n#define HASH_SIZE  16\n\nstatic int ipgre_net_id __read_mostly;\nstruct ipgre_net {\n\tstruct ip_tunnel __rcu *tunnels[4][HASH_SIZE];\n\n\tstruct net_device *fb_tunnel_dev;\n};\n\n/* Tunnel hash table */\n\n/*\n   4 hash tables:\n\n   3: (remote,local)\n   2: (remote,*)\n   1: (*,local)\n   0: (*,*)\n\n   We require exact key match i.e. if a key is present in packet\n   it will match only tunnel with the same key; if it is not present,\n   it will match only keyless tunnel.\n\n   All keysless packets, if not matched configured keyless tunnels\n   will match fallback tunnel.\n */\n\n#define HASH(addr) (((__force u32)addr^((__force u32)addr>>4))&0xF)\n\n#define tunnels_r_l\ttunnels[3]\n#define tunnels_r\ttunnels[2]\n#define tunnels_l\ttunnels[1]\n#define tunnels_wc\ttunnels[0]\n/*\n * Locking : hash tables are protected by RCU and RTNL\n */\n\n#define for_each_ip_tunnel_rcu(start) \\\n\tfor (t = rcu_dereference(start); t; t = rcu_dereference(t->next))\n\n/* often modified stats are per cpu, other are shared (netdev->stats) */\nstruct pcpu_tstats {\n\tunsigned long\trx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_packets;\n\tunsigned long\ttx_bytes;\n};\n\nstatic struct net_device_stats *ipgre_get_stats(struct net_device *dev)\n{\n\tstruct pcpu_tstats sum = { 0 };\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tconst struct pcpu_tstats *tstats = per_cpu_ptr(dev->tstats, i);\n\n\t\tsum.rx_packets += tstats->rx_packets;\n\t\tsum.rx_bytes   += tstats->rx_bytes;\n\t\tsum.tx_packets += tstats->tx_packets;\n\t\tsum.tx_bytes   += tstats->tx_bytes;\n\t}\n\tdev->stats.rx_packets = sum.rx_packets;\n\tdev->stats.rx_bytes   = sum.rx_bytes;\n\tdev->stats.tx_packets = sum.tx_packets;\n\tdev->stats.tx_bytes   = sum.tx_bytes;\n\treturn &dev->stats;\n}\n\n/* Given src, dst and key, find appropriate for input tunnel. */\n\nstatic struct ip_tunnel * ipgre_tunnel_lookup(struct net_device *dev,\n\t\t\t\t\t      __be32 remote, __be32 local,\n\t\t\t\t\t      __be32 key, __be16 gre_proto)\n{\n\tstruct net *net = dev_net(dev);\n\tint link = dev->ifindex;\n\tunsigned int h0 = HASH(remote);\n\tunsigned int h1 = HASH(key);\n\tstruct ip_tunnel *t, *cand = NULL;\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\tint dev_type = (gre_proto == htons(ETH_P_TEB)) ?\n\t\t       ARPHRD_ETHER : ARPHRD_IPGRE;\n\tint score, cand_score = 4;\n\n\tfor_each_ip_tunnel_rcu(ign->tunnels_r_l[h0 ^ h1]) {\n\t\tif (local != t->parms.iph.saddr ||\n\t\t    remote != t->parms.iph.daddr ||\n\t\t    key != t->parms.i_key ||\n\t\t    !(t->dev->flags & IFF_UP))\n\t\t\tcontinue;\n\n\t\tif (t->dev->type != ARPHRD_IPGRE &&\n\t\t    t->dev->type != dev_type)\n\t\t\tcontinue;\n\n\t\tscore = 0;\n\t\tif (t->parms.link != link)\n\t\t\tscore |= 1;\n\t\tif (t->dev->type != dev_type)\n\t\t\tscore |= 2;\n\t\tif (score == 0)\n\t\t\treturn t;\n\n\t\tif (score < cand_score) {\n\t\t\tcand = t;\n\t\t\tcand_score = score;\n\t\t}\n\t}\n\n\tfor_each_ip_tunnel_rcu(ign->tunnels_r[h0 ^ h1]) {\n\t\tif (remote != t->parms.iph.daddr ||\n\t\t    key != t->parms.i_key ||\n\t\t    !(t->dev->flags & IFF_UP))\n\t\t\tcontinue;\n\n\t\tif (t->dev->type != ARPHRD_IPGRE &&\n\t\t    t->dev->type != dev_type)\n\t\t\tcontinue;\n\n\t\tscore = 0;\n\t\tif (t->parms.link != link)\n\t\t\tscore |= 1;\n\t\tif (t->dev->type != dev_type)\n\t\t\tscore |= 2;\n\t\tif (score == 0)\n\t\t\treturn t;\n\n\t\tif (score < cand_score) {\n\t\t\tcand = t;\n\t\t\tcand_score = score;\n\t\t}\n\t}\n\n\tfor_each_ip_tunnel_rcu(ign->tunnels_l[h1]) {\n\t\tif ((local != t->parms.iph.saddr &&\n\t\t     (local != t->parms.iph.daddr ||\n\t\t      !ipv4_is_multicast(local))) ||\n\t\t    key != t->parms.i_key ||\n\t\t    !(t->dev->flags & IFF_UP))\n\t\t\tcontinue;\n\n\t\tif (t->dev->type != ARPHRD_IPGRE &&\n\t\t    t->dev->type != dev_type)\n\t\t\tcontinue;\n\n\t\tscore = 0;\n\t\tif (t->parms.link != link)\n\t\t\tscore |= 1;\n\t\tif (t->dev->type != dev_type)\n\t\t\tscore |= 2;\n\t\tif (score == 0)\n\t\t\treturn t;\n\n\t\tif (score < cand_score) {\n\t\t\tcand = t;\n\t\t\tcand_score = score;\n\t\t}\n\t}\n\n\tfor_each_ip_tunnel_rcu(ign->tunnels_wc[h1]) {\n\t\tif (t->parms.i_key != key ||\n\t\t    !(t->dev->flags & IFF_UP))\n\t\t\tcontinue;\n\n\t\tif (t->dev->type != ARPHRD_IPGRE &&\n\t\t    t->dev->type != dev_type)\n\t\t\tcontinue;\n\n\t\tscore = 0;\n\t\tif (t->parms.link != link)\n\t\t\tscore |= 1;\n\t\tif (t->dev->type != dev_type)\n\t\t\tscore |= 2;\n\t\tif (score == 0)\n\t\t\treturn t;\n\n\t\tif (score < cand_score) {\n\t\t\tcand = t;\n\t\t\tcand_score = score;\n\t\t}\n\t}\n\n\tif (cand != NULL)\n\t\treturn cand;\n\n\tdev = ign->fb_tunnel_dev;\n\tif (dev->flags & IFF_UP)\n\t\treturn netdev_priv(dev);\n\n\treturn NULL;\n}\n\nstatic struct ip_tunnel __rcu **__ipgre_bucket(struct ipgre_net *ign,\n\t\tstruct ip_tunnel_parm *parms)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\t__be32 key = parms->i_key;\n\tunsigned int h = HASH(key);\n\tint prio = 0;\n\n\tif (local)\n\t\tprio |= 1;\n\tif (remote && !ipv4_is_multicast(remote)) {\n\t\tprio |= 2;\n\t\th ^= HASH(remote);\n\t}\n\n\treturn &ign->tunnels[prio][h];\n}\n\nstatic inline struct ip_tunnel __rcu **ipgre_bucket(struct ipgre_net *ign,\n\t\tstruct ip_tunnel *t)\n{\n\treturn __ipgre_bucket(ign, &t->parms);\n}\n\nstatic void ipgre_tunnel_link(struct ipgre_net *ign, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp = ipgre_bucket(ign, t);\n\n\trcu_assign_pointer(t->next, rtnl_dereference(*tp));\n\trcu_assign_pointer(*tp, t);\n}\n\nstatic void ipgre_tunnel_unlink(struct ipgre_net *ign, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp;\n\tstruct ip_tunnel *iter;\n\n\tfor (tp = ipgre_bucket(ign, t);\n\t     (iter = rtnl_dereference(*tp)) != NULL;\n\t     tp = &iter->next) {\n\t\tif (t == iter) {\n\t\t\trcu_assign_pointer(*tp, t->next);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic struct ip_tunnel *ipgre_tunnel_find(struct net *net,\n\t\t\t\t\t   struct ip_tunnel_parm *parms,\n\t\t\t\t\t   int type)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\t__be32 key = parms->i_key;\n\tint link = parms->link;\n\tstruct ip_tunnel *t;\n\tstruct ip_tunnel __rcu **tp;\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\n\tfor (tp = __ipgre_bucket(ign, parms);\n\t     (t = rtnl_dereference(*tp)) != NULL;\n\t     tp = &t->next)\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    remote == t->parms.iph.daddr &&\n\t\t    key == t->parms.i_key &&\n\t\t    link == t->parms.link &&\n\t\t    type == t->dev->type)\n\t\t\tbreak;\n\n\treturn t;\n}\n\nstatic struct ip_tunnel *ipgre_tunnel_locate(struct net *net,\n\t\tstruct ip_tunnel_parm *parms, int create)\n{\n\tstruct ip_tunnel *t, *nt;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\n\tt = ipgre_tunnel_find(net, parms, ARPHRD_IPGRE);\n\tif (t || !create)\n\t\treturn t;\n\n\tif (parms->name[0])\n\t\tstrlcpy(name, parms->name, IFNAMSIZ);\n\telse\n\t\tstrcpy(name, \"gre%d\");\n\n\tdev = alloc_netdev(sizeof(*t), name, ipgre_tunnel_setup);\n\tif (!dev)\n\t\treturn NULL;\n\n\tdev_net_set(dev, net);\n\n\tif (strchr(name, '%')) {\n\t\tif (dev_alloc_name(dev, name) < 0)\n\t\t\tgoto failed_free;\n\t}\n\n\tnt = netdev_priv(dev);\n\tnt->parms = *parms;\n\tdev->rtnl_link_ops = &ipgre_link_ops;\n\n\tdev->mtu = ipgre_tunnel_bind_dev(dev);\n\n\tif (register_netdevice(dev) < 0)\n\t\tgoto failed_free;\n\n\tdev_hold(dev);\n\tipgre_tunnel_link(ign, nt);\n\treturn nt;\n\nfailed_free:\n\tfree_netdev(dev);\n\treturn NULL;\n}\n\nstatic void ipgre_tunnel_uninit(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\n\tipgre_tunnel_unlink(ign, netdev_priv(dev));\n\tdev_put(dev);\n}\n\n\nstatic void ipgre_err(struct sk_buff *skb, u32 info)\n{\n\n/* All the routers (except for Linux) return only\n   8 bytes of packet payload. It means, that precise relaying of\n   ICMP in the real Internet is absolutely infeasible.\n\n   Moreover, Cisco \"wise men\" put GRE key to the third word\n   in GRE header. It makes impossible maintaining even soft state for keyed\n   GRE tunnels with enabled checksum. Tell them \"thank you\".\n\n   Well, I wonder, rfc1812 was written by Cisco employee,\n   what the hell these idiots break standrads established\n   by themself???\n */\n\n\tstruct iphdr *iph = (struct iphdr *)skb->data;\n\t__be16\t     *p = (__be16*)(skb->data+(iph->ihl<<2));\n\tint grehlen = (iph->ihl<<2) + 4;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct ip_tunnel *t;\n\t__be16 flags;\n\n\tflags = p[0];\n\tif (flags&(GRE_CSUM|GRE_KEY|GRE_SEQ|GRE_ROUTING|GRE_VERSION)) {\n\t\tif (flags&(GRE_VERSION|GRE_ROUTING))\n\t\t\treturn;\n\t\tif (flags&GRE_KEY) {\n\t\t\tgrehlen += 4;\n\t\t\tif (flags&GRE_CSUM)\n\t\t\t\tgrehlen += 4;\n\t\t}\n\t}\n\n\t/* If only 8 bytes returned, keyed message will be dropped here */\n\tif (skb_headlen(skb) < grehlen)\n\t\treturn;\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_PARAMETERPROB:\n\t\treturn;\n\n\tcase ICMP_DEST_UNREACH:\n\t\tswitch (code) {\n\t\tcase ICMP_SR_FAILED:\n\t\tcase ICMP_PORT_UNREACH:\n\t\t\t/* Impossible event. */\n\t\t\treturn;\n\t\tcase ICMP_FRAG_NEEDED:\n\t\t\t/* Soft state for pmtu is maintained by IP core. */\n\t\t\treturn;\n\t\tdefault:\n\t\t\t/* All others are translated to HOST_UNREACH.\n\t\t\t   rfc2003 contains \"deep thoughts\" about NET_UNREACH,\n\t\t\t   I believe they are just ether pollution. --ANK\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\tif (code != ICMP_EXC_TTL)\n\t\t\treturn;\n\t\tbreak;\n\t}\n\n\trcu_read_lock();\n\tt = ipgre_tunnel_lookup(skb->dev, iph->daddr, iph->saddr,\n\t\t\t\tflags & GRE_KEY ?\n\t\t\t\t*(((__be32 *)p) + (grehlen / 4) - 1) : 0,\n\t\t\t\tp[1]);\n\tif (t == NULL || t->parms.iph.daddr == 0 ||\n\t    ipv4_is_multicast(t->parms.iph.daddr))\n\t\tgoto out;\n\n\tif (t->parms.iph.ttl == 0 && type == ICMP_TIME_EXCEEDED)\n\t\tgoto out;\n\n\tif (time_before(jiffies, t->err_time + IPTUNNEL_ERR_TIMEO))\n\t\tt->err_count++;\n\telse\n\t\tt->err_count = 1;\n\tt->err_time = jiffies;\nout:\n\trcu_read_unlock();\n}\n\nstatic inline void ipgre_ecn_decapsulate(struct iphdr *iph, struct sk_buff *skb)\n{\n\tif (INET_ECN_is_ce(iph->tos)) {\n\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\tIP_ECN_set_ce(ip_hdr(skb));\n\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tIP6_ECN_set_ce(ipv6_hdr(skb));\n\t\t}\n\t}\n}\n\nstatic inline u8\nipgre_ecn_encapsulate(u8 tos, struct iphdr *old_iph, struct sk_buff *skb)\n{\n\tu8 inner = 0;\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\tinner = old_iph->tos;\n\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\tinner = ipv6_get_dsfield((struct ipv6hdr *)old_iph);\n\treturn INET_ECN_encapsulate(tos, inner);\n}\n\nstatic int ipgre_rcv(struct sk_buff *skb)\n{\n\tstruct iphdr *iph;\n\tu8     *h;\n\t__be16    flags;\n\t__sum16   csum = 0;\n\t__be32 key = 0;\n\tu32    seqno = 0;\n\tstruct ip_tunnel *tunnel;\n\tint    offset = 4;\n\t__be16 gre_proto;\n\n\tif (!pskb_may_pull(skb, 16))\n\t\tgoto drop_nolock;\n\n\tiph = ip_hdr(skb);\n\th = skb->data;\n\tflags = *(__be16*)h;\n\n\tif (flags&(GRE_CSUM|GRE_KEY|GRE_ROUTING|GRE_SEQ|GRE_VERSION)) {\n\t\t/* - Version must be 0.\n\t\t   - We do not support routing headers.\n\t\t */\n\t\tif (flags&(GRE_VERSION|GRE_ROUTING))\n\t\t\tgoto drop_nolock;\n\n\t\tif (flags&GRE_CSUM) {\n\t\t\tswitch (skb->ip_summed) {\n\t\t\tcase CHECKSUM_COMPLETE:\n\t\t\t\tcsum = csum_fold(skb->csum);\n\t\t\t\tif (!csum)\n\t\t\t\t\tbreak;\n\t\t\t\t/* fall through */\n\t\t\tcase CHECKSUM_NONE:\n\t\t\t\tskb->csum = 0;\n\t\t\t\tcsum = __skb_checksum_complete(skb);\n\t\t\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\t\t}\n\t\t\toffset += 4;\n\t\t}\n\t\tif (flags&GRE_KEY) {\n\t\t\tkey = *(__be32*)(h + offset);\n\t\t\toffset += 4;\n\t\t}\n\t\tif (flags&GRE_SEQ) {\n\t\t\tseqno = ntohl(*(__be32*)(h + offset));\n\t\t\toffset += 4;\n\t\t}\n\t}\n\n\tgre_proto = *(__be16 *)(h + 2);\n\n\trcu_read_lock();\n\tif ((tunnel = ipgre_tunnel_lookup(skb->dev,\n\t\t\t\t\t  iph->saddr, iph->daddr, key,\n\t\t\t\t\t  gre_proto))) {\n\t\tstruct pcpu_tstats *tstats;\n\n\t\tsecpath_reset(skb);\n\n\t\tskb->protocol = gre_proto;\n\t\t/* WCCP version 1 and 2 protocol decoding.\n\t\t * - Change protocol to IP\n\t\t * - When dealing with WCCPv2, Skip extra 4 bytes in GRE header\n\t\t */\n\t\tif (flags == 0 && gre_proto == htons(ETH_P_WCCP)) {\n\t\t\tskb->protocol = htons(ETH_P_IP);\n\t\t\tif ((*(h + offset) & 0xF0) != 0x40)\n\t\t\t\toffset += 4;\n\t\t}\n\n\t\tskb->mac_header = skb->network_header;\n\t\t__pskb_pull(skb, offset);\n\t\tskb_postpull_rcsum(skb, skb_transport_header(skb), offset);\n\t\tskb->pkt_type = PACKET_HOST;\n#ifdef CONFIG_NET_IPGRE_BROADCAST\n\t\tif (ipv4_is_multicast(iph->daddr)) {\n\t\t\t/* Looped back packet, drop it! */\n\t\t\tif (rt_is_output_route(skb_rtable(skb)))\n\t\t\t\tgoto drop;\n\t\t\ttunnel->dev->stats.multicast++;\n\t\t\tskb->pkt_type = PACKET_BROADCAST;\n\t\t}\n#endif\n\n\t\tif (((flags&GRE_CSUM) && csum) ||\n\t\t    (!(flags&GRE_CSUM) && tunnel->parms.i_flags&GRE_CSUM)) {\n\t\t\ttunnel->dev->stats.rx_crc_errors++;\n\t\t\ttunnel->dev->stats.rx_errors++;\n\t\t\tgoto drop;\n\t\t}\n\t\tif (tunnel->parms.i_flags&GRE_SEQ) {\n\t\t\tif (!(flags&GRE_SEQ) ||\n\t\t\t    (tunnel->i_seqno && (s32)(seqno - tunnel->i_seqno) < 0)) {\n\t\t\t\ttunnel->dev->stats.rx_fifo_errors++;\n\t\t\t\ttunnel->dev->stats.rx_errors++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\ttunnel->i_seqno = seqno + 1;\n\t\t}\n\n\t\t/* Warning: All skb pointers will be invalidated! */\n\t\tif (tunnel->dev->type == ARPHRD_ETHER) {\n\t\t\tif (!pskb_may_pull(skb, ETH_HLEN)) {\n\t\t\t\ttunnel->dev->stats.rx_length_errors++;\n\t\t\t\ttunnel->dev->stats.rx_errors++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\n\t\t\tiph = ip_hdr(skb);\n\t\t\tskb->protocol = eth_type_trans(skb, tunnel->dev);\n\t\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t\t}\n\n\t\ttstats = this_cpu_ptr(tunnel->dev->tstats);\n\t\ttstats->rx_packets++;\n\t\ttstats->rx_bytes += skb->len;\n\n\t\t__skb_tunnel_rx(skb, tunnel->dev);\n\n\t\tskb_reset_network_header(skb);\n\t\tipgre_ecn_decapsulate(iph, skb);\n\n\t\tnetif_rx(skb);\n\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);\n\ndrop:\n\trcu_read_unlock();\ndrop_nolock:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic netdev_tx_t ipgre_tunnel_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct pcpu_tstats *tstats;\n\tstruct iphdr  *old_iph = ip_hdr(skb);\n\tstruct iphdr  *tiph;\n\tu8     tos;\n\t__be16 df;\n\tstruct rtable *rt;     \t\t\t/* Route to the other host */\n\tstruct net_device *tdev;\t\t/* Device to other host */\n\tstruct iphdr  *iph;\t\t\t/* Our new IP header */\n\tunsigned int max_headroom;\t\t/* The extra header space needed */\n\tint    gre_hlen;\n\t__be32 dst;\n\tint    mtu;\n\n\tif (dev->type == ARPHRD_ETHER)\n\t\tIPCB(skb)->flags = 0;\n\n\tif (dev->header_ops && dev->type == ARPHRD_IPGRE) {\n\t\tgre_hlen = 0;\n\t\ttiph = (struct iphdr *)skb->data;\n\t} else {\n\t\tgre_hlen = tunnel->hlen;\n\t\ttiph = &tunnel->parms.iph;\n\t}\n\n\tif ((dst = tiph->daddr) == 0) {\n\t\t/* NBMA tunnel */\n\n\t\tif (skb_dst(skb) == NULL) {\n\t\t\tdev->stats.tx_fifo_errors++;\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\trt = skb_rtable(skb);\n\t\t\tif ((dst = rt->rt_gateway) == 0)\n\t\t\t\tgoto tx_error_icmp;\n\t\t}\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\telse if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tstruct in6_addr *addr6;\n\t\t\tint addr_type;\n\t\t\tstruct neighbour *neigh = skb_dst(skb)->neighbour;\n\n\t\t\tif (neigh == NULL)\n\t\t\t\tgoto tx_error;\n\n\t\t\taddr6 = (struct in6_addr *)&neigh->primary_key;\n\t\t\taddr_type = ipv6_addr_type(addr6);\n\n\t\t\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t\t\taddr6 = &ipv6_hdr(skb)->daddr;\n\t\t\t\taddr_type = ipv6_addr_type(addr6);\n\t\t\t}\n\n\t\t\tif ((addr_type & IPV6_ADDR_COMPATv4) == 0)\n\t\t\t\tgoto tx_error_icmp;\n\n\t\t\tdst = addr6->s6_addr32[3];\n\t\t}\n#endif\n\t\telse\n\t\t\tgoto tx_error;\n\t}\n\n\ttos = tiph->tos;\n\tif (tos == 1) {\n\t\ttos = 0;\n\t\tif (skb->protocol == htons(ETH_P_IP))\n\t\t\ttos = old_iph->tos;\n\t\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\t\ttos = ipv6_get_dsfield((struct ipv6hdr *)old_iph);\n\t}\n\n\t{\n\t\tstruct flowi fl = {\n\t\t\t.oif = tunnel->parms.link,\n\t\t\t.fl4_dst = dst,\n\t\t\t.fl4_src = tiph->saddr,\n\t\t\t.fl4_tos = RT_TOS(tos),\n\t\t\t.proto = IPPROTO_GRE,\n\t\t\t.fl_gre_key = tunnel->parms.o_key\n\t\t};\n\t\tif (ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\tdev->stats.tx_carrier_errors++;\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n\ttdev = rt->dst.dev;\n\n\tif (tdev == dev) {\n\t\tip_rt_put(rt);\n\t\tdev->stats.collisions++;\n\t\tgoto tx_error;\n\t}\n\n\tdf = tiph->frag_off;\n\tif (df)\n\t\tmtu = dst_mtu(&rt->dst) - dev->hard_header_len - tunnel->hlen;\n\telse\n\t\tmtu = skb_dst(skb) ? dst_mtu(skb_dst(skb)) : dev->mtu;\n\n\tif (skb_dst(skb))\n\t\tskb_dst(skb)->ops->update_pmtu(skb_dst(skb), mtu);\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\tdf |= (old_iph->frag_off&htons(IP_DF));\n\n\t\tif ((old_iph->frag_off&htons(IP_DF)) &&\n\t\t    mtu < ntohs(old_iph->tot_len)) {\n\t\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\telse if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\tstruct rt6_info *rt6 = (struct rt6_info *)skb_dst(skb);\n\n\t\tif (rt6 && mtu < dst_mtu(skb_dst(skb)) && mtu >= IPV6_MIN_MTU) {\n\t\t\tif ((tunnel->parms.iph.daddr &&\n\t\t\t     !ipv4_is_multicast(tunnel->parms.iph.daddr)) ||\n\t\t\t    rt6->rt6i_dst.plen == 128) {\n\t\t\t\trt6->rt6i_flags |= RTF_MODIFIED;\n\t\t\t\tdst_metric_set(skb_dst(skb), RTAX_MTU, mtu);\n\t\t\t}\n\t\t}\n\n\t\tif (mtu >= IPV6_MIN_MTU && mtu < skb->len - tunnel->hlen + gre_hlen) {\n\t\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n#endif\n\n\tif (tunnel->err_count > 0) {\n\t\tif (time_before(jiffies,\n\t\t\t\ttunnel->err_time + IPTUNNEL_ERR_TIMEO)) {\n\t\t\ttunnel->err_count--;\n\n\t\t\tdst_link_failure(skb);\n\t\t} else\n\t\t\ttunnel->err_count = 0;\n\t}\n\n\tmax_headroom = LL_RESERVED_SPACE(tdev) + gre_hlen + rt->dst.header_len;\n\n\tif (skb_headroom(skb) < max_headroom || skb_shared(skb)||\n\t    (skb_cloned(skb) && !skb_clone_writable(skb, 0))) {\n\t\tstruct sk_buff *new_skb = skb_realloc_headroom(skb, max_headroom);\n\t\tif (max_headroom > dev->needed_headroom)\n\t\t\tdev->needed_headroom = max_headroom;\n\t\tif (!new_skb) {\n\t\t\tip_rt_put(rt);\n\t\t\tdev->stats.tx_dropped++;\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(new_skb, skb->sk);\n\t\tdev_kfree_skb(skb);\n\t\tskb = new_skb;\n\t\told_iph = ip_hdr(skb);\n\t}\n\n\tskb_reset_transport_header(skb);\n\tskb_push(skb, gre_hlen);\n\tskb_reset_network_header(skb);\n\tmemset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));\n\tIPCB(skb)->flags &= ~(IPSKB_XFRM_TUNNEL_SIZE | IPSKB_XFRM_TRANSFORMED |\n\t\t\t      IPSKB_REROUTED);\n\tskb_dst_drop(skb);\n\tskb_dst_set(skb, &rt->dst);\n\n\t/*\n\t *\tPush down and install the IPIP header.\n\t */\n\n\tiph \t\t\t=\tip_hdr(skb);\n\tiph->version\t\t=\t4;\n\tiph->ihl\t\t=\tsizeof(struct iphdr) >> 2;\n\tiph->frag_off\t\t=\tdf;\n\tiph->protocol\t\t=\tIPPROTO_GRE;\n\tiph->tos\t\t=\tipgre_ecn_encapsulate(tos, old_iph, skb);\n\tiph->daddr\t\t=\trt->rt_dst;\n\tiph->saddr\t\t=\trt->rt_src;\n\n\tif ((iph->ttl = tiph->ttl) == 0) {\n\t\tif (skb->protocol == htons(ETH_P_IP))\n\t\t\tiph->ttl = old_iph->ttl;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\t\tiph->ttl = ((struct ipv6hdr *)old_iph)->hop_limit;\n#endif\n\t\telse\n\t\t\tiph->ttl = ip4_dst_hoplimit(&rt->dst);\n\t}\n\n\t((__be16 *)(iph + 1))[0] = tunnel->parms.o_flags;\n\t((__be16 *)(iph + 1))[1] = (dev->type == ARPHRD_ETHER) ?\n\t\t\t\t   htons(ETH_P_TEB) : skb->protocol;\n\n\tif (tunnel->parms.o_flags&(GRE_KEY|GRE_CSUM|GRE_SEQ)) {\n\t\t__be32 *ptr = (__be32*)(((u8*)iph) + tunnel->hlen - 4);\n\n\t\tif (tunnel->parms.o_flags&GRE_SEQ) {\n\t\t\t++tunnel->o_seqno;\n\t\t\t*ptr = htonl(tunnel->o_seqno);\n\t\t\tptr--;\n\t\t}\n\t\tif (tunnel->parms.o_flags&GRE_KEY) {\n\t\t\t*ptr = tunnel->parms.o_key;\n\t\t\tptr--;\n\t\t}\n\t\tif (tunnel->parms.o_flags&GRE_CSUM) {\n\t\t\t*ptr = 0;\n\t\t\t*(__sum16*)ptr = ip_compute_csum((void*)(iph+1), skb->len - sizeof(struct iphdr));\n\t\t}\n\t}\n\n\tnf_reset(skb);\n\ttstats = this_cpu_ptr(dev->tstats);\n\t__IPTUNNEL_XMIT(tstats, &dev->stats);\n\treturn NETDEV_TX_OK;\n\ntx_error_icmp:\n\tdst_link_failure(skb);\n\ntx_error:\n\tdev->stats.tx_errors++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic int ipgre_tunnel_bind_dev(struct net_device *dev)\n{\n\tstruct net_device *tdev = NULL;\n\tstruct ip_tunnel *tunnel;\n\tstruct iphdr *iph;\n\tint hlen = LL_MAX_HEADER;\n\tint mtu = ETH_DATA_LEN;\n\tint addend = sizeof(struct iphdr) + 4;\n\n\ttunnel = netdev_priv(dev);\n\tiph = &tunnel->parms.iph;\n\n\t/* Guess output device to choose reasonable mtu and needed_headroom */\n\n\tif (iph->daddr) {\n\t\tstruct flowi fl = {\n\t\t\t.oif = tunnel->parms.link,\n\t\t\t.fl4_dst = iph->daddr,\n\t\t\t.fl4_src = iph->saddr,\n\t\t\t.fl4_tos = RT_TOS(iph->tos),\n\t\t\t.proto = IPPROTO_GRE,\n\t\t\t.fl_gre_key = tunnel->parms.o_key\n\t\t};\n\t\tstruct rtable *rt;\n\n\t\tif (!ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\ttdev = rt->dst.dev;\n\t\t\tip_rt_put(rt);\n\t\t}\n\n\t\tif (dev->type != ARPHRD_ETHER)\n\t\t\tdev->flags |= IFF_POINTOPOINT;\n\t}\n\n\tif (!tdev && tunnel->parms.link)\n\t\ttdev = __dev_get_by_index(dev_net(dev), tunnel->parms.link);\n\n\tif (tdev) {\n\t\thlen = tdev->hard_header_len + tdev->needed_headroom;\n\t\tmtu = tdev->mtu;\n\t}\n\tdev->iflink = tunnel->parms.link;\n\n\t/* Precalculate GRE options length */\n\tif (tunnel->parms.o_flags&(GRE_CSUM|GRE_KEY|GRE_SEQ)) {\n\t\tif (tunnel->parms.o_flags&GRE_CSUM)\n\t\t\taddend += 4;\n\t\tif (tunnel->parms.o_flags&GRE_KEY)\n\t\t\taddend += 4;\n\t\tif (tunnel->parms.o_flags&GRE_SEQ)\n\t\t\taddend += 4;\n\t}\n\tdev->needed_headroom = addend + hlen;\n\tmtu -= dev->hard_header_len + addend;\n\n\tif (mtu < 68)\n\t\tmtu = 68;\n\n\ttunnel->hlen = addend;\n\n\treturn mtu;\n}\n\nstatic int\nipgre_tunnel_ioctl (struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tint err = 0;\n\tstruct ip_tunnel_parm p;\n\tstruct ip_tunnel *t;\n\tstruct net *net = dev_net(dev);\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\n\tswitch (cmd) {\n\tcase SIOCGETTUNNEL:\n\t\tt = NULL;\n\t\tif (dev == ign->fb_tunnel_dev) {\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tt = ipgre_tunnel_locate(net, &p, 0);\n\t\t}\n\t\tif (t == NULL)\n\t\t\tt = netdev_priv(dev);\n\t\tmemcpy(&p, &t->parms, sizeof(p));\n\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &p, sizeof(p)))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase SIOCADDTUNNEL:\n\tcase SIOCCHGTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\tgoto done;\n\n\t\terr = -EINVAL;\n\t\tif (p.iph.version != 4 || p.iph.protocol != IPPROTO_GRE ||\n\t\t    p.iph.ihl != 5 || (p.iph.frag_off&htons(~IP_DF)) ||\n\t\t    ((p.i_flags|p.o_flags)&(GRE_VERSION|GRE_ROUTING)))\n\t\t\tgoto done;\n\t\tif (p.iph.ttl)\n\t\t\tp.iph.frag_off |= htons(IP_DF);\n\n\t\tif (!(p.i_flags&GRE_KEY))\n\t\t\tp.i_key = 0;\n\t\tif (!(p.o_flags&GRE_KEY))\n\t\t\tp.o_key = 0;\n\n\t\tt = ipgre_tunnel_locate(net, &p, cmd == SIOCADDTUNNEL);\n\n\t\tif (dev != ign->fb_tunnel_dev && cmd == SIOCCHGTUNNEL) {\n\t\t\tif (t != NULL) {\n\t\t\t\tif (t->dev != dev) {\n\t\t\t\t\terr = -EEXIST;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tunsigned int nflags = 0;\n\n\t\t\t\tt = netdev_priv(dev);\n\n\t\t\t\tif (ipv4_is_multicast(p.iph.daddr))\n\t\t\t\t\tnflags = IFF_BROADCAST;\n\t\t\t\telse if (p.iph.daddr)\n\t\t\t\t\tnflags = IFF_POINTOPOINT;\n\n\t\t\t\tif ((dev->flags^nflags)&(IFF_POINTOPOINT|IFF_BROADCAST)) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tipgre_tunnel_unlink(ign, t);\n\t\t\t\tsynchronize_net();\n\t\t\t\tt->parms.iph.saddr = p.iph.saddr;\n\t\t\t\tt->parms.iph.daddr = p.iph.daddr;\n\t\t\t\tt->parms.i_key = p.i_key;\n\t\t\t\tt->parms.o_key = p.o_key;\n\t\t\t\tmemcpy(dev->dev_addr, &p.iph.saddr, 4);\n\t\t\t\tmemcpy(dev->broadcast, &p.iph.daddr, 4);\n\t\t\t\tipgre_tunnel_link(ign, t);\n\t\t\t\tnetdev_state_change(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (t) {\n\t\t\terr = 0;\n\t\t\tif (cmd == SIOCCHGTUNNEL) {\n\t\t\t\tt->parms.iph.ttl = p.iph.ttl;\n\t\t\t\tt->parms.iph.tos = p.iph.tos;\n\t\t\t\tt->parms.iph.frag_off = p.iph.frag_off;\n\t\t\t\tif (t->parms.link != p.link) {\n\t\t\t\t\tt->parms.link = p.link;\n\t\t\t\t\tdev->mtu = ipgre_tunnel_bind_dev(dev);\n\t\t\t\t\tnetdev_state_change(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &t->parms, sizeof(p)))\n\t\t\t\terr = -EFAULT;\n\t\t} else\n\t\t\terr = (cmd == SIOCADDTUNNEL ? -ENOBUFS : -ENOENT);\n\t\tbreak;\n\n\tcase SIOCDELTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\tif (dev == ign->fb_tunnel_dev) {\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\t\tgoto done;\n\t\t\terr = -ENOENT;\n\t\t\tif ((t = ipgre_tunnel_locate(net, &p, 0)) == NULL)\n\t\t\t\tgoto done;\n\t\t\terr = -EPERM;\n\t\t\tif (t == netdev_priv(ign->fb_tunnel_dev))\n\t\t\t\tgoto done;\n\t\t\tdev = t->dev;\n\t\t}\n\t\tunregister_netdevice(dev);\n\t\terr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\ndone:\n\treturn err;\n}\n\nstatic int ipgre_tunnel_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tif (new_mtu < 68 ||\n\t    new_mtu > 0xFFF8 - dev->hard_header_len - tunnel->hlen)\n\t\treturn -EINVAL;\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\n/* Nice toy. Unfortunately, useless in real life :-)\n   It allows to construct virtual multiprotocol broadcast \"LAN\"\n   over the Internet, provided multicast routing is tuned.\n\n\n   I have no idea was this bicycle invented before me,\n   so that I had to set ARPHRD_IPGRE to a random value.\n   I have an impression, that Cisco could make something similar,\n   but this feature is apparently missing in IOS<=11.2(8).\n\n   I set up 10.66.66/24 and fec0:6666:6666::0/96 as virtual networks\n   with broadcast 224.66.66.66. If you have access to mbone, play with me :-)\n\n   ping -t 255 224.66.66.66\n\n   If nobody answers, mbone does not work.\n\n   ip tunnel add Universe mode gre remote 224.66.66.66 local <Your_real_addr> ttl 255\n   ip addr add 10.66.66.<somewhat>/24 dev Universe\n   ifconfig Universe up\n   ifconfig Universe add fe80::<Your_real_addr>/10\n   ifconfig Universe add fec0:6666:6666::<Your_real_addr>/96\n   ftp 10.66.66.66\n   ...\n   ftp fec0:6666:6666::193.233.7.65\n   ...\n\n */\n\nstatic int ipgre_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\tunsigned short type,\n\t\t\tconst void *daddr, const void *saddr, unsigned int len)\n{\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\tstruct iphdr *iph = (struct iphdr *)skb_push(skb, t->hlen);\n\t__be16 *p = (__be16*)(iph+1);\n\n\tmemcpy(iph, &t->parms.iph, sizeof(struct iphdr));\n\tp[0]\t\t= t->parms.o_flags;\n\tp[1]\t\t= htons(type);\n\n\t/*\n\t *\tSet the source hardware address.\n\t */\n\n\tif (saddr)\n\t\tmemcpy(&iph->saddr, saddr, 4);\n\tif (daddr)\n\t\tmemcpy(&iph->daddr, daddr, 4);\n\tif (iph->daddr)\n\t\treturn t->hlen;\n\n\treturn -t->hlen;\n}\n\nstatic int ipgre_header_parse(const struct sk_buff *skb, unsigned char *haddr)\n{\n\tstruct iphdr *iph = (struct iphdr *) skb_mac_header(skb);\n\tmemcpy(haddr, &iph->saddr, 4);\n\treturn 4;\n}\n\nstatic const struct header_ops ipgre_header_ops = {\n\t.create\t= ipgre_header,\n\t.parse\t= ipgre_header_parse,\n};\n\n#ifdef CONFIG_NET_IPGRE_BROADCAST\nstatic int ipgre_open(struct net_device *dev)\n{\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\n\tif (ipv4_is_multicast(t->parms.iph.daddr)) {\n\t\tstruct flowi fl = {\n\t\t\t.oif = t->parms.link,\n\t\t\t.fl4_dst = t->parms.iph.daddr,\n\t\t\t.fl4_src = t->parms.iph.saddr,\n\t\t\t.fl4_tos = RT_TOS(t->parms.iph.tos),\n\t\t\t.proto = IPPROTO_GRE,\n\t\t\t.fl_gre_key = t->parms.o_key\n\t\t};\n\t\tstruct rtable *rt;\n\n\t\tif (ip_route_output_key(dev_net(dev), &rt, &fl))\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tdev = rt->dst.dev;\n\t\tip_rt_put(rt);\n\t\tif (__in_dev_get_rtnl(dev) == NULL)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tt->mlink = dev->ifindex;\n\t\tip_mc_inc_group(__in_dev_get_rtnl(dev), t->parms.iph.daddr);\n\t}\n\treturn 0;\n}\n\nstatic int ipgre_close(struct net_device *dev)\n{\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\n\tif (ipv4_is_multicast(t->parms.iph.daddr) && t->mlink) {\n\t\tstruct in_device *in_dev;\n\t\tin_dev = inetdev_by_index(dev_net(dev), t->mlink);\n\t\tif (in_dev)\n\t\t\tip_mc_dec_group(in_dev, t->parms.iph.daddr);\n\t}\n\treturn 0;\n}\n\n#endif\n\nstatic const struct net_device_ops ipgre_netdev_ops = {\n\t.ndo_init\t\t= ipgre_tunnel_init,\n\t.ndo_uninit\t\t= ipgre_tunnel_uninit,\n#ifdef CONFIG_NET_IPGRE_BROADCAST\n\t.ndo_open\t\t= ipgre_open,\n\t.ndo_stop\t\t= ipgre_close,\n#endif\n\t.ndo_start_xmit\t\t= ipgre_tunnel_xmit,\n\t.ndo_do_ioctl\t\t= ipgre_tunnel_ioctl,\n\t.ndo_change_mtu\t\t= ipgre_tunnel_change_mtu,\n\t.ndo_get_stats\t\t= ipgre_get_stats,\n};\n\nstatic void ipgre_dev_free(struct net_device *dev)\n{\n\tfree_percpu(dev->tstats);\n\tfree_netdev(dev);\n}\n\nstatic void ipgre_tunnel_setup(struct net_device *dev)\n{\n\tdev->netdev_ops\t\t= &ipgre_netdev_ops;\n\tdev->destructor \t= ipgre_dev_free;\n\n\tdev->type\t\t= ARPHRD_IPGRE;\n\tdev->needed_headroom \t= LL_MAX_HEADER + sizeof(struct iphdr) + 4;\n\tdev->mtu\t\t= ETH_DATA_LEN - sizeof(struct iphdr) - 4;\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->iflink\t\t= 0;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_NETNS_LOCAL;\n\tdev->priv_flags\t\t&= ~IFF_XMIT_DST_RELEASE;\n}\n\nstatic int ipgre_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel;\n\tstruct iphdr *iph;\n\n\ttunnel = netdev_priv(dev);\n\tiph = &tunnel->parms.iph;\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tmemcpy(dev->dev_addr, &tunnel->parms.iph.saddr, 4);\n\tmemcpy(dev->broadcast, &tunnel->parms.iph.daddr, 4);\n\n\tif (iph->daddr) {\n#ifdef CONFIG_NET_IPGRE_BROADCAST\n\t\tif (ipv4_is_multicast(iph->daddr)) {\n\t\t\tif (!iph->saddr)\n\t\t\t\treturn -EINVAL;\n\t\t\tdev->flags = IFF_BROADCAST;\n\t\t\tdev->header_ops = &ipgre_header_ops;\n\t\t}\n#endif\n\t} else\n\t\tdev->header_ops = &ipgre_header_ops;\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void ipgre_fb_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tiph->version\t\t= 4;\n\tiph->protocol\t\t= IPPROTO_GRE;\n\tiph->ihl\t\t= 5;\n\ttunnel->hlen\t\t= sizeof(struct iphdr) + 4;\n\n\tdev_hold(dev);\n}\n\n\nstatic const struct gre_protocol ipgre_protocol = {\n\t.handler     = ipgre_rcv,\n\t.err_handler = ipgre_err,\n};\n\nstatic void ipgre_destroy_tunnels(struct ipgre_net *ign, struct list_head *head)\n{\n\tint prio;\n\n\tfor (prio = 0; prio < 4; prio++) {\n\t\tint h;\n\t\tfor (h = 0; h < HASH_SIZE; h++) {\n\t\t\tstruct ip_tunnel *t;\n\n\t\t\tt = rtnl_dereference(ign->tunnels[prio][h]);\n\n\t\t\twhile (t != NULL) {\n\t\t\t\tunregister_netdevice_queue(t->dev, head);\n\t\t\t\tt = rtnl_dereference(t->next);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int __net_init ipgre_init_net(struct net *net)\n{\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\tint err;\n\n\tign->fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel), \"gre0\",\n\t\t\t\t\t   ipgre_tunnel_setup);\n\tif (!ign->fb_tunnel_dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_dev;\n\t}\n\tdev_net_set(ign->fb_tunnel_dev, net);\n\n\tipgre_fb_tunnel_init(ign->fb_tunnel_dev);\n\tign->fb_tunnel_dev->rtnl_link_ops = &ipgre_link_ops;\n\n\tif ((err = register_netdev(ign->fb_tunnel_dev)))\n\t\tgoto err_reg_dev;\n\n\trcu_assign_pointer(ign->tunnels_wc[0],\n\t\t\t   netdev_priv(ign->fb_tunnel_dev));\n\treturn 0;\n\nerr_reg_dev:\n\tipgre_dev_free(ign->fb_tunnel_dev);\nerr_alloc_dev:\n\treturn err;\n}\n\nstatic void __net_exit ipgre_exit_net(struct net *net)\n{\n\tstruct ipgre_net *ign;\n\tLIST_HEAD(list);\n\n\tign = net_generic(net, ipgre_net_id);\n\trtnl_lock();\n\tipgre_destroy_tunnels(ign, &list);\n\tunregister_netdevice_many(&list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations ipgre_net_ops = {\n\t.init = ipgre_init_net,\n\t.exit = ipgre_exit_net,\n\t.id   = &ipgre_net_id,\n\t.size = sizeof(struct ipgre_net),\n};\n\nstatic int ipgre_tunnel_validate(struct nlattr *tb[], struct nlattr *data[])\n{\n\t__be16 flags;\n\n\tif (!data)\n\t\treturn 0;\n\n\tflags = 0;\n\tif (data[IFLA_GRE_IFLAGS])\n\t\tflags |= nla_get_be16(data[IFLA_GRE_IFLAGS]);\n\tif (data[IFLA_GRE_OFLAGS])\n\t\tflags |= nla_get_be16(data[IFLA_GRE_OFLAGS]);\n\tif (flags & (GRE_VERSION|GRE_ROUTING))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int ipgre_tap_validate(struct nlattr *tb[], struct nlattr *data[])\n{\n\t__be32 daddr;\n\n\tif (tb[IFLA_ADDRESS]) {\n\t\tif (nla_len(tb[IFLA_ADDRESS]) != ETH_ALEN)\n\t\t\treturn -EINVAL;\n\t\tif (!is_valid_ether_addr(nla_data(tb[IFLA_ADDRESS])))\n\t\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\tif (!data)\n\t\tgoto out;\n\n\tif (data[IFLA_GRE_REMOTE]) {\n\t\tmemcpy(&daddr, nla_data(data[IFLA_GRE_REMOTE]), 4);\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t}\n\nout:\n\treturn ipgre_tunnel_validate(tb, data);\n}\n\nstatic void ipgre_netlink_parms(struct nlattr *data[],\n\t\t\t\tstruct ip_tunnel_parm *parms)\n{\n\tmemset(parms, 0, sizeof(*parms));\n\n\tparms->iph.protocol = IPPROTO_GRE;\n\n\tif (!data)\n\t\treturn;\n\n\tif (data[IFLA_GRE_LINK])\n\t\tparms->link = nla_get_u32(data[IFLA_GRE_LINK]);\n\n\tif (data[IFLA_GRE_IFLAGS])\n\t\tparms->i_flags = nla_get_be16(data[IFLA_GRE_IFLAGS]);\n\n\tif (data[IFLA_GRE_OFLAGS])\n\t\tparms->o_flags = nla_get_be16(data[IFLA_GRE_OFLAGS]);\n\n\tif (data[IFLA_GRE_IKEY])\n\t\tparms->i_key = nla_get_be32(data[IFLA_GRE_IKEY]);\n\n\tif (data[IFLA_GRE_OKEY])\n\t\tparms->o_key = nla_get_be32(data[IFLA_GRE_OKEY]);\n\n\tif (data[IFLA_GRE_LOCAL])\n\t\tparms->iph.saddr = nla_get_be32(data[IFLA_GRE_LOCAL]);\n\n\tif (data[IFLA_GRE_REMOTE])\n\t\tparms->iph.daddr = nla_get_be32(data[IFLA_GRE_REMOTE]);\n\n\tif (data[IFLA_GRE_TTL])\n\t\tparms->iph.ttl = nla_get_u8(data[IFLA_GRE_TTL]);\n\n\tif (data[IFLA_GRE_TOS])\n\t\tparms->iph.tos = nla_get_u8(data[IFLA_GRE_TOS]);\n\n\tif (!data[IFLA_GRE_PMTUDISC] || nla_get_u8(data[IFLA_GRE_PMTUDISC]))\n\t\tparms->iph.frag_off = htons(IP_DF);\n}\n\nstatic int ipgre_tap_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel;\n\n\ttunnel = netdev_priv(dev);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tipgre_tunnel_bind_dev(dev);\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic const struct net_device_ops ipgre_tap_netdev_ops = {\n\t.ndo_init\t\t= ipgre_tap_init,\n\t.ndo_uninit\t\t= ipgre_tunnel_uninit,\n\t.ndo_start_xmit\t\t= ipgre_tunnel_xmit,\n\t.ndo_set_mac_address \t= eth_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_change_mtu\t\t= ipgre_tunnel_change_mtu,\n\t.ndo_get_stats\t\t= ipgre_get_stats,\n};\n\nstatic void ipgre_tap_setup(struct net_device *dev)\n{\n\n\tether_setup(dev);\n\n\tdev->netdev_ops\t\t= &ipgre_tap_netdev_ops;\n\tdev->destructor \t= ipgre_dev_free;\n\n\tdev->iflink\t\t= 0;\n\tdev->features\t\t|= NETIF_F_NETNS_LOCAL;\n}\n\nstatic int ipgre_newlink(struct net *src_net, struct net_device *dev, struct nlattr *tb[],\n\t\t\t struct nlattr *data[])\n{\n\tstruct ip_tunnel *nt;\n\tstruct net *net = dev_net(dev);\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\tint mtu;\n\tint err;\n\n\tnt = netdev_priv(dev);\n\tipgre_netlink_parms(data, &nt->parms);\n\n\tif (ipgre_tunnel_find(net, &nt->parms, dev->type))\n\t\treturn -EEXIST;\n\n\tif (dev->type == ARPHRD_ETHER && !tb[IFLA_ADDRESS])\n\t\trandom_ether_addr(dev->dev_addr);\n\n\tmtu = ipgre_tunnel_bind_dev(dev);\n\tif (!tb[IFLA_MTU])\n\t\tdev->mtu = mtu;\n\n\t/* Can use a lockless transmit, unless we generate output sequences */\n\tif (!(nt->parms.o_flags & GRE_SEQ))\n\t\tdev->features |= NETIF_F_LLTX;\n\n\terr = register_netdevice(dev);\n\tif (err)\n\t\tgoto out;\n\n\tdev_hold(dev);\n\tipgre_tunnel_link(ign, nt);\n\nout:\n\treturn err;\n}\n\nstatic int ipgre_changelink(struct net_device *dev, struct nlattr *tb[],\n\t\t\t    struct nlattr *data[])\n{\n\tstruct ip_tunnel *t, *nt;\n\tstruct net *net = dev_net(dev);\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\tstruct ip_tunnel_parm p;\n\tint mtu;\n\n\tif (dev == ign->fb_tunnel_dev)\n\t\treturn -EINVAL;\n\n\tnt = netdev_priv(dev);\n\tipgre_netlink_parms(data, &p);\n\n\tt = ipgre_tunnel_locate(net, &p, 0);\n\n\tif (t) {\n\t\tif (t->dev != dev)\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tt = nt;\n\n\t\tif (dev->type != ARPHRD_ETHER) {\n\t\t\tunsigned int nflags = 0;\n\n\t\t\tif (ipv4_is_multicast(p.iph.daddr))\n\t\t\t\tnflags = IFF_BROADCAST;\n\t\t\telse if (p.iph.daddr)\n\t\t\t\tnflags = IFF_POINTOPOINT;\n\n\t\t\tif ((dev->flags ^ nflags) &\n\t\t\t    (IFF_POINTOPOINT | IFF_BROADCAST))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tipgre_tunnel_unlink(ign, t);\n\t\tt->parms.iph.saddr = p.iph.saddr;\n\t\tt->parms.iph.daddr = p.iph.daddr;\n\t\tt->parms.i_key = p.i_key;\n\t\tif (dev->type != ARPHRD_ETHER) {\n\t\t\tmemcpy(dev->dev_addr, &p.iph.saddr, 4);\n\t\t\tmemcpy(dev->broadcast, &p.iph.daddr, 4);\n\t\t}\n\t\tipgre_tunnel_link(ign, t);\n\t\tnetdev_state_change(dev);\n\t}\n\n\tt->parms.o_key = p.o_key;\n\tt->parms.iph.ttl = p.iph.ttl;\n\tt->parms.iph.tos = p.iph.tos;\n\tt->parms.iph.frag_off = p.iph.frag_off;\n\n\tif (t->parms.link != p.link) {\n\t\tt->parms.link = p.link;\n\t\tmtu = ipgre_tunnel_bind_dev(dev);\n\t\tif (!tb[IFLA_MTU])\n\t\t\tdev->mtu = mtu;\n\t\tnetdev_state_change(dev);\n\t}\n\n\treturn 0;\n}\n\nstatic size_t ipgre_get_size(const struct net_device *dev)\n{\n\treturn\n\t\t/* IFLA_GRE_LINK */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_IFLAGS */\n\t\tnla_total_size(2) +\n\t\t/* IFLA_GRE_OFLAGS */\n\t\tnla_total_size(2) +\n\t\t/* IFLA_GRE_IKEY */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_OKEY */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_LOCAL */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_REMOTE */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_TTL */\n\t\tnla_total_size(1) +\n\t\t/* IFLA_GRE_TOS */\n\t\tnla_total_size(1) +\n\t\t/* IFLA_GRE_PMTUDISC */\n\t\tnla_total_size(1) +\n\t\t0;\n}\n\nstatic int ipgre_fill_info(struct sk_buff *skb, const struct net_device *dev)\n{\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\tstruct ip_tunnel_parm *p = &t->parms;\n\n\tNLA_PUT_U32(skb, IFLA_GRE_LINK, p->link);\n\tNLA_PUT_BE16(skb, IFLA_GRE_IFLAGS, p->i_flags);\n\tNLA_PUT_BE16(skb, IFLA_GRE_OFLAGS, p->o_flags);\n\tNLA_PUT_BE32(skb, IFLA_GRE_IKEY, p->i_key);\n\tNLA_PUT_BE32(skb, IFLA_GRE_OKEY, p->o_key);\n\tNLA_PUT_BE32(skb, IFLA_GRE_LOCAL, p->iph.saddr);\n\tNLA_PUT_BE32(skb, IFLA_GRE_REMOTE, p->iph.daddr);\n\tNLA_PUT_U8(skb, IFLA_GRE_TTL, p->iph.ttl);\n\tNLA_PUT_U8(skb, IFLA_GRE_TOS, p->iph.tos);\n\tNLA_PUT_U8(skb, IFLA_GRE_PMTUDISC, !!(p->iph.frag_off & htons(IP_DF)));\n\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\n\nstatic const struct nla_policy ipgre_policy[IFLA_GRE_MAX + 1] = {\n\t[IFLA_GRE_LINK]\t\t= { .type = NLA_U32 },\n\t[IFLA_GRE_IFLAGS]\t= { .type = NLA_U16 },\n\t[IFLA_GRE_OFLAGS]\t= { .type = NLA_U16 },\n\t[IFLA_GRE_IKEY]\t\t= { .type = NLA_U32 },\n\t[IFLA_GRE_OKEY]\t\t= { .type = NLA_U32 },\n\t[IFLA_GRE_LOCAL]\t= { .len = FIELD_SIZEOF(struct iphdr, saddr) },\n\t[IFLA_GRE_REMOTE]\t= { .len = FIELD_SIZEOF(struct iphdr, daddr) },\n\t[IFLA_GRE_TTL]\t\t= { .type = NLA_U8 },\n\t[IFLA_GRE_TOS]\t\t= { .type = NLA_U8 },\n\t[IFLA_GRE_PMTUDISC]\t= { .type = NLA_U8 },\n};\n\nstatic struct rtnl_link_ops ipgre_link_ops __read_mostly = {\n\t.kind\t\t= \"gre\",\n\t.maxtype\t= IFLA_GRE_MAX,\n\t.policy\t\t= ipgre_policy,\n\t.priv_size\t= sizeof(struct ip_tunnel),\n\t.setup\t\t= ipgre_tunnel_setup,\n\t.validate\t= ipgre_tunnel_validate,\n\t.newlink\t= ipgre_newlink,\n\t.changelink\t= ipgre_changelink,\n\t.get_size\t= ipgre_get_size,\n\t.fill_info\t= ipgre_fill_info,\n};\n\nstatic struct rtnl_link_ops ipgre_tap_ops __read_mostly = {\n\t.kind\t\t= \"gretap\",\n\t.maxtype\t= IFLA_GRE_MAX,\n\t.policy\t\t= ipgre_policy,\n\t.priv_size\t= sizeof(struct ip_tunnel),\n\t.setup\t\t= ipgre_tap_setup,\n\t.validate\t= ipgre_tap_validate,\n\t.newlink\t= ipgre_newlink,\n\t.changelink\t= ipgre_changelink,\n\t.get_size\t= ipgre_get_size,\n\t.fill_info\t= ipgre_fill_info,\n};\n\n/*\n *\tAnd now the modules code and kernel interface.\n */\n\nstatic int __init ipgre_init(void)\n{\n\tint err;\n\n\tprintk(KERN_INFO \"GRE over IPv4 tunneling driver\\n\");\n\n\terr = register_pernet_device(&ipgre_net_ops);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = gre_add_protocol(&ipgre_protocol, GREPROTO_CISCO);\n\tif (err < 0) {\n\t\tprintk(KERN_INFO \"ipgre init: can't add protocol\\n\");\n\t\tgoto add_proto_failed;\n\t}\n\n\terr = rtnl_link_register(&ipgre_link_ops);\n\tif (err < 0)\n\t\tgoto rtnl_link_failed;\n\n\terr = rtnl_link_register(&ipgre_tap_ops);\n\tif (err < 0)\n\t\tgoto tap_ops_failed;\n\nout:\n\treturn err;\n\ntap_ops_failed:\n\trtnl_link_unregister(&ipgre_link_ops);\nrtnl_link_failed:\n\tgre_del_protocol(&ipgre_protocol, GREPROTO_CISCO);\nadd_proto_failed:\n\tunregister_pernet_device(&ipgre_net_ops);\n\tgoto out;\n}\n\nstatic void __exit ipgre_fini(void)\n{\n\trtnl_link_unregister(&ipgre_tap_ops);\n\trtnl_link_unregister(&ipgre_link_ops);\n\tif (gre_del_protocol(&ipgre_protocol, GREPROTO_CISCO) < 0)\n\t\tprintk(KERN_INFO \"ipgre close: can't remove protocol\\n\");\n\tunregister_pernet_device(&ipgre_net_ops);\n}\n\nmodule_init(ipgre_init);\nmodule_exit(ipgre_fini);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_RTNL_LINK(\"gre\");\nMODULE_ALIAS_RTNL_LINK(\"gretap\");\nMODULE_ALIAS(\"gre0\");\n", "/*\n *\tLinux NET3:\tIP/IP protocol decoder.\n *\n *\tAuthors:\n *\t\tSam Lantinga (slouken@cs.ucdavis.edu)  02/01/95\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tMerged and made usable non modular (its so tiny its silly as\n *\t\t\t\t\ta module taking up 2 pages).\n *\t\tAlan Cox\t: \tFixed bug with 1.3.18 and IPIP not working (now needs to set skb->h.iph)\n *\t\t\t\t\tto keep ip_forward happy.\n *\t\tAlan Cox\t:\tMore fixes for 1.3.21, and firewall fix. Maybe this will work soon 8).\n *\t\tKai Schulte\t:\tFixed #defines for IP_FIREWALL->FIREWALL\n *              David Woodhouse :       Perform some basic ICMP handling.\n *                                      IPIP Routing without decapsulation.\n *              Carlos Picoto   :       GRE over IP support\n *\t\tAlexey Kuznetsov:\tReworked. Really, now it is truncated version of ipv4/ip_gre.c.\n *\t\t\t\t\tI do not want to merge them together.\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n */\n\n/* tunnel.c: an IP tunnel driver\n\n\tThe purpose of this driver is to provide an IP tunnel through\n\twhich you can tunnel network traffic transparently across subnets.\n\n\tThis was written by looking at Nick Holloway's dummy driver\n\tThanks for the great code!\n\n\t\t-Sam Lantinga\t(slouken@cs.ucdavis.edu)  02/01/95\n\n\tMinor tweaks:\n\t\tCleaned up the code a little and added some pre-1.3.0 tweaks.\n\t\tdev->hard_header/hard_header_len changed to use no headers.\n\t\tComments/bracketing tweaked.\n\t\tMade the tunnels use dev->name not tunnel: when error reporting.\n\t\tAdded tx_dropped stat\n\n\t\t-Alan Cox\t(alan@lxorguk.ukuu.org.uk) 21 March 95\n\n\tReworked:\n\t\tChanged to tunnel to destination gateway in addition to the\n\t\t\ttunnel's pointopoint address\n\t\tAlmost completely rewritten\n\t\tNote:  There is currently no firewall or ICMP handling done.\n\n\t\t-Sam Lantinga\t(slouken@cs.ucdavis.edu) 02/13/96\n\n*/\n\n/* Things I wish I had known when writing the tunnel driver:\n\n\tWhen the tunnel_xmit() function is called, the skb contains the\n\tpacket to be sent (plus a great deal of extra info), and dev\n\tcontains the tunnel device that _we_ are.\n\n\tWhen we are passed a packet, we are expected to fill in the\n\tsource address with our source IP address.\n\n\tWhat is the proper way to allocate, copy and free a buffer?\n\tAfter you allocate it, it is a \"0 length\" chunk of memory\n\tstarting at zero.  If you want to add headers to the buffer\n\tlater, you'll have to call \"skb_reserve(skb, amount)\" with\n\tthe amount of memory you want reserved.  Then, you call\n\t\"skb_put(skb, amount)\" with the amount of space you want in\n\tthe buffer.  skb_put() returns a pointer to the top (#0) of\n\tthat buffer.  skb->len is set to the amount of space you have\n\t\"allocated\" with skb_put().  You can then write up to skb->len\n\tbytes to that buffer.  If you need more, you can call skb_put()\n\tagain with the additional amount of space you need.  You can\n\tfind out how much more space you can allocate by calling\n\t\"skb_tailroom(skb)\".\n\tNow, to add header space, call \"skb_push(skb, header_len)\".\n\tThis creates space at the beginning of the buffer and returns\n\ta pointer to this new space.  If later you need to strip a\n\theader from a buffer, call \"skb_pull(skb, header_len)\".\n\tskb_headroom() will return how much space is left at the top\n\tof the buffer (before the main data).  Remember, this headroom\n\tspace must be reserved before the skb_put() function is called.\n\t*/\n\n/*\n   This version of net/ipv4/ipip.c is cloned of net/ipv4/ip_gre.c\n\n   For comments look at net/ipv4/ip_gre.c --ANK\n */\n\n\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/in.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/if_arp.h>\n#include <linux/mroute.h>\n#include <linux/init.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/if_ether.h>\n\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/ipip.h>\n#include <net/inet_ecn.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n#define HASH_SIZE  16\n#define HASH(addr) (((__force u32)addr^((__force u32)addr>>4))&0xF)\n\nstatic int ipip_net_id __read_mostly;\nstruct ipip_net {\n\tstruct ip_tunnel __rcu *tunnels_r_l[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_r[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_l[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_wc[1];\n\tstruct ip_tunnel __rcu **tunnels[4];\n\n\tstruct net_device *fb_tunnel_dev;\n};\n\nstatic int ipip_tunnel_init(struct net_device *dev);\nstatic void ipip_tunnel_setup(struct net_device *dev);\nstatic void ipip_dev_free(struct net_device *dev);\n\n/*\n * Locking : hash tables are protected by RCU and RTNL\n */\n\n#define for_each_ip_tunnel_rcu(start) \\\n\tfor (t = rcu_dereference(start); t; t = rcu_dereference(t->next))\n\n/* often modified stats are per cpu, other are shared (netdev->stats) */\nstruct pcpu_tstats {\n\tunsigned long\trx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_packets;\n\tunsigned long\ttx_bytes;\n};\n\nstatic struct net_device_stats *ipip_get_stats(struct net_device *dev)\n{\n\tstruct pcpu_tstats sum = { 0 };\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tconst struct pcpu_tstats *tstats = per_cpu_ptr(dev->tstats, i);\n\n\t\tsum.rx_packets += tstats->rx_packets;\n\t\tsum.rx_bytes   += tstats->rx_bytes;\n\t\tsum.tx_packets += tstats->tx_packets;\n\t\tsum.tx_bytes   += tstats->tx_bytes;\n\t}\n\tdev->stats.rx_packets = sum.rx_packets;\n\tdev->stats.rx_bytes   = sum.rx_bytes;\n\tdev->stats.tx_packets = sum.tx_packets;\n\tdev->stats.tx_bytes   = sum.tx_bytes;\n\treturn &dev->stats;\n}\n\nstatic struct ip_tunnel * ipip_tunnel_lookup(struct net *net,\n\t\t__be32 remote, __be32 local)\n{\n\tunsigned int h0 = HASH(remote);\n\tunsigned int h1 = HASH(local);\n\tstruct ip_tunnel *t;\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\n\tfor_each_ip_tunnel_rcu(ipn->tunnels_r_l[h0 ^ h1])\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    remote == t->parms.iph.daddr && (t->dev->flags&IFF_UP))\n\t\t\treturn t;\n\n\tfor_each_ip_tunnel_rcu(ipn->tunnels_r[h0])\n\t\tif (remote == t->parms.iph.daddr && (t->dev->flags&IFF_UP))\n\t\t\treturn t;\n\n\tfor_each_ip_tunnel_rcu(ipn->tunnels_l[h1])\n\t\tif (local == t->parms.iph.saddr && (t->dev->flags&IFF_UP))\n\t\t\treturn t;\n\n\tt = rcu_dereference(ipn->tunnels_wc[0]);\n\tif (t && (t->dev->flags&IFF_UP))\n\t\treturn t;\n\treturn NULL;\n}\n\nstatic struct ip_tunnel __rcu **__ipip_bucket(struct ipip_net *ipn,\n\t\tstruct ip_tunnel_parm *parms)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\tunsigned int h = 0;\n\tint prio = 0;\n\n\tif (remote) {\n\t\tprio |= 2;\n\t\th ^= HASH(remote);\n\t}\n\tif (local) {\n\t\tprio |= 1;\n\t\th ^= HASH(local);\n\t}\n\treturn &ipn->tunnels[prio][h];\n}\n\nstatic inline struct ip_tunnel __rcu **ipip_bucket(struct ipip_net *ipn,\n\t\tstruct ip_tunnel *t)\n{\n\treturn __ipip_bucket(ipn, &t->parms);\n}\n\nstatic void ipip_tunnel_unlink(struct ipip_net *ipn, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp;\n\tstruct ip_tunnel *iter;\n\n\tfor (tp = ipip_bucket(ipn, t);\n\t     (iter = rtnl_dereference(*tp)) != NULL;\n\t     tp = &iter->next) {\n\t\tif (t == iter) {\n\t\t\trcu_assign_pointer(*tp, t->next);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void ipip_tunnel_link(struct ipip_net *ipn, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp = ipip_bucket(ipn, t);\n\n\trcu_assign_pointer(t->next, rtnl_dereference(*tp));\n\trcu_assign_pointer(*tp, t);\n}\n\nstatic struct ip_tunnel * ipip_tunnel_locate(struct net *net,\n\t\tstruct ip_tunnel_parm *parms, int create)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\tstruct ip_tunnel *t, *nt;\n\tstruct ip_tunnel __rcu **tp;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\n\tfor (tp = __ipip_bucket(ipn, parms);\n\t\t (t = rtnl_dereference(*tp)) != NULL;\n\t\t tp = &t->next) {\n\t\tif (local == t->parms.iph.saddr && remote == t->parms.iph.daddr)\n\t\t\treturn t;\n\t}\n\tif (!create)\n\t\treturn NULL;\n\n\tif (parms->name[0])\n\t\tstrlcpy(name, parms->name, IFNAMSIZ);\n\telse\n\t\tstrcpy(name, \"tunl%d\");\n\n\tdev = alloc_netdev(sizeof(*t), name, ipip_tunnel_setup);\n\tif (dev == NULL)\n\t\treturn NULL;\n\n\tdev_net_set(dev, net);\n\n\tif (strchr(name, '%')) {\n\t\tif (dev_alloc_name(dev, name) < 0)\n\t\t\tgoto failed_free;\n\t}\n\n\tnt = netdev_priv(dev);\n\tnt->parms = *parms;\n\n\tif (ipip_tunnel_init(dev) < 0)\n\t\tgoto failed_free;\n\n\tif (register_netdevice(dev) < 0)\n\t\tgoto failed_free;\n\n\tdev_hold(dev);\n\tipip_tunnel_link(ipn, nt);\n\treturn nt;\n\nfailed_free:\n\tipip_dev_free(dev);\n\treturn NULL;\n}\n\n/* called with RTNL */\nstatic void ipip_tunnel_uninit(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\n\tif (dev == ipn->fb_tunnel_dev)\n\t\trcu_assign_pointer(ipn->tunnels_wc[0], NULL);\n\telse\n\t\tipip_tunnel_unlink(ipn, netdev_priv(dev));\n\tdev_put(dev);\n}\n\nstatic int ipip_err(struct sk_buff *skb, u32 info)\n{\n\n/* All the routers (except for Linux) return only\n   8 bytes of packet payload. It means, that precise relaying of\n   ICMP in the real Internet is absolutely infeasible.\n */\n\tstruct iphdr *iph = (struct iphdr *)skb->data;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct ip_tunnel *t;\n\tint err;\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_PARAMETERPROB:\n\t\treturn 0;\n\n\tcase ICMP_DEST_UNREACH:\n\t\tswitch (code) {\n\t\tcase ICMP_SR_FAILED:\n\t\tcase ICMP_PORT_UNREACH:\n\t\t\t/* Impossible event. */\n\t\t\treturn 0;\n\t\tcase ICMP_FRAG_NEEDED:\n\t\t\t/* Soft state for pmtu is maintained by IP core. */\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\t/* All others are translated to HOST_UNREACH.\n\t\t\t   rfc2003 contains \"deep thoughts\" about NET_UNREACH,\n\t\t\t   I believe they are just ether pollution. --ANK\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\tif (code != ICMP_EXC_TTL)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\terr = -ENOENT;\n\n\trcu_read_lock();\n\tt = ipip_tunnel_lookup(dev_net(skb->dev), iph->daddr, iph->saddr);\n\tif (t == NULL || t->parms.iph.daddr == 0)\n\t\tgoto out;\n\n\terr = 0;\n\tif (t->parms.iph.ttl == 0 && type == ICMP_TIME_EXCEEDED)\n\t\tgoto out;\n\n\tif (time_before(jiffies, t->err_time + IPTUNNEL_ERR_TIMEO))\n\t\tt->err_count++;\n\telse\n\t\tt->err_count = 1;\n\tt->err_time = jiffies;\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic inline void ipip_ecn_decapsulate(const struct iphdr *outer_iph,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct iphdr *inner_iph = ip_hdr(skb);\n\n\tif (INET_ECN_is_ce(outer_iph->tos))\n\t\tIP_ECN_set_ce(inner_iph);\n}\n\nstatic int ipip_rcv(struct sk_buff *skb)\n{\n\tstruct ip_tunnel *tunnel;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\trcu_read_lock();\n\ttunnel = ipip_tunnel_lookup(dev_net(skb->dev), iph->saddr, iph->daddr);\n\tif (tunnel != NULL) {\n\t\tstruct pcpu_tstats *tstats;\n\n\t\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\t\trcu_read_unlock();\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\tsecpath_reset(skb);\n\n\t\tskb->mac_header = skb->network_header;\n\t\tskb_reset_network_header(skb);\n\t\tskb->protocol = htons(ETH_P_IP);\n\t\tskb->pkt_type = PACKET_HOST;\n\n\t\ttstats = this_cpu_ptr(tunnel->dev->tstats);\n\t\ttstats->rx_packets++;\n\t\ttstats->rx_bytes += skb->len;\n\n\t\t__skb_tunnel_rx(skb, tunnel->dev);\n\n\t\tipip_ecn_decapsulate(iph, skb);\n\n\t\tnetif_rx(skb);\n\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\trcu_read_unlock();\n\n\treturn -1;\n}\n\n/*\n *\tThis function assumes it is being called from dev_queue_xmit()\n *\tand that skb is filled properly by that function.\n */\n\nstatic netdev_tx_t ipip_tunnel_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct pcpu_tstats *tstats;\n\tstruct iphdr  *tiph = &tunnel->parms.iph;\n\tu8     tos = tunnel->parms.iph.tos;\n\t__be16 df = tiph->frag_off;\n\tstruct rtable *rt;     \t\t\t/* Route to the other host */\n\tstruct net_device *tdev;\t\t/* Device to other host */\n\tstruct iphdr  *old_iph = ip_hdr(skb);\n\tstruct iphdr  *iph;\t\t\t/* Our new IP header */\n\tunsigned int max_headroom;\t\t/* The extra header space needed */\n\t__be32 dst = tiph->daddr;\n\tint    mtu;\n\n\tif (skb->protocol != htons(ETH_P_IP))\n\t\tgoto tx_error;\n\n\tif (tos & 1)\n\t\ttos = old_iph->tos;\n\n\tif (!dst) {\n\t\t/* NBMA tunnel */\n\t\tif ((rt = skb_rtable(skb)) == NULL) {\n\t\t\tdev->stats.tx_fifo_errors++;\n\t\t\tgoto tx_error;\n\t\t}\n\t\tif ((dst = rt->rt_gateway) == 0)\n\t\t\tgoto tx_error_icmp;\n\t}\n\n\t{\n\t\tstruct flowi fl = {\n\t\t\t.oif = tunnel->parms.link,\n\t\t\t.fl4_dst = dst,\n\t\t\t.fl4_src= tiph->saddr,\n\t\t\t.fl4_tos = RT_TOS(tos),\n\t\t\t.proto = IPPROTO_IPIP\n\t\t};\n\n\t\tif (ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\tdev->stats.tx_carrier_errors++;\n\t\t\tgoto tx_error_icmp;\n\t\t}\n\t}\n\ttdev = rt->dst.dev;\n\n\tif (tdev == dev) {\n\t\tip_rt_put(rt);\n\t\tdev->stats.collisions++;\n\t\tgoto tx_error;\n\t}\n\n\tdf |= old_iph->frag_off & htons(IP_DF);\n\n\tif (df) {\n\t\tmtu = dst_mtu(&rt->dst) - sizeof(struct iphdr);\n\n\t\tif (mtu < 68) {\n\t\t\tdev->stats.collisions++;\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\tif (skb_dst(skb))\n\t\t\tskb_dst(skb)->ops->update_pmtu(skb_dst(skb), mtu);\n\n\t\tif ((old_iph->frag_off & htons(IP_DF)) &&\n\t\t    mtu < ntohs(old_iph->tot_len)) {\n\t\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED,\n\t\t\t\t  htonl(mtu));\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n\n\tif (tunnel->err_count > 0) {\n\t\tif (time_before(jiffies,\n\t\t\t\ttunnel->err_time + IPTUNNEL_ERR_TIMEO)) {\n\t\t\ttunnel->err_count--;\n\t\t\tdst_link_failure(skb);\n\t\t} else\n\t\t\ttunnel->err_count = 0;\n\t}\n\n\t/*\n\t * Okay, now see if we can stuff it in the buffer as-is.\n\t */\n\tmax_headroom = (LL_RESERVED_SPACE(tdev)+sizeof(struct iphdr));\n\n\tif (skb_headroom(skb) < max_headroom || skb_shared(skb) ||\n\t    (skb_cloned(skb) && !skb_clone_writable(skb, 0))) {\n\t\tstruct sk_buff *new_skb = skb_realloc_headroom(skb, max_headroom);\n\t\tif (!new_skb) {\n\t\t\tip_rt_put(rt);\n\t\t\tdev->stats.tx_dropped++;\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(new_skb, skb->sk);\n\t\tdev_kfree_skb(skb);\n\t\tskb = new_skb;\n\t\told_iph = ip_hdr(skb);\n\t}\n\n\tskb->transport_header = skb->network_header;\n\tskb_push(skb, sizeof(struct iphdr));\n\tskb_reset_network_header(skb);\n\tmemset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));\n\tIPCB(skb)->flags &= ~(IPSKB_XFRM_TUNNEL_SIZE | IPSKB_XFRM_TRANSFORMED |\n\t\t\t      IPSKB_REROUTED);\n\tskb_dst_drop(skb);\n\tskb_dst_set(skb, &rt->dst);\n\n\t/*\n\t *\tPush down and install the IPIP header.\n\t */\n\n\tiph \t\t\t=\tip_hdr(skb);\n\tiph->version\t\t=\t4;\n\tiph->ihl\t\t=\tsizeof(struct iphdr)>>2;\n\tiph->frag_off\t\t=\tdf;\n\tiph->protocol\t\t=\tIPPROTO_IPIP;\n\tiph->tos\t\t=\tINET_ECN_encapsulate(tos, old_iph->tos);\n\tiph->daddr\t\t=\trt->rt_dst;\n\tiph->saddr\t\t=\trt->rt_src;\n\n\tif ((iph->ttl = tiph->ttl) == 0)\n\t\tiph->ttl\t=\told_iph->ttl;\n\n\tnf_reset(skb);\n\ttstats = this_cpu_ptr(dev->tstats);\n\t__IPTUNNEL_XMIT(tstats, &dev->stats);\n\treturn NETDEV_TX_OK;\n\ntx_error_icmp:\n\tdst_link_failure(skb);\ntx_error:\n\tdev->stats.tx_errors++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void ipip_tunnel_bind_dev(struct net_device *dev)\n{\n\tstruct net_device *tdev = NULL;\n\tstruct ip_tunnel *tunnel;\n\tstruct iphdr *iph;\n\n\ttunnel = netdev_priv(dev);\n\tiph = &tunnel->parms.iph;\n\n\tif (iph->daddr) {\n\t\tstruct flowi fl = {\n\t\t\t.oif = tunnel->parms.link,\n\t\t\t.fl4_dst = iph->daddr,\n\t\t\t.fl4_src = iph->saddr,\n\t\t\t.fl4_tos = RT_TOS(iph->tos),\n\t\t\t.proto = IPPROTO_IPIP\n\t\t};\n\t\tstruct rtable *rt;\n\n\t\tif (!ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\ttdev = rt->dst.dev;\n\t\t\tip_rt_put(rt);\n\t\t}\n\t\tdev->flags |= IFF_POINTOPOINT;\n\t}\n\n\tif (!tdev && tunnel->parms.link)\n\t\ttdev = __dev_get_by_index(dev_net(dev), tunnel->parms.link);\n\n\tif (tdev) {\n\t\tdev->hard_header_len = tdev->hard_header_len + sizeof(struct iphdr);\n\t\tdev->mtu = tdev->mtu - sizeof(struct iphdr);\n\t}\n\tdev->iflink = tunnel->parms.link;\n}\n\nstatic int\nipip_tunnel_ioctl (struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tint err = 0;\n\tstruct ip_tunnel_parm p;\n\tstruct ip_tunnel *t;\n\tstruct net *net = dev_net(dev);\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\n\tswitch (cmd) {\n\tcase SIOCGETTUNNEL:\n\t\tt = NULL;\n\t\tif (dev == ipn->fb_tunnel_dev) {\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tt = ipip_tunnel_locate(net, &p, 0);\n\t\t}\n\t\tif (t == NULL)\n\t\t\tt = netdev_priv(dev);\n\t\tmemcpy(&p, &t->parms, sizeof(p));\n\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &p, sizeof(p)))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase SIOCADDTUNNEL:\n\tcase SIOCCHGTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\tgoto done;\n\n\t\terr = -EINVAL;\n\t\tif (p.iph.version != 4 || p.iph.protocol != IPPROTO_IPIP ||\n\t\t    p.iph.ihl != 5 || (p.iph.frag_off&htons(~IP_DF)))\n\t\t\tgoto done;\n\t\tif (p.iph.ttl)\n\t\t\tp.iph.frag_off |= htons(IP_DF);\n\n\t\tt = ipip_tunnel_locate(net, &p, cmd == SIOCADDTUNNEL);\n\n\t\tif (dev != ipn->fb_tunnel_dev && cmd == SIOCCHGTUNNEL) {\n\t\t\tif (t != NULL) {\n\t\t\t\tif (t->dev != dev) {\n\t\t\t\t\terr = -EEXIST;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (((dev->flags&IFF_POINTOPOINT) && !p.iph.daddr) ||\n\t\t\t\t    (!(dev->flags&IFF_POINTOPOINT) && p.iph.daddr)) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tt = netdev_priv(dev);\n\t\t\t\tipip_tunnel_unlink(ipn, t);\n\t\t\t\tsynchronize_net();\n\t\t\t\tt->parms.iph.saddr = p.iph.saddr;\n\t\t\t\tt->parms.iph.daddr = p.iph.daddr;\n\t\t\t\tmemcpy(dev->dev_addr, &p.iph.saddr, 4);\n\t\t\t\tmemcpy(dev->broadcast, &p.iph.daddr, 4);\n\t\t\t\tipip_tunnel_link(ipn, t);\n\t\t\t\tnetdev_state_change(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (t) {\n\t\t\terr = 0;\n\t\t\tif (cmd == SIOCCHGTUNNEL) {\n\t\t\t\tt->parms.iph.ttl = p.iph.ttl;\n\t\t\t\tt->parms.iph.tos = p.iph.tos;\n\t\t\t\tt->parms.iph.frag_off = p.iph.frag_off;\n\t\t\t\tif (t->parms.link != p.link) {\n\t\t\t\t\tt->parms.link = p.link;\n\t\t\t\t\tipip_tunnel_bind_dev(dev);\n\t\t\t\t\tnetdev_state_change(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &t->parms, sizeof(p)))\n\t\t\t\terr = -EFAULT;\n\t\t} else\n\t\t\terr = (cmd == SIOCADDTUNNEL ? -ENOBUFS : -ENOENT);\n\t\tbreak;\n\n\tcase SIOCDELTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\tif (dev == ipn->fb_tunnel_dev) {\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\t\tgoto done;\n\t\t\terr = -ENOENT;\n\t\t\tif ((t = ipip_tunnel_locate(net, &p, 0)) == NULL)\n\t\t\t\tgoto done;\n\t\t\terr = -EPERM;\n\t\t\tif (t->dev == ipn->fb_tunnel_dev)\n\t\t\t\tgoto done;\n\t\t\tdev = t->dev;\n\t\t}\n\t\tunregister_netdevice(dev);\n\t\terr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\ndone:\n\treturn err;\n}\n\nstatic int ipip_tunnel_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tif (new_mtu < 68 || new_mtu > 0xFFF8 - sizeof(struct iphdr))\n\t\treturn -EINVAL;\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\nstatic const struct net_device_ops ipip_netdev_ops = {\n\t.ndo_uninit\t= ipip_tunnel_uninit,\n\t.ndo_start_xmit\t= ipip_tunnel_xmit,\n\t.ndo_do_ioctl\t= ipip_tunnel_ioctl,\n\t.ndo_change_mtu\t= ipip_tunnel_change_mtu,\n\t.ndo_get_stats  = ipip_get_stats,\n};\n\nstatic void ipip_dev_free(struct net_device *dev)\n{\n\tfree_percpu(dev->tstats);\n\tfree_netdev(dev);\n}\n\nstatic void ipip_tunnel_setup(struct net_device *dev)\n{\n\tdev->netdev_ops\t\t= &ipip_netdev_ops;\n\tdev->destructor\t\t= ipip_dev_free;\n\n\tdev->type\t\t= ARPHRD_TUNNEL;\n\tdev->hard_header_len \t= LL_MAX_HEADER + sizeof(struct iphdr);\n\tdev->mtu\t\t= ETH_DATA_LEN - sizeof(struct iphdr);\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->iflink\t\t= 0;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_NETNS_LOCAL;\n\tdev->features\t\t|= NETIF_F_LLTX;\n\tdev->priv_flags\t\t&= ~IFF_XMIT_DST_RELEASE;\n}\n\nstatic int ipip_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tmemcpy(dev->dev_addr, &tunnel->parms.iph.saddr, 4);\n\tmemcpy(dev->broadcast, &tunnel->parms.iph.daddr, 4);\n\n\tipip_tunnel_bind_dev(dev);\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int __net_init ipip_fb_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\tstruct ipip_net *ipn = net_generic(dev_net(dev), ipip_net_id);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tiph->version\t\t= 4;\n\tiph->protocol\t\t= IPPROTO_IPIP;\n\tiph->ihl\t\t= 5;\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\tdev_hold(dev);\n\trcu_assign_pointer(ipn->tunnels_wc[0], tunnel);\n\treturn 0;\n}\n\nstatic struct xfrm_tunnel ipip_handler __read_mostly = {\n\t.handler\t=\tipip_rcv,\n\t.err_handler\t=\tipip_err,\n\t.priority\t=\t1,\n};\n\nstatic const char banner[] __initconst =\n\tKERN_INFO \"IPv4 over IPv4 tunneling driver\\n\";\n\nstatic void ipip_destroy_tunnels(struct ipip_net *ipn, struct list_head *head)\n{\n\tint prio;\n\n\tfor (prio = 1; prio < 4; prio++) {\n\t\tint h;\n\t\tfor (h = 0; h < HASH_SIZE; h++) {\n\t\t\tstruct ip_tunnel *t;\n\n\t\t\tt = rtnl_dereference(ipn->tunnels[prio][h]);\n\t\t\twhile (t != NULL) {\n\t\t\t\tunregister_netdevice_queue(t->dev, head);\n\t\t\t\tt = rtnl_dereference(t->next);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int __net_init ipip_init_net(struct net *net)\n{\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\tint err;\n\n\tipn->tunnels[0] = ipn->tunnels_wc;\n\tipn->tunnels[1] = ipn->tunnels_l;\n\tipn->tunnels[2] = ipn->tunnels_r;\n\tipn->tunnels[3] = ipn->tunnels_r_l;\n\n\tipn->fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel),\n\t\t\t\t\t   \"tunl0\",\n\t\t\t\t\t   ipip_tunnel_setup);\n\tif (!ipn->fb_tunnel_dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_dev;\n\t}\n\tdev_net_set(ipn->fb_tunnel_dev, net);\n\n\terr = ipip_fb_tunnel_init(ipn->fb_tunnel_dev);\n\tif (err)\n\t\tgoto err_reg_dev;\n\n\tif ((err = register_netdev(ipn->fb_tunnel_dev)))\n\t\tgoto err_reg_dev;\n\n\treturn 0;\n\nerr_reg_dev:\n\tipip_dev_free(ipn->fb_tunnel_dev);\nerr_alloc_dev:\n\t/* nothing */\n\treturn err;\n}\n\nstatic void __net_exit ipip_exit_net(struct net *net)\n{\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\tLIST_HEAD(list);\n\n\trtnl_lock();\n\tipip_destroy_tunnels(ipn, &list);\n\tunregister_netdevice_queue(ipn->fb_tunnel_dev, &list);\n\tunregister_netdevice_many(&list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations ipip_net_ops = {\n\t.init = ipip_init_net,\n\t.exit = ipip_exit_net,\n\t.id   = &ipip_net_id,\n\t.size = sizeof(struct ipip_net),\n};\n\nstatic int __init ipip_init(void)\n{\n\tint err;\n\n\tprintk(banner);\n\n\terr = register_pernet_device(&ipip_net_ops);\n\tif (err < 0)\n\t\treturn err;\n\terr = xfrm4_tunnel_register(&ipip_handler, AF_INET);\n\tif (err < 0) {\n\t\tunregister_pernet_device(&ipip_net_ops);\n\t\tprintk(KERN_INFO \"ipip init: can't register tunnel\\n\");\n\t}\n\treturn err;\n}\n\nstatic void __exit ipip_fini(void)\n{\n\tif (xfrm4_tunnel_deregister(&ipip_handler, AF_INET))\n\t\tprintk(KERN_INFO \"ipip close: can't deregister tunnel\\n\");\n\n\tunregister_pernet_device(&ipip_net_ops);\n}\n\nmodule_init(ipip_init);\nmodule_exit(ipip_fini);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"tunl0\");\n", "/*\n *\tIPv6 over IPv4 tunnel device - Simple Internet Transition (SIT)\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\tAlexey Kuznetsov\t<kuznet@ms2.inr.ac.ru>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n *\n *\tChanges:\n * Roger Venning <r.venning@telstra.com>:\t6to4 support\n * Nate Thompson <nate@thebog.net>:\t\t6to4 support\n * Fred Templin <fred.l.templin@boeing.com>:\tisatap support\n */\n\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/icmp.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/init.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/if_ether.h>\n\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_fib.h>\n#include <net/ip6_route.h>\n#include <net/ndisc.h>\n#include <net/addrconf.h>\n#include <net/ip.h>\n#include <net/udp.h>\n#include <net/icmp.h>\n#include <net/ipip.h>\n#include <net/inet_ecn.h>\n#include <net/xfrm.h>\n#include <net/dsfield.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n/*\n   This version of net/ipv6/sit.c is cloned of net/ipv4/ip_gre.c\n\n   For comments look at net/ipv4/ip_gre.c --ANK\n */\n\n#define HASH_SIZE  16\n#define HASH(addr) (((__force u32)addr^((__force u32)addr>>4))&0xF)\n\nstatic int ipip6_tunnel_init(struct net_device *dev);\nstatic void ipip6_tunnel_setup(struct net_device *dev);\nstatic void ipip6_dev_free(struct net_device *dev);\n\nstatic int sit_net_id __read_mostly;\nstruct sit_net {\n\tstruct ip_tunnel __rcu *tunnels_r_l[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_r[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_l[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_wc[1];\n\tstruct ip_tunnel __rcu **tunnels[4];\n\n\tstruct net_device *fb_tunnel_dev;\n};\n\n/*\n * Locking : hash tables are protected by RCU and RTNL\n */\n\n#define for_each_ip_tunnel_rcu(start) \\\n\tfor (t = rcu_dereference(start); t; t = rcu_dereference(t->next))\n\n/* often modified stats are per cpu, other are shared (netdev->stats) */\nstruct pcpu_tstats {\n\tunsigned long\trx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_packets;\n\tunsigned long\ttx_bytes;\n};\n\nstatic struct net_device_stats *ipip6_get_stats(struct net_device *dev)\n{\n\tstruct pcpu_tstats sum = { 0 };\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tconst struct pcpu_tstats *tstats = per_cpu_ptr(dev->tstats, i);\n\n\t\tsum.rx_packets += tstats->rx_packets;\n\t\tsum.rx_bytes   += tstats->rx_bytes;\n\t\tsum.tx_packets += tstats->tx_packets;\n\t\tsum.tx_bytes   += tstats->tx_bytes;\n\t}\n\tdev->stats.rx_packets = sum.rx_packets;\n\tdev->stats.rx_bytes   = sum.rx_bytes;\n\tdev->stats.tx_packets = sum.tx_packets;\n\tdev->stats.tx_bytes   = sum.tx_bytes;\n\treturn &dev->stats;\n}\n/*\n * Must be invoked with rcu_read_lock\n */\nstatic struct ip_tunnel * ipip6_tunnel_lookup(struct net *net,\n\t\tstruct net_device *dev, __be32 remote, __be32 local)\n{\n\tunsigned int h0 = HASH(remote);\n\tunsigned int h1 = HASH(local);\n\tstruct ip_tunnel *t;\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\n\tfor_each_ip_tunnel_rcu(sitn->tunnels_r_l[h0 ^ h1]) {\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    remote == t->parms.iph.daddr &&\n\t\t    (!dev || !t->parms.link || dev->iflink == t->parms.link) &&\n\t\t    (t->dev->flags & IFF_UP))\n\t\t\treturn t;\n\t}\n\tfor_each_ip_tunnel_rcu(sitn->tunnels_r[h0]) {\n\t\tif (remote == t->parms.iph.daddr &&\n\t\t    (!dev || !t->parms.link || dev->iflink == t->parms.link) &&\n\t\t    (t->dev->flags & IFF_UP))\n\t\t\treturn t;\n\t}\n\tfor_each_ip_tunnel_rcu(sitn->tunnels_l[h1]) {\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    (!dev || !t->parms.link || dev->iflink == t->parms.link) &&\n\t\t    (t->dev->flags & IFF_UP))\n\t\t\treturn t;\n\t}\n\tt = rcu_dereference(sitn->tunnels_wc[0]);\n\tif ((t != NULL) && (t->dev->flags & IFF_UP))\n\t\treturn t;\n\treturn NULL;\n}\n\nstatic struct ip_tunnel __rcu **__ipip6_bucket(struct sit_net *sitn,\n\t\tstruct ip_tunnel_parm *parms)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\tunsigned int h = 0;\n\tint prio = 0;\n\n\tif (remote) {\n\t\tprio |= 2;\n\t\th ^= HASH(remote);\n\t}\n\tif (local) {\n\t\tprio |= 1;\n\t\th ^= HASH(local);\n\t}\n\treturn &sitn->tunnels[prio][h];\n}\n\nstatic inline struct ip_tunnel __rcu **ipip6_bucket(struct sit_net *sitn,\n\t\tstruct ip_tunnel *t)\n{\n\treturn __ipip6_bucket(sitn, &t->parms);\n}\n\nstatic void ipip6_tunnel_unlink(struct sit_net *sitn, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp;\n\tstruct ip_tunnel *iter;\n\n\tfor (tp = ipip6_bucket(sitn, t);\n\t     (iter = rtnl_dereference(*tp)) != NULL;\n\t     tp = &iter->next) {\n\t\tif (t == iter) {\n\t\t\trcu_assign_pointer(*tp, t->next);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void ipip6_tunnel_link(struct sit_net *sitn, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp = ipip6_bucket(sitn, t);\n\n\trcu_assign_pointer(t->next, rtnl_dereference(*tp));\n\trcu_assign_pointer(*tp, t);\n}\n\nstatic void ipip6_tunnel_clone_6rd(struct net_device *dev, struct sit_net *sitn)\n{\n#ifdef CONFIG_IPV6_SIT_6RD\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\n\tif (t->dev == sitn->fb_tunnel_dev) {\n\t\tipv6_addr_set(&t->ip6rd.prefix, htonl(0x20020000), 0, 0, 0);\n\t\tt->ip6rd.relay_prefix = 0;\n\t\tt->ip6rd.prefixlen = 16;\n\t\tt->ip6rd.relay_prefixlen = 0;\n\t} else {\n\t\tstruct ip_tunnel *t0 = netdev_priv(sitn->fb_tunnel_dev);\n\t\tmemcpy(&t->ip6rd, &t0->ip6rd, sizeof(t->ip6rd));\n\t}\n#endif\n}\n\nstatic struct ip_tunnel *ipip6_tunnel_locate(struct net *net,\n\t\tstruct ip_tunnel_parm *parms, int create)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\tstruct ip_tunnel *t, *nt;\n\tstruct ip_tunnel __rcu **tp;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\n\tfor (tp = __ipip6_bucket(sitn, parms);\n\t    (t = rtnl_dereference(*tp)) != NULL;\n\t     tp = &t->next) {\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    remote == t->parms.iph.daddr &&\n\t\t    parms->link == t->parms.link) {\n\t\t\tif (create)\n\t\t\t\treturn NULL;\n\t\t\telse\n\t\t\t\treturn t;\n\t\t}\n\t}\n\tif (!create)\n\t\tgoto failed;\n\n\tif (parms->name[0])\n\t\tstrlcpy(name, parms->name, IFNAMSIZ);\n\telse\n\t\tstrcpy(name, \"sit%d\");\n\n\tdev = alloc_netdev(sizeof(*t), name, ipip6_tunnel_setup);\n\tif (dev == NULL)\n\t\treturn NULL;\n\n\tdev_net_set(dev, net);\n\n\tif (strchr(name, '%')) {\n\t\tif (dev_alloc_name(dev, name) < 0)\n\t\t\tgoto failed_free;\n\t}\n\n\tnt = netdev_priv(dev);\n\n\tnt->parms = *parms;\n\tif (ipip6_tunnel_init(dev) < 0)\n\t\tgoto failed_free;\n\tipip6_tunnel_clone_6rd(dev, sitn);\n\n\tif (parms->i_flags & SIT_ISATAP)\n\t\tdev->priv_flags |= IFF_ISATAP;\n\n\tif (register_netdevice(dev) < 0)\n\t\tgoto failed_free;\n\n\tdev_hold(dev);\n\n\tipip6_tunnel_link(sitn, nt);\n\treturn nt;\n\nfailed_free:\n\tipip6_dev_free(dev);\nfailed:\n\treturn NULL;\n}\n\n#define for_each_prl_rcu(start)\t\t\t\\\n\tfor (prl = rcu_dereference(start);\t\\\n\t     prl;\t\t\t\t\\\n\t     prl = rcu_dereference(prl->next))\n\nstatic struct ip_tunnel_prl_entry *\n__ipip6_tunnel_locate_prl(struct ip_tunnel *t, __be32 addr)\n{\n\tstruct ip_tunnel_prl_entry *prl;\n\n\tfor_each_prl_rcu(t->prl)\n\t\tif (prl->addr == addr)\n\t\t\tbreak;\n\treturn prl;\n\n}\n\nstatic int ipip6_tunnel_get_prl(struct ip_tunnel *t,\n\t\t\t\tstruct ip_tunnel_prl __user *a)\n{\n\tstruct ip_tunnel_prl kprl, *kp;\n\tstruct ip_tunnel_prl_entry *prl;\n\tunsigned int cmax, c = 0, ca, len;\n\tint ret = 0;\n\n\tif (copy_from_user(&kprl, a, sizeof(kprl)))\n\t\treturn -EFAULT;\n\tcmax = kprl.datalen / sizeof(kprl);\n\tif (cmax > 1 && kprl.addr != htonl(INADDR_ANY))\n\t\tcmax = 1;\n\n\t/* For simple GET or for root users,\n\t * we try harder to allocate.\n\t */\n\tkp = (cmax <= 1 || capable(CAP_NET_ADMIN)) ?\n\t\tkcalloc(cmax, sizeof(*kp), GFP_KERNEL) :\n\t\tNULL;\n\n\trcu_read_lock();\n\n\tca = t->prl_count < cmax ? t->prl_count : cmax;\n\n\tif (!kp) {\n\t\t/* We don't try hard to allocate much memory for\n\t\t * non-root users.\n\t\t * For root users, retry allocating enough memory for\n\t\t * the answer.\n\t\t */\n\t\tkp = kcalloc(ca, sizeof(*kp), GFP_ATOMIC);\n\t\tif (!kp) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tc = 0;\n\tfor_each_prl_rcu(t->prl) {\n\t\tif (c >= cmax)\n\t\t\tbreak;\n\t\tif (kprl.addr != htonl(INADDR_ANY) && prl->addr != kprl.addr)\n\t\t\tcontinue;\n\t\tkp[c].addr = prl->addr;\n\t\tkp[c].flags = prl->flags;\n\t\tc++;\n\t\tif (kprl.addr != htonl(INADDR_ANY))\n\t\t\tbreak;\n\t}\nout:\n\trcu_read_unlock();\n\n\tlen = sizeof(*kp) * c;\n\tret = 0;\n\tif ((len && copy_to_user(a + 1, kp, len)) || put_user(len, &a->datalen))\n\t\tret = -EFAULT;\n\n\tkfree(kp);\n\n\treturn ret;\n}\n\nstatic int\nipip6_tunnel_add_prl(struct ip_tunnel *t, struct ip_tunnel_prl *a, int chg)\n{\n\tstruct ip_tunnel_prl_entry *p;\n\tint err = 0;\n\n\tif (a->addr == htonl(INADDR_ANY))\n\t\treturn -EINVAL;\n\n\tASSERT_RTNL();\n\n\tfor (p = rtnl_dereference(t->prl); p; p = rtnl_dereference(p->next)) {\n\t\tif (p->addr == a->addr) {\n\t\t\tif (chg) {\n\t\t\t\tp->flags = a->flags;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\terr = -EEXIST;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (chg) {\n\t\terr = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tp = kzalloc(sizeof(struct ip_tunnel_prl_entry), GFP_KERNEL);\n\tif (!p) {\n\t\terr = -ENOBUFS;\n\t\tgoto out;\n\t}\n\n\tp->next = t->prl;\n\tp->addr = a->addr;\n\tp->flags = a->flags;\n\tt->prl_count++;\n\trcu_assign_pointer(t->prl, p);\nout:\n\treturn err;\n}\n\nstatic void prl_entry_destroy_rcu(struct rcu_head *head)\n{\n\tkfree(container_of(head, struct ip_tunnel_prl_entry, rcu_head));\n}\n\nstatic void prl_list_destroy_rcu(struct rcu_head *head)\n{\n\tstruct ip_tunnel_prl_entry *p, *n;\n\n\tp = container_of(head, struct ip_tunnel_prl_entry, rcu_head);\n\tdo {\n\t\tn = p->next;\n\t\tkfree(p);\n\t\tp = n;\n\t} while (p);\n}\n\nstatic int\nipip6_tunnel_del_prl(struct ip_tunnel *t, struct ip_tunnel_prl *a)\n{\n\tstruct ip_tunnel_prl_entry *x, **p;\n\tint err = 0;\n\n\tASSERT_RTNL();\n\n\tif (a && a->addr != htonl(INADDR_ANY)) {\n\t\tfor (p = &t->prl; *p; p = &(*p)->next) {\n\t\t\tif ((*p)->addr == a->addr) {\n\t\t\t\tx = *p;\n\t\t\t\t*p = x->next;\n\t\t\t\tcall_rcu(&x->rcu_head, prl_entry_destroy_rcu);\n\t\t\t\tt->prl_count--;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = -ENXIO;\n\t} else {\n\t\tif (t->prl) {\n\t\t\tt->prl_count = 0;\n\t\t\tx = t->prl;\n\t\t\tcall_rcu(&x->rcu_head, prl_list_destroy_rcu);\n\t\t\tt->prl = NULL;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n\nstatic int\nisatap_chksrc(struct sk_buff *skb, struct iphdr *iph, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel_prl_entry *p;\n\tint ok = 1;\n\n\trcu_read_lock();\n\tp = __ipip6_tunnel_locate_prl(t, iph->saddr);\n\tif (p) {\n\t\tif (p->flags & PRL_DEFAULT)\n\t\t\tskb->ndisc_nodetype = NDISC_NODETYPE_DEFAULT;\n\t\telse\n\t\t\tskb->ndisc_nodetype = NDISC_NODETYPE_NODEFAULT;\n\t} else {\n\t\tstruct in6_addr *addr6 = &ipv6_hdr(skb)->saddr;\n\t\tif (ipv6_addr_is_isatap(addr6) &&\n\t\t    (addr6->s6_addr32[3] == iph->saddr) &&\n\t\t    ipv6_chk_prefix(addr6, t->dev))\n\t\t\tskb->ndisc_nodetype = NDISC_NODETYPE_HOST;\n\t\telse\n\t\t\tok = 0;\n\t}\n\trcu_read_unlock();\n\treturn ok;\n}\n\nstatic void ipip6_tunnel_uninit(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\n\tif (dev == sitn->fb_tunnel_dev) {\n\t\trcu_assign_pointer(sitn->tunnels_wc[0], NULL);\n\t} else {\n\t\tipip6_tunnel_unlink(sitn, netdev_priv(dev));\n\t\tipip6_tunnel_del_prl(netdev_priv(dev), NULL);\n\t}\n\tdev_put(dev);\n}\n\n\nstatic int ipip6_err(struct sk_buff *skb, u32 info)\n{\n\n/* All the routers (except for Linux) return only\n   8 bytes of packet payload. It means, that precise relaying of\n   ICMP in the real Internet is absolutely infeasible.\n */\n\tstruct iphdr *iph = (struct iphdr*)skb->data;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct ip_tunnel *t;\n\tint err;\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_PARAMETERPROB:\n\t\treturn 0;\n\n\tcase ICMP_DEST_UNREACH:\n\t\tswitch (code) {\n\t\tcase ICMP_SR_FAILED:\n\t\tcase ICMP_PORT_UNREACH:\n\t\t\t/* Impossible event. */\n\t\t\treturn 0;\n\t\tcase ICMP_FRAG_NEEDED:\n\t\t\t/* Soft state for pmtu is maintained by IP core. */\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\t/* All others are translated to HOST_UNREACH.\n\t\t\t   rfc2003 contains \"deep thoughts\" about NET_UNREACH,\n\t\t\t   I believe they are just ether pollution. --ANK\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\tif (code != ICMP_EXC_TTL)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\terr = -ENOENT;\n\n\trcu_read_lock();\n\tt = ipip6_tunnel_lookup(dev_net(skb->dev),\n\t\t\t\tskb->dev,\n\t\t\t\tiph->daddr,\n\t\t\t\tiph->saddr);\n\tif (t == NULL || t->parms.iph.daddr == 0)\n\t\tgoto out;\n\n\terr = 0;\n\tif (t->parms.iph.ttl == 0 && type == ICMP_TIME_EXCEEDED)\n\t\tgoto out;\n\n\tif (time_before(jiffies, t->err_time + IPTUNNEL_ERR_TIMEO))\n\t\tt->err_count++;\n\telse\n\t\tt->err_count = 1;\n\tt->err_time = jiffies;\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic inline void ipip6_ecn_decapsulate(struct iphdr *iph, struct sk_buff *skb)\n{\n\tif (INET_ECN_is_ce(iph->tos))\n\t\tIP6_ECN_set_ce(ipv6_hdr(skb));\n}\n\nstatic int ipip6_rcv(struct sk_buff *skb)\n{\n\tstruct iphdr *iph;\n\tstruct ip_tunnel *tunnel;\n\n\tif (!pskb_may_pull(skb, sizeof(struct ipv6hdr)))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\n\trcu_read_lock();\n\ttunnel = ipip6_tunnel_lookup(dev_net(skb->dev), skb->dev,\n\t\t\t\t     iph->saddr, iph->daddr);\n\tif (tunnel != NULL) {\n\t\tstruct pcpu_tstats *tstats;\n\n\t\tsecpath_reset(skb);\n\t\tskb->mac_header = skb->network_header;\n\t\tskb_reset_network_header(skb);\n\t\tIPCB(skb)->flags = 0;\n\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\tskb->pkt_type = PACKET_HOST;\n\n\t\tif ((tunnel->dev->priv_flags & IFF_ISATAP) &&\n\t\t    !isatap_chksrc(skb, iph, tunnel)) {\n\t\t\ttunnel->dev->stats.rx_errors++;\n\t\t\trcu_read_unlock();\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\ttstats = this_cpu_ptr(tunnel->dev->tstats);\n\t\ttstats->rx_packets++;\n\t\ttstats->rx_bytes += skb->len;\n\n\t\t__skb_tunnel_rx(skb, tunnel->dev);\n\n\t\tipip6_ecn_decapsulate(iph, skb);\n\n\t\tnetif_rx(skb);\n\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\n\t/* no tunnel matched,  let upstream know, ipsec may handle it */\n\trcu_read_unlock();\n\treturn 1;\nout:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\n/*\n * Returns the embedded IPv4 address if the IPv6 address\n * comes from 6rd / 6to4 (RFC 3056) addr space.\n */\nstatic inline\n__be32 try_6rd(struct in6_addr *v6dst, struct ip_tunnel *tunnel)\n{\n\t__be32 dst = 0;\n\n#ifdef CONFIG_IPV6_SIT_6RD\n\tif (ipv6_prefix_equal(v6dst, &tunnel->ip6rd.prefix,\n\t\t\t      tunnel->ip6rd.prefixlen)) {\n\t\tunsigned int pbw0, pbi0;\n\t\tint pbi1;\n\t\tu32 d;\n\n\t\tpbw0 = tunnel->ip6rd.prefixlen >> 5;\n\t\tpbi0 = tunnel->ip6rd.prefixlen & 0x1f;\n\n\t\td = (ntohl(v6dst->s6_addr32[pbw0]) << pbi0) >>\n\t\t    tunnel->ip6rd.relay_prefixlen;\n\n\t\tpbi1 = pbi0 - tunnel->ip6rd.relay_prefixlen;\n\t\tif (pbi1 > 0)\n\t\t\td |= ntohl(v6dst->s6_addr32[pbw0 + 1]) >>\n\t\t\t     (32 - pbi1);\n\n\t\tdst = tunnel->ip6rd.relay_prefix | htonl(d);\n\t}\n#else\n\tif (v6dst->s6_addr16[0] == htons(0x2002)) {\n\t\t/* 6to4 v6 addr has 16 bits prefix, 32 v4addr, 16 SLA, ... */\n\t\tmemcpy(&dst, &v6dst->s6_addr16[1], 4);\n\t}\n#endif\n\treturn dst;\n}\n\n/*\n *\tThis function assumes it is being called from dev_queue_xmit()\n *\tand that skb is filled properly by that function.\n */\n\nstatic netdev_tx_t ipip6_tunnel_xmit(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct pcpu_tstats *tstats;\n\tstruct iphdr  *tiph = &tunnel->parms.iph;\n\tstruct ipv6hdr *iph6 = ipv6_hdr(skb);\n\tu8     tos = tunnel->parms.iph.tos;\n\t__be16 df = tiph->frag_off;\n\tstruct rtable *rt;     \t\t\t/* Route to the other host */\n\tstruct net_device *tdev;\t\t/* Device to other host */\n\tstruct iphdr  *iph;\t\t\t/* Our new IP header */\n\tunsigned int max_headroom;\t\t/* The extra header space needed */\n\t__be32 dst = tiph->daddr;\n\tint    mtu;\n\tstruct in6_addr *addr6;\n\tint addr_type;\n\n\tif (skb->protocol != htons(ETH_P_IPV6))\n\t\tgoto tx_error;\n\n\t/* ISATAP (RFC4214) - must come before 6to4 */\n\tif (dev->priv_flags & IFF_ISATAP) {\n\t\tstruct neighbour *neigh = NULL;\n\n\t\tif (skb_dst(skb))\n\t\t\tneigh = skb_dst(skb)->neighbour;\n\n\t\tif (neigh == NULL) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_DEBUG \"sit: nexthop == NULL\\n\");\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\taddr6 = (struct in6_addr*)&neigh->primary_key;\n\t\taddr_type = ipv6_addr_type(addr6);\n\n\t\tif ((addr_type & IPV6_ADDR_UNICAST) &&\n\t\t     ipv6_addr_is_isatap(addr6))\n\t\t\tdst = addr6->s6_addr32[3];\n\t\telse\n\t\t\tgoto tx_error;\n\t}\n\n\tif (!dst)\n\t\tdst = try_6rd(&iph6->daddr, tunnel);\n\n\tif (!dst) {\n\t\tstruct neighbour *neigh = NULL;\n\n\t\tif (skb_dst(skb))\n\t\t\tneigh = skb_dst(skb)->neighbour;\n\n\t\tif (neigh == NULL) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_DEBUG \"sit: nexthop == NULL\\n\");\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\taddr6 = (struct in6_addr*)&neigh->primary_key;\n\t\taddr_type = ipv6_addr_type(addr6);\n\n\t\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t\taddr6 = &ipv6_hdr(skb)->daddr;\n\t\t\taddr_type = ipv6_addr_type(addr6);\n\t\t}\n\n\t\tif ((addr_type & IPV6_ADDR_COMPATv4) == 0)\n\t\t\tgoto tx_error_icmp;\n\n\t\tdst = addr6->s6_addr32[3];\n\t}\n\n\t{\n\t\tstruct flowi fl = { .fl4_dst = dst,\n\t\t\t\t    .fl4_src = tiph->saddr,\n\t\t\t\t    .fl4_tos = RT_TOS(tos),\n\t\t\t\t    .oif = tunnel->parms.link,\n\t\t\t\t    .proto = IPPROTO_IPV6 };\n\t\tif (ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\tdev->stats.tx_carrier_errors++;\n\t\t\tgoto tx_error_icmp;\n\t\t}\n\t}\n\tif (rt->rt_type != RTN_UNICAST) {\n\t\tip_rt_put(rt);\n\t\tdev->stats.tx_carrier_errors++;\n\t\tgoto tx_error_icmp;\n\t}\n\ttdev = rt->dst.dev;\n\n\tif (tdev == dev) {\n\t\tip_rt_put(rt);\n\t\tdev->stats.collisions++;\n\t\tgoto tx_error;\n\t}\n\n\tif (df) {\n\t\tmtu = dst_mtu(&rt->dst) - sizeof(struct iphdr);\n\n\t\tif (mtu < 68) {\n\t\t\tdev->stats.collisions++;\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\tif (mtu < IPV6_MIN_MTU) {\n\t\t\tmtu = IPV6_MIN_MTU;\n\t\t\tdf = 0;\n\t\t}\n\n\t\tif (tunnel->parms.iph.daddr && skb_dst(skb))\n\t\t\tskb_dst(skb)->ops->update_pmtu(skb_dst(skb), mtu);\n\n\t\tif (skb->len > mtu) {\n\t\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n\n\tif (tunnel->err_count > 0) {\n\t\tif (time_before(jiffies,\n\t\t\t\ttunnel->err_time + IPTUNNEL_ERR_TIMEO)) {\n\t\t\ttunnel->err_count--;\n\t\t\tdst_link_failure(skb);\n\t\t} else\n\t\t\ttunnel->err_count = 0;\n\t}\n\n\t/*\n\t * Okay, now see if we can stuff it in the buffer as-is.\n\t */\n\tmax_headroom = LL_RESERVED_SPACE(tdev)+sizeof(struct iphdr);\n\n\tif (skb_headroom(skb) < max_headroom || skb_shared(skb) ||\n\t    (skb_cloned(skb) && !skb_clone_writable(skb, 0))) {\n\t\tstruct sk_buff *new_skb = skb_realloc_headroom(skb, max_headroom);\n\t\tif (!new_skb) {\n\t\t\tip_rt_put(rt);\n\t\t\tdev->stats.tx_dropped++;\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(new_skb, skb->sk);\n\t\tdev_kfree_skb(skb);\n\t\tskb = new_skb;\n\t\tiph6 = ipv6_hdr(skb);\n\t}\n\n\tskb->transport_header = skb->network_header;\n\tskb_push(skb, sizeof(struct iphdr));\n\tskb_reset_network_header(skb);\n\tmemset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));\n\tIPCB(skb)->flags = 0;\n\tskb_dst_drop(skb);\n\tskb_dst_set(skb, &rt->dst);\n\n\t/*\n\t *\tPush down and install the IPIP header.\n\t */\n\n\tiph \t\t\t=\tip_hdr(skb);\n\tiph->version\t\t=\t4;\n\tiph->ihl\t\t=\tsizeof(struct iphdr)>>2;\n\tiph->frag_off\t\t=\tdf;\n\tiph->protocol\t\t=\tIPPROTO_IPV6;\n\tiph->tos\t\t=\tINET_ECN_encapsulate(tos, ipv6_get_dsfield(iph6));\n\tiph->daddr\t\t=\trt->rt_dst;\n\tiph->saddr\t\t=\trt->rt_src;\n\n\tif ((iph->ttl = tiph->ttl) == 0)\n\t\tiph->ttl\t=\tiph6->hop_limit;\n\n\tnf_reset(skb);\n\ttstats = this_cpu_ptr(dev->tstats);\n\t__IPTUNNEL_XMIT(tstats, &dev->stats);\n\treturn NETDEV_TX_OK;\n\ntx_error_icmp:\n\tdst_link_failure(skb);\ntx_error:\n\tdev->stats.tx_errors++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void ipip6_tunnel_bind_dev(struct net_device *dev)\n{\n\tstruct net_device *tdev = NULL;\n\tstruct ip_tunnel *tunnel;\n\tstruct iphdr *iph;\n\n\ttunnel = netdev_priv(dev);\n\tiph = &tunnel->parms.iph;\n\n\tif (iph->daddr) {\n\t\tstruct flowi fl = { .fl4_dst = iph->daddr,\n\t\t\t\t    .fl4_src = iph->saddr,\n\t\t\t\t    .fl4_tos = RT_TOS(iph->tos),\n\t\t\t\t    .oif = tunnel->parms.link,\n\t\t\t\t    .proto = IPPROTO_IPV6 };\n\t\tstruct rtable *rt;\n\t\tif (!ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\ttdev = rt->dst.dev;\n\t\t\tip_rt_put(rt);\n\t\t}\n\t\tdev->flags |= IFF_POINTOPOINT;\n\t}\n\n\tif (!tdev && tunnel->parms.link)\n\t\ttdev = __dev_get_by_index(dev_net(dev), tunnel->parms.link);\n\n\tif (tdev) {\n\t\tdev->hard_header_len = tdev->hard_header_len + sizeof(struct iphdr);\n\t\tdev->mtu = tdev->mtu - sizeof(struct iphdr);\n\t\tif (dev->mtu < IPV6_MIN_MTU)\n\t\t\tdev->mtu = IPV6_MIN_MTU;\n\t}\n\tdev->iflink = tunnel->parms.link;\n}\n\nstatic int\nipip6_tunnel_ioctl (struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tint err = 0;\n\tstruct ip_tunnel_parm p;\n\tstruct ip_tunnel_prl prl;\n\tstruct ip_tunnel *t;\n\tstruct net *net = dev_net(dev);\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n#ifdef CONFIG_IPV6_SIT_6RD\n\tstruct ip_tunnel_6rd ip6rd;\n#endif\n\n\tswitch (cmd) {\n\tcase SIOCGETTUNNEL:\n#ifdef CONFIG_IPV6_SIT_6RD\n\tcase SIOCGET6RD:\n#endif\n\t\tt = NULL;\n\t\tif (dev == sitn->fb_tunnel_dev) {\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tt = ipip6_tunnel_locate(net, &p, 0);\n\t\t}\n\t\tif (t == NULL)\n\t\t\tt = netdev_priv(dev);\n\n\t\terr = -EFAULT;\n\t\tif (cmd == SIOCGETTUNNEL) {\n\t\t\tmemcpy(&p, &t->parms, sizeof(p));\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &p,\n\t\t\t\t\t sizeof(p)))\n\t\t\t\tgoto done;\n#ifdef CONFIG_IPV6_SIT_6RD\n\t\t} else {\n\t\t\tipv6_addr_copy(&ip6rd.prefix, &t->ip6rd.prefix);\n\t\t\tip6rd.relay_prefix = t->ip6rd.relay_prefix;\n\t\t\tip6rd.prefixlen = t->ip6rd.prefixlen;\n\t\t\tip6rd.relay_prefixlen = t->ip6rd.relay_prefixlen;\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &ip6rd,\n\t\t\t\t\t sizeof(ip6rd)))\n\t\t\t\tgoto done;\n#endif\n\t\t}\n\t\terr = 0;\n\t\tbreak;\n\n\tcase SIOCADDTUNNEL:\n\tcase SIOCCHGTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\tgoto done;\n\n\t\terr = -EINVAL;\n\t\tif (p.iph.version != 4 || p.iph.protocol != IPPROTO_IPV6 ||\n\t\t    p.iph.ihl != 5 || (p.iph.frag_off&htons(~IP_DF)))\n\t\t\tgoto done;\n\t\tif (p.iph.ttl)\n\t\t\tp.iph.frag_off |= htons(IP_DF);\n\n\t\tt = ipip6_tunnel_locate(net, &p, cmd == SIOCADDTUNNEL);\n\n\t\tif (dev != sitn->fb_tunnel_dev && cmd == SIOCCHGTUNNEL) {\n\t\t\tif (t != NULL) {\n\t\t\t\tif (t->dev != dev) {\n\t\t\t\t\terr = -EEXIST;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (((dev->flags&IFF_POINTOPOINT) && !p.iph.daddr) ||\n\t\t\t\t    (!(dev->flags&IFF_POINTOPOINT) && p.iph.daddr)) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tt = netdev_priv(dev);\n\t\t\t\tipip6_tunnel_unlink(sitn, t);\n\t\t\t\tsynchronize_net();\n\t\t\t\tt->parms.iph.saddr = p.iph.saddr;\n\t\t\t\tt->parms.iph.daddr = p.iph.daddr;\n\t\t\t\tmemcpy(dev->dev_addr, &p.iph.saddr, 4);\n\t\t\t\tmemcpy(dev->broadcast, &p.iph.daddr, 4);\n\t\t\t\tipip6_tunnel_link(sitn, t);\n\t\t\t\tnetdev_state_change(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (t) {\n\t\t\terr = 0;\n\t\t\tif (cmd == SIOCCHGTUNNEL) {\n\t\t\t\tt->parms.iph.ttl = p.iph.ttl;\n\t\t\t\tt->parms.iph.tos = p.iph.tos;\n\t\t\t\tif (t->parms.link != p.link) {\n\t\t\t\t\tt->parms.link = p.link;\n\t\t\t\t\tipip6_tunnel_bind_dev(dev);\n\t\t\t\t\tnetdev_state_change(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &t->parms, sizeof(p)))\n\t\t\t\terr = -EFAULT;\n\t\t} else\n\t\t\terr = (cmd == SIOCADDTUNNEL ? -ENOBUFS : -ENOENT);\n\t\tbreak;\n\n\tcase SIOCDELTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\tif (dev == sitn->fb_tunnel_dev) {\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\t\tgoto done;\n\t\t\terr = -ENOENT;\n\t\t\tif ((t = ipip6_tunnel_locate(net, &p, 0)) == NULL)\n\t\t\t\tgoto done;\n\t\t\terr = -EPERM;\n\t\t\tif (t == netdev_priv(sitn->fb_tunnel_dev))\n\t\t\t\tgoto done;\n\t\t\tdev = t->dev;\n\t\t}\n\t\tunregister_netdevice(dev);\n\t\terr = 0;\n\t\tbreak;\n\n\tcase SIOCGETPRL:\n\t\terr = -EINVAL;\n\t\tif (dev == sitn->fb_tunnel_dev)\n\t\t\tgoto done;\n\t\terr = -ENOENT;\n\t\tif (!(t = netdev_priv(dev)))\n\t\t\tgoto done;\n\t\terr = ipip6_tunnel_get_prl(t, ifr->ifr_ifru.ifru_data);\n\t\tbreak;\n\n\tcase SIOCADDPRL:\n\tcase SIOCDELPRL:\n\tcase SIOCCHGPRL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\t\terr = -EINVAL;\n\t\tif (dev == sitn->fb_tunnel_dev)\n\t\t\tgoto done;\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&prl, ifr->ifr_ifru.ifru_data, sizeof(prl)))\n\t\t\tgoto done;\n\t\terr = -ENOENT;\n\t\tif (!(t = netdev_priv(dev)))\n\t\t\tgoto done;\n\n\t\tswitch (cmd) {\n\t\tcase SIOCDELPRL:\n\t\t\terr = ipip6_tunnel_del_prl(t, &prl);\n\t\t\tbreak;\n\t\tcase SIOCADDPRL:\n\t\tcase SIOCCHGPRL:\n\t\t\terr = ipip6_tunnel_add_prl(t, &prl, cmd == SIOCCHGPRL);\n\t\t\tbreak;\n\t\t}\n\t\tnetdev_state_change(dev);\n\t\tbreak;\n\n#ifdef CONFIG_IPV6_SIT_6RD\n\tcase SIOCADD6RD:\n\tcase SIOCCHG6RD:\n\tcase SIOCDEL6RD:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&ip6rd, ifr->ifr_ifru.ifru_data,\n\t\t\t\t   sizeof(ip6rd)))\n\t\t\tgoto done;\n\n\t\tt = netdev_priv(dev);\n\n\t\tif (cmd != SIOCDEL6RD) {\n\t\t\tstruct in6_addr prefix;\n\t\t\t__be32 relay_prefix;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (ip6rd.relay_prefixlen > 32 ||\n\t\t\t    ip6rd.prefixlen + (32 - ip6rd.relay_prefixlen) > 64)\n\t\t\t\tgoto done;\n\n\t\t\tipv6_addr_prefix(&prefix, &ip6rd.prefix,\n\t\t\t\t\t ip6rd.prefixlen);\n\t\t\tif (!ipv6_addr_equal(&prefix, &ip6rd.prefix))\n\t\t\t\tgoto done;\n\t\t\tif (ip6rd.relay_prefixlen)\n\t\t\t\trelay_prefix = ip6rd.relay_prefix &\n\t\t\t\t\t       htonl(0xffffffffUL <<\n\t\t\t\t\t\t     (32 - ip6rd.relay_prefixlen));\n\t\t\telse\n\t\t\t\trelay_prefix = 0;\n\t\t\tif (relay_prefix != ip6rd.relay_prefix)\n\t\t\t\tgoto done;\n\n\t\t\tipv6_addr_copy(&t->ip6rd.prefix, &prefix);\n\t\t\tt->ip6rd.relay_prefix = relay_prefix;\n\t\t\tt->ip6rd.prefixlen = ip6rd.prefixlen;\n\t\t\tt->ip6rd.relay_prefixlen = ip6rd.relay_prefixlen;\n\t\t} else\n\t\t\tipip6_tunnel_clone_6rd(dev, sitn);\n\n\t\terr = 0;\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\ndone:\n\treturn err;\n}\n\nstatic int ipip6_tunnel_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tif (new_mtu < IPV6_MIN_MTU || new_mtu > 0xFFF8 - sizeof(struct iphdr))\n\t\treturn -EINVAL;\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\nstatic const struct net_device_ops ipip6_netdev_ops = {\n\t.ndo_uninit\t= ipip6_tunnel_uninit,\n\t.ndo_start_xmit\t= ipip6_tunnel_xmit,\n\t.ndo_do_ioctl\t= ipip6_tunnel_ioctl,\n\t.ndo_change_mtu\t= ipip6_tunnel_change_mtu,\n\t.ndo_get_stats\t= ipip6_get_stats,\n};\n\nstatic void ipip6_dev_free(struct net_device *dev)\n{\n\tfree_percpu(dev->tstats);\n\tfree_netdev(dev);\n}\n\nstatic void ipip6_tunnel_setup(struct net_device *dev)\n{\n\tdev->netdev_ops\t\t= &ipip6_netdev_ops;\n\tdev->destructor \t= ipip6_dev_free;\n\n\tdev->type\t\t= ARPHRD_SIT;\n\tdev->hard_header_len \t= LL_MAX_HEADER + sizeof(struct iphdr);\n\tdev->mtu\t\t= ETH_DATA_LEN - sizeof(struct iphdr);\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->priv_flags\t       &= ~IFF_XMIT_DST_RELEASE;\n\tdev->iflink\t\t= 0;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_NETNS_LOCAL;\n\tdev->features\t\t|= NETIF_F_LLTX;\n}\n\nstatic int ipip6_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tmemcpy(dev->dev_addr, &tunnel->parms.iph.saddr, 4);\n\tmemcpy(dev->broadcast, &tunnel->parms.iph.daddr, 4);\n\n\tipip6_tunnel_bind_dev(dev);\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int __net_init ipip6_fb_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\tstruct net *net = dev_net(dev);\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tiph->version\t\t= 4;\n\tiph->protocol\t\t= IPPROTO_IPV6;\n\tiph->ihl\t\t= 5;\n\tiph->ttl\t\t= 64;\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\tdev_hold(dev);\n\tsitn->tunnels_wc[0]\t= tunnel;\n\treturn 0;\n}\n\nstatic struct xfrm_tunnel sit_handler __read_mostly = {\n\t.handler\t=\tipip6_rcv,\n\t.err_handler\t=\tipip6_err,\n\t.priority\t=\t1,\n};\n\nstatic void __net_exit sit_destroy_tunnels(struct sit_net *sitn, struct list_head *head)\n{\n\tint prio;\n\n\tfor (prio = 1; prio < 4; prio++) {\n\t\tint h;\n\t\tfor (h = 0; h < HASH_SIZE; h++) {\n\t\t\tstruct ip_tunnel *t = sitn->tunnels[prio][h];\n\n\t\t\twhile (t != NULL) {\n\t\t\t\tunregister_netdevice_queue(t->dev, head);\n\t\t\t\tt = t->next;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int __net_init sit_init_net(struct net *net)\n{\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\tint err;\n\n\tsitn->tunnels[0] = sitn->tunnels_wc;\n\tsitn->tunnels[1] = sitn->tunnels_l;\n\tsitn->tunnels[2] = sitn->tunnels_r;\n\tsitn->tunnels[3] = sitn->tunnels_r_l;\n\n\tsitn->fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel), \"sit0\",\n\t\t\t\t\t   ipip6_tunnel_setup);\n\tif (!sitn->fb_tunnel_dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_dev;\n\t}\n\tdev_net_set(sitn->fb_tunnel_dev, net);\n\n\terr = ipip6_fb_tunnel_init(sitn->fb_tunnel_dev);\n\tif (err)\n\t\tgoto err_dev_free;\n\n\tipip6_tunnel_clone_6rd(sitn->fb_tunnel_dev, sitn);\n\n\tif ((err = register_netdev(sitn->fb_tunnel_dev)))\n\t\tgoto err_reg_dev;\n\n\treturn 0;\n\nerr_reg_dev:\n\tdev_put(sitn->fb_tunnel_dev);\nerr_dev_free:\n\tipip6_dev_free(sitn->fb_tunnel_dev);\nerr_alloc_dev:\n\treturn err;\n}\n\nstatic void __net_exit sit_exit_net(struct net *net)\n{\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\tLIST_HEAD(list);\n\n\trtnl_lock();\n\tsit_destroy_tunnels(sitn, &list);\n\tunregister_netdevice_queue(sitn->fb_tunnel_dev, &list);\n\tunregister_netdevice_many(&list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations sit_net_ops = {\n\t.init = sit_init_net,\n\t.exit = sit_exit_net,\n\t.id   = &sit_net_id,\n\t.size = sizeof(struct sit_net),\n};\n\nstatic void __exit sit_cleanup(void)\n{\n\txfrm4_tunnel_deregister(&sit_handler, AF_INET6);\n\n\tunregister_pernet_device(&sit_net_ops);\n\trcu_barrier(); /* Wait for completion of call_rcu()'s */\n}\n\nstatic int __init sit_init(void)\n{\n\tint err;\n\n\tprintk(KERN_INFO \"IPv6 over IPv4 tunneling driver\\n\");\n\n\terr = register_pernet_device(&sit_net_ops);\n\tif (err < 0)\n\t\treturn err;\n\terr = xfrm4_tunnel_register(&sit_handler, AF_INET6);\n\tif (err < 0) {\n\t\tunregister_pernet_device(&sit_net_ops);\n\t\tprintk(KERN_INFO \"sit init: Can't add protocol\\n\");\n\t}\n\treturn err;\n}\n\nmodule_init(sit_init);\nmodule_exit(sit_cleanup);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"sit0\");\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the Interfaces handler.\n *\n * Version:\t@(#)dev.h\t1.0.10\t08/12/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tDonald J. Becker, <becker@cesdis.gsfc.nasa.gov>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tBjorn Ekwall. <bj0rn@blox.se>\n *              Pekka Riikonen <priikone@poseidon.pspt.fi>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\t\tMoved to /usr/include/linux for NET3\n */\n#ifndef _LINUX_NETDEVICE_H\n#define _LINUX_NETDEVICE_H\n\n#include <linux/if.h>\n#include <linux/if_ether.h>\n#include <linux/if_packet.h>\n#include <linux/if_link.h>\n\n#ifdef __KERNEL__\n#include <linux/pm_qos_params.h>\n#include <linux/timer.h>\n#include <linux/delay.h>\n#include <linux/mm.h>\n#include <asm/atomic.h>\n#include <asm/cache.h>\n#include <asm/byteorder.h>\n\n#include <linux/device.h>\n#include <linux/percpu.h>\n#include <linux/rculist.h>\n#include <linux/dmaengine.h>\n#include <linux/workqueue.h>\n\n#include <linux/ethtool.h>\n#include <net/net_namespace.h>\n#include <net/dsa.h>\n#ifdef CONFIG_DCB\n#include <net/dcbnl.h>\n#endif\n\nstruct vlan_group;\nstruct netpoll_info;\nstruct phy_device;\n/* 802.11 specific */\nstruct wireless_dev;\n\t\t\t\t\t/* source back-compat hooks */\n#define SET_ETHTOOL_OPS(netdev,ops) \\\n\t( (netdev)->ethtool_ops = (ops) )\n\n#define HAVE_ALLOC_NETDEV\t\t/* feature macro: alloc_xxxdev\n\t\t\t\t\t   functions are available. */\n#define HAVE_FREE_NETDEV\t\t/* free_netdev() */\n#define HAVE_NETDEV_PRIV\t\t/* netdev_priv() */\n\n/* hardware address assignment types */\n#define NET_ADDR_PERM\t\t0\t/* address is permanent (default) */\n#define NET_ADDR_RANDOM\t\t1\t/* address is generated randomly */\n#define NET_ADDR_STOLEN\t\t2\t/* address is stolen from other device */\n\n/* Backlog congestion levels */\n#define NET_RX_SUCCESS\t\t0\t/* keep 'em coming, baby */\n#define NET_RX_DROP\t\t1\t/* packet dropped */\n\n/*\n * Transmit return codes: transmit return codes originate from three different\n * namespaces:\n *\n * - qdisc return codes\n * - driver transmit return codes\n * - errno values\n *\n * Drivers are allowed to return any one of those in their hard_start_xmit()\n * function. Real network devices commonly used with qdiscs should only return\n * the driver transmit return codes though - when qdiscs are used, the actual\n * transmission happens asynchronously, so the value is not propagated to\n * higher layers. Virtual network devices transmit synchronously, in this case\n * the driver transmit return codes are consumed by dev_queue_xmit(), all\n * others are propagated to higher layers.\n */\n\n/* qdisc ->enqueue() return codes. */\n#define NET_XMIT_SUCCESS\t0x00\n#define NET_XMIT_DROP\t\t0x01\t/* skb dropped\t\t\t*/\n#define NET_XMIT_CN\t\t0x02\t/* congestion notification\t*/\n#define NET_XMIT_POLICED\t0x03\t/* skb is shot by police\t*/\n#define NET_XMIT_MASK\t\t0x0f\t/* qdisc flags in net/sch_generic.h */\n\n/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It\n * indicates that the device will soon be dropping packets, or already drops\n * some packets of the same priority; prompting us to send less aggressively. */\n#define net_xmit_eval(e)\t((e) == NET_XMIT_CN ? 0 : (e))\n#define net_xmit_errno(e)\t((e) != NET_XMIT_CN ? -ENOBUFS : 0)\n\n/* Driver transmit return codes */\n#define NETDEV_TX_MASK\t\t0xf0\n\nenum netdev_tx {\n\t__NETDEV_TX_MIN\t = INT_MIN,\t/* make sure enum is signed */\n\tNETDEV_TX_OK\t = 0x00,\t/* driver took care of packet */\n\tNETDEV_TX_BUSY\t = 0x10,\t/* driver tx path was busy*/\n\tNETDEV_TX_LOCKED = 0x20,\t/* driver tx lock was already taken */\n};\ntypedef enum netdev_tx netdev_tx_t;\n\n/*\n * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;\n * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.\n */\nstatic inline bool dev_xmit_complete(int rc)\n{\n\t/*\n\t * Positive cases with an skb consumed by a driver:\n\t * - successful transmission (rc == NETDEV_TX_OK)\n\t * - error while transmitting (rc < 0)\n\t * - error while queueing to a different device (rc & NET_XMIT_MASK)\n\t */\n\tif (likely(rc < NET_XMIT_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\n#endif\n\n#define MAX_ADDR_LEN\t32\t\t/* Largest hardware address length */\n\n#ifdef  __KERNEL__\n/*\n *\tCompute the worst case header length according to the protocols\n *\tused.\n */\n\n#if defined(CONFIG_WLAN) || defined(CONFIG_AX25) || defined(CONFIG_AX25_MODULE)\n# if defined(CONFIG_MAC80211_MESH)\n#  define LL_MAX_HEADER 128\n# else\n#  define LL_MAX_HEADER 96\n# endif\n#elif defined(CONFIG_TR) || defined(CONFIG_TR_MODULE)\n# define LL_MAX_HEADER 48\n#else\n# define LL_MAX_HEADER 32\n#endif\n\n#if !defined(CONFIG_NET_IPIP) && !defined(CONFIG_NET_IPIP_MODULE) && \\\n    !defined(CONFIG_NET_IPGRE) &&  !defined(CONFIG_NET_IPGRE_MODULE) && \\\n    !defined(CONFIG_IPV6_SIT) && !defined(CONFIG_IPV6_SIT_MODULE) && \\\n    !defined(CONFIG_IPV6_TUNNEL) && !defined(CONFIG_IPV6_TUNNEL_MODULE)\n#define MAX_HEADER LL_MAX_HEADER\n#else\n#define MAX_HEADER (LL_MAX_HEADER + 48)\n#endif\n\n/*\n *\tOld network device statistics. Fields are native words\n *\t(unsigned long) so they can be read and written atomically.\n */\n\nstruct net_device_stats {\n\tunsigned long\trx_packets;\n\tunsigned long\ttx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_bytes;\n\tunsigned long\trx_errors;\n\tunsigned long\ttx_errors;\n\tunsigned long\trx_dropped;\n\tunsigned long\ttx_dropped;\n\tunsigned long\tmulticast;\n\tunsigned long\tcollisions;\n\tunsigned long\trx_length_errors;\n\tunsigned long\trx_over_errors;\n\tunsigned long\trx_crc_errors;\n\tunsigned long\trx_frame_errors;\n\tunsigned long\trx_fifo_errors;\n\tunsigned long\trx_missed_errors;\n\tunsigned long\ttx_aborted_errors;\n\tunsigned long\ttx_carrier_errors;\n\tunsigned long\ttx_fifo_errors;\n\tunsigned long\ttx_heartbeat_errors;\n\tunsigned long\ttx_window_errors;\n\tunsigned long\trx_compressed;\n\tunsigned long\ttx_compressed;\n};\n\n#endif  /*  __KERNEL__  */\n\n\n/* Media selection options. */\nenum {\n        IF_PORT_UNKNOWN = 0,\n        IF_PORT_10BASE2,\n        IF_PORT_10BASET,\n        IF_PORT_AUI,\n        IF_PORT_100BASET,\n        IF_PORT_100BASETX,\n        IF_PORT_100BASEFX\n};\n\n#ifdef __KERNEL__\n\n#include <linux/cache.h>\n#include <linux/skbuff.h>\n\nstruct neighbour;\nstruct neigh_parms;\nstruct sk_buff;\n\nstruct netdev_hw_addr {\n\tstruct list_head\tlist;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n\tunsigned char\t\ttype;\n#define NETDEV_HW_ADDR_T_LAN\t\t1\n#define NETDEV_HW_ADDR_T_SAN\t\t2\n#define NETDEV_HW_ADDR_T_SLAVE\t\t3\n#define NETDEV_HW_ADDR_T_UNICAST\t4\n#define NETDEV_HW_ADDR_T_MULTICAST\t5\n\tbool\t\t\tsynced;\n\tbool\t\t\tglobal_use;\n\tint\t\t\trefcount;\n\tstruct rcu_head\t\trcu_head;\n};\n\nstruct netdev_hw_addr_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n#define netdev_hw_addr_list_count(l) ((l)->count)\n#define netdev_hw_addr_list_empty(l) (netdev_hw_addr_list_count(l) == 0)\n#define netdev_hw_addr_list_for_each(ha, l) \\\n\tlist_for_each_entry(ha, &(l)->list, list)\n\n#define netdev_uc_count(dev) netdev_hw_addr_list_count(&(dev)->uc)\n#define netdev_uc_empty(dev) netdev_hw_addr_list_empty(&(dev)->uc)\n#define netdev_for_each_uc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->uc)\n\n#define netdev_mc_count(dev) netdev_hw_addr_list_count(&(dev)->mc)\n#define netdev_mc_empty(dev) netdev_hw_addr_list_empty(&(dev)->mc)\n#define netdev_for_each_mc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->mc)\n\nstruct hh_cache {\n\tstruct hh_cache *hh_next;\t/* Next entry\t\t\t     */\n\tatomic_t\thh_refcnt;\t/* number of users                   */\n/*\n * We want hh_output, hh_len, hh_lock and hh_data be a in a separate\n * cache line on SMP.\n * They are mostly read, but hh_refcnt may be changed quite frequently,\n * incurring cache line ping pongs.\n */\n\t__be16\t\thh_type ____cacheline_aligned_in_smp;\n\t\t\t\t\t/* protocol identifier, f.e ETH_P_IP\n                                         *  NOTE:  For VLANs, this will be the\n                                         *  encapuslated type. --BLG\n                                         */\n\tu16\t\thh_len;\t\t/* length of header */\n\tint\t\t(*hh_output)(struct sk_buff *skb);\n\tseqlock_t\thh_lock;\n\n\t/* cached hardware header; allow for machine alignment needs.        */\n#define HH_DATA_MOD\t16\n#define HH_DATA_OFF(__len) \\\n\t(HH_DATA_MOD - (((__len - 1) & (HH_DATA_MOD - 1)) + 1))\n#define HH_DATA_ALIGN(__len) \\\n\t(((__len)+(HH_DATA_MOD-1))&~(HH_DATA_MOD - 1))\n\tunsigned long\thh_data[HH_DATA_ALIGN(LL_MAX_HEADER) / sizeof(long)];\n};\n\nstatic inline void hh_cache_put(struct hh_cache *hh)\n{\n\tif (atomic_dec_and_test(&hh->hh_refcnt))\n\t\tkfree(hh);\n}\n\n/* Reserve HH_DATA_MOD byte aligned hard_header_len, but at least that much.\n * Alternative is:\n *   dev->hard_header_len ? (dev->hard_header_len +\n *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0\n *\n * We could use other alignment values, but we must maintain the\n * relationship HH alignment <= LL alignment.\n *\n * LL_ALLOCATED_SPACE also takes into account the tailroom the device\n * may need.\n */\n#define LL_RESERVED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_RESERVED_SPACE_EXTRA(dev,extra) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(extra))&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_ALLOCATED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(dev)->needed_tailroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n\nstruct header_ops {\n\tint\t(*create) (struct sk_buff *skb, struct net_device *dev,\n\t\t\t   unsigned short type, const void *daddr,\n\t\t\t   const void *saddr, unsigned len);\n\tint\t(*parse)(const struct sk_buff *skb, unsigned char *haddr);\n\tint\t(*rebuild)(struct sk_buff *skb);\n#define HAVE_HEADER_CACHE\n\tint\t(*cache)(const struct neighbour *neigh, struct hh_cache *hh);\n\tvoid\t(*cache_update)(struct hh_cache *hh,\n\t\t\t\tconst struct net_device *dev,\n\t\t\t\tconst unsigned char *haddr);\n};\n\n/* These flag bits are private to the generic network queueing\n * layer, they may not be explicitly referenced by any other\n * code.\n */\n\nenum netdev_state_t {\n\t__LINK_STATE_START,\n\t__LINK_STATE_PRESENT,\n\t__LINK_STATE_NOCARRIER,\n\t__LINK_STATE_LINKWATCH_PENDING,\n\t__LINK_STATE_DORMANT,\n};\n\n\n/*\n * This structure holds at boot time configured netdevice settings. They\n * are then used in the device probing.\n */\nstruct netdev_boot_setup {\n\tchar name[IFNAMSIZ];\n\tstruct ifmap map;\n};\n#define NETDEV_BOOT_SETUP_MAX 8\n\nextern int __init netdev_boot_setup(char *str);\n\n/*\n * Structure for NAPI scheduling similar to tasklet but with weighting\n */\nstruct napi_struct {\n\t/* The poll_list must only be managed by the entity which\n\t * changes the state of the NAPI_STATE_SCHED bit.  This means\n\t * whoever atomically sets that bit can add this napi_struct\n\t * to the per-cpu poll_list, and whoever clears that bit\n\t * can remove from the list right before clearing the bit.\n\t */\n\tstruct list_head\tpoll_list;\n\n\tunsigned long\t\tstate;\n\tint\t\t\tweight;\n\tint\t\t\t(*poll)(struct napi_struct *, int);\n#ifdef CONFIG_NETPOLL\n\tspinlock_t\t\tpoll_lock;\n\tint\t\t\tpoll_owner;\n#endif\n\n\tunsigned int\t\tgro_count;\n\n\tstruct net_device\t*dev;\n\tstruct list_head\tdev_list;\n\tstruct sk_buff\t\t*gro_list;\n\tstruct sk_buff\t\t*skb;\n};\n\nenum {\n\tNAPI_STATE_SCHED,\t/* Poll is scheduled */\n\tNAPI_STATE_DISABLE,\t/* Disable pending */\n\tNAPI_STATE_NPSVC,\t/* Netpoll - don't dequeue from poll_list */\n};\n\nenum gro_result {\n\tGRO_MERGED,\n\tGRO_MERGED_FREE,\n\tGRO_HELD,\n\tGRO_NORMAL,\n\tGRO_DROP,\n};\ntypedef enum gro_result gro_result_t;\n\ntypedef struct sk_buff *rx_handler_func_t(struct sk_buff *skb);\n\nextern void __napi_schedule(struct napi_struct *n);\n\nstatic inline int napi_disable_pending(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nstatic inline int napi_schedule_prep(struct napi_struct *n)\n{\n\treturn !napi_disable_pending(n) &&\n\t\t!test_and_set_bit(NAPI_STATE_SCHED, &n->state);\n}\n\n/**\n *\tnapi_schedule - schedule NAPI poll\n *\t@n: napi context\n *\n * Schedule NAPI poll routine to be called if it is not already\n * running.\n */\nstatic inline void napi_schedule(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule(n);\n}\n\n/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */\nstatic inline int napi_reschedule(struct napi_struct *napi)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\t__napi_schedule(napi);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/**\n *\tnapi_complete - NAPI processing complete\n *\t@n: napi context\n *\n * Mark NAPI processing as complete.\n */\nextern void __napi_complete(struct napi_struct *n);\nextern void napi_complete(struct napi_struct *n);\n\n/**\n *\tnapi_disable - prevent NAPI from scheduling\n *\t@n: napi context\n *\n * Stop NAPI from being scheduled on this context.\n * Waits till any outstanding processing completes.\n */\nstatic inline void napi_disable(struct napi_struct *n)\n{\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\twhile (test_and_set_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: napi context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nstatic inline void napi_enable(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tsmp_mb__before_clear_bit();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n}\n\n#ifdef CONFIG_SMP\n/**\n *\tnapi_synchronize - wait until NAPI is not running\n *\t@n: napi context\n *\n * Wait until NAPI is done being scheduled on this context.\n * Waits till any outstanding processing completes but\n * does not disable future activations.\n */\nstatic inline void napi_synchronize(const struct napi_struct *n)\n{\n\twhile (test_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n}\n#else\n# define napi_synchronize(n)\tbarrier()\n#endif\n\nenum netdev_queue_state_t {\n\t__QUEUE_STATE_XOFF,\n\t__QUEUE_STATE_FROZEN,\n#define QUEUE_STATE_XOFF_OR_FROZEN ((1 << __QUEUE_STATE_XOFF)\t\t| \\\n\t\t\t\t    (1 << __QUEUE_STATE_FROZEN))\n};\n\nstruct netdev_queue {\n/*\n * read mostly part\n */\n\tstruct net_device\t*dev;\n\tstruct Qdisc\t\t*qdisc;\n\tunsigned long\t\tstate;\n\tstruct Qdisc\t\t*qdisc_sleeping;\n#ifdef CONFIG_RPS\n\tstruct kobject\t\tkobj;\n#endif\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tint\t\t\tnuma_node;\n#endif\n/*\n * write mostly part\n */\n\tspinlock_t\t\t_xmit_lock ____cacheline_aligned_in_smp;\n\tint\t\t\txmit_lock_owner;\n\t/*\n\t * please use this field instead of dev->trans_start\n\t */\n\tunsigned long\t\ttrans_start;\n} ____cacheline_aligned_in_smp;\n\nstatic inline int netdev_queue_numa_node_read(const struct netdev_queue *q)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\treturn q->numa_node;\n#else\n\treturn NUMA_NO_NODE;\n#endif\n}\n\nstatic inline void netdev_queue_numa_node_write(struct netdev_queue *q, int node)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tq->numa_node = node;\n#endif\n}\n\n#ifdef CONFIG_RPS\n/*\n * This structure holds an RPS map which can be of variable length.  The\n * map is an array of CPUs.\n */\nstruct rps_map {\n\tunsigned int len;\n\tstruct rcu_head rcu;\n\tu16 cpus[0];\n};\n#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + (_num * sizeof(u16)))\n\n/*\n * The rps_dev_flow structure contains the mapping of a flow to a CPU and the\n * tail pointer for that CPU's input queue at the time of last enqueue.\n */\nstruct rps_dev_flow {\n\tu16 cpu;\n\tu16 fill;\n\tunsigned int last_qtail;\n};\n\n/*\n * The rps_dev_flow_table structure contains a table of flow mappings.\n */\nstruct rps_dev_flow_table {\n\tunsigned int mask;\n\tstruct rcu_head rcu;\n\tstruct work_struct free_work;\n\tstruct rps_dev_flow flows[0];\n};\n#define RPS_DEV_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_dev_flow_table) + \\\n    (_num * sizeof(struct rps_dev_flow)))\n\n/*\n * The rps_sock_flow_table contains mappings of flows to the last CPU\n * on which they were processed by the application (set in recvmsg).\n */\nstruct rps_sock_flow_table {\n\tunsigned int mask;\n\tu16 ents[0];\n};\n#define\tRPS_SOCK_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_sock_flow_table) + \\\n    (_num * sizeof(u16)))\n\n#define RPS_NO_CPU 0xffff\n\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t\tu32 hash)\n{\n\tif (table && hash) {\n\t\tunsigned int cpu, index = hash & table->mask;\n\n\t\t/* We only give a hint, preemption can change cpu under us */\n\t\tcpu = raw_smp_processor_id();\n\n\t\tif (table->ents[index] != cpu)\n\t\t\ttable->ents[index] = cpu;\n\t}\n}\n\nstatic inline void rps_reset_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t       u32 hash)\n{\n\tif (table && hash)\n\t\ttable->ents[hash & table->mask] = RPS_NO_CPU;\n}\n\nextern struct rps_sock_flow_table __rcu *rps_sock_flow_table;\n\n/* This structure contains an instance of an RX queue. */\nstruct netdev_rx_queue {\n\tstruct rps_map __rcu\t\t*rps_map;\n\tstruct rps_dev_flow_table __rcu\t*rps_flow_table;\n\tstruct kobject\t\t\tkobj;\n\tstruct net_device\t\t*dev;\n} ____cacheline_aligned_in_smp;\n#endif /* CONFIG_RPS */\n\n#ifdef CONFIG_XPS\n/*\n * This structure holds an XPS map which can be of variable length.  The\n * map is an array of queues.\n */\nstruct xps_map {\n\tunsigned int len;\n\tunsigned int alloc_len;\n\tstruct rcu_head rcu;\n\tu16 queues[0];\n};\n#define XPS_MAP_SIZE(_num) (sizeof(struct xps_map) + (_num * sizeof(u16)))\n#define XPS_MIN_MAP_ALLOC ((L1_CACHE_BYTES - sizeof(struct xps_map))\t\\\n    / sizeof(u16))\n\n/*\n * This structure holds all XPS maps for device.  Maps are indexed by CPU.\n */\nstruct xps_dev_maps {\n\tstruct rcu_head rcu;\n\tstruct xps_map __rcu *cpu_map[0];\n};\n#define XPS_DEV_MAPS_SIZE (sizeof(struct xps_dev_maps) +\t\t\\\n    (nr_cpu_ids * sizeof(struct xps_map *)))\n#endif /* CONFIG_XPS */\n\n/*\n * This structure defines the management hooks for network devices.\n * The following hooks can be defined; unless noted otherwise, they are\n * optional and can be filled with a null pointer.\n *\n * int (*ndo_init)(struct net_device *dev);\n *     This function is called once when network device is registered.\n *     The network device can use this to any late stage initializaton\n *     or semantic validattion. It can fail with an error code which will\n *     be propogated back to register_netdev\n *\n * void (*ndo_uninit)(struct net_device *dev);\n *     This function is called when device is unregistered or when registration\n *     fails. It is not called if init fails.\n *\n * int (*ndo_open)(struct net_device *dev);\n *     This function is called when network device transistions to the up\n *     state.\n *\n * int (*ndo_stop)(struct net_device *dev);\n *     This function is called when network device transistions to the down\n *     state.\n *\n * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,\n *                               struct net_device *dev);\n *\tCalled when a packet needs to be transmitted.\n *\tMust return NETDEV_TX_OK , NETDEV_TX_BUSY.\n *        (can also return NETDEV_TX_LOCKED iff NETIF_F_LLTX)\n *\tRequired can not be NULL.\n *\n * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb);\n *\tCalled to decide which queue to when device supports multiple\n *\ttransmit queues.\n *\n * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);\n *\tThis function is called to allow device receiver to make\n *\tchanges to configuration when multicast or promiscious is enabled.\n *\n * void (*ndo_set_rx_mode)(struct net_device *dev);\n *\tThis function is called device changes address list filtering.\n *\n * void (*ndo_set_multicast_list)(struct net_device *dev);\n *\tThis function is called when the multicast address list changes.\n *\n * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);\n *\tThis function  is called when the Media Access Control address\n *\tneeds to be changed. If this interface is not defined, the\n *\tmac address can not be changed.\n *\n * int (*ndo_validate_addr)(struct net_device *dev);\n *\tTest if Media Access Control address is valid for the device.\n *\n * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);\n *\tCalled when a user request an ioctl which can't be handled by\n *\tthe generic interface code. If not defined ioctl's return\n *\tnot supported error code.\n *\n * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);\n *\tUsed to set network devices bus interface parameters. This interface\n *\tis retained for legacy reason, new devices should use the bus\n *\tinterface (PCI) for low level management.\n *\n * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);\n *\tCalled when a user wants to change the Maximum Transfer Unit\n *\tof a device. If not defined, any request to change MTU will\n *\twill return an error.\n *\n * void (*ndo_tx_timeout)(struct net_device *dev);\n *\tCallback uses when the transmitter has not made any progress\n *\tfor dev->watchdog ticks.\n *\n * struct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,\n *                      struct rtnl_link_stats64 *storage);\n * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n *\tCalled when a user wants to get the network device usage\n *\tstatistics. Drivers must do one of the following:\n *\t1. Define @ndo_get_stats64 to fill in a zero-initialised\n *\t   rtnl_link_stats64 structure passed by the caller.\n *\t2. Define @ndo_get_stats to update a net_device_stats structure\n *\t   (which should normally be dev->stats) and return a pointer to\n *\t   it. The structure may be changed asynchronously only if each\n *\t   field is written atomically.\n *\t3. Update dev->stats asynchronously and atomically, and define\n *\t   neither operation.\n *\n * void (*ndo_vlan_rx_register)(struct net_device *dev, struct vlan_group *grp);\n *\tIf device support VLAN receive acceleration\n *\t(ie. dev->features & NETIF_F_HW_VLAN_RX), then this function is called\n *\twhen vlan groups for the device changes.  Note: grp is NULL\n *\tif no vlan's groups are being used.\n *\n * void (*ndo_vlan_rx_add_vid)(struct net_device *dev, unsigned short vid);\n *\tIf device support VLAN filtering (dev->features & NETIF_F_HW_VLAN_FILTER)\n *\tthis function is called when a VLAN id is registered.\n *\n * void (*ndo_vlan_rx_kill_vid)(struct net_device *dev, unsigned short vid);\n *\tIf device support VLAN filtering (dev->features & NETIF_F_HW_VLAN_FILTER)\n *\tthis function is called when a VLAN id is unregistered.\n *\n * void (*ndo_poll_controller)(struct net_device *dev);\n *\n *\tSR-IOV management functions.\n * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);\n * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan, u8 qos);\n * int (*ndo_set_vf_tx_rate)(struct net_device *dev, int vf, int rate);\n * int (*ndo_get_vf_config)(struct net_device *dev,\n *\t\t\t    int vf, struct ifla_vf_info *ivf);\n * int (*ndo_set_vf_port)(struct net_device *dev, int vf,\n *\t\t\t  struct nlattr *port[]);\n * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);\n */\n#define HAVE_NET_DEVICE_OPS\nstruct net_device_ops {\n\tint\t\t\t(*ndo_init)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_uninit)(struct net_device *dev);\n\tint\t\t\t(*ndo_open)(struct net_device *dev);\n\tint\t\t\t(*ndo_stop)(struct net_device *dev);\n\tnetdev_tx_t\t\t(*ndo_start_xmit) (struct sk_buff *skb,\n\t\t\t\t\t\t   struct net_device *dev);\n\tu16\t\t\t(*ndo_select_queue)(struct net_device *dev,\n\t\t\t\t\t\t    struct sk_buff *skb);\n\tvoid\t\t\t(*ndo_change_rx_flags)(struct net_device *dev,\n\t\t\t\t\t\t       int flags);\n\tvoid\t\t\t(*ndo_set_rx_mode)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_set_multicast_list)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_mac_address)(struct net_device *dev,\n\t\t\t\t\t\t       void *addr);\n\tint\t\t\t(*ndo_validate_addr)(struct net_device *dev);\n\tint\t\t\t(*ndo_do_ioctl)(struct net_device *dev,\n\t\t\t\t\t        struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_set_config)(struct net_device *dev,\n\t\t\t\t\t          struct ifmap *map);\n\tint\t\t\t(*ndo_change_mtu)(struct net_device *dev,\n\t\t\t\t\t\t  int new_mtu);\n\tint\t\t\t(*ndo_neigh_setup)(struct net_device *dev,\n\t\t\t\t\t\t   struct neigh_parms *);\n\tvoid\t\t\t(*ndo_tx_timeout) (struct net_device *dev);\n\n\tstruct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,\n\t\t\t\t\t\t     struct rtnl_link_stats64 *storage);\n\tstruct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n\n\tvoid\t\t\t(*ndo_vlan_rx_register)(struct net_device *dev,\n\t\t\t\t\t\t        struct vlan_group *grp);\n\tvoid\t\t\t(*ndo_vlan_rx_add_vid)(struct net_device *dev,\n\t\t\t\t\t\t       unsigned short vid);\n\tvoid\t\t\t(*ndo_vlan_rx_kill_vid)(struct net_device *dev,\n\t\t\t\t\t\t        unsigned short vid);\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\tvoid                    (*ndo_poll_controller)(struct net_device *dev);\n\tint\t\t\t(*ndo_netpoll_setup)(struct net_device *dev,\n\t\t\t\t\t\t     struct netpoll_info *info);\n\tvoid\t\t\t(*ndo_netpoll_cleanup)(struct net_device *dev);\n#endif\n\tint\t\t\t(*ndo_set_vf_mac)(struct net_device *dev,\n\t\t\t\t\t\t  int queue, u8 *mac);\n\tint\t\t\t(*ndo_set_vf_vlan)(struct net_device *dev,\n\t\t\t\t\t\t   int queue, u16 vlan, u8 qos);\n\tint\t\t\t(*ndo_set_vf_tx_rate)(struct net_device *dev,\n\t\t\t\t\t\t      int vf, int rate);\n\tint\t\t\t(*ndo_get_vf_config)(struct net_device *dev,\n\t\t\t\t\t\t     int vf,\n\t\t\t\t\t\t     struct ifla_vf_info *ivf);\n\tint\t\t\t(*ndo_set_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct nlattr *port[]);\n\tint\t\t\t(*ndo_get_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, struct sk_buff *skb);\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n\tint\t\t\t(*ndo_fcoe_enable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_disable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_ddp_setup)(struct net_device *dev,\n\t\t\t\t\t\t      u16 xid,\n\t\t\t\t\t\t      struct scatterlist *sgl,\n\t\t\t\t\t\t      unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_ddp_done)(struct net_device *dev,\n\t\t\t\t\t\t     u16 xid);\n#define NETDEV_FCOE_WWNN 0\n#define NETDEV_FCOE_WWPN 1\n\tint\t\t\t(*ndo_fcoe_get_wwn)(struct net_device *dev,\n\t\t\t\t\t\t    u64 *wwn, int type);\n#endif\n};\n\n/*\n *\tThe DEVICE structure.\n *\tActually, this whole structure is a big mistake.  It mixes I/O\n *\tdata with strictly \"high-level\" data, and it has to know about\n *\talmost every data structure used in the INET module.\n *\n *\tFIXME: cleanup struct net_device such that network protocol info\n *\tmoves out.\n */\n\nstruct net_device {\n\n\t/*\n\t * This is the first field of the \"visible\" part of this structure\n\t * (i.e. as seen by users in the \"Space.c\" file).  It is the name\n\t * of the interface.\n\t */\n\tchar\t\t\tname[IFNAMSIZ];\n\n\tstruct pm_qos_request_list pm_qos_req;\n\n\t/* device name hash chain */\n\tstruct hlist_node\tname_hlist;\n\t/* snmp alias */\n\tchar \t\t\t*ifalias;\n\n\t/*\n\t *\tI/O specific fields\n\t *\tFIXME: Merge these and struct ifmap into one\n\t */\n\tunsigned long\t\tmem_end;\t/* shared mem end\t*/\n\tunsigned long\t\tmem_start;\t/* shared mem start\t*/\n\tunsigned long\t\tbase_addr;\t/* device I/O address\t*/\n\tunsigned int\t\tirq;\t\t/* device IRQ number\t*/\n\n\t/*\n\t *\tSome hardware also needs these fields, but they are not\n\t *\tpart of the usual set specified in Space.c.\n\t */\n\n\tunsigned char\t\tif_port;\t/* Selectable AUI, TP,..*/\n\tunsigned char\t\tdma;\t\t/* DMA channel\t\t*/\n\n\tunsigned long\t\tstate;\n\n\tstruct list_head\tdev_list;\n\tstruct list_head\tnapi_list;\n\tstruct list_head\tunreg_list;\n\n\t/* Net device features */\n\tunsigned long\t\tfeatures;\n#define NETIF_F_SG\t\t1\t/* Scatter/gather IO. */\n#define NETIF_F_IP_CSUM\t\t2\t/* Can checksum TCP/UDP over IPv4. */\n#define NETIF_F_NO_CSUM\t\t4\t/* Does not require checksum. F.e. loopack. */\n#define NETIF_F_HW_CSUM\t\t8\t/* Can checksum all the packets. */\n#define NETIF_F_IPV6_CSUM\t16\t/* Can checksum TCP/UDP over IPV6 */\n#define NETIF_F_HIGHDMA\t\t32\t/* Can DMA to high memory. */\n#define NETIF_F_FRAGLIST\t64\t/* Scatter/gather IO. */\n#define NETIF_F_HW_VLAN_TX\t128\t/* Transmit VLAN hw acceleration */\n#define NETIF_F_HW_VLAN_RX\t256\t/* Receive VLAN hw acceleration */\n#define NETIF_F_HW_VLAN_FILTER\t512\t/* Receive filtering on VLAN */\n#define NETIF_F_VLAN_CHALLENGED\t1024\t/* Device cannot handle VLAN packets */\n#define NETIF_F_GSO\t\t2048\t/* Enable software GSO. */\n#define NETIF_F_LLTX\t\t4096\t/* LockLess TX - deprecated. Please */\n\t\t\t\t\t/* do not use LLTX in new drivers */\n#define NETIF_F_NETNS_LOCAL\t8192\t/* Does not change network namespaces */\n#define NETIF_F_GRO\t\t16384\t/* Generic receive offload */\n#define NETIF_F_LRO\t\t32768\t/* large receive offload */\n\n/* the GSO_MASK reserves bits 16 through 23 */\n#define NETIF_F_FCOE_CRC\t(1 << 24) /* FCoE CRC32 */\n#define NETIF_F_SCTP_CSUM\t(1 << 25) /* SCTP checksum offload */\n#define NETIF_F_FCOE_MTU\t(1 << 26) /* Supports max FCoE MTU, 2158 bytes*/\n#define NETIF_F_NTUPLE\t\t(1 << 27) /* N-tuple filters supported */\n#define NETIF_F_RXHASH\t\t(1 << 28) /* Receive hashing offload */\n\n\t/* Segmentation offload features */\n#define NETIF_F_GSO_SHIFT\t16\n#define NETIF_F_GSO_MASK\t0x00ff0000\n#define NETIF_F_TSO\t\t(SKB_GSO_TCPV4 << NETIF_F_GSO_SHIFT)\n#define NETIF_F_UFO\t\t(SKB_GSO_UDP << NETIF_F_GSO_SHIFT)\n#define NETIF_F_GSO_ROBUST\t(SKB_GSO_DODGY << NETIF_F_GSO_SHIFT)\n#define NETIF_F_TSO_ECN\t\t(SKB_GSO_TCP_ECN << NETIF_F_GSO_SHIFT)\n#define NETIF_F_TSO6\t\t(SKB_GSO_TCPV6 << NETIF_F_GSO_SHIFT)\n#define NETIF_F_FSO\t\t(SKB_GSO_FCOE << NETIF_F_GSO_SHIFT)\n\n\t/* List of features with software fallbacks. */\n#define NETIF_F_GSO_SOFTWARE\t(NETIF_F_TSO | NETIF_F_TSO_ECN | \\\n\t\t\t\t NETIF_F_TSO6 | NETIF_F_UFO)\n\n\n#define NETIF_F_GEN_CSUM\t(NETIF_F_NO_CSUM | NETIF_F_HW_CSUM)\n#define NETIF_F_V4_CSUM\t\t(NETIF_F_GEN_CSUM | NETIF_F_IP_CSUM)\n#define NETIF_F_V6_CSUM\t\t(NETIF_F_GEN_CSUM | NETIF_F_IPV6_CSUM)\n#define NETIF_F_ALL_CSUM\t(NETIF_F_V4_CSUM | NETIF_F_V6_CSUM)\n\n\t/*\n\t * If one device supports one of these features, then enable them\n\t * for all in netdev_increment_features.\n\t */\n#define NETIF_F_ONE_FOR_ALL\t(NETIF_F_GSO_SOFTWARE | NETIF_F_GSO_ROBUST | \\\n\t\t\t\t NETIF_F_SG | NETIF_F_HIGHDMA |\t\t\\\n\t\t\t\t NETIF_F_FRAGLIST)\n\n\t/* Interface index. Unique device identifier\t*/\n\tint\t\t\tifindex;\n\tint\t\t\tiflink;\n\n\tstruct net_device_stats\tstats;\n\tatomic_long_t\t\trx_dropped; /* dropped packets by core network\n\t\t\t\t\t     * Do not use this in drivers.\n\t\t\t\t\t     */\n\n#ifdef CONFIG_WIRELESS_EXT\n\t/* List of functions to handle Wireless Extensions (instead of ioctl).\n\t * See <net/iw_handler.h> for details. Jean II */\n\tconst struct iw_handler_def *\twireless_handlers;\n\t/* Instance data managed by the core of Wireless Extensions. */\n\tstruct iw_public_data *\twireless_data;\n#endif\n\t/* Management operations */\n\tconst struct net_device_ops *netdev_ops;\n\tconst struct ethtool_ops *ethtool_ops;\n\n\t/* Hardware header description */\n\tconst struct header_ops *header_ops;\n\n\tunsigned int\t\tflags;\t/* interface flags (a la BSD)\t*/\n\tunsigned short\t\tgflags;\n        unsigned int            priv_flags; /* Like 'flags' but invisible to userspace. */\n\tunsigned short\t\tpadded;\t/* How much padding added by alloc_netdev() */\n\n\tunsigned char\t\toperstate; /* RFC2863 operstate */\n\tunsigned char\t\tlink_mode; /* mapping policy to operstate */\n\n\tunsigned int\t\tmtu;\t/* interface MTU value\t\t*/\n\tunsigned short\t\ttype;\t/* interface hardware type\t*/\n\tunsigned short\t\thard_header_len;\t/* hardware hdr length\t*/\n\n\t/* extra head- and tailroom the hardware may need, but not in all cases\n\t * can this be guaranteed, especially tailroom. Some cases also use\n\t * LL_MAX_HEADER instead to allocate the skb.\n\t */\n\tunsigned short\t\tneeded_headroom;\n\tunsigned short\t\tneeded_tailroom;\n\n\t/* Interface address info. */\n\tunsigned char\t\tperm_addr[MAX_ADDR_LEN]; /* permanent hw address */\n\tunsigned char\t\taddr_assign_type; /* hw address assignment type */\n\tunsigned char\t\taddr_len;\t/* hardware address length\t*/\n\tunsigned short          dev_id;\t\t/* for shared network cards */\n\n\tspinlock_t\t\taddr_list_lock;\n\tstruct netdev_hw_addr_list\tuc;\t/* Unicast mac addresses */\n\tstruct netdev_hw_addr_list\tmc;\t/* Multicast mac addresses */\n\tint\t\t\tuc_promisc;\n\tunsigned int\t\tpromiscuity;\n\tunsigned int\t\tallmulti;\n\n\n\t/* Protocol specific pointers */\n\n#if defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE)\n\tstruct vlan_group __rcu\t*vlgrp;\t\t/* VLAN group */\n#endif\n#ifdef CONFIG_NET_DSA\n\tvoid\t\t\t*dsa_ptr;\t/* dsa specific data */\n#endif\n\tvoid \t\t\t*atalk_ptr;\t/* AppleTalk link \t*/\n\tstruct in_device __rcu\t*ip_ptr;\t/* IPv4 specific data\t*/\n\tstruct dn_dev __rcu     *dn_ptr;        /* DECnet specific data */\n\tstruct inet6_dev __rcu\t*ip6_ptr;       /* IPv6 specific data */\n\tvoid\t\t\t*ec_ptr;\t/* Econet specific data\t*/\n\tvoid\t\t\t*ax25_ptr;\t/* AX.25 specific data */\n\tstruct wireless_dev\t*ieee80211_ptr;\t/* IEEE 802.11 specific data,\n\t\t\t\t\t\t   assign before registering */\n\n/*\n * Cache lines mostly used on receive path (including eth_type_trans())\n */\n\tunsigned long\t\tlast_rx;\t/* Time of last Rx\n\t\t\t\t\t\t * This should not be set in\n\t\t\t\t\t\t * drivers, unless really needed,\n\t\t\t\t\t\t * because network stack (bonding)\n\t\t\t\t\t\t * use it if/when necessary, to\n\t\t\t\t\t\t * avoid dirtying this cache line.\n\t\t\t\t\t\t */\n\n\tstruct net_device\t*master; /* Pointer to master device of a group,\n\t\t\t\t\t  * which this device is member of.\n\t\t\t\t\t  */\n\n\t/* Interface address info used in eth_type_trans() */\n\tunsigned char\t\t*dev_addr;\t/* hw address, (before bcast\n\t\t\t\t\t\t   because most packets are\n\t\t\t\t\t\t   unicast) */\n\n\tstruct netdev_hw_addr_list\tdev_addrs; /* list of device\n\t\t\t\t\t\t      hw addresses */\n\n\tunsigned char\t\tbroadcast[MAX_ADDR_LEN];\t/* hw bcast add\t*/\n\n#ifdef CONFIG_RPS\n\tstruct kset\t\t*queues_kset;\n\n\tstruct netdev_rx_queue\t*_rx;\n\n\t/* Number of RX queues allocated at register_netdev() time */\n\tunsigned int\t\tnum_rx_queues;\n\n\t/* Number of RX queues currently active in device */\n\tunsigned int\t\treal_num_rx_queues;\n#endif\n\n\trx_handler_func_t __rcu\t*rx_handler;\n\tvoid __rcu\t\t*rx_handler_data;\n\n\tstruct netdev_queue __rcu *ingress_queue;\n\n/*\n * Cache lines mostly used on transmit path\n */\n\tstruct netdev_queue\t*_tx ____cacheline_aligned_in_smp;\n\n\t/* Number of TX queues allocated at alloc_netdev_mq() time  */\n\tunsigned int\t\tnum_tx_queues;\n\n\t/* Number of TX queues currently active in device  */\n\tunsigned int\t\treal_num_tx_queues;\n\n\t/* root qdisc from userspace point of view */\n\tstruct Qdisc\t\t*qdisc;\n\n\tunsigned long\t\ttx_queue_len;\t/* Max frames per queue allowed */\n\tspinlock_t\t\ttx_global_lock;\n\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps __rcu *xps_maps;\n#endif\n\n\t/* These may be needed for future network-power-down code. */\n\n\t/*\n\t * trans_start here is expensive for high speed devices on SMP,\n\t * please use netdev_queue->trans_start instead.\n\t */\n\tunsigned long\t\ttrans_start;\t/* Time (in jiffies) of last Tx\t*/\n\n\tint\t\t\twatchdog_timeo; /* used by dev_watchdog() */\n\tstruct timer_list\twatchdog_timer;\n\n\t/* Number of references to this device */\n\tint __percpu\t\t*pcpu_refcnt;\n\n\t/* delayed register/unregister */\n\tstruct list_head\ttodo_list;\n\t/* device index hash chain */\n\tstruct hlist_node\tindex_hlist;\n\n\tstruct list_head\tlink_watch_list;\n\n\t/* register/unregister state machine */\n\tenum { NETREG_UNINITIALIZED=0,\n\t       NETREG_REGISTERED,\t/* completed register_netdevice */\n\t       NETREG_UNREGISTERING,\t/* called unregister_netdevice */\n\t       NETREG_UNREGISTERED,\t/* completed unregister todo */\n\t       NETREG_RELEASED,\t\t/* called free_netdev */\n\t       NETREG_DUMMY,\t\t/* dummy device for NAPI poll */\n\t} reg_state:16;\n\n\tenum {\n\t\tRTNL_LINK_INITIALIZED,\n\t\tRTNL_LINK_INITIALIZING,\n\t} rtnl_link_state:16;\n\n\t/* Called from unregister, can be used to call free_netdev */\n\tvoid (*destructor)(struct net_device *dev);\n\n#ifdef CONFIG_NETPOLL\n\tstruct netpoll_info\t*npinfo;\n#endif\n\n#ifdef CONFIG_NET_NS\n\t/* Network namespace this network device is inside */\n\tstruct net\t\t*nd_net;\n#endif\n\n\t/* mid-layer private */\n\tunion {\n\t\tvoid\t\t\t\t*ml_priv;\n\t\tstruct pcpu_lstats __percpu\t*lstats; /* loopback stats */\n\t\tstruct pcpu_tstats __percpu\t*tstats; /* tunnel stats */\n\t\tstruct pcpu_dstats __percpu\t*dstats; /* dummy stats */\n\t};\n\t/* GARP */\n\tstruct garp_port __rcu\t*garp_port;\n\n\t/* class/net/name entry */\n\tstruct device\t\tdev;\n\t/* space for optional device, statistics, and wireless sysfs groups */\n\tconst struct attribute_group *sysfs_groups[4];\n\n\t/* rtnetlink link ops */\n\tconst struct rtnl_link_ops *rtnl_link_ops;\n\n\t/* VLAN feature mask */\n\tunsigned long vlan_features;\n\n\t/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SIZE\t\t65536\n\tunsigned int\t\tgso_max_size;\n\n#ifdef CONFIG_DCB\n\t/* Data Center Bridging netlink ops */\n\tconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\n\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n\t/* max exchange id for FCoE LRO by ddp */\n\tunsigned int\t\tfcoe_ddp_xid;\n#endif\n\t/* n-tuple filter list attached to this device */\n\tstruct ethtool_rx_ntuple_list ethtool_ntuple_list;\n\n\t/* phy device may attach itself for hardware timestamping */\n\tstruct phy_device *phydev;\n};\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n\n#define\tNETDEV_ALIGN\t\t32\n\nstatic inline\nstruct netdev_queue *netdev_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t unsigned int index)\n{\n\treturn &dev->_tx[index];\n}\n\nstatic inline void netdev_for_each_tx_queue(struct net_device *dev,\n\t\t\t\t\t    void (*f)(struct net_device *,\n\t\t\t\t\t\t      struct netdev_queue *,\n\t\t\t\t\t\t      void *),\n\t\t\t\t\t    void *arg)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tf(dev, &dev->_tx[i], arg);\n}\n\n/*\n * Net namespace inlines\n */\nstatic inline\nstruct net *dev_net(const struct net_device *dev)\n{\n\treturn read_pnet(&dev->nd_net);\n}\n\nstatic inline\nvoid dev_net_set(struct net_device *dev, struct net *net)\n{\n#ifdef CONFIG_NET_NS\n\trelease_net(dev->nd_net);\n\tdev->nd_net = hold_net(net);\n#endif\n}\n\nstatic inline bool netdev_uses_dsa_tags(struct net_device *dev)\n{\n#ifdef CONFIG_NET_DSA_TAG_DSA\n\tif (dev->dsa_ptr != NULL)\n\t\treturn dsa_uses_dsa_tags(dev->dsa_ptr);\n#endif\n\n\treturn 0;\n}\n\n#ifndef CONFIG_NET_NS\nstatic inline void skb_set_dev(struct sk_buff *skb, struct net_device *dev)\n{\n\tskb->dev = dev;\n}\n#else /* CONFIG_NET_NS */\nvoid skb_set_dev(struct sk_buff *skb, struct net_device *dev);\n#endif\n\nstatic inline bool netdev_uses_trailer_tags(struct net_device *dev)\n{\n#ifdef CONFIG_NET_DSA_TAG_TRAILER\n\tif (dev->dsa_ptr != NULL)\n\t\treturn dsa_uses_trailer_tags(dev->dsa_ptr);\n#endif\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_priv - access network device private data\n *\t@dev: network device\n *\n * Get network device private data\n */\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\n\treturn (char *)dev + ALIGN(sizeof(struct net_device), NETDEV_ALIGN);\n}\n\n/* Set the sysfs physical device reference for the network logical device\n * if set prior to registration will cause a symlink during initialization.\n */\n#define SET_NETDEV_DEV(net, pdev)\t((net)->dev.parent = (pdev))\n\n/* Set the sysfs device type for the network logical device to allow\n * fin grained indentification of different network device types. For\n * example Ethernet, Wirelss LAN, Bluetooth, WiMAX etc.\n */\n#define SET_NETDEV_DEVTYPE(net, devtype)\t((net)->dev.type = (devtype))\n\n/**\n *\tnetif_napi_add - initialize a napi context\n *\t@dev:  network device\n *\t@napi: napi context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * netif_napi_add() must be used to initialize a napi context prior to calling\n * *any* of the other napi related functions.\n */\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight);\n\n/**\n *  netif_napi_del - remove a napi context\n *  @napi: napi context\n *\n *  netif_napi_del() removes a napi context from the network device napi list\n */\nvoid netif_napi_del(struct napi_struct *napi);\n\nstruct napi_gro_cb {\n\t/* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */\n\tvoid *frag0;\n\n\t/* Length of frag0. */\n\tunsigned int frag0_len;\n\n\t/* This indicates where we are processing relative to skb->data. */\n\tint data_offset;\n\n\t/* This is non-zero if the packet may be of the same flow. */\n\tint same_flow;\n\n\t/* This is non-zero if the packet cannot be merged with the new skb. */\n\tint flush;\n\n\t/* Number of segments aggregated. */\n\tint count;\n\n\t/* Free the skb? */\n\tint free;\n};\n\n#define NAPI_GRO_CB(skb) ((struct napi_gro_cb *)(skb)->cb)\n\nstruct packet_type {\n\t__be16\t\t\ttype;\t/* This is really htons(ether_type). */\n\tstruct net_device\t*dev;\t/* NULL is wildcarded here\t     */\n\tint\t\t\t(*func) (struct sk_buff *,\n\t\t\t\t\t struct net_device *,\n\t\t\t\t\t struct packet_type *,\n\t\t\t\t\t struct net_device *);\n\tstruct sk_buff\t\t*(*gso_segment)(struct sk_buff *skb,\n\t\t\t\t\t\tint features);\n\tint\t\t\t(*gso_send_check)(struct sk_buff *skb);\n\tstruct sk_buff\t\t**(*gro_receive)(struct sk_buff **head,\n\t\t\t\t\t       struct sk_buff *skb);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb);\n\tvoid\t\t\t*af_packet_priv;\n\tstruct list_head\tlist;\n};\n\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n\nextern rwlock_t\t\t\t\tdev_base_lock;\t\t/* Device list lock */\n\n\n#define for_each_netdev(net, d)\t\t\\\n\t\tlist_for_each_entry(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_reverse(net, d)\t\\\n\t\tlist_for_each_entry_reverse(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_rcu(net, d)\t\t\\\n\t\tlist_for_each_entry_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_safe(net, d, n)\t\\\n\t\tlist_for_each_entry_safe(d, n, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue(net, d)\t\t\\\n\t\tlist_for_each_entry_continue(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue_rcu(net, d)\t\t\\\n\tlist_for_each_entry_continue_rcu(d, &(net)->dev_base_head, dev_list)\n#define net_device_entry(lh)\tlist_entry(lh, struct net_device, dev_list)\n\nstatic inline struct net_device *next_net_device(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = dev->dev_list.next;\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *next_net_device_rcu(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = rcu_dereference(dev->dev_list.next);\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *first_net_device(struct net *net)\n{\n\treturn list_empty(&net->dev_base_head) ? NULL :\n\t\tnet_device_entry(net->dev_base_head.next);\n}\n\nextern int \t\t\tnetdev_boot_setup_check(struct net_device *dev);\nextern unsigned long\t\tnetdev_boot_base(const char *prefix, int unit);\nextern struct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t\t      const char *hwaddr);\nextern struct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type);\nextern struct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type);\nextern void\t\tdev_add_pack(struct packet_type *pt);\nextern void\t\tdev_remove_pack(struct packet_type *pt);\nextern void\t\t__dev_remove_pack(struct packet_type *pt);\n\nextern struct net_device\t*dev_get_by_flags_rcu(struct net *net, unsigned short flags,\n\t\t\t\t\t\t      unsigned short mask);\nextern struct net_device\t*dev_get_by_name(struct net *net, const char *name);\nextern struct net_device\t*dev_get_by_name_rcu(struct net *net, const char *name);\nextern struct net_device\t*__dev_get_by_name(struct net *net, const char *name);\nextern int\t\tdev_alloc_name(struct net_device *dev, const char *name);\nextern int\t\tdev_open(struct net_device *dev);\nextern int\t\tdev_close(struct net_device *dev);\nextern void\t\tdev_disable_lro(struct net_device *dev);\nextern int\t\tdev_queue_xmit(struct sk_buff *skb);\nextern int\t\tregister_netdevice(struct net_device *dev);\nextern void\t\tunregister_netdevice_queue(struct net_device *dev,\n\t\t\t\t\t\t   struct list_head *head);\nextern void\t\tunregister_netdevice_many(struct list_head *head);\nstatic inline void unregister_netdevice(struct net_device *dev)\n{\n\tunregister_netdevice_queue(dev, NULL);\n}\n\nextern int \t\tnetdev_refcnt_read(const struct net_device *dev);\nextern void\t\tfree_netdev(struct net_device *dev);\nextern void\t\tsynchronize_net(void);\nextern int \t\tregister_netdevice_notifier(struct notifier_block *nb);\nextern int\t\tunregister_netdevice_notifier(struct notifier_block *nb);\nextern int\t\tinit_dummy_netdev(struct net_device *dev);\nextern void\t\tnetdev_resync_ops(struct net_device *dev);\n\nextern int call_netdevice_notifiers(unsigned long val, struct net_device *dev);\nextern struct net_device\t*dev_get_by_index(struct net *net, int ifindex);\nextern struct net_device\t*__dev_get_by_index(struct net *net, int ifindex);\nextern struct net_device\t*dev_get_by_index_rcu(struct net *net, int ifindex);\nextern int\t\tdev_restart(struct net_device *dev);\n#ifdef CONFIG_NETPOLL_TRAP\nextern int\t\tnetpoll_trap(void);\n#endif\nextern int\t       skb_gro_receive(struct sk_buff **head,\n\t\t\t\t       struct sk_buff *skb);\nextern void\t       skb_gro_reset_offset(struct sk_buff *skb);\n\nstatic inline unsigned int skb_gro_offset(const struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline unsigned int skb_gro_len(const struct sk_buff *skb)\n{\n\treturn skb->len - NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline void skb_gro_pull(struct sk_buff *skb, unsigned int len)\n{\n\tNAPI_GRO_CB(skb)->data_offset += len;\n}\n\nstatic inline void *skb_gro_header_fast(struct sk_buff *skb,\n\t\t\t\t\tunsigned int offset)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 + offset;\n}\n\nstatic inline int skb_gro_header_hard(struct sk_buff *skb, unsigned int hlen)\n{\n\treturn NAPI_GRO_CB(skb)->frag0_len < hlen;\n}\n\nstatic inline void *skb_gro_header_slow(struct sk_buff *skb, unsigned int hlen,\n\t\t\t\t\tunsigned int offset)\n{\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\treturn pskb_may_pull(skb, hlen) ? skb->data + offset : NULL;\n}\n\nstatic inline void *skb_gro_mac_header(struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 ?: skb_mac_header(skb);\n}\n\nstatic inline void *skb_gro_network_header(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->frag0 ?: skb->data) +\n\t       skb_network_offset(skb);\n}\n\nstatic inline int dev_hard_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t  unsigned short type,\n\t\t\t\t  const void *daddr, const void *saddr,\n\t\t\t\t  unsigned len)\n{\n\tif (!dev->header_ops || !dev->header_ops->create)\n\t\treturn 0;\n\n\treturn dev->header_ops->create(skb, dev, type, daddr, saddr, len);\n}\n\nstatic inline int dev_parse_header(const struct sk_buff *skb,\n\t\t\t\t   unsigned char *haddr)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse)\n\t\treturn 0;\n\treturn dev->header_ops->parse(skb, haddr);\n}\n\ntypedef int gifconf_func_t(struct net_device * dev, char __user * bufptr, int len);\nextern int\t\tregister_gifconf(unsigned int family, gifconf_func_t * gifconf);\nstatic inline int unregister_gifconf(unsigned int family)\n{\n\treturn register_gifconf(family, NULL);\n}\n\n/*\n * Incoming packets are placed on per-cpu queues\n */\nstruct softnet_data {\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n\tstruct list_head\tpoll_list;\n\tstruct sk_buff\t\t*completion_queue;\n\tstruct sk_buff_head\tprocess_queue;\n\n\t/* stats */\n\tunsigned int\t\tprocessed;\n\tunsigned int\t\ttime_squeeze;\n\tunsigned int\t\tcpu_collision;\n\tunsigned int\t\treceived_rps;\n\n#ifdef CONFIG_RPS\n\tstruct softnet_data\t*rps_ipi_list;\n\n\t/* Elements below can be accessed between CPUs for RPS */\n\tstruct call_single_data\tcsd ____cacheline_aligned_in_smp;\n\tstruct softnet_data\t*rps_ipi_next;\n\tunsigned int\t\tcpu;\n\tunsigned int\t\tinput_queue_head;\n\tunsigned int\t\tinput_queue_tail;\n#endif\n\tunsigned\t\tdropped;\n\tstruct sk_buff_head\tinput_pkt_queue;\n\tstruct napi_struct\tbacklog;\n};\n\nstatic inline void input_queue_head_incr(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tsd->input_queue_head++;\n#endif\n}\n\nstatic inline void input_queue_tail_incr_save(struct softnet_data *sd,\n\t\t\t\t\t      unsigned int *qtail)\n{\n#ifdef CONFIG_RPS\n\t*qtail = ++sd->input_queue_tail;\n#endif\n}\n\nDECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\n\n#define HAVE_NETIF_QUEUE\n\nextern void __netif_schedule(struct Qdisc *q);\n\nstatic inline void netif_schedule_queue(struct netdev_queue *txq)\n{\n\tif (!test_bit(__QUEUE_STATE_XOFF, &txq->state))\n\t\t__netif_schedule(txq->qdisc);\n}\n\nstatic inline void netif_tx_schedule_all(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tnetif_schedule_queue(netdev_get_tx_queue(dev, i));\n}\n\nstatic inline void netif_tx_start_queue(struct netdev_queue *dev_queue)\n{\n\tclear_bit(__QUEUE_STATE_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_start_queue - allow transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n */\nstatic inline void netif_start_queue(struct net_device *dev)\n{\n\tnetif_tx_start_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_start_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nstatic inline void netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_NETPOLL_TRAP\n\tif (netpoll_trap()) {\n\t\tnetif_tx_start_queue(dev_queue);\n\t\treturn;\n\t}\n#endif\n\tif (test_and_clear_bit(__QUEUE_STATE_XOFF, &dev_queue->state))\n\t\t__netif_schedule(dev_queue->qdisc);\n}\n\n/**\n *\tnetif_wake_queue - restart transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are available.\n */\nstatic inline void netif_wake_queue(struct net_device *dev)\n{\n\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_wake_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n}\n\nstatic inline void netif_tx_stop_queue(struct netdev_queue *dev_queue)\n{\n\tif (WARN_ON(!dev_queue)) {\n\t\tprintk(KERN_INFO \"netif_stop_queue() cannot be called before \"\n\t\t       \"register_netdev()\");\n\t\treturn;\n\t}\n\tset_bit(__QUEUE_STATE_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_stop_queue - stop transmitted packets\n *\t@dev: network device\n *\n *\tStop upper layers calling the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are unavailable.\n */\nstatic inline void netif_stop_queue(struct net_device *dev)\n{\n\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\n\nstatic inline int netif_tx_queue_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn test_bit(__QUEUE_STATE_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_queue_stopped - test if transmit queue is flowblocked\n *\t@dev: network device\n *\n *\tTest if transmit queue on device is currently unable to send.\n */\nstatic inline int netif_queue_stopped(const struct net_device *dev)\n{\n\treturn netif_tx_queue_stopped(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline int netif_tx_queue_frozen_or_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_XOFF_OR_FROZEN;\n}\n\n/**\n *\tnetif_running - test if up\n *\t@dev: network device\n *\n *\tTest if the device has been brought up.\n */\nstatic inline int netif_running(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_START, &dev->state);\n}\n\n/*\n * Routines to manage the subqueues on a device.  We only need start\n * stop, and a check if it's stopped.  All other device management is\n * done at the overall netdevice level.\n * Also test the device if we're multiqueue.\n */\n\n/**\n *\tnetif_start_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Start individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_start_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_start_queue(txq);\n}\n\n/**\n *\tnetif_stop_subqueue - stop sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Stop individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_stop_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n#ifdef CONFIG_NETPOLL_TRAP\n\tif (netpoll_trap())\n\t\treturn;\n#endif\n\tnetif_tx_stop_queue(txq);\n}\n\n/**\n *\tnetif_subqueue_stopped - test status of subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Check individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline int __netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\treturn netif_tx_queue_stopped(txq);\n}\n\nstatic inline int netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\treturn __netif_subqueue_stopped(dev, skb_get_queue_mapping(skb));\n}\n\n/**\n *\tnetif_wake_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Resume individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_wake_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n#ifdef CONFIG_NETPOLL_TRAP\n\tif (netpoll_trap())\n\t\treturn;\n#endif\n\tif (test_and_clear_bit(__QUEUE_STATE_XOFF, &txq->state))\n\t\t__netif_schedule(txq->qdisc);\n}\n\n/*\n * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used\n * as a distribution range limit for the returned value.\n */\nstatic inline u16 skb_tx_hash(const struct net_device *dev,\n\t\t\t      const struct sk_buff *skb)\n{\n\treturn __skb_tx_hash(dev, skb, dev->real_num_tx_queues);\n}\n\n/**\n *\tnetif_is_multiqueue - test if device has multiple transmit queues\n *\t@dev: network device\n *\n * Check if device has multiple transmit queues\n */\nstatic inline int netif_is_multiqueue(const struct net_device *dev)\n{\n\treturn dev->num_tx_queues > 1;\n}\n\nextern int netif_set_real_num_tx_queues(struct net_device *dev,\n\t\t\t\t\tunsigned int txq);\n\n#ifdef CONFIG_RPS\nextern int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\tunsigned int rxq);\n#else\nstatic inline int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\t\tunsigned int rxq)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline int netif_copy_real_num_queues(struct net_device *to_dev,\n\t\t\t\t\t     const struct net_device *from_dev)\n{\n\tnetif_set_real_num_tx_queues(to_dev, from_dev->real_num_tx_queues);\n#ifdef CONFIG_RPS\n\treturn netif_set_real_num_rx_queues(to_dev,\n\t\t\t\t\t    from_dev->real_num_rx_queues);\n#else\n\treturn 0;\n#endif\n}\n\n/* Use this variant when it is known for sure that it\n * is executing from hardware interrupt context or with hardware interrupts\n * disabled.\n */\nextern void dev_kfree_skb_irq(struct sk_buff *skb);\n\n/* Use this variant in places where it could be invoked\n * from either hardware interrupt or other context, with hardware interrupts\n * either disabled or enabled.\n */\nextern void dev_kfree_skb_any(struct sk_buff *skb);\n\n#define HAVE_NETIF_RX 1\nextern int\t\tnetif_rx(struct sk_buff *skb);\nextern int\t\tnetif_rx_ni(struct sk_buff *skb);\n#define HAVE_NETIF_RECEIVE_SKB 1\nextern int\t\tnetif_receive_skb(struct sk_buff *skb);\nextern gro_result_t\tdev_gro_receive(struct napi_struct *napi,\n\t\t\t\t\tstruct sk_buff *skb);\nextern gro_result_t\tnapi_skb_finish(gro_result_t ret, struct sk_buff *skb);\nextern gro_result_t\tnapi_gro_receive(struct napi_struct *napi,\n\t\t\t\t\t struct sk_buff *skb);\nextern void\t\tnapi_gro_flush(struct napi_struct *napi);\nextern struct sk_buff *\tnapi_get_frags(struct napi_struct *napi);\nextern gro_result_t\tnapi_frags_finish(struct napi_struct *napi,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  gro_result_t ret);\nextern struct sk_buff *\tnapi_frags_skb(struct napi_struct *napi);\nextern gro_result_t\tnapi_gro_frags(struct napi_struct *napi);\n\nstatic inline void napi_free_frags(struct napi_struct *napi)\n{\n\tkfree_skb(napi->skb);\n\tnapi->skb = NULL;\n}\n\nextern int netdev_rx_handler_register(struct net_device *dev,\n\t\t\t\t      rx_handler_func_t *rx_handler,\n\t\t\t\t      void *rx_handler_data);\nextern void netdev_rx_handler_unregister(struct net_device *dev);\n\nextern int\t\tdev_valid_name(const char *name);\nextern int\t\tdev_ioctl(struct net *net, unsigned int cmd, void __user *);\nextern int\t\tdev_ethtool(struct net *net, struct ifreq *);\nextern unsigned\t\tdev_get_flags(const struct net_device *);\nextern int\t\t__dev_change_flags(struct net_device *, unsigned int flags);\nextern int\t\tdev_change_flags(struct net_device *, unsigned);\nextern void\t\t__dev_notify_flags(struct net_device *, unsigned int old_flags);\nextern int\t\tdev_change_name(struct net_device *, const char *);\nextern int\t\tdev_set_alias(struct net_device *, const char *, size_t);\nextern int\t\tdev_change_net_namespace(struct net_device *,\n\t\t\t\t\t\t struct net *, const char *);\nextern int\t\tdev_set_mtu(struct net_device *, int);\nextern int\t\tdev_set_mac_address(struct net_device *,\n\t\t\t\t\t    struct sockaddr *);\nextern int\t\tdev_hard_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    struct netdev_queue *txq);\nextern int\t\tdev_forward_skb(struct net_device *dev,\n\t\t\t\t\tstruct sk_buff *skb);\n\nextern int\t\tnetdev_budget;\n\n/* Called by rtnetlink.c:rtnl_unlock() */\nextern void netdev_run_todo(void);\n\n/**\n *\tdev_put - release reference to device\n *\t@dev: network device\n *\n * Release reference to device to allow it to be freed.\n */\nstatic inline void dev_put(struct net_device *dev)\n{\n\tirqsafe_cpu_dec(*dev->pcpu_refcnt);\n}\n\n/**\n *\tdev_hold - get reference to device\n *\t@dev: network device\n *\n * Hold reference to device to keep it from being freed.\n */\nstatic inline void dev_hold(struct net_device *dev)\n{\n\tirqsafe_cpu_inc(*dev->pcpu_refcnt);\n}\n\n/* Carrier loss detection, dial on demand. The functions netif_carrier_on\n * and _off may be called from IRQ context, but it is caller\n * who is responsible for serialization of these calls.\n *\n * The name carrier is inappropriate, these functions should really be\n * called netif_lowerlayer_*() because they represent the state of any\n * kind of lower layer not just hardware media.\n */\n\nextern void linkwatch_fire_event(struct net_device *dev);\nextern void linkwatch_forget_dev(struct net_device *dev);\n\n/**\n *\tnetif_carrier_ok - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline int netif_carrier_ok(const struct net_device *dev)\n{\n\treturn !test_bit(__LINK_STATE_NOCARRIER, &dev->state);\n}\n\nextern unsigned long dev_trans_start(struct net_device *dev);\n\nextern void __netdev_watchdog_up(struct net_device *dev);\n\nextern void netif_carrier_on(struct net_device *dev);\n\nextern void netif_carrier_off(struct net_device *dev);\n\nextern void netif_notify_peers(struct net_device *dev);\n\n/**\n *\tnetif_dormant_on - mark device as dormant.\n *\t@dev: network device\n *\n * Mark device as dormant (as per RFC2863).\n *\n * The dormant state indicates that the relevant interface is not\n * actually in a condition to pass packets (i.e., it is not 'up') but is\n * in a \"pending\" state, waiting for some external event.  For \"on-\n * demand\" interfaces, this new state identifies the situation where the\n * interface is waiting for events to place it in the up state.\n *\n */\nstatic inline void netif_dormant_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant_off - set device as not dormant.\n *\t@dev: network device\n *\n * Device is not in dormant state.\n */\nstatic inline void netif_dormant_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline int netif_dormant(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_DORMANT, &dev->state);\n}\n\n\n/**\n *\tnetif_oper_up - test if device is operational\n *\t@dev: network device\n *\n * Check if carrier is operational\n */\nstatic inline int netif_oper_up(const struct net_device *dev)\n{\n\treturn (dev->operstate == IF_OPER_UP ||\n\t\tdev->operstate == IF_OPER_UNKNOWN /* backward compat */);\n}\n\n/**\n *\tnetif_device_present - is device available or removed\n *\t@dev: network device\n *\n * Check if device has not been removed from system.\n */\nstatic inline int netif_device_present(struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_PRESENT, &dev->state);\n}\n\nextern void netif_device_detach(struct net_device *dev);\n\nextern void netif_device_attach(struct net_device *dev);\n\n/*\n * Network interface message level settings\n */\n#define HAVE_NETIF_MSG 1\n\nenum {\n\tNETIF_MSG_DRV\t\t= 0x0001,\n\tNETIF_MSG_PROBE\t\t= 0x0002,\n\tNETIF_MSG_LINK\t\t= 0x0004,\n\tNETIF_MSG_TIMER\t\t= 0x0008,\n\tNETIF_MSG_IFDOWN\t= 0x0010,\n\tNETIF_MSG_IFUP\t\t= 0x0020,\n\tNETIF_MSG_RX_ERR\t= 0x0040,\n\tNETIF_MSG_TX_ERR\t= 0x0080,\n\tNETIF_MSG_TX_QUEUED\t= 0x0100,\n\tNETIF_MSG_INTR\t\t= 0x0200,\n\tNETIF_MSG_TX_DONE\t= 0x0400,\n\tNETIF_MSG_RX_STATUS\t= 0x0800,\n\tNETIF_MSG_PKTDATA\t= 0x1000,\n\tNETIF_MSG_HW\t\t= 0x2000,\n\tNETIF_MSG_WOL\t\t= 0x4000,\n};\n\n#define netif_msg_drv(p)\t((p)->msg_enable & NETIF_MSG_DRV)\n#define netif_msg_probe(p)\t((p)->msg_enable & NETIF_MSG_PROBE)\n#define netif_msg_link(p)\t((p)->msg_enable & NETIF_MSG_LINK)\n#define netif_msg_timer(p)\t((p)->msg_enable & NETIF_MSG_TIMER)\n#define netif_msg_ifdown(p)\t((p)->msg_enable & NETIF_MSG_IFDOWN)\n#define netif_msg_ifup(p)\t((p)->msg_enable & NETIF_MSG_IFUP)\n#define netif_msg_rx_err(p)\t((p)->msg_enable & NETIF_MSG_RX_ERR)\n#define netif_msg_tx_err(p)\t((p)->msg_enable & NETIF_MSG_TX_ERR)\n#define netif_msg_tx_queued(p)\t((p)->msg_enable & NETIF_MSG_TX_QUEUED)\n#define netif_msg_intr(p)\t((p)->msg_enable & NETIF_MSG_INTR)\n#define netif_msg_tx_done(p)\t((p)->msg_enable & NETIF_MSG_TX_DONE)\n#define netif_msg_rx_status(p)\t((p)->msg_enable & NETIF_MSG_RX_STATUS)\n#define netif_msg_pktdata(p)\t((p)->msg_enable & NETIF_MSG_PKTDATA)\n#define netif_msg_hw(p)\t\t((p)->msg_enable & NETIF_MSG_HW)\n#define netif_msg_wol(p)\t((p)->msg_enable & NETIF_MSG_WOL)\n\nstatic inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)\n{\n\t/* use default */\n\tif (debug_value < 0 || debug_value >= (sizeof(u32) * 8))\n\t\treturn default_msg_enable_bits;\n\tif (debug_value == 0)\t/* no output */\n\t\treturn 0;\n\t/* set low N bits */\n\treturn (1 << debug_value) - 1;\n}\n\nstatic inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)\n{\n\tspin_lock(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = cpu;\n}\n\nstatic inline void __netif_tx_lock_bh(struct netdev_queue *txq)\n{\n\tspin_lock_bh(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = smp_processor_id();\n}\n\nstatic inline int __netif_tx_trylock(struct netdev_queue *txq)\n{\n\tint ok = spin_trylock(&txq->_xmit_lock);\n\tif (likely(ok))\n\t\ttxq->xmit_lock_owner = smp_processor_id();\n\treturn ok;\n}\n\nstatic inline void __netif_tx_unlock(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_unlock_bh(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock_bh(&txq->_xmit_lock);\n}\n\nstatic inline void txq_trans_update(struct netdev_queue *txq)\n{\n\tif (txq->xmit_lock_owner != -1)\n\t\ttxq->trans_start = jiffies;\n}\n\n/**\n *\tnetif_tx_lock - grab network device transmit lock\n *\t@dev: network device\n *\n * Get network device transmit lock\n */\nstatic inline void netif_tx_lock(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tspin_lock(&dev->tx_global_lock);\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* We are the only thread of execution doing a\n\t\t * freeze, but we have to grab the _xmit_lock in\n\t\t * order to synchronize with threads which are in\n\t\t * the ->hard_start_xmit() handler and already\n\t\t * checked the frozen bit.\n\t\t */\n\t\t__netif_tx_lock(txq, cpu);\n\t\tset_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic inline void netif_tx_lock_bh(struct net_device *dev)\n{\n\tlocal_bh_disable();\n\tnetif_tx_lock(dev);\n}\n\nstatic inline void netif_tx_unlock(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* No need to grab the _xmit_lock here.  If the\n\t\t * queue is not stopped for another reason, we\n\t\t * force a schedule.\n\t\t */\n\t\tclear_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\tnetif_schedule_queue(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n}\n\nstatic inline void netif_tx_unlock_bh(struct net_device *dev)\n{\n\tnetif_tx_unlock(dev);\n\tlocal_bh_enable();\n}\n\n#define HARD_TX_LOCK(dev, txq, cpu) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_lock(txq, cpu);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define HARD_TX_UNLOCK(dev, txq) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_unlock(txq);\t\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\nstatic inline void netif_tx_disable(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tlocal_bh_disable();\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t__netif_tx_lock(txq, cpu);\n\t\tnetif_tx_stop_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n\tlocal_bh_enable();\n}\n\nstatic inline void netif_addr_lock(struct net_device *dev)\n{\n\tspin_lock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_lock_bh(struct net_device *dev)\n{\n\tspin_lock_bh(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock(struct net_device *dev)\n{\n\tspin_unlock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock_bh(struct net_device *dev)\n{\n\tspin_unlock_bh(&dev->addr_list_lock);\n}\n\n/*\n * dev_addrs walker. Should be used only for read access. Call with\n * rcu_read_lock held.\n */\n#define for_each_dev_addr(dev, ha) \\\n\t\tlist_for_each_entry_rcu(ha, &dev->dev_addrs.list, list)\n\n/* These functions live elsewhere (drivers/net/net_init.c, but related) */\n\nextern void\t\tether_setup(struct net_device *dev);\n\n/* Support for loadable net-drivers */\nextern struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\t\t\t       void (*setup)(struct net_device *),\n\t\t\t\t       unsigned int txqs, unsigned int rxqs);\n#define alloc_netdev(sizeof_priv, name, setup) \\\n\talloc_netdev_mqs(sizeof_priv, name, setup, 1, 1)\n\n#define alloc_netdev_mq(sizeof_priv, name, setup, count) \\\n\talloc_netdev_mqs(sizeof_priv, name, setup, count, count)\n\nextern int\t\tregister_netdev(struct net_device *dev);\nextern void\t\tunregister_netdev(struct net_device *dev);\n\n/* General hardware address lists handling functions */\nextern int __hw_addr_add_multiple(struct netdev_hw_addr_list *to_list,\n\t\t\t\t  struct netdev_hw_addr_list *from_list,\n\t\t\t\t  int addr_len, unsigned char addr_type);\nextern void __hw_addr_del_multiple(struct netdev_hw_addr_list *to_list,\n\t\t\t\t   struct netdev_hw_addr_list *from_list,\n\t\t\t\t   int addr_len, unsigned char addr_type);\nextern int __hw_addr_sync(struct netdev_hw_addr_list *to_list,\n\t\t\t  struct netdev_hw_addr_list *from_list,\n\t\t\t  int addr_len);\nextern void __hw_addr_unsync(struct netdev_hw_addr_list *to_list,\n\t\t\t     struct netdev_hw_addr_list *from_list,\n\t\t\t     int addr_len);\nextern void __hw_addr_flush(struct netdev_hw_addr_list *list);\nextern void __hw_addr_init(struct netdev_hw_addr_list *list);\n\n/* Functions used for device addresses handling */\nextern int dev_addr_add(struct net_device *dev, unsigned char *addr,\n\t\t\tunsigned char addr_type);\nextern int dev_addr_del(struct net_device *dev, unsigned char *addr,\n\t\t\tunsigned char addr_type);\nextern int dev_addr_add_multiple(struct net_device *to_dev,\n\t\t\t\t struct net_device *from_dev,\n\t\t\t\t unsigned char addr_type);\nextern int dev_addr_del_multiple(struct net_device *to_dev,\n\t\t\t\t struct net_device *from_dev,\n\t\t\t\t unsigned char addr_type);\nextern void dev_addr_flush(struct net_device *dev);\nextern int dev_addr_init(struct net_device *dev);\n\n/* Functions used for unicast addresses handling */\nextern int dev_uc_add(struct net_device *dev, unsigned char *addr);\nextern int dev_uc_del(struct net_device *dev, unsigned char *addr);\nextern int dev_uc_sync(struct net_device *to, struct net_device *from);\nextern void dev_uc_unsync(struct net_device *to, struct net_device *from);\nextern void dev_uc_flush(struct net_device *dev);\nextern void dev_uc_init(struct net_device *dev);\n\n/* Functions used for multicast addresses handling */\nextern int dev_mc_add(struct net_device *dev, unsigned char *addr);\nextern int dev_mc_add_global(struct net_device *dev, unsigned char *addr);\nextern int dev_mc_del(struct net_device *dev, unsigned char *addr);\nextern int dev_mc_del_global(struct net_device *dev, unsigned char *addr);\nextern int dev_mc_sync(struct net_device *to, struct net_device *from);\nextern void dev_mc_unsync(struct net_device *to, struct net_device *from);\nextern void dev_mc_flush(struct net_device *dev);\nextern void dev_mc_init(struct net_device *dev);\n\n/* Functions used for secondary unicast and multicast support */\nextern void\t\tdev_set_rx_mode(struct net_device *dev);\nextern void\t\t__dev_set_rx_mode(struct net_device *dev);\nextern int\t\tdev_set_promiscuity(struct net_device *dev, int inc);\nextern int\t\tdev_set_allmulti(struct net_device *dev, int inc);\nextern void\t\tnetdev_state_change(struct net_device *dev);\nextern int\t\tnetdev_bonding_change(struct net_device *dev,\n\t\t\t\t\t      unsigned long event);\nextern void\t\tnetdev_features_change(struct net_device *dev);\n/* Load a device via the kmod */\nextern void\t\tdev_load(struct net *net, const char *name);\nextern void\t\tdev_mcast_init(void);\nextern struct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\t       struct rtnl_link_stats64 *storage);\n\nextern int\t\tnetdev_max_backlog;\nextern int\t\tnetdev_tstamp_prequeue;\nextern int\t\tweight_p;\nextern int\t\tnetdev_set_master(struct net_device *dev, struct net_device *master);\nextern int skb_checksum_help(struct sk_buff *skb);\nextern struct sk_buff *skb_gso_segment(struct sk_buff *skb, int features);\n#ifdef CONFIG_BUG\nextern void netdev_rx_csum_fault(struct net_device *dev);\n#else\nstatic inline void netdev_rx_csum_fault(struct net_device *dev)\n{\n}\n#endif\n/* rx skb timestamps */\nextern void\t\tnet_enable_timestamp(void);\nextern void\t\tnet_disable_timestamp(void);\n\n#ifdef CONFIG_PROC_FS\nextern void *dev_seq_start(struct seq_file *seq, loff_t *pos);\nextern void *dev_seq_next(struct seq_file *seq, void *v, loff_t *pos);\nextern void dev_seq_stop(struct seq_file *seq, void *v);\n#endif\n\nextern int netdev_class_create_file(struct class_attribute *class_attr);\nextern void netdev_class_remove_file(struct class_attribute *class_attr);\n\nextern struct kobj_ns_type_operations net_ns_type_operations;\n\nextern char *netdev_drivername(const struct net_device *dev, char *buffer, int len);\n\nextern void linkwatch_run_queue(void);\n\nunsigned long netdev_increment_features(unsigned long all, unsigned long one,\n\t\t\t\t\tunsigned long mask);\nunsigned long netdev_fix_features(unsigned long features, const char *name);\n\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev);\n\nint netif_skb_features(struct sk_buff *skb);\n\nstatic inline int net_gso_ok(int features, int gso_type)\n{\n\tint feature = gso_type << NETIF_F_GSO_SHIFT;\n\treturn (features & feature) == feature;\n}\n\nstatic inline int skb_gso_ok(struct sk_buff *skb, int features)\n{\n\treturn net_gso_ok(features, skb_shinfo(skb)->gso_type) &&\n\t       (!skb_has_frag_list(skb) || (features & NETIF_F_FRAGLIST));\n}\n\nstatic inline int netif_needs_gso(struct sk_buff *skb, int features)\n{\n\treturn skb_is_gso(skb) && (!skb_gso_ok(skb, features) ||\n\t\tunlikely(skb->ip_summed != CHECKSUM_PARTIAL));\n}\n\nstatic inline void netif_set_gso_max_size(struct net_device *dev,\n\t\t\t\t\t  unsigned int size)\n{\n\tdev->gso_max_size = size;\n}\n\nextern int __skb_bond_should_drop(struct sk_buff *skb,\n\t\t\t\t  struct net_device *master);\n\nstatic inline int skb_bond_should_drop(struct sk_buff *skb,\n\t\t\t\t       struct net_device *master)\n{\n\tif (master)\n\t\treturn __skb_bond_should_drop(skb, master);\n\treturn 0;\n}\n\nextern struct pernet_operations __net_initdata loopback_net_ops;\n\nstatic inline int dev_ethtool_get_settings(struct net_device *dev,\n\t\t\t\t\t   struct ethtool_cmd *cmd)\n{\n\tif (!dev->ethtool_ops || !dev->ethtool_ops->get_settings)\n\t\treturn -EOPNOTSUPP;\n\treturn dev->ethtool_ops->get_settings(dev, cmd);\n}\n\nstatic inline u32 dev_ethtool_get_rx_csum(struct net_device *dev)\n{\n\tif (!dev->ethtool_ops || !dev->ethtool_ops->get_rx_csum)\n\t\treturn 0;\n\treturn dev->ethtool_ops->get_rx_csum(dev);\n}\n\nstatic inline u32 dev_ethtool_get_flags(struct net_device *dev)\n{\n\tif (!dev->ethtool_ops || !dev->ethtool_ops->get_flags)\n\t\treturn 0;\n\treturn dev->ethtool_ops->get_flags(dev);\n}\n\n/* Logging, debugging and troubleshooting/diagnostic helpers. */\n\n/* netdev_printk helpers, similar to dev_printk */\n\nstatic inline const char *netdev_name(const struct net_device *dev)\n{\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\treturn \"(unregistered net_device)\";\n\treturn dev->name;\n}\n\nextern int netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t const char *format, ...)\n\t__attribute__ ((format (printf, 3, 4)));\nextern int netdev_emerg(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_alert(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_crit(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_err(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_warn(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_notice(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\nextern int netdev_info(const struct net_device *dev, const char *format, ...)\n\t__attribute__ ((format (printf, 2, 3)));\n\n#define MODULE_ALIAS_NETDEV(device) \\\n\tMODULE_ALIAS(\"netdev-\" device)\n\n#if defined(DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n\tnetdev_printk(KERN_DEBUG, __dev, format, ##args)\n#elif defined(CONFIG_DYNAMIC_DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tdynamic_dev_dbg((__dev)->dev.parent, \"%s: \" format,\t\\\n\t\t\tnetdev_name(__dev), ##args);\t\t\\\n} while (0)\n#else\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, __dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netdev_vdbg\tnetdev_dbg\n#else\n\n#define netdev_vdbg(dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, dev, format, ##args);\t\\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n * netdev_WARN() acts like dev_printk(), but with the key difference\n * of using a WARN/WARN_ON to get the message out, including the\n * file/line information and a backtrace.\n */\n#define netdev_WARN(dev, format, args...)\t\t\t\\\n\tWARN(1, \"netdevice: %s\\n\" format, netdev_name(dev), ##args);\n\n/* netif printk helpers, similar to netdev_printk */\n\n#define netif_printk(priv, type, level, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t  \t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_printk(level, (dev), fmt, ##args);\t\\\n} while (0)\n\n#define netif_level(level, priv, type, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_##level(dev, fmt, ##args);\t\t\\\n} while (0)\n\n#define netif_emerg(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(emerg, priv, type, dev, fmt, ##args)\n#define netif_alert(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(alert, priv, type, dev, fmt, ##args)\n#define netif_crit(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(crit, priv, type, dev, fmt, ##args)\n#define netif_err(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(err, priv, type, dev, fmt, ##args)\n#define netif_warn(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(warn, priv, type, dev, fmt, ##args)\n#define netif_notice(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(notice, priv, type, dev, fmt, ##args)\n#define netif_info(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(info, priv, type, dev, fmt, ##args)\n\n#if defined(DEBUG)\n#define netif_dbg(priv, type, dev, format, args...)\t\t\\\n\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args)\n#elif defined(CONFIG_DYNAMIC_DEBUG)\n#define netif_dbg(priv, type, netdev, format, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tdynamic_dev_dbg((netdev)->dev.parent,\t\t\\\n\t\t\t\t\"%s: \" format,\t\t\t\\\n\t\t\t\tnetdev_name(netdev), ##args);\t\\\n} while (0)\n#else\n#define netif_dbg(priv, type, dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\t\\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netif_vdbg\tnetif_dbg\n#else\n#define netif_vdbg(priv, type, dev, format, args...)\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n#endif /* __KERNEL__ */\n\n#endif\t/* _LINUX_NETDEVICE_H */\n", "/*\n * \tNET3\tProtocol independent device support routines.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n * \t\tAuthors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *              \t\t\tto 2 if register_netdev gets called\n *              \t\t\tbefore net_dev_init & also removed a\n *              \t\t\tfew lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *\t\tAlan Cox\t: \tFix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *\t\tAlan Cox\t: \tFixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *              \t\t\tindefinitely on dev->refcnt\n * \t\tJ Hadi Salim\t:\t- Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <asm/uaccess.h>\n#include <asm/system.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/notifier.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <linux/rtnetlink.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/stat.h>\n#include <net/dst.h>\n#include <net/pkt_sched.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/kmod.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/wext.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <linux/pci.h>\n#include <linux/inetdevice.h>\n\n#include \"net-sysfs.h\"\n\n/* Instead of increasing this, you should create a hash table. */\n#define MAX_GRO_SKBS 8\n\n/* This should be increased if a protocol with a bigger head is added. */\n#define GRO_MAX_HEAD (MAX_HEADER + 128)\n\n/*\n *\tThe list of packet types we will receive (as opposed to discard)\n *\tand the routines to invoke.\n *\n *\tWhy 16. Because with 16 the only overlap we get on a hash of the\n *\tlow nibble of the protocol value is RARP/SNAP/X.25.\n *\n *      NOTE:  That is no longer true with the addition of VLAN tags.  Not\n *             sure which should go first, but I bet it won't make much\n *             difference if we are running VLANs.  The good news is that\n *             this protocol won't be in the list unless compiled in, so\n *             the average user (w/out VLANs) will not be adversely affected.\n *             --BLG\n *\n *\t\t0800\tIP\n *\t\t8100    802.1Q VLAN\n *\t\t0001\t802.3\n *\t\t0002\tAX.25\n *\t\t0004\t802.2\n *\t\t8035\tRARP\n *\t\t0005\tSNAP\n *\t\t0805\tX.25\n *\t\t0806\tARP\n *\t\t8137\tIPX\n *\t\t0009\tLocaltalk\n *\t\t86DD\tIPv6\n */\n\n#define PTYPE_HASH_SIZE\t(16)\n#define PTYPE_HASH_MASK\t(PTYPE_HASH_SIZE - 1)\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstatic struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstatic struct list_head ptype_all __read_mostly;\t/* Taps */\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned hash = full_name_hash(name, strnlen(name, IFNAMSIZ));\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_lock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic inline void rps_unlock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_unlock(&sd->input_pkt_queue.lock);\n#endif\n}\n\n/* Device list insertion */\nstatic int list_netdevice(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock_bh(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock_bh(&dev_base_lock);\n\treturn 0;\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\t/* Unlink dev from the device chain */\n\twrite_lock_bh(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\thlist_del_rcu(&dev->name_hlist);\n\thlist_del_rcu(&dev->index_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] =\n\t{ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE802_TR, ARPHRD_IEEE80211,\n\t ARPHRD_IEEE80211_PRISM, ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET,\n\t ARPHRD_PHONET_PIPE, ARPHRD_IEEE802154,\n\t ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] =\n\t{\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t \"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t \"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t \"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t \"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t \"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t \"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t \"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t \"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t \"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t \"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t \"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t \"_xmit_FCFABRIC\", \"_xmit_IEEE802_TR\", \"_xmit_IEEE80211\",\n\t \"_xmit_IEEE80211_PRISM\", \"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\",\n\t \"_xmit_PHONET_PIPE\", \"_xmit_IEEE802154\",\n\t \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n\n\t\tProtocol management and registration routines\n\n*******************************************************************************/\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn &ptype_all;\n\telse\n\t\treturn &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tprintk(KERN_WARNING \"dev_remove_pack: %p not found.\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n/******************************************************************************\n\n\t\t      Device Boot-time Settings Routines\n\n*******************************************************************************/\n\n/* Boot time configuration table */\nstatic struct netdev_boot_setup dev_boot_setup[NETDEV_BOOT_SETUP_MAX];\n\n/**\n *\tnetdev_boot_setup_add\t- add new setup entry\n *\t@name: name of the device\n *\t@map: configured settings for the device\n *\n *\tAdds new setup entry to the dev_boot_setup list.  The function\n *\treturns 0 on error and 1 on success.  This is a generic routine to\n *\tall netdevices.\n */\nstatic int netdev_boot_setup_add(char *name, struct ifmap *map)\n{\n\tstruct netdev_boot_setup *s;\n\tint i;\n\n\ts = dev_boot_setup;\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] == '\\0' || s[i].name[0] == ' ') {\n\t\t\tmemset(s[i].name, 0, sizeof(s[i].name));\n\t\t\tstrlcpy(s[i].name, name, IFNAMSIZ);\n\t\t\tmemcpy(&s[i].map, map, sizeof(s[i].map));\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i >= NETDEV_BOOT_SETUP_MAX ? 0 : 1;\n}\n\n/**\n *\tnetdev_boot_setup_check\t- check boot time settings\n *\t@dev: the netdevice\n *\n * \tCheck boot time settings for the device.\n *\tThe found settings are set for the device to be used\n *\tlater in the device probing.\n *\tReturns 0 if no settings found, 1 if they are.\n */\nint netdev_boot_setup_check(struct net_device *dev)\n{\n\tstruct netdev_boot_setup *s = dev_boot_setup;\n\tint i;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] != '\\0' && s[i].name[0] != ' ' &&\n\t\t    !strcmp(dev->name, s[i].name)) {\n\t\t\tdev->irq \t= s[i].map.irq;\n\t\t\tdev->base_addr \t= s[i].map.base_addr;\n\t\t\tdev->mem_start \t= s[i].map.mem_start;\n\t\t\tdev->mem_end \t= s[i].map.mem_end;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_boot_setup_check);\n\n\n/**\n *\tnetdev_boot_base\t- get address from boot time settings\n *\t@prefix: prefix for network device\n *\t@unit: id for network device\n *\n * \tCheck boot time settings for the base address of device.\n *\tThe found settings are set for the device to be used\n *\tlater in the device probing.\n *\tReturns 0 if no settings found.\n */\nunsigned long netdev_boot_base(const char *prefix, int unit)\n{\n\tconst struct netdev_boot_setup *s = dev_boot_setup;\n\tchar name[IFNAMSIZ];\n\tint i;\n\n\tsprintf(name, \"%s%d\", prefix, unit);\n\n\t/*\n\t * If device already registered then return base of 1\n\t * to indicate not to probe for this interface\n\t */\n\tif (__dev_get_by_name(&init_net, name))\n\t\treturn 1;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++)\n\t\tif (!strcmp(name, s[i].name))\n\t\t\treturn s[i].map.base_addr;\n\treturn 0;\n}\n\n/*\n * Saves at boot time configured settings for any netdevice.\n */\nint __init netdev_boot_setup(char *str)\n{\n\tint ints[5];\n\tstruct ifmap map;\n\n\tstr = get_options(str, ARRAY_SIZE(ints), ints);\n\tif (!str || !*str)\n\t\treturn 0;\n\n\t/* Save settings */\n\tmemset(&map, 0, sizeof(map));\n\tif (ints[0] > 0)\n\t\tmap.irq = ints[1];\n\tif (ints[0] > 1)\n\t\tmap.base_addr = ints[2];\n\tif (ints[0] > 2)\n\t\tmap.mem_start = ints[3];\n\tif (ints[0] > 3)\n\t\tmap.mem_end = ints[4];\n\n\t/* Add new entry to the list */\n\treturn netdev_boot_setup_add(str, &map);\n}\n\n__setup(\"netdev=\", netdev_boot_setup);\n\n/*******************************************************************************\n\n\t\t\t    Device Interface Subroutines\n\n*******************************************************************************/\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct hlist_node *p;\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry(dev, p, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n *\tdev_get_by_name_rcu\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name.\n *\tIf the name is found a pointer to the device is returned.\n * \tIf the name is not found then %NULL is returned.\n *\tThe reference counters are not incremented so the caller must be\n *\tcareful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct hlist_node *p;\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry_rcu(dev, p, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct hlist_node *p;\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, p, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct hlist_node *p;\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, p, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tfor_each_netdev(net, dev)\n\t\tif (dev->type == type)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_getfirstbyhwtype);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\tdev_get_by_flags_rcu - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trcu_read_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *dev_get_by_flags_rcu(struct net *net, unsigned short if_flags,\n\t\t\t\t    unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tret = NULL;\n\tfor_each_netdev_rcu(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_get_by_flags_rcu);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tto allow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nint dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn 0;\n\tif (strlen(name) >= IFNAMSIZ)\n\t\treturn 0;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn 0;\n\n\twhile (*name) {\n\t\tif (*name == '/' || isspace(*name))\n\t\t\treturn 0;\n\t\tname++;\n\t}\n\treturn 1;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tp = strnchr(name, IFNAMSIZ-1, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\tset_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tif (buf != name)\n\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!__dev_get_by_name(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tstruct net *net;\n\tint ret;\n\n\tBUG_ON(!dev_net(dev));\n\tnet = dev_net(dev);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_get_valid_name(struct net_device *dev, const char *name, bool fmt)\n{\n\tstruct net *net;\n\n\tBUG_ON(!dev_net(dev));\n\tnet = dev_net(dev);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (fmt && strchr(name, '%'))\n\t\treturn dev_alloc_name(dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\tif (dev->flags & IFF_UP)\n\t\treturn -EBUSY;\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0)\n\t\treturn 0;\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(dev, newname, 1);\n\tif (err < 0)\n\t\treturn err;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\treturn ret;\n\t}\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_del(&dev->name_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tprintk(KERN_ERR\n\t\t\t       \"%s: name change rollback failed: %d.\\n\",\n\t\t\t       dev->name, ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tASSERT_RTNL();\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (!len) {\n\t\tif (dev->ifalias) {\n\t\t\tkfree(dev->ifalias);\n\t\t\tdev->ifalias = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tdev->ifalias = krealloc(dev->ifalias, len + 1, GFP_KERNEL);\n\tif (!dev->ifalias)\n\t\treturn -ENOMEM;\n\n\tstrlcpy(dev->ifalias, alias, len+1);\n\treturn len;\n}\n\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tcall_netdevice_notifiers(NETDEV_CHANGE, dev);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\nint netdev_bonding_change(struct net_device *dev, unsigned long event)\n{\n\treturn call_netdevice_notifiers(event, dev);\n}\nEXPORT_SYMBOL(netdev_bonding_change);\n\n/**\n *\tdev_load \t- load a network module\n *\t@net: the applicable net namespace\n *\t@name: name of interface\n *\n *\tIf a network interface is not present and the process has suitable\n *\tprivileges this function loads the module. If module loading is not\n *\tavailable in this kernel then it becomes a nop.\n */\n\nvoid dev_load(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tint no_module;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\trcu_read_unlock();\n\n\tno_module = !dev;\n\tif (no_module && capable(CAP_NET_ADMIN))\n\t\tno_module = request_module(\"netdev-%s\", name);\n\tif (no_module && capable(CAP_SYS_MODULE)) {\n\t\tif (!request_module(\"%s\", name))\n\t\t\tpr_err(\"Loading kernel module for a network device \"\n\"with CAP_SYS_MODULE (deprecated).  Use CAP_NET_ADMIN and alias netdev-%s \"\n\"instead\\n\", name);\n\t}\n}\nEXPORT_SYMBOL(dev_load);\n\nstatic int __dev_open(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tIs it even present?\n\t */\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\tret = call_netdevice_notifiers(NETDEV_PRE_UP, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t *\tCall device private open method\n\t */\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\t/*\n\t *\tIf it went open OK then:\n\t */\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\t/*\n\t\t *\tSet the flags.\n\t\t */\n\t\tdev->flags |= IFF_UP;\n\n\t\t/*\n\t\t *\tEnable NET_DMA\n\t\t */\n\t\tnet_dmaengine_get();\n\n\t\t/*\n\t\t *\tInitialize multicasting status\n\t\t */\n\t\tdev_set_rx_mode(dev);\n\n\t\t/*\n\t\t *\tWakeup transmit queue engine\n\t\t */\n\t\tdev_activate(dev);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev:\tdevice to open\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev)\n{\n\tint ret;\n\n\t/*\n\t *\tIs it already up?\n\t */\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\t/*\n\t *\tOpen device\n\t */\n\tret = __dev_open(dev);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/*\n\t *\t... and announce new interface.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic int __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/*\n\t\t *\tTell people we are going down, so that they can\n\t\t *\tprepare to death, when device is still operating.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_clear_bit(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\t/*\n\t\t *\tDevice is now down.\n\t\t */\n\n\t\tdev->flags &= ~IFF_UP;\n\n\t\t/*\n\t\t *\tShutdown NET_DMA\n\t\t */\n\t\tnet_dmaengine_put();\n\t}\n\n\treturn 0;\n}\n\nstatic int __dev_close(struct net_device *dev)\n{\n\tint retval;\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\tretval = __dev_close_many(&single);\n\tlist_del(&single);\n\treturn retval;\n}\n\nint dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(tmp_list);\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_move(&dev->unreg_list, &tmp_list);\n\n\t__dev_close_many(head);\n\n\t/*\n\t * Tell people we are down\n\t */\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\t/* rollback_registered_many needs the complete original list */\n\tlist_splice(&tmp_list, head);\n\treturn 0;\n}\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nint dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\tdev_close_many(&single);\n\tlist_del(&single);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tif (dev->ethtool_ops && dev->ethtool_ops->get_flags &&\n\t    dev->ethtool_ops->set_flags) {\n\t\tu32 flags = dev->ethtool_ops->get_flags(dev);\n\t\tif (flags & ETH_FLAG_LRO) {\n\t\t\tflags &= ~ETH_FLAG_LRO;\n\t\t\tdev->ethtool_ops->set_flags(dev, flags);\n\t\t}\n\t}\n\tWARN_ON(dev->features & NETIF_F_LRO);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\n\nstatic int dev_boot_phase = 1;\n\n/*\n *\tDevice change register/unregister. These are not inline or static\n *\tas we export them to the world.\n */\n\n/**\n *\tregister_netdevice_notifier - register a network notifier block\n *\t@nb: notifier\n *\n *\tRegister a notifier to be called when network device events occur.\n *\tThe notifier passed is linked into the kernel structures and must\n *\tnot be reused until it has been unregistered. A negative errno code\n *\tis returned on a failure.\n *\n * \tWhen registered all registration and up events are replayed\n *\tto the new notifier to allow device to have a race free\n *\tview of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net_device *last;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\terr = nb->notifier_call(nb, NETDEV_REGISTER, dev);\n\t\t\terr = notifier_to_errno(err);\n\t\t\tif (err)\n\t\t\t\tgoto rollback;\n\n\t\t\tif (!(dev->flags & IFF_UP))\n\t\t\t\tcontinue;\n\n\t\t\tnb->notifier_call(nb, NETDEV_UP, dev);\n\t\t}\n\t}\n\nunlock:\n\trtnl_unlock();\n\treturn err;\n\nrollback:\n\tlast = dev;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev == last)\n\t\t\t\tbreak;\n\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tnb->notifier_call(nb, NETDEV_GOING_DOWN, dev);\n\t\t\t\tnb->notifier_call(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tnb->notifier_call(nb, NETDEV_UNREGISTER, dev);\n\t\t\tnb->notifier_call(nb, NETDEV_UNREGISTER_BATCH, dev);\n\t\t}\n\t}\n\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n *\tunregister_netdevice_notifier - unregister a network notifier block\n *\t@nb: notifier\n *\n *\tUnregister a notifier previously registered by\n *\tregister_netdevice_notifier(). The notifier is unlinked into the\n *\tkernel structures and may then be reused. A negative errno code\n *\tis returned on a failure.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn raw_notifier_call_chain(&netdev_chain, val, dev);\n}\n\n/* When > 0 there are consumers of rx skb time stamps */\nstatic atomic_t netstamp_needed = ATOMIC_INIT(0);\n\nvoid net_enable_timestamp(void)\n{\n\tatomic_inc(&netstamp_needed);\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n\tatomic_dec(&netstamp_needed);\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tif (atomic_read(&netstamp_needed))\n\t\t__net_timestamp(skb);\n\telse\n\t\tskb->tstamp.tv64 = 0;\n}\n\nstatic inline void net_timestamp_check(struct sk_buff *skb)\n{\n\tif (!skb->tstamp.tv64 && atomic_read(&netstamp_needed))\n\t\t__net_timestamp(skb);\n}\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tskb_orphan(skb);\n\tnf_reset(skb);\n\n\tif (unlikely(!(dev->flags & IFF_UP) ||\n\t\t     (skb->len > (dev->mtu + dev->hard_header_len + VLAN_HLEN)))) {\n\t\tatomic_long_inc(&dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\tskb_set_dev(skb, dev);\n\tskb->tstamp.tv64 = 0;\n\tskb->pkt_type = PACKET_HOST;\n\tskb->protocol = eth_type_trans(skb, dev);\n\treturn netif_rx(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tatomic_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nstatic void dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif ((ptype->dev == dev || !ptype->dev) &&\n\t\t    (ptype->af_packet_priv == NULL ||\n\t\t     (struct sock *)ptype->af_packet_priv != skb->sk)) {\n\t\t\tif (pt_prev) {\n\t\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\t\tpt_prev = ptype;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\t\tif (!skb2)\n\t\t\t\tbreak;\n\n\t\t\tnet_timestamp_set(skb2);\n\n\t\t\t/* skb->nh should be correctly\n\t\t\t   set by sender, so that the second statement is\n\t\t\t   just protection against buggy protocols.\n\t\t\t */\n\t\t\tskb_reset_mac_header(skb2);\n\n\t\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t\t    skb2->network_header > skb2->tail) {\n\t\t\t\tif (net_ratelimit())\n\t\t\t\t\tprintk(KERN_CRIT \"protocol %04x is \"\n\t\t\t\t\t       \"buggy, dev %s\\n\",\n\t\t\t\t\t       ntohs(skb2->protocol),\n\t\t\t\t\t       dev->name);\n\t\t\t\tskb_reset_network_header(skb2);\n\t\t\t}\n\n\t\t\tskb2->transport_header = skb2->network_header;\n\t\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\t\tpt_prev = ptype;\n\t\t}\n\t}\n\tif (pt_prev)\n\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\trcu_read_unlock();\n}\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater then real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tint rc;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (txq < dev->real_num_tx_queues)\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n\t}\n\n\tdev->real_num_tx_queues = txq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_RPS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\nstatic inline void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = &__get_cpu_var(softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nvoid dev_kfree_skb_irq(struct sk_buff *skb)\n{\n\tif (atomic_dec_and_test(&skb->users)) {\n\t\tstruct softnet_data *sd;\n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\tsd = &__get_cpu_var(softnet_data);\n\t\tskb->next = sd->completion_queue;\n\t\tsd->completion_queue = skb;\n\t\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\t\tlocal_irq_restore(flags);\n\t}\n}\nEXPORT_SYMBOL(dev_kfree_skb_irq);\n\nvoid dev_kfree_skb_any(struct sk_buff *skb)\n{\n\tif (in_irq() || irqs_disabled())\n\t\tdev_kfree_skb_irq(skb);\n\telse\n\t\tdev_kfree_skb(skb);\n}\nEXPORT_SYMBOL(dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/**\n * skb_dev_set -- assign a new device to a buffer\n * @skb: buffer for the new device\n * @dev: network device\n *\n * If an skb is owned by a device already, we have to reset\n * all data private to the namespace a device belongs to\n * before assigning it a new device.\n */\n#ifdef CONFIG_NET_NS\nvoid skb_set_dev(struct sk_buff *skb, struct net_device *dev)\n{\n\tskb_dst_drop(skb);\n\tif (skb->dev && !net_eq(dev_net(skb->dev), dev_net(dev))) {\n\t\tsecpath_reset(skb);\n\t\tnf_reset(skb);\n\t\tskb_init_secmark(skb);\n\t\tskb->mark = 0;\n\t\tskb->priority = 0;\n\t\tskb->nf_trace = 0;\n\t\tskb->ipvs_property = 0;\n#ifdef CONFIG_NET_SCHED\n\t\tskb->tc_index = 0;\n#endif\n\t}\n\tskb->dev = dev;\n}\nEXPORT_SYMBOL(skb_set_dev);\n#endif /* CONFIG_NET_NS */\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_shinfo(skb)->gso_size)) {\n\t\t/* Let GSO fix up the checksum. */\n\t\tgoto out_set_summed;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tBUG_ON(offset >= skb_headlen(skb));\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tBUG_ON(offset + sizeof(__sum16) > skb_headlen(skb));\n\n\tif (skb_cloned(skb) &&\n\t    !skb_clone_writable(skb, offset + sizeof(__sum16))) {\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\n/**\n *\tskb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n */\nstruct sk_buff *skb_gso_segment(struct sk_buff *skb, int features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EPROTONOSUPPORT);\n\tstruct packet_type *ptype;\n\t__be16 type = skb->protocol;\n\tint vlan_depth = ETH_HLEN;\n\tint err;\n\n\twhile (type == htons(ETH_P_8021Q)) {\n\t\tstruct vlan_hdr *vh;\n\n\t\tif (unlikely(!pskb_may_pull(skb, vlan_depth + VLAN_HLEN)))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tvh = (struct vlan_hdr *)(skb->data + vlan_depth);\n\t\ttype = vh->h_vlan_encapsulated_proto;\n\t\tvlan_depth += VLAN_HLEN;\n\t}\n\n\tskb_reset_mac_header(skb);\n\tskb->mac_len = skb->network_header - skb->mac_header;\n\t__skb_pull(skb, skb->mac_len);\n\n\tif (unlikely(skb->ip_summed != CHECKSUM_PARTIAL)) {\n\t\tstruct net_device *dev = skb->dev;\n\t\tstruct ethtool_drvinfo info = {};\n\n\t\tif (dev && dev->ethtool_ops && dev->ethtool_ops->get_drvinfo)\n\t\t\tdev->ethtool_ops->get_drvinfo(dev, &info);\n\n\t\tWARN(1, \"%s: caps=(0x%lx, 0x%lx) len=%d data_len=%d ip_summed=%d\\n\",\n\t\t     info.driver, dev ? dev->features : 0L,\n\t\t     skb->sk ? skb->sk->sk_route_caps : 0L,\n\t\t     skb->len, skb->data_len, skb->ip_summed);\n\n\t\tif (skb_header_cloned(skb) &&\n\t\t    (err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC)))\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype,\n\t\t\t&ptype_base[ntohs(type) & PTYPE_HASH_MASK], list) {\n\t\tif (ptype->type == type && !ptype->dev && ptype->gso_segment) {\n\t\t\tif (unlikely(skb->ip_summed != CHECKSUM_PARTIAL)) {\n\t\t\t\terr = ptype->gso_send_check(skb);\n\t\t\t\tsegs = ERR_PTR(err);\n\t\t\t\tif (err || skb_gso_ok(skb, features))\n\t\t\t\t\tbreak;\n\t\t\t\t__skb_push(skb, (skb->data -\n\t\t\t\t\t\t skb_network_header(skb)));\n\t\t\t}\n\t\t\tsegs = ptype->gso_segment(skb, features);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\n\treturn segs;\n}\nEXPORT_SYMBOL(skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev)\n{\n\tif (net_ratelimit()) {\n\t\tprintk(KERN_ERR \"%s: hw csum failure.\\n\",\n\t\t\tdev ? dev->name : \"<unknown>\");\n\t\tdump_stack();\n\t}\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* Actually, we should eliminate this check as soon as we know, that:\n * 1. IOMMU is present and allows to map all the memory.\n * 2. No high memory really exists on this machine.\n */\n\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tif (PageHighMem(skb_shinfo(skb)->frags[i].page))\n\t\t\t\treturn 1;\n\t}\n\n\tif (PCI_DMA_BUS_IS_PHYS) {\n\t\tstruct device *pdev = dev->dev.parent;\n\n\t\tif (!pdev)\n\t\t\treturn 0;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tdma_addr_t addr = page_to_phys(skb_shinfo(skb)->frags[i].page);\n\t\t\tif (!pdev->dma_mask || addr + PAGE_SIZE - 1 > *pdev->dma_mask)\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\nstruct dev_gso_cb {\n\tvoid (*destructor)(struct sk_buff *skb);\n};\n\n#define DEV_GSO_CB(skb) ((struct dev_gso_cb *)(skb)->cb)\n\nstatic void dev_gso_skb_destructor(struct sk_buff *skb)\n{\n\tstruct dev_gso_cb *cb;\n\n\tdo {\n\t\tstruct sk_buff *nskb = skb->next;\n\n\t\tskb->next = nskb->next;\n\t\tnskb->next = NULL;\n\t\tkfree_skb(nskb);\n\t} while (skb->next);\n\n\tcb = DEV_GSO_CB(skb);\n\tif (cb->destructor)\n\t\tcb->destructor(skb);\n}\n\n/**\n *\tdev_gso_segment - Perform emulated hardware segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: device features as applicable to this skb\n *\n *\tThis function segments the given skb and stores the list of segments\n *\tin skb->next.\n */\nstatic int dev_gso_segment(struct sk_buff *skb, int features)\n{\n\tstruct sk_buff *segs;\n\n\tsegs = skb_gso_segment(skb, features);\n\n\t/* Verifying header integrity only. */\n\tif (!segs)\n\t\treturn 0;\n\n\tif (IS_ERR(segs))\n\t\treturn PTR_ERR(segs);\n\n\tskb->next = segs;\n\tDEV_GSO_CB(skb)->destructor = skb->destructor;\n\tskb->destructor = dev_gso_skb_destructor;\n\n\treturn 0;\n}\n\n/*\n * Try to orphan skb early, right before transmission by the device.\n * We cannot orphan skb if tx timestamp is requested or the sk-reference\n * is needed on driver level for other reasons, e.g. see net/can/raw.c\n */\nstatic inline void skb_orphan_try(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (sk && !skb_shinfo(skb)->tx_flags) {\n\t\t/* skb_tx_hash() wont be able to get sk.\n\t\t * We copy sk_hash into skb->rxhash\n\t\t */\n\t\tif (!skb->rxhash)\n\t\t\tskb->rxhash = sk->sk_hash;\n\t\tskb_orphan(skb);\n\t}\n}\n\nstatic bool can_checksum_protocol(unsigned long features, __be16 protocol)\n{\n\treturn ((features & NETIF_F_GEN_CSUM) ||\n\t\t((features & NETIF_F_V4_CSUM) &&\n\t\t protocol == htons(ETH_P_IP)) ||\n\t\t((features & NETIF_F_V6_CSUM) &&\n\t\t protocol == htons(ETH_P_IPV6)) ||\n\t\t((features & NETIF_F_FCOE_CRC) &&\n\t\t protocol == htons(ETH_P_FCOE)));\n}\n\nstatic int harmonize_features(struct sk_buff *skb, __be16 protocol, int features)\n{\n\tif (!can_checksum_protocol(features, protocol)) {\n\t\tfeatures &= ~NETIF_F_ALL_CSUM;\n\t\tfeatures &= ~NETIF_F_SG;\n\t} else if (illegal_highdma(skb->dev, skb)) {\n\t\tfeatures &= ~NETIF_F_SG;\n\t}\n\n\treturn features;\n}\n\nint netif_skb_features(struct sk_buff *skb)\n{\n\t__be16 protocol = skb->protocol;\n\tint features = skb->dev->features;\n\n\tif (protocol == htons(ETH_P_8021Q)) {\n\t\tstruct vlan_ethhdr *veh = (struct vlan_ethhdr *)skb->data;\n\t\tprotocol = veh->h_vlan_encapsulated_proto;\n\t} else if (!vlan_tx_tag_present(skb)) {\n\t\treturn harmonize_features(skb, protocol, features);\n\t}\n\n\tfeatures &= (skb->dev->vlan_features | NETIF_F_HW_VLAN_TX);\n\n\tif (protocol != htons(ETH_P_8021Q)) {\n\t\treturn harmonize_features(skb, protocol, features);\n\t} else {\n\t\tfeatures &= NETIF_F_SG | NETIF_F_HIGHDMA | NETIF_F_FRAGLIST |\n\t\t\t\tNETIF_F_GEN_CSUM | NETIF_F_HW_VLAN_TX;\n\t\treturn harmonize_features(skb, protocol, features);\n\t}\n}\nEXPORT_SYMBOL(netif_skb_features);\n\n/*\n * Returns true if either:\n *\t1. skb has frag_list and the device doesn't support FRAGLIST, or\n *\t2. skb is fragmented and the device does not support SG, or if\n *\t   at least one of fragments is in highmem and device does not\n *\t   support DMA from it.\n */\nstatic inline int skb_needs_linearize(struct sk_buff *skb,\n\t\t\t\t      int features)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t\t\t((skb_has_frag_list(skb) &&\n\t\t\t\t!(features & NETIF_F_FRAGLIST)) ||\n\t\t\t(skb_shinfo(skb)->nr_frags &&\n\t\t\t\t!(features & NETIF_F_SG)));\n}\n\nint dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\tstruct netdev_queue *txq)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint rc = NETDEV_TX_OK;\n\n\tif (likely(!skb->next)) {\n\t\tint features;\n\n\t\t/*\n\t\t * If device doesnt need skb->dst, release it right now while\n\t\t * its hot in this cpu cache\n\t\t */\n\t\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\t\tskb_dst_drop(skb);\n\n\t\tif (!list_empty(&ptype_all))\n\t\t\tdev_queue_xmit_nit(skb, dev);\n\n\t\tskb_orphan_try(skb);\n\n\t\tfeatures = netif_skb_features(skb);\n\n\t\tif (vlan_tx_tag_present(skb) &&\n\t\t    !(features & NETIF_F_HW_VLAN_TX)) {\n\t\t\tskb = __vlan_put_tag(skb, vlan_tx_tag_get(skb));\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\n\t\t\tskb->vlan_tci = 0;\n\t\t}\n\n\t\tif (netif_needs_gso(skb, features)) {\n\t\t\tif (unlikely(dev_gso_segment(skb, features)))\n\t\t\t\tgoto out_kfree_skb;\n\t\t\tif (skb->next)\n\t\t\t\tgoto gso;\n\t\t} else {\n\t\t\tif (skb_needs_linearize(skb, features) &&\n\t\t\t    __skb_linearize(skb))\n\t\t\t\tgoto out_kfree_skb;\n\n\t\t\t/* If packet is not checksummed and device does not\n\t\t\t * support checksumming for this protocol, complete\n\t\t\t * checksumming here.\n\t\t\t */\n\t\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\tskb_checksum_start_offset(skb));\n\t\t\t\tif (!(features & NETIF_F_ALL_CSUM) &&\n\t\t\t\t     skb_checksum_help(skb))\n\t\t\t\t\tgoto out_kfree_skb;\n\t\t\t}\n\t\t}\n\n\t\trc = ops->ndo_start_xmit(skb, dev);\n\t\ttrace_net_dev_xmit(skb, rc);\n\t\tif (rc == NETDEV_TX_OK)\n\t\t\ttxq_trans_update(txq);\n\t\treturn rc;\n\t}\n\ngso:\n\tdo {\n\t\tstruct sk_buff *nskb = skb->next;\n\n\t\tskb->next = nskb->next;\n\t\tnskb->next = NULL;\n\n\t\t/*\n\t\t * If device doesnt need nskb->dst, release it right now while\n\t\t * its hot in this cpu cache\n\t\t */\n\t\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\t\tskb_dst_drop(nskb);\n\n\t\trc = ops->ndo_start_xmit(nskb, dev);\n\t\ttrace_net_dev_xmit(nskb, rc);\n\t\tif (unlikely(rc != NETDEV_TX_OK)) {\n\t\t\tif (rc & ~NETDEV_TX_MASK)\n\t\t\t\tgoto out_kfree_gso_skb;\n\t\t\tnskb->next = skb->next;\n\t\t\tskb->next = nskb;\n\t\t\treturn rc;\n\t\t}\n\t\ttxq_trans_update(txq);\n\t\tif (unlikely(netif_tx_queue_stopped(txq) && skb->next))\n\t\t\treturn NETDEV_TX_BUSY;\n\t} while (skb->next);\n\nout_kfree_gso_skb:\n\tif (likely(skb->next == NULL))\n\t\tskb->destructor = DEV_GSO_CB(skb)->destructor;\nout_kfree_skb:\n\tkfree_skb(skb);\nout:\n\treturn rc;\n}\n\nstatic u32 hashrnd __read_mostly;\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nu16 __skb_tx_hash(const struct net_device *dev, const struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues)\n{\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\thash = skb_get_rx_queue(skb);\n\t\twhile (unlikely(hash >= num_tx_queues))\n\t\t\thash -= num_tx_queues;\n\t\treturn hash;\n\t}\n\n\tif (skb->sk && skb->sk->sk_hash)\n\t\thash = skb->sk->sk_hash;\n\telse\n\t\thash = (__force u16) skb->protocol ^ skb->rxhash;\n\thash = jhash_1word(hash, hashrnd);\n\n\treturn (u16) (((u64) hash * num_tx_queues) >> 32);\n}\nEXPORT_SYMBOL(__skb_tx_hash);\n\nstatic inline u16 dev_cap_txqueue(struct net_device *dev, u16 queue_index)\n{\n\tif (unlikely(queue_index >= dev->real_num_tx_queues)) {\n\t\tif (net_ratelimit()) {\n\t\t\tpr_warning(\"%s selects TX queue %d, but \"\n\t\t\t\t\"real number of TX queues is %d\\n\",\n\t\t\t\tdev->name, queue_index, dev->real_num_tx_queues);\n\t\t}\n\t\treturn 0;\n\t}\n\treturn queue_index;\n}\n\nstatic inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\trcu_read_lock();\n\tdev_maps = rcu_dereference(dev->xps_maps);\n\tif (dev_maps) {\n\t\tmap = rcu_dereference(\n\t\t    dev_maps->cpu_map[raw_smp_processor_id()]);\n\t\tif (map) {\n\t\t\tif (map->len == 1)\n\t\t\t\tqueue_index = map->queues[0];\n\t\t\telse {\n\t\t\t\tu32 hash;\n\t\t\t\tif (skb->sk && skb->sk->sk_hash)\n\t\t\t\t\thash = skb->sk->sk_hash;\n\t\t\t\telse\n\t\t\t\t\thash = (__force u16) skb->protocol ^\n\t\t\t\t\t    skb->rxhash;\n\t\t\t\thash = jhash_1word(hash, hashrnd);\n\t\t\t\tqueue_index = map->queues[\n\t\t\t\t    ((u64)hash * map->len) >> 32];\n\t\t\t}\n\t\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\t\tqueue_index = -1;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nstatic struct netdev_queue *dev_pick_tx(struct net_device *dev,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tint queue_index;\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (dev->real_num_tx_queues == 1)\n\t\tqueue_index = 0;\n\telse if (ops->ndo_select_queue) {\n\t\tqueue_index = ops->ndo_select_queue(dev, skb);\n\t\tqueue_index = dev_cap_txqueue(dev, queue_index);\n\t} else {\n\t\tstruct sock *sk = skb->sk;\n\t\tqueue_index = sk_tx_queue_get(sk);\n\n\t\tif (queue_index < 0 || skb->ooo_okay ||\n\t\t    queue_index >= dev->real_num_tx_queues) {\n\t\t\tint old_index = queue_index;\n\n\t\t\tqueue_index = get_xps_queue(dev, skb);\n\t\t\tif (queue_index < 0)\n\t\t\t\tqueue_index = skb_tx_hash(dev, skb);\n\n\t\t\tif (queue_index != old_index && sk) {\n\t\t\t\tstruct dst_entry *dst =\n\t\t\t\t    rcu_dereference_check(sk->sk_dst_cache, 1);\n\n\t\t\t\tif (dst && skb_dst(skb) == dst)\n\t\t\t\t\tsk_tx_queue_set(sk, queue_index);\n\t\t\t}\n\t\t}\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tbool contended = qdisc_is_running(q);\n\tint rc;\n\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits __QDISC_STATE_RUNNING owner to get the lock more often\n\t * and dequeue packets faster.\n\t */\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\tkfree_skb(skb);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\t\tif (!(dev->priv_flags & IFF_XMIT_DST_RELEASE))\n\t\t\tskb_dst_force(skb);\n\n\t\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t} else\n\t\t\tqdisc_run_end(q);\n\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\tskb_dst_force(skb);\n\t\trc = qdisc_enqueue_root(skb, q);\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\nstatic DEFINE_PER_CPU(int, xmit_recursion);\n#define RECURSION_LIMIT 10\n\n/**\n *\tdev_queue_xmit - transmit a buffer\n *\t@skb: buffer to transmit\n *\n *\tQueue a buffer for transmission to a network device. The caller must\n *\thave set the device and priority and built the buffer before calling\n *\tthis function. The function can be called from an interrupt.\n *\n *\tA negative errno code is returned on a failure. A success does not\n *\tguarantee the frame will be transmitted as it may be dropped due\n *\tto congestion or traffic shaping.\n *\n * -----------------------------------------------------------------------------------\n *      I notice this method can also return errors from the queue disciplines,\n *      including NET_XMIT_DROP, which is a positive value.  So, errors can also\n *      be positive.\n *\n *      Regardless of the return value, the skb is consumed, so it is currently\n *      difficult to retry a send to this method.  (You can bump the ref count\n *      before sending to hold a reference for retry if you are careful.)\n *\n *      When calling this method, interrupts MUST be enabled.  This is because\n *      the BH enable code must have IRQs enabled so that it will not deadlock.\n *          --BLG\n */\nint dev_queue_xmit(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\ttxq = dev_pick_tx(dev, skb);\n\tq = rcu_dereference_bh(txq->qdisc);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_verd = SET_TC_AT(skb->tc_verd, AT_EGRESS);\n#endif\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t   loopback, all the sorts of tunnels...\n\n\t   Really, it is unlikely that netif_tx_lock protection is necessary\n\t   here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t   counters.)\n\t   However, it is possible, that they rely on protection\n\t   made by us here.\n\n\t   Check this and shot the lock. It is not prone from deadlocks.\n\t   Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\tif (txq->xmit_lock_owner != cpu) {\n\n\t\t\tif (__this_cpu_read(xmit_recursion) > RECURSION_LIMIT)\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_tx_queue_stopped(txq)) {\n\t\t\t\t__this_cpu_inc(xmit_recursion);\n\t\t\t\trc = dev_hard_start_xmit(skb, dev, txq);\n\t\t\t\t__this_cpu_dec(xmit_recursion);\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_CRIT \"Virtual device %s asks to \"\n\t\t\t\t       \"queue packet!\\n\", dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_CRIT \"Dead loop on virtual device \"\n\t\t\t\t       \"%s, fix it urgently!\\n\", dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tkfree_skb(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\nEXPORT_SYMBOL(dev_queue_xmit);\n\n\n/*=======================================================================\n\t\t\tReceiver routines\n  =======================================================================*/\n\nint netdev_max_backlog __read_mostly = 1000;\nint netdev_tstamp_prequeue __read_mostly = 1;\nint netdev_budget __read_mostly = 300;\nint weight_p __read_mostly = 64;            /* old backlog weight */\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n/*\n * __skb_get_rxhash: calculate a flow hash based on src/dst addresses\n * and src/dst port numbers. Returns a non-zero hash number on success\n * and 0 on failure.\n */\n__u32 __skb_get_rxhash(struct sk_buff *skb)\n{\n\tint nhoff, hash = 0, poff;\n\tstruct ipv6hdr *ip6;\n\tstruct iphdr *ip;\n\tu8 ip_proto;\n\tu32 addr1, addr2, ihl;\n\tunion {\n\t\tu32 v32;\n\t\tu16 v16[2];\n\t} ports;\n\n\tnhoff = skb_network_offset(skb);\n\n\tswitch (skb->protocol) {\n\tcase __constant_htons(ETH_P_IP):\n\t\tif (!pskb_may_pull(skb, sizeof(*ip) + nhoff))\n\t\t\tgoto done;\n\n\t\tip = (struct iphdr *) (skb->data + nhoff);\n\t\tif (ip->frag_off & htons(IP_MF | IP_OFFSET))\n\t\t\tip_proto = 0;\n\t\telse\n\t\t\tip_proto = ip->protocol;\n\t\taddr1 = (__force u32) ip->saddr;\n\t\taddr2 = (__force u32) ip->daddr;\n\t\tihl = ip->ihl;\n\t\tbreak;\n\tcase __constant_htons(ETH_P_IPV6):\n\t\tif (!pskb_may_pull(skb, sizeof(*ip6) + nhoff))\n\t\t\tgoto done;\n\n\t\tip6 = (struct ipv6hdr *) (skb->data + nhoff);\n\t\tip_proto = ip6->nexthdr;\n\t\taddr1 = (__force u32) ip6->saddr.s6_addr32[3];\n\t\taddr2 = (__force u32) ip6->daddr.s6_addr32[3];\n\t\tihl = (40 >> 2);\n\t\tbreak;\n\tdefault:\n\t\tgoto done;\n\t}\n\n\tports.v32 = 0;\n\tpoff = proto_ports_offset(ip_proto);\n\tif (poff >= 0) {\n\t\tnhoff += ihl * 4 + poff;\n\t\tif (pskb_may_pull(skb, nhoff + 4)) {\n\t\t\tports.v32 = * (__force u32 *) (skb->data + nhoff);\n\t\t\tif (ports.v16[1] < ports.v16[0])\n\t\t\t\tswap(ports.v16[0], ports.v16[1]);\n\t\t}\n\t}\n\n\t/* get a consistent hash (same value on both flow directions) */\n\tif (addr2 < addr1)\n\t\tswap(addr1, addr2);\n\n\thash = jhash_3words(addr1, addr2, ports.v32, hashrnd);\n\tif (!hash)\n\t\thash = 1;\n\ndone:\n\treturn hash;\n}\nEXPORT_SYMBOL(__skb_get_rxhash);\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tstruct netdev_rx_queue *rxqueue;\n\tstruct rps_map *map;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_sock_flow_table *sock_flow_table;\n\tint cpu = -1;\n\tu16 tcpu;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue = dev->_rx + index;\n\t} else\n\t\trxqueue = dev->_rx;\n\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (map) {\n\t\tif (map->len == 1 &&\n\t\t    !rcu_dereference_raw(rxqueue->rps_flow_table)) {\n\t\t\ttcpu = map->cpus[0];\n\t\t\tif (cpu_online(tcpu))\n\t\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t} else if (!rcu_dereference_raw(rxqueue->rps_flow_table)) {\n\t\tgoto done;\n\t}\n\n\tskb_reset_network_header(skb);\n\tif (!skb_get_rxhash(skb))\n\t\tgoto done;\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tu16 next_cpu;\n\t\tstruct rps_dev_flow *rflow;\n\n\t\trflow = &flow_table->flows[skb->rxhash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\tnext_cpu = sock_flow_table->ents[skb->rxhash &\n\t\t    sock_flow_table->mask];\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (equal to RPS_NO_CPU).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu == RPS_NO_CPU || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = rflow->cpu = next_cpu;\n\t\t\tif (tcpu != RPS_NO_CPU)\n\t\t\t\trflow->last_qtail = per_cpu(softnet_data,\n\t\t\t\t    tcpu).input_queue_head;\n\t\t}\n\t\tif (tcpu != RPS_NO_CPU && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (map) {\n\t\ttcpu = map->cpus[((u64) skb->rxhash * map->len) >> 32];\n\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int rps_ipi_queued(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *mysd = &__get_cpu_var(softnet_data);\n\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\treturn 0;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tlocal_irq_save(flags);\n\n\trps_lock(sd);\n\tif (skb_queue_len(&sd->input_pkt_queue) <= netdev_max_backlog) {\n\t\tif (skb_queue_len(&sd->input_pkt_queue)) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock(sd);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state)) {\n\t\t\tif (!rps_ipi_queued(sd))\n\t\t\t\t____napi_schedule(sd, &sd->backlog);\n\t\t}\n\t\tgoto enqueue;\n\t}\n\n\tsd->dropped++;\n\trps_unlock(sd);\n\n\tlocal_irq_restore(flags);\n\n\tatomic_long_inc(&skb->dev->rx_dropped);\n\tkfree_skb(skb);\n\treturn NET_RX_DROP;\n}\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process.  It always succeeds. The buffer\n *\tmay be dropped during processing for congestion control or by the\n *\tprotocol layers.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\n\nint netif_rx(struct sk_buff *skb)\n{\n\tint ret;\n\n\t/* if netpoll wants it, pretend we never saw it */\n\tif (netpoll_rx(skb))\n\t\treturn NET_RX_DROP;\n\n\tif (netdev_tstamp_prequeue)\n\t\tnet_timestamp_check(skb);\n\n\ttrace_netif_rx(skb);\n#ifdef CONFIG_RPS\n\t{\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\t}\n#else\n\t{\n\t\tunsigned int qtail;\n\t\tret = enqueue_to_backlog(skb, get_cpu(), &qtail);\n\t\tput_cpu();\n\t}\n#endif\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_rx);\n\nint netif_rx_ni(struct sk_buff *skb)\n{\n\tint err;\n\n\tpreempt_disable();\n\terr = netif_rx(skb);\n\tif (local_softirq_pending())\n\t\tdo_softirq();\n\tpreempt_enable();\n\n\treturn err;\n}\nEXPORT_SYMBOL(netif_rx_ni);\n\nstatic void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = &__get_cpu_var(softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(atomic_read(&skb->users));\n\t\t\ttrace_kfree_skb(skb, net_tx_action);\n\t\t\t__kfree_skb(skb);\n\t\t}\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock;\n\n\t\t\thead = head->next_sched;\n\n\t\t\troot_lock = qdisc_lock(q);\n\t\t\tif (spin_trylock(root_lock)) {\n\t\t\t\tsmp_mb__before_clear_bit();\n\t\t\t\tclear_bit(__QDISC_STATE_SCHED,\n\t\t\t\t\t  &q->state);\n\t\t\t\tqdisc_run(q);\n\t\t\t\tspin_unlock(root_lock);\n\t\t\t} else {\n\t\t\t\tif (!test_bit(__QDISC_STATE_DEACTIVATED,\n\t\t\t\t\t      &q->state)) {\n\t\t\t\t\t__netif_reschedule(q);\n\t\t\t\t} else {\n\t\t\t\t\tsmp_mb__before_clear_bit();\n\t\t\t\t\tclear_bit(__QDISC_STATE_SCHED,\n\t\t\t\t\t\t  &q->state);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n#if (defined(CONFIG_BRIDGE) || defined(CONFIG_BRIDGE_MODULE)) && \\\n    (defined(CONFIG_ATM_LANE) || defined(CONFIG_ATM_LANE_MODULE))\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\n#ifdef CONFIG_NET_CLS_ACT\n/* TODO: Maybe we should just force sch_ingress to be compiled in\n * when CONFIG_NET_CLS_ACT is? otherwise some useless instructions\n * a compare and 2 stores extra right now if we dont have it on\n * but have CONFIG_NET_CLS_ACT\n * NOTE: This doesnt stop any functionality; if you dont have\n * the ingress scheduler, you just cant add policies on ingress.\n *\n */\nstatic int ing_filter(struct sk_buff *skb, struct netdev_queue *rxq)\n{\n\tstruct net_device *dev = skb->dev;\n\tu32 ttl = G_TC_RTTL(skb->tc_verd);\n\tint result = TC_ACT_OK;\n\tstruct Qdisc *q;\n\n\tif (unlikely(MAX_RED_LOOP < ttl++)) {\n\t\tif (net_ratelimit())\n\t\t\tpr_warning( \"Redir loop detected Dropping packet (%d->%d)\\n\",\n\t\t\t       skb->skb_iif, dev->ifindex);\n\t\treturn TC_ACT_SHOT;\n\t}\n\n\tskb->tc_verd = SET_TC_RTTL(skb->tc_verd, ttl);\n\tskb->tc_verd = SET_TC_AT(skb->tc_verd, AT_INGRESS);\n\n\tq = rxq->qdisc;\n\tif (q != &noop_qdisc) {\n\t\tspin_lock(qdisc_lock(q));\n\t\tif (likely(!test_bit(__QDISC_STATE_DEACTIVATED, &q->state)))\n\t\t\tresult = qdisc_enqueue_root(skb, q);\n\t\tspin_unlock(qdisc_lock(q));\n\t}\n\n\treturn result;\n}\n\nstatic inline struct sk_buff *handle_ing(struct sk_buff *skb,\n\t\t\t\t\t struct packet_type **pt_prev,\n\t\t\t\t\t int *ret, struct net_device *orig_dev)\n{\n\tstruct netdev_queue *rxq = rcu_dereference(skb->dev->ingress_queue);\n\n\tif (!rxq || rxq->qdisc == &noop_qdisc)\n\t\tgoto out;\n\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tswitch (ing_filter(skb, rxq)) {\n\tcase TC_ACT_SHOT:\n\tcase TC_ACT_STOLEN:\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\nout:\n\tskb->tc_verd = 0;\n\treturn skb;\n}\n#endif\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive hander for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tASSERT_RTNL();\n\n\tif (dev->rx_handler)\n\t\treturn -EBUSY;\n\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive hander from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\trcu_assign_pointer(dev->rx_handler, NULL);\n\trcu_assign_pointer(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\nstatic inline void skb_bond_set_mac_by_master(struct sk_buff *skb,\n\t\t\t\t\t      struct net_device *master)\n{\n\tif (skb->pkt_type == PACKET_HOST) {\n\t\tu16 *dest = (u16 *) eth_hdr(skb)->h_dest;\n\n\t\tmemcpy(dest, master->dev_addr, ETH_ALEN);\n\t}\n}\n\n/* On bonding slaves other than the currently active slave, suppress\n * duplicates except for 802.3ad ETH_P_SLOW, alb non-mcast/bcast, and\n * ARP on active-backup slaves with arp_validate enabled.\n */\nint __skb_bond_should_drop(struct sk_buff *skb, struct net_device *master)\n{\n\tstruct net_device *dev = skb->dev;\n\n\tif (master->priv_flags & IFF_MASTER_ARPMON)\n\t\tdev->last_rx = jiffies;\n\n\tif ((master->priv_flags & IFF_MASTER_ALB) &&\n\t    (master->priv_flags & IFF_BRIDGE_PORT)) {\n\t\t/* Do address unmangle. The local destination address\n\t\t * will be always the one master has. Provides the right\n\t\t * functionality in a bridge.\n\t\t */\n\t\tskb_bond_set_mac_by_master(skb, master);\n\t}\n\n\tif (dev->priv_flags & IFF_SLAVE_INACTIVE) {\n\t\tif ((dev->priv_flags & IFF_SLAVE_NEEDARP) &&\n\t\t    skb->protocol == __cpu_to_be16(ETH_P_ARP))\n\t\t\treturn 0;\n\n\t\tif (master->priv_flags & IFF_MASTER_ALB) {\n\t\t\tif (skb->pkt_type != PACKET_BROADCAST &&\n\t\t\t    skb->pkt_type != PACKET_MULTICAST)\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (master->priv_flags & IFF_MASTER_8023AD &&\n\t\t    skb->protocol == __cpu_to_be16(ETH_P_SLOW))\n\t\t\treturn 0;\n\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__skb_bond_should_drop);\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct net_device *orig_dev;\n\tstruct net_device *master;\n\tstruct net_device *null_or_orig;\n\tstruct net_device *orig_or_bond;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tif (!netdev_tstamp_prequeue)\n\t\tnet_timestamp_check(skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\t/* if we've gotten here through NAPI, check netpoll */\n\tif (netpoll_receive_skb(skb))\n\t\treturn NET_RX_DROP;\n\n\tif (!skb->skb_iif)\n\t\tskb->skb_iif = skb->dev->ifindex;\n\n\t/*\n\t * bonding note: skbs received on inactive slaves should only\n\t * be delivered to pkt handlers that are exact matches.  Also\n\t * the deliver_no_wcard flag will be set.  If packet handlers\n\t * are sensitive to duplicate packets these skbs will need to\n\t * be dropped at the handler.\n\t */\n\tnull_or_orig = NULL;\n\torig_dev = skb->dev;\n\tmaster = ACCESS_ONCE(orig_dev->master);\n\tif (skb->deliver_no_wcard)\n\t\tnull_or_orig = orig_dev;\n\telse if (master) {\n\t\tif (skb_bond_should_drop(skb, master)) {\n\t\t\tskb->deliver_no_wcard = 1;\n\t\t\tnull_or_orig = orig_dev; /* deliver only exact match */\n\t\t} else\n\t\t\tskb->dev = master;\n\t}\n\n\t__this_cpu_inc(softnet_data.processed);\n\tskb_reset_network_header(skb);\n\tskb_reset_transport_header(skb);\n\tskb->mac_len = skb->network_header - skb->mac_header;\n\n\tpt_prev = NULL;\n\n\trcu_read_lock();\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (skb->tc_verd & TC_NCLS) {\n\t\tskb->tc_verd = CLR_TC_NCLS(skb->tc_verd);\n\t\tgoto ncls;\n\t}\n#endif\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (ptype->dev == null_or_orig || ptype->dev == skb->dev ||\n\t\t    ptype->dev == orig_dev) {\n\t\t\tif (pt_prev)\n\t\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = ptype;\n\t\t}\n\t}\n\n#ifdef CONFIG_NET_CLS_ACT\n\tskb = handle_ing(skb, &pt_prev, &ret, orig_dev);\n\tif (!skb)\n\t\tgoto out;\nncls:\n#endif\n\n\t/* Handle special case of bridge or macvlan */\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tskb = rx_handler(skb);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_hwaccel_do_receive(&skb)) {\n\t\t\tret = __netif_receive_skb(skb);\n\t\t\tgoto out;\n\t\t} else if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * Make sure frames received on VLAN interfaces stacked on\n\t * bonding interfaces still make their way to any base bonding\n\t * device that may have registered for a specific ptype.  The\n\t * handler may have to adjust skb->dev and orig_dev.\n\t */\n\torig_or_bond = orig_dev;\n\tif ((skb->dev->priv_flags & IFF_802_1Q_VLAN) &&\n\t    (vlan_dev_real_dev(skb->dev)->priv_flags & IFF_BONDING)) {\n\t\torig_or_bond = vlan_dev_real_dev(skb->dev);\n\t}\n\n\ttype = skb->protocol;\n\tlist_for_each_entry_rcu(ptype,\n\t\t\t&ptype_base[ntohs(type) & PTYPE_HASH_MASK], list) {\n\t\tif (ptype->type == type && (ptype->dev == null_or_orig ||\n\t\t     ptype->dev == skb->dev || ptype->dev == orig_dev ||\n\t\t     ptype->dev == orig_or_bond)) {\n\t\t\tif (pt_prev)\n\t\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = ptype;\n\t\t}\n\t}\n\n\tif (pt_prev) {\n\t\tret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t} else {\n\t\tatomic_long_inc(&skb->dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\tif (netdev_tstamp_prequeue)\n\t\tnet_timestamp_check(skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n#ifdef CONFIG_RPS\n\t{\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu, ret;\n\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t} else {\n\t\t\trcu_read_unlock();\n\t\t\tret = __netif_receive_skb(skb);\n\t\t}\n\n\t\treturn ret;\n\t}\n#else\n\treturn __netif_receive_skb(skb);\n#endif\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/* Network device is going away, flush any packets still pending\n * Called with irqs disabled.\n */\nstatic void flush_backlog(void *arg)\n{\n\tstruct net_device *dev = arg;\n\tstruct softnet_data *sd = &__get_cpu_var(softnet_data);\n\tstruct sk_buff *skb, *tmp;\n\n\trps_lock(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev == dev) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock(sd);\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev == dev) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n}\n\nstatic int napi_gro_complete(struct sk_buff *skb)\n{\n\tstruct packet_type *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &ptype_base[ntohs(type) & PTYPE_HASH_MASK];\n\tint err = -ENOENT;\n\n\tif (NAPI_GRO_CB(skb)->count == 1) {\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || ptype->dev || !ptype->gro_complete)\n\t\t\tcontinue;\n\n\t\terr = ptype->gro_complete(skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (err) {\n\t\tWARN_ON(&ptype->list == head);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\nout:\n\treturn netif_receive_skb(skb);\n}\n\ninline void napi_gro_flush(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb, *next;\n\n\tfor (skb = napi->gro_list; skb; skb = next) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\t\tnapi_gro_complete(skb);\n\t}\n\n\tnapi->gro_count = 0;\n\tnapi->gro_list = NULL;\n}\nEXPORT_SYMBOL(napi_gro_flush);\n\nenum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct packet_type *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &ptype_base[ntohs(type) & PTYPE_HASH_MASK];\n\tint same_flow;\n\tint mac_len;\n\tenum gro_result ret;\n\n\tif (!(skb->dev->features & NETIF_F_GRO) || netpoll_rx_on(skb))\n\t\tgoto normal;\n\n\tif (skb_is_gso(skb) || skb_has_frag_list(skb))\n\t\tgoto normal;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || ptype->dev || !ptype->gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tmac_len = skb->network_header - skb->mac_header;\n\t\tskb->mac_len = mac_len;\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = 0;\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\n\t\tpp = ptype->gro_receive(&napi->gro_list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tstruct sk_buff *nskb = *pp;\n\n\t\t*pp = nskb->next;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t\tnapi->gro_count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush || napi->gro_count >= MAX_GRO_SKBS)\n\t\tgoto normal;\n\n\tnapi->gro_count++;\n\tNAPI_GRO_CB(skb)->count = 1;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tskb->next = napi->gro_list;\n\tnapi->gro_list = skb;\n\tret = GRO_HELD;\n\npull:\n\tif (skb_headlen(skb) < skb_gro_offset(skb)) {\n\t\tint grow = skb_gro_offset(skb) - skb_headlen(skb);\n\n\t\tBUG_ON(skb->end - skb->tail < grow);\n\n\t\tmemcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);\n\n\t\tskb->tail += grow;\n\t\tskb->data_len -= grow;\n\n\t\tskb_shinfo(skb)->frags[0].page_offset += grow;\n\t\tskb_shinfo(skb)->frags[0].size -= grow;\n\n\t\tif (unlikely(!skb_shinfo(skb)->frags[0].size)) {\n\t\t\tput_page(skb_shinfo(skb)->frags[0].page);\n\t\t\tmemmove(skb_shinfo(skb)->frags,\n\t\t\t\tskb_shinfo(skb)->frags + 1,\n\t\t\t\t--skb_shinfo(skb)->nr_frags * sizeof(skb_frag_t));\n\t\t}\n\t}\n\nok:\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}\nEXPORT_SYMBOL(dev_gro_receive);\n\nstatic inline gro_result_t\n__napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff *p;\n\n\tfor (p = napi->gro_list; p; p = p->next) {\n\t\tunsigned long diffs;\n\n\t\tdiffs = (unsigned long)p->dev ^ (unsigned long)skb->dev;\n\t\tdiffs |= p->vlan_tci ^ skb->vlan_tci;\n\t\tdiffs |= compare_ether_header(skb_mac_header(p),\n\t\t\t\t\t      skb_gro_mac_header(skb));\n\t\tNAPI_GRO_CB(p)->same_flow = !diffs;\n\t\tNAPI_GRO_CB(p)->flush = 0;\n\t}\n\n\treturn dev_gro_receive(napi, skb);\n}\n\ngro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\t\tif (netif_receive_skb(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\tcase GRO_MERGED_FREE:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_HELD:\n\tcase GRO_MERGED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_skb_finish);\n\nvoid skb_gro_reset_offset(struct sk_buff *skb)\n{\n\tNAPI_GRO_CB(skb)->data_offset = 0;\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\n\tif (skb->mac_header == skb->tail &&\n\t    !PageHighMem(skb_shinfo(skb)->frags[0].page)) {\n\t\tNAPI_GRO_CB(skb)->frag0 =\n\t\t\tpage_address(skb_shinfo(skb)->frags[0].page) +\n\t\t\tskb_shinfo(skb)->frags[0].page_offset;\n\t\tNAPI_GRO_CB(skb)->frag0_len = skb_shinfo(skb)->frags[0].size;\n\t}\n}\nEXPORT_SYMBOL(skb_gro_reset_offset);\n\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tskb_gro_reset_offset(skb);\n\n\treturn napi_skb_finish(__napi_gro_receive(napi, skb), skb);\n}\nEXPORT_SYMBOL(napi_gro_receive);\n\nstatic void napi_reuse_skb(struct napi_struct *napi, struct sk_buff *skb)\n{\n\t__skb_pull(skb, skb_headlen(skb));\n\tskb_reserve(skb, NET_IP_ALIGN - skb_headroom(skb));\n\tskb->vlan_tci = 0;\n\tskb->dev = napi->dev;\n\tskb->skb_iif = 0;\n\n\tnapi->skb = skb;\n}\n\nstruct sk_buff *napi_get_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\n\tif (!skb) {\n\t\tskb = netdev_alloc_skb_ip_align(napi->dev, GRO_MAX_HEAD);\n\t\tif (skb)\n\t\t\tnapi->skb = skb;\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_get_frags);\n\ngro_result_t napi_frags_finish(struct napi_struct *napi, struct sk_buff *skb,\n\t\t\t       gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\tcase GRO_HELD:\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\n\t\tif (ret == GRO_HELD)\n\t\t\tskb_gro_pull(skb, -ETH_HLEN);\n\t\telse if (netif_receive_skb(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\tcase GRO_MERGED_FREE:\n\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_frags_finish);\n\nstruct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tstruct ethhdr *eth;\n\tunsigned int hlen;\n\tunsigned int off;\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*eth);\n\teth = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\teth = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!eth)) {\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\tskb = NULL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, sizeof(*eth));\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.  We'll fix it up properly at the end.\n\t */\n\tskb->protocol = eth->h_proto;\n\nout:\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_frags_skb);\n\ngro_result_t napi_gro_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi_frags_skb(napi);\n\n\tif (!skb)\n\t\treturn GRO_DROP;\n\n\treturn napi_frags_finish(napi, skb, __napi_gro_receive(napi, skb));\n}\nEXPORT_SYMBOL(napi_gro_frags);\n\n/*\n * net_rps_action sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\twhile (remsd) {\n\t\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\t\tif (cpu_online(remsd->cpu))\n\t\t\t\t__smp_call_function_single(remsd->cpu,\n\t\t\t\t\t\t\t   &remsd->csd, 0);\n\t\t\tremsd = next;\n\t\t}\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tint work = 0;\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\n#ifdef CONFIG_RPS\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd->rps_ipi_list) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n#endif\n\tnapi->weight = weight_p;\n\tlocal_irq_disable();\n\twhile (work < quota) {\n\t\tstruct sk_buff *skb;\n\t\tunsigned int qlen;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\tlocal_irq_enable();\n\t\t\t__netif_receive_skb(skb);\n\t\t\tlocal_irq_disable();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota) {\n\t\t\t\tlocal_irq_enable();\n\t\t\t\treturn work;\n\t\t\t}\n\t\t}\n\n\t\trps_lock(sd);\n\t\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\t\tif (qlen)\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\n\t\tif (qlen < quota - work) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set on backlog.\n\t\t\t * we can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tlist_del(&napi->poll_list);\n\t\t\tnapi->state = 0;\n\n\t\t\tquota = work + qlen;\n\t\t}\n\t\trps_unlock(sd);\n\t}\n\tlocal_irq_enable();\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(&__get_cpu_var(softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\nvoid __napi_complete(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tBUG_ON(n->gro_list);\n\n\tlist_del(&n->poll_list);\n\tsmp_mb__before_clear_bit();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n}\nEXPORT_SYMBOL(__napi_complete);\n\nvoid napi_complete(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\t/*\n\t * don't let napi dequeue from the cpu poll list\n\t * just in case its running on a different cpu\n\t */\n\tif (unlikely(test_bit(NAPI_STATE_NPSVC, &n->state)))\n\t\treturn;\n\n\tnapi_gro_flush(n);\n\tlocal_irq_save(flags);\n\t__napi_complete(n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(napi_complete);\n\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight)\n{\n\tINIT_LIST_HEAD(&napi->poll_list);\n\tnapi->gro_count = 0;\n\tnapi->gro_list = NULL;\n\tnapi->skb = NULL;\n\tnapi->poll = poll;\n\tnapi->weight = weight;\n\tlist_add(&napi->dev_list, &dev->napi_list);\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tspin_lock_init(&napi->poll_lock);\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n}\nEXPORT_SYMBOL(netif_napi_add);\n\nvoid netif_napi_del(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb, *next;\n\n\tlist_del_init(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tfor (skb = napi->gro_list; skb; skb = next) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\t\tkfree_skb(skb);\n\t}\n\n\tnapi->gro_list = NULL;\n\tnapi->gro_count = 0;\n}\nEXPORT_SYMBOL(netif_napi_del);\n\nstatic void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = &__get_cpu_var(softnet_data);\n\tunsigned long time_limit = jiffies + 2;\n\tint budget = netdev_budget;\n\tvoid *have;\n\n\tlocal_irq_disable();\n\n\twhile (!list_empty(&sd->poll_list)) {\n\t\tstruct napi_struct *n;\n\t\tint work, weight;\n\n\t\t/* If softirq window is exhuasted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 || time_after(jiffies, time_limit)))\n\t\t\tgoto softnet_break;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Even though interrupts have been re-enabled, this\n\t\t * access is safe because interrupts can only add new\n\t\t * entries to the tail of this list, and only ->poll()\n\t\t * calls can remove this head entry from the list.\n\t\t */\n\t\tn = list_first_entry(&sd->poll_list, struct napi_struct, poll_list);\n\n\t\thave = netpoll_poll_lock(n);\n\n\t\tweight = n->weight;\n\n\t\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t\t * with netpoll's poll_napi().  Only the entity which\n\t\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t\t * actually make the ->poll() call.  Therefore we avoid\n\t\t * accidently calling ->poll() when NAPI is not scheduled.\n\t\t */\n\t\twork = 0;\n\t\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\t\twork = n->poll(n, weight);\n\t\t\ttrace_napi_poll(n);\n\t\t}\n\n\t\tWARN_ON_ONCE(work > weight);\n\n\t\tbudget -= work;\n\n\t\tlocal_irq_disable();\n\n\t\t/* Drivers must not modify the NAPI state if they\n\t\t * consume the entire weight.  In such cases this code\n\t\t * still \"owns\" the NAPI instance and therefore can\n\t\t * move the instance around on the list at-will.\n\t\t */\n\t\tif (unlikely(work == weight)) {\n\t\t\tif (unlikely(napi_disable_pending(n))) {\n\t\t\t\tlocal_irq_enable();\n\t\t\t\tnapi_complete(n);\n\t\t\t\tlocal_irq_disable();\n\t\t\t} else\n\t\t\t\tlist_move_tail(&n->poll_list, &sd->poll_list);\n\t\t}\n\n\t\tnetpoll_poll_unlock(have);\n\t}\nout:\n\tnet_rps_action_and_irq_enable(sd);\n\n#ifdef CONFIG_NET_DMA\n\t/*\n\t * There may not be any more sk_buffs coming right now, so push\n\t * any pending DMA copies to hardware\n\t */\n\tdma_issue_pending_all();\n#endif\n\n\treturn;\n\nsoftnet_break:\n\tsd->time_squeeze++;\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\tgoto out;\n}\n\nstatic gifconf_func_t *gifconf_list[NPROTO];\n\n/**\n *\tregister_gifconf\t-\tregister a SIOCGIF handler\n *\t@family: Address family\n *\t@gifconf: Function handler\n *\n *\tRegister protocol dependent address dumping routines. The handler\n *\tthat is passed must not be freed or reused until it has been replaced\n *\tby another handler.\n */\nint register_gifconf(unsigned int family, gifconf_func_t *gifconf)\n{\n\tif (family >= NPROTO)\n\t\treturn -EINVAL;\n\tgifconf_list[family] = gifconf;\n\treturn 0;\n}\nEXPORT_SYMBOL(register_gifconf);\n\n\n/*\n *\tMap an interface index to its name (SIOCGIFNAME)\n */\n\n/*\n *\tWe need this ioctl for efficient implementation of the\n *\tif_indextoname() function required by the IPv6 API.  Without\n *\tit, we would have to search all the interfaces to find a\n *\tmatch.  --pb\n */\n\nstatic int dev_ifname(struct net *net, struct ifreq __user *arg)\n{\n\tstruct net_device *dev;\n\tstruct ifreq ifr;\n\n\t/*\n\t *\tFetch the caller's info block.\n\t */\n\n\tif (copy_from_user(&ifr, arg, sizeof(struct ifreq)))\n\t\treturn -EFAULT;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifr.ifr_ifindex);\n\tif (!dev) {\n\t\trcu_read_unlock();\n\t\treturn -ENODEV;\n\t}\n\n\tstrcpy(ifr.ifr_name, dev->name);\n\trcu_read_unlock();\n\n\tif (copy_to_user(arg, &ifr, sizeof(struct ifreq)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n *\tPerform a SIOCGIFCONF call. This structure will change\n *\tsize eventually, and there is nothing I can do about it.\n *\tThus we will need a 'compatibility mode'.\n */\n\nstatic int dev_ifconf(struct net *net, char __user *arg)\n{\n\tstruct ifconf ifc;\n\tstruct net_device *dev;\n\tchar __user *pos;\n\tint len;\n\tint total;\n\tint i;\n\n\t/*\n\t *\tFetch the caller's info block.\n\t */\n\n\tif (copy_from_user(&ifc, arg, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\tpos = ifc.ifc_buf;\n\tlen = ifc.ifc_len;\n\n\t/*\n\t *\tLoop over the interfaces, and write an info block for each.\n\t */\n\n\ttotal = 0;\n\tfor_each_netdev(net, dev) {\n\t\tfor (i = 0; i < NPROTO; i++) {\n\t\t\tif (gifconf_list[i]) {\n\t\t\t\tint done;\n\t\t\t\tif (!pos)\n\t\t\t\t\tdone = gifconf_list[i](dev, NULL, 0);\n\t\t\t\telse\n\t\t\t\t\tdone = gifconf_list[i](dev, pos + total,\n\t\t\t\t\t\t\t       len - total);\n\t\t\t\tif (done < 0)\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\ttotal += done;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t *\tAll done.  Write the updated control block back to the caller.\n\t */\n\tifc.ifc_len = total;\n\n\t/*\n\t * \tBoth BSD and Solaris return 0 here, so we do too.\n\t */\n\treturn copy_to_user(arg, &ifc, sizeof(struct ifconf)) ? -EFAULT : 0;\n}\n\n#ifdef CONFIG_PROC_FS\n/*\n *\tThis is invoked by the /proc filesystem handler to display a device\n *\tin detail.\n */\nvoid *dev_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct net *net = seq_file_net(seq);\n\tloff_t off;\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tif (!*pos)\n\t\treturn SEQ_START_TOKEN;\n\n\toff = 1;\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (off++ == *pos)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\n\nvoid *dev_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct net_device *dev = (v == SEQ_START_TOKEN) ?\n\t\t\t\t  first_net_device(seq_file_net(seq)) :\n\t\t\t\t  next_net_device((struct net_device *)v);\n\n\t++*pos;\n\treturn rcu_dereference(dev);\n}\n\nvoid dev_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic void dev_seq_printf_stats(struct seq_file *seq, struct net_device *dev)\n{\n\tstruct rtnl_link_stats64 temp;\n\tconst struct rtnl_link_stats64 *stats = dev_get_stats(dev, &temp);\n\n\tseq_printf(seq, \"%6s: %7llu %7llu %4llu %4llu %4llu %5llu %10llu %9llu \"\n\t\t   \"%8llu %7llu %4llu %4llu %4llu %5llu %7llu %10llu\\n\",\n\t\t   dev->name, stats->rx_bytes, stats->rx_packets,\n\t\t   stats->rx_errors,\n\t\t   stats->rx_dropped + stats->rx_missed_errors,\n\t\t   stats->rx_fifo_errors,\n\t\t   stats->rx_length_errors + stats->rx_over_errors +\n\t\t    stats->rx_crc_errors + stats->rx_frame_errors,\n\t\t   stats->rx_compressed, stats->multicast,\n\t\t   stats->tx_bytes, stats->tx_packets,\n\t\t   stats->tx_errors, stats->tx_dropped,\n\t\t   stats->tx_fifo_errors, stats->collisions,\n\t\t   stats->tx_carrier_errors +\n\t\t    stats->tx_aborted_errors +\n\t\t    stats->tx_window_errors +\n\t\t    stats->tx_heartbeat_errors,\n\t\t   stats->tx_compressed);\n}\n\n/*\n *\tCalled from the PROCfs module. This now uses the new arbitrary sized\n *\t/proc/net interface to create /proc/net/dev\n */\nstatic int dev_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"Inter-|   Receive                            \"\n\t\t\t      \"                    |  Transmit\\n\"\n\t\t\t      \" face |bytes    packets errs drop fifo frame \"\n\t\t\t      \"compressed multicast|bytes    packets errs \"\n\t\t\t      \"drop fifo colls carrier compressed\\n\");\n\telse\n\t\tdev_seq_printf_stats(seq, v);\n\treturn 0;\n}\n\nstatic struct softnet_data *softnet_get_online(loff_t *pos)\n{\n\tstruct softnet_data *sd = NULL;\n\n\twhile (*pos < nr_cpu_ids)\n\t\tif (cpu_online(*pos)) {\n\t\t\tsd = &per_cpu(softnet_data, *pos);\n\t\t\tbreak;\n\t\t} else\n\t\t\t++*pos;\n\treturn sd;\n}\n\nstatic void *softnet_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\treturn softnet_get_online(pos);\n}\n\nstatic void *softnet_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn softnet_get_online(pos);\n}\n\nstatic void softnet_seq_stop(struct seq_file *seq, void *v)\n{\n}\n\nstatic int softnet_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct softnet_data *sd = v;\n\n\tseq_printf(seq, \"%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\\n\",\n\t\t   sd->processed, sd->dropped, sd->time_squeeze, 0,\n\t\t   0, 0, 0, 0, /* was fastroute */\n\t\t   sd->cpu_collision, sd->received_rps);\n\treturn 0;\n}\n\nstatic const struct seq_operations dev_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = dev_seq_show,\n};\n\nstatic int dev_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &dev_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations dev_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open    = dev_seq_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = seq_release_net,\n};\n\nstatic const struct seq_operations softnet_seq_ops = {\n\t.start = softnet_seq_start,\n\t.next  = softnet_seq_next,\n\t.stop  = softnet_seq_stop,\n\t.show  = softnet_seq_show,\n};\n\nstatic int softnet_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &softnet_seq_ops);\n}\n\nstatic const struct file_operations softnet_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open    = softnet_seq_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = seq_release,\n};\n\nstatic void *ptype_get_idx(loff_t pos)\n{\n\tstruct packet_type *pt = NULL;\n\tloff_t i = 0;\n\tint t;\n\n\tlist_for_each_entry_rcu(pt, &ptype_all, list) {\n\t\tif (i == pos)\n\t\t\treturn pt;\n\t\t++i;\n\t}\n\n\tfor (t = 0; t < PTYPE_HASH_SIZE; t++) {\n\t\tlist_for_each_entry_rcu(pt, &ptype_base[t], list) {\n\t\t\tif (i == pos)\n\t\t\t\treturn pt;\n\t\t\t++i;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void *ptype_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\trcu_read_lock();\n\treturn *pos ? ptype_get_idx(*pos - 1) : SEQ_START_TOKEN;\n}\n\nstatic void *ptype_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct packet_type *pt;\n\tstruct list_head *nxt;\n\tint hash;\n\n\t++*pos;\n\tif (v == SEQ_START_TOKEN)\n\t\treturn ptype_get_idx(0);\n\n\tpt = v;\n\tnxt = pt->list.next;\n\tif (pt->type == htons(ETH_P_ALL)) {\n\t\tif (nxt != &ptype_all)\n\t\t\tgoto found;\n\t\thash = 0;\n\t\tnxt = ptype_base[0].next;\n\t} else\n\t\thash = ntohs(pt->type) & PTYPE_HASH_MASK;\n\n\twhile (nxt == &ptype_base[hash]) {\n\t\tif (++hash >= PTYPE_HASH_SIZE)\n\t\t\treturn NULL;\n\t\tnxt = ptype_base[hash].next;\n\t}\nfound:\n\treturn list_entry(nxt, struct packet_type, list);\n}\n\nstatic void ptype_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic int ptype_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct packet_type *pt = v;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"Type Device      Function\\n\");\n\telse if (pt->dev == NULL || dev_net(pt->dev) == seq_file_net(seq)) {\n\t\tif (pt->type == htons(ETH_P_ALL))\n\t\t\tseq_puts(seq, \"ALL \");\n\t\telse\n\t\t\tseq_printf(seq, \"%04x\", ntohs(pt->type));\n\n\t\tseq_printf(seq, \" %-8s %pF\\n\",\n\t\t\t   pt->dev ? pt->dev->name : \"\", pt->func);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations ptype_seq_ops = {\n\t.start = ptype_seq_start,\n\t.next  = ptype_seq_next,\n\t.stop  = ptype_seq_stop,\n\t.show  = ptype_seq_show,\n};\n\nstatic int ptype_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &ptype_seq_ops,\n\t\t\tsizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations ptype_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open    = ptype_seq_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = seq_release_net,\n};\n\n\nstatic int __net_init dev_proc_net_init(struct net *net)\n{\n\tint rc = -ENOMEM;\n\n\tif (!proc_net_fops_create(net, \"dev\", S_IRUGO, &dev_seq_fops))\n\t\tgoto out;\n\tif (!proc_net_fops_create(net, \"softnet_stat\", S_IRUGO, &softnet_seq_fops))\n\t\tgoto out_dev;\n\tif (!proc_net_fops_create(net, \"ptype\", S_IRUGO, &ptype_seq_fops))\n\t\tgoto out_softnet;\n\n\tif (wext_proc_init(net))\n\t\tgoto out_ptype;\n\trc = 0;\nout:\n\treturn rc;\nout_ptype:\n\tproc_net_remove(net, \"ptype\");\nout_softnet:\n\tproc_net_remove(net, \"softnet_stat\");\nout_dev:\n\tproc_net_remove(net, \"dev\");\n\tgoto out;\n}\n\nstatic void __net_exit dev_proc_net_exit(struct net *net)\n{\n\twext_proc_exit(net);\n\n\tproc_net_remove(net, \"ptype\");\n\tproc_net_remove(net, \"softnet_stat\");\n\tproc_net_remove(net, \"dev\");\n}\n\nstatic struct pernet_operations __net_initdata dev_proc_ops = {\n\t.init = dev_proc_net_init,\n\t.exit = dev_proc_net_exit,\n};\n\nstatic int __init dev_proc_init(void)\n{\n\treturn register_pernet_subsys(&dev_proc_ops);\n}\n#else\n#define dev_proc_init() 0\n#endif\t/* CONFIG_PROC_FS */\n\n\n/**\n *\tnetdev_set_master\t-\tset up master/slave pair\n *\t@slave: slave device\n *\t@master: new master device\n *\n *\tChanges the master device of the slave. Pass %NULL to break the\n *\tbonding. The caller must hold the RTNL semaphore. On a failure\n *\ta negative errno code is returned. On success the reference counts\n *\tare adjusted, %RTM_NEWLINK is sent to the routing socket and the\n *\tfunction returns zero.\n */\nint netdev_set_master(struct net_device *slave, struct net_device *master)\n{\n\tstruct net_device *old = slave->master;\n\n\tASSERT_RTNL();\n\n\tif (master) {\n\t\tif (old)\n\t\t\treturn -EBUSY;\n\t\tdev_hold(master);\n\t}\n\n\tslave->master = master;\n\n\tif (old) {\n\t\tsynchronize_net();\n\t\tdev_put(old);\n\t}\n\tif (master)\n\t\tslave->flags |= IFF_SLAVE;\n\telse\n\t\tslave->flags &= ~IFF_SLAVE;\n\n\trtmsg_ifinfo(RTM_NEWLINK, slave, IFF_SLAVE);\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_master);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif ((dev->flags & IFF_UP) && ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned short old_flags = dev->flags;\n\tuid_t uid;\n\tgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tprintk(KERN_WARNING \"%s: promiscuity touches roof, \"\n\t\t\t\t\"set promiscuity failed, promiscuity feature \"\n\t\t\t\t\"of device might be broken.\\n\", dev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tprintk(KERN_INFO \"device %s %s promiscuous mode\\n\",\n\t\t       dev->name, (dev->flags & IFF_PROMISC) ? \"entered\" :\n\t\t\t\t\t\t\t       \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(current->audit_context, GFP_ATOMIC,\n\t\t\t\tAUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t\"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\tdev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t(old_flags & IFF_PROMISC),\n\t\t\t\taudit_get_loginuid(current),\n\t\t\t\tuid, gid,\n\t\t\t\taudit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned short old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\tunsigned short old_flags = dev->flags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tprintk(KERN_WARNING \"%s: allmulti touches roof, \"\n\t\t\t\t\"set allmulti failed, allmulti feature of \"\n\t\t\t\t\"device might be broken.\\n\", dev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n\telse {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1);\n\t\t\tdev->uc_promisc = 1;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1);\n\t\t\tdev->uc_promisc = 0;\n\t\t}\n\n\t\tif (ops->ndo_set_multicast_list)\n\t\t\tops->ndo_set_multicast_list(dev);\n\t}\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned dev_get_flags(const struct net_device *dev)\n{\n\tunsigned flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tint old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\t/* Bit is different  ? */\n\t\tret = ((old_flags & IFF_UP) ? __dev_close : __dev_open)(dev);\n\n\t\tif (!ret)\n\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\t\tdev_set_promiscuity(dev, inc);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t   is important. Some (broken) drivers set IFF_PROMISC, when\n\t   IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\tdev_set_allmulti(dev, inc);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE)))\n\t\tcall_netdevice_notifiers(NETDEV_CHANGE, dev);\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned flags)\n{\n\tint ret, changes;\n\tint old_flags = dev->flags;\n\n\tret = __dev_change_flags(dev, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = old_flags ^ dev->flags;\n\tif (changes)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, changes);\n\n\t__dev_notify_flags(dev, old_flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\n/**\n *\tdev_set_mtu - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\t/*\tMTU must be positive.\t */\n\tif (new_mtu < 0)\n\t\treturn -EINVAL;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = 0;\n\tif (ops->ndo_change_mtu)\n\t\terr = ops->ndo_change_mtu(dev, new_mtu);\n\telse\n\t\tdev->mtu = new_mtu;\n\n\tif (!err && dev->flags & IFF_UP)\n\t\tcall_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (!err)\n\t\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\n/*\n *\tPerform the SIOCxIFxxx calls, inside rcu_read_lock()\n */\nstatic int dev_ifsioc_locked(struct net *net, struct ifreq *ifr, unsigned int cmd)\n{\n\tint err;\n\tstruct net_device *dev = dev_get_by_name_rcu(net, ifr->ifr_name);\n\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tswitch (cmd) {\n\tcase SIOCGIFFLAGS:\t/* Get interface flags */\n\t\tifr->ifr_flags = (short) dev_get_flags(dev);\n\t\treturn 0;\n\n\tcase SIOCGIFMETRIC:\t/* Get the metric on the interface\n\t\t\t\t   (currently unused) */\n\t\tifr->ifr_metric = 0;\n\t\treturn 0;\n\n\tcase SIOCGIFMTU:\t/* Get the MTU of a device */\n\t\tifr->ifr_mtu = dev->mtu;\n\t\treturn 0;\n\n\tcase SIOCGIFHWADDR:\n\t\tif (!dev->addr_len)\n\t\t\tmemset(ifr->ifr_hwaddr.sa_data, 0, sizeof ifr->ifr_hwaddr.sa_data);\n\t\telse\n\t\t\tmemcpy(ifr->ifr_hwaddr.sa_data, dev->dev_addr,\n\t\t\t       min(sizeof ifr->ifr_hwaddr.sa_data, (size_t) dev->addr_len));\n\t\tifr->ifr_hwaddr.sa_family = dev->type;\n\t\treturn 0;\n\n\tcase SIOCGIFSLAVE:\n\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase SIOCGIFMAP:\n\t\tifr->ifr_map.mem_start = dev->mem_start;\n\t\tifr->ifr_map.mem_end   = dev->mem_end;\n\t\tifr->ifr_map.base_addr = dev->base_addr;\n\t\tifr->ifr_map.irq       = dev->irq;\n\t\tifr->ifr_map.dma       = dev->dma;\n\t\tifr->ifr_map.port      = dev->if_port;\n\t\treturn 0;\n\n\tcase SIOCGIFINDEX:\n\t\tifr->ifr_ifindex = dev->ifindex;\n\t\treturn 0;\n\n\tcase SIOCGIFTXQLEN:\n\t\tifr->ifr_qlen = dev->tx_queue_len;\n\t\treturn 0;\n\n\tdefault:\n\t\t/* dev_ioctl() should ensure this case\n\t\t * is never reached\n\t\t */\n\t\tWARN_ON(1);\n\t\terr = -EINVAL;\n\t\tbreak;\n\n\t}\n\treturn err;\n}\n\n/*\n *\tPerform the SIOCxIFxxx calls, inside rtnl_lock()\n */\nstatic int dev_ifsioc(struct net *net, struct ifreq *ifr, unsigned int cmd)\n{\n\tint err;\n\tstruct net_device *dev = __dev_get_by_name(net, ifr->ifr_name);\n\tconst struct net_device_ops *ops;\n\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tops = dev->netdev_ops;\n\n\tswitch (cmd) {\n\tcase SIOCSIFFLAGS:\t/* Set interface flags */\n\t\treturn dev_change_flags(dev, ifr->ifr_flags);\n\n\tcase SIOCSIFMETRIC:\t/* Set the metric on the interface\n\t\t\t\t   (currently unused) */\n\t\treturn -EOPNOTSUPP;\n\n\tcase SIOCSIFMTU:\t/* Set the MTU of a device */\n\t\treturn dev_set_mtu(dev, ifr->ifr_mtu);\n\n\tcase SIOCSIFHWADDR:\n\t\treturn dev_set_mac_address(dev, &ifr->ifr_hwaddr);\n\n\tcase SIOCSIFHWBROADCAST:\n\t\tif (ifr->ifr_hwaddr.sa_family != dev->type)\n\t\t\treturn -EINVAL;\n\t\tmemcpy(dev->broadcast, ifr->ifr_hwaddr.sa_data,\n\t\t       min(sizeof ifr->ifr_hwaddr.sa_data, (size_t) dev->addr_len));\n\t\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\t\treturn 0;\n\n\tcase SIOCSIFMAP:\n\t\tif (ops->ndo_set_config) {\n\t\t\tif (!netif_device_present(dev))\n\t\t\t\treturn -ENODEV;\n\t\t\treturn ops->ndo_set_config(dev, &ifr->ifr_map);\n\t\t}\n\t\treturn -EOPNOTSUPP;\n\n\tcase SIOCADDMULTI:\n\t\tif ((!ops->ndo_set_multicast_list && !ops->ndo_set_rx_mode) ||\n\t\t    ifr->ifr_hwaddr.sa_family != AF_UNSPEC)\n\t\t\treturn -EINVAL;\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t\treturn dev_mc_add_global(dev, ifr->ifr_hwaddr.sa_data);\n\n\tcase SIOCDELMULTI:\n\t\tif ((!ops->ndo_set_multicast_list && !ops->ndo_set_rx_mode) ||\n\t\t    ifr->ifr_hwaddr.sa_family != AF_UNSPEC)\n\t\t\treturn -EINVAL;\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t\treturn dev_mc_del_global(dev, ifr->ifr_hwaddr.sa_data);\n\n\tcase SIOCSIFTXQLEN:\n\t\tif (ifr->ifr_qlen < 0)\n\t\t\treturn -EINVAL;\n\t\tdev->tx_queue_len = ifr->ifr_qlen;\n\t\treturn 0;\n\n\tcase SIOCSIFNAME:\n\t\tifr->ifr_newname[IFNAMSIZ-1] = '\\0';\n\t\treturn dev_change_name(dev, ifr->ifr_newname);\n\n\t/*\n\t *\tUnknown or private ioctl\n\t */\n\tdefault:\n\t\tif ((cmd >= SIOCDEVPRIVATE &&\n\t\t    cmd <= SIOCDEVPRIVATE + 15) ||\n\t\t    cmd == SIOCBONDENSLAVE ||\n\t\t    cmd == SIOCBONDRELEASE ||\n\t\t    cmd == SIOCBONDSETHWADDR ||\n\t\t    cmd == SIOCBONDSLAVEINFOQUERY ||\n\t\t    cmd == SIOCBONDINFOQUERY ||\n\t\t    cmd == SIOCBONDCHANGEACTIVE ||\n\t\t    cmd == SIOCGMIIPHY ||\n\t\t    cmd == SIOCGMIIREG ||\n\t\t    cmd == SIOCSMIIREG ||\n\t\t    cmd == SIOCBRADDIF ||\n\t\t    cmd == SIOCBRDELIF ||\n\t\t    cmd == SIOCSHWTSTAMP ||\n\t\t    cmd == SIOCWANDEV) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (ops->ndo_do_ioctl) {\n\t\t\t\tif (netif_device_present(dev))\n\t\t\t\t\terr = ops->ndo_do_ioctl(dev, ifr, cmd);\n\t\t\t\telse\n\t\t\t\t\terr = -ENODEV;\n\t\t\t}\n\t\t} else\n\t\t\terr = -EINVAL;\n\n\t}\n\treturn err;\n}\n\n/*\n *\tThis function handles all \"interface\"-type I/O control requests. The actual\n *\t'doing' part of this is dev_ifsioc above.\n */\n\n/**\n *\tdev_ioctl\t-\tnetwork device ioctl\n *\t@net: the applicable net namespace\n *\t@cmd: command to issue\n *\t@arg: pointer to a struct ifreq in user space\n *\n *\tIssue ioctl functions to devices. This is normally called by the\n *\tuser space syscall interfaces but can sometimes be useful for\n *\tother purposes. The return value is the return from the syscall if\n *\tpositive or a negative errno code on error.\n */\n\nint dev_ioctl(struct net *net, unsigned int cmd, void __user *arg)\n{\n\tstruct ifreq ifr;\n\tint ret;\n\tchar *colon;\n\n\t/* One special case: SIOCGIFCONF takes ifconf argument\n\t   and requires shared lock, because it sleeps writing\n\t   to user space.\n\t */\n\n\tif (cmd == SIOCGIFCONF) {\n\t\trtnl_lock();\n\t\tret = dev_ifconf(net, (char __user *) arg);\n\t\trtnl_unlock();\n\t\treturn ret;\n\t}\n\tif (cmd == SIOCGIFNAME)\n\t\treturn dev_ifname(net, (struct ifreq __user *)arg);\n\n\tif (copy_from_user(&ifr, arg, sizeof(struct ifreq)))\n\t\treturn -EFAULT;\n\n\tifr.ifr_name[IFNAMSIZ-1] = 0;\n\n\tcolon = strchr(ifr.ifr_name, ':');\n\tif (colon)\n\t\t*colon = 0;\n\n\t/*\n\t *\tSee which interface the caller is talking about.\n\t */\n\n\tswitch (cmd) {\n\t/*\n\t *\tThese ioctl calls:\n\t *\t- can be done by all.\n\t *\t- atomic and do not require locking.\n\t *\t- return a value\n\t */\n\tcase SIOCGIFFLAGS:\n\tcase SIOCGIFMETRIC:\n\tcase SIOCGIFMTU:\n\tcase SIOCGIFHWADDR:\n\tcase SIOCGIFSLAVE:\n\tcase SIOCGIFMAP:\n\tcase SIOCGIFINDEX:\n\tcase SIOCGIFTXQLEN:\n\t\tdev_load(net, ifr.ifr_name);\n\t\trcu_read_lock();\n\t\tret = dev_ifsioc_locked(net, &ifr, cmd);\n\t\trcu_read_unlock();\n\t\tif (!ret) {\n\t\t\tif (colon)\n\t\t\t\t*colon = ':';\n\t\t\tif (copy_to_user(arg, &ifr,\n\t\t\t\t\t sizeof(struct ifreq)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t\treturn ret;\n\n\tcase SIOCETHTOOL:\n\t\tdev_load(net, ifr.ifr_name);\n\t\trtnl_lock();\n\t\tret = dev_ethtool(net, &ifr);\n\t\trtnl_unlock();\n\t\tif (!ret) {\n\t\t\tif (colon)\n\t\t\t\t*colon = ':';\n\t\t\tif (copy_to_user(arg, &ifr,\n\t\t\t\t\t sizeof(struct ifreq)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t\treturn ret;\n\n\t/*\n\t *\tThese ioctl calls:\n\t *\t- require superuser power.\n\t *\t- require strict serialization.\n\t *\t- return a value\n\t */\n\tcase SIOCGMIIPHY:\n\tcase SIOCGMIIREG:\n\tcase SIOCSIFNAME:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tdev_load(net, ifr.ifr_name);\n\t\trtnl_lock();\n\t\tret = dev_ifsioc(net, &ifr, cmd);\n\t\trtnl_unlock();\n\t\tif (!ret) {\n\t\t\tif (colon)\n\t\t\t\t*colon = ':';\n\t\t\tif (copy_to_user(arg, &ifr,\n\t\t\t\t\t sizeof(struct ifreq)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t\treturn ret;\n\n\t/*\n\t *\tThese ioctl calls:\n\t *\t- require superuser power.\n\t *\t- require strict serialization.\n\t *\t- do not return a value\n\t */\n\tcase SIOCSIFFLAGS:\n\tcase SIOCSIFMETRIC:\n\tcase SIOCSIFMTU:\n\tcase SIOCSIFMAP:\n\tcase SIOCSIFHWADDR:\n\tcase SIOCSIFSLAVE:\n\tcase SIOCADDMULTI:\n\tcase SIOCDELMULTI:\n\tcase SIOCSIFHWBROADCAST:\n\tcase SIOCSIFTXQLEN:\n\tcase SIOCSMIIREG:\n\tcase SIOCBONDENSLAVE:\n\tcase SIOCBONDRELEASE:\n\tcase SIOCBONDSETHWADDR:\n\tcase SIOCBONDCHANGEACTIVE:\n\tcase SIOCBRADDIF:\n\tcase SIOCBRDELIF:\n\tcase SIOCSHWTSTAMP:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\t/* fall through */\n\tcase SIOCBONDSLAVEINFOQUERY:\n\tcase SIOCBONDINFOQUERY:\n\t\tdev_load(net, ifr.ifr_name);\n\t\trtnl_lock();\n\t\tret = dev_ifsioc(net, &ifr, cmd);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tcase SIOCGIFMEM:\n\t\t/* Get the per device memory space. We can add this but\n\t\t * currently do not support it */\n\tcase SIOCSIFMEM:\n\t\t/* Set the per device memory buffer space.\n\t\t * Not applicable in our case */\n\tcase SIOCSIFLINK:\n\t\treturn -EINVAL;\n\n\t/*\n\t *\tUnknown or private ioctl.\n\t */\n\tdefault:\n\t\tif (cmd == SIOCWANDEV ||\n\t\t    (cmd >= SIOCDEVPRIVATE &&\n\t\t     cmd <= SIOCDEVPRIVATE + 15)) {\n\t\t\tdev_load(net, ifr.ifr_name);\n\t\t\trtnl_lock();\n\t\t\tret = dev_ifsioc(net, &ifr, cmd);\n\t\t\trtnl_unlock();\n\t\t\tif (!ret && copy_to_user(arg, &ifr,\n\t\t\t\t\t\t sizeof(struct ifreq)))\n\t\t\t\tret = -EFAULT;\n\t\t\treturn ret;\n\t\t}\n\t\t/* Take care of Wireless Extensions */\n\t\tif (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST)\n\t\t\treturn wext_handle_ioctl(net, &ifr, cmd, arg);\n\t\treturn -EINVAL;\n\t}\n}\n\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tstatic int ifindex;\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nstatic LIST_HEAD(net_todo_list);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n}\n\nstatic void rollback_registered_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never \"\n\t\t\t\t \"was registered\\n\", dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tdev_close_many(head);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t   this device. They should clean all the things.\n\t\t*/\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\trtmsg_ifinfo(RTM_DELLINK, dev, ~0U);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\t/* Notifier chain MUST detach us from master device. */\n\t\tWARN_ON(dev->master);\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n\t}\n\n\t/* Process any work delayed until the end of the batch */\n\tdev = list_first_entry(head, struct net_device, unreg_list);\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER_BATCH, dev);\n\n\trcu_barrier();\n\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tdev_put(dev);\n}\n\nstatic void rollback_registered(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\trollback_registered_many(&single);\n\tlist_del(&single);\n}\n\nunsigned long netdev_fix_features(unsigned long features, const char *name)\n{\n\t/* Fix illegal SG+CSUM combinations. */\n\tif ((features & NETIF_F_SG) &&\n\t    !(features & NETIF_F_ALL_CSUM)) {\n\t\tif (name)\n\t\t\tprintk(KERN_NOTICE \"%s: Dropping NETIF_F_SG since no \"\n\t\t\t       \"checksum feature.\\n\", name);\n\t\tfeatures &= ~NETIF_F_SG;\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_SG)) {\n\t\tif (name)\n\t\t\tprintk(KERN_NOTICE \"%s: Dropping NETIF_F_TSO since no \"\n\t\t\t       \"SG feature.\\n\", name);\n\t\tfeatures &= ~NETIF_F_TSO;\n\t}\n\n\tif (features & NETIF_F_UFO) {\n\t\t/* maybe split UFO into V4 and V6? */\n\t\tif (!((features & NETIF_F_GEN_CSUM) ||\n\t\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))\n\t\t\t    == (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\t\tif (name)\n\t\t\t\tprintk(KERN_ERR \"%s: Dropping NETIF_F_UFO \"\n\t\t\t\t       \"since no checksum offload features.\\n\",\n\t\t\t\t       name);\n\t\t\tfeatures &= ~NETIF_F_UFO;\n\t\t}\n\n\t\tif (!(features & NETIF_F_SG)) {\n\t\t\tif (name)\n\t\t\t\tprintk(KERN_ERR \"%s: Dropping NETIF_F_UFO \"\n\t\t\t\t       \"since no NETIF_F_SG feature.\\n\", name);\n\t\t\tfeatures &= ~NETIF_F_UFO;\n\t\t}\n\t}\n\n\treturn features;\n}\nEXPORT_SYMBOL(netdev_fix_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (netif_carrier_ok(rootdev)) {\n\t\tif (!netif_carrier_ok(dev))\n\t\t\tnetif_carrier_on(dev);\n\t} else {\n\t\tif (netif_carrier_ok(dev))\n\t\t\tnetif_carrier_off(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\n#ifdef CONFIG_RPS\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\n\tBUG_ON(count < 1);\n\n\trx = kcalloc(count, sizeof(struct netdev_rx_queue), GFP_KERNEL);\n\tif (!rx) {\n\t\tpr_err(\"netdev: Unable to allocate %u rx queues.\\n\", count);\n\t\treturn -ENOMEM;\n\t}\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++)\n\t\trx[i].dev = dev;\n\treturn 0;\n}\n#endif\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\n\tBUG_ON(count < 1);\n\n\ttx = kcalloc(count, sizeof(struct netdev_queue), GFP_KERNEL);\n\tif (!tx) {\n\t\tpr_err(\"netdev: Unable to allocate %u tx queues.\\n\",\n\t\t       count);\n\t\treturn -ENOMEM;\n\t}\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\n/**\n *\tregister_netdevice\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tCallers must hold the rtnl semaphore. You may want\n *\tregister_netdev() instead of this.\n *\n *\tBUGS:\n *\tThe locking appears insufficient to guarantee two parallel registers\n *\twill not get the same name.\n */\n\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tdev->iflink = -1;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = dev_get_valid_name(dev, dev->name, 0);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tdev->ifindex = dev_new_index(net);\n\tif (dev->iflink == -1)\n\t\tdev->iflink = dev->ifindex;\n\n\t/* Fix illegal checksum combinations */\n\tif ((dev->features & NETIF_F_HW_CSUM) &&\n\t    (dev->features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tprintk(KERN_NOTICE \"%s: mixed HW and IP checksum settings.\\n\",\n\t\t       dev->name);\n\t\tdev->features &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\tif ((dev->features & NETIF_F_NO_CSUM) &&\n\t    (dev->features & (NETIF_F_HW_CSUM|NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tprintk(KERN_NOTICE \"%s: mixed no checksumming and other settings.\\n\",\n\t\t       dev->name);\n\t\tdev->features &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM|NETIF_F_HW_CSUM);\n\t}\n\n\tdev->features = netdev_fix_features(dev->features, dev->name);\n\n\t/* Enable software GSO if SG is supported. */\n\tif (dev->features & NETIF_F_SG)\n\t\tdev->features |= NETIF_F_GSO;\n\n\t/* Enable GRO and NETIF_F_HIGHDMA for vlans by default,\n\t * vlan_dev_init() will do the dev->features check, so these features\n\t * are enabled only if supported by underlying device.\n\t */\n\tdev->vlan_features |= (NETIF_F_GRO | NETIF_F_HIGHDMA);\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = netdev_register_kobject(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\tdev->reg_state = NETREG_REGISTERED;\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tdev_init_scheduler(dev);\n\tdev_hold(dev);\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\trollback_registered(dev);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U);\n\nout:\n\treturn ret;\n\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\trtnl_lock();\n\n\t/*\n\t * If the name is a format string the caller wants us to do a\n\t * name allocation.\n\t */\n\tif (strchr(dev->name, '%')) {\n\t\terr = dev_alloc_name(dev, dev->name);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\terr = register_netdevice(dev);\nout:\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\n/*\n * netdev_wait_allrefs - wait until all references are gone.\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic void netdev_wait_allrefs(struct net_device *dev)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tint refcnt;\n\n\tlinkwatch_forget_dev(dev);\n\n\trebroadcast_time = warning_time = jiffies;\n\trefcnt = netdev_refcnt_read(dev);\n\n\twhile (refcnt != 0) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\t\t\t/* don't resend NETDEV_UNREGISTER_BATCH, _BATCH users\n\t\t\t * should have already handle it the first time */\n\n\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t     &dev->state)) {\n\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t * pending on unregister. If this\n\t\t\t\t * happens, we simply run the queue\n\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t * for this device.\n\t\t\t\t */\n\t\t\t\tlinkwatch_run_queue();\n\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tmsleep(250);\n\n\t\trefcnt = netdev_refcnt_read(dev);\n\n\t\tif (time_after(jiffies, warning_time + 10 * HZ)) {\n\t\t\tprintk(KERN_EMERG \"unregister_netdevice: \"\n\t\t\t       \"waiting for %s to become free. Usage \"\n\t\t\t       \"count = %d\\n\",\n\t\t\t       dev->name, refcnt);\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct list_head list;\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\twhile (!list_empty(&list)) {\n\t\tstruct net_device *dev\n\t\t\t= list_first_entry(&list, struct net_device, todo_list);\n\t\tlist_del(&dev->todo_list);\n\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tprintk(KERN_ERR \"network todo '%s' but state %d\\n\",\n\t\t\t       dev->name, dev->reg_state);\n\t\t\tdump_stack();\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\n\t\ton_each_cpu(flush_backlog, dev, 1);\n\n\t\tnetdev_wait_allrefs(dev);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev));\n\t\tWARN_ON(rcu_dereference_raw(dev->ip_ptr));\n\t\tWARN_ON(rcu_dereference_raw(dev->ip6_ptr));\n\t\tWARN_ON(dev->dn_ptr);\n\n\t\tif (dev->destructor)\n\t\t\tdev->destructor(dev);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64.  They have the same\n * fields in the same order, with only the type differing.\n */\nstatic void netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t\t    const struct net_device_stats *netdev_stats)\n{\n#if BITS_PER_LONG == 64\n        BUILD_BUG_ON(sizeof(*stats64) != sizeof(*netdev_stats));\n        memcpy(stats64, netdev_stats, sizeof(*stats64));\n#else\n\tsize_t i, n = sizeof(*stats64) / sizeof(u64);\n\tconst unsigned long *src = (const unsigned long *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(sizeof(*netdev_stats) / sizeof(unsigned long) !=\n\t\t     sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = src[i];\n#endif\n}\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\tstorage->rx_dropped += atomic_long_read(&dev->rx_dropped);\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tqueue->qdisc = &noop_qdisc;\n\tqueue->qdisc_sleeping = &noop_qdisc;\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\n/**\n *\talloc_netdev_mqs - allocate network device\n *\t@sizeof_priv:\tsize of private data to allocate space for\n *\t@name:\t\tdevice name format string\n *\t@setup:\t\tcallback to initialize device\n *\t@txqs:\t\tthe number of TX subqueues to allocate\n *\t@rxqs:\t\tthe number of RX subqueues to allocate\n *\n *\tAllocates a struct net_device with private data area for driver use\n *\tand performs basic initialization.  Also allocates subquue structs\n *\tfor each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tsize_t alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device \"\n\t\t       \"with zero queues.\\n\");\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_RPS\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device \"\n\t\t       \"with zero RX queues.\\n\");\n\t\treturn NULL;\n\t}\n#endif\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kzalloc(alloc_size, GFP_KERNEL);\n\tif (!p) {\n\t\tprintk(KERN_ERR \"alloc_netdev: Unable to allocate device.\\n\");\n\t\treturn NULL;\n\t}\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_p;\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_MAX_SIZE;\n\n\tINIT_LIST_HEAD(&dev->ethtool_ntuple_list.list);\n\tdev->ethtool_ntuple_list.count = 0;\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE;\n\tsetup(dev);\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n#ifdef CONFIG_RPS\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n#endif\n\n\tstrcpy(dev->name, name);\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n\tfree_percpu(dev->pcpu_refcnt);\n\tkfree(dev->_tx);\n#ifdef CONFIG_RPS\n\tkfree(dev->_rx);\n#endif\n\nfree_p:\n\tkfree(p);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n *\tfree_netdev - free network device\n *\t@dev: device\n *\n *\tThis function does the last stage of destroying an allocated device\n * \tinterface. The reference to the device object is released.\n *\tIf this is the last reference then it will be freed.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\trelease_net(dev_net(dev));\n\n\tkfree(dev->_tx);\n#ifdef CONFIG_RPS\n\tkfree(dev->_rx);\n#endif\n\n\tkfree(rcu_dereference_raw(dev->ingress_queue));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\t/* Clear ethtool n-tuple list */\n\tethtool_ntuple_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tkfree((char *)dev - dev->padded);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\trollback_registered(dev);\n\t\t/* Finish processing unregister after unlock */\n\t\tnet_set_todo(dev);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tif (!list_empty(head)) {\n\t\trollback_registered_many(head);\n\t\tlist_for_each_entry(dev, head, unreg_list)\n\t\t\tnet_set_todo(dev);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\tdev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint dev_change_net_namespace(struct net_device *dev, struct net *net, const char *pat)\n{\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\terr = -EINVAL;\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(dev_net(dev), net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (__dev_get_by_name(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\tif (dev_get_valid_name(dev, pat, 1))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\terr = -ENODEV;\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t   this device. They should clean all the things.\n\n\t   Note that dev->reg_state stays at NETREG_REGISTERED.\n\t   This is wanted because this way 8021q and macvlan know\n\t   the device is just moving and can keep their slaves up.\n\t*/\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER_BATCH, dev);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\n\t/* If there is an ifindex conflict assign a new one */\n\tif (__dev_get_by_index(net, dev->ifindex)) {\n\t\tint iflink = (dev->iflink == dev->ifindex);\n\t\tdev->ifindex = dev_new_index(net);\n\t\tif (iflink)\n\t\t\tdev->iflink = dev->ifindex;\n\t}\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dev_change_net_namespace);\n\nstatic int dev_cpu_callback(struct notifier_block *nfb,\n\t\t\t    unsigned long action,\n\t\t\t    void *ocpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu, oldcpu = (unsigned long)ocpu;\n\tstruct softnet_data *sd, *oldsd;\n\n\tif (action != CPU_DEAD && action != CPU_DEAD_FROZEN)\n\t\treturn NOTIFY_OK;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = __skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn NOTIFY_OK;\n}\n\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nunsigned long netdev_increment_features(unsigned long all, unsigned long one,\n\t\t\t\t\tunsigned long mask)\n{\n\t/* If device needs checksumming, downgrade to it. */\n\tif (all & NETIF_F_NO_CSUM && !(one & NETIF_F_NO_CSUM))\n\t\tall ^= NETIF_F_NO_CSUM | (one & NETIF_F_ALL_CSUM);\n\telse if (mask & NETIF_F_ALL_CSUM) {\n\t\t/* If one device supports v4/v6 checksumming, set for all. */\n\t\tif (one & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM) &&\n\t\t    !(all & NETIF_F_GEN_CSUM)) {\n\t\t\tall &= ~NETIF_F_ALL_CSUM;\n\t\t\tall |= one & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);\n\t\t}\n\n\t\t/* If one device supports hw checksumming, set for all. */\n\t\tif (one & NETIF_F_GEN_CSUM && !(all & NETIF_F_GEN_CSUM)) {\n\t\t\tall &= ~NETIF_F_ALL_CSUM;\n\t\t\tall |= NETIF_F_HW_CSUM;\n\t\t}\n\t}\n\n\tone |= NETIF_F_ALL_CSUM;\n\n\tone |= all & NETIF_F_ONE_FOR_ALL;\n\tall &= one | NETIF_F_LLTX | NETIF_F_GSO | NETIF_F_UFO;\n\tall |= one & mask & NETIF_F_ONE_FOR_ALL;\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head *netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc(sizeof(*hash) * NETDEV_HASHENTRIES, GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\t@buffer: buffer for resulting name\n *\t@len: size of buffer\n *\n *\tDetermine network driver for device.\n */\nchar *netdev_drivername(const struct net_device *dev, char *buffer, int len)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\n\tif (len <= 0 || !buffer)\n\t\treturn buffer;\n\tbuffer[0] = 0;\n\n\tparent = dev->dev.parent;\n\n\tif (!parent)\n\t\treturn buffer;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\tstrlcpy(buffer, driver->name, len);\n\treturn buffer;\n}\n\nstatic int __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t   struct va_format *vaf)\n{\n\tint r;\n\n\tif (dev && dev->dev.parent)\n\t\tr = dev_printk(level, dev->dev.parent, \"%s: %pV\",\n\t\t\t       netdev_name(dev), vaf);\n\telse if (dev)\n\t\tr = printk(\"%s%s: %pV\", level, netdev_name(dev), vaf);\n\telse\n\t\tr = printk(\"%s(NULL net_device): %pV\", level, vaf);\n\n\treturn r;\n}\n\nint netdev_printk(const char *level, const struct net_device *dev,\n\t\t  const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\tint r;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\tr = __netdev_printk(level, dev, &vaf);\n\tva_end(args);\n\n\treturn r;\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nint func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tint r;\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tr = __netdev_printk(level, dev, &vaf);\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\treturn r;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\trtnl_lock();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops)\n\t\t\tcontinue;\n\n\t\t/* Push remaing network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tprintk(KERN_EMERG \"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t__func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n\trtnl_unlock();\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\trtnl_lock();\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\tlist_del(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit = default_device_exit,\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tmemset(sd, 0, sizeof(*sd));\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n\t\tsd->completion_queue = NULL;\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tsd->csd.func = rps_trigger_softirq;\n\t\tsd->csd.info = sd;\n\t\tsd->csd.flags = 0;\n\t\tsd->cpu = i;\n#endif\n\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t\tsd->backlog.gro_list = NULL;\n\t\tsd->backlog.gro_count = 0;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\thotcpu_notifier(dev_cpu_callback, 0);\n\tdst_init();\n\tdev_mcast_init();\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n\nstatic int __init initialize_hashrnd(void)\n{\n\tget_random_bytes(&hashrnd, sizeof(hashrnd));\n\treturn 0;\n}\n\nlate_initcall_sync(initialize_hashrnd);\n\n", "/*\n *\tLinux NET3:\tGRE over IP protocol decoder.\n *\n *\tAuthors: Alexey Kuznetsov (kuznet@ms2.inr.ac.ru)\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n */\n\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/in.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/if_arp.h>\n#include <linux/mroute.h>\n#include <linux/init.h>\n#include <linux/in6.h>\n#include <linux/inetdevice.h>\n#include <linux/igmp.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/etherdevice.h>\n#include <linux/if_ether.h>\n\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/protocol.h>\n#include <net/ipip.h>\n#include <net/arp.h>\n#include <net/checksum.h>\n#include <net/dsfield.h>\n#include <net/inet_ecn.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n#include <net/rtnetlink.h>\n#include <net/gre.h>\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n#include <net/ipv6.h>\n#include <net/ip6_fib.h>\n#include <net/ip6_route.h>\n#endif\n\n/*\n   Problems & solutions\n   --------------------\n\n   1. The most important issue is detecting local dead loops.\n   They would cause complete host lockup in transmit, which\n   would be \"resolved\" by stack overflow or, if queueing is enabled,\n   with infinite looping in net_bh.\n\n   We cannot track such dead loops during route installation,\n   it is infeasible task. The most general solutions would be\n   to keep skb->encapsulation counter (sort of local ttl),\n   and silently drop packet when it expires. It is a good\n   solution, but it supposes maintaing new variable in ALL\n   skb, even if no tunneling is used.\n\n   Current solution: xmit_recursion breaks dead loops. This is a percpu\n   counter, since when we enter the first ndo_xmit(), cpu migration is\n   forbidden. We force an exit if this counter reaches RECURSION_LIMIT\n\n   2. Networking dead loops would not kill routers, but would really\n   kill network. IP hop limit plays role of \"t->recursion\" in this case,\n   if we copy it from packet being encapsulated to upper header.\n   It is very good solution, but it introduces two problems:\n\n   - Routing protocols, using packets with ttl=1 (OSPF, RIP2),\n     do not work over tunnels.\n   - traceroute does not work. I planned to relay ICMP from tunnel,\n     so that this problem would be solved and traceroute output\n     would even more informative. This idea appeared to be wrong:\n     only Linux complies to rfc1812 now (yes, guys, Linux is the only\n     true router now :-)), all routers (at least, in neighbourhood of mine)\n     return only 8 bytes of payload. It is the end.\n\n   Hence, if we want that OSPF worked or traceroute said something reasonable,\n   we should search for another solution.\n\n   One of them is to parse packet trying to detect inner encapsulation\n   made by our node. It is difficult or even impossible, especially,\n   taking into account fragmentation. TO be short, tt is not solution at all.\n\n   Current solution: The solution was UNEXPECTEDLY SIMPLE.\n   We force DF flag on tunnels with preconfigured hop limit,\n   that is ALL. :-) Well, it does not remove the problem completely,\n   but exponential growth of network traffic is changed to linear\n   (branches, that exceed pmtu are pruned) and tunnel mtu\n   fastly degrades to value <68, where looping stops.\n   Yes, it is not good if there exists a router in the loop,\n   which does not force DF, even when encapsulating packets have DF set.\n   But it is not our problem! Nobody could accuse us, we made\n   all that we could make. Even if it is your gated who injected\n   fatal route to network, even if it were you who configured\n   fatal static route: you are innocent. :-)\n\n\n\n   3. Really, ipv4/ipip.c, ipv4/ip_gre.c and ipv6/sit.c contain\n   practically identical code. It would be good to glue them\n   together, but it is not very evident, how to make them modular.\n   sit is integral part of IPv6, ipip and gre are naturally modular.\n   We could extract common parts (hash table, ioctl etc)\n   to a separate module (ip_tunnel.c).\n\n   Alexey Kuznetsov.\n */\n\nstatic struct rtnl_link_ops ipgre_link_ops __read_mostly;\nstatic int ipgre_tunnel_init(struct net_device *dev);\nstatic void ipgre_tunnel_setup(struct net_device *dev);\nstatic int ipgre_tunnel_bind_dev(struct net_device *dev);\n\n/* Fallback tunnel: no source, no destination, no key, no options */\n\n#define HASH_SIZE  16\n\nstatic int ipgre_net_id __read_mostly;\nstruct ipgre_net {\n\tstruct ip_tunnel __rcu *tunnels[4][HASH_SIZE];\n\n\tstruct net_device *fb_tunnel_dev;\n};\n\n/* Tunnel hash table */\n\n/*\n   4 hash tables:\n\n   3: (remote,local)\n   2: (remote,*)\n   1: (*,local)\n   0: (*,*)\n\n   We require exact key match i.e. if a key is present in packet\n   it will match only tunnel with the same key; if it is not present,\n   it will match only keyless tunnel.\n\n   All keysless packets, if not matched configured keyless tunnels\n   will match fallback tunnel.\n */\n\n#define HASH(addr) (((__force u32)addr^((__force u32)addr>>4))&0xF)\n\n#define tunnels_r_l\ttunnels[3]\n#define tunnels_r\ttunnels[2]\n#define tunnels_l\ttunnels[1]\n#define tunnels_wc\ttunnels[0]\n/*\n * Locking : hash tables are protected by RCU and RTNL\n */\n\n#define for_each_ip_tunnel_rcu(start) \\\n\tfor (t = rcu_dereference(start); t; t = rcu_dereference(t->next))\n\n/* often modified stats are per cpu, other are shared (netdev->stats) */\nstruct pcpu_tstats {\n\tunsigned long\trx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_packets;\n\tunsigned long\ttx_bytes;\n};\n\nstatic struct net_device_stats *ipgre_get_stats(struct net_device *dev)\n{\n\tstruct pcpu_tstats sum = { 0 };\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tconst struct pcpu_tstats *tstats = per_cpu_ptr(dev->tstats, i);\n\n\t\tsum.rx_packets += tstats->rx_packets;\n\t\tsum.rx_bytes   += tstats->rx_bytes;\n\t\tsum.tx_packets += tstats->tx_packets;\n\t\tsum.tx_bytes   += tstats->tx_bytes;\n\t}\n\tdev->stats.rx_packets = sum.rx_packets;\n\tdev->stats.rx_bytes   = sum.rx_bytes;\n\tdev->stats.tx_packets = sum.tx_packets;\n\tdev->stats.tx_bytes   = sum.tx_bytes;\n\treturn &dev->stats;\n}\n\n/* Given src, dst and key, find appropriate for input tunnel. */\n\nstatic struct ip_tunnel * ipgre_tunnel_lookup(struct net_device *dev,\n\t\t\t\t\t      __be32 remote, __be32 local,\n\t\t\t\t\t      __be32 key, __be16 gre_proto)\n{\n\tstruct net *net = dev_net(dev);\n\tint link = dev->ifindex;\n\tunsigned int h0 = HASH(remote);\n\tunsigned int h1 = HASH(key);\n\tstruct ip_tunnel *t, *cand = NULL;\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\tint dev_type = (gre_proto == htons(ETH_P_TEB)) ?\n\t\t       ARPHRD_ETHER : ARPHRD_IPGRE;\n\tint score, cand_score = 4;\n\n\tfor_each_ip_tunnel_rcu(ign->tunnels_r_l[h0 ^ h1]) {\n\t\tif (local != t->parms.iph.saddr ||\n\t\t    remote != t->parms.iph.daddr ||\n\t\t    key != t->parms.i_key ||\n\t\t    !(t->dev->flags & IFF_UP))\n\t\t\tcontinue;\n\n\t\tif (t->dev->type != ARPHRD_IPGRE &&\n\t\t    t->dev->type != dev_type)\n\t\t\tcontinue;\n\n\t\tscore = 0;\n\t\tif (t->parms.link != link)\n\t\t\tscore |= 1;\n\t\tif (t->dev->type != dev_type)\n\t\t\tscore |= 2;\n\t\tif (score == 0)\n\t\t\treturn t;\n\n\t\tif (score < cand_score) {\n\t\t\tcand = t;\n\t\t\tcand_score = score;\n\t\t}\n\t}\n\n\tfor_each_ip_tunnel_rcu(ign->tunnels_r[h0 ^ h1]) {\n\t\tif (remote != t->parms.iph.daddr ||\n\t\t    key != t->parms.i_key ||\n\t\t    !(t->dev->flags & IFF_UP))\n\t\t\tcontinue;\n\n\t\tif (t->dev->type != ARPHRD_IPGRE &&\n\t\t    t->dev->type != dev_type)\n\t\t\tcontinue;\n\n\t\tscore = 0;\n\t\tif (t->parms.link != link)\n\t\t\tscore |= 1;\n\t\tif (t->dev->type != dev_type)\n\t\t\tscore |= 2;\n\t\tif (score == 0)\n\t\t\treturn t;\n\n\t\tif (score < cand_score) {\n\t\t\tcand = t;\n\t\t\tcand_score = score;\n\t\t}\n\t}\n\n\tfor_each_ip_tunnel_rcu(ign->tunnels_l[h1]) {\n\t\tif ((local != t->parms.iph.saddr &&\n\t\t     (local != t->parms.iph.daddr ||\n\t\t      !ipv4_is_multicast(local))) ||\n\t\t    key != t->parms.i_key ||\n\t\t    !(t->dev->flags & IFF_UP))\n\t\t\tcontinue;\n\n\t\tif (t->dev->type != ARPHRD_IPGRE &&\n\t\t    t->dev->type != dev_type)\n\t\t\tcontinue;\n\n\t\tscore = 0;\n\t\tif (t->parms.link != link)\n\t\t\tscore |= 1;\n\t\tif (t->dev->type != dev_type)\n\t\t\tscore |= 2;\n\t\tif (score == 0)\n\t\t\treturn t;\n\n\t\tif (score < cand_score) {\n\t\t\tcand = t;\n\t\t\tcand_score = score;\n\t\t}\n\t}\n\n\tfor_each_ip_tunnel_rcu(ign->tunnels_wc[h1]) {\n\t\tif (t->parms.i_key != key ||\n\t\t    !(t->dev->flags & IFF_UP))\n\t\t\tcontinue;\n\n\t\tif (t->dev->type != ARPHRD_IPGRE &&\n\t\t    t->dev->type != dev_type)\n\t\t\tcontinue;\n\n\t\tscore = 0;\n\t\tif (t->parms.link != link)\n\t\t\tscore |= 1;\n\t\tif (t->dev->type != dev_type)\n\t\t\tscore |= 2;\n\t\tif (score == 0)\n\t\t\treturn t;\n\n\t\tif (score < cand_score) {\n\t\t\tcand = t;\n\t\t\tcand_score = score;\n\t\t}\n\t}\n\n\tif (cand != NULL)\n\t\treturn cand;\n\n\tdev = ign->fb_tunnel_dev;\n\tif (dev->flags & IFF_UP)\n\t\treturn netdev_priv(dev);\n\n\treturn NULL;\n}\n\nstatic struct ip_tunnel __rcu **__ipgre_bucket(struct ipgre_net *ign,\n\t\tstruct ip_tunnel_parm *parms)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\t__be32 key = parms->i_key;\n\tunsigned int h = HASH(key);\n\tint prio = 0;\n\n\tif (local)\n\t\tprio |= 1;\n\tif (remote && !ipv4_is_multicast(remote)) {\n\t\tprio |= 2;\n\t\th ^= HASH(remote);\n\t}\n\n\treturn &ign->tunnels[prio][h];\n}\n\nstatic inline struct ip_tunnel __rcu **ipgre_bucket(struct ipgre_net *ign,\n\t\tstruct ip_tunnel *t)\n{\n\treturn __ipgre_bucket(ign, &t->parms);\n}\n\nstatic void ipgre_tunnel_link(struct ipgre_net *ign, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp = ipgre_bucket(ign, t);\n\n\trcu_assign_pointer(t->next, rtnl_dereference(*tp));\n\trcu_assign_pointer(*tp, t);\n}\n\nstatic void ipgre_tunnel_unlink(struct ipgre_net *ign, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp;\n\tstruct ip_tunnel *iter;\n\n\tfor (tp = ipgre_bucket(ign, t);\n\t     (iter = rtnl_dereference(*tp)) != NULL;\n\t     tp = &iter->next) {\n\t\tif (t == iter) {\n\t\t\trcu_assign_pointer(*tp, t->next);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic struct ip_tunnel *ipgre_tunnel_find(struct net *net,\n\t\t\t\t\t   struct ip_tunnel_parm *parms,\n\t\t\t\t\t   int type)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\t__be32 key = parms->i_key;\n\tint link = parms->link;\n\tstruct ip_tunnel *t;\n\tstruct ip_tunnel __rcu **tp;\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\n\tfor (tp = __ipgre_bucket(ign, parms);\n\t     (t = rtnl_dereference(*tp)) != NULL;\n\t     tp = &t->next)\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    remote == t->parms.iph.daddr &&\n\t\t    key == t->parms.i_key &&\n\t\t    link == t->parms.link &&\n\t\t    type == t->dev->type)\n\t\t\tbreak;\n\n\treturn t;\n}\n\nstatic struct ip_tunnel *ipgre_tunnel_locate(struct net *net,\n\t\tstruct ip_tunnel_parm *parms, int create)\n{\n\tstruct ip_tunnel *t, *nt;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\n\tt = ipgre_tunnel_find(net, parms, ARPHRD_IPGRE);\n\tif (t || !create)\n\t\treturn t;\n\n\tif (parms->name[0])\n\t\tstrlcpy(name, parms->name, IFNAMSIZ);\n\telse\n\t\tstrcpy(name, \"gre%d\");\n\n\tdev = alloc_netdev(sizeof(*t), name, ipgre_tunnel_setup);\n\tif (!dev)\n\t\treturn NULL;\n\n\tdev_net_set(dev, net);\n\n\tif (strchr(name, '%')) {\n\t\tif (dev_alloc_name(dev, name) < 0)\n\t\t\tgoto failed_free;\n\t}\n\n\tnt = netdev_priv(dev);\n\tnt->parms = *parms;\n\tdev->rtnl_link_ops = &ipgre_link_ops;\n\n\tdev->mtu = ipgre_tunnel_bind_dev(dev);\n\n\tif (register_netdevice(dev) < 0)\n\t\tgoto failed_free;\n\n\tdev_hold(dev);\n\tipgre_tunnel_link(ign, nt);\n\treturn nt;\n\nfailed_free:\n\tfree_netdev(dev);\n\treturn NULL;\n}\n\nstatic void ipgre_tunnel_uninit(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\n\tipgre_tunnel_unlink(ign, netdev_priv(dev));\n\tdev_put(dev);\n}\n\n\nstatic void ipgre_err(struct sk_buff *skb, u32 info)\n{\n\n/* All the routers (except for Linux) return only\n   8 bytes of packet payload. It means, that precise relaying of\n   ICMP in the real Internet is absolutely infeasible.\n\n   Moreover, Cisco \"wise men\" put GRE key to the third word\n   in GRE header. It makes impossible maintaining even soft state for keyed\n   GRE tunnels with enabled checksum. Tell them \"thank you\".\n\n   Well, I wonder, rfc1812 was written by Cisco employee,\n   what the hell these idiots break standrads established\n   by themself???\n */\n\n\tstruct iphdr *iph = (struct iphdr *)skb->data;\n\t__be16\t     *p = (__be16*)(skb->data+(iph->ihl<<2));\n\tint grehlen = (iph->ihl<<2) + 4;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct ip_tunnel *t;\n\t__be16 flags;\n\n\tflags = p[0];\n\tif (flags&(GRE_CSUM|GRE_KEY|GRE_SEQ|GRE_ROUTING|GRE_VERSION)) {\n\t\tif (flags&(GRE_VERSION|GRE_ROUTING))\n\t\t\treturn;\n\t\tif (flags&GRE_KEY) {\n\t\t\tgrehlen += 4;\n\t\t\tif (flags&GRE_CSUM)\n\t\t\t\tgrehlen += 4;\n\t\t}\n\t}\n\n\t/* If only 8 bytes returned, keyed message will be dropped here */\n\tif (skb_headlen(skb) < grehlen)\n\t\treturn;\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_PARAMETERPROB:\n\t\treturn;\n\n\tcase ICMP_DEST_UNREACH:\n\t\tswitch (code) {\n\t\tcase ICMP_SR_FAILED:\n\t\tcase ICMP_PORT_UNREACH:\n\t\t\t/* Impossible event. */\n\t\t\treturn;\n\t\tcase ICMP_FRAG_NEEDED:\n\t\t\t/* Soft state for pmtu is maintained by IP core. */\n\t\t\treturn;\n\t\tdefault:\n\t\t\t/* All others are translated to HOST_UNREACH.\n\t\t\t   rfc2003 contains \"deep thoughts\" about NET_UNREACH,\n\t\t\t   I believe they are just ether pollution. --ANK\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\tif (code != ICMP_EXC_TTL)\n\t\t\treturn;\n\t\tbreak;\n\t}\n\n\trcu_read_lock();\n\tt = ipgre_tunnel_lookup(skb->dev, iph->daddr, iph->saddr,\n\t\t\t\tflags & GRE_KEY ?\n\t\t\t\t*(((__be32 *)p) + (grehlen / 4) - 1) : 0,\n\t\t\t\tp[1]);\n\tif (t == NULL || t->parms.iph.daddr == 0 ||\n\t    ipv4_is_multicast(t->parms.iph.daddr))\n\t\tgoto out;\n\n\tif (t->parms.iph.ttl == 0 && type == ICMP_TIME_EXCEEDED)\n\t\tgoto out;\n\n\tif (time_before(jiffies, t->err_time + IPTUNNEL_ERR_TIMEO))\n\t\tt->err_count++;\n\telse\n\t\tt->err_count = 1;\n\tt->err_time = jiffies;\nout:\n\trcu_read_unlock();\n}\n\nstatic inline void ipgre_ecn_decapsulate(struct iphdr *iph, struct sk_buff *skb)\n{\n\tif (INET_ECN_is_ce(iph->tos)) {\n\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\tIP_ECN_set_ce(ip_hdr(skb));\n\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tIP6_ECN_set_ce(ipv6_hdr(skb));\n\t\t}\n\t}\n}\n\nstatic inline u8\nipgre_ecn_encapsulate(u8 tos, struct iphdr *old_iph, struct sk_buff *skb)\n{\n\tu8 inner = 0;\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\tinner = old_iph->tos;\n\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\tinner = ipv6_get_dsfield((struct ipv6hdr *)old_iph);\n\treturn INET_ECN_encapsulate(tos, inner);\n}\n\nstatic int ipgre_rcv(struct sk_buff *skb)\n{\n\tstruct iphdr *iph;\n\tu8     *h;\n\t__be16    flags;\n\t__sum16   csum = 0;\n\t__be32 key = 0;\n\tu32    seqno = 0;\n\tstruct ip_tunnel *tunnel;\n\tint    offset = 4;\n\t__be16 gre_proto;\n\n\tif (!pskb_may_pull(skb, 16))\n\t\tgoto drop_nolock;\n\n\tiph = ip_hdr(skb);\n\th = skb->data;\n\tflags = *(__be16*)h;\n\n\tif (flags&(GRE_CSUM|GRE_KEY|GRE_ROUTING|GRE_SEQ|GRE_VERSION)) {\n\t\t/* - Version must be 0.\n\t\t   - We do not support routing headers.\n\t\t */\n\t\tif (flags&(GRE_VERSION|GRE_ROUTING))\n\t\t\tgoto drop_nolock;\n\n\t\tif (flags&GRE_CSUM) {\n\t\t\tswitch (skb->ip_summed) {\n\t\t\tcase CHECKSUM_COMPLETE:\n\t\t\t\tcsum = csum_fold(skb->csum);\n\t\t\t\tif (!csum)\n\t\t\t\t\tbreak;\n\t\t\t\t/* fall through */\n\t\t\tcase CHECKSUM_NONE:\n\t\t\t\tskb->csum = 0;\n\t\t\t\tcsum = __skb_checksum_complete(skb);\n\t\t\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\t\t}\n\t\t\toffset += 4;\n\t\t}\n\t\tif (flags&GRE_KEY) {\n\t\t\tkey = *(__be32*)(h + offset);\n\t\t\toffset += 4;\n\t\t}\n\t\tif (flags&GRE_SEQ) {\n\t\t\tseqno = ntohl(*(__be32*)(h + offset));\n\t\t\toffset += 4;\n\t\t}\n\t}\n\n\tgre_proto = *(__be16 *)(h + 2);\n\n\trcu_read_lock();\n\tif ((tunnel = ipgre_tunnel_lookup(skb->dev,\n\t\t\t\t\t  iph->saddr, iph->daddr, key,\n\t\t\t\t\t  gre_proto))) {\n\t\tstruct pcpu_tstats *tstats;\n\n\t\tsecpath_reset(skb);\n\n\t\tskb->protocol = gre_proto;\n\t\t/* WCCP version 1 and 2 protocol decoding.\n\t\t * - Change protocol to IP\n\t\t * - When dealing with WCCPv2, Skip extra 4 bytes in GRE header\n\t\t */\n\t\tif (flags == 0 && gre_proto == htons(ETH_P_WCCP)) {\n\t\t\tskb->protocol = htons(ETH_P_IP);\n\t\t\tif ((*(h + offset) & 0xF0) != 0x40)\n\t\t\t\toffset += 4;\n\t\t}\n\n\t\tskb->mac_header = skb->network_header;\n\t\t__pskb_pull(skb, offset);\n\t\tskb_postpull_rcsum(skb, skb_transport_header(skb), offset);\n\t\tskb->pkt_type = PACKET_HOST;\n#ifdef CONFIG_NET_IPGRE_BROADCAST\n\t\tif (ipv4_is_multicast(iph->daddr)) {\n\t\t\t/* Looped back packet, drop it! */\n\t\t\tif (rt_is_output_route(skb_rtable(skb)))\n\t\t\t\tgoto drop;\n\t\t\ttunnel->dev->stats.multicast++;\n\t\t\tskb->pkt_type = PACKET_BROADCAST;\n\t\t}\n#endif\n\n\t\tif (((flags&GRE_CSUM) && csum) ||\n\t\t    (!(flags&GRE_CSUM) && tunnel->parms.i_flags&GRE_CSUM)) {\n\t\t\ttunnel->dev->stats.rx_crc_errors++;\n\t\t\ttunnel->dev->stats.rx_errors++;\n\t\t\tgoto drop;\n\t\t}\n\t\tif (tunnel->parms.i_flags&GRE_SEQ) {\n\t\t\tif (!(flags&GRE_SEQ) ||\n\t\t\t    (tunnel->i_seqno && (s32)(seqno - tunnel->i_seqno) < 0)) {\n\t\t\t\ttunnel->dev->stats.rx_fifo_errors++;\n\t\t\t\ttunnel->dev->stats.rx_errors++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\ttunnel->i_seqno = seqno + 1;\n\t\t}\n\n\t\t/* Warning: All skb pointers will be invalidated! */\n\t\tif (tunnel->dev->type == ARPHRD_ETHER) {\n\t\t\tif (!pskb_may_pull(skb, ETH_HLEN)) {\n\t\t\t\ttunnel->dev->stats.rx_length_errors++;\n\t\t\t\ttunnel->dev->stats.rx_errors++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\n\t\t\tiph = ip_hdr(skb);\n\t\t\tskb->protocol = eth_type_trans(skb, tunnel->dev);\n\t\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t\t}\n\n\t\ttstats = this_cpu_ptr(tunnel->dev->tstats);\n\t\ttstats->rx_packets++;\n\t\ttstats->rx_bytes += skb->len;\n\n\t\t__skb_tunnel_rx(skb, tunnel->dev);\n\n\t\tskb_reset_network_header(skb);\n\t\tipgre_ecn_decapsulate(iph, skb);\n\n\t\tnetif_rx(skb);\n\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);\n\ndrop:\n\trcu_read_unlock();\ndrop_nolock:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic netdev_tx_t ipgre_tunnel_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct pcpu_tstats *tstats;\n\tstruct iphdr  *old_iph = ip_hdr(skb);\n\tstruct iphdr  *tiph;\n\tu8     tos;\n\t__be16 df;\n\tstruct rtable *rt;     \t\t\t/* Route to the other host */\n\tstruct net_device *tdev;\t\t/* Device to other host */\n\tstruct iphdr  *iph;\t\t\t/* Our new IP header */\n\tunsigned int max_headroom;\t\t/* The extra header space needed */\n\tint    gre_hlen;\n\t__be32 dst;\n\tint    mtu;\n\n\tif (dev->type == ARPHRD_ETHER)\n\t\tIPCB(skb)->flags = 0;\n\n\tif (dev->header_ops && dev->type == ARPHRD_IPGRE) {\n\t\tgre_hlen = 0;\n\t\ttiph = (struct iphdr *)skb->data;\n\t} else {\n\t\tgre_hlen = tunnel->hlen;\n\t\ttiph = &tunnel->parms.iph;\n\t}\n\n\tif ((dst = tiph->daddr) == 0) {\n\t\t/* NBMA tunnel */\n\n\t\tif (skb_dst(skb) == NULL) {\n\t\t\tdev->stats.tx_fifo_errors++;\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\trt = skb_rtable(skb);\n\t\t\tif ((dst = rt->rt_gateway) == 0)\n\t\t\t\tgoto tx_error_icmp;\n\t\t}\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\telse if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tstruct in6_addr *addr6;\n\t\t\tint addr_type;\n\t\t\tstruct neighbour *neigh = skb_dst(skb)->neighbour;\n\n\t\t\tif (neigh == NULL)\n\t\t\t\tgoto tx_error;\n\n\t\t\taddr6 = (struct in6_addr *)&neigh->primary_key;\n\t\t\taddr_type = ipv6_addr_type(addr6);\n\n\t\t\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t\t\taddr6 = &ipv6_hdr(skb)->daddr;\n\t\t\t\taddr_type = ipv6_addr_type(addr6);\n\t\t\t}\n\n\t\t\tif ((addr_type & IPV6_ADDR_COMPATv4) == 0)\n\t\t\t\tgoto tx_error_icmp;\n\n\t\t\tdst = addr6->s6_addr32[3];\n\t\t}\n#endif\n\t\telse\n\t\t\tgoto tx_error;\n\t}\n\n\ttos = tiph->tos;\n\tif (tos == 1) {\n\t\ttos = 0;\n\t\tif (skb->protocol == htons(ETH_P_IP))\n\t\t\ttos = old_iph->tos;\n\t\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\t\ttos = ipv6_get_dsfield((struct ipv6hdr *)old_iph);\n\t}\n\n\t{\n\t\tstruct flowi fl = {\n\t\t\t.oif = tunnel->parms.link,\n\t\t\t.fl4_dst = dst,\n\t\t\t.fl4_src = tiph->saddr,\n\t\t\t.fl4_tos = RT_TOS(tos),\n\t\t\t.proto = IPPROTO_GRE,\n\t\t\t.fl_gre_key = tunnel->parms.o_key\n\t\t};\n\t\tif (ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\tdev->stats.tx_carrier_errors++;\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n\ttdev = rt->dst.dev;\n\n\tif (tdev == dev) {\n\t\tip_rt_put(rt);\n\t\tdev->stats.collisions++;\n\t\tgoto tx_error;\n\t}\n\n\tdf = tiph->frag_off;\n\tif (df)\n\t\tmtu = dst_mtu(&rt->dst) - dev->hard_header_len - tunnel->hlen;\n\telse\n\t\tmtu = skb_dst(skb) ? dst_mtu(skb_dst(skb)) : dev->mtu;\n\n\tif (skb_dst(skb))\n\t\tskb_dst(skb)->ops->update_pmtu(skb_dst(skb), mtu);\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\tdf |= (old_iph->frag_off&htons(IP_DF));\n\n\t\tif ((old_iph->frag_off&htons(IP_DF)) &&\n\t\t    mtu < ntohs(old_iph->tot_len)) {\n\t\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\telse if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\tstruct rt6_info *rt6 = (struct rt6_info *)skb_dst(skb);\n\n\t\tif (rt6 && mtu < dst_mtu(skb_dst(skb)) && mtu >= IPV6_MIN_MTU) {\n\t\t\tif ((tunnel->parms.iph.daddr &&\n\t\t\t     !ipv4_is_multicast(tunnel->parms.iph.daddr)) ||\n\t\t\t    rt6->rt6i_dst.plen == 128) {\n\t\t\t\trt6->rt6i_flags |= RTF_MODIFIED;\n\t\t\t\tdst_metric_set(skb_dst(skb), RTAX_MTU, mtu);\n\t\t\t}\n\t\t}\n\n\t\tif (mtu >= IPV6_MIN_MTU && mtu < skb->len - tunnel->hlen + gre_hlen) {\n\t\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n#endif\n\n\tif (tunnel->err_count > 0) {\n\t\tif (time_before(jiffies,\n\t\t\t\ttunnel->err_time + IPTUNNEL_ERR_TIMEO)) {\n\t\t\ttunnel->err_count--;\n\n\t\t\tdst_link_failure(skb);\n\t\t} else\n\t\t\ttunnel->err_count = 0;\n\t}\n\n\tmax_headroom = LL_RESERVED_SPACE(tdev) + gre_hlen + rt->dst.header_len;\n\n\tif (skb_headroom(skb) < max_headroom || skb_shared(skb)||\n\t    (skb_cloned(skb) && !skb_clone_writable(skb, 0))) {\n\t\tstruct sk_buff *new_skb = skb_realloc_headroom(skb, max_headroom);\n\t\tif (max_headroom > dev->needed_headroom)\n\t\t\tdev->needed_headroom = max_headroom;\n\t\tif (!new_skb) {\n\t\t\tip_rt_put(rt);\n\t\t\tdev->stats.tx_dropped++;\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(new_skb, skb->sk);\n\t\tdev_kfree_skb(skb);\n\t\tskb = new_skb;\n\t\told_iph = ip_hdr(skb);\n\t}\n\n\tskb_reset_transport_header(skb);\n\tskb_push(skb, gre_hlen);\n\tskb_reset_network_header(skb);\n\tmemset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));\n\tIPCB(skb)->flags &= ~(IPSKB_XFRM_TUNNEL_SIZE | IPSKB_XFRM_TRANSFORMED |\n\t\t\t      IPSKB_REROUTED);\n\tskb_dst_drop(skb);\n\tskb_dst_set(skb, &rt->dst);\n\n\t/*\n\t *\tPush down and install the IPIP header.\n\t */\n\n\tiph \t\t\t=\tip_hdr(skb);\n\tiph->version\t\t=\t4;\n\tiph->ihl\t\t=\tsizeof(struct iphdr) >> 2;\n\tiph->frag_off\t\t=\tdf;\n\tiph->protocol\t\t=\tIPPROTO_GRE;\n\tiph->tos\t\t=\tipgre_ecn_encapsulate(tos, old_iph, skb);\n\tiph->daddr\t\t=\trt->rt_dst;\n\tiph->saddr\t\t=\trt->rt_src;\n\n\tif ((iph->ttl = tiph->ttl) == 0) {\n\t\tif (skb->protocol == htons(ETH_P_IP))\n\t\t\tiph->ttl = old_iph->ttl;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\t\tiph->ttl = ((struct ipv6hdr *)old_iph)->hop_limit;\n#endif\n\t\telse\n\t\t\tiph->ttl = ip4_dst_hoplimit(&rt->dst);\n\t}\n\n\t((__be16 *)(iph + 1))[0] = tunnel->parms.o_flags;\n\t((__be16 *)(iph + 1))[1] = (dev->type == ARPHRD_ETHER) ?\n\t\t\t\t   htons(ETH_P_TEB) : skb->protocol;\n\n\tif (tunnel->parms.o_flags&(GRE_KEY|GRE_CSUM|GRE_SEQ)) {\n\t\t__be32 *ptr = (__be32*)(((u8*)iph) + tunnel->hlen - 4);\n\n\t\tif (tunnel->parms.o_flags&GRE_SEQ) {\n\t\t\t++tunnel->o_seqno;\n\t\t\t*ptr = htonl(tunnel->o_seqno);\n\t\t\tptr--;\n\t\t}\n\t\tif (tunnel->parms.o_flags&GRE_KEY) {\n\t\t\t*ptr = tunnel->parms.o_key;\n\t\t\tptr--;\n\t\t}\n\t\tif (tunnel->parms.o_flags&GRE_CSUM) {\n\t\t\t*ptr = 0;\n\t\t\t*(__sum16*)ptr = ip_compute_csum((void*)(iph+1), skb->len - sizeof(struct iphdr));\n\t\t}\n\t}\n\n\tnf_reset(skb);\n\ttstats = this_cpu_ptr(dev->tstats);\n\t__IPTUNNEL_XMIT(tstats, &dev->stats);\n\treturn NETDEV_TX_OK;\n\ntx_error_icmp:\n\tdst_link_failure(skb);\n\ntx_error:\n\tdev->stats.tx_errors++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic int ipgre_tunnel_bind_dev(struct net_device *dev)\n{\n\tstruct net_device *tdev = NULL;\n\tstruct ip_tunnel *tunnel;\n\tstruct iphdr *iph;\n\tint hlen = LL_MAX_HEADER;\n\tint mtu = ETH_DATA_LEN;\n\tint addend = sizeof(struct iphdr) + 4;\n\n\ttunnel = netdev_priv(dev);\n\tiph = &tunnel->parms.iph;\n\n\t/* Guess output device to choose reasonable mtu and needed_headroom */\n\n\tif (iph->daddr) {\n\t\tstruct flowi fl = {\n\t\t\t.oif = tunnel->parms.link,\n\t\t\t.fl4_dst = iph->daddr,\n\t\t\t.fl4_src = iph->saddr,\n\t\t\t.fl4_tos = RT_TOS(iph->tos),\n\t\t\t.proto = IPPROTO_GRE,\n\t\t\t.fl_gre_key = tunnel->parms.o_key\n\t\t};\n\t\tstruct rtable *rt;\n\n\t\tif (!ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\ttdev = rt->dst.dev;\n\t\t\tip_rt_put(rt);\n\t\t}\n\n\t\tif (dev->type != ARPHRD_ETHER)\n\t\t\tdev->flags |= IFF_POINTOPOINT;\n\t}\n\n\tif (!tdev && tunnel->parms.link)\n\t\ttdev = __dev_get_by_index(dev_net(dev), tunnel->parms.link);\n\n\tif (tdev) {\n\t\thlen = tdev->hard_header_len + tdev->needed_headroom;\n\t\tmtu = tdev->mtu;\n\t}\n\tdev->iflink = tunnel->parms.link;\n\n\t/* Precalculate GRE options length */\n\tif (tunnel->parms.o_flags&(GRE_CSUM|GRE_KEY|GRE_SEQ)) {\n\t\tif (tunnel->parms.o_flags&GRE_CSUM)\n\t\t\taddend += 4;\n\t\tif (tunnel->parms.o_flags&GRE_KEY)\n\t\t\taddend += 4;\n\t\tif (tunnel->parms.o_flags&GRE_SEQ)\n\t\t\taddend += 4;\n\t}\n\tdev->needed_headroom = addend + hlen;\n\tmtu -= dev->hard_header_len + addend;\n\n\tif (mtu < 68)\n\t\tmtu = 68;\n\n\ttunnel->hlen = addend;\n\n\treturn mtu;\n}\n\nstatic int\nipgre_tunnel_ioctl (struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tint err = 0;\n\tstruct ip_tunnel_parm p;\n\tstruct ip_tunnel *t;\n\tstruct net *net = dev_net(dev);\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\n\tswitch (cmd) {\n\tcase SIOCGETTUNNEL:\n\t\tt = NULL;\n\t\tif (dev == ign->fb_tunnel_dev) {\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tt = ipgre_tunnel_locate(net, &p, 0);\n\t\t}\n\t\tif (t == NULL)\n\t\t\tt = netdev_priv(dev);\n\t\tmemcpy(&p, &t->parms, sizeof(p));\n\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &p, sizeof(p)))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase SIOCADDTUNNEL:\n\tcase SIOCCHGTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\tgoto done;\n\n\t\terr = -EINVAL;\n\t\tif (p.iph.version != 4 || p.iph.protocol != IPPROTO_GRE ||\n\t\t    p.iph.ihl != 5 || (p.iph.frag_off&htons(~IP_DF)) ||\n\t\t    ((p.i_flags|p.o_flags)&(GRE_VERSION|GRE_ROUTING)))\n\t\t\tgoto done;\n\t\tif (p.iph.ttl)\n\t\t\tp.iph.frag_off |= htons(IP_DF);\n\n\t\tif (!(p.i_flags&GRE_KEY))\n\t\t\tp.i_key = 0;\n\t\tif (!(p.o_flags&GRE_KEY))\n\t\t\tp.o_key = 0;\n\n\t\tt = ipgre_tunnel_locate(net, &p, cmd == SIOCADDTUNNEL);\n\n\t\tif (dev != ign->fb_tunnel_dev && cmd == SIOCCHGTUNNEL) {\n\t\t\tif (t != NULL) {\n\t\t\t\tif (t->dev != dev) {\n\t\t\t\t\terr = -EEXIST;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tunsigned int nflags = 0;\n\n\t\t\t\tt = netdev_priv(dev);\n\n\t\t\t\tif (ipv4_is_multicast(p.iph.daddr))\n\t\t\t\t\tnflags = IFF_BROADCAST;\n\t\t\t\telse if (p.iph.daddr)\n\t\t\t\t\tnflags = IFF_POINTOPOINT;\n\n\t\t\t\tif ((dev->flags^nflags)&(IFF_POINTOPOINT|IFF_BROADCAST)) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tipgre_tunnel_unlink(ign, t);\n\t\t\t\tsynchronize_net();\n\t\t\t\tt->parms.iph.saddr = p.iph.saddr;\n\t\t\t\tt->parms.iph.daddr = p.iph.daddr;\n\t\t\t\tt->parms.i_key = p.i_key;\n\t\t\t\tt->parms.o_key = p.o_key;\n\t\t\t\tmemcpy(dev->dev_addr, &p.iph.saddr, 4);\n\t\t\t\tmemcpy(dev->broadcast, &p.iph.daddr, 4);\n\t\t\t\tipgre_tunnel_link(ign, t);\n\t\t\t\tnetdev_state_change(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (t) {\n\t\t\terr = 0;\n\t\t\tif (cmd == SIOCCHGTUNNEL) {\n\t\t\t\tt->parms.iph.ttl = p.iph.ttl;\n\t\t\t\tt->parms.iph.tos = p.iph.tos;\n\t\t\t\tt->parms.iph.frag_off = p.iph.frag_off;\n\t\t\t\tif (t->parms.link != p.link) {\n\t\t\t\t\tt->parms.link = p.link;\n\t\t\t\t\tdev->mtu = ipgre_tunnel_bind_dev(dev);\n\t\t\t\t\tnetdev_state_change(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &t->parms, sizeof(p)))\n\t\t\t\terr = -EFAULT;\n\t\t} else\n\t\t\terr = (cmd == SIOCADDTUNNEL ? -ENOBUFS : -ENOENT);\n\t\tbreak;\n\n\tcase SIOCDELTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\tif (dev == ign->fb_tunnel_dev) {\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\t\tgoto done;\n\t\t\terr = -ENOENT;\n\t\t\tif ((t = ipgre_tunnel_locate(net, &p, 0)) == NULL)\n\t\t\t\tgoto done;\n\t\t\terr = -EPERM;\n\t\t\tif (t == netdev_priv(ign->fb_tunnel_dev))\n\t\t\t\tgoto done;\n\t\t\tdev = t->dev;\n\t\t}\n\t\tunregister_netdevice(dev);\n\t\terr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\ndone:\n\treturn err;\n}\n\nstatic int ipgre_tunnel_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tif (new_mtu < 68 ||\n\t    new_mtu > 0xFFF8 - dev->hard_header_len - tunnel->hlen)\n\t\treturn -EINVAL;\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\n/* Nice toy. Unfortunately, useless in real life :-)\n   It allows to construct virtual multiprotocol broadcast \"LAN\"\n   over the Internet, provided multicast routing is tuned.\n\n\n   I have no idea was this bicycle invented before me,\n   so that I had to set ARPHRD_IPGRE to a random value.\n   I have an impression, that Cisco could make something similar,\n   but this feature is apparently missing in IOS<=11.2(8).\n\n   I set up 10.66.66/24 and fec0:6666:6666::0/96 as virtual networks\n   with broadcast 224.66.66.66. If you have access to mbone, play with me :-)\n\n   ping -t 255 224.66.66.66\n\n   If nobody answers, mbone does not work.\n\n   ip tunnel add Universe mode gre remote 224.66.66.66 local <Your_real_addr> ttl 255\n   ip addr add 10.66.66.<somewhat>/24 dev Universe\n   ifconfig Universe up\n   ifconfig Universe add fe80::<Your_real_addr>/10\n   ifconfig Universe add fec0:6666:6666::<Your_real_addr>/96\n   ftp 10.66.66.66\n   ...\n   ftp fec0:6666:6666::193.233.7.65\n   ...\n\n */\n\nstatic int ipgre_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\tunsigned short type,\n\t\t\tconst void *daddr, const void *saddr, unsigned int len)\n{\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\tstruct iphdr *iph = (struct iphdr *)skb_push(skb, t->hlen);\n\t__be16 *p = (__be16*)(iph+1);\n\n\tmemcpy(iph, &t->parms.iph, sizeof(struct iphdr));\n\tp[0]\t\t= t->parms.o_flags;\n\tp[1]\t\t= htons(type);\n\n\t/*\n\t *\tSet the source hardware address.\n\t */\n\n\tif (saddr)\n\t\tmemcpy(&iph->saddr, saddr, 4);\n\tif (daddr)\n\t\tmemcpy(&iph->daddr, daddr, 4);\n\tif (iph->daddr)\n\t\treturn t->hlen;\n\n\treturn -t->hlen;\n}\n\nstatic int ipgre_header_parse(const struct sk_buff *skb, unsigned char *haddr)\n{\n\tstruct iphdr *iph = (struct iphdr *) skb_mac_header(skb);\n\tmemcpy(haddr, &iph->saddr, 4);\n\treturn 4;\n}\n\nstatic const struct header_ops ipgre_header_ops = {\n\t.create\t= ipgre_header,\n\t.parse\t= ipgre_header_parse,\n};\n\n#ifdef CONFIG_NET_IPGRE_BROADCAST\nstatic int ipgre_open(struct net_device *dev)\n{\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\n\tif (ipv4_is_multicast(t->parms.iph.daddr)) {\n\t\tstruct flowi fl = {\n\t\t\t.oif = t->parms.link,\n\t\t\t.fl4_dst = t->parms.iph.daddr,\n\t\t\t.fl4_src = t->parms.iph.saddr,\n\t\t\t.fl4_tos = RT_TOS(t->parms.iph.tos),\n\t\t\t.proto = IPPROTO_GRE,\n\t\t\t.fl_gre_key = t->parms.o_key\n\t\t};\n\t\tstruct rtable *rt;\n\n\t\tif (ip_route_output_key(dev_net(dev), &rt, &fl))\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tdev = rt->dst.dev;\n\t\tip_rt_put(rt);\n\t\tif (__in_dev_get_rtnl(dev) == NULL)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tt->mlink = dev->ifindex;\n\t\tip_mc_inc_group(__in_dev_get_rtnl(dev), t->parms.iph.daddr);\n\t}\n\treturn 0;\n}\n\nstatic int ipgre_close(struct net_device *dev)\n{\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\n\tif (ipv4_is_multicast(t->parms.iph.daddr) && t->mlink) {\n\t\tstruct in_device *in_dev;\n\t\tin_dev = inetdev_by_index(dev_net(dev), t->mlink);\n\t\tif (in_dev)\n\t\t\tip_mc_dec_group(in_dev, t->parms.iph.daddr);\n\t}\n\treturn 0;\n}\n\n#endif\n\nstatic const struct net_device_ops ipgre_netdev_ops = {\n\t.ndo_init\t\t= ipgre_tunnel_init,\n\t.ndo_uninit\t\t= ipgre_tunnel_uninit,\n#ifdef CONFIG_NET_IPGRE_BROADCAST\n\t.ndo_open\t\t= ipgre_open,\n\t.ndo_stop\t\t= ipgre_close,\n#endif\n\t.ndo_start_xmit\t\t= ipgre_tunnel_xmit,\n\t.ndo_do_ioctl\t\t= ipgre_tunnel_ioctl,\n\t.ndo_change_mtu\t\t= ipgre_tunnel_change_mtu,\n\t.ndo_get_stats\t\t= ipgre_get_stats,\n};\n\nstatic void ipgre_dev_free(struct net_device *dev)\n{\n\tfree_percpu(dev->tstats);\n\tfree_netdev(dev);\n}\n\nstatic void ipgre_tunnel_setup(struct net_device *dev)\n{\n\tdev->netdev_ops\t\t= &ipgre_netdev_ops;\n\tdev->destructor \t= ipgre_dev_free;\n\n\tdev->type\t\t= ARPHRD_IPGRE;\n\tdev->needed_headroom \t= LL_MAX_HEADER + sizeof(struct iphdr) + 4;\n\tdev->mtu\t\t= ETH_DATA_LEN - sizeof(struct iphdr) - 4;\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->iflink\t\t= 0;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_NETNS_LOCAL;\n\tdev->priv_flags\t\t&= ~IFF_XMIT_DST_RELEASE;\n}\n\nstatic int ipgre_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel;\n\tstruct iphdr *iph;\n\n\ttunnel = netdev_priv(dev);\n\tiph = &tunnel->parms.iph;\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tmemcpy(dev->dev_addr, &tunnel->parms.iph.saddr, 4);\n\tmemcpy(dev->broadcast, &tunnel->parms.iph.daddr, 4);\n\n\tif (iph->daddr) {\n#ifdef CONFIG_NET_IPGRE_BROADCAST\n\t\tif (ipv4_is_multicast(iph->daddr)) {\n\t\t\tif (!iph->saddr)\n\t\t\t\treturn -EINVAL;\n\t\t\tdev->flags = IFF_BROADCAST;\n\t\t\tdev->header_ops = &ipgre_header_ops;\n\t\t}\n#endif\n\t} else\n\t\tdev->header_ops = &ipgre_header_ops;\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void ipgre_fb_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tiph->version\t\t= 4;\n\tiph->protocol\t\t= IPPROTO_GRE;\n\tiph->ihl\t\t= 5;\n\ttunnel->hlen\t\t= sizeof(struct iphdr) + 4;\n\n\tdev_hold(dev);\n}\n\n\nstatic const struct gre_protocol ipgre_protocol = {\n\t.handler     = ipgre_rcv,\n\t.err_handler = ipgre_err,\n};\n\nstatic void ipgre_destroy_tunnels(struct ipgre_net *ign, struct list_head *head)\n{\n\tint prio;\n\n\tfor (prio = 0; prio < 4; prio++) {\n\t\tint h;\n\t\tfor (h = 0; h < HASH_SIZE; h++) {\n\t\t\tstruct ip_tunnel *t;\n\n\t\t\tt = rtnl_dereference(ign->tunnels[prio][h]);\n\n\t\t\twhile (t != NULL) {\n\t\t\t\tunregister_netdevice_queue(t->dev, head);\n\t\t\t\tt = rtnl_dereference(t->next);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int __net_init ipgre_init_net(struct net *net)\n{\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\tint err;\n\n\tign->fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel), \"gre0\",\n\t\t\t\t\t   ipgre_tunnel_setup);\n\tif (!ign->fb_tunnel_dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_dev;\n\t}\n\tdev_net_set(ign->fb_tunnel_dev, net);\n\n\tipgre_fb_tunnel_init(ign->fb_tunnel_dev);\n\tign->fb_tunnel_dev->rtnl_link_ops = &ipgre_link_ops;\n\n\tif ((err = register_netdev(ign->fb_tunnel_dev)))\n\t\tgoto err_reg_dev;\n\n\trcu_assign_pointer(ign->tunnels_wc[0],\n\t\t\t   netdev_priv(ign->fb_tunnel_dev));\n\treturn 0;\n\nerr_reg_dev:\n\tipgre_dev_free(ign->fb_tunnel_dev);\nerr_alloc_dev:\n\treturn err;\n}\n\nstatic void __net_exit ipgre_exit_net(struct net *net)\n{\n\tstruct ipgre_net *ign;\n\tLIST_HEAD(list);\n\n\tign = net_generic(net, ipgre_net_id);\n\trtnl_lock();\n\tipgre_destroy_tunnels(ign, &list);\n\tunregister_netdevice_many(&list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations ipgre_net_ops = {\n\t.init = ipgre_init_net,\n\t.exit = ipgre_exit_net,\n\t.id   = &ipgre_net_id,\n\t.size = sizeof(struct ipgre_net),\n};\n\nstatic int ipgre_tunnel_validate(struct nlattr *tb[], struct nlattr *data[])\n{\n\t__be16 flags;\n\n\tif (!data)\n\t\treturn 0;\n\n\tflags = 0;\n\tif (data[IFLA_GRE_IFLAGS])\n\t\tflags |= nla_get_be16(data[IFLA_GRE_IFLAGS]);\n\tif (data[IFLA_GRE_OFLAGS])\n\t\tflags |= nla_get_be16(data[IFLA_GRE_OFLAGS]);\n\tif (flags & (GRE_VERSION|GRE_ROUTING))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int ipgre_tap_validate(struct nlattr *tb[], struct nlattr *data[])\n{\n\t__be32 daddr;\n\n\tif (tb[IFLA_ADDRESS]) {\n\t\tif (nla_len(tb[IFLA_ADDRESS]) != ETH_ALEN)\n\t\t\treturn -EINVAL;\n\t\tif (!is_valid_ether_addr(nla_data(tb[IFLA_ADDRESS])))\n\t\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\tif (!data)\n\t\tgoto out;\n\n\tif (data[IFLA_GRE_REMOTE]) {\n\t\tmemcpy(&daddr, nla_data(data[IFLA_GRE_REMOTE]), 4);\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t}\n\nout:\n\treturn ipgre_tunnel_validate(tb, data);\n}\n\nstatic void ipgre_netlink_parms(struct nlattr *data[],\n\t\t\t\tstruct ip_tunnel_parm *parms)\n{\n\tmemset(parms, 0, sizeof(*parms));\n\n\tparms->iph.protocol = IPPROTO_GRE;\n\n\tif (!data)\n\t\treturn;\n\n\tif (data[IFLA_GRE_LINK])\n\t\tparms->link = nla_get_u32(data[IFLA_GRE_LINK]);\n\n\tif (data[IFLA_GRE_IFLAGS])\n\t\tparms->i_flags = nla_get_be16(data[IFLA_GRE_IFLAGS]);\n\n\tif (data[IFLA_GRE_OFLAGS])\n\t\tparms->o_flags = nla_get_be16(data[IFLA_GRE_OFLAGS]);\n\n\tif (data[IFLA_GRE_IKEY])\n\t\tparms->i_key = nla_get_be32(data[IFLA_GRE_IKEY]);\n\n\tif (data[IFLA_GRE_OKEY])\n\t\tparms->o_key = nla_get_be32(data[IFLA_GRE_OKEY]);\n\n\tif (data[IFLA_GRE_LOCAL])\n\t\tparms->iph.saddr = nla_get_be32(data[IFLA_GRE_LOCAL]);\n\n\tif (data[IFLA_GRE_REMOTE])\n\t\tparms->iph.daddr = nla_get_be32(data[IFLA_GRE_REMOTE]);\n\n\tif (data[IFLA_GRE_TTL])\n\t\tparms->iph.ttl = nla_get_u8(data[IFLA_GRE_TTL]);\n\n\tif (data[IFLA_GRE_TOS])\n\t\tparms->iph.tos = nla_get_u8(data[IFLA_GRE_TOS]);\n\n\tif (!data[IFLA_GRE_PMTUDISC] || nla_get_u8(data[IFLA_GRE_PMTUDISC]))\n\t\tparms->iph.frag_off = htons(IP_DF);\n}\n\nstatic int ipgre_tap_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel;\n\n\ttunnel = netdev_priv(dev);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tipgre_tunnel_bind_dev(dev);\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic const struct net_device_ops ipgre_tap_netdev_ops = {\n\t.ndo_init\t\t= ipgre_tap_init,\n\t.ndo_uninit\t\t= ipgre_tunnel_uninit,\n\t.ndo_start_xmit\t\t= ipgre_tunnel_xmit,\n\t.ndo_set_mac_address \t= eth_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_change_mtu\t\t= ipgre_tunnel_change_mtu,\n\t.ndo_get_stats\t\t= ipgre_get_stats,\n};\n\nstatic void ipgre_tap_setup(struct net_device *dev)\n{\n\n\tether_setup(dev);\n\n\tdev->netdev_ops\t\t= &ipgre_tap_netdev_ops;\n\tdev->destructor \t= ipgre_dev_free;\n\n\tdev->iflink\t\t= 0;\n\tdev->features\t\t|= NETIF_F_NETNS_LOCAL;\n}\n\nstatic int ipgre_newlink(struct net *src_net, struct net_device *dev, struct nlattr *tb[],\n\t\t\t struct nlattr *data[])\n{\n\tstruct ip_tunnel *nt;\n\tstruct net *net = dev_net(dev);\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\tint mtu;\n\tint err;\n\n\tnt = netdev_priv(dev);\n\tipgre_netlink_parms(data, &nt->parms);\n\n\tif (ipgre_tunnel_find(net, &nt->parms, dev->type))\n\t\treturn -EEXIST;\n\n\tif (dev->type == ARPHRD_ETHER && !tb[IFLA_ADDRESS])\n\t\trandom_ether_addr(dev->dev_addr);\n\n\tmtu = ipgre_tunnel_bind_dev(dev);\n\tif (!tb[IFLA_MTU])\n\t\tdev->mtu = mtu;\n\n\t/* Can use a lockless transmit, unless we generate output sequences */\n\tif (!(nt->parms.o_flags & GRE_SEQ))\n\t\tdev->features |= NETIF_F_LLTX;\n\n\terr = register_netdevice(dev);\n\tif (err)\n\t\tgoto out;\n\n\tdev_hold(dev);\n\tipgre_tunnel_link(ign, nt);\n\nout:\n\treturn err;\n}\n\nstatic int ipgre_changelink(struct net_device *dev, struct nlattr *tb[],\n\t\t\t    struct nlattr *data[])\n{\n\tstruct ip_tunnel *t, *nt;\n\tstruct net *net = dev_net(dev);\n\tstruct ipgre_net *ign = net_generic(net, ipgre_net_id);\n\tstruct ip_tunnel_parm p;\n\tint mtu;\n\n\tif (dev == ign->fb_tunnel_dev)\n\t\treturn -EINVAL;\n\n\tnt = netdev_priv(dev);\n\tipgre_netlink_parms(data, &p);\n\n\tt = ipgre_tunnel_locate(net, &p, 0);\n\n\tif (t) {\n\t\tif (t->dev != dev)\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tt = nt;\n\n\t\tif (dev->type != ARPHRD_ETHER) {\n\t\t\tunsigned int nflags = 0;\n\n\t\t\tif (ipv4_is_multicast(p.iph.daddr))\n\t\t\t\tnflags = IFF_BROADCAST;\n\t\t\telse if (p.iph.daddr)\n\t\t\t\tnflags = IFF_POINTOPOINT;\n\n\t\t\tif ((dev->flags ^ nflags) &\n\t\t\t    (IFF_POINTOPOINT | IFF_BROADCAST))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tipgre_tunnel_unlink(ign, t);\n\t\tt->parms.iph.saddr = p.iph.saddr;\n\t\tt->parms.iph.daddr = p.iph.daddr;\n\t\tt->parms.i_key = p.i_key;\n\t\tif (dev->type != ARPHRD_ETHER) {\n\t\t\tmemcpy(dev->dev_addr, &p.iph.saddr, 4);\n\t\t\tmemcpy(dev->broadcast, &p.iph.daddr, 4);\n\t\t}\n\t\tipgre_tunnel_link(ign, t);\n\t\tnetdev_state_change(dev);\n\t}\n\n\tt->parms.o_key = p.o_key;\n\tt->parms.iph.ttl = p.iph.ttl;\n\tt->parms.iph.tos = p.iph.tos;\n\tt->parms.iph.frag_off = p.iph.frag_off;\n\n\tif (t->parms.link != p.link) {\n\t\tt->parms.link = p.link;\n\t\tmtu = ipgre_tunnel_bind_dev(dev);\n\t\tif (!tb[IFLA_MTU])\n\t\t\tdev->mtu = mtu;\n\t\tnetdev_state_change(dev);\n\t}\n\n\treturn 0;\n}\n\nstatic size_t ipgre_get_size(const struct net_device *dev)\n{\n\treturn\n\t\t/* IFLA_GRE_LINK */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_IFLAGS */\n\t\tnla_total_size(2) +\n\t\t/* IFLA_GRE_OFLAGS */\n\t\tnla_total_size(2) +\n\t\t/* IFLA_GRE_IKEY */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_OKEY */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_LOCAL */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_REMOTE */\n\t\tnla_total_size(4) +\n\t\t/* IFLA_GRE_TTL */\n\t\tnla_total_size(1) +\n\t\t/* IFLA_GRE_TOS */\n\t\tnla_total_size(1) +\n\t\t/* IFLA_GRE_PMTUDISC */\n\t\tnla_total_size(1) +\n\t\t0;\n}\n\nstatic int ipgre_fill_info(struct sk_buff *skb, const struct net_device *dev)\n{\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\tstruct ip_tunnel_parm *p = &t->parms;\n\n\tNLA_PUT_U32(skb, IFLA_GRE_LINK, p->link);\n\tNLA_PUT_BE16(skb, IFLA_GRE_IFLAGS, p->i_flags);\n\tNLA_PUT_BE16(skb, IFLA_GRE_OFLAGS, p->o_flags);\n\tNLA_PUT_BE32(skb, IFLA_GRE_IKEY, p->i_key);\n\tNLA_PUT_BE32(skb, IFLA_GRE_OKEY, p->o_key);\n\tNLA_PUT_BE32(skb, IFLA_GRE_LOCAL, p->iph.saddr);\n\tNLA_PUT_BE32(skb, IFLA_GRE_REMOTE, p->iph.daddr);\n\tNLA_PUT_U8(skb, IFLA_GRE_TTL, p->iph.ttl);\n\tNLA_PUT_U8(skb, IFLA_GRE_TOS, p->iph.tos);\n\tNLA_PUT_U8(skb, IFLA_GRE_PMTUDISC, !!(p->iph.frag_off & htons(IP_DF)));\n\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\n\nstatic const struct nla_policy ipgre_policy[IFLA_GRE_MAX + 1] = {\n\t[IFLA_GRE_LINK]\t\t= { .type = NLA_U32 },\n\t[IFLA_GRE_IFLAGS]\t= { .type = NLA_U16 },\n\t[IFLA_GRE_OFLAGS]\t= { .type = NLA_U16 },\n\t[IFLA_GRE_IKEY]\t\t= { .type = NLA_U32 },\n\t[IFLA_GRE_OKEY]\t\t= { .type = NLA_U32 },\n\t[IFLA_GRE_LOCAL]\t= { .len = FIELD_SIZEOF(struct iphdr, saddr) },\n\t[IFLA_GRE_REMOTE]\t= { .len = FIELD_SIZEOF(struct iphdr, daddr) },\n\t[IFLA_GRE_TTL]\t\t= { .type = NLA_U8 },\n\t[IFLA_GRE_TOS]\t\t= { .type = NLA_U8 },\n\t[IFLA_GRE_PMTUDISC]\t= { .type = NLA_U8 },\n};\n\nstatic struct rtnl_link_ops ipgre_link_ops __read_mostly = {\n\t.kind\t\t= \"gre\",\n\t.maxtype\t= IFLA_GRE_MAX,\n\t.policy\t\t= ipgre_policy,\n\t.priv_size\t= sizeof(struct ip_tunnel),\n\t.setup\t\t= ipgre_tunnel_setup,\n\t.validate\t= ipgre_tunnel_validate,\n\t.newlink\t= ipgre_newlink,\n\t.changelink\t= ipgre_changelink,\n\t.get_size\t= ipgre_get_size,\n\t.fill_info\t= ipgre_fill_info,\n};\n\nstatic struct rtnl_link_ops ipgre_tap_ops __read_mostly = {\n\t.kind\t\t= \"gretap\",\n\t.maxtype\t= IFLA_GRE_MAX,\n\t.policy\t\t= ipgre_policy,\n\t.priv_size\t= sizeof(struct ip_tunnel),\n\t.setup\t\t= ipgre_tap_setup,\n\t.validate\t= ipgre_tap_validate,\n\t.newlink\t= ipgre_newlink,\n\t.changelink\t= ipgre_changelink,\n\t.get_size\t= ipgre_get_size,\n\t.fill_info\t= ipgre_fill_info,\n};\n\n/*\n *\tAnd now the modules code and kernel interface.\n */\n\nstatic int __init ipgre_init(void)\n{\n\tint err;\n\n\tprintk(KERN_INFO \"GRE over IPv4 tunneling driver\\n\");\n\n\terr = register_pernet_device(&ipgre_net_ops);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = gre_add_protocol(&ipgre_protocol, GREPROTO_CISCO);\n\tif (err < 0) {\n\t\tprintk(KERN_INFO \"ipgre init: can't add protocol\\n\");\n\t\tgoto add_proto_failed;\n\t}\n\n\terr = rtnl_link_register(&ipgre_link_ops);\n\tif (err < 0)\n\t\tgoto rtnl_link_failed;\n\n\terr = rtnl_link_register(&ipgre_tap_ops);\n\tif (err < 0)\n\t\tgoto tap_ops_failed;\n\nout:\n\treturn err;\n\ntap_ops_failed:\n\trtnl_link_unregister(&ipgre_link_ops);\nrtnl_link_failed:\n\tgre_del_protocol(&ipgre_protocol, GREPROTO_CISCO);\nadd_proto_failed:\n\tunregister_pernet_device(&ipgre_net_ops);\n\tgoto out;\n}\n\nstatic void __exit ipgre_fini(void)\n{\n\trtnl_link_unregister(&ipgre_tap_ops);\n\trtnl_link_unregister(&ipgre_link_ops);\n\tif (gre_del_protocol(&ipgre_protocol, GREPROTO_CISCO) < 0)\n\t\tprintk(KERN_INFO \"ipgre close: can't remove protocol\\n\");\n\tunregister_pernet_device(&ipgre_net_ops);\n}\n\nmodule_init(ipgre_init);\nmodule_exit(ipgre_fini);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_RTNL_LINK(\"gre\");\nMODULE_ALIAS_RTNL_LINK(\"gretap\");\nMODULE_ALIAS_NETDEV(\"gre0\");\n", "/*\n *\tLinux NET3:\tIP/IP protocol decoder.\n *\n *\tAuthors:\n *\t\tSam Lantinga (slouken@cs.ucdavis.edu)  02/01/95\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tMerged and made usable non modular (its so tiny its silly as\n *\t\t\t\t\ta module taking up 2 pages).\n *\t\tAlan Cox\t: \tFixed bug with 1.3.18 and IPIP not working (now needs to set skb->h.iph)\n *\t\t\t\t\tto keep ip_forward happy.\n *\t\tAlan Cox\t:\tMore fixes for 1.3.21, and firewall fix. Maybe this will work soon 8).\n *\t\tKai Schulte\t:\tFixed #defines for IP_FIREWALL->FIREWALL\n *              David Woodhouse :       Perform some basic ICMP handling.\n *                                      IPIP Routing without decapsulation.\n *              Carlos Picoto   :       GRE over IP support\n *\t\tAlexey Kuznetsov:\tReworked. Really, now it is truncated version of ipv4/ip_gre.c.\n *\t\t\t\t\tI do not want to merge them together.\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n */\n\n/* tunnel.c: an IP tunnel driver\n\n\tThe purpose of this driver is to provide an IP tunnel through\n\twhich you can tunnel network traffic transparently across subnets.\n\n\tThis was written by looking at Nick Holloway's dummy driver\n\tThanks for the great code!\n\n\t\t-Sam Lantinga\t(slouken@cs.ucdavis.edu)  02/01/95\n\n\tMinor tweaks:\n\t\tCleaned up the code a little and added some pre-1.3.0 tweaks.\n\t\tdev->hard_header/hard_header_len changed to use no headers.\n\t\tComments/bracketing tweaked.\n\t\tMade the tunnels use dev->name not tunnel: when error reporting.\n\t\tAdded tx_dropped stat\n\n\t\t-Alan Cox\t(alan@lxorguk.ukuu.org.uk) 21 March 95\n\n\tReworked:\n\t\tChanged to tunnel to destination gateway in addition to the\n\t\t\ttunnel's pointopoint address\n\t\tAlmost completely rewritten\n\t\tNote:  There is currently no firewall or ICMP handling done.\n\n\t\t-Sam Lantinga\t(slouken@cs.ucdavis.edu) 02/13/96\n\n*/\n\n/* Things I wish I had known when writing the tunnel driver:\n\n\tWhen the tunnel_xmit() function is called, the skb contains the\n\tpacket to be sent (plus a great deal of extra info), and dev\n\tcontains the tunnel device that _we_ are.\n\n\tWhen we are passed a packet, we are expected to fill in the\n\tsource address with our source IP address.\n\n\tWhat is the proper way to allocate, copy and free a buffer?\n\tAfter you allocate it, it is a \"0 length\" chunk of memory\n\tstarting at zero.  If you want to add headers to the buffer\n\tlater, you'll have to call \"skb_reserve(skb, amount)\" with\n\tthe amount of memory you want reserved.  Then, you call\n\t\"skb_put(skb, amount)\" with the amount of space you want in\n\tthe buffer.  skb_put() returns a pointer to the top (#0) of\n\tthat buffer.  skb->len is set to the amount of space you have\n\t\"allocated\" with skb_put().  You can then write up to skb->len\n\tbytes to that buffer.  If you need more, you can call skb_put()\n\tagain with the additional amount of space you need.  You can\n\tfind out how much more space you can allocate by calling\n\t\"skb_tailroom(skb)\".\n\tNow, to add header space, call \"skb_push(skb, header_len)\".\n\tThis creates space at the beginning of the buffer and returns\n\ta pointer to this new space.  If later you need to strip a\n\theader from a buffer, call \"skb_pull(skb, header_len)\".\n\tskb_headroom() will return how much space is left at the top\n\tof the buffer (before the main data).  Remember, this headroom\n\tspace must be reserved before the skb_put() function is called.\n\t*/\n\n/*\n   This version of net/ipv4/ipip.c is cloned of net/ipv4/ip_gre.c\n\n   For comments look at net/ipv4/ip_gre.c --ANK\n */\n\n\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/in.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/if_arp.h>\n#include <linux/mroute.h>\n#include <linux/init.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/if_ether.h>\n\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/ipip.h>\n#include <net/inet_ecn.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n#define HASH_SIZE  16\n#define HASH(addr) (((__force u32)addr^((__force u32)addr>>4))&0xF)\n\nstatic int ipip_net_id __read_mostly;\nstruct ipip_net {\n\tstruct ip_tunnel __rcu *tunnels_r_l[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_r[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_l[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_wc[1];\n\tstruct ip_tunnel __rcu **tunnels[4];\n\n\tstruct net_device *fb_tunnel_dev;\n};\n\nstatic int ipip_tunnel_init(struct net_device *dev);\nstatic void ipip_tunnel_setup(struct net_device *dev);\nstatic void ipip_dev_free(struct net_device *dev);\n\n/*\n * Locking : hash tables are protected by RCU and RTNL\n */\n\n#define for_each_ip_tunnel_rcu(start) \\\n\tfor (t = rcu_dereference(start); t; t = rcu_dereference(t->next))\n\n/* often modified stats are per cpu, other are shared (netdev->stats) */\nstruct pcpu_tstats {\n\tunsigned long\trx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_packets;\n\tunsigned long\ttx_bytes;\n};\n\nstatic struct net_device_stats *ipip_get_stats(struct net_device *dev)\n{\n\tstruct pcpu_tstats sum = { 0 };\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tconst struct pcpu_tstats *tstats = per_cpu_ptr(dev->tstats, i);\n\n\t\tsum.rx_packets += tstats->rx_packets;\n\t\tsum.rx_bytes   += tstats->rx_bytes;\n\t\tsum.tx_packets += tstats->tx_packets;\n\t\tsum.tx_bytes   += tstats->tx_bytes;\n\t}\n\tdev->stats.rx_packets = sum.rx_packets;\n\tdev->stats.rx_bytes   = sum.rx_bytes;\n\tdev->stats.tx_packets = sum.tx_packets;\n\tdev->stats.tx_bytes   = sum.tx_bytes;\n\treturn &dev->stats;\n}\n\nstatic struct ip_tunnel * ipip_tunnel_lookup(struct net *net,\n\t\t__be32 remote, __be32 local)\n{\n\tunsigned int h0 = HASH(remote);\n\tunsigned int h1 = HASH(local);\n\tstruct ip_tunnel *t;\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\n\tfor_each_ip_tunnel_rcu(ipn->tunnels_r_l[h0 ^ h1])\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    remote == t->parms.iph.daddr && (t->dev->flags&IFF_UP))\n\t\t\treturn t;\n\n\tfor_each_ip_tunnel_rcu(ipn->tunnels_r[h0])\n\t\tif (remote == t->parms.iph.daddr && (t->dev->flags&IFF_UP))\n\t\t\treturn t;\n\n\tfor_each_ip_tunnel_rcu(ipn->tunnels_l[h1])\n\t\tif (local == t->parms.iph.saddr && (t->dev->flags&IFF_UP))\n\t\t\treturn t;\n\n\tt = rcu_dereference(ipn->tunnels_wc[0]);\n\tif (t && (t->dev->flags&IFF_UP))\n\t\treturn t;\n\treturn NULL;\n}\n\nstatic struct ip_tunnel __rcu **__ipip_bucket(struct ipip_net *ipn,\n\t\tstruct ip_tunnel_parm *parms)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\tunsigned int h = 0;\n\tint prio = 0;\n\n\tif (remote) {\n\t\tprio |= 2;\n\t\th ^= HASH(remote);\n\t}\n\tif (local) {\n\t\tprio |= 1;\n\t\th ^= HASH(local);\n\t}\n\treturn &ipn->tunnels[prio][h];\n}\n\nstatic inline struct ip_tunnel __rcu **ipip_bucket(struct ipip_net *ipn,\n\t\tstruct ip_tunnel *t)\n{\n\treturn __ipip_bucket(ipn, &t->parms);\n}\n\nstatic void ipip_tunnel_unlink(struct ipip_net *ipn, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp;\n\tstruct ip_tunnel *iter;\n\n\tfor (tp = ipip_bucket(ipn, t);\n\t     (iter = rtnl_dereference(*tp)) != NULL;\n\t     tp = &iter->next) {\n\t\tif (t == iter) {\n\t\t\trcu_assign_pointer(*tp, t->next);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void ipip_tunnel_link(struct ipip_net *ipn, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp = ipip_bucket(ipn, t);\n\n\trcu_assign_pointer(t->next, rtnl_dereference(*tp));\n\trcu_assign_pointer(*tp, t);\n}\n\nstatic struct ip_tunnel * ipip_tunnel_locate(struct net *net,\n\t\tstruct ip_tunnel_parm *parms, int create)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\tstruct ip_tunnel *t, *nt;\n\tstruct ip_tunnel __rcu **tp;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\n\tfor (tp = __ipip_bucket(ipn, parms);\n\t\t (t = rtnl_dereference(*tp)) != NULL;\n\t\t tp = &t->next) {\n\t\tif (local == t->parms.iph.saddr && remote == t->parms.iph.daddr)\n\t\t\treturn t;\n\t}\n\tif (!create)\n\t\treturn NULL;\n\n\tif (parms->name[0])\n\t\tstrlcpy(name, parms->name, IFNAMSIZ);\n\telse\n\t\tstrcpy(name, \"tunl%d\");\n\n\tdev = alloc_netdev(sizeof(*t), name, ipip_tunnel_setup);\n\tif (dev == NULL)\n\t\treturn NULL;\n\n\tdev_net_set(dev, net);\n\n\tif (strchr(name, '%')) {\n\t\tif (dev_alloc_name(dev, name) < 0)\n\t\t\tgoto failed_free;\n\t}\n\n\tnt = netdev_priv(dev);\n\tnt->parms = *parms;\n\n\tif (ipip_tunnel_init(dev) < 0)\n\t\tgoto failed_free;\n\n\tif (register_netdevice(dev) < 0)\n\t\tgoto failed_free;\n\n\tdev_hold(dev);\n\tipip_tunnel_link(ipn, nt);\n\treturn nt;\n\nfailed_free:\n\tipip_dev_free(dev);\n\treturn NULL;\n}\n\n/* called with RTNL */\nstatic void ipip_tunnel_uninit(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\n\tif (dev == ipn->fb_tunnel_dev)\n\t\trcu_assign_pointer(ipn->tunnels_wc[0], NULL);\n\telse\n\t\tipip_tunnel_unlink(ipn, netdev_priv(dev));\n\tdev_put(dev);\n}\n\nstatic int ipip_err(struct sk_buff *skb, u32 info)\n{\n\n/* All the routers (except for Linux) return only\n   8 bytes of packet payload. It means, that precise relaying of\n   ICMP in the real Internet is absolutely infeasible.\n */\n\tstruct iphdr *iph = (struct iphdr *)skb->data;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct ip_tunnel *t;\n\tint err;\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_PARAMETERPROB:\n\t\treturn 0;\n\n\tcase ICMP_DEST_UNREACH:\n\t\tswitch (code) {\n\t\tcase ICMP_SR_FAILED:\n\t\tcase ICMP_PORT_UNREACH:\n\t\t\t/* Impossible event. */\n\t\t\treturn 0;\n\t\tcase ICMP_FRAG_NEEDED:\n\t\t\t/* Soft state for pmtu is maintained by IP core. */\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\t/* All others are translated to HOST_UNREACH.\n\t\t\t   rfc2003 contains \"deep thoughts\" about NET_UNREACH,\n\t\t\t   I believe they are just ether pollution. --ANK\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\tif (code != ICMP_EXC_TTL)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\terr = -ENOENT;\n\n\trcu_read_lock();\n\tt = ipip_tunnel_lookup(dev_net(skb->dev), iph->daddr, iph->saddr);\n\tif (t == NULL || t->parms.iph.daddr == 0)\n\t\tgoto out;\n\n\terr = 0;\n\tif (t->parms.iph.ttl == 0 && type == ICMP_TIME_EXCEEDED)\n\t\tgoto out;\n\n\tif (time_before(jiffies, t->err_time + IPTUNNEL_ERR_TIMEO))\n\t\tt->err_count++;\n\telse\n\t\tt->err_count = 1;\n\tt->err_time = jiffies;\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic inline void ipip_ecn_decapsulate(const struct iphdr *outer_iph,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct iphdr *inner_iph = ip_hdr(skb);\n\n\tif (INET_ECN_is_ce(outer_iph->tos))\n\t\tIP_ECN_set_ce(inner_iph);\n}\n\nstatic int ipip_rcv(struct sk_buff *skb)\n{\n\tstruct ip_tunnel *tunnel;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\trcu_read_lock();\n\ttunnel = ipip_tunnel_lookup(dev_net(skb->dev), iph->saddr, iph->daddr);\n\tif (tunnel != NULL) {\n\t\tstruct pcpu_tstats *tstats;\n\n\t\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\t\trcu_read_unlock();\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\tsecpath_reset(skb);\n\n\t\tskb->mac_header = skb->network_header;\n\t\tskb_reset_network_header(skb);\n\t\tskb->protocol = htons(ETH_P_IP);\n\t\tskb->pkt_type = PACKET_HOST;\n\n\t\ttstats = this_cpu_ptr(tunnel->dev->tstats);\n\t\ttstats->rx_packets++;\n\t\ttstats->rx_bytes += skb->len;\n\n\t\t__skb_tunnel_rx(skb, tunnel->dev);\n\n\t\tipip_ecn_decapsulate(iph, skb);\n\n\t\tnetif_rx(skb);\n\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\trcu_read_unlock();\n\n\treturn -1;\n}\n\n/*\n *\tThis function assumes it is being called from dev_queue_xmit()\n *\tand that skb is filled properly by that function.\n */\n\nstatic netdev_tx_t ipip_tunnel_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct pcpu_tstats *tstats;\n\tstruct iphdr  *tiph = &tunnel->parms.iph;\n\tu8     tos = tunnel->parms.iph.tos;\n\t__be16 df = tiph->frag_off;\n\tstruct rtable *rt;     \t\t\t/* Route to the other host */\n\tstruct net_device *tdev;\t\t/* Device to other host */\n\tstruct iphdr  *old_iph = ip_hdr(skb);\n\tstruct iphdr  *iph;\t\t\t/* Our new IP header */\n\tunsigned int max_headroom;\t\t/* The extra header space needed */\n\t__be32 dst = tiph->daddr;\n\tint    mtu;\n\n\tif (skb->protocol != htons(ETH_P_IP))\n\t\tgoto tx_error;\n\n\tif (tos & 1)\n\t\ttos = old_iph->tos;\n\n\tif (!dst) {\n\t\t/* NBMA tunnel */\n\t\tif ((rt = skb_rtable(skb)) == NULL) {\n\t\t\tdev->stats.tx_fifo_errors++;\n\t\t\tgoto tx_error;\n\t\t}\n\t\tif ((dst = rt->rt_gateway) == 0)\n\t\t\tgoto tx_error_icmp;\n\t}\n\n\t{\n\t\tstruct flowi fl = {\n\t\t\t.oif = tunnel->parms.link,\n\t\t\t.fl4_dst = dst,\n\t\t\t.fl4_src= tiph->saddr,\n\t\t\t.fl4_tos = RT_TOS(tos),\n\t\t\t.proto = IPPROTO_IPIP\n\t\t};\n\n\t\tif (ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\tdev->stats.tx_carrier_errors++;\n\t\t\tgoto tx_error_icmp;\n\t\t}\n\t}\n\ttdev = rt->dst.dev;\n\n\tif (tdev == dev) {\n\t\tip_rt_put(rt);\n\t\tdev->stats.collisions++;\n\t\tgoto tx_error;\n\t}\n\n\tdf |= old_iph->frag_off & htons(IP_DF);\n\n\tif (df) {\n\t\tmtu = dst_mtu(&rt->dst) - sizeof(struct iphdr);\n\n\t\tif (mtu < 68) {\n\t\t\tdev->stats.collisions++;\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\tif (skb_dst(skb))\n\t\t\tskb_dst(skb)->ops->update_pmtu(skb_dst(skb), mtu);\n\n\t\tif ((old_iph->frag_off & htons(IP_DF)) &&\n\t\t    mtu < ntohs(old_iph->tot_len)) {\n\t\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED,\n\t\t\t\t  htonl(mtu));\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n\n\tif (tunnel->err_count > 0) {\n\t\tif (time_before(jiffies,\n\t\t\t\ttunnel->err_time + IPTUNNEL_ERR_TIMEO)) {\n\t\t\ttunnel->err_count--;\n\t\t\tdst_link_failure(skb);\n\t\t} else\n\t\t\ttunnel->err_count = 0;\n\t}\n\n\t/*\n\t * Okay, now see if we can stuff it in the buffer as-is.\n\t */\n\tmax_headroom = (LL_RESERVED_SPACE(tdev)+sizeof(struct iphdr));\n\n\tif (skb_headroom(skb) < max_headroom || skb_shared(skb) ||\n\t    (skb_cloned(skb) && !skb_clone_writable(skb, 0))) {\n\t\tstruct sk_buff *new_skb = skb_realloc_headroom(skb, max_headroom);\n\t\tif (!new_skb) {\n\t\t\tip_rt_put(rt);\n\t\t\tdev->stats.tx_dropped++;\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(new_skb, skb->sk);\n\t\tdev_kfree_skb(skb);\n\t\tskb = new_skb;\n\t\told_iph = ip_hdr(skb);\n\t}\n\n\tskb->transport_header = skb->network_header;\n\tskb_push(skb, sizeof(struct iphdr));\n\tskb_reset_network_header(skb);\n\tmemset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));\n\tIPCB(skb)->flags &= ~(IPSKB_XFRM_TUNNEL_SIZE | IPSKB_XFRM_TRANSFORMED |\n\t\t\t      IPSKB_REROUTED);\n\tskb_dst_drop(skb);\n\tskb_dst_set(skb, &rt->dst);\n\n\t/*\n\t *\tPush down and install the IPIP header.\n\t */\n\n\tiph \t\t\t=\tip_hdr(skb);\n\tiph->version\t\t=\t4;\n\tiph->ihl\t\t=\tsizeof(struct iphdr)>>2;\n\tiph->frag_off\t\t=\tdf;\n\tiph->protocol\t\t=\tIPPROTO_IPIP;\n\tiph->tos\t\t=\tINET_ECN_encapsulate(tos, old_iph->tos);\n\tiph->daddr\t\t=\trt->rt_dst;\n\tiph->saddr\t\t=\trt->rt_src;\n\n\tif ((iph->ttl = tiph->ttl) == 0)\n\t\tiph->ttl\t=\told_iph->ttl;\n\n\tnf_reset(skb);\n\ttstats = this_cpu_ptr(dev->tstats);\n\t__IPTUNNEL_XMIT(tstats, &dev->stats);\n\treturn NETDEV_TX_OK;\n\ntx_error_icmp:\n\tdst_link_failure(skb);\ntx_error:\n\tdev->stats.tx_errors++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void ipip_tunnel_bind_dev(struct net_device *dev)\n{\n\tstruct net_device *tdev = NULL;\n\tstruct ip_tunnel *tunnel;\n\tstruct iphdr *iph;\n\n\ttunnel = netdev_priv(dev);\n\tiph = &tunnel->parms.iph;\n\n\tif (iph->daddr) {\n\t\tstruct flowi fl = {\n\t\t\t.oif = tunnel->parms.link,\n\t\t\t.fl4_dst = iph->daddr,\n\t\t\t.fl4_src = iph->saddr,\n\t\t\t.fl4_tos = RT_TOS(iph->tos),\n\t\t\t.proto = IPPROTO_IPIP\n\t\t};\n\t\tstruct rtable *rt;\n\n\t\tif (!ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\ttdev = rt->dst.dev;\n\t\t\tip_rt_put(rt);\n\t\t}\n\t\tdev->flags |= IFF_POINTOPOINT;\n\t}\n\n\tif (!tdev && tunnel->parms.link)\n\t\ttdev = __dev_get_by_index(dev_net(dev), tunnel->parms.link);\n\n\tif (tdev) {\n\t\tdev->hard_header_len = tdev->hard_header_len + sizeof(struct iphdr);\n\t\tdev->mtu = tdev->mtu - sizeof(struct iphdr);\n\t}\n\tdev->iflink = tunnel->parms.link;\n}\n\nstatic int\nipip_tunnel_ioctl (struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tint err = 0;\n\tstruct ip_tunnel_parm p;\n\tstruct ip_tunnel *t;\n\tstruct net *net = dev_net(dev);\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\n\tswitch (cmd) {\n\tcase SIOCGETTUNNEL:\n\t\tt = NULL;\n\t\tif (dev == ipn->fb_tunnel_dev) {\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tt = ipip_tunnel_locate(net, &p, 0);\n\t\t}\n\t\tif (t == NULL)\n\t\t\tt = netdev_priv(dev);\n\t\tmemcpy(&p, &t->parms, sizeof(p));\n\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &p, sizeof(p)))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase SIOCADDTUNNEL:\n\tcase SIOCCHGTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\tgoto done;\n\n\t\terr = -EINVAL;\n\t\tif (p.iph.version != 4 || p.iph.protocol != IPPROTO_IPIP ||\n\t\t    p.iph.ihl != 5 || (p.iph.frag_off&htons(~IP_DF)))\n\t\t\tgoto done;\n\t\tif (p.iph.ttl)\n\t\t\tp.iph.frag_off |= htons(IP_DF);\n\n\t\tt = ipip_tunnel_locate(net, &p, cmd == SIOCADDTUNNEL);\n\n\t\tif (dev != ipn->fb_tunnel_dev && cmd == SIOCCHGTUNNEL) {\n\t\t\tif (t != NULL) {\n\t\t\t\tif (t->dev != dev) {\n\t\t\t\t\terr = -EEXIST;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (((dev->flags&IFF_POINTOPOINT) && !p.iph.daddr) ||\n\t\t\t\t    (!(dev->flags&IFF_POINTOPOINT) && p.iph.daddr)) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tt = netdev_priv(dev);\n\t\t\t\tipip_tunnel_unlink(ipn, t);\n\t\t\t\tsynchronize_net();\n\t\t\t\tt->parms.iph.saddr = p.iph.saddr;\n\t\t\t\tt->parms.iph.daddr = p.iph.daddr;\n\t\t\t\tmemcpy(dev->dev_addr, &p.iph.saddr, 4);\n\t\t\t\tmemcpy(dev->broadcast, &p.iph.daddr, 4);\n\t\t\t\tipip_tunnel_link(ipn, t);\n\t\t\t\tnetdev_state_change(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (t) {\n\t\t\terr = 0;\n\t\t\tif (cmd == SIOCCHGTUNNEL) {\n\t\t\t\tt->parms.iph.ttl = p.iph.ttl;\n\t\t\t\tt->parms.iph.tos = p.iph.tos;\n\t\t\t\tt->parms.iph.frag_off = p.iph.frag_off;\n\t\t\t\tif (t->parms.link != p.link) {\n\t\t\t\t\tt->parms.link = p.link;\n\t\t\t\t\tipip_tunnel_bind_dev(dev);\n\t\t\t\t\tnetdev_state_change(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &t->parms, sizeof(p)))\n\t\t\t\terr = -EFAULT;\n\t\t} else\n\t\t\terr = (cmd == SIOCADDTUNNEL ? -ENOBUFS : -ENOENT);\n\t\tbreak;\n\n\tcase SIOCDELTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\tif (dev == ipn->fb_tunnel_dev) {\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\t\tgoto done;\n\t\t\terr = -ENOENT;\n\t\t\tif ((t = ipip_tunnel_locate(net, &p, 0)) == NULL)\n\t\t\t\tgoto done;\n\t\t\terr = -EPERM;\n\t\t\tif (t->dev == ipn->fb_tunnel_dev)\n\t\t\t\tgoto done;\n\t\t\tdev = t->dev;\n\t\t}\n\t\tunregister_netdevice(dev);\n\t\terr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\ndone:\n\treturn err;\n}\n\nstatic int ipip_tunnel_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tif (new_mtu < 68 || new_mtu > 0xFFF8 - sizeof(struct iphdr))\n\t\treturn -EINVAL;\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\nstatic const struct net_device_ops ipip_netdev_ops = {\n\t.ndo_uninit\t= ipip_tunnel_uninit,\n\t.ndo_start_xmit\t= ipip_tunnel_xmit,\n\t.ndo_do_ioctl\t= ipip_tunnel_ioctl,\n\t.ndo_change_mtu\t= ipip_tunnel_change_mtu,\n\t.ndo_get_stats  = ipip_get_stats,\n};\n\nstatic void ipip_dev_free(struct net_device *dev)\n{\n\tfree_percpu(dev->tstats);\n\tfree_netdev(dev);\n}\n\nstatic void ipip_tunnel_setup(struct net_device *dev)\n{\n\tdev->netdev_ops\t\t= &ipip_netdev_ops;\n\tdev->destructor\t\t= ipip_dev_free;\n\n\tdev->type\t\t= ARPHRD_TUNNEL;\n\tdev->hard_header_len \t= LL_MAX_HEADER + sizeof(struct iphdr);\n\tdev->mtu\t\t= ETH_DATA_LEN - sizeof(struct iphdr);\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->iflink\t\t= 0;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_NETNS_LOCAL;\n\tdev->features\t\t|= NETIF_F_LLTX;\n\tdev->priv_flags\t\t&= ~IFF_XMIT_DST_RELEASE;\n}\n\nstatic int ipip_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tmemcpy(dev->dev_addr, &tunnel->parms.iph.saddr, 4);\n\tmemcpy(dev->broadcast, &tunnel->parms.iph.daddr, 4);\n\n\tipip_tunnel_bind_dev(dev);\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int __net_init ipip_fb_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\tstruct ipip_net *ipn = net_generic(dev_net(dev), ipip_net_id);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tiph->version\t\t= 4;\n\tiph->protocol\t\t= IPPROTO_IPIP;\n\tiph->ihl\t\t= 5;\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\tdev_hold(dev);\n\trcu_assign_pointer(ipn->tunnels_wc[0], tunnel);\n\treturn 0;\n}\n\nstatic struct xfrm_tunnel ipip_handler __read_mostly = {\n\t.handler\t=\tipip_rcv,\n\t.err_handler\t=\tipip_err,\n\t.priority\t=\t1,\n};\n\nstatic const char banner[] __initconst =\n\tKERN_INFO \"IPv4 over IPv4 tunneling driver\\n\";\n\nstatic void ipip_destroy_tunnels(struct ipip_net *ipn, struct list_head *head)\n{\n\tint prio;\n\n\tfor (prio = 1; prio < 4; prio++) {\n\t\tint h;\n\t\tfor (h = 0; h < HASH_SIZE; h++) {\n\t\t\tstruct ip_tunnel *t;\n\n\t\t\tt = rtnl_dereference(ipn->tunnels[prio][h]);\n\t\t\twhile (t != NULL) {\n\t\t\t\tunregister_netdevice_queue(t->dev, head);\n\t\t\t\tt = rtnl_dereference(t->next);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int __net_init ipip_init_net(struct net *net)\n{\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\tint err;\n\n\tipn->tunnels[0] = ipn->tunnels_wc;\n\tipn->tunnels[1] = ipn->tunnels_l;\n\tipn->tunnels[2] = ipn->tunnels_r;\n\tipn->tunnels[3] = ipn->tunnels_r_l;\n\n\tipn->fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel),\n\t\t\t\t\t   \"tunl0\",\n\t\t\t\t\t   ipip_tunnel_setup);\n\tif (!ipn->fb_tunnel_dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_dev;\n\t}\n\tdev_net_set(ipn->fb_tunnel_dev, net);\n\n\terr = ipip_fb_tunnel_init(ipn->fb_tunnel_dev);\n\tif (err)\n\t\tgoto err_reg_dev;\n\n\tif ((err = register_netdev(ipn->fb_tunnel_dev)))\n\t\tgoto err_reg_dev;\n\n\treturn 0;\n\nerr_reg_dev:\n\tipip_dev_free(ipn->fb_tunnel_dev);\nerr_alloc_dev:\n\t/* nothing */\n\treturn err;\n}\n\nstatic void __net_exit ipip_exit_net(struct net *net)\n{\n\tstruct ipip_net *ipn = net_generic(net, ipip_net_id);\n\tLIST_HEAD(list);\n\n\trtnl_lock();\n\tipip_destroy_tunnels(ipn, &list);\n\tunregister_netdevice_queue(ipn->fb_tunnel_dev, &list);\n\tunregister_netdevice_many(&list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations ipip_net_ops = {\n\t.init = ipip_init_net,\n\t.exit = ipip_exit_net,\n\t.id   = &ipip_net_id,\n\t.size = sizeof(struct ipip_net),\n};\n\nstatic int __init ipip_init(void)\n{\n\tint err;\n\n\tprintk(banner);\n\n\terr = register_pernet_device(&ipip_net_ops);\n\tif (err < 0)\n\t\treturn err;\n\terr = xfrm4_tunnel_register(&ipip_handler, AF_INET);\n\tif (err < 0) {\n\t\tunregister_pernet_device(&ipip_net_ops);\n\t\tprintk(KERN_INFO \"ipip init: can't register tunnel\\n\");\n\t}\n\treturn err;\n}\n\nstatic void __exit ipip_fini(void)\n{\n\tif (xfrm4_tunnel_deregister(&ipip_handler, AF_INET))\n\t\tprintk(KERN_INFO \"ipip close: can't deregister tunnel\\n\");\n\n\tunregister_pernet_device(&ipip_net_ops);\n}\n\nmodule_init(ipip_init);\nmodule_exit(ipip_fini);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETDEV(\"tunl0\");\n", "/*\n *\tIPv6 over IPv4 tunnel device - Simple Internet Transition (SIT)\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\tAlexey Kuznetsov\t<kuznet@ms2.inr.ac.ru>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n *\n *\tChanges:\n * Roger Venning <r.venning@telstra.com>:\t6to4 support\n * Nate Thompson <nate@thebog.net>:\t\t6to4 support\n * Fred Templin <fred.l.templin@boeing.com>:\tisatap support\n */\n\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/icmp.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/init.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/if_ether.h>\n\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_fib.h>\n#include <net/ip6_route.h>\n#include <net/ndisc.h>\n#include <net/addrconf.h>\n#include <net/ip.h>\n#include <net/udp.h>\n#include <net/icmp.h>\n#include <net/ipip.h>\n#include <net/inet_ecn.h>\n#include <net/xfrm.h>\n#include <net/dsfield.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n/*\n   This version of net/ipv6/sit.c is cloned of net/ipv4/ip_gre.c\n\n   For comments look at net/ipv4/ip_gre.c --ANK\n */\n\n#define HASH_SIZE  16\n#define HASH(addr) (((__force u32)addr^((__force u32)addr>>4))&0xF)\n\nstatic int ipip6_tunnel_init(struct net_device *dev);\nstatic void ipip6_tunnel_setup(struct net_device *dev);\nstatic void ipip6_dev_free(struct net_device *dev);\n\nstatic int sit_net_id __read_mostly;\nstruct sit_net {\n\tstruct ip_tunnel __rcu *tunnels_r_l[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_r[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_l[HASH_SIZE];\n\tstruct ip_tunnel __rcu *tunnels_wc[1];\n\tstruct ip_tunnel __rcu **tunnels[4];\n\n\tstruct net_device *fb_tunnel_dev;\n};\n\n/*\n * Locking : hash tables are protected by RCU and RTNL\n */\n\n#define for_each_ip_tunnel_rcu(start) \\\n\tfor (t = rcu_dereference(start); t; t = rcu_dereference(t->next))\n\n/* often modified stats are per cpu, other are shared (netdev->stats) */\nstruct pcpu_tstats {\n\tunsigned long\trx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_packets;\n\tunsigned long\ttx_bytes;\n};\n\nstatic struct net_device_stats *ipip6_get_stats(struct net_device *dev)\n{\n\tstruct pcpu_tstats sum = { 0 };\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tconst struct pcpu_tstats *tstats = per_cpu_ptr(dev->tstats, i);\n\n\t\tsum.rx_packets += tstats->rx_packets;\n\t\tsum.rx_bytes   += tstats->rx_bytes;\n\t\tsum.tx_packets += tstats->tx_packets;\n\t\tsum.tx_bytes   += tstats->tx_bytes;\n\t}\n\tdev->stats.rx_packets = sum.rx_packets;\n\tdev->stats.rx_bytes   = sum.rx_bytes;\n\tdev->stats.tx_packets = sum.tx_packets;\n\tdev->stats.tx_bytes   = sum.tx_bytes;\n\treturn &dev->stats;\n}\n/*\n * Must be invoked with rcu_read_lock\n */\nstatic struct ip_tunnel * ipip6_tunnel_lookup(struct net *net,\n\t\tstruct net_device *dev, __be32 remote, __be32 local)\n{\n\tunsigned int h0 = HASH(remote);\n\tunsigned int h1 = HASH(local);\n\tstruct ip_tunnel *t;\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\n\tfor_each_ip_tunnel_rcu(sitn->tunnels_r_l[h0 ^ h1]) {\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    remote == t->parms.iph.daddr &&\n\t\t    (!dev || !t->parms.link || dev->iflink == t->parms.link) &&\n\t\t    (t->dev->flags & IFF_UP))\n\t\t\treturn t;\n\t}\n\tfor_each_ip_tunnel_rcu(sitn->tunnels_r[h0]) {\n\t\tif (remote == t->parms.iph.daddr &&\n\t\t    (!dev || !t->parms.link || dev->iflink == t->parms.link) &&\n\t\t    (t->dev->flags & IFF_UP))\n\t\t\treturn t;\n\t}\n\tfor_each_ip_tunnel_rcu(sitn->tunnels_l[h1]) {\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    (!dev || !t->parms.link || dev->iflink == t->parms.link) &&\n\t\t    (t->dev->flags & IFF_UP))\n\t\t\treturn t;\n\t}\n\tt = rcu_dereference(sitn->tunnels_wc[0]);\n\tif ((t != NULL) && (t->dev->flags & IFF_UP))\n\t\treturn t;\n\treturn NULL;\n}\n\nstatic struct ip_tunnel __rcu **__ipip6_bucket(struct sit_net *sitn,\n\t\tstruct ip_tunnel_parm *parms)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\tunsigned int h = 0;\n\tint prio = 0;\n\n\tif (remote) {\n\t\tprio |= 2;\n\t\th ^= HASH(remote);\n\t}\n\tif (local) {\n\t\tprio |= 1;\n\t\th ^= HASH(local);\n\t}\n\treturn &sitn->tunnels[prio][h];\n}\n\nstatic inline struct ip_tunnel __rcu **ipip6_bucket(struct sit_net *sitn,\n\t\tstruct ip_tunnel *t)\n{\n\treturn __ipip6_bucket(sitn, &t->parms);\n}\n\nstatic void ipip6_tunnel_unlink(struct sit_net *sitn, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp;\n\tstruct ip_tunnel *iter;\n\n\tfor (tp = ipip6_bucket(sitn, t);\n\t     (iter = rtnl_dereference(*tp)) != NULL;\n\t     tp = &iter->next) {\n\t\tif (t == iter) {\n\t\t\trcu_assign_pointer(*tp, t->next);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void ipip6_tunnel_link(struct sit_net *sitn, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel __rcu **tp = ipip6_bucket(sitn, t);\n\n\trcu_assign_pointer(t->next, rtnl_dereference(*tp));\n\trcu_assign_pointer(*tp, t);\n}\n\nstatic void ipip6_tunnel_clone_6rd(struct net_device *dev, struct sit_net *sitn)\n{\n#ifdef CONFIG_IPV6_SIT_6RD\n\tstruct ip_tunnel *t = netdev_priv(dev);\n\n\tif (t->dev == sitn->fb_tunnel_dev) {\n\t\tipv6_addr_set(&t->ip6rd.prefix, htonl(0x20020000), 0, 0, 0);\n\t\tt->ip6rd.relay_prefix = 0;\n\t\tt->ip6rd.prefixlen = 16;\n\t\tt->ip6rd.relay_prefixlen = 0;\n\t} else {\n\t\tstruct ip_tunnel *t0 = netdev_priv(sitn->fb_tunnel_dev);\n\t\tmemcpy(&t->ip6rd, &t0->ip6rd, sizeof(t->ip6rd));\n\t}\n#endif\n}\n\nstatic struct ip_tunnel *ipip6_tunnel_locate(struct net *net,\n\t\tstruct ip_tunnel_parm *parms, int create)\n{\n\t__be32 remote = parms->iph.daddr;\n\t__be32 local = parms->iph.saddr;\n\tstruct ip_tunnel *t, *nt;\n\tstruct ip_tunnel __rcu **tp;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\n\tfor (tp = __ipip6_bucket(sitn, parms);\n\t    (t = rtnl_dereference(*tp)) != NULL;\n\t     tp = &t->next) {\n\t\tif (local == t->parms.iph.saddr &&\n\t\t    remote == t->parms.iph.daddr &&\n\t\t    parms->link == t->parms.link) {\n\t\t\tif (create)\n\t\t\t\treturn NULL;\n\t\t\telse\n\t\t\t\treturn t;\n\t\t}\n\t}\n\tif (!create)\n\t\tgoto failed;\n\n\tif (parms->name[0])\n\t\tstrlcpy(name, parms->name, IFNAMSIZ);\n\telse\n\t\tstrcpy(name, \"sit%d\");\n\n\tdev = alloc_netdev(sizeof(*t), name, ipip6_tunnel_setup);\n\tif (dev == NULL)\n\t\treturn NULL;\n\n\tdev_net_set(dev, net);\n\n\tif (strchr(name, '%')) {\n\t\tif (dev_alloc_name(dev, name) < 0)\n\t\t\tgoto failed_free;\n\t}\n\n\tnt = netdev_priv(dev);\n\n\tnt->parms = *parms;\n\tif (ipip6_tunnel_init(dev) < 0)\n\t\tgoto failed_free;\n\tipip6_tunnel_clone_6rd(dev, sitn);\n\n\tif (parms->i_flags & SIT_ISATAP)\n\t\tdev->priv_flags |= IFF_ISATAP;\n\n\tif (register_netdevice(dev) < 0)\n\t\tgoto failed_free;\n\n\tdev_hold(dev);\n\n\tipip6_tunnel_link(sitn, nt);\n\treturn nt;\n\nfailed_free:\n\tipip6_dev_free(dev);\nfailed:\n\treturn NULL;\n}\n\n#define for_each_prl_rcu(start)\t\t\t\\\n\tfor (prl = rcu_dereference(start);\t\\\n\t     prl;\t\t\t\t\\\n\t     prl = rcu_dereference(prl->next))\n\nstatic struct ip_tunnel_prl_entry *\n__ipip6_tunnel_locate_prl(struct ip_tunnel *t, __be32 addr)\n{\n\tstruct ip_tunnel_prl_entry *prl;\n\n\tfor_each_prl_rcu(t->prl)\n\t\tif (prl->addr == addr)\n\t\t\tbreak;\n\treturn prl;\n\n}\n\nstatic int ipip6_tunnel_get_prl(struct ip_tunnel *t,\n\t\t\t\tstruct ip_tunnel_prl __user *a)\n{\n\tstruct ip_tunnel_prl kprl, *kp;\n\tstruct ip_tunnel_prl_entry *prl;\n\tunsigned int cmax, c = 0, ca, len;\n\tint ret = 0;\n\n\tif (copy_from_user(&kprl, a, sizeof(kprl)))\n\t\treturn -EFAULT;\n\tcmax = kprl.datalen / sizeof(kprl);\n\tif (cmax > 1 && kprl.addr != htonl(INADDR_ANY))\n\t\tcmax = 1;\n\n\t/* For simple GET or for root users,\n\t * we try harder to allocate.\n\t */\n\tkp = (cmax <= 1 || capable(CAP_NET_ADMIN)) ?\n\t\tkcalloc(cmax, sizeof(*kp), GFP_KERNEL) :\n\t\tNULL;\n\n\trcu_read_lock();\n\n\tca = t->prl_count < cmax ? t->prl_count : cmax;\n\n\tif (!kp) {\n\t\t/* We don't try hard to allocate much memory for\n\t\t * non-root users.\n\t\t * For root users, retry allocating enough memory for\n\t\t * the answer.\n\t\t */\n\t\tkp = kcalloc(ca, sizeof(*kp), GFP_ATOMIC);\n\t\tif (!kp) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tc = 0;\n\tfor_each_prl_rcu(t->prl) {\n\t\tif (c >= cmax)\n\t\t\tbreak;\n\t\tif (kprl.addr != htonl(INADDR_ANY) && prl->addr != kprl.addr)\n\t\t\tcontinue;\n\t\tkp[c].addr = prl->addr;\n\t\tkp[c].flags = prl->flags;\n\t\tc++;\n\t\tif (kprl.addr != htonl(INADDR_ANY))\n\t\t\tbreak;\n\t}\nout:\n\trcu_read_unlock();\n\n\tlen = sizeof(*kp) * c;\n\tret = 0;\n\tif ((len && copy_to_user(a + 1, kp, len)) || put_user(len, &a->datalen))\n\t\tret = -EFAULT;\n\n\tkfree(kp);\n\n\treturn ret;\n}\n\nstatic int\nipip6_tunnel_add_prl(struct ip_tunnel *t, struct ip_tunnel_prl *a, int chg)\n{\n\tstruct ip_tunnel_prl_entry *p;\n\tint err = 0;\n\n\tif (a->addr == htonl(INADDR_ANY))\n\t\treturn -EINVAL;\n\n\tASSERT_RTNL();\n\n\tfor (p = rtnl_dereference(t->prl); p; p = rtnl_dereference(p->next)) {\n\t\tif (p->addr == a->addr) {\n\t\t\tif (chg) {\n\t\t\t\tp->flags = a->flags;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\terr = -EEXIST;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (chg) {\n\t\terr = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tp = kzalloc(sizeof(struct ip_tunnel_prl_entry), GFP_KERNEL);\n\tif (!p) {\n\t\terr = -ENOBUFS;\n\t\tgoto out;\n\t}\n\n\tp->next = t->prl;\n\tp->addr = a->addr;\n\tp->flags = a->flags;\n\tt->prl_count++;\n\trcu_assign_pointer(t->prl, p);\nout:\n\treturn err;\n}\n\nstatic void prl_entry_destroy_rcu(struct rcu_head *head)\n{\n\tkfree(container_of(head, struct ip_tunnel_prl_entry, rcu_head));\n}\n\nstatic void prl_list_destroy_rcu(struct rcu_head *head)\n{\n\tstruct ip_tunnel_prl_entry *p, *n;\n\n\tp = container_of(head, struct ip_tunnel_prl_entry, rcu_head);\n\tdo {\n\t\tn = p->next;\n\t\tkfree(p);\n\t\tp = n;\n\t} while (p);\n}\n\nstatic int\nipip6_tunnel_del_prl(struct ip_tunnel *t, struct ip_tunnel_prl *a)\n{\n\tstruct ip_tunnel_prl_entry *x, **p;\n\tint err = 0;\n\n\tASSERT_RTNL();\n\n\tif (a && a->addr != htonl(INADDR_ANY)) {\n\t\tfor (p = &t->prl; *p; p = &(*p)->next) {\n\t\t\tif ((*p)->addr == a->addr) {\n\t\t\t\tx = *p;\n\t\t\t\t*p = x->next;\n\t\t\t\tcall_rcu(&x->rcu_head, prl_entry_destroy_rcu);\n\t\t\t\tt->prl_count--;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = -ENXIO;\n\t} else {\n\t\tif (t->prl) {\n\t\t\tt->prl_count = 0;\n\t\t\tx = t->prl;\n\t\t\tcall_rcu(&x->rcu_head, prl_list_destroy_rcu);\n\t\t\tt->prl = NULL;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n\nstatic int\nisatap_chksrc(struct sk_buff *skb, struct iphdr *iph, struct ip_tunnel *t)\n{\n\tstruct ip_tunnel_prl_entry *p;\n\tint ok = 1;\n\n\trcu_read_lock();\n\tp = __ipip6_tunnel_locate_prl(t, iph->saddr);\n\tif (p) {\n\t\tif (p->flags & PRL_DEFAULT)\n\t\t\tskb->ndisc_nodetype = NDISC_NODETYPE_DEFAULT;\n\t\telse\n\t\t\tskb->ndisc_nodetype = NDISC_NODETYPE_NODEFAULT;\n\t} else {\n\t\tstruct in6_addr *addr6 = &ipv6_hdr(skb)->saddr;\n\t\tif (ipv6_addr_is_isatap(addr6) &&\n\t\t    (addr6->s6_addr32[3] == iph->saddr) &&\n\t\t    ipv6_chk_prefix(addr6, t->dev))\n\t\t\tskb->ndisc_nodetype = NDISC_NODETYPE_HOST;\n\t\telse\n\t\t\tok = 0;\n\t}\n\trcu_read_unlock();\n\treturn ok;\n}\n\nstatic void ipip6_tunnel_uninit(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\n\tif (dev == sitn->fb_tunnel_dev) {\n\t\trcu_assign_pointer(sitn->tunnels_wc[0], NULL);\n\t} else {\n\t\tipip6_tunnel_unlink(sitn, netdev_priv(dev));\n\t\tipip6_tunnel_del_prl(netdev_priv(dev), NULL);\n\t}\n\tdev_put(dev);\n}\n\n\nstatic int ipip6_err(struct sk_buff *skb, u32 info)\n{\n\n/* All the routers (except for Linux) return only\n   8 bytes of packet payload. It means, that precise relaying of\n   ICMP in the real Internet is absolutely infeasible.\n */\n\tstruct iphdr *iph = (struct iphdr*)skb->data;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct ip_tunnel *t;\n\tint err;\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_PARAMETERPROB:\n\t\treturn 0;\n\n\tcase ICMP_DEST_UNREACH:\n\t\tswitch (code) {\n\t\tcase ICMP_SR_FAILED:\n\t\tcase ICMP_PORT_UNREACH:\n\t\t\t/* Impossible event. */\n\t\t\treturn 0;\n\t\tcase ICMP_FRAG_NEEDED:\n\t\t\t/* Soft state for pmtu is maintained by IP core. */\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\t/* All others are translated to HOST_UNREACH.\n\t\t\t   rfc2003 contains \"deep thoughts\" about NET_UNREACH,\n\t\t\t   I believe they are just ether pollution. --ANK\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\tif (code != ICMP_EXC_TTL)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\terr = -ENOENT;\n\n\trcu_read_lock();\n\tt = ipip6_tunnel_lookup(dev_net(skb->dev),\n\t\t\t\tskb->dev,\n\t\t\t\tiph->daddr,\n\t\t\t\tiph->saddr);\n\tif (t == NULL || t->parms.iph.daddr == 0)\n\t\tgoto out;\n\n\terr = 0;\n\tif (t->parms.iph.ttl == 0 && type == ICMP_TIME_EXCEEDED)\n\t\tgoto out;\n\n\tif (time_before(jiffies, t->err_time + IPTUNNEL_ERR_TIMEO))\n\t\tt->err_count++;\n\telse\n\t\tt->err_count = 1;\n\tt->err_time = jiffies;\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic inline void ipip6_ecn_decapsulate(struct iphdr *iph, struct sk_buff *skb)\n{\n\tif (INET_ECN_is_ce(iph->tos))\n\t\tIP6_ECN_set_ce(ipv6_hdr(skb));\n}\n\nstatic int ipip6_rcv(struct sk_buff *skb)\n{\n\tstruct iphdr *iph;\n\tstruct ip_tunnel *tunnel;\n\n\tif (!pskb_may_pull(skb, sizeof(struct ipv6hdr)))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\n\trcu_read_lock();\n\ttunnel = ipip6_tunnel_lookup(dev_net(skb->dev), skb->dev,\n\t\t\t\t     iph->saddr, iph->daddr);\n\tif (tunnel != NULL) {\n\t\tstruct pcpu_tstats *tstats;\n\n\t\tsecpath_reset(skb);\n\t\tskb->mac_header = skb->network_header;\n\t\tskb_reset_network_header(skb);\n\t\tIPCB(skb)->flags = 0;\n\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\tskb->pkt_type = PACKET_HOST;\n\n\t\tif ((tunnel->dev->priv_flags & IFF_ISATAP) &&\n\t\t    !isatap_chksrc(skb, iph, tunnel)) {\n\t\t\ttunnel->dev->stats.rx_errors++;\n\t\t\trcu_read_unlock();\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\ttstats = this_cpu_ptr(tunnel->dev->tstats);\n\t\ttstats->rx_packets++;\n\t\ttstats->rx_bytes += skb->len;\n\n\t\t__skb_tunnel_rx(skb, tunnel->dev);\n\n\t\tipip6_ecn_decapsulate(iph, skb);\n\n\t\tnetif_rx(skb);\n\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\n\t/* no tunnel matched,  let upstream know, ipsec may handle it */\n\trcu_read_unlock();\n\treturn 1;\nout:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\n/*\n * Returns the embedded IPv4 address if the IPv6 address\n * comes from 6rd / 6to4 (RFC 3056) addr space.\n */\nstatic inline\n__be32 try_6rd(struct in6_addr *v6dst, struct ip_tunnel *tunnel)\n{\n\t__be32 dst = 0;\n\n#ifdef CONFIG_IPV6_SIT_6RD\n\tif (ipv6_prefix_equal(v6dst, &tunnel->ip6rd.prefix,\n\t\t\t      tunnel->ip6rd.prefixlen)) {\n\t\tunsigned int pbw0, pbi0;\n\t\tint pbi1;\n\t\tu32 d;\n\n\t\tpbw0 = tunnel->ip6rd.prefixlen >> 5;\n\t\tpbi0 = tunnel->ip6rd.prefixlen & 0x1f;\n\n\t\td = (ntohl(v6dst->s6_addr32[pbw0]) << pbi0) >>\n\t\t    tunnel->ip6rd.relay_prefixlen;\n\n\t\tpbi1 = pbi0 - tunnel->ip6rd.relay_prefixlen;\n\t\tif (pbi1 > 0)\n\t\t\td |= ntohl(v6dst->s6_addr32[pbw0 + 1]) >>\n\t\t\t     (32 - pbi1);\n\n\t\tdst = tunnel->ip6rd.relay_prefix | htonl(d);\n\t}\n#else\n\tif (v6dst->s6_addr16[0] == htons(0x2002)) {\n\t\t/* 6to4 v6 addr has 16 bits prefix, 32 v4addr, 16 SLA, ... */\n\t\tmemcpy(&dst, &v6dst->s6_addr16[1], 4);\n\t}\n#endif\n\treturn dst;\n}\n\n/*\n *\tThis function assumes it is being called from dev_queue_xmit()\n *\tand that skb is filled properly by that function.\n */\n\nstatic netdev_tx_t ipip6_tunnel_xmit(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct pcpu_tstats *tstats;\n\tstruct iphdr  *tiph = &tunnel->parms.iph;\n\tstruct ipv6hdr *iph6 = ipv6_hdr(skb);\n\tu8     tos = tunnel->parms.iph.tos;\n\t__be16 df = tiph->frag_off;\n\tstruct rtable *rt;     \t\t\t/* Route to the other host */\n\tstruct net_device *tdev;\t\t/* Device to other host */\n\tstruct iphdr  *iph;\t\t\t/* Our new IP header */\n\tunsigned int max_headroom;\t\t/* The extra header space needed */\n\t__be32 dst = tiph->daddr;\n\tint    mtu;\n\tstruct in6_addr *addr6;\n\tint addr_type;\n\n\tif (skb->protocol != htons(ETH_P_IPV6))\n\t\tgoto tx_error;\n\n\t/* ISATAP (RFC4214) - must come before 6to4 */\n\tif (dev->priv_flags & IFF_ISATAP) {\n\t\tstruct neighbour *neigh = NULL;\n\n\t\tif (skb_dst(skb))\n\t\t\tneigh = skb_dst(skb)->neighbour;\n\n\t\tif (neigh == NULL) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_DEBUG \"sit: nexthop == NULL\\n\");\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\taddr6 = (struct in6_addr*)&neigh->primary_key;\n\t\taddr_type = ipv6_addr_type(addr6);\n\n\t\tif ((addr_type & IPV6_ADDR_UNICAST) &&\n\t\t     ipv6_addr_is_isatap(addr6))\n\t\t\tdst = addr6->s6_addr32[3];\n\t\telse\n\t\t\tgoto tx_error;\n\t}\n\n\tif (!dst)\n\t\tdst = try_6rd(&iph6->daddr, tunnel);\n\n\tif (!dst) {\n\t\tstruct neighbour *neigh = NULL;\n\n\t\tif (skb_dst(skb))\n\t\t\tneigh = skb_dst(skb)->neighbour;\n\n\t\tif (neigh == NULL) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_DEBUG \"sit: nexthop == NULL\\n\");\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\taddr6 = (struct in6_addr*)&neigh->primary_key;\n\t\taddr_type = ipv6_addr_type(addr6);\n\n\t\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t\taddr6 = &ipv6_hdr(skb)->daddr;\n\t\t\taddr_type = ipv6_addr_type(addr6);\n\t\t}\n\n\t\tif ((addr_type & IPV6_ADDR_COMPATv4) == 0)\n\t\t\tgoto tx_error_icmp;\n\n\t\tdst = addr6->s6_addr32[3];\n\t}\n\n\t{\n\t\tstruct flowi fl = { .fl4_dst = dst,\n\t\t\t\t    .fl4_src = tiph->saddr,\n\t\t\t\t    .fl4_tos = RT_TOS(tos),\n\t\t\t\t    .oif = tunnel->parms.link,\n\t\t\t\t    .proto = IPPROTO_IPV6 };\n\t\tif (ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\tdev->stats.tx_carrier_errors++;\n\t\t\tgoto tx_error_icmp;\n\t\t}\n\t}\n\tif (rt->rt_type != RTN_UNICAST) {\n\t\tip_rt_put(rt);\n\t\tdev->stats.tx_carrier_errors++;\n\t\tgoto tx_error_icmp;\n\t}\n\ttdev = rt->dst.dev;\n\n\tif (tdev == dev) {\n\t\tip_rt_put(rt);\n\t\tdev->stats.collisions++;\n\t\tgoto tx_error;\n\t}\n\n\tif (df) {\n\t\tmtu = dst_mtu(&rt->dst) - sizeof(struct iphdr);\n\n\t\tif (mtu < 68) {\n\t\t\tdev->stats.collisions++;\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\n\t\tif (mtu < IPV6_MIN_MTU) {\n\t\t\tmtu = IPV6_MIN_MTU;\n\t\t\tdf = 0;\n\t\t}\n\n\t\tif (tunnel->parms.iph.daddr && skb_dst(skb))\n\t\t\tskb_dst(skb)->ops->update_pmtu(skb_dst(skb), mtu);\n\n\t\tif (skb->len > mtu) {\n\t\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\t\tip_rt_put(rt);\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n\n\tif (tunnel->err_count > 0) {\n\t\tif (time_before(jiffies,\n\t\t\t\ttunnel->err_time + IPTUNNEL_ERR_TIMEO)) {\n\t\t\ttunnel->err_count--;\n\t\t\tdst_link_failure(skb);\n\t\t} else\n\t\t\ttunnel->err_count = 0;\n\t}\n\n\t/*\n\t * Okay, now see if we can stuff it in the buffer as-is.\n\t */\n\tmax_headroom = LL_RESERVED_SPACE(tdev)+sizeof(struct iphdr);\n\n\tif (skb_headroom(skb) < max_headroom || skb_shared(skb) ||\n\t    (skb_cloned(skb) && !skb_clone_writable(skb, 0))) {\n\t\tstruct sk_buff *new_skb = skb_realloc_headroom(skb, max_headroom);\n\t\tif (!new_skb) {\n\t\t\tip_rt_put(rt);\n\t\t\tdev->stats.tx_dropped++;\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(new_skb, skb->sk);\n\t\tdev_kfree_skb(skb);\n\t\tskb = new_skb;\n\t\tiph6 = ipv6_hdr(skb);\n\t}\n\n\tskb->transport_header = skb->network_header;\n\tskb_push(skb, sizeof(struct iphdr));\n\tskb_reset_network_header(skb);\n\tmemset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));\n\tIPCB(skb)->flags = 0;\n\tskb_dst_drop(skb);\n\tskb_dst_set(skb, &rt->dst);\n\n\t/*\n\t *\tPush down and install the IPIP header.\n\t */\n\n\tiph \t\t\t=\tip_hdr(skb);\n\tiph->version\t\t=\t4;\n\tiph->ihl\t\t=\tsizeof(struct iphdr)>>2;\n\tiph->frag_off\t\t=\tdf;\n\tiph->protocol\t\t=\tIPPROTO_IPV6;\n\tiph->tos\t\t=\tINET_ECN_encapsulate(tos, ipv6_get_dsfield(iph6));\n\tiph->daddr\t\t=\trt->rt_dst;\n\tiph->saddr\t\t=\trt->rt_src;\n\n\tif ((iph->ttl = tiph->ttl) == 0)\n\t\tiph->ttl\t=\tiph6->hop_limit;\n\n\tnf_reset(skb);\n\ttstats = this_cpu_ptr(dev->tstats);\n\t__IPTUNNEL_XMIT(tstats, &dev->stats);\n\treturn NETDEV_TX_OK;\n\ntx_error_icmp:\n\tdst_link_failure(skb);\ntx_error:\n\tdev->stats.tx_errors++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void ipip6_tunnel_bind_dev(struct net_device *dev)\n{\n\tstruct net_device *tdev = NULL;\n\tstruct ip_tunnel *tunnel;\n\tstruct iphdr *iph;\n\n\ttunnel = netdev_priv(dev);\n\tiph = &tunnel->parms.iph;\n\n\tif (iph->daddr) {\n\t\tstruct flowi fl = { .fl4_dst = iph->daddr,\n\t\t\t\t    .fl4_src = iph->saddr,\n\t\t\t\t    .fl4_tos = RT_TOS(iph->tos),\n\t\t\t\t    .oif = tunnel->parms.link,\n\t\t\t\t    .proto = IPPROTO_IPV6 };\n\t\tstruct rtable *rt;\n\t\tif (!ip_route_output_key(dev_net(dev), &rt, &fl)) {\n\t\t\ttdev = rt->dst.dev;\n\t\t\tip_rt_put(rt);\n\t\t}\n\t\tdev->flags |= IFF_POINTOPOINT;\n\t}\n\n\tif (!tdev && tunnel->parms.link)\n\t\ttdev = __dev_get_by_index(dev_net(dev), tunnel->parms.link);\n\n\tif (tdev) {\n\t\tdev->hard_header_len = tdev->hard_header_len + sizeof(struct iphdr);\n\t\tdev->mtu = tdev->mtu - sizeof(struct iphdr);\n\t\tif (dev->mtu < IPV6_MIN_MTU)\n\t\t\tdev->mtu = IPV6_MIN_MTU;\n\t}\n\tdev->iflink = tunnel->parms.link;\n}\n\nstatic int\nipip6_tunnel_ioctl (struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tint err = 0;\n\tstruct ip_tunnel_parm p;\n\tstruct ip_tunnel_prl prl;\n\tstruct ip_tunnel *t;\n\tstruct net *net = dev_net(dev);\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n#ifdef CONFIG_IPV6_SIT_6RD\n\tstruct ip_tunnel_6rd ip6rd;\n#endif\n\n\tswitch (cmd) {\n\tcase SIOCGETTUNNEL:\n#ifdef CONFIG_IPV6_SIT_6RD\n\tcase SIOCGET6RD:\n#endif\n\t\tt = NULL;\n\t\tif (dev == sitn->fb_tunnel_dev) {\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tt = ipip6_tunnel_locate(net, &p, 0);\n\t\t}\n\t\tif (t == NULL)\n\t\t\tt = netdev_priv(dev);\n\n\t\terr = -EFAULT;\n\t\tif (cmd == SIOCGETTUNNEL) {\n\t\t\tmemcpy(&p, &t->parms, sizeof(p));\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &p,\n\t\t\t\t\t sizeof(p)))\n\t\t\t\tgoto done;\n#ifdef CONFIG_IPV6_SIT_6RD\n\t\t} else {\n\t\t\tipv6_addr_copy(&ip6rd.prefix, &t->ip6rd.prefix);\n\t\t\tip6rd.relay_prefix = t->ip6rd.relay_prefix;\n\t\t\tip6rd.prefixlen = t->ip6rd.prefixlen;\n\t\t\tip6rd.relay_prefixlen = t->ip6rd.relay_prefixlen;\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &ip6rd,\n\t\t\t\t\t sizeof(ip6rd)))\n\t\t\t\tgoto done;\n#endif\n\t\t}\n\t\terr = 0;\n\t\tbreak;\n\n\tcase SIOCADDTUNNEL:\n\tcase SIOCCHGTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\tgoto done;\n\n\t\terr = -EINVAL;\n\t\tif (p.iph.version != 4 || p.iph.protocol != IPPROTO_IPV6 ||\n\t\t    p.iph.ihl != 5 || (p.iph.frag_off&htons(~IP_DF)))\n\t\t\tgoto done;\n\t\tif (p.iph.ttl)\n\t\t\tp.iph.frag_off |= htons(IP_DF);\n\n\t\tt = ipip6_tunnel_locate(net, &p, cmd == SIOCADDTUNNEL);\n\n\t\tif (dev != sitn->fb_tunnel_dev && cmd == SIOCCHGTUNNEL) {\n\t\t\tif (t != NULL) {\n\t\t\t\tif (t->dev != dev) {\n\t\t\t\t\terr = -EEXIST;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (((dev->flags&IFF_POINTOPOINT) && !p.iph.daddr) ||\n\t\t\t\t    (!(dev->flags&IFF_POINTOPOINT) && p.iph.daddr)) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tt = netdev_priv(dev);\n\t\t\t\tipip6_tunnel_unlink(sitn, t);\n\t\t\t\tsynchronize_net();\n\t\t\t\tt->parms.iph.saddr = p.iph.saddr;\n\t\t\t\tt->parms.iph.daddr = p.iph.daddr;\n\t\t\t\tmemcpy(dev->dev_addr, &p.iph.saddr, 4);\n\t\t\t\tmemcpy(dev->broadcast, &p.iph.daddr, 4);\n\t\t\t\tipip6_tunnel_link(sitn, t);\n\t\t\t\tnetdev_state_change(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (t) {\n\t\t\terr = 0;\n\t\t\tif (cmd == SIOCCHGTUNNEL) {\n\t\t\t\tt->parms.iph.ttl = p.iph.ttl;\n\t\t\t\tt->parms.iph.tos = p.iph.tos;\n\t\t\t\tif (t->parms.link != p.link) {\n\t\t\t\t\tt->parms.link = p.link;\n\t\t\t\t\tipip6_tunnel_bind_dev(dev);\n\t\t\t\t\tnetdev_state_change(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (copy_to_user(ifr->ifr_ifru.ifru_data, &t->parms, sizeof(p)))\n\t\t\t\terr = -EFAULT;\n\t\t} else\n\t\t\terr = (cmd == SIOCADDTUNNEL ? -ENOBUFS : -ENOENT);\n\t\tbreak;\n\n\tcase SIOCDELTUNNEL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\tif (dev == sitn->fb_tunnel_dev) {\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&p, ifr->ifr_ifru.ifru_data, sizeof(p)))\n\t\t\t\tgoto done;\n\t\t\terr = -ENOENT;\n\t\t\tif ((t = ipip6_tunnel_locate(net, &p, 0)) == NULL)\n\t\t\t\tgoto done;\n\t\t\terr = -EPERM;\n\t\t\tif (t == netdev_priv(sitn->fb_tunnel_dev))\n\t\t\t\tgoto done;\n\t\t\tdev = t->dev;\n\t\t}\n\t\tunregister_netdevice(dev);\n\t\terr = 0;\n\t\tbreak;\n\n\tcase SIOCGETPRL:\n\t\terr = -EINVAL;\n\t\tif (dev == sitn->fb_tunnel_dev)\n\t\t\tgoto done;\n\t\terr = -ENOENT;\n\t\tif (!(t = netdev_priv(dev)))\n\t\t\tgoto done;\n\t\terr = ipip6_tunnel_get_prl(t, ifr->ifr_ifru.ifru_data);\n\t\tbreak;\n\n\tcase SIOCADDPRL:\n\tcase SIOCDELPRL:\n\tcase SIOCCHGPRL:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\t\terr = -EINVAL;\n\t\tif (dev == sitn->fb_tunnel_dev)\n\t\t\tgoto done;\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&prl, ifr->ifr_ifru.ifru_data, sizeof(prl)))\n\t\t\tgoto done;\n\t\terr = -ENOENT;\n\t\tif (!(t = netdev_priv(dev)))\n\t\t\tgoto done;\n\n\t\tswitch (cmd) {\n\t\tcase SIOCDELPRL:\n\t\t\terr = ipip6_tunnel_del_prl(t, &prl);\n\t\t\tbreak;\n\t\tcase SIOCADDPRL:\n\t\tcase SIOCCHGPRL:\n\t\t\terr = ipip6_tunnel_add_prl(t, &prl, cmd == SIOCCHGPRL);\n\t\t\tbreak;\n\t\t}\n\t\tnetdev_state_change(dev);\n\t\tbreak;\n\n#ifdef CONFIG_IPV6_SIT_6RD\n\tcase SIOCADD6RD:\n\tcase SIOCCHG6RD:\n\tcase SIOCDEL6RD:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tgoto done;\n\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&ip6rd, ifr->ifr_ifru.ifru_data,\n\t\t\t\t   sizeof(ip6rd)))\n\t\t\tgoto done;\n\n\t\tt = netdev_priv(dev);\n\n\t\tif (cmd != SIOCDEL6RD) {\n\t\t\tstruct in6_addr prefix;\n\t\t\t__be32 relay_prefix;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (ip6rd.relay_prefixlen > 32 ||\n\t\t\t    ip6rd.prefixlen + (32 - ip6rd.relay_prefixlen) > 64)\n\t\t\t\tgoto done;\n\n\t\t\tipv6_addr_prefix(&prefix, &ip6rd.prefix,\n\t\t\t\t\t ip6rd.prefixlen);\n\t\t\tif (!ipv6_addr_equal(&prefix, &ip6rd.prefix))\n\t\t\t\tgoto done;\n\t\t\tif (ip6rd.relay_prefixlen)\n\t\t\t\trelay_prefix = ip6rd.relay_prefix &\n\t\t\t\t\t       htonl(0xffffffffUL <<\n\t\t\t\t\t\t     (32 - ip6rd.relay_prefixlen));\n\t\t\telse\n\t\t\t\trelay_prefix = 0;\n\t\t\tif (relay_prefix != ip6rd.relay_prefix)\n\t\t\t\tgoto done;\n\n\t\t\tipv6_addr_copy(&t->ip6rd.prefix, &prefix);\n\t\t\tt->ip6rd.relay_prefix = relay_prefix;\n\t\t\tt->ip6rd.prefixlen = ip6rd.prefixlen;\n\t\t\tt->ip6rd.relay_prefixlen = ip6rd.relay_prefixlen;\n\t\t} else\n\t\t\tipip6_tunnel_clone_6rd(dev, sitn);\n\n\t\terr = 0;\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\ndone:\n\treturn err;\n}\n\nstatic int ipip6_tunnel_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tif (new_mtu < IPV6_MIN_MTU || new_mtu > 0xFFF8 - sizeof(struct iphdr))\n\t\treturn -EINVAL;\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\nstatic const struct net_device_ops ipip6_netdev_ops = {\n\t.ndo_uninit\t= ipip6_tunnel_uninit,\n\t.ndo_start_xmit\t= ipip6_tunnel_xmit,\n\t.ndo_do_ioctl\t= ipip6_tunnel_ioctl,\n\t.ndo_change_mtu\t= ipip6_tunnel_change_mtu,\n\t.ndo_get_stats\t= ipip6_get_stats,\n};\n\nstatic void ipip6_dev_free(struct net_device *dev)\n{\n\tfree_percpu(dev->tstats);\n\tfree_netdev(dev);\n}\n\nstatic void ipip6_tunnel_setup(struct net_device *dev)\n{\n\tdev->netdev_ops\t\t= &ipip6_netdev_ops;\n\tdev->destructor \t= ipip6_dev_free;\n\n\tdev->type\t\t= ARPHRD_SIT;\n\tdev->hard_header_len \t= LL_MAX_HEADER + sizeof(struct iphdr);\n\tdev->mtu\t\t= ETH_DATA_LEN - sizeof(struct iphdr);\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->priv_flags\t       &= ~IFF_XMIT_DST_RELEASE;\n\tdev->iflink\t\t= 0;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_NETNS_LOCAL;\n\tdev->features\t\t|= NETIF_F_LLTX;\n}\n\nstatic int ipip6_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tmemcpy(dev->dev_addr, &tunnel->parms.iph.saddr, 4);\n\tmemcpy(dev->broadcast, &tunnel->parms.iph.daddr, 4);\n\n\tipip6_tunnel_bind_dev(dev);\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int __net_init ipip6_fb_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\tstruct net *net = dev_net(dev);\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\n\ttunnel->dev = dev;\n\tstrcpy(tunnel->parms.name, dev->name);\n\n\tiph->version\t\t= 4;\n\tiph->protocol\t\t= IPPROTO_IPV6;\n\tiph->ihl\t\t= 5;\n\tiph->ttl\t\t= 64;\n\n\tdev->tstats = alloc_percpu(struct pcpu_tstats);\n\tif (!dev->tstats)\n\t\treturn -ENOMEM;\n\tdev_hold(dev);\n\tsitn->tunnels_wc[0]\t= tunnel;\n\treturn 0;\n}\n\nstatic struct xfrm_tunnel sit_handler __read_mostly = {\n\t.handler\t=\tipip6_rcv,\n\t.err_handler\t=\tipip6_err,\n\t.priority\t=\t1,\n};\n\nstatic void __net_exit sit_destroy_tunnels(struct sit_net *sitn, struct list_head *head)\n{\n\tint prio;\n\n\tfor (prio = 1; prio < 4; prio++) {\n\t\tint h;\n\t\tfor (h = 0; h < HASH_SIZE; h++) {\n\t\t\tstruct ip_tunnel *t = sitn->tunnels[prio][h];\n\n\t\t\twhile (t != NULL) {\n\t\t\t\tunregister_netdevice_queue(t->dev, head);\n\t\t\t\tt = t->next;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int __net_init sit_init_net(struct net *net)\n{\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\tint err;\n\n\tsitn->tunnels[0] = sitn->tunnels_wc;\n\tsitn->tunnels[1] = sitn->tunnels_l;\n\tsitn->tunnels[2] = sitn->tunnels_r;\n\tsitn->tunnels[3] = sitn->tunnels_r_l;\n\n\tsitn->fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel), \"sit0\",\n\t\t\t\t\t   ipip6_tunnel_setup);\n\tif (!sitn->fb_tunnel_dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_dev;\n\t}\n\tdev_net_set(sitn->fb_tunnel_dev, net);\n\n\terr = ipip6_fb_tunnel_init(sitn->fb_tunnel_dev);\n\tif (err)\n\t\tgoto err_dev_free;\n\n\tipip6_tunnel_clone_6rd(sitn->fb_tunnel_dev, sitn);\n\n\tif ((err = register_netdev(sitn->fb_tunnel_dev)))\n\t\tgoto err_reg_dev;\n\n\treturn 0;\n\nerr_reg_dev:\n\tdev_put(sitn->fb_tunnel_dev);\nerr_dev_free:\n\tipip6_dev_free(sitn->fb_tunnel_dev);\nerr_alloc_dev:\n\treturn err;\n}\n\nstatic void __net_exit sit_exit_net(struct net *net)\n{\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\tLIST_HEAD(list);\n\n\trtnl_lock();\n\tsit_destroy_tunnels(sitn, &list);\n\tunregister_netdevice_queue(sitn->fb_tunnel_dev, &list);\n\tunregister_netdevice_many(&list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations sit_net_ops = {\n\t.init = sit_init_net,\n\t.exit = sit_exit_net,\n\t.id   = &sit_net_id,\n\t.size = sizeof(struct sit_net),\n};\n\nstatic void __exit sit_cleanup(void)\n{\n\txfrm4_tunnel_deregister(&sit_handler, AF_INET6);\n\n\tunregister_pernet_device(&sit_net_ops);\n\trcu_barrier(); /* Wait for completion of call_rcu()'s */\n}\n\nstatic int __init sit_init(void)\n{\n\tint err;\n\n\tprintk(KERN_INFO \"IPv6 over IPv4 tunneling driver\\n\");\n\n\terr = register_pernet_device(&sit_net_ops);\n\tif (err < 0)\n\t\treturn err;\n\terr = xfrm4_tunnel_register(&sit_handler, AF_INET6);\n\tif (err < 0) {\n\t\tunregister_pernet_device(&sit_net_ops);\n\t\tprintk(KERN_INFO \"sit init: Can't add protocol\\n\");\n\t}\n\treturn err;\n}\n\nmodule_init(sit_init);\nmodule_exit(sit_cleanup);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETDEV(\"sit0\");\n"], "filenames": ["include/linux/netdevice.h", "net/core/dev.c", "net/ipv4/ip_gre.c", "net/ipv4/ipip.c", "net/ipv6/sit.c"], "buggy_code_start_loc": [2394, 1116, 1768, 916, 1293], "buggy_code_end_loc": [2394, 1124, 1769, 917, 1294], "fixing_code_start_loc": [2395, 1117, 1768, 916, 1293], "fixing_code_end_loc": [2398, 1132, 1769, 917, 1294], "type": "NVD-CWE-noinfo", "message": "The dev_load function in net/core/dev.c in the Linux kernel before 2.6.38 allows local users to bypass an intended CAP_SYS_MODULE capability requirement and load arbitrary modules by leveraging the CAP_NET_ADMIN capability.", "other": {"cve": {"id": "CVE-2011-1019", "sourceIdentifier": "secalert@redhat.com", "published": "2013-03-01T12:37:47.067", "lastModified": "2023-02-13T03:23:21.677", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The dev_load function in net/core/dev.c in the Linux kernel before 2.6.38 allows local users to bypass an intended CAP_SYS_MODULE capability requirement and load arbitrary modules by leveraging the CAP_NET_ADMIN capability."}, {"lang": "es", "value": "La funci\u00f3n dev_load en net/core/dev.c en el kernel de Linux anterior a v2.6.38 permite a usuarios locales eludir las capacidades CAP_SYS_MODULE requeridas y cargar modulos arbitrarios mediante el aprovechamiento de la capacidad CAP_NET_ADMIN."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:N/I:P/A:N", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 1.9}, "baseSeverity": "LOW", "exploitabilityScore": 3.4, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.38", "matchCriteriaId": "9988A98F-3440-467E-8ADA-1E413DC25C21"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v2.6/ChangeLog-2.6.38", "source": "secalert@redhat.com", "tags": ["Broken Link"]}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=8909c9ad8ff03611c9c96c9a92656213e4bb495b", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/02/25/1", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=680360", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/8909c9ad8ff03611c9c96c9a92656213e4bb495b", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/8909c9ad8ff03611c9c96c9a92656213e4bb495b"}}