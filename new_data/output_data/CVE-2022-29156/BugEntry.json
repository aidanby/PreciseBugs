{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * RDMA Transport Layer\n *\n * Copyright (c) 2014 - 2018 ProfitBricks GmbH. All rights reserved.\n * Copyright (c) 2018 - 2019 1&1 IONOS Cloud GmbH. All rights reserved.\n * Copyright (c) 2019 - 2020 1&1 IONOS SE. All rights reserved.\n */\n\n#undef pr_fmt\n#define pr_fmt(fmt) KBUILD_MODNAME \" L\" __stringify(__LINE__) \": \" fmt\n\n#include <linux/module.h>\n#include <linux/rculist.h>\n#include <linux/random.h>\n\n#include \"rtrs-clt.h\"\n#include \"rtrs-log.h\"\n\n#define RTRS_CONNECT_TIMEOUT_MS 30000\n/*\n * Wait a bit before trying to reconnect after a failure\n * in order to give server time to finish clean up which\n * leads to \"false positives\" failed reconnect attempts\n */\n#define RTRS_RECONNECT_BACKOFF 1000\n/*\n * Wait for additional random time between 0 and 8 seconds\n * before starting to reconnect to avoid clients reconnecting\n * all at once in case of a major network outage\n */\n#define RTRS_RECONNECT_SEED 8\n\n#define FIRST_CONN 0x01\n/* limit to 128 * 4k = 512k max IO */\n#define RTRS_MAX_SEGMENTS          128\n\nMODULE_DESCRIPTION(\"RDMA Transport Client\");\nMODULE_LICENSE(\"GPL\");\n\nstatic const struct rtrs_rdma_dev_pd_ops dev_pd_ops;\nstatic struct rtrs_rdma_dev_pd dev_pd = {\n\t.ops = &dev_pd_ops\n};\n\nstatic struct workqueue_struct *rtrs_wq;\nstatic struct class *rtrs_clt_dev_class;\n\nstatic inline bool rtrs_clt_is_connected(const struct rtrs_clt_sess *clt)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tbool connected = false;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry)\n\t\tconnected |= READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTED;\n\trcu_read_unlock();\n\n\treturn connected;\n}\n\nstatic struct rtrs_permit *\n__rtrs_get_permit(struct rtrs_clt_sess *clt, enum rtrs_clt_con_type con_type)\n{\n\tsize_t max_depth = clt->queue_depth;\n\tstruct rtrs_permit *permit;\n\tint bit;\n\n\t/*\n\t * Adapted from null_blk get_tag(). Callers from different cpus may\n\t * grab the same bit, since find_first_zero_bit is not atomic.\n\t * But then the test_and_set_bit_lock will fail for all the\n\t * callers but one, so that they will loop again.\n\t * This way an explicit spinlock is not required.\n\t */\n\tdo {\n\t\tbit = find_first_zero_bit(clt->permits_map, max_depth);\n\t\tif (bit >= max_depth)\n\t\t\treturn NULL;\n\t} while (test_and_set_bit_lock(bit, clt->permits_map));\n\n\tpermit = get_permit(clt, bit);\n\tWARN_ON(permit->mem_id != bit);\n\tpermit->cpu_id = raw_smp_processor_id();\n\tpermit->con_type = con_type;\n\n\treturn permit;\n}\n\nstatic inline void __rtrs_put_permit(struct rtrs_clt_sess *clt,\n\t\t\t\t      struct rtrs_permit *permit)\n{\n\tclear_bit_unlock(permit->mem_id, clt->permits_map);\n}\n\n/**\n * rtrs_clt_get_permit() - allocates permit for future RDMA operation\n * @clt:\tCurrent session\n * @con_type:\tType of connection to use with the permit\n * @can_wait:\tWait type\n *\n * Description:\n *    Allocates permit for the following RDMA operation.  Permit is used\n *    to preallocate all resources and to propagate memory pressure\n *    up earlier.\n *\n * Context:\n *    Can sleep if @wait == RTRS_PERMIT_WAIT\n */\nstruct rtrs_permit *rtrs_clt_get_permit(struct rtrs_clt_sess *clt,\n\t\t\t\t\t  enum rtrs_clt_con_type con_type,\n\t\t\t\t\t  enum wait_type can_wait)\n{\n\tstruct rtrs_permit *permit;\n\tDEFINE_WAIT(wait);\n\n\tpermit = __rtrs_get_permit(clt, con_type);\n\tif (permit || !can_wait)\n\t\treturn permit;\n\n\tdo {\n\t\tprepare_to_wait(&clt->permits_wait, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tpermit = __rtrs_get_permit(clt, con_type);\n\t\tif (permit)\n\t\t\tbreak;\n\n\t\tio_schedule();\n\t} while (1);\n\n\tfinish_wait(&clt->permits_wait, &wait);\n\n\treturn permit;\n}\nEXPORT_SYMBOL(rtrs_clt_get_permit);\n\n/**\n * rtrs_clt_put_permit() - puts allocated permit\n * @clt:\tCurrent session\n * @permit:\tPermit to be freed\n *\n * Context:\n *    Does not matter\n */\nvoid rtrs_clt_put_permit(struct rtrs_clt_sess *clt,\n\t\t\t struct rtrs_permit *permit)\n{\n\tif (WARN_ON(!test_bit(permit->mem_id, clt->permits_map)))\n\t\treturn;\n\n\t__rtrs_put_permit(clt, permit);\n\n\t/*\n\t * rtrs_clt_get_permit() adds itself to the &clt->permits_wait list\n\t * before calling schedule(). So if rtrs_clt_get_permit() is sleeping\n\t * it must have added itself to &clt->permits_wait before\n\t * __rtrs_put_permit() finished.\n\t * Hence it is safe to guard wake_up() with a waitqueue_active() test.\n\t */\n\tif (waitqueue_active(&clt->permits_wait))\n\t\twake_up(&clt->permits_wait);\n}\nEXPORT_SYMBOL(rtrs_clt_put_permit);\n\n/**\n * rtrs_permit_to_clt_con() - returns RDMA connection pointer by the permit\n * @clt_path: client path pointer\n * @permit: permit for the allocation of the RDMA buffer\n * Note:\n *     IO connection starts from 1.\n *     0 connection is for user messages.\n */\nstatic\nstruct rtrs_clt_con *rtrs_permit_to_clt_con(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t    struct rtrs_permit *permit)\n{\n\tint id = 0;\n\n\tif (permit->con_type == RTRS_IO_CON)\n\t\tid = (permit->cpu_id % (clt_path->s.irq_con_num - 1)) + 1;\n\n\treturn to_clt_con(clt_path->s.con[id]);\n}\n\n/**\n * rtrs_clt_change_state() - change the session state through session state\n * machine.\n *\n * @clt_path: client path to change the state of.\n * @new_state: state to change to.\n *\n * returns true if sess's state is changed to new state, otherwise return false.\n *\n * Locks:\n * state_wq lock must be hold.\n */\nstatic bool rtrs_clt_change_state(struct rtrs_clt_path *clt_path,\n\t\t\t\t     enum rtrs_clt_state new_state)\n{\n\tenum rtrs_clt_state old_state;\n\tbool changed = false;\n\n\tlockdep_assert_held(&clt_path->state_wq.lock);\n\n\told_state = clt_path->state;\n\tswitch (new_state) {\n\tcase RTRS_CLT_CONNECTING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_RECONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_RECONNECTING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTED:\n\t\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tcase RTRS_CLT_CLOSED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CONNECTED:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CLOSING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tcase RTRS_CLT_RECONNECTING:\n\t\tcase RTRS_CLT_CONNECTED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CLOSED:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CLOSING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_DEAD:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CLOSED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (changed) {\n\t\tclt_path->state = new_state;\n\t\twake_up_locked(&clt_path->state_wq);\n\t}\n\n\treturn changed;\n}\n\nstatic bool rtrs_clt_change_state_from_to(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t   enum rtrs_clt_state old_state,\n\t\t\t\t\t   enum rtrs_clt_state new_state)\n{\n\tbool changed = false;\n\n\tspin_lock_irq(&clt_path->state_wq.lock);\n\tif (clt_path->state == old_state)\n\t\tchanged = rtrs_clt_change_state(clt_path, new_state);\n\tspin_unlock_irq(&clt_path->state_wq.lock);\n\n\treturn changed;\n}\n\nstatic void rtrs_rdma_error_recovery(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tif (rtrs_clt_change_state_from_to(clt_path,\n\t\t\t\t\t   RTRS_CLT_CONNECTED,\n\t\t\t\t\t   RTRS_CLT_RECONNECTING)) {\n\t\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\t\tunsigned int delay_ms;\n\n\t\t/*\n\t\t * Normal scenario, reconnect if we were successfully connected\n\t\t */\n\t\tdelay_ms = clt->reconnect_delay_sec * 1000;\n\t\tqueue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork,\n\t\t\t\t   msecs_to_jiffies(delay_ms +\n\t\t\t\t\t\t    prandom_u32() % RTRS_RECONNECT_SEED));\n\t} else {\n\t\t/*\n\t\t * Error can happen just on establishing new connection,\n\t\t * so notify waiter with error state, waiter is responsible\n\t\t * for cleaning the rest and reconnect if needed.\n\t\t */\n\t\trtrs_clt_change_state_from_to(clt_path,\n\t\t\t\t\t       RTRS_CLT_CONNECTING,\n\t\t\t\t\t       RTRS_CLT_CONNECTING_ERR);\n\t}\n}\n\nstatic void rtrs_clt_fast_reg_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(con->c.path, \"Failed IB_WR_REG_MR: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_rdma_error_recovery(con);\n\t}\n}\n\nstatic struct ib_cqe fast_reg_cqe = {\n\t.done = rtrs_clt_fast_reg_done\n};\n\nstatic void complete_rdma_req(struct rtrs_clt_io_req *req, int errno,\n\t\t\t      bool notify, bool can_wait);\n\nstatic void rtrs_clt_inv_rkey_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_io_req *req =\n\t\tcontainer_of(wc->wr_cqe, typeof(*req), inv_cqe);\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(con->c.path, \"Failed IB_WR_LOCAL_INV: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_rdma_error_recovery(con);\n\t}\n\treq->need_inv = false;\n\tif (req->need_inv_comp)\n\t\tcomplete(&req->inv_comp);\n\telse\n\t\t/* Complete request from INV callback */\n\t\tcomplete_rdma_req(req, req->inv_errno, true, false);\n}\n\nstatic int rtrs_inv_rkey(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct ib_send_wr wr = {\n\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t.wr_cqe\t\t    = &req->inv_cqe,\n\t\t.send_flags\t    = IB_SEND_SIGNALED,\n\t\t.ex.invalidate_rkey = req->mr->rkey,\n\t};\n\treq->inv_cqe.done = rtrs_clt_inv_rkey_done;\n\n\treturn ib_post_send(con->c.qp, &wr, NULL);\n}\n\nstatic void complete_rdma_req(struct rtrs_clt_io_req *req, int errno,\n\t\t\t      bool notify, bool can_wait)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_clt_path *clt_path;\n\tint err;\n\n\tif (WARN_ON(!req->in_use))\n\t\treturn;\n\tif (WARN_ON(!req->con))\n\t\treturn;\n\tclt_path = to_clt_path(con->c.path);\n\n\tif (req->sg_cnt) {\n\t\tif (req->dir == DMA_FROM_DEVICE && req->need_inv) {\n\t\t\t/*\n\t\t\t * We are here to invalidate read requests\n\t\t\t * ourselves.  In normal scenario server should\n\t\t\t * send INV for all read requests, but\n\t\t\t * we are here, thus two things could happen:\n\t\t\t *\n\t\t\t *    1.  this is failover, when errno != 0\n\t\t\t *        and can_wait == 1,\n\t\t\t *\n\t\t\t *    2.  something totally bad happened and\n\t\t\t *        server forgot to send INV, so we\n\t\t\t *        should do that ourselves.\n\t\t\t */\n\n\t\t\tif (can_wait) {\n\t\t\t\treq->need_inv_comp = true;\n\t\t\t} else {\n\t\t\t\t/* This should be IO path, so always notify */\n\t\t\t\tWARN_ON(!notify);\n\t\t\t\t/* Save errno for INV callback */\n\t\t\t\treq->inv_errno = errno;\n\t\t\t}\n\n\t\t\trefcount_inc(&req->ref);\n\t\t\terr = rtrs_inv_rkey(req);\n\t\t\tif (err) {\n\t\t\t\trtrs_err(con->c.path, \"Send INV WR key=%#x: %d\\n\",\n\t\t\t\t\t  req->mr->rkey, err);\n\t\t\t} else if (can_wait) {\n\t\t\t\twait_for_completion(&req->inv_comp);\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Something went wrong, so request will be\n\t\t\t\t * completed from INV callback.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(1);\n\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tif (!refcount_dec_and_test(&req->ref))\n\t\t\t\treturn;\n\t\t}\n\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\tif (!refcount_dec_and_test(&req->ref))\n\t\treturn;\n\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\tatomic_dec(&clt_path->stats->inflight);\n\n\treq->in_use = false;\n\treq->con = NULL;\n\n\tif (errno) {\n\t\trtrs_err_rl(con->c.path, \"IO request failed: error=%d path=%s [%s:%u] notify=%d\\n\",\n\t\t\t    errno, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port, notify);\n\t}\n\n\tif (notify)\n\t\treq->conf(req->priv, errno);\n}\n\nstatic int rtrs_post_send_rdma(struct rtrs_clt_con *con,\n\t\t\t\tstruct rtrs_clt_io_req *req,\n\t\t\t\tstruct rtrs_rbuf *rbuf, u32 off,\n\t\t\t\tu32 imm, struct ib_send_wr *wr)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tenum ib_send_flags flags;\n\tstruct ib_sge sge;\n\n\tif (!req->sg_size) {\n\t\trtrs_wrn(con->c.path,\n\t\t\t \"Doing RDMA Write failed, no data supplied\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* user data and user message in the first list element */\n\tsge.addr   = req->iu->dma_addr;\n\tsge.length = req->sg_size;\n\tsge.lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\n\t/*\n\t * From time to time we have to post signalled sends,\n\t * or send queue will fill up and only QP reset can help.\n\t */\n\tflags = atomic_inc_return(&con->c.wr_cnt) % clt_path->s.signal_interval ?\n\t\t\t0 : IB_SEND_SIGNALED;\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      req->iu->dma_addr,\n\t\t\t\t      req->sg_size, DMA_TO_DEVICE);\n\n\treturn rtrs_iu_post_rdma_write_imm(&con->c, req->iu, &sge, 1,\n\t\t\t\t\t    rbuf->rkey, rbuf->addr + off,\n\t\t\t\t\t    imm, flags, wr, NULL);\n}\n\nstatic void process_io_rsp(struct rtrs_clt_path *clt_path, u32 msg_id,\n\t\t\t   s16 errno, bool w_inval)\n{\n\tstruct rtrs_clt_io_req *req;\n\n\tif (WARN_ON(msg_id >= clt_path->queue_depth))\n\t\treturn;\n\n\treq = &clt_path->reqs[msg_id];\n\t/* Drop need_inv if server responded with send with invalidation */\n\treq->need_inv &= !w_inval;\n\tcomplete_rdma_req(req, errno, true, false);\n}\n\nstatic void rtrs_clt_recv_done(struct rtrs_clt_con *con, struct ib_wc *wc)\n{\n\tstruct rtrs_iu *iu;\n\tint err;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tWARN_ON((clt_path->flags & RTRS_MSG_NEW_RKEY_F) == 0);\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu,\n\t\t\t  cqe);\n\terr = rtrs_iu_post_recv(&con->c, iu);\n\tif (err) {\n\t\trtrs_err(con->c.path, \"post iu failed %d\\n\", err);\n\t\trtrs_rdma_error_recovery(con);\n\t}\n}\n\nstatic void rtrs_clt_rkey_rsp_done(struct rtrs_clt_con *con, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_msg_rkey_rsp *msg;\n\tu32 imm_type, imm_payload;\n\tbool w_inval = false;\n\tstruct rtrs_iu *iu;\n\tu32 buf_id;\n\tint err;\n\n\tWARN_ON((clt_path->flags & RTRS_MSG_NEW_RKEY_F) == 0);\n\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\n\tif (wc->byte_len < sizeof(*msg)) {\n\t\trtrs_err(con->c.path, \"rkey response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\tib_dma_sync_single_for_cpu(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t   iu->size, DMA_FROM_DEVICE);\n\tmsg = iu->buf;\n\tif (le16_to_cpu(msg->type) != RTRS_MSG_RKEY_RSP) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"rkey response is malformed: type %d\\n\",\n\t\t\t  le16_to_cpu(msg->type));\n\t\tgoto out;\n\t}\n\tbuf_id = le16_to_cpu(msg->buf_id);\n\tif (WARN_ON(buf_id >= clt_path->queue_depth))\n\t\tgoto out;\n\n\trtrs_from_imm(be32_to_cpu(wc->ex.imm_data), &imm_type, &imm_payload);\n\tif (imm_type == RTRS_IO_RSP_IMM ||\n\t    imm_type == RTRS_IO_RSP_W_INV_IMM) {\n\t\tu32 msg_id;\n\n\t\tw_inval = (imm_type == RTRS_IO_RSP_W_INV_IMM);\n\t\trtrs_from_io_rsp_imm(imm_payload, &msg_id, &err);\n\n\t\tif (WARN_ON(buf_id != msg_id))\n\t\t\tgoto out;\n\t\tclt_path->rbufs[buf_id].rkey = le32_to_cpu(msg->rkey);\n\t\tprocess_io_rsp(clt_path, msg_id, err, w_inval);\n\t}\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t      iu->size, DMA_FROM_DEVICE);\n\treturn rtrs_clt_recv_done(con, wc);\nout:\n\trtrs_rdma_error_recovery(con);\n}\n\nstatic void rtrs_clt_rdma_done(struct ib_cq *cq, struct ib_wc *wc);\n\nstatic struct ib_cqe io_comp_cqe = {\n\t.done = rtrs_clt_rdma_done\n};\n\n/*\n * Post x2 empty WRs: first is for this RDMA with IMM,\n * second is for RECV with INV, which happened earlier.\n */\nstatic int rtrs_post_recv_empty_x2(struct rtrs_con *con, struct ib_cqe *cqe)\n{\n\tstruct ib_recv_wr wr_arr[2], *wr;\n\tint i;\n\n\tmemset(wr_arr, 0, sizeof(wr_arr));\n\tfor (i = 0; i < ARRAY_SIZE(wr_arr); i++) {\n\t\twr = &wr_arr[i];\n\t\twr->wr_cqe  = cqe;\n\t\tif (i)\n\t\t\t/* Chain backwards */\n\t\t\twr->next = &wr_arr[i - 1];\n\t}\n\n\treturn ib_post_recv(con->qp, wr, NULL);\n}\n\nstatic void rtrs_clt_rdma_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tu32 imm_type, imm_payload;\n\tbool w_inval = false;\n\tint err;\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\t\trtrs_err(clt_path->clt, \"RDMA failed: %s\\n\",\n\t\t\t\t  ib_wc_status_msg(wc->status));\n\t\t\trtrs_rdma_error_recovery(con);\n\t\t}\n\t\treturn;\n\t}\n\trtrs_clt_update_wc_stats(con);\n\n\tswitch (wc->opcode) {\n\tcase IB_WC_RECV_RDMA_WITH_IMM:\n\t\t/*\n\t\t * post_recv() RDMA write completions of IO reqs (read/write)\n\t\t * and hb\n\t\t */\n\t\tif (WARN_ON(wc->wr_cqe->done != rtrs_clt_rdma_done))\n\t\t\treturn;\n\t\trtrs_from_imm(be32_to_cpu(wc->ex.imm_data),\n\t\t\t       &imm_type, &imm_payload);\n\t\tif (imm_type == RTRS_IO_RSP_IMM ||\n\t\t    imm_type == RTRS_IO_RSP_W_INV_IMM) {\n\t\t\tu32 msg_id;\n\n\t\t\tw_inval = (imm_type == RTRS_IO_RSP_W_INV_IMM);\n\t\t\trtrs_from_io_rsp_imm(imm_payload, &msg_id, &err);\n\n\t\t\tprocess_io_rsp(clt_path, msg_id, err, w_inval);\n\t\t} else if (imm_type == RTRS_HB_MSG_IMM) {\n\t\t\tWARN_ON(con->c.cid);\n\t\t\trtrs_send_hb_ack(&clt_path->s);\n\t\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\t\t} else if (imm_type == RTRS_HB_ACK_IMM) {\n\t\t\tWARN_ON(con->c.cid);\n\t\t\tclt_path->s.hb_missed_cnt = 0;\n\t\t\tclt_path->s.hb_cur_latency =\n\t\t\t\tktime_sub(ktime_get(), clt_path->s.hb_last_sent);\n\t\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\t\t} else {\n\t\t\trtrs_wrn(con->c.path, \"Unknown IMM type %u\\n\",\n\t\t\t\t  imm_type);\n\t\t}\n\t\tif (w_inval)\n\t\t\t/*\n\t\t\t * Post x2 empty WRs: first is for this RDMA with IMM,\n\t\t\t * second is for RECV with INV, which happened earlier.\n\t\t\t */\n\t\t\terr = rtrs_post_recv_empty_x2(&con->c, &io_comp_cqe);\n\t\telse\n\t\t\terr = rtrs_post_recv_empty(&con->c, &io_comp_cqe);\n\t\tif (err) {\n\t\t\trtrs_err(con->c.path, \"rtrs_post_recv_empty(): %d\\n\",\n\t\t\t\t  err);\n\t\t\trtrs_rdma_error_recovery(con);\n\t\t}\n\t\tbreak;\n\tcase IB_WC_RECV:\n\t\t/*\n\t\t * Key invalidations from server side\n\t\t */\n\t\tWARN_ON(!(wc->wc_flags & IB_WC_WITH_INVALIDATE ||\n\t\t\t  wc->wc_flags & IB_WC_WITH_IMM));\n\t\tWARN_ON(wc->wr_cqe->done != rtrs_clt_rdma_done);\n\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F) {\n\t\t\tif (wc->wc_flags & IB_WC_WITH_INVALIDATE)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\n\t\t\treturn  rtrs_clt_rkey_rsp_done(con, wc);\n\t\t}\n\t\tbreak;\n\tcase IB_WC_RDMA_WRITE:\n\t\t/*\n\t\t * post_send() RDMA write completions of IO reqs (read/write)\n\t\t * and hb.\n\t\t */\n\t\tbreak;\n\n\tdefault:\n\t\trtrs_wrn(clt_path->clt, \"Unexpected WC type: %d\\n\", wc->opcode);\n\t\treturn;\n\t}\n}\n\nstatic int post_recv_io(struct rtrs_clt_con *con, size_t q_size)\n{\n\tint err, i;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tfor (i = 0; i < q_size; i++) {\n\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F) {\n\t\t\tstruct rtrs_iu *iu = &con->rsp_ius[i];\n\n\t\t\terr = rtrs_iu_post_recv(&con->c, iu);\n\t\t} else {\n\t\t\terr = rtrs_post_recv_empty(&con->c, &io_comp_cqe);\n\t\t}\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int post_recv_path(struct rtrs_clt_path *clt_path)\n{\n\tsize_t q_size = 0;\n\tint err, cid;\n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (cid == 0)\n\t\t\tq_size = SERVICE_CON_QUEUE_DEPTH;\n\t\telse\n\t\t\tq_size = clt_path->queue_depth;\n\n\t\t/*\n\t\t * x2 for RDMA read responses + FR key invalidations,\n\t\t * RDMA writes do not require any FR registrations.\n\t\t */\n\t\tq_size *= 2;\n\n\t\terr = post_recv_io(to_clt_con(clt_path->s.con[cid]), q_size);\n\t\tif (err) {\n\t\t\trtrs_err(clt_path->clt, \"post_recv_io(), err: %d\\n\",\n\t\t\t\t err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstruct path_it {\n\tint i;\n\tstruct list_head skip_list;\n\tstruct rtrs_clt_sess *clt;\n\tstruct rtrs_clt_path *(*next_path)(struct path_it *it);\n};\n\n/**\n * list_next_or_null_rr_rcu - get next list element in round-robin fashion.\n * @head:\tthe head for the list.\n * @ptr:        the list head to take the next element from.\n * @type:       the type of the struct this is embedded in.\n * @memb:       the name of the list_head within the struct.\n *\n * Next element returned in round-robin fashion, i.e. head will be skipped,\n * but if list is observed as empty, NULL will be returned.\n *\n * This primitive may safely run concurrently with the _rcu list-mutation\n * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().\n */\n#define list_next_or_null_rr_rcu(head, ptr, type, memb) \\\n({ \\\n\tlist_next_or_null_rcu(head, ptr, type, memb) ?: \\\n\t\tlist_next_or_null_rcu(head, READ_ONCE((ptr)->next), \\\n\t\t\t\t      type, memb); \\\n})\n\n/**\n * get_next_path_rr() - Returns path in round-robin fashion.\n * @it:\tthe path pointer\n *\n * Related to @MP_POLICY_RR\n *\n * Locks:\n *    rcu_read_lock() must be hold.\n */\nstatic struct rtrs_clt_path *get_next_path_rr(struct path_it *it)\n{\n\tstruct rtrs_clt_path __rcu **ppcpu_path;\n\tstruct rtrs_clt_path *path;\n\tstruct rtrs_clt_sess *clt;\n\n\tclt = it->clt;\n\n\t/*\n\t * Here we use two RCU objects: @paths_list and @pcpu_path\n\t * pointer.  See rtrs_clt_remove_path_from_arr() for details\n\t * how that is handled.\n\t */\n\n\tppcpu_path = this_cpu_ptr(clt->pcpu_path);\n\tpath = rcu_dereference(*ppcpu_path);\n\tif (!path)\n\t\tpath = list_first_or_null_rcu(&clt->paths_list,\n\t\t\t\t\t      typeof(*path), s.entry);\n\telse\n\t\tpath = list_next_or_null_rr_rcu(&clt->paths_list,\n\t\t\t\t\t\t&path->s.entry,\n\t\t\t\t\t\ttypeof(*path),\n\t\t\t\t\t\ts.entry);\n\trcu_assign_pointer(*ppcpu_path, path);\n\n\treturn path;\n}\n\n/**\n * get_next_path_min_inflight() - Returns path with minimal inflight count.\n * @it:\tthe path pointer\n *\n * Related to @MP_POLICY_MIN_INFLIGHT\n *\n * Locks:\n *    rcu_read_lock() must be hold.\n */\nstatic struct rtrs_clt_path *get_next_path_min_inflight(struct path_it *it)\n{\n\tstruct rtrs_clt_path *min_path = NULL;\n\tstruct rtrs_clt_sess *clt = it->clt;\n\tstruct rtrs_clt_path *clt_path;\n\tint min_inflight = INT_MAX;\n\tint inflight;\n\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (!list_empty(raw_cpu_ptr(clt_path->mp_skip_entry)))\n\t\t\tcontinue;\n\n\t\tinflight = atomic_read(&clt_path->stats->inflight);\n\n\t\tif (inflight < min_inflight) {\n\t\t\tmin_inflight = inflight;\n\t\t\tmin_path = clt_path;\n\t\t}\n\t}\n\n\t/*\n\t * add the path to the skip list, so that next time we can get\n\t * a different one\n\t */\n\tif (min_path)\n\t\tlist_add(raw_cpu_ptr(min_path->mp_skip_entry), &it->skip_list);\n\n\treturn min_path;\n}\n\n/**\n * get_next_path_min_latency() - Returns path with minimal latency.\n * @it:\tthe path pointer\n *\n * Return: a path with the lowest latency or NULL if all paths are tried\n *\n * Locks:\n *    rcu_read_lock() must be hold.\n *\n * Related to @MP_POLICY_MIN_LATENCY\n *\n * This DOES skip an already-tried path.\n * There is a skip-list to skip a path if the path has tried but failed.\n * It will try the minimum latency path and then the second minimum latency\n * path and so on. Finally it will return NULL if all paths are tried.\n * Therefore the caller MUST check the returned\n * path is NULL and trigger the IO error.\n */\nstatic struct rtrs_clt_path *get_next_path_min_latency(struct path_it *it)\n{\n\tstruct rtrs_clt_path *min_path = NULL;\n\tstruct rtrs_clt_sess *clt = it->clt;\n\tstruct rtrs_clt_path *clt_path;\n\tktime_t min_latency = KTIME_MAX;\n\tktime_t latency;\n\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (!list_empty(raw_cpu_ptr(clt_path->mp_skip_entry)))\n\t\t\tcontinue;\n\n\t\tlatency = clt_path->s.hb_cur_latency;\n\n\t\tif (latency < min_latency) {\n\t\t\tmin_latency = latency;\n\t\t\tmin_path = clt_path;\n\t\t}\n\t}\n\n\t/*\n\t * add the path to the skip list, so that next time we can get\n\t * a different one\n\t */\n\tif (min_path)\n\t\tlist_add(raw_cpu_ptr(min_path->mp_skip_entry), &it->skip_list);\n\n\treturn min_path;\n}\n\nstatic inline void path_it_init(struct path_it *it, struct rtrs_clt_sess *clt)\n{\n\tINIT_LIST_HEAD(&it->skip_list);\n\tit->clt = clt;\n\tit->i = 0;\n\n\tif (clt->mp_policy == MP_POLICY_RR)\n\t\tit->next_path = get_next_path_rr;\n\telse if (clt->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\tit->next_path = get_next_path_min_inflight;\n\telse\n\t\tit->next_path = get_next_path_min_latency;\n}\n\nstatic inline void path_it_deinit(struct path_it *it)\n{\n\tstruct list_head *skip, *tmp;\n\t/*\n\t * The skip_list is used only for the MIN_INFLIGHT policy.\n\t * We need to remove paths from it, so that next IO can insert\n\t * paths (->mp_skip_entry) into a skip_list again.\n\t */\n\tlist_for_each_safe(skip, tmp, &it->skip_list)\n\t\tlist_del_init(skip);\n}\n\n/**\n * rtrs_clt_init_req() - Initialize an rtrs_clt_io_req holding information\n * about an inflight IO.\n * The user buffer holding user control message (not data) is copied into\n * the corresponding buffer of rtrs_iu (req->iu->buf), which later on will\n * also hold the control message of rtrs.\n * @req: an io request holding information about IO.\n * @clt_path: client path\n * @conf: conformation callback function to notify upper layer.\n * @permit: permit for allocation of RDMA remote buffer\n * @priv: private pointer\n * @vec: kernel vector containing control message\n * @usr_len: length of the user message\n * @sg: scater list for IO data\n * @sg_cnt: number of scater list entries\n * @data_len: length of the IO data\n * @dir: direction of the IO.\n */\nstatic void rtrs_clt_init_req(struct rtrs_clt_io_req *req,\n\t\t\t      struct rtrs_clt_path *clt_path,\n\t\t\t      void (*conf)(void *priv, int errno),\n\t\t\t      struct rtrs_permit *permit, void *priv,\n\t\t\t      const struct kvec *vec, size_t usr_len,\n\t\t\t      struct scatterlist *sg, size_t sg_cnt,\n\t\t\t      size_t data_len, int dir)\n{\n\tstruct iov_iter iter;\n\tsize_t len;\n\n\treq->permit = permit;\n\treq->in_use = true;\n\treq->usr_len = usr_len;\n\treq->data_len = data_len;\n\treq->sglist = sg;\n\treq->sg_cnt = sg_cnt;\n\treq->priv = priv;\n\treq->dir = dir;\n\treq->con = rtrs_permit_to_clt_con(clt_path, permit);\n\treq->conf = conf;\n\treq->need_inv = false;\n\treq->need_inv_comp = false;\n\treq->inv_errno = 0;\n\trefcount_set(&req->ref, 1);\n\treq->mp_policy = clt_path->clt->mp_policy;\n\n\tiov_iter_kvec(&iter, READ, vec, 1, usr_len);\n\tlen = _copy_from_iter(req->iu->buf, usr_len, &iter);\n\tWARN_ON(len != usr_len);\n\n\treinit_completion(&req->inv_comp);\n}\n\nstatic struct rtrs_clt_io_req *\nrtrs_clt_get_req(struct rtrs_clt_path *clt_path,\n\t\t void (*conf)(void *priv, int errno),\n\t\t struct rtrs_permit *permit, void *priv,\n\t\t const struct kvec *vec, size_t usr_len,\n\t\t struct scatterlist *sg, size_t sg_cnt,\n\t\t size_t data_len, int dir)\n{\n\tstruct rtrs_clt_io_req *req;\n\n\treq = &clt_path->reqs[permit->mem_id];\n\trtrs_clt_init_req(req, clt_path, conf, permit, priv, vec, usr_len,\n\t\t\t   sg, sg_cnt, data_len, dir);\n\treturn req;\n}\n\nstatic struct rtrs_clt_io_req *\nrtrs_clt_get_copy_req(struct rtrs_clt_path *alive_path,\n\t\t       struct rtrs_clt_io_req *fail_req)\n{\n\tstruct rtrs_clt_io_req *req;\n\tstruct kvec vec = {\n\t\t.iov_base = fail_req->iu->buf,\n\t\t.iov_len  = fail_req->usr_len\n\t};\n\n\treq = &alive_path->reqs[fail_req->permit->mem_id];\n\trtrs_clt_init_req(req, alive_path, fail_req->conf, fail_req->permit,\n\t\t\t   fail_req->priv, &vec, fail_req->usr_len,\n\t\t\t   fail_req->sglist, fail_req->sg_cnt,\n\t\t\t   fail_req->data_len, fail_req->dir);\n\treturn req;\n}\n\nstatic int rtrs_post_rdma_write_sg(struct rtrs_clt_con *con,\n\t\t\t\t   struct rtrs_clt_io_req *req,\n\t\t\t\t   struct rtrs_rbuf *rbuf, bool fr_en,\n\t\t\t\t   u32 size, u32 imm, struct ib_send_wr *wr,\n\t\t\t\t   struct ib_send_wr *tail)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct ib_sge *sge = req->sge;\n\tenum ib_send_flags flags;\n\tstruct scatterlist *sg;\n\tsize_t num_sge;\n\tint i;\n\tstruct ib_send_wr *ptail = NULL;\n\n\tif (fr_en) {\n\t\ti = 0;\n\t\tsge[i].addr   = req->mr->iova;\n\t\tsge[i].length = req->mr->length;\n\t\tsge[i].lkey   = req->mr->lkey;\n\t\ti++;\n\t\tnum_sge = 2;\n\t\tptail = tail;\n\t} else {\n\t\tfor_each_sg(req->sglist, sg, req->sg_cnt, i) {\n\t\t\tsge[i].addr   = sg_dma_address(sg);\n\t\t\tsge[i].length = sg_dma_len(sg);\n\t\t\tsge[i].lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\t\t}\n\t\tnum_sge = 1 + req->sg_cnt;\n\t}\n\tsge[i].addr   = req->iu->dma_addr;\n\tsge[i].length = size;\n\tsge[i].lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\n\t/*\n\t * From time to time we have to post signalled sends,\n\t * or send queue will fill up and only QP reset can help.\n\t */\n\tflags = atomic_inc_return(&con->c.wr_cnt) % clt_path->s.signal_interval ?\n\t\t\t0 : IB_SEND_SIGNALED;\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      req->iu->dma_addr,\n\t\t\t\t      size, DMA_TO_DEVICE);\n\n\treturn rtrs_iu_post_rdma_write_imm(&con->c, req->iu, sge, num_sge,\n\t\t\t\t\t    rbuf->rkey, rbuf->addr, imm,\n\t\t\t\t\t    flags, wr, ptail);\n}\n\nstatic int rtrs_map_sg_fr(struct rtrs_clt_io_req *req, size_t count)\n{\n\tint nr;\n\n\t/* Align the MR to a 4K page size to match the block virt boundary */\n\tnr = ib_map_mr_sg(req->mr, req->sglist, count, NULL, SZ_4K);\n\tif (nr < 0)\n\t\treturn nr;\n\tif (nr < req->sg_cnt)\n\t\treturn -EINVAL;\n\tib_update_fast_reg_key(req->mr, ib_inc_rkey(req->mr->rkey));\n\n\treturn nr;\n}\n\nstatic int rtrs_clt_write_req(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rtrs_msg_rdma_write *msg;\n\n\tstruct rtrs_rbuf *rbuf;\n\tint ret, count = 0;\n\tu32 imm, buf_id;\n\tstruct ib_reg_wr rwr;\n\tstruct ib_send_wr inv_wr;\n\tstruct ib_send_wr *wr = NULL;\n\tbool fr_en = false;\n\n\tconst size_t tsize = sizeof(*msg) + req->data_len + req->usr_len;\n\n\tif (tsize > clt_path->chunk_size) {\n\t\trtrs_wrn(s, \"Write request failed, size too big %zu > %d\\n\",\n\t\t\t  tsize, clt_path->chunk_size);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (req->sg_cnt) {\n\t\tcount = ib_dma_map_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t      req->sg_cnt, req->dir);\n\t\tif (!count) {\n\t\t\trtrs_wrn(s, \"Write request failed, map failed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\t/* put rtrs msg after sg and user message */\n\tmsg = req->iu->buf + req->usr_len;\n\tmsg->type = cpu_to_le16(RTRS_MSG_WRITE);\n\tmsg->usr_len = cpu_to_le16(req->usr_len);\n\n\t/* rtrs message on server side will be after user data and message */\n\timm = req->permit->mem_off + req->data_len + req->usr_len;\n\timm = rtrs_to_io_req_imm(imm);\n\tbuf_id = req->permit->mem_id;\n\treq->sg_size = tsize;\n\trbuf = &clt_path->rbufs[buf_id];\n\n\tif (count) {\n\t\tret = rtrs_map_sg_fr(req, count);\n\t\tif (ret < 0) {\n\t\t\trtrs_err_rl(s,\n\t\t\t\t    \"Write request failed, failed to map fast reg. data, err: %d\\n\",\n\t\t\t\t    ret);\n\t\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t\t\treturn ret;\n\t\t}\n\t\tinv_wr = (struct ib_send_wr) {\n\t\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t\t.wr_cqe\t\t    = &req->inv_cqe,\n\t\t\t.send_flags\t    = IB_SEND_SIGNALED,\n\t\t\t.ex.invalidate_rkey = req->mr->rkey,\n\t\t};\n\t\treq->inv_cqe.done = rtrs_clt_inv_rkey_done;\n\t\trwr = (struct ib_reg_wr) {\n\t\t\t.wr.opcode = IB_WR_REG_MR,\n\t\t\t.wr.wr_cqe = &fast_reg_cqe,\n\t\t\t.mr = req->mr,\n\t\t\t.key = req->mr->rkey,\n\t\t\t.access = (IB_ACCESS_LOCAL_WRITE),\n\t\t};\n\t\twr = &rwr.wr;\n\t\tfr_en = true;\n\t\trefcount_inc(&req->ref);\n\t}\n\t/*\n\t * Update stats now, after request is successfully sent it is not\n\t * safe anymore to touch it.\n\t */\n\trtrs_clt_update_all_stats(req, WRITE);\n\n\tret = rtrs_post_rdma_write_sg(req->con, req, rbuf, fr_en,\n\t\t\t\t      req->usr_len + sizeof(*msg),\n\t\t\t\t      imm, wr, &inv_wr);\n\tif (ret) {\n\t\trtrs_err_rl(s,\n\t\t\t    \"Write request failed: error=%d path=%s [%s:%u]\\n\",\n\t\t\t    ret, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port);\n\t\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\t\tatomic_dec(&clt_path->stats->inflight);\n\t\tif (req->sg_cnt)\n\t\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\n\treturn ret;\n}\n\nstatic int rtrs_clt_read_req(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rtrs_msg_rdma_read *msg;\n\tstruct rtrs_ib_dev *dev = clt_path->s.dev;\n\n\tstruct ib_reg_wr rwr;\n\tstruct ib_send_wr *wr = NULL;\n\n\tint ret, count = 0;\n\tu32 imm, buf_id;\n\n\tconst size_t tsize = sizeof(*msg) + req->data_len + req->usr_len;\n\n\tif (tsize > clt_path->chunk_size) {\n\t\trtrs_wrn(s,\n\t\t\t  \"Read request failed, message size is %zu, bigger than CHUNK_SIZE %d\\n\",\n\t\t\t  tsize, clt_path->chunk_size);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (req->sg_cnt) {\n\t\tcount = ib_dma_map_sg(dev->ib_dev, req->sglist, req->sg_cnt,\n\t\t\t\t      req->dir);\n\t\tif (!count) {\n\t\t\trtrs_wrn(s,\n\t\t\t\t  \"Read request failed, dma map failed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\t/* put our message into req->buf after user message*/\n\tmsg = req->iu->buf + req->usr_len;\n\tmsg->type = cpu_to_le16(RTRS_MSG_READ);\n\tmsg->usr_len = cpu_to_le16(req->usr_len);\n\n\tif (count) {\n\t\tret = rtrs_map_sg_fr(req, count);\n\t\tif (ret < 0) {\n\t\t\trtrs_err_rl(s,\n\t\t\t\t     \"Read request failed, failed to map  fast reg. data, err: %d\\n\",\n\t\t\t\t     ret);\n\t\t\tib_dma_unmap_sg(dev->ib_dev, req->sglist, req->sg_cnt,\n\t\t\t\t\treq->dir);\n\t\t\treturn ret;\n\t\t}\n\t\trwr = (struct ib_reg_wr) {\n\t\t\t.wr.opcode = IB_WR_REG_MR,\n\t\t\t.wr.wr_cqe = &fast_reg_cqe,\n\t\t\t.mr = req->mr,\n\t\t\t.key = req->mr->rkey,\n\t\t\t.access = (IB_ACCESS_LOCAL_WRITE |\n\t\t\t\t   IB_ACCESS_REMOTE_WRITE),\n\t\t};\n\t\twr = &rwr.wr;\n\n\t\tmsg->sg_cnt = cpu_to_le16(1);\n\t\tmsg->flags = cpu_to_le16(RTRS_MSG_NEED_INVAL_F);\n\n\t\tmsg->desc[0].addr = cpu_to_le64(req->mr->iova);\n\t\tmsg->desc[0].key = cpu_to_le32(req->mr->rkey);\n\t\tmsg->desc[0].len = cpu_to_le32(req->mr->length);\n\n\t\t/* Further invalidation is required */\n\t\treq->need_inv = !!RTRS_MSG_NEED_INVAL_F;\n\n\t} else {\n\t\tmsg->sg_cnt = 0;\n\t\tmsg->flags = 0;\n\t}\n\t/*\n\t * rtrs message will be after the space reserved for disk data and\n\t * user message\n\t */\n\timm = req->permit->mem_off + req->data_len + req->usr_len;\n\timm = rtrs_to_io_req_imm(imm);\n\tbuf_id = req->permit->mem_id;\n\n\treq->sg_size  = sizeof(*msg);\n\treq->sg_size += le16_to_cpu(msg->sg_cnt) * sizeof(struct rtrs_sg_desc);\n\treq->sg_size += req->usr_len;\n\n\t/*\n\t * Update stats now, after request is successfully sent it is not\n\t * safe anymore to touch it.\n\t */\n\trtrs_clt_update_all_stats(req, READ);\n\n\tret = rtrs_post_send_rdma(req->con, req, &clt_path->rbufs[buf_id],\n\t\t\t\t   req->data_len, imm, wr);\n\tif (ret) {\n\t\trtrs_err_rl(s,\n\t\t\t    \"Read request failed: error=%d path=%s [%s:%u]\\n\",\n\t\t\t    ret, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port);\n\t\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\t\tatomic_dec(&clt_path->stats->inflight);\n\t\treq->need_inv = false;\n\t\tif (req->sg_cnt)\n\t\t\tib_dma_unmap_sg(dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\n\treturn ret;\n}\n\n/**\n * rtrs_clt_failover_req() - Try to find an active path for a failed request\n * @clt: clt context\n * @fail_req: a failed io request.\n */\nstatic int rtrs_clt_failover_req(struct rtrs_clt_sess *clt,\n\t\t\t\t struct rtrs_clt_io_req *fail_req)\n{\n\tstruct rtrs_clt_path *alive_path;\n\tstruct rtrs_clt_io_req *req;\n\tint err = -ECONNABORTED;\n\tstruct path_it it;\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (alive_path = it.next_path(&it)) && it.i < it.clt->paths_num;\n\t     it.i++) {\n\t\tif (READ_ONCE(alive_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\t\treq = rtrs_clt_get_copy_req(alive_path, fail_req);\n\t\tif (req->dir == DMA_TO_DEVICE)\n\t\t\terr = rtrs_clt_write_req(req);\n\t\telse\n\t\t\terr = rtrs_clt_read_req(req);\n\t\tif (err) {\n\t\t\treq->in_use = false;\n\t\t\tcontinue;\n\t\t}\n\t\t/* Success path */\n\t\trtrs_clt_inc_failover_cnt(alive_path->stats);\n\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic void fail_all_outstanding_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_clt_io_req *req;\n\tint i, err;\n\n\tif (!clt_path->reqs)\n\t\treturn;\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\tif (!req->in_use)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Safely (without notification) complete failed request.\n\t\t * After completion this request is still useble and can\n\t\t * be failovered to another path.\n\t\t */\n\t\tcomplete_rdma_req(req, -ECONNABORTED, false, true);\n\n\t\terr = rtrs_clt_failover_req(clt, req);\n\t\tif (err)\n\t\t\t/* Failover failed, notify anyway */\n\t\t\treq->conf(req->priv, err);\n\t}\n}\n\nstatic void free_path_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_io_req *req;\n\tint i;\n\n\tif (!clt_path->reqs)\n\t\treturn;\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\tif (req->mr)\n\t\t\tib_dereg_mr(req->mr);\n\t\tkfree(req->sge);\n\t\trtrs_iu_free(req->iu, clt_path->s.dev->ib_dev, 1);\n\t}\n\tkfree(clt_path->reqs);\n\tclt_path->reqs = NULL;\n}\n\nstatic int alloc_path_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_io_req *req;\n\tint i, err = -ENOMEM;\n\n\tclt_path->reqs = kcalloc(clt_path->queue_depth,\n\t\t\t\t sizeof(*clt_path->reqs),\n\t\t\t\t GFP_KERNEL);\n\tif (!clt_path->reqs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\treq->iu = rtrs_iu_alloc(1, clt_path->max_hdr_size, GFP_KERNEL,\n\t\t\t\t\t clt_path->s.dev->ib_dev,\n\t\t\t\t\t DMA_TO_DEVICE,\n\t\t\t\t\t rtrs_clt_rdma_done);\n\t\tif (!req->iu)\n\t\t\tgoto out;\n\n\t\treq->sge = kcalloc(2, sizeof(*req->sge), GFP_KERNEL);\n\t\tif (!req->sge)\n\t\t\tgoto out;\n\n\t\treq->mr = ib_alloc_mr(clt_path->s.dev->ib_pd,\n\t\t\t\t      IB_MR_TYPE_MEM_REG,\n\t\t\t\t      clt_path->max_pages_per_mr);\n\t\tif (IS_ERR(req->mr)) {\n\t\t\terr = PTR_ERR(req->mr);\n\t\t\treq->mr = NULL;\n\t\t\tpr_err(\"Failed to alloc clt_path->max_pages_per_mr %d\\n\",\n\t\t\t       clt_path->max_pages_per_mr);\n\t\t\tgoto out;\n\t\t}\n\n\t\tinit_completion(&req->inv_comp);\n\t}\n\n\treturn 0;\n\nout:\n\tfree_path_reqs(clt_path);\n\n\treturn err;\n}\n\nstatic int alloc_permits(struct rtrs_clt_sess *clt)\n{\n\tunsigned int chunk_bits;\n\tint err, i;\n\n\tclt->permits_map = kcalloc(BITS_TO_LONGS(clt->queue_depth),\n\t\t\t\t   sizeof(long), GFP_KERNEL);\n\tif (!clt->permits_map) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\tclt->permits = kcalloc(clt->queue_depth, permit_size(clt), GFP_KERNEL);\n\tif (!clt->permits) {\n\t\terr = -ENOMEM;\n\t\tgoto err_map;\n\t}\n\tchunk_bits = ilog2(clt->queue_depth - 1) + 1;\n\tfor (i = 0; i < clt->queue_depth; i++) {\n\t\tstruct rtrs_permit *permit;\n\n\t\tpermit = get_permit(clt, i);\n\t\tpermit->mem_id = i;\n\t\tpermit->mem_off = i << (MAX_IMM_PAYL_BITS - chunk_bits);\n\t}\n\n\treturn 0;\n\nerr_map:\n\tkfree(clt->permits_map);\n\tclt->permits_map = NULL;\nout_err:\n\treturn err;\n}\n\nstatic void free_permits(struct rtrs_clt_sess *clt)\n{\n\tif (clt->permits_map) {\n\t\tsize_t sz = clt->queue_depth;\n\n\t\twait_event(clt->permits_wait,\n\t\t\t   find_first_bit(clt->permits_map, sz) >= sz);\n\t}\n\tkfree(clt->permits_map);\n\tclt->permits_map = NULL;\n\tkfree(clt->permits);\n\tclt->permits = NULL;\n}\n\nstatic void query_fast_reg_mode(struct rtrs_clt_path *clt_path)\n{\n\tstruct ib_device *ib_dev;\n\tu64 max_pages_per_mr;\n\tint mr_page_shift;\n\n\tib_dev = clt_path->s.dev->ib_dev;\n\n\t/*\n\t * Use the smallest page size supported by the HCA, down to a\n\t * minimum of 4096 bytes. We're unlikely to build large sglists\n\t * out of smaller entries.\n\t */\n\tmr_page_shift      = max(12, ffs(ib_dev->attrs.page_size_cap) - 1);\n\tmax_pages_per_mr   = ib_dev->attrs.max_mr_size;\n\tdo_div(max_pages_per_mr, (1ull << mr_page_shift));\n\tclt_path->max_pages_per_mr =\n\t\tmin3(clt_path->max_pages_per_mr, (u32)max_pages_per_mr,\n\t\t     ib_dev->attrs.max_fast_reg_page_list_len);\n\tclt_path->clt->max_segments =\n\t\tmin(clt_path->max_pages_per_mr, clt_path->clt->max_segments);\n}\n\nstatic bool rtrs_clt_change_state_get_old(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t   enum rtrs_clt_state new_state,\n\t\t\t\t\t   enum rtrs_clt_state *old_state)\n{\n\tbool changed;\n\n\tspin_lock_irq(&clt_path->state_wq.lock);\n\tif (old_state)\n\t\t*old_state = clt_path->state;\n\tchanged = rtrs_clt_change_state(clt_path, new_state);\n\tspin_unlock_irq(&clt_path->state_wq.lock);\n\n\treturn changed;\n}\n\nstatic void rtrs_clt_hb_err_handler(struct rtrs_con *c)\n{\n\tstruct rtrs_clt_con *con = container_of(c, typeof(*con), c);\n\n\trtrs_rdma_error_recovery(con);\n}\n\nstatic void rtrs_clt_init_hb(struct rtrs_clt_path *clt_path)\n{\n\trtrs_init_hb(&clt_path->s, &io_comp_cqe,\n\t\t      RTRS_HB_INTERVAL_MS,\n\t\t      RTRS_HB_MISSED_MAX,\n\t\t      rtrs_clt_hb_err_handler,\n\t\t      rtrs_wq);\n}\n\nstatic void rtrs_clt_reconnect_work(struct work_struct *work);\nstatic void rtrs_clt_close_work(struct work_struct *work);\n\nstatic struct rtrs_clt_path *alloc_path(struct rtrs_clt_sess *clt,\n\t\t\t\t\tconst struct rtrs_addr *path,\n\t\t\t\t\tsize_t con_num, u32 nr_poll_queues)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tint err = -ENOMEM;\n\tint cpu;\n\tsize_t total_con;\n\n\tclt_path = kzalloc(sizeof(*clt_path), GFP_KERNEL);\n\tif (!clt_path)\n\t\tgoto err;\n\n\t/*\n\t * irqmode and poll\n\t * +1: Extra connection for user messages\n\t */\n\ttotal_con = con_num + nr_poll_queues + 1;\n\tclt_path->s.con = kcalloc(total_con, sizeof(*clt_path->s.con),\n\t\t\t\t  GFP_KERNEL);\n\tif (!clt_path->s.con)\n\t\tgoto err_free_path;\n\n\tclt_path->s.con_num = total_con;\n\tclt_path->s.irq_con_num = con_num + 1;\n\n\tclt_path->stats = kzalloc(sizeof(*clt_path->stats), GFP_KERNEL);\n\tif (!clt_path->stats)\n\t\tgoto err_free_con;\n\n\tmutex_init(&clt_path->init_mutex);\n\tuuid_gen(&clt_path->s.uuid);\n\tmemcpy(&clt_path->s.dst_addr, path->dst,\n\t       rdma_addr_size((struct sockaddr *)path->dst));\n\n\t/*\n\t * rdma_resolve_addr() passes src_addr to cma_bind_addr, which\n\t * checks the sa_family to be non-zero. If user passed src_addr=NULL\n\t * the sess->src_addr will contain only zeros, which is then fine.\n\t */\n\tif (path->src)\n\t\tmemcpy(&clt_path->s.src_addr, path->src,\n\t\t       rdma_addr_size((struct sockaddr *)path->src));\n\tstrscpy(clt_path->s.sessname, clt->sessname,\n\t\tsizeof(clt_path->s.sessname));\n\tclt_path->clt = clt;\n\tclt_path->max_pages_per_mr = RTRS_MAX_SEGMENTS;\n\tinit_waitqueue_head(&clt_path->state_wq);\n\tclt_path->state = RTRS_CLT_CONNECTING;\n\tatomic_set(&clt_path->connected_cnt, 0);\n\tINIT_WORK(&clt_path->close_work, rtrs_clt_close_work);\n\tINIT_DELAYED_WORK(&clt_path->reconnect_dwork, rtrs_clt_reconnect_work);\n\trtrs_clt_init_hb(clt_path);\n\n\tclt_path->mp_skip_entry = alloc_percpu(typeof(*clt_path->mp_skip_entry));\n\tif (!clt_path->mp_skip_entry)\n\t\tgoto err_free_stats;\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_LIST_HEAD(per_cpu_ptr(clt_path->mp_skip_entry, cpu));\n\n\terr = rtrs_clt_init_stats(clt_path->stats);\n\tif (err)\n\t\tgoto err_free_percpu;\n\n\treturn clt_path;\n\nerr_free_percpu:\n\tfree_percpu(clt_path->mp_skip_entry);\nerr_free_stats:\n\tkfree(clt_path->stats);\nerr_free_con:\n\tkfree(clt_path->s.con);\nerr_free_path:\n\tkfree(clt_path);\nerr:\n\treturn ERR_PTR(err);\n}\n\nvoid free_path(struct rtrs_clt_path *clt_path)\n{\n\tfree_percpu(clt_path->mp_skip_entry);\n\tmutex_destroy(&clt_path->init_mutex);\n\tkfree(clt_path->s.con);\n\tkfree(clt_path->rbufs);\n\tkfree(clt_path);\n}\n\nstatic int create_con(struct rtrs_clt_path *clt_path, unsigned int cid)\n{\n\tstruct rtrs_clt_con *con;\n\n\tcon = kzalloc(sizeof(*con), GFP_KERNEL);\n\tif (!con)\n\t\treturn -ENOMEM;\n\n\t/* Map first two connections to the first CPU */\n\tcon->cpu  = (cid ? cid - 1 : 0) % nr_cpu_ids;\n\tcon->c.cid = cid;\n\tcon->c.path = &clt_path->s;\n\t/* Align with srv, init as 1 */\n\tatomic_set(&con->c.wr_cnt, 1);\n\tmutex_init(&con->con_mutex);\n\n\tclt_path->s.con[cid] = &con->c;\n\n\treturn 0;\n}\n\nstatic void destroy_con(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tclt_path->s.con[con->c.cid] = NULL;\n\tmutex_destroy(&con->con_mutex);\n\tkfree(con);\n}\n\nstatic int create_con_cq_qp(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tu32 max_send_wr, max_recv_wr, cq_num, max_send_sge, wr_limit;\n\tint err, cq_vector;\n\tstruct rtrs_msg_rkey_rsp *rsp;\n\n\tlockdep_assert_held(&con->con_mutex);\n\tif (con->c.cid == 0) {\n\t\tmax_send_sge = 1;\n\t\t/* We must be the first here */\n\t\tif (WARN_ON(clt_path->s.dev))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * The whole session uses device from user connection.\n\t\t * Be careful not to close user connection before ib dev\n\t\t * is gracefully put.\n\t\t */\n\t\tclt_path->s.dev = rtrs_ib_dev_find_or_add(con->c.cm_id->device,\n\t\t\t\t\t\t       &dev_pd);\n\t\tif (!clt_path->s.dev) {\n\t\t\trtrs_wrn(clt_path->clt,\n\t\t\t\t  \"rtrs_ib_dev_find_get_or_add(): no memory\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tclt_path->s.dev_ref = 1;\n\t\tquery_fast_reg_mode(clt_path);\n\t\twr_limit = clt_path->s.dev->ib_dev->attrs.max_qp_wr;\n\t\t/*\n\t\t * Two (request + registration) completion for send\n\t\t * Two for recv if always_invalidate is set on server\n\t\t * or one for recv.\n\t\t * + 2 for drain and heartbeat\n\t\t * in case qp gets into error state.\n\t\t */\n\t\tmax_send_wr =\n\t\t\tmin_t(int, wr_limit, SERVICE_CON_QUEUE_DEPTH * 2 + 2);\n\t\tmax_recv_wr = max_send_wr;\n\t} else {\n\t\t/*\n\t\t * Here we assume that session members are correctly set.\n\t\t * This is always true if user connection (cid == 0) is\n\t\t * established first.\n\t\t */\n\t\tif (WARN_ON(!clt_path->s.dev))\n\t\t\treturn -EINVAL;\n\t\tif (WARN_ON(!clt_path->queue_depth))\n\t\t\treturn -EINVAL;\n\n\t\twr_limit = clt_path->s.dev->ib_dev->attrs.max_qp_wr;\n\t\t/* Shared between connections */\n\t\tclt_path->s.dev_ref++;\n\t\tmax_send_wr = min_t(int, wr_limit,\n\t\t\t      /* QD * (REQ + RSP + FR REGS or INVS) + drain */\n\t\t\t      clt_path->queue_depth * 3 + 1);\n\t\tmax_recv_wr = min_t(int, wr_limit,\n\t\t\t      clt_path->queue_depth * 3 + 1);\n\t\tmax_send_sge = 2;\n\t}\n\tatomic_set(&con->c.sq_wr_avail, max_send_wr);\n\tcq_num = max_send_wr + max_recv_wr;\n\t/* alloc iu to recv new rkey reply when server reports flags set */\n\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F || con->c.cid == 0) {\n\t\tcon->rsp_ius = rtrs_iu_alloc(cq_num, sizeof(*rsp),\n\t\t\t\t\t      GFP_KERNEL,\n\t\t\t\t\t      clt_path->s.dev->ib_dev,\n\t\t\t\t\t      DMA_FROM_DEVICE,\n\t\t\t\t\t      rtrs_clt_rdma_done);\n\t\tif (!con->rsp_ius)\n\t\t\treturn -ENOMEM;\n\t\tcon->queue_num = cq_num;\n\t}\n\tcq_num = max_send_wr + max_recv_wr;\n\tcq_vector = con->cpu % clt_path->s.dev->ib_dev->num_comp_vectors;\n\tif (con->c.cid >= clt_path->s.irq_con_num)\n\t\terr = rtrs_cq_qp_create(&clt_path->s, &con->c, max_send_sge,\n\t\t\t\t\tcq_vector, cq_num, max_send_wr,\n\t\t\t\t\tmax_recv_wr, IB_POLL_DIRECT);\n\telse\n\t\terr = rtrs_cq_qp_create(&clt_path->s, &con->c, max_send_sge,\n\t\t\t\t\tcq_vector, cq_num, max_send_wr,\n\t\t\t\t\tmax_recv_wr, IB_POLL_SOFTIRQ);\n\t/*\n\t * In case of error we do not bother to clean previous allocations,\n\t * since destroy_con_cq_qp() must be called.\n\t */\n\treturn err;\n}\n\nstatic void destroy_con_cq_qp(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\t/*\n\t * Be careful here: destroy_con_cq_qp() can be called even\n\t * create_con_cq_qp() failed, see comments there.\n\t */\n\tlockdep_assert_held(&con->con_mutex);\n\trtrs_cq_qp_destroy(&con->c);\n\tif (con->rsp_ius) {\n\t\trtrs_iu_free(con->rsp_ius, clt_path->s.dev->ib_dev,\n\t\t\t     con->queue_num);\n\t\tcon->rsp_ius = NULL;\n\t\tcon->queue_num = 0;\n\t}\n\tif (clt_path->s.dev_ref && !--clt_path->s.dev_ref) {\n\t\trtrs_ib_dev_put(clt_path->s.dev);\n\t\tclt_path->s.dev = NULL;\n\t}\n}\n\nstatic void stop_cm(struct rtrs_clt_con *con)\n{\n\trdma_disconnect(con->c.cm_id);\n\tif (con->c.qp)\n\t\tib_drain_qp(con->c.qp);\n}\n\nstatic void destroy_cm(struct rtrs_clt_con *con)\n{\n\trdma_destroy_id(con->c.cm_id);\n\tcon->c.cm_id = NULL;\n}\n\nstatic int rtrs_rdma_addr_resolved(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tint err;\n\n\tmutex_lock(&con->con_mutex);\n\terr = create_con_cq_qp(con);\n\tmutex_unlock(&con->con_mutex);\n\tif (err) {\n\t\trtrs_err(s, \"create_con_cq_qp(), err: %d\\n\", err);\n\t\treturn err;\n\t}\n\terr = rdma_resolve_route(con->c.cm_id, RTRS_CONNECT_TIMEOUT_MS);\n\tif (err)\n\t\trtrs_err(s, \"Resolving route failed, err: %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int rtrs_rdma_route_resolved(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_msg_conn_req msg;\n\tstruct rdma_conn_param param;\n\n\tint err;\n\n\tparam = (struct rdma_conn_param) {\n\t\t.retry_count = 7,\n\t\t.rnr_retry_count = 7,\n\t\t.private_data = &msg,\n\t\t.private_data_len = sizeof(msg),\n\t};\n\n\tmsg = (struct rtrs_msg_conn_req) {\n\t\t.magic = cpu_to_le16(RTRS_MAGIC),\n\t\t.version = cpu_to_le16(RTRS_PROTO_VER),\n\t\t.cid = cpu_to_le16(con->c.cid),\n\t\t.cid_num = cpu_to_le16(clt_path->s.con_num),\n\t\t.recon_cnt = cpu_to_le16(clt_path->s.recon_cnt),\n\t};\n\tmsg.first_conn = clt_path->for_new_clt ? FIRST_CONN : 0;\n\tuuid_copy(&msg.sess_uuid, &clt_path->s.uuid);\n\tuuid_copy(&msg.paths_uuid, &clt->paths_uuid);\n\n\terr = rdma_connect_locked(con->c.cm_id, &param);\n\tif (err)\n\t\trtrs_err(clt, \"rdma_connect_locked(): %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int rtrs_rdma_conn_established(struct rtrs_clt_con *con,\n\t\t\t\t       struct rdma_cm_event *ev)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tconst struct rtrs_msg_conn_rsp *msg;\n\tu16 version, queue_depth;\n\tint errno;\n\tu8 len;\n\n\tmsg = ev->param.conn.private_data;\n\tlen = ev->param.conn.private_data_len;\n\tif (len < sizeof(*msg)) {\n\t\trtrs_err(clt, \"Invalid RTRS connection response\\n\");\n\t\treturn -ECONNRESET;\n\t}\n\tif (le16_to_cpu(msg->magic) != RTRS_MAGIC) {\n\t\trtrs_err(clt, \"Invalid RTRS magic\\n\");\n\t\treturn -ECONNRESET;\n\t}\n\tversion = le16_to_cpu(msg->version);\n\tif (version >> 8 != RTRS_PROTO_VER_MAJOR) {\n\t\trtrs_err(clt, \"Unsupported major RTRS version: %d, expected %d\\n\",\n\t\t\t  version >> 8, RTRS_PROTO_VER_MAJOR);\n\t\treturn -ECONNRESET;\n\t}\n\terrno = le16_to_cpu(msg->errno);\n\tif (errno) {\n\t\trtrs_err(clt, \"Invalid RTRS message: errno %d\\n\",\n\t\t\t  errno);\n\t\treturn -ECONNRESET;\n\t}\n\tif (con->c.cid == 0) {\n\t\tqueue_depth = le16_to_cpu(msg->queue_depth);\n\n\t\tif (clt_path->queue_depth > 0 && queue_depth != clt_path->queue_depth) {\n\t\t\trtrs_err(clt, \"Error: queue depth changed\\n\");\n\n\t\t\t/*\n\t\t\t * Stop any more reconnection attempts\n\t\t\t */\n\t\t\tclt_path->reconnect_attempts = -1;\n\t\t\trtrs_err(clt,\n\t\t\t\t\"Disabling auto-reconnect. Trigger a manual reconnect after issue is resolved\\n\");\n\t\t\treturn -ECONNRESET;\n\t\t}\n\n\t\tif (!clt_path->rbufs) {\n\t\t\tclt_path->rbufs = kcalloc(queue_depth,\n\t\t\t\t\t\t  sizeof(*clt_path->rbufs),\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\t\tif (!clt_path->rbufs)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tclt_path->queue_depth = queue_depth;\n\t\tclt_path->s.signal_interval = min_not_zero(queue_depth,\n\t\t\t\t\t\t(unsigned short) SERVICE_CON_QUEUE_DEPTH);\n\t\tclt_path->max_hdr_size = le32_to_cpu(msg->max_hdr_size);\n\t\tclt_path->max_io_size = le32_to_cpu(msg->max_io_size);\n\t\tclt_path->flags = le32_to_cpu(msg->flags);\n\t\tclt_path->chunk_size = clt_path->max_io_size + clt_path->max_hdr_size;\n\n\t\t/*\n\t\t * Global IO size is always a minimum.\n\t\t * If while a reconnection server sends us a value a bit\n\t\t * higher - client does not care and uses cached minimum.\n\t\t *\n\t\t * Since we can have several sessions (paths) restablishing\n\t\t * connections in parallel, use lock.\n\t\t */\n\t\tmutex_lock(&clt->paths_mutex);\n\t\tclt->queue_depth = clt_path->queue_depth;\n\t\tclt->max_io_size = min_not_zero(clt_path->max_io_size,\n\t\t\t\t\t\tclt->max_io_size);\n\t\tmutex_unlock(&clt->paths_mutex);\n\n\t\t/*\n\t\t * Cache the hca_port and hca_name for sysfs\n\t\t */\n\t\tclt_path->hca_port = con->c.cm_id->port_num;\n\t\tscnprintf(clt_path->hca_name, sizeof(clt_path->hca_name),\n\t\t\t  clt_path->s.dev->ib_dev->name);\n\t\tclt_path->s.src_addr = con->c.cm_id->route.addr.src_addr;\n\t\t/* set for_new_clt, to allow future reconnect on any path */\n\t\tclt_path->for_new_clt = 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void flag_success_on_conn(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tatomic_inc(&clt_path->connected_cnt);\n\tcon->cm_err = 1;\n}\n\nstatic int rtrs_rdma_conn_rejected(struct rtrs_clt_con *con,\n\t\t\t\t    struct rdma_cm_event *ev)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tconst struct rtrs_msg_conn_rsp *msg;\n\tconst char *rej_msg;\n\tint status, errno;\n\tu8 data_len;\n\n\tstatus = ev->status;\n\trej_msg = rdma_reject_msg(con->c.cm_id, status);\n\tmsg = rdma_consumer_reject_data(con->c.cm_id, ev, &data_len);\n\n\tif (msg && data_len >= sizeof(*msg)) {\n\t\terrno = (int16_t)le16_to_cpu(msg->errno);\n\t\tif (errno == -EBUSY)\n\t\t\trtrs_err(s,\n\t\t\t\t  \"Previous session is still exists on the server, please reconnect later\\n\");\n\t\telse\n\t\t\trtrs_err(s,\n\t\t\t\t  \"Connect rejected: status %d (%s), rtrs errno %d\\n\",\n\t\t\t\t  status, rej_msg, errno);\n\t} else {\n\t\trtrs_err(s,\n\t\t\t  \"Connect rejected but with malformed message: status %d (%s)\\n\",\n\t\t\t  status, rej_msg);\n\t}\n\n\treturn -ECONNRESET;\n}\n\nvoid rtrs_clt_close_conns(struct rtrs_clt_path *clt_path, bool wait)\n{\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CLOSING, NULL))\n\t\tqueue_work(rtrs_wq, &clt_path->close_work);\n\tif (wait)\n\t\tflush_work(&clt_path->close_work);\n}\n\nstatic inline void flag_error_on_conn(struct rtrs_clt_con *con, int cm_err)\n{\n\tif (con->cm_err == 1) {\n\t\tstruct rtrs_clt_path *clt_path;\n\n\t\tclt_path = to_clt_path(con->c.path);\n\t\tif (atomic_dec_and_test(&clt_path->connected_cnt))\n\n\t\t\twake_up(&clt_path->state_wq);\n\t}\n\tcon->cm_err = cm_err;\n}\n\nstatic int rtrs_clt_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\t\t\t     struct rdma_cm_event *ev)\n{\n\tstruct rtrs_clt_con *con = cm_id->context;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tint cm_err = 0;\n\n\tswitch (ev->event) {\n\tcase RDMA_CM_EVENT_ADDR_RESOLVED:\n\t\tcm_err = rtrs_rdma_addr_resolved(con);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ROUTE_RESOLVED:\n\t\tcm_err = rtrs_rdma_route_resolved(con);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\tcm_err = rtrs_rdma_conn_established(con, ev);\n\t\tif (!cm_err) {\n\t\t\t/*\n\t\t\t * Report success and wake up. Here we abuse state_wq,\n\t\t\t * i.e. wake up without state change, but we set cm_err.\n\t\t\t */\n\t\t\tflag_success_on_conn(con);\n\t\t\twake_up(&clt_path->state_wq);\n\t\t\treturn 0;\n\t\t}\n\t\tbreak;\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tcm_err = rtrs_rdma_conn_rejected(con, ev);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\t/* No message for disconnecting */\n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\tcase RDMA_CM_EVENT_ADDR_CHANGE:\n\tcase RDMA_CM_EVENT_TIMEWAIT_EXIT:\n\t\trtrs_wrn(s, \"CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ADDR_ERROR:\n\tcase RDMA_CM_EVENT_ROUTE_ERROR:\n\t\trtrs_wrn(s, \"CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -EHOSTUNREACH;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\t/*\n\t\t * Device removal is a special case.  Queue close and return 0.\n\t\t */\n\t\trtrs_clt_close_conns(clt_path, false);\n\t\treturn 0;\n\tdefault:\n\t\trtrs_err(s, \"Unexpected RDMA CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\t}\n\n\tif (cm_err) {\n\t\t/*\n\t\t * cm error makes sense only on connection establishing,\n\t\t * in other cases we rely on normal procedure of reconnecting.\n\t\t */\n\t\tflag_error_on_conn(con, cm_err);\n\t\trtrs_rdma_error_recovery(con);\n\t}\n\n\treturn 0;\n}\n\nstatic int create_cm(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rdma_cm_id *cm_id;\n\tint err;\n\n\tcm_id = rdma_create_id(&init_net, rtrs_clt_rdma_cm_handler, con,\n\t\t\t       clt_path->s.dst_addr.ss_family == AF_IB ?\n\t\t\t       RDMA_PS_IB : RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id)) {\n\t\terr = PTR_ERR(cm_id);\n\t\trtrs_err(s, \"Failed to create CM ID, err: %d\\n\", err);\n\n\t\treturn err;\n\t}\n\tcon->c.cm_id = cm_id;\n\tcon->cm_err = 0;\n\t/* allow the port to be reused */\n\terr = rdma_set_reuseaddr(cm_id, 1);\n\tif (err != 0) {\n\t\trtrs_err(s, \"Set address reuse failed, err: %d\\n\", err);\n\t\tgoto destroy_cm;\n\t}\n\terr = rdma_resolve_addr(cm_id, (struct sockaddr *)&clt_path->s.src_addr,\n\t\t\t\t(struct sockaddr *)&clt_path->s.dst_addr,\n\t\t\t\tRTRS_CONNECT_TIMEOUT_MS);\n\tif (err) {\n\t\trtrs_err(s, \"Failed to resolve address, err: %d\\n\", err);\n\t\tgoto destroy_cm;\n\t}\n\t/*\n\t * Combine connection status and session events. This is needed\n\t * for waiting two possible cases: cm_err has something meaningful\n\t * or session state was really changed to error by device removal.\n\t */\n\terr = wait_event_interruptible_timeout(\n\t\t\tclt_path->state_wq,\n\t\t\tcon->cm_err || clt_path->state != RTRS_CLT_CONNECTING,\n\t\t\tmsecs_to_jiffies(RTRS_CONNECT_TIMEOUT_MS));\n\tif (err == 0 || err == -ERESTARTSYS) {\n\t\tif (err == 0)\n\t\t\terr = -ETIMEDOUT;\n\t\t/* Timedout or interrupted */\n\t\tgoto errr;\n\t}\n\tif (con->cm_err < 0) {\n\t\terr = con->cm_err;\n\t\tgoto errr;\n\t}\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTING) {\n\t\t/* Device removal */\n\t\terr = -ECONNABORTED;\n\t\tgoto errr;\n\t}\n\n\treturn 0;\n\nerrr:\n\tstop_cm(con);\n\tmutex_lock(&con->con_mutex);\n\tdestroy_con_cq_qp(con);\n\tmutex_unlock(&con->con_mutex);\ndestroy_cm:\n\tdestroy_cm(con);\n\n\treturn err;\n}\n\nstatic void rtrs_clt_path_up(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tint up;\n\n\t/*\n\t * We can fire RECONNECTED event only when all paths were\n\t * connected on rtrs_clt_open(), then each was disconnected\n\t * and the first one connected again.  That's why this nasty\n\t * game with counter value.\n\t */\n\n\tmutex_lock(&clt->paths_ev_mutex);\n\tup = ++clt->paths_up;\n\t/*\n\t * Here it is safe to access paths num directly since up counter\n\t * is greater than MAX_PATHS_NUM only while rtrs_clt_open() is\n\t * in progress, thus paths removals are impossible.\n\t */\n\tif (up > MAX_PATHS_NUM && up == MAX_PATHS_NUM + clt->paths_num)\n\t\tclt->paths_up = clt->paths_num;\n\telse if (up == 1)\n\t\tclt->link_ev(clt->priv, RTRS_CLT_LINK_EV_RECONNECTED);\n\tmutex_unlock(&clt->paths_ev_mutex);\n\n\t/* Mark session as established */\n\tclt_path->established = true;\n\tclt_path->reconnect_attempts = 0;\n\tclt_path->stats->reconnects.successful_cnt++;\n}\n\nstatic void rtrs_clt_path_down(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\n\tif (!clt_path->established)\n\t\treturn;\n\n\tclt_path->established = false;\n\tmutex_lock(&clt->paths_ev_mutex);\n\tWARN_ON(!clt->paths_up);\n\tif (--clt->paths_up == 0)\n\t\tclt->link_ev(clt->priv, RTRS_CLT_LINK_EV_DISCONNECTED);\n\tmutex_unlock(&clt->paths_ev_mutex);\n}\n\nstatic void rtrs_clt_stop_and_destroy_conns(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_con *con;\n\tunsigned int cid;\n\n\tWARN_ON(READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTED);\n\n\t/*\n\t * Possible race with rtrs_clt_open(), when DEVICE_REMOVAL comes\n\t * exactly in between.  Start destroying after it finishes.\n\t */\n\tmutex_lock(&clt_path->init_mutex);\n\tmutex_unlock(&clt_path->init_mutex);\n\n\t/*\n\t * All IO paths must observe !CONNECTED state before we\n\t * free everything.\n\t */\n\tsynchronize_rcu();\n\n\trtrs_stop_hb(&clt_path->s);\n\n\t/*\n\t * The order it utterly crucial: firstly disconnect and complete all\n\t * rdma requests with error (thus set in_use=false for requests),\n\t * then fail outstanding requests checking in_use for each, and\n\t * eventually notify upper layer about session disconnection.\n\t */\n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (!clt_path->s.con[cid])\n\t\t\tbreak;\n\t\tcon = to_clt_con(clt_path->s.con[cid]);\n\t\tstop_cm(con);\n\t}\n\tfail_all_outstanding_reqs(clt_path);\n\tfree_path_reqs(clt_path);\n\trtrs_clt_path_down(clt_path);\n\n\t/*\n\t * Wait for graceful shutdown, namely when peer side invokes\n\t * rdma_disconnect(). 'connected_cnt' is decremented only on\n\t * CM events, thus if other side had crashed and hb has detected\n\t * something is wrong, here we will stuck for exactly timeout ms,\n\t * since CM does not fire anything.  That is fine, we are not in\n\t * hurry.\n\t */\n\twait_event_timeout(clt_path->state_wq,\n\t\t\t   !atomic_read(&clt_path->connected_cnt),\n\t\t\t   msecs_to_jiffies(RTRS_CONNECT_TIMEOUT_MS));\n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (!clt_path->s.con[cid])\n\t\t\tbreak;\n\t\tcon = to_clt_con(clt_path->s.con[cid]);\n\t\tmutex_lock(&con->con_mutex);\n\t\tdestroy_con_cq_qp(con);\n\t\tmutex_unlock(&con->con_mutex);\n\t\tdestroy_cm(con);\n\t\tdestroy_con(con);\n\t}\n}\n\nstatic inline bool xchg_paths(struct rtrs_clt_path __rcu **rcu_ppcpu_path,\n\t\t\t      struct rtrs_clt_path *clt_path,\n\t\t\t      struct rtrs_clt_path *next)\n{\n\tstruct rtrs_clt_path **ppcpu_path;\n\n\t/* Call cmpxchg() without sparse warnings */\n\tppcpu_path = (typeof(ppcpu_path))rcu_ppcpu_path;\n\treturn clt_path == cmpxchg(ppcpu_path, clt_path, next);\n}\n\nstatic void rtrs_clt_remove_path_from_arr(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_clt_path *next;\n\tbool wait_for_grace = false;\n\tint cpu;\n\n\tmutex_lock(&clt->paths_mutex);\n\tlist_del_rcu(&clt_path->s.entry);\n\n\t/* Make sure everybody observes path removal. */\n\tsynchronize_rcu();\n\n\t/*\n\t * At this point nobody sees @sess in the list, but still we have\n\t * dangling pointer @pcpu_path which _can_ point to @sess.  Since\n\t * nobody can observe @sess in the list, we guarantee that IO path\n\t * will not assign @sess to @pcpu_path, i.e. @pcpu_path can be equal\n\t * to @sess, but can never again become @sess.\n\t */\n\n\t/*\n\t * Decrement paths number only after grace period, because\n\t * caller of do_each_path() must firstly observe list without\n\t * path and only then decremented paths number.\n\t *\n\t * Otherwise there can be the following situation:\n\t *    o Two paths exist and IO is coming.\n\t *    o One path is removed:\n\t *      CPU#0                          CPU#1\n\t *      do_each_path():                rtrs_clt_remove_path_from_arr():\n\t *          path = get_next_path()\n\t *          ^^^                            list_del_rcu(path)\n\t *          [!CONNECTED path]              clt->paths_num--\n\t *                                              ^^^^^^^^^\n\t *          load clt->paths_num                 from 2 to 1\n\t *                    ^^^^^^^^^\n\t *                    sees 1\n\t *\n\t *      path is observed as !CONNECTED, but do_each_path() loop\n\t *      ends, because expression i < clt->paths_num is false.\n\t */\n\tclt->paths_num--;\n\n\t/*\n\t * Get @next connection from current @sess which is going to be\n\t * removed.  If @sess is the last element, then @next is NULL.\n\t */\n\trcu_read_lock();\n\tnext = list_next_or_null_rr_rcu(&clt->paths_list, &clt_path->s.entry,\n\t\t\t\t\ttypeof(*next), s.entry);\n\trcu_read_unlock();\n\n\t/*\n\t * @pcpu paths can still point to the path which is going to be\n\t * removed, so change the pointer manually.\n\t */\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct rtrs_clt_path __rcu **ppcpu_path;\n\n\t\tppcpu_path = per_cpu_ptr(clt->pcpu_path, cpu);\n\t\tif (rcu_dereference_protected(*ppcpu_path,\n\t\t\tlockdep_is_held(&clt->paths_mutex)) != clt_path)\n\t\t\t/*\n\t\t\t * synchronize_rcu() was called just after deleting\n\t\t\t * entry from the list, thus IO code path cannot\n\t\t\t * change pointer back to the pointer which is going\n\t\t\t * to be removed, we are safe here.\n\t\t\t */\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We race with IO code path, which also changes pointer,\n\t\t * thus we have to be careful not to overwrite it.\n\t\t */\n\t\tif (xchg_paths(ppcpu_path, clt_path, next))\n\t\t\t/*\n\t\t\t * @ppcpu_path was successfully replaced with @next,\n\t\t\t * that means that someone could also pick up the\n\t\t\t * @sess and dereferencing it right now, so wait for\n\t\t\t * a grace period is required.\n\t\t\t */\n\t\t\twait_for_grace = true;\n\t}\n\tif (wait_for_grace)\n\t\tsynchronize_rcu();\n\n\tmutex_unlock(&clt->paths_mutex);\n}\n\nstatic void rtrs_clt_add_path_to_arr(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\n\tmutex_lock(&clt->paths_mutex);\n\tclt->paths_num++;\n\n\tlist_add_tail_rcu(&clt_path->s.entry, &clt->paths_list);\n\tmutex_unlock(&clt->paths_mutex);\n}\n\nstatic void rtrs_clt_close_work(struct work_struct *work)\n{\n\tstruct rtrs_clt_path *clt_path;\n\n\tclt_path = container_of(work, struct rtrs_clt_path, close_work);\n\n\tcancel_delayed_work_sync(&clt_path->reconnect_dwork);\n\trtrs_clt_stop_and_destroy_conns(clt_path);\n\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CLOSED, NULL);\n}\n\nstatic int init_conns(struct rtrs_clt_path *clt_path)\n{\n\tunsigned int cid;\n\tint err;\n\n\t/*\n\t * On every new session connections increase reconnect counter\n\t * to avoid clashes with previous sessions not yet closed\n\t * sessions on a server side.\n\t */\n\tclt_path->s.recon_cnt++;\n\n\t/* Establish all RDMA connections  */\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\terr = create_con(clt_path, cid);\n\t\tif (err)\n\t\t\tgoto destroy;\n\n\t\terr = create_cm(to_clt_con(clt_path->s.con[cid]));\n\t\tif (err) {\n\t\t\tdestroy_con(to_clt_con(clt_path->s.con[cid]));\n\t\t\tgoto destroy;\n\t\t}\n\t}\n\terr = alloc_path_reqs(clt_path);\n\tif (err)\n\t\tgoto destroy;\n\n\trtrs_start_hb(&clt_path->s);\n\n\treturn 0;\n\ndestroy:\n\twhile (cid--) {\n\t\tstruct rtrs_clt_con *con = to_clt_con(clt_path->s.con[cid]);\n\n\t\tstop_cm(con);\n\n\t\tmutex_lock(&con->con_mutex);\n\t\tdestroy_con_cq_qp(con);\n\t\tmutex_unlock(&con->con_mutex);\n\t\tdestroy_cm(con);\n\t\tdestroy_con(con);\n\t}\n\t/*\n\t * If we've never taken async path and got an error, say,\n\t * doing rdma_resolve_addr(), switch to CONNECTION_ERR state\n\t * manually to keep reconnecting.\n\t */\n\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING_ERR, NULL);\n\n\treturn err;\n}\n\nstatic void rtrs_clt_info_req_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_iu *iu;\n\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\trtrs_iu_free(iu, clt_path->s.dev->ib_dev, 1);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(clt_path->clt, \"Path info request send failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING_ERR, NULL);\n\t\treturn;\n\t}\n\n\trtrs_clt_update_wc_stats(con);\n}\n\nstatic int process_info_rsp(struct rtrs_clt_path *clt_path,\n\t\t\t    const struct rtrs_msg_info_rsp *msg)\n{\n\tunsigned int sg_cnt, total_len;\n\tint i, sgi;\n\n\tsg_cnt = le16_to_cpu(msg->sg_cnt);\n\tif (!sg_cnt || (clt_path->queue_depth % sg_cnt)) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"Incorrect sg_cnt %d, is not multiple\\n\",\n\t\t\t  sg_cnt);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Check if IB immediate data size is enough to hold the mem_id and\n\t * the offset inside the memory chunk.\n\t */\n\tif ((ilog2(sg_cnt - 1) + 1) + (ilog2(clt_path->chunk_size - 1) + 1) >\n\t    MAX_IMM_PAYL_BITS) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"RDMA immediate size (%db) not enough to encode %d buffers of size %dB\\n\",\n\t\t\t  MAX_IMM_PAYL_BITS, sg_cnt, clt_path->chunk_size);\n\t\treturn -EINVAL;\n\t}\n\ttotal_len = 0;\n\tfor (sgi = 0, i = 0; sgi < sg_cnt && i < clt_path->queue_depth; sgi++) {\n\t\tconst struct rtrs_sg_desc *desc = &msg->desc[sgi];\n\t\tu32 len, rkey;\n\t\tu64 addr;\n\n\t\taddr = le64_to_cpu(desc->addr);\n\t\trkey = le32_to_cpu(desc->key);\n\t\tlen  = le32_to_cpu(desc->len);\n\n\t\ttotal_len += len;\n\n\t\tif (!len || (len % clt_path->chunk_size)) {\n\t\t\trtrs_err(clt_path->clt, \"Incorrect [%d].len %d\\n\",\n\t\t\t\t  sgi,\n\t\t\t\t  len);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfor ( ; len && i < clt_path->queue_depth; i++) {\n\t\t\tclt_path->rbufs[i].addr = addr;\n\t\t\tclt_path->rbufs[i].rkey = rkey;\n\n\t\t\tlen  -= clt_path->chunk_size;\n\t\t\taddr += clt_path->chunk_size;\n\t\t}\n\t}\n\t/* Sanity check */\n\tif (sgi != sg_cnt || i != clt_path->queue_depth) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"Incorrect sg vector, not fully mapped\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (total_len != clt_path->chunk_size * clt_path->queue_depth) {\n\t\trtrs_err(clt_path->clt, \"Incorrect total_len %d\\n\", total_len);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void rtrs_clt_info_rsp_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_msg_info_rsp *msg;\n\tenum rtrs_clt_state state;\n\tstruct rtrs_iu *iu;\n\tsize_t rx_sz;\n\tint err;\n\n\tstate = RTRS_CLT_CONNECTING_ERR;\n\n\tWARN_ON(con->c.cid);\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(clt_path->clt, \"Path info response recv failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\tgoto out;\n\t}\n\tWARN_ON(wc->opcode != IB_WC_RECV);\n\n\tif (wc->byte_len < sizeof(*msg)) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\tib_dma_sync_single_for_cpu(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t   iu->size, DMA_FROM_DEVICE);\n\tmsg = iu->buf;\n\tif (le16_to_cpu(msg->type) != RTRS_MSG_INFO_RSP) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: type %d\\n\",\n\t\t\t  le16_to_cpu(msg->type));\n\t\tgoto out;\n\t}\n\trx_sz  = sizeof(*msg);\n\trx_sz += sizeof(msg->desc[0]) * le16_to_cpu(msg->sg_cnt);\n\tif (wc->byte_len < rx_sz) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\terr = process_info_rsp(clt_path, msg);\n\tif (err)\n\t\tgoto out;\n\n\terr = post_recv_path(clt_path);\n\tif (err)\n\t\tgoto out;\n\n\tstate = RTRS_CLT_CONNECTED;\n\nout:\n\trtrs_clt_update_wc_stats(con);\n\trtrs_iu_free(iu, clt_path->s.dev->ib_dev, 1);\n\trtrs_clt_change_state_get_old(clt_path, state, NULL);\n}\n\nstatic int rtrs_send_path_info(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_con *usr_con = to_clt_con(clt_path->s.con[0]);\n\tstruct rtrs_msg_info_req *msg;\n\tstruct rtrs_iu *tx_iu, *rx_iu;\n\tsize_t rx_sz;\n\tint err;\n\n\trx_sz  = sizeof(struct rtrs_msg_info_rsp);\n\trx_sz += sizeof(struct rtrs_sg_desc) * clt_path->queue_depth;\n\n\ttx_iu = rtrs_iu_alloc(1, sizeof(struct rtrs_msg_info_req), GFP_KERNEL,\n\t\t\t       clt_path->s.dev->ib_dev, DMA_TO_DEVICE,\n\t\t\t       rtrs_clt_info_req_done);\n\trx_iu = rtrs_iu_alloc(1, rx_sz, GFP_KERNEL, clt_path->s.dev->ib_dev,\n\t\t\t       DMA_FROM_DEVICE, rtrs_clt_info_rsp_done);\n\tif (!tx_iu || !rx_iu) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\t/* Prepare for getting info response */\n\terr = rtrs_iu_post_recv(&usr_con->c, rx_iu);\n\tif (err) {\n\t\trtrs_err(clt_path->clt, \"rtrs_iu_post_recv(), err: %d\\n\", err);\n\t\tgoto out;\n\t}\n\trx_iu = NULL;\n\n\tmsg = tx_iu->buf;\n\tmsg->type = cpu_to_le16(RTRS_MSG_INFO_REQ);\n\tmemcpy(msg->pathname, clt_path->s.sessname, sizeof(msg->pathname));\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      tx_iu->dma_addr,\n\t\t\t\t      tx_iu->size, DMA_TO_DEVICE);\n\n\t/* Send info request */\n\terr = rtrs_iu_post_send(&usr_con->c, tx_iu, sizeof(*msg), NULL);\n\tif (err) {\n\t\trtrs_err(clt_path->clt, \"rtrs_iu_post_send(), err: %d\\n\", err);\n\t\tgoto out;\n\t}\n\ttx_iu = NULL;\n\n\t/* Wait for state change */\n\twait_event_interruptible_timeout(clt_path->state_wq,\n\t\t\t\t\t clt_path->state != RTRS_CLT_CONNECTING,\n\t\t\t\t\t msecs_to_jiffies(\n\t\t\t\t\t\t RTRS_CONNECT_TIMEOUT_MS));\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED) {\n\t\tif (READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTING_ERR)\n\t\t\terr = -ECONNRESET;\n\t\telse\n\t\t\terr = -ETIMEDOUT;\n\t}\n\nout:\n\tif (tx_iu)\n\t\trtrs_iu_free(tx_iu, clt_path->s.dev->ib_dev, 1);\n\tif (rx_iu)\n\t\trtrs_iu_free(rx_iu, clt_path->s.dev->ib_dev, 1);\n\tif (err)\n\t\t/* If we've never taken async path because of malloc problems */\n\t\trtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t      RTRS_CLT_CONNECTING_ERR, NULL);\n\n\treturn err;\n}\n\n/**\n * init_path() - establishes all path connections and does handshake\n * @clt_path: client path.\n * In case of error full close or reconnect procedure should be taken,\n * because reconnect or close async works can be started.\n */\nstatic int init_path(struct rtrs_clt_path *clt_path)\n{\n\tint err;\n\tchar str[NAME_MAX];\n\tstruct rtrs_addr path = {\n\t\t.src = &clt_path->s.src_addr,\n\t\t.dst = &clt_path->s.dst_addr,\n\t};\n\n\trtrs_addr_to_str(&path, str, sizeof(str));\n\n\tmutex_lock(&clt_path->init_mutex);\n\terr = init_conns(clt_path);\n\tif (err) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"init_conns() failed: err=%d path=%s [%s:%u]\\n\", err,\n\t\t\t str, clt_path->hca_name, clt_path->hca_port);\n\t\tgoto out;\n\t}\n\terr = rtrs_send_path_info(clt_path);\n\tif (err) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"rtrs_send_path_info() failed: err=%d path=%s [%s:%u]\\n\",\n\t\t\t err, str, clt_path->hca_name, clt_path->hca_port);\n\t\tgoto out;\n\t}\n\trtrs_clt_path_up(clt_path);\nout:\n\tmutex_unlock(&clt_path->init_mutex);\n\n\treturn err;\n}\n\nstatic void rtrs_clt_reconnect_work(struct work_struct *work)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tstruct rtrs_clt_sess *clt;\n\tunsigned int delay_ms;\n\tint err;\n\n\tclt_path = container_of(to_delayed_work(work), struct rtrs_clt_path,\n\t\t\t\treconnect_dwork);\n\tclt = clt_path->clt;\n\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_RECONNECTING)\n\t\treturn;\n\n\tif (clt_path->reconnect_attempts >= clt->max_reconnect_attempts) {\n\t\t/* Close a path completely if max attempts is reached */\n\t\trtrs_clt_close_conns(clt_path, false);\n\t\treturn;\n\t}\n\tclt_path->reconnect_attempts++;\n\n\t/* Stop everything */\n\trtrs_clt_stop_and_destroy_conns(clt_path);\n\tmsleep(RTRS_RECONNECT_BACKOFF);\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING, NULL)) {\n\t\terr = init_path(clt_path);\n\t\tif (err)\n\t\t\tgoto reconnect_again;\n\t}\n\n\treturn;\n\nreconnect_again:\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_RECONNECTING, NULL)) {\n\t\tclt_path->stats->reconnects.fail_cnt++;\n\t\tdelay_ms = clt->reconnect_delay_sec * 1000;\n\t\tqueue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork,\n\t\t\t\t   msecs_to_jiffies(delay_ms +\n\t\t\t\t\t\t    prandom_u32() %\n\t\t\t\t\t\t    RTRS_RECONNECT_SEED));\n\t}\n}\n\nstatic void rtrs_clt_dev_release(struct device *dev)\n{\n\tstruct rtrs_clt_sess *clt = container_of(dev, struct rtrs_clt_sess,\n\t\t\t\t\t\t dev);\n\n\tkfree(clt);\n}\n\nstatic struct rtrs_clt_sess *alloc_clt(const char *sessname, size_t paths_num,\n\t\t\t\t  u16 port, size_t pdu_sz, void *priv,\n\t\t\t\t  void\t(*link_ev)(void *priv,\n\t\t\t\t\t\t   enum rtrs_clt_link_ev ev),\n\t\t\t\t  unsigned int reconnect_delay_sec,\n\t\t\t\t  unsigned int max_reconnect_attempts)\n{\n\tstruct rtrs_clt_sess *clt;\n\tint err;\n\n\tif (!paths_num || paths_num > MAX_PATHS_NUM)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strlen(sessname) >= sizeof(clt->sessname))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tclt = kzalloc(sizeof(*clt), GFP_KERNEL);\n\tif (!clt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tclt->pcpu_path = alloc_percpu(typeof(*clt->pcpu_path));\n\tif (!clt->pcpu_path) {\n\t\tkfree(clt);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tuuid_gen(&clt->paths_uuid);\n\tINIT_LIST_HEAD_RCU(&clt->paths_list);\n\tclt->paths_num = paths_num;\n\tclt->paths_up = MAX_PATHS_NUM;\n\tclt->port = port;\n\tclt->pdu_sz = pdu_sz;\n\tclt->max_segments = RTRS_MAX_SEGMENTS;\n\tclt->reconnect_delay_sec = reconnect_delay_sec;\n\tclt->max_reconnect_attempts = max_reconnect_attempts;\n\tclt->priv = priv;\n\tclt->link_ev = link_ev;\n\tclt->mp_policy = MP_POLICY_MIN_INFLIGHT;\n\tstrscpy(clt->sessname, sessname, sizeof(clt->sessname));\n\tinit_waitqueue_head(&clt->permits_wait);\n\tmutex_init(&clt->paths_ev_mutex);\n\tmutex_init(&clt->paths_mutex);\n\n\tclt->dev.class = rtrs_clt_dev_class;\n\tclt->dev.release = rtrs_clt_dev_release;\n\terr = dev_set_name(&clt->dev, \"%s\", sessname);\n\tif (err)\n\t\tgoto err;\n\t/*\n\t * Suppress user space notification until\n\t * sysfs files are created\n\t */\n\tdev_set_uevent_suppress(&clt->dev, true);\n\terr = device_register(&clt->dev);\n\tif (err) {\n\t\tput_device(&clt->dev);\n\t\tgoto err;\n\t}\n\n\tclt->kobj_paths = kobject_create_and_add(\"paths\", &clt->dev.kobj);\n\tif (!clt->kobj_paths) {\n\t\terr = -ENOMEM;\n\t\tgoto err_dev;\n\t}\n\terr = rtrs_clt_create_sysfs_root_files(clt);\n\tif (err) {\n\t\tkobject_del(clt->kobj_paths);\n\t\tkobject_put(clt->kobj_paths);\n\t\tgoto err_dev;\n\t}\n\tdev_set_uevent_suppress(&clt->dev, false);\n\tkobject_uevent(&clt->dev.kobj, KOBJ_ADD);\n\n\treturn clt;\nerr_dev:\n\tdevice_unregister(&clt->dev);\nerr:\n\tfree_percpu(clt->pcpu_path);\n\tkfree(clt);\n\treturn ERR_PTR(err);\n}\n\nstatic void free_clt(struct rtrs_clt_sess *clt)\n{\n\tfree_permits(clt);\n\tfree_percpu(clt->pcpu_path);\n\tmutex_destroy(&clt->paths_ev_mutex);\n\tmutex_destroy(&clt->paths_mutex);\n\t/* release callback will free clt in last put */\n\tdevice_unregister(&clt->dev);\n}\n\n/**\n * rtrs_clt_open() - Open a path to an RTRS server\n * @ops: holds the link event callback and the private pointer.\n * @sessname: name of the session\n * @paths: Paths to be established defined by their src and dst addresses\n * @paths_num: Number of elements in the @paths array\n * @port: port to be used by the RTRS session\n * @pdu_sz: Size of extra payload which can be accessed after permit allocation.\n * @reconnect_delay_sec: time between reconnect tries\n * @max_reconnect_attempts: Number of times to reconnect on error before giving\n *\t\t\t    up, 0 for * disabled, -1 for forever\n * @nr_poll_queues: number of polling mode connection using IB_POLL_DIRECT flag\n *\n * Starts session establishment with the rtrs_server. The function can block\n * up to ~2000ms before it returns.\n *\n * Return a valid pointer on success otherwise PTR_ERR.\n */\nstruct rtrs_clt_sess *rtrs_clt_open(struct rtrs_clt_ops *ops,\n\t\t\t\t const char *pathname,\n\t\t\t\t const struct rtrs_addr *paths,\n\t\t\t\t size_t paths_num, u16 port,\n\t\t\t\t size_t pdu_sz, u8 reconnect_delay_sec,\n\t\t\t\t s16 max_reconnect_attempts, u32 nr_poll_queues)\n{\n\tstruct rtrs_clt_path *clt_path, *tmp;\n\tstruct rtrs_clt_sess *clt;\n\tint err, i;\n\n\tif (strchr(pathname, '/') || strchr(pathname, '.')) {\n\t\tpr_err(\"pathname cannot contain / and .\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tclt = alloc_clt(pathname, paths_num, port, pdu_sz, ops->priv,\n\t\t\tops->link_ev,\n\t\t\treconnect_delay_sec,\n\t\t\tmax_reconnect_attempts);\n\tif (IS_ERR(clt)) {\n\t\terr = PTR_ERR(clt);\n\t\tgoto out;\n\t}\n\tfor (i = 0; i < paths_num; i++) {\n\t\tstruct rtrs_clt_path *clt_path;\n\n\t\tclt_path = alloc_path(clt, &paths[i], nr_cpu_ids,\n\t\t\t\t  nr_poll_queues);\n\t\tif (IS_ERR(clt_path)) {\n\t\t\terr = PTR_ERR(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\t\tif (!i)\n\t\t\tclt_path->for_new_clt = 1;\n\t\tlist_add_tail_rcu(&clt_path->s.entry, &clt->paths_list);\n\n\t\terr = init_path(clt_path);\n\t\tif (err) {\n\t\t\tlist_del_rcu(&clt_path->s.entry);\n\t\t\trtrs_clt_close_conns(clt_path, true);\n\t\t\tfree_percpu(clt_path->stats->pcpu_stats);\n\t\t\tkfree(clt_path->stats);\n\t\t\tfree_path(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\n\t\terr = rtrs_clt_create_path_files(clt_path);\n\t\tif (err) {\n\t\t\tlist_del_rcu(&clt_path->s.entry);\n\t\t\trtrs_clt_close_conns(clt_path, true);\n\t\t\tfree_percpu(clt_path->stats->pcpu_stats);\n\t\t\tkfree(clt_path->stats);\n\t\t\tfree_path(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\t}\n\terr = alloc_permits(clt);\n\tif (err)\n\t\tgoto close_all_path;\n\n\treturn clt;\n\nclose_all_path:\n\tlist_for_each_entry_safe(clt_path, tmp, &clt->paths_list, s.entry) {\n\t\trtrs_clt_destroy_path_files(clt_path, NULL);\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\trtrs_clt_destroy_sysfs_root(clt);\n\tfree_clt(clt);\n\nout:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL(rtrs_clt_open);\n\n/**\n * rtrs_clt_close() - Close a path\n * @clt: Session handle. Session is freed upon return.\n */\nvoid rtrs_clt_close(struct rtrs_clt_sess *clt)\n{\n\tstruct rtrs_clt_path *clt_path, *tmp;\n\n\t/* Firstly forbid sysfs access */\n\trtrs_clt_destroy_sysfs_root(clt);\n\n\t/* Now it is safe to iterate over all paths without locks */\n\tlist_for_each_entry_safe(clt_path, tmp, &clt->paths_list, s.entry) {\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\trtrs_clt_destroy_path_files(clt_path, NULL);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\tfree_clt(clt);\n}\nEXPORT_SYMBOL(rtrs_clt_close);\n\nint rtrs_clt_reconnect_from_sysfs(struct rtrs_clt_path *clt_path)\n{\n\tenum rtrs_clt_state old_state;\n\tint err = -EBUSY;\n\tbool changed;\n\n\tchanged = rtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t\t RTRS_CLT_RECONNECTING,\n\t\t\t\t\t\t &old_state);\n\tif (changed) {\n\t\tclt_path->reconnect_attempts = 0;\n\t\tqueue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork, 0);\n\t}\n\tif (changed || old_state == RTRS_CLT_RECONNECTING) {\n\t\t/*\n\t\t * flush_delayed_work() queues pending work for immediate\n\t\t * execution, so do the flush if we have queued something\n\t\t * right now or work is pending.\n\t\t */\n\t\tflush_delayed_work(&clt_path->reconnect_dwork);\n\t\terr = (READ_ONCE(clt_path->state) ==\n\t\t       RTRS_CLT_CONNECTED ? 0 : -ENOTCONN);\n\t}\n\n\treturn err;\n}\n\nint rtrs_clt_remove_path_from_sysfs(struct rtrs_clt_path *clt_path,\n\t\t\t\t     const struct attribute *sysfs_self)\n{\n\tenum rtrs_clt_state old_state;\n\tbool changed;\n\n\t/*\n\t * Continue stopping path till state was changed to DEAD or\n\t * state was observed as DEAD:\n\t * 1. State was changed to DEAD - we were fast and nobody\n\t *    invoked rtrs_clt_reconnect(), which can again start\n\t *    reconnecting.\n\t * 2. State was observed as DEAD - we have someone in parallel\n\t *    removing the path.\n\t */\n\tdo {\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\tchanged = rtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t\t\tRTRS_CLT_DEAD,\n\t\t\t\t\t\t\t&old_state);\n\t} while (!changed && old_state != RTRS_CLT_DEAD);\n\n\tif (changed) {\n\t\trtrs_clt_remove_path_from_arr(clt_path);\n\t\trtrs_clt_destroy_path_files(clt_path, sysfs_self);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\n\treturn 0;\n}\n\nvoid rtrs_clt_set_max_reconnect_attempts(struct rtrs_clt_sess *clt, int value)\n{\n\tclt->max_reconnect_attempts = (unsigned int)value;\n}\n\nint rtrs_clt_get_max_reconnect_attempts(const struct rtrs_clt_sess *clt)\n{\n\treturn (int)clt->max_reconnect_attempts;\n}\n\n/**\n * rtrs_clt_request() - Request data transfer to/from server via RDMA.\n *\n * @dir:\tREAD/WRITE\n * @ops:\tcallback function to be called as confirmation, and the pointer.\n * @clt:\tSession\n * @permit:\tPreallocated permit\n * @vec:\tMessage that is sent to server together with the request.\n *\t\tSum of len of all @vec elements limited to <= IO_MSG_SIZE.\n *\t\tSince the msg is copied internally it can be allocated on stack.\n * @nr:\t\tNumber of elements in @vec.\n * @data_len:\tlength of data sent to/from server\n * @sg:\t\tPages to be sent/received to/from server.\n * @sg_cnt:\tNumber of elements in the @sg\n *\n * Return:\n * 0:\t\tSuccess\n * <0:\t\tError\n *\n * On dir=READ rtrs client will request a data transfer from Server to client.\n * The data that the server will respond with will be stored in @sg when\n * the user receives an %RTRS_CLT_RDMA_EV_RDMA_REQUEST_WRITE_COMPL event.\n * On dir=WRITE rtrs client will rdma write data in sg to server side.\n */\nint rtrs_clt_request(int dir, struct rtrs_clt_req_ops *ops,\n\t\t     struct rtrs_clt_sess *clt, struct rtrs_permit *permit,\n\t\t     const struct kvec *vec, size_t nr, size_t data_len,\n\t\t     struct scatterlist *sg, unsigned int sg_cnt)\n{\n\tstruct rtrs_clt_io_req *req;\n\tstruct rtrs_clt_path *clt_path;\n\n\tenum dma_data_direction dma_dir;\n\tint err = -ECONNABORTED, i;\n\tsize_t usr_len, hdr_len;\n\tstruct path_it it;\n\n\t/* Get kvec length */\n\tfor (i = 0, usr_len = 0; i < nr; i++)\n\t\tusr_len += vec[i].iov_len;\n\n\tif (dir == READ) {\n\t\thdr_len = sizeof(struct rtrs_msg_rdma_read) +\n\t\t\t  sg_cnt * sizeof(struct rtrs_sg_desc);\n\t\tdma_dir = DMA_FROM_DEVICE;\n\t} else {\n\t\thdr_len = sizeof(struct rtrs_msg_rdma_write);\n\t\tdma_dir = DMA_TO_DEVICE;\n\t}\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (clt_path = it.next_path(&it)) && it.i < it.clt->paths_num; it.i++) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (usr_len + hdr_len > clt_path->max_hdr_size) {\n\t\t\trtrs_wrn_rl(clt_path->clt,\n\t\t\t\t     \"%s request failed, user message size is %zu and header length %zu, but max size is %u\\n\",\n\t\t\t\t     dir == READ ? \"Read\" : \"Write\",\n\t\t\t\t     usr_len, hdr_len, clt_path->max_hdr_size);\n\t\t\terr = -EMSGSIZE;\n\t\t\tbreak;\n\t\t}\n\t\treq = rtrs_clt_get_req(clt_path, ops->conf_fn, permit, ops->priv,\n\t\t\t\t       vec, usr_len, sg, sg_cnt, data_len,\n\t\t\t\t       dma_dir);\n\t\tif (dir == READ)\n\t\t\terr = rtrs_clt_read_req(req);\n\t\telse\n\t\t\terr = rtrs_clt_write_req(req);\n\t\tif (err) {\n\t\t\treq->in_use = false;\n\t\t\tcontinue;\n\t\t}\n\t\t/* Success path */\n\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn err;\n}\nEXPORT_SYMBOL(rtrs_clt_request);\n\nint rtrs_clt_rdma_cq_direct(struct rtrs_clt_sess *clt, unsigned int index)\n{\n\t/* If no path, return -1 for block layer not to try again */\n\tint cnt = -1;\n\tstruct rtrs_con *con;\n\tstruct rtrs_clt_path *clt_path;\n\tstruct path_it it;\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (clt_path = it.next_path(&it)) && it.i < it.clt->paths_num; it.i++) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tcon = clt_path->s.con[index + 1];\n\t\tcnt = ib_process_cq_direct(con->cq, -1);\n\t\tif (cnt)\n\t\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn cnt;\n}\nEXPORT_SYMBOL(rtrs_clt_rdma_cq_direct);\n\n/**\n * rtrs_clt_query() - queries RTRS session attributes\n *@clt: session pointer\n *@attr: query results for session attributes.\n * Returns:\n *    0 on success\n *    -ECOMM\t\tno connection to the server\n */\nint rtrs_clt_query(struct rtrs_clt_sess *clt, struct rtrs_attrs *attr)\n{\n\tif (!rtrs_clt_is_connected(clt))\n\t\treturn -ECOMM;\n\n\tattr->queue_depth      = clt->queue_depth;\n\tattr->max_segments     = clt->max_segments;\n\t/* Cap max_io_size to min of remote buffer size and the fr pages */\n\tattr->max_io_size = min_t(int, clt->max_io_size,\n\t\t\t\t  clt->max_segments * SZ_4K);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(rtrs_clt_query);\n\nint rtrs_clt_create_path_from_sysfs(struct rtrs_clt_sess *clt,\n\t\t\t\t     struct rtrs_addr *addr)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tint err;\n\n\tclt_path = alloc_path(clt, addr, nr_cpu_ids, 0);\n\tif (IS_ERR(clt_path))\n\t\treturn PTR_ERR(clt_path);\n\n\tmutex_lock(&clt->paths_mutex);\n\tif (clt->paths_num == 0) {\n\t\t/*\n\t\t * When all the paths are removed for a session,\n\t\t * the addition of the first path is like a new session for\n\t\t * the storage server\n\t\t */\n\t\tclt_path->for_new_clt = 1;\n\t}\n\n\tmutex_unlock(&clt->paths_mutex);\n\n\t/*\n\t * It is totally safe to add path in CONNECTING state: coming\n\t * IO will never grab it.  Also it is very important to add\n\t * path before init, since init fires LINK_CONNECTED event.\n\t */\n\trtrs_clt_add_path_to_arr(clt_path);\n\n\terr = init_path(clt_path);\n\tif (err)\n\t\tgoto close_path;\n\n\terr = rtrs_clt_create_path_files(clt_path);\n\tif (err)\n\t\tgoto close_path;\n\n\treturn 0;\n\nclose_path:\n\trtrs_clt_remove_path_from_arr(clt_path);\n\trtrs_clt_close_conns(clt_path, true);\n\tfree_percpu(clt_path->stats->pcpu_stats);\n\tkfree(clt_path->stats);\n\tfree_path(clt_path);\n\n\treturn err;\n}\n\nstatic int rtrs_clt_ib_dev_init(struct rtrs_ib_dev *dev)\n{\n\tif (!(dev->ib_dev->attrs.device_cap_flags &\n\t      IB_DEVICE_MEM_MGT_EXTENSIONS)) {\n\t\tpr_err(\"Memory registrations not supported.\\n\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct rtrs_rdma_dev_pd_ops dev_pd_ops = {\n\t.init = rtrs_clt_ib_dev_init\n};\n\nstatic int __init rtrs_client_init(void)\n{\n\trtrs_rdma_dev_pd_init(0, &dev_pd);\n\n\trtrs_clt_dev_class = class_create(THIS_MODULE, \"rtrs-client\");\n\tif (IS_ERR(rtrs_clt_dev_class)) {\n\t\tpr_err(\"Failed to create rtrs-client dev class\\n\");\n\t\treturn PTR_ERR(rtrs_clt_dev_class);\n\t}\n\trtrs_wq = alloc_workqueue(\"rtrs_client_wq\", 0, 0);\n\tif (!rtrs_wq) {\n\t\tclass_destroy(rtrs_clt_dev_class);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit rtrs_client_exit(void)\n{\n\tdestroy_workqueue(rtrs_wq);\n\tclass_destroy(rtrs_clt_dev_class);\n\trtrs_rdma_dev_pd_deinit(&dev_pd);\n}\n\nmodule_init(rtrs_client_init);\nmodule_exit(rtrs_client_exit);\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * RDMA Transport Layer\n *\n * Copyright (c) 2014 - 2018 ProfitBricks GmbH. All rights reserved.\n * Copyright (c) 2018 - 2019 1&1 IONOS Cloud GmbH. All rights reserved.\n * Copyright (c) 2019 - 2020 1&1 IONOS SE. All rights reserved.\n */\n\n#undef pr_fmt\n#define pr_fmt(fmt) KBUILD_MODNAME \" L\" __stringify(__LINE__) \": \" fmt\n\n#include <linux/module.h>\n#include <linux/rculist.h>\n#include <linux/random.h>\n\n#include \"rtrs-clt.h\"\n#include \"rtrs-log.h\"\n\n#define RTRS_CONNECT_TIMEOUT_MS 30000\n/*\n * Wait a bit before trying to reconnect after a failure\n * in order to give server time to finish clean up which\n * leads to \"false positives\" failed reconnect attempts\n */\n#define RTRS_RECONNECT_BACKOFF 1000\n/*\n * Wait for additional random time between 0 and 8 seconds\n * before starting to reconnect to avoid clients reconnecting\n * all at once in case of a major network outage\n */\n#define RTRS_RECONNECT_SEED 8\n\n#define FIRST_CONN 0x01\n/* limit to 128 * 4k = 512k max IO */\n#define RTRS_MAX_SEGMENTS          128\n\nMODULE_DESCRIPTION(\"RDMA Transport Client\");\nMODULE_LICENSE(\"GPL\");\n\nstatic const struct rtrs_rdma_dev_pd_ops dev_pd_ops;\nstatic struct rtrs_rdma_dev_pd dev_pd = {\n\t.ops = &dev_pd_ops\n};\n\nstatic struct workqueue_struct *rtrs_wq;\nstatic struct class *rtrs_clt_dev_class;\n\nstatic inline bool rtrs_clt_is_connected(const struct rtrs_clt_sess *clt)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tbool connected = false;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry)\n\t\tconnected |= READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTED;\n\trcu_read_unlock();\n\n\treturn connected;\n}\n\nstatic struct rtrs_permit *\n__rtrs_get_permit(struct rtrs_clt_sess *clt, enum rtrs_clt_con_type con_type)\n{\n\tsize_t max_depth = clt->queue_depth;\n\tstruct rtrs_permit *permit;\n\tint bit;\n\n\t/*\n\t * Adapted from null_blk get_tag(). Callers from different cpus may\n\t * grab the same bit, since find_first_zero_bit is not atomic.\n\t * But then the test_and_set_bit_lock will fail for all the\n\t * callers but one, so that they will loop again.\n\t * This way an explicit spinlock is not required.\n\t */\n\tdo {\n\t\tbit = find_first_zero_bit(clt->permits_map, max_depth);\n\t\tif (bit >= max_depth)\n\t\t\treturn NULL;\n\t} while (test_and_set_bit_lock(bit, clt->permits_map));\n\n\tpermit = get_permit(clt, bit);\n\tWARN_ON(permit->mem_id != bit);\n\tpermit->cpu_id = raw_smp_processor_id();\n\tpermit->con_type = con_type;\n\n\treturn permit;\n}\n\nstatic inline void __rtrs_put_permit(struct rtrs_clt_sess *clt,\n\t\t\t\t      struct rtrs_permit *permit)\n{\n\tclear_bit_unlock(permit->mem_id, clt->permits_map);\n}\n\n/**\n * rtrs_clt_get_permit() - allocates permit for future RDMA operation\n * @clt:\tCurrent session\n * @con_type:\tType of connection to use with the permit\n * @can_wait:\tWait type\n *\n * Description:\n *    Allocates permit for the following RDMA operation.  Permit is used\n *    to preallocate all resources and to propagate memory pressure\n *    up earlier.\n *\n * Context:\n *    Can sleep if @wait == RTRS_PERMIT_WAIT\n */\nstruct rtrs_permit *rtrs_clt_get_permit(struct rtrs_clt_sess *clt,\n\t\t\t\t\t  enum rtrs_clt_con_type con_type,\n\t\t\t\t\t  enum wait_type can_wait)\n{\n\tstruct rtrs_permit *permit;\n\tDEFINE_WAIT(wait);\n\n\tpermit = __rtrs_get_permit(clt, con_type);\n\tif (permit || !can_wait)\n\t\treturn permit;\n\n\tdo {\n\t\tprepare_to_wait(&clt->permits_wait, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tpermit = __rtrs_get_permit(clt, con_type);\n\t\tif (permit)\n\t\t\tbreak;\n\n\t\tio_schedule();\n\t} while (1);\n\n\tfinish_wait(&clt->permits_wait, &wait);\n\n\treturn permit;\n}\nEXPORT_SYMBOL(rtrs_clt_get_permit);\n\n/**\n * rtrs_clt_put_permit() - puts allocated permit\n * @clt:\tCurrent session\n * @permit:\tPermit to be freed\n *\n * Context:\n *    Does not matter\n */\nvoid rtrs_clt_put_permit(struct rtrs_clt_sess *clt,\n\t\t\t struct rtrs_permit *permit)\n{\n\tif (WARN_ON(!test_bit(permit->mem_id, clt->permits_map)))\n\t\treturn;\n\n\t__rtrs_put_permit(clt, permit);\n\n\t/*\n\t * rtrs_clt_get_permit() adds itself to the &clt->permits_wait list\n\t * before calling schedule(). So if rtrs_clt_get_permit() is sleeping\n\t * it must have added itself to &clt->permits_wait before\n\t * __rtrs_put_permit() finished.\n\t * Hence it is safe to guard wake_up() with a waitqueue_active() test.\n\t */\n\tif (waitqueue_active(&clt->permits_wait))\n\t\twake_up(&clt->permits_wait);\n}\nEXPORT_SYMBOL(rtrs_clt_put_permit);\n\n/**\n * rtrs_permit_to_clt_con() - returns RDMA connection pointer by the permit\n * @clt_path: client path pointer\n * @permit: permit for the allocation of the RDMA buffer\n * Note:\n *     IO connection starts from 1.\n *     0 connection is for user messages.\n */\nstatic\nstruct rtrs_clt_con *rtrs_permit_to_clt_con(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t    struct rtrs_permit *permit)\n{\n\tint id = 0;\n\n\tif (permit->con_type == RTRS_IO_CON)\n\t\tid = (permit->cpu_id % (clt_path->s.irq_con_num - 1)) + 1;\n\n\treturn to_clt_con(clt_path->s.con[id]);\n}\n\n/**\n * rtrs_clt_change_state() - change the session state through session state\n * machine.\n *\n * @clt_path: client path to change the state of.\n * @new_state: state to change to.\n *\n * returns true if sess's state is changed to new state, otherwise return false.\n *\n * Locks:\n * state_wq lock must be hold.\n */\nstatic bool rtrs_clt_change_state(struct rtrs_clt_path *clt_path,\n\t\t\t\t     enum rtrs_clt_state new_state)\n{\n\tenum rtrs_clt_state old_state;\n\tbool changed = false;\n\n\tlockdep_assert_held(&clt_path->state_wq.lock);\n\n\told_state = clt_path->state;\n\tswitch (new_state) {\n\tcase RTRS_CLT_CONNECTING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_RECONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_RECONNECTING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTED:\n\t\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tcase RTRS_CLT_CLOSED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CONNECTED:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CLOSING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tcase RTRS_CLT_RECONNECTING:\n\t\tcase RTRS_CLT_CONNECTED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CLOSED:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CLOSING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_DEAD:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CLOSED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (changed) {\n\t\tclt_path->state = new_state;\n\t\twake_up_locked(&clt_path->state_wq);\n\t}\n\n\treturn changed;\n}\n\nstatic bool rtrs_clt_change_state_from_to(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t   enum rtrs_clt_state old_state,\n\t\t\t\t\t   enum rtrs_clt_state new_state)\n{\n\tbool changed = false;\n\n\tspin_lock_irq(&clt_path->state_wq.lock);\n\tif (clt_path->state == old_state)\n\t\tchanged = rtrs_clt_change_state(clt_path, new_state);\n\tspin_unlock_irq(&clt_path->state_wq.lock);\n\n\treturn changed;\n}\n\nstatic void rtrs_rdma_error_recovery(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tif (rtrs_clt_change_state_from_to(clt_path,\n\t\t\t\t\t   RTRS_CLT_CONNECTED,\n\t\t\t\t\t   RTRS_CLT_RECONNECTING)) {\n\t\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\t\tunsigned int delay_ms;\n\n\t\t/*\n\t\t * Normal scenario, reconnect if we were successfully connected\n\t\t */\n\t\tdelay_ms = clt->reconnect_delay_sec * 1000;\n\t\tqueue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork,\n\t\t\t\t   msecs_to_jiffies(delay_ms +\n\t\t\t\t\t\t    prandom_u32() % RTRS_RECONNECT_SEED));\n\t} else {\n\t\t/*\n\t\t * Error can happen just on establishing new connection,\n\t\t * so notify waiter with error state, waiter is responsible\n\t\t * for cleaning the rest and reconnect if needed.\n\t\t */\n\t\trtrs_clt_change_state_from_to(clt_path,\n\t\t\t\t\t       RTRS_CLT_CONNECTING,\n\t\t\t\t\t       RTRS_CLT_CONNECTING_ERR);\n\t}\n}\n\nstatic void rtrs_clt_fast_reg_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(con->c.path, \"Failed IB_WR_REG_MR: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_rdma_error_recovery(con);\n\t}\n}\n\nstatic struct ib_cqe fast_reg_cqe = {\n\t.done = rtrs_clt_fast_reg_done\n};\n\nstatic void complete_rdma_req(struct rtrs_clt_io_req *req, int errno,\n\t\t\t      bool notify, bool can_wait);\n\nstatic void rtrs_clt_inv_rkey_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_io_req *req =\n\t\tcontainer_of(wc->wr_cqe, typeof(*req), inv_cqe);\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(con->c.path, \"Failed IB_WR_LOCAL_INV: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_rdma_error_recovery(con);\n\t}\n\treq->need_inv = false;\n\tif (req->need_inv_comp)\n\t\tcomplete(&req->inv_comp);\n\telse\n\t\t/* Complete request from INV callback */\n\t\tcomplete_rdma_req(req, req->inv_errno, true, false);\n}\n\nstatic int rtrs_inv_rkey(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct ib_send_wr wr = {\n\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t.wr_cqe\t\t    = &req->inv_cqe,\n\t\t.send_flags\t    = IB_SEND_SIGNALED,\n\t\t.ex.invalidate_rkey = req->mr->rkey,\n\t};\n\treq->inv_cqe.done = rtrs_clt_inv_rkey_done;\n\n\treturn ib_post_send(con->c.qp, &wr, NULL);\n}\n\nstatic void complete_rdma_req(struct rtrs_clt_io_req *req, int errno,\n\t\t\t      bool notify, bool can_wait)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_clt_path *clt_path;\n\tint err;\n\n\tif (WARN_ON(!req->in_use))\n\t\treturn;\n\tif (WARN_ON(!req->con))\n\t\treturn;\n\tclt_path = to_clt_path(con->c.path);\n\n\tif (req->sg_cnt) {\n\t\tif (req->dir == DMA_FROM_DEVICE && req->need_inv) {\n\t\t\t/*\n\t\t\t * We are here to invalidate read requests\n\t\t\t * ourselves.  In normal scenario server should\n\t\t\t * send INV for all read requests, but\n\t\t\t * we are here, thus two things could happen:\n\t\t\t *\n\t\t\t *    1.  this is failover, when errno != 0\n\t\t\t *        and can_wait == 1,\n\t\t\t *\n\t\t\t *    2.  something totally bad happened and\n\t\t\t *        server forgot to send INV, so we\n\t\t\t *        should do that ourselves.\n\t\t\t */\n\n\t\t\tif (can_wait) {\n\t\t\t\treq->need_inv_comp = true;\n\t\t\t} else {\n\t\t\t\t/* This should be IO path, so always notify */\n\t\t\t\tWARN_ON(!notify);\n\t\t\t\t/* Save errno for INV callback */\n\t\t\t\treq->inv_errno = errno;\n\t\t\t}\n\n\t\t\trefcount_inc(&req->ref);\n\t\t\terr = rtrs_inv_rkey(req);\n\t\t\tif (err) {\n\t\t\t\trtrs_err(con->c.path, \"Send INV WR key=%#x: %d\\n\",\n\t\t\t\t\t  req->mr->rkey, err);\n\t\t\t} else if (can_wait) {\n\t\t\t\twait_for_completion(&req->inv_comp);\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Something went wrong, so request will be\n\t\t\t\t * completed from INV callback.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(1);\n\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tif (!refcount_dec_and_test(&req->ref))\n\t\t\t\treturn;\n\t\t}\n\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\tif (!refcount_dec_and_test(&req->ref))\n\t\treturn;\n\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\tatomic_dec(&clt_path->stats->inflight);\n\n\treq->in_use = false;\n\treq->con = NULL;\n\n\tif (errno) {\n\t\trtrs_err_rl(con->c.path, \"IO request failed: error=%d path=%s [%s:%u] notify=%d\\n\",\n\t\t\t    errno, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port, notify);\n\t}\n\n\tif (notify)\n\t\treq->conf(req->priv, errno);\n}\n\nstatic int rtrs_post_send_rdma(struct rtrs_clt_con *con,\n\t\t\t\tstruct rtrs_clt_io_req *req,\n\t\t\t\tstruct rtrs_rbuf *rbuf, u32 off,\n\t\t\t\tu32 imm, struct ib_send_wr *wr)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tenum ib_send_flags flags;\n\tstruct ib_sge sge;\n\n\tif (!req->sg_size) {\n\t\trtrs_wrn(con->c.path,\n\t\t\t \"Doing RDMA Write failed, no data supplied\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* user data and user message in the first list element */\n\tsge.addr   = req->iu->dma_addr;\n\tsge.length = req->sg_size;\n\tsge.lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\n\t/*\n\t * From time to time we have to post signalled sends,\n\t * or send queue will fill up and only QP reset can help.\n\t */\n\tflags = atomic_inc_return(&con->c.wr_cnt) % clt_path->s.signal_interval ?\n\t\t\t0 : IB_SEND_SIGNALED;\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      req->iu->dma_addr,\n\t\t\t\t      req->sg_size, DMA_TO_DEVICE);\n\n\treturn rtrs_iu_post_rdma_write_imm(&con->c, req->iu, &sge, 1,\n\t\t\t\t\t    rbuf->rkey, rbuf->addr + off,\n\t\t\t\t\t    imm, flags, wr, NULL);\n}\n\nstatic void process_io_rsp(struct rtrs_clt_path *clt_path, u32 msg_id,\n\t\t\t   s16 errno, bool w_inval)\n{\n\tstruct rtrs_clt_io_req *req;\n\n\tif (WARN_ON(msg_id >= clt_path->queue_depth))\n\t\treturn;\n\n\treq = &clt_path->reqs[msg_id];\n\t/* Drop need_inv if server responded with send with invalidation */\n\treq->need_inv &= !w_inval;\n\tcomplete_rdma_req(req, errno, true, false);\n}\n\nstatic void rtrs_clt_recv_done(struct rtrs_clt_con *con, struct ib_wc *wc)\n{\n\tstruct rtrs_iu *iu;\n\tint err;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tWARN_ON((clt_path->flags & RTRS_MSG_NEW_RKEY_F) == 0);\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu,\n\t\t\t  cqe);\n\terr = rtrs_iu_post_recv(&con->c, iu);\n\tif (err) {\n\t\trtrs_err(con->c.path, \"post iu failed %d\\n\", err);\n\t\trtrs_rdma_error_recovery(con);\n\t}\n}\n\nstatic void rtrs_clt_rkey_rsp_done(struct rtrs_clt_con *con, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_msg_rkey_rsp *msg;\n\tu32 imm_type, imm_payload;\n\tbool w_inval = false;\n\tstruct rtrs_iu *iu;\n\tu32 buf_id;\n\tint err;\n\n\tWARN_ON((clt_path->flags & RTRS_MSG_NEW_RKEY_F) == 0);\n\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\n\tif (wc->byte_len < sizeof(*msg)) {\n\t\trtrs_err(con->c.path, \"rkey response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\tib_dma_sync_single_for_cpu(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t   iu->size, DMA_FROM_DEVICE);\n\tmsg = iu->buf;\n\tif (le16_to_cpu(msg->type) != RTRS_MSG_RKEY_RSP) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"rkey response is malformed: type %d\\n\",\n\t\t\t  le16_to_cpu(msg->type));\n\t\tgoto out;\n\t}\n\tbuf_id = le16_to_cpu(msg->buf_id);\n\tif (WARN_ON(buf_id >= clt_path->queue_depth))\n\t\tgoto out;\n\n\trtrs_from_imm(be32_to_cpu(wc->ex.imm_data), &imm_type, &imm_payload);\n\tif (imm_type == RTRS_IO_RSP_IMM ||\n\t    imm_type == RTRS_IO_RSP_W_INV_IMM) {\n\t\tu32 msg_id;\n\n\t\tw_inval = (imm_type == RTRS_IO_RSP_W_INV_IMM);\n\t\trtrs_from_io_rsp_imm(imm_payload, &msg_id, &err);\n\n\t\tif (WARN_ON(buf_id != msg_id))\n\t\t\tgoto out;\n\t\tclt_path->rbufs[buf_id].rkey = le32_to_cpu(msg->rkey);\n\t\tprocess_io_rsp(clt_path, msg_id, err, w_inval);\n\t}\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t      iu->size, DMA_FROM_DEVICE);\n\treturn rtrs_clt_recv_done(con, wc);\nout:\n\trtrs_rdma_error_recovery(con);\n}\n\nstatic void rtrs_clt_rdma_done(struct ib_cq *cq, struct ib_wc *wc);\n\nstatic struct ib_cqe io_comp_cqe = {\n\t.done = rtrs_clt_rdma_done\n};\n\n/*\n * Post x2 empty WRs: first is for this RDMA with IMM,\n * second is for RECV with INV, which happened earlier.\n */\nstatic int rtrs_post_recv_empty_x2(struct rtrs_con *con, struct ib_cqe *cqe)\n{\n\tstruct ib_recv_wr wr_arr[2], *wr;\n\tint i;\n\n\tmemset(wr_arr, 0, sizeof(wr_arr));\n\tfor (i = 0; i < ARRAY_SIZE(wr_arr); i++) {\n\t\twr = &wr_arr[i];\n\t\twr->wr_cqe  = cqe;\n\t\tif (i)\n\t\t\t/* Chain backwards */\n\t\t\twr->next = &wr_arr[i - 1];\n\t}\n\n\treturn ib_post_recv(con->qp, wr, NULL);\n}\n\nstatic void rtrs_clt_rdma_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tu32 imm_type, imm_payload;\n\tbool w_inval = false;\n\tint err;\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\t\trtrs_err(clt_path->clt, \"RDMA failed: %s\\n\",\n\t\t\t\t  ib_wc_status_msg(wc->status));\n\t\t\trtrs_rdma_error_recovery(con);\n\t\t}\n\t\treturn;\n\t}\n\trtrs_clt_update_wc_stats(con);\n\n\tswitch (wc->opcode) {\n\tcase IB_WC_RECV_RDMA_WITH_IMM:\n\t\t/*\n\t\t * post_recv() RDMA write completions of IO reqs (read/write)\n\t\t * and hb\n\t\t */\n\t\tif (WARN_ON(wc->wr_cqe->done != rtrs_clt_rdma_done))\n\t\t\treturn;\n\t\trtrs_from_imm(be32_to_cpu(wc->ex.imm_data),\n\t\t\t       &imm_type, &imm_payload);\n\t\tif (imm_type == RTRS_IO_RSP_IMM ||\n\t\t    imm_type == RTRS_IO_RSP_W_INV_IMM) {\n\t\t\tu32 msg_id;\n\n\t\t\tw_inval = (imm_type == RTRS_IO_RSP_W_INV_IMM);\n\t\t\trtrs_from_io_rsp_imm(imm_payload, &msg_id, &err);\n\n\t\t\tprocess_io_rsp(clt_path, msg_id, err, w_inval);\n\t\t} else if (imm_type == RTRS_HB_MSG_IMM) {\n\t\t\tWARN_ON(con->c.cid);\n\t\t\trtrs_send_hb_ack(&clt_path->s);\n\t\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\t\t} else if (imm_type == RTRS_HB_ACK_IMM) {\n\t\t\tWARN_ON(con->c.cid);\n\t\t\tclt_path->s.hb_missed_cnt = 0;\n\t\t\tclt_path->s.hb_cur_latency =\n\t\t\t\tktime_sub(ktime_get(), clt_path->s.hb_last_sent);\n\t\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\t\t} else {\n\t\t\trtrs_wrn(con->c.path, \"Unknown IMM type %u\\n\",\n\t\t\t\t  imm_type);\n\t\t}\n\t\tif (w_inval)\n\t\t\t/*\n\t\t\t * Post x2 empty WRs: first is for this RDMA with IMM,\n\t\t\t * second is for RECV with INV, which happened earlier.\n\t\t\t */\n\t\t\terr = rtrs_post_recv_empty_x2(&con->c, &io_comp_cqe);\n\t\telse\n\t\t\terr = rtrs_post_recv_empty(&con->c, &io_comp_cqe);\n\t\tif (err) {\n\t\t\trtrs_err(con->c.path, \"rtrs_post_recv_empty(): %d\\n\",\n\t\t\t\t  err);\n\t\t\trtrs_rdma_error_recovery(con);\n\t\t}\n\t\tbreak;\n\tcase IB_WC_RECV:\n\t\t/*\n\t\t * Key invalidations from server side\n\t\t */\n\t\tWARN_ON(!(wc->wc_flags & IB_WC_WITH_INVALIDATE ||\n\t\t\t  wc->wc_flags & IB_WC_WITH_IMM));\n\t\tWARN_ON(wc->wr_cqe->done != rtrs_clt_rdma_done);\n\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F) {\n\t\t\tif (wc->wc_flags & IB_WC_WITH_INVALIDATE)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\n\t\t\treturn  rtrs_clt_rkey_rsp_done(con, wc);\n\t\t}\n\t\tbreak;\n\tcase IB_WC_RDMA_WRITE:\n\t\t/*\n\t\t * post_send() RDMA write completions of IO reqs (read/write)\n\t\t * and hb.\n\t\t */\n\t\tbreak;\n\n\tdefault:\n\t\trtrs_wrn(clt_path->clt, \"Unexpected WC type: %d\\n\", wc->opcode);\n\t\treturn;\n\t}\n}\n\nstatic int post_recv_io(struct rtrs_clt_con *con, size_t q_size)\n{\n\tint err, i;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tfor (i = 0; i < q_size; i++) {\n\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F) {\n\t\t\tstruct rtrs_iu *iu = &con->rsp_ius[i];\n\n\t\t\terr = rtrs_iu_post_recv(&con->c, iu);\n\t\t} else {\n\t\t\terr = rtrs_post_recv_empty(&con->c, &io_comp_cqe);\n\t\t}\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int post_recv_path(struct rtrs_clt_path *clt_path)\n{\n\tsize_t q_size = 0;\n\tint err, cid;\n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (cid == 0)\n\t\t\tq_size = SERVICE_CON_QUEUE_DEPTH;\n\t\telse\n\t\t\tq_size = clt_path->queue_depth;\n\n\t\t/*\n\t\t * x2 for RDMA read responses + FR key invalidations,\n\t\t * RDMA writes do not require any FR registrations.\n\t\t */\n\t\tq_size *= 2;\n\n\t\terr = post_recv_io(to_clt_con(clt_path->s.con[cid]), q_size);\n\t\tif (err) {\n\t\t\trtrs_err(clt_path->clt, \"post_recv_io(), err: %d\\n\",\n\t\t\t\t err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstruct path_it {\n\tint i;\n\tstruct list_head skip_list;\n\tstruct rtrs_clt_sess *clt;\n\tstruct rtrs_clt_path *(*next_path)(struct path_it *it);\n};\n\n/**\n * list_next_or_null_rr_rcu - get next list element in round-robin fashion.\n * @head:\tthe head for the list.\n * @ptr:        the list head to take the next element from.\n * @type:       the type of the struct this is embedded in.\n * @memb:       the name of the list_head within the struct.\n *\n * Next element returned in round-robin fashion, i.e. head will be skipped,\n * but if list is observed as empty, NULL will be returned.\n *\n * This primitive may safely run concurrently with the _rcu list-mutation\n * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().\n */\n#define list_next_or_null_rr_rcu(head, ptr, type, memb) \\\n({ \\\n\tlist_next_or_null_rcu(head, ptr, type, memb) ?: \\\n\t\tlist_next_or_null_rcu(head, READ_ONCE((ptr)->next), \\\n\t\t\t\t      type, memb); \\\n})\n\n/**\n * get_next_path_rr() - Returns path in round-robin fashion.\n * @it:\tthe path pointer\n *\n * Related to @MP_POLICY_RR\n *\n * Locks:\n *    rcu_read_lock() must be hold.\n */\nstatic struct rtrs_clt_path *get_next_path_rr(struct path_it *it)\n{\n\tstruct rtrs_clt_path __rcu **ppcpu_path;\n\tstruct rtrs_clt_path *path;\n\tstruct rtrs_clt_sess *clt;\n\n\tclt = it->clt;\n\n\t/*\n\t * Here we use two RCU objects: @paths_list and @pcpu_path\n\t * pointer.  See rtrs_clt_remove_path_from_arr() for details\n\t * how that is handled.\n\t */\n\n\tppcpu_path = this_cpu_ptr(clt->pcpu_path);\n\tpath = rcu_dereference(*ppcpu_path);\n\tif (!path)\n\t\tpath = list_first_or_null_rcu(&clt->paths_list,\n\t\t\t\t\t      typeof(*path), s.entry);\n\telse\n\t\tpath = list_next_or_null_rr_rcu(&clt->paths_list,\n\t\t\t\t\t\t&path->s.entry,\n\t\t\t\t\t\ttypeof(*path),\n\t\t\t\t\t\ts.entry);\n\trcu_assign_pointer(*ppcpu_path, path);\n\n\treturn path;\n}\n\n/**\n * get_next_path_min_inflight() - Returns path with minimal inflight count.\n * @it:\tthe path pointer\n *\n * Related to @MP_POLICY_MIN_INFLIGHT\n *\n * Locks:\n *    rcu_read_lock() must be hold.\n */\nstatic struct rtrs_clt_path *get_next_path_min_inflight(struct path_it *it)\n{\n\tstruct rtrs_clt_path *min_path = NULL;\n\tstruct rtrs_clt_sess *clt = it->clt;\n\tstruct rtrs_clt_path *clt_path;\n\tint min_inflight = INT_MAX;\n\tint inflight;\n\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (!list_empty(raw_cpu_ptr(clt_path->mp_skip_entry)))\n\t\t\tcontinue;\n\n\t\tinflight = atomic_read(&clt_path->stats->inflight);\n\n\t\tif (inflight < min_inflight) {\n\t\t\tmin_inflight = inflight;\n\t\t\tmin_path = clt_path;\n\t\t}\n\t}\n\n\t/*\n\t * add the path to the skip list, so that next time we can get\n\t * a different one\n\t */\n\tif (min_path)\n\t\tlist_add(raw_cpu_ptr(min_path->mp_skip_entry), &it->skip_list);\n\n\treturn min_path;\n}\n\n/**\n * get_next_path_min_latency() - Returns path with minimal latency.\n * @it:\tthe path pointer\n *\n * Return: a path with the lowest latency or NULL if all paths are tried\n *\n * Locks:\n *    rcu_read_lock() must be hold.\n *\n * Related to @MP_POLICY_MIN_LATENCY\n *\n * This DOES skip an already-tried path.\n * There is a skip-list to skip a path if the path has tried but failed.\n * It will try the minimum latency path and then the second minimum latency\n * path and so on. Finally it will return NULL if all paths are tried.\n * Therefore the caller MUST check the returned\n * path is NULL and trigger the IO error.\n */\nstatic struct rtrs_clt_path *get_next_path_min_latency(struct path_it *it)\n{\n\tstruct rtrs_clt_path *min_path = NULL;\n\tstruct rtrs_clt_sess *clt = it->clt;\n\tstruct rtrs_clt_path *clt_path;\n\tktime_t min_latency = KTIME_MAX;\n\tktime_t latency;\n\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (!list_empty(raw_cpu_ptr(clt_path->mp_skip_entry)))\n\t\t\tcontinue;\n\n\t\tlatency = clt_path->s.hb_cur_latency;\n\n\t\tif (latency < min_latency) {\n\t\t\tmin_latency = latency;\n\t\t\tmin_path = clt_path;\n\t\t}\n\t}\n\n\t/*\n\t * add the path to the skip list, so that next time we can get\n\t * a different one\n\t */\n\tif (min_path)\n\t\tlist_add(raw_cpu_ptr(min_path->mp_skip_entry), &it->skip_list);\n\n\treturn min_path;\n}\n\nstatic inline void path_it_init(struct path_it *it, struct rtrs_clt_sess *clt)\n{\n\tINIT_LIST_HEAD(&it->skip_list);\n\tit->clt = clt;\n\tit->i = 0;\n\n\tif (clt->mp_policy == MP_POLICY_RR)\n\t\tit->next_path = get_next_path_rr;\n\telse if (clt->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\tit->next_path = get_next_path_min_inflight;\n\telse\n\t\tit->next_path = get_next_path_min_latency;\n}\n\nstatic inline void path_it_deinit(struct path_it *it)\n{\n\tstruct list_head *skip, *tmp;\n\t/*\n\t * The skip_list is used only for the MIN_INFLIGHT policy.\n\t * We need to remove paths from it, so that next IO can insert\n\t * paths (->mp_skip_entry) into a skip_list again.\n\t */\n\tlist_for_each_safe(skip, tmp, &it->skip_list)\n\t\tlist_del_init(skip);\n}\n\n/**\n * rtrs_clt_init_req() - Initialize an rtrs_clt_io_req holding information\n * about an inflight IO.\n * The user buffer holding user control message (not data) is copied into\n * the corresponding buffer of rtrs_iu (req->iu->buf), which later on will\n * also hold the control message of rtrs.\n * @req: an io request holding information about IO.\n * @clt_path: client path\n * @conf: conformation callback function to notify upper layer.\n * @permit: permit for allocation of RDMA remote buffer\n * @priv: private pointer\n * @vec: kernel vector containing control message\n * @usr_len: length of the user message\n * @sg: scater list for IO data\n * @sg_cnt: number of scater list entries\n * @data_len: length of the IO data\n * @dir: direction of the IO.\n */\nstatic void rtrs_clt_init_req(struct rtrs_clt_io_req *req,\n\t\t\t      struct rtrs_clt_path *clt_path,\n\t\t\t      void (*conf)(void *priv, int errno),\n\t\t\t      struct rtrs_permit *permit, void *priv,\n\t\t\t      const struct kvec *vec, size_t usr_len,\n\t\t\t      struct scatterlist *sg, size_t sg_cnt,\n\t\t\t      size_t data_len, int dir)\n{\n\tstruct iov_iter iter;\n\tsize_t len;\n\n\treq->permit = permit;\n\treq->in_use = true;\n\treq->usr_len = usr_len;\n\treq->data_len = data_len;\n\treq->sglist = sg;\n\treq->sg_cnt = sg_cnt;\n\treq->priv = priv;\n\treq->dir = dir;\n\treq->con = rtrs_permit_to_clt_con(clt_path, permit);\n\treq->conf = conf;\n\treq->need_inv = false;\n\treq->need_inv_comp = false;\n\treq->inv_errno = 0;\n\trefcount_set(&req->ref, 1);\n\treq->mp_policy = clt_path->clt->mp_policy;\n\n\tiov_iter_kvec(&iter, READ, vec, 1, usr_len);\n\tlen = _copy_from_iter(req->iu->buf, usr_len, &iter);\n\tWARN_ON(len != usr_len);\n\n\treinit_completion(&req->inv_comp);\n}\n\nstatic struct rtrs_clt_io_req *\nrtrs_clt_get_req(struct rtrs_clt_path *clt_path,\n\t\t void (*conf)(void *priv, int errno),\n\t\t struct rtrs_permit *permit, void *priv,\n\t\t const struct kvec *vec, size_t usr_len,\n\t\t struct scatterlist *sg, size_t sg_cnt,\n\t\t size_t data_len, int dir)\n{\n\tstruct rtrs_clt_io_req *req;\n\n\treq = &clt_path->reqs[permit->mem_id];\n\trtrs_clt_init_req(req, clt_path, conf, permit, priv, vec, usr_len,\n\t\t\t   sg, sg_cnt, data_len, dir);\n\treturn req;\n}\n\nstatic struct rtrs_clt_io_req *\nrtrs_clt_get_copy_req(struct rtrs_clt_path *alive_path,\n\t\t       struct rtrs_clt_io_req *fail_req)\n{\n\tstruct rtrs_clt_io_req *req;\n\tstruct kvec vec = {\n\t\t.iov_base = fail_req->iu->buf,\n\t\t.iov_len  = fail_req->usr_len\n\t};\n\n\treq = &alive_path->reqs[fail_req->permit->mem_id];\n\trtrs_clt_init_req(req, alive_path, fail_req->conf, fail_req->permit,\n\t\t\t   fail_req->priv, &vec, fail_req->usr_len,\n\t\t\t   fail_req->sglist, fail_req->sg_cnt,\n\t\t\t   fail_req->data_len, fail_req->dir);\n\treturn req;\n}\n\nstatic int rtrs_post_rdma_write_sg(struct rtrs_clt_con *con,\n\t\t\t\t   struct rtrs_clt_io_req *req,\n\t\t\t\t   struct rtrs_rbuf *rbuf, bool fr_en,\n\t\t\t\t   u32 size, u32 imm, struct ib_send_wr *wr,\n\t\t\t\t   struct ib_send_wr *tail)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct ib_sge *sge = req->sge;\n\tenum ib_send_flags flags;\n\tstruct scatterlist *sg;\n\tsize_t num_sge;\n\tint i;\n\tstruct ib_send_wr *ptail = NULL;\n\n\tif (fr_en) {\n\t\ti = 0;\n\t\tsge[i].addr   = req->mr->iova;\n\t\tsge[i].length = req->mr->length;\n\t\tsge[i].lkey   = req->mr->lkey;\n\t\ti++;\n\t\tnum_sge = 2;\n\t\tptail = tail;\n\t} else {\n\t\tfor_each_sg(req->sglist, sg, req->sg_cnt, i) {\n\t\t\tsge[i].addr   = sg_dma_address(sg);\n\t\t\tsge[i].length = sg_dma_len(sg);\n\t\t\tsge[i].lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\t\t}\n\t\tnum_sge = 1 + req->sg_cnt;\n\t}\n\tsge[i].addr   = req->iu->dma_addr;\n\tsge[i].length = size;\n\tsge[i].lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\n\t/*\n\t * From time to time we have to post signalled sends,\n\t * or send queue will fill up and only QP reset can help.\n\t */\n\tflags = atomic_inc_return(&con->c.wr_cnt) % clt_path->s.signal_interval ?\n\t\t\t0 : IB_SEND_SIGNALED;\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      req->iu->dma_addr,\n\t\t\t\t      size, DMA_TO_DEVICE);\n\n\treturn rtrs_iu_post_rdma_write_imm(&con->c, req->iu, sge, num_sge,\n\t\t\t\t\t    rbuf->rkey, rbuf->addr, imm,\n\t\t\t\t\t    flags, wr, ptail);\n}\n\nstatic int rtrs_map_sg_fr(struct rtrs_clt_io_req *req, size_t count)\n{\n\tint nr;\n\n\t/* Align the MR to a 4K page size to match the block virt boundary */\n\tnr = ib_map_mr_sg(req->mr, req->sglist, count, NULL, SZ_4K);\n\tif (nr < 0)\n\t\treturn nr;\n\tif (nr < req->sg_cnt)\n\t\treturn -EINVAL;\n\tib_update_fast_reg_key(req->mr, ib_inc_rkey(req->mr->rkey));\n\n\treturn nr;\n}\n\nstatic int rtrs_clt_write_req(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rtrs_msg_rdma_write *msg;\n\n\tstruct rtrs_rbuf *rbuf;\n\tint ret, count = 0;\n\tu32 imm, buf_id;\n\tstruct ib_reg_wr rwr;\n\tstruct ib_send_wr inv_wr;\n\tstruct ib_send_wr *wr = NULL;\n\tbool fr_en = false;\n\n\tconst size_t tsize = sizeof(*msg) + req->data_len + req->usr_len;\n\n\tif (tsize > clt_path->chunk_size) {\n\t\trtrs_wrn(s, \"Write request failed, size too big %zu > %d\\n\",\n\t\t\t  tsize, clt_path->chunk_size);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (req->sg_cnt) {\n\t\tcount = ib_dma_map_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t      req->sg_cnt, req->dir);\n\t\tif (!count) {\n\t\t\trtrs_wrn(s, \"Write request failed, map failed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\t/* put rtrs msg after sg and user message */\n\tmsg = req->iu->buf + req->usr_len;\n\tmsg->type = cpu_to_le16(RTRS_MSG_WRITE);\n\tmsg->usr_len = cpu_to_le16(req->usr_len);\n\n\t/* rtrs message on server side will be after user data and message */\n\timm = req->permit->mem_off + req->data_len + req->usr_len;\n\timm = rtrs_to_io_req_imm(imm);\n\tbuf_id = req->permit->mem_id;\n\treq->sg_size = tsize;\n\trbuf = &clt_path->rbufs[buf_id];\n\n\tif (count) {\n\t\tret = rtrs_map_sg_fr(req, count);\n\t\tif (ret < 0) {\n\t\t\trtrs_err_rl(s,\n\t\t\t\t    \"Write request failed, failed to map fast reg. data, err: %d\\n\",\n\t\t\t\t    ret);\n\t\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t\t\treturn ret;\n\t\t}\n\t\tinv_wr = (struct ib_send_wr) {\n\t\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t\t.wr_cqe\t\t    = &req->inv_cqe,\n\t\t\t.send_flags\t    = IB_SEND_SIGNALED,\n\t\t\t.ex.invalidate_rkey = req->mr->rkey,\n\t\t};\n\t\treq->inv_cqe.done = rtrs_clt_inv_rkey_done;\n\t\trwr = (struct ib_reg_wr) {\n\t\t\t.wr.opcode = IB_WR_REG_MR,\n\t\t\t.wr.wr_cqe = &fast_reg_cqe,\n\t\t\t.mr = req->mr,\n\t\t\t.key = req->mr->rkey,\n\t\t\t.access = (IB_ACCESS_LOCAL_WRITE),\n\t\t};\n\t\twr = &rwr.wr;\n\t\tfr_en = true;\n\t\trefcount_inc(&req->ref);\n\t}\n\t/*\n\t * Update stats now, after request is successfully sent it is not\n\t * safe anymore to touch it.\n\t */\n\trtrs_clt_update_all_stats(req, WRITE);\n\n\tret = rtrs_post_rdma_write_sg(req->con, req, rbuf, fr_en,\n\t\t\t\t      req->usr_len + sizeof(*msg),\n\t\t\t\t      imm, wr, &inv_wr);\n\tif (ret) {\n\t\trtrs_err_rl(s,\n\t\t\t    \"Write request failed: error=%d path=%s [%s:%u]\\n\",\n\t\t\t    ret, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port);\n\t\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\t\tatomic_dec(&clt_path->stats->inflight);\n\t\tif (req->sg_cnt)\n\t\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\n\treturn ret;\n}\n\nstatic int rtrs_clt_read_req(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rtrs_msg_rdma_read *msg;\n\tstruct rtrs_ib_dev *dev = clt_path->s.dev;\n\n\tstruct ib_reg_wr rwr;\n\tstruct ib_send_wr *wr = NULL;\n\n\tint ret, count = 0;\n\tu32 imm, buf_id;\n\n\tconst size_t tsize = sizeof(*msg) + req->data_len + req->usr_len;\n\n\tif (tsize > clt_path->chunk_size) {\n\t\trtrs_wrn(s,\n\t\t\t  \"Read request failed, message size is %zu, bigger than CHUNK_SIZE %d\\n\",\n\t\t\t  tsize, clt_path->chunk_size);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (req->sg_cnt) {\n\t\tcount = ib_dma_map_sg(dev->ib_dev, req->sglist, req->sg_cnt,\n\t\t\t\t      req->dir);\n\t\tif (!count) {\n\t\t\trtrs_wrn(s,\n\t\t\t\t  \"Read request failed, dma map failed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\t/* put our message into req->buf after user message*/\n\tmsg = req->iu->buf + req->usr_len;\n\tmsg->type = cpu_to_le16(RTRS_MSG_READ);\n\tmsg->usr_len = cpu_to_le16(req->usr_len);\n\n\tif (count) {\n\t\tret = rtrs_map_sg_fr(req, count);\n\t\tif (ret < 0) {\n\t\t\trtrs_err_rl(s,\n\t\t\t\t     \"Read request failed, failed to map  fast reg. data, err: %d\\n\",\n\t\t\t\t     ret);\n\t\t\tib_dma_unmap_sg(dev->ib_dev, req->sglist, req->sg_cnt,\n\t\t\t\t\treq->dir);\n\t\t\treturn ret;\n\t\t}\n\t\trwr = (struct ib_reg_wr) {\n\t\t\t.wr.opcode = IB_WR_REG_MR,\n\t\t\t.wr.wr_cqe = &fast_reg_cqe,\n\t\t\t.mr = req->mr,\n\t\t\t.key = req->mr->rkey,\n\t\t\t.access = (IB_ACCESS_LOCAL_WRITE |\n\t\t\t\t   IB_ACCESS_REMOTE_WRITE),\n\t\t};\n\t\twr = &rwr.wr;\n\n\t\tmsg->sg_cnt = cpu_to_le16(1);\n\t\tmsg->flags = cpu_to_le16(RTRS_MSG_NEED_INVAL_F);\n\n\t\tmsg->desc[0].addr = cpu_to_le64(req->mr->iova);\n\t\tmsg->desc[0].key = cpu_to_le32(req->mr->rkey);\n\t\tmsg->desc[0].len = cpu_to_le32(req->mr->length);\n\n\t\t/* Further invalidation is required */\n\t\treq->need_inv = !!RTRS_MSG_NEED_INVAL_F;\n\n\t} else {\n\t\tmsg->sg_cnt = 0;\n\t\tmsg->flags = 0;\n\t}\n\t/*\n\t * rtrs message will be after the space reserved for disk data and\n\t * user message\n\t */\n\timm = req->permit->mem_off + req->data_len + req->usr_len;\n\timm = rtrs_to_io_req_imm(imm);\n\tbuf_id = req->permit->mem_id;\n\n\treq->sg_size  = sizeof(*msg);\n\treq->sg_size += le16_to_cpu(msg->sg_cnt) * sizeof(struct rtrs_sg_desc);\n\treq->sg_size += req->usr_len;\n\n\t/*\n\t * Update stats now, after request is successfully sent it is not\n\t * safe anymore to touch it.\n\t */\n\trtrs_clt_update_all_stats(req, READ);\n\n\tret = rtrs_post_send_rdma(req->con, req, &clt_path->rbufs[buf_id],\n\t\t\t\t   req->data_len, imm, wr);\n\tif (ret) {\n\t\trtrs_err_rl(s,\n\t\t\t    \"Read request failed: error=%d path=%s [%s:%u]\\n\",\n\t\t\t    ret, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port);\n\t\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\t\tatomic_dec(&clt_path->stats->inflight);\n\t\treq->need_inv = false;\n\t\tif (req->sg_cnt)\n\t\t\tib_dma_unmap_sg(dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\n\treturn ret;\n}\n\n/**\n * rtrs_clt_failover_req() - Try to find an active path for a failed request\n * @clt: clt context\n * @fail_req: a failed io request.\n */\nstatic int rtrs_clt_failover_req(struct rtrs_clt_sess *clt,\n\t\t\t\t struct rtrs_clt_io_req *fail_req)\n{\n\tstruct rtrs_clt_path *alive_path;\n\tstruct rtrs_clt_io_req *req;\n\tint err = -ECONNABORTED;\n\tstruct path_it it;\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (alive_path = it.next_path(&it)) && it.i < it.clt->paths_num;\n\t     it.i++) {\n\t\tif (READ_ONCE(alive_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\t\treq = rtrs_clt_get_copy_req(alive_path, fail_req);\n\t\tif (req->dir == DMA_TO_DEVICE)\n\t\t\terr = rtrs_clt_write_req(req);\n\t\telse\n\t\t\terr = rtrs_clt_read_req(req);\n\t\tif (err) {\n\t\t\treq->in_use = false;\n\t\t\tcontinue;\n\t\t}\n\t\t/* Success path */\n\t\trtrs_clt_inc_failover_cnt(alive_path->stats);\n\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic void fail_all_outstanding_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_clt_io_req *req;\n\tint i, err;\n\n\tif (!clt_path->reqs)\n\t\treturn;\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\tif (!req->in_use)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Safely (without notification) complete failed request.\n\t\t * After completion this request is still useble and can\n\t\t * be failovered to another path.\n\t\t */\n\t\tcomplete_rdma_req(req, -ECONNABORTED, false, true);\n\n\t\terr = rtrs_clt_failover_req(clt, req);\n\t\tif (err)\n\t\t\t/* Failover failed, notify anyway */\n\t\t\treq->conf(req->priv, err);\n\t}\n}\n\nstatic void free_path_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_io_req *req;\n\tint i;\n\n\tif (!clt_path->reqs)\n\t\treturn;\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\tif (req->mr)\n\t\t\tib_dereg_mr(req->mr);\n\t\tkfree(req->sge);\n\t\trtrs_iu_free(req->iu, clt_path->s.dev->ib_dev, 1);\n\t}\n\tkfree(clt_path->reqs);\n\tclt_path->reqs = NULL;\n}\n\nstatic int alloc_path_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_io_req *req;\n\tint i, err = -ENOMEM;\n\n\tclt_path->reqs = kcalloc(clt_path->queue_depth,\n\t\t\t\t sizeof(*clt_path->reqs),\n\t\t\t\t GFP_KERNEL);\n\tif (!clt_path->reqs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\treq->iu = rtrs_iu_alloc(1, clt_path->max_hdr_size, GFP_KERNEL,\n\t\t\t\t\t clt_path->s.dev->ib_dev,\n\t\t\t\t\t DMA_TO_DEVICE,\n\t\t\t\t\t rtrs_clt_rdma_done);\n\t\tif (!req->iu)\n\t\t\tgoto out;\n\n\t\treq->sge = kcalloc(2, sizeof(*req->sge), GFP_KERNEL);\n\t\tif (!req->sge)\n\t\t\tgoto out;\n\n\t\treq->mr = ib_alloc_mr(clt_path->s.dev->ib_pd,\n\t\t\t\t      IB_MR_TYPE_MEM_REG,\n\t\t\t\t      clt_path->max_pages_per_mr);\n\t\tif (IS_ERR(req->mr)) {\n\t\t\terr = PTR_ERR(req->mr);\n\t\t\treq->mr = NULL;\n\t\t\tpr_err(\"Failed to alloc clt_path->max_pages_per_mr %d\\n\",\n\t\t\t       clt_path->max_pages_per_mr);\n\t\t\tgoto out;\n\t\t}\n\n\t\tinit_completion(&req->inv_comp);\n\t}\n\n\treturn 0;\n\nout:\n\tfree_path_reqs(clt_path);\n\n\treturn err;\n}\n\nstatic int alloc_permits(struct rtrs_clt_sess *clt)\n{\n\tunsigned int chunk_bits;\n\tint err, i;\n\n\tclt->permits_map = kcalloc(BITS_TO_LONGS(clt->queue_depth),\n\t\t\t\t   sizeof(long), GFP_KERNEL);\n\tif (!clt->permits_map) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\tclt->permits = kcalloc(clt->queue_depth, permit_size(clt), GFP_KERNEL);\n\tif (!clt->permits) {\n\t\terr = -ENOMEM;\n\t\tgoto err_map;\n\t}\n\tchunk_bits = ilog2(clt->queue_depth - 1) + 1;\n\tfor (i = 0; i < clt->queue_depth; i++) {\n\t\tstruct rtrs_permit *permit;\n\n\t\tpermit = get_permit(clt, i);\n\t\tpermit->mem_id = i;\n\t\tpermit->mem_off = i << (MAX_IMM_PAYL_BITS - chunk_bits);\n\t}\n\n\treturn 0;\n\nerr_map:\n\tkfree(clt->permits_map);\n\tclt->permits_map = NULL;\nout_err:\n\treturn err;\n}\n\nstatic void free_permits(struct rtrs_clt_sess *clt)\n{\n\tif (clt->permits_map) {\n\t\tsize_t sz = clt->queue_depth;\n\n\t\twait_event(clt->permits_wait,\n\t\t\t   find_first_bit(clt->permits_map, sz) >= sz);\n\t}\n\tkfree(clt->permits_map);\n\tclt->permits_map = NULL;\n\tkfree(clt->permits);\n\tclt->permits = NULL;\n}\n\nstatic void query_fast_reg_mode(struct rtrs_clt_path *clt_path)\n{\n\tstruct ib_device *ib_dev;\n\tu64 max_pages_per_mr;\n\tint mr_page_shift;\n\n\tib_dev = clt_path->s.dev->ib_dev;\n\n\t/*\n\t * Use the smallest page size supported by the HCA, down to a\n\t * minimum of 4096 bytes. We're unlikely to build large sglists\n\t * out of smaller entries.\n\t */\n\tmr_page_shift      = max(12, ffs(ib_dev->attrs.page_size_cap) - 1);\n\tmax_pages_per_mr   = ib_dev->attrs.max_mr_size;\n\tdo_div(max_pages_per_mr, (1ull << mr_page_shift));\n\tclt_path->max_pages_per_mr =\n\t\tmin3(clt_path->max_pages_per_mr, (u32)max_pages_per_mr,\n\t\t     ib_dev->attrs.max_fast_reg_page_list_len);\n\tclt_path->clt->max_segments =\n\t\tmin(clt_path->max_pages_per_mr, clt_path->clt->max_segments);\n}\n\nstatic bool rtrs_clt_change_state_get_old(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t   enum rtrs_clt_state new_state,\n\t\t\t\t\t   enum rtrs_clt_state *old_state)\n{\n\tbool changed;\n\n\tspin_lock_irq(&clt_path->state_wq.lock);\n\tif (old_state)\n\t\t*old_state = clt_path->state;\n\tchanged = rtrs_clt_change_state(clt_path, new_state);\n\tspin_unlock_irq(&clt_path->state_wq.lock);\n\n\treturn changed;\n}\n\nstatic void rtrs_clt_hb_err_handler(struct rtrs_con *c)\n{\n\tstruct rtrs_clt_con *con = container_of(c, typeof(*con), c);\n\n\trtrs_rdma_error_recovery(con);\n}\n\nstatic void rtrs_clt_init_hb(struct rtrs_clt_path *clt_path)\n{\n\trtrs_init_hb(&clt_path->s, &io_comp_cqe,\n\t\t      RTRS_HB_INTERVAL_MS,\n\t\t      RTRS_HB_MISSED_MAX,\n\t\t      rtrs_clt_hb_err_handler,\n\t\t      rtrs_wq);\n}\n\nstatic void rtrs_clt_reconnect_work(struct work_struct *work);\nstatic void rtrs_clt_close_work(struct work_struct *work);\n\nstatic struct rtrs_clt_path *alloc_path(struct rtrs_clt_sess *clt,\n\t\t\t\t\tconst struct rtrs_addr *path,\n\t\t\t\t\tsize_t con_num, u32 nr_poll_queues)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tint err = -ENOMEM;\n\tint cpu;\n\tsize_t total_con;\n\n\tclt_path = kzalloc(sizeof(*clt_path), GFP_KERNEL);\n\tif (!clt_path)\n\t\tgoto err;\n\n\t/*\n\t * irqmode and poll\n\t * +1: Extra connection for user messages\n\t */\n\ttotal_con = con_num + nr_poll_queues + 1;\n\tclt_path->s.con = kcalloc(total_con, sizeof(*clt_path->s.con),\n\t\t\t\t  GFP_KERNEL);\n\tif (!clt_path->s.con)\n\t\tgoto err_free_path;\n\n\tclt_path->s.con_num = total_con;\n\tclt_path->s.irq_con_num = con_num + 1;\n\n\tclt_path->stats = kzalloc(sizeof(*clt_path->stats), GFP_KERNEL);\n\tif (!clt_path->stats)\n\t\tgoto err_free_con;\n\n\tmutex_init(&clt_path->init_mutex);\n\tuuid_gen(&clt_path->s.uuid);\n\tmemcpy(&clt_path->s.dst_addr, path->dst,\n\t       rdma_addr_size((struct sockaddr *)path->dst));\n\n\t/*\n\t * rdma_resolve_addr() passes src_addr to cma_bind_addr, which\n\t * checks the sa_family to be non-zero. If user passed src_addr=NULL\n\t * the sess->src_addr will contain only zeros, which is then fine.\n\t */\n\tif (path->src)\n\t\tmemcpy(&clt_path->s.src_addr, path->src,\n\t\t       rdma_addr_size((struct sockaddr *)path->src));\n\tstrscpy(clt_path->s.sessname, clt->sessname,\n\t\tsizeof(clt_path->s.sessname));\n\tclt_path->clt = clt;\n\tclt_path->max_pages_per_mr = RTRS_MAX_SEGMENTS;\n\tinit_waitqueue_head(&clt_path->state_wq);\n\tclt_path->state = RTRS_CLT_CONNECTING;\n\tatomic_set(&clt_path->connected_cnt, 0);\n\tINIT_WORK(&clt_path->close_work, rtrs_clt_close_work);\n\tINIT_DELAYED_WORK(&clt_path->reconnect_dwork, rtrs_clt_reconnect_work);\n\trtrs_clt_init_hb(clt_path);\n\n\tclt_path->mp_skip_entry = alloc_percpu(typeof(*clt_path->mp_skip_entry));\n\tif (!clt_path->mp_skip_entry)\n\t\tgoto err_free_stats;\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_LIST_HEAD(per_cpu_ptr(clt_path->mp_skip_entry, cpu));\n\n\terr = rtrs_clt_init_stats(clt_path->stats);\n\tif (err)\n\t\tgoto err_free_percpu;\n\n\treturn clt_path;\n\nerr_free_percpu:\n\tfree_percpu(clt_path->mp_skip_entry);\nerr_free_stats:\n\tkfree(clt_path->stats);\nerr_free_con:\n\tkfree(clt_path->s.con);\nerr_free_path:\n\tkfree(clt_path);\nerr:\n\treturn ERR_PTR(err);\n}\n\nvoid free_path(struct rtrs_clt_path *clt_path)\n{\n\tfree_percpu(clt_path->mp_skip_entry);\n\tmutex_destroy(&clt_path->init_mutex);\n\tkfree(clt_path->s.con);\n\tkfree(clt_path->rbufs);\n\tkfree(clt_path);\n}\n\nstatic int create_con(struct rtrs_clt_path *clt_path, unsigned int cid)\n{\n\tstruct rtrs_clt_con *con;\n\n\tcon = kzalloc(sizeof(*con), GFP_KERNEL);\n\tif (!con)\n\t\treturn -ENOMEM;\n\n\t/* Map first two connections to the first CPU */\n\tcon->cpu  = (cid ? cid - 1 : 0) % nr_cpu_ids;\n\tcon->c.cid = cid;\n\tcon->c.path = &clt_path->s;\n\t/* Align with srv, init as 1 */\n\tatomic_set(&con->c.wr_cnt, 1);\n\tmutex_init(&con->con_mutex);\n\n\tclt_path->s.con[cid] = &con->c;\n\n\treturn 0;\n}\n\nstatic void destroy_con(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tclt_path->s.con[con->c.cid] = NULL;\n\tmutex_destroy(&con->con_mutex);\n\tkfree(con);\n}\n\nstatic int create_con_cq_qp(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tu32 max_send_wr, max_recv_wr, cq_num, max_send_sge, wr_limit;\n\tint err, cq_vector;\n\tstruct rtrs_msg_rkey_rsp *rsp;\n\n\tlockdep_assert_held(&con->con_mutex);\n\tif (con->c.cid == 0) {\n\t\tmax_send_sge = 1;\n\t\t/* We must be the first here */\n\t\tif (WARN_ON(clt_path->s.dev))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * The whole session uses device from user connection.\n\t\t * Be careful not to close user connection before ib dev\n\t\t * is gracefully put.\n\t\t */\n\t\tclt_path->s.dev = rtrs_ib_dev_find_or_add(con->c.cm_id->device,\n\t\t\t\t\t\t       &dev_pd);\n\t\tif (!clt_path->s.dev) {\n\t\t\trtrs_wrn(clt_path->clt,\n\t\t\t\t  \"rtrs_ib_dev_find_get_or_add(): no memory\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tclt_path->s.dev_ref = 1;\n\t\tquery_fast_reg_mode(clt_path);\n\t\twr_limit = clt_path->s.dev->ib_dev->attrs.max_qp_wr;\n\t\t/*\n\t\t * Two (request + registration) completion for send\n\t\t * Two for recv if always_invalidate is set on server\n\t\t * or one for recv.\n\t\t * + 2 for drain and heartbeat\n\t\t * in case qp gets into error state.\n\t\t */\n\t\tmax_send_wr =\n\t\t\tmin_t(int, wr_limit, SERVICE_CON_QUEUE_DEPTH * 2 + 2);\n\t\tmax_recv_wr = max_send_wr;\n\t} else {\n\t\t/*\n\t\t * Here we assume that session members are correctly set.\n\t\t * This is always true if user connection (cid == 0) is\n\t\t * established first.\n\t\t */\n\t\tif (WARN_ON(!clt_path->s.dev))\n\t\t\treturn -EINVAL;\n\t\tif (WARN_ON(!clt_path->queue_depth))\n\t\t\treturn -EINVAL;\n\n\t\twr_limit = clt_path->s.dev->ib_dev->attrs.max_qp_wr;\n\t\t/* Shared between connections */\n\t\tclt_path->s.dev_ref++;\n\t\tmax_send_wr = min_t(int, wr_limit,\n\t\t\t      /* QD * (REQ + RSP + FR REGS or INVS) + drain */\n\t\t\t      clt_path->queue_depth * 3 + 1);\n\t\tmax_recv_wr = min_t(int, wr_limit,\n\t\t\t      clt_path->queue_depth * 3 + 1);\n\t\tmax_send_sge = 2;\n\t}\n\tatomic_set(&con->c.sq_wr_avail, max_send_wr);\n\tcq_num = max_send_wr + max_recv_wr;\n\t/* alloc iu to recv new rkey reply when server reports flags set */\n\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F || con->c.cid == 0) {\n\t\tcon->rsp_ius = rtrs_iu_alloc(cq_num, sizeof(*rsp),\n\t\t\t\t\t      GFP_KERNEL,\n\t\t\t\t\t      clt_path->s.dev->ib_dev,\n\t\t\t\t\t      DMA_FROM_DEVICE,\n\t\t\t\t\t      rtrs_clt_rdma_done);\n\t\tif (!con->rsp_ius)\n\t\t\treturn -ENOMEM;\n\t\tcon->queue_num = cq_num;\n\t}\n\tcq_num = max_send_wr + max_recv_wr;\n\tcq_vector = con->cpu % clt_path->s.dev->ib_dev->num_comp_vectors;\n\tif (con->c.cid >= clt_path->s.irq_con_num)\n\t\terr = rtrs_cq_qp_create(&clt_path->s, &con->c, max_send_sge,\n\t\t\t\t\tcq_vector, cq_num, max_send_wr,\n\t\t\t\t\tmax_recv_wr, IB_POLL_DIRECT);\n\telse\n\t\terr = rtrs_cq_qp_create(&clt_path->s, &con->c, max_send_sge,\n\t\t\t\t\tcq_vector, cq_num, max_send_wr,\n\t\t\t\t\tmax_recv_wr, IB_POLL_SOFTIRQ);\n\t/*\n\t * In case of error we do not bother to clean previous allocations,\n\t * since destroy_con_cq_qp() must be called.\n\t */\n\treturn err;\n}\n\nstatic void destroy_con_cq_qp(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\t/*\n\t * Be careful here: destroy_con_cq_qp() can be called even\n\t * create_con_cq_qp() failed, see comments there.\n\t */\n\tlockdep_assert_held(&con->con_mutex);\n\trtrs_cq_qp_destroy(&con->c);\n\tif (con->rsp_ius) {\n\t\trtrs_iu_free(con->rsp_ius, clt_path->s.dev->ib_dev,\n\t\t\t     con->queue_num);\n\t\tcon->rsp_ius = NULL;\n\t\tcon->queue_num = 0;\n\t}\n\tif (clt_path->s.dev_ref && !--clt_path->s.dev_ref) {\n\t\trtrs_ib_dev_put(clt_path->s.dev);\n\t\tclt_path->s.dev = NULL;\n\t}\n}\n\nstatic void stop_cm(struct rtrs_clt_con *con)\n{\n\trdma_disconnect(con->c.cm_id);\n\tif (con->c.qp)\n\t\tib_drain_qp(con->c.qp);\n}\n\nstatic void destroy_cm(struct rtrs_clt_con *con)\n{\n\trdma_destroy_id(con->c.cm_id);\n\tcon->c.cm_id = NULL;\n}\n\nstatic int rtrs_rdma_addr_resolved(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tint err;\n\n\tmutex_lock(&con->con_mutex);\n\terr = create_con_cq_qp(con);\n\tmutex_unlock(&con->con_mutex);\n\tif (err) {\n\t\trtrs_err(s, \"create_con_cq_qp(), err: %d\\n\", err);\n\t\treturn err;\n\t}\n\terr = rdma_resolve_route(con->c.cm_id, RTRS_CONNECT_TIMEOUT_MS);\n\tif (err)\n\t\trtrs_err(s, \"Resolving route failed, err: %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int rtrs_rdma_route_resolved(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_msg_conn_req msg;\n\tstruct rdma_conn_param param;\n\n\tint err;\n\n\tparam = (struct rdma_conn_param) {\n\t\t.retry_count = 7,\n\t\t.rnr_retry_count = 7,\n\t\t.private_data = &msg,\n\t\t.private_data_len = sizeof(msg),\n\t};\n\n\tmsg = (struct rtrs_msg_conn_req) {\n\t\t.magic = cpu_to_le16(RTRS_MAGIC),\n\t\t.version = cpu_to_le16(RTRS_PROTO_VER),\n\t\t.cid = cpu_to_le16(con->c.cid),\n\t\t.cid_num = cpu_to_le16(clt_path->s.con_num),\n\t\t.recon_cnt = cpu_to_le16(clt_path->s.recon_cnt),\n\t};\n\tmsg.first_conn = clt_path->for_new_clt ? FIRST_CONN : 0;\n\tuuid_copy(&msg.sess_uuid, &clt_path->s.uuid);\n\tuuid_copy(&msg.paths_uuid, &clt->paths_uuid);\n\n\terr = rdma_connect_locked(con->c.cm_id, &param);\n\tif (err)\n\t\trtrs_err(clt, \"rdma_connect_locked(): %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int rtrs_rdma_conn_established(struct rtrs_clt_con *con,\n\t\t\t\t       struct rdma_cm_event *ev)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tconst struct rtrs_msg_conn_rsp *msg;\n\tu16 version, queue_depth;\n\tint errno;\n\tu8 len;\n\n\tmsg = ev->param.conn.private_data;\n\tlen = ev->param.conn.private_data_len;\n\tif (len < sizeof(*msg)) {\n\t\trtrs_err(clt, \"Invalid RTRS connection response\\n\");\n\t\treturn -ECONNRESET;\n\t}\n\tif (le16_to_cpu(msg->magic) != RTRS_MAGIC) {\n\t\trtrs_err(clt, \"Invalid RTRS magic\\n\");\n\t\treturn -ECONNRESET;\n\t}\n\tversion = le16_to_cpu(msg->version);\n\tif (version >> 8 != RTRS_PROTO_VER_MAJOR) {\n\t\trtrs_err(clt, \"Unsupported major RTRS version: %d, expected %d\\n\",\n\t\t\t  version >> 8, RTRS_PROTO_VER_MAJOR);\n\t\treturn -ECONNRESET;\n\t}\n\terrno = le16_to_cpu(msg->errno);\n\tif (errno) {\n\t\trtrs_err(clt, \"Invalid RTRS message: errno %d\\n\",\n\t\t\t  errno);\n\t\treturn -ECONNRESET;\n\t}\n\tif (con->c.cid == 0) {\n\t\tqueue_depth = le16_to_cpu(msg->queue_depth);\n\n\t\tif (clt_path->queue_depth > 0 && queue_depth != clt_path->queue_depth) {\n\t\t\trtrs_err(clt, \"Error: queue depth changed\\n\");\n\n\t\t\t/*\n\t\t\t * Stop any more reconnection attempts\n\t\t\t */\n\t\t\tclt_path->reconnect_attempts = -1;\n\t\t\trtrs_err(clt,\n\t\t\t\t\"Disabling auto-reconnect. Trigger a manual reconnect after issue is resolved\\n\");\n\t\t\treturn -ECONNRESET;\n\t\t}\n\n\t\tif (!clt_path->rbufs) {\n\t\t\tclt_path->rbufs = kcalloc(queue_depth,\n\t\t\t\t\t\t  sizeof(*clt_path->rbufs),\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\t\tif (!clt_path->rbufs)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tclt_path->queue_depth = queue_depth;\n\t\tclt_path->s.signal_interval = min_not_zero(queue_depth,\n\t\t\t\t\t\t(unsigned short) SERVICE_CON_QUEUE_DEPTH);\n\t\tclt_path->max_hdr_size = le32_to_cpu(msg->max_hdr_size);\n\t\tclt_path->max_io_size = le32_to_cpu(msg->max_io_size);\n\t\tclt_path->flags = le32_to_cpu(msg->flags);\n\t\tclt_path->chunk_size = clt_path->max_io_size + clt_path->max_hdr_size;\n\n\t\t/*\n\t\t * Global IO size is always a minimum.\n\t\t * If while a reconnection server sends us a value a bit\n\t\t * higher - client does not care and uses cached minimum.\n\t\t *\n\t\t * Since we can have several sessions (paths) restablishing\n\t\t * connections in parallel, use lock.\n\t\t */\n\t\tmutex_lock(&clt->paths_mutex);\n\t\tclt->queue_depth = clt_path->queue_depth;\n\t\tclt->max_io_size = min_not_zero(clt_path->max_io_size,\n\t\t\t\t\t\tclt->max_io_size);\n\t\tmutex_unlock(&clt->paths_mutex);\n\n\t\t/*\n\t\t * Cache the hca_port and hca_name for sysfs\n\t\t */\n\t\tclt_path->hca_port = con->c.cm_id->port_num;\n\t\tscnprintf(clt_path->hca_name, sizeof(clt_path->hca_name),\n\t\t\t  clt_path->s.dev->ib_dev->name);\n\t\tclt_path->s.src_addr = con->c.cm_id->route.addr.src_addr;\n\t\t/* set for_new_clt, to allow future reconnect on any path */\n\t\tclt_path->for_new_clt = 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void flag_success_on_conn(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tatomic_inc(&clt_path->connected_cnt);\n\tcon->cm_err = 1;\n}\n\nstatic int rtrs_rdma_conn_rejected(struct rtrs_clt_con *con,\n\t\t\t\t    struct rdma_cm_event *ev)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tconst struct rtrs_msg_conn_rsp *msg;\n\tconst char *rej_msg;\n\tint status, errno;\n\tu8 data_len;\n\n\tstatus = ev->status;\n\trej_msg = rdma_reject_msg(con->c.cm_id, status);\n\tmsg = rdma_consumer_reject_data(con->c.cm_id, ev, &data_len);\n\n\tif (msg && data_len >= sizeof(*msg)) {\n\t\terrno = (int16_t)le16_to_cpu(msg->errno);\n\t\tif (errno == -EBUSY)\n\t\t\trtrs_err(s,\n\t\t\t\t  \"Previous session is still exists on the server, please reconnect later\\n\");\n\t\telse\n\t\t\trtrs_err(s,\n\t\t\t\t  \"Connect rejected: status %d (%s), rtrs errno %d\\n\",\n\t\t\t\t  status, rej_msg, errno);\n\t} else {\n\t\trtrs_err(s,\n\t\t\t  \"Connect rejected but with malformed message: status %d (%s)\\n\",\n\t\t\t  status, rej_msg);\n\t}\n\n\treturn -ECONNRESET;\n}\n\nvoid rtrs_clt_close_conns(struct rtrs_clt_path *clt_path, bool wait)\n{\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CLOSING, NULL))\n\t\tqueue_work(rtrs_wq, &clt_path->close_work);\n\tif (wait)\n\t\tflush_work(&clt_path->close_work);\n}\n\nstatic inline void flag_error_on_conn(struct rtrs_clt_con *con, int cm_err)\n{\n\tif (con->cm_err == 1) {\n\t\tstruct rtrs_clt_path *clt_path;\n\n\t\tclt_path = to_clt_path(con->c.path);\n\t\tif (atomic_dec_and_test(&clt_path->connected_cnt))\n\n\t\t\twake_up(&clt_path->state_wq);\n\t}\n\tcon->cm_err = cm_err;\n}\n\nstatic int rtrs_clt_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\t\t\t     struct rdma_cm_event *ev)\n{\n\tstruct rtrs_clt_con *con = cm_id->context;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tint cm_err = 0;\n\n\tswitch (ev->event) {\n\tcase RDMA_CM_EVENT_ADDR_RESOLVED:\n\t\tcm_err = rtrs_rdma_addr_resolved(con);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ROUTE_RESOLVED:\n\t\tcm_err = rtrs_rdma_route_resolved(con);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\tcm_err = rtrs_rdma_conn_established(con, ev);\n\t\tif (!cm_err) {\n\t\t\t/*\n\t\t\t * Report success and wake up. Here we abuse state_wq,\n\t\t\t * i.e. wake up without state change, but we set cm_err.\n\t\t\t */\n\t\t\tflag_success_on_conn(con);\n\t\t\twake_up(&clt_path->state_wq);\n\t\t\treturn 0;\n\t\t}\n\t\tbreak;\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tcm_err = rtrs_rdma_conn_rejected(con, ev);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\t/* No message for disconnecting */\n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\tcase RDMA_CM_EVENT_ADDR_CHANGE:\n\tcase RDMA_CM_EVENT_TIMEWAIT_EXIT:\n\t\trtrs_wrn(s, \"CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ADDR_ERROR:\n\tcase RDMA_CM_EVENT_ROUTE_ERROR:\n\t\trtrs_wrn(s, \"CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -EHOSTUNREACH;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\t/*\n\t\t * Device removal is a special case.  Queue close and return 0.\n\t\t */\n\t\trtrs_clt_close_conns(clt_path, false);\n\t\treturn 0;\n\tdefault:\n\t\trtrs_err(s, \"Unexpected RDMA CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\t}\n\n\tif (cm_err) {\n\t\t/*\n\t\t * cm error makes sense only on connection establishing,\n\t\t * in other cases we rely on normal procedure of reconnecting.\n\t\t */\n\t\tflag_error_on_conn(con, cm_err);\n\t\trtrs_rdma_error_recovery(con);\n\t}\n\n\treturn 0;\n}\n\nstatic int create_cm(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rdma_cm_id *cm_id;\n\tint err;\n\n\tcm_id = rdma_create_id(&init_net, rtrs_clt_rdma_cm_handler, con,\n\t\t\t       clt_path->s.dst_addr.ss_family == AF_IB ?\n\t\t\t       RDMA_PS_IB : RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id)) {\n\t\terr = PTR_ERR(cm_id);\n\t\trtrs_err(s, \"Failed to create CM ID, err: %d\\n\", err);\n\n\t\treturn err;\n\t}\n\tcon->c.cm_id = cm_id;\n\tcon->cm_err = 0;\n\t/* allow the port to be reused */\n\terr = rdma_set_reuseaddr(cm_id, 1);\n\tif (err != 0) {\n\t\trtrs_err(s, \"Set address reuse failed, err: %d\\n\", err);\n\t\tgoto destroy_cm;\n\t}\n\terr = rdma_resolve_addr(cm_id, (struct sockaddr *)&clt_path->s.src_addr,\n\t\t\t\t(struct sockaddr *)&clt_path->s.dst_addr,\n\t\t\t\tRTRS_CONNECT_TIMEOUT_MS);\n\tif (err) {\n\t\trtrs_err(s, \"Failed to resolve address, err: %d\\n\", err);\n\t\tgoto destroy_cm;\n\t}\n\t/*\n\t * Combine connection status and session events. This is needed\n\t * for waiting two possible cases: cm_err has something meaningful\n\t * or session state was really changed to error by device removal.\n\t */\n\terr = wait_event_interruptible_timeout(\n\t\t\tclt_path->state_wq,\n\t\t\tcon->cm_err || clt_path->state != RTRS_CLT_CONNECTING,\n\t\t\tmsecs_to_jiffies(RTRS_CONNECT_TIMEOUT_MS));\n\tif (err == 0 || err == -ERESTARTSYS) {\n\t\tif (err == 0)\n\t\t\terr = -ETIMEDOUT;\n\t\t/* Timedout or interrupted */\n\t\tgoto errr;\n\t}\n\tif (con->cm_err < 0) {\n\t\terr = con->cm_err;\n\t\tgoto errr;\n\t}\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTING) {\n\t\t/* Device removal */\n\t\terr = -ECONNABORTED;\n\t\tgoto errr;\n\t}\n\n\treturn 0;\n\nerrr:\n\tstop_cm(con);\n\tmutex_lock(&con->con_mutex);\n\tdestroy_con_cq_qp(con);\n\tmutex_unlock(&con->con_mutex);\ndestroy_cm:\n\tdestroy_cm(con);\n\n\treturn err;\n}\n\nstatic void rtrs_clt_path_up(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tint up;\n\n\t/*\n\t * We can fire RECONNECTED event only when all paths were\n\t * connected on rtrs_clt_open(), then each was disconnected\n\t * and the first one connected again.  That's why this nasty\n\t * game with counter value.\n\t */\n\n\tmutex_lock(&clt->paths_ev_mutex);\n\tup = ++clt->paths_up;\n\t/*\n\t * Here it is safe to access paths num directly since up counter\n\t * is greater than MAX_PATHS_NUM only while rtrs_clt_open() is\n\t * in progress, thus paths removals are impossible.\n\t */\n\tif (up > MAX_PATHS_NUM && up == MAX_PATHS_NUM + clt->paths_num)\n\t\tclt->paths_up = clt->paths_num;\n\telse if (up == 1)\n\t\tclt->link_ev(clt->priv, RTRS_CLT_LINK_EV_RECONNECTED);\n\tmutex_unlock(&clt->paths_ev_mutex);\n\n\t/* Mark session as established */\n\tclt_path->established = true;\n\tclt_path->reconnect_attempts = 0;\n\tclt_path->stats->reconnects.successful_cnt++;\n}\n\nstatic void rtrs_clt_path_down(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\n\tif (!clt_path->established)\n\t\treturn;\n\n\tclt_path->established = false;\n\tmutex_lock(&clt->paths_ev_mutex);\n\tWARN_ON(!clt->paths_up);\n\tif (--clt->paths_up == 0)\n\t\tclt->link_ev(clt->priv, RTRS_CLT_LINK_EV_DISCONNECTED);\n\tmutex_unlock(&clt->paths_ev_mutex);\n}\n\nstatic void rtrs_clt_stop_and_destroy_conns(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_con *con;\n\tunsigned int cid;\n\n\tWARN_ON(READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTED);\n\n\t/*\n\t * Possible race with rtrs_clt_open(), when DEVICE_REMOVAL comes\n\t * exactly in between.  Start destroying after it finishes.\n\t */\n\tmutex_lock(&clt_path->init_mutex);\n\tmutex_unlock(&clt_path->init_mutex);\n\n\t/*\n\t * All IO paths must observe !CONNECTED state before we\n\t * free everything.\n\t */\n\tsynchronize_rcu();\n\n\trtrs_stop_hb(&clt_path->s);\n\n\t/*\n\t * The order it utterly crucial: firstly disconnect and complete all\n\t * rdma requests with error (thus set in_use=false for requests),\n\t * then fail outstanding requests checking in_use for each, and\n\t * eventually notify upper layer about session disconnection.\n\t */\n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (!clt_path->s.con[cid])\n\t\t\tbreak;\n\t\tcon = to_clt_con(clt_path->s.con[cid]);\n\t\tstop_cm(con);\n\t}\n\tfail_all_outstanding_reqs(clt_path);\n\tfree_path_reqs(clt_path);\n\trtrs_clt_path_down(clt_path);\n\n\t/*\n\t * Wait for graceful shutdown, namely when peer side invokes\n\t * rdma_disconnect(). 'connected_cnt' is decremented only on\n\t * CM events, thus if other side had crashed and hb has detected\n\t * something is wrong, here we will stuck for exactly timeout ms,\n\t * since CM does not fire anything.  That is fine, we are not in\n\t * hurry.\n\t */\n\twait_event_timeout(clt_path->state_wq,\n\t\t\t   !atomic_read(&clt_path->connected_cnt),\n\t\t\t   msecs_to_jiffies(RTRS_CONNECT_TIMEOUT_MS));\n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (!clt_path->s.con[cid])\n\t\t\tbreak;\n\t\tcon = to_clt_con(clt_path->s.con[cid]);\n\t\tmutex_lock(&con->con_mutex);\n\t\tdestroy_con_cq_qp(con);\n\t\tmutex_unlock(&con->con_mutex);\n\t\tdestroy_cm(con);\n\t\tdestroy_con(con);\n\t}\n}\n\nstatic inline bool xchg_paths(struct rtrs_clt_path __rcu **rcu_ppcpu_path,\n\t\t\t      struct rtrs_clt_path *clt_path,\n\t\t\t      struct rtrs_clt_path *next)\n{\n\tstruct rtrs_clt_path **ppcpu_path;\n\n\t/* Call cmpxchg() without sparse warnings */\n\tppcpu_path = (typeof(ppcpu_path))rcu_ppcpu_path;\n\treturn clt_path == cmpxchg(ppcpu_path, clt_path, next);\n}\n\nstatic void rtrs_clt_remove_path_from_arr(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_clt_path *next;\n\tbool wait_for_grace = false;\n\tint cpu;\n\n\tmutex_lock(&clt->paths_mutex);\n\tlist_del_rcu(&clt_path->s.entry);\n\n\t/* Make sure everybody observes path removal. */\n\tsynchronize_rcu();\n\n\t/*\n\t * At this point nobody sees @sess in the list, but still we have\n\t * dangling pointer @pcpu_path which _can_ point to @sess.  Since\n\t * nobody can observe @sess in the list, we guarantee that IO path\n\t * will not assign @sess to @pcpu_path, i.e. @pcpu_path can be equal\n\t * to @sess, but can never again become @sess.\n\t */\n\n\t/*\n\t * Decrement paths number only after grace period, because\n\t * caller of do_each_path() must firstly observe list without\n\t * path and only then decremented paths number.\n\t *\n\t * Otherwise there can be the following situation:\n\t *    o Two paths exist and IO is coming.\n\t *    o One path is removed:\n\t *      CPU#0                          CPU#1\n\t *      do_each_path():                rtrs_clt_remove_path_from_arr():\n\t *          path = get_next_path()\n\t *          ^^^                            list_del_rcu(path)\n\t *          [!CONNECTED path]              clt->paths_num--\n\t *                                              ^^^^^^^^^\n\t *          load clt->paths_num                 from 2 to 1\n\t *                    ^^^^^^^^^\n\t *                    sees 1\n\t *\n\t *      path is observed as !CONNECTED, but do_each_path() loop\n\t *      ends, because expression i < clt->paths_num is false.\n\t */\n\tclt->paths_num--;\n\n\t/*\n\t * Get @next connection from current @sess which is going to be\n\t * removed.  If @sess is the last element, then @next is NULL.\n\t */\n\trcu_read_lock();\n\tnext = list_next_or_null_rr_rcu(&clt->paths_list, &clt_path->s.entry,\n\t\t\t\t\ttypeof(*next), s.entry);\n\trcu_read_unlock();\n\n\t/*\n\t * @pcpu paths can still point to the path which is going to be\n\t * removed, so change the pointer manually.\n\t */\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct rtrs_clt_path __rcu **ppcpu_path;\n\n\t\tppcpu_path = per_cpu_ptr(clt->pcpu_path, cpu);\n\t\tif (rcu_dereference_protected(*ppcpu_path,\n\t\t\tlockdep_is_held(&clt->paths_mutex)) != clt_path)\n\t\t\t/*\n\t\t\t * synchronize_rcu() was called just after deleting\n\t\t\t * entry from the list, thus IO code path cannot\n\t\t\t * change pointer back to the pointer which is going\n\t\t\t * to be removed, we are safe here.\n\t\t\t */\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We race with IO code path, which also changes pointer,\n\t\t * thus we have to be careful not to overwrite it.\n\t\t */\n\t\tif (xchg_paths(ppcpu_path, clt_path, next))\n\t\t\t/*\n\t\t\t * @ppcpu_path was successfully replaced with @next,\n\t\t\t * that means that someone could also pick up the\n\t\t\t * @sess and dereferencing it right now, so wait for\n\t\t\t * a grace period is required.\n\t\t\t */\n\t\t\twait_for_grace = true;\n\t}\n\tif (wait_for_grace)\n\t\tsynchronize_rcu();\n\n\tmutex_unlock(&clt->paths_mutex);\n}\n\nstatic void rtrs_clt_add_path_to_arr(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\n\tmutex_lock(&clt->paths_mutex);\n\tclt->paths_num++;\n\n\tlist_add_tail_rcu(&clt_path->s.entry, &clt->paths_list);\n\tmutex_unlock(&clt->paths_mutex);\n}\n\nstatic void rtrs_clt_close_work(struct work_struct *work)\n{\n\tstruct rtrs_clt_path *clt_path;\n\n\tclt_path = container_of(work, struct rtrs_clt_path, close_work);\n\n\tcancel_delayed_work_sync(&clt_path->reconnect_dwork);\n\trtrs_clt_stop_and_destroy_conns(clt_path);\n\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CLOSED, NULL);\n}\n\nstatic int init_conns(struct rtrs_clt_path *clt_path)\n{\n\tunsigned int cid;\n\tint err;\n\n\t/*\n\t * On every new session connections increase reconnect counter\n\t * to avoid clashes with previous sessions not yet closed\n\t * sessions on a server side.\n\t */\n\tclt_path->s.recon_cnt++;\n\n\t/* Establish all RDMA connections  */\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\terr = create_con(clt_path, cid);\n\t\tif (err)\n\t\t\tgoto destroy;\n\n\t\terr = create_cm(to_clt_con(clt_path->s.con[cid]));\n\t\tif (err) {\n\t\t\tdestroy_con(to_clt_con(clt_path->s.con[cid]));\n\t\t\tgoto destroy;\n\t\t}\n\t}\n\terr = alloc_path_reqs(clt_path);\n\tif (err)\n\t\tgoto destroy;\n\n\trtrs_start_hb(&clt_path->s);\n\n\treturn 0;\n\ndestroy:\n\twhile (cid--) {\n\t\tstruct rtrs_clt_con *con = to_clt_con(clt_path->s.con[cid]);\n\n\t\tstop_cm(con);\n\n\t\tmutex_lock(&con->con_mutex);\n\t\tdestroy_con_cq_qp(con);\n\t\tmutex_unlock(&con->con_mutex);\n\t\tdestroy_cm(con);\n\t\tdestroy_con(con);\n\t}\n\t/*\n\t * If we've never taken async path and got an error, say,\n\t * doing rdma_resolve_addr(), switch to CONNECTION_ERR state\n\t * manually to keep reconnecting.\n\t */\n\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING_ERR, NULL);\n\n\treturn err;\n}\n\nstatic void rtrs_clt_info_req_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_iu *iu;\n\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\trtrs_iu_free(iu, clt_path->s.dev->ib_dev, 1);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(clt_path->clt, \"Path info request send failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING_ERR, NULL);\n\t\treturn;\n\t}\n\n\trtrs_clt_update_wc_stats(con);\n}\n\nstatic int process_info_rsp(struct rtrs_clt_path *clt_path,\n\t\t\t    const struct rtrs_msg_info_rsp *msg)\n{\n\tunsigned int sg_cnt, total_len;\n\tint i, sgi;\n\n\tsg_cnt = le16_to_cpu(msg->sg_cnt);\n\tif (!sg_cnt || (clt_path->queue_depth % sg_cnt)) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"Incorrect sg_cnt %d, is not multiple\\n\",\n\t\t\t  sg_cnt);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Check if IB immediate data size is enough to hold the mem_id and\n\t * the offset inside the memory chunk.\n\t */\n\tif ((ilog2(sg_cnt - 1) + 1) + (ilog2(clt_path->chunk_size - 1) + 1) >\n\t    MAX_IMM_PAYL_BITS) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"RDMA immediate size (%db) not enough to encode %d buffers of size %dB\\n\",\n\t\t\t  MAX_IMM_PAYL_BITS, sg_cnt, clt_path->chunk_size);\n\t\treturn -EINVAL;\n\t}\n\ttotal_len = 0;\n\tfor (sgi = 0, i = 0; sgi < sg_cnt && i < clt_path->queue_depth; sgi++) {\n\t\tconst struct rtrs_sg_desc *desc = &msg->desc[sgi];\n\t\tu32 len, rkey;\n\t\tu64 addr;\n\n\t\taddr = le64_to_cpu(desc->addr);\n\t\trkey = le32_to_cpu(desc->key);\n\t\tlen  = le32_to_cpu(desc->len);\n\n\t\ttotal_len += len;\n\n\t\tif (!len || (len % clt_path->chunk_size)) {\n\t\t\trtrs_err(clt_path->clt, \"Incorrect [%d].len %d\\n\",\n\t\t\t\t  sgi,\n\t\t\t\t  len);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfor ( ; len && i < clt_path->queue_depth; i++) {\n\t\t\tclt_path->rbufs[i].addr = addr;\n\t\t\tclt_path->rbufs[i].rkey = rkey;\n\n\t\t\tlen  -= clt_path->chunk_size;\n\t\t\taddr += clt_path->chunk_size;\n\t\t}\n\t}\n\t/* Sanity check */\n\tif (sgi != sg_cnt || i != clt_path->queue_depth) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"Incorrect sg vector, not fully mapped\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (total_len != clt_path->chunk_size * clt_path->queue_depth) {\n\t\trtrs_err(clt_path->clt, \"Incorrect total_len %d\\n\", total_len);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void rtrs_clt_info_rsp_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_msg_info_rsp *msg;\n\tenum rtrs_clt_state state;\n\tstruct rtrs_iu *iu;\n\tsize_t rx_sz;\n\tint err;\n\n\tstate = RTRS_CLT_CONNECTING_ERR;\n\n\tWARN_ON(con->c.cid);\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(clt_path->clt, \"Path info response recv failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\tgoto out;\n\t}\n\tWARN_ON(wc->opcode != IB_WC_RECV);\n\n\tif (wc->byte_len < sizeof(*msg)) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\tib_dma_sync_single_for_cpu(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t   iu->size, DMA_FROM_DEVICE);\n\tmsg = iu->buf;\n\tif (le16_to_cpu(msg->type) != RTRS_MSG_INFO_RSP) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: type %d\\n\",\n\t\t\t  le16_to_cpu(msg->type));\n\t\tgoto out;\n\t}\n\trx_sz  = sizeof(*msg);\n\trx_sz += sizeof(msg->desc[0]) * le16_to_cpu(msg->sg_cnt);\n\tif (wc->byte_len < rx_sz) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\terr = process_info_rsp(clt_path, msg);\n\tif (err)\n\t\tgoto out;\n\n\terr = post_recv_path(clt_path);\n\tif (err)\n\t\tgoto out;\n\n\tstate = RTRS_CLT_CONNECTED;\n\nout:\n\trtrs_clt_update_wc_stats(con);\n\trtrs_iu_free(iu, clt_path->s.dev->ib_dev, 1);\n\trtrs_clt_change_state_get_old(clt_path, state, NULL);\n}\n\nstatic int rtrs_send_path_info(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_con *usr_con = to_clt_con(clt_path->s.con[0]);\n\tstruct rtrs_msg_info_req *msg;\n\tstruct rtrs_iu *tx_iu, *rx_iu;\n\tsize_t rx_sz;\n\tint err;\n\n\trx_sz  = sizeof(struct rtrs_msg_info_rsp);\n\trx_sz += sizeof(struct rtrs_sg_desc) * clt_path->queue_depth;\n\n\ttx_iu = rtrs_iu_alloc(1, sizeof(struct rtrs_msg_info_req), GFP_KERNEL,\n\t\t\t       clt_path->s.dev->ib_dev, DMA_TO_DEVICE,\n\t\t\t       rtrs_clt_info_req_done);\n\trx_iu = rtrs_iu_alloc(1, rx_sz, GFP_KERNEL, clt_path->s.dev->ib_dev,\n\t\t\t       DMA_FROM_DEVICE, rtrs_clt_info_rsp_done);\n\tif (!tx_iu || !rx_iu) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\t/* Prepare for getting info response */\n\terr = rtrs_iu_post_recv(&usr_con->c, rx_iu);\n\tif (err) {\n\t\trtrs_err(clt_path->clt, \"rtrs_iu_post_recv(), err: %d\\n\", err);\n\t\tgoto out;\n\t}\n\trx_iu = NULL;\n\n\tmsg = tx_iu->buf;\n\tmsg->type = cpu_to_le16(RTRS_MSG_INFO_REQ);\n\tmemcpy(msg->pathname, clt_path->s.sessname, sizeof(msg->pathname));\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      tx_iu->dma_addr,\n\t\t\t\t      tx_iu->size, DMA_TO_DEVICE);\n\n\t/* Send info request */\n\terr = rtrs_iu_post_send(&usr_con->c, tx_iu, sizeof(*msg), NULL);\n\tif (err) {\n\t\trtrs_err(clt_path->clt, \"rtrs_iu_post_send(), err: %d\\n\", err);\n\t\tgoto out;\n\t}\n\ttx_iu = NULL;\n\n\t/* Wait for state change */\n\twait_event_interruptible_timeout(clt_path->state_wq,\n\t\t\t\t\t clt_path->state != RTRS_CLT_CONNECTING,\n\t\t\t\t\t msecs_to_jiffies(\n\t\t\t\t\t\t RTRS_CONNECT_TIMEOUT_MS));\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED) {\n\t\tif (READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTING_ERR)\n\t\t\terr = -ECONNRESET;\n\t\telse\n\t\t\terr = -ETIMEDOUT;\n\t}\n\nout:\n\tif (tx_iu)\n\t\trtrs_iu_free(tx_iu, clt_path->s.dev->ib_dev, 1);\n\tif (rx_iu)\n\t\trtrs_iu_free(rx_iu, clt_path->s.dev->ib_dev, 1);\n\tif (err)\n\t\t/* If we've never taken async path because of malloc problems */\n\t\trtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t      RTRS_CLT_CONNECTING_ERR, NULL);\n\n\treturn err;\n}\n\n/**\n * init_path() - establishes all path connections and does handshake\n * @clt_path: client path.\n * In case of error full close or reconnect procedure should be taken,\n * because reconnect or close async works can be started.\n */\nstatic int init_path(struct rtrs_clt_path *clt_path)\n{\n\tint err;\n\tchar str[NAME_MAX];\n\tstruct rtrs_addr path = {\n\t\t.src = &clt_path->s.src_addr,\n\t\t.dst = &clt_path->s.dst_addr,\n\t};\n\n\trtrs_addr_to_str(&path, str, sizeof(str));\n\n\tmutex_lock(&clt_path->init_mutex);\n\terr = init_conns(clt_path);\n\tif (err) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"init_conns() failed: err=%d path=%s [%s:%u]\\n\", err,\n\t\t\t str, clt_path->hca_name, clt_path->hca_port);\n\t\tgoto out;\n\t}\n\terr = rtrs_send_path_info(clt_path);\n\tif (err) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"rtrs_send_path_info() failed: err=%d path=%s [%s:%u]\\n\",\n\t\t\t err, str, clt_path->hca_name, clt_path->hca_port);\n\t\tgoto out;\n\t}\n\trtrs_clt_path_up(clt_path);\nout:\n\tmutex_unlock(&clt_path->init_mutex);\n\n\treturn err;\n}\n\nstatic void rtrs_clt_reconnect_work(struct work_struct *work)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tstruct rtrs_clt_sess *clt;\n\tunsigned int delay_ms;\n\tint err;\n\n\tclt_path = container_of(to_delayed_work(work), struct rtrs_clt_path,\n\t\t\t\treconnect_dwork);\n\tclt = clt_path->clt;\n\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_RECONNECTING)\n\t\treturn;\n\n\tif (clt_path->reconnect_attempts >= clt->max_reconnect_attempts) {\n\t\t/* Close a path completely if max attempts is reached */\n\t\trtrs_clt_close_conns(clt_path, false);\n\t\treturn;\n\t}\n\tclt_path->reconnect_attempts++;\n\n\t/* Stop everything */\n\trtrs_clt_stop_and_destroy_conns(clt_path);\n\tmsleep(RTRS_RECONNECT_BACKOFF);\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING, NULL)) {\n\t\terr = init_path(clt_path);\n\t\tif (err)\n\t\t\tgoto reconnect_again;\n\t}\n\n\treturn;\n\nreconnect_again:\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_RECONNECTING, NULL)) {\n\t\tclt_path->stats->reconnects.fail_cnt++;\n\t\tdelay_ms = clt->reconnect_delay_sec * 1000;\n\t\tqueue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork,\n\t\t\t\t   msecs_to_jiffies(delay_ms +\n\t\t\t\t\t\t    prandom_u32() %\n\t\t\t\t\t\t    RTRS_RECONNECT_SEED));\n\t}\n}\n\nstatic void rtrs_clt_dev_release(struct device *dev)\n{\n\tstruct rtrs_clt_sess *clt = container_of(dev, struct rtrs_clt_sess,\n\t\t\t\t\t\t dev);\n\n\tmutex_destroy(&clt->paths_ev_mutex);\n\tmutex_destroy(&clt->paths_mutex);\n\tkfree(clt);\n}\n\nstatic struct rtrs_clt_sess *alloc_clt(const char *sessname, size_t paths_num,\n\t\t\t\t  u16 port, size_t pdu_sz, void *priv,\n\t\t\t\t  void\t(*link_ev)(void *priv,\n\t\t\t\t\t\t   enum rtrs_clt_link_ev ev),\n\t\t\t\t  unsigned int reconnect_delay_sec,\n\t\t\t\t  unsigned int max_reconnect_attempts)\n{\n\tstruct rtrs_clt_sess *clt;\n\tint err;\n\n\tif (!paths_num || paths_num > MAX_PATHS_NUM)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strlen(sessname) >= sizeof(clt->sessname))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tclt = kzalloc(sizeof(*clt), GFP_KERNEL);\n\tif (!clt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tclt->pcpu_path = alloc_percpu(typeof(*clt->pcpu_path));\n\tif (!clt->pcpu_path) {\n\t\tkfree(clt);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tclt->dev.class = rtrs_clt_dev_class;\n\tclt->dev.release = rtrs_clt_dev_release;\n\tuuid_gen(&clt->paths_uuid);\n\tINIT_LIST_HEAD_RCU(&clt->paths_list);\n\tclt->paths_num = paths_num;\n\tclt->paths_up = MAX_PATHS_NUM;\n\tclt->port = port;\n\tclt->pdu_sz = pdu_sz;\n\tclt->max_segments = RTRS_MAX_SEGMENTS;\n\tclt->reconnect_delay_sec = reconnect_delay_sec;\n\tclt->max_reconnect_attempts = max_reconnect_attempts;\n\tclt->priv = priv;\n\tclt->link_ev = link_ev;\n\tclt->mp_policy = MP_POLICY_MIN_INFLIGHT;\n\tstrscpy(clt->sessname, sessname, sizeof(clt->sessname));\n\tinit_waitqueue_head(&clt->permits_wait);\n\tmutex_init(&clt->paths_ev_mutex);\n\tmutex_init(&clt->paths_mutex);\n\tdevice_initialize(&clt->dev);\n\n\terr = dev_set_name(&clt->dev, \"%s\", sessname);\n\tif (err)\n\t\tgoto err_put;\n\n\t/*\n\t * Suppress user space notification until\n\t * sysfs files are created\n\t */\n\tdev_set_uevent_suppress(&clt->dev, true);\n\terr = device_add(&clt->dev);\n\tif (err)\n\t\tgoto err_put;\n\n\tclt->kobj_paths = kobject_create_and_add(\"paths\", &clt->dev.kobj);\n\tif (!clt->kobj_paths) {\n\t\terr = -ENOMEM;\n\t\tgoto err_del;\n\t}\n\terr = rtrs_clt_create_sysfs_root_files(clt);\n\tif (err) {\n\t\tkobject_del(clt->kobj_paths);\n\t\tkobject_put(clt->kobj_paths);\n\t\tgoto err_del;\n\t}\n\tdev_set_uevent_suppress(&clt->dev, false);\n\tkobject_uevent(&clt->dev.kobj, KOBJ_ADD);\n\n\treturn clt;\nerr_del:\n\tdevice_del(&clt->dev);\nerr_put:\n\tfree_percpu(clt->pcpu_path);\n\tput_device(&clt->dev);\n\treturn ERR_PTR(err);\n}\n\nstatic void free_clt(struct rtrs_clt_sess *clt)\n{\n\tfree_permits(clt);\n\tfree_percpu(clt->pcpu_path);\n\n\t/*\n\t * release callback will free clt and destroy mutexes in last put\n\t */\n\tdevice_unregister(&clt->dev);\n}\n\n/**\n * rtrs_clt_open() - Open a path to an RTRS server\n * @ops: holds the link event callback and the private pointer.\n * @sessname: name of the session\n * @paths: Paths to be established defined by their src and dst addresses\n * @paths_num: Number of elements in the @paths array\n * @port: port to be used by the RTRS session\n * @pdu_sz: Size of extra payload which can be accessed after permit allocation.\n * @reconnect_delay_sec: time between reconnect tries\n * @max_reconnect_attempts: Number of times to reconnect on error before giving\n *\t\t\t    up, 0 for * disabled, -1 for forever\n * @nr_poll_queues: number of polling mode connection using IB_POLL_DIRECT flag\n *\n * Starts session establishment with the rtrs_server. The function can block\n * up to ~2000ms before it returns.\n *\n * Return a valid pointer on success otherwise PTR_ERR.\n */\nstruct rtrs_clt_sess *rtrs_clt_open(struct rtrs_clt_ops *ops,\n\t\t\t\t const char *pathname,\n\t\t\t\t const struct rtrs_addr *paths,\n\t\t\t\t size_t paths_num, u16 port,\n\t\t\t\t size_t pdu_sz, u8 reconnect_delay_sec,\n\t\t\t\t s16 max_reconnect_attempts, u32 nr_poll_queues)\n{\n\tstruct rtrs_clt_path *clt_path, *tmp;\n\tstruct rtrs_clt_sess *clt;\n\tint err, i;\n\n\tif (strchr(pathname, '/') || strchr(pathname, '.')) {\n\t\tpr_err(\"pathname cannot contain / and .\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tclt = alloc_clt(pathname, paths_num, port, pdu_sz, ops->priv,\n\t\t\tops->link_ev,\n\t\t\treconnect_delay_sec,\n\t\t\tmax_reconnect_attempts);\n\tif (IS_ERR(clt)) {\n\t\terr = PTR_ERR(clt);\n\t\tgoto out;\n\t}\n\tfor (i = 0; i < paths_num; i++) {\n\t\tstruct rtrs_clt_path *clt_path;\n\n\t\tclt_path = alloc_path(clt, &paths[i], nr_cpu_ids,\n\t\t\t\t  nr_poll_queues);\n\t\tif (IS_ERR(clt_path)) {\n\t\t\terr = PTR_ERR(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\t\tif (!i)\n\t\t\tclt_path->for_new_clt = 1;\n\t\tlist_add_tail_rcu(&clt_path->s.entry, &clt->paths_list);\n\n\t\terr = init_path(clt_path);\n\t\tif (err) {\n\t\t\tlist_del_rcu(&clt_path->s.entry);\n\t\t\trtrs_clt_close_conns(clt_path, true);\n\t\t\tfree_percpu(clt_path->stats->pcpu_stats);\n\t\t\tkfree(clt_path->stats);\n\t\t\tfree_path(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\n\t\terr = rtrs_clt_create_path_files(clt_path);\n\t\tif (err) {\n\t\t\tlist_del_rcu(&clt_path->s.entry);\n\t\t\trtrs_clt_close_conns(clt_path, true);\n\t\t\tfree_percpu(clt_path->stats->pcpu_stats);\n\t\t\tkfree(clt_path->stats);\n\t\t\tfree_path(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\t}\n\terr = alloc_permits(clt);\n\tif (err)\n\t\tgoto close_all_path;\n\n\treturn clt;\n\nclose_all_path:\n\tlist_for_each_entry_safe(clt_path, tmp, &clt->paths_list, s.entry) {\n\t\trtrs_clt_destroy_path_files(clt_path, NULL);\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\trtrs_clt_destroy_sysfs_root(clt);\n\tfree_clt(clt);\n\nout:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL(rtrs_clt_open);\n\n/**\n * rtrs_clt_close() - Close a path\n * @clt: Session handle. Session is freed upon return.\n */\nvoid rtrs_clt_close(struct rtrs_clt_sess *clt)\n{\n\tstruct rtrs_clt_path *clt_path, *tmp;\n\n\t/* Firstly forbid sysfs access */\n\trtrs_clt_destroy_sysfs_root(clt);\n\n\t/* Now it is safe to iterate over all paths without locks */\n\tlist_for_each_entry_safe(clt_path, tmp, &clt->paths_list, s.entry) {\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\trtrs_clt_destroy_path_files(clt_path, NULL);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\tfree_clt(clt);\n}\nEXPORT_SYMBOL(rtrs_clt_close);\n\nint rtrs_clt_reconnect_from_sysfs(struct rtrs_clt_path *clt_path)\n{\n\tenum rtrs_clt_state old_state;\n\tint err = -EBUSY;\n\tbool changed;\n\n\tchanged = rtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t\t RTRS_CLT_RECONNECTING,\n\t\t\t\t\t\t &old_state);\n\tif (changed) {\n\t\tclt_path->reconnect_attempts = 0;\n\t\tqueue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork, 0);\n\t}\n\tif (changed || old_state == RTRS_CLT_RECONNECTING) {\n\t\t/*\n\t\t * flush_delayed_work() queues pending work for immediate\n\t\t * execution, so do the flush if we have queued something\n\t\t * right now or work is pending.\n\t\t */\n\t\tflush_delayed_work(&clt_path->reconnect_dwork);\n\t\terr = (READ_ONCE(clt_path->state) ==\n\t\t       RTRS_CLT_CONNECTED ? 0 : -ENOTCONN);\n\t}\n\n\treturn err;\n}\n\nint rtrs_clt_remove_path_from_sysfs(struct rtrs_clt_path *clt_path,\n\t\t\t\t     const struct attribute *sysfs_self)\n{\n\tenum rtrs_clt_state old_state;\n\tbool changed;\n\n\t/*\n\t * Continue stopping path till state was changed to DEAD or\n\t * state was observed as DEAD:\n\t * 1. State was changed to DEAD - we were fast and nobody\n\t *    invoked rtrs_clt_reconnect(), which can again start\n\t *    reconnecting.\n\t * 2. State was observed as DEAD - we have someone in parallel\n\t *    removing the path.\n\t */\n\tdo {\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\tchanged = rtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t\t\tRTRS_CLT_DEAD,\n\t\t\t\t\t\t\t&old_state);\n\t} while (!changed && old_state != RTRS_CLT_DEAD);\n\n\tif (changed) {\n\t\trtrs_clt_remove_path_from_arr(clt_path);\n\t\trtrs_clt_destroy_path_files(clt_path, sysfs_self);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\n\treturn 0;\n}\n\nvoid rtrs_clt_set_max_reconnect_attempts(struct rtrs_clt_sess *clt, int value)\n{\n\tclt->max_reconnect_attempts = (unsigned int)value;\n}\n\nint rtrs_clt_get_max_reconnect_attempts(const struct rtrs_clt_sess *clt)\n{\n\treturn (int)clt->max_reconnect_attempts;\n}\n\n/**\n * rtrs_clt_request() - Request data transfer to/from server via RDMA.\n *\n * @dir:\tREAD/WRITE\n * @ops:\tcallback function to be called as confirmation, and the pointer.\n * @clt:\tSession\n * @permit:\tPreallocated permit\n * @vec:\tMessage that is sent to server together with the request.\n *\t\tSum of len of all @vec elements limited to <= IO_MSG_SIZE.\n *\t\tSince the msg is copied internally it can be allocated on stack.\n * @nr:\t\tNumber of elements in @vec.\n * @data_len:\tlength of data sent to/from server\n * @sg:\t\tPages to be sent/received to/from server.\n * @sg_cnt:\tNumber of elements in the @sg\n *\n * Return:\n * 0:\t\tSuccess\n * <0:\t\tError\n *\n * On dir=READ rtrs client will request a data transfer from Server to client.\n * The data that the server will respond with will be stored in @sg when\n * the user receives an %RTRS_CLT_RDMA_EV_RDMA_REQUEST_WRITE_COMPL event.\n * On dir=WRITE rtrs client will rdma write data in sg to server side.\n */\nint rtrs_clt_request(int dir, struct rtrs_clt_req_ops *ops,\n\t\t     struct rtrs_clt_sess *clt, struct rtrs_permit *permit,\n\t\t     const struct kvec *vec, size_t nr, size_t data_len,\n\t\t     struct scatterlist *sg, unsigned int sg_cnt)\n{\n\tstruct rtrs_clt_io_req *req;\n\tstruct rtrs_clt_path *clt_path;\n\n\tenum dma_data_direction dma_dir;\n\tint err = -ECONNABORTED, i;\n\tsize_t usr_len, hdr_len;\n\tstruct path_it it;\n\n\t/* Get kvec length */\n\tfor (i = 0, usr_len = 0; i < nr; i++)\n\t\tusr_len += vec[i].iov_len;\n\n\tif (dir == READ) {\n\t\thdr_len = sizeof(struct rtrs_msg_rdma_read) +\n\t\t\t  sg_cnt * sizeof(struct rtrs_sg_desc);\n\t\tdma_dir = DMA_FROM_DEVICE;\n\t} else {\n\t\thdr_len = sizeof(struct rtrs_msg_rdma_write);\n\t\tdma_dir = DMA_TO_DEVICE;\n\t}\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (clt_path = it.next_path(&it)) && it.i < it.clt->paths_num; it.i++) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (usr_len + hdr_len > clt_path->max_hdr_size) {\n\t\t\trtrs_wrn_rl(clt_path->clt,\n\t\t\t\t     \"%s request failed, user message size is %zu and header length %zu, but max size is %u\\n\",\n\t\t\t\t     dir == READ ? \"Read\" : \"Write\",\n\t\t\t\t     usr_len, hdr_len, clt_path->max_hdr_size);\n\t\t\terr = -EMSGSIZE;\n\t\t\tbreak;\n\t\t}\n\t\treq = rtrs_clt_get_req(clt_path, ops->conf_fn, permit, ops->priv,\n\t\t\t\t       vec, usr_len, sg, sg_cnt, data_len,\n\t\t\t\t       dma_dir);\n\t\tif (dir == READ)\n\t\t\terr = rtrs_clt_read_req(req);\n\t\telse\n\t\t\terr = rtrs_clt_write_req(req);\n\t\tif (err) {\n\t\t\treq->in_use = false;\n\t\t\tcontinue;\n\t\t}\n\t\t/* Success path */\n\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn err;\n}\nEXPORT_SYMBOL(rtrs_clt_request);\n\nint rtrs_clt_rdma_cq_direct(struct rtrs_clt_sess *clt, unsigned int index)\n{\n\t/* If no path, return -1 for block layer not to try again */\n\tint cnt = -1;\n\tstruct rtrs_con *con;\n\tstruct rtrs_clt_path *clt_path;\n\tstruct path_it it;\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (clt_path = it.next_path(&it)) && it.i < it.clt->paths_num; it.i++) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tcon = clt_path->s.con[index + 1];\n\t\tcnt = ib_process_cq_direct(con->cq, -1);\n\t\tif (cnt)\n\t\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn cnt;\n}\nEXPORT_SYMBOL(rtrs_clt_rdma_cq_direct);\n\n/**\n * rtrs_clt_query() - queries RTRS session attributes\n *@clt: session pointer\n *@attr: query results for session attributes.\n * Returns:\n *    0 on success\n *    -ECOMM\t\tno connection to the server\n */\nint rtrs_clt_query(struct rtrs_clt_sess *clt, struct rtrs_attrs *attr)\n{\n\tif (!rtrs_clt_is_connected(clt))\n\t\treturn -ECOMM;\n\n\tattr->queue_depth      = clt->queue_depth;\n\tattr->max_segments     = clt->max_segments;\n\t/* Cap max_io_size to min of remote buffer size and the fr pages */\n\tattr->max_io_size = min_t(int, clt->max_io_size,\n\t\t\t\t  clt->max_segments * SZ_4K);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(rtrs_clt_query);\n\nint rtrs_clt_create_path_from_sysfs(struct rtrs_clt_sess *clt,\n\t\t\t\t     struct rtrs_addr *addr)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tint err;\n\n\tclt_path = alloc_path(clt, addr, nr_cpu_ids, 0);\n\tif (IS_ERR(clt_path))\n\t\treturn PTR_ERR(clt_path);\n\n\tmutex_lock(&clt->paths_mutex);\n\tif (clt->paths_num == 0) {\n\t\t/*\n\t\t * When all the paths are removed for a session,\n\t\t * the addition of the first path is like a new session for\n\t\t * the storage server\n\t\t */\n\t\tclt_path->for_new_clt = 1;\n\t}\n\n\tmutex_unlock(&clt->paths_mutex);\n\n\t/*\n\t * It is totally safe to add path in CONNECTING state: coming\n\t * IO will never grab it.  Also it is very important to add\n\t * path before init, since init fires LINK_CONNECTED event.\n\t */\n\trtrs_clt_add_path_to_arr(clt_path);\n\n\terr = init_path(clt_path);\n\tif (err)\n\t\tgoto close_path;\n\n\terr = rtrs_clt_create_path_files(clt_path);\n\tif (err)\n\t\tgoto close_path;\n\n\treturn 0;\n\nclose_path:\n\trtrs_clt_remove_path_from_arr(clt_path);\n\trtrs_clt_close_conns(clt_path, true);\n\tfree_percpu(clt_path->stats->pcpu_stats);\n\tkfree(clt_path->stats);\n\tfree_path(clt_path);\n\n\treturn err;\n}\n\nstatic int rtrs_clt_ib_dev_init(struct rtrs_ib_dev *dev)\n{\n\tif (!(dev->ib_dev->attrs.device_cap_flags &\n\t      IB_DEVICE_MEM_MGT_EXTENSIONS)) {\n\t\tpr_err(\"Memory registrations not supported.\\n\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct rtrs_rdma_dev_pd_ops dev_pd_ops = {\n\t.init = rtrs_clt_ib_dev_init\n};\n\nstatic int __init rtrs_client_init(void)\n{\n\trtrs_rdma_dev_pd_init(0, &dev_pd);\n\n\trtrs_clt_dev_class = class_create(THIS_MODULE, \"rtrs-client\");\n\tif (IS_ERR(rtrs_clt_dev_class)) {\n\t\tpr_err(\"Failed to create rtrs-client dev class\\n\");\n\t\treturn PTR_ERR(rtrs_clt_dev_class);\n\t}\n\trtrs_wq = alloc_workqueue(\"rtrs_client_wq\", 0, 0);\n\tif (!rtrs_wq) {\n\t\tclass_destroy(rtrs_clt_dev_class);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit rtrs_client_exit(void)\n{\n\tdestroy_workqueue(rtrs_wq);\n\tclass_destroy(rtrs_clt_dev_class);\n\trtrs_rdma_dev_pd_deinit(&dev_pd);\n}\n\nmodule_init(rtrs_client_init);\nmodule_exit(rtrs_client_exit);\n"], "filenames": ["drivers/infiniband/ulp/rtrs/rtrs-clt.c"], "buggy_code_start_loc": [2684], "buggy_code_end_loc": [2777], "fixing_code_start_loc": [2685], "fixing_code_end_loc": [2780], "type": "CWE-415", "message": "drivers/infiniband/ulp/rtrs/rtrs-clt.c in the Linux kernel before 5.16.12 has a double free related to rtrs_clt_dev_release.", "other": {"cve": {"id": "CVE-2022-29156", "sourceIdentifier": "cve@mitre.org", "published": "2022-04-13T07:15:28.597", "lastModified": "2023-04-11T18:14:17.013", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "drivers/infiniband/ulp/rtrs/rtrs-clt.c in the Linux kernel before 5.16.12 has a double free related to rtrs_clt_dev_release."}, {"lang": "es", "value": "El archivo drivers/infiniband/ulp/rtrs/rtrs-clt.c en el kernel de Linux versiones anteriores a 5.16.12, presenta una doble liberaci\u00f3n relacionado con rtrs_clt_dev_release"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-415"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.10.20", "versionEndExcluding": "5.10.103", "matchCriteriaId": "C7EE69BB-9CCD-4928-961B-AC68C8A3E287"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.11.3", "versionEndExcluding": "5.15.26", "matchCriteriaId": "DF173D1A-ADB0-4D85-888B-69BAAC98D354"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.16", "versionEndExcluding": "5.16.12", "matchCriteriaId": "C76BAB21-7F23-4AD8-A25F-CA7B262A2698"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "108A2215-50FB-4074-94CF-C130FA14566D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300e:-:*:*:*:*:*:*:*", "matchCriteriaId": "7AFC73CE-ABB9-42D3-9A71-3F5BC5381E0E"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "6770B6C3-732E-4E22-BF1C-2D2FD610061C"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "9F9C8C20-42EB-4AB5-BD97-212DEB070C43"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "234DEFE0-5CE5-4B0A-96B8-5D227CB8ED31"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "CDDF61B7-EC5C-467C-B710-B89F502CD04F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0B4AD8A-F172-4558-AEC6-FF424BA2D912"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "8497A4C9-8474-4A62-8331-3FE862ED4098"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "32F0B6C0-F930-480D-962B-3F4EFDCC13C7"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500e:-:*:*:*:*:*:*:*", "matchCriteriaId": "803BC414-B250-4E3A-A478-A3881340D6B8"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "7FFF7106-ED78-49BA-9EC5-B889E3685D53"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "E63D8B0F-006E-4801-BF9D-1C001BBFB4F9"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "0FEB3337-BFDE-462A-908B-176F92053CEC"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700e:-:*:*:*:*:*:*:*", "matchCriteriaId": "736AEAE9-782B-4F71-9893-DED53367E102"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "56409CEC-5A1E-4450-AA42-641E459CC2AF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "B06F4839-D16A-4A61-9BB5-55B13F41E47F"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.16.12", "source": "cve@mitre.org", "tags": ["Mailing List", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/8700af2cc18c919b2a83e74e0479038fd113c15d", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20220602-0002/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/8700af2cc18c919b2a83e74e0479038fd113c15d"}}