{"buggy_code": ["/*\n * Copyright (C) 2018 Intel Corporation. All rights reserved.\n *\n * SPDX-License-Identifier: BSD-3-Clause\n */\n\n#include <types.h>\n#include <errno.h>\n#include <asm/lib/bits.h>\n#include <asm/guest/vm.h>\n#include <asm/vtd.h>\n#include <ptdev.h>\n#include <asm/per_cpu.h>\n#include <asm/ioapic.h>\n#include <asm/pgtable.h>\n#include <asm/irq.h>\n\n/*\n * Check if the IRQ is single-destination and return the destination vCPU if so.\n *\n * VT-d PI (posted mode) cannot support multicast/broadcast IRQs.\n * If returns NULL, this means it is multicast/broadcast IRQ and\n * we can only handle it in remapped mode.\n * If returns non-NULL, the destination vCPU is returned, which means it is\n * single-destination IRQ and we can handle it in posted mode.\n *\n * @pre (vm != NULL) && (info != NULL)\n */\nstatic struct acrn_vcpu *is_single_destination(struct acrn_vm *vm, const struct msi_info *info)\n{\n\tuint64_t vdmask;\n\tuint16_t vid;\n\tstruct acrn_vcpu *vcpu = NULL;\n\n\tvdmask = vlapic_calc_dest_noshort(vm, false, (uint32_t)(info->addr.bits.dest_field),\n\t\t(bool)(info->addr.bits.dest_mode == MSI_ADDR_DESTMODE_PHYS),\n\t\t(bool)(info->data.bits.delivery_mode == MSI_DATA_DELMODE_LOPRI));\n\n\tvid = ffs64(vdmask);\n\n\t/* Can only post fixed and Lowpri IRQs */\n\tif ((info->data.bits.delivery_mode == MSI_DATA_DELMODE_FIXED)\n\t\t|| (info->data.bits.delivery_mode == MSI_DATA_DELMODE_LOPRI)) {\n\t\t/* Can only post single-destination IRQs */\n\t\tif (vdmask == (1UL << vid)) {\n\t\t\tvcpu = vcpu_from_vid(vm, vid);\n\t\t}\n\t}\n\n\treturn vcpu;\n}\n\nstatic uint32_t calculate_logical_dest_mask(uint64_t pdmask)\n{\n\tuint32_t dest_mask = 0UL;\n\tuint64_t pcpu_mask = pdmask;\n\tuint16_t pcpu_id;\n\n\tpcpu_id = ffs64(pcpu_mask);\n\twhile (pcpu_id < MAX_PCPU_NUM) {\n\t\tbitmap_clear_nolock(pcpu_id, &pcpu_mask);\n\t\tdest_mask |= per_cpu(lapic_ldr, pcpu_id);\n\t\tpcpu_id = ffs64(pcpu_mask);\n\t}\n\treturn dest_mask;\n}\n\n/**\n * @pre entry != NULL\n */\nstatic void ptirq_free_irte(const struct ptirq_remapping_info *entry)\n{\n\tstruct intr_source intr_src;\n\n\tif (entry->irte_idx < CONFIG_MAX_IR_ENTRIES) {\n\t\tif (entry->intr_type == PTDEV_INTR_MSI) {\n\t\t\tintr_src.is_msi = true;\n\t\t\tintr_src.src.msi.value = entry->phys_sid.msi_id.bdf;\n\t\t} else {\n\t\t\tintr_src.is_msi = false;\n\t\t\tintr_src.src.ioapic_id = ioapic_irq_to_ioapic_id(entry->allocated_pirq);\n\t\t}\n\t\tdmar_free_irte(&intr_src, entry->irte_idx);\n\t}\n}\n\n/*\n * pid_paddr = 0: invalid address, indicate that remapped mode shall be used\n *\n * pid_paddr != 0: physical address of posted interrupt descriptor, indicate\n * that posted mode shall be used\n */\nstatic void ptirq_build_physical_msi(struct acrn_vm *vm,\n\tstruct ptirq_remapping_info *entry, uint32_t vector, uint64_t pid_paddr, uint16_t irte_idx)\n{\n\tuint64_t vdmask, pdmask;\n\tuint32_t dest, delmode, dest_mask;\n\tbool phys;\n\tunion dmar_ir_entry irte;\n\tunion irte_index ir_index;\n\tint32_t ret;\n\tstruct intr_source intr_src;\n\n\t/* get physical destination cpu mask */\n\tdest = entry->vmsi.addr.bits.dest_field;\n\tphys = (entry->vmsi.addr.bits.dest_mode == MSI_ADDR_DESTMODE_PHYS);\n\n\tvdmask = vlapic_calc_dest_noshort(vm, false, dest, phys, false);\n\tpdmask = vcpumask2pcpumask(vm, vdmask);\n\n\t/* get physical delivery mode */\n\tdelmode = entry->vmsi.data.bits.delivery_mode;\n\tif ((delmode != MSI_DATA_DELMODE_FIXED) && (delmode != MSI_DATA_DELMODE_LOPRI)) {\n\t\tdelmode = MSI_DATA_DELMODE_LOPRI;\n\t}\n\n\tdest_mask = calculate_logical_dest_mask(pdmask);\n\n\t/* Using phys_irq as index in the corresponding IOMMU */\n\tirte.value.lo_64 = 0UL;\n\tirte.value.hi_64 = 0UL;\n\tirte.bits.remap.vector = vector;\n\tirte.bits.remap.delivery_mode = delmode;\n\tirte.bits.remap.dest_mode = MSI_ADDR_DESTMODE_LOGICAL;\n\tirte.bits.remap.rh = MSI_ADDR_RH;\n\tirte.bits.remap.dest = dest_mask;\n\n\tintr_src.is_msi = true;\n\tintr_src.pid_paddr = pid_paddr;\n\tintr_src.src.msi.value = entry->phys_sid.msi_id.bdf;\n\tif (entry->irte_idx == INVALID_IRTE_ID) {\n\t\tentry->irte_idx = irte_idx;\n\t}\n\tret = dmar_assign_irte(&intr_src, &irte, entry->irte_idx, &ir_index.index);\n\n\tif (ret == 0) {\n\t\tentry->pmsi.data.full = 0U;\n\t\tentry->pmsi.addr.full = 0UL;\n\t\tentry->irte_idx = ir_index.index;\n\t\tif (ir_index.index != INVALID_IRTE_ID) {\n\t\t\t/*\n\t\t\t * Update the MSI interrupt source to point to the IRTE\n\t\t\t * SHV is set to 0 as ACRN disables MMC (Multi-Message Capable\n\t\t\t * for MSI devices.\n\t\t\t */\n\t\t\tentry->pmsi.addr.ir_bits.intr_index_high = ir_index.bits.index_high;\n\t\t\tentry->pmsi.addr.ir_bits.shv = 0U;\n\t\t\tentry->pmsi.addr.ir_bits.intr_format = 0x1U;\n\t\t\tentry->pmsi.addr.ir_bits.intr_index_low = ir_index.bits.index_low;\n\t\t\tentry->pmsi.addr.ir_bits.constant = 0xFEEU;\n\t\t}\n\t} else {\n\t\t/* In case there is no corresponding IOMMU, for example, if the\n\t\t * IOMMU is ignored, pass the MSI info in Compatibility Format\n\t\t */\n\t\tentry->pmsi.data = entry->vmsi.data;\n\t\tentry->pmsi.data.bits.delivery_mode = delmode;\n\t\tentry->pmsi.data.bits.vector = vector;\n\n\t\tentry->pmsi.addr = entry->vmsi.addr;\n\t\tentry->pmsi.addr.bits.dest_field = dest_mask;\n\t\tentry->pmsi.addr.bits.rh = MSI_ADDR_RH;\n\t\tentry->pmsi.addr.bits.dest_mode = MSI_ADDR_DESTMODE_LOGICAL;\n\t}\n\tdev_dbg(DBG_LEVEL_IRQ, \"MSI %s addr:data = 0x%lx:%x(V) -> 0x%lx:%x(P)\",\n\t\t(entry->pmsi.addr.ir_bits.intr_format != 0U) ? \" Remappable Format\" : \"Compatibility Format\",\n\t\tentry->vmsi.addr.full, entry->vmsi.data.full,\n\t\tentry->pmsi.addr.full, entry->pmsi.data.full);\n}\n\nstatic union ioapic_rte\nptirq_build_physical_rte(struct acrn_vm *vm, struct ptirq_remapping_info *entry)\n{\n\tunion ioapic_rte rte;\n\tuint32_t phys_irq = entry->allocated_pirq;\n\tunion source_id *virt_sid = &entry->virt_sid;\n\tunion irte_index ir_index;\n\tunion dmar_ir_entry irte;\n\tstruct intr_source intr_src;\n\tint32_t ret;\n\n\tif (virt_sid->intx_id.ctlr == INTX_CTLR_IOAPIC) {\n\t\tuint64_t vdmask, pdmask;\n\t\tuint32_t dest, delmode, dest_mask, vector;\n\t\tunion ioapic_rte virt_rte;\n\t\tbool phys;\n\n\t\tvioapic_get_rte(vm, virt_sid->intx_id.gsi, &virt_rte);\n\t\trte = virt_rte;\n\n\t\t/* init polarity & pin state */\n\t\tif (rte.bits.intr_polarity == IOAPIC_RTE_INTPOL_ALO) {\n\t\t\tif (entry->polarity == 0U) {\n\t\t\t\tvioapic_set_irqline_nolock(vm, virt_sid->intx_id.gsi, GSI_SET_HIGH);\n\t\t\t}\n\t\t\tentry->polarity = 1U;\n\t\t} else {\n\t\t\tif (entry->polarity == 1U) {\n\t\t\t\tvioapic_set_irqline_nolock(vm, virt_sid->intx_id.gsi, GSI_SET_LOW);\n\t\t\t}\n\t\t\tentry->polarity = 0U;\n\t\t}\n\n\t\t/* physical destination cpu mask */\n\t\tphys = (virt_rte.bits.dest_mode == IOAPIC_RTE_DESTMODE_PHY);\n\t\tdest = (uint32_t)virt_rte.bits.dest_field;\n\t\tvdmask = vlapic_calc_dest_noshort(vm, false, dest, phys, false);\n\t\tpdmask = vcpumask2pcpumask(vm, vdmask);\n\n\t\t/* physical delivery mode */\n\t\tdelmode = virt_rte.bits.delivery_mode;\n\t\tif ((delmode != IOAPIC_RTE_DELMODE_FIXED) &&\n\t\t\t(delmode != IOAPIC_RTE_DELMODE_LOPRI)) {\n\t\t\tdelmode = IOAPIC_RTE_DELMODE_LOPRI;\n\t\t}\n\n\t\t/* update physical delivery mode, dest mode(logical) & vector */\n\t\tvector = irq_to_vector(phys_irq);\n\t\tdest_mask = calculate_logical_dest_mask(pdmask);\n\n\t\tirte.value.lo_64 = 0UL;\n\t\tirte.value.hi_64 = 0UL;\n\t\tirte.bits.remap.vector = vector;\n\t\tirte.bits.remap.delivery_mode = delmode;\n\t\tirte.bits.remap.dest_mode = IOAPIC_RTE_DESTMODE_LOGICAL;\n\t\tirte.bits.remap.dest = dest_mask;\n\t\tirte.bits.remap.trigger_mode = rte.bits.trigger_mode;\n\n\t\tintr_src.is_msi = false;\n\t\tintr_src.pid_paddr = 0UL;\n\t\tintr_src.src.ioapic_id = ioapic_irq_to_ioapic_id(phys_irq);\n\t\tret = dmar_assign_irte(&intr_src, &irte, entry->irte_idx, &ir_index.index);\n\n\t\tif (ret == 0) {\n\t\t\tentry->irte_idx = ir_index.index;\n\t\t\tif (ir_index.index != INVALID_IRTE_ID) {\n\t\t\t\trte.ir_bits.vector = vector;\n\t\t\t\trte.ir_bits.constant = 0U;\n\t\t\t\trte.ir_bits.intr_index_high = ir_index.bits.index_high;\n\t\t\t\trte.ir_bits.intr_format = 1U;\n\t\t\t\trte.ir_bits.intr_index_low = ir_index.bits.index_low;\n\t\t\t} else {\n\t\t\t\trte.bits.intr_mask = 1;\n\t\t\t}\n\t\t} else {\n\t\t\trte.bits.dest_mode = IOAPIC_RTE_DESTMODE_LOGICAL;\n\t\t\trte.bits.delivery_mode = delmode;\n\t\t\trte.bits.vector = vector;\n\t\t\trte.bits.dest_field = dest_mask;\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"IOAPIC RTE %s = 0x%x:%x(V) -> 0x%x:%x(P)\",\n\t\t\t(rte.ir_bits.intr_format != 0U) ? \"Remappable Format\" : \"Compatibility Format\",\n\t\t\tvirt_rte.u.hi_32, virt_rte.u.lo_32,\n\t\t\trte.u.hi_32, rte.u.lo_32);\n\t} else {\n\t\tenum vpic_trigger trigger;\n\t\tunion ioapic_rte phys_rte;\n\n\t\t/* just update trigger mode */\n\t\tioapic_get_rte(phys_irq, &phys_rte);\n\t\trte = phys_rte;\n\t\trte.bits.trigger_mode = IOAPIC_RTE_TRGRMODE_EDGE;\n\t\tvpic_get_irqline_trigger_mode(vm_pic(vm), (uint32_t)virt_sid->intx_id.gsi, &trigger);\n\t\tif (trigger == LEVEL_TRIGGER) {\n\t\t\trte.bits.trigger_mode = IOAPIC_RTE_TRGRMODE_LEVEL;\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"IOAPIC RTE %s = 0x%x:%x(P) -> 0x%x:%x(P)\",\n\t\t\t(rte.ir_bits.intr_format != 0U) ? \"Remappable Format\" : \"Compatibility Format\",\n\t\t\tphys_rte.u.hi_32, phys_rte.u.lo_32,\n\t\t\trte.u.hi_32, rte.u.lo_32);\n\t}\n\n\treturn rte;\n}\n\n/* add msix entry for a vm, based on msi id (phys_bdf+msix_index)\n * - if the entry not be added by any vm, allocate it\n * - if the entry already be added by sos_vm, then change the owner to current vm\n * - if the entry already be added by other vm, return NULL\n */\nstatic struct ptirq_remapping_info *add_msix_remapping(struct acrn_vm *vm,\n\tuint16_t virt_bdf, uint16_t phys_bdf, uint32_t entry_nr)\n{\n\tstruct ptirq_remapping_info *entry;\n\tDEFINE_MSI_SID(phys_sid, phys_bdf, entry_nr);\n\tDEFINE_MSI_SID(virt_sid, virt_bdf, entry_nr);\n\n\tentry = find_ptirq_entry(PTDEV_INTR_MSI, &phys_sid, NULL);\n\tif (entry == NULL) {\n\t\tentry = ptirq_alloc_entry(vm, PTDEV_INTR_MSI);\n\t\tif (entry != NULL) {\n\t\t\tentry->phys_sid.value = phys_sid.value;\n\t\t\tentry->virt_sid.value = virt_sid.value;\n\t\t\tentry->release_cb = ptirq_free_irte;\n\n\t\t\t/* update msi source and active entry */\n\t\t\tif (ptirq_activate_entry(entry, IRQ_INVALID) < 0) {\n\t\t\t\tptirq_release_entry(entry);\n\t\t\t\tentry = NULL;\n\t\t\t}\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"VM%d MSIX add vector mapping vbdf%x:pbdf%x idx=%d\",\n\t\t\tvm->vm_id, virt_bdf, phys_bdf, entry_nr);\n\t}\n\n\treturn entry;\n}\n\n/* deactive & remove mapping entry of vbdf:entry_nr for vm */\nstatic void\nremove_msix_remapping(const struct acrn_vm *vm, uint16_t phys_bdf, uint32_t entry_nr)\n{\n\tstruct ptirq_remapping_info *entry;\n\tDEFINE_MSI_SID(phys_sid, phys_bdf, entry_nr);\n\tstruct intr_source intr_src;\n\n\tentry = find_ptirq_entry(PTDEV_INTR_MSI, &phys_sid, NULL);\n\tif ((entry != NULL) && (entry->vm == vm)) {\n\t\tif (is_entry_active(entry)) {\n\t\t\t/*TODO: disable MSIX device when HV can in future */\n\t\t\tptirq_deactivate_entry(entry);\n\t\t}\n\n\t\tintr_src.is_msi = true;\n\t\tintr_src.src.msi.value = entry->phys_sid.msi_id.bdf;\n\t\tdmar_free_irte(&intr_src, entry->irte_idx);\n\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"VM%d MSIX remove vector mapping vbdf-pbdf:0x%x-0x%x idx=%d\",\n\t\t\tvm->vm_id, entry->virt_sid.msi_id.bdf, phys_bdf, entry_nr);\n\n\t\tptirq_release_entry(entry);\n\t}\n\n}\n\n/* add intx entry for a vm, based on intx id (phys_pin)\n * - if the entry not be added by any vm, allocate it\n * - if the entry already be added by sos_vm, then change the owner to current vm\n * - if the entry already be added by other vm, return NULL\n */\nstatic struct ptirq_remapping_info *add_intx_remapping(struct acrn_vm *vm, uint32_t virt_gsi,\n\t\tuint32_t phys_gsi, enum intx_ctlr vgsi_ctlr)\n{\n\tstruct ptirq_remapping_info *entry = NULL;\n\tDEFINE_INTX_SID(phys_sid, phys_gsi, INTX_CTLR_IOAPIC);\n\tDEFINE_INTX_SID(virt_sid, virt_gsi, vgsi_ctlr);\n\tuint32_t phys_irq = ioapic_gsi_to_irq(phys_gsi);\n\n\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &phys_sid, NULL);\n\tif (entry == NULL) {\n\t\tif (find_ptirq_entry(PTDEV_INTR_INTX, &virt_sid, vm) == NULL) {\n\t\t\tentry = ptirq_alloc_entry(vm, PTDEV_INTR_INTX);\n\t\t\tif (entry != NULL) {\n\t\t\t\tentry->phys_sid.value = phys_sid.value;\n\t\t\t\tentry->virt_sid.value = virt_sid.value;\n\t\t\t\tentry->release_cb = ptirq_free_irte;\n\n\t\t\t\t/* activate entry */\n\t\t\t\tif (ptirq_activate_entry(entry, phys_irq) < 0) {\n\t\t\t\t\tptirq_release_entry(entry);\n\t\t\t\t\tentry = NULL;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tpr_err(\"INTX re-add vpin %d\", virt_gsi);\n\t\t}\n\t} else if (entry->vm != vm) {\n\t\tif (is_sos_vm(entry->vm)) {\n\t\t\tentry->vm = vm;\n\t\t\tentry->virt_sid.value = virt_sid.value;\n\t\t\tentry->polarity = 0U;\n\t\t} else {\n\t\t\tpr_err(\"INTX gsi%d already in vm%d with vgsi%d, not able to add into vm%d with vgsi%d\",\n\t\t\t\t\tphys_gsi, entry->vm->vm_id, entry->virt_sid.intx_id.gsi, vm->vm_id, virt_gsi);\n\t\t\tentry = NULL;\n\t\t}\n\t} else {\n\t\t/* The mapping has already been added to the VM. No action\n\t\t * required.\n\t\t */\n\t}\n\n\n\t/*\n\t * ptirq entry is either created or transferred from SOS VM to Post-launched VM\n\t */\n\n\tif (entry != NULL) {\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"VM%d INTX add pin mapping vgsi%d:pgsi%d\",\n\t\t\tentry->vm->vm_id, virt_gsi, phys_gsi);\n\t}\n\n\treturn entry;\n}\n\n/* deactive & remove mapping entry of vpin for vm */\nstatic void remove_intx_remapping(const struct acrn_vm *vm, uint32_t virt_gsi, enum intx_ctlr vgsi_ctlr)\n{\n\tuint32_t phys_irq;\n\tstruct ptirq_remapping_info *entry;\n\tstruct intr_source intr_src;\n\tDEFINE_INTX_SID(virt_sid, virt_gsi, vgsi_ctlr);\n\n\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &virt_sid, vm);\n\tif (entry != NULL) {\n\t\tif (is_entry_active(entry)) {\n\t\t\tphys_irq = entry->allocated_pirq;\n\t\t\t/* disable interrupt */\n\t\t\tioapic_gsi_mask_irq(phys_irq);\n\n\t\t\tptirq_deactivate_entry(entry);\n\t\t\tintr_src.is_msi = false;\n\t\t\tintr_src.src.ioapic_id = ioapic_irq_to_ioapic_id(phys_irq);\n\n\t\t\tdmar_free_irte(&intr_src, entry->irte_idx);\n\t\t\tdev_dbg(DBG_LEVEL_IRQ,\n\t\t\t\t\"deactive %s intx entry:pgsi=%d, pirq=%d \",\n\t\t\t\t(vgsi_ctlr == INTX_CTLR_PIC) ? \"vPIC\" : \"vIOAPIC\",\n\t\t\t\tentry->phys_sid.intx_id.gsi, phys_irq);\n\t\t\tdev_dbg(DBG_LEVEL_IRQ, \"from vm%d vgsi=%d\\n\",\n\t\t\t\tentry->vm->vm_id, virt_gsi);\n\t\t}\n\n\t\tptirq_release_entry(entry);\n\t}\n}\n\nstatic void ptirq_handle_intx(struct acrn_vm *vm,\n\t\tconst struct ptirq_remapping_info *entry)\n{\n\tconst union source_id *virt_sid = &entry->virt_sid;\n\n\tswitch (virt_sid->intx_id.ctlr) {\n\tcase INTX_CTLR_IOAPIC:\n\t{\n\t\tunion ioapic_rte rte;\n\t\tbool trigger_lvl = false;\n\n\t\t/* INTX_CTLR_IOAPIC means we have vioapic enabled */\n\t\tvioapic_get_rte(vm, (uint32_t)virt_sid->intx_id.gsi, &rte);\n\t\tif (rte.bits.trigger_mode == IOAPIC_RTE_TRGRMODE_LEVEL) {\n\t\t\ttrigger_lvl = true;\n\t\t}\n\n\t\tif (trigger_lvl) {\n\t\t\tif (entry->polarity != 0U) {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_sid->intx_id.gsi, GSI_SET_LOW);\n\t\t\t} else {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_sid->intx_id.gsi, GSI_SET_HIGH);\n\t\t\t}\n\t\t} else {\n\t\t\tif (entry->polarity != 0U) {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_sid->intx_id.gsi, GSI_FALLING_PULSE);\n\t\t\t} else {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_sid->intx_id.gsi, GSI_RAISING_PULSE);\n\t\t\t}\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_PTIRQ,\n\t\t\t\"dev-assign: irq=0x%x assert vr: 0x%x vRTE=0x%lx\",\n\t\t\tentry->allocated_pirq,\n\t\t\tirq_to_vector(entry->allocated_pirq),\n\t\t\trte.full);\n\t\tbreak;\n\t}\n\tcase INTX_CTLR_PIC:\n\t{\n\t\tenum vpic_trigger trigger;\n\n\t\t/* INTX_CTLR_PIC means we have vpic enabled */\n\t\tvpic_get_irqline_trigger_mode(vm_pic(vm), virt_sid->intx_id.gsi, &trigger);\n\t\tif (trigger == LEVEL_TRIGGER) {\n\t\t\tvpic_set_irqline(vm_pic(vm), virt_sid->intx_id.gsi, GSI_SET_HIGH);\n\t\t} else {\n\t\t\tvpic_set_irqline(vm_pic(vm), virt_sid->intx_id.gsi, GSI_RAISING_PULSE);\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\t/*\n\t\t * In this switch statement, virt_sid->intx_id.ctlr shall\n\t\t * either be INTX_CTLR_IOAPIC or INTX_CTLR_PIC.\n\t\t * Gracefully return if prior case clauses have not been met.\n\t\t */\n\t\tbreak;\n\t}\n}\n\nvoid ptirq_softirq(uint16_t pcpu_id)\n{\n\twhile (1) {\n\t\tstruct ptirq_remapping_info *entry = ptirq_dequeue_softirq(pcpu_id);\n\t\tstruct msi_info *vmsi;\n\n\t\tif (entry == NULL) {\n\t\t\tbreak;\n\t\t}\n\n\t\tvmsi = &entry->vmsi;\n\n\t\t/* skip any inactive entry */\n\t\tif (!is_entry_active(entry)) {\n\t\t\t/* service next item */\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* handle real request */\n\t\tif (entry->intr_type == PTDEV_INTR_INTX) {\n\t\t\tptirq_handle_intx(entry->vm, entry);\n\t\t} else {\n\t\t\tif (vmsi != NULL) {\n\t\t\t\t/* TODO: vmsi destmode check required */\n\t\t\t\t(void)vlapic_inject_msi(entry->vm, vmsi->addr.full, vmsi->data.full);\n\t\t\t\tdev_dbg(DBG_LEVEL_PTIRQ, \"dev-assign: irq=0x%x MSI VR: 0x%x-0x%x\",\n\t\t\t\t\tentry->allocated_pirq, vmsi->data.bits.vector,\n\t\t\t\t\tirq_to_vector(entry->allocated_pirq));\n\t\t\t\tdev_dbg(DBG_LEVEL_PTIRQ, \" vmsi_addr: 0x%lx vmsi_data: 0x%x\",\n\t\t\t\t\tvmsi->addr.full, vmsi->data.full);\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid ptirq_intx_ack(struct acrn_vm *vm, uint32_t virt_gsi, enum intx_ctlr vgsi_ctlr)\n{\n\tuint32_t phys_irq;\n\tstruct ptirq_remapping_info *entry;\n\tDEFINE_INTX_SID(virt_sid, virt_gsi, vgsi_ctlr);\n\n\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &virt_sid, vm);\n\tif (entry != NULL) {\n\t\tphys_irq = entry->allocated_pirq;\n\n\t\t/* NOTE: only Level trigger will process EOI/ACK and if we got here\n\t\t * means we have this vioapic or vpic or both enabled\n\t\t */\n\t\tswitch (vgsi_ctlr) {\n\t\tcase INTX_CTLR_IOAPIC:\n\t\t\tif (entry->polarity != 0U) {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_gsi, GSI_SET_HIGH);\n\t\t\t} else {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_gsi, GSI_SET_LOW);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase INTX_CTLR_PIC:\n\t\t\tvpic_set_irqline(vm_pic(vm), virt_gsi, GSI_SET_LOW);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * In this switch statement, vgsi_ctlr shall either be\n\t\t\t * INTX_CTLR_IOAPIC or INTX_CTLR_PIC.\n\t\t\t * Gracefully return if prior case clauses have not been met.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_PTIRQ, \"dev-assign: irq=0x%x acked vr: 0x%x\",\n\t\t\t\tphys_irq, irq_to_vector(phys_irq));\n\t\tioapic_gsi_unmask_irq(phys_irq);\n\t}\n}\n\n/* Main entry for PCI device assignment with MSI and MSI-X\n * MSI can up to 8 vectors and MSI-X can up to 1024 Vectors\n * We use entry_nr to indicate coming vectors\n * entry_nr = 0 means first vector\n * user must provide bdf and entry_nr\n */\nint32_t ptirq_prepare_msix_remap(struct acrn_vm *vm, uint16_t virt_bdf, uint16_t phys_bdf,\n\t\t\t\tuint16_t entry_nr, struct msi_info *info, uint16_t irte_idx)\n{\n\tstruct ptirq_remapping_info *entry;\n\tint32_t ret = -ENODEV;\n\tunion pci_bdf vbdf;\n\n\t/*\n\t * adds the mapping entries at runtime, if the\n\t * entry already be held by others, return error.\n\t */\n\tspinlock_obtain(&ptdev_lock);\n\tentry = add_msix_remapping(vm, virt_bdf, phys_bdf, entry_nr);\n\tspinlock_release(&ptdev_lock);\n\n\tif (entry != NULL) {\n\t\tret = 0;\n\t\tentry->vmsi = *info;\n\n\t\t/* build physical config MSI, update to info->pmsi_xxx */\n\t\tif (is_lapic_pt_configured(vm)) {\n\t\t\tenum vm_vlapic_mode vlapic_mode = check_vm_vlapic_mode(vm);\n\n\t\t\tif (vlapic_mode == VM_VLAPIC_X2APIC) {\n\t\t\t\t/*\n\t\t\t\t * All the vCPUs are in x2APIC mode and LAPIC is Pass-through\n\t\t\t\t * Use guest vector to program the interrupt source\n\t\t\t\t */\n\t\t\t\tptirq_build_physical_msi(vm, entry, (uint32_t)info->data.bits.vector, 0UL, irte_idx);\n\t\t\t} else if (vlapic_mode == VM_VLAPIC_XAPIC) {\n\t\t\t\t/*\n\t\t\t\t * All the vCPUs are in xAPIC mode and LAPIC is emulated\n\t\t\t\t * Use host vector to program the interrupt source\n\t\t\t\t */\n\t\t\t\tptirq_build_physical_msi(vm, entry, irq_to_vector(entry->allocated_pirq), 0UL, irte_idx);\n\t\t\t} else if (vlapic_mode == VM_VLAPIC_TRANSITION) {\n\t\t\t\t/*\n\t\t\t\t * vCPUs are in middle of transition, so do not program interrupt source\n\t\t\t\t * TODO: Devices programmed during transistion do not work after transition\n\t\t\t\t * as device is not programmed with interrupt info. Need to implement a\n\t\t\t\t * method to get interrupts working after transition.\n\t\t\t\t */\n\t\t\t\tret = -EFAULT;\n\t\t\t} else {\n\t\t\t\t/* Do nothing for VM_VLAPIC_DISABLED */\n\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t} else {\n\t\t\tstruct acrn_vcpu *vcpu = is_single_destination(vm, info);\n\n\t\t\tif (is_pi_capable(vm) && (vcpu != NULL)) {\n\t\t\t\tptirq_build_physical_msi(vm, entry,\n\t\t\t\t\t(uint32_t)info->data.bits.vector, hva2hpa(get_pi_desc(vcpu)), irte_idx);\n\t\t\t} else {\n\t\t\t\t/* Go with remapped mode if we cannot handle it in posted mode */\n\t\t\t\tptirq_build_physical_msi(vm, entry, irq_to_vector(entry->allocated_pirq), 0UL, irte_idx);\n\t\t\t}\n\t\t}\n\n\t\tif (ret == 0) {\n\t\t\t*info = entry->pmsi;\n\t\t\tvbdf.value = virt_bdf;\n\t\t\tdev_dbg(DBG_LEVEL_IRQ, \"PCI %x:%x.%x MSI VR[%d] 0x%x->0x%x assigned to vm%d\",\n\t\t\t\tvbdf.bits.b, vbdf.bits.d, vbdf.bits.f, entry_nr, entry->vmsi.data.bits.vector,\n\t\t\t\tirq_to_vector(entry->allocated_pirq), entry->vm->vm_id);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void activate_physical_ioapic(struct acrn_vm *vm,\n\t\tstruct ptirq_remapping_info *entry)\n{\n\tunion ioapic_rte rte;\n\tuint32_t phys_irq = entry->allocated_pirq;\n\tuint64_t intr_mask;\n\tbool is_lvl_trigger = false;\n\n\t/* disable interrupt */\n\tioapic_gsi_mask_irq(phys_irq);\n\n\t/* build physical IOAPIC RTE */\n\trte = ptirq_build_physical_rte(vm, entry);\n\tintr_mask = rte.bits.intr_mask;\n\n\t/* update irq trigger mode according to info in guest */\n\tif (rte.bits.trigger_mode == IOAPIC_RTE_TRGRMODE_LEVEL) {\n\t\tis_lvl_trigger = true;\n\t}\n\tset_irq_trigger_mode(phys_irq, is_lvl_trigger);\n\n\t/* set rte entry when masked */\n\trte.bits.intr_mask = IOAPIC_RTE_MASK_SET;\n\tioapic_set_rte(phys_irq, rte);\n\n\tif (intr_mask == IOAPIC_RTE_MASK_CLR) {\n\t\tioapic_gsi_unmask_irq(phys_irq);\n\t}\n}\n\n/* Main entry for PCI/Legacy device assignment with INTx, calling from vIOAPIC\n * or vPIC\n */\nint32_t ptirq_intx_pin_remap(struct acrn_vm *vm, uint32_t virt_gsi, enum intx_ctlr vgsi_ctlr)\n{\n\tint32_t status = 0;\n\tstruct ptirq_remapping_info *entry = NULL;\n\tDEFINE_INTX_SID(virt_sid, virt_gsi, vgsi_ctlr);\n\tDEFINE_INTX_SID(alt_virt_sid, virt_gsi, vgsi_ctlr);\n\n\t/*\n\t * virt pin could come from primary vPIC, secondary vPIC or vIOAPIC\n\t * while phys pin is always means for physical IOAPIC.\n\t *\n\t * Device Model should pre-hold the mapping entries by calling\n\t * ptirq_add_intx_remapping for UOS.\n\t *\n\t * For SOS(sos_vm), it adds the mapping entries at runtime, if the\n\t * entry already be held by others, return error.\n\t */\n\n\t/* no remap for vuart intx */\n\tif (!is_vuart_intx(vm, virt_sid.intx_id.gsi)) {\n\t\t/* query if we have virt to phys mapping */\n\t\tspinlock_obtain(&ptdev_lock);\n\t\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &virt_sid, vm);\n\t\tif (entry == NULL) {\n\t\t\tif (is_sos_vm(vm)) {\n\n\t\t\t\t/* for sos_vm, there is chance of vpin source switch\n\t\t\t\t * between vPIC & vIOAPIC for one legacy phys_pin.\n\t\t\t\t *\n\t\t\t\t * here checks if there is already mapping entry from\n\t\t\t\t * the other vpin source for legacy pin. If yes, then\n\t\t\t\t * switch vpin source is needed\n\t\t\t\t */\n\t\t\t\tif (virt_gsi < NR_LEGACY_PIN) {\n\n\t\t\t\t\tif (vgsi_ctlr == INTX_CTLR_PIC) {\n\t\t\t\t\t\talt_virt_sid.intx_id.ctlr = INTX_CTLR_IOAPIC;\n\t\t\t\t\t} else {\n\t\t\t\t\t\talt_virt_sid.intx_id.ctlr = INTX_CTLR_PIC;\n\t\t\t\t\t}\n\n\t\t\t\t\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &alt_virt_sid, vm);\n\t\t\t\t\tif (entry != NULL) {\n\t\t\t\t\t\tuint32_t phys_gsi = virt_gsi;\n\n\t\t\t\t\t\tremove_intx_remapping(vm, alt_virt_sid.intx_id.gsi,\n\t\t\t\t\t\t\talt_virt_sid.intx_id.ctlr);\n\t\t\t\t\t\tentry = add_intx_remapping(vm, virt_gsi, phys_gsi, vgsi_ctlr);\n\t\t\t\t\t\tif (entry == NULL) {\n\t\t\t\t\t\t\tpr_err(\"%s, add intx remapping failed\", __func__);\n\t\t\t\t\t\t\tstatus = -ENODEV;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdev_dbg(DBG_LEVEL_IRQ,\n\t\t\t\t\t\t\t\t\"IOAPIC gsi=%hhu pirq=%u vgsi=%d from %s to %s for vm%d\",\n\t\t\t\t\t\t\t\tentry->phys_sid.intx_id.gsi,\n\t\t\t\t\t\t\t\tentry->allocated_pirq, entry->virt_sid.intx_id.gsi,\n\t\t\t\t\t\t\t\t(vgsi_ctlr == INTX_CTLR_IOAPIC) ? \"vPIC\" : \"vIOAPIC\",\n\t\t\t\t\t\t\t\t(vgsi_ctlr == INTX_CTLR_IOAPIC) ? \"vIOPIC\" : \"vPIC\",\n\t\t\t\t\t\t\t\tentry->vm->vm_id);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t/* entry could be updated by above switch check */\n\t\t\t\tif (entry == NULL) {\n\t\t\t\t\tuint32_t phys_gsi = virt_gsi;\n\n\t\t\t\t\tentry = add_intx_remapping(vm, virt_gsi, phys_gsi, vgsi_ctlr);\n\t\t\t\t\tif (entry == NULL) {\n\t\t\t\t\t\tpr_err(\"%s, add intx remapping failed\",\n\t\t\t\t\t\t\t\t__func__);\n\t\t\t\t\t\tstatus = -ENODEV;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* ptirq_intx_pin_remap is triggered by vPIC/vIOAPIC\n\t\t\t\t * everytime a pin get unmask, here filter out pins\n\t\t\t\t * not get mapped.\n\t\t\t\t */\n\t\t\t\tstatus = -ENODEV;\n\t\t\t}\n\t\t}\n\t\tspinlock_release(&ptdev_lock);\n\t} else {\n\t\tstatus = -EINVAL;\n\t}\n\n\tif (status == 0) {\n\t\tactivate_physical_ioapic(vm, entry);\n\t}\n\n\treturn status;\n}\n\n/* @pre vm != NULL\n * except sos_vm, Device Model should call this function to pre-hold ptdev intx\n * entries:\n * - the entry is identified by phys_pin:\n *   one entry vs. one phys_pin\n * - currently, one phys_pin can only be held by one pin source (vPIC or\n *   vIOAPIC)\n */\nint32_t ptirq_add_intx_remapping(struct acrn_vm *vm, uint32_t virt_gsi, uint32_t phys_gsi, bool pic_pin)\n{\n\tstruct ptirq_remapping_info *entry;\n\tenum intx_ctlr vgsi_ctlr = pic_pin ? INTX_CTLR_PIC : INTX_CTLR_IOAPIC;\n\n\tspinlock_obtain(&ptdev_lock);\n\tentry = add_intx_remapping(vm, virt_gsi, phys_gsi, vgsi_ctlr);\n\tspinlock_release(&ptdev_lock);\n\n\treturn (entry != NULL) ? 0 : -ENODEV;\n}\n\n/*\n * @pre vm != NULL\n */\nvoid ptirq_remove_intx_remapping(const struct acrn_vm *vm, uint32_t virt_gsi, bool pic_pin)\n{\n\tenum intx_ctlr vgsi_ctlr = pic_pin ? INTX_CTLR_PIC : INTX_CTLR_IOAPIC;\n\n\tspinlock_obtain(&ptdev_lock);\n\tremove_intx_remapping(vm, virt_gsi, vgsi_ctlr);\n\tspinlock_release(&ptdev_lock);\n}\n\n/*\n * @pre vm != NULL\n */\nvoid ptirq_remove_msix_remapping(const struct acrn_vm *vm, uint16_t phys_bdf,\n\t\tuint32_t vector_count)\n{\n\tuint32_t i;\n\n\tfor (i = 0U; i < vector_count; i++) {\n\t\tspinlock_obtain(&ptdev_lock);\n\t\tremove_msix_remapping(vm, phys_bdf, i);\n\t\tspinlock_release(&ptdev_lock);\n\t}\n}\n\n/*\n * @pre vm != NULL\n */\nvoid ptirq_remove_configured_intx_remappings(const struct acrn_vm *vm)\n{\n\tconst struct acrn_vm_config *vm_config = get_vm_config(vm->vm_id);\n\tuint32_t i;\n\n\tfor (i = 0; i < vm_config->pt_intx_num; i++) {\n\t\tptirq_remove_intx_remapping(vm, vm_config->pt_intx[i].virt_gsi, false);\n\t}\n}\n", "/*\n * Copyright (C) 2018 Intel Corporation. All rights reserved.\n *\n * SPDX-License-Identifier: BSD-3-Clause\n */\n\n#define pr_prefix\t\t\"iommu: \"\n\n#include <types.h>\n#include <errno.h>\n#include <asm/lib/bits.h>\n#include <asm/lib/spinlock.h>\n#include <asm/cpu_caps.h>\n#include <irq.h>\n#include <asm/irq.h>\n#include <asm/io.h>\n#include <asm/mmu.h>\n#include <asm/lapic.h>\n#include <asm/vtd.h>\n#include <ticks.h>\n#include <logmsg.h>\n#include <asm/board.h>\n#include <asm/vm_config.h>\n#include <pci.h>\n#include <asm/platform_caps.h>\n\n#define DBG_IOMMU 0\n\n#if DBG_IOMMU\n#define DBG_LEVEL_IOMMU LOG_INFO\n#define DMAR_FAULT_LOOP_MAX 10\n#else\n#define DBG_LEVEL_IOMMU 6U\n#endif\n#define LEVEL_WIDTH 9U\n\n#define ROOT_ENTRY_LOWER_PRESENT_POS        (0U)\n#define ROOT_ENTRY_LOWER_PRESENT_MASK       (1UL << ROOT_ENTRY_LOWER_PRESENT_POS)\n#define ROOT_ENTRY_LOWER_CTP_POS            (12U)\n#define ROOT_ENTRY_LOWER_CTP_MASK           (0xFFFFFFFFFFFFFUL << ROOT_ENTRY_LOWER_CTP_POS)\n\n#define CONFIG_MAX_IOMMU_NUM\t\tDRHD_COUNT\n\n/* 4 iommu fault register state */\n#define\tIOMMU_FAULT_REGISTER_STATE_NUM\t4U\n#define\tIOMMU_FAULT_REGISTER_SIZE\t4U\n\n#define CTX_ENTRY_UPPER_AW_POS          (0U)\n#define CTX_ENTRY_UPPER_AW_MASK         (0x7UL << CTX_ENTRY_UPPER_AW_POS)\n#define CTX_ENTRY_UPPER_DID_POS         (8U)\n#define CTX_ENTRY_UPPER_DID_MASK        (0xFFFFUL << CTX_ENTRY_UPPER_DID_POS)\n#define CTX_ENTRY_LOWER_P_POS           (0U)\n#define CTX_ENTRY_LOWER_P_MASK          (0x1UL << CTX_ENTRY_LOWER_P_POS)\n#define CTX_ENTRY_LOWER_FPD_POS         (1U)\n#define CTX_ENTRY_LOWER_FPD_MASK        (0x1UL << CTX_ENTRY_LOWER_FPD_POS)\n#define CTX_ENTRY_LOWER_TT_POS          (2U)\n#define CTX_ENTRY_LOWER_TT_MASK         (0x3UL << CTX_ENTRY_LOWER_TT_POS)\n#define CTX_ENTRY_LOWER_SLPTPTR_POS     (12U)\n#define CTX_ENTRY_LOWER_SLPTPTR_MASK    (0xFFFFFFFFFFFFFUL <<  CTX_ENTRY_LOWER_SLPTPTR_POS)\n\nstatic inline uint64_t dmar_get_bitslice(uint64_t var, uint64_t mask, uint32_t pos)\n{\n\treturn ((var & mask) >> pos);\n}\n\nstatic inline uint64_t dmar_set_bitslice(uint64_t var, uint64_t mask, uint32_t pos, uint64_t val)\n{\n\treturn ((var & ~mask) | ((val << pos) & mask));\n}\n\n/* translation type */\n#define DMAR_CTX_TT_UNTRANSLATED    0x0UL\n#define DMAR_CTX_TT_ALL             0x1UL\n#define DMAR_CTX_TT_PASSTHROUGH     0x2UL\n\n/* Fault event MSI data register */\n#define DMAR_MSI_DELIVERY_MODE_SHIFT     (8U)\n#define DMAR_MSI_DELIVERY_FIXED          (0U << DMAR_MSI_DELIVERY_MODE_SHIFT)\n#define DMAR_MSI_DELIVERY_LOWPRI         (1U << DMAR_MSI_DELIVERY_MODE_SHIFT)\n\n/* Fault event MSI address register */\n#define DMAR_MSI_DEST_MODE_SHIFT         (2U)\n#define DMAR_MSI_DEST_MODE_PHYS          (0U << DMAR_MSI_DEST_MODE_SHIFT)\n#define DMAR_MSI_DEST_MODE_LOGIC         (1U << DMAR_MSI_DEST_MODE_SHIFT)\n#define DMAR_MSI_REDIRECTION_SHIFT       (3U)\n#define DMAR_MSI_REDIRECTION_CPU         (0U << DMAR_MSI_REDIRECTION_SHIFT)\n#define DMAR_MSI_REDIRECTION_LOWPRI      (1U << DMAR_MSI_REDIRECTION_SHIFT)\n\n#define DMAR_INVALIDATION_QUEUE_SIZE\t4096U\n#define DMAR_QI_INV_ENTRY_SIZE\t\t16U\n#define DMAR_NUM_IR_ENTRIES_PER_PAGE\t256U\n\n#define DMAR_INV_STATUS_WRITE_SHIFT\t5U\n#define DMAR_INV_CONTEXT_CACHE_DESC\t0x01UL\n#define DMAR_INV_IOTLB_DESC\t\t0x02UL\n#define DMAR_INV_IEC_DESC\t\t0x04UL\n#define DMAR_INV_WAIT_DESC\t\t0x05UL\n#define DMAR_INV_STATUS_WRITE\t\t(1UL << DMAR_INV_STATUS_WRITE_SHIFT)\n#define DMAR_INV_STATUS_INCOMPLETE\t0UL\n#define DMAR_INV_STATUS_COMPLETED\t1UL\n#define DMAR_INV_STATUS_DATA_SHIFT\t32U\n#define DMAR_INV_STATUS_DATA\t\t(DMAR_INV_STATUS_COMPLETED << DMAR_INV_STATUS_DATA_SHIFT)\n#define DMAR_INV_WAIT_DESC_LOWER\t(DMAR_INV_STATUS_WRITE | DMAR_INV_WAIT_DESC | DMAR_INV_STATUS_DATA)\n\n#define DMAR_IR_ENABLE_EIM_SHIFT\t11UL\n#define DMAR_IR_ENABLE_EIM\t\t(1UL << DMAR_IR_ENABLE_EIM_SHIFT)\n\nenum dmar_cirg_type {\n\tDMAR_CIRG_RESERVED = 0,\n\tDMAR_CIRG_GLOBAL,\n\tDMAR_CIRG_DOMAIN,\n\tDMAR_CIRG_DEVICE\n};\n\nenum dmar_iirg_type {\n\tDMAR_IIRG_RESERVED = 0,\n\tDMAR_IIRG_GLOBAL,\n\tDMAR_IIRG_DOMAIN,\n\tDMAR_IIRG_PAGE\n};\n\n/* dmar unit runtime data */\nstruct dmar_drhd_rt {\n\tuint32_t index;\n\tspinlock_t lock;\n\n\tstruct dmar_drhd *drhd;\n\n\tuint64_t root_table_addr;\n\tuint64_t ir_table_addr;\n\tuint64_t irte_alloc_bitmap[CONFIG_MAX_IR_ENTRIES / 64U];\n\tuint64_t irte_reserved_bitmap[CONFIG_MAX_IR_ENTRIES / 64U];\n\tuint64_t qi_queue;\n\tuint16_t qi_tail;\n\n\tuint64_t cap;\n\tuint64_t ecap;\n\tuint32_t gcmd;  /* sw cache value of global cmd register */\n\n\tuint32_t dmar_irq;\n\n\tbool cap_pw_coherency;  /* page-walk coherency */\n\tuint8_t cap_msagaw;\n\tuint16_t cap_num_fault_regs;\n\tuint16_t cap_fault_reg_offset;\n\tuint16_t ecap_iotlb_offset;\n\tuint32_t fault_state[IOMMU_FAULT_REGISTER_STATE_NUM]; /* 32bit registers */\n};\n\nstruct context_table {\n\tstruct page buses[CONFIG_IOMMU_BUS_NUM];\n};\n\nstruct intr_remap_table {\n\tstruct page tables[CONFIG_MAX_IR_ENTRIES/DMAR_NUM_IR_ENTRIES_PER_PAGE];\n};\n\nstatic inline uint8_t *get_root_table(uint32_t dmar_index)\n{\n\tstatic struct page root_tables[CONFIG_MAX_IOMMU_NUM] __aligned(PAGE_SIZE);\n\treturn root_tables[dmar_index].contents;\n}\n\nstatic inline uint8_t *get_ctx_table(uint32_t dmar_index, uint8_t bus_no)\n{\n\tstatic struct context_table ctx_tables[CONFIG_MAX_IOMMU_NUM] __aligned(PAGE_SIZE);\n\treturn ctx_tables[dmar_index].buses[bus_no].contents;\n}\n\n/*\n * @pre dmar_index < CONFIG_MAX_IOMMU_NUM\n */\nstatic inline void *get_qi_queue(uint32_t dmar_index)\n{\n\tstatic struct page qi_queues[CONFIG_MAX_IOMMU_NUM] __aligned(PAGE_SIZE);\n\treturn (void *)qi_queues[dmar_index].contents;\n}\n\nstatic inline void *get_ir_table(uint32_t dmar_index)\n{\n\tstatic struct intr_remap_table ir_tables[CONFIG_MAX_IOMMU_NUM] __aligned(PAGE_SIZE);\n\treturn (void *)ir_tables[dmar_index].tables[0].contents;\n}\n\nstatic struct dmar_drhd_rt dmar_drhd_units[MAX_DRHDS];\nstatic bool iommu_page_walk_coherent = true;\nstatic struct dmar_info *platform_dmar_info = NULL;\n\n/* Domain id 0 is reserved in some cases per VT-d */\n#define MAX_DOMAIN_NUM (CONFIG_MAX_VM_NUM + 1)\n\nstatic inline uint16_t vmid_to_domainid(uint16_t vm_id)\n{\n\treturn vm_id + 1U;\n}\n\nstatic int32_t dmar_register_hrhd(struct dmar_drhd_rt *dmar_unit);\nstatic struct dmar_drhd_rt *device_to_dmaru(uint8_t bus, uint8_t devfun);\n\nstatic int32_t register_hrhd_units(void)\n{\n\tstruct dmar_drhd_rt *drhd_rt;\n\tuint32_t i;\n\tint32_t ret = 0;\n\n\tfor (i = 0U; i < platform_dmar_info->drhd_count; i++) {\n\t\tdrhd_rt = &dmar_drhd_units[i];\n\t\tdrhd_rt->index = i;\n\t\tdrhd_rt->drhd = &platform_dmar_info->drhd_units[i];\n\t\tdrhd_rt->dmar_irq = IRQ_INVALID;\n\n\t\tset_paging_supervisor(drhd_rt->drhd->reg_base_addr, PAGE_SIZE);\n\n\t\tret = dmar_register_hrhd(drhd_rt);\n\t\tif (ret != 0) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((iommu_cap_pi(drhd_rt->cap) == 0U) || (!is_apicv_advanced_feature_supported())) {\n\t\t\tplatform_caps.pi = false;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic uint32_t iommu_read32(const struct dmar_drhd_rt *dmar_unit, uint32_t offset)\n{\n\treturn mmio_read32(hpa2hva(dmar_unit->drhd->reg_base_addr + offset));\n}\n\nstatic uint64_t iommu_read64(const struct dmar_drhd_rt *dmar_unit, uint32_t offset)\n{\n\treturn mmio_read64(hpa2hva(dmar_unit->drhd->reg_base_addr + offset));\n}\n\nstatic void iommu_write32(const struct dmar_drhd_rt *dmar_unit, uint32_t offset, uint32_t value)\n{\n\tmmio_write32(value, hpa2hva(dmar_unit->drhd->reg_base_addr + offset));\n}\n\nstatic void iommu_write64(const struct dmar_drhd_rt *dmar_unit, uint32_t offset, uint64_t value)\n{\n\tmmio_write64(value, hpa2hva(dmar_unit->drhd->reg_base_addr + offset));\n}\n\nstatic inline void dmar_wait_completion(const struct dmar_drhd_rt *dmar_unit, uint32_t offset,\n\tuint32_t mask, uint32_t pre_condition, uint32_t *status)\n{\n\t/* variable start isn't used when built as release version */\n\t__unused uint64_t start = cpu_ticks();\n\n\tdo {\n\t\t*status = iommu_read32(dmar_unit, offset);\n\t\tASSERT(((cpu_ticks() - start) < TICKS_PER_MS),\n\t\t\t\"DMAR OP Timeout!\");\n\t\tasm_pause();\n\t} while( (*status & mask) == pre_condition);\n}\n\n/* Flush CPU cache when root table, context table or second-level translation teable updated\n * In the context of ACRN, GPA to HPA mapping relationship is not changed after VM created,\n * skip flushing iotlb to avoid performance penalty.\n */\nvoid iommu_flush_cache(const void *p, uint32_t size)\n{\n\t/* if vtd support page-walk coherency, no need to flush cacheline */\n\tif (!iommu_page_walk_coherent) {\n\t\tflush_cache_range(p, size);\n\t}\n}\n\n#if DBG_IOMMU\nstatic inline uint8_t iommu_cap_rwbf(uint64_t cap)\n{\n\treturn ((uint8_t)(cap >> 4U) & 1U);\n}\n\nstatic inline uint8_t iommu_ecap_sc(uint64_t ecap)\n{\n\treturn ((uint8_t)(ecap >> 7U) & 1U);\n}\n\nstatic void dmar_unit_show_capability(struct dmar_drhd_rt *dmar_unit)\n{\n\tpr_info(\"dmar unit[0x%x]\", dmar_unit->drhd->reg_base_addr);\n\tpr_info(\"\\tNumDomain:%d\", iommu_cap_ndoms(dmar_unit->cap));\n\tpr_info(\"\\tAdvancedFaultLogging:%d\", iommu_cap_afl(dmar_unit->cap));\n\tpr_info(\"\\tRequiredWBFlush:%d\", iommu_cap_rwbf(dmar_unit->cap));\n\tpr_info(\"\\tProtectedLowMemRegion:%d\", iommu_cap_plmr(dmar_unit->cap));\n\tpr_info(\"\\tProtectedHighMemRegion:%d\", iommu_cap_phmr(dmar_unit->cap));\n\tpr_info(\"\\tCachingMode:%d\", iommu_cap_caching_mode(dmar_unit->cap));\n\tpr_info(\"\\tSAGAW:0x%x\", iommu_cap_sagaw(dmar_unit->cap));\n\tpr_info(\"\\tMGAW:%d\", iommu_cap_mgaw(dmar_unit->cap));\n\tpr_info(\"\\tZeroLenRead:%d\", iommu_cap_zlr(dmar_unit->cap));\n\tpr_info(\"\\tLargePageSupport:0x%x\", iommu_cap_super_page_val(dmar_unit->cap));\n\tpr_info(\"\\tPageSelectiveInvalidation:%d\", iommu_cap_pgsel_inv(dmar_unit->cap));\n\tpr_info(\"\\tPageSelectInvalidation:%d\", iommu_cap_pgsel_inv(dmar_unit->cap));\n\tpr_info(\"\\tNumOfFaultRecordingReg:%d\", iommu_cap_num_fault_regs(dmar_unit->cap));\n\tpr_info(\"\\tMAMV:0x%x\", iommu_cap_max_amask_val(dmar_unit->cap));\n\tpr_info(\"\\tWriteDraining:%d\", iommu_cap_write_drain(dmar_unit->cap));\n\tpr_info(\"\\tReadDraining:%d\", iommu_cap_read_drain(dmar_unit->cap));\n\tpr_info(\"\\tPostInterrupts:%d\\n\", iommu_cap_pi(dmar_unit->cap));\n\tpr_info(\"\\tPage-walk Coherency:%d\", iommu_ecap_c(dmar_unit->ecap));\n\tpr_info(\"\\tQueuedInvalidation:%d\", iommu_ecap_qi(dmar_unit->ecap));\n\tpr_info(\"\\tDeviceTLB:%d\", iommu_ecap_dt(dmar_unit->ecap));\n\tpr_info(\"\\tInterruptRemapping:%d\", iommu_ecap_ir(dmar_unit->ecap));\n\tpr_info(\"\\tExtendedInterruptMode:%d\", iommu_ecap_eim(dmar_unit->ecap));\n\tpr_info(\"\\tPassThrough:%d\", iommu_ecap_pt(dmar_unit->ecap));\n\tpr_info(\"\\tSnoopControl:%d\", iommu_ecap_sc(dmar_unit->ecap));\n\tpr_info(\"\\tIOTLB RegOffset:0x%x\", iommu_ecap_iro(dmar_unit->ecap));\n\tpr_info(\"\\tMHMV:0x%x\", iommu_ecap_mhmv(dmar_unit->ecap));\n\tpr_info(\"\\tECS:%d\", iommu_ecap_ecs(dmar_unit->ecap));\n\tpr_info(\"\\tMTS:%d\", iommu_ecap_mts(dmar_unit->ecap));\n\tpr_info(\"\\tNEST:%d\", iommu_ecap_nest(dmar_unit->ecap));\n\tpr_info(\"\\tDIS:%d\", iommu_ecap_dis(dmar_unit->ecap));\n\tpr_info(\"\\tPRS:%d\", iommu_ecap_prs(dmar_unit->ecap));\n\tpr_info(\"\\tERS:%d\", iommu_ecap_ers(dmar_unit->ecap));\n\tpr_info(\"\\tSRS:%d\", iommu_ecap_srs(dmar_unit->ecap));\n\tpr_info(\"\\tNWFS:%d\", iommu_ecap_nwfs(dmar_unit->ecap));\n\tpr_info(\"\\tEAFS:%d\", iommu_ecap_eafs(dmar_unit->ecap));\n\tpr_info(\"\\tPSS:0x%x\", iommu_ecap_pss(dmar_unit->ecap));\n\tpr_info(\"\\tPASID:%d\", iommu_ecap_pasid(dmar_unit->ecap));\n\tpr_info(\"\\tDIT:%d\", iommu_ecap_dit(dmar_unit->ecap));\n\tpr_info(\"\\tPDS:%d\\n\", iommu_ecap_pds(dmar_unit->ecap));\n}\n#endif\n\nstatic inline uint8_t width_to_level(uint32_t width)\n{\n\treturn (uint8_t)(((width - 12U) + (LEVEL_WIDTH)-1U) / (LEVEL_WIDTH));\n}\n\nstatic inline uint8_t width_to_agaw(uint32_t width)\n{\n\treturn width_to_level(width) - 2U;\n}\n\nstatic uint8_t dmar_unit_get_msagw(const struct dmar_drhd_rt *dmar_unit)\n{\n\tuint8_t i;\n\tuint8_t sgaw = iommu_cap_sagaw(dmar_unit->cap);\n\n\tfor (i = 5U; i > 0U; ) {\n\t\ti--;\n\t\tif (((1U << i) & sgaw) != 0U) {\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn i;\n}\n\nstatic bool dmar_unit_support_aw(const struct dmar_drhd_rt *dmar_unit, uint32_t addr_width)\n{\n\tuint8_t aw;\n\n\taw = width_to_agaw(addr_width);\n\n\treturn (((1U << aw) & iommu_cap_sagaw(dmar_unit->cap)) != 0U);\n}\n\nstatic void dmar_enable_intr_remapping(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif ((dmar_unit->gcmd & DMA_GCMD_IRE) == 0U) {\n\t\tdmar_unit->gcmd |= DMA_GCMD_IRE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd);\n\t\t/* 32-bit register */\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_IRES, 0U, &status);\n#if DBG_IOMMU\n\t\tstatus = iommu_read32(dmar_unit, DMAR_GSTS_REG);\n#endif\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: gsr:0x%x\", __func__, status);\n}\n\nstatic void dmar_enable_translation(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif ((dmar_unit->gcmd & DMA_GCMD_TE) == 0U) {\n\t\tdmar_unit->gcmd |= DMA_GCMD_TE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd);\n\t\t/* 32-bit register */\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_TES, 0U, &status);\n#if DBG_IOMMU\n\t\tstatus = iommu_read32(dmar_unit, DMAR_GSTS_REG);\n#endif\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n\n\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: gsr:0x%x\", __func__, status);\n}\n\nstatic void dmar_disable_intr_remapping(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif ((dmar_unit->gcmd & DMA_GCMD_IRE) != 0U) {\n\t\tdmar_unit->gcmd &= ~DMA_GCMD_IRE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd);\n\t\t/* 32-bit register */\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_IRES, DMA_GSTS_IRES, &status);\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_disable_translation(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif ((dmar_unit->gcmd & DMA_GCMD_TE) != 0U) {\n\t\tdmar_unit->gcmd &= ~DMA_GCMD_TE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd);\n\t\t/* 32-bit register */\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_TES, DMA_GSTS_TES, &status);\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic int32_t dmar_register_hrhd(struct dmar_drhd_rt *dmar_unit)\n{\n\tint32_t ret = 0;\n\n\tdev_dbg(DBG_LEVEL_IOMMU, \"Register dmar uint [%d] @0x%lx\", dmar_unit->index, dmar_unit->drhd->reg_base_addr);\n\n\tspinlock_init(&dmar_unit->lock);\n\n\tdmar_unit->cap = iommu_read64(dmar_unit, DMAR_CAP_REG);\n\tdmar_unit->ecap = iommu_read64(dmar_unit, DMAR_ECAP_REG);\n\n\t/*\n\t * The initialization of \"dmar_unit->gcmd\" shall be done via reading from Global Status Register rather than\n\t * Global Command Register.\n\t * According to Chapter 10.4.4 Global Command Register in VT-d spec, Global Command Register is a write-only\n\t * register to control remapping hardware. Global Status Register is the corresponding read-only register to\n\t * report remapping hardware status.\n\t */\n\tdmar_unit->gcmd = iommu_read32(dmar_unit, DMAR_GSTS_REG);\n\n\tdmar_unit->cap_msagaw = dmar_unit_get_msagw(dmar_unit);\n\n\tdmar_unit->cap_num_fault_regs = iommu_cap_num_fault_regs(dmar_unit->cap);\n\tdmar_unit->cap_fault_reg_offset = iommu_cap_fault_reg_offset(dmar_unit->cap);\n\tdmar_unit->ecap_iotlb_offset = iommu_ecap_iro(dmar_unit->ecap) * 16U;\n\tdmar_unit->root_table_addr = hva2hpa(get_root_table(dmar_unit->index));\n\tdmar_unit->ir_table_addr = hva2hpa(get_ir_table(dmar_unit->index));\n\n#if DBG_IOMMU\n\tpr_info(\"version:0x%x, cap:0x%lx, ecap:0x%lx\",\n\t\tiommu_read32(dmar_unit, DMAR_VER_REG), dmar_unit->cap, dmar_unit->ecap);\n\tpr_info(\"sagaw:0x%x, msagaw:0x%x, iotlb offset 0x%x\",\n\t\tiommu_cap_sagaw(dmar_unit->cap), dmar_unit->cap_msagaw, dmar_unit->ecap_iotlb_offset);\n\n\tdmar_unit_show_capability(dmar_unit);\n#endif\n\n\t/* check capability */\n\tif ((iommu_cap_super_page_val(dmar_unit->cap) & 0x1U) == 0U) {\n\t\tpr_fatal(\"%s: dmar uint doesn't support 2MB page!\\n\", __func__);\n\t\tret = -ENODEV;\n\t} else if ((iommu_cap_super_page_val(dmar_unit->cap) & 0x2U) == 0U) {\n\t\tpr_fatal(\"%s: dmar uint doesn't support 1GB page!\\n\", __func__);\n\t\tret = -ENODEV;\n\t} else if (iommu_ecap_qi(dmar_unit->ecap) == 0U) {\n\t\tpr_fatal(\"%s: dmar unit doesn't support Queued Invalidation!\", __func__);\n\t\tret = -ENODEV;\n\t} else if (iommu_ecap_ir(dmar_unit->ecap) == 0U) {\n\t\tpr_fatal(\"%s: dmar unit doesn't support Interrupt Remapping!\", __func__);\n\t\tret = -ENODEV;\n\t} else if (iommu_ecap_eim(dmar_unit->ecap) == 0U) {\n\t\tpr_fatal(\"%s: dmar unit doesn't support Extended Interrupt Mode!\", __func__);\n\t\tret = -ENODEV;\n\t} else {\n\t\tif ((iommu_ecap_c(dmar_unit->ecap) == 0U) && (!dmar_unit->drhd->ignore)) {\n\t\t\tiommu_page_walk_coherent = false;\n\t\t}\n\t\tdmar_disable_translation(dmar_unit);\n\t}\n\n\treturn ret;\n}\n\nstatic struct dmar_drhd_rt *ioapic_to_dmaru(uint16_t ioapic_id, union pci_bdf *sid)\n{\n\tstruct dmar_drhd_rt *dmar_unit = NULL;\n\tuint32_t i, j;\n\tbool found = false;\n\n\tfor (j = 0U; j < platform_dmar_info->drhd_count; j++) {\n\t\tdmar_unit = &dmar_drhd_units[j];\n\t\tfor (i = 0U; i < dmar_unit->drhd->dev_cnt; i++) {\n\t\t\tif ((dmar_unit->drhd->devices[i].type == ACPI_DMAR_SCOPE_TYPE_IOAPIC) &&\n\t\t\t\t\t(dmar_unit->drhd->devices[i].id == ioapic_id)) {\n\t\t\t\tsid->fields.devfun = dmar_unit->drhd->devices[i].devfun;\n\t\t\t\tsid->fields.bus = dmar_unit->drhd->devices[i].bus;\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (found) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (j == platform_dmar_info->drhd_count) {\n\t\tdmar_unit = NULL;\n\t}\n\n\treturn dmar_unit;\n}\n\nstatic struct dmar_drhd_rt *device_to_dmaru(uint8_t bus, uint8_t devfun)\n{\n\tstruct dmar_drhd_rt *dmaru = NULL;\n\tuint16_t bdf = ((uint16_t)bus << 8U) | devfun;\n\tuint32_t index = pci_lookup_drhd_for_pbdf(bdf);\n\n\tif (index == INVALID_DRHD_INDEX) {\n\t\tpr_fatal(\"BDF %02x:%02x:%x has no IOMMU\\n\", bus, devfun >> 3U, devfun & 7U);\n\t\t/*\n\t\t * pci_lookup_drhd_for_pbdf would return -1U for any of the reasons\n\t\t * 1) PCI device with bus, devfun does not exist on platform\n\t\t * 2) ACRN had issues finding the device with bus, devfun during init\n\t\t * 3) DMAR tables provided by ACPI for this platform are incorrect\n\t\t */\n\t} else {\n\t\tdmaru = &dmar_drhd_units[index];\n\t}\n\n\treturn dmaru;\n}\n\nstatic void dmar_issue_qi_request(struct dmar_drhd_rt *dmar_unit, struct dmar_entry invalidate_desc)\n{\n\tstruct dmar_entry *invalidate_desc_ptr;\n\tuint32_t qi_status = 0U;\n\tuint64_t start;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\n\tinvalidate_desc_ptr = (struct dmar_entry *)(dmar_unit->qi_queue + dmar_unit->qi_tail);\n\n\tinvalidate_desc_ptr->hi_64 = invalidate_desc.hi_64;\n\tinvalidate_desc_ptr->lo_64 = invalidate_desc.lo_64;\n\tdmar_unit->qi_tail = (dmar_unit->qi_tail + DMAR_QI_INV_ENTRY_SIZE) % DMAR_INVALIDATION_QUEUE_SIZE;\n\n\tinvalidate_desc_ptr++;\n\n\tinvalidate_desc_ptr->hi_64 = hva2hpa(&qi_status);\n\tinvalidate_desc_ptr->lo_64 = DMAR_INV_WAIT_DESC_LOWER;\n\tdmar_unit->qi_tail = (dmar_unit->qi_tail + DMAR_QI_INV_ENTRY_SIZE) % DMAR_INVALIDATION_QUEUE_SIZE;\n\n\tqi_status = DMAR_INV_STATUS_INCOMPLETE;\n\tiommu_write32(dmar_unit, DMAR_IQT_REG, dmar_unit->qi_tail);\n\n\tstart = cpu_ticks();\n\twhile (qi_status != DMAR_INV_STATUS_COMPLETED) {\n\t\tif ((cpu_ticks() - start) > TICKS_PER_MS) {\n\t\t\tpr_err(\"DMAR OP Timeout! @ %s\", __func__);\n\t\t\tbreak;\n\t\t}\n\t\tasm_pause();\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\n/*\n * did: domain id\n * sid: source id\n * fm: function mask\n * cirg: cache-invalidation request granularity\n */\nstatic void dmar_invalid_context_cache(struct dmar_drhd_rt *dmar_unit,\n\tuint16_t did, uint16_t sid, uint8_t fm, enum dmar_cirg_type cirg)\n{\n\tstruct dmar_entry invalidate_desc;\n\n\tinvalidate_desc.hi_64 = 0UL;\n\tinvalidate_desc.lo_64 = DMAR_INV_CONTEXT_CACHE_DESC;\n\tswitch (cirg) {\n\tcase DMAR_CIRG_GLOBAL:\n\t\tinvalidate_desc.lo_64 |= DMA_CONTEXT_GLOBAL_INVL;\n\t\tbreak;\n\tcase DMAR_CIRG_DOMAIN:\n\t\tinvalidate_desc.lo_64 |= DMA_CONTEXT_DOMAIN_INVL | dma_ccmd_did(did);\n\t\tbreak;\n\tcase DMAR_CIRG_DEVICE:\n\t\tinvalidate_desc.lo_64 |= DMA_CONTEXT_DEVICE_INVL | dma_ccmd_did(did) | dma_ccmd_sid(sid) | dma_ccmd_fm(fm);\n\t\tbreak;\n\tdefault:\n\t\tinvalidate_desc.lo_64 = 0UL;\n\t\tpr_err(\"unknown CIRG type\");\n\t\tbreak;\n\t}\n\n\tif (invalidate_desc.lo_64 != 0UL) {\n\t\tdmar_issue_qi_request(dmar_unit, invalidate_desc);\n\t}\n}\n\nstatic void dmar_invalid_context_cache_global(struct dmar_drhd_rt *dmar_unit)\n{\n\tdmar_invalid_context_cache(dmar_unit, 0U, 0U, 0U, DMAR_CIRG_GLOBAL);\n}\n\nstatic void dmar_invalid_iotlb(struct dmar_drhd_rt *dmar_unit, uint16_t did, uint64_t address, uint8_t am,\n\t\t\t       bool hint, enum dmar_iirg_type iirg)\n{\n\t/* set Drain Reads & Drain Writes,\n\t * if hardware doesn't support it, will be ignored by hardware\n\t */\n\tstruct dmar_entry invalidate_desc;\n\tuint64_t addr = 0UL;\n\n\tinvalidate_desc.hi_64 = 0UL;\n\n\tinvalidate_desc.lo_64 = DMA_IOTLB_DR | DMA_IOTLB_DW | DMAR_INV_IOTLB_DESC;\n\n\tswitch (iirg) {\n\tcase DMAR_IIRG_GLOBAL:\n\t\tinvalidate_desc.lo_64 |= DMA_IOTLB_GLOBAL_INVL;\n\t\tbreak;\n\tcase DMAR_IIRG_DOMAIN:\n\t\tinvalidate_desc.lo_64 |= DMA_IOTLB_DOMAIN_INVL | dma_iotlb_did(did);\n\t\tbreak;\n\tcase DMAR_IIRG_PAGE:\n\t\tinvalidate_desc.lo_64 |= DMA_IOTLB_PAGE_INVL | dma_iotlb_did(did);\n\t\taddr = address | dma_iotlb_invl_addr_am(am);\n\t\tif (hint) {\n\t\t\taddr |= DMA_IOTLB_INVL_ADDR_IH_UNMODIFIED;\n\t\t}\n\t\tinvalidate_desc.hi_64 |= addr;\n\t\tbreak;\n\tdefault:\n\t\tinvalidate_desc.lo_64 = 0UL;\n\t\tpr_err(\"unknown IIRG type\");\n\t}\n\n\tif (invalidate_desc.lo_64 != 0UL) {\n\t\tdmar_issue_qi_request(dmar_unit, invalidate_desc);\n\t}\n}\n\n/* Invalidate IOTLB globally,\n * all iotlb entries are invalidated,\n * all PASID-cache entries are invalidated,\n * all paging-structure-cache entries are invalidated.\n */\nstatic void dmar_invalid_iotlb_global(struct dmar_drhd_rt *dmar_unit)\n{\n\tdmar_invalid_iotlb(dmar_unit, 0U, 0UL, 0U, false, DMAR_IIRG_GLOBAL);\n}\n\n/* @pre dmar_unit->ir_table_addr != NULL */\nstatic void dmar_set_intr_remap_table(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint64_t address;\n\tuint32_t status;\n\tuint8_t size;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\n\t/* Set number of bits needed to represent the entries minus 1 */\n\tsize = (uint8_t) fls32(CONFIG_MAX_IR_ENTRIES) - 1U;\n\taddress = dmar_unit->ir_table_addr | DMAR_IR_ENABLE_EIM | size;\n\n\tiommu_write64(dmar_unit, DMAR_IRTA_REG, address);\n\n\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd | DMA_GCMD_SIRTP);\n\n\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_IRTPS, 0U, &status);\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_invalid_iec(struct dmar_drhd_rt *dmar_unit, uint16_t intr_index,\n\t\t\t\tuint8_t index_mask, bool is_global)\n{\n\tstruct dmar_entry invalidate_desc;\n\n\tinvalidate_desc.hi_64 = 0UL;\n\tinvalidate_desc.lo_64 = DMAR_INV_IEC_DESC;\n\n\tif (is_global) {\n\t\tinvalidate_desc.lo_64 |= DMAR_IEC_GLOBAL_INVL;\n\t} else {\n\t\tinvalidate_desc.lo_64 |= DMAR_IECI_INDEXED | dma_iec_index(intr_index, index_mask);\n\t}\n\n\tif (invalidate_desc.lo_64 != 0UL) {\n\t\tdmar_issue_qi_request(dmar_unit, invalidate_desc);\n\t}\n}\n\nstatic void dmar_invalid_iec_global(struct dmar_drhd_rt *dmar_unit)\n{\n\tdmar_invalid_iec(dmar_unit, 0U, 0U, true);\n}\n\n/* @pre dmar_unit->root_table_addr != NULL */\nstatic void dmar_set_root_table(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tiommu_write64(dmar_unit, DMAR_RTADDR_REG, dmar_unit->root_table_addr);\n\n\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd | DMA_GCMD_SRTP);\n\n\t/* 32-bit register */\n\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_RTPS, 0U, &status);\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_fault_event_mask(struct dmar_drhd_rt *dmar_unit)\n{\n\tspinlock_obtain(&(dmar_unit->lock));\n\tiommu_write32(dmar_unit, DMAR_FECTL_REG, DMA_FECTL_IM);\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_fault_event_unmask(struct dmar_drhd_rt *dmar_unit)\n{\n\tspinlock_obtain(&(dmar_unit->lock));\n\tiommu_write32(dmar_unit, DMAR_FECTL_REG, 0U);\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_fault_msi_write(struct dmar_drhd_rt *dmar_unit,\n\t\t\tuint32_t vector)\n{\n\tuint32_t data;\n\tuint32_t addr_low;\n\tuint32_t lapic_id = get_cur_lapic_id();\n\n\tdata = DMAR_MSI_DELIVERY_LOWPRI | vector;\n\t/* redirection hint: 0\n\t * destination mode: 0\n\t */\n\taddr_low = 0xFEE00000U | ((uint32_t)(lapic_id) << 12U);\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tiommu_write32(dmar_unit, DMAR_FEDATA_REG, data);\n\tiommu_write32(dmar_unit, DMAR_FEADDR_REG, addr_low);\n\tspinlock_release(&(dmar_unit->lock));\n}\n\n#if DBG_IOMMU\nstatic void fault_status_analysis(uint32_t status)\n{\n\tif (dma_fsts_pfo(status)) {\n\t\tpr_info(\"Primary Fault Overflow\");\n\t}\n\n\tif (dma_fsts_ppf(status)) {\n\t\tpr_info(\"Primary Pending Fault\");\n\t}\n\n\tif (dma_fsts_afo(status)) {\n\t\tpr_info(\"Advanced Fault Overflow\");\n\t}\n\n\tif (dma_fsts_apf(status)) {\n\t\tpr_info(\"Advanced Pending Fault\");\n\t}\n\n\tif (dma_fsts_iqe(status)) {\n\t\tpr_info(\"Invalidation Queue Error\");\n\t}\n\n\tif (dma_fsts_ice(status)) {\n\t\tpr_info(\"Invalidation Completion Error\");\n\t}\n\n\tif (dma_fsts_ite(status)) {\n\t\tpr_info(\"Invalidation Time-out Error\");\n\t}\n\n\tif (dma_fsts_pro(status)) {\n\t\tpr_info(\"Page Request Overflow\");\n\t}\n}\n#endif\n\nstatic void fault_record_analysis(__unused uint64_t low, uint64_t high)\n{\n\tunion pci_bdf dmar_bdf;\n\n\tif (!dma_frcd_up_f(high)) {\n\t\tdmar_bdf.value = dma_frcd_up_sid(high);\n\t\t/* currently skip PASID related parsing */\n\t\tpr_info(\"%s, Reason: 0x%x, SID: %x.%x.%x @0x%lx\",\n\t\t\t(dma_frcd_up_t(high) != 0U) ? \"Read/Atomic\" : \"Write\", dma_frcd_up_fr(high),\n\t\t\tdmar_bdf.bits.b, dmar_bdf.bits.d, dmar_bdf.bits.f, low);\n#if DBG_IOMMU\n\t\tif (iommu_ecap_dt(dmar_unit->ecap) != 0U) {\n\t\t\tpr_info(\"Address Type: 0x%x\", dma_frcd_up_at(high));\n\t\t}\n#endif\n\t}\n}\n\nstatic void dmar_fault_handler(uint32_t irq, void *data)\n{\n\tstruct dmar_drhd_rt *dmar_unit = (struct dmar_drhd_rt *)data;\n\tuint32_t fsr;\n\tuint32_t index;\n\tuint32_t record_reg_offset;\n\tstruct dmar_entry fault_record;\n\tint32_t loop = 0;\n\n\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: irq = %d\", __func__, irq);\n\n\tfsr = iommu_read32(dmar_unit, DMAR_FSTS_REG);\n\n#if DBG_IOMMU\n\tfault_status_analysis(fsr);\n#endif\n\n\twhile (dma_fsts_ppf(fsr)) {\n\t\tloop++;\n\t\tindex = dma_fsts_fri(fsr);\n\t\trecord_reg_offset = (uint32_t)dmar_unit->cap_fault_reg_offset + (index * 16U);\n\t\tif (index >= dmar_unit->cap_num_fault_regs) {\n\t\t\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: invalid FR Index\", __func__);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* read 128-bit fault recording register */\n\t\tfault_record.lo_64 = iommu_read64(dmar_unit, record_reg_offset);\n\t\tfault_record.hi_64 = iommu_read64(dmar_unit, record_reg_offset + 8U);\n\n\t\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: record[%d] @0x%x:  0x%lx, 0x%lx\",\n\t\t\t__func__, index, record_reg_offset, fault_record.lo_64, fault_record.hi_64);\n\n\t\tfault_record_analysis(fault_record.lo_64, fault_record.hi_64);\n\n\t\t/* write to clear */\n\t\tiommu_write64(dmar_unit, record_reg_offset, fault_record.lo_64);\n\t\tiommu_write64(dmar_unit, record_reg_offset + 8U, fault_record.hi_64);\n\n#ifdef DMAR_FAULT_LOOP_MAX\n\t\tif (loop > DMAR_FAULT_LOOP_MAX) {\n\t\t\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: loop more than %d times\", __func__, DMAR_FAULT_LOOP_MAX);\n\t\t\tbreak;\n\t\t}\n#endif\n\n\t\tfsr = iommu_read32(dmar_unit, DMAR_FSTS_REG);\n\t}\n}\n\nstatic void dmar_setup_interrupt(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t vector;\n\tint32_t retval = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif (dmar_unit->dmar_irq == IRQ_INVALID) {\n\t\tretval = request_irq(IRQ_INVALID, dmar_fault_handler, dmar_unit, IRQF_NONE);\n\t\tdmar_unit->dmar_irq = (uint32_t)retval;\n\t}\n\tspinlock_release(&(dmar_unit->lock));\n\t/* the panic will only happen before any VM starts running */\n\tif (retval < 0) {\n\t\tpanic(\"dmar[%d] fail to setup interrupt\", dmar_unit->index);\n\t}\n\n\tvector = irq_to_vector(dmar_unit->dmar_irq);\n\tdev_dbg(DBG_LEVEL_IOMMU, \"irq#%d vector#%d for dmar_unit\", dmar_unit->dmar_irq, vector);\n\n\tdmar_fault_msi_write(dmar_unit, vector);\n\tdmar_fault_event_unmask(dmar_unit);\n}\n\nstatic void dmar_enable_qi(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\n\tdmar_unit->qi_queue = hva2hpa(get_qi_queue(dmar_unit->index));\n\tiommu_write64(dmar_unit, DMAR_IQA_REG, dmar_unit->qi_queue);\n\n\tiommu_write32(dmar_unit, DMAR_IQT_REG, 0U);\n\n\tif ((dmar_unit->gcmd & DMA_GCMD_QIE) == 0U) {\n\t\tdmar_unit->gcmd |= DMA_GCMD_QIE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG,\tdmar_unit->gcmd);\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_QIES, 0U, &status);\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_disable_qi(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\n\tif ((dmar_unit->gcmd & DMA_GCMD_QIE) == DMA_GCMD_QIE) {\n\t\tdmar_unit->gcmd &= ~DMA_GCMD_QIE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG,\tdmar_unit->gcmd);\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_QIES, DMA_GSTS_QIES, &status);\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void prepare_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tdev_dbg(DBG_LEVEL_IOMMU, \"enable dmar uint [0x%x]\", dmar_unit->drhd->reg_base_addr);\n\tdmar_setup_interrupt(dmar_unit);\n\tdmar_set_root_table(dmar_unit);\n\tdmar_enable_qi(dmar_unit);\n\tdmar_set_intr_remap_table(dmar_unit);\n}\n\nstatic void enable_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tdev_dbg(DBG_LEVEL_IOMMU, \"enable dmar uint [0x%x]\", dmar_unit->drhd->reg_base_addr);\n\tdmar_invalid_context_cache_global(dmar_unit);\n\tdmar_invalid_iotlb_global(dmar_unit);\n\tdmar_invalid_iec_global(dmar_unit);\n\tdmar_enable_translation(dmar_unit);\n}\n\nstatic void disable_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tdmar_disable_qi(dmar_unit);\n\tdmar_disable_translation(dmar_unit);\n\tdmar_fault_event_mask(dmar_unit);\n\tdmar_disable_intr_remapping(dmar_unit);\n}\n\nstatic void suspend_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t i;\n\n\tdmar_invalid_context_cache_global(dmar_unit);\n\tdmar_invalid_iotlb_global(dmar_unit);\n\tdmar_invalid_iec_global(dmar_unit);\n\n\tdisable_dmar(dmar_unit);\n\n\t/* save IOMMU fault register state */\n\tfor (i = 0U; i < IOMMU_FAULT_REGISTER_STATE_NUM; i++) {\n\t\tdmar_unit->fault_state[i] =  iommu_read32(dmar_unit, DMAR_FECTL_REG + (i * IOMMU_FAULT_REGISTER_SIZE));\n\t}\n}\n\nstatic void resume_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t i;\n\n\t/* restore IOMMU fault register state */\n\tfor (i = 0U; i < IOMMU_FAULT_REGISTER_STATE_NUM; i++) {\n\t\tiommu_write32(dmar_unit, DMAR_FECTL_REG + (i * IOMMU_FAULT_REGISTER_SIZE), dmar_unit->fault_state[i]);\n\t}\n\tprepare_dmar(dmar_unit);\n\tenable_dmar(dmar_unit);\n\tdmar_enable_intr_remapping(dmar_unit);\n}\n\nstatic inline bool is_dmar_unit_ignored(const struct dmar_drhd_rt *dmar_unit)\n{\n\tbool ignored = false;\n\n\tif ((dmar_unit != NULL) && (dmar_unit->drhd->ignore)) {\n\t\tignored = true;\n\t}\n\n\treturn ignored;\n}\n\nstatic bool is_dmar_unit_valid(const struct dmar_drhd_rt *dmar_unit, union pci_bdf sid)\n{\n\tbool valid = false;\n\n\tif (dmar_unit == NULL) {\n\t\tpr_err(\"no dmar unit found for device: %x:%x.%x\", sid.bits.b, sid.bits.d, sid.bits.f);\n\t} else if (dmar_unit->drhd->ignore) {\n\t\tdev_dbg(DBG_LEVEL_IOMMU, \"device is ignored : %x:%x.%x\", sid.bits.b, sid.bits.d, sid.bits.f);\n\t} else {\n\t\tvalid = true;\n\t}\n\n\treturn valid;\n}\n\n/* @pre bus < CONFIG_IOMMU_BUS_NUM */\nstatic int32_t iommu_attach_device(const struct iommu_domain *domain, uint8_t bus, uint8_t devfun)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tstruct dmar_entry *root_table;\n\tuint64_t context_table_addr;\n\tstruct dmar_entry *context;\n\tstruct dmar_entry *root_entry;\n\tstruct dmar_entry *context_entry;\n\tuint64_t hi_64 = 0UL;\n\tuint64_t lo_64 = 0UL;\n\tint32_t ret = -EINVAL;\n\t/* source id */\n\tunion pci_bdf sid;\n\n\tsid.fields.bus = bus;\n\tsid.fields.devfun = devfun;\n\n\tdmar_unit = device_to_dmaru(bus, devfun);\n\tif (is_dmar_unit_valid(dmar_unit, sid) && dmar_unit_support_aw(dmar_unit, domain->addr_width)) {\n\t\troot_table = (struct dmar_entry *)hpa2hva(dmar_unit->root_table_addr);\n\t\troot_entry = root_table + bus;\n\n\t\tif (dmar_get_bitslice(root_entry->lo_64,\n\t\t\t\t\tROOT_ENTRY_LOWER_PRESENT_MASK,\n\t\t\t\t\tROOT_ENTRY_LOWER_PRESENT_POS) == 0UL) {\n\t\t\t/* create context table for the bus if not present */\n\t\t\tcontext_table_addr = hva2hpa(get_ctx_table(dmar_unit->index, bus));\n\n\t\t\tcontext_table_addr = context_table_addr >> PAGE_SHIFT;\n\n\t\t\tlo_64 = dmar_set_bitslice(lo_64,\n\t\t\t\t\tROOT_ENTRY_LOWER_CTP_MASK, ROOT_ENTRY_LOWER_CTP_POS, context_table_addr);\n\t\t\tlo_64 = dmar_set_bitslice(lo_64,\n\t\t\t\t\tROOT_ENTRY_LOWER_PRESENT_MASK, ROOT_ENTRY_LOWER_PRESENT_POS, 1UL);\n\n\t\t\troot_entry->hi_64 = 0UL;\n\t\t\troot_entry->lo_64 = lo_64;\n\t\t\tiommu_flush_cache(root_entry, sizeof(struct dmar_entry));\n\t\t} else {\n\t\t\tcontext_table_addr = dmar_get_bitslice(root_entry->lo_64,\n\t\t\t\t\tROOT_ENTRY_LOWER_CTP_MASK, ROOT_ENTRY_LOWER_CTP_POS);\n\t\t}\n\n\t\tcontext_table_addr = context_table_addr << PAGE_SHIFT;\n\n\t\tcontext = (struct dmar_entry *)hpa2hva(context_table_addr);\n\t\tcontext_entry = context + devfun;\n\n\t\tif (dmar_get_bitslice(context_entry->lo_64, CTX_ENTRY_LOWER_P_MASK, CTX_ENTRY_LOWER_P_POS) != 0UL) {\n\t\t\t/* the context entry should not be present */\n\t\t\tpr_err(\"%s: context entry@0x%lx (Lower:%x) \", __func__, context_entry, context_entry->lo_64);\n\t\t\tpr_err(\"already present for %x:%x.%x\", bus, sid.bits.d, sid.bits.f);\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\t/* setup context entry for the devfun */\n\t\t\t/* TODO: add Device TLB support */\n\t\t\thi_64 = dmar_set_bitslice(hi_64, CTX_ENTRY_UPPER_AW_MASK, CTX_ENTRY_UPPER_AW_POS,\n\t\t\t\t\t(uint64_t)width_to_agaw(domain->addr_width));\n\t\t\tlo_64 = dmar_set_bitslice(lo_64, CTX_ENTRY_LOWER_TT_MASK, CTX_ENTRY_LOWER_TT_POS,\n\t\t\t\t\tDMAR_CTX_TT_UNTRANSLATED);\n\t\t\thi_64 = dmar_set_bitslice(hi_64, CTX_ENTRY_UPPER_DID_MASK, CTX_ENTRY_UPPER_DID_POS,\n\t\t\t\t(uint64_t)vmid_to_domainid(domain->vm_id));\n\t\t\tlo_64 = dmar_set_bitslice(lo_64, CTX_ENTRY_LOWER_SLPTPTR_MASK, CTX_ENTRY_LOWER_SLPTPTR_POS,\n\t\t\t\tdomain->trans_table_ptr >> PAGE_SHIFT);\n\t\t\tlo_64 = dmar_set_bitslice(lo_64, CTX_ENTRY_LOWER_P_MASK, CTX_ENTRY_LOWER_P_POS, 1UL);\n\n\t\t\tcontext_entry->hi_64 = hi_64;\n\t\t\tcontext_entry->lo_64 = lo_64;\n\t\t\tiommu_flush_cache(context_entry, sizeof(struct dmar_entry));\n\t\t\tret = 0;\n\t\t}\n\t} else if (is_dmar_unit_ignored(dmar_unit)) {\n\t       ret = 0;\n\t}\n\n\treturn ret;\n}\n\n/* @pre bus < CONFIG_IOMMU_BUS_NUM */\nstatic int32_t iommu_detach_device(const struct iommu_domain *domain, uint8_t bus, uint8_t devfun)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tstruct dmar_entry *root_table;\n\tuint64_t context_table_addr;\n\tstruct dmar_entry *context;\n\tstruct dmar_entry *root_entry;\n\tstruct dmar_entry *context_entry;\n\t/* source id */\n\tunion pci_bdf sid;\n\tint32_t ret = -EINVAL;\n\n\tdmar_unit = device_to_dmaru(bus, devfun);\n\n\tsid.fields.bus = bus;\n\tsid.fields.devfun = devfun;\n\n\tif (is_dmar_unit_valid(dmar_unit, sid)) {\n\t\troot_table = (struct dmar_entry *)hpa2hva(dmar_unit->root_table_addr);\n\t\troot_entry = root_table + bus;\n\t\tret = 0;\n\n\t\tcontext_table_addr = dmar_get_bitslice(root_entry->lo_64,  ROOT_ENTRY_LOWER_CTP_MASK,\n\t\t\t\t\t\t\tROOT_ENTRY_LOWER_CTP_POS);\n\t\tcontext_table_addr = context_table_addr << PAGE_SHIFT;\n\t\tcontext = (struct dmar_entry *)hpa2hva(context_table_addr);\n\n\t\tcontext_entry = context + devfun;\n\n\t\tif ((context == NULL) || (context_entry == NULL)) {\n\t\t\tpr_err(\"dmar context entry is invalid\");\n\t\t\tret = -EINVAL;\n\t\t} else if ((uint16_t)dmar_get_bitslice(context_entry->hi_64, CTX_ENTRY_UPPER_DID_MASK,\n\t\t\t\t\t\tCTX_ENTRY_UPPER_DID_POS) != vmid_to_domainid(domain->vm_id)) {\n\t\t\tpr_err(\"%s: domain id mismatch\", __func__);\n\t\t\tret = -EPERM;\n\t\t} else {\n\t\t\t/* clear the present bit first */\n\t\t\tcontext_entry->lo_64 = 0UL;\n\t\t\tcontext_entry->hi_64 = 0UL;\n\t\t\tiommu_flush_cache(context_entry, sizeof(struct dmar_entry));\n\n\t\t\tdmar_invalid_context_cache(dmar_unit, vmid_to_domainid(domain->vm_id), sid.value, 0U,\n\t\t\t\t\t\t\tDMAR_CIRG_DEVICE);\n\t\t\tdmar_invalid_iotlb(dmar_unit, vmid_to_domainid(domain->vm_id), 0UL, 0U, false,\n\t\t\t\t\t\t\tDMAR_IIRG_DOMAIN);\n\t\t}\n\t} else if (is_dmar_unit_ignored(dmar_unit)) {\n\t       ret = 0;\n\t}\n\n\treturn ret;\n}\n\n/*\n * @pre action != NULL\n * As an internal API, VT-d code can guarantee action is not NULL.\n */\nstatic void do_action_for_iommus(void (*action)(struct dmar_drhd_rt *))\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tuint32_t i;\n\n\tfor (i = 0U; i < platform_dmar_info->drhd_count; i++) {\n\t\tdmar_unit = &dmar_drhd_units[i];\n\t\tif (!dmar_unit->drhd->ignore) {\n\t\t\taction(dmar_unit);\n\t\t} else {\n\t\t\tdev_dbg(DBG_LEVEL_IOMMU, \"ignore dmar_unit @0x%x\", dmar_unit->drhd->reg_base_addr);\n\t\t}\n\t}\n}\n\nstruct iommu_domain *create_iommu_domain(uint16_t vm_id, uint64_t translation_table, uint32_t addr_width)\n{\n\tstatic struct iommu_domain iommu_domains[MAX_DOMAIN_NUM];\n\tstruct iommu_domain *domain;\n\n\t/* TODO: check if a domain with the vm_id exists */\n\n\tif (translation_table == 0UL) {\n\t\tpr_err(\"translation table is NULL\");\n\t\tdomain = NULL;\n\t} else {\n\t\t/*\n\t\t * A hypercall is called to create an iommu domain for a valid VM,\n\t\t * and hv code limit the VM number to CONFIG_MAX_VM_NUM.\n\t\t * So the array iommu_domains will not be accessed out of range.\n\t\t */\n\t\tdomain = &iommu_domains[vmid_to_domainid(vm_id)];\n\n\t\tdomain->vm_id = vm_id;\n\t\tdomain->trans_table_ptr = translation_table;\n\t\tdomain->addr_width = addr_width;\n\n\t\tdev_dbg(DBG_LEVEL_IOMMU, \"create domain [%d]: vm_id = %hu, ept@0x%x\",\n\t\t\tvmid_to_domainid(domain->vm_id), domain->vm_id, domain->trans_table_ptr);\n\t}\n\n\treturn domain;\n}\n\n/**\n * @pre domain != NULL\n */\nvoid destroy_iommu_domain(struct iommu_domain *domain)\n{\n\t/* TODO: check if any device assigned to this domain */\n\t(void)memset(domain, 0U, sizeof(*domain));\n}\n\n/*\n * @pre (from_domain != NULL) || (to_domain != NULL)\n */\n\nint32_t move_pt_device(const struct iommu_domain *from_domain, const struct iommu_domain *to_domain, uint8_t bus, uint8_t devfun)\n{\n\tint32_t status = 0;\n\tuint16_t bus_local = bus;\n\n\t/* TODO: check if the device assigned */\n\n\tif (bus_local < CONFIG_IOMMU_BUS_NUM) {\n\t\tif (from_domain != NULL) {\n\t\t\tstatus = iommu_detach_device(from_domain, bus, devfun);\n\t\t}\n\n\t\tif ((status == 0) && (to_domain != NULL)) {\n\t\t\tstatus = iommu_attach_device(to_domain, bus, devfun);\n\t\t}\n\t} else {\n\t\tstatus = -EINVAL;\n\t}\n\n\treturn status;\n}\n\nvoid enable_iommu(void)\n{\n\tdo_action_for_iommus(enable_dmar);\n}\n\nvoid suspend_iommu(void)\n{\n\tdo_action_for_iommus(suspend_dmar);\n}\n\nvoid resume_iommu(void)\n{\n\tdo_action_for_iommus(resume_dmar);\n}\n\n/**\n * @post return != NULL\n * @post return->drhd_count > 0U\n */\nstatic struct dmar_info *get_dmar_info(void)\n{\n#ifdef CONFIG_ACPI_PARSE_ENABLED\n\tparse_dmar_table(&plat_dmar_info);\n#endif\n\treturn &plat_dmar_info;\n}\n\nint32_t init_iommu(void)\n{\n\tint32_t ret = 0;\n\n\tplatform_dmar_info = get_dmar_info();\n\n\tif ((platform_dmar_info == NULL) || (platform_dmar_info->drhd_count == 0U)) {\n\t\tpr_fatal(\"%s: can't find dmar info\\n\", __func__);\n\t\tret = -ENODEV;\n\t} else if (platform_dmar_info->drhd_count > CONFIG_MAX_IOMMU_NUM) {\n\t\tpr_fatal(\"%s: dmar count(%d) beyond the limitation(%d)\\n\",\n\t\t\t\t__func__, platform_dmar_info->drhd_count, CONFIG_MAX_IOMMU_NUM);\n\t\tret = -EINVAL;\n\t} else {\n\t\tret = register_hrhd_units();\n\t\tif (ret == 0) {\n\t\t\tdo_action_for_iommus(prepare_dmar);\n\t\t}\n\t}\n\treturn ret;\n}\n\n/* Allocate continuous IRTEs specified by num, num can be 1, 2, 4, 8, 16, 32 */\nstatic uint16_t alloc_irtes(struct dmar_drhd_rt *dmar_unit, const uint16_t num)\n{\n\tuint16_t irte_idx;\n\tuint64_t mask = (1UL << num) - 1U;\n\tuint64_t test_mask;\n\n\tASSERT((bitmap_weight(num) == 1U) && (num <= 32U));\n\n\tspinlock_obtain(&dmar_unit->lock);\n\tfor (irte_idx = 0U; irte_idx < CONFIG_MAX_IR_ENTRIES; irte_idx += num) {\n\t\ttest_mask = mask << (irte_idx & 0x3FU);\n\t\tif ((dmar_unit->irte_alloc_bitmap[irte_idx >> 6U] & test_mask) == 0UL) {\n\t\t\tdmar_unit->irte_alloc_bitmap[irte_idx >> 6U] |= test_mask;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspinlock_release(&dmar_unit->lock);\n\n\treturn (irte_idx < CONFIG_MAX_IR_ENTRIES) ? irte_idx: INVALID_IRTE_ID;\n}\n\nstatic bool is_irte_reserved(const struct dmar_drhd_rt *dmar_unit, uint16_t index)\n{\n\treturn ((dmar_unit->irte_reserved_bitmap[index >> 6U] & (1UL << (index & 0x3FU))) != 0UL);\n}\n\nint32_t dmar_reserve_irte(const struct intr_source *intr_src, uint16_t num, uint16_t *start_id)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tunion pci_bdf sid;\n\tuint64_t mask = (1UL << num) - 1U;\n\tint32_t ret = -EINVAL;\n\n\tif (intr_src->is_msi) {\n\t\tdmar_unit = device_to_dmaru((uint8_t)intr_src->src.msi.bits.b, intr_src->src.msi.fields.devfun);\n\t\tsid.value = (uint16_t)(intr_src->src.msi.value);\n\t} else {\n\t\tdmar_unit = ioapic_to_dmaru(intr_src->src.ioapic_id, &sid);\n\t}\n\n\tif (is_dmar_unit_valid(dmar_unit, sid)) {\n\t\t*start_id = alloc_irtes(dmar_unit, num);\n\t\tif (*start_id < CONFIG_MAX_IR_ENTRIES) {\n\t\t\tdmar_unit->irte_reserved_bitmap[*start_id >> 6U] |= mask << (*start_id & 0x3FU);\n\t\t}\n\t\tret = 0;\n\t}\n\n\tpr_dbg(\"%s: for dev 0x%x:%x.%x, reserve %u entry for MSI(%d), start from %d\",\n\t\t__func__, sid.bits.b, sid.bits.d, sid.bits.f, num, intr_src->is_msi, *start_id);\n\treturn ret;\n}\n\nint32_t dmar_assign_irte(const struct intr_source *intr_src, union dmar_ir_entry *irte,\n\tuint16_t idx_in, uint16_t *idx_out)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tunion dmar_ir_entry *ir_table, *ir_entry;\n\tunion pci_bdf sid;\n\tuint64_t trigger_mode;\n\tint32_t ret = -EINVAL;\n\n\tif (intr_src->is_msi) {\n\t\tdmar_unit = device_to_dmaru((uint8_t)intr_src->src.msi.bits.b, intr_src->src.msi.fields.devfun);\n\t\tsid.value = (uint16_t)(intr_src->src.msi.value);\n\t\ttrigger_mode = 0x0UL;\n\t} else {\n\t\tdmar_unit = ioapic_to_dmaru(intr_src->src.ioapic_id, &sid);\n\t\ttrigger_mode = irte->bits.remap.trigger_mode;\n\t}\n\n\tif (is_dmar_unit_valid(dmar_unit, sid)) {\n\t\tdmar_enable_intr_remapping(dmar_unit);\n\n\t\tir_table = (union dmar_ir_entry *)hpa2hva(dmar_unit->ir_table_addr);\n\t\t*idx_out = idx_in;\n\t\tif (idx_in == INVALID_IRTE_ID) {\n\t\t\t*idx_out = alloc_irtes(dmar_unit, 1U);\n\t\t}\n\t\tif (*idx_out < CONFIG_MAX_IR_ENTRIES) {\n\t\t\tir_entry = ir_table + *idx_out;\n\n\t\t\tif (intr_src->pid_paddr != 0UL) {\n\t\t\t\tunion dmar_ir_entry irte_pi;\n\n\t\t\t\t/* irte is in remapped mode format, convert to posted mode format */\n\t\t\t\tirte_pi.value.lo_64 = 0UL;\n\t\t\t\tirte_pi.value.hi_64 = 0UL;\n\n\t\t\t\tirte_pi.bits.post.vector = irte->bits.remap.vector;\n\n\t\t\t\tirte_pi.bits.post.svt = 0x1UL;\n\t\t\t\tirte_pi.bits.post.sid = sid.value;\n\t\t\t\tirte_pi.bits.post.present = 0x1UL;\n\t\t\t\tirte_pi.bits.post.mode = 0x1UL;\n\n\t\t\t\tirte_pi.bits.post.pda_l = (intr_src->pid_paddr) >> 6U;\n\t\t\t\tirte_pi.bits.post.pda_h = (intr_src->pid_paddr) >> 32U;\n\n\t\t\t\t*ir_entry = irte_pi;\n\t\t\t} else {\n\t\t\t\t/* Fields that have not been initialized explicitly default to 0 */\n\t\t\t\tirte->bits.remap.svt = 0x1UL;\n\t\t\t\tirte->bits.remap.sid = sid.value;\n\t\t\t\tirte->bits.remap.present = 0x1UL;\n\t\t\t\tirte->bits.remap.trigger_mode = trigger_mode;\n\n\t\t\t\t*ir_entry = *irte;\n\t\t\t}\n\t\t\tiommu_flush_cache(ir_entry, sizeof(union dmar_ir_entry));\n\t\t\tdmar_invalid_iec(dmar_unit, *idx_out, 0U, false);\n\t\t}\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\nvoid dmar_free_irte(const struct intr_source *intr_src, uint16_t index)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tunion dmar_ir_entry *ir_table, *ir_entry;\n\tunion pci_bdf sid;\n\n\tif (intr_src->is_msi) {\n\t\tdmar_unit = device_to_dmaru((uint8_t)intr_src->src.msi.bits.b, intr_src->src.msi.fields.devfun);\n\t} else {\n\t\tdmar_unit = ioapic_to_dmaru(intr_src->src.ioapic_id, &sid);\n\t}\n\n\tif (is_dmar_unit_valid(dmar_unit, sid)) {\n\t\tir_table = (union dmar_ir_entry *)hpa2hva(dmar_unit->ir_table_addr);\n\t\tir_entry = ir_table + index;\n\t\tir_entry->bits.remap.present = 0x0UL;\n\n\t\tiommu_flush_cache(ir_entry, sizeof(union dmar_ir_entry));\n\t\tdmar_invalid_iec(dmar_unit, index, 0U, false);\n\n\t\tif (!is_irte_reserved(dmar_unit, index)) {\n\t\t\tspinlock_obtain(&dmar_unit->lock);\n\t\t\tbitmap_clear_nolock(index & 0x3FU, &dmar_unit->irte_alloc_bitmap[index >> 6U]);\n\t\t\tspinlock_release(&dmar_unit->lock);\n\t\t}\n\t}\n\n}\n"], "fixing_code": ["/*\n * Copyright (C) 2018 Intel Corporation. All rights reserved.\n *\n * SPDX-License-Identifier: BSD-3-Clause\n */\n\n#include <types.h>\n#include <errno.h>\n#include <asm/lib/bits.h>\n#include <asm/guest/vm.h>\n#include <asm/vtd.h>\n#include <ptdev.h>\n#include <asm/per_cpu.h>\n#include <asm/ioapic.h>\n#include <asm/pgtable.h>\n#include <asm/irq.h>\n\n/*\n * Check if the IRQ is single-destination and return the destination vCPU if so.\n *\n * VT-d PI (posted mode) cannot support multicast/broadcast IRQs.\n * If returns NULL, this means it is multicast/broadcast IRQ and\n * we can only handle it in remapped mode.\n * If returns non-NULL, the destination vCPU is returned, which means it is\n * single-destination IRQ and we can handle it in posted mode.\n *\n * @pre (vm != NULL) && (info != NULL)\n */\nstatic struct acrn_vcpu *is_single_destination(struct acrn_vm *vm, const struct msi_info *info)\n{\n\tuint64_t vdmask;\n\tuint16_t vid;\n\tstruct acrn_vcpu *vcpu = NULL;\n\n\tvdmask = vlapic_calc_dest_noshort(vm, false, (uint32_t)(info->addr.bits.dest_field),\n\t\t(bool)(info->addr.bits.dest_mode == MSI_ADDR_DESTMODE_PHYS),\n\t\t(bool)(info->data.bits.delivery_mode == MSI_DATA_DELMODE_LOPRI));\n\n\tvid = ffs64(vdmask);\n\n\t/* Can only post fixed and Lowpri IRQs */\n\tif ((info->data.bits.delivery_mode == MSI_DATA_DELMODE_FIXED)\n\t\t|| (info->data.bits.delivery_mode == MSI_DATA_DELMODE_LOPRI)) {\n\t\t/* Can only post single-destination IRQs */\n\t\tif (vdmask == (1UL << vid)) {\n\t\t\tvcpu = vcpu_from_vid(vm, vid);\n\t\t}\n\t}\n\n\treturn vcpu;\n}\n\nstatic uint32_t calculate_logical_dest_mask(uint64_t pdmask)\n{\n\tuint32_t dest_mask = 0UL;\n\tuint64_t pcpu_mask = pdmask;\n\tuint16_t pcpu_id;\n\n\tpcpu_id = ffs64(pcpu_mask);\n\twhile (pcpu_id < MAX_PCPU_NUM) {\n\t\tbitmap_clear_nolock(pcpu_id, &pcpu_mask);\n\t\tdest_mask |= per_cpu(lapic_ldr, pcpu_id);\n\t\tpcpu_id = ffs64(pcpu_mask);\n\t}\n\treturn dest_mask;\n}\n\n/**\n * @pre entry != NULL\n */\nstatic void ptirq_free_irte(const struct ptirq_remapping_info *entry)\n{\n\tstruct intr_source intr_src;\n\n\tif (entry->intr_type == PTDEV_INTR_MSI) {\n\t\tintr_src.is_msi = true;\n\t\tintr_src.src.msi.value = entry->phys_sid.msi_id.bdf;\n\t} else {\n\t\tintr_src.is_msi = false;\n\t\tintr_src.src.ioapic_id = ioapic_irq_to_ioapic_id(entry->allocated_pirq);\n\t}\n\tdmar_free_irte(&intr_src, entry->irte_idx);\n}\n\n/*\n * pid_paddr = 0: invalid address, indicate that remapped mode shall be used\n *\n * pid_paddr != 0: physical address of posted interrupt descriptor, indicate\n * that posted mode shall be used\n */\nstatic void ptirq_build_physical_msi(struct acrn_vm *vm,\n\tstruct ptirq_remapping_info *entry, uint32_t vector, uint64_t pid_paddr, uint16_t irte_idx)\n{\n\tuint64_t vdmask, pdmask;\n\tuint32_t dest, delmode, dest_mask;\n\tbool phys;\n\tunion dmar_ir_entry irte;\n\tunion irte_index ir_index;\n\tint32_t ret;\n\tstruct intr_source intr_src;\n\n\t/* get physical destination cpu mask */\n\tdest = entry->vmsi.addr.bits.dest_field;\n\tphys = (entry->vmsi.addr.bits.dest_mode == MSI_ADDR_DESTMODE_PHYS);\n\n\tvdmask = vlapic_calc_dest_noshort(vm, false, dest, phys, false);\n\tpdmask = vcpumask2pcpumask(vm, vdmask);\n\n\t/* get physical delivery mode */\n\tdelmode = entry->vmsi.data.bits.delivery_mode;\n\tif ((delmode != MSI_DATA_DELMODE_FIXED) && (delmode != MSI_DATA_DELMODE_LOPRI)) {\n\t\tdelmode = MSI_DATA_DELMODE_LOPRI;\n\t}\n\n\tdest_mask = calculate_logical_dest_mask(pdmask);\n\n\t/* Using phys_irq as index in the corresponding IOMMU */\n\tirte.value.lo_64 = 0UL;\n\tirte.value.hi_64 = 0UL;\n\tirte.bits.remap.vector = vector;\n\tirte.bits.remap.delivery_mode = delmode;\n\tirte.bits.remap.dest_mode = MSI_ADDR_DESTMODE_LOGICAL;\n\tirte.bits.remap.rh = MSI_ADDR_RH;\n\tirte.bits.remap.dest = dest_mask;\n\n\tintr_src.is_msi = true;\n\tintr_src.pid_paddr = pid_paddr;\n\tintr_src.src.msi.value = entry->phys_sid.msi_id.bdf;\n\tif (entry->irte_idx == INVALID_IRTE_ID) {\n\t\tentry->irte_idx = irte_idx;\n\t}\n\tret = dmar_assign_irte(&intr_src, &irte, entry->irte_idx, &ir_index.index);\n\n\tif (ret == 0) {\n\t\tentry->pmsi.data.full = 0U;\n\t\tentry->pmsi.addr.full = 0UL;\n\t\tentry->irte_idx = ir_index.index;\n\t\tif (ir_index.index != INVALID_IRTE_ID) {\n\t\t\t/*\n\t\t\t * Update the MSI interrupt source to point to the IRTE\n\t\t\t * SHV is set to 0 as ACRN disables MMC (Multi-Message Capable\n\t\t\t * for MSI devices.\n\t\t\t */\n\t\t\tentry->pmsi.addr.ir_bits.intr_index_high = ir_index.bits.index_high;\n\t\t\tentry->pmsi.addr.ir_bits.shv = 0U;\n\t\t\tentry->pmsi.addr.ir_bits.intr_format = 0x1U;\n\t\t\tentry->pmsi.addr.ir_bits.intr_index_low = ir_index.bits.index_low;\n\t\t\tentry->pmsi.addr.ir_bits.constant = 0xFEEU;\n\t\t}\n\t} else {\n\t\t/* In case there is no corresponding IOMMU, for example, if the\n\t\t * IOMMU is ignored, pass the MSI info in Compatibility Format\n\t\t */\n\t\tentry->pmsi.data = entry->vmsi.data;\n\t\tentry->pmsi.data.bits.delivery_mode = delmode;\n\t\tentry->pmsi.data.bits.vector = vector;\n\n\t\tentry->pmsi.addr = entry->vmsi.addr;\n\t\tentry->pmsi.addr.bits.dest_field = dest_mask;\n\t\tentry->pmsi.addr.bits.rh = MSI_ADDR_RH;\n\t\tentry->pmsi.addr.bits.dest_mode = MSI_ADDR_DESTMODE_LOGICAL;\n\t}\n\tdev_dbg(DBG_LEVEL_IRQ, \"MSI %s addr:data = 0x%lx:%x(V) -> 0x%lx:%x(P)\",\n\t\t(entry->pmsi.addr.ir_bits.intr_format != 0U) ? \" Remappable Format\" : \"Compatibility Format\",\n\t\tentry->vmsi.addr.full, entry->vmsi.data.full,\n\t\tentry->pmsi.addr.full, entry->pmsi.data.full);\n}\n\nstatic union ioapic_rte\nptirq_build_physical_rte(struct acrn_vm *vm, struct ptirq_remapping_info *entry)\n{\n\tunion ioapic_rte rte;\n\tuint32_t phys_irq = entry->allocated_pirq;\n\tunion source_id *virt_sid = &entry->virt_sid;\n\tunion irte_index ir_index;\n\tunion dmar_ir_entry irte;\n\tstruct intr_source intr_src;\n\tint32_t ret;\n\n\tif (virt_sid->intx_id.ctlr == INTX_CTLR_IOAPIC) {\n\t\tuint64_t vdmask, pdmask;\n\t\tuint32_t dest, delmode, dest_mask, vector;\n\t\tunion ioapic_rte virt_rte;\n\t\tbool phys;\n\n\t\tvioapic_get_rte(vm, virt_sid->intx_id.gsi, &virt_rte);\n\t\trte = virt_rte;\n\n\t\t/* init polarity & pin state */\n\t\tif (rte.bits.intr_polarity == IOAPIC_RTE_INTPOL_ALO) {\n\t\t\tif (entry->polarity == 0U) {\n\t\t\t\tvioapic_set_irqline_nolock(vm, virt_sid->intx_id.gsi, GSI_SET_HIGH);\n\t\t\t}\n\t\t\tentry->polarity = 1U;\n\t\t} else {\n\t\t\tif (entry->polarity == 1U) {\n\t\t\t\tvioapic_set_irqline_nolock(vm, virt_sid->intx_id.gsi, GSI_SET_LOW);\n\t\t\t}\n\t\t\tentry->polarity = 0U;\n\t\t}\n\n\t\t/* physical destination cpu mask */\n\t\tphys = (virt_rte.bits.dest_mode == IOAPIC_RTE_DESTMODE_PHY);\n\t\tdest = (uint32_t)virt_rte.bits.dest_field;\n\t\tvdmask = vlapic_calc_dest_noshort(vm, false, dest, phys, false);\n\t\tpdmask = vcpumask2pcpumask(vm, vdmask);\n\n\t\t/* physical delivery mode */\n\t\tdelmode = virt_rte.bits.delivery_mode;\n\t\tif ((delmode != IOAPIC_RTE_DELMODE_FIXED) &&\n\t\t\t(delmode != IOAPIC_RTE_DELMODE_LOPRI)) {\n\t\t\tdelmode = IOAPIC_RTE_DELMODE_LOPRI;\n\t\t}\n\n\t\t/* update physical delivery mode, dest mode(logical) & vector */\n\t\tvector = irq_to_vector(phys_irq);\n\t\tdest_mask = calculate_logical_dest_mask(pdmask);\n\n\t\tirte.value.lo_64 = 0UL;\n\t\tirte.value.hi_64 = 0UL;\n\t\tirte.bits.remap.vector = vector;\n\t\tirte.bits.remap.delivery_mode = delmode;\n\t\tirte.bits.remap.dest_mode = IOAPIC_RTE_DESTMODE_LOGICAL;\n\t\tirte.bits.remap.dest = dest_mask;\n\t\tirte.bits.remap.trigger_mode = rte.bits.trigger_mode;\n\n\t\tintr_src.is_msi = false;\n\t\tintr_src.pid_paddr = 0UL;\n\t\tintr_src.src.ioapic_id = ioapic_irq_to_ioapic_id(phys_irq);\n\t\tret = dmar_assign_irte(&intr_src, &irte, entry->irte_idx, &ir_index.index);\n\n\t\tif (ret == 0) {\n\t\t\tentry->irte_idx = ir_index.index;\n\t\t\tif (ir_index.index != INVALID_IRTE_ID) {\n\t\t\t\trte.ir_bits.vector = vector;\n\t\t\t\trte.ir_bits.constant = 0U;\n\t\t\t\trte.ir_bits.intr_index_high = ir_index.bits.index_high;\n\t\t\t\trte.ir_bits.intr_format = 1U;\n\t\t\t\trte.ir_bits.intr_index_low = ir_index.bits.index_low;\n\t\t\t} else {\n\t\t\t\trte.bits.intr_mask = 1;\n\t\t\t}\n\t\t} else {\n\t\t\trte.bits.dest_mode = IOAPIC_RTE_DESTMODE_LOGICAL;\n\t\t\trte.bits.delivery_mode = delmode;\n\t\t\trte.bits.vector = vector;\n\t\t\trte.bits.dest_field = dest_mask;\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"IOAPIC RTE %s = 0x%x:%x(V) -> 0x%x:%x(P)\",\n\t\t\t(rte.ir_bits.intr_format != 0U) ? \"Remappable Format\" : \"Compatibility Format\",\n\t\t\tvirt_rte.u.hi_32, virt_rte.u.lo_32,\n\t\t\trte.u.hi_32, rte.u.lo_32);\n\t} else {\n\t\tenum vpic_trigger trigger;\n\t\tunion ioapic_rte phys_rte;\n\n\t\t/* just update trigger mode */\n\t\tioapic_get_rte(phys_irq, &phys_rte);\n\t\trte = phys_rte;\n\t\trte.bits.trigger_mode = IOAPIC_RTE_TRGRMODE_EDGE;\n\t\tvpic_get_irqline_trigger_mode(vm_pic(vm), (uint32_t)virt_sid->intx_id.gsi, &trigger);\n\t\tif (trigger == LEVEL_TRIGGER) {\n\t\t\trte.bits.trigger_mode = IOAPIC_RTE_TRGRMODE_LEVEL;\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"IOAPIC RTE %s = 0x%x:%x(P) -> 0x%x:%x(P)\",\n\t\t\t(rte.ir_bits.intr_format != 0U) ? \"Remappable Format\" : \"Compatibility Format\",\n\t\t\tphys_rte.u.hi_32, phys_rte.u.lo_32,\n\t\t\trte.u.hi_32, rte.u.lo_32);\n\t}\n\n\treturn rte;\n}\n\n/* add msix entry for a vm, based on msi id (phys_bdf+msix_index)\n * - if the entry not be added by any vm, allocate it\n * - if the entry already be added by sos_vm, then change the owner to current vm\n * - if the entry already be added by other vm, return NULL\n */\nstatic struct ptirq_remapping_info *add_msix_remapping(struct acrn_vm *vm,\n\tuint16_t virt_bdf, uint16_t phys_bdf, uint32_t entry_nr)\n{\n\tstruct ptirq_remapping_info *entry;\n\tDEFINE_MSI_SID(phys_sid, phys_bdf, entry_nr);\n\tDEFINE_MSI_SID(virt_sid, virt_bdf, entry_nr);\n\n\tentry = find_ptirq_entry(PTDEV_INTR_MSI, &phys_sid, NULL);\n\tif (entry == NULL) {\n\t\tentry = ptirq_alloc_entry(vm, PTDEV_INTR_MSI);\n\t\tif (entry != NULL) {\n\t\t\tentry->phys_sid.value = phys_sid.value;\n\t\t\tentry->virt_sid.value = virt_sid.value;\n\t\t\tentry->release_cb = ptirq_free_irte;\n\n\t\t\t/* update msi source and active entry */\n\t\t\tif (ptirq_activate_entry(entry, IRQ_INVALID) < 0) {\n\t\t\t\tptirq_release_entry(entry);\n\t\t\t\tentry = NULL;\n\t\t\t}\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"VM%d MSIX add vector mapping vbdf%x:pbdf%x idx=%d\",\n\t\t\tvm->vm_id, virt_bdf, phys_bdf, entry_nr);\n\t}\n\n\treturn entry;\n}\n\n/* deactive & remove mapping entry of vbdf:entry_nr for vm */\nstatic void\nremove_msix_remapping(const struct acrn_vm *vm, uint16_t phys_bdf, uint32_t entry_nr)\n{\n\tstruct ptirq_remapping_info *entry;\n\tDEFINE_MSI_SID(phys_sid, phys_bdf, entry_nr);\n\tstruct intr_source intr_src;\n\n\tentry = find_ptirq_entry(PTDEV_INTR_MSI, &phys_sid, NULL);\n\tif ((entry != NULL) && (entry->vm == vm)) {\n\t\tif (is_entry_active(entry)) {\n\t\t\t/*TODO: disable MSIX device when HV can in future */\n\t\t\tptirq_deactivate_entry(entry);\n\t\t}\n\n\t\tintr_src.is_msi = true;\n\t\tintr_src.src.msi.value = entry->phys_sid.msi_id.bdf;\n\t\tdmar_free_irte(&intr_src, entry->irte_idx);\n\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"VM%d MSIX remove vector mapping vbdf-pbdf:0x%x-0x%x idx=%d\",\n\t\t\tvm->vm_id, entry->virt_sid.msi_id.bdf, phys_bdf, entry_nr);\n\n\t\tptirq_release_entry(entry);\n\t}\n\n}\n\n/* add intx entry for a vm, based on intx id (phys_pin)\n * - if the entry not be added by any vm, allocate it\n * - if the entry already be added by sos_vm, then change the owner to current vm\n * - if the entry already be added by other vm, return NULL\n */\nstatic struct ptirq_remapping_info *add_intx_remapping(struct acrn_vm *vm, uint32_t virt_gsi,\n\t\tuint32_t phys_gsi, enum intx_ctlr vgsi_ctlr)\n{\n\tstruct ptirq_remapping_info *entry = NULL;\n\tDEFINE_INTX_SID(phys_sid, phys_gsi, INTX_CTLR_IOAPIC);\n\tDEFINE_INTX_SID(virt_sid, virt_gsi, vgsi_ctlr);\n\tuint32_t phys_irq = ioapic_gsi_to_irq(phys_gsi);\n\n\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &phys_sid, NULL);\n\tif (entry == NULL) {\n\t\tif (find_ptirq_entry(PTDEV_INTR_INTX, &virt_sid, vm) == NULL) {\n\t\t\tentry = ptirq_alloc_entry(vm, PTDEV_INTR_INTX);\n\t\t\tif (entry != NULL) {\n\t\t\t\tentry->phys_sid.value = phys_sid.value;\n\t\t\t\tentry->virt_sid.value = virt_sid.value;\n\t\t\t\tentry->release_cb = ptirq_free_irte;\n\n\t\t\t\t/* activate entry */\n\t\t\t\tif (ptirq_activate_entry(entry, phys_irq) < 0) {\n\t\t\t\t\tptirq_release_entry(entry);\n\t\t\t\t\tentry = NULL;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tpr_err(\"INTX re-add vpin %d\", virt_gsi);\n\t\t}\n\t} else if (entry->vm != vm) {\n\t\tif (is_sos_vm(entry->vm)) {\n\t\t\tentry->vm = vm;\n\t\t\tentry->virt_sid.value = virt_sid.value;\n\t\t\tentry->polarity = 0U;\n\t\t} else {\n\t\t\tpr_err(\"INTX gsi%d already in vm%d with vgsi%d, not able to add into vm%d with vgsi%d\",\n\t\t\t\t\tphys_gsi, entry->vm->vm_id, entry->virt_sid.intx_id.gsi, vm->vm_id, virt_gsi);\n\t\t\tentry = NULL;\n\t\t}\n\t} else {\n\t\t/* The mapping has already been added to the VM. No action\n\t\t * required.\n\t\t */\n\t}\n\n\n\t/*\n\t * ptirq entry is either created or transferred from SOS VM to Post-launched VM\n\t */\n\n\tif (entry != NULL) {\n\t\tdev_dbg(DBG_LEVEL_IRQ, \"VM%d INTX add pin mapping vgsi%d:pgsi%d\",\n\t\t\tentry->vm->vm_id, virt_gsi, phys_gsi);\n\t}\n\n\treturn entry;\n}\n\n/* deactive & remove mapping entry of vpin for vm */\nstatic void remove_intx_remapping(const struct acrn_vm *vm, uint32_t virt_gsi, enum intx_ctlr vgsi_ctlr)\n{\n\tuint32_t phys_irq;\n\tstruct ptirq_remapping_info *entry;\n\tstruct intr_source intr_src;\n\tDEFINE_INTX_SID(virt_sid, virt_gsi, vgsi_ctlr);\n\n\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &virt_sid, vm);\n\tif (entry != NULL) {\n\t\tif (is_entry_active(entry)) {\n\t\t\tphys_irq = entry->allocated_pirq;\n\t\t\t/* disable interrupt */\n\t\t\tioapic_gsi_mask_irq(phys_irq);\n\n\t\t\tptirq_deactivate_entry(entry);\n\t\t\tintr_src.is_msi = false;\n\t\t\tintr_src.src.ioapic_id = ioapic_irq_to_ioapic_id(phys_irq);\n\n\t\t\tdmar_free_irte(&intr_src, entry->irte_idx);\n\t\t\tdev_dbg(DBG_LEVEL_IRQ,\n\t\t\t\t\"deactive %s intx entry:pgsi=%d, pirq=%d \",\n\t\t\t\t(vgsi_ctlr == INTX_CTLR_PIC) ? \"vPIC\" : \"vIOAPIC\",\n\t\t\t\tentry->phys_sid.intx_id.gsi, phys_irq);\n\t\t\tdev_dbg(DBG_LEVEL_IRQ, \"from vm%d vgsi=%d\\n\",\n\t\t\t\tentry->vm->vm_id, virt_gsi);\n\t\t}\n\n\t\tptirq_release_entry(entry);\n\t}\n}\n\nstatic void ptirq_handle_intx(struct acrn_vm *vm,\n\t\tconst struct ptirq_remapping_info *entry)\n{\n\tconst union source_id *virt_sid = &entry->virt_sid;\n\n\tswitch (virt_sid->intx_id.ctlr) {\n\tcase INTX_CTLR_IOAPIC:\n\t{\n\t\tunion ioapic_rte rte;\n\t\tbool trigger_lvl = false;\n\n\t\t/* INTX_CTLR_IOAPIC means we have vioapic enabled */\n\t\tvioapic_get_rte(vm, (uint32_t)virt_sid->intx_id.gsi, &rte);\n\t\tif (rte.bits.trigger_mode == IOAPIC_RTE_TRGRMODE_LEVEL) {\n\t\t\ttrigger_lvl = true;\n\t\t}\n\n\t\tif (trigger_lvl) {\n\t\t\tif (entry->polarity != 0U) {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_sid->intx_id.gsi, GSI_SET_LOW);\n\t\t\t} else {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_sid->intx_id.gsi, GSI_SET_HIGH);\n\t\t\t}\n\t\t} else {\n\t\t\tif (entry->polarity != 0U) {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_sid->intx_id.gsi, GSI_FALLING_PULSE);\n\t\t\t} else {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_sid->intx_id.gsi, GSI_RAISING_PULSE);\n\t\t\t}\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_PTIRQ,\n\t\t\t\"dev-assign: irq=0x%x assert vr: 0x%x vRTE=0x%lx\",\n\t\t\tentry->allocated_pirq,\n\t\t\tirq_to_vector(entry->allocated_pirq),\n\t\t\trte.full);\n\t\tbreak;\n\t}\n\tcase INTX_CTLR_PIC:\n\t{\n\t\tenum vpic_trigger trigger;\n\n\t\t/* INTX_CTLR_PIC means we have vpic enabled */\n\t\tvpic_get_irqline_trigger_mode(vm_pic(vm), virt_sid->intx_id.gsi, &trigger);\n\t\tif (trigger == LEVEL_TRIGGER) {\n\t\t\tvpic_set_irqline(vm_pic(vm), virt_sid->intx_id.gsi, GSI_SET_HIGH);\n\t\t} else {\n\t\t\tvpic_set_irqline(vm_pic(vm), virt_sid->intx_id.gsi, GSI_RAISING_PULSE);\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\t/*\n\t\t * In this switch statement, virt_sid->intx_id.ctlr shall\n\t\t * either be INTX_CTLR_IOAPIC or INTX_CTLR_PIC.\n\t\t * Gracefully return if prior case clauses have not been met.\n\t\t */\n\t\tbreak;\n\t}\n}\n\nvoid ptirq_softirq(uint16_t pcpu_id)\n{\n\twhile (1) {\n\t\tstruct ptirq_remapping_info *entry = ptirq_dequeue_softirq(pcpu_id);\n\t\tstruct msi_info *vmsi;\n\n\t\tif (entry == NULL) {\n\t\t\tbreak;\n\t\t}\n\n\t\tvmsi = &entry->vmsi;\n\n\t\t/* skip any inactive entry */\n\t\tif (!is_entry_active(entry)) {\n\t\t\t/* service next item */\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* handle real request */\n\t\tif (entry->intr_type == PTDEV_INTR_INTX) {\n\t\t\tptirq_handle_intx(entry->vm, entry);\n\t\t} else {\n\t\t\tif (vmsi != NULL) {\n\t\t\t\t/* TODO: vmsi destmode check required */\n\t\t\t\t(void)vlapic_inject_msi(entry->vm, vmsi->addr.full, vmsi->data.full);\n\t\t\t\tdev_dbg(DBG_LEVEL_PTIRQ, \"dev-assign: irq=0x%x MSI VR: 0x%x-0x%x\",\n\t\t\t\t\tentry->allocated_pirq, vmsi->data.bits.vector,\n\t\t\t\t\tirq_to_vector(entry->allocated_pirq));\n\t\t\t\tdev_dbg(DBG_LEVEL_PTIRQ, \" vmsi_addr: 0x%lx vmsi_data: 0x%x\",\n\t\t\t\t\tvmsi->addr.full, vmsi->data.full);\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid ptirq_intx_ack(struct acrn_vm *vm, uint32_t virt_gsi, enum intx_ctlr vgsi_ctlr)\n{\n\tuint32_t phys_irq;\n\tstruct ptirq_remapping_info *entry;\n\tDEFINE_INTX_SID(virt_sid, virt_gsi, vgsi_ctlr);\n\n\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &virt_sid, vm);\n\tif (entry != NULL) {\n\t\tphys_irq = entry->allocated_pirq;\n\n\t\t/* NOTE: only Level trigger will process EOI/ACK and if we got here\n\t\t * means we have this vioapic or vpic or both enabled\n\t\t */\n\t\tswitch (vgsi_ctlr) {\n\t\tcase INTX_CTLR_IOAPIC:\n\t\t\tif (entry->polarity != 0U) {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_gsi, GSI_SET_HIGH);\n\t\t\t} else {\n\t\t\t\tvioapic_set_irqline_lock(vm, virt_gsi, GSI_SET_LOW);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase INTX_CTLR_PIC:\n\t\t\tvpic_set_irqline(vm_pic(vm), virt_gsi, GSI_SET_LOW);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * In this switch statement, vgsi_ctlr shall either be\n\t\t\t * INTX_CTLR_IOAPIC or INTX_CTLR_PIC.\n\t\t\t * Gracefully return if prior case clauses have not been met.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\tdev_dbg(DBG_LEVEL_PTIRQ, \"dev-assign: irq=0x%x acked vr: 0x%x\",\n\t\t\t\tphys_irq, irq_to_vector(phys_irq));\n\t\tioapic_gsi_unmask_irq(phys_irq);\n\t}\n}\n\n/* Main entry for PCI device assignment with MSI and MSI-X\n * MSI can up to 8 vectors and MSI-X can up to 1024 Vectors\n * We use entry_nr to indicate coming vectors\n * entry_nr = 0 means first vector\n * user must provide bdf and entry_nr\n */\nint32_t ptirq_prepare_msix_remap(struct acrn_vm *vm, uint16_t virt_bdf, uint16_t phys_bdf,\n\t\t\t\tuint16_t entry_nr, struct msi_info *info, uint16_t irte_idx)\n{\n\tstruct ptirq_remapping_info *entry;\n\tint32_t ret = -ENODEV;\n\tunion pci_bdf vbdf;\n\n\t/*\n\t * adds the mapping entries at runtime, if the\n\t * entry already be held by others, return error.\n\t */\n\tspinlock_obtain(&ptdev_lock);\n\tentry = add_msix_remapping(vm, virt_bdf, phys_bdf, entry_nr);\n\tspinlock_release(&ptdev_lock);\n\n\tif (entry != NULL) {\n\t\tret = 0;\n\t\tentry->vmsi = *info;\n\n\t\t/* build physical config MSI, update to info->pmsi_xxx */\n\t\tif (is_lapic_pt_configured(vm)) {\n\t\t\tenum vm_vlapic_mode vlapic_mode = check_vm_vlapic_mode(vm);\n\n\t\t\tif (vlapic_mode == VM_VLAPIC_X2APIC) {\n\t\t\t\t/*\n\t\t\t\t * All the vCPUs are in x2APIC mode and LAPIC is Pass-through\n\t\t\t\t * Use guest vector to program the interrupt source\n\t\t\t\t */\n\t\t\t\tptirq_build_physical_msi(vm, entry, (uint32_t)info->data.bits.vector, 0UL, irte_idx);\n\t\t\t} else if (vlapic_mode == VM_VLAPIC_XAPIC) {\n\t\t\t\t/*\n\t\t\t\t * All the vCPUs are in xAPIC mode and LAPIC is emulated\n\t\t\t\t * Use host vector to program the interrupt source\n\t\t\t\t */\n\t\t\t\tptirq_build_physical_msi(vm, entry, irq_to_vector(entry->allocated_pirq), 0UL, irte_idx);\n\t\t\t} else if (vlapic_mode == VM_VLAPIC_TRANSITION) {\n\t\t\t\t/*\n\t\t\t\t * vCPUs are in middle of transition, so do not program interrupt source\n\t\t\t\t * TODO: Devices programmed during transistion do not work after transition\n\t\t\t\t * as device is not programmed with interrupt info. Need to implement a\n\t\t\t\t * method to get interrupts working after transition.\n\t\t\t\t */\n\t\t\t\tret = -EFAULT;\n\t\t\t} else {\n\t\t\t\t/* Do nothing for VM_VLAPIC_DISABLED */\n\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t} else {\n\t\t\tstruct acrn_vcpu *vcpu = is_single_destination(vm, info);\n\n\t\t\tif (is_pi_capable(vm) && (vcpu != NULL)) {\n\t\t\t\tptirq_build_physical_msi(vm, entry,\n\t\t\t\t\t(uint32_t)info->data.bits.vector, hva2hpa(get_pi_desc(vcpu)), irte_idx);\n\t\t\t} else {\n\t\t\t\t/* Go with remapped mode if we cannot handle it in posted mode */\n\t\t\t\tptirq_build_physical_msi(vm, entry, irq_to_vector(entry->allocated_pirq), 0UL, irte_idx);\n\t\t\t}\n\t\t}\n\n\t\tif (ret == 0) {\n\t\t\t*info = entry->pmsi;\n\t\t\tvbdf.value = virt_bdf;\n\t\t\tdev_dbg(DBG_LEVEL_IRQ, \"PCI %x:%x.%x MSI VR[%d] 0x%x->0x%x assigned to vm%d\",\n\t\t\t\tvbdf.bits.b, vbdf.bits.d, vbdf.bits.f, entry_nr, entry->vmsi.data.bits.vector,\n\t\t\t\tirq_to_vector(entry->allocated_pirq), entry->vm->vm_id);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void activate_physical_ioapic(struct acrn_vm *vm,\n\t\tstruct ptirq_remapping_info *entry)\n{\n\tunion ioapic_rte rte;\n\tuint32_t phys_irq = entry->allocated_pirq;\n\tuint64_t intr_mask;\n\tbool is_lvl_trigger = false;\n\n\t/* disable interrupt */\n\tioapic_gsi_mask_irq(phys_irq);\n\n\t/* build physical IOAPIC RTE */\n\trte = ptirq_build_physical_rte(vm, entry);\n\tintr_mask = rte.bits.intr_mask;\n\n\t/* update irq trigger mode according to info in guest */\n\tif (rte.bits.trigger_mode == IOAPIC_RTE_TRGRMODE_LEVEL) {\n\t\tis_lvl_trigger = true;\n\t}\n\tset_irq_trigger_mode(phys_irq, is_lvl_trigger);\n\n\t/* set rte entry when masked */\n\trte.bits.intr_mask = IOAPIC_RTE_MASK_SET;\n\tioapic_set_rte(phys_irq, rte);\n\n\tif (intr_mask == IOAPIC_RTE_MASK_CLR) {\n\t\tioapic_gsi_unmask_irq(phys_irq);\n\t}\n}\n\n/* Main entry for PCI/Legacy device assignment with INTx, calling from vIOAPIC\n * or vPIC\n */\nint32_t ptirq_intx_pin_remap(struct acrn_vm *vm, uint32_t virt_gsi, enum intx_ctlr vgsi_ctlr)\n{\n\tint32_t status = 0;\n\tstruct ptirq_remapping_info *entry = NULL;\n\tDEFINE_INTX_SID(virt_sid, virt_gsi, vgsi_ctlr);\n\tDEFINE_INTX_SID(alt_virt_sid, virt_gsi, vgsi_ctlr);\n\n\t/*\n\t * virt pin could come from primary vPIC, secondary vPIC or vIOAPIC\n\t * while phys pin is always means for physical IOAPIC.\n\t *\n\t * Device Model should pre-hold the mapping entries by calling\n\t * ptirq_add_intx_remapping for UOS.\n\t *\n\t * For SOS(sos_vm), it adds the mapping entries at runtime, if the\n\t * entry already be held by others, return error.\n\t */\n\n\t/* no remap for vuart intx */\n\tif (!is_vuart_intx(vm, virt_sid.intx_id.gsi)) {\n\t\t/* query if we have virt to phys mapping */\n\t\tspinlock_obtain(&ptdev_lock);\n\t\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &virt_sid, vm);\n\t\tif (entry == NULL) {\n\t\t\tif (is_sos_vm(vm)) {\n\n\t\t\t\t/* for sos_vm, there is chance of vpin source switch\n\t\t\t\t * between vPIC & vIOAPIC for one legacy phys_pin.\n\t\t\t\t *\n\t\t\t\t * here checks if there is already mapping entry from\n\t\t\t\t * the other vpin source for legacy pin. If yes, then\n\t\t\t\t * switch vpin source is needed\n\t\t\t\t */\n\t\t\t\tif (virt_gsi < NR_LEGACY_PIN) {\n\n\t\t\t\t\tif (vgsi_ctlr == INTX_CTLR_PIC) {\n\t\t\t\t\t\talt_virt_sid.intx_id.ctlr = INTX_CTLR_IOAPIC;\n\t\t\t\t\t} else {\n\t\t\t\t\t\talt_virt_sid.intx_id.ctlr = INTX_CTLR_PIC;\n\t\t\t\t\t}\n\n\t\t\t\t\tentry = find_ptirq_entry(PTDEV_INTR_INTX, &alt_virt_sid, vm);\n\t\t\t\t\tif (entry != NULL) {\n\t\t\t\t\t\tuint32_t phys_gsi = virt_gsi;\n\n\t\t\t\t\t\tremove_intx_remapping(vm, alt_virt_sid.intx_id.gsi,\n\t\t\t\t\t\t\talt_virt_sid.intx_id.ctlr);\n\t\t\t\t\t\tentry = add_intx_remapping(vm, virt_gsi, phys_gsi, vgsi_ctlr);\n\t\t\t\t\t\tif (entry == NULL) {\n\t\t\t\t\t\t\tpr_err(\"%s, add intx remapping failed\", __func__);\n\t\t\t\t\t\t\tstatus = -ENODEV;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdev_dbg(DBG_LEVEL_IRQ,\n\t\t\t\t\t\t\t\t\"IOAPIC gsi=%hhu pirq=%u vgsi=%d from %s to %s for vm%d\",\n\t\t\t\t\t\t\t\tentry->phys_sid.intx_id.gsi,\n\t\t\t\t\t\t\t\tentry->allocated_pirq, entry->virt_sid.intx_id.gsi,\n\t\t\t\t\t\t\t\t(vgsi_ctlr == INTX_CTLR_IOAPIC) ? \"vPIC\" : \"vIOAPIC\",\n\t\t\t\t\t\t\t\t(vgsi_ctlr == INTX_CTLR_IOAPIC) ? \"vIOPIC\" : \"vPIC\",\n\t\t\t\t\t\t\t\tentry->vm->vm_id);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t/* entry could be updated by above switch check */\n\t\t\t\tif (entry == NULL) {\n\t\t\t\t\tuint32_t phys_gsi = virt_gsi;\n\n\t\t\t\t\tentry = add_intx_remapping(vm, virt_gsi, phys_gsi, vgsi_ctlr);\n\t\t\t\t\tif (entry == NULL) {\n\t\t\t\t\t\tpr_err(\"%s, add intx remapping failed\",\n\t\t\t\t\t\t\t\t__func__);\n\t\t\t\t\t\tstatus = -ENODEV;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* ptirq_intx_pin_remap is triggered by vPIC/vIOAPIC\n\t\t\t\t * everytime a pin get unmask, here filter out pins\n\t\t\t\t * not get mapped.\n\t\t\t\t */\n\t\t\t\tstatus = -ENODEV;\n\t\t\t}\n\t\t}\n\t\tspinlock_release(&ptdev_lock);\n\t} else {\n\t\tstatus = -EINVAL;\n\t}\n\n\tif (status == 0) {\n\t\tactivate_physical_ioapic(vm, entry);\n\t}\n\n\treturn status;\n}\n\n/* @pre vm != NULL\n * except sos_vm, Device Model should call this function to pre-hold ptdev intx\n * entries:\n * - the entry is identified by phys_pin:\n *   one entry vs. one phys_pin\n * - currently, one phys_pin can only be held by one pin source (vPIC or\n *   vIOAPIC)\n */\nint32_t ptirq_add_intx_remapping(struct acrn_vm *vm, uint32_t virt_gsi, uint32_t phys_gsi, bool pic_pin)\n{\n\tstruct ptirq_remapping_info *entry;\n\tenum intx_ctlr vgsi_ctlr = pic_pin ? INTX_CTLR_PIC : INTX_CTLR_IOAPIC;\n\n\tspinlock_obtain(&ptdev_lock);\n\tentry = add_intx_remapping(vm, virt_gsi, phys_gsi, vgsi_ctlr);\n\tspinlock_release(&ptdev_lock);\n\n\treturn (entry != NULL) ? 0 : -ENODEV;\n}\n\n/*\n * @pre vm != NULL\n */\nvoid ptirq_remove_intx_remapping(const struct acrn_vm *vm, uint32_t virt_gsi, bool pic_pin)\n{\n\tenum intx_ctlr vgsi_ctlr = pic_pin ? INTX_CTLR_PIC : INTX_CTLR_IOAPIC;\n\n\tspinlock_obtain(&ptdev_lock);\n\tremove_intx_remapping(vm, virt_gsi, vgsi_ctlr);\n\tspinlock_release(&ptdev_lock);\n}\n\n/*\n * @pre vm != NULL\n */\nvoid ptirq_remove_msix_remapping(const struct acrn_vm *vm, uint16_t phys_bdf,\n\t\tuint32_t vector_count)\n{\n\tuint32_t i;\n\n\tfor (i = 0U; i < vector_count; i++) {\n\t\tspinlock_obtain(&ptdev_lock);\n\t\tremove_msix_remapping(vm, phys_bdf, i);\n\t\tspinlock_release(&ptdev_lock);\n\t}\n}\n\n/*\n * @pre vm != NULL\n */\nvoid ptirq_remove_configured_intx_remappings(const struct acrn_vm *vm)\n{\n\tconst struct acrn_vm_config *vm_config = get_vm_config(vm->vm_id);\n\tuint32_t i;\n\n\tfor (i = 0; i < vm_config->pt_intx_num; i++) {\n\t\tptirq_remove_intx_remapping(vm, vm_config->pt_intx[i].virt_gsi, false);\n\t}\n}\n", "/*\n * Copyright (C) 2018 Intel Corporation. All rights reserved.\n *\n * SPDX-License-Identifier: BSD-3-Clause\n */\n\n#define pr_prefix\t\t\"iommu: \"\n\n#include <types.h>\n#include <errno.h>\n#include <asm/lib/bits.h>\n#include <asm/lib/spinlock.h>\n#include <asm/cpu_caps.h>\n#include <irq.h>\n#include <asm/irq.h>\n#include <asm/io.h>\n#include <asm/mmu.h>\n#include <asm/lapic.h>\n#include <asm/vtd.h>\n#include <ticks.h>\n#include <logmsg.h>\n#include <asm/board.h>\n#include <asm/vm_config.h>\n#include <pci.h>\n#include <asm/platform_caps.h>\n\n#define DBG_IOMMU 0\n\n#if DBG_IOMMU\n#define DBG_LEVEL_IOMMU LOG_INFO\n#define DMAR_FAULT_LOOP_MAX 10\n#else\n#define DBG_LEVEL_IOMMU 6U\n#endif\n#define LEVEL_WIDTH 9U\n\n#define ROOT_ENTRY_LOWER_PRESENT_POS        (0U)\n#define ROOT_ENTRY_LOWER_PRESENT_MASK       (1UL << ROOT_ENTRY_LOWER_PRESENT_POS)\n#define ROOT_ENTRY_LOWER_CTP_POS            (12U)\n#define ROOT_ENTRY_LOWER_CTP_MASK           (0xFFFFFFFFFFFFFUL << ROOT_ENTRY_LOWER_CTP_POS)\n\n#define CONFIG_MAX_IOMMU_NUM\t\tDRHD_COUNT\n\n/* 4 iommu fault register state */\n#define\tIOMMU_FAULT_REGISTER_STATE_NUM\t4U\n#define\tIOMMU_FAULT_REGISTER_SIZE\t4U\n\n#define CTX_ENTRY_UPPER_AW_POS          (0U)\n#define CTX_ENTRY_UPPER_AW_MASK         (0x7UL << CTX_ENTRY_UPPER_AW_POS)\n#define CTX_ENTRY_UPPER_DID_POS         (8U)\n#define CTX_ENTRY_UPPER_DID_MASK        (0xFFFFUL << CTX_ENTRY_UPPER_DID_POS)\n#define CTX_ENTRY_LOWER_P_POS           (0U)\n#define CTX_ENTRY_LOWER_P_MASK          (0x1UL << CTX_ENTRY_LOWER_P_POS)\n#define CTX_ENTRY_LOWER_FPD_POS         (1U)\n#define CTX_ENTRY_LOWER_FPD_MASK        (0x1UL << CTX_ENTRY_LOWER_FPD_POS)\n#define CTX_ENTRY_LOWER_TT_POS          (2U)\n#define CTX_ENTRY_LOWER_TT_MASK         (0x3UL << CTX_ENTRY_LOWER_TT_POS)\n#define CTX_ENTRY_LOWER_SLPTPTR_POS     (12U)\n#define CTX_ENTRY_LOWER_SLPTPTR_MASK    (0xFFFFFFFFFFFFFUL <<  CTX_ENTRY_LOWER_SLPTPTR_POS)\n\nstatic inline uint64_t dmar_get_bitslice(uint64_t var, uint64_t mask, uint32_t pos)\n{\n\treturn ((var & mask) >> pos);\n}\n\nstatic inline uint64_t dmar_set_bitslice(uint64_t var, uint64_t mask, uint32_t pos, uint64_t val)\n{\n\treturn ((var & ~mask) | ((val << pos) & mask));\n}\n\n/* translation type */\n#define DMAR_CTX_TT_UNTRANSLATED    0x0UL\n#define DMAR_CTX_TT_ALL             0x1UL\n#define DMAR_CTX_TT_PASSTHROUGH     0x2UL\n\n/* Fault event MSI data register */\n#define DMAR_MSI_DELIVERY_MODE_SHIFT     (8U)\n#define DMAR_MSI_DELIVERY_FIXED          (0U << DMAR_MSI_DELIVERY_MODE_SHIFT)\n#define DMAR_MSI_DELIVERY_LOWPRI         (1U << DMAR_MSI_DELIVERY_MODE_SHIFT)\n\n/* Fault event MSI address register */\n#define DMAR_MSI_DEST_MODE_SHIFT         (2U)\n#define DMAR_MSI_DEST_MODE_PHYS          (0U << DMAR_MSI_DEST_MODE_SHIFT)\n#define DMAR_MSI_DEST_MODE_LOGIC         (1U << DMAR_MSI_DEST_MODE_SHIFT)\n#define DMAR_MSI_REDIRECTION_SHIFT       (3U)\n#define DMAR_MSI_REDIRECTION_CPU         (0U << DMAR_MSI_REDIRECTION_SHIFT)\n#define DMAR_MSI_REDIRECTION_LOWPRI      (1U << DMAR_MSI_REDIRECTION_SHIFT)\n\n#define DMAR_INVALIDATION_QUEUE_SIZE\t4096U\n#define DMAR_QI_INV_ENTRY_SIZE\t\t16U\n#define DMAR_NUM_IR_ENTRIES_PER_PAGE\t256U\n\n#define DMAR_INV_STATUS_WRITE_SHIFT\t5U\n#define DMAR_INV_CONTEXT_CACHE_DESC\t0x01UL\n#define DMAR_INV_IOTLB_DESC\t\t0x02UL\n#define DMAR_INV_IEC_DESC\t\t0x04UL\n#define DMAR_INV_WAIT_DESC\t\t0x05UL\n#define DMAR_INV_STATUS_WRITE\t\t(1UL << DMAR_INV_STATUS_WRITE_SHIFT)\n#define DMAR_INV_STATUS_INCOMPLETE\t0UL\n#define DMAR_INV_STATUS_COMPLETED\t1UL\n#define DMAR_INV_STATUS_DATA_SHIFT\t32U\n#define DMAR_INV_STATUS_DATA\t\t(DMAR_INV_STATUS_COMPLETED << DMAR_INV_STATUS_DATA_SHIFT)\n#define DMAR_INV_WAIT_DESC_LOWER\t(DMAR_INV_STATUS_WRITE | DMAR_INV_WAIT_DESC | DMAR_INV_STATUS_DATA)\n\n#define DMAR_IR_ENABLE_EIM_SHIFT\t11UL\n#define DMAR_IR_ENABLE_EIM\t\t(1UL << DMAR_IR_ENABLE_EIM_SHIFT)\n\nenum dmar_cirg_type {\n\tDMAR_CIRG_RESERVED = 0,\n\tDMAR_CIRG_GLOBAL,\n\tDMAR_CIRG_DOMAIN,\n\tDMAR_CIRG_DEVICE\n};\n\nenum dmar_iirg_type {\n\tDMAR_IIRG_RESERVED = 0,\n\tDMAR_IIRG_GLOBAL,\n\tDMAR_IIRG_DOMAIN,\n\tDMAR_IIRG_PAGE\n};\n\n/* dmar unit runtime data */\nstruct dmar_drhd_rt {\n\tuint32_t index;\n\tspinlock_t lock;\n\n\tstruct dmar_drhd *drhd;\n\n\tuint64_t root_table_addr;\n\tuint64_t ir_table_addr;\n\tuint64_t irte_alloc_bitmap[CONFIG_MAX_IR_ENTRIES / 64U];\n\tuint64_t irte_reserved_bitmap[CONFIG_MAX_IR_ENTRIES / 64U];\n\tuint64_t qi_queue;\n\tuint16_t qi_tail;\n\n\tuint64_t cap;\n\tuint64_t ecap;\n\tuint32_t gcmd;  /* sw cache value of global cmd register */\n\n\tuint32_t dmar_irq;\n\n\tbool cap_pw_coherency;  /* page-walk coherency */\n\tuint8_t cap_msagaw;\n\tuint16_t cap_num_fault_regs;\n\tuint16_t cap_fault_reg_offset;\n\tuint16_t ecap_iotlb_offset;\n\tuint32_t fault_state[IOMMU_FAULT_REGISTER_STATE_NUM]; /* 32bit registers */\n};\n\nstruct context_table {\n\tstruct page buses[CONFIG_IOMMU_BUS_NUM];\n};\n\nstruct intr_remap_table {\n\tstruct page tables[CONFIG_MAX_IR_ENTRIES/DMAR_NUM_IR_ENTRIES_PER_PAGE];\n};\n\nstatic inline uint8_t *get_root_table(uint32_t dmar_index)\n{\n\tstatic struct page root_tables[CONFIG_MAX_IOMMU_NUM] __aligned(PAGE_SIZE);\n\treturn root_tables[dmar_index].contents;\n}\n\nstatic inline uint8_t *get_ctx_table(uint32_t dmar_index, uint8_t bus_no)\n{\n\tstatic struct context_table ctx_tables[CONFIG_MAX_IOMMU_NUM] __aligned(PAGE_SIZE);\n\treturn ctx_tables[dmar_index].buses[bus_no].contents;\n}\n\n/*\n * @pre dmar_index < CONFIG_MAX_IOMMU_NUM\n */\nstatic inline void *get_qi_queue(uint32_t dmar_index)\n{\n\tstatic struct page qi_queues[CONFIG_MAX_IOMMU_NUM] __aligned(PAGE_SIZE);\n\treturn (void *)qi_queues[dmar_index].contents;\n}\n\nstatic inline void *get_ir_table(uint32_t dmar_index)\n{\n\tstatic struct intr_remap_table ir_tables[CONFIG_MAX_IOMMU_NUM] __aligned(PAGE_SIZE);\n\treturn (void *)ir_tables[dmar_index].tables[0].contents;\n}\n\nstatic struct dmar_drhd_rt dmar_drhd_units[MAX_DRHDS];\nstatic bool iommu_page_walk_coherent = true;\nstatic struct dmar_info *platform_dmar_info = NULL;\n\n/* Domain id 0 is reserved in some cases per VT-d */\n#define MAX_DOMAIN_NUM (CONFIG_MAX_VM_NUM + 1)\n\nstatic inline uint16_t vmid_to_domainid(uint16_t vm_id)\n{\n\treturn vm_id + 1U;\n}\n\nstatic int32_t dmar_register_hrhd(struct dmar_drhd_rt *dmar_unit);\nstatic struct dmar_drhd_rt *device_to_dmaru(uint8_t bus, uint8_t devfun);\n\nstatic int32_t register_hrhd_units(void)\n{\n\tstruct dmar_drhd_rt *drhd_rt;\n\tuint32_t i;\n\tint32_t ret = 0;\n\n\tfor (i = 0U; i < platform_dmar_info->drhd_count; i++) {\n\t\tdrhd_rt = &dmar_drhd_units[i];\n\t\tdrhd_rt->index = i;\n\t\tdrhd_rt->drhd = &platform_dmar_info->drhd_units[i];\n\t\tdrhd_rt->dmar_irq = IRQ_INVALID;\n\n\t\tset_paging_supervisor(drhd_rt->drhd->reg_base_addr, PAGE_SIZE);\n\n\t\tret = dmar_register_hrhd(drhd_rt);\n\t\tif (ret != 0) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((iommu_cap_pi(drhd_rt->cap) == 0U) || (!is_apicv_advanced_feature_supported())) {\n\t\t\tplatform_caps.pi = false;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic uint32_t iommu_read32(const struct dmar_drhd_rt *dmar_unit, uint32_t offset)\n{\n\treturn mmio_read32(hpa2hva(dmar_unit->drhd->reg_base_addr + offset));\n}\n\nstatic uint64_t iommu_read64(const struct dmar_drhd_rt *dmar_unit, uint32_t offset)\n{\n\treturn mmio_read64(hpa2hva(dmar_unit->drhd->reg_base_addr + offset));\n}\n\nstatic void iommu_write32(const struct dmar_drhd_rt *dmar_unit, uint32_t offset, uint32_t value)\n{\n\tmmio_write32(value, hpa2hva(dmar_unit->drhd->reg_base_addr + offset));\n}\n\nstatic void iommu_write64(const struct dmar_drhd_rt *dmar_unit, uint32_t offset, uint64_t value)\n{\n\tmmio_write64(value, hpa2hva(dmar_unit->drhd->reg_base_addr + offset));\n}\n\nstatic inline void dmar_wait_completion(const struct dmar_drhd_rt *dmar_unit, uint32_t offset,\n\tuint32_t mask, uint32_t pre_condition, uint32_t *status)\n{\n\t/* variable start isn't used when built as release version */\n\t__unused uint64_t start = cpu_ticks();\n\n\tdo {\n\t\t*status = iommu_read32(dmar_unit, offset);\n\t\tASSERT(((cpu_ticks() - start) < TICKS_PER_MS),\n\t\t\t\"DMAR OP Timeout!\");\n\t\tasm_pause();\n\t} while( (*status & mask) == pre_condition);\n}\n\n/* Flush CPU cache when root table, context table or second-level translation teable updated\n * In the context of ACRN, GPA to HPA mapping relationship is not changed after VM created,\n * skip flushing iotlb to avoid performance penalty.\n */\nvoid iommu_flush_cache(const void *p, uint32_t size)\n{\n\t/* if vtd support page-walk coherency, no need to flush cacheline */\n\tif (!iommu_page_walk_coherent) {\n\t\tflush_cache_range(p, size);\n\t}\n}\n\n#if DBG_IOMMU\nstatic inline uint8_t iommu_cap_rwbf(uint64_t cap)\n{\n\treturn ((uint8_t)(cap >> 4U) & 1U);\n}\n\nstatic inline uint8_t iommu_ecap_sc(uint64_t ecap)\n{\n\treturn ((uint8_t)(ecap >> 7U) & 1U);\n}\n\nstatic void dmar_unit_show_capability(struct dmar_drhd_rt *dmar_unit)\n{\n\tpr_info(\"dmar unit[0x%x]\", dmar_unit->drhd->reg_base_addr);\n\tpr_info(\"\\tNumDomain:%d\", iommu_cap_ndoms(dmar_unit->cap));\n\tpr_info(\"\\tAdvancedFaultLogging:%d\", iommu_cap_afl(dmar_unit->cap));\n\tpr_info(\"\\tRequiredWBFlush:%d\", iommu_cap_rwbf(dmar_unit->cap));\n\tpr_info(\"\\tProtectedLowMemRegion:%d\", iommu_cap_plmr(dmar_unit->cap));\n\tpr_info(\"\\tProtectedHighMemRegion:%d\", iommu_cap_phmr(dmar_unit->cap));\n\tpr_info(\"\\tCachingMode:%d\", iommu_cap_caching_mode(dmar_unit->cap));\n\tpr_info(\"\\tSAGAW:0x%x\", iommu_cap_sagaw(dmar_unit->cap));\n\tpr_info(\"\\tMGAW:%d\", iommu_cap_mgaw(dmar_unit->cap));\n\tpr_info(\"\\tZeroLenRead:%d\", iommu_cap_zlr(dmar_unit->cap));\n\tpr_info(\"\\tLargePageSupport:0x%x\", iommu_cap_super_page_val(dmar_unit->cap));\n\tpr_info(\"\\tPageSelectiveInvalidation:%d\", iommu_cap_pgsel_inv(dmar_unit->cap));\n\tpr_info(\"\\tPageSelectInvalidation:%d\", iommu_cap_pgsel_inv(dmar_unit->cap));\n\tpr_info(\"\\tNumOfFaultRecordingReg:%d\", iommu_cap_num_fault_regs(dmar_unit->cap));\n\tpr_info(\"\\tMAMV:0x%x\", iommu_cap_max_amask_val(dmar_unit->cap));\n\tpr_info(\"\\tWriteDraining:%d\", iommu_cap_write_drain(dmar_unit->cap));\n\tpr_info(\"\\tReadDraining:%d\", iommu_cap_read_drain(dmar_unit->cap));\n\tpr_info(\"\\tPostInterrupts:%d\\n\", iommu_cap_pi(dmar_unit->cap));\n\tpr_info(\"\\tPage-walk Coherency:%d\", iommu_ecap_c(dmar_unit->ecap));\n\tpr_info(\"\\tQueuedInvalidation:%d\", iommu_ecap_qi(dmar_unit->ecap));\n\tpr_info(\"\\tDeviceTLB:%d\", iommu_ecap_dt(dmar_unit->ecap));\n\tpr_info(\"\\tInterruptRemapping:%d\", iommu_ecap_ir(dmar_unit->ecap));\n\tpr_info(\"\\tExtendedInterruptMode:%d\", iommu_ecap_eim(dmar_unit->ecap));\n\tpr_info(\"\\tPassThrough:%d\", iommu_ecap_pt(dmar_unit->ecap));\n\tpr_info(\"\\tSnoopControl:%d\", iommu_ecap_sc(dmar_unit->ecap));\n\tpr_info(\"\\tIOTLB RegOffset:0x%x\", iommu_ecap_iro(dmar_unit->ecap));\n\tpr_info(\"\\tMHMV:0x%x\", iommu_ecap_mhmv(dmar_unit->ecap));\n\tpr_info(\"\\tECS:%d\", iommu_ecap_ecs(dmar_unit->ecap));\n\tpr_info(\"\\tMTS:%d\", iommu_ecap_mts(dmar_unit->ecap));\n\tpr_info(\"\\tNEST:%d\", iommu_ecap_nest(dmar_unit->ecap));\n\tpr_info(\"\\tDIS:%d\", iommu_ecap_dis(dmar_unit->ecap));\n\tpr_info(\"\\tPRS:%d\", iommu_ecap_prs(dmar_unit->ecap));\n\tpr_info(\"\\tERS:%d\", iommu_ecap_ers(dmar_unit->ecap));\n\tpr_info(\"\\tSRS:%d\", iommu_ecap_srs(dmar_unit->ecap));\n\tpr_info(\"\\tNWFS:%d\", iommu_ecap_nwfs(dmar_unit->ecap));\n\tpr_info(\"\\tEAFS:%d\", iommu_ecap_eafs(dmar_unit->ecap));\n\tpr_info(\"\\tPSS:0x%x\", iommu_ecap_pss(dmar_unit->ecap));\n\tpr_info(\"\\tPASID:%d\", iommu_ecap_pasid(dmar_unit->ecap));\n\tpr_info(\"\\tDIT:%d\", iommu_ecap_dit(dmar_unit->ecap));\n\tpr_info(\"\\tPDS:%d\\n\", iommu_ecap_pds(dmar_unit->ecap));\n}\n#endif\n\nstatic inline uint8_t width_to_level(uint32_t width)\n{\n\treturn (uint8_t)(((width - 12U) + (LEVEL_WIDTH)-1U) / (LEVEL_WIDTH));\n}\n\nstatic inline uint8_t width_to_agaw(uint32_t width)\n{\n\treturn width_to_level(width) - 2U;\n}\n\nstatic uint8_t dmar_unit_get_msagw(const struct dmar_drhd_rt *dmar_unit)\n{\n\tuint8_t i;\n\tuint8_t sgaw = iommu_cap_sagaw(dmar_unit->cap);\n\n\tfor (i = 5U; i > 0U; ) {\n\t\ti--;\n\t\tif (((1U << i) & sgaw) != 0U) {\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn i;\n}\n\nstatic bool dmar_unit_support_aw(const struct dmar_drhd_rt *dmar_unit, uint32_t addr_width)\n{\n\tuint8_t aw;\n\n\taw = width_to_agaw(addr_width);\n\n\treturn (((1U << aw) & iommu_cap_sagaw(dmar_unit->cap)) != 0U);\n}\n\nstatic void dmar_enable_intr_remapping(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif ((dmar_unit->gcmd & DMA_GCMD_IRE) == 0U) {\n\t\tdmar_unit->gcmd |= DMA_GCMD_IRE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd);\n\t\t/* 32-bit register */\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_IRES, 0U, &status);\n#if DBG_IOMMU\n\t\tstatus = iommu_read32(dmar_unit, DMAR_GSTS_REG);\n#endif\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: gsr:0x%x\", __func__, status);\n}\n\nstatic void dmar_enable_translation(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif ((dmar_unit->gcmd & DMA_GCMD_TE) == 0U) {\n\t\tdmar_unit->gcmd |= DMA_GCMD_TE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd);\n\t\t/* 32-bit register */\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_TES, 0U, &status);\n#if DBG_IOMMU\n\t\tstatus = iommu_read32(dmar_unit, DMAR_GSTS_REG);\n#endif\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n\n\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: gsr:0x%x\", __func__, status);\n}\n\nstatic void dmar_disable_intr_remapping(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif ((dmar_unit->gcmd & DMA_GCMD_IRE) != 0U) {\n\t\tdmar_unit->gcmd &= ~DMA_GCMD_IRE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd);\n\t\t/* 32-bit register */\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_IRES, DMA_GSTS_IRES, &status);\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_disable_translation(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif ((dmar_unit->gcmd & DMA_GCMD_TE) != 0U) {\n\t\tdmar_unit->gcmd &= ~DMA_GCMD_TE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd);\n\t\t/* 32-bit register */\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_TES, DMA_GSTS_TES, &status);\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic int32_t dmar_register_hrhd(struct dmar_drhd_rt *dmar_unit)\n{\n\tint32_t ret = 0;\n\n\tdev_dbg(DBG_LEVEL_IOMMU, \"Register dmar uint [%d] @0x%lx\", dmar_unit->index, dmar_unit->drhd->reg_base_addr);\n\n\tspinlock_init(&dmar_unit->lock);\n\n\tdmar_unit->cap = iommu_read64(dmar_unit, DMAR_CAP_REG);\n\tdmar_unit->ecap = iommu_read64(dmar_unit, DMAR_ECAP_REG);\n\n\t/*\n\t * The initialization of \"dmar_unit->gcmd\" shall be done via reading from Global Status Register rather than\n\t * Global Command Register.\n\t * According to Chapter 10.4.4 Global Command Register in VT-d spec, Global Command Register is a write-only\n\t * register to control remapping hardware. Global Status Register is the corresponding read-only register to\n\t * report remapping hardware status.\n\t */\n\tdmar_unit->gcmd = iommu_read32(dmar_unit, DMAR_GSTS_REG);\n\n\tdmar_unit->cap_msagaw = dmar_unit_get_msagw(dmar_unit);\n\n\tdmar_unit->cap_num_fault_regs = iommu_cap_num_fault_regs(dmar_unit->cap);\n\tdmar_unit->cap_fault_reg_offset = iommu_cap_fault_reg_offset(dmar_unit->cap);\n\tdmar_unit->ecap_iotlb_offset = iommu_ecap_iro(dmar_unit->ecap) * 16U;\n\tdmar_unit->root_table_addr = hva2hpa(get_root_table(dmar_unit->index));\n\tdmar_unit->ir_table_addr = hva2hpa(get_ir_table(dmar_unit->index));\n\n#if DBG_IOMMU\n\tpr_info(\"version:0x%x, cap:0x%lx, ecap:0x%lx\",\n\t\tiommu_read32(dmar_unit, DMAR_VER_REG), dmar_unit->cap, dmar_unit->ecap);\n\tpr_info(\"sagaw:0x%x, msagaw:0x%x, iotlb offset 0x%x\",\n\t\tiommu_cap_sagaw(dmar_unit->cap), dmar_unit->cap_msagaw, dmar_unit->ecap_iotlb_offset);\n\n\tdmar_unit_show_capability(dmar_unit);\n#endif\n\n\t/* check capability */\n\tif ((iommu_cap_super_page_val(dmar_unit->cap) & 0x1U) == 0U) {\n\t\tpr_fatal(\"%s: dmar uint doesn't support 2MB page!\\n\", __func__);\n\t\tret = -ENODEV;\n\t} else if ((iommu_cap_super_page_val(dmar_unit->cap) & 0x2U) == 0U) {\n\t\tpr_fatal(\"%s: dmar uint doesn't support 1GB page!\\n\", __func__);\n\t\tret = -ENODEV;\n\t} else if (iommu_ecap_qi(dmar_unit->ecap) == 0U) {\n\t\tpr_fatal(\"%s: dmar unit doesn't support Queued Invalidation!\", __func__);\n\t\tret = -ENODEV;\n\t} else if (iommu_ecap_ir(dmar_unit->ecap) == 0U) {\n\t\tpr_fatal(\"%s: dmar unit doesn't support Interrupt Remapping!\", __func__);\n\t\tret = -ENODEV;\n\t} else if (iommu_ecap_eim(dmar_unit->ecap) == 0U) {\n\t\tpr_fatal(\"%s: dmar unit doesn't support Extended Interrupt Mode!\", __func__);\n\t\tret = -ENODEV;\n\t} else {\n\t\tif ((iommu_ecap_c(dmar_unit->ecap) == 0U) && (!dmar_unit->drhd->ignore)) {\n\t\t\tiommu_page_walk_coherent = false;\n\t\t}\n\t\tdmar_disable_translation(dmar_unit);\n\t}\n\n\treturn ret;\n}\n\nstatic struct dmar_drhd_rt *ioapic_to_dmaru(uint16_t ioapic_id, union pci_bdf *sid)\n{\n\tstruct dmar_drhd_rt *dmar_unit = NULL;\n\tuint32_t i, j;\n\tbool found = false;\n\n\tfor (j = 0U; j < platform_dmar_info->drhd_count; j++) {\n\t\tdmar_unit = &dmar_drhd_units[j];\n\t\tfor (i = 0U; i < dmar_unit->drhd->dev_cnt; i++) {\n\t\t\tif ((dmar_unit->drhd->devices[i].type == ACPI_DMAR_SCOPE_TYPE_IOAPIC) &&\n\t\t\t\t\t(dmar_unit->drhd->devices[i].id == ioapic_id)) {\n\t\t\t\tsid->fields.devfun = dmar_unit->drhd->devices[i].devfun;\n\t\t\t\tsid->fields.bus = dmar_unit->drhd->devices[i].bus;\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (found) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (j == platform_dmar_info->drhd_count) {\n\t\tdmar_unit = NULL;\n\t}\n\n\treturn dmar_unit;\n}\n\nstatic struct dmar_drhd_rt *device_to_dmaru(uint8_t bus, uint8_t devfun)\n{\n\tstruct dmar_drhd_rt *dmaru = NULL;\n\tuint16_t bdf = ((uint16_t)bus << 8U) | devfun;\n\tuint32_t index = pci_lookup_drhd_for_pbdf(bdf);\n\n\tif (index == INVALID_DRHD_INDEX) {\n\t\tpr_fatal(\"BDF %02x:%02x:%x has no IOMMU\\n\", bus, devfun >> 3U, devfun & 7U);\n\t\t/*\n\t\t * pci_lookup_drhd_for_pbdf would return -1U for any of the reasons\n\t\t * 1) PCI device with bus, devfun does not exist on platform\n\t\t * 2) ACRN had issues finding the device with bus, devfun during init\n\t\t * 3) DMAR tables provided by ACPI for this platform are incorrect\n\t\t */\n\t} else {\n\t\tdmaru = &dmar_drhd_units[index];\n\t}\n\n\treturn dmaru;\n}\n\nstatic void dmar_issue_qi_request(struct dmar_drhd_rt *dmar_unit, struct dmar_entry invalidate_desc)\n{\n\tstruct dmar_entry *invalidate_desc_ptr;\n\tuint32_t qi_status = 0U;\n\tuint64_t start;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\n\tinvalidate_desc_ptr = (struct dmar_entry *)(dmar_unit->qi_queue + dmar_unit->qi_tail);\n\n\tinvalidate_desc_ptr->hi_64 = invalidate_desc.hi_64;\n\tinvalidate_desc_ptr->lo_64 = invalidate_desc.lo_64;\n\tdmar_unit->qi_tail = (dmar_unit->qi_tail + DMAR_QI_INV_ENTRY_SIZE) % DMAR_INVALIDATION_QUEUE_SIZE;\n\n\tinvalidate_desc_ptr++;\n\n\tinvalidate_desc_ptr->hi_64 = hva2hpa(&qi_status);\n\tinvalidate_desc_ptr->lo_64 = DMAR_INV_WAIT_DESC_LOWER;\n\tdmar_unit->qi_tail = (dmar_unit->qi_tail + DMAR_QI_INV_ENTRY_SIZE) % DMAR_INVALIDATION_QUEUE_SIZE;\n\n\tqi_status = DMAR_INV_STATUS_INCOMPLETE;\n\tiommu_write32(dmar_unit, DMAR_IQT_REG, dmar_unit->qi_tail);\n\n\tstart = cpu_ticks();\n\twhile (qi_status != DMAR_INV_STATUS_COMPLETED) {\n\t\tif ((cpu_ticks() - start) > TICKS_PER_MS) {\n\t\t\tpr_err(\"DMAR OP Timeout! @ %s\", __func__);\n\t\t\tbreak;\n\t\t}\n\t\tasm_pause();\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\n/*\n * did: domain id\n * sid: source id\n * fm: function mask\n * cirg: cache-invalidation request granularity\n */\nstatic void dmar_invalid_context_cache(struct dmar_drhd_rt *dmar_unit,\n\tuint16_t did, uint16_t sid, uint8_t fm, enum dmar_cirg_type cirg)\n{\n\tstruct dmar_entry invalidate_desc;\n\n\tinvalidate_desc.hi_64 = 0UL;\n\tinvalidate_desc.lo_64 = DMAR_INV_CONTEXT_CACHE_DESC;\n\tswitch (cirg) {\n\tcase DMAR_CIRG_GLOBAL:\n\t\tinvalidate_desc.lo_64 |= DMA_CONTEXT_GLOBAL_INVL;\n\t\tbreak;\n\tcase DMAR_CIRG_DOMAIN:\n\t\tinvalidate_desc.lo_64 |= DMA_CONTEXT_DOMAIN_INVL | dma_ccmd_did(did);\n\t\tbreak;\n\tcase DMAR_CIRG_DEVICE:\n\t\tinvalidate_desc.lo_64 |= DMA_CONTEXT_DEVICE_INVL | dma_ccmd_did(did) | dma_ccmd_sid(sid) | dma_ccmd_fm(fm);\n\t\tbreak;\n\tdefault:\n\t\tinvalidate_desc.lo_64 = 0UL;\n\t\tpr_err(\"unknown CIRG type\");\n\t\tbreak;\n\t}\n\n\tif (invalidate_desc.lo_64 != 0UL) {\n\t\tdmar_issue_qi_request(dmar_unit, invalidate_desc);\n\t}\n}\n\nstatic void dmar_invalid_context_cache_global(struct dmar_drhd_rt *dmar_unit)\n{\n\tdmar_invalid_context_cache(dmar_unit, 0U, 0U, 0U, DMAR_CIRG_GLOBAL);\n}\n\nstatic void dmar_invalid_iotlb(struct dmar_drhd_rt *dmar_unit, uint16_t did, uint64_t address, uint8_t am,\n\t\t\t       bool hint, enum dmar_iirg_type iirg)\n{\n\t/* set Drain Reads & Drain Writes,\n\t * if hardware doesn't support it, will be ignored by hardware\n\t */\n\tstruct dmar_entry invalidate_desc;\n\tuint64_t addr = 0UL;\n\n\tinvalidate_desc.hi_64 = 0UL;\n\n\tinvalidate_desc.lo_64 = DMA_IOTLB_DR | DMA_IOTLB_DW | DMAR_INV_IOTLB_DESC;\n\n\tswitch (iirg) {\n\tcase DMAR_IIRG_GLOBAL:\n\t\tinvalidate_desc.lo_64 |= DMA_IOTLB_GLOBAL_INVL;\n\t\tbreak;\n\tcase DMAR_IIRG_DOMAIN:\n\t\tinvalidate_desc.lo_64 |= DMA_IOTLB_DOMAIN_INVL | dma_iotlb_did(did);\n\t\tbreak;\n\tcase DMAR_IIRG_PAGE:\n\t\tinvalidate_desc.lo_64 |= DMA_IOTLB_PAGE_INVL | dma_iotlb_did(did);\n\t\taddr = address | dma_iotlb_invl_addr_am(am);\n\t\tif (hint) {\n\t\t\taddr |= DMA_IOTLB_INVL_ADDR_IH_UNMODIFIED;\n\t\t}\n\t\tinvalidate_desc.hi_64 |= addr;\n\t\tbreak;\n\tdefault:\n\t\tinvalidate_desc.lo_64 = 0UL;\n\t\tpr_err(\"unknown IIRG type\");\n\t}\n\n\tif (invalidate_desc.lo_64 != 0UL) {\n\t\tdmar_issue_qi_request(dmar_unit, invalidate_desc);\n\t}\n}\n\n/* Invalidate IOTLB globally,\n * all iotlb entries are invalidated,\n * all PASID-cache entries are invalidated,\n * all paging-structure-cache entries are invalidated.\n */\nstatic void dmar_invalid_iotlb_global(struct dmar_drhd_rt *dmar_unit)\n{\n\tdmar_invalid_iotlb(dmar_unit, 0U, 0UL, 0U, false, DMAR_IIRG_GLOBAL);\n}\n\n/* @pre dmar_unit->ir_table_addr != NULL */\nstatic void dmar_set_intr_remap_table(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint64_t address;\n\tuint32_t status;\n\tuint8_t size;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\n\t/* Set number of bits needed to represent the entries minus 1 */\n\tsize = (uint8_t) fls32(CONFIG_MAX_IR_ENTRIES) - 1U;\n\taddress = dmar_unit->ir_table_addr | DMAR_IR_ENABLE_EIM | size;\n\n\tiommu_write64(dmar_unit, DMAR_IRTA_REG, address);\n\n\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd | DMA_GCMD_SIRTP);\n\n\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_IRTPS, 0U, &status);\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_invalid_iec(struct dmar_drhd_rt *dmar_unit, uint16_t intr_index,\n\t\t\t\tuint8_t index_mask, bool is_global)\n{\n\tstruct dmar_entry invalidate_desc;\n\n\tinvalidate_desc.hi_64 = 0UL;\n\tinvalidate_desc.lo_64 = DMAR_INV_IEC_DESC;\n\n\tif (is_global) {\n\t\tinvalidate_desc.lo_64 |= DMAR_IEC_GLOBAL_INVL;\n\t} else {\n\t\tinvalidate_desc.lo_64 |= DMAR_IECI_INDEXED | dma_iec_index(intr_index, index_mask);\n\t}\n\n\tif (invalidate_desc.lo_64 != 0UL) {\n\t\tdmar_issue_qi_request(dmar_unit, invalidate_desc);\n\t}\n}\n\nstatic void dmar_invalid_iec_global(struct dmar_drhd_rt *dmar_unit)\n{\n\tdmar_invalid_iec(dmar_unit, 0U, 0U, true);\n}\n\n/* @pre dmar_unit->root_table_addr != NULL */\nstatic void dmar_set_root_table(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tiommu_write64(dmar_unit, DMAR_RTADDR_REG, dmar_unit->root_table_addr);\n\n\tiommu_write32(dmar_unit, DMAR_GCMD_REG, dmar_unit->gcmd | DMA_GCMD_SRTP);\n\n\t/* 32-bit register */\n\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_RTPS, 0U, &status);\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_fault_event_mask(struct dmar_drhd_rt *dmar_unit)\n{\n\tspinlock_obtain(&(dmar_unit->lock));\n\tiommu_write32(dmar_unit, DMAR_FECTL_REG, DMA_FECTL_IM);\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_fault_event_unmask(struct dmar_drhd_rt *dmar_unit)\n{\n\tspinlock_obtain(&(dmar_unit->lock));\n\tiommu_write32(dmar_unit, DMAR_FECTL_REG, 0U);\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_fault_msi_write(struct dmar_drhd_rt *dmar_unit,\n\t\t\tuint32_t vector)\n{\n\tuint32_t data;\n\tuint32_t addr_low;\n\tuint32_t lapic_id = get_cur_lapic_id();\n\n\tdata = DMAR_MSI_DELIVERY_LOWPRI | vector;\n\t/* redirection hint: 0\n\t * destination mode: 0\n\t */\n\taddr_low = 0xFEE00000U | ((uint32_t)(lapic_id) << 12U);\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tiommu_write32(dmar_unit, DMAR_FEDATA_REG, data);\n\tiommu_write32(dmar_unit, DMAR_FEADDR_REG, addr_low);\n\tspinlock_release(&(dmar_unit->lock));\n}\n\n#if DBG_IOMMU\nstatic void fault_status_analysis(uint32_t status)\n{\n\tif (dma_fsts_pfo(status)) {\n\t\tpr_info(\"Primary Fault Overflow\");\n\t}\n\n\tif (dma_fsts_ppf(status)) {\n\t\tpr_info(\"Primary Pending Fault\");\n\t}\n\n\tif (dma_fsts_afo(status)) {\n\t\tpr_info(\"Advanced Fault Overflow\");\n\t}\n\n\tif (dma_fsts_apf(status)) {\n\t\tpr_info(\"Advanced Pending Fault\");\n\t}\n\n\tif (dma_fsts_iqe(status)) {\n\t\tpr_info(\"Invalidation Queue Error\");\n\t}\n\n\tif (dma_fsts_ice(status)) {\n\t\tpr_info(\"Invalidation Completion Error\");\n\t}\n\n\tif (dma_fsts_ite(status)) {\n\t\tpr_info(\"Invalidation Time-out Error\");\n\t}\n\n\tif (dma_fsts_pro(status)) {\n\t\tpr_info(\"Page Request Overflow\");\n\t}\n}\n#endif\n\nstatic void fault_record_analysis(__unused uint64_t low, uint64_t high)\n{\n\tunion pci_bdf dmar_bdf;\n\n\tif (!dma_frcd_up_f(high)) {\n\t\tdmar_bdf.value = dma_frcd_up_sid(high);\n\t\t/* currently skip PASID related parsing */\n\t\tpr_info(\"%s, Reason: 0x%x, SID: %x.%x.%x @0x%lx\",\n\t\t\t(dma_frcd_up_t(high) != 0U) ? \"Read/Atomic\" : \"Write\", dma_frcd_up_fr(high),\n\t\t\tdmar_bdf.bits.b, dmar_bdf.bits.d, dmar_bdf.bits.f, low);\n#if DBG_IOMMU\n\t\tif (iommu_ecap_dt(dmar_unit->ecap) != 0U) {\n\t\t\tpr_info(\"Address Type: 0x%x\", dma_frcd_up_at(high));\n\t\t}\n#endif\n\t}\n}\n\nstatic void dmar_fault_handler(uint32_t irq, void *data)\n{\n\tstruct dmar_drhd_rt *dmar_unit = (struct dmar_drhd_rt *)data;\n\tuint32_t fsr;\n\tuint32_t index;\n\tuint32_t record_reg_offset;\n\tstruct dmar_entry fault_record;\n\tint32_t loop = 0;\n\n\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: irq = %d\", __func__, irq);\n\n\tfsr = iommu_read32(dmar_unit, DMAR_FSTS_REG);\n\n#if DBG_IOMMU\n\tfault_status_analysis(fsr);\n#endif\n\n\twhile (dma_fsts_ppf(fsr)) {\n\t\tloop++;\n\t\tindex = dma_fsts_fri(fsr);\n\t\trecord_reg_offset = (uint32_t)dmar_unit->cap_fault_reg_offset + (index * 16U);\n\t\tif (index >= dmar_unit->cap_num_fault_regs) {\n\t\t\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: invalid FR Index\", __func__);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* read 128-bit fault recording register */\n\t\tfault_record.lo_64 = iommu_read64(dmar_unit, record_reg_offset);\n\t\tfault_record.hi_64 = iommu_read64(dmar_unit, record_reg_offset + 8U);\n\n\t\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: record[%d] @0x%x:  0x%lx, 0x%lx\",\n\t\t\t__func__, index, record_reg_offset, fault_record.lo_64, fault_record.hi_64);\n\n\t\tfault_record_analysis(fault_record.lo_64, fault_record.hi_64);\n\n\t\t/* write to clear */\n\t\tiommu_write64(dmar_unit, record_reg_offset, fault_record.lo_64);\n\t\tiommu_write64(dmar_unit, record_reg_offset + 8U, fault_record.hi_64);\n\n#ifdef DMAR_FAULT_LOOP_MAX\n\t\tif (loop > DMAR_FAULT_LOOP_MAX) {\n\t\t\tdev_dbg(DBG_LEVEL_IOMMU, \"%s: loop more than %d times\", __func__, DMAR_FAULT_LOOP_MAX);\n\t\t\tbreak;\n\t\t}\n#endif\n\n\t\tfsr = iommu_read32(dmar_unit, DMAR_FSTS_REG);\n\t}\n}\n\nstatic void dmar_setup_interrupt(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t vector;\n\tint32_t retval = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\tif (dmar_unit->dmar_irq == IRQ_INVALID) {\n\t\tretval = request_irq(IRQ_INVALID, dmar_fault_handler, dmar_unit, IRQF_NONE);\n\t\tdmar_unit->dmar_irq = (uint32_t)retval;\n\t}\n\tspinlock_release(&(dmar_unit->lock));\n\t/* the panic will only happen before any VM starts running */\n\tif (retval < 0) {\n\t\tpanic(\"dmar[%d] fail to setup interrupt\", dmar_unit->index);\n\t}\n\n\tvector = irq_to_vector(dmar_unit->dmar_irq);\n\tdev_dbg(DBG_LEVEL_IOMMU, \"irq#%d vector#%d for dmar_unit\", dmar_unit->dmar_irq, vector);\n\n\tdmar_fault_msi_write(dmar_unit, vector);\n\tdmar_fault_event_unmask(dmar_unit);\n}\n\nstatic void dmar_enable_qi(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\n\tdmar_unit->qi_queue = hva2hpa(get_qi_queue(dmar_unit->index));\n\tiommu_write64(dmar_unit, DMAR_IQA_REG, dmar_unit->qi_queue);\n\n\tiommu_write32(dmar_unit, DMAR_IQT_REG, 0U);\n\n\tif ((dmar_unit->gcmd & DMA_GCMD_QIE) == 0U) {\n\t\tdmar_unit->gcmd |= DMA_GCMD_QIE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG,\tdmar_unit->gcmd);\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_QIES, 0U, &status);\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void dmar_disable_qi(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t status = 0;\n\n\tspinlock_obtain(&(dmar_unit->lock));\n\n\tif ((dmar_unit->gcmd & DMA_GCMD_QIE) == DMA_GCMD_QIE) {\n\t\tdmar_unit->gcmd &= ~DMA_GCMD_QIE;\n\t\tiommu_write32(dmar_unit, DMAR_GCMD_REG,\tdmar_unit->gcmd);\n\t\tdmar_wait_completion(dmar_unit, DMAR_GSTS_REG, DMA_GSTS_QIES, DMA_GSTS_QIES, &status);\n\t}\n\n\tspinlock_release(&(dmar_unit->lock));\n}\n\nstatic void prepare_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tdev_dbg(DBG_LEVEL_IOMMU, \"enable dmar uint [0x%x]\", dmar_unit->drhd->reg_base_addr);\n\tdmar_setup_interrupt(dmar_unit);\n\tdmar_set_root_table(dmar_unit);\n\tdmar_enable_qi(dmar_unit);\n\tdmar_set_intr_remap_table(dmar_unit);\n}\n\nstatic void enable_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tdev_dbg(DBG_LEVEL_IOMMU, \"enable dmar uint [0x%x]\", dmar_unit->drhd->reg_base_addr);\n\tdmar_invalid_context_cache_global(dmar_unit);\n\tdmar_invalid_iotlb_global(dmar_unit);\n\tdmar_invalid_iec_global(dmar_unit);\n\tdmar_enable_translation(dmar_unit);\n}\n\nstatic void disable_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tdmar_disable_qi(dmar_unit);\n\tdmar_disable_translation(dmar_unit);\n\tdmar_fault_event_mask(dmar_unit);\n\tdmar_disable_intr_remapping(dmar_unit);\n}\n\nstatic void suspend_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t i;\n\n\tdmar_invalid_context_cache_global(dmar_unit);\n\tdmar_invalid_iotlb_global(dmar_unit);\n\tdmar_invalid_iec_global(dmar_unit);\n\n\tdisable_dmar(dmar_unit);\n\n\t/* save IOMMU fault register state */\n\tfor (i = 0U; i < IOMMU_FAULT_REGISTER_STATE_NUM; i++) {\n\t\tdmar_unit->fault_state[i] =  iommu_read32(dmar_unit, DMAR_FECTL_REG + (i * IOMMU_FAULT_REGISTER_SIZE));\n\t}\n}\n\nstatic void resume_dmar(struct dmar_drhd_rt *dmar_unit)\n{\n\tuint32_t i;\n\n\t/* restore IOMMU fault register state */\n\tfor (i = 0U; i < IOMMU_FAULT_REGISTER_STATE_NUM; i++) {\n\t\tiommu_write32(dmar_unit, DMAR_FECTL_REG + (i * IOMMU_FAULT_REGISTER_SIZE), dmar_unit->fault_state[i]);\n\t}\n\tprepare_dmar(dmar_unit);\n\tenable_dmar(dmar_unit);\n\tdmar_enable_intr_remapping(dmar_unit);\n}\n\nstatic inline bool is_dmar_unit_ignored(const struct dmar_drhd_rt *dmar_unit)\n{\n\tbool ignored = false;\n\n\tif ((dmar_unit != NULL) && (dmar_unit->drhd->ignore)) {\n\t\tignored = true;\n\t}\n\n\treturn ignored;\n}\n\nstatic bool is_dmar_unit_valid(const struct dmar_drhd_rt *dmar_unit, union pci_bdf sid)\n{\n\tbool valid = false;\n\n\tif (dmar_unit == NULL) {\n\t\tpr_err(\"no dmar unit found for device: %x:%x.%x\", sid.bits.b, sid.bits.d, sid.bits.f);\n\t} else if (dmar_unit->drhd->ignore) {\n\t\tdev_dbg(DBG_LEVEL_IOMMU, \"device is ignored : %x:%x.%x\", sid.bits.b, sid.bits.d, sid.bits.f);\n\t} else {\n\t\tvalid = true;\n\t}\n\n\treturn valid;\n}\n\n/* @pre bus < CONFIG_IOMMU_BUS_NUM */\nstatic int32_t iommu_attach_device(const struct iommu_domain *domain, uint8_t bus, uint8_t devfun)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tstruct dmar_entry *root_table;\n\tuint64_t context_table_addr;\n\tstruct dmar_entry *context;\n\tstruct dmar_entry *root_entry;\n\tstruct dmar_entry *context_entry;\n\tuint64_t hi_64 = 0UL;\n\tuint64_t lo_64 = 0UL;\n\tint32_t ret = -EINVAL;\n\t/* source id */\n\tunion pci_bdf sid;\n\n\tsid.fields.bus = bus;\n\tsid.fields.devfun = devfun;\n\n\tdmar_unit = device_to_dmaru(bus, devfun);\n\tif (is_dmar_unit_valid(dmar_unit, sid) && dmar_unit_support_aw(dmar_unit, domain->addr_width)) {\n\t\troot_table = (struct dmar_entry *)hpa2hva(dmar_unit->root_table_addr);\n\t\troot_entry = root_table + bus;\n\n\t\tif (dmar_get_bitslice(root_entry->lo_64,\n\t\t\t\t\tROOT_ENTRY_LOWER_PRESENT_MASK,\n\t\t\t\t\tROOT_ENTRY_LOWER_PRESENT_POS) == 0UL) {\n\t\t\t/* create context table for the bus if not present */\n\t\t\tcontext_table_addr = hva2hpa(get_ctx_table(dmar_unit->index, bus));\n\n\t\t\tcontext_table_addr = context_table_addr >> PAGE_SHIFT;\n\n\t\t\tlo_64 = dmar_set_bitslice(lo_64,\n\t\t\t\t\tROOT_ENTRY_LOWER_CTP_MASK, ROOT_ENTRY_LOWER_CTP_POS, context_table_addr);\n\t\t\tlo_64 = dmar_set_bitslice(lo_64,\n\t\t\t\t\tROOT_ENTRY_LOWER_PRESENT_MASK, ROOT_ENTRY_LOWER_PRESENT_POS, 1UL);\n\n\t\t\troot_entry->hi_64 = 0UL;\n\t\t\troot_entry->lo_64 = lo_64;\n\t\t\tiommu_flush_cache(root_entry, sizeof(struct dmar_entry));\n\t\t} else {\n\t\t\tcontext_table_addr = dmar_get_bitslice(root_entry->lo_64,\n\t\t\t\t\tROOT_ENTRY_LOWER_CTP_MASK, ROOT_ENTRY_LOWER_CTP_POS);\n\t\t}\n\n\t\tcontext_table_addr = context_table_addr << PAGE_SHIFT;\n\n\t\tcontext = (struct dmar_entry *)hpa2hva(context_table_addr);\n\t\tcontext_entry = context + devfun;\n\n\t\tif (dmar_get_bitslice(context_entry->lo_64, CTX_ENTRY_LOWER_P_MASK, CTX_ENTRY_LOWER_P_POS) != 0UL) {\n\t\t\t/* the context entry should not be present */\n\t\t\tpr_err(\"%s: context entry@0x%lx (Lower:%x) \", __func__, context_entry, context_entry->lo_64);\n\t\t\tpr_err(\"already present for %x:%x.%x\", bus, sid.bits.d, sid.bits.f);\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\t/* setup context entry for the devfun */\n\t\t\t/* TODO: add Device TLB support */\n\t\t\thi_64 = dmar_set_bitslice(hi_64, CTX_ENTRY_UPPER_AW_MASK, CTX_ENTRY_UPPER_AW_POS,\n\t\t\t\t\t(uint64_t)width_to_agaw(domain->addr_width));\n\t\t\tlo_64 = dmar_set_bitslice(lo_64, CTX_ENTRY_LOWER_TT_MASK, CTX_ENTRY_LOWER_TT_POS,\n\t\t\t\t\tDMAR_CTX_TT_UNTRANSLATED);\n\t\t\thi_64 = dmar_set_bitslice(hi_64, CTX_ENTRY_UPPER_DID_MASK, CTX_ENTRY_UPPER_DID_POS,\n\t\t\t\t(uint64_t)vmid_to_domainid(domain->vm_id));\n\t\t\tlo_64 = dmar_set_bitslice(lo_64, CTX_ENTRY_LOWER_SLPTPTR_MASK, CTX_ENTRY_LOWER_SLPTPTR_POS,\n\t\t\t\tdomain->trans_table_ptr >> PAGE_SHIFT);\n\t\t\tlo_64 = dmar_set_bitslice(lo_64, CTX_ENTRY_LOWER_P_MASK, CTX_ENTRY_LOWER_P_POS, 1UL);\n\n\t\t\tcontext_entry->hi_64 = hi_64;\n\t\t\tcontext_entry->lo_64 = lo_64;\n\t\t\tiommu_flush_cache(context_entry, sizeof(struct dmar_entry));\n\t\t\tret = 0;\n\t\t}\n\t} else if (is_dmar_unit_ignored(dmar_unit)) {\n\t       ret = 0;\n\t}\n\n\treturn ret;\n}\n\n/* @pre bus < CONFIG_IOMMU_BUS_NUM */\nstatic int32_t iommu_detach_device(const struct iommu_domain *domain, uint8_t bus, uint8_t devfun)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tstruct dmar_entry *root_table;\n\tuint64_t context_table_addr;\n\tstruct dmar_entry *context;\n\tstruct dmar_entry *root_entry;\n\tstruct dmar_entry *context_entry;\n\t/* source id */\n\tunion pci_bdf sid;\n\tint32_t ret = -EINVAL;\n\n\tdmar_unit = device_to_dmaru(bus, devfun);\n\n\tsid.fields.bus = bus;\n\tsid.fields.devfun = devfun;\n\n\tif (is_dmar_unit_valid(dmar_unit, sid)) {\n\t\troot_table = (struct dmar_entry *)hpa2hva(dmar_unit->root_table_addr);\n\t\troot_entry = root_table + bus;\n\t\tret = 0;\n\n\t\tcontext_table_addr = dmar_get_bitslice(root_entry->lo_64,  ROOT_ENTRY_LOWER_CTP_MASK,\n\t\t\t\t\t\t\tROOT_ENTRY_LOWER_CTP_POS);\n\t\tcontext_table_addr = context_table_addr << PAGE_SHIFT;\n\t\tcontext = (struct dmar_entry *)hpa2hva(context_table_addr);\n\n\t\tcontext_entry = context + devfun;\n\n\t\tif ((context == NULL) || (context_entry == NULL)) {\n\t\t\tpr_err(\"dmar context entry is invalid\");\n\t\t\tret = -EINVAL;\n\t\t} else if ((uint16_t)dmar_get_bitslice(context_entry->hi_64, CTX_ENTRY_UPPER_DID_MASK,\n\t\t\t\t\t\tCTX_ENTRY_UPPER_DID_POS) != vmid_to_domainid(domain->vm_id)) {\n\t\t\tpr_err(\"%s: domain id mismatch\", __func__);\n\t\t\tret = -EPERM;\n\t\t} else {\n\t\t\t/* clear the present bit first */\n\t\t\tcontext_entry->lo_64 = 0UL;\n\t\t\tcontext_entry->hi_64 = 0UL;\n\t\t\tiommu_flush_cache(context_entry, sizeof(struct dmar_entry));\n\n\t\t\tdmar_invalid_context_cache(dmar_unit, vmid_to_domainid(domain->vm_id), sid.value, 0U,\n\t\t\t\t\t\t\tDMAR_CIRG_DEVICE);\n\t\t\tdmar_invalid_iotlb(dmar_unit, vmid_to_domainid(domain->vm_id), 0UL, 0U, false,\n\t\t\t\t\t\t\tDMAR_IIRG_DOMAIN);\n\t\t}\n\t} else if (is_dmar_unit_ignored(dmar_unit)) {\n\t       ret = 0;\n\t}\n\n\treturn ret;\n}\n\n/*\n * @pre action != NULL\n * As an internal API, VT-d code can guarantee action is not NULL.\n */\nstatic void do_action_for_iommus(void (*action)(struct dmar_drhd_rt *))\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tuint32_t i;\n\n\tfor (i = 0U; i < platform_dmar_info->drhd_count; i++) {\n\t\tdmar_unit = &dmar_drhd_units[i];\n\t\tif (!dmar_unit->drhd->ignore) {\n\t\t\taction(dmar_unit);\n\t\t} else {\n\t\t\tdev_dbg(DBG_LEVEL_IOMMU, \"ignore dmar_unit @0x%x\", dmar_unit->drhd->reg_base_addr);\n\t\t}\n\t}\n}\n\nstruct iommu_domain *create_iommu_domain(uint16_t vm_id, uint64_t translation_table, uint32_t addr_width)\n{\n\tstatic struct iommu_domain iommu_domains[MAX_DOMAIN_NUM];\n\tstruct iommu_domain *domain;\n\n\t/* TODO: check if a domain with the vm_id exists */\n\n\tif (translation_table == 0UL) {\n\t\tpr_err(\"translation table is NULL\");\n\t\tdomain = NULL;\n\t} else {\n\t\t/*\n\t\t * A hypercall is called to create an iommu domain for a valid VM,\n\t\t * and hv code limit the VM number to CONFIG_MAX_VM_NUM.\n\t\t * So the array iommu_domains will not be accessed out of range.\n\t\t */\n\t\tdomain = &iommu_domains[vmid_to_domainid(vm_id)];\n\n\t\tdomain->vm_id = vm_id;\n\t\tdomain->trans_table_ptr = translation_table;\n\t\tdomain->addr_width = addr_width;\n\n\t\tdev_dbg(DBG_LEVEL_IOMMU, \"create domain [%d]: vm_id = %hu, ept@0x%x\",\n\t\t\tvmid_to_domainid(domain->vm_id), domain->vm_id, domain->trans_table_ptr);\n\t}\n\n\treturn domain;\n}\n\n/**\n * @pre domain != NULL\n */\nvoid destroy_iommu_domain(struct iommu_domain *domain)\n{\n\t/* TODO: check if any device assigned to this domain */\n\t(void)memset(domain, 0U, sizeof(*domain));\n}\n\n/*\n * @pre (from_domain != NULL) || (to_domain != NULL)\n */\n\nint32_t move_pt_device(const struct iommu_domain *from_domain, const struct iommu_domain *to_domain, uint8_t bus, uint8_t devfun)\n{\n\tint32_t status = 0;\n\tuint16_t bus_local = bus;\n\n\t/* TODO: check if the device assigned */\n\n\tif (bus_local < CONFIG_IOMMU_BUS_NUM) {\n\t\tif (from_domain != NULL) {\n\t\t\tstatus = iommu_detach_device(from_domain, bus, devfun);\n\t\t}\n\n\t\tif ((status == 0) && (to_domain != NULL)) {\n\t\t\tstatus = iommu_attach_device(to_domain, bus, devfun);\n\t\t}\n\t} else {\n\t\tstatus = -EINVAL;\n\t}\n\n\treturn status;\n}\n\nvoid enable_iommu(void)\n{\n\tdo_action_for_iommus(enable_dmar);\n}\n\nvoid suspend_iommu(void)\n{\n\tdo_action_for_iommus(suspend_dmar);\n}\n\nvoid resume_iommu(void)\n{\n\tdo_action_for_iommus(resume_dmar);\n}\n\n/**\n * @post return != NULL\n * @post return->drhd_count > 0U\n */\nstatic struct dmar_info *get_dmar_info(void)\n{\n#ifdef CONFIG_ACPI_PARSE_ENABLED\n\tparse_dmar_table(&plat_dmar_info);\n#endif\n\treturn &plat_dmar_info;\n}\n\nint32_t init_iommu(void)\n{\n\tint32_t ret = 0;\n\n\tplatform_dmar_info = get_dmar_info();\n\n\tif ((platform_dmar_info == NULL) || (platform_dmar_info->drhd_count == 0U)) {\n\t\tpr_fatal(\"%s: can't find dmar info\\n\", __func__);\n\t\tret = -ENODEV;\n\t} else if (platform_dmar_info->drhd_count > CONFIG_MAX_IOMMU_NUM) {\n\t\tpr_fatal(\"%s: dmar count(%d) beyond the limitation(%d)\\n\",\n\t\t\t\t__func__, platform_dmar_info->drhd_count, CONFIG_MAX_IOMMU_NUM);\n\t\tret = -EINVAL;\n\t} else {\n\t\tret = register_hrhd_units();\n\t\tif (ret == 0) {\n\t\t\tdo_action_for_iommus(prepare_dmar);\n\t\t}\n\t}\n\treturn ret;\n}\n\n/* Allocate continuous IRTEs specified by num, num can be 1, 2, 4, 8, 16, 32 */\nstatic uint16_t alloc_irtes(struct dmar_drhd_rt *dmar_unit, const uint16_t num)\n{\n\tuint16_t irte_idx;\n\tuint64_t mask = (1UL << num) - 1U;\n\tuint64_t test_mask;\n\n\tASSERT((bitmap_weight(num) == 1U) && (num <= 32U));\n\n\tspinlock_obtain(&dmar_unit->lock);\n\tfor (irte_idx = 0U; irte_idx < CONFIG_MAX_IR_ENTRIES; irte_idx += num) {\n\t\ttest_mask = mask << (irte_idx & 0x3FU);\n\t\tif ((dmar_unit->irte_alloc_bitmap[irte_idx >> 6U] & test_mask) == 0UL) {\n\t\t\tdmar_unit->irte_alloc_bitmap[irte_idx >> 6U] |= test_mask;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspinlock_release(&dmar_unit->lock);\n\n\treturn (irte_idx < CONFIG_MAX_IR_ENTRIES) ? irte_idx: INVALID_IRTE_ID;\n}\n\nstatic bool is_irte_reserved(const struct dmar_drhd_rt *dmar_unit, uint16_t index)\n{\n\treturn ((dmar_unit->irte_reserved_bitmap[index >> 6U] & (1UL << (index & 0x3FU))) != 0UL);\n}\n\nint32_t dmar_reserve_irte(const struct intr_source *intr_src, uint16_t num, uint16_t *start_id)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tunion pci_bdf sid;\n\tuint64_t mask = (1UL << num) - 1U;\n\tint32_t ret = -EINVAL;\n\n\tif (intr_src->is_msi) {\n\t\tdmar_unit = device_to_dmaru((uint8_t)intr_src->src.msi.bits.b, intr_src->src.msi.fields.devfun);\n\t\tsid.value = (uint16_t)(intr_src->src.msi.value);\n\t} else {\n\t\tdmar_unit = ioapic_to_dmaru(intr_src->src.ioapic_id, &sid);\n\t}\n\n\tif (is_dmar_unit_valid(dmar_unit, sid)) {\n\t\t*start_id = alloc_irtes(dmar_unit, num);\n\t\tif (*start_id < CONFIG_MAX_IR_ENTRIES) {\n\t\t\tdmar_unit->irte_reserved_bitmap[*start_id >> 6U] |= mask << (*start_id & 0x3FU);\n\t\t}\n\t\tret = 0;\n\t}\n\n\tpr_dbg(\"%s: for dev 0x%x:%x.%x, reserve %u entry for MSI(%d), start from %d\",\n\t\t__func__, sid.bits.b, sid.bits.d, sid.bits.f, num, intr_src->is_msi, *start_id);\n\treturn ret;\n}\n\nint32_t dmar_assign_irte(const struct intr_source *intr_src, union dmar_ir_entry *irte,\n\tuint16_t idx_in, uint16_t *idx_out)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tunion dmar_ir_entry *ir_table, *ir_entry;\n\tunion pci_bdf sid;\n\tuint64_t trigger_mode;\n\tint32_t ret = -EINVAL;\n\n\tif (intr_src->is_msi) {\n\t\tdmar_unit = device_to_dmaru((uint8_t)intr_src->src.msi.bits.b, intr_src->src.msi.fields.devfun);\n\t\tsid.value = (uint16_t)(intr_src->src.msi.value);\n\t\ttrigger_mode = 0x0UL;\n\t} else {\n\t\tdmar_unit = ioapic_to_dmaru(intr_src->src.ioapic_id, &sid);\n\t\ttrigger_mode = irte->bits.remap.trigger_mode;\n\t}\n\n\tif (is_dmar_unit_valid(dmar_unit, sid)) {\n\t\tdmar_enable_intr_remapping(dmar_unit);\n\n\t\tir_table = (union dmar_ir_entry *)hpa2hva(dmar_unit->ir_table_addr);\n\t\t*idx_out = idx_in;\n\t\tif (idx_in == INVALID_IRTE_ID) {\n\t\t\t*idx_out = alloc_irtes(dmar_unit, 1U);\n\t\t}\n\t\tif (*idx_out < CONFIG_MAX_IR_ENTRIES) {\n\t\t\tir_entry = ir_table + *idx_out;\n\n\t\t\tif (intr_src->pid_paddr != 0UL) {\n\t\t\t\tunion dmar_ir_entry irte_pi;\n\n\t\t\t\t/* irte is in remapped mode format, convert to posted mode format */\n\t\t\t\tirte_pi.value.lo_64 = 0UL;\n\t\t\t\tirte_pi.value.hi_64 = 0UL;\n\n\t\t\t\tirte_pi.bits.post.vector = irte->bits.remap.vector;\n\n\t\t\t\tirte_pi.bits.post.svt = 0x1UL;\n\t\t\t\tirte_pi.bits.post.sid = sid.value;\n\t\t\t\tirte_pi.bits.post.present = 0x1UL;\n\t\t\t\tirte_pi.bits.post.mode = 0x1UL;\n\n\t\t\t\tirte_pi.bits.post.pda_l = (intr_src->pid_paddr) >> 6U;\n\t\t\t\tirte_pi.bits.post.pda_h = (intr_src->pid_paddr) >> 32U;\n\n\t\t\t\t*ir_entry = irte_pi;\n\t\t\t} else {\n\t\t\t\t/* Fields that have not been initialized explicitly default to 0 */\n\t\t\t\tirte->bits.remap.svt = 0x1UL;\n\t\t\t\tirte->bits.remap.sid = sid.value;\n\t\t\t\tirte->bits.remap.present = 0x1UL;\n\t\t\t\tirte->bits.remap.trigger_mode = trigger_mode;\n\n\t\t\t\t*ir_entry = *irte;\n\t\t\t}\n\t\t\tiommu_flush_cache(ir_entry, sizeof(union dmar_ir_entry));\n\t\t\tdmar_invalid_iec(dmar_unit, *idx_out, 0U, false);\n\t\t}\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\nvoid dmar_free_irte(const struct intr_source *intr_src, uint16_t index)\n{\n\tstruct dmar_drhd_rt *dmar_unit;\n\tunion dmar_ir_entry *ir_table, *ir_entry;\n\tunion pci_bdf sid;\n\n\tif (intr_src->is_msi) {\n\t\tdmar_unit = device_to_dmaru((uint8_t)intr_src->src.msi.bits.b, intr_src->src.msi.fields.devfun);\n\t} else {\n\t\tdmar_unit = ioapic_to_dmaru(intr_src->src.ioapic_id, &sid);\n\t}\n\n\tif (is_dmar_unit_valid(dmar_unit, sid) && (index < CONFIG_MAX_IR_ENTRIES)) {\n\t\tir_table = (union dmar_ir_entry *)hpa2hva(dmar_unit->ir_table_addr);\n\t\tir_entry = ir_table + index;\n\t\tir_entry->bits.remap.present = 0x0UL;\n\n\t\tiommu_flush_cache(ir_entry, sizeof(union dmar_ir_entry));\n\t\tdmar_invalid_iec(dmar_unit, index, 0U, false);\n\n\t\tif (!is_irte_reserved(dmar_unit, index)) {\n\t\t\tspinlock_obtain(&dmar_unit->lock);\n\t\t\tbitmap_clear_nolock(index & 0x3FU, &dmar_unit->irte_alloc_bitmap[index >> 6U]);\n\t\t\tspinlock_release(&dmar_unit->lock);\n\t\t}\n\t}\n\n}\n"], "filenames": ["hypervisor/arch/x86/guest/assign.c", "hypervisor/arch/x86/vtd.c"], "buggy_code_start_loc": [75, 1401], "buggy_code_end_loc": [85, 1402], "fixing_code_start_loc": [75, 1401], "fixing_code_end_loc": [83, 1402], "type": "CWE-120", "message": "An issue was discovered in ACRN before 2.5. dmar_free_irte in hypervisor/arch/x86/vtd.c allows an irte_alloc_bitmap buffer overflow.", "other": {"cve": {"id": "CVE-2021-36148", "sourceIdentifier": "cve@mitre.org", "published": "2021-07-02T22:15:08.987", "lastModified": "2021-07-08T18:11:15.057", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in ACRN before 2.5. dmar_free_irte in hypervisor/arch/x86/vtd.c allows an irte_alloc_bitmap buffer overflow."}, {"lang": "es", "value": "Se ha detectado un problema en ACRN versiones anteriores a 2.5,. la funci\u00f3n dmar_free_irte en el archivo hypervisor/arch/x86/vtd.c permite un desbordamiento del b\u00fafer irte_alloc_bitmap"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.8}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-120"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:acrn:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.5", "matchCriteriaId": "CF2731E5-0DF2-4018-8266-7028AC919320"}]}]}], "references": [{"url": "https://github.com/projectacrn/acrn-hypervisor/commit/25c0e3817eb332660dd63d1d4522e63dcc94e79a", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/projectacrn/acrn-hypervisor/commit/25c0e3817eb332660dd63d1d4522e63dcc94e79a"}}