{"buggy_code": ["// SPDX-License-Identifier: Apache-2.0\n// Copyright Authors of Cilium\n\npackage clustermesh\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"text/tabwriter\"\n\t\"time\"\n\n\tappsv1 \"k8s.io/api/apps/v1\"\n\tcorev1 \"k8s.io/api/core/v1\"\n\trbacv1 \"k8s.io/api/rbac/v1\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/types\"\n\t\"k8s.io/apimachinery/pkg/util/intstr\"\n\n\t\"github.com/blang/semver/v4\"\n\t\"github.com/cilium/cilium/api/v1/models\"\n\tciliumv2 \"github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2\"\n\t\"github.com/cilium/cilium/pkg/versioncheck\"\n\n\t\"github.com/cilium/cilium-cli/defaults\"\n\t\"github.com/cilium/cilium-cli/internal/certs\"\n\t\"github.com/cilium/cilium-cli/internal/helm\"\n\t\"github.com/cilium/cilium-cli/internal/utils\"\n\t\"github.com/cilium/cilium-cli/k8s\"\n\t\"github.com/cilium/cilium-cli/status\"\n)\n\nconst (\n\tconfigNameClusterID   = \"cluster-id\"\n\tconfigNameClusterName = \"cluster-name\"\n\tconfigNameTunnel      = \"tunnel\"\n\n\tcaSuffix   = \".etcd-client-ca.crt\"\n\tkeySuffix  = \".etcd-client.key\"\n\tcertSuffix = \".etcd-client.crt\"\n)\n\nvar (\n\treplicas                 = int32(1)\n\tdeploymentMaxSurge       = intstr.FromInt(1)\n\tdeploymentMaxUnavailable = intstr.FromInt(1)\n\tsecretDefaultMode        = int32(0400)\n)\n\nvar clusterRole = &rbacv1.ClusterRole{\n\tObjectMeta: metav1.ObjectMeta{\n\t\tName: defaults.ClusterMeshClusterRoleName,\n\t},\n\tRules: []rbacv1.PolicyRule{\n\t\t{\n\t\t\tAPIGroups: []string{\"discovery.k8s.io\"},\n\t\t\tResources: []string{\"endpointslices\"},\n\t\t\tVerbs:     []string{\"get\", \"list\", \"watch\"},\n\t\t},\n\t\t{\n\t\t\tAPIGroups: []string{\"\"},\n\t\t\tResources: []string{\"namespaces\", \"services\", \"endpoints\"},\n\t\t\tVerbs:     []string{\"get\", \"list\", \"watch\"},\n\t\t},\n\t\t{\n\t\t\tAPIGroups: []string{\"apiextensions.k8s.io\"},\n\t\t\tResources: []string{\"customresourcedefinitions\"},\n\t\t\tVerbs:     []string{\"list\"},\n\t\t},\n\t\t{\n\t\t\tAPIGroups: []string{\"cilium.io\"},\n\t\t\tResources: []string{\n\t\t\t\t\"ciliumnodes\",\n\t\t\t\t\"ciliumnodes/status\",\n\t\t\t\t\"ciliumexternalworkloads\",\n\t\t\t\t\"ciliumexternalworkloads/status\",\n\t\t\t\t\"ciliumidentities\",\n\t\t\t\t\"ciliumidentities/status\",\n\t\t\t\t\"ciliumendpoints\",\n\t\t\t\t\"ciliumendpoints/status\",\n\t\t\t},\n\t\t\tVerbs: []string{\"*\"},\n\t\t},\n\t},\n}\n\nfunc (k *K8sClusterMesh) generateService() (*corev1.Service, error) {\n\tsvc := &corev1.Service{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:        defaults.ClusterMeshServiceName,\n\t\t\tLabels:      defaults.ClusterMeshDeploymentLabels,\n\t\t\tAnnotations: map[string]string{},\n\t\t},\n\t\tSpec: corev1.ServiceSpec{\n\t\t\tType: corev1.ServiceTypeClusterIP,\n\t\t\tPorts: []corev1.ServicePort{\n\t\t\t\t{Port: int32(2379)},\n\t\t\t},\n\t\t\tSelector: defaults.ClusterMeshDeploymentLabels,\n\t\t},\n\t}\n\n\tif k.params.ServiceType != \"\" {\n\t\tif k.params.ServiceType == \"NodePort\" {\n\t\t\tk.Log(\"\u26a0\ufe0f  Using service type NodePort may fail when nodes are removed from the cluster!\")\n\t\t}\n\t\tsvc.Spec.Type = corev1.ServiceType(k.params.ServiceType)\n\t} else {\n\t\tswitch k.flavor.Kind {\n\t\tcase k8s.KindGKE:\n\t\t\tk.Log(\"\ud83d\udd2e Auto-exposing service within GCP VPC (cloud.google.com/load-balancer-type=Internal)\")\n\t\t\tsvc.Spec.Type = corev1.ServiceTypeLoadBalancer\n\t\t\tsvc.ObjectMeta.Annotations[\"cloud.google.com/load-balancer-type\"] = \"Internal\"\n\t\t\t// if all the clusters are in the same region the next annotation can be removed\n\t\t\tsvc.ObjectMeta.Annotations[\"networking.gke.io/internal-load-balancer-allow-global-access\"] = \"true\"\n\t\tcase k8s.KindAKS:\n\t\t\tk.Log(\"\ud83d\udd2e Auto-exposing service within Azure VPC (service.beta.kubernetes.io/azure-load-balancer-internal)\")\n\t\t\tsvc.Spec.Type = corev1.ServiceTypeLoadBalancer\n\t\t\tsvc.ObjectMeta.Annotations[\"service.beta.kubernetes.io/azure-load-balancer-internal\"] = \"true\"\n\t\tcase k8s.KindEKS:\n\t\t\tk.Log(\"\ud83d\udd2e Auto-exposing service within AWS VPC (service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\")\n\t\t\tsvc.Spec.Type = corev1.ServiceTypeLoadBalancer\n\t\t\tsvc.ObjectMeta.Annotations[\"service.beta.kubernetes.io/aws-load-balancer-internal\"] = \"0.0.0.0/0\"\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"cannot auto-detect service type, please specify using '--service-type' option\")\n\t\t}\n\t}\n\n\treturn svc, nil\n}\n\nvar initContainerArgs = []string{`rm -rf /var/run/etcd/*;\nexport ETCDCTL_API=3;\n/usr/local/bin/etcd --data-dir=/var/run/etcd --name=clustermesh-apiserver --listen-client-urls=http://127.0.0.1:2379 --advertise-client-urls=http://127.0.0.1:2379 --initial-cluster-token=clustermesh-apiserver --initial-cluster-state=new --auto-compaction-retention=1 &\nexport rootpw=\"$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16)\";\necho $rootpw | etcdctl --interactive=false user add root;\netcdctl user grant-role root root;\nexport vmpw=\"$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16)\";\necho $vmpw | etcdctl --interactive=false user add externalworkload;\netcdctl role add externalworkload;\netcdctl role grant-permission externalworkload --from-key read '';\netcdctl role grant-permission externalworkload readwrite --prefix cilium/state/noderegister/v1/;\netcdctl role grant-permission externalworkload readwrite --prefix cilium/.initlock/;\netcdctl user grant-role externalworkload externalworkload;\nexport remotepw=\"$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16)\";\necho $remotepw | etcdctl --interactive=false user add remote;\netcdctl role add remote;\netcdctl role grant-permission remote --from-key read '';\netcdctl user grant-role remote remote;\netcdctl auth enable;\nexit`}\n\nfunc (k *K8sClusterMesh) apiserverImage(imagePathMode utils.ImagePathMode) string {\n\treturn utils.BuildImagePath(k.params.ApiserverImage, k.params.ApiserverVersion, defaults.ClusterMeshApiserverImage, k.imageVersion, imagePathMode)\n}\n\nfunc (k *K8sClusterMesh) etcdImage() string {\n\tetcdVersion := \"v3.5.4\"\n\tif k.clusterArch == \"amd64\" {\n\t\treturn \"quay.io/coreos/etcd:\" + etcdVersion\n\t}\n\treturn \"quay.io/coreos/etcd:\" + etcdVersion + \"-\" + k.clusterArch\n}\n\nfunc (k *K8sClusterMesh) etcdEnvs() []corev1.EnvVar {\n\tenvs := []corev1.EnvVar{\n\t\t{\n\t\t\tName:  \"ETCDCTL_API\",\n\t\t\tValue: \"3\",\n\t\t},\n\t\t{\n\t\t\tName: \"HOSTNAME_IP\",\n\t\t\tValueFrom: &corev1.EnvVarSource{\n\t\t\t\tFieldRef: &corev1.ObjectFieldSelector{\n\t\t\t\t\tFieldPath: \"status.podIP\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\tif k.clusterArch == \"arm64\" {\n\t\tenvs = append(envs, corev1.EnvVar{\n\t\t\tName:  \"ETCD_UNSUPPORTED_ARCH\",\n\t\t\tValue: \"arm64\",\n\t\t})\n\t}\n\treturn envs\n}\n\nfunc (k *K8sClusterMesh) generateDeployment(clustermeshApiserverArgs []string) *appsv1.Deployment {\n\n\tdeployment := &appsv1.Deployment{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:   defaults.ClusterMeshDeploymentName,\n\t\t\tLabels: defaults.ClusterMeshDeploymentLabels,\n\t\t},\n\t\tSpec: appsv1.DeploymentSpec{\n\t\t\tReplicas: &replicas,\n\t\t\tSelector: &metav1.LabelSelector{\n\t\t\t\tMatchLabels: defaults.ClusterMeshDeploymentLabels,\n\t\t\t},\n\t\t\tStrategy: appsv1.DeploymentStrategy{\n\t\t\t\tType: appsv1.RollingUpdateDeploymentStrategyType,\n\t\t\t\tRollingUpdate: &appsv1.RollingUpdateDeployment{\n\t\t\t\t\tMaxUnavailable: &deploymentMaxUnavailable,\n\t\t\t\t\tMaxSurge:       &deploymentMaxSurge,\n\t\t\t\t},\n\t\t\t},\n\t\t\tTemplate: corev1.PodTemplateSpec{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\t\tName:   defaults.ClusterMeshDeploymentName,\n\t\t\t\t\tLabels: defaults.ClusterMeshDeploymentLabels,\n\t\t\t\t},\n\t\t\t\tSpec: corev1.PodSpec{\n\t\t\t\t\tRestartPolicy:      corev1.RestartPolicyAlways,\n\t\t\t\t\tServiceAccountName: defaults.ClusterMeshServiceAccountName,\n\t\t\t\t\tContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:    \"etcd\",\n\t\t\t\t\t\t\tCommand: []string{\"/usr/local/bin/etcd\"},\n\t\t\t\t\t\t\tArgs: []string{\n\t\t\t\t\t\t\t\t\"--data-dir=/var/run/etcd\",\n\t\t\t\t\t\t\t\t\"--name=clustermesh-apiserver\",\n\t\t\t\t\t\t\t\t\"--client-cert-auth\",\n\t\t\t\t\t\t\t\t\"--trusted-ca-file=/var/lib/etcd-secrets/ca.crt\",\n\t\t\t\t\t\t\t\t\"--cert-file=/var/lib/etcd-secrets/tls.crt\",\n\t\t\t\t\t\t\t\t\"--key-file=/var/lib/etcd-secrets/tls.key\",\n\t\t\t\t\t\t\t\t// Surrounding the IPv4 address with brackets works in this case, since etcd\n\t\t\t\t\t\t\t\t// uses net.SplitHostPort() internally and it accepts that format.\n\t\t\t\t\t\t\t\t\"--listen-client-urls=https://127.0.0.1:2379,https://[$(HOSTNAME_IP)]:2379\",\n\t\t\t\t\t\t\t\t\"--advertise-client-urls=https://[$(HOSTNAME_IP)]:2379\",\n\t\t\t\t\t\t\t\t\"--initial-cluster-token=clustermesh-apiserver\",\n\t\t\t\t\t\t\t\t\"--auto-compaction-retention=1\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tImage:           k.etcdImage(),\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tEnv:             k.etcdEnvs(),\n\t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"etcd-server-secrets\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/var/lib/etcd-secrets\",\n\t\t\t\t\t\t\t\t\tReadOnly:  true,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"etcd-data-dir\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/var/run/etcd\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:    \"apiserver\",\n\t\t\t\t\t\t\tCommand: []string{\"/usr/bin/clustermesh-apiserver\"},\n\t\t\t\t\t\t\tArgs: append(clustermeshApiserverArgs,\n\t\t\t\t\t\t\t\t\"--cluster-name=\"+k.clusterName,\n\t\t\t\t\t\t\t\t\"--cluster-id=\"+k.clusterID,\n\t\t\t\t\t\t\t\t\"--kvstore-opt\",\n\t\t\t\t\t\t\t\t\"etcd.config=/var/lib/cilium/etcd-config.yaml\",\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\tImage:           k.apiserverImage(utils.ImagePathIncludeDigest),\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tEnv: []corev1.EnvVar{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName: \"CILIUM_CLUSTER_NAME\",\n\t\t\t\t\t\t\t\t\tValueFrom: &corev1.EnvVarSource{\n\t\t\t\t\t\t\t\t\t\tConfigMapKeyRef: &corev1.ConfigMapKeySelector{\n\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ConfigMapName,\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\tKey: configNameClusterName,\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName: \"CILIUM_CLUSTER_ID\",\n\t\t\t\t\t\t\t\t\tValueFrom: &corev1.EnvVarSource{\n\t\t\t\t\t\t\t\t\t\tConfigMapKeyRef: &corev1.ConfigMapKeySelector{\n\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ConfigMapName,\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\tKey: configNameClusterID,\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName: \"CILIUM_IDENTITY_ALLOCATION_MODE\",\n\t\t\t\t\t\t\t\t\tValueFrom: &corev1.EnvVarSource{\n\t\t\t\t\t\t\t\t\t\tConfigMapKeyRef: &corev1.ConfigMapKeySelector{\n\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ConfigMapName,\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\tKey: \"identity-allocation-mode\",\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"etcd-admin-client\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/var/lib/cilium/etcd-secrets\",\n\t\t\t\t\t\t\t\t\tReadOnly:  true,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tInitContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:            \"etcd-init\",\n\t\t\t\t\t\t\tCommand:         []string{\"/bin/sh\", \"-c\"},\n\t\t\t\t\t\t\tArgs:            initContainerArgs,\n\t\t\t\t\t\t\tImage:           k.etcdImage(),\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tEnv:             k.etcdEnvs(),\n\t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"etcd-data-dir\",\n\t\t\t\t\t\t\t\t\tMountPath: \"etcd-data-dir\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tVolumes: []corev1.Volume{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"etcd-data-dir\",\n\t\t\t\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\t\t\t\tEmptyDir: &corev1.EmptyDirVolumeSource{},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"etcd-server-secrets\",\n\t\t\t\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\t\t\t\tProjected: &corev1.ProjectedVolumeSource{\n\t\t\t\t\t\t\t\t\tDefaultMode: &secretDefaultMode,\n\t\t\t\t\t\t\t\t\tSources: []corev1.VolumeProjection{\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tSecret: &corev1.SecretProjection{\n\t\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.CASecretName,\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\tItems: []corev1.KeyToPath{\n\t\t\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tKey:  defaults.CASecretCertName,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tPath: \"ca.crt\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tSecret: &corev1.SecretProjection{\n\t\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ClusterMeshServerSecretName,\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"etcd-admin-client\",\n\t\t\t\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\t\t\t\tProjected: &corev1.ProjectedVolumeSource{\n\t\t\t\t\t\t\t\t\tDefaultMode: &secretDefaultMode,\n\t\t\t\t\t\t\t\t\tSources: []corev1.VolumeProjection{\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tSecret: &corev1.SecretProjection{\n\t\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.CASecretName,\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\tItems: []corev1.KeyToPath{\n\t\t\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tKey:  defaults.CASecretCertName,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tPath: \"ca.crt\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tSecret: &corev1.SecretProjection{\n\t\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ClusterMeshAdminSecretName,\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\treturn deployment\n}\n\ntype k8sClusterMeshImplementation interface {\n\tCreateSecret(ctx context.Context, namespace string, secret *corev1.Secret, opts metav1.CreateOptions) (*corev1.Secret, error)\n\tPatchSecret(ctx context.Context, namespace, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions) (*corev1.Secret, error)\n\tDeleteSecret(ctx context.Context, namespace, name string, opts metav1.DeleteOptions) error\n\tGetSecret(ctx context.Context, namespace, name string, opts metav1.GetOptions) (*corev1.Secret, error)\n\tCreateServiceAccount(ctx context.Context, namespace string, account *corev1.ServiceAccount, opts metav1.CreateOptions) (*corev1.ServiceAccount, error)\n\tDeleteServiceAccount(ctx context.Context, namespace, name string, opts metav1.DeleteOptions) error\n\tCreateClusterRole(ctx context.Context, role *rbacv1.ClusterRole, opts metav1.CreateOptions) (*rbacv1.ClusterRole, error)\n\tDeleteClusterRole(ctx context.Context, name string, opts metav1.DeleteOptions) error\n\tCreateClusterRoleBinding(ctx context.Context, role *rbacv1.ClusterRoleBinding, opts metav1.CreateOptions) (*rbacv1.ClusterRoleBinding, error)\n\tDeleteClusterRoleBinding(ctx context.Context, name string, opts metav1.DeleteOptions) error\n\tGetConfigMap(ctx context.Context, namespace, name string, opts metav1.GetOptions) (*corev1.ConfigMap, error)\n\tCreateDeployment(ctx context.Context, namespace string, deployment *appsv1.Deployment, opts metav1.CreateOptions) (*appsv1.Deployment, error)\n\tGetDeployment(ctx context.Context, namespace, name string, opts metav1.GetOptions) (*appsv1.Deployment, error)\n\tDeleteDeployment(ctx context.Context, namespace, name string, opts metav1.DeleteOptions) error\n\tCheckDeploymentStatus(ctx context.Context, namespace, deployment string) error\n\tCreateService(ctx context.Context, namespace string, service *corev1.Service, opts metav1.CreateOptions) (*corev1.Service, error)\n\tDeleteService(ctx context.Context, namespace, name string, opts metav1.DeleteOptions) error\n\tGetService(ctx context.Context, namespace, name string, opts metav1.GetOptions) (*corev1.Service, error)\n\tPatchDaemonSet(ctx context.Context, namespace, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions) (*appsv1.DaemonSet, error)\n\tGetDaemonSet(ctx context.Context, namespace, name string, options metav1.GetOptions) (*appsv1.DaemonSet, error)\n\tListNodes(ctx context.Context, options metav1.ListOptions) (*corev1.NodeList, error)\n\tListPods(ctx context.Context, namespace string, options metav1.ListOptions) (*corev1.PodList, error)\n\tAutodetectFlavor(ctx context.Context) k8s.Flavor\n\tCiliumStatus(ctx context.Context, namespace, pod string) (*models.StatusResponse, error)\n\tClusterName() string\n\tListCiliumExternalWorkloads(ctx context.Context, opts metav1.ListOptions) (*ciliumv2.CiliumExternalWorkloadList, error)\n\tGetCiliumExternalWorkload(ctx context.Context, name string, opts metav1.GetOptions) (*ciliumv2.CiliumExternalWorkload, error)\n\tCreateCiliumExternalWorkload(ctx context.Context, cew *ciliumv2.CiliumExternalWorkload, opts metav1.CreateOptions) (*ciliumv2.CiliumExternalWorkload, error)\n\tDeleteCiliumExternalWorkload(ctx context.Context, name string, opts metav1.DeleteOptions) error\n\tGetRunningCiliumVersion(ctx context.Context, namespace string) (string, error)\n\tListCiliumEndpoints(ctx context.Context, namespace string, options metav1.ListOptions) (*ciliumv2.CiliumEndpointList, error)\n\tGetPlatform(ctx context.Context) (*k8s.Platform, error)\n\tCiliumLogs(ctx context.Context, namespace, pod string, since time.Time, filter *regexp.Regexp) (string, error)\n\tGetHelmState(ctx context.Context, namespace string, secretName string) (*helm.State, error)\n}\n\ntype K8sClusterMesh struct {\n\tclient          k8sClusterMeshImplementation\n\tcertManager     *certs.CertManager\n\tstatusCollector *status.K8sStatusCollector\n\tflavor          k8s.Flavor\n\tparams          Parameters\n\tclusterName     string\n\tclusterID       string\n\timageVersion    string\n\tclusterArch     string\n}\n\ntype Parameters struct {\n\tNamespace            string\n\tServiceType          string\n\tDestinationContext   string\n\tWait                 bool\n\tWaitDuration         time.Duration\n\tDestinationEndpoints []string\n\tSourceEndpoints      []string\n\tSkipServiceCheck     bool\n\tApiserverImage       string\n\tApiserverVersion     string\n\tCreateCA             bool\n\tWriter               io.Writer\n\tLabels               map[string]string\n\tIPv4AllocCIDR        string\n\tIPv6AllocCIDR        string\n\tAll                  bool\n\tConfigOverwrites     []string\n\tRetries              int\n\tHelmValuesSecretName string\n\tOutput               string\n}\n\nfunc (p Parameters) validateParams() error {\n\tif p.ApiserverImage != defaults.ClusterMeshApiserverImage {\n\t\treturn nil\n\t} else if err := utils.CheckVersion(p.ApiserverVersion); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (p Parameters) waitTimeout() time.Duration {\n\tif p.WaitDuration != time.Duration(0) {\n\t\treturn p.WaitDuration\n\t}\n\n\treturn time.Minute * 15\n}\n\nfunc NewK8sClusterMesh(client k8sClusterMeshImplementation, p Parameters) *K8sClusterMesh {\n\treturn &K8sClusterMesh{\n\t\tclient:      client,\n\t\tparams:      p,\n\t\tcertManager: certs.NewCertManager(client, certs.Parameters{Namespace: p.Namespace}),\n\t}\n}\n\nfunc (k *K8sClusterMesh) Log(format string, a ...interface{}) {\n\tfmt.Fprintf(k.params.Writer, format+\"\\n\", a...)\n}\n\nfunc (k *K8sClusterMesh) GetClusterConfig(ctx context.Context) error {\n\tk.flavor = k.client.AutodetectFlavor(ctx)\n\n\tcm, err := k.client.GetConfigMap(ctx, k.params.Namespace, defaults.ConfigMapName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to retrieve ConfigMap %q: %w\", defaults.ConfigMapName, err)\n\t}\n\n\tclusterID := cm.Data[configNameClusterID]\n\tif clusterID == \"\" {\n\t\tclusterID = \"0\"\n\t}\n\tk.clusterID = clusterID\n\n\tclusterName := cm.Data[configNameClusterName]\n\tif clusterName == \"\" {\n\t\tclusterName = \"default\"\n\t}\n\tk.clusterName = clusterName\n\n\tif clusterID == \"0\" || clusterName == \"default\" {\n\t\tk.Log(\"\u26a0\ufe0f  Cluster not configured for clustermesh, use '--cluster-id' and '--cluster-name' with 'cilium install'. External workloads may still be configured.\")\n\t}\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) Disable(ctx context.Context) error {\n\tk.Log(\"\ud83d\udd25 Deleting clustermesh-apiserver...\")\n\tk.client.DeleteService(ctx, k.params.Namespace, defaults.ClusterMeshServiceName, metav1.DeleteOptions{})\n\tk.client.DeleteDeployment(ctx, k.params.Namespace, defaults.ClusterMeshDeploymentName, metav1.DeleteOptions{})\n\tk.client.DeleteClusterRoleBinding(ctx, defaults.ClusterMeshClusterRoleName, metav1.DeleteOptions{})\n\tk.client.DeleteClusterRole(ctx, defaults.ClusterMeshClusterRoleName, metav1.DeleteOptions{})\n\tk.client.DeleteServiceAccount(ctx, k.params.Namespace, defaults.ClusterMeshServiceAccountName, metav1.DeleteOptions{})\n\tk.client.DeleteSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, metav1.DeleteOptions{})\n\n\tk.deleteCertificates(ctx)\n\n\tk.Log(\"\u2705 ClusterMesh disabled.\")\n\n\treturn nil\n}\n\nfunc (p Parameters) validateForEnable() error {\n\tswitch corev1.ServiceType(p.ServiceType) {\n\tcase corev1.ServiceTypeClusterIP:\n\tcase corev1.ServiceTypeNodePort:\n\tcase corev1.ServiceTypeLoadBalancer:\n\tcase corev1.ServiceTypeExternalName:\n\tcase \"\":\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown service type %q\", p.ServiceType)\n\t}\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) Enable(ctx context.Context) error {\n\tif err := k.params.validateParams(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := k.params.validateForEnable(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := k.GetClusterConfig(ctx); err != nil {\n\t\treturn err\n\t}\n\n\tvar err error\n\tk.imageVersion, err = k.client.GetRunningCiliumVersion(ctx, k.params.Namespace)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tp, err := k.client.GetPlatform(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tk.clusterArch = p.Arch\n\n\tsvc, err := k.generateService()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t_, err = k.client.GetDeployment(ctx, k.params.Namespace, defaults.ClusterMeshDeploymentName, metav1.GetOptions{})\n\tif err == nil {\n\t\tk.Log(\"\u2705 ClusterMesh is already enabled\")\n\t\treturn nil\n\t}\n\n\tif err := k.installCertificates(ctx); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2728 Deploying clustermesh-apiserver from %s...\", k.apiserverImage(utils.ImagePathExcludeDigest))\n\tif _, err := k.client.CreateServiceAccount(ctx, k.params.Namespace, k8s.NewServiceAccount(defaults.ClusterMeshServiceAccountName), metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := k.client.CreateClusterRole(ctx, clusterRole, metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := k.client.CreateClusterRoleBinding(ctx, k8s.NewClusterRoleBinding(defaults.ClusterMeshClusterRoleName, k.params.Namespace, defaults.ClusterMeshServiceAccountName), metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tfor i, opt := range k.params.ConfigOverwrites {\n\t\tif !strings.HasPrefix(opt, \"--\") {\n\t\t\tk.params.ConfigOverwrites[i] = \"--\" + opt\n\t\t}\n\t}\n\n\tif _, err := k.client.CreateDeployment(ctx, k.params.Namespace, k.generateDeployment(k.params.ConfigOverwrites), metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := k.client.CreateService(ctx, k.params.Namespace, svc, metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 ClusterMesh enabled!\")\n\n\treturn nil\n}\n\ntype accessInformation struct {\n\tServiceIPs           []string `json:\"service_ips,omitempty\"`\n\tServicePort          int      `json:\"service_port,omitempty\"`\n\tClusterID            string   `json:\"cluster_id,omitempty\"`\n\tClusterName          string   `json:\"cluster_name,omitempty\"`\n\tCA                   []byte   `json:\"ca,omitempty\"`\n\tClientCert           []byte   `json:\"client_cert,omitempty\"`\n\tClientKey            []byte   `json:\"client_key,omitempty\"`\n\tExternalWorkloadCert []byte   `json:\"external_workload_cert,omitempty\"`\n\tExternalWorkloadKey  []byte   `json:\"external_workload_key,omitempty\"`\n\tTunnel               string   `json:\"tunnel,omitempty\"`\n}\n\nfunc (ai *accessInformation) etcdConfiguration() string {\n\tcfg := \"endpoints:\\n\"\n\tcfg += \"- https://\" + ai.ClusterName + \".mesh.cilium.io:\" + fmt.Sprintf(\"%d\", ai.ServicePort) + \"\\n\"\n\tcfg += \"trusted-ca-file: /var/lib/cilium/clustermesh/\" + ai.ClusterName + caSuffix + \"\\n\"\n\tcfg += \"key-file: /var/lib/cilium/clustermesh/\" + ai.ClusterName + keySuffix + \"\\n\"\n\tcfg += \"cert-file: /var/lib/cilium/clustermesh/\" + ai.ClusterName + certSuffix + \"\\n\"\n\n\treturn cfg\n}\n\nfunc (ai *accessInformation) validate() bool {\n\treturn ai.ClusterName != \"\" &&\n\t\tai.ClusterName != \"default\" &&\n\t\tai.ClusterID != \"\" &&\n\t\tai.ClusterID != \"0\"\n}\n\nfunc getDeprecatedName(secretName string) string {\n\tswitch secretName {\n\tcase defaults.ClusterMeshServerSecretName,\n\t\tdefaults.ClusterMeshAdminSecretName,\n\t\tdefaults.ClusterMeshClientSecretName,\n\t\tdefaults.ClusterMeshExternalWorkloadSecretName:\n\t\treturn secretName + \"s\"\n\tdefault:\n\t\treturn \"\"\n\t}\n}\n\n// We had inconsistency in naming clustermesh secrets between Helm installation and Cilium CLI installation\n// Cilium CLI was naming clustermesh secrets with trailing 's'. eg. 'clustermesh-apiserver-client-certs' instead of `clustermesh-apiserver-client-cert`\n// This caused Cilium CLI 'clustermesh status' command to fail when Cilium is installed using Helm\n// getSecret handles both secret names and logs warning if deprecated secret name is found\nfunc (k *K8sClusterMesh) getSecret(ctx context.Context, client k8sClusterMeshImplementation, secretName string) (*corev1.Secret, error) {\n\n\tsecret, err := client.GetSecret(ctx, k.params.Namespace, secretName, metav1.GetOptions{})\n\tif err != nil {\n\t\tdeprecatedSecretName := getDeprecatedName(secretName)\n\t\tif deprecatedSecretName == \"\" {\n\t\t\treturn nil, fmt.Errorf(\"unable to get secret %q: %w\", secretName, err)\n\t\t}\n\n\t\tk.Log(\"Trying to get secret %s by deprecated name %s\", secretName, deprecatedSecretName)\n\n\t\tsecret, err = client.GetSecret(ctx, k.params.Namespace, deprecatedSecretName, metav1.GetOptions{})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to get secret %q: %w\", deprecatedSecretName, err)\n\t\t}\n\n\t\tk.Log(\"\u26a0\ufe0f Deprecated secret name %q, should be changed to %q\", secret.Name, secretName)\n\n\t}\n\n\treturn secret, err\n}\n\nfunc (k *K8sClusterMesh) extractAccessInformation(ctx context.Context, client k8sClusterMeshImplementation, endpoints []string, verbose bool, getExternalWorkLoadSecret bool) (*accessInformation, error) {\n\tcm, err := client.GetConfigMap(ctx, k.params.Namespace, defaults.ConfigMapName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve ConfigMap %q: %w\", defaults.ConfigMapName, err)\n\t}\n\n\tif _, ok := cm.Data[configNameClusterName]; !ok {\n\t\treturn nil, fmt.Errorf(\"%s is not set in ConfigMap %q\", configNameClusterName, defaults.ConfigMapName)\n\t}\n\n\tclusterID := cm.Data[configNameClusterID]\n\tclusterName := cm.Data[configNameClusterName]\n\n\tif verbose {\n\t\tk.Log(\"\u2728 Extracting access information of cluster %s...\", clusterName)\n\t}\n\tsvc, err := client.GetService(ctx, k.params.Namespace, defaults.ClusterMeshServiceName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get clustermesh service %q: %w\", defaults.ClusterMeshServiceName, err)\n\t}\n\n\tif verbose {\n\t\tk.Log(\"\ud83d\udd11 Extracting secrets from cluster %s...\", clusterName)\n\t}\n\tcaSecret, err := client.GetSecret(ctx, k.params.Namespace, defaults.CASecretName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get secret %q to retrieve CA: %s\", defaults.CASecretName, err)\n\t}\n\n\tcaCert, ok := caSecret.Data[defaults.CASecretCertName]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"secret %q does not contain CA cert %q\", defaults.CASecretName, defaults.CASecretCertName)\n\t}\n\n\tmeshSecret, err := k.getSecret(ctx, client, defaults.ClusterMeshClientSecretName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get client secret to access clustermesh service: %w\", err)\n\t}\n\n\tclientKey, ok := meshSecret.Data[corev1.TLSPrivateKeyKey]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"secret %q does not contain key %q\", meshSecret.Name, corev1.TLSPrivateKeyKey)\n\t}\n\n\tclientCert, ok := meshSecret.Data[corev1.TLSCertKey]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"secret %q does not contain key %q\", meshSecret.Name, corev1.TLSCertKey)\n\t}\n\n\t// ExternalWorkload secret is created by 'clustermesh enable' command, but it isn't created by Helm. We should try to load this secret only when needed\n\tvar externalWorkloadKey, externalWorkloadCert []byte\n\tif getExternalWorkLoadSecret {\n\t\texternalWorkloadSecret, err := k.getSecret(ctx, client, defaults.ClusterMeshExternalWorkloadSecretName)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to get external workload secret to access clustermesh service\")\n\t\t}\n\n\t\texternalWorkloadKey, ok = externalWorkloadSecret.Data[corev1.TLSPrivateKeyKey]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"secret %q does not contain key %q\", externalWorkloadSecret.Namespace, corev1.TLSPrivateKeyKey)\n\t\t}\n\n\t\texternalWorkloadCert, ok = externalWorkloadSecret.Data[corev1.TLSCertKey]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"secret %q does not contain key %q\", externalWorkloadSecret.Namespace, corev1.TLSCertKey)\n\t\t}\n\t}\n\n\tai := &accessInformation{\n\t\tClusterID:            clusterID,\n\t\tClusterName:          clusterName,\n\t\tCA:                   caCert,\n\t\tClientKey:            clientKey,\n\t\tClientCert:           clientCert,\n\t\tExternalWorkloadKey:  externalWorkloadKey,\n\t\tExternalWorkloadCert: externalWorkloadCert,\n\t\tServiceIPs:           []string{},\n\t\tTunnel:               cm.Data[configNameTunnel],\n\t}\n\n\tswitch {\n\tcase len(endpoints) > 0:\n\t\tfor _, endpoint := range endpoints {\n\t\t\tip, port, err := net.SplitHostPort(endpoint)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid endpoint %q, must be IP:PORT: %w\", endpoint, err)\n\t\t\t}\n\n\t\t\tintPort, err := strconv.Atoi(port)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid port %q: %w\", port, err)\n\t\t\t}\n\n\t\t\tif ai.ServicePort == 0 {\n\t\t\t\tai.ServicePort = intPort\n\t\t\t} else if ai.ServicePort != intPort {\n\t\t\t\treturn nil, fmt.Errorf(\"port mismatch (%d != %d), all endpoints must use the same port number\", ai.ServicePort, intPort)\n\t\t\t}\n\n\t\t\tai.ServiceIPs = append(ai.ServiceIPs, ip)\n\t\t}\n\n\tcase svc.Spec.Type == corev1.ServiceTypeClusterIP:\n\t\tif len(svc.Spec.Ports) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"port of service could not be derived, service has no ports\")\n\t\t}\n\t\tif svc.Spec.Ports[0].Port == 0 {\n\t\t\treturn nil, fmt.Errorf(\"port is not set in service\")\n\t\t}\n\t\tai.ServicePort = int(svc.Spec.Ports[0].Port)\n\n\t\tif svc.Spec.ClusterIP == \"\" {\n\t\t\treturn nil, fmt.Errorf(\"IP of service could not be derived, service has no ClusterIP\")\n\t\t}\n\t\tai.ServiceIPs = append(ai.ServiceIPs, svc.Spec.ClusterIP)\n\n\tcase svc.Spec.Type == corev1.ServiceTypeNodePort:\n\t\tif len(svc.Spec.Ports) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"port of service could not be derived, service has no ports\")\n\t\t}\n\n\t\tif svc.Spec.Ports[0].NodePort == 0 {\n\t\t\treturn nil, fmt.Errorf(\"nodeport is not set in service\")\n\t\t}\n\t\tai.ServicePort = int(svc.Spec.Ports[0].NodePort)\n\n\t\tnodes, err := client.ListNodes(ctx, metav1.ListOptions{})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to list nodes in cluster: %w\", err)\n\t\t}\n\n\t\tfor _, node := range nodes.Items {\n\t\t\tnodeIP := \"\"\n\t\t\tfor _, address := range node.Status.Addresses {\n\t\t\t\tswitch address.Type {\n\t\t\t\tcase corev1.NodeExternalIP:\n\t\t\t\t\tnodeIP = address.Address\n\t\t\t\tcase corev1.NodeInternalIP:\n\t\t\t\t\tif nodeIP == \"\" {\n\t\t\t\t\t\tnodeIP = address.Address\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif nodeIP != \"\" {\n\t\t\t\tai.ServiceIPs = append(ai.ServiceIPs, nodeIP)\n\n\t\t\t\t// We can't really support multiple nodes as\n\t\t\t\t// the NodePort will be different and the\n\t\t\t\t// current use of hostAliases will lead to\n\t\t\t\t// DNS-style RR requiring all endpoints to use\n\t\t\t\t// the same port\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tk.Log(\"\u26a0\ufe0f  Service type NodePort detected! Service may fail when nodes are removed from the cluster!\")\n\n\tcase svc.Spec.Type == corev1.ServiceTypeLoadBalancer:\n\t\tif len(svc.Spec.Ports) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"port of service could not be derived, service has no ports\")\n\t\t}\n\n\t\tai.ServicePort = int(svc.Spec.Ports[0].Port)\n\n\t\tfor _, ingressStatus := range svc.Status.LoadBalancer.Ingress {\n\t\t\tif ingressStatus.Hostname == \"\" {\n\t\t\t\tai.ServiceIPs = append(ai.ServiceIPs, ingressStatus.IP)\n\t\t\t} else {\n\t\t\t\tk.Log(\"Hostname based ingress detected, trying to resolve it\")\n\n\t\t\t\tips, err := net.LookupHost(ingressStatus.Hostname)\n\t\t\t\tif err != nil {\n\t\t\t\t\tk.Log(fmt.Sprintf(\"Could not resolve the hostname of the ingress, falling back to the static IP. Error: %v\", err))\n\t\t\t\t\tai.ServiceIPs = append(ai.ServiceIPs, ingressStatus.IP)\n\t\t\t\t} else {\n\t\t\t\t\tk.Log(\"Hostname resolved, using the found ip(s)\")\n\t\t\t\t\tai.ServiceIPs = append(ai.ServiceIPs, ips...)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tswitch {\n\tcase len(ai.ServiceIPs) > 0:\n\t\tif verbose {\n\t\t\tk.Log(\"\u2139\ufe0f  Found ClusterMesh service IPs: %s\", ai.ServiceIPs)\n\t\t}\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unable to derive service IPs automatically\")\n\t}\n\n\treturn ai, nil\n}\n\nfunc (k *K8sClusterMesh) patchConfig(ctx context.Context, client k8sClusterMeshImplementation, ai *accessInformation) error {\n\t_, err := client.GetSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, metav1.GetOptions{})\n\tif err != nil {\n\t\tk.Log(\"\ud83d\udd11 Secret %s does not exist yet, creating it...\", defaults.ClusterMeshSecretName)\n\t\t_, err = client.CreateSecret(ctx, k.params.Namespace, k8s.NewSecret(defaults.ClusterMeshSecretName, k.params.Namespace, map[string][]byte{}), metav1.CreateOptions{})\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to create secret: %w\", err)\n\t\t}\n\t}\n\n\tk.Log(\"\ud83d\udd11 Patching existing secret %s...\", defaults.ClusterMeshSecretName)\n\n\tetcdBase64 := `\"` + ai.ClusterName + `\": \"` + base64.StdEncoding.EncodeToString([]byte(ai.etcdConfiguration())) + `\"`\n\tcaBase64 := `\"` + ai.ClusterName + caSuffix + `\": \"` + base64.StdEncoding.EncodeToString(ai.CA) + `\"`\n\tkeyBase64 := `\"` + ai.ClusterName + keySuffix + `\": \"` + base64.StdEncoding.EncodeToString(ai.ClientKey) + `\"`\n\tcertBase64 := `\"` + ai.ClusterName + certSuffix + `\": \"` + base64.StdEncoding.EncodeToString(ai.ClientCert) + `\"`\n\n\tpatch := []byte(`{\"data\":{` + etcdBase64 + `,` + caBase64 + `,` + keyBase64 + `,` + certBase64 + `}}`)\n\t_, err = client.PatchSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, types.StrategicMergePatchType, patch, metav1.PatchOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to patch secret %s with patch %q: %w\", defaults.ClusterMeshSecretName, patch, err)\n\t}\n\n\tvar aliases []string\n\tfor _, ip := range ai.ServiceIPs {\n\t\taliases = append(aliases, `{\"ip\":\"`+ip+`\", \"hostnames\":[\"`+ai.ClusterName+`.mesh.cilium.io\"]}`)\n\t}\n\n\tpatch = []byte(`{\"spec\":{\"template\":{\"spec\":{\"hostAliases\":[` + strings.Join(aliases, \",\") + `]}}}}`)\n\n\tk.Log(\"\u2728 Patching DaemonSet with IP aliases %s...\", defaults.ClusterMeshSecretName)\n\t_, err = client.PatchDaemonSet(ctx, k.params.Namespace, defaults.AgentDaemonSetName, types.StrategicMergePatchType, patch, metav1.PatchOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to patch DaemonSet %s with patch %q: %w\", defaults.AgentDaemonSetName, patch, err)\n\t}\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) Connect(ctx context.Context) error {\n\tremoteCluster, err := k8s.NewClient(k.params.DestinationContext, \"\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create Kubernetes client to access remote cluster %q: %w\", k.params.DestinationContext, err)\n\t}\n\n\taiRemote, err := k.extractAccessInformation(ctx, remoteCluster, k.params.DestinationEndpoints, true, false)\n\tif err != nil {\n\t\tk.Log(\"\u274c Unable to retrieve access information of remote cluster %q: %s\", remoteCluster.ClusterName(), err)\n\t\treturn err\n\t}\n\n\tif !aiRemote.validate() {\n\t\treturn fmt.Errorf(\"remote cluster has non-unique name (%s) and/or ID (%s)\", aiRemote.ClusterName, aiRemote.ClusterID)\n\t}\n\n\taiLocal, err := k.extractAccessInformation(ctx, k.client, k.params.SourceEndpoints, true, false)\n\tif err != nil {\n\t\tk.Log(\"\u274c Unable to retrieve access information of local cluster %q: %s\", k.client.ClusterName(), err)\n\t\treturn err\n\t}\n\n\tif !aiLocal.validate() {\n\t\treturn fmt.Errorf(\"local cluster has the default name (cluster name: %s) and/or ID 0 (cluster ID: %s)\",\n\t\t\taiLocal.ClusterName, aiLocal.ClusterID)\n\t}\n\n\tcid, err := strconv.Atoi(aiRemote.ClusterID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"remote cluster has non-numeric cluster ID %s. Only numeric values 1-255 are allowed\", aiRemote.ClusterID)\n\t}\n\tif cid < 1 || cid > 255 {\n\t\treturn fmt.Errorf(\"remote cluster has cluster ID %d out of acceptable range (1-255)\", cid)\n\t}\n\n\tif aiRemote.ClusterName == aiLocal.ClusterName {\n\t\treturn fmt.Errorf(\"remote and local cluster have the same, non-unique name: %s\", aiLocal.ClusterName)\n\t}\n\n\tif aiRemote.ClusterID == aiLocal.ClusterID {\n\t\treturn fmt.Errorf(\"remote and local cluster have the same, non-unique ID: %s\", aiLocal.ClusterID)\n\t}\n\n\tk.Log(\"\u2728 Connecting cluster %s -> %s...\", k.client.ClusterName(), remoteCluster.ClusterName())\n\tif err := k.patchConfig(ctx, k.client, aiRemote); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2728 Connecting cluster %s -> %s...\", remoteCluster.ClusterName(), k.client.ClusterName())\n\tif err := k.patchConfig(ctx, remoteCluster, aiLocal); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 Connected cluster %s and %s!\", k.client.ClusterName(), remoteCluster.ClusterName())\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) disconnectCluster(ctx context.Context, src, dst k8sClusterMeshImplementation) error {\n\tcm, err := dst.GetConfigMap(ctx, k.params.Namespace, defaults.ConfigMapName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to retrieve ConfigMap %q: %w\", defaults.ConfigMapName, err)\n\t}\n\n\tif _, ok := cm.Data[configNameClusterName]; !ok {\n\t\treturn fmt.Errorf(\"%s is not set in ConfigMap %q\", configNameClusterName, defaults.ConfigMapName)\n\t}\n\n\tclusterName := cm.Data[configNameClusterName]\n\n\tk.Log(\"\ud83d\udd11 Patching existing secret %s...\", defaults.ClusterMeshSecretName)\n\tmeshSecret, err := src.GetSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"clustermesh configuration secret %s does not exist\", defaults.ClusterMeshSecretName)\n\t}\n\n\tfor _, suffix := range []string{\"\", caSuffix, keySuffix, certSuffix} {\n\t\tif _, ok := meshSecret.Data[clusterName+suffix]; !ok {\n\t\t\tk.Log(\"\u26a0\ufe0f  Key %q does not exist in secret. Cluster already disconnected?\", clusterName+suffix)\n\t\t\tcontinue\n\t\t}\n\n\t\tpatch := []byte(`[{\"op\": \"remove\", \"path\": \"/data/` + clusterName + suffix + `\"}]`)\n\t\t_, err = src.PatchSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, types.JSONPatchType, patch, metav1.PatchOptions{})\n\t\tif err != nil {\n\t\t\tk.Log(\"\u274c Warning: Unable to patch secret %s with path %q: %s\", defaults.ClusterMeshSecretName, patch, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) Disconnect(ctx context.Context) error {\n\tremoteCluster, err := k8s.NewClient(k.params.DestinationContext, \"\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create Kubernetes client to access remote cluster %q: %w\", k.params.DestinationContext, err)\n\t}\n\n\tif err := k.disconnectCluster(ctx, k.client, remoteCluster); err != nil {\n\t\treturn err\n\t}\n\n\tif err := k.disconnectCluster(ctx, remoteCluster, k.client); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 Disconnected cluster %s and %s.\", k.client.ClusterName(), remoteCluster.ClusterName())\n\n\treturn nil\n}\n\ntype Status struct {\n\tAccessInformation *accessInformation  `json:\"access_information,omitempty\"`\n\tService           *corev1.Service     `json:\"service,omitempty\"`\n\tConnectivity      *ConnectivityStatus `json:\"connectivity,omitempty\"`\n}\n\nfunc (k *K8sClusterMesh) statusAccessInformation(ctx context.Context, log bool, getExternalWorkloadSecret bool) (*accessInformation, error) {\n\tw := utils.NewWaitObserver(ctx, utils.WaitParameters{Log: func(err error, wait string) {\n\t\tif log {\n\t\t\tk.Log(\"\u231b Waiting (%s) for access information: %s\", wait, err)\n\t\t}\n\t}})\n\tdefer w.Cancel()\n\nretry:\n\tai, err := k.extractAccessInformation(ctx, k.client, []string{}, false, getExternalWorkloadSecret)\n\tif err != nil && k.params.Wait {\n\t\tif err := w.Retry(err); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tgoto retry\n\t}\n\n\treturn ai, err\n}\n\nfunc (k *K8sClusterMesh) statusService(ctx context.Context) (*corev1.Service, error) {\n\tw := utils.NewWaitObserver(ctx, utils.WaitParameters{Log: func(err error, wait string) {\n\t\tk.Log(\"\u231b Waiting (%s) for ClusterMesh service to be available: %s\", wait, err)\n\t}})\n\tdefer w.Cancel()\n\nretry:\n\tsvc, err := k.client.GetService(ctx, k.params.Namespace, defaults.ClusterMeshServiceName, metav1.GetOptions{})\n\tif err != nil {\n\t\tif k.params.Wait {\n\t\t\tif err := w.Retry(err); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tgoto retry\n\t\t}\n\n\t\treturn nil, fmt.Errorf(\"clustermesh-apiserver cannot be found: %w\", err)\n\t}\n\n\treturn svc, nil\n}\n\nfunc (k *K8sClusterMesh) waitForDeployment(ctx context.Context) error {\n\tk.Log(\"\u231b [%s] Waiting for deployment %s to become ready...\", k.client.ClusterName(), defaults.ClusterMeshDeploymentName)\n\n\tfor k.client.CheckDeploymentStatus(ctx, k.params.Namespace, defaults.ClusterMeshDeploymentName) != nil {\n\t\tselect {\n\t\tcase <-time.After(time.Second):\n\t\tcase <-ctx.Done():\n\t\t\treturn fmt.Errorf(\"waiting for deployment %s to become ready has been interrupted: %w\", defaults.ClusterMeshDeploymentName, ctx.Err())\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype StatisticalStatus struct {\n\tMin int64   `json:\"min,omitempty\"`\n\tAvg float64 `json:\"avg,omitempty\"`\n\tMax int64   `json:\"max,omitempty\"`\n}\n\ntype ClusterStats struct {\n\tConfigured int `json:\"configured,omitempty\"`\n\tConnected  int `json:\"connected,omitempty\"`\n}\n\ntype ConnectivityStatus struct {\n\tGlobalServices StatisticalStatus        `json:\"global_services,omitempty\"`\n\tConnected      StatisticalStatus        `json:\"connected,omitempty\"`\n\tClusters       map[string]*ClusterStats `json:\"clusters,omitempty\"`\n\tTotal          int64                    `json:\"total,omitempty\"`\n\tNotReady       int64                    `json:\"not_ready,omitempty\"`\n\tErrors         status.ErrorCountMapMap  `json:\"errors,omitempty\"`\n}\n\nfunc (c *ConnectivityStatus) addError(pod, cluster string, err error) {\n\tm := c.Errors[pod]\n\tif m == nil {\n\t\tm = status.ErrorCountMap{}\n\t\tc.Errors[pod] = m\n\t}\n\n\tif m[cluster] == nil {\n\t\tm[cluster] = &status.ErrorCount{}\n\t}\n\n\tm[cluster].Errors = append(m[cluster].Errors, err)\n}\n\nfunc (c *ConnectivityStatus) parseAgentStatus(name string, s *status.ClusterMeshAgentConnectivityStatus) {\n\tif c.GlobalServices.Min < 0 || c.GlobalServices.Min > s.GlobalServices {\n\t\tc.GlobalServices.Min = s.GlobalServices\n\t}\n\n\tif c.GlobalServices.Max < s.GlobalServices {\n\t\tc.GlobalServices.Max = s.GlobalServices\n\t}\n\n\tc.GlobalServices.Avg += float64(s.GlobalServices)\n\tc.Total++\n\n\tready := int64(0)\n\tfor _, cluster := range s.Clusters {\n\t\tstats, ok := c.Clusters[cluster.Name]\n\t\tif !ok {\n\t\t\tstats = &ClusterStats{}\n\t\t\tc.Clusters[cluster.Name] = stats\n\t\t}\n\n\t\tstats.Configured++\n\n\t\tif cluster.Ready {\n\t\t\tready++\n\t\t\tstats.Connected++\n\t\t} else {\n\t\t\tc.addError(name, cluster.Name, fmt.Errorf(\"cluster is not ready: %s\", cluster.Status))\n\t\t}\n\t}\n\n\tif ready != int64(len(s.Clusters)) {\n\t\tc.NotReady++\n\t}\n\n\tif c.Connected.Min < 0 || c.Connected.Min > ready {\n\t\tc.Connected.Min = ready\n\t}\n\n\tif c.Connected.Max < ready {\n\t\tc.Connected.Max = ready\n\t}\n\n\tc.Connected.Avg += float64(ready)\n}\n\nfunc (k *K8sClusterMesh) statusConnectivity(ctx context.Context) (*ConnectivityStatus, error) {\n\tw := utils.NewWaitObserver(ctx, utils.WaitParameters{Log: func(err error, wait string) {\n\t\tk.Log(\"\u231b Waiting (%s) for clusters to be connected: %s\", wait, err)\n\t}})\n\tdefer w.Cancel()\n\nretry:\n\tstatus, err := k.determineStatusConnectivity(ctx)\n\tif k.params.Wait {\n\t\tif err == nil {\n\t\t\tif status.NotReady > 0 {\n\t\t\t\terr = fmt.Errorf(\"%d clusters not ready\", status.NotReady)\n\t\t\t}\n\t\t\tif len(status.Errors) > 0 {\n\t\t\t\terr = fmt.Errorf(\"%d clusters have errors\", len(status.Errors))\n\t\t\t}\n\t\t}\n\n\t\tif err != nil {\n\t\t\tif err := w.Retry(err); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tgoto retry\n\t\t}\n\t}\n\n\treturn status, err\n}\n\nfunc (k *K8sClusterMesh) determineStatusConnectivity(ctx context.Context) (*ConnectivityStatus, error) {\n\tstats := &ConnectivityStatus{\n\t\tGlobalServices: StatisticalStatus{Min: -1},\n\t\tConnected:      StatisticalStatus{Min: -1},\n\t\tErrors:         status.ErrorCountMapMap{},\n\t\tClusters:       map[string]*ClusterStats{},\n\t}\n\n\tpods, err := k.client.ListPods(ctx, k.params.Namespace, metav1.ListOptions{LabelSelector: defaults.AgentPodSelector})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to list cilium pods: %w\", err)\n\t}\n\n\tfor _, pod := range pods.Items {\n\t\ts, err := k.statusCollector.ClusterMeshConnectivity(ctx, pod.Name)\n\t\tif err != nil {\n\t\t\tif errors.Is(err, status.ErrClusterMeshStatusNotAvailable) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, fmt.Errorf(\"unable to determine status of cilium pod %q: %w\", pod.Name, err)\n\t\t}\n\n\t\tstats.parseAgentStatus(pod.Name, s)\n\t}\n\n\tif len(pods.Items) > 0 {\n\t\tstats.GlobalServices.Avg /= float64(len(pods.Items))\n\t\tstats.Connected.Avg /= float64(len(pods.Items))\n\t}\n\n\treturn stats, nil\n}\n\nfunc (k *K8sClusterMesh) Status(ctx context.Context) (*Status, error) {\n\terr := k.GetClusterConfig(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcollector, err := status.NewK8sStatusCollector(k.client, status.K8sStatusParameters{\n\t\tNamespace: k.params.Namespace,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to create client to collect status: %w\", err)\n\t}\n\n\tk.statusCollector = collector\n\n\tctx, cancel := context.WithTimeout(ctx, k.params.waitTimeout())\n\tdefer cancel()\n\n\ts := &Status{}\n\ts.AccessInformation, err = k.statusAccessInformation(ctx, true, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tk.Log(\"\u2705 Cluster access information is available:\")\n\tfor _, ip := range s.AccessInformation.ServiceIPs {\n\t\tk.Log(\"  - %s:%d\", ip, s.AccessInformation.ServicePort)\n\t}\n\n\ts.Service, err = k.statusService(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tk.Log(\"\u2705 Service %q of type %q found\", defaults.ClusterMeshServiceName, s.Service.Spec.Type)\n\n\tif s.Service.Spec.Type == corev1.ServiceTypeLoadBalancer {\n\t\tif len(s.AccessInformation.ServiceIPs) == 0 {\n\t\t\tk.Log(\"\u274c Service is of type LoadBalancer but has no IPs assigned\")\n\t\t\treturn nil, fmt.Errorf(\"no IP available to reach cluster\")\n\t\t}\n\t}\n\n\tif k.params.Wait {\n\t\terr = k.waitForDeployment(ctx)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\ts.Connectivity, err = k.statusConnectivity(ctx)\n\n\tif k.params.Output == status.OutputJSON {\n\t\tjsonStatus, err := json.MarshalIndent(s, \"\", \" \")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to marshal status to JSON\")\n\t\t}\n\t\tfmt.Println(string(jsonStatus))\n\t\treturn s, nil\n\t}\n\n\tif s.Connectivity != nil {\n\t\tif s.Connectivity.NotReady > 0 {\n\t\t\tk.Log(\"\u26a0\ufe0f  %d/%d nodes are not connected to all clusters [min:%d / avg:%.1f / max:%d]\",\n\t\t\t\ts.Connectivity.NotReady,\n\t\t\t\ts.Connectivity.Total,\n\t\t\t\ts.Connectivity.Connected.Min,\n\t\t\t\ts.Connectivity.Connected.Avg,\n\t\t\t\ts.Connectivity.Connected.Max)\n\t\t} else if len(s.Connectivity.Clusters) > 0 {\n\t\t\tk.Log(\"\u2705 All %d nodes are connected to all clusters [min:%d / avg:%.1f / max:%d]\",\n\t\t\t\ts.Connectivity.Total,\n\t\t\t\ts.Connectivity.Connected.Min,\n\t\t\t\ts.Connectivity.Connected.Avg,\n\t\t\t\ts.Connectivity.Connected.Max)\n\t\t}\n\n\t\tk.Log(\"\ud83d\udd0c Cluster Connections:\")\n\t\tfor cluster, stats := range s.Connectivity.Clusters {\n\t\t\tk.Log(\"- %s: %d/%d configured, %d/%d connected\",\n\t\t\t\tcluster, stats.Configured, s.Connectivity.Total,\n\t\t\t\tstats.Connected, s.Connectivity.Total)\n\t\t}\n\n\t\tk.Log(\"\ud83d\udd00 Global services: [ min:%d / avg:%.1f / max:%d ]\",\n\t\t\ts.Connectivity.GlobalServices.Min,\n\t\t\ts.Connectivity.GlobalServices.Avg,\n\t\t\ts.Connectivity.GlobalServices.Max)\n\n\t\tif len(s.Connectivity.Errors) > 0 {\n\t\t\tk.Log(\"\u274c %d Errors:\", len(s.Connectivity.Errors))\n\n\t\t\tfor podName, clusters := range s.Connectivity.Errors {\n\t\t\t\tfor clusterName, a := range clusters {\n\t\t\t\t\tfor _, err := range a.Errors {\n\t\t\t\t\t\tk.Log(\"\u274c %s is not connected to cluster %s: %s\", podName, clusterName, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s, nil\n}\n\nfunc (k *K8sClusterMesh) CreateExternalWorkload(ctx context.Context, names []string) error {\n\tcount := 0\n\tfor _, name := range names {\n\t\tcew := &ciliumv2.CiliumExternalWorkload{\n\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\tName:        name,\n\t\t\t\tLabels:      k.params.Labels,\n\t\t\t\tAnnotations: map[string]string{},\n\t\t\t},\n\t\t\tSpec: ciliumv2.CiliumExternalWorkloadSpec{\n\t\t\t\tIPv4AllocCIDR: k.params.IPv4AllocCIDR,\n\t\t\t\tIPv6AllocCIDR: k.params.IPv6AllocCIDR,\n\t\t\t},\n\t\t}\n\n\t\t_, err := k.client.CreateCiliumExternalWorkload(ctx, cew, metav1.CreateOptions{})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcount++\n\t}\n\tk.Log(\"\u2705 Added %d external workload resources.\", count)\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) DeleteExternalWorkload(ctx context.Context, names []string) error {\n\tvar errs []string\n\tcount := 0\n\n\tif len(names) == 0 && k.params.All {\n\t\tcewList, err := k.client.ListCiliumExternalWorkloads(ctx, metav1.ListOptions{})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, cew := range cewList.Items {\n\t\t\tnames = append(names, cew.Name)\n\t\t}\n\t}\n\tfor _, name := range names {\n\t\terr := k.client.DeleteCiliumExternalWorkload(ctx, name, metav1.DeleteOptions{})\n\t\tif err != nil {\n\t\t\terrs = append(errs, err.Error())\n\t\t} else {\n\t\t\tcount++\n\t\t}\n\t}\n\tif count > 0 {\n\t\tk.Log(\"\u2705 Removed %d external workload resources.\", count)\n\t} else {\n\t\tk.Log(\"\u2139\ufe0f  No external workload resources to remove.\")\n\t}\n\tif len(errs) > 0 {\n\t\treturn errors.New(strings.Join(errs, \", \"))\n\t}\n\treturn nil\n}\n\nvar installScriptFmt = `#!/bin/bash\nCILIUM_IMAGE=${1:-%[1]s}\nCLUSTER_ADDR=${2:-%[2]s}\nCONFIG_OVERWRITES=${3:-%[3]s}\n\nset -e\nshopt -s extglob\n\n# Run without sudo if not available (e.g., running as root)\nSUDO=\nif [ ! \"$(whoami)\" = \"root\" ] ; then\n    SUDO=sudo\nfi\n\nif [ \"$1\" = \"uninstall\" ] ; then\n    if [ -n \"$(${SUDO} docker ps -a -q -f name=cilium)\" ]; then\n        echo \"Shutting down running Cilium agent\"\n        ${SUDO} docker rm -f cilium || true\n    fi\n    if [ -f /usr/bin/cilium ] ; then\n        echo \"Removing /usr/bin/cilium\"\n        ${SUDO} rm /usr/bin/cilium\n    fi\n    pushd /etc\n    if [ -f resolv.conf.orig ] ; then\n        echo \"Restoring /etc/resolv.conf\"\n        ${SUDO} mv -f resolv.conf.orig resolv.conf\n    elif [ -f resolv.conf.link ] && [ -f $(cat resolv.conf.link) ] ; then\n        echo \"Restoring systemd resolved config...\"\n        if [ -f /usr/lib/systemd/resolved.conf.d/cilium-kube-dns.conf ] ; then\n\t    ${SUDO} rm /usr/lib/systemd/resolved.conf.d/cilium-kube-dns.conf\n        fi\n        ${SUDO} systemctl daemon-reload\n        ${SUDO} systemctl reenable systemd-resolved.service\n        ${SUDO} service systemd-resolved restart\n        ${SUDO} ln -fs $(cat resolv.conf.link) resolv.conf\n        ${SUDO} rm resolv.conf.link\n    fi\n    popd\n    exit 0\nfi\n\nif [ -z \"$CLUSTER_ADDR\" ] ; then\n    echo \"CLUSTER_ADDR must be defined to the IP:PORT at which the clustermesh-apiserver is reachable.\"\n    exit 1\nfi\n\nport='@(6553[0-5]|655[0-2][0-9]|65[0-4][0-9][0-9]|6[0-4][0-9][0-9][0-9]|[1-5][0-9][0-9][0-9][0-9]|[1-9][0-9][0-9][0-9]|[1-9][0-9][0-9]|[1-9][0-9]|[1-9])'\nbyte='@(25[0-5]|2[0-4][0-9]|[1][0-9][0-9]|[1-9][0-9]|[0-9])'\nipv4=\"$byte\\.$byte\\.$byte\\.$byte\"\n\n# Default port is for a HostPort service\ncase \"$CLUSTER_ADDR\" in\n    \\[+([0-9a-fA-F:])\\]:$port)\n\tCLUSTER_PORT=${CLUSTER_ADDR##\\[*\\]:}\n\tCLUSTER_IP=${CLUSTER_ADDR#\\[}\n\tCLUSTER_IP=${CLUSTER_IP%%\\]:*}\n\t;;\n    [^[]$ipv4:$port)\n\tCLUSTER_PORT=${CLUSTER_ADDR##*:}\n\tCLUSTER_IP=${CLUSTER_ADDR%%:*}\n\t;;\n    *:*)\n\techo \"Malformed CLUSTER_ADDR: $CLUSTER_ADDR\"\n\texit 1\n\t;;\n    *)\n\tCLUSTER_PORT=2379\n\tCLUSTER_IP=$CLUSTER_ADDR\n\t;;\nesac\n\n${SUDO} mkdir -p /var/lib/cilium/etcd\n${SUDO} tee /var/lib/cilium/etcd/ca.crt <<EOF >/dev/null\n%[4]sEOF\n${SUDO} tee /var/lib/cilium/etcd/tls.crt <<EOF >/dev/null\n%[5]sEOF\n${SUDO} tee /var/lib/cilium/etcd/tls.key <<EOF >/dev/null\n%[6]sEOF\n${SUDO} tee /var/lib/cilium/etcd/config.yaml <<EOF >/dev/null\n---\ntrusted-ca-file: /var/lib/cilium/etcd/ca.crt\ncert-file: /var/lib/cilium/etcd/tls.crt\nkey-file: /var/lib/cilium/etcd/tls.key\nendpoints:\n- https://clustermesh-apiserver.cilium.io:$CLUSTER_PORT\nEOF\n\nCILIUM_OPTS=\" --join-cluster %[8]s --enable-endpoint-health-checking=false\"\nCILIUM_OPTS+=\" --kvstore etcd --kvstore-opt etcd.config=/var/lib/cilium/etcd/config.yaml\"\nif [ -n \"$HOST_IP\" ] ; then\n    CILIUM_OPTS+=\" --ipv4-node $HOST_IP\"\nfi\nif [ -n \"$CONFIG_OVERWRITES\" ] ; then\n    CILIUM_OPTS+=\" $CONFIG_OVERWRITES\"\nfi\n\nDOCKER_OPTS=\" -d --log-driver local --restart always\"\nDOCKER_OPTS+=\" --privileged --network host --cap-add NET_ADMIN --cap-add SYS_MODULE\"\n# Run cilium agent in the host's cgroup namespace so that\n# socket-based load balancing works as expected.\n# See https://github.com/cilium/cilium/pull/16259 for more details.\nDOCKER_OPTS+=\" --cgroupns=host\"\nDOCKER_OPTS+=\" --volume /var/lib/cilium/etcd:/var/lib/cilium/etcd\"\nDOCKER_OPTS+=\" --volume /var/run/cilium:/var/run/cilium\"\nDOCKER_OPTS+=\" --volume /boot:/boot\"\nDOCKER_OPTS+=\" --volume /lib/modules:/lib/modules\"\nDOCKER_OPTS+=\" --volume /sys/fs/bpf:/sys/fs/bpf\"\nDOCKER_OPTS+=\" --volume /run/xtables.lock:/run/xtables.lock\"\nDOCKER_OPTS+=\" --add-host clustermesh-apiserver.cilium.io:$CLUSTER_IP\"\n\ncilium_started=false\nretries=%[7]s\nwhile [ $cilium_started = false ]; do\n    if [ -n \"$(${SUDO} docker ps -a -q -f name=cilium)\" ]; then\n        echo \"Shutting down running Cilium agent\"\n        ${SUDO} docker rm -f cilium || true\n    fi\n\n    echo \"Launching Cilium agent $CILIUM_IMAGE...\"\n    ${SUDO} docker run --name cilium $DOCKER_OPTS $CILIUM_IMAGE cilium-agent $CILIUM_OPTS\n\n    # Copy Cilium CLI\n    ${SUDO} docker cp cilium:/usr/bin/cilium /usr/bin/cilium\n\n    # Wait for cilium agent to become available\n    for ((i = 0 ; i < 12; i++)); do\n        if ${SUDO} cilium status --brief > /dev/null 2>&1; then\n            cilium_started=true\n            break\n        fi\n        sleep 5s\n        echo \"Waiting for Cilium daemon to come up...\"\n    done\n\n    echo \"Cilium status:\"\n    ${SUDO} cilium status || true\n\n    if [ \"$cilium_started\" = true ] ; then\n        echo 'Cilium successfully started!'\n    else\n        if [ $retries -eq 0 ]; then\n            >&2 echo 'Timeout waiting for Cilium to start, retries exhausted.'\n            exit 1\n        fi\n        ((retries--))\n        echo \"Restarting Cilium...\"\n    fi\ndone\n\n# Wait for kube-dns service to become available\nkubedns=\"\"\nfor ((i = 0 ; i < 24; i++)); do\n    kubedns=$(${SUDO} cilium service list get -o jsonpath='{[?(@.spec.frontend-address.port==53)].spec.frontend-address.ip}')\n    if [ -n \"$kubedns\" ] ; then\n        break\n    fi\n    sleep 5s\n    echo \"Waiting for kube-dns service to come available...\"\ndone\n\nnamespace=$(${SUDO} cilium endpoint get -l reserved:host -o jsonpath='{$[0].status.identity.labels}' | tr -d \"[]\\\"\" | tr \",\" \"\\n\" | grep io.kubernetes.pod.namespace | cut -d= -f2)\n\nif [ -n \"$kubedns\" ] ; then\n    if grep \"nameserver $kubedns\" /etc/resolv.conf ; then\n\techo \"kube-dns IP $kubedns already in /etc/resolv.conf\"\n    else\n\tlinkval=$(readlink /etc/resolv.conf) && echo \"$linkval\" | ${SUDO} tee /etc/resolv.conf.link || true\n\tif [[ \"$linkval\" == *\"/systemd/\"* ]] ; then\n\t    echo \"updating systemd resolved with kube-dns IP $kubedns\"\n\t    ${SUDO} mkdir -p /usr/lib/systemd/resolved.conf.d\n\t    ${SUDO} tee /usr/lib/systemd/resolved.conf.d/cilium-kube-dns.conf <<EOF >/dev/null\n# This file is installed by Cilium to use kube dns server from a non-k8s node.\n[Resolve]\nDNS=$kubedns\nDomains=${namespace}.svc.cluster.local svc.cluster.local cluster.local\nEOF\n\t    ${SUDO} systemctl daemon-reload\n\t    ${SUDO} systemctl reenable systemd-resolved.service\n\t    ${SUDO} service systemd-resolved restart\n\t    ${SUDO} ln -fs /run/systemd/resolve/resolv.conf /etc/resolv.conf\n\telse\n\t    echo \"Adding kube-dns IP $kubedns to /etc/resolv.conf\"\n\t    ${SUDO} cp /etc/resolv.conf /etc/resolv.conf.orig\n\t    resolvconf=\"nameserver $kubedns\\n$(cat /etc/resolv.conf)\\nsearch ${namespace}.svc.cluster.local svc.cluster.local cluster.local\\n\"\n\t    printf \"$resolvconf\" | ${SUDO} tee /etc/resolv.conf\n\tfi\n    fi\nelse\n    >&2 echo \"kube-dns not found.\"\n    exit 1\nfi\n`\n\nfunc (k *K8sClusterMesh) WriteExternalWorkloadInstallScript(ctx context.Context, writer io.Writer) error {\n\tdaemonSet, err := k.client.GetDaemonSet(ctx, k.params.Namespace, defaults.AgentDaemonSetName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn err\n\t}\n\tif daemonSet == nil {\n\t\treturn fmt.Errorf(\"DaemonSet %s is not available\", defaults.AgentDaemonSetName)\n\t}\n\tk.Log(\"\u2705 Using image from Cilium DaemonSet: %s\", daemonSet.Spec.Template.Spec.Containers[0].Image)\n\n\tai, err := k.statusAccessInformation(ctx, false, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif ai.Tunnel != \"\" && ai.Tunnel != \"vxlan\" {\n\t\treturn fmt.Errorf(\"datapath not using vxlan, please install Cilium with '--config tunnel=vxlan'\")\n\t}\n\n\tclusterAddr := fmt.Sprintf(\"%s:%d\", ai.ServiceIPs[0], ai.ServicePort)\n\tk.Log(\"\u2705 Using clustermesh-apiserver service address: %s\", clusterAddr)\n\n\tconfigOverwrites := \"\"\n\tif len(k.params.ConfigOverwrites) > 0 {\n\t\tfor i, opt := range k.params.ConfigOverwrites {\n\t\t\tif !strings.HasPrefix(opt, \"--\") {\n\t\t\t\tk.params.ConfigOverwrites[i] = \"--\" + opt\n\t\t\t}\n\t\t}\n\t\tconfigOverwrites = strings.Join(k.params.ConfigOverwrites, \" \")\n\t}\n\n\tif k.params.Retries <= 0 {\n\t\tk.params.Retries = 1\n\t}\n\n\tvar ciliumVer semver.Version\n\n\thelmState, err := k.client.GetHelmState(ctx, k.params.Namespace, k.params.HelmValuesSecretName)\n\tif err != nil {\n\t\t// Try to retrieve version from image tag\n\t\tv, err := k.client.GetRunningCiliumVersion(ctx, k.params.Namespace)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tciliumVer, err = utils.ParseCiliumVersion(v)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to parse Cilium version %s: %w\", v, err)\n\t\t}\n\n\t} else {\n\t\tciliumVer = helmState.Version\n\t}\n\n\tsockLBOpt := \"--bpf-lb-sock\"\n\tif ciliumVer.LT(versioncheck.MustVersion(\"1.12.0\")) {\n\t\t// Before 1.12, the socket LB was enabled via --enable-host-reachable-services flag\n\t\tsockLBOpt = \"--enable-host-reachable-services\"\n\t}\n\n\tfmt.Fprintf(writer, installScriptFmt,\n\t\tdaemonSet.Spec.Template.Spec.Containers[0].Image, clusterAddr,\n\t\tconfigOverwrites,\n\t\tstring(ai.CA), string(ai.ExternalWorkloadCert), string(ai.ExternalWorkloadKey),\n\t\tstrconv.Itoa(k.params.Retries), sockLBOpt)\n\treturn nil\n}\n\nfunc formatCEW(cew ciliumv2.CiliumExternalWorkload) string {\n\tvar items []string\n\tip := cew.Status.IP\n\tif ip == \"\" {\n\t\tip = \"N/A\"\n\t}\n\titems = append(items, fmt.Sprintf(\"IP: %s\", ip))\n\tvar labels []string\n\tfor key, value := range cew.Labels {\n\t\tlabels = append(labels, fmt.Sprintf(\"%s=%s\", key, value))\n\t}\n\titems = append(items, fmt.Sprintf(\"Labels: %s\", strings.Join(labels, \",\")))\n\treturn strings.Join(items, \", \")\n}\n\nfunc (k *K8sClusterMesh) ExternalWorkloadStatus(ctx context.Context, names []string) error {\n\tcollector, err := status.NewK8sStatusCollector(k.client, status.K8sStatusParameters{\n\t\tNamespace: k.params.Namespace,\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create client to collect status: %w\", err)\n\t}\n\n\tk.statusCollector = collector\n\n\tctx, cancel := context.WithTimeout(ctx, k.params.waitTimeout())\n\tdefer cancel()\n\n\tai, err := k.statusAccessInformation(ctx, true, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 Cluster access information is available:\")\n\tfor _, ip := range ai.ServiceIPs {\n\t\tk.Log(\"\t - %s:%d\", ip, ai.ServicePort)\n\t}\n\n\tsvc, err := k.statusService(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 Service %q of type %q found\", defaults.ClusterMeshServiceName, svc.Spec.Type)\n\n\tif svc.Spec.Type == corev1.ServiceTypeLoadBalancer {\n\t\tif len(ai.ServiceIPs) == 0 {\n\t\t\tk.Log(\"\u274c Service is of type LoadBalancer but has no IPs assigned\")\n\t\t\treturn fmt.Errorf(\"no IP available to reach cluster\")\n\t\t}\n\t}\n\tvar cews []ciliumv2.CiliumExternalWorkload\n\n\tif len(names) == 0 {\n\t\tcewList, err := k.client.ListCiliumExternalWorkloads(ctx, metav1.ListOptions{})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcews = cewList.Items\n\t\tif len(cews) == 0 {\n\t\t\tk.Log(\"\u26a0\ufe0f  No external workloads found.\")\n\t\t\treturn nil\n\t\t}\n\t} else {\n\t\tfor _, name := range names {\n\t\t\tcew, err := k.client.GetCiliumExternalWorkload(ctx, name, metav1.GetOptions{})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcews = append(cews, *cew)\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\tw := tabwriter.NewWriter(&buf, 0, 0, 4, ' ', 0)\n\n\theader := \"External Workloads\"\n\tfor _, cew := range cews {\n\t\tfmt.Fprintf(w, \"%s\\t%s\\t%s\\n\", header, cew.Name, formatCEW(cew))\n\t\theader = \"\"\n\t}\n\n\tw.Flush()\n\tfmt.Println(buf.String())\n\treturn err\n}\n"], "fixing_code": ["// SPDX-License-Identifier: Apache-2.0\n// Copyright Authors of Cilium\n\npackage clustermesh\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"text/tabwriter\"\n\t\"time\"\n\n\tappsv1 \"k8s.io/api/apps/v1\"\n\tcorev1 \"k8s.io/api/core/v1\"\n\trbacv1 \"k8s.io/api/rbac/v1\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/types\"\n\t\"k8s.io/apimachinery/pkg/util/intstr\"\n\n\t\"github.com/blang/semver/v4\"\n\t\"github.com/cilium/cilium/api/v1/models\"\n\tciliumv2 \"github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2\"\n\t\"github.com/cilium/cilium/pkg/versioncheck\"\n\n\t\"github.com/cilium/cilium-cli/defaults\"\n\t\"github.com/cilium/cilium-cli/internal/certs\"\n\t\"github.com/cilium/cilium-cli/internal/helm\"\n\t\"github.com/cilium/cilium-cli/internal/utils\"\n\t\"github.com/cilium/cilium-cli/k8s\"\n\t\"github.com/cilium/cilium-cli/status\"\n)\n\nconst (\n\tconfigNameClusterID   = \"cluster-id\"\n\tconfigNameClusterName = \"cluster-name\"\n\tconfigNameTunnel      = \"tunnel\"\n\n\tcaSuffix   = \".etcd-client-ca.crt\"\n\tkeySuffix  = \".etcd-client.key\"\n\tcertSuffix = \".etcd-client.crt\"\n)\n\nvar (\n\treplicas                 = int32(1)\n\tdeploymentMaxSurge       = intstr.FromInt(1)\n\tdeploymentMaxUnavailable = intstr.FromInt(1)\n\tsecretDefaultMode        = int32(0400)\n)\n\nvar clusterRole = &rbacv1.ClusterRole{\n\tObjectMeta: metav1.ObjectMeta{\n\t\tName: defaults.ClusterMeshClusterRoleName,\n\t},\n\tRules: []rbacv1.PolicyRule{\n\t\t{\n\t\t\tAPIGroups: []string{\"discovery.k8s.io\"},\n\t\t\tResources: []string{\"endpointslices\"},\n\t\t\tVerbs:     []string{\"get\", \"list\", \"watch\"},\n\t\t},\n\t\t{\n\t\t\tAPIGroups: []string{\"\"},\n\t\t\tResources: []string{\"namespaces\", \"services\", \"endpoints\"},\n\t\t\tVerbs:     []string{\"get\", \"list\", \"watch\"},\n\t\t},\n\t\t{\n\t\t\tAPIGroups: []string{\"apiextensions.k8s.io\"},\n\t\t\tResources: []string{\"customresourcedefinitions\"},\n\t\t\tVerbs:     []string{\"list\"},\n\t\t},\n\t\t{\n\t\t\tAPIGroups: []string{\"cilium.io\"},\n\t\t\tResources: []string{\n\t\t\t\t\"ciliumnodes\",\n\t\t\t\t\"ciliumnodes/status\",\n\t\t\t\t\"ciliumexternalworkloads\",\n\t\t\t\t\"ciliumexternalworkloads/status\",\n\t\t\t\t\"ciliumidentities\",\n\t\t\t\t\"ciliumidentities/status\",\n\t\t\t\t\"ciliumendpoints\",\n\t\t\t\t\"ciliumendpoints/status\",\n\t\t\t},\n\t\t\tVerbs: []string{\"*\"},\n\t\t},\n\t},\n}\n\nfunc (k *K8sClusterMesh) generateService() (*corev1.Service, error) {\n\tsvc := &corev1.Service{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:        defaults.ClusterMeshServiceName,\n\t\t\tLabels:      defaults.ClusterMeshDeploymentLabels,\n\t\t\tAnnotations: map[string]string{},\n\t\t},\n\t\tSpec: corev1.ServiceSpec{\n\t\t\tType: corev1.ServiceTypeClusterIP,\n\t\t\tPorts: []corev1.ServicePort{\n\t\t\t\t{Port: int32(2379)},\n\t\t\t},\n\t\t\tSelector: defaults.ClusterMeshDeploymentLabels,\n\t\t},\n\t}\n\n\tif k.params.ServiceType != \"\" {\n\t\tif k.params.ServiceType == \"NodePort\" {\n\t\t\tk.Log(\"\u26a0\ufe0f  Using service type NodePort may fail when nodes are removed from the cluster!\")\n\t\t}\n\t\tsvc.Spec.Type = corev1.ServiceType(k.params.ServiceType)\n\t} else {\n\t\tswitch k.flavor.Kind {\n\t\tcase k8s.KindGKE:\n\t\t\tk.Log(\"\ud83d\udd2e Auto-exposing service within GCP VPC (cloud.google.com/load-balancer-type=Internal)\")\n\t\t\tsvc.Spec.Type = corev1.ServiceTypeLoadBalancer\n\t\t\tsvc.ObjectMeta.Annotations[\"cloud.google.com/load-balancer-type\"] = \"Internal\"\n\t\t\t// if all the clusters are in the same region the next annotation can be removed\n\t\t\tsvc.ObjectMeta.Annotations[\"networking.gke.io/internal-load-balancer-allow-global-access\"] = \"true\"\n\t\tcase k8s.KindAKS:\n\t\t\tk.Log(\"\ud83d\udd2e Auto-exposing service within Azure VPC (service.beta.kubernetes.io/azure-load-balancer-internal)\")\n\t\t\tsvc.Spec.Type = corev1.ServiceTypeLoadBalancer\n\t\t\tsvc.ObjectMeta.Annotations[\"service.beta.kubernetes.io/azure-load-balancer-internal\"] = \"true\"\n\t\tcase k8s.KindEKS:\n\t\t\tk.Log(\"\ud83d\udd2e Auto-exposing service within AWS VPC (service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\")\n\t\t\tsvc.Spec.Type = corev1.ServiceTypeLoadBalancer\n\t\t\tsvc.ObjectMeta.Annotations[\"service.beta.kubernetes.io/aws-load-balancer-internal\"] = \"0.0.0.0/0\"\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"cannot auto-detect service type, please specify using '--service-type' option\")\n\t\t}\n\t}\n\n\treturn svc, nil\n}\n\nvar initContainerArgs = []string{`rm -rf /var/run/etcd/*;\nexport ETCDCTL_API=3;\n/usr/local/bin/etcd --data-dir=/var/run/etcd --name=clustermesh-apiserver --listen-client-urls=http://127.0.0.1:2379 --advertise-client-urls=http://127.0.0.1:2379 --initial-cluster-token=clustermesh-apiserver --initial-cluster-state=new --auto-compaction-retention=1 &\nexport rootpw=\"$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16)\";\necho $rootpw | etcdctl --interactive=false user add root;\netcdctl user grant-role root root;\nexport vmpw=\"$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16)\";\necho $vmpw | etcdctl --interactive=false user add externalworkload;\netcdctl role add externalworkload;\netcdctl role grant-permission externalworkload --from-key read '';\netcdctl role grant-permission externalworkload readwrite --prefix cilium/state/noderegister/v1/;\netcdctl role grant-permission externalworkload readwrite --prefix cilium/.initlock/;\netcdctl user grant-role externalworkload externalworkload;\nexport remotepw=\"$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16)\";\necho $remotepw | etcdctl --interactive=false user add remote;\netcdctl role add remote;\netcdctl role grant-permission remote --from-key read '';\netcdctl user grant-role remote remote;\netcdctl auth enable;\nexit`}\n\nfunc (k *K8sClusterMesh) apiserverImage(imagePathMode utils.ImagePathMode) string {\n\treturn utils.BuildImagePath(k.params.ApiserverImage, k.params.ApiserverVersion, defaults.ClusterMeshApiserverImage, k.imageVersion, imagePathMode)\n}\n\nfunc (k *K8sClusterMesh) etcdImage() string {\n\tetcdVersion := \"v3.5.4\"\n\tif k.clusterArch == \"amd64\" {\n\t\treturn \"quay.io/coreos/etcd:\" + etcdVersion\n\t}\n\treturn \"quay.io/coreos/etcd:\" + etcdVersion + \"-\" + k.clusterArch\n}\n\nfunc (k *K8sClusterMesh) etcdEnvs() []corev1.EnvVar {\n\tenvs := []corev1.EnvVar{\n\t\t{\n\t\t\tName:  \"ETCDCTL_API\",\n\t\t\tValue: \"3\",\n\t\t},\n\t\t{\n\t\t\tName: \"HOSTNAME_IP\",\n\t\t\tValueFrom: &corev1.EnvVarSource{\n\t\t\t\tFieldRef: &corev1.ObjectFieldSelector{\n\t\t\t\t\tFieldPath: \"status.podIP\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\tif k.clusterArch == \"arm64\" {\n\t\tenvs = append(envs, corev1.EnvVar{\n\t\t\tName:  \"ETCD_UNSUPPORTED_ARCH\",\n\t\t\tValue: \"arm64\",\n\t\t})\n\t}\n\treturn envs\n}\n\nfunc (k *K8sClusterMesh) generateDeployment(clustermeshApiserverArgs []string) *appsv1.Deployment {\n\n\tdeployment := &appsv1.Deployment{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:   defaults.ClusterMeshDeploymentName,\n\t\t\tLabels: defaults.ClusterMeshDeploymentLabels,\n\t\t},\n\t\tSpec: appsv1.DeploymentSpec{\n\t\t\tReplicas: &replicas,\n\t\t\tSelector: &metav1.LabelSelector{\n\t\t\t\tMatchLabels: defaults.ClusterMeshDeploymentLabels,\n\t\t\t},\n\t\t\tStrategy: appsv1.DeploymentStrategy{\n\t\t\t\tType: appsv1.RollingUpdateDeploymentStrategyType,\n\t\t\t\tRollingUpdate: &appsv1.RollingUpdateDeployment{\n\t\t\t\t\tMaxUnavailable: &deploymentMaxUnavailable,\n\t\t\t\t\tMaxSurge:       &deploymentMaxSurge,\n\t\t\t\t},\n\t\t\t},\n\t\t\tTemplate: corev1.PodTemplateSpec{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\t\tName:   defaults.ClusterMeshDeploymentName,\n\t\t\t\t\tLabels: defaults.ClusterMeshDeploymentLabels,\n\t\t\t\t},\n\t\t\t\tSpec: corev1.PodSpec{\n\t\t\t\t\tRestartPolicy:      corev1.RestartPolicyAlways,\n\t\t\t\t\tServiceAccountName: defaults.ClusterMeshServiceAccountName,\n\t\t\t\t\tContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:    \"etcd\",\n\t\t\t\t\t\t\tCommand: []string{\"/usr/local/bin/etcd\"},\n\t\t\t\t\t\t\tArgs: []string{\n\t\t\t\t\t\t\t\t\"--data-dir=/var/run/etcd\",\n\t\t\t\t\t\t\t\t\"--name=clustermesh-apiserver\",\n\t\t\t\t\t\t\t\t\"--client-cert-auth\",\n\t\t\t\t\t\t\t\t\"--trusted-ca-file=/var/lib/etcd-secrets/ca.crt\",\n\t\t\t\t\t\t\t\t\"--cert-file=/var/lib/etcd-secrets/tls.crt\",\n\t\t\t\t\t\t\t\t\"--key-file=/var/lib/etcd-secrets/tls.key\",\n\t\t\t\t\t\t\t\t// Surrounding the IPv4 address with brackets works in this case, since etcd\n\t\t\t\t\t\t\t\t// uses net.SplitHostPort() internally and it accepts that format.\n\t\t\t\t\t\t\t\t\"--listen-client-urls=https://127.0.0.1:2379,https://[$(HOSTNAME_IP)]:2379\",\n\t\t\t\t\t\t\t\t\"--advertise-client-urls=https://[$(HOSTNAME_IP)]:2379\",\n\t\t\t\t\t\t\t\t\"--initial-cluster-token=clustermesh-apiserver\",\n\t\t\t\t\t\t\t\t\"--auto-compaction-retention=1\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tImage:           k.etcdImage(),\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tEnv:             k.etcdEnvs(),\n\t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"etcd-server-secrets\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/var/lib/etcd-secrets\",\n\t\t\t\t\t\t\t\t\tReadOnly:  true,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"etcd-data-dir\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/var/run/etcd\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:    \"apiserver\",\n\t\t\t\t\t\t\tCommand: []string{\"/usr/bin/clustermesh-apiserver\"},\n\t\t\t\t\t\t\tArgs: append(clustermeshApiserverArgs,\n\t\t\t\t\t\t\t\t\"--cluster-name=\"+k.clusterName,\n\t\t\t\t\t\t\t\t\"--cluster-id=\"+k.clusterID,\n\t\t\t\t\t\t\t\t\"--kvstore-opt\",\n\t\t\t\t\t\t\t\t\"etcd.config=/var/lib/cilium/etcd-config.yaml\",\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\tImage:           k.apiserverImage(utils.ImagePathIncludeDigest),\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tEnv: []corev1.EnvVar{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName: \"CILIUM_CLUSTER_NAME\",\n\t\t\t\t\t\t\t\t\tValueFrom: &corev1.EnvVarSource{\n\t\t\t\t\t\t\t\t\t\tConfigMapKeyRef: &corev1.ConfigMapKeySelector{\n\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ConfigMapName,\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\tKey: configNameClusterName,\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName: \"CILIUM_CLUSTER_ID\",\n\t\t\t\t\t\t\t\t\tValueFrom: &corev1.EnvVarSource{\n\t\t\t\t\t\t\t\t\t\tConfigMapKeyRef: &corev1.ConfigMapKeySelector{\n\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ConfigMapName,\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\tKey: configNameClusterID,\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName: \"CILIUM_IDENTITY_ALLOCATION_MODE\",\n\t\t\t\t\t\t\t\t\tValueFrom: &corev1.EnvVarSource{\n\t\t\t\t\t\t\t\t\t\tConfigMapKeyRef: &corev1.ConfigMapKeySelector{\n\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ConfigMapName,\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\tKey: \"identity-allocation-mode\",\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"etcd-admin-client\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/var/lib/cilium/etcd-secrets\",\n\t\t\t\t\t\t\t\t\tReadOnly:  true,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tInitContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:            \"etcd-init\",\n\t\t\t\t\t\t\tCommand:         []string{\"/bin/sh\", \"-c\"},\n\t\t\t\t\t\t\tArgs:            initContainerArgs,\n\t\t\t\t\t\t\tImage:           k.etcdImage(),\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tEnv:             k.etcdEnvs(),\n\t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"etcd-data-dir\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/var/run/etcd\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tVolumes: []corev1.Volume{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"etcd-data-dir\",\n\t\t\t\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\t\t\t\tEmptyDir: &corev1.EmptyDirVolumeSource{},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"etcd-server-secrets\",\n\t\t\t\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\t\t\t\tProjected: &corev1.ProjectedVolumeSource{\n\t\t\t\t\t\t\t\t\tDefaultMode: &secretDefaultMode,\n\t\t\t\t\t\t\t\t\tSources: []corev1.VolumeProjection{\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tSecret: &corev1.SecretProjection{\n\t\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.CASecretName,\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\tItems: []corev1.KeyToPath{\n\t\t\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tKey:  defaults.CASecretCertName,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tPath: \"ca.crt\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tSecret: &corev1.SecretProjection{\n\t\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ClusterMeshServerSecretName,\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"etcd-admin-client\",\n\t\t\t\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\t\t\t\tProjected: &corev1.ProjectedVolumeSource{\n\t\t\t\t\t\t\t\t\tDefaultMode: &secretDefaultMode,\n\t\t\t\t\t\t\t\t\tSources: []corev1.VolumeProjection{\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tSecret: &corev1.SecretProjection{\n\t\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.CASecretName,\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\tItems: []corev1.KeyToPath{\n\t\t\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tKey:  defaults.CASecretCertName,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tPath: \"ca.crt\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tSecret: &corev1.SecretProjection{\n\t\t\t\t\t\t\t\t\t\t\t\tLocalObjectReference: corev1.LocalObjectReference{\n\t\t\t\t\t\t\t\t\t\t\t\t\tName: defaults.ClusterMeshAdminSecretName,\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\treturn deployment\n}\n\ntype k8sClusterMeshImplementation interface {\n\tCreateSecret(ctx context.Context, namespace string, secret *corev1.Secret, opts metav1.CreateOptions) (*corev1.Secret, error)\n\tPatchSecret(ctx context.Context, namespace, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions) (*corev1.Secret, error)\n\tDeleteSecret(ctx context.Context, namespace, name string, opts metav1.DeleteOptions) error\n\tGetSecret(ctx context.Context, namespace, name string, opts metav1.GetOptions) (*corev1.Secret, error)\n\tCreateServiceAccount(ctx context.Context, namespace string, account *corev1.ServiceAccount, opts metav1.CreateOptions) (*corev1.ServiceAccount, error)\n\tDeleteServiceAccount(ctx context.Context, namespace, name string, opts metav1.DeleteOptions) error\n\tCreateClusterRole(ctx context.Context, role *rbacv1.ClusterRole, opts metav1.CreateOptions) (*rbacv1.ClusterRole, error)\n\tDeleteClusterRole(ctx context.Context, name string, opts metav1.DeleteOptions) error\n\tCreateClusterRoleBinding(ctx context.Context, role *rbacv1.ClusterRoleBinding, opts metav1.CreateOptions) (*rbacv1.ClusterRoleBinding, error)\n\tDeleteClusterRoleBinding(ctx context.Context, name string, opts metav1.DeleteOptions) error\n\tGetConfigMap(ctx context.Context, namespace, name string, opts metav1.GetOptions) (*corev1.ConfigMap, error)\n\tCreateDeployment(ctx context.Context, namespace string, deployment *appsv1.Deployment, opts metav1.CreateOptions) (*appsv1.Deployment, error)\n\tGetDeployment(ctx context.Context, namespace, name string, opts metav1.GetOptions) (*appsv1.Deployment, error)\n\tDeleteDeployment(ctx context.Context, namespace, name string, opts metav1.DeleteOptions) error\n\tCheckDeploymentStatus(ctx context.Context, namespace, deployment string) error\n\tCreateService(ctx context.Context, namespace string, service *corev1.Service, opts metav1.CreateOptions) (*corev1.Service, error)\n\tDeleteService(ctx context.Context, namespace, name string, opts metav1.DeleteOptions) error\n\tGetService(ctx context.Context, namespace, name string, opts metav1.GetOptions) (*corev1.Service, error)\n\tPatchDaemonSet(ctx context.Context, namespace, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions) (*appsv1.DaemonSet, error)\n\tGetDaemonSet(ctx context.Context, namespace, name string, options metav1.GetOptions) (*appsv1.DaemonSet, error)\n\tListNodes(ctx context.Context, options metav1.ListOptions) (*corev1.NodeList, error)\n\tListPods(ctx context.Context, namespace string, options metav1.ListOptions) (*corev1.PodList, error)\n\tAutodetectFlavor(ctx context.Context) k8s.Flavor\n\tCiliumStatus(ctx context.Context, namespace, pod string) (*models.StatusResponse, error)\n\tClusterName() string\n\tListCiliumExternalWorkloads(ctx context.Context, opts metav1.ListOptions) (*ciliumv2.CiliumExternalWorkloadList, error)\n\tGetCiliumExternalWorkload(ctx context.Context, name string, opts metav1.GetOptions) (*ciliumv2.CiliumExternalWorkload, error)\n\tCreateCiliumExternalWorkload(ctx context.Context, cew *ciliumv2.CiliumExternalWorkload, opts metav1.CreateOptions) (*ciliumv2.CiliumExternalWorkload, error)\n\tDeleteCiliumExternalWorkload(ctx context.Context, name string, opts metav1.DeleteOptions) error\n\tGetRunningCiliumVersion(ctx context.Context, namespace string) (string, error)\n\tListCiliumEndpoints(ctx context.Context, namespace string, options metav1.ListOptions) (*ciliumv2.CiliumEndpointList, error)\n\tGetPlatform(ctx context.Context) (*k8s.Platform, error)\n\tCiliumLogs(ctx context.Context, namespace, pod string, since time.Time, filter *regexp.Regexp) (string, error)\n\tGetHelmState(ctx context.Context, namespace string, secretName string) (*helm.State, error)\n}\n\ntype K8sClusterMesh struct {\n\tclient          k8sClusterMeshImplementation\n\tcertManager     *certs.CertManager\n\tstatusCollector *status.K8sStatusCollector\n\tflavor          k8s.Flavor\n\tparams          Parameters\n\tclusterName     string\n\tclusterID       string\n\timageVersion    string\n\tclusterArch     string\n}\n\ntype Parameters struct {\n\tNamespace            string\n\tServiceType          string\n\tDestinationContext   string\n\tWait                 bool\n\tWaitDuration         time.Duration\n\tDestinationEndpoints []string\n\tSourceEndpoints      []string\n\tSkipServiceCheck     bool\n\tApiserverImage       string\n\tApiserverVersion     string\n\tCreateCA             bool\n\tWriter               io.Writer\n\tLabels               map[string]string\n\tIPv4AllocCIDR        string\n\tIPv6AllocCIDR        string\n\tAll                  bool\n\tConfigOverwrites     []string\n\tRetries              int\n\tHelmValuesSecretName string\n\tOutput               string\n}\n\nfunc (p Parameters) validateParams() error {\n\tif p.ApiserverImage != defaults.ClusterMeshApiserverImage {\n\t\treturn nil\n\t} else if err := utils.CheckVersion(p.ApiserverVersion); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (p Parameters) waitTimeout() time.Duration {\n\tif p.WaitDuration != time.Duration(0) {\n\t\treturn p.WaitDuration\n\t}\n\n\treturn time.Minute * 15\n}\n\nfunc NewK8sClusterMesh(client k8sClusterMeshImplementation, p Parameters) *K8sClusterMesh {\n\treturn &K8sClusterMesh{\n\t\tclient:      client,\n\t\tparams:      p,\n\t\tcertManager: certs.NewCertManager(client, certs.Parameters{Namespace: p.Namespace}),\n\t}\n}\n\nfunc (k *K8sClusterMesh) Log(format string, a ...interface{}) {\n\tfmt.Fprintf(k.params.Writer, format+\"\\n\", a...)\n}\n\nfunc (k *K8sClusterMesh) GetClusterConfig(ctx context.Context) error {\n\tk.flavor = k.client.AutodetectFlavor(ctx)\n\n\tcm, err := k.client.GetConfigMap(ctx, k.params.Namespace, defaults.ConfigMapName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to retrieve ConfigMap %q: %w\", defaults.ConfigMapName, err)\n\t}\n\n\tclusterID := cm.Data[configNameClusterID]\n\tif clusterID == \"\" {\n\t\tclusterID = \"0\"\n\t}\n\tk.clusterID = clusterID\n\n\tclusterName := cm.Data[configNameClusterName]\n\tif clusterName == \"\" {\n\t\tclusterName = \"default\"\n\t}\n\tk.clusterName = clusterName\n\n\tif clusterID == \"0\" || clusterName == \"default\" {\n\t\tk.Log(\"\u26a0\ufe0f  Cluster not configured for clustermesh, use '--cluster-id' and '--cluster-name' with 'cilium install'. External workloads may still be configured.\")\n\t}\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) Disable(ctx context.Context) error {\n\tk.Log(\"\ud83d\udd25 Deleting clustermesh-apiserver...\")\n\tk.client.DeleteService(ctx, k.params.Namespace, defaults.ClusterMeshServiceName, metav1.DeleteOptions{})\n\tk.client.DeleteDeployment(ctx, k.params.Namespace, defaults.ClusterMeshDeploymentName, metav1.DeleteOptions{})\n\tk.client.DeleteClusterRoleBinding(ctx, defaults.ClusterMeshClusterRoleName, metav1.DeleteOptions{})\n\tk.client.DeleteClusterRole(ctx, defaults.ClusterMeshClusterRoleName, metav1.DeleteOptions{})\n\tk.client.DeleteServiceAccount(ctx, k.params.Namespace, defaults.ClusterMeshServiceAccountName, metav1.DeleteOptions{})\n\tk.client.DeleteSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, metav1.DeleteOptions{})\n\n\tk.deleteCertificates(ctx)\n\n\tk.Log(\"\u2705 ClusterMesh disabled.\")\n\n\treturn nil\n}\n\nfunc (p Parameters) validateForEnable() error {\n\tswitch corev1.ServiceType(p.ServiceType) {\n\tcase corev1.ServiceTypeClusterIP:\n\tcase corev1.ServiceTypeNodePort:\n\tcase corev1.ServiceTypeLoadBalancer:\n\tcase corev1.ServiceTypeExternalName:\n\tcase \"\":\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown service type %q\", p.ServiceType)\n\t}\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) Enable(ctx context.Context) error {\n\tif err := k.params.validateParams(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := k.params.validateForEnable(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := k.GetClusterConfig(ctx); err != nil {\n\t\treturn err\n\t}\n\n\tvar err error\n\tk.imageVersion, err = k.client.GetRunningCiliumVersion(ctx, k.params.Namespace)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tp, err := k.client.GetPlatform(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tk.clusterArch = p.Arch\n\n\tsvc, err := k.generateService()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t_, err = k.client.GetDeployment(ctx, k.params.Namespace, defaults.ClusterMeshDeploymentName, metav1.GetOptions{})\n\tif err == nil {\n\t\tk.Log(\"\u2705 ClusterMesh is already enabled\")\n\t\treturn nil\n\t}\n\n\tif err := k.installCertificates(ctx); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2728 Deploying clustermesh-apiserver from %s...\", k.apiserverImage(utils.ImagePathExcludeDigest))\n\tif _, err := k.client.CreateServiceAccount(ctx, k.params.Namespace, k8s.NewServiceAccount(defaults.ClusterMeshServiceAccountName), metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := k.client.CreateClusterRole(ctx, clusterRole, metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := k.client.CreateClusterRoleBinding(ctx, k8s.NewClusterRoleBinding(defaults.ClusterMeshClusterRoleName, k.params.Namespace, defaults.ClusterMeshServiceAccountName), metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tfor i, opt := range k.params.ConfigOverwrites {\n\t\tif !strings.HasPrefix(opt, \"--\") {\n\t\t\tk.params.ConfigOverwrites[i] = \"--\" + opt\n\t\t}\n\t}\n\n\tif _, err := k.client.CreateDeployment(ctx, k.params.Namespace, k.generateDeployment(k.params.ConfigOverwrites), metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := k.client.CreateService(ctx, k.params.Namespace, svc, metav1.CreateOptions{}); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 ClusterMesh enabled!\")\n\n\treturn nil\n}\n\ntype accessInformation struct {\n\tServiceIPs           []string `json:\"service_ips,omitempty\"`\n\tServicePort          int      `json:\"service_port,omitempty\"`\n\tClusterID            string   `json:\"cluster_id,omitempty\"`\n\tClusterName          string   `json:\"cluster_name,omitempty\"`\n\tCA                   []byte   `json:\"ca,omitempty\"`\n\tClientCert           []byte   `json:\"client_cert,omitempty\"`\n\tClientKey            []byte   `json:\"client_key,omitempty\"`\n\tExternalWorkloadCert []byte   `json:\"external_workload_cert,omitempty\"`\n\tExternalWorkloadKey  []byte   `json:\"external_workload_key,omitempty\"`\n\tTunnel               string   `json:\"tunnel,omitempty\"`\n}\n\nfunc (ai *accessInformation) etcdConfiguration() string {\n\tcfg := \"endpoints:\\n\"\n\tcfg += \"- https://\" + ai.ClusterName + \".mesh.cilium.io:\" + fmt.Sprintf(\"%d\", ai.ServicePort) + \"\\n\"\n\tcfg += \"trusted-ca-file: /var/lib/cilium/clustermesh/\" + ai.ClusterName + caSuffix + \"\\n\"\n\tcfg += \"key-file: /var/lib/cilium/clustermesh/\" + ai.ClusterName + keySuffix + \"\\n\"\n\tcfg += \"cert-file: /var/lib/cilium/clustermesh/\" + ai.ClusterName + certSuffix + \"\\n\"\n\n\treturn cfg\n}\n\nfunc (ai *accessInformation) validate() bool {\n\treturn ai.ClusterName != \"\" &&\n\t\tai.ClusterName != \"default\" &&\n\t\tai.ClusterID != \"\" &&\n\t\tai.ClusterID != \"0\"\n}\n\nfunc getDeprecatedName(secretName string) string {\n\tswitch secretName {\n\tcase defaults.ClusterMeshServerSecretName,\n\t\tdefaults.ClusterMeshAdminSecretName,\n\t\tdefaults.ClusterMeshClientSecretName,\n\t\tdefaults.ClusterMeshExternalWorkloadSecretName:\n\t\treturn secretName + \"s\"\n\tdefault:\n\t\treturn \"\"\n\t}\n}\n\n// We had inconsistency in naming clustermesh secrets between Helm installation and Cilium CLI installation\n// Cilium CLI was naming clustermesh secrets with trailing 's'. eg. 'clustermesh-apiserver-client-certs' instead of `clustermesh-apiserver-client-cert`\n// This caused Cilium CLI 'clustermesh status' command to fail when Cilium is installed using Helm\n// getSecret handles both secret names and logs warning if deprecated secret name is found\nfunc (k *K8sClusterMesh) getSecret(ctx context.Context, client k8sClusterMeshImplementation, secretName string) (*corev1.Secret, error) {\n\n\tsecret, err := client.GetSecret(ctx, k.params.Namespace, secretName, metav1.GetOptions{})\n\tif err != nil {\n\t\tdeprecatedSecretName := getDeprecatedName(secretName)\n\t\tif deprecatedSecretName == \"\" {\n\t\t\treturn nil, fmt.Errorf(\"unable to get secret %q: %w\", secretName, err)\n\t\t}\n\n\t\tk.Log(\"Trying to get secret %s by deprecated name %s\", secretName, deprecatedSecretName)\n\n\t\tsecret, err = client.GetSecret(ctx, k.params.Namespace, deprecatedSecretName, metav1.GetOptions{})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to get secret %q: %w\", deprecatedSecretName, err)\n\t\t}\n\n\t\tk.Log(\"\u26a0\ufe0f Deprecated secret name %q, should be changed to %q\", secret.Name, secretName)\n\n\t}\n\n\treturn secret, err\n}\n\nfunc (k *K8sClusterMesh) extractAccessInformation(ctx context.Context, client k8sClusterMeshImplementation, endpoints []string, verbose bool, getExternalWorkLoadSecret bool) (*accessInformation, error) {\n\tcm, err := client.GetConfigMap(ctx, k.params.Namespace, defaults.ConfigMapName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve ConfigMap %q: %w\", defaults.ConfigMapName, err)\n\t}\n\n\tif _, ok := cm.Data[configNameClusterName]; !ok {\n\t\treturn nil, fmt.Errorf(\"%s is not set in ConfigMap %q\", configNameClusterName, defaults.ConfigMapName)\n\t}\n\n\tclusterID := cm.Data[configNameClusterID]\n\tclusterName := cm.Data[configNameClusterName]\n\n\tif verbose {\n\t\tk.Log(\"\u2728 Extracting access information of cluster %s...\", clusterName)\n\t}\n\tsvc, err := client.GetService(ctx, k.params.Namespace, defaults.ClusterMeshServiceName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get clustermesh service %q: %w\", defaults.ClusterMeshServiceName, err)\n\t}\n\n\tif verbose {\n\t\tk.Log(\"\ud83d\udd11 Extracting secrets from cluster %s...\", clusterName)\n\t}\n\tcaSecret, err := client.GetSecret(ctx, k.params.Namespace, defaults.CASecretName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get secret %q to retrieve CA: %s\", defaults.CASecretName, err)\n\t}\n\n\tcaCert, ok := caSecret.Data[defaults.CASecretCertName]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"secret %q does not contain CA cert %q\", defaults.CASecretName, defaults.CASecretCertName)\n\t}\n\n\tmeshSecret, err := k.getSecret(ctx, client, defaults.ClusterMeshClientSecretName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get client secret to access clustermesh service: %w\", err)\n\t}\n\n\tclientKey, ok := meshSecret.Data[corev1.TLSPrivateKeyKey]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"secret %q does not contain key %q\", meshSecret.Name, corev1.TLSPrivateKeyKey)\n\t}\n\n\tclientCert, ok := meshSecret.Data[corev1.TLSCertKey]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"secret %q does not contain key %q\", meshSecret.Name, corev1.TLSCertKey)\n\t}\n\n\t// ExternalWorkload secret is created by 'clustermesh enable' command, but it isn't created by Helm. We should try to load this secret only when needed\n\tvar externalWorkloadKey, externalWorkloadCert []byte\n\tif getExternalWorkLoadSecret {\n\t\texternalWorkloadSecret, err := k.getSecret(ctx, client, defaults.ClusterMeshExternalWorkloadSecretName)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to get external workload secret to access clustermesh service\")\n\t\t}\n\n\t\texternalWorkloadKey, ok = externalWorkloadSecret.Data[corev1.TLSPrivateKeyKey]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"secret %q does not contain key %q\", externalWorkloadSecret.Namespace, corev1.TLSPrivateKeyKey)\n\t\t}\n\n\t\texternalWorkloadCert, ok = externalWorkloadSecret.Data[corev1.TLSCertKey]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"secret %q does not contain key %q\", externalWorkloadSecret.Namespace, corev1.TLSCertKey)\n\t\t}\n\t}\n\n\tai := &accessInformation{\n\t\tClusterID:            clusterID,\n\t\tClusterName:          clusterName,\n\t\tCA:                   caCert,\n\t\tClientKey:            clientKey,\n\t\tClientCert:           clientCert,\n\t\tExternalWorkloadKey:  externalWorkloadKey,\n\t\tExternalWorkloadCert: externalWorkloadCert,\n\t\tServiceIPs:           []string{},\n\t\tTunnel:               cm.Data[configNameTunnel],\n\t}\n\n\tswitch {\n\tcase len(endpoints) > 0:\n\t\tfor _, endpoint := range endpoints {\n\t\t\tip, port, err := net.SplitHostPort(endpoint)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid endpoint %q, must be IP:PORT: %w\", endpoint, err)\n\t\t\t}\n\n\t\t\tintPort, err := strconv.Atoi(port)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid port %q: %w\", port, err)\n\t\t\t}\n\n\t\t\tif ai.ServicePort == 0 {\n\t\t\t\tai.ServicePort = intPort\n\t\t\t} else if ai.ServicePort != intPort {\n\t\t\t\treturn nil, fmt.Errorf(\"port mismatch (%d != %d), all endpoints must use the same port number\", ai.ServicePort, intPort)\n\t\t\t}\n\n\t\t\tai.ServiceIPs = append(ai.ServiceIPs, ip)\n\t\t}\n\n\tcase svc.Spec.Type == corev1.ServiceTypeClusterIP:\n\t\tif len(svc.Spec.Ports) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"port of service could not be derived, service has no ports\")\n\t\t}\n\t\tif svc.Spec.Ports[0].Port == 0 {\n\t\t\treturn nil, fmt.Errorf(\"port is not set in service\")\n\t\t}\n\t\tai.ServicePort = int(svc.Spec.Ports[0].Port)\n\n\t\tif svc.Spec.ClusterIP == \"\" {\n\t\t\treturn nil, fmt.Errorf(\"IP of service could not be derived, service has no ClusterIP\")\n\t\t}\n\t\tai.ServiceIPs = append(ai.ServiceIPs, svc.Spec.ClusterIP)\n\n\tcase svc.Spec.Type == corev1.ServiceTypeNodePort:\n\t\tif len(svc.Spec.Ports) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"port of service could not be derived, service has no ports\")\n\t\t}\n\n\t\tif svc.Spec.Ports[0].NodePort == 0 {\n\t\t\treturn nil, fmt.Errorf(\"nodeport is not set in service\")\n\t\t}\n\t\tai.ServicePort = int(svc.Spec.Ports[0].NodePort)\n\n\t\tnodes, err := client.ListNodes(ctx, metav1.ListOptions{})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to list nodes in cluster: %w\", err)\n\t\t}\n\n\t\tfor _, node := range nodes.Items {\n\t\t\tnodeIP := \"\"\n\t\t\tfor _, address := range node.Status.Addresses {\n\t\t\t\tswitch address.Type {\n\t\t\t\tcase corev1.NodeExternalIP:\n\t\t\t\t\tnodeIP = address.Address\n\t\t\t\tcase corev1.NodeInternalIP:\n\t\t\t\t\tif nodeIP == \"\" {\n\t\t\t\t\t\tnodeIP = address.Address\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif nodeIP != \"\" {\n\t\t\t\tai.ServiceIPs = append(ai.ServiceIPs, nodeIP)\n\n\t\t\t\t// We can't really support multiple nodes as\n\t\t\t\t// the NodePort will be different and the\n\t\t\t\t// current use of hostAliases will lead to\n\t\t\t\t// DNS-style RR requiring all endpoints to use\n\t\t\t\t// the same port\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tk.Log(\"\u26a0\ufe0f  Service type NodePort detected! Service may fail when nodes are removed from the cluster!\")\n\n\tcase svc.Spec.Type == corev1.ServiceTypeLoadBalancer:\n\t\tif len(svc.Spec.Ports) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"port of service could not be derived, service has no ports\")\n\t\t}\n\n\t\tai.ServicePort = int(svc.Spec.Ports[0].Port)\n\n\t\tfor _, ingressStatus := range svc.Status.LoadBalancer.Ingress {\n\t\t\tif ingressStatus.Hostname == \"\" {\n\t\t\t\tai.ServiceIPs = append(ai.ServiceIPs, ingressStatus.IP)\n\t\t\t} else {\n\t\t\t\tk.Log(\"Hostname based ingress detected, trying to resolve it\")\n\n\t\t\t\tips, err := net.LookupHost(ingressStatus.Hostname)\n\t\t\t\tif err != nil {\n\t\t\t\t\tk.Log(fmt.Sprintf(\"Could not resolve the hostname of the ingress, falling back to the static IP. Error: %v\", err))\n\t\t\t\t\tai.ServiceIPs = append(ai.ServiceIPs, ingressStatus.IP)\n\t\t\t\t} else {\n\t\t\t\t\tk.Log(\"Hostname resolved, using the found ip(s)\")\n\t\t\t\t\tai.ServiceIPs = append(ai.ServiceIPs, ips...)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tswitch {\n\tcase len(ai.ServiceIPs) > 0:\n\t\tif verbose {\n\t\t\tk.Log(\"\u2139\ufe0f  Found ClusterMesh service IPs: %s\", ai.ServiceIPs)\n\t\t}\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unable to derive service IPs automatically\")\n\t}\n\n\treturn ai, nil\n}\n\nfunc (k *K8sClusterMesh) patchConfig(ctx context.Context, client k8sClusterMeshImplementation, ai *accessInformation) error {\n\t_, err := client.GetSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, metav1.GetOptions{})\n\tif err != nil {\n\t\tk.Log(\"\ud83d\udd11 Secret %s does not exist yet, creating it...\", defaults.ClusterMeshSecretName)\n\t\t_, err = client.CreateSecret(ctx, k.params.Namespace, k8s.NewSecret(defaults.ClusterMeshSecretName, k.params.Namespace, map[string][]byte{}), metav1.CreateOptions{})\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to create secret: %w\", err)\n\t\t}\n\t}\n\n\tk.Log(\"\ud83d\udd11 Patching existing secret %s...\", defaults.ClusterMeshSecretName)\n\n\tetcdBase64 := `\"` + ai.ClusterName + `\": \"` + base64.StdEncoding.EncodeToString([]byte(ai.etcdConfiguration())) + `\"`\n\tcaBase64 := `\"` + ai.ClusterName + caSuffix + `\": \"` + base64.StdEncoding.EncodeToString(ai.CA) + `\"`\n\tkeyBase64 := `\"` + ai.ClusterName + keySuffix + `\": \"` + base64.StdEncoding.EncodeToString(ai.ClientKey) + `\"`\n\tcertBase64 := `\"` + ai.ClusterName + certSuffix + `\": \"` + base64.StdEncoding.EncodeToString(ai.ClientCert) + `\"`\n\n\tpatch := []byte(`{\"data\":{` + etcdBase64 + `,` + caBase64 + `,` + keyBase64 + `,` + certBase64 + `}}`)\n\t_, err = client.PatchSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, types.StrategicMergePatchType, patch, metav1.PatchOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to patch secret %s with patch %q: %w\", defaults.ClusterMeshSecretName, patch, err)\n\t}\n\n\tvar aliases []string\n\tfor _, ip := range ai.ServiceIPs {\n\t\taliases = append(aliases, `{\"ip\":\"`+ip+`\", \"hostnames\":[\"`+ai.ClusterName+`.mesh.cilium.io\"]}`)\n\t}\n\n\tpatch = []byte(`{\"spec\":{\"template\":{\"spec\":{\"hostAliases\":[` + strings.Join(aliases, \",\") + `]}}}}`)\n\n\tk.Log(\"\u2728 Patching DaemonSet with IP aliases %s...\", defaults.ClusterMeshSecretName)\n\t_, err = client.PatchDaemonSet(ctx, k.params.Namespace, defaults.AgentDaemonSetName, types.StrategicMergePatchType, patch, metav1.PatchOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to patch DaemonSet %s with patch %q: %w\", defaults.AgentDaemonSetName, patch, err)\n\t}\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) Connect(ctx context.Context) error {\n\tremoteCluster, err := k8s.NewClient(k.params.DestinationContext, \"\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create Kubernetes client to access remote cluster %q: %w\", k.params.DestinationContext, err)\n\t}\n\n\taiRemote, err := k.extractAccessInformation(ctx, remoteCluster, k.params.DestinationEndpoints, true, false)\n\tif err != nil {\n\t\tk.Log(\"\u274c Unable to retrieve access information of remote cluster %q: %s\", remoteCluster.ClusterName(), err)\n\t\treturn err\n\t}\n\n\tif !aiRemote.validate() {\n\t\treturn fmt.Errorf(\"remote cluster has non-unique name (%s) and/or ID (%s)\", aiRemote.ClusterName, aiRemote.ClusterID)\n\t}\n\n\taiLocal, err := k.extractAccessInformation(ctx, k.client, k.params.SourceEndpoints, true, false)\n\tif err != nil {\n\t\tk.Log(\"\u274c Unable to retrieve access information of local cluster %q: %s\", k.client.ClusterName(), err)\n\t\treturn err\n\t}\n\n\tif !aiLocal.validate() {\n\t\treturn fmt.Errorf(\"local cluster has the default name (cluster name: %s) and/or ID 0 (cluster ID: %s)\",\n\t\t\taiLocal.ClusterName, aiLocal.ClusterID)\n\t}\n\n\tcid, err := strconv.Atoi(aiRemote.ClusterID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"remote cluster has non-numeric cluster ID %s. Only numeric values 1-255 are allowed\", aiRemote.ClusterID)\n\t}\n\tif cid < 1 || cid > 255 {\n\t\treturn fmt.Errorf(\"remote cluster has cluster ID %d out of acceptable range (1-255)\", cid)\n\t}\n\n\tif aiRemote.ClusterName == aiLocal.ClusterName {\n\t\treturn fmt.Errorf(\"remote and local cluster have the same, non-unique name: %s\", aiLocal.ClusterName)\n\t}\n\n\tif aiRemote.ClusterID == aiLocal.ClusterID {\n\t\treturn fmt.Errorf(\"remote and local cluster have the same, non-unique ID: %s\", aiLocal.ClusterID)\n\t}\n\n\tk.Log(\"\u2728 Connecting cluster %s -> %s...\", k.client.ClusterName(), remoteCluster.ClusterName())\n\tif err := k.patchConfig(ctx, k.client, aiRemote); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2728 Connecting cluster %s -> %s...\", remoteCluster.ClusterName(), k.client.ClusterName())\n\tif err := k.patchConfig(ctx, remoteCluster, aiLocal); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 Connected cluster %s and %s!\", k.client.ClusterName(), remoteCluster.ClusterName())\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) disconnectCluster(ctx context.Context, src, dst k8sClusterMeshImplementation) error {\n\tcm, err := dst.GetConfigMap(ctx, k.params.Namespace, defaults.ConfigMapName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to retrieve ConfigMap %q: %w\", defaults.ConfigMapName, err)\n\t}\n\n\tif _, ok := cm.Data[configNameClusterName]; !ok {\n\t\treturn fmt.Errorf(\"%s is not set in ConfigMap %q\", configNameClusterName, defaults.ConfigMapName)\n\t}\n\n\tclusterName := cm.Data[configNameClusterName]\n\n\tk.Log(\"\ud83d\udd11 Patching existing secret %s...\", defaults.ClusterMeshSecretName)\n\tmeshSecret, err := src.GetSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"clustermesh configuration secret %s does not exist\", defaults.ClusterMeshSecretName)\n\t}\n\n\tfor _, suffix := range []string{\"\", caSuffix, keySuffix, certSuffix} {\n\t\tif _, ok := meshSecret.Data[clusterName+suffix]; !ok {\n\t\t\tk.Log(\"\u26a0\ufe0f  Key %q does not exist in secret. Cluster already disconnected?\", clusterName+suffix)\n\t\t\tcontinue\n\t\t}\n\n\t\tpatch := []byte(`[{\"op\": \"remove\", \"path\": \"/data/` + clusterName + suffix + `\"}]`)\n\t\t_, err = src.PatchSecret(ctx, k.params.Namespace, defaults.ClusterMeshSecretName, types.JSONPatchType, patch, metav1.PatchOptions{})\n\t\tif err != nil {\n\t\t\tk.Log(\"\u274c Warning: Unable to patch secret %s with path %q: %s\", defaults.ClusterMeshSecretName, patch, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) Disconnect(ctx context.Context) error {\n\tremoteCluster, err := k8s.NewClient(k.params.DestinationContext, \"\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create Kubernetes client to access remote cluster %q: %w\", k.params.DestinationContext, err)\n\t}\n\n\tif err := k.disconnectCluster(ctx, k.client, remoteCluster); err != nil {\n\t\treturn err\n\t}\n\n\tif err := k.disconnectCluster(ctx, remoteCluster, k.client); err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 Disconnected cluster %s and %s.\", k.client.ClusterName(), remoteCluster.ClusterName())\n\n\treturn nil\n}\n\ntype Status struct {\n\tAccessInformation *accessInformation  `json:\"access_information,omitempty\"`\n\tService           *corev1.Service     `json:\"service,omitempty\"`\n\tConnectivity      *ConnectivityStatus `json:\"connectivity,omitempty\"`\n}\n\nfunc (k *K8sClusterMesh) statusAccessInformation(ctx context.Context, log bool, getExternalWorkloadSecret bool) (*accessInformation, error) {\n\tw := utils.NewWaitObserver(ctx, utils.WaitParameters{Log: func(err error, wait string) {\n\t\tif log {\n\t\t\tk.Log(\"\u231b Waiting (%s) for access information: %s\", wait, err)\n\t\t}\n\t}})\n\tdefer w.Cancel()\n\nretry:\n\tai, err := k.extractAccessInformation(ctx, k.client, []string{}, false, getExternalWorkloadSecret)\n\tif err != nil && k.params.Wait {\n\t\tif err := w.Retry(err); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tgoto retry\n\t}\n\n\treturn ai, err\n}\n\nfunc (k *K8sClusterMesh) statusService(ctx context.Context) (*corev1.Service, error) {\n\tw := utils.NewWaitObserver(ctx, utils.WaitParameters{Log: func(err error, wait string) {\n\t\tk.Log(\"\u231b Waiting (%s) for ClusterMesh service to be available: %s\", wait, err)\n\t}})\n\tdefer w.Cancel()\n\nretry:\n\tsvc, err := k.client.GetService(ctx, k.params.Namespace, defaults.ClusterMeshServiceName, metav1.GetOptions{})\n\tif err != nil {\n\t\tif k.params.Wait {\n\t\t\tif err := w.Retry(err); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tgoto retry\n\t\t}\n\n\t\treturn nil, fmt.Errorf(\"clustermesh-apiserver cannot be found: %w\", err)\n\t}\n\n\treturn svc, nil\n}\n\nfunc (k *K8sClusterMesh) waitForDeployment(ctx context.Context) error {\n\tk.Log(\"\u231b [%s] Waiting for deployment %s to become ready...\", k.client.ClusterName(), defaults.ClusterMeshDeploymentName)\n\n\tfor k.client.CheckDeploymentStatus(ctx, k.params.Namespace, defaults.ClusterMeshDeploymentName) != nil {\n\t\tselect {\n\t\tcase <-time.After(time.Second):\n\t\tcase <-ctx.Done():\n\t\t\treturn fmt.Errorf(\"waiting for deployment %s to become ready has been interrupted: %w\", defaults.ClusterMeshDeploymentName, ctx.Err())\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype StatisticalStatus struct {\n\tMin int64   `json:\"min,omitempty\"`\n\tAvg float64 `json:\"avg,omitempty\"`\n\tMax int64   `json:\"max,omitempty\"`\n}\n\ntype ClusterStats struct {\n\tConfigured int `json:\"configured,omitempty\"`\n\tConnected  int `json:\"connected,omitempty\"`\n}\n\ntype ConnectivityStatus struct {\n\tGlobalServices StatisticalStatus        `json:\"global_services,omitempty\"`\n\tConnected      StatisticalStatus        `json:\"connected,omitempty\"`\n\tClusters       map[string]*ClusterStats `json:\"clusters,omitempty\"`\n\tTotal          int64                    `json:\"total,omitempty\"`\n\tNotReady       int64                    `json:\"not_ready,omitempty\"`\n\tErrors         status.ErrorCountMapMap  `json:\"errors,omitempty\"`\n}\n\nfunc (c *ConnectivityStatus) addError(pod, cluster string, err error) {\n\tm := c.Errors[pod]\n\tif m == nil {\n\t\tm = status.ErrorCountMap{}\n\t\tc.Errors[pod] = m\n\t}\n\n\tif m[cluster] == nil {\n\t\tm[cluster] = &status.ErrorCount{}\n\t}\n\n\tm[cluster].Errors = append(m[cluster].Errors, err)\n}\n\nfunc (c *ConnectivityStatus) parseAgentStatus(name string, s *status.ClusterMeshAgentConnectivityStatus) {\n\tif c.GlobalServices.Min < 0 || c.GlobalServices.Min > s.GlobalServices {\n\t\tc.GlobalServices.Min = s.GlobalServices\n\t}\n\n\tif c.GlobalServices.Max < s.GlobalServices {\n\t\tc.GlobalServices.Max = s.GlobalServices\n\t}\n\n\tc.GlobalServices.Avg += float64(s.GlobalServices)\n\tc.Total++\n\n\tready := int64(0)\n\tfor _, cluster := range s.Clusters {\n\t\tstats, ok := c.Clusters[cluster.Name]\n\t\tif !ok {\n\t\t\tstats = &ClusterStats{}\n\t\t\tc.Clusters[cluster.Name] = stats\n\t\t}\n\n\t\tstats.Configured++\n\n\t\tif cluster.Ready {\n\t\t\tready++\n\t\t\tstats.Connected++\n\t\t} else {\n\t\t\tc.addError(name, cluster.Name, fmt.Errorf(\"cluster is not ready: %s\", cluster.Status))\n\t\t}\n\t}\n\n\tif ready != int64(len(s.Clusters)) {\n\t\tc.NotReady++\n\t}\n\n\tif c.Connected.Min < 0 || c.Connected.Min > ready {\n\t\tc.Connected.Min = ready\n\t}\n\n\tif c.Connected.Max < ready {\n\t\tc.Connected.Max = ready\n\t}\n\n\tc.Connected.Avg += float64(ready)\n}\n\nfunc (k *K8sClusterMesh) statusConnectivity(ctx context.Context) (*ConnectivityStatus, error) {\n\tw := utils.NewWaitObserver(ctx, utils.WaitParameters{Log: func(err error, wait string) {\n\t\tk.Log(\"\u231b Waiting (%s) for clusters to be connected: %s\", wait, err)\n\t}})\n\tdefer w.Cancel()\n\nretry:\n\tstatus, err := k.determineStatusConnectivity(ctx)\n\tif k.params.Wait {\n\t\tif err == nil {\n\t\t\tif status.NotReady > 0 {\n\t\t\t\terr = fmt.Errorf(\"%d clusters not ready\", status.NotReady)\n\t\t\t}\n\t\t\tif len(status.Errors) > 0 {\n\t\t\t\terr = fmt.Errorf(\"%d clusters have errors\", len(status.Errors))\n\t\t\t}\n\t\t}\n\n\t\tif err != nil {\n\t\t\tif err := w.Retry(err); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tgoto retry\n\t\t}\n\t}\n\n\treturn status, err\n}\n\nfunc (k *K8sClusterMesh) determineStatusConnectivity(ctx context.Context) (*ConnectivityStatus, error) {\n\tstats := &ConnectivityStatus{\n\t\tGlobalServices: StatisticalStatus{Min: -1},\n\t\tConnected:      StatisticalStatus{Min: -1},\n\t\tErrors:         status.ErrorCountMapMap{},\n\t\tClusters:       map[string]*ClusterStats{},\n\t}\n\n\tpods, err := k.client.ListPods(ctx, k.params.Namespace, metav1.ListOptions{LabelSelector: defaults.AgentPodSelector})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to list cilium pods: %w\", err)\n\t}\n\n\tfor _, pod := range pods.Items {\n\t\ts, err := k.statusCollector.ClusterMeshConnectivity(ctx, pod.Name)\n\t\tif err != nil {\n\t\t\tif errors.Is(err, status.ErrClusterMeshStatusNotAvailable) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, fmt.Errorf(\"unable to determine status of cilium pod %q: %w\", pod.Name, err)\n\t\t}\n\n\t\tstats.parseAgentStatus(pod.Name, s)\n\t}\n\n\tif len(pods.Items) > 0 {\n\t\tstats.GlobalServices.Avg /= float64(len(pods.Items))\n\t\tstats.Connected.Avg /= float64(len(pods.Items))\n\t}\n\n\treturn stats, nil\n}\n\nfunc (k *K8sClusterMesh) Status(ctx context.Context) (*Status, error) {\n\terr := k.GetClusterConfig(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcollector, err := status.NewK8sStatusCollector(k.client, status.K8sStatusParameters{\n\t\tNamespace: k.params.Namespace,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to create client to collect status: %w\", err)\n\t}\n\n\tk.statusCollector = collector\n\n\tctx, cancel := context.WithTimeout(ctx, k.params.waitTimeout())\n\tdefer cancel()\n\n\ts := &Status{}\n\ts.AccessInformation, err = k.statusAccessInformation(ctx, true, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tk.Log(\"\u2705 Cluster access information is available:\")\n\tfor _, ip := range s.AccessInformation.ServiceIPs {\n\t\tk.Log(\"  - %s:%d\", ip, s.AccessInformation.ServicePort)\n\t}\n\n\ts.Service, err = k.statusService(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tk.Log(\"\u2705 Service %q of type %q found\", defaults.ClusterMeshServiceName, s.Service.Spec.Type)\n\n\tif s.Service.Spec.Type == corev1.ServiceTypeLoadBalancer {\n\t\tif len(s.AccessInformation.ServiceIPs) == 0 {\n\t\t\tk.Log(\"\u274c Service is of type LoadBalancer but has no IPs assigned\")\n\t\t\treturn nil, fmt.Errorf(\"no IP available to reach cluster\")\n\t\t}\n\t}\n\n\tif k.params.Wait {\n\t\terr = k.waitForDeployment(ctx)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\ts.Connectivity, err = k.statusConnectivity(ctx)\n\n\tif k.params.Output == status.OutputJSON {\n\t\tjsonStatus, err := json.MarshalIndent(s, \"\", \" \")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to marshal status to JSON\")\n\t\t}\n\t\tfmt.Println(string(jsonStatus))\n\t\treturn s, nil\n\t}\n\n\tif s.Connectivity != nil {\n\t\tif s.Connectivity.NotReady > 0 {\n\t\t\tk.Log(\"\u26a0\ufe0f  %d/%d nodes are not connected to all clusters [min:%d / avg:%.1f / max:%d]\",\n\t\t\t\ts.Connectivity.NotReady,\n\t\t\t\ts.Connectivity.Total,\n\t\t\t\ts.Connectivity.Connected.Min,\n\t\t\t\ts.Connectivity.Connected.Avg,\n\t\t\t\ts.Connectivity.Connected.Max)\n\t\t} else if len(s.Connectivity.Clusters) > 0 {\n\t\t\tk.Log(\"\u2705 All %d nodes are connected to all clusters [min:%d / avg:%.1f / max:%d]\",\n\t\t\t\ts.Connectivity.Total,\n\t\t\t\ts.Connectivity.Connected.Min,\n\t\t\t\ts.Connectivity.Connected.Avg,\n\t\t\t\ts.Connectivity.Connected.Max)\n\t\t}\n\n\t\tk.Log(\"\ud83d\udd0c Cluster Connections:\")\n\t\tfor cluster, stats := range s.Connectivity.Clusters {\n\t\t\tk.Log(\"- %s: %d/%d configured, %d/%d connected\",\n\t\t\t\tcluster, stats.Configured, s.Connectivity.Total,\n\t\t\t\tstats.Connected, s.Connectivity.Total)\n\t\t}\n\n\t\tk.Log(\"\ud83d\udd00 Global services: [ min:%d / avg:%.1f / max:%d ]\",\n\t\t\ts.Connectivity.GlobalServices.Min,\n\t\t\ts.Connectivity.GlobalServices.Avg,\n\t\t\ts.Connectivity.GlobalServices.Max)\n\n\t\tif len(s.Connectivity.Errors) > 0 {\n\t\t\tk.Log(\"\u274c %d Errors:\", len(s.Connectivity.Errors))\n\n\t\t\tfor podName, clusters := range s.Connectivity.Errors {\n\t\t\t\tfor clusterName, a := range clusters {\n\t\t\t\t\tfor _, err := range a.Errors {\n\t\t\t\t\t\tk.Log(\"\u274c %s is not connected to cluster %s: %s\", podName, clusterName, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s, nil\n}\n\nfunc (k *K8sClusterMesh) CreateExternalWorkload(ctx context.Context, names []string) error {\n\tcount := 0\n\tfor _, name := range names {\n\t\tcew := &ciliumv2.CiliumExternalWorkload{\n\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\tName:        name,\n\t\t\t\tLabels:      k.params.Labels,\n\t\t\t\tAnnotations: map[string]string{},\n\t\t\t},\n\t\t\tSpec: ciliumv2.CiliumExternalWorkloadSpec{\n\t\t\t\tIPv4AllocCIDR: k.params.IPv4AllocCIDR,\n\t\t\t\tIPv6AllocCIDR: k.params.IPv6AllocCIDR,\n\t\t\t},\n\t\t}\n\n\t\t_, err := k.client.CreateCiliumExternalWorkload(ctx, cew, metav1.CreateOptions{})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcount++\n\t}\n\tk.Log(\"\u2705 Added %d external workload resources.\", count)\n\treturn nil\n}\n\nfunc (k *K8sClusterMesh) DeleteExternalWorkload(ctx context.Context, names []string) error {\n\tvar errs []string\n\tcount := 0\n\n\tif len(names) == 0 && k.params.All {\n\t\tcewList, err := k.client.ListCiliumExternalWorkloads(ctx, metav1.ListOptions{})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, cew := range cewList.Items {\n\t\t\tnames = append(names, cew.Name)\n\t\t}\n\t}\n\tfor _, name := range names {\n\t\terr := k.client.DeleteCiliumExternalWorkload(ctx, name, metav1.DeleteOptions{})\n\t\tif err != nil {\n\t\t\terrs = append(errs, err.Error())\n\t\t} else {\n\t\t\tcount++\n\t\t}\n\t}\n\tif count > 0 {\n\t\tk.Log(\"\u2705 Removed %d external workload resources.\", count)\n\t} else {\n\t\tk.Log(\"\u2139\ufe0f  No external workload resources to remove.\")\n\t}\n\tif len(errs) > 0 {\n\t\treturn errors.New(strings.Join(errs, \", \"))\n\t}\n\treturn nil\n}\n\nvar installScriptFmt = `#!/bin/bash\nCILIUM_IMAGE=${1:-%[1]s}\nCLUSTER_ADDR=${2:-%[2]s}\nCONFIG_OVERWRITES=${3:-%[3]s}\n\nset -e\nshopt -s extglob\n\n# Run without sudo if not available (e.g., running as root)\nSUDO=\nif [ ! \"$(whoami)\" = \"root\" ] ; then\n    SUDO=sudo\nfi\n\nif [ \"$1\" = \"uninstall\" ] ; then\n    if [ -n \"$(${SUDO} docker ps -a -q -f name=cilium)\" ]; then\n        echo \"Shutting down running Cilium agent\"\n        ${SUDO} docker rm -f cilium || true\n    fi\n    if [ -f /usr/bin/cilium ] ; then\n        echo \"Removing /usr/bin/cilium\"\n        ${SUDO} rm /usr/bin/cilium\n    fi\n    pushd /etc\n    if [ -f resolv.conf.orig ] ; then\n        echo \"Restoring /etc/resolv.conf\"\n        ${SUDO} mv -f resolv.conf.orig resolv.conf\n    elif [ -f resolv.conf.link ] && [ -f $(cat resolv.conf.link) ] ; then\n        echo \"Restoring systemd resolved config...\"\n        if [ -f /usr/lib/systemd/resolved.conf.d/cilium-kube-dns.conf ] ; then\n\t    ${SUDO} rm /usr/lib/systemd/resolved.conf.d/cilium-kube-dns.conf\n        fi\n        ${SUDO} systemctl daemon-reload\n        ${SUDO} systemctl reenable systemd-resolved.service\n        ${SUDO} service systemd-resolved restart\n        ${SUDO} ln -fs $(cat resolv.conf.link) resolv.conf\n        ${SUDO} rm resolv.conf.link\n    fi\n    popd\n    exit 0\nfi\n\nif [ -z \"$CLUSTER_ADDR\" ] ; then\n    echo \"CLUSTER_ADDR must be defined to the IP:PORT at which the clustermesh-apiserver is reachable.\"\n    exit 1\nfi\n\nport='@(6553[0-5]|655[0-2][0-9]|65[0-4][0-9][0-9]|6[0-4][0-9][0-9][0-9]|[1-5][0-9][0-9][0-9][0-9]|[1-9][0-9][0-9][0-9]|[1-9][0-9][0-9]|[1-9][0-9]|[1-9])'\nbyte='@(25[0-5]|2[0-4][0-9]|[1][0-9][0-9]|[1-9][0-9]|[0-9])'\nipv4=\"$byte\\.$byte\\.$byte\\.$byte\"\n\n# Default port is for a HostPort service\ncase \"$CLUSTER_ADDR\" in\n    \\[+([0-9a-fA-F:])\\]:$port)\n\tCLUSTER_PORT=${CLUSTER_ADDR##\\[*\\]:}\n\tCLUSTER_IP=${CLUSTER_ADDR#\\[}\n\tCLUSTER_IP=${CLUSTER_IP%%\\]:*}\n\t;;\n    [^[]$ipv4:$port)\n\tCLUSTER_PORT=${CLUSTER_ADDR##*:}\n\tCLUSTER_IP=${CLUSTER_ADDR%%:*}\n\t;;\n    *:*)\n\techo \"Malformed CLUSTER_ADDR: $CLUSTER_ADDR\"\n\texit 1\n\t;;\n    *)\n\tCLUSTER_PORT=2379\n\tCLUSTER_IP=$CLUSTER_ADDR\n\t;;\nesac\n\n${SUDO} mkdir -p /var/lib/cilium/etcd\n${SUDO} tee /var/lib/cilium/etcd/ca.crt <<EOF >/dev/null\n%[4]sEOF\n${SUDO} tee /var/lib/cilium/etcd/tls.crt <<EOF >/dev/null\n%[5]sEOF\n${SUDO} tee /var/lib/cilium/etcd/tls.key <<EOF >/dev/null\n%[6]sEOF\n${SUDO} tee /var/lib/cilium/etcd/config.yaml <<EOF >/dev/null\n---\ntrusted-ca-file: /var/lib/cilium/etcd/ca.crt\ncert-file: /var/lib/cilium/etcd/tls.crt\nkey-file: /var/lib/cilium/etcd/tls.key\nendpoints:\n- https://clustermesh-apiserver.cilium.io:$CLUSTER_PORT\nEOF\n\nCILIUM_OPTS=\" --join-cluster %[8]s --enable-endpoint-health-checking=false\"\nCILIUM_OPTS+=\" --kvstore etcd --kvstore-opt etcd.config=/var/lib/cilium/etcd/config.yaml\"\nif [ -n \"$HOST_IP\" ] ; then\n    CILIUM_OPTS+=\" --ipv4-node $HOST_IP\"\nfi\nif [ -n \"$CONFIG_OVERWRITES\" ] ; then\n    CILIUM_OPTS+=\" $CONFIG_OVERWRITES\"\nfi\n\nDOCKER_OPTS=\" -d --log-driver local --restart always\"\nDOCKER_OPTS+=\" --privileged --network host --cap-add NET_ADMIN --cap-add SYS_MODULE\"\n# Run cilium agent in the host's cgroup namespace so that\n# socket-based load balancing works as expected.\n# See https://github.com/cilium/cilium/pull/16259 for more details.\nDOCKER_OPTS+=\" --cgroupns=host\"\nDOCKER_OPTS+=\" --volume /var/lib/cilium/etcd:/var/lib/cilium/etcd\"\nDOCKER_OPTS+=\" --volume /var/run/cilium:/var/run/cilium\"\nDOCKER_OPTS+=\" --volume /boot:/boot\"\nDOCKER_OPTS+=\" --volume /lib/modules:/lib/modules\"\nDOCKER_OPTS+=\" --volume /sys/fs/bpf:/sys/fs/bpf\"\nDOCKER_OPTS+=\" --volume /run/xtables.lock:/run/xtables.lock\"\nDOCKER_OPTS+=\" --add-host clustermesh-apiserver.cilium.io:$CLUSTER_IP\"\n\ncilium_started=false\nretries=%[7]s\nwhile [ $cilium_started = false ]; do\n    if [ -n \"$(${SUDO} docker ps -a -q -f name=cilium)\" ]; then\n        echo \"Shutting down running Cilium agent\"\n        ${SUDO} docker rm -f cilium || true\n    fi\n\n    echo \"Launching Cilium agent $CILIUM_IMAGE...\"\n    ${SUDO} docker run --name cilium $DOCKER_OPTS $CILIUM_IMAGE cilium-agent $CILIUM_OPTS\n\n    # Copy Cilium CLI\n    ${SUDO} docker cp cilium:/usr/bin/cilium /usr/bin/cilium\n\n    # Wait for cilium agent to become available\n    for ((i = 0 ; i < 12; i++)); do\n        if ${SUDO} cilium status --brief > /dev/null 2>&1; then\n            cilium_started=true\n            break\n        fi\n        sleep 5s\n        echo \"Waiting for Cilium daemon to come up...\"\n    done\n\n    echo \"Cilium status:\"\n    ${SUDO} cilium status || true\n\n    if [ \"$cilium_started\" = true ] ; then\n        echo 'Cilium successfully started!'\n    else\n        if [ $retries -eq 0 ]; then\n            >&2 echo 'Timeout waiting for Cilium to start, retries exhausted.'\n            exit 1\n        fi\n        ((retries--))\n        echo \"Restarting Cilium...\"\n    fi\ndone\n\n# Wait for kube-dns service to become available\nkubedns=\"\"\nfor ((i = 0 ; i < 24; i++)); do\n    kubedns=$(${SUDO} cilium service list get -o jsonpath='{[?(@.spec.frontend-address.port==53)].spec.frontend-address.ip}')\n    if [ -n \"$kubedns\" ] ; then\n        break\n    fi\n    sleep 5s\n    echo \"Waiting for kube-dns service to come available...\"\ndone\n\nnamespace=$(${SUDO} cilium endpoint get -l reserved:host -o jsonpath='{$[0].status.identity.labels}' | tr -d \"[]\\\"\" | tr \",\" \"\\n\" | grep io.kubernetes.pod.namespace | cut -d= -f2)\n\nif [ -n \"$kubedns\" ] ; then\n    if grep \"nameserver $kubedns\" /etc/resolv.conf ; then\n\techo \"kube-dns IP $kubedns already in /etc/resolv.conf\"\n    else\n\tlinkval=$(readlink /etc/resolv.conf) && echo \"$linkval\" | ${SUDO} tee /etc/resolv.conf.link || true\n\tif [[ \"$linkval\" == *\"/systemd/\"* ]] ; then\n\t    echo \"updating systemd resolved with kube-dns IP $kubedns\"\n\t    ${SUDO} mkdir -p /usr/lib/systemd/resolved.conf.d\n\t    ${SUDO} tee /usr/lib/systemd/resolved.conf.d/cilium-kube-dns.conf <<EOF >/dev/null\n# This file is installed by Cilium to use kube dns server from a non-k8s node.\n[Resolve]\nDNS=$kubedns\nDomains=${namespace}.svc.cluster.local svc.cluster.local cluster.local\nEOF\n\t    ${SUDO} systemctl daemon-reload\n\t    ${SUDO} systemctl reenable systemd-resolved.service\n\t    ${SUDO} service systemd-resolved restart\n\t    ${SUDO} ln -fs /run/systemd/resolve/resolv.conf /etc/resolv.conf\n\telse\n\t    echo \"Adding kube-dns IP $kubedns to /etc/resolv.conf\"\n\t    ${SUDO} cp /etc/resolv.conf /etc/resolv.conf.orig\n\t    resolvconf=\"nameserver $kubedns\\n$(cat /etc/resolv.conf)\\nsearch ${namespace}.svc.cluster.local svc.cluster.local cluster.local\\n\"\n\t    printf \"$resolvconf\" | ${SUDO} tee /etc/resolv.conf\n\tfi\n    fi\nelse\n    >&2 echo \"kube-dns not found.\"\n    exit 1\nfi\n`\n\nfunc (k *K8sClusterMesh) WriteExternalWorkloadInstallScript(ctx context.Context, writer io.Writer) error {\n\tdaemonSet, err := k.client.GetDaemonSet(ctx, k.params.Namespace, defaults.AgentDaemonSetName, metav1.GetOptions{})\n\tif err != nil {\n\t\treturn err\n\t}\n\tif daemonSet == nil {\n\t\treturn fmt.Errorf(\"DaemonSet %s is not available\", defaults.AgentDaemonSetName)\n\t}\n\tk.Log(\"\u2705 Using image from Cilium DaemonSet: %s\", daemonSet.Spec.Template.Spec.Containers[0].Image)\n\n\tai, err := k.statusAccessInformation(ctx, false, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif ai.Tunnel != \"\" && ai.Tunnel != \"vxlan\" {\n\t\treturn fmt.Errorf(\"datapath not using vxlan, please install Cilium with '--config tunnel=vxlan'\")\n\t}\n\n\tclusterAddr := fmt.Sprintf(\"%s:%d\", ai.ServiceIPs[0], ai.ServicePort)\n\tk.Log(\"\u2705 Using clustermesh-apiserver service address: %s\", clusterAddr)\n\n\tconfigOverwrites := \"\"\n\tif len(k.params.ConfigOverwrites) > 0 {\n\t\tfor i, opt := range k.params.ConfigOverwrites {\n\t\t\tif !strings.HasPrefix(opt, \"--\") {\n\t\t\t\tk.params.ConfigOverwrites[i] = \"--\" + opt\n\t\t\t}\n\t\t}\n\t\tconfigOverwrites = strings.Join(k.params.ConfigOverwrites, \" \")\n\t}\n\n\tif k.params.Retries <= 0 {\n\t\tk.params.Retries = 1\n\t}\n\n\tvar ciliumVer semver.Version\n\n\thelmState, err := k.client.GetHelmState(ctx, k.params.Namespace, k.params.HelmValuesSecretName)\n\tif err != nil {\n\t\t// Try to retrieve version from image tag\n\t\tv, err := k.client.GetRunningCiliumVersion(ctx, k.params.Namespace)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tciliumVer, err = utils.ParseCiliumVersion(v)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to parse Cilium version %s: %w\", v, err)\n\t\t}\n\n\t} else {\n\t\tciliumVer = helmState.Version\n\t}\n\n\tsockLBOpt := \"--bpf-lb-sock\"\n\tif ciliumVer.LT(versioncheck.MustVersion(\"1.12.0\")) {\n\t\t// Before 1.12, the socket LB was enabled via --enable-host-reachable-services flag\n\t\tsockLBOpt = \"--enable-host-reachable-services\"\n\t}\n\n\tfmt.Fprintf(writer, installScriptFmt,\n\t\tdaemonSet.Spec.Template.Spec.Containers[0].Image, clusterAddr,\n\t\tconfigOverwrites,\n\t\tstring(ai.CA), string(ai.ExternalWorkloadCert), string(ai.ExternalWorkloadKey),\n\t\tstrconv.Itoa(k.params.Retries), sockLBOpt)\n\treturn nil\n}\n\nfunc formatCEW(cew ciliumv2.CiliumExternalWorkload) string {\n\tvar items []string\n\tip := cew.Status.IP\n\tif ip == \"\" {\n\t\tip = \"N/A\"\n\t}\n\titems = append(items, fmt.Sprintf(\"IP: %s\", ip))\n\tvar labels []string\n\tfor key, value := range cew.Labels {\n\t\tlabels = append(labels, fmt.Sprintf(\"%s=%s\", key, value))\n\t}\n\titems = append(items, fmt.Sprintf(\"Labels: %s\", strings.Join(labels, \",\")))\n\treturn strings.Join(items, \", \")\n}\n\nfunc (k *K8sClusterMesh) ExternalWorkloadStatus(ctx context.Context, names []string) error {\n\tcollector, err := status.NewK8sStatusCollector(k.client, status.K8sStatusParameters{\n\t\tNamespace: k.params.Namespace,\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create client to collect status: %w\", err)\n\t}\n\n\tk.statusCollector = collector\n\n\tctx, cancel := context.WithTimeout(ctx, k.params.waitTimeout())\n\tdefer cancel()\n\n\tai, err := k.statusAccessInformation(ctx, true, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 Cluster access information is available:\")\n\tfor _, ip := range ai.ServiceIPs {\n\t\tk.Log(\"\t - %s:%d\", ip, ai.ServicePort)\n\t}\n\n\tsvc, err := k.statusService(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tk.Log(\"\u2705 Service %q of type %q found\", defaults.ClusterMeshServiceName, svc.Spec.Type)\n\n\tif svc.Spec.Type == corev1.ServiceTypeLoadBalancer {\n\t\tif len(ai.ServiceIPs) == 0 {\n\t\t\tk.Log(\"\u274c Service is of type LoadBalancer but has no IPs assigned\")\n\t\t\treturn fmt.Errorf(\"no IP available to reach cluster\")\n\t\t}\n\t}\n\tvar cews []ciliumv2.CiliumExternalWorkload\n\n\tif len(names) == 0 {\n\t\tcewList, err := k.client.ListCiliumExternalWorkloads(ctx, metav1.ListOptions{})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcews = cewList.Items\n\t\tif len(cews) == 0 {\n\t\t\tk.Log(\"\u26a0\ufe0f  No external workloads found.\")\n\t\t\treturn nil\n\t\t}\n\t} else {\n\t\tfor _, name := range names {\n\t\t\tcew, err := k.client.GetCiliumExternalWorkload(ctx, name, metav1.GetOptions{})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcews = append(cews, *cew)\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\tw := tabwriter.NewWriter(&buf, 0, 0, 4, ' ', 0)\n\n\theader := \"External Workloads\"\n\tfor _, cew := range cews {\n\t\tfmt.Fprintf(w, \"%s\\t%s\\t%s\\n\", header, cew.Name, formatCEW(cew))\n\t\theader = \"\"\n\t}\n\n\tw.Flush()\n\tfmt.Println(buf.String())\n\treturn err\n}\n"], "filenames": ["clustermesh/clustermesh.go"], "buggy_code_start_loc": [323], "buggy_code_end_loc": [324], "fixing_code_start_loc": [323], "fixing_code_end_loc": [324], "type": "CWE-280", "message": "`cilium-cli` is the command line interface to install, manage, and troubleshoot Kubernetes clusters running Cilium. Prior to version 0.13.2,`cilium-cli`, when used to configure cluster mesh functionality, can remove the enforcement of user permissions on the `etcd` store used to mirror local cluster information to remote clusters. Users who have set up cluster meshes using the Cilium Helm chart are not affected by this issue. Due to an incorrect mount point specification, the settings specified by the `initContainer` that configures `etcd` users and their permissions are overwritten when using `cilium-cli` to configure a cluster mesh. An attacker who has already gained access to a valid key and certificate for an `etcd` cluster compromised in this manner could then modify state in that `etcd` cluster. This issue is patched in `cilium-cli` 0.13.2. As a workaround, one may use Cilium's Helm charts to create their cluster.", "other": {"cve": {"id": "CVE-2023-28114", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-22T19:15:12.020", "lastModified": "2023-03-29T19:40:55.483", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "`cilium-cli` is the command line interface to install, manage, and troubleshoot Kubernetes clusters running Cilium. Prior to version 0.13.2,`cilium-cli`, when used to configure cluster mesh functionality, can remove the enforcement of user permissions on the `etcd` store used to mirror local cluster information to remote clusters. Users who have set up cluster meshes using the Cilium Helm chart are not affected by this issue. Due to an incorrect mount point specification, the settings specified by the `initContainer` that configures `etcd` users and their permissions are overwritten when using `cilium-cli` to configure a cluster mesh. An attacker who has already gained access to a valid key and certificate for an `etcd` cluster compromised in this manner could then modify state in that `etcd` cluster. This issue is patched in `cilium-cli` 0.13.2. As a workaround, one may use Cilium's Helm charts to create their cluster."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:A/AC:L/PR:L/UI:N/S:C/C:N/I:L/A:N", "attackVector": "ADJACENT_NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 4.1, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.3, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:A/AC:L/PR:H/UI:N/S:C/C:L/I:L/A:N", "attackVector": "ADJACENT_NETWORK", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 4.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.7, "impactScore": 2.7}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-280"}, {"lang": "en", "value": "CWE-755"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:cilium:cilium-cli:*:*:*:*:*:*:*:*", "versionEndExcluding": "0.13.2", "matchCriteriaId": "745EA41C-AFCC-43BA-863C-D419E59E0A91"}]}]}], "references": [{"url": "https://artifacthub.io/packages/helm/cilium/cilium", "source": "security-advisories@github.com", "tags": ["Product"]}, {"url": "https://github.com/cilium/cilium-cli/commit/fb1427025764e1eebc4a7710d902c4f22cae2610", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/cilium/cilium-cli/releases/tag/v0.13.2", "source": "security-advisories@github.com", "tags": ["Release Notes"]}, {"url": "https://github.com/cilium/cilium-cli/security/advisories/GHSA-6f27-3p6c-p5jc", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/cilium/cilium-cli/commit/fb1427025764e1eebc4a7710d902c4f22cae2610"}}