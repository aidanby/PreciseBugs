{"buggy_code": ["\"\"\"\nDocker manager\n\nThe docker manager is responsible for communicating with the docker-daemon and\nis a wrapper around the docker module. It has methods\nfor creating docker networks, docker volumes, start containers and retrieve\nresults from finished containers.\n\"\"\"\nimport os\nimport time\nimport logging\nimport docker\nimport re\nimport shutil\n\nfrom typing import NamedTuple\nfrom pathlib import Path\n\nfrom vantage6.common import logger_name\nfrom vantage6.common import get_database_config\nfrom vantage6.common.docker.addons import get_container, running_in_docker\nfrom vantage6.common.globals import APPNAME, BASIC_PROCESSING_IMAGE\nfrom vantage6.common.task_status import TaskStatus, has_task_failed\nfrom vantage6.common.docker.network_manager import NetworkManager\nfrom vantage6.algorithm.tools.wrappers import get_column_names\nfrom vantage6.cli.context import NodeContext\nfrom vantage6.node.context import DockerNodeContext\nfrom vantage6.node.docker.docker_base import DockerBaseManager\nfrom vantage6.node.docker.vpn_manager import VPNManager\nfrom vantage6.node.docker.task_manager import DockerTaskManager\nfrom vantage6.node.docker.squid import Squid\nfrom vantage6.common.client.node_client import NodeClient\nfrom vantage6.node.docker.exceptions import (\n    UnknownAlgorithmStartFail,\n    PermanentAlgorithmStartFail,\n    AlgorithmContainerNotFound\n)\n\nlog = logging.getLogger(logger_name(__name__))\n\n\nclass Result(NamedTuple):\n    \"\"\"\n    Data class to store the result of the docker image.\n\n    Attributes\n    ----------\n    run_id: int\n        ID of the current algorithm run\n    logs: str\n        Logs attached to current algorithm run\n    data: str\n        Output data of the algorithm\n    status_code: int\n        Status code of the algorithm run\n    \"\"\"\n    run_id: int\n    task_id: int\n    logs: str\n    data: str\n    status: str\n    parent_id: int | None\n\n\nclass ToBeKilled(NamedTuple):\n    \"\"\" Data class to store which tasks should be killed \"\"\"\n    task_id: int\n    run_id: int\n    organization_id: int\n\n\nclass KilledRun(NamedTuple):\n    \"\"\" Data class to store which algorithms have been killed \"\"\"\n    run_id: int\n    task_id: int\n    parent_id: int\n\n\nclass DockerManager(DockerBaseManager):\n    \"\"\"\n    Wrapper for the docker-py module.\n\n    This class manages tasks related to Docker, such as logging in to\n    docker registries, managing input/output files, logs etc. Results\n    can be retrieved through `get_result()` which returns the first available\n    algorithm result.\n    \"\"\"\n    log = logging.getLogger(logger_name(__name__))\n\n    def __init__(self, ctx: DockerNodeContext | NodeContext,\n                 isolated_network_mgr: NetworkManager,\n                 vpn_manager: VPNManager, tasks_dir: Path, client: NodeClient,\n                 proxy: Squid | None = None) -> None:\n        \"\"\" Initialization of DockerManager creates docker connection and\n            sets some default values.\n\n            Parameters\n            ----------\n            ctx: DockerNodeContext | NodeContext\n                Context object from which some settings are obtained\n            isolated_network_mgr: NetworkManager\n                Manager for the isolated network\n            vpn_manager: VPNManager\n                VPN Manager object\n            tasks_dir: Path\n                Directory in which this task's data are stored\n            client: NodeClient\n                Client object to communicate with the server\n            proxy: Squid | None\n                Squid proxy object\n        \"\"\"\n        self.log.debug(\"Initializing DockerManager\")\n        super().__init__(isolated_network_mgr)\n\n        self.data_volume_name = ctx.docker_volume_name\n        config = ctx.config\n        self.algorithm_env = config.get('algorithm_env', {})\n        self.vpn_manager = vpn_manager\n        self.client = client\n        self.__tasks_dir = tasks_dir\n        self.alpine_image = config.get('alpine')\n        self.proxy = proxy\n\n        # keep track of the running containers\n        self.active_tasks: list[DockerTaskManager] = []\n\n        # keep track of the containers that have failed to start\n        self.failed_tasks: list[DockerTaskManager] = []\n\n        # before a task is executed it gets exposed to these policies\n        self._policies = config.get(\"policies\", {})\n\n        # node name is used to identify algorithm containers belonging\n        # to this node. This is required as multiple nodes may run at\n        # a single machine sharing the docker daemon while using a\n        # different server. Using a different server means that there\n        # could be duplicate result_id's running at the node at the same\n        # time.\n        self.node_name = ctx.name\n\n        # name of the container that is running the node\n        self.node_container_name = ctx.docker_container_name\n\n        # login to the registries\n        docker_registries = ctx.config.get(\"docker_registries\", [])\n        self.login_to_registries(docker_registries)\n\n        # set database uri and whether or not it is a file\n        self._set_database(ctx.databases)\n\n        # keep track of linked docker services\n        self.linked_services: list[str] = []\n\n        # set algorithm device requests\n        self.algorithm_device_requests = []\n        if 'algorithm_device_requests' in config:\n            self._set_algorithm_device_requests(\n                config['algorithm_device_requests']\n            )\n\n    def _set_database(self, databases: dict | list) -> None:\n        \"\"\"\n        Set database location and whether or not it is a file\n\n        Parameters\n        ----------\n        databases: dict | list\n            databases as specified in the config file\n        \"\"\"\n        db_labels = [db['label'] for db in databases]\n\n        # If we're running in a docker container, database_uri would point\n        # to a path on the *host* (since it's been read from the config\n        # file). That's no good here. Therefore, we expect the CLI to set\n        # the environment variables for us. This has the added bonus that we\n        # can override the URI from the command line as well.\n        self.databases = {}\n        for label in db_labels:\n            label_upper = label.upper()\n            db_config = get_database_config(databases, label)\n            if running_in_docker():\n                uri = os.environ[f'{label_upper}_DATABASE_URI']\n            else:\n                uri = db_config['uri']\n\n            if running_in_docker():\n                db_is_file = Path(f'/mnt/{uri}').exists()\n                if db_is_file:\n                    uri = f'/mnt/{uri}'\n            else:\n                db_is_file = Path(uri).exists()\n\n            if db_is_file:\n                # We'll copy the file to the folder `data` in our task_dir.\n                self.log.info(f'Copying {uri} to {self.__tasks_dir}')\n                shutil.copy(uri, self.__tasks_dir)\n                uri = self.__tasks_dir / os.path.basename(uri)\n\n            self.databases[label] = {'uri': uri, 'is_file': db_is_file,\n                                     'type': db_config['type'],\n                                     'env': db_config.get('env', {})}\n        self.log.debug(f\"Databases: {self.databases}\")\n\n    def _set_algorithm_device_requests(self, device_requests_config: dict) \\\n            -> None:\n        \"\"\"\n        Configure device access for the algorithm container.\n\n        Parameters\n        ----------\n        device_requests_config: dict\n           A dictionary containing configuration options for device access.\n           Supported keys:\n           - 'gpu': A boolean value indicating whether GPU access is required.\n        \"\"\"\n        device_requests = []\n        if device_requests_config.get('gpu', False):\n            device = docker.types.DeviceRequest(count=-1,\n                                                capabilities=[['gpu']])\n            device_requests.append(device)\n        self.algorithm_device_requests = device_requests\n\n    def create_volume(self, volume_name: str) -> None:\n        \"\"\"\n        Create a temporary volume for a single run.\n\n        A single run can consist of multiple algorithm containers. It is\n        important to note that all algorithm containers having the same job_id\n        have access to this container.\n\n        Parameters\n        ----------\n        volume_name: str\n            Name of the volume to be created\n        \"\"\"\n        try:\n            self.docker.volumes.get(volume_name)\n            self.log.debug(f\"Volume {volume_name} already exists.\")\n\n        except docker.errors.NotFound:\n            self.log.debug(f\"Creating volume {volume_name}\")\n            self.docker.volumes.create(volume_name)\n\n    def is_docker_image_allowed(\n        self, docker_image_name: str, task_info: dict\n    ) -> bool:\n        \"\"\"\n        Checks the docker image name.\n\n        Against a list of regular expressions as defined in the configuration\n        file. If no expressions are defined, all docker images are accepted.\n\n        Parameters\n        ----------\n        docker_image_name: str\n            uri to the docker image\n        task_info: dict\n            Dictionary with information about the task\n\n        Returns\n        -------\n        bool\n            Whether docker image is allowed or not\n        \"\"\"\n        # in case of subtasks, don't check anymore, as parent has already\n        # been checked\n        if task_info['parent'] is not None:\n            return True\n\n        # check if algorithm matches any of the regex cases\n        allow_basics = self._policies.get('allow_basics_algorithm', True)\n        allowed_algorithms = self._policies.get('allowed_algorithms')\n        if docker_image_name.startswith(BASIC_PROCESSING_IMAGE):\n            if not allow_basics:\n                self.log.warn(\"A task was sent with a basics algorithm that \"\n                              \"this node does not allow to run.\")\n                return False\n            # else: basics are allowed, so we don't need to check the regex\n        elif allowed_algorithms:\n            if isinstance(allowed_algorithms, str):\n                allowed_algorithms = [allowed_algorithms]\n            found = False\n            for regex_expr in allowed_algorithms:\n                expr_ = re.compile(regex_expr)\n                if expr_.match(docker_image_name):\n                    found = True\n\n            if not found:\n                self.log.warn(\"A task was sent with a docker image that this\"\n                              \" node does not allow to run.\")\n                return False\n\n        # check if user or their organization is allowed\n        allowed_users = self._policies.get('allowed_users', [])\n        allowed_orgs = self._policies.get('allowed_organizations', [])\n        if allowed_users or allowed_orgs:\n            is_allowed = self.client.check_user_allowed_to_send_task(\n                allowed_users, allowed_orgs, task_info['init_org']['id'],\n                task_info['init_user']['id']\n            )\n            if not is_allowed:\n                self.log.warn(\"A task was sent by a user or organization that \"\n                              \"this node does not allow to start tasks.\")\n                return False\n\n        # if no limits are declared, log warning\n        if not self._policies:\n            self.log.warn(\"All docker images are allowed on this Node!\")\n\n        return True\n\n    def is_running(self, run_id: int) -> bool:\n        \"\"\"\n        Check if a container is already running for <run_id>.\n\n        Parameters\n        ----------\n        run_id: int\n            run_id of the algorithm container to be found\n\n        Returns\n        -------\n        bool\n            Whether or not algorithm container is running already\n        \"\"\"\n        running_containers = self.docker.containers.list(filters={\n            \"label\": [\n                f\"{APPNAME}-type=algorithm\",\n                f\"node={self.node_name}\",\n                f\"run_id={run_id}\"\n            ]\n        })\n        return bool(running_containers)\n\n    def cleanup_tasks(self) -> list[KilledRun]:\n        \"\"\"\n        Stop all active tasks\n\n        Returns\n        -------\n        list[KilledRun]:\n            List of information on tasks that have been killed\n        \"\"\"\n        run_ids_killed = []\n        if self.active_tasks:\n            self.log.debug(f'Killing {len(self.active_tasks)} active task(s)')\n        while self.active_tasks:\n            task = self.active_tasks.pop()\n            task.cleanup()\n            run_ids_killed.append(KilledRun(\n                run_id=task.run_id,\n                task_id=task.task_id,\n                parent_id=task.parent_id\n            ))\n        return run_ids_killed\n\n    def cleanup(self) -> None:\n        \"\"\"\n        Stop all active tasks and delete the isolated network\n\n        Note: the temporary docker volumes are kept as they may still be used\n        by a parent container\n        \"\"\"\n        # note: the function `cleanup_tasks` returns a list of tasks that were\n        # killed, but we don't register them as killed so they will be run\n        # again when the node is restarted\n        self.cleanup_tasks()\n        for service in self.linked_services:\n            self.isolated_network_mgr.disconnect(service)\n\n        # remove the node container from the network, it runs this code.. so\n        # it does not make sense to delete it just yet\n        self.isolated_network_mgr.disconnect(self.node_container_name)\n\n        # remove the connected containers and the network\n        self.isolated_network_mgr.delete(kill_containers=True)\n\n    def run(self, run_id: int, task_info: dict, image: str,\n            docker_input: bytes, tmp_vol_name: str, token: str,\n            databases_to_use: list[str]\n            ) -> tuple[TaskStatus, list[dict] | None]:\n        \"\"\"\n        Checks if docker task is running. If not, creates DockerTaskManager to\n        run the task\n\n        Parameters\n        ----------\n        run_id: int\n            Server run identifier\n        task_info: dict\n            Dictionary with task information\n        image: str\n            Docker image name\n        docker_input: bytes\n            Input that can be read by docker container\n        tmp_vol_name: str\n            Name of temporary docker volume assigned to the algorithm\n        token: str\n            Bearer token that the container can use\n        databases_to_use: list[str]\n            Labels of the databases to use\n\n        Returns\n        -------\n        TaskStatus, list[dict] | None\n            Returns a tuple with the status of the task and a description of\n            each port on the VPN client that forwards traffic to the algorithm\n            container (``None`` if VPN is not set up).\n        \"\"\"\n        # Verify that an allowed image is used\n        if not self.is_docker_image_allowed(image, task_info):\n            msg = f\"Docker image {image} is not allowed on this Node!\"\n            self.log.critical(msg)\n            return TaskStatus.NOT_ALLOWED,  None\n\n        # Check that this task is not already running\n        if self.is_running(run_id):\n            self.log.warn(\"Task is already being executed, discarding task\")\n            self.log.debug(f\"run_id={run_id} is discarded\")\n            return TaskStatus.ACTIVE, None\n\n        task = DockerTaskManager(\n            image=image,\n            run_id=run_id,\n            task_info=task_info,\n            vpn_manager=self.vpn_manager,\n            node_name=self.node_name,\n            tasks_dir=self.__tasks_dir,\n            isolated_network_mgr=self.isolated_network_mgr,\n            databases=self.databases,\n            docker_volume_name=self.data_volume_name,\n            alpine_image=self.alpine_image,\n            proxy=self.proxy,\n            device_requests=self.algorithm_device_requests\n        )\n\n        # attempt to kick of the task. If it fails do to unknown reasons we try\n        # again. If it fails permanently we add it to the failed tasks to be\n        # handled by the speaking worker of the node\n        attempts = 1\n        while not (task.status == TaskStatus.ACTIVE) and attempts < 3:\n            try:\n                vpn_ports = task.run(\n                    docker_input=docker_input, tmp_vol_name=tmp_vol_name,\n                    token=token, algorithm_env=self.algorithm_env,\n                    databases_to_use=databases_to_use\n                )\n\n            except UnknownAlgorithmStartFail:\n                self.log.exception(f'Failed to start run {run_id} for an '\n                                   'unknown reason. Retrying...')\n                time.sleep(1)  # add some time before retrying the next attempt\n\n            except PermanentAlgorithmStartFail:\n                break\n\n            attempts += 1\n\n        # keep track of the active container\n        if has_task_failed(task.status):\n            self.failed_tasks.append(task)\n            return task.status, None\n        else:\n            self.active_tasks.append(task)\n            return task.status, vpn_ports\n\n    def get_result(self) -> Result:\n        \"\"\"\n        Returns the oldest (FIFO) finished docker container.\n\n        This is a blocking method until a finished container shows up. Once the\n        container is obtained and the results are read, the container is\n        removed from the docker environment.\n\n        Returns\n        -------\n        Result\n            result of the docker image\n        \"\"\"\n\n        # get finished results and get the first one, if no result is available\n        # this is blocking\n        finished_tasks = []\n        while (not finished_tasks) and (not self.failed_tasks):\n            for task in self.active_tasks:\n\n                try:\n                    if task.is_finished():\n                        finished_tasks.append(task)\n                        self.active_tasks.remove(task)\n                        break\n                except AlgorithmContainerNotFound:\n                    self.log.exception(f'Failed to find container for '\n                                       f'result {task.result_id}')\n                    self.failed_tasks.append(task)\n                    self.active_tasks.remove(task)\n                    break\n\n            # sleep for a second before checking again\n            time.sleep(1)\n\n        if finished_tasks:\n            # at least one task is finished\n\n            finished_task = finished_tasks.pop()\n            self.log.debug(f\"Run id={finished_task.run_id} is finished\")\n\n            # Check exit status and report\n            logs = finished_task.report_status()\n\n            # Cleanup containers\n            finished_task.cleanup()\n\n            # Retrieve results from file\n            results = finished_task.get_results()\n\n            # remove the VPN ports of this run from the database\n            self.client.request(\n                'port', params={'run_id': finished_task.run_id},\n                method=\"DELETE\"\n            )\n        else:\n            # at least one task failed to start\n            finished_task = self.failed_tasks.pop()\n            logs = 'Container failed'\n            results = b''\n\n        return Result(\n            run_id=finished_task.run_id,\n            task_id=finished_task.task_id,\n            logs=logs,\n            data=results,\n            status=finished_task.status,\n            parent_id=finished_task.parent_id,\n        )\n\n    def login_to_registries(self, registries: list = []) -> None:\n        \"\"\"\n        Login to the docker registries\n\n        Parameters\n        ----------\n        registries: list\n            list of registries to login to\n        \"\"\"\n        for registry in registries:\n            try:\n                self.docker.login(\n                    username=registry.get(\"username\"),\n                    password=registry.get(\"password\"),\n                    registry=registry.get(\"registry\")\n                )\n                self.log.info(f\"Logged in to {registry.get('registry')}\")\n            except docker.errors.APIError as e:\n                self.log.warn(f\"Could not login to {registry.get('registry')}\")\n                self.log.debug(e)\n\n    def link_container_to_network(self, container_name: str,\n                                  config_alias: str) -> None:\n        \"\"\"\n        Link a docker container to the isolated docker network\n\n        Parameters\n        ----------\n        container_name: str\n            Name of the docker container to be linked to the network\n        config_alias: str\n            Alias of the docker container defined in the config file\n        \"\"\"\n        container = get_container(\n            docker_client=self.docker, name=container_name\n        )\n        if not container:\n            self.log.error(f\"Could not link docker container {container_name} \"\n                           \"that was specified in the configuration file to \"\n                           \"the isolated docker network.\")\n            self.log.error(\"Container not found!\")\n            return\n        self.isolated_network_mgr.connect(\n            container_name=container_name,\n            aliases=[config_alias]\n        )\n        self.linked_services.append(container_name)\n\n    def kill_selected_tasks(\n        self, org_id: int, kill_list: list[ToBeKilled] = None\n    ) -> list[KilledRun]:\n        \"\"\"\n        Kill tasks specified by a kill list, if they are currently running on\n        this node\n\n        Parameters\n        ----------\n        org_id: int\n            The organization id of this node\n        kill_list: list[ToBeKilled]\n            A list of info about tasks that should be killed.\n\n        Returns\n        -------\n        list[KilledRun]\n            List with information on killed tasks\n        \"\"\"\n        killed_list = []\n        for container_to_kill in kill_list:\n            if container_to_kill['organization_id'] != org_id:\n                continue  # this run is on another node\n            # find the task\n            task = next((\n                t for t in self.active_tasks\n                if t.run_id == container_to_kill['run_id']\n            ), None)\n            if task:\n                self.log.info(\n                    f\"Killing containers for run_id={task.run_id}\")\n                self.active_tasks.remove(task)\n                task.cleanup()\n                killed_list.append(KilledRun(\n                    run_id=task.run_id,\n                    task_id=task.task_id,\n                    parent_id=task.parent_id,\n                ))\n            else:\n                self.log.warn(\n                    \"Received instruction to kill run_id=\"\n                    f\"{container_to_kill['run_id']}, but it was not \"\n                    \"found running on this node.\")\n        return killed_list\n\n    def kill_tasks(self, org_id: int,\n                   kill_list: list[ToBeKilled] = None) -> list[KilledRun]:\n        \"\"\"\n        Kill tasks currently running on this node.\n\n        Parameters\n        ----------\n        org_id: int\n            The organization id of this node\n        kill_list: list[ToBeKilled] (optional)\n            A list of info on tasks that should be killed. If the list\n            is not specified, all running algorithm containers will be killed.\n\n        Returns\n        -------\n        list[KilledRun]\n            List of dictionaries with information on killed tasks\n        \"\"\"\n        if kill_list:\n            killed_runs = self.kill_selected_tasks(org_id=org_id,\n                                                   kill_list=kill_list)\n        else:\n            # received instruction to kill all tasks on this node\n            self.log.warn(\n                \"Received instruction from server to kill all algorithms \"\n                \"running on this node. Executing that now...\")\n            killed_runs = self.cleanup_tasks()\n            if len(killed_runs):\n                self.log.warn(\n                    \"Killed the following run ids as instructed via socket:\"\n                    f\" {', '.join([str(r.run_id) for r in killed_runs])}\"\n                )\n            else:\n                self.log.warn(\n                    \"Instructed to kill tasks but none were running\"\n                )\n        return killed_runs\n\n    def get_column_names(self, label: str, type_: str) -> list[str]:\n        \"\"\"\n        Get column names from a node database\n\n        Parameters\n        ----------\n        label: str\n            Label of the database\n        type_: str\n            Type of the database\n\n        Returns\n        -------\n        list[str]\n            List of column names\n        \"\"\"\n        db = self.databases.get(label)\n        if not db:\n            self.log.error(\"Database with label %s not found\", label)\n            return []\n        if not db['is_file']:\n            self.log.error(\"Database with label %s is not a file. Cannot\"\n                           \" determine columns without query\", label)\n            return []\n        if db['type'] == 'excel':\n            self.log.error(\"Cannot determine columns for excel database \"\n                           \" without a worksheet\")\n            return []\n        if type_ not in ('csv', 'sparql'):\n            self.log.error(\"Cannot determine columns for database of type %s.\"\n                           \"Only csv and sparql are supported\", type_)\n            return []\n        return get_column_names(db['uri'], type_)\n"], "fixing_code": ["\"\"\"\nDocker manager\n\nThe docker manager is responsible for communicating with the docker-daemon and\nis a wrapper around the docker module. It has methods\nfor creating docker networks, docker volumes, start containers and retrieve\nresults from finished containers.\n\"\"\"\nimport os\nimport time\nimport logging\nimport docker\nimport re\nimport shutil\n\nfrom typing import NamedTuple\nfrom pathlib import Path\n\nfrom vantage6.common import logger_name\nfrom vantage6.common import get_database_config\nfrom vantage6.common.docker.addons import get_container, running_in_docker\nfrom vantage6.common.globals import APPNAME, BASIC_PROCESSING_IMAGE\nfrom vantage6.common.task_status import TaskStatus, has_task_failed\nfrom vantage6.common.docker.network_manager import NetworkManager\nfrom vantage6.algorithm.tools.wrappers import get_column_names\nfrom vantage6.cli.context import NodeContext\nfrom vantage6.node.context import DockerNodeContext\nfrom vantage6.node.docker.docker_base import DockerBaseManager\nfrom vantage6.node.docker.vpn_manager import VPNManager\nfrom vantage6.node.docker.task_manager import DockerTaskManager\nfrom vantage6.node.docker.squid import Squid\nfrom vantage6.common.client.node_client import NodeClient\nfrom vantage6.node.docker.exceptions import (\n    UnknownAlgorithmStartFail,\n    PermanentAlgorithmStartFail,\n    AlgorithmContainerNotFound\n)\n\nlog = logging.getLogger(logger_name(__name__))\n\n\nclass Result(NamedTuple):\n    \"\"\"\n    Data class to store the result of the docker image.\n\n    Attributes\n    ----------\n    run_id: int\n        ID of the current algorithm run\n    logs: str\n        Logs attached to current algorithm run\n    data: str\n        Output data of the algorithm\n    status_code: int\n        Status code of the algorithm run\n    \"\"\"\n    run_id: int\n    task_id: int\n    logs: str\n    data: str\n    status: str\n    parent_id: int | None\n\n\nclass ToBeKilled(NamedTuple):\n    \"\"\" Data class to store which tasks should be killed \"\"\"\n    task_id: int\n    run_id: int\n    organization_id: int\n\n\nclass KilledRun(NamedTuple):\n    \"\"\" Data class to store which algorithms have been killed \"\"\"\n    run_id: int\n    task_id: int\n    parent_id: int\n\n\nclass DockerManager(DockerBaseManager):\n    \"\"\"\n    Wrapper for the docker-py module.\n\n    This class manages tasks related to Docker, such as logging in to\n    docker registries, managing input/output files, logs etc. Results\n    can be retrieved through `get_result()` which returns the first available\n    algorithm result.\n    \"\"\"\n    log = logging.getLogger(logger_name(__name__))\n\n    def __init__(self, ctx: DockerNodeContext | NodeContext,\n                 isolated_network_mgr: NetworkManager,\n                 vpn_manager: VPNManager, tasks_dir: Path, client: NodeClient,\n                 proxy: Squid | None = None) -> None:\n        \"\"\" Initialization of DockerManager creates docker connection and\n            sets some default values.\n\n            Parameters\n            ----------\n            ctx: DockerNodeContext | NodeContext\n                Context object from which some settings are obtained\n            isolated_network_mgr: NetworkManager\n                Manager for the isolated network\n            vpn_manager: VPNManager\n                VPN Manager object\n            tasks_dir: Path\n                Directory in which this task's data are stored\n            client: NodeClient\n                Client object to communicate with the server\n            proxy: Squid | None\n                Squid proxy object\n        \"\"\"\n        self.log.debug(\"Initializing DockerManager\")\n        super().__init__(isolated_network_mgr)\n\n        self.data_volume_name = ctx.docker_volume_name\n        config = ctx.config\n        self.algorithm_env = config.get('algorithm_env', {})\n        self.vpn_manager = vpn_manager\n        self.client = client\n        self.__tasks_dir = tasks_dir\n        self.alpine_image = config.get('alpine')\n        self.proxy = proxy\n\n        # keep track of the running containers\n        self.active_tasks: list[DockerTaskManager] = []\n\n        # keep track of the containers that have failed to start\n        self.failed_tasks: list[DockerTaskManager] = []\n\n        # before a task is executed it gets exposed to these policies\n        self._policies = config.get(\"policies\", {})\n\n        # node name is used to identify algorithm containers belonging\n        # to this node. This is required as multiple nodes may run at\n        # a single machine sharing the docker daemon while using a\n        # different server. Using a different server means that there\n        # could be duplicate result_id's running at the node at the same\n        # time.\n        self.node_name = ctx.name\n\n        # name of the container that is running the node\n        self.node_container_name = ctx.docker_container_name\n\n        # login to the registries\n        docker_registries = ctx.config.get(\"docker_registries\", [])\n        self.login_to_registries(docker_registries)\n\n        # set database uri and whether or not it is a file\n        self._set_database(ctx.databases)\n\n        # keep track of linked docker services\n        self.linked_services: list[str] = []\n\n        # set algorithm device requests\n        self.algorithm_device_requests = []\n        if 'algorithm_device_requests' in config:\n            self._set_algorithm_device_requests(\n                config['algorithm_device_requests']\n            )\n\n    def _set_database(self, databases: dict | list) -> None:\n        \"\"\"\n        Set database location and whether or not it is a file\n\n        Parameters\n        ----------\n        databases: dict | list\n            databases as specified in the config file\n        \"\"\"\n        db_labels = [db['label'] for db in databases]\n\n        # If we're running in a docker container, database_uri would point\n        # to a path on the *host* (since it's been read from the config\n        # file). That's no good here. Therefore, we expect the CLI to set\n        # the environment variables for us. This has the added bonus that we\n        # can override the URI from the command line as well.\n        self.databases = {}\n        for label in db_labels:\n            label_upper = label.upper()\n            db_config = get_database_config(databases, label)\n            if running_in_docker():\n                uri = os.environ[f'{label_upper}_DATABASE_URI']\n            else:\n                uri = db_config['uri']\n\n            if running_in_docker():\n                db_is_file = Path(f'/mnt/{uri}').exists()\n                if db_is_file:\n                    uri = f'/mnt/{uri}'\n            else:\n                db_is_file = Path(uri).exists()\n\n            if db_is_file:\n                # We'll copy the file to the folder `data` in our task_dir.\n                self.log.info(f'Copying {uri} to {self.__tasks_dir}')\n                shutil.copy(uri, self.__tasks_dir)\n                uri = self.__tasks_dir / os.path.basename(uri)\n\n            self.databases[label] = {'uri': uri, 'is_file': db_is_file,\n                                     'type': db_config['type'],\n                                     'env': db_config.get('env', {})}\n        self.log.debug(f\"Databases: {self.databases}\")\n\n    def _set_algorithm_device_requests(self, device_requests_config: dict) \\\n            -> None:\n        \"\"\"\n        Configure device access for the algorithm container.\n\n        Parameters\n        ----------\n        device_requests_config: dict\n           A dictionary containing configuration options for device access.\n           Supported keys:\n           - 'gpu': A boolean value indicating whether GPU access is required.\n        \"\"\"\n        device_requests = []\n        if device_requests_config.get('gpu', False):\n            device = docker.types.DeviceRequest(count=-1,\n                                                capabilities=[['gpu']])\n            device_requests.append(device)\n        self.algorithm_device_requests = device_requests\n\n    def create_volume(self, volume_name: str) -> None:\n        \"\"\"\n        Create a temporary volume for a single run.\n\n        A single run can consist of multiple algorithm containers. It is\n        important to note that all algorithm containers having the same job_id\n        have access to this container.\n\n        Parameters\n        ----------\n        volume_name: str\n            Name of the volume to be created\n        \"\"\"\n        try:\n            self.docker.volumes.get(volume_name)\n            self.log.debug(f\"Volume {volume_name} already exists.\")\n\n        except docker.errors.NotFound:\n            self.log.debug(f\"Creating volume {volume_name}\")\n            self.docker.volumes.create(volume_name)\n\n    def is_docker_image_allowed(\n        self, docker_image_name: str, task_info: dict\n    ) -> bool:\n        \"\"\"\n        Checks the docker image name.\n\n        Against a list of regular expressions as defined in the configuration\n        file. If no expressions are defined, all docker images are accepted.\n\n        Parameters\n        ----------\n        docker_image_name: str\n            uri to the docker image\n        task_info: dict\n            Dictionary with information about the task\n\n        Returns\n        -------\n        bool\n            Whether docker image is allowed or not\n        \"\"\"\n        # check if algorithm matches any of the regex cases\n        allow_basics = self._policies.get('allow_basics_algorithm', True)\n        allowed_algorithms = self._policies.get('allowed_algorithms')\n        if docker_image_name.startswith(BASIC_PROCESSING_IMAGE):\n            if not allow_basics:\n                self.log.warn(\"A task was sent with a basics algorithm that \"\n                              \"this node does not allow to run.\")\n                return False\n            # else: basics are allowed, so we don't need to check the regex\n        elif allowed_algorithms:\n            if isinstance(allowed_algorithms, str):\n                allowed_algorithms = [allowed_algorithms]\n            found = False\n            for regex_expr in allowed_algorithms:\n                expr_ = re.compile(regex_expr)\n                if expr_.match(docker_image_name):\n                    found = True\n\n            if not found:\n                self.log.warn(\"A task was sent with a docker image that this\"\n                              \" node does not allow to run.\")\n                return False\n\n        # check if user or their organization is allowed\n        allowed_users = self._policies.get('allowed_users', [])\n        allowed_orgs = self._policies.get('allowed_organizations', [])\n        if allowed_users or allowed_orgs:\n            is_allowed = self.client.check_user_allowed_to_send_task(\n                allowed_users, allowed_orgs, task_info['init_org']['id'],\n                task_info['init_user']['id']\n            )\n            if not is_allowed:\n                self.log.warn(\"A task was sent by a user or organization that \"\n                              \"this node does not allow to start tasks.\")\n                return False\n\n        # if no limits are declared, log warning\n        if not self._policies:\n            self.log.warn(\"All docker images are allowed on this Node!\")\n\n        return True\n\n    def is_running(self, run_id: int) -> bool:\n        \"\"\"\n        Check if a container is already running for <run_id>.\n\n        Parameters\n        ----------\n        run_id: int\n            run_id of the algorithm container to be found\n\n        Returns\n        -------\n        bool\n            Whether or not algorithm container is running already\n        \"\"\"\n        running_containers = self.docker.containers.list(filters={\n            \"label\": [\n                f\"{APPNAME}-type=algorithm\",\n                f\"node={self.node_name}\",\n                f\"run_id={run_id}\"\n            ]\n        })\n        return bool(running_containers)\n\n    def cleanup_tasks(self) -> list[KilledRun]:\n        \"\"\"\n        Stop all active tasks\n\n        Returns\n        -------\n        list[KilledRun]:\n            List of information on tasks that have been killed\n        \"\"\"\n        run_ids_killed = []\n        if self.active_tasks:\n            self.log.debug(f'Killing {len(self.active_tasks)} active task(s)')\n        while self.active_tasks:\n            task = self.active_tasks.pop()\n            task.cleanup()\n            run_ids_killed.append(KilledRun(\n                run_id=task.run_id,\n                task_id=task.task_id,\n                parent_id=task.parent_id\n            ))\n        return run_ids_killed\n\n    def cleanup(self) -> None:\n        \"\"\"\n        Stop all active tasks and delete the isolated network\n\n        Note: the temporary docker volumes are kept as they may still be used\n        by a parent container\n        \"\"\"\n        # note: the function `cleanup_tasks` returns a list of tasks that were\n        # killed, but we don't register them as killed so they will be run\n        # again when the node is restarted\n        self.cleanup_tasks()\n        for service in self.linked_services:\n            self.isolated_network_mgr.disconnect(service)\n\n        # remove the node container from the network, it runs this code.. so\n        # it does not make sense to delete it just yet\n        self.isolated_network_mgr.disconnect(self.node_container_name)\n\n        # remove the connected containers and the network\n        self.isolated_network_mgr.delete(kill_containers=True)\n\n    def run(self, run_id: int, task_info: dict, image: str,\n            docker_input: bytes, tmp_vol_name: str, token: str,\n            databases_to_use: list[str]\n            ) -> tuple[TaskStatus, list[dict] | None]:\n        \"\"\"\n        Checks if docker task is running. If not, creates DockerTaskManager to\n        run the task\n\n        Parameters\n        ----------\n        run_id: int\n            Server run identifier\n        task_info: dict\n            Dictionary with task information\n        image: str\n            Docker image name\n        docker_input: bytes\n            Input that can be read by docker container\n        tmp_vol_name: str\n            Name of temporary docker volume assigned to the algorithm\n        token: str\n            Bearer token that the container can use\n        databases_to_use: list[str]\n            Labels of the databases to use\n\n        Returns\n        -------\n        TaskStatus, list[dict] | None\n            Returns a tuple with the status of the task and a description of\n            each port on the VPN client that forwards traffic to the algorithm\n            container (``None`` if VPN is not set up).\n        \"\"\"\n        # Verify that an allowed image is used\n        if not self.is_docker_image_allowed(image, task_info):\n            msg = f\"Docker image {image} is not allowed on this Node!\"\n            self.log.critical(msg)\n            return TaskStatus.NOT_ALLOWED,  None\n\n        # Check that this task is not already running\n        if self.is_running(run_id):\n            self.log.warn(\"Task is already being executed, discarding task\")\n            self.log.debug(f\"run_id={run_id} is discarded\")\n            return TaskStatus.ACTIVE, None\n\n        task = DockerTaskManager(\n            image=image,\n            run_id=run_id,\n            task_info=task_info,\n            vpn_manager=self.vpn_manager,\n            node_name=self.node_name,\n            tasks_dir=self.__tasks_dir,\n            isolated_network_mgr=self.isolated_network_mgr,\n            databases=self.databases,\n            docker_volume_name=self.data_volume_name,\n            alpine_image=self.alpine_image,\n            proxy=self.proxy,\n            device_requests=self.algorithm_device_requests\n        )\n\n        # attempt to kick of the task. If it fails do to unknown reasons we try\n        # again. If it fails permanently we add it to the failed tasks to be\n        # handled by the speaking worker of the node\n        attempts = 1\n        while not (task.status == TaskStatus.ACTIVE) and attempts < 3:\n            try:\n                vpn_ports = task.run(\n                    docker_input=docker_input, tmp_vol_name=tmp_vol_name,\n                    token=token, algorithm_env=self.algorithm_env,\n                    databases_to_use=databases_to_use\n                )\n\n            except UnknownAlgorithmStartFail:\n                self.log.exception(f'Failed to start run {run_id} for an '\n                                   'unknown reason. Retrying...')\n                time.sleep(1)  # add some time before retrying the next attempt\n\n            except PermanentAlgorithmStartFail:\n                break\n\n            attempts += 1\n\n        # keep track of the active container\n        if has_task_failed(task.status):\n            self.failed_tasks.append(task)\n            return task.status, None\n        else:\n            self.active_tasks.append(task)\n            return task.status, vpn_ports\n\n    def get_result(self) -> Result:\n        \"\"\"\n        Returns the oldest (FIFO) finished docker container.\n\n        This is a blocking method until a finished container shows up. Once the\n        container is obtained and the results are read, the container is\n        removed from the docker environment.\n\n        Returns\n        -------\n        Result\n            result of the docker image\n        \"\"\"\n\n        # get finished results and get the first one, if no result is available\n        # this is blocking\n        finished_tasks = []\n        while (not finished_tasks) and (not self.failed_tasks):\n            for task in self.active_tasks:\n\n                try:\n                    if task.is_finished():\n                        finished_tasks.append(task)\n                        self.active_tasks.remove(task)\n                        break\n                except AlgorithmContainerNotFound:\n                    self.log.exception(f'Failed to find container for '\n                                       f'result {task.result_id}')\n                    self.failed_tasks.append(task)\n                    self.active_tasks.remove(task)\n                    break\n\n            # sleep for a second before checking again\n            time.sleep(1)\n\n        if finished_tasks:\n            # at least one task is finished\n\n            finished_task = finished_tasks.pop()\n            self.log.debug(f\"Run id={finished_task.run_id} is finished\")\n\n            # Check exit status and report\n            logs = finished_task.report_status()\n\n            # Cleanup containers\n            finished_task.cleanup()\n\n            # Retrieve results from file\n            results = finished_task.get_results()\n\n            # remove the VPN ports of this run from the database\n            self.client.request(\n                'port', params={'run_id': finished_task.run_id},\n                method=\"DELETE\"\n            )\n        else:\n            # at least one task failed to start\n            finished_task = self.failed_tasks.pop()\n            logs = 'Container failed'\n            results = b''\n\n        return Result(\n            run_id=finished_task.run_id,\n            task_id=finished_task.task_id,\n            logs=logs,\n            data=results,\n            status=finished_task.status,\n            parent_id=finished_task.parent_id,\n        )\n\n    def login_to_registries(self, registries: list = []) -> None:\n        \"\"\"\n        Login to the docker registries\n\n        Parameters\n        ----------\n        registries: list\n            list of registries to login to\n        \"\"\"\n        for registry in registries:\n            try:\n                self.docker.login(\n                    username=registry.get(\"username\"),\n                    password=registry.get(\"password\"),\n                    registry=registry.get(\"registry\")\n                )\n                self.log.info(f\"Logged in to {registry.get('registry')}\")\n            except docker.errors.APIError as e:\n                self.log.warn(f\"Could not login to {registry.get('registry')}\")\n                self.log.debug(e)\n\n    def link_container_to_network(self, container_name: str,\n                                  config_alias: str) -> None:\n        \"\"\"\n        Link a docker container to the isolated docker network\n\n        Parameters\n        ----------\n        container_name: str\n            Name of the docker container to be linked to the network\n        config_alias: str\n            Alias of the docker container defined in the config file\n        \"\"\"\n        container = get_container(\n            docker_client=self.docker, name=container_name\n        )\n        if not container:\n            self.log.error(f\"Could not link docker container {container_name} \"\n                           \"that was specified in the configuration file to \"\n                           \"the isolated docker network.\")\n            self.log.error(\"Container not found!\")\n            return\n        self.isolated_network_mgr.connect(\n            container_name=container_name,\n            aliases=[config_alias]\n        )\n        self.linked_services.append(container_name)\n\n    def kill_selected_tasks(\n        self, org_id: int, kill_list: list[ToBeKilled] = None\n    ) -> list[KilledRun]:\n        \"\"\"\n        Kill tasks specified by a kill list, if they are currently running on\n        this node\n\n        Parameters\n        ----------\n        org_id: int\n            The organization id of this node\n        kill_list: list[ToBeKilled]\n            A list of info about tasks that should be killed.\n\n        Returns\n        -------\n        list[KilledRun]\n            List with information on killed tasks\n        \"\"\"\n        killed_list = []\n        for container_to_kill in kill_list:\n            if container_to_kill['organization_id'] != org_id:\n                continue  # this run is on another node\n            # find the task\n            task = next((\n                t for t in self.active_tasks\n                if t.run_id == container_to_kill['run_id']\n            ), None)\n            if task:\n                self.log.info(\n                    f\"Killing containers for run_id={task.run_id}\")\n                self.active_tasks.remove(task)\n                task.cleanup()\n                killed_list.append(KilledRun(\n                    run_id=task.run_id,\n                    task_id=task.task_id,\n                    parent_id=task.parent_id,\n                ))\n            else:\n                self.log.warn(\n                    \"Received instruction to kill run_id=\"\n                    f\"{container_to_kill['run_id']}, but it was not \"\n                    \"found running on this node.\")\n        return killed_list\n\n    def kill_tasks(self, org_id: int,\n                   kill_list: list[ToBeKilled] = None) -> list[KilledRun]:\n        \"\"\"\n        Kill tasks currently running on this node.\n\n        Parameters\n        ----------\n        org_id: int\n            The organization id of this node\n        kill_list: list[ToBeKilled] (optional)\n            A list of info on tasks that should be killed. If the list\n            is not specified, all running algorithm containers will be killed.\n\n        Returns\n        -------\n        list[KilledRun]\n            List of dictionaries with information on killed tasks\n        \"\"\"\n        if kill_list:\n            killed_runs = self.kill_selected_tasks(org_id=org_id,\n                                                   kill_list=kill_list)\n        else:\n            # received instruction to kill all tasks on this node\n            self.log.warn(\n                \"Received instruction from server to kill all algorithms \"\n                \"running on this node. Executing that now...\")\n            killed_runs = self.cleanup_tasks()\n            if len(killed_runs):\n                self.log.warn(\n                    \"Killed the following run ids as instructed via socket:\"\n                    f\" {', '.join([str(r.run_id) for r in killed_runs])}\"\n                )\n            else:\n                self.log.warn(\n                    \"Instructed to kill tasks but none were running\"\n                )\n        return killed_runs\n\n    def get_column_names(self, label: str, type_: str) -> list[str]:\n        \"\"\"\n        Get column names from a node database\n\n        Parameters\n        ----------\n        label: str\n            Label of the database\n        type_: str\n            Type of the database\n\n        Returns\n        -------\n        list[str]\n            List of column names\n        \"\"\"\n        db = self.databases.get(label)\n        if not db:\n            self.log.error(\"Database with label %s not found\", label)\n            return []\n        if not db['is_file']:\n            self.log.error(\"Database with label %s is not a file. Cannot\"\n                           \" determine columns without query\", label)\n            return []\n        if db['type'] == 'excel':\n            self.log.error(\"Cannot determine columns for excel database \"\n                           \" without a worksheet\")\n            return []\n        if type_ not in ('csv', 'sparql'):\n            self.log.error(\"Cannot determine columns for database of type %s.\"\n                           \"Only csv and sparql are supported\", type_)\n            return []\n        return get_column_names(db['uri'], type_)\n"], "filenames": ["vantage6-node/vantage6/node/docker/docker_manager.py"], "buggy_code_start_loc": [265], "buggy_code_end_loc": [270], "fixing_code_start_loc": [264], "fixing_code_end_loc": [264], "type": "CWE-345", "message": "vantage6 is a framework to manage and deploy privacy enhancing technologies like Federated Learning (FL) and Multi-Party Computation (MPC). In affected versions a node does not check if an image is allowed to run if a `parent_id` is set. A malicious party that breaches the server may modify it to set a fake `parent_id` and send a task of a non-whitelisted algorithm. The node will then execute it because the `parent_id` that is set prevents checks from being run. This impacts all servers that are breached by an expert user. This vulnerability has been patched in version 4.1.2. All users are advised to upgrade. There are no known workarounds for this vulnerability.\n", "other": {"cve": {"id": "CVE-2023-47631", "sourceIdentifier": "security-advisories@github.com", "published": "2023-11-14T21:15:13.230", "lastModified": "2023-11-30T05:39:17.947", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "vantage6 is a framework to manage and deploy privacy enhancing technologies like Federated Learning (FL) and Multi-Party Computation (MPC). In affected versions a node does not check if an image is allowed to run if a `parent_id` is set. A malicious party that breaches the server may modify it to set a fake `parent_id` and send a task of a non-whitelisted algorithm. The node will then execute it because the `parent_id` that is set prevents checks from being run. This impacts all servers that are breached by an expert user. This vulnerability has been patched in version 4.1.2. All users are advised to upgrade. There are no known workarounds for this vulnerability.\n"}, {"lang": "es", "value": "vantage6 es un framework para gestionar e implementar tecnolog\u00edas que mejoran la privacidad, como el Federated Learning (FL) y la Multi-Party Computation (MPC). En las versiones afectadas, un nodo no verifica si se permite ejecutar una imagen si se establece un \"parent_id\". Una parte malintencionada que infrinja el servidor puede modificarlo para establecer un \"parent_id\" falso y enviar una tarea de un algoritmo no incluido en la lista blanca. Luego, el nodo lo ejecutar\u00e1 porque el `parent_id` que est\u00e1 configurado impide que se ejecuten comprobaciones. Esto afecta a todos los servidores que son vulnerados por un usuario experto. Esta vulnerabilidad ha sido parcheada en la versi\u00f3n 4.1.2. Se recomienda a todos los usuarios que actualicen. No se conocen workarounds para esta vulnerabilidad."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.2, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.2, "impactScore": 5.9}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-345"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:vantage6:vantage6:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.1.2", "matchCriteriaId": "AA80D733-13E5-422D-AE21-D8A229C86329"}, {"vulnerable": true, "criteria": "cpe:2.3:a:vantage6:vantage6:4.1.2:rc1:*:*:*:*:*:*", "matchCriteriaId": "6D2D1C28-176B-419C-8467-81E91C501EC6"}]}]}], "references": [{"url": "https://github.com/vantage6/vantage6/blob/version/4.1.1/vantage6-node/vantage6/node/docker/docker_manager.py#L265-L268", "source": "security-advisories@github.com", "tags": ["Product"]}, {"url": "https://github.com/vantage6/vantage6/commit/bf83521eb12fa80aa5fc92ef1692010a9a7f8243", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/vantage6/vantage6/security/advisories/GHSA-vc3v-ppc7-v486", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/vantage6/vantage6/commit/bf83521eb12fa80aa5fc92ef1692010a9a7f8243"}}