{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n */\n\n/*\n * Changes:\tPedro Roque\t:\tRetransmit queue handled by TCP.\n *\t\t\t\t:\tFragmentation on mtu decrease\n *\t\t\t\t:\tSegment collapse on retransmit\n *\t\t\t\t:\tAF independence\n *\n *\t\tLinus Torvalds\t:\tsend_delayed_ack\n *\t\tDavid S. Miller\t:\tCharge memory using the right skb\n *\t\t\t\t\tduring syn/ack processing.\n *\t\tDavid S. Miller :\tOutput engine completely rewritten.\n *\t\tAndrea Arcangeli:\tSYNACK carry ts_recent in tsecr.\n *\t\tCacophonix Gaul :\tdraft-minshall-nagle-01\n *\t\tJ Hadi Salim\t:\tECN support\n *\n */\n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <net/tcp.h>\n\n#include <linux/compiler.h>\n#include <linux/gfp.h>\n#include <linux/module.h>\n#include <linux/static_key.h>\n\n#include <trace/events/tcp.h>\n\nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\n\t\t\t   int push_one, gfp_t gfp);\n\n/* Account for new data that has been sent to the network. */\nstatic void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int prior_packets = tp->packets_out;\n\n\ttp->snd_nxt = TCP_SKB_CB(skb)->end_seq;\n\n\t__skb_unlink(skb, &sk->sk_write_queue);\n\ttcp_rbtree_insert(&sk->tcp_rtx_queue, skb);\n\n\tif (tp->highest_sack == NULL)\n\t\ttp->highest_sack = skb;\n\n\ttp->packets_out += tcp_skb_pcount(skb);\n\tif (!prior_packets || icsk->icsk_pending == ICSK_TIME_LOSS_PROBE)\n\t\ttcp_rearm_rto(sk);\n\n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPORIGDATASENT,\n\t\t      tcp_skb_pcount(skb));\n}\n\n/* SND.NXT, if window was not shrunk or the amount of shrunk was less than one\n * window scaling factor due to loss of precision.\n * If window has been shrunk, what should we make? It is not clear at all.\n * Using SND.UNA we will fail to open window, SND.NXT is out of window. :-(\n * Anything in between SND.UNA...SND.UNA+SND.WND also can be already\n * invalid. OK, let's make this for now:\n */\nstatic inline __u32 tcp_acceptable_seq(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!before(tcp_wnd_end(tp), tp->snd_nxt) ||\n\t    (tp->rx_opt.wscale_ok &&\n\t     ((tp->snd_nxt - tcp_wnd_end(tp)) < (1 << tp->rx_opt.rcv_wscale))))\n\t\treturn tp->snd_nxt;\n\telse\n\t\treturn tcp_wnd_end(tp);\n}\n\n/* Calculate mss to advertise in SYN segment.\n * RFC1122, RFC1063, draft-ietf-tcpimpl-pmtud-01 state that:\n *\n * 1. It is independent of path mtu.\n * 2. Ideally, it is maximal possible segment size i.e. 65535-40.\n * 3. For IPv4 it is reasonable to calculate it from maximal MTU of\n *    attached devices, because some buggy hosts are confused by\n *    large MSS.\n * 4. We do not make 3, we advertise MSS, calculated from first\n *    hop device mtu, but allow to raise it to ip_rt_min_advmss.\n *    This may be overridden via information stored in routing table.\n * 5. Value 65535 for MSS is valid in IPv6 and means \"as large as possible,\n *    probably even Jumbo\".\n */\nstatic __u16 tcp_advertise_mss(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tint mss = tp->advmss;\n\n\tif (dst) {\n\t\tunsigned int metric = dst_metric_advmss(dst);\n\n\t\tif (metric < mss) {\n\t\t\tmss = metric;\n\t\t\ttp->advmss = mss;\n\t\t}\n\t}\n\n\treturn (__u16)mss;\n}\n\n/* RFC2861. Reset CWND after idle period longer RTO to \"restart window\".\n * This is the first part of cwnd validation mechanism.\n */\nvoid tcp_cwnd_restart(struct sock *sk, s32 delta)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 restart_cwnd = tcp_init_cwnd(tp, __sk_dst_get(sk));\n\tu32 cwnd = tp->snd_cwnd;\n\n\ttcp_ca_event(sk, CA_EVENT_CWND_RESTART);\n\n\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\trestart_cwnd = min(restart_cwnd, cwnd);\n\n\twhile ((delta -= inet_csk(sk)->icsk_rto) > 0 && cwnd > restart_cwnd)\n\t\tcwnd >>= 1;\n\ttp->snd_cwnd = max(cwnd, restart_cwnd);\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->snd_cwnd_used = 0;\n}\n\n/* Congestion state accounting after a packet has been sent. */\nstatic void tcp_event_data_sent(struct tcp_sock *tp,\n\t\t\t\tstruct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst u32 now = tcp_jiffies32;\n\n\tif (tcp_packets_in_flight(tp) == 0)\n\t\ttcp_ca_event(sk, CA_EVENT_TX_START);\n\n\ttp->lsndtime = now;\n\n\t/* If it is a reply for ato after last received\n\t * packet, enter pingpong mode.\n\t */\n\tif ((u32)(now - icsk->icsk_ack.lrcvtime) < icsk->icsk_ack.ato)\n\t\ticsk->icsk_ack.pingpong = 1;\n}\n\n/* Account for an ACK we sent. */\nstatic inline void tcp_event_ack_sent(struct sock *sk, unsigned int pkts,\n\t\t\t\t      u32 rcv_nxt)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (unlikely(tp->compressed_ack > TCP_FASTRETRANS_THRESH)) {\n\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,\n\t\t\t      tp->compressed_ack - TCP_FASTRETRANS_THRESH);\n\t\ttp->compressed_ack = TCP_FASTRETRANS_THRESH;\n\t\tif (hrtimer_try_to_cancel(&tp->compressed_ack_timer) == 1)\n\t\t\t__sock_put(sk);\n\t}\n\n\tif (unlikely(rcv_nxt != tp->rcv_nxt))\n\t\treturn;  /* Special ACK sent by DCTCP to reflect ECN */\n\ttcp_dec_quickack_mode(sk, pkts);\n\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);\n}\n\n/* Determine a window scaling and initial window to offer.\n * Based on the assumption that the given amount of space\n * will be offered. Store the results in the tp structure.\n * NOTE: for smooth operation initial space offering should\n * be a multiple of mss if possible. We assume here that mss >= 1.\n * This MUST be enforced by all callers.\n */\nvoid tcp_select_initial_window(const struct sock *sk, int __space, __u32 mss,\n\t\t\t       __u32 *rcv_wnd, __u32 *window_clamp,\n\t\t\t       int wscale_ok, __u8 *rcv_wscale,\n\t\t\t       __u32 init_rcv_wnd)\n{\n\tunsigned int space = (__space < 0 ? 0 : __space);\n\n\t/* If no clamp set the clamp to the max possible scaled window */\n\tif (*window_clamp == 0)\n\t\t(*window_clamp) = (U16_MAX << TCP_MAX_WSCALE);\n\tspace = min(*window_clamp, space);\n\n\t/* Quantize space offering to a multiple of mss if possible. */\n\tif (space > mss)\n\t\tspace = rounddown(space, mss);\n\n\t/* NOTE: offering an initial window larger than 32767\n\t * will break some buggy TCP stacks. If the admin tells us\n\t * it is likely we could be speaking with such a buggy stack\n\t * we will truncate our initial window offering to 32K-1\n\t * unless the remote has sent us a window scaling option,\n\t * which we interpret as a sign the remote TCP is not\n\t * misinterpreting the window field as a signed quantity.\n\t */\n\tif (sock_net(sk)->ipv4.sysctl_tcp_workaround_signed_windows)\n\t\t(*rcv_wnd) = min(space, MAX_TCP_WINDOW);\n\telse\n\t\t(*rcv_wnd) = min_t(u32, space, U16_MAX);\n\n\tif (init_rcv_wnd)\n\t\t*rcv_wnd = min(*rcv_wnd, init_rcv_wnd * mss);\n\n\t(*rcv_wscale) = 0;\n\tif (wscale_ok) {\n\t\t/* Set window scaling on max possible window */\n\t\tspace = max_t(u32, space, sock_net(sk)->ipv4.sysctl_tcp_rmem[2]);\n\t\tspace = max_t(u32, space, sysctl_rmem_max);\n\t\tspace = min_t(u32, space, *window_clamp);\n\t\twhile (space > U16_MAX && (*rcv_wscale) < TCP_MAX_WSCALE) {\n\t\t\tspace >>= 1;\n\t\t\t(*rcv_wscale)++;\n\t\t}\n\t}\n\t/* Set the clamp no higher than max representable value */\n\t(*window_clamp) = min_t(__u32, U16_MAX << (*rcv_wscale), *window_clamp);\n}\nEXPORT_SYMBOL(tcp_select_initial_window);\n\n/* Chose a new window to advertise, update state in tcp_sock for the\n * socket, and return result with RFC1323 scaling applied.  The return\n * value can be stuffed directly into th->window for an outgoing\n * frame.\n */\nstatic u16 tcp_select_window(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 old_win = tp->rcv_wnd;\n\tu32 cur_win = tcp_receive_window(tp);\n\tu32 new_win = __tcp_select_window(sk);\n\n\t/* Never shrink the offered window */\n\tif (new_win < cur_win) {\n\t\t/* Danger Will Robinson!\n\t\t * Don't update rcv_wup/rcv_wnd here or else\n\t\t * we will not be able to advertise a zero\n\t\t * window in time.  --DaveM\n\t\t *\n\t\t * Relax Will Robinson.\n\t\t */\n\t\tif (new_win == 0)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPWANTZEROWINDOWADV);\n\t\tnew_win = ALIGN(cur_win, 1 << tp->rx_opt.rcv_wscale);\n\t}\n\ttp->rcv_wnd = new_win;\n\ttp->rcv_wup = tp->rcv_nxt;\n\n\t/* Make sure we do not exceed the maximum possible\n\t * scaled window.\n\t */\n\tif (!tp->rx_opt.rcv_wscale &&\n\t    sock_net(sk)->ipv4.sysctl_tcp_workaround_signed_windows)\n\t\tnew_win = min(new_win, MAX_TCP_WINDOW);\n\telse\n\t\tnew_win = min(new_win, (65535U << tp->rx_opt.rcv_wscale));\n\n\t/* RFC1323 scaling applied */\n\tnew_win >>= tp->rx_opt.rcv_wscale;\n\n\t/* If we advertise zero window, disable fast path. */\n\tif (new_win == 0) {\n\t\ttp->pred_flags = 0;\n\t\tif (old_win)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPTOZEROWINDOWADV);\n\t} else if (old_win == 0) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFROMZEROWINDOWADV);\n\t}\n\n\treturn new_win;\n}\n\n/* Packet ECN state for a SYN-ACK */\nstatic void tcp_ecn_send_synack(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_CWR;\n\tif (!(tp->ecn_flags & TCP_ECN_OK))\n\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_ECE;\n\telse if (tcp_ca_needs_ecn(sk) ||\n\t\t tcp_bpf_ca_needs_ecn(sk))\n\t\tINET_ECN_xmit(sk);\n}\n\n/* Packet ECN state for a SYN.  */\nstatic void tcp_ecn_send_syn(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool bpf_needs_ecn = tcp_bpf_ca_needs_ecn(sk);\n\tbool use_ecn = sock_net(sk)->ipv4.sysctl_tcp_ecn == 1 ||\n\t\ttcp_ca_needs_ecn(sk) || bpf_needs_ecn;\n\n\tif (!use_ecn) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_feature(dst, RTAX_FEATURE_ECN))\n\t\t\tuse_ecn = true;\n\t}\n\n\ttp->ecn_flags = 0;\n\n\tif (use_ecn) {\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ECE | TCPHDR_CWR;\n\t\ttp->ecn_flags = TCP_ECN_OK;\n\t\tif (tcp_ca_needs_ecn(sk) || bpf_needs_ecn)\n\t\t\tINET_ECN_xmit(sk);\n\t}\n}\n\nstatic void tcp_ecn_clear_syn(struct sock *sk, struct sk_buff *skb)\n{\n\tif (sock_net(sk)->ipv4.sysctl_tcp_ecn_fallback)\n\t\t/* tp->ecn_flags are cleared at a later point in time when\n\t\t * SYN ACK is ultimatively being received.\n\t\t */\n\t\tTCP_SKB_CB(skb)->tcp_flags &= ~(TCPHDR_ECE | TCPHDR_CWR);\n}\n\nstatic void\ntcp_ecn_make_synack(const struct request_sock *req, struct tcphdr *th)\n{\n\tif (inet_rsk(req)->ecn_ok)\n\t\tth->ece = 1;\n}\n\n/* Set up ECN state for a packet on a ESTABLISHED socket that is about to\n * be sent.\n */\nstatic void tcp_ecn_send(struct sock *sk, struct sk_buff *skb,\n\t\t\t struct tcphdr *th, int tcp_header_len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->ecn_flags & TCP_ECN_OK) {\n\t\t/* Not-retransmitted data segment: set ECT and inject CWR. */\n\t\tif (skb->len != tcp_header_len &&\n\t\t    !before(TCP_SKB_CB(skb)->seq, tp->snd_nxt)) {\n\t\t\tINET_ECN_xmit(sk);\n\t\t\tif (tp->ecn_flags & TCP_ECN_QUEUE_CWR) {\n\t\t\t\ttp->ecn_flags &= ~TCP_ECN_QUEUE_CWR;\n\t\t\t\tth->cwr = 1;\n\t\t\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_ECN;\n\t\t\t}\n\t\t} else if (!tcp_ca_needs_ecn(sk)) {\n\t\t\t/* ACK or retransmitted segment: clear ECT|CE */\n\t\t\tINET_ECN_dontxmit(sk);\n\t\t}\n\t\tif (tp->ecn_flags & TCP_ECN_DEMAND_CWR)\n\t\t\tth->ece = 1;\n\t}\n}\n\n/* Constructs common control bits of non-data skb. If SYN/FIN is present,\n * auto increment end seqno.\n */\nstatic void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)\n{\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tTCP_SKB_CB(skb)->tcp_flags = flags;\n\tTCP_SKB_CB(skb)->sacked = 0;\n\n\ttcp_skb_pcount_set(skb, 1);\n\n\tTCP_SKB_CB(skb)->seq = seq;\n\tif (flags & (TCPHDR_SYN | TCPHDR_FIN))\n\t\tseq++;\n\tTCP_SKB_CB(skb)->end_seq = seq;\n}\n\nstatic inline bool tcp_urg_mode(const struct tcp_sock *tp)\n{\n\treturn tp->snd_una != tp->snd_up;\n}\n\n#define OPTION_SACK_ADVERTISE\t(1 << 0)\n#define OPTION_TS\t\t(1 << 1)\n#define OPTION_MD5\t\t(1 << 2)\n#define OPTION_WSCALE\t\t(1 << 3)\n#define OPTION_FAST_OPEN_COOKIE\t(1 << 8)\n#define OPTION_SMC\t\t(1 << 9)\n\nstatic void smc_options_write(__be32 *ptr, u16 *options)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (unlikely(OPTION_SMC & *options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t\t       (TCPOPT_EXP <<  8) |\n\t\t\t\t       (TCPOLEN_EXP_SMC_BASE));\n\t\t\t*ptr++ = htonl(TCPOPT_SMC_MAGIC);\n\t\t}\n\t}\n#endif\n}\n\nstruct tcp_out_options {\n\tu16 options;\t\t/* bit field of OPTION_* */\n\tu16 mss;\t\t/* 0 to disable */\n\tu8 ws;\t\t\t/* window scale, 0 to disable */\n\tu8 num_sack_blocks;\t/* number of SACK blocks to include */\n\tu8 hash_size;\t\t/* bytes in hash_location */\n\t__u8 *hash_location;\t/* temporary pointer, overloaded */\n\t__u32 tsval, tsecr;\t/* need to include OPTION_TS */\n\tstruct tcp_fastopen_cookie *fastopen_cookie;\t/* Fast open cookie */\n};\n\n/* Write previously computed TCP options to the packet.\n *\n * Beware: Something in the Internet is very sensitive to the ordering of\n * TCP options, we learned this through the hard way, so be careful here.\n * Luckily we can at least blame others for their non-compliance but from\n * inter-operability perspective it seems that we're somewhat stuck with\n * the ordering which we have been using if we want to keep working with\n * those broken things (not that it currently hurts anybody as there isn't\n * particular reason why the ordering would need to be changed).\n *\n * At least SACK_PERM as the first option is known to lead to a disaster\n * (but it may well be that other scenarios fail similarly).\n */\nstatic void tcp_options_write(__be32 *ptr, struct tcp_sock *tp,\n\t\t\t      struct tcp_out_options *opts)\n{\n\tu16 options = opts->options;\t/* mungable copy */\n\n\tif (unlikely(OPTION_MD5 & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\t/* overload cookie hash location */\n\t\topts->hash_location = (__u8 *)ptr;\n\t\tptr += 4;\n\t}\n\n\tif (unlikely(opts->mss)) {\n\t\t*ptr++ = htonl((TCPOPT_MSS << 24) |\n\t\t\t       (TCPOLEN_MSS << 16) |\n\t\t\t       opts->mss);\n\t}\n\n\tif (likely(OPTION_TS & options)) {\n\t\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |\n\t\t\t\t       (TCPOLEN_SACK_PERM << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t\toptions &= ~OPTION_SACK_ADVERTISE;\n\t\t} else {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t}\n\t\t*ptr++ = htonl(opts->tsval);\n\t\t*ptr++ = htonl(opts->tsecr);\n\t}\n\n\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_SACK_PERM << 8) |\n\t\t\t       TCPOLEN_SACK_PERM);\n\t}\n\n\tif (unlikely(OPTION_WSCALE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_WINDOW << 16) |\n\t\t\t       (TCPOLEN_WINDOW << 8) |\n\t\t\t       opts->ws);\n\t}\n\n\tif (unlikely(opts->num_sack_blocks)) {\n\t\tstruct tcp_sack_block *sp = tp->rx_opt.dsack ?\n\t\t\ttp->duplicate_sack : tp->selective_acks;\n\t\tint this_sack;\n\n\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t       (TCPOPT_SACK <<  8) |\n\t\t\t       (TCPOLEN_SACK_BASE + (opts->num_sack_blocks *\n\t\t\t\t\t\t     TCPOLEN_SACK_PERBLOCK)));\n\n\t\tfor (this_sack = 0; this_sack < opts->num_sack_blocks;\n\t\t     ++this_sack) {\n\t\t\t*ptr++ = htonl(sp[this_sack].start_seq);\n\t\t\t*ptr++ = htonl(sp[this_sack].end_seq);\n\t\t}\n\n\t\ttp->rx_opt.dsack = 0;\n\t}\n\n\tif (unlikely(OPTION_FAST_OPEN_COOKIE & options)) {\n\t\tstruct tcp_fastopen_cookie *foc = opts->fastopen_cookie;\n\t\tu8 *p = (u8 *)ptr;\n\t\tu32 len; /* Fast Open option length */\n\n\t\tif (foc->exp) {\n\t\t\tlen = TCPOLEN_EXP_FASTOPEN_BASE + foc->len;\n\t\t\t*ptr = htonl((TCPOPT_EXP << 24) | (len << 16) |\n\t\t\t\t     TCPOPT_FASTOPEN_MAGIC);\n\t\t\tp += TCPOLEN_EXP_FASTOPEN_BASE;\n\t\t} else {\n\t\t\tlen = TCPOLEN_FASTOPEN_BASE + foc->len;\n\t\t\t*p++ = TCPOPT_FASTOPEN;\n\t\t\t*p++ = len;\n\t\t}\n\n\t\tmemcpy(p, foc->val, foc->len);\n\t\tif ((len & 3) == 2) {\n\t\t\tp[foc->len] = TCPOPT_NOP;\n\t\t\tp[foc->len + 1] = TCPOPT_NOP;\n\t\t}\n\t\tptr += (len + 3) >> 2;\n\t}\n\n\tsmc_options_write(ptr, &options);\n}\n\nstatic void smc_set_option(const struct tcp_sock *tp,\n\t\t\t   struct tcp_out_options *opts,\n\t\t\t   unsigned int *remaining)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc) {\n\t\t\tif (*remaining >= TCPOLEN_EXP_SMC_BASE_ALIGNED) {\n\t\t\t\topts->options |= OPTION_SMC;\n\t\t\t\t*remaining -= TCPOLEN_EXP_SMC_BASE_ALIGNED;\n\t\t\t}\n\t\t}\n\t}\n#endif\n}\n\nstatic void smc_set_option_cond(const struct tcp_sock *tp,\n\t\t\t\tconst struct inet_request_sock *ireq,\n\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\tunsigned int *remaining)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc && ireq->smc_ok) {\n\t\t\tif (*remaining >= TCPOLEN_EXP_SMC_BASE_ALIGNED) {\n\t\t\t\topts->options |= OPTION_SMC;\n\t\t\t\t*remaining -= TCPOLEN_EXP_SMC_BASE_ALIGNED;\n\t\t\t}\n\t\t}\n\t}\n#endif\n}\n\n/* Compute TCP options for SYN packets. This is not the final\n * network wire format yet.\n */\nstatic unsigned int tcp_syn_options(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\tstruct tcp_md5sig_key **md5)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int remaining = MAX_TCP_OPTION_SPACE;\n\tstruct tcp_fastopen_request *fastopen = tp->fastopen_req;\n\n\t*md5 = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (unlikely(rcu_access_pointer(tp->md5sig_info))) {\n\t\t*md5 = tp->af_specific->md5_lookup(sk, sk);\n\t\tif (*md5) {\n\t\t\topts->options |= OPTION_MD5;\n\t\t\tremaining -= TCPOLEN_MD5SIG_ALIGNED;\n\t\t}\n\t}\n#endif\n\n\t/* We always get an MSS option.  The option bytes which will be seen in\n\t * normal data packets should timestamps be used, must be in the MSS\n\t * advertised.  But we subtract them from tp->mss_cache so that\n\t * calculations in tcp_sendmsg are simpler etc.  So account for this\n\t * fact here if necessary.  If we don't do this correctly, as a\n\t * receiver we won't recognize data packets as being full sized when we\n\t * should, and thus we won't abide by the delayed ACK rules correctly.\n\t * SACKs don't matter, we never delay an ACK when we have any of those\n\t * going out.  */\n\topts->mss = tcp_advertise_mss(sk);\n\tremaining -= TCPOLEN_MSS_ALIGNED;\n\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_timestamps && !*md5)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = tcp_skb_timestamp(skb) + tp->tsoffset;\n\t\topts->tsecr = tp->rx_opt.ts_recent;\n\t\tremaining -= TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_window_scaling)) {\n\t\topts->ws = tp->rx_opt.rcv_wscale;\n\t\topts->options |= OPTION_WSCALE;\n\t\tremaining -= TCPOLEN_WSCALE_ALIGNED;\n\t}\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_sack)) {\n\t\topts->options |= OPTION_SACK_ADVERTISE;\n\t\tif (unlikely(!(OPTION_TS & opts->options)))\n\t\t\tremaining -= TCPOLEN_SACKPERM_ALIGNED;\n\t}\n\n\tif (fastopen && fastopen->cookie.len >= 0) {\n\t\tu32 need = fastopen->cookie.len;\n\n\t\tneed += fastopen->cookie.exp ? TCPOLEN_EXP_FASTOPEN_BASE :\n\t\t\t\t\t       TCPOLEN_FASTOPEN_BASE;\n\t\tneed = (need + 3) & ~3U;  /* Align to 32 bits */\n\t\tif (remaining >= need) {\n\t\t\topts->options |= OPTION_FAST_OPEN_COOKIE;\n\t\t\topts->fastopen_cookie = &fastopen->cookie;\n\t\t\tremaining -= need;\n\t\t\ttp->syn_fastopen = 1;\n\t\t\ttp->syn_fastopen_exp = fastopen->cookie.exp ? 1 : 0;\n\t\t}\n\t}\n\n\tsmc_set_option(tp, opts, &remaining);\n\n\treturn MAX_TCP_OPTION_SPACE - remaining;\n}\n\n/* Set up TCP options for SYN-ACKs. */\nstatic unsigned int tcp_synack_options(const struct sock *sk,\n\t\t\t\t       struct request_sock *req,\n\t\t\t\t       unsigned int mss, struct sk_buff *skb,\n\t\t\t\t       struct tcp_out_options *opts,\n\t\t\t\t       const struct tcp_md5sig_key *md5,\n\t\t\t\t       struct tcp_fastopen_cookie *foc,\n\t\t\t\t       enum tcp_synack_type synack_type)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tunsigned int remaining = MAX_TCP_OPTION_SPACE;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (md5) {\n\t\topts->options |= OPTION_MD5;\n\t\tremaining -= TCPOLEN_MD5SIG_ALIGNED;\n\n\t\t/* We can't fit any SACK blocks in a packet with MD5 + TS\n\t\t * options. There was discussion about disabling SACK\n\t\t * rather than TS in order to fit in better with old,\n\t\t * buggy kernels, but that was deemed to be unnecessary.\n\t\t */\n\t\tif (synack_type != TCP_SYNACK_COOKIE)\n\t\t\tireq->tstamp_ok &= !ireq->sack_ok;\n\t}\n#endif\n\n\t/* We always send an MSS option. */\n\topts->mss = mss;\n\tremaining -= TCPOLEN_MSS_ALIGNED;\n\n\tif (likely(ireq->wscale_ok)) {\n\t\topts->ws = ireq->rcv_wscale;\n\t\topts->options |= OPTION_WSCALE;\n\t\tremaining -= TCPOLEN_WSCALE_ALIGNED;\n\t}\n\tif (likely(ireq->tstamp_ok)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = tcp_skb_timestamp(skb) + tcp_rsk(req)->ts_off;\n\t\topts->tsecr = req->ts_recent;\n\t\tremaining -= TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\tif (likely(ireq->sack_ok)) {\n\t\topts->options |= OPTION_SACK_ADVERTISE;\n\t\tif (unlikely(!ireq->tstamp_ok))\n\t\t\tremaining -= TCPOLEN_SACKPERM_ALIGNED;\n\t}\n\tif (foc != NULL && foc->len >= 0) {\n\t\tu32 need = foc->len;\n\n\t\tneed += foc->exp ? TCPOLEN_EXP_FASTOPEN_BASE :\n\t\t\t\t   TCPOLEN_FASTOPEN_BASE;\n\t\tneed = (need + 3) & ~3U;  /* Align to 32 bits */\n\t\tif (remaining >= need) {\n\t\t\topts->options |= OPTION_FAST_OPEN_COOKIE;\n\t\t\topts->fastopen_cookie = foc;\n\t\t\tremaining -= need;\n\t\t}\n\t}\n\n\tsmc_set_option_cond(tcp_sk(sk), ireq, opts, &remaining);\n\n\treturn MAX_TCP_OPTION_SPACE - remaining;\n}\n\n/* Compute TCP options for ESTABLISHED sockets. This is not the\n * final wire format yet.\n */\nstatic unsigned int tcp_established_options(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\t\tstruct tcp_md5sig_key **md5)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int size = 0;\n\tunsigned int eff_sacks;\n\n\topts->options = 0;\n\n\t*md5 = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (unlikely(rcu_access_pointer(tp->md5sig_info))) {\n\t\t*md5 = tp->af_specific->md5_lookup(sk, sk);\n\t\tif (*md5) {\n\t\t\topts->options |= OPTION_MD5;\n\t\t\tsize += TCPOLEN_MD5SIG_ALIGNED;\n\t\t}\n\t}\n#endif\n\n\tif (likely(tp->rx_opt.tstamp_ok)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = skb ? tcp_skb_timestamp(skb) + tp->tsoffset : 0;\n\t\topts->tsecr = tp->rx_opt.ts_recent;\n\t\tsize += TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\n\teff_sacks = tp->rx_opt.num_sacks + tp->rx_opt.dsack;\n\tif (unlikely(eff_sacks)) {\n\t\tconst unsigned int remaining = MAX_TCP_OPTION_SPACE - size;\n\t\topts->num_sack_blocks =\n\t\t\tmin_t(unsigned int, eff_sacks,\n\t\t\t      (remaining - TCPOLEN_SACK_BASE_ALIGNED) /\n\t\t\t      TCPOLEN_SACK_PERBLOCK);\n\t\tif (likely(opts->num_sack_blocks))\n\t\t\tsize += TCPOLEN_SACK_BASE_ALIGNED +\n\t\t\t\topts->num_sack_blocks * TCPOLEN_SACK_PERBLOCK;\n\t}\n\n\treturn size;\n}\n\n\n/* TCP SMALL QUEUES (TSQ)\n *\n * TSQ goal is to keep small amount of skbs per tcp flow in tx queues (qdisc+dev)\n * to reduce RTT and bufferbloat.\n * We do this using a special skb destructor (tcp_wfree).\n *\n * Its important tcp_wfree() can be replaced by sock_wfree() in the event skb\n * needs to be reallocated in a driver.\n * The invariant being skb->truesize subtracted from sk->sk_wmem_alloc\n *\n * Since transmit from skb destructor is forbidden, we use a tasklet\n * to process all sockets that eventually need to send more skbs.\n * We use one tasklet per cpu, with its own queue of sockets.\n */\nstruct tsq_tasklet {\n\tstruct tasklet_struct\ttasklet;\n\tstruct list_head\thead; /* queue of tcp sockets */\n};\nstatic DEFINE_PER_CPU(struct tsq_tasklet, tsq_tasklet);\n\nstatic void tcp_tsq_write(struct sock *sk)\n{\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_FIN_WAIT1 | TCPF_CLOSING |\n\t     TCPF_CLOSE_WAIT  | TCPF_LAST_ACK)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (tp->lost_out > tp->retrans_out &&\n\t\t    tp->snd_cwnd > tcp_packets_in_flight(tp)) {\n\t\t\ttcp_mstamp_refresh(tp);\n\t\t\ttcp_xmit_retransmit_queue(sk);\n\t\t}\n\n\t\ttcp_write_xmit(sk, tcp_current_mss(sk), tp->nonagle,\n\t\t\t       0, GFP_ATOMIC);\n\t}\n}\n\nstatic void tcp_tsq_handler(struct sock *sk)\n{\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\ttcp_tsq_write(sk);\n\telse if (!test_and_set_bit(TCP_TSQ_DEFERRED, &sk->sk_tsq_flags))\n\t\tsock_hold(sk);\n\tbh_unlock_sock(sk);\n}\n/*\n * One tasklet per cpu tries to send more skbs.\n * We run in tasklet context but need to disable irqs when\n * transferring tsq->head because tcp_wfree() might\n * interrupt us (non NAPI drivers)\n */\nstatic void tcp_tasklet_func(unsigned long data)\n{\n\tstruct tsq_tasklet *tsq = (struct tsq_tasklet *)data;\n\tLIST_HEAD(list);\n\tunsigned long flags;\n\tstruct list_head *q, *n;\n\tstruct tcp_sock *tp;\n\tstruct sock *sk;\n\n\tlocal_irq_save(flags);\n\tlist_splice_init(&tsq->head, &list);\n\tlocal_irq_restore(flags);\n\n\tlist_for_each_safe(q, n, &list) {\n\t\ttp = list_entry(q, struct tcp_sock, tsq_node);\n\t\tlist_del(&tp->tsq_node);\n\n\t\tsk = (struct sock *)tp;\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(TSQ_QUEUED, &sk->sk_tsq_flags);\n\n\t\ttcp_tsq_handler(sk);\n\t\tsk_free(sk);\n\t}\n}\n\n#define TCP_DEFERRED_ALL (TCPF_TSQ_DEFERRED |\t\t\\\n\t\t\t  TCPF_WRITE_TIMER_DEFERRED |\t\\\n\t\t\t  TCPF_DELACK_TIMER_DEFERRED |\t\\\n\t\t\t  TCPF_MTU_REDUCED_DEFERRED)\n/**\n * tcp_release_cb - tcp release_sock() callback\n * @sk: socket\n *\n * called from release_sock() to perform protocol dependent\n * actions before socket release.\n */\nvoid tcp_release_cb(struct sock *sk)\n{\n\tunsigned long flags, nflags;\n\n\t/* perform an atomic operation only if at least one flag is set */\n\tdo {\n\t\tflags = sk->sk_tsq_flags;\n\t\tif (!(flags & TCP_DEFERRED_ALL))\n\t\t\treturn;\n\t\tnflags = flags & ~TCP_DEFERRED_ALL;\n\t} while (cmpxchg(&sk->sk_tsq_flags, flags, nflags) != flags);\n\n\tif (flags & TCPF_TSQ_DEFERRED) {\n\t\ttcp_tsq_write(sk);\n\t\t__sock_put(sk);\n\t}\n\t/* Here begins the tricky part :\n\t * We are called from release_sock() with :\n\t * 1) BH disabled\n\t * 2) sk_lock.slock spinlock held\n\t * 3) socket owned by us (sk->sk_lock.owned == 1)\n\t *\n\t * But following code is meant to be called from BH handlers,\n\t * so we should keep BH disabled, but early release socket ownership\n\t */\n\tsock_release_ownership(sk);\n\n\tif (flags & TCPF_WRITE_TIMER_DEFERRED) {\n\t\ttcp_write_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_DELACK_TIMER_DEFERRED) {\n\t\ttcp_delack_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_MTU_REDUCED_DEFERRED) {\n\t\tinet_csk(sk)->icsk_af_ops->mtu_reduced(sk);\n\t\t__sock_put(sk);\n\t}\n}\nEXPORT_SYMBOL(tcp_release_cb);\n\nvoid __init tcp_tasklet_init(void)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct tsq_tasklet *tsq = &per_cpu(tsq_tasklet, i);\n\n\t\tINIT_LIST_HEAD(&tsq->head);\n\t\ttasklet_init(&tsq->tasklet,\n\t\t\t     tcp_tasklet_func,\n\t\t\t     (unsigned long)tsq);\n\t}\n}\n\n/*\n * Write buffer destructor automatically called from kfree_skb.\n * We can't xmit new skbs from this context, as we might already\n * hold qdisc lock.\n */\nvoid tcp_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long flags, nval, oval;\n\n\t/* Keep one reference on sk_wmem_alloc.\n\t * Will be released by sk_free() from here or tcp_tasklet_func()\n\t */\n\tWARN_ON(refcount_sub_and_test(skb->truesize - 1, &sk->sk_wmem_alloc));\n\n\t/* If this softirq is serviced by ksoftirqd, we are likely under stress.\n\t * Wait until our queues (qdisc + devices) are drained.\n\t * This gives :\n\t * - less callbacks to tcp_write_xmit(), reducing stress (batches)\n\t * - chance for incoming ACK (processed by another cpu maybe)\n\t *   to migrate this flow (skb->ooo_okay will be eventually set)\n\t */\n\tif (refcount_read(&sk->sk_wmem_alloc) >= SKB_TRUESIZE(1) && this_cpu_ksoftirqd() == current)\n\t\tgoto out;\n\n\tfor (oval = READ_ONCE(sk->sk_tsq_flags);; oval = nval) {\n\t\tstruct tsq_tasklet *tsq;\n\t\tbool empty;\n\n\t\tif (!(oval & TSQF_THROTTLED) || (oval & TSQF_QUEUED))\n\t\t\tgoto out;\n\n\t\tnval = (oval & ~TSQF_THROTTLED) | TSQF_QUEUED;\n\t\tnval = cmpxchg(&sk->sk_tsq_flags, oval, nval);\n\t\tif (nval != oval)\n\t\t\tcontinue;\n\n\t\t/* queue this socket to tasklet queue */\n\t\tlocal_irq_save(flags);\n\t\ttsq = this_cpu_ptr(&tsq_tasklet);\n\t\tempty = list_empty(&tsq->head);\n\t\tlist_add(&tp->tsq_node, &tsq->head);\n\t\tif (empty)\n\t\t\ttasklet_schedule(&tsq->tasklet);\n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\nout:\n\tsk_free(sk);\n}\n\n/* Note: Called under soft irq.\n * We can call TCP stack right away, unless socket is owned by user.\n */\nenum hrtimer_restart tcp_pace_kick(struct hrtimer *timer)\n{\n\tstruct tcp_sock *tp = container_of(timer, struct tcp_sock, pacing_timer);\n\tstruct sock *sk = (struct sock *)tp;\n\n\ttcp_tsq_handler(sk);\n\tsock_put(sk);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void tcp_internal_pacing(struct sock *sk, const struct sk_buff *skb)\n{\n\tu64 len_ns;\n\tu32 rate;\n\n\tif (!tcp_needs_internal_pacing(sk))\n\t\treturn;\n\trate = sk->sk_pacing_rate;\n\tif (!rate || rate == ~0U)\n\t\treturn;\n\n\tlen_ns = (u64)skb->len * NSEC_PER_SEC;\n\tdo_div(len_ns, rate);\n\thrtimer_start(&tcp_sk(sk)->pacing_timer,\n\t\t      ktime_add_ns(ktime_get(), len_ns),\n\t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\n\tsock_hold(sk);\n}\n\nstatic void tcp_update_skb_after_send(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tskb->skb_mstamp = tp->tcp_mstamp;\n\tlist_move_tail(&skb->tcp_tsorted_anchor, &tp->tsorted_sent_queue);\n}\n\n/* This routine actually transmits TCP packets queued in by\n * tcp_do_sendmsg().  This is used by both the initial\n * transmission and possible later retransmissions.\n * All SKB's seen here are completely headerless.  It is our\n * job to build the TCP header, and pass the packet down to\n * IP so it can do the same plus pass the packet off to the\n * device.\n *\n * We are working here with either a clone of the original\n * SKB, or a fresh unique copy made by the retransmit engine.\n */\nstatic int __tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,\n\t\t\t      int clone_it, gfp_t gfp_mask, u32 rcv_nxt)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet;\n\tstruct tcp_sock *tp;\n\tstruct tcp_skb_cb *tcb;\n\tstruct tcp_out_options opts;\n\tunsigned int tcp_options_size, tcp_header_size;\n\tstruct sk_buff *oskb = NULL;\n\tstruct tcp_md5sig_key *md5;\n\tstruct tcphdr *th;\n\tint err;\n\n\tBUG_ON(!skb || !tcp_skb_pcount(skb));\n\ttp = tcp_sk(sk);\n\n\tif (clone_it) {\n\t\tTCP_SKB_CB(skb)->tx.in_flight = TCP_SKB_CB(skb)->end_seq\n\t\t\t- tp->snd_una;\n\t\toskb = skb;\n\n\t\ttcp_skb_tsorted_save(oskb) {\n\t\t\tif (unlikely(skb_cloned(oskb)))\n\t\t\t\tskb = pskb_copy(oskb, gfp_mask);\n\t\t\telse\n\t\t\t\tskb = skb_clone(oskb, gfp_mask);\n\t\t} tcp_skb_tsorted_restore(oskb);\n\n\t\tif (unlikely(!skb))\n\t\t\treturn -ENOBUFS;\n\t}\n\tskb->skb_mstamp = tp->tcp_mstamp;\n\n\tinet = inet_sk(sk);\n\ttcb = TCP_SKB_CB(skb);\n\tmemset(&opts, 0, sizeof(opts));\n\n\tif (unlikely(tcb->tcp_flags & TCPHDR_SYN))\n\t\ttcp_options_size = tcp_syn_options(sk, skb, &opts, &md5);\n\telse\n\t\ttcp_options_size = tcp_established_options(sk, skb, &opts,\n\t\t\t\t\t\t\t   &md5);\n\ttcp_header_size = tcp_options_size + sizeof(struct tcphdr);\n\n\t/* if no packet is in qdisc/device queue, then allow XPS to select\n\t * another queue. We can be called from tcp_tsq_handler()\n\t * which holds one reference to sk.\n\t *\n\t * TODO: Ideally, in-flight pure ACK packets should not matter here.\n\t * One way to get this would be to set skb->truesize = 2 on them.\n\t */\n\tskb->ooo_okay = sk_wmem_alloc_get(sk) < SKB_TRUESIZE(1);\n\n\t/* If we had to use memory reserve to allocate this skb,\n\t * this might cause drops if packet is looped back :\n\t * Other socket might not have SOCK_MEMALLOC.\n\t * Packets not looped back do not care about pfmemalloc.\n\t */\n\tskb->pfmemalloc = 0;\n\n\tskb_push(skb, tcp_header_size);\n\tskb_reset_transport_header(skb);\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = skb_is_tcp_pure_ack(skb) ? __sock_wfree : tcp_wfree;\n\tskb_set_hash_from_sk(skb, sk);\n\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\n\tskb_set_dst_pending_confirm(skb, sk->sk_dst_pending_confirm);\n\n\t/* Build TCP header and checksum it. */\n\tth = (struct tcphdr *)skb->data;\n\tth->source\t\t= inet->inet_sport;\n\tth->dest\t\t= inet->inet_dport;\n\tth->seq\t\t\t= htonl(tcb->seq);\n\tth->ack_seq\t\t= htonl(rcv_nxt);\n\t*(((__be16 *)th) + 6)\t= htons(((tcp_header_size >> 2) << 12) |\n\t\t\t\t\ttcb->tcp_flags);\n\n\tth->check\t\t= 0;\n\tth->urg_ptr\t\t= 0;\n\n\t/* The urg_mode check is necessary during a below snd_una win probe */\n\tif (unlikely(tcp_urg_mode(tp) && before(tcb->seq, tp->snd_up))) {\n\t\tif (before(tp->snd_up, tcb->seq + 0x10000)) {\n\t\t\tth->urg_ptr = htons(tp->snd_up - tcb->seq);\n\t\t\tth->urg = 1;\n\t\t} else if (after(tcb->seq + 0xFFFF, tp->snd_nxt)) {\n\t\t\tth->urg_ptr = htons(0xFFFF);\n\t\t\tth->urg = 1;\n\t\t}\n\t}\n\n\ttcp_options_write((__be32 *)(th + 1), tp, &opts);\n\tskb_shinfo(skb)->gso_type = sk->sk_gso_type;\n\tif (likely(!(tcb->tcp_flags & TCPHDR_SYN))) {\n\t\tth->window      = htons(tcp_select_window(sk));\n\t\ttcp_ecn_send(sk, skb, th, tcp_header_size);\n\t} else {\n\t\t/* RFC1323: The window in SYN & SYN/ACK segments\n\t\t * is never scaled.\n\t\t */\n\t\tth->window\t= htons(min(tp->rcv_wnd, 65535U));\n\t}\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Calculate the MD5 hash, as we have all we need now */\n\tif (md5) {\n\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t\ttp->af_specific->calc_md5_hash(opts.hash_location,\n\t\t\t\t\t       md5, sk, skb);\n\t}\n#endif\n\n\ticsk->icsk_af_ops->send_check(sk, skb);\n\n\tif (likely(tcb->tcp_flags & TCPHDR_ACK))\n\t\ttcp_event_ack_sent(sk, tcp_skb_pcount(skb), rcv_nxt);\n\n\tif (skb->len != tcp_header_size) {\n\t\ttcp_event_data_sent(tp, sk);\n\t\ttp->data_segs_out += tcp_skb_pcount(skb);\n\t\ttp->bytes_sent += skb->len - tcp_header_size;\n\t\ttcp_internal_pacing(sk, skb);\n\t}\n\n\tif (after(tcb->end_seq, tp->snd_nxt) || tcb->seq == tcb->end_seq)\n\t\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS,\n\t\t\t      tcp_skb_pcount(skb));\n\n\ttp->segs_out += tcp_skb_pcount(skb);\n\t/* OK, its time to fill skb_shinfo(skb)->gso_{segs|size} */\n\tskb_shinfo(skb)->gso_segs = tcp_skb_pcount(skb);\n\tskb_shinfo(skb)->gso_size = tcp_skb_mss(skb);\n\n\t/* Our usage of tstamp should remain private */\n\tskb->tstamp = 0;\n\n\t/* Cleanup our debris for IP stacks */\n\tmemset(skb->cb, 0, max(sizeof(struct inet_skb_parm),\n\t\t\t       sizeof(struct inet6_skb_parm)));\n\n\terr = icsk->icsk_af_ops->queue_xmit(sk, skb, &inet->cork.fl);\n\n\tif (unlikely(err > 0)) {\n\t\ttcp_enter_cwr(sk);\n\t\terr = net_xmit_eval(err);\n\t}\n\tif (!err && oskb) {\n\t\ttcp_update_skb_after_send(tp, oskb);\n\t\ttcp_rate_skb_sent(sk, oskb);\n\t}\n\treturn err;\n}\n\nstatic int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,\n\t\t\t    gfp_t gfp_mask)\n{\n\treturn __tcp_transmit_skb(sk, skb, clone_it, gfp_mask,\n\t\t\t\t  tcp_sk(sk)->rcv_nxt);\n}\n\n/* This routine just queues the buffer for sending.\n *\n * NOTE: probe0 timer is not checked, do not forget tcp_push_pending_frames,\n * otherwise socket can stall.\n */\nstatic void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Advance write_seq and place onto the write_queue. */\n\tWRITE_ONCE(tp->write_seq, TCP_SKB_CB(skb)->end_seq);\n\t__skb_header_release(skb);\n\ttcp_add_write_queue_tail(sk, skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n}\n\n/* Initialize TSO segments for a packet. */\nstatic void tcp_set_skb_tso_segs(struct sk_buff *skb, unsigned int mss_now)\n{\n\tif (skb->len <= mss_now) {\n\t\t/* Avoid the costly divide in the normal\n\t\t * non-TSO case.\n\t\t */\n\t\ttcp_skb_pcount_set(skb, 1);\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = 0;\n\t} else {\n\t\ttcp_skb_pcount_set(skb, DIV_ROUND_UP(skb->len, mss_now));\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = mss_now;\n\t}\n}\n\n/* Pcount in the middle of the write queue got changed, we need to do various\n * tweaks to fix counters\n */\nstatic void tcp_adjust_pcount(struct sock *sk, const struct sk_buff *skb, int decr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->packets_out -= decr;\n\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\ttp->sacked_out -= decr;\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS)\n\t\ttp->retrans_out -= decr;\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_LOST)\n\t\ttp->lost_out -= decr;\n\n\t/* Reno case is special. Sigh... */\n\tif (tcp_is_reno(tp) && decr > 0)\n\t\ttp->sacked_out -= min_t(u32, tp->sacked_out, decr);\n\n\tif (tp->lost_skb_hint &&\n\t    before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(tp->lost_skb_hint)->seq) &&\n\t    (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED))\n\t\ttp->lost_cnt_hint -= decr;\n\n\ttcp_verify_left_out(tp);\n}\n\nstatic bool tcp_has_tx_tstamp(const struct sk_buff *skb)\n{\n\treturn TCP_SKB_CB(skb)->txstamp_ack ||\n\t\t(skb_shinfo(skb)->tx_flags & SKBTX_ANY_TSTAMP);\n}\n\nstatic void tcp_fragment_tstamp(struct sk_buff *skb, struct sk_buff *skb2)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (unlikely(tcp_has_tx_tstamp(skb)) &&\n\t    !before(shinfo->tskey, TCP_SKB_CB(skb2)->seq)) {\n\t\tstruct skb_shared_info *shinfo2 = skb_shinfo(skb2);\n\t\tu8 tsflags = shinfo->tx_flags & SKBTX_ANY_TSTAMP;\n\n\t\tshinfo->tx_flags &= ~tsflags;\n\t\tshinfo2->tx_flags |= tsflags;\n\t\tswap(shinfo->tskey, shinfo2->tskey);\n\t\tTCP_SKB_CB(skb2)->txstamp_ack = TCP_SKB_CB(skb)->txstamp_ack;\n\t\tTCP_SKB_CB(skb)->txstamp_ack = 0;\n\t}\n}\n\nstatic void tcp_skb_fragment_eor(struct sk_buff *skb, struct sk_buff *skb2)\n{\n\tTCP_SKB_CB(skb2)->eor = TCP_SKB_CB(skb)->eor;\n\tTCP_SKB_CB(skb)->eor = 0;\n}\n\n/* Insert buff after skb on the write or rtx queue of sk.  */\nstatic void tcp_insert_write_queue_after(struct sk_buff *skb,\n\t\t\t\t\t struct sk_buff *buff,\n\t\t\t\t\t struct sock *sk,\n\t\t\t\t\t enum tcp_queue tcp_queue)\n{\n\tif (tcp_queue == TCP_FRAG_IN_WRITE_QUEUE)\n\t\t__skb_queue_after(&sk->sk_write_queue, skb, buff);\n\telse\n\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, buff);\n}\n\n/* Function to create two new TCP segments.  Shrinks the given segment\n * to the specified size and appends a new segment with the rest of the\n * packet to the list.  This won't be called frequently, I hope.\n * Remember, these are still headerless SKBs at this point.\n */\nint tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,\n\t\t struct sk_buff *skb, u32 len,\n\t\t unsigned int mss_now, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint nsize, old_factor;\n\tlong limit;\n\tint nlen;\n\tu8 flags;\n\n\tif (WARN_ON(len > skb->len))\n\t\treturn -EINVAL;\n\n\tnsize = skb_headlen(skb) - len;\n\tif (nsize < 0)\n\t\tnsize = 0;\n\n\t/* tcp_sendmsg() can overshoot sk_wmem_queued by one full size skb.\n\t * We need some allowance to not penalize applications setting small\n\t * SO_SNDBUF values.\n\t * Also allow first and last skb in retransmit queue to be split.\n\t */\n\tlimit = sk->sk_sndbuf + 2 * SKB_TRUESIZE(GSO_MAX_SIZE);\n\tif (unlikely((sk->sk_wmem_queued >> 1) > limit &&\n\t\t     tcp_queue != TCP_FRAG_IN_WRITE_QUEUE &&\n\t\t     skb != tcp_rtx_queue_head(sk) &&\n\t\t     skb != tcp_rtx_queue_tail(sk))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPWQUEUETOOBIG);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (skb_unclone(skb, gfp))\n\t\treturn -ENOMEM;\n\n\t/* Get a new skb... force flag on. */\n\tbuff = sk_stream_alloc_skb(sk, nsize, gfp, true);\n\tif (!buff)\n\t\treturn -ENOMEM; /* We'll just try again later. */\n\n\tsk->sk_wmem_queued += buff->truesize;\n\tsk_mem_charge(sk, buff->truesize);\n\tnlen = skb->len - len - nsize;\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t/* Correct the sequence numbers. */\n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t/* PSH and FIN should only be set in the second packet. */\n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\tTCP_SKB_CB(buff)->sacked = TCP_SKB_CB(skb)->sacked;\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tskb_split(skb, buff, len);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\n\tbuff->tstamp = skb->tstamp;\n\ttcp_fragment_tstamp(skb, buff);\n\n\told_factor = tcp_skb_pcount(skb);\n\n\t/* Fix up tso_factor for both original and new SKB.  */\n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t/* Update delivered info for the new segment */\n\tTCP_SKB_CB(buff)->tx = TCP_SKB_CB(skb)->tx;\n\n\t/* If this packet has been sent out already, we must\n\t * adjust the various packet counters.\n\t */\n\tif (!before(tp->snd_nxt, TCP_SKB_CB(buff)->end_seq)) {\n\t\tint diff = old_factor - tcp_skb_pcount(skb) -\n\t\t\ttcp_skb_pcount(buff);\n\n\t\tif (diff)\n\t\t\ttcp_adjust_pcount(sk, skb, diff);\n\t}\n\n\t/* Link BUFF into the send queue. */\n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, tcp_queue);\n\tif (tcp_queue == TCP_FRAG_IN_RTX_QUEUE)\n\t\tlist_add(&buff->tcp_tsorted_anchor, &skb->tcp_tsorted_anchor);\n\n\treturn 0;\n}\n\n/* This is similar to __pskb_pull_tail(). The difference is that pulled\n * data is not copied, but immediately discarded.\n */\nstatic int __pskb_trim_head(struct sk_buff *skb, int len)\n{\n\tstruct skb_shared_info *shinfo;\n\tint i, k, eat;\n\n\teat = min_t(int, len, skb_headlen(skb));\n\tif (eat) {\n\t\t__skb_pull(skb, eat);\n\t\tlen -= eat;\n\t\tif (!len)\n\t\t\treturn 0;\n\t}\n\teat = len;\n\tk = 0;\n\tshinfo = skb_shinfo(skb);\n\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\tint size = skb_frag_size(&shinfo->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tshinfo->frags[k] = shinfo->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tshinfo->frags[k].page_offset += eat;\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[k], eat);\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tshinfo->nr_frags = k;\n\n\tskb->data_len -= len;\n\tskb->len = skb->data_len;\n\treturn len;\n}\n\n/* Remove acked data from a packet in the transmit queue. */\nint tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)\n{\n\tu32 delta_truesize;\n\n\tif (skb_unclone(skb, GFP_ATOMIC))\n\t\treturn -ENOMEM;\n\n\tdelta_truesize = __pskb_trim_head(skb, len);\n\n\tTCP_SKB_CB(skb)->seq += len;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tif (delta_truesize) {\n\t\tskb->truesize\t   -= delta_truesize;\n\t\tsk->sk_wmem_queued -= delta_truesize;\n\t\tsk_mem_uncharge(sk, delta_truesize);\n\t\tsock_set_flag(sk, SOCK_QUEUE_SHRUNK);\n\t}\n\n\t/* Any change of skb->len requires recalculation of tso factor. */\n\tif (tcp_skb_pcount(skb) > 1)\n\t\ttcp_set_skb_tso_segs(skb, tcp_skb_mss(skb));\n\n\treturn 0;\n}\n\n/* Calculate MSS not accounting any TCP options.  */\nstatic inline int __tcp_mtu_to_mss(struct sock *sk, int pmtu)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tint mss_now;\n\n\t/* Calculate base mss without TCP options:\n\t   It is MMS_S - sizeof(tcphdr) of rfc1122\n\t */\n\tmss_now = pmtu - icsk->icsk_af_ops->net_header_len - sizeof(struct tcphdr);\n\n\t/* IPv6 adds a frag_hdr in case RTAX_FEATURE_ALLFRAG is set */\n\tif (icsk->icsk_af_ops->net_frag_header_len) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_allfrag(dst))\n\t\t\tmss_now -= icsk->icsk_af_ops->net_frag_header_len;\n\t}\n\n\t/* Clamp it (mss_clamp does not include tcp options) */\n\tif (mss_now > tp->rx_opt.mss_clamp)\n\t\tmss_now = tp->rx_opt.mss_clamp;\n\n\t/* Now subtract optional transport overhead */\n\tmss_now -= icsk->icsk_ext_hdr_len;\n\n\t/* Then reserve room for full set of TCP options and 8 bytes of data */\n\tmss_now = max(mss_now, sock_net(sk)->ipv4.sysctl_tcp_min_snd_mss);\n\treturn mss_now;\n}\n\n/* Calculate MSS. Not accounting for SACKs here.  */\nint tcp_mtu_to_mss(struct sock *sk, int pmtu)\n{\n\t/* Subtract TCP options size, not including SACKs */\n\treturn __tcp_mtu_to_mss(sk, pmtu) -\n\t       (tcp_sk(sk)->tcp_header_len - sizeof(struct tcphdr));\n}\nEXPORT_SYMBOL(tcp_mtu_to_mss);\n\n/* Inverse of above */\nint tcp_mss_to_mtu(struct sock *sk, int mss)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tint mtu;\n\n\tmtu = mss +\n\t      tp->tcp_header_len +\n\t      icsk->icsk_ext_hdr_len +\n\t      icsk->icsk_af_ops->net_header_len;\n\n\t/* IPv6 adds a frag_hdr in case RTAX_FEATURE_ALLFRAG is set */\n\tif (icsk->icsk_af_ops->net_frag_header_len) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_allfrag(dst))\n\t\t\tmtu += icsk->icsk_af_ops->net_frag_header_len;\n\t}\n\treturn mtu;\n}\nEXPORT_SYMBOL(tcp_mss_to_mtu);\n\n/* MTU probing init per socket */\nvoid tcp_mtup_init(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\n\ticsk->icsk_mtup.enabled = net->ipv4.sysctl_tcp_mtu_probing > 1;\n\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp + sizeof(struct tcphdr) +\n\t\t\t       icsk->icsk_af_ops->net_header_len;\n\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, net->ipv4.sysctl_tcp_base_mss);\n\ticsk->icsk_mtup.probe_size = 0;\n\tif (icsk->icsk_mtup.enabled)\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n}\nEXPORT_SYMBOL(tcp_mtup_init);\n\n/* This function synchronize snd mss to current pmtu/exthdr set.\n\n   tp->rx_opt.user_mss is mss set by user by TCP_MAXSEG. It does NOT counts\n   for TCP options, but includes only bare TCP header.\n\n   tp->rx_opt.mss_clamp is mss negotiated at connection setup.\n   It is minimum of user_mss and mss received with SYN.\n   It also does not include TCP options.\n\n   inet_csk(sk)->icsk_pmtu_cookie is last pmtu, seen by this function.\n\n   tp->mss_cache is current effective sending mss, including\n   all tcp options except for SACKs. It is evaluated,\n   taking into account current pmtu, but never exceeds\n   tp->rx_opt.mss_clamp.\n\n   NOTE1. rfc1122 clearly states that advertised MSS\n   DOES NOT include either tcp or ip options.\n\n   NOTE2. inet_csk(sk)->icsk_pmtu_cookie and tp->mss_cache\n   are READ ONLY outside this function.\t\t--ANK (980731)\n */\nunsigned int tcp_sync_mss(struct sock *sk, u32 pmtu)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint mss_now;\n\n\tif (icsk->icsk_mtup.search_high > pmtu)\n\t\ticsk->icsk_mtup.search_high = pmtu;\n\n\tmss_now = tcp_mtu_to_mss(sk, pmtu);\n\tmss_now = tcp_bound_to_half_wnd(tp, mss_now);\n\n\t/* And store cached results */\n\ticsk->icsk_pmtu_cookie = pmtu;\n\tif (icsk->icsk_mtup.enabled)\n\t\tmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low));\n\ttp->mss_cache = mss_now;\n\n\treturn mss_now;\n}\nEXPORT_SYMBOL(tcp_sync_mss);\n\n/* Compute the current effective MSS, taking SACKs and IP options,\n * and even PMTU discovery events into account.\n */\nunsigned int tcp_current_mss(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tu32 mss_now;\n\tunsigned int header_len;\n\tstruct tcp_out_options opts;\n\tstruct tcp_md5sig_key *md5;\n\n\tmss_now = tp->mss_cache;\n\n\tif (dst) {\n\t\tu32 mtu = dst_mtu(dst);\n\t\tif (mtu != inet_csk(sk)->icsk_pmtu_cookie)\n\t\t\tmss_now = tcp_sync_mss(sk, mtu);\n\t}\n\n\theader_len = tcp_established_options(sk, NULL, &opts, &md5) +\n\t\t     sizeof(struct tcphdr);\n\t/* The mss_cache is sized based on tp->tcp_header_len, which assumes\n\t * some common options. If this is an odd packet (because we have SACK\n\t * blocks etc) then our calculated header_len will be different, and\n\t * we have to adjust mss_now correspondingly */\n\tif (header_len != tp->tcp_header_len) {\n\t\tint delta = (int) header_len - tp->tcp_header_len;\n\t\tmss_now -= delta;\n\t}\n\n\treturn mss_now;\n}\n\n/* RFC2861, slow part. Adjust cwnd, after it was not full during one rto.\n * As additional protections, we do not touch cwnd in retransmission phases,\n * and if application hit its sndbuf limit recently.\n */\nstatic void tcp_cwnd_application_limited(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Open &&\n\t    sk->sk_socket && !test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\t/* Limited by application or receiver window. */\n\t\tu32 init_win = tcp_init_cwnd(tp, __sk_dst_get(sk));\n\t\tu32 win_used = max(tp->snd_cwnd_used, init_win);\n\t\tif (win_used < tp->snd_cwnd) {\n\t\t\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\t\t\ttp->snd_cwnd = (tp->snd_cwnd + win_used) >> 1;\n\t\t}\n\t\ttp->snd_cwnd_used = 0;\n\t}\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n}\n\nstatic void tcp_cwnd_validate(struct sock *sk, bool is_cwnd_limited)\n{\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Track the maximum number of outstanding packets in each\n\t * window, and remember whether we were cwnd-limited then.\n\t */\n\tif (!before(tp->snd_una, tp->max_packets_seq) ||\n\t    tp->packets_out > tp->max_packets_out ||\n\t    is_cwnd_limited) {\n\t\ttp->max_packets_out = tp->packets_out;\n\t\ttp->max_packets_seq = tp->snd_nxt;\n\t\ttp->is_cwnd_limited = is_cwnd_limited;\n\t}\n\n\tif (tcp_is_cwnd_limited(sk)) {\n\t\t/* Network is feed fully. */\n\t\ttp->snd_cwnd_used = 0;\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t} else {\n\t\t/* Network starves. */\n\t\tif (tp->packets_out > tp->snd_cwnd_used)\n\t\t\ttp->snd_cwnd_used = tp->packets_out;\n\n\t\tif (sock_net(sk)->ipv4.sysctl_tcp_slow_start_after_idle &&\n\t\t    (s32)(tcp_jiffies32 - tp->snd_cwnd_stamp) >= inet_csk(sk)->icsk_rto &&\n\t\t    !ca_ops->cong_control)\n\t\t\ttcp_cwnd_application_limited(sk);\n\n\t\t/* The following conditions together indicate the starvation\n\t\t * is caused by insufficient sender buffer:\n\t\t * 1) just sent some data (see tcp_write_xmit)\n\t\t * 2) not cwnd limited (this else condition)\n\t\t * 3) no more data to send (tcp_write_queue_empty())\n\t\t * 4) application is hitting buffer limit (SOCK_NOSPACE)\n\t\t */\n\t\tif (tcp_write_queue_empty(sk) && sk->sk_socket &&\n\t\t    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags) &&\n\t\t    (1 << sk->sk_state) & (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))\n\t\t\ttcp_chrono_start(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n}\n\n/* Minshall's variant of the Nagle send check. */\nstatic bool tcp_minshall_check(const struct tcp_sock *tp)\n{\n\treturn after(tp->snd_sml, tp->snd_una) &&\n\t\t!after(tp->snd_sml, tp->snd_nxt);\n}\n\n/* Update snd_sml if this skb is under mss\n * Note that a TSO packet might end with a sub-mss segment\n * The test is really :\n * if ((skb->len % mss) != 0)\n *        tp->snd_sml = TCP_SKB_CB(skb)->end_seq;\n * But we can avoid doing the divide again given we already have\n *  skb_pcount = skb->len / mss_now\n */\nstatic void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss_now,\n\t\t\t\tconst struct sk_buff *skb)\n{\n\tif (skb->len < tcp_skb_pcount(skb) * mss_now)\n\t\ttp->snd_sml = TCP_SKB_CB(skb)->end_seq;\n}\n\n/* Return false, if packet can be sent now without violation Nagle's rules:\n * 1. It is full sized. (provided by caller in %partial bool)\n * 2. Or it contains FIN. (already checked by caller)\n * 3. Or TCP_CORK is not set, and TCP_NODELAY is set.\n * 4. Or TCP_CORK is not set, and all sent packets are ACKed.\n *    With Minshall's modification: all sent small packets are ACKed.\n */\nstatic bool tcp_nagle_check(bool partial, const struct tcp_sock *tp,\n\t\t\t    int nonagle)\n{\n\treturn partial &&\n\t\t((nonagle & TCP_NAGLE_CORK) ||\n\t\t (!nonagle && tp->packets_out && tcp_minshall_check(tp)));\n}\n\n/* Return how many segs we'd like on a TSO packet,\n * to send one TSO packet per ms\n */\nstatic u32 tcp_tso_autosize(const struct sock *sk, unsigned int mss_now,\n\t\t\t    int min_tso_segs)\n{\n\tu32 bytes, segs;\n\n\tbytes = min(sk->sk_pacing_rate >> sk->sk_pacing_shift,\n\t\t    sk->sk_gso_max_size - 1 - MAX_TCP_HEADER);\n\n\t/* Goal is to send at least one packet per ms,\n\t * not one big TSO packet every 100 ms.\n\t * This preserves ACK clocking and is consistent\n\t * with tcp_tso_should_defer() heuristic.\n\t */\n\tsegs = max_t(u32, bytes / mss_now, min_tso_segs);\n\n\treturn segs;\n}\n\n/* Return the number of segments we want in the skb we are transmitting.\n * See if congestion control module wants to decide; otherwise, autosize.\n */\nstatic u32 tcp_tso_segs(struct sock *sk, unsigned int mss_now)\n{\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tu32 min_tso, tso_segs;\n\n\tmin_tso = ca_ops->min_tso_segs ?\n\t\t\tca_ops->min_tso_segs(sk) :\n\t\t\tsock_net(sk)->ipv4.sysctl_tcp_min_tso_segs;\n\n\ttso_segs = tcp_tso_autosize(sk, mss_now, min_tso);\n\treturn min_t(u32, tso_segs, sk->sk_gso_max_segs);\n}\n\n/* Returns the portion of skb which can be sent right away */\nstatic unsigned int tcp_mss_split_point(const struct sock *sk,\n\t\t\t\t\tconst struct sk_buff *skb,\n\t\t\t\t\tunsigned int mss_now,\n\t\t\t\t\tunsigned int max_segs,\n\t\t\t\t\tint nonagle)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu32 partial, needed, window, max_len;\n\n\twindow = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\tmax_len = mss_now * max_segs;\n\n\tif (likely(max_len <= window && skb != tcp_write_queue_tail(sk)))\n\t\treturn max_len;\n\n\tneeded = min(skb->len, window);\n\n\tif (max_len <= needed)\n\t\treturn max_len;\n\n\tpartial = needed % mss_now;\n\t/* If last segment is not a full MSS, check if Nagle rules allow us\n\t * to include this last segment in this skb.\n\t * Otherwise, we'll split the skb at last MSS boundary\n\t */\n\tif (tcp_nagle_check(partial != 0, tp, nonagle))\n\t\treturn needed - partial;\n\n\treturn needed;\n}\n\n/* Can at least one segment of SKB be sent right now, according to the\n * congestion window rules?  If so, return how many segments are allowed.\n */\nstatic inline unsigned int tcp_cwnd_test(const struct tcp_sock *tp,\n\t\t\t\t\t const struct sk_buff *skb)\n{\n\tu32 in_flight, cwnd, halfcwnd;\n\n\t/* Don't be strict about the congestion window for the final FIN.  */\n\tif ((TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) &&\n\t    tcp_skb_pcount(skb) == 1)\n\t\treturn 1;\n\n\tin_flight = tcp_packets_in_flight(tp);\n\tcwnd = tp->snd_cwnd;\n\tif (in_flight >= cwnd)\n\t\treturn 0;\n\n\t/* For better scheduling, ensure we have at least\n\t * 2 GSO packets in flight.\n\t */\n\thalfcwnd = max(cwnd >> 1, 1U);\n\treturn min(halfcwnd, cwnd - in_flight);\n}\n\n/* Initialize TSO state of a skb.\n * This must be invoked the first time we consider transmitting\n * SKB onto the wire.\n */\nstatic int tcp_init_tso_segs(struct sk_buff *skb, unsigned int mss_now)\n{\n\tint tso_segs = tcp_skb_pcount(skb);\n\n\tif (!tso_segs || (tso_segs > 1 && tcp_skb_mss(skb) != mss_now)) {\n\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\ttso_segs = tcp_skb_pcount(skb);\n\t}\n\treturn tso_segs;\n}\n\n\n/* Return true if the Nagle test allows this packet to be\n * sent now.\n */\nstatic inline bool tcp_nagle_test(const struct tcp_sock *tp, const struct sk_buff *skb,\n\t\t\t\t  unsigned int cur_mss, int nonagle)\n{\n\t/* Nagle rule does not apply to frames, which sit in the middle of the\n\t * write_queue (they have no chances to get new data).\n\t *\n\t * This is implemented in the callers, where they modify the 'nonagle'\n\t * argument based upon the location of SKB in the send queue.\n\t */\n\tif (nonagle & TCP_NAGLE_PUSH)\n\t\treturn true;\n\n\t/* Don't use the nagle rule for urgent data (or for the final FIN). */\n\tif (tcp_urg_mode(tp) || (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN))\n\t\treturn true;\n\n\tif (!tcp_nagle_check(skb->len < cur_mss, tp, nonagle))\n\t\treturn true;\n\n\treturn false;\n}\n\n/* Does at least the first segment of SKB fit into the send window? */\nstatic bool tcp_snd_wnd_test(const struct tcp_sock *tp,\n\t\t\t     const struct sk_buff *skb,\n\t\t\t     unsigned int cur_mss)\n{\n\tu32 end_seq = TCP_SKB_CB(skb)->end_seq;\n\n\tif (skb->len > cur_mss)\n\t\tend_seq = TCP_SKB_CB(skb)->seq + cur_mss;\n\n\treturn !after(end_seq, tcp_wnd_end(tp));\n}\n\n/* Trim TSO SKB to LEN bytes, put the remaining data into a new packet\n * which is put after SKB on the list.  It is very much like\n * tcp_fragment() except that it may make several kinds of assumptions\n * in order to speed up the splitting operation.  In particular, we\n * know that all the data is in scatter-gather pages, and that the\n * packet has never been sent out before (and thus is not cloned).\n */\nstatic int tso_fragment(struct sock *sk, enum tcp_queue tcp_queue,\n\t\t\tstruct sk_buff *skb, unsigned int len,\n\t\t\tunsigned int mss_now, gfp_t gfp)\n{\n\tstruct sk_buff *buff;\n\tint nlen = skb->len - len;\n\tu8 flags;\n\n\t/* All of a TSO frame must be composed of paged data.  */\n\tif (skb->len != skb->data_len)\n\t\treturn tcp_fragment(sk, tcp_queue, skb, len, mss_now, gfp);\n\n\tbuff = sk_stream_alloc_skb(sk, 0, gfp, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOMEM;\n\n\tsk->sk_wmem_queued += buff->truesize;\n\tsk_mem_charge(sk, buff->truesize);\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t/* Correct the sequence numbers. */\n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t/* PSH and FIN should only be set in the second packet. */\n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\n\t/* This packet was never sent out yet, so no SACK bits. */\n\tTCP_SKB_CB(buff)->sacked = 0;\n\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\tskb_split(skb, buff, len);\n\ttcp_fragment_tstamp(skb, buff);\n\n\t/* Fix up tso_factor for both original and new SKB.  */\n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t/* Link BUFF into the send queue. */\n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, tcp_queue);\n\n\treturn 0;\n}\n\n/* Try to defer sending, if possible, in order to minimize the amount\n * of TSO splitting we do.  View it as a kind of TSO Nagle test.\n *\n * This algorithm is from John Heffner.\n */\nstatic bool tcp_tso_should_defer(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t bool *is_cwnd_limited,\n\t\t\t\t bool *is_rwnd_limited,\n\t\t\t\t u32 max_segs)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 age, send_win, cong_win, limit, in_flight;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *head;\n\tint win_divisor;\n\n\tif (icsk->icsk_ca_state >= TCP_CA_Recovery)\n\t\tgoto send_now;\n\n\t/* Avoid bursty behavior by allowing defer\n\t * only if the last write was recent.\n\t */\n\tif ((s32)(tcp_jiffies32 - tp->lsndtime) > 0)\n\t\tgoto send_now;\n\n\tin_flight = tcp_packets_in_flight(tp);\n\n\tBUG_ON(tcp_skb_pcount(skb) <= 1);\n\tBUG_ON(tp->snd_cwnd <= in_flight);\n\n\tsend_win = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t/* From in_flight test above, we know that cwnd > in_flight.  */\n\tcong_win = (tp->snd_cwnd - in_flight) * tp->mss_cache;\n\n\tlimit = min(send_win, cong_win);\n\n\t/* If a full-sized TSO skb can be sent, do it. */\n\tif (limit >= max_segs * tp->mss_cache)\n\t\tgoto send_now;\n\n\t/* Middle in queue won't get any more data, full sendable already? */\n\tif ((skb != tcp_write_queue_tail(sk)) && (limit >= skb->len))\n\t\tgoto send_now;\n\n\twin_divisor = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_tso_win_divisor);\n\tif (win_divisor) {\n\t\tu32 chunk = min(tp->snd_wnd, tp->snd_cwnd * tp->mss_cache);\n\n\t\t/* If at least some fraction of a window is available,\n\t\t * just use it.\n\t\t */\n\t\tchunk /= win_divisor;\n\t\tif (limit >= chunk)\n\t\t\tgoto send_now;\n\t} else {\n\t\t/* Different approach, try not to defer past a single\n\t\t * ACK.  Receiver should ACK every other full sized\n\t\t * frame, so if we have space for more than 3 frames\n\t\t * then send now.\n\t\t */\n\t\tif (limit > tcp_max_tso_deferred_mss(tp) * tp->mss_cache)\n\t\t\tgoto send_now;\n\t}\n\n\t/* TODO : use tsorted_sent_queue ? */\n\thead = tcp_rtx_queue_head(sk);\n\tif (!head)\n\t\tgoto send_now;\n\tage = tcp_stamp_us_delta(tp->tcp_mstamp, head->skb_mstamp);\n\t/* If next ACK is likely to come too late (half srtt), do not defer */\n\tif (age < (tp->srtt_us >> 4))\n\t\tgoto send_now;\n\n\t/* Ok, it looks like it is advisable to defer.\n\t * Three cases are tracked :\n\t * 1) We are cwnd-limited\n\t * 2) We are rwnd-limited\n\t * 3) We are application limited.\n\t */\n\tif (cong_win < send_win) {\n\t\tif (cong_win <= skb->len) {\n\t\t\t*is_cwnd_limited = true;\n\t\t\treturn true;\n\t\t}\n\t} else {\n\t\tif (send_win <= skb->len) {\n\t\t\t*is_rwnd_limited = true;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t/* If this packet won't get more data, do not wait. */\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\tgoto send_now;\n\n\treturn true;\n\nsend_now:\n\treturn false;\n}\n\nstatic inline void tcp_mtu_check_reprobe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 interval;\n\ts32 delta;\n\n\tinterval = net->ipv4.sysctl_tcp_probe_interval;\n\tdelta = tcp_jiffies32 - icsk->icsk_mtup.probe_timestamp;\n\tif (unlikely(delta >= interval * HZ)) {\n\t\tint mss = tcp_current_mss(sk);\n\n\t\t/* Update current search range */\n\t\ticsk->icsk_mtup.probe_size = 0;\n\t\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp +\n\t\t\tsizeof(struct tcphdr) +\n\t\t\ticsk->icsk_af_ops->net_header_len;\n\t\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);\n\n\t\t/* Update probe time stamp */\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n\t}\n}\n\nstatic bool tcp_can_coalesce_send_queue_head(struct sock *sk, int len)\n{\n\tstruct sk_buff *skb, *next;\n\n\tskb = tcp_send_head(sk);\n\ttcp_for_write_queue_from_safe(skb, next, sk) {\n\t\tif (len <= skb->len)\n\t\t\tbreak;\n\n\t\tif (unlikely(TCP_SKB_CB(skb)->eor) || tcp_has_tx_tstamp(skb))\n\t\t\treturn false;\n\n\t\tlen -= skb->len;\n\t}\n\n\treturn true;\n}\n\n/* Create a new MTU probe if we are ready.\n * MTU probe is regularly attempting to increase the path MTU by\n * deliberately sending larger packets.  This discovers routing\n * changes resulting in larger path MTUs.\n *\n * Returns 0 if we should wait to probe (no cwnd available),\n *         1 if a probe was sent,\n *         -1 otherwise\n */\nstatic int tcp_mtu_probe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *nskb, *next;\n\tstruct net *net = sock_net(sk);\n\tint probe_size;\n\tint size_needed;\n\tint copy, len;\n\tint mss_now;\n\tint interval;\n\n\t/* Not currently probing/verifying,\n\t * not in recovery,\n\t * have enough cwnd, and\n\t * not SACKing (the variable headers throw things off)\n\t */\n\tif (likely(!icsk->icsk_mtup.enabled ||\n\t\t   icsk->icsk_mtup.probe_size ||\n\t\t   inet_csk(sk)->icsk_ca_state != TCP_CA_Open ||\n\t\t   tp->snd_cwnd < 11 ||\n\t\t   tp->rx_opt.num_sacks || tp->rx_opt.dsack))\n\t\treturn -1;\n\n\t/* Use binary search for probe_size between tcp_mss_base,\n\t * and current mss_clamp. if (search_high - search_low)\n\t * smaller than a threshold, backoff from probing.\n\t */\n\tmss_now = tcp_current_mss(sk);\n\tprobe_size = tcp_mtu_to_mss(sk, (icsk->icsk_mtup.search_high +\n\t\t\t\t    icsk->icsk_mtup.search_low) >> 1);\n\tsize_needed = probe_size + (tp->reordering + 1) * tp->mss_cache;\n\tinterval = icsk->icsk_mtup.search_high - icsk->icsk_mtup.search_low;\n\t/* When misfortune happens, we are reprobing actively,\n\t * and then reprobe timer has expired. We stick with current\n\t * probing process by not resetting search range to its orignal.\n\t */\n\tif (probe_size > tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_high) ||\n\t\tinterval < net->ipv4.sysctl_tcp_probe_threshold) {\n\t\t/* Check whether enough time has elaplased for\n\t\t * another round of probing.\n\t\t */\n\t\ttcp_mtu_check_reprobe(sk);\n\t\treturn -1;\n\t}\n\n\t/* Have enough data in the send queue to probe? */\n\tif (tp->write_seq - tp->snd_nxt < size_needed)\n\t\treturn -1;\n\n\tif (tp->snd_wnd < size_needed)\n\t\treturn -1;\n\tif (after(tp->snd_nxt + size_needed, tcp_wnd_end(tp)))\n\t\treturn 0;\n\n\t/* Do we need to wait to drain cwnd? With none in flight, don't stall */\n\tif (tcp_packets_in_flight(tp) + 2 > tp->snd_cwnd) {\n\t\tif (!tcp_packets_in_flight(tp))\n\t\t\treturn -1;\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\tif (!tcp_can_coalesce_send_queue_head(sk, probe_size))\n\t\treturn -1;\n\n\t/* We're allowed to probe.  Build it now. */\n\tnskb = sk_stream_alloc_skb(sk, probe_size, GFP_ATOMIC, false);\n\tif (!nskb)\n\t\treturn -1;\n\tsk->sk_wmem_queued += nskb->truesize;\n\tsk_mem_charge(sk, nskb->truesize);\n\n\tskb = tcp_send_head(sk);\n\n\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(skb)->seq;\n\tTCP_SKB_CB(nskb)->end_seq = TCP_SKB_CB(skb)->seq + probe_size;\n\tTCP_SKB_CB(nskb)->tcp_flags = TCPHDR_ACK;\n\tTCP_SKB_CB(nskb)->sacked = 0;\n\tnskb->csum = 0;\n\tnskb->ip_summed = CHECKSUM_PARTIAL;\n\n\ttcp_insert_write_queue_before(nskb, skb, sk);\n\ttcp_highest_sack_replace(sk, skb, nskb);\n\n\tlen = 0;\n\ttcp_for_write_queue_from_safe(skb, next, sk) {\n\t\tcopy = min_t(int, skb->len, probe_size - len);\n\t\tskb_copy_bits(skb, 0, skb_put(nskb, copy), copy);\n\n\t\tif (skb->len <= copy) {\n\t\t\t/* We've eaten all the data from this skb.\n\t\t\t * Throw it away. */\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\t\t\t/* If this is the last SKB we copy and eor is set\n\t\t\t * we need to propagate it to the new skb.\n\t\t\t */\n\t\t\tTCP_SKB_CB(nskb)->eor = TCP_SKB_CB(skb)->eor;\n\t\t\ttcp_skb_collapse_tstamp(nskb, skb);\n\t\t\ttcp_unlink_write_queue(skb, sk);\n\t\t\tsk_wmem_free_skb(sk, skb);\n\t\t} else {\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags &\n\t\t\t\t\t\t   ~(TCPHDR_FIN|TCPHDR_PSH);\n\t\t\tif (!skb_shinfo(skb)->nr_frags) {\n\t\t\t\tskb_pull(skb, copy);\n\t\t\t} else {\n\t\t\t\t__pskb_trim_head(skb, copy);\n\t\t\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\t\t}\n\t\t\tTCP_SKB_CB(skb)->seq += copy;\n\t\t}\n\n\t\tlen += copy;\n\n\t\tif (len >= probe_size)\n\t\t\tbreak;\n\t}\n\ttcp_init_tso_segs(nskb, nskb->len);\n\n\t/* We're ready to send.  If this fails, the probe will\n\t * be resegmented into mss-sized pieces by tcp_write_xmit().\n\t */\n\tif (!tcp_transmit_skb(sk, nskb, 1, GFP_ATOMIC)) {\n\t\t/* Decrement cwnd here because we are sending\n\t\t * effectively two packets. */\n\t\ttp->snd_cwnd--;\n\t\ttcp_event_new_data_sent(sk, nskb);\n\n\t\ticsk->icsk_mtup.probe_size = tcp_mss_to_mtu(sk, nskb->len);\n\t\ttp->mtu_probe.probe_seq_start = TCP_SKB_CB(nskb)->seq;\n\t\ttp->mtu_probe.probe_seq_end = TCP_SKB_CB(nskb)->end_seq;\n\n\t\treturn 1;\n\t}\n\n\treturn -1;\n}\n\nstatic bool tcp_pacing_check(const struct sock *sk)\n{\n\treturn tcp_needs_internal_pacing(sk) &&\n\t       hrtimer_is_queued(&tcp_sk(sk)->pacing_timer);\n}\n\n/* TCP Small Queues :\n * Control number of packets in qdisc/devices to two packets / or ~1 ms.\n * (These limits are doubled for retransmits)\n * This allows for :\n *  - better RTT estimation and ACK scheduling\n *  - faster recovery\n *  - high rates\n * Alas, some drivers / subsystems require a fair amount\n * of queued bytes to ensure line rate.\n * One example is wifi aggregation (802.11 AMPDU)\n */\nstatic bool tcp_small_queue_check(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t  unsigned int factor)\n{\n\tunsigned int limit;\n\n\tlimit = max(2 * skb->truesize, sk->sk_pacing_rate >> sk->sk_pacing_shift);\n\tlimit = min_t(u32, limit,\n\t\t      sock_net(sk)->ipv4.sysctl_tcp_limit_output_bytes);\n\tlimit <<= factor;\n\n\tif (refcount_read(&sk->sk_wmem_alloc) > limit) {\n\t\t/* Always send skb if rtx queue is empty.\n\t\t * No need to wait for TX completion to call us back,\n\t\t * after softirq/tasklet schedule.\n\t\t * This helps when TX completions are delayed too much.\n\t\t */\n\t\tif (tcp_rtx_queue_empty(sk))\n\t\t\treturn false;\n\n\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t/* It is possible TX completion already happened\n\t\t * before we set TSQ_THROTTLED, so we must\n\t\t * test again the condition.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tif (refcount_read(&sk->sk_wmem_alloc) > limit)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void tcp_chrono_set(struct tcp_sock *tp, const enum tcp_chrono new)\n{\n\tconst u32 now = tcp_jiffies32;\n\tenum tcp_chrono old = tp->chrono_type;\n\n\tif (old > TCP_CHRONO_UNSPEC)\n\t\ttp->chrono_stat[old - 1] += now - tp->chrono_start;\n\ttp->chrono_start = now;\n\ttp->chrono_type = new;\n}\n\nvoid tcp_chrono_start(struct sock *sk, const enum tcp_chrono type)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If there are multiple conditions worthy of tracking in a\n\t * chronograph then the highest priority enum takes precedence\n\t * over the other conditions. So that if something \"more interesting\"\n\t * starts happening, stop the previous chrono and start a new one.\n\t */\n\tif (type > tp->chrono_type)\n\t\ttcp_chrono_set(tp, type);\n}\n\nvoid tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\n\t/* There are multiple conditions worthy of tracking in a\n\t * chronograph, so that the highest priority enum takes\n\t * precedence over the other conditions (see tcp_chrono_start).\n\t * If a condition stops, we only stop chrono tracking if\n\t * it's the \"most interesting\" or current chrono we are\n\t * tracking and starts busy chrono if we have pending data.\n\t */\n\tif (tcp_rtx_and_write_queues_empty(sk))\n\t\ttcp_chrono_set(tp, TCP_CHRONO_UNSPEC);\n\telse if (type == tp->chrono_type)\n\t\ttcp_chrono_set(tp, TCP_CHRONO_BUSY);\n}\n\n/* This routine writes packets to the network.  It advances the\n * send_head.  This happens as incoming acks open up the remote\n * window for us.\n *\n * LARGESEND note: !tcp_urg_mode is overkill, only frames between\n * snd_up-64k-mss .. snd_up cannot be large. However, taking into\n * account rare use of URG, this is not a big flaw.\n *\n * Send at most one packet when push_one > 0. Temporarily ignore\n * cwnd limit to force at most one packet out when push_one == 2.\n\n * Returns true, if no segments are in flight and we have queued segments,\n * but cannot send anything now because of SWS or another problem.\n */\nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\n\t\t\t   int push_one, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int tso_segs, sent_pkts;\n\tint cwnd_quota;\n\tint result;\n\tbool is_cwnd_limited = false, is_rwnd_limited = false;\n\tu32 max_segs;\n\n\tsent_pkts = 0;\n\n\ttcp_mstamp_refresh(tp);\n\tif (!push_one) {\n\t\t/* Do MTU probing. */\n\t\tresult = tcp_mtu_probe(sk);\n\t\tif (!result) {\n\t\t\treturn false;\n\t\t} else if (result > 0) {\n\t\t\tsent_pkts = 1;\n\t\t}\n\t}\n\n\tmax_segs = tcp_tso_segs(sk, mss_now);\n\twhile ((skb = tcp_send_head(sk))) {\n\t\tunsigned int limit;\n\n\t\tif (tcp_pacing_check(sk))\n\t\t\tbreak;\n\n\t\ttso_segs = tcp_init_tso_segs(skb, mss_now);\n\t\tBUG_ON(!tso_segs);\n\n\t\tif (unlikely(tp->repair) && tp->repair_queue == TCP_SEND_QUEUE) {\n\t\t\t/* \"skb_mstamp\" is used as a start point for the retransmit timer */\n\t\t\ttcp_update_skb_after_send(tp, skb);\n\t\t\tgoto repair; /* Skip network transmission */\n\t\t}\n\n\t\tcwnd_quota = tcp_cwnd_test(tp, skb);\n\t\tif (!cwnd_quota) {\n\t\t\tif (push_one == 2)\n\t\t\t\t/* Force out a loss probe pkt. */\n\t\t\t\tcwnd_quota = 1;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now))) {\n\t\t\tis_rwnd_limited = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tso_segs == 1) {\n\t\t\tif (unlikely(!tcp_nagle_test(tp, skb, mss_now,\n\t\t\t\t\t\t     (tcp_skb_is_last(sk, skb) ?\n\t\t\t\t\t\t      nonagle : TCP_NAGLE_PUSH))))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (!push_one &&\n\t\t\t    tcp_tso_should_defer(sk, skb, &is_cwnd_limited,\n\t\t\t\t\t\t &is_rwnd_limited, max_segs))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tlimit = mss_now;\n\t\tif (tso_segs > 1 && !tcp_urg_mode(tp))\n\t\t\tlimit = tcp_mss_split_point(sk, skb, mss_now,\n\t\t\t\t\t\t    min_t(unsigned int,\n\t\t\t\t\t\t\t  cwnd_quota,\n\t\t\t\t\t\t\t  max_segs),\n\t\t\t\t\t\t    nonagle);\n\n\t\tif (skb->len > limit &&\n\t\t    unlikely(tso_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t\t  skb, limit, mss_now, gfp)))\n\t\t\tbreak;\n\n\t\tif (tcp_small_queue_check(sk, skb, 0))\n\t\t\tbreak;\n\n\t\t/* Argh, we hit an empty skb(), presumably a thread\n\t\t * is sleeping in sendmsg()/sk_stream_wait_memory().\n\t\t * We do not want to send a pure-ack packet and have\n\t\t * a strange looking rtx queue with empty packet(s).\n\t\t */\n\t\tif (TCP_SKB_CB(skb)->end_seq == TCP_SKB_CB(skb)->seq)\n\t\t\tbreak;\n\n\t\tif (unlikely(tcp_transmit_skb(sk, skb, 1, gfp)))\n\t\t\tbreak;\n\nrepair:\n\t\t/* Advance the send_head.  This one is sent out.\n\t\t * This call will increment packets_out.\n\t\t */\n\t\ttcp_event_new_data_sent(sk, skb);\n\n\t\ttcp_minshall_update(tp, mss_now, skb);\n\t\tsent_pkts += tcp_skb_pcount(skb);\n\n\t\tif (push_one)\n\t\t\tbreak;\n\t}\n\n\tif (is_rwnd_limited)\n\t\ttcp_chrono_start(sk, TCP_CHRONO_RWND_LIMITED);\n\telse\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_RWND_LIMITED);\n\n\tis_cwnd_limited |= (tcp_packets_in_flight(tp) >= tp->snd_cwnd);\n\tif (likely(sent_pkts || is_cwnd_limited))\n\t\ttcp_cwnd_validate(sk, is_cwnd_limited);\n\n\tif (likely(sent_pkts)) {\n\t\tif (tcp_in_cwnd_reduction(sk))\n\t\t\ttp->prr_out += sent_pkts;\n\n\t\t/* Send one loss probe per tail loss episode. */\n\t\tif (push_one != 2)\n\t\t\ttcp_schedule_loss_probe(sk, false);\n\t\treturn false;\n\t}\n\treturn !tp->packets_out && !tcp_write_queue_empty(sk);\n}\n\nbool tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 timeout, rto_delta_us;\n\tint early_retrans;\n\n\t/* Don't do any loss probe on a Fast Open connection before 3WHS\n\t * finishes.\n\t */\n\tif (tp->fastopen_rsk)\n\t\treturn false;\n\n\tearly_retrans = sock_net(sk)->ipv4.sysctl_tcp_early_retrans;\n\t/* Schedule a loss probe in 2*RTT for SACK capable connections\n\t * not in loss recovery, that are either limited by cwnd or application.\n\t */\n\tif ((early_retrans != 3 && early_retrans != 4) ||\n\t    !tp->packets_out || !tcp_is_sack(tp) ||\n\t    (icsk->icsk_ca_state != TCP_CA_Open &&\n\t     icsk->icsk_ca_state != TCP_CA_CWR))\n\t\treturn false;\n\n\t/* Probe timeout is 2*rtt. Add minimum RTO to account\n\t * for delayed ack when there's one outstanding packet. If no RTT\n\t * sample is available then probe after TCP_TIMEOUT_INIT.\n\t */\n\tif (tp->srtt_us) {\n\t\ttimeout = usecs_to_jiffies(tp->srtt_us >> 2);\n\t\tif (tp->packets_out == 1)\n\t\t\ttimeout += TCP_RTO_MIN;\n\t\telse\n\t\t\ttimeout += TCP_TIMEOUT_MIN;\n\t} else {\n\t\ttimeout = TCP_TIMEOUT_INIT;\n\t}\n\n\t/* If the RTO formula yields an earlier time, then use that time. */\n\trto_delta_us = advancing_rto ?\n\t\t\tjiffies_to_usecs(inet_csk(sk)->icsk_rto) :\n\t\t\ttcp_rto_delta_us(sk);  /* How far in future is RTO? */\n\tif (rto_delta_us > 0)\n\t\ttimeout = min_t(u32, timeout, usecs_to_jiffies(rto_delta_us));\n\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_LOSS_PROBE, timeout,\n\t\t\t\t  TCP_RTO_MAX);\n\treturn true;\n}\n\n/* Thanks to skb fast clones, we can detect if a prior transmit of\n * a packet is still in a qdisc or driver queue.\n * In this case, there is very little point doing a retransmit !\n */\nstatic bool skb_still_in_host_queue(const struct sock *sk,\n\t\t\t\t    const struct sk_buff *skb)\n{\n\tif (unlikely(skb_fclone_busy(sk, skb))) {\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t      LINUX_MIB_TCPSPURIOUS_RTX_HOSTQUEUES);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* When probe timeout (PTO) fires, try send a new segment if possible, else\n * retransmit the last segment.\n */\nvoid tcp_send_loss_probe(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint pcount;\n\tint mss = tcp_current_mss(sk);\n\n\t/* At most one outstanding TLP */\n\tif (tp->tlp_high_seq)\n\t\tgoto rearm_timer;\n\n\ttp->tlp_retrans = 0;\n\tskb = tcp_send_head(sk);\n\tif (skb && tcp_snd_wnd_test(tp, skb, mss)) {\n\t\tpcount = tp->packets_out;\n\t\ttcp_write_xmit(sk, mss, TCP_NAGLE_OFF, 2, GFP_ATOMIC);\n\t\tif (tp->packets_out > pcount)\n\t\t\tgoto probe_sent;\n\t\tgoto rearm_timer;\n\t}\n\tskb = skb_rb_last(&sk->tcp_rtx_queue);\n\tif (unlikely(!skb)) {\n\t\tWARN_ONCE(tp->packets_out,\n\t\t\t  \"invalid inflight: %u state %u cwnd %u mss %d\\n\",\n\t\t\t  tp->packets_out, sk->sk_state, tp->snd_cwnd, mss);\n\t\tinet_csk(sk)->icsk_pending = 0;\n\t\treturn;\n\t}\n\n\tif (skb_still_in_host_queue(sk, skb))\n\t\tgoto rearm_timer;\n\n\tpcount = tcp_skb_pcount(skb);\n\tif (WARN_ON(!pcount))\n\t\tgoto rearm_timer;\n\n\tif ((pcount > 1) && (skb->len > (pcount - 1) * mss)) {\n\t\tif (unlikely(tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t\t  (pcount - 1) * mss, mss,\n\t\t\t\t\t  GFP_ATOMIC)))\n\t\t\tgoto rearm_timer;\n\t\tskb = skb_rb_next(skb);\n\t}\n\n\tif (WARN_ON(!skb || !tcp_skb_pcount(skb)))\n\t\tgoto rearm_timer;\n\n\tif (__tcp_retransmit_skb(sk, skb, 1))\n\t\tgoto rearm_timer;\n\n\ttp->tlp_retrans = 1;\n\nprobe_sent:\n\t/* Record snd_nxt for loss detection. */\n\ttp->tlp_high_seq = tp->snd_nxt;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSPROBES);\n\t/* Reset s.t. tcp_rearm_rto will restart timer from now */\n\tinet_csk(sk)->icsk_pending = 0;\nrearm_timer:\n\ttcp_rearm_rto(sk);\n}\n\n/* Push out any pending frames which were held back due to\n * TCP_CORK or attempt at coalescing tiny packets.\n * The socket must be locked by the caller.\n */\nvoid __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\n\t\t\t       int nonagle)\n{\n\t/* If we are closed, the bytes will have to remain here.\n\t * In time closedown will finish, we empty the write queue and\n\t * all will be happy.\n\t */\n\tif (unlikely(sk->sk_state == TCP_CLOSE))\n\t\treturn;\n\n\tif (tcp_write_xmit(sk, cur_mss, nonagle, 0,\n\t\t\t   sk_gfp_mask(sk, GFP_ATOMIC)))\n\t\ttcp_check_probe_timer(sk);\n}\n\n/* Send _single_ skb sitting at the send head. This function requires\n * true push pending frames to setup probe timer etc.\n */\nvoid tcp_push_one(struct sock *sk, unsigned int mss_now)\n{\n\tstruct sk_buff *skb = tcp_send_head(sk);\n\n\tBUG_ON(!skb || skb->len < mss_now);\n\n\ttcp_write_xmit(sk, mss_now, TCP_NAGLE_PUSH, 1, sk->sk_allocation);\n}\n\n/* This function returns the amount that we can raise the\n * usable window based on the following constraints\n *\n * 1. The window can never be shrunk once it is offered (RFC 793)\n * 2. We limit memory per socket\n *\n * RFC 1122:\n * \"the suggested [SWS] avoidance algorithm for the receiver is to keep\n *  RECV.NEXT + RCV.WIN fixed until:\n *  RCV.BUFF - RCV.USER - RCV.WINDOW >= min(1/2 RCV.BUFF, MSS)\"\n *\n * i.e. don't raise the right edge of the window until you can raise\n * it at least MSS bytes.\n *\n * Unfortunately, the recommended algorithm breaks header prediction,\n * since header prediction assumes th->window stays fixed.\n *\n * Strictly speaking, keeping th->window fixed violates the receiver\n * side SWS prevention criteria. The problem is that under this rule\n * a stream of single byte packets will cause the right side of the\n * window to always advance by a single byte.\n *\n * Of course, if the sender implements sender side SWS prevention\n * then this will not be a problem.\n *\n * BSD seems to make the following compromise:\n *\n *\tIf the free space is less than the 1/4 of the maximum\n *\tspace available and the free space is less than 1/2 mss,\n *\tthen set the window to 0.\n *\t[ Actually, bsd uses MSS and 1/4 of maximal _window_ ]\n *\tOtherwise, just prevent the window from shrinking\n *\tand from being larger than the largest representable value.\n *\n * This prevents incremental opening of the window in the regime\n * where TCP is limited by the speed of the reader side taking\n * data out of the TCP receive queue. It does nothing about\n * those cases where the window is constrained on the sender side\n * because the pipeline is full.\n *\n * BSD also seems to \"accidentally\" limit itself to windows that are a\n * multiple of MSS, at least until the free space gets quite small.\n * This would appear to be a side effect of the mbuf implementation.\n * Combining these two algorithms results in the observed behavior\n * of having a fixed window size at almost all times.\n *\n * Below we obtain similar behavior by forcing the offered window to\n * a multiple of the mss when it is feasible to do so.\n *\n * Note, we don't \"adjust\" for TIMESTAMP or SACK option bytes.\n * Regular options like TIMESTAMP are taken into account.\n */\nu32 __tcp_select_window(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t/* MSS for the peer's data.  Previous versions used mss_clamp\n\t * here.  I don't know if the value based on our guesses\n\t * of peer's MSS is better for the performance.  It's more correct\n\t * but may be worse for the performance because of rcv_mss\n\t * fluctuations.  --SAW  1998/11/1\n\t */\n\tint mss = icsk->icsk_ack.rcv_mss;\n\tint free_space = tcp_space(sk);\n\tint allowed_space = tcp_full_space(sk);\n\tint full_space = min_t(int, tp->window_clamp, allowed_space);\n\tint window;\n\n\tif (unlikely(mss > full_space)) {\n\t\tmss = full_space;\n\t\tif (mss <= 0)\n\t\t\treturn 0;\n\t}\n\tif (free_space < (full_space >> 1)) {\n\t\ticsk->icsk_ack.quick = 0;\n\n\t\tif (tcp_under_memory_pressure(sk))\n\t\t\ttp->rcv_ssthresh = min(tp->rcv_ssthresh,\n\t\t\t\t\t       4U * tp->advmss);\n\n\t\t/* free_space might become our new window, make sure we don't\n\t\t * increase it due to wscale.\n\t\t */\n\t\tfree_space = round_down(free_space, 1 << tp->rx_opt.rcv_wscale);\n\n\t\t/* if free space is less than mss estimate, or is below 1/16th\n\t\t * of the maximum allowed, try to move to zero-window, else\n\t\t * tcp_clamp_window() will grow rcv buf up to tcp_rmem[2], and\n\t\t * new incoming data is dropped due to memory limits.\n\t\t * With large window, mss test triggers way too late in order\n\t\t * to announce zero window in time before rmem limit kicks in.\n\t\t */\n\t\tif (free_space < (allowed_space >> 4) || free_space < mss)\n\t\t\treturn 0;\n\t}\n\n\tif (free_space > tp->rcv_ssthresh)\n\t\tfree_space = tp->rcv_ssthresh;\n\n\t/* Don't do rounding if we are using window scaling, since the\n\t * scaled window will not line up with the MSS boundary anyway.\n\t */\n\tif (tp->rx_opt.rcv_wscale) {\n\t\twindow = free_space;\n\n\t\t/* Advertise enough space so that it won't get scaled away.\n\t\t * Import case: prevent zero window announcement if\n\t\t * 1<<rcv_wscale > mss.\n\t\t */\n\t\twindow = ALIGN(window, (1 << tp->rx_opt.rcv_wscale));\n\t} else {\n\t\twindow = tp->rcv_wnd;\n\t\t/* Get the largest window that is a nice multiple of mss.\n\t\t * Window clamp already applied above.\n\t\t * If our current window offering is within 1 mss of the\n\t\t * free space we just keep it. This prevents the divide\n\t\t * and multiply from happening most of the time.\n\t\t * We also don't do any window rounding when the free space\n\t\t * is too small.\n\t\t */\n\t\tif (window <= free_space - mss || window > free_space)\n\t\t\twindow = rounddown(free_space, mss);\n\t\telse if (mss == full_space &&\n\t\t\t free_space > window + (full_space >> 1))\n\t\t\twindow = free_space;\n\t}\n\n\treturn window;\n}\n\nvoid tcp_skb_collapse_tstamp(struct sk_buff *skb,\n\t\t\t     const struct sk_buff *next_skb)\n{\n\tif (unlikely(tcp_has_tx_tstamp(next_skb))) {\n\t\tconst struct skb_shared_info *next_shinfo =\n\t\t\tskb_shinfo(next_skb);\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\tshinfo->tx_flags |= next_shinfo->tx_flags & SKBTX_ANY_TSTAMP;\n\t\tshinfo->tskey = next_shinfo->tskey;\n\t\tTCP_SKB_CB(skb)->txstamp_ack |=\n\t\t\tTCP_SKB_CB(next_skb)->txstamp_ack;\n\t}\n}\n\n/* Collapses two adjacent SKB's during retransmission. */\nstatic bool tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *next_skb = skb_rb_next(skb);\n\tint next_skb_size;\n\n\tnext_skb_size = next_skb->len;\n\n\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\n\n\tif (next_skb_size) {\n\t\tif (next_skb_size <= skb_availroom(skb))\n\t\t\tskb_copy_bits(next_skb, 0, skb_put(skb, next_skb_size),\n\t\t\t\t      next_skb_size);\n\t\telse if (!tcp_skb_shift(skb, next_skb, 1, next_skb_size))\n\t\t\treturn false;\n\t}\n\ttcp_highest_sack_replace(sk, next_skb, skb);\n\n\t/* Update sequence range on original skb. */\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;\n\n\t/* Merge over control information. This moves PSH/FIN etc. over */\n\tTCP_SKB_CB(skb)->tcp_flags |= TCP_SKB_CB(next_skb)->tcp_flags;\n\n\t/* All done, get rid of second SKB and account for it so\n\t * packet counting does not break.\n\t */\n\tTCP_SKB_CB(skb)->sacked |= TCP_SKB_CB(next_skb)->sacked & TCPCB_EVER_RETRANS;\n\tTCP_SKB_CB(skb)->eor = TCP_SKB_CB(next_skb)->eor;\n\n\t/* changed transmit queue under us so clear hints */\n\ttcp_clear_retrans_hints_partial(tp);\n\tif (next_skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = skb;\n\n\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\n\n\ttcp_skb_collapse_tstamp(skb, next_skb);\n\n\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\n\treturn true;\n}\n\n/* Check if coalescing SKBs is legal. */\nstatic bool tcp_can_collapse(const struct sock *sk, const struct sk_buff *skb)\n{\n\tif (tcp_skb_pcount(skb) > 1)\n\t\treturn false;\n\tif (skb_cloned(skb))\n\t\treturn false;\n\t/* Some heuristics for collapsing over SACK'd could be invented */\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\treturn false;\n\n\treturn true;\n}\n\n/* Collapse packets in the retransmit queue to make to create\n * less packets on the wire. This is only done on retransmission.\n */\nstatic void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *to,\n\t\t\t\t     int space)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb = to, *tmp;\n\tbool first = true;\n\n\tif (!sock_net(sk)->ipv4.sysctl_tcp_retrans_collapse)\n\t\treturn;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)\n\t\treturn;\n\n\tskb_rbtree_walk_from_safe(skb, tmp) {\n\t\tif (!tcp_can_collapse(sk, skb))\n\t\t\tbreak;\n\n\t\tif (!tcp_skb_can_collapse_to(to))\n\t\t\tbreak;\n\n\t\tspace -= skb->len;\n\n\t\tif (first) {\n\t\t\tfirst = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (space < 0)\n\t\t\tbreak;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, tcp_wnd_end(tp)))\n\t\t\tbreak;\n\n\t\tif (!tcp_collapse_retrans(sk, to))\n\t\t\tbreak;\n\t}\n}\n\n/* This retransmits one SKB.  Policy decisions and retransmit queue\n * state updates are done by the caller.  Returns non-zero if an\n * error occurred which prevented the send.\n */\nint __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int cur_mss;\n\tint diff, len, err;\n\n\n\t/* Inconclusive MTU probe */\n\tif (icsk->icsk_mtup.probe_size)\n\t\ticsk->icsk_mtup.probe_size = 0;\n\n\t/* Do not sent more than we queued. 1/4 is reserved for possible\n\t * copying overhead: fragmentation, tunneling, mangling etc.\n\t */\n\tif (refcount_read(&sk->sk_wmem_alloc) >\n\t    min_t(u32, sk->sk_wmem_queued + (sk->sk_wmem_queued >> 2),\n\t\t  sk->sk_sndbuf))\n\t\treturn -EAGAIN;\n\n\tif (skb_still_in_host_queue(sk, skb))\n\t\treturn -EBUSY;\n\n\tif (before(TCP_SKB_CB(skb)->seq, tp->snd_una)) {\n\t\tif (unlikely(before(TCP_SKB_CB(skb)->end_seq, tp->snd_una))) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\n\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\n\n\tcur_mss = tcp_current_mss(sk);\n\n\t/* If receiver has shrunk his window, and skb is out of\n\t * new window, do not retransmit it. The exception is the\n\t * case, when window is shrunk to zero. In this case\n\t * our retransmit serves as a zero window probe.\n\t */\n\tif (!before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp)) &&\n\t    TCP_SKB_CB(skb)->seq != tp->snd_una)\n\t\treturn -EAGAIN;\n\n\tlen = cur_mss * segs;\n\tif (skb->len > len) {\n\t\tif (tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb, len,\n\t\t\t\t cur_mss, GFP_ATOMIC))\n\t\t\treturn -ENOMEM; /* We'll try again later. */\n\t} else {\n\t\tif (skb_unclone(skb, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\tdiff = tcp_skb_pcount(skb);\n\t\ttcp_set_skb_tso_segs(skb, cur_mss);\n\t\tdiff -= tcp_skb_pcount(skb);\n\t\tif (diff)\n\t\t\ttcp_adjust_pcount(sk, skb, diff);\n\t\tif (skb->len < cur_mss)\n\t\t\ttcp_retrans_try_collapse(sk, skb, cur_mss);\n\t}\n\n\t/* RFC3168, section 6.1.1.1. ECN fallback */\n\tif ((TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN_ECN) == TCPHDR_SYN_ECN)\n\t\ttcp_ecn_clear_syn(sk, skb);\n\n\t/* Update global and local TCP statistics. */\n\tsegs = tcp_skb_pcount(skb);\n\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS, segs);\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\n\ttp->total_retrans += segs;\n\ttp->bytes_retrans += skb->len;\n\n\t/* make sure skb->data is aligned on arches that require it\n\t * and check if ack-trimming & collapsing extended the headroom\n\t * beyond what csum_start can cover.\n\t */\n\tif (unlikely((NET_IP_ALIGN && ((unsigned long)skb->data & 3)) ||\n\t\t     skb_headroom(skb) >= 0xFFFF)) {\n\t\tstruct sk_buff *nskb;\n\n\t\ttcp_skb_tsorted_save(skb) {\n\t\t\tnskb = __pskb_copy(skb, MAX_TCP_HEADER, GFP_ATOMIC);\n\t\t\terr = nskb ? tcp_transmit_skb(sk, nskb, 0, GFP_ATOMIC) :\n\t\t\t\t     -ENOBUFS;\n\t\t} tcp_skb_tsorted_restore(skb);\n\n\t\tif (!err) {\n\t\t\ttcp_update_skb_after_send(tp, skb);\n\t\t\ttcp_rate_skb_sent(sk, skb);\n\t\t}\n\t} else {\n\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n\t}\n\n\tif (BPF_SOCK_OPS_TEST_FLAG(tp, BPF_SOCK_OPS_RETRANS_CB_FLAG))\n\t\ttcp_call_bpf_3arg(sk, BPF_SOCK_OPS_RETRANS_CB,\n\t\t\t\t  TCP_SKB_CB(skb)->seq, segs, err);\n\n\tif (likely(!err)) {\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_EVER_RETRANS;\n\t\ttrace_tcp_retransmit_skb(sk, skb);\n\t} else if (err != -EBUSY) {\n\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPRETRANSFAIL, segs);\n\t}\n\treturn err;\n}\n\nint tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint err = __tcp_retransmit_skb(sk, skb, segs);\n\n\tif (err == 0) {\n#if FASTRETRANS_DEBUG > 0\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS) {\n\t\t\tnet_dbg_ratelimited(\"retrans_out leaked\\n\");\n\t\t}\n#endif\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_RETRANS;\n\t\ttp->retrans_out += tcp_skb_pcount(skb);\n\t}\n\n\t/* Save stamp of the first (attempted) retransmit. */\n\tif (!tp->retrans_stamp)\n\t\ttp->retrans_stamp = tcp_skb_timestamp(skb);\n\n\tif (tp->undo_retrans < 0)\n\t\ttp->undo_retrans = 0;\n\ttp->undo_retrans += tcp_skb_pcount(skb);\n\treturn err;\n}\n\n/* This gets called after a retransmit timeout, and the initially\n * retransmitted data is acknowledged.  It tries to continue\n * resending the rest of the retransmit queue, until either\n * we've sent it all or the congestion window limit is reached.\n */\nvoid tcp_xmit_retransmit_queue(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sk_buff *skb, *rtx_head, *hole = NULL;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 max_segs;\n\tint mib_idx;\n\n\tif (!tp->packets_out)\n\t\treturn;\n\n\trtx_head = tcp_rtx_queue_head(sk);\n\tskb = tp->retransmit_skb_hint ?: rtx_head;\n\tmax_segs = tcp_tso_segs(sk, tcp_current_mss(sk));\n\tskb_rbtree_walk_from(skb) {\n\t\t__u8 sacked;\n\t\tint segs;\n\n\t\tif (tcp_pacing_check(sk))\n\t\t\tbreak;\n\n\t\t/* we could do better than to assign each time */\n\t\tif (!hole)\n\t\t\ttp->retransmit_skb_hint = skb;\n\n\t\tsegs = tp->snd_cwnd - tcp_packets_in_flight(tp);\n\t\tif (segs <= 0)\n\t\t\treturn;\n\t\tsacked = TCP_SKB_CB(skb)->sacked;\n\t\t/* In case tcp_shift_skb_data() have aggregated large skbs,\n\t\t * we need to make sure not sending too bigs TSO packets\n\t\t */\n\t\tsegs = min_t(int, segs, max_segs);\n\n\t\tif (tp->retrans_out >= tp->lost_out) {\n\t\t\tbreak;\n\t\t} else if (!(sacked & TCPCB_LOST)) {\n\t\t\tif (!hole && !(sacked & (TCPCB_SACKED_RETRANS|TCPCB_SACKED_ACKED)))\n\t\t\t\thole = skb;\n\t\t\tcontinue;\n\n\t\t} else {\n\t\t\tif (icsk->icsk_ca_state != TCP_CA_Loss)\n\t\t\t\tmib_idx = LINUX_MIB_TCPFASTRETRANS;\n\t\t\telse\n\t\t\t\tmib_idx = LINUX_MIB_TCPSLOWSTARTRETRANS;\n\t\t}\n\n\t\tif (sacked & (TCPCB_SACKED_ACKED|TCPCB_SACKED_RETRANS))\n\t\t\tcontinue;\n\n\t\tif (tcp_small_queue_check(sk, skb, 1))\n\t\t\treturn;\n\n\t\tif (tcp_retransmit_skb(sk, skb, segs))\n\t\t\treturn;\n\n\t\tNET_ADD_STATS(sock_net(sk), mib_idx, tcp_skb_pcount(skb));\n\n\t\tif (tcp_in_cwnd_reduction(sk))\n\t\t\ttp->prr_out += tcp_skb_pcount(skb);\n\n\t\tif (skb == rtx_head &&\n\t\t    icsk->icsk_pending != ICSK_TIME_REO_TIMEOUT)\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t\t  inet_csk(sk)->icsk_rto,\n\t\t\t\t\t\t  TCP_RTO_MAX);\n\t}\n}\n\n/* We allow to exceed memory limits for FIN packets to expedite\n * connection tear down and (memory) recovery.\n * Otherwise tcp_send_fin() could be tempted to either delay FIN\n * or even be forced to close flow without any FIN.\n * In general, we want to allow one skb per socket to avoid hangs\n * with edge trigger epoll()\n */\nvoid sk_forced_mem_schedule(struct sock *sk, int size)\n{\n\tint amt;\n\n\tif (size <= sk->sk_forward_alloc)\n\t\treturn;\n\tamt = sk_mem_pages(size);\n\tsk->sk_forward_alloc += amt * SK_MEM_QUANTUM;\n\tsk_memory_allocated_add(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_charge_skmem(sk->sk_memcg, amt);\n}\n\n/* Send a FIN. The caller locks the socket for us.\n * We should try to send a FIN packet really hard, but eventually give up.\n */\nvoid tcp_send_fin(struct sock *sk)\n{\n\tstruct sk_buff *skb, *tskb = tcp_write_queue_tail(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Optimization, tack on the FIN if we have one skb in write queue and\n\t * this skb was not yet sent, or we are under memory pressure.\n\t * Note: in the latter case, FIN packet will be sent after a timeout,\n\t * as TCP stack thinks it has already been transmitted.\n\t */\n\tif (!tskb && tcp_under_memory_pressure(sk))\n\t\ttskb = skb_rb_last(&sk->tcp_rtx_queue);\n\n\tif (tskb) {\ncoalesce:\n\t\tTCP_SKB_CB(tskb)->tcp_flags |= TCPHDR_FIN;\n\t\tTCP_SKB_CB(tskb)->end_seq++;\n\t\ttp->write_seq++;\n\t\tif (tcp_write_queue_empty(sk)) {\n\t\t\t/* This means tskb was already sent.\n\t\t\t * Pretend we included the FIN on previous transmit.\n\t\t\t * We need to set tp->snd_nxt to the value it would have\n\t\t\t * if FIN had been sent. This is because retransmit path\n\t\t\t * does not change tp->snd_nxt.\n\t\t\t */\n\t\t\ttp->snd_nxt++;\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tskb = alloc_skb_fclone(MAX_TCP_HEADER, sk->sk_allocation);\n\t\tif (unlikely(!skb)) {\n\t\t\tif (tskb)\n\t\t\t\tgoto coalesce;\n\t\t\treturn;\n\t\t}\n\t\tINIT_LIST_HEAD(&skb->tcp_tsorted_anchor);\n\t\tskb_reserve(skb, MAX_TCP_HEADER);\n\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t/* FIN eats a sequence byte, write_seq advanced by tcp_queue_skb(). */\n\t\ttcp_init_nondata_skb(skb, tp->write_seq,\n\t\t\t\t     TCPHDR_ACK | TCPHDR_FIN);\n\t\ttcp_queue_skb(sk, skb);\n\t}\n\t__tcp_push_pending_frames(sk, tcp_current_mss(sk), TCP_NAGLE_OFF);\n}\n\n/* We get here when a process closes a file descriptor (either due to\n * an explicit close() or as a byproduct of exit()'ing) and there\n * was unread data in the receive queue.  This behavior is recommended\n * by RFC 2525, section 2.17.  -DaveM\n */\nvoid tcp_send_active_reset(struct sock *sk, gfp_t priority)\n{\n\tstruct sk_buff *skb;\n\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_OUTRSTS);\n\n\t/* NOTE: No TCP options attached and we never retransmit this. */\n\tskb = alloc_skb(MAX_TCP_HEADER, priority);\n\tif (!skb) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);\n\t\treturn;\n\t}\n\n\t/* Reserve space for headers and prepare control bits. */\n\tskb_reserve(skb, MAX_TCP_HEADER);\n\ttcp_init_nondata_skb(skb, tcp_acceptable_seq(sk),\n\t\t\t     TCPHDR_ACK | TCPHDR_RST);\n\ttcp_mstamp_refresh(tcp_sk(sk));\n\t/* Send it off. */\n\tif (tcp_transmit_skb(sk, skb, 0, priority))\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);\n\n\t/* skb of trace_tcp_send_reset() keeps the skb that caused RST,\n\t * skb here is different to the troublesome skb, so use NULL\n\t */\n\ttrace_tcp_send_reset(sk, NULL);\n}\n\n/* Send a crossed SYN-ACK during socket establishment.\n * WARNING: This routine must only be called when we have already sent\n * a SYN packet that crossed the incoming SYN that caused this routine\n * to get called. If this assumption fails then the initial rcv_wnd\n * and rcv_wscale values will not be correct.\n */\nint tcp_send_synack(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (!skb || !(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\tpr_err(\"%s: wrong queue state\\n\", __func__);\n\t\treturn -EFAULT;\n\t}\n\tif (!(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_ACK)) {\n\t\tif (skb_cloned(skb)) {\n\t\t\tstruct sk_buff *nskb;\n\n\t\t\ttcp_skb_tsorted_save(skb) {\n\t\t\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\t\t\t} tcp_skb_tsorted_restore(skb);\n\t\t\tif (!nskb)\n\t\t\t\treturn -ENOMEM;\n\t\t\tINIT_LIST_HEAD(&nskb->tcp_tsorted_anchor);\n\t\t\ttcp_highest_sack_replace(sk, skb, nskb);\n\t\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\t\t\t__skb_header_release(nskb);\n\t\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, nskb);\n\t\t\tsk->sk_wmem_queued += nskb->truesize;\n\t\t\tsk_mem_charge(sk, nskb->truesize);\n\t\t\tskb = nskb;\n\t\t}\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ACK;\n\t\ttcp_ecn_send_synack(sk, skb);\n\t}\n\treturn tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n}\n\n/**\n * tcp_make_synack - Prepare a SYN-ACK.\n * sk: listener socket\n * dst: dst entry attached to the SYNACK\n * req: request_sock pointer\n *\n * Allocate one skb and build a SYNACK packet.\n * @dst is consumed : Caller should not use it again.\n */\nstruct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t\tstruct request_sock *req,\n\t\t\t\tstruct tcp_fastopen_cookie *foc,\n\t\t\t\tenum tcp_synack_type synack_type)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_md5sig_key *md5 = NULL;\n\tstruct tcp_out_options opts;\n\tstruct sk_buff *skb;\n\tint tcp_header_size;\n\tstruct tcphdr *th;\n\tint mss;\n\n\tskb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);\n\tif (unlikely(!skb)) {\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\t/* Reserve space for headers. */\n\tskb_reserve(skb, MAX_TCP_HEADER);\n\n\tswitch (synack_type) {\n\tcase TCP_SYNACK_NORMAL:\n\t\tskb_set_owner_w(skb, req_to_sk(req));\n\t\tbreak;\n\tcase TCP_SYNACK_COOKIE:\n\t\t/* Under synflood, we do not attach skb to a socket,\n\t\t * to avoid false sharing.\n\t\t */\n\t\tbreak;\n\tcase TCP_SYNACK_FASTOPEN:\n\t\t/* sk is a const pointer, because we want to express multiple\n\t\t * cpu might call us concurrently.\n\t\t * sk->sk_wmem_alloc in an atomic, we can promote to rw.\n\t\t */\n\t\tskb_set_owner_w(skb, (struct sock *)sk);\n\t\tbreak;\n\t}\n\tskb_dst_set(skb, dst);\n\n\tmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\n\n\tmemset(&opts, 0, sizeof(opts));\n#ifdef CONFIG_SYN_COOKIES\n\tif (unlikely(req->cookie_ts))\n\t\tskb->skb_mstamp = cookie_init_timestamp(req);\n\telse\n#endif\n\t\tskb->skb_mstamp = tcp_clock_us();\n\n#ifdef CONFIG_TCP_MD5SIG\n\trcu_read_lock();\n\tmd5 = tcp_rsk(req)->af_specific->req_md5_lookup(sk, req_to_sk(req));\n#endif\n\tskb_set_hash(skb, tcp_rsk(req)->txhash, PKT_HASH_TYPE_L4);\n\ttcp_header_size = tcp_synack_options(sk, req, mss, skb, &opts, md5,\n\t\t\t\t\t     foc, synack_type) + sizeof(*th);\n\n\tskb_push(skb, tcp_header_size);\n\tskb_reset_transport_header(skb);\n\n\tth = (struct tcphdr *)skb->data;\n\tmemset(th, 0, sizeof(struct tcphdr));\n\tth->syn = 1;\n\tth->ack = 1;\n\ttcp_ecn_make_synack(req, th);\n\tth->source = htons(ireq->ir_num);\n\tth->dest = ireq->ir_rmt_port;\n\tskb->mark = ireq->ir_mark;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tth->seq = htonl(tcp_rsk(req)->snt_isn);\n\t/* XXX data is queued and acked as is. No buffer/window check */\n\tth->ack_seq = htonl(tcp_rsk(req)->rcv_nxt);\n\n\t/* RFC1323: The window in SYN & SYN/ACK segments is never scaled. */\n\tth->window = htons(min(req->rsk_rcv_wnd, 65535U));\n\ttcp_options_write((__be32 *)(th + 1), NULL, &opts);\n\tth->doff = (tcp_header_size >> 2);\n\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_OUTSEGS);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Okay, we have all we need - do the md5 hash if needed */\n\tif (md5)\n\t\ttcp_rsk(req)->af_specific->calc_md5_hash(opts.hash_location,\n\t\t\t\t\t       md5, req_to_sk(req), skb);\n\trcu_read_unlock();\n#endif\n\n\t/* Do not fool tcpdump (if any), clean our debris */\n\tskb->tstamp = 0;\n\treturn skb;\n}\nEXPORT_SYMBOL(tcp_make_synack);\n\nstatic void tcp_ca_dst_init(struct sock *sk, const struct dst_entry *dst)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct tcp_congestion_ops *ca;\n\tu32 ca_key = dst_metric(dst, RTAX_CC_ALGO);\n\n\tif (ca_key == TCP_CA_UNSPEC)\n\t\treturn;\n\n\trcu_read_lock();\n\tca = tcp_ca_find_key(ca_key);\n\tif (likely(ca && try_module_get(ca->owner))) {\n\t\tmodule_put(icsk->icsk_ca_ops->owner);\n\t\ticsk->icsk_ca_dst_locked = tcp_ca_dst_locked(dst);\n\t\ticsk->icsk_ca_ops = ca;\n\t}\n\trcu_read_unlock();\n}\n\n/* Do all connect socket setups that can be done AF independent. */\nstatic void tcp_connect_init(struct sock *sk)\n{\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u8 rcv_wscale;\n\tu32 rcv_wnd;\n\n\t/* We'll fix this up when we get a response from the other end.\n\t * See tcp_input.c:tcp_rcv_state_process case TCP_SYN_SENT.\n\t */\n\ttp->tcp_header_len = sizeof(struct tcphdr);\n\tif (sock_net(sk)->ipv4.sysctl_tcp_timestamps)\n\t\ttp->tcp_header_len += TCPOLEN_TSTAMP_ALIGNED;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (tp->af_specific->md5_lookup(sk, sk))\n\t\ttp->tcp_header_len += TCPOLEN_MD5SIG_ALIGNED;\n#endif\n\n\t/* If user gave his TCP_MAXSEG, record it to clamp */\n\tif (tp->rx_opt.user_mss)\n\t\ttp->rx_opt.mss_clamp = tp->rx_opt.user_mss;\n\ttp->max_window = 0;\n\ttcp_mtup_init(sk);\n\ttcp_sync_mss(sk, dst_mtu(dst));\n\n\ttcp_ca_dst_init(sk, dst);\n\n\tif (!tp->window_clamp)\n\t\ttp->window_clamp = dst_metric(dst, RTAX_WINDOW);\n\ttp->advmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\n\n\ttcp_initialize_rcv_mss(sk);\n\n\t/* limit the window selection if the user enforce a smaller rx buffer */\n\tif (sk->sk_userlocks & SOCK_RCVBUF_LOCK &&\n\t    (tp->window_clamp > tcp_full_space(sk) || tp->window_clamp == 0))\n\t\ttp->window_clamp = tcp_full_space(sk);\n\n\trcv_wnd = tcp_rwnd_init_bpf(sk);\n\tif (rcv_wnd == 0)\n\t\trcv_wnd = dst_metric(dst, RTAX_INITRWND);\n\n\ttcp_select_initial_window(sk, tcp_full_space(sk),\n\t\t\t\t  tp->advmss - (tp->rx_opt.ts_recent_stamp ? tp->tcp_header_len - sizeof(struct tcphdr) : 0),\n\t\t\t\t  &tp->rcv_wnd,\n\t\t\t\t  &tp->window_clamp,\n\t\t\t\t  sock_net(sk)->ipv4.sysctl_tcp_window_scaling,\n\t\t\t\t  &rcv_wscale,\n\t\t\t\t  rcv_wnd);\n\n\ttp->rx_opt.rcv_wscale = rcv_wscale;\n\ttp->rcv_ssthresh = tp->rcv_wnd;\n\n\tsk->sk_err = 0;\n\tsock_reset_flag(sk, SOCK_DONE);\n\ttp->snd_wnd = 0;\n\ttcp_init_wl(tp, 0);\n\ttcp_write_queue_purge(sk);\n\ttp->snd_una = tp->write_seq;\n\ttp->snd_sml = tp->write_seq;\n\ttp->snd_up = tp->write_seq;\n\ttp->snd_nxt = tp->write_seq;\n\n\tif (likely(!tp->repair))\n\t\ttp->rcv_nxt = 0;\n\telse\n\t\ttp->rcv_tstamp = tcp_jiffies32;\n\ttp->rcv_wup = tp->rcv_nxt;\n\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\n\tinet_csk(sk)->icsk_rto = tcp_timeout_init(sk);\n\tinet_csk(sk)->icsk_retransmits = 0;\n\ttcp_clear_retrans(tp);\n}\n\nstatic void tcp_connect_queue_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\ttcb->end_seq += skb->len;\n\t__skb_header_release(skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n\tWRITE_ONCE(tp->write_seq, tcb->end_seq);\n\ttp->packets_out += tcp_skb_pcount(skb);\n}\n\n/* Build and send a SYN with data and (cached) Fast Open cookie. However,\n * queue a data-only packet after the regular SYN, such that regular SYNs\n * are retransmitted on timeouts. Also if the remote SYN-ACK acknowledges\n * only the SYN sequence, the data are retransmitted in the first ACK.\n * If cookie is not cached or other error occurs, falls back to send a\n * regular SYN with Fast Open cookie request option.\n */\nstatic int tcp_send_syn_data(struct sock *sk, struct sk_buff *syn)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_fastopen_request *fo = tp->fastopen_req;\n\tint space, err = 0;\n\tstruct sk_buff *syn_data;\n\n\ttp->rx_opt.mss_clamp = tp->advmss;  /* If MSS is not cached */\n\tif (!tcp_fastopen_cookie_check(sk, &tp->rx_opt.mss_clamp, &fo->cookie))\n\t\tgoto fallback;\n\n\t/* MSS for SYN-data is based on cached MSS and bounded by PMTU and\n\t * user-MSS. Reserve maximum option space for middleboxes that add\n\t * private TCP options. The cost is reduced data space in SYN :(\n\t */\n\ttp->rx_opt.mss_clamp = tcp_mss_clamp(tp, tp->rx_opt.mss_clamp);\n\n\tspace = __tcp_mtu_to_mss(sk, inet_csk(sk)->icsk_pmtu_cookie) -\n\t\tMAX_TCP_OPTION_SPACE;\n\n\tspace = min_t(size_t, space, fo->size);\n\n\t/* limit to order-0 allocations */\n\tspace = min_t(size_t, space, SKB_MAX_HEAD(MAX_TCP_HEADER));\n\n\tsyn_data = sk_stream_alloc_skb(sk, space, sk->sk_allocation, false);\n\tif (!syn_data)\n\t\tgoto fallback;\n\tsyn_data->ip_summed = CHECKSUM_PARTIAL;\n\tmemcpy(syn_data->cb, syn->cb, sizeof(syn->cb));\n\tif (space) {\n\t\tint copied = copy_from_iter(skb_put(syn_data, space), space,\n\t\t\t\t\t    &fo->data->msg_iter);\n\t\tif (unlikely(!copied)) {\n\t\t\ttcp_skb_tsorted_anchor_cleanup(syn_data);\n\t\t\tkfree_skb(syn_data);\n\t\t\tgoto fallback;\n\t\t}\n\t\tif (copied != space) {\n\t\t\tskb_trim(syn_data, copied);\n\t\t\tspace = copied;\n\t\t}\n\t}\n\t/* No more data pending in inet_wait_for_connect() */\n\tif (space == fo->size)\n\t\tfo->data = NULL;\n\tfo->copied = space;\n\n\ttcp_connect_queue_skb(sk, syn_data);\n\tif (syn_data->len)\n\t\ttcp_chrono_start(sk, TCP_CHRONO_BUSY);\n\n\terr = tcp_transmit_skb(sk, syn_data, 1, sk->sk_allocation);\n\n\tsyn->skb_mstamp = syn_data->skb_mstamp;\n\n\t/* Now full SYN+DATA was cloned and sent (or not),\n\t * remove the SYN from the original skb (syn_data)\n\t * we keep in write queue in case of a retransmit, as we\n\t * also have the SYN packet (with no data) in the same queue.\n\t */\n\tTCP_SKB_CB(syn_data)->seq++;\n\tTCP_SKB_CB(syn_data)->tcp_flags = TCPHDR_ACK | TCPHDR_PSH;\n\tif (!err) {\n\t\ttp->syn_data = (fo->copied > 0);\n\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, syn_data);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPORIGDATASENT);\n\t\tgoto done;\n\t}\n\n\t/* data was not sent, put it in write_queue */\n\t__skb_queue_tail(&sk->sk_write_queue, syn_data);\n\ttp->packets_out -= tcp_skb_pcount(syn_data);\n\nfallback:\n\t/* Send a regular SYN with Fast Open cookie request option */\n\tif (fo->cookie.len > 0)\n\t\tfo->cookie.len = 0;\n\terr = tcp_transmit_skb(sk, syn, 1, sk->sk_allocation);\n\tif (err)\n\t\ttp->syn_fastopen = 0;\ndone:\n\tfo->cookie.len = -1;  /* Exclude Fast Open option for SYN retries */\n\treturn err;\n}\n\n/* Build a SYN and send it off. */\nint tcp_connect(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint err;\n\n\ttcp_call_bpf(sk, BPF_SOCK_OPS_TCP_CONNECT_CB, 0, NULL);\n\n\tif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\n\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\n\n\ttcp_connect_init(sk);\n\n\tif (unlikely(tp->repair)) {\n\t\ttcp_finish_connect(sk, NULL);\n\t\treturn 0;\n\t}\n\n\tbuff = sk_stream_alloc_skb(sk, 0, sk->sk_allocation, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOBUFS;\n\n\ttcp_init_nondata_skb(buff, tp->write_seq++, TCPHDR_SYN);\n\ttcp_mstamp_refresh(tp);\n\ttp->retrans_stamp = tcp_time_stamp(tp);\n\ttcp_connect_queue_skb(sk, buff);\n\ttcp_ecn_send_syn(sk, buff);\n\ttcp_rbtree_insert(&sk->tcp_rtx_queue, buff);\n\n\t/* Send off SYN; include data in Fast Open. */\n\terr = tp->fastopen_req ? tcp_send_syn_data(sk, buff) :\n\t      tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);\n\tif (err == -ECONNREFUSED)\n\t\treturn err;\n\n\t/* We change tp->snd_nxt after the tcp_transmit_skb() call\n\t * in order to make this packet get counted in tcpOutSegs.\n\t */\n\ttp->snd_nxt = tp->write_seq;\n\ttp->pushed_seq = tp->write_seq;\n\tbuff = tcp_send_head(sk);\n\tif (unlikely(buff)) {\n\t\ttp->snd_nxt\t= TCP_SKB_CB(buff)->seq;\n\t\ttp->pushed_seq\t= TCP_SKB_CB(buff)->seq;\n\t}\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ACTIVEOPENS);\n\n\t/* Timer for repeating the SYN until an answer. */\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_connect);\n\n/* Send out a delayed ack, the caller does the policy checking\n * to see if we should even be here.  See tcp_input.c:tcp_ack_snd_check()\n * for details.\n */\nvoid tcp_send_delayed_ack(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint ato = icsk->icsk_ack.ato;\n\tunsigned long timeout;\n\n\tif (ato > TCP_DELACK_MIN) {\n\t\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t\tint max_ato = HZ / 2;\n\n\t\tif (icsk->icsk_ack.pingpong ||\n\t\t    (icsk->icsk_ack.pending & ICSK_ACK_PUSHED))\n\t\t\tmax_ato = TCP_DELACK_MAX;\n\n\t\t/* Slow path, intersegment interval is \"high\". */\n\n\t\t/* If some rtt estimate is known, use it to bound delayed ack.\n\t\t * Do not use inet_csk(sk)->icsk_rto here, use results of rtt measurements\n\t\t * directly.\n\t\t */\n\t\tif (tp->srtt_us) {\n\t\t\tint rtt = max_t(int, usecs_to_jiffies(tp->srtt_us >> 3),\n\t\t\t\t\tTCP_DELACK_MIN);\n\n\t\t\tif (rtt < max_ato)\n\t\t\t\tmax_ato = rtt;\n\t\t}\n\n\t\tato = min(ato, max_ato);\n\t}\n\n\t/* Stay within the limit we were given */\n\ttimeout = jiffies + ato;\n\n\t/* Use new timeout only if there wasn't a older one earlier. */\n\tif (icsk->icsk_ack.pending & ICSK_ACK_TIMER) {\n\t\t/* If delack timer was blocked or is about to expire,\n\t\t * send ACK now.\n\t\t */\n\t\tif (icsk->icsk_ack.blocked ||\n\t\t    time_before_eq(icsk->icsk_ack.timeout, jiffies + (ato >> 2))) {\n\t\t\ttcp_send_ack(sk);\n\t\t\treturn;\n\t\t}\n\n\t\tif (!time_before(timeout, icsk->icsk_ack.timeout))\n\t\t\ttimeout = icsk->icsk_ack.timeout;\n\t}\n\ticsk->icsk_ack.pending |= ICSK_ACK_SCHED | ICSK_ACK_TIMER;\n\ticsk->icsk_ack.timeout = timeout;\n\tsk_reset_timer(sk, &icsk->icsk_delack_timer, timeout);\n}\n\n/* This routine sends an ack and also updates the window. */\nvoid __tcp_send_ack(struct sock *sk, u32 rcv_nxt)\n{\n\tstruct sk_buff *buff;\n\n\t/* If we have been reset, we may not send again. */\n\tif (sk->sk_state == TCP_CLOSE)\n\t\treturn;\n\n\t/* We are not putting this on the write queue, so\n\t * tcp_transmit_skb() will set the ownership to this\n\t * sock.\n\t */\n\tbuff = alloc_skb(MAX_TCP_HEADER,\n\t\t\t sk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\n\tif (unlikely(!buff)) {\n\t\tinet_csk_schedule_ack(sk);\n\t\tinet_csk(sk)->icsk_ack.ato = TCP_ATO_MIN;\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,\n\t\t\t\t\t  TCP_DELACK_MAX, TCP_RTO_MAX);\n\t\treturn;\n\t}\n\n\t/* Reserve space for headers and prepare control bits. */\n\tskb_reserve(buff, MAX_TCP_HEADER);\n\ttcp_init_nondata_skb(buff, tcp_acceptable_seq(sk), TCPHDR_ACK);\n\n\t/* We do not want pure acks influencing TCP Small Queues or fq/pacing\n\t * too much.\n\t * SKB_TRUESIZE(max(1 .. 66, MAX_TCP_HEADER)) is unfortunately ~784\n\t */\n\tskb_set_tcp_pure_ack(buff);\n\n\t/* Send it off, this clears delayed acks for us. */\n\t__tcp_transmit_skb(sk, buff, 0, (__force gfp_t)0, rcv_nxt);\n}\nEXPORT_SYMBOL_GPL(__tcp_send_ack);\n\nvoid tcp_send_ack(struct sock *sk)\n{\n\t__tcp_send_ack(sk, tcp_sk(sk)->rcv_nxt);\n}\n\n/* This routine sends a packet with an out of date sequence\n * number. It assumes the other end will try to ack it.\n *\n * Question: what should we make while urgent mode?\n * 4.4BSD forces sending single byte of data. We cannot send\n * out of window data, because we have SND.NXT==SND.MAX...\n *\n * Current solution: to send TWO zero-length segments in urgent mode:\n * one is with SEG.SEQ=SND.UNA to deliver urgent pointer, another is\n * out-of-date with SND.UNA-1 to probe window.\n */\nstatic int tcp_xmit_probe_skb(struct sock *sk, int urgent, int mib)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\t/* We don't queue it, tcp_transmit_skb() sets ownership. */\n\tskb = alloc_skb(MAX_TCP_HEADER,\n\t\t\tsk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\n\tif (!skb)\n\t\treturn -1;\n\n\t/* Reserve space for headers and set control bits. */\n\tskb_reserve(skb, MAX_TCP_HEADER);\n\t/* Use a previous sequence.  This should cause the other\n\t * end to send an ack.  Don't queue or clone SKB, just\n\t * send it.\n\t */\n\ttcp_init_nondata_skb(skb, tp->snd_una - !urgent, TCPHDR_ACK);\n\tNET_INC_STATS(sock_net(sk), mib);\n\treturn tcp_transmit_skb(sk, skb, 0, (__force gfp_t)0);\n}\n\n/* Called from setsockopt( ... TCP_REPAIR ) */\nvoid tcp_send_window_probe(struct sock *sk)\n{\n\tif (sk->sk_state == TCP_ESTABLISHED) {\n\t\ttcp_sk(sk)->snd_wl1 = tcp_sk(sk)->rcv_nxt - 1;\n\t\ttcp_mstamp_refresh(tcp_sk(sk));\n\t\ttcp_xmit_probe_skb(sk, 0, LINUX_MIB_TCPWINPROBE);\n\t}\n}\n\n/* Initiate keepalive or window probe from timer. */\nint tcp_write_wakeup(struct sock *sk, int mib)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\treturn -1;\n\n\tskb = tcp_send_head(sk);\n\tif (skb && before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp))) {\n\t\tint err;\n\t\tunsigned int mss = tcp_current_mss(sk);\n\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t\tif (before(tp->pushed_seq, TCP_SKB_CB(skb)->end_seq))\n\t\t\ttp->pushed_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t/* We are probing the opening of a window\n\t\t * but the window size is != 0\n\t\t * must have been a result SWS avoidance ( sender )\n\t\t */\n\t\tif (seg_size < TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq ||\n\t\t    skb->len > mss) {\n\t\t\tseg_size = min(seg_size, mss);\n\t\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\n\t\t\t\treturn -1;\n\t\t} else if (!tcp_skb_pcount(skb))\n\t\t\ttcp_set_skb_tso_segs(skb, mss);\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n\t\tif (!err)\n\t\t\ttcp_event_new_data_sent(sk, skb);\n\t\treturn err;\n\t} else {\n\t\tif (between(tp->snd_up, tp->snd_una + 1, tp->snd_una + 0xFFFF))\n\t\t\ttcp_xmit_probe_skb(sk, 1, mib);\n\t\treturn tcp_xmit_probe_skb(sk, 0, mib);\n\t}\n}\n\n/* A window probe timeout has occurred.  If window is not closed send\n * a partial packet else a zero probe.\n */\nvoid tcp_send_probe0(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tunsigned long probe_max;\n\tint err;\n\n\terr = tcp_write_wakeup(sk, LINUX_MIB_TCPWINPROBE);\n\n\tif (tp->packets_out || tcp_write_queue_empty(sk)) {\n\t\t/* Cancel probe timer, if it is not required. */\n\t\ticsk->icsk_probes_out = 0;\n\t\ticsk->icsk_backoff = 0;\n\t\treturn;\n\t}\n\n\tif (err <= 0) {\n\t\tif (icsk->icsk_backoff < net->ipv4.sysctl_tcp_retries2)\n\t\t\ticsk->icsk_backoff++;\n\t\ticsk->icsk_probes_out++;\n\t\tprobe_max = TCP_RTO_MAX;\n\t} else {\n\t\t/* If packet was not sent due to local congestion,\n\t\t * do not backoff and do not remember icsk_probes_out.\n\t\t * Let local senders to fight for local resources.\n\t\t *\n\t\t * Use accumulated backoff yet.\n\t\t */\n\t\tif (!icsk->icsk_probes_out)\n\t\t\ticsk->icsk_probes_out = 1;\n\t\tprobe_max = TCP_RESOURCE_PROBE_INTERVAL;\n\t}\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\n\t\t\t\t  tcp_probe0_when(sk, probe_max),\n\t\t\t\t  TCP_RTO_MAX);\n}\n\nint tcp_rtx_synack(const struct sock *sk, struct request_sock *req)\n{\n\tconst struct tcp_request_sock_ops *af_ops = tcp_rsk(req)->af_specific;\n\tstruct flowi fl;\n\tint res;\n\n\ttcp_rsk(req)->txhash = net_tx_rndhash();\n\tres = af_ops->send_synack(sk, NULL, &fl, req, NULL, TCP_SYNACK_NORMAL);\n\tif (!res) {\n\t\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\n\t\tif (unlikely(tcp_passive_fastopen(sk)))\n\t\t\ttcp_sk(sk)->total_retrans++;\n\t\ttrace_tcp_retransmit_synack(sk, req);\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL(tcp_rtx_synack);\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n */\n\n/*\n * Changes:\tPedro Roque\t:\tRetransmit queue handled by TCP.\n *\t\t\t\t:\tFragmentation on mtu decrease\n *\t\t\t\t:\tSegment collapse on retransmit\n *\t\t\t\t:\tAF independence\n *\n *\t\tLinus Torvalds\t:\tsend_delayed_ack\n *\t\tDavid S. Miller\t:\tCharge memory using the right skb\n *\t\t\t\t\tduring syn/ack processing.\n *\t\tDavid S. Miller :\tOutput engine completely rewritten.\n *\t\tAndrea Arcangeli:\tSYNACK carry ts_recent in tsecr.\n *\t\tCacophonix Gaul :\tdraft-minshall-nagle-01\n *\t\tJ Hadi Salim\t:\tECN support\n *\n */\n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <net/tcp.h>\n\n#include <linux/compiler.h>\n#include <linux/gfp.h>\n#include <linux/module.h>\n#include <linux/static_key.h>\n\n#include <trace/events/tcp.h>\n\nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\n\t\t\t   int push_one, gfp_t gfp);\n\n/* Account for new data that has been sent to the network. */\nstatic void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int prior_packets = tp->packets_out;\n\n\ttp->snd_nxt = TCP_SKB_CB(skb)->end_seq;\n\n\t__skb_unlink(skb, &sk->sk_write_queue);\n\ttcp_rbtree_insert(&sk->tcp_rtx_queue, skb);\n\n\tif (tp->highest_sack == NULL)\n\t\ttp->highest_sack = skb;\n\n\ttp->packets_out += tcp_skb_pcount(skb);\n\tif (!prior_packets || icsk->icsk_pending == ICSK_TIME_LOSS_PROBE)\n\t\ttcp_rearm_rto(sk);\n\n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPORIGDATASENT,\n\t\t      tcp_skb_pcount(skb));\n}\n\n/* SND.NXT, if window was not shrunk or the amount of shrunk was less than one\n * window scaling factor due to loss of precision.\n * If window has been shrunk, what should we make? It is not clear at all.\n * Using SND.UNA we will fail to open window, SND.NXT is out of window. :-(\n * Anything in between SND.UNA...SND.UNA+SND.WND also can be already\n * invalid. OK, let's make this for now:\n */\nstatic inline __u32 tcp_acceptable_seq(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!before(tcp_wnd_end(tp), tp->snd_nxt) ||\n\t    (tp->rx_opt.wscale_ok &&\n\t     ((tp->snd_nxt - tcp_wnd_end(tp)) < (1 << tp->rx_opt.rcv_wscale))))\n\t\treturn tp->snd_nxt;\n\telse\n\t\treturn tcp_wnd_end(tp);\n}\n\n/* Calculate mss to advertise in SYN segment.\n * RFC1122, RFC1063, draft-ietf-tcpimpl-pmtud-01 state that:\n *\n * 1. It is independent of path mtu.\n * 2. Ideally, it is maximal possible segment size i.e. 65535-40.\n * 3. For IPv4 it is reasonable to calculate it from maximal MTU of\n *    attached devices, because some buggy hosts are confused by\n *    large MSS.\n * 4. We do not make 3, we advertise MSS, calculated from first\n *    hop device mtu, but allow to raise it to ip_rt_min_advmss.\n *    This may be overridden via information stored in routing table.\n * 5. Value 65535 for MSS is valid in IPv6 and means \"as large as possible,\n *    probably even Jumbo\".\n */\nstatic __u16 tcp_advertise_mss(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tint mss = tp->advmss;\n\n\tif (dst) {\n\t\tunsigned int metric = dst_metric_advmss(dst);\n\n\t\tif (metric < mss) {\n\t\t\tmss = metric;\n\t\t\ttp->advmss = mss;\n\t\t}\n\t}\n\n\treturn (__u16)mss;\n}\n\n/* RFC2861. Reset CWND after idle period longer RTO to \"restart window\".\n * This is the first part of cwnd validation mechanism.\n */\nvoid tcp_cwnd_restart(struct sock *sk, s32 delta)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 restart_cwnd = tcp_init_cwnd(tp, __sk_dst_get(sk));\n\tu32 cwnd = tp->snd_cwnd;\n\n\ttcp_ca_event(sk, CA_EVENT_CWND_RESTART);\n\n\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\trestart_cwnd = min(restart_cwnd, cwnd);\n\n\twhile ((delta -= inet_csk(sk)->icsk_rto) > 0 && cwnd > restart_cwnd)\n\t\tcwnd >>= 1;\n\ttp->snd_cwnd = max(cwnd, restart_cwnd);\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->snd_cwnd_used = 0;\n}\n\n/* Congestion state accounting after a packet has been sent. */\nstatic void tcp_event_data_sent(struct tcp_sock *tp,\n\t\t\t\tstruct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst u32 now = tcp_jiffies32;\n\n\tif (tcp_packets_in_flight(tp) == 0)\n\t\ttcp_ca_event(sk, CA_EVENT_TX_START);\n\n\ttp->lsndtime = now;\n\n\t/* If it is a reply for ato after last received\n\t * packet, enter pingpong mode.\n\t */\n\tif ((u32)(now - icsk->icsk_ack.lrcvtime) < icsk->icsk_ack.ato)\n\t\ticsk->icsk_ack.pingpong = 1;\n}\n\n/* Account for an ACK we sent. */\nstatic inline void tcp_event_ack_sent(struct sock *sk, unsigned int pkts,\n\t\t\t\t      u32 rcv_nxt)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (unlikely(tp->compressed_ack > TCP_FASTRETRANS_THRESH)) {\n\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,\n\t\t\t      tp->compressed_ack - TCP_FASTRETRANS_THRESH);\n\t\ttp->compressed_ack = TCP_FASTRETRANS_THRESH;\n\t\tif (hrtimer_try_to_cancel(&tp->compressed_ack_timer) == 1)\n\t\t\t__sock_put(sk);\n\t}\n\n\tif (unlikely(rcv_nxt != tp->rcv_nxt))\n\t\treturn;  /* Special ACK sent by DCTCP to reflect ECN */\n\ttcp_dec_quickack_mode(sk, pkts);\n\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);\n}\n\n/* Determine a window scaling and initial window to offer.\n * Based on the assumption that the given amount of space\n * will be offered. Store the results in the tp structure.\n * NOTE: for smooth operation initial space offering should\n * be a multiple of mss if possible. We assume here that mss >= 1.\n * This MUST be enforced by all callers.\n */\nvoid tcp_select_initial_window(const struct sock *sk, int __space, __u32 mss,\n\t\t\t       __u32 *rcv_wnd, __u32 *window_clamp,\n\t\t\t       int wscale_ok, __u8 *rcv_wscale,\n\t\t\t       __u32 init_rcv_wnd)\n{\n\tunsigned int space = (__space < 0 ? 0 : __space);\n\n\t/* If no clamp set the clamp to the max possible scaled window */\n\tif (*window_clamp == 0)\n\t\t(*window_clamp) = (U16_MAX << TCP_MAX_WSCALE);\n\tspace = min(*window_clamp, space);\n\n\t/* Quantize space offering to a multiple of mss if possible. */\n\tif (space > mss)\n\t\tspace = rounddown(space, mss);\n\n\t/* NOTE: offering an initial window larger than 32767\n\t * will break some buggy TCP stacks. If the admin tells us\n\t * it is likely we could be speaking with such a buggy stack\n\t * we will truncate our initial window offering to 32K-1\n\t * unless the remote has sent us a window scaling option,\n\t * which we interpret as a sign the remote TCP is not\n\t * misinterpreting the window field as a signed quantity.\n\t */\n\tif (sock_net(sk)->ipv4.sysctl_tcp_workaround_signed_windows)\n\t\t(*rcv_wnd) = min(space, MAX_TCP_WINDOW);\n\telse\n\t\t(*rcv_wnd) = min_t(u32, space, U16_MAX);\n\n\tif (init_rcv_wnd)\n\t\t*rcv_wnd = min(*rcv_wnd, init_rcv_wnd * mss);\n\n\t(*rcv_wscale) = 0;\n\tif (wscale_ok) {\n\t\t/* Set window scaling on max possible window */\n\t\tspace = max_t(u32, space, sock_net(sk)->ipv4.sysctl_tcp_rmem[2]);\n\t\tspace = max_t(u32, space, sysctl_rmem_max);\n\t\tspace = min_t(u32, space, *window_clamp);\n\t\twhile (space > U16_MAX && (*rcv_wscale) < TCP_MAX_WSCALE) {\n\t\t\tspace >>= 1;\n\t\t\t(*rcv_wscale)++;\n\t\t}\n\t}\n\t/* Set the clamp no higher than max representable value */\n\t(*window_clamp) = min_t(__u32, U16_MAX << (*rcv_wscale), *window_clamp);\n}\nEXPORT_SYMBOL(tcp_select_initial_window);\n\n/* Chose a new window to advertise, update state in tcp_sock for the\n * socket, and return result with RFC1323 scaling applied.  The return\n * value can be stuffed directly into th->window for an outgoing\n * frame.\n */\nstatic u16 tcp_select_window(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 old_win = tp->rcv_wnd;\n\tu32 cur_win = tcp_receive_window(tp);\n\tu32 new_win = __tcp_select_window(sk);\n\n\t/* Never shrink the offered window */\n\tif (new_win < cur_win) {\n\t\t/* Danger Will Robinson!\n\t\t * Don't update rcv_wup/rcv_wnd here or else\n\t\t * we will not be able to advertise a zero\n\t\t * window in time.  --DaveM\n\t\t *\n\t\t * Relax Will Robinson.\n\t\t */\n\t\tif (new_win == 0)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPWANTZEROWINDOWADV);\n\t\tnew_win = ALIGN(cur_win, 1 << tp->rx_opt.rcv_wscale);\n\t}\n\ttp->rcv_wnd = new_win;\n\ttp->rcv_wup = tp->rcv_nxt;\n\n\t/* Make sure we do not exceed the maximum possible\n\t * scaled window.\n\t */\n\tif (!tp->rx_opt.rcv_wscale &&\n\t    sock_net(sk)->ipv4.sysctl_tcp_workaround_signed_windows)\n\t\tnew_win = min(new_win, MAX_TCP_WINDOW);\n\telse\n\t\tnew_win = min(new_win, (65535U << tp->rx_opt.rcv_wscale));\n\n\t/* RFC1323 scaling applied */\n\tnew_win >>= tp->rx_opt.rcv_wscale;\n\n\t/* If we advertise zero window, disable fast path. */\n\tif (new_win == 0) {\n\t\ttp->pred_flags = 0;\n\t\tif (old_win)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPTOZEROWINDOWADV);\n\t} else if (old_win == 0) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFROMZEROWINDOWADV);\n\t}\n\n\treturn new_win;\n}\n\n/* Packet ECN state for a SYN-ACK */\nstatic void tcp_ecn_send_synack(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_CWR;\n\tif (!(tp->ecn_flags & TCP_ECN_OK))\n\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_ECE;\n\telse if (tcp_ca_needs_ecn(sk) ||\n\t\t tcp_bpf_ca_needs_ecn(sk))\n\t\tINET_ECN_xmit(sk);\n}\n\n/* Packet ECN state for a SYN.  */\nstatic void tcp_ecn_send_syn(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool bpf_needs_ecn = tcp_bpf_ca_needs_ecn(sk);\n\tbool use_ecn = sock_net(sk)->ipv4.sysctl_tcp_ecn == 1 ||\n\t\ttcp_ca_needs_ecn(sk) || bpf_needs_ecn;\n\n\tif (!use_ecn) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_feature(dst, RTAX_FEATURE_ECN))\n\t\t\tuse_ecn = true;\n\t}\n\n\ttp->ecn_flags = 0;\n\n\tif (use_ecn) {\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ECE | TCPHDR_CWR;\n\t\ttp->ecn_flags = TCP_ECN_OK;\n\t\tif (tcp_ca_needs_ecn(sk) || bpf_needs_ecn)\n\t\t\tINET_ECN_xmit(sk);\n\t}\n}\n\nstatic void tcp_ecn_clear_syn(struct sock *sk, struct sk_buff *skb)\n{\n\tif (sock_net(sk)->ipv4.sysctl_tcp_ecn_fallback)\n\t\t/* tp->ecn_flags are cleared at a later point in time when\n\t\t * SYN ACK is ultimatively being received.\n\t\t */\n\t\tTCP_SKB_CB(skb)->tcp_flags &= ~(TCPHDR_ECE | TCPHDR_CWR);\n}\n\nstatic void\ntcp_ecn_make_synack(const struct request_sock *req, struct tcphdr *th)\n{\n\tif (inet_rsk(req)->ecn_ok)\n\t\tth->ece = 1;\n}\n\n/* Set up ECN state for a packet on a ESTABLISHED socket that is about to\n * be sent.\n */\nstatic void tcp_ecn_send(struct sock *sk, struct sk_buff *skb,\n\t\t\t struct tcphdr *th, int tcp_header_len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->ecn_flags & TCP_ECN_OK) {\n\t\t/* Not-retransmitted data segment: set ECT and inject CWR. */\n\t\tif (skb->len != tcp_header_len &&\n\t\t    !before(TCP_SKB_CB(skb)->seq, tp->snd_nxt)) {\n\t\t\tINET_ECN_xmit(sk);\n\t\t\tif (tp->ecn_flags & TCP_ECN_QUEUE_CWR) {\n\t\t\t\ttp->ecn_flags &= ~TCP_ECN_QUEUE_CWR;\n\t\t\t\tth->cwr = 1;\n\t\t\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_ECN;\n\t\t\t}\n\t\t} else if (!tcp_ca_needs_ecn(sk)) {\n\t\t\t/* ACK or retransmitted segment: clear ECT|CE */\n\t\t\tINET_ECN_dontxmit(sk);\n\t\t}\n\t\tif (tp->ecn_flags & TCP_ECN_DEMAND_CWR)\n\t\t\tth->ece = 1;\n\t}\n}\n\n/* Constructs common control bits of non-data skb. If SYN/FIN is present,\n * auto increment end seqno.\n */\nstatic void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)\n{\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tTCP_SKB_CB(skb)->tcp_flags = flags;\n\tTCP_SKB_CB(skb)->sacked = 0;\n\n\ttcp_skb_pcount_set(skb, 1);\n\n\tTCP_SKB_CB(skb)->seq = seq;\n\tif (flags & (TCPHDR_SYN | TCPHDR_FIN))\n\t\tseq++;\n\tTCP_SKB_CB(skb)->end_seq = seq;\n}\n\nstatic inline bool tcp_urg_mode(const struct tcp_sock *tp)\n{\n\treturn tp->snd_una != tp->snd_up;\n}\n\n#define OPTION_SACK_ADVERTISE\t(1 << 0)\n#define OPTION_TS\t\t(1 << 1)\n#define OPTION_MD5\t\t(1 << 2)\n#define OPTION_WSCALE\t\t(1 << 3)\n#define OPTION_FAST_OPEN_COOKIE\t(1 << 8)\n#define OPTION_SMC\t\t(1 << 9)\n\nstatic void smc_options_write(__be32 *ptr, u16 *options)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (unlikely(OPTION_SMC & *options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t\t       (TCPOPT_EXP <<  8) |\n\t\t\t\t       (TCPOLEN_EXP_SMC_BASE));\n\t\t\t*ptr++ = htonl(TCPOPT_SMC_MAGIC);\n\t\t}\n\t}\n#endif\n}\n\nstruct tcp_out_options {\n\tu16 options;\t\t/* bit field of OPTION_* */\n\tu16 mss;\t\t/* 0 to disable */\n\tu8 ws;\t\t\t/* window scale, 0 to disable */\n\tu8 num_sack_blocks;\t/* number of SACK blocks to include */\n\tu8 hash_size;\t\t/* bytes in hash_location */\n\t__u8 *hash_location;\t/* temporary pointer, overloaded */\n\t__u32 tsval, tsecr;\t/* need to include OPTION_TS */\n\tstruct tcp_fastopen_cookie *fastopen_cookie;\t/* Fast open cookie */\n};\n\n/* Write previously computed TCP options to the packet.\n *\n * Beware: Something in the Internet is very sensitive to the ordering of\n * TCP options, we learned this through the hard way, so be careful here.\n * Luckily we can at least blame others for their non-compliance but from\n * inter-operability perspective it seems that we're somewhat stuck with\n * the ordering which we have been using if we want to keep working with\n * those broken things (not that it currently hurts anybody as there isn't\n * particular reason why the ordering would need to be changed).\n *\n * At least SACK_PERM as the first option is known to lead to a disaster\n * (but it may well be that other scenarios fail similarly).\n */\nstatic void tcp_options_write(__be32 *ptr, struct tcp_sock *tp,\n\t\t\t      struct tcp_out_options *opts)\n{\n\tu16 options = opts->options;\t/* mungable copy */\n\n\tif (unlikely(OPTION_MD5 & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\t/* overload cookie hash location */\n\t\topts->hash_location = (__u8 *)ptr;\n\t\tptr += 4;\n\t}\n\n\tif (unlikely(opts->mss)) {\n\t\t*ptr++ = htonl((TCPOPT_MSS << 24) |\n\t\t\t       (TCPOLEN_MSS << 16) |\n\t\t\t       opts->mss);\n\t}\n\n\tif (likely(OPTION_TS & options)) {\n\t\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |\n\t\t\t\t       (TCPOLEN_SACK_PERM << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t\toptions &= ~OPTION_SACK_ADVERTISE;\n\t\t} else {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t}\n\t\t*ptr++ = htonl(opts->tsval);\n\t\t*ptr++ = htonl(opts->tsecr);\n\t}\n\n\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_SACK_PERM << 8) |\n\t\t\t       TCPOLEN_SACK_PERM);\n\t}\n\n\tif (unlikely(OPTION_WSCALE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_WINDOW << 16) |\n\t\t\t       (TCPOLEN_WINDOW << 8) |\n\t\t\t       opts->ws);\n\t}\n\n\tif (unlikely(opts->num_sack_blocks)) {\n\t\tstruct tcp_sack_block *sp = tp->rx_opt.dsack ?\n\t\t\ttp->duplicate_sack : tp->selective_acks;\n\t\tint this_sack;\n\n\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t       (TCPOPT_SACK <<  8) |\n\t\t\t       (TCPOLEN_SACK_BASE + (opts->num_sack_blocks *\n\t\t\t\t\t\t     TCPOLEN_SACK_PERBLOCK)));\n\n\t\tfor (this_sack = 0; this_sack < opts->num_sack_blocks;\n\t\t     ++this_sack) {\n\t\t\t*ptr++ = htonl(sp[this_sack].start_seq);\n\t\t\t*ptr++ = htonl(sp[this_sack].end_seq);\n\t\t}\n\n\t\ttp->rx_opt.dsack = 0;\n\t}\n\n\tif (unlikely(OPTION_FAST_OPEN_COOKIE & options)) {\n\t\tstruct tcp_fastopen_cookie *foc = opts->fastopen_cookie;\n\t\tu8 *p = (u8 *)ptr;\n\t\tu32 len; /* Fast Open option length */\n\n\t\tif (foc->exp) {\n\t\t\tlen = TCPOLEN_EXP_FASTOPEN_BASE + foc->len;\n\t\t\t*ptr = htonl((TCPOPT_EXP << 24) | (len << 16) |\n\t\t\t\t     TCPOPT_FASTOPEN_MAGIC);\n\t\t\tp += TCPOLEN_EXP_FASTOPEN_BASE;\n\t\t} else {\n\t\t\tlen = TCPOLEN_FASTOPEN_BASE + foc->len;\n\t\t\t*p++ = TCPOPT_FASTOPEN;\n\t\t\t*p++ = len;\n\t\t}\n\n\t\tmemcpy(p, foc->val, foc->len);\n\t\tif ((len & 3) == 2) {\n\t\t\tp[foc->len] = TCPOPT_NOP;\n\t\t\tp[foc->len + 1] = TCPOPT_NOP;\n\t\t}\n\t\tptr += (len + 3) >> 2;\n\t}\n\n\tsmc_options_write(ptr, &options);\n}\n\nstatic void smc_set_option(const struct tcp_sock *tp,\n\t\t\t   struct tcp_out_options *opts,\n\t\t\t   unsigned int *remaining)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc) {\n\t\t\tif (*remaining >= TCPOLEN_EXP_SMC_BASE_ALIGNED) {\n\t\t\t\topts->options |= OPTION_SMC;\n\t\t\t\t*remaining -= TCPOLEN_EXP_SMC_BASE_ALIGNED;\n\t\t\t}\n\t\t}\n\t}\n#endif\n}\n\nstatic void smc_set_option_cond(const struct tcp_sock *tp,\n\t\t\t\tconst struct inet_request_sock *ireq,\n\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\tunsigned int *remaining)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc && ireq->smc_ok) {\n\t\t\tif (*remaining >= TCPOLEN_EXP_SMC_BASE_ALIGNED) {\n\t\t\t\topts->options |= OPTION_SMC;\n\t\t\t\t*remaining -= TCPOLEN_EXP_SMC_BASE_ALIGNED;\n\t\t\t}\n\t\t}\n\t}\n#endif\n}\n\n/* Compute TCP options for SYN packets. This is not the final\n * network wire format yet.\n */\nstatic unsigned int tcp_syn_options(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\tstruct tcp_md5sig_key **md5)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int remaining = MAX_TCP_OPTION_SPACE;\n\tstruct tcp_fastopen_request *fastopen = tp->fastopen_req;\n\n\t*md5 = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (unlikely(rcu_access_pointer(tp->md5sig_info))) {\n\t\t*md5 = tp->af_specific->md5_lookup(sk, sk);\n\t\tif (*md5) {\n\t\t\topts->options |= OPTION_MD5;\n\t\t\tremaining -= TCPOLEN_MD5SIG_ALIGNED;\n\t\t}\n\t}\n#endif\n\n\t/* We always get an MSS option.  The option bytes which will be seen in\n\t * normal data packets should timestamps be used, must be in the MSS\n\t * advertised.  But we subtract them from tp->mss_cache so that\n\t * calculations in tcp_sendmsg are simpler etc.  So account for this\n\t * fact here if necessary.  If we don't do this correctly, as a\n\t * receiver we won't recognize data packets as being full sized when we\n\t * should, and thus we won't abide by the delayed ACK rules correctly.\n\t * SACKs don't matter, we never delay an ACK when we have any of those\n\t * going out.  */\n\topts->mss = tcp_advertise_mss(sk);\n\tremaining -= TCPOLEN_MSS_ALIGNED;\n\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_timestamps && !*md5)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = tcp_skb_timestamp(skb) + tp->tsoffset;\n\t\topts->tsecr = tp->rx_opt.ts_recent;\n\t\tremaining -= TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_window_scaling)) {\n\t\topts->ws = tp->rx_opt.rcv_wscale;\n\t\topts->options |= OPTION_WSCALE;\n\t\tremaining -= TCPOLEN_WSCALE_ALIGNED;\n\t}\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_sack)) {\n\t\topts->options |= OPTION_SACK_ADVERTISE;\n\t\tif (unlikely(!(OPTION_TS & opts->options)))\n\t\t\tremaining -= TCPOLEN_SACKPERM_ALIGNED;\n\t}\n\n\tif (fastopen && fastopen->cookie.len >= 0) {\n\t\tu32 need = fastopen->cookie.len;\n\n\t\tneed += fastopen->cookie.exp ? TCPOLEN_EXP_FASTOPEN_BASE :\n\t\t\t\t\t       TCPOLEN_FASTOPEN_BASE;\n\t\tneed = (need + 3) & ~3U;  /* Align to 32 bits */\n\t\tif (remaining >= need) {\n\t\t\topts->options |= OPTION_FAST_OPEN_COOKIE;\n\t\t\topts->fastopen_cookie = &fastopen->cookie;\n\t\t\tremaining -= need;\n\t\t\ttp->syn_fastopen = 1;\n\t\t\ttp->syn_fastopen_exp = fastopen->cookie.exp ? 1 : 0;\n\t\t}\n\t}\n\n\tsmc_set_option(tp, opts, &remaining);\n\n\treturn MAX_TCP_OPTION_SPACE - remaining;\n}\n\n/* Set up TCP options for SYN-ACKs. */\nstatic unsigned int tcp_synack_options(const struct sock *sk,\n\t\t\t\t       struct request_sock *req,\n\t\t\t\t       unsigned int mss, struct sk_buff *skb,\n\t\t\t\t       struct tcp_out_options *opts,\n\t\t\t\t       const struct tcp_md5sig_key *md5,\n\t\t\t\t       struct tcp_fastopen_cookie *foc,\n\t\t\t\t       enum tcp_synack_type synack_type)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tunsigned int remaining = MAX_TCP_OPTION_SPACE;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (md5) {\n\t\topts->options |= OPTION_MD5;\n\t\tremaining -= TCPOLEN_MD5SIG_ALIGNED;\n\n\t\t/* We can't fit any SACK blocks in a packet with MD5 + TS\n\t\t * options. There was discussion about disabling SACK\n\t\t * rather than TS in order to fit in better with old,\n\t\t * buggy kernels, but that was deemed to be unnecessary.\n\t\t */\n\t\tif (synack_type != TCP_SYNACK_COOKIE)\n\t\t\tireq->tstamp_ok &= !ireq->sack_ok;\n\t}\n#endif\n\n\t/* We always send an MSS option. */\n\topts->mss = mss;\n\tremaining -= TCPOLEN_MSS_ALIGNED;\n\n\tif (likely(ireq->wscale_ok)) {\n\t\topts->ws = ireq->rcv_wscale;\n\t\topts->options |= OPTION_WSCALE;\n\t\tremaining -= TCPOLEN_WSCALE_ALIGNED;\n\t}\n\tif (likely(ireq->tstamp_ok)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = tcp_skb_timestamp(skb) + tcp_rsk(req)->ts_off;\n\t\topts->tsecr = req->ts_recent;\n\t\tremaining -= TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\tif (likely(ireq->sack_ok)) {\n\t\topts->options |= OPTION_SACK_ADVERTISE;\n\t\tif (unlikely(!ireq->tstamp_ok))\n\t\t\tremaining -= TCPOLEN_SACKPERM_ALIGNED;\n\t}\n\tif (foc != NULL && foc->len >= 0) {\n\t\tu32 need = foc->len;\n\n\t\tneed += foc->exp ? TCPOLEN_EXP_FASTOPEN_BASE :\n\t\t\t\t   TCPOLEN_FASTOPEN_BASE;\n\t\tneed = (need + 3) & ~3U;  /* Align to 32 bits */\n\t\tif (remaining >= need) {\n\t\t\topts->options |= OPTION_FAST_OPEN_COOKIE;\n\t\t\topts->fastopen_cookie = foc;\n\t\t\tremaining -= need;\n\t\t}\n\t}\n\n\tsmc_set_option_cond(tcp_sk(sk), ireq, opts, &remaining);\n\n\treturn MAX_TCP_OPTION_SPACE - remaining;\n}\n\n/* Compute TCP options for ESTABLISHED sockets. This is not the\n * final wire format yet.\n */\nstatic unsigned int tcp_established_options(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\t\tstruct tcp_md5sig_key **md5)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int size = 0;\n\tunsigned int eff_sacks;\n\n\topts->options = 0;\n\n\t*md5 = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (unlikely(rcu_access_pointer(tp->md5sig_info))) {\n\t\t*md5 = tp->af_specific->md5_lookup(sk, sk);\n\t\tif (*md5) {\n\t\t\topts->options |= OPTION_MD5;\n\t\t\tsize += TCPOLEN_MD5SIG_ALIGNED;\n\t\t}\n\t}\n#endif\n\n\tif (likely(tp->rx_opt.tstamp_ok)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = skb ? tcp_skb_timestamp(skb) + tp->tsoffset : 0;\n\t\topts->tsecr = tp->rx_opt.ts_recent;\n\t\tsize += TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\n\teff_sacks = tp->rx_opt.num_sacks + tp->rx_opt.dsack;\n\tif (unlikely(eff_sacks)) {\n\t\tconst unsigned int remaining = MAX_TCP_OPTION_SPACE - size;\n\t\topts->num_sack_blocks =\n\t\t\tmin_t(unsigned int, eff_sacks,\n\t\t\t      (remaining - TCPOLEN_SACK_BASE_ALIGNED) /\n\t\t\t      TCPOLEN_SACK_PERBLOCK);\n\t\tif (likely(opts->num_sack_blocks))\n\t\t\tsize += TCPOLEN_SACK_BASE_ALIGNED +\n\t\t\t\topts->num_sack_blocks * TCPOLEN_SACK_PERBLOCK;\n\t}\n\n\treturn size;\n}\n\n\n/* TCP SMALL QUEUES (TSQ)\n *\n * TSQ goal is to keep small amount of skbs per tcp flow in tx queues (qdisc+dev)\n * to reduce RTT and bufferbloat.\n * We do this using a special skb destructor (tcp_wfree).\n *\n * Its important tcp_wfree() can be replaced by sock_wfree() in the event skb\n * needs to be reallocated in a driver.\n * The invariant being skb->truesize subtracted from sk->sk_wmem_alloc\n *\n * Since transmit from skb destructor is forbidden, we use a tasklet\n * to process all sockets that eventually need to send more skbs.\n * We use one tasklet per cpu, with its own queue of sockets.\n */\nstruct tsq_tasklet {\n\tstruct tasklet_struct\ttasklet;\n\tstruct list_head\thead; /* queue of tcp sockets */\n};\nstatic DEFINE_PER_CPU(struct tsq_tasklet, tsq_tasklet);\n\nstatic void tcp_tsq_write(struct sock *sk)\n{\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_FIN_WAIT1 | TCPF_CLOSING |\n\t     TCPF_CLOSE_WAIT  | TCPF_LAST_ACK)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (tp->lost_out > tp->retrans_out &&\n\t\t    tp->snd_cwnd > tcp_packets_in_flight(tp)) {\n\t\t\ttcp_mstamp_refresh(tp);\n\t\t\ttcp_xmit_retransmit_queue(sk);\n\t\t}\n\n\t\ttcp_write_xmit(sk, tcp_current_mss(sk), tp->nonagle,\n\t\t\t       0, GFP_ATOMIC);\n\t}\n}\n\nstatic void tcp_tsq_handler(struct sock *sk)\n{\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\ttcp_tsq_write(sk);\n\telse if (!test_and_set_bit(TCP_TSQ_DEFERRED, &sk->sk_tsq_flags))\n\t\tsock_hold(sk);\n\tbh_unlock_sock(sk);\n}\n/*\n * One tasklet per cpu tries to send more skbs.\n * We run in tasklet context but need to disable irqs when\n * transferring tsq->head because tcp_wfree() might\n * interrupt us (non NAPI drivers)\n */\nstatic void tcp_tasklet_func(unsigned long data)\n{\n\tstruct tsq_tasklet *tsq = (struct tsq_tasklet *)data;\n\tLIST_HEAD(list);\n\tunsigned long flags;\n\tstruct list_head *q, *n;\n\tstruct tcp_sock *tp;\n\tstruct sock *sk;\n\n\tlocal_irq_save(flags);\n\tlist_splice_init(&tsq->head, &list);\n\tlocal_irq_restore(flags);\n\n\tlist_for_each_safe(q, n, &list) {\n\t\ttp = list_entry(q, struct tcp_sock, tsq_node);\n\t\tlist_del(&tp->tsq_node);\n\n\t\tsk = (struct sock *)tp;\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(TSQ_QUEUED, &sk->sk_tsq_flags);\n\n\t\ttcp_tsq_handler(sk);\n\t\tsk_free(sk);\n\t}\n}\n\n#define TCP_DEFERRED_ALL (TCPF_TSQ_DEFERRED |\t\t\\\n\t\t\t  TCPF_WRITE_TIMER_DEFERRED |\t\\\n\t\t\t  TCPF_DELACK_TIMER_DEFERRED |\t\\\n\t\t\t  TCPF_MTU_REDUCED_DEFERRED)\n/**\n * tcp_release_cb - tcp release_sock() callback\n * @sk: socket\n *\n * called from release_sock() to perform protocol dependent\n * actions before socket release.\n */\nvoid tcp_release_cb(struct sock *sk)\n{\n\tunsigned long flags, nflags;\n\n\t/* perform an atomic operation only if at least one flag is set */\n\tdo {\n\t\tflags = sk->sk_tsq_flags;\n\t\tif (!(flags & TCP_DEFERRED_ALL))\n\t\t\treturn;\n\t\tnflags = flags & ~TCP_DEFERRED_ALL;\n\t} while (cmpxchg(&sk->sk_tsq_flags, flags, nflags) != flags);\n\n\tif (flags & TCPF_TSQ_DEFERRED) {\n\t\ttcp_tsq_write(sk);\n\t\t__sock_put(sk);\n\t}\n\t/* Here begins the tricky part :\n\t * We are called from release_sock() with :\n\t * 1) BH disabled\n\t * 2) sk_lock.slock spinlock held\n\t * 3) socket owned by us (sk->sk_lock.owned == 1)\n\t *\n\t * But following code is meant to be called from BH handlers,\n\t * so we should keep BH disabled, but early release socket ownership\n\t */\n\tsock_release_ownership(sk);\n\n\tif (flags & TCPF_WRITE_TIMER_DEFERRED) {\n\t\ttcp_write_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_DELACK_TIMER_DEFERRED) {\n\t\ttcp_delack_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_MTU_REDUCED_DEFERRED) {\n\t\tinet_csk(sk)->icsk_af_ops->mtu_reduced(sk);\n\t\t__sock_put(sk);\n\t}\n}\nEXPORT_SYMBOL(tcp_release_cb);\n\nvoid __init tcp_tasklet_init(void)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct tsq_tasklet *tsq = &per_cpu(tsq_tasklet, i);\n\n\t\tINIT_LIST_HEAD(&tsq->head);\n\t\ttasklet_init(&tsq->tasklet,\n\t\t\t     tcp_tasklet_func,\n\t\t\t     (unsigned long)tsq);\n\t}\n}\n\n/*\n * Write buffer destructor automatically called from kfree_skb.\n * We can't xmit new skbs from this context, as we might already\n * hold qdisc lock.\n */\nvoid tcp_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long flags, nval, oval;\n\n\t/* Keep one reference on sk_wmem_alloc.\n\t * Will be released by sk_free() from here or tcp_tasklet_func()\n\t */\n\tWARN_ON(refcount_sub_and_test(skb->truesize - 1, &sk->sk_wmem_alloc));\n\n\t/* If this softirq is serviced by ksoftirqd, we are likely under stress.\n\t * Wait until our queues (qdisc + devices) are drained.\n\t * This gives :\n\t * - less callbacks to tcp_write_xmit(), reducing stress (batches)\n\t * - chance for incoming ACK (processed by another cpu maybe)\n\t *   to migrate this flow (skb->ooo_okay will be eventually set)\n\t */\n\tif (refcount_read(&sk->sk_wmem_alloc) >= SKB_TRUESIZE(1) && this_cpu_ksoftirqd() == current)\n\t\tgoto out;\n\n\tfor (oval = READ_ONCE(sk->sk_tsq_flags);; oval = nval) {\n\t\tstruct tsq_tasklet *tsq;\n\t\tbool empty;\n\n\t\tif (!(oval & TSQF_THROTTLED) || (oval & TSQF_QUEUED))\n\t\t\tgoto out;\n\n\t\tnval = (oval & ~TSQF_THROTTLED) | TSQF_QUEUED;\n\t\tnval = cmpxchg(&sk->sk_tsq_flags, oval, nval);\n\t\tif (nval != oval)\n\t\t\tcontinue;\n\n\t\t/* queue this socket to tasklet queue */\n\t\tlocal_irq_save(flags);\n\t\ttsq = this_cpu_ptr(&tsq_tasklet);\n\t\tempty = list_empty(&tsq->head);\n\t\tlist_add(&tp->tsq_node, &tsq->head);\n\t\tif (empty)\n\t\t\ttasklet_schedule(&tsq->tasklet);\n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\nout:\n\tsk_free(sk);\n}\n\n/* Note: Called under soft irq.\n * We can call TCP stack right away, unless socket is owned by user.\n */\nenum hrtimer_restart tcp_pace_kick(struct hrtimer *timer)\n{\n\tstruct tcp_sock *tp = container_of(timer, struct tcp_sock, pacing_timer);\n\tstruct sock *sk = (struct sock *)tp;\n\n\ttcp_tsq_handler(sk);\n\tsock_put(sk);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void tcp_internal_pacing(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tktime_t expire, now;\n\tu64 len_ns;\n\tu32 rate;\n\n\tif (!tcp_needs_internal_pacing(sk))\n\t\treturn;\n\trate = sk->sk_pacing_rate;\n\tif (!rate || rate == ~0U)\n\t\treturn;\n\n\tlen_ns = (u64)skb->len * NSEC_PER_SEC;\n\tdo_div(len_ns, rate);\n\tnow = ktime_get();\n\t/* If hrtimer is already armed, then our caller has not\n\t * used tcp_pacing_check().\n\t */\n\tif (unlikely(hrtimer_is_queued(&tp->pacing_timer))) {\n\t\texpire = hrtimer_get_softexpires(&tp->pacing_timer);\n\t\tif (ktime_after(expire, now))\n\t\t\tnow = expire;\n\t\tif (hrtimer_try_to_cancel(&tp->pacing_timer) == 1)\n\t\t\t__sock_put(sk);\n\t}\n\thrtimer_start(&tp->pacing_timer, ktime_add_ns(now, len_ns),\n\t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\n\tsock_hold(sk);\n}\n\nstatic bool tcp_pacing_check(const struct sock *sk)\n{\n\treturn tcp_needs_internal_pacing(sk) &&\n\t       hrtimer_is_queued(&tcp_sk(sk)->pacing_timer);\n}\n\nstatic void tcp_update_skb_after_send(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tskb->skb_mstamp = tp->tcp_mstamp;\n\tlist_move_tail(&skb->tcp_tsorted_anchor, &tp->tsorted_sent_queue);\n}\n\n/* This routine actually transmits TCP packets queued in by\n * tcp_do_sendmsg().  This is used by both the initial\n * transmission and possible later retransmissions.\n * All SKB's seen here are completely headerless.  It is our\n * job to build the TCP header, and pass the packet down to\n * IP so it can do the same plus pass the packet off to the\n * device.\n *\n * We are working here with either a clone of the original\n * SKB, or a fresh unique copy made by the retransmit engine.\n */\nstatic int __tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,\n\t\t\t      int clone_it, gfp_t gfp_mask, u32 rcv_nxt)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet;\n\tstruct tcp_sock *tp;\n\tstruct tcp_skb_cb *tcb;\n\tstruct tcp_out_options opts;\n\tunsigned int tcp_options_size, tcp_header_size;\n\tstruct sk_buff *oskb = NULL;\n\tstruct tcp_md5sig_key *md5;\n\tstruct tcphdr *th;\n\tint err;\n\n\tBUG_ON(!skb || !tcp_skb_pcount(skb));\n\ttp = tcp_sk(sk);\n\n\tif (clone_it) {\n\t\tTCP_SKB_CB(skb)->tx.in_flight = TCP_SKB_CB(skb)->end_seq\n\t\t\t- tp->snd_una;\n\t\toskb = skb;\n\n\t\ttcp_skb_tsorted_save(oskb) {\n\t\t\tif (unlikely(skb_cloned(oskb)))\n\t\t\t\tskb = pskb_copy(oskb, gfp_mask);\n\t\t\telse\n\t\t\t\tskb = skb_clone(oskb, gfp_mask);\n\t\t} tcp_skb_tsorted_restore(oskb);\n\n\t\tif (unlikely(!skb))\n\t\t\treturn -ENOBUFS;\n\t}\n\tskb->skb_mstamp = tp->tcp_mstamp;\n\n\tinet = inet_sk(sk);\n\ttcb = TCP_SKB_CB(skb);\n\tmemset(&opts, 0, sizeof(opts));\n\n\tif (unlikely(tcb->tcp_flags & TCPHDR_SYN))\n\t\ttcp_options_size = tcp_syn_options(sk, skb, &opts, &md5);\n\telse\n\t\ttcp_options_size = tcp_established_options(sk, skb, &opts,\n\t\t\t\t\t\t\t   &md5);\n\ttcp_header_size = tcp_options_size + sizeof(struct tcphdr);\n\n\t/* if no packet is in qdisc/device queue, then allow XPS to select\n\t * another queue. We can be called from tcp_tsq_handler()\n\t * which holds one reference to sk.\n\t *\n\t * TODO: Ideally, in-flight pure ACK packets should not matter here.\n\t * One way to get this would be to set skb->truesize = 2 on them.\n\t */\n\tskb->ooo_okay = sk_wmem_alloc_get(sk) < SKB_TRUESIZE(1);\n\n\t/* If we had to use memory reserve to allocate this skb,\n\t * this might cause drops if packet is looped back :\n\t * Other socket might not have SOCK_MEMALLOC.\n\t * Packets not looped back do not care about pfmemalloc.\n\t */\n\tskb->pfmemalloc = 0;\n\n\tskb_push(skb, tcp_header_size);\n\tskb_reset_transport_header(skb);\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = skb_is_tcp_pure_ack(skb) ? __sock_wfree : tcp_wfree;\n\tskb_set_hash_from_sk(skb, sk);\n\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\n\tskb_set_dst_pending_confirm(skb, sk->sk_dst_pending_confirm);\n\n\t/* Build TCP header and checksum it. */\n\tth = (struct tcphdr *)skb->data;\n\tth->source\t\t= inet->inet_sport;\n\tth->dest\t\t= inet->inet_dport;\n\tth->seq\t\t\t= htonl(tcb->seq);\n\tth->ack_seq\t\t= htonl(rcv_nxt);\n\t*(((__be16 *)th) + 6)\t= htons(((tcp_header_size >> 2) << 12) |\n\t\t\t\t\ttcb->tcp_flags);\n\n\tth->check\t\t= 0;\n\tth->urg_ptr\t\t= 0;\n\n\t/* The urg_mode check is necessary during a below snd_una win probe */\n\tif (unlikely(tcp_urg_mode(tp) && before(tcb->seq, tp->snd_up))) {\n\t\tif (before(tp->snd_up, tcb->seq + 0x10000)) {\n\t\t\tth->urg_ptr = htons(tp->snd_up - tcb->seq);\n\t\t\tth->urg = 1;\n\t\t} else if (after(tcb->seq + 0xFFFF, tp->snd_nxt)) {\n\t\t\tth->urg_ptr = htons(0xFFFF);\n\t\t\tth->urg = 1;\n\t\t}\n\t}\n\n\ttcp_options_write((__be32 *)(th + 1), tp, &opts);\n\tskb_shinfo(skb)->gso_type = sk->sk_gso_type;\n\tif (likely(!(tcb->tcp_flags & TCPHDR_SYN))) {\n\t\tth->window      = htons(tcp_select_window(sk));\n\t\ttcp_ecn_send(sk, skb, th, tcp_header_size);\n\t} else {\n\t\t/* RFC1323: The window in SYN & SYN/ACK segments\n\t\t * is never scaled.\n\t\t */\n\t\tth->window\t= htons(min(tp->rcv_wnd, 65535U));\n\t}\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Calculate the MD5 hash, as we have all we need now */\n\tif (md5) {\n\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t\ttp->af_specific->calc_md5_hash(opts.hash_location,\n\t\t\t\t\t       md5, sk, skb);\n\t}\n#endif\n\n\ticsk->icsk_af_ops->send_check(sk, skb);\n\n\tif (likely(tcb->tcp_flags & TCPHDR_ACK))\n\t\ttcp_event_ack_sent(sk, tcp_skb_pcount(skb), rcv_nxt);\n\n\tif (skb->len != tcp_header_size) {\n\t\ttcp_event_data_sent(tp, sk);\n\t\ttp->data_segs_out += tcp_skb_pcount(skb);\n\t\ttp->bytes_sent += skb->len - tcp_header_size;\n\t\ttcp_internal_pacing(sk, skb);\n\t}\n\n\tif (after(tcb->end_seq, tp->snd_nxt) || tcb->seq == tcb->end_seq)\n\t\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS,\n\t\t\t      tcp_skb_pcount(skb));\n\n\ttp->segs_out += tcp_skb_pcount(skb);\n\t/* OK, its time to fill skb_shinfo(skb)->gso_{segs|size} */\n\tskb_shinfo(skb)->gso_segs = tcp_skb_pcount(skb);\n\tskb_shinfo(skb)->gso_size = tcp_skb_mss(skb);\n\n\t/* Our usage of tstamp should remain private */\n\tskb->tstamp = 0;\n\n\t/* Cleanup our debris for IP stacks */\n\tmemset(skb->cb, 0, max(sizeof(struct inet_skb_parm),\n\t\t\t       sizeof(struct inet6_skb_parm)));\n\n\terr = icsk->icsk_af_ops->queue_xmit(sk, skb, &inet->cork.fl);\n\n\tif (unlikely(err > 0)) {\n\t\ttcp_enter_cwr(sk);\n\t\terr = net_xmit_eval(err);\n\t}\n\tif (!err && oskb) {\n\t\ttcp_update_skb_after_send(tp, oskb);\n\t\ttcp_rate_skb_sent(sk, oskb);\n\t}\n\treturn err;\n}\n\nstatic int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,\n\t\t\t    gfp_t gfp_mask)\n{\n\treturn __tcp_transmit_skb(sk, skb, clone_it, gfp_mask,\n\t\t\t\t  tcp_sk(sk)->rcv_nxt);\n}\n\n/* This routine just queues the buffer for sending.\n *\n * NOTE: probe0 timer is not checked, do not forget tcp_push_pending_frames,\n * otherwise socket can stall.\n */\nstatic void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Advance write_seq and place onto the write_queue. */\n\tWRITE_ONCE(tp->write_seq, TCP_SKB_CB(skb)->end_seq);\n\t__skb_header_release(skb);\n\ttcp_add_write_queue_tail(sk, skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n}\n\n/* Initialize TSO segments for a packet. */\nstatic void tcp_set_skb_tso_segs(struct sk_buff *skb, unsigned int mss_now)\n{\n\tif (skb->len <= mss_now) {\n\t\t/* Avoid the costly divide in the normal\n\t\t * non-TSO case.\n\t\t */\n\t\ttcp_skb_pcount_set(skb, 1);\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = 0;\n\t} else {\n\t\ttcp_skb_pcount_set(skb, DIV_ROUND_UP(skb->len, mss_now));\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = mss_now;\n\t}\n}\n\n/* Pcount in the middle of the write queue got changed, we need to do various\n * tweaks to fix counters\n */\nstatic void tcp_adjust_pcount(struct sock *sk, const struct sk_buff *skb, int decr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->packets_out -= decr;\n\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\ttp->sacked_out -= decr;\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS)\n\t\ttp->retrans_out -= decr;\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_LOST)\n\t\ttp->lost_out -= decr;\n\n\t/* Reno case is special. Sigh... */\n\tif (tcp_is_reno(tp) && decr > 0)\n\t\ttp->sacked_out -= min_t(u32, tp->sacked_out, decr);\n\n\tif (tp->lost_skb_hint &&\n\t    before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(tp->lost_skb_hint)->seq) &&\n\t    (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED))\n\t\ttp->lost_cnt_hint -= decr;\n\n\ttcp_verify_left_out(tp);\n}\n\nstatic bool tcp_has_tx_tstamp(const struct sk_buff *skb)\n{\n\treturn TCP_SKB_CB(skb)->txstamp_ack ||\n\t\t(skb_shinfo(skb)->tx_flags & SKBTX_ANY_TSTAMP);\n}\n\nstatic void tcp_fragment_tstamp(struct sk_buff *skb, struct sk_buff *skb2)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (unlikely(tcp_has_tx_tstamp(skb)) &&\n\t    !before(shinfo->tskey, TCP_SKB_CB(skb2)->seq)) {\n\t\tstruct skb_shared_info *shinfo2 = skb_shinfo(skb2);\n\t\tu8 tsflags = shinfo->tx_flags & SKBTX_ANY_TSTAMP;\n\n\t\tshinfo->tx_flags &= ~tsflags;\n\t\tshinfo2->tx_flags |= tsflags;\n\t\tswap(shinfo->tskey, shinfo2->tskey);\n\t\tTCP_SKB_CB(skb2)->txstamp_ack = TCP_SKB_CB(skb)->txstamp_ack;\n\t\tTCP_SKB_CB(skb)->txstamp_ack = 0;\n\t}\n}\n\nstatic void tcp_skb_fragment_eor(struct sk_buff *skb, struct sk_buff *skb2)\n{\n\tTCP_SKB_CB(skb2)->eor = TCP_SKB_CB(skb)->eor;\n\tTCP_SKB_CB(skb)->eor = 0;\n}\n\n/* Insert buff after skb on the write or rtx queue of sk.  */\nstatic void tcp_insert_write_queue_after(struct sk_buff *skb,\n\t\t\t\t\t struct sk_buff *buff,\n\t\t\t\t\t struct sock *sk,\n\t\t\t\t\t enum tcp_queue tcp_queue)\n{\n\tif (tcp_queue == TCP_FRAG_IN_WRITE_QUEUE)\n\t\t__skb_queue_after(&sk->sk_write_queue, skb, buff);\n\telse\n\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, buff);\n}\n\n/* Function to create two new TCP segments.  Shrinks the given segment\n * to the specified size and appends a new segment with the rest of the\n * packet to the list.  This won't be called frequently, I hope.\n * Remember, these are still headerless SKBs at this point.\n */\nint tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,\n\t\t struct sk_buff *skb, u32 len,\n\t\t unsigned int mss_now, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint nsize, old_factor;\n\tlong limit;\n\tint nlen;\n\tu8 flags;\n\n\tif (WARN_ON(len > skb->len))\n\t\treturn -EINVAL;\n\n\tnsize = skb_headlen(skb) - len;\n\tif (nsize < 0)\n\t\tnsize = 0;\n\n\t/* tcp_sendmsg() can overshoot sk_wmem_queued by one full size skb.\n\t * We need some allowance to not penalize applications setting small\n\t * SO_SNDBUF values.\n\t * Also allow first and last skb in retransmit queue to be split.\n\t */\n\tlimit = sk->sk_sndbuf + 2 * SKB_TRUESIZE(GSO_MAX_SIZE);\n\tif (unlikely((sk->sk_wmem_queued >> 1) > limit &&\n\t\t     tcp_queue != TCP_FRAG_IN_WRITE_QUEUE &&\n\t\t     skb != tcp_rtx_queue_head(sk) &&\n\t\t     skb != tcp_rtx_queue_tail(sk))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPWQUEUETOOBIG);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (skb_unclone(skb, gfp))\n\t\treturn -ENOMEM;\n\n\t/* Get a new skb... force flag on. */\n\tbuff = sk_stream_alloc_skb(sk, nsize, gfp, true);\n\tif (!buff)\n\t\treturn -ENOMEM; /* We'll just try again later. */\n\n\tsk->sk_wmem_queued += buff->truesize;\n\tsk_mem_charge(sk, buff->truesize);\n\tnlen = skb->len - len - nsize;\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t/* Correct the sequence numbers. */\n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t/* PSH and FIN should only be set in the second packet. */\n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\tTCP_SKB_CB(buff)->sacked = TCP_SKB_CB(skb)->sacked;\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tskb_split(skb, buff, len);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\n\tbuff->tstamp = skb->tstamp;\n\ttcp_fragment_tstamp(skb, buff);\n\n\told_factor = tcp_skb_pcount(skb);\n\n\t/* Fix up tso_factor for both original and new SKB.  */\n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t/* Update delivered info for the new segment */\n\tTCP_SKB_CB(buff)->tx = TCP_SKB_CB(skb)->tx;\n\n\t/* If this packet has been sent out already, we must\n\t * adjust the various packet counters.\n\t */\n\tif (!before(tp->snd_nxt, TCP_SKB_CB(buff)->end_seq)) {\n\t\tint diff = old_factor - tcp_skb_pcount(skb) -\n\t\t\ttcp_skb_pcount(buff);\n\n\t\tif (diff)\n\t\t\ttcp_adjust_pcount(sk, skb, diff);\n\t}\n\n\t/* Link BUFF into the send queue. */\n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, tcp_queue);\n\tif (tcp_queue == TCP_FRAG_IN_RTX_QUEUE)\n\t\tlist_add(&buff->tcp_tsorted_anchor, &skb->tcp_tsorted_anchor);\n\n\treturn 0;\n}\n\n/* This is similar to __pskb_pull_tail(). The difference is that pulled\n * data is not copied, but immediately discarded.\n */\nstatic int __pskb_trim_head(struct sk_buff *skb, int len)\n{\n\tstruct skb_shared_info *shinfo;\n\tint i, k, eat;\n\n\teat = min_t(int, len, skb_headlen(skb));\n\tif (eat) {\n\t\t__skb_pull(skb, eat);\n\t\tlen -= eat;\n\t\tif (!len)\n\t\t\treturn 0;\n\t}\n\teat = len;\n\tk = 0;\n\tshinfo = skb_shinfo(skb);\n\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\tint size = skb_frag_size(&shinfo->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tshinfo->frags[k] = shinfo->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tshinfo->frags[k].page_offset += eat;\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[k], eat);\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tshinfo->nr_frags = k;\n\n\tskb->data_len -= len;\n\tskb->len = skb->data_len;\n\treturn len;\n}\n\n/* Remove acked data from a packet in the transmit queue. */\nint tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)\n{\n\tu32 delta_truesize;\n\n\tif (skb_unclone(skb, GFP_ATOMIC))\n\t\treturn -ENOMEM;\n\n\tdelta_truesize = __pskb_trim_head(skb, len);\n\n\tTCP_SKB_CB(skb)->seq += len;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tif (delta_truesize) {\n\t\tskb->truesize\t   -= delta_truesize;\n\t\tsk->sk_wmem_queued -= delta_truesize;\n\t\tsk_mem_uncharge(sk, delta_truesize);\n\t\tsock_set_flag(sk, SOCK_QUEUE_SHRUNK);\n\t}\n\n\t/* Any change of skb->len requires recalculation of tso factor. */\n\tif (tcp_skb_pcount(skb) > 1)\n\t\ttcp_set_skb_tso_segs(skb, tcp_skb_mss(skb));\n\n\treturn 0;\n}\n\n/* Calculate MSS not accounting any TCP options.  */\nstatic inline int __tcp_mtu_to_mss(struct sock *sk, int pmtu)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tint mss_now;\n\n\t/* Calculate base mss without TCP options:\n\t   It is MMS_S - sizeof(tcphdr) of rfc1122\n\t */\n\tmss_now = pmtu - icsk->icsk_af_ops->net_header_len - sizeof(struct tcphdr);\n\n\t/* IPv6 adds a frag_hdr in case RTAX_FEATURE_ALLFRAG is set */\n\tif (icsk->icsk_af_ops->net_frag_header_len) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_allfrag(dst))\n\t\t\tmss_now -= icsk->icsk_af_ops->net_frag_header_len;\n\t}\n\n\t/* Clamp it (mss_clamp does not include tcp options) */\n\tif (mss_now > tp->rx_opt.mss_clamp)\n\t\tmss_now = tp->rx_opt.mss_clamp;\n\n\t/* Now subtract optional transport overhead */\n\tmss_now -= icsk->icsk_ext_hdr_len;\n\n\t/* Then reserve room for full set of TCP options and 8 bytes of data */\n\tmss_now = max(mss_now, sock_net(sk)->ipv4.sysctl_tcp_min_snd_mss);\n\treturn mss_now;\n}\n\n/* Calculate MSS. Not accounting for SACKs here.  */\nint tcp_mtu_to_mss(struct sock *sk, int pmtu)\n{\n\t/* Subtract TCP options size, not including SACKs */\n\treturn __tcp_mtu_to_mss(sk, pmtu) -\n\t       (tcp_sk(sk)->tcp_header_len - sizeof(struct tcphdr));\n}\nEXPORT_SYMBOL(tcp_mtu_to_mss);\n\n/* Inverse of above */\nint tcp_mss_to_mtu(struct sock *sk, int mss)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tint mtu;\n\n\tmtu = mss +\n\t      tp->tcp_header_len +\n\t      icsk->icsk_ext_hdr_len +\n\t      icsk->icsk_af_ops->net_header_len;\n\n\t/* IPv6 adds a frag_hdr in case RTAX_FEATURE_ALLFRAG is set */\n\tif (icsk->icsk_af_ops->net_frag_header_len) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_allfrag(dst))\n\t\t\tmtu += icsk->icsk_af_ops->net_frag_header_len;\n\t}\n\treturn mtu;\n}\nEXPORT_SYMBOL(tcp_mss_to_mtu);\n\n/* MTU probing init per socket */\nvoid tcp_mtup_init(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\n\ticsk->icsk_mtup.enabled = net->ipv4.sysctl_tcp_mtu_probing > 1;\n\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp + sizeof(struct tcphdr) +\n\t\t\t       icsk->icsk_af_ops->net_header_len;\n\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, net->ipv4.sysctl_tcp_base_mss);\n\ticsk->icsk_mtup.probe_size = 0;\n\tif (icsk->icsk_mtup.enabled)\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n}\nEXPORT_SYMBOL(tcp_mtup_init);\n\n/* This function synchronize snd mss to current pmtu/exthdr set.\n\n   tp->rx_opt.user_mss is mss set by user by TCP_MAXSEG. It does NOT counts\n   for TCP options, but includes only bare TCP header.\n\n   tp->rx_opt.mss_clamp is mss negotiated at connection setup.\n   It is minimum of user_mss and mss received with SYN.\n   It also does not include TCP options.\n\n   inet_csk(sk)->icsk_pmtu_cookie is last pmtu, seen by this function.\n\n   tp->mss_cache is current effective sending mss, including\n   all tcp options except for SACKs. It is evaluated,\n   taking into account current pmtu, but never exceeds\n   tp->rx_opt.mss_clamp.\n\n   NOTE1. rfc1122 clearly states that advertised MSS\n   DOES NOT include either tcp or ip options.\n\n   NOTE2. inet_csk(sk)->icsk_pmtu_cookie and tp->mss_cache\n   are READ ONLY outside this function.\t\t--ANK (980731)\n */\nunsigned int tcp_sync_mss(struct sock *sk, u32 pmtu)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint mss_now;\n\n\tif (icsk->icsk_mtup.search_high > pmtu)\n\t\ticsk->icsk_mtup.search_high = pmtu;\n\n\tmss_now = tcp_mtu_to_mss(sk, pmtu);\n\tmss_now = tcp_bound_to_half_wnd(tp, mss_now);\n\n\t/* And store cached results */\n\ticsk->icsk_pmtu_cookie = pmtu;\n\tif (icsk->icsk_mtup.enabled)\n\t\tmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low));\n\ttp->mss_cache = mss_now;\n\n\treturn mss_now;\n}\nEXPORT_SYMBOL(tcp_sync_mss);\n\n/* Compute the current effective MSS, taking SACKs and IP options,\n * and even PMTU discovery events into account.\n */\nunsigned int tcp_current_mss(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tu32 mss_now;\n\tunsigned int header_len;\n\tstruct tcp_out_options opts;\n\tstruct tcp_md5sig_key *md5;\n\n\tmss_now = tp->mss_cache;\n\n\tif (dst) {\n\t\tu32 mtu = dst_mtu(dst);\n\t\tif (mtu != inet_csk(sk)->icsk_pmtu_cookie)\n\t\t\tmss_now = tcp_sync_mss(sk, mtu);\n\t}\n\n\theader_len = tcp_established_options(sk, NULL, &opts, &md5) +\n\t\t     sizeof(struct tcphdr);\n\t/* The mss_cache is sized based on tp->tcp_header_len, which assumes\n\t * some common options. If this is an odd packet (because we have SACK\n\t * blocks etc) then our calculated header_len will be different, and\n\t * we have to adjust mss_now correspondingly */\n\tif (header_len != tp->tcp_header_len) {\n\t\tint delta = (int) header_len - tp->tcp_header_len;\n\t\tmss_now -= delta;\n\t}\n\n\treturn mss_now;\n}\n\n/* RFC2861, slow part. Adjust cwnd, after it was not full during one rto.\n * As additional protections, we do not touch cwnd in retransmission phases,\n * and if application hit its sndbuf limit recently.\n */\nstatic void tcp_cwnd_application_limited(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Open &&\n\t    sk->sk_socket && !test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\t/* Limited by application or receiver window. */\n\t\tu32 init_win = tcp_init_cwnd(tp, __sk_dst_get(sk));\n\t\tu32 win_used = max(tp->snd_cwnd_used, init_win);\n\t\tif (win_used < tp->snd_cwnd) {\n\t\t\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\t\t\ttp->snd_cwnd = (tp->snd_cwnd + win_used) >> 1;\n\t\t}\n\t\ttp->snd_cwnd_used = 0;\n\t}\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n}\n\nstatic void tcp_cwnd_validate(struct sock *sk, bool is_cwnd_limited)\n{\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Track the maximum number of outstanding packets in each\n\t * window, and remember whether we were cwnd-limited then.\n\t */\n\tif (!before(tp->snd_una, tp->max_packets_seq) ||\n\t    tp->packets_out > tp->max_packets_out ||\n\t    is_cwnd_limited) {\n\t\ttp->max_packets_out = tp->packets_out;\n\t\ttp->max_packets_seq = tp->snd_nxt;\n\t\ttp->is_cwnd_limited = is_cwnd_limited;\n\t}\n\n\tif (tcp_is_cwnd_limited(sk)) {\n\t\t/* Network is feed fully. */\n\t\ttp->snd_cwnd_used = 0;\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t} else {\n\t\t/* Network starves. */\n\t\tif (tp->packets_out > tp->snd_cwnd_used)\n\t\t\ttp->snd_cwnd_used = tp->packets_out;\n\n\t\tif (sock_net(sk)->ipv4.sysctl_tcp_slow_start_after_idle &&\n\t\t    (s32)(tcp_jiffies32 - tp->snd_cwnd_stamp) >= inet_csk(sk)->icsk_rto &&\n\t\t    !ca_ops->cong_control)\n\t\t\ttcp_cwnd_application_limited(sk);\n\n\t\t/* The following conditions together indicate the starvation\n\t\t * is caused by insufficient sender buffer:\n\t\t * 1) just sent some data (see tcp_write_xmit)\n\t\t * 2) not cwnd limited (this else condition)\n\t\t * 3) no more data to send (tcp_write_queue_empty())\n\t\t * 4) application is hitting buffer limit (SOCK_NOSPACE)\n\t\t */\n\t\tif (tcp_write_queue_empty(sk) && sk->sk_socket &&\n\t\t    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags) &&\n\t\t    (1 << sk->sk_state) & (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))\n\t\t\ttcp_chrono_start(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n}\n\n/* Minshall's variant of the Nagle send check. */\nstatic bool tcp_minshall_check(const struct tcp_sock *tp)\n{\n\treturn after(tp->snd_sml, tp->snd_una) &&\n\t\t!after(tp->snd_sml, tp->snd_nxt);\n}\n\n/* Update snd_sml if this skb is under mss\n * Note that a TSO packet might end with a sub-mss segment\n * The test is really :\n * if ((skb->len % mss) != 0)\n *        tp->snd_sml = TCP_SKB_CB(skb)->end_seq;\n * But we can avoid doing the divide again given we already have\n *  skb_pcount = skb->len / mss_now\n */\nstatic void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss_now,\n\t\t\t\tconst struct sk_buff *skb)\n{\n\tif (skb->len < tcp_skb_pcount(skb) * mss_now)\n\t\ttp->snd_sml = TCP_SKB_CB(skb)->end_seq;\n}\n\n/* Return false, if packet can be sent now without violation Nagle's rules:\n * 1. It is full sized. (provided by caller in %partial bool)\n * 2. Or it contains FIN. (already checked by caller)\n * 3. Or TCP_CORK is not set, and TCP_NODELAY is set.\n * 4. Or TCP_CORK is not set, and all sent packets are ACKed.\n *    With Minshall's modification: all sent small packets are ACKed.\n */\nstatic bool tcp_nagle_check(bool partial, const struct tcp_sock *tp,\n\t\t\t    int nonagle)\n{\n\treturn partial &&\n\t\t((nonagle & TCP_NAGLE_CORK) ||\n\t\t (!nonagle && tp->packets_out && tcp_minshall_check(tp)));\n}\n\n/* Return how many segs we'd like on a TSO packet,\n * to send one TSO packet per ms\n */\nstatic u32 tcp_tso_autosize(const struct sock *sk, unsigned int mss_now,\n\t\t\t    int min_tso_segs)\n{\n\tu32 bytes, segs;\n\n\tbytes = min(sk->sk_pacing_rate >> sk->sk_pacing_shift,\n\t\t    sk->sk_gso_max_size - 1 - MAX_TCP_HEADER);\n\n\t/* Goal is to send at least one packet per ms,\n\t * not one big TSO packet every 100 ms.\n\t * This preserves ACK clocking and is consistent\n\t * with tcp_tso_should_defer() heuristic.\n\t */\n\tsegs = max_t(u32, bytes / mss_now, min_tso_segs);\n\n\treturn segs;\n}\n\n/* Return the number of segments we want in the skb we are transmitting.\n * See if congestion control module wants to decide; otherwise, autosize.\n */\nstatic u32 tcp_tso_segs(struct sock *sk, unsigned int mss_now)\n{\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tu32 min_tso, tso_segs;\n\n\tmin_tso = ca_ops->min_tso_segs ?\n\t\t\tca_ops->min_tso_segs(sk) :\n\t\t\tsock_net(sk)->ipv4.sysctl_tcp_min_tso_segs;\n\n\ttso_segs = tcp_tso_autosize(sk, mss_now, min_tso);\n\treturn min_t(u32, tso_segs, sk->sk_gso_max_segs);\n}\n\n/* Returns the portion of skb which can be sent right away */\nstatic unsigned int tcp_mss_split_point(const struct sock *sk,\n\t\t\t\t\tconst struct sk_buff *skb,\n\t\t\t\t\tunsigned int mss_now,\n\t\t\t\t\tunsigned int max_segs,\n\t\t\t\t\tint nonagle)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu32 partial, needed, window, max_len;\n\n\twindow = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\tmax_len = mss_now * max_segs;\n\n\tif (likely(max_len <= window && skb != tcp_write_queue_tail(sk)))\n\t\treturn max_len;\n\n\tneeded = min(skb->len, window);\n\n\tif (max_len <= needed)\n\t\treturn max_len;\n\n\tpartial = needed % mss_now;\n\t/* If last segment is not a full MSS, check if Nagle rules allow us\n\t * to include this last segment in this skb.\n\t * Otherwise, we'll split the skb at last MSS boundary\n\t */\n\tif (tcp_nagle_check(partial != 0, tp, nonagle))\n\t\treturn needed - partial;\n\n\treturn needed;\n}\n\n/* Can at least one segment of SKB be sent right now, according to the\n * congestion window rules?  If so, return how many segments are allowed.\n */\nstatic inline unsigned int tcp_cwnd_test(const struct tcp_sock *tp,\n\t\t\t\t\t const struct sk_buff *skb)\n{\n\tu32 in_flight, cwnd, halfcwnd;\n\n\t/* Don't be strict about the congestion window for the final FIN.  */\n\tif ((TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) &&\n\t    tcp_skb_pcount(skb) == 1)\n\t\treturn 1;\n\n\tin_flight = tcp_packets_in_flight(tp);\n\tcwnd = tp->snd_cwnd;\n\tif (in_flight >= cwnd)\n\t\treturn 0;\n\n\t/* For better scheduling, ensure we have at least\n\t * 2 GSO packets in flight.\n\t */\n\thalfcwnd = max(cwnd >> 1, 1U);\n\treturn min(halfcwnd, cwnd - in_flight);\n}\n\n/* Initialize TSO state of a skb.\n * This must be invoked the first time we consider transmitting\n * SKB onto the wire.\n */\nstatic int tcp_init_tso_segs(struct sk_buff *skb, unsigned int mss_now)\n{\n\tint tso_segs = tcp_skb_pcount(skb);\n\n\tif (!tso_segs || (tso_segs > 1 && tcp_skb_mss(skb) != mss_now)) {\n\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\ttso_segs = tcp_skb_pcount(skb);\n\t}\n\treturn tso_segs;\n}\n\n\n/* Return true if the Nagle test allows this packet to be\n * sent now.\n */\nstatic inline bool tcp_nagle_test(const struct tcp_sock *tp, const struct sk_buff *skb,\n\t\t\t\t  unsigned int cur_mss, int nonagle)\n{\n\t/* Nagle rule does not apply to frames, which sit in the middle of the\n\t * write_queue (they have no chances to get new data).\n\t *\n\t * This is implemented in the callers, where they modify the 'nonagle'\n\t * argument based upon the location of SKB in the send queue.\n\t */\n\tif (nonagle & TCP_NAGLE_PUSH)\n\t\treturn true;\n\n\t/* Don't use the nagle rule for urgent data (or for the final FIN). */\n\tif (tcp_urg_mode(tp) || (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN))\n\t\treturn true;\n\n\tif (!tcp_nagle_check(skb->len < cur_mss, tp, nonagle))\n\t\treturn true;\n\n\treturn false;\n}\n\n/* Does at least the first segment of SKB fit into the send window? */\nstatic bool tcp_snd_wnd_test(const struct tcp_sock *tp,\n\t\t\t     const struct sk_buff *skb,\n\t\t\t     unsigned int cur_mss)\n{\n\tu32 end_seq = TCP_SKB_CB(skb)->end_seq;\n\n\tif (skb->len > cur_mss)\n\t\tend_seq = TCP_SKB_CB(skb)->seq + cur_mss;\n\n\treturn !after(end_seq, tcp_wnd_end(tp));\n}\n\n/* Trim TSO SKB to LEN bytes, put the remaining data into a new packet\n * which is put after SKB on the list.  It is very much like\n * tcp_fragment() except that it may make several kinds of assumptions\n * in order to speed up the splitting operation.  In particular, we\n * know that all the data is in scatter-gather pages, and that the\n * packet has never been sent out before (and thus is not cloned).\n */\nstatic int tso_fragment(struct sock *sk, enum tcp_queue tcp_queue,\n\t\t\tstruct sk_buff *skb, unsigned int len,\n\t\t\tunsigned int mss_now, gfp_t gfp)\n{\n\tstruct sk_buff *buff;\n\tint nlen = skb->len - len;\n\tu8 flags;\n\n\t/* All of a TSO frame must be composed of paged data.  */\n\tif (skb->len != skb->data_len)\n\t\treturn tcp_fragment(sk, tcp_queue, skb, len, mss_now, gfp);\n\n\tbuff = sk_stream_alloc_skb(sk, 0, gfp, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOMEM;\n\n\tsk->sk_wmem_queued += buff->truesize;\n\tsk_mem_charge(sk, buff->truesize);\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t/* Correct the sequence numbers. */\n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t/* PSH and FIN should only be set in the second packet. */\n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\n\t/* This packet was never sent out yet, so no SACK bits. */\n\tTCP_SKB_CB(buff)->sacked = 0;\n\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\tskb_split(skb, buff, len);\n\ttcp_fragment_tstamp(skb, buff);\n\n\t/* Fix up tso_factor for both original and new SKB.  */\n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t/* Link BUFF into the send queue. */\n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, tcp_queue);\n\n\treturn 0;\n}\n\n/* Try to defer sending, if possible, in order to minimize the amount\n * of TSO splitting we do.  View it as a kind of TSO Nagle test.\n *\n * This algorithm is from John Heffner.\n */\nstatic bool tcp_tso_should_defer(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t bool *is_cwnd_limited,\n\t\t\t\t bool *is_rwnd_limited,\n\t\t\t\t u32 max_segs)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 age, send_win, cong_win, limit, in_flight;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *head;\n\tint win_divisor;\n\n\tif (icsk->icsk_ca_state >= TCP_CA_Recovery)\n\t\tgoto send_now;\n\n\t/* Avoid bursty behavior by allowing defer\n\t * only if the last write was recent.\n\t */\n\tif ((s32)(tcp_jiffies32 - tp->lsndtime) > 0)\n\t\tgoto send_now;\n\n\tin_flight = tcp_packets_in_flight(tp);\n\n\tBUG_ON(tcp_skb_pcount(skb) <= 1);\n\tBUG_ON(tp->snd_cwnd <= in_flight);\n\n\tsend_win = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t/* From in_flight test above, we know that cwnd > in_flight.  */\n\tcong_win = (tp->snd_cwnd - in_flight) * tp->mss_cache;\n\n\tlimit = min(send_win, cong_win);\n\n\t/* If a full-sized TSO skb can be sent, do it. */\n\tif (limit >= max_segs * tp->mss_cache)\n\t\tgoto send_now;\n\n\t/* Middle in queue won't get any more data, full sendable already? */\n\tif ((skb != tcp_write_queue_tail(sk)) && (limit >= skb->len))\n\t\tgoto send_now;\n\n\twin_divisor = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_tso_win_divisor);\n\tif (win_divisor) {\n\t\tu32 chunk = min(tp->snd_wnd, tp->snd_cwnd * tp->mss_cache);\n\n\t\t/* If at least some fraction of a window is available,\n\t\t * just use it.\n\t\t */\n\t\tchunk /= win_divisor;\n\t\tif (limit >= chunk)\n\t\t\tgoto send_now;\n\t} else {\n\t\t/* Different approach, try not to defer past a single\n\t\t * ACK.  Receiver should ACK every other full sized\n\t\t * frame, so if we have space for more than 3 frames\n\t\t * then send now.\n\t\t */\n\t\tif (limit > tcp_max_tso_deferred_mss(tp) * tp->mss_cache)\n\t\t\tgoto send_now;\n\t}\n\n\t/* TODO : use tsorted_sent_queue ? */\n\thead = tcp_rtx_queue_head(sk);\n\tif (!head)\n\t\tgoto send_now;\n\tage = tcp_stamp_us_delta(tp->tcp_mstamp, head->skb_mstamp);\n\t/* If next ACK is likely to come too late (half srtt), do not defer */\n\tif (age < (tp->srtt_us >> 4))\n\t\tgoto send_now;\n\n\t/* Ok, it looks like it is advisable to defer.\n\t * Three cases are tracked :\n\t * 1) We are cwnd-limited\n\t * 2) We are rwnd-limited\n\t * 3) We are application limited.\n\t */\n\tif (cong_win < send_win) {\n\t\tif (cong_win <= skb->len) {\n\t\t\t*is_cwnd_limited = true;\n\t\t\treturn true;\n\t\t}\n\t} else {\n\t\tif (send_win <= skb->len) {\n\t\t\t*is_rwnd_limited = true;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t/* If this packet won't get more data, do not wait. */\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\tgoto send_now;\n\n\treturn true;\n\nsend_now:\n\treturn false;\n}\n\nstatic inline void tcp_mtu_check_reprobe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 interval;\n\ts32 delta;\n\n\tinterval = net->ipv4.sysctl_tcp_probe_interval;\n\tdelta = tcp_jiffies32 - icsk->icsk_mtup.probe_timestamp;\n\tif (unlikely(delta >= interval * HZ)) {\n\t\tint mss = tcp_current_mss(sk);\n\n\t\t/* Update current search range */\n\t\ticsk->icsk_mtup.probe_size = 0;\n\t\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp +\n\t\t\tsizeof(struct tcphdr) +\n\t\t\ticsk->icsk_af_ops->net_header_len;\n\t\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);\n\n\t\t/* Update probe time stamp */\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n\t}\n}\n\nstatic bool tcp_can_coalesce_send_queue_head(struct sock *sk, int len)\n{\n\tstruct sk_buff *skb, *next;\n\n\tskb = tcp_send_head(sk);\n\ttcp_for_write_queue_from_safe(skb, next, sk) {\n\t\tif (len <= skb->len)\n\t\t\tbreak;\n\n\t\tif (unlikely(TCP_SKB_CB(skb)->eor) || tcp_has_tx_tstamp(skb))\n\t\t\treturn false;\n\n\t\tlen -= skb->len;\n\t}\n\n\treturn true;\n}\n\n/* Create a new MTU probe if we are ready.\n * MTU probe is regularly attempting to increase the path MTU by\n * deliberately sending larger packets.  This discovers routing\n * changes resulting in larger path MTUs.\n *\n * Returns 0 if we should wait to probe (no cwnd available),\n *         1 if a probe was sent,\n *         -1 otherwise\n */\nstatic int tcp_mtu_probe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *nskb, *next;\n\tstruct net *net = sock_net(sk);\n\tint probe_size;\n\tint size_needed;\n\tint copy, len;\n\tint mss_now;\n\tint interval;\n\n\t/* Not currently probing/verifying,\n\t * not in recovery,\n\t * have enough cwnd, and\n\t * not SACKing (the variable headers throw things off)\n\t */\n\tif (likely(!icsk->icsk_mtup.enabled ||\n\t\t   icsk->icsk_mtup.probe_size ||\n\t\t   inet_csk(sk)->icsk_ca_state != TCP_CA_Open ||\n\t\t   tp->snd_cwnd < 11 ||\n\t\t   tp->rx_opt.num_sacks || tp->rx_opt.dsack))\n\t\treturn -1;\n\n\t/* Use binary search for probe_size between tcp_mss_base,\n\t * and current mss_clamp. if (search_high - search_low)\n\t * smaller than a threshold, backoff from probing.\n\t */\n\tmss_now = tcp_current_mss(sk);\n\tprobe_size = tcp_mtu_to_mss(sk, (icsk->icsk_mtup.search_high +\n\t\t\t\t    icsk->icsk_mtup.search_low) >> 1);\n\tsize_needed = probe_size + (tp->reordering + 1) * tp->mss_cache;\n\tinterval = icsk->icsk_mtup.search_high - icsk->icsk_mtup.search_low;\n\t/* When misfortune happens, we are reprobing actively,\n\t * and then reprobe timer has expired. We stick with current\n\t * probing process by not resetting search range to its orignal.\n\t */\n\tif (probe_size > tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_high) ||\n\t\tinterval < net->ipv4.sysctl_tcp_probe_threshold) {\n\t\t/* Check whether enough time has elaplased for\n\t\t * another round of probing.\n\t\t */\n\t\ttcp_mtu_check_reprobe(sk);\n\t\treturn -1;\n\t}\n\n\t/* Have enough data in the send queue to probe? */\n\tif (tp->write_seq - tp->snd_nxt < size_needed)\n\t\treturn -1;\n\n\tif (tp->snd_wnd < size_needed)\n\t\treturn -1;\n\tif (after(tp->snd_nxt + size_needed, tcp_wnd_end(tp)))\n\t\treturn 0;\n\n\t/* Do we need to wait to drain cwnd? With none in flight, don't stall */\n\tif (tcp_packets_in_flight(tp) + 2 > tp->snd_cwnd) {\n\t\tif (!tcp_packets_in_flight(tp))\n\t\t\treturn -1;\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\tif (!tcp_can_coalesce_send_queue_head(sk, probe_size))\n\t\treturn -1;\n\n\tif (tcp_pacing_check(sk))\n\t\treturn -1;\n\n\t/* We're allowed to probe.  Build it now. */\n\tnskb = sk_stream_alloc_skb(sk, probe_size, GFP_ATOMIC, false);\n\tif (!nskb)\n\t\treturn -1;\n\tsk->sk_wmem_queued += nskb->truesize;\n\tsk_mem_charge(sk, nskb->truesize);\n\n\tskb = tcp_send_head(sk);\n\n\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(skb)->seq;\n\tTCP_SKB_CB(nskb)->end_seq = TCP_SKB_CB(skb)->seq + probe_size;\n\tTCP_SKB_CB(nskb)->tcp_flags = TCPHDR_ACK;\n\tTCP_SKB_CB(nskb)->sacked = 0;\n\tnskb->csum = 0;\n\tnskb->ip_summed = CHECKSUM_PARTIAL;\n\n\ttcp_insert_write_queue_before(nskb, skb, sk);\n\ttcp_highest_sack_replace(sk, skb, nskb);\n\n\tlen = 0;\n\ttcp_for_write_queue_from_safe(skb, next, sk) {\n\t\tcopy = min_t(int, skb->len, probe_size - len);\n\t\tskb_copy_bits(skb, 0, skb_put(nskb, copy), copy);\n\n\t\tif (skb->len <= copy) {\n\t\t\t/* We've eaten all the data from this skb.\n\t\t\t * Throw it away. */\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\t\t\t/* If this is the last SKB we copy and eor is set\n\t\t\t * we need to propagate it to the new skb.\n\t\t\t */\n\t\t\tTCP_SKB_CB(nskb)->eor = TCP_SKB_CB(skb)->eor;\n\t\t\ttcp_skb_collapse_tstamp(nskb, skb);\n\t\t\ttcp_unlink_write_queue(skb, sk);\n\t\t\tsk_wmem_free_skb(sk, skb);\n\t\t} else {\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags &\n\t\t\t\t\t\t   ~(TCPHDR_FIN|TCPHDR_PSH);\n\t\t\tif (!skb_shinfo(skb)->nr_frags) {\n\t\t\t\tskb_pull(skb, copy);\n\t\t\t} else {\n\t\t\t\t__pskb_trim_head(skb, copy);\n\t\t\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\t\t}\n\t\t\tTCP_SKB_CB(skb)->seq += copy;\n\t\t}\n\n\t\tlen += copy;\n\n\t\tif (len >= probe_size)\n\t\t\tbreak;\n\t}\n\ttcp_init_tso_segs(nskb, nskb->len);\n\n\t/* We're ready to send.  If this fails, the probe will\n\t * be resegmented into mss-sized pieces by tcp_write_xmit().\n\t */\n\tif (!tcp_transmit_skb(sk, nskb, 1, GFP_ATOMIC)) {\n\t\t/* Decrement cwnd here because we are sending\n\t\t * effectively two packets. */\n\t\ttp->snd_cwnd--;\n\t\ttcp_event_new_data_sent(sk, nskb);\n\n\t\ticsk->icsk_mtup.probe_size = tcp_mss_to_mtu(sk, nskb->len);\n\t\ttp->mtu_probe.probe_seq_start = TCP_SKB_CB(nskb)->seq;\n\t\ttp->mtu_probe.probe_seq_end = TCP_SKB_CB(nskb)->end_seq;\n\n\t\treturn 1;\n\t}\n\n\treturn -1;\n}\n\n/* TCP Small Queues :\n * Control number of packets in qdisc/devices to two packets / or ~1 ms.\n * (These limits are doubled for retransmits)\n * This allows for :\n *  - better RTT estimation and ACK scheduling\n *  - faster recovery\n *  - high rates\n * Alas, some drivers / subsystems require a fair amount\n * of queued bytes to ensure line rate.\n * One example is wifi aggregation (802.11 AMPDU)\n */\nstatic bool tcp_small_queue_check(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t  unsigned int factor)\n{\n\tunsigned int limit;\n\n\tlimit = max(2 * skb->truesize, sk->sk_pacing_rate >> sk->sk_pacing_shift);\n\tlimit = min_t(u32, limit,\n\t\t      sock_net(sk)->ipv4.sysctl_tcp_limit_output_bytes);\n\tlimit <<= factor;\n\n\tif (refcount_read(&sk->sk_wmem_alloc) > limit) {\n\t\t/* Always send skb if rtx queue is empty.\n\t\t * No need to wait for TX completion to call us back,\n\t\t * after softirq/tasklet schedule.\n\t\t * This helps when TX completions are delayed too much.\n\t\t */\n\t\tif (tcp_rtx_queue_empty(sk))\n\t\t\treturn false;\n\n\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t/* It is possible TX completion already happened\n\t\t * before we set TSQ_THROTTLED, so we must\n\t\t * test again the condition.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tif (refcount_read(&sk->sk_wmem_alloc) > limit)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void tcp_chrono_set(struct tcp_sock *tp, const enum tcp_chrono new)\n{\n\tconst u32 now = tcp_jiffies32;\n\tenum tcp_chrono old = tp->chrono_type;\n\n\tif (old > TCP_CHRONO_UNSPEC)\n\t\ttp->chrono_stat[old - 1] += now - tp->chrono_start;\n\ttp->chrono_start = now;\n\ttp->chrono_type = new;\n}\n\nvoid tcp_chrono_start(struct sock *sk, const enum tcp_chrono type)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If there are multiple conditions worthy of tracking in a\n\t * chronograph then the highest priority enum takes precedence\n\t * over the other conditions. So that if something \"more interesting\"\n\t * starts happening, stop the previous chrono and start a new one.\n\t */\n\tif (type > tp->chrono_type)\n\t\ttcp_chrono_set(tp, type);\n}\n\nvoid tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\n\t/* There are multiple conditions worthy of tracking in a\n\t * chronograph, so that the highest priority enum takes\n\t * precedence over the other conditions (see tcp_chrono_start).\n\t * If a condition stops, we only stop chrono tracking if\n\t * it's the \"most interesting\" or current chrono we are\n\t * tracking and starts busy chrono if we have pending data.\n\t */\n\tif (tcp_rtx_and_write_queues_empty(sk))\n\t\ttcp_chrono_set(tp, TCP_CHRONO_UNSPEC);\n\telse if (type == tp->chrono_type)\n\t\ttcp_chrono_set(tp, TCP_CHRONO_BUSY);\n}\n\n/* This routine writes packets to the network.  It advances the\n * send_head.  This happens as incoming acks open up the remote\n * window for us.\n *\n * LARGESEND note: !tcp_urg_mode is overkill, only frames between\n * snd_up-64k-mss .. snd_up cannot be large. However, taking into\n * account rare use of URG, this is not a big flaw.\n *\n * Send at most one packet when push_one > 0. Temporarily ignore\n * cwnd limit to force at most one packet out when push_one == 2.\n\n * Returns true, if no segments are in flight and we have queued segments,\n * but cannot send anything now because of SWS or another problem.\n */\nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\n\t\t\t   int push_one, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int tso_segs, sent_pkts;\n\tint cwnd_quota;\n\tint result;\n\tbool is_cwnd_limited = false, is_rwnd_limited = false;\n\tu32 max_segs;\n\n\tsent_pkts = 0;\n\n\ttcp_mstamp_refresh(tp);\n\tif (!push_one) {\n\t\t/* Do MTU probing. */\n\t\tresult = tcp_mtu_probe(sk);\n\t\tif (!result) {\n\t\t\treturn false;\n\t\t} else if (result > 0) {\n\t\t\tsent_pkts = 1;\n\t\t}\n\t}\n\n\tmax_segs = tcp_tso_segs(sk, mss_now);\n\twhile ((skb = tcp_send_head(sk))) {\n\t\tunsigned int limit;\n\n\t\tif (tcp_pacing_check(sk))\n\t\t\tbreak;\n\n\t\ttso_segs = tcp_init_tso_segs(skb, mss_now);\n\t\tBUG_ON(!tso_segs);\n\n\t\tif (unlikely(tp->repair) && tp->repair_queue == TCP_SEND_QUEUE) {\n\t\t\t/* \"skb_mstamp\" is used as a start point for the retransmit timer */\n\t\t\ttcp_update_skb_after_send(tp, skb);\n\t\t\tgoto repair; /* Skip network transmission */\n\t\t}\n\n\t\tcwnd_quota = tcp_cwnd_test(tp, skb);\n\t\tif (!cwnd_quota) {\n\t\t\tif (push_one == 2)\n\t\t\t\t/* Force out a loss probe pkt. */\n\t\t\t\tcwnd_quota = 1;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now))) {\n\t\t\tis_rwnd_limited = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tso_segs == 1) {\n\t\t\tif (unlikely(!tcp_nagle_test(tp, skb, mss_now,\n\t\t\t\t\t\t     (tcp_skb_is_last(sk, skb) ?\n\t\t\t\t\t\t      nonagle : TCP_NAGLE_PUSH))))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (!push_one &&\n\t\t\t    tcp_tso_should_defer(sk, skb, &is_cwnd_limited,\n\t\t\t\t\t\t &is_rwnd_limited, max_segs))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tlimit = mss_now;\n\t\tif (tso_segs > 1 && !tcp_urg_mode(tp))\n\t\t\tlimit = tcp_mss_split_point(sk, skb, mss_now,\n\t\t\t\t\t\t    min_t(unsigned int,\n\t\t\t\t\t\t\t  cwnd_quota,\n\t\t\t\t\t\t\t  max_segs),\n\t\t\t\t\t\t    nonagle);\n\n\t\tif (skb->len > limit &&\n\t\t    unlikely(tso_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t\t  skb, limit, mss_now, gfp)))\n\t\t\tbreak;\n\n\t\tif (tcp_small_queue_check(sk, skb, 0))\n\t\t\tbreak;\n\n\t\t/* Argh, we hit an empty skb(), presumably a thread\n\t\t * is sleeping in sendmsg()/sk_stream_wait_memory().\n\t\t * We do not want to send a pure-ack packet and have\n\t\t * a strange looking rtx queue with empty packet(s).\n\t\t */\n\t\tif (TCP_SKB_CB(skb)->end_seq == TCP_SKB_CB(skb)->seq)\n\t\t\tbreak;\n\n\t\tif (unlikely(tcp_transmit_skb(sk, skb, 1, gfp)))\n\t\t\tbreak;\n\nrepair:\n\t\t/* Advance the send_head.  This one is sent out.\n\t\t * This call will increment packets_out.\n\t\t */\n\t\ttcp_event_new_data_sent(sk, skb);\n\n\t\ttcp_minshall_update(tp, mss_now, skb);\n\t\tsent_pkts += tcp_skb_pcount(skb);\n\n\t\tif (push_one)\n\t\t\tbreak;\n\t}\n\n\tif (is_rwnd_limited)\n\t\ttcp_chrono_start(sk, TCP_CHRONO_RWND_LIMITED);\n\telse\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_RWND_LIMITED);\n\n\tis_cwnd_limited |= (tcp_packets_in_flight(tp) >= tp->snd_cwnd);\n\tif (likely(sent_pkts || is_cwnd_limited))\n\t\ttcp_cwnd_validate(sk, is_cwnd_limited);\n\n\tif (likely(sent_pkts)) {\n\t\tif (tcp_in_cwnd_reduction(sk))\n\t\t\ttp->prr_out += sent_pkts;\n\n\t\t/* Send one loss probe per tail loss episode. */\n\t\tif (push_one != 2)\n\t\t\ttcp_schedule_loss_probe(sk, false);\n\t\treturn false;\n\t}\n\treturn !tp->packets_out && !tcp_write_queue_empty(sk);\n}\n\nbool tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 timeout, rto_delta_us;\n\tint early_retrans;\n\n\t/* Don't do any loss probe on a Fast Open connection before 3WHS\n\t * finishes.\n\t */\n\tif (tp->fastopen_rsk)\n\t\treturn false;\n\n\tearly_retrans = sock_net(sk)->ipv4.sysctl_tcp_early_retrans;\n\t/* Schedule a loss probe in 2*RTT for SACK capable connections\n\t * not in loss recovery, that are either limited by cwnd or application.\n\t */\n\tif ((early_retrans != 3 && early_retrans != 4) ||\n\t    !tp->packets_out || !tcp_is_sack(tp) ||\n\t    (icsk->icsk_ca_state != TCP_CA_Open &&\n\t     icsk->icsk_ca_state != TCP_CA_CWR))\n\t\treturn false;\n\n\t/* Probe timeout is 2*rtt. Add minimum RTO to account\n\t * for delayed ack when there's one outstanding packet. If no RTT\n\t * sample is available then probe after TCP_TIMEOUT_INIT.\n\t */\n\tif (tp->srtt_us) {\n\t\ttimeout = usecs_to_jiffies(tp->srtt_us >> 2);\n\t\tif (tp->packets_out == 1)\n\t\t\ttimeout += TCP_RTO_MIN;\n\t\telse\n\t\t\ttimeout += TCP_TIMEOUT_MIN;\n\t} else {\n\t\ttimeout = TCP_TIMEOUT_INIT;\n\t}\n\n\t/* If the RTO formula yields an earlier time, then use that time. */\n\trto_delta_us = advancing_rto ?\n\t\t\tjiffies_to_usecs(inet_csk(sk)->icsk_rto) :\n\t\t\ttcp_rto_delta_us(sk);  /* How far in future is RTO? */\n\tif (rto_delta_us > 0)\n\t\ttimeout = min_t(u32, timeout, usecs_to_jiffies(rto_delta_us));\n\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_LOSS_PROBE, timeout,\n\t\t\t\t  TCP_RTO_MAX);\n\treturn true;\n}\n\n/* Thanks to skb fast clones, we can detect if a prior transmit of\n * a packet is still in a qdisc or driver queue.\n * In this case, there is very little point doing a retransmit !\n */\nstatic bool skb_still_in_host_queue(const struct sock *sk,\n\t\t\t\t    const struct sk_buff *skb)\n{\n\tif (unlikely(skb_fclone_busy(sk, skb))) {\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t      LINUX_MIB_TCPSPURIOUS_RTX_HOSTQUEUES);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* When probe timeout (PTO) fires, try send a new segment if possible, else\n * retransmit the last segment.\n */\nvoid tcp_send_loss_probe(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint pcount;\n\tint mss = tcp_current_mss(sk);\n\n\t/* At most one outstanding TLP */\n\tif (tp->tlp_high_seq)\n\t\tgoto rearm_timer;\n\n\ttp->tlp_retrans = 0;\n\tskb = tcp_send_head(sk);\n\tif (skb && tcp_snd_wnd_test(tp, skb, mss)) {\n\t\tpcount = tp->packets_out;\n\t\ttcp_write_xmit(sk, mss, TCP_NAGLE_OFF, 2, GFP_ATOMIC);\n\t\tif (tp->packets_out > pcount)\n\t\t\tgoto probe_sent;\n\t\tgoto rearm_timer;\n\t}\n\tskb = skb_rb_last(&sk->tcp_rtx_queue);\n\tif (unlikely(!skb)) {\n\t\tWARN_ONCE(tp->packets_out,\n\t\t\t  \"invalid inflight: %u state %u cwnd %u mss %d\\n\",\n\t\t\t  tp->packets_out, sk->sk_state, tp->snd_cwnd, mss);\n\t\tinet_csk(sk)->icsk_pending = 0;\n\t\treturn;\n\t}\n\n\tif (skb_still_in_host_queue(sk, skb))\n\t\tgoto rearm_timer;\n\n\tpcount = tcp_skb_pcount(skb);\n\tif (WARN_ON(!pcount))\n\t\tgoto rearm_timer;\n\n\tif ((pcount > 1) && (skb->len > (pcount - 1) * mss)) {\n\t\tif (unlikely(tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t\t  (pcount - 1) * mss, mss,\n\t\t\t\t\t  GFP_ATOMIC)))\n\t\t\tgoto rearm_timer;\n\t\tskb = skb_rb_next(skb);\n\t}\n\n\tif (WARN_ON(!skb || !tcp_skb_pcount(skb)))\n\t\tgoto rearm_timer;\n\n\tif (__tcp_retransmit_skb(sk, skb, 1))\n\t\tgoto rearm_timer;\n\n\ttp->tlp_retrans = 1;\n\nprobe_sent:\n\t/* Record snd_nxt for loss detection. */\n\ttp->tlp_high_seq = tp->snd_nxt;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSPROBES);\n\t/* Reset s.t. tcp_rearm_rto will restart timer from now */\n\tinet_csk(sk)->icsk_pending = 0;\nrearm_timer:\n\ttcp_rearm_rto(sk);\n}\n\n/* Push out any pending frames which were held back due to\n * TCP_CORK or attempt at coalescing tiny packets.\n * The socket must be locked by the caller.\n */\nvoid __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\n\t\t\t       int nonagle)\n{\n\t/* If we are closed, the bytes will have to remain here.\n\t * In time closedown will finish, we empty the write queue and\n\t * all will be happy.\n\t */\n\tif (unlikely(sk->sk_state == TCP_CLOSE))\n\t\treturn;\n\n\tif (tcp_write_xmit(sk, cur_mss, nonagle, 0,\n\t\t\t   sk_gfp_mask(sk, GFP_ATOMIC)))\n\t\ttcp_check_probe_timer(sk);\n}\n\n/* Send _single_ skb sitting at the send head. This function requires\n * true push pending frames to setup probe timer etc.\n */\nvoid tcp_push_one(struct sock *sk, unsigned int mss_now)\n{\n\tstruct sk_buff *skb = tcp_send_head(sk);\n\n\tBUG_ON(!skb || skb->len < mss_now);\n\n\ttcp_write_xmit(sk, mss_now, TCP_NAGLE_PUSH, 1, sk->sk_allocation);\n}\n\n/* This function returns the amount that we can raise the\n * usable window based on the following constraints\n *\n * 1. The window can never be shrunk once it is offered (RFC 793)\n * 2. We limit memory per socket\n *\n * RFC 1122:\n * \"the suggested [SWS] avoidance algorithm for the receiver is to keep\n *  RECV.NEXT + RCV.WIN fixed until:\n *  RCV.BUFF - RCV.USER - RCV.WINDOW >= min(1/2 RCV.BUFF, MSS)\"\n *\n * i.e. don't raise the right edge of the window until you can raise\n * it at least MSS bytes.\n *\n * Unfortunately, the recommended algorithm breaks header prediction,\n * since header prediction assumes th->window stays fixed.\n *\n * Strictly speaking, keeping th->window fixed violates the receiver\n * side SWS prevention criteria. The problem is that under this rule\n * a stream of single byte packets will cause the right side of the\n * window to always advance by a single byte.\n *\n * Of course, if the sender implements sender side SWS prevention\n * then this will not be a problem.\n *\n * BSD seems to make the following compromise:\n *\n *\tIf the free space is less than the 1/4 of the maximum\n *\tspace available and the free space is less than 1/2 mss,\n *\tthen set the window to 0.\n *\t[ Actually, bsd uses MSS and 1/4 of maximal _window_ ]\n *\tOtherwise, just prevent the window from shrinking\n *\tand from being larger than the largest representable value.\n *\n * This prevents incremental opening of the window in the regime\n * where TCP is limited by the speed of the reader side taking\n * data out of the TCP receive queue. It does nothing about\n * those cases where the window is constrained on the sender side\n * because the pipeline is full.\n *\n * BSD also seems to \"accidentally\" limit itself to windows that are a\n * multiple of MSS, at least until the free space gets quite small.\n * This would appear to be a side effect of the mbuf implementation.\n * Combining these two algorithms results in the observed behavior\n * of having a fixed window size at almost all times.\n *\n * Below we obtain similar behavior by forcing the offered window to\n * a multiple of the mss when it is feasible to do so.\n *\n * Note, we don't \"adjust\" for TIMESTAMP or SACK option bytes.\n * Regular options like TIMESTAMP are taken into account.\n */\nu32 __tcp_select_window(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t/* MSS for the peer's data.  Previous versions used mss_clamp\n\t * here.  I don't know if the value based on our guesses\n\t * of peer's MSS is better for the performance.  It's more correct\n\t * but may be worse for the performance because of rcv_mss\n\t * fluctuations.  --SAW  1998/11/1\n\t */\n\tint mss = icsk->icsk_ack.rcv_mss;\n\tint free_space = tcp_space(sk);\n\tint allowed_space = tcp_full_space(sk);\n\tint full_space = min_t(int, tp->window_clamp, allowed_space);\n\tint window;\n\n\tif (unlikely(mss > full_space)) {\n\t\tmss = full_space;\n\t\tif (mss <= 0)\n\t\t\treturn 0;\n\t}\n\tif (free_space < (full_space >> 1)) {\n\t\ticsk->icsk_ack.quick = 0;\n\n\t\tif (tcp_under_memory_pressure(sk))\n\t\t\ttp->rcv_ssthresh = min(tp->rcv_ssthresh,\n\t\t\t\t\t       4U * tp->advmss);\n\n\t\t/* free_space might become our new window, make sure we don't\n\t\t * increase it due to wscale.\n\t\t */\n\t\tfree_space = round_down(free_space, 1 << tp->rx_opt.rcv_wscale);\n\n\t\t/* if free space is less than mss estimate, or is below 1/16th\n\t\t * of the maximum allowed, try to move to zero-window, else\n\t\t * tcp_clamp_window() will grow rcv buf up to tcp_rmem[2], and\n\t\t * new incoming data is dropped due to memory limits.\n\t\t * With large window, mss test triggers way too late in order\n\t\t * to announce zero window in time before rmem limit kicks in.\n\t\t */\n\t\tif (free_space < (allowed_space >> 4) || free_space < mss)\n\t\t\treturn 0;\n\t}\n\n\tif (free_space > tp->rcv_ssthresh)\n\t\tfree_space = tp->rcv_ssthresh;\n\n\t/* Don't do rounding if we are using window scaling, since the\n\t * scaled window will not line up with the MSS boundary anyway.\n\t */\n\tif (tp->rx_opt.rcv_wscale) {\n\t\twindow = free_space;\n\n\t\t/* Advertise enough space so that it won't get scaled away.\n\t\t * Import case: prevent zero window announcement if\n\t\t * 1<<rcv_wscale > mss.\n\t\t */\n\t\twindow = ALIGN(window, (1 << tp->rx_opt.rcv_wscale));\n\t} else {\n\t\twindow = tp->rcv_wnd;\n\t\t/* Get the largest window that is a nice multiple of mss.\n\t\t * Window clamp already applied above.\n\t\t * If our current window offering is within 1 mss of the\n\t\t * free space we just keep it. This prevents the divide\n\t\t * and multiply from happening most of the time.\n\t\t * We also don't do any window rounding when the free space\n\t\t * is too small.\n\t\t */\n\t\tif (window <= free_space - mss || window > free_space)\n\t\t\twindow = rounddown(free_space, mss);\n\t\telse if (mss == full_space &&\n\t\t\t free_space > window + (full_space >> 1))\n\t\t\twindow = free_space;\n\t}\n\n\treturn window;\n}\n\nvoid tcp_skb_collapse_tstamp(struct sk_buff *skb,\n\t\t\t     const struct sk_buff *next_skb)\n{\n\tif (unlikely(tcp_has_tx_tstamp(next_skb))) {\n\t\tconst struct skb_shared_info *next_shinfo =\n\t\t\tskb_shinfo(next_skb);\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\tshinfo->tx_flags |= next_shinfo->tx_flags & SKBTX_ANY_TSTAMP;\n\t\tshinfo->tskey = next_shinfo->tskey;\n\t\tTCP_SKB_CB(skb)->txstamp_ack |=\n\t\t\tTCP_SKB_CB(next_skb)->txstamp_ack;\n\t}\n}\n\n/* Collapses two adjacent SKB's during retransmission. */\nstatic bool tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *next_skb = skb_rb_next(skb);\n\tint next_skb_size;\n\n\tnext_skb_size = next_skb->len;\n\n\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\n\n\tif (next_skb_size) {\n\t\tif (next_skb_size <= skb_availroom(skb))\n\t\t\tskb_copy_bits(next_skb, 0, skb_put(skb, next_skb_size),\n\t\t\t\t      next_skb_size);\n\t\telse if (!tcp_skb_shift(skb, next_skb, 1, next_skb_size))\n\t\t\treturn false;\n\t}\n\ttcp_highest_sack_replace(sk, next_skb, skb);\n\n\t/* Update sequence range on original skb. */\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;\n\n\t/* Merge over control information. This moves PSH/FIN etc. over */\n\tTCP_SKB_CB(skb)->tcp_flags |= TCP_SKB_CB(next_skb)->tcp_flags;\n\n\t/* All done, get rid of second SKB and account for it so\n\t * packet counting does not break.\n\t */\n\tTCP_SKB_CB(skb)->sacked |= TCP_SKB_CB(next_skb)->sacked & TCPCB_EVER_RETRANS;\n\tTCP_SKB_CB(skb)->eor = TCP_SKB_CB(next_skb)->eor;\n\n\t/* changed transmit queue under us so clear hints */\n\ttcp_clear_retrans_hints_partial(tp);\n\tif (next_skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = skb;\n\n\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\n\n\ttcp_skb_collapse_tstamp(skb, next_skb);\n\n\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\n\treturn true;\n}\n\n/* Check if coalescing SKBs is legal. */\nstatic bool tcp_can_collapse(const struct sock *sk, const struct sk_buff *skb)\n{\n\tif (tcp_skb_pcount(skb) > 1)\n\t\treturn false;\n\tif (skb_cloned(skb))\n\t\treturn false;\n\t/* Some heuristics for collapsing over SACK'd could be invented */\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\treturn false;\n\n\treturn true;\n}\n\n/* Collapse packets in the retransmit queue to make to create\n * less packets on the wire. This is only done on retransmission.\n */\nstatic void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *to,\n\t\t\t\t     int space)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb = to, *tmp;\n\tbool first = true;\n\n\tif (!sock_net(sk)->ipv4.sysctl_tcp_retrans_collapse)\n\t\treturn;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)\n\t\treturn;\n\n\tskb_rbtree_walk_from_safe(skb, tmp) {\n\t\tif (!tcp_can_collapse(sk, skb))\n\t\t\tbreak;\n\n\t\tif (!tcp_skb_can_collapse_to(to))\n\t\t\tbreak;\n\n\t\tspace -= skb->len;\n\n\t\tif (first) {\n\t\t\tfirst = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (space < 0)\n\t\t\tbreak;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, tcp_wnd_end(tp)))\n\t\t\tbreak;\n\n\t\tif (!tcp_collapse_retrans(sk, to))\n\t\t\tbreak;\n\t}\n}\n\n/* This retransmits one SKB.  Policy decisions and retransmit queue\n * state updates are done by the caller.  Returns non-zero if an\n * error occurred which prevented the send.\n */\nint __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int cur_mss;\n\tint diff, len, err;\n\n\n\t/* Inconclusive MTU probe */\n\tif (icsk->icsk_mtup.probe_size)\n\t\ticsk->icsk_mtup.probe_size = 0;\n\n\t/* Do not sent more than we queued. 1/4 is reserved for possible\n\t * copying overhead: fragmentation, tunneling, mangling etc.\n\t */\n\tif (refcount_read(&sk->sk_wmem_alloc) >\n\t    min_t(u32, sk->sk_wmem_queued + (sk->sk_wmem_queued >> 2),\n\t\t  sk->sk_sndbuf))\n\t\treturn -EAGAIN;\n\n\tif (skb_still_in_host_queue(sk, skb))\n\t\treturn -EBUSY;\n\n\tif (before(TCP_SKB_CB(skb)->seq, tp->snd_una)) {\n\t\tif (unlikely(before(TCP_SKB_CB(skb)->end_seq, tp->snd_una))) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\n\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\n\n\tcur_mss = tcp_current_mss(sk);\n\n\t/* If receiver has shrunk his window, and skb is out of\n\t * new window, do not retransmit it. The exception is the\n\t * case, when window is shrunk to zero. In this case\n\t * our retransmit serves as a zero window probe.\n\t */\n\tif (!before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp)) &&\n\t    TCP_SKB_CB(skb)->seq != tp->snd_una)\n\t\treturn -EAGAIN;\n\n\tlen = cur_mss * segs;\n\tif (skb->len > len) {\n\t\tif (tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb, len,\n\t\t\t\t cur_mss, GFP_ATOMIC))\n\t\t\treturn -ENOMEM; /* We'll try again later. */\n\t} else {\n\t\tif (skb_unclone(skb, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\tdiff = tcp_skb_pcount(skb);\n\t\ttcp_set_skb_tso_segs(skb, cur_mss);\n\t\tdiff -= tcp_skb_pcount(skb);\n\t\tif (diff)\n\t\t\ttcp_adjust_pcount(sk, skb, diff);\n\t\tif (skb->len < cur_mss)\n\t\t\ttcp_retrans_try_collapse(sk, skb, cur_mss);\n\t}\n\n\t/* RFC3168, section 6.1.1.1. ECN fallback */\n\tif ((TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN_ECN) == TCPHDR_SYN_ECN)\n\t\ttcp_ecn_clear_syn(sk, skb);\n\n\t/* Update global and local TCP statistics. */\n\tsegs = tcp_skb_pcount(skb);\n\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS, segs);\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\n\ttp->total_retrans += segs;\n\ttp->bytes_retrans += skb->len;\n\n\t/* make sure skb->data is aligned on arches that require it\n\t * and check if ack-trimming & collapsing extended the headroom\n\t * beyond what csum_start can cover.\n\t */\n\tif (unlikely((NET_IP_ALIGN && ((unsigned long)skb->data & 3)) ||\n\t\t     skb_headroom(skb) >= 0xFFFF)) {\n\t\tstruct sk_buff *nskb;\n\n\t\ttcp_skb_tsorted_save(skb) {\n\t\t\tnskb = __pskb_copy(skb, MAX_TCP_HEADER, GFP_ATOMIC);\n\t\t\terr = nskb ? tcp_transmit_skb(sk, nskb, 0, GFP_ATOMIC) :\n\t\t\t\t     -ENOBUFS;\n\t\t} tcp_skb_tsorted_restore(skb);\n\n\t\tif (!err) {\n\t\t\ttcp_update_skb_after_send(tp, skb);\n\t\t\ttcp_rate_skb_sent(sk, skb);\n\t\t}\n\t} else {\n\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n\t}\n\n\tif (BPF_SOCK_OPS_TEST_FLAG(tp, BPF_SOCK_OPS_RETRANS_CB_FLAG))\n\t\ttcp_call_bpf_3arg(sk, BPF_SOCK_OPS_RETRANS_CB,\n\t\t\t\t  TCP_SKB_CB(skb)->seq, segs, err);\n\n\tif (likely(!err)) {\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_EVER_RETRANS;\n\t\ttrace_tcp_retransmit_skb(sk, skb);\n\t} else if (err != -EBUSY) {\n\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPRETRANSFAIL, segs);\n\t}\n\treturn err;\n}\n\nint tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint err = __tcp_retransmit_skb(sk, skb, segs);\n\n\tif (err == 0) {\n#if FASTRETRANS_DEBUG > 0\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS) {\n\t\t\tnet_dbg_ratelimited(\"retrans_out leaked\\n\");\n\t\t}\n#endif\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_RETRANS;\n\t\ttp->retrans_out += tcp_skb_pcount(skb);\n\t}\n\n\t/* Save stamp of the first (attempted) retransmit. */\n\tif (!tp->retrans_stamp)\n\t\ttp->retrans_stamp = tcp_skb_timestamp(skb);\n\n\tif (tp->undo_retrans < 0)\n\t\ttp->undo_retrans = 0;\n\ttp->undo_retrans += tcp_skb_pcount(skb);\n\treturn err;\n}\n\n/* This gets called after a retransmit timeout, and the initially\n * retransmitted data is acknowledged.  It tries to continue\n * resending the rest of the retransmit queue, until either\n * we've sent it all or the congestion window limit is reached.\n */\nvoid tcp_xmit_retransmit_queue(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sk_buff *skb, *rtx_head, *hole = NULL;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 max_segs;\n\tint mib_idx;\n\n\tif (!tp->packets_out)\n\t\treturn;\n\n\trtx_head = tcp_rtx_queue_head(sk);\n\tskb = tp->retransmit_skb_hint ?: rtx_head;\n\tmax_segs = tcp_tso_segs(sk, tcp_current_mss(sk));\n\tskb_rbtree_walk_from(skb) {\n\t\t__u8 sacked;\n\t\tint segs;\n\n\t\tif (tcp_pacing_check(sk))\n\t\t\tbreak;\n\n\t\t/* we could do better than to assign each time */\n\t\tif (!hole)\n\t\t\ttp->retransmit_skb_hint = skb;\n\n\t\tsegs = tp->snd_cwnd - tcp_packets_in_flight(tp);\n\t\tif (segs <= 0)\n\t\t\treturn;\n\t\tsacked = TCP_SKB_CB(skb)->sacked;\n\t\t/* In case tcp_shift_skb_data() have aggregated large skbs,\n\t\t * we need to make sure not sending too bigs TSO packets\n\t\t */\n\t\tsegs = min_t(int, segs, max_segs);\n\n\t\tif (tp->retrans_out >= tp->lost_out) {\n\t\t\tbreak;\n\t\t} else if (!(sacked & TCPCB_LOST)) {\n\t\t\tif (!hole && !(sacked & (TCPCB_SACKED_RETRANS|TCPCB_SACKED_ACKED)))\n\t\t\t\thole = skb;\n\t\t\tcontinue;\n\n\t\t} else {\n\t\t\tif (icsk->icsk_ca_state != TCP_CA_Loss)\n\t\t\t\tmib_idx = LINUX_MIB_TCPFASTRETRANS;\n\t\t\telse\n\t\t\t\tmib_idx = LINUX_MIB_TCPSLOWSTARTRETRANS;\n\t\t}\n\n\t\tif (sacked & (TCPCB_SACKED_ACKED|TCPCB_SACKED_RETRANS))\n\t\t\tcontinue;\n\n\t\tif (tcp_small_queue_check(sk, skb, 1))\n\t\t\treturn;\n\n\t\tif (tcp_retransmit_skb(sk, skb, segs))\n\t\t\treturn;\n\n\t\tNET_ADD_STATS(sock_net(sk), mib_idx, tcp_skb_pcount(skb));\n\n\t\tif (tcp_in_cwnd_reduction(sk))\n\t\t\ttp->prr_out += tcp_skb_pcount(skb);\n\n\t\tif (skb == rtx_head &&\n\t\t    icsk->icsk_pending != ICSK_TIME_REO_TIMEOUT)\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t\t  inet_csk(sk)->icsk_rto,\n\t\t\t\t\t\t  TCP_RTO_MAX);\n\t}\n}\n\n/* We allow to exceed memory limits for FIN packets to expedite\n * connection tear down and (memory) recovery.\n * Otherwise tcp_send_fin() could be tempted to either delay FIN\n * or even be forced to close flow without any FIN.\n * In general, we want to allow one skb per socket to avoid hangs\n * with edge trigger epoll()\n */\nvoid sk_forced_mem_schedule(struct sock *sk, int size)\n{\n\tint amt;\n\n\tif (size <= sk->sk_forward_alloc)\n\t\treturn;\n\tamt = sk_mem_pages(size);\n\tsk->sk_forward_alloc += amt * SK_MEM_QUANTUM;\n\tsk_memory_allocated_add(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_charge_skmem(sk->sk_memcg, amt);\n}\n\n/* Send a FIN. The caller locks the socket for us.\n * We should try to send a FIN packet really hard, but eventually give up.\n */\nvoid tcp_send_fin(struct sock *sk)\n{\n\tstruct sk_buff *skb, *tskb = tcp_write_queue_tail(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Optimization, tack on the FIN if we have one skb in write queue and\n\t * this skb was not yet sent, or we are under memory pressure.\n\t * Note: in the latter case, FIN packet will be sent after a timeout,\n\t * as TCP stack thinks it has already been transmitted.\n\t */\n\tif (!tskb && tcp_under_memory_pressure(sk))\n\t\ttskb = skb_rb_last(&sk->tcp_rtx_queue);\n\n\tif (tskb) {\ncoalesce:\n\t\tTCP_SKB_CB(tskb)->tcp_flags |= TCPHDR_FIN;\n\t\tTCP_SKB_CB(tskb)->end_seq++;\n\t\ttp->write_seq++;\n\t\tif (tcp_write_queue_empty(sk)) {\n\t\t\t/* This means tskb was already sent.\n\t\t\t * Pretend we included the FIN on previous transmit.\n\t\t\t * We need to set tp->snd_nxt to the value it would have\n\t\t\t * if FIN had been sent. This is because retransmit path\n\t\t\t * does not change tp->snd_nxt.\n\t\t\t */\n\t\t\ttp->snd_nxt++;\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tskb = alloc_skb_fclone(MAX_TCP_HEADER, sk->sk_allocation);\n\t\tif (unlikely(!skb)) {\n\t\t\tif (tskb)\n\t\t\t\tgoto coalesce;\n\t\t\treturn;\n\t\t}\n\t\tINIT_LIST_HEAD(&skb->tcp_tsorted_anchor);\n\t\tskb_reserve(skb, MAX_TCP_HEADER);\n\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t/* FIN eats a sequence byte, write_seq advanced by tcp_queue_skb(). */\n\t\ttcp_init_nondata_skb(skb, tp->write_seq,\n\t\t\t\t     TCPHDR_ACK | TCPHDR_FIN);\n\t\ttcp_queue_skb(sk, skb);\n\t}\n\t__tcp_push_pending_frames(sk, tcp_current_mss(sk), TCP_NAGLE_OFF);\n}\n\n/* We get here when a process closes a file descriptor (either due to\n * an explicit close() or as a byproduct of exit()'ing) and there\n * was unread data in the receive queue.  This behavior is recommended\n * by RFC 2525, section 2.17.  -DaveM\n */\nvoid tcp_send_active_reset(struct sock *sk, gfp_t priority)\n{\n\tstruct sk_buff *skb;\n\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_OUTRSTS);\n\n\t/* NOTE: No TCP options attached and we never retransmit this. */\n\tskb = alloc_skb(MAX_TCP_HEADER, priority);\n\tif (!skb) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);\n\t\treturn;\n\t}\n\n\t/* Reserve space for headers and prepare control bits. */\n\tskb_reserve(skb, MAX_TCP_HEADER);\n\ttcp_init_nondata_skb(skb, tcp_acceptable_seq(sk),\n\t\t\t     TCPHDR_ACK | TCPHDR_RST);\n\ttcp_mstamp_refresh(tcp_sk(sk));\n\t/* Send it off. */\n\tif (tcp_transmit_skb(sk, skb, 0, priority))\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);\n\n\t/* skb of trace_tcp_send_reset() keeps the skb that caused RST,\n\t * skb here is different to the troublesome skb, so use NULL\n\t */\n\ttrace_tcp_send_reset(sk, NULL);\n}\n\n/* Send a crossed SYN-ACK during socket establishment.\n * WARNING: This routine must only be called when we have already sent\n * a SYN packet that crossed the incoming SYN that caused this routine\n * to get called. If this assumption fails then the initial rcv_wnd\n * and rcv_wscale values will not be correct.\n */\nint tcp_send_synack(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (!skb || !(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\tpr_err(\"%s: wrong queue state\\n\", __func__);\n\t\treturn -EFAULT;\n\t}\n\tif (!(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_ACK)) {\n\t\tif (skb_cloned(skb)) {\n\t\t\tstruct sk_buff *nskb;\n\n\t\t\ttcp_skb_tsorted_save(skb) {\n\t\t\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\t\t\t} tcp_skb_tsorted_restore(skb);\n\t\t\tif (!nskb)\n\t\t\t\treturn -ENOMEM;\n\t\t\tINIT_LIST_HEAD(&nskb->tcp_tsorted_anchor);\n\t\t\ttcp_highest_sack_replace(sk, skb, nskb);\n\t\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\t\t\t__skb_header_release(nskb);\n\t\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, nskb);\n\t\t\tsk->sk_wmem_queued += nskb->truesize;\n\t\t\tsk_mem_charge(sk, nskb->truesize);\n\t\t\tskb = nskb;\n\t\t}\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ACK;\n\t\ttcp_ecn_send_synack(sk, skb);\n\t}\n\treturn tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n}\n\n/**\n * tcp_make_synack - Prepare a SYN-ACK.\n * sk: listener socket\n * dst: dst entry attached to the SYNACK\n * req: request_sock pointer\n *\n * Allocate one skb and build a SYNACK packet.\n * @dst is consumed : Caller should not use it again.\n */\nstruct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t\tstruct request_sock *req,\n\t\t\t\tstruct tcp_fastopen_cookie *foc,\n\t\t\t\tenum tcp_synack_type synack_type)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_md5sig_key *md5 = NULL;\n\tstruct tcp_out_options opts;\n\tstruct sk_buff *skb;\n\tint tcp_header_size;\n\tstruct tcphdr *th;\n\tint mss;\n\n\tskb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);\n\tif (unlikely(!skb)) {\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\t/* Reserve space for headers. */\n\tskb_reserve(skb, MAX_TCP_HEADER);\n\n\tswitch (synack_type) {\n\tcase TCP_SYNACK_NORMAL:\n\t\tskb_set_owner_w(skb, req_to_sk(req));\n\t\tbreak;\n\tcase TCP_SYNACK_COOKIE:\n\t\t/* Under synflood, we do not attach skb to a socket,\n\t\t * to avoid false sharing.\n\t\t */\n\t\tbreak;\n\tcase TCP_SYNACK_FASTOPEN:\n\t\t/* sk is a const pointer, because we want to express multiple\n\t\t * cpu might call us concurrently.\n\t\t * sk->sk_wmem_alloc in an atomic, we can promote to rw.\n\t\t */\n\t\tskb_set_owner_w(skb, (struct sock *)sk);\n\t\tbreak;\n\t}\n\tskb_dst_set(skb, dst);\n\n\tmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\n\n\tmemset(&opts, 0, sizeof(opts));\n#ifdef CONFIG_SYN_COOKIES\n\tif (unlikely(req->cookie_ts))\n\t\tskb->skb_mstamp = cookie_init_timestamp(req);\n\telse\n#endif\n\t\tskb->skb_mstamp = tcp_clock_us();\n\n#ifdef CONFIG_TCP_MD5SIG\n\trcu_read_lock();\n\tmd5 = tcp_rsk(req)->af_specific->req_md5_lookup(sk, req_to_sk(req));\n#endif\n\tskb_set_hash(skb, tcp_rsk(req)->txhash, PKT_HASH_TYPE_L4);\n\ttcp_header_size = tcp_synack_options(sk, req, mss, skb, &opts, md5,\n\t\t\t\t\t     foc, synack_type) + sizeof(*th);\n\n\tskb_push(skb, tcp_header_size);\n\tskb_reset_transport_header(skb);\n\n\tth = (struct tcphdr *)skb->data;\n\tmemset(th, 0, sizeof(struct tcphdr));\n\tth->syn = 1;\n\tth->ack = 1;\n\ttcp_ecn_make_synack(req, th);\n\tth->source = htons(ireq->ir_num);\n\tth->dest = ireq->ir_rmt_port;\n\tskb->mark = ireq->ir_mark;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tth->seq = htonl(tcp_rsk(req)->snt_isn);\n\t/* XXX data is queued and acked as is. No buffer/window check */\n\tth->ack_seq = htonl(tcp_rsk(req)->rcv_nxt);\n\n\t/* RFC1323: The window in SYN & SYN/ACK segments is never scaled. */\n\tth->window = htons(min(req->rsk_rcv_wnd, 65535U));\n\ttcp_options_write((__be32 *)(th + 1), NULL, &opts);\n\tth->doff = (tcp_header_size >> 2);\n\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_OUTSEGS);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Okay, we have all we need - do the md5 hash if needed */\n\tif (md5)\n\t\ttcp_rsk(req)->af_specific->calc_md5_hash(opts.hash_location,\n\t\t\t\t\t       md5, req_to_sk(req), skb);\n\trcu_read_unlock();\n#endif\n\n\t/* Do not fool tcpdump (if any), clean our debris */\n\tskb->tstamp = 0;\n\treturn skb;\n}\nEXPORT_SYMBOL(tcp_make_synack);\n\nstatic void tcp_ca_dst_init(struct sock *sk, const struct dst_entry *dst)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct tcp_congestion_ops *ca;\n\tu32 ca_key = dst_metric(dst, RTAX_CC_ALGO);\n\n\tif (ca_key == TCP_CA_UNSPEC)\n\t\treturn;\n\n\trcu_read_lock();\n\tca = tcp_ca_find_key(ca_key);\n\tif (likely(ca && try_module_get(ca->owner))) {\n\t\tmodule_put(icsk->icsk_ca_ops->owner);\n\t\ticsk->icsk_ca_dst_locked = tcp_ca_dst_locked(dst);\n\t\ticsk->icsk_ca_ops = ca;\n\t}\n\trcu_read_unlock();\n}\n\n/* Do all connect socket setups that can be done AF independent. */\nstatic void tcp_connect_init(struct sock *sk)\n{\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u8 rcv_wscale;\n\tu32 rcv_wnd;\n\n\t/* We'll fix this up when we get a response from the other end.\n\t * See tcp_input.c:tcp_rcv_state_process case TCP_SYN_SENT.\n\t */\n\ttp->tcp_header_len = sizeof(struct tcphdr);\n\tif (sock_net(sk)->ipv4.sysctl_tcp_timestamps)\n\t\ttp->tcp_header_len += TCPOLEN_TSTAMP_ALIGNED;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (tp->af_specific->md5_lookup(sk, sk))\n\t\ttp->tcp_header_len += TCPOLEN_MD5SIG_ALIGNED;\n#endif\n\n\t/* If user gave his TCP_MAXSEG, record it to clamp */\n\tif (tp->rx_opt.user_mss)\n\t\ttp->rx_opt.mss_clamp = tp->rx_opt.user_mss;\n\ttp->max_window = 0;\n\ttcp_mtup_init(sk);\n\ttcp_sync_mss(sk, dst_mtu(dst));\n\n\ttcp_ca_dst_init(sk, dst);\n\n\tif (!tp->window_clamp)\n\t\ttp->window_clamp = dst_metric(dst, RTAX_WINDOW);\n\ttp->advmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\n\n\ttcp_initialize_rcv_mss(sk);\n\n\t/* limit the window selection if the user enforce a smaller rx buffer */\n\tif (sk->sk_userlocks & SOCK_RCVBUF_LOCK &&\n\t    (tp->window_clamp > tcp_full_space(sk) || tp->window_clamp == 0))\n\t\ttp->window_clamp = tcp_full_space(sk);\n\n\trcv_wnd = tcp_rwnd_init_bpf(sk);\n\tif (rcv_wnd == 0)\n\t\trcv_wnd = dst_metric(dst, RTAX_INITRWND);\n\n\ttcp_select_initial_window(sk, tcp_full_space(sk),\n\t\t\t\t  tp->advmss - (tp->rx_opt.ts_recent_stamp ? tp->tcp_header_len - sizeof(struct tcphdr) : 0),\n\t\t\t\t  &tp->rcv_wnd,\n\t\t\t\t  &tp->window_clamp,\n\t\t\t\t  sock_net(sk)->ipv4.sysctl_tcp_window_scaling,\n\t\t\t\t  &rcv_wscale,\n\t\t\t\t  rcv_wnd);\n\n\ttp->rx_opt.rcv_wscale = rcv_wscale;\n\ttp->rcv_ssthresh = tp->rcv_wnd;\n\n\tsk->sk_err = 0;\n\tsock_reset_flag(sk, SOCK_DONE);\n\ttp->snd_wnd = 0;\n\ttcp_init_wl(tp, 0);\n\ttcp_write_queue_purge(sk);\n\ttp->snd_una = tp->write_seq;\n\ttp->snd_sml = tp->write_seq;\n\ttp->snd_up = tp->write_seq;\n\ttp->snd_nxt = tp->write_seq;\n\n\tif (likely(!tp->repair))\n\t\ttp->rcv_nxt = 0;\n\telse\n\t\ttp->rcv_tstamp = tcp_jiffies32;\n\ttp->rcv_wup = tp->rcv_nxt;\n\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\n\tinet_csk(sk)->icsk_rto = tcp_timeout_init(sk);\n\tinet_csk(sk)->icsk_retransmits = 0;\n\ttcp_clear_retrans(tp);\n}\n\nstatic void tcp_connect_queue_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\ttcb->end_seq += skb->len;\n\t__skb_header_release(skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n\tWRITE_ONCE(tp->write_seq, tcb->end_seq);\n\ttp->packets_out += tcp_skb_pcount(skb);\n}\n\n/* Build and send a SYN with data and (cached) Fast Open cookie. However,\n * queue a data-only packet after the regular SYN, such that regular SYNs\n * are retransmitted on timeouts. Also if the remote SYN-ACK acknowledges\n * only the SYN sequence, the data are retransmitted in the first ACK.\n * If cookie is not cached or other error occurs, falls back to send a\n * regular SYN with Fast Open cookie request option.\n */\nstatic int tcp_send_syn_data(struct sock *sk, struct sk_buff *syn)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_fastopen_request *fo = tp->fastopen_req;\n\tint space, err = 0;\n\tstruct sk_buff *syn_data;\n\n\ttp->rx_opt.mss_clamp = tp->advmss;  /* If MSS is not cached */\n\tif (!tcp_fastopen_cookie_check(sk, &tp->rx_opt.mss_clamp, &fo->cookie))\n\t\tgoto fallback;\n\n\t/* MSS for SYN-data is based on cached MSS and bounded by PMTU and\n\t * user-MSS. Reserve maximum option space for middleboxes that add\n\t * private TCP options. The cost is reduced data space in SYN :(\n\t */\n\ttp->rx_opt.mss_clamp = tcp_mss_clamp(tp, tp->rx_opt.mss_clamp);\n\n\tspace = __tcp_mtu_to_mss(sk, inet_csk(sk)->icsk_pmtu_cookie) -\n\t\tMAX_TCP_OPTION_SPACE;\n\n\tspace = min_t(size_t, space, fo->size);\n\n\t/* limit to order-0 allocations */\n\tspace = min_t(size_t, space, SKB_MAX_HEAD(MAX_TCP_HEADER));\n\n\tsyn_data = sk_stream_alloc_skb(sk, space, sk->sk_allocation, false);\n\tif (!syn_data)\n\t\tgoto fallback;\n\tsyn_data->ip_summed = CHECKSUM_PARTIAL;\n\tmemcpy(syn_data->cb, syn->cb, sizeof(syn->cb));\n\tif (space) {\n\t\tint copied = copy_from_iter(skb_put(syn_data, space), space,\n\t\t\t\t\t    &fo->data->msg_iter);\n\t\tif (unlikely(!copied)) {\n\t\t\ttcp_skb_tsorted_anchor_cleanup(syn_data);\n\t\t\tkfree_skb(syn_data);\n\t\t\tgoto fallback;\n\t\t}\n\t\tif (copied != space) {\n\t\t\tskb_trim(syn_data, copied);\n\t\t\tspace = copied;\n\t\t}\n\t}\n\t/* No more data pending in inet_wait_for_connect() */\n\tif (space == fo->size)\n\t\tfo->data = NULL;\n\tfo->copied = space;\n\n\ttcp_connect_queue_skb(sk, syn_data);\n\tif (syn_data->len)\n\t\ttcp_chrono_start(sk, TCP_CHRONO_BUSY);\n\n\terr = tcp_transmit_skb(sk, syn_data, 1, sk->sk_allocation);\n\n\tsyn->skb_mstamp = syn_data->skb_mstamp;\n\n\t/* Now full SYN+DATA was cloned and sent (or not),\n\t * remove the SYN from the original skb (syn_data)\n\t * we keep in write queue in case of a retransmit, as we\n\t * also have the SYN packet (with no data) in the same queue.\n\t */\n\tTCP_SKB_CB(syn_data)->seq++;\n\tTCP_SKB_CB(syn_data)->tcp_flags = TCPHDR_ACK | TCPHDR_PSH;\n\tif (!err) {\n\t\ttp->syn_data = (fo->copied > 0);\n\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, syn_data);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPORIGDATASENT);\n\t\tgoto done;\n\t}\n\n\t/* data was not sent, put it in write_queue */\n\t__skb_queue_tail(&sk->sk_write_queue, syn_data);\n\ttp->packets_out -= tcp_skb_pcount(syn_data);\n\nfallback:\n\t/* Send a regular SYN with Fast Open cookie request option */\n\tif (fo->cookie.len > 0)\n\t\tfo->cookie.len = 0;\n\terr = tcp_transmit_skb(sk, syn, 1, sk->sk_allocation);\n\tif (err)\n\t\ttp->syn_fastopen = 0;\ndone:\n\tfo->cookie.len = -1;  /* Exclude Fast Open option for SYN retries */\n\treturn err;\n}\n\n/* Build a SYN and send it off. */\nint tcp_connect(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint err;\n\n\ttcp_call_bpf(sk, BPF_SOCK_OPS_TCP_CONNECT_CB, 0, NULL);\n\n\tif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\n\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\n\n\ttcp_connect_init(sk);\n\n\tif (unlikely(tp->repair)) {\n\t\ttcp_finish_connect(sk, NULL);\n\t\treturn 0;\n\t}\n\n\tbuff = sk_stream_alloc_skb(sk, 0, sk->sk_allocation, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOBUFS;\n\n\ttcp_init_nondata_skb(buff, tp->write_seq++, TCPHDR_SYN);\n\ttcp_mstamp_refresh(tp);\n\ttp->retrans_stamp = tcp_time_stamp(tp);\n\ttcp_connect_queue_skb(sk, buff);\n\ttcp_ecn_send_syn(sk, buff);\n\ttcp_rbtree_insert(&sk->tcp_rtx_queue, buff);\n\n\t/* Send off SYN; include data in Fast Open. */\n\terr = tp->fastopen_req ? tcp_send_syn_data(sk, buff) :\n\t      tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);\n\tif (err == -ECONNREFUSED)\n\t\treturn err;\n\n\t/* We change tp->snd_nxt after the tcp_transmit_skb() call\n\t * in order to make this packet get counted in tcpOutSegs.\n\t */\n\ttp->snd_nxt = tp->write_seq;\n\ttp->pushed_seq = tp->write_seq;\n\tbuff = tcp_send_head(sk);\n\tif (unlikely(buff)) {\n\t\ttp->snd_nxt\t= TCP_SKB_CB(buff)->seq;\n\t\ttp->pushed_seq\t= TCP_SKB_CB(buff)->seq;\n\t}\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ACTIVEOPENS);\n\n\t/* Timer for repeating the SYN until an answer. */\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_connect);\n\n/* Send out a delayed ack, the caller does the policy checking\n * to see if we should even be here.  See tcp_input.c:tcp_ack_snd_check()\n * for details.\n */\nvoid tcp_send_delayed_ack(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint ato = icsk->icsk_ack.ato;\n\tunsigned long timeout;\n\n\tif (ato > TCP_DELACK_MIN) {\n\t\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t\tint max_ato = HZ / 2;\n\n\t\tif (icsk->icsk_ack.pingpong ||\n\t\t    (icsk->icsk_ack.pending & ICSK_ACK_PUSHED))\n\t\t\tmax_ato = TCP_DELACK_MAX;\n\n\t\t/* Slow path, intersegment interval is \"high\". */\n\n\t\t/* If some rtt estimate is known, use it to bound delayed ack.\n\t\t * Do not use inet_csk(sk)->icsk_rto here, use results of rtt measurements\n\t\t * directly.\n\t\t */\n\t\tif (tp->srtt_us) {\n\t\t\tint rtt = max_t(int, usecs_to_jiffies(tp->srtt_us >> 3),\n\t\t\t\t\tTCP_DELACK_MIN);\n\n\t\t\tif (rtt < max_ato)\n\t\t\t\tmax_ato = rtt;\n\t\t}\n\n\t\tato = min(ato, max_ato);\n\t}\n\n\t/* Stay within the limit we were given */\n\ttimeout = jiffies + ato;\n\n\t/* Use new timeout only if there wasn't a older one earlier. */\n\tif (icsk->icsk_ack.pending & ICSK_ACK_TIMER) {\n\t\t/* If delack timer was blocked or is about to expire,\n\t\t * send ACK now.\n\t\t */\n\t\tif (icsk->icsk_ack.blocked ||\n\t\t    time_before_eq(icsk->icsk_ack.timeout, jiffies + (ato >> 2))) {\n\t\t\ttcp_send_ack(sk);\n\t\t\treturn;\n\t\t}\n\n\t\tif (!time_before(timeout, icsk->icsk_ack.timeout))\n\t\t\ttimeout = icsk->icsk_ack.timeout;\n\t}\n\ticsk->icsk_ack.pending |= ICSK_ACK_SCHED | ICSK_ACK_TIMER;\n\ticsk->icsk_ack.timeout = timeout;\n\tsk_reset_timer(sk, &icsk->icsk_delack_timer, timeout);\n}\n\n/* This routine sends an ack and also updates the window. */\nvoid __tcp_send_ack(struct sock *sk, u32 rcv_nxt)\n{\n\tstruct sk_buff *buff;\n\n\t/* If we have been reset, we may not send again. */\n\tif (sk->sk_state == TCP_CLOSE)\n\t\treturn;\n\n\t/* We are not putting this on the write queue, so\n\t * tcp_transmit_skb() will set the ownership to this\n\t * sock.\n\t */\n\tbuff = alloc_skb(MAX_TCP_HEADER,\n\t\t\t sk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\n\tif (unlikely(!buff)) {\n\t\tinet_csk_schedule_ack(sk);\n\t\tinet_csk(sk)->icsk_ack.ato = TCP_ATO_MIN;\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,\n\t\t\t\t\t  TCP_DELACK_MAX, TCP_RTO_MAX);\n\t\treturn;\n\t}\n\n\t/* Reserve space for headers and prepare control bits. */\n\tskb_reserve(buff, MAX_TCP_HEADER);\n\ttcp_init_nondata_skb(buff, tcp_acceptable_seq(sk), TCPHDR_ACK);\n\n\t/* We do not want pure acks influencing TCP Small Queues or fq/pacing\n\t * too much.\n\t * SKB_TRUESIZE(max(1 .. 66, MAX_TCP_HEADER)) is unfortunately ~784\n\t */\n\tskb_set_tcp_pure_ack(buff);\n\n\t/* Send it off, this clears delayed acks for us. */\n\t__tcp_transmit_skb(sk, buff, 0, (__force gfp_t)0, rcv_nxt);\n}\nEXPORT_SYMBOL_GPL(__tcp_send_ack);\n\nvoid tcp_send_ack(struct sock *sk)\n{\n\t__tcp_send_ack(sk, tcp_sk(sk)->rcv_nxt);\n}\n\n/* This routine sends a packet with an out of date sequence\n * number. It assumes the other end will try to ack it.\n *\n * Question: what should we make while urgent mode?\n * 4.4BSD forces sending single byte of data. We cannot send\n * out of window data, because we have SND.NXT==SND.MAX...\n *\n * Current solution: to send TWO zero-length segments in urgent mode:\n * one is with SEG.SEQ=SND.UNA to deliver urgent pointer, another is\n * out-of-date with SND.UNA-1 to probe window.\n */\nstatic int tcp_xmit_probe_skb(struct sock *sk, int urgent, int mib)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\t/* We don't queue it, tcp_transmit_skb() sets ownership. */\n\tskb = alloc_skb(MAX_TCP_HEADER,\n\t\t\tsk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\n\tif (!skb)\n\t\treturn -1;\n\n\t/* Reserve space for headers and set control bits. */\n\tskb_reserve(skb, MAX_TCP_HEADER);\n\t/* Use a previous sequence.  This should cause the other\n\t * end to send an ack.  Don't queue or clone SKB, just\n\t * send it.\n\t */\n\ttcp_init_nondata_skb(skb, tp->snd_una - !urgent, TCPHDR_ACK);\n\tNET_INC_STATS(sock_net(sk), mib);\n\treturn tcp_transmit_skb(sk, skb, 0, (__force gfp_t)0);\n}\n\n/* Called from setsockopt( ... TCP_REPAIR ) */\nvoid tcp_send_window_probe(struct sock *sk)\n{\n\tif (sk->sk_state == TCP_ESTABLISHED) {\n\t\ttcp_sk(sk)->snd_wl1 = tcp_sk(sk)->rcv_nxt - 1;\n\t\ttcp_mstamp_refresh(tcp_sk(sk));\n\t\ttcp_xmit_probe_skb(sk, 0, LINUX_MIB_TCPWINPROBE);\n\t}\n}\n\n/* Initiate keepalive or window probe from timer. */\nint tcp_write_wakeup(struct sock *sk, int mib)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\treturn -1;\n\n\tskb = tcp_send_head(sk);\n\tif (skb && before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp))) {\n\t\tint err;\n\t\tunsigned int mss = tcp_current_mss(sk);\n\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t\tif (before(tp->pushed_seq, TCP_SKB_CB(skb)->end_seq))\n\t\t\ttp->pushed_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t/* We are probing the opening of a window\n\t\t * but the window size is != 0\n\t\t * must have been a result SWS avoidance ( sender )\n\t\t */\n\t\tif (seg_size < TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq ||\n\t\t    skb->len > mss) {\n\t\t\tseg_size = min(seg_size, mss);\n\t\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\n\t\t\t\treturn -1;\n\t\t} else if (!tcp_skb_pcount(skb))\n\t\t\ttcp_set_skb_tso_segs(skb, mss);\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n\t\tif (!err)\n\t\t\ttcp_event_new_data_sent(sk, skb);\n\t\treturn err;\n\t} else {\n\t\tif (between(tp->snd_up, tp->snd_una + 1, tp->snd_una + 0xFFFF))\n\t\t\ttcp_xmit_probe_skb(sk, 1, mib);\n\t\treturn tcp_xmit_probe_skb(sk, 0, mib);\n\t}\n}\n\n/* A window probe timeout has occurred.  If window is not closed send\n * a partial packet else a zero probe.\n */\nvoid tcp_send_probe0(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tunsigned long probe_max;\n\tint err;\n\n\terr = tcp_write_wakeup(sk, LINUX_MIB_TCPWINPROBE);\n\n\tif (tp->packets_out || tcp_write_queue_empty(sk)) {\n\t\t/* Cancel probe timer, if it is not required. */\n\t\ticsk->icsk_probes_out = 0;\n\t\ticsk->icsk_backoff = 0;\n\t\treturn;\n\t}\n\n\tif (err <= 0) {\n\t\tif (icsk->icsk_backoff < net->ipv4.sysctl_tcp_retries2)\n\t\t\ticsk->icsk_backoff++;\n\t\ticsk->icsk_probes_out++;\n\t\tprobe_max = TCP_RTO_MAX;\n\t} else {\n\t\t/* If packet was not sent due to local congestion,\n\t\t * do not backoff and do not remember icsk_probes_out.\n\t\t * Let local senders to fight for local resources.\n\t\t *\n\t\t * Use accumulated backoff yet.\n\t\t */\n\t\tif (!icsk->icsk_probes_out)\n\t\t\ticsk->icsk_probes_out = 1;\n\t\tprobe_max = TCP_RESOURCE_PROBE_INTERVAL;\n\t}\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\n\t\t\t\t  tcp_probe0_when(sk, probe_max),\n\t\t\t\t  TCP_RTO_MAX);\n}\n\nint tcp_rtx_synack(const struct sock *sk, struct request_sock *req)\n{\n\tconst struct tcp_request_sock_ops *af_ops = tcp_rsk(req)->af_specific;\n\tstruct flowi fl;\n\tint res;\n\n\ttcp_rsk(req)->txhash = net_tx_rndhash();\n\tres = af_ops->send_synack(sk, NULL, &fl, req, NULL, TCP_SYNACK_NORMAL);\n\tif (!res) {\n\t\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\n\t\tif (unlikely(tcp_passive_fastopen(sk)))\n\t\t\ttcp_sk(sk)->total_retrans++;\n\t\ttrace_tcp_retransmit_synack(sk, req);\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL(tcp_rtx_synack);\n"], "filenames": ["net/ipv4/tcp_output.c"], "buggy_code_start_loc": [970], "buggy_code_end_loc": [2201], "fixing_code_start_loc": [971], "fixing_code_end_loc": [2215], "type": "NVD-CWE-Other", "message": "An issue was discovered in the Linux Kernel from 4.18 to 4.19, an improper update of sock reference in TCP pacing can lead to memory/netns leak, which can be used by remote clients.", "other": {"cve": {"id": "CVE-2022-1678", "sourceIdentifier": "security@openanolis.org", "published": "2022-05-25T15:15:07.887", "lastModified": "2022-10-27T16:55:25.710", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux Kernel from 4.18 to 4.19, an improper update of sock reference in TCP pacing can lead to memory/netns leak, which can be used by remote clients."}, {"lang": "es", "value": "Se ha detectado un problema en el Kernel de Linux de la 4.18 a 4.19, una actualizaci\u00f3n inapropiada de la referencia sock en el paso TCP puede conllevar a una p\u00e9rdida de memoria/netns, que puede ser usada por clientes remotos"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security@openanolis.org", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}, {"source": "security@openanolis.org", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-911"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.18", "versionEndIncluding": "4.19", "matchCriteriaId": "B2A2E0DA-9949-495C-AC30-A21F364D71E1"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:active_iq_unified_manager:-:*:*:*:*:vmware_vsphere:*:*", "matchCriteriaId": "3A756737-1CC4-42C2-A4DF-E1C893B4E2D5"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:cloud_volumes_ontap_mediator:-:*:*:*:*:*:*:*", "matchCriteriaId": "280AA828-6FA9-4260-8EC1-019423B966E1"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:e-series_santricity_os_controller:*:*:*:*:*:*:*:*", "versionStartIncluding": "11.0", "versionEndIncluding": "11.70.2", "matchCriteriaId": "A0DA944C-4992-424D-BC82-474585DAC5DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:element_software:-:*:*:*:*:*:*:*", "matchCriteriaId": "85DF4B3F-4BBC-42B7-B729-096934523D63"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:hci_management_node:-:*:*:*:*:*:*:*", "matchCriteriaId": "A3C19813-E823-456A-B1CE-EC0684CE1953"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:solidfire:-:*:*:*:*:*:*:*", "matchCriteriaId": "A6E9EF0C-AFA8-4F7B-9FDC-1E0F7C26E737"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:storagegrid:-:*:*:*:*:*:*:*", "matchCriteriaId": "8ADFF451-740F-4DBA-BD23-3881945D3E40"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:bootstrap_os:-:*:*:*:*:*:*:*", "matchCriteriaId": "95BA156C-C977-4F0C-8DFB-3FAE9CC8C02D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:hci_compute_node:-:*:*:*:*:*:*:*", "matchCriteriaId": "AD7447BC-F315-4298-A822-549942FC118B"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "6770B6C3-732E-4E22-BF1C-2D2FD610061C"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "9F9C8C20-42EB-4AB5-BD97-212DEB070C43"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "7FFF7106-ED78-49BA-9EC5-B889E3685D53"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "E63D8B0F-006E-4801-BF9D-1C001BBFB4F9"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "56409CEC-5A1E-4450-AA42-641E459CC2AF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "B06F4839-D16A-4A61-9BB5-55B13F41E47F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "108A2215-50FB-4074-94CF-C130FA14566D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300e:-:*:*:*:*:*:*:*", "matchCriteriaId": "7AFC73CE-ABB9-42D3-9A71-3F5BC5381E0E"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "32F0B6C0-F930-480D-962B-3F4EFDCC13C7"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500e:-:*:*:*:*:*:*:*", "matchCriteriaId": "803BC414-B250-4E3A-A478-A3881340D6B8"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "0FEB3337-BFDE-462A-908B-176F92053CEC"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700e:-:*:*:*:*:*:*:*", "matchCriteriaId": "736AEAE9-782B-4F71-9893-DED53367E102"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0B4AD8A-F172-4558-AEC6-FF424BA2D912"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "8497A4C9-8474-4A62-8331-3FE862ED4098"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "234DEFE0-5CE5-4B0A-96B8-5D227CB8ED31"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "CDDF61B7-EC5C-467C-B710-B89F502CD04F"}]}]}], "references": [{"url": "https://anas.openanolis.cn/cves/detail/CVE-2022-1678", "source": "security@openanolis.org", "tags": ["Third Party Advisory"]}, {"url": "https://anas.openanolis.cn/errata/detail/ANSA-2022:0143", "source": "security@openanolis.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.openanolis.cn/show_bug.cgi?id=61", "source": "security@openanolis.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://gitee.com/anolis/cloud-kernel/commit/bed537da691b", "source": "security@openanolis.org", "tags": ["Permissions Required"]}, {"url": "https://github.com/torvalds/linux/commit/0a70f118475e037732557796accd0878a00fc25a", "source": "security@openanolis.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lore.kernel.org/all/20200602080425.93712-1-kerneljasonxing@gmail.com/", "source": "security@openanolis.org", "tags": ["Exploit", "Patch", "Vendor Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20220715-0001/", "source": "security@openanolis.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0a70f118475e037732557796accd0878a00fc25a"}}