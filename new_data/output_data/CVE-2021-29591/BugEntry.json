{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <stddef.h>\n\n#include <cstring>\n#include <vector>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/context_util.h\"\n#include \"tensorflow/lite/core/subgraph.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace while_kernel {\n\nnamespace {\n\n// Propagate tensor shapes and types from `src_tensor_indices` in `src_subgraph`\n// to `dst_tensor_indices` in `dst_subgraph`.\n//\n// When `resize_subgraph_inputs` is true, the function calls subgraphs's\n// `ResizeInputTensor` function, and it may trigger the memory planner to\n// reallocate memory.\n// When `resize_subgraph_inputs` is false, it implies `context` belongs to\n// `dst_subgraph`. The function calls `context->ResizeTensor`. This happens\n// when resizing `While` op's outputs.\ntemplate <typename SrcVector, typename DstVector>\nTfLiteStatus CopyTensorsShapeAndType(TfLiteContext* context,\n                                     Subgraph* src_subgraph,\n                                     const SrcVector& src_tensor_indices,\n                                     Subgraph* dst_subgraph,\n                                     const DstVector& dst_tensor_indices,\n                                     bool resize_subgraph_inputs) {\n  TF_LITE_ENSURE_EQ(context, src_tensor_indices.size(),\n                    dst_tensor_indices.size());\n  for (int i = 0; i < src_tensor_indices.size(); ++i) {\n    const TfLiteTensor* src_tensor =\n        src_subgraph->tensor(src_tensor_indices[i]);\n\n    TfLiteTensor* dst_tensor = dst_subgraph->tensor(dst_tensor_indices[i]);\n    if (resize_subgraph_inputs) {\n      std::vector<int> dims(src_tensor->dims->data,\n                            src_tensor->dims->data + src_tensor->dims->size);\n      dst_subgraph->ResizeInputTensor(dst_tensor_indices[i], dims);\n    } else {\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, dst_tensor,\n                                         TfLiteIntArrayCopy(src_tensor->dims)));\n    }\n    dst_tensor->type = src_tensor->type;\n  }\n  return kTfLiteOk;\n}\n\n// Copy the tensors data from tensors `src_tensor_indices` in `src_subgraph`\n// to `dst_tensor_indices` in `dst_subgraph`.\ntemplate <typename SrcVector, typename DstVector>\nTfLiteStatus CopyTensorsData(TfLiteContext* context, Subgraph* src_subgraph,\n                             const SrcVector& src_tensor_indices,\n                             Subgraph* dst_subgraph,\n                             const DstVector& dst_tensor_indices) {\n  TF_LITE_ENSURE_EQ(context, src_tensor_indices.size(),\n                    dst_tensor_indices.size());\n  for (int i = 0; i < src_tensor_indices.size(); ++i) {\n    const TfLiteTensor* src_tensor =\n        src_subgraph->tensor(src_tensor_indices[i]);\n    TfLiteTensor* dst_tensor = dst_subgraph->tensor(dst_tensor_indices[i]);\n    if (IsDynamicTensor(dst_tensor)) {\n      TfLiteTensorRealloc(src_tensor->bytes, dst_tensor);\n    }\n    TF_LITE_ENSURE_EQ(context, src_tensor->bytes, dst_tensor->bytes);\n    memcpy(dst_tensor->data.raw, src_tensor->data.raw, src_tensor->bytes);\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus CheckCondOutput(TfLiteContext* context,\n                             const TfLiteTensor* cond_output) {\n  // The condition output must be a single boolean value.\n  TF_LITE_ENSURE_TYPES_EQ(context, cond_output->type, kTfLiteBool);\n  if (cond_output->dims->size == 0) {\n    // It's okay if it's a 0D scalar.\n    return kTfLiteOk;\n  }\n  // Otherwise it must be 1D with shape [1].\n  TF_LITE_ENSURE_EQ(context, cond_output->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, cond_output->dims->data[0], 1);\n  return kTfLiteOk;\n}\n\n}  // namespace\n\nstruct OpData {\n  int cond_subgraph_index;\n  int body_subgraph_index;\n  bool cond_has_dynamic_output_tensors;\n  bool body_has_dynamic_output_tensors;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  auto* op_data = new OpData;\n  const auto* params = reinterpret_cast<const TfLiteWhileParams*>(buffer);\n  op_data->cond_subgraph_index = params->cond_subgraph_index;\n  op_data->body_subgraph_index = params->body_subgraph_index;\n  op_data->cond_has_dynamic_output_tensors = false;\n  op_data->body_has_dynamic_output_tensors = false;\n  return op_data;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  int num_inputs = node->inputs->size;\n  // The number of outputs should be the same as number of inputs.\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, num_inputs);\n\n  // Check subgraph indices and get subgraphs.\n  Subgraph* this_subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n  auto* subgraphs = this_subgraph->GetSubgraphs();\n  TF_LITE_ENSURE(context, op_data->cond_subgraph_index < subgraphs->size());\n  TF_LITE_ENSURE(context, op_data->body_subgraph_index < subgraphs->size());\n\n  Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();\n  Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();\n\n  // Check input & output count of the condition subgraph.\n  TF_LITE_ENSURE_EQ(context, cond_subgraph->inputs().size(), num_inputs);\n  TF_LITE_ENSURE_EQ(context, cond_subgraph->outputs().size(), 1);\n\n  // Check input & output count of the body subgraph.\n  TF_LITE_ENSURE_EQ(context, body_subgraph->inputs().size(), num_inputs);\n  TF_LITE_ENSURE_EQ(context, body_subgraph->outputs().size(), num_inputs);\n\n  // Prepare and check the condition subgraph.\n  TF_LITE_ENSURE_OK(\n      context, CopyTensorsShapeAndType(\n                   context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                   cond_subgraph, cond_subgraph->inputs(), true));\n  TF_LITE_ENSURE_OK(context, cond_subgraph->AllocateTensors());\n  TfLiteTensor* cond_output =\n      cond_subgraph->tensor(cond_subgraph->outputs()[0]);\n  // This should rarely happens. In most cases the output is static with shape\n  // [1]. However theoretically intermediate tensors in the cond subgraph\n  // can be dynamic.\n  if (IsDynamicTensor(cond_output)) {\n    op_data->cond_has_dynamic_output_tensors = true;\n  } else {\n    TF_LITE_ENSURE_STATUS(CheckCondOutput(context, cond_output));\n  }\n\n  // Prepare and check the body subgraph.\n  TF_LITE_ENSURE_OK(\n      context, CopyTensorsShapeAndType(\n                   context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                   body_subgraph, body_subgraph->inputs(), true));\n  TF_LITE_ENSURE_OK(context, body_subgraph->AllocateTensors());\n  if (body_subgraph->HasDynamicTensors()) {\n    op_data->body_has_dynamic_output_tensors = true;\n  } else {\n    for (int i = 0; i < num_inputs; ++i) {\n      TfLiteTensor* body_input =\n          body_subgraph->tensor(body_subgraph->inputs()[i]);\n      TfLiteTensor* body_output =\n          body_subgraph->tensor(body_subgraph->outputs()[i]);\n      TF_LITE_ENSURE_TYPES_EQ(context, body_input->type, body_output->type);\n\n      TF_LITE_ENSURE(context, !IsDynamicTensor(body_output));\n      if (!TfLiteIntArrayEqual(body_input->dims, body_output->dims)) {\n        // If the output shape of the body subgraph is static w.r.t. a fixed\n        // input size, but it's different from input size, it's still considered\n        // dynamic. For example: If a subgraph keeps padding its input with a\n        // fixed padding, the output shape is static w.r.t the input shape and\n        // padding, but running it in a loop will keep bloating the tensor.\n        op_data->body_has_dynamic_output_tensors = true;\n        break;\n      }\n    }\n  }\n  for (int i = 0; i < num_inputs; ++i) {\n    TfLiteTensor* output;\n    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &output));\n    if (op_data->body_has_dynamic_output_tensors) {\n      SetTensorToDynamic(output);\n    } else {\n      TfLiteTensor* body_output =\n          body_subgraph->tensor(body_subgraph->outputs()[i]);\n      TfLiteIntArray* output_size = TfLiteIntArrayCopy(body_output->dims);\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, output, output_size));\n    }\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  Subgraph* this_subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n  auto* subgraphs = this_subgraph->GetSubgraphs();\n  Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();\n  Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();\n\n  // The follow graph illustrates the current implementation.\n  //\n  // This Subgraph          Cond Subgraph         Body Subgraph\n  // +-----------+   (1)   +------------+   (3)   +------------+\n  // |   WHILE   |-------->|  SUBGRAPH  |-------->|  SUBGRAPH  |\n  // |   INPUT   |        /|   INPUT    |<-----   |   INPUT    |\n  // +-----------+       / +------------+      \\  +------------+\n  //                    /        |              \\       |\n  //               (6) /         | (2)       (5) \\      | (4)\n  //                  /          v                \\     v\n  // +-----------+   /     +------------+         +------------+\n  // |   WHILE   |<--      |  SUBGRAPH  |         |  SUBGRAPH  |\n  // |   OUTPUT  |         |   OUTPUT   |         |   OUTPUT   |\n  // +-----------+         +------------+         +------------+\n  //\n  // (1) Copy the inputs of WHILE op to the inputs of condition subgraph.\n  // (2) Invoke condition subgraph.\n  //     Jump to step 5 if result is false.\n  // (3) Copy the inputs of condition subgraph to the inputs of body subgraph.\n  // (4) Invoke body subgraph.\n  // (5) Copy the outputs of body subgraph to the inputs condition subgraph.\n  //     Jump back to step 2!\n  // (6) Copy the inputs of condition subgraph to the outputs of WHILE op.\n  //\n  // If the body subgraph has dynamic sized outputs, it's required to resize the\n  // tensor before copying in step 1, 3, 4 and 6.\n  //\n  // Note the flow is carefully designed to handle the dynamic sized output\n  // case. The loop invariant is: The newest value is in the inputs of condition\n  // subgraph. This is always true before step 2.\n  //\n  // This is the best we can do without sharing tensor buffer across subgraph\n  // boundary. Currently we copy the input / output between the subgraphs. This\n  // isn't optimized yet and a lot of redundant copies are made.\n  // TODO(b/120234921): Optimize and avoid copying tensors between subgraphs.\n\n  if (op_data->body_has_dynamic_output_tensors) {\n    // If body subgraph has dynamic outputs, the input of condition subgraph may\n    // be changed in the last invocation and may need resizing.\n    TF_LITE_ENSURE_OK(\n        context, CopyTensorsShapeAndType(\n                     context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                     cond_subgraph, cond_subgraph->inputs(), true));\n    TF_LITE_ENSURE_OK(context, cond_subgraph->AllocateTensors());\n  }\n  TF_LITE_ENSURE_OK(\n      context,\n      CopyTensorsData(context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                      cond_subgraph, cond_subgraph->inputs()));\n\n  while (true) {\n    TF_LITE_ENSURE_OK(context, cond_subgraph->Invoke());\n    int cond_subgraph_output_index = cond_subgraph->outputs()[0];\n    cond_subgraph->EnsureTensorDataIsReadable(cond_subgraph_output_index);\n    TfLiteTensor* cond_output =\n        cond_subgraph->tensor(cond_subgraph_output_index);\n    if (op_data->cond_has_dynamic_output_tensors) {\n      TF_LITE_ENSURE_STATUS(CheckCondOutput(context, cond_output));\n    }\n\n    if (!cond_output->data.b[0]) {\n      break;\n    }\n    if (op_data->body_has_dynamic_output_tensors) {\n      TF_LITE_ENSURE_OK(context,\n                        CopyTensorsShapeAndType(\n                            context, cond_subgraph, cond_subgraph->inputs(),\n                            body_subgraph, body_subgraph->inputs(), true));\n      TF_LITE_ENSURE_OK(context, body_subgraph->AllocateTensors());\n    }\n\n    TF_LITE_ENSURE_OK(\n        context,\n        CopyTensorsData(context, cond_subgraph, cond_subgraph->inputs(),\n                        body_subgraph, body_subgraph->inputs()));\n\n    TF_LITE_ENSURE_OK(context, body_subgraph->Invoke());\n\n    for (int tensor_index : body_subgraph->outputs()) {\n      body_subgraph->EnsureTensorDataIsReadable(tensor_index);\n    }\n\n    if (op_data->body_has_dynamic_output_tensors) {\n      TF_LITE_ENSURE_OK(context,\n                        CopyTensorsShapeAndType(\n                            context, body_subgraph, body_subgraph->outputs(),\n                            cond_subgraph, cond_subgraph->inputs(), true));\n      TF_LITE_ENSURE_OK(context, cond_subgraph->AllocateTensors());\n    }\n\n    TF_LITE_ENSURE_OK(\n        context,\n        CopyTensorsData(context, body_subgraph, body_subgraph->outputs(),\n                        cond_subgraph, cond_subgraph->inputs()));\n  }\n\n  // Note that copying from body's output will fail if body is never invoked.\n  // TODO(b/120234921): Optimize and avoid copying tensors between subgraphs.\n  if (op_data->body_has_dynamic_output_tensors) {\n    TF_LITE_ENSURE_OK(\n        context, CopyTensorsShapeAndType(\n                     context, cond_subgraph, cond_subgraph->inputs(),\n                     this_subgraph, TfLiteIntArrayView(node->outputs), false));\n  }\n\n  TF_LITE_ENSURE_OK(\n      context,\n      CopyTensorsData(context, cond_subgraph, cond_subgraph->inputs(),\n                      this_subgraph, TfLiteIntArrayView(node->outputs)));\n  return kTfLiteOk;\n}\n\n}  // namespace while_kernel\n\nTfLiteRegistration* Register_WHILE() {\n  static TfLiteRegistration r = {while_kernel::Init, while_kernel::Free,\n                                 while_kernel::Prepare, while_kernel::Eval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <stddef.h>\n\n#include <cstring>\n#include <vector>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/context_util.h\"\n#include \"tensorflow/lite/core/subgraph.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace while_kernel {\n\nnamespace {\n\n// Propagate tensor shapes and types from `src_tensor_indices` in `src_subgraph`\n// to `dst_tensor_indices` in `dst_subgraph`.\n//\n// When `resize_subgraph_inputs` is true, the function calls subgraphs's\n// `ResizeInputTensor` function, and it may trigger the memory planner to\n// reallocate memory.\n// When `resize_subgraph_inputs` is false, it implies `context` belongs to\n// `dst_subgraph`. The function calls `context->ResizeTensor`. This happens\n// when resizing `While` op's outputs.\ntemplate <typename SrcVector, typename DstVector>\nTfLiteStatus CopyTensorsShapeAndType(TfLiteContext* context,\n                                     Subgraph* src_subgraph,\n                                     const SrcVector& src_tensor_indices,\n                                     Subgraph* dst_subgraph,\n                                     const DstVector& dst_tensor_indices,\n                                     bool resize_subgraph_inputs) {\n  TF_LITE_ENSURE_EQ(context, src_tensor_indices.size(),\n                    dst_tensor_indices.size());\n  for (int i = 0; i < src_tensor_indices.size(); ++i) {\n    const TfLiteTensor* src_tensor =\n        src_subgraph->tensor(src_tensor_indices[i]);\n\n    TfLiteTensor* dst_tensor = dst_subgraph->tensor(dst_tensor_indices[i]);\n    if (resize_subgraph_inputs) {\n      std::vector<int> dims(src_tensor->dims->data,\n                            src_tensor->dims->data + src_tensor->dims->size);\n      dst_subgraph->ResizeInputTensor(dst_tensor_indices[i], dims);\n    } else {\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, dst_tensor,\n                                         TfLiteIntArrayCopy(src_tensor->dims)));\n    }\n    dst_tensor->type = src_tensor->type;\n  }\n  return kTfLiteOk;\n}\n\n// Copy the tensors data from tensors `src_tensor_indices` in `src_subgraph`\n// to `dst_tensor_indices` in `dst_subgraph`.\ntemplate <typename SrcVector, typename DstVector>\nTfLiteStatus CopyTensorsData(TfLiteContext* context, Subgraph* src_subgraph,\n                             const SrcVector& src_tensor_indices,\n                             Subgraph* dst_subgraph,\n                             const DstVector& dst_tensor_indices) {\n  TF_LITE_ENSURE_EQ(context, src_tensor_indices.size(),\n                    dst_tensor_indices.size());\n  for (int i = 0; i < src_tensor_indices.size(); ++i) {\n    const TfLiteTensor* src_tensor =\n        src_subgraph->tensor(src_tensor_indices[i]);\n    TfLiteTensor* dst_tensor = dst_subgraph->tensor(dst_tensor_indices[i]);\n    if (IsDynamicTensor(dst_tensor)) {\n      TfLiteTensorRealloc(src_tensor->bytes, dst_tensor);\n    }\n    TF_LITE_ENSURE_EQ(context, src_tensor->bytes, dst_tensor->bytes);\n    memcpy(dst_tensor->data.raw, src_tensor->data.raw, src_tensor->bytes);\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus CheckCondOutput(TfLiteContext* context,\n                             const TfLiteTensor* cond_output) {\n  // The condition output must be a single boolean value.\n  TF_LITE_ENSURE_TYPES_EQ(context, cond_output->type, kTfLiteBool);\n  if (cond_output->dims->size == 0) {\n    // It's okay if it's a 0D scalar.\n    return kTfLiteOk;\n  }\n  // Otherwise it must be 1D with shape [1].\n  TF_LITE_ENSURE_EQ(context, cond_output->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, cond_output->dims->data[0], 1);\n  return kTfLiteOk;\n}\n\n}  // namespace\n\nstruct OpData {\n  int cond_subgraph_index;\n  int body_subgraph_index;\n  bool cond_has_dynamic_output_tensors;\n  bool body_has_dynamic_output_tensors;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  auto* op_data = new OpData;\n  const auto* params = reinterpret_cast<const TfLiteWhileParams*>(buffer);\n  op_data->cond_subgraph_index = params->cond_subgraph_index;\n  op_data->body_subgraph_index = params->body_subgraph_index;\n  op_data->cond_has_dynamic_output_tensors = false;\n  op_data->body_has_dynamic_output_tensors = false;\n  return op_data;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  int num_inputs = node->inputs->size;\n  // The number of outputs should be the same as number of inputs.\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, num_inputs);\n\n  // Check subgraph indices and get subgraphs.\n  Subgraph* this_subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n  auto* subgraphs = this_subgraph->GetSubgraphs();\n  TF_LITE_ENSURE(context, op_data->cond_subgraph_index < subgraphs->size());\n  TF_LITE_ENSURE(context, op_data->body_subgraph_index < subgraphs->size());\n  TF_LITE_ENSURE(context,\n                 op_data->cond_subgraph_index != op_data->body_subgraph_index);\n\n  Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();\n  Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();\n\n  // Check input & output count of the condition subgraph.\n  TF_LITE_ENSURE_EQ(context, cond_subgraph->inputs().size(), num_inputs);\n  TF_LITE_ENSURE_EQ(context, cond_subgraph->outputs().size(), 1);\n\n  // Check input & output count of the body subgraph.\n  TF_LITE_ENSURE_EQ(context, body_subgraph->inputs().size(), num_inputs);\n  TF_LITE_ENSURE_EQ(context, body_subgraph->outputs().size(), num_inputs);\n\n  // Prepare and check the condition subgraph.\n  TF_LITE_ENSURE_OK(\n      context, CopyTensorsShapeAndType(\n                   context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                   cond_subgraph, cond_subgraph->inputs(), true));\n  TF_LITE_ENSURE_OK(context, cond_subgraph->AllocateTensors());\n  TfLiteTensor* cond_output =\n      cond_subgraph->tensor(cond_subgraph->outputs()[0]);\n  // This should rarely happens. In most cases the output is static with shape\n  // [1]. However theoretically intermediate tensors in the cond subgraph\n  // can be dynamic.\n  if (IsDynamicTensor(cond_output)) {\n    op_data->cond_has_dynamic_output_tensors = true;\n  } else {\n    TF_LITE_ENSURE_STATUS(CheckCondOutput(context, cond_output));\n  }\n\n  // Prepare and check the body subgraph.\n  TF_LITE_ENSURE_OK(\n      context, CopyTensorsShapeAndType(\n                   context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                   body_subgraph, body_subgraph->inputs(), true));\n  TF_LITE_ENSURE_OK(context, body_subgraph->AllocateTensors());\n  if (body_subgraph->HasDynamicTensors()) {\n    op_data->body_has_dynamic_output_tensors = true;\n  } else {\n    for (int i = 0; i < num_inputs; ++i) {\n      TfLiteTensor* body_input =\n          body_subgraph->tensor(body_subgraph->inputs()[i]);\n      TfLiteTensor* body_output =\n          body_subgraph->tensor(body_subgraph->outputs()[i]);\n      TF_LITE_ENSURE_TYPES_EQ(context, body_input->type, body_output->type);\n\n      TF_LITE_ENSURE(context, !IsDynamicTensor(body_output));\n      if (!TfLiteIntArrayEqual(body_input->dims, body_output->dims)) {\n        // If the output shape of the body subgraph is static w.r.t. a fixed\n        // input size, but it's different from input size, it's still considered\n        // dynamic. For example: If a subgraph keeps padding its input with a\n        // fixed padding, the output shape is static w.r.t the input shape and\n        // padding, but running it in a loop will keep bloating the tensor.\n        op_data->body_has_dynamic_output_tensors = true;\n        break;\n      }\n    }\n  }\n  for (int i = 0; i < num_inputs; ++i) {\n    TfLiteTensor* output;\n    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &output));\n    if (op_data->body_has_dynamic_output_tensors) {\n      SetTensorToDynamic(output);\n    } else {\n      TfLiteTensor* body_output =\n          body_subgraph->tensor(body_subgraph->outputs()[i]);\n      TfLiteIntArray* output_size = TfLiteIntArrayCopy(body_output->dims);\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, output, output_size));\n    }\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  Subgraph* this_subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n  auto* subgraphs = this_subgraph->GetSubgraphs();\n  Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();\n  Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();\n\n  // The follow graph illustrates the current implementation.\n  //\n  // This Subgraph          Cond Subgraph         Body Subgraph\n  // +-----------+   (1)   +------------+   (3)   +------------+\n  // |   WHILE   |-------->|  SUBGRAPH  |-------->|  SUBGRAPH  |\n  // |   INPUT   |        /|   INPUT    |<-----   |   INPUT    |\n  // +-----------+       / +------------+      \\  +------------+\n  //                    /        |              \\       |\n  //               (6) /         | (2)       (5) \\      | (4)\n  //                  /          v                \\     v\n  // +-----------+   /     +------------+         +------------+\n  // |   WHILE   |<--      |  SUBGRAPH  |         |  SUBGRAPH  |\n  // |   OUTPUT  |         |   OUTPUT   |         |   OUTPUT   |\n  // +-----------+         +------------+         +------------+\n  //\n  // (1) Copy the inputs of WHILE op to the inputs of condition subgraph.\n  // (2) Invoke condition subgraph.\n  //     Jump to step 5 if result is false.\n  // (3) Copy the inputs of condition subgraph to the inputs of body subgraph.\n  // (4) Invoke body subgraph.\n  // (5) Copy the outputs of body subgraph to the inputs condition subgraph.\n  //     Jump back to step 2!\n  // (6) Copy the inputs of condition subgraph to the outputs of WHILE op.\n  //\n  // If the body subgraph has dynamic sized outputs, it's required to resize the\n  // tensor before copying in step 1, 3, 4 and 6.\n  //\n  // Note the flow is carefully designed to handle the dynamic sized output\n  // case. The loop invariant is: The newest value is in the inputs of condition\n  // subgraph. This is always true before step 2.\n  //\n  // This is the best we can do without sharing tensor buffer across subgraph\n  // boundary. Currently we copy the input / output between the subgraphs. This\n  // isn't optimized yet and a lot of redundant copies are made.\n  // TODO(b/120234921): Optimize and avoid copying tensors between subgraphs.\n\n  if (op_data->body_has_dynamic_output_tensors) {\n    // If body subgraph has dynamic outputs, the input of condition subgraph may\n    // be changed in the last invocation and may need resizing.\n    TF_LITE_ENSURE_OK(\n        context, CopyTensorsShapeAndType(\n                     context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                     cond_subgraph, cond_subgraph->inputs(), true));\n    TF_LITE_ENSURE_OK(context, cond_subgraph->AllocateTensors());\n  }\n  TF_LITE_ENSURE_OK(\n      context,\n      CopyTensorsData(context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                      cond_subgraph, cond_subgraph->inputs()));\n\n  while (true) {\n    TF_LITE_ENSURE_OK(context, cond_subgraph->Invoke());\n    int cond_subgraph_output_index = cond_subgraph->outputs()[0];\n    cond_subgraph->EnsureTensorDataIsReadable(cond_subgraph_output_index);\n    TfLiteTensor* cond_output =\n        cond_subgraph->tensor(cond_subgraph_output_index);\n    if (op_data->cond_has_dynamic_output_tensors) {\n      TF_LITE_ENSURE_STATUS(CheckCondOutput(context, cond_output));\n    }\n\n    if (!cond_output->data.b[0]) {\n      break;\n    }\n    if (op_data->body_has_dynamic_output_tensors) {\n      TF_LITE_ENSURE_OK(context,\n                        CopyTensorsShapeAndType(\n                            context, cond_subgraph, cond_subgraph->inputs(),\n                            body_subgraph, body_subgraph->inputs(), true));\n      TF_LITE_ENSURE_OK(context, body_subgraph->AllocateTensors());\n    }\n\n    TF_LITE_ENSURE_OK(\n        context,\n        CopyTensorsData(context, cond_subgraph, cond_subgraph->inputs(),\n                        body_subgraph, body_subgraph->inputs()));\n\n    TF_LITE_ENSURE_OK(context, body_subgraph->Invoke());\n\n    for (int tensor_index : body_subgraph->outputs()) {\n      body_subgraph->EnsureTensorDataIsReadable(tensor_index);\n    }\n\n    if (op_data->body_has_dynamic_output_tensors) {\n      TF_LITE_ENSURE_OK(context,\n                        CopyTensorsShapeAndType(\n                            context, body_subgraph, body_subgraph->outputs(),\n                            cond_subgraph, cond_subgraph->inputs(), true));\n      TF_LITE_ENSURE_OK(context, cond_subgraph->AllocateTensors());\n    }\n\n    TF_LITE_ENSURE_OK(\n        context,\n        CopyTensorsData(context, body_subgraph, body_subgraph->outputs(),\n                        cond_subgraph, cond_subgraph->inputs()));\n  }\n\n  // Note that copying from body's output will fail if body is never invoked.\n  // TODO(b/120234921): Optimize and avoid copying tensors between subgraphs.\n  if (op_data->body_has_dynamic_output_tensors) {\n    TF_LITE_ENSURE_OK(\n        context, CopyTensorsShapeAndType(\n                     context, cond_subgraph, cond_subgraph->inputs(),\n                     this_subgraph, TfLiteIntArrayView(node->outputs), false));\n  }\n\n  TF_LITE_ENSURE_OK(\n      context,\n      CopyTensorsData(context, cond_subgraph, cond_subgraph->inputs(),\n                      this_subgraph, TfLiteIntArrayView(node->outputs)));\n  return kTfLiteOk;\n}\n\n}  // namespace while_kernel\n\nTfLiteRegistration* Register_WHILE() {\n  static TfLiteRegistration r = {while_kernel::Init, while_kernel::Free,\n                                 while_kernel::Prepare, while_kernel::Eval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/while.cc"], "buggy_code_start_loc": [140], "buggy_code_end_loc": [140], "fixing_code_start_loc": [141], "fixing_code_end_loc": [143], "type": "CWE-674", "message": "TensorFlow is an end-to-end open source platform for machine learning. TFlite graphs must not have loops between nodes. However, this condition was not checked and an attacker could craft models that would result in infinite loop during evaluation. In certain cases, the infinite loop would be replaced by stack overflow due to too many recursive calls. For example, the `While` implementation(https://github.com/tensorflow/tensorflow/blob/106d8f4fb89335a2c52d7c895b7a7485465ca8d9/tensorflow/lite/kernels/while.cc) could be tricked into a scneario where both the body and the loop subgraphs are the same. Evaluating one of the subgraphs means calling the `Eval` function for the other and this quickly exhaust all stack space. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range. Please consult our security guide(https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.", "other": {"cve": {"id": "CVE-2021-29591", "sourceIdentifier": "security-advisories@github.com", "published": "2021-05-14T20:15:15.017", "lastModified": "2022-04-25T20:09:28.313", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. TFlite graphs must not have loops between nodes. However, this condition was not checked and an attacker could craft models that would result in infinite loop during evaluation. In certain cases, the infinite loop would be replaced by stack overflow due to too many recursive calls. For example, the `While` implementation(https://github.com/tensorflow/tensorflow/blob/106d8f4fb89335a2c52d7c895b7a7485465ca8d9/tensorflow/lite/kernels/while.cc) could be tricked into a scneario where both the body and the loop subgraphs are the same. Evaluating one of the subgraphs means calling the `Eval` function for the other and this quickly exhaust all stack space. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range. Please consult our security guide(https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;Los gr\u00e1ficos TFlite no deben tener bucles entre nodos.&#xa0;Sin embargo, esta condici\u00f3n no fue comprobada y un atacante podr\u00eda dise\u00f1ar modelos que dar\u00edan como resultado un bucle infinito durante la evaluaci\u00f3n.&#xa0;En determinados casos, el bucle infinito ser\u00eda reemplazado por un desbordamiento de la pila debido a demasiadas llamadas recursivas.&#xa0;Por ejemplo, la implementaci\u00f3n \"While\" (https://github.com/tensorflow/tensorflow/blob/106d8f4fb89335a2c52d7c895b7a7485465ca8d9/tensorflow/lite/kernels/ while.cc) podr\u00eda enga\u00f1arse en un argunto donde tanto el cuerpo como los subgrafos de bucle son lo mismo.&#xa0;Evaluar uno de los subgrafos significa llamar a la funci\u00f3n \"Eval\" para el otro y esto agota r\u00e1pidamente todo el espacio de la pila.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.5.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow 2.4.2, TensorFlow 2.3.3,&#xa0;TensorFlow 2.2.3 y TensorFlow 2.1.4, ya que tambi\u00e9n est\u00e1n afectados y a\u00fan se encuentran en el rango compatible.&#xa0;Consulte nuestra gu\u00eda de seguridad (https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) para obtener m\u00e1s informaci\u00f3n sobre el modelo de seguridad y c\u00f3mo contactarnos con problemas y preguntas"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:L/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "LOW", "availabilityImpact": "HIGH", "baseScore": 7.3, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-674"}, {"lang": "en", "value": "CWE-835"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-835"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.1.4", "matchCriteriaId": "323ABCCE-24EB-47CC-87F6-48C101477587"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.3", "matchCriteriaId": "64ABA90C-0649-4BB0-89C9-83C14BBDCC0F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.3", "matchCriteriaId": "0F83E0CF-CBF6-4C24-8683-3E7A5DC95BA9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.2", "matchCriteriaId": "8259531B-A8AC-4F8B-B60F-B69DE4767C03"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/9c1dc920d8ffb4893d6c9d27d1f039607b326743", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/c6173f5fe66cdbab74f4f869311fe6aae2ba35f4", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-cwv3-863g-39vx", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/9c1dc920d8ffb4893d6c9d27d1f039607b326743"}}