{"buggy_code": ["/*\n *  i386 translation\n *\n *  Copyright (c) 2003 Fabrice Bellard\n *\n * This library is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2 of the License, or (at your option) any later version.\n *\n * This library is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with this library; if not, see <http://www.gnu.org/licenses/>.\n */\n#include \"qemu/osdep.h\"\n\n#include \"qemu/host-utils.h\"\n#include \"cpu.h\"\n#include \"disas/disas.h\"\n#include \"exec/exec-all.h\"\n#include \"tcg-op.h\"\n#include \"exec/cpu_ldst.h\"\n\n#include \"exec/helper-proto.h\"\n#include \"exec/helper-gen.h\"\n\n#include \"trace-tcg.h\"\n#include \"exec/log.h\"\n\n\n#define PREFIX_REPZ   0x01\n#define PREFIX_REPNZ  0x02\n#define PREFIX_LOCK   0x04\n#define PREFIX_DATA   0x08\n#define PREFIX_ADR    0x10\n#define PREFIX_VEX    0x20\n\n#ifdef TARGET_X86_64\n#define CODE64(s) ((s)->code64)\n#define REX_X(s) ((s)->rex_x)\n#define REX_B(s) ((s)->rex_b)\n#else\n#define CODE64(s) 0\n#define REX_X(s) 0\n#define REX_B(s) 0\n#endif\n\n#ifdef TARGET_X86_64\n# define ctztl  ctz64\n# define clztl  clz64\n#else\n# define ctztl  ctz32\n# define clztl  clz32\n#endif\n\n/* For a switch indexed by MODRM, match all memory operands for a given OP.  */\n#define CASE_MODRM_MEM_OP(OP) \\\n    case (0 << 6) | (OP << 3) | 0 ... (0 << 6) | (OP << 3) | 7: \\\n    case (1 << 6) | (OP << 3) | 0 ... (1 << 6) | (OP << 3) | 7: \\\n    case (2 << 6) | (OP << 3) | 0 ... (2 << 6) | (OP << 3) | 7\n\n#define CASE_MODRM_OP(OP) \\\n    case (0 << 6) | (OP << 3) | 0 ... (0 << 6) | (OP << 3) | 7: \\\n    case (1 << 6) | (OP << 3) | 0 ... (1 << 6) | (OP << 3) | 7: \\\n    case (2 << 6) | (OP << 3) | 0 ... (2 << 6) | (OP << 3) | 7: \\\n    case (3 << 6) | (OP << 3) | 0 ... (3 << 6) | (OP << 3) | 7\n\n//#define MACRO_TEST   1\n\n/* global register indexes */\nstatic TCGv_env cpu_env;\nstatic TCGv cpu_A0;\nstatic TCGv cpu_cc_dst, cpu_cc_src, cpu_cc_src2, cpu_cc_srcT;\nstatic TCGv_i32 cpu_cc_op;\nstatic TCGv cpu_regs[CPU_NB_REGS];\nstatic TCGv cpu_seg_base[6];\nstatic TCGv_i64 cpu_bndl[4];\nstatic TCGv_i64 cpu_bndu[4];\n/* local temps */\nstatic TCGv cpu_T0, cpu_T1;\n/* local register indexes (only used inside old micro ops) */\nstatic TCGv cpu_tmp0, cpu_tmp4;\nstatic TCGv_ptr cpu_ptr0, cpu_ptr1;\nstatic TCGv_i32 cpu_tmp2_i32, cpu_tmp3_i32;\nstatic TCGv_i64 cpu_tmp1_i64;\n\n#include \"exec/gen-icount.h\"\n\n#ifdef TARGET_X86_64\nstatic int x86_64_hregs;\n#endif\n\ntypedef struct DisasContext {\n    /* current insn context */\n    int override; /* -1 if no override */\n    int prefix;\n    TCGMemOp aflag;\n    TCGMemOp dflag;\n    target_ulong pc_start;\n    target_ulong pc; /* pc = eip + cs_base */\n    int is_jmp; /* 1 = means jump (stop translation), 2 means CPU\n                   static state change (stop translation) */\n    /* current block context */\n    target_ulong cs_base; /* base of CS segment */\n    int pe;     /* protected mode */\n    int code32; /* 32 bit code segment */\n#ifdef TARGET_X86_64\n    int lma;    /* long mode active */\n    int code64; /* 64 bit code segment */\n    int rex_x, rex_b;\n#endif\n    int vex_l;  /* vex vector length */\n    int vex_v;  /* vex vvvv register, without 1's compliment.  */\n    int ss32;   /* 32 bit stack segment */\n    CCOp cc_op;  /* current CC operation */\n    bool cc_op_dirty;\n    int addseg; /* non zero if either DS/ES/SS have a non zero base */\n    int f_st;   /* currently unused */\n    int vm86;   /* vm86 mode */\n    int cpl;\n    int iopl;\n    int tf;     /* TF cpu flag */\n    int singlestep_enabled; /* \"hardware\" single step enabled */\n    int jmp_opt; /* use direct block chaining for direct jumps */\n    int repz_opt; /* optimize jumps within repz instructions */\n    int mem_index; /* select memory access functions */\n    uint64_t flags; /* all execution flags */\n    struct TranslationBlock *tb;\n    int popl_esp_hack; /* for correct popl with esp base handling */\n    int rip_offset; /* only used in x86_64, but left for simplicity */\n    int cpuid_features;\n    int cpuid_ext_features;\n    int cpuid_ext2_features;\n    int cpuid_ext3_features;\n    int cpuid_7_0_ebx_features;\n    int cpuid_xsave_features;\n} DisasContext;\n\nstatic void gen_eob(DisasContext *s);\nstatic void gen_jmp(DisasContext *s, target_ulong eip);\nstatic void gen_jmp_tb(DisasContext *s, target_ulong eip, int tb_num);\nstatic void gen_op(DisasContext *s1, int op, TCGMemOp ot, int d);\n\n/* i386 arith/logic operations */\nenum {\n    OP_ADDL,\n    OP_ORL,\n    OP_ADCL,\n    OP_SBBL,\n    OP_ANDL,\n    OP_SUBL,\n    OP_XORL,\n    OP_CMPL,\n};\n\n/* i386 shift ops */\nenum {\n    OP_ROL,\n    OP_ROR,\n    OP_RCL,\n    OP_RCR,\n    OP_SHL,\n    OP_SHR,\n    OP_SHL1, /* undocumented */\n    OP_SAR = 7,\n};\n\nenum {\n    JCC_O,\n    JCC_B,\n    JCC_Z,\n    JCC_BE,\n    JCC_S,\n    JCC_P,\n    JCC_L,\n    JCC_LE,\n};\n\nenum {\n    /* I386 int registers */\n    OR_EAX,   /* MUST be even numbered */\n    OR_ECX,\n    OR_EDX,\n    OR_EBX,\n    OR_ESP,\n    OR_EBP,\n    OR_ESI,\n    OR_EDI,\n\n    OR_TMP0 = 16,    /* temporary operand register */\n    OR_TMP1,\n    OR_A0, /* temporary register used when doing address evaluation */\n};\n\nenum {\n    USES_CC_DST  = 1,\n    USES_CC_SRC  = 2,\n    USES_CC_SRC2 = 4,\n    USES_CC_SRCT = 8,\n};\n\n/* Bit set if the global variable is live after setting CC_OP to X.  */\nstatic const uint8_t cc_op_live[CC_OP_NB] = {\n    [CC_OP_DYNAMIC] = USES_CC_DST | USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_EFLAGS] = USES_CC_SRC,\n    [CC_OP_MULB ... CC_OP_MULQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_ADDB ... CC_OP_ADDQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_ADCB ... CC_OP_ADCQ] = USES_CC_DST | USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_SUBB ... CC_OP_SUBQ] = USES_CC_DST | USES_CC_SRC | USES_CC_SRCT,\n    [CC_OP_SBBB ... CC_OP_SBBQ] = USES_CC_DST | USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_LOGICB ... CC_OP_LOGICQ] = USES_CC_DST,\n    [CC_OP_INCB ... CC_OP_INCQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_DECB ... CC_OP_DECQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_SHLB ... CC_OP_SHLQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_SARB ... CC_OP_SARQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_BMILGB ... CC_OP_BMILGQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_ADCX] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_ADOX] = USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_ADCOX] = USES_CC_DST | USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_CLR] = 0,\n    [CC_OP_POPCNT] = USES_CC_SRC,\n};\n\nstatic void set_cc_op(DisasContext *s, CCOp op)\n{\n    int dead;\n\n    if (s->cc_op == op) {\n        return;\n    }\n\n    /* Discard CC computation that will no longer be used.  */\n    dead = cc_op_live[s->cc_op] & ~cc_op_live[op];\n    if (dead & USES_CC_DST) {\n        tcg_gen_discard_tl(cpu_cc_dst);\n    }\n    if (dead & USES_CC_SRC) {\n        tcg_gen_discard_tl(cpu_cc_src);\n    }\n    if (dead & USES_CC_SRC2) {\n        tcg_gen_discard_tl(cpu_cc_src2);\n    }\n    if (dead & USES_CC_SRCT) {\n        tcg_gen_discard_tl(cpu_cc_srcT);\n    }\n\n    if (op == CC_OP_DYNAMIC) {\n        /* The DYNAMIC setting is translator only, and should never be\n           stored.  Thus we always consider it clean.  */\n        s->cc_op_dirty = false;\n    } else {\n        /* Discard any computed CC_OP value (see shifts).  */\n        if (s->cc_op == CC_OP_DYNAMIC) {\n            tcg_gen_discard_i32(cpu_cc_op);\n        }\n        s->cc_op_dirty = true;\n    }\n    s->cc_op = op;\n}\n\nstatic void gen_update_cc_op(DisasContext *s)\n{\n    if (s->cc_op_dirty) {\n        tcg_gen_movi_i32(cpu_cc_op, s->cc_op);\n        s->cc_op_dirty = false;\n    }\n}\n\n#ifdef TARGET_X86_64\n\n#define NB_OP_SIZES 4\n\n#else /* !TARGET_X86_64 */\n\n#define NB_OP_SIZES 3\n\n#endif /* !TARGET_X86_64 */\n\n#if defined(HOST_WORDS_BIGENDIAN)\n#define REG_B_OFFSET (sizeof(target_ulong) - 1)\n#define REG_H_OFFSET (sizeof(target_ulong) - 2)\n#define REG_W_OFFSET (sizeof(target_ulong) - 2)\n#define REG_L_OFFSET (sizeof(target_ulong) - 4)\n#define REG_LH_OFFSET (sizeof(target_ulong) - 8)\n#else\n#define REG_B_OFFSET 0\n#define REG_H_OFFSET 1\n#define REG_W_OFFSET 0\n#define REG_L_OFFSET 0\n#define REG_LH_OFFSET 4\n#endif\n\n/* In instruction encodings for byte register accesses the\n * register number usually indicates \"low 8 bits of register N\";\n * however there are some special cases where N 4..7 indicates\n * [AH, CH, DH, BH], ie \"bits 15..8 of register N-4\". Return\n * true for this special case, false otherwise.\n */\nstatic inline bool byte_reg_is_xH(int reg)\n{\n    if (reg < 4) {\n        return false;\n    }\n#ifdef TARGET_X86_64\n    if (reg >= 8 || x86_64_hregs) {\n        return false;\n    }\n#endif\n    return true;\n}\n\n/* Select the size of a push/pop operation.  */\nstatic inline TCGMemOp mo_pushpop(DisasContext *s, TCGMemOp ot)\n{\n    if (CODE64(s)) {\n        return ot == MO_16 ? MO_16 : MO_64;\n    } else {\n        return ot;\n    }\n}\n\n/* Select the size of the stack pointer.  */\nstatic inline TCGMemOp mo_stacksize(DisasContext *s)\n{\n    return CODE64(s) ? MO_64 : s->ss32 ? MO_32 : MO_16;\n}\n\n/* Select only size 64 else 32.  Used for SSE operand sizes.  */\nstatic inline TCGMemOp mo_64_32(TCGMemOp ot)\n{\n#ifdef TARGET_X86_64\n    return ot == MO_64 ? MO_64 : MO_32;\n#else\n    return MO_32;\n#endif\n}\n\n/* Select size 8 if lsb of B is clear, else OT.  Used for decoding\n   byte vs word opcodes.  */\nstatic inline TCGMemOp mo_b_d(int b, TCGMemOp ot)\n{\n    return b & 1 ? ot : MO_8;\n}\n\n/* Select size 8 if lsb of B is clear, else OT capped at 32.\n   Used for decoding operand size of port opcodes.  */\nstatic inline TCGMemOp mo_b_d32(int b, TCGMemOp ot)\n{\n    return b & 1 ? (ot == MO_16 ? MO_16 : MO_32) : MO_8;\n}\n\nstatic void gen_op_mov_reg_v(TCGMemOp ot, int reg, TCGv t0)\n{\n    switch(ot) {\n    case MO_8:\n        if (!byte_reg_is_xH(reg)) {\n            tcg_gen_deposit_tl(cpu_regs[reg], cpu_regs[reg], t0, 0, 8);\n        } else {\n            tcg_gen_deposit_tl(cpu_regs[reg - 4], cpu_regs[reg - 4], t0, 8, 8);\n        }\n        break;\n    case MO_16:\n        tcg_gen_deposit_tl(cpu_regs[reg], cpu_regs[reg], t0, 0, 16);\n        break;\n    case MO_32:\n        /* For x86_64, this sets the higher half of register to zero.\n           For i386, this is equivalent to a mov. */\n        tcg_gen_ext32u_tl(cpu_regs[reg], t0);\n        break;\n#ifdef TARGET_X86_64\n    case MO_64:\n        tcg_gen_mov_tl(cpu_regs[reg], t0);\n        break;\n#endif\n    default:\n        tcg_abort();\n    }\n}\n\nstatic inline void gen_op_mov_v_reg(TCGMemOp ot, TCGv t0, int reg)\n{\n    if (ot == MO_8 && byte_reg_is_xH(reg)) {\n        tcg_gen_extract_tl(t0, cpu_regs[reg - 4], 8, 8);\n    } else {\n        tcg_gen_mov_tl(t0, cpu_regs[reg]);\n    }\n}\n\nstatic void gen_add_A0_im(DisasContext *s, int val)\n{\n    tcg_gen_addi_tl(cpu_A0, cpu_A0, val);\n    if (!CODE64(s)) {\n        tcg_gen_ext32u_tl(cpu_A0, cpu_A0);\n    }\n}\n\nstatic inline void gen_op_jmp_v(TCGv dest)\n{\n    tcg_gen_st_tl(dest, cpu_env, offsetof(CPUX86State, eip));\n}\n\nstatic inline void gen_op_add_reg_im(TCGMemOp size, int reg, int32_t val)\n{\n    tcg_gen_addi_tl(cpu_tmp0, cpu_regs[reg], val);\n    gen_op_mov_reg_v(size, reg, cpu_tmp0);\n}\n\nstatic inline void gen_op_add_reg_T0(TCGMemOp size, int reg)\n{\n    tcg_gen_add_tl(cpu_tmp0, cpu_regs[reg], cpu_T0);\n    gen_op_mov_reg_v(size, reg, cpu_tmp0);\n}\n\nstatic inline void gen_op_ld_v(DisasContext *s, int idx, TCGv t0, TCGv a0)\n{\n    tcg_gen_qemu_ld_tl(t0, a0, s->mem_index, idx | MO_LE);\n}\n\nstatic inline void gen_op_st_v(DisasContext *s, int idx, TCGv t0, TCGv a0)\n{\n    tcg_gen_qemu_st_tl(t0, a0, s->mem_index, idx | MO_LE);\n}\n\nstatic inline void gen_op_st_rm_T0_A0(DisasContext *s, int idx, int d)\n{\n    if (d == OR_TMP0) {\n        gen_op_st_v(s, idx, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_reg_v(idx, d, cpu_T0);\n    }\n}\n\nstatic inline void gen_jmp_im(target_ulong pc)\n{\n    tcg_gen_movi_tl(cpu_tmp0, pc);\n    gen_op_jmp_v(cpu_tmp0);\n}\n\n/* Compute SEG:REG into A0.  SEG is selected from the override segment\n   (OVR_SEG) and the default segment (DEF_SEG).  OVR_SEG may be -1 to\n   indicate no override.  */\nstatic void gen_lea_v_seg(DisasContext *s, TCGMemOp aflag, TCGv a0,\n                          int def_seg, int ovr_seg)\n{\n    switch (aflag) {\n#ifdef TARGET_X86_64\n    case MO_64:\n        if (ovr_seg < 0) {\n            tcg_gen_mov_tl(cpu_A0, a0);\n            return;\n        }\n        break;\n#endif\n    case MO_32:\n        /* 32 bit address */\n        if (ovr_seg < 0 && s->addseg) {\n            ovr_seg = def_seg;\n        }\n        if (ovr_seg < 0) {\n            tcg_gen_ext32u_tl(cpu_A0, a0);\n            return;\n        }\n        break;\n    case MO_16:\n        /* 16 bit address */\n        tcg_gen_ext16u_tl(cpu_A0, a0);\n        a0 = cpu_A0;\n        if (ovr_seg < 0) {\n            if (s->addseg) {\n                ovr_seg = def_seg;\n            } else {\n                return;\n            }\n        }\n        break;\n    default:\n        tcg_abort();\n    }\n\n    if (ovr_seg >= 0) {\n        TCGv seg = cpu_seg_base[ovr_seg];\n\n        if (aflag == MO_64) {\n            tcg_gen_add_tl(cpu_A0, a0, seg);\n        } else if (CODE64(s)) {\n            tcg_gen_ext32u_tl(cpu_A0, a0);\n            tcg_gen_add_tl(cpu_A0, cpu_A0, seg);\n        } else {\n            tcg_gen_add_tl(cpu_A0, a0, seg);\n            tcg_gen_ext32u_tl(cpu_A0, cpu_A0);\n        }\n    }\n}\n\nstatic inline void gen_string_movl_A0_ESI(DisasContext *s)\n{\n    gen_lea_v_seg(s, s->aflag, cpu_regs[R_ESI], R_DS, s->override);\n}\n\nstatic inline void gen_string_movl_A0_EDI(DisasContext *s)\n{\n    gen_lea_v_seg(s, s->aflag, cpu_regs[R_EDI], R_ES, -1);\n}\n\nstatic inline void gen_op_movl_T0_Dshift(TCGMemOp ot)\n{\n    tcg_gen_ld32s_tl(cpu_T0, cpu_env, offsetof(CPUX86State, df));\n    tcg_gen_shli_tl(cpu_T0, cpu_T0, ot);\n};\n\nstatic TCGv gen_ext_tl(TCGv dst, TCGv src, TCGMemOp size, bool sign)\n{\n    switch (size) {\n    case MO_8:\n        if (sign) {\n            tcg_gen_ext8s_tl(dst, src);\n        } else {\n            tcg_gen_ext8u_tl(dst, src);\n        }\n        return dst;\n    case MO_16:\n        if (sign) {\n            tcg_gen_ext16s_tl(dst, src);\n        } else {\n            tcg_gen_ext16u_tl(dst, src);\n        }\n        return dst;\n#ifdef TARGET_X86_64\n    case MO_32:\n        if (sign) {\n            tcg_gen_ext32s_tl(dst, src);\n        } else {\n            tcg_gen_ext32u_tl(dst, src);\n        }\n        return dst;\n#endif\n    default:\n        return src;\n    }\n}\n\nstatic void gen_extu(TCGMemOp ot, TCGv reg)\n{\n    gen_ext_tl(reg, reg, ot, false);\n}\n\nstatic void gen_exts(TCGMemOp ot, TCGv reg)\n{\n    gen_ext_tl(reg, reg, ot, true);\n}\n\nstatic inline void gen_op_jnz_ecx(TCGMemOp size, TCGLabel *label1)\n{\n    tcg_gen_mov_tl(cpu_tmp0, cpu_regs[R_ECX]);\n    gen_extu(size, cpu_tmp0);\n    tcg_gen_brcondi_tl(TCG_COND_NE, cpu_tmp0, 0, label1);\n}\n\nstatic inline void gen_op_jz_ecx(TCGMemOp size, TCGLabel *label1)\n{\n    tcg_gen_mov_tl(cpu_tmp0, cpu_regs[R_ECX]);\n    gen_extu(size, cpu_tmp0);\n    tcg_gen_brcondi_tl(TCG_COND_EQ, cpu_tmp0, 0, label1);\n}\n\nstatic void gen_helper_in_func(TCGMemOp ot, TCGv v, TCGv_i32 n)\n{\n    switch (ot) {\n    case MO_8:\n        gen_helper_inb(v, cpu_env, n);\n        break;\n    case MO_16:\n        gen_helper_inw(v, cpu_env, n);\n        break;\n    case MO_32:\n        gen_helper_inl(v, cpu_env, n);\n        break;\n    default:\n        tcg_abort();\n    }\n}\n\nstatic void gen_helper_out_func(TCGMemOp ot, TCGv_i32 v, TCGv_i32 n)\n{\n    switch (ot) {\n    case MO_8:\n        gen_helper_outb(cpu_env, v, n);\n        break;\n    case MO_16:\n        gen_helper_outw(cpu_env, v, n);\n        break;\n    case MO_32:\n        gen_helper_outl(cpu_env, v, n);\n        break;\n    default:\n        tcg_abort();\n    }\n}\n\nstatic void gen_check_io(DisasContext *s, TCGMemOp ot, target_ulong cur_eip,\n                         uint32_t svm_flags)\n{\n    target_ulong next_eip;\n\n    if (s->pe && (s->cpl > s->iopl || s->vm86)) {\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        switch (ot) {\n        case MO_8:\n            gen_helper_check_iob(cpu_env, cpu_tmp2_i32);\n            break;\n        case MO_16:\n            gen_helper_check_iow(cpu_env, cpu_tmp2_i32);\n            break;\n        case MO_32:\n            gen_helper_check_iol(cpu_env, cpu_tmp2_i32);\n            break;\n        default:\n            tcg_abort();\n        }\n    }\n    if(s->flags & HF_SVMI_MASK) {\n        gen_update_cc_op(s);\n        gen_jmp_im(cur_eip);\n        svm_flags |= (1 << (4 + ot));\n        next_eip = s->pc - s->cs_base;\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        gen_helper_svm_check_io(cpu_env, cpu_tmp2_i32,\n                                tcg_const_i32(svm_flags),\n                                tcg_const_i32(next_eip - cur_eip));\n    }\n}\n\nstatic inline void gen_movs(DisasContext *s, TCGMemOp ot)\n{\n    gen_string_movl_A0_ESI(s);\n    gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    gen_string_movl_A0_EDI(s);\n    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_ESI);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n}\n\nstatic void gen_op_update1_cc(void)\n{\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n}\n\nstatic void gen_op_update2_cc(void)\n{\n    tcg_gen_mov_tl(cpu_cc_src, cpu_T1);\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n}\n\nstatic void gen_op_update3_cc(TCGv reg)\n{\n    tcg_gen_mov_tl(cpu_cc_src2, reg);\n    tcg_gen_mov_tl(cpu_cc_src, cpu_T1);\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n}\n\nstatic inline void gen_op_testl_T0_T1_cc(void)\n{\n    tcg_gen_and_tl(cpu_cc_dst, cpu_T0, cpu_T1);\n}\n\nstatic void gen_op_update_neg_cc(void)\n{\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n    tcg_gen_neg_tl(cpu_cc_src, cpu_T0);\n    tcg_gen_movi_tl(cpu_cc_srcT, 0);\n}\n\n/* compute all eflags to cc_src */\nstatic void gen_compute_eflags(DisasContext *s)\n{\n    TCGv zero, dst, src1, src2;\n    int live, dead;\n\n    if (s->cc_op == CC_OP_EFLAGS) {\n        return;\n    }\n    if (s->cc_op == CC_OP_CLR) {\n        tcg_gen_movi_tl(cpu_cc_src, CC_Z | CC_P);\n        set_cc_op(s, CC_OP_EFLAGS);\n        return;\n    }\n\n    TCGV_UNUSED(zero);\n    dst = cpu_cc_dst;\n    src1 = cpu_cc_src;\n    src2 = cpu_cc_src2;\n\n    /* Take care to not read values that are not live.  */\n    live = cc_op_live[s->cc_op] & ~USES_CC_SRCT;\n    dead = live ^ (USES_CC_DST | USES_CC_SRC | USES_CC_SRC2);\n    if (dead) {\n        zero = tcg_const_tl(0);\n        if (dead & USES_CC_DST) {\n            dst = zero;\n        }\n        if (dead & USES_CC_SRC) {\n            src1 = zero;\n        }\n        if (dead & USES_CC_SRC2) {\n            src2 = zero;\n        }\n    }\n\n    gen_update_cc_op(s);\n    gen_helper_cc_compute_all(cpu_cc_src, dst, src1, src2, cpu_cc_op);\n    set_cc_op(s, CC_OP_EFLAGS);\n\n    if (dead) {\n        tcg_temp_free(zero);\n    }\n}\n\ntypedef struct CCPrepare {\n    TCGCond cond;\n    TCGv reg;\n    TCGv reg2;\n    target_ulong imm;\n    target_ulong mask;\n    bool use_reg2;\n    bool no_setcond;\n} CCPrepare;\n\n/* compute eflags.C to reg */\nstatic CCPrepare gen_prepare_eflags_c(DisasContext *s, TCGv reg)\n{\n    TCGv t0, t1;\n    int size, shift;\n\n    switch (s->cc_op) {\n    case CC_OP_SUBB ... CC_OP_SUBQ:\n        /* (DATA_TYPE)CC_SRCT < (DATA_TYPE)CC_SRC */\n        size = s->cc_op - CC_OP_SUBB;\n        t1 = gen_ext_tl(cpu_tmp0, cpu_cc_src, size, false);\n        /* If no temporary was used, be careful not to alias t1 and t0.  */\n        t0 = TCGV_EQUAL(t1, cpu_cc_src) ? cpu_tmp0 : reg;\n        tcg_gen_mov_tl(t0, cpu_cc_srcT);\n        gen_extu(size, t0);\n        goto add_sub;\n\n    case CC_OP_ADDB ... CC_OP_ADDQ:\n        /* (DATA_TYPE)CC_DST < (DATA_TYPE)CC_SRC */\n        size = s->cc_op - CC_OP_ADDB;\n        t1 = gen_ext_tl(cpu_tmp0, cpu_cc_src, size, false);\n        t0 = gen_ext_tl(reg, cpu_cc_dst, size, false);\n    add_sub:\n        return (CCPrepare) { .cond = TCG_COND_LTU, .reg = t0,\n                             .reg2 = t1, .mask = -1, .use_reg2 = true };\n\n    case CC_OP_LOGICB ... CC_OP_LOGICQ:\n    case CC_OP_CLR:\n    case CC_OP_POPCNT:\n        return (CCPrepare) { .cond = TCG_COND_NEVER, .mask = -1 };\n\n    case CC_OP_INCB ... CC_OP_INCQ:\n    case CC_OP_DECB ... CC_OP_DECQ:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = -1, .no_setcond = true };\n\n    case CC_OP_SHLB ... CC_OP_SHLQ:\n        /* (CC_SRC >> (DATA_BITS - 1)) & 1 */\n        size = s->cc_op - CC_OP_SHLB;\n        shift = (8 << size) - 1;\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = (target_ulong)1 << shift };\n\n    case CC_OP_MULB ... CC_OP_MULQ:\n        return (CCPrepare) { .cond = TCG_COND_NE,\n                             .reg = cpu_cc_src, .mask = -1 };\n\n    case CC_OP_BMILGB ... CC_OP_BMILGQ:\n        size = s->cc_op - CC_OP_BMILGB;\n        t0 = gen_ext_tl(reg, cpu_cc_src, size, false);\n        return (CCPrepare) { .cond = TCG_COND_EQ, .reg = t0, .mask = -1 };\n\n    case CC_OP_ADCX:\n    case CC_OP_ADCOX:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_dst,\n                             .mask = -1, .no_setcond = true };\n\n    case CC_OP_EFLAGS:\n    case CC_OP_SARB ... CC_OP_SARQ:\n        /* CC_SRC & 1 */\n        return (CCPrepare) { .cond = TCG_COND_NE,\n                             .reg = cpu_cc_src, .mask = CC_C };\n\n    default:\n       /* The need to compute only C from CC_OP_DYNAMIC is important\n          in efficiently implementing e.g. INC at the start of a TB.  */\n       gen_update_cc_op(s);\n       gen_helper_cc_compute_c(reg, cpu_cc_dst, cpu_cc_src,\n                               cpu_cc_src2, cpu_cc_op);\n       return (CCPrepare) { .cond = TCG_COND_NE, .reg = reg,\n                            .mask = -1, .no_setcond = true };\n    }\n}\n\n/* compute eflags.P to reg */\nstatic CCPrepare gen_prepare_eflags_p(DisasContext *s, TCGv reg)\n{\n    gen_compute_eflags(s);\n    return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                         .mask = CC_P };\n}\n\n/* compute eflags.S to reg */\nstatic CCPrepare gen_prepare_eflags_s(DisasContext *s, TCGv reg)\n{\n    switch (s->cc_op) {\n    case CC_OP_DYNAMIC:\n        gen_compute_eflags(s);\n        /* FALLTHRU */\n    case CC_OP_EFLAGS:\n    case CC_OP_ADCX:\n    case CC_OP_ADOX:\n    case CC_OP_ADCOX:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = CC_S };\n    case CC_OP_CLR:\n    case CC_OP_POPCNT:\n        return (CCPrepare) { .cond = TCG_COND_NEVER, .mask = -1 };\n    default:\n        {\n            TCGMemOp size = (s->cc_op - CC_OP_ADDB) & 3;\n            TCGv t0 = gen_ext_tl(reg, cpu_cc_dst, size, true);\n            return (CCPrepare) { .cond = TCG_COND_LT, .reg = t0, .mask = -1 };\n        }\n    }\n}\n\n/* compute eflags.O to reg */\nstatic CCPrepare gen_prepare_eflags_o(DisasContext *s, TCGv reg)\n{\n    switch (s->cc_op) {\n    case CC_OP_ADOX:\n    case CC_OP_ADCOX:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src2,\n                             .mask = -1, .no_setcond = true };\n    case CC_OP_CLR:\n    case CC_OP_POPCNT:\n        return (CCPrepare) { .cond = TCG_COND_NEVER, .mask = -1 };\n    default:\n        gen_compute_eflags(s);\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = CC_O };\n    }\n}\n\n/* compute eflags.Z to reg */\nstatic CCPrepare gen_prepare_eflags_z(DisasContext *s, TCGv reg)\n{\n    switch (s->cc_op) {\n    case CC_OP_DYNAMIC:\n        gen_compute_eflags(s);\n        /* FALLTHRU */\n    case CC_OP_EFLAGS:\n    case CC_OP_ADCX:\n    case CC_OP_ADOX:\n    case CC_OP_ADCOX:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = CC_Z };\n    case CC_OP_CLR:\n        return (CCPrepare) { .cond = TCG_COND_ALWAYS, .mask = -1 };\n    case CC_OP_POPCNT:\n        return (CCPrepare) { .cond = TCG_COND_EQ, .reg = cpu_cc_src,\n                             .mask = -1 };\n    default:\n        {\n            TCGMemOp size = (s->cc_op - CC_OP_ADDB) & 3;\n            TCGv t0 = gen_ext_tl(reg, cpu_cc_dst, size, false);\n            return (CCPrepare) { .cond = TCG_COND_EQ, .reg = t0, .mask = -1 };\n        }\n    }\n}\n\n/* perform a conditional store into register 'reg' according to jump opcode\n   value 'b'. In the fast case, T0 is guaranted not to be used. */\nstatic CCPrepare gen_prepare_cc(DisasContext *s, int b, TCGv reg)\n{\n    int inv, jcc_op, cond;\n    TCGMemOp size;\n    CCPrepare cc;\n    TCGv t0;\n\n    inv = b & 1;\n    jcc_op = (b >> 1) & 7;\n\n    switch (s->cc_op) {\n    case CC_OP_SUBB ... CC_OP_SUBQ:\n        /* We optimize relational operators for the cmp/jcc case.  */\n        size = s->cc_op - CC_OP_SUBB;\n        switch (jcc_op) {\n        case JCC_BE:\n            tcg_gen_mov_tl(cpu_tmp4, cpu_cc_srcT);\n            gen_extu(size, cpu_tmp4);\n            t0 = gen_ext_tl(cpu_tmp0, cpu_cc_src, size, false);\n            cc = (CCPrepare) { .cond = TCG_COND_LEU, .reg = cpu_tmp4,\n                               .reg2 = t0, .mask = -1, .use_reg2 = true };\n            break;\n\n        case JCC_L:\n            cond = TCG_COND_LT;\n            goto fast_jcc_l;\n        case JCC_LE:\n            cond = TCG_COND_LE;\n        fast_jcc_l:\n            tcg_gen_mov_tl(cpu_tmp4, cpu_cc_srcT);\n            gen_exts(size, cpu_tmp4);\n            t0 = gen_ext_tl(cpu_tmp0, cpu_cc_src, size, true);\n            cc = (CCPrepare) { .cond = cond, .reg = cpu_tmp4,\n                               .reg2 = t0, .mask = -1, .use_reg2 = true };\n            break;\n\n        default:\n            goto slow_jcc;\n        }\n        break;\n\n    default:\n    slow_jcc:\n        /* This actually generates good code for JC, JZ and JS.  */\n        switch (jcc_op) {\n        case JCC_O:\n            cc = gen_prepare_eflags_o(s, reg);\n            break;\n        case JCC_B:\n            cc = gen_prepare_eflags_c(s, reg);\n            break;\n        case JCC_Z:\n            cc = gen_prepare_eflags_z(s, reg);\n            break;\n        case JCC_BE:\n            gen_compute_eflags(s);\n            cc = (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                               .mask = CC_Z | CC_C };\n            break;\n        case JCC_S:\n            cc = gen_prepare_eflags_s(s, reg);\n            break;\n        case JCC_P:\n            cc = gen_prepare_eflags_p(s, reg);\n            break;\n        case JCC_L:\n            gen_compute_eflags(s);\n            if (TCGV_EQUAL(reg, cpu_cc_src)) {\n                reg = cpu_tmp0;\n            }\n            tcg_gen_shri_tl(reg, cpu_cc_src, 4); /* CC_O -> CC_S */\n            tcg_gen_xor_tl(reg, reg, cpu_cc_src);\n            cc = (CCPrepare) { .cond = TCG_COND_NE, .reg = reg,\n                               .mask = CC_S };\n            break;\n        default:\n        case JCC_LE:\n            gen_compute_eflags(s);\n            if (TCGV_EQUAL(reg, cpu_cc_src)) {\n                reg = cpu_tmp0;\n            }\n            tcg_gen_shri_tl(reg, cpu_cc_src, 4); /* CC_O -> CC_S */\n            tcg_gen_xor_tl(reg, reg, cpu_cc_src);\n            cc = (CCPrepare) { .cond = TCG_COND_NE, .reg = reg,\n                               .mask = CC_S | CC_Z };\n            break;\n        }\n        break;\n    }\n\n    if (inv) {\n        cc.cond = tcg_invert_cond(cc.cond);\n    }\n    return cc;\n}\n\nstatic void gen_setcc1(DisasContext *s, int b, TCGv reg)\n{\n    CCPrepare cc = gen_prepare_cc(s, b, reg);\n\n    if (cc.no_setcond) {\n        if (cc.cond == TCG_COND_EQ) {\n            tcg_gen_xori_tl(reg, cc.reg, 1);\n        } else {\n            tcg_gen_mov_tl(reg, cc.reg);\n        }\n        return;\n    }\n\n    if (cc.cond == TCG_COND_NE && !cc.use_reg2 && cc.imm == 0 &&\n        cc.mask != 0 && (cc.mask & (cc.mask - 1)) == 0) {\n        tcg_gen_shri_tl(reg, cc.reg, ctztl(cc.mask));\n        tcg_gen_andi_tl(reg, reg, 1);\n        return;\n    }\n    if (cc.mask != -1) {\n        tcg_gen_andi_tl(reg, cc.reg, cc.mask);\n        cc.reg = reg;\n    }\n    if (cc.use_reg2) {\n        tcg_gen_setcond_tl(cc.cond, reg, cc.reg, cc.reg2);\n    } else {\n        tcg_gen_setcondi_tl(cc.cond, reg, cc.reg, cc.imm);\n    }\n}\n\nstatic inline void gen_compute_eflags_c(DisasContext *s, TCGv reg)\n{\n    gen_setcc1(s, JCC_B << 1, reg);\n}\n\n/* generate a conditional jump to label 'l1' according to jump opcode\n   value 'b'. In the fast case, T0 is guaranted not to be used. */\nstatic inline void gen_jcc1_noeob(DisasContext *s, int b, TCGLabel *l1)\n{\n    CCPrepare cc = gen_prepare_cc(s, b, cpu_T0);\n\n    if (cc.mask != -1) {\n        tcg_gen_andi_tl(cpu_T0, cc.reg, cc.mask);\n        cc.reg = cpu_T0;\n    }\n    if (cc.use_reg2) {\n        tcg_gen_brcond_tl(cc.cond, cc.reg, cc.reg2, l1);\n    } else {\n        tcg_gen_brcondi_tl(cc.cond, cc.reg, cc.imm, l1);\n    }\n}\n\n/* Generate a conditional jump to label 'l1' according to jump opcode\n   value 'b'. In the fast case, T0 is guaranted not to be used.\n   A translation block must end soon.  */\nstatic inline void gen_jcc1(DisasContext *s, int b, TCGLabel *l1)\n{\n    CCPrepare cc = gen_prepare_cc(s, b, cpu_T0);\n\n    gen_update_cc_op(s);\n    if (cc.mask != -1) {\n        tcg_gen_andi_tl(cpu_T0, cc.reg, cc.mask);\n        cc.reg = cpu_T0;\n    }\n    set_cc_op(s, CC_OP_DYNAMIC);\n    if (cc.use_reg2) {\n        tcg_gen_brcond_tl(cc.cond, cc.reg, cc.reg2, l1);\n    } else {\n        tcg_gen_brcondi_tl(cc.cond, cc.reg, cc.imm, l1);\n    }\n}\n\n/* XXX: does not work with gdbstub \"ice\" single step - not a\n   serious problem */\nstatic TCGLabel *gen_jz_ecx_string(DisasContext *s, target_ulong next_eip)\n{\n    TCGLabel *l1 = gen_new_label();\n    TCGLabel *l2 = gen_new_label();\n    gen_op_jnz_ecx(s->aflag, l1);\n    gen_set_label(l2);\n    gen_jmp_tb(s, next_eip, 1);\n    gen_set_label(l1);\n    return l2;\n}\n\nstatic inline void gen_stos(DisasContext *s, TCGMemOp ot)\n{\n    gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n    gen_string_movl_A0_EDI(s);\n    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n}\n\nstatic inline void gen_lods(DisasContext *s, TCGMemOp ot)\n{\n    gen_string_movl_A0_ESI(s);\n    gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    gen_op_mov_reg_v(ot, R_EAX, cpu_T0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_ESI);\n}\n\nstatic inline void gen_scas(DisasContext *s, TCGMemOp ot)\n{\n    gen_string_movl_A0_EDI(s);\n    gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n    gen_op(s, OP_CMPL, ot, R_EAX);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n}\n\nstatic inline void gen_cmps(DisasContext *s, TCGMemOp ot)\n{\n    gen_string_movl_A0_EDI(s);\n    gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n    gen_string_movl_A0_ESI(s);\n    gen_op(s, OP_CMPL, ot, OR_TMP0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_ESI);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n}\n\nstatic void gen_bpt_io(DisasContext *s, TCGv_i32 t_port, int ot)\n{\n    if (s->flags & HF_IOBPT_MASK) {\n        TCGv_i32 t_size = tcg_const_i32(1 << ot);\n        TCGv t_next = tcg_const_tl(s->pc - s->cs_base);\n\n        gen_helper_bpt_io(cpu_env, t_port, t_size, t_next);\n        tcg_temp_free_i32(t_size);\n        tcg_temp_free(t_next);\n    }\n}\n\n\nstatic inline void gen_ins(DisasContext *s, TCGMemOp ot)\n{\n    if (s->tb->cflags & CF_USE_ICOUNT) {\n        gen_io_start();\n    }\n    gen_string_movl_A0_EDI(s);\n    /* Note: we must do this dummy write first to be restartable in\n       case of page fault. */\n    tcg_gen_movi_tl(cpu_T0, 0);\n    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n    tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_EDX]);\n    tcg_gen_andi_i32(cpu_tmp2_i32, cpu_tmp2_i32, 0xffff);\n    gen_helper_in_func(ot, cpu_T0, cpu_tmp2_i32);\n    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n    gen_bpt_io(s, cpu_tmp2_i32, ot);\n    if (s->tb->cflags & CF_USE_ICOUNT) {\n        gen_io_end();\n    }\n}\n\nstatic inline void gen_outs(DisasContext *s, TCGMemOp ot)\n{\n    if (s->tb->cflags & CF_USE_ICOUNT) {\n        gen_io_start();\n    }\n    gen_string_movl_A0_ESI(s);\n    gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n\n    tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_EDX]);\n    tcg_gen_andi_i32(cpu_tmp2_i32, cpu_tmp2_i32, 0xffff);\n    tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T0);\n    gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_ESI);\n    gen_bpt_io(s, cpu_tmp2_i32, ot);\n    if (s->tb->cflags & CF_USE_ICOUNT) {\n        gen_io_end();\n    }\n}\n\n/* same method as Valgrind : we generate jumps to current or next\n   instruction */\n#define GEN_REPZ(op)                                                          \\\nstatic inline void gen_repz_ ## op(DisasContext *s, TCGMemOp ot,              \\\n                                 target_ulong cur_eip, target_ulong next_eip) \\\n{                                                                             \\\n    TCGLabel *l2;                                                             \\\n    gen_update_cc_op(s);                                                      \\\n    l2 = gen_jz_ecx_string(s, next_eip);                                      \\\n    gen_ ## op(s, ot);                                                        \\\n    gen_op_add_reg_im(s->aflag, R_ECX, -1);                                   \\\n    /* a loop would cause two single step exceptions if ECX = 1               \\\n       before rep string_insn */                                              \\\n    if (s->repz_opt)                                                          \\\n        gen_op_jz_ecx(s->aflag, l2);                                          \\\n    gen_jmp(s, cur_eip);                                                      \\\n}\n\n#define GEN_REPZ2(op)                                                         \\\nstatic inline void gen_repz_ ## op(DisasContext *s, TCGMemOp ot,              \\\n                                   target_ulong cur_eip,                      \\\n                                   target_ulong next_eip,                     \\\n                                   int nz)                                    \\\n{                                                                             \\\n    TCGLabel *l2;                                                             \\\n    gen_update_cc_op(s);                                                      \\\n    l2 = gen_jz_ecx_string(s, next_eip);                                      \\\n    gen_ ## op(s, ot);                                                        \\\n    gen_op_add_reg_im(s->aflag, R_ECX, -1);                                   \\\n    gen_update_cc_op(s);                                                      \\\n    gen_jcc1(s, (JCC_Z << 1) | (nz ^ 1), l2);                                 \\\n    if (s->repz_opt)                                                          \\\n        gen_op_jz_ecx(s->aflag, l2);                                          \\\n    gen_jmp(s, cur_eip);                                                      \\\n}\n\nGEN_REPZ(movs)\nGEN_REPZ(stos)\nGEN_REPZ(lods)\nGEN_REPZ(ins)\nGEN_REPZ(outs)\nGEN_REPZ2(scas)\nGEN_REPZ2(cmps)\n\nstatic void gen_helper_fp_arith_ST0_FT0(int op)\n{\n    switch (op) {\n    case 0:\n        gen_helper_fadd_ST0_FT0(cpu_env);\n        break;\n    case 1:\n        gen_helper_fmul_ST0_FT0(cpu_env);\n        break;\n    case 2:\n        gen_helper_fcom_ST0_FT0(cpu_env);\n        break;\n    case 3:\n        gen_helper_fcom_ST0_FT0(cpu_env);\n        break;\n    case 4:\n        gen_helper_fsub_ST0_FT0(cpu_env);\n        break;\n    case 5:\n        gen_helper_fsubr_ST0_FT0(cpu_env);\n        break;\n    case 6:\n        gen_helper_fdiv_ST0_FT0(cpu_env);\n        break;\n    case 7:\n        gen_helper_fdivr_ST0_FT0(cpu_env);\n        break;\n    }\n}\n\n/* NOTE the exception in \"r\" op ordering */\nstatic void gen_helper_fp_arith_STN_ST0(int op, int opreg)\n{\n    TCGv_i32 tmp = tcg_const_i32(opreg);\n    switch (op) {\n    case 0:\n        gen_helper_fadd_STN_ST0(cpu_env, tmp);\n        break;\n    case 1:\n        gen_helper_fmul_STN_ST0(cpu_env, tmp);\n        break;\n    case 4:\n        gen_helper_fsubr_STN_ST0(cpu_env, tmp);\n        break;\n    case 5:\n        gen_helper_fsub_STN_ST0(cpu_env, tmp);\n        break;\n    case 6:\n        gen_helper_fdivr_STN_ST0(cpu_env, tmp);\n        break;\n    case 7:\n        gen_helper_fdiv_STN_ST0(cpu_env, tmp);\n        break;\n    }\n}\n\n/* if d == OR_TMP0, it means memory operand (address in A0) */\nstatic void gen_op(DisasContext *s1, int op, TCGMemOp ot, int d)\n{\n    if (d != OR_TMP0) {\n        gen_op_mov_v_reg(ot, cpu_T0, d);\n    } else if (!(s1->prefix & PREFIX_LOCK)) {\n        gen_op_ld_v(s1, ot, cpu_T0, cpu_A0);\n    }\n    switch(op) {\n    case OP_ADCL:\n        gen_compute_eflags_c(s1, cpu_tmp4);\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_add_tl(cpu_T0, cpu_tmp4, cpu_T1);\n            tcg_gen_atomic_add_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_tmp4);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update3_cc(cpu_tmp4);\n        set_cc_op(s1, CC_OP_ADCB + ot);\n        break;\n    case OP_SBBL:\n        gen_compute_eflags_c(s1, cpu_tmp4);\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_add_tl(cpu_T0, cpu_T1, cpu_tmp4);\n            tcg_gen_neg_tl(cpu_T0, cpu_T0);\n            tcg_gen_atomic_add_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_sub_tl(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_sub_tl(cpu_T0, cpu_T0, cpu_tmp4);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update3_cc(cpu_tmp4);\n        set_cc_op(s1, CC_OP_SBBB + ot);\n        break;\n    case OP_ADDL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_atomic_add_fetch_tl(cpu_T0, cpu_A0, cpu_T1,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update2_cc();\n        set_cc_op(s1, CC_OP_ADDB + ot);\n        break;\n    case OP_SUBL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_neg_tl(cpu_T0, cpu_T1);\n            tcg_gen_atomic_fetch_add_tl(cpu_cc_srcT, cpu_A0, cpu_T0,\n                                        s1->mem_index, ot | MO_LE);\n            tcg_gen_sub_tl(cpu_T0, cpu_cc_srcT, cpu_T1);\n        } else {\n            tcg_gen_mov_tl(cpu_cc_srcT, cpu_T0);\n            tcg_gen_sub_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update2_cc();\n        set_cc_op(s1, CC_OP_SUBB + ot);\n        break;\n    default:\n    case OP_ANDL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_atomic_and_fetch_tl(cpu_T0, cpu_A0, cpu_T1,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_and_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update1_cc();\n        set_cc_op(s1, CC_OP_LOGICB + ot);\n        break;\n    case OP_ORL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_atomic_or_fetch_tl(cpu_T0, cpu_A0, cpu_T1,\n                                       s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update1_cc();\n        set_cc_op(s1, CC_OP_LOGICB + ot);\n        break;\n    case OP_XORL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_atomic_xor_fetch_tl(cpu_T0, cpu_A0, cpu_T1,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_xor_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update1_cc();\n        set_cc_op(s1, CC_OP_LOGICB + ot);\n        break;\n    case OP_CMPL:\n        tcg_gen_mov_tl(cpu_cc_src, cpu_T1);\n        tcg_gen_mov_tl(cpu_cc_srcT, cpu_T0);\n        tcg_gen_sub_tl(cpu_cc_dst, cpu_T0, cpu_T1);\n        set_cc_op(s1, CC_OP_SUBB + ot);\n        break;\n    }\n}\n\n/* if d == OR_TMP0, it means memory operand (address in A0) */\nstatic void gen_inc(DisasContext *s1, TCGMemOp ot, int d, int c)\n{\n    if (s1->prefix & PREFIX_LOCK) {\n        tcg_gen_movi_tl(cpu_T0, c > 0 ? 1 : -1);\n        tcg_gen_atomic_add_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                    s1->mem_index, ot | MO_LE);\n    } else {\n        if (d != OR_TMP0) {\n            gen_op_mov_v_reg(ot, cpu_T0, d);\n        } else {\n            gen_op_ld_v(s1, ot, cpu_T0, cpu_A0);\n        }\n        tcg_gen_addi_tl(cpu_T0, cpu_T0, (c > 0 ? 1 : -1));\n        gen_op_st_rm_T0_A0(s1, ot, d);\n    }\n\n    gen_compute_eflags_c(s1, cpu_cc_src);\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n    set_cc_op(s1, (c > 0 ? CC_OP_INCB : CC_OP_DECB) + ot);\n}\n\nstatic void gen_shift_flags(DisasContext *s, TCGMemOp ot, TCGv result,\n                            TCGv shm1, TCGv count, bool is_right)\n{\n    TCGv_i32 z32, s32, oldop;\n    TCGv z_tl;\n\n    /* Store the results into the CC variables.  If we know that the\n       variable must be dead, store unconditionally.  Otherwise we'll\n       need to not disrupt the current contents.  */\n    z_tl = tcg_const_tl(0);\n    if (cc_op_live[s->cc_op] & USES_CC_DST) {\n        tcg_gen_movcond_tl(TCG_COND_NE, cpu_cc_dst, count, z_tl,\n                           result, cpu_cc_dst);\n    } else {\n        tcg_gen_mov_tl(cpu_cc_dst, result);\n    }\n    if (cc_op_live[s->cc_op] & USES_CC_SRC) {\n        tcg_gen_movcond_tl(TCG_COND_NE, cpu_cc_src, count, z_tl,\n                           shm1, cpu_cc_src);\n    } else {\n        tcg_gen_mov_tl(cpu_cc_src, shm1);\n    }\n    tcg_temp_free(z_tl);\n\n    /* Get the two potential CC_OP values into temporaries.  */\n    tcg_gen_movi_i32(cpu_tmp2_i32, (is_right ? CC_OP_SARB : CC_OP_SHLB) + ot);\n    if (s->cc_op == CC_OP_DYNAMIC) {\n        oldop = cpu_cc_op;\n    } else {\n        tcg_gen_movi_i32(cpu_tmp3_i32, s->cc_op);\n        oldop = cpu_tmp3_i32;\n    }\n\n    /* Conditionally store the CC_OP value.  */\n    z32 = tcg_const_i32(0);\n    s32 = tcg_temp_new_i32();\n    tcg_gen_trunc_tl_i32(s32, count);\n    tcg_gen_movcond_i32(TCG_COND_NE, cpu_cc_op, s32, z32, cpu_tmp2_i32, oldop);\n    tcg_temp_free_i32(z32);\n    tcg_temp_free_i32(s32);\n\n    /* The CC_OP value is no longer predictable.  */\n    set_cc_op(s, CC_OP_DYNAMIC);\n}\n\nstatic void gen_shift_rm_T1(DisasContext *s, TCGMemOp ot, int op1,\n                            int is_right, int is_arith)\n{\n    target_ulong mask = (ot == MO_64 ? 0x3f : 0x1f);\n\n    /* load */\n    if (op1 == OR_TMP0) {\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    }\n\n    tcg_gen_andi_tl(cpu_T1, cpu_T1, mask);\n    tcg_gen_subi_tl(cpu_tmp0, cpu_T1, 1);\n\n    if (is_right) {\n        if (is_arith) {\n            gen_exts(ot, cpu_T0);\n            tcg_gen_sar_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n            tcg_gen_sar_tl(cpu_T0, cpu_T0, cpu_T1);\n        } else {\n            gen_extu(ot, cpu_T0);\n            tcg_gen_shr_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n            tcg_gen_shr_tl(cpu_T0, cpu_T0, cpu_T1);\n        }\n    } else {\n        tcg_gen_shl_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n        tcg_gen_shl_tl(cpu_T0, cpu_T0, cpu_T1);\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    gen_shift_flags(s, ot, cpu_T0, cpu_tmp0, cpu_T1, is_right);\n}\n\nstatic void gen_shift_rm_im(DisasContext *s, TCGMemOp ot, int op1, int op2,\n                            int is_right, int is_arith)\n{\n    int mask = (ot == MO_64 ? 0x3f : 0x1f);\n\n    /* load */\n    if (op1 == OR_TMP0)\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    else\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n\n    op2 &= mask;\n    if (op2 != 0) {\n        if (is_right) {\n            if (is_arith) {\n                gen_exts(ot, cpu_T0);\n                tcg_gen_sari_tl(cpu_tmp4, cpu_T0, op2 - 1);\n                tcg_gen_sari_tl(cpu_T0, cpu_T0, op2);\n            } else {\n                gen_extu(ot, cpu_T0);\n                tcg_gen_shri_tl(cpu_tmp4, cpu_T0, op2 - 1);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, op2);\n            }\n        } else {\n            tcg_gen_shli_tl(cpu_tmp4, cpu_T0, op2 - 1);\n            tcg_gen_shli_tl(cpu_T0, cpu_T0, op2);\n        }\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    /* update eflags if non zero shift */\n    if (op2 != 0) {\n        tcg_gen_mov_tl(cpu_cc_src, cpu_tmp4);\n        tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n        set_cc_op(s, (is_right ? CC_OP_SARB : CC_OP_SHLB) + ot);\n    }\n}\n\nstatic void gen_rot_rm_T1(DisasContext *s, TCGMemOp ot, int op1, int is_right)\n{\n    target_ulong mask = (ot == MO_64 ? 0x3f : 0x1f);\n    TCGv_i32 t0, t1;\n\n    /* load */\n    if (op1 == OR_TMP0) {\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    }\n\n    tcg_gen_andi_tl(cpu_T1, cpu_T1, mask);\n\n    switch (ot) {\n    case MO_8:\n        /* Replicate the 8-bit input so that a 32-bit rotate works.  */\n        tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n        tcg_gen_muli_tl(cpu_T0, cpu_T0, 0x01010101);\n        goto do_long;\n    case MO_16:\n        /* Replicate the 16-bit input so that a 32-bit rotate works.  */\n        tcg_gen_deposit_tl(cpu_T0, cpu_T0, cpu_T0, 16, 16);\n        goto do_long;\n    do_long:\n#ifdef TARGET_X86_64\n    case MO_32:\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        if (is_right) {\n            tcg_gen_rotr_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n        } else {\n            tcg_gen_rotl_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n        }\n        tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n        break;\n#endif\n    default:\n        if (is_right) {\n            tcg_gen_rotr_tl(cpu_T0, cpu_T0, cpu_T1);\n        } else {\n            tcg_gen_rotl_tl(cpu_T0, cpu_T0, cpu_T1);\n        }\n        break;\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    /* We'll need the flags computed into CC_SRC.  */\n    gen_compute_eflags(s);\n\n    /* The value that was \"rotated out\" is now present at the other end\n       of the word.  Compute C into CC_DST and O into CC_SRC2.  Note that\n       since we've computed the flags into CC_SRC, these variables are\n       currently dead.  */\n    if (is_right) {\n        tcg_gen_shri_tl(cpu_cc_src2, cpu_T0, mask - 1);\n        tcg_gen_shri_tl(cpu_cc_dst, cpu_T0, mask);\n        tcg_gen_andi_tl(cpu_cc_dst, cpu_cc_dst, 1);\n    } else {\n        tcg_gen_shri_tl(cpu_cc_src2, cpu_T0, mask);\n        tcg_gen_andi_tl(cpu_cc_dst, cpu_T0, 1);\n    }\n    tcg_gen_andi_tl(cpu_cc_src2, cpu_cc_src2, 1);\n    tcg_gen_xor_tl(cpu_cc_src2, cpu_cc_src2, cpu_cc_dst);\n\n    /* Now conditionally store the new CC_OP value.  If the shift count\n       is 0 we keep the CC_OP_EFLAGS setting so that only CC_SRC is live.\n       Otherwise reuse CC_OP_ADCOX which have the C and O flags split out\n       exactly as we computed above.  */\n    t0 = tcg_const_i32(0);\n    t1 = tcg_temp_new_i32();\n    tcg_gen_trunc_tl_i32(t1, cpu_T1);\n    tcg_gen_movi_i32(cpu_tmp2_i32, CC_OP_ADCOX); \n    tcg_gen_movi_i32(cpu_tmp3_i32, CC_OP_EFLAGS);\n    tcg_gen_movcond_i32(TCG_COND_NE, cpu_cc_op, t1, t0,\n                        cpu_tmp2_i32, cpu_tmp3_i32);\n    tcg_temp_free_i32(t0);\n    tcg_temp_free_i32(t1);\n\n    /* The CC_OP value is no longer predictable.  */ \n    set_cc_op(s, CC_OP_DYNAMIC);\n}\n\nstatic void gen_rot_rm_im(DisasContext *s, TCGMemOp ot, int op1, int op2,\n                          int is_right)\n{\n    int mask = (ot == MO_64 ? 0x3f : 0x1f);\n    int shift;\n\n    /* load */\n    if (op1 == OR_TMP0) {\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    }\n\n    op2 &= mask;\n    if (op2 != 0) {\n        switch (ot) {\n#ifdef TARGET_X86_64\n        case MO_32:\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n            if (is_right) {\n                tcg_gen_rotri_i32(cpu_tmp2_i32, cpu_tmp2_i32, op2);\n            } else {\n                tcg_gen_rotli_i32(cpu_tmp2_i32, cpu_tmp2_i32, op2);\n            }\n            tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n            break;\n#endif\n        default:\n            if (is_right) {\n                tcg_gen_rotri_tl(cpu_T0, cpu_T0, op2);\n            } else {\n                tcg_gen_rotli_tl(cpu_T0, cpu_T0, op2);\n            }\n            break;\n        case MO_8:\n            mask = 7;\n            goto do_shifts;\n        case MO_16:\n            mask = 15;\n        do_shifts:\n            shift = op2 & mask;\n            if (is_right) {\n                shift = mask + 1 - shift;\n            }\n            gen_extu(ot, cpu_T0);\n            tcg_gen_shli_tl(cpu_tmp0, cpu_T0, shift);\n            tcg_gen_shri_tl(cpu_T0, cpu_T0, mask + 1 - shift);\n            tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_tmp0);\n            break;\n        }\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    if (op2 != 0) {\n        /* Compute the flags into CC_SRC.  */\n        gen_compute_eflags(s);\n\n        /* The value that was \"rotated out\" is now present at the other end\n           of the word.  Compute C into CC_DST and O into CC_SRC2.  Note that\n           since we've computed the flags into CC_SRC, these variables are\n           currently dead.  */\n        if (is_right) {\n            tcg_gen_shri_tl(cpu_cc_src2, cpu_T0, mask - 1);\n            tcg_gen_shri_tl(cpu_cc_dst, cpu_T0, mask);\n            tcg_gen_andi_tl(cpu_cc_dst, cpu_cc_dst, 1);\n        } else {\n            tcg_gen_shri_tl(cpu_cc_src2, cpu_T0, mask);\n            tcg_gen_andi_tl(cpu_cc_dst, cpu_T0, 1);\n        }\n        tcg_gen_andi_tl(cpu_cc_src2, cpu_cc_src2, 1);\n        tcg_gen_xor_tl(cpu_cc_src2, cpu_cc_src2, cpu_cc_dst);\n        set_cc_op(s, CC_OP_ADCOX);\n    }\n}\n\n/* XXX: add faster immediate = 1 case */\nstatic void gen_rotc_rm_T1(DisasContext *s, TCGMemOp ot, int op1,\n                           int is_right)\n{\n    gen_compute_eflags(s);\n    assert(s->cc_op == CC_OP_EFLAGS);\n\n    /* load */\n    if (op1 == OR_TMP0)\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    else\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    \n    if (is_right) {\n        switch (ot) {\n        case MO_8:\n            gen_helper_rcrb(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n        case MO_16:\n            gen_helper_rcrw(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n        case MO_32:\n            gen_helper_rcrl(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_helper_rcrq(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n#endif\n        default:\n            tcg_abort();\n        }\n    } else {\n        switch (ot) {\n        case MO_8:\n            gen_helper_rclb(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n        case MO_16:\n            gen_helper_rclw(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n        case MO_32:\n            gen_helper_rcll(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_helper_rclq(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n#endif\n        default:\n            tcg_abort();\n        }\n    }\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n}\n\n/* XXX: add faster immediate case */\nstatic void gen_shiftd_rm_T1(DisasContext *s, TCGMemOp ot, int op1,\n                             bool is_right, TCGv count_in)\n{\n    target_ulong mask = (ot == MO_64 ? 63 : 31);\n    TCGv count;\n\n    /* load */\n    if (op1 == OR_TMP0) {\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    }\n\n    count = tcg_temp_new();\n    tcg_gen_andi_tl(count, count_in, mask);\n\n    switch (ot) {\n    case MO_16:\n        /* Note: we implement the Intel behaviour for shift count > 16.\n           This means \"shrdw C, B, A\" shifts A:B:A >> C.  Build the B:A\n           portion by constructing it as a 32-bit value.  */\n        if (is_right) {\n            tcg_gen_deposit_tl(cpu_tmp0, cpu_T0, cpu_T1, 16, 16);\n            tcg_gen_mov_tl(cpu_T1, cpu_T0);\n            tcg_gen_mov_tl(cpu_T0, cpu_tmp0);\n        } else {\n            tcg_gen_deposit_tl(cpu_T1, cpu_T0, cpu_T1, 16, 16);\n        }\n        /* FALLTHRU */\n#ifdef TARGET_X86_64\n    case MO_32:\n        /* Concatenate the two 32-bit values and use a 64-bit shift.  */\n        tcg_gen_subi_tl(cpu_tmp0, count, 1);\n        if (is_right) {\n            tcg_gen_concat_tl_i64(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_shr_i64(cpu_tmp0, cpu_T0, cpu_tmp0);\n            tcg_gen_shr_i64(cpu_T0, cpu_T0, count);\n        } else {\n            tcg_gen_concat_tl_i64(cpu_T0, cpu_T1, cpu_T0);\n            tcg_gen_shl_i64(cpu_tmp0, cpu_T0, cpu_tmp0);\n            tcg_gen_shl_i64(cpu_T0, cpu_T0, count);\n            tcg_gen_shri_i64(cpu_tmp0, cpu_tmp0, 32);\n            tcg_gen_shri_i64(cpu_T0, cpu_T0, 32);\n        }\n        break;\n#endif\n    default:\n        tcg_gen_subi_tl(cpu_tmp0, count, 1);\n        if (is_right) {\n            tcg_gen_shr_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n\n            tcg_gen_subfi_tl(cpu_tmp4, mask + 1, count);\n            tcg_gen_shr_tl(cpu_T0, cpu_T0, count);\n            tcg_gen_shl_tl(cpu_T1, cpu_T1, cpu_tmp4);\n        } else {\n            tcg_gen_shl_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n            if (ot == MO_16) {\n                /* Only needed if count > 16, for Intel behaviour.  */\n                tcg_gen_subfi_tl(cpu_tmp4, 33, count);\n                tcg_gen_shr_tl(cpu_tmp4, cpu_T1, cpu_tmp4);\n                tcg_gen_or_tl(cpu_tmp0, cpu_tmp0, cpu_tmp4);\n            }\n\n            tcg_gen_subfi_tl(cpu_tmp4, mask + 1, count);\n            tcg_gen_shl_tl(cpu_T0, cpu_T0, count);\n            tcg_gen_shr_tl(cpu_T1, cpu_T1, cpu_tmp4);\n        }\n        tcg_gen_movi_tl(cpu_tmp4, 0);\n        tcg_gen_movcond_tl(TCG_COND_EQ, cpu_T1, count, cpu_tmp4,\n                           cpu_tmp4, cpu_T1);\n        tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_T1);\n        break;\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    gen_shift_flags(s, ot, cpu_T0, cpu_tmp0, count, is_right);\n    tcg_temp_free(count);\n}\n\nstatic void gen_shift(DisasContext *s1, int op, TCGMemOp ot, int d, int s)\n{\n    if (s != OR_TMP1)\n        gen_op_mov_v_reg(ot, cpu_T1, s);\n    switch(op) {\n    case OP_ROL:\n        gen_rot_rm_T1(s1, ot, d, 0);\n        break;\n    case OP_ROR:\n        gen_rot_rm_T1(s1, ot, d, 1);\n        break;\n    case OP_SHL:\n    case OP_SHL1:\n        gen_shift_rm_T1(s1, ot, d, 0, 0);\n        break;\n    case OP_SHR:\n        gen_shift_rm_T1(s1, ot, d, 1, 0);\n        break;\n    case OP_SAR:\n        gen_shift_rm_T1(s1, ot, d, 1, 1);\n        break;\n    case OP_RCL:\n        gen_rotc_rm_T1(s1, ot, d, 0);\n        break;\n    case OP_RCR:\n        gen_rotc_rm_T1(s1, ot, d, 1);\n        break;\n    }\n}\n\nstatic void gen_shifti(DisasContext *s1, int op, TCGMemOp ot, int d, int c)\n{\n    switch(op) {\n    case OP_ROL:\n        gen_rot_rm_im(s1, ot, d, c, 0);\n        break;\n    case OP_ROR:\n        gen_rot_rm_im(s1, ot, d, c, 1);\n        break;\n    case OP_SHL:\n    case OP_SHL1:\n        gen_shift_rm_im(s1, ot, d, c, 0, 0);\n        break;\n    case OP_SHR:\n        gen_shift_rm_im(s1, ot, d, c, 1, 0);\n        break;\n    case OP_SAR:\n        gen_shift_rm_im(s1, ot, d, c, 1, 1);\n        break;\n    default:\n        /* currently not optimized */\n        tcg_gen_movi_tl(cpu_T1, c);\n        gen_shift(s1, op, ot, d, OR_TMP1);\n        break;\n    }\n}\n\n/* Decompose an address.  */\n\ntypedef struct AddressParts {\n    int def_seg;\n    int base;\n    int index;\n    int scale;\n    target_long disp;\n} AddressParts;\n\nstatic AddressParts gen_lea_modrm_0(CPUX86State *env, DisasContext *s,\n                                    int modrm)\n{\n    int def_seg, base, index, scale, mod, rm;\n    target_long disp;\n    bool havesib;\n\n    def_seg = R_DS;\n    index = -1;\n    scale = 0;\n    disp = 0;\n\n    mod = (modrm >> 6) & 3;\n    rm = modrm & 7;\n    base = rm | REX_B(s);\n\n    if (mod == 3) {\n        /* Normally filtered out earlier, but including this path\n           simplifies multi-byte nop, as well as bndcl, bndcu, bndcn.  */\n        goto done;\n    }\n\n    switch (s->aflag) {\n    case MO_64:\n    case MO_32:\n        havesib = 0;\n        if (rm == 4) {\n            int code = cpu_ldub_code(env, s->pc++);\n            scale = (code >> 6) & 3;\n            index = ((code >> 3) & 7) | REX_X(s);\n            if (index == 4) {\n                index = -1;  /* no index */\n            }\n            base = (code & 7) | REX_B(s);\n            havesib = 1;\n        }\n\n        switch (mod) {\n        case 0:\n            if ((base & 7) == 5) {\n                base = -1;\n                disp = (int32_t)cpu_ldl_code(env, s->pc);\n                s->pc += 4;\n                if (CODE64(s) && !havesib) {\n                    base = -2;\n                    disp += s->pc + s->rip_offset;\n                }\n            }\n            break;\n        case 1:\n            disp = (int8_t)cpu_ldub_code(env, s->pc++);\n            break;\n        default:\n        case 2:\n            disp = (int32_t)cpu_ldl_code(env, s->pc);\n            s->pc += 4;\n            break;\n        }\n\n        /* For correct popl handling with esp.  */\n        if (base == R_ESP && s->popl_esp_hack) {\n            disp += s->popl_esp_hack;\n        }\n        if (base == R_EBP || base == R_ESP) {\n            def_seg = R_SS;\n        }\n        break;\n\n    case MO_16:\n        if (mod == 0) {\n            if (rm == 6) {\n                base = -1;\n                disp = cpu_lduw_code(env, s->pc);\n                s->pc += 2;\n                break;\n            }\n        } else if (mod == 1) {\n            disp = (int8_t)cpu_ldub_code(env, s->pc++);\n        } else {\n            disp = (int16_t)cpu_lduw_code(env, s->pc);\n            s->pc += 2;\n        }\n\n        switch (rm) {\n        case 0:\n            base = R_EBX;\n            index = R_ESI;\n            break;\n        case 1:\n            base = R_EBX;\n            index = R_EDI;\n            break;\n        case 2:\n            base = R_EBP;\n            index = R_ESI;\n            def_seg = R_SS;\n            break;\n        case 3:\n            base = R_EBP;\n            index = R_EDI;\n            def_seg = R_SS;\n            break;\n        case 4:\n            base = R_ESI;\n            break;\n        case 5:\n            base = R_EDI;\n            break;\n        case 6:\n            base = R_EBP;\n            def_seg = R_SS;\n            break;\n        default:\n        case 7:\n            base = R_EBX;\n            break;\n        }\n        break;\n\n    default:\n        tcg_abort();\n    }\n\n done:\n    return (AddressParts){ def_seg, base, index, scale, disp };\n}\n\n/* Compute the address, with a minimum number of TCG ops.  */\nstatic TCGv gen_lea_modrm_1(AddressParts a)\n{\n    TCGv ea;\n\n    TCGV_UNUSED(ea);\n    if (a.index >= 0) {\n        if (a.scale == 0) {\n            ea = cpu_regs[a.index];\n        } else {\n            tcg_gen_shli_tl(cpu_A0, cpu_regs[a.index], a.scale);\n            ea = cpu_A0;\n        }\n        if (a.base >= 0) {\n            tcg_gen_add_tl(cpu_A0, ea, cpu_regs[a.base]);\n            ea = cpu_A0;\n        }\n    } else if (a.base >= 0) {\n        ea = cpu_regs[a.base];\n    }\n    if (TCGV_IS_UNUSED(ea)) {\n        tcg_gen_movi_tl(cpu_A0, a.disp);\n        ea = cpu_A0;\n    } else if (a.disp != 0) {\n        tcg_gen_addi_tl(cpu_A0, ea, a.disp);\n        ea = cpu_A0;\n    }\n\n    return ea;\n}\n\nstatic void gen_lea_modrm(CPUX86State *env, DisasContext *s, int modrm)\n{\n    AddressParts a = gen_lea_modrm_0(env, s, modrm);\n    TCGv ea = gen_lea_modrm_1(a);\n    gen_lea_v_seg(s, s->aflag, ea, a.def_seg, s->override);\n}\n\nstatic void gen_nop_modrm(CPUX86State *env, DisasContext *s, int modrm)\n{\n    (void)gen_lea_modrm_0(env, s, modrm);\n}\n\n/* Used for BNDCL, BNDCU, BNDCN.  */\nstatic void gen_bndck(CPUX86State *env, DisasContext *s, int modrm,\n                      TCGCond cond, TCGv_i64 bndv)\n{\n    TCGv ea = gen_lea_modrm_1(gen_lea_modrm_0(env, s, modrm));\n\n    tcg_gen_extu_tl_i64(cpu_tmp1_i64, ea);\n    if (!CODE64(s)) {\n        tcg_gen_ext32u_i64(cpu_tmp1_i64, cpu_tmp1_i64);\n    }\n    tcg_gen_setcond_i64(cond, cpu_tmp1_i64, cpu_tmp1_i64, bndv);\n    tcg_gen_extrl_i64_i32(cpu_tmp2_i32, cpu_tmp1_i64);\n    gen_helper_bndck(cpu_env, cpu_tmp2_i32);\n}\n\n/* used for LEA and MOV AX, mem */\nstatic void gen_add_A0_ds_seg(DisasContext *s)\n{\n    gen_lea_v_seg(s, s->aflag, cpu_A0, R_DS, s->override);\n}\n\n/* generate modrm memory load or store of 'reg'. TMP0 is used if reg ==\n   OR_TMP0 */\nstatic void gen_ldst_modrm(CPUX86State *env, DisasContext *s, int modrm,\n                           TCGMemOp ot, int reg, int is_store)\n{\n    int mod, rm;\n\n    mod = (modrm >> 6) & 3;\n    rm = (modrm & 7) | REX_B(s);\n    if (mod == 3) {\n        if (is_store) {\n            if (reg != OR_TMP0)\n                gen_op_mov_v_reg(ot, cpu_T0, reg);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n            if (reg != OR_TMP0)\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n        }\n    } else {\n        gen_lea_modrm(env, s, modrm);\n        if (is_store) {\n            if (reg != OR_TMP0)\n                gen_op_mov_v_reg(ot, cpu_T0, reg);\n            gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            if (reg != OR_TMP0)\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n        }\n    }\n}\n\nstatic inline uint32_t insn_get(CPUX86State *env, DisasContext *s, TCGMemOp ot)\n{\n    uint32_t ret;\n\n    switch (ot) {\n    case MO_8:\n        ret = cpu_ldub_code(env, s->pc);\n        s->pc++;\n        break;\n    case MO_16:\n        ret = cpu_lduw_code(env, s->pc);\n        s->pc += 2;\n        break;\n    case MO_32:\n#ifdef TARGET_X86_64\n    case MO_64:\n#endif\n        ret = cpu_ldl_code(env, s->pc);\n        s->pc += 4;\n        break;\n    default:\n        tcg_abort();\n    }\n    return ret;\n}\n\nstatic inline int insn_const_size(TCGMemOp ot)\n{\n    if (ot <= MO_32) {\n        return 1 << ot;\n    } else {\n        return 4;\n    }\n}\n\nstatic inline bool use_goto_tb(DisasContext *s, target_ulong pc)\n{\n#ifndef CONFIG_USER_ONLY\n    return (pc & TARGET_PAGE_MASK) == (s->tb->pc & TARGET_PAGE_MASK) ||\n           (pc & TARGET_PAGE_MASK) == (s->pc_start & TARGET_PAGE_MASK);\n#else\n    return true;\n#endif\n}\n\nstatic inline void gen_goto_tb(DisasContext *s, int tb_num, target_ulong eip)\n{\n    target_ulong pc = s->cs_base + eip;\n\n    if (use_goto_tb(s, pc))  {\n        /* jump to same page: we can use a direct jump */\n        tcg_gen_goto_tb(tb_num);\n        gen_jmp_im(eip);\n        tcg_gen_exit_tb((uintptr_t)s->tb + tb_num);\n    } else {\n        /* jump to another page: currently not optimized */\n        gen_jmp_im(eip);\n        gen_eob(s);\n    }\n}\n\nstatic inline void gen_jcc(DisasContext *s, int b,\n                           target_ulong val, target_ulong next_eip)\n{\n    TCGLabel *l1, *l2;\n\n    if (s->jmp_opt) {\n        l1 = gen_new_label();\n        gen_jcc1(s, b, l1);\n\n        gen_goto_tb(s, 0, next_eip);\n\n        gen_set_label(l1);\n        gen_goto_tb(s, 1, val);\n        s->is_jmp = DISAS_TB_JUMP;\n    } else {\n        l1 = gen_new_label();\n        l2 = gen_new_label();\n        gen_jcc1(s, b, l1);\n\n        gen_jmp_im(next_eip);\n        tcg_gen_br(l2);\n\n        gen_set_label(l1);\n        gen_jmp_im(val);\n        gen_set_label(l2);\n        gen_eob(s);\n    }\n}\n\nstatic void gen_cmovcc1(CPUX86State *env, DisasContext *s, TCGMemOp ot, int b,\n                        int modrm, int reg)\n{\n    CCPrepare cc;\n\n    gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n\n    cc = gen_prepare_cc(s, b, cpu_T1);\n    if (cc.mask != -1) {\n        TCGv t0 = tcg_temp_new();\n        tcg_gen_andi_tl(t0, cc.reg, cc.mask);\n        cc.reg = t0;\n    }\n    if (!cc.use_reg2) {\n        cc.reg2 = tcg_const_tl(cc.imm);\n    }\n\n    tcg_gen_movcond_tl(cc.cond, cpu_T0, cc.reg, cc.reg2,\n                       cpu_T0, cpu_regs[reg]);\n    gen_op_mov_reg_v(ot, reg, cpu_T0);\n\n    if (cc.mask != -1) {\n        tcg_temp_free(cc.reg);\n    }\n    if (!cc.use_reg2) {\n        tcg_temp_free(cc.reg2);\n    }\n}\n\nstatic inline void gen_op_movl_T0_seg(int seg_reg)\n{\n    tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                     offsetof(CPUX86State,segs[seg_reg].selector));\n}\n\nstatic inline void gen_op_movl_seg_T0_vm(int seg_reg)\n{\n    tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n    tcg_gen_st32_tl(cpu_T0, cpu_env,\n                    offsetof(CPUX86State,segs[seg_reg].selector));\n    tcg_gen_shli_tl(cpu_seg_base[seg_reg], cpu_T0, 4);\n}\n\n/* move T0 to seg_reg and compute if the CPU state may change. Never\n   call this function with seg_reg == R_CS */\nstatic void gen_movl_seg_T0(DisasContext *s, int seg_reg)\n{\n    if (s->pe && !s->vm86) {\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        gen_helper_load_seg(cpu_env, tcg_const_i32(seg_reg), cpu_tmp2_i32);\n        /* abort translation because the addseg value may change or\n           because ss32 may change. For R_SS, translation must always\n           stop as a special handling must be done to disable hardware\n           interrupts for the next instruction */\n        if (seg_reg == R_SS || (s->code32 && seg_reg < R_FS))\n            s->is_jmp = DISAS_TB_JUMP;\n    } else {\n        gen_op_movl_seg_T0_vm(seg_reg);\n        if (seg_reg == R_SS)\n            s->is_jmp = DISAS_TB_JUMP;\n    }\n}\n\nstatic inline int svm_is_rep(int prefixes)\n{\n    return ((prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) ? 8 : 0);\n}\n\nstatic inline void\ngen_svm_check_intercept_param(DisasContext *s, target_ulong pc_start,\n                              uint32_t type, uint64_t param)\n{\n    /* no SVM activated; fast case */\n    if (likely(!(s->flags & HF_SVMI_MASK)))\n        return;\n    gen_update_cc_op(s);\n    gen_jmp_im(pc_start - s->cs_base);\n    gen_helper_svm_check_intercept_param(cpu_env, tcg_const_i32(type),\n                                         tcg_const_i64(param));\n}\n\nstatic inline void\ngen_svm_check_intercept(DisasContext *s, target_ulong pc_start, uint64_t type)\n{\n    gen_svm_check_intercept_param(s, pc_start, type, 0);\n}\n\nstatic inline void gen_stack_update(DisasContext *s, int addend)\n{\n    gen_op_add_reg_im(mo_stacksize(s), R_ESP, addend);\n}\n\n/* Generate a push. It depends on ss32, addseg and dflag.  */\nstatic void gen_push_v(DisasContext *s, TCGv val)\n{\n    TCGMemOp d_ot = mo_pushpop(s, s->dflag);\n    TCGMemOp a_ot = mo_stacksize(s);\n    int size = 1 << d_ot;\n    TCGv new_esp = cpu_A0;\n\n    tcg_gen_subi_tl(cpu_A0, cpu_regs[R_ESP], size);\n\n    if (!CODE64(s)) {\n        if (s->addseg) {\n            new_esp = cpu_tmp4;\n            tcg_gen_mov_tl(new_esp, cpu_A0);\n        }\n        gen_lea_v_seg(s, a_ot, cpu_A0, R_SS, -1);\n    }\n\n    gen_op_st_v(s, d_ot, val, cpu_A0);\n    gen_op_mov_reg_v(a_ot, R_ESP, new_esp);\n}\n\n/* two step pop is necessary for precise exceptions */\nstatic TCGMemOp gen_pop_T0(DisasContext *s)\n{\n    TCGMemOp d_ot = mo_pushpop(s, s->dflag);\n\n    gen_lea_v_seg(s, mo_stacksize(s), cpu_regs[R_ESP], R_SS, -1);\n    gen_op_ld_v(s, d_ot, cpu_T0, cpu_A0);\n\n    return d_ot;\n}\n\nstatic inline void gen_pop_update(DisasContext *s, TCGMemOp ot)\n{\n    gen_stack_update(s, 1 << ot);\n}\n\nstatic inline void gen_stack_A0(DisasContext *s)\n{\n    gen_lea_v_seg(s, s->ss32 ? MO_32 : MO_16, cpu_regs[R_ESP], R_SS, -1);\n}\n\nstatic void gen_pusha(DisasContext *s)\n{\n    TCGMemOp s_ot = s->ss32 ? MO_32 : MO_16;\n    TCGMemOp d_ot = s->dflag;\n    int size = 1 << d_ot;\n    int i;\n\n    for (i = 0; i < 8; i++) {\n        tcg_gen_addi_tl(cpu_A0, cpu_regs[R_ESP], (i - 8) * size);\n        gen_lea_v_seg(s, s_ot, cpu_A0, R_SS, -1);\n        gen_op_st_v(s, d_ot, cpu_regs[7 - i], cpu_A0);\n    }\n\n    gen_stack_update(s, -8 * size);\n}\n\nstatic void gen_popa(DisasContext *s)\n{\n    TCGMemOp s_ot = s->ss32 ? MO_32 : MO_16;\n    TCGMemOp d_ot = s->dflag;\n    int size = 1 << d_ot;\n    int i;\n\n    for (i = 0; i < 8; i++) {\n        /* ESP is not reloaded */\n        if (7 - i == R_ESP) {\n            continue;\n        }\n        tcg_gen_addi_tl(cpu_A0, cpu_regs[R_ESP], i * size);\n        gen_lea_v_seg(s, s_ot, cpu_A0, R_SS, -1);\n        gen_op_ld_v(s, d_ot, cpu_T0, cpu_A0);\n        gen_op_mov_reg_v(d_ot, 7 - i, cpu_T0);\n    }\n\n    gen_stack_update(s, 8 * size);\n}\n\nstatic void gen_enter(DisasContext *s, int esp_addend, int level)\n{\n    TCGMemOp d_ot = mo_pushpop(s, s->dflag);\n    TCGMemOp a_ot = CODE64(s) ? MO_64 : s->ss32 ? MO_32 : MO_16;\n    int size = 1 << d_ot;\n\n    /* Push BP; compute FrameTemp into T1.  */\n    tcg_gen_subi_tl(cpu_T1, cpu_regs[R_ESP], size);\n    gen_lea_v_seg(s, a_ot, cpu_T1, R_SS, -1);\n    gen_op_st_v(s, d_ot, cpu_regs[R_EBP], cpu_A0);\n\n    level &= 31;\n    if (level != 0) {\n        int i;\n\n        /* Copy level-1 pointers from the previous frame.  */\n        for (i = 1; i < level; ++i) {\n            tcg_gen_subi_tl(cpu_A0, cpu_regs[R_EBP], size * i);\n            gen_lea_v_seg(s, a_ot, cpu_A0, R_SS, -1);\n            gen_op_ld_v(s, d_ot, cpu_tmp0, cpu_A0);\n\n            tcg_gen_subi_tl(cpu_A0, cpu_T1, size * i);\n            gen_lea_v_seg(s, a_ot, cpu_A0, R_SS, -1);\n            gen_op_st_v(s, d_ot, cpu_tmp0, cpu_A0);\n        }\n\n        /* Push the current FrameTemp as the last level.  */\n        tcg_gen_subi_tl(cpu_A0, cpu_T1, size * level);\n        gen_lea_v_seg(s, a_ot, cpu_A0, R_SS, -1);\n        gen_op_st_v(s, d_ot, cpu_T1, cpu_A0);\n    }\n\n    /* Copy the FrameTemp value to EBP.  */\n    gen_op_mov_reg_v(a_ot, R_EBP, cpu_T1);\n\n    /* Compute the final value of ESP.  */\n    tcg_gen_subi_tl(cpu_T1, cpu_T1, esp_addend + size * level);\n    gen_op_mov_reg_v(a_ot, R_ESP, cpu_T1);\n}\n\nstatic void gen_leave(DisasContext *s)\n{\n    TCGMemOp d_ot = mo_pushpop(s, s->dflag);\n    TCGMemOp a_ot = mo_stacksize(s);\n\n    gen_lea_v_seg(s, a_ot, cpu_regs[R_EBP], R_SS, -1);\n    gen_op_ld_v(s, d_ot, cpu_T0, cpu_A0);\n\n    tcg_gen_addi_tl(cpu_T1, cpu_regs[R_EBP], 1 << d_ot);\n\n    gen_op_mov_reg_v(d_ot, R_EBP, cpu_T0);\n    gen_op_mov_reg_v(a_ot, R_ESP, cpu_T1);\n}\n\nstatic void gen_exception(DisasContext *s, int trapno, target_ulong cur_eip)\n{\n    gen_update_cc_op(s);\n    gen_jmp_im(cur_eip);\n    gen_helper_raise_exception(cpu_env, tcg_const_i32(trapno));\n    s->is_jmp = DISAS_TB_JUMP;\n}\n\n/* Generate #UD for the current instruction.  The assumption here is that\n   the instruction is known, but it isn't allowed in the current cpu mode.  */\nstatic void gen_illegal_opcode(DisasContext *s)\n{\n    gen_exception(s, EXCP06_ILLOP, s->pc_start - s->cs_base);\n}\n\n/* Similarly, except that the assumption here is that we don't decode\n   the instruction at all -- either a missing opcode, an unimplemented\n   feature, or just a bogus instruction stream.  */\nstatic void gen_unknown_opcode(CPUX86State *env, DisasContext *s)\n{\n    gen_illegal_opcode(s);\n\n    if (qemu_loglevel_mask(LOG_UNIMP)) {\n        target_ulong pc = s->pc_start, end = s->pc;\n        qemu_log_lock();\n        qemu_log(\"ILLOPC: \" TARGET_FMT_lx \":\", pc);\n        for (; pc < end; ++pc) {\n            qemu_log(\" %02x\", cpu_ldub_code(env, pc));\n        }\n        qemu_log(\"\\n\");\n        qemu_log_unlock();\n    }\n}\n\n/* an interrupt is different from an exception because of the\n   privilege checks */\nstatic void gen_interrupt(DisasContext *s, int intno,\n                          target_ulong cur_eip, target_ulong next_eip)\n{\n    gen_update_cc_op(s);\n    gen_jmp_im(cur_eip);\n    gen_helper_raise_interrupt(cpu_env, tcg_const_i32(intno),\n                               tcg_const_i32(next_eip - cur_eip));\n    s->is_jmp = DISAS_TB_JUMP;\n}\n\nstatic void gen_debug(DisasContext *s, target_ulong cur_eip)\n{\n    gen_update_cc_op(s);\n    gen_jmp_im(cur_eip);\n    gen_helper_debug(cpu_env);\n    s->is_jmp = DISAS_TB_JUMP;\n}\n\nstatic void gen_set_hflag(DisasContext *s, uint32_t mask)\n{\n    if ((s->flags & mask) == 0) {\n        TCGv_i32 t = tcg_temp_new_i32();\n        tcg_gen_ld_i32(t, cpu_env, offsetof(CPUX86State, hflags));\n        tcg_gen_ori_i32(t, t, mask);\n        tcg_gen_st_i32(t, cpu_env, offsetof(CPUX86State, hflags));\n        tcg_temp_free_i32(t);\n        s->flags |= mask;\n    }\n}\n\nstatic void gen_reset_hflag(DisasContext *s, uint32_t mask)\n{\n    if (s->flags & mask) {\n        TCGv_i32 t = tcg_temp_new_i32();\n        tcg_gen_ld_i32(t, cpu_env, offsetof(CPUX86State, hflags));\n        tcg_gen_andi_i32(t, t, ~mask);\n        tcg_gen_st_i32(t, cpu_env, offsetof(CPUX86State, hflags));\n        tcg_temp_free_i32(t);\n        s->flags &= ~mask;\n    }\n}\n\n/* Clear BND registers during legacy branches.  */\nstatic void gen_bnd_jmp(DisasContext *s)\n{\n    /* Clear the registers only if BND prefix is missing, MPX is enabled,\n       and if the BNDREGs are known to be in use (non-zero) already.\n       The helper itself will check BNDPRESERVE at runtime.  */\n    if ((s->prefix & PREFIX_REPNZ) == 0\n        && (s->flags & HF_MPX_EN_MASK) != 0\n        && (s->flags & HF_MPX_IU_MASK) != 0) {\n        gen_helper_bnd_jmp(cpu_env);\n    }\n}\n\n/* Generate an end of block. Trace exception is also generated if needed.\n   If INHIBIT, set HF_INHIBIT_IRQ_MASK if it isn't already set.\n   If RECHECK_TF, emit a rechecking helper for #DB, ignoring the state of\n   S->TF.  This is used by the syscall/sysret insns.  */\nstatic void gen_eob_worker(DisasContext *s, bool inhibit, bool recheck_tf)\n{\n    gen_update_cc_op(s);\n\n    /* If several instructions disable interrupts, only the first does it.  */\n    if (inhibit && !(s->flags & HF_INHIBIT_IRQ_MASK)) {\n        gen_set_hflag(s, HF_INHIBIT_IRQ_MASK);\n    } else {\n        gen_reset_hflag(s, HF_INHIBIT_IRQ_MASK);\n    }\n\n    if (s->tb->flags & HF_RF_MASK) {\n        gen_helper_reset_rf(cpu_env);\n    }\n    if (s->singlestep_enabled) {\n        gen_helper_debug(cpu_env);\n    } else if (recheck_tf) {\n        gen_helper_rechecking_single_step(cpu_env);\n        tcg_gen_exit_tb(0);\n    } else if (s->tf) {\n        gen_helper_single_step(cpu_env);\n    } else {\n        tcg_gen_exit_tb(0);\n    }\n    s->is_jmp = DISAS_TB_JUMP;\n}\n\n/* End of block.\n   If INHIBIT, set HF_INHIBIT_IRQ_MASK if it isn't already set.  */\nstatic void gen_eob_inhibit_irq(DisasContext *s, bool inhibit)\n{\n    gen_eob_worker(s, inhibit, false);\n}\n\n/* End of block, resetting the inhibit irq flag.  */\nstatic void gen_eob(DisasContext *s)\n{\n    gen_eob_worker(s, false, false);\n}\n\n/* generate a jump to eip. No segment change must happen before as a\n   direct call to the next block may occur */\nstatic void gen_jmp_tb(DisasContext *s, target_ulong eip, int tb_num)\n{\n    gen_update_cc_op(s);\n    set_cc_op(s, CC_OP_DYNAMIC);\n    if (s->jmp_opt) {\n        gen_goto_tb(s, tb_num, eip);\n        s->is_jmp = DISAS_TB_JUMP;\n    } else {\n        gen_jmp_im(eip);\n        gen_eob(s);\n    }\n}\n\nstatic void gen_jmp(DisasContext *s, target_ulong eip)\n{\n    gen_jmp_tb(s, eip, 0);\n}\n\nstatic inline void gen_ldq_env_A0(DisasContext *s, int offset)\n{\n    tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, offset);\n}\n\nstatic inline void gen_stq_env_A0(DisasContext *s, int offset)\n{\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, offset);\n    tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n}\n\nstatic inline void gen_ldo_env_A0(DisasContext *s, int offset)\n{\n    int mem_index = s->mem_index;\n    tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0, mem_index, MO_LEQ);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, offset + offsetof(ZMMReg, ZMM_Q(0)));\n    tcg_gen_addi_tl(cpu_tmp0, cpu_A0, 8);\n    tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_tmp0, mem_index, MO_LEQ);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, offset + offsetof(ZMMReg, ZMM_Q(1)));\n}\n\nstatic inline void gen_sto_env_A0(DisasContext *s, int offset)\n{\n    int mem_index = s->mem_index;\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, offset + offsetof(ZMMReg, ZMM_Q(0)));\n    tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0, mem_index, MO_LEQ);\n    tcg_gen_addi_tl(cpu_tmp0, cpu_A0, 8);\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, offset + offsetof(ZMMReg, ZMM_Q(1)));\n    tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_tmp0, mem_index, MO_LEQ);\n}\n\nstatic inline void gen_op_movo(int d_offset, int s_offset)\n{\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, s_offset + offsetof(ZMMReg, ZMM_Q(0)));\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, d_offset + offsetof(ZMMReg, ZMM_Q(0)));\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, s_offset + offsetof(ZMMReg, ZMM_Q(1)));\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, d_offset + offsetof(ZMMReg, ZMM_Q(1)));\n}\n\nstatic inline void gen_op_movq(int d_offset, int s_offset)\n{\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, s_offset);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, d_offset);\n}\n\nstatic inline void gen_op_movl(int d_offset, int s_offset)\n{\n    tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env, s_offset);\n    tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, d_offset);\n}\n\nstatic inline void gen_op_movq_env_0(int d_offset)\n{\n    tcg_gen_movi_i64(cpu_tmp1_i64, 0);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, d_offset);\n}\n\ntypedef void (*SSEFunc_i_ep)(TCGv_i32 val, TCGv_ptr env, TCGv_ptr reg);\ntypedef void (*SSEFunc_l_ep)(TCGv_i64 val, TCGv_ptr env, TCGv_ptr reg);\ntypedef void (*SSEFunc_0_epi)(TCGv_ptr env, TCGv_ptr reg, TCGv_i32 val);\ntypedef void (*SSEFunc_0_epl)(TCGv_ptr env, TCGv_ptr reg, TCGv_i64 val);\ntypedef void (*SSEFunc_0_epp)(TCGv_ptr env, TCGv_ptr reg_a, TCGv_ptr reg_b);\ntypedef void (*SSEFunc_0_eppi)(TCGv_ptr env, TCGv_ptr reg_a, TCGv_ptr reg_b,\n                               TCGv_i32 val);\ntypedef void (*SSEFunc_0_ppi)(TCGv_ptr reg_a, TCGv_ptr reg_b, TCGv_i32 val);\ntypedef void (*SSEFunc_0_eppt)(TCGv_ptr env, TCGv_ptr reg_a, TCGv_ptr reg_b,\n                               TCGv val);\n\n#define SSE_SPECIAL ((void *)1)\n#define SSE_DUMMY ((void *)2)\n\n#define MMX_OP2(x) { gen_helper_ ## x ## _mmx, gen_helper_ ## x ## _xmm }\n#define SSE_FOP(x) { gen_helper_ ## x ## ps, gen_helper_ ## x ## pd, \\\n                     gen_helper_ ## x ## ss, gen_helper_ ## x ## sd, }\n\nstatic const SSEFunc_0_epp sse_op_table1[256][4] = {\n    /* 3DNow! extensions */\n    [0x0e] = { SSE_DUMMY }, /* femms */\n    [0x0f] = { SSE_DUMMY }, /* pf... */\n    /* pure SSE operations */\n    [0x10] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movups, movupd, movss, movsd */\n    [0x11] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movups, movupd, movss, movsd */\n    [0x12] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movlps, movlpd, movsldup, movddup */\n    [0x13] = { SSE_SPECIAL, SSE_SPECIAL },  /* movlps, movlpd */\n    [0x14] = { gen_helper_punpckldq_xmm, gen_helper_punpcklqdq_xmm },\n    [0x15] = { gen_helper_punpckhdq_xmm, gen_helper_punpckhqdq_xmm },\n    [0x16] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL },  /* movhps, movhpd, movshdup */\n    [0x17] = { SSE_SPECIAL, SSE_SPECIAL },  /* movhps, movhpd */\n\n    [0x28] = { SSE_SPECIAL, SSE_SPECIAL },  /* movaps, movapd */\n    [0x29] = { SSE_SPECIAL, SSE_SPECIAL },  /* movaps, movapd */\n    [0x2a] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* cvtpi2ps, cvtpi2pd, cvtsi2ss, cvtsi2sd */\n    [0x2b] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movntps, movntpd, movntss, movntsd */\n    [0x2c] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* cvttps2pi, cvttpd2pi, cvttsd2si, cvttss2si */\n    [0x2d] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* cvtps2pi, cvtpd2pi, cvtsd2si, cvtss2si */\n    [0x2e] = { gen_helper_ucomiss, gen_helper_ucomisd },\n    [0x2f] = { gen_helper_comiss, gen_helper_comisd },\n    [0x50] = { SSE_SPECIAL, SSE_SPECIAL }, /* movmskps, movmskpd */\n    [0x51] = SSE_FOP(sqrt),\n    [0x52] = { gen_helper_rsqrtps, NULL, gen_helper_rsqrtss, NULL },\n    [0x53] = { gen_helper_rcpps, NULL, gen_helper_rcpss, NULL },\n    [0x54] = { gen_helper_pand_xmm, gen_helper_pand_xmm }, /* andps, andpd */\n    [0x55] = { gen_helper_pandn_xmm, gen_helper_pandn_xmm }, /* andnps, andnpd */\n    [0x56] = { gen_helper_por_xmm, gen_helper_por_xmm }, /* orps, orpd */\n    [0x57] = { gen_helper_pxor_xmm, gen_helper_pxor_xmm }, /* xorps, xorpd */\n    [0x58] = SSE_FOP(add),\n    [0x59] = SSE_FOP(mul),\n    [0x5a] = { gen_helper_cvtps2pd, gen_helper_cvtpd2ps,\n               gen_helper_cvtss2sd, gen_helper_cvtsd2ss },\n    [0x5b] = { gen_helper_cvtdq2ps, gen_helper_cvtps2dq, gen_helper_cvttps2dq },\n    [0x5c] = SSE_FOP(sub),\n    [0x5d] = SSE_FOP(min),\n    [0x5e] = SSE_FOP(div),\n    [0x5f] = SSE_FOP(max),\n\n    [0xc2] = SSE_FOP(cmpeq),\n    [0xc6] = { (SSEFunc_0_epp)gen_helper_shufps,\n               (SSEFunc_0_epp)gen_helper_shufpd }, /* XXX: casts */\n\n    /* SSSE3, SSE4, MOVBE, CRC32, BMI1, BMI2, ADX.  */\n    [0x38] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL },\n    [0x3a] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL },\n\n    /* MMX ops and their SSE extensions */\n    [0x60] = MMX_OP2(punpcklbw),\n    [0x61] = MMX_OP2(punpcklwd),\n    [0x62] = MMX_OP2(punpckldq),\n    [0x63] = MMX_OP2(packsswb),\n    [0x64] = MMX_OP2(pcmpgtb),\n    [0x65] = MMX_OP2(pcmpgtw),\n    [0x66] = MMX_OP2(pcmpgtl),\n    [0x67] = MMX_OP2(packuswb),\n    [0x68] = MMX_OP2(punpckhbw),\n    [0x69] = MMX_OP2(punpckhwd),\n    [0x6a] = MMX_OP2(punpckhdq),\n    [0x6b] = MMX_OP2(packssdw),\n    [0x6c] = { NULL, gen_helper_punpcklqdq_xmm },\n    [0x6d] = { NULL, gen_helper_punpckhqdq_xmm },\n    [0x6e] = { SSE_SPECIAL, SSE_SPECIAL }, /* movd mm, ea */\n    [0x6f] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movq, movdqa, , movqdu */\n    [0x70] = { (SSEFunc_0_epp)gen_helper_pshufw_mmx,\n               (SSEFunc_0_epp)gen_helper_pshufd_xmm,\n               (SSEFunc_0_epp)gen_helper_pshufhw_xmm,\n               (SSEFunc_0_epp)gen_helper_pshuflw_xmm }, /* XXX: casts */\n    [0x71] = { SSE_SPECIAL, SSE_SPECIAL }, /* shiftw */\n    [0x72] = { SSE_SPECIAL, SSE_SPECIAL }, /* shiftd */\n    [0x73] = { SSE_SPECIAL, SSE_SPECIAL }, /* shiftq */\n    [0x74] = MMX_OP2(pcmpeqb),\n    [0x75] = MMX_OP2(pcmpeqw),\n    [0x76] = MMX_OP2(pcmpeql),\n    [0x77] = { SSE_DUMMY }, /* emms */\n    [0x78] = { NULL, SSE_SPECIAL, NULL, SSE_SPECIAL }, /* extrq_i, insertq_i */\n    [0x79] = { NULL, gen_helper_extrq_r, NULL, gen_helper_insertq_r },\n    [0x7c] = { NULL, gen_helper_haddpd, NULL, gen_helper_haddps },\n    [0x7d] = { NULL, gen_helper_hsubpd, NULL, gen_helper_hsubps },\n    [0x7e] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movd, movd, , movq */\n    [0x7f] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movq, movdqa, movdqu */\n    [0xc4] = { SSE_SPECIAL, SSE_SPECIAL }, /* pinsrw */\n    [0xc5] = { SSE_SPECIAL, SSE_SPECIAL }, /* pextrw */\n    [0xd0] = { NULL, gen_helper_addsubpd, NULL, gen_helper_addsubps },\n    [0xd1] = MMX_OP2(psrlw),\n    [0xd2] = MMX_OP2(psrld),\n    [0xd3] = MMX_OP2(psrlq),\n    [0xd4] = MMX_OP2(paddq),\n    [0xd5] = MMX_OP2(pmullw),\n    [0xd6] = { NULL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL },\n    [0xd7] = { SSE_SPECIAL, SSE_SPECIAL }, /* pmovmskb */\n    [0xd8] = MMX_OP2(psubusb),\n    [0xd9] = MMX_OP2(psubusw),\n    [0xda] = MMX_OP2(pminub),\n    [0xdb] = MMX_OP2(pand),\n    [0xdc] = MMX_OP2(paddusb),\n    [0xdd] = MMX_OP2(paddusw),\n    [0xde] = MMX_OP2(pmaxub),\n    [0xdf] = MMX_OP2(pandn),\n    [0xe0] = MMX_OP2(pavgb),\n    [0xe1] = MMX_OP2(psraw),\n    [0xe2] = MMX_OP2(psrad),\n    [0xe3] = MMX_OP2(pavgw),\n    [0xe4] = MMX_OP2(pmulhuw),\n    [0xe5] = MMX_OP2(pmulhw),\n    [0xe6] = { NULL, gen_helper_cvttpd2dq, gen_helper_cvtdq2pd, gen_helper_cvtpd2dq },\n    [0xe7] = { SSE_SPECIAL , SSE_SPECIAL },  /* movntq, movntq */\n    [0xe8] = MMX_OP2(psubsb),\n    [0xe9] = MMX_OP2(psubsw),\n    [0xea] = MMX_OP2(pminsw),\n    [0xeb] = MMX_OP2(por),\n    [0xec] = MMX_OP2(paddsb),\n    [0xed] = MMX_OP2(paddsw),\n    [0xee] = MMX_OP2(pmaxsw),\n    [0xef] = MMX_OP2(pxor),\n    [0xf0] = { NULL, NULL, NULL, SSE_SPECIAL }, /* lddqu */\n    [0xf1] = MMX_OP2(psllw),\n    [0xf2] = MMX_OP2(pslld),\n    [0xf3] = MMX_OP2(psllq),\n    [0xf4] = MMX_OP2(pmuludq),\n    [0xf5] = MMX_OP2(pmaddwd),\n    [0xf6] = MMX_OP2(psadbw),\n    [0xf7] = { (SSEFunc_0_epp)gen_helper_maskmov_mmx,\n               (SSEFunc_0_epp)gen_helper_maskmov_xmm }, /* XXX: casts */\n    [0xf8] = MMX_OP2(psubb),\n    [0xf9] = MMX_OP2(psubw),\n    [0xfa] = MMX_OP2(psubl),\n    [0xfb] = MMX_OP2(psubq),\n    [0xfc] = MMX_OP2(paddb),\n    [0xfd] = MMX_OP2(paddw),\n    [0xfe] = MMX_OP2(paddl),\n};\n\nstatic const SSEFunc_0_epp sse_op_table2[3 * 8][2] = {\n    [0 + 2] = MMX_OP2(psrlw),\n    [0 + 4] = MMX_OP2(psraw),\n    [0 + 6] = MMX_OP2(psllw),\n    [8 + 2] = MMX_OP2(psrld),\n    [8 + 4] = MMX_OP2(psrad),\n    [8 + 6] = MMX_OP2(pslld),\n    [16 + 2] = MMX_OP2(psrlq),\n    [16 + 3] = { NULL, gen_helper_psrldq_xmm },\n    [16 + 6] = MMX_OP2(psllq),\n    [16 + 7] = { NULL, gen_helper_pslldq_xmm },\n};\n\nstatic const SSEFunc_0_epi sse_op_table3ai[] = {\n    gen_helper_cvtsi2ss,\n    gen_helper_cvtsi2sd\n};\n\n#ifdef TARGET_X86_64\nstatic const SSEFunc_0_epl sse_op_table3aq[] = {\n    gen_helper_cvtsq2ss,\n    gen_helper_cvtsq2sd\n};\n#endif\n\nstatic const SSEFunc_i_ep sse_op_table3bi[] = {\n    gen_helper_cvttss2si,\n    gen_helper_cvtss2si,\n    gen_helper_cvttsd2si,\n    gen_helper_cvtsd2si\n};\n\n#ifdef TARGET_X86_64\nstatic const SSEFunc_l_ep sse_op_table3bq[] = {\n    gen_helper_cvttss2sq,\n    gen_helper_cvtss2sq,\n    gen_helper_cvttsd2sq,\n    gen_helper_cvtsd2sq\n};\n#endif\n\nstatic const SSEFunc_0_epp sse_op_table4[8][4] = {\n    SSE_FOP(cmpeq),\n    SSE_FOP(cmplt),\n    SSE_FOP(cmple),\n    SSE_FOP(cmpunord),\n    SSE_FOP(cmpneq),\n    SSE_FOP(cmpnlt),\n    SSE_FOP(cmpnle),\n    SSE_FOP(cmpord),\n};\n\nstatic const SSEFunc_0_epp sse_op_table5[256] = {\n    [0x0c] = gen_helper_pi2fw,\n    [0x0d] = gen_helper_pi2fd,\n    [0x1c] = gen_helper_pf2iw,\n    [0x1d] = gen_helper_pf2id,\n    [0x8a] = gen_helper_pfnacc,\n    [0x8e] = gen_helper_pfpnacc,\n    [0x90] = gen_helper_pfcmpge,\n    [0x94] = gen_helper_pfmin,\n    [0x96] = gen_helper_pfrcp,\n    [0x97] = gen_helper_pfrsqrt,\n    [0x9a] = gen_helper_pfsub,\n    [0x9e] = gen_helper_pfadd,\n    [0xa0] = gen_helper_pfcmpgt,\n    [0xa4] = gen_helper_pfmax,\n    [0xa6] = gen_helper_movq, /* pfrcpit1; no need to actually increase precision */\n    [0xa7] = gen_helper_movq, /* pfrsqit1 */\n    [0xaa] = gen_helper_pfsubr,\n    [0xae] = gen_helper_pfacc,\n    [0xb0] = gen_helper_pfcmpeq,\n    [0xb4] = gen_helper_pfmul,\n    [0xb6] = gen_helper_movq, /* pfrcpit2 */\n    [0xb7] = gen_helper_pmulhrw_mmx,\n    [0xbb] = gen_helper_pswapd,\n    [0xbf] = gen_helper_pavgb_mmx /* pavgusb */\n};\n\nstruct SSEOpHelper_epp {\n    SSEFunc_0_epp op[2];\n    uint32_t ext_mask;\n};\n\nstruct SSEOpHelper_eppi {\n    SSEFunc_0_eppi op[2];\n    uint32_t ext_mask;\n};\n\n#define SSSE3_OP(x) { MMX_OP2(x), CPUID_EXT_SSSE3 }\n#define SSE41_OP(x) { { NULL, gen_helper_ ## x ## _xmm }, CPUID_EXT_SSE41 }\n#define SSE42_OP(x) { { NULL, gen_helper_ ## x ## _xmm }, CPUID_EXT_SSE42 }\n#define SSE41_SPECIAL { { NULL, SSE_SPECIAL }, CPUID_EXT_SSE41 }\n#define PCLMULQDQ_OP(x) { { NULL, gen_helper_ ## x ## _xmm }, \\\n        CPUID_EXT_PCLMULQDQ }\n#define AESNI_OP(x) { { NULL, gen_helper_ ## x ## _xmm }, CPUID_EXT_AES }\n\nstatic const struct SSEOpHelper_epp sse_op_table6[256] = {\n    [0x00] = SSSE3_OP(pshufb),\n    [0x01] = SSSE3_OP(phaddw),\n    [0x02] = SSSE3_OP(phaddd),\n    [0x03] = SSSE3_OP(phaddsw),\n    [0x04] = SSSE3_OP(pmaddubsw),\n    [0x05] = SSSE3_OP(phsubw),\n    [0x06] = SSSE3_OP(phsubd),\n    [0x07] = SSSE3_OP(phsubsw),\n    [0x08] = SSSE3_OP(psignb),\n    [0x09] = SSSE3_OP(psignw),\n    [0x0a] = SSSE3_OP(psignd),\n    [0x0b] = SSSE3_OP(pmulhrsw),\n    [0x10] = SSE41_OP(pblendvb),\n    [0x14] = SSE41_OP(blendvps),\n    [0x15] = SSE41_OP(blendvpd),\n    [0x17] = SSE41_OP(ptest),\n    [0x1c] = SSSE3_OP(pabsb),\n    [0x1d] = SSSE3_OP(pabsw),\n    [0x1e] = SSSE3_OP(pabsd),\n    [0x20] = SSE41_OP(pmovsxbw),\n    [0x21] = SSE41_OP(pmovsxbd),\n    [0x22] = SSE41_OP(pmovsxbq),\n    [0x23] = SSE41_OP(pmovsxwd),\n    [0x24] = SSE41_OP(pmovsxwq),\n    [0x25] = SSE41_OP(pmovsxdq),\n    [0x28] = SSE41_OP(pmuldq),\n    [0x29] = SSE41_OP(pcmpeqq),\n    [0x2a] = SSE41_SPECIAL, /* movntqda */\n    [0x2b] = SSE41_OP(packusdw),\n    [0x30] = SSE41_OP(pmovzxbw),\n    [0x31] = SSE41_OP(pmovzxbd),\n    [0x32] = SSE41_OP(pmovzxbq),\n    [0x33] = SSE41_OP(pmovzxwd),\n    [0x34] = SSE41_OP(pmovzxwq),\n    [0x35] = SSE41_OP(pmovzxdq),\n    [0x37] = SSE42_OP(pcmpgtq),\n    [0x38] = SSE41_OP(pminsb),\n    [0x39] = SSE41_OP(pminsd),\n    [0x3a] = SSE41_OP(pminuw),\n    [0x3b] = SSE41_OP(pminud),\n    [0x3c] = SSE41_OP(pmaxsb),\n    [0x3d] = SSE41_OP(pmaxsd),\n    [0x3e] = SSE41_OP(pmaxuw),\n    [0x3f] = SSE41_OP(pmaxud),\n    [0x40] = SSE41_OP(pmulld),\n    [0x41] = SSE41_OP(phminposuw),\n    [0xdb] = AESNI_OP(aesimc),\n    [0xdc] = AESNI_OP(aesenc),\n    [0xdd] = AESNI_OP(aesenclast),\n    [0xde] = AESNI_OP(aesdec),\n    [0xdf] = AESNI_OP(aesdeclast),\n};\n\nstatic const struct SSEOpHelper_eppi sse_op_table7[256] = {\n    [0x08] = SSE41_OP(roundps),\n    [0x09] = SSE41_OP(roundpd),\n    [0x0a] = SSE41_OP(roundss),\n    [0x0b] = SSE41_OP(roundsd),\n    [0x0c] = SSE41_OP(blendps),\n    [0x0d] = SSE41_OP(blendpd),\n    [0x0e] = SSE41_OP(pblendw),\n    [0x0f] = SSSE3_OP(palignr),\n    [0x14] = SSE41_SPECIAL, /* pextrb */\n    [0x15] = SSE41_SPECIAL, /* pextrw */\n    [0x16] = SSE41_SPECIAL, /* pextrd/pextrq */\n    [0x17] = SSE41_SPECIAL, /* extractps */\n    [0x20] = SSE41_SPECIAL, /* pinsrb */\n    [0x21] = SSE41_SPECIAL, /* insertps */\n    [0x22] = SSE41_SPECIAL, /* pinsrd/pinsrq */\n    [0x40] = SSE41_OP(dpps),\n    [0x41] = SSE41_OP(dppd),\n    [0x42] = SSE41_OP(mpsadbw),\n    [0x44] = PCLMULQDQ_OP(pclmulqdq),\n    [0x60] = SSE42_OP(pcmpestrm),\n    [0x61] = SSE42_OP(pcmpestri),\n    [0x62] = SSE42_OP(pcmpistrm),\n    [0x63] = SSE42_OP(pcmpistri),\n    [0xdf] = AESNI_OP(aeskeygenassist),\n};\n\nstatic void gen_sse(CPUX86State *env, DisasContext *s, int b,\n                    target_ulong pc_start, int rex_r)\n{\n    int b1, op1_offset, op2_offset, is_xmm, val;\n    int modrm, mod, rm, reg;\n    SSEFunc_0_epp sse_fn_epp;\n    SSEFunc_0_eppi sse_fn_eppi;\n    SSEFunc_0_ppi sse_fn_ppi;\n    SSEFunc_0_eppt sse_fn_eppt;\n    TCGMemOp ot;\n\n    b &= 0xff;\n    if (s->prefix & PREFIX_DATA)\n        b1 = 1;\n    else if (s->prefix & PREFIX_REPZ)\n        b1 = 2;\n    else if (s->prefix & PREFIX_REPNZ)\n        b1 = 3;\n    else\n        b1 = 0;\n    sse_fn_epp = sse_op_table1[b][b1];\n    if (!sse_fn_epp) {\n        goto unknown_op;\n    }\n    if ((b <= 0x5f && b >= 0x10) || b == 0xc6 || b == 0xc2) {\n        is_xmm = 1;\n    } else {\n        if (b1 == 0) {\n            /* MMX case */\n            is_xmm = 0;\n        } else {\n            is_xmm = 1;\n        }\n    }\n    /* simple MMX/SSE operation */\n    if (s->flags & HF_TS_MASK) {\n        gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n        return;\n    }\n    if (s->flags & HF_EM_MASK) {\n    illegal_op:\n        gen_illegal_opcode(s);\n        return;\n    }\n    if (is_xmm\n        && !(s->flags & HF_OSFXSR_MASK)\n        && ((b != 0x38 && b != 0x3a) || (s->prefix & PREFIX_DATA))) {\n        goto unknown_op;\n    }\n    if (b == 0x0e) {\n        if (!(s->cpuid_ext2_features & CPUID_EXT2_3DNOW)) {\n            /* If we were fully decoding this we might use illegal_op.  */\n            goto unknown_op;\n        }\n        /* femms */\n        gen_helper_emms(cpu_env);\n        return;\n    }\n    if (b == 0x77) {\n        /* emms */\n        gen_helper_emms(cpu_env);\n        return;\n    }\n    /* prepare MMX state (XXX: optimize by storing fptt and fptags in\n       the static cpu state) */\n    if (!is_xmm) {\n        gen_helper_enter_mmx(cpu_env);\n    }\n\n    modrm = cpu_ldub_code(env, s->pc++);\n    reg = ((modrm >> 3) & 7);\n    if (is_xmm)\n        reg |= rex_r;\n    mod = (modrm >> 6) & 3;\n    if (sse_fn_epp == SSE_SPECIAL) {\n        b |= (b1 << 8);\n        switch(b) {\n        case 0x0e7: /* movntq */\n            if (mod == 3) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_stq_env_A0(s, offsetof(CPUX86State, fpregs[reg].mmx));\n            break;\n        case 0x1e7: /* movntdq */\n        case 0x02b: /* movntps */\n        case 0x12b: /* movntps */\n            if (mod == 3)\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            gen_sto_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            break;\n        case 0x3f0: /* lddqu */\n            if (mod == 3)\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            gen_ldo_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            break;\n        case 0x22b: /* movntss */\n        case 0x32b: /* movntsd */\n            if (mod == 3)\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if (b1 & 1) {\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                    xmm_regs[reg].ZMM_L(0)));\n                gen_op_st_v(s, MO_32, cpu_T0, cpu_A0);\n            }\n            break;\n        case 0x6e: /* movd mm, ea */\n#ifdef TARGET_X86_64\n            if (s->dflag == MO_64) {\n                gen_ldst_modrm(env, s, modrm, MO_64, OR_TMP0, 0);\n                tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State,fpregs[reg].mmx));\n            } else\n#endif\n            {\n                gen_ldst_modrm(env, s, modrm, MO_32, OR_TMP0, 0);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                                 offsetof(CPUX86State,fpregs[reg].mmx));\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_movl_mm_T0_mmx(cpu_ptr0, cpu_tmp2_i32);\n            }\n            break;\n        case 0x16e: /* movd xmm, ea */\n#ifdef TARGET_X86_64\n            if (s->dflag == MO_64) {\n                gen_ldst_modrm(env, s, modrm, MO_64, OR_TMP0, 0);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                                 offsetof(CPUX86State,xmm_regs[reg]));\n                gen_helper_movq_mm_T0_xmm(cpu_ptr0, cpu_T0);\n            } else\n#endif\n            {\n                gen_ldst_modrm(env, s, modrm, MO_32, OR_TMP0, 0);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                                 offsetof(CPUX86State,xmm_regs[reg]));\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_movl_mm_T0_xmm(cpu_ptr0, cpu_tmp2_i32);\n            }\n            break;\n        case 0x6f: /* movq mm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State, fpregs[reg].mmx));\n            } else {\n                rm = (modrm & 7);\n                tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env,\n                               offsetof(CPUX86State,fpregs[rm].mmx));\n                tcg_gen_st_i64(cpu_tmp1_i64, cpu_env,\n                               offsetof(CPUX86State,fpregs[reg].mmx));\n            }\n            break;\n        case 0x010: /* movups */\n        case 0x110: /* movupd */\n        case 0x028: /* movaps */\n        case 0x128: /* movapd */\n        case 0x16f: /* movdqa xmm, ea */\n        case 0x26f: /* movdqu xmm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldo_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movo(offsetof(CPUX86State,xmm_regs[reg]),\n                            offsetof(CPUX86State,xmm_regs[rm]));\n            }\n            break;\n        case 0x210: /* movss xmm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, MO_32, cpu_T0, cpu_A0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n                tcg_gen_movi_tl(cpu_T0, 0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(1)));\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)));\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(0)));\n            }\n            break;\n        case 0x310: /* movsd xmm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n                tcg_gen_movi_tl(cpu_T0, 0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)));\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            }\n            break;\n        case 0x012: /* movlps */\n        case 0x112: /* movlpd */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                /* movhlps */\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(1)));\n            }\n            break;\n        case 0x212: /* movsldup */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldo_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(0)));\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(2)));\n            }\n            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(1)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)));\n            break;\n        case 0x312: /* movddup */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            }\n            gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(1)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)));\n            break;\n        case 0x016: /* movhps */\n        case 0x116: /* movhpd */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(1)));\n            } else {\n                /* movlhps */\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(1)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            }\n            break;\n        case 0x216: /* movshdup */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldo_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(1)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(1)));\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(3)));\n            }\n            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_L(1)));\n            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)));\n            break;\n        case 0x178:\n        case 0x378:\n            {\n                int bit_index, field_length;\n\n                if (b1 == 1 && reg != 0)\n                    goto illegal_op;\n                field_length = cpu_ldub_code(env, s->pc++) & 0x3F;\n                bit_index = cpu_ldub_code(env, s->pc++) & 0x3F;\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env,\n                    offsetof(CPUX86State,xmm_regs[reg]));\n                if (b1 == 1)\n                    gen_helper_extrq_i(cpu_env, cpu_ptr0,\n                                       tcg_const_i32(bit_index),\n                                       tcg_const_i32(field_length));\n                else\n                    gen_helper_insertq_i(cpu_env, cpu_ptr0,\n                                         tcg_const_i32(bit_index),\n                                         tcg_const_i32(field_length));\n            }\n            break;\n        case 0x7e: /* movd ea, mm */\n#ifdef TARGET_X86_64\n            if (s->dflag == MO_64) {\n                tcg_gen_ld_i64(cpu_T0, cpu_env,\n                               offsetof(CPUX86State,fpregs[reg].mmx));\n                gen_ldst_modrm(env, s, modrm, MO_64, OR_TMP0, 1);\n            } else\n#endif\n            {\n                tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                                 offsetof(CPUX86State,fpregs[reg].mmx.MMX_L(0)));\n                gen_ldst_modrm(env, s, modrm, MO_32, OR_TMP0, 1);\n            }\n            break;\n        case 0x17e: /* movd ea, xmm */\n#ifdef TARGET_X86_64\n            if (s->dflag == MO_64) {\n                tcg_gen_ld_i64(cpu_T0, cpu_env,\n                               offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)));\n                gen_ldst_modrm(env, s, modrm, MO_64, OR_TMP0, 1);\n            } else\n#endif\n            {\n                tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                                 offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n                gen_ldst_modrm(env, s, modrm, MO_32, OR_TMP0, 1);\n            }\n            break;\n        case 0x27e: /* movq xmm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            }\n            gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(1)));\n            break;\n        case 0x7f: /* movq ea, mm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State, fpregs[reg].mmx));\n            } else {\n                rm = (modrm & 7);\n                gen_op_movq(offsetof(CPUX86State,fpregs[rm].mmx),\n                            offsetof(CPUX86State,fpregs[reg].mmx));\n            }\n            break;\n        case 0x011: /* movups */\n        case 0x111: /* movupd */\n        case 0x029: /* movaps */\n        case 0x129: /* movapd */\n        case 0x17f: /* movdqa ea, xmm */\n        case 0x27f: /* movdqu ea, xmm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_sto_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movo(offsetof(CPUX86State,xmm_regs[rm]),\n                            offsetof(CPUX86State,xmm_regs[reg]));\n            }\n            break;\n        case 0x211: /* movss ea, xmm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n                gen_op_st_v(s, MO_32, cpu_T0, cpu_A0);\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[rm].ZMM_L(0)),\n                            offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n            }\n            break;\n        case 0x311: /* movsd ea, xmm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)));\n            }\n            break;\n        case 0x013: /* movlps */\n        case 0x113: /* movlpd */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                goto illegal_op;\n            }\n            break;\n        case 0x017: /* movhps */\n        case 0x117: /* movhpd */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(1)));\n            } else {\n                goto illegal_op;\n            }\n            break;\n        case 0x71: /* shift mm, im */\n        case 0x72:\n        case 0x73:\n        case 0x171: /* shift xmm, im */\n        case 0x172:\n        case 0x173:\n            if (b1 >= 2) {\n\t        goto unknown_op;\n            }\n            val = cpu_ldub_code(env, s->pc++);\n            if (is_xmm) {\n                tcg_gen_movi_tl(cpu_T0, val);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_t0.ZMM_L(0)));\n                tcg_gen_movi_tl(cpu_T0, 0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_t0.ZMM_L(1)));\n                op1_offset = offsetof(CPUX86State,xmm_t0);\n            } else {\n                tcg_gen_movi_tl(cpu_T0, val);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,mmx_t0.MMX_L(0)));\n                tcg_gen_movi_tl(cpu_T0, 0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,mmx_t0.MMX_L(1)));\n                op1_offset = offsetof(CPUX86State,mmx_t0);\n            }\n            sse_fn_epp = sse_op_table2[((b - 1) & 3) * 8 +\n                                       (((modrm >> 3)) & 7)][b1];\n            if (!sse_fn_epp) {\n                goto unknown_op;\n            }\n            if (is_xmm) {\n                rm = (modrm & 7) | REX_B(s);\n                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);\n            } else {\n                rm = (modrm & 7);\n                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n            }\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op2_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op1_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n            break;\n        case 0x050: /* movmskps */\n            rm = (modrm & 7) | REX_B(s);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                             offsetof(CPUX86State,xmm_regs[rm]));\n            gen_helper_movmskps(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            break;\n        case 0x150: /* movmskpd */\n            rm = (modrm & 7) | REX_B(s);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                             offsetof(CPUX86State,xmm_regs[rm]));\n            gen_helper_movmskpd(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            break;\n        case 0x02a: /* cvtpi2ps */\n        case 0x12a: /* cvtpi2pd */\n            gen_helper_enter_mmx(cpu_env);\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                op2_offset = offsetof(CPUX86State,mmx_t0);\n                gen_ldq_env_A0(s, op2_offset);\n            } else {\n                rm = (modrm & 7);\n                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n            }\n            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            switch(b >> 8) {\n            case 0x0:\n                gen_helper_cvtpi2ps(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            default:\n            case 0x1:\n                gen_helper_cvtpi2pd(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            }\n            break;\n        case 0x22a: /* cvtsi2ss */\n        case 0x32a: /* cvtsi2sd */\n            ot = mo_64_32(s->dflag);\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            if (ot == MO_32) {\n                SSEFunc_0_epi sse_fn_epi = sse_op_table3ai[(b >> 8) & 1];\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                sse_fn_epi(cpu_env, cpu_ptr0, cpu_tmp2_i32);\n            } else {\n#ifdef TARGET_X86_64\n                SSEFunc_0_epl sse_fn_epl = sse_op_table3aq[(b >> 8) & 1];\n                sse_fn_epl(cpu_env, cpu_ptr0, cpu_T0);\n#else\n                goto illegal_op;\n#endif\n            }\n            break;\n        case 0x02c: /* cvttps2pi */\n        case 0x12c: /* cvttpd2pi */\n        case 0x02d: /* cvtps2pi */\n        case 0x12d: /* cvtpd2pi */\n            gen_helper_enter_mmx(cpu_env);\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                op2_offset = offsetof(CPUX86State,xmm_t0);\n                gen_ldo_env_A0(s, op2_offset);\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);\n            }\n            op1_offset = offsetof(CPUX86State,fpregs[reg & 7].mmx);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            switch(b) {\n            case 0x02c:\n                gen_helper_cvttps2pi(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            case 0x12c:\n                gen_helper_cvttpd2pi(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            case 0x02d:\n                gen_helper_cvtps2pi(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            case 0x12d:\n                gen_helper_cvtpd2pi(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            }\n            break;\n        case 0x22c: /* cvttss2si */\n        case 0x32c: /* cvttsd2si */\n        case 0x22d: /* cvtss2si */\n        case 0x32d: /* cvtsd2si */\n            ot = mo_64_32(s->dflag);\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                if ((b >> 8) & 1) {\n                    gen_ldq_env_A0(s, offsetof(CPUX86State, xmm_t0.ZMM_Q(0)));\n                } else {\n                    gen_op_ld_v(s, MO_32, cpu_T0, cpu_A0);\n                    tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_t0.ZMM_L(0)));\n                }\n                op2_offset = offsetof(CPUX86State,xmm_t0);\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);\n            }\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op2_offset);\n            if (ot == MO_32) {\n                SSEFunc_i_ep sse_fn_i_ep =\n                    sse_op_table3bi[((b >> 7) & 2) | (b & 1)];\n                sse_fn_i_ep(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n                tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n            } else {\n#ifdef TARGET_X86_64\n                SSEFunc_l_ep sse_fn_l_ep =\n                    sse_op_table3bq[((b >> 7) & 2) | (b & 1)];\n                sse_fn_l_ep(cpu_T0, cpu_env, cpu_ptr0);\n#else\n                goto illegal_op;\n#endif\n            }\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n            break;\n        case 0xc4: /* pinsrw */\n        case 0x1c4:\n            s->rip_offset = 1;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            val = cpu_ldub_code(env, s->pc++);\n            if (b1) {\n                val &= 7;\n                tcg_gen_st16_tl(cpu_T0, cpu_env,\n                                offsetof(CPUX86State,xmm_regs[reg].ZMM_W(val)));\n            } else {\n                val &= 3;\n                tcg_gen_st16_tl(cpu_T0, cpu_env,\n                                offsetof(CPUX86State,fpregs[reg].mmx.MMX_W(val)));\n            }\n            break;\n        case 0xc5: /* pextrw */\n        case 0x1c5:\n            if (mod != 3)\n                goto illegal_op;\n            ot = mo_64_32(s->dflag);\n            val = cpu_ldub_code(env, s->pc++);\n            if (b1) {\n                val &= 7;\n                rm = (modrm & 7) | REX_B(s);\n                tcg_gen_ld16u_tl(cpu_T0, cpu_env,\n                                 offsetof(CPUX86State,xmm_regs[rm].ZMM_W(val)));\n            } else {\n                val &= 3;\n                rm = (modrm & 7);\n                tcg_gen_ld16u_tl(cpu_T0, cpu_env,\n                                offsetof(CPUX86State,fpregs[rm].mmx.MMX_W(val)));\n            }\n            reg = ((modrm >> 3) & 7) | rex_r;\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n            break;\n        case 0x1d6: /* movq ea, xmm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)));\n                gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(1)));\n            }\n            break;\n        case 0x2d6: /* movq2dq */\n            gen_helper_enter_mmx(cpu_env);\n            rm = (modrm & 7);\n            gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                        offsetof(CPUX86State,fpregs[rm].mmx));\n            gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(1)));\n            break;\n        case 0x3d6: /* movdq2q */\n            gen_helper_enter_mmx(cpu_env);\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_movq(offsetof(CPUX86State,fpregs[reg & 7].mmx),\n                        offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            break;\n        case 0xd7: /* pmovmskb */\n        case 0x1d7:\n            if (mod != 3)\n                goto illegal_op;\n            if (b1) {\n                rm = (modrm & 7) | REX_B(s);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,xmm_regs[rm]));\n                gen_helper_pmovmskb_xmm(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n            } else {\n                rm = (modrm & 7);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,fpregs[rm].mmx));\n                gen_helper_pmovmskb_mmx(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n            }\n            reg = ((modrm >> 3) & 7) | rex_r;\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            break;\n\n        case 0x138:\n        case 0x038:\n            b = modrm;\n            if ((b & 0xf0) == 0xf0) {\n                goto do_0f_38_fx;\n            }\n            modrm = cpu_ldub_code(env, s->pc++);\n            rm = modrm & 7;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            if (b1 >= 2) {\n                goto unknown_op;\n            }\n\n            sse_fn_epp = sse_op_table6[b].op[b1];\n            if (!sse_fn_epp) {\n                goto unknown_op;\n            }\n            if (!(s->cpuid_ext_features & sse_op_table6[b].ext_mask))\n                goto illegal_op;\n\n            if (b1) {\n                op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n                if (mod == 3) {\n                    op2_offset = offsetof(CPUX86State,xmm_regs[rm | REX_B(s)]);\n                } else {\n                    op2_offset = offsetof(CPUX86State,xmm_t0);\n                    gen_lea_modrm(env, s, modrm);\n                    switch (b) {\n                    case 0x20: case 0x30: /* pmovsxbw, pmovzxbw */\n                    case 0x23: case 0x33: /* pmovsxwd, pmovzxwd */\n                    case 0x25: case 0x35: /* pmovsxdq, pmovzxdq */\n                        gen_ldq_env_A0(s, op2_offset +\n                                        offsetof(ZMMReg, ZMM_Q(0)));\n                        break;\n                    case 0x21: case 0x31: /* pmovsxbd, pmovzxbd */\n                    case 0x24: case 0x34: /* pmovsxwq, pmovzxwq */\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, op2_offset +\n                                        offsetof(ZMMReg, ZMM_L(0)));\n                        break;\n                    case 0x22: case 0x32: /* pmovsxbq, pmovzxbq */\n                        tcg_gen_qemu_ld_tl(cpu_tmp0, cpu_A0,\n                                           s->mem_index, MO_LEUW);\n                        tcg_gen_st16_tl(cpu_tmp0, cpu_env, op2_offset +\n                                        offsetof(ZMMReg, ZMM_W(0)));\n                        break;\n                    case 0x2a:            /* movntqda */\n                        gen_ldo_env_A0(s, op1_offset);\n                        return;\n                    default:\n                        gen_ldo_env_A0(s, op2_offset);\n                    }\n                }\n            } else {\n                op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);\n                if (mod == 3) {\n                    op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n                } else {\n                    op2_offset = offsetof(CPUX86State,mmx_t0);\n                    gen_lea_modrm(env, s, modrm);\n                    gen_ldq_env_A0(s, op2_offset);\n                }\n            }\n            if (sse_fn_epp == SSE_SPECIAL) {\n                goto unknown_op;\n            }\n\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n\n            if (b == 0x17) {\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n            break;\n\n        case 0x238:\n        case 0x338:\n        do_0f_38_fx:\n            /* Various integer extensions at 0f 38 f[0-f].  */\n            b = modrm | (b1 << 8);\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n\n            switch (b) {\n            case 0x3f0: /* crc32 Gd,Eb */\n            case 0x3f1: /* crc32 Gd,Ey */\n            do_crc32:\n                if (!(s->cpuid_ext_features & CPUID_EXT_SSE42)) {\n                    goto illegal_op;\n                }\n                if ((b & 0xff) == 0xf0) {\n                    ot = MO_8;\n                } else if (s->dflag != MO_64) {\n                    ot = (s->prefix & PREFIX_DATA ? MO_16 : MO_32);\n                } else {\n                    ot = MO_64;\n                }\n\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[reg]);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                gen_helper_crc32(cpu_T0, cpu_tmp2_i32,\n                                 cpu_T0, tcg_const_i32(8 << ot));\n\n                ot = mo_64_32(s->dflag);\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                break;\n\n            case 0x1f0: /* crc32 or movbe */\n            case 0x1f1:\n                /* For these insns, the f3 prefix is supposed to have priority\n                   over the 66 prefix, but that's not what we implement above\n                   setting b1.  */\n                if (s->prefix & PREFIX_REPNZ) {\n                    goto do_crc32;\n                }\n                /* FALLTHRU */\n            case 0x0f0: /* movbe Gy,My */\n            case 0x0f1: /* movbe My,Gy */\n                if (!(s->cpuid_ext_features & CPUID_EXT_MOVBE)) {\n                    goto illegal_op;\n                }\n                if (s->dflag != MO_64) {\n                    ot = (s->prefix & PREFIX_DATA ? MO_16 : MO_32);\n                } else {\n                    ot = MO_64;\n                }\n\n                gen_lea_modrm(env, s, modrm);\n                if ((b & 1) == 0) {\n                    tcg_gen_qemu_ld_tl(cpu_T0, cpu_A0,\n                                       s->mem_index, ot | MO_BE);\n                    gen_op_mov_reg_v(ot, reg, cpu_T0);\n                } else {\n                    tcg_gen_qemu_st_tl(cpu_regs[reg], cpu_A0,\n                                       s->mem_index, ot | MO_BE);\n                }\n                break;\n\n            case 0x0f2: /* andn Gy, By, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                tcg_gen_andc_tl(cpu_T0, cpu_regs[s->vex_v], cpu_T0);\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                gen_op_update1_cc();\n                set_cc_op(s, CC_OP_LOGICB + ot);\n                break;\n\n            case 0x0f7: /* bextr Gy, Ey, By */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                {\n                    TCGv bound, zero;\n\n                    gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                    /* Extract START, and shift the operand.\n                       Shifts larger than operand size get zeros.  */\n                    tcg_gen_ext8u_tl(cpu_A0, cpu_regs[s->vex_v]);\n                    tcg_gen_shr_tl(cpu_T0, cpu_T0, cpu_A0);\n\n                    bound = tcg_const_tl(ot == MO_64 ? 63 : 31);\n                    zero = tcg_const_tl(0);\n                    tcg_gen_movcond_tl(TCG_COND_LEU, cpu_T0, cpu_A0, bound,\n                                       cpu_T0, zero);\n                    tcg_temp_free(zero);\n\n                    /* Extract the LEN into a mask.  Lengths larger than\n                       operand size get all ones.  */\n                    tcg_gen_extract_tl(cpu_A0, cpu_regs[s->vex_v], 8, 8);\n                    tcg_gen_movcond_tl(TCG_COND_LEU, cpu_A0, cpu_A0, bound,\n                                       cpu_A0, bound);\n                    tcg_temp_free(bound);\n                    tcg_gen_movi_tl(cpu_T1, 1);\n                    tcg_gen_shl_tl(cpu_T1, cpu_T1, cpu_A0);\n                    tcg_gen_subi_tl(cpu_T1, cpu_T1, 1);\n                    tcg_gen_and_tl(cpu_T0, cpu_T0, cpu_T1);\n\n                    gen_op_mov_reg_v(ot, reg, cpu_T0);\n                    gen_op_update1_cc();\n                    set_cc_op(s, CC_OP_LOGICB + ot);\n                }\n                break;\n\n            case 0x0f5: /* bzhi Gy, Ey, By */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                tcg_gen_ext8u_tl(cpu_T1, cpu_regs[s->vex_v]);\n                {\n                    TCGv bound = tcg_const_tl(ot == MO_64 ? 63 : 31);\n                    /* Note that since we're using BMILG (in order to get O\n                       cleared) we need to store the inverse into C.  */\n                    tcg_gen_setcond_tl(TCG_COND_LT, cpu_cc_src,\n                                       cpu_T1, bound);\n                    tcg_gen_movcond_tl(TCG_COND_GT, cpu_T1, cpu_T1,\n                                       bound, bound, cpu_T1);\n                    tcg_temp_free(bound);\n                }\n                tcg_gen_movi_tl(cpu_A0, -1);\n                tcg_gen_shl_tl(cpu_A0, cpu_A0, cpu_T1);\n                tcg_gen_andc_tl(cpu_T0, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                gen_op_update1_cc();\n                set_cc_op(s, CC_OP_BMILGB + ot);\n                break;\n\n            case 0x3f6: /* mulx By, Gy, rdx, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                switch (ot) {\n                default:\n                    tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                    tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EDX]);\n                    tcg_gen_mulu2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                      cpu_tmp2_i32, cpu_tmp3_i32);\n                    tcg_gen_extu_i32_tl(cpu_regs[s->vex_v], cpu_tmp2_i32);\n                    tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp3_i32);\n                    break;\n#ifdef TARGET_X86_64\n                case MO_64:\n                    tcg_gen_mulu2_i64(cpu_T0, cpu_T1,\n                                      cpu_T0, cpu_regs[R_EDX]);\n                    tcg_gen_mov_i64(cpu_regs[s->vex_v], cpu_T0);\n                    tcg_gen_mov_i64(cpu_regs[reg], cpu_T1);\n                    break;\n#endif\n                }\n                break;\n\n            case 0x3f5: /* pdep Gy, By, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                /* Note that by zero-extending the mask operand, we\n                   automatically handle zero-extending the result.  */\n                if (ot == MO_64) {\n                    tcg_gen_mov_tl(cpu_T1, cpu_regs[s->vex_v]);\n                } else {\n                    tcg_gen_ext32u_tl(cpu_T1, cpu_regs[s->vex_v]);\n                }\n                gen_helper_pdep(cpu_regs[reg], cpu_T0, cpu_T1);\n                break;\n\n            case 0x2f5: /* pext Gy, By, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                /* Note that by zero-extending the mask operand, we\n                   automatically handle zero-extending the result.  */\n                if (ot == MO_64) {\n                    tcg_gen_mov_tl(cpu_T1, cpu_regs[s->vex_v]);\n                } else {\n                    tcg_gen_ext32u_tl(cpu_T1, cpu_regs[s->vex_v]);\n                }\n                gen_helper_pext(cpu_regs[reg], cpu_T0, cpu_T1);\n                break;\n\n            case 0x1f6: /* adcx Gy, Ey */\n            case 0x2f6: /* adox Gy, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_ADX)) {\n                    goto illegal_op;\n                } else {\n                    TCGv carry_in, carry_out, zero;\n                    int end_op;\n\n                    ot = mo_64_32(s->dflag);\n                    gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n\n                    /* Re-use the carry-out from a previous round.  */\n                    TCGV_UNUSED(carry_in);\n                    carry_out = (b == 0x1f6 ? cpu_cc_dst : cpu_cc_src2);\n                    switch (s->cc_op) {\n                    case CC_OP_ADCX:\n                        if (b == 0x1f6) {\n                            carry_in = cpu_cc_dst;\n                            end_op = CC_OP_ADCX;\n                        } else {\n                            end_op = CC_OP_ADCOX;\n                        }\n                        break;\n                    case CC_OP_ADOX:\n                        if (b == 0x1f6) {\n                            end_op = CC_OP_ADCOX;\n                        } else {\n                            carry_in = cpu_cc_src2;\n                            end_op = CC_OP_ADOX;\n                        }\n                        break;\n                    case CC_OP_ADCOX:\n                        end_op = CC_OP_ADCOX;\n                        carry_in = carry_out;\n                        break;\n                    default:\n                        end_op = (b == 0x1f6 ? CC_OP_ADCX : CC_OP_ADOX);\n                        break;\n                    }\n                    /* If we can't reuse carry-out, get it out of EFLAGS.  */\n                    if (TCGV_IS_UNUSED(carry_in)) {\n                        if (s->cc_op != CC_OP_ADCX && s->cc_op != CC_OP_ADOX) {\n                            gen_compute_eflags(s);\n                        }\n                        carry_in = cpu_tmp0;\n                        tcg_gen_extract_tl(carry_in, cpu_cc_src,\n                                           ctz32(b == 0x1f6 ? CC_C : CC_O), 1);\n                    }\n\n                    switch (ot) {\n#ifdef TARGET_X86_64\n                    case MO_32:\n                        /* If we know TL is 64-bit, and we want a 32-bit\n                           result, just do everything in 64-bit arithmetic.  */\n                        tcg_gen_ext32u_i64(cpu_regs[reg], cpu_regs[reg]);\n                        tcg_gen_ext32u_i64(cpu_T0, cpu_T0);\n                        tcg_gen_add_i64(cpu_T0, cpu_T0, cpu_regs[reg]);\n                        tcg_gen_add_i64(cpu_T0, cpu_T0, carry_in);\n                        tcg_gen_ext32u_i64(cpu_regs[reg], cpu_T0);\n                        tcg_gen_shri_i64(carry_out, cpu_T0, 32);\n                        break;\n#endif\n                    default:\n                        /* Otherwise compute the carry-out in two steps.  */\n                        zero = tcg_const_tl(0);\n                        tcg_gen_add2_tl(cpu_T0, carry_out,\n                                        cpu_T0, zero,\n                                        carry_in, zero);\n                        tcg_gen_add2_tl(cpu_regs[reg], carry_out,\n                                        cpu_regs[reg], carry_out,\n                                        cpu_T0, zero);\n                        tcg_temp_free(zero);\n                        break;\n                    }\n                    set_cc_op(s, end_op);\n                }\n                break;\n\n            case 0x1f7: /* shlx Gy, Ey, By */\n            case 0x2f7: /* sarx Gy, Ey, By */\n            case 0x3f7: /* shrx Gy, Ey, By */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                if (ot == MO_64) {\n                    tcg_gen_andi_tl(cpu_T1, cpu_regs[s->vex_v], 63);\n                } else {\n                    tcg_gen_andi_tl(cpu_T1, cpu_regs[s->vex_v], 31);\n                }\n                if (b == 0x1f7) {\n                    tcg_gen_shl_tl(cpu_T0, cpu_T0, cpu_T1);\n                } else if (b == 0x2f7) {\n                    if (ot != MO_64) {\n                        tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n                    }\n                    tcg_gen_sar_tl(cpu_T0, cpu_T0, cpu_T1);\n                } else {\n                    if (ot != MO_64) {\n                        tcg_gen_ext32u_tl(cpu_T0, cpu_T0);\n                    }\n                    tcg_gen_shr_tl(cpu_T0, cpu_T0, cpu_T1);\n                }\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                break;\n\n            case 0x0f3:\n            case 0x1f3:\n            case 0x2f3:\n            case 0x3f3: /* Group 17 */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n\n                switch (reg & 7) {\n                case 1: /* blsr By,Ey */\n                    tcg_gen_neg_tl(cpu_T1, cpu_T0);\n                    tcg_gen_and_tl(cpu_T0, cpu_T0, cpu_T1);\n                    gen_op_mov_reg_v(ot, s->vex_v, cpu_T0);\n                    gen_op_update2_cc();\n                    set_cc_op(s, CC_OP_BMILGB + ot);\n                    break;\n\n                case 2: /* blsmsk By,Ey */\n                    tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n                    tcg_gen_subi_tl(cpu_T0, cpu_T0, 1);\n                    tcg_gen_xor_tl(cpu_T0, cpu_T0, cpu_cc_src);\n                    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                    set_cc_op(s, CC_OP_BMILGB + ot);\n                    break;\n\n                case 3: /* blsi By, Ey */\n                    tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n                    tcg_gen_subi_tl(cpu_T0, cpu_T0, 1);\n                    tcg_gen_and_tl(cpu_T0, cpu_T0, cpu_cc_src);\n                    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                    set_cc_op(s, CC_OP_BMILGB + ot);\n                    break;\n\n                default:\n                    goto unknown_op;\n                }\n                break;\n\n            default:\n                goto unknown_op;\n            }\n            break;\n\n        case 0x03a:\n        case 0x13a:\n            b = modrm;\n            modrm = cpu_ldub_code(env, s->pc++);\n            rm = modrm & 7;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            if (b1 >= 2) {\n                goto unknown_op;\n            }\n\n            sse_fn_eppi = sse_op_table7[b].op[b1];\n            if (!sse_fn_eppi) {\n                goto unknown_op;\n            }\n            if (!(s->cpuid_ext_features & sse_op_table7[b].ext_mask))\n                goto illegal_op;\n\n            if (sse_fn_eppi == SSE_SPECIAL) {\n                ot = mo_64_32(s->dflag);\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3)\n                    gen_lea_modrm(env, s, modrm);\n                reg = ((modrm >> 3) & 7) | rex_r;\n                val = cpu_ldub_code(env, s->pc++);\n                switch (b) {\n                case 0x14: /* pextrb */\n                    tcg_gen_ld8u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                                            xmm_regs[reg].ZMM_B(val & 15)));\n                    if (mod == 3) {\n                        gen_op_mov_reg_v(ot, rm, cpu_T0);\n                    } else {\n                        tcg_gen_qemu_st_tl(cpu_T0, cpu_A0,\n                                           s->mem_index, MO_UB);\n                    }\n                    break;\n                case 0x15: /* pextrw */\n                    tcg_gen_ld16u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                                            xmm_regs[reg].ZMM_W(val & 7)));\n                    if (mod == 3) {\n                        gen_op_mov_reg_v(ot, rm, cpu_T0);\n                    } else {\n                        tcg_gen_qemu_st_tl(cpu_T0, cpu_A0,\n                                           s->mem_index, MO_LEUW);\n                    }\n                    break;\n                case 0x16:\n                    if (ot == MO_32) { /* pextrd */\n                        tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env,\n                                        offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(val & 3)));\n                        if (mod == 3) {\n                            tcg_gen_extu_i32_tl(cpu_regs[rm], cpu_tmp2_i32);\n                        } else {\n                            tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                                s->mem_index, MO_LEUL);\n                        }\n                    } else { /* pextrq */\n#ifdef TARGET_X86_64\n                        tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env,\n                                        offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_Q(val & 1)));\n                        if (mod == 3) {\n                            tcg_gen_mov_i64(cpu_regs[rm], cpu_tmp1_i64);\n                        } else {\n                            tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                                s->mem_index, MO_LEQ);\n                        }\n#else\n                        goto illegal_op;\n#endif\n                    }\n                    break;\n                case 0x17: /* extractps */\n                    tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                                            xmm_regs[reg].ZMM_L(val & 3)));\n                    if (mod == 3) {\n                        gen_op_mov_reg_v(ot, rm, cpu_T0);\n                    } else {\n                        tcg_gen_qemu_st_tl(cpu_T0, cpu_A0,\n                                           s->mem_index, MO_LEUL);\n                    }\n                    break;\n                case 0x20: /* pinsrb */\n                    if (mod == 3) {\n                        gen_op_mov_v_reg(MO_32, cpu_T0, rm);\n                    } else {\n                        tcg_gen_qemu_ld_tl(cpu_T0, cpu_A0,\n                                           s->mem_index, MO_UB);\n                    }\n                    tcg_gen_st8_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                                            xmm_regs[reg].ZMM_B(val & 15)));\n                    break;\n                case 0x21: /* insertps */\n                    if (mod == 3) {\n                        tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env,\n                                        offsetof(CPUX86State,xmm_regs[rm]\n                                                .ZMM_L((val >> 6) & 3)));\n                    } else {\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                    tcg_gen_st_i32(cpu_tmp2_i32, cpu_env,\n                                    offsetof(CPUX86State,xmm_regs[reg]\n                                            .ZMM_L((val >> 4) & 3)));\n                    if ((val >> 0) & 1)\n                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),\n                                        cpu_env, offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(0)));\n                    if ((val >> 1) & 1)\n                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),\n                                        cpu_env, offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(1)));\n                    if ((val >> 2) & 1)\n                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),\n                                        cpu_env, offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(2)));\n                    if ((val >> 3) & 1)\n                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),\n                                        cpu_env, offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(3)));\n                    break;\n                case 0x22:\n                    if (ot == MO_32) { /* pinsrd */\n                        if (mod == 3) {\n                            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[rm]);\n                        } else {\n                            tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                                s->mem_index, MO_LEUL);\n                        }\n                        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env,\n                                        offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(val & 3)));\n                    } else { /* pinsrq */\n#ifdef TARGET_X86_64\n                        if (mod == 3) {\n                            gen_op_mov_v_reg(ot, cpu_tmp1_i64, rm);\n                        } else {\n                            tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                                s->mem_index, MO_LEQ);\n                        }\n                        tcg_gen_st_i64(cpu_tmp1_i64, cpu_env,\n                                        offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_Q(val & 1)));\n#else\n                        goto illegal_op;\n#endif\n                    }\n                    break;\n                }\n                return;\n            }\n\n            if (b1) {\n                op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n                if (mod == 3) {\n                    op2_offset = offsetof(CPUX86State,xmm_regs[rm | REX_B(s)]);\n                } else {\n                    op2_offset = offsetof(CPUX86State,xmm_t0);\n                    gen_lea_modrm(env, s, modrm);\n                    gen_ldo_env_A0(s, op2_offset);\n                }\n            } else {\n                op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);\n                if (mod == 3) {\n                    op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n                } else {\n                    op2_offset = offsetof(CPUX86State,mmx_t0);\n                    gen_lea_modrm(env, s, modrm);\n                    gen_ldq_env_A0(s, op2_offset);\n                }\n            }\n            val = cpu_ldub_code(env, s->pc++);\n\n            if ((b & 0xfc) == 0x60) { /* pcmpXstrX */\n                set_cc_op(s, CC_OP_EFLAGS);\n\n                if (s->dflag == MO_64) {\n                    /* The helper must use entire 64-bit gp registers */\n                    val |= 1 << 8;\n                }\n            }\n\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_eppi(cpu_env, cpu_ptr0, cpu_ptr1, tcg_const_i32(val));\n            break;\n\n        case 0x33a:\n            /* Various integer extensions at 0f 3a f[0-f].  */\n            b = modrm | (b1 << 8);\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n\n            switch (b) {\n            case 0x3f0: /* rorx Gy,Ey, Ib */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                b = cpu_ldub_code(env, s->pc++);\n                if (ot == MO_64) {\n                    tcg_gen_rotri_tl(cpu_T0, cpu_T0, b & 63);\n                } else {\n                    tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                    tcg_gen_rotri_i32(cpu_tmp2_i32, cpu_tmp2_i32, b & 31);\n                    tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n                }\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                break;\n\n            default:\n                goto unknown_op;\n            }\n            break;\n\n        default:\n        unknown_op:\n            gen_unknown_opcode(env, s);\n            return;\n        }\n    } else {\n        /* generic MMX or SSE operation */\n        switch(b) {\n        case 0x70: /* pshufx insn */\n        case 0xc6: /* pshufx insn */\n        case 0xc2: /* compare insns */\n            s->rip_offset = 1;\n            break;\n        default:\n            break;\n        }\n        if (is_xmm) {\n            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n            if (mod != 3) {\n                int sz = 4;\n\n                gen_lea_modrm(env, s, modrm);\n                op2_offset = offsetof(CPUX86State,xmm_t0);\n\n                switch (b) {\n                case 0x50 ... 0x5a:\n                case 0x5c ... 0x5f:\n                case 0xc2:\n                    /* Most sse scalar operations.  */\n                    if (b1 == 2) {\n                        sz = 2;\n                    } else if (b1 == 3) {\n                        sz = 3;\n                    }\n                    break;\n\n                case 0x2e:  /* ucomis[sd] */\n                case 0x2f:  /* comis[sd] */\n                    if (b1 == 0) {\n                        sz = 2;\n                    } else {\n                        sz = 3;\n                    }\n                    break;\n                }\n\n                switch (sz) {\n                case 2:\n                    /* 32 bit access */\n                    gen_op_ld_v(s, MO_32, cpu_T0, cpu_A0);\n                    tcg_gen_st32_tl(cpu_T0, cpu_env,\n                                    offsetof(CPUX86State,xmm_t0.ZMM_L(0)));\n                    break;\n                case 3:\n                    /* 64 bit access */\n                    gen_ldq_env_A0(s, offsetof(CPUX86State, xmm_t0.ZMM_D(0)));\n                    break;\n                default:\n                    /* 128 bit access */\n                    gen_ldo_env_A0(s, op2_offset);\n                    break;\n                }\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);\n            }\n        } else {\n            op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                op2_offset = offsetof(CPUX86State,mmx_t0);\n                gen_ldq_env_A0(s, op2_offset);\n            } else {\n                rm = (modrm & 7);\n                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n            }\n        }\n        switch(b) {\n        case 0x0f: /* 3DNow! data insns */\n            val = cpu_ldub_code(env, s->pc++);\n            sse_fn_epp = sse_op_table5[val];\n            if (!sse_fn_epp) {\n                goto unknown_op;\n            }\n            if (!(s->cpuid_ext2_features & CPUID_EXT2_3DNOW)) {\n                goto illegal_op;\n            }\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n            break;\n        case 0x70: /* pshufx insn */\n        case 0xc6: /* pshufx insn */\n            val = cpu_ldub_code(env, s->pc++);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            /* XXX: introduce a new table? */\n            sse_fn_ppi = (SSEFunc_0_ppi)sse_fn_epp;\n            sse_fn_ppi(cpu_ptr0, cpu_ptr1, tcg_const_i32(val));\n            break;\n        case 0xc2:\n            /* compare insns */\n            val = cpu_ldub_code(env, s->pc++);\n            if (val >= 8)\n                goto unknown_op;\n            sse_fn_epp = sse_op_table4[val][b1];\n\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n            break;\n        case 0xf7:\n            /* maskmov : we must prepare A0 */\n            if (mod != 3)\n                goto illegal_op;\n            tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EDI]);\n            gen_extu(s->aflag, cpu_A0);\n            gen_add_A0_ds_seg(s);\n\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            /* XXX: introduce a new table? */\n            sse_fn_eppt = (SSEFunc_0_eppt)sse_fn_epp;\n            sse_fn_eppt(cpu_env, cpu_ptr0, cpu_ptr1, cpu_A0);\n            break;\n        default:\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n            break;\n        }\n        if (b == 0x2e || b == 0x2f) {\n            set_cc_op(s, CC_OP_EFLAGS);\n        }\n    }\n}\n\n/* convert one instruction. s->is_jmp is set if the translation must\n   be stopped. Return the next pc value */\nstatic target_ulong disas_insn(CPUX86State *env, DisasContext *s,\n                               target_ulong pc_start)\n{\n    int b, prefixes;\n    int shift;\n    TCGMemOp ot, aflag, dflag;\n    int modrm, reg, rm, mod, op, opreg, val;\n    target_ulong next_eip, tval;\n    int rex_w, rex_r;\n\n    s->pc_start = s->pc = pc_start;\n    prefixes = 0;\n    s->override = -1;\n    rex_w = -1;\n    rex_r = 0;\n#ifdef TARGET_X86_64\n    s->rex_x = 0;\n    s->rex_b = 0;\n    x86_64_hregs = 0;\n#endif\n    s->rip_offset = 0; /* for relative ip address */\n    s->vex_l = 0;\n    s->vex_v = 0;\n next_byte:\n    b = cpu_ldub_code(env, s->pc);\n    s->pc++;\n    /* Collect prefixes.  */\n    switch (b) {\n    case 0xf3:\n        prefixes |= PREFIX_REPZ;\n        goto next_byte;\n    case 0xf2:\n        prefixes |= PREFIX_REPNZ;\n        goto next_byte;\n    case 0xf0:\n        prefixes |= PREFIX_LOCK;\n        goto next_byte;\n    case 0x2e:\n        s->override = R_CS;\n        goto next_byte;\n    case 0x36:\n        s->override = R_SS;\n        goto next_byte;\n    case 0x3e:\n        s->override = R_DS;\n        goto next_byte;\n    case 0x26:\n        s->override = R_ES;\n        goto next_byte;\n    case 0x64:\n        s->override = R_FS;\n        goto next_byte;\n    case 0x65:\n        s->override = R_GS;\n        goto next_byte;\n    case 0x66:\n        prefixes |= PREFIX_DATA;\n        goto next_byte;\n    case 0x67:\n        prefixes |= PREFIX_ADR;\n        goto next_byte;\n#ifdef TARGET_X86_64\n    case 0x40 ... 0x4f:\n        if (CODE64(s)) {\n            /* REX prefix */\n            rex_w = (b >> 3) & 1;\n            rex_r = (b & 0x4) << 1;\n            s->rex_x = (b & 0x2) << 2;\n            REX_B(s) = (b & 0x1) << 3;\n            x86_64_hregs = 1; /* select uniform byte register addressing */\n            goto next_byte;\n        }\n        break;\n#endif\n    case 0xc5: /* 2-byte VEX */\n    case 0xc4: /* 3-byte VEX */\n        /* VEX prefixes cannot be used except in 32-bit mode.\n           Otherwise the instruction is LES or LDS.  */\n        if (s->code32 && !s->vm86) {\n            static const int pp_prefix[4] = {\n                0, PREFIX_DATA, PREFIX_REPZ, PREFIX_REPNZ\n            };\n            int vex3, vex2 = cpu_ldub_code(env, s->pc);\n\n            if (!CODE64(s) && (vex2 & 0xc0) != 0xc0) {\n                /* 4.1.4.6: In 32-bit mode, bits [7:6] must be 11b,\n                   otherwise the instruction is LES or LDS.  */\n                break;\n            }\n            s->pc++;\n\n            /* 4.1.1-4.1.3: No preceding lock, 66, f2, f3, or rex prefixes. */\n            if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ\n                            | PREFIX_LOCK | PREFIX_DATA)) {\n                goto illegal_op;\n            }\n#ifdef TARGET_X86_64\n            if (x86_64_hregs) {\n                goto illegal_op;\n            }\n#endif\n            rex_r = (~vex2 >> 4) & 8;\n            if (b == 0xc5) {\n                vex3 = vex2;\n                b = cpu_ldub_code(env, s->pc++);\n            } else {\n#ifdef TARGET_X86_64\n                s->rex_x = (~vex2 >> 3) & 8;\n                s->rex_b = (~vex2 >> 2) & 8;\n#endif\n                vex3 = cpu_ldub_code(env, s->pc++);\n                rex_w = (vex3 >> 7) & 1;\n                switch (vex2 & 0x1f) {\n                case 0x01: /* Implied 0f leading opcode bytes.  */\n                    b = cpu_ldub_code(env, s->pc++) | 0x100;\n                    break;\n                case 0x02: /* Implied 0f 38 leading opcode bytes.  */\n                    b = 0x138;\n                    break;\n                case 0x03: /* Implied 0f 3a leading opcode bytes.  */\n                    b = 0x13a;\n                    break;\n                default:   /* Reserved for future use.  */\n                    goto unknown_op;\n                }\n            }\n            s->vex_v = (~vex3 >> 3) & 0xf;\n            s->vex_l = (vex3 >> 2) & 1;\n            prefixes |= pp_prefix[vex3 & 3] | PREFIX_VEX;\n        }\n        break;\n    }\n\n    /* Post-process prefixes.  */\n    if (CODE64(s)) {\n        /* In 64-bit mode, the default data size is 32-bit.  Select 64-bit\n           data with rex_w, and 16-bit data with 0x66; rex_w takes precedence\n           over 0x66 if both are present.  */\n        dflag = (rex_w > 0 ? MO_64 : prefixes & PREFIX_DATA ? MO_16 : MO_32);\n        /* In 64-bit mode, 0x67 selects 32-bit addressing.  */\n        aflag = (prefixes & PREFIX_ADR ? MO_32 : MO_64);\n    } else {\n        /* In 16/32-bit mode, 0x66 selects the opposite data size.  */\n        if (s->code32 ^ ((prefixes & PREFIX_DATA) != 0)) {\n            dflag = MO_32;\n        } else {\n            dflag = MO_16;\n        }\n        /* In 16/32-bit mode, 0x67 selects the opposite addressing.  */\n        if (s->code32 ^ ((prefixes & PREFIX_ADR) != 0)) {\n            aflag = MO_32;\n        }  else {\n            aflag = MO_16;\n        }\n    }\n\n    s->prefix = prefixes;\n    s->aflag = aflag;\n    s->dflag = dflag;\n\n    /* now check op code */\n reswitch:\n    switch(b) {\n    case 0x0f:\n        /**************************/\n        /* extended op code */\n        b = cpu_ldub_code(env, s->pc++) | 0x100;\n        goto reswitch;\n\n        /**************************/\n        /* arith & logic */\n    case 0x00 ... 0x05:\n    case 0x08 ... 0x0d:\n    case 0x10 ... 0x15:\n    case 0x18 ... 0x1d:\n    case 0x20 ... 0x25:\n    case 0x28 ... 0x2d:\n    case 0x30 ... 0x35:\n    case 0x38 ... 0x3d:\n        {\n            int op, f, val;\n            op = (b >> 3) & 7;\n            f = (b >> 1) & 3;\n\n            ot = mo_b_d(b, dflag);\n\n            switch(f) {\n            case 0: /* OP Ev, Gv */\n                modrm = cpu_ldub_code(env, s->pc++);\n                reg = ((modrm >> 3) & 7) | rex_r;\n                mod = (modrm >> 6) & 3;\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3) {\n                    gen_lea_modrm(env, s, modrm);\n                    opreg = OR_TMP0;\n                } else if (op == OP_XORL && rm == reg) {\n                xor_zero:\n                    /* xor reg, reg optimisation */\n                    set_cc_op(s, CC_OP_CLR);\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                    gen_op_mov_reg_v(ot, reg, cpu_T0);\n                    break;\n                } else {\n                    opreg = rm;\n                }\n                gen_op_mov_v_reg(ot, cpu_T1, reg);\n                gen_op(s, op, ot, opreg);\n                break;\n            case 1: /* OP Gv, Ev */\n                modrm = cpu_ldub_code(env, s->pc++);\n                mod = (modrm >> 6) & 3;\n                reg = ((modrm >> 3) & 7) | rex_r;\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3) {\n                    gen_lea_modrm(env, s, modrm);\n                    gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n                } else if (op == OP_XORL && rm == reg) {\n                    goto xor_zero;\n                } else {\n                    gen_op_mov_v_reg(ot, cpu_T1, rm);\n                }\n                gen_op(s, op, ot, reg);\n                break;\n            case 2: /* OP A, Iv */\n                val = insn_get(env, s, ot);\n                tcg_gen_movi_tl(cpu_T1, val);\n                gen_op(s, op, ot, OR_EAX);\n                break;\n            }\n        }\n        break;\n\n    case 0x82:\n        if (CODE64(s))\n            goto illegal_op;\n    case 0x80: /* GRP1 */\n    case 0x81:\n    case 0x83:\n        {\n            int val;\n\n            ot = mo_b_d(b, dflag);\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n            op = (modrm >> 3) & 7;\n\n            if (mod != 3) {\n                if (b == 0x83)\n                    s->rip_offset = 1;\n                else\n                    s->rip_offset = insn_const_size(ot);\n                gen_lea_modrm(env, s, modrm);\n                opreg = OR_TMP0;\n            } else {\n                opreg = rm;\n            }\n\n            switch(b) {\n            default:\n            case 0x80:\n            case 0x81:\n            case 0x82:\n                val = insn_get(env, s, ot);\n                break;\n            case 0x83:\n                val = (int8_t)insn_get(env, s, MO_8);\n                break;\n            }\n            tcg_gen_movi_tl(cpu_T1, val);\n            gen_op(s, op, ot, opreg);\n        }\n        break;\n\n        /**************************/\n        /* inc, dec, and other misc arith */\n    case 0x40 ... 0x47: /* inc Gv */\n        ot = dflag;\n        gen_inc(s, ot, OR_EAX + (b & 7), 1);\n        break;\n    case 0x48 ... 0x4f: /* dec Gv */\n        ot = dflag;\n        gen_inc(s, ot, OR_EAX + (b & 7), -1);\n        break;\n    case 0xf6: /* GRP3 */\n    case 0xf7:\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        op = (modrm >> 3) & 7;\n        if (mod != 3) {\n            if (op == 0) {\n                s->rip_offset = insn_const_size(ot);\n            }\n            gen_lea_modrm(env, s, modrm);\n            /* For those below that handle locked memory, don't load here.  */\n            if (!(s->prefix & PREFIX_LOCK)\n                || op != 2) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n\n        switch(op) {\n        case 0: /* test */\n            val = insn_get(env, s, ot);\n            tcg_gen_movi_tl(cpu_T1, val);\n            gen_op_testl_T0_T1_cc();\n            set_cc_op(s, CC_OP_LOGICB + ot);\n            break;\n        case 2: /* not */\n            if (s->prefix & PREFIX_LOCK) {\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                tcg_gen_movi_tl(cpu_T0, ~0);\n                tcg_gen_atomic_xor_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                            s->mem_index, ot | MO_LE);\n            } else {\n                tcg_gen_not_tl(cpu_T0, cpu_T0);\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n            break;\n        case 3: /* neg */\n            if (s->prefix & PREFIX_LOCK) {\n                TCGLabel *label1;\n                TCGv a0, t0, t1, t2;\n\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                a0 = tcg_temp_local_new();\n                t0 = tcg_temp_local_new();\n                label1 = gen_new_label();\n\n                tcg_gen_mov_tl(a0, cpu_A0);\n                tcg_gen_mov_tl(t0, cpu_T0);\n\n                gen_set_label(label1);\n                t1 = tcg_temp_new();\n                t2 = tcg_temp_new();\n                tcg_gen_mov_tl(t2, t0);\n                tcg_gen_neg_tl(t1, t0);\n                tcg_gen_atomic_cmpxchg_tl(t0, a0, t0, t1,\n                                          s->mem_index, ot | MO_LE);\n                tcg_temp_free(t1);\n                tcg_gen_brcond_tl(TCG_COND_NE, t0, t2, label1);\n\n                tcg_temp_free(t2);\n                tcg_temp_free(a0);\n                tcg_gen_mov_tl(cpu_T0, t0);\n                tcg_temp_free(t0);\n            } else {\n                tcg_gen_neg_tl(cpu_T0, cpu_T0);\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n            gen_op_update_neg_cc();\n            set_cc_op(s, CC_OP_SUBB + ot);\n            break;\n        case 4: /* mul */\n            switch(ot) {\n            case MO_8:\n                gen_op_mov_v_reg(MO_8, cpu_T1, R_EAX);\n                tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext8u_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_andi_tl(cpu_cc_src, cpu_T0, 0xff00);\n                set_cc_op(s, CC_OP_MULB);\n                break;\n            case MO_16:\n                gen_op_mov_v_reg(MO_16, cpu_T1, R_EAX);\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext16u_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, 16);\n                gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n                set_cc_op(s, CC_OP_MULW);\n                break;\n            default:\n            case MO_32:\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EAX]);\n                tcg_gen_mulu2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                  cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EAX], cpu_tmp2_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EDX], cpu_tmp3_i32);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULL);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                tcg_gen_mulu2_i64(cpu_regs[R_EAX], cpu_regs[R_EDX],\n                                  cpu_T0, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULQ);\n                break;\n#endif\n            }\n            break;\n        case 5: /* imul */\n            switch(ot) {\n            case MO_8:\n                gen_op_mov_v_reg(MO_8, cpu_T1, R_EAX);\n                tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext8s_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_ext8s_tl(cpu_tmp0, cpu_T0);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n                set_cc_op(s, CC_OP_MULB);\n                break;\n            case MO_16:\n                gen_op_mov_v_reg(MO_16, cpu_T1, R_EAX);\n                tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext16s_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_ext16s_tl(cpu_tmp0, cpu_T0);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, 16);\n                gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n                set_cc_op(s, CC_OP_MULW);\n                break;\n            default:\n            case MO_32:\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EAX]);\n                tcg_gen_muls2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                  cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EAX], cpu_tmp2_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EDX], cpu_tmp3_i32);\n                tcg_gen_sari_i32(cpu_tmp2_i32, cpu_tmp2_i32, 31);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_sub_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_cc_src, cpu_tmp2_i32);\n                set_cc_op(s, CC_OP_MULL);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                tcg_gen_muls2_i64(cpu_regs[R_EAX], cpu_regs[R_EDX],\n                                  cpu_T0, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_sari_tl(cpu_cc_src, cpu_regs[R_EAX], 63);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULQ);\n                break;\n#endif\n            }\n            break;\n        case 6: /* div */\n            switch(ot) {\n            case MO_8:\n                gen_helper_divb_AL(cpu_env, cpu_T0);\n                break;\n            case MO_16:\n                gen_helper_divw_AX(cpu_env, cpu_T0);\n                break;\n            default:\n            case MO_32:\n                gen_helper_divl_EAX(cpu_env, cpu_T0);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                gen_helper_divq_EAX(cpu_env, cpu_T0);\n                break;\n#endif\n            }\n            break;\n        case 7: /* idiv */\n            switch(ot) {\n            case MO_8:\n                gen_helper_idivb_AL(cpu_env, cpu_T0);\n                break;\n            case MO_16:\n                gen_helper_idivw_AX(cpu_env, cpu_T0);\n                break;\n            default:\n            case MO_32:\n                gen_helper_idivl_EAX(cpu_env, cpu_T0);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                gen_helper_idivq_EAX(cpu_env, cpu_T0);\n                break;\n#endif\n            }\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0xfe: /* GRP4 */\n    case 0xff: /* GRP5 */\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        op = (modrm >> 3) & 7;\n        if (op >= 2 && b == 0xfe) {\n            goto unknown_op;\n        }\n        if (CODE64(s)) {\n            if (op == 2 || op == 4) {\n                /* operand size for jumps is 64 bit */\n                ot = MO_64;\n            } else if (op == 3 || op == 5) {\n                ot = dflag != MO_16 ? MO_32 + (rex_w == 1) : MO_16;\n            } else if (op == 6) {\n                /* default push size is 64 bit */\n                ot = mo_pushpop(s, dflag);\n            }\n        }\n        if (mod != 3) {\n            gen_lea_modrm(env, s, modrm);\n            if (op >= 2 && op != 3 && op != 5)\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n\n        switch(op) {\n        case 0: /* inc Ev */\n            if (mod != 3)\n                opreg = OR_TMP0;\n            else\n                opreg = rm;\n            gen_inc(s, ot, opreg, 1);\n            break;\n        case 1: /* dec Ev */\n            if (mod != 3)\n                opreg = OR_TMP0;\n            else\n                opreg = rm;\n            gen_inc(s, ot, opreg, -1);\n            break;\n        case 2: /* call Ev */\n            /* XXX: optimize if memory (no 'and' is necessary) */\n            if (dflag == MO_16) {\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n            }\n            next_eip = s->pc - s->cs_base;\n            tcg_gen_movi_tl(cpu_T1, next_eip);\n            gen_push_v(s, cpu_T1);\n            gen_op_jmp_v(cpu_T0);\n            gen_bnd_jmp(s);\n            gen_eob(s);\n            break;\n        case 3: /* lcall Ev */\n            gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 1 << ot);\n            gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        do_lcall:\n            if (s->pe && !s->vm86) {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lcall_protected(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                           tcg_const_i32(dflag - 1),\n                                           tcg_const_tl(s->pc - s->cs_base));\n            } else {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lcall_real(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                      tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(s->pc - s->cs_base));\n            }\n            gen_eob(s);\n            break;\n        case 4: /* jmp Ev */\n            if (dflag == MO_16) {\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n            }\n            gen_op_jmp_v(cpu_T0);\n            gen_bnd_jmp(s);\n            gen_eob(s);\n            break;\n        case 5: /* ljmp Ev */\n            gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 1 << ot);\n            gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        do_ljmp:\n            if (s->pe && !s->vm86) {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_ljmp_protected(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                          tcg_const_tl(s->pc - s->cs_base));\n            } else {\n                gen_op_movl_seg_T0_vm(R_CS);\n                gen_op_jmp_v(cpu_T1);\n            }\n            gen_eob(s);\n            break;\n        case 6: /* push Ev */\n            gen_push_v(s, cpu_T0);\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x84: /* test Ev, Gv */\n    case 0x85:\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_op_mov_v_reg(ot, cpu_T1, reg);\n        gen_op_testl_T0_T1_cc();\n        set_cc_op(s, CC_OP_LOGICB + ot);\n        break;\n\n    case 0xa8: /* test eAX, Iv */\n    case 0xa9:\n        ot = mo_b_d(b, dflag);\n        val = insn_get(env, s, ot);\n\n        gen_op_mov_v_reg(ot, cpu_T0, OR_EAX);\n        tcg_gen_movi_tl(cpu_T1, val);\n        gen_op_testl_T0_T1_cc();\n        set_cc_op(s, CC_OP_LOGICB + ot);\n        break;\n\n    case 0x98: /* CWDE/CBW */\n        switch (dflag) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n            tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_64, R_EAX, cpu_T0);\n            break;\n#endif\n        case MO_32:\n            gen_op_mov_v_reg(MO_16, cpu_T0, R_EAX);\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_32, R_EAX, cpu_T0);\n            break;\n        case MO_16:\n            gen_op_mov_v_reg(MO_8, cpu_T0, R_EAX);\n            tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n            break;\n        default:\n            tcg_abort();\n        }\n        break;\n    case 0x99: /* CDQ/CWD */\n        switch (dflag) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_op_mov_v_reg(MO_64, cpu_T0, R_EAX);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 63);\n            gen_op_mov_reg_v(MO_64, R_EDX, cpu_T0);\n            break;\n#endif\n        case MO_32:\n            gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n            tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 31);\n            gen_op_mov_reg_v(MO_32, R_EDX, cpu_T0);\n            break;\n        case MO_16:\n            gen_op_mov_v_reg(MO_16, cpu_T0, R_EAX);\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 15);\n            gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n            break;\n        default:\n            tcg_abort();\n        }\n        break;\n    case 0x1af: /* imul Gv, Ev */\n    case 0x69: /* imul Gv, Ev, I */\n    case 0x6b:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        if (b == 0x69)\n            s->rip_offset = insn_const_size(ot);\n        else if (b == 0x6b)\n            s->rip_offset = 1;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        if (b == 0x69) {\n            val = insn_get(env, s, ot);\n            tcg_gen_movi_tl(cpu_T1, val);\n        } else if (b == 0x6b) {\n            val = (int8_t)insn_get(env, s, MO_8);\n            tcg_gen_movi_tl(cpu_T1, val);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T1, reg);\n        }\n        switch (ot) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            tcg_gen_muls2_i64(cpu_regs[reg], cpu_T1, cpu_T0, cpu_T1);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[reg]);\n            tcg_gen_sari_tl(cpu_cc_src, cpu_cc_dst, 63);\n            tcg_gen_sub_tl(cpu_cc_src, cpu_cc_src, cpu_T1);\n            break;\n#endif\n        case MO_32:\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n            tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n            tcg_gen_muls2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                              cpu_tmp2_i32, cpu_tmp3_i32);\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            tcg_gen_sari_i32(cpu_tmp2_i32, cpu_tmp2_i32, 31);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[reg]);\n            tcg_gen_sub_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n            tcg_gen_extu_i32_tl(cpu_cc_src, cpu_tmp2_i32);\n            break;\n        default:\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            tcg_gen_ext16s_tl(cpu_T1, cpu_T1);\n            /* XXX: use 32 bit mul which could be faster */\n            tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n            tcg_gen_ext16s_tl(cpu_tmp0, cpu_T0);\n            tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n            break;\n        }\n        set_cc_op(s, CC_OP_MULB + ot);\n        break;\n    case 0x1c0:\n    case 0x1c1: /* xadd Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        gen_op_mov_v_reg(ot, cpu_T0, reg);\n        if (mod == 3) {\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_mov_v_reg(ot, cpu_T1, rm);\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            gen_lea_modrm(env, s, modrm);\n            if (s->prefix & PREFIX_LOCK) {\n                tcg_gen_atomic_fetch_add_tl(cpu_T1, cpu_A0, cpu_T0,\n                                            s->mem_index, ot | MO_LE);\n                tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            } else {\n                gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n                tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n            }\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        }\n        gen_op_update2_cc();\n        set_cc_op(s, CC_OP_ADDB + ot);\n        break;\n    case 0x1b0:\n    case 0x1b1: /* cmpxchg Ev, Gv */\n        {\n            TCGv oldv, newv, cmpv;\n\n            ot = mo_b_d(b, dflag);\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            oldv = tcg_temp_new();\n            newv = tcg_temp_new();\n            cmpv = tcg_temp_new();\n            gen_op_mov_v_reg(ot, newv, reg);\n            tcg_gen_mov_tl(cmpv, cpu_regs[R_EAX]);\n\n            if (s->prefix & PREFIX_LOCK) {\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_atomic_cmpxchg_tl(oldv, cpu_A0, cmpv, newv,\n                                          s->mem_index, ot | MO_LE);\n                gen_op_mov_reg_v(ot, R_EAX, oldv);\n            } else {\n                if (mod == 3) {\n                    rm = (modrm & 7) | REX_B(s);\n                    gen_op_mov_v_reg(ot, oldv, rm);\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    gen_op_ld_v(s, ot, oldv, cpu_A0);\n                    rm = 0; /* avoid warning */\n                }\n                gen_extu(ot, oldv);\n                gen_extu(ot, cmpv);\n                /* store value = (old == cmp ? new : old);  */\n                tcg_gen_movcond_tl(TCG_COND_EQ, newv, oldv, cmpv, newv, oldv);\n                if (mod == 3) {\n                    gen_op_mov_reg_v(ot, R_EAX, oldv);\n                    gen_op_mov_reg_v(ot, rm, newv);\n                } else {\n                    /* Perform an unconditional store cycle like physical cpu;\n                       must be before changing accumulator to ensure\n                       idempotency if the store faults and the instruction\n                       is restarted */\n                    gen_op_st_v(s, ot, newv, cpu_A0);\n                    gen_op_mov_reg_v(ot, R_EAX, oldv);\n                }\n            }\n            tcg_gen_mov_tl(cpu_cc_src, oldv);\n            tcg_gen_mov_tl(cpu_cc_srcT, cmpv);\n            tcg_gen_sub_tl(cpu_cc_dst, cmpv, oldv);\n            set_cc_op(s, CC_OP_SUBB + ot);\n            tcg_temp_free(oldv);\n            tcg_temp_free(newv);\n            tcg_temp_free(cmpv);\n        }\n        break;\n    case 0x1c7: /* cmpxchg8b */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if ((mod == 3) || ((modrm & 0x38) != 0x8))\n            goto illegal_op;\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            if (!(s->cpuid_ext_features & CPUID_EXT_CX16))\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if ((s->prefix & PREFIX_LOCK) && parallel_cpus) {\n                gen_helper_cmpxchg16b(cpu_env, cpu_A0);\n            } else {\n                gen_helper_cmpxchg16b_unlocked(cpu_env, cpu_A0);\n            }\n        } else\n#endif        \n        {\n            if (!(s->cpuid_features & CPUID_CX8))\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if ((s->prefix & PREFIX_LOCK) && parallel_cpus) {\n                gen_helper_cmpxchg8b(cpu_env, cpu_A0);\n            } else {\n                gen_helper_cmpxchg8b_unlocked(cpu_env, cpu_A0);\n            }\n        }\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n\n        /**************************/\n        /* push/pop */\n    case 0x50 ... 0x57: /* push */\n        gen_op_mov_v_reg(MO_32, cpu_T0, (b & 7) | REX_B(s));\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x58 ... 0x5f: /* pop */\n        ot = gen_pop_T0(s);\n        /* NOTE: order is important for pop %sp */\n        gen_pop_update(s, ot);\n        gen_op_mov_reg_v(ot, (b & 7) | REX_B(s), cpu_T0);\n        break;\n    case 0x60: /* pusha */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_pusha(s);\n        break;\n    case 0x61: /* popa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_popa(s);\n        break;\n    case 0x68: /* push Iv */\n    case 0x6a:\n        ot = mo_pushpop(s, dflag);\n        if (b == 0x68)\n            val = insn_get(env, s, ot);\n        else\n            val = (int8_t)insn_get(env, s, MO_8);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x8f: /* pop Ev */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        ot = gen_pop_T0(s);\n        if (mod == 3) {\n            /* NOTE: order is important for pop %sp */\n            gen_pop_update(s, ot);\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            /* NOTE: order is important too for MMU exceptions */\n            s->popl_esp_hack = 1 << ot;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            s->popl_esp_hack = 0;\n            gen_pop_update(s, ot);\n        }\n        break;\n    case 0xc8: /* enter */\n        {\n            int level;\n            val = cpu_lduw_code(env, s->pc);\n            s->pc += 2;\n            level = cpu_ldub_code(env, s->pc++);\n            gen_enter(s, val, level);\n        }\n        break;\n    case 0xc9: /* leave */\n        gen_leave(s);\n        break;\n    case 0x06: /* push es */\n    case 0x0e: /* push cs */\n    case 0x16: /* push ss */\n    case 0x1e: /* push ds */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_op_movl_T0_seg(b >> 3);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x1a0: /* push fs */\n    case 0x1a8: /* push gs */\n        gen_op_movl_T0_seg((b >> 3) & 7);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x07: /* pop es */\n    case 0x17: /* pop ss */\n    case 0x1f: /* pop ds */\n        if (CODE64(s))\n            goto illegal_op;\n        reg = b >> 3;\n        ot = gen_pop_T0(s);\n        gen_movl_seg_T0(s, reg);\n        gen_pop_update(s, ot);\n        /* Note that reg == R_SS in gen_movl_seg_T0 always sets is_jmp.  */\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            if (reg == R_SS) {\n                s->tf = 0;\n                gen_eob_inhibit_irq(s, true);\n            } else {\n                gen_eob(s);\n            }\n        }\n        break;\n    case 0x1a1: /* pop fs */\n    case 0x1a9: /* pop gs */\n        ot = gen_pop_T0(s);\n        gen_movl_seg_T0(s, (b >> 3) & 7);\n        gen_pop_update(s, ot);\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n\n        /**************************/\n        /* mov */\n    case 0x88:\n    case 0x89: /* mov Gv, Ev */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        /* generate a generic store */\n        gen_ldst_modrm(env, s, modrm, ot, reg, 1);\n        break;\n    case 0xc6:\n    case 0xc7: /* mov Ev, Iv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod != 3) {\n            s->rip_offset = insn_const_size(ot);\n            gen_lea_modrm(env, s, modrm);\n        }\n        val = insn_get(env, s, ot);\n        tcg_gen_movi_tl(cpu_T0, val);\n        if (mod != 3) {\n            gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_mov_reg_v(ot, (modrm & 7) | REX_B(s), cpu_T0);\n        }\n        break;\n    case 0x8a:\n    case 0x8b: /* mov Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n        break;\n    case 0x8e: /* mov seg, Gv */\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        if (reg >= 6 || reg == R_CS)\n            goto illegal_op;\n        gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n        gen_movl_seg_T0(s, reg);\n        /* Note that reg == R_SS in gen_movl_seg_T0 always sets is_jmp.  */\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            if (reg == R_SS) {\n                s->tf = 0;\n                gen_eob_inhibit_irq(s, true);\n            } else {\n                gen_eob(s);\n            }\n        }\n        break;\n    case 0x8c: /* mov Gv, seg */\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        if (reg >= 6)\n            goto illegal_op;\n        gen_op_movl_T0_seg(reg);\n        ot = mod == 3 ? dflag : MO_16;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n        break;\n\n    case 0x1b6: /* movzbS Gv, Eb */\n    case 0x1b7: /* movzwS Gv, Eb */\n    case 0x1be: /* movsbS Gv, Eb */\n    case 0x1bf: /* movswS Gv, Eb */\n        {\n            TCGMemOp d_ot;\n            TCGMemOp s_ot;\n\n            /* d_ot is the size of destination */\n            d_ot = dflag;\n            /* ot is the size of source */\n            ot = (b & 1) + MO_8;\n            /* s_ot is the sign+size of source */\n            s_ot = b & 8 ? MO_SIGN | ot : ot;\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n\n            if (mod == 3) {\n                if (s_ot == MO_SB && byte_reg_is_xH(rm)) {\n                    tcg_gen_sextract_tl(cpu_T0, cpu_regs[rm - 4], 8, 8);\n                } else {\n                    gen_op_mov_v_reg(ot, cpu_T0, rm);\n                    switch (s_ot) {\n                    case MO_UB:\n                        tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n                        break;\n                    case MO_SB:\n                        tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n                        break;\n                    case MO_UW:\n                        tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n                        break;\n                    default:\n                    case MO_SW:\n                        tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n                        break;\n                    }\n                }\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            } else {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, s_ot, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            }\n        }\n        break;\n\n    case 0x8d: /* lea */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        reg = ((modrm >> 3) & 7) | rex_r;\n        {\n            AddressParts a = gen_lea_modrm_0(env, s, modrm);\n            TCGv ea = gen_lea_modrm_1(a);\n            gen_lea_v_seg(s, s->aflag, ea, -1, -1);\n            gen_op_mov_reg_v(dflag, reg, cpu_A0);\n        }\n        break;\n\n    case 0xa0: /* mov EAX, Ov */\n    case 0xa1:\n    case 0xa2: /* mov Ov, EAX */\n    case 0xa3:\n        {\n            target_ulong offset_addr;\n\n            ot = mo_b_d(b, dflag);\n            switch (s->aflag) {\n#ifdef TARGET_X86_64\n            case MO_64:\n                offset_addr = cpu_ldq_code(env, s->pc);\n                s->pc += 8;\n                break;\n#endif\n            default:\n                offset_addr = insn_get(env, s, s->aflag);\n                break;\n            }\n            tcg_gen_movi_tl(cpu_A0, offset_addr);\n            gen_add_A0_ds_seg(s);\n            if ((b & 2) == 0) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(ot, R_EAX, cpu_T0);\n            } else {\n                gen_op_mov_v_reg(ot, cpu_T0, R_EAX);\n                gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n            }\n        }\n        break;\n    case 0xd7: /* xlat */\n        tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EBX]);\n        tcg_gen_ext8u_tl(cpu_T0, cpu_regs[R_EAX]);\n        tcg_gen_add_tl(cpu_A0, cpu_A0, cpu_T0);\n        gen_extu(s->aflag, cpu_A0);\n        gen_add_A0_ds_seg(s);\n        gen_op_ld_v(s, MO_8, cpu_T0, cpu_A0);\n        gen_op_mov_reg_v(MO_8, R_EAX, cpu_T0);\n        break;\n    case 0xb0 ... 0xb7: /* mov R, Ib */\n        val = insn_get(env, s, MO_8);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_op_mov_reg_v(MO_8, (b & 7) | REX_B(s), cpu_T0);\n        break;\n    case 0xb8 ... 0xbf: /* mov R, Iv */\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            uint64_t tmp;\n            /* 64 bit case */\n            tmp = cpu_ldq_code(env, s->pc);\n            s->pc += 8;\n            reg = (b & 7) | REX_B(s);\n            tcg_gen_movi_tl(cpu_T0, tmp);\n            gen_op_mov_reg_v(MO_64, reg, cpu_T0);\n        } else\n#endif\n        {\n            ot = dflag;\n            val = insn_get(env, s, ot);\n            reg = (b & 7) | REX_B(s);\n            tcg_gen_movi_tl(cpu_T0, val);\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n        }\n        break;\n\n    case 0x91 ... 0x97: /* xchg R, EAX */\n    do_xchg_reg_eax:\n        ot = dflag;\n        reg = (b & 7) | REX_B(s);\n        rm = R_EAX;\n        goto do_xchg_reg;\n    case 0x86:\n    case 0x87: /* xchg Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3) {\n            rm = (modrm & 7) | REX_B(s);\n        do_xchg_reg:\n            gen_op_mov_v_reg(ot, cpu_T0, reg);\n            gen_op_mov_v_reg(ot, cpu_T1, rm);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        } else {\n            gen_lea_modrm(env, s, modrm);\n            gen_op_mov_v_reg(ot, cpu_T0, reg);\n            /* for xchg, lock is implicit */\n            tcg_gen_atomic_xchg_tl(cpu_T1, cpu_A0, cpu_T0,\n                                   s->mem_index, ot | MO_LE);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        }\n        break;\n    case 0xc4: /* les Gv */\n        /* In CODE64 this is VEX3; see above.  */\n        op = R_ES;\n        goto do_lxx;\n    case 0xc5: /* lds Gv */\n        /* In CODE64 this is VEX2; see above.  */\n        op = R_DS;\n        goto do_lxx;\n    case 0x1b2: /* lss Gv */\n        op = R_SS;\n        goto do_lxx;\n    case 0x1b4: /* lfs Gv */\n        op = R_FS;\n        goto do_lxx;\n    case 0x1b5: /* lgs Gv */\n        op = R_GS;\n    do_lxx:\n        ot = dflag != MO_16 ? MO_32 : MO_16;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_lea_modrm(env, s, modrm);\n        gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n        gen_add_A0_im(s, 1 << ot);\n        /* load the segment first to handle exceptions properly */\n        gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        gen_movl_seg_T0(s, op);\n        /* then put the data */\n        gen_op_mov_reg_v(ot, reg, cpu_T1);\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n\n        /************************/\n        /* shifts */\n    case 0xc0:\n    case 0xc1:\n        /* shift Ev,Ib */\n        shift = 2;\n    grp2:\n        {\n            ot = mo_b_d(b, dflag);\n            modrm = cpu_ldub_code(env, s->pc++);\n            mod = (modrm >> 6) & 3;\n            op = (modrm >> 3) & 7;\n\n            if (mod != 3) {\n                if (shift == 2) {\n                    s->rip_offset = 1;\n                }\n                gen_lea_modrm(env, s, modrm);\n                opreg = OR_TMP0;\n            } else {\n                opreg = (modrm & 7) | REX_B(s);\n            }\n\n            /* simpler op */\n            if (shift == 0) {\n                gen_shift(s, op, ot, opreg, OR_ECX);\n            } else {\n                if (shift == 2) {\n                    shift = cpu_ldub_code(env, s->pc++);\n                }\n                gen_shifti(s, op, ot, opreg, shift);\n            }\n        }\n        break;\n    case 0xd0:\n    case 0xd1:\n        /* shift Ev,1 */\n        shift = 1;\n        goto grp2;\n    case 0xd2:\n    case 0xd3:\n        /* shift Ev,cl */\n        shift = 0;\n        goto grp2;\n\n    case 0x1a4: /* shld imm */\n        op = 0;\n        shift = 1;\n        goto do_shiftd;\n    case 0x1a5: /* shld cl */\n        op = 0;\n        shift = 0;\n        goto do_shiftd;\n    case 0x1ac: /* shrd imm */\n        op = 1;\n        shift = 1;\n        goto do_shiftd;\n    case 0x1ad: /* shrd cl */\n        op = 1;\n        shift = 0;\n    do_shiftd:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        if (mod != 3) {\n            gen_lea_modrm(env, s, modrm);\n            opreg = OR_TMP0;\n        } else {\n            opreg = rm;\n        }\n        gen_op_mov_v_reg(ot, cpu_T1, reg);\n\n        if (shift) {\n            TCGv imm = tcg_const_tl(cpu_ldub_code(env, s->pc++));\n            gen_shiftd_rm_T1(s, ot, opreg, op, imm);\n            tcg_temp_free(imm);\n        } else {\n            gen_shiftd_rm_T1(s, ot, opreg, op, cpu_regs[R_ECX]);\n        }\n        break;\n\n        /************************/\n        /* floats */\n    case 0xd8 ... 0xdf:\n        if (s->flags & (HF_EM_MASK | HF_TS_MASK)) {\n            /* if CR0.EM or CR0.TS are set, generate an FPU exception */\n            /* XXX: what to do if illegal op ? */\n            gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n            break;\n        }\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = modrm & 7;\n        op = ((b & 7) << 3) | ((modrm >> 3) & 7);\n        if (mod != 3) {\n            /* memory op */\n            gen_lea_modrm(env, s, modrm);\n            switch(op) {\n            case 0x00 ... 0x07: /* fxxxs */\n            case 0x10 ... 0x17: /* fixxxl */\n            case 0x20 ... 0x27: /* fxxxl */\n            case 0x30 ... 0x37: /* fixxx */\n                {\n                    int op1;\n                    op1 = op & 7;\n\n                    switch(op >> 4) {\n                    case 0:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_flds_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 1:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_fildl_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 2:\n                        tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        gen_helper_fldl_FT0(cpu_env, cpu_tmp1_i64);\n                        break;\n                    case 3:\n                    default:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LESW);\n                        gen_helper_fildl_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    }\n\n                    gen_helper_fp_arith_ST0_FT0(op1);\n                    if (op1 == 3) {\n                        /* fcomp needs pop */\n                        gen_helper_fpop(cpu_env);\n                    }\n                }\n                break;\n            case 0x08: /* flds */\n            case 0x0a: /* fsts */\n            case 0x0b: /* fstps */\n            case 0x18 ... 0x1b: /* fildl, fisttpl, fistl, fistpl */\n            case 0x28 ... 0x2b: /* fldl, fisttpll, fstl, fstpl */\n            case 0x38 ... 0x3b: /* filds, fisttps, fists, fistps */\n                switch(op & 7) {\n                case 0:\n                    switch(op >> 4) {\n                    case 0:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_flds_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 1:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_fildl_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 2:\n                        tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        gen_helper_fldl_ST0(cpu_env, cpu_tmp1_i64);\n                        break;\n                    case 3:\n                    default:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LESW);\n                        gen_helper_fildl_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    }\n                    break;\n                case 1:\n                    /* XXX: the corresponding CPUID bit must be tested ! */\n                    switch(op >> 4) {\n                    case 1:\n                        gen_helper_fisttl_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 2:\n                        gen_helper_fisttll_ST0(cpu_tmp1_i64, cpu_env);\n                        tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        break;\n                    case 3:\n                    default:\n                        gen_helper_fistt_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUW);\n                        break;\n                    }\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    switch(op >> 4) {\n                    case 0:\n                        gen_helper_fsts_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 1:\n                        gen_helper_fistl_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 2:\n                        gen_helper_fstl_ST0(cpu_tmp1_i64, cpu_env);\n                        tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        break;\n                    case 3:\n                    default:\n                        gen_helper_fist_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUW);\n                        break;\n                    }\n                    if ((op & 7) == 3)\n                        gen_helper_fpop(cpu_env);\n                    break;\n                }\n                break;\n            case 0x0c: /* fldenv mem */\n                gen_helper_fldenv(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x0d: /* fldcw mem */\n                tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                gen_helper_fldcw(cpu_env, cpu_tmp2_i32);\n                break;\n            case 0x0e: /* fnstenv mem */\n                gen_helper_fstenv(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x0f: /* fnstcw mem */\n                gen_helper_fnstcw(cpu_tmp2_i32, cpu_env);\n                tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                break;\n            case 0x1d: /* fldt mem */\n                gen_helper_fldt_ST0(cpu_env, cpu_A0);\n                break;\n            case 0x1f: /* fstpt mem */\n                gen_helper_fstt_ST0(cpu_env, cpu_A0);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x2c: /* frstor mem */\n                gen_helper_frstor(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x2e: /* fnsave mem */\n                gen_helper_fsave(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x2f: /* fnstsw mem */\n                gen_helper_fnstsw(cpu_tmp2_i32, cpu_env);\n                tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                break;\n            case 0x3c: /* fbld */\n                gen_helper_fbld_ST0(cpu_env, cpu_A0);\n                break;\n            case 0x3e: /* fbstp */\n                gen_helper_fbst_ST0(cpu_env, cpu_A0);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x3d: /* fildll */\n                tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n                gen_helper_fildll_ST0(cpu_env, cpu_tmp1_i64);\n                break;\n            case 0x3f: /* fistpll */\n                gen_helper_fistll_ST0(cpu_tmp1_i64, cpu_env);\n                tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n                gen_helper_fpop(cpu_env);\n                break;\n            default:\n                goto unknown_op;\n            }\n        } else {\n            /* register float ops */\n            opreg = rm;\n\n            switch(op) {\n            case 0x08: /* fld sti */\n                gen_helper_fpush(cpu_env);\n                gen_helper_fmov_ST0_STN(cpu_env,\n                                        tcg_const_i32((opreg + 1) & 7));\n                break;\n            case 0x09: /* fxchg sti */\n            case 0x29: /* fxchg4 sti, undocumented op */\n            case 0x39: /* fxchg7 sti, undocumented op */\n                gen_helper_fxchg_ST0_STN(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x0a: /* grp d9/2 */\n                switch(rm) {\n                case 0: /* fnop */\n                    /* check exceptions (FreeBSD FPU probe) */\n                    gen_helper_fwait(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x0c: /* grp d9/4 */\n                switch(rm) {\n                case 0: /* fchs */\n                    gen_helper_fchs_ST0(cpu_env);\n                    break;\n                case 1: /* fabs */\n                    gen_helper_fabs_ST0(cpu_env);\n                    break;\n                case 4: /* ftst */\n                    gen_helper_fldz_FT0(cpu_env);\n                    gen_helper_fcom_ST0_FT0(cpu_env);\n                    break;\n                case 5: /* fxam */\n                    gen_helper_fxam_ST0(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x0d: /* grp d9/5 */\n                {\n                    switch(rm) {\n                    case 0:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fld1_ST0(cpu_env);\n                        break;\n                    case 1:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldl2t_ST0(cpu_env);\n                        break;\n                    case 2:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldl2e_ST0(cpu_env);\n                        break;\n                    case 3:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldpi_ST0(cpu_env);\n                        break;\n                    case 4:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldlg2_ST0(cpu_env);\n                        break;\n                    case 5:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldln2_ST0(cpu_env);\n                        break;\n                    case 6:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldz_ST0(cpu_env);\n                        break;\n                    default:\n                        goto unknown_op;\n                    }\n                }\n                break;\n            case 0x0e: /* grp d9/6 */\n                switch(rm) {\n                case 0: /* f2xm1 */\n                    gen_helper_f2xm1(cpu_env);\n                    break;\n                case 1: /* fyl2x */\n                    gen_helper_fyl2x(cpu_env);\n                    break;\n                case 2: /* fptan */\n                    gen_helper_fptan(cpu_env);\n                    break;\n                case 3: /* fpatan */\n                    gen_helper_fpatan(cpu_env);\n                    break;\n                case 4: /* fxtract */\n                    gen_helper_fxtract(cpu_env);\n                    break;\n                case 5: /* fprem1 */\n                    gen_helper_fprem1(cpu_env);\n                    break;\n                case 6: /* fdecstp */\n                    gen_helper_fdecstp(cpu_env);\n                    break;\n                default:\n                case 7: /* fincstp */\n                    gen_helper_fincstp(cpu_env);\n                    break;\n                }\n                break;\n            case 0x0f: /* grp d9/7 */\n                switch(rm) {\n                case 0: /* fprem */\n                    gen_helper_fprem(cpu_env);\n                    break;\n                case 1: /* fyl2xp1 */\n                    gen_helper_fyl2xp1(cpu_env);\n                    break;\n                case 2: /* fsqrt */\n                    gen_helper_fsqrt(cpu_env);\n                    break;\n                case 3: /* fsincos */\n                    gen_helper_fsincos(cpu_env);\n                    break;\n                case 5: /* fscale */\n                    gen_helper_fscale(cpu_env);\n                    break;\n                case 4: /* frndint */\n                    gen_helper_frndint(cpu_env);\n                    break;\n                case 6: /* fsin */\n                    gen_helper_fsin(cpu_env);\n                    break;\n                default:\n                case 7: /* fcos */\n                    gen_helper_fcos(cpu_env);\n                    break;\n                }\n                break;\n            case 0x00: case 0x01: case 0x04 ... 0x07: /* fxxx st, sti */\n            case 0x20: case 0x21: case 0x24 ... 0x27: /* fxxx sti, st */\n            case 0x30: case 0x31: case 0x34 ... 0x37: /* fxxxp sti, st */\n                {\n                    int op1;\n\n                    op1 = op & 7;\n                    if (op >= 0x20) {\n                        gen_helper_fp_arith_STN_ST0(op1, opreg);\n                        if (op >= 0x30)\n                            gen_helper_fpop(cpu_env);\n                    } else {\n                        gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                        gen_helper_fp_arith_ST0_FT0(op1);\n                    }\n                }\n                break;\n            case 0x02: /* fcom */\n            case 0x22: /* fcom2, undocumented op */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcom_ST0_FT0(cpu_env);\n                break;\n            case 0x03: /* fcomp */\n            case 0x23: /* fcomp3, undocumented op */\n            case 0x32: /* fcomp5, undocumented op */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcom_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x15: /* da/5 */\n                switch(rm) {\n                case 1: /* fucompp */\n                    gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(1));\n                    gen_helper_fucom_ST0_FT0(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x1c:\n                switch(rm) {\n                case 0: /* feni (287 only, just do nop here) */\n                    break;\n                case 1: /* fdisi (287 only, just do nop here) */\n                    break;\n                case 2: /* fclex */\n                    gen_helper_fclex(cpu_env);\n                    break;\n                case 3: /* fninit */\n                    gen_helper_fninit(cpu_env);\n                    break;\n                case 4: /* fsetpm (287 only, just do nop here) */\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x1d: /* fucomi */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucomi_ST0_FT0(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x1e: /* fcomi */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcomi_ST0_FT0(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x28: /* ffree sti */\n                gen_helper_ffree_STN(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x2a: /* fst sti */\n                gen_helper_fmov_STN_ST0(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x2b: /* fstp sti */\n            case 0x0b: /* fstp1 sti, undocumented op */\n            case 0x3a: /* fstp8 sti, undocumented op */\n            case 0x3b: /* fstp9 sti, undocumented op */\n                gen_helper_fmov_STN_ST0(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x2c: /* fucom st(i) */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucom_ST0_FT0(cpu_env);\n                break;\n            case 0x2d: /* fucomp st(i) */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucom_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x33: /* de/3 */\n                switch(rm) {\n                case 1: /* fcompp */\n                    gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(1));\n                    gen_helper_fcom_ST0_FT0(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x38: /* ffreep sti, undocumented op */\n                gen_helper_ffree_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x3c: /* df/4 */\n                switch(rm) {\n                case 0:\n                    gen_helper_fnstsw(cpu_tmp2_i32, cpu_env);\n                    tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n                    gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x3d: /* fucomip */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucomi_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x3e: /* fcomip */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcomi_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x10 ... 0x13: /* fcmovxx */\n            case 0x18 ... 0x1b:\n                {\n                    int op1;\n                    TCGLabel *l1;\n                    static const uint8_t fcmov_cc[8] = {\n                        (JCC_B << 1),\n                        (JCC_Z << 1),\n                        (JCC_BE << 1),\n                        (JCC_P << 1),\n                    };\n\n                    if (!(s->cpuid_features & CPUID_CMOV)) {\n                        goto illegal_op;\n                    }\n                    op1 = fcmov_cc[op & 3] | (((op >> 3) & 1) ^ 1);\n                    l1 = gen_new_label();\n                    gen_jcc1_noeob(s, op1, l1);\n                    gen_helper_fmov_ST0_STN(cpu_env, tcg_const_i32(opreg));\n                    gen_set_label(l1);\n                }\n                break;\n            default:\n                goto unknown_op;\n            }\n        }\n        break;\n        /************************/\n        /* string ops */\n\n    case 0xa4: /* movsS */\n    case 0xa5:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_movs(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_movs(s, ot);\n        }\n        break;\n\n    case 0xaa: /* stosS */\n    case 0xab:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_stos(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_stos(s, ot);\n        }\n        break;\n    case 0xac: /* lodsS */\n    case 0xad:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_lods(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_lods(s, ot);\n        }\n        break;\n    case 0xae: /* scasS */\n    case 0xaf:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & PREFIX_REPNZ) {\n            gen_repz_scas(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 1);\n        } else if (prefixes & PREFIX_REPZ) {\n            gen_repz_scas(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 0);\n        } else {\n            gen_scas(s, ot);\n        }\n        break;\n\n    case 0xa6: /* cmpsS */\n    case 0xa7:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & PREFIX_REPNZ) {\n            gen_repz_cmps(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 1);\n        } else if (prefixes & PREFIX_REPZ) {\n            gen_repz_cmps(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 0);\n        } else {\n            gen_cmps(s, ot);\n        }\n        break;\n    case 0x6c: /* insS */\n    case 0x6d:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base, \n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes) | 4);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_ins(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_ins(s, ot);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n        }\n        break;\n    case 0x6e: /* outsS */\n    case 0x6f:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes) | 4);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_outs(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_outs(s, ot);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n        }\n        break;\n\n        /************************/\n        /* port I/O */\n\n    case 0xe4:\n    case 0xe5:\n        ot = mo_b_d32(b, dflag);\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes));\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_movi_i32(cpu_tmp2_i32, val);\n        gen_helper_in_func(ot, cpu_T1, cpu_tmp2_i32);\n        gen_op_mov_reg_v(ot, R_EAX, cpu_T1);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xe6:\n    case 0xe7:\n        ot = mo_b_d32(b, dflag);\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes));\n        gen_op_mov_v_reg(ot, cpu_T1, R_EAX);\n\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_movi_i32(cpu_tmp2_i32, val);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xec:\n    case 0xed:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes));\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        gen_helper_in_func(ot, cpu_T1, cpu_tmp2_i32);\n        gen_op_mov_reg_v(ot, R_EAX, cpu_T1);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xee:\n    case 0xef:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes));\n        gen_op_mov_v_reg(ot, cpu_T1, R_EAX);\n\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n\n        /************************/\n        /* control */\n    case 0xc2: /* ret im */\n        val = cpu_ldsw_code(env, s->pc);\n        s->pc += 2;\n        ot = gen_pop_T0(s);\n        gen_stack_update(s, val + (1 << ot));\n        /* Note that gen_pop_T0 uses a zero-extending load.  */\n        gen_op_jmp_v(cpu_T0);\n        gen_bnd_jmp(s);\n        gen_eob(s);\n        break;\n    case 0xc3: /* ret */\n        ot = gen_pop_T0(s);\n        gen_pop_update(s, ot);\n        /* Note that gen_pop_T0 uses a zero-extending load.  */\n        gen_op_jmp_v(cpu_T0);\n        gen_bnd_jmp(s);\n        gen_eob(s);\n        break;\n    case 0xca: /* lret im */\n        val = cpu_ldsw_code(env, s->pc);\n        s->pc += 2;\n    do_lret:\n        if (s->pe && !s->vm86) {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_lret_protected(cpu_env, tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(val));\n        } else {\n            gen_stack_A0(s);\n            /* pop offset */\n            gen_op_ld_v(s, dflag, cpu_T0, cpu_A0);\n            /* NOTE: keeping EIP updated is not a problem in case of\n               exception */\n            gen_op_jmp_v(cpu_T0);\n            /* pop selector */\n            gen_add_A0_im(s, 1 << dflag);\n            gen_op_ld_v(s, dflag, cpu_T0, cpu_A0);\n            gen_op_movl_seg_T0_vm(R_CS);\n            /* add stack offset */\n            gen_stack_update(s, val + (2 << dflag));\n        }\n        gen_eob(s);\n        break;\n    case 0xcb: /* lret */\n        val = 0;\n        goto do_lret;\n    case 0xcf: /* iret */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_IRET);\n        if (!s->pe) {\n            /* real mode */\n            gen_helper_iret_real(cpu_env, tcg_const_i32(dflag - 1));\n            set_cc_op(s, CC_OP_EFLAGS);\n        } else if (s->vm86) {\n            if (s->iopl != 3) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_helper_iret_real(cpu_env, tcg_const_i32(dflag - 1));\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n        } else {\n            gen_helper_iret_protected(cpu_env, tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(s->pc - s->cs_base));\n            set_cc_op(s, CC_OP_EFLAGS);\n        }\n        gen_eob(s);\n        break;\n    case 0xe8: /* call im */\n        {\n            if (dflag != MO_16) {\n                tval = (int32_t)insn_get(env, s, MO_32);\n            } else {\n                tval = (int16_t)insn_get(env, s, MO_16);\n            }\n            next_eip = s->pc - s->cs_base;\n            tval += next_eip;\n            if (dflag == MO_16) {\n                tval &= 0xffff;\n            } else if (!CODE64(s)) {\n                tval &= 0xffffffff;\n            }\n            tcg_gen_movi_tl(cpu_T0, next_eip);\n            gen_push_v(s, cpu_T0);\n            gen_bnd_jmp(s);\n            gen_jmp(s, tval);\n        }\n        break;\n    case 0x9a: /* lcall im */\n        {\n            unsigned int selector, offset;\n\n            if (CODE64(s))\n                goto illegal_op;\n            ot = dflag;\n            offset = insn_get(env, s, ot);\n            selector = insn_get(env, s, MO_16);\n\n            tcg_gen_movi_tl(cpu_T0, selector);\n            tcg_gen_movi_tl(cpu_T1, offset);\n        }\n        goto do_lcall;\n    case 0xe9: /* jmp im */\n        if (dflag != MO_16) {\n            tval = (int32_t)insn_get(env, s, MO_32);\n        } else {\n            tval = (int16_t)insn_get(env, s, MO_16);\n        }\n        tval += s->pc - s->cs_base;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        } else if (!CODE64(s)) {\n            tval &= 0xffffffff;\n        }\n        gen_bnd_jmp(s);\n        gen_jmp(s, tval);\n        break;\n    case 0xea: /* ljmp im */\n        {\n            unsigned int selector, offset;\n\n            if (CODE64(s))\n                goto illegal_op;\n            ot = dflag;\n            offset = insn_get(env, s, ot);\n            selector = insn_get(env, s, MO_16);\n\n            tcg_gen_movi_tl(cpu_T0, selector);\n            tcg_gen_movi_tl(cpu_T1, offset);\n        }\n        goto do_ljmp;\n    case 0xeb: /* jmp Jb */\n        tval = (int8_t)insn_get(env, s, MO_8);\n        tval += s->pc - s->cs_base;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        }\n        gen_jmp(s, tval);\n        break;\n    case 0x70 ... 0x7f: /* jcc Jb */\n        tval = (int8_t)insn_get(env, s, MO_8);\n        goto do_jcc;\n    case 0x180 ... 0x18f: /* jcc Jv */\n        if (dflag != MO_16) {\n            tval = (int32_t)insn_get(env, s, MO_32);\n        } else {\n            tval = (int16_t)insn_get(env, s, MO_16);\n        }\n    do_jcc:\n        next_eip = s->pc - s->cs_base;\n        tval += next_eip;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        }\n        gen_bnd_jmp(s);\n        gen_jcc(s, b, tval, next_eip);\n        break;\n\n    case 0x190 ... 0x19f: /* setcc Gv */\n        modrm = cpu_ldub_code(env, s->pc++);\n        gen_setcc1(s, b, cpu_T0);\n        gen_ldst_modrm(env, s, modrm, MO_8, OR_TMP0, 1);\n        break;\n    case 0x140 ... 0x14f: /* cmov Gv, Ev */\n        if (!(s->cpuid_features & CPUID_CMOV)) {\n            goto illegal_op;\n        }\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        gen_cmovcc1(env, s, ot, b, modrm, reg);\n        break;\n\n        /************************/\n        /* flags */\n    case 0x9c: /* pushf */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_PUSHF);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_helper_read_eflags(cpu_T0, cpu_env);\n            gen_push_v(s, cpu_T0);\n        }\n        break;\n    case 0x9d: /* popf */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_POPF);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            ot = gen_pop_T0(s);\n            if (s->cpl == 0) {\n                if (dflag != MO_16) {\n                    gen_helper_write_eflags(cpu_env, cpu_T0,\n                                            tcg_const_i32((TF_MASK | AC_MASK |\n                                                           ID_MASK | NT_MASK |\n                                                           IF_MASK |\n                                                           IOPL_MASK)));\n                } else {\n                    gen_helper_write_eflags(cpu_env, cpu_T0,\n                                            tcg_const_i32((TF_MASK | AC_MASK |\n                                                           ID_MASK | NT_MASK |\n                                                           IF_MASK | IOPL_MASK)\n                                                          & 0xffff));\n                }\n            } else {\n                if (s->cpl <= s->iopl) {\n                    if (dflag != MO_16) {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                                tcg_const_i32((TF_MASK |\n                                                               AC_MASK |\n                                                               ID_MASK |\n                                                               NT_MASK |\n                                                               IF_MASK)));\n                    } else {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                                tcg_const_i32((TF_MASK |\n                                                               AC_MASK |\n                                                               ID_MASK |\n                                                               NT_MASK |\n                                                               IF_MASK)\n                                                              & 0xffff));\n                    }\n                } else {\n                    if (dflag != MO_16) {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                           tcg_const_i32((TF_MASK | AC_MASK |\n                                                          ID_MASK | NT_MASK)));\n                    } else {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                           tcg_const_i32((TF_MASK | AC_MASK |\n                                                          ID_MASK | NT_MASK)\n                                                         & 0xffff));\n                    }\n                }\n            }\n            gen_pop_update(s, ot);\n            set_cc_op(s, CC_OP_EFLAGS);\n            /* abort translation because TF/AC flag may change */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n    case 0x9e: /* sahf */\n        if (CODE64(s) && !(s->cpuid_ext3_features & CPUID_EXT3_LAHF_LM))\n            goto illegal_op;\n        gen_op_mov_v_reg(MO_8, cpu_T0, R_AH);\n        gen_compute_eflags(s);\n        tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, CC_O);\n        tcg_gen_andi_tl(cpu_T0, cpu_T0, CC_S | CC_Z | CC_A | CC_P | CC_C);\n        tcg_gen_or_tl(cpu_cc_src, cpu_cc_src, cpu_T0);\n        break;\n    case 0x9f: /* lahf */\n        if (CODE64(s) && !(s->cpuid_ext3_features & CPUID_EXT3_LAHF_LM))\n            goto illegal_op;\n        gen_compute_eflags(s);\n        /* Note: gen_compute_eflags() only gives the condition codes */\n        tcg_gen_ori_tl(cpu_T0, cpu_cc_src, 0x02);\n        gen_op_mov_reg_v(MO_8, R_AH, cpu_T0);\n        break;\n    case 0xf5: /* cmc */\n        gen_compute_eflags(s);\n        tcg_gen_xori_tl(cpu_cc_src, cpu_cc_src, CC_C);\n        break;\n    case 0xf8: /* clc */\n        gen_compute_eflags(s);\n        tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, ~CC_C);\n        break;\n    case 0xf9: /* stc */\n        gen_compute_eflags(s);\n        tcg_gen_ori_tl(cpu_cc_src, cpu_cc_src, CC_C);\n        break;\n    case 0xfc: /* cld */\n        tcg_gen_movi_i32(cpu_tmp2_i32, 1);\n        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, df));\n        break;\n    case 0xfd: /* std */\n        tcg_gen_movi_i32(cpu_tmp2_i32, -1);\n        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, df));\n        break;\n\n        /************************/\n        /* bit operations */\n    case 0x1ba: /* bt/bts/btr/btc Gv, im */\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        op = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        if (mod != 3) {\n            s->rip_offset = 1;\n            gen_lea_modrm(env, s, modrm);\n            if (!(s->prefix & PREFIX_LOCK)) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n        /* load shift */\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T1, val);\n        if (op < 4)\n            goto unknown_op;\n        op -= 4;\n        goto bt_op;\n    case 0x1a3: /* bt Gv, Ev */\n        op = 0;\n        goto do_btx;\n    case 0x1ab: /* bts */\n        op = 1;\n        goto do_btx;\n    case 0x1b3: /* btr */\n        op = 2;\n        goto do_btx;\n    case 0x1bb: /* btc */\n        op = 3;\n    do_btx:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        gen_op_mov_v_reg(MO_32, cpu_T1, reg);\n        if (mod != 3) {\n            AddressParts a = gen_lea_modrm_0(env, s, modrm);\n            /* specific case: we need to add a displacement */\n            gen_exts(ot, cpu_T1);\n            tcg_gen_sari_tl(cpu_tmp0, cpu_T1, 3 + ot);\n            tcg_gen_shli_tl(cpu_tmp0, cpu_tmp0, ot);\n            tcg_gen_add_tl(cpu_A0, gen_lea_modrm_1(a), cpu_tmp0);\n            gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n            if (!(s->prefix & PREFIX_LOCK)) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n    bt_op:\n        tcg_gen_andi_tl(cpu_T1, cpu_T1, (1 << (3 + ot)) - 1);\n        tcg_gen_movi_tl(cpu_tmp0, 1);\n        tcg_gen_shl_tl(cpu_tmp0, cpu_tmp0, cpu_T1);\n        if (s->prefix & PREFIX_LOCK) {\n            switch (op) {\n            case 0: /* bt */\n                /* Needs no atomic ops; we surpressed the normal\n                   memory load for LOCK above so do it now.  */\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n                break;\n            case 1: /* bts */\n                tcg_gen_atomic_fetch_or_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                           s->mem_index, ot | MO_LE);\n                break;\n            case 2: /* btr */\n                tcg_gen_not_tl(cpu_tmp0, cpu_tmp0);\n                tcg_gen_atomic_fetch_and_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                            s->mem_index, ot | MO_LE);\n                break;\n            default:\n            case 3: /* btc */\n                tcg_gen_atomic_fetch_xor_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                            s->mem_index, ot | MO_LE);\n                break;\n            }\n            tcg_gen_shr_tl(cpu_tmp4, cpu_T0, cpu_T1);\n        } else {\n            tcg_gen_shr_tl(cpu_tmp4, cpu_T0, cpu_T1);\n            switch (op) {\n            case 0: /* bt */\n                /* Data already loaded; nothing to do.  */\n                break;\n            case 1: /* bts */\n                tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            case 2: /* btr */\n                tcg_gen_andc_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            default:\n            case 3: /* btc */\n                tcg_gen_xor_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            }\n            if (op != 0) {\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n        }\n\n        /* Delay all CC updates until after the store above.  Note that\n           C is the result of the test, Z is unchanged, and the others\n           are all undefined.  */\n        switch (s->cc_op) {\n        case CC_OP_MULB ... CC_OP_MULQ:\n        case CC_OP_ADDB ... CC_OP_ADDQ:\n        case CC_OP_ADCB ... CC_OP_ADCQ:\n        case CC_OP_SUBB ... CC_OP_SUBQ:\n        case CC_OP_SBBB ... CC_OP_SBBQ:\n        case CC_OP_LOGICB ... CC_OP_LOGICQ:\n        case CC_OP_INCB ... CC_OP_INCQ:\n        case CC_OP_DECB ... CC_OP_DECQ:\n        case CC_OP_SHLB ... CC_OP_SHLQ:\n        case CC_OP_SARB ... CC_OP_SARQ:\n        case CC_OP_BMILGB ... CC_OP_BMILGQ:\n            /* Z was going to be computed from the non-zero status of CC_DST.\n               We can get that same Z value (and the new C value) by leaving\n               CC_DST alone, setting CC_SRC, and using a CC_OP_SAR of the\n               same width.  */\n            tcg_gen_mov_tl(cpu_cc_src, cpu_tmp4);\n            set_cc_op(s, ((s->cc_op - CC_OP_MULB) & 3) + CC_OP_SARB);\n            break;\n        default:\n            /* Otherwise, generate EFLAGS and replace the C bit.  */\n            gen_compute_eflags(s);\n            tcg_gen_deposit_tl(cpu_cc_src, cpu_cc_src, cpu_tmp4,\n                               ctz32(CC_C), 1);\n            break;\n        }\n        break;\n    case 0x1bc: /* bsf / tzcnt */\n    case 0x1bd: /* bsr / lzcnt */\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_extu(ot, cpu_T0);\n\n        /* Note that lzcnt and tzcnt are in different extensions.  */\n        if ((prefixes & PREFIX_REPZ)\n            && (b & 1\n                ? s->cpuid_ext3_features & CPUID_EXT3_ABM\n                : s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)) {\n            int size = 8 << ot;\n            /* For lzcnt/tzcnt, C bit is defined related to the input. */\n            tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n            if (b & 1) {\n                /* For lzcnt, reduce the target_ulong result by the\n                   number of zeros that we expect to find at the top.  */\n                tcg_gen_clzi_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS);\n                tcg_gen_subi_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS - size);\n            } else {\n                /* For tzcnt, a zero input must return the operand size.  */\n                tcg_gen_ctzi_tl(cpu_T0, cpu_T0, size);\n            }\n            /* For lzcnt/tzcnt, Z bit is defined related to the result.  */\n            gen_op_update1_cc();\n            set_cc_op(s, CC_OP_BMILGB + ot);\n        } else {\n            /* For bsr/bsf, only the Z bit is defined and it is related\n               to the input and not the result.  */\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n            set_cc_op(s, CC_OP_LOGICB + ot);\n\n            /* ??? The manual says that the output is undefined when the\n               input is zero, but real hardware leaves it unchanged, and\n               real programs appear to depend on that.  Accomplish this\n               by passing the output as the value to return upon zero.  */\n            if (b & 1) {\n                /* For bsr, return the bit index of the first 1 bit,\n                   not the count of leading zeros.  */\n                tcg_gen_xori_tl(cpu_T1, cpu_regs[reg], TARGET_LONG_BITS - 1);\n                tcg_gen_clz_tl(cpu_T0, cpu_T0, cpu_T1);\n                tcg_gen_xori_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS - 1);\n            } else {\n                tcg_gen_ctz_tl(cpu_T0, cpu_T0, cpu_regs[reg]);\n            }\n        }\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n        break;\n        /************************/\n        /* bcd */\n    case 0x27: /* daa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_daa(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x2f: /* das */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_das(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x37: /* aaa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_aaa(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x3f: /* aas */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_aas(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0xd4: /* aam */\n        if (CODE64(s))\n            goto illegal_op;\n        val = cpu_ldub_code(env, s->pc++);\n        if (val == 0) {\n            gen_exception(s, EXCP00_DIVZ, pc_start - s->cs_base);\n        } else {\n            gen_helper_aam(cpu_env, tcg_const_i32(val));\n            set_cc_op(s, CC_OP_LOGICB);\n        }\n        break;\n    case 0xd5: /* aad */\n        if (CODE64(s))\n            goto illegal_op;\n        val = cpu_ldub_code(env, s->pc++);\n        gen_helper_aad(cpu_env, tcg_const_i32(val));\n        set_cc_op(s, CC_OP_LOGICB);\n        break;\n        /************************/\n        /* misc */\n    case 0x90: /* nop */\n        /* XXX: correct lock test for all insn */\n        if (prefixes & PREFIX_LOCK) {\n            goto illegal_op;\n        }\n        /* If REX_B is set, then this is xchg eax, r8d, not a nop.  */\n        if (REX_B(s)) {\n            goto do_xchg_reg_eax;\n        }\n        if (prefixes & PREFIX_REPZ) {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_pause(cpu_env, tcg_const_i32(s->pc - pc_start));\n            s->is_jmp = DISAS_TB_JUMP;\n        }\n        break;\n    case 0x9b: /* fwait */\n        if ((s->flags & (HF_MP_MASK | HF_TS_MASK)) ==\n            (HF_MP_MASK | HF_TS_MASK)) {\n            gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n        } else {\n            gen_helper_fwait(cpu_env);\n        }\n        break;\n    case 0xcc: /* int3 */\n        gen_interrupt(s, EXCP03_INT3, pc_start - s->cs_base, s->pc - s->cs_base);\n        break;\n    case 0xcd: /* int N */\n        val = cpu_ldub_code(env, s->pc++);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_interrupt(s, val, pc_start - s->cs_base, s->pc - s->cs_base);\n        }\n        break;\n    case 0xce: /* into */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_into(cpu_env, tcg_const_i32(s->pc - pc_start));\n        break;\n#ifdef WANT_ICEBP\n    case 0xf1: /* icebp (undocumented, exits to external debugger) */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_ICEBP);\n#if 1\n        gen_debug(s, pc_start - s->cs_base);\n#else\n        /* start debug */\n        tb_flush(CPU(x86_env_get_cpu(env)));\n        qemu_set_log(CPU_LOG_INT | CPU_LOG_TB_IN_ASM);\n#endif\n        break;\n#endif\n    case 0xfa: /* cli */\n        if (!s->vm86) {\n            if (s->cpl <= s->iopl) {\n                gen_helper_cli(cpu_env);\n            } else {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            }\n        } else {\n            if (s->iopl == 3) {\n                gen_helper_cli(cpu_env);\n            } else {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            }\n        }\n        break;\n    case 0xfb: /* sti */\n        if (s->vm86 ? s->iopl == 3 : s->cpl <= s->iopl) {\n            gen_helper_sti(cpu_env);\n            /* interruptions are enabled only the first insn after sti */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob_inhibit_irq(s, true);\n        } else {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        }\n        break;\n    case 0x62: /* bound */\n        if (CODE64(s))\n            goto illegal_op;\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_op_mov_v_reg(ot, cpu_T0, reg);\n        gen_lea_modrm(env, s, modrm);\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        if (ot == MO_16) {\n            gen_helper_boundw(cpu_env, cpu_A0, cpu_tmp2_i32);\n        } else {\n            gen_helper_boundl(cpu_env, cpu_A0, cpu_tmp2_i32);\n        }\n        break;\n    case 0x1c8 ... 0x1cf: /* bswap reg */\n        reg = (b & 7) | REX_B(s);\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            gen_op_mov_v_reg(MO_64, cpu_T0, reg);\n            tcg_gen_bswap64_i64(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_64, reg, cpu_T0);\n        } else\n#endif\n        {\n            gen_op_mov_v_reg(MO_32, cpu_T0, reg);\n            tcg_gen_ext32u_tl(cpu_T0, cpu_T0);\n            tcg_gen_bswap32_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_32, reg, cpu_T0);\n        }\n        break;\n    case 0xd6: /* salc */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_compute_eflags_c(s, cpu_T0);\n        tcg_gen_neg_tl(cpu_T0, cpu_T0);\n        gen_op_mov_reg_v(MO_8, R_EAX, cpu_T0);\n        break;\n    case 0xe0: /* loopnz */\n    case 0xe1: /* loopz */\n    case 0xe2: /* loop */\n    case 0xe3: /* jecxz */\n        {\n            TCGLabel *l1, *l2, *l3;\n\n            tval = (int8_t)insn_get(env, s, MO_8);\n            next_eip = s->pc - s->cs_base;\n            tval += next_eip;\n            if (dflag == MO_16) {\n                tval &= 0xffff;\n            }\n\n            l1 = gen_new_label();\n            l2 = gen_new_label();\n            l3 = gen_new_label();\n            b &= 3;\n            switch(b) {\n            case 0: /* loopnz */\n            case 1: /* loopz */\n                gen_op_add_reg_im(s->aflag, R_ECX, -1);\n                gen_op_jz_ecx(s->aflag, l3);\n                gen_jcc1(s, (JCC_Z << 1) | (b ^ 1), l1);\n                break;\n            case 2: /* loop */\n                gen_op_add_reg_im(s->aflag, R_ECX, -1);\n                gen_op_jnz_ecx(s->aflag, l1);\n                break;\n            default:\n            case 3: /* jcxz */\n                gen_op_jz_ecx(s->aflag, l1);\n                break;\n            }\n\n            gen_set_label(l3);\n            gen_jmp_im(next_eip);\n            tcg_gen_br(l2);\n\n            gen_set_label(l1);\n            gen_jmp_im(tval);\n            gen_set_label(l2);\n            gen_eob(s);\n        }\n        break;\n    case 0x130: /* wrmsr */\n    case 0x132: /* rdmsr */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            if (b & 2) {\n                gen_helper_rdmsr(cpu_env);\n            } else {\n                gen_helper_wrmsr(cpu_env);\n            }\n        }\n        break;\n    case 0x131: /* rdtsc */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        gen_helper_rdtsc(cpu_env);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0x133: /* rdpmc */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_rdpmc(cpu_env);\n        break;\n    case 0x134: /* sysenter */\n        /* For Intel SYSENTER is valid on 64-bit */\n        if (CODE64(s) && env->cpuid_vendor1 != CPUID_VENDOR_INTEL_1)\n            goto illegal_op;\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysenter(cpu_env);\n            gen_eob(s);\n        }\n        break;\n    case 0x135: /* sysexit */\n        /* For Intel SYSEXIT is valid on 64-bit */\n        if (CODE64(s) && env->cpuid_vendor1 != CPUID_VENDOR_INTEL_1)\n            goto illegal_op;\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysexit(cpu_env, tcg_const_i32(dflag - 1));\n            gen_eob(s);\n        }\n        break;\n#ifdef TARGET_X86_64\n    case 0x105: /* syscall */\n        /* XXX: is it usable in real mode ? */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_syscall(cpu_env, tcg_const_i32(s->pc - pc_start));\n        /* TF handling for the syscall insn is different. The TF bit is  checked\n           after the syscall insn completes. This allows #DB to not be\n           generated after one has entered CPL0 if TF is set in FMASK.  */\n        gen_eob_worker(s, false, true);\n        break;\n    case 0x107: /* sysret */\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysret(cpu_env, tcg_const_i32(dflag - 1));\n            /* condition codes are modified only in long mode */\n            if (s->lma) {\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n            /* TF handling for the sysret insn is different. The TF bit is\n               checked after the sysret insn completes. This allows #DB to be\n               generated \"as if\" the syscall insn in userspace has just\n               completed.  */\n            gen_eob_worker(s, false, true);\n        }\n        break;\n#endif\n    case 0x1a2: /* cpuid */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_cpuid(cpu_env);\n        break;\n    case 0xf4: /* hlt */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_hlt(cpu_env, tcg_const_i32(s->pc - pc_start));\n            s->is_jmp = DISAS_TB_JUMP;\n        }\n        break;\n    case 0x100:\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        op = (modrm >> 3) & 7;\n        switch(op) {\n        case 0: /* sldt */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_LDTR_READ);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                             offsetof(CPUX86State, ldt.selector));\n            ot = mod == 3 ? dflag : MO_16;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 2: /* lldt */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_LDTR_WRITE);\n                gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lldt(cpu_env, cpu_tmp2_i32);\n            }\n            break;\n        case 1: /* str */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_TR_READ);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                             offsetof(CPUX86State, tr.selector));\n            ot = mod == 3 ? dflag : MO_16;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 3: /* ltr */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_TR_WRITE);\n                gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_ltr(cpu_env, cpu_tmp2_i32);\n            }\n            break;\n        case 4: /* verr */\n        case 5: /* verw */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            gen_update_cc_op(s);\n            if (op == 4) {\n                gen_helper_verr(cpu_env, cpu_T0);\n            } else {\n                gen_helper_verw(cpu_env, cpu_T0);\n            }\n            set_cc_op(s, CC_OP_EFLAGS);\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x101:\n        modrm = cpu_ldub_code(env, s->pc++);\n        switch (modrm) {\n        CASE_MODRM_MEM_OP(0): /* sgdt */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_GDTR_READ);\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0,\n                             cpu_env, offsetof(CPUX86State, gdt.limit));\n            gen_op_st_v(s, MO_16, cpu_T0, cpu_A0);\n            gen_add_A0_im(s, 2);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, gdt.base));\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            gen_op_st_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            break;\n\n        case 0xc8: /* monitor */\n            if (!(s->cpuid_ext_features & CPUID_EXT_MONITOR) || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EAX]);\n            gen_extu(s->aflag, cpu_A0);\n            gen_add_A0_ds_seg(s);\n            gen_helper_monitor(cpu_env, cpu_A0);\n            break;\n\n        case 0xc9: /* mwait */\n            if (!(s->cpuid_ext_features & CPUID_EXT_MONITOR) || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_mwait(cpu_env, tcg_const_i32(s->pc - pc_start));\n            gen_eob(s);\n            break;\n\n        case 0xca: /* clac */\n            if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_SMAP)\n                || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_helper_clac(cpu_env);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xcb: /* stac */\n            if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_SMAP)\n                || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_helper_stac(cpu_env);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(1): /* sidt */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_IDTR_READ);\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.limit));\n            gen_op_st_v(s, MO_16, cpu_T0, cpu_A0);\n            gen_add_A0_im(s, 2);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.base));\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            gen_op_st_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            break;\n\n        case 0xd0: /* xgetbv */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (s->prefix & (PREFIX_LOCK | PREFIX_DATA\n                                 | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_xgetbv(cpu_tmp1_i64, cpu_env, cpu_tmp2_i32);\n            tcg_gen_extr_i64_tl(cpu_regs[R_EAX], cpu_regs[R_EDX], cpu_tmp1_i64);\n            break;\n\n        case 0xd1: /* xsetbv */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (s->prefix & (PREFIX_LOCK | PREFIX_DATA\n                                 | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_xsetbv(cpu_env, cpu_tmp2_i32, cpu_tmp1_i64);\n            /* End TB because translation flags may change.  */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xd8: /* VMRUN */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmrun(cpu_env, tcg_const_i32(s->aflag - 1),\n                             tcg_const_i32(s->pc - pc_start));\n            tcg_gen_exit_tb(0);\n            s->is_jmp = DISAS_TB_JUMP;\n            break;\n\n        case 0xd9: /* VMMCALL */\n            if (!(s->flags & HF_SVME_MASK)) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmmcall(cpu_env);\n            break;\n\n        case 0xda: /* VMLOAD */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmload(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        case 0xdb: /* VMSAVE */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmsave(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        case 0xdc: /* STGI */\n            if ((!(s->flags & HF_SVME_MASK)\n                   && !(s->cpuid_ext3_features & CPUID_EXT3_SKINIT))\n                || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_stgi(cpu_env);\n            break;\n\n        case 0xdd: /* CLGI */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_clgi(cpu_env);\n            break;\n\n        case 0xde: /* SKINIT */\n            if ((!(s->flags & HF_SVME_MASK)\n                 && !(s->cpuid_ext3_features & CPUID_EXT3_SKINIT))\n                || !s->pe) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_skinit(cpu_env);\n            break;\n\n        case 0xdf: /* INVLPGA */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_invlpga(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        CASE_MODRM_MEM_OP(2): /* lgdt */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_GDTR_WRITE);\n            gen_lea_modrm(env, s, modrm);\n            gen_op_ld_v(s, MO_16, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 2);\n            gen_op_ld_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State, gdt.base));\n            tcg_gen_st32_tl(cpu_T1, cpu_env, offsetof(CPUX86State, gdt.limit));\n            break;\n\n        CASE_MODRM_MEM_OP(3): /* lidt */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_IDTR_WRITE);\n            gen_lea_modrm(env, s, modrm);\n            gen_op_ld_v(s, MO_16, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 2);\n            gen_op_ld_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.base));\n            tcg_gen_st32_tl(cpu_T1, cpu_env, offsetof(CPUX86State, idt.limit));\n            break;\n\n        CASE_MODRM_OP(4): /* smsw */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_READ_CR0);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, cr[0]));\n            if (CODE64(s)) {\n                mod = (modrm >> 6) & 3;\n                ot = (mod != 3 ? MO_16 : s->dflag);\n            } else {\n                ot = MO_16;\n            }\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 0xee: /* rdpkru */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_rdpkru(cpu_tmp1_i64, cpu_env, cpu_tmp2_i32);\n            tcg_gen_extr_i64_tl(cpu_regs[R_EAX], cpu_regs[R_EDX], cpu_tmp1_i64);\n            break;\n        case 0xef: /* wrpkru */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_wrpkru(cpu_env, cpu_tmp2_i32, cpu_tmp1_i64);\n            break;\n        CASE_MODRM_OP(6): /* lmsw */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_CR0);\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            gen_helper_lmsw(cpu_env, cpu_T0);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(7): /* invlpg */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_invlpg(cpu_env, cpu_A0);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xf8: /* swapgs */\n#ifdef TARGET_X86_64\n            if (CODE64(s)) {\n                if (s->cpl != 0) {\n                    gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                } else {\n                    tcg_gen_mov_tl(cpu_T0, cpu_seg_base[R_GS]);\n                    tcg_gen_ld_tl(cpu_seg_base[R_GS], cpu_env,\n                                  offsetof(CPUX86State, kernelgsbase));\n                    tcg_gen_st_tl(cpu_T0, cpu_env,\n                                  offsetof(CPUX86State, kernelgsbase));\n                }\n                break;\n            }\n#endif\n            goto illegal_op;\n\n        case 0xf9: /* rdtscp */\n            if (!(s->cpuid_ext2_features & CPUID_EXT2_RDTSCP)) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_io_start();\n            }\n            gen_helper_rdtscp(cpu_env);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_io_end();\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n            break;\n\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x108: /* invd */\n    case 0x109: /* wbinvd */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_svm_check_intercept(s, pc_start, (b & 2) ? SVM_EXIT_INVD : SVM_EXIT_WBINVD);\n            /* nothing to do */\n        }\n        break;\n    case 0x63: /* arpl or movslS (x86_64) */\n#ifdef TARGET_X86_64\n        if (CODE64(s)) {\n            int d_ot;\n            /* d_ot is the size of destination */\n            d_ot = dflag;\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n\n            if (mod == 3) {\n                gen_op_mov_v_reg(MO_32, cpu_T0, rm);\n                /* sign extend */\n                if (d_ot == MO_64) {\n                    tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n                }\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            } else {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, MO_32 | MO_SIGN, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            }\n        } else\n#endif\n        {\n            TCGLabel *label1;\n            TCGv t0, t1, t2, a0;\n\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            t0 = tcg_temp_local_new();\n            t1 = tcg_temp_local_new();\n            t2 = tcg_temp_local_new();\n            ot = MO_16;\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = (modrm >> 3) & 7;\n            mod = (modrm >> 6) & 3;\n            rm = modrm & 7;\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, ot, t0, cpu_A0);\n                a0 = tcg_temp_local_new();\n                tcg_gen_mov_tl(a0, cpu_A0);\n            } else {\n                gen_op_mov_v_reg(ot, t0, rm);\n                TCGV_UNUSED(a0);\n            }\n            gen_op_mov_v_reg(ot, t1, reg);\n            tcg_gen_andi_tl(cpu_tmp0, t0, 3);\n            tcg_gen_andi_tl(t1, t1, 3);\n            tcg_gen_movi_tl(t2, 0);\n            label1 = gen_new_label();\n            tcg_gen_brcond_tl(TCG_COND_GE, cpu_tmp0, t1, label1);\n            tcg_gen_andi_tl(t0, t0, ~3);\n            tcg_gen_or_tl(t0, t0, t1);\n            tcg_gen_movi_tl(t2, CC_Z);\n            gen_set_label(label1);\n            if (mod != 3) {\n                gen_op_st_v(s, ot, t0, a0);\n                tcg_temp_free(a0);\n           } else {\n                gen_op_mov_reg_v(ot, rm, t0);\n            }\n            gen_compute_eflags(s);\n            tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, ~CC_Z);\n            tcg_gen_or_tl(cpu_cc_src, cpu_cc_src, t2);\n            tcg_temp_free(t0);\n            tcg_temp_free(t1);\n            tcg_temp_free(t2);\n        }\n        break;\n    case 0x102: /* lar */\n    case 0x103: /* lsl */\n        {\n            TCGLabel *label1;\n            TCGv t0;\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            ot = dflag != MO_16 ? MO_32 : MO_16;\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            t0 = tcg_temp_local_new();\n            gen_update_cc_op(s);\n            if (b == 0x102) {\n                gen_helper_lar(t0, cpu_env, cpu_T0);\n            } else {\n                gen_helper_lsl(t0, cpu_env, cpu_T0);\n            }\n            tcg_gen_andi_tl(cpu_tmp0, cpu_cc_src, CC_Z);\n            label1 = gen_new_label();\n            tcg_gen_brcondi_tl(TCG_COND_EQ, cpu_tmp0, 0, label1);\n            gen_op_mov_reg_v(ot, reg, t0);\n            gen_set_label(label1);\n            set_cc_op(s, CC_OP_EFLAGS);\n            tcg_temp_free(t0);\n        }\n        break;\n    case 0x118:\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        op = (modrm >> 3) & 7;\n        switch(op) {\n        case 0: /* prefetchnta */\n        case 1: /* prefetchnt0 */\n        case 2: /* prefetchnt0 */\n        case 3: /* prefetchnt0 */\n            if (mod == 3)\n                goto illegal_op;\n            gen_nop_modrm(env, s, modrm);\n            /* nothing more to do */\n            break;\n        default: /* nop (multi byte) */\n            gen_nop_modrm(env, s, modrm);\n            break;\n        }\n        break;\n    case 0x11a:\n        modrm = cpu_ldub_code(env, s->pc++);\n        if (s->flags & HF_MPX_EN_MASK) {\n            mod = (modrm >> 6) & 3;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (prefixes & PREFIX_REPZ) {\n                /* bndcl */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                gen_bndck(env, s, modrm, TCG_COND_LTU, cpu_bndl[reg]);\n            } else if (prefixes & PREFIX_REPNZ) {\n                /* bndcu */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                TCGv_i64 notu = tcg_temp_new_i64();\n                tcg_gen_not_i64(notu, cpu_bndu[reg]);\n                gen_bndck(env, s, modrm, TCG_COND_GTU, notu);\n                tcg_temp_free_i64(notu);\n            } else if (prefixes & PREFIX_DATA) {\n                /* bndmov -- from reg/mem */\n                if (reg >= 4 || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                if (mod == 3) {\n                    int reg2 = (modrm & 7) | REX_B(s);\n                    if (reg2 >= 4 || (prefixes & PREFIX_LOCK)) {\n                        goto illegal_op;\n                    }\n                    if (s->flags & HF_MPX_IU_MASK) {\n                        tcg_gen_mov_i64(cpu_bndl[reg], cpu_bndl[reg2]);\n                        tcg_gen_mov_i64(cpu_bndu[reg], cpu_bndu[reg2]);\n                    }\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    if (CODE64(s)) {\n                        tcg_gen_qemu_ld_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 8);\n                        tcg_gen_qemu_ld_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                    } else {\n                        tcg_gen_qemu_ld_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 4);\n                        tcg_gen_qemu_ld_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                    /* bnd registers are now in-use */\n                    gen_set_hflag(s, HF_MPX_IU_MASK);\n                }\n            } else if (mod != 3) {\n                /* bndldx */\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16\n                    || a.base < -1) {\n                    goto illegal_op;\n                }\n                if (a.base >= 0) {\n                    tcg_gen_addi_tl(cpu_A0, cpu_regs[a.base], a.disp);\n                } else {\n                    tcg_gen_movi_tl(cpu_A0, 0);\n                }\n                gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n                if (a.index >= 0) {\n                    tcg_gen_mov_tl(cpu_T0, cpu_regs[a.index]);\n                } else {\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                }\n                if (CODE64(s)) {\n                    gen_helper_bndldx64(cpu_bndl[reg], cpu_env, cpu_A0, cpu_T0);\n                    tcg_gen_ld_i64(cpu_bndu[reg], cpu_env,\n                                   offsetof(CPUX86State, mmx_t0.MMX_Q(0)));\n                } else {\n                    gen_helper_bndldx32(cpu_bndu[reg], cpu_env, cpu_A0, cpu_T0);\n                    tcg_gen_ext32u_i64(cpu_bndl[reg], cpu_bndu[reg]);\n                    tcg_gen_shri_i64(cpu_bndu[reg], cpu_bndu[reg], 32);\n                }\n                gen_set_hflag(s, HF_MPX_IU_MASK);\n            }\n        }\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x11b:\n        modrm = cpu_ldub_code(env, s->pc++);\n        if (s->flags & HF_MPX_EN_MASK) {\n            mod = (modrm >> 6) & 3;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (mod != 3 && (prefixes & PREFIX_REPZ)) {\n                /* bndmk */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (a.base >= 0) {\n                    tcg_gen_extu_tl_i64(cpu_bndl[reg], cpu_regs[a.base]);\n                    if (!CODE64(s)) {\n                        tcg_gen_ext32u_i64(cpu_bndl[reg], cpu_bndl[reg]);\n                    }\n                } else if (a.base == -1) {\n                    /* no base register has lower bound of 0 */\n                    tcg_gen_movi_i64(cpu_bndl[reg], 0);\n                } else {\n                    /* rip-relative generates #ud */\n                    goto illegal_op;\n                }\n                tcg_gen_not_tl(cpu_A0, gen_lea_modrm_1(a));\n                if (!CODE64(s)) {\n                    tcg_gen_ext32u_tl(cpu_A0, cpu_A0);\n                }\n                tcg_gen_extu_tl_i64(cpu_bndu[reg], cpu_A0);\n                /* bnd registers are now in-use */\n                gen_set_hflag(s, HF_MPX_IU_MASK);\n                break;\n            } else if (prefixes & PREFIX_REPNZ) {\n                /* bndcn */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                gen_bndck(env, s, modrm, TCG_COND_GTU, cpu_bndu[reg]);\n            } else if (prefixes & PREFIX_DATA) {\n                /* bndmov -- to reg/mem */\n                if (reg >= 4 || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                if (mod == 3) {\n                    int reg2 = (modrm & 7) | REX_B(s);\n                    if (reg2 >= 4 || (prefixes & PREFIX_LOCK)) {\n                        goto illegal_op;\n                    }\n                    if (s->flags & HF_MPX_IU_MASK) {\n                        tcg_gen_mov_i64(cpu_bndl[reg2], cpu_bndl[reg]);\n                        tcg_gen_mov_i64(cpu_bndu[reg2], cpu_bndu[reg]);\n                    }\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    if (CODE64(s)) {\n                        tcg_gen_qemu_st_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 8);\n                        tcg_gen_qemu_st_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                    } else {\n                        tcg_gen_qemu_st_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 4);\n                        tcg_gen_qemu_st_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                }\n            } else if (mod != 3) {\n                /* bndstx */\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16\n                    || a.base < -1) {\n                    goto illegal_op;\n                }\n                if (a.base >= 0) {\n                    tcg_gen_addi_tl(cpu_A0, cpu_regs[a.base], a.disp);\n                } else {\n                    tcg_gen_movi_tl(cpu_A0, 0);\n                }\n                gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n                if (a.index >= 0) {\n                    tcg_gen_mov_tl(cpu_T0, cpu_regs[a.index]);\n                } else {\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                }\n                if (CODE64(s)) {\n                    gen_helper_bndstx64(cpu_env, cpu_A0, cpu_T0,\n                                        cpu_bndl[reg], cpu_bndu[reg]);\n                } else {\n                    gen_helper_bndstx32(cpu_env, cpu_A0, cpu_T0,\n                                        cpu_bndl[reg], cpu_bndu[reg]);\n                }\n            }\n        }\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x119: case 0x11c ... 0x11f: /* nop (multi byte) */\n        modrm = cpu_ldub_code(env, s->pc++);\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x120: /* mov reg, crN */\n    case 0x122: /* mov crN, reg */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            modrm = cpu_ldub_code(env, s->pc++);\n            /* Ignore the mod bits (assume (modrm&0xc0)==0xc0).\n             * AMD documentation (24594.pdf) and testing of\n             * intel 386 and 486 processors all show that the mod bits\n             * are assumed to be 1's, regardless of actual values.\n             */\n            rm = (modrm & 7) | REX_B(s);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (CODE64(s))\n                ot = MO_64;\n            else\n                ot = MO_32;\n            if ((prefixes & PREFIX_LOCK) && (reg == 0) &&\n                (s->cpuid_ext3_features & CPUID_EXT3_CR8LEG)) {\n                reg = 8;\n            }\n            switch(reg) {\n            case 0:\n            case 2:\n            case 3:\n            case 4:\n            case 8:\n                gen_update_cc_op(s);\n                gen_jmp_im(pc_start - s->cs_base);\n                if (b & 2) {\n                    gen_op_mov_v_reg(ot, cpu_T0, rm);\n                    gen_helper_write_crN(cpu_env, tcg_const_i32(reg),\n                                         cpu_T0);\n                    gen_jmp_im(s->pc - s->cs_base);\n                    gen_eob(s);\n                } else {\n                    gen_helper_read_crN(cpu_T0, cpu_env, tcg_const_i32(reg));\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n                break;\n            default:\n                goto unknown_op;\n            }\n        }\n        break;\n    case 0x121: /* mov reg, drN */\n    case 0x123: /* mov drN, reg */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            modrm = cpu_ldub_code(env, s->pc++);\n            /* Ignore the mod bits (assume (modrm&0xc0)==0xc0).\n             * AMD documentation (24594.pdf) and testing of\n             * intel 386 and 486 processors all show that the mod bits\n             * are assumed to be 1's, regardless of actual values.\n             */\n            rm = (modrm & 7) | REX_B(s);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (CODE64(s))\n                ot = MO_64;\n            else\n                ot = MO_32;\n            if (reg >= 8) {\n                goto illegal_op;\n            }\n            if (b & 2) {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_DR0 + reg);\n                gen_op_mov_v_reg(ot, cpu_T0, rm);\n                tcg_gen_movi_i32(cpu_tmp2_i32, reg);\n                gen_helper_set_dr(cpu_env, cpu_tmp2_i32, cpu_T0);\n                gen_jmp_im(s->pc - s->cs_base);\n                gen_eob(s);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_READ_DR0 + reg);\n                tcg_gen_movi_i32(cpu_tmp2_i32, reg);\n                gen_helper_get_dr(cpu_T0, cpu_env, cpu_tmp2_i32);\n                gen_op_mov_reg_v(ot, rm, cpu_T0);\n            }\n        }\n        break;\n    case 0x106: /* clts */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_CR0);\n            gen_helper_clts(cpu_env);\n            /* abort block because static cpu state changed */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n    /* MMX/3DNow!/SSE/SSE2/SSE3/SSSE3/SSE4 support */\n    case 0x1c3: /* MOVNTI reg, mem */\n        if (!(s->cpuid_features & CPUID_SSE2))\n            goto illegal_op;\n        ot = mo_64_32(dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        reg = ((modrm >> 3) & 7) | rex_r;\n        /* generate a generic store */\n        gen_ldst_modrm(env, s, modrm, ot, reg, 1);\n        break;\n    case 0x1ae:\n        modrm = cpu_ldub_code(env, s->pc++);\n        switch (modrm) {\n        CASE_MODRM_MEM_OP(0): /* fxsave */\n            if (!(s->cpuid_features & CPUID_FXSR)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            if ((s->flags & HF_EM_MASK) || (s->flags & HF_TS_MASK)) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_fxsave(cpu_env, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(1): /* fxrstor */\n            if (!(s->cpuid_features & CPUID_FXSR)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            if ((s->flags & HF_EM_MASK) || (s->flags & HF_TS_MASK)) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_fxrstor(cpu_env, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(2): /* ldmxcsr */\n            if ((s->flags & HF_EM_MASK) || !(s->flags & HF_OSFXSR_MASK)) {\n                goto illegal_op;\n            }\n            if (s->flags & HF_TS_MASK) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0, s->mem_index, MO_LEUL);\n            gen_helper_ldmxcsr(cpu_env, cpu_tmp2_i32);\n            break;\n\n        CASE_MODRM_MEM_OP(3): /* stmxcsr */\n            if ((s->flags & HF_EM_MASK) || !(s->flags & HF_OSFXSR_MASK)) {\n                goto illegal_op;\n            }\n            if (s->flags & HF_TS_MASK) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State, mxcsr));\n            gen_op_st_v(s, MO_32, cpu_T0, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(4): /* xsave */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (prefixes & (PREFIX_LOCK | PREFIX_DATA\n                                | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            gen_helper_xsave(cpu_env, cpu_A0, cpu_tmp1_i64);\n            break;\n\n        CASE_MODRM_MEM_OP(5): /* xrstor */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (prefixes & (PREFIX_LOCK | PREFIX_DATA\n                                | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            gen_helper_xrstor(cpu_env, cpu_A0, cpu_tmp1_i64);\n            /* XRSTOR is how MPX is enabled, which changes how\n               we translate.  Thus we need to end the TB.  */\n            gen_update_cc_op(s);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(6): /* xsaveopt / clwb */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            if (prefixes & PREFIX_DATA) {\n                /* clwb */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_CLWB)) {\n                    goto illegal_op;\n                }\n                gen_nop_modrm(env, s, modrm);\n            } else {\n                /* xsaveopt */\n                if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                    || (s->cpuid_xsave_features & CPUID_XSAVE_XSAVEOPT) == 0\n                    || (prefixes & (PREFIX_REPZ | PREFIX_REPNZ))) {\n                    goto illegal_op;\n                }\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                      cpu_regs[R_EDX]);\n                gen_helper_xsaveopt(cpu_env, cpu_A0, cpu_tmp1_i64);\n            }\n            break;\n\n        CASE_MODRM_MEM_OP(7): /* clflush / clflushopt */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            if (prefixes & PREFIX_DATA) {\n                /* clflushopt */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_CLFLUSHOPT)) {\n                    goto illegal_op;\n                }\n            } else {\n                /* clflush */\n                if ((s->prefix & (PREFIX_REPZ | PREFIX_REPNZ))\n                    || !(s->cpuid_features & CPUID_CLFLUSH)) {\n                    goto illegal_op;\n                }\n            }\n            gen_nop_modrm(env, s, modrm);\n            break;\n\n        case 0xc0 ... 0xc7: /* rdfsbase (f3 0f ae /0) */\n        case 0xc8 ... 0xc8: /* rdgsbase (f3 0f ae /1) */\n        case 0xd0 ... 0xd7: /* wrfsbase (f3 0f ae /2) */\n        case 0xd8 ... 0xd8: /* wrgsbase (f3 0f ae /3) */\n            if (CODE64(s)\n                && (prefixes & PREFIX_REPZ)\n                && !(prefixes & PREFIX_LOCK)\n                && (s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_FSGSBASE)) {\n                TCGv base, treg, src, dst;\n\n                /* Preserve hflags bits by testing CR4 at runtime.  */\n                tcg_gen_movi_i32(cpu_tmp2_i32, CR4_FSGSBASE_MASK);\n                gen_helper_cr4_testbit(cpu_env, cpu_tmp2_i32);\n\n                base = cpu_seg_base[modrm & 8 ? R_GS : R_FS];\n                treg = cpu_regs[(modrm & 7) | REX_B(s)];\n\n                if (modrm & 0x10) {\n                    /* wr*base */\n                    dst = base, src = treg;\n                } else {\n                    /* rd*base */\n                    dst = treg, src = base;\n                }\n\n                if (s->dflag == MO_32) {\n                    tcg_gen_ext32u_tl(dst, src);\n                } else {\n                    tcg_gen_mov_tl(dst, src);\n                }\n                break;\n            }\n            goto unknown_op;\n\n        case 0xf8: /* sfence / pcommit */\n            if (prefixes & PREFIX_DATA) {\n                /* pcommit */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_PCOMMIT)\n                    || (prefixes & PREFIX_LOCK)) {\n                    goto illegal_op;\n                }\n                break;\n            }\n            /* fallthru */\n        case 0xf9 ... 0xff: /* sfence */\n            if (!(s->cpuid_features & CPUID_SSE)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_ST_ST | TCG_BAR_SC);\n            break;\n        case 0xe8 ... 0xef: /* lfence */\n            if (!(s->cpuid_features & CPUID_SSE)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_LD_LD | TCG_BAR_SC);\n            break;\n        case 0xf0 ... 0xf7: /* mfence */\n            if (!(s->cpuid_features & CPUID_SSE2)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_ALL | TCG_BAR_SC);\n            break;\n\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x10d: /* 3DNow! prefetch(w) */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x1aa: /* rsm */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_RSM);\n        if (!(s->flags & HF_SMM_MASK))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_jmp_im(s->pc - s->cs_base);\n        gen_helper_rsm(cpu_env);\n        gen_eob(s);\n        break;\n    case 0x1b8: /* SSE4.2 popcnt */\n        if ((prefixes & (PREFIX_REPZ | PREFIX_LOCK | PREFIX_REPNZ)) !=\n             PREFIX_REPZ)\n            goto illegal_op;\n        if (!(s->cpuid_ext_features & CPUID_EXT_POPCNT))\n            goto illegal_op;\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        if (s->prefix & PREFIX_DATA) {\n            ot = MO_16;\n        } else {\n            ot = mo_64_32(dflag);\n        }\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_extu(ot, cpu_T0);\n        tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n        tcg_gen_ctpop_tl(cpu_T0, cpu_T0);\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n\n        set_cc_op(s, CC_OP_POPCNT);\n        break;\n    case 0x10e ... 0x10f:\n        /* 3DNow! instructions, ignore prefixes */\n        s->prefix &= ~(PREFIX_REPZ | PREFIX_REPNZ | PREFIX_DATA);\n    case 0x110 ... 0x117:\n    case 0x128 ... 0x12f:\n    case 0x138 ... 0x13a:\n    case 0x150 ... 0x179:\n    case 0x17c ... 0x17f:\n    case 0x1c2:\n    case 0x1c4 ... 0x1c6:\n    case 0x1d0 ... 0x1fe:\n        gen_sse(env, s, b, pc_start, rex_r);\n        break;\n    default:\n        goto unknown_op;\n    }\n    return s->pc;\n illegal_op:\n    gen_illegal_opcode(s);\n    return s->pc;\n unknown_op:\n    gen_unknown_opcode(env, s);\n    return s->pc;\n}\n\nvoid tcg_x86_init(void)\n{\n    static const char reg_names[CPU_NB_REGS][4] = {\n#ifdef TARGET_X86_64\n        [R_EAX] = \"rax\",\n        [R_EBX] = \"rbx\",\n        [R_ECX] = \"rcx\",\n        [R_EDX] = \"rdx\",\n        [R_ESI] = \"rsi\",\n        [R_EDI] = \"rdi\",\n        [R_EBP] = \"rbp\",\n        [R_ESP] = \"rsp\",\n        [8]  = \"r8\",\n        [9]  = \"r9\",\n        [10] = \"r10\",\n        [11] = \"r11\",\n        [12] = \"r12\",\n        [13] = \"r13\",\n        [14] = \"r14\",\n        [15] = \"r15\",\n#else\n        [R_EAX] = \"eax\",\n        [R_EBX] = \"ebx\",\n        [R_ECX] = \"ecx\",\n        [R_EDX] = \"edx\",\n        [R_ESI] = \"esi\",\n        [R_EDI] = \"edi\",\n        [R_EBP] = \"ebp\",\n        [R_ESP] = \"esp\",\n#endif\n    };\n    static const char seg_base_names[6][8] = {\n        [R_CS] = \"cs_base\",\n        [R_DS] = \"ds_base\",\n        [R_ES] = \"es_base\",\n        [R_FS] = \"fs_base\",\n        [R_GS] = \"gs_base\",\n        [R_SS] = \"ss_base\",\n    };\n    static const char bnd_regl_names[4][8] = {\n        \"bnd0_lb\", \"bnd1_lb\", \"bnd2_lb\", \"bnd3_lb\"\n    };\n    static const char bnd_regu_names[4][8] = {\n        \"bnd0_ub\", \"bnd1_ub\", \"bnd2_ub\", \"bnd3_ub\"\n    };\n    int i;\n    static bool initialized;\n\n    if (initialized) {\n        return;\n    }\n    initialized = true;\n\n    cpu_env = tcg_global_reg_new_ptr(TCG_AREG0, \"env\");\n    tcg_ctx.tcg_env = cpu_env;\n    cpu_cc_op = tcg_global_mem_new_i32(cpu_env,\n                                       offsetof(CPUX86State, cc_op), \"cc_op\");\n    cpu_cc_dst = tcg_global_mem_new(cpu_env, offsetof(CPUX86State, cc_dst),\n                                    \"cc_dst\");\n    cpu_cc_src = tcg_global_mem_new(cpu_env, offsetof(CPUX86State, cc_src),\n                                    \"cc_src\");\n    cpu_cc_src2 = tcg_global_mem_new(cpu_env, offsetof(CPUX86State, cc_src2),\n                                     \"cc_src2\");\n\n    for (i = 0; i < CPU_NB_REGS; ++i) {\n        cpu_regs[i] = tcg_global_mem_new(cpu_env,\n                                         offsetof(CPUX86State, regs[i]),\n                                         reg_names[i]);\n    }\n\n    for (i = 0; i < 6; ++i) {\n        cpu_seg_base[i]\n            = tcg_global_mem_new(cpu_env,\n                                 offsetof(CPUX86State, segs[i].base),\n                                 seg_base_names[i]);\n    }\n\n    for (i = 0; i < 4; ++i) {\n        cpu_bndl[i]\n            = tcg_global_mem_new_i64(cpu_env,\n                                     offsetof(CPUX86State, bnd_regs[i].lb),\n                                     bnd_regl_names[i]);\n        cpu_bndu[i]\n            = tcg_global_mem_new_i64(cpu_env,\n                                     offsetof(CPUX86State, bnd_regs[i].ub),\n                                     bnd_regu_names[i]);\n    }\n}\n\n/* generate intermediate code for basic block 'tb'.  */\nvoid gen_intermediate_code(CPUX86State *env, TranslationBlock *tb)\n{\n    X86CPU *cpu = x86_env_get_cpu(env);\n    CPUState *cs = CPU(cpu);\n    DisasContext dc1, *dc = &dc1;\n    target_ulong pc_ptr;\n    uint32_t flags;\n    target_ulong pc_start;\n    target_ulong cs_base;\n    int num_insns;\n    int max_insns;\n\n    /* generate intermediate code */\n    pc_start = tb->pc;\n    cs_base = tb->cs_base;\n    flags = tb->flags;\n\n    dc->pe = (flags >> HF_PE_SHIFT) & 1;\n    dc->code32 = (flags >> HF_CS32_SHIFT) & 1;\n    dc->ss32 = (flags >> HF_SS32_SHIFT) & 1;\n    dc->addseg = (flags >> HF_ADDSEG_SHIFT) & 1;\n    dc->f_st = 0;\n    dc->vm86 = (flags >> VM_SHIFT) & 1;\n    dc->cpl = (flags >> HF_CPL_SHIFT) & 3;\n    dc->iopl = (flags >> IOPL_SHIFT) & 3;\n    dc->tf = (flags >> TF_SHIFT) & 1;\n    dc->singlestep_enabled = cs->singlestep_enabled;\n    dc->cc_op = CC_OP_DYNAMIC;\n    dc->cc_op_dirty = false;\n    dc->cs_base = cs_base;\n    dc->tb = tb;\n    dc->popl_esp_hack = 0;\n    /* select memory access functions */\n    dc->mem_index = 0;\n#ifdef CONFIG_SOFTMMU\n    dc->mem_index = cpu_mmu_index(env, false);\n#endif\n    dc->cpuid_features = env->features[FEAT_1_EDX];\n    dc->cpuid_ext_features = env->features[FEAT_1_ECX];\n    dc->cpuid_ext2_features = env->features[FEAT_8000_0001_EDX];\n    dc->cpuid_ext3_features = env->features[FEAT_8000_0001_ECX];\n    dc->cpuid_7_0_ebx_features = env->features[FEAT_7_0_EBX];\n    dc->cpuid_xsave_features = env->features[FEAT_XSAVE];\n#ifdef TARGET_X86_64\n    dc->lma = (flags >> HF_LMA_SHIFT) & 1;\n    dc->code64 = (flags >> HF_CS64_SHIFT) & 1;\n#endif\n    dc->flags = flags;\n    dc->jmp_opt = !(dc->tf || cs->singlestep_enabled ||\n                    (flags & HF_INHIBIT_IRQ_MASK));\n    /* Do not optimize repz jumps at all in icount mode, because\n       rep movsS instructions are execured with different paths\n       in !repz_opt and repz_opt modes. The first one was used\n       always except single step mode. And this setting\n       disables jumps optimization and control paths become\n       equivalent in run and single step modes.\n       Now there will be no jump optimization for repz in\n       record/replay modes and there will always be an\n       additional step for ecx=0 when icount is enabled.\n     */\n    dc->repz_opt = !dc->jmp_opt && !(tb->cflags & CF_USE_ICOUNT);\n#if 0\n    /* check addseg logic */\n    if (!dc->addseg && (dc->vm86 || !dc->pe || !dc->code32))\n        printf(\"ERROR addseg\\n\");\n#endif\n\n    cpu_T0 = tcg_temp_new();\n    cpu_T1 = tcg_temp_new();\n    cpu_A0 = tcg_temp_new();\n\n    cpu_tmp0 = tcg_temp_new();\n    cpu_tmp1_i64 = tcg_temp_new_i64();\n    cpu_tmp2_i32 = tcg_temp_new_i32();\n    cpu_tmp3_i32 = tcg_temp_new_i32();\n    cpu_tmp4 = tcg_temp_new();\n    cpu_ptr0 = tcg_temp_new_ptr();\n    cpu_ptr1 = tcg_temp_new_ptr();\n    cpu_cc_srcT = tcg_temp_local_new();\n\n    dc->is_jmp = DISAS_NEXT;\n    pc_ptr = pc_start;\n    num_insns = 0;\n    max_insns = tb->cflags & CF_COUNT_MASK;\n    if (max_insns == 0) {\n        max_insns = CF_COUNT_MASK;\n    }\n    if (max_insns > TCG_MAX_INSNS) {\n        max_insns = TCG_MAX_INSNS;\n    }\n\n    gen_tb_start(tb);\n    for(;;) {\n        tcg_gen_insn_start(pc_ptr, dc->cc_op);\n        num_insns++;\n\n        /* If RF is set, suppress an internally generated breakpoint.  */\n        if (unlikely(cpu_breakpoint_test(cs, pc_ptr,\n                                         tb->flags & HF_RF_MASK\n                                         ? BP_GDB : BP_ANY))) {\n            gen_debug(dc, pc_ptr - dc->cs_base);\n            /* The address covered by the breakpoint must be included in\n               [tb->pc, tb->pc + tb->size) in order to for it to be\n               properly cleared -- thus we increment the PC here so that\n               the logic setting tb->size below does the right thing.  */\n            pc_ptr += 1;\n            goto done_generating;\n        }\n        if (num_insns == max_insns && (tb->cflags & CF_LAST_IO)) {\n            gen_io_start();\n        }\n\n        pc_ptr = disas_insn(env, dc, pc_ptr);\n        /* stop translation if indicated */\n        if (dc->is_jmp)\n            break;\n        /* if single step mode, we generate only one instruction and\n           generate an exception */\n        /* if irq were inhibited with HF_INHIBIT_IRQ_MASK, we clear\n           the flag and abort the translation to give the irqs a\n           change to be happen */\n        if (dc->tf || dc->singlestep_enabled ||\n            (flags & HF_INHIBIT_IRQ_MASK)) {\n            gen_jmp_im(pc_ptr - dc->cs_base);\n            gen_eob(dc);\n            break;\n        }\n        /* Do not cross the boundary of the pages in icount mode,\n           it can cause an exception. Do it only when boundary is\n           crossed by the first instruction in the block.\n           If current instruction already crossed the bound - it's ok,\n           because an exception hasn't stopped this code.\n         */\n        if ((tb->cflags & CF_USE_ICOUNT)\n            && ((pc_ptr & TARGET_PAGE_MASK)\n                != ((pc_ptr + TARGET_MAX_INSN_SIZE - 1) & TARGET_PAGE_MASK)\n                || (pc_ptr & ~TARGET_PAGE_MASK) == 0)) {\n            gen_jmp_im(pc_ptr - dc->cs_base);\n            gen_eob(dc);\n            break;\n        }\n        /* if too long translation, stop generation too */\n        if (tcg_op_buf_full() ||\n            (pc_ptr - pc_start) >= (TARGET_PAGE_SIZE - 32) ||\n            num_insns >= max_insns) {\n            gen_jmp_im(pc_ptr - dc->cs_base);\n            gen_eob(dc);\n            break;\n        }\n        if (singlestep) {\n            gen_jmp_im(pc_ptr - dc->cs_base);\n            gen_eob(dc);\n            break;\n        }\n    }\n    if (tb->cflags & CF_LAST_IO)\n        gen_io_end();\ndone_generating:\n    gen_tb_end(tb, num_insns);\n\n#ifdef DEBUG_DISAS\n    if (qemu_loglevel_mask(CPU_LOG_TB_IN_ASM)\n        && qemu_log_in_addr_range(pc_start)) {\n        int disas_flags;\n        qemu_log_lock();\n        qemu_log(\"----------------\\n\");\n        qemu_log(\"IN: %s\\n\", lookup_symbol(pc_start));\n#ifdef TARGET_X86_64\n        if (dc->code64)\n            disas_flags = 2;\n        else\n#endif\n            disas_flags = !dc->code32;\n        log_target_disas(cs, pc_start, pc_ptr - pc_start, disas_flags);\n        qemu_log(\"\\n\");\n        qemu_log_unlock();\n    }\n#endif\n\n    tb->size = pc_ptr - pc_start;\n    tb->icount = num_insns;\n}\n\nvoid restore_state_to_opc(CPUX86State *env, TranslationBlock *tb,\n                          target_ulong *data)\n{\n    int cc_op = data[1];\n    env->eip = data[0] - tb->cs_base;\n    if (cc_op != CC_OP_DYNAMIC) {\n        env->cc_op = cc_op;\n    }\n}\n"], "fixing_code": ["/*\n *  i386 translation\n *\n *  Copyright (c) 2003 Fabrice Bellard\n *\n * This library is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2 of the License, or (at your option) any later version.\n *\n * This library is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with this library; if not, see <http://www.gnu.org/licenses/>.\n */\n#include \"qemu/osdep.h\"\n\n#include \"qemu/host-utils.h\"\n#include \"cpu.h\"\n#include \"disas/disas.h\"\n#include \"exec/exec-all.h\"\n#include \"tcg-op.h\"\n#include \"exec/cpu_ldst.h\"\n\n#include \"exec/helper-proto.h\"\n#include \"exec/helper-gen.h\"\n\n#include \"trace-tcg.h\"\n#include \"exec/log.h\"\n\n\n#define PREFIX_REPZ   0x01\n#define PREFIX_REPNZ  0x02\n#define PREFIX_LOCK   0x04\n#define PREFIX_DATA   0x08\n#define PREFIX_ADR    0x10\n#define PREFIX_VEX    0x20\n\n#ifdef TARGET_X86_64\n#define CODE64(s) ((s)->code64)\n#define REX_X(s) ((s)->rex_x)\n#define REX_B(s) ((s)->rex_b)\n#else\n#define CODE64(s) 0\n#define REX_X(s) 0\n#define REX_B(s) 0\n#endif\n\n#ifdef TARGET_X86_64\n# define ctztl  ctz64\n# define clztl  clz64\n#else\n# define ctztl  ctz32\n# define clztl  clz32\n#endif\n\n/* For a switch indexed by MODRM, match all memory operands for a given OP.  */\n#define CASE_MODRM_MEM_OP(OP) \\\n    case (0 << 6) | (OP << 3) | 0 ... (0 << 6) | (OP << 3) | 7: \\\n    case (1 << 6) | (OP << 3) | 0 ... (1 << 6) | (OP << 3) | 7: \\\n    case (2 << 6) | (OP << 3) | 0 ... (2 << 6) | (OP << 3) | 7\n\n#define CASE_MODRM_OP(OP) \\\n    case (0 << 6) | (OP << 3) | 0 ... (0 << 6) | (OP << 3) | 7: \\\n    case (1 << 6) | (OP << 3) | 0 ... (1 << 6) | (OP << 3) | 7: \\\n    case (2 << 6) | (OP << 3) | 0 ... (2 << 6) | (OP << 3) | 7: \\\n    case (3 << 6) | (OP << 3) | 0 ... (3 << 6) | (OP << 3) | 7\n\n//#define MACRO_TEST   1\n\n/* global register indexes */\nstatic TCGv_env cpu_env;\nstatic TCGv cpu_A0;\nstatic TCGv cpu_cc_dst, cpu_cc_src, cpu_cc_src2, cpu_cc_srcT;\nstatic TCGv_i32 cpu_cc_op;\nstatic TCGv cpu_regs[CPU_NB_REGS];\nstatic TCGv cpu_seg_base[6];\nstatic TCGv_i64 cpu_bndl[4];\nstatic TCGv_i64 cpu_bndu[4];\n/* local temps */\nstatic TCGv cpu_T0, cpu_T1;\n/* local register indexes (only used inside old micro ops) */\nstatic TCGv cpu_tmp0, cpu_tmp4;\nstatic TCGv_ptr cpu_ptr0, cpu_ptr1;\nstatic TCGv_i32 cpu_tmp2_i32, cpu_tmp3_i32;\nstatic TCGv_i64 cpu_tmp1_i64;\n\n#include \"exec/gen-icount.h\"\n\n#ifdef TARGET_X86_64\nstatic int x86_64_hregs;\n#endif\n\ntypedef struct DisasContext {\n    /* current insn context */\n    int override; /* -1 if no override */\n    int prefix;\n    TCGMemOp aflag;\n    TCGMemOp dflag;\n    target_ulong pc_start;\n    target_ulong pc; /* pc = eip + cs_base */\n    int is_jmp; /* 1 = means jump (stop translation), 2 means CPU\n                   static state change (stop translation) */\n    /* current block context */\n    target_ulong cs_base; /* base of CS segment */\n    int pe;     /* protected mode */\n    int code32; /* 32 bit code segment */\n#ifdef TARGET_X86_64\n    int lma;    /* long mode active */\n    int code64; /* 64 bit code segment */\n    int rex_x, rex_b;\n#endif\n    int vex_l;  /* vex vector length */\n    int vex_v;  /* vex vvvv register, without 1's compliment.  */\n    int ss32;   /* 32 bit stack segment */\n    CCOp cc_op;  /* current CC operation */\n    bool cc_op_dirty;\n    int addseg; /* non zero if either DS/ES/SS have a non zero base */\n    int f_st;   /* currently unused */\n    int vm86;   /* vm86 mode */\n    int cpl;\n    int iopl;\n    int tf;     /* TF cpu flag */\n    int singlestep_enabled; /* \"hardware\" single step enabled */\n    int jmp_opt; /* use direct block chaining for direct jumps */\n    int repz_opt; /* optimize jumps within repz instructions */\n    int mem_index; /* select memory access functions */\n    uint64_t flags; /* all execution flags */\n    struct TranslationBlock *tb;\n    int popl_esp_hack; /* for correct popl with esp base handling */\n    int rip_offset; /* only used in x86_64, but left for simplicity */\n    int cpuid_features;\n    int cpuid_ext_features;\n    int cpuid_ext2_features;\n    int cpuid_ext3_features;\n    int cpuid_7_0_ebx_features;\n    int cpuid_xsave_features;\n} DisasContext;\n\nstatic void gen_eob(DisasContext *s);\nstatic void gen_jmp(DisasContext *s, target_ulong eip);\nstatic void gen_jmp_tb(DisasContext *s, target_ulong eip, int tb_num);\nstatic void gen_op(DisasContext *s1, int op, TCGMemOp ot, int d);\n\n/* i386 arith/logic operations */\nenum {\n    OP_ADDL,\n    OP_ORL,\n    OP_ADCL,\n    OP_SBBL,\n    OP_ANDL,\n    OP_SUBL,\n    OP_XORL,\n    OP_CMPL,\n};\n\n/* i386 shift ops */\nenum {\n    OP_ROL,\n    OP_ROR,\n    OP_RCL,\n    OP_RCR,\n    OP_SHL,\n    OP_SHR,\n    OP_SHL1, /* undocumented */\n    OP_SAR = 7,\n};\n\nenum {\n    JCC_O,\n    JCC_B,\n    JCC_Z,\n    JCC_BE,\n    JCC_S,\n    JCC_P,\n    JCC_L,\n    JCC_LE,\n};\n\nenum {\n    /* I386 int registers */\n    OR_EAX,   /* MUST be even numbered */\n    OR_ECX,\n    OR_EDX,\n    OR_EBX,\n    OR_ESP,\n    OR_EBP,\n    OR_ESI,\n    OR_EDI,\n\n    OR_TMP0 = 16,    /* temporary operand register */\n    OR_TMP1,\n    OR_A0, /* temporary register used when doing address evaluation */\n};\n\nenum {\n    USES_CC_DST  = 1,\n    USES_CC_SRC  = 2,\n    USES_CC_SRC2 = 4,\n    USES_CC_SRCT = 8,\n};\n\n/* Bit set if the global variable is live after setting CC_OP to X.  */\nstatic const uint8_t cc_op_live[CC_OP_NB] = {\n    [CC_OP_DYNAMIC] = USES_CC_DST | USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_EFLAGS] = USES_CC_SRC,\n    [CC_OP_MULB ... CC_OP_MULQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_ADDB ... CC_OP_ADDQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_ADCB ... CC_OP_ADCQ] = USES_CC_DST | USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_SUBB ... CC_OP_SUBQ] = USES_CC_DST | USES_CC_SRC | USES_CC_SRCT,\n    [CC_OP_SBBB ... CC_OP_SBBQ] = USES_CC_DST | USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_LOGICB ... CC_OP_LOGICQ] = USES_CC_DST,\n    [CC_OP_INCB ... CC_OP_INCQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_DECB ... CC_OP_DECQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_SHLB ... CC_OP_SHLQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_SARB ... CC_OP_SARQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_BMILGB ... CC_OP_BMILGQ] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_ADCX] = USES_CC_DST | USES_CC_SRC,\n    [CC_OP_ADOX] = USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_ADCOX] = USES_CC_DST | USES_CC_SRC | USES_CC_SRC2,\n    [CC_OP_CLR] = 0,\n    [CC_OP_POPCNT] = USES_CC_SRC,\n};\n\nstatic void set_cc_op(DisasContext *s, CCOp op)\n{\n    int dead;\n\n    if (s->cc_op == op) {\n        return;\n    }\n\n    /* Discard CC computation that will no longer be used.  */\n    dead = cc_op_live[s->cc_op] & ~cc_op_live[op];\n    if (dead & USES_CC_DST) {\n        tcg_gen_discard_tl(cpu_cc_dst);\n    }\n    if (dead & USES_CC_SRC) {\n        tcg_gen_discard_tl(cpu_cc_src);\n    }\n    if (dead & USES_CC_SRC2) {\n        tcg_gen_discard_tl(cpu_cc_src2);\n    }\n    if (dead & USES_CC_SRCT) {\n        tcg_gen_discard_tl(cpu_cc_srcT);\n    }\n\n    if (op == CC_OP_DYNAMIC) {\n        /* The DYNAMIC setting is translator only, and should never be\n           stored.  Thus we always consider it clean.  */\n        s->cc_op_dirty = false;\n    } else {\n        /* Discard any computed CC_OP value (see shifts).  */\n        if (s->cc_op == CC_OP_DYNAMIC) {\n            tcg_gen_discard_i32(cpu_cc_op);\n        }\n        s->cc_op_dirty = true;\n    }\n    s->cc_op = op;\n}\n\nstatic void gen_update_cc_op(DisasContext *s)\n{\n    if (s->cc_op_dirty) {\n        tcg_gen_movi_i32(cpu_cc_op, s->cc_op);\n        s->cc_op_dirty = false;\n    }\n}\n\n#ifdef TARGET_X86_64\n\n#define NB_OP_SIZES 4\n\n#else /* !TARGET_X86_64 */\n\n#define NB_OP_SIZES 3\n\n#endif /* !TARGET_X86_64 */\n\n#if defined(HOST_WORDS_BIGENDIAN)\n#define REG_B_OFFSET (sizeof(target_ulong) - 1)\n#define REG_H_OFFSET (sizeof(target_ulong) - 2)\n#define REG_W_OFFSET (sizeof(target_ulong) - 2)\n#define REG_L_OFFSET (sizeof(target_ulong) - 4)\n#define REG_LH_OFFSET (sizeof(target_ulong) - 8)\n#else\n#define REG_B_OFFSET 0\n#define REG_H_OFFSET 1\n#define REG_W_OFFSET 0\n#define REG_L_OFFSET 0\n#define REG_LH_OFFSET 4\n#endif\n\n/* In instruction encodings for byte register accesses the\n * register number usually indicates \"low 8 bits of register N\";\n * however there are some special cases where N 4..7 indicates\n * [AH, CH, DH, BH], ie \"bits 15..8 of register N-4\". Return\n * true for this special case, false otherwise.\n */\nstatic inline bool byte_reg_is_xH(int reg)\n{\n    if (reg < 4) {\n        return false;\n    }\n#ifdef TARGET_X86_64\n    if (reg >= 8 || x86_64_hregs) {\n        return false;\n    }\n#endif\n    return true;\n}\n\n/* Select the size of a push/pop operation.  */\nstatic inline TCGMemOp mo_pushpop(DisasContext *s, TCGMemOp ot)\n{\n    if (CODE64(s)) {\n        return ot == MO_16 ? MO_16 : MO_64;\n    } else {\n        return ot;\n    }\n}\n\n/* Select the size of the stack pointer.  */\nstatic inline TCGMemOp mo_stacksize(DisasContext *s)\n{\n    return CODE64(s) ? MO_64 : s->ss32 ? MO_32 : MO_16;\n}\n\n/* Select only size 64 else 32.  Used for SSE operand sizes.  */\nstatic inline TCGMemOp mo_64_32(TCGMemOp ot)\n{\n#ifdef TARGET_X86_64\n    return ot == MO_64 ? MO_64 : MO_32;\n#else\n    return MO_32;\n#endif\n}\n\n/* Select size 8 if lsb of B is clear, else OT.  Used for decoding\n   byte vs word opcodes.  */\nstatic inline TCGMemOp mo_b_d(int b, TCGMemOp ot)\n{\n    return b & 1 ? ot : MO_8;\n}\n\n/* Select size 8 if lsb of B is clear, else OT capped at 32.\n   Used for decoding operand size of port opcodes.  */\nstatic inline TCGMemOp mo_b_d32(int b, TCGMemOp ot)\n{\n    return b & 1 ? (ot == MO_16 ? MO_16 : MO_32) : MO_8;\n}\n\nstatic void gen_op_mov_reg_v(TCGMemOp ot, int reg, TCGv t0)\n{\n    switch(ot) {\n    case MO_8:\n        if (!byte_reg_is_xH(reg)) {\n            tcg_gen_deposit_tl(cpu_regs[reg], cpu_regs[reg], t0, 0, 8);\n        } else {\n            tcg_gen_deposit_tl(cpu_regs[reg - 4], cpu_regs[reg - 4], t0, 8, 8);\n        }\n        break;\n    case MO_16:\n        tcg_gen_deposit_tl(cpu_regs[reg], cpu_regs[reg], t0, 0, 16);\n        break;\n    case MO_32:\n        /* For x86_64, this sets the higher half of register to zero.\n           For i386, this is equivalent to a mov. */\n        tcg_gen_ext32u_tl(cpu_regs[reg], t0);\n        break;\n#ifdef TARGET_X86_64\n    case MO_64:\n        tcg_gen_mov_tl(cpu_regs[reg], t0);\n        break;\n#endif\n    default:\n        tcg_abort();\n    }\n}\n\nstatic inline void gen_op_mov_v_reg(TCGMemOp ot, TCGv t0, int reg)\n{\n    if (ot == MO_8 && byte_reg_is_xH(reg)) {\n        tcg_gen_extract_tl(t0, cpu_regs[reg - 4], 8, 8);\n    } else {\n        tcg_gen_mov_tl(t0, cpu_regs[reg]);\n    }\n}\n\nstatic void gen_add_A0_im(DisasContext *s, int val)\n{\n    tcg_gen_addi_tl(cpu_A0, cpu_A0, val);\n    if (!CODE64(s)) {\n        tcg_gen_ext32u_tl(cpu_A0, cpu_A0);\n    }\n}\n\nstatic inline void gen_op_jmp_v(TCGv dest)\n{\n    tcg_gen_st_tl(dest, cpu_env, offsetof(CPUX86State, eip));\n}\n\nstatic inline void gen_op_add_reg_im(TCGMemOp size, int reg, int32_t val)\n{\n    tcg_gen_addi_tl(cpu_tmp0, cpu_regs[reg], val);\n    gen_op_mov_reg_v(size, reg, cpu_tmp0);\n}\n\nstatic inline void gen_op_add_reg_T0(TCGMemOp size, int reg)\n{\n    tcg_gen_add_tl(cpu_tmp0, cpu_regs[reg], cpu_T0);\n    gen_op_mov_reg_v(size, reg, cpu_tmp0);\n}\n\nstatic inline void gen_op_ld_v(DisasContext *s, int idx, TCGv t0, TCGv a0)\n{\n    tcg_gen_qemu_ld_tl(t0, a0, s->mem_index, idx | MO_LE);\n}\n\nstatic inline void gen_op_st_v(DisasContext *s, int idx, TCGv t0, TCGv a0)\n{\n    tcg_gen_qemu_st_tl(t0, a0, s->mem_index, idx | MO_LE);\n}\n\nstatic inline void gen_op_st_rm_T0_A0(DisasContext *s, int idx, int d)\n{\n    if (d == OR_TMP0) {\n        gen_op_st_v(s, idx, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_reg_v(idx, d, cpu_T0);\n    }\n}\n\nstatic inline void gen_jmp_im(target_ulong pc)\n{\n    tcg_gen_movi_tl(cpu_tmp0, pc);\n    gen_op_jmp_v(cpu_tmp0);\n}\n\n/* Compute SEG:REG into A0.  SEG is selected from the override segment\n   (OVR_SEG) and the default segment (DEF_SEG).  OVR_SEG may be -1 to\n   indicate no override.  */\nstatic void gen_lea_v_seg(DisasContext *s, TCGMemOp aflag, TCGv a0,\n                          int def_seg, int ovr_seg)\n{\n    switch (aflag) {\n#ifdef TARGET_X86_64\n    case MO_64:\n        if (ovr_seg < 0) {\n            tcg_gen_mov_tl(cpu_A0, a0);\n            return;\n        }\n        break;\n#endif\n    case MO_32:\n        /* 32 bit address */\n        if (ovr_seg < 0 && s->addseg) {\n            ovr_seg = def_seg;\n        }\n        if (ovr_seg < 0) {\n            tcg_gen_ext32u_tl(cpu_A0, a0);\n            return;\n        }\n        break;\n    case MO_16:\n        /* 16 bit address */\n        tcg_gen_ext16u_tl(cpu_A0, a0);\n        a0 = cpu_A0;\n        if (ovr_seg < 0) {\n            if (s->addseg) {\n                ovr_seg = def_seg;\n            } else {\n                return;\n            }\n        }\n        break;\n    default:\n        tcg_abort();\n    }\n\n    if (ovr_seg >= 0) {\n        TCGv seg = cpu_seg_base[ovr_seg];\n\n        if (aflag == MO_64) {\n            tcg_gen_add_tl(cpu_A0, a0, seg);\n        } else if (CODE64(s)) {\n            tcg_gen_ext32u_tl(cpu_A0, a0);\n            tcg_gen_add_tl(cpu_A0, cpu_A0, seg);\n        } else {\n            tcg_gen_add_tl(cpu_A0, a0, seg);\n            tcg_gen_ext32u_tl(cpu_A0, cpu_A0);\n        }\n    }\n}\n\nstatic inline void gen_string_movl_A0_ESI(DisasContext *s)\n{\n    gen_lea_v_seg(s, s->aflag, cpu_regs[R_ESI], R_DS, s->override);\n}\n\nstatic inline void gen_string_movl_A0_EDI(DisasContext *s)\n{\n    gen_lea_v_seg(s, s->aflag, cpu_regs[R_EDI], R_ES, -1);\n}\n\nstatic inline void gen_op_movl_T0_Dshift(TCGMemOp ot)\n{\n    tcg_gen_ld32s_tl(cpu_T0, cpu_env, offsetof(CPUX86State, df));\n    tcg_gen_shli_tl(cpu_T0, cpu_T0, ot);\n};\n\nstatic TCGv gen_ext_tl(TCGv dst, TCGv src, TCGMemOp size, bool sign)\n{\n    switch (size) {\n    case MO_8:\n        if (sign) {\n            tcg_gen_ext8s_tl(dst, src);\n        } else {\n            tcg_gen_ext8u_tl(dst, src);\n        }\n        return dst;\n    case MO_16:\n        if (sign) {\n            tcg_gen_ext16s_tl(dst, src);\n        } else {\n            tcg_gen_ext16u_tl(dst, src);\n        }\n        return dst;\n#ifdef TARGET_X86_64\n    case MO_32:\n        if (sign) {\n            tcg_gen_ext32s_tl(dst, src);\n        } else {\n            tcg_gen_ext32u_tl(dst, src);\n        }\n        return dst;\n#endif\n    default:\n        return src;\n    }\n}\n\nstatic void gen_extu(TCGMemOp ot, TCGv reg)\n{\n    gen_ext_tl(reg, reg, ot, false);\n}\n\nstatic void gen_exts(TCGMemOp ot, TCGv reg)\n{\n    gen_ext_tl(reg, reg, ot, true);\n}\n\nstatic inline void gen_op_jnz_ecx(TCGMemOp size, TCGLabel *label1)\n{\n    tcg_gen_mov_tl(cpu_tmp0, cpu_regs[R_ECX]);\n    gen_extu(size, cpu_tmp0);\n    tcg_gen_brcondi_tl(TCG_COND_NE, cpu_tmp0, 0, label1);\n}\n\nstatic inline void gen_op_jz_ecx(TCGMemOp size, TCGLabel *label1)\n{\n    tcg_gen_mov_tl(cpu_tmp0, cpu_regs[R_ECX]);\n    gen_extu(size, cpu_tmp0);\n    tcg_gen_brcondi_tl(TCG_COND_EQ, cpu_tmp0, 0, label1);\n}\n\nstatic void gen_helper_in_func(TCGMemOp ot, TCGv v, TCGv_i32 n)\n{\n    switch (ot) {\n    case MO_8:\n        gen_helper_inb(v, cpu_env, n);\n        break;\n    case MO_16:\n        gen_helper_inw(v, cpu_env, n);\n        break;\n    case MO_32:\n        gen_helper_inl(v, cpu_env, n);\n        break;\n    default:\n        tcg_abort();\n    }\n}\n\nstatic void gen_helper_out_func(TCGMemOp ot, TCGv_i32 v, TCGv_i32 n)\n{\n    switch (ot) {\n    case MO_8:\n        gen_helper_outb(cpu_env, v, n);\n        break;\n    case MO_16:\n        gen_helper_outw(cpu_env, v, n);\n        break;\n    case MO_32:\n        gen_helper_outl(cpu_env, v, n);\n        break;\n    default:\n        tcg_abort();\n    }\n}\n\nstatic void gen_check_io(DisasContext *s, TCGMemOp ot, target_ulong cur_eip,\n                         uint32_t svm_flags)\n{\n    target_ulong next_eip;\n\n    if (s->pe && (s->cpl > s->iopl || s->vm86)) {\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        switch (ot) {\n        case MO_8:\n            gen_helper_check_iob(cpu_env, cpu_tmp2_i32);\n            break;\n        case MO_16:\n            gen_helper_check_iow(cpu_env, cpu_tmp2_i32);\n            break;\n        case MO_32:\n            gen_helper_check_iol(cpu_env, cpu_tmp2_i32);\n            break;\n        default:\n            tcg_abort();\n        }\n    }\n    if(s->flags & HF_SVMI_MASK) {\n        gen_update_cc_op(s);\n        gen_jmp_im(cur_eip);\n        svm_flags |= (1 << (4 + ot));\n        next_eip = s->pc - s->cs_base;\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        gen_helper_svm_check_io(cpu_env, cpu_tmp2_i32,\n                                tcg_const_i32(svm_flags),\n                                tcg_const_i32(next_eip - cur_eip));\n    }\n}\n\nstatic inline void gen_movs(DisasContext *s, TCGMemOp ot)\n{\n    gen_string_movl_A0_ESI(s);\n    gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    gen_string_movl_A0_EDI(s);\n    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_ESI);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n}\n\nstatic void gen_op_update1_cc(void)\n{\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n}\n\nstatic void gen_op_update2_cc(void)\n{\n    tcg_gen_mov_tl(cpu_cc_src, cpu_T1);\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n}\n\nstatic void gen_op_update3_cc(TCGv reg)\n{\n    tcg_gen_mov_tl(cpu_cc_src2, reg);\n    tcg_gen_mov_tl(cpu_cc_src, cpu_T1);\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n}\n\nstatic inline void gen_op_testl_T0_T1_cc(void)\n{\n    tcg_gen_and_tl(cpu_cc_dst, cpu_T0, cpu_T1);\n}\n\nstatic void gen_op_update_neg_cc(void)\n{\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n    tcg_gen_neg_tl(cpu_cc_src, cpu_T0);\n    tcg_gen_movi_tl(cpu_cc_srcT, 0);\n}\n\n/* compute all eflags to cc_src */\nstatic void gen_compute_eflags(DisasContext *s)\n{\n    TCGv zero, dst, src1, src2;\n    int live, dead;\n\n    if (s->cc_op == CC_OP_EFLAGS) {\n        return;\n    }\n    if (s->cc_op == CC_OP_CLR) {\n        tcg_gen_movi_tl(cpu_cc_src, CC_Z | CC_P);\n        set_cc_op(s, CC_OP_EFLAGS);\n        return;\n    }\n\n    TCGV_UNUSED(zero);\n    dst = cpu_cc_dst;\n    src1 = cpu_cc_src;\n    src2 = cpu_cc_src2;\n\n    /* Take care to not read values that are not live.  */\n    live = cc_op_live[s->cc_op] & ~USES_CC_SRCT;\n    dead = live ^ (USES_CC_DST | USES_CC_SRC | USES_CC_SRC2);\n    if (dead) {\n        zero = tcg_const_tl(0);\n        if (dead & USES_CC_DST) {\n            dst = zero;\n        }\n        if (dead & USES_CC_SRC) {\n            src1 = zero;\n        }\n        if (dead & USES_CC_SRC2) {\n            src2 = zero;\n        }\n    }\n\n    gen_update_cc_op(s);\n    gen_helper_cc_compute_all(cpu_cc_src, dst, src1, src2, cpu_cc_op);\n    set_cc_op(s, CC_OP_EFLAGS);\n\n    if (dead) {\n        tcg_temp_free(zero);\n    }\n}\n\ntypedef struct CCPrepare {\n    TCGCond cond;\n    TCGv reg;\n    TCGv reg2;\n    target_ulong imm;\n    target_ulong mask;\n    bool use_reg2;\n    bool no_setcond;\n} CCPrepare;\n\n/* compute eflags.C to reg */\nstatic CCPrepare gen_prepare_eflags_c(DisasContext *s, TCGv reg)\n{\n    TCGv t0, t1;\n    int size, shift;\n\n    switch (s->cc_op) {\n    case CC_OP_SUBB ... CC_OP_SUBQ:\n        /* (DATA_TYPE)CC_SRCT < (DATA_TYPE)CC_SRC */\n        size = s->cc_op - CC_OP_SUBB;\n        t1 = gen_ext_tl(cpu_tmp0, cpu_cc_src, size, false);\n        /* If no temporary was used, be careful not to alias t1 and t0.  */\n        t0 = TCGV_EQUAL(t1, cpu_cc_src) ? cpu_tmp0 : reg;\n        tcg_gen_mov_tl(t0, cpu_cc_srcT);\n        gen_extu(size, t0);\n        goto add_sub;\n\n    case CC_OP_ADDB ... CC_OP_ADDQ:\n        /* (DATA_TYPE)CC_DST < (DATA_TYPE)CC_SRC */\n        size = s->cc_op - CC_OP_ADDB;\n        t1 = gen_ext_tl(cpu_tmp0, cpu_cc_src, size, false);\n        t0 = gen_ext_tl(reg, cpu_cc_dst, size, false);\n    add_sub:\n        return (CCPrepare) { .cond = TCG_COND_LTU, .reg = t0,\n                             .reg2 = t1, .mask = -1, .use_reg2 = true };\n\n    case CC_OP_LOGICB ... CC_OP_LOGICQ:\n    case CC_OP_CLR:\n    case CC_OP_POPCNT:\n        return (CCPrepare) { .cond = TCG_COND_NEVER, .mask = -1 };\n\n    case CC_OP_INCB ... CC_OP_INCQ:\n    case CC_OP_DECB ... CC_OP_DECQ:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = -1, .no_setcond = true };\n\n    case CC_OP_SHLB ... CC_OP_SHLQ:\n        /* (CC_SRC >> (DATA_BITS - 1)) & 1 */\n        size = s->cc_op - CC_OP_SHLB;\n        shift = (8 << size) - 1;\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = (target_ulong)1 << shift };\n\n    case CC_OP_MULB ... CC_OP_MULQ:\n        return (CCPrepare) { .cond = TCG_COND_NE,\n                             .reg = cpu_cc_src, .mask = -1 };\n\n    case CC_OP_BMILGB ... CC_OP_BMILGQ:\n        size = s->cc_op - CC_OP_BMILGB;\n        t0 = gen_ext_tl(reg, cpu_cc_src, size, false);\n        return (CCPrepare) { .cond = TCG_COND_EQ, .reg = t0, .mask = -1 };\n\n    case CC_OP_ADCX:\n    case CC_OP_ADCOX:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_dst,\n                             .mask = -1, .no_setcond = true };\n\n    case CC_OP_EFLAGS:\n    case CC_OP_SARB ... CC_OP_SARQ:\n        /* CC_SRC & 1 */\n        return (CCPrepare) { .cond = TCG_COND_NE,\n                             .reg = cpu_cc_src, .mask = CC_C };\n\n    default:\n       /* The need to compute only C from CC_OP_DYNAMIC is important\n          in efficiently implementing e.g. INC at the start of a TB.  */\n       gen_update_cc_op(s);\n       gen_helper_cc_compute_c(reg, cpu_cc_dst, cpu_cc_src,\n                               cpu_cc_src2, cpu_cc_op);\n       return (CCPrepare) { .cond = TCG_COND_NE, .reg = reg,\n                            .mask = -1, .no_setcond = true };\n    }\n}\n\n/* compute eflags.P to reg */\nstatic CCPrepare gen_prepare_eflags_p(DisasContext *s, TCGv reg)\n{\n    gen_compute_eflags(s);\n    return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                         .mask = CC_P };\n}\n\n/* compute eflags.S to reg */\nstatic CCPrepare gen_prepare_eflags_s(DisasContext *s, TCGv reg)\n{\n    switch (s->cc_op) {\n    case CC_OP_DYNAMIC:\n        gen_compute_eflags(s);\n        /* FALLTHRU */\n    case CC_OP_EFLAGS:\n    case CC_OP_ADCX:\n    case CC_OP_ADOX:\n    case CC_OP_ADCOX:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = CC_S };\n    case CC_OP_CLR:\n    case CC_OP_POPCNT:\n        return (CCPrepare) { .cond = TCG_COND_NEVER, .mask = -1 };\n    default:\n        {\n            TCGMemOp size = (s->cc_op - CC_OP_ADDB) & 3;\n            TCGv t0 = gen_ext_tl(reg, cpu_cc_dst, size, true);\n            return (CCPrepare) { .cond = TCG_COND_LT, .reg = t0, .mask = -1 };\n        }\n    }\n}\n\n/* compute eflags.O to reg */\nstatic CCPrepare gen_prepare_eflags_o(DisasContext *s, TCGv reg)\n{\n    switch (s->cc_op) {\n    case CC_OP_ADOX:\n    case CC_OP_ADCOX:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src2,\n                             .mask = -1, .no_setcond = true };\n    case CC_OP_CLR:\n    case CC_OP_POPCNT:\n        return (CCPrepare) { .cond = TCG_COND_NEVER, .mask = -1 };\n    default:\n        gen_compute_eflags(s);\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = CC_O };\n    }\n}\n\n/* compute eflags.Z to reg */\nstatic CCPrepare gen_prepare_eflags_z(DisasContext *s, TCGv reg)\n{\n    switch (s->cc_op) {\n    case CC_OP_DYNAMIC:\n        gen_compute_eflags(s);\n        /* FALLTHRU */\n    case CC_OP_EFLAGS:\n    case CC_OP_ADCX:\n    case CC_OP_ADOX:\n    case CC_OP_ADCOX:\n        return (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                             .mask = CC_Z };\n    case CC_OP_CLR:\n        return (CCPrepare) { .cond = TCG_COND_ALWAYS, .mask = -1 };\n    case CC_OP_POPCNT:\n        return (CCPrepare) { .cond = TCG_COND_EQ, .reg = cpu_cc_src,\n                             .mask = -1 };\n    default:\n        {\n            TCGMemOp size = (s->cc_op - CC_OP_ADDB) & 3;\n            TCGv t0 = gen_ext_tl(reg, cpu_cc_dst, size, false);\n            return (CCPrepare) { .cond = TCG_COND_EQ, .reg = t0, .mask = -1 };\n        }\n    }\n}\n\n/* perform a conditional store into register 'reg' according to jump opcode\n   value 'b'. In the fast case, T0 is guaranted not to be used. */\nstatic CCPrepare gen_prepare_cc(DisasContext *s, int b, TCGv reg)\n{\n    int inv, jcc_op, cond;\n    TCGMemOp size;\n    CCPrepare cc;\n    TCGv t0;\n\n    inv = b & 1;\n    jcc_op = (b >> 1) & 7;\n\n    switch (s->cc_op) {\n    case CC_OP_SUBB ... CC_OP_SUBQ:\n        /* We optimize relational operators for the cmp/jcc case.  */\n        size = s->cc_op - CC_OP_SUBB;\n        switch (jcc_op) {\n        case JCC_BE:\n            tcg_gen_mov_tl(cpu_tmp4, cpu_cc_srcT);\n            gen_extu(size, cpu_tmp4);\n            t0 = gen_ext_tl(cpu_tmp0, cpu_cc_src, size, false);\n            cc = (CCPrepare) { .cond = TCG_COND_LEU, .reg = cpu_tmp4,\n                               .reg2 = t0, .mask = -1, .use_reg2 = true };\n            break;\n\n        case JCC_L:\n            cond = TCG_COND_LT;\n            goto fast_jcc_l;\n        case JCC_LE:\n            cond = TCG_COND_LE;\n        fast_jcc_l:\n            tcg_gen_mov_tl(cpu_tmp4, cpu_cc_srcT);\n            gen_exts(size, cpu_tmp4);\n            t0 = gen_ext_tl(cpu_tmp0, cpu_cc_src, size, true);\n            cc = (CCPrepare) { .cond = cond, .reg = cpu_tmp4,\n                               .reg2 = t0, .mask = -1, .use_reg2 = true };\n            break;\n\n        default:\n            goto slow_jcc;\n        }\n        break;\n\n    default:\n    slow_jcc:\n        /* This actually generates good code for JC, JZ and JS.  */\n        switch (jcc_op) {\n        case JCC_O:\n            cc = gen_prepare_eflags_o(s, reg);\n            break;\n        case JCC_B:\n            cc = gen_prepare_eflags_c(s, reg);\n            break;\n        case JCC_Z:\n            cc = gen_prepare_eflags_z(s, reg);\n            break;\n        case JCC_BE:\n            gen_compute_eflags(s);\n            cc = (CCPrepare) { .cond = TCG_COND_NE, .reg = cpu_cc_src,\n                               .mask = CC_Z | CC_C };\n            break;\n        case JCC_S:\n            cc = gen_prepare_eflags_s(s, reg);\n            break;\n        case JCC_P:\n            cc = gen_prepare_eflags_p(s, reg);\n            break;\n        case JCC_L:\n            gen_compute_eflags(s);\n            if (TCGV_EQUAL(reg, cpu_cc_src)) {\n                reg = cpu_tmp0;\n            }\n            tcg_gen_shri_tl(reg, cpu_cc_src, 4); /* CC_O -> CC_S */\n            tcg_gen_xor_tl(reg, reg, cpu_cc_src);\n            cc = (CCPrepare) { .cond = TCG_COND_NE, .reg = reg,\n                               .mask = CC_S };\n            break;\n        default:\n        case JCC_LE:\n            gen_compute_eflags(s);\n            if (TCGV_EQUAL(reg, cpu_cc_src)) {\n                reg = cpu_tmp0;\n            }\n            tcg_gen_shri_tl(reg, cpu_cc_src, 4); /* CC_O -> CC_S */\n            tcg_gen_xor_tl(reg, reg, cpu_cc_src);\n            cc = (CCPrepare) { .cond = TCG_COND_NE, .reg = reg,\n                               .mask = CC_S | CC_Z };\n            break;\n        }\n        break;\n    }\n\n    if (inv) {\n        cc.cond = tcg_invert_cond(cc.cond);\n    }\n    return cc;\n}\n\nstatic void gen_setcc1(DisasContext *s, int b, TCGv reg)\n{\n    CCPrepare cc = gen_prepare_cc(s, b, reg);\n\n    if (cc.no_setcond) {\n        if (cc.cond == TCG_COND_EQ) {\n            tcg_gen_xori_tl(reg, cc.reg, 1);\n        } else {\n            tcg_gen_mov_tl(reg, cc.reg);\n        }\n        return;\n    }\n\n    if (cc.cond == TCG_COND_NE && !cc.use_reg2 && cc.imm == 0 &&\n        cc.mask != 0 && (cc.mask & (cc.mask - 1)) == 0) {\n        tcg_gen_shri_tl(reg, cc.reg, ctztl(cc.mask));\n        tcg_gen_andi_tl(reg, reg, 1);\n        return;\n    }\n    if (cc.mask != -1) {\n        tcg_gen_andi_tl(reg, cc.reg, cc.mask);\n        cc.reg = reg;\n    }\n    if (cc.use_reg2) {\n        tcg_gen_setcond_tl(cc.cond, reg, cc.reg, cc.reg2);\n    } else {\n        tcg_gen_setcondi_tl(cc.cond, reg, cc.reg, cc.imm);\n    }\n}\n\nstatic inline void gen_compute_eflags_c(DisasContext *s, TCGv reg)\n{\n    gen_setcc1(s, JCC_B << 1, reg);\n}\n\n/* generate a conditional jump to label 'l1' according to jump opcode\n   value 'b'. In the fast case, T0 is guaranted not to be used. */\nstatic inline void gen_jcc1_noeob(DisasContext *s, int b, TCGLabel *l1)\n{\n    CCPrepare cc = gen_prepare_cc(s, b, cpu_T0);\n\n    if (cc.mask != -1) {\n        tcg_gen_andi_tl(cpu_T0, cc.reg, cc.mask);\n        cc.reg = cpu_T0;\n    }\n    if (cc.use_reg2) {\n        tcg_gen_brcond_tl(cc.cond, cc.reg, cc.reg2, l1);\n    } else {\n        tcg_gen_brcondi_tl(cc.cond, cc.reg, cc.imm, l1);\n    }\n}\n\n/* Generate a conditional jump to label 'l1' according to jump opcode\n   value 'b'. In the fast case, T0 is guaranted not to be used.\n   A translation block must end soon.  */\nstatic inline void gen_jcc1(DisasContext *s, int b, TCGLabel *l1)\n{\n    CCPrepare cc = gen_prepare_cc(s, b, cpu_T0);\n\n    gen_update_cc_op(s);\n    if (cc.mask != -1) {\n        tcg_gen_andi_tl(cpu_T0, cc.reg, cc.mask);\n        cc.reg = cpu_T0;\n    }\n    set_cc_op(s, CC_OP_DYNAMIC);\n    if (cc.use_reg2) {\n        tcg_gen_brcond_tl(cc.cond, cc.reg, cc.reg2, l1);\n    } else {\n        tcg_gen_brcondi_tl(cc.cond, cc.reg, cc.imm, l1);\n    }\n}\n\n/* XXX: does not work with gdbstub \"ice\" single step - not a\n   serious problem */\nstatic TCGLabel *gen_jz_ecx_string(DisasContext *s, target_ulong next_eip)\n{\n    TCGLabel *l1 = gen_new_label();\n    TCGLabel *l2 = gen_new_label();\n    gen_op_jnz_ecx(s->aflag, l1);\n    gen_set_label(l2);\n    gen_jmp_tb(s, next_eip, 1);\n    gen_set_label(l1);\n    return l2;\n}\n\nstatic inline void gen_stos(DisasContext *s, TCGMemOp ot)\n{\n    gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n    gen_string_movl_A0_EDI(s);\n    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n}\n\nstatic inline void gen_lods(DisasContext *s, TCGMemOp ot)\n{\n    gen_string_movl_A0_ESI(s);\n    gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    gen_op_mov_reg_v(ot, R_EAX, cpu_T0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_ESI);\n}\n\nstatic inline void gen_scas(DisasContext *s, TCGMemOp ot)\n{\n    gen_string_movl_A0_EDI(s);\n    gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n    gen_op(s, OP_CMPL, ot, R_EAX);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n}\n\nstatic inline void gen_cmps(DisasContext *s, TCGMemOp ot)\n{\n    gen_string_movl_A0_EDI(s);\n    gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n    gen_string_movl_A0_ESI(s);\n    gen_op(s, OP_CMPL, ot, OR_TMP0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_ESI);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n}\n\nstatic void gen_bpt_io(DisasContext *s, TCGv_i32 t_port, int ot)\n{\n    if (s->flags & HF_IOBPT_MASK) {\n        TCGv_i32 t_size = tcg_const_i32(1 << ot);\n        TCGv t_next = tcg_const_tl(s->pc - s->cs_base);\n\n        gen_helper_bpt_io(cpu_env, t_port, t_size, t_next);\n        tcg_temp_free_i32(t_size);\n        tcg_temp_free(t_next);\n    }\n}\n\n\nstatic inline void gen_ins(DisasContext *s, TCGMemOp ot)\n{\n    if (s->tb->cflags & CF_USE_ICOUNT) {\n        gen_io_start();\n    }\n    gen_string_movl_A0_EDI(s);\n    /* Note: we must do this dummy write first to be restartable in\n       case of page fault. */\n    tcg_gen_movi_tl(cpu_T0, 0);\n    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n    tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_EDX]);\n    tcg_gen_andi_i32(cpu_tmp2_i32, cpu_tmp2_i32, 0xffff);\n    gen_helper_in_func(ot, cpu_T0, cpu_tmp2_i32);\n    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_EDI);\n    gen_bpt_io(s, cpu_tmp2_i32, ot);\n    if (s->tb->cflags & CF_USE_ICOUNT) {\n        gen_io_end();\n    }\n}\n\nstatic inline void gen_outs(DisasContext *s, TCGMemOp ot)\n{\n    if (s->tb->cflags & CF_USE_ICOUNT) {\n        gen_io_start();\n    }\n    gen_string_movl_A0_ESI(s);\n    gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n\n    tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_EDX]);\n    tcg_gen_andi_i32(cpu_tmp2_i32, cpu_tmp2_i32, 0xffff);\n    tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T0);\n    gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n    gen_op_movl_T0_Dshift(ot);\n    gen_op_add_reg_T0(s->aflag, R_ESI);\n    gen_bpt_io(s, cpu_tmp2_i32, ot);\n    if (s->tb->cflags & CF_USE_ICOUNT) {\n        gen_io_end();\n    }\n}\n\n/* same method as Valgrind : we generate jumps to current or next\n   instruction */\n#define GEN_REPZ(op)                                                          \\\nstatic inline void gen_repz_ ## op(DisasContext *s, TCGMemOp ot,              \\\n                                 target_ulong cur_eip, target_ulong next_eip) \\\n{                                                                             \\\n    TCGLabel *l2;                                                             \\\n    gen_update_cc_op(s);                                                      \\\n    l2 = gen_jz_ecx_string(s, next_eip);                                      \\\n    gen_ ## op(s, ot);                                                        \\\n    gen_op_add_reg_im(s->aflag, R_ECX, -1);                                   \\\n    /* a loop would cause two single step exceptions if ECX = 1               \\\n       before rep string_insn */                                              \\\n    if (s->repz_opt)                                                          \\\n        gen_op_jz_ecx(s->aflag, l2);                                          \\\n    gen_jmp(s, cur_eip);                                                      \\\n}\n\n#define GEN_REPZ2(op)                                                         \\\nstatic inline void gen_repz_ ## op(DisasContext *s, TCGMemOp ot,              \\\n                                   target_ulong cur_eip,                      \\\n                                   target_ulong next_eip,                     \\\n                                   int nz)                                    \\\n{                                                                             \\\n    TCGLabel *l2;                                                             \\\n    gen_update_cc_op(s);                                                      \\\n    l2 = gen_jz_ecx_string(s, next_eip);                                      \\\n    gen_ ## op(s, ot);                                                        \\\n    gen_op_add_reg_im(s->aflag, R_ECX, -1);                                   \\\n    gen_update_cc_op(s);                                                      \\\n    gen_jcc1(s, (JCC_Z << 1) | (nz ^ 1), l2);                                 \\\n    if (s->repz_opt)                                                          \\\n        gen_op_jz_ecx(s->aflag, l2);                                          \\\n    gen_jmp(s, cur_eip);                                                      \\\n}\n\nGEN_REPZ(movs)\nGEN_REPZ(stos)\nGEN_REPZ(lods)\nGEN_REPZ(ins)\nGEN_REPZ(outs)\nGEN_REPZ2(scas)\nGEN_REPZ2(cmps)\n\nstatic void gen_helper_fp_arith_ST0_FT0(int op)\n{\n    switch (op) {\n    case 0:\n        gen_helper_fadd_ST0_FT0(cpu_env);\n        break;\n    case 1:\n        gen_helper_fmul_ST0_FT0(cpu_env);\n        break;\n    case 2:\n        gen_helper_fcom_ST0_FT0(cpu_env);\n        break;\n    case 3:\n        gen_helper_fcom_ST0_FT0(cpu_env);\n        break;\n    case 4:\n        gen_helper_fsub_ST0_FT0(cpu_env);\n        break;\n    case 5:\n        gen_helper_fsubr_ST0_FT0(cpu_env);\n        break;\n    case 6:\n        gen_helper_fdiv_ST0_FT0(cpu_env);\n        break;\n    case 7:\n        gen_helper_fdivr_ST0_FT0(cpu_env);\n        break;\n    }\n}\n\n/* NOTE the exception in \"r\" op ordering */\nstatic void gen_helper_fp_arith_STN_ST0(int op, int opreg)\n{\n    TCGv_i32 tmp = tcg_const_i32(opreg);\n    switch (op) {\n    case 0:\n        gen_helper_fadd_STN_ST0(cpu_env, tmp);\n        break;\n    case 1:\n        gen_helper_fmul_STN_ST0(cpu_env, tmp);\n        break;\n    case 4:\n        gen_helper_fsubr_STN_ST0(cpu_env, tmp);\n        break;\n    case 5:\n        gen_helper_fsub_STN_ST0(cpu_env, tmp);\n        break;\n    case 6:\n        gen_helper_fdivr_STN_ST0(cpu_env, tmp);\n        break;\n    case 7:\n        gen_helper_fdiv_STN_ST0(cpu_env, tmp);\n        break;\n    }\n}\n\n/* if d == OR_TMP0, it means memory operand (address in A0) */\nstatic void gen_op(DisasContext *s1, int op, TCGMemOp ot, int d)\n{\n    if (d != OR_TMP0) {\n        gen_op_mov_v_reg(ot, cpu_T0, d);\n    } else if (!(s1->prefix & PREFIX_LOCK)) {\n        gen_op_ld_v(s1, ot, cpu_T0, cpu_A0);\n    }\n    switch(op) {\n    case OP_ADCL:\n        gen_compute_eflags_c(s1, cpu_tmp4);\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_add_tl(cpu_T0, cpu_tmp4, cpu_T1);\n            tcg_gen_atomic_add_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_tmp4);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update3_cc(cpu_tmp4);\n        set_cc_op(s1, CC_OP_ADCB + ot);\n        break;\n    case OP_SBBL:\n        gen_compute_eflags_c(s1, cpu_tmp4);\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_add_tl(cpu_T0, cpu_T1, cpu_tmp4);\n            tcg_gen_neg_tl(cpu_T0, cpu_T0);\n            tcg_gen_atomic_add_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_sub_tl(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_sub_tl(cpu_T0, cpu_T0, cpu_tmp4);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update3_cc(cpu_tmp4);\n        set_cc_op(s1, CC_OP_SBBB + ot);\n        break;\n    case OP_ADDL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_atomic_add_fetch_tl(cpu_T0, cpu_A0, cpu_T1,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update2_cc();\n        set_cc_op(s1, CC_OP_ADDB + ot);\n        break;\n    case OP_SUBL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_neg_tl(cpu_T0, cpu_T1);\n            tcg_gen_atomic_fetch_add_tl(cpu_cc_srcT, cpu_A0, cpu_T0,\n                                        s1->mem_index, ot | MO_LE);\n            tcg_gen_sub_tl(cpu_T0, cpu_cc_srcT, cpu_T1);\n        } else {\n            tcg_gen_mov_tl(cpu_cc_srcT, cpu_T0);\n            tcg_gen_sub_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update2_cc();\n        set_cc_op(s1, CC_OP_SUBB + ot);\n        break;\n    default:\n    case OP_ANDL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_atomic_and_fetch_tl(cpu_T0, cpu_A0, cpu_T1,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_and_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update1_cc();\n        set_cc_op(s1, CC_OP_LOGICB + ot);\n        break;\n    case OP_ORL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_atomic_or_fetch_tl(cpu_T0, cpu_A0, cpu_T1,\n                                       s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update1_cc();\n        set_cc_op(s1, CC_OP_LOGICB + ot);\n        break;\n    case OP_XORL:\n        if (s1->prefix & PREFIX_LOCK) {\n            tcg_gen_atomic_xor_fetch_tl(cpu_T0, cpu_A0, cpu_T1,\n                                        s1->mem_index, ot | MO_LE);\n        } else {\n            tcg_gen_xor_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_st_rm_T0_A0(s1, ot, d);\n        }\n        gen_op_update1_cc();\n        set_cc_op(s1, CC_OP_LOGICB + ot);\n        break;\n    case OP_CMPL:\n        tcg_gen_mov_tl(cpu_cc_src, cpu_T1);\n        tcg_gen_mov_tl(cpu_cc_srcT, cpu_T0);\n        tcg_gen_sub_tl(cpu_cc_dst, cpu_T0, cpu_T1);\n        set_cc_op(s1, CC_OP_SUBB + ot);\n        break;\n    }\n}\n\n/* if d == OR_TMP0, it means memory operand (address in A0) */\nstatic void gen_inc(DisasContext *s1, TCGMemOp ot, int d, int c)\n{\n    if (s1->prefix & PREFIX_LOCK) {\n        tcg_gen_movi_tl(cpu_T0, c > 0 ? 1 : -1);\n        tcg_gen_atomic_add_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                    s1->mem_index, ot | MO_LE);\n    } else {\n        if (d != OR_TMP0) {\n            gen_op_mov_v_reg(ot, cpu_T0, d);\n        } else {\n            gen_op_ld_v(s1, ot, cpu_T0, cpu_A0);\n        }\n        tcg_gen_addi_tl(cpu_T0, cpu_T0, (c > 0 ? 1 : -1));\n        gen_op_st_rm_T0_A0(s1, ot, d);\n    }\n\n    gen_compute_eflags_c(s1, cpu_cc_src);\n    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n    set_cc_op(s1, (c > 0 ? CC_OP_INCB : CC_OP_DECB) + ot);\n}\n\nstatic void gen_shift_flags(DisasContext *s, TCGMemOp ot, TCGv result,\n                            TCGv shm1, TCGv count, bool is_right)\n{\n    TCGv_i32 z32, s32, oldop;\n    TCGv z_tl;\n\n    /* Store the results into the CC variables.  If we know that the\n       variable must be dead, store unconditionally.  Otherwise we'll\n       need to not disrupt the current contents.  */\n    z_tl = tcg_const_tl(0);\n    if (cc_op_live[s->cc_op] & USES_CC_DST) {\n        tcg_gen_movcond_tl(TCG_COND_NE, cpu_cc_dst, count, z_tl,\n                           result, cpu_cc_dst);\n    } else {\n        tcg_gen_mov_tl(cpu_cc_dst, result);\n    }\n    if (cc_op_live[s->cc_op] & USES_CC_SRC) {\n        tcg_gen_movcond_tl(TCG_COND_NE, cpu_cc_src, count, z_tl,\n                           shm1, cpu_cc_src);\n    } else {\n        tcg_gen_mov_tl(cpu_cc_src, shm1);\n    }\n    tcg_temp_free(z_tl);\n\n    /* Get the two potential CC_OP values into temporaries.  */\n    tcg_gen_movi_i32(cpu_tmp2_i32, (is_right ? CC_OP_SARB : CC_OP_SHLB) + ot);\n    if (s->cc_op == CC_OP_DYNAMIC) {\n        oldop = cpu_cc_op;\n    } else {\n        tcg_gen_movi_i32(cpu_tmp3_i32, s->cc_op);\n        oldop = cpu_tmp3_i32;\n    }\n\n    /* Conditionally store the CC_OP value.  */\n    z32 = tcg_const_i32(0);\n    s32 = tcg_temp_new_i32();\n    tcg_gen_trunc_tl_i32(s32, count);\n    tcg_gen_movcond_i32(TCG_COND_NE, cpu_cc_op, s32, z32, cpu_tmp2_i32, oldop);\n    tcg_temp_free_i32(z32);\n    tcg_temp_free_i32(s32);\n\n    /* The CC_OP value is no longer predictable.  */\n    set_cc_op(s, CC_OP_DYNAMIC);\n}\n\nstatic void gen_shift_rm_T1(DisasContext *s, TCGMemOp ot, int op1,\n                            int is_right, int is_arith)\n{\n    target_ulong mask = (ot == MO_64 ? 0x3f : 0x1f);\n\n    /* load */\n    if (op1 == OR_TMP0) {\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    }\n\n    tcg_gen_andi_tl(cpu_T1, cpu_T1, mask);\n    tcg_gen_subi_tl(cpu_tmp0, cpu_T1, 1);\n\n    if (is_right) {\n        if (is_arith) {\n            gen_exts(ot, cpu_T0);\n            tcg_gen_sar_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n            tcg_gen_sar_tl(cpu_T0, cpu_T0, cpu_T1);\n        } else {\n            gen_extu(ot, cpu_T0);\n            tcg_gen_shr_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n            tcg_gen_shr_tl(cpu_T0, cpu_T0, cpu_T1);\n        }\n    } else {\n        tcg_gen_shl_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n        tcg_gen_shl_tl(cpu_T0, cpu_T0, cpu_T1);\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    gen_shift_flags(s, ot, cpu_T0, cpu_tmp0, cpu_T1, is_right);\n}\n\nstatic void gen_shift_rm_im(DisasContext *s, TCGMemOp ot, int op1, int op2,\n                            int is_right, int is_arith)\n{\n    int mask = (ot == MO_64 ? 0x3f : 0x1f);\n\n    /* load */\n    if (op1 == OR_TMP0)\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    else\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n\n    op2 &= mask;\n    if (op2 != 0) {\n        if (is_right) {\n            if (is_arith) {\n                gen_exts(ot, cpu_T0);\n                tcg_gen_sari_tl(cpu_tmp4, cpu_T0, op2 - 1);\n                tcg_gen_sari_tl(cpu_T0, cpu_T0, op2);\n            } else {\n                gen_extu(ot, cpu_T0);\n                tcg_gen_shri_tl(cpu_tmp4, cpu_T0, op2 - 1);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, op2);\n            }\n        } else {\n            tcg_gen_shli_tl(cpu_tmp4, cpu_T0, op2 - 1);\n            tcg_gen_shli_tl(cpu_T0, cpu_T0, op2);\n        }\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    /* update eflags if non zero shift */\n    if (op2 != 0) {\n        tcg_gen_mov_tl(cpu_cc_src, cpu_tmp4);\n        tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n        set_cc_op(s, (is_right ? CC_OP_SARB : CC_OP_SHLB) + ot);\n    }\n}\n\nstatic void gen_rot_rm_T1(DisasContext *s, TCGMemOp ot, int op1, int is_right)\n{\n    target_ulong mask = (ot == MO_64 ? 0x3f : 0x1f);\n    TCGv_i32 t0, t1;\n\n    /* load */\n    if (op1 == OR_TMP0) {\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    }\n\n    tcg_gen_andi_tl(cpu_T1, cpu_T1, mask);\n\n    switch (ot) {\n    case MO_8:\n        /* Replicate the 8-bit input so that a 32-bit rotate works.  */\n        tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n        tcg_gen_muli_tl(cpu_T0, cpu_T0, 0x01010101);\n        goto do_long;\n    case MO_16:\n        /* Replicate the 16-bit input so that a 32-bit rotate works.  */\n        tcg_gen_deposit_tl(cpu_T0, cpu_T0, cpu_T0, 16, 16);\n        goto do_long;\n    do_long:\n#ifdef TARGET_X86_64\n    case MO_32:\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        if (is_right) {\n            tcg_gen_rotr_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n        } else {\n            tcg_gen_rotl_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n        }\n        tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n        break;\n#endif\n    default:\n        if (is_right) {\n            tcg_gen_rotr_tl(cpu_T0, cpu_T0, cpu_T1);\n        } else {\n            tcg_gen_rotl_tl(cpu_T0, cpu_T0, cpu_T1);\n        }\n        break;\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    /* We'll need the flags computed into CC_SRC.  */\n    gen_compute_eflags(s);\n\n    /* The value that was \"rotated out\" is now present at the other end\n       of the word.  Compute C into CC_DST and O into CC_SRC2.  Note that\n       since we've computed the flags into CC_SRC, these variables are\n       currently dead.  */\n    if (is_right) {\n        tcg_gen_shri_tl(cpu_cc_src2, cpu_T0, mask - 1);\n        tcg_gen_shri_tl(cpu_cc_dst, cpu_T0, mask);\n        tcg_gen_andi_tl(cpu_cc_dst, cpu_cc_dst, 1);\n    } else {\n        tcg_gen_shri_tl(cpu_cc_src2, cpu_T0, mask);\n        tcg_gen_andi_tl(cpu_cc_dst, cpu_T0, 1);\n    }\n    tcg_gen_andi_tl(cpu_cc_src2, cpu_cc_src2, 1);\n    tcg_gen_xor_tl(cpu_cc_src2, cpu_cc_src2, cpu_cc_dst);\n\n    /* Now conditionally store the new CC_OP value.  If the shift count\n       is 0 we keep the CC_OP_EFLAGS setting so that only CC_SRC is live.\n       Otherwise reuse CC_OP_ADCOX which have the C and O flags split out\n       exactly as we computed above.  */\n    t0 = tcg_const_i32(0);\n    t1 = tcg_temp_new_i32();\n    tcg_gen_trunc_tl_i32(t1, cpu_T1);\n    tcg_gen_movi_i32(cpu_tmp2_i32, CC_OP_ADCOX); \n    tcg_gen_movi_i32(cpu_tmp3_i32, CC_OP_EFLAGS);\n    tcg_gen_movcond_i32(TCG_COND_NE, cpu_cc_op, t1, t0,\n                        cpu_tmp2_i32, cpu_tmp3_i32);\n    tcg_temp_free_i32(t0);\n    tcg_temp_free_i32(t1);\n\n    /* The CC_OP value is no longer predictable.  */ \n    set_cc_op(s, CC_OP_DYNAMIC);\n}\n\nstatic void gen_rot_rm_im(DisasContext *s, TCGMemOp ot, int op1, int op2,\n                          int is_right)\n{\n    int mask = (ot == MO_64 ? 0x3f : 0x1f);\n    int shift;\n\n    /* load */\n    if (op1 == OR_TMP0) {\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    }\n\n    op2 &= mask;\n    if (op2 != 0) {\n        switch (ot) {\n#ifdef TARGET_X86_64\n        case MO_32:\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n            if (is_right) {\n                tcg_gen_rotri_i32(cpu_tmp2_i32, cpu_tmp2_i32, op2);\n            } else {\n                tcg_gen_rotli_i32(cpu_tmp2_i32, cpu_tmp2_i32, op2);\n            }\n            tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n            break;\n#endif\n        default:\n            if (is_right) {\n                tcg_gen_rotri_tl(cpu_T0, cpu_T0, op2);\n            } else {\n                tcg_gen_rotli_tl(cpu_T0, cpu_T0, op2);\n            }\n            break;\n        case MO_8:\n            mask = 7;\n            goto do_shifts;\n        case MO_16:\n            mask = 15;\n        do_shifts:\n            shift = op2 & mask;\n            if (is_right) {\n                shift = mask + 1 - shift;\n            }\n            gen_extu(ot, cpu_T0);\n            tcg_gen_shli_tl(cpu_tmp0, cpu_T0, shift);\n            tcg_gen_shri_tl(cpu_T0, cpu_T0, mask + 1 - shift);\n            tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_tmp0);\n            break;\n        }\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    if (op2 != 0) {\n        /* Compute the flags into CC_SRC.  */\n        gen_compute_eflags(s);\n\n        /* The value that was \"rotated out\" is now present at the other end\n           of the word.  Compute C into CC_DST and O into CC_SRC2.  Note that\n           since we've computed the flags into CC_SRC, these variables are\n           currently dead.  */\n        if (is_right) {\n            tcg_gen_shri_tl(cpu_cc_src2, cpu_T0, mask - 1);\n            tcg_gen_shri_tl(cpu_cc_dst, cpu_T0, mask);\n            tcg_gen_andi_tl(cpu_cc_dst, cpu_cc_dst, 1);\n        } else {\n            tcg_gen_shri_tl(cpu_cc_src2, cpu_T0, mask);\n            tcg_gen_andi_tl(cpu_cc_dst, cpu_T0, 1);\n        }\n        tcg_gen_andi_tl(cpu_cc_src2, cpu_cc_src2, 1);\n        tcg_gen_xor_tl(cpu_cc_src2, cpu_cc_src2, cpu_cc_dst);\n        set_cc_op(s, CC_OP_ADCOX);\n    }\n}\n\n/* XXX: add faster immediate = 1 case */\nstatic void gen_rotc_rm_T1(DisasContext *s, TCGMemOp ot, int op1,\n                           int is_right)\n{\n    gen_compute_eflags(s);\n    assert(s->cc_op == CC_OP_EFLAGS);\n\n    /* load */\n    if (op1 == OR_TMP0)\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    else\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    \n    if (is_right) {\n        switch (ot) {\n        case MO_8:\n            gen_helper_rcrb(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n        case MO_16:\n            gen_helper_rcrw(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n        case MO_32:\n            gen_helper_rcrl(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_helper_rcrq(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n#endif\n        default:\n            tcg_abort();\n        }\n    } else {\n        switch (ot) {\n        case MO_8:\n            gen_helper_rclb(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n        case MO_16:\n            gen_helper_rclw(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n        case MO_32:\n            gen_helper_rcll(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_helper_rclq(cpu_T0, cpu_env, cpu_T0, cpu_T1);\n            break;\n#endif\n        default:\n            tcg_abort();\n        }\n    }\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n}\n\n/* XXX: add faster immediate case */\nstatic void gen_shiftd_rm_T1(DisasContext *s, TCGMemOp ot, int op1,\n                             bool is_right, TCGv count_in)\n{\n    target_ulong mask = (ot == MO_64 ? 63 : 31);\n    TCGv count;\n\n    /* load */\n    if (op1 == OR_TMP0) {\n        gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n    } else {\n        gen_op_mov_v_reg(ot, cpu_T0, op1);\n    }\n\n    count = tcg_temp_new();\n    tcg_gen_andi_tl(count, count_in, mask);\n\n    switch (ot) {\n    case MO_16:\n        /* Note: we implement the Intel behaviour for shift count > 16.\n           This means \"shrdw C, B, A\" shifts A:B:A >> C.  Build the B:A\n           portion by constructing it as a 32-bit value.  */\n        if (is_right) {\n            tcg_gen_deposit_tl(cpu_tmp0, cpu_T0, cpu_T1, 16, 16);\n            tcg_gen_mov_tl(cpu_T1, cpu_T0);\n            tcg_gen_mov_tl(cpu_T0, cpu_tmp0);\n        } else {\n            tcg_gen_deposit_tl(cpu_T1, cpu_T0, cpu_T1, 16, 16);\n        }\n        /* FALLTHRU */\n#ifdef TARGET_X86_64\n    case MO_32:\n        /* Concatenate the two 32-bit values and use a 64-bit shift.  */\n        tcg_gen_subi_tl(cpu_tmp0, count, 1);\n        if (is_right) {\n            tcg_gen_concat_tl_i64(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_shr_i64(cpu_tmp0, cpu_T0, cpu_tmp0);\n            tcg_gen_shr_i64(cpu_T0, cpu_T0, count);\n        } else {\n            tcg_gen_concat_tl_i64(cpu_T0, cpu_T1, cpu_T0);\n            tcg_gen_shl_i64(cpu_tmp0, cpu_T0, cpu_tmp0);\n            tcg_gen_shl_i64(cpu_T0, cpu_T0, count);\n            tcg_gen_shri_i64(cpu_tmp0, cpu_tmp0, 32);\n            tcg_gen_shri_i64(cpu_T0, cpu_T0, 32);\n        }\n        break;\n#endif\n    default:\n        tcg_gen_subi_tl(cpu_tmp0, count, 1);\n        if (is_right) {\n            tcg_gen_shr_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n\n            tcg_gen_subfi_tl(cpu_tmp4, mask + 1, count);\n            tcg_gen_shr_tl(cpu_T0, cpu_T0, count);\n            tcg_gen_shl_tl(cpu_T1, cpu_T1, cpu_tmp4);\n        } else {\n            tcg_gen_shl_tl(cpu_tmp0, cpu_T0, cpu_tmp0);\n            if (ot == MO_16) {\n                /* Only needed if count > 16, for Intel behaviour.  */\n                tcg_gen_subfi_tl(cpu_tmp4, 33, count);\n                tcg_gen_shr_tl(cpu_tmp4, cpu_T1, cpu_tmp4);\n                tcg_gen_or_tl(cpu_tmp0, cpu_tmp0, cpu_tmp4);\n            }\n\n            tcg_gen_subfi_tl(cpu_tmp4, mask + 1, count);\n            tcg_gen_shl_tl(cpu_T0, cpu_T0, count);\n            tcg_gen_shr_tl(cpu_T1, cpu_T1, cpu_tmp4);\n        }\n        tcg_gen_movi_tl(cpu_tmp4, 0);\n        tcg_gen_movcond_tl(TCG_COND_EQ, cpu_T1, count, cpu_tmp4,\n                           cpu_tmp4, cpu_T1);\n        tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_T1);\n        break;\n    }\n\n    /* store */\n    gen_op_st_rm_T0_A0(s, ot, op1);\n\n    gen_shift_flags(s, ot, cpu_T0, cpu_tmp0, count, is_right);\n    tcg_temp_free(count);\n}\n\nstatic void gen_shift(DisasContext *s1, int op, TCGMemOp ot, int d, int s)\n{\n    if (s != OR_TMP1)\n        gen_op_mov_v_reg(ot, cpu_T1, s);\n    switch(op) {\n    case OP_ROL:\n        gen_rot_rm_T1(s1, ot, d, 0);\n        break;\n    case OP_ROR:\n        gen_rot_rm_T1(s1, ot, d, 1);\n        break;\n    case OP_SHL:\n    case OP_SHL1:\n        gen_shift_rm_T1(s1, ot, d, 0, 0);\n        break;\n    case OP_SHR:\n        gen_shift_rm_T1(s1, ot, d, 1, 0);\n        break;\n    case OP_SAR:\n        gen_shift_rm_T1(s1, ot, d, 1, 1);\n        break;\n    case OP_RCL:\n        gen_rotc_rm_T1(s1, ot, d, 0);\n        break;\n    case OP_RCR:\n        gen_rotc_rm_T1(s1, ot, d, 1);\n        break;\n    }\n}\n\nstatic void gen_shifti(DisasContext *s1, int op, TCGMemOp ot, int d, int c)\n{\n    switch(op) {\n    case OP_ROL:\n        gen_rot_rm_im(s1, ot, d, c, 0);\n        break;\n    case OP_ROR:\n        gen_rot_rm_im(s1, ot, d, c, 1);\n        break;\n    case OP_SHL:\n    case OP_SHL1:\n        gen_shift_rm_im(s1, ot, d, c, 0, 0);\n        break;\n    case OP_SHR:\n        gen_shift_rm_im(s1, ot, d, c, 1, 0);\n        break;\n    case OP_SAR:\n        gen_shift_rm_im(s1, ot, d, c, 1, 1);\n        break;\n    default:\n        /* currently not optimized */\n        tcg_gen_movi_tl(cpu_T1, c);\n        gen_shift(s1, op, ot, d, OR_TMP1);\n        break;\n    }\n}\n\n/* Decompose an address.  */\n\ntypedef struct AddressParts {\n    int def_seg;\n    int base;\n    int index;\n    int scale;\n    target_long disp;\n} AddressParts;\n\nstatic AddressParts gen_lea_modrm_0(CPUX86State *env, DisasContext *s,\n                                    int modrm)\n{\n    int def_seg, base, index, scale, mod, rm;\n    target_long disp;\n    bool havesib;\n\n    def_seg = R_DS;\n    index = -1;\n    scale = 0;\n    disp = 0;\n\n    mod = (modrm >> 6) & 3;\n    rm = modrm & 7;\n    base = rm | REX_B(s);\n\n    if (mod == 3) {\n        /* Normally filtered out earlier, but including this path\n           simplifies multi-byte nop, as well as bndcl, bndcu, bndcn.  */\n        goto done;\n    }\n\n    switch (s->aflag) {\n    case MO_64:\n    case MO_32:\n        havesib = 0;\n        if (rm == 4) {\n            int code = cpu_ldub_code(env, s->pc++);\n            scale = (code >> 6) & 3;\n            index = ((code >> 3) & 7) | REX_X(s);\n            if (index == 4) {\n                index = -1;  /* no index */\n            }\n            base = (code & 7) | REX_B(s);\n            havesib = 1;\n        }\n\n        switch (mod) {\n        case 0:\n            if ((base & 7) == 5) {\n                base = -1;\n                disp = (int32_t)cpu_ldl_code(env, s->pc);\n                s->pc += 4;\n                if (CODE64(s) && !havesib) {\n                    base = -2;\n                    disp += s->pc + s->rip_offset;\n                }\n            }\n            break;\n        case 1:\n            disp = (int8_t)cpu_ldub_code(env, s->pc++);\n            break;\n        default:\n        case 2:\n            disp = (int32_t)cpu_ldl_code(env, s->pc);\n            s->pc += 4;\n            break;\n        }\n\n        /* For correct popl handling with esp.  */\n        if (base == R_ESP && s->popl_esp_hack) {\n            disp += s->popl_esp_hack;\n        }\n        if (base == R_EBP || base == R_ESP) {\n            def_seg = R_SS;\n        }\n        break;\n\n    case MO_16:\n        if (mod == 0) {\n            if (rm == 6) {\n                base = -1;\n                disp = cpu_lduw_code(env, s->pc);\n                s->pc += 2;\n                break;\n            }\n        } else if (mod == 1) {\n            disp = (int8_t)cpu_ldub_code(env, s->pc++);\n        } else {\n            disp = (int16_t)cpu_lduw_code(env, s->pc);\n            s->pc += 2;\n        }\n\n        switch (rm) {\n        case 0:\n            base = R_EBX;\n            index = R_ESI;\n            break;\n        case 1:\n            base = R_EBX;\n            index = R_EDI;\n            break;\n        case 2:\n            base = R_EBP;\n            index = R_ESI;\n            def_seg = R_SS;\n            break;\n        case 3:\n            base = R_EBP;\n            index = R_EDI;\n            def_seg = R_SS;\n            break;\n        case 4:\n            base = R_ESI;\n            break;\n        case 5:\n            base = R_EDI;\n            break;\n        case 6:\n            base = R_EBP;\n            def_seg = R_SS;\n            break;\n        default:\n        case 7:\n            base = R_EBX;\n            break;\n        }\n        break;\n\n    default:\n        tcg_abort();\n    }\n\n done:\n    return (AddressParts){ def_seg, base, index, scale, disp };\n}\n\n/* Compute the address, with a minimum number of TCG ops.  */\nstatic TCGv gen_lea_modrm_1(AddressParts a)\n{\n    TCGv ea;\n\n    TCGV_UNUSED(ea);\n    if (a.index >= 0) {\n        if (a.scale == 0) {\n            ea = cpu_regs[a.index];\n        } else {\n            tcg_gen_shli_tl(cpu_A0, cpu_regs[a.index], a.scale);\n            ea = cpu_A0;\n        }\n        if (a.base >= 0) {\n            tcg_gen_add_tl(cpu_A0, ea, cpu_regs[a.base]);\n            ea = cpu_A0;\n        }\n    } else if (a.base >= 0) {\n        ea = cpu_regs[a.base];\n    }\n    if (TCGV_IS_UNUSED(ea)) {\n        tcg_gen_movi_tl(cpu_A0, a.disp);\n        ea = cpu_A0;\n    } else if (a.disp != 0) {\n        tcg_gen_addi_tl(cpu_A0, ea, a.disp);\n        ea = cpu_A0;\n    }\n\n    return ea;\n}\n\nstatic void gen_lea_modrm(CPUX86State *env, DisasContext *s, int modrm)\n{\n    AddressParts a = gen_lea_modrm_0(env, s, modrm);\n    TCGv ea = gen_lea_modrm_1(a);\n    gen_lea_v_seg(s, s->aflag, ea, a.def_seg, s->override);\n}\n\nstatic void gen_nop_modrm(CPUX86State *env, DisasContext *s, int modrm)\n{\n    (void)gen_lea_modrm_0(env, s, modrm);\n}\n\n/* Used for BNDCL, BNDCU, BNDCN.  */\nstatic void gen_bndck(CPUX86State *env, DisasContext *s, int modrm,\n                      TCGCond cond, TCGv_i64 bndv)\n{\n    TCGv ea = gen_lea_modrm_1(gen_lea_modrm_0(env, s, modrm));\n\n    tcg_gen_extu_tl_i64(cpu_tmp1_i64, ea);\n    if (!CODE64(s)) {\n        tcg_gen_ext32u_i64(cpu_tmp1_i64, cpu_tmp1_i64);\n    }\n    tcg_gen_setcond_i64(cond, cpu_tmp1_i64, cpu_tmp1_i64, bndv);\n    tcg_gen_extrl_i64_i32(cpu_tmp2_i32, cpu_tmp1_i64);\n    gen_helper_bndck(cpu_env, cpu_tmp2_i32);\n}\n\n/* used for LEA and MOV AX, mem */\nstatic void gen_add_A0_ds_seg(DisasContext *s)\n{\n    gen_lea_v_seg(s, s->aflag, cpu_A0, R_DS, s->override);\n}\n\n/* generate modrm memory load or store of 'reg'. TMP0 is used if reg ==\n   OR_TMP0 */\nstatic void gen_ldst_modrm(CPUX86State *env, DisasContext *s, int modrm,\n                           TCGMemOp ot, int reg, int is_store)\n{\n    int mod, rm;\n\n    mod = (modrm >> 6) & 3;\n    rm = (modrm & 7) | REX_B(s);\n    if (mod == 3) {\n        if (is_store) {\n            if (reg != OR_TMP0)\n                gen_op_mov_v_reg(ot, cpu_T0, reg);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n            if (reg != OR_TMP0)\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n        }\n    } else {\n        gen_lea_modrm(env, s, modrm);\n        if (is_store) {\n            if (reg != OR_TMP0)\n                gen_op_mov_v_reg(ot, cpu_T0, reg);\n            gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            if (reg != OR_TMP0)\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n        }\n    }\n}\n\nstatic inline uint32_t insn_get(CPUX86State *env, DisasContext *s, TCGMemOp ot)\n{\n    uint32_t ret;\n\n    switch (ot) {\n    case MO_8:\n        ret = cpu_ldub_code(env, s->pc);\n        s->pc++;\n        break;\n    case MO_16:\n        ret = cpu_lduw_code(env, s->pc);\n        s->pc += 2;\n        break;\n    case MO_32:\n#ifdef TARGET_X86_64\n    case MO_64:\n#endif\n        ret = cpu_ldl_code(env, s->pc);\n        s->pc += 4;\n        break;\n    default:\n        tcg_abort();\n    }\n    return ret;\n}\n\nstatic inline int insn_const_size(TCGMemOp ot)\n{\n    if (ot <= MO_32) {\n        return 1 << ot;\n    } else {\n        return 4;\n    }\n}\n\nstatic inline bool use_goto_tb(DisasContext *s, target_ulong pc)\n{\n#ifndef CONFIG_USER_ONLY\n    return (pc & TARGET_PAGE_MASK) == (s->tb->pc & TARGET_PAGE_MASK) ||\n           (pc & TARGET_PAGE_MASK) == (s->pc_start & TARGET_PAGE_MASK);\n#else\n    return true;\n#endif\n}\n\nstatic inline void gen_goto_tb(DisasContext *s, int tb_num, target_ulong eip)\n{\n    target_ulong pc = s->cs_base + eip;\n\n    if (use_goto_tb(s, pc))  {\n        /* jump to same page: we can use a direct jump */\n        tcg_gen_goto_tb(tb_num);\n        gen_jmp_im(eip);\n        tcg_gen_exit_tb((uintptr_t)s->tb + tb_num);\n    } else {\n        /* jump to another page: currently not optimized */\n        gen_jmp_im(eip);\n        gen_eob(s);\n    }\n}\n\nstatic inline void gen_jcc(DisasContext *s, int b,\n                           target_ulong val, target_ulong next_eip)\n{\n    TCGLabel *l1, *l2;\n\n    if (s->jmp_opt) {\n        l1 = gen_new_label();\n        gen_jcc1(s, b, l1);\n\n        gen_goto_tb(s, 0, next_eip);\n\n        gen_set_label(l1);\n        gen_goto_tb(s, 1, val);\n        s->is_jmp = DISAS_TB_JUMP;\n    } else {\n        l1 = gen_new_label();\n        l2 = gen_new_label();\n        gen_jcc1(s, b, l1);\n\n        gen_jmp_im(next_eip);\n        tcg_gen_br(l2);\n\n        gen_set_label(l1);\n        gen_jmp_im(val);\n        gen_set_label(l2);\n        gen_eob(s);\n    }\n}\n\nstatic void gen_cmovcc1(CPUX86State *env, DisasContext *s, TCGMemOp ot, int b,\n                        int modrm, int reg)\n{\n    CCPrepare cc;\n\n    gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n\n    cc = gen_prepare_cc(s, b, cpu_T1);\n    if (cc.mask != -1) {\n        TCGv t0 = tcg_temp_new();\n        tcg_gen_andi_tl(t0, cc.reg, cc.mask);\n        cc.reg = t0;\n    }\n    if (!cc.use_reg2) {\n        cc.reg2 = tcg_const_tl(cc.imm);\n    }\n\n    tcg_gen_movcond_tl(cc.cond, cpu_T0, cc.reg, cc.reg2,\n                       cpu_T0, cpu_regs[reg]);\n    gen_op_mov_reg_v(ot, reg, cpu_T0);\n\n    if (cc.mask != -1) {\n        tcg_temp_free(cc.reg);\n    }\n    if (!cc.use_reg2) {\n        tcg_temp_free(cc.reg2);\n    }\n}\n\nstatic inline void gen_op_movl_T0_seg(int seg_reg)\n{\n    tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                     offsetof(CPUX86State,segs[seg_reg].selector));\n}\n\nstatic inline void gen_op_movl_seg_T0_vm(int seg_reg)\n{\n    tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n    tcg_gen_st32_tl(cpu_T0, cpu_env,\n                    offsetof(CPUX86State,segs[seg_reg].selector));\n    tcg_gen_shli_tl(cpu_seg_base[seg_reg], cpu_T0, 4);\n}\n\n/* move T0 to seg_reg and compute if the CPU state may change. Never\n   call this function with seg_reg == R_CS */\nstatic void gen_movl_seg_T0(DisasContext *s, int seg_reg)\n{\n    if (s->pe && !s->vm86) {\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        gen_helper_load_seg(cpu_env, tcg_const_i32(seg_reg), cpu_tmp2_i32);\n        /* abort translation because the addseg value may change or\n           because ss32 may change. For R_SS, translation must always\n           stop as a special handling must be done to disable hardware\n           interrupts for the next instruction */\n        if (seg_reg == R_SS || (s->code32 && seg_reg < R_FS))\n            s->is_jmp = DISAS_TB_JUMP;\n    } else {\n        gen_op_movl_seg_T0_vm(seg_reg);\n        if (seg_reg == R_SS)\n            s->is_jmp = DISAS_TB_JUMP;\n    }\n}\n\nstatic inline int svm_is_rep(int prefixes)\n{\n    return ((prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) ? 8 : 0);\n}\n\nstatic inline void\ngen_svm_check_intercept_param(DisasContext *s, target_ulong pc_start,\n                              uint32_t type, uint64_t param)\n{\n    /* no SVM activated; fast case */\n    if (likely(!(s->flags & HF_SVMI_MASK)))\n        return;\n    gen_update_cc_op(s);\n    gen_jmp_im(pc_start - s->cs_base);\n    gen_helper_svm_check_intercept_param(cpu_env, tcg_const_i32(type),\n                                         tcg_const_i64(param));\n}\n\nstatic inline void\ngen_svm_check_intercept(DisasContext *s, target_ulong pc_start, uint64_t type)\n{\n    gen_svm_check_intercept_param(s, pc_start, type, 0);\n}\n\nstatic inline void gen_stack_update(DisasContext *s, int addend)\n{\n    gen_op_add_reg_im(mo_stacksize(s), R_ESP, addend);\n}\n\n/* Generate a push. It depends on ss32, addseg and dflag.  */\nstatic void gen_push_v(DisasContext *s, TCGv val)\n{\n    TCGMemOp d_ot = mo_pushpop(s, s->dflag);\n    TCGMemOp a_ot = mo_stacksize(s);\n    int size = 1 << d_ot;\n    TCGv new_esp = cpu_A0;\n\n    tcg_gen_subi_tl(cpu_A0, cpu_regs[R_ESP], size);\n\n    if (!CODE64(s)) {\n        if (s->addseg) {\n            new_esp = cpu_tmp4;\n            tcg_gen_mov_tl(new_esp, cpu_A0);\n        }\n        gen_lea_v_seg(s, a_ot, cpu_A0, R_SS, -1);\n    }\n\n    gen_op_st_v(s, d_ot, val, cpu_A0);\n    gen_op_mov_reg_v(a_ot, R_ESP, new_esp);\n}\n\n/* two step pop is necessary for precise exceptions */\nstatic TCGMemOp gen_pop_T0(DisasContext *s)\n{\n    TCGMemOp d_ot = mo_pushpop(s, s->dflag);\n\n    gen_lea_v_seg(s, mo_stacksize(s), cpu_regs[R_ESP], R_SS, -1);\n    gen_op_ld_v(s, d_ot, cpu_T0, cpu_A0);\n\n    return d_ot;\n}\n\nstatic inline void gen_pop_update(DisasContext *s, TCGMemOp ot)\n{\n    gen_stack_update(s, 1 << ot);\n}\n\nstatic inline void gen_stack_A0(DisasContext *s)\n{\n    gen_lea_v_seg(s, s->ss32 ? MO_32 : MO_16, cpu_regs[R_ESP], R_SS, -1);\n}\n\nstatic void gen_pusha(DisasContext *s)\n{\n    TCGMemOp s_ot = s->ss32 ? MO_32 : MO_16;\n    TCGMemOp d_ot = s->dflag;\n    int size = 1 << d_ot;\n    int i;\n\n    for (i = 0; i < 8; i++) {\n        tcg_gen_addi_tl(cpu_A0, cpu_regs[R_ESP], (i - 8) * size);\n        gen_lea_v_seg(s, s_ot, cpu_A0, R_SS, -1);\n        gen_op_st_v(s, d_ot, cpu_regs[7 - i], cpu_A0);\n    }\n\n    gen_stack_update(s, -8 * size);\n}\n\nstatic void gen_popa(DisasContext *s)\n{\n    TCGMemOp s_ot = s->ss32 ? MO_32 : MO_16;\n    TCGMemOp d_ot = s->dflag;\n    int size = 1 << d_ot;\n    int i;\n\n    for (i = 0; i < 8; i++) {\n        /* ESP is not reloaded */\n        if (7 - i == R_ESP) {\n            continue;\n        }\n        tcg_gen_addi_tl(cpu_A0, cpu_regs[R_ESP], i * size);\n        gen_lea_v_seg(s, s_ot, cpu_A0, R_SS, -1);\n        gen_op_ld_v(s, d_ot, cpu_T0, cpu_A0);\n        gen_op_mov_reg_v(d_ot, 7 - i, cpu_T0);\n    }\n\n    gen_stack_update(s, 8 * size);\n}\n\nstatic void gen_enter(DisasContext *s, int esp_addend, int level)\n{\n    TCGMemOp d_ot = mo_pushpop(s, s->dflag);\n    TCGMemOp a_ot = CODE64(s) ? MO_64 : s->ss32 ? MO_32 : MO_16;\n    int size = 1 << d_ot;\n\n    /* Push BP; compute FrameTemp into T1.  */\n    tcg_gen_subi_tl(cpu_T1, cpu_regs[R_ESP], size);\n    gen_lea_v_seg(s, a_ot, cpu_T1, R_SS, -1);\n    gen_op_st_v(s, d_ot, cpu_regs[R_EBP], cpu_A0);\n\n    level &= 31;\n    if (level != 0) {\n        int i;\n\n        /* Copy level-1 pointers from the previous frame.  */\n        for (i = 1; i < level; ++i) {\n            tcg_gen_subi_tl(cpu_A0, cpu_regs[R_EBP], size * i);\n            gen_lea_v_seg(s, a_ot, cpu_A0, R_SS, -1);\n            gen_op_ld_v(s, d_ot, cpu_tmp0, cpu_A0);\n\n            tcg_gen_subi_tl(cpu_A0, cpu_T1, size * i);\n            gen_lea_v_seg(s, a_ot, cpu_A0, R_SS, -1);\n            gen_op_st_v(s, d_ot, cpu_tmp0, cpu_A0);\n        }\n\n        /* Push the current FrameTemp as the last level.  */\n        tcg_gen_subi_tl(cpu_A0, cpu_T1, size * level);\n        gen_lea_v_seg(s, a_ot, cpu_A0, R_SS, -1);\n        gen_op_st_v(s, d_ot, cpu_T1, cpu_A0);\n    }\n\n    /* Copy the FrameTemp value to EBP.  */\n    gen_op_mov_reg_v(a_ot, R_EBP, cpu_T1);\n\n    /* Compute the final value of ESP.  */\n    tcg_gen_subi_tl(cpu_T1, cpu_T1, esp_addend + size * level);\n    gen_op_mov_reg_v(a_ot, R_ESP, cpu_T1);\n}\n\nstatic void gen_leave(DisasContext *s)\n{\n    TCGMemOp d_ot = mo_pushpop(s, s->dflag);\n    TCGMemOp a_ot = mo_stacksize(s);\n\n    gen_lea_v_seg(s, a_ot, cpu_regs[R_EBP], R_SS, -1);\n    gen_op_ld_v(s, d_ot, cpu_T0, cpu_A0);\n\n    tcg_gen_addi_tl(cpu_T1, cpu_regs[R_EBP], 1 << d_ot);\n\n    gen_op_mov_reg_v(d_ot, R_EBP, cpu_T0);\n    gen_op_mov_reg_v(a_ot, R_ESP, cpu_T1);\n}\n\nstatic void gen_exception(DisasContext *s, int trapno, target_ulong cur_eip)\n{\n    gen_update_cc_op(s);\n    gen_jmp_im(cur_eip);\n    gen_helper_raise_exception(cpu_env, tcg_const_i32(trapno));\n    s->is_jmp = DISAS_TB_JUMP;\n}\n\n/* Generate #UD for the current instruction.  The assumption here is that\n   the instruction is known, but it isn't allowed in the current cpu mode.  */\nstatic void gen_illegal_opcode(DisasContext *s)\n{\n    gen_exception(s, EXCP06_ILLOP, s->pc_start - s->cs_base);\n}\n\n/* Similarly, except that the assumption here is that we don't decode\n   the instruction at all -- either a missing opcode, an unimplemented\n   feature, or just a bogus instruction stream.  */\nstatic void gen_unknown_opcode(CPUX86State *env, DisasContext *s)\n{\n    gen_illegal_opcode(s);\n\n    if (qemu_loglevel_mask(LOG_UNIMP)) {\n        target_ulong pc = s->pc_start, end = s->pc;\n        qemu_log_lock();\n        qemu_log(\"ILLOPC: \" TARGET_FMT_lx \":\", pc);\n        for (; pc < end; ++pc) {\n            qemu_log(\" %02x\", cpu_ldub_code(env, pc));\n        }\n        qemu_log(\"\\n\");\n        qemu_log_unlock();\n    }\n}\n\n/* an interrupt is different from an exception because of the\n   privilege checks */\nstatic void gen_interrupt(DisasContext *s, int intno,\n                          target_ulong cur_eip, target_ulong next_eip)\n{\n    gen_update_cc_op(s);\n    gen_jmp_im(cur_eip);\n    gen_helper_raise_interrupt(cpu_env, tcg_const_i32(intno),\n                               tcg_const_i32(next_eip - cur_eip));\n    s->is_jmp = DISAS_TB_JUMP;\n}\n\nstatic void gen_debug(DisasContext *s, target_ulong cur_eip)\n{\n    gen_update_cc_op(s);\n    gen_jmp_im(cur_eip);\n    gen_helper_debug(cpu_env);\n    s->is_jmp = DISAS_TB_JUMP;\n}\n\nstatic void gen_set_hflag(DisasContext *s, uint32_t mask)\n{\n    if ((s->flags & mask) == 0) {\n        TCGv_i32 t = tcg_temp_new_i32();\n        tcg_gen_ld_i32(t, cpu_env, offsetof(CPUX86State, hflags));\n        tcg_gen_ori_i32(t, t, mask);\n        tcg_gen_st_i32(t, cpu_env, offsetof(CPUX86State, hflags));\n        tcg_temp_free_i32(t);\n        s->flags |= mask;\n    }\n}\n\nstatic void gen_reset_hflag(DisasContext *s, uint32_t mask)\n{\n    if (s->flags & mask) {\n        TCGv_i32 t = tcg_temp_new_i32();\n        tcg_gen_ld_i32(t, cpu_env, offsetof(CPUX86State, hflags));\n        tcg_gen_andi_i32(t, t, ~mask);\n        tcg_gen_st_i32(t, cpu_env, offsetof(CPUX86State, hflags));\n        tcg_temp_free_i32(t);\n        s->flags &= ~mask;\n    }\n}\n\n/* Clear BND registers during legacy branches.  */\nstatic void gen_bnd_jmp(DisasContext *s)\n{\n    /* Clear the registers only if BND prefix is missing, MPX is enabled,\n       and if the BNDREGs are known to be in use (non-zero) already.\n       The helper itself will check BNDPRESERVE at runtime.  */\n    if ((s->prefix & PREFIX_REPNZ) == 0\n        && (s->flags & HF_MPX_EN_MASK) != 0\n        && (s->flags & HF_MPX_IU_MASK) != 0) {\n        gen_helper_bnd_jmp(cpu_env);\n    }\n}\n\n/* Generate an end of block. Trace exception is also generated if needed.\n   If INHIBIT, set HF_INHIBIT_IRQ_MASK if it isn't already set.\n   If RECHECK_TF, emit a rechecking helper for #DB, ignoring the state of\n   S->TF.  This is used by the syscall/sysret insns.  */\nstatic void gen_eob_worker(DisasContext *s, bool inhibit, bool recheck_tf)\n{\n    gen_update_cc_op(s);\n\n    /* If several instructions disable interrupts, only the first does it.  */\n    if (inhibit && !(s->flags & HF_INHIBIT_IRQ_MASK)) {\n        gen_set_hflag(s, HF_INHIBIT_IRQ_MASK);\n    } else {\n        gen_reset_hflag(s, HF_INHIBIT_IRQ_MASK);\n    }\n\n    if (s->tb->flags & HF_RF_MASK) {\n        gen_helper_reset_rf(cpu_env);\n    }\n    if (s->singlestep_enabled) {\n        gen_helper_debug(cpu_env);\n    } else if (recheck_tf) {\n        gen_helper_rechecking_single_step(cpu_env);\n        tcg_gen_exit_tb(0);\n    } else if (s->tf) {\n        gen_helper_single_step(cpu_env);\n    } else {\n        tcg_gen_exit_tb(0);\n    }\n    s->is_jmp = DISAS_TB_JUMP;\n}\n\n/* End of block.\n   If INHIBIT, set HF_INHIBIT_IRQ_MASK if it isn't already set.  */\nstatic void gen_eob_inhibit_irq(DisasContext *s, bool inhibit)\n{\n    gen_eob_worker(s, inhibit, false);\n}\n\n/* End of block, resetting the inhibit irq flag.  */\nstatic void gen_eob(DisasContext *s)\n{\n    gen_eob_worker(s, false, false);\n}\n\n/* generate a jump to eip. No segment change must happen before as a\n   direct call to the next block may occur */\nstatic void gen_jmp_tb(DisasContext *s, target_ulong eip, int tb_num)\n{\n    gen_update_cc_op(s);\n    set_cc_op(s, CC_OP_DYNAMIC);\n    if (s->jmp_opt) {\n        gen_goto_tb(s, tb_num, eip);\n        s->is_jmp = DISAS_TB_JUMP;\n    } else {\n        gen_jmp_im(eip);\n        gen_eob(s);\n    }\n}\n\nstatic void gen_jmp(DisasContext *s, target_ulong eip)\n{\n    gen_jmp_tb(s, eip, 0);\n}\n\nstatic inline void gen_ldq_env_A0(DisasContext *s, int offset)\n{\n    tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, offset);\n}\n\nstatic inline void gen_stq_env_A0(DisasContext *s, int offset)\n{\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, offset);\n    tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n}\n\nstatic inline void gen_ldo_env_A0(DisasContext *s, int offset)\n{\n    int mem_index = s->mem_index;\n    tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0, mem_index, MO_LEQ);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, offset + offsetof(ZMMReg, ZMM_Q(0)));\n    tcg_gen_addi_tl(cpu_tmp0, cpu_A0, 8);\n    tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_tmp0, mem_index, MO_LEQ);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, offset + offsetof(ZMMReg, ZMM_Q(1)));\n}\n\nstatic inline void gen_sto_env_A0(DisasContext *s, int offset)\n{\n    int mem_index = s->mem_index;\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, offset + offsetof(ZMMReg, ZMM_Q(0)));\n    tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0, mem_index, MO_LEQ);\n    tcg_gen_addi_tl(cpu_tmp0, cpu_A0, 8);\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, offset + offsetof(ZMMReg, ZMM_Q(1)));\n    tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_tmp0, mem_index, MO_LEQ);\n}\n\nstatic inline void gen_op_movo(int d_offset, int s_offset)\n{\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, s_offset + offsetof(ZMMReg, ZMM_Q(0)));\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, d_offset + offsetof(ZMMReg, ZMM_Q(0)));\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, s_offset + offsetof(ZMMReg, ZMM_Q(1)));\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, d_offset + offsetof(ZMMReg, ZMM_Q(1)));\n}\n\nstatic inline void gen_op_movq(int d_offset, int s_offset)\n{\n    tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, s_offset);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, d_offset);\n}\n\nstatic inline void gen_op_movl(int d_offset, int s_offset)\n{\n    tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env, s_offset);\n    tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, d_offset);\n}\n\nstatic inline void gen_op_movq_env_0(int d_offset)\n{\n    tcg_gen_movi_i64(cpu_tmp1_i64, 0);\n    tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, d_offset);\n}\n\ntypedef void (*SSEFunc_i_ep)(TCGv_i32 val, TCGv_ptr env, TCGv_ptr reg);\ntypedef void (*SSEFunc_l_ep)(TCGv_i64 val, TCGv_ptr env, TCGv_ptr reg);\ntypedef void (*SSEFunc_0_epi)(TCGv_ptr env, TCGv_ptr reg, TCGv_i32 val);\ntypedef void (*SSEFunc_0_epl)(TCGv_ptr env, TCGv_ptr reg, TCGv_i64 val);\ntypedef void (*SSEFunc_0_epp)(TCGv_ptr env, TCGv_ptr reg_a, TCGv_ptr reg_b);\ntypedef void (*SSEFunc_0_eppi)(TCGv_ptr env, TCGv_ptr reg_a, TCGv_ptr reg_b,\n                               TCGv_i32 val);\ntypedef void (*SSEFunc_0_ppi)(TCGv_ptr reg_a, TCGv_ptr reg_b, TCGv_i32 val);\ntypedef void (*SSEFunc_0_eppt)(TCGv_ptr env, TCGv_ptr reg_a, TCGv_ptr reg_b,\n                               TCGv val);\n\n#define SSE_SPECIAL ((void *)1)\n#define SSE_DUMMY ((void *)2)\n\n#define MMX_OP2(x) { gen_helper_ ## x ## _mmx, gen_helper_ ## x ## _xmm }\n#define SSE_FOP(x) { gen_helper_ ## x ## ps, gen_helper_ ## x ## pd, \\\n                     gen_helper_ ## x ## ss, gen_helper_ ## x ## sd, }\n\nstatic const SSEFunc_0_epp sse_op_table1[256][4] = {\n    /* 3DNow! extensions */\n    [0x0e] = { SSE_DUMMY }, /* femms */\n    [0x0f] = { SSE_DUMMY }, /* pf... */\n    /* pure SSE operations */\n    [0x10] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movups, movupd, movss, movsd */\n    [0x11] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movups, movupd, movss, movsd */\n    [0x12] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movlps, movlpd, movsldup, movddup */\n    [0x13] = { SSE_SPECIAL, SSE_SPECIAL },  /* movlps, movlpd */\n    [0x14] = { gen_helper_punpckldq_xmm, gen_helper_punpcklqdq_xmm },\n    [0x15] = { gen_helper_punpckhdq_xmm, gen_helper_punpckhqdq_xmm },\n    [0x16] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL },  /* movhps, movhpd, movshdup */\n    [0x17] = { SSE_SPECIAL, SSE_SPECIAL },  /* movhps, movhpd */\n\n    [0x28] = { SSE_SPECIAL, SSE_SPECIAL },  /* movaps, movapd */\n    [0x29] = { SSE_SPECIAL, SSE_SPECIAL },  /* movaps, movapd */\n    [0x2a] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* cvtpi2ps, cvtpi2pd, cvtsi2ss, cvtsi2sd */\n    [0x2b] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movntps, movntpd, movntss, movntsd */\n    [0x2c] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* cvttps2pi, cvttpd2pi, cvttsd2si, cvttss2si */\n    [0x2d] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* cvtps2pi, cvtpd2pi, cvtsd2si, cvtss2si */\n    [0x2e] = { gen_helper_ucomiss, gen_helper_ucomisd },\n    [0x2f] = { gen_helper_comiss, gen_helper_comisd },\n    [0x50] = { SSE_SPECIAL, SSE_SPECIAL }, /* movmskps, movmskpd */\n    [0x51] = SSE_FOP(sqrt),\n    [0x52] = { gen_helper_rsqrtps, NULL, gen_helper_rsqrtss, NULL },\n    [0x53] = { gen_helper_rcpps, NULL, gen_helper_rcpss, NULL },\n    [0x54] = { gen_helper_pand_xmm, gen_helper_pand_xmm }, /* andps, andpd */\n    [0x55] = { gen_helper_pandn_xmm, gen_helper_pandn_xmm }, /* andnps, andnpd */\n    [0x56] = { gen_helper_por_xmm, gen_helper_por_xmm }, /* orps, orpd */\n    [0x57] = { gen_helper_pxor_xmm, gen_helper_pxor_xmm }, /* xorps, xorpd */\n    [0x58] = SSE_FOP(add),\n    [0x59] = SSE_FOP(mul),\n    [0x5a] = { gen_helper_cvtps2pd, gen_helper_cvtpd2ps,\n               gen_helper_cvtss2sd, gen_helper_cvtsd2ss },\n    [0x5b] = { gen_helper_cvtdq2ps, gen_helper_cvtps2dq, gen_helper_cvttps2dq },\n    [0x5c] = SSE_FOP(sub),\n    [0x5d] = SSE_FOP(min),\n    [0x5e] = SSE_FOP(div),\n    [0x5f] = SSE_FOP(max),\n\n    [0xc2] = SSE_FOP(cmpeq),\n    [0xc6] = { (SSEFunc_0_epp)gen_helper_shufps,\n               (SSEFunc_0_epp)gen_helper_shufpd }, /* XXX: casts */\n\n    /* SSSE3, SSE4, MOVBE, CRC32, BMI1, BMI2, ADX.  */\n    [0x38] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL },\n    [0x3a] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL },\n\n    /* MMX ops and their SSE extensions */\n    [0x60] = MMX_OP2(punpcklbw),\n    [0x61] = MMX_OP2(punpcklwd),\n    [0x62] = MMX_OP2(punpckldq),\n    [0x63] = MMX_OP2(packsswb),\n    [0x64] = MMX_OP2(pcmpgtb),\n    [0x65] = MMX_OP2(pcmpgtw),\n    [0x66] = MMX_OP2(pcmpgtl),\n    [0x67] = MMX_OP2(packuswb),\n    [0x68] = MMX_OP2(punpckhbw),\n    [0x69] = MMX_OP2(punpckhwd),\n    [0x6a] = MMX_OP2(punpckhdq),\n    [0x6b] = MMX_OP2(packssdw),\n    [0x6c] = { NULL, gen_helper_punpcklqdq_xmm },\n    [0x6d] = { NULL, gen_helper_punpckhqdq_xmm },\n    [0x6e] = { SSE_SPECIAL, SSE_SPECIAL }, /* movd mm, ea */\n    [0x6f] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movq, movdqa, , movqdu */\n    [0x70] = { (SSEFunc_0_epp)gen_helper_pshufw_mmx,\n               (SSEFunc_0_epp)gen_helper_pshufd_xmm,\n               (SSEFunc_0_epp)gen_helper_pshufhw_xmm,\n               (SSEFunc_0_epp)gen_helper_pshuflw_xmm }, /* XXX: casts */\n    [0x71] = { SSE_SPECIAL, SSE_SPECIAL }, /* shiftw */\n    [0x72] = { SSE_SPECIAL, SSE_SPECIAL }, /* shiftd */\n    [0x73] = { SSE_SPECIAL, SSE_SPECIAL }, /* shiftq */\n    [0x74] = MMX_OP2(pcmpeqb),\n    [0x75] = MMX_OP2(pcmpeqw),\n    [0x76] = MMX_OP2(pcmpeql),\n    [0x77] = { SSE_DUMMY }, /* emms */\n    [0x78] = { NULL, SSE_SPECIAL, NULL, SSE_SPECIAL }, /* extrq_i, insertq_i */\n    [0x79] = { NULL, gen_helper_extrq_r, NULL, gen_helper_insertq_r },\n    [0x7c] = { NULL, gen_helper_haddpd, NULL, gen_helper_haddps },\n    [0x7d] = { NULL, gen_helper_hsubpd, NULL, gen_helper_hsubps },\n    [0x7e] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movd, movd, , movq */\n    [0x7f] = { SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL }, /* movq, movdqa, movdqu */\n    [0xc4] = { SSE_SPECIAL, SSE_SPECIAL }, /* pinsrw */\n    [0xc5] = { SSE_SPECIAL, SSE_SPECIAL }, /* pextrw */\n    [0xd0] = { NULL, gen_helper_addsubpd, NULL, gen_helper_addsubps },\n    [0xd1] = MMX_OP2(psrlw),\n    [0xd2] = MMX_OP2(psrld),\n    [0xd3] = MMX_OP2(psrlq),\n    [0xd4] = MMX_OP2(paddq),\n    [0xd5] = MMX_OP2(pmullw),\n    [0xd6] = { NULL, SSE_SPECIAL, SSE_SPECIAL, SSE_SPECIAL },\n    [0xd7] = { SSE_SPECIAL, SSE_SPECIAL }, /* pmovmskb */\n    [0xd8] = MMX_OP2(psubusb),\n    [0xd9] = MMX_OP2(psubusw),\n    [0xda] = MMX_OP2(pminub),\n    [0xdb] = MMX_OP2(pand),\n    [0xdc] = MMX_OP2(paddusb),\n    [0xdd] = MMX_OP2(paddusw),\n    [0xde] = MMX_OP2(pmaxub),\n    [0xdf] = MMX_OP2(pandn),\n    [0xe0] = MMX_OP2(pavgb),\n    [0xe1] = MMX_OP2(psraw),\n    [0xe2] = MMX_OP2(psrad),\n    [0xe3] = MMX_OP2(pavgw),\n    [0xe4] = MMX_OP2(pmulhuw),\n    [0xe5] = MMX_OP2(pmulhw),\n    [0xe6] = { NULL, gen_helper_cvttpd2dq, gen_helper_cvtdq2pd, gen_helper_cvtpd2dq },\n    [0xe7] = { SSE_SPECIAL , SSE_SPECIAL },  /* movntq, movntq */\n    [0xe8] = MMX_OP2(psubsb),\n    [0xe9] = MMX_OP2(psubsw),\n    [0xea] = MMX_OP2(pminsw),\n    [0xeb] = MMX_OP2(por),\n    [0xec] = MMX_OP2(paddsb),\n    [0xed] = MMX_OP2(paddsw),\n    [0xee] = MMX_OP2(pmaxsw),\n    [0xef] = MMX_OP2(pxor),\n    [0xf0] = { NULL, NULL, NULL, SSE_SPECIAL }, /* lddqu */\n    [0xf1] = MMX_OP2(psllw),\n    [0xf2] = MMX_OP2(pslld),\n    [0xf3] = MMX_OP2(psllq),\n    [0xf4] = MMX_OP2(pmuludq),\n    [0xf5] = MMX_OP2(pmaddwd),\n    [0xf6] = MMX_OP2(psadbw),\n    [0xf7] = { (SSEFunc_0_epp)gen_helper_maskmov_mmx,\n               (SSEFunc_0_epp)gen_helper_maskmov_xmm }, /* XXX: casts */\n    [0xf8] = MMX_OP2(psubb),\n    [0xf9] = MMX_OP2(psubw),\n    [0xfa] = MMX_OP2(psubl),\n    [0xfb] = MMX_OP2(psubq),\n    [0xfc] = MMX_OP2(paddb),\n    [0xfd] = MMX_OP2(paddw),\n    [0xfe] = MMX_OP2(paddl),\n};\n\nstatic const SSEFunc_0_epp sse_op_table2[3 * 8][2] = {\n    [0 + 2] = MMX_OP2(psrlw),\n    [0 + 4] = MMX_OP2(psraw),\n    [0 + 6] = MMX_OP2(psllw),\n    [8 + 2] = MMX_OP2(psrld),\n    [8 + 4] = MMX_OP2(psrad),\n    [8 + 6] = MMX_OP2(pslld),\n    [16 + 2] = MMX_OP2(psrlq),\n    [16 + 3] = { NULL, gen_helper_psrldq_xmm },\n    [16 + 6] = MMX_OP2(psllq),\n    [16 + 7] = { NULL, gen_helper_pslldq_xmm },\n};\n\nstatic const SSEFunc_0_epi sse_op_table3ai[] = {\n    gen_helper_cvtsi2ss,\n    gen_helper_cvtsi2sd\n};\n\n#ifdef TARGET_X86_64\nstatic const SSEFunc_0_epl sse_op_table3aq[] = {\n    gen_helper_cvtsq2ss,\n    gen_helper_cvtsq2sd\n};\n#endif\n\nstatic const SSEFunc_i_ep sse_op_table3bi[] = {\n    gen_helper_cvttss2si,\n    gen_helper_cvtss2si,\n    gen_helper_cvttsd2si,\n    gen_helper_cvtsd2si\n};\n\n#ifdef TARGET_X86_64\nstatic const SSEFunc_l_ep sse_op_table3bq[] = {\n    gen_helper_cvttss2sq,\n    gen_helper_cvtss2sq,\n    gen_helper_cvttsd2sq,\n    gen_helper_cvtsd2sq\n};\n#endif\n\nstatic const SSEFunc_0_epp sse_op_table4[8][4] = {\n    SSE_FOP(cmpeq),\n    SSE_FOP(cmplt),\n    SSE_FOP(cmple),\n    SSE_FOP(cmpunord),\n    SSE_FOP(cmpneq),\n    SSE_FOP(cmpnlt),\n    SSE_FOP(cmpnle),\n    SSE_FOP(cmpord),\n};\n\nstatic const SSEFunc_0_epp sse_op_table5[256] = {\n    [0x0c] = gen_helper_pi2fw,\n    [0x0d] = gen_helper_pi2fd,\n    [0x1c] = gen_helper_pf2iw,\n    [0x1d] = gen_helper_pf2id,\n    [0x8a] = gen_helper_pfnacc,\n    [0x8e] = gen_helper_pfpnacc,\n    [0x90] = gen_helper_pfcmpge,\n    [0x94] = gen_helper_pfmin,\n    [0x96] = gen_helper_pfrcp,\n    [0x97] = gen_helper_pfrsqrt,\n    [0x9a] = gen_helper_pfsub,\n    [0x9e] = gen_helper_pfadd,\n    [0xa0] = gen_helper_pfcmpgt,\n    [0xa4] = gen_helper_pfmax,\n    [0xa6] = gen_helper_movq, /* pfrcpit1; no need to actually increase precision */\n    [0xa7] = gen_helper_movq, /* pfrsqit1 */\n    [0xaa] = gen_helper_pfsubr,\n    [0xae] = gen_helper_pfacc,\n    [0xb0] = gen_helper_pfcmpeq,\n    [0xb4] = gen_helper_pfmul,\n    [0xb6] = gen_helper_movq, /* pfrcpit2 */\n    [0xb7] = gen_helper_pmulhrw_mmx,\n    [0xbb] = gen_helper_pswapd,\n    [0xbf] = gen_helper_pavgb_mmx /* pavgusb */\n};\n\nstruct SSEOpHelper_epp {\n    SSEFunc_0_epp op[2];\n    uint32_t ext_mask;\n};\n\nstruct SSEOpHelper_eppi {\n    SSEFunc_0_eppi op[2];\n    uint32_t ext_mask;\n};\n\n#define SSSE3_OP(x) { MMX_OP2(x), CPUID_EXT_SSSE3 }\n#define SSE41_OP(x) { { NULL, gen_helper_ ## x ## _xmm }, CPUID_EXT_SSE41 }\n#define SSE42_OP(x) { { NULL, gen_helper_ ## x ## _xmm }, CPUID_EXT_SSE42 }\n#define SSE41_SPECIAL { { NULL, SSE_SPECIAL }, CPUID_EXT_SSE41 }\n#define PCLMULQDQ_OP(x) { { NULL, gen_helper_ ## x ## _xmm }, \\\n        CPUID_EXT_PCLMULQDQ }\n#define AESNI_OP(x) { { NULL, gen_helper_ ## x ## _xmm }, CPUID_EXT_AES }\n\nstatic const struct SSEOpHelper_epp sse_op_table6[256] = {\n    [0x00] = SSSE3_OP(pshufb),\n    [0x01] = SSSE3_OP(phaddw),\n    [0x02] = SSSE3_OP(phaddd),\n    [0x03] = SSSE3_OP(phaddsw),\n    [0x04] = SSSE3_OP(pmaddubsw),\n    [0x05] = SSSE3_OP(phsubw),\n    [0x06] = SSSE3_OP(phsubd),\n    [0x07] = SSSE3_OP(phsubsw),\n    [0x08] = SSSE3_OP(psignb),\n    [0x09] = SSSE3_OP(psignw),\n    [0x0a] = SSSE3_OP(psignd),\n    [0x0b] = SSSE3_OP(pmulhrsw),\n    [0x10] = SSE41_OP(pblendvb),\n    [0x14] = SSE41_OP(blendvps),\n    [0x15] = SSE41_OP(blendvpd),\n    [0x17] = SSE41_OP(ptest),\n    [0x1c] = SSSE3_OP(pabsb),\n    [0x1d] = SSSE3_OP(pabsw),\n    [0x1e] = SSSE3_OP(pabsd),\n    [0x20] = SSE41_OP(pmovsxbw),\n    [0x21] = SSE41_OP(pmovsxbd),\n    [0x22] = SSE41_OP(pmovsxbq),\n    [0x23] = SSE41_OP(pmovsxwd),\n    [0x24] = SSE41_OP(pmovsxwq),\n    [0x25] = SSE41_OP(pmovsxdq),\n    [0x28] = SSE41_OP(pmuldq),\n    [0x29] = SSE41_OP(pcmpeqq),\n    [0x2a] = SSE41_SPECIAL, /* movntqda */\n    [0x2b] = SSE41_OP(packusdw),\n    [0x30] = SSE41_OP(pmovzxbw),\n    [0x31] = SSE41_OP(pmovzxbd),\n    [0x32] = SSE41_OP(pmovzxbq),\n    [0x33] = SSE41_OP(pmovzxwd),\n    [0x34] = SSE41_OP(pmovzxwq),\n    [0x35] = SSE41_OP(pmovzxdq),\n    [0x37] = SSE42_OP(pcmpgtq),\n    [0x38] = SSE41_OP(pminsb),\n    [0x39] = SSE41_OP(pminsd),\n    [0x3a] = SSE41_OP(pminuw),\n    [0x3b] = SSE41_OP(pminud),\n    [0x3c] = SSE41_OP(pmaxsb),\n    [0x3d] = SSE41_OP(pmaxsd),\n    [0x3e] = SSE41_OP(pmaxuw),\n    [0x3f] = SSE41_OP(pmaxud),\n    [0x40] = SSE41_OP(pmulld),\n    [0x41] = SSE41_OP(phminposuw),\n    [0xdb] = AESNI_OP(aesimc),\n    [0xdc] = AESNI_OP(aesenc),\n    [0xdd] = AESNI_OP(aesenclast),\n    [0xde] = AESNI_OP(aesdec),\n    [0xdf] = AESNI_OP(aesdeclast),\n};\n\nstatic const struct SSEOpHelper_eppi sse_op_table7[256] = {\n    [0x08] = SSE41_OP(roundps),\n    [0x09] = SSE41_OP(roundpd),\n    [0x0a] = SSE41_OP(roundss),\n    [0x0b] = SSE41_OP(roundsd),\n    [0x0c] = SSE41_OP(blendps),\n    [0x0d] = SSE41_OP(blendpd),\n    [0x0e] = SSE41_OP(pblendw),\n    [0x0f] = SSSE3_OP(palignr),\n    [0x14] = SSE41_SPECIAL, /* pextrb */\n    [0x15] = SSE41_SPECIAL, /* pextrw */\n    [0x16] = SSE41_SPECIAL, /* pextrd/pextrq */\n    [0x17] = SSE41_SPECIAL, /* extractps */\n    [0x20] = SSE41_SPECIAL, /* pinsrb */\n    [0x21] = SSE41_SPECIAL, /* insertps */\n    [0x22] = SSE41_SPECIAL, /* pinsrd/pinsrq */\n    [0x40] = SSE41_OP(dpps),\n    [0x41] = SSE41_OP(dppd),\n    [0x42] = SSE41_OP(mpsadbw),\n    [0x44] = PCLMULQDQ_OP(pclmulqdq),\n    [0x60] = SSE42_OP(pcmpestrm),\n    [0x61] = SSE42_OP(pcmpestri),\n    [0x62] = SSE42_OP(pcmpistrm),\n    [0x63] = SSE42_OP(pcmpistri),\n    [0xdf] = AESNI_OP(aeskeygenassist),\n};\n\nstatic void gen_sse(CPUX86State *env, DisasContext *s, int b,\n                    target_ulong pc_start, int rex_r)\n{\n    int b1, op1_offset, op2_offset, is_xmm, val;\n    int modrm, mod, rm, reg;\n    SSEFunc_0_epp sse_fn_epp;\n    SSEFunc_0_eppi sse_fn_eppi;\n    SSEFunc_0_ppi sse_fn_ppi;\n    SSEFunc_0_eppt sse_fn_eppt;\n    TCGMemOp ot;\n\n    b &= 0xff;\n    if (s->prefix & PREFIX_DATA)\n        b1 = 1;\n    else if (s->prefix & PREFIX_REPZ)\n        b1 = 2;\n    else if (s->prefix & PREFIX_REPNZ)\n        b1 = 3;\n    else\n        b1 = 0;\n    sse_fn_epp = sse_op_table1[b][b1];\n    if (!sse_fn_epp) {\n        goto unknown_op;\n    }\n    if ((b <= 0x5f && b >= 0x10) || b == 0xc6 || b == 0xc2) {\n        is_xmm = 1;\n    } else {\n        if (b1 == 0) {\n            /* MMX case */\n            is_xmm = 0;\n        } else {\n            is_xmm = 1;\n        }\n    }\n    /* simple MMX/SSE operation */\n    if (s->flags & HF_TS_MASK) {\n        gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n        return;\n    }\n    if (s->flags & HF_EM_MASK) {\n    illegal_op:\n        gen_illegal_opcode(s);\n        return;\n    }\n    if (is_xmm\n        && !(s->flags & HF_OSFXSR_MASK)\n        && ((b != 0x38 && b != 0x3a) || (s->prefix & PREFIX_DATA))) {\n        goto unknown_op;\n    }\n    if (b == 0x0e) {\n        if (!(s->cpuid_ext2_features & CPUID_EXT2_3DNOW)) {\n            /* If we were fully decoding this we might use illegal_op.  */\n            goto unknown_op;\n        }\n        /* femms */\n        gen_helper_emms(cpu_env);\n        return;\n    }\n    if (b == 0x77) {\n        /* emms */\n        gen_helper_emms(cpu_env);\n        return;\n    }\n    /* prepare MMX state (XXX: optimize by storing fptt and fptags in\n       the static cpu state) */\n    if (!is_xmm) {\n        gen_helper_enter_mmx(cpu_env);\n    }\n\n    modrm = cpu_ldub_code(env, s->pc++);\n    reg = ((modrm >> 3) & 7);\n    if (is_xmm)\n        reg |= rex_r;\n    mod = (modrm >> 6) & 3;\n    if (sse_fn_epp == SSE_SPECIAL) {\n        b |= (b1 << 8);\n        switch(b) {\n        case 0x0e7: /* movntq */\n            if (mod == 3) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_stq_env_A0(s, offsetof(CPUX86State, fpregs[reg].mmx));\n            break;\n        case 0x1e7: /* movntdq */\n        case 0x02b: /* movntps */\n        case 0x12b: /* movntps */\n            if (mod == 3)\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            gen_sto_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            break;\n        case 0x3f0: /* lddqu */\n            if (mod == 3)\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            gen_ldo_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            break;\n        case 0x22b: /* movntss */\n        case 0x32b: /* movntsd */\n            if (mod == 3)\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if (b1 & 1) {\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                    xmm_regs[reg].ZMM_L(0)));\n                gen_op_st_v(s, MO_32, cpu_T0, cpu_A0);\n            }\n            break;\n        case 0x6e: /* movd mm, ea */\n#ifdef TARGET_X86_64\n            if (s->dflag == MO_64) {\n                gen_ldst_modrm(env, s, modrm, MO_64, OR_TMP0, 0);\n                tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State,fpregs[reg].mmx));\n            } else\n#endif\n            {\n                gen_ldst_modrm(env, s, modrm, MO_32, OR_TMP0, 0);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                                 offsetof(CPUX86State,fpregs[reg].mmx));\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_movl_mm_T0_mmx(cpu_ptr0, cpu_tmp2_i32);\n            }\n            break;\n        case 0x16e: /* movd xmm, ea */\n#ifdef TARGET_X86_64\n            if (s->dflag == MO_64) {\n                gen_ldst_modrm(env, s, modrm, MO_64, OR_TMP0, 0);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                                 offsetof(CPUX86State,xmm_regs[reg]));\n                gen_helper_movq_mm_T0_xmm(cpu_ptr0, cpu_T0);\n            } else\n#endif\n            {\n                gen_ldst_modrm(env, s, modrm, MO_32, OR_TMP0, 0);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                                 offsetof(CPUX86State,xmm_regs[reg]));\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_movl_mm_T0_xmm(cpu_ptr0, cpu_tmp2_i32);\n            }\n            break;\n        case 0x6f: /* movq mm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State, fpregs[reg].mmx));\n            } else {\n                rm = (modrm & 7);\n                tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env,\n                               offsetof(CPUX86State,fpregs[rm].mmx));\n                tcg_gen_st_i64(cpu_tmp1_i64, cpu_env,\n                               offsetof(CPUX86State,fpregs[reg].mmx));\n            }\n            break;\n        case 0x010: /* movups */\n        case 0x110: /* movupd */\n        case 0x028: /* movaps */\n        case 0x128: /* movapd */\n        case 0x16f: /* movdqa xmm, ea */\n        case 0x26f: /* movdqu xmm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldo_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movo(offsetof(CPUX86State,xmm_regs[reg]),\n                            offsetof(CPUX86State,xmm_regs[rm]));\n            }\n            break;\n        case 0x210: /* movss xmm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, MO_32, cpu_T0, cpu_A0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n                tcg_gen_movi_tl(cpu_T0, 0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(1)));\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)));\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(0)));\n            }\n            break;\n        case 0x310: /* movsd xmm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n                tcg_gen_movi_tl(cpu_T0, 0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)));\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            }\n            break;\n        case 0x012: /* movlps */\n        case 0x112: /* movlpd */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                /* movhlps */\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(1)));\n            }\n            break;\n        case 0x212: /* movsldup */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldo_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(0)));\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(2)));\n            }\n            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(1)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)));\n            break;\n        case 0x312: /* movddup */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            }\n            gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(1)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)));\n            break;\n        case 0x016: /* movhps */\n        case 0x116: /* movhpd */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(1)));\n            } else {\n                /* movlhps */\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(1)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            }\n            break;\n        case 0x216: /* movshdup */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldo_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(1)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(1)));\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_L(3)));\n            }\n            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_L(1)));\n            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].ZMM_L(2)),\n                        offsetof(CPUX86State,xmm_regs[reg].ZMM_L(3)));\n            break;\n        case 0x178:\n        case 0x378:\n            {\n                int bit_index, field_length;\n\n                if (b1 == 1 && reg != 0)\n                    goto illegal_op;\n                field_length = cpu_ldub_code(env, s->pc++) & 0x3F;\n                bit_index = cpu_ldub_code(env, s->pc++) & 0x3F;\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env,\n                    offsetof(CPUX86State,xmm_regs[reg]));\n                if (b1 == 1)\n                    gen_helper_extrq_i(cpu_env, cpu_ptr0,\n                                       tcg_const_i32(bit_index),\n                                       tcg_const_i32(field_length));\n                else\n                    gen_helper_insertq_i(cpu_env, cpu_ptr0,\n                                         tcg_const_i32(bit_index),\n                                         tcg_const_i32(field_length));\n            }\n            break;\n        case 0x7e: /* movd ea, mm */\n#ifdef TARGET_X86_64\n            if (s->dflag == MO_64) {\n                tcg_gen_ld_i64(cpu_T0, cpu_env,\n                               offsetof(CPUX86State,fpregs[reg].mmx));\n                gen_ldst_modrm(env, s, modrm, MO_64, OR_TMP0, 1);\n            } else\n#endif\n            {\n                tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                                 offsetof(CPUX86State,fpregs[reg].mmx.MMX_L(0)));\n                gen_ldst_modrm(env, s, modrm, MO_32, OR_TMP0, 1);\n            }\n            break;\n        case 0x17e: /* movd ea, xmm */\n#ifdef TARGET_X86_64\n            if (s->dflag == MO_64) {\n                tcg_gen_ld_i64(cpu_T0, cpu_env,\n                               offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)));\n                gen_ldst_modrm(env, s, modrm, MO_64, OR_TMP0, 1);\n            } else\n#endif\n            {\n                tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                                 offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n                gen_ldst_modrm(env, s, modrm, MO_32, OR_TMP0, 1);\n            }\n            break;\n        case 0x27e: /* movq xmm, ea */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_ldq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            }\n            gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(1)));\n            break;\n        case 0x7f: /* movq ea, mm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State, fpregs[reg].mmx));\n            } else {\n                rm = (modrm & 7);\n                gen_op_movq(offsetof(CPUX86State,fpregs[rm].mmx),\n                            offsetof(CPUX86State,fpregs[reg].mmx));\n            }\n            break;\n        case 0x011: /* movups */\n        case 0x111: /* movupd */\n        case 0x029: /* movaps */\n        case 0x129: /* movapd */\n        case 0x17f: /* movdqa ea, xmm */\n        case 0x27f: /* movdqu ea, xmm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_sto_env_A0(s, offsetof(CPUX86State, xmm_regs[reg]));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movo(offsetof(CPUX86State,xmm_regs[rm]),\n                            offsetof(CPUX86State,xmm_regs[reg]));\n            }\n            break;\n        case 0x211: /* movss ea, xmm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n                gen_op_st_v(s, MO_32, cpu_T0, cpu_A0);\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movl(offsetof(CPUX86State,xmm_regs[rm].ZMM_L(0)),\n                            offsetof(CPUX86State,xmm_regs[reg].ZMM_L(0)));\n            }\n            break;\n        case 0x311: /* movsd ea, xmm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)));\n            }\n            break;\n        case 0x013: /* movlps */\n        case 0x113: /* movlpd */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                goto illegal_op;\n            }\n            break;\n        case 0x017: /* movhps */\n        case 0x117: /* movhpd */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(1)));\n            } else {\n                goto illegal_op;\n            }\n            break;\n        case 0x71: /* shift mm, im */\n        case 0x72:\n        case 0x73:\n        case 0x171: /* shift xmm, im */\n        case 0x172:\n        case 0x173:\n            if (b1 >= 2) {\n\t        goto unknown_op;\n            }\n            val = cpu_ldub_code(env, s->pc++);\n            if (is_xmm) {\n                tcg_gen_movi_tl(cpu_T0, val);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_t0.ZMM_L(0)));\n                tcg_gen_movi_tl(cpu_T0, 0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_t0.ZMM_L(1)));\n                op1_offset = offsetof(CPUX86State,xmm_t0);\n            } else {\n                tcg_gen_movi_tl(cpu_T0, val);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,mmx_t0.MMX_L(0)));\n                tcg_gen_movi_tl(cpu_T0, 0);\n                tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,mmx_t0.MMX_L(1)));\n                op1_offset = offsetof(CPUX86State,mmx_t0);\n            }\n            sse_fn_epp = sse_op_table2[((b - 1) & 3) * 8 +\n                                       (((modrm >> 3)) & 7)][b1];\n            if (!sse_fn_epp) {\n                goto unknown_op;\n            }\n            if (is_xmm) {\n                rm = (modrm & 7) | REX_B(s);\n                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);\n            } else {\n                rm = (modrm & 7);\n                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n            }\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op2_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op1_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n            break;\n        case 0x050: /* movmskps */\n            rm = (modrm & 7) | REX_B(s);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                             offsetof(CPUX86State,xmm_regs[rm]));\n            gen_helper_movmskps(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            break;\n        case 0x150: /* movmskpd */\n            rm = (modrm & 7) | REX_B(s);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, \n                             offsetof(CPUX86State,xmm_regs[rm]));\n            gen_helper_movmskpd(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            break;\n        case 0x02a: /* cvtpi2ps */\n        case 0x12a: /* cvtpi2pd */\n            gen_helper_enter_mmx(cpu_env);\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                op2_offset = offsetof(CPUX86State,mmx_t0);\n                gen_ldq_env_A0(s, op2_offset);\n            } else {\n                rm = (modrm & 7);\n                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n            }\n            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            switch(b >> 8) {\n            case 0x0:\n                gen_helper_cvtpi2ps(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            default:\n            case 0x1:\n                gen_helper_cvtpi2pd(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            }\n            break;\n        case 0x22a: /* cvtsi2ss */\n        case 0x32a: /* cvtsi2sd */\n            ot = mo_64_32(s->dflag);\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            if (ot == MO_32) {\n                SSEFunc_0_epi sse_fn_epi = sse_op_table3ai[(b >> 8) & 1];\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                sse_fn_epi(cpu_env, cpu_ptr0, cpu_tmp2_i32);\n            } else {\n#ifdef TARGET_X86_64\n                SSEFunc_0_epl sse_fn_epl = sse_op_table3aq[(b >> 8) & 1];\n                sse_fn_epl(cpu_env, cpu_ptr0, cpu_T0);\n#else\n                goto illegal_op;\n#endif\n            }\n            break;\n        case 0x02c: /* cvttps2pi */\n        case 0x12c: /* cvttpd2pi */\n        case 0x02d: /* cvtps2pi */\n        case 0x12d: /* cvtpd2pi */\n            gen_helper_enter_mmx(cpu_env);\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                op2_offset = offsetof(CPUX86State,xmm_t0);\n                gen_ldo_env_A0(s, op2_offset);\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);\n            }\n            op1_offset = offsetof(CPUX86State,fpregs[reg & 7].mmx);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            switch(b) {\n            case 0x02c:\n                gen_helper_cvttps2pi(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            case 0x12c:\n                gen_helper_cvttpd2pi(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            case 0x02d:\n                gen_helper_cvtps2pi(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            case 0x12d:\n                gen_helper_cvtpd2pi(cpu_env, cpu_ptr0, cpu_ptr1);\n                break;\n            }\n            break;\n        case 0x22c: /* cvttss2si */\n        case 0x32c: /* cvttsd2si */\n        case 0x22d: /* cvtss2si */\n        case 0x32d: /* cvtsd2si */\n            ot = mo_64_32(s->dflag);\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                if ((b >> 8) & 1) {\n                    gen_ldq_env_A0(s, offsetof(CPUX86State, xmm_t0.ZMM_Q(0)));\n                } else {\n                    gen_op_ld_v(s, MO_32, cpu_T0, cpu_A0);\n                    tcg_gen_st32_tl(cpu_T0, cpu_env, offsetof(CPUX86State,xmm_t0.ZMM_L(0)));\n                }\n                op2_offset = offsetof(CPUX86State,xmm_t0);\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);\n            }\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op2_offset);\n            if (ot == MO_32) {\n                SSEFunc_i_ep sse_fn_i_ep =\n                    sse_op_table3bi[((b >> 7) & 2) | (b & 1)];\n                sse_fn_i_ep(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n                tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n            } else {\n#ifdef TARGET_X86_64\n                SSEFunc_l_ep sse_fn_l_ep =\n                    sse_op_table3bq[((b >> 7) & 2) | (b & 1)];\n                sse_fn_l_ep(cpu_T0, cpu_env, cpu_ptr0);\n#else\n                goto illegal_op;\n#endif\n            }\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n            break;\n        case 0xc4: /* pinsrw */\n        case 0x1c4:\n            s->rip_offset = 1;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            val = cpu_ldub_code(env, s->pc++);\n            if (b1) {\n                val &= 7;\n                tcg_gen_st16_tl(cpu_T0, cpu_env,\n                                offsetof(CPUX86State,xmm_regs[reg].ZMM_W(val)));\n            } else {\n                val &= 3;\n                tcg_gen_st16_tl(cpu_T0, cpu_env,\n                                offsetof(CPUX86State,fpregs[reg].mmx.MMX_W(val)));\n            }\n            break;\n        case 0xc5: /* pextrw */\n        case 0x1c5:\n            if (mod != 3)\n                goto illegal_op;\n            ot = mo_64_32(s->dflag);\n            val = cpu_ldub_code(env, s->pc++);\n            if (b1) {\n                val &= 7;\n                rm = (modrm & 7) | REX_B(s);\n                tcg_gen_ld16u_tl(cpu_T0, cpu_env,\n                                 offsetof(CPUX86State,xmm_regs[rm].ZMM_W(val)));\n            } else {\n                val &= 3;\n                rm = (modrm & 7);\n                tcg_gen_ld16u_tl(cpu_T0, cpu_env,\n                                offsetof(CPUX86State,fpregs[rm].mmx.MMX_W(val)));\n            }\n            reg = ((modrm >> 3) & 7) | rex_r;\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n            break;\n        case 0x1d6: /* movq ea, xmm */\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_stq_env_A0(s, offsetof(CPUX86State,\n                                           xmm_regs[reg].ZMM_Q(0)));\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                gen_op_movq(offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)),\n                            offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)));\n                gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(1)));\n            }\n            break;\n        case 0x2d6: /* movq2dq */\n            gen_helper_enter_mmx(cpu_env);\n            rm = (modrm & 7);\n            gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(0)),\n                        offsetof(CPUX86State,fpregs[rm].mmx));\n            gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[reg].ZMM_Q(1)));\n            break;\n        case 0x3d6: /* movdq2q */\n            gen_helper_enter_mmx(cpu_env);\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_movq(offsetof(CPUX86State,fpregs[reg & 7].mmx),\n                        offsetof(CPUX86State,xmm_regs[rm].ZMM_Q(0)));\n            break;\n        case 0xd7: /* pmovmskb */\n        case 0x1d7:\n            if (mod != 3)\n                goto illegal_op;\n            if (b1) {\n                rm = (modrm & 7) | REX_B(s);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,xmm_regs[rm]));\n                gen_helper_pmovmskb_xmm(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n            } else {\n                rm = (modrm & 7);\n                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,fpregs[rm].mmx));\n                gen_helper_pmovmskb_mmx(cpu_tmp2_i32, cpu_env, cpu_ptr0);\n            }\n            reg = ((modrm >> 3) & 7) | rex_r;\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            break;\n\n        case 0x138:\n        case 0x038:\n            b = modrm;\n            if ((b & 0xf0) == 0xf0) {\n                goto do_0f_38_fx;\n            }\n            modrm = cpu_ldub_code(env, s->pc++);\n            rm = modrm & 7;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            if (b1 >= 2) {\n                goto unknown_op;\n            }\n\n            sse_fn_epp = sse_op_table6[b].op[b1];\n            if (!sse_fn_epp) {\n                goto unknown_op;\n            }\n            if (!(s->cpuid_ext_features & sse_op_table6[b].ext_mask))\n                goto illegal_op;\n\n            if (b1) {\n                op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n                if (mod == 3) {\n                    op2_offset = offsetof(CPUX86State,xmm_regs[rm | REX_B(s)]);\n                } else {\n                    op2_offset = offsetof(CPUX86State,xmm_t0);\n                    gen_lea_modrm(env, s, modrm);\n                    switch (b) {\n                    case 0x20: case 0x30: /* pmovsxbw, pmovzxbw */\n                    case 0x23: case 0x33: /* pmovsxwd, pmovzxwd */\n                    case 0x25: case 0x35: /* pmovsxdq, pmovzxdq */\n                        gen_ldq_env_A0(s, op2_offset +\n                                        offsetof(ZMMReg, ZMM_Q(0)));\n                        break;\n                    case 0x21: case 0x31: /* pmovsxbd, pmovzxbd */\n                    case 0x24: case 0x34: /* pmovsxwq, pmovzxwq */\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, op2_offset +\n                                        offsetof(ZMMReg, ZMM_L(0)));\n                        break;\n                    case 0x22: case 0x32: /* pmovsxbq, pmovzxbq */\n                        tcg_gen_qemu_ld_tl(cpu_tmp0, cpu_A0,\n                                           s->mem_index, MO_LEUW);\n                        tcg_gen_st16_tl(cpu_tmp0, cpu_env, op2_offset +\n                                        offsetof(ZMMReg, ZMM_W(0)));\n                        break;\n                    case 0x2a:            /* movntqda */\n                        gen_ldo_env_A0(s, op1_offset);\n                        return;\n                    default:\n                        gen_ldo_env_A0(s, op2_offset);\n                    }\n                }\n            } else {\n                op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);\n                if (mod == 3) {\n                    op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n                } else {\n                    op2_offset = offsetof(CPUX86State,mmx_t0);\n                    gen_lea_modrm(env, s, modrm);\n                    gen_ldq_env_A0(s, op2_offset);\n                }\n            }\n            if (sse_fn_epp == SSE_SPECIAL) {\n                goto unknown_op;\n            }\n\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n\n            if (b == 0x17) {\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n            break;\n\n        case 0x238:\n        case 0x338:\n        do_0f_38_fx:\n            /* Various integer extensions at 0f 38 f[0-f].  */\n            b = modrm | (b1 << 8);\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n\n            switch (b) {\n            case 0x3f0: /* crc32 Gd,Eb */\n            case 0x3f1: /* crc32 Gd,Ey */\n            do_crc32:\n                if (!(s->cpuid_ext_features & CPUID_EXT_SSE42)) {\n                    goto illegal_op;\n                }\n                if ((b & 0xff) == 0xf0) {\n                    ot = MO_8;\n                } else if (s->dflag != MO_64) {\n                    ot = (s->prefix & PREFIX_DATA ? MO_16 : MO_32);\n                } else {\n                    ot = MO_64;\n                }\n\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[reg]);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                gen_helper_crc32(cpu_T0, cpu_tmp2_i32,\n                                 cpu_T0, tcg_const_i32(8 << ot));\n\n                ot = mo_64_32(s->dflag);\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                break;\n\n            case 0x1f0: /* crc32 or movbe */\n            case 0x1f1:\n                /* For these insns, the f3 prefix is supposed to have priority\n                   over the 66 prefix, but that's not what we implement above\n                   setting b1.  */\n                if (s->prefix & PREFIX_REPNZ) {\n                    goto do_crc32;\n                }\n                /* FALLTHRU */\n            case 0x0f0: /* movbe Gy,My */\n            case 0x0f1: /* movbe My,Gy */\n                if (!(s->cpuid_ext_features & CPUID_EXT_MOVBE)) {\n                    goto illegal_op;\n                }\n                if (s->dflag != MO_64) {\n                    ot = (s->prefix & PREFIX_DATA ? MO_16 : MO_32);\n                } else {\n                    ot = MO_64;\n                }\n\n                gen_lea_modrm(env, s, modrm);\n                if ((b & 1) == 0) {\n                    tcg_gen_qemu_ld_tl(cpu_T0, cpu_A0,\n                                       s->mem_index, ot | MO_BE);\n                    gen_op_mov_reg_v(ot, reg, cpu_T0);\n                } else {\n                    tcg_gen_qemu_st_tl(cpu_regs[reg], cpu_A0,\n                                       s->mem_index, ot | MO_BE);\n                }\n                break;\n\n            case 0x0f2: /* andn Gy, By, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                tcg_gen_andc_tl(cpu_T0, cpu_regs[s->vex_v], cpu_T0);\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                gen_op_update1_cc();\n                set_cc_op(s, CC_OP_LOGICB + ot);\n                break;\n\n            case 0x0f7: /* bextr Gy, Ey, By */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                {\n                    TCGv bound, zero;\n\n                    gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                    /* Extract START, and shift the operand.\n                       Shifts larger than operand size get zeros.  */\n                    tcg_gen_ext8u_tl(cpu_A0, cpu_regs[s->vex_v]);\n                    tcg_gen_shr_tl(cpu_T0, cpu_T0, cpu_A0);\n\n                    bound = tcg_const_tl(ot == MO_64 ? 63 : 31);\n                    zero = tcg_const_tl(0);\n                    tcg_gen_movcond_tl(TCG_COND_LEU, cpu_T0, cpu_A0, bound,\n                                       cpu_T0, zero);\n                    tcg_temp_free(zero);\n\n                    /* Extract the LEN into a mask.  Lengths larger than\n                       operand size get all ones.  */\n                    tcg_gen_extract_tl(cpu_A0, cpu_regs[s->vex_v], 8, 8);\n                    tcg_gen_movcond_tl(TCG_COND_LEU, cpu_A0, cpu_A0, bound,\n                                       cpu_A0, bound);\n                    tcg_temp_free(bound);\n                    tcg_gen_movi_tl(cpu_T1, 1);\n                    tcg_gen_shl_tl(cpu_T1, cpu_T1, cpu_A0);\n                    tcg_gen_subi_tl(cpu_T1, cpu_T1, 1);\n                    tcg_gen_and_tl(cpu_T0, cpu_T0, cpu_T1);\n\n                    gen_op_mov_reg_v(ot, reg, cpu_T0);\n                    gen_op_update1_cc();\n                    set_cc_op(s, CC_OP_LOGICB + ot);\n                }\n                break;\n\n            case 0x0f5: /* bzhi Gy, Ey, By */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                tcg_gen_ext8u_tl(cpu_T1, cpu_regs[s->vex_v]);\n                {\n                    TCGv bound = tcg_const_tl(ot == MO_64 ? 63 : 31);\n                    /* Note that since we're using BMILG (in order to get O\n                       cleared) we need to store the inverse into C.  */\n                    tcg_gen_setcond_tl(TCG_COND_LT, cpu_cc_src,\n                                       cpu_T1, bound);\n                    tcg_gen_movcond_tl(TCG_COND_GT, cpu_T1, cpu_T1,\n                                       bound, bound, cpu_T1);\n                    tcg_temp_free(bound);\n                }\n                tcg_gen_movi_tl(cpu_A0, -1);\n                tcg_gen_shl_tl(cpu_A0, cpu_A0, cpu_T1);\n                tcg_gen_andc_tl(cpu_T0, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                gen_op_update1_cc();\n                set_cc_op(s, CC_OP_BMILGB + ot);\n                break;\n\n            case 0x3f6: /* mulx By, Gy, rdx, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                switch (ot) {\n                default:\n                    tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                    tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EDX]);\n                    tcg_gen_mulu2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                      cpu_tmp2_i32, cpu_tmp3_i32);\n                    tcg_gen_extu_i32_tl(cpu_regs[s->vex_v], cpu_tmp2_i32);\n                    tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp3_i32);\n                    break;\n#ifdef TARGET_X86_64\n                case MO_64:\n                    tcg_gen_mulu2_i64(cpu_T0, cpu_T1,\n                                      cpu_T0, cpu_regs[R_EDX]);\n                    tcg_gen_mov_i64(cpu_regs[s->vex_v], cpu_T0);\n                    tcg_gen_mov_i64(cpu_regs[reg], cpu_T1);\n                    break;\n#endif\n                }\n                break;\n\n            case 0x3f5: /* pdep Gy, By, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                /* Note that by zero-extending the mask operand, we\n                   automatically handle zero-extending the result.  */\n                if (ot == MO_64) {\n                    tcg_gen_mov_tl(cpu_T1, cpu_regs[s->vex_v]);\n                } else {\n                    tcg_gen_ext32u_tl(cpu_T1, cpu_regs[s->vex_v]);\n                }\n                gen_helper_pdep(cpu_regs[reg], cpu_T0, cpu_T1);\n                break;\n\n            case 0x2f5: /* pext Gy, By, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                /* Note that by zero-extending the mask operand, we\n                   automatically handle zero-extending the result.  */\n                if (ot == MO_64) {\n                    tcg_gen_mov_tl(cpu_T1, cpu_regs[s->vex_v]);\n                } else {\n                    tcg_gen_ext32u_tl(cpu_T1, cpu_regs[s->vex_v]);\n                }\n                gen_helper_pext(cpu_regs[reg], cpu_T0, cpu_T1);\n                break;\n\n            case 0x1f6: /* adcx Gy, Ey */\n            case 0x2f6: /* adox Gy, Ey */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_ADX)) {\n                    goto illegal_op;\n                } else {\n                    TCGv carry_in, carry_out, zero;\n                    int end_op;\n\n                    ot = mo_64_32(s->dflag);\n                    gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n\n                    /* Re-use the carry-out from a previous round.  */\n                    TCGV_UNUSED(carry_in);\n                    carry_out = (b == 0x1f6 ? cpu_cc_dst : cpu_cc_src2);\n                    switch (s->cc_op) {\n                    case CC_OP_ADCX:\n                        if (b == 0x1f6) {\n                            carry_in = cpu_cc_dst;\n                            end_op = CC_OP_ADCX;\n                        } else {\n                            end_op = CC_OP_ADCOX;\n                        }\n                        break;\n                    case CC_OP_ADOX:\n                        if (b == 0x1f6) {\n                            end_op = CC_OP_ADCOX;\n                        } else {\n                            carry_in = cpu_cc_src2;\n                            end_op = CC_OP_ADOX;\n                        }\n                        break;\n                    case CC_OP_ADCOX:\n                        end_op = CC_OP_ADCOX;\n                        carry_in = carry_out;\n                        break;\n                    default:\n                        end_op = (b == 0x1f6 ? CC_OP_ADCX : CC_OP_ADOX);\n                        break;\n                    }\n                    /* If we can't reuse carry-out, get it out of EFLAGS.  */\n                    if (TCGV_IS_UNUSED(carry_in)) {\n                        if (s->cc_op != CC_OP_ADCX && s->cc_op != CC_OP_ADOX) {\n                            gen_compute_eflags(s);\n                        }\n                        carry_in = cpu_tmp0;\n                        tcg_gen_extract_tl(carry_in, cpu_cc_src,\n                                           ctz32(b == 0x1f6 ? CC_C : CC_O), 1);\n                    }\n\n                    switch (ot) {\n#ifdef TARGET_X86_64\n                    case MO_32:\n                        /* If we know TL is 64-bit, and we want a 32-bit\n                           result, just do everything in 64-bit arithmetic.  */\n                        tcg_gen_ext32u_i64(cpu_regs[reg], cpu_regs[reg]);\n                        tcg_gen_ext32u_i64(cpu_T0, cpu_T0);\n                        tcg_gen_add_i64(cpu_T0, cpu_T0, cpu_regs[reg]);\n                        tcg_gen_add_i64(cpu_T0, cpu_T0, carry_in);\n                        tcg_gen_ext32u_i64(cpu_regs[reg], cpu_T0);\n                        tcg_gen_shri_i64(carry_out, cpu_T0, 32);\n                        break;\n#endif\n                    default:\n                        /* Otherwise compute the carry-out in two steps.  */\n                        zero = tcg_const_tl(0);\n                        tcg_gen_add2_tl(cpu_T0, carry_out,\n                                        cpu_T0, zero,\n                                        carry_in, zero);\n                        tcg_gen_add2_tl(cpu_regs[reg], carry_out,\n                                        cpu_regs[reg], carry_out,\n                                        cpu_T0, zero);\n                        tcg_temp_free(zero);\n                        break;\n                    }\n                    set_cc_op(s, end_op);\n                }\n                break;\n\n            case 0x1f7: /* shlx Gy, Ey, By */\n            case 0x2f7: /* sarx Gy, Ey, By */\n            case 0x3f7: /* shrx Gy, Ey, By */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                if (ot == MO_64) {\n                    tcg_gen_andi_tl(cpu_T1, cpu_regs[s->vex_v], 63);\n                } else {\n                    tcg_gen_andi_tl(cpu_T1, cpu_regs[s->vex_v], 31);\n                }\n                if (b == 0x1f7) {\n                    tcg_gen_shl_tl(cpu_T0, cpu_T0, cpu_T1);\n                } else if (b == 0x2f7) {\n                    if (ot != MO_64) {\n                        tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n                    }\n                    tcg_gen_sar_tl(cpu_T0, cpu_T0, cpu_T1);\n                } else {\n                    if (ot != MO_64) {\n                        tcg_gen_ext32u_tl(cpu_T0, cpu_T0);\n                    }\n                    tcg_gen_shr_tl(cpu_T0, cpu_T0, cpu_T1);\n                }\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                break;\n\n            case 0x0f3:\n            case 0x1f3:\n            case 0x2f3:\n            case 0x3f3: /* Group 17 */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n\n                switch (reg & 7) {\n                case 1: /* blsr By,Ey */\n                    tcg_gen_neg_tl(cpu_T1, cpu_T0);\n                    tcg_gen_and_tl(cpu_T0, cpu_T0, cpu_T1);\n                    gen_op_mov_reg_v(ot, s->vex_v, cpu_T0);\n                    gen_op_update2_cc();\n                    set_cc_op(s, CC_OP_BMILGB + ot);\n                    break;\n\n                case 2: /* blsmsk By,Ey */\n                    tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n                    tcg_gen_subi_tl(cpu_T0, cpu_T0, 1);\n                    tcg_gen_xor_tl(cpu_T0, cpu_T0, cpu_cc_src);\n                    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                    set_cc_op(s, CC_OP_BMILGB + ot);\n                    break;\n\n                case 3: /* blsi By, Ey */\n                    tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n                    tcg_gen_subi_tl(cpu_T0, cpu_T0, 1);\n                    tcg_gen_and_tl(cpu_T0, cpu_T0, cpu_cc_src);\n                    tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                    set_cc_op(s, CC_OP_BMILGB + ot);\n                    break;\n\n                default:\n                    goto unknown_op;\n                }\n                break;\n\n            default:\n                goto unknown_op;\n            }\n            break;\n\n        case 0x03a:\n        case 0x13a:\n            b = modrm;\n            modrm = cpu_ldub_code(env, s->pc++);\n            rm = modrm & 7;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            if (b1 >= 2) {\n                goto unknown_op;\n            }\n\n            sse_fn_eppi = sse_op_table7[b].op[b1];\n            if (!sse_fn_eppi) {\n                goto unknown_op;\n            }\n            if (!(s->cpuid_ext_features & sse_op_table7[b].ext_mask))\n                goto illegal_op;\n\n            if (sse_fn_eppi == SSE_SPECIAL) {\n                ot = mo_64_32(s->dflag);\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3)\n                    gen_lea_modrm(env, s, modrm);\n                reg = ((modrm >> 3) & 7) | rex_r;\n                val = cpu_ldub_code(env, s->pc++);\n                switch (b) {\n                case 0x14: /* pextrb */\n                    tcg_gen_ld8u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                                            xmm_regs[reg].ZMM_B(val & 15)));\n                    if (mod == 3) {\n                        gen_op_mov_reg_v(ot, rm, cpu_T0);\n                    } else {\n                        tcg_gen_qemu_st_tl(cpu_T0, cpu_A0,\n                                           s->mem_index, MO_UB);\n                    }\n                    break;\n                case 0x15: /* pextrw */\n                    tcg_gen_ld16u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                                            xmm_regs[reg].ZMM_W(val & 7)));\n                    if (mod == 3) {\n                        gen_op_mov_reg_v(ot, rm, cpu_T0);\n                    } else {\n                        tcg_gen_qemu_st_tl(cpu_T0, cpu_A0,\n                                           s->mem_index, MO_LEUW);\n                    }\n                    break;\n                case 0x16:\n                    if (ot == MO_32) { /* pextrd */\n                        tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env,\n                                        offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(val & 3)));\n                        if (mod == 3) {\n                            tcg_gen_extu_i32_tl(cpu_regs[rm], cpu_tmp2_i32);\n                        } else {\n                            tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                                s->mem_index, MO_LEUL);\n                        }\n                    } else { /* pextrq */\n#ifdef TARGET_X86_64\n                        tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env,\n                                        offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_Q(val & 1)));\n                        if (mod == 3) {\n                            tcg_gen_mov_i64(cpu_regs[rm], cpu_tmp1_i64);\n                        } else {\n                            tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                                s->mem_index, MO_LEQ);\n                        }\n#else\n                        goto illegal_op;\n#endif\n                    }\n                    break;\n                case 0x17: /* extractps */\n                    tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                                            xmm_regs[reg].ZMM_L(val & 3)));\n                    if (mod == 3) {\n                        gen_op_mov_reg_v(ot, rm, cpu_T0);\n                    } else {\n                        tcg_gen_qemu_st_tl(cpu_T0, cpu_A0,\n                                           s->mem_index, MO_LEUL);\n                    }\n                    break;\n                case 0x20: /* pinsrb */\n                    if (mod == 3) {\n                        gen_op_mov_v_reg(MO_32, cpu_T0, rm);\n                    } else {\n                        tcg_gen_qemu_ld_tl(cpu_T0, cpu_A0,\n                                           s->mem_index, MO_UB);\n                    }\n                    tcg_gen_st8_tl(cpu_T0, cpu_env, offsetof(CPUX86State,\n                                            xmm_regs[reg].ZMM_B(val & 15)));\n                    break;\n                case 0x21: /* insertps */\n                    if (mod == 3) {\n                        tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env,\n                                        offsetof(CPUX86State,xmm_regs[rm]\n                                                .ZMM_L((val >> 6) & 3)));\n                    } else {\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                    tcg_gen_st_i32(cpu_tmp2_i32, cpu_env,\n                                    offsetof(CPUX86State,xmm_regs[reg]\n                                            .ZMM_L((val >> 4) & 3)));\n                    if ((val >> 0) & 1)\n                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),\n                                        cpu_env, offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(0)));\n                    if ((val >> 1) & 1)\n                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),\n                                        cpu_env, offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(1)));\n                    if ((val >> 2) & 1)\n                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),\n                                        cpu_env, offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(2)));\n                    if ((val >> 3) & 1)\n                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),\n                                        cpu_env, offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(3)));\n                    break;\n                case 0x22:\n                    if (ot == MO_32) { /* pinsrd */\n                        if (mod == 3) {\n                            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[rm]);\n                        } else {\n                            tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                                s->mem_index, MO_LEUL);\n                        }\n                        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env,\n                                        offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_L(val & 3)));\n                    } else { /* pinsrq */\n#ifdef TARGET_X86_64\n                        if (mod == 3) {\n                            gen_op_mov_v_reg(ot, cpu_tmp1_i64, rm);\n                        } else {\n                            tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                                s->mem_index, MO_LEQ);\n                        }\n                        tcg_gen_st_i64(cpu_tmp1_i64, cpu_env,\n                                        offsetof(CPUX86State,\n                                                xmm_regs[reg].ZMM_Q(val & 1)));\n#else\n                        goto illegal_op;\n#endif\n                    }\n                    break;\n                }\n                return;\n            }\n\n            if (b1) {\n                op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n                if (mod == 3) {\n                    op2_offset = offsetof(CPUX86State,xmm_regs[rm | REX_B(s)]);\n                } else {\n                    op2_offset = offsetof(CPUX86State,xmm_t0);\n                    gen_lea_modrm(env, s, modrm);\n                    gen_ldo_env_A0(s, op2_offset);\n                }\n            } else {\n                op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);\n                if (mod == 3) {\n                    op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n                } else {\n                    op2_offset = offsetof(CPUX86State,mmx_t0);\n                    gen_lea_modrm(env, s, modrm);\n                    gen_ldq_env_A0(s, op2_offset);\n                }\n            }\n            val = cpu_ldub_code(env, s->pc++);\n\n            if ((b & 0xfc) == 0x60) { /* pcmpXstrX */\n                set_cc_op(s, CC_OP_EFLAGS);\n\n                if (s->dflag == MO_64) {\n                    /* The helper must use entire 64-bit gp registers */\n                    val |= 1 << 8;\n                }\n            }\n\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_eppi(cpu_env, cpu_ptr0, cpu_ptr1, tcg_const_i32(val));\n            break;\n\n        case 0x33a:\n            /* Various integer extensions at 0f 3a f[0-f].  */\n            b = modrm | (b1 << 8);\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n\n            switch (b) {\n            case 0x3f0: /* rorx Gy,Ey, Ib */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI2)\n                    || !(s->prefix & PREFIX_VEX)\n                    || s->vex_l != 0) {\n                    goto illegal_op;\n                }\n                ot = mo_64_32(s->dflag);\n                gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n                b = cpu_ldub_code(env, s->pc++);\n                if (ot == MO_64) {\n                    tcg_gen_rotri_tl(cpu_T0, cpu_T0, b & 63);\n                } else {\n                    tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                    tcg_gen_rotri_i32(cpu_tmp2_i32, cpu_tmp2_i32, b & 31);\n                    tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n                }\n                gen_op_mov_reg_v(ot, reg, cpu_T0);\n                break;\n\n            default:\n                goto unknown_op;\n            }\n            break;\n\n        default:\n        unknown_op:\n            gen_unknown_opcode(env, s);\n            return;\n        }\n    } else {\n        /* generic MMX or SSE operation */\n        switch(b) {\n        case 0x70: /* pshufx insn */\n        case 0xc6: /* pshufx insn */\n        case 0xc2: /* compare insns */\n            s->rip_offset = 1;\n            break;\n        default:\n            break;\n        }\n        if (is_xmm) {\n            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);\n            if (mod != 3) {\n                int sz = 4;\n\n                gen_lea_modrm(env, s, modrm);\n                op2_offset = offsetof(CPUX86State,xmm_t0);\n\n                switch (b) {\n                case 0x50 ... 0x5a:\n                case 0x5c ... 0x5f:\n                case 0xc2:\n                    /* Most sse scalar operations.  */\n                    if (b1 == 2) {\n                        sz = 2;\n                    } else if (b1 == 3) {\n                        sz = 3;\n                    }\n                    break;\n\n                case 0x2e:  /* ucomis[sd] */\n                case 0x2f:  /* comis[sd] */\n                    if (b1 == 0) {\n                        sz = 2;\n                    } else {\n                        sz = 3;\n                    }\n                    break;\n                }\n\n                switch (sz) {\n                case 2:\n                    /* 32 bit access */\n                    gen_op_ld_v(s, MO_32, cpu_T0, cpu_A0);\n                    tcg_gen_st32_tl(cpu_T0, cpu_env,\n                                    offsetof(CPUX86State,xmm_t0.ZMM_L(0)));\n                    break;\n                case 3:\n                    /* 64 bit access */\n                    gen_ldq_env_A0(s, offsetof(CPUX86State, xmm_t0.ZMM_D(0)));\n                    break;\n                default:\n                    /* 128 bit access */\n                    gen_ldo_env_A0(s, op2_offset);\n                    break;\n                }\n            } else {\n                rm = (modrm & 7) | REX_B(s);\n                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);\n            }\n        } else {\n            op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                op2_offset = offsetof(CPUX86State,mmx_t0);\n                gen_ldq_env_A0(s, op2_offset);\n            } else {\n                rm = (modrm & 7);\n                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);\n            }\n        }\n        switch(b) {\n        case 0x0f: /* 3DNow! data insns */\n            val = cpu_ldub_code(env, s->pc++);\n            sse_fn_epp = sse_op_table5[val];\n            if (!sse_fn_epp) {\n                goto unknown_op;\n            }\n            if (!(s->cpuid_ext2_features & CPUID_EXT2_3DNOW)) {\n                goto illegal_op;\n            }\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n            break;\n        case 0x70: /* pshufx insn */\n        case 0xc6: /* pshufx insn */\n            val = cpu_ldub_code(env, s->pc++);\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            /* XXX: introduce a new table? */\n            sse_fn_ppi = (SSEFunc_0_ppi)sse_fn_epp;\n            sse_fn_ppi(cpu_ptr0, cpu_ptr1, tcg_const_i32(val));\n            break;\n        case 0xc2:\n            /* compare insns */\n            val = cpu_ldub_code(env, s->pc++);\n            if (val >= 8)\n                goto unknown_op;\n            sse_fn_epp = sse_op_table4[val][b1];\n\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n            break;\n        case 0xf7:\n            /* maskmov : we must prepare A0 */\n            if (mod != 3)\n                goto illegal_op;\n            tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EDI]);\n            gen_extu(s->aflag, cpu_A0);\n            gen_add_A0_ds_seg(s);\n\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            /* XXX: introduce a new table? */\n            sse_fn_eppt = (SSEFunc_0_eppt)sse_fn_epp;\n            sse_fn_eppt(cpu_env, cpu_ptr0, cpu_ptr1, cpu_A0);\n            break;\n        default:\n            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);\n            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);\n            sse_fn_epp(cpu_env, cpu_ptr0, cpu_ptr1);\n            break;\n        }\n        if (b == 0x2e || b == 0x2f) {\n            set_cc_op(s, CC_OP_EFLAGS);\n        }\n    }\n}\n\n/* convert one instruction. s->is_jmp is set if the translation must\n   be stopped. Return the next pc value */\nstatic target_ulong disas_insn(CPUX86State *env, DisasContext *s,\n                               target_ulong pc_start)\n{\n    int b, prefixes;\n    int shift;\n    TCGMemOp ot, aflag, dflag;\n    int modrm, reg, rm, mod, op, opreg, val;\n    target_ulong next_eip, tval;\n    int rex_w, rex_r;\n\n    s->pc_start = s->pc = pc_start;\n    prefixes = 0;\n    s->override = -1;\n    rex_w = -1;\n    rex_r = 0;\n#ifdef TARGET_X86_64\n    s->rex_x = 0;\n    s->rex_b = 0;\n    x86_64_hregs = 0;\n#endif\n    s->rip_offset = 0; /* for relative ip address */\n    s->vex_l = 0;\n    s->vex_v = 0;\n next_byte:\n    /* x86 has an upper limit of 15 bytes for an instruction. Since we\n     * do not want to decode and generate IR for an illegal\n     * instruction, the following check limits the instruction size to\n     * 25 bytes: 14 prefix + 1 opc + 6 (modrm+sib+ofs) + 4 imm */\n    if (s->pc - pc_start > 14) {\n        goto illegal_op;\n    }\n    b = cpu_ldub_code(env, s->pc);\n    s->pc++;\n    /* Collect prefixes.  */\n    switch (b) {\n    case 0xf3:\n        prefixes |= PREFIX_REPZ;\n        goto next_byte;\n    case 0xf2:\n        prefixes |= PREFIX_REPNZ;\n        goto next_byte;\n    case 0xf0:\n        prefixes |= PREFIX_LOCK;\n        goto next_byte;\n    case 0x2e:\n        s->override = R_CS;\n        goto next_byte;\n    case 0x36:\n        s->override = R_SS;\n        goto next_byte;\n    case 0x3e:\n        s->override = R_DS;\n        goto next_byte;\n    case 0x26:\n        s->override = R_ES;\n        goto next_byte;\n    case 0x64:\n        s->override = R_FS;\n        goto next_byte;\n    case 0x65:\n        s->override = R_GS;\n        goto next_byte;\n    case 0x66:\n        prefixes |= PREFIX_DATA;\n        goto next_byte;\n    case 0x67:\n        prefixes |= PREFIX_ADR;\n        goto next_byte;\n#ifdef TARGET_X86_64\n    case 0x40 ... 0x4f:\n        if (CODE64(s)) {\n            /* REX prefix */\n            rex_w = (b >> 3) & 1;\n            rex_r = (b & 0x4) << 1;\n            s->rex_x = (b & 0x2) << 2;\n            REX_B(s) = (b & 0x1) << 3;\n            x86_64_hregs = 1; /* select uniform byte register addressing */\n            goto next_byte;\n        }\n        break;\n#endif\n    case 0xc5: /* 2-byte VEX */\n    case 0xc4: /* 3-byte VEX */\n        /* VEX prefixes cannot be used except in 32-bit mode.\n           Otherwise the instruction is LES or LDS.  */\n        if (s->code32 && !s->vm86) {\n            static const int pp_prefix[4] = {\n                0, PREFIX_DATA, PREFIX_REPZ, PREFIX_REPNZ\n            };\n            int vex3, vex2 = cpu_ldub_code(env, s->pc);\n\n            if (!CODE64(s) && (vex2 & 0xc0) != 0xc0) {\n                /* 4.1.4.6: In 32-bit mode, bits [7:6] must be 11b,\n                   otherwise the instruction is LES or LDS.  */\n                break;\n            }\n            s->pc++;\n\n            /* 4.1.1-4.1.3: No preceding lock, 66, f2, f3, or rex prefixes. */\n            if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ\n                            | PREFIX_LOCK | PREFIX_DATA)) {\n                goto illegal_op;\n            }\n#ifdef TARGET_X86_64\n            if (x86_64_hregs) {\n                goto illegal_op;\n            }\n#endif\n            rex_r = (~vex2 >> 4) & 8;\n            if (b == 0xc5) {\n                vex3 = vex2;\n                b = cpu_ldub_code(env, s->pc++);\n            } else {\n#ifdef TARGET_X86_64\n                s->rex_x = (~vex2 >> 3) & 8;\n                s->rex_b = (~vex2 >> 2) & 8;\n#endif\n                vex3 = cpu_ldub_code(env, s->pc++);\n                rex_w = (vex3 >> 7) & 1;\n                switch (vex2 & 0x1f) {\n                case 0x01: /* Implied 0f leading opcode bytes.  */\n                    b = cpu_ldub_code(env, s->pc++) | 0x100;\n                    break;\n                case 0x02: /* Implied 0f 38 leading opcode bytes.  */\n                    b = 0x138;\n                    break;\n                case 0x03: /* Implied 0f 3a leading opcode bytes.  */\n                    b = 0x13a;\n                    break;\n                default:   /* Reserved for future use.  */\n                    goto unknown_op;\n                }\n            }\n            s->vex_v = (~vex3 >> 3) & 0xf;\n            s->vex_l = (vex3 >> 2) & 1;\n            prefixes |= pp_prefix[vex3 & 3] | PREFIX_VEX;\n        }\n        break;\n    }\n\n    /* Post-process prefixes.  */\n    if (CODE64(s)) {\n        /* In 64-bit mode, the default data size is 32-bit.  Select 64-bit\n           data with rex_w, and 16-bit data with 0x66; rex_w takes precedence\n           over 0x66 if both are present.  */\n        dflag = (rex_w > 0 ? MO_64 : prefixes & PREFIX_DATA ? MO_16 : MO_32);\n        /* In 64-bit mode, 0x67 selects 32-bit addressing.  */\n        aflag = (prefixes & PREFIX_ADR ? MO_32 : MO_64);\n    } else {\n        /* In 16/32-bit mode, 0x66 selects the opposite data size.  */\n        if (s->code32 ^ ((prefixes & PREFIX_DATA) != 0)) {\n            dflag = MO_32;\n        } else {\n            dflag = MO_16;\n        }\n        /* In 16/32-bit mode, 0x67 selects the opposite addressing.  */\n        if (s->code32 ^ ((prefixes & PREFIX_ADR) != 0)) {\n            aflag = MO_32;\n        }  else {\n            aflag = MO_16;\n        }\n    }\n\n    s->prefix = prefixes;\n    s->aflag = aflag;\n    s->dflag = dflag;\n\n    /* now check op code */\n reswitch:\n    switch(b) {\n    case 0x0f:\n        /**************************/\n        /* extended op code */\n        b = cpu_ldub_code(env, s->pc++) | 0x100;\n        goto reswitch;\n\n        /**************************/\n        /* arith & logic */\n    case 0x00 ... 0x05:\n    case 0x08 ... 0x0d:\n    case 0x10 ... 0x15:\n    case 0x18 ... 0x1d:\n    case 0x20 ... 0x25:\n    case 0x28 ... 0x2d:\n    case 0x30 ... 0x35:\n    case 0x38 ... 0x3d:\n        {\n            int op, f, val;\n            op = (b >> 3) & 7;\n            f = (b >> 1) & 3;\n\n            ot = mo_b_d(b, dflag);\n\n            switch(f) {\n            case 0: /* OP Ev, Gv */\n                modrm = cpu_ldub_code(env, s->pc++);\n                reg = ((modrm >> 3) & 7) | rex_r;\n                mod = (modrm >> 6) & 3;\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3) {\n                    gen_lea_modrm(env, s, modrm);\n                    opreg = OR_TMP0;\n                } else if (op == OP_XORL && rm == reg) {\n                xor_zero:\n                    /* xor reg, reg optimisation */\n                    set_cc_op(s, CC_OP_CLR);\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                    gen_op_mov_reg_v(ot, reg, cpu_T0);\n                    break;\n                } else {\n                    opreg = rm;\n                }\n                gen_op_mov_v_reg(ot, cpu_T1, reg);\n                gen_op(s, op, ot, opreg);\n                break;\n            case 1: /* OP Gv, Ev */\n                modrm = cpu_ldub_code(env, s->pc++);\n                mod = (modrm >> 6) & 3;\n                reg = ((modrm >> 3) & 7) | rex_r;\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3) {\n                    gen_lea_modrm(env, s, modrm);\n                    gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n                } else if (op == OP_XORL && rm == reg) {\n                    goto xor_zero;\n                } else {\n                    gen_op_mov_v_reg(ot, cpu_T1, rm);\n                }\n                gen_op(s, op, ot, reg);\n                break;\n            case 2: /* OP A, Iv */\n                val = insn_get(env, s, ot);\n                tcg_gen_movi_tl(cpu_T1, val);\n                gen_op(s, op, ot, OR_EAX);\n                break;\n            }\n        }\n        break;\n\n    case 0x82:\n        if (CODE64(s))\n            goto illegal_op;\n    case 0x80: /* GRP1 */\n    case 0x81:\n    case 0x83:\n        {\n            int val;\n\n            ot = mo_b_d(b, dflag);\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n            op = (modrm >> 3) & 7;\n\n            if (mod != 3) {\n                if (b == 0x83)\n                    s->rip_offset = 1;\n                else\n                    s->rip_offset = insn_const_size(ot);\n                gen_lea_modrm(env, s, modrm);\n                opreg = OR_TMP0;\n            } else {\n                opreg = rm;\n            }\n\n            switch(b) {\n            default:\n            case 0x80:\n            case 0x81:\n            case 0x82:\n                val = insn_get(env, s, ot);\n                break;\n            case 0x83:\n                val = (int8_t)insn_get(env, s, MO_8);\n                break;\n            }\n            tcg_gen_movi_tl(cpu_T1, val);\n            gen_op(s, op, ot, opreg);\n        }\n        break;\n\n        /**************************/\n        /* inc, dec, and other misc arith */\n    case 0x40 ... 0x47: /* inc Gv */\n        ot = dflag;\n        gen_inc(s, ot, OR_EAX + (b & 7), 1);\n        break;\n    case 0x48 ... 0x4f: /* dec Gv */\n        ot = dflag;\n        gen_inc(s, ot, OR_EAX + (b & 7), -1);\n        break;\n    case 0xf6: /* GRP3 */\n    case 0xf7:\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        op = (modrm >> 3) & 7;\n        if (mod != 3) {\n            if (op == 0) {\n                s->rip_offset = insn_const_size(ot);\n            }\n            gen_lea_modrm(env, s, modrm);\n            /* For those below that handle locked memory, don't load here.  */\n            if (!(s->prefix & PREFIX_LOCK)\n                || op != 2) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n\n        switch(op) {\n        case 0: /* test */\n            val = insn_get(env, s, ot);\n            tcg_gen_movi_tl(cpu_T1, val);\n            gen_op_testl_T0_T1_cc();\n            set_cc_op(s, CC_OP_LOGICB + ot);\n            break;\n        case 2: /* not */\n            if (s->prefix & PREFIX_LOCK) {\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                tcg_gen_movi_tl(cpu_T0, ~0);\n                tcg_gen_atomic_xor_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                            s->mem_index, ot | MO_LE);\n            } else {\n                tcg_gen_not_tl(cpu_T0, cpu_T0);\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n            break;\n        case 3: /* neg */\n            if (s->prefix & PREFIX_LOCK) {\n                TCGLabel *label1;\n                TCGv a0, t0, t1, t2;\n\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                a0 = tcg_temp_local_new();\n                t0 = tcg_temp_local_new();\n                label1 = gen_new_label();\n\n                tcg_gen_mov_tl(a0, cpu_A0);\n                tcg_gen_mov_tl(t0, cpu_T0);\n\n                gen_set_label(label1);\n                t1 = tcg_temp_new();\n                t2 = tcg_temp_new();\n                tcg_gen_mov_tl(t2, t0);\n                tcg_gen_neg_tl(t1, t0);\n                tcg_gen_atomic_cmpxchg_tl(t0, a0, t0, t1,\n                                          s->mem_index, ot | MO_LE);\n                tcg_temp_free(t1);\n                tcg_gen_brcond_tl(TCG_COND_NE, t0, t2, label1);\n\n                tcg_temp_free(t2);\n                tcg_temp_free(a0);\n                tcg_gen_mov_tl(cpu_T0, t0);\n                tcg_temp_free(t0);\n            } else {\n                tcg_gen_neg_tl(cpu_T0, cpu_T0);\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n            gen_op_update_neg_cc();\n            set_cc_op(s, CC_OP_SUBB + ot);\n            break;\n        case 4: /* mul */\n            switch(ot) {\n            case MO_8:\n                gen_op_mov_v_reg(MO_8, cpu_T1, R_EAX);\n                tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext8u_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_andi_tl(cpu_cc_src, cpu_T0, 0xff00);\n                set_cc_op(s, CC_OP_MULB);\n                break;\n            case MO_16:\n                gen_op_mov_v_reg(MO_16, cpu_T1, R_EAX);\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext16u_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, 16);\n                gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n                set_cc_op(s, CC_OP_MULW);\n                break;\n            default:\n            case MO_32:\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EAX]);\n                tcg_gen_mulu2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                  cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EAX], cpu_tmp2_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EDX], cpu_tmp3_i32);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULL);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                tcg_gen_mulu2_i64(cpu_regs[R_EAX], cpu_regs[R_EDX],\n                                  cpu_T0, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULQ);\n                break;\n#endif\n            }\n            break;\n        case 5: /* imul */\n            switch(ot) {\n            case MO_8:\n                gen_op_mov_v_reg(MO_8, cpu_T1, R_EAX);\n                tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext8s_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_ext8s_tl(cpu_tmp0, cpu_T0);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n                set_cc_op(s, CC_OP_MULB);\n                break;\n            case MO_16:\n                gen_op_mov_v_reg(MO_16, cpu_T1, R_EAX);\n                tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext16s_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_ext16s_tl(cpu_tmp0, cpu_T0);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, 16);\n                gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n                set_cc_op(s, CC_OP_MULW);\n                break;\n            default:\n            case MO_32:\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EAX]);\n                tcg_gen_muls2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                  cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EAX], cpu_tmp2_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EDX], cpu_tmp3_i32);\n                tcg_gen_sari_i32(cpu_tmp2_i32, cpu_tmp2_i32, 31);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_sub_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_cc_src, cpu_tmp2_i32);\n                set_cc_op(s, CC_OP_MULL);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                tcg_gen_muls2_i64(cpu_regs[R_EAX], cpu_regs[R_EDX],\n                                  cpu_T0, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_sari_tl(cpu_cc_src, cpu_regs[R_EAX], 63);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULQ);\n                break;\n#endif\n            }\n            break;\n        case 6: /* div */\n            switch(ot) {\n            case MO_8:\n                gen_helper_divb_AL(cpu_env, cpu_T0);\n                break;\n            case MO_16:\n                gen_helper_divw_AX(cpu_env, cpu_T0);\n                break;\n            default:\n            case MO_32:\n                gen_helper_divl_EAX(cpu_env, cpu_T0);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                gen_helper_divq_EAX(cpu_env, cpu_T0);\n                break;\n#endif\n            }\n            break;\n        case 7: /* idiv */\n            switch(ot) {\n            case MO_8:\n                gen_helper_idivb_AL(cpu_env, cpu_T0);\n                break;\n            case MO_16:\n                gen_helper_idivw_AX(cpu_env, cpu_T0);\n                break;\n            default:\n            case MO_32:\n                gen_helper_idivl_EAX(cpu_env, cpu_T0);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                gen_helper_idivq_EAX(cpu_env, cpu_T0);\n                break;\n#endif\n            }\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0xfe: /* GRP4 */\n    case 0xff: /* GRP5 */\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        op = (modrm >> 3) & 7;\n        if (op >= 2 && b == 0xfe) {\n            goto unknown_op;\n        }\n        if (CODE64(s)) {\n            if (op == 2 || op == 4) {\n                /* operand size for jumps is 64 bit */\n                ot = MO_64;\n            } else if (op == 3 || op == 5) {\n                ot = dflag != MO_16 ? MO_32 + (rex_w == 1) : MO_16;\n            } else if (op == 6) {\n                /* default push size is 64 bit */\n                ot = mo_pushpop(s, dflag);\n            }\n        }\n        if (mod != 3) {\n            gen_lea_modrm(env, s, modrm);\n            if (op >= 2 && op != 3 && op != 5)\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n\n        switch(op) {\n        case 0: /* inc Ev */\n            if (mod != 3)\n                opreg = OR_TMP0;\n            else\n                opreg = rm;\n            gen_inc(s, ot, opreg, 1);\n            break;\n        case 1: /* dec Ev */\n            if (mod != 3)\n                opreg = OR_TMP0;\n            else\n                opreg = rm;\n            gen_inc(s, ot, opreg, -1);\n            break;\n        case 2: /* call Ev */\n            /* XXX: optimize if memory (no 'and' is necessary) */\n            if (dflag == MO_16) {\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n            }\n            next_eip = s->pc - s->cs_base;\n            tcg_gen_movi_tl(cpu_T1, next_eip);\n            gen_push_v(s, cpu_T1);\n            gen_op_jmp_v(cpu_T0);\n            gen_bnd_jmp(s);\n            gen_eob(s);\n            break;\n        case 3: /* lcall Ev */\n            gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 1 << ot);\n            gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        do_lcall:\n            if (s->pe && !s->vm86) {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lcall_protected(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                           tcg_const_i32(dflag - 1),\n                                           tcg_const_tl(s->pc - s->cs_base));\n            } else {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lcall_real(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                      tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(s->pc - s->cs_base));\n            }\n            gen_eob(s);\n            break;\n        case 4: /* jmp Ev */\n            if (dflag == MO_16) {\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n            }\n            gen_op_jmp_v(cpu_T0);\n            gen_bnd_jmp(s);\n            gen_eob(s);\n            break;\n        case 5: /* ljmp Ev */\n            gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 1 << ot);\n            gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        do_ljmp:\n            if (s->pe && !s->vm86) {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_ljmp_protected(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                          tcg_const_tl(s->pc - s->cs_base));\n            } else {\n                gen_op_movl_seg_T0_vm(R_CS);\n                gen_op_jmp_v(cpu_T1);\n            }\n            gen_eob(s);\n            break;\n        case 6: /* push Ev */\n            gen_push_v(s, cpu_T0);\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x84: /* test Ev, Gv */\n    case 0x85:\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_op_mov_v_reg(ot, cpu_T1, reg);\n        gen_op_testl_T0_T1_cc();\n        set_cc_op(s, CC_OP_LOGICB + ot);\n        break;\n\n    case 0xa8: /* test eAX, Iv */\n    case 0xa9:\n        ot = mo_b_d(b, dflag);\n        val = insn_get(env, s, ot);\n\n        gen_op_mov_v_reg(ot, cpu_T0, OR_EAX);\n        tcg_gen_movi_tl(cpu_T1, val);\n        gen_op_testl_T0_T1_cc();\n        set_cc_op(s, CC_OP_LOGICB + ot);\n        break;\n\n    case 0x98: /* CWDE/CBW */\n        switch (dflag) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n            tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_64, R_EAX, cpu_T0);\n            break;\n#endif\n        case MO_32:\n            gen_op_mov_v_reg(MO_16, cpu_T0, R_EAX);\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_32, R_EAX, cpu_T0);\n            break;\n        case MO_16:\n            gen_op_mov_v_reg(MO_8, cpu_T0, R_EAX);\n            tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n            break;\n        default:\n            tcg_abort();\n        }\n        break;\n    case 0x99: /* CDQ/CWD */\n        switch (dflag) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_op_mov_v_reg(MO_64, cpu_T0, R_EAX);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 63);\n            gen_op_mov_reg_v(MO_64, R_EDX, cpu_T0);\n            break;\n#endif\n        case MO_32:\n            gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n            tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 31);\n            gen_op_mov_reg_v(MO_32, R_EDX, cpu_T0);\n            break;\n        case MO_16:\n            gen_op_mov_v_reg(MO_16, cpu_T0, R_EAX);\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 15);\n            gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n            break;\n        default:\n            tcg_abort();\n        }\n        break;\n    case 0x1af: /* imul Gv, Ev */\n    case 0x69: /* imul Gv, Ev, I */\n    case 0x6b:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        if (b == 0x69)\n            s->rip_offset = insn_const_size(ot);\n        else if (b == 0x6b)\n            s->rip_offset = 1;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        if (b == 0x69) {\n            val = insn_get(env, s, ot);\n            tcg_gen_movi_tl(cpu_T1, val);\n        } else if (b == 0x6b) {\n            val = (int8_t)insn_get(env, s, MO_8);\n            tcg_gen_movi_tl(cpu_T1, val);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T1, reg);\n        }\n        switch (ot) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            tcg_gen_muls2_i64(cpu_regs[reg], cpu_T1, cpu_T0, cpu_T1);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[reg]);\n            tcg_gen_sari_tl(cpu_cc_src, cpu_cc_dst, 63);\n            tcg_gen_sub_tl(cpu_cc_src, cpu_cc_src, cpu_T1);\n            break;\n#endif\n        case MO_32:\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n            tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n            tcg_gen_muls2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                              cpu_tmp2_i32, cpu_tmp3_i32);\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            tcg_gen_sari_i32(cpu_tmp2_i32, cpu_tmp2_i32, 31);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[reg]);\n            tcg_gen_sub_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n            tcg_gen_extu_i32_tl(cpu_cc_src, cpu_tmp2_i32);\n            break;\n        default:\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            tcg_gen_ext16s_tl(cpu_T1, cpu_T1);\n            /* XXX: use 32 bit mul which could be faster */\n            tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n            tcg_gen_ext16s_tl(cpu_tmp0, cpu_T0);\n            tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n            break;\n        }\n        set_cc_op(s, CC_OP_MULB + ot);\n        break;\n    case 0x1c0:\n    case 0x1c1: /* xadd Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        gen_op_mov_v_reg(ot, cpu_T0, reg);\n        if (mod == 3) {\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_mov_v_reg(ot, cpu_T1, rm);\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            gen_lea_modrm(env, s, modrm);\n            if (s->prefix & PREFIX_LOCK) {\n                tcg_gen_atomic_fetch_add_tl(cpu_T1, cpu_A0, cpu_T0,\n                                            s->mem_index, ot | MO_LE);\n                tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            } else {\n                gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n                tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n            }\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        }\n        gen_op_update2_cc();\n        set_cc_op(s, CC_OP_ADDB + ot);\n        break;\n    case 0x1b0:\n    case 0x1b1: /* cmpxchg Ev, Gv */\n        {\n            TCGv oldv, newv, cmpv;\n\n            ot = mo_b_d(b, dflag);\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            oldv = tcg_temp_new();\n            newv = tcg_temp_new();\n            cmpv = tcg_temp_new();\n            gen_op_mov_v_reg(ot, newv, reg);\n            tcg_gen_mov_tl(cmpv, cpu_regs[R_EAX]);\n\n            if (s->prefix & PREFIX_LOCK) {\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_atomic_cmpxchg_tl(oldv, cpu_A0, cmpv, newv,\n                                          s->mem_index, ot | MO_LE);\n                gen_op_mov_reg_v(ot, R_EAX, oldv);\n            } else {\n                if (mod == 3) {\n                    rm = (modrm & 7) | REX_B(s);\n                    gen_op_mov_v_reg(ot, oldv, rm);\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    gen_op_ld_v(s, ot, oldv, cpu_A0);\n                    rm = 0; /* avoid warning */\n                }\n                gen_extu(ot, oldv);\n                gen_extu(ot, cmpv);\n                /* store value = (old == cmp ? new : old);  */\n                tcg_gen_movcond_tl(TCG_COND_EQ, newv, oldv, cmpv, newv, oldv);\n                if (mod == 3) {\n                    gen_op_mov_reg_v(ot, R_EAX, oldv);\n                    gen_op_mov_reg_v(ot, rm, newv);\n                } else {\n                    /* Perform an unconditional store cycle like physical cpu;\n                       must be before changing accumulator to ensure\n                       idempotency if the store faults and the instruction\n                       is restarted */\n                    gen_op_st_v(s, ot, newv, cpu_A0);\n                    gen_op_mov_reg_v(ot, R_EAX, oldv);\n                }\n            }\n            tcg_gen_mov_tl(cpu_cc_src, oldv);\n            tcg_gen_mov_tl(cpu_cc_srcT, cmpv);\n            tcg_gen_sub_tl(cpu_cc_dst, cmpv, oldv);\n            set_cc_op(s, CC_OP_SUBB + ot);\n            tcg_temp_free(oldv);\n            tcg_temp_free(newv);\n            tcg_temp_free(cmpv);\n        }\n        break;\n    case 0x1c7: /* cmpxchg8b */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if ((mod == 3) || ((modrm & 0x38) != 0x8))\n            goto illegal_op;\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            if (!(s->cpuid_ext_features & CPUID_EXT_CX16))\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if ((s->prefix & PREFIX_LOCK) && parallel_cpus) {\n                gen_helper_cmpxchg16b(cpu_env, cpu_A0);\n            } else {\n                gen_helper_cmpxchg16b_unlocked(cpu_env, cpu_A0);\n            }\n        } else\n#endif        \n        {\n            if (!(s->cpuid_features & CPUID_CX8))\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if ((s->prefix & PREFIX_LOCK) && parallel_cpus) {\n                gen_helper_cmpxchg8b(cpu_env, cpu_A0);\n            } else {\n                gen_helper_cmpxchg8b_unlocked(cpu_env, cpu_A0);\n            }\n        }\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n\n        /**************************/\n        /* push/pop */\n    case 0x50 ... 0x57: /* push */\n        gen_op_mov_v_reg(MO_32, cpu_T0, (b & 7) | REX_B(s));\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x58 ... 0x5f: /* pop */\n        ot = gen_pop_T0(s);\n        /* NOTE: order is important for pop %sp */\n        gen_pop_update(s, ot);\n        gen_op_mov_reg_v(ot, (b & 7) | REX_B(s), cpu_T0);\n        break;\n    case 0x60: /* pusha */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_pusha(s);\n        break;\n    case 0x61: /* popa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_popa(s);\n        break;\n    case 0x68: /* push Iv */\n    case 0x6a:\n        ot = mo_pushpop(s, dflag);\n        if (b == 0x68)\n            val = insn_get(env, s, ot);\n        else\n            val = (int8_t)insn_get(env, s, MO_8);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x8f: /* pop Ev */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        ot = gen_pop_T0(s);\n        if (mod == 3) {\n            /* NOTE: order is important for pop %sp */\n            gen_pop_update(s, ot);\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            /* NOTE: order is important too for MMU exceptions */\n            s->popl_esp_hack = 1 << ot;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            s->popl_esp_hack = 0;\n            gen_pop_update(s, ot);\n        }\n        break;\n    case 0xc8: /* enter */\n        {\n            int level;\n            val = cpu_lduw_code(env, s->pc);\n            s->pc += 2;\n            level = cpu_ldub_code(env, s->pc++);\n            gen_enter(s, val, level);\n        }\n        break;\n    case 0xc9: /* leave */\n        gen_leave(s);\n        break;\n    case 0x06: /* push es */\n    case 0x0e: /* push cs */\n    case 0x16: /* push ss */\n    case 0x1e: /* push ds */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_op_movl_T0_seg(b >> 3);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x1a0: /* push fs */\n    case 0x1a8: /* push gs */\n        gen_op_movl_T0_seg((b >> 3) & 7);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x07: /* pop es */\n    case 0x17: /* pop ss */\n    case 0x1f: /* pop ds */\n        if (CODE64(s))\n            goto illegal_op;\n        reg = b >> 3;\n        ot = gen_pop_T0(s);\n        gen_movl_seg_T0(s, reg);\n        gen_pop_update(s, ot);\n        /* Note that reg == R_SS in gen_movl_seg_T0 always sets is_jmp.  */\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            if (reg == R_SS) {\n                s->tf = 0;\n                gen_eob_inhibit_irq(s, true);\n            } else {\n                gen_eob(s);\n            }\n        }\n        break;\n    case 0x1a1: /* pop fs */\n    case 0x1a9: /* pop gs */\n        ot = gen_pop_T0(s);\n        gen_movl_seg_T0(s, (b >> 3) & 7);\n        gen_pop_update(s, ot);\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n\n        /**************************/\n        /* mov */\n    case 0x88:\n    case 0x89: /* mov Gv, Ev */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        /* generate a generic store */\n        gen_ldst_modrm(env, s, modrm, ot, reg, 1);\n        break;\n    case 0xc6:\n    case 0xc7: /* mov Ev, Iv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod != 3) {\n            s->rip_offset = insn_const_size(ot);\n            gen_lea_modrm(env, s, modrm);\n        }\n        val = insn_get(env, s, ot);\n        tcg_gen_movi_tl(cpu_T0, val);\n        if (mod != 3) {\n            gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_mov_reg_v(ot, (modrm & 7) | REX_B(s), cpu_T0);\n        }\n        break;\n    case 0x8a:\n    case 0x8b: /* mov Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n        break;\n    case 0x8e: /* mov seg, Gv */\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        if (reg >= 6 || reg == R_CS)\n            goto illegal_op;\n        gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n        gen_movl_seg_T0(s, reg);\n        /* Note that reg == R_SS in gen_movl_seg_T0 always sets is_jmp.  */\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            if (reg == R_SS) {\n                s->tf = 0;\n                gen_eob_inhibit_irq(s, true);\n            } else {\n                gen_eob(s);\n            }\n        }\n        break;\n    case 0x8c: /* mov Gv, seg */\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        if (reg >= 6)\n            goto illegal_op;\n        gen_op_movl_T0_seg(reg);\n        ot = mod == 3 ? dflag : MO_16;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n        break;\n\n    case 0x1b6: /* movzbS Gv, Eb */\n    case 0x1b7: /* movzwS Gv, Eb */\n    case 0x1be: /* movsbS Gv, Eb */\n    case 0x1bf: /* movswS Gv, Eb */\n        {\n            TCGMemOp d_ot;\n            TCGMemOp s_ot;\n\n            /* d_ot is the size of destination */\n            d_ot = dflag;\n            /* ot is the size of source */\n            ot = (b & 1) + MO_8;\n            /* s_ot is the sign+size of source */\n            s_ot = b & 8 ? MO_SIGN | ot : ot;\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n\n            if (mod == 3) {\n                if (s_ot == MO_SB && byte_reg_is_xH(rm)) {\n                    tcg_gen_sextract_tl(cpu_T0, cpu_regs[rm - 4], 8, 8);\n                } else {\n                    gen_op_mov_v_reg(ot, cpu_T0, rm);\n                    switch (s_ot) {\n                    case MO_UB:\n                        tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n                        break;\n                    case MO_SB:\n                        tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n                        break;\n                    case MO_UW:\n                        tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n                        break;\n                    default:\n                    case MO_SW:\n                        tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n                        break;\n                    }\n                }\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            } else {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, s_ot, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            }\n        }\n        break;\n\n    case 0x8d: /* lea */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        reg = ((modrm >> 3) & 7) | rex_r;\n        {\n            AddressParts a = gen_lea_modrm_0(env, s, modrm);\n            TCGv ea = gen_lea_modrm_1(a);\n            gen_lea_v_seg(s, s->aflag, ea, -1, -1);\n            gen_op_mov_reg_v(dflag, reg, cpu_A0);\n        }\n        break;\n\n    case 0xa0: /* mov EAX, Ov */\n    case 0xa1:\n    case 0xa2: /* mov Ov, EAX */\n    case 0xa3:\n        {\n            target_ulong offset_addr;\n\n            ot = mo_b_d(b, dflag);\n            switch (s->aflag) {\n#ifdef TARGET_X86_64\n            case MO_64:\n                offset_addr = cpu_ldq_code(env, s->pc);\n                s->pc += 8;\n                break;\n#endif\n            default:\n                offset_addr = insn_get(env, s, s->aflag);\n                break;\n            }\n            tcg_gen_movi_tl(cpu_A0, offset_addr);\n            gen_add_A0_ds_seg(s);\n            if ((b & 2) == 0) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(ot, R_EAX, cpu_T0);\n            } else {\n                gen_op_mov_v_reg(ot, cpu_T0, R_EAX);\n                gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n            }\n        }\n        break;\n    case 0xd7: /* xlat */\n        tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EBX]);\n        tcg_gen_ext8u_tl(cpu_T0, cpu_regs[R_EAX]);\n        tcg_gen_add_tl(cpu_A0, cpu_A0, cpu_T0);\n        gen_extu(s->aflag, cpu_A0);\n        gen_add_A0_ds_seg(s);\n        gen_op_ld_v(s, MO_8, cpu_T0, cpu_A0);\n        gen_op_mov_reg_v(MO_8, R_EAX, cpu_T0);\n        break;\n    case 0xb0 ... 0xb7: /* mov R, Ib */\n        val = insn_get(env, s, MO_8);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_op_mov_reg_v(MO_8, (b & 7) | REX_B(s), cpu_T0);\n        break;\n    case 0xb8 ... 0xbf: /* mov R, Iv */\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            uint64_t tmp;\n            /* 64 bit case */\n            tmp = cpu_ldq_code(env, s->pc);\n            s->pc += 8;\n            reg = (b & 7) | REX_B(s);\n            tcg_gen_movi_tl(cpu_T0, tmp);\n            gen_op_mov_reg_v(MO_64, reg, cpu_T0);\n        } else\n#endif\n        {\n            ot = dflag;\n            val = insn_get(env, s, ot);\n            reg = (b & 7) | REX_B(s);\n            tcg_gen_movi_tl(cpu_T0, val);\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n        }\n        break;\n\n    case 0x91 ... 0x97: /* xchg R, EAX */\n    do_xchg_reg_eax:\n        ot = dflag;\n        reg = (b & 7) | REX_B(s);\n        rm = R_EAX;\n        goto do_xchg_reg;\n    case 0x86:\n    case 0x87: /* xchg Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3) {\n            rm = (modrm & 7) | REX_B(s);\n        do_xchg_reg:\n            gen_op_mov_v_reg(ot, cpu_T0, reg);\n            gen_op_mov_v_reg(ot, cpu_T1, rm);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        } else {\n            gen_lea_modrm(env, s, modrm);\n            gen_op_mov_v_reg(ot, cpu_T0, reg);\n            /* for xchg, lock is implicit */\n            tcg_gen_atomic_xchg_tl(cpu_T1, cpu_A0, cpu_T0,\n                                   s->mem_index, ot | MO_LE);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        }\n        break;\n    case 0xc4: /* les Gv */\n        /* In CODE64 this is VEX3; see above.  */\n        op = R_ES;\n        goto do_lxx;\n    case 0xc5: /* lds Gv */\n        /* In CODE64 this is VEX2; see above.  */\n        op = R_DS;\n        goto do_lxx;\n    case 0x1b2: /* lss Gv */\n        op = R_SS;\n        goto do_lxx;\n    case 0x1b4: /* lfs Gv */\n        op = R_FS;\n        goto do_lxx;\n    case 0x1b5: /* lgs Gv */\n        op = R_GS;\n    do_lxx:\n        ot = dflag != MO_16 ? MO_32 : MO_16;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_lea_modrm(env, s, modrm);\n        gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n        gen_add_A0_im(s, 1 << ot);\n        /* load the segment first to handle exceptions properly */\n        gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        gen_movl_seg_T0(s, op);\n        /* then put the data */\n        gen_op_mov_reg_v(ot, reg, cpu_T1);\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n\n        /************************/\n        /* shifts */\n    case 0xc0:\n    case 0xc1:\n        /* shift Ev,Ib */\n        shift = 2;\n    grp2:\n        {\n            ot = mo_b_d(b, dflag);\n            modrm = cpu_ldub_code(env, s->pc++);\n            mod = (modrm >> 6) & 3;\n            op = (modrm >> 3) & 7;\n\n            if (mod != 3) {\n                if (shift == 2) {\n                    s->rip_offset = 1;\n                }\n                gen_lea_modrm(env, s, modrm);\n                opreg = OR_TMP0;\n            } else {\n                opreg = (modrm & 7) | REX_B(s);\n            }\n\n            /* simpler op */\n            if (shift == 0) {\n                gen_shift(s, op, ot, opreg, OR_ECX);\n            } else {\n                if (shift == 2) {\n                    shift = cpu_ldub_code(env, s->pc++);\n                }\n                gen_shifti(s, op, ot, opreg, shift);\n            }\n        }\n        break;\n    case 0xd0:\n    case 0xd1:\n        /* shift Ev,1 */\n        shift = 1;\n        goto grp2;\n    case 0xd2:\n    case 0xd3:\n        /* shift Ev,cl */\n        shift = 0;\n        goto grp2;\n\n    case 0x1a4: /* shld imm */\n        op = 0;\n        shift = 1;\n        goto do_shiftd;\n    case 0x1a5: /* shld cl */\n        op = 0;\n        shift = 0;\n        goto do_shiftd;\n    case 0x1ac: /* shrd imm */\n        op = 1;\n        shift = 1;\n        goto do_shiftd;\n    case 0x1ad: /* shrd cl */\n        op = 1;\n        shift = 0;\n    do_shiftd:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        if (mod != 3) {\n            gen_lea_modrm(env, s, modrm);\n            opreg = OR_TMP0;\n        } else {\n            opreg = rm;\n        }\n        gen_op_mov_v_reg(ot, cpu_T1, reg);\n\n        if (shift) {\n            TCGv imm = tcg_const_tl(cpu_ldub_code(env, s->pc++));\n            gen_shiftd_rm_T1(s, ot, opreg, op, imm);\n            tcg_temp_free(imm);\n        } else {\n            gen_shiftd_rm_T1(s, ot, opreg, op, cpu_regs[R_ECX]);\n        }\n        break;\n\n        /************************/\n        /* floats */\n    case 0xd8 ... 0xdf:\n        if (s->flags & (HF_EM_MASK | HF_TS_MASK)) {\n            /* if CR0.EM or CR0.TS are set, generate an FPU exception */\n            /* XXX: what to do if illegal op ? */\n            gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n            break;\n        }\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = modrm & 7;\n        op = ((b & 7) << 3) | ((modrm >> 3) & 7);\n        if (mod != 3) {\n            /* memory op */\n            gen_lea_modrm(env, s, modrm);\n            switch(op) {\n            case 0x00 ... 0x07: /* fxxxs */\n            case 0x10 ... 0x17: /* fixxxl */\n            case 0x20 ... 0x27: /* fxxxl */\n            case 0x30 ... 0x37: /* fixxx */\n                {\n                    int op1;\n                    op1 = op & 7;\n\n                    switch(op >> 4) {\n                    case 0:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_flds_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 1:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_fildl_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 2:\n                        tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        gen_helper_fldl_FT0(cpu_env, cpu_tmp1_i64);\n                        break;\n                    case 3:\n                    default:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LESW);\n                        gen_helper_fildl_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    }\n\n                    gen_helper_fp_arith_ST0_FT0(op1);\n                    if (op1 == 3) {\n                        /* fcomp needs pop */\n                        gen_helper_fpop(cpu_env);\n                    }\n                }\n                break;\n            case 0x08: /* flds */\n            case 0x0a: /* fsts */\n            case 0x0b: /* fstps */\n            case 0x18 ... 0x1b: /* fildl, fisttpl, fistl, fistpl */\n            case 0x28 ... 0x2b: /* fldl, fisttpll, fstl, fstpl */\n            case 0x38 ... 0x3b: /* filds, fisttps, fists, fistps */\n                switch(op & 7) {\n                case 0:\n                    switch(op >> 4) {\n                    case 0:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_flds_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 1:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_fildl_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 2:\n                        tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        gen_helper_fldl_ST0(cpu_env, cpu_tmp1_i64);\n                        break;\n                    case 3:\n                    default:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LESW);\n                        gen_helper_fildl_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    }\n                    break;\n                case 1:\n                    /* XXX: the corresponding CPUID bit must be tested ! */\n                    switch(op >> 4) {\n                    case 1:\n                        gen_helper_fisttl_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 2:\n                        gen_helper_fisttll_ST0(cpu_tmp1_i64, cpu_env);\n                        tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        break;\n                    case 3:\n                    default:\n                        gen_helper_fistt_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUW);\n                        break;\n                    }\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    switch(op >> 4) {\n                    case 0:\n                        gen_helper_fsts_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 1:\n                        gen_helper_fistl_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 2:\n                        gen_helper_fstl_ST0(cpu_tmp1_i64, cpu_env);\n                        tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        break;\n                    case 3:\n                    default:\n                        gen_helper_fist_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUW);\n                        break;\n                    }\n                    if ((op & 7) == 3)\n                        gen_helper_fpop(cpu_env);\n                    break;\n                }\n                break;\n            case 0x0c: /* fldenv mem */\n                gen_helper_fldenv(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x0d: /* fldcw mem */\n                tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                gen_helper_fldcw(cpu_env, cpu_tmp2_i32);\n                break;\n            case 0x0e: /* fnstenv mem */\n                gen_helper_fstenv(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x0f: /* fnstcw mem */\n                gen_helper_fnstcw(cpu_tmp2_i32, cpu_env);\n                tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                break;\n            case 0x1d: /* fldt mem */\n                gen_helper_fldt_ST0(cpu_env, cpu_A0);\n                break;\n            case 0x1f: /* fstpt mem */\n                gen_helper_fstt_ST0(cpu_env, cpu_A0);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x2c: /* frstor mem */\n                gen_helper_frstor(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x2e: /* fnsave mem */\n                gen_helper_fsave(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x2f: /* fnstsw mem */\n                gen_helper_fnstsw(cpu_tmp2_i32, cpu_env);\n                tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                break;\n            case 0x3c: /* fbld */\n                gen_helper_fbld_ST0(cpu_env, cpu_A0);\n                break;\n            case 0x3e: /* fbstp */\n                gen_helper_fbst_ST0(cpu_env, cpu_A0);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x3d: /* fildll */\n                tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n                gen_helper_fildll_ST0(cpu_env, cpu_tmp1_i64);\n                break;\n            case 0x3f: /* fistpll */\n                gen_helper_fistll_ST0(cpu_tmp1_i64, cpu_env);\n                tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n                gen_helper_fpop(cpu_env);\n                break;\n            default:\n                goto unknown_op;\n            }\n        } else {\n            /* register float ops */\n            opreg = rm;\n\n            switch(op) {\n            case 0x08: /* fld sti */\n                gen_helper_fpush(cpu_env);\n                gen_helper_fmov_ST0_STN(cpu_env,\n                                        tcg_const_i32((opreg + 1) & 7));\n                break;\n            case 0x09: /* fxchg sti */\n            case 0x29: /* fxchg4 sti, undocumented op */\n            case 0x39: /* fxchg7 sti, undocumented op */\n                gen_helper_fxchg_ST0_STN(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x0a: /* grp d9/2 */\n                switch(rm) {\n                case 0: /* fnop */\n                    /* check exceptions (FreeBSD FPU probe) */\n                    gen_helper_fwait(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x0c: /* grp d9/4 */\n                switch(rm) {\n                case 0: /* fchs */\n                    gen_helper_fchs_ST0(cpu_env);\n                    break;\n                case 1: /* fabs */\n                    gen_helper_fabs_ST0(cpu_env);\n                    break;\n                case 4: /* ftst */\n                    gen_helper_fldz_FT0(cpu_env);\n                    gen_helper_fcom_ST0_FT0(cpu_env);\n                    break;\n                case 5: /* fxam */\n                    gen_helper_fxam_ST0(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x0d: /* grp d9/5 */\n                {\n                    switch(rm) {\n                    case 0:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fld1_ST0(cpu_env);\n                        break;\n                    case 1:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldl2t_ST0(cpu_env);\n                        break;\n                    case 2:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldl2e_ST0(cpu_env);\n                        break;\n                    case 3:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldpi_ST0(cpu_env);\n                        break;\n                    case 4:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldlg2_ST0(cpu_env);\n                        break;\n                    case 5:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldln2_ST0(cpu_env);\n                        break;\n                    case 6:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldz_ST0(cpu_env);\n                        break;\n                    default:\n                        goto unknown_op;\n                    }\n                }\n                break;\n            case 0x0e: /* grp d9/6 */\n                switch(rm) {\n                case 0: /* f2xm1 */\n                    gen_helper_f2xm1(cpu_env);\n                    break;\n                case 1: /* fyl2x */\n                    gen_helper_fyl2x(cpu_env);\n                    break;\n                case 2: /* fptan */\n                    gen_helper_fptan(cpu_env);\n                    break;\n                case 3: /* fpatan */\n                    gen_helper_fpatan(cpu_env);\n                    break;\n                case 4: /* fxtract */\n                    gen_helper_fxtract(cpu_env);\n                    break;\n                case 5: /* fprem1 */\n                    gen_helper_fprem1(cpu_env);\n                    break;\n                case 6: /* fdecstp */\n                    gen_helper_fdecstp(cpu_env);\n                    break;\n                default:\n                case 7: /* fincstp */\n                    gen_helper_fincstp(cpu_env);\n                    break;\n                }\n                break;\n            case 0x0f: /* grp d9/7 */\n                switch(rm) {\n                case 0: /* fprem */\n                    gen_helper_fprem(cpu_env);\n                    break;\n                case 1: /* fyl2xp1 */\n                    gen_helper_fyl2xp1(cpu_env);\n                    break;\n                case 2: /* fsqrt */\n                    gen_helper_fsqrt(cpu_env);\n                    break;\n                case 3: /* fsincos */\n                    gen_helper_fsincos(cpu_env);\n                    break;\n                case 5: /* fscale */\n                    gen_helper_fscale(cpu_env);\n                    break;\n                case 4: /* frndint */\n                    gen_helper_frndint(cpu_env);\n                    break;\n                case 6: /* fsin */\n                    gen_helper_fsin(cpu_env);\n                    break;\n                default:\n                case 7: /* fcos */\n                    gen_helper_fcos(cpu_env);\n                    break;\n                }\n                break;\n            case 0x00: case 0x01: case 0x04 ... 0x07: /* fxxx st, sti */\n            case 0x20: case 0x21: case 0x24 ... 0x27: /* fxxx sti, st */\n            case 0x30: case 0x31: case 0x34 ... 0x37: /* fxxxp sti, st */\n                {\n                    int op1;\n\n                    op1 = op & 7;\n                    if (op >= 0x20) {\n                        gen_helper_fp_arith_STN_ST0(op1, opreg);\n                        if (op >= 0x30)\n                            gen_helper_fpop(cpu_env);\n                    } else {\n                        gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                        gen_helper_fp_arith_ST0_FT0(op1);\n                    }\n                }\n                break;\n            case 0x02: /* fcom */\n            case 0x22: /* fcom2, undocumented op */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcom_ST0_FT0(cpu_env);\n                break;\n            case 0x03: /* fcomp */\n            case 0x23: /* fcomp3, undocumented op */\n            case 0x32: /* fcomp5, undocumented op */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcom_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x15: /* da/5 */\n                switch(rm) {\n                case 1: /* fucompp */\n                    gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(1));\n                    gen_helper_fucom_ST0_FT0(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x1c:\n                switch(rm) {\n                case 0: /* feni (287 only, just do nop here) */\n                    break;\n                case 1: /* fdisi (287 only, just do nop here) */\n                    break;\n                case 2: /* fclex */\n                    gen_helper_fclex(cpu_env);\n                    break;\n                case 3: /* fninit */\n                    gen_helper_fninit(cpu_env);\n                    break;\n                case 4: /* fsetpm (287 only, just do nop here) */\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x1d: /* fucomi */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucomi_ST0_FT0(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x1e: /* fcomi */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcomi_ST0_FT0(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x28: /* ffree sti */\n                gen_helper_ffree_STN(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x2a: /* fst sti */\n                gen_helper_fmov_STN_ST0(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x2b: /* fstp sti */\n            case 0x0b: /* fstp1 sti, undocumented op */\n            case 0x3a: /* fstp8 sti, undocumented op */\n            case 0x3b: /* fstp9 sti, undocumented op */\n                gen_helper_fmov_STN_ST0(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x2c: /* fucom st(i) */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucom_ST0_FT0(cpu_env);\n                break;\n            case 0x2d: /* fucomp st(i) */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucom_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x33: /* de/3 */\n                switch(rm) {\n                case 1: /* fcompp */\n                    gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(1));\n                    gen_helper_fcom_ST0_FT0(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x38: /* ffreep sti, undocumented op */\n                gen_helper_ffree_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x3c: /* df/4 */\n                switch(rm) {\n                case 0:\n                    gen_helper_fnstsw(cpu_tmp2_i32, cpu_env);\n                    tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n                    gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x3d: /* fucomip */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucomi_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x3e: /* fcomip */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcomi_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x10 ... 0x13: /* fcmovxx */\n            case 0x18 ... 0x1b:\n                {\n                    int op1;\n                    TCGLabel *l1;\n                    static const uint8_t fcmov_cc[8] = {\n                        (JCC_B << 1),\n                        (JCC_Z << 1),\n                        (JCC_BE << 1),\n                        (JCC_P << 1),\n                    };\n\n                    if (!(s->cpuid_features & CPUID_CMOV)) {\n                        goto illegal_op;\n                    }\n                    op1 = fcmov_cc[op & 3] | (((op >> 3) & 1) ^ 1);\n                    l1 = gen_new_label();\n                    gen_jcc1_noeob(s, op1, l1);\n                    gen_helper_fmov_ST0_STN(cpu_env, tcg_const_i32(opreg));\n                    gen_set_label(l1);\n                }\n                break;\n            default:\n                goto unknown_op;\n            }\n        }\n        break;\n        /************************/\n        /* string ops */\n\n    case 0xa4: /* movsS */\n    case 0xa5:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_movs(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_movs(s, ot);\n        }\n        break;\n\n    case 0xaa: /* stosS */\n    case 0xab:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_stos(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_stos(s, ot);\n        }\n        break;\n    case 0xac: /* lodsS */\n    case 0xad:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_lods(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_lods(s, ot);\n        }\n        break;\n    case 0xae: /* scasS */\n    case 0xaf:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & PREFIX_REPNZ) {\n            gen_repz_scas(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 1);\n        } else if (prefixes & PREFIX_REPZ) {\n            gen_repz_scas(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 0);\n        } else {\n            gen_scas(s, ot);\n        }\n        break;\n\n    case 0xa6: /* cmpsS */\n    case 0xa7:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & PREFIX_REPNZ) {\n            gen_repz_cmps(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 1);\n        } else if (prefixes & PREFIX_REPZ) {\n            gen_repz_cmps(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 0);\n        } else {\n            gen_cmps(s, ot);\n        }\n        break;\n    case 0x6c: /* insS */\n    case 0x6d:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base, \n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes) | 4);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_ins(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_ins(s, ot);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n        }\n        break;\n    case 0x6e: /* outsS */\n    case 0x6f:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes) | 4);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_outs(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_outs(s, ot);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n        }\n        break;\n\n        /************************/\n        /* port I/O */\n\n    case 0xe4:\n    case 0xe5:\n        ot = mo_b_d32(b, dflag);\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes));\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_movi_i32(cpu_tmp2_i32, val);\n        gen_helper_in_func(ot, cpu_T1, cpu_tmp2_i32);\n        gen_op_mov_reg_v(ot, R_EAX, cpu_T1);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xe6:\n    case 0xe7:\n        ot = mo_b_d32(b, dflag);\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes));\n        gen_op_mov_v_reg(ot, cpu_T1, R_EAX);\n\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_movi_i32(cpu_tmp2_i32, val);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xec:\n    case 0xed:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes));\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        gen_helper_in_func(ot, cpu_T1, cpu_tmp2_i32);\n        gen_op_mov_reg_v(ot, R_EAX, cpu_T1);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xee:\n    case 0xef:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes));\n        gen_op_mov_v_reg(ot, cpu_T1, R_EAX);\n\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n\n        /************************/\n        /* control */\n    case 0xc2: /* ret im */\n        val = cpu_ldsw_code(env, s->pc);\n        s->pc += 2;\n        ot = gen_pop_T0(s);\n        gen_stack_update(s, val + (1 << ot));\n        /* Note that gen_pop_T0 uses a zero-extending load.  */\n        gen_op_jmp_v(cpu_T0);\n        gen_bnd_jmp(s);\n        gen_eob(s);\n        break;\n    case 0xc3: /* ret */\n        ot = gen_pop_T0(s);\n        gen_pop_update(s, ot);\n        /* Note that gen_pop_T0 uses a zero-extending load.  */\n        gen_op_jmp_v(cpu_T0);\n        gen_bnd_jmp(s);\n        gen_eob(s);\n        break;\n    case 0xca: /* lret im */\n        val = cpu_ldsw_code(env, s->pc);\n        s->pc += 2;\n    do_lret:\n        if (s->pe && !s->vm86) {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_lret_protected(cpu_env, tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(val));\n        } else {\n            gen_stack_A0(s);\n            /* pop offset */\n            gen_op_ld_v(s, dflag, cpu_T0, cpu_A0);\n            /* NOTE: keeping EIP updated is not a problem in case of\n               exception */\n            gen_op_jmp_v(cpu_T0);\n            /* pop selector */\n            gen_add_A0_im(s, 1 << dflag);\n            gen_op_ld_v(s, dflag, cpu_T0, cpu_A0);\n            gen_op_movl_seg_T0_vm(R_CS);\n            /* add stack offset */\n            gen_stack_update(s, val + (2 << dflag));\n        }\n        gen_eob(s);\n        break;\n    case 0xcb: /* lret */\n        val = 0;\n        goto do_lret;\n    case 0xcf: /* iret */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_IRET);\n        if (!s->pe) {\n            /* real mode */\n            gen_helper_iret_real(cpu_env, tcg_const_i32(dflag - 1));\n            set_cc_op(s, CC_OP_EFLAGS);\n        } else if (s->vm86) {\n            if (s->iopl != 3) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_helper_iret_real(cpu_env, tcg_const_i32(dflag - 1));\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n        } else {\n            gen_helper_iret_protected(cpu_env, tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(s->pc - s->cs_base));\n            set_cc_op(s, CC_OP_EFLAGS);\n        }\n        gen_eob(s);\n        break;\n    case 0xe8: /* call im */\n        {\n            if (dflag != MO_16) {\n                tval = (int32_t)insn_get(env, s, MO_32);\n            } else {\n                tval = (int16_t)insn_get(env, s, MO_16);\n            }\n            next_eip = s->pc - s->cs_base;\n            tval += next_eip;\n            if (dflag == MO_16) {\n                tval &= 0xffff;\n            } else if (!CODE64(s)) {\n                tval &= 0xffffffff;\n            }\n            tcg_gen_movi_tl(cpu_T0, next_eip);\n            gen_push_v(s, cpu_T0);\n            gen_bnd_jmp(s);\n            gen_jmp(s, tval);\n        }\n        break;\n    case 0x9a: /* lcall im */\n        {\n            unsigned int selector, offset;\n\n            if (CODE64(s))\n                goto illegal_op;\n            ot = dflag;\n            offset = insn_get(env, s, ot);\n            selector = insn_get(env, s, MO_16);\n\n            tcg_gen_movi_tl(cpu_T0, selector);\n            tcg_gen_movi_tl(cpu_T1, offset);\n        }\n        goto do_lcall;\n    case 0xe9: /* jmp im */\n        if (dflag != MO_16) {\n            tval = (int32_t)insn_get(env, s, MO_32);\n        } else {\n            tval = (int16_t)insn_get(env, s, MO_16);\n        }\n        tval += s->pc - s->cs_base;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        } else if (!CODE64(s)) {\n            tval &= 0xffffffff;\n        }\n        gen_bnd_jmp(s);\n        gen_jmp(s, tval);\n        break;\n    case 0xea: /* ljmp im */\n        {\n            unsigned int selector, offset;\n\n            if (CODE64(s))\n                goto illegal_op;\n            ot = dflag;\n            offset = insn_get(env, s, ot);\n            selector = insn_get(env, s, MO_16);\n\n            tcg_gen_movi_tl(cpu_T0, selector);\n            tcg_gen_movi_tl(cpu_T1, offset);\n        }\n        goto do_ljmp;\n    case 0xeb: /* jmp Jb */\n        tval = (int8_t)insn_get(env, s, MO_8);\n        tval += s->pc - s->cs_base;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        }\n        gen_jmp(s, tval);\n        break;\n    case 0x70 ... 0x7f: /* jcc Jb */\n        tval = (int8_t)insn_get(env, s, MO_8);\n        goto do_jcc;\n    case 0x180 ... 0x18f: /* jcc Jv */\n        if (dflag != MO_16) {\n            tval = (int32_t)insn_get(env, s, MO_32);\n        } else {\n            tval = (int16_t)insn_get(env, s, MO_16);\n        }\n    do_jcc:\n        next_eip = s->pc - s->cs_base;\n        tval += next_eip;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        }\n        gen_bnd_jmp(s);\n        gen_jcc(s, b, tval, next_eip);\n        break;\n\n    case 0x190 ... 0x19f: /* setcc Gv */\n        modrm = cpu_ldub_code(env, s->pc++);\n        gen_setcc1(s, b, cpu_T0);\n        gen_ldst_modrm(env, s, modrm, MO_8, OR_TMP0, 1);\n        break;\n    case 0x140 ... 0x14f: /* cmov Gv, Ev */\n        if (!(s->cpuid_features & CPUID_CMOV)) {\n            goto illegal_op;\n        }\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        gen_cmovcc1(env, s, ot, b, modrm, reg);\n        break;\n\n        /************************/\n        /* flags */\n    case 0x9c: /* pushf */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_PUSHF);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_helper_read_eflags(cpu_T0, cpu_env);\n            gen_push_v(s, cpu_T0);\n        }\n        break;\n    case 0x9d: /* popf */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_POPF);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            ot = gen_pop_T0(s);\n            if (s->cpl == 0) {\n                if (dflag != MO_16) {\n                    gen_helper_write_eflags(cpu_env, cpu_T0,\n                                            tcg_const_i32((TF_MASK | AC_MASK |\n                                                           ID_MASK | NT_MASK |\n                                                           IF_MASK |\n                                                           IOPL_MASK)));\n                } else {\n                    gen_helper_write_eflags(cpu_env, cpu_T0,\n                                            tcg_const_i32((TF_MASK | AC_MASK |\n                                                           ID_MASK | NT_MASK |\n                                                           IF_MASK | IOPL_MASK)\n                                                          & 0xffff));\n                }\n            } else {\n                if (s->cpl <= s->iopl) {\n                    if (dflag != MO_16) {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                                tcg_const_i32((TF_MASK |\n                                                               AC_MASK |\n                                                               ID_MASK |\n                                                               NT_MASK |\n                                                               IF_MASK)));\n                    } else {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                                tcg_const_i32((TF_MASK |\n                                                               AC_MASK |\n                                                               ID_MASK |\n                                                               NT_MASK |\n                                                               IF_MASK)\n                                                              & 0xffff));\n                    }\n                } else {\n                    if (dflag != MO_16) {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                           tcg_const_i32((TF_MASK | AC_MASK |\n                                                          ID_MASK | NT_MASK)));\n                    } else {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                           tcg_const_i32((TF_MASK | AC_MASK |\n                                                          ID_MASK | NT_MASK)\n                                                         & 0xffff));\n                    }\n                }\n            }\n            gen_pop_update(s, ot);\n            set_cc_op(s, CC_OP_EFLAGS);\n            /* abort translation because TF/AC flag may change */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n    case 0x9e: /* sahf */\n        if (CODE64(s) && !(s->cpuid_ext3_features & CPUID_EXT3_LAHF_LM))\n            goto illegal_op;\n        gen_op_mov_v_reg(MO_8, cpu_T0, R_AH);\n        gen_compute_eflags(s);\n        tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, CC_O);\n        tcg_gen_andi_tl(cpu_T0, cpu_T0, CC_S | CC_Z | CC_A | CC_P | CC_C);\n        tcg_gen_or_tl(cpu_cc_src, cpu_cc_src, cpu_T0);\n        break;\n    case 0x9f: /* lahf */\n        if (CODE64(s) && !(s->cpuid_ext3_features & CPUID_EXT3_LAHF_LM))\n            goto illegal_op;\n        gen_compute_eflags(s);\n        /* Note: gen_compute_eflags() only gives the condition codes */\n        tcg_gen_ori_tl(cpu_T0, cpu_cc_src, 0x02);\n        gen_op_mov_reg_v(MO_8, R_AH, cpu_T0);\n        break;\n    case 0xf5: /* cmc */\n        gen_compute_eflags(s);\n        tcg_gen_xori_tl(cpu_cc_src, cpu_cc_src, CC_C);\n        break;\n    case 0xf8: /* clc */\n        gen_compute_eflags(s);\n        tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, ~CC_C);\n        break;\n    case 0xf9: /* stc */\n        gen_compute_eflags(s);\n        tcg_gen_ori_tl(cpu_cc_src, cpu_cc_src, CC_C);\n        break;\n    case 0xfc: /* cld */\n        tcg_gen_movi_i32(cpu_tmp2_i32, 1);\n        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, df));\n        break;\n    case 0xfd: /* std */\n        tcg_gen_movi_i32(cpu_tmp2_i32, -1);\n        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, df));\n        break;\n\n        /************************/\n        /* bit operations */\n    case 0x1ba: /* bt/bts/btr/btc Gv, im */\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        op = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        if (mod != 3) {\n            s->rip_offset = 1;\n            gen_lea_modrm(env, s, modrm);\n            if (!(s->prefix & PREFIX_LOCK)) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n        /* load shift */\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T1, val);\n        if (op < 4)\n            goto unknown_op;\n        op -= 4;\n        goto bt_op;\n    case 0x1a3: /* bt Gv, Ev */\n        op = 0;\n        goto do_btx;\n    case 0x1ab: /* bts */\n        op = 1;\n        goto do_btx;\n    case 0x1b3: /* btr */\n        op = 2;\n        goto do_btx;\n    case 0x1bb: /* btc */\n        op = 3;\n    do_btx:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        gen_op_mov_v_reg(MO_32, cpu_T1, reg);\n        if (mod != 3) {\n            AddressParts a = gen_lea_modrm_0(env, s, modrm);\n            /* specific case: we need to add a displacement */\n            gen_exts(ot, cpu_T1);\n            tcg_gen_sari_tl(cpu_tmp0, cpu_T1, 3 + ot);\n            tcg_gen_shli_tl(cpu_tmp0, cpu_tmp0, ot);\n            tcg_gen_add_tl(cpu_A0, gen_lea_modrm_1(a), cpu_tmp0);\n            gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n            if (!(s->prefix & PREFIX_LOCK)) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n    bt_op:\n        tcg_gen_andi_tl(cpu_T1, cpu_T1, (1 << (3 + ot)) - 1);\n        tcg_gen_movi_tl(cpu_tmp0, 1);\n        tcg_gen_shl_tl(cpu_tmp0, cpu_tmp0, cpu_T1);\n        if (s->prefix & PREFIX_LOCK) {\n            switch (op) {\n            case 0: /* bt */\n                /* Needs no atomic ops; we surpressed the normal\n                   memory load for LOCK above so do it now.  */\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n                break;\n            case 1: /* bts */\n                tcg_gen_atomic_fetch_or_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                           s->mem_index, ot | MO_LE);\n                break;\n            case 2: /* btr */\n                tcg_gen_not_tl(cpu_tmp0, cpu_tmp0);\n                tcg_gen_atomic_fetch_and_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                            s->mem_index, ot | MO_LE);\n                break;\n            default:\n            case 3: /* btc */\n                tcg_gen_atomic_fetch_xor_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                            s->mem_index, ot | MO_LE);\n                break;\n            }\n            tcg_gen_shr_tl(cpu_tmp4, cpu_T0, cpu_T1);\n        } else {\n            tcg_gen_shr_tl(cpu_tmp4, cpu_T0, cpu_T1);\n            switch (op) {\n            case 0: /* bt */\n                /* Data already loaded; nothing to do.  */\n                break;\n            case 1: /* bts */\n                tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            case 2: /* btr */\n                tcg_gen_andc_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            default:\n            case 3: /* btc */\n                tcg_gen_xor_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            }\n            if (op != 0) {\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n        }\n\n        /* Delay all CC updates until after the store above.  Note that\n           C is the result of the test, Z is unchanged, and the others\n           are all undefined.  */\n        switch (s->cc_op) {\n        case CC_OP_MULB ... CC_OP_MULQ:\n        case CC_OP_ADDB ... CC_OP_ADDQ:\n        case CC_OP_ADCB ... CC_OP_ADCQ:\n        case CC_OP_SUBB ... CC_OP_SUBQ:\n        case CC_OP_SBBB ... CC_OP_SBBQ:\n        case CC_OP_LOGICB ... CC_OP_LOGICQ:\n        case CC_OP_INCB ... CC_OP_INCQ:\n        case CC_OP_DECB ... CC_OP_DECQ:\n        case CC_OP_SHLB ... CC_OP_SHLQ:\n        case CC_OP_SARB ... CC_OP_SARQ:\n        case CC_OP_BMILGB ... CC_OP_BMILGQ:\n            /* Z was going to be computed from the non-zero status of CC_DST.\n               We can get that same Z value (and the new C value) by leaving\n               CC_DST alone, setting CC_SRC, and using a CC_OP_SAR of the\n               same width.  */\n            tcg_gen_mov_tl(cpu_cc_src, cpu_tmp4);\n            set_cc_op(s, ((s->cc_op - CC_OP_MULB) & 3) + CC_OP_SARB);\n            break;\n        default:\n            /* Otherwise, generate EFLAGS and replace the C bit.  */\n            gen_compute_eflags(s);\n            tcg_gen_deposit_tl(cpu_cc_src, cpu_cc_src, cpu_tmp4,\n                               ctz32(CC_C), 1);\n            break;\n        }\n        break;\n    case 0x1bc: /* bsf / tzcnt */\n    case 0x1bd: /* bsr / lzcnt */\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_extu(ot, cpu_T0);\n\n        /* Note that lzcnt and tzcnt are in different extensions.  */\n        if ((prefixes & PREFIX_REPZ)\n            && (b & 1\n                ? s->cpuid_ext3_features & CPUID_EXT3_ABM\n                : s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)) {\n            int size = 8 << ot;\n            /* For lzcnt/tzcnt, C bit is defined related to the input. */\n            tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n            if (b & 1) {\n                /* For lzcnt, reduce the target_ulong result by the\n                   number of zeros that we expect to find at the top.  */\n                tcg_gen_clzi_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS);\n                tcg_gen_subi_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS - size);\n            } else {\n                /* For tzcnt, a zero input must return the operand size.  */\n                tcg_gen_ctzi_tl(cpu_T0, cpu_T0, size);\n            }\n            /* For lzcnt/tzcnt, Z bit is defined related to the result.  */\n            gen_op_update1_cc();\n            set_cc_op(s, CC_OP_BMILGB + ot);\n        } else {\n            /* For bsr/bsf, only the Z bit is defined and it is related\n               to the input and not the result.  */\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n            set_cc_op(s, CC_OP_LOGICB + ot);\n\n            /* ??? The manual says that the output is undefined when the\n               input is zero, but real hardware leaves it unchanged, and\n               real programs appear to depend on that.  Accomplish this\n               by passing the output as the value to return upon zero.  */\n            if (b & 1) {\n                /* For bsr, return the bit index of the first 1 bit,\n                   not the count of leading zeros.  */\n                tcg_gen_xori_tl(cpu_T1, cpu_regs[reg], TARGET_LONG_BITS - 1);\n                tcg_gen_clz_tl(cpu_T0, cpu_T0, cpu_T1);\n                tcg_gen_xori_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS - 1);\n            } else {\n                tcg_gen_ctz_tl(cpu_T0, cpu_T0, cpu_regs[reg]);\n            }\n        }\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n        break;\n        /************************/\n        /* bcd */\n    case 0x27: /* daa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_daa(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x2f: /* das */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_das(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x37: /* aaa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_aaa(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x3f: /* aas */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_aas(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0xd4: /* aam */\n        if (CODE64(s))\n            goto illegal_op;\n        val = cpu_ldub_code(env, s->pc++);\n        if (val == 0) {\n            gen_exception(s, EXCP00_DIVZ, pc_start - s->cs_base);\n        } else {\n            gen_helper_aam(cpu_env, tcg_const_i32(val));\n            set_cc_op(s, CC_OP_LOGICB);\n        }\n        break;\n    case 0xd5: /* aad */\n        if (CODE64(s))\n            goto illegal_op;\n        val = cpu_ldub_code(env, s->pc++);\n        gen_helper_aad(cpu_env, tcg_const_i32(val));\n        set_cc_op(s, CC_OP_LOGICB);\n        break;\n        /************************/\n        /* misc */\n    case 0x90: /* nop */\n        /* XXX: correct lock test for all insn */\n        if (prefixes & PREFIX_LOCK) {\n            goto illegal_op;\n        }\n        /* If REX_B is set, then this is xchg eax, r8d, not a nop.  */\n        if (REX_B(s)) {\n            goto do_xchg_reg_eax;\n        }\n        if (prefixes & PREFIX_REPZ) {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_pause(cpu_env, tcg_const_i32(s->pc - pc_start));\n            s->is_jmp = DISAS_TB_JUMP;\n        }\n        break;\n    case 0x9b: /* fwait */\n        if ((s->flags & (HF_MP_MASK | HF_TS_MASK)) ==\n            (HF_MP_MASK | HF_TS_MASK)) {\n            gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n        } else {\n            gen_helper_fwait(cpu_env);\n        }\n        break;\n    case 0xcc: /* int3 */\n        gen_interrupt(s, EXCP03_INT3, pc_start - s->cs_base, s->pc - s->cs_base);\n        break;\n    case 0xcd: /* int N */\n        val = cpu_ldub_code(env, s->pc++);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_interrupt(s, val, pc_start - s->cs_base, s->pc - s->cs_base);\n        }\n        break;\n    case 0xce: /* into */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_into(cpu_env, tcg_const_i32(s->pc - pc_start));\n        break;\n#ifdef WANT_ICEBP\n    case 0xf1: /* icebp (undocumented, exits to external debugger) */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_ICEBP);\n#if 1\n        gen_debug(s, pc_start - s->cs_base);\n#else\n        /* start debug */\n        tb_flush(CPU(x86_env_get_cpu(env)));\n        qemu_set_log(CPU_LOG_INT | CPU_LOG_TB_IN_ASM);\n#endif\n        break;\n#endif\n    case 0xfa: /* cli */\n        if (!s->vm86) {\n            if (s->cpl <= s->iopl) {\n                gen_helper_cli(cpu_env);\n            } else {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            }\n        } else {\n            if (s->iopl == 3) {\n                gen_helper_cli(cpu_env);\n            } else {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            }\n        }\n        break;\n    case 0xfb: /* sti */\n        if (s->vm86 ? s->iopl == 3 : s->cpl <= s->iopl) {\n            gen_helper_sti(cpu_env);\n            /* interruptions are enabled only the first insn after sti */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob_inhibit_irq(s, true);\n        } else {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        }\n        break;\n    case 0x62: /* bound */\n        if (CODE64(s))\n            goto illegal_op;\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_op_mov_v_reg(ot, cpu_T0, reg);\n        gen_lea_modrm(env, s, modrm);\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        if (ot == MO_16) {\n            gen_helper_boundw(cpu_env, cpu_A0, cpu_tmp2_i32);\n        } else {\n            gen_helper_boundl(cpu_env, cpu_A0, cpu_tmp2_i32);\n        }\n        break;\n    case 0x1c8 ... 0x1cf: /* bswap reg */\n        reg = (b & 7) | REX_B(s);\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            gen_op_mov_v_reg(MO_64, cpu_T0, reg);\n            tcg_gen_bswap64_i64(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_64, reg, cpu_T0);\n        } else\n#endif\n        {\n            gen_op_mov_v_reg(MO_32, cpu_T0, reg);\n            tcg_gen_ext32u_tl(cpu_T0, cpu_T0);\n            tcg_gen_bswap32_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_32, reg, cpu_T0);\n        }\n        break;\n    case 0xd6: /* salc */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_compute_eflags_c(s, cpu_T0);\n        tcg_gen_neg_tl(cpu_T0, cpu_T0);\n        gen_op_mov_reg_v(MO_8, R_EAX, cpu_T0);\n        break;\n    case 0xe0: /* loopnz */\n    case 0xe1: /* loopz */\n    case 0xe2: /* loop */\n    case 0xe3: /* jecxz */\n        {\n            TCGLabel *l1, *l2, *l3;\n\n            tval = (int8_t)insn_get(env, s, MO_8);\n            next_eip = s->pc - s->cs_base;\n            tval += next_eip;\n            if (dflag == MO_16) {\n                tval &= 0xffff;\n            }\n\n            l1 = gen_new_label();\n            l2 = gen_new_label();\n            l3 = gen_new_label();\n            b &= 3;\n            switch(b) {\n            case 0: /* loopnz */\n            case 1: /* loopz */\n                gen_op_add_reg_im(s->aflag, R_ECX, -1);\n                gen_op_jz_ecx(s->aflag, l3);\n                gen_jcc1(s, (JCC_Z << 1) | (b ^ 1), l1);\n                break;\n            case 2: /* loop */\n                gen_op_add_reg_im(s->aflag, R_ECX, -1);\n                gen_op_jnz_ecx(s->aflag, l1);\n                break;\n            default:\n            case 3: /* jcxz */\n                gen_op_jz_ecx(s->aflag, l1);\n                break;\n            }\n\n            gen_set_label(l3);\n            gen_jmp_im(next_eip);\n            tcg_gen_br(l2);\n\n            gen_set_label(l1);\n            gen_jmp_im(tval);\n            gen_set_label(l2);\n            gen_eob(s);\n        }\n        break;\n    case 0x130: /* wrmsr */\n    case 0x132: /* rdmsr */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            if (b & 2) {\n                gen_helper_rdmsr(cpu_env);\n            } else {\n                gen_helper_wrmsr(cpu_env);\n            }\n        }\n        break;\n    case 0x131: /* rdtsc */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        gen_helper_rdtsc(cpu_env);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0x133: /* rdpmc */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_rdpmc(cpu_env);\n        break;\n    case 0x134: /* sysenter */\n        /* For Intel SYSENTER is valid on 64-bit */\n        if (CODE64(s) && env->cpuid_vendor1 != CPUID_VENDOR_INTEL_1)\n            goto illegal_op;\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysenter(cpu_env);\n            gen_eob(s);\n        }\n        break;\n    case 0x135: /* sysexit */\n        /* For Intel SYSEXIT is valid on 64-bit */\n        if (CODE64(s) && env->cpuid_vendor1 != CPUID_VENDOR_INTEL_1)\n            goto illegal_op;\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysexit(cpu_env, tcg_const_i32(dflag - 1));\n            gen_eob(s);\n        }\n        break;\n#ifdef TARGET_X86_64\n    case 0x105: /* syscall */\n        /* XXX: is it usable in real mode ? */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_syscall(cpu_env, tcg_const_i32(s->pc - pc_start));\n        /* TF handling for the syscall insn is different. The TF bit is  checked\n           after the syscall insn completes. This allows #DB to not be\n           generated after one has entered CPL0 if TF is set in FMASK.  */\n        gen_eob_worker(s, false, true);\n        break;\n    case 0x107: /* sysret */\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysret(cpu_env, tcg_const_i32(dflag - 1));\n            /* condition codes are modified only in long mode */\n            if (s->lma) {\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n            /* TF handling for the sysret insn is different. The TF bit is\n               checked after the sysret insn completes. This allows #DB to be\n               generated \"as if\" the syscall insn in userspace has just\n               completed.  */\n            gen_eob_worker(s, false, true);\n        }\n        break;\n#endif\n    case 0x1a2: /* cpuid */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_cpuid(cpu_env);\n        break;\n    case 0xf4: /* hlt */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_hlt(cpu_env, tcg_const_i32(s->pc - pc_start));\n            s->is_jmp = DISAS_TB_JUMP;\n        }\n        break;\n    case 0x100:\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        op = (modrm >> 3) & 7;\n        switch(op) {\n        case 0: /* sldt */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_LDTR_READ);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                             offsetof(CPUX86State, ldt.selector));\n            ot = mod == 3 ? dflag : MO_16;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 2: /* lldt */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_LDTR_WRITE);\n                gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lldt(cpu_env, cpu_tmp2_i32);\n            }\n            break;\n        case 1: /* str */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_TR_READ);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                             offsetof(CPUX86State, tr.selector));\n            ot = mod == 3 ? dflag : MO_16;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 3: /* ltr */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_TR_WRITE);\n                gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_ltr(cpu_env, cpu_tmp2_i32);\n            }\n            break;\n        case 4: /* verr */\n        case 5: /* verw */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            gen_update_cc_op(s);\n            if (op == 4) {\n                gen_helper_verr(cpu_env, cpu_T0);\n            } else {\n                gen_helper_verw(cpu_env, cpu_T0);\n            }\n            set_cc_op(s, CC_OP_EFLAGS);\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x101:\n        modrm = cpu_ldub_code(env, s->pc++);\n        switch (modrm) {\n        CASE_MODRM_MEM_OP(0): /* sgdt */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_GDTR_READ);\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0,\n                             cpu_env, offsetof(CPUX86State, gdt.limit));\n            gen_op_st_v(s, MO_16, cpu_T0, cpu_A0);\n            gen_add_A0_im(s, 2);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, gdt.base));\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            gen_op_st_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            break;\n\n        case 0xc8: /* monitor */\n            if (!(s->cpuid_ext_features & CPUID_EXT_MONITOR) || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EAX]);\n            gen_extu(s->aflag, cpu_A0);\n            gen_add_A0_ds_seg(s);\n            gen_helper_monitor(cpu_env, cpu_A0);\n            break;\n\n        case 0xc9: /* mwait */\n            if (!(s->cpuid_ext_features & CPUID_EXT_MONITOR) || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_mwait(cpu_env, tcg_const_i32(s->pc - pc_start));\n            gen_eob(s);\n            break;\n\n        case 0xca: /* clac */\n            if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_SMAP)\n                || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_helper_clac(cpu_env);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xcb: /* stac */\n            if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_SMAP)\n                || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_helper_stac(cpu_env);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(1): /* sidt */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_IDTR_READ);\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.limit));\n            gen_op_st_v(s, MO_16, cpu_T0, cpu_A0);\n            gen_add_A0_im(s, 2);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.base));\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            gen_op_st_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            break;\n\n        case 0xd0: /* xgetbv */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (s->prefix & (PREFIX_LOCK | PREFIX_DATA\n                                 | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_xgetbv(cpu_tmp1_i64, cpu_env, cpu_tmp2_i32);\n            tcg_gen_extr_i64_tl(cpu_regs[R_EAX], cpu_regs[R_EDX], cpu_tmp1_i64);\n            break;\n\n        case 0xd1: /* xsetbv */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (s->prefix & (PREFIX_LOCK | PREFIX_DATA\n                                 | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_xsetbv(cpu_env, cpu_tmp2_i32, cpu_tmp1_i64);\n            /* End TB because translation flags may change.  */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xd8: /* VMRUN */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmrun(cpu_env, tcg_const_i32(s->aflag - 1),\n                             tcg_const_i32(s->pc - pc_start));\n            tcg_gen_exit_tb(0);\n            s->is_jmp = DISAS_TB_JUMP;\n            break;\n\n        case 0xd9: /* VMMCALL */\n            if (!(s->flags & HF_SVME_MASK)) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmmcall(cpu_env);\n            break;\n\n        case 0xda: /* VMLOAD */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmload(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        case 0xdb: /* VMSAVE */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmsave(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        case 0xdc: /* STGI */\n            if ((!(s->flags & HF_SVME_MASK)\n                   && !(s->cpuid_ext3_features & CPUID_EXT3_SKINIT))\n                || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_stgi(cpu_env);\n            break;\n\n        case 0xdd: /* CLGI */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_clgi(cpu_env);\n            break;\n\n        case 0xde: /* SKINIT */\n            if ((!(s->flags & HF_SVME_MASK)\n                 && !(s->cpuid_ext3_features & CPUID_EXT3_SKINIT))\n                || !s->pe) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_skinit(cpu_env);\n            break;\n\n        case 0xdf: /* INVLPGA */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_invlpga(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        CASE_MODRM_MEM_OP(2): /* lgdt */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_GDTR_WRITE);\n            gen_lea_modrm(env, s, modrm);\n            gen_op_ld_v(s, MO_16, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 2);\n            gen_op_ld_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State, gdt.base));\n            tcg_gen_st32_tl(cpu_T1, cpu_env, offsetof(CPUX86State, gdt.limit));\n            break;\n\n        CASE_MODRM_MEM_OP(3): /* lidt */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_IDTR_WRITE);\n            gen_lea_modrm(env, s, modrm);\n            gen_op_ld_v(s, MO_16, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 2);\n            gen_op_ld_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.base));\n            tcg_gen_st32_tl(cpu_T1, cpu_env, offsetof(CPUX86State, idt.limit));\n            break;\n\n        CASE_MODRM_OP(4): /* smsw */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_READ_CR0);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, cr[0]));\n            if (CODE64(s)) {\n                mod = (modrm >> 6) & 3;\n                ot = (mod != 3 ? MO_16 : s->dflag);\n            } else {\n                ot = MO_16;\n            }\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 0xee: /* rdpkru */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_rdpkru(cpu_tmp1_i64, cpu_env, cpu_tmp2_i32);\n            tcg_gen_extr_i64_tl(cpu_regs[R_EAX], cpu_regs[R_EDX], cpu_tmp1_i64);\n            break;\n        case 0xef: /* wrpkru */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_wrpkru(cpu_env, cpu_tmp2_i32, cpu_tmp1_i64);\n            break;\n        CASE_MODRM_OP(6): /* lmsw */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_CR0);\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            gen_helper_lmsw(cpu_env, cpu_T0);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(7): /* invlpg */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_invlpg(cpu_env, cpu_A0);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xf8: /* swapgs */\n#ifdef TARGET_X86_64\n            if (CODE64(s)) {\n                if (s->cpl != 0) {\n                    gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                } else {\n                    tcg_gen_mov_tl(cpu_T0, cpu_seg_base[R_GS]);\n                    tcg_gen_ld_tl(cpu_seg_base[R_GS], cpu_env,\n                                  offsetof(CPUX86State, kernelgsbase));\n                    tcg_gen_st_tl(cpu_T0, cpu_env,\n                                  offsetof(CPUX86State, kernelgsbase));\n                }\n                break;\n            }\n#endif\n            goto illegal_op;\n\n        case 0xf9: /* rdtscp */\n            if (!(s->cpuid_ext2_features & CPUID_EXT2_RDTSCP)) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_io_start();\n            }\n            gen_helper_rdtscp(cpu_env);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_io_end();\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n            break;\n\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x108: /* invd */\n    case 0x109: /* wbinvd */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_svm_check_intercept(s, pc_start, (b & 2) ? SVM_EXIT_INVD : SVM_EXIT_WBINVD);\n            /* nothing to do */\n        }\n        break;\n    case 0x63: /* arpl or movslS (x86_64) */\n#ifdef TARGET_X86_64\n        if (CODE64(s)) {\n            int d_ot;\n            /* d_ot is the size of destination */\n            d_ot = dflag;\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n\n            if (mod == 3) {\n                gen_op_mov_v_reg(MO_32, cpu_T0, rm);\n                /* sign extend */\n                if (d_ot == MO_64) {\n                    tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n                }\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            } else {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, MO_32 | MO_SIGN, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            }\n        } else\n#endif\n        {\n            TCGLabel *label1;\n            TCGv t0, t1, t2, a0;\n\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            t0 = tcg_temp_local_new();\n            t1 = tcg_temp_local_new();\n            t2 = tcg_temp_local_new();\n            ot = MO_16;\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = (modrm >> 3) & 7;\n            mod = (modrm >> 6) & 3;\n            rm = modrm & 7;\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, ot, t0, cpu_A0);\n                a0 = tcg_temp_local_new();\n                tcg_gen_mov_tl(a0, cpu_A0);\n            } else {\n                gen_op_mov_v_reg(ot, t0, rm);\n                TCGV_UNUSED(a0);\n            }\n            gen_op_mov_v_reg(ot, t1, reg);\n            tcg_gen_andi_tl(cpu_tmp0, t0, 3);\n            tcg_gen_andi_tl(t1, t1, 3);\n            tcg_gen_movi_tl(t2, 0);\n            label1 = gen_new_label();\n            tcg_gen_brcond_tl(TCG_COND_GE, cpu_tmp0, t1, label1);\n            tcg_gen_andi_tl(t0, t0, ~3);\n            tcg_gen_or_tl(t0, t0, t1);\n            tcg_gen_movi_tl(t2, CC_Z);\n            gen_set_label(label1);\n            if (mod != 3) {\n                gen_op_st_v(s, ot, t0, a0);\n                tcg_temp_free(a0);\n           } else {\n                gen_op_mov_reg_v(ot, rm, t0);\n            }\n            gen_compute_eflags(s);\n            tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, ~CC_Z);\n            tcg_gen_or_tl(cpu_cc_src, cpu_cc_src, t2);\n            tcg_temp_free(t0);\n            tcg_temp_free(t1);\n            tcg_temp_free(t2);\n        }\n        break;\n    case 0x102: /* lar */\n    case 0x103: /* lsl */\n        {\n            TCGLabel *label1;\n            TCGv t0;\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            ot = dflag != MO_16 ? MO_32 : MO_16;\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            t0 = tcg_temp_local_new();\n            gen_update_cc_op(s);\n            if (b == 0x102) {\n                gen_helper_lar(t0, cpu_env, cpu_T0);\n            } else {\n                gen_helper_lsl(t0, cpu_env, cpu_T0);\n            }\n            tcg_gen_andi_tl(cpu_tmp0, cpu_cc_src, CC_Z);\n            label1 = gen_new_label();\n            tcg_gen_brcondi_tl(TCG_COND_EQ, cpu_tmp0, 0, label1);\n            gen_op_mov_reg_v(ot, reg, t0);\n            gen_set_label(label1);\n            set_cc_op(s, CC_OP_EFLAGS);\n            tcg_temp_free(t0);\n        }\n        break;\n    case 0x118:\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        op = (modrm >> 3) & 7;\n        switch(op) {\n        case 0: /* prefetchnta */\n        case 1: /* prefetchnt0 */\n        case 2: /* prefetchnt0 */\n        case 3: /* prefetchnt0 */\n            if (mod == 3)\n                goto illegal_op;\n            gen_nop_modrm(env, s, modrm);\n            /* nothing more to do */\n            break;\n        default: /* nop (multi byte) */\n            gen_nop_modrm(env, s, modrm);\n            break;\n        }\n        break;\n    case 0x11a:\n        modrm = cpu_ldub_code(env, s->pc++);\n        if (s->flags & HF_MPX_EN_MASK) {\n            mod = (modrm >> 6) & 3;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (prefixes & PREFIX_REPZ) {\n                /* bndcl */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                gen_bndck(env, s, modrm, TCG_COND_LTU, cpu_bndl[reg]);\n            } else if (prefixes & PREFIX_REPNZ) {\n                /* bndcu */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                TCGv_i64 notu = tcg_temp_new_i64();\n                tcg_gen_not_i64(notu, cpu_bndu[reg]);\n                gen_bndck(env, s, modrm, TCG_COND_GTU, notu);\n                tcg_temp_free_i64(notu);\n            } else if (prefixes & PREFIX_DATA) {\n                /* bndmov -- from reg/mem */\n                if (reg >= 4 || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                if (mod == 3) {\n                    int reg2 = (modrm & 7) | REX_B(s);\n                    if (reg2 >= 4 || (prefixes & PREFIX_LOCK)) {\n                        goto illegal_op;\n                    }\n                    if (s->flags & HF_MPX_IU_MASK) {\n                        tcg_gen_mov_i64(cpu_bndl[reg], cpu_bndl[reg2]);\n                        tcg_gen_mov_i64(cpu_bndu[reg], cpu_bndu[reg2]);\n                    }\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    if (CODE64(s)) {\n                        tcg_gen_qemu_ld_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 8);\n                        tcg_gen_qemu_ld_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                    } else {\n                        tcg_gen_qemu_ld_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 4);\n                        tcg_gen_qemu_ld_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                    /* bnd registers are now in-use */\n                    gen_set_hflag(s, HF_MPX_IU_MASK);\n                }\n            } else if (mod != 3) {\n                /* bndldx */\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16\n                    || a.base < -1) {\n                    goto illegal_op;\n                }\n                if (a.base >= 0) {\n                    tcg_gen_addi_tl(cpu_A0, cpu_regs[a.base], a.disp);\n                } else {\n                    tcg_gen_movi_tl(cpu_A0, 0);\n                }\n                gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n                if (a.index >= 0) {\n                    tcg_gen_mov_tl(cpu_T0, cpu_regs[a.index]);\n                } else {\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                }\n                if (CODE64(s)) {\n                    gen_helper_bndldx64(cpu_bndl[reg], cpu_env, cpu_A0, cpu_T0);\n                    tcg_gen_ld_i64(cpu_bndu[reg], cpu_env,\n                                   offsetof(CPUX86State, mmx_t0.MMX_Q(0)));\n                } else {\n                    gen_helper_bndldx32(cpu_bndu[reg], cpu_env, cpu_A0, cpu_T0);\n                    tcg_gen_ext32u_i64(cpu_bndl[reg], cpu_bndu[reg]);\n                    tcg_gen_shri_i64(cpu_bndu[reg], cpu_bndu[reg], 32);\n                }\n                gen_set_hflag(s, HF_MPX_IU_MASK);\n            }\n        }\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x11b:\n        modrm = cpu_ldub_code(env, s->pc++);\n        if (s->flags & HF_MPX_EN_MASK) {\n            mod = (modrm >> 6) & 3;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (mod != 3 && (prefixes & PREFIX_REPZ)) {\n                /* bndmk */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (a.base >= 0) {\n                    tcg_gen_extu_tl_i64(cpu_bndl[reg], cpu_regs[a.base]);\n                    if (!CODE64(s)) {\n                        tcg_gen_ext32u_i64(cpu_bndl[reg], cpu_bndl[reg]);\n                    }\n                } else if (a.base == -1) {\n                    /* no base register has lower bound of 0 */\n                    tcg_gen_movi_i64(cpu_bndl[reg], 0);\n                } else {\n                    /* rip-relative generates #ud */\n                    goto illegal_op;\n                }\n                tcg_gen_not_tl(cpu_A0, gen_lea_modrm_1(a));\n                if (!CODE64(s)) {\n                    tcg_gen_ext32u_tl(cpu_A0, cpu_A0);\n                }\n                tcg_gen_extu_tl_i64(cpu_bndu[reg], cpu_A0);\n                /* bnd registers are now in-use */\n                gen_set_hflag(s, HF_MPX_IU_MASK);\n                break;\n            } else if (prefixes & PREFIX_REPNZ) {\n                /* bndcn */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                gen_bndck(env, s, modrm, TCG_COND_GTU, cpu_bndu[reg]);\n            } else if (prefixes & PREFIX_DATA) {\n                /* bndmov -- to reg/mem */\n                if (reg >= 4 || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                if (mod == 3) {\n                    int reg2 = (modrm & 7) | REX_B(s);\n                    if (reg2 >= 4 || (prefixes & PREFIX_LOCK)) {\n                        goto illegal_op;\n                    }\n                    if (s->flags & HF_MPX_IU_MASK) {\n                        tcg_gen_mov_i64(cpu_bndl[reg2], cpu_bndl[reg]);\n                        tcg_gen_mov_i64(cpu_bndu[reg2], cpu_bndu[reg]);\n                    }\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    if (CODE64(s)) {\n                        tcg_gen_qemu_st_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 8);\n                        tcg_gen_qemu_st_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                    } else {\n                        tcg_gen_qemu_st_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 4);\n                        tcg_gen_qemu_st_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                }\n            } else if (mod != 3) {\n                /* bndstx */\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16\n                    || a.base < -1) {\n                    goto illegal_op;\n                }\n                if (a.base >= 0) {\n                    tcg_gen_addi_tl(cpu_A0, cpu_regs[a.base], a.disp);\n                } else {\n                    tcg_gen_movi_tl(cpu_A0, 0);\n                }\n                gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n                if (a.index >= 0) {\n                    tcg_gen_mov_tl(cpu_T0, cpu_regs[a.index]);\n                } else {\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                }\n                if (CODE64(s)) {\n                    gen_helper_bndstx64(cpu_env, cpu_A0, cpu_T0,\n                                        cpu_bndl[reg], cpu_bndu[reg]);\n                } else {\n                    gen_helper_bndstx32(cpu_env, cpu_A0, cpu_T0,\n                                        cpu_bndl[reg], cpu_bndu[reg]);\n                }\n            }\n        }\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x119: case 0x11c ... 0x11f: /* nop (multi byte) */\n        modrm = cpu_ldub_code(env, s->pc++);\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x120: /* mov reg, crN */\n    case 0x122: /* mov crN, reg */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            modrm = cpu_ldub_code(env, s->pc++);\n            /* Ignore the mod bits (assume (modrm&0xc0)==0xc0).\n             * AMD documentation (24594.pdf) and testing of\n             * intel 386 and 486 processors all show that the mod bits\n             * are assumed to be 1's, regardless of actual values.\n             */\n            rm = (modrm & 7) | REX_B(s);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (CODE64(s))\n                ot = MO_64;\n            else\n                ot = MO_32;\n            if ((prefixes & PREFIX_LOCK) && (reg == 0) &&\n                (s->cpuid_ext3_features & CPUID_EXT3_CR8LEG)) {\n                reg = 8;\n            }\n            switch(reg) {\n            case 0:\n            case 2:\n            case 3:\n            case 4:\n            case 8:\n                gen_update_cc_op(s);\n                gen_jmp_im(pc_start - s->cs_base);\n                if (b & 2) {\n                    gen_op_mov_v_reg(ot, cpu_T0, rm);\n                    gen_helper_write_crN(cpu_env, tcg_const_i32(reg),\n                                         cpu_T0);\n                    gen_jmp_im(s->pc - s->cs_base);\n                    gen_eob(s);\n                } else {\n                    gen_helper_read_crN(cpu_T0, cpu_env, tcg_const_i32(reg));\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n                break;\n            default:\n                goto unknown_op;\n            }\n        }\n        break;\n    case 0x121: /* mov reg, drN */\n    case 0x123: /* mov drN, reg */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            modrm = cpu_ldub_code(env, s->pc++);\n            /* Ignore the mod bits (assume (modrm&0xc0)==0xc0).\n             * AMD documentation (24594.pdf) and testing of\n             * intel 386 and 486 processors all show that the mod bits\n             * are assumed to be 1's, regardless of actual values.\n             */\n            rm = (modrm & 7) | REX_B(s);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (CODE64(s))\n                ot = MO_64;\n            else\n                ot = MO_32;\n            if (reg >= 8) {\n                goto illegal_op;\n            }\n            if (b & 2) {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_DR0 + reg);\n                gen_op_mov_v_reg(ot, cpu_T0, rm);\n                tcg_gen_movi_i32(cpu_tmp2_i32, reg);\n                gen_helper_set_dr(cpu_env, cpu_tmp2_i32, cpu_T0);\n                gen_jmp_im(s->pc - s->cs_base);\n                gen_eob(s);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_READ_DR0 + reg);\n                tcg_gen_movi_i32(cpu_tmp2_i32, reg);\n                gen_helper_get_dr(cpu_T0, cpu_env, cpu_tmp2_i32);\n                gen_op_mov_reg_v(ot, rm, cpu_T0);\n            }\n        }\n        break;\n    case 0x106: /* clts */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_CR0);\n            gen_helper_clts(cpu_env);\n            /* abort block because static cpu state changed */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n    /* MMX/3DNow!/SSE/SSE2/SSE3/SSSE3/SSE4 support */\n    case 0x1c3: /* MOVNTI reg, mem */\n        if (!(s->cpuid_features & CPUID_SSE2))\n            goto illegal_op;\n        ot = mo_64_32(dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        reg = ((modrm >> 3) & 7) | rex_r;\n        /* generate a generic store */\n        gen_ldst_modrm(env, s, modrm, ot, reg, 1);\n        break;\n    case 0x1ae:\n        modrm = cpu_ldub_code(env, s->pc++);\n        switch (modrm) {\n        CASE_MODRM_MEM_OP(0): /* fxsave */\n            if (!(s->cpuid_features & CPUID_FXSR)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            if ((s->flags & HF_EM_MASK) || (s->flags & HF_TS_MASK)) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_fxsave(cpu_env, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(1): /* fxrstor */\n            if (!(s->cpuid_features & CPUID_FXSR)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            if ((s->flags & HF_EM_MASK) || (s->flags & HF_TS_MASK)) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_fxrstor(cpu_env, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(2): /* ldmxcsr */\n            if ((s->flags & HF_EM_MASK) || !(s->flags & HF_OSFXSR_MASK)) {\n                goto illegal_op;\n            }\n            if (s->flags & HF_TS_MASK) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0, s->mem_index, MO_LEUL);\n            gen_helper_ldmxcsr(cpu_env, cpu_tmp2_i32);\n            break;\n\n        CASE_MODRM_MEM_OP(3): /* stmxcsr */\n            if ((s->flags & HF_EM_MASK) || !(s->flags & HF_OSFXSR_MASK)) {\n                goto illegal_op;\n            }\n            if (s->flags & HF_TS_MASK) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State, mxcsr));\n            gen_op_st_v(s, MO_32, cpu_T0, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(4): /* xsave */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (prefixes & (PREFIX_LOCK | PREFIX_DATA\n                                | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            gen_helper_xsave(cpu_env, cpu_A0, cpu_tmp1_i64);\n            break;\n\n        CASE_MODRM_MEM_OP(5): /* xrstor */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (prefixes & (PREFIX_LOCK | PREFIX_DATA\n                                | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            gen_helper_xrstor(cpu_env, cpu_A0, cpu_tmp1_i64);\n            /* XRSTOR is how MPX is enabled, which changes how\n               we translate.  Thus we need to end the TB.  */\n            gen_update_cc_op(s);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(6): /* xsaveopt / clwb */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            if (prefixes & PREFIX_DATA) {\n                /* clwb */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_CLWB)) {\n                    goto illegal_op;\n                }\n                gen_nop_modrm(env, s, modrm);\n            } else {\n                /* xsaveopt */\n                if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                    || (s->cpuid_xsave_features & CPUID_XSAVE_XSAVEOPT) == 0\n                    || (prefixes & (PREFIX_REPZ | PREFIX_REPNZ))) {\n                    goto illegal_op;\n                }\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                      cpu_regs[R_EDX]);\n                gen_helper_xsaveopt(cpu_env, cpu_A0, cpu_tmp1_i64);\n            }\n            break;\n\n        CASE_MODRM_MEM_OP(7): /* clflush / clflushopt */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            if (prefixes & PREFIX_DATA) {\n                /* clflushopt */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_CLFLUSHOPT)) {\n                    goto illegal_op;\n                }\n            } else {\n                /* clflush */\n                if ((s->prefix & (PREFIX_REPZ | PREFIX_REPNZ))\n                    || !(s->cpuid_features & CPUID_CLFLUSH)) {\n                    goto illegal_op;\n                }\n            }\n            gen_nop_modrm(env, s, modrm);\n            break;\n\n        case 0xc0 ... 0xc7: /* rdfsbase (f3 0f ae /0) */\n        case 0xc8 ... 0xc8: /* rdgsbase (f3 0f ae /1) */\n        case 0xd0 ... 0xd7: /* wrfsbase (f3 0f ae /2) */\n        case 0xd8 ... 0xd8: /* wrgsbase (f3 0f ae /3) */\n            if (CODE64(s)\n                && (prefixes & PREFIX_REPZ)\n                && !(prefixes & PREFIX_LOCK)\n                && (s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_FSGSBASE)) {\n                TCGv base, treg, src, dst;\n\n                /* Preserve hflags bits by testing CR4 at runtime.  */\n                tcg_gen_movi_i32(cpu_tmp2_i32, CR4_FSGSBASE_MASK);\n                gen_helper_cr4_testbit(cpu_env, cpu_tmp2_i32);\n\n                base = cpu_seg_base[modrm & 8 ? R_GS : R_FS];\n                treg = cpu_regs[(modrm & 7) | REX_B(s)];\n\n                if (modrm & 0x10) {\n                    /* wr*base */\n                    dst = base, src = treg;\n                } else {\n                    /* rd*base */\n                    dst = treg, src = base;\n                }\n\n                if (s->dflag == MO_32) {\n                    tcg_gen_ext32u_tl(dst, src);\n                } else {\n                    tcg_gen_mov_tl(dst, src);\n                }\n                break;\n            }\n            goto unknown_op;\n\n        case 0xf8: /* sfence / pcommit */\n            if (prefixes & PREFIX_DATA) {\n                /* pcommit */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_PCOMMIT)\n                    || (prefixes & PREFIX_LOCK)) {\n                    goto illegal_op;\n                }\n                break;\n            }\n            /* fallthru */\n        case 0xf9 ... 0xff: /* sfence */\n            if (!(s->cpuid_features & CPUID_SSE)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_ST_ST | TCG_BAR_SC);\n            break;\n        case 0xe8 ... 0xef: /* lfence */\n            if (!(s->cpuid_features & CPUID_SSE)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_LD_LD | TCG_BAR_SC);\n            break;\n        case 0xf0 ... 0xf7: /* mfence */\n            if (!(s->cpuid_features & CPUID_SSE2)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_ALL | TCG_BAR_SC);\n            break;\n\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x10d: /* 3DNow! prefetch(w) */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x1aa: /* rsm */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_RSM);\n        if (!(s->flags & HF_SMM_MASK))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_jmp_im(s->pc - s->cs_base);\n        gen_helper_rsm(cpu_env);\n        gen_eob(s);\n        break;\n    case 0x1b8: /* SSE4.2 popcnt */\n        if ((prefixes & (PREFIX_REPZ | PREFIX_LOCK | PREFIX_REPNZ)) !=\n             PREFIX_REPZ)\n            goto illegal_op;\n        if (!(s->cpuid_ext_features & CPUID_EXT_POPCNT))\n            goto illegal_op;\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        if (s->prefix & PREFIX_DATA) {\n            ot = MO_16;\n        } else {\n            ot = mo_64_32(dflag);\n        }\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_extu(ot, cpu_T0);\n        tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n        tcg_gen_ctpop_tl(cpu_T0, cpu_T0);\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n\n        set_cc_op(s, CC_OP_POPCNT);\n        break;\n    case 0x10e ... 0x10f:\n        /* 3DNow! instructions, ignore prefixes */\n        s->prefix &= ~(PREFIX_REPZ | PREFIX_REPNZ | PREFIX_DATA);\n    case 0x110 ... 0x117:\n    case 0x128 ... 0x12f:\n    case 0x138 ... 0x13a:\n    case 0x150 ... 0x179:\n    case 0x17c ... 0x17f:\n    case 0x1c2:\n    case 0x1c4 ... 0x1c6:\n    case 0x1d0 ... 0x1fe:\n        gen_sse(env, s, b, pc_start, rex_r);\n        break;\n    default:\n        goto unknown_op;\n    }\n    return s->pc;\n illegal_op:\n    gen_illegal_opcode(s);\n    return s->pc;\n unknown_op:\n    gen_unknown_opcode(env, s);\n    return s->pc;\n}\n\nvoid tcg_x86_init(void)\n{\n    static const char reg_names[CPU_NB_REGS][4] = {\n#ifdef TARGET_X86_64\n        [R_EAX] = \"rax\",\n        [R_EBX] = \"rbx\",\n        [R_ECX] = \"rcx\",\n        [R_EDX] = \"rdx\",\n        [R_ESI] = \"rsi\",\n        [R_EDI] = \"rdi\",\n        [R_EBP] = \"rbp\",\n        [R_ESP] = \"rsp\",\n        [8]  = \"r8\",\n        [9]  = \"r9\",\n        [10] = \"r10\",\n        [11] = \"r11\",\n        [12] = \"r12\",\n        [13] = \"r13\",\n        [14] = \"r14\",\n        [15] = \"r15\",\n#else\n        [R_EAX] = \"eax\",\n        [R_EBX] = \"ebx\",\n        [R_ECX] = \"ecx\",\n        [R_EDX] = \"edx\",\n        [R_ESI] = \"esi\",\n        [R_EDI] = \"edi\",\n        [R_EBP] = \"ebp\",\n        [R_ESP] = \"esp\",\n#endif\n    };\n    static const char seg_base_names[6][8] = {\n        [R_CS] = \"cs_base\",\n        [R_DS] = \"ds_base\",\n        [R_ES] = \"es_base\",\n        [R_FS] = \"fs_base\",\n        [R_GS] = \"gs_base\",\n        [R_SS] = \"ss_base\",\n    };\n    static const char bnd_regl_names[4][8] = {\n        \"bnd0_lb\", \"bnd1_lb\", \"bnd2_lb\", \"bnd3_lb\"\n    };\n    static const char bnd_regu_names[4][8] = {\n        \"bnd0_ub\", \"bnd1_ub\", \"bnd2_ub\", \"bnd3_ub\"\n    };\n    int i;\n    static bool initialized;\n\n    if (initialized) {\n        return;\n    }\n    initialized = true;\n\n    cpu_env = tcg_global_reg_new_ptr(TCG_AREG0, \"env\");\n    tcg_ctx.tcg_env = cpu_env;\n    cpu_cc_op = tcg_global_mem_new_i32(cpu_env,\n                                       offsetof(CPUX86State, cc_op), \"cc_op\");\n    cpu_cc_dst = tcg_global_mem_new(cpu_env, offsetof(CPUX86State, cc_dst),\n                                    \"cc_dst\");\n    cpu_cc_src = tcg_global_mem_new(cpu_env, offsetof(CPUX86State, cc_src),\n                                    \"cc_src\");\n    cpu_cc_src2 = tcg_global_mem_new(cpu_env, offsetof(CPUX86State, cc_src2),\n                                     \"cc_src2\");\n\n    for (i = 0; i < CPU_NB_REGS; ++i) {\n        cpu_regs[i] = tcg_global_mem_new(cpu_env,\n                                         offsetof(CPUX86State, regs[i]),\n                                         reg_names[i]);\n    }\n\n    for (i = 0; i < 6; ++i) {\n        cpu_seg_base[i]\n            = tcg_global_mem_new(cpu_env,\n                                 offsetof(CPUX86State, segs[i].base),\n                                 seg_base_names[i]);\n    }\n\n    for (i = 0; i < 4; ++i) {\n        cpu_bndl[i]\n            = tcg_global_mem_new_i64(cpu_env,\n                                     offsetof(CPUX86State, bnd_regs[i].lb),\n                                     bnd_regl_names[i]);\n        cpu_bndu[i]\n            = tcg_global_mem_new_i64(cpu_env,\n                                     offsetof(CPUX86State, bnd_regs[i].ub),\n                                     bnd_regu_names[i]);\n    }\n}\n\n/* generate intermediate code for basic block 'tb'.  */\nvoid gen_intermediate_code(CPUX86State *env, TranslationBlock *tb)\n{\n    X86CPU *cpu = x86_env_get_cpu(env);\n    CPUState *cs = CPU(cpu);\n    DisasContext dc1, *dc = &dc1;\n    target_ulong pc_ptr;\n    uint32_t flags;\n    target_ulong pc_start;\n    target_ulong cs_base;\n    int num_insns;\n    int max_insns;\n\n    /* generate intermediate code */\n    pc_start = tb->pc;\n    cs_base = tb->cs_base;\n    flags = tb->flags;\n\n    dc->pe = (flags >> HF_PE_SHIFT) & 1;\n    dc->code32 = (flags >> HF_CS32_SHIFT) & 1;\n    dc->ss32 = (flags >> HF_SS32_SHIFT) & 1;\n    dc->addseg = (flags >> HF_ADDSEG_SHIFT) & 1;\n    dc->f_st = 0;\n    dc->vm86 = (flags >> VM_SHIFT) & 1;\n    dc->cpl = (flags >> HF_CPL_SHIFT) & 3;\n    dc->iopl = (flags >> IOPL_SHIFT) & 3;\n    dc->tf = (flags >> TF_SHIFT) & 1;\n    dc->singlestep_enabled = cs->singlestep_enabled;\n    dc->cc_op = CC_OP_DYNAMIC;\n    dc->cc_op_dirty = false;\n    dc->cs_base = cs_base;\n    dc->tb = tb;\n    dc->popl_esp_hack = 0;\n    /* select memory access functions */\n    dc->mem_index = 0;\n#ifdef CONFIG_SOFTMMU\n    dc->mem_index = cpu_mmu_index(env, false);\n#endif\n    dc->cpuid_features = env->features[FEAT_1_EDX];\n    dc->cpuid_ext_features = env->features[FEAT_1_ECX];\n    dc->cpuid_ext2_features = env->features[FEAT_8000_0001_EDX];\n    dc->cpuid_ext3_features = env->features[FEAT_8000_0001_ECX];\n    dc->cpuid_7_0_ebx_features = env->features[FEAT_7_0_EBX];\n    dc->cpuid_xsave_features = env->features[FEAT_XSAVE];\n#ifdef TARGET_X86_64\n    dc->lma = (flags >> HF_LMA_SHIFT) & 1;\n    dc->code64 = (flags >> HF_CS64_SHIFT) & 1;\n#endif\n    dc->flags = flags;\n    dc->jmp_opt = !(dc->tf || cs->singlestep_enabled ||\n                    (flags & HF_INHIBIT_IRQ_MASK));\n    /* Do not optimize repz jumps at all in icount mode, because\n       rep movsS instructions are execured with different paths\n       in !repz_opt and repz_opt modes. The first one was used\n       always except single step mode. And this setting\n       disables jumps optimization and control paths become\n       equivalent in run and single step modes.\n       Now there will be no jump optimization for repz in\n       record/replay modes and there will always be an\n       additional step for ecx=0 when icount is enabled.\n     */\n    dc->repz_opt = !dc->jmp_opt && !(tb->cflags & CF_USE_ICOUNT);\n#if 0\n    /* check addseg logic */\n    if (!dc->addseg && (dc->vm86 || !dc->pe || !dc->code32))\n        printf(\"ERROR addseg\\n\");\n#endif\n\n    cpu_T0 = tcg_temp_new();\n    cpu_T1 = tcg_temp_new();\n    cpu_A0 = tcg_temp_new();\n\n    cpu_tmp0 = tcg_temp_new();\n    cpu_tmp1_i64 = tcg_temp_new_i64();\n    cpu_tmp2_i32 = tcg_temp_new_i32();\n    cpu_tmp3_i32 = tcg_temp_new_i32();\n    cpu_tmp4 = tcg_temp_new();\n    cpu_ptr0 = tcg_temp_new_ptr();\n    cpu_ptr1 = tcg_temp_new_ptr();\n    cpu_cc_srcT = tcg_temp_local_new();\n\n    dc->is_jmp = DISAS_NEXT;\n    pc_ptr = pc_start;\n    num_insns = 0;\n    max_insns = tb->cflags & CF_COUNT_MASK;\n    if (max_insns == 0) {\n        max_insns = CF_COUNT_MASK;\n    }\n    if (max_insns > TCG_MAX_INSNS) {\n        max_insns = TCG_MAX_INSNS;\n    }\n\n    gen_tb_start(tb);\n    for(;;) {\n        tcg_gen_insn_start(pc_ptr, dc->cc_op);\n        num_insns++;\n\n        /* If RF is set, suppress an internally generated breakpoint.  */\n        if (unlikely(cpu_breakpoint_test(cs, pc_ptr,\n                                         tb->flags & HF_RF_MASK\n                                         ? BP_GDB : BP_ANY))) {\n            gen_debug(dc, pc_ptr - dc->cs_base);\n            /* The address covered by the breakpoint must be included in\n               [tb->pc, tb->pc + tb->size) in order to for it to be\n               properly cleared -- thus we increment the PC here so that\n               the logic setting tb->size below does the right thing.  */\n            pc_ptr += 1;\n            goto done_generating;\n        }\n        if (num_insns == max_insns && (tb->cflags & CF_LAST_IO)) {\n            gen_io_start();\n        }\n\n        pc_ptr = disas_insn(env, dc, pc_ptr);\n        /* stop translation if indicated */\n        if (dc->is_jmp)\n            break;\n        /* if single step mode, we generate only one instruction and\n           generate an exception */\n        /* if irq were inhibited with HF_INHIBIT_IRQ_MASK, we clear\n           the flag and abort the translation to give the irqs a\n           change to be happen */\n        if (dc->tf || dc->singlestep_enabled ||\n            (flags & HF_INHIBIT_IRQ_MASK)) {\n            gen_jmp_im(pc_ptr - dc->cs_base);\n            gen_eob(dc);\n            break;\n        }\n        /* Do not cross the boundary of the pages in icount mode,\n           it can cause an exception. Do it only when boundary is\n           crossed by the first instruction in the block.\n           If current instruction already crossed the bound - it's ok,\n           because an exception hasn't stopped this code.\n         */\n        if ((tb->cflags & CF_USE_ICOUNT)\n            && ((pc_ptr & TARGET_PAGE_MASK)\n                != ((pc_ptr + TARGET_MAX_INSN_SIZE - 1) & TARGET_PAGE_MASK)\n                || (pc_ptr & ~TARGET_PAGE_MASK) == 0)) {\n            gen_jmp_im(pc_ptr - dc->cs_base);\n            gen_eob(dc);\n            break;\n        }\n        /* if too long translation, stop generation too */\n        if (tcg_op_buf_full() ||\n            (pc_ptr - pc_start) >= (TARGET_PAGE_SIZE - 32) ||\n            num_insns >= max_insns) {\n            gen_jmp_im(pc_ptr - dc->cs_base);\n            gen_eob(dc);\n            break;\n        }\n        if (singlestep) {\n            gen_jmp_im(pc_ptr - dc->cs_base);\n            gen_eob(dc);\n            break;\n        }\n    }\n    if (tb->cflags & CF_LAST_IO)\n        gen_io_end();\ndone_generating:\n    gen_tb_end(tb, num_insns);\n\n#ifdef DEBUG_DISAS\n    if (qemu_loglevel_mask(CPU_LOG_TB_IN_ASM)\n        && qemu_log_in_addr_range(pc_start)) {\n        int disas_flags;\n        qemu_log_lock();\n        qemu_log(\"----------------\\n\");\n        qemu_log(\"IN: %s\\n\", lookup_symbol(pc_start));\n#ifdef TARGET_X86_64\n        if (dc->code64)\n            disas_flags = 2;\n        else\n#endif\n            disas_flags = !dc->code32;\n        log_target_disas(cs, pc_start, pc_ptr - pc_start, disas_flags);\n        qemu_log(\"\\n\");\n        qemu_log_unlock();\n    }\n#endif\n\n    tb->size = pc_ptr - pc_start;\n    tb->icount = num_insns;\n}\n\nvoid restore_state_to_opc(CPUX86State *env, TranslationBlock *tb,\n                          target_ulong *data)\n{\n    int cc_op = data[1];\n    env->eip = data[0] - tb->cs_base;\n    if (cc_op != CC_OP_DYNAMIC) {\n        env->cc_op = cc_op;\n    }\n}\n"], "filenames": ["target/i386/translate.c"], "buggy_code_start_loc": [4420], "buggy_code_end_loc": [4420], "fixing_code_start_loc": [4421], "fixing_code_end_loc": [4428], "type": "CWE-94", "message": "** DISPUTED ** The disas_insn function in target/i386/translate.c in QEMU before 2.9.0, when TCG mode without hardware acceleration is used, does not limit the instruction size, which allows local users to gain privileges by creating a modified basic block that injects code into a setuid program, as demonstrated by procmail. NOTE: the vendor has stated \"this bug does not violate any security guarantees QEMU makes.\"", "other": {"cve": {"id": "CVE-2017-8284", "sourceIdentifier": "cve@mitre.org", "published": "2017-04-26T14:59:00.270", "lastModified": "2019-10-03T00:03:26.223", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "** DISPUTED ** The disas_insn function in target/i386/translate.c in QEMU before 2.9.0, when TCG mode without hardware acceleration is used, does not limit the instruction size, which allows local users to gain privileges by creating a modified basic block that injects code into a setuid program, as demonstrated by procmail. NOTE: the vendor has stated \"this bug does not violate any security guarantees QEMU makes.\""}, {"lang": "es", "value": "** EN DISPUTA ** La funci\u00f3n disas_insn en target / i386 / translate.c en QEMU para las versiones anteriores a la 2.9.0, cuando se utiliza el modo TCG sin aceleraci\u00f3n de hardware, no limita el tama\u00f1o de instrucci\u00f3n, lo que permite a los usuarios locales obtener privilegios creando un bloque b\u00e1sico modificado que inyecta c\u00f3digo en un programa setuid, como lo demuestra procmail. NOTA: el proveedor ha declarado que \"este error no viola las garant\u00edas de seguridad de QEMU.\""}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-94"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:qemu:qemu:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.8.1.1", "matchCriteriaId": "A965C2D1-C447-4324-95A4-27285ECF8909"}]}]}], "references": [{"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=1122", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/qemu/qemu/commit/30663fd26c0307e414622c7a8607fbc04f92ec14", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/qemu/qemu/commit/30663fd26c0307e414622c7a8607fbc04f92ec14"}}