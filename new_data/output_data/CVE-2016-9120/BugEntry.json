{"buggy_code": ["/*\n *\n * drivers/staging/android/ion/ion.c\n *\n * Copyright (C) 2011 Google, Inc.\n *\n * This software is licensed under the terms of the GNU General Public\n * License version 2, as published by the Free Software Foundation, and\n * may be copied, distributed, and modified under those terms.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n */\n\n#include <linux/device.h>\n#include <linux/err.h>\n#include <linux/file.h>\n#include <linux/freezer.h>\n#include <linux/fs.h>\n#include <linux/anon_inodes.h>\n#include <linux/kthread.h>\n#include <linux/list.h>\n#include <linux/memblock.h>\n#include <linux/miscdevice.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/mm_types.h>\n#include <linux/rbtree.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n#include <linux/vmalloc.h>\n#include <linux/debugfs.h>\n#include <linux/dma-buf.h>\n#include <linux/idr.h>\n\n#include \"ion.h\"\n#include \"ion_priv.h\"\n#include \"compat_ion.h\"\n\n/**\n * struct ion_device - the metadata of the ion device node\n * @dev:\t\tthe actual misc device\n * @buffers:\t\tan rb tree of all the existing buffers\n * @buffer_lock:\tlock protecting the tree of buffers\n * @lock:\t\trwsem protecting the tree of heaps and clients\n * @heaps:\t\tlist of all the heaps in the system\n * @user_clients:\tlist of all the clients created from userspace\n */\nstruct ion_device {\n\tstruct miscdevice dev;\n\tstruct rb_root buffers;\n\tstruct mutex buffer_lock;\n\tstruct rw_semaphore lock;\n\tstruct plist_head heaps;\n\tlong (*custom_ioctl)(struct ion_client *client, unsigned int cmd,\n\t\t\t     unsigned long arg);\n\tstruct rb_root clients;\n\tstruct dentry *debug_root;\n\tstruct dentry *heaps_debug_root;\n\tstruct dentry *clients_debug_root;\n};\n\n/**\n * struct ion_client - a process/hw block local address space\n * @node:\t\tnode in the tree of all clients\n * @dev:\t\tbackpointer to ion device\n * @handles:\t\tan rb tree of all the handles in this client\n * @idr:\t\tan idr space for allocating handle ids\n * @lock:\t\tlock protecting the tree of handles\n * @name:\t\tused for debugging\n * @display_name:\tused for debugging (unique version of @name)\n * @display_serial:\tused for debugging (to make display_name unique)\n * @task:\t\tused for debugging\n *\n * A client represents a list of buffers this client may access.\n * The mutex stored here is used to protect both handles tree\n * as well as the handles themselves, and should be held while modifying either.\n */\nstruct ion_client {\n\tstruct rb_node node;\n\tstruct ion_device *dev;\n\tstruct rb_root handles;\n\tstruct idr idr;\n\tstruct mutex lock;\n\tconst char *name;\n\tchar *display_name;\n\tint display_serial;\n\tstruct task_struct *task;\n\tpid_t pid;\n\tstruct dentry *debug_root;\n};\n\n/**\n * ion_handle - a client local reference to a buffer\n * @ref:\t\treference count\n * @client:\t\tback pointer to the client the buffer resides in\n * @buffer:\t\tpointer to the buffer\n * @node:\t\tnode in the client's handle rbtree\n * @kmap_cnt:\t\tcount of times this client has mapped to kernel\n * @id:\t\t\tclient-unique id allocated by client->idr\n *\n * Modifications to node, map_cnt or mapping should be protected by the\n * lock in the client.  Other fields are never changed after initialization.\n */\nstruct ion_handle {\n\tstruct kref ref;\n\tstruct ion_client *client;\n\tstruct ion_buffer *buffer;\n\tstruct rb_node node;\n\tunsigned int kmap_cnt;\n\tint id;\n};\n\nbool ion_buffer_fault_user_mappings(struct ion_buffer *buffer)\n{\n\treturn (buffer->flags & ION_FLAG_CACHED) &&\n\t\t!(buffer->flags & ION_FLAG_CACHED_NEEDS_SYNC);\n}\n\nbool ion_buffer_cached(struct ion_buffer *buffer)\n{\n\treturn !!(buffer->flags & ION_FLAG_CACHED);\n}\n\nstatic inline struct page *ion_buffer_page(struct page *page)\n{\n\treturn (struct page *)((unsigned long)page & ~(1UL));\n}\n\nstatic inline bool ion_buffer_page_is_dirty(struct page *page)\n{\n\treturn !!((unsigned long)page & 1UL);\n}\n\nstatic inline void ion_buffer_page_dirty(struct page **page)\n{\n\t*page = (struct page *)((unsigned long)(*page) | 1UL);\n}\n\nstatic inline void ion_buffer_page_clean(struct page **page)\n{\n\t*page = (struct page *)((unsigned long)(*page) & ~(1UL));\n}\n\n/* this function should only be called while dev->lock is held */\nstatic void ion_buffer_add(struct ion_device *dev,\n\t\t\t   struct ion_buffer *buffer)\n{\n\tstruct rb_node **p = &dev->buffers.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct ion_buffer *entry;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct ion_buffer, node);\n\n\t\tif (buffer < entry) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (buffer > entry) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\tpr_err(\"%s: buffer already found.\", __func__);\n\t\t\tBUG();\n\t\t}\n\t}\n\n\trb_link_node(&buffer->node, parent, p);\n\trb_insert_color(&buffer->node, &dev->buffers);\n}\n\n/* this function should only be called while dev->lock is held */\nstatic struct ion_buffer *ion_buffer_create(struct ion_heap *heap,\n\t\t\t\t     struct ion_device *dev,\n\t\t\t\t     unsigned long len,\n\t\t\t\t     unsigned long align,\n\t\t\t\t     unsigned long flags)\n{\n\tstruct ion_buffer *buffer;\n\tstruct sg_table *table;\n\tstruct scatterlist *sg;\n\tint i, ret;\n\n\tbuffer = kzalloc(sizeof(struct ion_buffer), GFP_KERNEL);\n\tif (!buffer)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuffer->heap = heap;\n\tbuffer->flags = flags;\n\tkref_init(&buffer->ref);\n\n\tret = heap->ops->allocate(heap, buffer, len, align, flags);\n\n\tif (ret) {\n\t\tif (!(heap->flags & ION_HEAP_FLAG_DEFER_FREE))\n\t\t\tgoto err2;\n\n\t\tion_heap_freelist_drain(heap, 0);\n\t\tret = heap->ops->allocate(heap, buffer, len, align,\n\t\t\t\t\t  flags);\n\t\tif (ret)\n\t\t\tgoto err2;\n\t}\n\n\tbuffer->dev = dev;\n\tbuffer->size = len;\n\n\ttable = heap->ops->map_dma(heap, buffer);\n\tif (WARN_ONCE(table == NULL,\n\t\t\t\"heap->ops->map_dma should return ERR_PTR on error\"))\n\t\ttable = ERR_PTR(-EINVAL);\n\tif (IS_ERR(table)) {\n\t\tret = -EINVAL;\n\t\tgoto err1;\n\t}\n\n\tbuffer->sg_table = table;\n\tif (ion_buffer_fault_user_mappings(buffer)) {\n\t\tint num_pages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;\n\t\tstruct scatterlist *sg;\n\t\tint i, j, k = 0;\n\n\t\tbuffer->pages = vmalloc(sizeof(struct page *) * num_pages);\n\t\tif (!buffer->pages) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor_each_sg(table->sgl, sg, table->nents, i) {\n\t\t\tstruct page *page = sg_page(sg);\n\n\t\t\tfor (j = 0; j < sg->length / PAGE_SIZE; j++)\n\t\t\t\tbuffer->pages[k++] = page++;\n\t\t}\n\t}\n\n\tbuffer->dev = dev;\n\tbuffer->size = len;\n\tINIT_LIST_HEAD(&buffer->vmas);\n\tmutex_init(&buffer->lock);\n\t/*\n\t * this will set up dma addresses for the sglist -- it is not\n\t * technically correct as per the dma api -- a specific\n\t * device isn't really taking ownership here.  However, in practice on\n\t * our systems the only dma_address space is physical addresses.\n\t * Additionally, we can't afford the overhead of invalidating every\n\t * allocation via dma_map_sg. The implicit contract here is that\n\t * memory coming from the heaps is ready for dma, ie if it has a\n\t * cached mapping that mapping has been invalidated\n\t */\n\tfor_each_sg(buffer->sg_table->sgl, sg, buffer->sg_table->nents, i) {\n\t\tsg_dma_address(sg) = sg_phys(sg);\n\t\tsg_dma_len(sg) = sg->length;\n\t}\n\tmutex_lock(&dev->buffer_lock);\n\tion_buffer_add(dev, buffer);\n\tmutex_unlock(&dev->buffer_lock);\n\treturn buffer;\n\nerr:\n\theap->ops->unmap_dma(heap, buffer);\nerr1:\n\theap->ops->free(buffer);\nerr2:\n\tkfree(buffer);\n\treturn ERR_PTR(ret);\n}\n\nvoid ion_buffer_destroy(struct ion_buffer *buffer)\n{\n\tif (WARN_ON(buffer->kmap_cnt > 0))\n\t\tbuffer->heap->ops->unmap_kernel(buffer->heap, buffer);\n\tbuffer->heap->ops->unmap_dma(buffer->heap, buffer);\n\tbuffer->heap->ops->free(buffer);\n\tvfree(buffer->pages);\n\tkfree(buffer);\n}\n\nstatic void _ion_buffer_destroy(struct kref *kref)\n{\n\tstruct ion_buffer *buffer = container_of(kref, struct ion_buffer, ref);\n\tstruct ion_heap *heap = buffer->heap;\n\tstruct ion_device *dev = buffer->dev;\n\n\tmutex_lock(&dev->buffer_lock);\n\trb_erase(&buffer->node, &dev->buffers);\n\tmutex_unlock(&dev->buffer_lock);\n\n\tif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\n\t\tion_heap_freelist_add(heap, buffer);\n\telse\n\t\tion_buffer_destroy(buffer);\n}\n\nstatic void ion_buffer_get(struct ion_buffer *buffer)\n{\n\tkref_get(&buffer->ref);\n}\n\nstatic int ion_buffer_put(struct ion_buffer *buffer)\n{\n\treturn kref_put(&buffer->ref, _ion_buffer_destroy);\n}\n\nstatic void ion_buffer_add_to_handle(struct ion_buffer *buffer)\n{\n\tmutex_lock(&buffer->lock);\n\tbuffer->handle_count++;\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic void ion_buffer_remove_from_handle(struct ion_buffer *buffer)\n{\n\t/*\n\t * when a buffer is removed from a handle, if it is not in\n\t * any other handles, copy the taskcomm and the pid of the\n\t * process it's being removed from into the buffer.  At this\n\t * point there will be no way to track what processes this buffer is\n\t * being used by, it only exists as a dma_buf file descriptor.\n\t * The taskcomm and pid can provide a debug hint as to where this fd\n\t * is in the system\n\t */\n\tmutex_lock(&buffer->lock);\n\tbuffer->handle_count--;\n\tBUG_ON(buffer->handle_count < 0);\n\tif (!buffer->handle_count) {\n\t\tstruct task_struct *task;\n\n\t\ttask = current->group_leader;\n\t\tget_task_comm(buffer->task_comm, task);\n\t\tbuffer->pid = task_pid_nr(task);\n\t}\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic struct ion_handle *ion_handle_create(struct ion_client *client,\n\t\t\t\t     struct ion_buffer *buffer)\n{\n\tstruct ion_handle *handle;\n\n\thandle = kzalloc(sizeof(struct ion_handle), GFP_KERNEL);\n\tif (!handle)\n\t\treturn ERR_PTR(-ENOMEM);\n\tkref_init(&handle->ref);\n\tRB_CLEAR_NODE(&handle->node);\n\thandle->client = client;\n\tion_buffer_get(buffer);\n\tion_buffer_add_to_handle(buffer);\n\thandle->buffer = buffer;\n\n\treturn handle;\n}\n\nstatic void ion_handle_kmap_put(struct ion_handle *);\n\nstatic void ion_handle_destroy(struct kref *kref)\n{\n\tstruct ion_handle *handle = container_of(kref, struct ion_handle, ref);\n\tstruct ion_client *client = handle->client;\n\tstruct ion_buffer *buffer = handle->buffer;\n\n\tmutex_lock(&buffer->lock);\n\twhile (handle->kmap_cnt)\n\t\tion_handle_kmap_put(handle);\n\tmutex_unlock(&buffer->lock);\n\n\tidr_remove(&client->idr, handle->id);\n\tif (!RB_EMPTY_NODE(&handle->node))\n\t\trb_erase(&handle->node, &client->handles);\n\n\tion_buffer_remove_from_handle(buffer);\n\tion_buffer_put(buffer);\n\n\tkfree(handle);\n}\n\nstruct ion_buffer *ion_handle_buffer(struct ion_handle *handle)\n{\n\treturn handle->buffer;\n}\n\nstatic void ion_handle_get(struct ion_handle *handle)\n{\n\tkref_get(&handle->ref);\n}\n\nstatic int ion_handle_put(struct ion_handle *handle)\n{\n\tstruct ion_client *client = handle->client;\n\tint ret;\n\n\tmutex_lock(&client->lock);\n\tret = kref_put(&handle->ref, ion_handle_destroy);\n\tmutex_unlock(&client->lock);\n\n\treturn ret;\n}\n\nstatic struct ion_handle *ion_handle_lookup(struct ion_client *client,\n\t\t\t\t\t    struct ion_buffer *buffer)\n{\n\tstruct rb_node *n = client->handles.rb_node;\n\n\twhile (n) {\n\t\tstruct ion_handle *entry = rb_entry(n, struct ion_handle, node);\n\n\t\tif (buffer < entry->buffer)\n\t\t\tn = n->rb_left;\n\t\telse if (buffer > entry->buffer)\n\t\t\tn = n->rb_right;\n\t\telse\n\t\t\treturn entry;\n\t}\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n\t\t\t\t\t\tint id)\n{\n\tstruct ion_handle *handle;\n\n\tmutex_lock(&client->lock);\n\thandle = idr_find(&client->idr, id);\n\tif (handle)\n\t\tion_handle_get(handle);\n\tmutex_unlock(&client->lock);\n\n\treturn handle ? handle : ERR_PTR(-EINVAL);\n}\n\nstatic bool ion_handle_validate(struct ion_client *client,\n\t\t\t\tstruct ion_handle *handle)\n{\n\tWARN_ON(!mutex_is_locked(&client->lock));\n\treturn idr_find(&client->idr, handle->id) == handle;\n}\n\nstatic int ion_handle_add(struct ion_client *client, struct ion_handle *handle)\n{\n\tint id;\n\tstruct rb_node **p = &client->handles.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct ion_handle *entry;\n\n\tid = idr_alloc(&client->idr, handle, 1, 0, GFP_KERNEL);\n\tif (id < 0)\n\t\treturn id;\n\n\thandle->id = id;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct ion_handle, node);\n\n\t\tif (handle->buffer < entry->buffer)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (handle->buffer > entry->buffer)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\tWARN(1, \"%s: buffer already found.\", __func__);\n\t}\n\n\trb_link_node(&handle->node, parent, p);\n\trb_insert_color(&handle->node, &client->handles);\n\n\treturn 0;\n}\n\nstruct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\tstruct ion_handle *handle;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_buffer *buffer = NULL;\n\tstruct ion_heap *heap;\n\tint ret;\n\n\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n\t\t len, align, heap_id_mask, flags);\n\t/*\n\t * traverse the list of heaps available in this system in priority\n\t * order.  If the heap type is supported by the client, and matches the\n\t * request of the caller allocate from it.  Repeat until allocate has\n\t * succeeded or all heaps have been tried\n\t */\n\tlen = PAGE_ALIGN(len);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&dev->lock);\n\tplist_for_each_entry(heap, &dev->heaps, node) {\n\t\t/* if the caller didn't specify this heap id */\n\t\tif (!((1 << heap->id) & heap_id_mask))\n\t\t\tcontinue;\n\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n\t\tif (!IS_ERR(buffer))\n\t\t\tbreak;\n\t}\n\tup_read(&dev->lock);\n\n\tif (buffer == NULL)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (IS_ERR(buffer))\n\t\treturn ERR_CAST(buffer);\n\n\thandle = ion_handle_create(client, buffer);\n\n\t/*\n\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n\t * and ion_handle_create will take a second reference, drop one here\n\t */\n\tion_buffer_put(buffer);\n\n\tif (IS_ERR(handle))\n\t\treturn handle;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\n\treturn handle;\n}\nEXPORT_SYMBOL(ion_alloc);\n\nvoid ion_free(struct ion_client *client, struct ion_handle *handle)\n{\n\tbool valid_handle;\n\n\tBUG_ON(client != handle->client);\n\n\tmutex_lock(&client->lock);\n\tvalid_handle = ion_handle_validate(client, handle);\n\n\tif (!valid_handle) {\n\t\tWARN(1, \"%s: invalid handle passed to free.\\n\", __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn;\n\t}\n\tmutex_unlock(&client->lock);\n\tion_handle_put(handle);\n}\nEXPORT_SYMBOL(ion_free);\n\nint ion_phys(struct ion_client *client, struct ion_handle *handle,\n\t     ion_phys_addr_t *addr, size_t *len)\n{\n\tstruct ion_buffer *buffer;\n\tint ret;\n\n\tmutex_lock(&client->lock);\n\tif (!ion_handle_validate(client, handle)) {\n\t\tmutex_unlock(&client->lock);\n\t\treturn -EINVAL;\n\t}\n\n\tbuffer = handle->buffer;\n\n\tif (!buffer->heap->ops->phys) {\n\t\tpr_err(\"%s: ion_phys is not implemented by this heap (name=%s, type=%d).\\n\",\n\t\t\t__func__, buffer->heap->name, buffer->heap->type);\n\t\tmutex_unlock(&client->lock);\n\t\treturn -ENODEV;\n\t}\n\tmutex_unlock(&client->lock);\n\tret = buffer->heap->ops->phys(buffer->heap, buffer, addr, len);\n\treturn ret;\n}\nEXPORT_SYMBOL(ion_phys);\n\nstatic void *ion_buffer_kmap_get(struct ion_buffer *buffer)\n{\n\tvoid *vaddr;\n\n\tif (buffer->kmap_cnt) {\n\t\tbuffer->kmap_cnt++;\n\t\treturn buffer->vaddr;\n\t}\n\tvaddr = buffer->heap->ops->map_kernel(buffer->heap, buffer);\n\tif (WARN_ONCE(vaddr == NULL,\n\t\t\t\"heap->ops->map_kernel should return ERR_PTR on error\"))\n\t\treturn ERR_PTR(-EINVAL);\n\tif (IS_ERR(vaddr))\n\t\treturn vaddr;\n\tbuffer->vaddr = vaddr;\n\tbuffer->kmap_cnt++;\n\treturn vaddr;\n}\n\nstatic void *ion_handle_kmap_get(struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer = handle->buffer;\n\tvoid *vaddr;\n\n\tif (handle->kmap_cnt) {\n\t\thandle->kmap_cnt++;\n\t\treturn buffer->vaddr;\n\t}\n\tvaddr = ion_buffer_kmap_get(buffer);\n\tif (IS_ERR(vaddr))\n\t\treturn vaddr;\n\thandle->kmap_cnt++;\n\treturn vaddr;\n}\n\nstatic void ion_buffer_kmap_put(struct ion_buffer *buffer)\n{\n\tbuffer->kmap_cnt--;\n\tif (!buffer->kmap_cnt) {\n\t\tbuffer->heap->ops->unmap_kernel(buffer->heap, buffer);\n\t\tbuffer->vaddr = NULL;\n\t}\n}\n\nstatic void ion_handle_kmap_put(struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer = handle->buffer;\n\n\tif (!handle->kmap_cnt) {\n\t\tWARN(1, \"%s: Double unmap detected! bailing...\\n\", __func__);\n\t\treturn;\n\t}\n\thandle->kmap_cnt--;\n\tif (!handle->kmap_cnt)\n\t\tion_buffer_kmap_put(buffer);\n}\n\nvoid *ion_map_kernel(struct ion_client *client, struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer;\n\tvoid *vaddr;\n\n\tmutex_lock(&client->lock);\n\tif (!ion_handle_validate(client, handle)) {\n\t\tpr_err(\"%s: invalid handle passed to map_kernel.\\n\",\n\t\t       __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tbuffer = handle->buffer;\n\n\tif (!handle->buffer->heap->ops->map_kernel) {\n\t\tpr_err(\"%s: map_kernel is not implemented by this heap.\\n\",\n\t\t       __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\n\tmutex_lock(&buffer->lock);\n\tvaddr = ion_handle_kmap_get(handle);\n\tmutex_unlock(&buffer->lock);\n\tmutex_unlock(&client->lock);\n\treturn vaddr;\n}\nEXPORT_SYMBOL(ion_map_kernel);\n\nvoid ion_unmap_kernel(struct ion_client *client, struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer;\n\n\tmutex_lock(&client->lock);\n\tbuffer = handle->buffer;\n\tmutex_lock(&buffer->lock);\n\tion_handle_kmap_put(handle);\n\tmutex_unlock(&buffer->lock);\n\tmutex_unlock(&client->lock);\n}\nEXPORT_SYMBOL(ion_unmap_kernel);\n\nstatic struct mutex debugfs_mutex;\nstatic struct rb_root *ion_root_client;\nstatic int is_client_alive(struct ion_client *client)\n{\n\tstruct rb_node *node;\n\tstruct ion_client *tmp;\n\tstruct ion_device *dev;\n\n\tnode = ion_root_client->rb_node;\n\tdev = container_of(ion_root_client, struct ion_device, clients);\n\n\tdown_read(&dev->lock);\n\twhile (node) {\n\t\ttmp = rb_entry(node, struct ion_client, node);\n\t\tif (client < tmp) {\n\t\t\tnode = node->rb_left;\n\t\t} else if (client > tmp) {\n\t\t\tnode = node->rb_right;\n\t\t} else {\n\t\t\tup_read(&dev->lock);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tup_read(&dev->lock);\n\treturn 0;\n}\n\nstatic int ion_debug_client_show(struct seq_file *s, void *unused)\n{\n\tstruct ion_client *client = s->private;\n\tstruct rb_node *n;\n\tsize_t sizes[ION_NUM_HEAP_IDS] = {0};\n\tconst char *names[ION_NUM_HEAP_IDS] = {NULL};\n\tint i;\n\n\tmutex_lock(&debugfs_mutex);\n\tif (!is_client_alive(client)) {\n\t\tseq_printf(s, \"ion_client 0x%p dead, can't dump its buffers\\n\",\n\t\t\t   client);\n\t\tmutex_unlock(&debugfs_mutex);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&client->lock);\n\tfor (n = rb_first(&client->handles); n; n = rb_next(n)) {\n\t\tstruct ion_handle *handle = rb_entry(n, struct ion_handle,\n\t\t\t\t\t\t     node);\n\t\tunsigned int id = handle->buffer->heap->id;\n\n\t\tif (!names[id])\n\t\t\tnames[id] = handle->buffer->heap->name;\n\t\tsizes[id] += handle->buffer->size;\n\t}\n\tmutex_unlock(&client->lock);\n\tmutex_unlock(&debugfs_mutex);\n\n\tseq_printf(s, \"%16.16s: %16.16s\\n\", \"heap_name\", \"size_in_bytes\");\n\tfor (i = 0; i < ION_NUM_HEAP_IDS; i++) {\n\t\tif (!names[i])\n\t\t\tcontinue;\n\t\tseq_printf(s, \"%16.16s: %16zu\\n\", names[i], sizes[i]);\n\t}\n\treturn 0;\n}\n\nstatic int ion_debug_client_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, ion_debug_client_show, inode->i_private);\n}\n\nstatic const struct file_operations debug_client_fops = {\n\t.open = ion_debug_client_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int ion_get_client_serial(const struct rb_root *root,\n\t\t\t\t\tconst unsigned char *name)\n{\n\tint serial = -1;\n\tstruct rb_node *node;\n\n\tfor (node = rb_first(root); node; node = rb_next(node)) {\n\t\tstruct ion_client *client = rb_entry(node, struct ion_client,\n\t\t\t\t\t\tnode);\n\n\t\tif (strcmp(client->name, name))\n\t\t\tcontinue;\n\t\tserial = max(serial, client->display_serial);\n\t}\n\treturn serial + 1;\n}\n\nstruct ion_client *ion_client_create(struct ion_device *dev,\n\t\t\t\t     const char *name)\n{\n\tstruct ion_client *client;\n\tstruct task_struct *task;\n\tstruct rb_node **p;\n\tstruct rb_node *parent = NULL;\n\tstruct ion_client *entry;\n\tpid_t pid;\n\n\tif (!name) {\n\t\tpr_err(\"%s: Name cannot be null\\n\", __func__);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tget_task_struct(current->group_leader);\n\ttask_lock(current->group_leader);\n\tpid = task_pid_nr(current->group_leader);\n\t/*\n\t * don't bother to store task struct for kernel threads,\n\t * they can't be killed anyway\n\t */\n\tif (current->group_leader->flags & PF_KTHREAD) {\n\t\tput_task_struct(current->group_leader);\n\t\ttask = NULL;\n\t} else {\n\t\ttask = current->group_leader;\n\t}\n\ttask_unlock(current->group_leader);\n\n\tclient = kzalloc(sizeof(struct ion_client), GFP_KERNEL);\n\tif (!client)\n\t\tgoto err_put_task_struct;\n\n\tclient->dev = dev;\n\tclient->handles = RB_ROOT;\n\tidr_init(&client->idr);\n\tmutex_init(&client->lock);\n\tclient->task = task;\n\tclient->pid = pid;\n\tclient->name = kstrdup(name, GFP_KERNEL);\n\tif (!client->name)\n\t\tgoto err_free_client;\n\n\tdown_write(&dev->lock);\n\tclient->display_serial = ion_get_client_serial(&dev->clients, name);\n\tclient->display_name = kasprintf(\n\t\tGFP_KERNEL, \"%s-%d\", name, client->display_serial);\n\tif (!client->display_name) {\n\t\tup_write(&dev->lock);\n\t\tgoto err_free_client_name;\n\t}\n\tp = &dev->clients.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct ion_client, node);\n\n\t\tif (client < entry)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (client > entry)\n\t\t\tp = &(*p)->rb_right;\n\t}\n\trb_link_node(&client->node, parent, p);\n\trb_insert_color(&client->node, &dev->clients);\n\n\tclient->debug_root = debugfs_create_file(client->display_name, 0664,\n\t\t\t\t\t\tdev->clients_debug_root,\n\t\t\t\t\t\tclient, &debug_client_fops);\n\tif (!client->debug_root) {\n\t\tchar buf[256], *path;\n\n\t\tpath = dentry_path(dev->clients_debug_root, buf, 256);\n\t\tpr_err(\"Failed to create client debugfs at %s/%s\\n\",\n\t\t\tpath, client->display_name);\n\t}\n\n\tup_write(&dev->lock);\n\n\treturn client;\n\nerr_free_client_name:\n\tkfree(client->name);\nerr_free_client:\n\tkfree(client);\nerr_put_task_struct:\n\tif (task)\n\t\tput_task_struct(current->group_leader);\n\treturn ERR_PTR(-ENOMEM);\n}\nEXPORT_SYMBOL(ion_client_create);\n\nvoid ion_client_destroy(struct ion_client *client)\n{\n\tstruct ion_device *dev = client->dev;\n\tstruct rb_node *n;\n\n\tpr_debug(\"%s: %d\\n\", __func__, __LINE__);\n\tmutex_lock(&debugfs_mutex);\n\twhile ((n = rb_first(&client->handles))) {\n\t\tstruct ion_handle *handle = rb_entry(n, struct ion_handle,\n\t\t\t\t\t\t     node);\n\t\tion_handle_destroy(&handle->ref);\n\t}\n\n\tidr_destroy(&client->idr);\n\n\tdown_write(&dev->lock);\n\tif (client->task)\n\t\tput_task_struct(client->task);\n\trb_erase(&client->node, &dev->clients);\n\tdebugfs_remove_recursive(client->debug_root);\n\tup_write(&dev->lock);\n\n\tkfree(client->display_name);\n\tkfree(client->name);\n\tkfree(client);\n\tmutex_unlock(&debugfs_mutex);\n}\nEXPORT_SYMBOL(ion_client_destroy);\n\nstruct sg_table *ion_sg_table(struct ion_client *client,\n\t\t\t      struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer;\n\tstruct sg_table *table;\n\n\tmutex_lock(&client->lock);\n\tif (!ion_handle_validate(client, handle)) {\n\t\tpr_err(\"%s: invalid handle passed to map_dma.\\n\",\n\t\t       __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tbuffer = handle->buffer;\n\ttable = buffer->sg_table;\n\tmutex_unlock(&client->lock);\n\treturn table;\n}\nEXPORT_SYMBOL(ion_sg_table);\n\nstatic void ion_buffer_sync_for_device(struct ion_buffer *buffer,\n\t\t\t\t       struct device *dev,\n\t\t\t\t       enum dma_data_direction direction);\n\nstatic struct sg_table *ion_map_dma_buf(struct dma_buf_attachment *attachment,\n\t\t\t\t\tenum dma_data_direction direction)\n{\n\tstruct dma_buf *dmabuf = attachment->dmabuf;\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\n\tion_buffer_sync_for_device(buffer, attachment->dev, direction);\n\treturn buffer->sg_table;\n}\n\nstatic void ion_unmap_dma_buf(struct dma_buf_attachment *attachment,\n\t\t\t      struct sg_table *table,\n\t\t\t      enum dma_data_direction direction)\n{\n}\n\nvoid ion_pages_sync_for_device(struct device *dev, struct page *page,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_table(&sg, 1);\n\tsg_set_page(&sg, page, size, 0);\n\t/*\n\t * This is not correct - sg_dma_address needs a dma_addr_t that is valid\n\t * for the targeted device, but this works on the currently targeted\n\t * hardware.\n\t */\n\tsg_dma_address(&sg) = page_to_phys(page);\n\tdma_sync_sg_for_device(dev, &sg, 1, dir);\n}\n\nstruct ion_vma_list {\n\tstruct list_head list;\n\tstruct vm_area_struct *vma;\n};\n\nstatic void ion_buffer_sync_for_device(struct ion_buffer *buffer,\n\t\t\t\t       struct device *dev,\n\t\t\t\t       enum dma_data_direction dir)\n{\n\tstruct ion_vma_list *vma_list;\n\tint pages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;\n\tint i;\n\n\tpr_debug(\"%s: syncing for device %s\\n\", __func__,\n\t\t dev ? dev_name(dev) : \"null\");\n\n\tif (!ion_buffer_fault_user_mappings(buffer))\n\t\treturn;\n\n\tmutex_lock(&buffer->lock);\n\tfor (i = 0; i < pages; i++) {\n\t\tstruct page *page = buffer->pages[i];\n\n\t\tif (ion_buffer_page_is_dirty(page))\n\t\t\tion_pages_sync_for_device(dev, ion_buffer_page(page),\n\t\t\t\t\t\t\tPAGE_SIZE, dir);\n\n\t\tion_buffer_page_clean(buffer->pages + i);\n\t}\n\tlist_for_each_entry(vma_list, &buffer->vmas, list) {\n\t\tstruct vm_area_struct *vma = vma_list->vma;\n\n\t\tzap_page_range(vma, vma->vm_start, vma->vm_end - vma->vm_start,\n\t\t\t       NULL);\n\t}\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic int ion_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct ion_buffer *buffer = vma->vm_private_data;\n\tunsigned long pfn;\n\tint ret;\n\n\tmutex_lock(&buffer->lock);\n\tion_buffer_page_dirty(buffer->pages + vmf->pgoff);\n\tBUG_ON(!buffer->pages || !buffer->pages[vmf->pgoff]);\n\n\tpfn = page_to_pfn(ion_buffer_page(buffer->pages[vmf->pgoff]));\n\tret = vm_insert_pfn(vma, (unsigned long)vmf->virtual_address, pfn);\n\tmutex_unlock(&buffer->lock);\n\tif (ret)\n\t\treturn VM_FAULT_ERROR;\n\n\treturn VM_FAULT_NOPAGE;\n}\n\nstatic void ion_vm_open(struct vm_area_struct *vma)\n{\n\tstruct ion_buffer *buffer = vma->vm_private_data;\n\tstruct ion_vma_list *vma_list;\n\n\tvma_list = kmalloc(sizeof(struct ion_vma_list), GFP_KERNEL);\n\tif (!vma_list)\n\t\treturn;\n\tvma_list->vma = vma;\n\tmutex_lock(&buffer->lock);\n\tlist_add(&vma_list->list, &buffer->vmas);\n\tmutex_unlock(&buffer->lock);\n\tpr_debug(\"%s: adding %p\\n\", __func__, vma);\n}\n\nstatic void ion_vm_close(struct vm_area_struct *vma)\n{\n\tstruct ion_buffer *buffer = vma->vm_private_data;\n\tstruct ion_vma_list *vma_list, *tmp;\n\n\tpr_debug(\"%s\\n\", __func__);\n\tmutex_lock(&buffer->lock);\n\tlist_for_each_entry_safe(vma_list, tmp, &buffer->vmas, list) {\n\t\tif (vma_list->vma != vma)\n\t\t\tcontinue;\n\t\tlist_del(&vma_list->list);\n\t\tkfree(vma_list);\n\t\tpr_debug(\"%s: deleting %p\\n\", __func__, vma);\n\t\tbreak;\n\t}\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic const struct vm_operations_struct ion_vma_ops = {\n\t.open = ion_vm_open,\n\t.close = ion_vm_close,\n\t.fault = ion_vm_fault,\n};\n\nstatic int ion_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\tint ret = 0;\n\n\tif (!buffer->heap->ops->map_user) {\n\t\tpr_err(\"%s: this heap does not define a method for mapping to userspace\\n\",\n\t\t\t__func__);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ion_buffer_fault_user_mappings(buffer)) {\n\t\tvma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND |\n\t\t\t\t\t\t\tVM_DONTDUMP;\n\t\tvma->vm_private_data = buffer;\n\t\tvma->vm_ops = &ion_vma_ops;\n\t\tion_vm_open(vma);\n\t\treturn 0;\n\t}\n\n\tif (!(buffer->flags & ION_FLAG_CACHED))\n\t\tvma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);\n\n\tmutex_lock(&buffer->lock);\n\t/* now map it to userspace */\n\tret = buffer->heap->ops->map_user(buffer->heap, buffer, vma);\n\tmutex_unlock(&buffer->lock);\n\n\tif (ret)\n\t\tpr_err(\"%s: failure mapping buffer to userspace\\n\",\n\t\t       __func__);\n\n\treturn ret;\n}\n\nstatic void ion_dma_buf_release(struct dma_buf *dmabuf)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\n\tion_buffer_put(buffer);\n}\n\nstatic void *ion_dma_buf_kmap(struct dma_buf *dmabuf, unsigned long offset)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\n\treturn buffer->vaddr + offset * PAGE_SIZE;\n}\n\nstatic void ion_dma_buf_kunmap(struct dma_buf *dmabuf, unsigned long offset,\n\t\t\t       void *ptr)\n{\n}\n\nstatic int ion_dma_buf_begin_cpu_access(struct dma_buf *dmabuf, size_t start,\n\t\t\t\t\tsize_t len,\n\t\t\t\t\tenum dma_data_direction direction)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\tvoid *vaddr;\n\n\tif (!buffer->heap->ops->map_kernel) {\n\t\tpr_err(\"%s: map kernel is not implemented by this heap.\\n\",\n\t\t       __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&buffer->lock);\n\tvaddr = ion_buffer_kmap_get(buffer);\n\tmutex_unlock(&buffer->lock);\n\treturn PTR_ERR_OR_ZERO(vaddr);\n}\n\nstatic void ion_dma_buf_end_cpu_access(struct dma_buf *dmabuf, size_t start,\n\t\t\t\t       size_t len,\n\t\t\t\t       enum dma_data_direction direction)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\n\tmutex_lock(&buffer->lock);\n\tion_buffer_kmap_put(buffer);\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic struct dma_buf_ops dma_buf_ops = {\n\t.map_dma_buf = ion_map_dma_buf,\n\t.unmap_dma_buf = ion_unmap_dma_buf,\n\t.mmap = ion_mmap,\n\t.release = ion_dma_buf_release,\n\t.begin_cpu_access = ion_dma_buf_begin_cpu_access,\n\t.end_cpu_access = ion_dma_buf_end_cpu_access,\n\t.kmap_atomic = ion_dma_buf_kmap,\n\t.kunmap_atomic = ion_dma_buf_kunmap,\n\t.kmap = ion_dma_buf_kmap,\n\t.kunmap = ion_dma_buf_kunmap,\n};\n\nstruct dma_buf *ion_share_dma_buf(struct ion_client *client,\n\t\t\t\t\t\tstruct ion_handle *handle)\n{\n\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\tstruct ion_buffer *buffer;\n\tstruct dma_buf *dmabuf;\n\tbool valid_handle;\n\n\tmutex_lock(&client->lock);\n\tvalid_handle = ion_handle_validate(client, handle);\n\tif (!valid_handle) {\n\t\tWARN(1, \"%s: invalid handle passed to share.\\n\", __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tbuffer = handle->buffer;\n\tion_buffer_get(buffer);\n\tmutex_unlock(&client->lock);\n\n\texp_info.ops = &dma_buf_ops;\n\texp_info.size = buffer->size;\n\texp_info.flags = O_RDWR;\n\texp_info.priv = buffer;\n\n\tdmabuf = dma_buf_export(&exp_info);\n\tif (IS_ERR(dmabuf)) {\n\t\tion_buffer_put(buffer);\n\t\treturn dmabuf;\n\t}\n\n\treturn dmabuf;\n}\nEXPORT_SYMBOL(ion_share_dma_buf);\n\nint ion_share_dma_buf_fd(struct ion_client *client, struct ion_handle *handle)\n{\n\tstruct dma_buf *dmabuf;\n\tint fd;\n\n\tdmabuf = ion_share_dma_buf(client, handle);\n\tif (IS_ERR(dmabuf))\n\t\treturn PTR_ERR(dmabuf);\n\n\tfd = dma_buf_fd(dmabuf, O_CLOEXEC);\n\tif (fd < 0)\n\t\tdma_buf_put(dmabuf);\n\n\treturn fd;\n}\nEXPORT_SYMBOL(ion_share_dma_buf_fd);\n\nstruct ion_handle *ion_import_dma_buf(struct ion_client *client,\n\t\t\t\t      struct dma_buf *dmabuf)\n{\n\tstruct ion_buffer *buffer;\n\tstruct ion_handle *handle;\n\tint ret;\n\n\t/* if this memory came from ion */\n\n\tif (dmabuf->ops != &dma_buf_ops) {\n\t\tpr_err(\"%s: can not import dmabuf from another exporter\\n\",\n\t\t       __func__);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tbuffer = dmabuf->priv;\n\n\tmutex_lock(&client->lock);\n\t/* if a handle exists for this buffer just take a reference to it */\n\thandle = ion_handle_lookup(client, buffer);\n\tif (!IS_ERR(handle)) {\n\t\tion_handle_get(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tgoto end;\n\t}\n\n\thandle = ion_handle_create(client, buffer);\n\tif (IS_ERR(handle)) {\n\t\tmutex_unlock(&client->lock);\n\t\tgoto end;\n\t}\n\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\nend:\n\treturn handle;\n}\nEXPORT_SYMBOL(ion_import_dma_buf);\n\nstruct ion_handle *ion_import_dma_buf_fd(struct ion_client *client, int fd)\n{\n\tstruct dma_buf *dmabuf;\n\tstruct ion_handle *handle;\n\n\tdmabuf = dma_buf_get(fd);\n\tif (IS_ERR(dmabuf))\n\t\treturn ERR_CAST(dmabuf);\n\n\thandle = ion_import_dma_buf(client, dmabuf);\n\tdma_buf_put(dmabuf);\n\treturn handle;\n}\nEXPORT_SYMBOL(ion_import_dma_buf_fd);\n\nstatic int ion_sync_for_device(struct ion_client *client, int fd)\n{\n\tstruct dma_buf *dmabuf;\n\tstruct ion_buffer *buffer;\n\n\tdmabuf = dma_buf_get(fd);\n\tif (IS_ERR(dmabuf))\n\t\treturn PTR_ERR(dmabuf);\n\n\t/* if this memory came from ion */\n\tif (dmabuf->ops != &dma_buf_ops) {\n\t\tpr_err(\"%s: can not sync dmabuf from another exporter\\n\",\n\t\t       __func__);\n\t\tdma_buf_put(dmabuf);\n\t\treturn -EINVAL;\n\t}\n\tbuffer = dmabuf->priv;\n\n\tdma_sync_sg_for_device(NULL, buffer->sg_table->sgl,\n\t\t\t       buffer->sg_table->nents, DMA_BIDIRECTIONAL);\n\tdma_buf_put(dmabuf);\n\treturn 0;\n}\n\n/* fix up the cases where the ioctl direction bits are incorrect */\nstatic unsigned int ion_ioctl_dir(unsigned int cmd)\n{\n\tswitch (cmd) {\n\tcase ION_IOC_SYNC:\n\tcase ION_IOC_FREE:\n\tcase ION_IOC_CUSTOM:\n\t\treturn _IOC_WRITE;\n\tdefault:\n\t\treturn _IOC_DIR(cmd);\n\t}\n}\n\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int ion_release(struct inode *inode, struct file *file)\n{\n\tstruct ion_client *client = file->private_data;\n\n\tpr_debug(\"%s: %d\\n\", __func__, __LINE__);\n\tion_client_destroy(client);\n\treturn 0;\n}\n\nstatic int ion_open(struct inode *inode, struct file *file)\n{\n\tstruct miscdevice *miscdev = file->private_data;\n\tstruct ion_device *dev = container_of(miscdev, struct ion_device, dev);\n\tstruct ion_client *client;\n\tchar debug_name[64];\n\n\tpr_debug(\"%s: %d\\n\", __func__, __LINE__);\n\tsnprintf(debug_name, 64, \"%u\", task_pid_nr(current->group_leader));\n\tclient = ion_client_create(dev, debug_name);\n\tif (IS_ERR(client))\n\t\treturn PTR_ERR(client);\n\tfile->private_data = client;\n\n\treturn 0;\n}\n\nstatic const struct file_operations ion_fops = {\n\t.owner          = THIS_MODULE,\n\t.open           = ion_open,\n\t.release        = ion_release,\n\t.unlocked_ioctl = ion_ioctl,\n\t.compat_ioctl   = compat_ion_ioctl,\n};\n\nstatic size_t ion_debug_heap_total(struct ion_client *client,\n\t\t\t\t   unsigned int id)\n{\n\tsize_t size = 0;\n\tstruct rb_node *n;\n\n\tmutex_lock(&client->lock);\n\tfor (n = rb_first(&client->handles); n; n = rb_next(n)) {\n\t\tstruct ion_handle *handle = rb_entry(n,\n\t\t\t\t\t\t     struct ion_handle,\n\t\t\t\t\t\t     node);\n\t\tif (handle->buffer->heap->id == id)\n\t\t\tsize += handle->buffer->size;\n\t}\n\tmutex_unlock(&client->lock);\n\treturn size;\n}\n\nstatic int ion_debug_heap_show(struct seq_file *s, void *unused)\n{\n\tstruct ion_heap *heap = s->private;\n\tstruct ion_device *dev = heap->dev;\n\tstruct rb_node *n;\n\tsize_t total_size = 0;\n\tsize_t total_orphaned_size = 0;\n\n\tseq_printf(s, \"%16s %16s %16s\\n\", \"client\", \"pid\", \"size\");\n\tseq_puts(s, \"----------------------------------------------------\\n\");\n\n\tmutex_lock(&debugfs_mutex);\n\tfor (n = rb_first(&dev->clients); n; n = rb_next(n)) {\n\t\tstruct ion_client *client = rb_entry(n, struct ion_client,\n\t\t\t\t\t\t     node);\n\t\tsize_t size = ion_debug_heap_total(client, heap->id);\n\n\t\tif (!size)\n\t\t\tcontinue;\n\t\tif (client->task) {\n\t\t\tchar task_comm[TASK_COMM_LEN];\n\n\t\t\tget_task_comm(task_comm, client->task);\n\t\t\tseq_printf(s, \"%16s %16u %16zu\\n\", task_comm,\n\t\t\t\t   client->pid, size);\n\t\t} else {\n\t\t\tseq_printf(s, \"%16s %16u %16zu\\n\", client->name,\n\t\t\t\t   client->pid, size);\n\t\t}\n\t}\n\tmutex_unlock(&debugfs_mutex);\n\n\tseq_puts(s, \"----------------------------------------------------\\n\");\n\tseq_puts(s, \"orphaned allocations (info is from last known client):\\n\");\n\tmutex_lock(&dev->buffer_lock);\n\tfor (n = rb_first(&dev->buffers); n; n = rb_next(n)) {\n\t\tstruct ion_buffer *buffer = rb_entry(n, struct ion_buffer,\n\t\t\t\t\t\t     node);\n\t\tif (buffer->heap->id != heap->id)\n\t\t\tcontinue;\n\t\ttotal_size += buffer->size;\n\t\tif (!buffer->handle_count) {\n\t\t\tseq_printf(s, \"%16s %16u %16zu %d %d\\n\",\n\t\t\t\t   buffer->task_comm, buffer->pid,\n\t\t\t\t   buffer->size, buffer->kmap_cnt,\n\t\t\t\t   atomic_read(&buffer->ref.refcount));\n\t\t\ttotal_orphaned_size += buffer->size;\n\t\t}\n\t}\n\tmutex_unlock(&dev->buffer_lock);\n\tseq_puts(s, \"----------------------------------------------------\\n\");\n\tseq_printf(s, \"%16s %16zu\\n\", \"total orphaned\",\n\t\t   total_orphaned_size);\n\tseq_printf(s, \"%16s %16zu\\n\", \"total \", total_size);\n\tif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\n\t\tseq_printf(s, \"%16s %16zu\\n\", \"deferred free\",\n\t\t\t\theap->free_list_size);\n\tseq_puts(s, \"----------------------------------------------------\\n\");\n\n\tif (heap->debug_show)\n\t\theap->debug_show(heap, s, unused);\n\n\treturn 0;\n}\n\nstatic int ion_debug_heap_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, ion_debug_heap_show, inode->i_private);\n}\n\nstatic const struct file_operations debug_heap_fops = {\n\t.open = ion_debug_heap_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int debug_shrink_set(void *data, u64 val)\n{\n\tstruct ion_heap *heap = data;\n\tstruct shrink_control sc;\n\tint objs;\n\n\tsc.gfp_mask = -1;\n\tsc.nr_to_scan = val;\n\n\tif (!val) {\n\t\tobjs = heap->shrinker.count_objects(&heap->shrinker, &sc);\n\t\tsc.nr_to_scan = objs;\n\t}\n\n\theap->shrinker.scan_objects(&heap->shrinker, &sc);\n\treturn 0;\n}\n\nstatic int debug_shrink_get(void *data, u64 *val)\n{\n\tstruct ion_heap *heap = data;\n\tstruct shrink_control sc;\n\tint objs;\n\n\tsc.gfp_mask = -1;\n\tsc.nr_to_scan = 0;\n\n\tobjs = heap->shrinker.count_objects(&heap->shrinker, &sc);\n\t*val = objs;\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(debug_shrink_fops, debug_shrink_get,\n\t\t\tdebug_shrink_set, \"%llu\\n\");\n\nvoid ion_device_add_heap(struct ion_device *dev, struct ion_heap *heap)\n{\n\tstruct dentry *debug_file;\n\n\tif (!heap->ops->allocate || !heap->ops->free || !heap->ops->map_dma ||\n\t    !heap->ops->unmap_dma)\n\t\tpr_err(\"%s: can not add heap with invalid ops struct.\\n\",\n\t\t       __func__);\n\n\tspin_lock_init(&heap->free_lock);\n\theap->free_list_size = 0;\n\n\tif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\n\t\tion_heap_init_deferred_free(heap);\n\n\tif ((heap->flags & ION_HEAP_FLAG_DEFER_FREE) || heap->ops->shrink)\n\t\tion_heap_init_shrinker(heap);\n\n\theap->dev = dev;\n\tdown_write(&dev->lock);\n\t/*\n\t * use negative heap->id to reverse the priority -- when traversing\n\t * the list later attempt higher id numbers first\n\t */\n\tplist_node_init(&heap->node, -heap->id);\n\tplist_add(&heap->node, &dev->heaps);\n\tdebug_file = debugfs_create_file(heap->name, 0664,\n\t\t\t\t\tdev->heaps_debug_root, heap,\n\t\t\t\t\t&debug_heap_fops);\n\n\tif (!debug_file) {\n\t\tchar buf[256], *path;\n\n\t\tpath = dentry_path(dev->heaps_debug_root, buf, 256);\n\t\tpr_err(\"Failed to create heap debugfs at %s/%s\\n\",\n\t\t\tpath, heap->name);\n\t}\n\n\tif (heap->shrinker.count_objects && heap->shrinker.scan_objects) {\n\t\tchar debug_name[64];\n\n\t\tsnprintf(debug_name, 64, \"%s_shrink\", heap->name);\n\t\tdebug_file = debugfs_create_file(\n\t\t\tdebug_name, 0644, dev->heaps_debug_root, heap,\n\t\t\t&debug_shrink_fops);\n\t\tif (!debug_file) {\n\t\t\tchar buf[256], *path;\n\n\t\t\tpath = dentry_path(dev->heaps_debug_root, buf, 256);\n\t\t\tpr_err(\"Failed to create heap shrinker debugfs at %s/%s\\n\",\n\t\t\t\tpath, debug_name);\n\t\t}\n\t}\n\n\tup_write(&dev->lock);\n}\nEXPORT_SYMBOL(ion_device_add_heap);\n\nstruct ion_device *ion_device_create(long (*custom_ioctl)\n\t\t\t\t     (struct ion_client *client,\n\t\t\t\t      unsigned int cmd,\n\t\t\t\t      unsigned long arg))\n{\n\tstruct ion_device *idev;\n\tint ret;\n\n\tidev = kzalloc(sizeof(struct ion_device), GFP_KERNEL);\n\tif (!idev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tidev->dev.minor = MISC_DYNAMIC_MINOR;\n\tidev->dev.name = \"ion\";\n\tidev->dev.fops = &ion_fops;\n\tidev->dev.parent = NULL;\n\tret = misc_register(&idev->dev);\n\tif (ret) {\n\t\tpr_err(\"ion: failed to register misc device.\\n\");\n\t\tkfree(idev);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tidev->debug_root = debugfs_create_dir(\"ion\", NULL);\n\tif (!idev->debug_root) {\n\t\tpr_err(\"ion: failed to create debugfs root directory.\\n\");\n\t\tgoto debugfs_done;\n\t}\n\tidev->heaps_debug_root = debugfs_create_dir(\"heaps\", idev->debug_root);\n\tif (!idev->heaps_debug_root) {\n\t\tpr_err(\"ion: failed to create debugfs heaps directory.\\n\");\n\t\tgoto debugfs_done;\n\t}\n\tidev->clients_debug_root = debugfs_create_dir(\"clients\",\n\t\t\t\t\t\tidev->debug_root);\n\tif (!idev->clients_debug_root)\n\t\tpr_err(\"ion: failed to create debugfs clients directory.\\n\");\n\ndebugfs_done:\n\n\tidev->custom_ioctl = custom_ioctl;\n\tidev->buffers = RB_ROOT;\n\tmutex_init(&idev->buffer_lock);\n\tinit_rwsem(&idev->lock);\n\tplist_head_init(&idev->heaps);\n\tidev->clients = RB_ROOT;\n\tion_root_client = &idev->clients;\n\tmutex_init(&debugfs_mutex);\n\treturn idev;\n}\nEXPORT_SYMBOL(ion_device_create);\n\nvoid ion_device_destroy(struct ion_device *dev)\n{\n\tmisc_deregister(&dev->dev);\n\tdebugfs_remove_recursive(dev->debug_root);\n\t/* XXX need to free the heaps and clients ? */\n\tkfree(dev);\n}\nEXPORT_SYMBOL(ion_device_destroy);\n\nvoid __init ion_reserve(struct ion_platform_data *data)\n{\n\tint i;\n\n\tfor (i = 0; i < data->nr; i++) {\n\t\tif (data->heaps[i].size == 0)\n\t\t\tcontinue;\n\n\t\tif (data->heaps[i].base == 0) {\n\t\t\tphys_addr_t paddr;\n\n\t\t\tpaddr = memblock_alloc_base(data->heaps[i].size,\n\t\t\t\t\t\t    data->heaps[i].align,\n\t\t\t\t\t\t    MEMBLOCK_ALLOC_ANYWHERE);\n\t\t\tif (!paddr) {\n\t\t\t\tpr_err(\"%s: error allocating memblock for heap %d\\n\",\n\t\t\t\t\t__func__, i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tdata->heaps[i].base = paddr;\n\t\t} else {\n\t\t\tint ret = memblock_reserve(data->heaps[i].base,\n\t\t\t\t\t       data->heaps[i].size);\n\t\t\tif (ret)\n\t\t\t\tpr_err(\"memblock reserve of %zx@%lx failed\\n\",\n\t\t\t\t       data->heaps[i].size,\n\t\t\t\t       data->heaps[i].base);\n\t\t}\n\t\tpr_info(\"%s: %s reserved base %lx size %zu\\n\", __func__,\n\t\t\tdata->heaps[i].name,\n\t\t\tdata->heaps[i].base,\n\t\t\tdata->heaps[i].size);\n\t}\n}\n"], "fixing_code": ["/*\n *\n * drivers/staging/android/ion/ion.c\n *\n * Copyright (C) 2011 Google, Inc.\n *\n * This software is licensed under the terms of the GNU General Public\n * License version 2, as published by the Free Software Foundation, and\n * may be copied, distributed, and modified under those terms.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n */\n\n#include <linux/device.h>\n#include <linux/err.h>\n#include <linux/file.h>\n#include <linux/freezer.h>\n#include <linux/fs.h>\n#include <linux/anon_inodes.h>\n#include <linux/kthread.h>\n#include <linux/list.h>\n#include <linux/memblock.h>\n#include <linux/miscdevice.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/mm_types.h>\n#include <linux/rbtree.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n#include <linux/vmalloc.h>\n#include <linux/debugfs.h>\n#include <linux/dma-buf.h>\n#include <linux/idr.h>\n\n#include \"ion.h\"\n#include \"ion_priv.h\"\n#include \"compat_ion.h\"\n\n/**\n * struct ion_device - the metadata of the ion device node\n * @dev:\t\tthe actual misc device\n * @buffers:\t\tan rb tree of all the existing buffers\n * @buffer_lock:\tlock protecting the tree of buffers\n * @lock:\t\trwsem protecting the tree of heaps and clients\n * @heaps:\t\tlist of all the heaps in the system\n * @user_clients:\tlist of all the clients created from userspace\n */\nstruct ion_device {\n\tstruct miscdevice dev;\n\tstruct rb_root buffers;\n\tstruct mutex buffer_lock;\n\tstruct rw_semaphore lock;\n\tstruct plist_head heaps;\n\tlong (*custom_ioctl)(struct ion_client *client, unsigned int cmd,\n\t\t\t     unsigned long arg);\n\tstruct rb_root clients;\n\tstruct dentry *debug_root;\n\tstruct dentry *heaps_debug_root;\n\tstruct dentry *clients_debug_root;\n};\n\n/**\n * struct ion_client - a process/hw block local address space\n * @node:\t\tnode in the tree of all clients\n * @dev:\t\tbackpointer to ion device\n * @handles:\t\tan rb tree of all the handles in this client\n * @idr:\t\tan idr space for allocating handle ids\n * @lock:\t\tlock protecting the tree of handles\n * @name:\t\tused for debugging\n * @display_name:\tused for debugging (unique version of @name)\n * @display_serial:\tused for debugging (to make display_name unique)\n * @task:\t\tused for debugging\n *\n * A client represents a list of buffers this client may access.\n * The mutex stored here is used to protect both handles tree\n * as well as the handles themselves, and should be held while modifying either.\n */\nstruct ion_client {\n\tstruct rb_node node;\n\tstruct ion_device *dev;\n\tstruct rb_root handles;\n\tstruct idr idr;\n\tstruct mutex lock;\n\tconst char *name;\n\tchar *display_name;\n\tint display_serial;\n\tstruct task_struct *task;\n\tpid_t pid;\n\tstruct dentry *debug_root;\n};\n\n/**\n * ion_handle - a client local reference to a buffer\n * @ref:\t\treference count\n * @client:\t\tback pointer to the client the buffer resides in\n * @buffer:\t\tpointer to the buffer\n * @node:\t\tnode in the client's handle rbtree\n * @kmap_cnt:\t\tcount of times this client has mapped to kernel\n * @id:\t\t\tclient-unique id allocated by client->idr\n *\n * Modifications to node, map_cnt or mapping should be protected by the\n * lock in the client.  Other fields are never changed after initialization.\n */\nstruct ion_handle {\n\tstruct kref ref;\n\tstruct ion_client *client;\n\tstruct ion_buffer *buffer;\n\tstruct rb_node node;\n\tunsigned int kmap_cnt;\n\tint id;\n};\n\nbool ion_buffer_fault_user_mappings(struct ion_buffer *buffer)\n{\n\treturn (buffer->flags & ION_FLAG_CACHED) &&\n\t\t!(buffer->flags & ION_FLAG_CACHED_NEEDS_SYNC);\n}\n\nbool ion_buffer_cached(struct ion_buffer *buffer)\n{\n\treturn !!(buffer->flags & ION_FLAG_CACHED);\n}\n\nstatic inline struct page *ion_buffer_page(struct page *page)\n{\n\treturn (struct page *)((unsigned long)page & ~(1UL));\n}\n\nstatic inline bool ion_buffer_page_is_dirty(struct page *page)\n{\n\treturn !!((unsigned long)page & 1UL);\n}\n\nstatic inline void ion_buffer_page_dirty(struct page **page)\n{\n\t*page = (struct page *)((unsigned long)(*page) | 1UL);\n}\n\nstatic inline void ion_buffer_page_clean(struct page **page)\n{\n\t*page = (struct page *)((unsigned long)(*page) & ~(1UL));\n}\n\n/* this function should only be called while dev->lock is held */\nstatic void ion_buffer_add(struct ion_device *dev,\n\t\t\t   struct ion_buffer *buffer)\n{\n\tstruct rb_node **p = &dev->buffers.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct ion_buffer *entry;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct ion_buffer, node);\n\n\t\tif (buffer < entry) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (buffer > entry) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\tpr_err(\"%s: buffer already found.\", __func__);\n\t\t\tBUG();\n\t\t}\n\t}\n\n\trb_link_node(&buffer->node, parent, p);\n\trb_insert_color(&buffer->node, &dev->buffers);\n}\n\n/* this function should only be called while dev->lock is held */\nstatic struct ion_buffer *ion_buffer_create(struct ion_heap *heap,\n\t\t\t\t     struct ion_device *dev,\n\t\t\t\t     unsigned long len,\n\t\t\t\t     unsigned long align,\n\t\t\t\t     unsigned long flags)\n{\n\tstruct ion_buffer *buffer;\n\tstruct sg_table *table;\n\tstruct scatterlist *sg;\n\tint i, ret;\n\n\tbuffer = kzalloc(sizeof(struct ion_buffer), GFP_KERNEL);\n\tif (!buffer)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuffer->heap = heap;\n\tbuffer->flags = flags;\n\tkref_init(&buffer->ref);\n\n\tret = heap->ops->allocate(heap, buffer, len, align, flags);\n\n\tif (ret) {\n\t\tif (!(heap->flags & ION_HEAP_FLAG_DEFER_FREE))\n\t\t\tgoto err2;\n\n\t\tion_heap_freelist_drain(heap, 0);\n\t\tret = heap->ops->allocate(heap, buffer, len, align,\n\t\t\t\t\t  flags);\n\t\tif (ret)\n\t\t\tgoto err2;\n\t}\n\n\tbuffer->dev = dev;\n\tbuffer->size = len;\n\n\ttable = heap->ops->map_dma(heap, buffer);\n\tif (WARN_ONCE(table == NULL,\n\t\t\t\"heap->ops->map_dma should return ERR_PTR on error\"))\n\t\ttable = ERR_PTR(-EINVAL);\n\tif (IS_ERR(table)) {\n\t\tret = -EINVAL;\n\t\tgoto err1;\n\t}\n\n\tbuffer->sg_table = table;\n\tif (ion_buffer_fault_user_mappings(buffer)) {\n\t\tint num_pages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;\n\t\tstruct scatterlist *sg;\n\t\tint i, j, k = 0;\n\n\t\tbuffer->pages = vmalloc(sizeof(struct page *) * num_pages);\n\t\tif (!buffer->pages) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor_each_sg(table->sgl, sg, table->nents, i) {\n\t\t\tstruct page *page = sg_page(sg);\n\n\t\t\tfor (j = 0; j < sg->length / PAGE_SIZE; j++)\n\t\t\t\tbuffer->pages[k++] = page++;\n\t\t}\n\t}\n\n\tbuffer->dev = dev;\n\tbuffer->size = len;\n\tINIT_LIST_HEAD(&buffer->vmas);\n\tmutex_init(&buffer->lock);\n\t/*\n\t * this will set up dma addresses for the sglist -- it is not\n\t * technically correct as per the dma api -- a specific\n\t * device isn't really taking ownership here.  However, in practice on\n\t * our systems the only dma_address space is physical addresses.\n\t * Additionally, we can't afford the overhead of invalidating every\n\t * allocation via dma_map_sg. The implicit contract here is that\n\t * memory coming from the heaps is ready for dma, ie if it has a\n\t * cached mapping that mapping has been invalidated\n\t */\n\tfor_each_sg(buffer->sg_table->sgl, sg, buffer->sg_table->nents, i) {\n\t\tsg_dma_address(sg) = sg_phys(sg);\n\t\tsg_dma_len(sg) = sg->length;\n\t}\n\tmutex_lock(&dev->buffer_lock);\n\tion_buffer_add(dev, buffer);\n\tmutex_unlock(&dev->buffer_lock);\n\treturn buffer;\n\nerr:\n\theap->ops->unmap_dma(heap, buffer);\nerr1:\n\theap->ops->free(buffer);\nerr2:\n\tkfree(buffer);\n\treturn ERR_PTR(ret);\n}\n\nvoid ion_buffer_destroy(struct ion_buffer *buffer)\n{\n\tif (WARN_ON(buffer->kmap_cnt > 0))\n\t\tbuffer->heap->ops->unmap_kernel(buffer->heap, buffer);\n\tbuffer->heap->ops->unmap_dma(buffer->heap, buffer);\n\tbuffer->heap->ops->free(buffer);\n\tvfree(buffer->pages);\n\tkfree(buffer);\n}\n\nstatic void _ion_buffer_destroy(struct kref *kref)\n{\n\tstruct ion_buffer *buffer = container_of(kref, struct ion_buffer, ref);\n\tstruct ion_heap *heap = buffer->heap;\n\tstruct ion_device *dev = buffer->dev;\n\n\tmutex_lock(&dev->buffer_lock);\n\trb_erase(&buffer->node, &dev->buffers);\n\tmutex_unlock(&dev->buffer_lock);\n\n\tif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\n\t\tion_heap_freelist_add(heap, buffer);\n\telse\n\t\tion_buffer_destroy(buffer);\n}\n\nstatic void ion_buffer_get(struct ion_buffer *buffer)\n{\n\tkref_get(&buffer->ref);\n}\n\nstatic int ion_buffer_put(struct ion_buffer *buffer)\n{\n\treturn kref_put(&buffer->ref, _ion_buffer_destroy);\n}\n\nstatic void ion_buffer_add_to_handle(struct ion_buffer *buffer)\n{\n\tmutex_lock(&buffer->lock);\n\tbuffer->handle_count++;\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic void ion_buffer_remove_from_handle(struct ion_buffer *buffer)\n{\n\t/*\n\t * when a buffer is removed from a handle, if it is not in\n\t * any other handles, copy the taskcomm and the pid of the\n\t * process it's being removed from into the buffer.  At this\n\t * point there will be no way to track what processes this buffer is\n\t * being used by, it only exists as a dma_buf file descriptor.\n\t * The taskcomm and pid can provide a debug hint as to where this fd\n\t * is in the system\n\t */\n\tmutex_lock(&buffer->lock);\n\tbuffer->handle_count--;\n\tBUG_ON(buffer->handle_count < 0);\n\tif (!buffer->handle_count) {\n\t\tstruct task_struct *task;\n\n\t\ttask = current->group_leader;\n\t\tget_task_comm(buffer->task_comm, task);\n\t\tbuffer->pid = task_pid_nr(task);\n\t}\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic struct ion_handle *ion_handle_create(struct ion_client *client,\n\t\t\t\t     struct ion_buffer *buffer)\n{\n\tstruct ion_handle *handle;\n\n\thandle = kzalloc(sizeof(struct ion_handle), GFP_KERNEL);\n\tif (!handle)\n\t\treturn ERR_PTR(-ENOMEM);\n\tkref_init(&handle->ref);\n\tRB_CLEAR_NODE(&handle->node);\n\thandle->client = client;\n\tion_buffer_get(buffer);\n\tion_buffer_add_to_handle(buffer);\n\thandle->buffer = buffer;\n\n\treturn handle;\n}\n\nstatic void ion_handle_kmap_put(struct ion_handle *);\n\nstatic void ion_handle_destroy(struct kref *kref)\n{\n\tstruct ion_handle *handle = container_of(kref, struct ion_handle, ref);\n\tstruct ion_client *client = handle->client;\n\tstruct ion_buffer *buffer = handle->buffer;\n\n\tmutex_lock(&buffer->lock);\n\twhile (handle->kmap_cnt)\n\t\tion_handle_kmap_put(handle);\n\tmutex_unlock(&buffer->lock);\n\n\tidr_remove(&client->idr, handle->id);\n\tif (!RB_EMPTY_NODE(&handle->node))\n\t\trb_erase(&handle->node, &client->handles);\n\n\tion_buffer_remove_from_handle(buffer);\n\tion_buffer_put(buffer);\n\n\tkfree(handle);\n}\n\nstruct ion_buffer *ion_handle_buffer(struct ion_handle *handle)\n{\n\treturn handle->buffer;\n}\n\nstatic void ion_handle_get(struct ion_handle *handle)\n{\n\tkref_get(&handle->ref);\n}\n\nstatic int ion_handle_put_nolock(struct ion_handle *handle)\n{\n\tint ret;\n\n\tret = kref_put(&handle->ref, ion_handle_destroy);\n\n\treturn ret;\n}\n\nint ion_handle_put(struct ion_handle *handle)\n{\n\tstruct ion_client *client = handle->client;\n\tint ret;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_put_nolock(handle);\n\tmutex_unlock(&client->lock);\n\n\treturn ret;\n}\n\nstatic struct ion_handle *ion_handle_lookup(struct ion_client *client,\n\t\t\t\t\t    struct ion_buffer *buffer)\n{\n\tstruct rb_node *n = client->handles.rb_node;\n\n\twhile (n) {\n\t\tstruct ion_handle *entry = rb_entry(n, struct ion_handle, node);\n\n\t\tif (buffer < entry->buffer)\n\t\t\tn = n->rb_left;\n\t\telse if (buffer > entry->buffer)\n\t\t\tn = n->rb_right;\n\t\telse\n\t\t\treturn entry;\n\t}\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic struct ion_handle *ion_handle_get_by_id_nolock(struct ion_client *client,\n\t\t\t\t\t\tint id)\n{\n\tstruct ion_handle *handle;\n\n\thandle = idr_find(&client->idr, id);\n\tif (handle)\n\t\tion_handle_get(handle);\n\n\treturn handle ? handle : ERR_PTR(-EINVAL);\n}\n\nstruct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n\t\t\t\t\t\tint id)\n{\n\tstruct ion_handle *handle;\n\n\tmutex_lock(&client->lock);\n\thandle = ion_handle_get_by_id_nolock(client, id);\n\tmutex_unlock(&client->lock);\n\n\treturn handle;\n}\n\nstatic bool ion_handle_validate(struct ion_client *client,\n\t\t\t\tstruct ion_handle *handle)\n{\n\tWARN_ON(!mutex_is_locked(&client->lock));\n\treturn idr_find(&client->idr, handle->id) == handle;\n}\n\nstatic int ion_handle_add(struct ion_client *client, struct ion_handle *handle)\n{\n\tint id;\n\tstruct rb_node **p = &client->handles.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct ion_handle *entry;\n\n\tid = idr_alloc(&client->idr, handle, 1, 0, GFP_KERNEL);\n\tif (id < 0)\n\t\treturn id;\n\n\thandle->id = id;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct ion_handle, node);\n\n\t\tif (handle->buffer < entry->buffer)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (handle->buffer > entry->buffer)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\tWARN(1, \"%s: buffer already found.\", __func__);\n\t}\n\n\trb_link_node(&handle->node, parent, p);\n\trb_insert_color(&handle->node, &client->handles);\n\n\treturn 0;\n}\n\nstruct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\tstruct ion_handle *handle;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_buffer *buffer = NULL;\n\tstruct ion_heap *heap;\n\tint ret;\n\n\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n\t\t len, align, heap_id_mask, flags);\n\t/*\n\t * traverse the list of heaps available in this system in priority\n\t * order.  If the heap type is supported by the client, and matches the\n\t * request of the caller allocate from it.  Repeat until allocate has\n\t * succeeded or all heaps have been tried\n\t */\n\tlen = PAGE_ALIGN(len);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&dev->lock);\n\tplist_for_each_entry(heap, &dev->heaps, node) {\n\t\t/* if the caller didn't specify this heap id */\n\t\tif (!((1 << heap->id) & heap_id_mask))\n\t\t\tcontinue;\n\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n\t\tif (!IS_ERR(buffer))\n\t\t\tbreak;\n\t}\n\tup_read(&dev->lock);\n\n\tif (buffer == NULL)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (IS_ERR(buffer))\n\t\treturn ERR_CAST(buffer);\n\n\thandle = ion_handle_create(client, buffer);\n\n\t/*\n\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n\t * and ion_handle_create will take a second reference, drop one here\n\t */\n\tion_buffer_put(buffer);\n\n\tif (IS_ERR(handle))\n\t\treturn handle;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\n\treturn handle;\n}\nEXPORT_SYMBOL(ion_alloc);\n\nstatic void ion_free_nolock(struct ion_client *client, struct ion_handle *handle)\n{\n\tbool valid_handle;\n\n\tBUG_ON(client != handle->client);\n\n\tvalid_handle = ion_handle_validate(client, handle);\n\n\tif (!valid_handle) {\n\t\tWARN(1, \"%s: invalid handle passed to free.\\n\", __func__);\n\t\treturn;\n\t}\n\tion_handle_put_nolock(handle);\n}\n\nvoid ion_free(struct ion_client *client, struct ion_handle *handle)\n{\n\tBUG_ON(client != handle->client);\n\n\tmutex_lock(&client->lock);\n\tion_free_nolock(client, handle);\n\tmutex_unlock(&client->lock);\n}\nEXPORT_SYMBOL(ion_free);\n\nint ion_phys(struct ion_client *client, struct ion_handle *handle,\n\t     ion_phys_addr_t *addr, size_t *len)\n{\n\tstruct ion_buffer *buffer;\n\tint ret;\n\n\tmutex_lock(&client->lock);\n\tif (!ion_handle_validate(client, handle)) {\n\t\tmutex_unlock(&client->lock);\n\t\treturn -EINVAL;\n\t}\n\n\tbuffer = handle->buffer;\n\n\tif (!buffer->heap->ops->phys) {\n\t\tpr_err(\"%s: ion_phys is not implemented by this heap (name=%s, type=%d).\\n\",\n\t\t\t__func__, buffer->heap->name, buffer->heap->type);\n\t\tmutex_unlock(&client->lock);\n\t\treturn -ENODEV;\n\t}\n\tmutex_unlock(&client->lock);\n\tret = buffer->heap->ops->phys(buffer->heap, buffer, addr, len);\n\treturn ret;\n}\nEXPORT_SYMBOL(ion_phys);\n\nstatic void *ion_buffer_kmap_get(struct ion_buffer *buffer)\n{\n\tvoid *vaddr;\n\n\tif (buffer->kmap_cnt) {\n\t\tbuffer->kmap_cnt++;\n\t\treturn buffer->vaddr;\n\t}\n\tvaddr = buffer->heap->ops->map_kernel(buffer->heap, buffer);\n\tif (WARN_ONCE(vaddr == NULL,\n\t\t\t\"heap->ops->map_kernel should return ERR_PTR on error\"))\n\t\treturn ERR_PTR(-EINVAL);\n\tif (IS_ERR(vaddr))\n\t\treturn vaddr;\n\tbuffer->vaddr = vaddr;\n\tbuffer->kmap_cnt++;\n\treturn vaddr;\n}\n\nstatic void *ion_handle_kmap_get(struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer = handle->buffer;\n\tvoid *vaddr;\n\n\tif (handle->kmap_cnt) {\n\t\thandle->kmap_cnt++;\n\t\treturn buffer->vaddr;\n\t}\n\tvaddr = ion_buffer_kmap_get(buffer);\n\tif (IS_ERR(vaddr))\n\t\treturn vaddr;\n\thandle->kmap_cnt++;\n\treturn vaddr;\n}\n\nstatic void ion_buffer_kmap_put(struct ion_buffer *buffer)\n{\n\tbuffer->kmap_cnt--;\n\tif (!buffer->kmap_cnt) {\n\t\tbuffer->heap->ops->unmap_kernel(buffer->heap, buffer);\n\t\tbuffer->vaddr = NULL;\n\t}\n}\n\nstatic void ion_handle_kmap_put(struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer = handle->buffer;\n\n\tif (!handle->kmap_cnt) {\n\t\tWARN(1, \"%s: Double unmap detected! bailing...\\n\", __func__);\n\t\treturn;\n\t}\n\thandle->kmap_cnt--;\n\tif (!handle->kmap_cnt)\n\t\tion_buffer_kmap_put(buffer);\n}\n\nvoid *ion_map_kernel(struct ion_client *client, struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer;\n\tvoid *vaddr;\n\n\tmutex_lock(&client->lock);\n\tif (!ion_handle_validate(client, handle)) {\n\t\tpr_err(\"%s: invalid handle passed to map_kernel.\\n\",\n\t\t       __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tbuffer = handle->buffer;\n\n\tif (!handle->buffer->heap->ops->map_kernel) {\n\t\tpr_err(\"%s: map_kernel is not implemented by this heap.\\n\",\n\t\t       __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\n\tmutex_lock(&buffer->lock);\n\tvaddr = ion_handle_kmap_get(handle);\n\tmutex_unlock(&buffer->lock);\n\tmutex_unlock(&client->lock);\n\treturn vaddr;\n}\nEXPORT_SYMBOL(ion_map_kernel);\n\nvoid ion_unmap_kernel(struct ion_client *client, struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer;\n\n\tmutex_lock(&client->lock);\n\tbuffer = handle->buffer;\n\tmutex_lock(&buffer->lock);\n\tion_handle_kmap_put(handle);\n\tmutex_unlock(&buffer->lock);\n\tmutex_unlock(&client->lock);\n}\nEXPORT_SYMBOL(ion_unmap_kernel);\n\nstatic struct mutex debugfs_mutex;\nstatic struct rb_root *ion_root_client;\nstatic int is_client_alive(struct ion_client *client)\n{\n\tstruct rb_node *node;\n\tstruct ion_client *tmp;\n\tstruct ion_device *dev;\n\n\tnode = ion_root_client->rb_node;\n\tdev = container_of(ion_root_client, struct ion_device, clients);\n\n\tdown_read(&dev->lock);\n\twhile (node) {\n\t\ttmp = rb_entry(node, struct ion_client, node);\n\t\tif (client < tmp) {\n\t\t\tnode = node->rb_left;\n\t\t} else if (client > tmp) {\n\t\t\tnode = node->rb_right;\n\t\t} else {\n\t\t\tup_read(&dev->lock);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tup_read(&dev->lock);\n\treturn 0;\n}\n\nstatic int ion_debug_client_show(struct seq_file *s, void *unused)\n{\n\tstruct ion_client *client = s->private;\n\tstruct rb_node *n;\n\tsize_t sizes[ION_NUM_HEAP_IDS] = {0};\n\tconst char *names[ION_NUM_HEAP_IDS] = {NULL};\n\tint i;\n\n\tmutex_lock(&debugfs_mutex);\n\tif (!is_client_alive(client)) {\n\t\tseq_printf(s, \"ion_client 0x%p dead, can't dump its buffers\\n\",\n\t\t\t   client);\n\t\tmutex_unlock(&debugfs_mutex);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&client->lock);\n\tfor (n = rb_first(&client->handles); n; n = rb_next(n)) {\n\t\tstruct ion_handle *handle = rb_entry(n, struct ion_handle,\n\t\t\t\t\t\t     node);\n\t\tunsigned int id = handle->buffer->heap->id;\n\n\t\tif (!names[id])\n\t\t\tnames[id] = handle->buffer->heap->name;\n\t\tsizes[id] += handle->buffer->size;\n\t}\n\tmutex_unlock(&client->lock);\n\tmutex_unlock(&debugfs_mutex);\n\n\tseq_printf(s, \"%16.16s: %16.16s\\n\", \"heap_name\", \"size_in_bytes\");\n\tfor (i = 0; i < ION_NUM_HEAP_IDS; i++) {\n\t\tif (!names[i])\n\t\t\tcontinue;\n\t\tseq_printf(s, \"%16.16s: %16zu\\n\", names[i], sizes[i]);\n\t}\n\treturn 0;\n}\n\nstatic int ion_debug_client_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, ion_debug_client_show, inode->i_private);\n}\n\nstatic const struct file_operations debug_client_fops = {\n\t.open = ion_debug_client_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int ion_get_client_serial(const struct rb_root *root,\n\t\t\t\t\tconst unsigned char *name)\n{\n\tint serial = -1;\n\tstruct rb_node *node;\n\n\tfor (node = rb_first(root); node; node = rb_next(node)) {\n\t\tstruct ion_client *client = rb_entry(node, struct ion_client,\n\t\t\t\t\t\tnode);\n\n\t\tif (strcmp(client->name, name))\n\t\t\tcontinue;\n\t\tserial = max(serial, client->display_serial);\n\t}\n\treturn serial + 1;\n}\n\nstruct ion_client *ion_client_create(struct ion_device *dev,\n\t\t\t\t     const char *name)\n{\n\tstruct ion_client *client;\n\tstruct task_struct *task;\n\tstruct rb_node **p;\n\tstruct rb_node *parent = NULL;\n\tstruct ion_client *entry;\n\tpid_t pid;\n\n\tif (!name) {\n\t\tpr_err(\"%s: Name cannot be null\\n\", __func__);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tget_task_struct(current->group_leader);\n\ttask_lock(current->group_leader);\n\tpid = task_pid_nr(current->group_leader);\n\t/*\n\t * don't bother to store task struct for kernel threads,\n\t * they can't be killed anyway\n\t */\n\tif (current->group_leader->flags & PF_KTHREAD) {\n\t\tput_task_struct(current->group_leader);\n\t\ttask = NULL;\n\t} else {\n\t\ttask = current->group_leader;\n\t}\n\ttask_unlock(current->group_leader);\n\n\tclient = kzalloc(sizeof(struct ion_client), GFP_KERNEL);\n\tif (!client)\n\t\tgoto err_put_task_struct;\n\n\tclient->dev = dev;\n\tclient->handles = RB_ROOT;\n\tidr_init(&client->idr);\n\tmutex_init(&client->lock);\n\tclient->task = task;\n\tclient->pid = pid;\n\tclient->name = kstrdup(name, GFP_KERNEL);\n\tif (!client->name)\n\t\tgoto err_free_client;\n\n\tdown_write(&dev->lock);\n\tclient->display_serial = ion_get_client_serial(&dev->clients, name);\n\tclient->display_name = kasprintf(\n\t\tGFP_KERNEL, \"%s-%d\", name, client->display_serial);\n\tif (!client->display_name) {\n\t\tup_write(&dev->lock);\n\t\tgoto err_free_client_name;\n\t}\n\tp = &dev->clients.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct ion_client, node);\n\n\t\tif (client < entry)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (client > entry)\n\t\t\tp = &(*p)->rb_right;\n\t}\n\trb_link_node(&client->node, parent, p);\n\trb_insert_color(&client->node, &dev->clients);\n\n\tclient->debug_root = debugfs_create_file(client->display_name, 0664,\n\t\t\t\t\t\tdev->clients_debug_root,\n\t\t\t\t\t\tclient, &debug_client_fops);\n\tif (!client->debug_root) {\n\t\tchar buf[256], *path;\n\n\t\tpath = dentry_path(dev->clients_debug_root, buf, 256);\n\t\tpr_err(\"Failed to create client debugfs at %s/%s\\n\",\n\t\t\tpath, client->display_name);\n\t}\n\n\tup_write(&dev->lock);\n\n\treturn client;\n\nerr_free_client_name:\n\tkfree(client->name);\nerr_free_client:\n\tkfree(client);\nerr_put_task_struct:\n\tif (task)\n\t\tput_task_struct(current->group_leader);\n\treturn ERR_PTR(-ENOMEM);\n}\nEXPORT_SYMBOL(ion_client_create);\n\nvoid ion_client_destroy(struct ion_client *client)\n{\n\tstruct ion_device *dev = client->dev;\n\tstruct rb_node *n;\n\n\tpr_debug(\"%s: %d\\n\", __func__, __LINE__);\n\tmutex_lock(&debugfs_mutex);\n\twhile ((n = rb_first(&client->handles))) {\n\t\tstruct ion_handle *handle = rb_entry(n, struct ion_handle,\n\t\t\t\t\t\t     node);\n\t\tion_handle_destroy(&handle->ref);\n\t}\n\n\tidr_destroy(&client->idr);\n\n\tdown_write(&dev->lock);\n\tif (client->task)\n\t\tput_task_struct(client->task);\n\trb_erase(&client->node, &dev->clients);\n\tdebugfs_remove_recursive(client->debug_root);\n\tup_write(&dev->lock);\n\n\tkfree(client->display_name);\n\tkfree(client->name);\n\tkfree(client);\n\tmutex_unlock(&debugfs_mutex);\n}\nEXPORT_SYMBOL(ion_client_destroy);\n\nstruct sg_table *ion_sg_table(struct ion_client *client,\n\t\t\t      struct ion_handle *handle)\n{\n\tstruct ion_buffer *buffer;\n\tstruct sg_table *table;\n\n\tmutex_lock(&client->lock);\n\tif (!ion_handle_validate(client, handle)) {\n\t\tpr_err(\"%s: invalid handle passed to map_dma.\\n\",\n\t\t       __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tbuffer = handle->buffer;\n\ttable = buffer->sg_table;\n\tmutex_unlock(&client->lock);\n\treturn table;\n}\nEXPORT_SYMBOL(ion_sg_table);\n\nstatic void ion_buffer_sync_for_device(struct ion_buffer *buffer,\n\t\t\t\t       struct device *dev,\n\t\t\t\t       enum dma_data_direction direction);\n\nstatic struct sg_table *ion_map_dma_buf(struct dma_buf_attachment *attachment,\n\t\t\t\t\tenum dma_data_direction direction)\n{\n\tstruct dma_buf *dmabuf = attachment->dmabuf;\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\n\tion_buffer_sync_for_device(buffer, attachment->dev, direction);\n\treturn buffer->sg_table;\n}\n\nstatic void ion_unmap_dma_buf(struct dma_buf_attachment *attachment,\n\t\t\t      struct sg_table *table,\n\t\t\t      enum dma_data_direction direction)\n{\n}\n\nvoid ion_pages_sync_for_device(struct device *dev, struct page *page,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_table(&sg, 1);\n\tsg_set_page(&sg, page, size, 0);\n\t/*\n\t * This is not correct - sg_dma_address needs a dma_addr_t that is valid\n\t * for the targeted device, but this works on the currently targeted\n\t * hardware.\n\t */\n\tsg_dma_address(&sg) = page_to_phys(page);\n\tdma_sync_sg_for_device(dev, &sg, 1, dir);\n}\n\nstruct ion_vma_list {\n\tstruct list_head list;\n\tstruct vm_area_struct *vma;\n};\n\nstatic void ion_buffer_sync_for_device(struct ion_buffer *buffer,\n\t\t\t\t       struct device *dev,\n\t\t\t\t       enum dma_data_direction dir)\n{\n\tstruct ion_vma_list *vma_list;\n\tint pages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;\n\tint i;\n\n\tpr_debug(\"%s: syncing for device %s\\n\", __func__,\n\t\t dev ? dev_name(dev) : \"null\");\n\n\tif (!ion_buffer_fault_user_mappings(buffer))\n\t\treturn;\n\n\tmutex_lock(&buffer->lock);\n\tfor (i = 0; i < pages; i++) {\n\t\tstruct page *page = buffer->pages[i];\n\n\t\tif (ion_buffer_page_is_dirty(page))\n\t\t\tion_pages_sync_for_device(dev, ion_buffer_page(page),\n\t\t\t\t\t\t\tPAGE_SIZE, dir);\n\n\t\tion_buffer_page_clean(buffer->pages + i);\n\t}\n\tlist_for_each_entry(vma_list, &buffer->vmas, list) {\n\t\tstruct vm_area_struct *vma = vma_list->vma;\n\n\t\tzap_page_range(vma, vma->vm_start, vma->vm_end - vma->vm_start,\n\t\t\t       NULL);\n\t}\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic int ion_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct ion_buffer *buffer = vma->vm_private_data;\n\tunsigned long pfn;\n\tint ret;\n\n\tmutex_lock(&buffer->lock);\n\tion_buffer_page_dirty(buffer->pages + vmf->pgoff);\n\tBUG_ON(!buffer->pages || !buffer->pages[vmf->pgoff]);\n\n\tpfn = page_to_pfn(ion_buffer_page(buffer->pages[vmf->pgoff]));\n\tret = vm_insert_pfn(vma, (unsigned long)vmf->virtual_address, pfn);\n\tmutex_unlock(&buffer->lock);\n\tif (ret)\n\t\treturn VM_FAULT_ERROR;\n\n\treturn VM_FAULT_NOPAGE;\n}\n\nstatic void ion_vm_open(struct vm_area_struct *vma)\n{\n\tstruct ion_buffer *buffer = vma->vm_private_data;\n\tstruct ion_vma_list *vma_list;\n\n\tvma_list = kmalloc(sizeof(struct ion_vma_list), GFP_KERNEL);\n\tif (!vma_list)\n\t\treturn;\n\tvma_list->vma = vma;\n\tmutex_lock(&buffer->lock);\n\tlist_add(&vma_list->list, &buffer->vmas);\n\tmutex_unlock(&buffer->lock);\n\tpr_debug(\"%s: adding %p\\n\", __func__, vma);\n}\n\nstatic void ion_vm_close(struct vm_area_struct *vma)\n{\n\tstruct ion_buffer *buffer = vma->vm_private_data;\n\tstruct ion_vma_list *vma_list, *tmp;\n\n\tpr_debug(\"%s\\n\", __func__);\n\tmutex_lock(&buffer->lock);\n\tlist_for_each_entry_safe(vma_list, tmp, &buffer->vmas, list) {\n\t\tif (vma_list->vma != vma)\n\t\t\tcontinue;\n\t\tlist_del(&vma_list->list);\n\t\tkfree(vma_list);\n\t\tpr_debug(\"%s: deleting %p\\n\", __func__, vma);\n\t\tbreak;\n\t}\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic const struct vm_operations_struct ion_vma_ops = {\n\t.open = ion_vm_open,\n\t.close = ion_vm_close,\n\t.fault = ion_vm_fault,\n};\n\nstatic int ion_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\tint ret = 0;\n\n\tif (!buffer->heap->ops->map_user) {\n\t\tpr_err(\"%s: this heap does not define a method for mapping to userspace\\n\",\n\t\t\t__func__);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ion_buffer_fault_user_mappings(buffer)) {\n\t\tvma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND |\n\t\t\t\t\t\t\tVM_DONTDUMP;\n\t\tvma->vm_private_data = buffer;\n\t\tvma->vm_ops = &ion_vma_ops;\n\t\tion_vm_open(vma);\n\t\treturn 0;\n\t}\n\n\tif (!(buffer->flags & ION_FLAG_CACHED))\n\t\tvma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);\n\n\tmutex_lock(&buffer->lock);\n\t/* now map it to userspace */\n\tret = buffer->heap->ops->map_user(buffer->heap, buffer, vma);\n\tmutex_unlock(&buffer->lock);\n\n\tif (ret)\n\t\tpr_err(\"%s: failure mapping buffer to userspace\\n\",\n\t\t       __func__);\n\n\treturn ret;\n}\n\nstatic void ion_dma_buf_release(struct dma_buf *dmabuf)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\n\tion_buffer_put(buffer);\n}\n\nstatic void *ion_dma_buf_kmap(struct dma_buf *dmabuf, unsigned long offset)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\n\treturn buffer->vaddr + offset * PAGE_SIZE;\n}\n\nstatic void ion_dma_buf_kunmap(struct dma_buf *dmabuf, unsigned long offset,\n\t\t\t       void *ptr)\n{\n}\n\nstatic int ion_dma_buf_begin_cpu_access(struct dma_buf *dmabuf, size_t start,\n\t\t\t\t\tsize_t len,\n\t\t\t\t\tenum dma_data_direction direction)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\tvoid *vaddr;\n\n\tif (!buffer->heap->ops->map_kernel) {\n\t\tpr_err(\"%s: map kernel is not implemented by this heap.\\n\",\n\t\t       __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&buffer->lock);\n\tvaddr = ion_buffer_kmap_get(buffer);\n\tmutex_unlock(&buffer->lock);\n\treturn PTR_ERR_OR_ZERO(vaddr);\n}\n\nstatic void ion_dma_buf_end_cpu_access(struct dma_buf *dmabuf, size_t start,\n\t\t\t\t       size_t len,\n\t\t\t\t       enum dma_data_direction direction)\n{\n\tstruct ion_buffer *buffer = dmabuf->priv;\n\n\tmutex_lock(&buffer->lock);\n\tion_buffer_kmap_put(buffer);\n\tmutex_unlock(&buffer->lock);\n}\n\nstatic struct dma_buf_ops dma_buf_ops = {\n\t.map_dma_buf = ion_map_dma_buf,\n\t.unmap_dma_buf = ion_unmap_dma_buf,\n\t.mmap = ion_mmap,\n\t.release = ion_dma_buf_release,\n\t.begin_cpu_access = ion_dma_buf_begin_cpu_access,\n\t.end_cpu_access = ion_dma_buf_end_cpu_access,\n\t.kmap_atomic = ion_dma_buf_kmap,\n\t.kunmap_atomic = ion_dma_buf_kunmap,\n\t.kmap = ion_dma_buf_kmap,\n\t.kunmap = ion_dma_buf_kunmap,\n};\n\nstruct dma_buf *ion_share_dma_buf(struct ion_client *client,\n\t\t\t\t\t\tstruct ion_handle *handle)\n{\n\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\tstruct ion_buffer *buffer;\n\tstruct dma_buf *dmabuf;\n\tbool valid_handle;\n\n\tmutex_lock(&client->lock);\n\tvalid_handle = ion_handle_validate(client, handle);\n\tif (!valid_handle) {\n\t\tWARN(1, \"%s: invalid handle passed to share.\\n\", __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tbuffer = handle->buffer;\n\tion_buffer_get(buffer);\n\tmutex_unlock(&client->lock);\n\n\texp_info.ops = &dma_buf_ops;\n\texp_info.size = buffer->size;\n\texp_info.flags = O_RDWR;\n\texp_info.priv = buffer;\n\n\tdmabuf = dma_buf_export(&exp_info);\n\tif (IS_ERR(dmabuf)) {\n\t\tion_buffer_put(buffer);\n\t\treturn dmabuf;\n\t}\n\n\treturn dmabuf;\n}\nEXPORT_SYMBOL(ion_share_dma_buf);\n\nint ion_share_dma_buf_fd(struct ion_client *client, struct ion_handle *handle)\n{\n\tstruct dma_buf *dmabuf;\n\tint fd;\n\n\tdmabuf = ion_share_dma_buf(client, handle);\n\tif (IS_ERR(dmabuf))\n\t\treturn PTR_ERR(dmabuf);\n\n\tfd = dma_buf_fd(dmabuf, O_CLOEXEC);\n\tif (fd < 0)\n\t\tdma_buf_put(dmabuf);\n\n\treturn fd;\n}\nEXPORT_SYMBOL(ion_share_dma_buf_fd);\n\nstruct ion_handle *ion_import_dma_buf(struct ion_client *client,\n\t\t\t\t      struct dma_buf *dmabuf)\n{\n\tstruct ion_buffer *buffer;\n\tstruct ion_handle *handle;\n\tint ret;\n\n\t/* if this memory came from ion */\n\n\tif (dmabuf->ops != &dma_buf_ops) {\n\t\tpr_err(\"%s: can not import dmabuf from another exporter\\n\",\n\t\t       __func__);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tbuffer = dmabuf->priv;\n\n\tmutex_lock(&client->lock);\n\t/* if a handle exists for this buffer just take a reference to it */\n\thandle = ion_handle_lookup(client, buffer);\n\tif (!IS_ERR(handle)) {\n\t\tion_handle_get(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tgoto end;\n\t}\n\n\thandle = ion_handle_create(client, buffer);\n\tif (IS_ERR(handle)) {\n\t\tmutex_unlock(&client->lock);\n\t\tgoto end;\n\t}\n\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\nend:\n\treturn handle;\n}\nEXPORT_SYMBOL(ion_import_dma_buf);\n\nstruct ion_handle *ion_import_dma_buf_fd(struct ion_client *client, int fd)\n{\n\tstruct dma_buf *dmabuf;\n\tstruct ion_handle *handle;\n\n\tdmabuf = dma_buf_get(fd);\n\tif (IS_ERR(dmabuf))\n\t\treturn ERR_CAST(dmabuf);\n\n\thandle = ion_import_dma_buf(client, dmabuf);\n\tdma_buf_put(dmabuf);\n\treturn handle;\n}\nEXPORT_SYMBOL(ion_import_dma_buf_fd);\n\nstatic int ion_sync_for_device(struct ion_client *client, int fd)\n{\n\tstruct dma_buf *dmabuf;\n\tstruct ion_buffer *buffer;\n\n\tdmabuf = dma_buf_get(fd);\n\tif (IS_ERR(dmabuf))\n\t\treturn PTR_ERR(dmabuf);\n\n\t/* if this memory came from ion */\n\tif (dmabuf->ops != &dma_buf_ops) {\n\t\tpr_err(\"%s: can not sync dmabuf from another exporter\\n\",\n\t\t       __func__);\n\t\tdma_buf_put(dmabuf);\n\t\treturn -EINVAL;\n\t}\n\tbuffer = dmabuf->priv;\n\n\tdma_sync_sg_for_device(NULL, buffer->sg_table->sgl,\n\t\t\t       buffer->sg_table->nents, DMA_BIDIRECTIONAL);\n\tdma_buf_put(dmabuf);\n\treturn 0;\n}\n\n/* fix up the cases where the ioctl direction bits are incorrect */\nstatic unsigned int ion_ioctl_dir(unsigned int cmd)\n{\n\tswitch (cmd) {\n\tcase ION_IOC_SYNC:\n\tcase ION_IOC_FREE:\n\tcase ION_IOC_CUSTOM:\n\t\treturn _IOC_WRITE;\n\tdefault:\n\t\treturn _IOC_DIR(cmd);\n\t}\n}\n\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int ion_release(struct inode *inode, struct file *file)\n{\n\tstruct ion_client *client = file->private_data;\n\n\tpr_debug(\"%s: %d\\n\", __func__, __LINE__);\n\tion_client_destroy(client);\n\treturn 0;\n}\n\nstatic int ion_open(struct inode *inode, struct file *file)\n{\n\tstruct miscdevice *miscdev = file->private_data;\n\tstruct ion_device *dev = container_of(miscdev, struct ion_device, dev);\n\tstruct ion_client *client;\n\tchar debug_name[64];\n\n\tpr_debug(\"%s: %d\\n\", __func__, __LINE__);\n\tsnprintf(debug_name, 64, \"%u\", task_pid_nr(current->group_leader));\n\tclient = ion_client_create(dev, debug_name);\n\tif (IS_ERR(client))\n\t\treturn PTR_ERR(client);\n\tfile->private_data = client;\n\n\treturn 0;\n}\n\nstatic const struct file_operations ion_fops = {\n\t.owner          = THIS_MODULE,\n\t.open           = ion_open,\n\t.release        = ion_release,\n\t.unlocked_ioctl = ion_ioctl,\n\t.compat_ioctl   = compat_ion_ioctl,\n};\n\nstatic size_t ion_debug_heap_total(struct ion_client *client,\n\t\t\t\t   unsigned int id)\n{\n\tsize_t size = 0;\n\tstruct rb_node *n;\n\n\tmutex_lock(&client->lock);\n\tfor (n = rb_first(&client->handles); n; n = rb_next(n)) {\n\t\tstruct ion_handle *handle = rb_entry(n,\n\t\t\t\t\t\t     struct ion_handle,\n\t\t\t\t\t\t     node);\n\t\tif (handle->buffer->heap->id == id)\n\t\t\tsize += handle->buffer->size;\n\t}\n\tmutex_unlock(&client->lock);\n\treturn size;\n}\n\nstatic int ion_debug_heap_show(struct seq_file *s, void *unused)\n{\n\tstruct ion_heap *heap = s->private;\n\tstruct ion_device *dev = heap->dev;\n\tstruct rb_node *n;\n\tsize_t total_size = 0;\n\tsize_t total_orphaned_size = 0;\n\n\tseq_printf(s, \"%16s %16s %16s\\n\", \"client\", \"pid\", \"size\");\n\tseq_puts(s, \"----------------------------------------------------\\n\");\n\n\tmutex_lock(&debugfs_mutex);\n\tfor (n = rb_first(&dev->clients); n; n = rb_next(n)) {\n\t\tstruct ion_client *client = rb_entry(n, struct ion_client,\n\t\t\t\t\t\t     node);\n\t\tsize_t size = ion_debug_heap_total(client, heap->id);\n\n\t\tif (!size)\n\t\t\tcontinue;\n\t\tif (client->task) {\n\t\t\tchar task_comm[TASK_COMM_LEN];\n\n\t\t\tget_task_comm(task_comm, client->task);\n\t\t\tseq_printf(s, \"%16s %16u %16zu\\n\", task_comm,\n\t\t\t\t   client->pid, size);\n\t\t} else {\n\t\t\tseq_printf(s, \"%16s %16u %16zu\\n\", client->name,\n\t\t\t\t   client->pid, size);\n\t\t}\n\t}\n\tmutex_unlock(&debugfs_mutex);\n\n\tseq_puts(s, \"----------------------------------------------------\\n\");\n\tseq_puts(s, \"orphaned allocations (info is from last known client):\\n\");\n\tmutex_lock(&dev->buffer_lock);\n\tfor (n = rb_first(&dev->buffers); n; n = rb_next(n)) {\n\t\tstruct ion_buffer *buffer = rb_entry(n, struct ion_buffer,\n\t\t\t\t\t\t     node);\n\t\tif (buffer->heap->id != heap->id)\n\t\t\tcontinue;\n\t\ttotal_size += buffer->size;\n\t\tif (!buffer->handle_count) {\n\t\t\tseq_printf(s, \"%16s %16u %16zu %d %d\\n\",\n\t\t\t\t   buffer->task_comm, buffer->pid,\n\t\t\t\t   buffer->size, buffer->kmap_cnt,\n\t\t\t\t   atomic_read(&buffer->ref.refcount));\n\t\t\ttotal_orphaned_size += buffer->size;\n\t\t}\n\t}\n\tmutex_unlock(&dev->buffer_lock);\n\tseq_puts(s, \"----------------------------------------------------\\n\");\n\tseq_printf(s, \"%16s %16zu\\n\", \"total orphaned\",\n\t\t   total_orphaned_size);\n\tseq_printf(s, \"%16s %16zu\\n\", \"total \", total_size);\n\tif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\n\t\tseq_printf(s, \"%16s %16zu\\n\", \"deferred free\",\n\t\t\t\theap->free_list_size);\n\tseq_puts(s, \"----------------------------------------------------\\n\");\n\n\tif (heap->debug_show)\n\t\theap->debug_show(heap, s, unused);\n\n\treturn 0;\n}\n\nstatic int ion_debug_heap_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, ion_debug_heap_show, inode->i_private);\n}\n\nstatic const struct file_operations debug_heap_fops = {\n\t.open = ion_debug_heap_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int debug_shrink_set(void *data, u64 val)\n{\n\tstruct ion_heap *heap = data;\n\tstruct shrink_control sc;\n\tint objs;\n\n\tsc.gfp_mask = -1;\n\tsc.nr_to_scan = val;\n\n\tif (!val) {\n\t\tobjs = heap->shrinker.count_objects(&heap->shrinker, &sc);\n\t\tsc.nr_to_scan = objs;\n\t}\n\n\theap->shrinker.scan_objects(&heap->shrinker, &sc);\n\treturn 0;\n}\n\nstatic int debug_shrink_get(void *data, u64 *val)\n{\n\tstruct ion_heap *heap = data;\n\tstruct shrink_control sc;\n\tint objs;\n\n\tsc.gfp_mask = -1;\n\tsc.nr_to_scan = 0;\n\n\tobjs = heap->shrinker.count_objects(&heap->shrinker, &sc);\n\t*val = objs;\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(debug_shrink_fops, debug_shrink_get,\n\t\t\tdebug_shrink_set, \"%llu\\n\");\n\nvoid ion_device_add_heap(struct ion_device *dev, struct ion_heap *heap)\n{\n\tstruct dentry *debug_file;\n\n\tif (!heap->ops->allocate || !heap->ops->free || !heap->ops->map_dma ||\n\t    !heap->ops->unmap_dma)\n\t\tpr_err(\"%s: can not add heap with invalid ops struct.\\n\",\n\t\t       __func__);\n\n\tspin_lock_init(&heap->free_lock);\n\theap->free_list_size = 0;\n\n\tif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\n\t\tion_heap_init_deferred_free(heap);\n\n\tif ((heap->flags & ION_HEAP_FLAG_DEFER_FREE) || heap->ops->shrink)\n\t\tion_heap_init_shrinker(heap);\n\n\theap->dev = dev;\n\tdown_write(&dev->lock);\n\t/*\n\t * use negative heap->id to reverse the priority -- when traversing\n\t * the list later attempt higher id numbers first\n\t */\n\tplist_node_init(&heap->node, -heap->id);\n\tplist_add(&heap->node, &dev->heaps);\n\tdebug_file = debugfs_create_file(heap->name, 0664,\n\t\t\t\t\tdev->heaps_debug_root, heap,\n\t\t\t\t\t&debug_heap_fops);\n\n\tif (!debug_file) {\n\t\tchar buf[256], *path;\n\n\t\tpath = dentry_path(dev->heaps_debug_root, buf, 256);\n\t\tpr_err(\"Failed to create heap debugfs at %s/%s\\n\",\n\t\t\tpath, heap->name);\n\t}\n\n\tif (heap->shrinker.count_objects && heap->shrinker.scan_objects) {\n\t\tchar debug_name[64];\n\n\t\tsnprintf(debug_name, 64, \"%s_shrink\", heap->name);\n\t\tdebug_file = debugfs_create_file(\n\t\t\tdebug_name, 0644, dev->heaps_debug_root, heap,\n\t\t\t&debug_shrink_fops);\n\t\tif (!debug_file) {\n\t\t\tchar buf[256], *path;\n\n\t\t\tpath = dentry_path(dev->heaps_debug_root, buf, 256);\n\t\t\tpr_err(\"Failed to create heap shrinker debugfs at %s/%s\\n\",\n\t\t\t\tpath, debug_name);\n\t\t}\n\t}\n\n\tup_write(&dev->lock);\n}\nEXPORT_SYMBOL(ion_device_add_heap);\n\nstruct ion_device *ion_device_create(long (*custom_ioctl)\n\t\t\t\t     (struct ion_client *client,\n\t\t\t\t      unsigned int cmd,\n\t\t\t\t      unsigned long arg))\n{\n\tstruct ion_device *idev;\n\tint ret;\n\n\tidev = kzalloc(sizeof(struct ion_device), GFP_KERNEL);\n\tif (!idev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tidev->dev.minor = MISC_DYNAMIC_MINOR;\n\tidev->dev.name = \"ion\";\n\tidev->dev.fops = &ion_fops;\n\tidev->dev.parent = NULL;\n\tret = misc_register(&idev->dev);\n\tif (ret) {\n\t\tpr_err(\"ion: failed to register misc device.\\n\");\n\t\tkfree(idev);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tidev->debug_root = debugfs_create_dir(\"ion\", NULL);\n\tif (!idev->debug_root) {\n\t\tpr_err(\"ion: failed to create debugfs root directory.\\n\");\n\t\tgoto debugfs_done;\n\t}\n\tidev->heaps_debug_root = debugfs_create_dir(\"heaps\", idev->debug_root);\n\tif (!idev->heaps_debug_root) {\n\t\tpr_err(\"ion: failed to create debugfs heaps directory.\\n\");\n\t\tgoto debugfs_done;\n\t}\n\tidev->clients_debug_root = debugfs_create_dir(\"clients\",\n\t\t\t\t\t\tidev->debug_root);\n\tif (!idev->clients_debug_root)\n\t\tpr_err(\"ion: failed to create debugfs clients directory.\\n\");\n\ndebugfs_done:\n\n\tidev->custom_ioctl = custom_ioctl;\n\tidev->buffers = RB_ROOT;\n\tmutex_init(&idev->buffer_lock);\n\tinit_rwsem(&idev->lock);\n\tplist_head_init(&idev->heaps);\n\tidev->clients = RB_ROOT;\n\tion_root_client = &idev->clients;\n\tmutex_init(&debugfs_mutex);\n\treturn idev;\n}\nEXPORT_SYMBOL(ion_device_create);\n\nvoid ion_device_destroy(struct ion_device *dev)\n{\n\tmisc_deregister(&dev->dev);\n\tdebugfs_remove_recursive(dev->debug_root);\n\t/* XXX need to free the heaps and clients ? */\n\tkfree(dev);\n}\nEXPORT_SYMBOL(ion_device_destroy);\n\nvoid __init ion_reserve(struct ion_platform_data *data)\n{\n\tint i;\n\n\tfor (i = 0; i < data->nr; i++) {\n\t\tif (data->heaps[i].size == 0)\n\t\t\tcontinue;\n\n\t\tif (data->heaps[i].base == 0) {\n\t\t\tphys_addr_t paddr;\n\n\t\t\tpaddr = memblock_alloc_base(data->heaps[i].size,\n\t\t\t\t\t\t    data->heaps[i].align,\n\t\t\t\t\t\t    MEMBLOCK_ALLOC_ANYWHERE);\n\t\t\tif (!paddr) {\n\t\t\t\tpr_err(\"%s: error allocating memblock for heap %d\\n\",\n\t\t\t\t\t__func__, i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tdata->heaps[i].base = paddr;\n\t\t} else {\n\t\t\tint ret = memblock_reserve(data->heaps[i].base,\n\t\t\t\t\t       data->heaps[i].size);\n\t\t\tif (ret)\n\t\t\t\tpr_err(\"memblock reserve of %zx@%lx failed\\n\",\n\t\t\t\t       data->heaps[i].size,\n\t\t\t\t       data->heaps[i].base);\n\t\t}\n\t\tpr_info(\"%s: %s reserved base %lx size %zu\\n\", __func__,\n\t\t\tdata->heaps[i].name,\n\t\t\tdata->heaps[i].base,\n\t\t\tdata->heaps[i].size);\n\t}\n}\n"], "filenames": ["drivers/staging/android/ion/ion.c"], "buggy_code_start_loc": [390], "buggy_code_end_loc": [1340], "fixing_code_start_loc": [390], "fixing_code_end_loc": [1369], "type": "CWE-264", "message": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.", "other": {"cve": {"id": "CVE-2016-9120", "sourceIdentifier": "security@android.com", "published": "2016-12-08T21:59:02.443", "lastModified": "2023-01-19T16:04:46.787", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time."}, {"lang": "es", "value": "Condici\u00f3n de carrera en la funci\u00f3n ion_ioctl en drivers/staging/android/ion/ion.c en el kernel de Linux en versiones anteriores a 4.6 permite a usuarios locales obtener privilegios o provocar una denegaci\u00f3n de servicio (uso despu\u00e9s de liberaci\u00f3n de memoria) llamando a ION_IOC_FREE en dos CPUs al mismo tiempo."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 9.3}, "baseSeverity": "HIGH", "exploitabilityScore": 8.6, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}, {"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.14", "versionEndExcluding": "3.16.40", "matchCriteriaId": "60A5B395-4A36-45E0-8830-765BF463B6D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.51", "matchCriteriaId": "BD39D0CC-7B47-4272-BA45-3E0A840F668D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.41", "matchCriteriaId": "9019BEC9-FE77-4506-A019-B8B4D8BCEBAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.65", "matchCriteriaId": "68BF7913-3500-47F8-9563-A09C85A2BC6F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.6", "matchCriteriaId": "628AFDA5-6C82-4DB8-8280-D1D7C58BBFE7"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=9590232bb4f4cc824f3425a6e1349afbe6d6d2b7", "source": "security@android.com", "tags": ["Issue Tracking", "Patch", "Vendor Advisory"]}, {"url": "http://source.android.com/security/bulletin/2016-12-01.html", "source": "security@android.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/94669", "source": "security@android.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/9590232bb4f4cc824f3425a6e1349afbe6d6d2b7", "source": "security@android.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/9590232bb4f4cc824f3425a6e1349afbe6d6d2b7"}}