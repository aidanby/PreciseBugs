{"buggy_code": ["{% extends \"base.html\" %}\n\n{% block title %}API Explorer{% endblock %}\n\n{% block extra_head %}\n<script src=\"{{ base_url }}-/static/json-format-highlight-1.0.1.js\"></script>\n{% endblock %}\n\n{% block content %}\n\n<h1>API Explorer</h1>\n\n<p>Use this tool to try out the\n  {% if datasette_version %}\n    <a href=\"https://docs.datasette.io/en/{{ datasette_version }}/json_api.html\">Datasette API</a>.\n  {% else %}\n    Datasette API.\n  {% endif %}\n</p>\n<details open style=\"border: 2px solid #ccc; border-bottom: none; padding: 0.5em\">\n  <summary style=\"cursor: pointer;\">GET</summary>\n  <form method=\"get\" id=\"api-explorer-get\" style=\"margin-top: 0.7em\">\n    <div>\n      <label for=\"path\">API path:</label>\n      <input type=\"text\" id=\"path\" name=\"path\" style=\"width: 60%\">\n      <input type=\"submit\" value=\"GET\">\n    </div>\n  </form>\n</details>\n<details style=\"border: 2px solid #ccc; padding: 0.5em\">\n  <summary style=\"cursor: pointer\">POST</summary>\n  <form method=\"post\" id=\"api-explorer-post\" style=\"margin-top: 0.7em\">\n    <div>\n      <label for=\"path\">API path:</label>\n      <input type=\"text\" id=\"path\" name=\"path\" style=\"width: 60%\">\n    </div>\n    <div style=\"margin: 0.5em 0\">\n      <label for=\"apiJson\" style=\"vertical-align: top\">JSON:</label>\n      <textarea id=\"apiJson\" name=\"json\" style=\"width: 60%; height: 200px; font-family: monospace; font-size: 0.8em;\"></textarea>\n    </div>\n    <p><button id=\"json-format\" type=\"button\">Format JSON</button> <input type=\"submit\" value=\"POST\"></p>\n  </form>\n</details>\n\n<div id=\"output\" style=\"display: none\">\n  <h2>API response: HTTP <span id=\"response-status\"></span></h2>\n  </h2>\n  <ul class=\"errors message-error\"></ul>\n  <pre></pre>\n</div>\n\n<script>\ndocument.querySelector('#json-format').addEventListener('click', (ev) => {\n  ev.preventDefault();\n  let json = document.querySelector('textarea[name=\"json\"]').value.trim();\n  if (!json) {\n    return;\n  }\n  try {\n    const parsed = JSON.parse(json);\n    document.querySelector('textarea[name=\"json\"]').value = JSON.stringify(parsed, null, 2);\n  } catch (e) {\n    alert(\"Error parsing JSON: \" + e);\n  }\n});\nvar postForm = document.getElementById('api-explorer-post');\nvar getForm = document.getElementById('api-explorer-get');\nvar output = document.getElementById('output');\nvar errorList = output.querySelector('.errors');\n\n// On first load or fragment change populate forms from # in URL, if present\nif (window.location.hash) {\n  onFragmentChange();\n}\nfunction onFragmentChange() {\n  var hash = window.location.hash.slice(1);\n  // Treat hash as a foo=bar string and parse it:\n  var params = new URLSearchParams(hash);\n  var method = params.get('method');\n  if (method == 'GET') {\n    getForm.closest('details').open = true;\n    postForm.closest('details').open = false;\n    getForm.querySelector('input[name=\"path\"]').value = params.get('path');\n  } else if (method == 'POST') {\n    postForm.closest('details').open = true;\n    getForm.closest('details').open = false;\n    postForm.querySelector('input[name=\"path\"]').value = params.get('path');\n    postForm.querySelector('textarea[name=\"json\"]').value = params.get('json');\n  }\n}\nwindow.addEventListener('hashchange', () => {\n  onFragmentChange();\n  // Animate scroll to top of page\n  window.scrollTo({top: 0, behavior: 'smooth'});\n});\n\n// Cause GET and POST regions to toggle each other\nvar getDetails = getForm.closest('details');\nvar postDetails = postForm.closest('details');\ngetDetails.addEventListener('toggle', (ev) => {\n  if (getDetails.open) {\n    postDetails.open = false;\n  }\n});\npostDetails.addEventListener('toggle', (ev) => {\n  if (postDetails.open) {\n    getDetails.open = false;\n  }\n});\n\ngetForm.addEventListener(\"submit\", (ev) => {\n  ev.preventDefault();\n  var formData = new FormData(getForm);\n  // Update URL fragment hash\n  var serialized = new URLSearchParams(formData).toString() + '&method=GET';\n  window.history.pushState({}, \"\", location.pathname + '#' + serialized);\n  // Send the request\n  var path = formData.get('path');\n  fetch(path, {\n    method: 'GET',\n    headers: {\n      'Accept': 'application/json',\n    }\n  }).then((response) => {\n    output.style.display = 'block';\n    document.getElementById('response-status').textContent = response.status;\n    return response.json();\n  }).then((data) => {\n    output.querySelector('pre').innerHTML = jsonFormatHighlight(data);\n    errorList.style.display = 'none';\n  }).catch((error) => {\n    alert(error);\n  });\n});\n\npostForm.addEventListener(\"submit\", (ev) => {\n  ev.preventDefault();\n  var formData = new FormData(postForm);\n  // Update URL fragment hash\n  var serialized = new URLSearchParams(formData).toString() + '&method=POST';\n  window.history.pushState({}, \"\", location.pathname + '#' + serialized);\n  // Send the request\n  var json = formData.get('json');\n  var path = formData.get('path');\n  // Validate JSON\n  if (!json.length) {\n    json = '{}';\n  }\n  try {\n    var data = JSON.parse(json);\n  } catch (err) {\n    alert(\"Invalid JSON: \" + err);\n    return;\n  }\n  // POST JSON to path with content-type application/json\n  fetch(path, {\n    method: 'POST',\n    body: json,\n    headers: {\n      'Content-Type': 'application/json',\n    }\n  }).then(r => {\n    document.getElementById('response-status').textContent = r.status;\n    return r.json();\n  }).then(data => {\n    if (data.errors) {\n      errorList.style.display = 'block';\n      errorList.innerHTML = '';\n      data.errors.forEach(error => {\n        var li = document.createElement('li');\n        li.textContent = error;\n        errorList.appendChild(li);\n      });\n    } else {\n      errorList.style.display = 'none';\n    }\n    output.querySelector('pre').innerHTML = jsonFormatHighlight(data);\n    output.style.display = 'block';\n  }).catch(err => {\n    alert(\"Error: \" + err);\n  });\n});\n</script>\n\n{% if example_links %}\n<h2>API endpoints</h2>\n<ul class=\"bullets\">\n  {% for database in example_links %}\n    <li>Database: <strong>{{ database.name }}</strong></li>\n    <ul class=\"bullets\">\n      {% for link in database.links %}\n        <li><a href=\"{{ api_path(link) }}\">{{ link.path }}</a> - {{ link.label }} </li>\n      {% endfor %}\n      {% for table in database.tables %}\n        <li><strong>{{ table.name }}</strong>\n          <ul class=\"bullets\">\n            {% for link in table.links %}\n              <li><a href=\"{{ api_path(link) }}\">{{ link.path }}</a> - {{ link.label }} </li>\n            {% endfor %}\n          </ul>\n        </li>\n      {% endfor %}\n    </ul>\n  {% endfor %}\n</ul>\n{% endif %}\n\n{% endblock %}\n", "__version__ = \"1.0a3\"\n__version_info__ = tuple(__version__.split(\".\"))\n", "import json\nfrom datasette.utils.asgi import Response, Forbidden\nfrom datasette.utils import (\n    actor_matches_allow,\n    add_cors_headers,\n    tilde_encode,\n    tilde_decode,\n)\nfrom .base import BaseView, View\nimport secrets\nimport urllib\n\n\nclass JsonDataView(BaseView):\n    name = \"json_data\"\n\n    def __init__(\n        self,\n        datasette,\n        filename,\n        data_callback,\n        needs_request=False,\n        permission=\"view-instance\",\n    ):\n        self.ds = datasette\n        self.filename = filename\n        self.data_callback = data_callback\n        self.needs_request = needs_request\n        self.permission = permission\n\n    async def get(self, request):\n        as_format = request.url_vars[\"format\"]\n        if self.permission:\n            await self.ds.ensure_permissions(request.actor, [self.permission])\n        if self.needs_request:\n            data = self.data_callback(request)\n        else:\n            data = self.data_callback()\n        if as_format:\n            headers = {}\n            if self.ds.cors:\n                add_cors_headers(headers)\n            return Response(\n                json.dumps(data),\n                content_type=\"application/json; charset=utf-8\",\n                headers=headers,\n            )\n\n        else:\n            return await self.render(\n                [\"show_json.html\"],\n                request=request,\n                context={\n                    \"filename\": self.filename,\n                    \"data_json\": json.dumps(data, indent=4),\n                },\n            )\n\n\nclass PatternPortfolioView(View):\n    async def get(self, request, datasette):\n        await datasette.ensure_permissions(request.actor, [\"view-instance\"])\n        return Response.html(\n            await datasette.render_template(\n                \"patterns.html\",\n                request=request,\n                view_name=\"patterns\",\n            )\n        )\n\n\nclass AuthTokenView(BaseView):\n    name = \"auth_token\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        token = request.args.get(\"token\") or \"\"\n        if not self.ds._root_token:\n            raise Forbidden(\"Root token has already been used\")\n        if secrets.compare_digest(token, self.ds._root_token):\n            self.ds._root_token = None\n            response = Response.redirect(self.ds.urls.instance())\n            response.set_cookie(\n                \"ds_actor\", self.ds.sign({\"a\": {\"id\": \"root\"}}, \"actor\")\n            )\n            return response\n        else:\n            raise Forbidden(\"Invalid token\")\n\n\nclass LogoutView(BaseView):\n    name = \"logout\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        if not request.actor:\n            return Response.redirect(self.ds.urls.instance())\n        return await self.render(\n            [\"logout.html\"],\n            request,\n            {\"actor\": request.actor},\n        )\n\n    async def post(self, request):\n        response = Response.redirect(self.ds.urls.instance())\n        response.set_cookie(\"ds_actor\", \"\", expires=0, max_age=0)\n        self.ds.add_message(request, \"You are now logged out\", self.ds.WARNING)\n        return response\n\n\nclass PermissionsDebugView(BaseView):\n    name = \"permissions_debug\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        await self.ds.ensure_permissions(request.actor, [\"view-instance\"])\n        if not await self.ds.permission_allowed(request.actor, \"permissions-debug\"):\n            raise Forbidden(\"Permission denied\")\n        return await self.render(\n            [\"permissions_debug.html\"],\n            request,\n            # list() avoids error if check is performed during template render:\n            {\n                \"permission_checks\": list(reversed(self.ds._permission_checks)),\n                \"permissions\": list(self.ds.permissions.values()),\n            },\n        )\n\n    async def post(self, request):\n        await self.ds.ensure_permissions(request.actor, [\"view-instance\"])\n        if not await self.ds.permission_allowed(request.actor, \"permissions-debug\"):\n            raise Forbidden(\"Permission denied\")\n        vars = await request.post_vars()\n        actor = json.loads(vars[\"actor\"])\n        permission = vars[\"permission\"]\n        resource_1 = vars[\"resource_1\"]\n        resource_2 = vars[\"resource_2\"]\n        resource = []\n        if resource_1:\n            resource.append(resource_1)\n        if resource_2:\n            resource.append(resource_2)\n        resource = tuple(resource)\n        if len(resource) == 1:\n            resource = resource[0]\n        result = await self.ds.permission_allowed(\n            actor, permission, resource, default=\"USE_DEFAULT\"\n        )\n        return Response.json(\n            {\n                \"actor\": actor,\n                \"permission\": permission,\n                \"resource\": resource,\n                \"result\": result,\n            }\n        )\n\n\nclass AllowDebugView(BaseView):\n    name = \"allow_debug\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        errors = []\n        actor_input = request.args.get(\"actor\") or '{\"id\": \"root\"}'\n        try:\n            actor = json.loads(actor_input)\n            actor_input = json.dumps(actor, indent=4)\n        except json.decoder.JSONDecodeError as ex:\n            errors.append(f\"Actor JSON error: {ex}\")\n        allow_input = request.args.get(\"allow\") or '{\"id\": \"*\"}'\n        try:\n            allow = json.loads(allow_input)\n            allow_input = json.dumps(allow, indent=4)\n        except json.decoder.JSONDecodeError as ex:\n            errors.append(f\"Allow JSON error: {ex}\")\n\n        result = None\n        if not errors:\n            result = str(actor_matches_allow(actor, allow))\n\n        return await self.render(\n            [\"allow_debug.html\"],\n            request,\n            {\n                \"result\": result,\n                \"error\": \"\\n\\n\".join(errors) if errors else \"\",\n                \"actor_input\": actor_input,\n                \"allow_input\": allow_input,\n            },\n        )\n\n\nclass MessagesDebugView(BaseView):\n    name = \"messages_debug\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        await self.ds.ensure_permissions(request.actor, [\"view-instance\"])\n        return await self.render([\"messages_debug.html\"], request)\n\n    async def post(self, request):\n        await self.ds.ensure_permissions(request.actor, [\"view-instance\"])\n        post = await request.post_vars()\n        message = post.get(\"message\", \"\")\n        message_type = post.get(\"message_type\") or \"INFO\"\n        assert message_type in (\"INFO\", \"WARNING\", \"ERROR\", \"all\")\n        datasette = self.ds\n        if message_type == \"all\":\n            datasette.add_message(request, message, datasette.INFO)\n            datasette.add_message(request, message, datasette.WARNING)\n            datasette.add_message(request, message, datasette.ERROR)\n        else:\n            datasette.add_message(request, message, getattr(datasette, message_type))\n        return Response.redirect(self.ds.urls.instance())\n\n\nclass CreateTokenView(BaseView):\n    name = \"create_token\"\n    has_json_alternate = False\n\n    def check_permission(self, request):\n        if not self.ds.setting(\"allow_signed_tokens\"):\n            raise Forbidden(\"Signed tokens are not enabled for this Datasette instance\")\n        if not request.actor:\n            raise Forbidden(\"You must be logged in to create a token\")\n        if not request.actor.get(\"id\"):\n            raise Forbidden(\n                \"You must be logged in as an actor with an ID to create a token\"\n            )\n        if request.actor.get(\"token\"):\n            raise Forbidden(\n                \"Token authentication cannot be used to create additional tokens\"\n            )\n\n    async def shared(self, request):\n        self.check_permission(request)\n        # Build list of databases and tables the user has permission to view\n        database_with_tables = []\n        for database in self.ds.databases.values():\n            if database.name in (\"_internal\", \"_memory\"):\n                continue\n            if not await self.ds.permission_allowed(\n                request.actor, \"view-database\", database.name\n            ):\n                continue\n            hidden_tables = await database.hidden_table_names()\n            tables = []\n            for table in await database.table_names():\n                if table in hidden_tables:\n                    continue\n                if not await self.ds.permission_allowed(\n                    request.actor,\n                    \"view-table\",\n                    resource=(database.name, table),\n                ):\n                    continue\n                tables.append({\"name\": table, \"encoded\": tilde_encode(table)})\n            database_with_tables.append(\n                {\n                    \"name\": database.name,\n                    \"encoded\": tilde_encode(database.name),\n                    \"tables\": tables,\n                }\n            )\n        return {\n            \"actor\": request.actor,\n            \"all_permissions\": self.ds.permissions.keys(),\n            \"database_permissions\": [\n                key\n                for key, value in self.ds.permissions.items()\n                if value.takes_database\n            ],\n            \"resource_permissions\": [\n                key\n                for key, value in self.ds.permissions.items()\n                if value.takes_resource\n            ],\n            \"database_with_tables\": database_with_tables,\n        }\n\n    async def get(self, request):\n        self.check_permission(request)\n        return await self.render(\n            [\"create_token.html\"], request, await self.shared(request)\n        )\n\n    async def post(self, request):\n        self.check_permission(request)\n        post = await request.post_vars()\n        errors = []\n        expires_after = None\n        if post.get(\"expire_type\"):\n            duration_string = post.get(\"expire_duration\")\n            if (\n                not duration_string\n                or not duration_string.isdigit()\n                or not int(duration_string) > 0\n            ):\n                errors.append(\"Invalid expire duration\")\n            else:\n                unit = post[\"expire_type\"]\n                if unit == \"minutes\":\n                    expires_after = int(duration_string) * 60\n                elif unit == \"hours\":\n                    expires_after = int(duration_string) * 60 * 60\n                elif unit == \"days\":\n                    expires_after = int(duration_string) * 60 * 60 * 24\n                else:\n                    errors.append(\"Invalid expire duration unit\")\n\n        # Are there any restrictions?\n        restrict_all = []\n        restrict_database = {}\n        restrict_resource = {}\n\n        for key in post:\n            if key.startswith(\"all:\") and key.count(\":\") == 1:\n                restrict_all.append(key.split(\":\")[1])\n            elif key.startswith(\"database:\") and key.count(\":\") == 2:\n                bits = key.split(\":\")\n                database = tilde_decode(bits[1])\n                action = bits[2]\n                restrict_database.setdefault(database, []).append(action)\n            elif key.startswith(\"resource:\") and key.count(\":\") == 3:\n                bits = key.split(\":\")\n                database = tilde_decode(bits[1])\n                resource = tilde_decode(bits[2])\n                action = bits[3]\n                restrict_resource.setdefault(database, {}).setdefault(\n                    resource, []\n                ).append(action)\n\n        token = self.ds.create_token(\n            request.actor[\"id\"],\n            expires_after=expires_after,\n            restrict_all=restrict_all,\n            restrict_database=restrict_database,\n            restrict_resource=restrict_resource,\n        )\n        token_bits = self.ds.unsign(token[len(\"dstok_\") :], namespace=\"token\")\n        context = await self.shared(request)\n        context.update({\"errors\": errors, \"token\": token, \"token_bits\": token_bits})\n        return await self.render([\"create_token.html\"], request, context)\n\n\nclass ApiExplorerView(BaseView):\n    name = \"api_explorer\"\n    has_json_alternate = False\n\n    async def example_links(self, request):\n        databases = []\n        for name, db in self.ds.databases.items():\n            if name == \"_internal\":\n                continue\n            database_visible, _ = await self.ds.check_visibility(\n                request.actor,\n                \"view-database\",\n                name,\n            )\n            if not database_visible:\n                continue\n            tables = []\n            table_names = await db.table_names()\n            for table in table_names:\n                visible, _ = await self.ds.check_visibility(\n                    request.actor,\n                    \"view-table\",\n                    (name, table),\n                )\n                if not visible:\n                    continue\n                table_links = []\n                tables.append({\"name\": table, \"links\": table_links})\n                table_links.append(\n                    {\n                        \"label\": \"Get rows for {}\".format(table),\n                        \"method\": \"GET\",\n                        \"path\": self.ds.urls.table(name, table, format=\"json\"),\n                    }\n                )\n                # If not mutable don't show any write APIs\n                if not db.is_mutable:\n                    continue\n\n                if await self.ds.permission_allowed(\n                    request.actor, \"insert-row\", (name, table)\n                ):\n                    pks = await db.primary_keys(table)\n                    table_links.extend(\n                        [\n                            {\n                                \"path\": self.ds.urls.table(name, table) + \"/-/insert\",\n                                \"method\": \"POST\",\n                                \"label\": \"Insert rows into {}\".format(table),\n                                \"json\": {\n                                    \"rows\": [\n                                        {\n                                            column: None\n                                            for column in await db.table_columns(table)\n                                            if column not in pks\n                                        }\n                                    ]\n                                },\n                            },\n                            {\n                                \"path\": self.ds.urls.table(name, table) + \"/-/upsert\",\n                                \"method\": \"POST\",\n                                \"label\": \"Upsert rows into {}\".format(table),\n                                \"json\": {\n                                    \"rows\": [\n                                        {\n                                            column: None\n                                            for column in await db.table_columns(table)\n                                            if column not in pks\n                                        }\n                                    ]\n                                },\n                            },\n                        ]\n                    )\n                if await self.ds.permission_allowed(\n                    request.actor, \"drop-table\", (name, table)\n                ):\n                    table_links.append(\n                        {\n                            \"path\": self.ds.urls.table(name, table) + \"/-/drop\",\n                            \"label\": \"Drop table {}\".format(table),\n                            \"json\": {\"confirm\": False},\n                            \"method\": \"POST\",\n                        }\n                    )\n            database_links = []\n            if (\n                await self.ds.permission_allowed(request.actor, \"create-table\", name)\n                and db.is_mutable\n            ):\n                database_links.append(\n                    {\n                        \"path\": self.ds.urls.database(name) + \"/-/create\",\n                        \"label\": \"Create table in {}\".format(name),\n                        \"json\": {\n                            \"table\": \"new_table\",\n                            \"columns\": [\n                                {\"name\": \"id\", \"type\": \"integer\"},\n                                {\"name\": \"name\", \"type\": \"text\"},\n                            ],\n                            \"pk\": \"id\",\n                        },\n                        \"method\": \"POST\",\n                    }\n                )\n            if database_links or tables:\n                databases.append(\n                    {\n                        \"name\": name,\n                        \"links\": database_links,\n                        \"tables\": tables,\n                    }\n                )\n        # Sort so that mutable databases are first\n        databases.sort(key=lambda d: not self.ds.databases[d[\"name\"]].is_mutable)\n        return databases\n\n    async def get(self, request):\n        def api_path(link):\n            return \"/-/api#{}\".format(\n                urllib.parse.urlencode(\n                    {\n                        key: json.dumps(value, indent=2) if key == \"json\" else value\n                        for key, value in link.items()\n                        if key in (\"path\", \"method\", \"json\")\n                    }\n                )\n            )\n\n        return await self.render(\n            [\"api_explorer.html\"],\n            request,\n            {\n                \"example_links\": await self.example_links(request),\n                \"api_path\": api_path,\n            },\n        )\n", ".. _changelog:\n\n=========\nChangelog\n=========\n\n.. _v1_0_a3:\n\n1.0a3 (2023-08-09)\n------------------\n\nThis alpha release previews the updated design for Datasette's default JSON API. (:issue:`782`)\n\nThe new :ref:`default JSON representation <json_api_default>` for both table pages (``/dbname/table.json``) and arbitrary SQL queries (``/dbname.json?sql=...``) is now shaped like this:\n\n.. code-block:: json\n\n    {\n      \"ok\": true,\n      \"rows\": [\n        {\n          \"id\": 3,\n          \"name\": \"Detroit\"\n        },\n        {\n          \"id\": 2,\n          \"name\": \"Los Angeles\"\n        },\n        {\n          \"id\": 4,\n          \"name\": \"Memnonia\"\n        },\n        {\n          \"id\": 1,\n          \"name\": \"San Francisco\"\n        }\n      ],\n      \"truncated\": false\n    }\n\nTables will include an additional ``\"next\"`` key for pagination, which can be passed to ``?_next=`` to fetch the next page of results.\n\nThe various ``?_shape=`` options continue to work as before - see :ref:`json_api_shapes` for details.\n\nA new ``?_extra=`` mechanism is available for tables, but has not yet been stabilized or documented. Details on that are available in :issue:`262`.\n\nSmaller changes\n~~~~~~~~~~~~~~~\n\n- Datasette documentation now shows YAML examples for :ref:`metadata` by default, with a tab interface for switching to JSON. (:issue:`1153`)\n- :ref:`plugin_register_output_renderer` plugins now have access to ``error`` and ``truncated`` arguments, allowing them to display error messages and take into account truncated results. (:issue:`2130`)\n- ``render_cell()`` plugin hook now also supports an optional ``request`` argument. (:issue:`2007`)\n- New ``Justfile`` to support development workflows for Datasette using `Just <https://github.com/casey/just>`__.\n- ``datasette.render_template()`` can now accepts a ``datasette.views.Context`` subclass as an alternative to a dictionary. (:issue:`2127`)\n- ``datasette install -e path`` option for editable installations, useful while developing plugins. (:issue:`2106`)\n- When started with the ``--cors`` option Datasette now serves an ``Access-Control-Max-Age: 3600`` header, ensuring CORS OPTIONS requests are repeated no more than once an hour. (:issue:`2079`)\n- Fixed a bug where the ``_internal`` database could display ``None`` instead of ``null`` for in-memory databases. (:issue:`1970`)\n\n.. _v0_64_2:\n\n0.64.2 (2023-03-08)\n-------------------\n\n- Fixed a bug with ``datasette publish cloudrun`` where deploys all used the same Docker image tag. This was mostly inconsequential as the service is deployed as soon as the image has been pushed to the registry, but could result in the incorrect image being deployed if two different deploys for two separate services ran at exactly the same time. (:issue:`2036`)\n\n.. _v0_64_1:\n\n0.64.1 (2023-01-11)\n-------------------\n\n- Documentation now links to a current source of information for installing Python 3. (:issue:`1987`)\n- Incorrectly calling the Datasette constructor using ``Datasette(\"path/to/data.db\")`` instead of ``Datasette([\"path/to/data.db\"])`` now returns a useful error message. (:issue:`1985`)\n\n.. _v0_64:\n\n0.64 (2023-01-09)\n-----------------\n\n- Datasette now **strongly recommends against allowing arbitrary SQL queries if you are using SpatiaLite**. SpatiaLite includes SQL functions that could cause the Datasette server to crash. See :ref:`spatialite` for more details.\n- New :ref:`setting_default_allow_sql` setting, providing an easier way to disable all arbitrary SQL execution by end users: ``datasette --setting default_allow_sql off``. See also :ref:`authentication_permissions_execute_sql`. (:issue:`1409`)\n- `Building a location to time zone API with SpatiaLite <https://datasette.io/tutorials/spatialite>`__ is a new Datasette tutorial showing how to safely use SpatiaLite to create a location to time zone API.\n- New documentation about :ref:`how to debug problems loading SQLite extensions <installation_extensions>`. The error message shown when an extension cannot be loaded has also been improved. (:issue:`1979`)\n- Fixed an accessibility issue: the ``<select>`` elements in the table filter form now show an outline when they are currently focused. (:issue:`1771`)\n\n.. _v0_63_3:\n\n0.63.3 (2022-12-17)\n-------------------\n\n- Fixed a bug where ``datasette --root``, when running in Docker, would only output the URL to sign in root when the server shut down, not when it started up. (:issue:`1958`)\n- You no longer need to ensure ``await datasette.invoke_startup()`` has been called in order for Datasette to start correctly serving requests - this is now handled automatically the first time the server receives a request. This fixes a bug experienced when Datasette is served directly by an ASGI application server such as Uvicorn or Gunicorn. It also fixes a bug with the `datasette-gunicorn <https://datasette.io/plugins/datasette-gunicorn>`__ plugin. (:issue:`1955`)\n\n.. _v1_0_a2:\n\n1.0a2 (2022-12-14)\n------------------\n\nThe third Datasette 1.0 alpha release adds upsert support to the JSON API, plus the ability to specify finely grained permissions when creating an API token.\n\nSee `Datasette 1.0a2: Upserts and finely grained permissions <https://simonwillison.net/2022/Dec/15/datasette-1a2/>`__ for an extended, annotated version of these release notes.\n\n- New ``/db/table/-/upsert`` API, :ref:`documented here <TableUpsertView>`. upsert is an update-or-insert: existing rows will have specified keys updated, but if no row matches the incoming primary key a brand new row will be inserted instead. (:issue:`1878`)\n- New :ref:`plugin_register_permissions` plugin hook. Plugins can now register named permissions, which will then be listed in various interfaces that show available permissions. (:issue:`1940`)\n- The ``/db/-/create`` API for :ref:`creating a table <TableCreateView>` now accepts ``\"ignore\": true`` and ``\"replace\": true`` options when called with the ``\"rows\"`` property that creates a new table based on an example set of rows. This means the API can be called multiple times with different rows, setting rules for what should happen if a primary key collides with an existing row. (:issue:`1927`)\n- Arbitrary permissions can now be configured at the instance, database and resource (table, SQL view or canned query) level in Datasette's :ref:`metadata` JSON and YAML files. The new ``\"permissions\"`` key can be used to specify which actors should have which permissions. See :ref:`authentication_permissions_other` for details. (:issue:`1636`)\n- The ``/-/create-token`` page can now be used to create API tokens which are restricted to just a subset of actions, including against specific databases or resources. See :ref:`CreateTokenView` for details. (:issue:`1947`)\n- Likewise, the ``datasette create-token`` CLI command can now create tokens with :ref:`a subset of permissions <authentication_cli_create_token_restrict>`. (:issue:`1855`)\n- New :ref:`datasette.create_token() API method <datasette_create_token>` for programmatically creating signed API tokens. (:issue:`1951`)\n- ``/db/-/create`` API now requires actor to have ``insert-row`` permission in order to use the ``\"row\"`` or ``\"rows\"`` properties. (:issue:`1937`)\n\n.. _v1_0_a1:\n\n1.0a1 (2022-12-01)\n------------------\n\n- Write APIs now serve correct CORS headers if Datasette is started in ``--cors`` mode. See the full list of :ref:`CORS headers <json_api>` in the documentation. (:issue:`1922`)\n- Fixed a bug where the ``_memory`` database could be written to even though writes were not persisted. (:issue:`1917`)\n- The https://latest.datasette.io/ demo instance now includes an ``ephemeral`` database which can be used to test Datasette's write APIs, using the new `datasette-ephemeral-tables <https://datasette.io/plugins/datasette-ephemeral-tables>`_ plugin to drop any created tables after five minutes. This database is only available if you sign in as the root user using the link on the homepage. (:issue:`1915`)\n- Fixed a bug where hitting the write endpoints with a ``GET`` request returned a 500 error. It now returns a 405 (method not allowed) error instead. (:issue:`1916`)\n- The list of endpoints in the API explorer now lists mutable databases first. (:issue:`1918`)\n- The ``\"ignore\": true`` and ``\"replace\": true`` options for the insert API are :ref:`now documented <TableInsertView>`. (:issue:`1924`)\n\n.. _v1_0_a0:\n\n1.0a0 (2022-11-29)\n------------------\n\nThis first alpha release of Datasette 1.0 introduces a brand new collection of APIs for writing to the database (:issue:`1850`), as well as a new API token mechanism baked into Datasette core. Previously, API tokens have only been supported by installing additional plugins.\n\nThis is very much a preview: expect many more backwards incompatible API changes prior to the full 1.0 release.\n\nFeedback enthusiastically welcomed, either through `issue comments <https://github.com/simonw/datasette/issues/1850>`__ or via the `Datasette Discord <https://datasette.io/discord>`__ community.\n\nSigned API tokens\n~~~~~~~~~~~~~~~~~\n\n- New ``/-/create-token`` page allowing authenticated users to create signed API tokens that can act on their behalf, see :ref:`CreateTokenView`. (:issue:`1852`)\n- New ``datasette create-token`` command for creating tokens from the command line: :ref:`authentication_cli_create_token`.\n- New :ref:`setting_allow_signed_tokens` setting which can be used to turn off signed token support. (:issue:`1856`)\n- New :ref:`setting_max_signed_tokens_ttl` setting for restricting the maximum allowed duration of a signed token. (:issue:`1858`)\n\nWrite API\n~~~~~~~~~\n\n- New API explorer at ``/-/api`` for trying out the API. (:issue:`1871`)\n- ``/db/-/create`` API for :ref:`TableCreateView`. (:issue:`1882`)\n- ``/db/table/-/insert`` API for :ref:`TableInsertView`. (:issue:`1851`)\n- ``/db/table/-/drop`` API for :ref:`TableDropView`. (:issue:`1874`)\n- ``/db/table/pk/-/update`` API for :ref:`RowUpdateView`. (:issue:`1863`)\n- ``/db/table/pk/-/delete`` API for :ref:`RowDeleteView`. (:issue:`1864`)\n\n.. _v0_63_2:\n\n0.63.2 (2022-11-18)\n-------------------\n\n- Fixed a bug in ``datasette publish heroku`` where deployments failed due to an older version of Python being requested. (:issue:`1905`)\n- New ``datasette publish heroku --generate-dir <dir>`` option for generating a Heroku deployment directory without deploying it.\n\n.. _v0_63_1:\n\n0.63.1 (2022-11-10)\n-------------------\n\n- Fixed a bug where Datasette's table filter form would not redirect correctly when run behind a proxy using the :ref:`base_url <setting_base_url>` setting. (:issue:`1883`)\n- SQL query is now shown wrapped in a ``<textarea>`` if a query exceeds a time limit. (:issue:`1876`)\n- Fixed an intermittent \"Too many open files\" error while running the test suite. (:issue:`1843`)\n- New :ref:`database_close` internal method.\n\n.. _v0_63:\n\n0.63 (2022-10-27)\n-----------------\n\nSee `Datasette 0.63: The annotated release notes <https://simonwillison.net/2022/Oct/27/datasette-0-63/>`__ for more background on the changes in this release.\n\nFeatures\n~~~~~~~~\n\n- Now tested against Python 3.11. Docker containers used by ``datasette publish`` and ``datasette package`` both now use that version of Python. (:issue:`1853`)\n- ``--load-extension`` option now supports entrypoints. Thanks, Alex Garcia. (`#1789 <https://github.com/simonw/datasette/pull/1789>`__)\n- Facet size can now be set per-table with the new ``facet_size`` table metadata option. (:issue:`1804`)\n- The :ref:`setting_truncate_cells_html` setting now also affects long URLs in columns. (:issue:`1805`)\n- The non-JavaScript SQL editor textarea now increases height to fit the SQL query. (:issue:`1786`)\n- Facets are now displayed with better line-breaks in long values. Thanks, Daniel Rech. (`#1794 <https://github.com/simonw/datasette/pull/1794>`__)\n- The ``settings.json`` file used in :ref:`config_dir` is now validated on startup. (:issue:`1816`)\n- SQL queries can now include leading SQL comments, using ``/* ... */`` or ``-- ...`` syntax. Thanks,  Charles Nepote. (:issue:`1860`)\n- SQL query is now re-displayed when terminated with a time limit error. (:issue:`1819`)\n- The :ref:`inspect data <performance_inspect>` mechanism is now used to speed up server startup - thanks, Forest Gregg. (:issue:`1834`)\n- In :ref:`config_dir` databases with filenames ending in ``.sqlite`` or ``.sqlite3`` are now automatically added to the Datasette instance. (:issue:`1646`)\n- Breadcrumb navigation display now respects the current user's permissions. (:issue:`1831`)\n\nPlugin hooks and internals\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- The :ref:`plugin_hook_prepare_jinja2_environment` plugin hook now accepts an optional ``datasette`` argument. Hook implementations can also now return an ``async`` function which will be awaited automatically. (:issue:`1809`)\n- ``Database(is_mutable=)`` now defaults to ``True``. (:issue:`1808`)\n- The :ref:`datasette.check_visibility() <datasette_check_visibility>` method now accepts an optional ``permissions=`` list, allowing it to take multiple permissions into account at once when deciding if something should be shown as public or private. This has been used to correctly display padlock icons in more places in the Datasette interface. (:issue:`1829`)\n- Datasette no longer enforces upper bounds on its dependencies. (:issue:`1800`)\n\nDocumentation\n~~~~~~~~~~~~~\n\n- New tutorial: `Cleaning data with sqlite-utils and Datasette <https://datasette.io/tutorials/clean-data>`__.\n- Screenshots in the documentation are now maintained using `shot-scraper <https://shot-scraper.datasette.io/>`__, as described in `Automating screenshots for the Datasette documentation using shot-scraper <https://simonwillison.net/2022/Oct/14/automating-screenshots/>`__. (:issue:`1844`)\n- More detailed command descriptions on the :ref:`CLI reference <cli_reference>` page. (:issue:`1787`)\n- New documentation on :ref:`deploying_openrc` - thanks, Adam Simpson. (`#1825 <https://github.com/simonw/datasette/pull/1825>`__)\n\n.. _v0_62:\n\n0.62 (2022-08-14)\n-------------------\n\nDatasette can now run entirely in your browser using WebAssembly. Try out `Datasette Lite <https://lite.datasette.io/>`__, take a look `at the code <https://github.com/simonw/datasette-lite>`__ or read more about it in `Datasette Lite: a server-side Python web application running in a browser <https://simonwillison.net/2022/May/4/datasette-lite/>`__.\n\nDatasette now has a `Discord community <https://discord.gg/ktd74dm5mw>`__ for questions and discussions about Datasette and its ecosystem of projects.\n\nFeatures\n~~~~~~~~\n\n- Datasette is now compatible with `Pyodide <https://pyodide.org/>`__.  This is the enabling technology behind `Datasette Lite <https://lite.datasette.io/>`__. (:issue:`1733`)\n- Database file downloads now implement conditional GET using ETags. (:issue:`1739`)\n- HTML for facet results and suggested results has been extracted out into new templates ``_facet_results.html`` and ``_suggested_facets.html``. Thanks, M. Nasimul Haque. (`#1759 <https://github.com/simonw/datasette/pull/1759>`__)\n- Datasette now runs some SQL queries in parallel. This has limited impact on performance, see `this research issue <https://github.com/simonw/datasette/issues/1727>`__ for details.\n- New ``--nolock`` option for ignoring file locks when opening read-only databases. (:issue:`1744`)\n- Spaces in the database names in URLs are now encoded as ``+`` rather than ``~20``. (:issue:`1701`)\n- ``<Binary: 2427344 bytes>`` is now displayed as ``<Binary: 2,427,344 bytes>`` and is accompanied by tooltip showing \"2.3MB\". (:issue:`1712`)\n- The base Docker image used by ``datasette publish cloudrun``, ``datasette package`` and the `official Datasette image <https://hub.docker.com/datasetteproject/datasette>`__ has been upgraded to ``3.10.6-slim-bullseye``.  (:issue:`1768`)\n- Canned writable queries against immutable databases now show a warning message. (:issue:`1728`)\n- ``datasette publish cloudrun`` has a new ``--timeout`` option which can be used to increase the time limit applied by the Google Cloud build environment. Thanks, Tim Sherratt. (`#1717 <https://github.com/simonw/datasette/pull/1717>`__)\n- ``datasette publish cloudrun`` has new ``--min-instances`` and ``--max-instances`` options. (:issue:`1779`)\n\nPlugin hooks\n~~~~~~~~~~~~\n\n- New plugin hook: :ref:`handle_exception() <plugin_hook_handle_exception>`, for custom handling of exceptions caught by Datasette. (:issue:`1770`)\n- The :ref:`render_cell() <plugin_hook_render_cell>` plugin hook is now also passed a ``row`` argument, representing the ``sqlite3.Row`` object that is being rendered. (:issue:`1300`)\n- The :ref:`configuration directory <config_dir>` is now stored in ``datasette.config_dir``, making it available to plugins. Thanks, Chris Amico. (`#1766 <https://github.com/simonw/datasette/pull/1766>`__)\n\nBug fixes\n~~~~~~~~~\n\n- Don't show the facet option in the cog menu if faceting is not allowed. (:issue:`1683`)\n- ``?_sort`` and ``?_sort_desc`` now work if the column that is being sorted has been excluded from the query using ``?_col=`` or ``?_nocol=``. (:issue:`1773`)\n- Fixed bug where ``?_sort_desc`` was duplicated in the URL every time the Apply button was clicked. (:issue:`1738`)\n\nDocumentation\n~~~~~~~~~~~~~\n\n- Examples in the documentation now include a copy-to-clipboard button. (:issue:`1748`)\n- Documentation now uses the `Furo <https://github.com/pradyunsg/furo>`__ Sphinx theme. (:issue:`1746`)\n- Code examples in the documentation are now all formatted using Black. (:issue:`1718`)\n- ``Request.fake()`` method is now documented, see :ref:`internals_request`.\n- New documentation for plugin authors: :ref:`testing_plugins_register_in_test`. (:issue:`903`)\n\n.. _v0_61_1:\n\n0.61.1 (2022-03-23)\n-------------------\n\n- Fixed a bug where databases with a different route from their name (as used by the `datasette-hashed-urls plugin <https://datasette.io/plugins/datasette-hashed-urls>`__) returned errors when executing custom SQL queries. (:issue:`1682`)\n\n.. _v0_61:\n\n0.61 (2022-03-23)\n-----------------\n\nIn preparation for Datasette 1.0, this release includes two potentially backwards-incompatible changes. Hashed URL mode has been moved to a separate plugin, and the way Datasette generates URLs to databases and tables with special characters in their name such as ``/`` and ``.`` has changed.\n\nDatasette also now requires Python 3.7 or higher.\n\n- URLs within Datasette now use a different encoding scheme for tables or databases that include \"special\" characters outside of the range of ``a-zA-Z0-9_-``. This scheme is explained here: :ref:`internals_tilde_encoding`. (:issue:`1657`)\n- Removed hashed URL mode from Datasette. The new ``datasette-hashed-urls`` plugin can be used to achieve the same result, see :ref:`performance_hashed_urls` for details. (:issue:`1661`)\n- Databases can now have a custom path within the Datasette instance that is independent of the database name, using the ``db.route`` property. (:issue:`1668`)\n- Datasette is now covered by a `Code of Conduct <https://github.com/simonw/datasette/blob/main/CODE_OF_CONDUCT.md>`__. (:issue:`1654`)\n- Python 3.6 is no longer supported. (:issue:`1577`)\n- Tests now run against Python 3.11-dev. (:issue:`1621`)\n- New :ref:`datasette.ensure_permissions(actor, permissions) <datasette_ensure_permissions>` internal method for checking multiple permissions at once. (:issue:`1675`)\n- New :ref:`datasette.check_visibility(actor, action, resource=None) <datasette_check_visibility>` internal method for checking if a user can see a resource that would otherwise be invisible to unauthenticated users. (:issue:`1678`)\n- Table and row HTML pages now include a ``<link rel=\"alternate\" type=\"application/json+datasette\" href=\"...\">`` element and return a ``Link: URL; rel=\"alternate\"; type=\"application/json+datasette\"`` HTTP header pointing to the JSON version of those pages. (:issue:`1533`)\n- ``Access-Control-Expose-Headers: Link`` is now added to the CORS headers, allowing remote JavaScript to access that header.\n- Canned queries are now shown at the top of the database page, directly below the SQL editor. Previously they were shown at the bottom, below the list of tables. (:issue:`1612`)\n- Datasette now has a default favicon. (:issue:`1603`)\n- ``sqlite_stat`` tables are now hidden by default. (:issue:`1587`)\n- SpatiaLite tables ``data_licenses``, ``KNN`` and ``KNN2`` are now hidden by default. (:issue:`1601`)\n- SQL query tracing mechanism now works for queries executed in ``asyncio`` sub-tasks, such as those created by ``asyncio.gather()``. (:issue:`1576`)\n- :ref:`internals_tracer` mechanism is now documented.\n- Common Datasette symbols can now be imported directly from the top-level ``datasette`` package, see :ref:`internals_shortcuts`. Those symbols are ``Response``, ``Forbidden``, ``NotFound``, ``hookimpl``, ``actor_matches_allow``. (:issue:`957`)\n- ``/-/versions`` page now returns additional details for libraries used by SpatiaLite. (:issue:`1607`)\n- Documentation now links to the `Datasette Tutorials <https://datasette.io/tutorials>`__.\n- Datasette will now also look for SpatiaLite in ``/opt/homebrew`` - thanks, Dan Peterson. (`#1649 <https://github.com/simonw/datasette/pull/1649>`__)\n- Fixed bug where :ref:`custom pages <custom_pages>` did not work on Windows. Thanks, Robert Christie. (:issue:`1545`)\n- Fixed error caused when a table had a column named ``n``. (:issue:`1228`)\n\n.. _v0_60_2:\n\n0.60.2 (2022-02-07)\n-------------------\n\n- Fixed a bug where Datasette would open the same file twice with two different database names if you ran ``datasette file.db file.db``. (:issue:`1632`)\n\n.. _v0_60_1:\n\n0.60.1 (2022-01-20)\n-------------------\n\n- Fixed a bug where installation on Python 3.6 stopped working due to a change to an underlying dependency. This release can now be installed on Python 3.6, but is the last release of Datasette that will support anything less than Python 3.7. (:issue:`1609`)\n\n.. _v0_60:\n\n0.60 (2022-01-13)\n-----------------\n\nPlugins and internals\n~~~~~~~~~~~~~~~~~~~~~\n\n- New plugin hook: :ref:`plugin_hook_filters_from_request`, which runs on the table page and can be used to support new custom query string parameters that modify the SQL query. (:issue:`473`)\n- Added two additional methods for writing to the database: :ref:`database_execute_write_script` and :ref:`database_execute_write_many`. (:issue:`1570`)\n- The :ref:`db.execute_write() <database_execute_write>` internal method now defaults to blocking until the write operation has completed. Previously it defaulted to queuing the write and then continuing to run code while the write was in the queue. (:issue:`1579`)\n- Database write connections now execute the :ref:`plugin_hook_prepare_connection` plugin hook. (:issue:`1564`)\n- The ``Datasette()`` constructor no longer requires the ``files=`` argument, and is now documented at :ref:`internals_datasette`. (:issue:`1563`)\n- The tracing feature now traces write queries, not just read queries. (:issue:`1568`)\n- The query string variables exposed by ``request.args`` will now include blank strings for arguments such as ``foo`` in ``?foo=&bar=1`` rather than ignoring those parameters entirely. (:issue:`1551`)\n\nFaceting\n~~~~~~~~\n\n- The number of unique values in a facet is now always displayed. Previously it was only displayed if the user specified ``?_facet_size=max``. (:issue:`1556`)\n- Facets of type ``date`` or ``array`` can now be configured in ``metadata.json``, see :ref:`facets_metadata`. Thanks, David Larlet. (:issue:`1552`)\n- New ``?_nosuggest=1`` parameter for table views, which disables facet suggestion. (:issue:`1557`)\n- Fixed bug where ``?_facet_array=tags&_facet=tags`` would only display one of the two selected facets. (:issue:`625`)\n\nOther small fixes\n~~~~~~~~~~~~~~~~~\n\n- Made several performance improvements to the database schema introspection code that runs when Datasette first starts up. (:issue:`1555`)\n- Label columns detected for foreign keys are now case-insensitive, so ``Name`` or ``TITLE`` will be detected in the same way as ``name`` or ``title``. (:issue:`1544`)\n- Upgraded Pluggy dependency to 1.0. (:issue:`1575`)\n- Now using `Plausible analytics <https://plausible.io/>`__ for the Datasette documentation.\n- ``explain query plan`` is now allowed with varying amounts of whitespace in the query. (:issue:`1588`)\n- New :ref:`cli_reference` page showing the output of ``--help`` for each of the ``datasette`` sub-commands. This lead to several small improvements to the help copy. (:issue:`1594`)\n- Fixed bug where writable canned queries could not be used with custom templates.  (:issue:`1547`)\n- Improved fix for a bug where columns with a underscore prefix could result in unnecessary hidden form fields. (:issue:`1527`)\n\n.. _v0_59_4:\n\n0.59.4 (2021-11-29)\n-------------------\n\n- Fixed bug where columns with a leading underscore could not be removed from the interactive filters list. (:issue:`1527`)\n- Fixed bug where columns with a leading underscore were not correctly linked to by the \"Links from other tables\" interface on the row page. (:issue:`1525`)\n- Upgraded dependencies ``aiofiles``, ``black`` and ``janus``.\n\n.. _v0_59_3:\n\n0.59.3 (2021-11-20)\n-------------------\n\n- Fixed numerous bugs when running Datasette :ref:`behind a proxy <deploying_proxy>` with a prefix URL path using the :ref:`setting_base_url` setting. A live demo of this mode is now available at `datasette-apache-proxy-demo.datasette.io/prefix/ <https://datasette-apache-proxy-demo.datasette.io/prefix/>`__. (:issue:`1519`, :issue:`838`)\n- ``?column__arraycontains=`` and ``?column__arraynotcontains=`` table parameters now also work against SQL views. (:issue:`448`)\n- ``?_facet_array=column`` no longer returns incorrect counts if columns contain the same value more than once.\n\n.. _v0_59_2:\n\n0.59.2 (2021-11-13)\n-------------------\n\n- Column names with a leading underscore now work correctly when used as a facet. (:issue:`1506`)\n- Applying ``?_nocol=`` to a column no longer removes that column from the filtering interface. (:issue:`1503`)\n- Official Datasette Docker container now uses Debian Bullseye as the base image. (:issue:`1497`)\n- Datasette is four years old today! Here's the `original release announcement <https://simonwillison.net/2017/Nov/13/datasette/>`__ from 2017.\n\n.. _v0_59_1:\n\n0.59.1 (2021-10-24)\n-------------------\n\n- Fix compatibility with Python 3.10. (:issue:`1482`)\n- Documentation on how to use :ref:`sql_parameters` with integer and floating point values. (:issue:`1496`)\n\n.. _v0_59:\n\n0.59 (2021-10-14)\n-----------------\n\n- Columns can now have associated metadata descriptions in ``metadata.json``, see :ref:`metadata_column_descriptions`. (:issue:`942`)\n- New :ref:`register_commands() <plugin_hook_register_commands>` plugin hook allows plugins to register additional Datasette CLI commands, e.g. ``datasette mycommand file.db``. (:issue:`1449`)\n- Adding ``?_facet_size=max`` to a table page now shows the number of unique values in each facet. (:issue:`1423`)\n- Upgraded dependency `httpx 0.20 <https://github.com/encode/httpx/releases/tag/0.20.0>`__ - the undocumented ``allow_redirects=`` parameter to :ref:`internals_datasette_client` is now ``follow_redirects=``, and defaults to ``False`` where it previously defaulted to ``True``. (:issue:`1488`)\n- The ``--cors`` option now causes Datasette to return the ``Access-Control-Allow-Headers: Authorization`` header, in addition to ``Access-Control-Allow-Origin: *``. (`#1467 <https://github.com/simonw/datasette/pull/1467>`__)\n- Code that figures out which named parameters a SQL query takes in order to display form fields for them is no longer confused by strings that contain colon characters. (:issue:`1421`)\n- Renamed ``--help-config`` option to ``--help-settings``. (:issue:`1431`)\n- ``datasette.databases`` property is now a documented API. (:issue:`1443`)\n- The ``base.html`` template now wraps everything other than the ``<footer>`` in a ``<div class=\"not-footer\">`` element, to help with advanced CSS customization. (:issue:`1446`)\n- The :ref:`render_cell() <plugin_hook_render_cell>` plugin hook can now return an awaitable function. This means the hook can execute SQL queries. (:issue:`1425`)\n- :ref:`plugin_register_routes` plugin hook now accepts an optional ``datasette`` argument. (:issue:`1404`)\n- New ``hide_sql`` canned query option for defaulting to hiding the SQL query used by a canned query, see :ref:`canned_queries_options`. (:issue:`1422`)\n- New ``--cpu`` option for :ref:`datasette publish cloudrun <publish_cloud_run>`. (:issue:`1420`)\n- If `Rich <https://github.com/willmcgugan/rich>`__ is installed in the same virtual environment as Datasette, it will be used to provide enhanced display of error tracebacks on the console. (:issue:`1416`)\n- ``datasette.utils`` :ref:`internals_utils_parse_metadata` function, used by the new `datasette-remote-metadata plugin <https://datasette.io/plugins/datasette-remote-metadata>`__, is now a documented API. (:issue:`1405`)\n- Fixed bug where ``?_next=x&_sort=rowid`` could throw an error. (:issue:`1470`)\n- Column cog menu no longer shows the option to facet by a column that is already selected by the default facets in metadata. (:issue:`1469`)\n\n.. _v0_58_1:\n\n0.58.1 (2021-07-16)\n-------------------\n\n- Fix for an intermittent race condition caused by the ``refresh_schemas()`` internal function. (:issue:`1231`)\n\n.. _v0_58:\n\n0.58 (2021-07-14)\n-----------------\n\n- New ``datasette --uds /tmp/datasette.sock`` option for binding Datasette to a Unix domain socket, see :ref:`proxy documentation <deploying_proxy>` (:issue:`1388`)\n- ``\"searchmode\": \"raw\"`` table metadata option for defaulting a table to executing SQLite full-text search syntax without first escaping it, see :ref:`full_text_search_advanced_queries`. (:issue:`1389`)\n- New plugin hook: :ref:`plugin_hook_get_metadata`, for returning custom metadata for an instance, database or table. Thanks, Brandon Roberts! (:issue:`1384`)\n- New plugin hook: :ref:`plugin_hook_skip_csrf`, for opting out of CSRF protection based on the incoming request. (:issue:`1377`)\n- The :ref:`menu_links() <plugin_hook_menu_links>`, :ref:`table_actions() <plugin_hook_table_actions>` and :ref:`database_actions() <plugin_hook_database_actions>` plugin hooks all gained a new optional ``request`` argument providing access to the current request. (:issue:`1371`)\n- Major performance improvement for Datasette faceting. (:issue:`1394`)\n- Improved documentation for :ref:`deploying_proxy` to recommend using ``ProxyPreservehost On`` with Apache. (:issue:`1387`)\n- ``POST`` requests to endpoints that do not support that HTTP verb now return a 405 error.\n- ``db.path`` can now be provided as a ``pathlib.Path`` object, useful when writing unit tests for plugins. Thanks, Chris Amico. (:issue:`1365`)\n\n.. _v0_57_1:\n\n0.57.1 (2021-06-08)\n-------------------\n\n- Fixed visual display glitch with global navigation menu. (:issue:`1367`)\n- No longer truncates the list of table columns displayed on the ``/database`` page. (:issue:`1364`)\n\n.. _v0_57:\n\n0.57 (2021-06-05)\n-----------------\n\n.. warning::\n    This release fixes a `reflected cross-site scripting <https://owasp.org/www-community/attacks/xss/#reflected-xss-attacks>`__ security hole with the ``?_trace=1`` feature. You should upgrade to this version, or to Datasette 0.56.1, as soon as possible. (:issue:`1360`)\n\nIn addition to the security fix, this release includes ``?_col=`` and ``?_nocol=`` options for controlling which columns are displayed for a table, ``?_facet_size=`` for increasing the number of facet results returned, re-display of your SQL query should an error occur and numerous bug fixes.\n\nNew features\n~~~~~~~~~~~~\n\n- If an error occurs while executing a user-provided SQL query, that query is now re-displayed in an editable form along with the error message. (:issue:`619`)\n-  New ``?_col=`` and ``?_nocol=`` parameters to show and hide columns in a table, plus an interface for hiding and showing columns in the column cog menu. (:issue:`615`)\n- A new ``?_facet_size=`` parameter for customizing the number of facet results returned on a table or view page. (:issue:`1332`)\n- ``?_facet_size=max`` sets that to the maximum, which defaults to 1,000 and is controlled by the the :ref:`setting_max_returned_rows` setting. If facet results are truncated the \u2026 at the bottom of the facet list now links to this parameter. (:issue:`1337`)\n- ``?_nofacet=1`` option to disable all facet calculations on a page, used as a performance optimization for CSV exports and ``?_shape=array/object``. (:issue:`1349`, :issue:`263`)\n- ``?_nocount=1`` option to disable full query result counts. (:issue:`1353`)\n- ``?_trace=1`` debugging option is now controlled by the new :ref:`setting_trace_debug` setting, which is turned off by default. (:issue:`1359`)\n\nBug fixes and other improvements\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- :ref:`custom_pages` now work correctly when combined with the :ref:`setting_base_url` setting. (:issue:`1238`)\n- Fixed intermittent error displaying the index page when the user did not have permission to access one of the tables. Thanks, Guy Freeman. (:issue:`1305`)\n- Columns with the name \"Link\" are no longer incorrectly displayed in bold. (:issue:`1308`)\n- Fixed error caused by tables with a single quote in their names. (:issue:`1257`)\n- Updated dependencies: ``pytest-asyncio``, ``Black``, ``jinja2``, ``aiofiles``, ``click``, and ``itsdangerous``.\n- The official Datasette Docker image now supports ``apt-get install``. (:issue:`1320`)\n- The Heroku runtime used by ``datasette publish heroku`` is now ``python-3.8.10``.\n\n.. _v0_56_1:\n\n0.56.1 (2021-06-05)\n-------------------\n\n.. warning::\n    This release fixes a `reflected cross-site scripting <https://owasp.org/www-community/attacks/xss/#reflected-xss-attacks>`__ security hole with the ``?_trace=1`` feature. You should upgrade to this version, or to Datasette 0.57, as soon as possible. (:issue:`1360`)\n\n.. _v0_56:\n\n0.56 (2021-03-28)\n-----------------\n\nDocumentation improvements, bug fixes and support for SpatiaLite 5.\n\n- The SQL editor can now be resized by dragging a handle. (:issue:`1236`)\n- Fixed a bug with JSON faceting and the ``__arraycontains`` filter caused by tables with spaces in their names. (:issue:`1239`)\n- Upgraded ``httpx`` dependency. (:issue:`1005`)\n- JSON faceting is now suggested even if a column contains blank strings. (:issue:`1246`)\n- New :ref:`datasette.add_memory_database() <datasette_add_memory_database>` method. (:issue:`1247`)\n- The :ref:`Response.asgi_send() <internals_response_asgi_send>` method is now documented. (:issue:`1266`)\n- The official Datasette Docker image now bundles SpatiaLite version 5. (:issue:`1278`)\n- Fixed a ``no such table: pragma_database_list`` bug when running Datasette against SQLite versions prior to SQLite 3.16.0. (:issue:`1276`)\n- HTML lists displayed in table cells are now styled correctly. Thanks, Bob Whitelock. (:issue:`1141`, `#1252 <https://github.com/simonw/datasette/pull/1252>`__)\n- Configuration directory mode now correctly serves immutable databases that are listed in ``inspect-data.json``. Thanks Campbell Allen and Frankie Robertson. (`#1031 <https://github.com/simonw/datasette/pull/1031>`__, `#1229 <https://github.com/simonw/datasette/pull/1229>`__)\n\n.. _v0_55:\n\n0.55 (2021-02-18)\n-----------------\n\nSupport for cross-database SQL queries and built-in support for serving via HTTPS.\n\n- The new ``--crossdb`` command-line option causes Datasette to attach up to ten database files to the same ``/_memory`` database connection. This enables cross-database SQL queries, including the ability to use joins and unions to combine data from tables that exist in different database files. See :ref:`cross_database_queries` for details. (:issue:`283`)\n- ``--ssl-keyfile`` and ``--ssl-certfile`` options can be used to specify a TLS certificate, allowing Datasette to serve traffic over ``https://`` without needing to run it behind a separate proxy. (:issue:`1221`)\n- The ``/:memory:`` page has been renamed (and redirected) to ``/_memory`` for consistency with the new ``/_internal`` database introduced in Datasette 0.54. (:issue:`1205`)\n- Added plugin testing documentation on :ref:`testing_plugins_pdb`. (:issue:`1207`)\n- The `official Datasette Docker image <https://hub.docker.com/r/datasetteproject/datasette>`__ now uses Python 3.7.10, applying `the latest security fix <https://www.python.org/downloads/release/python-3710/>`__ for that Python version. (:issue:`1235`)\n\n.. _v0_54_1:\n\n0.54.1 (2021-02-02)\n-------------------\n\n- Fixed a bug where ``?_search=`` and ``?_sort=`` parameters were incorrectly duplicated when the filter form on the table page was re-submitted. (:issue:`1214`)\n\n.. _v0_54:\n\n0.54 (2021-01-25)\n-----------------\n\nThe two big new features in this release are the ``_internal`` SQLite in-memory database storing details of all connected databases and tables, and support for JavaScript modules in plugins and additional scripts.\n\nFor additional commentary on this release, see `Datasette 0.54, the annotated release notes <https://simonwillison.net/2021/Jan/25/datasette/>`__.\n\nThe _internal database\n~~~~~~~~~~~~~~~~~~~~~~\n\nAs part of ongoing work to help Datasette handle much larger numbers of connected databases and tables (see `Datasette Library <https://github.com/simonw/datasette/issues/417>`__) Datasette now maintains an in-memory SQLite database with details of all of the attached databases, tables, columns, indexes and foreign keys. (:issue:`1150`)\n\nThis will support future improvements such as a searchable, paginated homepage of all available tables.\n\nYou can explore an example of this database by `signing in as root <https://latest.datasette.io/login-as-root>`__ to the ``latest.datasette.io`` demo instance and then navigating to `latest.datasette.io/_internal <https://latest.datasette.io/_internal>`__.\n\nPlugins can use these tables to introspect attached data in an efficient way. Plugin authors should note that this is not yet considered a stable interface, so any plugins that use this may need to make changes prior to Datasette 1.0 if the ``_internal`` table schemas change.\n\nNamed in-memory database support\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs part of the work building the ``_internal`` database, Datasette now supports named in-memory databases that can be shared across multiple connections. This allows plugins to create in-memory databases which will persist data for the lifetime of the Datasette server process. (:issue:`1151`)\n\nThe new ``memory_name=`` parameter to the :ref:`internals_database` can be used to create named, shared in-memory databases.\n\nJavaScript modules\n~~~~~~~~~~~~~~~~~~\n\n`JavaScript modules <https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules>`__ were introduced in ECMAScript 2015 and provide native browser support for the ``import`` and ``export`` keywords.\n\nTo use modules, JavaScript needs to be included in ``<script>`` tags with a ``type=\"module\"`` attribute.\n\nDatasette now has the ability to output ``<script type=\"module\">`` in places where you may wish to take advantage of modules. The ``extra_js_urls`` option described in :ref:`customization_css_and_javascript` can now be used with modules, and module support is also available for the :ref:`extra_body_script() <plugin_hook_extra_body_script>` plugin hook. (:issue:`1186`, :issue:`1187`)\n\n`datasette-leaflet-freedraw <https://datasette.io/plugins/datasette-leaflet-freedraw>`__ is the first example of a Datasette plugin that takes advantage of the new support for JavaScript modules. See `Drawing shapes on a map to query a SpatiaLite database <https://simonwillison.net/2021/Jan/24/drawing-shapes-spatialite/>`__ for more on this plugin.\n\nCode formatting with Black and Prettier\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDatasette adopted `Black <https://github.com/psf/black>`__ for opinionated Python code formatting in June 2019. Datasette now also embraces `Prettier <https://prettier.io/>`__ for JavaScript formatting, which like Black is enforced by tests in continuous integration. Instructions for using these two tools can be found in the new section on :ref:`contributing_formatting` in the contributors documentation. (:issue:`1167`)\n\nOther changes\n~~~~~~~~~~~~~\n\n- Datasette can now open multiple database files with the same name, e.g. if you run ``datasette path/to/one.db path/to/other/one.db``. (:issue:`509`)\n- ``datasette publish cloudrun`` now sets ``force_https_urls`` for every deployment, fixing some incorrect ``http://`` links. (:issue:`1178`)\n- Fixed a bug in the example nginx configuration in :ref:`deploying_proxy`. (:issue:`1091`)\n- The :ref:`Datasette Ecosystem <ecosystem>` documentation page has been reduced in size in favour of the ``datasette.io`` `tools <https://datasette.io/tools>`__ and `plugins <https://datasette.io/plugins>`__ directories. (:issue:`1182`)\n- The request object now provides a ``request.full_path`` property, which returns the path including any query string. (:issue:`1184`)\n- Better error message for disallowed ``PRAGMA`` clauses in SQL queries. (:issue:`1185`)\n- ``datasette publish heroku`` now deploys using ``python-3.8.7``.\n- New plugin testing documentation on :ref:`testing_plugins_pytest_httpx`. (:issue:`1198`)\n- All ``?_*`` query string parameters passed to the table page are now persisted in hidden form fields, so parameters such as ``?_size=10`` will be correctly passed to the next page when query filters are changed. (:issue:`1194`)\n- Fixed a bug loading a database file called ``test-database (1).sqlite``. (:issue:`1181`)\n\n\n.. _v0_53:\n\n0.53 (2020-12-10)\n-----------------\n\nDatasette has an official project website now, at https://datasette.io/. This release mainly updates the documentation to reflect the new site.\n\n- New ``?column__arraynotcontains=`` table filter. (:issue:`1132`)\n- ``datasette serve`` has a new ``--create`` option, which will create blank database files if they do not already exist rather than exiting with an error. (:issue:`1135`)\n-  New ``?_header=off`` option for CSV export which omits the CSV header row, :ref:`documented here <csv_export_url_parameters>`. (:issue:`1133`)\n- \"Powered by Datasette\" link in the footer now links to https://datasette.io/. (:issue:`1138`)\n- Project news no longer lives in the README - it can now be found at https://datasette.io/news. (:issue:`1137`)\n\n.. _v0_52_5:\n\n0.52.5 (2020-12-09)\n-------------------\n\n- Fix for error caused by combining the ``_searchmode=raw`` and ``?_search_COLUMN`` parameters. (:issue:`1134`)\n\n.. _v0_52_4:\n\n0.52.4 (2020-12-05)\n-------------------\n\n- Show `pysqlite3 <https://github.com/coleifer/pysqlite3>`__ version on ``/-/versions``, if installed. (:issue:`1125`)\n- Errors output by Datasette (e.g. for invalid SQL queries) now go to ``stderr``, not ``stdout``. (:issue:`1131`)\n- Fix for a startup error on windows caused by unnecessary ``from os import EX_CANTCREAT`` - thanks, Abdussamet Ko\u00e7ak.  (:issue:`1094`)\n\n.. _v0_52_3:\n\n0.52.3 (2020-12-03)\n-------------------\n\n- Fixed bug where static assets would 404 for Datasette installed on ARM Amazon Linux. (:issue:`1124`)\n\n.. _v0_52_2:\n\n0.52.2 (2020-12-02)\n-------------------\n\n- Generated columns from SQLite 3.31.0 or higher are now correctly displayed. (:issue:`1116`)\n- Error message if you attempt to open a SpatiaLite database now suggests using ``--load-extension=spatialite`` if it detects that the extension is available in a common location. (:issue:`1115`)\n- ``OPTIONS`` requests against the ``/database`` page no longer raise a 500 error. (:issue:`1100`)\n- Databases larger than 32MB that are published to Cloud Run can now be downloaded. (:issue:`749`)\n- Fix for misaligned cog icon on table and database pages. Thanks, Abdussamet Ko\u00e7ak. (:issue:`1121`)\n\n.. _v0_52_1:\n\n0.52.1 (2020-11-29)\n-------------------\n\n- Documentation on :ref:`testing_plugins` now recommends using :ref:`internals_datasette_client`. (:issue:`1102`)\n- Fix bug where compound foreign keys produced broken links. (:issue:`1098`)\n- ``datasette --load-module=spatialite`` now also checks for ``/usr/local/lib/mod_spatialite.so``. Thanks, Dan Peterson. (:issue:`1114`)\n\n.. _v0_52:\n\n0.52 (2020-11-28)\n-----------------\n\nThis release includes a number of changes relating to an internal rebranding effort: Datasette's **configuration** mechanism (things like ``datasette --config default_page_size:10``) has been renamed to **settings**.\n\n- New ``--setting default_page_size 10`` option as a replacement for ``--config default_page_size:10`` (note the lack of a colon). The ``--config`` option is deprecated but will continue working until Datasette 1.0. (:issue:`992`)\n- The ``/-/config`` introspection page is now ``/-/settings``, and the previous page redirects to the new one. (:issue:`1103`)\n- The ``config.json`` file in :ref:`config_dir` is now called ``settings.json``. (:issue:`1104`)\n- The undocumented ``datasette.config()`` internal method has been replaced by a documented :ref:`datasette_setting` method. (:issue:`1107`)\n\nAlso in this release:\n\n- New plugin hook: :ref:`plugin_hook_database_actions`, which adds menu items to a new cog menu shown at the top of the database page. (:issue:`1077`)\n- ``datasette publish cloudrun`` has a new ``--apt-get-install`` option that can be used to install additional Ubuntu packages as part of the deployment. This is useful for deploying the new `datasette-ripgrep plugin <https://github.com/simonw/datasette-ripgrep>`__. (:issue:`1110`)\n- Swept the documentation to remove words that minimize involved difficulty. (:issue:`1089`)\n\nAnd some bug fixes:\n\n- Foreign keys linking to rows with blank label columns now display as a hyphen, allowing those links to be clicked. (:issue:`1086`)\n- Fixed bug where row pages could sometimes 500 if the underlying queries exceeded a time limit. (:issue:`1088`)\n- Fixed a bug where the table action menu could appear partially obscured by the edge of the page. (:issue:`1084`)\n\n.. _v0_51_1:\n\n0.51.1 (2020-10-31)\n-------------------\n\n- Improvements to the new :ref:`binary` documentation page.\n\n.. _v0_51:\n\n0.51 (2020-10-31)\n-----------------\n\nA new visual design, plugin hooks for adding navigation options, better handling of binary data, URL building utility methods and better support for running Datasette behind a proxy.\n\nNew visual design\n~~~~~~~~~~~~~~~~~\n\nDatasette is no longer white and grey with blue and purple links! `Natalie Downe <https://twitter.com/natbat>`__ has been working on a visual refresh, the first iteration of which is included in this release. (`#1056 <https://github.com/simonw/datasette/pull/1056>`__)\n\n.. image:: datasette-0.51.png\n   :width: 740px\n   :alt: Screenshot showing Datasette's new visual look\n\nPlugins can now add links within Datasette\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA number of existing Datasette plugins add new pages to the Datasette interface, providig tools for things like `uploading CSVs <https://github.com/simonw/datasette-upload-csvs>`__, `editing table schemas <https://github.com/simonw/datasette-edit-schema>`__ or `configuring full-text search <https://github.com/simonw/datasette-configure-fts>`__.\n\nPlugins like this can now link to themselves from other parts of Datasette interface. The :ref:`plugin_hook_menu_links` hook (:issue:`1064`) lets plugins add links to Datasette's new top-right application menu, and the :ref:`plugin_hook_table_actions` hook (:issue:`1066`) adds links to a new \"table actions\" menu on the table page.\n\nThe demo at `latest.datasette.io <https://latest.datasette.io/>`__ now includes some example plugins. To see the new table actions menu first `sign into that demo as root <https://latest.datasette.io/login-as-root>`__ and then visit the `facetable <https://latest.datasette.io/fixtures/facetable>`__ table to see the new cog icon menu at the top of the page.\n\nBinary data\n~~~~~~~~~~~\n\nSQLite tables can contain binary data in ``BLOB`` columns. Datasette now provides links for users to download this data directly from Datasette, and uses those links to make binary data available from CSV exports. See :ref:`binary` for more details. (:issue:`1036` and :issue:`1034`).\n\nURL building\n~~~~~~~~~~~~\n\nThe new :ref:`internals_datasette_urls` family of methods can be used to generate URLs to key pages within the Datasette interface, both within custom templates and Datasette plugins. See :ref:`writing_plugins_building_urls` for more details. (:issue:`904`)\n\nRunning Datasette behind a proxy\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe :ref:`setting_base_url` configuration option is designed to help run Datasette on a specific path behind a proxy - for example if you want to run an instance of Datasette at ``/my-datasette/`` within your existing site's URL hierarchy, proxied behind nginx or Apache.\n\nSupport for this configuration option has been greatly improved (:issue:`1023`), and guidelines for using it are now available in a new documentation section on :ref:`deploying_proxy`. (:issue:`1027`)\n\nSmaller changes\n~~~~~~~~~~~~~~~\n\n- Wide tables shown within Datasette now scroll horizontally (:issue:`998`). This is achieved using a new ``<div class=\"table-wrapper\">`` element which may impact the implementation of some plugins (for example `this change to datasette-cluster-map <https://github.com/simonw/datasette-cluster-map/commit/fcb4abbe7df9071c5ab57defd39147de7145b34e>`__).\n- New :ref:`permissions_debug_menu` permission. (:issue:`1068`)\n- Removed ``--debug`` option, which didn't do anything. (:issue:`814`)\n- ``Link:`` HTTP header pagination. (:issue:`1014`)\n- ``x`` button for clearing filters. (:issue:`1016`)\n- Edit SQL button on canned queries, (:issue:`1019`)\n- ``--load-extension=spatialite`` shortcut. (:issue:`1028`)\n- scale-in animation for column action menu. (:issue:`1039`)\n- Option to pass a list of templates to ``.render_template()`` is now documented. (:issue:`1045`)\n- New ``datasette.urls.static_plugins()`` method. (:issue:`1033`)\n- ``datasette -o`` option now opens the most relevant page. (:issue:`976`)\n- ``datasette --cors`` option now enables access to ``/database.db`` downloads. (:issue:`1057`)\n- Database file downloads now implement cascading permissions, so you can download a database if you have ``view-database-download`` permission even if you do not have permission to access the Datasette instance. (:issue:`1058`)\n- New documentation on :ref:`writing_plugins_designing_urls`. (:issue:`1053`)\n\n.. _v0_50_2:\n\n0.50.2 (2020-10-09)\n-------------------\n\n- Fixed another bug introduced in 0.50 where column header links on the table page were broken. (:issue:`1011`)\n\n.. _v0_50_1:\n\n0.50.1 (2020-10-09)\n-------------------\n\n- Fixed a bug introduced in 0.50 where the export as JSON/CSV links on the table, row and query pages were broken. (:issue:`1010`)\n\n.. _v0_50:\n\n0.50 (2020-10-09)\n-----------------\n\nThe key new feature in this release is the **column actions** menu on the table page (:issue:`891`). This can be used to sort a column in ascending or descending order, facet data by that column or filter the table to just rows that have a value for that column.\n\nPlugin authors can use the new :ref:`internals_datasette_client` object to make internal HTTP requests from their plugins, allowing them to make use of Datasette's JSON API. (:issue:`943`)\n\nNew :ref:`deploying` documentation with guides for deploying Datasette on a Linux server :ref:`using systemd <deploying_systemd>` or to hosting providers :ref:`that support buildpacks <deploying_buildpacks>`. (:issue:`514`, :issue:`997`)\n\nOther improvements in this release:\n\n- :ref:`publish_cloud_run` documentation now covers Google Cloud SDK options. Thanks, Geoffrey Hing. (`#995 <https://github.com/simonw/datasette/pull/995>`__)\n- New ``datasette -o`` option which opens your browser as soon as Datasette starts up. (:issue:`970`)\n- Datasette now sets ``sqlite3.enable_callback_tracebacks(True)`` so that errors in custom SQL functions will display tracebacks. (:issue:`891`)\n- Fixed two rendering bugs with column headers in portrait mobile view. (:issue:`978`, :issue:`980`)\n- New ``db.table_column_details(table)`` introspection method for retrieving full details of the columns in a specific table, see :ref:`internals_database_introspection`.\n- Fixed a routing bug with custom page wildcard templates. (:issue:`996`)\n- ``datasette publish heroku`` now deploys using Python 3.8.6.\n- New ``datasette publish heroku --tar=`` option. (:issue:`969`)\n- ``OPTIONS`` requests against HTML pages no longer return a 500 error. (:issue:`1001`)\n- Datasette now supports Python 3.9.\n\nSee also `Datasette 0.50: The annotated release notes <https://simonwillison.net/2020/Oct/9/datasette-0-50/>`__.\n\n.. _v0_49_1:\n\n0.49.1 (2020-09-15)\n-------------------\n\n- Fixed a bug with writable canned queries that use magic parameters but accept no non-magic arguments. (:issue:`967`)\n\n.. _v0_49:\n\n0.49 (2020-09-14)\n-----------------\n\nSee also `Datasette 0.49: The annotated release notes <https://simonwillison.net/2020/Sep/15/datasette-0-49/>`__.\n\n- Writable canned queries now expose a JSON API, see :ref:`canned_queries_json_api`. (:issue:`880`)\n- New mechanism for defining page templates with custom path parameters - a template file called ``pages/about/{slug}.html`` will be used to render any requests to ``/about/something``. See :ref:`custom_pages_parameters`. (:issue:`944`)\n- ``register_output_renderer()`` render functions can now return a ``Response``. (:issue:`953`)\n- New ``--upgrade`` option for ``datasette install``. (:issue:`945`)\n- New ``datasette --pdb`` option. (:issue:`962`)\n- ``datasette --get`` exit code now reflects the internal HTTP status code. (:issue:`947`)\n- New ``raise_404()`` template function for returning 404 errors. (:issue:`964`)\n- ``datasette publish heroku`` now deploys using Python 3.8.5\n- Upgraded `CodeMirror <https://codemirror.net/>`__ to 5.57.0. (:issue:`948`)\n- Upgraded code style to Black 20.8b1. (:issue:`958`)\n- Fixed bug where selected facets were not correctly persisted in hidden form fields on the table page. (:issue:`963`)\n- Renamed the default error template from ``500.html`` to ``error.html``.\n- Custom error pages are now documented, see :ref:`custom_pages_errors`. (:issue:`965`)\n\n.. _v0_48:\n\n0.48 (2020-08-16)\n-----------------\n\n- Datasette documentation now lives at `docs.datasette.io <https://docs.datasette.io/>`__.\n- ``db.is_mutable`` property is now documented and tested, see :ref:`internals_database_introspection`.\n- The ``extra_template_vars``, ``extra_css_urls``, ``extra_js_urls`` and ``extra_body_script`` plugin hooks now all accept the same arguments. See :ref:`plugin_hook_extra_template_vars` for details. (:issue:`939`)\n- Those hooks now accept a new ``columns`` argument detailing the table columns that will be rendered on that page. (:issue:`938`)\n- Fixed bug where plugins calling ``db.execute_write_fn()`` could hang Datasette if the connection failed. (:issue:`935`)\n- Fixed bug with the ``?_nl=on`` output option and binary data. (:issue:`914`)\n\n.. _v0_47_3:\n\n0.47.3 (2020-08-15)\n-------------------\n\n- The ``datasette --get`` command-line mechanism now ensures any plugins using the ``startup()`` hook are correctly executed. (:issue:`934`)\n\n.. _v0_47_2:\n\n0.47.2 (2020-08-12)\n-------------------\n\n- Fixed an issue with the Docker image `published to Docker Hub <https://hub.docker.com/r/datasetteproject/datasette>`__. (:issue:`931`)\n\n.. _v0_47_1:\n\n0.47.1 (2020-08-11)\n-------------------\n\n- Fixed a bug where the ``sdist`` distribution of Datasette was not correctly including the template files. (:issue:`930`)\n\n.. _v0_47:\n\n0.47 (2020-08-11)\n-----------------\n\n- Datasette now has `a GitHub discussions forum <https://github.com/simonw/datasette/discussions>`__ for conversations about the project that go beyond just bug reports and issues.\n- Datasette can now be installed on macOS using Homebrew! Run ``brew install simonw/datasette/datasette``. See :ref:`installation_homebrew`. (:issue:`335`)\n- Two new commands: ``datasette install name-of-plugin`` and ``datasette uninstall name-of-plugin``. These are equivalent to ``pip install`` and ``pip uninstall`` but automatically run in the same virtual environment as Datasette, so users don't have to figure out where that virtual environment is - useful for installations created using Homebrew or ``pipx``. See :ref:`plugins_installing`. (:issue:`925`)\n- A new command-line option, ``datasette --get``, accepts a path to a URL within the Datasette instance. It will run that request through Datasette (without starting a web server) and print out the response. See :ref:`cli_datasette_get` for an example. (:issue:`926`)\n\n.. _v0_46:\n\n0.46 (2020-08-09)\n-----------------\n\n.. warning::\n    This release contains a security fix related to authenticated writable canned queries. If you are using this feature you should upgrade as soon as possible.\n\n- **Security fix:** CSRF tokens were incorrectly included in read-only canned query forms, which could allow them to be leaked to a sophisticated attacker. See `issue 918 <https://github.com/simonw/datasette/issues/918>`__ for details.\n- Datasette now supports GraphQL via the new `datasette-graphql <https://github.com/simonw/datasette-graphql>`__ plugin - see `GraphQL in Datasette with the new datasette-graphql plugin <https://simonwillison.net/2020/Aug/7/datasette-graphql/>`__.\n- Principle git branch has been renamed from ``master`` to ``main``. (:issue:`849`)\n- New debugging tool: ``/-/allow-debug tool`` (`demo here <https://latest.datasette.io/-/allow-debug>`__) helps test allow blocks against actors, as described in :ref:`authentication_permissions_allow`. (:issue:`908`)\n- New logo for the documentation, and a new project tagline: \"An open source multi-tool for exploring and publishing data\".\n- Whitespace in column values is now respected on display, using ``white-space: pre-wrap``. (:issue:`896`)\n- New ``await request.post_body()`` method for accessing the raw POST body, see :ref:`internals_request`. (:issue:`897`)\n- Database file downloads now include a ``content-length`` HTTP header, enabling download progress bars. (:issue:`905`)\n- File downloads now also correctly set the suggested file name using a ``content-disposition`` HTTP header. (:issue:`909`)\n- ``tests`` are now excluded from the Datasette package properly - thanks, abeyerpath. (:issue:`456`)\n- The Datasette package published to PyPI now includes ``sdist`` as well as ``bdist_wheel``.\n- Better titles for canned query pages. (:issue:`887`)\n- Now only loads Python files from a directory passed using the ``--plugins-dir`` option - thanks, Amjith Ramanujam. (`#890 <https://github.com/simonw/datasette/pull/890>`__)\n- New documentation section on :ref:`publish_vercel`.\n\n.. _v0_45:\n\n0.45 (2020-07-01)\n-----------------\n\nSee also `Datasette 0.45: The annotated release notes <https://simonwillison.net/2020/Jul/1/datasette-045/>`__.\n\nMagic parameters for canned queries, a log out feature, improved plugin documentation and four new plugin hooks.\n\nMagic parameters for canned queries\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCanned queries now support :ref:`canned_queries_magic_parameters`, which can be used to insert or select automatically generated values. For example::\n\n    insert into logs\n      (user_id, timestamp)\n    values\n      (:_actor_id, :_now_datetime_utc)\n\nThis inserts the currently authenticated actor ID and the current datetime. (:issue:`842`)\n\nLog out\n~~~~~~~\n\nThe :ref:`ds_actor cookie <authentication_ds_actor>` can be used by plugins (or by Datasette's :ref:`--root mechanism<authentication_root>`) to authenticate users. The new ``/-/logout`` page provides a way to clear that cookie.\n\nA \"Log out\" button now shows in the global navigation provided the user is authenticated using the ``ds_actor`` cookie. (:issue:`840`)\n\nBetter plugin documentation\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe plugin documentation has been re-arranged into four sections, including a brand new section on testing plugins. (:issue:`687`)\n\n- :ref:`plugins` introduces Datasette's plugin system and describes how to install and configure plugins.\n- :ref:`writing_plugins` describes how to author plugins, from  one-off single file plugins to packaged plugins that can be published to PyPI. It also describes how to start a plugin using the new `datasette-plugin <https://github.com/simonw/datasette-plugin>`__ cookiecutter template.\n- :ref:`plugin_hooks` is a full list of detailed documentation for every Datasette plugin hook.\n- :ref:`testing_plugins` describes how to write tests for Datasette plugins, using `pytest <https://docs.pytest.org/>`__ and `HTTPX <https://www.python-httpx.org/>`__.\n\nNew plugin hooks\n~~~~~~~~~~~~~~~~\n\n- :ref:`plugin_hook_register_magic_parameters` can be used to define new types of magic canned query parameters.\n- :ref:`plugin_hook_startup` can run custom code when Datasette first starts up. `datasette-init <https://github.com/simonw/datasette-init>`__ is a new plugin that uses this hook to create database tables and views on startup if they have not yet been created. (:issue:`834`)\n- :ref:`plugin_hook_canned_queries` lets plugins provide additional canned queries beyond those defined in Datasette's metadata. See `datasette-saved-queries <https://github.com/simonw/datasette-saved-queries>`__ for an example of this hook in action. (:issue:`852`)\n- :ref:`plugin_hook_forbidden` is a hook for customizing how Datasette responds to 403 forbidden errors. (:issue:`812`)\n\nSmaller changes\n~~~~~~~~~~~~~~~\n\n- Cascading view permissions - so if a user has ``view-table`` they can view the table page even if they do not have ``view-database`` or ``view-instance``. (:issue:`832`)\n- CSRF protection no longer applies to ``Authentication: Bearer token`` requests or requests without cookies. (:issue:`835`)\n- ``datasette.add_message()`` now works inside plugins. (:issue:`864`)\n- Workaround for \"Too many open files\" error in test runs. (:issue:`846`)\n- Respect existing ``scope[\"actor\"]`` if already set by ASGI middleware. (:issue:`854`)\n- New process for shipping :ref:`contributing_alpha_beta`. (:issue:`807`)\n- ``{{ csrftoken() }}`` now works when plugins render a template using ``datasette.render_template(..., request=request)``. (:issue:`863`)\n- Datasette now creates a single :ref:`internals_request` and uses it throughout the lifetime of the current HTTP request. (:issue:`870`)\n\n.. _v0_44:\n\n0.44 (2020-06-11)\n-----------------\n\nSee also `Datasette 0.44: The annotated release notes <https://simonwillison.net/2020/Jun/12/annotated-release-notes/>`__.\n\nAuthentication and permissions, writable canned queries, flash messages, new plugin hooks and more.\n\nAuthentication\n~~~~~~~~~~~~~~\n\nPrior to this release the Datasette ecosystem has treated authentication as exclusively the realm of plugins, most notably through `datasette-auth-github <https://github.com/simonw/datasette-auth-github>`__.\n\n0.44 introduces :ref:`authentication` as core Datasette concepts (:issue:`699`). This enables different plugins to share responsibility for authenticating requests - you might have one plugin that handles user accounts and another one that allows automated access via API keys, for example.\n\nYou'll need to install plugins if you want full user accounts, but default Datasette can now authenticate a single root user with the new ``--root`` command-line option, which outputs a one-time use URL to :ref:`authenticate as a root actor <authentication_root>` (:issue:`784`)::\n\n    datasette fixtures.db --root\n\n::\n\n    http://127.0.0.1:8001/-/auth-token?token=5b632f8cd44b868df625f5a6e2185d88eea5b22237fd3cc8773f107cc4fd6477\n    INFO:     Started server process [14973]\n    INFO:     Waiting for application startup.\n    INFO:     Application startup complete.\n    INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\n\nPlugins can implement new ways of authenticating users using the new :ref:`plugin_hook_actor_from_request` hook.\n\nPermissions\n~~~~~~~~~~~\n\nDatasette also now has a built-in concept of :ref:`authentication_permissions`. The permissions system answers the following question:\n\n    Is this **actor** allowed to perform this **action**, optionally against this particular **resource**?\n\nYou can use the new ``\"allow\"`` block syntax in ``metadata.json`` (or ``metadata.yaml``) to set required permissions at the instance, database, table or canned query level. For example, to restrict access to the ``fixtures.db`` database to the ``\"root\"`` user:\n\n.. code-block:: json\n\n    {\n        \"databases\": {\n            \"fixtures\": {\n                \"allow\": {\n                    \"id\" \"root\"\n                }\n            }\n        }\n    }\n\nSee :ref:`authentication_permissions_allow` for more details.\n\nPlugins can implement their own custom permission checks using the new :ref:`plugin_hook_permission_allowed` hook.\n\nA new debug page at ``/-/permissions`` shows recent permission checks, to help administrators and plugin authors understand exactly what checks are being performed. This tool defaults to only being available to the root user, but can be exposed to other users by plugins that respond to the ``permissions-debug`` permission. (:issue:`788`)\n\nWritable canned queries\n~~~~~~~~~~~~~~~~~~~~~~~\n\nDatasette's :ref:`canned_queries` feature lets you define SQL queries in ``metadata.json`` which can then be executed by users visiting a specific URL. https://latest.datasette.io/fixtures/neighborhood_search for example.\n\nCanned queries were previously restricted to ``SELECT``, but Datasette 0.44 introduces the ability for canned queries to execute ``INSERT`` or ``UPDATE`` queries as well, using the new ``\"write\": true`` property (:issue:`800`):\n\n.. code-block:: json\n\n    {\n        \"databases\": {\n            \"dogs\": {\n                \"queries\": {\n                    \"add_name\": {\n                        \"sql\": \"INSERT INTO names (name) VALUES (:name)\",\n                        \"write\": true\n                    }\n                }\n            }\n        }\n    }\n\nSee :ref:`canned_queries_writable` for more details.\n\nFlash messages\n~~~~~~~~~~~~~~\n\nWritable canned queries needed a mechanism to let the user know that the query has been successfully executed. The new flash messaging system (:issue:`790`) allows messages to persist in signed cookies which are then displayed to the user on the next page that they visit. Plugins can use this mechanism to display their own messages, see :ref:`datasette_add_message` for details.\n\nYou can try out the new messages using the ``/-/messages`` debug tool, for example at https://latest.datasette.io/-/messages\n\nSigned values and secrets\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBoth flash messages and user authentication needed a way to sign values and set signed cookies. Two new methods are now available for plugins to take advantage of this mechanism: :ref:`datasette_sign` and :ref:`datasette_unsign`.\n\nDatasette will generate a secret automatically when it starts up, but to avoid resetting the secret (and hence invalidating any cookies) every time the server restarts you should set your own secret. You can pass a secret to Datasette using the new ``--secret`` option or with a ``DATASETTE_SECRET`` environment variable. See :ref:`setting_secret` for more details.\n\nYou can also set a secret when you deploy Datasette using ``datasette publish`` or ``datasette package`` - see :ref:`setting_publish_secrets`.\n\nPlugins can now sign values and verify their signatures using the :ref:`datasette.sign() <datasette_sign>` and :ref:`datasette.unsign() <datasette_unsign>` methods.\n\nCSRF protection\n~~~~~~~~~~~~~~~\n\nSince writable canned queries are built using POST forms, Datasette now ships with :ref:`internals_csrf` (:issue:`798`). This applies automatically to any POST request, which means plugins need to include a ``csrftoken`` in any POST forms that they render. They can do that like so:\n\n.. code-block:: html\n\n    <input type=\"hidden\" name=\"csrftoken\" value=\"{{ csrftoken() }}\">\n\nCookie methods\n~~~~~~~~~~~~~~\n\nPlugins can now use the new :ref:`response.set_cookie() <internals_response_set_cookie>` method to set cookies.\n\nA new ``request.cookies`` method on the :ref:internals_request` can be used to read incoming cookies.\n\nregister_routes() plugin hooks\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPlugins can now register new views and routes via the :ref:`plugin_register_routes` plugin hook (:issue:`819`). View functions can be defined that accept any of the current ``datasette`` object, the current ``request``, or the ASGI ``scope``, ``send`` and ``receive`` objects.\n\nSmaller changes\n~~~~~~~~~~~~~~~\n\n- New internals documentation for :ref:`internals_request` and :ref:`internals_response`. (:issue:`706`)\n- ``request.url`` now respects the ``force_https_urls`` config setting. closes (:issue:`781`)\n- ``request.args.getlist()`` returns ``[]`` if missing. Removed ``request.raw_args`` entirely. (:issue:`774`)\n- New :ref:`datasette.get_database() <datasette_get_database>` method.\n- Added ``_`` prefix to many private, undocumented methods of the Datasette class. (:issue:`576`)\n- Removed the ``db.get_outbound_foreign_keys()`` method which duplicated the behaviour of ``db.foreign_keys_for_table()``.\n- New :ref:`await datasette.permission_allowed() <datasette_permission_allowed>` method.\n- ``/-/actor`` debugging endpoint for viewing the currently authenticated actor.\n- New ``request.cookies`` property.\n- ``/-/plugins`` endpoint now shows a list of hooks implemented by each plugin, e.g. https://latest.datasette.io/-/plugins?all=1\n- ``request.post_vars()`` method no longer discards empty values.\n- New \"params\" canned query key for explicitly setting named parameters, see :ref:`canned_queries_named_parameters`. (:issue:`797`)\n- ``request.args`` is now a :ref:`MultiParams <internals_multiparams>` object.\n- Fixed a bug with the ``datasette plugins`` command. (:issue:`802`)\n- Nicer pattern for using ``make_app_client()`` in tests. (:issue:`395`)\n- New ``request.actor`` property.\n- Fixed broken CSS on nested 404 pages. (:issue:`777`)\n- New ``request.url_vars`` property. (:issue:`822`)\n- Fixed a bug with the ``python tests/fixtures.py`` command for outputting Datasette's testing fixtures database and plugins. (:issue:`804`)\n- ``datasette publish heroku`` now deploys using Python 3.8.3.\n- Added a warning that the :ref:`plugin_register_facet_classes` hook is unstable and may change in the future. (:issue:`830`)\n- The ``{\"$env\": \"ENVIRONMENT_VARIBALE\"}`` mechanism (see :ref:`plugins_configuration_secret`) now works with variables inside nested lists. (:issue:`837`)\n\nThe road to Datasette 1.0\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nI've assembled a `milestone for Datasette 1.0 <https://github.com/simonw/datasette/milestone/7>`__. The focus of the 1.0 release will be the following:\n\n- Signify confidence in the quality/stability of Datasette\n- Give plugin authors confidence that their plugins will work for the whole 1.x release cycle\n- Provide the same confidence to developers building against Datasette JSON APIs\n\nIf you have thoughts about what you would like to see for Datasette 1.0 you can join `the conversation on issue #519 <https://github.com/simonw/datasette/issues/519>`__.\n\n.. _v0_43:\n\n0.43 (2020-05-28)\n-----------------\n\nThe main focus of this release is a major upgrade to the :ref:`plugin_register_output_renderer` plugin hook, which allows plugins to provide new output formats for Datasette such as `datasette-atom <https://github.com/simonw/datasette-atom>`__ and `datasette-ics <https://github.com/simonw/datasette-ics>`__.\n\n* Redesign of :ref:`plugin_register_output_renderer` to provide more context to the render callback and support an optional ``\"can_render\"`` callback that controls if a suggested link to the output format is provided. (:issue:`581`, :issue:`770`)\n* Visually distinguish float and integer columns - useful for figuring out why order-by-column might be returning unexpected results. (:issue:`729`)\n* The :ref:`internals_request`, which is passed to several plugin hooks, is now documented. (:issue:`706`)\n* New ``metadata.json`` option for setting a custom default page size for specific tables and views, see :ref:`metadata_page_size`. (:issue:`751`)\n* Canned queries can now be configured with a default URL fragment hash, useful when working with plugins such as `datasette-vega <https://github.com/simonw/datasette-vega>`__, see :ref:`canned_queries_options`. (:issue:`706`)\n* Fixed a bug in ``datasette publish`` when running on operating systems where the ``/tmp`` directory lives in a different volume, using a backport of the Python 3.8 ``shutil.copytree()`` function. (:issue:`744`)\n* Every plugin hook is now covered by the unit tests, and a new unit test checks that each plugin hook has at least one corresponding test. (:issue:`771`, :issue:`773`)\n\n.. _v0_42:\n\n0.42 (2020-05-08)\n-----------------\n\nA small release which provides improved internal methods for use in plugins, along with documentation. See :issue:`685`.\n\n* Added documentation for ``db.execute()``, see :ref:`database_execute`.\n* Renamed ``db.execute_against_connection_in_thread()`` to ``db.execute_fn()`` and made it a documented method, see :ref:`database_execute_fn`.\n* New ``results.first()`` and ``results.single_value()`` methods, plus documentation for the ``Results`` class - see :ref:`database_results`.\n\n.. _v0_41:\n\n0.41 (2020-05-06)\n-----------------\n\nYou can now create :ref:`custom pages <custom_pages>` within your Datasette instance using a custom template file. For example, adding a template file called ``templates/pages/about.html`` will result in a new page being served at ``/about`` on your instance. See the :ref:`custom pages documentation <custom_pages>` for full details, including how to return custom HTTP headers, redirects and status codes. (:issue:`648`)\n\n:ref:`config_dir` (:issue:`731`) allows you to define a custom Datasette instance as a directory. So instead of running the following::\n\n    datasette one.db two.db \\\n      --metadata=metadata.json \\\n      --template-dir=templates/ \\\n      --plugins-dir=plugins \\\n      --static css:css\n\nYou can instead arrange your files in a single directory called ``my-project`` and run this::\n\n    datasette my-project/\n\nAlso in this release:\n\n* New ``NOT LIKE`` table filter: ``?colname__notlike=expression``. (:issue:`750`)\n* Datasette now has a *pattern portfolio* at ``/-/patterns`` - e.g. https://latest.datasette.io/-/patterns. This is a page that shows every Datasette user interface component in one place, to aid core development and people building custom CSS themes. (:issue:`151`)\n* SQLite `PRAGMA functions <https://www.sqlite.org/pragma.html#pragfunc>`__ such as ``pragma_table_info(tablename)`` are now allowed in Datasette SQL queries. (:issue:`761`)\n* Datasette pages now consistently return a ``content-type`` of ``text/html; charset=utf-8\"``. (:issue:`752`)\n* Datasette now handles an ASGI ``raw_path`` value of ``None``, which should allow compatibility with the `Mangum <https://github.com/erm/mangum>`__ adapter for running ASGI apps on AWS Lambda. Thanks, Colin Dellow. (`#719 <https://github.com/simonw/datasette/pull/719>`__)\n* Installation documentation now covers how to :ref:`installation_pipx`. (:issue:`756`)\n* Improved the documentation for :ref:`full_text_search`. (:issue:`748`)\n\n.. _v0_40:\n\n0.40 (2020-04-21)\n-----------------\n\n* Datasette :ref:`metadata` can now be provided as a YAML file as an optional alternative to JSON. See :ref:`metadata_yaml`. (:issue:`713`)\n* Removed support for ``datasette publish now``, which used the the now-retired Zeit Now v1 hosting platform. A new plugin, `datasette-publish-now <https://github.com/simonw/datasette-publish-now>`__, can be installed to publish data to Zeit (`now Vercel <https://vercel.com/blog/zeit-is-now-vercel>`__) Now v2. (:issue:`710`)\n* Fixed a bug where the ``extra_template_vars(request, view_name)`` plugin hook was not receiving the correct ``view_name``. (:issue:`716`)\n* Variables added to the template context by the ``extra_template_vars()`` plugin hook are now shown in the ``?_context=1`` debugging mode (see :ref:`setting_template_debug`). (:issue:`693`)\n* Fixed a bug where the \"templates considered\" HTML comment was no longer being displayed. (:issue:`689`)\n* Fixed a ``datasette publish`` bug where ``--plugin-secret`` would over-ride plugin configuration in the provided ``metadata.json`` file. (:issue:`724`)\n* Added a new CSS class for customizing the canned query page. (:issue:`727`)\n\n.. _v0_39:\n\n0.39 (2020-03-24)\n-----------------\n\n* New :ref:`setting_base_url` configuration setting for serving up the correct links while running Datasette under a different URL prefix. (:issue:`394`)\n* New metadata settings ``\"sort\"`` and ``\"sort_desc\"`` for setting the default sort order for a table. See :ref:`metadata_default_sort`. (:issue:`702`)\n* Sort direction arrow now displays by default on the primary key. This means you only have to click once (not twice) to sort in reverse order. (:issue:`677`)\n* New ``await Request(scope, receive).post_vars()`` method for accessing POST form variables. (:issue:`700`)\n* :ref:`plugin_hooks` documentation now links to example uses of each plugin. (:issue:`709`)\n\n.. _v0_38:\n\n0.38 (2020-03-08)\n-----------------\n\n* The `Docker build <https://hub.docker.com/r/datasetteproject/datasette>`__ of Datasette now uses SQLite 3.31.1, upgraded from 3.26. (:issue:`695`)\n* ``datasette publish cloudrun`` now accepts an optional ``--memory=2Gi`` flag for setting the Cloud Run allocated memory to a value other than the default (256Mi). (:issue:`694`)\n* Fixed bug where templates that shipped with plugins were sometimes not being correctly loaded. (:issue:`697`)\n\n.. _v0_37_1:\n\n0.37.1 (2020-03-02)\n-------------------\n\n* Don't attempt to count table rows to display on the index page for databases > 100MB. (:issue:`688`)\n* Print exceptions if they occur in the write thread rather than silently swallowing them.\n* Handle the possibility of ``scope[\"path\"]`` being a string rather than bytes\n* Better documentation for the :ref:`plugin_hook_extra_template_vars` plugin hook.\n\n.. _v0_37:\n\n0.37 (2020-02-25)\n-----------------\n\n* Plugins now have a supported mechanism for writing to a database, using the new ``.execute_write()`` and ``.execute_write_fn()`` methods. :ref:`Documentation <database_execute_write>`. (:issue:`682`)\n* Immutable databases that have had their rows counted using the ``inspect`` command now use the calculated count more effectively - thanks, Kevin Keogh. (`#666 <https://github.com/simonw/datasette/pull/666>`__)\n* ``--reload`` no longer restarts the server if a database file is modified, unless that database was opened immutable mode with ``-i``. (:issue:`494`)\n* New ``?_searchmode=raw`` option turns off escaping for FTS queries in ``?_search=`` allowing full use of SQLite's `FTS5 query syntax <https://www.sqlite.org/fts5.html#full_text_query_syntax>`__. (:issue:`676`)\n\n.. _v0_36:\n\n0.36 (2020-02-21)\n-----------------\n\n* The ``datasette`` object passed to plugins now has API documentation: :ref:`internals_datasette`. (:issue:`576`)\n* New methods on ``datasette``: ``.add_database()`` and ``.remove_database()`` - :ref:`documentation <datasette_add_database>`. (:issue:`671`)\n* ``prepare_connection()`` plugin hook now takes optional ``datasette`` and ``database`` arguments - :ref:`plugin_hook_prepare_connection`. (:issue:`678`)\n* Added three new plugins and one new conversion tool to the :ref:`ecosystem`.\n\n.. _v0_35:\n\n0.35 (2020-02-04)\n-----------------\n\n* Added five new plugins and one new conversion tool to the :ref:`ecosystem`.\n* The ``Datasette`` class has a new ``render_template()`` method which can be used by plugins to render templates using Datasette's pre-configured `Jinja <https://jinja.palletsprojects.com/>`__ templating library.\n* You can now execute SQL queries that start with a ``-- comment`` - thanks, Jay Graves (`#653 <https://github.com/simonw/datasette/pull/653>`__)\n\n.. _v0_34:\n\n0.34 (2020-01-29)\n-----------------\n\n* ``_search=`` queries are now correctly escaped using a new ``escape_fts()`` custom SQL function. This means you can now run searches for strings like ``park.`` without seeing errors. (:issue:`651`)\n* `Google Cloud Run <https://cloud.google.com/run/>`__ is no longer in beta, so ``datasette publish cloudrun`` has been updated to work even if the user has not installed the ``gcloud`` beta components package. Thanks, Katie McLaughlin (`#660 <https://github.com/simonw/datasette/pull/660>`__)\n* ``datasette package`` now accepts a ``--port`` option for specifying which port the resulting Docker container should listen on. (:issue:`661`)\n\n.. _v0_33:\n\n0.33 (2019-12-22)\n-----------------\n\n* ``rowid`` is now included in dropdown menus for filtering tables (:issue:`636`)\n* Columns are now only suggested for faceting if they have at least one value with more than one record (:issue:`638`)\n* Queries with no results now display \"0 results\" (:issue:`637`)\n* Improved documentation for the ``--static`` option (:issue:`641`)\n* asyncio task information is now included on the ``/-/threads`` debug page\n* Bumped Uvicorn dependency 0.11\n* You can now use ``--port 0`` to listen on an available port\n* New :ref:`setting_template_debug` setting for debugging templates, e.g. https://latest.datasette.io/fixtures/roadside_attractions?_context=1 (:issue:`654`)\n\n.. _v0_32:\n\n0.32 (2019-11-14)\n-----------------\n\nDatasette now renders templates using `Jinja async mode <https://jinja.palletsprojects.com/en/2.10.x/api/#async-support>`__. This means plugins can provide custom template functions that perform asynchronous actions, for example the new `datasette-template-sql <https://github.com/simonw/datasette-template-sql>`__ plugin which allows custom templates to directly execute SQL queries and render their results. (:issue:`628`)\n\n.. _v0_31_2:\n\n0.31.2 (2019-11-13)\n-------------------\n\n- Fixed a bug where ``datasette publish heroku`` applications failed to start (:issue:`633`)\n- Fix for ``datasette publish`` with just ``--source_url`` - thanks, Stanley Zheng (:issue:`572`)\n- Deployments to Heroku now use Python 3.8.0 (:issue:`632`)\n\n.. _v0_31_1:\n\n0.31.1 (2019-11-12)\n-------------------\n\n- Deployments created using ``datasette publish``  now use ``python:3.8`` base Docker image (`#629 <https://github.com/simonw/datasette/pull/629>`__)\n\n.. _v0_31:\n\n0.31 (2019-11-11)\n-----------------\n\nThis version adds compatibility with Python 3.8 and breaks compatibility with Python 3.5.\n\nIf you are still running Python 3.5 you should stick with ``0.30.2``, which you can install like this::\n\n    pip install datasette==0.30.2\n\n- Format SQL button now works with read-only SQL queries - thanks, Tobias Kunze (`#602 <https://github.com/simonw/datasette/pull/602>`__)\n- New ``?column__notin=x,y,z`` filter for table views (:issue:`614`)\n- Table view now uses ``select col1, col2, col3`` instead of ``select *``\n- Database filenames can now contain spaces - thanks, Tobias Kunze (`#590 <https://github.com/simonw/datasette/pull/590>`__)\n- Removed obsolete ``?_group_count=col`` feature (:issue:`504`)\n- Improved user interface and documentation for ``datasette publish cloudrun`` (:issue:`608`)\n- Tables with indexes now show the ``CREATE INDEX`` statements on the table page (:issue:`618`)\n- Current version of `uvicorn <https://www.uvicorn.org/>`__ is now shown on ``/-/versions``\n- Python 3.8 is now supported! (:issue:`622`)\n- Python 3.5 is no longer supported.\n\n.. _v0_30_2:\n\n0.30.2 (2019-11-02)\n-------------------\n\n- ``/-/plugins`` page now uses distribution name e.g. ``datasette-cluster-map`` instead of the name of the underlying Python package (``datasette_cluster_map``) (:issue:`606`)\n- Array faceting is now only suggested for columns that contain arrays of strings (:issue:`562`)\n- Better documentation for the ``--host`` argument (:issue:`574`)\n- Don't show ``None`` with a broken link for the label on a nullable foreign key (:issue:`406`)\n\n.. _v0_30_1:\n\n0.30.1 (2019-10-30)\n-------------------\n\n- Fixed bug where ``?_where=`` parameter was not persisted in hidden form fields (:issue:`604`)\n- Fixed bug with .JSON representation of row pages - thanks, Chris Shaw (:issue:`603`)\n\n.. _v0_30:\n\n\n0.30 (2019-10-18)\n-----------------\n\n- Added ``/-/threads`` debugging page\n- Allow ``EXPLAIN WITH...`` (:issue:`583`)\n- Button to format SQL - thanks, Tobias Kunze (:issue:`136`)\n- Sort databases on homepage by argument order - thanks, Tobias Kunze (:issue:`585`)\n- Display metadata footer on custom SQL queries - thanks, Tobias Kunze (`#589 <https://github.com/simonw/datasette/pull/589>`__)\n- Use ``--platform=managed`` for ``publish cloudrun`` (:issue:`587`)\n- Fixed bug returning non-ASCII characters in CSV (:issue:`584`)\n- Fix for ``/foo`` v.s. ``/foo-bar`` bug (:issue:`601`)\n\n.. _v0_29_3:\n\n0.29.3 (2019-09-02)\n-------------------\n\n- Fixed implementation of CodeMirror on database page (:issue:`560`)\n- Documentation typo fixes - thanks, Min ho Kim (`#561 <https://github.com/simonw/datasette/pull/561>`__)\n- Mechanism for detecting if a table has FTS enabled now works if the table name used alternative escaping mechanisms (:issue:`570`) - for compatibility with `a recent change to sqlite-utils <https://github.com/simonw/sqlite-utils/pull/57>`__.\n\n.. _v0_29_2:\n\n0.29.2 (2019-07-13)\n-------------------\n\n- Bumped `Uvicorn <https://www.uvicorn.org/>`__ to 0.8.4, fixing a bug where the query string was not included in the server logs. (:issue:`559`)\n- Fixed bug where the navigation breadcrumbs were not displayed correctly on the page for a custom query. (:issue:`558`)\n- Fixed bug where custom query names containing unicode characters caused errors.\n\n.. _v0_29_1:\n\n0.29.1 (2019-07-11)\n-------------------\n\n- Fixed bug with static mounts using relative paths which could lead to traversal exploits (:issue:`555`) - thanks Abdussamet Kocak!\n- Datasette can now be run as a module: ``python -m datasette`` (:issue:`556`) - thanks, Abdussamet Kocak!\n\n.. _v0_29:\n\n0.29 (2019-07-07)\n-----------------\n\nASGI, new plugin hooks, facet by date and much, much more...\n\nASGI\n~~~~\n\n`ASGI <https://asgi.readthedocs.io/>`__ is the Asynchronous Server Gateway Interface standard. I've been wanting to convert Datasette into an ASGI application for over a year - `Port Datasette to ASGI #272 <https://github.com/simonw/datasette/issues/272>`__ tracks thirteen months of intermittent development - but with Datasette 0.29 the change is finally released. This also means Datasette now runs on top of `Uvicorn <https://www.uvicorn.org/>`__ and no longer depends on `Sanic <https://github.com/huge-success/sanic>`__.\n\nI wrote about the significance of this change in `Porting Datasette to ASGI, and Turtles all the way down <https://simonwillison.net/2019/Jun/23/datasette-asgi/>`__.\n\nThe most exciting consequence of this change is that Datasette plugins can now take advantage of the ASGI standard.\n\nNew plugin hook: asgi_wrapper\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe :ref:`plugin_asgi_wrapper` plugin hook allows plugins to entirely wrap the Datasette ASGI application in their own ASGI middleware. (:issue:`520`)\n\nTwo new plugins take advantage of this hook:\n\n* `datasette-auth-github <https://github.com/simonw/datasette-auth-github>`__ adds a authentication layer: users will have to sign in using their GitHub account before they can view data or interact with Datasette. You can also use it to restrict access to specific GitHub users, or to members of specified GitHub `organizations <https://help.github.com/en/articles/about-organizations>`__ or `teams <https://help.github.com/en/articles/organizing-members-into-teams>`__.\n\n* `datasette-cors <https://github.com/simonw/datasette-cors>`__ allows you to configure `CORS headers <https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS>`__ for your Datasette instance. You can use this to enable JavaScript running on a whitelisted set of domains to make ``fetch()`` calls to the JSON API provided by your Datasette instance.\n\nNew plugin hook: extra_template_vars\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe :ref:`plugin_hook_extra_template_vars` plugin hook allows plugins to inject their own additional variables into the Datasette template context. This can be used in conjunction with custom templates to customize the Datasette interface. `datasette-auth-github <https://github.com/simonw/datasette-auth-github>`__ uses this hook to add custom HTML to the new top navigation bar (which is designed to be modified by plugins, see :issue:`540`).\n\nSecret plugin configuration options\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPlugins like `datasette-auth-github <https://github.com/simonw/datasette-auth-github>`__ need a safe way to set secret configuration options. Since the default mechanism for configuring plugins exposes those settings in ``/-/metadata`` a new mechanism was needed. :ref:`plugins_configuration_secret` describes how plugins can now specify that their settings should be read from a file or an environment variable::\n\n    {\n        \"plugins\": {\n            \"datasette-auth-github\": {\n                \"client_secret\": {\n                    \"$env\": \"GITHUB_CLIENT_SECRET\"\n                }\n            }\n        }\n    }\n\nThese plugin secrets can be set directly using ``datasette publish``. See :ref:`publish_custom_metadata_and_plugins` for details. (:issue:`538` and :issue:`543`)\n\nFacet by date\n~~~~~~~~~~~~~\n\nIf a column contains datetime values, Datasette can now facet that column by date. (:issue:`481`)\n\n.. _v0_29_medium_changes:\n\nEasier custom templates for table rows\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you want to customize the display of individual table rows, you can do so using a ``_table.html`` template include that looks something like this::\n\n    {% for row in display_rows %}\n        <div>\n            <h2>{{ row[\"title\"] }}</h2>\n            <p>{{ row[\"description\"] }}<lp>\n            <p>Category: {{ row.display(\"category_id\") }}</p>\n        </div>\n    {% endfor %}\n\nThis is a **backwards incompatible change**. If you previously had a custom template called ``_rows_and_columns.html`` you need to rename it to ``_table.html``.\n\nSee :ref:`customization_custom_templates` for full details.\n\n?_through= for joins through many-to-many tables\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe new ``?_through={json}`` argument to the Table view allows records to be filtered based on a many-to-many relationship. See :ref:`json_api_table_arguments` for full documentation - here's `an example <https://latest.datasette.io/fixtures/roadside_attractions?_through={%22table%22:%22roadside_attraction_characteristics%22,%22column%22:%22characteristic_id%22,%22value%22:%221%22}>`__. (:issue:`355`)\n\nThis feature was added to help support `facet by many-to-many <https://github.com/simonw/datasette/issues/551>`__, which isn't quite ready yet but will be coming in the next Datasette release.\n\nSmall changes\n~~~~~~~~~~~~~\n\n* Databases published using ``datasette publish`` now open in :ref:`performance_immutable_mode`. (:issue:`469`)\n* ``?col__date=`` now works for columns containing spaces\n* Automatic label detection (for deciding which column to show when linking to a foreign key) has been improved. (:issue:`485`)\n* Fixed bug where pagination broke when combined with an expanded foreign key. (:issue:`489`)\n* Contributors can now run ``pip install -e .[docs]`` to get all of the dependencies needed to build the documentation, including ``cd docs && make livehtml`` support.\n* Datasette's dependencies are now all specified using the ``~=`` match operator. (:issue:`532`)\n* ``white-space: pre-wrap`` now used for table creation SQL. (:issue:`505`)\n\n\n`Full list of commits <https://github.com/simonw/datasette/compare/0.28...0.29>`__ between 0.28 and 0.29.\n\n.. _v0_28:\n\n0.28 (2019-05-19)\n-----------------\n\nA `salmagundi <https://adamj.eu/tech/2019/01/18/a-salmagundi-of-django-alpha-announcements/>`__ of new features!\n\n.. _v0_28_databases_that_change:\n\nSupporting databases that change\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFrom the beginning of the project, Datasette has been designed with read-only databases in mind. If a database is guaranteed not to change it opens up all kinds of interesting opportunities - from taking advantage of SQLite immutable mode and HTTP caching to bundling static copies of the database directly in a Docker container. `The interesting ideas in Datasette <https://simonwillison.net/2018/Oct/4/datasette-ideas/>`__ explores this idea in detail.\n\nAs my goals for the project have developed, I realized that read-only databases are no longer the right default. SQLite actually supports concurrent access very well provided only one thread attempts to write to a database at a time, and I keep encountering sensible use-cases for running Datasette on top of a database that is processing inserts and updates.\n\nSo, as-of version 0.28 Datasette no longer assumes that a database file will not change. It is now safe to point Datasette at a SQLite database which is being updated by another process.\n\nMaking this change was a lot of work - see tracking tickets :issue:`418`, :issue:`419` and :issue:`420`. It required new thinking around how Datasette should calculate table counts (an expensive operation against a large, changing database) and also meant reconsidering the \"content hash\" URLs Datasette has used in the past to optimize the performance of HTTP caches.\n\nDatasette can still run against immutable files and gains numerous performance benefits from doing so, but this is no longer the default behaviour. Take a look at the new :ref:`performance` documentation section for details on how to make the most of Datasette against data that you know will be staying read-only and immutable.\n\n.. _v0_28_faceting:\n\nFaceting improvements, and faceting plugins\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDatasette :ref:`facets` provide an intuitive way to quickly summarize and interact with data. Previously the only supported faceting technique was column faceting, but 0.28 introduces two powerful new capabilities: facet-by-JSON-array and the ability to define further facet types using plugins.\n\nFacet by array (:issue:`359`) is only available if your SQLite installation provides the ``json1`` extension. Datasette will automatically detect columns that contain JSON arrays of values and offer a faceting interface against those columns - useful for modelling things like tags without needing to break them out into a new table. See :ref:`facet_by_json_array` for more.\n\nThe new :ref:`plugin_register_facet_classes` plugin hook (`#445 <https://github.com/simonw/datasette/pull/445>`__) can be used to register additional custom facet classes. Each facet class should provide two methods: ``suggest()`` which suggests facet selections that might be appropriate for a provided SQL query, and ``facet_results()`` which executes a facet operation and returns results. Datasette's own faceting implementations have been refactored to use the same API as these plugins.\n\n.. _v0_28_publish_cloudrun:\n\ndatasette publish cloudrun\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n`Google Cloud Run <https://cloud.google.com/run/>`__ is a brand new serverless hosting platform from Google, which allows you to build a Docker container which will run only when HTTP traffic is received and will shut down (and hence cost you nothing) the rest of the time. It's similar to Zeit's Now v1 Docker hosting platform which sadly is `no longer accepting signups <https://hyperion.alpha.spectrum.chat/zeit/now/cannot-create-now-v1-deployments~d206a0d4-5835-4af5-bb5c-a17f0171fb25?m=MTU0Njk2NzgwODM3OA==>`__ from new users.\n\nThe new ``datasette publish cloudrun`` command was contributed by Romain Primet (`#434 <https://github.com/simonw/datasette/pull/434>`__) and publishes selected databases to a new Datasette instance running on Google Cloud Run.\n\nSee :ref:`publish_cloud_run` for full documentation.\n\n.. _v0_28_register_output_renderer:\n\nregister_output_renderer plugins\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRuss Garrett implemented a new Datasette plugin hook called :ref:`register_output_renderer <plugin_register_output_renderer>` (`#441 <https://github.com/simonw/datasette/pull/441>`__) which allows plugins to create additional output renderers in addition to Datasette's default ``.json`` and ``.csv``.\n\nRuss's in-development `datasette-geo <https://github.com/russss/datasette-geo>`__ plugin includes `an example <https://github.com/russss/datasette-geo/blob/d4cecc020848bbde91e9e17bf352f7c70bc3dccf/datasette_plugin_geo/geojson.py>`__ of this hook being used to output ``.geojson`` automatically converted from SpatiaLite.\n\n.. _v0_28_medium_changes:\n\nMedium changes\n~~~~~~~~~~~~~~\n\n- Datasette now conforms to the `Black coding style <https://github.com/python/black>`__ (`#449 <https://github.com/simonw/datasette/pull/449>`__) - and has a unit test to enforce this in the future\n- New :ref:`json_api_table_arguments`:\n   - ``?columnname__in=value1,value2,value3`` filter for executing SQL IN queries against a table, see :ref:`table_arguments` (:issue:`433`)\n   - ``?columnname__date=yyyy-mm-dd`` filter which returns rows where the spoecified datetime column falls on the specified date (`583b22a <https://github.com/simonw/datasette/commit/583b22aa28e26c318de0189312350ab2688c90b1>`__)\n   - ``?tags__arraycontains=tag`` filter which acts against a JSON array contained in a column (`78e45ea <https://github.com/simonw/datasette/commit/78e45ead4d771007c57b307edf8fc920101f8733>`__)\n   - ``?_where=sql-fragment`` filter for the table view  (:issue:`429`)\n   - ``?_fts_table=mytable`` and ``?_fts_pk=mycolumn`` query string options can be used to specify which FTS table to use for a search query - see :ref:`full_text_search_table_or_view` (:issue:`428`)\n- You can now pass the same table filter multiple times - for example, ``?content__not=world&content__not=hello`` will return all rows where the content column is neither ``hello`` or ``world`` (:issue:`288`)\n- You can now specify ``about`` and ``about_url`` metadata (in addition to ``source`` and ``license``) linking to further information about a project - see :ref:`metadata_source_license_about`\n- New ``?_trace=1`` parameter now adds debug information showing every SQL query that was executed while constructing the page (:issue:`435`)\n- ``datasette inspect`` now just calculates table counts, and does not introspect other database metadata (:issue:`462`)\n- Removed ``/-/inspect`` page entirely - this will be replaced by something similar in the future, see :issue:`465`\n- Datasette can now run against an in-memory SQLite database. You can do this by starting it without passing any files or by using the new ``--memory`` option to ``datasette serve``. This can be useful for experimenting with SQLite queries that do not access any data, such as ``SELECT 1+1`` or ``SELECT sqlite_version()``.\n\n.. _v0_28_small_changes:\n\nSmall changes\n~~~~~~~~~~~~~\n\n- We now show the size of the database file next to the download link (:issue:`172`)\n- New ``/-/databases`` introspection page shows currently connected databases (:issue:`470`)\n- Binary data is no longer displayed on the table and row pages (`#442 <https://github.com/simonw/datasette/pull/442>`__ - thanks, Russ Garrett)\n- New show/hide SQL links on custom query pages (:issue:`415`)\n- The :ref:`extra_body_script <plugin_hook_extra_body_script>` plugin hook now accepts an optional ``view_name`` argument (`#443 <https://github.com/simonw/datasette/pull/443>`__ - thanks, Russ Garrett)\n- Bumped Jinja2 dependency to 2.10.1 (`#426 <https://github.com/simonw/datasette/pull/426>`__)\n- All table filters are now documented, and documentation is enforced via unit tests (`2c19a27 <https://github.com/simonw/datasette/commit/2c19a27d15a913e5f3dd443f04067169a6f24634>`__)\n- New project guideline: master should stay shippable at all times! (`31f36e1 <https://github.com/simonw/datasette/commit/31f36e1b97ccc3f4387c80698d018a69798b6228>`__)\n- Fixed a bug where ``sqlite_timelimit()`` occasionally failed to clean up after itself (`bac4e01 <https://github.com/simonw/datasette/commit/bac4e01f40ae7bd19d1eab1fb9349452c18de8f5>`__)\n- We no longer load additional plugins when executing pytest (:issue:`438`)\n- Homepage now links to database views if there are less than five tables in a database (:issue:`373`)\n- The ``--cors`` option is now respected by error pages (:issue:`453`)\n- ``datasette publish heroku`` now uses the ``--include-vcs-ignore`` option, which means it works under Travis CI (`#407 <https://github.com/simonw/datasette/pull/407>`__)\n- ``datasette publish heroku`` now publishes using Python 3.6.8 (`666c374 <https://github.com/simonw/datasette/commit/666c37415a898949fae0437099d62a35b1e9c430>`__)\n- Renamed ``datasette publish now`` to ``datasette publish nowv1`` (:issue:`472`)\n- ``datasette publish nowv1`` now accepts multiple ``--alias`` parameters (`09ef305 <https://github.com/simonw/datasette/commit/09ef305c687399384fe38487c075e8669682deb4>`__)\n- Removed the ``datasette skeleton`` command (:issue:`476`)\n- The :ref:`documentation on how to build the documentation <contributing_documentation>` now recommends ``sphinx-autobuild``\n\n.. _v0_27_1:\n\n0.27.1 (2019-05-09)\n-------------------\n\n- Tiny bugfix release: don't install ``tests/`` in the wrong place. Thanks, Veit Heller.\n\n.. _v0_27:\n\n0.27 (2019-01-31)\n-----------------\n\n- New command: ``datasette plugins`` (:ref:`documentation <plugins_installed>`) shows you the currently installed list of plugins.\n- Datasette can now output `newline-delimited JSON <http://ndjson.org/>`__ using the new ``?_shape=array&_nl=on`` query string option.\n- Added documentation on :ref:`ecosystem`.\n- Now using Python 3.7.2 as the base for the official `Datasette Docker image <https://hub.docker.com/r/datasetteproject/datasette/>`__.\n\n.. _v0_26_1:\n\n0.26.1 (2019-01-10)\n-------------------\n\n- ``/-/versions`` now includes SQLite ``compile_options`` (:issue:`396`)\n- `datasetteproject/datasette <https://hub.docker.com/r/datasetteproject/datasette>`__ Docker image now uses SQLite 3.26.0 (:issue:`397`)\n- Cleaned up some deprecation warnings under Python 3.7\n\n.. _v0_26:\n\n0.26 (2019-01-02)\n-----------------\n\n- ``datasette serve --reload`` now restarts Datasette if a database file changes on disk.\n- ``datasette publish now`` now takes an optional ``--alias mysite.now.sh`` argument. This will attempt to set an alias after the deploy completes.\n- Fixed a bug where the advanced CSV export form failed to include the currently selected filters (:issue:`393`)\n\n.. _v0_25_2:\n\n0.25.2 (2018-12-16)\n-------------------\n\n- ``datasette publish heroku`` now uses the ``python-3.6.7`` runtime\n- Added documentation on :ref:`how to build the documentation <contributing_documentation>`\n- Added documentation covering :ref:`our release process <contributing_release>`\n- Upgraded to pytest 4.0.2\n\n.. _v0_25_1:\n\n0.25.1 (2018-11-04)\n-------------------\n\nDocumentation improvements plus a fix for publishing to Zeit Now.\n\n- ``datasette publish now`` now uses Zeit's v1 platform, to work around the new 100MB image limit. Thanks, @slygent - closes :issue:`366`.\n\n.. _v0_25:\n\n0.25 (2018-09-19)\n-----------------\n\nNew plugin hooks, improved database view support and an easier way to use more recent versions of SQLite.\n\n- New ``publish_subcommand`` plugin hook. A plugin can now add additional ``datasette publish`` publishers in addition to the default ``now`` and ``heroku``, both of which have been refactored into default plugins. :ref:`publish_subcommand documentation <plugin_hook_publish_subcommand>`. Closes :issue:`349`\n- New ``render_cell`` plugin hook. Plugins can now customize how values are displayed in the HTML tables produced by Datasette's browsable interface. `datasette-json-html <https://github.com/simonw/datasette-json-html>`__ and `datasette-render-images <https://github.com/simonw/datasette-render-images>`__ are two new plugins that use this hook. :ref:`render_cell documentation <plugin_hook_render_cell>`. Closes :issue:`352`\n- New ``extra_body_script`` plugin hook, enabling plugins to provide additional JavaScript that should be added to the page footer. :ref:`extra_body_script documentation <plugin_hook_extra_body_script>`.\n- ``extra_css_urls`` and ``extra_js_urls`` hooks now take additional optional parameters, allowing them to be more selective about which pages they apply to. :ref:`Documentation <plugin_hook_extra_css_urls>`.\n- You can now use the :ref:`sortable_columns metadata setting <metadata_sortable_columns>` to explicitly enable sort-by-column in the interface for database views, as well as for specific tables.\n- The new ``fts_table`` and ``fts_pk`` metadata settings can now be used to :ref:`explicitly configure full-text search for a table or a view <full_text_search_table_or_view>`, even if that table is not directly coupled to the SQLite FTS feature in the database schema itself.\n- Datasette will now use `pysqlite3 <https://github.com/coleifer/pysqlite3>`__ in place of the standard library ``sqlite3`` module if it has been installed in the current environment. This makes it much easier to run Datasette against a more recent version of SQLite, including the just-released `SQLite 3.25.0 <https://www.sqlite.org/releaselog/3_25_0.html>`__ which adds window function support. More details on how to use this in :issue:`360`\n- New mechanism that allows :ref:`plugin configuration options <plugins_configuration>` to be set using ``metadata.json``.\n\n\n.. _v0_24:\n\n0.24 (2018-07-23)\n-----------------\n\nA number of small new features:\n\n- ``datasette publish heroku`` now supports ``--extra-options``, fixes `#334 <https://github.com/simonw/datasette/issues/334>`_\n- Custom error message if SpatiaLite is needed for specified database, closes `#331 <https://github.com/simonw/datasette/issues/331>`_\n- New config option: ``truncate_cells_html`` for :ref:`truncating long cell values <setting_truncate_cells_html>` in HTML view - closes `#330 <https://github.com/simonw/datasette/issues/330>`_\n- Documentation for :ref:`datasette publish and datasette package <publishing>`, closes `#337 <https://github.com/simonw/datasette/issues/337>`_\n- Fixed compatibility with Python 3.7\n- ``datasette publish heroku`` now supports app names via the ``-n`` option, which can also be used to overwrite an existing application [Russ Garrett]\n- Title and description metadata can now be set for :ref:`canned SQL queries <canned_queries>`, closes `#342 <https://github.com/simonw/datasette/issues/342>`_\n- New ``force_https_on`` config option, fixes ``https://`` API URLs when deploying to Zeit Now - closes `#333 <https://github.com/simonw/datasette/issues/333>`_\n- ``?_json_infinity=1`` query string argument for handling Infinity/-Infinity values in JSON, closes `#332 <https://github.com/simonw/datasette/issues/332>`_\n- URLs displayed in the results of custom SQL queries are now URLified, closes `#298 <https://github.com/simonw/datasette/issues/298>`_\n\n.. _v0_23_2:\n\n0.23.2 (2018-07-07)\n-------------------\n\nMinor bugfix and documentation release.\n\n- CSV export now respects ``--cors``, fixes `#326 <https://github.com/simonw/datasette/issues/326>`_\n- :ref:`Installation instructions <installation>`, including docker image - closes `#328 <https://github.com/simonw/datasette/issues/328>`_\n- Fix for row pages for tables with / in, closes `#325 <https://github.com/simonw/datasette/issues/325>`_\n\n.. _v0_23_1:\n\n0.23.1 (2018-06-21)\n-------------------\n\nMinor bugfix release.\n\n- Correctly display empty strings in HTML table, closes `#314 <https://github.com/simonw/datasette/issues/314>`_\n- Allow \".\" in database filenames, closes `#302 <https://github.com/simonw/datasette/issues/302>`_\n- 404s ending in slash redirect to remove that slash, closes `#309 <https://github.com/simonw/datasette/issues/309>`_\n- Fixed incorrect display of compound primary keys with foreign key\n  references. Closes `#319 <https://github.com/simonw/datasette/issues/319>`_\n- Docs + example of canned SQL query using || concatenation. Closes `#321 <https://github.com/simonw/datasette/issues/321>`_\n- Correctly display facets with value of 0 - closes `#318 <https://github.com/simonw/datasette/issues/318>`_\n- Default 'expand labels' to checked in CSV advanced export\n\n.. _v0_23:\n\n0.23 (2018-06-18)\n-----------------\n\nThis release features CSV export, improved options for foreign key expansions,\nnew configuration settings and improved support for SpatiaLite.\n\nSee `datasette/compare/0.22.1...0.23\n<https://github.com/simonw/datasette/compare/0.22.1...0.23>`_ for a full list of\ncommits added since the last release.\n\nCSV export\n~~~~~~~~~~\n\nAny Datasette table, view or custom SQL query can now be exported as CSV.\n\n.. image:: https://github.com/simonw/datasette-screenshots/blob/0.62/advanced-export.png?raw=true\n   :alt: Advanced export form. You can get the data in different JSON shapes, and CSV options are download file, expand labels and stream all rows.\n\nCheck out the :ref:`CSV export documentation <csv_export>` for more details, or\ntry the feature out on\nhttps://fivethirtyeight.datasettes.com/fivethirtyeight/bechdel%2Fmovies\n\nIf your table has more than :ref:`setting_max_returned_rows` (default 1,000)\nDatasette provides the option to *stream all rows*. This option takes advantage\nof async Python and Datasette's efficient :ref:`pagination <pagination>` to\niterate through the entire matching result set and stream it back as a\ndownloadable CSV file.\n\nForeign key expansions\n~~~~~~~~~~~~~~~~~~~~~~\n\nWhen Datasette detects a foreign key reference it attempts to resolve a label\nfor that reference (automatically or using the :ref:`label_columns` metadata\noption) so it can display a link to the associated row.\n\nThis expansion is now also available for JSON and CSV representations of the\ntable, using the new ``_labels=on`` query string option. See\n:ref:`expand_foreign_keys` for more details.\n\nNew configuration settings\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDatasette's :ref:`settings` now also supports boolean settings. A number of new\nconfiguration options have been added:\n\n* ``num_sql_threads`` - the number of threads used to execute SQLite queries. Defaults to 3.\n* ``allow_facet`` - enable or disable custom :ref:`facets` using the `_facet=` parameter. Defaults to on.\n* ``suggest_facets`` - should Datasette suggest facets? Defaults to on.\n* ``allow_download`` - should users be allowed to download the entire SQLite database? Defaults to on.\n* ``allow_sql`` - should users be allowed to execute custom SQL queries? Defaults to on.\n* ``default_cache_ttl`` - Default HTTP caching max-age header in seconds. Defaults to 365 days - caching can be disabled entirely by settings this to 0.\n* ``cache_size_kb`` - Set the amount of memory SQLite uses for its `per-connection cache <https://www.sqlite.org/pragma.html#pragma_cache_size>`_, in KB.\n* ``allow_csv_stream`` - allow users to stream entire result sets as a single CSV file. Defaults to on.\n* ``max_csv_mb`` - maximum size of a returned CSV file in MB. Defaults to 100MB, set to 0 to disable this limit.\n\nControl HTTP caching with ?_ttl=\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can now customize the HTTP max-age header that is sent on a per-URL basis, using the new ``?_ttl=`` query string parameter.\n\nYou can set this to any value in seconds, or you can set it to 0 to disable HTTP caching entirely.\n\nConsider for example this query which returns a randomly selected member of the Avengers::\n\n    select * from [avengers/avengers] order by random() limit 1\n\nIf you hit the following page repeatedly you will get the same result, due to HTTP caching:\n\n`/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random%28%29+limit+1 <https://fivethirtyeight.datasettes.com/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random%28%29+limit+1>`_\n\nBy adding `?_ttl=0` to the zero you can ensure the page will not be cached and get back a different super hero every time:\n\n`/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random%28%29+limit+1&_ttl=0 <https://fivethirtyeight.datasettes.com/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random%28%29+limit+1&_ttl=0>`_\n\nImproved support for SpatiaLite\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe `SpatiaLite module <https://www.gaia-gis.it/fossil/libspatialite/index>`_\nfor SQLite adds robust geospatial features to the database.\n\nGetting SpatiaLite working can be tricky, especially if you want to use the most\nrecent alpha version (with support for K-nearest neighbor).\n\nDatasette now includes :ref:`extensive documentation on SpatiaLite\n<spatialite>`, and thanks to `Ravi Kotecha <https://github.com/r4vi>`_ our GitHub\nrepo includes a `Dockerfile\n<https://github.com/simonw/datasette/blob/master/Dockerfile>`_ that can build\nthe latest SpatiaLite and configure it for use with Datasette.\n\nThe ``datasette publish`` and ``datasette package`` commands now accept a new\n``--spatialite`` argument which causes them to install and configure SpatiaLite\nas part of the container they deploy.\n\nlatest.datasette.io\n~~~~~~~~~~~~~~~~~~~\n\nEvery commit to Datasette master is now automatically deployed by Travis CI to\nhttps://latest.datasette.io/ - ensuring there is always a live demo of the\nlatest version of the software.\n\nThe demo uses `the fixtures\n<https://github.com/simonw/datasette/blob/master/tests/fixtures.py>`_ from our\nunit tests, ensuring it demonstrates the same range of functionality that is\ncovered by the tests.\n\nYou can see how the deployment mechanism works in our `.travis.yml\n<https://github.com/simonw/datasette/blob/master/.travis.yml>`_ file.\n\nMiscellaneous\n~~~~~~~~~~~~~\n\n* Got JSON data in one of your columns? Use the new ``?_json=COLNAME`` argument\n  to tell Datasette to return that JSON value directly rather than encoding it\n  as a string.\n* If you just want an array of the first value of each row, use the new\n  ``?_shape=arrayfirst`` option - `example\n  <https://latest.datasette.io/fixtures.json?sql=select+neighborhood+from+facetable+order+by+pk+limit+101&_shape=arrayfirst>`_.\n\n0.22.1 (2018-05-23)\n-------------------\n\nBugfix release, plus we now use `versioneer <https://github.com/warner/python-versioneer>`_ for our version numbers.\n\n- Faceting no longer breaks pagination, fixes `#282 <https://github.com/simonw/datasette/issues/282>`_\n- Add ``__version_info__`` derived from `__version__` [Robert Gieseke]\n\n  This might be tuple of more than two values (major and minor\n  version) if commits have been made after a release.\n- Add version number support with Versioneer. [Robert Gieseke]\n\n  Versioneer Licence:\n  Public Domain (CC0-1.0)\n\n  Closes `#273 <https://github.com/simonw/datasette/issues/273>`_\n- Refactor inspect logic [Russ Garrett]\n\n0.22 (2018-05-20)\n-----------------\n\nThe big new feature in this release is :ref:`facets`. Datasette can now apply faceted browse to any column in any table. It will also suggest possible facets. See the `Datasette Facets <https://simonwillison.net/2018/May/20/datasette-facets/>`_ announcement post for more details.\n\nIn addition to the work on facets:\n\n- Added `docs for introspection endpoints <https://docs.datasette.io/en/stable/introspection.html>`_\n\n- New ``--config`` option, added ``--help-config``, closes `#274 <https://github.com/simonw/datasette/issues/274>`_\n\n  Removed the ``--page_size=`` argument to ``datasette serve`` in favour of::\n\n      datasette serve --config default_page_size:50 mydb.db\n\n  Added new help section::\n\n      datasette --help-config\n\n  ::\n\n      Config options:\n        default_page_size            Default page size for the table view\n                                     (default=100)\n        max_returned_rows            Maximum rows that can be returned from a table\n                                     or custom query (default=1000)\n        sql_time_limit_ms            Time limit for a SQL query in milliseconds\n                                     (default=1000)\n        default_facet_size           Number of values to return for requested facets\n                                     (default=30)\n        facet_time_limit_ms          Time limit for calculating a requested facet\n                                     (default=200)\n        facet_suggest_time_limit_ms  Time limit for calculating a suggested facet\n                                     (default=50)\n- Only apply responsive table styles to ``.rows-and-column``\n\n  Otherwise they interfere with tables in the description, e.g. on\n  https://fivethirtyeight.datasettes.com/fivethirtyeight/nba-elo%2Fnbaallelo\n\n- Refactored views into new ``views/`` modules, refs `#256 <https://github.com/simonw/datasette/issues/256>`_\n- `Documentation for SQLite full-text search <https://docs.datasette.io/en/stable/full_text_search.html>`_ support, closes `#253 <https://github.com/simonw/datasette/issues/253>`_\n- ``/-/versions`` now includes SQLite ``fts_versions``, closes `#252 <https://github.com/simonw/datasette/issues/252>`_\n\n0.21 (2018-05-05)\n-----------------\n\nNew JSON ``_shape=`` options, the ability to set table ``_size=`` and a mechanism for searching within specific columns.\n\n- Default tests to using a longer timelimit\n\n  Every now and then a test will fail in Travis CI on Python 3.5 because it hit\n  the default 20ms SQL time limit.\n\n  Test fixtures now default to a 200ms time limit, and we only use the 20ms time\n  limit for the specific test that tests query interruption. This should make\n  our tests on Python 3.5 in Travis much more stable.\n- Support ``_search_COLUMN=text`` searches, closes `#237 <https://github.com/simonw/datasette/issues/237>`_\n- Show version on ``/-/plugins`` page, closes `#248 <https://github.com/simonw/datasette/issues/248>`_\n- ``?_size=max`` option, closes `#249 <https://github.com/simonw/datasette/issues/249>`_\n- Added ``/-/versions`` and ``/-/versions.json``, closes `#244 <https://github.com/simonw/datasette/issues/244>`_\n\n  Sample output::\n\n      {\n        \"python\": {\n          \"version\": \"3.6.3\",\n          \"full\": \"3.6.3 (default, Oct  4 2017, 06:09:38) \\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)]\"\n        },\n        \"datasette\": {\n          \"version\": \"0.20\"\n        },\n        \"sqlite\": {\n          \"version\": \"3.23.1\",\n          \"extensions\": {\n            \"json1\": null,\n            \"spatialite\": \"4.3.0a\"\n          }\n        }\n      }\n- Renamed ``?_sql_time_limit_ms=`` to ``?_timelimit``, closes `#242 <https://github.com/simonw/datasette/issues/242>`_\n- New ``?_shape=array`` option + tweaks to ``_shape``, closes `#245 <https://github.com/simonw/datasette/issues/245>`_\n\n  * Default is now ``?_shape=arrays`` (renamed from ``lists``)\n  * New ``?_shape=array`` returns an array of objects as the root object\n  * Changed ``?_shape=object`` to return the object as the root\n  * Updated docs\n\n- FTS tables now detected by ``inspect()``, closes `#240 <https://github.com/simonw/datasette/issues/240>`_\n- New ``?_size=XXX`` query string parameter for table view, closes `#229 <https://github.com/simonw/datasette/issues/229>`_\n\n  Also added documentation for all of the ``_special`` arguments.\n\n  Plus deleted some duplicate logic implementing ``_group_count``.\n- If ``max_returned_rows==page_size``, increment ``max_returned_rows`` - fixes `#230 <https://github.com/simonw/datasette/issues/230>`_\n- New ``hidden: True`` option for table metadata, closes `#239 <https://github.com/simonw/datasette/issues/239>`_\n- Hide ``idx_*`` tables if spatialite detected, closes `#228 <https://github.com/simonw/datasette/issues/228>`_\n- Added ``class=rows-and-columns`` to custom query results table\n- Added CSS class ``rows-and-columns`` to main table\n- ``label_column`` option in ``metadata.json`` - closes `#234 <https://github.com/simonw/datasette/issues/234>`_\n\n0.20 (2018-04-20)\n-----------------\n\nMostly new work on the :ref:`plugins` mechanism: plugins can now bundle static assets and custom templates, and ``datasette publish`` has a new ``--install=name-of-plugin`` option.\n\n- Add col-X classes to HTML table on custom query page\n- Fixed out-dated template in documentation\n- Plugins can now bundle custom templates, `#224 <https://github.com/simonw/datasette/issues/224>`_\n- Added /-/metadata /-/plugins /-/inspect, `#225 <https://github.com/simonw/datasette/issues/225>`_\n- Documentation for --install option, refs `#223 <https://github.com/simonw/datasette/issues/223>`_\n- Datasette publish/package --install option, `#223 <https://github.com/simonw/datasette/issues/223>`_\n- Fix for plugins in Python 3.5, `#222 <https://github.com/simonw/datasette/issues/222>`_\n- New plugin hooks: extra_css_urls() and extra_js_urls(), `#214 <https://github.com/simonw/datasette/issues/214>`_\n- /-/static-plugins/PLUGIN_NAME/ now serves static/ from plugins\n- <th> now gets class=\"col-X\" - plus added col-X documentation\n- Use to_css_class for table cell column classes\n\n  This ensures that columns with spaces in the name will still\n  generate usable CSS class names. Refs `#209 <https://github.com/simonw/datasette/issues/209>`_\n- Add column name classes to <td>s, make PK bold [Russ Garrett]\n- Don't duplicate simple primary keys in the link column [Russ Garrett]\n\n  When there's a simple (single-column) primary key, it looks weird to\n  duplicate it in the link column.\n\n  This change removes the second PK column and treats the link column as\n  if it were the PK column from a header/sorting perspective.\n- Correct escaping for HTML display of row links [Russ Garrett]\n- Longer time limit for test_paginate_compound_keys\n\n  It was failing intermittently in Travis - see `#209 <https://github.com/simonw/datasette/issues/209>`_\n- Use application/octet-stream for downloadable databases\n- Updated PyPI classifiers\n- Updated PyPI link to pypi.org\n\n0.19 (2018-04-16)\n-----------------\n\nThis is the first preview of the new Datasette plugins mechanism. Only two\nplugin hooks are available so far - for custom SQL functions and custom template\nfilters. There's plenty more to come - read `the documentation\n<https://docs.datasette.io/en/stable/plugins.html>`_ and get involved in\n`the tracking ticket <https://github.com/simonw/datasette/issues/14>`_ if you\nhave feedback on the direction so far.\n\n- Fix for ``_sort_desc=sortable_with_nulls`` test, refs `#216 <https://github.com/simonw/datasette/issues/216>`_\n\n- Fixed `#216 <https://github.com/simonw/datasette/issues/216>`_ - paginate correctly when sorting by nullable column\n\n- Initial documentation for plugins, closes `#213 <https://github.com/simonw/datasette/issues/213>`_\n\n  https://docs.datasette.io/en/stable/plugins.html\n\n- New ``--plugins-dir=plugins/`` option (`#212 <https://github.com/simonw/datasette/issues/212>`_)\n\n  New option causing Datasette to load and evaluate all of the Python files in\n  the specified directory and register any plugins that are defined in those\n  files.\n\n  This new option is available for the following commands::\n\n      datasette serve mydb.db --plugins-dir=plugins/\n      datasette publish now/heroku mydb.db --plugins-dir=plugins/\n      datasette package mydb.db --plugins-dir=plugins/\n\n- Start of the plugin system, based on pluggy (`#210 <https://github.com/simonw/datasette/issues/14>`_)\n\n  Uses https://pluggy.readthedocs.io/ originally created for the py.test project\n\n  We're starting with two plugin hooks:\n\n  ``prepare_connection(conn)``\n\n  This is called when a new SQLite connection is created. It can be used to register custom SQL functions.\n\n  ``prepare_jinja2_environment(env)``\n\n  This is called with the Jinja2 environment. It can be used to register custom template tags and filters.\n\n  An example plugin which uses these two hooks can be found at https://github.com/simonw/datasette-plugin-demos or installed using ``pip install datasette-plugin-demos``\n\n  Refs `#14 <https://github.com/simonw/datasette/issues/14>`_\n\n- Return HTTP 405 on InvalidUsage rather than 500. [Russ Garrett]\n\n  This also stops it filling up the logs. This happens for HEAD requests\n  at the moment - which perhaps should be handled better, but that's a\n  different issue.\n\n\n0.18 (2018-04-14)\n-----------------\n\nThis release introduces `support for units <https://docs.datasette.io/en/stable/metadata.html#specifying-units-for-a-column>`_,\ncontributed by Russ Garrett (`#203 <https://github.com/simonw/datasette/issues/203>`_).\nYou can now optionally specify the units for specific columns using ``metadata.json``.\nOnce specified, units will be displayed in the HTML view of your table. They also become\navailable for use in filters - if a column is configured with a unit of distance, you can\nrequest all rows where that column is less than 50 meters or more than 20 feet for example.\n\n- Link foreign keys which don't have labels. [Russ Garrett]\n\n  This renders unlabeled FKs as simple links.\n\n  Also includes bonus fixes for two minor issues:\n\n  * In foreign key link hrefs the primary key was escaped using HTML\n    escaping rather than URL escaping. This broke some non-integer PKs.\n  * Print tracebacks to console when handling 500 errors.\n\n- Fix SQLite error when loading rows with no incoming FKs. [Russ\n  Garrett]\n\n  This fixes an error caused by an invalid query when loading incoming FKs.\n\n  The error was ignored due to async but it still got printed to the\n  console.\n\n- Allow custom units to be registered with Pint. [Russ Garrett]\n- Support units in filters. [Russ Garrett]\n- Tidy up units support. [Russ Garrett]\n\n  * Add units to exported JSON\n  * Units key in metadata skeleton\n  * Docs\n\n- Initial units support. [Russ Garrett]\n\n  Add support for specifying units for a column in ``metadata.json`` and\n  rendering them on display using\n  `pint <https://pint.readthedocs.io/en/latest/>`_\n\n\n0.17 (2018-04-13)\n-----------------\n- Release 0.17 to fix issues with PyPI\n\n\n0.16 (2018-04-13)\n-----------------\n- Better mechanism for handling errors; 404s for missing table/database\n\n  New error mechanism closes `#193 <https://github.com/simonw/datasette/issues/193>`_\n\n  404s for missing tables/databases closes `#184 <https://github.com/simonw/datasette/issues/184>`_\n\n- long_description in markdown for the new PyPI\n- Hide SpatiaLite system tables. [Russ Garrett]\n- Allow ``explain select`` / ``explain query plan select`` `#201 <https://github.com/simonw/datasette/issues/201>`_\n- Datasette inspect now finds primary_keys `#195 <https://github.com/simonw/datasette/issues/195>`_\n- Ability to sort using form fields (for mobile portrait mode) `#199 <https://github.com/simonw/datasette/issues/199>`_\n\n  We now display sort options as a select box plus a descending checkbox, which\n  means you can apply sort orders even in portrait mode on a mobile phone where\n  the column headers are hidden.\n\n0.15 (2018-04-09)\n-----------------\n\nThe biggest new feature in this release is the ability to sort by column. On the\ntable page the column headers can now be clicked to apply sort (or descending\nsort), or you can specify ``?_sort=column`` or ``?_sort_desc=column`` directly\nin the URL.\n\n- ``table_rows`` => ``table_rows_count``, ``filtered_table_rows`` =>\n  ``filtered_table_rows_count``\n\n  Renamed properties. Closes `#194 <https://github.com/simonw/datasette/issues/194>`_\n\n- New ``sortable_columns`` option in ``metadata.json`` to control sort options.\n\n  You can now explicitly set which columns in a table can be used for sorting\n  using the ``_sort`` and ``_sort_desc`` arguments using ``metadata.json``::\n\n      {\n          \"databases\": {\n              \"database1\": {\n                  \"tables\": {\n                      \"example_table\": {\n                          \"sortable_columns\": [\n                              \"height\",\n                              \"weight\"\n                          ]\n                      }\n                  }\n              }\n          }\n      }\n\n  Refs `#189 <https://github.com/simonw/datasette/issues/189>`_\n\n- Column headers now link to sort/desc sort - refs `#189 <https://github.com/simonw/datasette/issues/189>`_\n\n- ``_sort`` and ``_sort_desc`` parameters for table views\n\n  Allows for paginated sorted results based on a specified column.\n\n  Refs `#189 <https://github.com/simonw/datasette/issues/189>`_\n\n- Total row count now correct even if ``_next`` applied\n\n- Use .custom_sql() for _group_count implementation (refs `#150 <https://github.com/simonw/datasette/issues/150>`_)\n\n- Make HTML title more readable in query template (`#180 <https://github.com/simonw/datasette/issues/180>`_) [Ryan Pitts]\n\n- New ``?_shape=objects/object/lists`` param for JSON API (`#192 <https://github.com/simonw/datasette/issues/192>`_)\n\n  New ``_shape=`` parameter replacing old ``.jsono`` extension\n\n  Now instead of this::\n\n      /database/table.jsono\n\n  We use the ``_shape`` parameter like this::\n\n      /database/table.json?_shape=objects\n\n  Also introduced a new ``_shape`` called ``object`` which looks like this::\n\n      /database/table.json?_shape=object\n\n  Returning an object for the rows key::\n\n      ...\n      \"rows\": {\n          \"pk1\": {\n              ...\n          },\n          \"pk2\": {\n              ...\n          }\n      }\n\n  Refs `#122 <https://github.com/simonw/datasette/issues/122>`_\n\n- Utility for writing test database fixtures to a .db file\n\n  ``python tests/fixtures.py /tmp/hello.db``\n\n  This is useful for making a SQLite database of the test fixtures for\n  interactive exploration.\n\n- Compound primary key ``_next=`` now plays well with extra filters\n\n  Closes `#190 <https://github.com/simonw/datasette/issues/190>`_\n\n- Fixed bug with keyset pagination over compound primary keys\n\n  Refs `#190 <https://github.com/simonw/datasette/issues/190>`_\n\n- Database/Table views inherit ``source/license/source_url/license_url``\n  metadata\n\n  If you set the ``source_url/license_url/source/license`` fields in your root\n  metadata those values will now be inherited all the way down to the database\n  and table templates.\n\n  The ``title/description`` are NOT inherited.\n\n  Also added unit tests for the HTML generated by the metadata.\n\n  Refs `#185 <https://github.com/simonw/datasette/issues/185>`_\n\n- Add metadata, if it exists, to heroku temp dir (`#178 <https://github.com/simonw/datasette/issues/178>`_) [Tony Hirst]\n- Initial documentation for pagination\n- Broke up test_app into test_api and test_html\n- Fixed bug with .json path regular expression\n\n  I had a table called ``geojson`` and it caused an exception because the regex\n  was matching ``.json`` and not ``\\.json``\n\n- Deploy to Heroku with Python 3.6.3\n\n0.14 (2017-12-09)\n-----------------\n\nThe theme of this release is customization: Datasette now allows every aspect\nof its presentation `to be customized <https://docs.datasette.io/en/stable/custom_templates.html>`_\neither using additional CSS or by providing entirely new templates.\n\nDatasette's `metadata.json format <https://docs.datasette.io/en/stable/metadata.html>`_\nhas also been expanded, to allow per-database and per-table metadata. A new\n``datasette skeleton`` command can be used to generate a skeleton JSON file\nready to be filled in with per-database and per-table details.\n\nThe ``metadata.json`` file can also be used to define\n`canned queries <https://docs.datasette.io/en/stable/sql_queries.html#canned-queries>`_,\nas a more powerful alternative to SQL views.\n\n- ``extra_css_urls``/``extra_js_urls`` in metadata\n\n  A mechanism in the ``metadata.json`` format for adding custom CSS and JS urls.\n\n  Create a ``metadata.json`` file that looks like this::\n\n      {\n          \"extra_css_urls\": [\n              \"https://simonwillison.net/static/css/all.bf8cd891642c.css\"\n          ],\n          \"extra_js_urls\": [\n              \"https://code.jquery.com/jquery-3.2.1.slim.min.js\"\n          ]\n      }\n\n  Then start datasette like this::\n\n      datasette mydb.db --metadata=metadata.json\n\n  The CSS and JavaScript files will be linked in the ``<head>`` of every page.\n\n  You can also specify a SRI (subresource integrity hash) for these assets::\n\n      {\n          \"extra_css_urls\": [\n              {\n                  \"url\": \"https://simonwillison.net/static/css/all.bf8cd891642c.css\",\n                  \"sri\": \"sha384-9qIZekWUyjCyDIf2YK1FRoKiPJq4PHt6tp/ulnuuyRBvazd0hG7pWbE99zvwSznI\"\n              }\n          ],\n          \"extra_js_urls\": [\n              {\n                  \"url\": \"https://code.jquery.com/jquery-3.2.1.slim.min.js\",\n                  \"sri\": \"sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g=\"\n              }\n          ]\n      }\n\n  Modern browsers will only execute the stylesheet or JavaScript if the SRI hash\n  matches the content served. You can generate hashes using https://www.srihash.org/\n\n- Auto-link column values that look like URLs (`#153 <https://github.com/simonw/datasette/issues/153>`_)\n\n- CSS styling hooks as classes on the body (`#153 <https://github.com/simonw/datasette/issues/153>`_)\n\n  Every template now gets CSS classes in the body designed to support custom\n  styling.\n\n  The index template (the top level page at ``/``) gets this::\n\n      <body class=\"index\">\n\n  The database template (``/dbname/``) gets this::\n\n      <body class=\"db db-dbname\">\n\n  The table template (``/dbname/tablename``) gets::\n\n      <body class=\"table db-dbname table-tablename\">\n\n  The row template (``/dbname/tablename/rowid``) gets::\n\n      <body class=\"row db-dbname table-tablename\">\n\n  The ``db-x`` and ``table-x`` classes use the database or table names themselves IF\n  they are valid CSS identifiers. If they aren't, we strip any invalid\n  characters out and append a 6 character md5 digest of the original name, in\n  order to ensure that multiple tables which resolve to the same stripped\n  character version still have different CSS classes.\n\n  Some examples (extracted from the unit tests)::\n\n      \"simple\" => \"simple\"\n      \"MixedCase\" => \"MixedCase\"\n      \"-no-leading-hyphens\" => \"no-leading-hyphens-65bea6\"\n      \"_no-leading-underscores\" => \"no-leading-underscores-b921bc\"\n      \"no spaces\" => \"no-spaces-7088d7\"\n      \"-\" => \"336d5e\"\n      \"no $ characters\" => \"no--characters-59e024\"\n\n- ``datasette --template-dir=mytemplates/`` argument\n\n  You can now pass an additional argument specifying a directory to look for\n  custom templates in.\n\n  Datasette will fall back on the default templates if a template is not\n  found in that directory.\n\n- Ability to over-ride templates for individual tables/databases.\n\n  It is now possible to over-ride templates on a per-database / per-row or per-\n  table basis.\n\n  When you access e.g. ``/mydatabase/mytable`` Datasette will look for the following::\n\n      - table-mydatabase-mytable.html\n      - table.html\n\n  If you provided a ``--template-dir`` argument to datasette serve it will look in\n  that directory first.\n\n  The lookup rules are as follows::\n\n      Index page (/):\n          index.html\n\n      Database page (/mydatabase):\n          database-mydatabase.html\n          database.html\n\n      Table page (/mydatabase/mytable):\n          table-mydatabase-mytable.html\n          table.html\n\n      Row page (/mydatabase/mytable/id):\n          row-mydatabase-mytable.html\n          row.html\n\n  If a table name has spaces or other unexpected characters in it, the template\n  filename will follow the same rules as our custom ``<body>`` CSS classes\n  - for example, a table called \"Food Trucks\"\n  will attempt to load the following templates::\n\n      table-mydatabase-Food-Trucks-399138.html\n      table.html\n\n  It is possible to extend the default templates using Jinja template\n  inheritance. If you want to customize EVERY row template with some additional\n  content you can do so by creating a row.html template like this::\n\n      {% extends \"default:row.html\" %}\n\n      {% block content %}\n      <h1>EXTRA HTML AT THE TOP OF THE CONTENT BLOCK</h1>\n      <p>This line renders the original block:</p>\n      {{ super() }}\n      {% endblock %}\n\n- ``--static`` option for datasette serve (`#160 <https://github.com/simonw/datasette/issues/160>`_)\n\n  You can now tell Datasette to serve static files from a specific location at a\n  specific mountpoint.\n\n  For example::\n\n    datasette serve mydb.db --static extra-css:/tmp/static/css\n\n  Now if you visit this URL::\n\n    http://localhost:8001/extra-css/blah.css\n\n  The following file will be served::\n\n    /tmp/static/css/blah.css\n\n- Canned query support.\n\n  Named canned queries can now be defined in ``metadata.json`` like this::\n\n      {\n          \"databases\": {\n              \"timezones\": {\n                  \"queries\": {\n                      \"timezone_for_point\": \"select tzid from timezones ...\"\n                  }\n              }\n          }\n      }\n\n  These will be shown in a new \"Queries\" section beneath \"Views\" on the database page.\n\n- New ``datasette skeleton`` command for generating ``metadata.json`` (`#164 <https://github.com/simonw/datasette/issues/164>`_)\n\n- ``metadata.json`` support for per-table/per-database metadata (`#165 <https://github.com/simonw/datasette/issues/165>`_)\n\n  Also added support for descriptions and HTML descriptions.\n\n  Here's an example metadata.json file illustrating custom per-database and per-\n  table metadata::\n\n      {\n          \"title\": \"Overall datasette title\",\n          \"description_html\": \"This is a <em>description with HTML</em>.\",\n          \"databases\": {\n              \"db1\": {\n                  \"title\": \"First database\",\n                  \"description\": \"This is a string description & has no HTML\",\n                  \"license_url\": \"http://example.com/\",\n              \"license\": \"The example license\",\n                  \"queries\": {\n                    \"canned_query\": \"select * from table1 limit 3;\"\n                  },\n                  \"tables\": {\n                      \"table1\": {\n                          \"title\": \"Custom title for table1\",\n                          \"description\": \"Tables can have descriptions too\",\n                          \"source\": \"This has a custom source\",\n                          \"source_url\": \"http://example.com/\"\n                      }\n                  }\n              }\n          }\n      }\n\n- Renamed ``datasette build`` command to ``datasette inspect`` (`#130 <https://github.com/simonw/datasette/issues/130>`_)\n\n- Upgrade to Sanic 0.7.0 (`#168 <https://github.com/simonw/datasette/issues/168>`_)\n\n  https://github.com/channelcat/sanic/releases/tag/0.7.0\n\n- Package and publish commands now accept ``--static`` and ``--template-dir``\n\n  Example usage::\n\n      datasette package --static css:extra-css/ --static js:extra-js/ \\\n        sf-trees.db --template-dir templates/ --tag sf-trees --branch master\n\n  This creates a local Docker image that includes copies of the templates/,\n  extra-css/ and extra-js/ directories. You can then run it like this::\n\n    docker run -p 8001:8001 sf-trees\n\n  For publishing to Zeit now::\n\n    datasette publish now --static css:extra-css/ --static js:extra-js/ \\\n      sf-trees.db --template-dir templates/ --name sf-trees --branch master\n\n- HTML comment showing which templates were considered for a page (`#171 <https://github.com/simonw/datasette/issues/171>`_)\n\n0.13 (2017-11-24)\n-----------------\n- Search now applies to current filters.\n\n  Combined search into the same form as filters.\n\n  Closes `#133`_\n\n- Much tidier design for table view header.\n\n  Closes `#147`_\n\n- Added ``?column__not=blah`` filter.\n\n  Closes `#148`_\n\n- Row page now resolves foreign keys.\n\n  Closes `#132`_\n\n- Further tweaks to select/input filter styling.\n\n  Refs `#86`_ - thanks for the help, @natbat!\n\n- Show linked foreign key in table cells.\n\n- Added UI for editing table filters.\n\n  Refs `#86`_\n\n- Hide FTS-created tables on index pages.\n\n  Closes `#129`_\n\n- Add publish to heroku support [Jacob Kaplan-Moss]\n\n  ``datasette publish heroku mydb.db``\n\n  Pull request `#104`_\n\n- Initial implementation of ``?_group_count=column``.\n\n  URL shortcut for counting rows grouped by one or more columns.\n\n  ``?_group_count=column1&_group_count=column2`` works as well.\n\n  SQL generated looks like this::\n\n      select \"qSpecies\", count(*) as \"count\"\n      from Street_Tree_List\n      group by \"qSpecies\"\n      order by \"count\" desc limit 100\n\n  Or for two columns like this::\n\n      select \"qSpecies\", \"qSiteInfo\", count(*) as \"count\"\n      from Street_Tree_List\n      group by \"qSpecies\", \"qSiteInfo\"\n      order by \"count\" desc limit 100\n\n  Refs `#44`_\n\n- Added ``--build=master`` option to datasette publish and package.\n\n  The ``datasette publish`` and ``datasette package`` commands both now accept an\n  optional ``--build`` argument. If provided, this can be used to specify a branch\n  published to GitHub that should be built into the container.\n\n  This makes it easier to test code that has not yet been officially released to\n  PyPI, e.g.::\n\n      datasette publish now mydb.db --branch=master\n\n- Implemented ``?_search=XXX`` + UI if a FTS table is detected.\n\n  Closes `#131`_\n\n- Added ``datasette --version`` support.\n\n- Table views now show expanded foreign key references, if possible.\n\n  If a table has foreign key columns, and those foreign key tables have\n  ``label_columns``, the TableView will now query those other tables for the\n  corresponding values and display those values as links in the corresponding\n  table cells.\n\n  label_columns are currently detected by the ``inspect()`` function, which looks\n  for any table that has just two columns - an ID column and one other - and\n  sets the ``label_column`` to be that second non-ID column.\n\n- Don't prevent tabbing to \"Run SQL\" button (`#117`_) [Robert Gieseke]\n\n  See comment in `#115`_\n\n- Add keyboard shortcut to execute SQL query (`#115`_) [Robert Gieseke]\n\n- Allow ``--load-extension`` to be set via environment variable.\n\n- Add support for ``?field__isnull=1`` (`#107`_) [Ray N]\n\n- Add spatialite, switch to debian and local build (`#114`_) [Ariel N\u00fa\u00f1ez]\n\n- Added ``--load-extension`` argument to datasette serve.\n\n  Allows loading of SQLite extensions. Refs `#110`_.\n\n.. _#133: https://github.com/simonw/datasette/issues/133\n.. _#147: https://github.com/simonw/datasette/issues/147\n.. _#148: https://github.com/simonw/datasette/issues/148\n.. _#132: https://github.com/simonw/datasette/issues/132\n.. _#86: https://github.com/simonw/datasette/issues/86\n.. _#129: https://github.com/simonw/datasette/issues/129\n.. _#104: https://github.com/simonw/datasette/issues/104\n.. _#44: https://github.com/simonw/datasette/issues/44\n.. _#131: https://github.com/simonw/datasette/issues/131\n.. _#115: https://github.com/simonw/datasette/issues/115\n.. _#117: https://github.com/simonw/datasette/issues/117\n.. _#107: https://github.com/simonw/datasette/issues/107\n.. _#114: https://github.com/simonw/datasette/issues/114\n.. _#110: https://github.com/simonw/datasette/issues/110\n\n0.12 (2017-11-16)\n-----------------\n- Added ``__version__``, now displayed as tooltip in page footer (`#108`_).\n- Added initial docs, including a changelog (`#99`_).\n- Turned on auto-escaping in Jinja.\n- Added a UI for editing named parameters (`#96`_).\n\n  You can now construct a custom SQL statement using SQLite named\n  parameters (e.g. ``:name``) and datasette will display form fields for\n  editing those parameters. `Here\u2019s an example`_ which lets you see the\n  most popular names for dogs of different species registered through\n  various dog registration schemes in Australia.\n\n.. _Here\u2019s an example: https://australian-dogs.now.sh/australian-dogs-3ba9628?sql=select+name%2C+count%28*%29+as+n+from+%28%0D%0A%0D%0Aselect+upper%28%22Animal+name%22%29+as+name+from+%5BAdelaide-City-Council-dog-registrations-2013%5D+where+Breed+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28Animal_Name%29+as+name+from+%5BAdelaide-City-Council-dog-registrations-2014%5D+where+Breed_Description+like+%3Abreed%0D%0A%0D%0Aunion+all+%0D%0A%0D%0Aselect+upper%28Animal_Name%29+as+name+from+%5BAdelaide-City-Council-dog-registrations-2015%5D+where+Breed_Description+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28%22AnimalName%22%29+as+name+from+%5BCity-of-Port-Adelaide-Enfield-Dog_Registrations_2016%5D+where+AnimalBreed+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28%22Animal+Name%22%29+as+name+from+%5BMitcham-dog-registrations-2015%5D+where+Breed+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28%22DOG_NAME%22%29+as+name+from+%5Bburnside-dog-registrations-2015%5D+where+DOG_BREED+like+%3Abreed%0D%0A%0D%0Aunion+all+%0D%0A%0D%0Aselect+upper%28%22Animal_Name%22%29+as+name+from+%5Bcity-of-playford-2015-dog-registration%5D+where+Breed_Description+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28%22Animal+Name%22%29+as+name+from+%5Bcity-of-prospect-dog-registration-details-2016%5D+where%22Breed+Description%22+like+%3Abreed%0D%0A%0D%0A%29+group+by+name+order+by+n+desc%3B&breed=pug\n\n- Pin to specific Jinja version. (`#100`_).\n- Default to 127.0.0.1 not 0.0.0.0. (`#98`_).\n- Added extra metadata options to publish and package commands. (`#92`_).\n\n  You can now run these commands like so::\n\n      datasette now publish mydb.db \\\n          --title=\"My Title\" \\\n          --source=\"Source\" \\\n          --source_url=\"http://www.example.com/\" \\\n          --license=\"CC0\" \\\n          --license_url=\"https://creativecommons.org/publicdomain/zero/1.0/\"\n\n  This will write those values into the metadata.json that is packaged with the\n  app. If you also pass ``--metadata=metadata.json`` that file will be updated with the extra\n  values before being written into the Docker image.\n- Added production-ready Dockerfile (`#94`_) [Andrew\n  Cutler]\n- New ``?_sql_time_limit_ms=10`` argument to database and table page (`#95`_)\n- SQL syntax highlighting with Codemirror (`#89`_) [Tom Dyson]\n\n.. _#89: https://github.com/simonw/datasette/issues/89\n.. _#92: https://github.com/simonw/datasette/issues/92\n.. _#94: https://github.com/simonw/datasette/issues/94\n.. _#95: https://github.com/simonw/datasette/issues/95\n.. _#96: https://github.com/simonw/datasette/issues/96\n.. _#98: https://github.com/simonw/datasette/issues/98\n.. _#99: https://github.com/simonw/datasette/issues/99\n.. _#100: https://github.com/simonw/datasette/issues/100\n.. _#108: https://github.com/simonw/datasette/issues/108\n\n0.11 (2017-11-14)\n-----------------\n- Added ``datasette publish now --force`` option.\n\n  This calls ``now`` with ``--force`` - useful as it means you get a fresh copy of datasette even if Now has already cached that docker layer.\n- Enable ``--cors`` by default when running in a container.\n\n0.10 (2017-11-14)\n-----------------\n- Fixed `#83`_ - 500 error on individual row pages.\n- Stop using sqlite WITH RECURSIVE in our tests.\n\n  The version of Python 3 running in Travis CI doesn't support this.\n\n.. _#83: https://github.com/simonw/datasette/issues/83\n\n0.9 (2017-11-13)\n----------------\n- Added ``--sql_time_limit_ms`` and ``--extra-options``.\n\n  The serve command now accepts ``--sql_time_limit_ms`` for customizing the SQL time\n  limit.\n\n  The publish and package commands now accept ``--extra-options`` which can be used\n  to specify additional options to be passed to the datasite serve command when\n  it executes inside the resulting Docker containers.\n\n0.8 (2017-11-13)\n----------------\n- V0.8 - added PyPI metadata, ready to ship.\n- Implemented offset/limit pagination for views (`#70`_).\n- Improved pagination. (`#78`_)\n- Limit on max rows returned, controlled by ``--max_returned_rows`` option. (`#69`_)\n\n  If someone executes 'select * from table' against a table with a million rows\n  in it, we could run into problems: just serializing that much data as JSON is\n  likely to lock up the server.\n\n  Solution: we now have a hard limit on the maximum number of rows that can be\n  returned by a query. If that limit is exceeded, the server will return a\n  ``\"truncated\": true`` field in the JSON.\n\n  This limit can be optionally controlled by the new ``--max_returned_rows``\n  option. Setting that option to 0 disables the limit entirely.\n\n.. _#70: https://github.com/simonw/datasette/issues/70\n.. _#78: https://github.com/simonw/datasette/issues/78\n.. _#69: https://github.com/simonw/datasette/issues/69\n", "import collections\nfrom datasette.app import Datasette\nfrom datasette.cli import cli\nfrom .fixtures import app_client, assert_permissions_checked, make_app_client\nfrom click.testing import CliRunner\nfrom bs4 import BeautifulSoup as Soup\nimport copy\nimport json\nfrom pprint import pprint\nimport pytest_asyncio\nimport pytest\nimport re\nimport time\nimport urllib\n\n\n@pytest.fixture(scope=\"module\")\ndef padlock_client():\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\n                    \"queries\": {\"two\": {\"sql\": \"select 1 + 1\"}},\n                }\n            }\n        }\n    ) as client:\n        yield client\n\n\n@pytest_asyncio.fixture\nasync def perms_ds():\n    ds = Datasette()\n    await ds.invoke_startup()\n    one = ds.add_memory_database(\"perms_ds_one\")\n    two = ds.add_memory_database(\"perms_ds_two\")\n    await one.execute_write(\"create table if not exists t1 (id integer primary key)\")\n    await one.execute_write(\"create table if not exists t2 (id integer primary key)\")\n    await two.execute_write(\"create table if not exists t1 (id integer primary key)\")\n    return ds\n\n\n@pytest.mark.parametrize(\n    \"allow,expected_anon,expected_auth\",\n    [\n        (None, 200, 200),\n        ({}, 403, 403),\n        ({\"id\": \"root\"}, 403, 200),\n    ],\n)\n@pytest.mark.parametrize(\n    \"path\",\n    (\n        \"/\",\n        \"/fixtures\",\n        \"/fixtures/compound_three_primary_keys\",\n        \"/fixtures/compound_three_primary_keys/a,a,a\",\n        \"/fixtures/two\",  # Query\n    ),\n)\ndef test_view_padlock(allow, expected_anon, expected_auth, path, padlock_client):\n    padlock_client.ds._metadata_local[\"allow\"] = allow\n    fragment = \"\ud83d\udd12</h1>\"\n    anon_response = padlock_client.get(path)\n    assert expected_anon == anon_response.status\n    if allow and anon_response.status == 200:\n        # Should be no padlock\n        assert fragment not in anon_response.text\n    auth_response = padlock_client.get(\n        path,\n        cookies={\"ds_actor\": padlock_client.actor_cookie({\"id\": \"root\"})},\n    )\n    assert expected_auth == auth_response.status\n    # Check for the padlock\n    if allow and expected_anon == 403 and expected_auth == 200:\n        assert fragment in auth_response.text\n    del padlock_client.ds._metadata_local[\"allow\"]\n\n\n@pytest.mark.parametrize(\n    \"allow,expected_anon,expected_auth\",\n    [\n        (None, 200, 200),\n        ({}, 403, 403),\n        ({\"id\": \"root\"}, 403, 200),\n    ],\n)\ndef test_view_database(allow, expected_anon, expected_auth):\n    with make_app_client(\n        metadata={\"databases\": {\"fixtures\": {\"allow\": allow}}}\n    ) as client:\n        for path in (\n            \"/fixtures\",\n            \"/fixtures/compound_three_primary_keys\",\n            \"/fixtures/compound_three_primary_keys/a,a,a\",\n        ):\n            anon_response = client.get(path)\n            assert expected_anon == anon_response.status, path\n            if allow and path == \"/fixtures\" and anon_response.status == 200:\n                # Should be no padlock\n                assert \">fixtures \ud83d\udd12</h1>\" not in anon_response.text\n            auth_response = client.get(\n                path,\n                cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})},\n            )\n            assert expected_auth == auth_response.status\n            if (\n                allow\n                and path == \"/fixtures\"\n                and expected_anon == 403\n                and expected_auth == 200\n            ):\n                assert \">fixtures \ud83d\udd12</h1>\" in auth_response.text\n\n\ndef test_database_list_respects_view_database():\n    with make_app_client(\n        metadata={\"databases\": {\"fixtures\": {\"allow\": {\"id\": \"root\"}}}},\n        extra_databases={\"data.db\": \"create table names (name text)\"},\n    ) as client:\n        anon_response = client.get(\"/\")\n        assert '<a href=\"/data\">data</a></h2>' in anon_response.text\n        assert '<a href=\"/fixtures\">fixtures</a>' not in anon_response.text\n        auth_response = client.get(\n            \"/\",\n            cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})},\n        )\n        assert '<a href=\"/data\">data</a></h2>' in auth_response.text\n        assert '<a href=\"/fixtures\">fixtures</a> \ud83d\udd12</h2>' in auth_response.text\n\n\ndef test_database_list_respects_view_table():\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"data\": {\n                    \"tables\": {\n                        \"names\": {\"allow\": {\"id\": \"root\"}},\n                        \"v\": {\"allow\": {\"id\": \"root\"}},\n                    }\n                }\n            }\n        },\n        extra_databases={\n            \"data.db\": \"create table names (name text); create view v as select * from names\"\n        },\n    ) as client:\n        html_fragments = [\n            \">names</a> \ud83d\udd12\",\n            \">v</a> \ud83d\udd12\",\n        ]\n        anon_response_text = client.get(\"/\").text\n        assert \"0 rows in 0 tables\" in anon_response_text\n        for html_fragment in html_fragments:\n            assert html_fragment not in anon_response_text\n        auth_response_text = client.get(\n            \"/\",\n            cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})},\n        ).text\n        for html_fragment in html_fragments:\n            assert html_fragment in auth_response_text\n\n\n@pytest.mark.parametrize(\n    \"allow,expected_anon,expected_auth\",\n    [\n        (None, 200, 200),\n        ({}, 403, 403),\n        ({\"id\": \"root\"}, 403, 200),\n    ],\n)\ndef test_view_table(allow, expected_anon, expected_auth):\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\n                    \"tables\": {\"compound_three_primary_keys\": {\"allow\": allow}}\n                }\n            }\n        }\n    ) as client:\n        anon_response = client.get(\"/fixtures/compound_three_primary_keys\")\n        assert expected_anon == anon_response.status\n        if allow and anon_response.status == 200:\n            # Should be no padlock\n            assert \">compound_three_primary_keys \ud83d\udd12</h1>\" not in anon_response.text\n        auth_response = client.get(\n            \"/fixtures/compound_three_primary_keys\",\n            cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})},\n        )\n        assert expected_auth == auth_response.status\n        if allow and expected_anon == 403 and expected_auth == 200:\n            assert \">compound_three_primary_keys \ud83d\udd12</h1>\" in auth_response.text\n\n\ndef test_table_list_respects_view_table():\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\n                    \"tables\": {\n                        \"compound_three_primary_keys\": {\"allow\": {\"id\": \"root\"}},\n                        # And a SQL view too:\n                        \"paginated_view\": {\"allow\": {\"id\": \"root\"}},\n                    }\n                }\n            }\n        }\n    ) as client:\n        html_fragments = [\n            \">compound_three_primary_keys</a> \ud83d\udd12\",\n            \">paginated_view</a> \ud83d\udd12\",\n        ]\n        anon_response = client.get(\"/fixtures\")\n        for html_fragment in html_fragments:\n            assert html_fragment not in anon_response.text\n        auth_response = client.get(\n            \"/fixtures\", cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})}\n        )\n        for html_fragment in html_fragments:\n            assert html_fragment in auth_response.text\n\n\n@pytest.mark.parametrize(\n    \"allow,expected_anon,expected_auth\",\n    [\n        (None, 200, 200),\n        ({}, 403, 403),\n        ({\"id\": \"root\"}, 403, 200),\n    ],\n)\ndef test_view_query(allow, expected_anon, expected_auth):\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\"queries\": {\"q\": {\"sql\": \"select 1 + 1\", \"allow\": allow}}}\n            }\n        }\n    ) as client:\n        anon_response = client.get(\"/fixtures/q\")\n        assert expected_anon == anon_response.status\n        if allow and anon_response.status == 200:\n            # Should be no padlock\n            assert \"\ud83d\udd12</h1>\" not in anon_response.text\n        auth_response = client.get(\n            \"/fixtures/q\", cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})}\n        )\n        assert expected_auth == auth_response.status\n        if allow and expected_anon == 403 and expected_auth == 200:\n            assert \">fixtures: q \ud83d\udd12</h1>\" in auth_response.text\n\n\n@pytest.mark.parametrize(\n    \"metadata\",\n    [\n        {\"allow_sql\": {\"id\": \"root\"}},\n        {\"databases\": {\"fixtures\": {\"allow_sql\": {\"id\": \"root\"}}}},\n    ],\n)\ndef test_execute_sql(metadata):\n    schema_re = re.compile(\"const schema = ({.*?});\", re.DOTALL)\n    with make_app_client(metadata=metadata) as client:\n        form_fragment = '<form class=\"sql\" action=\"/fixtures\"'\n\n        # Anonymous users - should not display the form:\n        anon_html = client.get(\"/fixtures\").text\n        assert form_fragment not in anon_html\n        # And const schema should be an empty object:\n        assert \"const schema = {};\" in anon_html\n        # This should 403:\n        assert client.get(\"/fixtures?sql=select+1\").status == 403\n        # ?_where= not allowed on tables:\n        assert client.get(\"/fixtures/facet_cities?_where=id=3\").status == 403\n\n        # But for logged in user all of these should work:\n        cookies = {\"ds_actor\": client.actor_cookie({\"id\": \"root\"})}\n        response_text = client.get(\"/fixtures\", cookies=cookies).text\n        # Extract the schema= portion of the JavaScript\n        schema_json = schema_re.search(response_text).group(1)\n        schema = json.loads(schema_json)\n        assert set(schema[\"attraction_characteristic\"]) == {\"name\", \"pk\"}\n        assert schema[\"paginated_view\"] == []\n        assert form_fragment in response_text\n        query_response = client.get(\"/fixtures?sql=select+1\", cookies=cookies)\n        assert query_response.status == 200\n        schema2 = json.loads(schema_re.search(query_response.text).group(1))\n        assert set(schema2[\"attraction_characteristic\"]) == {\"name\", \"pk\"}\n        assert (\n            client.get(\"/fixtures/facet_cities?_where=id=3\", cookies=cookies).status\n            == 200\n        )\n\n\ndef test_query_list_respects_view_query():\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\n                    \"queries\": {\"q\": {\"sql\": \"select 1 + 1\", \"allow\": {\"id\": \"root\"}}}\n                }\n            }\n        }\n    ) as client:\n        html_fragment = '<li><a href=\"/fixtures/q\" title=\"select 1 + 1\">q</a> \ud83d\udd12</li>'\n        anon_response = client.get(\"/fixtures\")\n        assert html_fragment not in anon_response.text\n        assert '\"/fixtures/q\"' not in anon_response.text\n        auth_response = client.get(\n            \"/fixtures\", cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})}\n        )\n        assert html_fragment in auth_response.text\n\n\n@pytest.mark.parametrize(\n    \"path,permissions\",\n    [\n        (\"/\", [\"view-instance\"]),\n        (\"/fixtures\", [\"view-instance\", (\"view-database\", \"fixtures\")]),\n        (\n            \"/fixtures/facetable/1\",\n            [\"view-instance\", (\"view-table\", (\"fixtures\", \"facetable\"))],\n        ),\n        (\n            \"/fixtures/simple_primary_key\",\n            [\n                \"view-instance\",\n                (\"view-database\", \"fixtures\"),\n                (\"view-table\", (\"fixtures\", \"simple_primary_key\")),\n            ],\n        ),\n        (\n            \"/fixtures?sql=select+1\",\n            [\n                \"view-instance\",\n                (\"view-database\", \"fixtures\"),\n                (\"execute-sql\", \"fixtures\"),\n            ],\n        ),\n        (\n            \"/fixtures.db\",\n            [\n                \"view-instance\",\n                (\"view-database\", \"fixtures\"),\n                (\"view-database-download\", \"fixtures\"),\n            ],\n        ),\n        (\n            \"/fixtures/neighborhood_search\",\n            [\n                \"view-instance\",\n                (\"view-database\", \"fixtures\"),\n                (\"view-query\", (\"fixtures\", \"neighborhood_search\")),\n            ],\n        ),\n    ],\n)\ndef test_permissions_checked(app_client, path, permissions):\n    # Needs file-backed app_client for /fixtures.db\n    app_client.ds._permission_checks.clear()\n    response = app_client.get(path)\n    assert response.status_code in (200, 403)\n    assert_permissions_checked(app_client.ds, permissions)\n\n\n@pytest.mark.asyncio\nasync def test_permissions_debug(ds_client):\n    ds_client.ds._permission_checks.clear()\n    assert (await ds_client.get(\"/-/permissions\")).status_code == 403\n    # With the cookie it should work\n    cookie = ds_client.actor_cookie({\"id\": \"root\"})\n    response = await ds_client.get(\"/-/permissions\", cookies={\"ds_actor\": cookie})\n    assert response.status_code == 200\n    # Should show one failure and one success\n    soup = Soup(response.text, \"html.parser\")\n    check_divs = soup.findAll(\"div\", {\"class\": \"check\"})\n    checks = [\n        {\n            \"action\": div.select_one(\".check-action\").text,\n            # True = green tick, False = red cross, None = gray None\n            \"result\": None\n            if div.select(\".check-result-no-opinion\")\n            else bool(div.select(\".check-result-true\")),\n            \"used_default\": bool(div.select(\".check-used-default\")),\n        }\n        for div in check_divs\n    ]\n    assert checks == [\n        {\"action\": \"permissions-debug\", \"result\": True, \"used_default\": False},\n        {\"action\": \"view-instance\", \"result\": None, \"used_default\": True},\n        {\"action\": \"debug-menu\", \"result\": False, \"used_default\": True},\n        {\"action\": \"view-instance\", \"result\": True, \"used_default\": True},\n        {\"action\": \"permissions-debug\", \"result\": False, \"used_default\": True},\n        {\"action\": \"view-instance\", \"result\": None, \"used_default\": True},\n    ]\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"actor,allow,expected_fragment\",\n    [\n        ('{\"id\":\"root\"}', \"{}\", \"Result: deny\"),\n        ('{\"id\":\"root\"}', '{\"id\": \"*\"}', \"Result: allow\"),\n        ('{\"', '{\"id\": \"*\"}', \"Actor JSON error\"),\n        ('{\"id\":\"root\"}', '\"*\"}', \"Allow JSON error\"),\n    ],\n)\nasync def test_allow_debug(ds_client, actor, allow, expected_fragment):\n    response = await ds_client.get(\n        \"/-/allow-debug?\" + urllib.parse.urlencode({\"actor\": actor, \"allow\": allow})\n    )\n    assert response.status_code == 200\n    assert expected_fragment in response.text\n\n\n@pytest.mark.parametrize(\n    \"allow,expected\",\n    [\n        ({\"id\": \"root\"}, 403),\n        ({\"id\": \"root\", \"unauthenticated\": True}, 200),\n    ],\n)\ndef test_allow_unauthenticated(allow, expected):\n    with make_app_client(metadata={\"allow\": allow}) as client:\n        assert expected == client.get(\"/\").status\n\n\n@pytest.fixture(scope=\"session\")\ndef view_instance_client():\n    with make_app_client(metadata={\"allow\": {}}) as client:\n        yield client\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\n        \"/\",\n        \"/fixtures\",\n        \"/fixtures/facetable\",\n        \"/-/metadata\",\n        \"/-/versions\",\n        \"/-/plugins\",\n        \"/-/settings\",\n        \"/-/threads\",\n        \"/-/databases\",\n        \"/-/permissions\",\n        \"/-/messages\",\n        \"/-/patterns\",\n    ],\n)\ndef test_view_instance(path, view_instance_client):\n    assert 403 == view_instance_client.get(path).status\n    if path not in (\"/-/permissions\", \"/-/messages\", \"/-/patterns\"):\n        assert 403 == view_instance_client.get(path + \".json\").status\n\n\n@pytest.fixture(scope=\"session\")\ndef cascade_app_client():\n    with make_app_client(is_immutable=True) as client:\n        yield client\n\n\n@pytest.mark.parametrize(\n    \"path,permissions,expected_status\",\n    [\n        (\"/\", [], 403),\n        (\"/\", [\"instance\"], 200),\n        # Can view table even if not allowed database or instance\n        (\"/fixtures/binary_data\", [], 403),\n        (\"/fixtures/binary_data\", [\"database\"], 403),\n        (\"/fixtures/binary_data\", [\"instance\"], 403),\n        (\"/fixtures/binary_data\", [\"table\"], 200),\n        (\"/fixtures/binary_data\", [\"table\", \"database\"], 200),\n        (\"/fixtures/binary_data\", [\"table\", \"database\", \"instance\"], 200),\n        # ... same for row\n        (\"/fixtures/binary_data/1\", [], 403),\n        (\"/fixtures/binary_data/1\", [\"database\"], 403),\n        (\"/fixtures/binary_data/1\", [\"instance\"], 403),\n        (\"/fixtures/binary_data/1\", [\"table\"], 200),\n        (\"/fixtures/binary_data/1\", [\"table\", \"database\"], 200),\n        (\"/fixtures/binary_data/1\", [\"table\", \"database\", \"instance\"], 200),\n        # Can view query even if not allowed database or instance\n        (\"/fixtures/magic_parameters\", [], 403),\n        (\"/fixtures/magic_parameters\", [\"database\"], 403),\n        (\"/fixtures/magic_parameters\", [\"instance\"], 403),\n        (\"/fixtures/magic_parameters\", [\"query\"], 200),\n        (\"/fixtures/magic_parameters\", [\"query\", \"database\"], 200),\n        (\"/fixtures/magic_parameters\", [\"query\", \"database\", \"instance\"], 200),\n        # Can view database even if not allowed instance\n        (\"/fixtures\", [], 403),\n        (\"/fixtures\", [\"instance\"], 403),\n        (\"/fixtures\", [\"database\"], 200),\n        # Downloading the fixtures.db file\n        (\"/fixtures.db\", [], 403),\n        (\"/fixtures.db\", [\"instance\"], 403),\n        (\"/fixtures.db\", [\"database\"], 200),\n        (\"/fixtures.db\", [\"download\"], 200),\n    ],\n)\ndef test_permissions_cascade(cascade_app_client, path, permissions, expected_status):\n    \"\"\"Test that e.g. having view-table but NOT view-database lets you view table page, etc\"\"\"\n    allow = {\"id\": \"*\"}\n    deny = {}\n    previous_metadata = cascade_app_client.ds.metadata()\n    updated_metadata = copy.deepcopy(previous_metadata)\n    actor = {\"id\": \"test\"}\n    if \"download\" in permissions:\n        actor[\"can_download\"] = 1\n    try:\n        # Set up the different allow blocks\n        updated_metadata[\"allow\"] = allow if \"instance\" in permissions else deny\n        updated_metadata[\"databases\"][\"fixtures\"][\"allow\"] = (\n            allow if \"database\" in permissions else deny\n        )\n        updated_metadata[\"databases\"][\"fixtures\"][\"tables\"][\"binary_data\"] = {\n            \"allow\": (allow if \"table\" in permissions else deny)\n        }\n        updated_metadata[\"databases\"][\"fixtures\"][\"queries\"][\"magic_parameters\"][\n            \"allow\"\n        ] = (allow if \"query\" in permissions else deny)\n        cascade_app_client.ds._metadata_local = updated_metadata\n        response = cascade_app_client.get(\n            path,\n            cookies={\"ds_actor\": cascade_app_client.actor_cookie(actor)},\n        )\n        assert (\n            response.status == expected_status\n        ), \"path: {}, permissions: {}, expected_status: {}, status: {}\".format(\n            path, permissions, expected_status, response.status\n        )\n    finally:\n        cascade_app_client.ds._metadata_local = previous_metadata\n\n\ndef test_padlocks_on_database_page(cascade_app_client):\n    metadata = {\n        \"databases\": {\n            \"fixtures\": {\n                \"allow\": {\"id\": \"test\"},\n                \"tables\": {\n                    \"123_starts_with_digits\": {\"allow\": True},\n                    \"simple_view\": {\"allow\": True},\n                },\n                \"queries\": {\"query_two\": {\"allow\": True, \"sql\": \"select 2\"}},\n            }\n        }\n    }\n    previous_metadata = cascade_app_client.ds._metadata_local\n    try:\n        cascade_app_client.ds._metadata_local = metadata\n        response = cascade_app_client.get(\n            \"/fixtures\",\n            cookies={\"ds_actor\": cascade_app_client.actor_cookie({\"id\": \"test\"})},\n        )\n        # Tables\n        assert \">123_starts_with_digits</a></h3>\" in response.text\n        assert \">Table With Space In Name</a> \ud83d\udd12</h3>\" in response.text\n        # Queries\n        assert \">from_async_hook</a> \ud83d\udd12</li>\" in response.text\n        assert \">query_two</a></li>\" in response.text\n        # Views\n        assert \">paginated_view</a> \ud83d\udd12</li>\" in response.text\n        assert \">simple_view</a></li>\" in response.text\n    finally:\n        cascade_app_client.ds._metadata_local = previous_metadata\n\n\nDEF = \"USE_DEFAULT\"\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"actor,permission,resource_1,resource_2,expected_result\",\n    (\n        # Without restrictions the defaults apply\n        ({\"id\": \"t\"}, \"view-instance\", None, None, DEF),\n        ({\"id\": \"t\"}, \"view-database\", \"one\", None, DEF),\n        ({\"id\": \"t\"}, \"view-table\", \"one\", \"t1\", DEF),\n        # If there is an _r block, everything gets denied unless explicitly allowed\n        ({\"id\": \"t\", \"_r\": {}}, \"view-instance\", None, None, False),\n        ({\"id\": \"t\", \"_r\": {}}, \"view-database\", \"one\", None, False),\n        ({\"id\": \"t\", \"_r\": {}}, \"view-table\", \"one\", \"t1\", False),\n        # Explicit allowing works at the \"a\" for all level:\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vi\"]}}, \"view-instance\", None, None, DEF),\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vd\"]}}, \"view-database\", \"one\", None, DEF),\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vt\"]}}, \"view-table\", \"one\", \"t1\", DEF),\n        # But not if it's the wrong permission\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vd\"]}}, \"view-instance\", None, None, False),\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vi\"]}}, \"view-database\", \"one\", None, False),\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vd\"]}}, \"view-table\", \"one\", \"t1\", False),\n        # Works at the \"d\" for database level:\n        ({\"id\": \"t\", \"_r\": {\"d\": {\"one\": [\"vd\"]}}}, \"view-database\", \"one\", None, DEF),\n        (\n            {\"id\": \"t\", \"_r\": {\"d\": {\"one\": [\"vdd\"]}}},\n            \"view-database-download\",\n            \"one\",\n            None,\n            DEF,\n        ),\n        ({\"id\": \"t\", \"_r\": {\"d\": {\"one\": [\"es\"]}}}, \"execute-sql\", \"one\", None, DEF),\n        # Works at the \"r\" for table level:\n        (\n            {\"id\": \"t\", \"_r\": {\"r\": {\"one\": {\"t1\": [\"vt\"]}}}},\n            \"view-table\",\n            \"one\",\n            \"t1\",\n            DEF,\n        ),\n        (\n            {\"id\": \"t\", \"_r\": {\"r\": {\"one\": {\"t1\": [\"vt\"]}}}},\n            \"view-table\",\n            \"one\",\n            \"t2\",\n            False,\n        ),\n        # non-abbreviations should work too\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"view-instance\"]}}, \"view-instance\", None, None, DEF),\n        (\n            {\"id\": \"t\", \"_r\": {\"d\": {\"one\": [\"view-database\"]}}},\n            \"view-database\",\n            \"one\",\n            None,\n            DEF,\n        ),\n        (\n            {\"id\": \"t\", \"_r\": {\"r\": {\"one\": {\"t1\": [\"view-table\"]}}}},\n            \"view-table\",\n            \"one\",\n            \"t1\",\n            DEF,\n        ),\n    ),\n)\nasync def test_actor_restricted_permissions(\n    perms_ds, actor, permission, resource_1, resource_2, expected_result\n):\n    cookies = {\"ds_actor\": perms_ds.sign({\"a\": {\"id\": \"root\"}}, \"actor\")}\n    csrftoken = (await perms_ds.client.get(\"/-/permissions\", cookies=cookies)).cookies[\n        \"ds_csrftoken\"\n    ]\n    cookies[\"ds_csrftoken\"] = csrftoken\n    response = await perms_ds.client.post(\n        \"/-/permissions\",\n        data={\n            \"actor\": json.dumps(actor),\n            \"permission\": permission,\n            \"resource_1\": resource_1,\n            \"resource_2\": resource_2,\n            \"csrftoken\": csrftoken,\n        },\n        cookies=cookies,\n    )\n    expected_resource = []\n    if resource_1:\n        expected_resource.append(resource_1)\n    if resource_2:\n        expected_resource.append(resource_2)\n    if len(expected_resource) == 1:\n        expected_resource = expected_resource[0]\n    expected = {\n        \"actor\": actor,\n        \"permission\": permission,\n        \"resource\": expected_resource,\n        \"result\": expected_result,\n    }\n    assert response.json() == expected\n\n\nPermMetadataTestCase = collections.namedtuple(\n    \"PermMetadataTestCase\",\n    \"metadata,actor,action,resource,expected_result\",\n)\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"metadata,actor,action,resource,expected_result\",\n    (\n        # Simple view-instance default=True example\n        PermMetadataTestCase(\n            metadata={},\n            actor=None,\n            action=\"view-instance\",\n            resource=None,\n            expected_result=True,\n        ),\n        # debug-menu on root\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"debug-menu\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user\"},\n            action=\"debug-menu\",\n            resource=None,\n            expected_result=True,\n        ),\n        # debug-menu on root, wrong actor\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"debug-menu\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user2\"},\n            action=\"debug-menu\",\n            resource=None,\n            expected_result=False,\n        ),\n        # create-table on root\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"create-table\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user\"},\n            action=\"create-table\",\n            resource=None,\n            expected_result=True,\n        ),\n        # create-table on database - no resource specified\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\"permissions\": {\"create-table\": {\"id\": \"user\"}}}\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"create-table\",\n            resource=None,\n            expected_result=False,\n        ),\n        # create-table on database\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\"permissions\": {\"create-table\": {\"id\": \"user\"}}}\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"create-table\",\n            resource=\"perms_ds_one\",\n            expected_result=True,\n        ),\n        # insert-row on root, wrong actor\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"insert-row\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user2\"},\n            action=\"insert-row\",\n            resource=(\"perms_ds_one\", \"t1\"),\n            expected_result=False,\n        ),\n        # insert-row on root, right actor\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"insert-row\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user\"},\n            action=\"insert-row\",\n            resource=(\"perms_ds_one\", \"t1\"),\n            expected_result=True,\n        ),\n        # insert-row on database\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\"permissions\": {\"insert-row\": {\"id\": \"user\"}}}\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"insert-row\",\n            resource=\"perms_ds_one\",\n            expected_result=True,\n        ),\n        # insert-row on table, wrong table\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"tables\": {\n                            \"t1\": {\"permissions\": {\"insert-row\": {\"id\": \"user\"}}}\n                        }\n                    }\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"insert-row\",\n            resource=(\"perms_ds_one\", \"t2\"),\n            expected_result=False,\n        ),\n        # insert-row on table, right table\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"tables\": {\n                            \"t1\": {\"permissions\": {\"insert-row\": {\"id\": \"user\"}}}\n                        }\n                    }\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"insert-row\",\n            resource=(\"perms_ds_one\", \"t1\"),\n            expected_result=True,\n        ),\n        # view-query on canned query, wrong actor\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"queries\": {\n                            \"q1\": {\n                                \"sql\": \"select 1 + 1\",\n                                \"permissions\": {\"view-query\": {\"id\": \"user\"}},\n                            }\n                        }\n                    }\n                }\n            },\n            actor={\"id\": \"user2\"},\n            action=\"view-query\",\n            resource=(\"perms_ds_one\", \"q1\"),\n            expected_result=False,\n        ),\n        # view-query on canned query, right actor\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"queries\": {\n                            \"q1\": {\n                                \"sql\": \"select 1 + 1\",\n                                \"permissions\": {\"view-query\": {\"id\": \"user\"}},\n                            }\n                        }\n                    }\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"view-query\",\n            resource=(\"perms_ds_one\", \"q1\"),\n            expected_result=True,\n        ),\n    ),\n)\nasync def test_permissions_in_metadata(\n    perms_ds, metadata, actor, action, resource, expected_result\n):\n    previous_metadata = perms_ds.metadata()\n    updated_metadata = copy.deepcopy(previous_metadata)\n    updated_metadata.update(metadata)\n    perms_ds._metadata_local = updated_metadata\n    try:\n        result = await perms_ds.permission_allowed(actor, action, resource)\n        if result != expected_result:\n            pprint(perms_ds._permission_checks)\n            assert result == expected_result\n    finally:\n        perms_ds._metadata_local = previous_metadata\n\n\n@pytest.mark.asyncio\nasync def test_actor_endpoint_allows_any_token():\n    ds = Datasette()\n    token = ds.sign(\n        {\n            \"a\": \"root\",\n            \"token\": \"dstok\",\n            \"t\": int(time.time()),\n            \"_r\": {\"a\": [\"debug-menu\"]},\n        },\n        namespace=\"token\",\n    )\n    response = await ds.client.get(\n        \"/-/actor.json\", headers={\"Authorization\": f\"Bearer dstok_{token}\"}\n    )\n    assert response.status_code == 200\n    assert response.json()[\"actor\"] == {\n        \"id\": \"root\",\n        \"token\": \"dstok\",\n        \"_r\": {\"a\": [\"debug-menu\"]},\n    }\n\n\n@pytest.mark.parametrize(\n    \"options,expected\",\n    (\n        ([], {\"id\": \"root\", \"token\": \"dstok\"}),\n        (\n            [\"--all\", \"debug-menu\"],\n            {\"_r\": {\"a\": [\"dm\"]}, \"id\": \"root\", \"token\": \"dstok\"},\n        ),\n        (\n            [\"-a\", \"debug-menu\", \"--all\", \"create-table\"],\n            {\"_r\": {\"a\": [\"dm\", \"ct\"]}, \"id\": \"root\", \"token\": \"dstok\"},\n        ),\n        (\n            [\"-r\", \"db1\", \"t1\", \"insert-row\"],\n            {\"_r\": {\"r\": {\"db1\": {\"t1\": [\"ir\"]}}}, \"id\": \"root\", \"token\": \"dstok\"},\n        ),\n        (\n            [\"-d\", \"db1\", \"create-table\"],\n            {\"_r\": {\"d\": {\"db1\": [\"ct\"]}}, \"id\": \"root\", \"token\": \"dstok\"},\n        ),\n        # And one with all of them multiple times using all the names\n        (\n            [\n                \"-a\",\n                \"debug-menu\",\n                \"--all\",\n                \"create-table\",\n                \"-r\",\n                \"db1\",\n                \"t1\",\n                \"insert-row\",\n                \"--resource\",\n                \"db1\",\n                \"t2\",\n                \"update-row\",\n                \"-d\",\n                \"db1\",\n                \"create-table\",\n                \"--database\",\n                \"db2\",\n                \"drop-table\",\n            ],\n            {\n                \"_r\": {\n                    \"a\": [\"dm\", \"ct\"],\n                    \"d\": {\"db1\": [\"ct\"], \"db2\": [\"dt\"]},\n                    \"r\": {\"db1\": {\"t1\": [\"ir\"], \"t2\": [\"ur\"]}},\n                },\n                \"id\": \"root\",\n                \"token\": \"dstok\",\n            },\n        ),\n    ),\n)\ndef test_cli_create_token(options, expected):\n    runner = CliRunner()\n    result1 = runner.invoke(\n        cli,\n        [\n            \"create-token\",\n            \"--secret\",\n            \"sekrit\",\n            \"root\",\n        ]\n        + options,\n    )\n    token = result1.output.strip()\n    result2 = runner.invoke(\n        cli,\n        [\n            \"serve\",\n            \"--secret\",\n            \"sekrit\",\n            \"--get\",\n            \"/-/actor.json\",\n            \"--token\",\n            token,\n        ],\n    )\n    assert 0 == result2.exit_code, result2.output\n    assert json.loads(result2.output) == {\"actor\": expected}\n"], "fixing_code": ["{% extends \"base.html\" %}\n\n{% block title %}API Explorer{% endblock %}\n\n{% block extra_head %}\n<script src=\"{{ base_url }}-/static/json-format-highlight-1.0.1.js\"></script>\n{% endblock %}\n\n{% block content %}\n\n<h1>API Explorer{% if private %} \ud83d\udd12{% endif %}</h1>\n\n<p>Use this tool to try out the\n  {% if datasette_version %}\n    <a href=\"https://docs.datasette.io/en/{{ datasette_version }}/json_api.html\">Datasette API</a>.\n  {% else %}\n    Datasette API.\n  {% endif %}\n</p>\n<details open style=\"border: 2px solid #ccc; border-bottom: none; padding: 0.5em\">\n  <summary style=\"cursor: pointer;\">GET</summary>\n  <form method=\"get\" id=\"api-explorer-get\" style=\"margin-top: 0.7em\">\n    <div>\n      <label for=\"path\">API path:</label>\n      <input type=\"text\" id=\"path\" name=\"path\" style=\"width: 60%\">\n      <input type=\"submit\" value=\"GET\">\n    </div>\n  </form>\n</details>\n<details style=\"border: 2px solid #ccc; padding: 0.5em\">\n  <summary style=\"cursor: pointer\">POST</summary>\n  <form method=\"post\" id=\"api-explorer-post\" style=\"margin-top: 0.7em\">\n    <div>\n      <label for=\"path\">API path:</label>\n      <input type=\"text\" id=\"path\" name=\"path\" style=\"width: 60%\">\n    </div>\n    <div style=\"margin: 0.5em 0\">\n      <label for=\"apiJson\" style=\"vertical-align: top\">JSON:</label>\n      <textarea id=\"apiJson\" name=\"json\" style=\"width: 60%; height: 200px; font-family: monospace; font-size: 0.8em;\"></textarea>\n    </div>\n    <p><button id=\"json-format\" type=\"button\">Format JSON</button> <input type=\"submit\" value=\"POST\"></p>\n  </form>\n</details>\n\n<div id=\"output\" style=\"display: none\">\n  <h2>API response: HTTP <span id=\"response-status\"></span></h2>\n  </h2>\n  <ul class=\"errors message-error\"></ul>\n  <pre></pre>\n</div>\n\n<script>\ndocument.querySelector('#json-format').addEventListener('click', (ev) => {\n  ev.preventDefault();\n  let json = document.querySelector('textarea[name=\"json\"]').value.trim();\n  if (!json) {\n    return;\n  }\n  try {\n    const parsed = JSON.parse(json);\n    document.querySelector('textarea[name=\"json\"]').value = JSON.stringify(parsed, null, 2);\n  } catch (e) {\n    alert(\"Error parsing JSON: \" + e);\n  }\n});\nvar postForm = document.getElementById('api-explorer-post');\nvar getForm = document.getElementById('api-explorer-get');\nvar output = document.getElementById('output');\nvar errorList = output.querySelector('.errors');\n\n// On first load or fragment change populate forms from # in URL, if present\nif (window.location.hash) {\n  onFragmentChange();\n}\nfunction onFragmentChange() {\n  var hash = window.location.hash.slice(1);\n  // Treat hash as a foo=bar string and parse it:\n  var params = new URLSearchParams(hash);\n  var method = params.get('method');\n  if (method == 'GET') {\n    getForm.closest('details').open = true;\n    postForm.closest('details').open = false;\n    getForm.querySelector('input[name=\"path\"]').value = params.get('path');\n  } else if (method == 'POST') {\n    postForm.closest('details').open = true;\n    getForm.closest('details').open = false;\n    postForm.querySelector('input[name=\"path\"]').value = params.get('path');\n    postForm.querySelector('textarea[name=\"json\"]').value = params.get('json');\n  }\n}\nwindow.addEventListener('hashchange', () => {\n  onFragmentChange();\n  // Animate scroll to top of page\n  window.scrollTo({top: 0, behavior: 'smooth'});\n});\n\n// Cause GET and POST regions to toggle each other\nvar getDetails = getForm.closest('details');\nvar postDetails = postForm.closest('details');\ngetDetails.addEventListener('toggle', (ev) => {\n  if (getDetails.open) {\n    postDetails.open = false;\n  }\n});\npostDetails.addEventListener('toggle', (ev) => {\n  if (postDetails.open) {\n    getDetails.open = false;\n  }\n});\n\ngetForm.addEventListener(\"submit\", (ev) => {\n  ev.preventDefault();\n  var formData = new FormData(getForm);\n  // Update URL fragment hash\n  var serialized = new URLSearchParams(formData).toString() + '&method=GET';\n  window.history.pushState({}, \"\", location.pathname + '#' + serialized);\n  // Send the request\n  var path = formData.get('path');\n  fetch(path, {\n    method: 'GET',\n    headers: {\n      'Accept': 'application/json',\n    }\n  }).then((response) => {\n    output.style.display = 'block';\n    document.getElementById('response-status').textContent = response.status;\n    return response.json();\n  }).then((data) => {\n    output.querySelector('pre').innerHTML = jsonFormatHighlight(data);\n    errorList.style.display = 'none';\n  }).catch((error) => {\n    alert(error);\n  });\n});\n\npostForm.addEventListener(\"submit\", (ev) => {\n  ev.preventDefault();\n  var formData = new FormData(postForm);\n  // Update URL fragment hash\n  var serialized = new URLSearchParams(formData).toString() + '&method=POST';\n  window.history.pushState({}, \"\", location.pathname + '#' + serialized);\n  // Send the request\n  var json = formData.get('json');\n  var path = formData.get('path');\n  // Validate JSON\n  if (!json.length) {\n    json = '{}';\n  }\n  try {\n    var data = JSON.parse(json);\n  } catch (err) {\n    alert(\"Invalid JSON: \" + err);\n    return;\n  }\n  // POST JSON to path with content-type application/json\n  fetch(path, {\n    method: 'POST',\n    body: json,\n    headers: {\n      'Content-Type': 'application/json',\n    }\n  }).then(r => {\n    document.getElementById('response-status').textContent = r.status;\n    return r.json();\n  }).then(data => {\n    if (data.errors) {\n      errorList.style.display = 'block';\n      errorList.innerHTML = '';\n      data.errors.forEach(error => {\n        var li = document.createElement('li');\n        li.textContent = error;\n        errorList.appendChild(li);\n      });\n    } else {\n      errorList.style.display = 'none';\n    }\n    output.querySelector('pre').innerHTML = jsonFormatHighlight(data);\n    output.style.display = 'block';\n  }).catch(err => {\n    alert(\"Error: \" + err);\n  });\n});\n</script>\n\n{% if example_links %}\n<h2>API endpoints</h2>\n<ul class=\"bullets\">\n  {% for database in example_links %}\n    <li>Database: <strong>{{ database.name }}</strong></li>\n    <ul class=\"bullets\">\n      {% for link in database.links %}\n        <li><a href=\"{{ api_path(link) }}\">{{ link.path }}</a> - {{ link.label }} </li>\n      {% endfor %}\n      {% for table in database.tables %}\n        <li><strong>{{ table.name }}</strong>\n          <ul class=\"bullets\">\n            {% for link in table.links %}\n              <li><a href=\"{{ api_path(link) }}\">{{ link.path }}</a> - {{ link.label }} </li>\n            {% endfor %}\n          </ul>\n        </li>\n      {% endfor %}\n    </ul>\n  {% endfor %}\n</ul>\n{% endif %}\n\n{% endblock %}\n", "__version__ = \"1.0a4\"\n__version_info__ = tuple(__version__.split(\".\"))\n", "import json\nfrom datasette.utils.asgi import Response, Forbidden\nfrom datasette.utils import (\n    actor_matches_allow,\n    add_cors_headers,\n    tilde_encode,\n    tilde_decode,\n)\nfrom .base import BaseView, View\nimport secrets\nimport urllib\n\n\nclass JsonDataView(BaseView):\n    name = \"json_data\"\n\n    def __init__(\n        self,\n        datasette,\n        filename,\n        data_callback,\n        needs_request=False,\n        permission=\"view-instance\",\n    ):\n        self.ds = datasette\n        self.filename = filename\n        self.data_callback = data_callback\n        self.needs_request = needs_request\n        self.permission = permission\n\n    async def get(self, request):\n        as_format = request.url_vars[\"format\"]\n        if self.permission:\n            await self.ds.ensure_permissions(request.actor, [self.permission])\n        if self.needs_request:\n            data = self.data_callback(request)\n        else:\n            data = self.data_callback()\n        if as_format:\n            headers = {}\n            if self.ds.cors:\n                add_cors_headers(headers)\n            return Response(\n                json.dumps(data),\n                content_type=\"application/json; charset=utf-8\",\n                headers=headers,\n            )\n\n        else:\n            return await self.render(\n                [\"show_json.html\"],\n                request=request,\n                context={\n                    \"filename\": self.filename,\n                    \"data_json\": json.dumps(data, indent=4),\n                },\n            )\n\n\nclass PatternPortfolioView(View):\n    async def get(self, request, datasette):\n        await datasette.ensure_permissions(request.actor, [\"view-instance\"])\n        return Response.html(\n            await datasette.render_template(\n                \"patterns.html\",\n                request=request,\n                view_name=\"patterns\",\n            )\n        )\n\n\nclass AuthTokenView(BaseView):\n    name = \"auth_token\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        token = request.args.get(\"token\") or \"\"\n        if not self.ds._root_token:\n            raise Forbidden(\"Root token has already been used\")\n        if secrets.compare_digest(token, self.ds._root_token):\n            self.ds._root_token = None\n            response = Response.redirect(self.ds.urls.instance())\n            response.set_cookie(\n                \"ds_actor\", self.ds.sign({\"a\": {\"id\": \"root\"}}, \"actor\")\n            )\n            return response\n        else:\n            raise Forbidden(\"Invalid token\")\n\n\nclass LogoutView(BaseView):\n    name = \"logout\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        if not request.actor:\n            return Response.redirect(self.ds.urls.instance())\n        return await self.render(\n            [\"logout.html\"],\n            request,\n            {\"actor\": request.actor},\n        )\n\n    async def post(self, request):\n        response = Response.redirect(self.ds.urls.instance())\n        response.set_cookie(\"ds_actor\", \"\", expires=0, max_age=0)\n        self.ds.add_message(request, \"You are now logged out\", self.ds.WARNING)\n        return response\n\n\nclass PermissionsDebugView(BaseView):\n    name = \"permissions_debug\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        await self.ds.ensure_permissions(request.actor, [\"view-instance\"])\n        if not await self.ds.permission_allowed(request.actor, \"permissions-debug\"):\n            raise Forbidden(\"Permission denied\")\n        return await self.render(\n            [\"permissions_debug.html\"],\n            request,\n            # list() avoids error if check is performed during template render:\n            {\n                \"permission_checks\": list(reversed(self.ds._permission_checks)),\n                \"permissions\": list(self.ds.permissions.values()),\n            },\n        )\n\n    async def post(self, request):\n        await self.ds.ensure_permissions(request.actor, [\"view-instance\"])\n        if not await self.ds.permission_allowed(request.actor, \"permissions-debug\"):\n            raise Forbidden(\"Permission denied\")\n        vars = await request.post_vars()\n        actor = json.loads(vars[\"actor\"])\n        permission = vars[\"permission\"]\n        resource_1 = vars[\"resource_1\"]\n        resource_2 = vars[\"resource_2\"]\n        resource = []\n        if resource_1:\n            resource.append(resource_1)\n        if resource_2:\n            resource.append(resource_2)\n        resource = tuple(resource)\n        if len(resource) == 1:\n            resource = resource[0]\n        result = await self.ds.permission_allowed(\n            actor, permission, resource, default=\"USE_DEFAULT\"\n        )\n        return Response.json(\n            {\n                \"actor\": actor,\n                \"permission\": permission,\n                \"resource\": resource,\n                \"result\": result,\n            }\n        )\n\n\nclass AllowDebugView(BaseView):\n    name = \"allow_debug\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        errors = []\n        actor_input = request.args.get(\"actor\") or '{\"id\": \"root\"}'\n        try:\n            actor = json.loads(actor_input)\n            actor_input = json.dumps(actor, indent=4)\n        except json.decoder.JSONDecodeError as ex:\n            errors.append(f\"Actor JSON error: {ex}\")\n        allow_input = request.args.get(\"allow\") or '{\"id\": \"*\"}'\n        try:\n            allow = json.loads(allow_input)\n            allow_input = json.dumps(allow, indent=4)\n        except json.decoder.JSONDecodeError as ex:\n            errors.append(f\"Allow JSON error: {ex}\")\n\n        result = None\n        if not errors:\n            result = str(actor_matches_allow(actor, allow))\n\n        return await self.render(\n            [\"allow_debug.html\"],\n            request,\n            {\n                \"result\": result,\n                \"error\": \"\\n\\n\".join(errors) if errors else \"\",\n                \"actor_input\": actor_input,\n                \"allow_input\": allow_input,\n            },\n        )\n\n\nclass MessagesDebugView(BaseView):\n    name = \"messages_debug\"\n    has_json_alternate = False\n\n    async def get(self, request):\n        await self.ds.ensure_permissions(request.actor, [\"view-instance\"])\n        return await self.render([\"messages_debug.html\"], request)\n\n    async def post(self, request):\n        await self.ds.ensure_permissions(request.actor, [\"view-instance\"])\n        post = await request.post_vars()\n        message = post.get(\"message\", \"\")\n        message_type = post.get(\"message_type\") or \"INFO\"\n        assert message_type in (\"INFO\", \"WARNING\", \"ERROR\", \"all\")\n        datasette = self.ds\n        if message_type == \"all\":\n            datasette.add_message(request, message, datasette.INFO)\n            datasette.add_message(request, message, datasette.WARNING)\n            datasette.add_message(request, message, datasette.ERROR)\n        else:\n            datasette.add_message(request, message, getattr(datasette, message_type))\n        return Response.redirect(self.ds.urls.instance())\n\n\nclass CreateTokenView(BaseView):\n    name = \"create_token\"\n    has_json_alternate = False\n\n    def check_permission(self, request):\n        if not self.ds.setting(\"allow_signed_tokens\"):\n            raise Forbidden(\"Signed tokens are not enabled for this Datasette instance\")\n        if not request.actor:\n            raise Forbidden(\"You must be logged in to create a token\")\n        if not request.actor.get(\"id\"):\n            raise Forbidden(\n                \"You must be logged in as an actor with an ID to create a token\"\n            )\n        if request.actor.get(\"token\"):\n            raise Forbidden(\n                \"Token authentication cannot be used to create additional tokens\"\n            )\n\n    async def shared(self, request):\n        self.check_permission(request)\n        # Build list of databases and tables the user has permission to view\n        database_with_tables = []\n        for database in self.ds.databases.values():\n            if database.name in (\"_internal\", \"_memory\"):\n                continue\n            if not await self.ds.permission_allowed(\n                request.actor, \"view-database\", database.name\n            ):\n                continue\n            hidden_tables = await database.hidden_table_names()\n            tables = []\n            for table in await database.table_names():\n                if table in hidden_tables:\n                    continue\n                if not await self.ds.permission_allowed(\n                    request.actor,\n                    \"view-table\",\n                    resource=(database.name, table),\n                ):\n                    continue\n                tables.append({\"name\": table, \"encoded\": tilde_encode(table)})\n            database_with_tables.append(\n                {\n                    \"name\": database.name,\n                    \"encoded\": tilde_encode(database.name),\n                    \"tables\": tables,\n                }\n            )\n        return {\n            \"actor\": request.actor,\n            \"all_permissions\": self.ds.permissions.keys(),\n            \"database_permissions\": [\n                key\n                for key, value in self.ds.permissions.items()\n                if value.takes_database\n            ],\n            \"resource_permissions\": [\n                key\n                for key, value in self.ds.permissions.items()\n                if value.takes_resource\n            ],\n            \"database_with_tables\": database_with_tables,\n        }\n\n    async def get(self, request):\n        self.check_permission(request)\n        return await self.render(\n            [\"create_token.html\"], request, await self.shared(request)\n        )\n\n    async def post(self, request):\n        self.check_permission(request)\n        post = await request.post_vars()\n        errors = []\n        expires_after = None\n        if post.get(\"expire_type\"):\n            duration_string = post.get(\"expire_duration\")\n            if (\n                not duration_string\n                or not duration_string.isdigit()\n                or not int(duration_string) > 0\n            ):\n                errors.append(\"Invalid expire duration\")\n            else:\n                unit = post[\"expire_type\"]\n                if unit == \"minutes\":\n                    expires_after = int(duration_string) * 60\n                elif unit == \"hours\":\n                    expires_after = int(duration_string) * 60 * 60\n                elif unit == \"days\":\n                    expires_after = int(duration_string) * 60 * 60 * 24\n                else:\n                    errors.append(\"Invalid expire duration unit\")\n\n        # Are there any restrictions?\n        restrict_all = []\n        restrict_database = {}\n        restrict_resource = {}\n\n        for key in post:\n            if key.startswith(\"all:\") and key.count(\":\") == 1:\n                restrict_all.append(key.split(\":\")[1])\n            elif key.startswith(\"database:\") and key.count(\":\") == 2:\n                bits = key.split(\":\")\n                database = tilde_decode(bits[1])\n                action = bits[2]\n                restrict_database.setdefault(database, []).append(action)\n            elif key.startswith(\"resource:\") and key.count(\":\") == 3:\n                bits = key.split(\":\")\n                database = tilde_decode(bits[1])\n                resource = tilde_decode(bits[2])\n                action = bits[3]\n                restrict_resource.setdefault(database, {}).setdefault(\n                    resource, []\n                ).append(action)\n\n        token = self.ds.create_token(\n            request.actor[\"id\"],\n            expires_after=expires_after,\n            restrict_all=restrict_all,\n            restrict_database=restrict_database,\n            restrict_resource=restrict_resource,\n        )\n        token_bits = self.ds.unsign(token[len(\"dstok_\") :], namespace=\"token\")\n        context = await self.shared(request)\n        context.update({\"errors\": errors, \"token\": token, \"token_bits\": token_bits})\n        return await self.render([\"create_token.html\"], request, context)\n\n\nclass ApiExplorerView(BaseView):\n    name = \"api_explorer\"\n    has_json_alternate = False\n\n    async def example_links(self, request):\n        databases = []\n        for name, db in self.ds.databases.items():\n            if name == \"_internal\":\n                continue\n            database_visible, _ = await self.ds.check_visibility(\n                request.actor, permissions=[(\"view-database\", name), \"view-instance\"]\n            )\n            if not database_visible:\n                continue\n            tables = []\n            table_names = await db.table_names()\n            for table in table_names:\n                visible, _ = await self.ds.check_visibility(\n                    request.actor,\n                    permissions=[\n                        (\"view-table\", (name, table)),\n                        (\"view-database\", name),\n                        \"view-instance\",\n                    ],\n                )\n                if not visible:\n                    continue\n                table_links = []\n                tables.append({\"name\": table, \"links\": table_links})\n                table_links.append(\n                    {\n                        \"label\": \"Get rows for {}\".format(table),\n                        \"method\": \"GET\",\n                        \"path\": self.ds.urls.table(name, table, format=\"json\"),\n                    }\n                )\n                # If not mutable don't show any write APIs\n                if not db.is_mutable:\n                    continue\n\n                if await self.ds.permission_allowed(\n                    request.actor, \"insert-row\", (name, table)\n                ):\n                    pks = await db.primary_keys(table)\n                    table_links.extend(\n                        [\n                            {\n                                \"path\": self.ds.urls.table(name, table) + \"/-/insert\",\n                                \"method\": \"POST\",\n                                \"label\": \"Insert rows into {}\".format(table),\n                                \"json\": {\n                                    \"rows\": [\n                                        {\n                                            column: None\n                                            for column in await db.table_columns(table)\n                                            if column not in pks\n                                        }\n                                    ]\n                                },\n                            },\n                            {\n                                \"path\": self.ds.urls.table(name, table) + \"/-/upsert\",\n                                \"method\": \"POST\",\n                                \"label\": \"Upsert rows into {}\".format(table),\n                                \"json\": {\n                                    \"rows\": [\n                                        {\n                                            column: None\n                                            for column in await db.table_columns(table)\n                                            if column not in pks\n                                        }\n                                    ]\n                                },\n                            },\n                        ]\n                    )\n                if await self.ds.permission_allowed(\n                    request.actor, \"drop-table\", (name, table)\n                ):\n                    table_links.append(\n                        {\n                            \"path\": self.ds.urls.table(name, table) + \"/-/drop\",\n                            \"label\": \"Drop table {}\".format(table),\n                            \"json\": {\"confirm\": False},\n                            \"method\": \"POST\",\n                        }\n                    )\n            database_links = []\n            if (\n                await self.ds.permission_allowed(request.actor, \"create-table\", name)\n                and db.is_mutable\n            ):\n                database_links.append(\n                    {\n                        \"path\": self.ds.urls.database(name) + \"/-/create\",\n                        \"label\": \"Create table in {}\".format(name),\n                        \"json\": {\n                            \"table\": \"new_table\",\n                            \"columns\": [\n                                {\"name\": \"id\", \"type\": \"integer\"},\n                                {\"name\": \"name\", \"type\": \"text\"},\n                            ],\n                            \"pk\": \"id\",\n                        },\n                        \"method\": \"POST\",\n                    }\n                )\n            if database_links or tables:\n                databases.append(\n                    {\n                        \"name\": name,\n                        \"links\": database_links,\n                        \"tables\": tables,\n                    }\n                )\n        # Sort so that mutable databases are first\n        databases.sort(key=lambda d: not self.ds.databases[d[\"name\"]].is_mutable)\n        return databases\n\n    async def get(self, request):\n        visible, private = await self.ds.check_visibility(\n            request.actor,\n            permissions=[\"view-instance\"],\n        )\n        if not visible:\n            raise Forbidden(\"You do not have permission to view this instance\")\n\n        def api_path(link):\n            return \"/-/api#{}\".format(\n                urllib.parse.urlencode(\n                    {\n                        key: json.dumps(value, indent=2) if key == \"json\" else value\n                        for key, value in link.items()\n                        if key in (\"path\", \"method\", \"json\")\n                    }\n                )\n            )\n\n        return await self.render(\n            [\"api_explorer.html\"],\n            request,\n            {\n                \"example_links\": await self.example_links(request),\n                \"api_path\": api_path,\n                \"private\": private,\n            },\n        )\n", ".. _changelog:\n\n=========\nChangelog\n=========\n\n.. _v1_0_a4:\n\n1.0a4 (2023-08-21)\n------------------\n\nThis alpha fixes a security issue with the ``/-/api`` API explorer. On authenticated Datasette instances (instances protected using plugins such as `datasette-auth-passwords <https://datasette.io/plugins/datasette-auth-passwords>`__) the API explorer interface could reveal the names of databases and tables within the protected instance. The data stored in those tables was not revealed.\n\nFor more information and workarounds, read `the security advisory <https://github.com/simonw/datasette/security/advisories/GHSA-7ch3-7pp7-7cpq>`__. The issue has been present in every previous alpha version of Datasette 1.0: versions 1.0a0, 1.0a1, 1.0a2 and 1.0a3.\n\nAlso in this alpha:\n\n- The new ``datasette plugins --requirements`` option outputs a list of currently installed plugins in Python ``requirements.txt`` format, useful for duplicating that installation elsewhere. (:issue:`2133`)\n- :ref:`canned_queries_writable` can now define a ``on_success_message_sql`` field in their configuration, containing a SQL query that should be executed upon successful completion of the write operation in order to generate a message to be shown to the user. (:issue:`2138`)\n- The automatically generated border color for a database is now shown in more places around the application. (:issue:`2119`)\n- Every instance of example shell script code in the documentation should now include a working copy button, free from additional syntax. (:issue:`2140`)\n\n.. _v1_0_a3:\n\n1.0a3 (2023-08-09)\n------------------\n\nThis alpha release previews the updated design for Datasette's default JSON API. (:issue:`782`)\n\nThe new :ref:`default JSON representation <json_api_default>` for both table pages (``/dbname/table.json``) and arbitrary SQL queries (``/dbname.json?sql=...``) is now shaped like this:\n\n.. code-block:: json\n\n    {\n      \"ok\": true,\n      \"rows\": [\n        {\n          \"id\": 3,\n          \"name\": \"Detroit\"\n        },\n        {\n          \"id\": 2,\n          \"name\": \"Los Angeles\"\n        },\n        {\n          \"id\": 4,\n          \"name\": \"Memnonia\"\n        },\n        {\n          \"id\": 1,\n          \"name\": \"San Francisco\"\n        }\n      ],\n      \"truncated\": false\n    }\n\nTables will include an additional ``\"next\"`` key for pagination, which can be passed to ``?_next=`` to fetch the next page of results.\n\nThe various ``?_shape=`` options continue to work as before - see :ref:`json_api_shapes` for details.\n\nA new ``?_extra=`` mechanism is available for tables, but has not yet been stabilized or documented. Details on that are available in :issue:`262`.\n\nSmaller changes\n~~~~~~~~~~~~~~~\n\n- Datasette documentation now shows YAML examples for :ref:`metadata` by default, with a tab interface for switching to JSON. (:issue:`1153`)\n- :ref:`plugin_register_output_renderer` plugins now have access to ``error`` and ``truncated`` arguments, allowing them to display error messages and take into account truncated results. (:issue:`2130`)\n- ``render_cell()`` plugin hook now also supports an optional ``request`` argument. (:issue:`2007`)\n- New ``Justfile`` to support development workflows for Datasette using `Just <https://github.com/casey/just>`__.\n- ``datasette.render_template()`` can now accepts a ``datasette.views.Context`` subclass as an alternative to a dictionary. (:issue:`2127`)\n- ``datasette install -e path`` option for editable installations, useful while developing plugins. (:issue:`2106`)\n- When started with the ``--cors`` option Datasette now serves an ``Access-Control-Max-Age: 3600`` header, ensuring CORS OPTIONS requests are repeated no more than once an hour. (:issue:`2079`)\n- Fixed a bug where the ``_internal`` database could display ``None`` instead of ``null`` for in-memory databases. (:issue:`1970`)\n\n.. _v0_64_2:\n\n0.64.2 (2023-03-08)\n-------------------\n\n- Fixed a bug with ``datasette publish cloudrun`` where deploys all used the same Docker image tag. This was mostly inconsequential as the service is deployed as soon as the image has been pushed to the registry, but could result in the incorrect image being deployed if two different deploys for two separate services ran at exactly the same time. (:issue:`2036`)\n\n.. _v0_64_1:\n\n0.64.1 (2023-01-11)\n-------------------\n\n- Documentation now links to a current source of information for installing Python 3. (:issue:`1987`)\n- Incorrectly calling the Datasette constructor using ``Datasette(\"path/to/data.db\")`` instead of ``Datasette([\"path/to/data.db\"])`` now returns a useful error message. (:issue:`1985`)\n\n.. _v0_64:\n\n0.64 (2023-01-09)\n-----------------\n\n- Datasette now **strongly recommends against allowing arbitrary SQL queries if you are using SpatiaLite**. SpatiaLite includes SQL functions that could cause the Datasette server to crash. See :ref:`spatialite` for more details.\n- New :ref:`setting_default_allow_sql` setting, providing an easier way to disable all arbitrary SQL execution by end users: ``datasette --setting default_allow_sql off``. See also :ref:`authentication_permissions_execute_sql`. (:issue:`1409`)\n- `Building a location to time zone API with SpatiaLite <https://datasette.io/tutorials/spatialite>`__ is a new Datasette tutorial showing how to safely use SpatiaLite to create a location to time zone API.\n- New documentation about :ref:`how to debug problems loading SQLite extensions <installation_extensions>`. The error message shown when an extension cannot be loaded has also been improved. (:issue:`1979`)\n- Fixed an accessibility issue: the ``<select>`` elements in the table filter form now show an outline when they are currently focused. (:issue:`1771`)\n\n.. _v0_63_3:\n\n0.63.3 (2022-12-17)\n-------------------\n\n- Fixed a bug where ``datasette --root``, when running in Docker, would only output the URL to sign in root when the server shut down, not when it started up. (:issue:`1958`)\n- You no longer need to ensure ``await datasette.invoke_startup()`` has been called in order for Datasette to start correctly serving requests - this is now handled automatically the first time the server receives a request. This fixes a bug experienced when Datasette is served directly by an ASGI application server such as Uvicorn or Gunicorn. It also fixes a bug with the `datasette-gunicorn <https://datasette.io/plugins/datasette-gunicorn>`__ plugin. (:issue:`1955`)\n\n.. _v1_0_a2:\n\n1.0a2 (2022-12-14)\n------------------\n\nThe third Datasette 1.0 alpha release adds upsert support to the JSON API, plus the ability to specify finely grained permissions when creating an API token.\n\nSee `Datasette 1.0a2: Upserts and finely grained permissions <https://simonwillison.net/2022/Dec/15/datasette-1a2/>`__ for an extended, annotated version of these release notes.\n\n- New ``/db/table/-/upsert`` API, :ref:`documented here <TableUpsertView>`. upsert is an update-or-insert: existing rows will have specified keys updated, but if no row matches the incoming primary key a brand new row will be inserted instead. (:issue:`1878`)\n- New :ref:`plugin_register_permissions` plugin hook. Plugins can now register named permissions, which will then be listed in various interfaces that show available permissions. (:issue:`1940`)\n- The ``/db/-/create`` API for :ref:`creating a table <TableCreateView>` now accepts ``\"ignore\": true`` and ``\"replace\": true`` options when called with the ``\"rows\"`` property that creates a new table based on an example set of rows. This means the API can be called multiple times with different rows, setting rules for what should happen if a primary key collides with an existing row. (:issue:`1927`)\n- Arbitrary permissions can now be configured at the instance, database and resource (table, SQL view or canned query) level in Datasette's :ref:`metadata` JSON and YAML files. The new ``\"permissions\"`` key can be used to specify which actors should have which permissions. See :ref:`authentication_permissions_other` for details. (:issue:`1636`)\n- The ``/-/create-token`` page can now be used to create API tokens which are restricted to just a subset of actions, including against specific databases or resources. See :ref:`CreateTokenView` for details. (:issue:`1947`)\n- Likewise, the ``datasette create-token`` CLI command can now create tokens with :ref:`a subset of permissions <authentication_cli_create_token_restrict>`. (:issue:`1855`)\n- New :ref:`datasette.create_token() API method <datasette_create_token>` for programmatically creating signed API tokens. (:issue:`1951`)\n- ``/db/-/create`` API now requires actor to have ``insert-row`` permission in order to use the ``\"row\"`` or ``\"rows\"`` properties. (:issue:`1937`)\n\n.. _v1_0_a1:\n\n1.0a1 (2022-12-01)\n------------------\n\n- Write APIs now serve correct CORS headers if Datasette is started in ``--cors`` mode. See the full list of :ref:`CORS headers <json_api>` in the documentation. (:issue:`1922`)\n- Fixed a bug where the ``_memory`` database could be written to even though writes were not persisted. (:issue:`1917`)\n- The https://latest.datasette.io/ demo instance now includes an ``ephemeral`` database which can be used to test Datasette's write APIs, using the new `datasette-ephemeral-tables <https://datasette.io/plugins/datasette-ephemeral-tables>`_ plugin to drop any created tables after five minutes. This database is only available if you sign in as the root user using the link on the homepage. (:issue:`1915`)\n- Fixed a bug where hitting the write endpoints with a ``GET`` request returned a 500 error. It now returns a 405 (method not allowed) error instead. (:issue:`1916`)\n- The list of endpoints in the API explorer now lists mutable databases first. (:issue:`1918`)\n- The ``\"ignore\": true`` and ``\"replace\": true`` options for the insert API are :ref:`now documented <TableInsertView>`. (:issue:`1924`)\n\n.. _v1_0_a0:\n\n1.0a0 (2022-11-29)\n------------------\n\nThis first alpha release of Datasette 1.0 introduces a brand new collection of APIs for writing to the database (:issue:`1850`), as well as a new API token mechanism baked into Datasette core. Previously, API tokens have only been supported by installing additional plugins.\n\nThis is very much a preview: expect many more backwards incompatible API changes prior to the full 1.0 release.\n\nFeedback enthusiastically welcomed, either through `issue comments <https://github.com/simonw/datasette/issues/1850>`__ or via the `Datasette Discord <https://datasette.io/discord>`__ community.\n\nSigned API tokens\n~~~~~~~~~~~~~~~~~\n\n- New ``/-/create-token`` page allowing authenticated users to create signed API tokens that can act on their behalf, see :ref:`CreateTokenView`. (:issue:`1852`)\n- New ``datasette create-token`` command for creating tokens from the command line: :ref:`authentication_cli_create_token`.\n- New :ref:`setting_allow_signed_tokens` setting which can be used to turn off signed token support. (:issue:`1856`)\n- New :ref:`setting_max_signed_tokens_ttl` setting for restricting the maximum allowed duration of a signed token. (:issue:`1858`)\n\nWrite API\n~~~~~~~~~\n\n- New API explorer at ``/-/api`` for trying out the API. (:issue:`1871`)\n- ``/db/-/create`` API for :ref:`TableCreateView`. (:issue:`1882`)\n- ``/db/table/-/insert`` API for :ref:`TableInsertView`. (:issue:`1851`)\n- ``/db/table/-/drop`` API for :ref:`TableDropView`. (:issue:`1874`)\n- ``/db/table/pk/-/update`` API for :ref:`RowUpdateView`. (:issue:`1863`)\n- ``/db/table/pk/-/delete`` API for :ref:`RowDeleteView`. (:issue:`1864`)\n\n.. _v0_63_2:\n\n0.63.2 (2022-11-18)\n-------------------\n\n- Fixed a bug in ``datasette publish heroku`` where deployments failed due to an older version of Python being requested. (:issue:`1905`)\n- New ``datasette publish heroku --generate-dir <dir>`` option for generating a Heroku deployment directory without deploying it.\n\n.. _v0_63_1:\n\n0.63.1 (2022-11-10)\n-------------------\n\n- Fixed a bug where Datasette's table filter form would not redirect correctly when run behind a proxy using the :ref:`base_url <setting_base_url>` setting. (:issue:`1883`)\n- SQL query is now shown wrapped in a ``<textarea>`` if a query exceeds a time limit. (:issue:`1876`)\n- Fixed an intermittent \"Too many open files\" error while running the test suite. (:issue:`1843`)\n- New :ref:`database_close` internal method.\n\n.. _v0_63:\n\n0.63 (2022-10-27)\n-----------------\n\nSee `Datasette 0.63: The annotated release notes <https://simonwillison.net/2022/Oct/27/datasette-0-63/>`__ for more background on the changes in this release.\n\nFeatures\n~~~~~~~~\n\n- Now tested against Python 3.11. Docker containers used by ``datasette publish`` and ``datasette package`` both now use that version of Python. (:issue:`1853`)\n- ``--load-extension`` option now supports entrypoints. Thanks, Alex Garcia. (`#1789 <https://github.com/simonw/datasette/pull/1789>`__)\n- Facet size can now be set per-table with the new ``facet_size`` table metadata option. (:issue:`1804`)\n- The :ref:`setting_truncate_cells_html` setting now also affects long URLs in columns. (:issue:`1805`)\n- The non-JavaScript SQL editor textarea now increases height to fit the SQL query. (:issue:`1786`)\n- Facets are now displayed with better line-breaks in long values. Thanks, Daniel Rech. (`#1794 <https://github.com/simonw/datasette/pull/1794>`__)\n- The ``settings.json`` file used in :ref:`config_dir` is now validated on startup. (:issue:`1816`)\n- SQL queries can now include leading SQL comments, using ``/* ... */`` or ``-- ...`` syntax. Thanks,  Charles Nepote. (:issue:`1860`)\n- SQL query is now re-displayed when terminated with a time limit error. (:issue:`1819`)\n- The :ref:`inspect data <performance_inspect>` mechanism is now used to speed up server startup - thanks, Forest Gregg. (:issue:`1834`)\n- In :ref:`config_dir` databases with filenames ending in ``.sqlite`` or ``.sqlite3`` are now automatically added to the Datasette instance. (:issue:`1646`)\n- Breadcrumb navigation display now respects the current user's permissions. (:issue:`1831`)\n\nPlugin hooks and internals\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- The :ref:`plugin_hook_prepare_jinja2_environment` plugin hook now accepts an optional ``datasette`` argument. Hook implementations can also now return an ``async`` function which will be awaited automatically. (:issue:`1809`)\n- ``Database(is_mutable=)`` now defaults to ``True``. (:issue:`1808`)\n- The :ref:`datasette.check_visibility() <datasette_check_visibility>` method now accepts an optional ``permissions=`` list, allowing it to take multiple permissions into account at once when deciding if something should be shown as public or private. This has been used to correctly display padlock icons in more places in the Datasette interface. (:issue:`1829`)\n- Datasette no longer enforces upper bounds on its dependencies. (:issue:`1800`)\n\nDocumentation\n~~~~~~~~~~~~~\n\n- New tutorial: `Cleaning data with sqlite-utils and Datasette <https://datasette.io/tutorials/clean-data>`__.\n- Screenshots in the documentation are now maintained using `shot-scraper <https://shot-scraper.datasette.io/>`__, as described in `Automating screenshots for the Datasette documentation using shot-scraper <https://simonwillison.net/2022/Oct/14/automating-screenshots/>`__. (:issue:`1844`)\n- More detailed command descriptions on the :ref:`CLI reference <cli_reference>` page. (:issue:`1787`)\n- New documentation on :ref:`deploying_openrc` - thanks, Adam Simpson. (`#1825 <https://github.com/simonw/datasette/pull/1825>`__)\n\n.. _v0_62:\n\n0.62 (2022-08-14)\n-------------------\n\nDatasette can now run entirely in your browser using WebAssembly. Try out `Datasette Lite <https://lite.datasette.io/>`__, take a look `at the code <https://github.com/simonw/datasette-lite>`__ or read more about it in `Datasette Lite: a server-side Python web application running in a browser <https://simonwillison.net/2022/May/4/datasette-lite/>`__.\n\nDatasette now has a `Discord community <https://discord.gg/ktd74dm5mw>`__ for questions and discussions about Datasette and its ecosystem of projects.\n\nFeatures\n~~~~~~~~\n\n- Datasette is now compatible with `Pyodide <https://pyodide.org/>`__.  This is the enabling technology behind `Datasette Lite <https://lite.datasette.io/>`__. (:issue:`1733`)\n- Database file downloads now implement conditional GET using ETags. (:issue:`1739`)\n- HTML for facet results and suggested results has been extracted out into new templates ``_facet_results.html`` and ``_suggested_facets.html``. Thanks, M. Nasimul Haque. (`#1759 <https://github.com/simonw/datasette/pull/1759>`__)\n- Datasette now runs some SQL queries in parallel. This has limited impact on performance, see `this research issue <https://github.com/simonw/datasette/issues/1727>`__ for details.\n- New ``--nolock`` option for ignoring file locks when opening read-only databases. (:issue:`1744`)\n- Spaces in the database names in URLs are now encoded as ``+`` rather than ``~20``. (:issue:`1701`)\n- ``<Binary: 2427344 bytes>`` is now displayed as ``<Binary: 2,427,344 bytes>`` and is accompanied by tooltip showing \"2.3MB\". (:issue:`1712`)\n- The base Docker image used by ``datasette publish cloudrun``, ``datasette package`` and the `official Datasette image <https://hub.docker.com/datasetteproject/datasette>`__ has been upgraded to ``3.10.6-slim-bullseye``.  (:issue:`1768`)\n- Canned writable queries against immutable databases now show a warning message. (:issue:`1728`)\n- ``datasette publish cloudrun`` has a new ``--timeout`` option which can be used to increase the time limit applied by the Google Cloud build environment. Thanks, Tim Sherratt. (`#1717 <https://github.com/simonw/datasette/pull/1717>`__)\n- ``datasette publish cloudrun`` has new ``--min-instances`` and ``--max-instances`` options. (:issue:`1779`)\n\nPlugin hooks\n~~~~~~~~~~~~\n\n- New plugin hook: :ref:`handle_exception() <plugin_hook_handle_exception>`, for custom handling of exceptions caught by Datasette. (:issue:`1770`)\n- The :ref:`render_cell() <plugin_hook_render_cell>` plugin hook is now also passed a ``row`` argument, representing the ``sqlite3.Row`` object that is being rendered. (:issue:`1300`)\n- The :ref:`configuration directory <config_dir>` is now stored in ``datasette.config_dir``, making it available to plugins. Thanks, Chris Amico. (`#1766 <https://github.com/simonw/datasette/pull/1766>`__)\n\nBug fixes\n~~~~~~~~~\n\n- Don't show the facet option in the cog menu if faceting is not allowed. (:issue:`1683`)\n- ``?_sort`` and ``?_sort_desc`` now work if the column that is being sorted has been excluded from the query using ``?_col=`` or ``?_nocol=``. (:issue:`1773`)\n- Fixed bug where ``?_sort_desc`` was duplicated in the URL every time the Apply button was clicked. (:issue:`1738`)\n\nDocumentation\n~~~~~~~~~~~~~\n\n- Examples in the documentation now include a copy-to-clipboard button. (:issue:`1748`)\n- Documentation now uses the `Furo <https://github.com/pradyunsg/furo>`__ Sphinx theme. (:issue:`1746`)\n- Code examples in the documentation are now all formatted using Black. (:issue:`1718`)\n- ``Request.fake()`` method is now documented, see :ref:`internals_request`.\n- New documentation for plugin authors: :ref:`testing_plugins_register_in_test`. (:issue:`903`)\n\n.. _v0_61_1:\n\n0.61.1 (2022-03-23)\n-------------------\n\n- Fixed a bug where databases with a different route from their name (as used by the `datasette-hashed-urls plugin <https://datasette.io/plugins/datasette-hashed-urls>`__) returned errors when executing custom SQL queries. (:issue:`1682`)\n\n.. _v0_61:\n\n0.61 (2022-03-23)\n-----------------\n\nIn preparation for Datasette 1.0, this release includes two potentially backwards-incompatible changes. Hashed URL mode has been moved to a separate plugin, and the way Datasette generates URLs to databases and tables with special characters in their name such as ``/`` and ``.`` has changed.\n\nDatasette also now requires Python 3.7 or higher.\n\n- URLs within Datasette now use a different encoding scheme for tables or databases that include \"special\" characters outside of the range of ``a-zA-Z0-9_-``. This scheme is explained here: :ref:`internals_tilde_encoding`. (:issue:`1657`)\n- Removed hashed URL mode from Datasette. The new ``datasette-hashed-urls`` plugin can be used to achieve the same result, see :ref:`performance_hashed_urls` for details. (:issue:`1661`)\n- Databases can now have a custom path within the Datasette instance that is independent of the database name, using the ``db.route`` property. (:issue:`1668`)\n- Datasette is now covered by a `Code of Conduct <https://github.com/simonw/datasette/blob/main/CODE_OF_CONDUCT.md>`__. (:issue:`1654`)\n- Python 3.6 is no longer supported. (:issue:`1577`)\n- Tests now run against Python 3.11-dev. (:issue:`1621`)\n- New :ref:`datasette.ensure_permissions(actor, permissions) <datasette_ensure_permissions>` internal method for checking multiple permissions at once. (:issue:`1675`)\n- New :ref:`datasette.check_visibility(actor, action, resource=None) <datasette_check_visibility>` internal method for checking if a user can see a resource that would otherwise be invisible to unauthenticated users. (:issue:`1678`)\n- Table and row HTML pages now include a ``<link rel=\"alternate\" type=\"application/json+datasette\" href=\"...\">`` element and return a ``Link: URL; rel=\"alternate\"; type=\"application/json+datasette\"`` HTTP header pointing to the JSON version of those pages. (:issue:`1533`)\n- ``Access-Control-Expose-Headers: Link`` is now added to the CORS headers, allowing remote JavaScript to access that header.\n- Canned queries are now shown at the top of the database page, directly below the SQL editor. Previously they were shown at the bottom, below the list of tables. (:issue:`1612`)\n- Datasette now has a default favicon. (:issue:`1603`)\n- ``sqlite_stat`` tables are now hidden by default. (:issue:`1587`)\n- SpatiaLite tables ``data_licenses``, ``KNN`` and ``KNN2`` are now hidden by default. (:issue:`1601`)\n- SQL query tracing mechanism now works for queries executed in ``asyncio`` sub-tasks, such as those created by ``asyncio.gather()``. (:issue:`1576`)\n- :ref:`internals_tracer` mechanism is now documented.\n- Common Datasette symbols can now be imported directly from the top-level ``datasette`` package, see :ref:`internals_shortcuts`. Those symbols are ``Response``, ``Forbidden``, ``NotFound``, ``hookimpl``, ``actor_matches_allow``. (:issue:`957`)\n- ``/-/versions`` page now returns additional details for libraries used by SpatiaLite. (:issue:`1607`)\n- Documentation now links to the `Datasette Tutorials <https://datasette.io/tutorials>`__.\n- Datasette will now also look for SpatiaLite in ``/opt/homebrew`` - thanks, Dan Peterson. (`#1649 <https://github.com/simonw/datasette/pull/1649>`__)\n- Fixed bug where :ref:`custom pages <custom_pages>` did not work on Windows. Thanks, Robert Christie. (:issue:`1545`)\n- Fixed error caused when a table had a column named ``n``. (:issue:`1228`)\n\n.. _v0_60_2:\n\n0.60.2 (2022-02-07)\n-------------------\n\n- Fixed a bug where Datasette would open the same file twice with two different database names if you ran ``datasette file.db file.db``. (:issue:`1632`)\n\n.. _v0_60_1:\n\n0.60.1 (2022-01-20)\n-------------------\n\n- Fixed a bug where installation on Python 3.6 stopped working due to a change to an underlying dependency. This release can now be installed on Python 3.6, but is the last release of Datasette that will support anything less than Python 3.7. (:issue:`1609`)\n\n.. _v0_60:\n\n0.60 (2022-01-13)\n-----------------\n\nPlugins and internals\n~~~~~~~~~~~~~~~~~~~~~\n\n- New plugin hook: :ref:`plugin_hook_filters_from_request`, which runs on the table page and can be used to support new custom query string parameters that modify the SQL query. (:issue:`473`)\n- Added two additional methods for writing to the database: :ref:`database_execute_write_script` and :ref:`database_execute_write_many`. (:issue:`1570`)\n- The :ref:`db.execute_write() <database_execute_write>` internal method now defaults to blocking until the write operation has completed. Previously it defaulted to queuing the write and then continuing to run code while the write was in the queue. (:issue:`1579`)\n- Database write connections now execute the :ref:`plugin_hook_prepare_connection` plugin hook. (:issue:`1564`)\n- The ``Datasette()`` constructor no longer requires the ``files=`` argument, and is now documented at :ref:`internals_datasette`. (:issue:`1563`)\n- The tracing feature now traces write queries, not just read queries. (:issue:`1568`)\n- The query string variables exposed by ``request.args`` will now include blank strings for arguments such as ``foo`` in ``?foo=&bar=1`` rather than ignoring those parameters entirely. (:issue:`1551`)\n\nFaceting\n~~~~~~~~\n\n- The number of unique values in a facet is now always displayed. Previously it was only displayed if the user specified ``?_facet_size=max``. (:issue:`1556`)\n- Facets of type ``date`` or ``array`` can now be configured in ``metadata.json``, see :ref:`facets_metadata`. Thanks, David Larlet. (:issue:`1552`)\n- New ``?_nosuggest=1`` parameter for table views, which disables facet suggestion. (:issue:`1557`)\n- Fixed bug where ``?_facet_array=tags&_facet=tags`` would only display one of the two selected facets. (:issue:`625`)\n\nOther small fixes\n~~~~~~~~~~~~~~~~~\n\n- Made several performance improvements to the database schema introspection code that runs when Datasette first starts up. (:issue:`1555`)\n- Label columns detected for foreign keys are now case-insensitive, so ``Name`` or ``TITLE`` will be detected in the same way as ``name`` or ``title``. (:issue:`1544`)\n- Upgraded Pluggy dependency to 1.0. (:issue:`1575`)\n- Now using `Plausible analytics <https://plausible.io/>`__ for the Datasette documentation.\n- ``explain query plan`` is now allowed with varying amounts of whitespace in the query. (:issue:`1588`)\n- New :ref:`cli_reference` page showing the output of ``--help`` for each of the ``datasette`` sub-commands. This lead to several small improvements to the help copy. (:issue:`1594`)\n- Fixed bug where writable canned queries could not be used with custom templates.  (:issue:`1547`)\n- Improved fix for a bug where columns with a underscore prefix could result in unnecessary hidden form fields. (:issue:`1527`)\n\n.. _v0_59_4:\n\n0.59.4 (2021-11-29)\n-------------------\n\n- Fixed bug where columns with a leading underscore could not be removed from the interactive filters list. (:issue:`1527`)\n- Fixed bug where columns with a leading underscore were not correctly linked to by the \"Links from other tables\" interface on the row page. (:issue:`1525`)\n- Upgraded dependencies ``aiofiles``, ``black`` and ``janus``.\n\n.. _v0_59_3:\n\n0.59.3 (2021-11-20)\n-------------------\n\n- Fixed numerous bugs when running Datasette :ref:`behind a proxy <deploying_proxy>` with a prefix URL path using the :ref:`setting_base_url` setting. A live demo of this mode is now available at `datasette-apache-proxy-demo.datasette.io/prefix/ <https://datasette-apache-proxy-demo.datasette.io/prefix/>`__. (:issue:`1519`, :issue:`838`)\n- ``?column__arraycontains=`` and ``?column__arraynotcontains=`` table parameters now also work against SQL views. (:issue:`448`)\n- ``?_facet_array=column`` no longer returns incorrect counts if columns contain the same value more than once.\n\n.. _v0_59_2:\n\n0.59.2 (2021-11-13)\n-------------------\n\n- Column names with a leading underscore now work correctly when used as a facet. (:issue:`1506`)\n- Applying ``?_nocol=`` to a column no longer removes that column from the filtering interface. (:issue:`1503`)\n- Official Datasette Docker container now uses Debian Bullseye as the base image. (:issue:`1497`)\n- Datasette is four years old today! Here's the `original release announcement <https://simonwillison.net/2017/Nov/13/datasette/>`__ from 2017.\n\n.. _v0_59_1:\n\n0.59.1 (2021-10-24)\n-------------------\n\n- Fix compatibility with Python 3.10. (:issue:`1482`)\n- Documentation on how to use :ref:`sql_parameters` with integer and floating point values. (:issue:`1496`)\n\n.. _v0_59:\n\n0.59 (2021-10-14)\n-----------------\n\n- Columns can now have associated metadata descriptions in ``metadata.json``, see :ref:`metadata_column_descriptions`. (:issue:`942`)\n- New :ref:`register_commands() <plugin_hook_register_commands>` plugin hook allows plugins to register additional Datasette CLI commands, e.g. ``datasette mycommand file.db``. (:issue:`1449`)\n- Adding ``?_facet_size=max`` to a table page now shows the number of unique values in each facet. (:issue:`1423`)\n- Upgraded dependency `httpx 0.20 <https://github.com/encode/httpx/releases/tag/0.20.0>`__ - the undocumented ``allow_redirects=`` parameter to :ref:`internals_datasette_client` is now ``follow_redirects=``, and defaults to ``False`` where it previously defaulted to ``True``. (:issue:`1488`)\n- The ``--cors`` option now causes Datasette to return the ``Access-Control-Allow-Headers: Authorization`` header, in addition to ``Access-Control-Allow-Origin: *``. (`#1467 <https://github.com/simonw/datasette/pull/1467>`__)\n- Code that figures out which named parameters a SQL query takes in order to display form fields for them is no longer confused by strings that contain colon characters. (:issue:`1421`)\n- Renamed ``--help-config`` option to ``--help-settings``. (:issue:`1431`)\n- ``datasette.databases`` property is now a documented API. (:issue:`1443`)\n- The ``base.html`` template now wraps everything other than the ``<footer>`` in a ``<div class=\"not-footer\">`` element, to help with advanced CSS customization. (:issue:`1446`)\n- The :ref:`render_cell() <plugin_hook_render_cell>` plugin hook can now return an awaitable function. This means the hook can execute SQL queries. (:issue:`1425`)\n- :ref:`plugin_register_routes` plugin hook now accepts an optional ``datasette`` argument. (:issue:`1404`)\n- New ``hide_sql`` canned query option for defaulting to hiding the SQL query used by a canned query, see :ref:`canned_queries_options`. (:issue:`1422`)\n- New ``--cpu`` option for :ref:`datasette publish cloudrun <publish_cloud_run>`. (:issue:`1420`)\n- If `Rich <https://github.com/willmcgugan/rich>`__ is installed in the same virtual environment as Datasette, it will be used to provide enhanced display of error tracebacks on the console. (:issue:`1416`)\n- ``datasette.utils`` :ref:`internals_utils_parse_metadata` function, used by the new `datasette-remote-metadata plugin <https://datasette.io/plugins/datasette-remote-metadata>`__, is now a documented API. (:issue:`1405`)\n- Fixed bug where ``?_next=x&_sort=rowid`` could throw an error. (:issue:`1470`)\n- Column cog menu no longer shows the option to facet by a column that is already selected by the default facets in metadata. (:issue:`1469`)\n\n.. _v0_58_1:\n\n0.58.1 (2021-07-16)\n-------------------\n\n- Fix for an intermittent race condition caused by the ``refresh_schemas()`` internal function. (:issue:`1231`)\n\n.. _v0_58:\n\n0.58 (2021-07-14)\n-----------------\n\n- New ``datasette --uds /tmp/datasette.sock`` option for binding Datasette to a Unix domain socket, see :ref:`proxy documentation <deploying_proxy>` (:issue:`1388`)\n- ``\"searchmode\": \"raw\"`` table metadata option for defaulting a table to executing SQLite full-text search syntax without first escaping it, see :ref:`full_text_search_advanced_queries`. (:issue:`1389`)\n- New plugin hook: :ref:`plugin_hook_get_metadata`, for returning custom metadata for an instance, database or table. Thanks, Brandon Roberts! (:issue:`1384`)\n- New plugin hook: :ref:`plugin_hook_skip_csrf`, for opting out of CSRF protection based on the incoming request. (:issue:`1377`)\n- The :ref:`menu_links() <plugin_hook_menu_links>`, :ref:`table_actions() <plugin_hook_table_actions>` and :ref:`database_actions() <plugin_hook_database_actions>` plugin hooks all gained a new optional ``request`` argument providing access to the current request. (:issue:`1371`)\n- Major performance improvement for Datasette faceting. (:issue:`1394`)\n- Improved documentation for :ref:`deploying_proxy` to recommend using ``ProxyPreservehost On`` with Apache. (:issue:`1387`)\n- ``POST`` requests to endpoints that do not support that HTTP verb now return a 405 error.\n- ``db.path`` can now be provided as a ``pathlib.Path`` object, useful when writing unit tests for plugins. Thanks, Chris Amico. (:issue:`1365`)\n\n.. _v0_57_1:\n\n0.57.1 (2021-06-08)\n-------------------\n\n- Fixed visual display glitch with global navigation menu. (:issue:`1367`)\n- No longer truncates the list of table columns displayed on the ``/database`` page. (:issue:`1364`)\n\n.. _v0_57:\n\n0.57 (2021-06-05)\n-----------------\n\n.. warning::\n    This release fixes a `reflected cross-site scripting <https://owasp.org/www-community/attacks/xss/#reflected-xss-attacks>`__ security hole with the ``?_trace=1`` feature. You should upgrade to this version, or to Datasette 0.56.1, as soon as possible. (:issue:`1360`)\n\nIn addition to the security fix, this release includes ``?_col=`` and ``?_nocol=`` options for controlling which columns are displayed for a table, ``?_facet_size=`` for increasing the number of facet results returned, re-display of your SQL query should an error occur and numerous bug fixes.\n\nNew features\n~~~~~~~~~~~~\n\n- If an error occurs while executing a user-provided SQL query, that query is now re-displayed in an editable form along with the error message. (:issue:`619`)\n-  New ``?_col=`` and ``?_nocol=`` parameters to show and hide columns in a table, plus an interface for hiding and showing columns in the column cog menu. (:issue:`615`)\n- A new ``?_facet_size=`` parameter for customizing the number of facet results returned on a table or view page. (:issue:`1332`)\n- ``?_facet_size=max`` sets that to the maximum, which defaults to 1,000 and is controlled by the the :ref:`setting_max_returned_rows` setting. If facet results are truncated the \u2026 at the bottom of the facet list now links to this parameter. (:issue:`1337`)\n- ``?_nofacet=1`` option to disable all facet calculations on a page, used as a performance optimization for CSV exports and ``?_shape=array/object``. (:issue:`1349`, :issue:`263`)\n- ``?_nocount=1`` option to disable full query result counts. (:issue:`1353`)\n- ``?_trace=1`` debugging option is now controlled by the new :ref:`setting_trace_debug` setting, which is turned off by default. (:issue:`1359`)\n\nBug fixes and other improvements\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- :ref:`custom_pages` now work correctly when combined with the :ref:`setting_base_url` setting. (:issue:`1238`)\n- Fixed intermittent error displaying the index page when the user did not have permission to access one of the tables. Thanks, Guy Freeman. (:issue:`1305`)\n- Columns with the name \"Link\" are no longer incorrectly displayed in bold. (:issue:`1308`)\n- Fixed error caused by tables with a single quote in their names. (:issue:`1257`)\n- Updated dependencies: ``pytest-asyncio``, ``Black``, ``jinja2``, ``aiofiles``, ``click``, and ``itsdangerous``.\n- The official Datasette Docker image now supports ``apt-get install``. (:issue:`1320`)\n- The Heroku runtime used by ``datasette publish heroku`` is now ``python-3.8.10``.\n\n.. _v0_56_1:\n\n0.56.1 (2021-06-05)\n-------------------\n\n.. warning::\n    This release fixes a `reflected cross-site scripting <https://owasp.org/www-community/attacks/xss/#reflected-xss-attacks>`__ security hole with the ``?_trace=1`` feature. You should upgrade to this version, or to Datasette 0.57, as soon as possible. (:issue:`1360`)\n\n.. _v0_56:\n\n0.56 (2021-03-28)\n-----------------\n\nDocumentation improvements, bug fixes and support for SpatiaLite 5.\n\n- The SQL editor can now be resized by dragging a handle. (:issue:`1236`)\n- Fixed a bug with JSON faceting and the ``__arraycontains`` filter caused by tables with spaces in their names. (:issue:`1239`)\n- Upgraded ``httpx`` dependency. (:issue:`1005`)\n- JSON faceting is now suggested even if a column contains blank strings. (:issue:`1246`)\n- New :ref:`datasette.add_memory_database() <datasette_add_memory_database>` method. (:issue:`1247`)\n- The :ref:`Response.asgi_send() <internals_response_asgi_send>` method is now documented. (:issue:`1266`)\n- The official Datasette Docker image now bundles SpatiaLite version 5. (:issue:`1278`)\n- Fixed a ``no such table: pragma_database_list`` bug when running Datasette against SQLite versions prior to SQLite 3.16.0. (:issue:`1276`)\n- HTML lists displayed in table cells are now styled correctly. Thanks, Bob Whitelock. (:issue:`1141`, `#1252 <https://github.com/simonw/datasette/pull/1252>`__)\n- Configuration directory mode now correctly serves immutable databases that are listed in ``inspect-data.json``. Thanks Campbell Allen and Frankie Robertson. (`#1031 <https://github.com/simonw/datasette/pull/1031>`__, `#1229 <https://github.com/simonw/datasette/pull/1229>`__)\n\n.. _v0_55:\n\n0.55 (2021-02-18)\n-----------------\n\nSupport for cross-database SQL queries and built-in support for serving via HTTPS.\n\n- The new ``--crossdb`` command-line option causes Datasette to attach up to ten database files to the same ``/_memory`` database connection. This enables cross-database SQL queries, including the ability to use joins and unions to combine data from tables that exist in different database files. See :ref:`cross_database_queries` for details. (:issue:`283`)\n- ``--ssl-keyfile`` and ``--ssl-certfile`` options can be used to specify a TLS certificate, allowing Datasette to serve traffic over ``https://`` without needing to run it behind a separate proxy. (:issue:`1221`)\n- The ``/:memory:`` page has been renamed (and redirected) to ``/_memory`` for consistency with the new ``/_internal`` database introduced in Datasette 0.54. (:issue:`1205`)\n- Added plugin testing documentation on :ref:`testing_plugins_pdb`. (:issue:`1207`)\n- The `official Datasette Docker image <https://hub.docker.com/r/datasetteproject/datasette>`__ now uses Python 3.7.10, applying `the latest security fix <https://www.python.org/downloads/release/python-3710/>`__ for that Python version. (:issue:`1235`)\n\n.. _v0_54_1:\n\n0.54.1 (2021-02-02)\n-------------------\n\n- Fixed a bug where ``?_search=`` and ``?_sort=`` parameters were incorrectly duplicated when the filter form on the table page was re-submitted. (:issue:`1214`)\n\n.. _v0_54:\n\n0.54 (2021-01-25)\n-----------------\n\nThe two big new features in this release are the ``_internal`` SQLite in-memory database storing details of all connected databases and tables, and support for JavaScript modules in plugins and additional scripts.\n\nFor additional commentary on this release, see `Datasette 0.54, the annotated release notes <https://simonwillison.net/2021/Jan/25/datasette/>`__.\n\nThe _internal database\n~~~~~~~~~~~~~~~~~~~~~~\n\nAs part of ongoing work to help Datasette handle much larger numbers of connected databases and tables (see `Datasette Library <https://github.com/simonw/datasette/issues/417>`__) Datasette now maintains an in-memory SQLite database with details of all of the attached databases, tables, columns, indexes and foreign keys. (:issue:`1150`)\n\nThis will support future improvements such as a searchable, paginated homepage of all available tables.\n\nYou can explore an example of this database by `signing in as root <https://latest.datasette.io/login-as-root>`__ to the ``latest.datasette.io`` demo instance and then navigating to `latest.datasette.io/_internal <https://latest.datasette.io/_internal>`__.\n\nPlugins can use these tables to introspect attached data in an efficient way. Plugin authors should note that this is not yet considered a stable interface, so any plugins that use this may need to make changes prior to Datasette 1.0 if the ``_internal`` table schemas change.\n\nNamed in-memory database support\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs part of the work building the ``_internal`` database, Datasette now supports named in-memory databases that can be shared across multiple connections. This allows plugins to create in-memory databases which will persist data for the lifetime of the Datasette server process. (:issue:`1151`)\n\nThe new ``memory_name=`` parameter to the :ref:`internals_database` can be used to create named, shared in-memory databases.\n\nJavaScript modules\n~~~~~~~~~~~~~~~~~~\n\n`JavaScript modules <https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules>`__ were introduced in ECMAScript 2015 and provide native browser support for the ``import`` and ``export`` keywords.\n\nTo use modules, JavaScript needs to be included in ``<script>`` tags with a ``type=\"module\"`` attribute.\n\nDatasette now has the ability to output ``<script type=\"module\">`` in places where you may wish to take advantage of modules. The ``extra_js_urls`` option described in :ref:`customization_css_and_javascript` can now be used with modules, and module support is also available for the :ref:`extra_body_script() <plugin_hook_extra_body_script>` plugin hook. (:issue:`1186`, :issue:`1187`)\n\n`datasette-leaflet-freedraw <https://datasette.io/plugins/datasette-leaflet-freedraw>`__ is the first example of a Datasette plugin that takes advantage of the new support for JavaScript modules. See `Drawing shapes on a map to query a SpatiaLite database <https://simonwillison.net/2021/Jan/24/drawing-shapes-spatialite/>`__ for more on this plugin.\n\nCode formatting with Black and Prettier\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDatasette adopted `Black <https://github.com/psf/black>`__ for opinionated Python code formatting in June 2019. Datasette now also embraces `Prettier <https://prettier.io/>`__ for JavaScript formatting, which like Black is enforced by tests in continuous integration. Instructions for using these two tools can be found in the new section on :ref:`contributing_formatting` in the contributors documentation. (:issue:`1167`)\n\nOther changes\n~~~~~~~~~~~~~\n\n- Datasette can now open multiple database files with the same name, e.g. if you run ``datasette path/to/one.db path/to/other/one.db``. (:issue:`509`)\n- ``datasette publish cloudrun`` now sets ``force_https_urls`` for every deployment, fixing some incorrect ``http://`` links. (:issue:`1178`)\n- Fixed a bug in the example nginx configuration in :ref:`deploying_proxy`. (:issue:`1091`)\n- The :ref:`Datasette Ecosystem <ecosystem>` documentation page has been reduced in size in favour of the ``datasette.io`` `tools <https://datasette.io/tools>`__ and `plugins <https://datasette.io/plugins>`__ directories. (:issue:`1182`)\n- The request object now provides a ``request.full_path`` property, which returns the path including any query string. (:issue:`1184`)\n- Better error message for disallowed ``PRAGMA`` clauses in SQL queries. (:issue:`1185`)\n- ``datasette publish heroku`` now deploys using ``python-3.8.7``.\n- New plugin testing documentation on :ref:`testing_plugins_pytest_httpx`. (:issue:`1198`)\n- All ``?_*`` query string parameters passed to the table page are now persisted in hidden form fields, so parameters such as ``?_size=10`` will be correctly passed to the next page when query filters are changed. (:issue:`1194`)\n- Fixed a bug loading a database file called ``test-database (1).sqlite``. (:issue:`1181`)\n\n\n.. _v0_53:\n\n0.53 (2020-12-10)\n-----------------\n\nDatasette has an official project website now, at https://datasette.io/. This release mainly updates the documentation to reflect the new site.\n\n- New ``?column__arraynotcontains=`` table filter. (:issue:`1132`)\n- ``datasette serve`` has a new ``--create`` option, which will create blank database files if they do not already exist rather than exiting with an error. (:issue:`1135`)\n-  New ``?_header=off`` option for CSV export which omits the CSV header row, :ref:`documented here <csv_export_url_parameters>`. (:issue:`1133`)\n- \"Powered by Datasette\" link in the footer now links to https://datasette.io/. (:issue:`1138`)\n- Project news no longer lives in the README - it can now be found at https://datasette.io/news. (:issue:`1137`)\n\n.. _v0_52_5:\n\n0.52.5 (2020-12-09)\n-------------------\n\n- Fix for error caused by combining the ``_searchmode=raw`` and ``?_search_COLUMN`` parameters. (:issue:`1134`)\n\n.. _v0_52_4:\n\n0.52.4 (2020-12-05)\n-------------------\n\n- Show `pysqlite3 <https://github.com/coleifer/pysqlite3>`__ version on ``/-/versions``, if installed. (:issue:`1125`)\n- Errors output by Datasette (e.g. for invalid SQL queries) now go to ``stderr``, not ``stdout``. (:issue:`1131`)\n- Fix for a startup error on windows caused by unnecessary ``from os import EX_CANTCREAT`` - thanks, Abdussamet Ko\u00e7ak.  (:issue:`1094`)\n\n.. _v0_52_3:\n\n0.52.3 (2020-12-03)\n-------------------\n\n- Fixed bug where static assets would 404 for Datasette installed on ARM Amazon Linux. (:issue:`1124`)\n\n.. _v0_52_2:\n\n0.52.2 (2020-12-02)\n-------------------\n\n- Generated columns from SQLite 3.31.0 or higher are now correctly displayed. (:issue:`1116`)\n- Error message if you attempt to open a SpatiaLite database now suggests using ``--load-extension=spatialite`` if it detects that the extension is available in a common location. (:issue:`1115`)\n- ``OPTIONS`` requests against the ``/database`` page no longer raise a 500 error. (:issue:`1100`)\n- Databases larger than 32MB that are published to Cloud Run can now be downloaded. (:issue:`749`)\n- Fix for misaligned cog icon on table and database pages. Thanks, Abdussamet Ko\u00e7ak. (:issue:`1121`)\n\n.. _v0_52_1:\n\n0.52.1 (2020-11-29)\n-------------------\n\n- Documentation on :ref:`testing_plugins` now recommends using :ref:`internals_datasette_client`. (:issue:`1102`)\n- Fix bug where compound foreign keys produced broken links. (:issue:`1098`)\n- ``datasette --load-module=spatialite`` now also checks for ``/usr/local/lib/mod_spatialite.so``. Thanks, Dan Peterson. (:issue:`1114`)\n\n.. _v0_52:\n\n0.52 (2020-11-28)\n-----------------\n\nThis release includes a number of changes relating to an internal rebranding effort: Datasette's **configuration** mechanism (things like ``datasette --config default_page_size:10``) has been renamed to **settings**.\n\n- New ``--setting default_page_size 10`` option as a replacement for ``--config default_page_size:10`` (note the lack of a colon). The ``--config`` option is deprecated but will continue working until Datasette 1.0. (:issue:`992`)\n- The ``/-/config`` introspection page is now ``/-/settings``, and the previous page redirects to the new one. (:issue:`1103`)\n- The ``config.json`` file in :ref:`config_dir` is now called ``settings.json``. (:issue:`1104`)\n- The undocumented ``datasette.config()`` internal method has been replaced by a documented :ref:`datasette_setting` method. (:issue:`1107`)\n\nAlso in this release:\n\n- New plugin hook: :ref:`plugin_hook_database_actions`, which adds menu items to a new cog menu shown at the top of the database page. (:issue:`1077`)\n- ``datasette publish cloudrun`` has a new ``--apt-get-install`` option that can be used to install additional Ubuntu packages as part of the deployment. This is useful for deploying the new `datasette-ripgrep plugin <https://github.com/simonw/datasette-ripgrep>`__. (:issue:`1110`)\n- Swept the documentation to remove words that minimize involved difficulty. (:issue:`1089`)\n\nAnd some bug fixes:\n\n- Foreign keys linking to rows with blank label columns now display as a hyphen, allowing those links to be clicked. (:issue:`1086`)\n- Fixed bug where row pages could sometimes 500 if the underlying queries exceeded a time limit. (:issue:`1088`)\n- Fixed a bug where the table action menu could appear partially obscured by the edge of the page. (:issue:`1084`)\n\n.. _v0_51_1:\n\n0.51.1 (2020-10-31)\n-------------------\n\n- Improvements to the new :ref:`binary` documentation page.\n\n.. _v0_51:\n\n0.51 (2020-10-31)\n-----------------\n\nA new visual design, plugin hooks for adding navigation options, better handling of binary data, URL building utility methods and better support for running Datasette behind a proxy.\n\nNew visual design\n~~~~~~~~~~~~~~~~~\n\nDatasette is no longer white and grey with blue and purple links! `Natalie Downe <https://twitter.com/natbat>`__ has been working on a visual refresh, the first iteration of which is included in this release. (`#1056 <https://github.com/simonw/datasette/pull/1056>`__)\n\n.. image:: datasette-0.51.png\n   :width: 740px\n   :alt: Screenshot showing Datasette's new visual look\n\nPlugins can now add links within Datasette\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA number of existing Datasette plugins add new pages to the Datasette interface, providig tools for things like `uploading CSVs <https://github.com/simonw/datasette-upload-csvs>`__, `editing table schemas <https://github.com/simonw/datasette-edit-schema>`__ or `configuring full-text search <https://github.com/simonw/datasette-configure-fts>`__.\n\nPlugins like this can now link to themselves from other parts of Datasette interface. The :ref:`plugin_hook_menu_links` hook (:issue:`1064`) lets plugins add links to Datasette's new top-right application menu, and the :ref:`plugin_hook_table_actions` hook (:issue:`1066`) adds links to a new \"table actions\" menu on the table page.\n\nThe demo at `latest.datasette.io <https://latest.datasette.io/>`__ now includes some example plugins. To see the new table actions menu first `sign into that demo as root <https://latest.datasette.io/login-as-root>`__ and then visit the `facetable <https://latest.datasette.io/fixtures/facetable>`__ table to see the new cog icon menu at the top of the page.\n\nBinary data\n~~~~~~~~~~~\n\nSQLite tables can contain binary data in ``BLOB`` columns. Datasette now provides links for users to download this data directly from Datasette, and uses those links to make binary data available from CSV exports. See :ref:`binary` for more details. (:issue:`1036` and :issue:`1034`).\n\nURL building\n~~~~~~~~~~~~\n\nThe new :ref:`internals_datasette_urls` family of methods can be used to generate URLs to key pages within the Datasette interface, both within custom templates and Datasette plugins. See :ref:`writing_plugins_building_urls` for more details. (:issue:`904`)\n\nRunning Datasette behind a proxy\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe :ref:`setting_base_url` configuration option is designed to help run Datasette on a specific path behind a proxy - for example if you want to run an instance of Datasette at ``/my-datasette/`` within your existing site's URL hierarchy, proxied behind nginx or Apache.\n\nSupport for this configuration option has been greatly improved (:issue:`1023`), and guidelines for using it are now available in a new documentation section on :ref:`deploying_proxy`. (:issue:`1027`)\n\nSmaller changes\n~~~~~~~~~~~~~~~\n\n- Wide tables shown within Datasette now scroll horizontally (:issue:`998`). This is achieved using a new ``<div class=\"table-wrapper\">`` element which may impact the implementation of some plugins (for example `this change to datasette-cluster-map <https://github.com/simonw/datasette-cluster-map/commit/fcb4abbe7df9071c5ab57defd39147de7145b34e>`__).\n- New :ref:`permissions_debug_menu` permission. (:issue:`1068`)\n- Removed ``--debug`` option, which didn't do anything. (:issue:`814`)\n- ``Link:`` HTTP header pagination. (:issue:`1014`)\n- ``x`` button for clearing filters. (:issue:`1016`)\n- Edit SQL button on canned queries, (:issue:`1019`)\n- ``--load-extension=spatialite`` shortcut. (:issue:`1028`)\n- scale-in animation for column action menu. (:issue:`1039`)\n- Option to pass a list of templates to ``.render_template()`` is now documented. (:issue:`1045`)\n- New ``datasette.urls.static_plugins()`` method. (:issue:`1033`)\n- ``datasette -o`` option now opens the most relevant page. (:issue:`976`)\n- ``datasette --cors`` option now enables access to ``/database.db`` downloads. (:issue:`1057`)\n- Database file downloads now implement cascading permissions, so you can download a database if you have ``view-database-download`` permission even if you do not have permission to access the Datasette instance. (:issue:`1058`)\n- New documentation on :ref:`writing_plugins_designing_urls`. (:issue:`1053`)\n\n.. _v0_50_2:\n\n0.50.2 (2020-10-09)\n-------------------\n\n- Fixed another bug introduced in 0.50 where column header links on the table page were broken. (:issue:`1011`)\n\n.. _v0_50_1:\n\n0.50.1 (2020-10-09)\n-------------------\n\n- Fixed a bug introduced in 0.50 where the export as JSON/CSV links on the table, row and query pages were broken. (:issue:`1010`)\n\n.. _v0_50:\n\n0.50 (2020-10-09)\n-----------------\n\nThe key new feature in this release is the **column actions** menu on the table page (:issue:`891`). This can be used to sort a column in ascending or descending order, facet data by that column or filter the table to just rows that have a value for that column.\n\nPlugin authors can use the new :ref:`internals_datasette_client` object to make internal HTTP requests from their plugins, allowing them to make use of Datasette's JSON API. (:issue:`943`)\n\nNew :ref:`deploying` documentation with guides for deploying Datasette on a Linux server :ref:`using systemd <deploying_systemd>` or to hosting providers :ref:`that support buildpacks <deploying_buildpacks>`. (:issue:`514`, :issue:`997`)\n\nOther improvements in this release:\n\n- :ref:`publish_cloud_run` documentation now covers Google Cloud SDK options. Thanks, Geoffrey Hing. (`#995 <https://github.com/simonw/datasette/pull/995>`__)\n- New ``datasette -o`` option which opens your browser as soon as Datasette starts up. (:issue:`970`)\n- Datasette now sets ``sqlite3.enable_callback_tracebacks(True)`` so that errors in custom SQL functions will display tracebacks. (:issue:`891`)\n- Fixed two rendering bugs with column headers in portrait mobile view. (:issue:`978`, :issue:`980`)\n- New ``db.table_column_details(table)`` introspection method for retrieving full details of the columns in a specific table, see :ref:`internals_database_introspection`.\n- Fixed a routing bug with custom page wildcard templates. (:issue:`996`)\n- ``datasette publish heroku`` now deploys using Python 3.8.6.\n- New ``datasette publish heroku --tar=`` option. (:issue:`969`)\n- ``OPTIONS`` requests against HTML pages no longer return a 500 error. (:issue:`1001`)\n- Datasette now supports Python 3.9.\n\nSee also `Datasette 0.50: The annotated release notes <https://simonwillison.net/2020/Oct/9/datasette-0-50/>`__.\n\n.. _v0_49_1:\n\n0.49.1 (2020-09-15)\n-------------------\n\n- Fixed a bug with writable canned queries that use magic parameters but accept no non-magic arguments. (:issue:`967`)\n\n.. _v0_49:\n\n0.49 (2020-09-14)\n-----------------\n\nSee also `Datasette 0.49: The annotated release notes <https://simonwillison.net/2020/Sep/15/datasette-0-49/>`__.\n\n- Writable canned queries now expose a JSON API, see :ref:`canned_queries_json_api`. (:issue:`880`)\n- New mechanism for defining page templates with custom path parameters - a template file called ``pages/about/{slug}.html`` will be used to render any requests to ``/about/something``. See :ref:`custom_pages_parameters`. (:issue:`944`)\n- ``register_output_renderer()`` render functions can now return a ``Response``. (:issue:`953`)\n- New ``--upgrade`` option for ``datasette install``. (:issue:`945`)\n- New ``datasette --pdb`` option. (:issue:`962`)\n- ``datasette --get`` exit code now reflects the internal HTTP status code. (:issue:`947`)\n- New ``raise_404()`` template function for returning 404 errors. (:issue:`964`)\n- ``datasette publish heroku`` now deploys using Python 3.8.5\n- Upgraded `CodeMirror <https://codemirror.net/>`__ to 5.57.0. (:issue:`948`)\n- Upgraded code style to Black 20.8b1. (:issue:`958`)\n- Fixed bug where selected facets were not correctly persisted in hidden form fields on the table page. (:issue:`963`)\n- Renamed the default error template from ``500.html`` to ``error.html``.\n- Custom error pages are now documented, see :ref:`custom_pages_errors`. (:issue:`965`)\n\n.. _v0_48:\n\n0.48 (2020-08-16)\n-----------------\n\n- Datasette documentation now lives at `docs.datasette.io <https://docs.datasette.io/>`__.\n- ``db.is_mutable`` property is now documented and tested, see :ref:`internals_database_introspection`.\n- The ``extra_template_vars``, ``extra_css_urls``, ``extra_js_urls`` and ``extra_body_script`` plugin hooks now all accept the same arguments. See :ref:`plugin_hook_extra_template_vars` for details. (:issue:`939`)\n- Those hooks now accept a new ``columns`` argument detailing the table columns that will be rendered on that page. (:issue:`938`)\n- Fixed bug where plugins calling ``db.execute_write_fn()`` could hang Datasette if the connection failed. (:issue:`935`)\n- Fixed bug with the ``?_nl=on`` output option and binary data. (:issue:`914`)\n\n.. _v0_47_3:\n\n0.47.3 (2020-08-15)\n-------------------\n\n- The ``datasette --get`` command-line mechanism now ensures any plugins using the ``startup()`` hook are correctly executed. (:issue:`934`)\n\n.. _v0_47_2:\n\n0.47.2 (2020-08-12)\n-------------------\n\n- Fixed an issue with the Docker image `published to Docker Hub <https://hub.docker.com/r/datasetteproject/datasette>`__. (:issue:`931`)\n\n.. _v0_47_1:\n\n0.47.1 (2020-08-11)\n-------------------\n\n- Fixed a bug where the ``sdist`` distribution of Datasette was not correctly including the template files. (:issue:`930`)\n\n.. _v0_47:\n\n0.47 (2020-08-11)\n-----------------\n\n- Datasette now has `a GitHub discussions forum <https://github.com/simonw/datasette/discussions>`__ for conversations about the project that go beyond just bug reports and issues.\n- Datasette can now be installed on macOS using Homebrew! Run ``brew install simonw/datasette/datasette``. See :ref:`installation_homebrew`. (:issue:`335`)\n- Two new commands: ``datasette install name-of-plugin`` and ``datasette uninstall name-of-plugin``. These are equivalent to ``pip install`` and ``pip uninstall`` but automatically run in the same virtual environment as Datasette, so users don't have to figure out where that virtual environment is - useful for installations created using Homebrew or ``pipx``. See :ref:`plugins_installing`. (:issue:`925`)\n- A new command-line option, ``datasette --get``, accepts a path to a URL within the Datasette instance. It will run that request through Datasette (without starting a web server) and print out the response. See :ref:`cli_datasette_get` for an example. (:issue:`926`)\n\n.. _v0_46:\n\n0.46 (2020-08-09)\n-----------------\n\n.. warning::\n    This release contains a security fix related to authenticated writable canned queries. If you are using this feature you should upgrade as soon as possible.\n\n- **Security fix:** CSRF tokens were incorrectly included in read-only canned query forms, which could allow them to be leaked to a sophisticated attacker. See `issue 918 <https://github.com/simonw/datasette/issues/918>`__ for details.\n- Datasette now supports GraphQL via the new `datasette-graphql <https://github.com/simonw/datasette-graphql>`__ plugin - see `GraphQL in Datasette with the new datasette-graphql plugin <https://simonwillison.net/2020/Aug/7/datasette-graphql/>`__.\n- Principle git branch has been renamed from ``master`` to ``main``. (:issue:`849`)\n- New debugging tool: ``/-/allow-debug tool`` (`demo here <https://latest.datasette.io/-/allow-debug>`__) helps test allow blocks against actors, as described in :ref:`authentication_permissions_allow`. (:issue:`908`)\n- New logo for the documentation, and a new project tagline: \"An open source multi-tool for exploring and publishing data\".\n- Whitespace in column values is now respected on display, using ``white-space: pre-wrap``. (:issue:`896`)\n- New ``await request.post_body()`` method for accessing the raw POST body, see :ref:`internals_request`. (:issue:`897`)\n- Database file downloads now include a ``content-length`` HTTP header, enabling download progress bars. (:issue:`905`)\n- File downloads now also correctly set the suggested file name using a ``content-disposition`` HTTP header. (:issue:`909`)\n- ``tests`` are now excluded from the Datasette package properly - thanks, abeyerpath. (:issue:`456`)\n- The Datasette package published to PyPI now includes ``sdist`` as well as ``bdist_wheel``.\n- Better titles for canned query pages. (:issue:`887`)\n- Now only loads Python files from a directory passed using the ``--plugins-dir`` option - thanks, Amjith Ramanujam. (`#890 <https://github.com/simonw/datasette/pull/890>`__)\n- New documentation section on :ref:`publish_vercel`.\n\n.. _v0_45:\n\n0.45 (2020-07-01)\n-----------------\n\nSee also `Datasette 0.45: The annotated release notes <https://simonwillison.net/2020/Jul/1/datasette-045/>`__.\n\nMagic parameters for canned queries, a log out feature, improved plugin documentation and four new plugin hooks.\n\nMagic parameters for canned queries\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCanned queries now support :ref:`canned_queries_magic_parameters`, which can be used to insert or select automatically generated values. For example::\n\n    insert into logs\n      (user_id, timestamp)\n    values\n      (:_actor_id, :_now_datetime_utc)\n\nThis inserts the currently authenticated actor ID and the current datetime. (:issue:`842`)\n\nLog out\n~~~~~~~\n\nThe :ref:`ds_actor cookie <authentication_ds_actor>` can be used by plugins (or by Datasette's :ref:`--root mechanism<authentication_root>`) to authenticate users. The new ``/-/logout`` page provides a way to clear that cookie.\n\nA \"Log out\" button now shows in the global navigation provided the user is authenticated using the ``ds_actor`` cookie. (:issue:`840`)\n\nBetter plugin documentation\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe plugin documentation has been re-arranged into four sections, including a brand new section on testing plugins. (:issue:`687`)\n\n- :ref:`plugins` introduces Datasette's plugin system and describes how to install and configure plugins.\n- :ref:`writing_plugins` describes how to author plugins, from  one-off single file plugins to packaged plugins that can be published to PyPI. It also describes how to start a plugin using the new `datasette-plugin <https://github.com/simonw/datasette-plugin>`__ cookiecutter template.\n- :ref:`plugin_hooks` is a full list of detailed documentation for every Datasette plugin hook.\n- :ref:`testing_plugins` describes how to write tests for Datasette plugins, using `pytest <https://docs.pytest.org/>`__ and `HTTPX <https://www.python-httpx.org/>`__.\n\nNew plugin hooks\n~~~~~~~~~~~~~~~~\n\n- :ref:`plugin_hook_register_magic_parameters` can be used to define new types of magic canned query parameters.\n- :ref:`plugin_hook_startup` can run custom code when Datasette first starts up. `datasette-init <https://github.com/simonw/datasette-init>`__ is a new plugin that uses this hook to create database tables and views on startup if they have not yet been created. (:issue:`834`)\n- :ref:`plugin_hook_canned_queries` lets plugins provide additional canned queries beyond those defined in Datasette's metadata. See `datasette-saved-queries <https://github.com/simonw/datasette-saved-queries>`__ for an example of this hook in action. (:issue:`852`)\n- :ref:`plugin_hook_forbidden` is a hook for customizing how Datasette responds to 403 forbidden errors. (:issue:`812`)\n\nSmaller changes\n~~~~~~~~~~~~~~~\n\n- Cascading view permissions - so if a user has ``view-table`` they can view the table page even if they do not have ``view-database`` or ``view-instance``. (:issue:`832`)\n- CSRF protection no longer applies to ``Authentication: Bearer token`` requests or requests without cookies. (:issue:`835`)\n- ``datasette.add_message()`` now works inside plugins. (:issue:`864`)\n- Workaround for \"Too many open files\" error in test runs. (:issue:`846`)\n- Respect existing ``scope[\"actor\"]`` if already set by ASGI middleware. (:issue:`854`)\n- New process for shipping :ref:`contributing_alpha_beta`. (:issue:`807`)\n- ``{{ csrftoken() }}`` now works when plugins render a template using ``datasette.render_template(..., request=request)``. (:issue:`863`)\n- Datasette now creates a single :ref:`internals_request` and uses it throughout the lifetime of the current HTTP request. (:issue:`870`)\n\n.. _v0_44:\n\n0.44 (2020-06-11)\n-----------------\n\nSee also `Datasette 0.44: The annotated release notes <https://simonwillison.net/2020/Jun/12/annotated-release-notes/>`__.\n\nAuthentication and permissions, writable canned queries, flash messages, new plugin hooks and more.\n\nAuthentication\n~~~~~~~~~~~~~~\n\nPrior to this release the Datasette ecosystem has treated authentication as exclusively the realm of plugins, most notably through `datasette-auth-github <https://github.com/simonw/datasette-auth-github>`__.\n\n0.44 introduces :ref:`authentication` as core Datasette concepts (:issue:`699`). This enables different plugins to share responsibility for authenticating requests - you might have one plugin that handles user accounts and another one that allows automated access via API keys, for example.\n\nYou'll need to install plugins if you want full user accounts, but default Datasette can now authenticate a single root user with the new ``--root`` command-line option, which outputs a one-time use URL to :ref:`authenticate as a root actor <authentication_root>` (:issue:`784`)::\n\n    datasette fixtures.db --root\n\n::\n\n    http://127.0.0.1:8001/-/auth-token?token=5b632f8cd44b868df625f5a6e2185d88eea5b22237fd3cc8773f107cc4fd6477\n    INFO:     Started server process [14973]\n    INFO:     Waiting for application startup.\n    INFO:     Application startup complete.\n    INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\n\nPlugins can implement new ways of authenticating users using the new :ref:`plugin_hook_actor_from_request` hook.\n\nPermissions\n~~~~~~~~~~~\n\nDatasette also now has a built-in concept of :ref:`authentication_permissions`. The permissions system answers the following question:\n\n    Is this **actor** allowed to perform this **action**, optionally against this particular **resource**?\n\nYou can use the new ``\"allow\"`` block syntax in ``metadata.json`` (or ``metadata.yaml``) to set required permissions at the instance, database, table or canned query level. For example, to restrict access to the ``fixtures.db`` database to the ``\"root\"`` user:\n\n.. code-block:: json\n\n    {\n        \"databases\": {\n            \"fixtures\": {\n                \"allow\": {\n                    \"id\" \"root\"\n                }\n            }\n        }\n    }\n\nSee :ref:`authentication_permissions_allow` for more details.\n\nPlugins can implement their own custom permission checks using the new :ref:`plugin_hook_permission_allowed` hook.\n\nA new debug page at ``/-/permissions`` shows recent permission checks, to help administrators and plugin authors understand exactly what checks are being performed. This tool defaults to only being available to the root user, but can be exposed to other users by plugins that respond to the ``permissions-debug`` permission. (:issue:`788`)\n\nWritable canned queries\n~~~~~~~~~~~~~~~~~~~~~~~\n\nDatasette's :ref:`canned_queries` feature lets you define SQL queries in ``metadata.json`` which can then be executed by users visiting a specific URL. https://latest.datasette.io/fixtures/neighborhood_search for example.\n\nCanned queries were previously restricted to ``SELECT``, but Datasette 0.44 introduces the ability for canned queries to execute ``INSERT`` or ``UPDATE`` queries as well, using the new ``\"write\": true`` property (:issue:`800`):\n\n.. code-block:: json\n\n    {\n        \"databases\": {\n            \"dogs\": {\n                \"queries\": {\n                    \"add_name\": {\n                        \"sql\": \"INSERT INTO names (name) VALUES (:name)\",\n                        \"write\": true\n                    }\n                }\n            }\n        }\n    }\n\nSee :ref:`canned_queries_writable` for more details.\n\nFlash messages\n~~~~~~~~~~~~~~\n\nWritable canned queries needed a mechanism to let the user know that the query has been successfully executed. The new flash messaging system (:issue:`790`) allows messages to persist in signed cookies which are then displayed to the user on the next page that they visit. Plugins can use this mechanism to display their own messages, see :ref:`datasette_add_message` for details.\n\nYou can try out the new messages using the ``/-/messages`` debug tool, for example at https://latest.datasette.io/-/messages\n\nSigned values and secrets\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBoth flash messages and user authentication needed a way to sign values and set signed cookies. Two new methods are now available for plugins to take advantage of this mechanism: :ref:`datasette_sign` and :ref:`datasette_unsign`.\n\nDatasette will generate a secret automatically when it starts up, but to avoid resetting the secret (and hence invalidating any cookies) every time the server restarts you should set your own secret. You can pass a secret to Datasette using the new ``--secret`` option or with a ``DATASETTE_SECRET`` environment variable. See :ref:`setting_secret` for more details.\n\nYou can also set a secret when you deploy Datasette using ``datasette publish`` or ``datasette package`` - see :ref:`setting_publish_secrets`.\n\nPlugins can now sign values and verify their signatures using the :ref:`datasette.sign() <datasette_sign>` and :ref:`datasette.unsign() <datasette_unsign>` methods.\n\nCSRF protection\n~~~~~~~~~~~~~~~\n\nSince writable canned queries are built using POST forms, Datasette now ships with :ref:`internals_csrf` (:issue:`798`). This applies automatically to any POST request, which means plugins need to include a ``csrftoken`` in any POST forms that they render. They can do that like so:\n\n.. code-block:: html\n\n    <input type=\"hidden\" name=\"csrftoken\" value=\"{{ csrftoken() }}\">\n\nCookie methods\n~~~~~~~~~~~~~~\n\nPlugins can now use the new :ref:`response.set_cookie() <internals_response_set_cookie>` method to set cookies.\n\nA new ``request.cookies`` method on the :ref:internals_request` can be used to read incoming cookies.\n\nregister_routes() plugin hooks\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPlugins can now register new views and routes via the :ref:`plugin_register_routes` plugin hook (:issue:`819`). View functions can be defined that accept any of the current ``datasette`` object, the current ``request``, or the ASGI ``scope``, ``send`` and ``receive`` objects.\n\nSmaller changes\n~~~~~~~~~~~~~~~\n\n- New internals documentation for :ref:`internals_request` and :ref:`internals_response`. (:issue:`706`)\n- ``request.url`` now respects the ``force_https_urls`` config setting. closes (:issue:`781`)\n- ``request.args.getlist()`` returns ``[]`` if missing. Removed ``request.raw_args`` entirely. (:issue:`774`)\n- New :ref:`datasette.get_database() <datasette_get_database>` method.\n- Added ``_`` prefix to many private, undocumented methods of the Datasette class. (:issue:`576`)\n- Removed the ``db.get_outbound_foreign_keys()`` method which duplicated the behaviour of ``db.foreign_keys_for_table()``.\n- New :ref:`await datasette.permission_allowed() <datasette_permission_allowed>` method.\n- ``/-/actor`` debugging endpoint for viewing the currently authenticated actor.\n- New ``request.cookies`` property.\n- ``/-/plugins`` endpoint now shows a list of hooks implemented by each plugin, e.g. https://latest.datasette.io/-/plugins?all=1\n- ``request.post_vars()`` method no longer discards empty values.\n- New \"params\" canned query key for explicitly setting named parameters, see :ref:`canned_queries_named_parameters`. (:issue:`797`)\n- ``request.args`` is now a :ref:`MultiParams <internals_multiparams>` object.\n- Fixed a bug with the ``datasette plugins`` command. (:issue:`802`)\n- Nicer pattern for using ``make_app_client()`` in tests. (:issue:`395`)\n- New ``request.actor`` property.\n- Fixed broken CSS on nested 404 pages. (:issue:`777`)\n- New ``request.url_vars`` property. (:issue:`822`)\n- Fixed a bug with the ``python tests/fixtures.py`` command for outputting Datasette's testing fixtures database and plugins. (:issue:`804`)\n- ``datasette publish heroku`` now deploys using Python 3.8.3.\n- Added a warning that the :ref:`plugin_register_facet_classes` hook is unstable and may change in the future. (:issue:`830`)\n- The ``{\"$env\": \"ENVIRONMENT_VARIBALE\"}`` mechanism (see :ref:`plugins_configuration_secret`) now works with variables inside nested lists. (:issue:`837`)\n\nThe road to Datasette 1.0\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nI've assembled a `milestone for Datasette 1.0 <https://github.com/simonw/datasette/milestone/7>`__. The focus of the 1.0 release will be the following:\n\n- Signify confidence in the quality/stability of Datasette\n- Give plugin authors confidence that their plugins will work for the whole 1.x release cycle\n- Provide the same confidence to developers building against Datasette JSON APIs\n\nIf you have thoughts about what you would like to see for Datasette 1.0 you can join `the conversation on issue #519 <https://github.com/simonw/datasette/issues/519>`__.\n\n.. _v0_43:\n\n0.43 (2020-05-28)\n-----------------\n\nThe main focus of this release is a major upgrade to the :ref:`plugin_register_output_renderer` plugin hook, which allows plugins to provide new output formats for Datasette such as `datasette-atom <https://github.com/simonw/datasette-atom>`__ and `datasette-ics <https://github.com/simonw/datasette-ics>`__.\n\n* Redesign of :ref:`plugin_register_output_renderer` to provide more context to the render callback and support an optional ``\"can_render\"`` callback that controls if a suggested link to the output format is provided. (:issue:`581`, :issue:`770`)\n* Visually distinguish float and integer columns - useful for figuring out why order-by-column might be returning unexpected results. (:issue:`729`)\n* The :ref:`internals_request`, which is passed to several plugin hooks, is now documented. (:issue:`706`)\n* New ``metadata.json`` option for setting a custom default page size for specific tables and views, see :ref:`metadata_page_size`. (:issue:`751`)\n* Canned queries can now be configured with a default URL fragment hash, useful when working with plugins such as `datasette-vega <https://github.com/simonw/datasette-vega>`__, see :ref:`canned_queries_options`. (:issue:`706`)\n* Fixed a bug in ``datasette publish`` when running on operating systems where the ``/tmp`` directory lives in a different volume, using a backport of the Python 3.8 ``shutil.copytree()`` function. (:issue:`744`)\n* Every plugin hook is now covered by the unit tests, and a new unit test checks that each plugin hook has at least one corresponding test. (:issue:`771`, :issue:`773`)\n\n.. _v0_42:\n\n0.42 (2020-05-08)\n-----------------\n\nA small release which provides improved internal methods for use in plugins, along with documentation. See :issue:`685`.\n\n* Added documentation for ``db.execute()``, see :ref:`database_execute`.\n* Renamed ``db.execute_against_connection_in_thread()`` to ``db.execute_fn()`` and made it a documented method, see :ref:`database_execute_fn`.\n* New ``results.first()`` and ``results.single_value()`` methods, plus documentation for the ``Results`` class - see :ref:`database_results`.\n\n.. _v0_41:\n\n0.41 (2020-05-06)\n-----------------\n\nYou can now create :ref:`custom pages <custom_pages>` within your Datasette instance using a custom template file. For example, adding a template file called ``templates/pages/about.html`` will result in a new page being served at ``/about`` on your instance. See the :ref:`custom pages documentation <custom_pages>` for full details, including how to return custom HTTP headers, redirects and status codes. (:issue:`648`)\n\n:ref:`config_dir` (:issue:`731`) allows you to define a custom Datasette instance as a directory. So instead of running the following::\n\n    datasette one.db two.db \\\n      --metadata=metadata.json \\\n      --template-dir=templates/ \\\n      --plugins-dir=plugins \\\n      --static css:css\n\nYou can instead arrange your files in a single directory called ``my-project`` and run this::\n\n    datasette my-project/\n\nAlso in this release:\n\n* New ``NOT LIKE`` table filter: ``?colname__notlike=expression``. (:issue:`750`)\n* Datasette now has a *pattern portfolio* at ``/-/patterns`` - e.g. https://latest.datasette.io/-/patterns. This is a page that shows every Datasette user interface component in one place, to aid core development and people building custom CSS themes. (:issue:`151`)\n* SQLite `PRAGMA functions <https://www.sqlite.org/pragma.html#pragfunc>`__ such as ``pragma_table_info(tablename)`` are now allowed in Datasette SQL queries. (:issue:`761`)\n* Datasette pages now consistently return a ``content-type`` of ``text/html; charset=utf-8\"``. (:issue:`752`)\n* Datasette now handles an ASGI ``raw_path`` value of ``None``, which should allow compatibility with the `Mangum <https://github.com/erm/mangum>`__ adapter for running ASGI apps on AWS Lambda. Thanks, Colin Dellow. (`#719 <https://github.com/simonw/datasette/pull/719>`__)\n* Installation documentation now covers how to :ref:`installation_pipx`. (:issue:`756`)\n* Improved the documentation for :ref:`full_text_search`. (:issue:`748`)\n\n.. _v0_40:\n\n0.40 (2020-04-21)\n-----------------\n\n* Datasette :ref:`metadata` can now be provided as a YAML file as an optional alternative to JSON. See :ref:`metadata_yaml`. (:issue:`713`)\n* Removed support for ``datasette publish now``, which used the the now-retired Zeit Now v1 hosting platform. A new plugin, `datasette-publish-now <https://github.com/simonw/datasette-publish-now>`__, can be installed to publish data to Zeit (`now Vercel <https://vercel.com/blog/zeit-is-now-vercel>`__) Now v2. (:issue:`710`)\n* Fixed a bug where the ``extra_template_vars(request, view_name)`` plugin hook was not receiving the correct ``view_name``. (:issue:`716`)\n* Variables added to the template context by the ``extra_template_vars()`` plugin hook are now shown in the ``?_context=1`` debugging mode (see :ref:`setting_template_debug`). (:issue:`693`)\n* Fixed a bug where the \"templates considered\" HTML comment was no longer being displayed. (:issue:`689`)\n* Fixed a ``datasette publish`` bug where ``--plugin-secret`` would over-ride plugin configuration in the provided ``metadata.json`` file. (:issue:`724`)\n* Added a new CSS class for customizing the canned query page. (:issue:`727`)\n\n.. _v0_39:\n\n0.39 (2020-03-24)\n-----------------\n\n* New :ref:`setting_base_url` configuration setting for serving up the correct links while running Datasette under a different URL prefix. (:issue:`394`)\n* New metadata settings ``\"sort\"`` and ``\"sort_desc\"`` for setting the default sort order for a table. See :ref:`metadata_default_sort`. (:issue:`702`)\n* Sort direction arrow now displays by default on the primary key. This means you only have to click once (not twice) to sort in reverse order. (:issue:`677`)\n* New ``await Request(scope, receive).post_vars()`` method for accessing POST form variables. (:issue:`700`)\n* :ref:`plugin_hooks` documentation now links to example uses of each plugin. (:issue:`709`)\n\n.. _v0_38:\n\n0.38 (2020-03-08)\n-----------------\n\n* The `Docker build <https://hub.docker.com/r/datasetteproject/datasette>`__ of Datasette now uses SQLite 3.31.1, upgraded from 3.26. (:issue:`695`)\n* ``datasette publish cloudrun`` now accepts an optional ``--memory=2Gi`` flag for setting the Cloud Run allocated memory to a value other than the default (256Mi). (:issue:`694`)\n* Fixed bug where templates that shipped with plugins were sometimes not being correctly loaded. (:issue:`697`)\n\n.. _v0_37_1:\n\n0.37.1 (2020-03-02)\n-------------------\n\n* Don't attempt to count table rows to display on the index page for databases > 100MB. (:issue:`688`)\n* Print exceptions if they occur in the write thread rather than silently swallowing them.\n* Handle the possibility of ``scope[\"path\"]`` being a string rather than bytes\n* Better documentation for the :ref:`plugin_hook_extra_template_vars` plugin hook.\n\n.. _v0_37:\n\n0.37 (2020-02-25)\n-----------------\n\n* Plugins now have a supported mechanism for writing to a database, using the new ``.execute_write()`` and ``.execute_write_fn()`` methods. :ref:`Documentation <database_execute_write>`. (:issue:`682`)\n* Immutable databases that have had their rows counted using the ``inspect`` command now use the calculated count more effectively - thanks, Kevin Keogh. (`#666 <https://github.com/simonw/datasette/pull/666>`__)\n* ``--reload`` no longer restarts the server if a database file is modified, unless that database was opened immutable mode with ``-i``. (:issue:`494`)\n* New ``?_searchmode=raw`` option turns off escaping for FTS queries in ``?_search=`` allowing full use of SQLite's `FTS5 query syntax <https://www.sqlite.org/fts5.html#full_text_query_syntax>`__. (:issue:`676`)\n\n.. _v0_36:\n\n0.36 (2020-02-21)\n-----------------\n\n* The ``datasette`` object passed to plugins now has API documentation: :ref:`internals_datasette`. (:issue:`576`)\n* New methods on ``datasette``: ``.add_database()`` and ``.remove_database()`` - :ref:`documentation <datasette_add_database>`. (:issue:`671`)\n* ``prepare_connection()`` plugin hook now takes optional ``datasette`` and ``database`` arguments - :ref:`plugin_hook_prepare_connection`. (:issue:`678`)\n* Added three new plugins and one new conversion tool to the :ref:`ecosystem`.\n\n.. _v0_35:\n\n0.35 (2020-02-04)\n-----------------\n\n* Added five new plugins and one new conversion tool to the :ref:`ecosystem`.\n* The ``Datasette`` class has a new ``render_template()`` method which can be used by plugins to render templates using Datasette's pre-configured `Jinja <https://jinja.palletsprojects.com/>`__ templating library.\n* You can now execute SQL queries that start with a ``-- comment`` - thanks, Jay Graves (`#653 <https://github.com/simonw/datasette/pull/653>`__)\n\n.. _v0_34:\n\n0.34 (2020-01-29)\n-----------------\n\n* ``_search=`` queries are now correctly escaped using a new ``escape_fts()`` custom SQL function. This means you can now run searches for strings like ``park.`` without seeing errors. (:issue:`651`)\n* `Google Cloud Run <https://cloud.google.com/run/>`__ is no longer in beta, so ``datasette publish cloudrun`` has been updated to work even if the user has not installed the ``gcloud`` beta components package. Thanks, Katie McLaughlin (`#660 <https://github.com/simonw/datasette/pull/660>`__)\n* ``datasette package`` now accepts a ``--port`` option for specifying which port the resulting Docker container should listen on. (:issue:`661`)\n\n.. _v0_33:\n\n0.33 (2019-12-22)\n-----------------\n\n* ``rowid`` is now included in dropdown menus for filtering tables (:issue:`636`)\n* Columns are now only suggested for faceting if they have at least one value with more than one record (:issue:`638`)\n* Queries with no results now display \"0 results\" (:issue:`637`)\n* Improved documentation for the ``--static`` option (:issue:`641`)\n* asyncio task information is now included on the ``/-/threads`` debug page\n* Bumped Uvicorn dependency 0.11\n* You can now use ``--port 0`` to listen on an available port\n* New :ref:`setting_template_debug` setting for debugging templates, e.g. https://latest.datasette.io/fixtures/roadside_attractions?_context=1 (:issue:`654`)\n\n.. _v0_32:\n\n0.32 (2019-11-14)\n-----------------\n\nDatasette now renders templates using `Jinja async mode <https://jinja.palletsprojects.com/en/2.10.x/api/#async-support>`__. This means plugins can provide custom template functions that perform asynchronous actions, for example the new `datasette-template-sql <https://github.com/simonw/datasette-template-sql>`__ plugin which allows custom templates to directly execute SQL queries and render their results. (:issue:`628`)\n\n.. _v0_31_2:\n\n0.31.2 (2019-11-13)\n-------------------\n\n- Fixed a bug where ``datasette publish heroku`` applications failed to start (:issue:`633`)\n- Fix for ``datasette publish`` with just ``--source_url`` - thanks, Stanley Zheng (:issue:`572`)\n- Deployments to Heroku now use Python 3.8.0 (:issue:`632`)\n\n.. _v0_31_1:\n\n0.31.1 (2019-11-12)\n-------------------\n\n- Deployments created using ``datasette publish``  now use ``python:3.8`` base Docker image (`#629 <https://github.com/simonw/datasette/pull/629>`__)\n\n.. _v0_31:\n\n0.31 (2019-11-11)\n-----------------\n\nThis version adds compatibility with Python 3.8 and breaks compatibility with Python 3.5.\n\nIf you are still running Python 3.5 you should stick with ``0.30.2``, which you can install like this::\n\n    pip install datasette==0.30.2\n\n- Format SQL button now works with read-only SQL queries - thanks, Tobias Kunze (`#602 <https://github.com/simonw/datasette/pull/602>`__)\n- New ``?column__notin=x,y,z`` filter for table views (:issue:`614`)\n- Table view now uses ``select col1, col2, col3`` instead of ``select *``\n- Database filenames can now contain spaces - thanks, Tobias Kunze (`#590 <https://github.com/simonw/datasette/pull/590>`__)\n- Removed obsolete ``?_group_count=col`` feature (:issue:`504`)\n- Improved user interface and documentation for ``datasette publish cloudrun`` (:issue:`608`)\n- Tables with indexes now show the ``CREATE INDEX`` statements on the table page (:issue:`618`)\n- Current version of `uvicorn <https://www.uvicorn.org/>`__ is now shown on ``/-/versions``\n- Python 3.8 is now supported! (:issue:`622`)\n- Python 3.5 is no longer supported.\n\n.. _v0_30_2:\n\n0.30.2 (2019-11-02)\n-------------------\n\n- ``/-/plugins`` page now uses distribution name e.g. ``datasette-cluster-map`` instead of the name of the underlying Python package (``datasette_cluster_map``) (:issue:`606`)\n- Array faceting is now only suggested for columns that contain arrays of strings (:issue:`562`)\n- Better documentation for the ``--host`` argument (:issue:`574`)\n- Don't show ``None`` with a broken link for the label on a nullable foreign key (:issue:`406`)\n\n.. _v0_30_1:\n\n0.30.1 (2019-10-30)\n-------------------\n\n- Fixed bug where ``?_where=`` parameter was not persisted in hidden form fields (:issue:`604`)\n- Fixed bug with .JSON representation of row pages - thanks, Chris Shaw (:issue:`603`)\n\n.. _v0_30:\n\n\n0.30 (2019-10-18)\n-----------------\n\n- Added ``/-/threads`` debugging page\n- Allow ``EXPLAIN WITH...`` (:issue:`583`)\n- Button to format SQL - thanks, Tobias Kunze (:issue:`136`)\n- Sort databases on homepage by argument order - thanks, Tobias Kunze (:issue:`585`)\n- Display metadata footer on custom SQL queries - thanks, Tobias Kunze (`#589 <https://github.com/simonw/datasette/pull/589>`__)\n- Use ``--platform=managed`` for ``publish cloudrun`` (:issue:`587`)\n- Fixed bug returning non-ASCII characters in CSV (:issue:`584`)\n- Fix for ``/foo`` v.s. ``/foo-bar`` bug (:issue:`601`)\n\n.. _v0_29_3:\n\n0.29.3 (2019-09-02)\n-------------------\n\n- Fixed implementation of CodeMirror on database page (:issue:`560`)\n- Documentation typo fixes - thanks, Min ho Kim (`#561 <https://github.com/simonw/datasette/pull/561>`__)\n- Mechanism for detecting if a table has FTS enabled now works if the table name used alternative escaping mechanisms (:issue:`570`) - for compatibility with `a recent change to sqlite-utils <https://github.com/simonw/sqlite-utils/pull/57>`__.\n\n.. _v0_29_2:\n\n0.29.2 (2019-07-13)\n-------------------\n\n- Bumped `Uvicorn <https://www.uvicorn.org/>`__ to 0.8.4, fixing a bug where the query string was not included in the server logs. (:issue:`559`)\n- Fixed bug where the navigation breadcrumbs were not displayed correctly on the page for a custom query. (:issue:`558`)\n- Fixed bug where custom query names containing unicode characters caused errors.\n\n.. _v0_29_1:\n\n0.29.1 (2019-07-11)\n-------------------\n\n- Fixed bug with static mounts using relative paths which could lead to traversal exploits (:issue:`555`) - thanks Abdussamet Kocak!\n- Datasette can now be run as a module: ``python -m datasette`` (:issue:`556`) - thanks, Abdussamet Kocak!\n\n.. _v0_29:\n\n0.29 (2019-07-07)\n-----------------\n\nASGI, new plugin hooks, facet by date and much, much more...\n\nASGI\n~~~~\n\n`ASGI <https://asgi.readthedocs.io/>`__ is the Asynchronous Server Gateway Interface standard. I've been wanting to convert Datasette into an ASGI application for over a year - `Port Datasette to ASGI #272 <https://github.com/simonw/datasette/issues/272>`__ tracks thirteen months of intermittent development - but with Datasette 0.29 the change is finally released. This also means Datasette now runs on top of `Uvicorn <https://www.uvicorn.org/>`__ and no longer depends on `Sanic <https://github.com/huge-success/sanic>`__.\n\nI wrote about the significance of this change in `Porting Datasette to ASGI, and Turtles all the way down <https://simonwillison.net/2019/Jun/23/datasette-asgi/>`__.\n\nThe most exciting consequence of this change is that Datasette plugins can now take advantage of the ASGI standard.\n\nNew plugin hook: asgi_wrapper\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe :ref:`plugin_asgi_wrapper` plugin hook allows plugins to entirely wrap the Datasette ASGI application in their own ASGI middleware. (:issue:`520`)\n\nTwo new plugins take advantage of this hook:\n\n* `datasette-auth-github <https://github.com/simonw/datasette-auth-github>`__ adds a authentication layer: users will have to sign in using their GitHub account before they can view data or interact with Datasette. You can also use it to restrict access to specific GitHub users, or to members of specified GitHub `organizations <https://help.github.com/en/articles/about-organizations>`__ or `teams <https://help.github.com/en/articles/organizing-members-into-teams>`__.\n\n* `datasette-cors <https://github.com/simonw/datasette-cors>`__ allows you to configure `CORS headers <https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS>`__ for your Datasette instance. You can use this to enable JavaScript running on a whitelisted set of domains to make ``fetch()`` calls to the JSON API provided by your Datasette instance.\n\nNew plugin hook: extra_template_vars\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe :ref:`plugin_hook_extra_template_vars` plugin hook allows plugins to inject their own additional variables into the Datasette template context. This can be used in conjunction with custom templates to customize the Datasette interface. `datasette-auth-github <https://github.com/simonw/datasette-auth-github>`__ uses this hook to add custom HTML to the new top navigation bar (which is designed to be modified by plugins, see :issue:`540`).\n\nSecret plugin configuration options\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPlugins like `datasette-auth-github <https://github.com/simonw/datasette-auth-github>`__ need a safe way to set secret configuration options. Since the default mechanism for configuring plugins exposes those settings in ``/-/metadata`` a new mechanism was needed. :ref:`plugins_configuration_secret` describes how plugins can now specify that their settings should be read from a file or an environment variable::\n\n    {\n        \"plugins\": {\n            \"datasette-auth-github\": {\n                \"client_secret\": {\n                    \"$env\": \"GITHUB_CLIENT_SECRET\"\n                }\n            }\n        }\n    }\n\nThese plugin secrets can be set directly using ``datasette publish``. See :ref:`publish_custom_metadata_and_plugins` for details. (:issue:`538` and :issue:`543`)\n\nFacet by date\n~~~~~~~~~~~~~\n\nIf a column contains datetime values, Datasette can now facet that column by date. (:issue:`481`)\n\n.. _v0_29_medium_changes:\n\nEasier custom templates for table rows\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you want to customize the display of individual table rows, you can do so using a ``_table.html`` template include that looks something like this::\n\n    {% for row in display_rows %}\n        <div>\n            <h2>{{ row[\"title\"] }}</h2>\n            <p>{{ row[\"description\"] }}<lp>\n            <p>Category: {{ row.display(\"category_id\") }}</p>\n        </div>\n    {% endfor %}\n\nThis is a **backwards incompatible change**. If you previously had a custom template called ``_rows_and_columns.html`` you need to rename it to ``_table.html``.\n\nSee :ref:`customization_custom_templates` for full details.\n\n?_through= for joins through many-to-many tables\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe new ``?_through={json}`` argument to the Table view allows records to be filtered based on a many-to-many relationship. See :ref:`json_api_table_arguments` for full documentation - here's `an example <https://latest.datasette.io/fixtures/roadside_attractions?_through={%22table%22:%22roadside_attraction_characteristics%22,%22column%22:%22characteristic_id%22,%22value%22:%221%22}>`__. (:issue:`355`)\n\nThis feature was added to help support `facet by many-to-many <https://github.com/simonw/datasette/issues/551>`__, which isn't quite ready yet but will be coming in the next Datasette release.\n\nSmall changes\n~~~~~~~~~~~~~\n\n* Databases published using ``datasette publish`` now open in :ref:`performance_immutable_mode`. (:issue:`469`)\n* ``?col__date=`` now works for columns containing spaces\n* Automatic label detection (for deciding which column to show when linking to a foreign key) has been improved. (:issue:`485`)\n* Fixed bug where pagination broke when combined with an expanded foreign key. (:issue:`489`)\n* Contributors can now run ``pip install -e .[docs]`` to get all of the dependencies needed to build the documentation, including ``cd docs && make livehtml`` support.\n* Datasette's dependencies are now all specified using the ``~=`` match operator. (:issue:`532`)\n* ``white-space: pre-wrap`` now used for table creation SQL. (:issue:`505`)\n\n\n`Full list of commits <https://github.com/simonw/datasette/compare/0.28...0.29>`__ between 0.28 and 0.29.\n\n.. _v0_28:\n\n0.28 (2019-05-19)\n-----------------\n\nA `salmagundi <https://adamj.eu/tech/2019/01/18/a-salmagundi-of-django-alpha-announcements/>`__ of new features!\n\n.. _v0_28_databases_that_change:\n\nSupporting databases that change\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFrom the beginning of the project, Datasette has been designed with read-only databases in mind. If a database is guaranteed not to change it opens up all kinds of interesting opportunities - from taking advantage of SQLite immutable mode and HTTP caching to bundling static copies of the database directly in a Docker container. `The interesting ideas in Datasette <https://simonwillison.net/2018/Oct/4/datasette-ideas/>`__ explores this idea in detail.\n\nAs my goals for the project have developed, I realized that read-only databases are no longer the right default. SQLite actually supports concurrent access very well provided only one thread attempts to write to a database at a time, and I keep encountering sensible use-cases for running Datasette on top of a database that is processing inserts and updates.\n\nSo, as-of version 0.28 Datasette no longer assumes that a database file will not change. It is now safe to point Datasette at a SQLite database which is being updated by another process.\n\nMaking this change was a lot of work - see tracking tickets :issue:`418`, :issue:`419` and :issue:`420`. It required new thinking around how Datasette should calculate table counts (an expensive operation against a large, changing database) and also meant reconsidering the \"content hash\" URLs Datasette has used in the past to optimize the performance of HTTP caches.\n\nDatasette can still run against immutable files and gains numerous performance benefits from doing so, but this is no longer the default behaviour. Take a look at the new :ref:`performance` documentation section for details on how to make the most of Datasette against data that you know will be staying read-only and immutable.\n\n.. _v0_28_faceting:\n\nFaceting improvements, and faceting plugins\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDatasette :ref:`facets` provide an intuitive way to quickly summarize and interact with data. Previously the only supported faceting technique was column faceting, but 0.28 introduces two powerful new capabilities: facet-by-JSON-array and the ability to define further facet types using plugins.\n\nFacet by array (:issue:`359`) is only available if your SQLite installation provides the ``json1`` extension. Datasette will automatically detect columns that contain JSON arrays of values and offer a faceting interface against those columns - useful for modelling things like tags without needing to break them out into a new table. See :ref:`facet_by_json_array` for more.\n\nThe new :ref:`plugin_register_facet_classes` plugin hook (`#445 <https://github.com/simonw/datasette/pull/445>`__) can be used to register additional custom facet classes. Each facet class should provide two methods: ``suggest()`` which suggests facet selections that might be appropriate for a provided SQL query, and ``facet_results()`` which executes a facet operation and returns results. Datasette's own faceting implementations have been refactored to use the same API as these plugins.\n\n.. _v0_28_publish_cloudrun:\n\ndatasette publish cloudrun\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n`Google Cloud Run <https://cloud.google.com/run/>`__ is a brand new serverless hosting platform from Google, which allows you to build a Docker container which will run only when HTTP traffic is received and will shut down (and hence cost you nothing) the rest of the time. It's similar to Zeit's Now v1 Docker hosting platform which sadly is `no longer accepting signups <https://hyperion.alpha.spectrum.chat/zeit/now/cannot-create-now-v1-deployments~d206a0d4-5835-4af5-bb5c-a17f0171fb25?m=MTU0Njk2NzgwODM3OA==>`__ from new users.\n\nThe new ``datasette publish cloudrun`` command was contributed by Romain Primet (`#434 <https://github.com/simonw/datasette/pull/434>`__) and publishes selected databases to a new Datasette instance running on Google Cloud Run.\n\nSee :ref:`publish_cloud_run` for full documentation.\n\n.. _v0_28_register_output_renderer:\n\nregister_output_renderer plugins\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRuss Garrett implemented a new Datasette plugin hook called :ref:`register_output_renderer <plugin_register_output_renderer>` (`#441 <https://github.com/simonw/datasette/pull/441>`__) which allows plugins to create additional output renderers in addition to Datasette's default ``.json`` and ``.csv``.\n\nRuss's in-development `datasette-geo <https://github.com/russss/datasette-geo>`__ plugin includes `an example <https://github.com/russss/datasette-geo/blob/d4cecc020848bbde91e9e17bf352f7c70bc3dccf/datasette_plugin_geo/geojson.py>`__ of this hook being used to output ``.geojson`` automatically converted from SpatiaLite.\n\n.. _v0_28_medium_changes:\n\nMedium changes\n~~~~~~~~~~~~~~\n\n- Datasette now conforms to the `Black coding style <https://github.com/python/black>`__ (`#449 <https://github.com/simonw/datasette/pull/449>`__) - and has a unit test to enforce this in the future\n- New :ref:`json_api_table_arguments`:\n   - ``?columnname__in=value1,value2,value3`` filter for executing SQL IN queries against a table, see :ref:`table_arguments` (:issue:`433`)\n   - ``?columnname__date=yyyy-mm-dd`` filter which returns rows where the spoecified datetime column falls on the specified date (`583b22a <https://github.com/simonw/datasette/commit/583b22aa28e26c318de0189312350ab2688c90b1>`__)\n   - ``?tags__arraycontains=tag`` filter which acts against a JSON array contained in a column (`78e45ea <https://github.com/simonw/datasette/commit/78e45ead4d771007c57b307edf8fc920101f8733>`__)\n   - ``?_where=sql-fragment`` filter for the table view  (:issue:`429`)\n   - ``?_fts_table=mytable`` and ``?_fts_pk=mycolumn`` query string options can be used to specify which FTS table to use for a search query - see :ref:`full_text_search_table_or_view` (:issue:`428`)\n- You can now pass the same table filter multiple times - for example, ``?content__not=world&content__not=hello`` will return all rows where the content column is neither ``hello`` or ``world`` (:issue:`288`)\n- You can now specify ``about`` and ``about_url`` metadata (in addition to ``source`` and ``license``) linking to further information about a project - see :ref:`metadata_source_license_about`\n- New ``?_trace=1`` parameter now adds debug information showing every SQL query that was executed while constructing the page (:issue:`435`)\n- ``datasette inspect`` now just calculates table counts, and does not introspect other database metadata (:issue:`462`)\n- Removed ``/-/inspect`` page entirely - this will be replaced by something similar in the future, see :issue:`465`\n- Datasette can now run against an in-memory SQLite database. You can do this by starting it without passing any files or by using the new ``--memory`` option to ``datasette serve``. This can be useful for experimenting with SQLite queries that do not access any data, such as ``SELECT 1+1`` or ``SELECT sqlite_version()``.\n\n.. _v0_28_small_changes:\n\nSmall changes\n~~~~~~~~~~~~~\n\n- We now show the size of the database file next to the download link (:issue:`172`)\n- New ``/-/databases`` introspection page shows currently connected databases (:issue:`470`)\n- Binary data is no longer displayed on the table and row pages (`#442 <https://github.com/simonw/datasette/pull/442>`__ - thanks, Russ Garrett)\n- New show/hide SQL links on custom query pages (:issue:`415`)\n- The :ref:`extra_body_script <plugin_hook_extra_body_script>` plugin hook now accepts an optional ``view_name`` argument (`#443 <https://github.com/simonw/datasette/pull/443>`__ - thanks, Russ Garrett)\n- Bumped Jinja2 dependency to 2.10.1 (`#426 <https://github.com/simonw/datasette/pull/426>`__)\n- All table filters are now documented, and documentation is enforced via unit tests (`2c19a27 <https://github.com/simonw/datasette/commit/2c19a27d15a913e5f3dd443f04067169a6f24634>`__)\n- New project guideline: master should stay shippable at all times! (`31f36e1 <https://github.com/simonw/datasette/commit/31f36e1b97ccc3f4387c80698d018a69798b6228>`__)\n- Fixed a bug where ``sqlite_timelimit()`` occasionally failed to clean up after itself (`bac4e01 <https://github.com/simonw/datasette/commit/bac4e01f40ae7bd19d1eab1fb9349452c18de8f5>`__)\n- We no longer load additional plugins when executing pytest (:issue:`438`)\n- Homepage now links to database views if there are less than five tables in a database (:issue:`373`)\n- The ``--cors`` option is now respected by error pages (:issue:`453`)\n- ``datasette publish heroku`` now uses the ``--include-vcs-ignore`` option, which means it works under Travis CI (`#407 <https://github.com/simonw/datasette/pull/407>`__)\n- ``datasette publish heroku`` now publishes using Python 3.6.8 (`666c374 <https://github.com/simonw/datasette/commit/666c37415a898949fae0437099d62a35b1e9c430>`__)\n- Renamed ``datasette publish now`` to ``datasette publish nowv1`` (:issue:`472`)\n- ``datasette publish nowv1`` now accepts multiple ``--alias`` parameters (`09ef305 <https://github.com/simonw/datasette/commit/09ef305c687399384fe38487c075e8669682deb4>`__)\n- Removed the ``datasette skeleton`` command (:issue:`476`)\n- The :ref:`documentation on how to build the documentation <contributing_documentation>` now recommends ``sphinx-autobuild``\n\n.. _v0_27_1:\n\n0.27.1 (2019-05-09)\n-------------------\n\n- Tiny bugfix release: don't install ``tests/`` in the wrong place. Thanks, Veit Heller.\n\n.. _v0_27:\n\n0.27 (2019-01-31)\n-----------------\n\n- New command: ``datasette plugins`` (:ref:`documentation <plugins_installed>`) shows you the currently installed list of plugins.\n- Datasette can now output `newline-delimited JSON <http://ndjson.org/>`__ using the new ``?_shape=array&_nl=on`` query string option.\n- Added documentation on :ref:`ecosystem`.\n- Now using Python 3.7.2 as the base for the official `Datasette Docker image <https://hub.docker.com/r/datasetteproject/datasette/>`__.\n\n.. _v0_26_1:\n\n0.26.1 (2019-01-10)\n-------------------\n\n- ``/-/versions`` now includes SQLite ``compile_options`` (:issue:`396`)\n- `datasetteproject/datasette <https://hub.docker.com/r/datasetteproject/datasette>`__ Docker image now uses SQLite 3.26.0 (:issue:`397`)\n- Cleaned up some deprecation warnings under Python 3.7\n\n.. _v0_26:\n\n0.26 (2019-01-02)\n-----------------\n\n- ``datasette serve --reload`` now restarts Datasette if a database file changes on disk.\n- ``datasette publish now`` now takes an optional ``--alias mysite.now.sh`` argument. This will attempt to set an alias after the deploy completes.\n- Fixed a bug where the advanced CSV export form failed to include the currently selected filters (:issue:`393`)\n\n.. _v0_25_2:\n\n0.25.2 (2018-12-16)\n-------------------\n\n- ``datasette publish heroku`` now uses the ``python-3.6.7`` runtime\n- Added documentation on :ref:`how to build the documentation <contributing_documentation>`\n- Added documentation covering :ref:`our release process <contributing_release>`\n- Upgraded to pytest 4.0.2\n\n.. _v0_25_1:\n\n0.25.1 (2018-11-04)\n-------------------\n\nDocumentation improvements plus a fix for publishing to Zeit Now.\n\n- ``datasette publish now`` now uses Zeit's v1 platform, to work around the new 100MB image limit. Thanks, @slygent - closes :issue:`366`.\n\n.. _v0_25:\n\n0.25 (2018-09-19)\n-----------------\n\nNew plugin hooks, improved database view support and an easier way to use more recent versions of SQLite.\n\n- New ``publish_subcommand`` plugin hook. A plugin can now add additional ``datasette publish`` publishers in addition to the default ``now`` and ``heroku``, both of which have been refactored into default plugins. :ref:`publish_subcommand documentation <plugin_hook_publish_subcommand>`. Closes :issue:`349`\n- New ``render_cell`` plugin hook. Plugins can now customize how values are displayed in the HTML tables produced by Datasette's browsable interface. `datasette-json-html <https://github.com/simonw/datasette-json-html>`__ and `datasette-render-images <https://github.com/simonw/datasette-render-images>`__ are two new plugins that use this hook. :ref:`render_cell documentation <plugin_hook_render_cell>`. Closes :issue:`352`\n- New ``extra_body_script`` plugin hook, enabling plugins to provide additional JavaScript that should be added to the page footer. :ref:`extra_body_script documentation <plugin_hook_extra_body_script>`.\n- ``extra_css_urls`` and ``extra_js_urls`` hooks now take additional optional parameters, allowing them to be more selective about which pages they apply to. :ref:`Documentation <plugin_hook_extra_css_urls>`.\n- You can now use the :ref:`sortable_columns metadata setting <metadata_sortable_columns>` to explicitly enable sort-by-column in the interface for database views, as well as for specific tables.\n- The new ``fts_table`` and ``fts_pk`` metadata settings can now be used to :ref:`explicitly configure full-text search for a table or a view <full_text_search_table_or_view>`, even if that table is not directly coupled to the SQLite FTS feature in the database schema itself.\n- Datasette will now use `pysqlite3 <https://github.com/coleifer/pysqlite3>`__ in place of the standard library ``sqlite3`` module if it has been installed in the current environment. This makes it much easier to run Datasette against a more recent version of SQLite, including the just-released `SQLite 3.25.0 <https://www.sqlite.org/releaselog/3_25_0.html>`__ which adds window function support. More details on how to use this in :issue:`360`\n- New mechanism that allows :ref:`plugin configuration options <plugins_configuration>` to be set using ``metadata.json``.\n\n\n.. _v0_24:\n\n0.24 (2018-07-23)\n-----------------\n\nA number of small new features:\n\n- ``datasette publish heroku`` now supports ``--extra-options``, fixes `#334 <https://github.com/simonw/datasette/issues/334>`_\n- Custom error message if SpatiaLite is needed for specified database, closes `#331 <https://github.com/simonw/datasette/issues/331>`_\n- New config option: ``truncate_cells_html`` for :ref:`truncating long cell values <setting_truncate_cells_html>` in HTML view - closes `#330 <https://github.com/simonw/datasette/issues/330>`_\n- Documentation for :ref:`datasette publish and datasette package <publishing>`, closes `#337 <https://github.com/simonw/datasette/issues/337>`_\n- Fixed compatibility with Python 3.7\n- ``datasette publish heroku`` now supports app names via the ``-n`` option, which can also be used to overwrite an existing application [Russ Garrett]\n- Title and description metadata can now be set for :ref:`canned SQL queries <canned_queries>`, closes `#342 <https://github.com/simonw/datasette/issues/342>`_\n- New ``force_https_on`` config option, fixes ``https://`` API URLs when deploying to Zeit Now - closes `#333 <https://github.com/simonw/datasette/issues/333>`_\n- ``?_json_infinity=1`` query string argument for handling Infinity/-Infinity values in JSON, closes `#332 <https://github.com/simonw/datasette/issues/332>`_\n- URLs displayed in the results of custom SQL queries are now URLified, closes `#298 <https://github.com/simonw/datasette/issues/298>`_\n\n.. _v0_23_2:\n\n0.23.2 (2018-07-07)\n-------------------\n\nMinor bugfix and documentation release.\n\n- CSV export now respects ``--cors``, fixes `#326 <https://github.com/simonw/datasette/issues/326>`_\n- :ref:`Installation instructions <installation>`, including docker image - closes `#328 <https://github.com/simonw/datasette/issues/328>`_\n- Fix for row pages for tables with / in, closes `#325 <https://github.com/simonw/datasette/issues/325>`_\n\n.. _v0_23_1:\n\n0.23.1 (2018-06-21)\n-------------------\n\nMinor bugfix release.\n\n- Correctly display empty strings in HTML table, closes `#314 <https://github.com/simonw/datasette/issues/314>`_\n- Allow \".\" in database filenames, closes `#302 <https://github.com/simonw/datasette/issues/302>`_\n- 404s ending in slash redirect to remove that slash, closes `#309 <https://github.com/simonw/datasette/issues/309>`_\n- Fixed incorrect display of compound primary keys with foreign key\n  references. Closes `#319 <https://github.com/simonw/datasette/issues/319>`_\n- Docs + example of canned SQL query using || concatenation. Closes `#321 <https://github.com/simonw/datasette/issues/321>`_\n- Correctly display facets with value of 0 - closes `#318 <https://github.com/simonw/datasette/issues/318>`_\n- Default 'expand labels' to checked in CSV advanced export\n\n.. _v0_23:\n\n0.23 (2018-06-18)\n-----------------\n\nThis release features CSV export, improved options for foreign key expansions,\nnew configuration settings and improved support for SpatiaLite.\n\nSee `datasette/compare/0.22.1...0.23\n<https://github.com/simonw/datasette/compare/0.22.1...0.23>`_ for a full list of\ncommits added since the last release.\n\nCSV export\n~~~~~~~~~~\n\nAny Datasette table, view or custom SQL query can now be exported as CSV.\n\n.. image:: https://github.com/simonw/datasette-screenshots/blob/0.62/advanced-export.png?raw=true\n   :alt: Advanced export form. You can get the data in different JSON shapes, and CSV options are download file, expand labels and stream all rows.\n\nCheck out the :ref:`CSV export documentation <csv_export>` for more details, or\ntry the feature out on\nhttps://fivethirtyeight.datasettes.com/fivethirtyeight/bechdel%2Fmovies\n\nIf your table has more than :ref:`setting_max_returned_rows` (default 1,000)\nDatasette provides the option to *stream all rows*. This option takes advantage\nof async Python and Datasette's efficient :ref:`pagination <pagination>` to\niterate through the entire matching result set and stream it back as a\ndownloadable CSV file.\n\nForeign key expansions\n~~~~~~~~~~~~~~~~~~~~~~\n\nWhen Datasette detects a foreign key reference it attempts to resolve a label\nfor that reference (automatically or using the :ref:`label_columns` metadata\noption) so it can display a link to the associated row.\n\nThis expansion is now also available for JSON and CSV representations of the\ntable, using the new ``_labels=on`` query string option. See\n:ref:`expand_foreign_keys` for more details.\n\nNew configuration settings\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDatasette's :ref:`settings` now also supports boolean settings. A number of new\nconfiguration options have been added:\n\n* ``num_sql_threads`` - the number of threads used to execute SQLite queries. Defaults to 3.\n* ``allow_facet`` - enable or disable custom :ref:`facets` using the `_facet=` parameter. Defaults to on.\n* ``suggest_facets`` - should Datasette suggest facets? Defaults to on.\n* ``allow_download`` - should users be allowed to download the entire SQLite database? Defaults to on.\n* ``allow_sql`` - should users be allowed to execute custom SQL queries? Defaults to on.\n* ``default_cache_ttl`` - Default HTTP caching max-age header in seconds. Defaults to 365 days - caching can be disabled entirely by settings this to 0.\n* ``cache_size_kb`` - Set the amount of memory SQLite uses for its `per-connection cache <https://www.sqlite.org/pragma.html#pragma_cache_size>`_, in KB.\n* ``allow_csv_stream`` - allow users to stream entire result sets as a single CSV file. Defaults to on.\n* ``max_csv_mb`` - maximum size of a returned CSV file in MB. Defaults to 100MB, set to 0 to disable this limit.\n\nControl HTTP caching with ?_ttl=\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can now customize the HTTP max-age header that is sent on a per-URL basis, using the new ``?_ttl=`` query string parameter.\n\nYou can set this to any value in seconds, or you can set it to 0 to disable HTTP caching entirely.\n\nConsider for example this query which returns a randomly selected member of the Avengers::\n\n    select * from [avengers/avengers] order by random() limit 1\n\nIf you hit the following page repeatedly you will get the same result, due to HTTP caching:\n\n`/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random%28%29+limit+1 <https://fivethirtyeight.datasettes.com/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random%28%29+limit+1>`_\n\nBy adding `?_ttl=0` to the zero you can ensure the page will not be cached and get back a different super hero every time:\n\n`/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random%28%29+limit+1&_ttl=0 <https://fivethirtyeight.datasettes.com/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random%28%29+limit+1&_ttl=0>`_\n\nImproved support for SpatiaLite\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe `SpatiaLite module <https://www.gaia-gis.it/fossil/libspatialite/index>`_\nfor SQLite adds robust geospatial features to the database.\n\nGetting SpatiaLite working can be tricky, especially if you want to use the most\nrecent alpha version (with support for K-nearest neighbor).\n\nDatasette now includes :ref:`extensive documentation on SpatiaLite\n<spatialite>`, and thanks to `Ravi Kotecha <https://github.com/r4vi>`_ our GitHub\nrepo includes a `Dockerfile\n<https://github.com/simonw/datasette/blob/master/Dockerfile>`_ that can build\nthe latest SpatiaLite and configure it for use with Datasette.\n\nThe ``datasette publish`` and ``datasette package`` commands now accept a new\n``--spatialite`` argument which causes them to install and configure SpatiaLite\nas part of the container they deploy.\n\nlatest.datasette.io\n~~~~~~~~~~~~~~~~~~~\n\nEvery commit to Datasette master is now automatically deployed by Travis CI to\nhttps://latest.datasette.io/ - ensuring there is always a live demo of the\nlatest version of the software.\n\nThe demo uses `the fixtures\n<https://github.com/simonw/datasette/blob/master/tests/fixtures.py>`_ from our\nunit tests, ensuring it demonstrates the same range of functionality that is\ncovered by the tests.\n\nYou can see how the deployment mechanism works in our `.travis.yml\n<https://github.com/simonw/datasette/blob/master/.travis.yml>`_ file.\n\nMiscellaneous\n~~~~~~~~~~~~~\n\n* Got JSON data in one of your columns? Use the new ``?_json=COLNAME`` argument\n  to tell Datasette to return that JSON value directly rather than encoding it\n  as a string.\n* If you just want an array of the first value of each row, use the new\n  ``?_shape=arrayfirst`` option - `example\n  <https://latest.datasette.io/fixtures.json?sql=select+neighborhood+from+facetable+order+by+pk+limit+101&_shape=arrayfirst>`_.\n\n0.22.1 (2018-05-23)\n-------------------\n\nBugfix release, plus we now use `versioneer <https://github.com/warner/python-versioneer>`_ for our version numbers.\n\n- Faceting no longer breaks pagination, fixes `#282 <https://github.com/simonw/datasette/issues/282>`_\n- Add ``__version_info__`` derived from `__version__` [Robert Gieseke]\n\n  This might be tuple of more than two values (major and minor\n  version) if commits have been made after a release.\n- Add version number support with Versioneer. [Robert Gieseke]\n\n  Versioneer Licence:\n  Public Domain (CC0-1.0)\n\n  Closes `#273 <https://github.com/simonw/datasette/issues/273>`_\n- Refactor inspect logic [Russ Garrett]\n\n0.22 (2018-05-20)\n-----------------\n\nThe big new feature in this release is :ref:`facets`. Datasette can now apply faceted browse to any column in any table. It will also suggest possible facets. See the `Datasette Facets <https://simonwillison.net/2018/May/20/datasette-facets/>`_ announcement post for more details.\n\nIn addition to the work on facets:\n\n- Added `docs for introspection endpoints <https://docs.datasette.io/en/stable/introspection.html>`_\n\n- New ``--config`` option, added ``--help-config``, closes `#274 <https://github.com/simonw/datasette/issues/274>`_\n\n  Removed the ``--page_size=`` argument to ``datasette serve`` in favour of::\n\n      datasette serve --config default_page_size:50 mydb.db\n\n  Added new help section::\n\n      datasette --help-config\n\n  ::\n\n      Config options:\n        default_page_size            Default page size for the table view\n                                     (default=100)\n        max_returned_rows            Maximum rows that can be returned from a table\n                                     or custom query (default=1000)\n        sql_time_limit_ms            Time limit for a SQL query in milliseconds\n                                     (default=1000)\n        default_facet_size           Number of values to return for requested facets\n                                     (default=30)\n        facet_time_limit_ms          Time limit for calculating a requested facet\n                                     (default=200)\n        facet_suggest_time_limit_ms  Time limit for calculating a suggested facet\n                                     (default=50)\n- Only apply responsive table styles to ``.rows-and-column``\n\n  Otherwise they interfere with tables in the description, e.g. on\n  https://fivethirtyeight.datasettes.com/fivethirtyeight/nba-elo%2Fnbaallelo\n\n- Refactored views into new ``views/`` modules, refs `#256 <https://github.com/simonw/datasette/issues/256>`_\n- `Documentation for SQLite full-text search <https://docs.datasette.io/en/stable/full_text_search.html>`_ support, closes `#253 <https://github.com/simonw/datasette/issues/253>`_\n- ``/-/versions`` now includes SQLite ``fts_versions``, closes `#252 <https://github.com/simonw/datasette/issues/252>`_\n\n0.21 (2018-05-05)\n-----------------\n\nNew JSON ``_shape=`` options, the ability to set table ``_size=`` and a mechanism for searching within specific columns.\n\n- Default tests to using a longer timelimit\n\n  Every now and then a test will fail in Travis CI on Python 3.5 because it hit\n  the default 20ms SQL time limit.\n\n  Test fixtures now default to a 200ms time limit, and we only use the 20ms time\n  limit for the specific test that tests query interruption. This should make\n  our tests on Python 3.5 in Travis much more stable.\n- Support ``_search_COLUMN=text`` searches, closes `#237 <https://github.com/simonw/datasette/issues/237>`_\n- Show version on ``/-/plugins`` page, closes `#248 <https://github.com/simonw/datasette/issues/248>`_\n- ``?_size=max`` option, closes `#249 <https://github.com/simonw/datasette/issues/249>`_\n- Added ``/-/versions`` and ``/-/versions.json``, closes `#244 <https://github.com/simonw/datasette/issues/244>`_\n\n  Sample output::\n\n      {\n        \"python\": {\n          \"version\": \"3.6.3\",\n          \"full\": \"3.6.3 (default, Oct  4 2017, 06:09:38) \\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)]\"\n        },\n        \"datasette\": {\n          \"version\": \"0.20\"\n        },\n        \"sqlite\": {\n          \"version\": \"3.23.1\",\n          \"extensions\": {\n            \"json1\": null,\n            \"spatialite\": \"4.3.0a\"\n          }\n        }\n      }\n- Renamed ``?_sql_time_limit_ms=`` to ``?_timelimit``, closes `#242 <https://github.com/simonw/datasette/issues/242>`_\n- New ``?_shape=array`` option + tweaks to ``_shape``, closes `#245 <https://github.com/simonw/datasette/issues/245>`_\n\n  * Default is now ``?_shape=arrays`` (renamed from ``lists``)\n  * New ``?_shape=array`` returns an array of objects as the root object\n  * Changed ``?_shape=object`` to return the object as the root\n  * Updated docs\n\n- FTS tables now detected by ``inspect()``, closes `#240 <https://github.com/simonw/datasette/issues/240>`_\n- New ``?_size=XXX`` query string parameter for table view, closes `#229 <https://github.com/simonw/datasette/issues/229>`_\n\n  Also added documentation for all of the ``_special`` arguments.\n\n  Plus deleted some duplicate logic implementing ``_group_count``.\n- If ``max_returned_rows==page_size``, increment ``max_returned_rows`` - fixes `#230 <https://github.com/simonw/datasette/issues/230>`_\n- New ``hidden: True`` option for table metadata, closes `#239 <https://github.com/simonw/datasette/issues/239>`_\n- Hide ``idx_*`` tables if spatialite detected, closes `#228 <https://github.com/simonw/datasette/issues/228>`_\n- Added ``class=rows-and-columns`` to custom query results table\n- Added CSS class ``rows-and-columns`` to main table\n- ``label_column`` option in ``metadata.json`` - closes `#234 <https://github.com/simonw/datasette/issues/234>`_\n\n0.20 (2018-04-20)\n-----------------\n\nMostly new work on the :ref:`plugins` mechanism: plugins can now bundle static assets and custom templates, and ``datasette publish`` has a new ``--install=name-of-plugin`` option.\n\n- Add col-X classes to HTML table on custom query page\n- Fixed out-dated template in documentation\n- Plugins can now bundle custom templates, `#224 <https://github.com/simonw/datasette/issues/224>`_\n- Added /-/metadata /-/plugins /-/inspect, `#225 <https://github.com/simonw/datasette/issues/225>`_\n- Documentation for --install option, refs `#223 <https://github.com/simonw/datasette/issues/223>`_\n- Datasette publish/package --install option, `#223 <https://github.com/simonw/datasette/issues/223>`_\n- Fix for plugins in Python 3.5, `#222 <https://github.com/simonw/datasette/issues/222>`_\n- New plugin hooks: extra_css_urls() and extra_js_urls(), `#214 <https://github.com/simonw/datasette/issues/214>`_\n- /-/static-plugins/PLUGIN_NAME/ now serves static/ from plugins\n- <th> now gets class=\"col-X\" - plus added col-X documentation\n- Use to_css_class for table cell column classes\n\n  This ensures that columns with spaces in the name will still\n  generate usable CSS class names. Refs `#209 <https://github.com/simonw/datasette/issues/209>`_\n- Add column name classes to <td>s, make PK bold [Russ Garrett]\n- Don't duplicate simple primary keys in the link column [Russ Garrett]\n\n  When there's a simple (single-column) primary key, it looks weird to\n  duplicate it in the link column.\n\n  This change removes the second PK column and treats the link column as\n  if it were the PK column from a header/sorting perspective.\n- Correct escaping for HTML display of row links [Russ Garrett]\n- Longer time limit for test_paginate_compound_keys\n\n  It was failing intermittently in Travis - see `#209 <https://github.com/simonw/datasette/issues/209>`_\n- Use application/octet-stream for downloadable databases\n- Updated PyPI classifiers\n- Updated PyPI link to pypi.org\n\n0.19 (2018-04-16)\n-----------------\n\nThis is the first preview of the new Datasette plugins mechanism. Only two\nplugin hooks are available so far - for custom SQL functions and custom template\nfilters. There's plenty more to come - read `the documentation\n<https://docs.datasette.io/en/stable/plugins.html>`_ and get involved in\n`the tracking ticket <https://github.com/simonw/datasette/issues/14>`_ if you\nhave feedback on the direction so far.\n\n- Fix for ``_sort_desc=sortable_with_nulls`` test, refs `#216 <https://github.com/simonw/datasette/issues/216>`_\n\n- Fixed `#216 <https://github.com/simonw/datasette/issues/216>`_ - paginate correctly when sorting by nullable column\n\n- Initial documentation for plugins, closes `#213 <https://github.com/simonw/datasette/issues/213>`_\n\n  https://docs.datasette.io/en/stable/plugins.html\n\n- New ``--plugins-dir=plugins/`` option (`#212 <https://github.com/simonw/datasette/issues/212>`_)\n\n  New option causing Datasette to load and evaluate all of the Python files in\n  the specified directory and register any plugins that are defined in those\n  files.\n\n  This new option is available for the following commands::\n\n      datasette serve mydb.db --plugins-dir=plugins/\n      datasette publish now/heroku mydb.db --plugins-dir=plugins/\n      datasette package mydb.db --plugins-dir=plugins/\n\n- Start of the plugin system, based on pluggy (`#210 <https://github.com/simonw/datasette/issues/14>`_)\n\n  Uses https://pluggy.readthedocs.io/ originally created for the py.test project\n\n  We're starting with two plugin hooks:\n\n  ``prepare_connection(conn)``\n\n  This is called when a new SQLite connection is created. It can be used to register custom SQL functions.\n\n  ``prepare_jinja2_environment(env)``\n\n  This is called with the Jinja2 environment. It can be used to register custom template tags and filters.\n\n  An example plugin which uses these two hooks can be found at https://github.com/simonw/datasette-plugin-demos or installed using ``pip install datasette-plugin-demos``\n\n  Refs `#14 <https://github.com/simonw/datasette/issues/14>`_\n\n- Return HTTP 405 on InvalidUsage rather than 500. [Russ Garrett]\n\n  This also stops it filling up the logs. This happens for HEAD requests\n  at the moment - which perhaps should be handled better, but that's a\n  different issue.\n\n\n0.18 (2018-04-14)\n-----------------\n\nThis release introduces `support for units <https://docs.datasette.io/en/stable/metadata.html#specifying-units-for-a-column>`_,\ncontributed by Russ Garrett (`#203 <https://github.com/simonw/datasette/issues/203>`_).\nYou can now optionally specify the units for specific columns using ``metadata.json``.\nOnce specified, units will be displayed in the HTML view of your table. They also become\navailable for use in filters - if a column is configured with a unit of distance, you can\nrequest all rows where that column is less than 50 meters or more than 20 feet for example.\n\n- Link foreign keys which don't have labels. [Russ Garrett]\n\n  This renders unlabeled FKs as simple links.\n\n  Also includes bonus fixes for two minor issues:\n\n  * In foreign key link hrefs the primary key was escaped using HTML\n    escaping rather than URL escaping. This broke some non-integer PKs.\n  * Print tracebacks to console when handling 500 errors.\n\n- Fix SQLite error when loading rows with no incoming FKs. [Russ\n  Garrett]\n\n  This fixes an error caused by an invalid query when loading incoming FKs.\n\n  The error was ignored due to async but it still got printed to the\n  console.\n\n- Allow custom units to be registered with Pint. [Russ Garrett]\n- Support units in filters. [Russ Garrett]\n- Tidy up units support. [Russ Garrett]\n\n  * Add units to exported JSON\n  * Units key in metadata skeleton\n  * Docs\n\n- Initial units support. [Russ Garrett]\n\n  Add support for specifying units for a column in ``metadata.json`` and\n  rendering them on display using\n  `pint <https://pint.readthedocs.io/en/latest/>`_\n\n\n0.17 (2018-04-13)\n-----------------\n- Release 0.17 to fix issues with PyPI\n\n\n0.16 (2018-04-13)\n-----------------\n- Better mechanism for handling errors; 404s for missing table/database\n\n  New error mechanism closes `#193 <https://github.com/simonw/datasette/issues/193>`_\n\n  404s for missing tables/databases closes `#184 <https://github.com/simonw/datasette/issues/184>`_\n\n- long_description in markdown for the new PyPI\n- Hide SpatiaLite system tables. [Russ Garrett]\n- Allow ``explain select`` / ``explain query plan select`` `#201 <https://github.com/simonw/datasette/issues/201>`_\n- Datasette inspect now finds primary_keys `#195 <https://github.com/simonw/datasette/issues/195>`_\n- Ability to sort using form fields (for mobile portrait mode) `#199 <https://github.com/simonw/datasette/issues/199>`_\n\n  We now display sort options as a select box plus a descending checkbox, which\n  means you can apply sort orders even in portrait mode on a mobile phone where\n  the column headers are hidden.\n\n0.15 (2018-04-09)\n-----------------\n\nThe biggest new feature in this release is the ability to sort by column. On the\ntable page the column headers can now be clicked to apply sort (or descending\nsort), or you can specify ``?_sort=column`` or ``?_sort_desc=column`` directly\nin the URL.\n\n- ``table_rows`` => ``table_rows_count``, ``filtered_table_rows`` =>\n  ``filtered_table_rows_count``\n\n  Renamed properties. Closes `#194 <https://github.com/simonw/datasette/issues/194>`_\n\n- New ``sortable_columns`` option in ``metadata.json`` to control sort options.\n\n  You can now explicitly set which columns in a table can be used for sorting\n  using the ``_sort`` and ``_sort_desc`` arguments using ``metadata.json``::\n\n      {\n          \"databases\": {\n              \"database1\": {\n                  \"tables\": {\n                      \"example_table\": {\n                          \"sortable_columns\": [\n                              \"height\",\n                              \"weight\"\n                          ]\n                      }\n                  }\n              }\n          }\n      }\n\n  Refs `#189 <https://github.com/simonw/datasette/issues/189>`_\n\n- Column headers now link to sort/desc sort - refs `#189 <https://github.com/simonw/datasette/issues/189>`_\n\n- ``_sort`` and ``_sort_desc`` parameters for table views\n\n  Allows for paginated sorted results based on a specified column.\n\n  Refs `#189 <https://github.com/simonw/datasette/issues/189>`_\n\n- Total row count now correct even if ``_next`` applied\n\n- Use .custom_sql() for _group_count implementation (refs `#150 <https://github.com/simonw/datasette/issues/150>`_)\n\n- Make HTML title more readable in query template (`#180 <https://github.com/simonw/datasette/issues/180>`_) [Ryan Pitts]\n\n- New ``?_shape=objects/object/lists`` param for JSON API (`#192 <https://github.com/simonw/datasette/issues/192>`_)\n\n  New ``_shape=`` parameter replacing old ``.jsono`` extension\n\n  Now instead of this::\n\n      /database/table.jsono\n\n  We use the ``_shape`` parameter like this::\n\n      /database/table.json?_shape=objects\n\n  Also introduced a new ``_shape`` called ``object`` which looks like this::\n\n      /database/table.json?_shape=object\n\n  Returning an object for the rows key::\n\n      ...\n      \"rows\": {\n          \"pk1\": {\n              ...\n          },\n          \"pk2\": {\n              ...\n          }\n      }\n\n  Refs `#122 <https://github.com/simonw/datasette/issues/122>`_\n\n- Utility for writing test database fixtures to a .db file\n\n  ``python tests/fixtures.py /tmp/hello.db``\n\n  This is useful for making a SQLite database of the test fixtures for\n  interactive exploration.\n\n- Compound primary key ``_next=`` now plays well with extra filters\n\n  Closes `#190 <https://github.com/simonw/datasette/issues/190>`_\n\n- Fixed bug with keyset pagination over compound primary keys\n\n  Refs `#190 <https://github.com/simonw/datasette/issues/190>`_\n\n- Database/Table views inherit ``source/license/source_url/license_url``\n  metadata\n\n  If you set the ``source_url/license_url/source/license`` fields in your root\n  metadata those values will now be inherited all the way down to the database\n  and table templates.\n\n  The ``title/description`` are NOT inherited.\n\n  Also added unit tests for the HTML generated by the metadata.\n\n  Refs `#185 <https://github.com/simonw/datasette/issues/185>`_\n\n- Add metadata, if it exists, to heroku temp dir (`#178 <https://github.com/simonw/datasette/issues/178>`_) [Tony Hirst]\n- Initial documentation for pagination\n- Broke up test_app into test_api and test_html\n- Fixed bug with .json path regular expression\n\n  I had a table called ``geojson`` and it caused an exception because the regex\n  was matching ``.json`` and not ``\\.json``\n\n- Deploy to Heroku with Python 3.6.3\n\n0.14 (2017-12-09)\n-----------------\n\nThe theme of this release is customization: Datasette now allows every aspect\nof its presentation `to be customized <https://docs.datasette.io/en/stable/custom_templates.html>`_\neither using additional CSS or by providing entirely new templates.\n\nDatasette's `metadata.json format <https://docs.datasette.io/en/stable/metadata.html>`_\nhas also been expanded, to allow per-database and per-table metadata. A new\n``datasette skeleton`` command can be used to generate a skeleton JSON file\nready to be filled in with per-database and per-table details.\n\nThe ``metadata.json`` file can also be used to define\n`canned queries <https://docs.datasette.io/en/stable/sql_queries.html#canned-queries>`_,\nas a more powerful alternative to SQL views.\n\n- ``extra_css_urls``/``extra_js_urls`` in metadata\n\n  A mechanism in the ``metadata.json`` format for adding custom CSS and JS urls.\n\n  Create a ``metadata.json`` file that looks like this::\n\n      {\n          \"extra_css_urls\": [\n              \"https://simonwillison.net/static/css/all.bf8cd891642c.css\"\n          ],\n          \"extra_js_urls\": [\n              \"https://code.jquery.com/jquery-3.2.1.slim.min.js\"\n          ]\n      }\n\n  Then start datasette like this::\n\n      datasette mydb.db --metadata=metadata.json\n\n  The CSS and JavaScript files will be linked in the ``<head>`` of every page.\n\n  You can also specify a SRI (subresource integrity hash) for these assets::\n\n      {\n          \"extra_css_urls\": [\n              {\n                  \"url\": \"https://simonwillison.net/static/css/all.bf8cd891642c.css\",\n                  \"sri\": \"sha384-9qIZekWUyjCyDIf2YK1FRoKiPJq4PHt6tp/ulnuuyRBvazd0hG7pWbE99zvwSznI\"\n              }\n          ],\n          \"extra_js_urls\": [\n              {\n                  \"url\": \"https://code.jquery.com/jquery-3.2.1.slim.min.js\",\n                  \"sri\": \"sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g=\"\n              }\n          ]\n      }\n\n  Modern browsers will only execute the stylesheet or JavaScript if the SRI hash\n  matches the content served. You can generate hashes using https://www.srihash.org/\n\n- Auto-link column values that look like URLs (`#153 <https://github.com/simonw/datasette/issues/153>`_)\n\n- CSS styling hooks as classes on the body (`#153 <https://github.com/simonw/datasette/issues/153>`_)\n\n  Every template now gets CSS classes in the body designed to support custom\n  styling.\n\n  The index template (the top level page at ``/``) gets this::\n\n      <body class=\"index\">\n\n  The database template (``/dbname/``) gets this::\n\n      <body class=\"db db-dbname\">\n\n  The table template (``/dbname/tablename``) gets::\n\n      <body class=\"table db-dbname table-tablename\">\n\n  The row template (``/dbname/tablename/rowid``) gets::\n\n      <body class=\"row db-dbname table-tablename\">\n\n  The ``db-x`` and ``table-x`` classes use the database or table names themselves IF\n  they are valid CSS identifiers. If they aren't, we strip any invalid\n  characters out and append a 6 character md5 digest of the original name, in\n  order to ensure that multiple tables which resolve to the same stripped\n  character version still have different CSS classes.\n\n  Some examples (extracted from the unit tests)::\n\n      \"simple\" => \"simple\"\n      \"MixedCase\" => \"MixedCase\"\n      \"-no-leading-hyphens\" => \"no-leading-hyphens-65bea6\"\n      \"_no-leading-underscores\" => \"no-leading-underscores-b921bc\"\n      \"no spaces\" => \"no-spaces-7088d7\"\n      \"-\" => \"336d5e\"\n      \"no $ characters\" => \"no--characters-59e024\"\n\n- ``datasette --template-dir=mytemplates/`` argument\n\n  You can now pass an additional argument specifying a directory to look for\n  custom templates in.\n\n  Datasette will fall back on the default templates if a template is not\n  found in that directory.\n\n- Ability to over-ride templates for individual tables/databases.\n\n  It is now possible to over-ride templates on a per-database / per-row or per-\n  table basis.\n\n  When you access e.g. ``/mydatabase/mytable`` Datasette will look for the following::\n\n      - table-mydatabase-mytable.html\n      - table.html\n\n  If you provided a ``--template-dir`` argument to datasette serve it will look in\n  that directory first.\n\n  The lookup rules are as follows::\n\n      Index page (/):\n          index.html\n\n      Database page (/mydatabase):\n          database-mydatabase.html\n          database.html\n\n      Table page (/mydatabase/mytable):\n          table-mydatabase-mytable.html\n          table.html\n\n      Row page (/mydatabase/mytable/id):\n          row-mydatabase-mytable.html\n          row.html\n\n  If a table name has spaces or other unexpected characters in it, the template\n  filename will follow the same rules as our custom ``<body>`` CSS classes\n  - for example, a table called \"Food Trucks\"\n  will attempt to load the following templates::\n\n      table-mydatabase-Food-Trucks-399138.html\n      table.html\n\n  It is possible to extend the default templates using Jinja template\n  inheritance. If you want to customize EVERY row template with some additional\n  content you can do so by creating a row.html template like this::\n\n      {% extends \"default:row.html\" %}\n\n      {% block content %}\n      <h1>EXTRA HTML AT THE TOP OF THE CONTENT BLOCK</h1>\n      <p>This line renders the original block:</p>\n      {{ super() }}\n      {% endblock %}\n\n- ``--static`` option for datasette serve (`#160 <https://github.com/simonw/datasette/issues/160>`_)\n\n  You can now tell Datasette to serve static files from a specific location at a\n  specific mountpoint.\n\n  For example::\n\n    datasette serve mydb.db --static extra-css:/tmp/static/css\n\n  Now if you visit this URL::\n\n    http://localhost:8001/extra-css/blah.css\n\n  The following file will be served::\n\n    /tmp/static/css/blah.css\n\n- Canned query support.\n\n  Named canned queries can now be defined in ``metadata.json`` like this::\n\n      {\n          \"databases\": {\n              \"timezones\": {\n                  \"queries\": {\n                      \"timezone_for_point\": \"select tzid from timezones ...\"\n                  }\n              }\n          }\n      }\n\n  These will be shown in a new \"Queries\" section beneath \"Views\" on the database page.\n\n- New ``datasette skeleton`` command for generating ``metadata.json`` (`#164 <https://github.com/simonw/datasette/issues/164>`_)\n\n- ``metadata.json`` support for per-table/per-database metadata (`#165 <https://github.com/simonw/datasette/issues/165>`_)\n\n  Also added support for descriptions and HTML descriptions.\n\n  Here's an example metadata.json file illustrating custom per-database and per-\n  table metadata::\n\n      {\n          \"title\": \"Overall datasette title\",\n          \"description_html\": \"This is a <em>description with HTML</em>.\",\n          \"databases\": {\n              \"db1\": {\n                  \"title\": \"First database\",\n                  \"description\": \"This is a string description & has no HTML\",\n                  \"license_url\": \"http://example.com/\",\n              \"license\": \"The example license\",\n                  \"queries\": {\n                    \"canned_query\": \"select * from table1 limit 3;\"\n                  },\n                  \"tables\": {\n                      \"table1\": {\n                          \"title\": \"Custom title for table1\",\n                          \"description\": \"Tables can have descriptions too\",\n                          \"source\": \"This has a custom source\",\n                          \"source_url\": \"http://example.com/\"\n                      }\n                  }\n              }\n          }\n      }\n\n- Renamed ``datasette build`` command to ``datasette inspect`` (`#130 <https://github.com/simonw/datasette/issues/130>`_)\n\n- Upgrade to Sanic 0.7.0 (`#168 <https://github.com/simonw/datasette/issues/168>`_)\n\n  https://github.com/channelcat/sanic/releases/tag/0.7.0\n\n- Package and publish commands now accept ``--static`` and ``--template-dir``\n\n  Example usage::\n\n      datasette package --static css:extra-css/ --static js:extra-js/ \\\n        sf-trees.db --template-dir templates/ --tag sf-trees --branch master\n\n  This creates a local Docker image that includes copies of the templates/,\n  extra-css/ and extra-js/ directories. You can then run it like this::\n\n    docker run -p 8001:8001 sf-trees\n\n  For publishing to Zeit now::\n\n    datasette publish now --static css:extra-css/ --static js:extra-js/ \\\n      sf-trees.db --template-dir templates/ --name sf-trees --branch master\n\n- HTML comment showing which templates were considered for a page (`#171 <https://github.com/simonw/datasette/issues/171>`_)\n\n0.13 (2017-11-24)\n-----------------\n- Search now applies to current filters.\n\n  Combined search into the same form as filters.\n\n  Closes `#133`_\n\n- Much tidier design for table view header.\n\n  Closes `#147`_\n\n- Added ``?column__not=blah`` filter.\n\n  Closes `#148`_\n\n- Row page now resolves foreign keys.\n\n  Closes `#132`_\n\n- Further tweaks to select/input filter styling.\n\n  Refs `#86`_ - thanks for the help, @natbat!\n\n- Show linked foreign key in table cells.\n\n- Added UI for editing table filters.\n\n  Refs `#86`_\n\n- Hide FTS-created tables on index pages.\n\n  Closes `#129`_\n\n- Add publish to heroku support [Jacob Kaplan-Moss]\n\n  ``datasette publish heroku mydb.db``\n\n  Pull request `#104`_\n\n- Initial implementation of ``?_group_count=column``.\n\n  URL shortcut for counting rows grouped by one or more columns.\n\n  ``?_group_count=column1&_group_count=column2`` works as well.\n\n  SQL generated looks like this::\n\n      select \"qSpecies\", count(*) as \"count\"\n      from Street_Tree_List\n      group by \"qSpecies\"\n      order by \"count\" desc limit 100\n\n  Or for two columns like this::\n\n      select \"qSpecies\", \"qSiteInfo\", count(*) as \"count\"\n      from Street_Tree_List\n      group by \"qSpecies\", \"qSiteInfo\"\n      order by \"count\" desc limit 100\n\n  Refs `#44`_\n\n- Added ``--build=master`` option to datasette publish and package.\n\n  The ``datasette publish`` and ``datasette package`` commands both now accept an\n  optional ``--build`` argument. If provided, this can be used to specify a branch\n  published to GitHub that should be built into the container.\n\n  This makes it easier to test code that has not yet been officially released to\n  PyPI, e.g.::\n\n      datasette publish now mydb.db --branch=master\n\n- Implemented ``?_search=XXX`` + UI if a FTS table is detected.\n\n  Closes `#131`_\n\n- Added ``datasette --version`` support.\n\n- Table views now show expanded foreign key references, if possible.\n\n  If a table has foreign key columns, and those foreign key tables have\n  ``label_columns``, the TableView will now query those other tables for the\n  corresponding values and display those values as links in the corresponding\n  table cells.\n\n  label_columns are currently detected by the ``inspect()`` function, which looks\n  for any table that has just two columns - an ID column and one other - and\n  sets the ``label_column`` to be that second non-ID column.\n\n- Don't prevent tabbing to \"Run SQL\" button (`#117`_) [Robert Gieseke]\n\n  See comment in `#115`_\n\n- Add keyboard shortcut to execute SQL query (`#115`_) [Robert Gieseke]\n\n- Allow ``--load-extension`` to be set via environment variable.\n\n- Add support for ``?field__isnull=1`` (`#107`_) [Ray N]\n\n- Add spatialite, switch to debian and local build (`#114`_) [Ariel N\u00fa\u00f1ez]\n\n- Added ``--load-extension`` argument to datasette serve.\n\n  Allows loading of SQLite extensions. Refs `#110`_.\n\n.. _#133: https://github.com/simonw/datasette/issues/133\n.. _#147: https://github.com/simonw/datasette/issues/147\n.. _#148: https://github.com/simonw/datasette/issues/148\n.. _#132: https://github.com/simonw/datasette/issues/132\n.. _#86: https://github.com/simonw/datasette/issues/86\n.. _#129: https://github.com/simonw/datasette/issues/129\n.. _#104: https://github.com/simonw/datasette/issues/104\n.. _#44: https://github.com/simonw/datasette/issues/44\n.. _#131: https://github.com/simonw/datasette/issues/131\n.. _#115: https://github.com/simonw/datasette/issues/115\n.. _#117: https://github.com/simonw/datasette/issues/117\n.. _#107: https://github.com/simonw/datasette/issues/107\n.. _#114: https://github.com/simonw/datasette/issues/114\n.. _#110: https://github.com/simonw/datasette/issues/110\n\n0.12 (2017-11-16)\n-----------------\n- Added ``__version__``, now displayed as tooltip in page footer (`#108`_).\n- Added initial docs, including a changelog (`#99`_).\n- Turned on auto-escaping in Jinja.\n- Added a UI for editing named parameters (`#96`_).\n\n  You can now construct a custom SQL statement using SQLite named\n  parameters (e.g. ``:name``) and datasette will display form fields for\n  editing those parameters. `Here\u2019s an example`_ which lets you see the\n  most popular names for dogs of different species registered through\n  various dog registration schemes in Australia.\n\n.. _Here\u2019s an example: https://australian-dogs.now.sh/australian-dogs-3ba9628?sql=select+name%2C+count%28*%29+as+n+from+%28%0D%0A%0D%0Aselect+upper%28%22Animal+name%22%29+as+name+from+%5BAdelaide-City-Council-dog-registrations-2013%5D+where+Breed+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28Animal_Name%29+as+name+from+%5BAdelaide-City-Council-dog-registrations-2014%5D+where+Breed_Description+like+%3Abreed%0D%0A%0D%0Aunion+all+%0D%0A%0D%0Aselect+upper%28Animal_Name%29+as+name+from+%5BAdelaide-City-Council-dog-registrations-2015%5D+where+Breed_Description+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28%22AnimalName%22%29+as+name+from+%5BCity-of-Port-Adelaide-Enfield-Dog_Registrations_2016%5D+where+AnimalBreed+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28%22Animal+Name%22%29+as+name+from+%5BMitcham-dog-registrations-2015%5D+where+Breed+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28%22DOG_NAME%22%29+as+name+from+%5Bburnside-dog-registrations-2015%5D+where+DOG_BREED+like+%3Abreed%0D%0A%0D%0Aunion+all+%0D%0A%0D%0Aselect+upper%28%22Animal_Name%22%29+as+name+from+%5Bcity-of-playford-2015-dog-registration%5D+where+Breed_Description+like+%3Abreed%0D%0A%0D%0Aunion+all%0D%0A%0D%0Aselect+upper%28%22Animal+Name%22%29+as+name+from+%5Bcity-of-prospect-dog-registration-details-2016%5D+where%22Breed+Description%22+like+%3Abreed%0D%0A%0D%0A%29+group+by+name+order+by+n+desc%3B&breed=pug\n\n- Pin to specific Jinja version. (`#100`_).\n- Default to 127.0.0.1 not 0.0.0.0. (`#98`_).\n- Added extra metadata options to publish and package commands. (`#92`_).\n\n  You can now run these commands like so::\n\n      datasette now publish mydb.db \\\n          --title=\"My Title\" \\\n          --source=\"Source\" \\\n          --source_url=\"http://www.example.com/\" \\\n          --license=\"CC0\" \\\n          --license_url=\"https://creativecommons.org/publicdomain/zero/1.0/\"\n\n  This will write those values into the metadata.json that is packaged with the\n  app. If you also pass ``--metadata=metadata.json`` that file will be updated with the extra\n  values before being written into the Docker image.\n- Added production-ready Dockerfile (`#94`_) [Andrew\n  Cutler]\n- New ``?_sql_time_limit_ms=10`` argument to database and table page (`#95`_)\n- SQL syntax highlighting with Codemirror (`#89`_) [Tom Dyson]\n\n.. _#89: https://github.com/simonw/datasette/issues/89\n.. _#92: https://github.com/simonw/datasette/issues/92\n.. _#94: https://github.com/simonw/datasette/issues/94\n.. _#95: https://github.com/simonw/datasette/issues/95\n.. _#96: https://github.com/simonw/datasette/issues/96\n.. _#98: https://github.com/simonw/datasette/issues/98\n.. _#99: https://github.com/simonw/datasette/issues/99\n.. _#100: https://github.com/simonw/datasette/issues/100\n.. _#108: https://github.com/simonw/datasette/issues/108\n\n0.11 (2017-11-14)\n-----------------\n- Added ``datasette publish now --force`` option.\n\n  This calls ``now`` with ``--force`` - useful as it means you get a fresh copy of datasette even if Now has already cached that docker layer.\n- Enable ``--cors`` by default when running in a container.\n\n0.10 (2017-11-14)\n-----------------\n- Fixed `#83`_ - 500 error on individual row pages.\n- Stop using sqlite WITH RECURSIVE in our tests.\n\n  The version of Python 3 running in Travis CI doesn't support this.\n\n.. _#83: https://github.com/simonw/datasette/issues/83\n\n0.9 (2017-11-13)\n----------------\n- Added ``--sql_time_limit_ms`` and ``--extra-options``.\n\n  The serve command now accepts ``--sql_time_limit_ms`` for customizing the SQL time\n  limit.\n\n  The publish and package commands now accept ``--extra-options`` which can be used\n  to specify additional options to be passed to the datasite serve command when\n  it executes inside the resulting Docker containers.\n\n0.8 (2017-11-13)\n----------------\n- V0.8 - added PyPI metadata, ready to ship.\n- Implemented offset/limit pagination for views (`#70`_).\n- Improved pagination. (`#78`_)\n- Limit on max rows returned, controlled by ``--max_returned_rows`` option. (`#69`_)\n\n  If someone executes 'select * from table' against a table with a million rows\n  in it, we could run into problems: just serializing that much data as JSON is\n  likely to lock up the server.\n\n  Solution: we now have a hard limit on the maximum number of rows that can be\n  returned by a query. If that limit is exceeded, the server will return a\n  ``\"truncated\": true`` field in the JSON.\n\n  This limit can be optionally controlled by the new ``--max_returned_rows``\n  option. Setting that option to 0 disables the limit entirely.\n\n.. _#70: https://github.com/simonw/datasette/issues/70\n.. _#78: https://github.com/simonw/datasette/issues/78\n.. _#69: https://github.com/simonw/datasette/issues/69\n", "import collections\nfrom datasette.app import Datasette\nfrom datasette.cli import cli\nfrom .fixtures import app_client, assert_permissions_checked, make_app_client\nfrom click.testing import CliRunner\nfrom bs4 import BeautifulSoup as Soup\nimport copy\nimport json\nfrom pprint import pprint\nimport pytest_asyncio\nimport pytest\nimport re\nimport time\nimport urllib\n\n\n@pytest.fixture(scope=\"module\")\ndef padlock_client():\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\n                    \"queries\": {\"two\": {\"sql\": \"select 1 + 1\"}},\n                }\n            }\n        }\n    ) as client:\n        yield client\n\n\n@pytest_asyncio.fixture\nasync def perms_ds():\n    ds = Datasette()\n    await ds.invoke_startup()\n    one = ds.add_memory_database(\"perms_ds_one\")\n    two = ds.add_memory_database(\"perms_ds_two\")\n    await one.execute_write(\"create table if not exists t1 (id integer primary key)\")\n    await one.execute_write(\"create table if not exists t2 (id integer primary key)\")\n    await two.execute_write(\"create table if not exists t1 (id integer primary key)\")\n    return ds\n\n\n@pytest.mark.parametrize(\n    \"allow,expected_anon,expected_auth\",\n    [\n        (None, 200, 200),\n        ({}, 403, 403),\n        ({\"id\": \"root\"}, 403, 200),\n    ],\n)\n@pytest.mark.parametrize(\n    \"path\",\n    (\n        \"/\",\n        \"/fixtures\",\n        \"/-/api\",\n        \"/fixtures/compound_three_primary_keys\",\n        \"/fixtures/compound_three_primary_keys/a,a,a\",\n        \"/fixtures/two\",  # Query\n    ),\n)\ndef test_view_padlock(allow, expected_anon, expected_auth, path, padlock_client):\n    padlock_client.ds._metadata_local[\"allow\"] = allow\n    fragment = \"\ud83d\udd12</h1>\"\n    anon_response = padlock_client.get(path)\n    assert expected_anon == anon_response.status\n    if allow and anon_response.status == 200:\n        # Should be no padlock\n        assert fragment not in anon_response.text\n    auth_response = padlock_client.get(\n        path,\n        cookies={\"ds_actor\": padlock_client.actor_cookie({\"id\": \"root\"})},\n    )\n    assert expected_auth == auth_response.status\n    # Check for the padlock\n    if allow and expected_anon == 403 and expected_auth == 200:\n        assert fragment in auth_response.text\n    del padlock_client.ds._metadata_local[\"allow\"]\n\n\n@pytest.mark.parametrize(\n    \"allow,expected_anon,expected_auth\",\n    [\n        (None, 200, 200),\n        ({}, 403, 403),\n        ({\"id\": \"root\"}, 403, 200),\n    ],\n)\ndef test_view_database(allow, expected_anon, expected_auth):\n    with make_app_client(\n        metadata={\"databases\": {\"fixtures\": {\"allow\": allow}}}\n    ) as client:\n        for path in (\n            \"/fixtures\",\n            \"/fixtures/compound_three_primary_keys\",\n            \"/fixtures/compound_three_primary_keys/a,a,a\",\n        ):\n            anon_response = client.get(path)\n            assert expected_anon == anon_response.status, path\n            if allow and path == \"/fixtures\" and anon_response.status == 200:\n                # Should be no padlock\n                assert \">fixtures \ud83d\udd12</h1>\" not in anon_response.text\n            auth_response = client.get(\n                path,\n                cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})},\n            )\n            assert expected_auth == auth_response.status\n            if (\n                allow\n                and path == \"/fixtures\"\n                and expected_anon == 403\n                and expected_auth == 200\n            ):\n                assert \">fixtures \ud83d\udd12</h1>\" in auth_response.text\n\n\ndef test_database_list_respects_view_database():\n    with make_app_client(\n        metadata={\"databases\": {\"fixtures\": {\"allow\": {\"id\": \"root\"}}}},\n        extra_databases={\"data.db\": \"create table names (name text)\"},\n    ) as client:\n        anon_response = client.get(\"/\")\n        assert '<a href=\"/data\">data</a></h2>' in anon_response.text\n        assert '<a href=\"/fixtures\">fixtures</a>' not in anon_response.text\n        auth_response = client.get(\n            \"/\",\n            cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})},\n        )\n        assert '<a href=\"/data\">data</a></h2>' in auth_response.text\n        assert '<a href=\"/fixtures\">fixtures</a> \ud83d\udd12</h2>' in auth_response.text\n\n\ndef test_database_list_respects_view_table():\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"data\": {\n                    \"tables\": {\n                        \"names\": {\"allow\": {\"id\": \"root\"}},\n                        \"v\": {\"allow\": {\"id\": \"root\"}},\n                    }\n                }\n            }\n        },\n        extra_databases={\n            \"data.db\": \"create table names (name text); create view v as select * from names\"\n        },\n    ) as client:\n        html_fragments = [\n            \">names</a> \ud83d\udd12\",\n            \">v</a> \ud83d\udd12\",\n        ]\n        anon_response_text = client.get(\"/\").text\n        assert \"0 rows in 0 tables\" in anon_response_text\n        for html_fragment in html_fragments:\n            assert html_fragment not in anon_response_text\n        auth_response_text = client.get(\n            \"/\",\n            cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})},\n        ).text\n        for html_fragment in html_fragments:\n            assert html_fragment in auth_response_text\n\n\n@pytest.mark.parametrize(\n    \"allow,expected_anon,expected_auth\",\n    [\n        (None, 200, 200),\n        ({}, 403, 403),\n        ({\"id\": \"root\"}, 403, 200),\n    ],\n)\ndef test_view_table(allow, expected_anon, expected_auth):\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\n                    \"tables\": {\"compound_three_primary_keys\": {\"allow\": allow}}\n                }\n            }\n        }\n    ) as client:\n        anon_response = client.get(\"/fixtures/compound_three_primary_keys\")\n        assert expected_anon == anon_response.status\n        if allow and anon_response.status == 200:\n            # Should be no padlock\n            assert \">compound_three_primary_keys \ud83d\udd12</h1>\" not in anon_response.text\n        auth_response = client.get(\n            \"/fixtures/compound_three_primary_keys\",\n            cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})},\n        )\n        assert expected_auth == auth_response.status\n        if allow and expected_anon == 403 and expected_auth == 200:\n            assert \">compound_three_primary_keys \ud83d\udd12</h1>\" in auth_response.text\n\n\ndef test_table_list_respects_view_table():\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\n                    \"tables\": {\n                        \"compound_three_primary_keys\": {\"allow\": {\"id\": \"root\"}},\n                        # And a SQL view too:\n                        \"paginated_view\": {\"allow\": {\"id\": \"root\"}},\n                    }\n                }\n            }\n        }\n    ) as client:\n        html_fragments = [\n            \">compound_three_primary_keys</a> \ud83d\udd12\",\n            \">paginated_view</a> \ud83d\udd12\",\n        ]\n        anon_response = client.get(\"/fixtures\")\n        for html_fragment in html_fragments:\n            assert html_fragment not in anon_response.text\n        auth_response = client.get(\n            \"/fixtures\", cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})}\n        )\n        for html_fragment in html_fragments:\n            assert html_fragment in auth_response.text\n\n\n@pytest.mark.parametrize(\n    \"allow,expected_anon,expected_auth\",\n    [\n        (None, 200, 200),\n        ({}, 403, 403),\n        ({\"id\": \"root\"}, 403, 200),\n    ],\n)\ndef test_view_query(allow, expected_anon, expected_auth):\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\"queries\": {\"q\": {\"sql\": \"select 1 + 1\", \"allow\": allow}}}\n            }\n        }\n    ) as client:\n        anon_response = client.get(\"/fixtures/q\")\n        assert expected_anon == anon_response.status\n        if allow and anon_response.status == 200:\n            # Should be no padlock\n            assert \"\ud83d\udd12</h1>\" not in anon_response.text\n        auth_response = client.get(\n            \"/fixtures/q\", cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})}\n        )\n        assert expected_auth == auth_response.status\n        if allow and expected_anon == 403 and expected_auth == 200:\n            assert \">fixtures: q \ud83d\udd12</h1>\" in auth_response.text\n\n\n@pytest.mark.parametrize(\n    \"metadata\",\n    [\n        {\"allow_sql\": {\"id\": \"root\"}},\n        {\"databases\": {\"fixtures\": {\"allow_sql\": {\"id\": \"root\"}}}},\n    ],\n)\ndef test_execute_sql(metadata):\n    schema_re = re.compile(\"const schema = ({.*?});\", re.DOTALL)\n    with make_app_client(metadata=metadata) as client:\n        form_fragment = '<form class=\"sql\" action=\"/fixtures\"'\n\n        # Anonymous users - should not display the form:\n        anon_html = client.get(\"/fixtures\").text\n        assert form_fragment not in anon_html\n        # And const schema should be an empty object:\n        assert \"const schema = {};\" in anon_html\n        # This should 403:\n        assert client.get(\"/fixtures?sql=select+1\").status == 403\n        # ?_where= not allowed on tables:\n        assert client.get(\"/fixtures/facet_cities?_where=id=3\").status == 403\n\n        # But for logged in user all of these should work:\n        cookies = {\"ds_actor\": client.actor_cookie({\"id\": \"root\"})}\n        response_text = client.get(\"/fixtures\", cookies=cookies).text\n        # Extract the schema= portion of the JavaScript\n        schema_json = schema_re.search(response_text).group(1)\n        schema = json.loads(schema_json)\n        assert set(schema[\"attraction_characteristic\"]) == {\"name\", \"pk\"}\n        assert schema[\"paginated_view\"] == []\n        assert form_fragment in response_text\n        query_response = client.get(\"/fixtures?sql=select+1\", cookies=cookies)\n        assert query_response.status == 200\n        schema2 = json.loads(schema_re.search(query_response.text).group(1))\n        assert set(schema2[\"attraction_characteristic\"]) == {\"name\", \"pk\"}\n        assert (\n            client.get(\"/fixtures/facet_cities?_where=id=3\", cookies=cookies).status\n            == 200\n        )\n\n\ndef test_query_list_respects_view_query():\n    with make_app_client(\n        metadata={\n            \"databases\": {\n                \"fixtures\": {\n                    \"queries\": {\"q\": {\"sql\": \"select 1 + 1\", \"allow\": {\"id\": \"root\"}}}\n                }\n            }\n        }\n    ) as client:\n        html_fragment = '<li><a href=\"/fixtures/q\" title=\"select 1 + 1\">q</a> \ud83d\udd12</li>'\n        anon_response = client.get(\"/fixtures\")\n        assert html_fragment not in anon_response.text\n        assert '\"/fixtures/q\"' not in anon_response.text\n        auth_response = client.get(\n            \"/fixtures\", cookies={\"ds_actor\": client.actor_cookie({\"id\": \"root\"})}\n        )\n        assert html_fragment in auth_response.text\n\n\n@pytest.mark.parametrize(\n    \"path,permissions\",\n    [\n        (\"/\", [\"view-instance\"]),\n        (\"/fixtures\", [\"view-instance\", (\"view-database\", \"fixtures\")]),\n        (\n            \"/fixtures/facetable/1\",\n            [\"view-instance\", (\"view-table\", (\"fixtures\", \"facetable\"))],\n        ),\n        (\n            \"/fixtures/simple_primary_key\",\n            [\n                \"view-instance\",\n                (\"view-database\", \"fixtures\"),\n                (\"view-table\", (\"fixtures\", \"simple_primary_key\")),\n            ],\n        ),\n        (\n            \"/fixtures?sql=select+1\",\n            [\n                \"view-instance\",\n                (\"view-database\", \"fixtures\"),\n                (\"execute-sql\", \"fixtures\"),\n            ],\n        ),\n        (\n            \"/fixtures.db\",\n            [\n                \"view-instance\",\n                (\"view-database\", \"fixtures\"),\n                (\"view-database-download\", \"fixtures\"),\n            ],\n        ),\n        (\n            \"/fixtures/neighborhood_search\",\n            [\n                \"view-instance\",\n                (\"view-database\", \"fixtures\"),\n                (\"view-query\", (\"fixtures\", \"neighborhood_search\")),\n            ],\n        ),\n    ],\n)\ndef test_permissions_checked(app_client, path, permissions):\n    # Needs file-backed app_client for /fixtures.db\n    app_client.ds._permission_checks.clear()\n    response = app_client.get(path)\n    assert response.status_code in (200, 403)\n    assert_permissions_checked(app_client.ds, permissions)\n\n\n@pytest.mark.asyncio\nasync def test_permissions_debug(ds_client):\n    ds_client.ds._permission_checks.clear()\n    assert (await ds_client.get(\"/-/permissions\")).status_code == 403\n    # With the cookie it should work\n    cookie = ds_client.actor_cookie({\"id\": \"root\"})\n    response = await ds_client.get(\"/-/permissions\", cookies={\"ds_actor\": cookie})\n    assert response.status_code == 200\n    # Should show one failure and one success\n    soup = Soup(response.text, \"html.parser\")\n    check_divs = soup.findAll(\"div\", {\"class\": \"check\"})\n    checks = [\n        {\n            \"action\": div.select_one(\".check-action\").text,\n            # True = green tick, False = red cross, None = gray None\n            \"result\": None\n            if div.select(\".check-result-no-opinion\")\n            else bool(div.select(\".check-result-true\")),\n            \"used_default\": bool(div.select(\".check-used-default\")),\n        }\n        for div in check_divs\n    ]\n    assert checks == [\n        {\"action\": \"permissions-debug\", \"result\": True, \"used_default\": False},\n        {\"action\": \"view-instance\", \"result\": None, \"used_default\": True},\n        {\"action\": \"debug-menu\", \"result\": False, \"used_default\": True},\n        {\"action\": \"view-instance\", \"result\": True, \"used_default\": True},\n        {\"action\": \"permissions-debug\", \"result\": False, \"used_default\": True},\n        {\"action\": \"view-instance\", \"result\": None, \"used_default\": True},\n    ]\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"actor,allow,expected_fragment\",\n    [\n        ('{\"id\":\"root\"}', \"{}\", \"Result: deny\"),\n        ('{\"id\":\"root\"}', '{\"id\": \"*\"}', \"Result: allow\"),\n        ('{\"', '{\"id\": \"*\"}', \"Actor JSON error\"),\n        ('{\"id\":\"root\"}', '\"*\"}', \"Allow JSON error\"),\n    ],\n)\nasync def test_allow_debug(ds_client, actor, allow, expected_fragment):\n    response = await ds_client.get(\n        \"/-/allow-debug?\" + urllib.parse.urlencode({\"actor\": actor, \"allow\": allow})\n    )\n    assert response.status_code == 200\n    assert expected_fragment in response.text\n\n\n@pytest.mark.parametrize(\n    \"allow,expected\",\n    [\n        ({\"id\": \"root\"}, 403),\n        ({\"id\": \"root\", \"unauthenticated\": True}, 200),\n    ],\n)\ndef test_allow_unauthenticated(allow, expected):\n    with make_app_client(metadata={\"allow\": allow}) as client:\n        assert expected == client.get(\"/\").status\n\n\n@pytest.fixture(scope=\"session\")\ndef view_instance_client():\n    with make_app_client(metadata={\"allow\": {}}) as client:\n        yield client\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\n        \"/\",\n        \"/fixtures\",\n        \"/fixtures/facetable\",\n        \"/-/metadata\",\n        \"/-/versions\",\n        \"/-/plugins\",\n        \"/-/settings\",\n        \"/-/threads\",\n        \"/-/databases\",\n        \"/-/permissions\",\n        \"/-/messages\",\n        \"/-/patterns\",\n    ],\n)\ndef test_view_instance(path, view_instance_client):\n    assert 403 == view_instance_client.get(path).status\n    if path not in (\"/-/permissions\", \"/-/messages\", \"/-/patterns\"):\n        assert 403 == view_instance_client.get(path + \".json\").status\n\n\n@pytest.fixture(scope=\"session\")\ndef cascade_app_client():\n    with make_app_client(is_immutable=True) as client:\n        yield client\n\n\n@pytest.mark.parametrize(\n    \"path,permissions,expected_status\",\n    [\n        (\"/\", [], 403),\n        (\"/\", [\"instance\"], 200),\n        # Can view table even if not allowed database or instance\n        (\"/fixtures/binary_data\", [], 403),\n        (\"/fixtures/binary_data\", [\"database\"], 403),\n        (\"/fixtures/binary_data\", [\"instance\"], 403),\n        (\"/fixtures/binary_data\", [\"table\"], 200),\n        (\"/fixtures/binary_data\", [\"table\", \"database\"], 200),\n        (\"/fixtures/binary_data\", [\"table\", \"database\", \"instance\"], 200),\n        # ... same for row\n        (\"/fixtures/binary_data/1\", [], 403),\n        (\"/fixtures/binary_data/1\", [\"database\"], 403),\n        (\"/fixtures/binary_data/1\", [\"instance\"], 403),\n        (\"/fixtures/binary_data/1\", [\"table\"], 200),\n        (\"/fixtures/binary_data/1\", [\"table\", \"database\"], 200),\n        (\"/fixtures/binary_data/1\", [\"table\", \"database\", \"instance\"], 200),\n        # Can view query even if not allowed database or instance\n        (\"/fixtures/magic_parameters\", [], 403),\n        (\"/fixtures/magic_parameters\", [\"database\"], 403),\n        (\"/fixtures/magic_parameters\", [\"instance\"], 403),\n        (\"/fixtures/magic_parameters\", [\"query\"], 200),\n        (\"/fixtures/magic_parameters\", [\"query\", \"database\"], 200),\n        (\"/fixtures/magic_parameters\", [\"query\", \"database\", \"instance\"], 200),\n        # Can view database even if not allowed instance\n        (\"/fixtures\", [], 403),\n        (\"/fixtures\", [\"instance\"], 403),\n        (\"/fixtures\", [\"database\"], 200),\n        # Downloading the fixtures.db file\n        (\"/fixtures.db\", [], 403),\n        (\"/fixtures.db\", [\"instance\"], 403),\n        (\"/fixtures.db\", [\"database\"], 200),\n        (\"/fixtures.db\", [\"download\"], 200),\n    ],\n)\ndef test_permissions_cascade(cascade_app_client, path, permissions, expected_status):\n    \"\"\"Test that e.g. having view-table but NOT view-database lets you view table page, etc\"\"\"\n    allow = {\"id\": \"*\"}\n    deny = {}\n    previous_metadata = cascade_app_client.ds.metadata()\n    updated_metadata = copy.deepcopy(previous_metadata)\n    actor = {\"id\": \"test\"}\n    if \"download\" in permissions:\n        actor[\"can_download\"] = 1\n    try:\n        # Set up the different allow blocks\n        updated_metadata[\"allow\"] = allow if \"instance\" in permissions else deny\n        updated_metadata[\"databases\"][\"fixtures\"][\"allow\"] = (\n            allow if \"database\" in permissions else deny\n        )\n        updated_metadata[\"databases\"][\"fixtures\"][\"tables\"][\"binary_data\"] = {\n            \"allow\": (allow if \"table\" in permissions else deny)\n        }\n        updated_metadata[\"databases\"][\"fixtures\"][\"queries\"][\"magic_parameters\"][\n            \"allow\"\n        ] = (allow if \"query\" in permissions else deny)\n        cascade_app_client.ds._metadata_local = updated_metadata\n        response = cascade_app_client.get(\n            path,\n            cookies={\"ds_actor\": cascade_app_client.actor_cookie(actor)},\n        )\n        assert (\n            response.status == expected_status\n        ), \"path: {}, permissions: {}, expected_status: {}, status: {}\".format(\n            path, permissions, expected_status, response.status\n        )\n    finally:\n        cascade_app_client.ds._metadata_local = previous_metadata\n\n\ndef test_padlocks_on_database_page(cascade_app_client):\n    metadata = {\n        \"databases\": {\n            \"fixtures\": {\n                \"allow\": {\"id\": \"test\"},\n                \"tables\": {\n                    \"123_starts_with_digits\": {\"allow\": True},\n                    \"simple_view\": {\"allow\": True},\n                },\n                \"queries\": {\"query_two\": {\"allow\": True, \"sql\": \"select 2\"}},\n            }\n        }\n    }\n    previous_metadata = cascade_app_client.ds._metadata_local\n    try:\n        cascade_app_client.ds._metadata_local = metadata\n        response = cascade_app_client.get(\n            \"/fixtures\",\n            cookies={\"ds_actor\": cascade_app_client.actor_cookie({\"id\": \"test\"})},\n        )\n        # Tables\n        assert \">123_starts_with_digits</a></h3>\" in response.text\n        assert \">Table With Space In Name</a> \ud83d\udd12</h3>\" in response.text\n        # Queries\n        assert \">from_async_hook</a> \ud83d\udd12</li>\" in response.text\n        assert \">query_two</a></li>\" in response.text\n        # Views\n        assert \">paginated_view</a> \ud83d\udd12</li>\" in response.text\n        assert \">simple_view</a></li>\" in response.text\n    finally:\n        cascade_app_client.ds._metadata_local = previous_metadata\n\n\nDEF = \"USE_DEFAULT\"\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"actor,permission,resource_1,resource_2,expected_result\",\n    (\n        # Without restrictions the defaults apply\n        ({\"id\": \"t\"}, \"view-instance\", None, None, DEF),\n        ({\"id\": \"t\"}, \"view-database\", \"one\", None, DEF),\n        ({\"id\": \"t\"}, \"view-table\", \"one\", \"t1\", DEF),\n        # If there is an _r block, everything gets denied unless explicitly allowed\n        ({\"id\": \"t\", \"_r\": {}}, \"view-instance\", None, None, False),\n        ({\"id\": \"t\", \"_r\": {}}, \"view-database\", \"one\", None, False),\n        ({\"id\": \"t\", \"_r\": {}}, \"view-table\", \"one\", \"t1\", False),\n        # Explicit allowing works at the \"a\" for all level:\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vi\"]}}, \"view-instance\", None, None, DEF),\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vd\"]}}, \"view-database\", \"one\", None, DEF),\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vt\"]}}, \"view-table\", \"one\", \"t1\", DEF),\n        # But not if it's the wrong permission\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vd\"]}}, \"view-instance\", None, None, False),\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vi\"]}}, \"view-database\", \"one\", None, False),\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"vd\"]}}, \"view-table\", \"one\", \"t1\", False),\n        # Works at the \"d\" for database level:\n        ({\"id\": \"t\", \"_r\": {\"d\": {\"one\": [\"vd\"]}}}, \"view-database\", \"one\", None, DEF),\n        (\n            {\"id\": \"t\", \"_r\": {\"d\": {\"one\": [\"vdd\"]}}},\n            \"view-database-download\",\n            \"one\",\n            None,\n            DEF,\n        ),\n        ({\"id\": \"t\", \"_r\": {\"d\": {\"one\": [\"es\"]}}}, \"execute-sql\", \"one\", None, DEF),\n        # Works at the \"r\" for table level:\n        (\n            {\"id\": \"t\", \"_r\": {\"r\": {\"one\": {\"t1\": [\"vt\"]}}}},\n            \"view-table\",\n            \"one\",\n            \"t1\",\n            DEF,\n        ),\n        (\n            {\"id\": \"t\", \"_r\": {\"r\": {\"one\": {\"t1\": [\"vt\"]}}}},\n            \"view-table\",\n            \"one\",\n            \"t2\",\n            False,\n        ),\n        # non-abbreviations should work too\n        ({\"id\": \"t\", \"_r\": {\"a\": [\"view-instance\"]}}, \"view-instance\", None, None, DEF),\n        (\n            {\"id\": \"t\", \"_r\": {\"d\": {\"one\": [\"view-database\"]}}},\n            \"view-database\",\n            \"one\",\n            None,\n            DEF,\n        ),\n        (\n            {\"id\": \"t\", \"_r\": {\"r\": {\"one\": {\"t1\": [\"view-table\"]}}}},\n            \"view-table\",\n            \"one\",\n            \"t1\",\n            DEF,\n        ),\n    ),\n)\nasync def test_actor_restricted_permissions(\n    perms_ds, actor, permission, resource_1, resource_2, expected_result\n):\n    cookies = {\"ds_actor\": perms_ds.sign({\"a\": {\"id\": \"root\"}}, \"actor\")}\n    csrftoken = (await perms_ds.client.get(\"/-/permissions\", cookies=cookies)).cookies[\n        \"ds_csrftoken\"\n    ]\n    cookies[\"ds_csrftoken\"] = csrftoken\n    response = await perms_ds.client.post(\n        \"/-/permissions\",\n        data={\n            \"actor\": json.dumps(actor),\n            \"permission\": permission,\n            \"resource_1\": resource_1,\n            \"resource_2\": resource_2,\n            \"csrftoken\": csrftoken,\n        },\n        cookies=cookies,\n    )\n    expected_resource = []\n    if resource_1:\n        expected_resource.append(resource_1)\n    if resource_2:\n        expected_resource.append(resource_2)\n    if len(expected_resource) == 1:\n        expected_resource = expected_resource[0]\n    expected = {\n        \"actor\": actor,\n        \"permission\": permission,\n        \"resource\": expected_resource,\n        \"result\": expected_result,\n    }\n    assert response.json() == expected\n\n\nPermMetadataTestCase = collections.namedtuple(\n    \"PermMetadataTestCase\",\n    \"metadata,actor,action,resource,expected_result\",\n)\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"metadata,actor,action,resource,expected_result\",\n    (\n        # Simple view-instance default=True example\n        PermMetadataTestCase(\n            metadata={},\n            actor=None,\n            action=\"view-instance\",\n            resource=None,\n            expected_result=True,\n        ),\n        # debug-menu on root\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"debug-menu\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user\"},\n            action=\"debug-menu\",\n            resource=None,\n            expected_result=True,\n        ),\n        # debug-menu on root, wrong actor\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"debug-menu\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user2\"},\n            action=\"debug-menu\",\n            resource=None,\n            expected_result=False,\n        ),\n        # create-table on root\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"create-table\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user\"},\n            action=\"create-table\",\n            resource=None,\n            expected_result=True,\n        ),\n        # create-table on database - no resource specified\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\"permissions\": {\"create-table\": {\"id\": \"user\"}}}\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"create-table\",\n            resource=None,\n            expected_result=False,\n        ),\n        # create-table on database\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\"permissions\": {\"create-table\": {\"id\": \"user\"}}}\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"create-table\",\n            resource=\"perms_ds_one\",\n            expected_result=True,\n        ),\n        # insert-row on root, wrong actor\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"insert-row\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user2\"},\n            action=\"insert-row\",\n            resource=(\"perms_ds_one\", \"t1\"),\n            expected_result=False,\n        ),\n        # insert-row on root, right actor\n        PermMetadataTestCase(\n            metadata={\"permissions\": {\"insert-row\": {\"id\": \"user\"}}},\n            actor={\"id\": \"user\"},\n            action=\"insert-row\",\n            resource=(\"perms_ds_one\", \"t1\"),\n            expected_result=True,\n        ),\n        # insert-row on database\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\"permissions\": {\"insert-row\": {\"id\": \"user\"}}}\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"insert-row\",\n            resource=\"perms_ds_one\",\n            expected_result=True,\n        ),\n        # insert-row on table, wrong table\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"tables\": {\n                            \"t1\": {\"permissions\": {\"insert-row\": {\"id\": \"user\"}}}\n                        }\n                    }\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"insert-row\",\n            resource=(\"perms_ds_one\", \"t2\"),\n            expected_result=False,\n        ),\n        # insert-row on table, right table\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"tables\": {\n                            \"t1\": {\"permissions\": {\"insert-row\": {\"id\": \"user\"}}}\n                        }\n                    }\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"insert-row\",\n            resource=(\"perms_ds_one\", \"t1\"),\n            expected_result=True,\n        ),\n        # view-query on canned query, wrong actor\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"queries\": {\n                            \"q1\": {\n                                \"sql\": \"select 1 + 1\",\n                                \"permissions\": {\"view-query\": {\"id\": \"user\"}},\n                            }\n                        }\n                    }\n                }\n            },\n            actor={\"id\": \"user2\"},\n            action=\"view-query\",\n            resource=(\"perms_ds_one\", \"q1\"),\n            expected_result=False,\n        ),\n        # view-query on canned query, right actor\n        PermMetadataTestCase(\n            metadata={\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"queries\": {\n                            \"q1\": {\n                                \"sql\": \"select 1 + 1\",\n                                \"permissions\": {\"view-query\": {\"id\": \"user\"}},\n                            }\n                        }\n                    }\n                }\n            },\n            actor={\"id\": \"user\"},\n            action=\"view-query\",\n            resource=(\"perms_ds_one\", \"q1\"),\n            expected_result=True,\n        ),\n    ),\n)\nasync def test_permissions_in_metadata(\n    perms_ds, metadata, actor, action, resource, expected_result\n):\n    previous_metadata = perms_ds.metadata()\n    updated_metadata = copy.deepcopy(previous_metadata)\n    updated_metadata.update(metadata)\n    perms_ds._metadata_local = updated_metadata\n    try:\n        result = await perms_ds.permission_allowed(actor, action, resource)\n        if result != expected_result:\n            pprint(perms_ds._permission_checks)\n            assert result == expected_result\n    finally:\n        perms_ds._metadata_local = previous_metadata\n\n\n@pytest.mark.asyncio\nasync def test_actor_endpoint_allows_any_token():\n    ds = Datasette()\n    token = ds.sign(\n        {\n            \"a\": \"root\",\n            \"token\": \"dstok\",\n            \"t\": int(time.time()),\n            \"_r\": {\"a\": [\"debug-menu\"]},\n        },\n        namespace=\"token\",\n    )\n    response = await ds.client.get(\n        \"/-/actor.json\", headers={\"Authorization\": f\"Bearer dstok_{token}\"}\n    )\n    assert response.status_code == 200\n    assert response.json()[\"actor\"] == {\n        \"id\": \"root\",\n        \"token\": \"dstok\",\n        \"_r\": {\"a\": [\"debug-menu\"]},\n    }\n\n\n@pytest.mark.parametrize(\n    \"options,expected\",\n    (\n        ([], {\"id\": \"root\", \"token\": \"dstok\"}),\n        (\n            [\"--all\", \"debug-menu\"],\n            {\"_r\": {\"a\": [\"dm\"]}, \"id\": \"root\", \"token\": \"dstok\"},\n        ),\n        (\n            [\"-a\", \"debug-menu\", \"--all\", \"create-table\"],\n            {\"_r\": {\"a\": [\"dm\", \"ct\"]}, \"id\": \"root\", \"token\": \"dstok\"},\n        ),\n        (\n            [\"-r\", \"db1\", \"t1\", \"insert-row\"],\n            {\"_r\": {\"r\": {\"db1\": {\"t1\": [\"ir\"]}}}, \"id\": \"root\", \"token\": \"dstok\"},\n        ),\n        (\n            [\"-d\", \"db1\", \"create-table\"],\n            {\"_r\": {\"d\": {\"db1\": [\"ct\"]}}, \"id\": \"root\", \"token\": \"dstok\"},\n        ),\n        # And one with all of them multiple times using all the names\n        (\n            [\n                \"-a\",\n                \"debug-menu\",\n                \"--all\",\n                \"create-table\",\n                \"-r\",\n                \"db1\",\n                \"t1\",\n                \"insert-row\",\n                \"--resource\",\n                \"db1\",\n                \"t2\",\n                \"update-row\",\n                \"-d\",\n                \"db1\",\n                \"create-table\",\n                \"--database\",\n                \"db2\",\n                \"drop-table\",\n            ],\n            {\n                \"_r\": {\n                    \"a\": [\"dm\", \"ct\"],\n                    \"d\": {\"db1\": [\"ct\"], \"db2\": [\"dt\"]},\n                    \"r\": {\"db1\": {\"t1\": [\"ir\"], \"t2\": [\"ur\"]}},\n                },\n                \"id\": \"root\",\n                \"token\": \"dstok\",\n            },\n        ),\n    ),\n)\ndef test_cli_create_token(options, expected):\n    runner = CliRunner()\n    result1 = runner.invoke(\n        cli,\n        [\n            \"create-token\",\n            \"--secret\",\n            \"sekrit\",\n            \"root\",\n        ]\n        + options,\n    )\n    token = result1.output.strip()\n    result2 = runner.invoke(\n        cli,\n        [\n            \"serve\",\n            \"--secret\",\n            \"sekrit\",\n            \"--get\",\n            \"/-/actor.json\",\n            \"--token\",\n            token,\n        ],\n    )\n    assert 0 == result2.exit_code, result2.output\n    assert json.loads(result2.output) == {\"actor\": expected}\n\n\n_visible_tables_re = re.compile(r\">\\/((\\w+)\\/(\\w+))\\.json<\\/a> - Get rows for\")\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"is_logged_in,metadata,expected_visible_tables\",\n    (\n        # Unprotected instance logged out user sees everything:\n        (\n            False,\n            None,\n            [\"perms_ds_one/t1\", \"perms_ds_one/t2\", \"perms_ds_two/t1\"],\n        ),\n        # Fully protected instance logged out user sees nothing\n        (False, {\"allow\": {\"id\": \"user\"}}, None),\n        # User with visibility of just perms_ds_one sees both tables there\n        (\n            True,\n            {\n                \"databases\": {\n                    \"perms_ds_one\": {\"allow\": {\"id\": \"user\"}},\n                    \"perms_ds_two\": {\"allow\": False},\n                }\n            },\n            [\"perms_ds_one/t1\", \"perms_ds_one/t2\"],\n        ),\n        # User with visibility of only table perms_ds_one/t1 sees just that one\n        (\n            True,\n            {\n                \"databases\": {\n                    \"perms_ds_one\": {\n                        \"allow\": {\"id\": \"user\"},\n                        \"tables\": {\"t2\": {\"allow\": False}},\n                    },\n                    \"perms_ds_two\": {\"allow\": False},\n                }\n            },\n            [\"perms_ds_one/t1\"],\n        ),\n    ),\n)\nasync def test_api_explorer_visibility(\n    perms_ds, is_logged_in, metadata, expected_visible_tables\n):\n    try:\n        prev_metadata = perms_ds._metadata_local\n        perms_ds._metadata_local = metadata or {}\n        cookies = {}\n        if is_logged_in:\n            cookies = {\"ds_actor\": perms_ds.client.actor_cookie({\"id\": \"user\"})}\n        response = await perms_ds.client.get(\"/-/api\", cookies=cookies)\n        if expected_visible_tables:\n            assert response.status_code == 200\n            # Search HTML for stuff matching:\n            # '>/perms_ds_one/t2.json</a> - Get rows for'\n            visible_tables = [\n                match[0] for match in _visible_tables_re.findall(response.text)\n            ]\n            assert visible_tables == expected_visible_tables\n        else:\n            assert response.status_code == 403\n    finally:\n        perms_ds._metadata_local = prev_metadata\n"], "filenames": ["datasette/templates/api_explorer.html", "datasette/version.py", "datasette/views/special.py", "docs/changelog.rst", "tests/test_permissions.py"], "buggy_code_start_loc": [11, 1, 357, 5, 55], "buggy_code_end_loc": [12, 2, 482, 5, 953], "fixing_code_start_loc": [11, 1, 357, 6, 56], "fixing_code_end_loc": [12, 2, 492, 22, 1021], "type": "NVD-CWE-noinfo", "message": "Datasette is an open source multi-tool for exploring and publishing data. This bug affects Datasette instances running a Datasette 1.0 alpha - 1.0a0, 1.0a1, 1.0a2 or 1.0a3 - in an online accessible location but with authentication enabled using a plugin such as datasette-auth-passwords. The `/-/api` API explorer endpoint could reveal the names of both databases and tables - but not their contents - to an unauthenticated user. Datasette 1.0a4 has a fix for this issue. This will block access to the API explorer but will still allow access to the Datasette read or write JSON APIs, as those use different URL patterns within the Datasette `/database` hierarchy. This issue is patched in version 1.0a4.", "other": {"cve": {"id": "CVE-2023-40570", "sourceIdentifier": "security-advisories@github.com", "published": "2023-08-25T01:15:09.077", "lastModified": "2023-08-31T13:50:50.933", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Datasette is an open source multi-tool for exploring and publishing data. This bug affects Datasette instances running a Datasette 1.0 alpha - 1.0a0, 1.0a1, 1.0a2 or 1.0a3 - in an online accessible location but with authentication enabled using a plugin such as datasette-auth-passwords. The `/-/api` API explorer endpoint could reveal the names of both databases and tables - but not their contents - to an unauthenticated user. Datasette 1.0a4 has a fix for this issue. This will block access to the API explorer but will still allow access to the Datasette read or write JSON APIs, as those use different URL patterns within the Datasette `/database` hierarchy. This issue is patched in version 1.0a4."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 1.4}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-213"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:datasette:datasette:1.0:alpha0:*:*:*:*:*:*", "matchCriteriaId": "7E0C61BC-7225-4FBD-ACF0-D98041330D2C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:datasette:datasette:1.0:alpha1:*:*:*:*:*:*", "matchCriteriaId": "E22D5C42-510C-4D03-8ABE-426C31DAA870"}, {"vulnerable": true, "criteria": "cpe:2.3:a:datasette:datasette:1.0:alpha2:*:*:*:*:*:*", "matchCriteriaId": "63CA31F1-FF36-4987-B87F-C858F934A963"}, {"vulnerable": true, "criteria": "cpe:2.3:a:datasette:datasette:1.0:alpha3:*:*:*:*:*:*", "matchCriteriaId": "701B7F05-BBE5-4F92-B4D1-C710183E851C"}]}]}], "references": [{"url": "https://github.com/simonw/datasette/commit/01e0558825b8f7ec17d3b691aa072daf122fcc74", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/simonw/datasette/security/advisories/GHSA-7ch3-7pp7-7cpq", "source": "security-advisories@github.com", "tags": ["Mitigation", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/simonw/datasette/commit/01e0558825b8f7ec17d3b691aa072daf122fcc74"}}