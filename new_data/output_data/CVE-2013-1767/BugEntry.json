{"buggy_code": ["/*\n * Resizable virtual memory filesystem for Linux.\n *\n * Copyright (C) 2000 Linus Torvalds.\n *\t\t 2000 Transmeta Corp.\n *\t\t 2000-2001 Christoph Rohland\n *\t\t 2000-2001 SAP AG\n *\t\t 2002 Red Hat Inc.\n * Copyright (C) 2002-2011 Hugh Dickins.\n * Copyright (C) 2011 Google Inc.\n * Copyright (C) 2002-2005 VERITAS Software Corporation.\n * Copyright (C) 2004 Andi Kleen, SuSE Labs\n *\n * Extended attribute support for tmpfs:\n * Copyright (c) 2004, Luke Kenneth Casson Leighton <lkcl@lkcl.net>\n * Copyright (c) 2004 Red Hat, Inc., James Morris <jmorris@redhat.com>\n *\n * tiny-shmem:\n * Copyright (c) 2004, 2008 Matt Mackall <mpm@selenic.com>\n *\n * This file is released under the GPL.\n */\n\n#include <linux/fs.h>\n#include <linux/init.h>\n#include <linux/vfs.h>\n#include <linux/mount.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n\nstatic struct vfsmount *shm_mnt;\n\n#ifdef CONFIG_SHMEM\n/*\n * This virtual memory filesystem is heavily based on the ramfs. It\n * extends ramfs by the ability to use swap and honor resource limits\n * which makes it a completely usable filesystem.\n */\n\n#include <linux/xattr.h>\n#include <linux/exportfs.h>\n#include <linux/posix_acl.h>\n#include <linux/generic_acl.h>\n#include <linux/mman.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/backing-dev.h>\n#include <linux/shmem_fs.h>\n#include <linux/writeback.h>\n#include <linux/blkdev.h>\n#include <linux/pagevec.h>\n#include <linux/percpu_counter.h>\n#include <linux/falloc.h>\n#include <linux/splice.h>\n#include <linux/security.h>\n#include <linux/swapops.h>\n#include <linux/mempolicy.h>\n#include <linux/namei.h>\n#include <linux/ctype.h>\n#include <linux/migrate.h>\n#include <linux/highmem.h>\n#include <linux/seq_file.h>\n#include <linux/magic.h>\n\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n\n#define BLOCKS_PER_PAGE  (PAGE_CACHE_SIZE/512)\n#define VM_ACCT(size)    (PAGE_CACHE_ALIGN(size) >> PAGE_SHIFT)\n\n/* Pretend that each entry is of this size in directory's i_size */\n#define BOGO_DIRENT_SIZE 20\n\n/* Symlink up to this size is kmalloc'ed instead of using a swappable page */\n#define SHORT_SYMLINK_LEN 128\n\n/*\n * shmem_fallocate and shmem_writepage communicate via inode->i_private\n * (with i_mutex making sure that it has only one user at a time):\n * we would prefer not to enlarge the shmem inode just for that.\n */\nstruct shmem_falloc {\n\tpgoff_t start;\t\t/* start of range currently being fallocated */\n\tpgoff_t next;\t\t/* the next page offset to be fallocated */\n\tpgoff_t nr_falloced;\t/* how many new pages have been fallocated */\n\tpgoff_t nr_unswapped;\t/* how often writepage refused to swap out */\n};\n\n/* Flag allocation requirements to shmem_getpage */\nenum sgp_type {\n\tSGP_READ,\t/* don't exceed i_size, don't allocate page */\n\tSGP_CACHE,\t/* don't exceed i_size, may allocate page */\n\tSGP_DIRTY,\t/* like SGP_CACHE, but set new page dirty */\n\tSGP_WRITE,\t/* may exceed i_size, may allocate !Uptodate page */\n\tSGP_FALLOC,\t/* like SGP_WRITE, but make existing page Uptodate */\n};\n\n#ifdef CONFIG_TMPFS\nstatic unsigned long shmem_default_max_blocks(void)\n{\n\treturn totalram_pages / 2;\n}\n\nstatic unsigned long shmem_default_max_inodes(void)\n{\n\treturn min(totalram_pages - totalhigh_pages, totalram_pages / 2);\n}\n#endif\n\nstatic bool shmem_should_replace_page(struct page *page, gfp_t gfp);\nstatic int shmem_replace_page(struct page **pagep, gfp_t gfp,\n\t\t\t\tstruct shmem_inode_info *info, pgoff_t index);\nstatic int shmem_getpage_gfp(struct inode *inode, pgoff_t index,\n\tstruct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type);\n\nstatic inline int shmem_getpage(struct inode *inode, pgoff_t index,\n\tstruct page **pagep, enum sgp_type sgp, int *fault_type)\n{\n\treturn shmem_getpage_gfp(inode, index, pagep, sgp,\n\t\t\tmapping_gfp_mask(inode->i_mapping), fault_type);\n}\n\nstatic inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\n/*\n * shmem_file_setup pre-accounts the whole fixed size of a VM object,\n * for shared memory and for shared anonymous (/dev/zero) mappings\n * (unless MAP_NORESERVE and sysctl_overcommit_memory <= 1),\n * consistent with the pre-accounting of private mappings ...\n */\nstatic inline int shmem_acct_size(unsigned long flags, loff_t size)\n{\n\treturn (flags & VM_NORESERVE) ?\n\t\t0 : security_vm_enough_memory_mm(current->mm, VM_ACCT(size));\n}\n\nstatic inline void shmem_unacct_size(unsigned long flags, loff_t size)\n{\n\tif (!(flags & VM_NORESERVE))\n\t\tvm_unacct_memory(VM_ACCT(size));\n}\n\n/*\n * ... whereas tmpfs objects are accounted incrementally as\n * pages are allocated, in order to allow huge sparse files.\n * shmem_getpage reports shmem_acct_block failure as -ENOSPC not -ENOMEM,\n * so that a failure on a sparse tmpfs mapping will give SIGBUS not OOM.\n */\nstatic inline int shmem_acct_block(unsigned long flags)\n{\n\treturn (flags & VM_NORESERVE) ?\n\t\tsecurity_vm_enough_memory_mm(current->mm, VM_ACCT(PAGE_CACHE_SIZE)) : 0;\n}\n\nstatic inline void shmem_unacct_blocks(unsigned long flags, long pages)\n{\n\tif (flags & VM_NORESERVE)\n\t\tvm_unacct_memory(pages * VM_ACCT(PAGE_CACHE_SIZE));\n}\n\nstatic const struct super_operations shmem_ops;\nstatic const struct address_space_operations shmem_aops;\nstatic const struct file_operations shmem_file_operations;\nstatic const struct inode_operations shmem_inode_operations;\nstatic const struct inode_operations shmem_dir_inode_operations;\nstatic const struct inode_operations shmem_special_inode_operations;\nstatic const struct vm_operations_struct shmem_vm_ops;\n\nstatic struct backing_dev_info shmem_backing_dev_info  __read_mostly = {\n\t.ra_pages\t= 0,\t/* No readahead */\n\t.capabilities\t= BDI_CAP_NO_ACCT_AND_WRITEBACK | BDI_CAP_SWAP_BACKED,\n};\n\nstatic LIST_HEAD(shmem_swaplist);\nstatic DEFINE_MUTEX(shmem_swaplist_mutex);\n\nstatic int shmem_reserve_inode(struct super_block *sb)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tif (sbinfo->max_inodes) {\n\t\tspin_lock(&sbinfo->stat_lock);\n\t\tif (!sbinfo->free_inodes) {\n\t\t\tspin_unlock(&sbinfo->stat_lock);\n\t\t\treturn -ENOSPC;\n\t\t}\n\t\tsbinfo->free_inodes--;\n\t\tspin_unlock(&sbinfo->stat_lock);\n\t}\n\treturn 0;\n}\n\nstatic void shmem_free_inode(struct super_block *sb)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tif (sbinfo->max_inodes) {\n\t\tspin_lock(&sbinfo->stat_lock);\n\t\tsbinfo->free_inodes++;\n\t\tspin_unlock(&sbinfo->stat_lock);\n\t}\n}\n\n/**\n * shmem_recalc_inode - recalculate the block usage of an inode\n * @inode: inode to recalc\n *\n * We have to calculate the free blocks since the mm can drop\n * undirtied hole pages behind our back.\n *\n * But normally   info->alloced == inode->i_mapping->nrpages + info->swapped\n * So mm freed is info->alloced - (inode->i_mapping->nrpages + info->swapped)\n *\n * It has to be called with the spinlock held.\n */\nstatic void shmem_recalc_inode(struct inode *inode)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tlong freed;\n\n\tfreed = info->alloced - info->swapped - inode->i_mapping->nrpages;\n\tif (freed > 0) {\n\t\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\t\tif (sbinfo->max_blocks)\n\t\t\tpercpu_counter_add(&sbinfo->used_blocks, -freed);\n\t\tinfo->alloced -= freed;\n\t\tinode->i_blocks -= freed * BLOCKS_PER_PAGE;\n\t\tshmem_unacct_blocks(info->flags, freed);\n\t}\n}\n\n/*\n * Replace item expected in radix tree by a new item, while holding tree lock.\n */\nstatic int shmem_radix_tree_replace(struct address_space *mapping,\n\t\t\tpgoff_t index, void *expected, void *replacement)\n{\n\tvoid **pslot;\n\tvoid *item = NULL;\n\n\tVM_BUG_ON(!expected);\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree, index);\n\tif (pslot)\n\t\titem = radix_tree_deref_slot_protected(pslot,\n\t\t\t\t\t\t\t&mapping->tree_lock);\n\tif (item != expected)\n\t\treturn -ENOENT;\n\tif (replacement)\n\t\tradix_tree_replace_slot(pslot, replacement);\n\telse\n\t\tradix_tree_delete(&mapping->page_tree, index);\n\treturn 0;\n}\n\n/*\n * Sometimes, before we decide whether to proceed or to fail, we must check\n * that an entry was not already brought back from swap by a racing thread.\n *\n * Checking page is not enough: by the time a SwapCache page is locked, it\n * might be reused, and again be SwapCache, using the same swap as before.\n */\nstatic bool shmem_confirm_swap(struct address_space *mapping,\n\t\t\t       pgoff_t index, swp_entry_t swap)\n{\n\tvoid *item;\n\n\trcu_read_lock();\n\titem = radix_tree_lookup(&mapping->page_tree, index);\n\trcu_read_unlock();\n\treturn item == swp_to_radix_entry(swap);\n}\n\n/*\n * Like add_to_page_cache_locked, but error if expected item has gone.\n */\nstatic int shmem_add_to_page_cache(struct page *page,\n\t\t\t\t   struct address_space *mapping,\n\t\t\t\t   pgoff_t index, gfp_t gfp, void *expected)\n{\n\tint error;\n\n\tVM_BUG_ON(!PageLocked(page));\n\tVM_BUG_ON(!PageSwapBacked(page));\n\n\tpage_cache_get(page);\n\tpage->mapping = mapping;\n\tpage->index = index;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\tif (!expected)\n\t\terror = radix_tree_insert(&mapping->page_tree, index, page);\n\telse\n\t\terror = shmem_radix_tree_replace(mapping, index, expected,\n\t\t\t\t\t\t\t\t page);\n\tif (!error) {\n\t\tmapping->nrpages++;\n\t\t__inc_zone_page_state(page, NR_FILE_PAGES);\n\t\t__inc_zone_page_state(page, NR_SHMEM);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t} else {\n\t\tpage->mapping = NULL;\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\tpage_cache_release(page);\n\t}\n\treturn error;\n}\n\n/*\n * Like delete_from_page_cache, but substitutes swap for page.\n */\nstatic void shmem_delete_from_page_cache(struct page *page, void *radswap)\n{\n\tstruct address_space *mapping = page->mapping;\n\tint error;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\terror = shmem_radix_tree_replace(mapping, page->index, page, radswap);\n\tpage->mapping = NULL;\n\tmapping->nrpages--;\n\t__dec_zone_page_state(page, NR_FILE_PAGES);\n\t__dec_zone_page_state(page, NR_SHMEM);\n\tspin_unlock_irq(&mapping->tree_lock);\n\tpage_cache_release(page);\n\tBUG_ON(error);\n}\n\n/*\n * Like find_get_pages, but collecting swap entries as well as pages.\n */\nstatic unsigned shmem_find_get_pages_and_swap(struct address_space *mapping,\n\t\t\t\t\tpgoff_t start, unsigned int nr_pages,\n\t\t\t\t\tstruct page **pages, pgoff_t *indices)\n{\n\tvoid **slot;\n\tunsigned int ret = 0;\n\tstruct radix_tree_iter iter;\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\trcu_read_lock();\nrestart:\n\tradix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {\n\t\tstruct page *page;\nrepeat:\n\t\tpage = radix_tree_deref_slot(slot);\n\t\tif (unlikely(!page))\n\t\t\tcontinue;\n\t\tif (radix_tree_exception(page)) {\n\t\t\tif (radix_tree_deref_retry(page))\n\t\t\t\tgoto restart;\n\t\t\t/*\n\t\t\t * Otherwise, we must be storing a swap entry\n\t\t\t * here as an exceptional entry: so return it\n\t\t\t * without attempting to raise page count.\n\t\t\t */\n\t\t\tgoto export;\n\t\t}\n\t\tif (!page_cache_get_speculative(page))\n\t\t\tgoto repeat;\n\n\t\t/* Has the page moved? */\n\t\tif (unlikely(page != *slot)) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto repeat;\n\t\t}\nexport:\n\t\tindices[ret] = iter.index;\n\t\tpages[ret] = page;\n\t\tif (++ret == nr_pages)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/*\n * Remove swap entry from radix tree, free the swap and its page cache.\n */\nstatic int shmem_free_swap(struct address_space *mapping,\n\t\t\t   pgoff_t index, void *radswap)\n{\n\tint error;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\terror = shmem_radix_tree_replace(mapping, index, radswap, NULL);\n\tspin_unlock_irq(&mapping->tree_lock);\n\tif (!error)\n\t\tfree_swap_and_cache(radix_to_swp_entry(radswap));\n\treturn error;\n}\n\n/*\n * Pagevec may contain swap entries, so shuffle up pages before releasing.\n */\nstatic void shmem_deswap_pagevec(struct pagevec *pvec)\n{\n\tint i, j;\n\n\tfor (i = 0, j = 0; i < pagevec_count(pvec); i++) {\n\t\tstruct page *page = pvec->pages[i];\n\t\tif (!radix_tree_exceptional_entry(page))\n\t\t\tpvec->pages[j++] = page;\n\t}\n\tpvec->nr = j;\n}\n\n/*\n * SysV IPC SHM_UNLOCK restore Unevictable pages to their evictable lists.\n */\nvoid shmem_unlock_mapping(struct address_space *mapping)\n{\n\tstruct pagevec pvec;\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tpgoff_t index = 0;\n\n\tpagevec_init(&pvec, 0);\n\t/*\n\t * Minor point, but we might as well stop if someone else SHM_LOCKs it.\n\t */\n\twhile (!mapping_unevictable(mapping)) {\n\t\t/*\n\t\t * Avoid pagevec_lookup(): find_get_pages() returns 0 as if it\n\t\t * has finished, if it hits a row of PAGEVEC_SIZE swap entries.\n\t\t */\n\t\tpvec.nr = shmem_find_get_pages_and_swap(mapping, index,\n\t\t\t\t\tPAGEVEC_SIZE, pvec.pages, indices);\n\t\tif (!pvec.nr)\n\t\t\tbreak;\n\t\tindex = indices[pvec.nr - 1] + 1;\n\t\tshmem_deswap_pagevec(&pvec);\n\t\tcheck_move_unevictable_pages(pvec.pages, pvec.nr);\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n}\n\n/*\n * Remove range of pages and swap entries from radix tree, and free them.\n * If !unfalloc, truncate or punch hole; if unfalloc, undo failed fallocate.\n */\nstatic void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,\n\t\t\t\t\t\t\t\t bool unfalloc)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tpgoff_t start = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\tpgoff_t end = (lend + 1) >> PAGE_CACHE_SHIFT;\n\tunsigned int partial_start = lstart & (PAGE_CACHE_SIZE - 1);\n\tunsigned int partial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);\n\tstruct pagevec pvec;\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tlong nr_swaps_freed = 0;\n\tpgoff_t index;\n\tint i;\n\n\tif (lend == -1)\n\t\tend = -1;\t/* unsigned, so actually very big */\n\n\tpagevec_init(&pvec, 0);\n\tindex = start;\n\twhile (index < end) {\n\t\tpvec.nr = shmem_find_get_pages_and_swap(mapping, index,\n\t\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE),\n\t\t\t\t\t\t\tpvec.pages, indices);\n\t\tif (!pvec.nr)\n\t\t\tbreak;\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tindex = indices[i];\n\t\t\tif (index >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (radix_tree_exceptional_entry(page)) {\n\t\t\t\tif (unfalloc)\n\t\t\t\t\tcontinue;\n\t\t\t\tnr_swaps_freed += !shmem_free_swap(mapping,\n\t\t\t\t\t\t\t\tindex, page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!trylock_page(page))\n\t\t\t\tcontinue;\n\t\t\tif (!unfalloc || !PageUptodate(page)) {\n\t\t\t\tif (page->mapping == mapping) {\n\t\t\t\t\tVM_BUG_ON(PageWriteback(page));\n\t\t\t\t\ttruncate_inode_page(mapping, page);\n\t\t\t\t}\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t}\n\t\tshmem_deswap_pagevec(&pvec);\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tcond_resched();\n\t\tindex++;\n\t}\n\n\tif (partial_start) {\n\t\tstruct page *page = NULL;\n\t\tshmem_getpage(inode, start - 1, &page, SGP_READ, NULL);\n\t\tif (page) {\n\t\t\tunsigned int top = PAGE_CACHE_SIZE;\n\t\t\tif (start > end) {\n\t\t\t\ttop = partial_end;\n\t\t\t\tpartial_end = 0;\n\t\t\t}\n\t\t\tzero_user_segment(page, partial_start, top);\n\t\t\tset_page_dirty(page);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\tif (partial_end) {\n\t\tstruct page *page = NULL;\n\t\tshmem_getpage(inode, end, &page, SGP_READ, NULL);\n\t\tif (page) {\n\t\t\tzero_user_segment(page, 0, partial_end);\n\t\t\tset_page_dirty(page);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\tif (start >= end)\n\t\treturn;\n\n\tindex = start;\n\tfor ( ; ; ) {\n\t\tcond_resched();\n\t\tpvec.nr = shmem_find_get_pages_and_swap(mapping, index,\n\t\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE),\n\t\t\t\t\t\t\tpvec.pages, indices);\n\t\tif (!pvec.nr) {\n\t\t\tif (index == start || unfalloc)\n\t\t\t\tbreak;\n\t\t\tindex = start;\n\t\t\tcontinue;\n\t\t}\n\t\tif ((index == start || unfalloc) && indices[0] >= end) {\n\t\t\tshmem_deswap_pagevec(&pvec);\n\t\t\tpagevec_release(&pvec);\n\t\t\tbreak;\n\t\t}\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tindex = indices[i];\n\t\t\tif (index >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (radix_tree_exceptional_entry(page)) {\n\t\t\t\tif (unfalloc)\n\t\t\t\t\tcontinue;\n\t\t\t\tnr_swaps_freed += !shmem_free_swap(mapping,\n\t\t\t\t\t\t\t\tindex, page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tlock_page(page);\n\t\t\tif (!unfalloc || !PageUptodate(page)) {\n\t\t\t\tif (page->mapping == mapping) {\n\t\t\t\t\tVM_BUG_ON(PageWriteback(page));\n\t\t\t\t\ttruncate_inode_page(mapping, page);\n\t\t\t\t}\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t}\n\t\tshmem_deswap_pagevec(&pvec);\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tindex++;\n\t}\n\n\tspin_lock(&info->lock);\n\tinfo->swapped -= nr_swaps_freed;\n\tshmem_recalc_inode(inode);\n\tspin_unlock(&info->lock);\n}\n\nvoid shmem_truncate_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\tshmem_undo_range(inode, lstart, lend, false);\n\tinode->i_ctime = inode->i_mtime = CURRENT_TIME;\n}\nEXPORT_SYMBOL_GPL(shmem_truncate_range);\n\nstatic int shmem_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error;\n\n\terror = inode_change_ok(inode, attr);\n\tif (error)\n\t\treturn error;\n\n\tif (S_ISREG(inode->i_mode) && (attr->ia_valid & ATTR_SIZE)) {\n\t\tloff_t oldsize = inode->i_size;\n\t\tloff_t newsize = attr->ia_size;\n\n\t\tif (newsize != oldsize) {\n\t\t\ti_size_write(inode, newsize);\n\t\t\tinode->i_ctime = inode->i_mtime = CURRENT_TIME;\n\t\t}\n\t\tif (newsize < oldsize) {\n\t\t\tloff_t holebegin = round_up(newsize, PAGE_SIZE);\n\t\t\tunmap_mapping_range(inode->i_mapping, holebegin, 0, 1);\n\t\t\tshmem_truncate_range(inode, newsize, (loff_t)-1);\n\t\t\t/* unmap again to remove racily COWed private pages */\n\t\t\tunmap_mapping_range(inode->i_mapping, holebegin, 0, 1);\n\t\t}\n\t}\n\n\tsetattr_copy(inode, attr);\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\tif (attr->ia_valid & ATTR_MODE)\n\t\terror = generic_acl_chmod(inode);\n#endif\n\treturn error;\n}\n\nstatic void shmem_evict_inode(struct inode *inode)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\n\tif (inode->i_mapping->a_ops == &shmem_aops) {\n\t\tshmem_unacct_size(info->flags, inode->i_size);\n\t\tinode->i_size = 0;\n\t\tshmem_truncate_range(inode, 0, (loff_t)-1);\n\t\tif (!list_empty(&info->swaplist)) {\n\t\t\tmutex_lock(&shmem_swaplist_mutex);\n\t\t\tlist_del_init(&info->swaplist);\n\t\t\tmutex_unlock(&shmem_swaplist_mutex);\n\t\t}\n\t} else\n\t\tkfree(info->symlink);\n\n\tsimple_xattrs_free(&info->xattrs);\n\tWARN_ON(inode->i_blocks);\n\tshmem_free_inode(inode->i_sb);\n\tclear_inode(inode);\n}\n\n/*\n * If swap found in inode, free it and move page from swapcache to filecache.\n */\nstatic int shmem_unuse_inode(struct shmem_inode_info *info,\n\t\t\t     swp_entry_t swap, struct page **pagep)\n{\n\tstruct address_space *mapping = info->vfs_inode.i_mapping;\n\tvoid *radswap;\n\tpgoff_t index;\n\tgfp_t gfp;\n\tint error = 0;\n\n\tradswap = swp_to_radix_entry(swap);\n\tindex = radix_tree_locate_item(&mapping->page_tree, radswap);\n\tif (index == -1)\n\t\treturn 0;\n\n\t/*\n\t * Move _head_ to start search for next from here.\n\t * But be careful: shmem_evict_inode checks list_empty without taking\n\t * mutex, and there's an instant in list_move_tail when info->swaplist\n\t * would appear empty, if it were the only one on shmem_swaplist.\n\t */\n\tif (shmem_swaplist.next != &info->swaplist)\n\t\tlist_move_tail(&shmem_swaplist, &info->swaplist);\n\n\tgfp = mapping_gfp_mask(mapping);\n\tif (shmem_should_replace_page(*pagep, gfp)) {\n\t\tmutex_unlock(&shmem_swaplist_mutex);\n\t\terror = shmem_replace_page(pagep, gfp, info, index);\n\t\tmutex_lock(&shmem_swaplist_mutex);\n\t\t/*\n\t\t * We needed to drop mutex to make that restrictive page\n\t\t * allocation, but the inode might have been freed while we\n\t\t * dropped it: although a racing shmem_evict_inode() cannot\n\t\t * complete without emptying the radix_tree, our page lock\n\t\t * on this swapcache page is not enough to prevent that -\n\t\t * free_swap_and_cache() of our swap entry will only\n\t\t * trylock_page(), removing swap from radix_tree whatever.\n\t\t *\n\t\t * We must not proceed to shmem_add_to_page_cache() if the\n\t\t * inode has been freed, but of course we cannot rely on\n\t\t * inode or mapping or info to check that.  However, we can\n\t\t * safely check if our swap entry is still in use (and here\n\t\t * it can't have got reused for another page): if it's still\n\t\t * in use, then the inode cannot have been freed yet, and we\n\t\t * can safely proceed (if it's no longer in use, that tells\n\t\t * nothing about the inode, but we don't need to unuse swap).\n\t\t */\n\t\tif (!page_swapcount(*pagep))\n\t\t\terror = -ENOENT;\n\t}\n\n\t/*\n\t * We rely on shmem_swaplist_mutex, not only to protect the swaplist,\n\t * but also to hold up shmem_evict_inode(): so inode cannot be freed\n\t * beneath us (pagelock doesn't help until the page is in pagecache).\n\t */\n\tif (!error)\n\t\terror = shmem_add_to_page_cache(*pagep, mapping, index,\n\t\t\t\t\t\tGFP_NOWAIT, radswap);\n\tif (error != -ENOMEM) {\n\t\t/*\n\t\t * Truncation and eviction use free_swap_and_cache(), which\n\t\t * only does trylock page: if we raced, best clean up here.\n\t\t */\n\t\tdelete_from_swap_cache(*pagep);\n\t\tset_page_dirty(*pagep);\n\t\tif (!error) {\n\t\t\tspin_lock(&info->lock);\n\t\t\tinfo->swapped--;\n\t\t\tspin_unlock(&info->lock);\n\t\t\tswap_free(swap);\n\t\t}\n\t\terror = 1;\t/* not an error, but entry was found */\n\t}\n\treturn error;\n}\n\n/*\n * Search through swapped inodes to find and replace swap by page.\n */\nint shmem_unuse(swp_entry_t swap, struct page *page)\n{\n\tstruct list_head *this, *next;\n\tstruct shmem_inode_info *info;\n\tint found = 0;\n\tint error = 0;\n\n\t/*\n\t * There's a faint possibility that swap page was replaced before\n\t * caller locked it: caller will come back later with the right page.\n\t */\n\tif (unlikely(!PageSwapCache(page) || page_private(page) != swap.val))\n\t\tgoto out;\n\n\t/*\n\t * Charge page using GFP_KERNEL while we can wait, before taking\n\t * the shmem_swaplist_mutex which might hold up shmem_writepage().\n\t * Charged back to the user (not to caller) when swap account is used.\n\t */\n\terror = mem_cgroup_cache_charge(page, current->mm, GFP_KERNEL);\n\tif (error)\n\t\tgoto out;\n\t/* No radix_tree_preload: swap entry keeps a place for page in tree */\n\n\tmutex_lock(&shmem_swaplist_mutex);\n\tlist_for_each_safe(this, next, &shmem_swaplist) {\n\t\tinfo = list_entry(this, struct shmem_inode_info, swaplist);\n\t\tif (info->swapped)\n\t\t\tfound = shmem_unuse_inode(info, swap, &page);\n\t\telse\n\t\t\tlist_del_init(&info->swaplist);\n\t\tcond_resched();\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&shmem_swaplist_mutex);\n\n\tif (found < 0)\n\t\terror = found;\nout:\n\tunlock_page(page);\n\tpage_cache_release(page);\n\treturn error;\n}\n\n/*\n * Move the page from the page cache to the swap cache.\n */\nstatic int shmem_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tstruct shmem_inode_info *info;\n\tstruct address_space *mapping;\n\tstruct inode *inode;\n\tswp_entry_t swap;\n\tpgoff_t index;\n\n\tBUG_ON(!PageLocked(page));\n\tmapping = page->mapping;\n\tindex = page->index;\n\tinode = mapping->host;\n\tinfo = SHMEM_I(inode);\n\tif (info->flags & VM_LOCKED)\n\t\tgoto redirty;\n\tif (!total_swap_pages)\n\t\tgoto redirty;\n\n\t/*\n\t * shmem_backing_dev_info's capabilities prevent regular writeback or\n\t * sync from ever calling shmem_writepage; but a stacking filesystem\n\t * might use ->writepage of its underlying filesystem, in which case\n\t * tmpfs should write out to swap only in response to memory pressure,\n\t * and not for the writeback threads or sync.\n\t */\n\tif (!wbc->for_reclaim) {\n\t\tWARN_ON_ONCE(1);\t/* Still happens? Tell us about it! */\n\t\tgoto redirty;\n\t}\n\n\t/*\n\t * This is somewhat ridiculous, but without plumbing a SWAP_MAP_FALLOC\n\t * value into swapfile.c, the only way we can correctly account for a\n\t * fallocated page arriving here is now to initialize it and write it.\n\t *\n\t * That's okay for a page already fallocated earlier, but if we have\n\t * not yet completed the fallocation, then (a) we want to keep track\n\t * of this page in case we have to undo it, and (b) it may not be a\n\t * good idea to continue anyway, once we're pushing into swap.  So\n\t * reactivate the page, and let shmem_fallocate() quit when too many.\n\t */\n\tif (!PageUptodate(page)) {\n\t\tif (inode->i_private) {\n\t\t\tstruct shmem_falloc *shmem_falloc;\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tshmem_falloc = inode->i_private;\n\t\t\tif (shmem_falloc &&\n\t\t\t    index >= shmem_falloc->start &&\n\t\t\t    index < shmem_falloc->next)\n\t\t\t\tshmem_falloc->nr_unswapped++;\n\t\t\telse\n\t\t\t\tshmem_falloc = NULL;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tif (shmem_falloc)\n\t\t\t\tgoto redirty;\n\t\t}\n\t\tclear_highpage(page);\n\t\tflush_dcache_page(page);\n\t\tSetPageUptodate(page);\n\t}\n\n\tswap = get_swap_page();\n\tif (!swap.val)\n\t\tgoto redirty;\n\n\t/*\n\t * Add inode to shmem_unuse()'s list of swapped-out inodes,\n\t * if it's not already there.  Do it now before the page is\n\t * moved to swap cache, when its pagelock no longer protects\n\t * the inode from eviction.  But don't unlock the mutex until\n\t * we've incremented swapped, because shmem_unuse_inode() will\n\t * prune a !swapped inode from the swaplist under this mutex.\n\t */\n\tmutex_lock(&shmem_swaplist_mutex);\n\tif (list_empty(&info->swaplist))\n\t\tlist_add_tail(&info->swaplist, &shmem_swaplist);\n\n\tif (add_to_swap_cache(page, swap, GFP_ATOMIC) == 0) {\n\t\tswap_shmem_alloc(swap);\n\t\tshmem_delete_from_page_cache(page, swp_to_radix_entry(swap));\n\n\t\tspin_lock(&info->lock);\n\t\tinfo->swapped++;\n\t\tshmem_recalc_inode(inode);\n\t\tspin_unlock(&info->lock);\n\n\t\tmutex_unlock(&shmem_swaplist_mutex);\n\t\tBUG_ON(page_mapped(page));\n\t\tswap_writepage(page, wbc);\n\t\treturn 0;\n\t}\n\n\tmutex_unlock(&shmem_swaplist_mutex);\n\tswapcache_free(swap, NULL);\nredirty:\n\tset_page_dirty(page);\n\tif (wbc->for_reclaim)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\t/* Return with page locked */\n\tunlock_page(page);\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\n#ifdef CONFIG_TMPFS\nstatic void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)\n{\n\tchar buffer[64];\n\n\tif (!mpol || mpol->mode == MPOL_DEFAULT)\n\t\treturn;\t\t/* show nothing */\n\n\tmpol_to_str(buffer, sizeof(buffer), mpol);\n\n\tseq_printf(seq, \",mpol=%s\", buffer);\n}\n\nstatic struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)\n{\n\tstruct mempolicy *mpol = NULL;\n\tif (sbinfo->mpol) {\n\t\tspin_lock(&sbinfo->stat_lock);\t/* prevent replace/use races */\n\t\tmpol = sbinfo->mpol;\n\t\tmpol_get(mpol);\n\t\tspin_unlock(&sbinfo->stat_lock);\n\t}\n\treturn mpol;\n}\n#endif /* CONFIG_TMPFS */\n\nstatic struct page *shmem_swapin(swp_entry_t swap, gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct vm_area_struct pvma;\n\tstruct page *page;\n\n\t/* Create a pseudo vma that just contains the policy */\n\tpvma.vm_start = 0;\n\t/* Bias interleave by inode number to distribute better across nodes */\n\tpvma.vm_pgoff = index + info->vfs_inode.i_ino;\n\tpvma.vm_ops = NULL;\n\tpvma.vm_policy = mpol_shared_policy_lookup(&info->policy, index);\n\n\tpage = swapin_readahead(swap, gfp, &pvma, 0);\n\n\t/* Drop reference taken by mpol_shared_policy_lookup() */\n\tmpol_cond_put(pvma.vm_policy);\n\n\treturn page;\n}\n\nstatic struct page *shmem_alloc_page(gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct vm_area_struct pvma;\n\tstruct page *page;\n\n\t/* Create a pseudo vma that just contains the policy */\n\tpvma.vm_start = 0;\n\t/* Bias interleave by inode number to distribute better across nodes */\n\tpvma.vm_pgoff = index + info->vfs_inode.i_ino;\n\tpvma.vm_ops = NULL;\n\tpvma.vm_policy = mpol_shared_policy_lookup(&info->policy, index);\n\n\tpage = alloc_page_vma(gfp, &pvma, 0);\n\n\t/* Drop reference taken by mpol_shared_policy_lookup() */\n\tmpol_cond_put(pvma.vm_policy);\n\n\treturn page;\n}\n#else /* !CONFIG_NUMA */\n#ifdef CONFIG_TMPFS\nstatic inline void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)\n{\n}\n#endif /* CONFIG_TMPFS */\n\nstatic inline struct page *shmem_swapin(swp_entry_t swap, gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\treturn swapin_readahead(swap, gfp, NULL, 0);\n}\n\nstatic inline struct page *shmem_alloc_page(gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\treturn alloc_page(gfp);\n}\n#endif /* CONFIG_NUMA */\n\n#if !defined(CONFIG_NUMA) || !defined(CONFIG_TMPFS)\nstatic inline struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)\n{\n\treturn NULL;\n}\n#endif\n\n/*\n * When a page is moved from swapcache to shmem filecache (either by the\n * usual swapin of shmem_getpage_gfp(), or by the less common swapoff of\n * shmem_unuse_inode()), it may have been read in earlier from swap, in\n * ignorance of the mapping it belongs to.  If that mapping has special\n * constraints (like the gma500 GEM driver, which requires RAM below 4GB),\n * we may need to copy to a suitable page before moving to filecache.\n *\n * In a future release, this may well be extended to respect cpuset and\n * NUMA mempolicy, and applied also to anonymous pages in do_swap_page();\n * but for now it is a simple matter of zone.\n */\nstatic bool shmem_should_replace_page(struct page *page, gfp_t gfp)\n{\n\treturn page_zonenum(page) > gfp_zone(gfp);\n}\n\nstatic int shmem_replace_page(struct page **pagep, gfp_t gfp,\n\t\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct page *oldpage, *newpage;\n\tstruct address_space *swap_mapping;\n\tpgoff_t swap_index;\n\tint error;\n\n\toldpage = *pagep;\n\tswap_index = page_private(oldpage);\n\tswap_mapping = page_mapping(oldpage);\n\n\t/*\n\t * We have arrived here because our zones are constrained, so don't\n\t * limit chance of success by further cpuset and node constraints.\n\t */\n\tgfp &= ~GFP_CONSTRAINT_MASK;\n\tnewpage = shmem_alloc_page(gfp, info, index);\n\tif (!newpage)\n\t\treturn -ENOMEM;\n\n\tpage_cache_get(newpage);\n\tcopy_highpage(newpage, oldpage);\n\tflush_dcache_page(newpage);\n\n\t__set_page_locked(newpage);\n\tSetPageUptodate(newpage);\n\tSetPageSwapBacked(newpage);\n\tset_page_private(newpage, swap_index);\n\tSetPageSwapCache(newpage);\n\n\t/*\n\t * Our caller will very soon move newpage out of swapcache, but it's\n\t * a nice clean interface for us to replace oldpage by newpage there.\n\t */\n\tspin_lock_irq(&swap_mapping->tree_lock);\n\terror = shmem_radix_tree_replace(swap_mapping, swap_index, oldpage,\n\t\t\t\t\t\t\t\t   newpage);\n\tif (!error) {\n\t\t__inc_zone_page_state(newpage, NR_FILE_PAGES);\n\t\t__dec_zone_page_state(oldpage, NR_FILE_PAGES);\n\t}\n\tspin_unlock_irq(&swap_mapping->tree_lock);\n\n\tif (unlikely(error)) {\n\t\t/*\n\t\t * Is this possible?  I think not, now that our callers check\n\t\t * both PageSwapCache and page_private after getting page lock;\n\t\t * but be defensive.  Reverse old to newpage for clear and free.\n\t\t */\n\t\toldpage = newpage;\n\t} else {\n\t\tmem_cgroup_replace_page_cache(oldpage, newpage);\n\t\tlru_cache_add_anon(newpage);\n\t\t*pagep = newpage;\n\t}\n\n\tClearPageSwapCache(oldpage);\n\tset_page_private(oldpage, 0);\n\n\tunlock_page(oldpage);\n\tpage_cache_release(oldpage);\n\tpage_cache_release(oldpage);\n\treturn error;\n}\n\n/*\n * shmem_getpage_gfp - find page in cache, or get from swap, or allocate\n *\n * If we allocate a new one we do not mark it dirty. That's up to the\n * vm. If we swap it in we mark it dirty since we also free the swap\n * entry since a page cannot live in both the swap and page cache\n */\nstatic int shmem_getpage_gfp(struct inode *inode, pgoff_t index,\n\tstruct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct shmem_inode_info *info;\n\tstruct shmem_sb_info *sbinfo;\n\tstruct page *page;\n\tswp_entry_t swap;\n\tint error;\n\tint once = 0;\n\tint alloced = 0;\n\n\tif (index > (MAX_LFS_FILESIZE >> PAGE_CACHE_SHIFT))\n\t\treturn -EFBIG;\nrepeat:\n\tswap.val = 0;\n\tpage = find_lock_page(mapping, index);\n\tif (radix_tree_exceptional_entry(page)) {\n\t\tswap = radix_to_swp_entry(page);\n\t\tpage = NULL;\n\t}\n\n\tif (sgp != SGP_WRITE && sgp != SGP_FALLOC &&\n\t    ((loff_t)index << PAGE_CACHE_SHIFT) >= i_size_read(inode)) {\n\t\terror = -EINVAL;\n\t\tgoto failed;\n\t}\n\n\t/* fallocated page? */\n\tif (page && !PageUptodate(page)) {\n\t\tif (sgp != SGP_READ)\n\t\t\tgoto clear;\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tpage = NULL;\n\t}\n\tif (page || (sgp == SGP_READ && !swap.val)) {\n\t\t*pagep = page;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Fast cache lookup did not find it:\n\t * bring it back from swap or allocate.\n\t */\n\tinfo = SHMEM_I(inode);\n\tsbinfo = SHMEM_SB(inode->i_sb);\n\n\tif (swap.val) {\n\t\t/* Look it up and read it in.. */\n\t\tpage = lookup_swap_cache(swap);\n\t\tif (!page) {\n\t\t\t/* here we actually do the io */\n\t\t\tif (fault_type)\n\t\t\t\t*fault_type |= VM_FAULT_MAJOR;\n\t\t\tpage = shmem_swapin(swap, gfp, info, index);\n\t\t\tif (!page) {\n\t\t\t\terror = -ENOMEM;\n\t\t\t\tgoto failed;\n\t\t\t}\n\t\t}\n\n\t\t/* We have to do this with page locked to prevent races */\n\t\tlock_page(page);\n\t\tif (!PageSwapCache(page) || page_private(page) != swap.val ||\n\t\t    !shmem_confirm_swap(mapping, index, swap)) {\n\t\t\terror = -EEXIST;\t/* try again */\n\t\t\tgoto unlock;\n\t\t}\n\t\tif (!PageUptodate(page)) {\n\t\t\terror = -EIO;\n\t\t\tgoto failed;\n\t\t}\n\t\twait_on_page_writeback(page);\n\n\t\tif (shmem_should_replace_page(page, gfp)) {\n\t\t\terror = shmem_replace_page(&page, gfp, info, index);\n\t\t\tif (error)\n\t\t\t\tgoto failed;\n\t\t}\n\n\t\terror = mem_cgroup_cache_charge(page, current->mm,\n\t\t\t\t\t\tgfp & GFP_RECLAIM_MASK);\n\t\tif (!error) {\n\t\t\terror = shmem_add_to_page_cache(page, mapping, index,\n\t\t\t\t\t\tgfp, swp_to_radix_entry(swap));\n\t\t\t/*\n\t\t\t * We already confirmed swap under page lock, and make\n\t\t\t * no memory allocation here, so usually no possibility\n\t\t\t * of error; but free_swap_and_cache() only trylocks a\n\t\t\t * page, so it is just possible that the entry has been\n\t\t\t * truncated or holepunched since swap was confirmed.\n\t\t\t * shmem_undo_range() will have done some of the\n\t\t\t * unaccounting, now delete_from_swap_cache() will do\n\t\t\t * the rest (including mem_cgroup_uncharge_swapcache).\n\t\t\t * Reset swap.val? No, leave it so \"failed\" goes back to\n\t\t\t * \"repeat\": reading a hole and writing should succeed.\n\t\t\t */\n\t\t\tif (error)\n\t\t\t\tdelete_from_swap_cache(page);\n\t\t}\n\t\tif (error)\n\t\t\tgoto failed;\n\n\t\tspin_lock(&info->lock);\n\t\tinfo->swapped--;\n\t\tshmem_recalc_inode(inode);\n\t\tspin_unlock(&info->lock);\n\n\t\tdelete_from_swap_cache(page);\n\t\tset_page_dirty(page);\n\t\tswap_free(swap);\n\n\t} else {\n\t\tif (shmem_acct_block(info->flags)) {\n\t\t\terror = -ENOSPC;\n\t\t\tgoto failed;\n\t\t}\n\t\tif (sbinfo->max_blocks) {\n\t\t\tif (percpu_counter_compare(&sbinfo->used_blocks,\n\t\t\t\t\t\tsbinfo->max_blocks) >= 0) {\n\t\t\t\terror = -ENOSPC;\n\t\t\t\tgoto unacct;\n\t\t\t}\n\t\t\tpercpu_counter_inc(&sbinfo->used_blocks);\n\t\t}\n\n\t\tpage = shmem_alloc_page(gfp, info, index);\n\t\tif (!page) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto decused;\n\t\t}\n\n\t\tSetPageSwapBacked(page);\n\t\t__set_page_locked(page);\n\t\terror = mem_cgroup_cache_charge(page, current->mm,\n\t\t\t\t\t\tgfp & GFP_RECLAIM_MASK);\n\t\tif (error)\n\t\t\tgoto decused;\n\t\terror = radix_tree_preload(gfp & GFP_RECLAIM_MASK);\n\t\tif (!error) {\n\t\t\terror = shmem_add_to_page_cache(page, mapping, index,\n\t\t\t\t\t\t\tgfp, NULL);\n\t\t\tradix_tree_preload_end();\n\t\t}\n\t\tif (error) {\n\t\t\tmem_cgroup_uncharge_cache_page(page);\n\t\t\tgoto decused;\n\t\t}\n\t\tlru_cache_add_anon(page);\n\n\t\tspin_lock(&info->lock);\n\t\tinfo->alloced++;\n\t\tinode->i_blocks += BLOCKS_PER_PAGE;\n\t\tshmem_recalc_inode(inode);\n\t\tspin_unlock(&info->lock);\n\t\talloced = true;\n\n\t\t/*\n\t\t * Let SGP_FALLOC use the SGP_WRITE optimization on a new page.\n\t\t */\n\t\tif (sgp == SGP_FALLOC)\n\t\t\tsgp = SGP_WRITE;\nclear:\n\t\t/*\n\t\t * Let SGP_WRITE caller clear ends if write does not fill page;\n\t\t * but SGP_FALLOC on a page fallocated earlier must initialize\n\t\t * it now, lest undo on failure cancel our earlier guarantee.\n\t\t */\n\t\tif (sgp != SGP_WRITE) {\n\t\t\tclear_highpage(page);\n\t\t\tflush_dcache_page(page);\n\t\t\tSetPageUptodate(page);\n\t\t}\n\t\tif (sgp == SGP_DIRTY)\n\t\t\tset_page_dirty(page);\n\t}\n\n\t/* Perhaps the file has been truncated since we checked */\n\tif (sgp != SGP_WRITE && sgp != SGP_FALLOC &&\n\t    ((loff_t)index << PAGE_CACHE_SHIFT) >= i_size_read(inode)) {\n\t\terror = -EINVAL;\n\t\tif (alloced)\n\t\t\tgoto trunc;\n\t\telse\n\t\t\tgoto failed;\n\t}\n\t*pagep = page;\n\treturn 0;\n\n\t/*\n\t * Error recovery.\n\t */\ntrunc:\n\tinfo = SHMEM_I(inode);\n\tClearPageDirty(page);\n\tdelete_from_page_cache(page);\n\tspin_lock(&info->lock);\n\tinfo->alloced--;\n\tinode->i_blocks -= BLOCKS_PER_PAGE;\n\tspin_unlock(&info->lock);\ndecused:\n\tsbinfo = SHMEM_SB(inode->i_sb);\n\tif (sbinfo->max_blocks)\n\t\tpercpu_counter_add(&sbinfo->used_blocks, -1);\nunacct:\n\tshmem_unacct_blocks(info->flags, 1);\nfailed:\n\tif (swap.val && error != -EINVAL &&\n\t    !shmem_confirm_swap(mapping, index, swap))\n\t\terror = -EEXIST;\nunlock:\n\tif (page) {\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t}\n\tif (error == -ENOSPC && !once++) {\n\t\tinfo = SHMEM_I(inode);\n\t\tspin_lock(&info->lock);\n\t\tshmem_recalc_inode(inode);\n\t\tspin_unlock(&info->lock);\n\t\tgoto repeat;\n\t}\n\tif (error == -EEXIST)\t/* from above or from radix_tree_insert */\n\t\tgoto repeat;\n\treturn error;\n}\n\nstatic int shmem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\n\tint error;\n\tint ret = VM_FAULT_LOCKED;\n\n\terror = shmem_getpage(inode, vmf->pgoff, &vmf->page, SGP_CACHE, &ret);\n\tif (error)\n\t\treturn ((error == -ENOMEM) ? VM_FAULT_OOM : VM_FAULT_SIGBUS);\n\n\tif (ret & VM_FAULT_MAJOR) {\n\t\tcount_vm_event(PGMAJFAULT);\n\t\tmem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_NUMA\nstatic int shmem_set_policy(struct vm_area_struct *vma, struct mempolicy *mpol)\n{\n\tstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\n\treturn mpol_set_shared_policy(&SHMEM_I(inode)->policy, vma, mpol);\n}\n\nstatic struct mempolicy *shmem_get_policy(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long addr)\n{\n\tstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\n\tpgoff_t index;\n\n\tindex = ((addr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\n\treturn mpol_shared_policy_lookup(&SHMEM_I(inode)->policy, index);\n}\n#endif\n\nint shmem_lock(struct file *file, int lock, struct user_struct *user)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tint retval = -ENOMEM;\n\n\tspin_lock(&info->lock);\n\tif (lock && !(info->flags & VM_LOCKED)) {\n\t\tif (!user_shm_lock(inode->i_size, user))\n\t\t\tgoto out_nomem;\n\t\tinfo->flags |= VM_LOCKED;\n\t\tmapping_set_unevictable(file->f_mapping);\n\t}\n\tif (!lock && (info->flags & VM_LOCKED) && user) {\n\t\tuser_shm_unlock(inode->i_size, user);\n\t\tinfo->flags &= ~VM_LOCKED;\n\t\tmapping_clear_unevictable(file->f_mapping);\n\t}\n\tretval = 0;\n\nout_nomem:\n\tspin_unlock(&info->lock);\n\treturn retval;\n}\n\nstatic int shmem_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tfile_accessed(file);\n\tvma->vm_ops = &shmem_vm_ops;\n\treturn 0;\n}\n\nstatic struct inode *shmem_get_inode(struct super_block *sb, const struct inode *dir,\n\t\t\t\t     umode_t mode, dev_t dev, unsigned long flags)\n{\n\tstruct inode *inode;\n\tstruct shmem_inode_info *info;\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\n\tif (shmem_reserve_inode(sb))\n\t\treturn NULL;\n\n\tinode = new_inode(sb);\n\tif (inode) {\n\t\tinode->i_ino = get_next_ino();\n\t\tinode_init_owner(inode, dir, mode);\n\t\tinode->i_blocks = 0;\n\t\tinode->i_mapping->backing_dev_info = &shmem_backing_dev_info;\n\t\tinode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;\n\t\tinode->i_generation = get_seconds();\n\t\tinfo = SHMEM_I(inode);\n\t\tmemset(info, 0, (char *)inode - (char *)info);\n\t\tspin_lock_init(&info->lock);\n\t\tinfo->flags = flags & VM_NORESERVE;\n\t\tINIT_LIST_HEAD(&info->swaplist);\n\t\tsimple_xattrs_init(&info->xattrs);\n\t\tcache_no_acl(inode);\n\n\t\tswitch (mode & S_IFMT) {\n\t\tdefault:\n\t\t\tinode->i_op = &shmem_special_inode_operations;\n\t\t\tinit_special_inode(inode, mode, dev);\n\t\t\tbreak;\n\t\tcase S_IFREG:\n\t\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\t\tinode->i_op = &shmem_inode_operations;\n\t\t\tinode->i_fop = &shmem_file_operations;\n\t\t\tmpol_shared_policy_init(&info->policy,\n\t\t\t\t\t\t shmem_get_sbmpol(sbinfo));\n\t\t\tbreak;\n\t\tcase S_IFDIR:\n\t\t\tinc_nlink(inode);\n\t\t\t/* Some things misbehave if size == 0 on a directory */\n\t\t\tinode->i_size = 2 * BOGO_DIRENT_SIZE;\n\t\t\tinode->i_op = &shmem_dir_inode_operations;\n\t\t\tinode->i_fop = &simple_dir_operations;\n\t\t\tbreak;\n\t\tcase S_IFLNK:\n\t\t\t/*\n\t\t\t * Must not load anything in the rbtree,\n\t\t\t * mpol_free_shared_policy will not be called.\n\t\t\t */\n\t\t\tmpol_shared_policy_init(&info->policy, NULL);\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tshmem_free_inode(sb);\n\treturn inode;\n}\n\n#ifdef CONFIG_TMPFS\nstatic const struct inode_operations shmem_symlink_inode_operations;\nstatic const struct inode_operations shmem_short_symlink_operations;\n\n#ifdef CONFIG_TMPFS_XATTR\nstatic int shmem_initxattrs(struct inode *, const struct xattr *, void *);\n#else\n#define shmem_initxattrs NULL\n#endif\n\nstatic int\nshmem_write_begin(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned flags,\n\t\t\tstruct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tpgoff_t index = pos >> PAGE_CACHE_SHIFT;\n\treturn shmem_getpage(inode, index, pagep, SGP_WRITE, NULL);\n}\n\nstatic int\nshmem_write_end(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\n\tif (pos + copied > inode->i_size)\n\t\ti_size_write(inode, pos + copied);\n\n\tif (!PageUptodate(page)) {\n\t\tif (copied < PAGE_CACHE_SIZE) {\n\t\t\tunsigned from = pos & (PAGE_CACHE_SIZE - 1);\n\t\t\tzero_user_segments(page, 0, from,\n\t\t\t\t\tfrom + copied, PAGE_CACHE_SIZE);\n\t\t}\n\t\tSetPageUptodate(page);\n\t}\n\tset_page_dirty(page);\n\tunlock_page(page);\n\tpage_cache_release(page);\n\n\treturn copied;\n}\n\nstatic void do_shmem_file_read(struct file *filp, loff_t *ppos, read_descriptor_t *desc, read_actor_t actor)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\tpgoff_t index;\n\tunsigned long offset;\n\tenum sgp_type sgp = SGP_READ;\n\n\t/*\n\t * Might this read be for a stacking filesystem?  Then when reading\n\t * holes of a sparse file, we actually need to allocate those pages,\n\t * and even mark them dirty, so it cannot exceed the max_blocks limit.\n\t */\n\tif (segment_eq(get_fs(), KERNEL_DS))\n\t\tsgp = SGP_DIRTY;\n\n\tindex = *ppos >> PAGE_CACHE_SHIFT;\n\toffset = *ppos & ~PAGE_CACHE_MASK;\n\n\tfor (;;) {\n\t\tstruct page *page = NULL;\n\t\tpgoff_t end_index;\n\t\tunsigned long nr, ret;\n\t\tloff_t i_size = i_size_read(inode);\n\n\t\tend_index = i_size >> PAGE_CACHE_SHIFT;\n\t\tif (index > end_index)\n\t\t\tbreak;\n\t\tif (index == end_index) {\n\t\t\tnr = i_size & ~PAGE_CACHE_MASK;\n\t\t\tif (nr <= offset)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tdesc->error = shmem_getpage(inode, index, &page, sgp, NULL);\n\t\tif (desc->error) {\n\t\t\tif (desc->error == -EINVAL)\n\t\t\t\tdesc->error = 0;\n\t\t\tbreak;\n\t\t}\n\t\tif (page)\n\t\t\tunlock_page(page);\n\n\t\t/*\n\t\t * We must evaluate after, since reads (unlike writes)\n\t\t * are called without i_mutex protection against truncate\n\t\t */\n\t\tnr = PAGE_CACHE_SIZE;\n\t\ti_size = i_size_read(inode);\n\t\tend_index = i_size >> PAGE_CACHE_SHIFT;\n\t\tif (index == end_index) {\n\t\t\tnr = i_size & ~PAGE_CACHE_MASK;\n\t\t\tif (nr <= offset) {\n\t\t\t\tif (page)\n\t\t\t\t\tpage_cache_release(page);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tnr -= offset;\n\n\t\tif (page) {\n\t\t\t/*\n\t\t\t * If users can be writing to this page using arbitrary\n\t\t\t * virtual addresses, take care about potential aliasing\n\t\t\t * before reading the page on the kernel side.\n\t\t\t */\n\t\t\tif (mapping_writably_mapped(mapping))\n\t\t\t\tflush_dcache_page(page);\n\t\t\t/*\n\t\t\t * Mark the page accessed if we read the beginning.\n\t\t\t */\n\t\t\tif (!offset)\n\t\t\t\tmark_page_accessed(page);\n\t\t} else {\n\t\t\tpage = ZERO_PAGE(0);\n\t\t\tpage_cache_get(page);\n\t\t}\n\n\t\t/*\n\t\t * Ok, we have the page, and it's up-to-date, so\n\t\t * now we can copy it to user space...\n\t\t *\n\t\t * The actor routine returns how many bytes were actually used..\n\t\t * NOTE! This may not be the same as how much of a user buffer\n\t\t * we filled up (we may be padding etc), so we can only update\n\t\t * \"pos\" here (the actor routine has to update the user buffer\n\t\t * pointers and the remaining count).\n\t\t */\n\t\tret = actor(desc, page, offset, nr);\n\t\toffset += ret;\n\t\tindex += offset >> PAGE_CACHE_SHIFT;\n\t\toffset &= ~PAGE_CACHE_MASK;\n\n\t\tpage_cache_release(page);\n\t\tif (ret != nr || !desc->count)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\t*ppos = ((loff_t) index << PAGE_CACHE_SHIFT) + offset;\n\tfile_accessed(filp);\n}\n\nstatic ssize_t shmem_file_aio_read(struct kiocb *iocb,\n\t\tconst struct iovec *iov, unsigned long nr_segs, loff_t pos)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tssize_t retval;\n\tunsigned long seg;\n\tsize_t count;\n\tloff_t *ppos = &iocb->ki_pos;\n\n\tretval = generic_segment_checks(iov, &nr_segs, &count, VERIFY_WRITE);\n\tif (retval)\n\t\treturn retval;\n\n\tfor (seg = 0; seg < nr_segs; seg++) {\n\t\tread_descriptor_t desc;\n\n\t\tdesc.written = 0;\n\t\tdesc.arg.buf = iov[seg].iov_base;\n\t\tdesc.count = iov[seg].iov_len;\n\t\tif (desc.count == 0)\n\t\t\tcontinue;\n\t\tdesc.error = 0;\n\t\tdo_shmem_file_read(filp, ppos, &desc, file_read_actor);\n\t\tretval += desc.written;\n\t\tif (desc.error) {\n\t\t\tretval = retval ?: desc.error;\n\t\t\tbreak;\n\t\t}\n\t\tif (desc.count > 0)\n\t\t\tbreak;\n\t}\n\treturn retval;\n}\n\nstatic ssize_t shmem_file_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\t\tunsigned int flags)\n{\n\tstruct address_space *mapping = in->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tunsigned int loff, nr_pages, req_pages;\n\tstruct page *pages[PIPE_DEF_BUFFERS];\n\tstruct partial_page partial[PIPE_DEF_BUFFERS];\n\tstruct page *page;\n\tpgoff_t index, end_index;\n\tloff_t isize, left;\n\tint error, page_nr;\n\tstruct splice_pipe_desc spd = {\n\t\t.pages = pages,\n\t\t.partial = partial,\n\t\t.nr_pages_max = PIPE_DEF_BUFFERS,\n\t\t.flags = flags,\n\t\t.ops = &page_cache_pipe_buf_ops,\n\t\t.spd_release = spd_release_page,\n\t};\n\n\tisize = i_size_read(inode);\n\tif (unlikely(*ppos >= isize))\n\t\treturn 0;\n\n\tleft = isize - *ppos;\n\tif (unlikely(left < len))\n\t\tlen = left;\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n\tindex = *ppos >> PAGE_CACHE_SHIFT;\n\tloff = *ppos & ~PAGE_CACHE_MASK;\n\treq_pages = (len + loff + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\tnr_pages = min(req_pages, pipe->buffers);\n\n\tspd.nr_pages = find_get_pages_contig(mapping, index,\n\t\t\t\t\t\tnr_pages, spd.pages);\n\tindex += spd.nr_pages;\n\terror = 0;\n\n\twhile (spd.nr_pages < nr_pages) {\n\t\terror = shmem_getpage(inode, index, &page, SGP_CACHE, NULL);\n\t\tif (error)\n\t\t\tbreak;\n\t\tunlock_page(page);\n\t\tspd.pages[spd.nr_pages++] = page;\n\t\tindex++;\n\t}\n\n\tindex = *ppos >> PAGE_CACHE_SHIFT;\n\tnr_pages = spd.nr_pages;\n\tspd.nr_pages = 0;\n\n\tfor (page_nr = 0; page_nr < nr_pages; page_nr++) {\n\t\tunsigned int this_len;\n\n\t\tif (!len)\n\t\t\tbreak;\n\n\t\tthis_len = min_t(unsigned long, len, PAGE_CACHE_SIZE - loff);\n\t\tpage = spd.pages[page_nr];\n\n\t\tif (!PageUptodate(page) || page->mapping != mapping) {\n\t\t\terror = shmem_getpage(inode, index, &page,\n\t\t\t\t\t\t\tSGP_CACHE, NULL);\n\t\t\tif (error)\n\t\t\t\tbreak;\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(spd.pages[page_nr]);\n\t\t\tspd.pages[page_nr] = page;\n\t\t}\n\n\t\tisize = i_size_read(inode);\n\t\tend_index = (isize - 1) >> PAGE_CACHE_SHIFT;\n\t\tif (unlikely(!isize || index > end_index))\n\t\t\tbreak;\n\n\t\tif (end_index == index) {\n\t\t\tunsigned int plen;\n\n\t\t\tplen = ((isize - 1) & ~PAGE_CACHE_MASK) + 1;\n\t\t\tif (plen <= loff)\n\t\t\t\tbreak;\n\n\t\t\tthis_len = min(this_len, plen - loff);\n\t\t\tlen = this_len;\n\t\t}\n\n\t\tspd.partial[page_nr].offset = loff;\n\t\tspd.partial[page_nr].len = this_len;\n\t\tlen -= this_len;\n\t\tloff = 0;\n\t\tspd.nr_pages++;\n\t\tindex++;\n\t}\n\n\twhile (page_nr < nr_pages)\n\t\tpage_cache_release(spd.pages[page_nr++]);\n\n\tif (spd.nr_pages)\n\t\terror = splice_to_pipe(pipe, &spd);\n\n\tsplice_shrink_spd(&spd);\n\n\tif (error > 0) {\n\t\t*ppos += error;\n\t\tfile_accessed(in);\n\t}\n\treturn error;\n}\n\n/*\n * llseek SEEK_DATA or SEEK_HOLE through the radix_tree.\n */\nstatic pgoff_t shmem_seek_hole_data(struct address_space *mapping,\n\t\t\t\t    pgoff_t index, pgoff_t end, int whence)\n{\n\tstruct page *page;\n\tstruct pagevec pvec;\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tbool done = false;\n\tint i;\n\n\tpagevec_init(&pvec, 0);\n\tpvec.nr = 1;\t\t/* start small: we may be there already */\n\twhile (!done) {\n\t\tpvec.nr = shmem_find_get_pages_and_swap(mapping, index,\n\t\t\t\t\tpvec.nr, pvec.pages, indices);\n\t\tif (!pvec.nr) {\n\t\t\tif (whence == SEEK_DATA)\n\t\t\t\tindex = end;\n\t\t\tbreak;\n\t\t}\n\t\tfor (i = 0; i < pvec.nr; i++, index++) {\n\t\t\tif (index < indices[i]) {\n\t\t\t\tif (whence == SEEK_HOLE) {\n\t\t\t\t\tdone = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tindex = indices[i];\n\t\t\t}\n\t\t\tpage = pvec.pages[i];\n\t\t\tif (page && !radix_tree_exceptional_entry(page)) {\n\t\t\t\tif (!PageUptodate(page))\n\t\t\t\t\tpage = NULL;\n\t\t\t}\n\t\t\tif (index >= end ||\n\t\t\t    (page && whence == SEEK_DATA) ||\n\t\t\t    (!page && whence == SEEK_HOLE)) {\n\t\t\t\tdone = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tshmem_deswap_pagevec(&pvec);\n\t\tpagevec_release(&pvec);\n\t\tpvec.nr = PAGEVEC_SIZE;\n\t\tcond_resched();\n\t}\n\treturn index;\n}\n\nstatic loff_t shmem_file_llseek(struct file *file, loff_t offset, int whence)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tpgoff_t start, end;\n\tloff_t new_offset;\n\n\tif (whence != SEEK_DATA && whence != SEEK_HOLE)\n\t\treturn generic_file_llseek_size(file, offset, whence,\n\t\t\t\t\tMAX_LFS_FILESIZE, i_size_read(inode));\n\tmutex_lock(&inode->i_mutex);\n\t/* We're holding i_mutex so we can access i_size directly */\n\n\tif (offset < 0)\n\t\toffset = -EINVAL;\n\telse if (offset >= inode->i_size)\n\t\toffset = -ENXIO;\n\telse {\n\t\tstart = offset >> PAGE_CACHE_SHIFT;\n\t\tend = (inode->i_size + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\t\tnew_offset = shmem_seek_hole_data(mapping, start, end, whence);\n\t\tnew_offset <<= PAGE_CACHE_SHIFT;\n\t\tif (new_offset > offset) {\n\t\t\tif (new_offset < inode->i_size)\n\t\t\t\toffset = new_offset;\n\t\t\telse if (whence == SEEK_DATA)\n\t\t\t\toffset = -ENXIO;\n\t\t\telse\n\t\t\t\toffset = inode->i_size;\n\t\t}\n\t}\n\n\tif (offset >= 0 && offset != file->f_pos) {\n\t\tfile->f_pos = offset;\n\t\tfile->f_version = 0;\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\treturn offset;\n}\n\nstatic long shmem_fallocate(struct file *file, int mode, loff_t offset,\n\t\t\t\t\t\t\t loff_t len)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\tstruct shmem_falloc shmem_falloc;\n\tpgoff_t start, index, end;\n\tint error;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE) {\n\t\tstruct address_space *mapping = file->f_mapping;\n\t\tloff_t unmap_start = round_up(offset, PAGE_SIZE);\n\t\tloff_t unmap_end = round_down(offset + len, PAGE_SIZE) - 1;\n\n\t\tif ((u64)unmap_end > (u64)unmap_start)\n\t\t\tunmap_mapping_range(mapping, unmap_start,\n\t\t\t\t\t    1 + unmap_end - unmap_start, 0);\n\t\tshmem_truncate_range(inode, offset, offset + len - 1);\n\t\t/* No need to unmap again: hole-punching leaves COWed pages */\n\t\terror = 0;\n\t\tgoto out;\n\t}\n\n\t/* We need to check rlimit even when FALLOC_FL_KEEP_SIZE */\n\terror = inode_newsize_ok(inode, offset + len);\n\tif (error)\n\t\tgoto out;\n\n\tstart = offset >> PAGE_CACHE_SHIFT;\n\tend = (offset + len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\t/* Try to avoid a swapstorm if len is impossible to satisfy */\n\tif (sbinfo->max_blocks && end - start > sbinfo->max_blocks) {\n\t\terror = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tshmem_falloc.start = start;\n\tshmem_falloc.next  = start;\n\tshmem_falloc.nr_falloced = 0;\n\tshmem_falloc.nr_unswapped = 0;\n\tspin_lock(&inode->i_lock);\n\tinode->i_private = &shmem_falloc;\n\tspin_unlock(&inode->i_lock);\n\n\tfor (index = start; index < end; index++) {\n\t\tstruct page *page;\n\n\t\t/*\n\t\t * Good, the fallocate(2) manpage permits EINTR: we may have\n\t\t * been interrupted because we are using up too much memory.\n\t\t */\n\t\tif (signal_pending(current))\n\t\t\terror = -EINTR;\n\t\telse if (shmem_falloc.nr_unswapped > shmem_falloc.nr_falloced)\n\t\t\terror = -ENOMEM;\n\t\telse\n\t\t\terror = shmem_getpage(inode, index, &page, SGP_FALLOC,\n\t\t\t\t\t\t\t\t\tNULL);\n\t\tif (error) {\n\t\t\t/* Remove the !PageUptodate pages we added */\n\t\t\tshmem_undo_range(inode,\n\t\t\t\t(loff_t)start << PAGE_CACHE_SHIFT,\n\t\t\t\t(loff_t)index << PAGE_CACHE_SHIFT, true);\n\t\t\tgoto undone;\n\t\t}\n\n\t\t/*\n\t\t * Inform shmem_writepage() how far we have reached.\n\t\t * No need for lock or barrier: we have the page lock.\n\t\t */\n\t\tshmem_falloc.next++;\n\t\tif (!PageUptodate(page))\n\t\t\tshmem_falloc.nr_falloced++;\n\n\t\t/*\n\t\t * If !PageUptodate, leave it that way so that freeable pages\n\t\t * can be recognized if we need to rollback on error later.\n\t\t * But set_page_dirty so that memory pressure will swap rather\n\t\t * than free the pages we are allocating (and SGP_CACHE pages\n\t\t * might still be clean: we now need to mark those dirty too).\n\t\t */\n\t\tset_page_dirty(page);\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tcond_resched();\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) && offset + len > inode->i_size)\n\t\ti_size_write(inode, offset + len);\n\tinode->i_ctime = CURRENT_TIME;\nundone:\n\tspin_lock(&inode->i_lock);\n\tinode->i_private = NULL;\n\tspin_unlock(&inode->i_lock);\nout:\n\tmutex_unlock(&inode->i_mutex);\n\treturn error;\n}\n\nstatic int shmem_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(dentry->d_sb);\n\n\tbuf->f_type = TMPFS_MAGIC;\n\tbuf->f_bsize = PAGE_CACHE_SIZE;\n\tbuf->f_namelen = NAME_MAX;\n\tif (sbinfo->max_blocks) {\n\t\tbuf->f_blocks = sbinfo->max_blocks;\n\t\tbuf->f_bavail =\n\t\tbuf->f_bfree  = sbinfo->max_blocks -\n\t\t\t\tpercpu_counter_sum(&sbinfo->used_blocks);\n\t}\n\tif (sbinfo->max_inodes) {\n\t\tbuf->f_files = sbinfo->max_inodes;\n\t\tbuf->f_ffree = sbinfo->free_inodes;\n\t}\n\t/* else leave those fields 0 like simple_statfs */\n\treturn 0;\n}\n\n/*\n * File creation. Allocate an inode, and we're done..\n */\nstatic int\nshmem_mknod(struct inode *dir, struct dentry *dentry, umode_t mode, dev_t dev)\n{\n\tstruct inode *inode;\n\tint error = -ENOSPC;\n\n\tinode = shmem_get_inode(dir->i_sb, dir, mode, dev, VM_NORESERVE);\n\tif (inode) {\n\t\terror = security_inode_init_security(inode, dir,\n\t\t\t\t\t\t     &dentry->d_name,\n\t\t\t\t\t\t     shmem_initxattrs, NULL);\n\t\tif (error) {\n\t\t\tif (error != -EOPNOTSUPP) {\n\t\t\t\tiput(inode);\n\t\t\t\treturn error;\n\t\t\t}\n\t\t}\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t\terror = generic_acl_init(inode, dir);\n\t\tif (error) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n#else\n\t\terror = 0;\n#endif\n\t\tdir->i_size += BOGO_DIRENT_SIZE;\n\t\tdir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\t\td_instantiate(dentry, inode);\n\t\tdget(dentry); /* Extra count - pin the dentry in core */\n\t}\n\treturn error;\n}\n\nstatic int shmem_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)\n{\n\tint error;\n\n\tif ((error = shmem_mknod(dir, dentry, mode | S_IFDIR, 0)))\n\t\treturn error;\n\tinc_nlink(dir);\n\treturn 0;\n}\n\nstatic int shmem_create(struct inode *dir, struct dentry *dentry, umode_t mode,\n\t\tbool excl)\n{\n\treturn shmem_mknod(dir, dentry, mode | S_IFREG, 0);\n}\n\n/*\n * Link a file..\n */\nstatic int shmem_link(struct dentry *old_dentry, struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = old_dentry->d_inode;\n\tint ret;\n\n\t/*\n\t * No ordinary (disk based) filesystem counts links as inodes;\n\t * but each new link needs a new dentry, pinning lowmem, and\n\t * tmpfs dentries cannot be pruned until they are unlinked.\n\t */\n\tret = shmem_reserve_inode(inode->i_sb);\n\tif (ret)\n\t\tgoto out;\n\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tinode->i_ctime = dir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\tinc_nlink(inode);\n\tihold(inode);\t/* New dentry reference */\n\tdget(dentry);\t\t/* Extra pinning count for the created dentry */\n\td_instantiate(dentry, inode);\nout:\n\treturn ret;\n}\n\nstatic int shmem_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = dentry->d_inode;\n\n\tif (inode->i_nlink > 1 && !S_ISDIR(inode->i_mode))\n\t\tshmem_free_inode(inode->i_sb);\n\n\tdir->i_size -= BOGO_DIRENT_SIZE;\n\tinode->i_ctime = dir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\tdrop_nlink(inode);\n\tdput(dentry);\t/* Undo the count from \"create\" - this does all the work */\n\treturn 0;\n}\n\nstatic int shmem_rmdir(struct inode *dir, struct dentry *dentry)\n{\n\tif (!simple_empty(dentry))\n\t\treturn -ENOTEMPTY;\n\n\tdrop_nlink(dentry->d_inode);\n\tdrop_nlink(dir);\n\treturn shmem_unlink(dir, dentry);\n}\n\n/*\n * The VFS layer already does all the dentry stuff for rename,\n * we just have to decrement the usage count for the target if\n * it exists so that the VFS layer correctly free's it when it\n * gets overwritten.\n */\nstatic int shmem_rename(struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry)\n{\n\tstruct inode *inode = old_dentry->d_inode;\n\tint they_are_dirs = S_ISDIR(inode->i_mode);\n\n\tif (!simple_empty(new_dentry))\n\t\treturn -ENOTEMPTY;\n\n\tif (new_dentry->d_inode) {\n\t\t(void) shmem_unlink(new_dir, new_dentry);\n\t\tif (they_are_dirs)\n\t\t\tdrop_nlink(old_dir);\n\t} else if (they_are_dirs) {\n\t\tdrop_nlink(old_dir);\n\t\tinc_nlink(new_dir);\n\t}\n\n\told_dir->i_size -= BOGO_DIRENT_SIZE;\n\tnew_dir->i_size += BOGO_DIRENT_SIZE;\n\told_dir->i_ctime = old_dir->i_mtime =\n\tnew_dir->i_ctime = new_dir->i_mtime =\n\tinode->i_ctime = CURRENT_TIME;\n\treturn 0;\n}\n\nstatic int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *symname)\n{\n\tint error;\n\tint len;\n\tstruct inode *inode;\n\tstruct page *page;\n\tchar *kaddr;\n\tstruct shmem_inode_info *info;\n\n\tlen = strlen(symname) + 1;\n\tif (len > PAGE_CACHE_SIZE)\n\t\treturn -ENAMETOOLONG;\n\n\tinode = shmem_get_inode(dir->i_sb, dir, S_IFLNK|S_IRWXUGO, 0, VM_NORESERVE);\n\tif (!inode)\n\t\treturn -ENOSPC;\n\n\terror = security_inode_init_security(inode, dir, &dentry->d_name,\n\t\t\t\t\t     shmem_initxattrs, NULL);\n\tif (error) {\n\t\tif (error != -EOPNOTSUPP) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t\terror = 0;\n\t}\n\n\tinfo = SHMEM_I(inode);\n\tinode->i_size = len-1;\n\tif (len <= SHORT_SYMLINK_LEN) {\n\t\tinfo->symlink = kmemdup(symname, len, GFP_KERNEL);\n\t\tif (!info->symlink) {\n\t\t\tiput(inode);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tinode->i_op = &shmem_short_symlink_operations;\n\t} else {\n\t\terror = shmem_getpage(inode, 0, &page, SGP_WRITE, NULL);\n\t\tif (error) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\tinode->i_op = &shmem_symlink_inode_operations;\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(kaddr, symname, len);\n\t\tkunmap_atomic(kaddr);\n\t\tSetPageUptodate(page);\n\t\tset_page_dirty(page);\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t}\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tdir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\td_instantiate(dentry, inode);\n\tdget(dentry);\n\treturn 0;\n}\n\nstatic void *shmem_follow_short_symlink(struct dentry *dentry, struct nameidata *nd)\n{\n\tnd_set_link(nd, SHMEM_I(dentry->d_inode)->symlink);\n\treturn NULL;\n}\n\nstatic void *shmem_follow_link(struct dentry *dentry, struct nameidata *nd)\n{\n\tstruct page *page = NULL;\n\tint error = shmem_getpage(dentry->d_inode, 0, &page, SGP_READ, NULL);\n\tnd_set_link(nd, error ? ERR_PTR(error) : kmap(page));\n\tif (page)\n\t\tunlock_page(page);\n\treturn page;\n}\n\nstatic void shmem_put_link(struct dentry *dentry, struct nameidata *nd, void *cookie)\n{\n\tif (!IS_ERR(nd_get_link(nd))) {\n\t\tstruct page *page = cookie;\n\t\tkunmap(page);\n\t\tmark_page_accessed(page);\n\t\tpage_cache_release(page);\n\t}\n}\n\n#ifdef CONFIG_TMPFS_XATTR\n/*\n * Superblocks without xattr inode operations may get some security.* xattr\n * support from the LSM \"for free\". As soon as we have any other xattrs\n * like ACLs, we also need to implement the security.* handlers at\n * filesystem level, though.\n */\n\n/*\n * Callback for security_inode_init_security() for acquiring xattrs.\n */\nstatic int shmem_initxattrs(struct inode *inode,\n\t\t\t    const struct xattr *xattr_array,\n\t\t\t    void *fs_info)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tconst struct xattr *xattr;\n\tstruct simple_xattr *new_xattr;\n\tsize_t len;\n\n\tfor (xattr = xattr_array; xattr->name != NULL; xattr++) {\n\t\tnew_xattr = simple_xattr_alloc(xattr->value, xattr->value_len);\n\t\tif (!new_xattr)\n\t\t\treturn -ENOMEM;\n\n\t\tlen = strlen(xattr->name) + 1;\n\t\tnew_xattr->name = kmalloc(XATTR_SECURITY_PREFIX_LEN + len,\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!new_xattr->name) {\n\t\t\tkfree(new_xattr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tmemcpy(new_xattr->name, XATTR_SECURITY_PREFIX,\n\t\t       XATTR_SECURITY_PREFIX_LEN);\n\t\tmemcpy(new_xattr->name + XATTR_SECURITY_PREFIX_LEN,\n\t\t       xattr->name, len);\n\n\t\tsimple_xattr_list_add(&info->xattrs, new_xattr);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct xattr_handler *shmem_xattr_handlers[] = {\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t&generic_acl_access_handler,\n\t&generic_acl_default_handler,\n#endif\n\tNULL\n};\n\nstatic int shmem_xattr_validate(const char *name)\n{\n\tstruct { const char *prefix; size_t len; } arr[] = {\n\t\t{ XATTR_SECURITY_PREFIX, XATTR_SECURITY_PREFIX_LEN },\n\t\t{ XATTR_TRUSTED_PREFIX, XATTR_TRUSTED_PREFIX_LEN }\n\t};\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(arr); i++) {\n\t\tsize_t preflen = arr[i].len;\n\t\tif (strncmp(name, arr[i].prefix, preflen) == 0) {\n\t\t\tif (!name[preflen])\n\t\t\t\treturn -EINVAL;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EOPNOTSUPP;\n}\n\nstatic ssize_t shmem_getxattr(struct dentry *dentry, const char *name,\n\t\t\t      void *buffer, size_t size)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(dentry->d_inode);\n\tint err;\n\n\t/*\n\t * If this is a request for a synthetic attribute in the system.*\n\t * namespace use the generic infrastructure to resolve a handler\n\t * for it via sb->s_xattr.\n\t */\n\tif (!strncmp(name, XATTR_SYSTEM_PREFIX, XATTR_SYSTEM_PREFIX_LEN))\n\t\treturn generic_getxattr(dentry, name, buffer, size);\n\n\terr = shmem_xattr_validate(name);\n\tif (err)\n\t\treturn err;\n\n\treturn simple_xattr_get(&info->xattrs, name, buffer, size);\n}\n\nstatic int shmem_setxattr(struct dentry *dentry, const char *name,\n\t\t\t  const void *value, size_t size, int flags)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(dentry->d_inode);\n\tint err;\n\n\t/*\n\t * If this is a request for a synthetic attribute in the system.*\n\t * namespace use the generic infrastructure to resolve a handler\n\t * for it via sb->s_xattr.\n\t */\n\tif (!strncmp(name, XATTR_SYSTEM_PREFIX, XATTR_SYSTEM_PREFIX_LEN))\n\t\treturn generic_setxattr(dentry, name, value, size, flags);\n\n\terr = shmem_xattr_validate(name);\n\tif (err)\n\t\treturn err;\n\n\treturn simple_xattr_set(&info->xattrs, name, value, size, flags);\n}\n\nstatic int shmem_removexattr(struct dentry *dentry, const char *name)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(dentry->d_inode);\n\tint err;\n\n\t/*\n\t * If this is a request for a synthetic attribute in the system.*\n\t * namespace use the generic infrastructure to resolve a handler\n\t * for it via sb->s_xattr.\n\t */\n\tif (!strncmp(name, XATTR_SYSTEM_PREFIX, XATTR_SYSTEM_PREFIX_LEN))\n\t\treturn generic_removexattr(dentry, name);\n\n\terr = shmem_xattr_validate(name);\n\tif (err)\n\t\treturn err;\n\n\treturn simple_xattr_remove(&info->xattrs, name);\n}\n\nstatic ssize_t shmem_listxattr(struct dentry *dentry, char *buffer, size_t size)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(dentry->d_inode);\n\treturn simple_xattr_list(&info->xattrs, buffer, size);\n}\n#endif /* CONFIG_TMPFS_XATTR */\n\nstatic const struct inode_operations shmem_short_symlink_operations = {\n\t.readlink\t= generic_readlink,\n\t.follow_link\t= shmem_follow_short_symlink,\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n};\n\nstatic const struct inode_operations shmem_symlink_inode_operations = {\n\t.readlink\t= generic_readlink,\n\t.follow_link\t= shmem_follow_link,\n\t.put_link\t= shmem_put_link,\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n};\n\nstatic struct dentry *shmem_get_parent(struct dentry *child)\n{\n\treturn ERR_PTR(-ESTALE);\n}\n\nstatic int shmem_match(struct inode *ino, void *vfh)\n{\n\t__u32 *fh = vfh;\n\t__u64 inum = fh[2];\n\tinum = (inum << 32) | fh[1];\n\treturn ino->i_ino == inum && fh[0] == ino->i_generation;\n}\n\nstatic struct dentry *shmem_fh_to_dentry(struct super_block *sb,\n\t\tstruct fid *fid, int fh_len, int fh_type)\n{\n\tstruct inode *inode;\n\tstruct dentry *dentry = NULL;\n\tu64 inum;\n\n\tif (fh_len < 3)\n\t\treturn NULL;\n\n\tinum = fid->raw[2];\n\tinum = (inum << 32) | fid->raw[1];\n\n\tinode = ilookup5(sb, (unsigned long)(inum + fid->raw[0]),\n\t\t\tshmem_match, fid->raw);\n\tif (inode) {\n\t\tdentry = d_find_alias(inode);\n\t\tiput(inode);\n\t}\n\n\treturn dentry;\n}\n\nstatic int shmem_encode_fh(struct inode *inode, __u32 *fh, int *len,\n\t\t\t\tstruct inode *parent)\n{\n\tif (*len < 3) {\n\t\t*len = 3;\n\t\treturn 255;\n\t}\n\n\tif (inode_unhashed(inode)) {\n\t\t/* Unfortunately insert_inode_hash is not idempotent,\n\t\t * so as we hash inodes here rather than at creation\n\t\t * time, we need a lock to ensure we only try\n\t\t * to do it once\n\t\t */\n\t\tstatic DEFINE_SPINLOCK(lock);\n\t\tspin_lock(&lock);\n\t\tif (inode_unhashed(inode))\n\t\t\t__insert_inode_hash(inode,\n\t\t\t\t\t    inode->i_ino + inode->i_generation);\n\t\tspin_unlock(&lock);\n\t}\n\n\tfh[0] = inode->i_generation;\n\tfh[1] = inode->i_ino;\n\tfh[2] = ((__u64)inode->i_ino) >> 32;\n\n\t*len = 3;\n\treturn 1;\n}\n\nstatic const struct export_operations shmem_export_ops = {\n\t.get_parent     = shmem_get_parent,\n\t.encode_fh      = shmem_encode_fh,\n\t.fh_to_dentry\t= shmem_fh_to_dentry,\n};\n\nstatic int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,\n\t\t\t       bool remount)\n{\n\tchar *this_char, *value, *rest;\n\tuid_t uid;\n\tgid_t gid;\n\n\twhile (options != NULL) {\n\t\tthis_char = options;\n\t\tfor (;;) {\n\t\t\t/*\n\t\t\t * NUL-terminate this option: unfortunately,\n\t\t\t * mount options form a comma-separated list,\n\t\t\t * but mpol's nodelist may also contain commas.\n\t\t\t */\n\t\t\toptions = strchr(options, ',');\n\t\t\tif (options == NULL)\n\t\t\t\tbreak;\n\t\t\toptions++;\n\t\t\tif (!isdigit(*options)) {\n\t\t\t\toptions[-1] = '\\0';\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!*this_char)\n\t\t\tcontinue;\n\t\tif ((value = strchr(this_char,'=')) != NULL) {\n\t\t\t*value++ = 0;\n\t\t} else {\n\t\t\tprintk(KERN_ERR\n\t\t\t    \"tmpfs: No value for mount option '%s'\\n\",\n\t\t\t    this_char);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (!strcmp(this_char,\"size\")) {\n\t\t\tunsigned long long size;\n\t\t\tsize = memparse(value,&rest);\n\t\t\tif (*rest == '%') {\n\t\t\t\tsize <<= PAGE_SHIFT;\n\t\t\t\tsize *= totalram_pages;\n\t\t\t\tdo_div(size, 100);\n\t\t\t\trest++;\n\t\t\t}\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t\tsbinfo->max_blocks =\n\t\t\t\tDIV_ROUND_UP(size, PAGE_CACHE_SIZE);\n\t\t} else if (!strcmp(this_char,\"nr_blocks\")) {\n\t\t\tsbinfo->max_blocks = memparse(value, &rest);\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"nr_inodes\")) {\n\t\t\tsbinfo->max_inodes = memparse(value, &rest);\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"mode\")) {\n\t\t\tif (remount)\n\t\t\t\tcontinue;\n\t\t\tsbinfo->mode = simple_strtoul(value, &rest, 8) & 07777;\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"uid\")) {\n\t\t\tif (remount)\n\t\t\t\tcontinue;\n\t\t\tuid = simple_strtoul(value, &rest, 0);\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t\tsbinfo->uid = make_kuid(current_user_ns(), uid);\n\t\t\tif (!uid_valid(sbinfo->uid))\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"gid\")) {\n\t\t\tif (remount)\n\t\t\t\tcontinue;\n\t\t\tgid = simple_strtoul(value, &rest, 0);\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t\tsbinfo->gid = make_kgid(current_user_ns(), gid);\n\t\t\tif (!gid_valid(sbinfo->gid))\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"mpol\")) {\n\t\t\tif (mpol_parse_str(value, &sbinfo->mpol))\n\t\t\t\tgoto bad_val;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"tmpfs: Bad mount option %s\\n\",\n\t\t\t       this_char);\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n\nbad_val:\n\tprintk(KERN_ERR \"tmpfs: Bad value '%s' for mount option '%s'\\n\",\n\t       value, this_char);\n\treturn 1;\n\n}\n\nstatic int shmem_remount_fs(struct super_block *sb, int *flags, char *data)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tstruct shmem_sb_info config = *sbinfo;\n\tunsigned long inodes;\n\tint error = -EINVAL;\n\n\tif (shmem_parse_options(data, &config, true))\n\t\treturn error;\n\n\tspin_lock(&sbinfo->stat_lock);\n\tinodes = sbinfo->max_inodes - sbinfo->free_inodes;\n\tif (percpu_counter_compare(&sbinfo->used_blocks, config.max_blocks) > 0)\n\t\tgoto out;\n\tif (config.max_inodes < inodes)\n\t\tgoto out;\n\t/*\n\t * Those tests disallow limited->unlimited while any are in use;\n\t * but we must separately disallow unlimited->limited, because\n\t * in that case we have no record of how much is already in use.\n\t */\n\tif (config.max_blocks && !sbinfo->max_blocks)\n\t\tgoto out;\n\tif (config.max_inodes && !sbinfo->max_inodes)\n\t\tgoto out;\n\n\terror = 0;\n\tsbinfo->max_blocks  = config.max_blocks;\n\tsbinfo->max_inodes  = config.max_inodes;\n\tsbinfo->free_inodes = config.max_inodes - inodes;\n\n\tmpol_put(sbinfo->mpol);\n\tsbinfo->mpol        = config.mpol;\t/* transfers initial ref */\nout:\n\tspin_unlock(&sbinfo->stat_lock);\n\treturn error;\n}\n\nstatic int shmem_show_options(struct seq_file *seq, struct dentry *root)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(root->d_sb);\n\n\tif (sbinfo->max_blocks != shmem_default_max_blocks())\n\t\tseq_printf(seq, \",size=%luk\",\n\t\t\tsbinfo->max_blocks << (PAGE_CACHE_SHIFT - 10));\n\tif (sbinfo->max_inodes != shmem_default_max_inodes())\n\t\tseq_printf(seq, \",nr_inodes=%lu\", sbinfo->max_inodes);\n\tif (sbinfo->mode != (S_IRWXUGO | S_ISVTX))\n\t\tseq_printf(seq, \",mode=%03ho\", sbinfo->mode);\n\tif (!uid_eq(sbinfo->uid, GLOBAL_ROOT_UID))\n\t\tseq_printf(seq, \",uid=%u\",\n\t\t\t\tfrom_kuid_munged(&init_user_ns, sbinfo->uid));\n\tif (!gid_eq(sbinfo->gid, GLOBAL_ROOT_GID))\n\t\tseq_printf(seq, \",gid=%u\",\n\t\t\t\tfrom_kgid_munged(&init_user_ns, sbinfo->gid));\n\tshmem_show_mpol(seq, sbinfo->mpol);\n\treturn 0;\n}\n#endif /* CONFIG_TMPFS */\n\nstatic void shmem_put_super(struct super_block *sb)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\n\tpercpu_counter_destroy(&sbinfo->used_blocks);\n\tkfree(sbinfo);\n\tsb->s_fs_info = NULL;\n}\n\nint shmem_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct inode *inode;\n\tstruct shmem_sb_info *sbinfo;\n\tint err = -ENOMEM;\n\n\t/* Round up to L1_CACHE_BYTES to resist false sharing */\n\tsbinfo = kzalloc(max((int)sizeof(struct shmem_sb_info),\n\t\t\t\tL1_CACHE_BYTES), GFP_KERNEL);\n\tif (!sbinfo)\n\t\treturn -ENOMEM;\n\n\tsbinfo->mode = S_IRWXUGO | S_ISVTX;\n\tsbinfo->uid = current_fsuid();\n\tsbinfo->gid = current_fsgid();\n\tsb->s_fs_info = sbinfo;\n\n#ifdef CONFIG_TMPFS\n\t/*\n\t * Per default we only allow half of the physical ram per\n\t * tmpfs instance, limiting inodes to one per page of lowmem;\n\t * but the internal instance is left unlimited.\n\t */\n\tif (!(sb->s_flags & MS_NOUSER)) {\n\t\tsbinfo->max_blocks = shmem_default_max_blocks();\n\t\tsbinfo->max_inodes = shmem_default_max_inodes();\n\t\tif (shmem_parse_options(data, sbinfo, false)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto failed;\n\t\t}\n\t}\n\tsb->s_export_op = &shmem_export_ops;\n\tsb->s_flags |= MS_NOSEC;\n#else\n\tsb->s_flags |= MS_NOUSER;\n#endif\n\n\tspin_lock_init(&sbinfo->stat_lock);\n\tif (percpu_counter_init(&sbinfo->used_blocks, 0))\n\t\tgoto failed;\n\tsbinfo->free_inodes = sbinfo->max_inodes;\n\n\tsb->s_maxbytes = MAX_LFS_FILESIZE;\n\tsb->s_blocksize = PAGE_CACHE_SIZE;\n\tsb->s_blocksize_bits = PAGE_CACHE_SHIFT;\n\tsb->s_magic = TMPFS_MAGIC;\n\tsb->s_op = &shmem_ops;\n\tsb->s_time_gran = 1;\n#ifdef CONFIG_TMPFS_XATTR\n\tsb->s_xattr = shmem_xattr_handlers;\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\tsb->s_flags |= MS_POSIXACL;\n#endif\n\n\tinode = shmem_get_inode(sb, NULL, S_IFDIR | sbinfo->mode, 0, VM_NORESERVE);\n\tif (!inode)\n\t\tgoto failed;\n\tinode->i_uid = sbinfo->uid;\n\tinode->i_gid = sbinfo->gid;\n\tsb->s_root = d_make_root(inode);\n\tif (!sb->s_root)\n\t\tgoto failed;\n\treturn 0;\n\nfailed:\n\tshmem_put_super(sb);\n\treturn err;\n}\n\nstatic struct kmem_cache *shmem_inode_cachep;\n\nstatic struct inode *shmem_alloc_inode(struct super_block *sb)\n{\n\tstruct shmem_inode_info *info;\n\tinfo = kmem_cache_alloc(shmem_inode_cachep, GFP_KERNEL);\n\tif (!info)\n\t\treturn NULL;\n\treturn &info->vfs_inode;\n}\n\nstatic void shmem_destroy_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(shmem_inode_cachep, SHMEM_I(inode));\n}\n\nstatic void shmem_destroy_inode(struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode))\n\t\tmpol_free_shared_policy(&SHMEM_I(inode)->policy);\n\tcall_rcu(&inode->i_rcu, shmem_destroy_callback);\n}\n\nstatic void shmem_init_inode(void *foo)\n{\n\tstruct shmem_inode_info *info = foo;\n\tinode_init_once(&info->vfs_inode);\n}\n\nstatic int shmem_init_inodecache(void)\n{\n\tshmem_inode_cachep = kmem_cache_create(\"shmem_inode_cache\",\n\t\t\t\tsizeof(struct shmem_inode_info),\n\t\t\t\t0, SLAB_PANIC, shmem_init_inode);\n\treturn 0;\n}\n\nstatic void shmem_destroy_inodecache(void)\n{\n\tkmem_cache_destroy(shmem_inode_cachep);\n}\n\nstatic const struct address_space_operations shmem_aops = {\n\t.writepage\t= shmem_writepage,\n\t.set_page_dirty\t= __set_page_dirty_no_writeback,\n#ifdef CONFIG_TMPFS\n\t.write_begin\t= shmem_write_begin,\n\t.write_end\t= shmem_write_end,\n#endif\n\t.migratepage\t= migrate_page,\n\t.error_remove_page = generic_error_remove_page,\n};\n\nstatic const struct file_operations shmem_file_operations = {\n\t.mmap\t\t= shmem_mmap,\n#ifdef CONFIG_TMPFS\n\t.llseek\t\t= shmem_file_llseek,\n\t.read\t\t= do_sync_read,\n\t.write\t\t= do_sync_write,\n\t.aio_read\t= shmem_file_aio_read,\n\t.aio_write\t= generic_file_aio_write,\n\t.fsync\t\t= noop_fsync,\n\t.splice_read\t= shmem_file_splice_read,\n\t.splice_write\t= generic_file_splice_write,\n\t.fallocate\t= shmem_fallocate,\n#endif\n};\n\nstatic const struct inode_operations shmem_inode_operations = {\n\t.setattr\t= shmem_setattr,\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n};\n\nstatic const struct inode_operations shmem_dir_inode_operations = {\n#ifdef CONFIG_TMPFS\n\t.create\t\t= shmem_create,\n\t.lookup\t\t= simple_lookup,\n\t.link\t\t= shmem_link,\n\t.unlink\t\t= shmem_unlink,\n\t.symlink\t= shmem_symlink,\n\t.mkdir\t\t= shmem_mkdir,\n\t.rmdir\t\t= shmem_rmdir,\n\t.mknod\t\t= shmem_mknod,\n\t.rename\t\t= shmem_rename,\n#endif\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t.setattr\t= shmem_setattr,\n#endif\n};\n\nstatic const struct inode_operations shmem_special_inode_operations = {\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t.setattr\t= shmem_setattr,\n#endif\n};\n\nstatic const struct super_operations shmem_ops = {\n\t.alloc_inode\t= shmem_alloc_inode,\n\t.destroy_inode\t= shmem_destroy_inode,\n#ifdef CONFIG_TMPFS\n\t.statfs\t\t= shmem_statfs,\n\t.remount_fs\t= shmem_remount_fs,\n\t.show_options\t= shmem_show_options,\n#endif\n\t.evict_inode\t= shmem_evict_inode,\n\t.drop_inode\t= generic_delete_inode,\n\t.put_super\t= shmem_put_super,\n};\n\nstatic const struct vm_operations_struct shmem_vm_ops = {\n\t.fault\t\t= shmem_fault,\n#ifdef CONFIG_NUMA\n\t.set_policy     = shmem_set_policy,\n\t.get_policy     = shmem_get_policy,\n#endif\n\t.remap_pages\t= generic_file_remap_pages,\n};\n\nstatic struct dentry *shmem_mount(struct file_system_type *fs_type,\n\tint flags, const char *dev_name, void *data)\n{\n\treturn mount_nodev(fs_type, flags, data, shmem_fill_super);\n}\n\nstatic struct file_system_type shmem_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"tmpfs\",\n\t.mount\t\t= shmem_mount,\n\t.kill_sb\t= kill_litter_super,\n};\n\nint __init shmem_init(void)\n{\n\tint error;\n\n\terror = bdi_init(&shmem_backing_dev_info);\n\tif (error)\n\t\tgoto out4;\n\n\terror = shmem_init_inodecache();\n\tif (error)\n\t\tgoto out3;\n\n\terror = register_filesystem(&shmem_fs_type);\n\tif (error) {\n\t\tprintk(KERN_ERR \"Could not register tmpfs\\n\");\n\t\tgoto out2;\n\t}\n\n\tshm_mnt = vfs_kern_mount(&shmem_fs_type, MS_NOUSER,\n\t\t\t\t shmem_fs_type.name, NULL);\n\tif (IS_ERR(shm_mnt)) {\n\t\terror = PTR_ERR(shm_mnt);\n\t\tprintk(KERN_ERR \"Could not kern_mount tmpfs\\n\");\n\t\tgoto out1;\n\t}\n\treturn 0;\n\nout1:\n\tunregister_filesystem(&shmem_fs_type);\nout2:\n\tshmem_destroy_inodecache();\nout3:\n\tbdi_destroy(&shmem_backing_dev_info);\nout4:\n\tshm_mnt = ERR_PTR(error);\n\treturn error;\n}\n\n#else /* !CONFIG_SHMEM */\n\n/*\n * tiny-shmem: simple shmemfs and tmpfs using ramfs code\n *\n * This is intended for small system where the benefits of the full\n * shmem code (swap-backed and resource-limited) are outweighed by\n * their complexity. On systems without swap this code should be\n * effectively equivalent, but much lighter weight.\n */\n\n#include <linux/ramfs.h>\n\nstatic struct file_system_type shmem_fs_type = {\n\t.name\t\t= \"tmpfs\",\n\t.mount\t\t= ramfs_mount,\n\t.kill_sb\t= kill_litter_super,\n};\n\nint __init shmem_init(void)\n{\n\tBUG_ON(register_filesystem(&shmem_fs_type) != 0);\n\n\tshm_mnt = kern_mount(&shmem_fs_type);\n\tBUG_ON(IS_ERR(shm_mnt));\n\n\treturn 0;\n}\n\nint shmem_unuse(swp_entry_t swap, struct page *page)\n{\n\treturn 0;\n}\n\nint shmem_lock(struct file *file, int lock, struct user_struct *user)\n{\n\treturn 0;\n}\n\nvoid shmem_unlock_mapping(struct address_space *mapping)\n{\n}\n\nvoid shmem_truncate_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\ttruncate_inode_pages_range(inode->i_mapping, lstart, lend);\n}\nEXPORT_SYMBOL_GPL(shmem_truncate_range);\n\n#define shmem_vm_ops\t\t\t\tgeneric_file_vm_ops\n#define shmem_file_operations\t\t\tramfs_file_operations\n#define shmem_get_inode(sb, dir, mode, dev, flags)\tramfs_get_inode(sb, dir, mode, dev)\n#define shmem_acct_size(flags, size)\t\t0\n#define shmem_unacct_size(flags, size)\t\tdo {} while (0)\n\n#endif /* CONFIG_SHMEM */\n\n/* common code */\n\n/**\n * shmem_file_setup - get an unlinked file living in tmpfs\n * @name: name for dentry (to be seen in /proc/<pid>/maps\n * @size: size to be set for the file\n * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size\n */\nstruct file *shmem_file_setup(const char *name, loff_t size, unsigned long flags)\n{\n\tint error;\n\tstruct file *file;\n\tstruct inode *inode;\n\tstruct path path;\n\tstruct dentry *root;\n\tstruct qstr this;\n\n\tif (IS_ERR(shm_mnt))\n\t\treturn (void *)shm_mnt;\n\n\tif (size < 0 || size > MAX_LFS_FILESIZE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (shmem_acct_size(flags, size))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terror = -ENOMEM;\n\tthis.name = name;\n\tthis.len = strlen(name);\n\tthis.hash = 0; /* will go */\n\troot = shm_mnt->mnt_root;\n\tpath.dentry = d_alloc(root, &this);\n\tif (!path.dentry)\n\t\tgoto put_memory;\n\tpath.mnt = mntget(shm_mnt);\n\n\terror = -ENOSPC;\n\tinode = shmem_get_inode(root->d_sb, NULL, S_IFREG | S_IRWXUGO, 0, flags);\n\tif (!inode)\n\t\tgoto put_dentry;\n\n\td_instantiate(path.dentry, inode);\n\tinode->i_size = size;\n\tclear_nlink(inode);\t/* It is unlinked */\n#ifndef CONFIG_MMU\n\terror = ramfs_nommu_expand_for_mapping(inode, size);\n\tif (error)\n\t\tgoto put_dentry;\n#endif\n\n\terror = -ENFILE;\n\tfile = alloc_file(&path, FMODE_WRITE | FMODE_READ,\n\t\t  &shmem_file_operations);\n\tif (!file)\n\t\tgoto put_dentry;\n\n\treturn file;\n\nput_dentry:\n\tpath_put(&path);\nput_memory:\n\tshmem_unacct_size(flags, size);\n\treturn ERR_PTR(error);\n}\nEXPORT_SYMBOL_GPL(shmem_file_setup);\n\n/**\n * shmem_zero_setup - setup a shared anonymous mapping\n * @vma: the vma to be mmapped is prepared by do_mmap_pgoff\n */\nint shmem_zero_setup(struct vm_area_struct *vma)\n{\n\tstruct file *file;\n\tloff_t size = vma->vm_end - vma->vm_start;\n\n\tfile = shmem_file_setup(\"dev/zero\", size, vma->vm_flags);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tvma->vm_file = file;\n\tvma->vm_ops = &shmem_vm_ops;\n\treturn 0;\n}\n\n/**\n * shmem_read_mapping_page_gfp - read into page cache, using specified page allocation flags.\n * @mapping:\tthe page's address_space\n * @index:\tthe page index\n * @gfp:\tthe page allocator flags to use if allocating\n *\n * This behaves as a tmpfs \"read_cache_page_gfp(mapping, index, gfp)\",\n * with any new page allocations done using the specified allocation flags.\n * But read_cache_page_gfp() uses the ->readpage() method: which does not\n * suit tmpfs, since it may have pages in swapcache, and needs to find those\n * for itself; although drivers/gpu/drm i915 and ttm rely upon this support.\n *\n * i915_gem_object_get_pages_gtt() mixes __GFP_NORETRY | __GFP_NOWARN in\n * with the mapping_gfp_mask(), to avoid OOMing the machine unnecessarily.\n */\nstruct page *shmem_read_mapping_page_gfp(struct address_space *mapping,\n\t\t\t\t\t pgoff_t index, gfp_t gfp)\n{\n#ifdef CONFIG_SHMEM\n\tstruct inode *inode = mapping->host;\n\tstruct page *page;\n\tint error;\n\n\tBUG_ON(mapping->a_ops != &shmem_aops);\n\terror = shmem_getpage_gfp(inode, index, &page, SGP_CACHE, gfp, NULL);\n\tif (error)\n\t\tpage = ERR_PTR(error);\n\telse\n\t\tunlock_page(page);\n\treturn page;\n#else\n\t/*\n\t * The tiny !SHMEM case uses ramfs without swap\n\t */\n\treturn read_cache_page_gfp(mapping, index, gfp);\n#endif\n}\nEXPORT_SYMBOL_GPL(shmem_read_mapping_page_gfp);\n"], "fixing_code": ["/*\n * Resizable virtual memory filesystem for Linux.\n *\n * Copyright (C) 2000 Linus Torvalds.\n *\t\t 2000 Transmeta Corp.\n *\t\t 2000-2001 Christoph Rohland\n *\t\t 2000-2001 SAP AG\n *\t\t 2002 Red Hat Inc.\n * Copyright (C) 2002-2011 Hugh Dickins.\n * Copyright (C) 2011 Google Inc.\n * Copyright (C) 2002-2005 VERITAS Software Corporation.\n * Copyright (C) 2004 Andi Kleen, SuSE Labs\n *\n * Extended attribute support for tmpfs:\n * Copyright (c) 2004, Luke Kenneth Casson Leighton <lkcl@lkcl.net>\n * Copyright (c) 2004 Red Hat, Inc., James Morris <jmorris@redhat.com>\n *\n * tiny-shmem:\n * Copyright (c) 2004, 2008 Matt Mackall <mpm@selenic.com>\n *\n * This file is released under the GPL.\n */\n\n#include <linux/fs.h>\n#include <linux/init.h>\n#include <linux/vfs.h>\n#include <linux/mount.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n\nstatic struct vfsmount *shm_mnt;\n\n#ifdef CONFIG_SHMEM\n/*\n * This virtual memory filesystem is heavily based on the ramfs. It\n * extends ramfs by the ability to use swap and honor resource limits\n * which makes it a completely usable filesystem.\n */\n\n#include <linux/xattr.h>\n#include <linux/exportfs.h>\n#include <linux/posix_acl.h>\n#include <linux/generic_acl.h>\n#include <linux/mman.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/backing-dev.h>\n#include <linux/shmem_fs.h>\n#include <linux/writeback.h>\n#include <linux/blkdev.h>\n#include <linux/pagevec.h>\n#include <linux/percpu_counter.h>\n#include <linux/falloc.h>\n#include <linux/splice.h>\n#include <linux/security.h>\n#include <linux/swapops.h>\n#include <linux/mempolicy.h>\n#include <linux/namei.h>\n#include <linux/ctype.h>\n#include <linux/migrate.h>\n#include <linux/highmem.h>\n#include <linux/seq_file.h>\n#include <linux/magic.h>\n\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n\n#define BLOCKS_PER_PAGE  (PAGE_CACHE_SIZE/512)\n#define VM_ACCT(size)    (PAGE_CACHE_ALIGN(size) >> PAGE_SHIFT)\n\n/* Pretend that each entry is of this size in directory's i_size */\n#define BOGO_DIRENT_SIZE 20\n\n/* Symlink up to this size is kmalloc'ed instead of using a swappable page */\n#define SHORT_SYMLINK_LEN 128\n\n/*\n * shmem_fallocate and shmem_writepage communicate via inode->i_private\n * (with i_mutex making sure that it has only one user at a time):\n * we would prefer not to enlarge the shmem inode just for that.\n */\nstruct shmem_falloc {\n\tpgoff_t start;\t\t/* start of range currently being fallocated */\n\tpgoff_t next;\t\t/* the next page offset to be fallocated */\n\tpgoff_t nr_falloced;\t/* how many new pages have been fallocated */\n\tpgoff_t nr_unswapped;\t/* how often writepage refused to swap out */\n};\n\n/* Flag allocation requirements to shmem_getpage */\nenum sgp_type {\n\tSGP_READ,\t/* don't exceed i_size, don't allocate page */\n\tSGP_CACHE,\t/* don't exceed i_size, may allocate page */\n\tSGP_DIRTY,\t/* like SGP_CACHE, but set new page dirty */\n\tSGP_WRITE,\t/* may exceed i_size, may allocate !Uptodate page */\n\tSGP_FALLOC,\t/* like SGP_WRITE, but make existing page Uptodate */\n};\n\n#ifdef CONFIG_TMPFS\nstatic unsigned long shmem_default_max_blocks(void)\n{\n\treturn totalram_pages / 2;\n}\n\nstatic unsigned long shmem_default_max_inodes(void)\n{\n\treturn min(totalram_pages - totalhigh_pages, totalram_pages / 2);\n}\n#endif\n\nstatic bool shmem_should_replace_page(struct page *page, gfp_t gfp);\nstatic int shmem_replace_page(struct page **pagep, gfp_t gfp,\n\t\t\t\tstruct shmem_inode_info *info, pgoff_t index);\nstatic int shmem_getpage_gfp(struct inode *inode, pgoff_t index,\n\tstruct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type);\n\nstatic inline int shmem_getpage(struct inode *inode, pgoff_t index,\n\tstruct page **pagep, enum sgp_type sgp, int *fault_type)\n{\n\treturn shmem_getpage_gfp(inode, index, pagep, sgp,\n\t\t\tmapping_gfp_mask(inode->i_mapping), fault_type);\n}\n\nstatic inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\n/*\n * shmem_file_setup pre-accounts the whole fixed size of a VM object,\n * for shared memory and for shared anonymous (/dev/zero) mappings\n * (unless MAP_NORESERVE and sysctl_overcommit_memory <= 1),\n * consistent with the pre-accounting of private mappings ...\n */\nstatic inline int shmem_acct_size(unsigned long flags, loff_t size)\n{\n\treturn (flags & VM_NORESERVE) ?\n\t\t0 : security_vm_enough_memory_mm(current->mm, VM_ACCT(size));\n}\n\nstatic inline void shmem_unacct_size(unsigned long flags, loff_t size)\n{\n\tif (!(flags & VM_NORESERVE))\n\t\tvm_unacct_memory(VM_ACCT(size));\n}\n\n/*\n * ... whereas tmpfs objects are accounted incrementally as\n * pages are allocated, in order to allow huge sparse files.\n * shmem_getpage reports shmem_acct_block failure as -ENOSPC not -ENOMEM,\n * so that a failure on a sparse tmpfs mapping will give SIGBUS not OOM.\n */\nstatic inline int shmem_acct_block(unsigned long flags)\n{\n\treturn (flags & VM_NORESERVE) ?\n\t\tsecurity_vm_enough_memory_mm(current->mm, VM_ACCT(PAGE_CACHE_SIZE)) : 0;\n}\n\nstatic inline void shmem_unacct_blocks(unsigned long flags, long pages)\n{\n\tif (flags & VM_NORESERVE)\n\t\tvm_unacct_memory(pages * VM_ACCT(PAGE_CACHE_SIZE));\n}\n\nstatic const struct super_operations shmem_ops;\nstatic const struct address_space_operations shmem_aops;\nstatic const struct file_operations shmem_file_operations;\nstatic const struct inode_operations shmem_inode_operations;\nstatic const struct inode_operations shmem_dir_inode_operations;\nstatic const struct inode_operations shmem_special_inode_operations;\nstatic const struct vm_operations_struct shmem_vm_ops;\n\nstatic struct backing_dev_info shmem_backing_dev_info  __read_mostly = {\n\t.ra_pages\t= 0,\t/* No readahead */\n\t.capabilities\t= BDI_CAP_NO_ACCT_AND_WRITEBACK | BDI_CAP_SWAP_BACKED,\n};\n\nstatic LIST_HEAD(shmem_swaplist);\nstatic DEFINE_MUTEX(shmem_swaplist_mutex);\n\nstatic int shmem_reserve_inode(struct super_block *sb)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tif (sbinfo->max_inodes) {\n\t\tspin_lock(&sbinfo->stat_lock);\n\t\tif (!sbinfo->free_inodes) {\n\t\t\tspin_unlock(&sbinfo->stat_lock);\n\t\t\treturn -ENOSPC;\n\t\t}\n\t\tsbinfo->free_inodes--;\n\t\tspin_unlock(&sbinfo->stat_lock);\n\t}\n\treturn 0;\n}\n\nstatic void shmem_free_inode(struct super_block *sb)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tif (sbinfo->max_inodes) {\n\t\tspin_lock(&sbinfo->stat_lock);\n\t\tsbinfo->free_inodes++;\n\t\tspin_unlock(&sbinfo->stat_lock);\n\t}\n}\n\n/**\n * shmem_recalc_inode - recalculate the block usage of an inode\n * @inode: inode to recalc\n *\n * We have to calculate the free blocks since the mm can drop\n * undirtied hole pages behind our back.\n *\n * But normally   info->alloced == inode->i_mapping->nrpages + info->swapped\n * So mm freed is info->alloced - (inode->i_mapping->nrpages + info->swapped)\n *\n * It has to be called with the spinlock held.\n */\nstatic void shmem_recalc_inode(struct inode *inode)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tlong freed;\n\n\tfreed = info->alloced - info->swapped - inode->i_mapping->nrpages;\n\tif (freed > 0) {\n\t\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\t\tif (sbinfo->max_blocks)\n\t\t\tpercpu_counter_add(&sbinfo->used_blocks, -freed);\n\t\tinfo->alloced -= freed;\n\t\tinode->i_blocks -= freed * BLOCKS_PER_PAGE;\n\t\tshmem_unacct_blocks(info->flags, freed);\n\t}\n}\n\n/*\n * Replace item expected in radix tree by a new item, while holding tree lock.\n */\nstatic int shmem_radix_tree_replace(struct address_space *mapping,\n\t\t\tpgoff_t index, void *expected, void *replacement)\n{\n\tvoid **pslot;\n\tvoid *item = NULL;\n\n\tVM_BUG_ON(!expected);\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree, index);\n\tif (pslot)\n\t\titem = radix_tree_deref_slot_protected(pslot,\n\t\t\t\t\t\t\t&mapping->tree_lock);\n\tif (item != expected)\n\t\treturn -ENOENT;\n\tif (replacement)\n\t\tradix_tree_replace_slot(pslot, replacement);\n\telse\n\t\tradix_tree_delete(&mapping->page_tree, index);\n\treturn 0;\n}\n\n/*\n * Sometimes, before we decide whether to proceed or to fail, we must check\n * that an entry was not already brought back from swap by a racing thread.\n *\n * Checking page is not enough: by the time a SwapCache page is locked, it\n * might be reused, and again be SwapCache, using the same swap as before.\n */\nstatic bool shmem_confirm_swap(struct address_space *mapping,\n\t\t\t       pgoff_t index, swp_entry_t swap)\n{\n\tvoid *item;\n\n\trcu_read_lock();\n\titem = radix_tree_lookup(&mapping->page_tree, index);\n\trcu_read_unlock();\n\treturn item == swp_to_radix_entry(swap);\n}\n\n/*\n * Like add_to_page_cache_locked, but error if expected item has gone.\n */\nstatic int shmem_add_to_page_cache(struct page *page,\n\t\t\t\t   struct address_space *mapping,\n\t\t\t\t   pgoff_t index, gfp_t gfp, void *expected)\n{\n\tint error;\n\n\tVM_BUG_ON(!PageLocked(page));\n\tVM_BUG_ON(!PageSwapBacked(page));\n\n\tpage_cache_get(page);\n\tpage->mapping = mapping;\n\tpage->index = index;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\tif (!expected)\n\t\terror = radix_tree_insert(&mapping->page_tree, index, page);\n\telse\n\t\terror = shmem_radix_tree_replace(mapping, index, expected,\n\t\t\t\t\t\t\t\t page);\n\tif (!error) {\n\t\tmapping->nrpages++;\n\t\t__inc_zone_page_state(page, NR_FILE_PAGES);\n\t\t__inc_zone_page_state(page, NR_SHMEM);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t} else {\n\t\tpage->mapping = NULL;\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\tpage_cache_release(page);\n\t}\n\treturn error;\n}\n\n/*\n * Like delete_from_page_cache, but substitutes swap for page.\n */\nstatic void shmem_delete_from_page_cache(struct page *page, void *radswap)\n{\n\tstruct address_space *mapping = page->mapping;\n\tint error;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\terror = shmem_radix_tree_replace(mapping, page->index, page, radswap);\n\tpage->mapping = NULL;\n\tmapping->nrpages--;\n\t__dec_zone_page_state(page, NR_FILE_PAGES);\n\t__dec_zone_page_state(page, NR_SHMEM);\n\tspin_unlock_irq(&mapping->tree_lock);\n\tpage_cache_release(page);\n\tBUG_ON(error);\n}\n\n/*\n * Like find_get_pages, but collecting swap entries as well as pages.\n */\nstatic unsigned shmem_find_get_pages_and_swap(struct address_space *mapping,\n\t\t\t\t\tpgoff_t start, unsigned int nr_pages,\n\t\t\t\t\tstruct page **pages, pgoff_t *indices)\n{\n\tvoid **slot;\n\tunsigned int ret = 0;\n\tstruct radix_tree_iter iter;\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\trcu_read_lock();\nrestart:\n\tradix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {\n\t\tstruct page *page;\nrepeat:\n\t\tpage = radix_tree_deref_slot(slot);\n\t\tif (unlikely(!page))\n\t\t\tcontinue;\n\t\tif (radix_tree_exception(page)) {\n\t\t\tif (radix_tree_deref_retry(page))\n\t\t\t\tgoto restart;\n\t\t\t/*\n\t\t\t * Otherwise, we must be storing a swap entry\n\t\t\t * here as an exceptional entry: so return it\n\t\t\t * without attempting to raise page count.\n\t\t\t */\n\t\t\tgoto export;\n\t\t}\n\t\tif (!page_cache_get_speculative(page))\n\t\t\tgoto repeat;\n\n\t\t/* Has the page moved? */\n\t\tif (unlikely(page != *slot)) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto repeat;\n\t\t}\nexport:\n\t\tindices[ret] = iter.index;\n\t\tpages[ret] = page;\n\t\tif (++ret == nr_pages)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/*\n * Remove swap entry from radix tree, free the swap and its page cache.\n */\nstatic int shmem_free_swap(struct address_space *mapping,\n\t\t\t   pgoff_t index, void *radswap)\n{\n\tint error;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\terror = shmem_radix_tree_replace(mapping, index, radswap, NULL);\n\tspin_unlock_irq(&mapping->tree_lock);\n\tif (!error)\n\t\tfree_swap_and_cache(radix_to_swp_entry(radswap));\n\treturn error;\n}\n\n/*\n * Pagevec may contain swap entries, so shuffle up pages before releasing.\n */\nstatic void shmem_deswap_pagevec(struct pagevec *pvec)\n{\n\tint i, j;\n\n\tfor (i = 0, j = 0; i < pagevec_count(pvec); i++) {\n\t\tstruct page *page = pvec->pages[i];\n\t\tif (!radix_tree_exceptional_entry(page))\n\t\t\tpvec->pages[j++] = page;\n\t}\n\tpvec->nr = j;\n}\n\n/*\n * SysV IPC SHM_UNLOCK restore Unevictable pages to their evictable lists.\n */\nvoid shmem_unlock_mapping(struct address_space *mapping)\n{\n\tstruct pagevec pvec;\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tpgoff_t index = 0;\n\n\tpagevec_init(&pvec, 0);\n\t/*\n\t * Minor point, but we might as well stop if someone else SHM_LOCKs it.\n\t */\n\twhile (!mapping_unevictable(mapping)) {\n\t\t/*\n\t\t * Avoid pagevec_lookup(): find_get_pages() returns 0 as if it\n\t\t * has finished, if it hits a row of PAGEVEC_SIZE swap entries.\n\t\t */\n\t\tpvec.nr = shmem_find_get_pages_and_swap(mapping, index,\n\t\t\t\t\tPAGEVEC_SIZE, pvec.pages, indices);\n\t\tif (!pvec.nr)\n\t\t\tbreak;\n\t\tindex = indices[pvec.nr - 1] + 1;\n\t\tshmem_deswap_pagevec(&pvec);\n\t\tcheck_move_unevictable_pages(pvec.pages, pvec.nr);\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n}\n\n/*\n * Remove range of pages and swap entries from radix tree, and free them.\n * If !unfalloc, truncate or punch hole; if unfalloc, undo failed fallocate.\n */\nstatic void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,\n\t\t\t\t\t\t\t\t bool unfalloc)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tpgoff_t start = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\tpgoff_t end = (lend + 1) >> PAGE_CACHE_SHIFT;\n\tunsigned int partial_start = lstart & (PAGE_CACHE_SIZE - 1);\n\tunsigned int partial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);\n\tstruct pagevec pvec;\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tlong nr_swaps_freed = 0;\n\tpgoff_t index;\n\tint i;\n\n\tif (lend == -1)\n\t\tend = -1;\t/* unsigned, so actually very big */\n\n\tpagevec_init(&pvec, 0);\n\tindex = start;\n\twhile (index < end) {\n\t\tpvec.nr = shmem_find_get_pages_and_swap(mapping, index,\n\t\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE),\n\t\t\t\t\t\t\tpvec.pages, indices);\n\t\tif (!pvec.nr)\n\t\t\tbreak;\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tindex = indices[i];\n\t\t\tif (index >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (radix_tree_exceptional_entry(page)) {\n\t\t\t\tif (unfalloc)\n\t\t\t\t\tcontinue;\n\t\t\t\tnr_swaps_freed += !shmem_free_swap(mapping,\n\t\t\t\t\t\t\t\tindex, page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!trylock_page(page))\n\t\t\t\tcontinue;\n\t\t\tif (!unfalloc || !PageUptodate(page)) {\n\t\t\t\tif (page->mapping == mapping) {\n\t\t\t\t\tVM_BUG_ON(PageWriteback(page));\n\t\t\t\t\ttruncate_inode_page(mapping, page);\n\t\t\t\t}\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t}\n\t\tshmem_deswap_pagevec(&pvec);\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tcond_resched();\n\t\tindex++;\n\t}\n\n\tif (partial_start) {\n\t\tstruct page *page = NULL;\n\t\tshmem_getpage(inode, start - 1, &page, SGP_READ, NULL);\n\t\tif (page) {\n\t\t\tunsigned int top = PAGE_CACHE_SIZE;\n\t\t\tif (start > end) {\n\t\t\t\ttop = partial_end;\n\t\t\t\tpartial_end = 0;\n\t\t\t}\n\t\t\tzero_user_segment(page, partial_start, top);\n\t\t\tset_page_dirty(page);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\tif (partial_end) {\n\t\tstruct page *page = NULL;\n\t\tshmem_getpage(inode, end, &page, SGP_READ, NULL);\n\t\tif (page) {\n\t\t\tzero_user_segment(page, 0, partial_end);\n\t\t\tset_page_dirty(page);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\tif (start >= end)\n\t\treturn;\n\n\tindex = start;\n\tfor ( ; ; ) {\n\t\tcond_resched();\n\t\tpvec.nr = shmem_find_get_pages_and_swap(mapping, index,\n\t\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE),\n\t\t\t\t\t\t\tpvec.pages, indices);\n\t\tif (!pvec.nr) {\n\t\t\tif (index == start || unfalloc)\n\t\t\t\tbreak;\n\t\t\tindex = start;\n\t\t\tcontinue;\n\t\t}\n\t\tif ((index == start || unfalloc) && indices[0] >= end) {\n\t\t\tshmem_deswap_pagevec(&pvec);\n\t\t\tpagevec_release(&pvec);\n\t\t\tbreak;\n\t\t}\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tindex = indices[i];\n\t\t\tif (index >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (radix_tree_exceptional_entry(page)) {\n\t\t\t\tif (unfalloc)\n\t\t\t\t\tcontinue;\n\t\t\t\tnr_swaps_freed += !shmem_free_swap(mapping,\n\t\t\t\t\t\t\t\tindex, page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tlock_page(page);\n\t\t\tif (!unfalloc || !PageUptodate(page)) {\n\t\t\t\tif (page->mapping == mapping) {\n\t\t\t\t\tVM_BUG_ON(PageWriteback(page));\n\t\t\t\t\ttruncate_inode_page(mapping, page);\n\t\t\t\t}\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t}\n\t\tshmem_deswap_pagevec(&pvec);\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tindex++;\n\t}\n\n\tspin_lock(&info->lock);\n\tinfo->swapped -= nr_swaps_freed;\n\tshmem_recalc_inode(inode);\n\tspin_unlock(&info->lock);\n}\n\nvoid shmem_truncate_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\tshmem_undo_range(inode, lstart, lend, false);\n\tinode->i_ctime = inode->i_mtime = CURRENT_TIME;\n}\nEXPORT_SYMBOL_GPL(shmem_truncate_range);\n\nstatic int shmem_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error;\n\n\terror = inode_change_ok(inode, attr);\n\tif (error)\n\t\treturn error;\n\n\tif (S_ISREG(inode->i_mode) && (attr->ia_valid & ATTR_SIZE)) {\n\t\tloff_t oldsize = inode->i_size;\n\t\tloff_t newsize = attr->ia_size;\n\n\t\tif (newsize != oldsize) {\n\t\t\ti_size_write(inode, newsize);\n\t\t\tinode->i_ctime = inode->i_mtime = CURRENT_TIME;\n\t\t}\n\t\tif (newsize < oldsize) {\n\t\t\tloff_t holebegin = round_up(newsize, PAGE_SIZE);\n\t\t\tunmap_mapping_range(inode->i_mapping, holebegin, 0, 1);\n\t\t\tshmem_truncate_range(inode, newsize, (loff_t)-1);\n\t\t\t/* unmap again to remove racily COWed private pages */\n\t\t\tunmap_mapping_range(inode->i_mapping, holebegin, 0, 1);\n\t\t}\n\t}\n\n\tsetattr_copy(inode, attr);\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\tif (attr->ia_valid & ATTR_MODE)\n\t\terror = generic_acl_chmod(inode);\n#endif\n\treturn error;\n}\n\nstatic void shmem_evict_inode(struct inode *inode)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\n\tif (inode->i_mapping->a_ops == &shmem_aops) {\n\t\tshmem_unacct_size(info->flags, inode->i_size);\n\t\tinode->i_size = 0;\n\t\tshmem_truncate_range(inode, 0, (loff_t)-1);\n\t\tif (!list_empty(&info->swaplist)) {\n\t\t\tmutex_lock(&shmem_swaplist_mutex);\n\t\t\tlist_del_init(&info->swaplist);\n\t\t\tmutex_unlock(&shmem_swaplist_mutex);\n\t\t}\n\t} else\n\t\tkfree(info->symlink);\n\n\tsimple_xattrs_free(&info->xattrs);\n\tWARN_ON(inode->i_blocks);\n\tshmem_free_inode(inode->i_sb);\n\tclear_inode(inode);\n}\n\n/*\n * If swap found in inode, free it and move page from swapcache to filecache.\n */\nstatic int shmem_unuse_inode(struct shmem_inode_info *info,\n\t\t\t     swp_entry_t swap, struct page **pagep)\n{\n\tstruct address_space *mapping = info->vfs_inode.i_mapping;\n\tvoid *radswap;\n\tpgoff_t index;\n\tgfp_t gfp;\n\tint error = 0;\n\n\tradswap = swp_to_radix_entry(swap);\n\tindex = radix_tree_locate_item(&mapping->page_tree, radswap);\n\tif (index == -1)\n\t\treturn 0;\n\n\t/*\n\t * Move _head_ to start search for next from here.\n\t * But be careful: shmem_evict_inode checks list_empty without taking\n\t * mutex, and there's an instant in list_move_tail when info->swaplist\n\t * would appear empty, if it were the only one on shmem_swaplist.\n\t */\n\tif (shmem_swaplist.next != &info->swaplist)\n\t\tlist_move_tail(&shmem_swaplist, &info->swaplist);\n\n\tgfp = mapping_gfp_mask(mapping);\n\tif (shmem_should_replace_page(*pagep, gfp)) {\n\t\tmutex_unlock(&shmem_swaplist_mutex);\n\t\terror = shmem_replace_page(pagep, gfp, info, index);\n\t\tmutex_lock(&shmem_swaplist_mutex);\n\t\t/*\n\t\t * We needed to drop mutex to make that restrictive page\n\t\t * allocation, but the inode might have been freed while we\n\t\t * dropped it: although a racing shmem_evict_inode() cannot\n\t\t * complete without emptying the radix_tree, our page lock\n\t\t * on this swapcache page is not enough to prevent that -\n\t\t * free_swap_and_cache() of our swap entry will only\n\t\t * trylock_page(), removing swap from radix_tree whatever.\n\t\t *\n\t\t * We must not proceed to shmem_add_to_page_cache() if the\n\t\t * inode has been freed, but of course we cannot rely on\n\t\t * inode or mapping or info to check that.  However, we can\n\t\t * safely check if our swap entry is still in use (and here\n\t\t * it can't have got reused for another page): if it's still\n\t\t * in use, then the inode cannot have been freed yet, and we\n\t\t * can safely proceed (if it's no longer in use, that tells\n\t\t * nothing about the inode, but we don't need to unuse swap).\n\t\t */\n\t\tif (!page_swapcount(*pagep))\n\t\t\terror = -ENOENT;\n\t}\n\n\t/*\n\t * We rely on shmem_swaplist_mutex, not only to protect the swaplist,\n\t * but also to hold up shmem_evict_inode(): so inode cannot be freed\n\t * beneath us (pagelock doesn't help until the page is in pagecache).\n\t */\n\tif (!error)\n\t\terror = shmem_add_to_page_cache(*pagep, mapping, index,\n\t\t\t\t\t\tGFP_NOWAIT, radswap);\n\tif (error != -ENOMEM) {\n\t\t/*\n\t\t * Truncation and eviction use free_swap_and_cache(), which\n\t\t * only does trylock page: if we raced, best clean up here.\n\t\t */\n\t\tdelete_from_swap_cache(*pagep);\n\t\tset_page_dirty(*pagep);\n\t\tif (!error) {\n\t\t\tspin_lock(&info->lock);\n\t\t\tinfo->swapped--;\n\t\t\tspin_unlock(&info->lock);\n\t\t\tswap_free(swap);\n\t\t}\n\t\terror = 1;\t/* not an error, but entry was found */\n\t}\n\treturn error;\n}\n\n/*\n * Search through swapped inodes to find and replace swap by page.\n */\nint shmem_unuse(swp_entry_t swap, struct page *page)\n{\n\tstruct list_head *this, *next;\n\tstruct shmem_inode_info *info;\n\tint found = 0;\n\tint error = 0;\n\n\t/*\n\t * There's a faint possibility that swap page was replaced before\n\t * caller locked it: caller will come back later with the right page.\n\t */\n\tif (unlikely(!PageSwapCache(page) || page_private(page) != swap.val))\n\t\tgoto out;\n\n\t/*\n\t * Charge page using GFP_KERNEL while we can wait, before taking\n\t * the shmem_swaplist_mutex which might hold up shmem_writepage().\n\t * Charged back to the user (not to caller) when swap account is used.\n\t */\n\terror = mem_cgroup_cache_charge(page, current->mm, GFP_KERNEL);\n\tif (error)\n\t\tgoto out;\n\t/* No radix_tree_preload: swap entry keeps a place for page in tree */\n\n\tmutex_lock(&shmem_swaplist_mutex);\n\tlist_for_each_safe(this, next, &shmem_swaplist) {\n\t\tinfo = list_entry(this, struct shmem_inode_info, swaplist);\n\t\tif (info->swapped)\n\t\t\tfound = shmem_unuse_inode(info, swap, &page);\n\t\telse\n\t\t\tlist_del_init(&info->swaplist);\n\t\tcond_resched();\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&shmem_swaplist_mutex);\n\n\tif (found < 0)\n\t\terror = found;\nout:\n\tunlock_page(page);\n\tpage_cache_release(page);\n\treturn error;\n}\n\n/*\n * Move the page from the page cache to the swap cache.\n */\nstatic int shmem_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tstruct shmem_inode_info *info;\n\tstruct address_space *mapping;\n\tstruct inode *inode;\n\tswp_entry_t swap;\n\tpgoff_t index;\n\n\tBUG_ON(!PageLocked(page));\n\tmapping = page->mapping;\n\tindex = page->index;\n\tinode = mapping->host;\n\tinfo = SHMEM_I(inode);\n\tif (info->flags & VM_LOCKED)\n\t\tgoto redirty;\n\tif (!total_swap_pages)\n\t\tgoto redirty;\n\n\t/*\n\t * shmem_backing_dev_info's capabilities prevent regular writeback or\n\t * sync from ever calling shmem_writepage; but a stacking filesystem\n\t * might use ->writepage of its underlying filesystem, in which case\n\t * tmpfs should write out to swap only in response to memory pressure,\n\t * and not for the writeback threads or sync.\n\t */\n\tif (!wbc->for_reclaim) {\n\t\tWARN_ON_ONCE(1);\t/* Still happens? Tell us about it! */\n\t\tgoto redirty;\n\t}\n\n\t/*\n\t * This is somewhat ridiculous, but without plumbing a SWAP_MAP_FALLOC\n\t * value into swapfile.c, the only way we can correctly account for a\n\t * fallocated page arriving here is now to initialize it and write it.\n\t *\n\t * That's okay for a page already fallocated earlier, but if we have\n\t * not yet completed the fallocation, then (a) we want to keep track\n\t * of this page in case we have to undo it, and (b) it may not be a\n\t * good idea to continue anyway, once we're pushing into swap.  So\n\t * reactivate the page, and let shmem_fallocate() quit when too many.\n\t */\n\tif (!PageUptodate(page)) {\n\t\tif (inode->i_private) {\n\t\t\tstruct shmem_falloc *shmem_falloc;\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tshmem_falloc = inode->i_private;\n\t\t\tif (shmem_falloc &&\n\t\t\t    index >= shmem_falloc->start &&\n\t\t\t    index < shmem_falloc->next)\n\t\t\t\tshmem_falloc->nr_unswapped++;\n\t\t\telse\n\t\t\t\tshmem_falloc = NULL;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tif (shmem_falloc)\n\t\t\t\tgoto redirty;\n\t\t}\n\t\tclear_highpage(page);\n\t\tflush_dcache_page(page);\n\t\tSetPageUptodate(page);\n\t}\n\n\tswap = get_swap_page();\n\tif (!swap.val)\n\t\tgoto redirty;\n\n\t/*\n\t * Add inode to shmem_unuse()'s list of swapped-out inodes,\n\t * if it's not already there.  Do it now before the page is\n\t * moved to swap cache, when its pagelock no longer protects\n\t * the inode from eviction.  But don't unlock the mutex until\n\t * we've incremented swapped, because shmem_unuse_inode() will\n\t * prune a !swapped inode from the swaplist under this mutex.\n\t */\n\tmutex_lock(&shmem_swaplist_mutex);\n\tif (list_empty(&info->swaplist))\n\t\tlist_add_tail(&info->swaplist, &shmem_swaplist);\n\n\tif (add_to_swap_cache(page, swap, GFP_ATOMIC) == 0) {\n\t\tswap_shmem_alloc(swap);\n\t\tshmem_delete_from_page_cache(page, swp_to_radix_entry(swap));\n\n\t\tspin_lock(&info->lock);\n\t\tinfo->swapped++;\n\t\tshmem_recalc_inode(inode);\n\t\tspin_unlock(&info->lock);\n\n\t\tmutex_unlock(&shmem_swaplist_mutex);\n\t\tBUG_ON(page_mapped(page));\n\t\tswap_writepage(page, wbc);\n\t\treturn 0;\n\t}\n\n\tmutex_unlock(&shmem_swaplist_mutex);\n\tswapcache_free(swap, NULL);\nredirty:\n\tset_page_dirty(page);\n\tif (wbc->for_reclaim)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\t/* Return with page locked */\n\tunlock_page(page);\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\n#ifdef CONFIG_TMPFS\nstatic void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)\n{\n\tchar buffer[64];\n\n\tif (!mpol || mpol->mode == MPOL_DEFAULT)\n\t\treturn;\t\t/* show nothing */\n\n\tmpol_to_str(buffer, sizeof(buffer), mpol);\n\n\tseq_printf(seq, \",mpol=%s\", buffer);\n}\n\nstatic struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)\n{\n\tstruct mempolicy *mpol = NULL;\n\tif (sbinfo->mpol) {\n\t\tspin_lock(&sbinfo->stat_lock);\t/* prevent replace/use races */\n\t\tmpol = sbinfo->mpol;\n\t\tmpol_get(mpol);\n\t\tspin_unlock(&sbinfo->stat_lock);\n\t}\n\treturn mpol;\n}\n#endif /* CONFIG_TMPFS */\n\nstatic struct page *shmem_swapin(swp_entry_t swap, gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct vm_area_struct pvma;\n\tstruct page *page;\n\n\t/* Create a pseudo vma that just contains the policy */\n\tpvma.vm_start = 0;\n\t/* Bias interleave by inode number to distribute better across nodes */\n\tpvma.vm_pgoff = index + info->vfs_inode.i_ino;\n\tpvma.vm_ops = NULL;\n\tpvma.vm_policy = mpol_shared_policy_lookup(&info->policy, index);\n\n\tpage = swapin_readahead(swap, gfp, &pvma, 0);\n\n\t/* Drop reference taken by mpol_shared_policy_lookup() */\n\tmpol_cond_put(pvma.vm_policy);\n\n\treturn page;\n}\n\nstatic struct page *shmem_alloc_page(gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct vm_area_struct pvma;\n\tstruct page *page;\n\n\t/* Create a pseudo vma that just contains the policy */\n\tpvma.vm_start = 0;\n\t/* Bias interleave by inode number to distribute better across nodes */\n\tpvma.vm_pgoff = index + info->vfs_inode.i_ino;\n\tpvma.vm_ops = NULL;\n\tpvma.vm_policy = mpol_shared_policy_lookup(&info->policy, index);\n\n\tpage = alloc_page_vma(gfp, &pvma, 0);\n\n\t/* Drop reference taken by mpol_shared_policy_lookup() */\n\tmpol_cond_put(pvma.vm_policy);\n\n\treturn page;\n}\n#else /* !CONFIG_NUMA */\n#ifdef CONFIG_TMPFS\nstatic inline void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)\n{\n}\n#endif /* CONFIG_TMPFS */\n\nstatic inline struct page *shmem_swapin(swp_entry_t swap, gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\treturn swapin_readahead(swap, gfp, NULL, 0);\n}\n\nstatic inline struct page *shmem_alloc_page(gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\treturn alloc_page(gfp);\n}\n#endif /* CONFIG_NUMA */\n\n#if !defined(CONFIG_NUMA) || !defined(CONFIG_TMPFS)\nstatic inline struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)\n{\n\treturn NULL;\n}\n#endif\n\n/*\n * When a page is moved from swapcache to shmem filecache (either by the\n * usual swapin of shmem_getpage_gfp(), or by the less common swapoff of\n * shmem_unuse_inode()), it may have been read in earlier from swap, in\n * ignorance of the mapping it belongs to.  If that mapping has special\n * constraints (like the gma500 GEM driver, which requires RAM below 4GB),\n * we may need to copy to a suitable page before moving to filecache.\n *\n * In a future release, this may well be extended to respect cpuset and\n * NUMA mempolicy, and applied also to anonymous pages in do_swap_page();\n * but for now it is a simple matter of zone.\n */\nstatic bool shmem_should_replace_page(struct page *page, gfp_t gfp)\n{\n\treturn page_zonenum(page) > gfp_zone(gfp);\n}\n\nstatic int shmem_replace_page(struct page **pagep, gfp_t gfp,\n\t\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct page *oldpage, *newpage;\n\tstruct address_space *swap_mapping;\n\tpgoff_t swap_index;\n\tint error;\n\n\toldpage = *pagep;\n\tswap_index = page_private(oldpage);\n\tswap_mapping = page_mapping(oldpage);\n\n\t/*\n\t * We have arrived here because our zones are constrained, so don't\n\t * limit chance of success by further cpuset and node constraints.\n\t */\n\tgfp &= ~GFP_CONSTRAINT_MASK;\n\tnewpage = shmem_alloc_page(gfp, info, index);\n\tif (!newpage)\n\t\treturn -ENOMEM;\n\n\tpage_cache_get(newpage);\n\tcopy_highpage(newpage, oldpage);\n\tflush_dcache_page(newpage);\n\n\t__set_page_locked(newpage);\n\tSetPageUptodate(newpage);\n\tSetPageSwapBacked(newpage);\n\tset_page_private(newpage, swap_index);\n\tSetPageSwapCache(newpage);\n\n\t/*\n\t * Our caller will very soon move newpage out of swapcache, but it's\n\t * a nice clean interface for us to replace oldpage by newpage there.\n\t */\n\tspin_lock_irq(&swap_mapping->tree_lock);\n\terror = shmem_radix_tree_replace(swap_mapping, swap_index, oldpage,\n\t\t\t\t\t\t\t\t   newpage);\n\tif (!error) {\n\t\t__inc_zone_page_state(newpage, NR_FILE_PAGES);\n\t\t__dec_zone_page_state(oldpage, NR_FILE_PAGES);\n\t}\n\tspin_unlock_irq(&swap_mapping->tree_lock);\n\n\tif (unlikely(error)) {\n\t\t/*\n\t\t * Is this possible?  I think not, now that our callers check\n\t\t * both PageSwapCache and page_private after getting page lock;\n\t\t * but be defensive.  Reverse old to newpage for clear and free.\n\t\t */\n\t\toldpage = newpage;\n\t} else {\n\t\tmem_cgroup_replace_page_cache(oldpage, newpage);\n\t\tlru_cache_add_anon(newpage);\n\t\t*pagep = newpage;\n\t}\n\n\tClearPageSwapCache(oldpage);\n\tset_page_private(oldpage, 0);\n\n\tunlock_page(oldpage);\n\tpage_cache_release(oldpage);\n\tpage_cache_release(oldpage);\n\treturn error;\n}\n\n/*\n * shmem_getpage_gfp - find page in cache, or get from swap, or allocate\n *\n * If we allocate a new one we do not mark it dirty. That's up to the\n * vm. If we swap it in we mark it dirty since we also free the swap\n * entry since a page cannot live in both the swap and page cache\n */\nstatic int shmem_getpage_gfp(struct inode *inode, pgoff_t index,\n\tstruct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct shmem_inode_info *info;\n\tstruct shmem_sb_info *sbinfo;\n\tstruct page *page;\n\tswp_entry_t swap;\n\tint error;\n\tint once = 0;\n\tint alloced = 0;\n\n\tif (index > (MAX_LFS_FILESIZE >> PAGE_CACHE_SHIFT))\n\t\treturn -EFBIG;\nrepeat:\n\tswap.val = 0;\n\tpage = find_lock_page(mapping, index);\n\tif (radix_tree_exceptional_entry(page)) {\n\t\tswap = radix_to_swp_entry(page);\n\t\tpage = NULL;\n\t}\n\n\tif (sgp != SGP_WRITE && sgp != SGP_FALLOC &&\n\t    ((loff_t)index << PAGE_CACHE_SHIFT) >= i_size_read(inode)) {\n\t\terror = -EINVAL;\n\t\tgoto failed;\n\t}\n\n\t/* fallocated page? */\n\tif (page && !PageUptodate(page)) {\n\t\tif (sgp != SGP_READ)\n\t\t\tgoto clear;\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tpage = NULL;\n\t}\n\tif (page || (sgp == SGP_READ && !swap.val)) {\n\t\t*pagep = page;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Fast cache lookup did not find it:\n\t * bring it back from swap or allocate.\n\t */\n\tinfo = SHMEM_I(inode);\n\tsbinfo = SHMEM_SB(inode->i_sb);\n\n\tif (swap.val) {\n\t\t/* Look it up and read it in.. */\n\t\tpage = lookup_swap_cache(swap);\n\t\tif (!page) {\n\t\t\t/* here we actually do the io */\n\t\t\tif (fault_type)\n\t\t\t\t*fault_type |= VM_FAULT_MAJOR;\n\t\t\tpage = shmem_swapin(swap, gfp, info, index);\n\t\t\tif (!page) {\n\t\t\t\terror = -ENOMEM;\n\t\t\t\tgoto failed;\n\t\t\t}\n\t\t}\n\n\t\t/* We have to do this with page locked to prevent races */\n\t\tlock_page(page);\n\t\tif (!PageSwapCache(page) || page_private(page) != swap.val ||\n\t\t    !shmem_confirm_swap(mapping, index, swap)) {\n\t\t\terror = -EEXIST;\t/* try again */\n\t\t\tgoto unlock;\n\t\t}\n\t\tif (!PageUptodate(page)) {\n\t\t\terror = -EIO;\n\t\t\tgoto failed;\n\t\t}\n\t\twait_on_page_writeback(page);\n\n\t\tif (shmem_should_replace_page(page, gfp)) {\n\t\t\terror = shmem_replace_page(&page, gfp, info, index);\n\t\t\tif (error)\n\t\t\t\tgoto failed;\n\t\t}\n\n\t\terror = mem_cgroup_cache_charge(page, current->mm,\n\t\t\t\t\t\tgfp & GFP_RECLAIM_MASK);\n\t\tif (!error) {\n\t\t\terror = shmem_add_to_page_cache(page, mapping, index,\n\t\t\t\t\t\tgfp, swp_to_radix_entry(swap));\n\t\t\t/*\n\t\t\t * We already confirmed swap under page lock, and make\n\t\t\t * no memory allocation here, so usually no possibility\n\t\t\t * of error; but free_swap_and_cache() only trylocks a\n\t\t\t * page, so it is just possible that the entry has been\n\t\t\t * truncated or holepunched since swap was confirmed.\n\t\t\t * shmem_undo_range() will have done some of the\n\t\t\t * unaccounting, now delete_from_swap_cache() will do\n\t\t\t * the rest (including mem_cgroup_uncharge_swapcache).\n\t\t\t * Reset swap.val? No, leave it so \"failed\" goes back to\n\t\t\t * \"repeat\": reading a hole and writing should succeed.\n\t\t\t */\n\t\t\tif (error)\n\t\t\t\tdelete_from_swap_cache(page);\n\t\t}\n\t\tif (error)\n\t\t\tgoto failed;\n\n\t\tspin_lock(&info->lock);\n\t\tinfo->swapped--;\n\t\tshmem_recalc_inode(inode);\n\t\tspin_unlock(&info->lock);\n\n\t\tdelete_from_swap_cache(page);\n\t\tset_page_dirty(page);\n\t\tswap_free(swap);\n\n\t} else {\n\t\tif (shmem_acct_block(info->flags)) {\n\t\t\terror = -ENOSPC;\n\t\t\tgoto failed;\n\t\t}\n\t\tif (sbinfo->max_blocks) {\n\t\t\tif (percpu_counter_compare(&sbinfo->used_blocks,\n\t\t\t\t\t\tsbinfo->max_blocks) >= 0) {\n\t\t\t\terror = -ENOSPC;\n\t\t\t\tgoto unacct;\n\t\t\t}\n\t\t\tpercpu_counter_inc(&sbinfo->used_blocks);\n\t\t}\n\n\t\tpage = shmem_alloc_page(gfp, info, index);\n\t\tif (!page) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto decused;\n\t\t}\n\n\t\tSetPageSwapBacked(page);\n\t\t__set_page_locked(page);\n\t\terror = mem_cgroup_cache_charge(page, current->mm,\n\t\t\t\t\t\tgfp & GFP_RECLAIM_MASK);\n\t\tif (error)\n\t\t\tgoto decused;\n\t\terror = radix_tree_preload(gfp & GFP_RECLAIM_MASK);\n\t\tif (!error) {\n\t\t\terror = shmem_add_to_page_cache(page, mapping, index,\n\t\t\t\t\t\t\tgfp, NULL);\n\t\t\tradix_tree_preload_end();\n\t\t}\n\t\tif (error) {\n\t\t\tmem_cgroup_uncharge_cache_page(page);\n\t\t\tgoto decused;\n\t\t}\n\t\tlru_cache_add_anon(page);\n\n\t\tspin_lock(&info->lock);\n\t\tinfo->alloced++;\n\t\tinode->i_blocks += BLOCKS_PER_PAGE;\n\t\tshmem_recalc_inode(inode);\n\t\tspin_unlock(&info->lock);\n\t\talloced = true;\n\n\t\t/*\n\t\t * Let SGP_FALLOC use the SGP_WRITE optimization on a new page.\n\t\t */\n\t\tif (sgp == SGP_FALLOC)\n\t\t\tsgp = SGP_WRITE;\nclear:\n\t\t/*\n\t\t * Let SGP_WRITE caller clear ends if write does not fill page;\n\t\t * but SGP_FALLOC on a page fallocated earlier must initialize\n\t\t * it now, lest undo on failure cancel our earlier guarantee.\n\t\t */\n\t\tif (sgp != SGP_WRITE) {\n\t\t\tclear_highpage(page);\n\t\t\tflush_dcache_page(page);\n\t\t\tSetPageUptodate(page);\n\t\t}\n\t\tif (sgp == SGP_DIRTY)\n\t\t\tset_page_dirty(page);\n\t}\n\n\t/* Perhaps the file has been truncated since we checked */\n\tif (sgp != SGP_WRITE && sgp != SGP_FALLOC &&\n\t    ((loff_t)index << PAGE_CACHE_SHIFT) >= i_size_read(inode)) {\n\t\terror = -EINVAL;\n\t\tif (alloced)\n\t\t\tgoto trunc;\n\t\telse\n\t\t\tgoto failed;\n\t}\n\t*pagep = page;\n\treturn 0;\n\n\t/*\n\t * Error recovery.\n\t */\ntrunc:\n\tinfo = SHMEM_I(inode);\n\tClearPageDirty(page);\n\tdelete_from_page_cache(page);\n\tspin_lock(&info->lock);\n\tinfo->alloced--;\n\tinode->i_blocks -= BLOCKS_PER_PAGE;\n\tspin_unlock(&info->lock);\ndecused:\n\tsbinfo = SHMEM_SB(inode->i_sb);\n\tif (sbinfo->max_blocks)\n\t\tpercpu_counter_add(&sbinfo->used_blocks, -1);\nunacct:\n\tshmem_unacct_blocks(info->flags, 1);\nfailed:\n\tif (swap.val && error != -EINVAL &&\n\t    !shmem_confirm_swap(mapping, index, swap))\n\t\terror = -EEXIST;\nunlock:\n\tif (page) {\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t}\n\tif (error == -ENOSPC && !once++) {\n\t\tinfo = SHMEM_I(inode);\n\t\tspin_lock(&info->lock);\n\t\tshmem_recalc_inode(inode);\n\t\tspin_unlock(&info->lock);\n\t\tgoto repeat;\n\t}\n\tif (error == -EEXIST)\t/* from above or from radix_tree_insert */\n\t\tgoto repeat;\n\treturn error;\n}\n\nstatic int shmem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\n\tint error;\n\tint ret = VM_FAULT_LOCKED;\n\n\terror = shmem_getpage(inode, vmf->pgoff, &vmf->page, SGP_CACHE, &ret);\n\tif (error)\n\t\treturn ((error == -ENOMEM) ? VM_FAULT_OOM : VM_FAULT_SIGBUS);\n\n\tif (ret & VM_FAULT_MAJOR) {\n\t\tcount_vm_event(PGMAJFAULT);\n\t\tmem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_NUMA\nstatic int shmem_set_policy(struct vm_area_struct *vma, struct mempolicy *mpol)\n{\n\tstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\n\treturn mpol_set_shared_policy(&SHMEM_I(inode)->policy, vma, mpol);\n}\n\nstatic struct mempolicy *shmem_get_policy(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long addr)\n{\n\tstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\n\tpgoff_t index;\n\n\tindex = ((addr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\n\treturn mpol_shared_policy_lookup(&SHMEM_I(inode)->policy, index);\n}\n#endif\n\nint shmem_lock(struct file *file, int lock, struct user_struct *user)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tint retval = -ENOMEM;\n\n\tspin_lock(&info->lock);\n\tif (lock && !(info->flags & VM_LOCKED)) {\n\t\tif (!user_shm_lock(inode->i_size, user))\n\t\t\tgoto out_nomem;\n\t\tinfo->flags |= VM_LOCKED;\n\t\tmapping_set_unevictable(file->f_mapping);\n\t}\n\tif (!lock && (info->flags & VM_LOCKED) && user) {\n\t\tuser_shm_unlock(inode->i_size, user);\n\t\tinfo->flags &= ~VM_LOCKED;\n\t\tmapping_clear_unevictable(file->f_mapping);\n\t}\n\tretval = 0;\n\nout_nomem:\n\tspin_unlock(&info->lock);\n\treturn retval;\n}\n\nstatic int shmem_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tfile_accessed(file);\n\tvma->vm_ops = &shmem_vm_ops;\n\treturn 0;\n}\n\nstatic struct inode *shmem_get_inode(struct super_block *sb, const struct inode *dir,\n\t\t\t\t     umode_t mode, dev_t dev, unsigned long flags)\n{\n\tstruct inode *inode;\n\tstruct shmem_inode_info *info;\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\n\tif (shmem_reserve_inode(sb))\n\t\treturn NULL;\n\n\tinode = new_inode(sb);\n\tif (inode) {\n\t\tinode->i_ino = get_next_ino();\n\t\tinode_init_owner(inode, dir, mode);\n\t\tinode->i_blocks = 0;\n\t\tinode->i_mapping->backing_dev_info = &shmem_backing_dev_info;\n\t\tinode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;\n\t\tinode->i_generation = get_seconds();\n\t\tinfo = SHMEM_I(inode);\n\t\tmemset(info, 0, (char *)inode - (char *)info);\n\t\tspin_lock_init(&info->lock);\n\t\tinfo->flags = flags & VM_NORESERVE;\n\t\tINIT_LIST_HEAD(&info->swaplist);\n\t\tsimple_xattrs_init(&info->xattrs);\n\t\tcache_no_acl(inode);\n\n\t\tswitch (mode & S_IFMT) {\n\t\tdefault:\n\t\t\tinode->i_op = &shmem_special_inode_operations;\n\t\t\tinit_special_inode(inode, mode, dev);\n\t\t\tbreak;\n\t\tcase S_IFREG:\n\t\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\t\tinode->i_op = &shmem_inode_operations;\n\t\t\tinode->i_fop = &shmem_file_operations;\n\t\t\tmpol_shared_policy_init(&info->policy,\n\t\t\t\t\t\t shmem_get_sbmpol(sbinfo));\n\t\t\tbreak;\n\t\tcase S_IFDIR:\n\t\t\tinc_nlink(inode);\n\t\t\t/* Some things misbehave if size == 0 on a directory */\n\t\t\tinode->i_size = 2 * BOGO_DIRENT_SIZE;\n\t\t\tinode->i_op = &shmem_dir_inode_operations;\n\t\t\tinode->i_fop = &simple_dir_operations;\n\t\t\tbreak;\n\t\tcase S_IFLNK:\n\t\t\t/*\n\t\t\t * Must not load anything in the rbtree,\n\t\t\t * mpol_free_shared_policy will not be called.\n\t\t\t */\n\t\t\tmpol_shared_policy_init(&info->policy, NULL);\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tshmem_free_inode(sb);\n\treturn inode;\n}\n\n#ifdef CONFIG_TMPFS\nstatic const struct inode_operations shmem_symlink_inode_operations;\nstatic const struct inode_operations shmem_short_symlink_operations;\n\n#ifdef CONFIG_TMPFS_XATTR\nstatic int shmem_initxattrs(struct inode *, const struct xattr *, void *);\n#else\n#define shmem_initxattrs NULL\n#endif\n\nstatic int\nshmem_write_begin(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned flags,\n\t\t\tstruct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tpgoff_t index = pos >> PAGE_CACHE_SHIFT;\n\treturn shmem_getpage(inode, index, pagep, SGP_WRITE, NULL);\n}\n\nstatic int\nshmem_write_end(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\n\tif (pos + copied > inode->i_size)\n\t\ti_size_write(inode, pos + copied);\n\n\tif (!PageUptodate(page)) {\n\t\tif (copied < PAGE_CACHE_SIZE) {\n\t\t\tunsigned from = pos & (PAGE_CACHE_SIZE - 1);\n\t\t\tzero_user_segments(page, 0, from,\n\t\t\t\t\tfrom + copied, PAGE_CACHE_SIZE);\n\t\t}\n\t\tSetPageUptodate(page);\n\t}\n\tset_page_dirty(page);\n\tunlock_page(page);\n\tpage_cache_release(page);\n\n\treturn copied;\n}\n\nstatic void do_shmem_file_read(struct file *filp, loff_t *ppos, read_descriptor_t *desc, read_actor_t actor)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\tpgoff_t index;\n\tunsigned long offset;\n\tenum sgp_type sgp = SGP_READ;\n\n\t/*\n\t * Might this read be for a stacking filesystem?  Then when reading\n\t * holes of a sparse file, we actually need to allocate those pages,\n\t * and even mark them dirty, so it cannot exceed the max_blocks limit.\n\t */\n\tif (segment_eq(get_fs(), KERNEL_DS))\n\t\tsgp = SGP_DIRTY;\n\n\tindex = *ppos >> PAGE_CACHE_SHIFT;\n\toffset = *ppos & ~PAGE_CACHE_MASK;\n\n\tfor (;;) {\n\t\tstruct page *page = NULL;\n\t\tpgoff_t end_index;\n\t\tunsigned long nr, ret;\n\t\tloff_t i_size = i_size_read(inode);\n\n\t\tend_index = i_size >> PAGE_CACHE_SHIFT;\n\t\tif (index > end_index)\n\t\t\tbreak;\n\t\tif (index == end_index) {\n\t\t\tnr = i_size & ~PAGE_CACHE_MASK;\n\t\t\tif (nr <= offset)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tdesc->error = shmem_getpage(inode, index, &page, sgp, NULL);\n\t\tif (desc->error) {\n\t\t\tif (desc->error == -EINVAL)\n\t\t\t\tdesc->error = 0;\n\t\t\tbreak;\n\t\t}\n\t\tif (page)\n\t\t\tunlock_page(page);\n\n\t\t/*\n\t\t * We must evaluate after, since reads (unlike writes)\n\t\t * are called without i_mutex protection against truncate\n\t\t */\n\t\tnr = PAGE_CACHE_SIZE;\n\t\ti_size = i_size_read(inode);\n\t\tend_index = i_size >> PAGE_CACHE_SHIFT;\n\t\tif (index == end_index) {\n\t\t\tnr = i_size & ~PAGE_CACHE_MASK;\n\t\t\tif (nr <= offset) {\n\t\t\t\tif (page)\n\t\t\t\t\tpage_cache_release(page);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tnr -= offset;\n\n\t\tif (page) {\n\t\t\t/*\n\t\t\t * If users can be writing to this page using arbitrary\n\t\t\t * virtual addresses, take care about potential aliasing\n\t\t\t * before reading the page on the kernel side.\n\t\t\t */\n\t\t\tif (mapping_writably_mapped(mapping))\n\t\t\t\tflush_dcache_page(page);\n\t\t\t/*\n\t\t\t * Mark the page accessed if we read the beginning.\n\t\t\t */\n\t\t\tif (!offset)\n\t\t\t\tmark_page_accessed(page);\n\t\t} else {\n\t\t\tpage = ZERO_PAGE(0);\n\t\t\tpage_cache_get(page);\n\t\t}\n\n\t\t/*\n\t\t * Ok, we have the page, and it's up-to-date, so\n\t\t * now we can copy it to user space...\n\t\t *\n\t\t * The actor routine returns how many bytes were actually used..\n\t\t * NOTE! This may not be the same as how much of a user buffer\n\t\t * we filled up (we may be padding etc), so we can only update\n\t\t * \"pos\" here (the actor routine has to update the user buffer\n\t\t * pointers and the remaining count).\n\t\t */\n\t\tret = actor(desc, page, offset, nr);\n\t\toffset += ret;\n\t\tindex += offset >> PAGE_CACHE_SHIFT;\n\t\toffset &= ~PAGE_CACHE_MASK;\n\n\t\tpage_cache_release(page);\n\t\tif (ret != nr || !desc->count)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\t*ppos = ((loff_t) index << PAGE_CACHE_SHIFT) + offset;\n\tfile_accessed(filp);\n}\n\nstatic ssize_t shmem_file_aio_read(struct kiocb *iocb,\n\t\tconst struct iovec *iov, unsigned long nr_segs, loff_t pos)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tssize_t retval;\n\tunsigned long seg;\n\tsize_t count;\n\tloff_t *ppos = &iocb->ki_pos;\n\n\tretval = generic_segment_checks(iov, &nr_segs, &count, VERIFY_WRITE);\n\tif (retval)\n\t\treturn retval;\n\n\tfor (seg = 0; seg < nr_segs; seg++) {\n\t\tread_descriptor_t desc;\n\n\t\tdesc.written = 0;\n\t\tdesc.arg.buf = iov[seg].iov_base;\n\t\tdesc.count = iov[seg].iov_len;\n\t\tif (desc.count == 0)\n\t\t\tcontinue;\n\t\tdesc.error = 0;\n\t\tdo_shmem_file_read(filp, ppos, &desc, file_read_actor);\n\t\tretval += desc.written;\n\t\tif (desc.error) {\n\t\t\tretval = retval ?: desc.error;\n\t\t\tbreak;\n\t\t}\n\t\tif (desc.count > 0)\n\t\t\tbreak;\n\t}\n\treturn retval;\n}\n\nstatic ssize_t shmem_file_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\t\tunsigned int flags)\n{\n\tstruct address_space *mapping = in->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tunsigned int loff, nr_pages, req_pages;\n\tstruct page *pages[PIPE_DEF_BUFFERS];\n\tstruct partial_page partial[PIPE_DEF_BUFFERS];\n\tstruct page *page;\n\tpgoff_t index, end_index;\n\tloff_t isize, left;\n\tint error, page_nr;\n\tstruct splice_pipe_desc spd = {\n\t\t.pages = pages,\n\t\t.partial = partial,\n\t\t.nr_pages_max = PIPE_DEF_BUFFERS,\n\t\t.flags = flags,\n\t\t.ops = &page_cache_pipe_buf_ops,\n\t\t.spd_release = spd_release_page,\n\t};\n\n\tisize = i_size_read(inode);\n\tif (unlikely(*ppos >= isize))\n\t\treturn 0;\n\n\tleft = isize - *ppos;\n\tif (unlikely(left < len))\n\t\tlen = left;\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n\tindex = *ppos >> PAGE_CACHE_SHIFT;\n\tloff = *ppos & ~PAGE_CACHE_MASK;\n\treq_pages = (len + loff + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\tnr_pages = min(req_pages, pipe->buffers);\n\n\tspd.nr_pages = find_get_pages_contig(mapping, index,\n\t\t\t\t\t\tnr_pages, spd.pages);\n\tindex += spd.nr_pages;\n\terror = 0;\n\n\twhile (spd.nr_pages < nr_pages) {\n\t\terror = shmem_getpage(inode, index, &page, SGP_CACHE, NULL);\n\t\tif (error)\n\t\t\tbreak;\n\t\tunlock_page(page);\n\t\tspd.pages[spd.nr_pages++] = page;\n\t\tindex++;\n\t}\n\n\tindex = *ppos >> PAGE_CACHE_SHIFT;\n\tnr_pages = spd.nr_pages;\n\tspd.nr_pages = 0;\n\n\tfor (page_nr = 0; page_nr < nr_pages; page_nr++) {\n\t\tunsigned int this_len;\n\n\t\tif (!len)\n\t\t\tbreak;\n\n\t\tthis_len = min_t(unsigned long, len, PAGE_CACHE_SIZE - loff);\n\t\tpage = spd.pages[page_nr];\n\n\t\tif (!PageUptodate(page) || page->mapping != mapping) {\n\t\t\terror = shmem_getpage(inode, index, &page,\n\t\t\t\t\t\t\tSGP_CACHE, NULL);\n\t\t\tif (error)\n\t\t\t\tbreak;\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(spd.pages[page_nr]);\n\t\t\tspd.pages[page_nr] = page;\n\t\t}\n\n\t\tisize = i_size_read(inode);\n\t\tend_index = (isize - 1) >> PAGE_CACHE_SHIFT;\n\t\tif (unlikely(!isize || index > end_index))\n\t\t\tbreak;\n\n\t\tif (end_index == index) {\n\t\t\tunsigned int plen;\n\n\t\t\tplen = ((isize - 1) & ~PAGE_CACHE_MASK) + 1;\n\t\t\tif (plen <= loff)\n\t\t\t\tbreak;\n\n\t\t\tthis_len = min(this_len, plen - loff);\n\t\t\tlen = this_len;\n\t\t}\n\n\t\tspd.partial[page_nr].offset = loff;\n\t\tspd.partial[page_nr].len = this_len;\n\t\tlen -= this_len;\n\t\tloff = 0;\n\t\tspd.nr_pages++;\n\t\tindex++;\n\t}\n\n\twhile (page_nr < nr_pages)\n\t\tpage_cache_release(spd.pages[page_nr++]);\n\n\tif (spd.nr_pages)\n\t\terror = splice_to_pipe(pipe, &spd);\n\n\tsplice_shrink_spd(&spd);\n\n\tif (error > 0) {\n\t\t*ppos += error;\n\t\tfile_accessed(in);\n\t}\n\treturn error;\n}\n\n/*\n * llseek SEEK_DATA or SEEK_HOLE through the radix_tree.\n */\nstatic pgoff_t shmem_seek_hole_data(struct address_space *mapping,\n\t\t\t\t    pgoff_t index, pgoff_t end, int whence)\n{\n\tstruct page *page;\n\tstruct pagevec pvec;\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tbool done = false;\n\tint i;\n\n\tpagevec_init(&pvec, 0);\n\tpvec.nr = 1;\t\t/* start small: we may be there already */\n\twhile (!done) {\n\t\tpvec.nr = shmem_find_get_pages_and_swap(mapping, index,\n\t\t\t\t\tpvec.nr, pvec.pages, indices);\n\t\tif (!pvec.nr) {\n\t\t\tif (whence == SEEK_DATA)\n\t\t\t\tindex = end;\n\t\t\tbreak;\n\t\t}\n\t\tfor (i = 0; i < pvec.nr; i++, index++) {\n\t\t\tif (index < indices[i]) {\n\t\t\t\tif (whence == SEEK_HOLE) {\n\t\t\t\t\tdone = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tindex = indices[i];\n\t\t\t}\n\t\t\tpage = pvec.pages[i];\n\t\t\tif (page && !radix_tree_exceptional_entry(page)) {\n\t\t\t\tif (!PageUptodate(page))\n\t\t\t\t\tpage = NULL;\n\t\t\t}\n\t\t\tif (index >= end ||\n\t\t\t    (page && whence == SEEK_DATA) ||\n\t\t\t    (!page && whence == SEEK_HOLE)) {\n\t\t\t\tdone = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tshmem_deswap_pagevec(&pvec);\n\t\tpagevec_release(&pvec);\n\t\tpvec.nr = PAGEVEC_SIZE;\n\t\tcond_resched();\n\t}\n\treturn index;\n}\n\nstatic loff_t shmem_file_llseek(struct file *file, loff_t offset, int whence)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tpgoff_t start, end;\n\tloff_t new_offset;\n\n\tif (whence != SEEK_DATA && whence != SEEK_HOLE)\n\t\treturn generic_file_llseek_size(file, offset, whence,\n\t\t\t\t\tMAX_LFS_FILESIZE, i_size_read(inode));\n\tmutex_lock(&inode->i_mutex);\n\t/* We're holding i_mutex so we can access i_size directly */\n\n\tif (offset < 0)\n\t\toffset = -EINVAL;\n\telse if (offset >= inode->i_size)\n\t\toffset = -ENXIO;\n\telse {\n\t\tstart = offset >> PAGE_CACHE_SHIFT;\n\t\tend = (inode->i_size + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\t\tnew_offset = shmem_seek_hole_data(mapping, start, end, whence);\n\t\tnew_offset <<= PAGE_CACHE_SHIFT;\n\t\tif (new_offset > offset) {\n\t\t\tif (new_offset < inode->i_size)\n\t\t\t\toffset = new_offset;\n\t\t\telse if (whence == SEEK_DATA)\n\t\t\t\toffset = -ENXIO;\n\t\t\telse\n\t\t\t\toffset = inode->i_size;\n\t\t}\n\t}\n\n\tif (offset >= 0 && offset != file->f_pos) {\n\t\tfile->f_pos = offset;\n\t\tfile->f_version = 0;\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\treturn offset;\n}\n\nstatic long shmem_fallocate(struct file *file, int mode, loff_t offset,\n\t\t\t\t\t\t\t loff_t len)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\tstruct shmem_falloc shmem_falloc;\n\tpgoff_t start, index, end;\n\tint error;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE) {\n\t\tstruct address_space *mapping = file->f_mapping;\n\t\tloff_t unmap_start = round_up(offset, PAGE_SIZE);\n\t\tloff_t unmap_end = round_down(offset + len, PAGE_SIZE) - 1;\n\n\t\tif ((u64)unmap_end > (u64)unmap_start)\n\t\t\tunmap_mapping_range(mapping, unmap_start,\n\t\t\t\t\t    1 + unmap_end - unmap_start, 0);\n\t\tshmem_truncate_range(inode, offset, offset + len - 1);\n\t\t/* No need to unmap again: hole-punching leaves COWed pages */\n\t\terror = 0;\n\t\tgoto out;\n\t}\n\n\t/* We need to check rlimit even when FALLOC_FL_KEEP_SIZE */\n\terror = inode_newsize_ok(inode, offset + len);\n\tif (error)\n\t\tgoto out;\n\n\tstart = offset >> PAGE_CACHE_SHIFT;\n\tend = (offset + len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\t/* Try to avoid a swapstorm if len is impossible to satisfy */\n\tif (sbinfo->max_blocks && end - start > sbinfo->max_blocks) {\n\t\terror = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tshmem_falloc.start = start;\n\tshmem_falloc.next  = start;\n\tshmem_falloc.nr_falloced = 0;\n\tshmem_falloc.nr_unswapped = 0;\n\tspin_lock(&inode->i_lock);\n\tinode->i_private = &shmem_falloc;\n\tspin_unlock(&inode->i_lock);\n\n\tfor (index = start; index < end; index++) {\n\t\tstruct page *page;\n\n\t\t/*\n\t\t * Good, the fallocate(2) manpage permits EINTR: we may have\n\t\t * been interrupted because we are using up too much memory.\n\t\t */\n\t\tif (signal_pending(current))\n\t\t\terror = -EINTR;\n\t\telse if (shmem_falloc.nr_unswapped > shmem_falloc.nr_falloced)\n\t\t\terror = -ENOMEM;\n\t\telse\n\t\t\terror = shmem_getpage(inode, index, &page, SGP_FALLOC,\n\t\t\t\t\t\t\t\t\tNULL);\n\t\tif (error) {\n\t\t\t/* Remove the !PageUptodate pages we added */\n\t\t\tshmem_undo_range(inode,\n\t\t\t\t(loff_t)start << PAGE_CACHE_SHIFT,\n\t\t\t\t(loff_t)index << PAGE_CACHE_SHIFT, true);\n\t\t\tgoto undone;\n\t\t}\n\n\t\t/*\n\t\t * Inform shmem_writepage() how far we have reached.\n\t\t * No need for lock or barrier: we have the page lock.\n\t\t */\n\t\tshmem_falloc.next++;\n\t\tif (!PageUptodate(page))\n\t\t\tshmem_falloc.nr_falloced++;\n\n\t\t/*\n\t\t * If !PageUptodate, leave it that way so that freeable pages\n\t\t * can be recognized if we need to rollback on error later.\n\t\t * But set_page_dirty so that memory pressure will swap rather\n\t\t * than free the pages we are allocating (and SGP_CACHE pages\n\t\t * might still be clean: we now need to mark those dirty too).\n\t\t */\n\t\tset_page_dirty(page);\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tcond_resched();\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) && offset + len > inode->i_size)\n\t\ti_size_write(inode, offset + len);\n\tinode->i_ctime = CURRENT_TIME;\nundone:\n\tspin_lock(&inode->i_lock);\n\tinode->i_private = NULL;\n\tspin_unlock(&inode->i_lock);\nout:\n\tmutex_unlock(&inode->i_mutex);\n\treturn error;\n}\n\nstatic int shmem_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(dentry->d_sb);\n\n\tbuf->f_type = TMPFS_MAGIC;\n\tbuf->f_bsize = PAGE_CACHE_SIZE;\n\tbuf->f_namelen = NAME_MAX;\n\tif (sbinfo->max_blocks) {\n\t\tbuf->f_blocks = sbinfo->max_blocks;\n\t\tbuf->f_bavail =\n\t\tbuf->f_bfree  = sbinfo->max_blocks -\n\t\t\t\tpercpu_counter_sum(&sbinfo->used_blocks);\n\t}\n\tif (sbinfo->max_inodes) {\n\t\tbuf->f_files = sbinfo->max_inodes;\n\t\tbuf->f_ffree = sbinfo->free_inodes;\n\t}\n\t/* else leave those fields 0 like simple_statfs */\n\treturn 0;\n}\n\n/*\n * File creation. Allocate an inode, and we're done..\n */\nstatic int\nshmem_mknod(struct inode *dir, struct dentry *dentry, umode_t mode, dev_t dev)\n{\n\tstruct inode *inode;\n\tint error = -ENOSPC;\n\n\tinode = shmem_get_inode(dir->i_sb, dir, mode, dev, VM_NORESERVE);\n\tif (inode) {\n\t\terror = security_inode_init_security(inode, dir,\n\t\t\t\t\t\t     &dentry->d_name,\n\t\t\t\t\t\t     shmem_initxattrs, NULL);\n\t\tif (error) {\n\t\t\tif (error != -EOPNOTSUPP) {\n\t\t\t\tiput(inode);\n\t\t\t\treturn error;\n\t\t\t}\n\t\t}\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t\terror = generic_acl_init(inode, dir);\n\t\tif (error) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n#else\n\t\terror = 0;\n#endif\n\t\tdir->i_size += BOGO_DIRENT_SIZE;\n\t\tdir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\t\td_instantiate(dentry, inode);\n\t\tdget(dentry); /* Extra count - pin the dentry in core */\n\t}\n\treturn error;\n}\n\nstatic int shmem_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)\n{\n\tint error;\n\n\tif ((error = shmem_mknod(dir, dentry, mode | S_IFDIR, 0)))\n\t\treturn error;\n\tinc_nlink(dir);\n\treturn 0;\n}\n\nstatic int shmem_create(struct inode *dir, struct dentry *dentry, umode_t mode,\n\t\tbool excl)\n{\n\treturn shmem_mknod(dir, dentry, mode | S_IFREG, 0);\n}\n\n/*\n * Link a file..\n */\nstatic int shmem_link(struct dentry *old_dentry, struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = old_dentry->d_inode;\n\tint ret;\n\n\t/*\n\t * No ordinary (disk based) filesystem counts links as inodes;\n\t * but each new link needs a new dentry, pinning lowmem, and\n\t * tmpfs dentries cannot be pruned until they are unlinked.\n\t */\n\tret = shmem_reserve_inode(inode->i_sb);\n\tif (ret)\n\t\tgoto out;\n\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tinode->i_ctime = dir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\tinc_nlink(inode);\n\tihold(inode);\t/* New dentry reference */\n\tdget(dentry);\t\t/* Extra pinning count for the created dentry */\n\td_instantiate(dentry, inode);\nout:\n\treturn ret;\n}\n\nstatic int shmem_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = dentry->d_inode;\n\n\tif (inode->i_nlink > 1 && !S_ISDIR(inode->i_mode))\n\t\tshmem_free_inode(inode->i_sb);\n\n\tdir->i_size -= BOGO_DIRENT_SIZE;\n\tinode->i_ctime = dir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\tdrop_nlink(inode);\n\tdput(dentry);\t/* Undo the count from \"create\" - this does all the work */\n\treturn 0;\n}\n\nstatic int shmem_rmdir(struct inode *dir, struct dentry *dentry)\n{\n\tif (!simple_empty(dentry))\n\t\treturn -ENOTEMPTY;\n\n\tdrop_nlink(dentry->d_inode);\n\tdrop_nlink(dir);\n\treturn shmem_unlink(dir, dentry);\n}\n\n/*\n * The VFS layer already does all the dentry stuff for rename,\n * we just have to decrement the usage count for the target if\n * it exists so that the VFS layer correctly free's it when it\n * gets overwritten.\n */\nstatic int shmem_rename(struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry)\n{\n\tstruct inode *inode = old_dentry->d_inode;\n\tint they_are_dirs = S_ISDIR(inode->i_mode);\n\n\tif (!simple_empty(new_dentry))\n\t\treturn -ENOTEMPTY;\n\n\tif (new_dentry->d_inode) {\n\t\t(void) shmem_unlink(new_dir, new_dentry);\n\t\tif (they_are_dirs)\n\t\t\tdrop_nlink(old_dir);\n\t} else if (they_are_dirs) {\n\t\tdrop_nlink(old_dir);\n\t\tinc_nlink(new_dir);\n\t}\n\n\told_dir->i_size -= BOGO_DIRENT_SIZE;\n\tnew_dir->i_size += BOGO_DIRENT_SIZE;\n\told_dir->i_ctime = old_dir->i_mtime =\n\tnew_dir->i_ctime = new_dir->i_mtime =\n\tinode->i_ctime = CURRENT_TIME;\n\treturn 0;\n}\n\nstatic int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *symname)\n{\n\tint error;\n\tint len;\n\tstruct inode *inode;\n\tstruct page *page;\n\tchar *kaddr;\n\tstruct shmem_inode_info *info;\n\n\tlen = strlen(symname) + 1;\n\tif (len > PAGE_CACHE_SIZE)\n\t\treturn -ENAMETOOLONG;\n\n\tinode = shmem_get_inode(dir->i_sb, dir, S_IFLNK|S_IRWXUGO, 0, VM_NORESERVE);\n\tif (!inode)\n\t\treturn -ENOSPC;\n\n\terror = security_inode_init_security(inode, dir, &dentry->d_name,\n\t\t\t\t\t     shmem_initxattrs, NULL);\n\tif (error) {\n\t\tif (error != -EOPNOTSUPP) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t\terror = 0;\n\t}\n\n\tinfo = SHMEM_I(inode);\n\tinode->i_size = len-1;\n\tif (len <= SHORT_SYMLINK_LEN) {\n\t\tinfo->symlink = kmemdup(symname, len, GFP_KERNEL);\n\t\tif (!info->symlink) {\n\t\t\tiput(inode);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tinode->i_op = &shmem_short_symlink_operations;\n\t} else {\n\t\terror = shmem_getpage(inode, 0, &page, SGP_WRITE, NULL);\n\t\tif (error) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\tinode->i_op = &shmem_symlink_inode_operations;\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(kaddr, symname, len);\n\t\tkunmap_atomic(kaddr);\n\t\tSetPageUptodate(page);\n\t\tset_page_dirty(page);\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t}\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tdir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\td_instantiate(dentry, inode);\n\tdget(dentry);\n\treturn 0;\n}\n\nstatic void *shmem_follow_short_symlink(struct dentry *dentry, struct nameidata *nd)\n{\n\tnd_set_link(nd, SHMEM_I(dentry->d_inode)->symlink);\n\treturn NULL;\n}\n\nstatic void *shmem_follow_link(struct dentry *dentry, struct nameidata *nd)\n{\n\tstruct page *page = NULL;\n\tint error = shmem_getpage(dentry->d_inode, 0, &page, SGP_READ, NULL);\n\tnd_set_link(nd, error ? ERR_PTR(error) : kmap(page));\n\tif (page)\n\t\tunlock_page(page);\n\treturn page;\n}\n\nstatic void shmem_put_link(struct dentry *dentry, struct nameidata *nd, void *cookie)\n{\n\tif (!IS_ERR(nd_get_link(nd))) {\n\t\tstruct page *page = cookie;\n\t\tkunmap(page);\n\t\tmark_page_accessed(page);\n\t\tpage_cache_release(page);\n\t}\n}\n\n#ifdef CONFIG_TMPFS_XATTR\n/*\n * Superblocks without xattr inode operations may get some security.* xattr\n * support from the LSM \"for free\". As soon as we have any other xattrs\n * like ACLs, we also need to implement the security.* handlers at\n * filesystem level, though.\n */\n\n/*\n * Callback for security_inode_init_security() for acquiring xattrs.\n */\nstatic int shmem_initxattrs(struct inode *inode,\n\t\t\t    const struct xattr *xattr_array,\n\t\t\t    void *fs_info)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tconst struct xattr *xattr;\n\tstruct simple_xattr *new_xattr;\n\tsize_t len;\n\n\tfor (xattr = xattr_array; xattr->name != NULL; xattr++) {\n\t\tnew_xattr = simple_xattr_alloc(xattr->value, xattr->value_len);\n\t\tif (!new_xattr)\n\t\t\treturn -ENOMEM;\n\n\t\tlen = strlen(xattr->name) + 1;\n\t\tnew_xattr->name = kmalloc(XATTR_SECURITY_PREFIX_LEN + len,\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!new_xattr->name) {\n\t\t\tkfree(new_xattr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tmemcpy(new_xattr->name, XATTR_SECURITY_PREFIX,\n\t\t       XATTR_SECURITY_PREFIX_LEN);\n\t\tmemcpy(new_xattr->name + XATTR_SECURITY_PREFIX_LEN,\n\t\t       xattr->name, len);\n\n\t\tsimple_xattr_list_add(&info->xattrs, new_xattr);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct xattr_handler *shmem_xattr_handlers[] = {\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t&generic_acl_access_handler,\n\t&generic_acl_default_handler,\n#endif\n\tNULL\n};\n\nstatic int shmem_xattr_validate(const char *name)\n{\n\tstruct { const char *prefix; size_t len; } arr[] = {\n\t\t{ XATTR_SECURITY_PREFIX, XATTR_SECURITY_PREFIX_LEN },\n\t\t{ XATTR_TRUSTED_PREFIX, XATTR_TRUSTED_PREFIX_LEN }\n\t};\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(arr); i++) {\n\t\tsize_t preflen = arr[i].len;\n\t\tif (strncmp(name, arr[i].prefix, preflen) == 0) {\n\t\t\tif (!name[preflen])\n\t\t\t\treturn -EINVAL;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EOPNOTSUPP;\n}\n\nstatic ssize_t shmem_getxattr(struct dentry *dentry, const char *name,\n\t\t\t      void *buffer, size_t size)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(dentry->d_inode);\n\tint err;\n\n\t/*\n\t * If this is a request for a synthetic attribute in the system.*\n\t * namespace use the generic infrastructure to resolve a handler\n\t * for it via sb->s_xattr.\n\t */\n\tif (!strncmp(name, XATTR_SYSTEM_PREFIX, XATTR_SYSTEM_PREFIX_LEN))\n\t\treturn generic_getxattr(dentry, name, buffer, size);\n\n\terr = shmem_xattr_validate(name);\n\tif (err)\n\t\treturn err;\n\n\treturn simple_xattr_get(&info->xattrs, name, buffer, size);\n}\n\nstatic int shmem_setxattr(struct dentry *dentry, const char *name,\n\t\t\t  const void *value, size_t size, int flags)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(dentry->d_inode);\n\tint err;\n\n\t/*\n\t * If this is a request for a synthetic attribute in the system.*\n\t * namespace use the generic infrastructure to resolve a handler\n\t * for it via sb->s_xattr.\n\t */\n\tif (!strncmp(name, XATTR_SYSTEM_PREFIX, XATTR_SYSTEM_PREFIX_LEN))\n\t\treturn generic_setxattr(dentry, name, value, size, flags);\n\n\terr = shmem_xattr_validate(name);\n\tif (err)\n\t\treturn err;\n\n\treturn simple_xattr_set(&info->xattrs, name, value, size, flags);\n}\n\nstatic int shmem_removexattr(struct dentry *dentry, const char *name)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(dentry->d_inode);\n\tint err;\n\n\t/*\n\t * If this is a request for a synthetic attribute in the system.*\n\t * namespace use the generic infrastructure to resolve a handler\n\t * for it via sb->s_xattr.\n\t */\n\tif (!strncmp(name, XATTR_SYSTEM_PREFIX, XATTR_SYSTEM_PREFIX_LEN))\n\t\treturn generic_removexattr(dentry, name);\n\n\terr = shmem_xattr_validate(name);\n\tif (err)\n\t\treturn err;\n\n\treturn simple_xattr_remove(&info->xattrs, name);\n}\n\nstatic ssize_t shmem_listxattr(struct dentry *dentry, char *buffer, size_t size)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(dentry->d_inode);\n\treturn simple_xattr_list(&info->xattrs, buffer, size);\n}\n#endif /* CONFIG_TMPFS_XATTR */\n\nstatic const struct inode_operations shmem_short_symlink_operations = {\n\t.readlink\t= generic_readlink,\n\t.follow_link\t= shmem_follow_short_symlink,\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n};\n\nstatic const struct inode_operations shmem_symlink_inode_operations = {\n\t.readlink\t= generic_readlink,\n\t.follow_link\t= shmem_follow_link,\n\t.put_link\t= shmem_put_link,\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n};\n\nstatic struct dentry *shmem_get_parent(struct dentry *child)\n{\n\treturn ERR_PTR(-ESTALE);\n}\n\nstatic int shmem_match(struct inode *ino, void *vfh)\n{\n\t__u32 *fh = vfh;\n\t__u64 inum = fh[2];\n\tinum = (inum << 32) | fh[1];\n\treturn ino->i_ino == inum && fh[0] == ino->i_generation;\n}\n\nstatic struct dentry *shmem_fh_to_dentry(struct super_block *sb,\n\t\tstruct fid *fid, int fh_len, int fh_type)\n{\n\tstruct inode *inode;\n\tstruct dentry *dentry = NULL;\n\tu64 inum;\n\n\tif (fh_len < 3)\n\t\treturn NULL;\n\n\tinum = fid->raw[2];\n\tinum = (inum << 32) | fid->raw[1];\n\n\tinode = ilookup5(sb, (unsigned long)(inum + fid->raw[0]),\n\t\t\tshmem_match, fid->raw);\n\tif (inode) {\n\t\tdentry = d_find_alias(inode);\n\t\tiput(inode);\n\t}\n\n\treturn dentry;\n}\n\nstatic int shmem_encode_fh(struct inode *inode, __u32 *fh, int *len,\n\t\t\t\tstruct inode *parent)\n{\n\tif (*len < 3) {\n\t\t*len = 3;\n\t\treturn 255;\n\t}\n\n\tif (inode_unhashed(inode)) {\n\t\t/* Unfortunately insert_inode_hash is not idempotent,\n\t\t * so as we hash inodes here rather than at creation\n\t\t * time, we need a lock to ensure we only try\n\t\t * to do it once\n\t\t */\n\t\tstatic DEFINE_SPINLOCK(lock);\n\t\tspin_lock(&lock);\n\t\tif (inode_unhashed(inode))\n\t\t\t__insert_inode_hash(inode,\n\t\t\t\t\t    inode->i_ino + inode->i_generation);\n\t\tspin_unlock(&lock);\n\t}\n\n\tfh[0] = inode->i_generation;\n\tfh[1] = inode->i_ino;\n\tfh[2] = ((__u64)inode->i_ino) >> 32;\n\n\t*len = 3;\n\treturn 1;\n}\n\nstatic const struct export_operations shmem_export_ops = {\n\t.get_parent     = shmem_get_parent,\n\t.encode_fh      = shmem_encode_fh,\n\t.fh_to_dentry\t= shmem_fh_to_dentry,\n};\n\nstatic int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,\n\t\t\t       bool remount)\n{\n\tchar *this_char, *value, *rest;\n\tuid_t uid;\n\tgid_t gid;\n\n\twhile (options != NULL) {\n\t\tthis_char = options;\n\t\tfor (;;) {\n\t\t\t/*\n\t\t\t * NUL-terminate this option: unfortunately,\n\t\t\t * mount options form a comma-separated list,\n\t\t\t * but mpol's nodelist may also contain commas.\n\t\t\t */\n\t\t\toptions = strchr(options, ',');\n\t\t\tif (options == NULL)\n\t\t\t\tbreak;\n\t\t\toptions++;\n\t\t\tif (!isdigit(*options)) {\n\t\t\t\toptions[-1] = '\\0';\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!*this_char)\n\t\t\tcontinue;\n\t\tif ((value = strchr(this_char,'=')) != NULL) {\n\t\t\t*value++ = 0;\n\t\t} else {\n\t\t\tprintk(KERN_ERR\n\t\t\t    \"tmpfs: No value for mount option '%s'\\n\",\n\t\t\t    this_char);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (!strcmp(this_char,\"size\")) {\n\t\t\tunsigned long long size;\n\t\t\tsize = memparse(value,&rest);\n\t\t\tif (*rest == '%') {\n\t\t\t\tsize <<= PAGE_SHIFT;\n\t\t\t\tsize *= totalram_pages;\n\t\t\t\tdo_div(size, 100);\n\t\t\t\trest++;\n\t\t\t}\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t\tsbinfo->max_blocks =\n\t\t\t\tDIV_ROUND_UP(size, PAGE_CACHE_SIZE);\n\t\t} else if (!strcmp(this_char,\"nr_blocks\")) {\n\t\t\tsbinfo->max_blocks = memparse(value, &rest);\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"nr_inodes\")) {\n\t\t\tsbinfo->max_inodes = memparse(value, &rest);\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"mode\")) {\n\t\t\tif (remount)\n\t\t\t\tcontinue;\n\t\t\tsbinfo->mode = simple_strtoul(value, &rest, 8) & 07777;\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"uid\")) {\n\t\t\tif (remount)\n\t\t\t\tcontinue;\n\t\t\tuid = simple_strtoul(value, &rest, 0);\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t\tsbinfo->uid = make_kuid(current_user_ns(), uid);\n\t\t\tif (!uid_valid(sbinfo->uid))\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"gid\")) {\n\t\t\tif (remount)\n\t\t\t\tcontinue;\n\t\t\tgid = simple_strtoul(value, &rest, 0);\n\t\t\tif (*rest)\n\t\t\t\tgoto bad_val;\n\t\t\tsbinfo->gid = make_kgid(current_user_ns(), gid);\n\t\t\tif (!gid_valid(sbinfo->gid))\n\t\t\t\tgoto bad_val;\n\t\t} else if (!strcmp(this_char,\"mpol\")) {\n\t\t\tif (mpol_parse_str(value, &sbinfo->mpol))\n\t\t\t\tgoto bad_val;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"tmpfs: Bad mount option %s\\n\",\n\t\t\t       this_char);\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n\nbad_val:\n\tprintk(KERN_ERR \"tmpfs: Bad value '%s' for mount option '%s'\\n\",\n\t       value, this_char);\n\treturn 1;\n\n}\n\nstatic int shmem_remount_fs(struct super_block *sb, int *flags, char *data)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tstruct shmem_sb_info config = *sbinfo;\n\tunsigned long inodes;\n\tint error = -EINVAL;\n\n\tconfig.mpol = NULL;\n\tif (shmem_parse_options(data, &config, true))\n\t\treturn error;\n\n\tspin_lock(&sbinfo->stat_lock);\n\tinodes = sbinfo->max_inodes - sbinfo->free_inodes;\n\tif (percpu_counter_compare(&sbinfo->used_blocks, config.max_blocks) > 0)\n\t\tgoto out;\n\tif (config.max_inodes < inodes)\n\t\tgoto out;\n\t/*\n\t * Those tests disallow limited->unlimited while any are in use;\n\t * but we must separately disallow unlimited->limited, because\n\t * in that case we have no record of how much is already in use.\n\t */\n\tif (config.max_blocks && !sbinfo->max_blocks)\n\t\tgoto out;\n\tif (config.max_inodes && !sbinfo->max_inodes)\n\t\tgoto out;\n\n\terror = 0;\n\tsbinfo->max_blocks  = config.max_blocks;\n\tsbinfo->max_inodes  = config.max_inodes;\n\tsbinfo->free_inodes = config.max_inodes - inodes;\n\n\t/*\n\t * Preserve previous mempolicy unless mpol remount option was specified.\n\t */\n\tif (config.mpol) {\n\t\tmpol_put(sbinfo->mpol);\n\t\tsbinfo->mpol = config.mpol;\t/* transfers initial ref */\n\t}\nout:\n\tspin_unlock(&sbinfo->stat_lock);\n\treturn error;\n}\n\nstatic int shmem_show_options(struct seq_file *seq, struct dentry *root)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(root->d_sb);\n\n\tif (sbinfo->max_blocks != shmem_default_max_blocks())\n\t\tseq_printf(seq, \",size=%luk\",\n\t\t\tsbinfo->max_blocks << (PAGE_CACHE_SHIFT - 10));\n\tif (sbinfo->max_inodes != shmem_default_max_inodes())\n\t\tseq_printf(seq, \",nr_inodes=%lu\", sbinfo->max_inodes);\n\tif (sbinfo->mode != (S_IRWXUGO | S_ISVTX))\n\t\tseq_printf(seq, \",mode=%03ho\", sbinfo->mode);\n\tif (!uid_eq(sbinfo->uid, GLOBAL_ROOT_UID))\n\t\tseq_printf(seq, \",uid=%u\",\n\t\t\t\tfrom_kuid_munged(&init_user_ns, sbinfo->uid));\n\tif (!gid_eq(sbinfo->gid, GLOBAL_ROOT_GID))\n\t\tseq_printf(seq, \",gid=%u\",\n\t\t\t\tfrom_kgid_munged(&init_user_ns, sbinfo->gid));\n\tshmem_show_mpol(seq, sbinfo->mpol);\n\treturn 0;\n}\n#endif /* CONFIG_TMPFS */\n\nstatic void shmem_put_super(struct super_block *sb)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\n\tpercpu_counter_destroy(&sbinfo->used_blocks);\n\tkfree(sbinfo);\n\tsb->s_fs_info = NULL;\n}\n\nint shmem_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct inode *inode;\n\tstruct shmem_sb_info *sbinfo;\n\tint err = -ENOMEM;\n\n\t/* Round up to L1_CACHE_BYTES to resist false sharing */\n\tsbinfo = kzalloc(max((int)sizeof(struct shmem_sb_info),\n\t\t\t\tL1_CACHE_BYTES), GFP_KERNEL);\n\tif (!sbinfo)\n\t\treturn -ENOMEM;\n\n\tsbinfo->mode = S_IRWXUGO | S_ISVTX;\n\tsbinfo->uid = current_fsuid();\n\tsbinfo->gid = current_fsgid();\n\tsb->s_fs_info = sbinfo;\n\n#ifdef CONFIG_TMPFS\n\t/*\n\t * Per default we only allow half of the physical ram per\n\t * tmpfs instance, limiting inodes to one per page of lowmem;\n\t * but the internal instance is left unlimited.\n\t */\n\tif (!(sb->s_flags & MS_NOUSER)) {\n\t\tsbinfo->max_blocks = shmem_default_max_blocks();\n\t\tsbinfo->max_inodes = shmem_default_max_inodes();\n\t\tif (shmem_parse_options(data, sbinfo, false)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto failed;\n\t\t}\n\t}\n\tsb->s_export_op = &shmem_export_ops;\n\tsb->s_flags |= MS_NOSEC;\n#else\n\tsb->s_flags |= MS_NOUSER;\n#endif\n\n\tspin_lock_init(&sbinfo->stat_lock);\n\tif (percpu_counter_init(&sbinfo->used_blocks, 0))\n\t\tgoto failed;\n\tsbinfo->free_inodes = sbinfo->max_inodes;\n\n\tsb->s_maxbytes = MAX_LFS_FILESIZE;\n\tsb->s_blocksize = PAGE_CACHE_SIZE;\n\tsb->s_blocksize_bits = PAGE_CACHE_SHIFT;\n\tsb->s_magic = TMPFS_MAGIC;\n\tsb->s_op = &shmem_ops;\n\tsb->s_time_gran = 1;\n#ifdef CONFIG_TMPFS_XATTR\n\tsb->s_xattr = shmem_xattr_handlers;\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\tsb->s_flags |= MS_POSIXACL;\n#endif\n\n\tinode = shmem_get_inode(sb, NULL, S_IFDIR | sbinfo->mode, 0, VM_NORESERVE);\n\tif (!inode)\n\t\tgoto failed;\n\tinode->i_uid = sbinfo->uid;\n\tinode->i_gid = sbinfo->gid;\n\tsb->s_root = d_make_root(inode);\n\tif (!sb->s_root)\n\t\tgoto failed;\n\treturn 0;\n\nfailed:\n\tshmem_put_super(sb);\n\treturn err;\n}\n\nstatic struct kmem_cache *shmem_inode_cachep;\n\nstatic struct inode *shmem_alloc_inode(struct super_block *sb)\n{\n\tstruct shmem_inode_info *info;\n\tinfo = kmem_cache_alloc(shmem_inode_cachep, GFP_KERNEL);\n\tif (!info)\n\t\treturn NULL;\n\treturn &info->vfs_inode;\n}\n\nstatic void shmem_destroy_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(shmem_inode_cachep, SHMEM_I(inode));\n}\n\nstatic void shmem_destroy_inode(struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode))\n\t\tmpol_free_shared_policy(&SHMEM_I(inode)->policy);\n\tcall_rcu(&inode->i_rcu, shmem_destroy_callback);\n}\n\nstatic void shmem_init_inode(void *foo)\n{\n\tstruct shmem_inode_info *info = foo;\n\tinode_init_once(&info->vfs_inode);\n}\n\nstatic int shmem_init_inodecache(void)\n{\n\tshmem_inode_cachep = kmem_cache_create(\"shmem_inode_cache\",\n\t\t\t\tsizeof(struct shmem_inode_info),\n\t\t\t\t0, SLAB_PANIC, shmem_init_inode);\n\treturn 0;\n}\n\nstatic void shmem_destroy_inodecache(void)\n{\n\tkmem_cache_destroy(shmem_inode_cachep);\n}\n\nstatic const struct address_space_operations shmem_aops = {\n\t.writepage\t= shmem_writepage,\n\t.set_page_dirty\t= __set_page_dirty_no_writeback,\n#ifdef CONFIG_TMPFS\n\t.write_begin\t= shmem_write_begin,\n\t.write_end\t= shmem_write_end,\n#endif\n\t.migratepage\t= migrate_page,\n\t.error_remove_page = generic_error_remove_page,\n};\n\nstatic const struct file_operations shmem_file_operations = {\n\t.mmap\t\t= shmem_mmap,\n#ifdef CONFIG_TMPFS\n\t.llseek\t\t= shmem_file_llseek,\n\t.read\t\t= do_sync_read,\n\t.write\t\t= do_sync_write,\n\t.aio_read\t= shmem_file_aio_read,\n\t.aio_write\t= generic_file_aio_write,\n\t.fsync\t\t= noop_fsync,\n\t.splice_read\t= shmem_file_splice_read,\n\t.splice_write\t= generic_file_splice_write,\n\t.fallocate\t= shmem_fallocate,\n#endif\n};\n\nstatic const struct inode_operations shmem_inode_operations = {\n\t.setattr\t= shmem_setattr,\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n};\n\nstatic const struct inode_operations shmem_dir_inode_operations = {\n#ifdef CONFIG_TMPFS\n\t.create\t\t= shmem_create,\n\t.lookup\t\t= simple_lookup,\n\t.link\t\t= shmem_link,\n\t.unlink\t\t= shmem_unlink,\n\t.symlink\t= shmem_symlink,\n\t.mkdir\t\t= shmem_mkdir,\n\t.rmdir\t\t= shmem_rmdir,\n\t.mknod\t\t= shmem_mknod,\n\t.rename\t\t= shmem_rename,\n#endif\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t.setattr\t= shmem_setattr,\n#endif\n};\n\nstatic const struct inode_operations shmem_special_inode_operations = {\n#ifdef CONFIG_TMPFS_XATTR\n\t.setxattr\t= shmem_setxattr,\n\t.getxattr\t= shmem_getxattr,\n\t.listxattr\t= shmem_listxattr,\n\t.removexattr\t= shmem_removexattr,\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t.setattr\t= shmem_setattr,\n#endif\n};\n\nstatic const struct super_operations shmem_ops = {\n\t.alloc_inode\t= shmem_alloc_inode,\n\t.destroy_inode\t= shmem_destroy_inode,\n#ifdef CONFIG_TMPFS\n\t.statfs\t\t= shmem_statfs,\n\t.remount_fs\t= shmem_remount_fs,\n\t.show_options\t= shmem_show_options,\n#endif\n\t.evict_inode\t= shmem_evict_inode,\n\t.drop_inode\t= generic_delete_inode,\n\t.put_super\t= shmem_put_super,\n};\n\nstatic const struct vm_operations_struct shmem_vm_ops = {\n\t.fault\t\t= shmem_fault,\n#ifdef CONFIG_NUMA\n\t.set_policy     = shmem_set_policy,\n\t.get_policy     = shmem_get_policy,\n#endif\n\t.remap_pages\t= generic_file_remap_pages,\n};\n\nstatic struct dentry *shmem_mount(struct file_system_type *fs_type,\n\tint flags, const char *dev_name, void *data)\n{\n\treturn mount_nodev(fs_type, flags, data, shmem_fill_super);\n}\n\nstatic struct file_system_type shmem_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"tmpfs\",\n\t.mount\t\t= shmem_mount,\n\t.kill_sb\t= kill_litter_super,\n};\n\nint __init shmem_init(void)\n{\n\tint error;\n\n\terror = bdi_init(&shmem_backing_dev_info);\n\tif (error)\n\t\tgoto out4;\n\n\terror = shmem_init_inodecache();\n\tif (error)\n\t\tgoto out3;\n\n\terror = register_filesystem(&shmem_fs_type);\n\tif (error) {\n\t\tprintk(KERN_ERR \"Could not register tmpfs\\n\");\n\t\tgoto out2;\n\t}\n\n\tshm_mnt = vfs_kern_mount(&shmem_fs_type, MS_NOUSER,\n\t\t\t\t shmem_fs_type.name, NULL);\n\tif (IS_ERR(shm_mnt)) {\n\t\terror = PTR_ERR(shm_mnt);\n\t\tprintk(KERN_ERR \"Could not kern_mount tmpfs\\n\");\n\t\tgoto out1;\n\t}\n\treturn 0;\n\nout1:\n\tunregister_filesystem(&shmem_fs_type);\nout2:\n\tshmem_destroy_inodecache();\nout3:\n\tbdi_destroy(&shmem_backing_dev_info);\nout4:\n\tshm_mnt = ERR_PTR(error);\n\treturn error;\n}\n\n#else /* !CONFIG_SHMEM */\n\n/*\n * tiny-shmem: simple shmemfs and tmpfs using ramfs code\n *\n * This is intended for small system where the benefits of the full\n * shmem code (swap-backed and resource-limited) are outweighed by\n * their complexity. On systems without swap this code should be\n * effectively equivalent, but much lighter weight.\n */\n\n#include <linux/ramfs.h>\n\nstatic struct file_system_type shmem_fs_type = {\n\t.name\t\t= \"tmpfs\",\n\t.mount\t\t= ramfs_mount,\n\t.kill_sb\t= kill_litter_super,\n};\n\nint __init shmem_init(void)\n{\n\tBUG_ON(register_filesystem(&shmem_fs_type) != 0);\n\n\tshm_mnt = kern_mount(&shmem_fs_type);\n\tBUG_ON(IS_ERR(shm_mnt));\n\n\treturn 0;\n}\n\nint shmem_unuse(swp_entry_t swap, struct page *page)\n{\n\treturn 0;\n}\n\nint shmem_lock(struct file *file, int lock, struct user_struct *user)\n{\n\treturn 0;\n}\n\nvoid shmem_unlock_mapping(struct address_space *mapping)\n{\n}\n\nvoid shmem_truncate_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\ttruncate_inode_pages_range(inode->i_mapping, lstart, lend);\n}\nEXPORT_SYMBOL_GPL(shmem_truncate_range);\n\n#define shmem_vm_ops\t\t\t\tgeneric_file_vm_ops\n#define shmem_file_operations\t\t\tramfs_file_operations\n#define shmem_get_inode(sb, dir, mode, dev, flags)\tramfs_get_inode(sb, dir, mode, dev)\n#define shmem_acct_size(flags, size)\t\t0\n#define shmem_unacct_size(flags, size)\t\tdo {} while (0)\n\n#endif /* CONFIG_SHMEM */\n\n/* common code */\n\n/**\n * shmem_file_setup - get an unlinked file living in tmpfs\n * @name: name for dentry (to be seen in /proc/<pid>/maps\n * @size: size to be set for the file\n * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size\n */\nstruct file *shmem_file_setup(const char *name, loff_t size, unsigned long flags)\n{\n\tint error;\n\tstruct file *file;\n\tstruct inode *inode;\n\tstruct path path;\n\tstruct dentry *root;\n\tstruct qstr this;\n\n\tif (IS_ERR(shm_mnt))\n\t\treturn (void *)shm_mnt;\n\n\tif (size < 0 || size > MAX_LFS_FILESIZE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (shmem_acct_size(flags, size))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terror = -ENOMEM;\n\tthis.name = name;\n\tthis.len = strlen(name);\n\tthis.hash = 0; /* will go */\n\troot = shm_mnt->mnt_root;\n\tpath.dentry = d_alloc(root, &this);\n\tif (!path.dentry)\n\t\tgoto put_memory;\n\tpath.mnt = mntget(shm_mnt);\n\n\terror = -ENOSPC;\n\tinode = shmem_get_inode(root->d_sb, NULL, S_IFREG | S_IRWXUGO, 0, flags);\n\tif (!inode)\n\t\tgoto put_dentry;\n\n\td_instantiate(path.dentry, inode);\n\tinode->i_size = size;\n\tclear_nlink(inode);\t/* It is unlinked */\n#ifndef CONFIG_MMU\n\terror = ramfs_nommu_expand_for_mapping(inode, size);\n\tif (error)\n\t\tgoto put_dentry;\n#endif\n\n\terror = -ENFILE;\n\tfile = alloc_file(&path, FMODE_WRITE | FMODE_READ,\n\t\t  &shmem_file_operations);\n\tif (!file)\n\t\tgoto put_dentry;\n\n\treturn file;\n\nput_dentry:\n\tpath_put(&path);\nput_memory:\n\tshmem_unacct_size(flags, size);\n\treturn ERR_PTR(error);\n}\nEXPORT_SYMBOL_GPL(shmem_file_setup);\n\n/**\n * shmem_zero_setup - setup a shared anonymous mapping\n * @vma: the vma to be mmapped is prepared by do_mmap_pgoff\n */\nint shmem_zero_setup(struct vm_area_struct *vma)\n{\n\tstruct file *file;\n\tloff_t size = vma->vm_end - vma->vm_start;\n\n\tfile = shmem_file_setup(\"dev/zero\", size, vma->vm_flags);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tvma->vm_file = file;\n\tvma->vm_ops = &shmem_vm_ops;\n\treturn 0;\n}\n\n/**\n * shmem_read_mapping_page_gfp - read into page cache, using specified page allocation flags.\n * @mapping:\tthe page's address_space\n * @index:\tthe page index\n * @gfp:\tthe page allocator flags to use if allocating\n *\n * This behaves as a tmpfs \"read_cache_page_gfp(mapping, index, gfp)\",\n * with any new page allocations done using the specified allocation flags.\n * But read_cache_page_gfp() uses the ->readpage() method: which does not\n * suit tmpfs, since it may have pages in swapcache, and needs to find those\n * for itself; although drivers/gpu/drm i915 and ttm rely upon this support.\n *\n * i915_gem_object_get_pages_gtt() mixes __GFP_NORETRY | __GFP_NOWARN in\n * with the mapping_gfp_mask(), to avoid OOMing the machine unnecessarily.\n */\nstruct page *shmem_read_mapping_page_gfp(struct address_space *mapping,\n\t\t\t\t\t pgoff_t index, gfp_t gfp)\n{\n#ifdef CONFIG_SHMEM\n\tstruct inode *inode = mapping->host;\n\tstruct page *page;\n\tint error;\n\n\tBUG_ON(mapping->a_ops != &shmem_aops);\n\terror = shmem_getpage_gfp(inode, index, &page, SGP_CACHE, gfp, NULL);\n\tif (error)\n\t\tpage = ERR_PTR(error);\n\telse\n\t\tunlock_page(page);\n\treturn page;\n#else\n\t/*\n\t * The tiny !SHMEM case uses ramfs without swap\n\t */\n\treturn read_cache_page_gfp(mapping, index, gfp);\n#endif\n}\nEXPORT_SYMBOL_GPL(shmem_read_mapping_page_gfp);\n"], "filenames": ["mm/shmem.c"], "buggy_code_start_loc": [2488], "buggy_code_end_loc": [2515], "fixing_code_start_loc": [2489], "fixing_code_end_loc": [2521], "type": "CWE-399", "message": "Use-after-free vulnerability in the shmem_remount_fs function in mm/shmem.c in the Linux kernel before 3.7.10 allows local users to gain privileges or cause a denial of service (system crash) by remounting a tmpfs filesystem without specifying a required mpol (aka mempolicy) mount option.", "other": {"cve": {"id": "CVE-2013-1767", "sourceIdentifier": "secalert@redhat.com", "published": "2013-02-28T19:55:01.497", "lastModified": "2023-02-13T04:41:13.570", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Use-after-free vulnerability in the shmem_remount_fs function in mm/shmem.c in the Linux kernel before 3.7.10 allows local users to gain privileges or cause a denial of service (system crash) by remounting a tmpfs filesystem without specifying a required mpol (aka mempolicy) mount option."}, {"lang": "es", "value": "Vulnerabilidad en la gesti\u00f3n de recursos en la funci\u00f3n shmem_remount_fs mm / shmem.c en el kernel de Linux 3.7.10 antes de que permite a usuarios locales obtener privilegios o causar una denegaci\u00f3n de servicio (ca\u00edda del sistema) por volver a montar un sistema de ficheros tmpfs sin especificar una opci\u00f3n de montaje necesaria  mpol (conocido como mempolicy)."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:H/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "HIGH", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.2}, "baseSeverity": "MEDIUM", "exploitabilityScore": 1.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-399"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.7.9", "matchCriteriaId": "62C6B7CA-3757-4FC5-A52F-1630EBC52571"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.35:*:*:*:*:*:*:*", "matchCriteriaId": "F72412E3-8DA9-4CC9-A426-B534202ADBA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.36:*:*:*:*:*:*:*", "matchCriteriaId": "FCAA9D7A-3C3E-4C0B-9D38-EA80E68C2E46"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.37:*:*:*:*:*:*:*", "matchCriteriaId": "4A9E3AE5-3FCF-4CBB-A30B-082BCFBFB0CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.38:*:*:*:*:*:*:*", "matchCriteriaId": "CF715657-4C3A-4392-B85D-1BBF4DE45D89"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.39:*:*:*:*:*:*:*", "matchCriteriaId": "4B63C618-AC3D-4EF7-AFDF-27B9BF482B78"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.40:*:*:*:*:*:*:*", "matchCriteriaId": "C33DA5A9-5E40-4365-9602-82FB4DCD15B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.41:*:*:*:*:*:*:*", "matchCriteriaId": "EFAFDB74-40BD-46FA-89AC-617EB2C7160B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.42:*:*:*:*:*:*:*", "matchCriteriaId": "CF5F17DA-30A7-40CF-BD7C-CEDF06D64617"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.43:*:*:*:*:*:*:*", "matchCriteriaId": "71A276F5-BD9D-4C1B-90DF-9B0C15B6F7DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.44:*:*:*:*:*:*:*", "matchCriteriaId": "F8F6EBEC-3C29-444B-BB85-6EF239B59EC1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.21:*:*:*:*:*:*:*", "matchCriteriaId": "1FF841F3-48A7-41D7-9C45-A8170435A5EB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.22:*:*:*:*:*:*:*", "matchCriteriaId": "EF506916-A6DC-4B1E-90E5-959492AF55F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.23:*:*:*:*:*:*:*", "matchCriteriaId": "B3CDAD1F-2C6A-48C0-8FAB-C2659373FA25"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.24:*:*:*:*:*:*:*", "matchCriteriaId": "4FFE4B22-C96A-43D0-B993-F51EDD9C5E0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.25:*:*:*:*:*:*:*", "matchCriteriaId": "F571CC8B-B212-4553-B463-1DB01D616E8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.26:*:*:*:*:*:*:*", "matchCriteriaId": "84E3E151-D437-48ED-A529-731EEFF88567"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.27:*:*:*:*:*:*:*", "matchCriteriaId": "E9E3EA3C-CCA5-4433-86E0-3D02C4757A0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.28:*:*:*:*:*:*:*", "matchCriteriaId": "F7AC4F7D-9FA6-4CF1-B2E9-70BF7D4D177C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.29:*:*:*:*:*:*:*", "matchCriteriaId": "3CE3A80D-9648-43CC-8F99-D741ED6552BF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.30:*:*:*:*:*:*:*", "matchCriteriaId": "C8A98C03-A465-41B4-A551-A26FEC7FFD94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "916E97D4-1FAB-42F5-826B-653B1C0909A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.5:*:*:*:*:*:*:*", "matchCriteriaId": "33FD2217-C5D0-48C1-AD74-3527127FEF9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.6:*:*:*:*:*:*:*", "matchCriteriaId": "2E92971F-B629-4E0A-9A50-8B235F9704B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.7:*:*:*:*:*:*:*", "matchCriteriaId": "EDD3A069-3829-4EE2-9D5A-29459F29D4C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.8:*:*:*:*:*:*:*", "matchCriteriaId": "A4A0964C-CEB2-41D7-A69C-1599B05B6171"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "A0F36FAC-141D-476D-84C5-A558C199F904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "51D64824-25F6-4761-BD6A-29038A143744"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "E284C8A1-740F-454D-A774-99CD3A21B594"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "C70D72AE-0CBF-4324-9935-57E28EC6279C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F674B06B-7E86-4E41-9126-8152D0DDABAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.6:*:*:*:*:*:*:*", "matchCriteriaId": "7039B3EC-8B22-413E-B582-B4BEC6181241"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.7:*:*:*:*:*:*:*", "matchCriteriaId": "35CF1DD2-80B9-4476-8963-5C3EF52B33F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.8:*:*:*:*:*:*:*", "matchCriteriaId": "BFB0B05B-A5CE-4B9C-AE7F-83062868D35B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.9:*:*:*:*:*:*:*", "matchCriteriaId": "D166A66E-7454-47EC-BB56-861A9AFEAFE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7DA94F50-2A62-4300-BF4D-A342AAE35629"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.11:*:*:*:*:*:*:*", "matchCriteriaId": "252D937B-50DC-444F-AE73-5FCF6203DF27"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.12:*:*:*:*:*:*:*", "matchCriteriaId": "F6D8EE51-02C1-47BC-A92C-0A8ABEFD28FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.13:*:*:*:*:*:*:*", "matchCriteriaId": "7F20A5D7-3B38-4911-861A-04C8310D5916"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.14:*:*:*:*:*:*:*", "matchCriteriaId": "D472DE3A-71D8-4F40-9DDE-85929A2B047D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.15:*:*:*:*:*:*:*", "matchCriteriaId": "B2AED943-65A8-4FDB-BBD0-CCEF8682A48C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.16:*:*:*:*:*:*:*", "matchCriteriaId": "D4640185-F3D8-4575-A71D-4C889A93DE2C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.17:*:*:*:*:*:*:*", "matchCriteriaId": "144CCF7C-025E-4879-B2E7-ABB8E4390BE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.18:*:*:*:*:*:*:*", "matchCriteriaId": "B6FAA052-0B2B-40CE-8C98-919B8D08A5ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.19:*:*:*:*:*:*:*", "matchCriteriaId": "4B5A53DE-9C83-4A6B-96F3-23C03BF445D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.20:*:*:*:*:*:*:*", "matchCriteriaId": "063EB879-CB05-4E33-AA90-9E43516839B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.21:*:*:*:*:*:*:*", "matchCriteriaId": "2D25764F-4B02-4C65-954E-8C7D6632DE00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.22:*:*:*:*:*:*:*", "matchCriteriaId": "F31F5BF3-CD0A-465C-857F-273841BCD28A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.23:*:*:*:*:*:*:*", "matchCriteriaId": "FF302C8A-079B-42B9-B455-CD9083BFA067"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.24:*:*:*:*:*:*:*", "matchCriteriaId": "744999C0-33D3-4363-B3DB-E0D02CDD3918"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.1:*:*:*:*:*:*:*", "matchCriteriaId": "962B0C45-AB29-4383-AC16-C6E8245D0FF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.2:*:*:*:*:*:*:*", "matchCriteriaId": "A0EE126B-74B2-4F79-BFE1-3DC169F3F9B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.3:*:*:*:*:*:*:*", "matchCriteriaId": "392075E0-A9C7-4B4A-90F9-7F1ADFF5EFA7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.4:*:*:*:*:*:*:*", "matchCriteriaId": "ECC66968-06F0-4874-A95A-A292C36E45C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.5:*:*:*:*:*:*:*", "matchCriteriaId": "5FE986E6-1068-4E1B-8EAB-DF1EAF32B4E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.6:*:*:*:*:*:*:*", "matchCriteriaId": "543E8536-1A8E-4E76-B89F-1B1F9F26FAB8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.7:*:*:*:*:*:*:*", "matchCriteriaId": "EC2B45E3-31E1-4B46-85FA-3A84E75B8F84"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6:*:*:*:*:*:*:*", "matchCriteriaId": "DDB8CC75-D3EE-417C-A83D-CB6D666FE595"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.1:*:*:*:*:*:*:*", "matchCriteriaId": "09A072F1-7BEE-4236-ACBB-55DB8FEF4A03"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.2:*:*:*:*:*:*:*", "matchCriteriaId": "E19D5A58-17D6-4502-A57A-70B2F84817A4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.3:*:*:*:*:*:*:*", "matchCriteriaId": "D58BA035-1204-4DFA-98A1-12111FB6222E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.4:*:*:*:*:*:*:*", "matchCriteriaId": "A17F2E87-8EB8-476A-B5B5-9AE5CF53D9FE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.5:*:*:*:*:*:*:*", "matchCriteriaId": "A8CCC101-5852-4299-9B67-EA1B149D58C0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.6:*:*:*:*:*:*:*", "matchCriteriaId": "B8074D32-C252-4AD3-A579-1C5EDDD7014B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.7:*:*:*:*:*:*:*", "matchCriteriaId": "962AA802-8179-4606-AAC0-9363BAEABC9F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.8:*:*:*:*:*:*:*", "matchCriteriaId": "1286C858-D5A2-45F3-86D1-E50FE53FB23C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.9:*:*:*:*:*:*:*", "matchCriteriaId": "5AC4A13E-F560-4D01-98A3-E2A2B82EB25B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.10:*:*:*:*:*:*:*", "matchCriteriaId": "942C462A-5398-4BB9-A792-598682E1FEF2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.11:*:*:*:*:*:*:*", "matchCriteriaId": "B852F7E0-0282-483D-BB4D-18CB7A4F1392"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7:*:*:*:*:*:*:*", "matchCriteriaId": "53ED9A31-99CC-41C8-8B72-5B2A9B49AA6C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.1:*:*:*:*:*:*:*", "matchCriteriaId": "EFD646BC-62F7-47CF-B0BE-768F701F7D9A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.2:*:*:*:*:*:*:*", "matchCriteriaId": "F43D418E-87C1-4C83-9FF1-4F45B4F452DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.3:*:*:*:*:*:*:*", "matchCriteriaId": "680D0E00-F29A-487C-8770-8E7EAC672B7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.4:*:*:*:*:*:*:*", "matchCriteriaId": "2DCA96A4-A836-4E94-A39C-3AD3EA1D9611"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.5:*:*:*:*:*:*:*", "matchCriteriaId": "753C05E3-B603-4E36-B9BA-FAEDCBF62A7D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.6:*:*:*:*:*:*:*", "matchCriteriaId": "E385C2E0-B9F1-4564-8E6D-56FD9E762405"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.7:*:*:*:*:*:*:*", "matchCriteriaId": "041335D4-05E1-4004-9381-28AAD5994B47"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.8:*:*:*:*:*:*:*", "matchCriteriaId": "370F2AE5-3DBC-46B9-AC70-F052C9229C00"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=5f00110f7273f9ff04ac69a5f85bb535a4fd0987", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2013-05/msg00018.html", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2013-06/msg00005.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-0744.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-0882.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-0928.html", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.7.10", "source": "secalert@redhat.com"}, {"url": "http://www.mandriva.com/security/advisories?name=MDVSA-2013:176", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/02/25/23", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1787-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1788-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1792-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1793-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1794-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1795-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1796-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1797-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1798-1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=915592", "source": "secalert@redhat.com", "tags": ["Patch"]}, {"url": "https://github.com/torvalds/linux/commit/5f00110f7273f9ff04ac69a5f85bb535a4fd0987", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/5f00110f7273f9ff04ac69a5f85bb535a4fd0987"}}