{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/ext4/inode.c\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/fs/minix/inode.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  64-bit file support on 64-bit platforms by Jakub Jelinek\n *\t(jj@sunsite.ms.mff.cuni.cz)\n *\n *  Assorted race fixes, rewrite of ext4_get_block() by Al Viro, 2000\n */\n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/dax.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/buffer_head.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/mpage.h>\n#include <linux/namei.h>\n#include <linux/uio.h>\n#include <linux/bio.h>\n#include <linux/workqueue.h>\n#include <linux/kernel.h>\n#include <linux/printk.h>\n#include <linux/slab.h>\n#include <linux/bitops.h>\n#include <linux/iomap.h>\n#include <linux/iversion.h>\n\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n#include \"truncate.h\"\n\n#include <trace/events/ext4.h>\n\n#define MPAGE_DA_EXTENT_TAIL 0x01\n\nstatic __u32 ext4_inode_csum(struct inode *inode, struct ext4_inode *raw,\n\t\t\t      struct ext4_inode_info *ei)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\t__u16 dummy_csum = 0;\n\tint offset = offsetof(struct ext4_inode, i_checksum_lo);\n\tunsigned int csum_size = sizeof(dummy_csum);\n\n\tcsum = ext4_chksum(sbi, ei->i_csum_seed, (__u8 *)raw, offset);\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)&dummy_csum, csum_size);\n\toffset += csum_size;\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)raw + offset,\n\t\t\t   EXT4_GOOD_OLD_INODE_SIZE - offset);\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\toffset = offsetof(struct ext4_inode, i_checksum_hi);\n\t\tcsum = ext4_chksum(sbi, csum, (__u8 *)raw +\n\t\t\t\t   EXT4_GOOD_OLD_INODE_SIZE,\n\t\t\t\t   offset - EXT4_GOOD_OLD_INODE_SIZE);\n\t\tif (EXT4_FITS_IN_INODE(raw, ei, i_checksum_hi)) {\n\t\t\tcsum = ext4_chksum(sbi, csum, (__u8 *)&dummy_csum,\n\t\t\t\t\t   csum_size);\n\t\t\toffset += csum_size;\n\t\t}\n\t\tcsum = ext4_chksum(sbi, csum, (__u8 *)raw + offset,\n\t\t\t\t   EXT4_INODE_SIZE(inode->i_sb) - offset);\n\t}\n\n\treturn csum;\n}\n\nstatic int ext4_inode_csum_verify(struct inode *inode, struct ext4_inode *raw,\n\t\t\t\t  struct ext4_inode_info *ei)\n{\n\t__u32 provided, calculated;\n\n\tif (EXT4_SB(inode->i_sb)->s_es->s_creator_os !=\n\t    cpu_to_le32(EXT4_OS_LINUX) ||\n\t    !ext4_has_metadata_csum(inode->i_sb))\n\t\treturn 1;\n\n\tprovided = le16_to_cpu(raw->i_checksum_lo);\n\tcalculated = ext4_inode_csum(inode, raw, ei);\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw, ei, i_checksum_hi))\n\t\tprovided |= ((__u32)le16_to_cpu(raw->i_checksum_hi)) << 16;\n\telse\n\t\tcalculated &= 0xFFFF;\n\n\treturn provided == calculated;\n}\n\nstatic void ext4_inode_csum_set(struct inode *inode, struct ext4_inode *raw,\n\t\t\t\tstruct ext4_inode_info *ei)\n{\n\t__u32 csum;\n\n\tif (EXT4_SB(inode->i_sb)->s_es->s_creator_os !=\n\t    cpu_to_le32(EXT4_OS_LINUX) ||\n\t    !ext4_has_metadata_csum(inode->i_sb))\n\t\treturn;\n\n\tcsum = ext4_inode_csum(inode, raw, ei);\n\traw->i_checksum_lo = cpu_to_le16(csum & 0xFFFF);\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw, ei, i_checksum_hi))\n\t\traw->i_checksum_hi = cpu_to_le16(csum >> 16);\n}\n\nstatic inline int ext4_begin_ordered_truncate(struct inode *inode,\n\t\t\t\t\t      loff_t new_size)\n{\n\ttrace_ext4_begin_ordered_truncate(inode, new_size);\n\t/*\n\t * If jinode is zero, then we never opened the file for\n\t * writing, so there's no need to call\n\t * jbd2_journal_begin_ordered_truncate() since there's no\n\t * outstanding writes we need to flush.\n\t */\n\tif (!EXT4_I(inode)->jinode)\n\t\treturn 0;\n\treturn jbd2_journal_begin_ordered_truncate(EXT4_JOURNAL(inode),\n\t\t\t\t\t\t   EXT4_I(inode)->jinode,\n\t\t\t\t\t\t   new_size);\n}\n\nstatic void ext4_invalidatepage(struct page *page, unsigned int offset,\n\t\t\t\tunsigned int length);\nstatic int __ext4_journalled_writepage(struct page *page, unsigned int len);\nstatic int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh);\nstatic int ext4_meta_trans_blocks(struct inode *inode, int lblocks,\n\t\t\t\t  int pextents);\n\n/*\n * Test whether an inode is a fast symlink.\n * A fast symlink has its symlink data stored in ext4_inode_info->i_data.\n */\nint ext4_inode_is_fast_symlink(struct inode *inode)\n{\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL)) {\n\t\tint ea_blocks = EXT4_I(inode)->i_file_acl ?\n\t\t\t\tEXT4_CLUSTER_SIZE(inode->i_sb) >> 9 : 0;\n\n\t\tif (ext4_has_inline_data(inode))\n\t\t\treturn 0;\n\n\t\treturn (S_ISLNK(inode->i_mode) && inode->i_blocks - ea_blocks == 0);\n\t}\n\treturn S_ISLNK(inode->i_mode) && inode->i_size &&\n\t       (inode->i_size < EXT4_N_BLOCKS * 4);\n}\n\n/*\n * Called at the last iput() if i_nlink is zero.\n */\nvoid ext4_evict_inode(struct inode *inode)\n{\n\thandle_t *handle;\n\tint err;\n\t/*\n\t * Credits for final inode cleanup and freeing:\n\t * sb + inode (ext4_orphan_del()), block bitmap, group descriptor\n\t * (xattr block freeing), bitmap, group descriptor (inode freeing)\n\t */\n\tint extra_credits = 6;\n\tstruct ext4_xattr_inode_array *ea_inode_array = NULL;\n\n\ttrace_ext4_evict_inode(inode);\n\n\tif (inode->i_nlink) {\n\t\t/*\n\t\t * When journalling data dirty buffers are tracked only in the\n\t\t * journal. So although mm thinks everything is clean and\n\t\t * ready for reaping the inode might still have some pages to\n\t\t * write in the running transaction or waiting to be\n\t\t * checkpointed. Thus calling jbd2_journal_invalidatepage()\n\t\t * (via truncate_inode_pages()) to discard these buffers can\n\t\t * cause data loss. Also even if we did not discard these\n\t\t * buffers, we would have no way to find them after the inode\n\t\t * is reaped and thus user could see stale data if he tries to\n\t\t * read them before the transaction is checkpointed. So be\n\t\t * careful and force everything to disk here... We use\n\t\t * ei->i_datasync_tid to store the newest transaction\n\t\t * containing inode's data.\n\t\t *\n\t\t * Note that directories do not have this problem because they\n\t\t * don't use page cache.\n\t\t */\n\t\tif (inode->i_ino != EXT4_JOURNAL_INO &&\n\t\t    ext4_should_journal_data(inode) &&\n\t\t    (S_ISLNK(inode->i_mode) || S_ISREG(inode->i_mode)) &&\n\t\t    inode->i_data.nrpages) {\n\t\t\tjournal_t *journal = EXT4_SB(inode->i_sb)->s_journal;\n\t\t\ttid_t commit_tid = EXT4_I(inode)->i_datasync_tid;\n\n\t\t\tjbd2_complete_transaction(journal, commit_tid);\n\t\t\tfilemap_write_and_wait(&inode->i_data);\n\t\t}\n\t\ttruncate_inode_pages_final(&inode->i_data);\n\n\t\tgoto no_delete;\n\t}\n\n\tif (is_bad_inode(inode))\n\t\tgoto no_delete;\n\tdquot_initialize(inode);\n\n\tif (ext4_should_order_data(inode))\n\t\text4_begin_ordered_truncate(inode, 0);\n\ttruncate_inode_pages_final(&inode->i_data);\n\n\t/*\n\t * Protect us against freezing - iput() caller didn't have to have any\n\t * protection against it\n\t */\n\tsb_start_intwrite(inode->i_sb);\n\n\tif (!IS_NOQUOTA(inode))\n\t\textra_credits += EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb);\n\n\t/*\n\t * Block bitmap, group descriptor, and inode are accounted in both\n\t * ext4_blocks_for_truncate() and extra_credits. So subtract 3.\n\t */\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE,\n\t\t\t ext4_blocks_for_truncate(inode) + extra_credits - 3);\n\tif (IS_ERR(handle)) {\n\t\text4_std_error(inode->i_sb, PTR_ERR(handle));\n\t\t/*\n\t\t * If we're going to skip the normal cleanup, we still need to\n\t\t * make sure that the in-core orphan linked list is properly\n\t\t * cleaned up.\n\t\t */\n\t\text4_orphan_del(NULL, inode);\n\t\tsb_end_intwrite(inode->i_sb);\n\t\tgoto no_delete;\n\t}\n\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\t/*\n\t * Set inode->i_size to 0 before calling ext4_truncate(). We need\n\t * special handling of symlinks here because i_size is used to\n\t * determine whether ext4_inode_info->i_data contains symlink data or\n\t * block mappings. Setting i_size to 0 will remove its fast symlink\n\t * status. Erase i_data so that it becomes a valid empty block map.\n\t */\n\tif (ext4_inode_is_fast_symlink(inode))\n\t\tmemset(EXT4_I(inode)->i_data, 0, sizeof(EXT4_I(inode)->i_data));\n\tinode->i_size = 0;\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err) {\n\t\text4_warning(inode->i_sb,\n\t\t\t     \"couldn't mark inode dirty (err %d)\", err);\n\t\tgoto stop_handle;\n\t}\n\tif (inode->i_blocks) {\n\t\terr = ext4_truncate(inode);\n\t\tif (err) {\n\t\t\text4_error(inode->i_sb,\n\t\t\t\t   \"couldn't truncate inode %lu (err %d)\",\n\t\t\t\t   inode->i_ino, err);\n\t\t\tgoto stop_handle;\n\t\t}\n\t}\n\n\t/* Remove xattr references. */\n\terr = ext4_xattr_delete_inode(handle, inode, &ea_inode_array,\n\t\t\t\t      extra_credits);\n\tif (err) {\n\t\text4_warning(inode->i_sb, \"xattr delete (err %d)\", err);\nstop_handle:\n\t\text4_journal_stop(handle);\n\t\text4_orphan_del(NULL, inode);\n\t\tsb_end_intwrite(inode->i_sb);\n\t\text4_xattr_inode_array_free(ea_inode_array);\n\t\tgoto no_delete;\n\t}\n\n\t/*\n\t * Kill off the orphan record which ext4_truncate created.\n\t * AKPM: I think this can be inside the above `if'.\n\t * Note that ext4_orphan_del() has to be able to cope with the\n\t * deletion of a non-existent orphan - this is because we don't\n\t * know if ext4_truncate() actually created an orphan record.\n\t * (Well, we could do this if we need to, but heck - it works)\n\t */\n\text4_orphan_del(handle, inode);\n\tEXT4_I(inode)->i_dtime\t= (__u32)ktime_get_real_seconds();\n\n\t/*\n\t * One subtle ordering requirement: if anything has gone wrong\n\t * (transaction abort, IO errors, whatever), then we can still\n\t * do these next steps (the fs will already have been marked as\n\t * having errors), but we can't free the inode if the mark_dirty\n\t * fails.\n\t */\n\tif (ext4_mark_inode_dirty(handle, inode))\n\t\t/* If that failed, just do the required in-core inode clear. */\n\t\text4_clear_inode(inode);\n\telse\n\t\text4_free_inode(handle, inode);\n\text4_journal_stop(handle);\n\tsb_end_intwrite(inode->i_sb);\n\text4_xattr_inode_array_free(ea_inode_array);\n\treturn;\nno_delete:\n\text4_clear_inode(inode);\t/* We must guarantee clearing of inode... */\n}\n\n#ifdef CONFIG_QUOTA\nqsize_t *ext4_get_reserved_space(struct inode *inode)\n{\n\treturn &EXT4_I(inode)->i_reserved_quota;\n}\n#endif\n\n/*\n * Called with i_data_sem down, which is important since we can call\n * ext4_discard_preallocations() from here.\n */\nvoid ext4_da_update_reserve_space(struct inode *inode,\n\t\t\t\t\tint used, int quota_claim)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tspin_lock(&ei->i_block_reservation_lock);\n\ttrace_ext4_da_update_reserve_space(inode, used, quota_claim);\n\tif (unlikely(used > ei->i_reserved_data_blocks)) {\n\t\text4_warning(inode->i_sb, \"%s: ino %lu, used %d \"\n\t\t\t \"with only %d reserved data blocks\",\n\t\t\t __func__, inode->i_ino, used,\n\t\t\t ei->i_reserved_data_blocks);\n\t\tWARN_ON(1);\n\t\tused = ei->i_reserved_data_blocks;\n\t}\n\n\t/* Update per-inode reservations */\n\tei->i_reserved_data_blocks -= used;\n\tpercpu_counter_sub(&sbi->s_dirtyclusters_counter, used);\n\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\t/* Update quota subsystem for data blocks */\n\tif (quota_claim)\n\t\tdquot_claim_block(inode, EXT4_C2B(sbi, used));\n\telse {\n\t\t/*\n\t\t * We did fallocate with an offset that is already delayed\n\t\t * allocated. So on delayed allocated writeback we should\n\t\t * not re-claim the quota for fallocated blocks.\n\t\t */\n\t\tdquot_release_reservation_block(inode, EXT4_C2B(sbi, used));\n\t}\n\n\t/*\n\t * If we have done all the pending block allocations and if\n\t * there aren't any writers on the inode, we can discard the\n\t * inode's preallocations.\n\t */\n\tif ((ei->i_reserved_data_blocks == 0) &&\n\t    !inode_is_open_for_write(inode))\n\t\text4_discard_preallocations(inode);\n}\n\nstatic int __check_block_validity(struct inode *inode, const char *func,\n\t\t\t\tunsigned int line,\n\t\t\t\tstruct ext4_map_blocks *map)\n{\n\tif (ext4_has_feature_journal(inode->i_sb) &&\n\t    (inode->i_ino ==\n\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))\n\t\treturn 0;\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n\t\t\t\t   map->m_len)) {\n\t\text4_error_inode(inode, func, line, map->m_pblk,\n\t\t\t\t \"lblock %lu mapped to illegal pblock %llu \"\n\t\t\t\t \"(length %d)\", (unsigned long) map->m_lblk,\n\t\t\t\t map->m_pblk, map->m_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}\n\nint ext4_issue_zeroout(struct inode *inode, ext4_lblk_t lblk, ext4_fsblk_t pblk,\n\t\t       ext4_lblk_t len)\n{\n\tint ret;\n\n\tif (IS_ENCRYPTED(inode))\n\t\treturn fscrypt_zeroout_range(inode, lblk, pblk, len);\n\n\tret = sb_issue_zeroout(inode->i_sb, pblk, len, GFP_NOFS);\n\tif (ret > 0)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n#define check_block_validity(inode, map)\t\\\n\t__check_block_validity((inode), __func__, __LINE__, (map))\n\n#ifdef ES_AGGRESSIVE_TEST\nstatic void ext4_map_blocks_es_recheck(handle_t *handle,\n\t\t\t\t       struct inode *inode,\n\t\t\t\t       struct ext4_map_blocks *es_map,\n\t\t\t\t       struct ext4_map_blocks *map,\n\t\t\t\t       int flags)\n{\n\tint retval;\n\n\tmap->m_flags = 0;\n\t/*\n\t * There is a race window that the result is not the same.\n\t * e.g. xfstests #223 when dioread_nolock enables.  The reason\n\t * is that we lookup a block mapping in extent status tree with\n\t * out taking i_data_sem.  So at the time the unwritten extent\n\t * could be converted.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\n\t/*\n\t * We don't check m_len because extent will be collpased in status\n\t * tree.  So the m_len might not equal.\n\t */\n\tif (es_map->m_lblk != map->m_lblk ||\n\t    es_map->m_flags != map->m_flags ||\n\t    es_map->m_pblk != map->m_pblk) {\n\t\tprintk(\"ES cache assertion failed for inode: %lu \"\n\t\t       \"es_cached ex [%d/%d/%llu/%x] != \"\n\t\t       \"found ex [%d/%d/%llu/%x] retval %d flags %x\\n\",\n\t\t       inode->i_ino, es_map->m_lblk, es_map->m_len,\n\t\t       es_map->m_pblk, es_map->m_flags, map->m_lblk,\n\t\t       map->m_len, map->m_pblk, map->m_flags,\n\t\t       retval, flags);\n\t}\n}\n#endif /* ES_AGGRESSIVE_TEST */\n\n/*\n * The ext4_map_blocks() function tries to look up the requested blocks,\n * and returns if the blocks are already mapped.\n *\n * Otherwise it takes the write lock of the i_data_sem and allocate blocks\n * and store the allocated blocks in the result buffer head and mark it\n * mapped.\n *\n * If file type is extents based, it will call ext4_ext_map_blocks(),\n * Otherwise, call with ext4_ind_map_blocks() to handle indirect mapping\n * based files\n *\n * On success, it returns the number of blocks being mapped or allocated.  if\n * create==0 and the blocks are pre-allocated and unwritten, the resulting @map\n * is marked as unwritten. If the create == 1, it will mark @map as mapped.\n *\n * It returns 0 if plain look up failed (blocks have not been allocated), in\n * that case, @map is returned as unmapped but we still do fill map->m_len to\n * indicate the length of a hole starting at map->m_lblk.\n *\n * It returns the error in case of allocation failure.\n */\nint ext4_map_blocks(handle_t *handle, struct inode *inode,\n\t\t    struct ext4_map_blocks *map, int flags)\n{\n\tstruct extent_status es;\n\tint retval;\n\tint ret = 0;\n#ifdef ES_AGGRESSIVE_TEST\n\tstruct ext4_map_blocks orig_map;\n\n\tmemcpy(&orig_map, map, sizeof(*map));\n#endif\n\n\tmap->m_flags = 0;\n\text_debug(\"ext4_map_blocks(): inode %lu, flag %d, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, flags, map->m_len,\n\t\t  (unsigned long) map->m_lblk);\n\n\t/*\n\t * ext4_map_blocks returns an int, and m_len is an unsigned int\n\t */\n\tif (unlikely(map->m_len > INT_MAX))\n\t\tmap->m_len = INT_MAX;\n\n\t/* We can handle the block number less than EXT_MAX_BLOCKS */\n\tif (unlikely(map->m_lblk >= EXT_MAX_BLOCKS))\n\t\treturn -EFSCORRUPTED;\n\n\t/* Lookup extent status tree firstly */\n\tif (ext4_es_lookup_extent(inode, map->m_lblk, NULL, &es)) {\n\t\tif (ext4_es_is_written(&es) || ext4_es_is_unwritten(&es)) {\n\t\t\tmap->m_pblk = ext4_es_pblock(&es) +\n\t\t\t\t\tmap->m_lblk - es.es_lblk;\n\t\t\tmap->m_flags |= ext4_es_is_written(&es) ?\n\t\t\t\t\tEXT4_MAP_MAPPED : EXT4_MAP_UNWRITTEN;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t} else if (ext4_es_is_delayed(&es) || ext4_es_is_hole(&es)) {\n\t\t\tmap->m_pblk = 0;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t\tretval = 0;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n#ifdef ES_AGGRESSIVE_TEST\n\t\text4_map_blocks_es_recheck(handle, inode, map,\n\t\t\t\t\t   &orig_map, flags);\n#endif\n\t\tgoto found;\n\t}\n\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t}\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_es_scan_range(inode, &ext4_es_is_delayed, map->m_lblk,\n\t\t\t\t       map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk,\n\t\t\t\t\t    map->m_len, map->m_pblk, status);\n\t\tif (ret < 0)\n\t\t\tretval = ret;\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\nfound:\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t/* If it is only a block(s) look up */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0)\n\t\treturn retval;\n\n\t/*\n\t * Returns if the blocks have already allocated\n\t *\n\t * Note that if blocks have been preallocated\n\t * ext4_ext_get_block() returns the create = 0\n\t * with buffer head unmapped.\n\t */\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED)\n\t\t/*\n\t\t * If we need to convert extent to unwritten\n\t\t * we continue and do the actual work in\n\t\t * ext4_ext_map_blocks()\n\t\t */\n\t\tif (!(flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN))\n\t\t\treturn retval;\n\n\t/*\n\t * Here we clear m_flags because after allocating an new extent,\n\t * it will be set again.\n\t */\n\tmap->m_flags &= ~EXT4_MAP_FLAGS;\n\n\t/*\n\t * New blocks allocate and/or writing to unwritten extent\n\t * will possibly result in updating i_data, so we take\n\t * the write lock of i_data_sem, and call get_block()\n\t * with create == 1 flag.\n\t */\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\n\t/*\n\t * We need to check for EXT4 here because migrate\n\t * could have changed the inode type in between\n\t */\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags);\n\n\t\tif (retval > 0 && map->m_flags & EXT4_MAP_NEW) {\n\t\t\t/*\n\t\t\t * We allocated new blocks which will result in\n\t\t\t * i_data's format changing.  Force the migrate\n\t\t\t * to fail by clearing migrate flags\n\t\t\t */\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);\n\t\t}\n\n\t\t/*\n\t\t * Update reserved blocks/metadata blocks after successful\n\t\t * block allocation which had been deferred till now. We don't\n\t\t * support fallocate for non extent files. So we can update\n\t\t * reserve space here.\n\t\t */\n\t\tif ((retval > 0) &&\n\t\t\t(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))\n\t\t\text4_da_update_reserve_space(inode, retval, 1);\n\t}\n\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\t/*\n\t\t * We have to zeroout blocks before inserting them into extent\n\t\t * status tree. Otherwise someone could look them up there and\n\t\t * use them before they are really zeroed. We also have to\n\t\t * unmap metadata before zeroing as otherwise writeback can\n\t\t * overwrite zeros with stale data from block device.\n\t\t */\n\t\tif (flags & EXT4_GET_BLOCKS_ZERO &&\n\t\t    map->m_flags & EXT4_MAP_MAPPED &&\n\t\t    map->m_flags & EXT4_MAP_NEW) {\n\t\t\tret = ext4_issue_zeroout(inode, map->m_lblk,\n\t\t\t\t\t\t map->m_pblk, map->m_len);\n\t\t\tif (ret) {\n\t\t\t\tretval = ret;\n\t\t\t\tgoto out_sem;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * If the extent has been zeroed out, we don't need to update\n\t\t * extent status tree.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO) &&\n\t\t    ext4_es_lookup_extent(inode, map->m_lblk, NULL, &es)) {\n\t\t\tif (ext4_es_is_written(&es))\n\t\t\t\tgoto out_sem;\n\t\t}\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_es_scan_range(inode, &ext4_es_is_delayed, map->m_lblk,\n\t\t\t\t       map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk, map->m_len,\n\t\t\t\t\t    map->m_pblk, status);\n\t\tif (ret < 0) {\n\t\t\tretval = ret;\n\t\t\tgoto out_sem;\n\t\t}\n\t}\n\nout_sem:\n\tup_write((&EXT4_I(inode)->i_data_sem));\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * Inodes with freshly allocated blocks where contents will be\n\t\t * visible after transaction commit must be on transaction's\n\t\t * ordered data list.\n\t\t */\n\t\tif (map->m_flags & EXT4_MAP_NEW &&\n\t\t    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&\n\t\t    !(flags & EXT4_GET_BLOCKS_ZERO) &&\n\t\t    !ext4_is_quota_file(inode) &&\n\t\t    ext4_should_order_data(inode)) {\n\t\t\tloff_t start_byte =\n\t\t\t\t(loff_t)map->m_lblk << inode->i_blkbits;\n\t\t\tloff_t length = (loff_t)map->m_len << inode->i_blkbits;\n\n\t\t\tif (flags & EXT4_GET_BLOCKS_IO_SUBMIT)\n\t\t\t\tret = ext4_jbd2_inode_add_wait(handle, inode,\n\t\t\t\t\t\tstart_byte, length);\n\t\t\telse\n\t\t\t\tret = ext4_jbd2_inode_add_write(handle, inode,\n\t\t\t\t\t\tstart_byte, length);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\treturn retval;\n}\n\n/*\n * Update EXT4_MAP_FLAGS in bh->b_state. For buffer heads attached to pages\n * we have to be careful as someone else may be manipulating b_state as well.\n */\nstatic void ext4_update_bh_state(struct buffer_head *bh, unsigned long flags)\n{\n\tunsigned long old_state;\n\tunsigned long new_state;\n\n\tflags &= EXT4_MAP_FLAGS;\n\n\t/* Dummy buffer_head? Set non-atomically. */\n\tif (!bh->b_page) {\n\t\tbh->b_state = (bh->b_state & ~EXT4_MAP_FLAGS) | flags;\n\t\treturn;\n\t}\n\t/*\n\t * Someone else may be modifying b_state. Be careful! This is ugly but\n\t * once we get rid of using bh as a container for mapping information\n\t * to pass to / from get_block functions, this can go away.\n\t */\n\tdo {\n\t\told_state = READ_ONCE(bh->b_state);\n\t\tnew_state = (old_state & ~EXT4_MAP_FLAGS) | flags;\n\t} while (unlikely(\n\t\t cmpxchg(&bh->b_state, old_state, new_state) != old_state));\n}\n\nstatic int _ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t\t   struct buffer_head *bh, int flags)\n{\n\tstruct ext4_map_blocks map;\n\tint ret = 0;\n\n\tif (ext4_has_inline_data(inode))\n\t\treturn -ERANGE;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\n\tret = ext4_map_blocks(ext4_journal_current_handle(), inode, &map,\n\t\t\t      flags);\n\tif (ret > 0) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\text4_update_bh_state(bh, map.m_flags);\n\t\tbh->b_size = inode->i_sb->s_blocksize * map.m_len;\n\t\tret = 0;\n\t} else if (ret == 0) {\n\t\t/* hole case, need to fill in bh->b_size */\n\t\tbh->b_size = inode->i_sb->s_blocksize * map.m_len;\n\t}\n\treturn ret;\n}\n\nint ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t   struct buffer_head *bh, int create)\n{\n\treturn _ext4_get_block(inode, iblock, bh,\n\t\t\t       create ? EXT4_GET_BLOCKS_CREATE : 0);\n}\n\n/*\n * Get block function used when preparing for buffered write if we require\n * creating an unwritten extent if blocks haven't been allocated.  The extent\n * will be converted to written after the IO is complete.\n */\nint ext4_get_block_unwritten(struct inode *inode, sector_t iblock,\n\t\t\t     struct buffer_head *bh_result, int create)\n{\n\text4_debug(\"ext4_get_block_unwritten: inode %lu, create flag %d\\n\",\n\t\t   inode->i_ino, create);\n\treturn _ext4_get_block(inode, iblock, bh_result,\n\t\t\t       EXT4_GET_BLOCKS_IO_CREATE_EXT);\n}\n\n/* Maximum number of blocks we map for direct IO at once. */\n#define DIO_MAX_BLOCKS 4096\n\n/*\n * `handle' can be NULL if create is zero\n */\nstruct buffer_head *ext4_getblk(handle_t *handle, struct inode *inode,\n\t\t\t\text4_lblk_t block, int map_flags)\n{\n\tstruct ext4_map_blocks map;\n\tstruct buffer_head *bh;\n\tint create = map_flags & EXT4_GET_BLOCKS_CREATE;\n\tint err;\n\n\tJ_ASSERT(handle != NULL || create == 0);\n\n\tmap.m_lblk = block;\n\tmap.m_len = 1;\n\terr = ext4_map_blocks(handle, inode, &map, map_flags);\n\n\tif (err == 0)\n\t\treturn create ? ERR_PTR(-ENOSPC) : NULL;\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\tbh = sb_getblk(inode->i_sb, map.m_pblk);\n\tif (unlikely(!bh))\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (map.m_flags & EXT4_MAP_NEW) {\n\t\tJ_ASSERT(create != 0);\n\t\tJ_ASSERT(handle != NULL);\n\n\t\t/*\n\t\t * Now that we do not always journal data, we should\n\t\t * keep in mind whether this should always journal the\n\t\t * new buffer as metadata.  For now, regular file\n\t\t * writes use ext4_get_block instead, so it's not a\n\t\t * problem.\n\t\t */\n\t\tlock_buffer(bh);\n\t\tBUFFER_TRACE(bh, \"call get_create_access\");\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (unlikely(err)) {\n\t\t\tunlock_buffer(bh);\n\t\t\tgoto errout;\n\t\t}\n\t\tif (!buffer_uptodate(bh)) {\n\t\t\tmemset(bh->b_data, 0, inode->i_sb->s_blocksize);\n\t\t\tset_buffer_uptodate(bh);\n\t\t}\n\t\tunlock_buffer(bh);\n\t\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (unlikely(err))\n\t\t\tgoto errout;\n\t} else\n\t\tBUFFER_TRACE(bh, \"not a new buffer\");\n\treturn bh;\nerrout:\n\tbrelse(bh);\n\treturn ERR_PTR(err);\n}\n\nstruct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,\n\t\t\t       ext4_lblk_t block, int map_flags)\n{\n\tstruct buffer_head *bh;\n\n\tbh = ext4_getblk(handle, inode, block, map_flags);\n\tif (IS_ERR(bh))\n\t\treturn bh;\n\tif (!bh || ext4_buffer_uptodate(bh))\n\t\treturn bh;\n\tll_rw_block(REQ_OP_READ, REQ_META | REQ_PRIO, 1, &bh);\n\twait_on_buffer(bh);\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tput_bh(bh);\n\treturn ERR_PTR(-EIO);\n}\n\n/* Read a contiguous batch of blocks. */\nint ext4_bread_batch(struct inode *inode, ext4_lblk_t block, int bh_count,\n\t\t     bool wait, struct buffer_head **bhs)\n{\n\tint i, err;\n\n\tfor (i = 0; i < bh_count; i++) {\n\t\tbhs[i] = ext4_getblk(NULL, inode, block + i, 0 /* map_flags */);\n\t\tif (IS_ERR(bhs[i])) {\n\t\t\terr = PTR_ERR(bhs[i]);\n\t\t\tbh_count = i;\n\t\t\tgoto out_brelse;\n\t\t}\n\t}\n\n\tfor (i = 0; i < bh_count; i++)\n\t\t/* Note that NULL bhs[i] is valid because of holes. */\n\t\tif (bhs[i] && !ext4_buffer_uptodate(bhs[i]))\n\t\t\tll_rw_block(REQ_OP_READ, REQ_META | REQ_PRIO, 1,\n\t\t\t\t    &bhs[i]);\n\n\tif (!wait)\n\t\treturn 0;\n\n\tfor (i = 0; i < bh_count; i++)\n\t\tif (bhs[i])\n\t\t\twait_on_buffer(bhs[i]);\n\n\tfor (i = 0; i < bh_count; i++) {\n\t\tif (bhs[i] && !buffer_uptodate(bhs[i])) {\n\t\t\terr = -EIO;\n\t\t\tgoto out_brelse;\n\t\t}\n\t}\n\treturn 0;\n\nout_brelse:\n\tfor (i = 0; i < bh_count; i++) {\n\t\tbrelse(bhs[i]);\n\t\tbhs[i] = NULL;\n\t}\n\treturn err;\n}\n\nint ext4_walk_page_buffers(handle_t *handle,\n\t\t\t   struct buffer_head *head,\n\t\t\t   unsigned from,\n\t\t\t   unsigned to,\n\t\t\t   int *partial,\n\t\t\t   int (*fn)(handle_t *handle,\n\t\t\t\t     struct buffer_head *bh))\n{\n\tstruct buffer_head *bh;\n\tunsigned block_start, block_end;\n\tunsigned blocksize = head->b_size;\n\tint err, ret = 0;\n\tstruct buffer_head *next;\n\n\tfor (bh = head, block_start = 0;\n\t     ret == 0 && (bh != head || !block_start);\n\t     block_start = block_end, bh = next) {\n\t\tnext = bh->b_this_page;\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (partial && !buffer_uptodate(bh))\n\t\t\t\t*partial = 1;\n\t\t\tcontinue;\n\t\t}\n\t\terr = (*fn)(handle, bh);\n\t\tif (!ret)\n\t\t\tret = err;\n\t}\n\treturn ret;\n}\n\n/*\n * To preserve ordering, it is essential that the hole instantiation and\n * the data write be encapsulated in a single transaction.  We cannot\n * close off a transaction and start a new one between the ext4_get_block()\n * and the commit_write().  So doing the jbd2_journal_start at the start of\n * prepare_write() is the right place.\n *\n * Also, this function can nest inside ext4_writepage().  In that case, we\n * *know* that ext4_writepage() has generated enough buffer credits to do the\n * whole page.  So we won't block on the journal in that case, which is good,\n * because the caller may be PF_MEMALLOC.\n *\n * By accident, ext4 can be reentered when a transaction is open via\n * quota file writes.  If we were to commit the transaction while thus\n * reentered, there can be a deadlock - we would be holding a quota\n * lock, and the commit would never complete if another thread had a\n * transaction open and was blocking on the quota lock - a ranking\n * violation.\n *\n * So what we do is to rely on the fact that jbd2_journal_stop/journal_start\n * will _not_ run commit under these circumstances because handle->h_ref\n * is elevated.  We'll still have enough credits for the tiny quotafile\n * write.\n */\nint do_journal_get_write_access(handle_t *handle,\n\t\t\t\tstruct buffer_head *bh)\n{\n\tint dirty = buffer_dirty(bh);\n\tint ret;\n\n\tif (!buffer_mapped(bh) || buffer_freed(bh))\n\t\treturn 0;\n\t/*\n\t * __block_write_begin() could have dirtied some buffers. Clean\n\t * the dirty bit as jbd2_journal_get_write_access() could complain\n\t * otherwise about fs integrity issues. Setting of the dirty bit\n\t * by __block_write_begin() isn't a real problem here as we clear\n\t * the bit before releasing a page lock and thus writeback cannot\n\t * ever write the buffer.\n\t */\n\tif (dirty)\n\t\tclear_buffer_dirty(bh);\n\tBUFFER_TRACE(bh, \"get write access\");\n\tret = ext4_journal_get_write_access(handle, bh);\n\tif (!ret && dirty)\n\t\tret = ext4_handle_dirty_metadata(handle, NULL, bh);\n\treturn ret;\n}\n\n#ifdef CONFIG_FS_ENCRYPTION\nstatic int ext4_block_write_begin(struct page *page, loff_t pos, unsigned len,\n\t\t\t\t  get_block_t *get_block)\n{\n\tunsigned from = pos & (PAGE_SIZE - 1);\n\tunsigned to = from + len;\n\tstruct inode *inode = page->mapping->host;\n\tunsigned block_start, block_end;\n\tsector_t block;\n\tint err = 0;\n\tunsigned blocksize = inode->i_sb->s_blocksize;\n\tunsigned bbits;\n\tstruct buffer_head *bh, *head, *wait[2];\n\tint nr_wait = 0;\n\tint i;\n\n\tBUG_ON(!PageLocked(page));\n\tBUG_ON(from > PAGE_SIZE);\n\tBUG_ON(to > PAGE_SIZE);\n\tBUG_ON(from > to);\n\n\tif (!page_has_buffers(page))\n\t\tcreate_empty_buffers(page, blocksize, 0);\n\thead = page_buffers(page);\n\tbbits = ilog2(blocksize);\n\tblock = (sector_t)page->index << (PAGE_SHIFT - bbits);\n\n\tfor (bh = head, block_start = 0; bh != head || !block_start;\n\t    block++, block_start = block_end, bh = bh->b_this_page) {\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (PageUptodate(page)) {\n\t\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (buffer_new(bh))\n\t\t\tclear_buffer_new(bh);\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tWARN_ON(bh->b_size != blocksize);\n\t\t\terr = get_block(inode, block, bh, 1);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (buffer_new(bh)) {\n\t\t\t\tif (PageUptodate(page)) {\n\t\t\t\t\tclear_buffer_new(bh);\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\t\tmark_buffer_dirty(bh);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (block_end > to || block_start < from)\n\t\t\t\t\tzero_user_segments(page, to, block_end,\n\t\t\t\t\t\t\t   block_start, from);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (PageUptodate(page)) {\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!buffer_uptodate(bh) && !buffer_delay(bh) &&\n\t\t    !buffer_unwritten(bh) &&\n\t\t    (block_start < from || block_end > to)) {\n\t\t\tll_rw_block(REQ_OP_READ, 0, 1, &bh);\n\t\t\twait[nr_wait++] = bh;\n\t\t}\n\t}\n\t/*\n\t * If we issued read requests, let them complete.\n\t */\n\tfor (i = 0; i < nr_wait; i++) {\n\t\twait_on_buffer(wait[i]);\n\t\tif (!buffer_uptodate(wait[i]))\n\t\t\terr = -EIO;\n\t}\n\tif (unlikely(err)) {\n\t\tpage_zero_new_buffers(page, from, to);\n\t} else if (IS_ENCRYPTED(inode) && S_ISREG(inode->i_mode)) {\n\t\tfor (i = 0; i < nr_wait; i++) {\n\t\t\tint err2;\n\n\t\t\terr2 = fscrypt_decrypt_pagecache_blocks(page, blocksize,\n\t\t\t\t\t\t\t\tbh_offset(wait[i]));\n\t\t\tif (err2) {\n\t\t\t\tclear_buffer_uptodate(wait[i]);\n\t\t\t\terr = err2;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn err;\n}\n#endif\n\nstatic int ext4_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t    loff_t pos, unsigned len, unsigned flags,\n\t\t\t    struct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret, needed_blocks;\n\thandle_t *handle;\n\tint retries = 0;\n\tstruct page *page;\n\tpgoff_t index;\n\tunsigned from, to;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\ttrace_ext4_write_begin(inode, pos, len, flags);\n\t/*\n\t * Reserve one block more for addition to orphan list in case\n\t * we allocate blocks but write fails for some reason\n\t */\n\tneeded_blocks = ext4_writepage_trans_blocks(inode) + 1;\n\tindex = pos >> PAGE_SHIFT;\n\tfrom = pos & (PAGE_SIZE - 1);\n\tto = from + len;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) {\n\t\tret = ext4_try_to_write_inline_data(mapping, inode, pos, len,\n\t\t\t\t\t\t    flags, pagep);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (ret == 1)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * grab_cache_page_write_begin() can take a long time if the\n\t * system is thrashing due to memory pressure, or if the page\n\t * is being written back.  So grab it first before we start\n\t * the transaction handle.  This also allows us to allocate\n\t * the page (if needed) without using GFP_NOFS.\n\t */\nretry_grab:\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page)\n\t\treturn -ENOMEM;\n\tunlock_page(page);\n\nretry_journal:\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE, needed_blocks);\n\tif (IS_ERR(handle)) {\n\t\tput_page(page);\n\t\treturn PTR_ERR(handle);\n\t}\n\n\tlock_page(page);\n\tif (page->mapping != mapping) {\n\t\t/* The page got truncated from under us */\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\text4_journal_stop(handle);\n\t\tgoto retry_grab;\n\t}\n\t/* In case writeback began while the page was unlocked */\n\twait_for_stable_page(page);\n\n#ifdef CONFIG_FS_ENCRYPTION\n\tif (ext4_should_dioread_nolock(inode))\n\t\tret = ext4_block_write_begin(page, pos, len,\n\t\t\t\t\t     ext4_get_block_unwritten);\n\telse\n\t\tret = ext4_block_write_begin(page, pos, len,\n\t\t\t\t\t     ext4_get_block);\n#else\n\tif (ext4_should_dioread_nolock(inode))\n\t\tret = __block_write_begin(page, pos, len,\n\t\t\t\t\t  ext4_get_block_unwritten);\n\telse\n\t\tret = __block_write_begin(page, pos, len, ext4_get_block);\n#endif\n\tif (!ret && ext4_should_journal_data(inode)) {\n\t\tret = ext4_walk_page_buffers(handle, page_buffers(page),\n\t\t\t\t\t     from, to, NULL,\n\t\t\t\t\t     do_journal_get_write_access);\n\t}\n\n\tif (ret) {\n\t\tbool extended = (pos + len > inode->i_size) &&\n\t\t\t\t!ext4_verity_in_progress(inode);\n\n\t\tunlock_page(page);\n\t\t/*\n\t\t * __block_write_begin may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again. Don't need\n\t\t * i_size_read because we hold i_mutex.\n\t\t *\n\t\t * Add inode to orphan list in case we crash before\n\t\t * truncate finishes\n\t\t */\n\t\tif (extended && ext4_can_truncate(inode))\n\t\t\text4_orphan_add(handle, inode);\n\n\t\text4_journal_stop(handle);\n\t\tif (extended) {\n\t\t\text4_truncate_failed_write(inode);\n\t\t\t/*\n\t\t\t * If truncate failed early the inode might\n\t\t\t * still be on the orphan list; we need to\n\t\t\t * make sure the inode is removed from the\n\t\t\t * orphan list in that case.\n\t\t\t */\n\t\t\tif (inode->i_nlink)\n\t\t\t\text4_orphan_del(NULL, inode);\n\t\t}\n\n\t\tif (ret == -ENOSPC &&\n\t\t    ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\t\tgoto retry_journal;\n\t\tput_page(page);\n\t\treturn ret;\n\t}\n\t*pagep = page;\n\treturn ret;\n}\n\n/* For write_end() in data=journal mode */\nstatic int write_end_fn(handle_t *handle, struct buffer_head *bh)\n{\n\tint ret;\n\tif (!buffer_mapped(bh) || buffer_freed(bh))\n\t\treturn 0;\n\tset_buffer_uptodate(bh);\n\tret = ext4_handle_dirty_metadata(handle, NULL, bh);\n\tclear_buffer_meta(bh);\n\tclear_buffer_prio(bh);\n\treturn ret;\n}\n\n/*\n * We need to pick up the new inode size which generic_commit_write gave us\n * `file' can be NULL - eg, when called from page_symlink().\n *\n * ext4 never places buffers on inode->i_mapping->private_list.  metadata\n * buffers are managed internally.\n */\nstatic int ext4_write_end(struct file *file,\n\t\t\t  struct address_space *mapping,\n\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t  struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tloff_t old_size = inode->i_size;\n\tint ret = 0, ret2;\n\tint i_size_changed = 0;\n\tint inline_data = ext4_has_inline_data(inode);\n\tbool verity = ext4_verity_in_progress(inode);\n\n\ttrace_ext4_write_end(inode, pos, len, copied);\n\tif (inline_data) {\n\t\tret = ext4_write_inline_data_end(inode, pos, len,\n\t\t\t\t\t\t copied, page);\n\t\tif (ret < 0) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto errout;\n\t\t}\n\t\tcopied = ret;\n\t} else\n\t\tcopied = block_write_end(file, mapping, pos,\n\t\t\t\t\t len, copied, page, fsdata);\n\t/*\n\t * it's important to update i_size while still holding page lock:\n\t * page writeout could otherwise come in and zero beyond i_size.\n\t *\n\t * If FS_IOC_ENABLE_VERITY is running on this inode, then Merkle tree\n\t * blocks are being written past EOF, so skip the i_size update.\n\t */\n\tif (!verity)\n\t\ti_size_changed = ext4_update_inode_size(inode, pos + copied);\n\tunlock_page(page);\n\tput_page(page);\n\n\tif (old_size < pos && !verity)\n\t\tpagecache_isize_extended(inode, old_size, pos);\n\t/*\n\t * Don't mark the inode dirty under page lock. First, it unnecessarily\n\t * makes the holding time of page lock longer. Second, it forces lock\n\t * ordering of page lock and transaction start for journaling\n\t * filesystems.\n\t */\n\tif (i_size_changed || inline_data)\n\t\text4_mark_inode_dirty(handle, inode);\n\n\tif (pos + len > inode->i_size && !verity && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\nerrout:\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\tif (pos + len > inode->i_size && !verity) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}\n\n/*\n * This is a private version of page_zero_new_buffers() which doesn't\n * set the buffer to be dirty, since in data=journalled mode we need\n * to call ext4_handle_dirty_metadata() instead.\n */\nstatic void ext4_journalled_zero_new_buffers(handle_t *handle,\n\t\t\t\t\t    struct page *page,\n\t\t\t\t\t    unsigned from, unsigned to)\n{\n\tunsigned int block_start = 0, block_end;\n\tstruct buffer_head *head, *bh;\n\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tblock_end = block_start + bh->b_size;\n\t\tif (buffer_new(bh)) {\n\t\t\tif (block_end > from && block_start < to) {\n\t\t\t\tif (!PageUptodate(page)) {\n\t\t\t\t\tunsigned start, size;\n\n\t\t\t\t\tstart = max(from, block_start);\n\t\t\t\t\tsize = min(to, block_end) - start;\n\n\t\t\t\t\tzero_user(page, start, size);\n\t\t\t\t\twrite_end_fn(handle, bh);\n\t\t\t\t}\n\t\t\t\tclear_buffer_new(bh);\n\t\t\t}\n\t\t}\n\t\tblock_start = block_end;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n}\n\nstatic int ext4_journalled_write_end(struct file *file,\n\t\t\t\t     struct address_space *mapping,\n\t\t\t\t     loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t     struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tloff_t old_size = inode->i_size;\n\tint ret = 0, ret2;\n\tint partial = 0;\n\tunsigned from, to;\n\tint size_changed = 0;\n\tint inline_data = ext4_has_inline_data(inode);\n\tbool verity = ext4_verity_in_progress(inode);\n\n\ttrace_ext4_journalled_write_end(inode, pos, len, copied);\n\tfrom = pos & (PAGE_SIZE - 1);\n\tto = from + len;\n\n\tBUG_ON(!ext4_handle_valid(handle));\n\n\tif (inline_data) {\n\t\tret = ext4_write_inline_data_end(inode, pos, len,\n\t\t\t\t\t\t copied, page);\n\t\tif (ret < 0) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto errout;\n\t\t}\n\t\tcopied = ret;\n\t} else if (unlikely(copied < len) && !PageUptodate(page)) {\n\t\tcopied = 0;\n\t\text4_journalled_zero_new_buffers(handle, page, from, to);\n\t} else {\n\t\tif (unlikely(copied < len))\n\t\t\text4_journalled_zero_new_buffers(handle, page,\n\t\t\t\t\t\t\t from + copied, to);\n\t\tret = ext4_walk_page_buffers(handle, page_buffers(page), from,\n\t\t\t\t\t     from + copied, &partial,\n\t\t\t\t\t     write_end_fn);\n\t\tif (!partial)\n\t\t\tSetPageUptodate(page);\n\t}\n\tif (!verity)\n\t\tsize_changed = ext4_update_inode_size(inode, pos + copied);\n\text4_set_inode_state(inode, EXT4_STATE_JDATA);\n\tEXT4_I(inode)->i_datasync_tid = handle->h_transaction->t_tid;\n\tunlock_page(page);\n\tput_page(page);\n\n\tif (old_size < pos && !verity)\n\t\tpagecache_isize_extended(inode, old_size, pos);\n\n\tif (size_changed || inline_data) {\n\t\tret2 = ext4_mark_inode_dirty(handle, inode);\n\t\tif (!ret)\n\t\t\tret = ret2;\n\t}\n\n\tif (pos + len > inode->i_size && !verity && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\n\nerrout:\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\tif (pos + len > inode->i_size && !verity) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}\n\n/*\n * Reserve space for a single cluster\n */\nstatic int ext4_da_reserve_space(struct inode *inode)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint ret;\n\n\t/*\n\t * We will charge metadata quota at writeout time; this saves\n\t * us from metadata over-estimation, though we may go over by\n\t * a small amount in the end.  Here we just reserve for data.\n\t */\n\tret = dquot_reserve_block(inode, EXT4_C2B(sbi, 1));\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock(&ei->i_block_reservation_lock);\n\tif (ext4_claim_free_clusters(sbi, 1, 0)) {\n\t\tspin_unlock(&ei->i_block_reservation_lock);\n\t\tdquot_release_reservation_block(inode, EXT4_C2B(sbi, 1));\n\t\treturn -ENOSPC;\n\t}\n\tei->i_reserved_data_blocks++;\n\ttrace_ext4_da_reserve_space(inode);\n\tspin_unlock(&ei->i_block_reservation_lock);\n\n\treturn 0;       /* success */\n}\n\nvoid ext4_da_release_space(struct inode *inode, int to_free)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tif (!to_free)\n\t\treturn;\t\t/* Nothing to release, exit */\n\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\ttrace_ext4_da_release_space(inode, to_free);\n\tif (unlikely(to_free > ei->i_reserved_data_blocks)) {\n\t\t/*\n\t\t * if there aren't enough reserved blocks, then the\n\t\t * counter is messed up somewhere.  Since this\n\t\t * function is called from invalidate page, it's\n\t\t * harmless to return without any action.\n\t\t */\n\t\text4_warning(inode->i_sb, \"ext4_da_release_space: \"\n\t\t\t \"ino %lu, to_free %d with only %d reserved \"\n\t\t\t \"data blocks\", inode->i_ino, to_free,\n\t\t\t ei->i_reserved_data_blocks);\n\t\tWARN_ON(1);\n\t\tto_free = ei->i_reserved_data_blocks;\n\t}\n\tei->i_reserved_data_blocks -= to_free;\n\n\t/* update fs dirty data blocks counter */\n\tpercpu_counter_sub(&sbi->s_dirtyclusters_counter, to_free);\n\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\tdquot_release_reservation_block(inode, EXT4_C2B(sbi, to_free));\n}\n\n/*\n * Delayed allocation stuff\n */\n\nstruct mpage_da_data {\n\tstruct inode *inode;\n\tstruct writeback_control *wbc;\n\n\tpgoff_t first_page;\t/* The first page to write */\n\tpgoff_t next_page;\t/* Current page to examine */\n\tpgoff_t last_page;\t/* Last page to examine */\n\t/*\n\t * Extent to map - this can be after first_page because that can be\n\t * fully mapped. We somewhat abuse m_flags to store whether the extent\n\t * is delalloc or unwritten.\n\t */\n\tstruct ext4_map_blocks map;\n\tstruct ext4_io_submit io_submit;\t/* IO submission data */\n\tunsigned int do_map:1;\n};\n\nstatic void mpage_release_unused_pages(struct mpage_da_data *mpd,\n\t\t\t\t       bool invalidate)\n{\n\tint nr_pages, i;\n\tpgoff_t index, end;\n\tstruct pagevec pvec;\n\tstruct inode *inode = mpd->inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\t/* This is necessary when next_page == 0. */\n\tif (mpd->first_page >= mpd->next_page)\n\t\treturn;\n\n\tindex = mpd->first_page;\n\tend   = mpd->next_page - 1;\n\tif (invalidate) {\n\t\text4_lblk_t start, last;\n\t\tstart = index << (PAGE_SHIFT - inode->i_blkbits);\n\t\tlast = end << (PAGE_SHIFT - inode->i_blkbits);\n\t\text4_es_remove_extent(inode, start, last - start + 1);\n\t}\n\n\tpagevec_init(&pvec);\n\twhile (index <= end) {\n\t\tnr_pages = pagevec_lookup_range(&pvec, mapping, &index, end);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tBUG_ON(!PageLocked(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\tif (invalidate) {\n\t\t\t\tif (page_mapped(page))\n\t\t\t\t\tclear_page_dirty_for_io(page);\n\t\t\t\tblock_invalidatepage(page, 0, PAGE_SIZE);\n\t\t\t\tClearPageUptodate(page);\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n}\n\nstatic void ext4_print_free_blocks(struct inode *inode)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct super_block *sb = inode->i_sb;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\text4_msg(sb, KERN_CRIT, \"Total free blocks count %lld\",\n\t       EXT4_C2B(EXT4_SB(inode->i_sb),\n\t\t\text4_count_free_clusters(sb)));\n\text4_msg(sb, KERN_CRIT, \"Free/Dirty block details\");\n\text4_msg(sb, KERN_CRIT, \"free_blocks=%lld\",\n\t       (long long) EXT4_C2B(EXT4_SB(sb),\n\t\tpercpu_counter_sum(&sbi->s_freeclusters_counter)));\n\text4_msg(sb, KERN_CRIT, \"dirty_blocks=%lld\",\n\t       (long long) EXT4_C2B(EXT4_SB(sb),\n\t\tpercpu_counter_sum(&sbi->s_dirtyclusters_counter)));\n\text4_msg(sb, KERN_CRIT, \"Block reservation details\");\n\text4_msg(sb, KERN_CRIT, \"i_reserved_data_blocks=%u\",\n\t\t ei->i_reserved_data_blocks);\n\treturn;\n}\n\nstatic int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh)\n{\n\treturn (buffer_delay(bh) || buffer_unwritten(bh)) && buffer_dirty(bh);\n}\n\n/*\n * ext4_insert_delayed_block - adds a delayed block to the extents status\n *                             tree, incrementing the reserved cluster/block\n *                             count or making a pending reservation\n *                             where needed\n *\n * @inode - file containing the newly added block\n * @lblk - logical block to be added\n *\n * Returns 0 on success, negative error code on failure.\n */\nstatic int ext4_insert_delayed_block(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint ret;\n\tbool allocated = false;\n\n\t/*\n\t * If the cluster containing lblk is shared with a delayed,\n\t * written, or unwritten extent in a bigalloc file system, it's\n\t * already been accounted for and does not need to be reserved.\n\t * A pending reservation must be made for the cluster if it's\n\t * shared with a written or unwritten extent and doesn't already\n\t * have one.  Written and unwritten extents can be purged from the\n\t * extents status tree if the system is under memory pressure, so\n\t * it's necessary to examine the extent tree if a search of the\n\t * extents status tree doesn't get a match.\n\t */\n\tif (sbi->s_cluster_ratio == 1) {\n\t\tret = ext4_da_reserve_space(inode);\n\t\tif (ret != 0)   /* ENOSPC */\n\t\t\tgoto errout;\n\t} else {   /* bigalloc */\n\t\tif (!ext4_es_scan_clu(inode, &ext4_es_is_delonly, lblk)) {\n\t\t\tif (!ext4_es_scan_clu(inode,\n\t\t\t\t\t      &ext4_es_is_mapped, lblk)) {\n\t\t\t\tret = ext4_clu_mapped(inode,\n\t\t\t\t\t\t      EXT4_B2C(sbi, lblk));\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto errout;\n\t\t\t\tif (ret == 0) {\n\t\t\t\t\tret = ext4_da_reserve_space(inode);\n\t\t\t\t\tif (ret != 0)   /* ENOSPC */\n\t\t\t\t\t\tgoto errout;\n\t\t\t\t} else {\n\t\t\t\t\tallocated = true;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tallocated = true;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = ext4_es_insert_delayed_block(inode, lblk, allocated);\n\nerrout:\n\treturn ret;\n}\n\n/*\n * This function is grabs code from the very beginning of\n * ext4_map_blocks, but assumes that the caller is from delayed write\n * time. This function looks up the requested blocks and sets the\n * buffer delay bit under the protection of i_data_sem.\n */\nstatic int ext4_da_map_blocks(struct inode *inode, sector_t iblock,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      struct buffer_head *bh)\n{\n\tstruct extent_status es;\n\tint retval;\n\tsector_t invalid_block = ~((sector_t) 0xffff);\n#ifdef ES_AGGRESSIVE_TEST\n\tstruct ext4_map_blocks orig_map;\n\n\tmemcpy(&orig_map, map, sizeof(*map));\n#endif\n\n\tif (invalid_block < ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es))\n\t\tinvalid_block = ~0;\n\n\tmap->m_flags = 0;\n\text_debug(\"ext4_da_map_blocks(): inode %lu, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, map->m_len,\n\t\t  (unsigned long) map->m_lblk);\n\n\t/* Lookup extent status tree firstly */\n\tif (ext4_es_lookup_extent(inode, iblock, NULL, &es)) {\n\t\tif (ext4_es_is_hole(&es)) {\n\t\t\tretval = 0;\n\t\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto add_delayed;\n\t\t}\n\n\t\t/*\n\t\t * Delayed extent could be allocated by fallocate.\n\t\t * So we need to check it.\n\t\t */\n\t\tif (ext4_es_is_delayed(&es) && !ext4_es_is_unwritten(&es)) {\n\t\t\tmap_bh(bh, inode->i_sb, invalid_block);\n\t\t\tset_buffer_new(bh);\n\t\t\tset_buffer_delay(bh);\n\t\t\treturn 0;\n\t\t}\n\n\t\tmap->m_pblk = ext4_es_pblock(&es) + iblock - es.es_lblk;\n\t\tretval = es.es_len - (iblock - es.es_lblk);\n\t\tif (retval > map->m_len)\n\t\t\tretval = map->m_len;\n\t\tmap->m_len = retval;\n\t\tif (ext4_es_is_written(&es))\n\t\t\tmap->m_flags |= EXT4_MAP_MAPPED;\n\t\telse if (ext4_es_is_unwritten(&es))\n\t\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\telse\n\t\t\tBUG();\n\n#ifdef ES_AGGRESSIVE_TEST\n\t\text4_map_blocks_es_recheck(NULL, inode, map, &orig_map, 0);\n#endif\n\t\treturn retval;\n\t}\n\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_has_inline_data(inode))\n\t\tretval = 0;\n\telse if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tretval = ext4_ext_map_blocks(NULL, inode, map, 0);\n\telse\n\t\tretval = ext4_ind_map_blocks(NULL, inode, map, 0);\n\nadd_delayed:\n\tif (retval == 0) {\n\t\tint ret;\n\n\t\t/*\n\t\t * XXX: __block_prepare_write() unmaps passed block,\n\t\t * is it OK?\n\t\t */\n\n\t\tret = ext4_insert_delayed_block(inode, map->m_lblk);\n\t\tif (ret != 0) {\n\t\t\tretval = ret;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tmap_bh(bh, inode->i_sb, invalid_block);\n\t\tset_buffer_new(bh);\n\t\tset_buffer_delay(bh);\n\t} else if (retval > 0) {\n\t\tint ret;\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk, map->m_len,\n\t\t\t\t\t    map->m_pblk, status);\n\t\tif (ret != 0)\n\t\t\tretval = ret;\n\t}\n\nout_unlock:\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\n\treturn retval;\n}\n\n/*\n * This is a special get_block_t callback which is used by\n * ext4_da_write_begin().  It will either return mapped block or\n * reserve space for a single block.\n *\n * For delayed buffer_head we have BH_Mapped, BH_New, BH_Delay set.\n * We also have b_blocknr = -1 and b_bdev initialized properly\n *\n * For unwritten buffer_head we have BH_Mapped, BH_New, BH_Unwritten set.\n * We also have b_blocknr = physicalblock mapping unwritten extent and b_bdev\n * initialized properly.\n */\nint ext4_da_get_block_prep(struct inode *inode, sector_t iblock,\n\t\t\t   struct buffer_head *bh, int create)\n{\n\tstruct ext4_map_blocks map;\n\tint ret = 0;\n\n\tBUG_ON(create == 0);\n\tBUG_ON(bh->b_size != inode->i_sb->s_blocksize);\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = 1;\n\n\t/*\n\t * first, we need to know whether the block is allocated already\n\t * preallocated blocks are unmapped but should treated\n\t * the same as allocated blocks.\n\t */\n\tret = ext4_da_map_blocks(inode, iblock, &map, bh);\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\text4_update_bh_state(bh, map.m_flags);\n\n\tif (buffer_unwritten(bh)) {\n\t\t/* A delayed write to unwritten bh should be marked\n\t\t * new and mapped.  Mapped ensures that we don't do\n\t\t * get_block multiple times when we write to the same\n\t\t * offset and new ensures that we do proper zero out\n\t\t * for partial write.\n\t\t */\n\t\tset_buffer_new(bh);\n\t\tset_buffer_mapped(bh);\n\t}\n\treturn 0;\n}\n\nstatic int bget_one(handle_t *handle, struct buffer_head *bh)\n{\n\tget_bh(bh);\n\treturn 0;\n}\n\nstatic int bput_one(handle_t *handle, struct buffer_head *bh)\n{\n\tput_bh(bh);\n\treturn 0;\n}\n\nstatic int __ext4_journalled_writepage(struct page *page,\n\t\t\t\t       unsigned int len)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head *page_bufs = NULL;\n\thandle_t *handle = NULL;\n\tint ret = 0, err = 0;\n\tint inline_data = ext4_has_inline_data(inode);\n\tstruct buffer_head *inode_bh = NULL;\n\n\tClearPageChecked(page);\n\n\tif (inline_data) {\n\t\tBUG_ON(page->index != 0);\n\t\tBUG_ON(len > ext4_get_max_inline_size(inode));\n\t\tinode_bh = ext4_journalled_write_inline_data(inode, len, page);\n\t\tif (inode_bh == NULL)\n\t\t\tgoto out;\n\t} else {\n\t\tpage_bufs = page_buffers(page);\n\t\tif (!page_bufs) {\n\t\t\tBUG();\n\t\t\tgoto out;\n\t\t}\n\t\text4_walk_page_buffers(handle, page_bufs, 0, len,\n\t\t\t\t       NULL, bget_one);\n\t}\n\t/*\n\t * We need to release the page lock before we start the\n\t * journal, so grab a reference so the page won't disappear\n\t * out from under us.\n\t */\n\tget_page(page);\n\tunlock_page(page);\n\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,\n\t\t\t\t    ext4_writepage_trans_blocks(inode));\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tput_page(page);\n\t\tgoto out_no_pagelock;\n\t}\n\tBUG_ON(!ext4_handle_valid(handle));\n\n\tlock_page(page);\n\tput_page(page);\n\tif (page->mapping != mapping) {\n\t\t/* The page got truncated from under us */\n\t\text4_journal_stop(handle);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (inline_data) {\n\t\tret = ext4_mark_inode_dirty(handle, inode);\n\t} else {\n\t\tret = ext4_walk_page_buffers(handle, page_bufs, 0, len, NULL,\n\t\t\t\t\t     do_journal_get_write_access);\n\n\t\terr = ext4_walk_page_buffers(handle, page_bufs, 0, len, NULL,\n\t\t\t\t\t     write_end_fn);\n\t}\n\tif (ret == 0)\n\t\tret = err;\n\tEXT4_I(inode)->i_datasync_tid = handle->h_transaction->t_tid;\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\n\tif (!ext4_has_inline_data(inode))\n\t\text4_walk_page_buffers(NULL, page_bufs, 0, len,\n\t\t\t\t       NULL, bput_one);\n\text4_set_inode_state(inode, EXT4_STATE_JDATA);\nout:\n\tunlock_page(page);\nout_no_pagelock:\n\tbrelse(inode_bh);\n\treturn ret;\n}\n\n/*\n * Note that we don't need to start a transaction unless we're journaling data\n * because we should have holes filled from ext4_page_mkwrite(). We even don't\n * need to file the inode to the transaction's list in ordered mode because if\n * we are writing back data added by write(), the inode is already there and if\n * we are writing back data modified via mmap(), no one guarantees in which\n * transaction the data will hit the disk. In case we are journaling data, we\n * cannot start transaction directly because transaction start ranks above page\n * lock so we have to do some magic.\n *\n * This function can get called via...\n *   - ext4_writepages after taking page lock (have journal handle)\n *   - journal_submit_inode_data_buffers (no journal handle)\n *   - shrink_page_list via the kswapd/direct reclaim (no journal handle)\n *   - grab_page_cache when doing write_begin (have journal handle)\n *\n * We don't do any block allocation in this function. If we have page with\n * multiple blocks we need to write those buffer_heads that are mapped. This\n * is important for mmaped based write. So if we do with blocksize 1K\n * truncate(f, 1024);\n * a = mmap(f, 0, 4096);\n * a[0] = 'a';\n * truncate(f, 4096);\n * we have in the page first buffer_head mapped via page_mkwrite call back\n * but other buffer_heads would be unmapped but dirty (dirty done via the\n * do_wp_page). So writepage should write the first block. If we modify\n * the mmap area beyond 1024 we will again get a page_fault and the\n * page_mkwrite callback will do the block allocation and mark the\n * buffer_heads mapped.\n *\n * We redirty the page if we have any buffer_heads that is either delay or\n * unwritten in the page.\n *\n * We can get recursively called as show below.\n *\n *\text4_writepage() -> kmalloc() -> __alloc_pages() -> page_launder() ->\n *\t\text4_writepage()\n *\n * But since we don't do any block allocation we should not deadlock.\n * Page also have the dirty flag cleared so we don't get recurive page_lock.\n */\nstatic int ext4_writepage(struct page *page,\n\t\t\t  struct writeback_control *wbc)\n{\n\tint ret = 0;\n\tloff_t size;\n\tunsigned int len;\n\tstruct buffer_head *page_bufs = NULL;\n\tstruct inode *inode = page->mapping->host;\n\tstruct ext4_io_submit io_submit;\n\tbool keep_towrite = false;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb)))) {\n\t\text4_invalidatepage(page, 0, PAGE_SIZE);\n\t\tunlock_page(page);\n\t\treturn -EIO;\n\t}\n\n\ttrace_ext4_writepage(page);\n\tsize = i_size_read(inode);\n\tif (page->index == size >> PAGE_SHIFT &&\n\t    !ext4_verity_in_progress(inode))\n\t\tlen = size & ~PAGE_MASK;\n\telse\n\t\tlen = PAGE_SIZE;\n\n\tpage_bufs = page_buffers(page);\n\t/*\n\t * We cannot do block allocation or other extent handling in this\n\t * function. If there are buffers needing that, we have to redirty\n\t * the page. But we may reach here when we do a journal commit via\n\t * journal_submit_inode_data_buffers() and in that case we must write\n\t * allocated buffers to achieve data=ordered mode guarantees.\n\t *\n\t * Also, if there is only one buffer per page (the fs block\n\t * size == the page size), if one buffer needs block\n\t * allocation or needs to modify the extent tree to clear the\n\t * unwritten flag, we know that the page can't be written at\n\t * all, so we might as well refuse the write immediately.\n\t * Unfortunately if the block size != page size, we can't as\n\t * easily detect this case using ext4_walk_page_buffers(), but\n\t * for the extremely common case, this is an optimization that\n\t * skips a useless round trip through ext4_bio_write_page().\n\t */\n\tif (ext4_walk_page_buffers(NULL, page_bufs, 0, len, NULL,\n\t\t\t\t   ext4_bh_delay_or_unwritten)) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tif ((current->flags & PF_MEMALLOC) ||\n\t\t    (inode->i_sb->s_blocksize == PAGE_SIZE)) {\n\t\t\t/*\n\t\t\t * For memory cleaning there's no point in writing only\n\t\t\t * some buffers. So just bail out. Warn if we came here\n\t\t\t * from direct reclaim.\n\t\t\t */\n\t\t\tWARN_ON_ONCE((current->flags & (PF_MEMALLOC|PF_KSWAPD))\n\t\t\t\t\t\t\t== PF_MEMALLOC);\n\t\t\tunlock_page(page);\n\t\t\treturn 0;\n\t\t}\n\t\tkeep_towrite = true;\n\t}\n\n\tif (PageChecked(page) && ext4_should_journal_data(inode))\n\t\t/*\n\t\t * It's mmapped pagecache.  Add buffers and journal it.  There\n\t\t * doesn't seem much point in redirtying the page here.\n\t\t */\n\t\treturn __ext4_journalled_writepage(page, len);\n\n\text4_io_submit_init(&io_submit, wbc);\n\tio_submit.io_end = ext4_init_io_end(inode, GFP_NOFS);\n\tif (!io_submit.io_end) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tunlock_page(page);\n\t\treturn -ENOMEM;\n\t}\n\tret = ext4_bio_write_page(&io_submit, page, len, wbc, keep_towrite);\n\text4_io_submit(&io_submit);\n\t/* Drop io_end reference we got from init */\n\text4_put_io_end_defer(io_submit.io_end);\n\treturn ret;\n}\n\nstatic int mpage_submit_page(struct mpage_da_data *mpd, struct page *page)\n{\n\tint len;\n\tloff_t size;\n\tint err;\n\n\tBUG_ON(page->index != mpd->first_page);\n\tclear_page_dirty_for_io(page);\n\t/*\n\t * We have to be very careful here!  Nothing protects writeback path\n\t * against i_size changes and the page can be writeably mapped into\n\t * page tables. So an application can be growing i_size and writing\n\t * data through mmap while writeback runs. clear_page_dirty_for_io()\n\t * write-protects our page in page tables and the page cannot get\n\t * written to again until we release page lock. So only after\n\t * clear_page_dirty_for_io() we are safe to sample i_size for\n\t * ext4_bio_write_page() to zero-out tail of the written page. We rely\n\t * on the barrier provided by TestClearPageDirty in\n\t * clear_page_dirty_for_io() to make sure i_size is really sampled only\n\t * after page tables are updated.\n\t */\n\tsize = i_size_read(mpd->inode);\n\tif (page->index == size >> PAGE_SHIFT &&\n\t    !ext4_verity_in_progress(mpd->inode))\n\t\tlen = size & ~PAGE_MASK;\n\telse\n\t\tlen = PAGE_SIZE;\n\terr = ext4_bio_write_page(&mpd->io_submit, page, len, mpd->wbc, false);\n\tif (!err)\n\t\tmpd->wbc->nr_to_write--;\n\tmpd->first_page++;\n\n\treturn err;\n}\n\n#define BH_FLAGS ((1 << BH_Unwritten) | (1 << BH_Delay))\n\n/*\n * mballoc gives us at most this number of blocks...\n * XXX: That seems to be only a limitation of ext4_mb_normalize_request().\n * The rest of mballoc seems to handle chunks up to full group size.\n */\n#define MAX_WRITEPAGES_EXTENT_LEN 2048\n\n/*\n * mpage_add_bh_to_extent - try to add bh to extent of blocks to map\n *\n * @mpd - extent of blocks\n * @lblk - logical number of the block in the file\n * @bh - buffer head we want to add to the extent\n *\n * The function is used to collect contig. blocks in the same state. If the\n * buffer doesn't require mapping for writeback and we haven't started the\n * extent of buffers to map yet, the function returns 'true' immediately - the\n * caller can write the buffer right away. Otherwise the function returns true\n * if the block has been added to the extent, false if the block couldn't be\n * added.\n */\nstatic bool mpage_add_bh_to_extent(struct mpage_da_data *mpd, ext4_lblk_t lblk,\n\t\t\t\t   struct buffer_head *bh)\n{\n\tstruct ext4_map_blocks *map = &mpd->map;\n\n\t/* Buffer that doesn't need mapping for writeback? */\n\tif (!buffer_dirty(bh) || !buffer_mapped(bh) ||\n\t    (!buffer_delay(bh) && !buffer_unwritten(bh))) {\n\t\t/* So far no extent to map => we write the buffer right away */\n\t\tif (map->m_len == 0)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\n\t/* First block in the extent? */\n\tif (map->m_len == 0) {\n\t\t/* We cannot map unless handle is started... */\n\t\tif (!mpd->do_map)\n\t\t\treturn false;\n\t\tmap->m_lblk = lblk;\n\t\tmap->m_len = 1;\n\t\tmap->m_flags = bh->b_state & BH_FLAGS;\n\t\treturn true;\n\t}\n\n\t/* Don't go larger than mballoc is willing to allocate */\n\tif (map->m_len >= MAX_WRITEPAGES_EXTENT_LEN)\n\t\treturn false;\n\n\t/* Can we merge the block to our big extent? */\n\tif (lblk == map->m_lblk + map->m_len &&\n\t    (bh->b_state & BH_FLAGS) == map->m_flags) {\n\t\tmap->m_len++;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * mpage_process_page_bufs - submit page buffers for IO or add them to extent\n *\n * @mpd - extent of blocks for mapping\n * @head - the first buffer in the page\n * @bh - buffer we should start processing from\n * @lblk - logical number of the block in the file corresponding to @bh\n *\n * Walk through page buffers from @bh upto @head (exclusive) and either submit\n * the page for IO if all buffers in this page were mapped and there's no\n * accumulated extent of buffers to map or add buffers in the page to the\n * extent of buffers to map. The function returns 1 if the caller can continue\n * by processing the next page, 0 if it should stop adding buffers to the\n * extent to map because we cannot extend it anymore. It can also return value\n * < 0 in case of error during IO submission.\n */\nstatic int mpage_process_page_bufs(struct mpage_da_data *mpd,\n\t\t\t\t   struct buffer_head *head,\n\t\t\t\t   struct buffer_head *bh,\n\t\t\t\t   ext4_lblk_t lblk)\n{\n\tstruct inode *inode = mpd->inode;\n\tint err;\n\text4_lblk_t blocks = (i_size_read(inode) + i_blocksize(inode) - 1)\n\t\t\t\t\t\t\t>> inode->i_blkbits;\n\n\tif (ext4_verity_in_progress(inode))\n\t\tblocks = EXT_MAX_BLOCKS;\n\n\tdo {\n\t\tBUG_ON(buffer_locked(bh));\n\n\t\tif (lblk >= blocks || !mpage_add_bh_to_extent(mpd, lblk, bh)) {\n\t\t\t/* Found extent to map? */\n\t\t\tif (mpd->map.m_len)\n\t\t\t\treturn 0;\n\t\t\t/* Buffer needs mapping and handle is not started? */\n\t\t\tif (!mpd->do_map)\n\t\t\t\treturn 0;\n\t\t\t/* Everything mapped so far and we hit EOF */\n\t\t\tbreak;\n\t\t}\n\t} while (lblk++, (bh = bh->b_this_page) != head);\n\t/* So far everything mapped? Submit the page for IO. */\n\tif (mpd->map.m_len == 0) {\n\t\terr = mpage_submit_page(mpd, head->b_page);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\treturn lblk < blocks;\n}\n\n/*\n * mpage_process_page - update page buffers corresponding to changed extent and\n *\t\t       may submit fully mapped page for IO\n *\n * @mpd\t\t- description of extent to map, on return next extent to map\n * @m_lblk\t- logical block mapping.\n * @m_pblk\t- corresponding physical mapping.\n * @map_bh\t- determines on return whether this page requires any further\n *\t\t  mapping or not.\n * Scan given page buffers corresponding to changed extent and update buffer\n * state according to new extent state.\n * We map delalloc buffers to their physical location, clear unwritten bits.\n * If the given page is not fully mapped, we update @map to the next extent in\n * the given page that needs mapping & return @map_bh as true.\n */\nstatic int mpage_process_page(struct mpage_da_data *mpd, struct page *page,\n\t\t\t      ext4_lblk_t *m_lblk, ext4_fsblk_t *m_pblk,\n\t\t\t      bool *map_bh)\n{\n\tstruct buffer_head *head, *bh;\n\text4_io_end_t *io_end = mpd->io_submit.io_end;\n\text4_lblk_t lblk = *m_lblk;\n\text4_fsblk_t pblock = *m_pblk;\n\tint err = 0;\n\tint blkbits = mpd->inode->i_blkbits;\n\tssize_t io_end_size = 0;\n\tstruct ext4_io_end_vec *io_end_vec = ext4_last_io_end_vec(io_end);\n\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tif (lblk < mpd->map.m_lblk)\n\t\t\tcontinue;\n\t\tif (lblk >= mpd->map.m_lblk + mpd->map.m_len) {\n\t\t\t/*\n\t\t\t * Buffer after end of mapped extent.\n\t\t\t * Find next buffer in the page to map.\n\t\t\t */\n\t\t\tmpd->map.m_len = 0;\n\t\t\tmpd->map.m_flags = 0;\n\t\t\tio_end_vec->size += io_end_size;\n\t\t\tio_end_size = 0;\n\n\t\t\terr = mpage_process_page_bufs(mpd, head, bh, lblk);\n\t\t\tif (err > 0)\n\t\t\t\terr = 0;\n\t\t\tif (!err && mpd->map.m_len && mpd->map.m_lblk > lblk) {\n\t\t\t\tio_end_vec = ext4_alloc_io_end_vec(io_end);\n\t\t\t\tio_end_vec->offset = mpd->map.m_lblk << blkbits;\n\t\t\t}\n\t\t\t*map_bh = true;\n\t\t\tgoto out;\n\t\t}\n\t\tif (buffer_delay(bh)) {\n\t\t\tclear_buffer_delay(bh);\n\t\t\tbh->b_blocknr = pblock++;\n\t\t}\n\t\tclear_buffer_unwritten(bh);\n\t\tio_end_size += (1 << blkbits);\n\t} while (lblk++, (bh = bh->b_this_page) != head);\n\n\tio_end_vec->size += io_end_size;\n\tio_end_size = 0;\n\t*map_bh = false;\nout:\n\t*m_lblk = lblk;\n\t*m_pblk = pblock;\n\treturn err;\n}\n\n/*\n * mpage_map_buffers - update buffers corresponding to changed extent and\n *\t\t       submit fully mapped pages for IO\n *\n * @mpd - description of extent to map, on return next extent to map\n *\n * Scan buffers corresponding to changed extent (we expect corresponding pages\n * to be already locked) and update buffer state according to new extent state.\n * We map delalloc buffers to their physical location, clear unwritten bits,\n * and mark buffers as uninit when we perform writes to unwritten extents\n * and do extent conversion after IO is finished. If the last page is not fully\n * mapped, we update @map to the next extent in the last page that needs\n * mapping. Otherwise we submit the page for IO.\n */\nstatic int mpage_map_and_submit_buffers(struct mpage_da_data *mpd)\n{\n\tstruct pagevec pvec;\n\tint nr_pages, i;\n\tstruct inode *inode = mpd->inode;\n\tint bpp_bits = PAGE_SHIFT - inode->i_blkbits;\n\tpgoff_t start, end;\n\text4_lblk_t lblk;\n\text4_fsblk_t pblock;\n\tint err;\n\tbool map_bh = false;\n\n\tstart = mpd->map.m_lblk >> bpp_bits;\n\tend = (mpd->map.m_lblk + mpd->map.m_len - 1) >> bpp_bits;\n\tlblk = start << bpp_bits;\n\tpblock = mpd->map.m_pblk;\n\n\tpagevec_init(&pvec);\n\twhile (start <= end) {\n\t\tnr_pages = pagevec_lookup_range(&pvec, inode->i_mapping,\n\t\t\t\t\t\t&start, end);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\terr = mpage_process_page(mpd, page, &lblk, &pblock,\n\t\t\t\t\t\t &map_bh);\n\t\t\t/*\n\t\t\t * If map_bh is true, means page may require further bh\n\t\t\t * mapping, or maybe the page was submitted for IO.\n\t\t\t * So we return to call further extent mapping.\n\t\t\t */\n\t\t\tif (err < 0 || map_bh == true)\n\t\t\t\tgoto out;\n\t\t\t/* Page fully mapped - let IO run! */\n\t\t\terr = mpage_submit_page(mpd, page);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n\t/* Extent fully mapped and matches with page boundary. We are done. */\n\tmpd->map.m_len = 0;\n\tmpd->map.m_flags = 0;\n\treturn 0;\nout:\n\tpagevec_release(&pvec);\n\treturn err;\n}\n\nstatic int mpage_map_one_extent(handle_t *handle, struct mpage_da_data *mpd)\n{\n\tstruct inode *inode = mpd->inode;\n\tstruct ext4_map_blocks *map = &mpd->map;\n\tint get_blocks_flags;\n\tint err, dioread_nolock;\n\n\ttrace_ext4_da_write_pages_extent(inode, map);\n\t/*\n\t * Call ext4_map_blocks() to allocate any delayed allocation blocks, or\n\t * to convert an unwritten extent to be initialized (in the case\n\t * where we have written into one or more preallocated blocks).  It is\n\t * possible that we're going to need more metadata blocks than\n\t * previously reserved. However we must not fail because we're in\n\t * writeback and there is nothing we can do about it so it might result\n\t * in data loss.  So use reserved blocks to allocate metadata if\n\t * possible.\n\t *\n\t * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE if\n\t * the blocks in question are delalloc blocks.  This indicates\n\t * that the blocks and quotas has already been checked when\n\t * the data was copied into the page cache.\n\t */\n\tget_blocks_flags = EXT4_GET_BLOCKS_CREATE |\n\t\t\t   EXT4_GET_BLOCKS_METADATA_NOFAIL |\n\t\t\t   EXT4_GET_BLOCKS_IO_SUBMIT;\n\tdioread_nolock = ext4_should_dioread_nolock(inode);\n\tif (dioread_nolock)\n\t\tget_blocks_flags |= EXT4_GET_BLOCKS_IO_CREATE_EXT;\n\tif (map->m_flags & (1 << BH_Delay))\n\t\tget_blocks_flags |= EXT4_GET_BLOCKS_DELALLOC_RESERVE;\n\n\terr = ext4_map_blocks(handle, inode, map, get_blocks_flags);\n\tif (err < 0)\n\t\treturn err;\n\tif (dioread_nolock && (map->m_flags & EXT4_MAP_UNWRITTEN)) {\n\t\tif (!mpd->io_submit.io_end->handle &&\n\t\t    ext4_handle_valid(handle)) {\n\t\t\tmpd->io_submit.io_end->handle = handle->h_rsv_handle;\n\t\t\thandle->h_rsv_handle = NULL;\n\t\t}\n\t\text4_set_io_unwritten_flag(inode, mpd->io_submit.io_end);\n\t}\n\n\tBUG_ON(map->m_len == 0);\n\treturn 0;\n}\n\n/*\n * mpage_map_and_submit_extent - map extent starting at mpd->lblk of length\n *\t\t\t\t mpd->len and submit pages underlying it for IO\n *\n * @handle - handle for journal operations\n * @mpd - extent to map\n * @give_up_on_write - we set this to true iff there is a fatal error and there\n *                     is no hope of writing the data. The caller should discard\n *                     dirty pages to avoid infinite loops.\n *\n * The function maps extent starting at mpd->lblk of length mpd->len. If it is\n * delayed, blocks are allocated, if it is unwritten, we may need to convert\n * them to initialized or split the described range from larger unwritten\n * extent. Note that we need not map all the described range since allocation\n * can return less blocks or the range is covered by more unwritten extents. We\n * cannot map more because we are limited by reserved transaction credits. On\n * the other hand we always make sure that the last touched page is fully\n * mapped so that it can be written out (and thus forward progress is\n * guaranteed). After mapping we submit all mapped pages for IO.\n */\nstatic int mpage_map_and_submit_extent(handle_t *handle,\n\t\t\t\t       struct mpage_da_data *mpd,\n\t\t\t\t       bool *give_up_on_write)\n{\n\tstruct inode *inode = mpd->inode;\n\tstruct ext4_map_blocks *map = &mpd->map;\n\tint err;\n\tloff_t disksize;\n\tint progress = 0;\n\text4_io_end_t *io_end = mpd->io_submit.io_end;\n\tstruct ext4_io_end_vec *io_end_vec = ext4_alloc_io_end_vec(io_end);\n\n\tio_end_vec->offset = ((loff_t)map->m_lblk) << inode->i_blkbits;\n\tdo {\n\t\terr = mpage_map_one_extent(handle, mpd);\n\t\tif (err < 0) {\n\t\t\tstruct super_block *sb = inode->i_sb;\n\n\t\t\tif (ext4_forced_shutdown(EXT4_SB(sb)) ||\n\t\t\t    EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED)\n\t\t\t\tgoto invalidate_dirty_pages;\n\t\t\t/*\n\t\t\t * Let the uper layers retry transient errors.\n\t\t\t * In the case of ENOSPC, if ext4_count_free_blocks()\n\t\t\t * is non-zero, a commit should free up blocks.\n\t\t\t */\n\t\t\tif ((err == -ENOMEM) ||\n\t\t\t    (err == -ENOSPC && ext4_count_free_clusters(sb))) {\n\t\t\t\tif (progress)\n\t\t\t\t\tgoto update_disksize;\n\t\t\t\treturn err;\n\t\t\t}\n\t\t\text4_msg(sb, KERN_CRIT,\n\t\t\t\t \"Delayed block allocation failed for \"\n\t\t\t\t \"inode %lu at logical offset %llu with\"\n\t\t\t\t \" max blocks %u with error %d\",\n\t\t\t\t inode->i_ino,\n\t\t\t\t (unsigned long long)map->m_lblk,\n\t\t\t\t (unsigned)map->m_len, -err);\n\t\t\text4_msg(sb, KERN_CRIT,\n\t\t\t\t \"This should not happen!! Data will \"\n\t\t\t\t \"be lost\\n\");\n\t\t\tif (err == -ENOSPC)\n\t\t\t\text4_print_free_blocks(inode);\n\t\tinvalidate_dirty_pages:\n\t\t\t*give_up_on_write = true;\n\t\t\treturn err;\n\t\t}\n\t\tprogress = 1;\n\t\t/*\n\t\t * Update buffer state, submit mapped pages, and get us new\n\t\t * extent to map\n\t\t */\n\t\terr = mpage_map_and_submit_buffers(mpd);\n\t\tif (err < 0)\n\t\t\tgoto update_disksize;\n\t} while (map->m_len);\n\nupdate_disksize:\n\t/*\n\t * Update on-disk size after IO is submitted.  Races with\n\t * truncate are avoided by checking i_size under i_data_sem.\n\t */\n\tdisksize = ((loff_t)mpd->first_page) << PAGE_SHIFT;\n\tif (disksize > EXT4_I(inode)->i_disksize) {\n\t\tint err2;\n\t\tloff_t i_size;\n\n\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\ti_size = i_size_read(inode);\n\t\tif (disksize > i_size)\n\t\t\tdisksize = i_size;\n\t\tif (disksize > EXT4_I(inode)->i_disksize)\n\t\t\tEXT4_I(inode)->i_disksize = disksize;\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\terr2 = ext4_mark_inode_dirty(handle, inode);\n\t\tif (err2)\n\t\t\text4_error(inode->i_sb,\n\t\t\t\t   \"Failed to mark inode %lu dirty\",\n\t\t\t\t   inode->i_ino);\n\t\tif (!err)\n\t\t\terr = err2;\n\t}\n\treturn err;\n}\n\n/*\n * Calculate the total number of credits to reserve for one writepages\n * iteration. This is called from ext4_writepages(). We map an extent of\n * up to MAX_WRITEPAGES_EXTENT_LEN blocks and then we go on and finish mapping\n * the last partial page. So in total we can map MAX_WRITEPAGES_EXTENT_LEN +\n * bpp - 1 blocks in bpp different extents.\n */\nstatic int ext4_da_writepages_trans_blocks(struct inode *inode)\n{\n\tint bpp = ext4_journal_blocks_per_page(inode);\n\n\treturn ext4_meta_trans_blocks(inode,\n\t\t\t\tMAX_WRITEPAGES_EXTENT_LEN + bpp - 1, bpp);\n}\n\n/*\n * mpage_prepare_extent_to_map - find & lock contiguous range of dirty pages\n * \t\t\t\t and underlying extent to map\n *\n * @mpd - where to look for pages\n *\n * Walk dirty pages in the mapping. If they are fully mapped, submit them for\n * IO immediately. When we find a page which isn't mapped we start accumulating\n * extent of buffers underlying these pages that needs mapping (formed by\n * either delayed or unwritten buffers). We also lock the pages containing\n * these buffers. The extent found is returned in @mpd structure (starting at\n * mpd->lblk with length mpd->len blocks).\n *\n * Note that this function can attach bios to one io_end structure which are\n * neither logically nor physically contiguous. Although it may seem as an\n * unnecessary complication, it is actually inevitable in blocksize < pagesize\n * case as we need to track IO to all buffers underlying a page in one io_end.\n */\nstatic int mpage_prepare_extent_to_map(struct mpage_da_data *mpd)\n{\n\tstruct address_space *mapping = mpd->inode->i_mapping;\n\tstruct pagevec pvec;\n\tunsigned int nr_pages;\n\tlong left = mpd->wbc->nr_to_write;\n\tpgoff_t index = mpd->first_page;\n\tpgoff_t end = mpd->last_page;\n\txa_mark_t tag;\n\tint i, err = 0;\n\tint blkbits = mpd->inode->i_blkbits;\n\text4_lblk_t lblk;\n\tstruct buffer_head *head;\n\n\tif (mpd->wbc->sync_mode == WB_SYNC_ALL || mpd->wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\n\n\tpagevec_init(&pvec);\n\tmpd->map.m_len = 0;\n\tmpd->next_page = index;\n\twhile (index <= end) {\n\t\tnr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,\n\t\t\t\ttag);\n\t\tif (nr_pages == 0)\n\t\t\tgoto out;\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/*\n\t\t\t * Accumulated enough dirty pages? This doesn't apply\n\t\t\t * to WB_SYNC_ALL mode. For integrity sync we have to\n\t\t\t * keep going because someone may be concurrently\n\t\t\t * dirtying pages, and we might have synced a lot of\n\t\t\t * newly appeared dirty pages, but have not synced all\n\t\t\t * of the old dirty pages.\n\t\t\t */\n\t\t\tif (mpd->wbc->sync_mode == WB_SYNC_NONE && left <= 0)\n\t\t\t\tgoto out;\n\n\t\t\t/* If we can't merge this page, we are done. */\n\t\t\tif (mpd->map.m_len > 0 && mpd->next_page != page->index)\n\t\t\t\tgoto out;\n\n\t\t\tlock_page(page);\n\t\t\t/*\n\t\t\t * If the page is no longer dirty, or its mapping no\n\t\t\t * longer corresponds to inode we are writing (which\n\t\t\t * means it has been truncated or invalidated), or the\n\t\t\t * page is already under writeback and we are not doing\n\t\t\t * a data integrity writeback, skip the page\n\t\t\t */\n\t\t\tif (!PageDirty(page) ||\n\t\t\t    (PageWriteback(page) &&\n\t\t\t     (mpd->wbc->sync_mode == WB_SYNC_NONE)) ||\n\t\t\t    unlikely(page->mapping != mapping)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\twait_on_page_writeback(page);\n\t\t\tBUG_ON(PageWriteback(page));\n\n\t\t\tif (mpd->map.m_len == 0)\n\t\t\t\tmpd->first_page = page->index;\n\t\t\tmpd->next_page = page->index + 1;\n\t\t\t/* Add all dirty buffers to mpd */\n\t\t\tlblk = ((ext4_lblk_t)page->index) <<\n\t\t\t\t(PAGE_SHIFT - blkbits);\n\t\t\thead = page_buffers(page);\n\t\t\terr = mpage_process_page_bufs(mpd, head, head, lblk);\n\t\t\tif (err <= 0)\n\t\t\t\tgoto out;\n\t\t\terr = 0;\n\t\t\tleft--;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n\treturn 0;\nout:\n\tpagevec_release(&pvec);\n\treturn err;\n}\n\nstatic int ext4_writepages(struct address_space *mapping,\n\t\t\t   struct writeback_control *wbc)\n{\n\tpgoff_t\twriteback_index = 0;\n\tlong nr_to_write = wbc->nr_to_write;\n\tint range_whole = 0;\n\tint cycled = 1;\n\thandle_t *handle = NULL;\n\tstruct mpage_da_data mpd;\n\tstruct inode *inode = mapping->host;\n\tint needed_blocks, rsv_blocks = 0, ret = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);\n\tbool done;\n\tstruct blk_plug plug;\n\tbool give_up_on_write = false;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tpercpu_down_read(&sbi->s_journal_flag_rwsem);\n\ttrace_ext4_writepages(inode, wbc);\n\n\t/*\n\t * No pages to write? This is mainly a kludge to avoid starting\n\t * a transaction for special inodes like journal inode on last iput()\n\t * because that could violate lock ordering on umount\n\t */\n\tif (!mapping->nrpages || !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\tgoto out_writepages;\n\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = generic_writepages(mapping, wbc);\n\t\tgoto out_writepages;\n\t}\n\n\t/*\n\t * If the filesystem has aborted, it is read-only, so return\n\t * right away instead of dumping stack traces later on that\n\t * will obscure the real source of the problem.  We test\n\t * EXT4_MF_FS_ABORTED instead of sb->s_flag's SB_RDONLY because\n\t * the latter could be true if the filesystem is mounted\n\t * read-only, and in that case, ext4_writepages should\n\t * *never* be called, so if that ever happens, we would want\n\t * the stack trace.\n\t */\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(mapping->host->i_sb)) ||\n\t\t     sbi->s_mount_flags & EXT4_MF_FS_ABORTED)) {\n\t\tret = -EROFS;\n\t\tgoto out_writepages;\n\t}\n\n\t/*\n\t * If we have inline data and arrive here, it means that\n\t * we will soon create the block for the 1st page, so\n\t * we'd better clear the inline data here.\n\t */\n\tif (ext4_has_inline_data(inode)) {\n\t\t/* Just inode will be modified... */\n\t\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 1);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tgoto out_writepages;\n\t\t}\n\t\tBUG_ON(ext4_test_inode_state(inode,\n\t\t\t\tEXT4_STATE_MAY_INLINE_DATA));\n\t\text4_destroy_inline_data(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (ext4_should_dioread_nolock(inode)) {\n\t\t/*\n\t\t * We may need to convert up to one extent per block in\n\t\t * the page and we may dirty the inode.\n\t\t */\n\t\trsv_blocks = 1 + ext4_chunk_trans_blocks(inode,\n\t\t\t\t\t\tPAGE_SIZE >> inode->i_blkbits);\n\t}\n\n\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\trange_whole = 1;\n\n\tif (wbc->range_cyclic) {\n\t\twriteback_index = mapping->writeback_index;\n\t\tif (writeback_index)\n\t\t\tcycled = 0;\n\t\tmpd.first_page = writeback_index;\n\t\tmpd.last_page = -1;\n\t} else {\n\t\tmpd.first_page = wbc->range_start >> PAGE_SHIFT;\n\t\tmpd.last_page = wbc->range_end >> PAGE_SHIFT;\n\t}\n\n\tmpd.inode = inode;\n\tmpd.wbc = wbc;\n\text4_io_submit_init(&mpd.io_submit, wbc);\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, mpd.first_page, mpd.last_page);\n\tdone = false;\n\tblk_start_plug(&plug);\n\n\t/*\n\t * First writeback pages that don't need mapping - we can avoid\n\t * starting a transaction unnecessarily and also avoid being blocked\n\t * in the block layer on device congestion while having transaction\n\t * started.\n\t */\n\tmpd.do_map = 0;\n\tmpd.io_submit.io_end = ext4_init_io_end(inode, GFP_KERNEL);\n\tif (!mpd.io_submit.io_end) {\n\t\tret = -ENOMEM;\n\t\tgoto unplug;\n\t}\n\tret = mpage_prepare_extent_to_map(&mpd);\n\t/* Unlock pages we didn't use */\n\tmpage_release_unused_pages(&mpd, false);\n\t/* Submit prepared bio */\n\text4_io_submit(&mpd.io_submit);\n\text4_put_io_end_defer(mpd.io_submit.io_end);\n\tmpd.io_submit.io_end = NULL;\n\tif (ret < 0)\n\t\tgoto unplug;\n\n\twhile (!done && mpd.first_page <= mpd.last_page) {\n\t\t/* For each extent of pages we use new io_end */\n\t\tmpd.io_submit.io_end = ext4_init_io_end(inode, GFP_KERNEL);\n\t\tif (!mpd.io_submit.io_end) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We have two constraints: We find one extent to map and we\n\t\t * must always write out whole page (makes a difference when\n\t\t * blocksize < pagesize) so that we don't block on IO when we\n\t\t * try to write out the rest of the page. Journalled mode is\n\t\t * not supported by delalloc.\n\t\t */\n\t\tBUG_ON(ext4_should_journal_data(inode));\n\t\tneeded_blocks = ext4_da_writepages_trans_blocks(inode);\n\n\t\t/* start a new transaction */\n\t\thandle = ext4_journal_start_with_reserve(inode,\n\t\t\t\tEXT4_HT_WRITE_PAGE, needed_blocks, rsv_blocks);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\text4_msg(inode->i_sb, KERN_CRIT, \"%s: jbd2_start: \"\n\t\t\t       \"%ld pages, ino %lu; err %d\", __func__,\n\t\t\t\twbc->nr_to_write, inode->i_ino, ret);\n\t\t\t/* Release allocated io_end */\n\t\t\text4_put_io_end(mpd.io_submit.io_end);\n\t\t\tmpd.io_submit.io_end = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tmpd.do_map = 1;\n\n\t\ttrace_ext4_da_write_pages(inode, mpd.first_page, mpd.wbc);\n\t\tret = mpage_prepare_extent_to_map(&mpd);\n\t\tif (!ret) {\n\t\t\tif (mpd.map.m_len)\n\t\t\t\tret = mpage_map_and_submit_extent(handle, &mpd,\n\t\t\t\t\t&give_up_on_write);\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We scanned the whole range (or exhausted\n\t\t\t\t * nr_to_write), submitted what was mapped and\n\t\t\t\t * didn't find anything needing mapping. We are\n\t\t\t\t * done.\n\t\t\t\t */\n\t\t\t\tdone = true;\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Caution: If the handle is synchronous,\n\t\t * ext4_journal_stop() can wait for transaction commit\n\t\t * to finish which may depend on writeback of pages to\n\t\t * complete or on page lock to be released.  In that\n\t\t * case, we have to wait until after after we have\n\t\t * submitted all the IO, released page locks we hold,\n\t\t * and dropped io_end reference (for extent conversion\n\t\t * to be able to complete) before stopping the handle.\n\t\t */\n\t\tif (!ext4_handle_valid(handle) || handle->h_sync == 0) {\n\t\t\text4_journal_stop(handle);\n\t\t\thandle = NULL;\n\t\t\tmpd.do_map = 0;\n\t\t}\n\t\t/* Unlock pages we didn't use */\n\t\tmpage_release_unused_pages(&mpd, give_up_on_write);\n\t\t/* Submit prepared bio */\n\t\text4_io_submit(&mpd.io_submit);\n\n\t\t/*\n\t\t * Drop our io_end reference we got from init. We have\n\t\t * to be careful and use deferred io_end finishing if\n\t\t * we are still holding the transaction as we can\n\t\t * release the last reference to io_end which may end\n\t\t * up doing unwritten extent conversion.\n\t\t */\n\t\tif (handle) {\n\t\t\text4_put_io_end_defer(mpd.io_submit.io_end);\n\t\t\text4_journal_stop(handle);\n\t\t} else\n\t\t\text4_put_io_end(mpd.io_submit.io_end);\n\t\tmpd.io_submit.io_end = NULL;\n\n\t\tif (ret == -ENOSPC && sbi->s_journal) {\n\t\t\t/*\n\t\t\t * Commit the transaction which would\n\t\t\t * free blocks released in the transaction\n\t\t\t * and try again\n\t\t\t */\n\t\t\tjbd2_journal_force_commit_nested(sbi->s_journal);\n\t\t\tret = 0;\n\t\t\tcontinue;\n\t\t}\n\t\t/* Fatal error - ENOMEM, EIO... */\n\t\tif (ret)\n\t\t\tbreak;\n\t}\nunplug:\n\tblk_finish_plug(&plug);\n\tif (!ret && !cycled && wbc->nr_to_write > 0) {\n\t\tcycled = 1;\n\t\tmpd.last_page = writeback_index - 1;\n\t\tmpd.first_page = 0;\n\t\tgoto retry;\n\t}\n\n\t/* Update index */\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\t/*\n\t\t * Set the writeback_index so that range_cyclic\n\t\t * mode will write it back later\n\t\t */\n\t\tmapping->writeback_index = mpd.first_page;\n\nout_writepages:\n\ttrace_ext4_writepages_result(inode, wbc, ret,\n\t\t\t\t     nr_to_write - wbc->nr_to_write);\n\tpercpu_up_read(&sbi->s_journal_flag_rwsem);\n\treturn ret;\n}\n\nstatic int ext4_dax_writepages(struct address_space *mapping,\n\t\t\t       struct writeback_control *wbc)\n{\n\tint ret;\n\tlong nr_to_write = wbc->nr_to_write;\n\tstruct inode *inode = mapping->host;\n\tstruct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tpercpu_down_read(&sbi->s_journal_flag_rwsem);\n\ttrace_ext4_writepages(inode, wbc);\n\n\tret = dax_writeback_mapping_range(mapping, inode->i_sb->s_bdev, wbc);\n\ttrace_ext4_writepages_result(inode, wbc, ret,\n\t\t\t\t     nr_to_write - wbc->nr_to_write);\n\tpercpu_up_read(&sbi->s_journal_flag_rwsem);\n\treturn ret;\n}\n\nstatic int ext4_nonda_switch(struct super_block *sb)\n{\n\ts64 free_clusters, dirty_clusters;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\t/*\n\t * switch to non delalloc mode if we are running low\n\t * on free block. The free block accounting via percpu\n\t * counters can get slightly wrong with percpu_counter_batch getting\n\t * accumulated on each CPU without updating global counters\n\t * Delalloc need an accurate free block accounting. So switch\n\t * to non delalloc when we are near to error range.\n\t */\n\tfree_clusters =\n\t\tpercpu_counter_read_positive(&sbi->s_freeclusters_counter);\n\tdirty_clusters =\n\t\tpercpu_counter_read_positive(&sbi->s_dirtyclusters_counter);\n\t/*\n\t * Start pushing delalloc when 1/2 of free blocks are dirty.\n\t */\n\tif (dirty_clusters && (free_clusters < 2 * dirty_clusters))\n\t\ttry_to_writeback_inodes_sb(sb, WB_REASON_FS_FREE_SPACE);\n\n\tif (2 * free_clusters < 3 * dirty_clusters ||\n\t    free_clusters < (dirty_clusters + EXT4_FREECLUSTERS_WATERMARK)) {\n\t\t/*\n\t\t * free block count is less than 150% of dirty blocks\n\t\t * or free blocks is less than watermark\n\t\t */\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/* We always reserve for an inode update; the superblock could be there too */\nstatic int ext4_da_write_credits(struct inode *inode, loff_t pos, unsigned len)\n{\n\tif (likely(ext4_has_feature_large_file(inode->i_sb)))\n\t\treturn 1;\n\n\tif (pos + len <= 0x7fffffffULL)\n\t\treturn 1;\n\n\t/* We might need to update the superblock to set LARGE_FILE */\n\treturn 2;\n}\n\nstatic int ext4_da_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t       loff_t pos, unsigned len, unsigned flags,\n\t\t\t       struct page **pagep, void **fsdata)\n{\n\tint ret, retries = 0;\n\tstruct page *page;\n\tpgoff_t index;\n\tstruct inode *inode = mapping->host;\n\thandle_t *handle;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tindex = pos >> PAGE_SHIFT;\n\n\tif (ext4_nonda_switch(inode->i_sb) || S_ISLNK(inode->i_mode) ||\n\t    ext4_verity_in_progress(inode)) {\n\t\t*fsdata = (void *)FALL_BACK_TO_NONDELALLOC;\n\t\treturn ext4_write_begin(file, mapping, pos,\n\t\t\t\t\tlen, flags, pagep, fsdata);\n\t}\n\t*fsdata = (void *)0;\n\ttrace_ext4_da_write_begin(inode, pos, len, flags);\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) {\n\t\tret = ext4_da_write_inline_data_begin(mapping, inode,\n\t\t\t\t\t\t      pos, len, flags,\n\t\t\t\t\t\t      pagep, fsdata);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (ret == 1)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * grab_cache_page_write_begin() can take a long time if the\n\t * system is thrashing due to memory pressure, or if the page\n\t * is being written back.  So grab it first before we start\n\t * the transaction handle.  This also allows us to allocate\n\t * the page (if needed) without using GFP_NOFS.\n\t */\nretry_grab:\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page)\n\t\treturn -ENOMEM;\n\tunlock_page(page);\n\n\t/*\n\t * With delayed allocation, we don't log the i_disksize update\n\t * if there is delayed block allocation. But we still need\n\t * to journalling the i_disksize update if writes to the end\n\t * of file which has an already mapped buffer.\n\t */\nretry_journal:\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,\n\t\t\t\text4_da_write_credits(inode, pos, len));\n\tif (IS_ERR(handle)) {\n\t\tput_page(page);\n\t\treturn PTR_ERR(handle);\n\t}\n\n\tlock_page(page);\n\tif (page->mapping != mapping) {\n\t\t/* The page got truncated from under us */\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\text4_journal_stop(handle);\n\t\tgoto retry_grab;\n\t}\n\t/* In case writeback began while the page was unlocked */\n\twait_for_stable_page(page);\n\n#ifdef CONFIG_FS_ENCRYPTION\n\tret = ext4_block_write_begin(page, pos, len,\n\t\t\t\t     ext4_da_get_block_prep);\n#else\n\tret = __block_write_begin(page, pos, len, ext4_da_get_block_prep);\n#endif\n\tif (ret < 0) {\n\t\tunlock_page(page);\n\t\text4_journal_stop(handle);\n\t\t/*\n\t\t * block_write_begin may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again. Don't need\n\t\t * i_size_read because we hold i_mutex.\n\t\t */\n\t\tif (pos + len > inode->i_size)\n\t\t\text4_truncate_failed_write(inode);\n\n\t\tif (ret == -ENOSPC &&\n\t\t    ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\t\tgoto retry_journal;\n\n\t\tput_page(page);\n\t\treturn ret;\n\t}\n\n\t*pagep = page;\n\treturn ret;\n}\n\n/*\n * Check if we should update i_disksize\n * when write to the end of file but not require block allocation\n */\nstatic int ext4_da_should_update_i_disksize(struct page *page,\n\t\t\t\t\t    unsigned long offset)\n{\n\tstruct buffer_head *bh;\n\tstruct inode *inode = page->mapping->host;\n\tunsigned int idx;\n\tint i;\n\n\tbh = page_buffers(page);\n\tidx = offset >> inode->i_blkbits;\n\n\tfor (i = 0; i < idx; i++)\n\t\tbh = bh->b_this_page;\n\n\tif (!buffer_mapped(bh) || (buffer_delay(bh)) || buffer_unwritten(bh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_da_write_end(struct file *file,\n\t\t\t     struct address_space *mapping,\n\t\t\t     loff_t pos, unsigned len, unsigned copied,\n\t\t\t     struct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\thandle_t *handle = ext4_journal_current_handle();\n\tloff_t new_i_size;\n\tunsigned long start, end;\n\tint write_mode = (int)(unsigned long)fsdata;\n\n\tif (write_mode == FALL_BACK_TO_NONDELALLOC)\n\t\treturn ext4_write_end(file, mapping, pos,\n\t\t\t\t      len, copied, page, fsdata);\n\n\ttrace_ext4_da_write_end(inode, pos, len, copied);\n\tstart = pos & (PAGE_SIZE - 1);\n\tend = start + copied - 1;\n\n\t/*\n\t * generic_write_end() will run mark_inode_dirty() if i_size\n\t * changes.  So let's piggyback the i_disksize mark_inode_dirty\n\t * into that.\n\t */\n\tnew_i_size = pos + copied;\n\tif (copied && new_i_size > EXT4_I(inode)->i_disksize) {\n\t\tif (ext4_has_inline_data(inode) ||\n\t\t    ext4_da_should_update_i_disksize(page, end)) {\n\t\t\text4_update_i_disksize(inode, new_i_size);\n\t\t\t/* We need to mark inode dirty even if\n\t\t\t * new_i_size is less that inode->i_size\n\t\t\t * bu greater than i_disksize.(hint delalloc)\n\t\t\t */\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t}\n\t}\n\n\tif (write_mode != CONVERT_INLINE_DATA &&\n\t    ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA) &&\n\t    ext4_has_inline_data(inode))\n\t\tret2 = ext4_da_write_inline_data_end(inode, pos, len, copied,\n\t\t\t\t\t\t     page);\n\telse\n\t\tret2 = generic_write_end(file, mapping, pos, len, copied,\n\t\t\t\t\t\t\tpage, fsdata);\n\n\tcopied = ret2;\n\tif (ret2 < 0)\n\t\tret = ret2;\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\treturn ret ? ret : copied;\n}\n\n/*\n * Force all delayed allocation blocks to be allocated for a given inode.\n */\nint ext4_alloc_da_blocks(struct inode *inode)\n{\n\ttrace_ext4_alloc_da_blocks(inode);\n\n\tif (!EXT4_I(inode)->i_reserved_data_blocks)\n\t\treturn 0;\n\n\t/*\n\t * We do something simple for now.  The filemap_flush() will\n\t * also start triggering a write of the data blocks, which is\n\t * not strictly speaking necessary (and for users of\n\t * laptop_mode, not even desirable).  However, to do otherwise\n\t * would require replicating code paths in:\n\t *\n\t * ext4_writepages() ->\n\t *    write_cache_pages() ---> (via passed in callback function)\n\t *        __mpage_da_writepage() -->\n\t *           mpage_add_bh_to_extent()\n\t *           mpage_da_map_blocks()\n\t *\n\t * The problem is that write_cache_pages(), located in\n\t * mm/page-writeback.c, marks pages clean in preparation for\n\t * doing I/O, which is not desirable if we're not planning on\n\t * doing I/O at all.\n\t *\n\t * We could call write_cache_pages(), and then redirty all of\n\t * the pages by calling redirty_page_for_writepage() but that\n\t * would be ugly in the extreme.  So instead we would need to\n\t * replicate parts of the code in the above functions,\n\t * simplifying them because we wouldn't actually intend to\n\t * write out the pages, but rather only collect contiguous\n\t * logical block extents, call the multi-block allocator, and\n\t * then update the buffer heads with the block allocations.\n\t *\n\t * For now, though, we'll cheat by calling filemap_flush(),\n\t * which will map the blocks, and start the I/O, but not\n\t * actually wait for the I/O to complete.\n\t */\n\treturn filemap_flush(inode->i_mapping);\n}\n\n/*\n * bmap() is special.  It gets used by applications such as lilo and by\n * the swapper to find the on-disk block of a specific piece of data.\n *\n * Naturally, this is dangerous if the block concerned is still in the\n * journal.  If somebody makes a swapfile on an ext4 data-journaling\n * filesystem and enables swap, then they may get a nasty shock when the\n * data getting swapped to that swapfile suddenly gets overwritten by\n * the original zero's written out previously to the journal and\n * awaiting writeback in the kernel's buffer cache.\n *\n * So, if we see any bmap calls here on a modified, data-journaled file,\n * take extra steps to flush any blocks which might be in the cache.\n */\nstatic sector_t ext4_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\tjournal_t *journal;\n\tint err;\n\n\t/*\n\t * We can get here for an inline file via the FIBMAP ioctl\n\t */\n\tif (ext4_has_inline_data(inode))\n\t\treturn 0;\n\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) &&\n\t\t\ttest_opt(inode->i_sb, DELALLOC)) {\n\t\t/*\n\t\t * With delalloc we want to sync the file\n\t\t * so that we can make sure we allocate\n\t\t * blocks for file\n\t\t */\n\t\tfilemap_write_and_wait(mapping);\n\t}\n\n\tif (EXT4_JOURNAL(inode) &&\n\t    ext4_test_inode_state(inode, EXT4_STATE_JDATA)) {\n\t\t/*\n\t\t * This is a REALLY heavyweight approach, but the use of\n\t\t * bmap on dirty files is expected to be extremely rare:\n\t\t * only if we run lilo or swapon on a freshly made file\n\t\t * do we expect this to happen.\n\t\t *\n\t\t * (bmap requires CAP_SYS_RAWIO so this does not\n\t\t * represent an unprivileged user DOS attack --- we'd be\n\t\t * in trouble if mortal users could trigger this path at\n\t\t * will.)\n\t\t *\n\t\t * NB. EXT4_STATE_JDATA is not set on files other than\n\t\t * regular files.  If somebody wants to bmap a directory\n\t\t * or symlink and gets confused because the buffer\n\t\t * hasn't yet been flushed to disk, they deserve\n\t\t * everything they get.\n\t\t */\n\n\t\text4_clear_inode_state(inode, EXT4_STATE_JDATA);\n\t\tjournal = EXT4_JOURNAL(inode);\n\t\tjbd2_journal_lock_updates(journal);\n\t\terr = jbd2_journal_flush(journal);\n\t\tjbd2_journal_unlock_updates(journal);\n\n\t\tif (err)\n\t\t\treturn 0;\n\t}\n\n\treturn generic_block_bmap(mapping, block, ext4_get_block);\n}\n\nstatic int ext4_readpage(struct file *file, struct page *page)\n{\n\tint ret = -EAGAIN;\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_ext4_readpage(page);\n\n\tif (ext4_has_inline_data(inode))\n\t\tret = ext4_readpage_inline(inode, page);\n\n\tif (ret == -EAGAIN)\n\t\treturn ext4_mpage_readpages(page->mapping, NULL, page, 1,\n\t\t\t\t\t\tfalse);\n\n\treturn ret;\n}\n\nstatic int\next4_readpages(struct file *file, struct address_space *mapping,\n\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\tstruct inode *inode = mapping->host;\n\n\t/* If the file has inline data, no need to do readpages. */\n\tif (ext4_has_inline_data(inode))\n\t\treturn 0;\n\n\treturn ext4_mpage_readpages(mapping, pages, NULL, nr_pages, true);\n}\n\nstatic void ext4_invalidatepage(struct page *page, unsigned int offset,\n\t\t\t\tunsigned int length)\n{\n\ttrace_ext4_invalidatepage(page, offset, length);\n\n\t/* No journalling happens on data buffers when this function is used */\n\tWARN_ON(page_has_buffers(page) && buffer_jbd(page_buffers(page)));\n\n\tblock_invalidatepage(page, offset, length);\n}\n\nstatic int __ext4_journalled_invalidatepage(struct page *page,\n\t\t\t\t\t    unsigned int offset,\n\t\t\t\t\t    unsigned int length)\n{\n\tjournal_t *journal = EXT4_JOURNAL(page->mapping->host);\n\n\ttrace_ext4_journalled_invalidatepage(page, offset, length);\n\n\t/*\n\t * If it's a full truncate we just forget about the pending dirtying\n\t */\n\tif (offset == 0 && length == PAGE_SIZE)\n\t\tClearPageChecked(page);\n\n\treturn jbd2_journal_invalidatepage(journal, page, offset, length);\n}\n\n/* Wrapper for aops... */\nstatic void ext4_journalled_invalidatepage(struct page *page,\n\t\t\t\t\t   unsigned int offset,\n\t\t\t\t\t   unsigned int length)\n{\n\tWARN_ON(__ext4_journalled_invalidatepage(page, offset, length) < 0);\n}\n\nstatic int ext4_releasepage(struct page *page, gfp_t wait)\n{\n\tjournal_t *journal = EXT4_JOURNAL(page->mapping->host);\n\n\ttrace_ext4_releasepage(page);\n\n\t/* Page has dirty journalled data -> cannot release */\n\tif (PageChecked(page))\n\t\treturn 0;\n\tif (journal)\n\t\treturn jbd2_journal_try_to_free_buffers(journal, page, wait);\n\telse\n\t\treturn try_to_free_buffers(page);\n}\n\nstatic bool ext4_inode_datasync_dirty(struct inode *inode)\n{\n\tjournal_t *journal = EXT4_SB(inode->i_sb)->s_journal;\n\n\tif (journal)\n\t\treturn !jbd2_transaction_committed(journal,\n\t\t\t\t\tEXT4_I(inode)->i_datasync_tid);\n\t/* Any metadata buffers to write? */\n\tif (!list_empty(&inode->i_mapping->private_list))\n\t\treturn true;\n\treturn inode->i_state & I_DIRTY_DATASYNC;\n}\n\nstatic void ext4_set_iomap(struct inode *inode, struct iomap *iomap,\n\t\t\t   struct ext4_map_blocks *map, loff_t offset,\n\t\t\t   loff_t length)\n{\n\tu8 blkbits = inode->i_blkbits;\n\n\t/*\n\t * Writes that span EOF might trigger an I/O size update on completion,\n\t * so consider them to be dirty for the purpose of O_DSYNC, even if\n\t * there is no other metadata changes being made or are pending.\n\t */\n\tiomap->flags = 0;\n\tif (ext4_inode_datasync_dirty(inode) ||\n\t    offset + length > i_size_read(inode))\n\t\tiomap->flags |= IOMAP_F_DIRTY;\n\n\tif (map->m_flags & EXT4_MAP_NEW)\n\t\tiomap->flags |= IOMAP_F_NEW;\n\n\tiomap->bdev = inode->i_sb->s_bdev;\n\tiomap->dax_dev = EXT4_SB(inode->i_sb)->s_daxdev;\n\tiomap->offset = (u64) map->m_lblk << blkbits;\n\tiomap->length = (u64) map->m_len << blkbits;\n\n\t/*\n\t * Flags passed to ext4_map_blocks() for direct I/O writes can result\n\t * in m_flags having both EXT4_MAP_MAPPED and EXT4_MAP_UNWRITTEN bits\n\t * set. In order for any allocated unwritten extents to be converted\n\t * into written extents correctly within the ->end_io() handler, we\n\t * need to ensure that the iomap->type is set appropriately. Hence, the\n\t * reason why we need to check whether the EXT4_MAP_UNWRITTEN bit has\n\t * been set first.\n\t */\n\tif (map->m_flags & EXT4_MAP_UNWRITTEN) {\n\t\tiomap->type = IOMAP_UNWRITTEN;\n\t\tiomap->addr = (u64) map->m_pblk << blkbits;\n\t} else if (map->m_flags & EXT4_MAP_MAPPED) {\n\t\tiomap->type = IOMAP_MAPPED;\n\t\tiomap->addr = (u64) map->m_pblk << blkbits;\n\t} else {\n\t\tiomap->type = IOMAP_HOLE;\n\t\tiomap->addr = IOMAP_NULL_ADDR;\n\t}\n}\n\nstatic int ext4_iomap_alloc(struct inode *inode, struct ext4_map_blocks *map,\n\t\t\t    unsigned int flags)\n{\n\thandle_t *handle;\n\tu8 blkbits = inode->i_blkbits;\n\tint ret, dio_credits, m_flags = 0, retries = 0;\n\n\t/*\n\t * Trim the mapping request to the maximum value that we can map at\n\t * once for direct I/O.\n\t */\n\tif (map->m_len > DIO_MAX_BLOCKS)\n\t\tmap->m_len = DIO_MAX_BLOCKS;\n\tdio_credits = ext4_chunk_trans_blocks(inode, map->m_len);\n\nretry:\n\t/*\n\t * Either we allocate blocks and then don't get an unwritten extent, so\n\t * in that case we have reserved enough credits. Or, the blocks are\n\t * already allocated and unwritten. In that case, the extent conversion\n\t * fits into the credits as well.\n\t */\n\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS, dio_credits);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\t/*\n\t * DAX and direct I/O are the only two operations that are currently\n\t * supported with IOMAP_WRITE.\n\t */\n\tWARN_ON(!IS_DAX(inode) && !(flags & IOMAP_DIRECT));\n\tif (IS_DAX(inode))\n\t\tm_flags = EXT4_GET_BLOCKS_CREATE_ZERO;\n\t/*\n\t * We use i_size instead of i_disksize here because delalloc writeback\n\t * can complete at any point during the I/O and subsequently push the\n\t * i_disksize out to i_size. This could be beyond where direct I/O is\n\t * happening and thus expose allocated blocks to direct I/O reads.\n\t */\n\telse if ((map->m_lblk * (1 << blkbits)) >= i_size_read(inode))\n\t\tm_flags = EXT4_GET_BLOCKS_CREATE;\n\telse if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tm_flags = EXT4_GET_BLOCKS_IO_CREATE_EXT;\n\n\tret = ext4_map_blocks(handle, inode, map, m_flags);\n\n\t/*\n\t * We cannot fill holes in indirect tree based inodes as that could\n\t * expose stale data in the case of a crash. Use the magic error code\n\t * to fallback to buffered I/O.\n\t */\n\tif (!m_flags && !ret)\n\t\tret = -ENOTBLK;\n\n\text4_journal_stop(handle);\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\n\n\treturn ret;\n}\n\n\nstatic int ext4_iomap_begin(struct inode *inode, loff_t offset, loff_t length,\n\t\tunsigned flags, struct iomap *iomap, struct iomap *srcmap)\n{\n\tint ret;\n\tstruct ext4_map_blocks map;\n\tu8 blkbits = inode->i_blkbits;\n\n\tif ((offset >> blkbits) > EXT4_MAX_LOGICAL_BLOCK)\n\t\treturn -EINVAL;\n\n\tif (WARN_ON_ONCE(ext4_has_inline_data(inode)))\n\t\treturn -ERANGE;\n\n\t/*\n\t * Calculate the first and last logical blocks respectively.\n\t */\n\tmap.m_lblk = offset >> blkbits;\n\tmap.m_len = min_t(loff_t, (offset + length - 1) >> blkbits,\n\t\t\t  EXT4_MAX_LOGICAL_BLOCK) - map.m_lblk + 1;\n\n\tif (flags & IOMAP_WRITE)\n\t\tret = ext4_iomap_alloc(inode, &map, flags);\n\telse\n\t\tret = ext4_map_blocks(NULL, inode, &map, 0);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\text4_set_iomap(inode, iomap, &map, offset, length);\n\n\treturn 0;\n}\n\nstatic int ext4_iomap_end(struct inode *inode, loff_t offset, loff_t length,\n\t\t\t  ssize_t written, unsigned flags, struct iomap *iomap)\n{\n\t/*\n\t * Check to see whether an error occurred while writing out the data to\n\t * the allocated blocks. If so, return the magic error code so that we\n\t * fallback to buffered I/O and attempt to complete the remainder of\n\t * the I/O. Any blocks that may have been allocated in preparation for\n\t * the direct I/O will be reused during buffered I/O.\n\t */\n\tif (flags & (IOMAP_WRITE | IOMAP_DIRECT) && written == 0)\n\t\treturn -ENOTBLK;\n\n\treturn 0;\n}\n\nconst struct iomap_ops ext4_iomap_ops = {\n\t.iomap_begin\t\t= ext4_iomap_begin,\n\t.iomap_end\t\t= ext4_iomap_end,\n};\n\nstatic bool ext4_iomap_is_delalloc(struct inode *inode,\n\t\t\t\t   struct ext4_map_blocks *map)\n{\n\tstruct extent_status es;\n\text4_lblk_t offset = 0, end = map->m_lblk + map->m_len - 1;\n\n\text4_es_find_extent_range(inode, &ext4_es_is_delayed,\n\t\t\t\t  map->m_lblk, end, &es);\n\n\tif (!es.es_len || es.es_lblk > end)\n\t\treturn false;\n\n\tif (es.es_lblk > map->m_lblk) {\n\t\tmap->m_len = es.es_lblk - map->m_lblk;\n\t\treturn false;\n\t}\n\n\toffset = map->m_lblk - es.es_lblk;\n\tmap->m_len = es.es_len - offset;\n\n\treturn true;\n}\n\nstatic int ext4_iomap_begin_report(struct inode *inode, loff_t offset,\n\t\t\t\t   loff_t length, unsigned int flags,\n\t\t\t\t   struct iomap *iomap, struct iomap *srcmap)\n{\n\tint ret;\n\tbool delalloc = false;\n\tstruct ext4_map_blocks map;\n\tu8 blkbits = inode->i_blkbits;\n\n\tif ((offset >> blkbits) > EXT4_MAX_LOGICAL_BLOCK)\n\t\treturn -EINVAL;\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tret = ext4_inline_data_iomap(inode, iomap);\n\t\tif (ret != -EAGAIN) {\n\t\t\tif (ret == 0 && offset >= iomap->length)\n\t\t\t\tret = -ENOENT;\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the first and last logical block respectively.\n\t */\n\tmap.m_lblk = offset >> blkbits;\n\tmap.m_len = min_t(loff_t, (offset + length - 1) >> blkbits,\n\t\t\t  EXT4_MAX_LOGICAL_BLOCK) - map.m_lblk + 1;\n\n\tret = ext4_map_blocks(NULL, inode, &map, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret == 0)\n\t\tdelalloc = ext4_iomap_is_delalloc(inode, &map);\n\n\text4_set_iomap(inode, iomap, &map, offset, length);\n\tif (delalloc && iomap->type == IOMAP_HOLE)\n\t\tiomap->type = IOMAP_DELALLOC;\n\n\treturn 0;\n}\n\nconst struct iomap_ops ext4_iomap_report_ops = {\n\t.iomap_begin = ext4_iomap_begin_report,\n};\n\n/*\n * Pages can be marked dirty completely asynchronously from ext4's journalling\n * activity.  By filemap_sync_pte(), try_to_unmap_one(), etc.  We cannot do\n * much here because ->set_page_dirty is called under VFS locks.  The page is\n * not necessarily locked.\n *\n * We cannot just dirty the page and leave attached buffers clean, because the\n * buffers' dirty state is \"definitive\".  We cannot just set the buffers dirty\n * or jbddirty because all the journalling code will explode.\n *\n * So what we do is to mark the page \"pending dirty\" and next time writepage\n * is called, propagate that into the buffers appropriately.\n */\nstatic int ext4_journalled_set_page_dirty(struct page *page)\n{\n\tSetPageChecked(page);\n\treturn __set_page_dirty_nobuffers(page);\n}\n\nstatic int ext4_set_page_dirty(struct page *page)\n{\n\tWARN_ON_ONCE(!PageLocked(page) && !PageDirty(page));\n\tWARN_ON_ONCE(!page_has_buffers(page));\n\treturn __set_page_dirty_buffers(page);\n}\n\nstatic const struct address_space_operations ext4_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.writepages\t\t= ext4_writepages,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_write_end,\n\t.set_page_dirty\t\t= ext4_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= noop_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_journalled_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.writepages\t\t= ext4_writepages,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_journalled_write_end,\n\t.set_page_dirty\t\t= ext4_journalled_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_journalled_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= noop_direct_IO,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_da_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.writepages\t\t= ext4_writepages,\n\t.write_begin\t\t= ext4_da_write_begin,\n\t.write_end\t\t= ext4_da_write_end,\n\t.set_page_dirty\t\t= ext4_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= noop_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_dax_aops = {\n\t.writepages\t\t= ext4_dax_writepages,\n\t.direct_IO\t\t= noop_direct_IO,\n\t.set_page_dirty\t\t= noop_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= noop_invalidatepage,\n};\n\nvoid ext4_set_aops(struct inode *inode)\n{\n\tswitch (ext4_inode_journal_mode(inode)) {\n\tcase EXT4_INODE_ORDERED_DATA_MODE:\n\tcase EXT4_INODE_WRITEBACK_DATA_MODE:\n\t\tbreak;\n\tcase EXT4_INODE_JOURNAL_DATA_MODE:\n\t\tinode->i_mapping->a_ops = &ext4_journalled_aops;\n\t\treturn;\n\tdefault:\n\t\tBUG();\n\t}\n\tif (IS_DAX(inode))\n\t\tinode->i_mapping->a_ops = &ext4_dax_aops;\n\telse if (test_opt(inode->i_sb, DELALLOC))\n\t\tinode->i_mapping->a_ops = &ext4_da_aops;\n\telse\n\t\tinode->i_mapping->a_ops = &ext4_aops;\n}\n\nstatic int __ext4_block_zero_page_range(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from, loff_t length)\n{\n\text4_fsblk_t index = from >> PAGE_SHIFT;\n\tunsigned offset = from & (PAGE_SIZE-1);\n\tunsigned blocksize, pos;\n\text4_lblk_t iblock;\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head *bh;\n\tstruct page *page;\n\tint err = 0;\n\n\tpage = find_or_create_page(mapping, from >> PAGE_SHIFT,\n\t\t\t\t   mapping_gfp_constraint(mapping, ~__GFP_FS));\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tblocksize = inode->i_sb->s_blocksize;\n\n\tiblock = index << (PAGE_SHIFT - inode->i_sb->s_blocksize_bits);\n\n\tif (!page_has_buffers(page))\n\t\tcreate_empty_buffers(page, blocksize, 0);\n\n\t/* Find the buffer that contains \"offset\" */\n\tbh = page_buffers(page);\n\tpos = blocksize;\n\twhile (offset >= pos) {\n\t\tbh = bh->b_this_page;\n\t\tiblock++;\n\t\tpos += blocksize;\n\t}\n\tif (buffer_freed(bh)) {\n\t\tBUFFER_TRACE(bh, \"freed: skip\");\n\t\tgoto unlock;\n\t}\n\tif (!buffer_mapped(bh)) {\n\t\tBUFFER_TRACE(bh, \"unmapped\");\n\t\text4_get_block(inode, iblock, bh, 0);\n\t\t/* unmapped? It's a hole - nothing to do */\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tBUFFER_TRACE(bh, \"still unmapped\");\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t/* Ok, it's mapped. Make sure it's up-to-date */\n\tif (PageUptodate(page))\n\t\tset_buffer_uptodate(bh);\n\n\tif (!buffer_uptodate(bh)) {\n\t\terr = -EIO;\n\t\tll_rw_block(REQ_OP_READ, 0, 1, &bh);\n\t\twait_on_buffer(bh);\n\t\t/* Uhhuh. Read error. Complain and punt. */\n\t\tif (!buffer_uptodate(bh))\n\t\t\tgoto unlock;\n\t\tif (S_ISREG(inode->i_mode) && IS_ENCRYPTED(inode)) {\n\t\t\t/* We expect the key to be set. */\n\t\t\tBUG_ON(!fscrypt_has_encryption_key(inode));\n\t\t\tWARN_ON_ONCE(fscrypt_decrypt_pagecache_blocks(\n\t\t\t\t\tpage, blocksize, bh_offset(bh)));\n\t\t}\n\t}\n\tif (ext4_should_journal_data(inode)) {\n\t\tBUFFER_TRACE(bh, \"get write access\");\n\t\terr = ext4_journal_get_write_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\tzero_user(page, offset, length);\n\tBUFFER_TRACE(bh, \"zeroed end of block\");\n\n\tif (ext4_should_journal_data(inode)) {\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t} else {\n\t\terr = 0;\n\t\tmark_buffer_dirty(bh);\n\t\tif (ext4_should_order_data(inode))\n\t\t\terr = ext4_jbd2_inode_add_write(handle, inode, from,\n\t\t\t\t\tlength);\n\t}\n\nunlock:\n\tunlock_page(page);\n\tput_page(page);\n\treturn err;\n}\n\n/*\n * ext4_block_zero_page_range() zeros out a mapping of length 'length'\n * starting from file offset 'from'.  The range to be zero'd must\n * be contained with in one block.  If the specified range exceeds\n * the end of the block it will be shortened to end of the block\n * that cooresponds to 'from'\n */\nstatic int ext4_block_zero_page_range(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from, loff_t length)\n{\n\tstruct inode *inode = mapping->host;\n\tunsigned offset = from & (PAGE_SIZE-1);\n\tunsigned blocksize = inode->i_sb->s_blocksize;\n\tunsigned max = blocksize - (offset & (blocksize - 1));\n\n\t/*\n\t * correct length if it does not fall between\n\t * 'from' and the end of the block\n\t */\n\tif (length > max || length < 0)\n\t\tlength = max;\n\n\tif (IS_DAX(inode)) {\n\t\treturn iomap_zero_range(inode, from, length, NULL,\n\t\t\t\t\t&ext4_iomap_ops);\n\t}\n\treturn __ext4_block_zero_page_range(handle, mapping, from, length);\n}\n\n/*\n * ext4_block_truncate_page() zeroes out a mapping from file offset `from'\n * up to the end of the block which corresponds to `from'.\n * This required during truncate. We need to physically zero the tail end\n * of that block so it doesn't yield old data if the file is later grown.\n */\nstatic int ext4_block_truncate_page(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from)\n{\n\tunsigned offset = from & (PAGE_SIZE-1);\n\tunsigned length;\n\tunsigned blocksize;\n\tstruct inode *inode = mapping->host;\n\n\t/* If we are processing an encrypted inode during orphan list handling */\n\tif (IS_ENCRYPTED(inode) && !fscrypt_has_encryption_key(inode))\n\t\treturn 0;\n\n\tblocksize = inode->i_sb->s_blocksize;\n\tlength = blocksize - (offset & (blocksize - 1));\n\n\treturn ext4_block_zero_page_range(handle, mapping, from, length);\n}\n\nint ext4_zero_partial_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     loff_t lstart, loff_t length)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tstruct address_space *mapping = inode->i_mapping;\n\tunsigned partial_start, partial_end;\n\text4_fsblk_t start, end;\n\tloff_t byte_end = (lstart + length - 1);\n\tint err = 0;\n\n\tpartial_start = lstart & (sb->s_blocksize - 1);\n\tpartial_end = byte_end & (sb->s_blocksize - 1);\n\n\tstart = lstart >> sb->s_blocksize_bits;\n\tend = byte_end >> sb->s_blocksize_bits;\n\n\t/* Handle partial zero within the single block */\n\tif (start == end &&\n\t    (partial_start || (partial_end != sb->s_blocksize - 1))) {\n\t\terr = ext4_block_zero_page_range(handle, mapping,\n\t\t\t\t\t\t lstart, length);\n\t\treturn err;\n\t}\n\t/* Handle partial zero out on the start of the range */\n\tif (partial_start) {\n\t\terr = ext4_block_zero_page_range(handle, mapping,\n\t\t\t\t\t\t lstart, sb->s_blocksize);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\t/* Handle partial zero out on the end of the range */\n\tif (partial_end != sb->s_blocksize - 1)\n\t\terr = ext4_block_zero_page_range(handle, mapping,\n\t\t\t\t\t\t byte_end - partial_end,\n\t\t\t\t\t\t partial_end + 1);\n\treturn err;\n}\n\nint ext4_can_truncate(struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode))\n\t\treturn 1;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn 1;\n\tif (S_ISLNK(inode->i_mode))\n\t\treturn !ext4_inode_is_fast_symlink(inode);\n\treturn 0;\n}\n\n/*\n * We have to make sure i_disksize gets properly updated before we truncate\n * page cache due to hole punching or zero range. Otherwise i_disksize update\n * can get lost as it may have been postponed to submission of writeback but\n * that will never happen after we truncate page cache.\n */\nint ext4_update_disksize_before_punch(struct inode *inode, loff_t offset,\n\t\t\t\t      loff_t len)\n{\n\thandle_t *handle;\n\tloff_t size = i_size_read(inode);\n\n\tWARN_ON(!inode_is_locked(inode));\n\tif (offset > size || offset + len < size)\n\t\treturn 0;\n\n\tif (EXT4_I(inode)->i_disksize >= size)\n\t\treturn 0;\n\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\text4_update_i_disksize(inode, size);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n\n\treturn 0;\n}\n\nstatic void ext4_wait_dax_page(struct ext4_inode_info *ei)\n{\n\tup_write(&ei->i_mmap_sem);\n\tschedule();\n\tdown_write(&ei->i_mmap_sem);\n}\n\nint ext4_break_layouts(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct page *page;\n\tint error;\n\n\tif (WARN_ON_ONCE(!rwsem_is_locked(&ei->i_mmap_sem)))\n\t\treturn -EINVAL;\n\n\tdo {\n\t\tpage = dax_layout_busy_page(inode->i_mapping);\n\t\tif (!page)\n\t\t\treturn 0;\n\n\t\terror = ___wait_var_event(&page->_refcount,\n\t\t\t\tatomic_read(&page->_refcount) == 1,\n\t\t\t\tTASK_INTERRUPTIBLE, 0, 0,\n\t\t\t\text4_wait_dax_page(ei));\n\t} while (error == 0);\n\n\treturn error;\n}\n\n/*\n * ext4_punch_hole: punches a hole in a file by releasing the blocks\n * associated with the given offset and length\n *\n * @inode:  File inode\n * @offset: The offset where the hole will begin\n * @len:    The length of the hole\n *\n * Returns: 0 on success or negative on failure\n */\n\nint ext4_punch_hole(struct inode *inode, loff_t offset, loff_t length)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t first_block, stop_block;\n\tstruct address_space *mapping = inode->i_mapping;\n\tloff_t first_block_offset, last_block_offset;\n\thandle_t *handle;\n\tunsigned int credits;\n\tint ret = 0;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_punch_hole(inode, offset, length, 0);\n\n\text4_clear_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\n\tif (ext4_has_inline_data(inode)) {\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\t\tret = ext4_convert_inline_data(inode);\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\tret = filemap_write_and_wait_range(mapping, offset,\n\t\t\t\t\t\t   offset + length - 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tinode_lock(inode);\n\n\t/* No need to punch hole beyond i_size */\n\tif (offset >= inode->i_size)\n\t\tgoto out_mutex;\n\n\t/*\n\t * If the hole extends beyond i_size, set the hole\n\t * to end after the page that contains i_size\n\t */\n\tif (offset + length > inode->i_size) {\n\t\tlength = inode->i_size +\n\t\t   PAGE_SIZE - (inode->i_size & (PAGE_SIZE - 1)) -\n\t\t   offset;\n\t}\n\n\tif (offset & (sb->s_blocksize - 1) ||\n\t    (offset + length) & (sb->s_blocksize - 1)) {\n\t\t/*\n\t\t * Attach jinode to inode for jbd2 if we do any zeroing of\n\t\t * partial block\n\t\t */\n\t\tret = ext4_inode_attach_jinode(inode);\n\t\tif (ret < 0)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\tret = ext4_break_layouts(inode);\n\tif (ret)\n\t\tgoto out_dio;\n\n\tfirst_block_offset = round_up(offset, sb->s_blocksize);\n\tlast_block_offset = round_down((offset + length), sb->s_blocksize) - 1;\n\n\t/* Now release the pages and zero block aligned part of pages*/\n\tif (last_block_offset > first_block_offset) {\n\t\tret = ext4_update_disksize_before_punch(inode, offset, length);\n\t\tif (ret)\n\t\t\tgoto out_dio;\n\t\ttruncate_pagecache_range(inode, first_block_offset,\n\t\t\t\t\t last_block_offset);\n\t}\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\telse\n\t\tcredits = ext4_blocks_for_truncate(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(sb, ret);\n\t\tgoto out_dio;\n\t}\n\n\tret = ext4_zero_partial_blocks(handle, inode, offset,\n\t\t\t\t       length);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tfirst_block = (offset + sb->s_blocksize - 1) >>\n\t\tEXT4_BLOCK_SIZE_BITS(sb);\n\tstop_block = (offset + length) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* If there are blocks to remove, do it */\n\tif (stop_block > first_block) {\n\n\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\text4_discard_preallocations(inode);\n\n\t\tret = ext4_es_remove_extent(inode, first_block,\n\t\t\t\t\t    stop_block - first_block);\n\t\tif (ret) {\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto out_stop;\n\t\t}\n\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\t\tret = ext4_ext_remove_space(inode, first_block,\n\t\t\t\t\t\t    stop_block - 1);\n\t\telse\n\t\t\tret = ext4_ind_remove_space(handle, inode, first_block,\n\t\t\t\t\t\t    stop_block);\n\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t}\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\nout_mutex:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\nint ext4_inode_attach_jinode(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct jbd2_inode *jinode;\n\n\tif (ei->jinode || !EXT4_SB(inode->i_sb)->s_journal)\n\t\treturn 0;\n\n\tjinode = jbd2_alloc_inode(GFP_KERNEL);\n\tspin_lock(&inode->i_lock);\n\tif (!ei->jinode) {\n\t\tif (!jinode) {\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tei->jinode = jinode;\n\t\tjbd2_journal_init_jbd_inode(ei->jinode, inode);\n\t\tjinode = NULL;\n\t}\n\tspin_unlock(&inode->i_lock);\n\tif (unlikely(jinode != NULL))\n\t\tjbd2_free_inode(jinode);\n\treturn 0;\n}\n\n/*\n * ext4_truncate()\n *\n * We block out ext4_get_block() block instantiations across the entire\n * transaction, and VFS/VM ensures that ext4_truncate() cannot run\n * simultaneously on behalf of the same inode.\n *\n * As we work through the truncate and commit bits of it to the journal there\n * is one core, guiding principle: the file's tree must always be consistent on\n * disk.  We must be able to restart the truncate after a crash.\n *\n * The file's tree may be transiently inconsistent in memory (although it\n * probably isn't), but whenever we close off and commit a journal transaction,\n * the contents of (the filesystem + the journal) must be consistent and\n * restartable.  It's pretty simple, really: bottom up, right to left (although\n * left-to-right works OK too).\n *\n * Note that at recovery time, journal replay occurs *before* the restart of\n * truncate against the orphan inode list.\n *\n * The committed inode has the new, desired i_size (which is the same as\n * i_disksize in this case).  After a crash, ext4_orphan_cleanup() will see\n * that this inode's truncate did not complete and it will again call\n * ext4_truncate() to have another go.  So there will be instantiated blocks\n * to the right of the truncation point in a crashed ext4 filesystem.  But\n * that's fine - as long as they are linked from the inode, the post-crash\n * ext4_truncate() run will find them and release them.\n */\nint ext4_truncate(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tunsigned int credits;\n\tint err = 0;\n\thandle_t *handle;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\t/*\n\t * There is a possibility that we're either freeing the inode\n\t * or it's a completely new inode. In those cases we might not\n\t * have i_mutex locked because it's not necessary.\n\t */\n\tif (!(inode->i_state & (I_NEW|I_FREEING)))\n\t\tWARN_ON(!inode_is_locked(inode));\n\ttrace_ext4_truncate_enter(inode);\n\n\tif (!ext4_can_truncate(inode))\n\t\treturn 0;\n\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\n\tif (inode->i_size == 0 && !test_opt(inode->i_sb, NO_AUTO_DA_ALLOC))\n\t\text4_set_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE);\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tint has_inline = 1;\n\n\t\terr = ext4_inline_data_truncate(inode, &has_inline);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (has_inline)\n\t\t\treturn 0;\n\t}\n\n\t/* If we zero-out tail of the page, we have to create jinode for jbd2 */\n\tif (inode->i_size & (inode->i_sb->s_blocksize - 1)) {\n\t\tif (ext4_inode_attach_jinode(inode) < 0)\n\t\t\treturn 0;\n\t}\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\telse\n\t\tcredits = ext4_blocks_for_truncate(inode);\n\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\tif (inode->i_size & (inode->i_sb->s_blocksize - 1))\n\t\text4_block_truncate_page(handle, mapping, inode->i_size);\n\n\t/*\n\t * We add the inode to the orphan list, so that if this\n\t * truncate spans multiple transactions, and we crash, we will\n\t * resume the truncate when the filesystem recovers.  It also\n\t * marks the inode dirty, to catch the new size.\n\t *\n\t * Implication: the file must always be in a sane, consistent\n\t * truncatable state while each transaction commits.\n\t */\n\terr = ext4_orphan_add(handle, inode);\n\tif (err)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\n\text4_discard_preallocations(inode);\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\terr = ext4_ext_truncate(handle, inode);\n\telse\n\t\text4_ind_truncate(handle, inode);\n\n\tup_write(&ei->i_data_sem);\n\tif (err)\n\t\tgoto out_stop;\n\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\t/*\n\t * If this was a simple ftruncate() and the file will remain alive,\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_evict_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n\n\ttrace_ext4_truncate_exit(inode);\n\treturn err;\n}\n\n/*\n * ext4_get_inode_loc returns with an extra refcount against the inode's\n * underlying buffer_head on success. If 'in_mem' is true, we have all\n * data in memory that is needed to recreate the on-disk version of this\n * inode.\n */\nstatic int __ext4_get_inode_loc(struct inode *inode,\n\t\t\t\tstruct ext4_iloc *iloc, int in_mem)\n{\n\tstruct ext4_group_desc\t*gdp;\n\tstruct buffer_head\t*bh;\n\tstruct super_block\t*sb = inode->i_sb;\n\text4_fsblk_t\t\tblock;\n\tstruct blk_plug\t\tplug;\n\tint\t\t\tinodes_per_block, inode_offset;\n\n\tiloc->bh = NULL;\n\tif (inode->i_ino < EXT4_ROOT_INO ||\n\t    inode->i_ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count))\n\t\treturn -EFSCORRUPTED;\n\n\tiloc->block_group = (inode->i_ino - 1) / EXT4_INODES_PER_GROUP(sb);\n\tgdp = ext4_get_group_desc(sb, iloc->block_group, NULL);\n\tif (!gdp)\n\t\treturn -EIO;\n\n\t/*\n\t * Figure out the offset within the block group inode table\n\t */\n\tinodes_per_block = EXT4_SB(sb)->s_inodes_per_block;\n\tinode_offset = ((inode->i_ino - 1) %\n\t\t\tEXT4_INODES_PER_GROUP(sb));\n\tblock = ext4_inode_table(sb, gdp) + (inode_offset / inodes_per_block);\n\tiloc->offset = (inode_offset % inodes_per_block) * EXT4_INODE_SIZE(sb);\n\n\tbh = sb_getblk(sb, block);\n\tif (unlikely(!bh))\n\t\treturn -ENOMEM;\n\tif (!buffer_uptodate(bh)) {\n\t\tlock_buffer(bh);\n\n\t\t/*\n\t\t * If the buffer has the write error flag, we have failed\n\t\t * to write out another inode in the same block.  In this\n\t\t * case, we don't have to read the block because we may\n\t\t * read the old inode data successfully.\n\t\t */\n\t\tif (buffer_write_io_error(bh) && !buffer_uptodate(bh))\n\t\t\tset_buffer_uptodate(bh);\n\n\t\tif (buffer_uptodate(bh)) {\n\t\t\t/* someone brought it uptodate while we waited */\n\t\t\tunlock_buffer(bh);\n\t\t\tgoto has_buffer;\n\t\t}\n\n\t\t/*\n\t\t * If we have all information of the inode in memory and this\n\t\t * is the only valid inode in the block, we need not read the\n\t\t * block.\n\t\t */\n\t\tif (in_mem) {\n\t\t\tstruct buffer_head *bitmap_bh;\n\t\t\tint i, start;\n\n\t\t\tstart = inode_offset & ~(inodes_per_block - 1);\n\n\t\t\t/* Is the inode bitmap in cache? */\n\t\t\tbitmap_bh = sb_getblk(sb, ext4_inode_bitmap(sb, gdp));\n\t\t\tif (unlikely(!bitmap_bh))\n\t\t\t\tgoto make_io;\n\n\t\t\t/*\n\t\t\t * If the inode bitmap isn't in cache then the\n\t\t\t * optimisation may end up performing two reads instead\n\t\t\t * of one, so skip it.\n\t\t\t */\n\t\t\tif (!buffer_uptodate(bitmap_bh)) {\n\t\t\t\tbrelse(bitmap_bh);\n\t\t\t\tgoto make_io;\n\t\t\t}\n\t\t\tfor (i = start; i < start + inodes_per_block; i++) {\n\t\t\t\tif (i == inode_offset)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ext4_test_bit(i, bitmap_bh->b_data))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbrelse(bitmap_bh);\n\t\t\tif (i == start + inodes_per_block) {\n\t\t\t\t/* all other inodes are free, so skip I/O */\n\t\t\t\tmemset(bh->b_data, 0, bh->b_size);\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\tunlock_buffer(bh);\n\t\t\t\tgoto has_buffer;\n\t\t\t}\n\t\t}\n\nmake_io:\n\t\t/*\n\t\t * If we need to do any I/O, try to pre-readahead extra\n\t\t * blocks from the inode table.\n\t\t */\n\t\tblk_start_plug(&plug);\n\t\tif (EXT4_SB(sb)->s_inode_readahead_blks) {\n\t\t\text4_fsblk_t b, end, table;\n\t\t\tunsigned num;\n\t\t\t__u32 ra_blks = EXT4_SB(sb)->s_inode_readahead_blks;\n\n\t\t\ttable = ext4_inode_table(sb, gdp);\n\t\t\t/* s_inode_readahead_blks is always a power of 2 */\n\t\t\tb = block & ~((ext4_fsblk_t) ra_blks - 1);\n\t\t\tif (table > b)\n\t\t\t\tb = table;\n\t\t\tend = b + ra_blks;\n\t\t\tnum = EXT4_INODES_PER_GROUP(sb);\n\t\t\tif (ext4_has_group_desc_csum(sb))\n\t\t\t\tnum -= ext4_itable_unused_count(sb, gdp);\n\t\t\ttable += num / inodes_per_block;\n\t\t\tif (end > table)\n\t\t\t\tend = table;\n\t\t\twhile (b <= end)\n\t\t\t\tsb_breadahead(sb, b++);\n\t\t}\n\n\t\t/*\n\t\t * There are other valid inodes in the buffer, this inode\n\t\t * has in-inode xattrs, or we don't have this inode in memory.\n\t\t * Read the block from disk.\n\t\t */\n\t\ttrace_ext4_load_inode(inode);\n\t\tget_bh(bh);\n\t\tbh->b_end_io = end_buffer_read_sync;\n\t\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\t\tblk_finish_plug(&plug);\n\t\twait_on_buffer(bh);\n\t\tif (!buffer_uptodate(bh)) {\n\t\t\tEXT4_ERROR_INODE_BLOCK(inode, block,\n\t\t\t\t\t       \"unable to read itable block\");\n\t\t\tbrelse(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t}\nhas_buffer:\n\tiloc->bh = bh;\n\treturn 0;\n}\n\nint ext4_get_inode_loc(struct inode *inode, struct ext4_iloc *iloc)\n{\n\t/* We have all inode data except xattrs in memory here. */\n\treturn __ext4_get_inode_loc(inode, iloc,\n\t\t!ext4_test_inode_state(inode, EXT4_STATE_XATTR));\n}\n\nstatic bool ext4_should_use_dax(struct inode *inode)\n{\n\tif (!test_opt(inode->i_sb, DAX))\n\t\treturn false;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn false;\n\tif (ext4_should_journal_data(inode))\n\t\treturn false;\n\tif (ext4_has_inline_data(inode))\n\t\treturn false;\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_ENCRYPT))\n\t\treturn false;\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_VERITY))\n\t\treturn false;\n\treturn true;\n}\n\nvoid ext4_set_inode_flags(struct inode *inode)\n{\n\tunsigned int flags = EXT4_I(inode)->i_flags;\n\tunsigned int new_fl = 0;\n\n\tif (flags & EXT4_SYNC_FL)\n\t\tnew_fl |= S_SYNC;\n\tif (flags & EXT4_APPEND_FL)\n\t\tnew_fl |= S_APPEND;\n\tif (flags & EXT4_IMMUTABLE_FL)\n\t\tnew_fl |= S_IMMUTABLE;\n\tif (flags & EXT4_NOATIME_FL)\n\t\tnew_fl |= S_NOATIME;\n\tif (flags & EXT4_DIRSYNC_FL)\n\t\tnew_fl |= S_DIRSYNC;\n\tif (ext4_should_use_dax(inode))\n\t\tnew_fl |= S_DAX;\n\tif (flags & EXT4_ENCRYPT_FL)\n\t\tnew_fl |= S_ENCRYPTED;\n\tif (flags & EXT4_CASEFOLD_FL)\n\t\tnew_fl |= S_CASEFOLD;\n\tif (flags & EXT4_VERITY_FL)\n\t\tnew_fl |= S_VERITY;\n\tinode_set_flags(inode, new_fl,\n\t\t\tS_SYNC|S_APPEND|S_IMMUTABLE|S_NOATIME|S_DIRSYNC|S_DAX|\n\t\t\tS_ENCRYPTED|S_CASEFOLD|S_VERITY);\n}\n\nstatic blkcnt_t ext4_inode_blocks(struct ext4_inode *raw_inode,\n\t\t\t\t  struct ext4_inode_info *ei)\n{\n\tblkcnt_t i_blocks ;\n\tstruct inode *inode = &(ei->vfs_inode);\n\tstruct super_block *sb = inode->i_sb;\n\n\tif (ext4_has_feature_huge_file(sb)) {\n\t\t/* we are using combined 48 bit field */\n\t\ti_blocks = ((u64)le16_to_cpu(raw_inode->i_blocks_high)) << 32 |\n\t\t\t\t\tle32_to_cpu(raw_inode->i_blocks_lo);\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_HUGE_FILE)) {\n\t\t\t/* i_blocks represent file system block size */\n\t\t\treturn i_blocks  << (inode->i_blkbits - 9);\n\t\t} else {\n\t\t\treturn i_blocks;\n\t\t}\n\t} else {\n\t\treturn le32_to_cpu(raw_inode->i_blocks_lo);\n\t}\n}\n\nstatic inline int ext4_iget_extra_inode(struct inode *inode,\n\t\t\t\t\t struct ext4_inode *raw_inode,\n\t\t\t\t\t struct ext4_inode_info *ei)\n{\n\t__le32 *magic = (void *)raw_inode +\n\t\t\tEXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize;\n\n\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize + sizeof(__le32) <=\n\t    EXT4_INODE_SIZE(inode->i_sb) &&\n\t    *magic == cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t\treturn ext4_find_inline_data_nolock(inode);\n\t} else\n\t\tEXT4_I(inode)->i_inline_off = 0;\n\treturn 0;\n}\n\nint ext4_get_projid(struct inode *inode, kprojid_t *projid)\n{\n\tif (!ext4_has_feature_project(inode->i_sb))\n\t\treturn -EOPNOTSUPP;\n\t*projid = EXT4_I(inode)->i_projid;\n\treturn 0;\n}\n\n/*\n * ext4 has self-managed i_version for ea inodes, it stores the lower 32bit of\n * refcount in i_version, so use raw values if inode has EXT4_EA_INODE_FL flag\n * set.\n */\nstatic inline void ext4_inode_set_iversion_queried(struct inode *inode, u64 val)\n{\n\tif (unlikely(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL))\n\t\tinode_set_iversion_raw(inode, val);\n\telse\n\t\tinode_set_iversion_queried(inode, val);\n}\nstatic inline u64 ext4_inode_peek_iversion(const struct inode *inode)\n{\n\tif (unlikely(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL))\n\t\treturn inode_peek_iversion_raw(inode);\n\telse\n\t\treturn inode_peek_iversion(inode);\n}\n\nstruct inode *__ext4_iget(struct super_block *sb, unsigned long ino,\n\t\t\t  ext4_iget_flags flags, const char *function,\n\t\t\t  unsigned int line)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tloff_t size;\n\tint block;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tif ((!(flags & EXT4_IGET_SPECIAL) &&\n\t     (ino < EXT4_FIRST_INO(sb) && ino != EXT4_ROOT_INO)) ||\n\t    (ino < EXT4_ROOT_INO) ||\n\t    (ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count))) {\n\t\tif (flags & EXT4_IGET_HANDLE)\n\t\t\treturn ERR_PTR(-ESTALE);\n\t\t__ext4_error(sb, function, line,\n\t\t\t     \"inode #%lu: comm %s: iget: illegal inode #\",\n\t\t\t     ino, current->comm);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = NULL;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\n\tif ((ino == EXT4_ROOT_INO) && (raw_inode->i_links_count == 0)) {\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: root inode unallocated\");\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\n\tif ((flags & EXT4_IGET_HANDLE) &&\n\t    (raw_inode->i_links_count == 0) && (raw_inode->i_mode == 0)) {\n\t\tret = -ESTALE;\n\t\tgoto bad_inode;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t\tEXT4_INODE_SIZE(inode->i_sb) ||\n\t\t    (ei->i_extra_isize & 3)) {\n\t\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t\t \"iget: bad extra_isize %u \"\n\t\t\t\t\t \"(inode size %u)\",\n\t\t\t\t\t ei->i_extra_isize,\n\t\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = raw_inode->i_generation;\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\tif (!ext4_inode_csum_verify(inode, raw_inode, ei)) {\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: checksum invalid\");\n\t\tret = -EFSBADCRC;\n\t\tgoto bad_inode;\n\t}\n\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\ti_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\ti_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (ext4_has_feature_project(sb) &&\n\t    EXT4_INODE_SIZE(sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(raw_inode->i_projid);\n\telse\n\t\ti_projid = EXT4_DEF_PROJID;\n\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\ti_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\ti_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\ti_uid_write(inode, i_uid);\n\ti_gid_write(inode, i_gid);\n\tei->i_projid = make_kprojid(&init_user_ns, i_projid);\n\tset_nlink(inode, le16_to_cpu(raw_inode->i_links_count));\n\n\text4_clear_state_flags(ei);\t/* Only relevant on 32-bit archs */\n\tei->i_inline_off = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif ((inode->i_mode == 0 ||\n\t\t     !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) &&\n\t\t    ino != EXT4_BOOT_LOADER_INO) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those.\n\t\t * OR it is the EXT4_BOOT_LOADER_INO which is\n\t\t * not initialized on a new filesystem. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\text4_set_inode_flags(inode);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (ext4_has_feature_64bit(sb))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(sb, raw_inode);\n\tif ((size = i_size_read(inode)) < 0) {\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: bad i_size value: %lld\", size);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tBUILD_BUG_ON(sizeof(struct ext4_inode) & 3);\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\tret = ext4_iget_extra_inode(inode, raw_inode, ei);\n\t\t\tif (ret)\n\t\t\t\tgoto bad_inode;\n\t\t}\n\t}\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = le32_to_cpu(raw_inode->i_disk_version);\n\n\t\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\tivers |=\n\t\t    (__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t\t}\n\t\text4_inode_set_iversion_queried(inode, ivers);\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: bad extended attribute block %llu\",\n\t\t\t\t ei->i_file_acl);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\t/* validate the block references in the inode */\n\t\tif (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\t\t\tret = ext4_ext_check_inode(inode);\n\t\t\telse\n\t\t\t\tret = ext4_ind_check_inode(inode);\n\t\t}\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\t/* VFS does not allow setting these so must be corruption */\n\t\tif (IS_APPEND(inode) || IS_IMMUTABLE(inode)) {\n\t\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t\t \"iget: immutable or append flags \"\n\t\t\t\t\t \"not allowed on symlinks\");\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\tif (IS_ENCRYPTED(inode)) {\n\t\t\tinode->i_op = &ext4_encrypted_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t} else if (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_link = (char *)ei->i_data;\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else if (ino == EXT4_BOOT_LOADER_INO) {\n\t\tmake_bad_inode(inode);\n\t} else {\n\t\tret = -EFSCORRUPTED;\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: bogus i_mode (%o)\", inode->i_mode);\n\t\tgoto bad_inode;\n\t}\n\tif (IS_CASEFOLDED(inode) && !ext4_has_feature_casefold(inode->i_sb))\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"casefold flag without casefold feature\");\n\tbrelse(iloc.bh);\n\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}\n\nstatic int ext4_inode_blocks_set(handle_t *handle,\n\t\t\t\tstruct ext4_inode *raw_inode,\n\t\t\t\tstruct ext4_inode_info *ei)\n{\n\tstruct inode *inode = &(ei->vfs_inode);\n\tu64 i_blocks = inode->i_blocks;\n\tstruct super_block *sb = inode->i_sb;\n\n\tif (i_blocks <= ~0U) {\n\t\t/*\n\t\t * i_blocks can be represented in a 32 bit variable\n\t\t * as multiple of 512 bytes\n\t\t */\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = 0;\n\t\text4_clear_inode_flag(inode, EXT4_INODE_HUGE_FILE);\n\t\treturn 0;\n\t}\n\tif (!ext4_has_feature_huge_file(sb))\n\t\treturn -EFBIG;\n\n\tif (i_blocks <= 0xffffffffffffULL) {\n\t\t/*\n\t\t * i_blocks can be represented in a 48 bit variable\n\t\t * as multiple of 512 bytes\n\t\t */\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);\n\t\text4_clear_inode_flag(inode, EXT4_INODE_HUGE_FILE);\n\t} else {\n\t\text4_set_inode_flag(inode, EXT4_INODE_HUGE_FILE);\n\t\t/* i_block is stored in file system block size */\n\t\ti_blocks = i_blocks >> (inode->i_blkbits - 9);\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);\n\t}\n\treturn 0;\n}\n\nstruct other_inode {\n\tunsigned long\t\torig_ino;\n\tstruct ext4_inode\t*raw_inode;\n};\n\nstatic int other_inode_match(struct inode * inode, unsigned long ino,\n\t\t\t     void *data)\n{\n\tstruct other_inode *oi = (struct other_inode *) data;\n\n\tif ((inode->i_ino != ino) ||\n\t    (inode->i_state & (I_FREEING | I_WILL_FREE | I_NEW |\n\t\t\t       I_DIRTY_INODE)) ||\n\t    ((inode->i_state & I_DIRTY_TIME) == 0))\n\t\treturn 0;\n\tspin_lock(&inode->i_lock);\n\tif (((inode->i_state & (I_FREEING | I_WILL_FREE | I_NEW |\n\t\t\t\tI_DIRTY_INODE)) == 0) &&\n\t    (inode->i_state & I_DIRTY_TIME)) {\n\t\tstruct ext4_inode_info\t*ei = EXT4_I(inode);\n\n\t\tinode->i_state &= ~(I_DIRTY_TIME | I_DIRTY_TIME_EXPIRED);\n\t\tspin_unlock(&inode->i_lock);\n\n\t\tspin_lock(&ei->i_raw_lock);\n\t\tEXT4_INODE_SET_XTIME(i_ctime, inode, oi->raw_inode);\n\t\tEXT4_INODE_SET_XTIME(i_mtime, inode, oi->raw_inode);\n\t\tEXT4_INODE_SET_XTIME(i_atime, inode, oi->raw_inode);\n\t\text4_inode_csum_set(inode, oi->raw_inode, ei);\n\t\tspin_unlock(&ei->i_raw_lock);\n\t\ttrace_ext4_other_inode_update_time(inode, oi->orig_ino);\n\t\treturn -1;\n\t}\n\tspin_unlock(&inode->i_lock);\n\treturn -1;\n}\n\n/*\n * Opportunistically update the other time fields for other inodes in\n * the same inode table block.\n */\nstatic void ext4_update_other_inodes_time(struct super_block *sb,\n\t\t\t\t\t  unsigned long orig_ino, char *buf)\n{\n\tstruct other_inode oi;\n\tunsigned long ino;\n\tint i, inodes_per_block = EXT4_SB(sb)->s_inodes_per_block;\n\tint inode_size = EXT4_INODE_SIZE(sb);\n\n\toi.orig_ino = orig_ino;\n\t/*\n\t * Calculate the first inode in the inode table block.  Inode\n\t * numbers are one-based.  That is, the first inode in a block\n\t * (assuming 4k blocks and 256 byte inodes) is (n*16 + 1).\n\t */\n\tino = ((orig_ino - 1) & ~(inodes_per_block - 1)) + 1;\n\tfor (i = 0; i < inodes_per_block; i++, ino++, buf += inode_size) {\n\t\tif (ino == orig_ino)\n\t\t\tcontinue;\n\t\toi.raw_inode = (struct ext4_inode *) buf;\n\t\t(void) find_inode_nowait(sb, ino, other_inode_match, &oi);\n\t}\n}\n\n/*\n * Post the struct inode info into an on-disk inode location in the\n * buffer-cache.  This gobbles the caller's reference to the\n * buffer_head in the inode location struct.\n *\n * The caller must have write access to iloc->bh.\n */\nstatic int ext4_do_update_inode(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_iloc *iloc)\n{\n\tstruct ext4_inode *raw_inode = ext4_raw_inode(iloc);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct buffer_head *bh = iloc->bh;\n\tstruct super_block *sb = inode->i_sb;\n\tint err = 0, rc, block;\n\tint need_datasync = 0, set_large_file = 0;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tspin_lock(&ei->i_raw_lock);\n\n\t/* For fields not tracked in the in-memory inode,\n\t * initialise them to zero for new inodes. */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NEW))\n\t\tmemset(raw_inode, 0, EXT4_SB(inode->i_sb)->s_inode_size);\n\n\traw_inode->i_mode = cpu_to_le16(inode->i_mode);\n\ti_uid = i_uid_read(inode);\n\ti_gid = i_gid_read(inode);\n\ti_projid = from_kprojid(&init_user_ns, ei->i_projid);\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\traw_inode->i_uid_low = cpu_to_le16(low_16_bits(i_uid));\n\t\traw_inode->i_gid_low = cpu_to_le16(low_16_bits(i_gid));\n/*\n * Fix up interoperability with old kernels. Otherwise, old inodes get\n * re-used with the upper 16 bits of the uid/gid intact\n */\n\t\tif (ei->i_dtime && list_empty(&ei->i_orphan)) {\n\t\t\traw_inode->i_uid_high = 0;\n\t\t\traw_inode->i_gid_high = 0;\n\t\t} else {\n\t\t\traw_inode->i_uid_high =\n\t\t\t\tcpu_to_le16(high_16_bits(i_uid));\n\t\t\traw_inode->i_gid_high =\n\t\t\t\tcpu_to_le16(high_16_bits(i_gid));\n\t\t}\n\t} else {\n\t\traw_inode->i_uid_low = cpu_to_le16(fs_high2lowuid(i_uid));\n\t\traw_inode->i_gid_low = cpu_to_le16(fs_high2lowgid(i_gid));\n\t\traw_inode->i_uid_high = 0;\n\t\traw_inode->i_gid_high = 0;\n\t}\n\traw_inode->i_links_count = cpu_to_le16(inode->i_nlink);\n\n\tEXT4_INODE_SET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_SET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_SET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_SET_XTIME(i_crtime, ei, raw_inode);\n\n\terr = ext4_inode_blocks_set(handle, raw_inode, ei);\n\tif (err) {\n\t\tspin_unlock(&ei->i_raw_lock);\n\t\tgoto out_brelse;\n\t}\n\traw_inode->i_dtime = cpu_to_le32(ei->i_dtime);\n\traw_inode->i_flags = cpu_to_le32(ei->i_flags & 0xFFFFFFFF);\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT)))\n\t\traw_inode->i_file_acl_high =\n\t\t\tcpu_to_le16(ei->i_file_acl >> 32);\n\traw_inode->i_file_acl_lo = cpu_to_le32(ei->i_file_acl);\n\tif (ei->i_disksize != ext4_isize(inode->i_sb, raw_inode)) {\n\t\text4_isize_set(raw_inode, ei->i_disksize);\n\t\tneed_datasync = 1;\n\t}\n\tif (ei->i_disksize > 0x7fffffffULL) {\n\t\tif (!ext4_has_feature_large_file(sb) ||\n\t\t\t\tEXT4_SB(sb)->s_es->s_rev_level ==\n\t\t    cpu_to_le32(EXT4_GOOD_OLD_REV))\n\t\t\tset_large_file = 1;\n\t}\n\traw_inode->i_generation = cpu_to_le32(inode->i_generation);\n\tif (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) {\n\t\tif (old_valid_dev(inode->i_rdev)) {\n\t\t\traw_inode->i_block[0] =\n\t\t\t\tcpu_to_le32(old_encode_dev(inode->i_rdev));\n\t\t\traw_inode->i_block[1] = 0;\n\t\t} else {\n\t\t\traw_inode->i_block[0] = 0;\n\t\t\traw_inode->i_block[1] =\n\t\t\t\tcpu_to_le32(new_encode_dev(inode->i_rdev));\n\t\t\traw_inode->i_block[2] = 0;\n\t\t}\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\t\traw_inode->i_block[block] = ei->i_data[block];\n\t}\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = ext4_inode_peek_iversion(inode);\n\n\t\traw_inode->i_disk_version = cpu_to_le32(ivers);\n\t\tif (ei->i_extra_isize) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\traw_inode->i_version_hi =\n\t\t\t\t\tcpu_to_le32(ivers >> 32);\n\t\t\traw_inode->i_extra_isize =\n\t\t\t\tcpu_to_le16(ei->i_extra_isize);\n\t\t}\n\t}\n\n\tBUG_ON(!ext4_has_feature_project(inode->i_sb) &&\n\t       i_projid != EXT4_DEF_PROJID);\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\traw_inode->i_projid = cpu_to_le32(i_projid);\n\n\text4_inode_csum_set(inode, raw_inode, ei);\n\tspin_unlock(&ei->i_raw_lock);\n\tif (inode->i_sb->s_flags & SB_LAZYTIME)\n\t\text4_update_other_inodes_time(inode->i_sb, inode->i_ino,\n\t\t\t\t\t      bh->b_data);\n\n\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\trc = ext4_handle_dirty_metadata(handle, NULL, bh);\n\tif (!err)\n\t\terr = rc;\n\text4_clear_inode_state(inode, EXT4_STATE_NEW);\n\tif (set_large_file) {\n\t\tBUFFER_TRACE(EXT4_SB(sb)->s_sbh, \"get write access\");\n\t\terr = ext4_journal_get_write_access(handle, EXT4_SB(sb)->s_sbh);\n\t\tif (err)\n\t\t\tgoto out_brelse;\n\t\text4_set_feature_large_file(sb);\n\t\text4_handle_sync(handle);\n\t\terr = ext4_handle_dirty_super(handle, sb);\n\t}\n\text4_update_inode_fsync_trans(handle, inode, need_datasync);\nout_brelse:\n\tbrelse(bh);\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n\n/*\n * ext4_write_inode()\n *\n * We are called from a few places:\n *\n * - Within generic_file_aio_write() -> generic_write_sync() for O_SYNC files.\n *   Here, there will be no transaction running. We wait for any running\n *   transaction to commit.\n *\n * - Within flush work (sys_sync(), kupdate and such).\n *   We wait on commit, if told to.\n *\n * - Within iput_final() -> write_inode_now()\n *   We wait on commit, if told to.\n *\n * In all cases it is actually safe for us to return without doing anything,\n * because the inode has been copied into a raw inode buffer in\n * ext4_mark_inode_dirty().  This is a correctness thing for WB_SYNC_ALL\n * writeback.\n *\n * Note that we are absolutely dependent upon all inode dirtiers doing the\n * right thing: they *must* call mark_inode_dirty() after dirtying info in\n * which we are interested.\n *\n * It would be a bug for them to not do this.  The code:\n *\n *\tmark_inode_dirty(inode)\n *\tstuff();\n *\tinode->i_size = expr;\n *\n * is in error because write_inode() could occur while `stuff()' is running,\n * and the new i_size will be lost.  Plus the inode will no longer be on the\n * superblock's dirty inode list.\n */\nint ext4_write_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\tint err;\n\n\tif (WARN_ON_ONCE(current->flags & PF_MEMALLOC) ||\n\t    sb_rdonly(inode->i_sb))\n\t\treturn 0;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tif (EXT4_SB(inode->i_sb)->s_journal) {\n\t\tif (ext4_journal_current_handle()) {\n\t\t\tjbd_debug(1, \"called recursively, non-PF_MEMALLOC!\\n\");\n\t\t\tdump_stack();\n\t\t\treturn -EIO;\n\t\t}\n\n\t\t/*\n\t\t * No need to force transaction in WB_SYNC_NONE mode. Also\n\t\t * ext4_sync_fs() will force the commit after everything is\n\t\t * written.\n\t\t */\n\t\tif (wbc->sync_mode != WB_SYNC_ALL || wbc->for_sync)\n\t\t\treturn 0;\n\n\t\terr = jbd2_complete_transaction(EXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t\t\tEXT4_I(inode)->i_sync_tid);\n\t} else {\n\t\tstruct ext4_iloc iloc;\n\n\t\terr = __ext4_get_inode_loc(inode, &iloc, 0);\n\t\tif (err)\n\t\t\treturn err;\n\t\t/*\n\t\t * sync(2) will flush the whole buffer cache. No need to do\n\t\t * it here separately for each inode.\n\t\t */\n\t\tif (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync)\n\t\t\tsync_dirty_buffer(iloc.bh);\n\t\tif (buffer_req(iloc.bh) && !buffer_uptodate(iloc.bh)) {\n\t\t\tEXT4_ERROR_INODE_BLOCK(inode, iloc.bh->b_blocknr,\n\t\t\t\t\t \"IO error syncing inode\");\n\t\t\terr = -EIO;\n\t\t}\n\t\tbrelse(iloc.bh);\n\t}\n\treturn err;\n}\n\n/*\n * In data=journal mode ext4_journalled_invalidatepage() may fail to invalidate\n * buffers that are attached to a page stradding i_size and are undergoing\n * commit. In that case we have to wait for commit to finish and try again.\n */\nstatic void ext4_wait_for_tail_page_commit(struct inode *inode)\n{\n\tstruct page *page;\n\tunsigned offset;\n\tjournal_t *journal = EXT4_SB(inode->i_sb)->s_journal;\n\ttid_t commit_tid = 0;\n\tint ret;\n\n\toffset = inode->i_size & (PAGE_SIZE - 1);\n\t/*\n\t * All buffers in the last page remain valid? Then there's nothing to\n\t * do. We do the check mainly to optimize the common PAGE_SIZE ==\n\t * blocksize case\n\t */\n\tif (offset > PAGE_SIZE - i_blocksize(inode))\n\t\treturn;\n\twhile (1) {\n\t\tpage = find_lock_page(inode->i_mapping,\n\t\t\t\t      inode->i_size >> PAGE_SHIFT);\n\t\tif (!page)\n\t\t\treturn;\n\t\tret = __ext4_journalled_invalidatepage(page, offset,\n\t\t\t\t\t\tPAGE_SIZE - offset);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret != -EBUSY)\n\t\t\treturn;\n\t\tcommit_tid = 0;\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_committing_transaction)\n\t\t\tcommit_tid = journal->j_committing_transaction->t_tid;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tif (commit_tid)\n\t\t\tjbd2_log_wait_commit(journal, commit_tid);\n\t}\n}\n\n/*\n * ext4_setattr()\n *\n * Called from notify_change.\n *\n * We want to trap VFS attempts to truncate the file as soon as\n * possible.  In particular, we want to make sure that when the VFS\n * shrinks i_size, we put the inode on the orphan list and modify\n * i_disksize immediately, so that during the subsequent flushing of\n * dirty pages and freeing of disk blocks, we can guarantee that any\n * commit will leave the blocks being flushed in an unused state on\n * disk.  (On recovery, the inode will get truncated and the blocks will\n * be freed, so we have a strong guarantee that no future commit will\n * leave these blocks visible to the user.)\n *\n * Another thing we have to assure is that if we are in ordered mode\n * and inode is still attached to the committing transaction, we must\n * we start writeout of all the dirty pages which are being truncated.\n * This way we are sure that all the data written in the previous\n * transaction are already on disk (truncate waits for pages under\n * writeback).\n *\n * Called with inode->i_mutex down.\n */\nint ext4_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tint error, rc = 0;\n\tint orphan = 0;\n\tconst unsigned int ia_valid = attr->ia_valid;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tif (unlikely(IS_IMMUTABLE(inode)))\n\t\treturn -EPERM;\n\n\tif (unlikely(IS_APPEND(inode) &&\n\t\t     (ia_valid & (ATTR_MODE | ATTR_UID |\n\t\t\t\t  ATTR_GID | ATTR_TIMES_SET))))\n\t\treturn -EPERM;\n\n\terror = setattr_prepare(dentry, attr);\n\tif (error)\n\t\treturn error;\n\n\terror = fscrypt_prepare_setattr(dentry, attr);\n\tif (error)\n\t\treturn error;\n\n\terror = fsverity_prepare_setattr(dentry, attr);\n\tif (error)\n\t\treturn error;\n\n\tif (is_quota_modification(inode, attr)) {\n\t\terror = dquot_initialize(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif ((ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) ||\n\t    (ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid))) {\n\t\thandle_t *handle;\n\n\t\t/* (user+group)*(old+new) structure, inode write (sb,\n\t\t * inode block, ? - but truncate inode update has it) */\n\t\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA,\n\t\t\t(EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb) +\n\t\t\t EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb)) + 3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\n\t\t/* dquot_transfer() calls back ext4_get_inode_usage() which\n\t\t * counts xattr inode references.\n\t\t */\n\t\tdown_read(&EXT4_I(inode)->xattr_sem);\n\t\terror = dquot_transfer(inode, attr);\n\t\tup_read(&EXT4_I(inode)->xattr_sem);\n\n\t\tif (error) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn error;\n\t\t}\n\t\t/* Update corresponding info in inode so that everything is in\n\t\t * one transaction */\n\t\tif (attr->ia_valid & ATTR_UID)\n\t\t\tinode->i_uid = attr->ia_uid;\n\t\tif (attr->ia_valid & ATTR_GID)\n\t\t\tinode->i_gid = attr->ia_gid;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\thandle_t *handle;\n\t\tloff_t oldsize = inode->i_size;\n\t\tint shrink = (attr->ia_size < inode->i_size);\n\n\t\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\t\tif (attr->ia_size > sbi->s_bitmap_maxbytes)\n\t\t\t\treturn -EFBIG;\n\t\t}\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\treturn -EINVAL;\n\n\t\tif (IS_I_VERSION(inode) && attr->ia_size != inode->i_size)\n\t\t\tinode_inc_iversion(inode);\n\n\t\tif (shrink) {\n\t\t\tif (ext4_should_order_data(inode)) {\n\t\t\t\terror = ext4_begin_ordered_truncate(inode,\n\t\t\t\t\t\t\t    attr->ia_size);\n\t\t\t\tif (error)\n\t\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Blocks are going to be removed from the inode. Wait\n\t\t\t * for dio in flight.\n\t\t\t */\n\t\t\tinode_dio_wait(inode);\n\t\t}\n\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\t\trc = ext4_break_layouts(inode);\n\t\tif (rc) {\n\t\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t\treturn rc;\n\t\t}\n\n\t\tif (attr->ia_size != inode->i_size) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 3);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terror = PTR_ERR(handle);\n\t\t\t\tgoto out_mmap_sem;\n\t\t\t}\n\t\t\tif (ext4_handle_valid(handle) && shrink) {\n\t\t\t\terror = ext4_orphan_add(handle, inode);\n\t\t\t\torphan = 1;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Update c/mtime on truncate up, ext4_truncate() will\n\t\t\t * update c/mtime in shrink case below\n\t\t\t */\n\t\t\tif (!shrink) {\n\t\t\t\tinode->i_mtime = current_time(inode);\n\t\t\t\tinode->i_ctime = inode->i_mtime;\n\t\t\t}\n\t\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_I(inode)->i_disksize = attr->ia_size;\n\t\t\trc = ext4_mark_inode_dirty(handle, inode);\n\t\t\tif (!error)\n\t\t\t\terror = rc;\n\t\t\t/*\n\t\t\t * We have to update i_size under i_data_sem together\n\t\t\t * with i_disksize to avoid races with writeback code\n\t\t\t * running ext4_wb_update_i_disksize().\n\t\t\t */\n\t\t\tif (!error)\n\t\t\t\ti_size_write(inode, attr->ia_size);\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\text4_journal_stop(handle);\n\t\t\tif (error)\n\t\t\t\tgoto out_mmap_sem;\n\t\t\tif (!shrink) {\n\t\t\t\tpagecache_isize_extended(inode, oldsize,\n\t\t\t\t\t\t\t inode->i_size);\n\t\t\t} else if (ext4_should_journal_data(inode)) {\n\t\t\t\text4_wait_for_tail_page_commit(inode);\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Truncate pagecache after we've waited for commit\n\t\t * in data=journal mode to make pages freeable.\n\t\t */\n\t\ttruncate_pagecache(inode, inode->i_size);\n\t\t/*\n\t\t * Call ext4_truncate() even if i_size didn't change to\n\t\t * truncate possible preallocated blocks.\n\t\t */\n\t\tif (attr->ia_size <= oldsize) {\n\t\t\trc = ext4_truncate(inode);\n\t\t\tif (rc)\n\t\t\t\terror = rc;\n\t\t}\nout_mmap_sem:\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t}\n\n\tif (!error) {\n\t\tsetattr_copy(inode, attr);\n\t\tmark_inode_dirty(inode);\n\t}\n\n\t/*\n\t * If the call to ext4_truncate failed to get a transaction handle at\n\t * all, we need to clean up the in-core orphan list manually.\n\t */\n\tif (orphan && inode->i_nlink)\n\t\text4_orphan_del(NULL, inode);\n\n\tif (!error && (ia_valid & ATTR_MODE))\n\t\trc = posix_acl_chmod(inode, inode->i_mode);\n\nerr_out:\n\text4_std_error(inode->i_sb, error);\n\tif (!error)\n\t\terror = rc;\n\treturn error;\n}\n\nint ext4_getattr(const struct path *path, struct kstat *stat,\n\t\t u32 request_mask, unsigned int query_flags)\n{\n\tstruct inode *inode = d_inode(path->dentry);\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tunsigned int flags;\n\n\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_crtime)) {\n\t\tstat->result_mask |= STATX_BTIME;\n\t\tstat->btime.tv_sec = ei->i_crtime.tv_sec;\n\t\tstat->btime.tv_nsec = ei->i_crtime.tv_nsec;\n\t}\n\n\tflags = ei->i_flags & EXT4_FL_USER_VISIBLE;\n\tif (flags & EXT4_APPEND_FL)\n\t\tstat->attributes |= STATX_ATTR_APPEND;\n\tif (flags & EXT4_COMPR_FL)\n\t\tstat->attributes |= STATX_ATTR_COMPRESSED;\n\tif (flags & EXT4_ENCRYPT_FL)\n\t\tstat->attributes |= STATX_ATTR_ENCRYPTED;\n\tif (flags & EXT4_IMMUTABLE_FL)\n\t\tstat->attributes |= STATX_ATTR_IMMUTABLE;\n\tif (flags & EXT4_NODUMP_FL)\n\t\tstat->attributes |= STATX_ATTR_NODUMP;\n\n\tstat->attributes_mask |= (STATX_ATTR_APPEND |\n\t\t\t\t  STATX_ATTR_COMPRESSED |\n\t\t\t\t  STATX_ATTR_ENCRYPTED |\n\t\t\t\t  STATX_ATTR_IMMUTABLE |\n\t\t\t\t  STATX_ATTR_NODUMP);\n\n\tgeneric_fillattr(inode, stat);\n\treturn 0;\n}\n\nint ext4_file_getattr(const struct path *path, struct kstat *stat,\n\t\t      u32 request_mask, unsigned int query_flags)\n{\n\tstruct inode *inode = d_inode(path->dentry);\n\tu64 delalloc_blocks;\n\n\text4_getattr(path, stat, request_mask, query_flags);\n\n\t/*\n\t * If there is inline data in the inode, the inode will normally not\n\t * have data blocks allocated (it may have an external xattr block).\n\t * Report at least one sector for such files, so tools like tar, rsync,\n\t * others don't incorrectly think the file is completely sparse.\n\t */\n\tif (unlikely(ext4_has_inline_data(inode)))\n\t\tstat->blocks += (stat->size + 511) >> 9;\n\n\t/*\n\t * We can't update i_blocks if the block allocation is delayed\n\t * otherwise in the case of system crash before the real block\n\t * allocation is done, we will have i_blocks inconsistent with\n\t * on-disk file blocks.\n\t * We always keep i_blocks updated together with real\n\t * allocation. But to not confuse with user, stat\n\t * will return the blocks that include the delayed allocation\n\t * blocks for this file.\n\t */\n\tdelalloc_blocks = EXT4_C2B(EXT4_SB(inode->i_sb),\n\t\t\t\t   EXT4_I(inode)->i_reserved_data_blocks);\n\tstat->blocks += delalloc_blocks << (inode->i_sb->s_blocksize_bits - 9);\n\treturn 0;\n}\n\nstatic int ext4_index_trans_blocks(struct inode *inode, int lblocks,\n\t\t\t\t   int pextents)\n{\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn ext4_ind_trans_blocks(inode, lblocks);\n\treturn ext4_ext_index_trans_blocks(inode, pextents);\n}\n\n/*\n * Account for index blocks, block groups bitmaps and block group\n * descriptor blocks if modify datablocks and index blocks\n * worse case, the indexs blocks spread over different block groups\n *\n * If datablocks are discontiguous, they are possible to spread over\n * different block groups too. If they are contiguous, with flexbg,\n * they could still across block group boundary.\n *\n * Also account for superblock, inode, quota and xattr blocks\n */\nstatic int ext4_meta_trans_blocks(struct inode *inode, int lblocks,\n\t\t\t\t  int pextents)\n{\n\text4_group_t groups, ngroups = ext4_get_groups_count(inode->i_sb);\n\tint gdpblocks;\n\tint idxblocks;\n\tint ret = 0;\n\n\t/*\n\t * How many index blocks need to touch to map @lblocks logical blocks\n\t * to @pextents physical extents?\n\t */\n\tidxblocks = ext4_index_trans_blocks(inode, lblocks, pextents);\n\n\tret = idxblocks;\n\n\t/*\n\t * Now let's see how many group bitmaps and group descriptors need\n\t * to account\n\t */\n\tgroups = idxblocks + pextents;\n\tgdpblocks = groups;\n\tif (groups > ngroups)\n\t\tgroups = ngroups;\n\tif (groups > EXT4_SB(inode->i_sb)->s_gdb_count)\n\t\tgdpblocks = EXT4_SB(inode->i_sb)->s_gdb_count;\n\n\t/* bitmaps and block group descriptor blocks */\n\tret += groups + gdpblocks;\n\n\t/* Blocks for super block, inode, quota and xattr blocks */\n\tret += EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\n\treturn ret;\n}\n\n/*\n * Calculate the total number of credits to reserve to fit\n * the modification of a single pages into a single transaction,\n * which may include multiple chunks of block allocations.\n *\n * This could be called via ext4_write_begin()\n *\n * We need to consider the worse case, when\n * one new block per extent.\n */\nint ext4_writepage_trans_blocks(struct inode *inode)\n{\n\tint bpp = ext4_journal_blocks_per_page(inode);\n\tint ret;\n\n\tret = ext4_meta_trans_blocks(inode, bpp, bpp);\n\n\t/* Account for data blocks for journalled mode */\n\tif (ext4_should_journal_data(inode))\n\t\tret += bpp;\n\treturn ret;\n}\n\n/*\n * Calculate the journal credits for a chunk of data modification.\n *\n * This is called from DIO, fallocate or whoever calling\n * ext4_map_blocks() to map/allocate a chunk of contiguous disk blocks.\n *\n * journal buffers for data blocks are not included here, as DIO\n * and fallocate do no need to journal data buffers.\n */\nint ext4_chunk_trans_blocks(struct inode *inode, int nrblocks)\n{\n\treturn ext4_meta_trans_blocks(inode, nrblocks, 1);\n}\n\n/*\n * The caller must have previously called ext4_reserve_inode_write().\n * Give this, we know that the caller already has write access to iloc->bh.\n */\nint ext4_mark_iloc_dirty(handle_t *handle,\n\t\t\t struct inode *inode, struct ext4_iloc *iloc)\n{\n\tint err = 0;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb)))) {\n\t\tput_bh(iloc->bh);\n\t\treturn -EIO;\n\t}\n\tif (IS_I_VERSION(inode))\n\t\tinode_inc_iversion(inode);\n\n\t/* the do_update_inode consumes one bh->b_count */\n\tget_bh(iloc->bh);\n\n\t/* ext4_do_update_inode() does jbd2_journal_dirty_metadata */\n\terr = ext4_do_update_inode(handle, inode, iloc);\n\tput_bh(iloc->bh);\n\treturn err;\n}\n\n/*\n * On success, We end up with an outstanding reference count against\n * iloc->bh.  This _must_ be cleaned up later.\n */\n\nint\next4_reserve_inode_write(handle_t *handle, struct inode *inode,\n\t\t\t struct ext4_iloc *iloc)\n{\n\tint err;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\terr = ext4_get_inode_loc(inode, iloc);\n\tif (!err) {\n\t\tBUFFER_TRACE(iloc->bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, iloc->bh);\n\t\tif (err) {\n\t\t\tbrelse(iloc->bh);\n\t\t\tiloc->bh = NULL;\n\t\t}\n\t}\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n\nstatic int __ext4_expand_extra_isize(struct inode *inode,\n\t\t\t\t     unsigned int new_extra_isize,\n\t\t\t\t     struct ext4_iloc *iloc,\n\t\t\t\t     handle_t *handle, int *no_expand)\n{\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tint error;\n\n\traw_inode = ext4_raw_inode(iloc);\n\n\theader = IHDR(inode, raw_inode);\n\n\t/* No extended attributes present */\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||\n\t    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\tmemset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE +\n\t\t       EXT4_I(inode)->i_extra_isize, 0,\n\t\t       new_extra_isize - EXT4_I(inode)->i_extra_isize);\n\t\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\t\treturn 0;\n\t}\n\n\t/* try to expand with EAs present */\n\terror = ext4_expand_extra_isize_ea(inode, new_extra_isize,\n\t\t\t\t\t   raw_inode, handle);\n\tif (error) {\n\t\t/*\n\t\t * Inode size expansion failed; don't try again\n\t\t */\n\t\t*no_expand = 1;\n\t}\n\n\treturn error;\n}\n\n/*\n * Expand an inode by new_extra_isize bytes.\n * Returns 0 on success or negative error number on failure.\n */\nstatic int ext4_try_to_expand_extra_isize(struct inode *inode,\n\t\t\t\t\t  unsigned int new_extra_isize,\n\t\t\t\t\t  struct ext4_iloc iloc,\n\t\t\t\t\t  handle_t *handle)\n{\n\tint no_expand;\n\tint error;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND))\n\t\treturn -EOVERFLOW;\n\n\t/*\n\t * In nojournal mode, we can immediately attempt to expand\n\t * the inode.  When journaled, we first need to obtain extra\n\t * buffer credits since we may write into the EA block\n\t * with this same handle. If journal_extend fails, then it will\n\t * only result in a minor loss of functionality for that inode.\n\t * If this is felt to be critical, then e2fsck should be run to\n\t * force a large enough s_min_extra_isize.\n\t */\n\tif (ext4_journal_extend(handle,\n\t\t\t\tEXT4_DATA_TRANS_BLOCKS(inode->i_sb), 0) != 0)\n\t\treturn -ENOSPC;\n\n\tif (ext4_write_trylock_xattr(inode, &no_expand) == 0)\n\t\treturn -EBUSY;\n\n\terror = __ext4_expand_extra_isize(inode, new_extra_isize, &iloc,\n\t\t\t\t\t  handle, &no_expand);\n\text4_write_unlock_xattr(inode, &no_expand);\n\n\treturn error;\n}\n\nint ext4_expand_extra_isize(struct inode *inode,\n\t\t\t    unsigned int new_extra_isize,\n\t\t\t    struct ext4_iloc *iloc)\n{\n\thandle_t *handle;\n\tint no_expand;\n\tint error, rc;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND)) {\n\t\tbrelse(iloc->bh);\n\t\treturn -EOVERFLOW;\n\t}\n\n\thandle = ext4_journal_start(inode, EXT4_HT_INODE,\n\t\t\t\t    EXT4_DATA_TRANS_BLOCKS(inode->i_sb));\n\tif (IS_ERR(handle)) {\n\t\terror = PTR_ERR(handle);\n\t\tbrelse(iloc->bh);\n\t\treturn error;\n\t}\n\n\text4_write_lock_xattr(inode, &no_expand);\n\n\tBUFFER_TRACE(iloc->bh, \"get_write_access\");\n\terror = ext4_journal_get_write_access(handle, iloc->bh);\n\tif (error) {\n\t\tbrelse(iloc->bh);\n\t\tgoto out_stop;\n\t}\n\n\terror = __ext4_expand_extra_isize(inode, new_extra_isize, iloc,\n\t\t\t\t\t  handle, &no_expand);\n\n\trc = ext4_mark_iloc_dirty(handle, inode, iloc);\n\tif (!error)\n\t\terror = rc;\n\n\text4_write_unlock_xattr(inode, &no_expand);\nout_stop:\n\text4_journal_stop(handle);\n\treturn error;\n}\n\n/*\n * What we do here is to mark the in-core inode as clean with respect to inode\n * dirtiness (it may still be data-dirty).\n * This means that the in-core inode may be reaped by prune_icache\n * without having to perform any I/O.  This is a very good thing,\n * because *any* task may call prune_icache - even ones which\n * have a transaction open against a different journal.\n *\n * Is this cheating?  Not really.  Sure, we haven't written the\n * inode out, but prune_icache isn't a user-visible syncing function.\n * Whenever the user wants stuff synced (sys_sync, sys_msync, sys_fsync)\n * we start and wait on commits.\n */\nint ext4_mark_inode_dirty(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint err;\n\n\tmight_sleep();\n\ttrace_ext4_mark_inode_dirty(inode, _RET_IP_);\n\terr = ext4_reserve_inode_write(handle, inode, &iloc);\n\tif (err)\n\t\treturn err;\n\n\tif (EXT4_I(inode)->i_extra_isize < sbi->s_want_extra_isize)\n\t\text4_try_to_expand_extra_isize(inode, sbi->s_want_extra_isize,\n\t\t\t\t\t       iloc, handle);\n\n\treturn ext4_mark_iloc_dirty(handle, inode, &iloc);\n}\n\n/*\n * ext4_dirty_inode() is called from __mark_inode_dirty()\n *\n * We're really interested in the case where a file is being extended.\n * i_size has been changed by generic_commit_write() and we thus need\n * to include the updated inode in the current transaction.\n *\n * Also, dquot_alloc_block() will always dirty the inode when blocks\n * are allocated to the file.\n *\n * If the inode is marked synchronous, we don't honour that here - doing\n * so would cause a commit on atime updates, which we don't bother doing.\n * We handle synchronous inodes at the highest possible level.\n *\n * If only the I_DIRTY_TIME flag is set, we can skip everything.  If\n * I_DIRTY_TIME and I_DIRTY_SYNC is set, the only inode fields we need\n * to copy into the on-disk inode structure are the timestamp files.\n */\nvoid ext4_dirty_inode(struct inode *inode, int flags)\n{\n\thandle_t *handle;\n\n\tif (flags == I_DIRTY_TIME)\n\t\treturn;\n\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 2);\n\tif (IS_ERR(handle))\n\t\tgoto out;\n\n\text4_mark_inode_dirty(handle, inode);\n\n\text4_journal_stop(handle);\nout:\n\treturn;\n}\n\nint ext4_change_inode_journal_flag(struct inode *inode, int val)\n{\n\tjournal_t *journal;\n\thandle_t *handle;\n\tint err;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t/*\n\t * We have to be very careful here: changing a data block's\n\t * journaling status dynamically is dangerous.  If we write a\n\t * data block to the journal, change the status and then delete\n\t * that block, we risk forgetting to revoke the old log record\n\t * from the journal and so a subsequent replay can corrupt data.\n\t * So, first we make sure that the journal is empty and that\n\t * nobody is changing anything.\n\t */\n\n\tjournal = EXT4_JOURNAL(inode);\n\tif (!journal)\n\t\treturn 0;\n\tif (is_journal_aborted(journal))\n\t\treturn -EROFS;\n\n\t/* Wait for all existing dio workers */\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Before flushing the journal and switching inode's aops, we have\n\t * to flush all dirty data the inode has. There can be outstanding\n\t * delayed allocations, there can be unwritten extents created by\n\t * fallocate or buffered writes in dioread_nolock mode covered by\n\t * dirty data which can be converted only after flushing the dirty\n\t * data (and journalled aops don't know how to handle these cases).\n\t */\n\tif (val) {\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\t\terr = filemap_write_and_wait(inode->i_mapping);\n\t\tif (err < 0) {\n\t\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tpercpu_down_write(&sbi->s_journal_flag_rwsem);\n\tjbd2_journal_lock_updates(journal);\n\n\t/*\n\t * OK, there are no updates running now, and all cached data is\n\t * synced to disk.  We are now in a completely consistent state\n\t * which doesn't have anything in the journal, and we know that\n\t * no filesystem updates are running, so it is safe to modify\n\t * the inode's in-core data-journaling state flag now.\n\t */\n\n\tif (val)\n\t\text4_set_inode_flag(inode, EXT4_INODE_JOURNAL_DATA);\n\telse {\n\t\terr = jbd2_journal_flush(journal);\n\t\tif (err < 0) {\n\t\t\tjbd2_journal_unlock_updates(journal);\n\t\t\tpercpu_up_write(&sbi->s_journal_flag_rwsem);\n\t\t\treturn err;\n\t\t}\n\t\text4_clear_inode_flag(inode, EXT4_INODE_JOURNAL_DATA);\n\t}\n\text4_set_aops(inode);\n\n\tjbd2_journal_unlock_updates(journal);\n\tpercpu_up_write(&sbi->s_journal_flag_rwsem);\n\n\tif (val)\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\n\t/* Finally we can mark the inode as dirty. */\n\n\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\terr = ext4_mark_inode_dirty(handle, inode);\n\text4_handle_sync(handle);\n\text4_journal_stop(handle);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\nstatic int ext4_bh_unmapped(handle_t *handle, struct buffer_head *bh)\n{\n\treturn !buffer_mapped(bh);\n}\n\nvm_fault_t ext4_page_mkwrite(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page = vmf->page;\n\tloff_t size;\n\tunsigned long len;\n\tint err;\n\tvm_fault_t ret;\n\tstruct file *file = vma->vm_file;\n\tstruct inode *inode = file_inode(file);\n\tstruct address_space *mapping = inode->i_mapping;\n\thandle_t *handle;\n\tget_block_t *get_block;\n\tint retries = 0;\n\n\tif (unlikely(IS_IMMUTABLE(inode)))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tsb_start_pagefault(inode->i_sb);\n\tfile_update_time(vma->vm_file);\n\n\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\n\terr = ext4_convert_inline_data(inode);\n\tif (err)\n\t\tgoto out_ret;\n\n\t/* Delalloc case is easy... */\n\tif (test_opt(inode->i_sb, DELALLOC) &&\n\t    !ext4_should_journal_data(inode) &&\n\t    !ext4_nonda_switch(inode->i_sb)) {\n\t\tdo {\n\t\t\terr = block_page_mkwrite(vma, vmf,\n\t\t\t\t\t\t   ext4_da_get_block_prep);\n\t\t} while (err == -ENOSPC &&\n\t\t       ext4_should_retry_alloc(inode->i_sb, &retries));\n\t\tgoto out_ret;\n\t}\n\n\tlock_page(page);\n\tsize = i_size_read(inode);\n\t/* Page got truncated from under us? */\n\tif (page->mapping != mapping || page_offset(page) > size) {\n\t\tunlock_page(page);\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\tif (page->index == size >> PAGE_SHIFT)\n\t\tlen = size & ~PAGE_MASK;\n\telse\n\t\tlen = PAGE_SIZE;\n\t/*\n\t * Return if we have all the buffers mapped. This avoids the need to do\n\t * journal_start/journal_stop which can block and take a long time\n\t */\n\tif (page_has_buffers(page)) {\n\t\tif (!ext4_walk_page_buffers(NULL, page_buffers(page),\n\t\t\t\t\t    0, len, NULL,\n\t\t\t\t\t    ext4_bh_unmapped)) {\n\t\t\t/* Wait so that we don't change page under IO */\n\t\t\twait_for_stable_page(page);\n\t\t\tret = VM_FAULT_LOCKED;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tunlock_page(page);\n\t/* OK, we need to fill the hole... */\n\tif (ext4_should_dioread_nolock(inode))\n\t\tget_block = ext4_get_block_unwritten;\n\telse\n\t\tget_block = ext4_get_block;\nretry_alloc:\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,\n\t\t\t\t    ext4_writepage_trans_blocks(inode));\n\tif (IS_ERR(handle)) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out;\n\t}\n\terr = block_page_mkwrite(vma, vmf, get_block);\n\tif (!err && ext4_should_journal_data(inode)) {\n\t\tif (ext4_walk_page_buffers(handle, page_buffers(page), 0,\n\t\t\t  PAGE_SIZE, NULL, do_journal_get_write_access)) {\n\t\t\tunlock_page(page);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\text4_journal_stop(handle);\n\t\t\tgoto out;\n\t\t}\n\t\text4_set_inode_state(inode, EXT4_STATE_JDATA);\n\t}\n\text4_journal_stop(handle);\n\tif (err == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry_alloc;\nout_ret:\n\tret = block_page_mkwrite_return(err);\nout:\n\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\tsb_end_pagefault(inode->i_sb);\n\treturn ret;\n}\n\nvm_fault_t ext4_filemap_fault(struct vm_fault *vmf)\n{\n\tstruct inode *inode = file_inode(vmf->vma->vm_file);\n\tvm_fault_t ret;\n\n\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\tret = filemap_fault(vmf);\n\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\n\treturn ret;\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/ext4/super.c\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/fs/minix/inode.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  Big-endian to little-endian byte-swapping/bitmaps by\n *        David S. Miller (davem@caip.rutgers.edu), 1995\n */\n\n#include <linux/module.h>\n#include <linux/string.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/parser.h>\n#include <linux/buffer_head.h>\n#include <linux/exportfs.h>\n#include <linux/vfs.h>\n#include <linux/random.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/quotaops.h>\n#include <linux/seq_file.h>\n#include <linux/ctype.h>\n#include <linux/log2.h>\n#include <linux/crc16.h>\n#include <linux/dax.h>\n#include <linux/cleancache.h>\n#include <linux/uaccess.h>\n#include <linux/iversion.h>\n#include <linux/unicode.h>\n\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n\n#include \"ext4.h\"\n#include \"ext4_extents.h\"\t/* Needed for trace points definition */\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n#include \"mballoc.h\"\n#include \"fsmap.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/ext4.h>\n\nstatic struct ext4_lazy_init *ext4_li_info;\nstatic struct mutex ext4_li_mtx;\nstatic struct ratelimit_state ext4_mount_msg_ratelimit;\n\nstatic int ext4_load_journal(struct super_block *, struct ext4_super_block *,\n\t\t\t     unsigned long journal_devnum);\nstatic int ext4_show_options(struct seq_file *seq, struct dentry *root);\nstatic int ext4_commit_super(struct super_block *sb, int sync);\nstatic void ext4_mark_recovery_complete(struct super_block *sb,\n\t\t\t\t\tstruct ext4_super_block *es);\nstatic void ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es);\nstatic int ext4_sync_fs(struct super_block *sb, int wait);\nstatic int ext4_remount(struct super_block *sb, int *flags, char *data);\nstatic int ext4_statfs(struct dentry *dentry, struct kstatfs *buf);\nstatic int ext4_unfreeze(struct super_block *sb);\nstatic int ext4_freeze(struct super_block *sb);\nstatic struct dentry *ext4_mount(struct file_system_type *fs_type, int flags,\n\t\t       const char *dev_name, void *data);\nstatic inline int ext2_feature_set_ok(struct super_block *sb);\nstatic inline int ext3_feature_set_ok(struct super_block *sb);\nstatic int ext4_feature_set_ok(struct super_block *sb, int readonly);\nstatic void ext4_destroy_lazyinit_thread(void);\nstatic void ext4_unregister_li_request(struct super_block *sb);\nstatic void ext4_clear_request_list(void);\nstatic struct inode *ext4_get_journal_inode(struct super_block *sb,\n\t\t\t\t\t    unsigned int journal_inum);\n\n/*\n * Lock ordering\n *\n * Note the difference between i_mmap_sem (EXT4_I(inode)->i_mmap_sem) and\n * i_mmap_rwsem (inode->i_mmap_rwsem)!\n *\n * page fault path:\n * mmap_sem -> sb_start_pagefault -> i_mmap_sem (r) -> transaction start ->\n *   page lock -> i_data_sem (rw)\n *\n * buffered write path:\n * sb_start_write -> i_mutex -> mmap_sem\n * sb_start_write -> i_mutex -> transaction start -> page lock ->\n *   i_data_sem (rw)\n *\n * truncate:\n * sb_start_write -> i_mutex -> i_mmap_sem (w) -> i_mmap_rwsem (w) -> page lock\n * sb_start_write -> i_mutex -> i_mmap_sem (w) -> transaction start ->\n *   i_data_sem (rw)\n *\n * direct IO:\n * sb_start_write -> i_mutex -> mmap_sem\n * sb_start_write -> i_mutex -> transaction start -> i_data_sem (rw)\n *\n * writepages:\n * transaction start -> page lock(s) -> i_data_sem (rw)\n */\n\n#if !defined(CONFIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT2)\nstatic struct file_system_type ext2_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext2\",\n\t.mount\t\t= ext4_mount,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"ext2\");\nMODULE_ALIAS(\"ext2\");\n#define IS_EXT2_SB(sb) ((sb)->s_bdev->bd_holder == &ext2_fs_type)\n#else\n#define IS_EXT2_SB(sb) (0)\n#endif\n\n\nstatic struct file_system_type ext3_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext3\",\n\t.mount\t\t= ext4_mount,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"ext3\");\nMODULE_ALIAS(\"ext3\");\n#define IS_EXT3_SB(sb) ((sb)->s_bdev->bd_holder == &ext3_fs_type)\n\n/*\n * This works like sb_bread() except it uses ERR_PTR for error\n * returns.  Currently with sb_bread it's impossible to distinguish\n * between ENOMEM and EIO situations (since both result in a NULL\n * return.\n */\nstruct buffer_head *\next4_sb_bread(struct super_block *sb, sector_t block, int op_flags)\n{\n\tstruct buffer_head *bh = sb_getblk(sb, block);\n\n\tif (bh == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tll_rw_block(REQ_OP_READ, REQ_META | op_flags, 1, &bh);\n\twait_on_buffer(bh);\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tput_bh(bh);\n\treturn ERR_PTR(-EIO);\n}\n\nstatic int ext4_verify_csum_type(struct super_block *sb,\n\t\t\t\t struct ext4_super_block *es)\n{\n\tif (!ext4_has_feature_metadata_csum(sb))\n\t\treturn 1;\n\n\treturn es->s_checksum_type == EXT4_CRC32C_CHKSUM;\n}\n\nstatic __le32 ext4_superblock_csum(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tint offset = offsetof(struct ext4_super_block, s_checksum);\n\t__u32 csum;\n\n\tcsum = ext4_chksum(sbi, ~0, (char *)es, offset);\n\n\treturn cpu_to_le32(csum);\n}\n\nstatic int ext4_superblock_csum_verify(struct super_block *sb,\n\t\t\t\t       struct ext4_super_block *es)\n{\n\tif (!ext4_has_metadata_csum(sb))\n\t\treturn 1;\n\n\treturn es->s_checksum == ext4_superblock_csum(sb, es);\n}\n\nvoid ext4_superblock_csum_set(struct super_block *sb)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tif (!ext4_has_metadata_csum(sb))\n\t\treturn;\n\n\tes->s_checksum = ext4_superblock_csum(sb, es);\n}\n\nvoid *ext4_kvmalloc(size_t size, gfp_t flags)\n{\n\tvoid *ret;\n\n\tret = kmalloc(size, flags | __GFP_NOWARN);\n\tif (!ret)\n\t\tret = __vmalloc(size, flags, PAGE_KERNEL);\n\treturn ret;\n}\n\nvoid *ext4_kvzalloc(size_t size, gfp_t flags)\n{\n\tvoid *ret;\n\n\tret = kzalloc(size, flags | __GFP_NOWARN);\n\tif (!ret)\n\t\tret = __vmalloc(size, flags | __GFP_ZERO, PAGE_KERNEL);\n\treturn ret;\n}\n\next4_fsblk_t ext4_block_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_block_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_block_bitmap_hi) << 32 : 0);\n}\n\next4_fsblk_t ext4_inode_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_inode_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_bitmap_hi) << 32 : 0);\n}\n\next4_fsblk_t ext4_inode_table(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_inode_table_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_table_hi) << 32 : 0);\n}\n\n__u32 ext4_free_group_clusters(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_free_blocks_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_free_blocks_count_hi) << 16 : 0);\n}\n\n__u32 ext4_free_inodes_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_free_inodes_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_free_inodes_count_hi) << 16 : 0);\n}\n\n__u32 ext4_used_dirs_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_used_dirs_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_used_dirs_count_hi) << 16 : 0);\n}\n\n__u32 ext4_itable_unused_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_itable_unused_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_itable_unused_hi) << 16 : 0);\n}\n\nvoid ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_block_bitmap_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_block_bitmap_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_inode_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_inode_bitmap_lo  = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_inode_bitmap_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_inode_table_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_inode_table_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_inode_table_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_free_group_clusters_set(struct super_block *sb,\n\t\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_free_blocks_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_free_blocks_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_free_inodes_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_free_inodes_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_free_inodes_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_used_dirs_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_used_dirs_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_used_dirs_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_itable_unused_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_itable_unused_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_itable_unused_hi = cpu_to_le16(count >> 16);\n}\n\nstatic void __ext4_update_tstamp(__le32 *lo, __u8 *hi)\n{\n\ttime64_t now = ktime_get_real_seconds();\n\n\tnow = clamp_val(now, 0, (1ull << 40) - 1);\n\n\t*lo = cpu_to_le32(lower_32_bits(now));\n\t*hi = upper_32_bits(now);\n}\n\nstatic time64_t __ext4_get_tstamp(__le32 *lo, __u8 *hi)\n{\n\treturn ((time64_t)(*hi) << 32) + le32_to_cpu(*lo);\n}\n#define ext4_update_tstamp(es, tstamp) \\\n\t__ext4_update_tstamp(&(es)->tstamp, &(es)->tstamp ## _hi)\n#define ext4_get_tstamp(es, tstamp) \\\n\t__ext4_get_tstamp(&(es)->tstamp, &(es)->tstamp ## _hi)\n\nstatic void __save_error_info(struct super_block *sb, const char *func,\n\t\t\t    unsigned int line)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\tif (bdev_read_only(sb->s_bdev))\n\t\treturn;\n\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\text4_update_tstamp(es, s_last_error_time);\n\tstrncpy(es->s_last_error_func, func, sizeof(es->s_last_error_func));\n\tes->s_last_error_line = cpu_to_le32(line);\n\tif (!es->s_first_error_time) {\n\t\tes->s_first_error_time = es->s_last_error_time;\n\t\tes->s_first_error_time_hi = es->s_last_error_time_hi;\n\t\tstrncpy(es->s_first_error_func, func,\n\t\t\tsizeof(es->s_first_error_func));\n\t\tes->s_first_error_line = cpu_to_le32(line);\n\t\tes->s_first_error_ino = es->s_last_error_ino;\n\t\tes->s_first_error_block = es->s_last_error_block;\n\t}\n\t/*\n\t * Start the daily error reporting function if it hasn't been\n\t * started already\n\t */\n\tif (!es->s_error_count)\n\t\tmod_timer(&EXT4_SB(sb)->s_err_report, jiffies + 24*60*60*HZ);\n\tle32_add_cpu(&es->s_error_count, 1);\n}\n\nstatic void save_error_info(struct super_block *sb, const char *func,\n\t\t\t    unsigned int line)\n{\n\t__save_error_info(sb, func, line);\n\text4_commit_super(sb, 1);\n}\n\n/*\n * The del_gendisk() function uninitializes the disk-specific data\n * structures, including the bdi structure, without telling anyone\n * else.  Once this happens, any attempt to call mark_buffer_dirty()\n * (for example, by ext4_commit_super), will cause a kernel OOPS.\n * This is a kludge to prevent these oops until we can put in a proper\n * hook in del_gendisk() to inform the VFS and file system layers.\n */\nstatic int block_device_ejected(struct super_block *sb)\n{\n\tstruct inode *bd_inode = sb->s_bdev->bd_inode;\n\tstruct backing_dev_info *bdi = inode_to_bdi(bd_inode);\n\n\treturn bdi->dev == NULL;\n}\n\nstatic void ext4_journal_commit_callback(journal_t *journal, transaction_t *txn)\n{\n\tstruct super_block\t\t*sb = journal->j_private;\n\tstruct ext4_sb_info\t\t*sbi = EXT4_SB(sb);\n\tint\t\t\t\terror = is_journal_aborted(journal);\n\tstruct ext4_journal_cb_entry\t*jce;\n\n\tBUG_ON(txn->t_state == T_FINISHED);\n\n\text4_process_freed_data(sb, txn->t_tid);\n\n\tspin_lock(&sbi->s_md_lock);\n\twhile (!list_empty(&txn->t_private_list)) {\n\t\tjce = list_entry(txn->t_private_list.next,\n\t\t\t\t struct ext4_journal_cb_entry, jce_list);\n\t\tlist_del_init(&jce->jce_list);\n\t\tspin_unlock(&sbi->s_md_lock);\n\t\tjce->jce_func(sb, jce, error);\n\t\tspin_lock(&sbi->s_md_lock);\n\t}\n\tspin_unlock(&sbi->s_md_lock);\n}\n\nstatic bool system_going_down(void)\n{\n\treturn system_state == SYSTEM_HALT || system_state == SYSTEM_POWER_OFF\n\t\t|| system_state == SYSTEM_RESTART;\n}\n\n/* Deal with the reporting of failure conditions on a filesystem such as\n * inconsistencies detected or read IO failures.\n *\n * On ext2, we can store the error state of the filesystem in the\n * superblock.  That is not possible on ext4, because we may have other\n * write ordering constraints on the superblock which prevent us from\n * writing it out straight away; and given that the journal is about to\n * be aborted, we can't rely on the current, or future, transactions to\n * write out the superblock safely.\n *\n * We'll just use the jbd2_journal_abort() error code to record an error in\n * the journal instead.  On recovery, the journal will complain about\n * that error until we've noted it down and cleared it.\n */\n\nstatic void ext4_handle_error(struct super_block *sb)\n{\n\tif (test_opt(sb, WARN_ON_ERROR))\n\t\tWARN_ON_ONCE(1);\n\n\tif (sb_rdonly(sb))\n\t\treturn;\n\n\tif (!test_opt(sb, ERRORS_CONT)) {\n\t\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\t\tEXT4_SB(sb)->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\tif (journal)\n\t\t\tjbd2_journal_abort(journal, -EIO);\n\t}\n\t/*\n\t * We force ERRORS_RO behavior when system is rebooting. Otherwise we\n\t * could panic during 'reboot -f' as the underlying device got already\n\t * disabled.\n\t */\n\tif (test_opt(sb, ERRORS_RO) || system_going_down()) {\n\t\text4_msg(sb, KERN_CRIT, \"Remounting filesystem read-only\");\n\t\t/*\n\t\t * Make sure updated value of ->s_mount_flags will be visible\n\t\t * before ->s_flags update\n\t\t */\n\t\tsmp_wmb();\n\t\tsb->s_flags |= SB_RDONLY;\n\t} else if (test_opt(sb, ERRORS_PANIC)) {\n\t\tif (EXT4_SB(sb)->s_journal &&\n\t\t  !(EXT4_SB(sb)->s_journal->j_flags & JBD2_REC_ERR))\n\t\t\treturn;\n\t\tpanic(\"EXT4-fs (device %s): panic forced after error\\n\",\n\t\t\tsb->s_id);\n\t}\n}\n\n#define ext4_error_ratelimit(sb)\t\t\t\t\t\\\n\t\t___ratelimit(&(EXT4_SB(sb)->s_err_ratelimit_state),\t\\\n\t\t\t     \"EXT4-fs error\")\n\nvoid __ext4_error(struct super_block *sb, const char *function,\n\t\t  unsigned int line, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\ttrace_ext4_error(sb, function, line);\n\tif (ext4_error_ratelimit(sb)) {\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tprintk(KERN_CRIT\n\t\t       \"EXT4-fs error (device %s): %s:%d: comm %s: %pV\\n\",\n\t\t       sb->s_id, function, line, current->comm, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(sb, function, line);\n\text4_handle_error(sb);\n}\n\nvoid __ext4_error_inode(struct inode *inode, const char *function,\n\t\t\tunsigned int line, ext4_fsblk_t block,\n\t\t\tconst char *fmt, ...)\n{\n\tva_list args;\n\tstruct va_format vaf;\n\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn;\n\n\ttrace_ext4_error(inode->i_sb, function, line);\n\tes->s_last_error_ino = cpu_to_le32(inode->i_ino);\n\tes->s_last_error_block = cpu_to_le64(block);\n\tif (ext4_error_ratelimit(inode->i_sb)) {\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tif (block)\n\t\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s:%d: \"\n\t\t\t       \"inode #%lu: block %llu: comm %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       block, current->comm, &vaf);\n\t\telse\n\t\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s:%d: \"\n\t\t\t       \"inode #%lu: comm %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       current->comm, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(inode->i_sb, function, line);\n\text4_handle_error(inode->i_sb);\n}\n\nvoid __ext4_error_file(struct file *file, const char *function,\n\t\t       unsigned int line, ext4_fsblk_t block,\n\t\t       const char *fmt, ...)\n{\n\tva_list args;\n\tstruct va_format vaf;\n\tstruct ext4_super_block *es;\n\tstruct inode *inode = file_inode(file);\n\tchar pathname[80], *path;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn;\n\n\ttrace_ext4_error(inode->i_sb, function, line);\n\tes = EXT4_SB(inode->i_sb)->s_es;\n\tes->s_last_error_ino = cpu_to_le32(inode->i_ino);\n\tif (ext4_error_ratelimit(inode->i_sb)) {\n\t\tpath = file_path(file, pathname, sizeof(pathname));\n\t\tif (IS_ERR(path))\n\t\t\tpath = \"(unknown)\";\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tif (block)\n\t\t\tprintk(KERN_CRIT\n\t\t\t       \"EXT4-fs error (device %s): %s:%d: inode #%lu: \"\n\t\t\t       \"block %llu: comm %s: path %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       block, current->comm, path, &vaf);\n\t\telse\n\t\t\tprintk(KERN_CRIT\n\t\t\t       \"EXT4-fs error (device %s): %s:%d: inode #%lu: \"\n\t\t\t       \"comm %s: path %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       current->comm, path, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(inode->i_sb, function, line);\n\text4_handle_error(inode->i_sb);\n}\n\nconst char *ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t      char nbuf[16])\n{\n\tchar *errstr = NULL;\n\n\tswitch (errno) {\n\tcase -EFSCORRUPTED:\n\t\terrstr = \"Corrupt filesystem\";\n\t\tbreak;\n\tcase -EFSBADCRC:\n\t\terrstr = \"Filesystem failed CRC\";\n\t\tbreak;\n\tcase -EIO:\n\t\terrstr = \"IO failure\";\n\t\tbreak;\n\tcase -ENOMEM:\n\t\terrstr = \"Out of memory\";\n\t\tbreak;\n\tcase -EROFS:\n\t\tif (!sb || (EXT4_SB(sb)->s_journal &&\n\t\t\t    EXT4_SB(sb)->s_journal->j_flags & JBD2_ABORT))\n\t\t\terrstr = \"Journal has aborted\";\n\t\telse\n\t\t\terrstr = \"Readonly filesystem\";\n\t\tbreak;\n\tdefault:\n\t\t/* If the caller passed in an extra buffer for unknown\n\t\t * errors, textualise them now.  Else we just return\n\t\t * NULL. */\n\t\tif (nbuf) {\n\t\t\t/* Check for truncated error codes... */\n\t\t\tif (snprintf(nbuf, 16, \"error %d\", -errno) >= 0)\n\t\t\t\terrstr = nbuf;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn errstr;\n}\n\n/* __ext4_std_error decodes expected errors from journaling functions\n * automatically and invokes the appropriate error response.  */\n\nvoid __ext4_std_error(struct super_block *sb, const char *function,\n\t\t      unsigned int line, int errno)\n{\n\tchar nbuf[16];\n\tconst char *errstr;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\t/* Special case: if the error is EROFS, and we're not already\n\t * inside a transaction, then there's really no point in logging\n\t * an error. */\n\tif (errno == -EROFS && journal_current_handle() == NULL && sb_rdonly(sb))\n\t\treturn;\n\n\tif (ext4_error_ratelimit(sb)) {\n\t\terrstr = ext4_decode_error(sb, errno, nbuf);\n\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s) in %s:%d: %s\\n\",\n\t\t       sb->s_id, function, line, errstr);\n\t}\n\n\tsave_error_info(sb, function, line);\n\text4_handle_error(sb);\n}\n\n/*\n * ext4_abort is a much stronger failure handler than ext4_error.  The\n * abort function may be used to deal with unrecoverable failures such\n * as journal IO errors or ENOMEM at a critical moment in log management.\n *\n * We unconditionally force the filesystem into an ABORT|READONLY state,\n * unless the error response on the fs has been set to panic in which\n * case we take the easy way out and panic immediately.\n */\n\nvoid __ext4_abort(struct super_block *sb, const char *function,\n\t\tunsigned int line, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\tsave_error_info(sb, function, line);\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s:%d: %pV\\n\",\n\t       sb->s_id, function, line, &vaf);\n\tva_end(args);\n\n\tif (sb_rdonly(sb) == 0) {\n\t\text4_msg(sb, KERN_CRIT, \"Remounting filesystem read-only\");\n\t\tEXT4_SB(sb)->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\t/*\n\t\t * Make sure updated value of ->s_mount_flags will be visible\n\t\t * before ->s_flags update\n\t\t */\n\t\tsmp_wmb();\n\t\tsb->s_flags |= SB_RDONLY;\n\t\tif (EXT4_SB(sb)->s_journal)\n\t\t\tjbd2_journal_abort(EXT4_SB(sb)->s_journal, -EIO);\n\t\tsave_error_info(sb, function, line);\n\t}\n\tif (test_opt(sb, ERRORS_PANIC) && !system_going_down()) {\n\t\tif (EXT4_SB(sb)->s_journal &&\n\t\t  !(EXT4_SB(sb)->s_journal->j_flags & JBD2_REC_ERR))\n\t\t\treturn;\n\t\tpanic(\"EXT4-fs panic from previous error\\n\");\n\t}\n}\n\nvoid __ext4_msg(struct super_block *sb,\n\t\tconst char *prefix, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (!___ratelimit(&(EXT4_SB(sb)->s_msg_ratelimit_state), \"EXT4-fs\"))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(\"%sEXT4-fs (%s): %pV\\n\", prefix, sb->s_id, &vaf);\n\tva_end(args);\n}\n\n#define ext4_warning_ratelimit(sb)\t\t\t\t\t\\\n\t\t___ratelimit(&(EXT4_SB(sb)->s_warning_ratelimit_state),\t\\\n\t\t\t     \"EXT4-fs warning\")\n\nvoid __ext4_warning(struct super_block *sb, const char *function,\n\t\t    unsigned int line, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (!ext4_warning_ratelimit(sb))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(KERN_WARNING \"EXT4-fs warning (device %s): %s:%d: %pV\\n\",\n\t       sb->s_id, function, line, &vaf);\n\tva_end(args);\n}\n\nvoid __ext4_warning_inode(const struct inode *inode, const char *function,\n\t\t\t  unsigned int line, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (!ext4_warning_ratelimit(inode->i_sb))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(KERN_WARNING \"EXT4-fs warning (device %s): %s:%d: \"\n\t       \"inode #%lu: comm %s: %pV\\n\", inode->i_sb->s_id,\n\t       function, line, inode->i_ino, current->comm, &vaf);\n\tva_end(args);\n}\n\nvoid __ext4_grp_locked_error(const char *function, unsigned int line,\n\t\t\t     struct super_block *sb, ext4_group_t grp,\n\t\t\t     unsigned long ino, ext4_fsblk_t block,\n\t\t\t     const char *fmt, ...)\n__releases(bitlock)\n__acquires(bitlock)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\ttrace_ext4_error(sb, function, line);\n\tes->s_last_error_ino = cpu_to_le32(ino);\n\tes->s_last_error_block = cpu_to_le64(block);\n\t__save_error_info(sb, function, line);\n\n\tif (ext4_error_ratelimit(sb)) {\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s:%d: group %u, \",\n\t\t       sb->s_id, function, line, grp);\n\t\tif (ino)\n\t\t\tprintk(KERN_CONT \"inode %lu: \", ino);\n\t\tif (block)\n\t\t\tprintk(KERN_CONT \"block %llu:\",\n\t\t\t       (unsigned long long) block);\n\t\tprintk(KERN_CONT \"%pV\\n\", &vaf);\n\t\tva_end(args);\n\t}\n\n\tif (test_opt(sb, WARN_ON_ERROR))\n\t\tWARN_ON_ONCE(1);\n\n\tif (test_opt(sb, ERRORS_CONT)) {\n\t\text4_commit_super(sb, 0);\n\t\treturn;\n\t}\n\n\text4_unlock_group(sb, grp);\n\text4_commit_super(sb, 1);\n\text4_handle_error(sb);\n\t/*\n\t * We only get here in the ERRORS_RO case; relocking the group\n\t * may be dangerous, but nothing bad will happen since the\n\t * filesystem will have already been marked read/only and the\n\t * journal has been aborted.  We return 1 as a hint to callers\n\t * who might what to use the return value from\n\t * ext4_grp_locked_error() to distinguish between the\n\t * ERRORS_CONT and ERRORS_RO case, and perhaps return more\n\t * aggressively from the ext4 function in question, with a\n\t * more appropriate error code.\n\t */\n\text4_lock_group(sb, grp);\n\treturn;\n}\n\nvoid ext4_mark_group_bitmap_corrupted(struct super_block *sb,\n\t\t\t\t     ext4_group_t group,\n\t\t\t\t     unsigned int flags)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, group, NULL);\n\tint ret;\n\n\tif (flags & EXT4_GROUP_INFO_BBITMAP_CORRUPT) {\n\t\tret = ext4_test_and_set_bit(EXT4_GROUP_INFO_BBITMAP_CORRUPT_BIT,\n\t\t\t\t\t    &grp->bb_state);\n\t\tif (!ret)\n\t\t\tpercpu_counter_sub(&sbi->s_freeclusters_counter,\n\t\t\t\t\t   grp->bb_free);\n\t}\n\n\tif (flags & EXT4_GROUP_INFO_IBITMAP_CORRUPT) {\n\t\tret = ext4_test_and_set_bit(EXT4_GROUP_INFO_IBITMAP_CORRUPT_BIT,\n\t\t\t\t\t    &grp->bb_state);\n\t\tif (!ret && gdp) {\n\t\t\tint count;\n\n\t\t\tcount = ext4_free_inodes_count(sb, gdp);\n\t\t\tpercpu_counter_sub(&sbi->s_freeinodes_counter,\n\t\t\t\t\t   count);\n\t\t}\n\t}\n}\n\nvoid ext4_update_dynamic_rev(struct super_block *sb)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tif (le32_to_cpu(es->s_rev_level) > EXT4_GOOD_OLD_REV)\n\t\treturn;\n\n\text4_warning(sb,\n\t\t     \"updating to rev %d because of new feature flag, \"\n\t\t     \"running e2fsck is recommended\",\n\t\t     EXT4_DYNAMIC_REV);\n\n\tes->s_first_ino = cpu_to_le32(EXT4_GOOD_OLD_FIRST_INO);\n\tes->s_inode_size = cpu_to_le16(EXT4_GOOD_OLD_INODE_SIZE);\n\tes->s_rev_level = cpu_to_le32(EXT4_DYNAMIC_REV);\n\t/* leave es->s_feature_*compat flags alone */\n\t/* es->s_uuid will be set by e2fsck if empty */\n\n\t/*\n\t * The rest of the superblock fields should be zero, and if not it\n\t * means they are likely already in use, so leave them alone.  We\n\t * can leave it up to e2fsck to clean up any inconsistencies there.\n\t */\n}\n\n/*\n * Open the external journal device\n */\nstatic struct block_device *ext4_blkdev_get(dev_t dev, struct super_block *sb)\n{\n\tstruct block_device *bdev;\n\tchar b[BDEVNAME_SIZE];\n\n\tbdev = blkdev_get_by_dev(dev, FMODE_READ|FMODE_WRITE|FMODE_EXCL, sb);\n\tif (IS_ERR(bdev))\n\t\tgoto fail;\n\treturn bdev;\n\nfail:\n\text4_msg(sb, KERN_ERR, \"failed to open journal device %s: %ld\",\n\t\t\t__bdevname(dev, b), PTR_ERR(bdev));\n\treturn NULL;\n}\n\n/*\n * Release the journal device\n */\nstatic void ext4_blkdev_put(struct block_device *bdev)\n{\n\tblkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);\n}\n\nstatic void ext4_blkdev_remove(struct ext4_sb_info *sbi)\n{\n\tstruct block_device *bdev;\n\tbdev = sbi->journal_bdev;\n\tif (bdev) {\n\t\text4_blkdev_put(bdev);\n\t\tsbi->journal_bdev = NULL;\n\t}\n}\n\nstatic inline struct inode *orphan_list_entry(struct list_head *l)\n{\n\treturn &list_entry(l, struct ext4_inode_info, i_orphan)->vfs_inode;\n}\n\nstatic void dump_orphan_list(struct super_block *sb, struct ext4_sb_info *sbi)\n{\n\tstruct list_head *l;\n\n\text4_msg(sb, KERN_ERR, \"sb orphan head is %d\",\n\t\t le32_to_cpu(sbi->s_es->s_last_orphan));\n\n\tprintk(KERN_ERR \"sb_info orphan list:\\n\");\n\tlist_for_each(l, &sbi->s_orphan) {\n\t\tstruct inode *inode = orphan_list_entry(l);\n\t\tprintk(KERN_ERR \"  \"\n\t\t       \"inode %s:%lu at %p: mode %o, nlink %d, next %d\\n\",\n\t\t       inode->i_sb->s_id, inode->i_ino, inode,\n\t\t       inode->i_mode, inode->i_nlink,\n\t\t       NEXT_ORPHAN(inode));\n\t}\n}\n\n#ifdef CONFIG_QUOTA\nstatic int ext4_quota_off(struct super_block *sb, int type);\n\nstatic inline void ext4_quota_off_umount(struct super_block *sb)\n{\n\tint type;\n\n\t/* Use our quota_off function to clear inode flags etc. */\n\tfor (type = 0; type < EXT4_MAXQUOTAS; type++)\n\t\text4_quota_off(sb, type);\n}\n\n/*\n * This is a helper function which is used in the mount/remount\n * codepaths (which holds s_umount) to fetch the quota file name.\n */\nstatic inline char *get_qf_name(struct super_block *sb,\n\t\t\t\tstruct ext4_sb_info *sbi,\n\t\t\t\tint type)\n{\n\treturn rcu_dereference_protected(sbi->s_qf_names[type],\n\t\t\t\t\t lockdep_is_held(&sb->s_umount));\n}\n#else\nstatic inline void ext4_quota_off_umount(struct super_block *sb)\n{\n}\n#endif\n\nstatic void ext4_put_super(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tint aborted = 0;\n\tint i, err;\n\n\text4_unregister_li_request(sb);\n\text4_quota_off_umount(sb);\n\n\tdestroy_workqueue(sbi->rsv_conversion_wq);\n\n\tif (sbi->s_journal) {\n\t\taborted = is_journal_aborted(sbi->s_journal);\n\t\terr = jbd2_journal_destroy(sbi->s_journal);\n\t\tsbi->s_journal = NULL;\n\t\tif ((err < 0) && !aborted)\n\t\t\text4_abort(sb, \"Couldn't clean up the journal\");\n\t}\n\n\text4_unregister_sysfs(sb);\n\text4_es_unregister_shrinker(sbi);\n\tdel_timer_sync(&sbi->s_err_report);\n\text4_release_system_zone(sb);\n\text4_mb_release(sb);\n\text4_ext_release(sb);\n\n\tif (!sb_rdonly(sb) && !aborted) {\n\t\text4_clear_feature_journal_needs_recovery(sb);\n\t\tes->s_state = cpu_to_le16(sbi->s_mount_state);\n\t}\n\tif (!sb_rdonly(sb))\n\t\text4_commit_super(sb, 1);\n\n\tfor (i = 0; i < sbi->s_gdb_count; i++)\n\t\tbrelse(sbi->s_group_desc[i]);\n\tkvfree(sbi->s_group_desc);\n\tkvfree(sbi->s_flex_groups);\n\tpercpu_counter_destroy(&sbi->s_freeclusters_counter);\n\tpercpu_counter_destroy(&sbi->s_freeinodes_counter);\n\tpercpu_counter_destroy(&sbi->s_dirs_counter);\n\tpercpu_counter_destroy(&sbi->s_dirtyclusters_counter);\n\tpercpu_free_rwsem(&sbi->s_journal_flag_rwsem);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tkfree(get_qf_name(sb, sbi, i));\n#endif\n\n\t/* Debugging code just in case the in-memory inode orphan list\n\t * isn't empty.  The on-disk one can be non-empty if we've\n\t * detected an error and taken the fs readonly, but the\n\t * in-memory list had better be clean by this point. */\n\tif (!list_empty(&sbi->s_orphan))\n\t\tdump_orphan_list(sb, sbi);\n\tJ_ASSERT(list_empty(&sbi->s_orphan));\n\n\tsync_blockdev(sb->s_bdev);\n\tinvalidate_bdev(sb->s_bdev);\n\tif (sbi->journal_bdev && sbi->journal_bdev != sb->s_bdev) {\n\t\t/*\n\t\t * Invalidate the journal device's buffers.  We don't want them\n\t\t * floating about in memory - the physical journal device may\n\t\t * hotswapped, and it breaks the `ro-after' testing code.\n\t\t */\n\t\tsync_blockdev(sbi->journal_bdev);\n\t\tinvalidate_bdev(sbi->journal_bdev);\n\t\text4_blkdev_remove(sbi);\n\t}\n\n\text4_xattr_destroy_cache(sbi->s_ea_inode_cache);\n\tsbi->s_ea_inode_cache = NULL;\n\n\text4_xattr_destroy_cache(sbi->s_ea_block_cache);\n\tsbi->s_ea_block_cache = NULL;\n\n\tif (sbi->s_mmp_tsk)\n\t\tkthread_stop(sbi->s_mmp_tsk);\n\tbrelse(sbi->s_sbh);\n\tsb->s_fs_info = NULL;\n\t/*\n\t * Now that we are completely done shutting down the\n\t * superblock, we need to actually destroy the kobject.\n\t */\n\tkobject_put(&sbi->s_kobj);\n\twait_for_completion(&sbi->s_kobj_unregister);\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi->s_blockgroup_lock);\n\tfs_put_dax(sbi->s_daxdev);\n#ifdef CONFIG_UNICODE\n\tutf8_unload(sbi->s_encoding);\n#endif\n\tkfree(sbi);\n}\n\nstatic struct kmem_cache *ext4_inode_cachep;\n\n/*\n * Called inside transaction, so use GFP_NOFS\n */\nstatic struct inode *ext4_alloc_inode(struct super_block *sb)\n{\n\tstruct ext4_inode_info *ei;\n\n\tei = kmem_cache_alloc(ext4_inode_cachep, GFP_NOFS);\n\tif (!ei)\n\t\treturn NULL;\n\n\tinode_set_iversion(&ei->vfs_inode, 1);\n\tspin_lock_init(&ei->i_raw_lock);\n\tINIT_LIST_HEAD(&ei->i_prealloc_list);\n\tspin_lock_init(&ei->i_prealloc_lock);\n\text4_es_init_tree(&ei->i_es_tree);\n\trwlock_init(&ei->i_es_lock);\n\tINIT_LIST_HEAD(&ei->i_es_list);\n\tei->i_es_all_nr = 0;\n\tei->i_es_shk_nr = 0;\n\tei->i_es_shrink_lblk = 0;\n\tei->i_reserved_data_blocks = 0;\n\tei->i_da_metadata_calc_len = 0;\n\tei->i_da_metadata_calc_last_lblock = 0;\n\tspin_lock_init(&(ei->i_block_reservation_lock));\n\text4_init_pending_tree(&ei->i_pending_tree);\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n\tmemset(&ei->i_dquot, 0, sizeof(ei->i_dquot));\n#endif\n\tei->jinode = NULL;\n\tINIT_LIST_HEAD(&ei->i_rsv_conversion_list);\n\tspin_lock_init(&ei->i_completed_io_lock);\n\tei->i_sync_tid = 0;\n\tei->i_datasync_tid = 0;\n\tatomic_set(&ei->i_unwritten, 0);\n\tINIT_WORK(&ei->i_rsv_conversion_work, ext4_end_io_rsv_work);\n\treturn &ei->vfs_inode;\n}\n\nstatic int ext4_drop_inode(struct inode *inode)\n{\n\tint drop = generic_drop_inode(inode);\n\n\tif (!drop)\n\t\tdrop = fscrypt_drop_inode(inode);\n\n\ttrace_ext4_drop_inode(inode, drop);\n\treturn drop;\n}\n\nstatic void ext4_free_in_core_inode(struct inode *inode)\n{\n\tfscrypt_free_inode(inode);\n\tkmem_cache_free(ext4_inode_cachep, EXT4_I(inode));\n}\n\nstatic void ext4_destroy_inode(struct inode *inode)\n{\n\tif (!list_empty(&(EXT4_I(inode)->i_orphan))) {\n\t\text4_msg(inode->i_sb, KERN_ERR,\n\t\t\t \"Inode %lu (%p): orphan list check failed!\",\n\t\t\t inode->i_ino, EXT4_I(inode));\n\t\tprint_hex_dump(KERN_INFO, \"\", DUMP_PREFIX_ADDRESS, 16, 4,\n\t\t\t\tEXT4_I(inode), sizeof(struct ext4_inode_info),\n\t\t\t\ttrue);\n\t\tdump_stack();\n\t}\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct ext4_inode_info *ei = (struct ext4_inode_info *) foo;\n\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\tinit_rwsem(&ei->xattr_sem);\n\tinit_rwsem(&ei->i_data_sem);\n\tinit_rwsem(&ei->i_mmap_sem);\n\tinode_init_once(&ei->vfs_inode);\n}\n\nstatic int __init init_inodecache(void)\n{\n\text4_inode_cachep = kmem_cache_create_usercopy(\"ext4_inode_cache\",\n\t\t\t\tsizeof(struct ext4_inode_info), 0,\n\t\t\t\t(SLAB_RECLAIM_ACCOUNT|SLAB_MEM_SPREAD|\n\t\t\t\t\tSLAB_ACCOUNT),\n\t\t\t\toffsetof(struct ext4_inode_info, i_data),\n\t\t\t\tsizeof_field(struct ext4_inode_info, i_data),\n\t\t\t\tinit_once);\n\tif (ext4_inode_cachep == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void destroy_inodecache(void)\n{\n\t/*\n\t * Make sure all delayed rcu free inodes are flushed before we\n\t * destroy cache.\n\t */\n\trcu_barrier();\n\tkmem_cache_destroy(ext4_inode_cachep);\n}\n\nvoid ext4_clear_inode(struct inode *inode)\n{\n\tinvalidate_inode_buffers(inode);\n\tclear_inode(inode);\n\tdquot_drop(inode);\n\text4_discard_preallocations(inode);\n\text4_es_remove_extent(inode, 0, EXT_MAX_BLOCKS);\n\tif (EXT4_I(inode)->jinode) {\n\t\tjbd2_journal_release_jbd_inode(EXT4_JOURNAL(inode),\n\t\t\t\t\t       EXT4_I(inode)->jinode);\n\t\tjbd2_free_inode(EXT4_I(inode)->jinode);\n\t\tEXT4_I(inode)->jinode = NULL;\n\t}\n\tfscrypt_put_encryption_info(inode);\n\tfsverity_cleanup_inode(inode);\n}\n\nstatic struct inode *ext4_nfs_get_inode(struct super_block *sb,\n\t\t\t\t\tu64 ino, u32 generation)\n{\n\tstruct inode *inode;\n\n\t/*\n\t * Currently we don't know the generation for parent directory, so\n\t * a generation of 0 means \"accept any\"\n\t */\n\tinode = ext4_iget(sb, ino, EXT4_IGET_HANDLE);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (generation && inode->i_generation != generation) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\n\treturn inode;\n}\n\nstatic struct dentry *ext4_fh_to_dentry(struct super_block *sb, struct fid *fid,\n\t\t\t\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_dentry(sb, fid, fh_len, fh_type,\n\t\t\t\t    ext4_nfs_get_inode);\n}\n\nstatic struct dentry *ext4_fh_to_parent(struct super_block *sb, struct fid *fid,\n\t\t\t\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_parent(sb, fid, fh_len, fh_type,\n\t\t\t\t    ext4_nfs_get_inode);\n}\n\nstatic int ext4_nfs_commit_metadata(struct inode *inode)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_ALL\n\t};\n\n\ttrace_ext4_nfs_commit_metadata(inode);\n\treturn ext4_write_inode(inode, &wbc);\n}\n\n/*\n * Try to release metadata pages (indirect blocks, directories) which are\n * mapped via the block device.  Since these pages could have journal heads\n * which would prevent try_to_free_buffers() from freeing them, we must use\n * jbd2 layer's try_to_free_buffers() function to release them.\n */\nstatic int bdev_try_to_free_page(struct super_block *sb, struct page *page,\n\t\t\t\t gfp_t wait)\n{\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\tWARN_ON(PageChecked(page));\n\tif (!page_has_buffers(page))\n\t\treturn 0;\n\tif (journal)\n\t\treturn jbd2_journal_try_to_free_buffers(journal, page,\n\t\t\t\t\t\twait & ~__GFP_DIRECT_RECLAIM);\n\treturn try_to_free_buffers(page);\n}\n\n#ifdef CONFIG_FS_ENCRYPTION\nstatic int ext4_get_context(struct inode *inode, void *ctx, size_t len)\n{\n\treturn ext4_xattr_get(inode, EXT4_XATTR_INDEX_ENCRYPTION,\n\t\t\t\t EXT4_XATTR_NAME_ENCRYPTION_CONTEXT, ctx, len);\n}\n\nstatic int ext4_set_context(struct inode *inode, const void *ctx, size_t len,\n\t\t\t\t\t\t\tvoid *fs_data)\n{\n\thandle_t *handle = fs_data;\n\tint res, res2, credits, retries = 0;\n\n\t/*\n\t * Encrypting the root directory is not allowed because e2fsck expects\n\t * lost+found to exist and be unencrypted, and encrypting the root\n\t * directory would imply encrypting the lost+found directory as well as\n\t * the filename \"lost+found\" itself.\n\t */\n\tif (inode->i_ino == EXT4_ROOT_INO)\n\t\treturn -EPERM;\n\n\tif (WARN_ON_ONCE(IS_DAX(inode) && i_size_read(inode)))\n\t\treturn -EINVAL;\n\n\tres = ext4_convert_inline_data(inode);\n\tif (res)\n\t\treturn res;\n\n\t/*\n\t * If a journal handle was specified, then the encryption context is\n\t * being set on a new inode via inheritance and is part of a larger\n\t * transaction to create the inode.  Otherwise the encryption context is\n\t * being set on an existing inode in its own transaction.  Only in the\n\t * latter case should the \"retry on ENOSPC\" logic be used.\n\t */\n\n\tif (handle) {\n\t\tres = ext4_xattr_set_handle(handle, inode,\n\t\t\t\t\t    EXT4_XATTR_INDEX_ENCRYPTION,\n\t\t\t\t\t    EXT4_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\t\t    ctx, len, 0);\n\t\tif (!res) {\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_ENCRYPT);\n\t\t\text4_clear_inode_state(inode,\n\t\t\t\t\tEXT4_STATE_MAY_INLINE_DATA);\n\t\t\t/*\n\t\t\t * Update inode->i_flags - S_ENCRYPTED will be enabled,\n\t\t\t * S_DAX may be disabled\n\t\t\t */\n\t\t\text4_set_inode_flags(inode);\n\t\t}\n\t\treturn res;\n\t}\n\n\tres = dquot_initialize(inode);\n\tif (res)\n\t\treturn res;\nretry:\n\tres = ext4_xattr_set_credits(inode, len, false /* is_create */,\n\t\t\t\t     &credits);\n\tif (res)\n\t\treturn res;\n\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, credits);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\tres = ext4_xattr_set_handle(handle, inode, EXT4_XATTR_INDEX_ENCRYPTION,\n\t\t\t\t    EXT4_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\t    ctx, len, 0);\n\tif (!res) {\n\t\text4_set_inode_flag(inode, EXT4_INODE_ENCRYPT);\n\t\t/*\n\t\t * Update inode->i_flags - S_ENCRYPTED will be enabled,\n\t\t * S_DAX may be disabled\n\t\t */\n\t\text4_set_inode_flags(inode);\n\t\tres = ext4_mark_inode_dirty(handle, inode);\n\t\tif (res)\n\t\t\tEXT4_ERROR_INODE(inode, \"Failed to mark inode dirty\");\n\t}\n\tres2 = ext4_journal_stop(handle);\n\n\tif (res == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\n\tif (!res)\n\t\tres = res2;\n\treturn res;\n}\n\nstatic bool ext4_dummy_context(struct inode *inode)\n{\n\treturn DUMMY_ENCRYPTION_ENABLED(EXT4_SB(inode->i_sb));\n}\n\nstatic const struct fscrypt_operations ext4_cryptops = {\n\t.key_prefix\t\t= \"ext4:\",\n\t.get_context\t\t= ext4_get_context,\n\t.set_context\t\t= ext4_set_context,\n\t.dummy_context\t\t= ext4_dummy_context,\n\t.empty_dir\t\t= ext4_empty_dir,\n\t.max_namelen\t\t= EXT4_NAME_LEN,\n};\n#endif\n\n#ifdef CONFIG_QUOTA\nstatic const char * const quotatypes[] = INITQFNAMES;\n#define QTYPE2NAME(t) (quotatypes[t])\n\nstatic int ext4_write_dquot(struct dquot *dquot);\nstatic int ext4_acquire_dquot(struct dquot *dquot);\nstatic int ext4_release_dquot(struct dquot *dquot);\nstatic int ext4_mark_dquot_dirty(struct dquot *dquot);\nstatic int ext4_write_info(struct super_block *sb, int type);\nstatic int ext4_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t const struct path *path);\nstatic int ext4_quota_on_mount(struct super_block *sb, int type);\nstatic ssize_t ext4_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off);\nstatic ssize_t ext4_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off);\nstatic int ext4_quota_enable(struct super_block *sb, int type, int format_id,\n\t\t\t     unsigned int flags);\nstatic int ext4_enable_quotas(struct super_block *sb);\nstatic int ext4_get_next_id(struct super_block *sb, struct kqid *qid);\n\nstatic struct dquot **ext4_get_dquots(struct inode *inode)\n{\n\treturn EXT4_I(inode)->i_dquot;\n}\n\nstatic const struct dquot_operations ext4_quota_operations = {\n\t.get_reserved_space\t= ext4_get_reserved_space,\n\t.write_dquot\t\t= ext4_write_dquot,\n\t.acquire_dquot\t\t= ext4_acquire_dquot,\n\t.release_dquot\t\t= ext4_release_dquot,\n\t.mark_dirty\t\t= ext4_mark_dquot_dirty,\n\t.write_info\t\t= ext4_write_info,\n\t.alloc_dquot\t\t= dquot_alloc,\n\t.destroy_dquot\t\t= dquot_destroy,\n\t.get_projid\t\t= ext4_get_projid,\n\t.get_inode_usage\t= ext4_get_inode_usage,\n\t.get_next_id\t\t= ext4_get_next_id,\n};\n\nstatic const struct quotactl_ops ext4_qctl_operations = {\n\t.quota_on\t= ext4_quota_on,\n\t.quota_off\t= ext4_quota_off,\n\t.quota_sync\t= dquot_quota_sync,\n\t.get_state\t= dquot_get_state,\n\t.set_info\t= dquot_set_dqinfo,\n\t.get_dqblk\t= dquot_get_dqblk,\n\t.set_dqblk\t= dquot_set_dqblk,\n\t.get_nextdqblk\t= dquot_get_next_dqblk,\n};\n#endif\n\nstatic const struct super_operations ext4_sops = {\n\t.alloc_inode\t= ext4_alloc_inode,\n\t.free_inode\t= ext4_free_in_core_inode,\n\t.destroy_inode\t= ext4_destroy_inode,\n\t.write_inode\t= ext4_write_inode,\n\t.dirty_inode\t= ext4_dirty_inode,\n\t.drop_inode\t= ext4_drop_inode,\n\t.evict_inode\t= ext4_evict_inode,\n\t.put_super\t= ext4_put_super,\n\t.sync_fs\t= ext4_sync_fs,\n\t.freeze_fs\t= ext4_freeze,\n\t.unfreeze_fs\t= ext4_unfreeze,\n\t.statfs\t\t= ext4_statfs,\n\t.remount_fs\t= ext4_remount,\n\t.show_options\t= ext4_show_options,\n#ifdef CONFIG_QUOTA\n\t.quota_read\t= ext4_quota_read,\n\t.quota_write\t= ext4_quota_write,\n\t.get_dquots\t= ext4_get_dquots,\n#endif\n\t.bdev_try_to_free_page = bdev_try_to_free_page,\n};\n\nstatic const struct export_operations ext4_export_ops = {\n\t.fh_to_dentry = ext4_fh_to_dentry,\n\t.fh_to_parent = ext4_fh_to_parent,\n\t.get_parent = ext4_get_parent,\n\t.commit_metadata = ext4_nfs_commit_metadata,\n};\n\nenum {\n\tOpt_bsd_df, Opt_minix_df, Opt_grpid, Opt_nogrpid,\n\tOpt_resgid, Opt_resuid, Opt_sb, Opt_err_cont, Opt_err_panic, Opt_err_ro,\n\tOpt_nouid32, Opt_debug, Opt_removed,\n\tOpt_user_xattr, Opt_nouser_xattr, Opt_acl, Opt_noacl,\n\tOpt_auto_da_alloc, Opt_noauto_da_alloc, Opt_noload,\n\tOpt_commit, Opt_min_batch_time, Opt_max_batch_time, Opt_journal_dev,\n\tOpt_journal_path, Opt_journal_checksum, Opt_journal_async_commit,\n\tOpt_abort, Opt_data_journal, Opt_data_ordered, Opt_data_writeback,\n\tOpt_data_err_abort, Opt_data_err_ignore, Opt_test_dummy_encryption,\n\tOpt_usrjquota, Opt_grpjquota, Opt_offusrjquota, Opt_offgrpjquota,\n\tOpt_jqfmt_vfsold, Opt_jqfmt_vfsv0, Opt_jqfmt_vfsv1, Opt_quota,\n\tOpt_noquota, Opt_barrier, Opt_nobarrier, Opt_err,\n\tOpt_usrquota, Opt_grpquota, Opt_prjquota, Opt_i_version, Opt_dax,\n\tOpt_stripe, Opt_delalloc, Opt_nodelalloc, Opt_warn_on_error,\n\tOpt_nowarn_on_error, Opt_mblk_io_submit,\n\tOpt_lazytime, Opt_nolazytime, Opt_debug_want_extra_isize,\n\tOpt_nomblk_io_submit, Opt_block_validity, Opt_noblock_validity,\n\tOpt_inode_readahead_blks, Opt_journal_ioprio,\n\tOpt_dioread_nolock, Opt_dioread_lock,\n\tOpt_discard, Opt_nodiscard, Opt_init_itable, Opt_noinit_itable,\n\tOpt_max_dir_size_kb, Opt_nojournal_checksum, Opt_nombcache,\n};\n\nstatic const match_table_t tokens = {\n\t{Opt_bsd_df, \"bsddf\"},\n\t{Opt_minix_df, \"minixdf\"},\n\t{Opt_grpid, \"grpid\"},\n\t{Opt_grpid, \"bsdgroups\"},\n\t{Opt_nogrpid, \"nogrpid\"},\n\t{Opt_nogrpid, \"sysvgroups\"},\n\t{Opt_resgid, \"resgid=%u\"},\n\t{Opt_resuid, \"resuid=%u\"},\n\t{Opt_sb, \"sb=%u\"},\n\t{Opt_err_cont, \"errors=continue\"},\n\t{Opt_err_panic, \"errors=panic\"},\n\t{Opt_err_ro, \"errors=remount-ro\"},\n\t{Opt_nouid32, \"nouid32\"},\n\t{Opt_debug, \"debug\"},\n\t{Opt_removed, \"oldalloc\"},\n\t{Opt_removed, \"orlov\"},\n\t{Opt_user_xattr, \"user_xattr\"},\n\t{Opt_nouser_xattr, \"nouser_xattr\"},\n\t{Opt_acl, \"acl\"},\n\t{Opt_noacl, \"noacl\"},\n\t{Opt_noload, \"norecovery\"},\n\t{Opt_noload, \"noload\"},\n\t{Opt_removed, \"nobh\"},\n\t{Opt_removed, \"bh\"},\n\t{Opt_commit, \"commit=%u\"},\n\t{Opt_min_batch_time, \"min_batch_time=%u\"},\n\t{Opt_max_batch_time, \"max_batch_time=%u\"},\n\t{Opt_journal_dev, \"journal_dev=%u\"},\n\t{Opt_journal_path, \"journal_path=%s\"},\n\t{Opt_journal_checksum, \"journal_checksum\"},\n\t{Opt_nojournal_checksum, \"nojournal_checksum\"},\n\t{Opt_journal_async_commit, \"journal_async_commit\"},\n\t{Opt_abort, \"abort\"},\n\t{Opt_data_journal, \"data=journal\"},\n\t{Opt_data_ordered, \"data=ordered\"},\n\t{Opt_data_writeback, \"data=writeback\"},\n\t{Opt_data_err_abort, \"data_err=abort\"},\n\t{Opt_data_err_ignore, \"data_err=ignore\"},\n\t{Opt_offusrjquota, \"usrjquota=\"},\n\t{Opt_usrjquota, \"usrjquota=%s\"},\n\t{Opt_offgrpjquota, \"grpjquota=\"},\n\t{Opt_grpjquota, \"grpjquota=%s\"},\n\t{Opt_jqfmt_vfsold, \"jqfmt=vfsold\"},\n\t{Opt_jqfmt_vfsv0, \"jqfmt=vfsv0\"},\n\t{Opt_jqfmt_vfsv1, \"jqfmt=vfsv1\"},\n\t{Opt_grpquota, \"grpquota\"},\n\t{Opt_noquota, \"noquota\"},\n\t{Opt_quota, \"quota\"},\n\t{Opt_usrquota, \"usrquota\"},\n\t{Opt_prjquota, \"prjquota\"},\n\t{Opt_barrier, \"barrier=%u\"},\n\t{Opt_barrier, \"barrier\"},\n\t{Opt_nobarrier, \"nobarrier\"},\n\t{Opt_i_version, \"i_version\"},\n\t{Opt_dax, \"dax\"},\n\t{Opt_stripe, \"stripe=%u\"},\n\t{Opt_delalloc, \"delalloc\"},\n\t{Opt_warn_on_error, \"warn_on_error\"},\n\t{Opt_nowarn_on_error, \"nowarn_on_error\"},\n\t{Opt_lazytime, \"lazytime\"},\n\t{Opt_nolazytime, \"nolazytime\"},\n\t{Opt_debug_want_extra_isize, \"debug_want_extra_isize=%u\"},\n\t{Opt_nodelalloc, \"nodelalloc\"},\n\t{Opt_removed, \"mblk_io_submit\"},\n\t{Opt_removed, \"nomblk_io_submit\"},\n\t{Opt_block_validity, \"block_validity\"},\n\t{Opt_noblock_validity, \"noblock_validity\"},\n\t{Opt_inode_readahead_blks, \"inode_readahead_blks=%u\"},\n\t{Opt_journal_ioprio, \"journal_ioprio=%u\"},\n\t{Opt_auto_da_alloc, \"auto_da_alloc=%u\"},\n\t{Opt_auto_da_alloc, \"auto_da_alloc\"},\n\t{Opt_noauto_da_alloc, \"noauto_da_alloc\"},\n\t{Opt_dioread_nolock, \"dioread_nolock\"},\n\t{Opt_dioread_lock, \"dioread_lock\"},\n\t{Opt_discard, \"discard\"},\n\t{Opt_nodiscard, \"nodiscard\"},\n\t{Opt_init_itable, \"init_itable=%u\"},\n\t{Opt_init_itable, \"init_itable\"},\n\t{Opt_noinit_itable, \"noinit_itable\"},\n\t{Opt_max_dir_size_kb, \"max_dir_size_kb=%u\"},\n\t{Opt_test_dummy_encryption, \"test_dummy_encryption\"},\n\t{Opt_nombcache, \"nombcache\"},\n\t{Opt_nombcache, \"no_mbcache\"},\t/* for backward compatibility */\n\t{Opt_removed, \"check=none\"},\t/* mount option from ext2/3 */\n\t{Opt_removed, \"nocheck\"},\t/* mount option from ext2/3 */\n\t{Opt_removed, \"reservation\"},\t/* mount option from ext2/3 */\n\t{Opt_removed, \"noreservation\"}, /* mount option from ext2/3 */\n\t{Opt_removed, \"journal=%u\"},\t/* mount option from ext2/3 */\n\t{Opt_err, NULL},\n};\n\nstatic ext4_fsblk_t get_sb_block(void **data)\n{\n\text4_fsblk_t\tsb_block;\n\tchar\t\t*options = (char *) *data;\n\n\tif (!options || strncmp(options, \"sb=\", 3) != 0)\n\t\treturn 1;\t/* Default location */\n\n\toptions += 3;\n\t/* TODO: use simple_strtoll with >32bit ext4 */\n\tsb_block = simple_strtoul(options, &options, 0);\n\tif (*options && *options != ',') {\n\t\tprintk(KERN_ERR \"EXT4-fs: Invalid sb specification: %s\\n\",\n\t\t       (char *) *data);\n\t\treturn 1;\n\t}\n\tif (*options == ',')\n\t\toptions++;\n\t*data = (void *) options;\n\n\treturn sb_block;\n}\n\n#define DEFAULT_JOURNAL_IOPRIO (IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, 3))\nstatic const char deprecated_msg[] =\n\t\"Mount option \\\"%s\\\" will be removed by %s\\n\"\n\t\"Contact linux-ext4@vger.kernel.org if you think we should keep it.\\n\";\n\n#ifdef CONFIG_QUOTA\nstatic int set_qf_name(struct super_block *sb, int qtype, substring_t *args)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *qname, *old_qname = get_qf_name(sb, sbi, qtype);\n\tint ret = -1;\n\n\tif (sb_any_quota_loaded(sb) && !old_qname) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Cannot change journaled \"\n\t\t\t\"quota options when quota turned on\");\n\t\treturn -1;\n\t}\n\tif (ext4_has_feature_quota(sb)) {\n\t\text4_msg(sb, KERN_INFO, \"Journaled quota options \"\n\t\t\t \"ignored when QUOTA feature is enabled\");\n\t\treturn 1;\n\t}\n\tqname = match_strdup(args);\n\tif (!qname) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Not enough memory for storing quotafile name\");\n\t\treturn -1;\n\t}\n\tif (old_qname) {\n\t\tif (strcmp(old_qname, qname) == 0)\n\t\t\tret = 1;\n\t\telse\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"%s quota file already specified\",\n\t\t\t\t QTYPE2NAME(qtype));\n\t\tgoto errout;\n\t}\n\tif (strchr(qname, '/')) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"quotafile must be on filesystem root\");\n\t\tgoto errout;\n\t}\n\trcu_assign_pointer(sbi->s_qf_names[qtype], qname);\n\tset_opt(sb, QUOTA);\n\treturn 1;\nerrout:\n\tkfree(qname);\n\treturn ret;\n}\n\nstatic int clear_qf_name(struct super_block *sb, int qtype)\n{\n\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *old_qname = get_qf_name(sb, sbi, qtype);\n\n\tif (sb_any_quota_loaded(sb) && old_qname) {\n\t\text4_msg(sb, KERN_ERR, \"Cannot change journaled quota options\"\n\t\t\t\" when quota turned on\");\n\t\treturn -1;\n\t}\n\trcu_assign_pointer(sbi->s_qf_names[qtype], NULL);\n\tsynchronize_rcu();\n\tkfree(old_qname);\n\treturn 1;\n}\n#endif\n\n#define MOPT_SET\t0x0001\n#define MOPT_CLEAR\t0x0002\n#define MOPT_NOSUPPORT\t0x0004\n#define MOPT_EXPLICIT\t0x0008\n#define MOPT_CLEAR_ERR\t0x0010\n#define MOPT_GTE0\t0x0020\n#ifdef CONFIG_QUOTA\n#define MOPT_Q\t\t0\n#define MOPT_QFMT\t0x0040\n#else\n#define MOPT_Q\t\tMOPT_NOSUPPORT\n#define MOPT_QFMT\tMOPT_NOSUPPORT\n#endif\n#define MOPT_DATAJ\t0x0080\n#define MOPT_NO_EXT2\t0x0100\n#define MOPT_NO_EXT3\t0x0200\n#define MOPT_EXT4_ONLY\t(MOPT_NO_EXT2 | MOPT_NO_EXT3)\n#define MOPT_STRING\t0x0400\n\nstatic const struct mount_opts {\n\tint\ttoken;\n\tint\tmount_opt;\n\tint\tflags;\n} ext4_mount_opts[] = {\n\t{Opt_minix_df, EXT4_MOUNT_MINIX_DF, MOPT_SET},\n\t{Opt_bsd_df, EXT4_MOUNT_MINIX_DF, MOPT_CLEAR},\n\t{Opt_grpid, EXT4_MOUNT_GRPID, MOPT_SET},\n\t{Opt_nogrpid, EXT4_MOUNT_GRPID, MOPT_CLEAR},\n\t{Opt_block_validity, EXT4_MOUNT_BLOCK_VALIDITY, MOPT_SET},\n\t{Opt_noblock_validity, EXT4_MOUNT_BLOCK_VALIDITY, MOPT_CLEAR},\n\t{Opt_dioread_nolock, EXT4_MOUNT_DIOREAD_NOLOCK,\n\t MOPT_EXT4_ONLY | MOPT_SET},\n\t{Opt_dioread_lock, EXT4_MOUNT_DIOREAD_NOLOCK,\n\t MOPT_EXT4_ONLY | MOPT_CLEAR},\n\t{Opt_discard, EXT4_MOUNT_DISCARD, MOPT_SET},\n\t{Opt_nodiscard, EXT4_MOUNT_DISCARD, MOPT_CLEAR},\n\t{Opt_delalloc, EXT4_MOUNT_DELALLOC,\n\t MOPT_EXT4_ONLY | MOPT_SET | MOPT_EXPLICIT},\n\t{Opt_nodelalloc, EXT4_MOUNT_DELALLOC,\n\t MOPT_EXT4_ONLY | MOPT_CLEAR},\n\t{Opt_warn_on_error, EXT4_MOUNT_WARN_ON_ERROR, MOPT_SET},\n\t{Opt_nowarn_on_error, EXT4_MOUNT_WARN_ON_ERROR, MOPT_CLEAR},\n\t{Opt_nojournal_checksum, EXT4_MOUNT_JOURNAL_CHECKSUM,\n\t MOPT_EXT4_ONLY | MOPT_CLEAR},\n\t{Opt_journal_checksum, EXT4_MOUNT_JOURNAL_CHECKSUM,\n\t MOPT_EXT4_ONLY | MOPT_SET | MOPT_EXPLICIT},\n\t{Opt_journal_async_commit, (EXT4_MOUNT_JOURNAL_ASYNC_COMMIT |\n\t\t\t\t    EXT4_MOUNT_JOURNAL_CHECKSUM),\n\t MOPT_EXT4_ONLY | MOPT_SET | MOPT_EXPLICIT},\n\t{Opt_noload, EXT4_MOUNT_NOLOAD, MOPT_NO_EXT2 | MOPT_SET},\n\t{Opt_err_panic, EXT4_MOUNT_ERRORS_PANIC, MOPT_SET | MOPT_CLEAR_ERR},\n\t{Opt_err_ro, EXT4_MOUNT_ERRORS_RO, MOPT_SET | MOPT_CLEAR_ERR},\n\t{Opt_err_cont, EXT4_MOUNT_ERRORS_CONT, MOPT_SET | MOPT_CLEAR_ERR},\n\t{Opt_data_err_abort, EXT4_MOUNT_DATA_ERR_ABORT,\n\t MOPT_NO_EXT2},\n\t{Opt_data_err_ignore, EXT4_MOUNT_DATA_ERR_ABORT,\n\t MOPT_NO_EXT2},\n\t{Opt_barrier, EXT4_MOUNT_BARRIER, MOPT_SET},\n\t{Opt_nobarrier, EXT4_MOUNT_BARRIER, MOPT_CLEAR},\n\t{Opt_noauto_da_alloc, EXT4_MOUNT_NO_AUTO_DA_ALLOC, MOPT_SET},\n\t{Opt_auto_da_alloc, EXT4_MOUNT_NO_AUTO_DA_ALLOC, MOPT_CLEAR},\n\t{Opt_noinit_itable, EXT4_MOUNT_INIT_INODE_TABLE, MOPT_CLEAR},\n\t{Opt_commit, 0, MOPT_GTE0},\n\t{Opt_max_batch_time, 0, MOPT_GTE0},\n\t{Opt_min_batch_time, 0, MOPT_GTE0},\n\t{Opt_inode_readahead_blks, 0, MOPT_GTE0},\n\t{Opt_init_itable, 0, MOPT_GTE0},\n\t{Opt_dax, EXT4_MOUNT_DAX, MOPT_SET},\n\t{Opt_stripe, 0, MOPT_GTE0},\n\t{Opt_resuid, 0, MOPT_GTE0},\n\t{Opt_resgid, 0, MOPT_GTE0},\n\t{Opt_journal_dev, 0, MOPT_NO_EXT2 | MOPT_GTE0},\n\t{Opt_journal_path, 0, MOPT_NO_EXT2 | MOPT_STRING},\n\t{Opt_journal_ioprio, 0, MOPT_NO_EXT2 | MOPT_GTE0},\n\t{Opt_data_journal, EXT4_MOUNT_JOURNAL_DATA, MOPT_NO_EXT2 | MOPT_DATAJ},\n\t{Opt_data_ordered, EXT4_MOUNT_ORDERED_DATA, MOPT_NO_EXT2 | MOPT_DATAJ},\n\t{Opt_data_writeback, EXT4_MOUNT_WRITEBACK_DATA,\n\t MOPT_NO_EXT2 | MOPT_DATAJ},\n\t{Opt_user_xattr, EXT4_MOUNT_XATTR_USER, MOPT_SET},\n\t{Opt_nouser_xattr, EXT4_MOUNT_XATTR_USER, MOPT_CLEAR},\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\t{Opt_acl, EXT4_MOUNT_POSIX_ACL, MOPT_SET},\n\t{Opt_noacl, EXT4_MOUNT_POSIX_ACL, MOPT_CLEAR},\n#else\n\t{Opt_acl, 0, MOPT_NOSUPPORT},\n\t{Opt_noacl, 0, MOPT_NOSUPPORT},\n#endif\n\t{Opt_nouid32, EXT4_MOUNT_NO_UID32, MOPT_SET},\n\t{Opt_debug, EXT4_MOUNT_DEBUG, MOPT_SET},\n\t{Opt_debug_want_extra_isize, 0, MOPT_GTE0},\n\t{Opt_quota, EXT4_MOUNT_QUOTA | EXT4_MOUNT_USRQUOTA, MOPT_SET | MOPT_Q},\n\t{Opt_usrquota, EXT4_MOUNT_QUOTA | EXT4_MOUNT_USRQUOTA,\n\t\t\t\t\t\t\tMOPT_SET | MOPT_Q},\n\t{Opt_grpquota, EXT4_MOUNT_QUOTA | EXT4_MOUNT_GRPQUOTA,\n\t\t\t\t\t\t\tMOPT_SET | MOPT_Q},\n\t{Opt_prjquota, EXT4_MOUNT_QUOTA | EXT4_MOUNT_PRJQUOTA,\n\t\t\t\t\t\t\tMOPT_SET | MOPT_Q},\n\t{Opt_noquota, (EXT4_MOUNT_QUOTA | EXT4_MOUNT_USRQUOTA |\n\t\t       EXT4_MOUNT_GRPQUOTA | EXT4_MOUNT_PRJQUOTA),\n\t\t\t\t\t\t\tMOPT_CLEAR | MOPT_Q},\n\t{Opt_usrjquota, 0, MOPT_Q},\n\t{Opt_grpjquota, 0, MOPT_Q},\n\t{Opt_offusrjquota, 0, MOPT_Q},\n\t{Opt_offgrpjquota, 0, MOPT_Q},\n\t{Opt_jqfmt_vfsold, QFMT_VFS_OLD, MOPT_QFMT},\n\t{Opt_jqfmt_vfsv0, QFMT_VFS_V0, MOPT_QFMT},\n\t{Opt_jqfmt_vfsv1, QFMT_VFS_V1, MOPT_QFMT},\n\t{Opt_max_dir_size_kb, 0, MOPT_GTE0},\n\t{Opt_test_dummy_encryption, 0, MOPT_GTE0},\n\t{Opt_nombcache, EXT4_MOUNT_NO_MBCACHE, MOPT_SET},\n\t{Opt_err, 0, 0}\n};\n\n#ifdef CONFIG_UNICODE\nstatic const struct ext4_sb_encodings {\n\t__u16 magic;\n\tchar *name;\n\tchar *version;\n} ext4_sb_encoding_map[] = {\n\t{EXT4_ENC_UTF8_12_1, \"utf8\", \"12.1.0\"},\n};\n\nstatic int ext4_sb_read_encoding(const struct ext4_super_block *es,\n\t\t\t\t const struct ext4_sb_encodings **encoding,\n\t\t\t\t __u16 *flags)\n{\n\t__u16 magic = le16_to_cpu(es->s_encoding);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ext4_sb_encoding_map); i++)\n\t\tif (magic == ext4_sb_encoding_map[i].magic)\n\t\t\tbreak;\n\n\tif (i >= ARRAY_SIZE(ext4_sb_encoding_map))\n\t\treturn -EINVAL;\n\n\t*encoding = &ext4_sb_encoding_map[i];\n\t*flags = le16_to_cpu(es->s_encoding_flags);\n\n\treturn 0;\n}\n#endif\n\nstatic int handle_mount_opt(struct super_block *sb, char *opt, int token,\n\t\t\t    substring_t *args, unsigned long *journal_devnum,\n\t\t\t    unsigned int *journal_ioprio, int is_remount)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tconst struct mount_opts *m;\n\tkuid_t uid;\n\tkgid_t gid;\n\tint arg = 0;\n\n#ifdef CONFIG_QUOTA\n\tif (token == Opt_usrjquota)\n\t\treturn set_qf_name(sb, USRQUOTA, &args[0]);\n\telse if (token == Opt_grpjquota)\n\t\treturn set_qf_name(sb, GRPQUOTA, &args[0]);\n\telse if (token == Opt_offusrjquota)\n\t\treturn clear_qf_name(sb, USRQUOTA);\n\telse if (token == Opt_offgrpjquota)\n\t\treturn clear_qf_name(sb, GRPQUOTA);\n#endif\n\tswitch (token) {\n\tcase Opt_noacl:\n\tcase Opt_nouser_xattr:\n\t\text4_msg(sb, KERN_WARNING, deprecated_msg, opt, \"3.5\");\n\t\tbreak;\n\tcase Opt_sb:\n\t\treturn 1;\t/* handled by get_sb_block() */\n\tcase Opt_removed:\n\t\text4_msg(sb, KERN_WARNING, \"Ignoring removed %s option\", opt);\n\t\treturn 1;\n\tcase Opt_abort:\n\t\tsbi->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\treturn 1;\n\tcase Opt_i_version:\n\t\tsb->s_flags |= SB_I_VERSION;\n\t\treturn 1;\n\tcase Opt_lazytime:\n\t\tsb->s_flags |= SB_LAZYTIME;\n\t\treturn 1;\n\tcase Opt_nolazytime:\n\t\tsb->s_flags &= ~SB_LAZYTIME;\n\t\treturn 1;\n\t}\n\n\tfor (m = ext4_mount_opts; m->token != Opt_err; m++)\n\t\tif (token == m->token)\n\t\t\tbreak;\n\n\tif (m->token == Opt_err) {\n\t\text4_msg(sb, KERN_ERR, \"Unrecognized mount option \\\"%s\\\" \"\n\t\t\t \"or missing value\", opt);\n\t\treturn -1;\n\t}\n\n\tif ((m->flags & MOPT_NO_EXT2) && IS_EXT2_SB(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Mount option \\\"%s\\\" incompatible with ext2\", opt);\n\t\treturn -1;\n\t}\n\tif ((m->flags & MOPT_NO_EXT3) && IS_EXT3_SB(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Mount option \\\"%s\\\" incompatible with ext3\", opt);\n\t\treturn -1;\n\t}\n\n\tif (args->from && !(m->flags & MOPT_STRING) && match_int(args, &arg))\n\t\treturn -1;\n\tif (args->from && (m->flags & MOPT_GTE0) && (arg < 0))\n\t\treturn -1;\n\tif (m->flags & MOPT_EXPLICIT) {\n\t\tif (m->mount_opt & EXT4_MOUNT_DELALLOC) {\n\t\t\tset_opt2(sb, EXPLICIT_DELALLOC);\n\t\t} else if (m->mount_opt & EXT4_MOUNT_JOURNAL_CHECKSUM) {\n\t\t\tset_opt2(sb, EXPLICIT_JOURNAL_CHECKSUM);\n\t\t} else\n\t\t\treturn -1;\n\t}\n\tif (m->flags & MOPT_CLEAR_ERR)\n\t\tclear_opt(sb, ERRORS_MASK);\n\tif (token == Opt_noquota && sb_any_quota_loaded(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"Cannot change quota \"\n\t\t\t \"options when quota turned on\");\n\t\treturn -1;\n\t}\n\n\tif (m->flags & MOPT_NOSUPPORT) {\n\t\text4_msg(sb, KERN_ERR, \"%s option not supported\", opt);\n\t} else if (token == Opt_commit) {\n\t\tif (arg == 0)\n\t\t\targ = JBD2_DEFAULT_MAX_COMMIT_AGE;\n\t\telse if (arg > INT_MAX / HZ) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Invalid commit interval %d, \"\n\t\t\t\t \"must be smaller than %d\",\n\t\t\t\t arg, INT_MAX / HZ);\n\t\t\treturn -1;\n\t\t}\n\t\tsbi->s_commit_interval = HZ * arg;\n\t} else if (token == Opt_debug_want_extra_isize) {\n\t\tsbi->s_want_extra_isize = arg;\n\t} else if (token == Opt_max_batch_time) {\n\t\tsbi->s_max_batch_time = arg;\n\t} else if (token == Opt_min_batch_time) {\n\t\tsbi->s_min_batch_time = arg;\n\t} else if (token == Opt_inode_readahead_blks) {\n\t\tif (arg && (arg > (1 << 30) || !is_power_of_2(arg))) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"EXT4-fs: inode_readahead_blks must be \"\n\t\t\t\t \"0 or a power of 2 smaller than 2^31\");\n\t\t\treturn -1;\n\t\t}\n\t\tsbi->s_inode_readahead_blks = arg;\n\t} else if (token == Opt_init_itable) {\n\t\tset_opt(sb, INIT_INODE_TABLE);\n\t\tif (!args->from)\n\t\t\targ = EXT4_DEF_LI_WAIT_MULT;\n\t\tsbi->s_li_wait_mult = arg;\n\t} else if (token == Opt_max_dir_size_kb) {\n\t\tsbi->s_max_dir_size_kb = arg;\n\t} else if (token == Opt_stripe) {\n\t\tsbi->s_stripe = arg;\n\t} else if (token == Opt_resuid) {\n\t\tuid = make_kuid(current_user_ns(), arg);\n\t\tif (!uid_valid(uid)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Invalid uid value %d\", arg);\n\t\t\treturn -1;\n\t\t}\n\t\tsbi->s_resuid = uid;\n\t} else if (token == Opt_resgid) {\n\t\tgid = make_kgid(current_user_ns(), arg);\n\t\tif (!gid_valid(gid)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Invalid gid value %d\", arg);\n\t\t\treturn -1;\n\t\t}\n\t\tsbi->s_resgid = gid;\n\t} else if (token == Opt_journal_dev) {\n\t\tif (is_remount) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Cannot specify journal on remount\");\n\t\t\treturn -1;\n\t\t}\n\t\t*journal_devnum = arg;\n\t} else if (token == Opt_journal_path) {\n\t\tchar *journal_path;\n\t\tstruct inode *journal_inode;\n\t\tstruct path path;\n\t\tint error;\n\n\t\tif (is_remount) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Cannot specify journal on remount\");\n\t\t\treturn -1;\n\t\t}\n\t\tjournal_path = match_strdup(&args[0]);\n\t\tif (!journal_path) {\n\t\t\text4_msg(sb, KERN_ERR, \"error: could not dup \"\n\t\t\t\t\"journal device string\");\n\t\t\treturn -1;\n\t\t}\n\n\t\terror = kern_path(journal_path, LOOKUP_FOLLOW, &path);\n\t\tif (error) {\n\t\t\text4_msg(sb, KERN_ERR, \"error: could not find \"\n\t\t\t\t\"journal device path: error %d\", error);\n\t\t\tkfree(journal_path);\n\t\t\treturn -1;\n\t\t}\n\n\t\tjournal_inode = d_inode(path.dentry);\n\t\tif (!S_ISBLK(journal_inode->i_mode)) {\n\t\t\text4_msg(sb, KERN_ERR, \"error: journal path %s \"\n\t\t\t\t\"is not a block device\", journal_path);\n\t\t\tpath_put(&path);\n\t\t\tkfree(journal_path);\n\t\t\treturn -1;\n\t\t}\n\n\t\t*journal_devnum = new_encode_dev(journal_inode->i_rdev);\n\t\tpath_put(&path);\n\t\tkfree(journal_path);\n\t} else if (token == Opt_journal_ioprio) {\n\t\tif (arg > 7) {\n\t\t\text4_msg(sb, KERN_ERR, \"Invalid journal IO priority\"\n\t\t\t\t \" (must be 0-7)\");\n\t\t\treturn -1;\n\t\t}\n\t\t*journal_ioprio =\n\t\t\tIOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, arg);\n\t} else if (token == Opt_test_dummy_encryption) {\n#ifdef CONFIG_FS_ENCRYPTION\n\t\tsbi->s_mount_flags |= EXT4_MF_TEST_DUMMY_ENCRYPTION;\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"Test dummy encryption mode enabled\");\n#else\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"Test dummy encryption mount option ignored\");\n#endif\n\t} else if (m->flags & MOPT_DATAJ) {\n\t\tif (is_remount) {\n\t\t\tif (!sbi->s_journal)\n\t\t\t\text4_msg(sb, KERN_WARNING, \"Remounting file system with no journal so ignoring journalled data option\");\n\t\t\telse if (test_opt(sb, DATA_FLAGS) != m->mount_opt) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t \"Cannot change data mode on remount\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t} else {\n\t\t\tclear_opt(sb, DATA_FLAGS);\n\t\t\tsbi->s_mount_opt |= m->mount_opt;\n\t\t}\n#ifdef CONFIG_QUOTA\n\t} else if (m->flags & MOPT_QFMT) {\n\t\tif (sb_any_quota_loaded(sb) &&\n\t\t    sbi->s_jquota_fmt != m->mount_opt) {\n\t\t\text4_msg(sb, KERN_ERR, \"Cannot change journaled \"\n\t\t\t\t \"quota options when quota turned on\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (ext4_has_feature_quota(sb)) {\n\t\t\text4_msg(sb, KERN_INFO,\n\t\t\t\t \"Quota format mount options ignored \"\n\t\t\t\t \"when QUOTA feature is enabled\");\n\t\t\treturn 1;\n\t\t}\n\t\tsbi->s_jquota_fmt = m->mount_opt;\n#endif\n\t} else if (token == Opt_dax) {\n#ifdef CONFIG_FS_DAX\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\"DAX enabled. Warning: EXPERIMENTAL, use at your own risk\");\n\t\tsbi->s_mount_opt |= m->mount_opt;\n#else\n\t\text4_msg(sb, KERN_INFO, \"dax option not supported\");\n\t\treturn -1;\n#endif\n\t} else if (token == Opt_data_err_abort) {\n\t\tsbi->s_mount_opt |= m->mount_opt;\n\t} else if (token == Opt_data_err_ignore) {\n\t\tsbi->s_mount_opt &= ~m->mount_opt;\n\t} else {\n\t\tif (!args->from)\n\t\t\targ = 1;\n\t\tif (m->flags & MOPT_CLEAR)\n\t\t\targ = !arg;\n\t\telse if (unlikely(!(m->flags & MOPT_SET))) {\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"buggy handling of option %s\", opt);\n\t\t\tWARN_ON(1);\n\t\t\treturn -1;\n\t\t}\n\t\tif (arg != 0)\n\t\t\tsbi->s_mount_opt |= m->mount_opt;\n\t\telse\n\t\t\tsbi->s_mount_opt &= ~m->mount_opt;\n\t}\n\treturn 1;\n}\n\nstatic int parse_options(char *options, struct super_block *sb,\n\t\t\t unsigned long *journal_devnum,\n\t\t\t unsigned int *journal_ioprio,\n\t\t\t int is_remount)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *p, __maybe_unused *usr_qf_name, __maybe_unused *grp_qf_name;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint token;\n\n\tif (!options)\n\t\treturn 1;\n\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tif (!*p)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Initialize args struct so we know whether arg was\n\t\t * found; some options take optional arguments.\n\t\t */\n\t\targs[0].to = args[0].from = NULL;\n\t\ttoken = match_token(p, tokens, args);\n\t\tif (handle_mount_opt(sb, p, token, args, journal_devnum,\n\t\t\t\t     journal_ioprio, is_remount) < 0)\n\t\t\treturn 0;\n\t}\n#ifdef CONFIG_QUOTA\n\t/*\n\t * We do the test below only for project quotas. 'usrquota' and\n\t * 'grpquota' mount options are allowed even without quota feature\n\t * to support legacy quotas in quota files.\n\t */\n\tif (test_opt(sb, PRJQUOTA) && !ext4_has_feature_project(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"Project quota feature not enabled. \"\n\t\t\t \"Cannot enable project quota enforcement.\");\n\t\treturn 0;\n\t}\n\tusr_qf_name = get_qf_name(sb, sbi, USRQUOTA);\n\tgrp_qf_name = get_qf_name(sb, sbi, GRPQUOTA);\n\tif (usr_qf_name || grp_qf_name) {\n\t\tif (test_opt(sb, USRQUOTA) && usr_qf_name)\n\t\t\tclear_opt(sb, USRQUOTA);\n\n\t\tif (test_opt(sb, GRPQUOTA) && grp_qf_name)\n\t\t\tclear_opt(sb, GRPQUOTA);\n\n\t\tif (test_opt(sb, GRPQUOTA) || test_opt(sb, USRQUOTA)) {\n\t\t\text4_msg(sb, KERN_ERR, \"old and new quota \"\n\t\t\t\t\t\"format mixing\");\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!sbi->s_jquota_fmt) {\n\t\t\text4_msg(sb, KERN_ERR, \"journaled quota format \"\n\t\t\t\t\t\"not specified\");\n\t\t\treturn 0;\n\t\t}\n\t}\n#endif\n\treturn 1;\n}\n\nstatic inline void ext4_show_quota_options(struct seq_file *seq,\n\t\t\t\t\t   struct super_block *sb)\n{\n#if defined(CONFIG_QUOTA)\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *usr_qf_name, *grp_qf_name;\n\n\tif (sbi->s_jquota_fmt) {\n\t\tchar *fmtname = \"\";\n\n\t\tswitch (sbi->s_jquota_fmt) {\n\t\tcase QFMT_VFS_OLD:\n\t\t\tfmtname = \"vfsold\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V0:\n\t\t\tfmtname = \"vfsv0\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V1:\n\t\t\tfmtname = \"vfsv1\";\n\t\t\tbreak;\n\t\t}\n\t\tseq_printf(seq, \",jqfmt=%s\", fmtname);\n\t}\n\n\trcu_read_lock();\n\tusr_qf_name = rcu_dereference(sbi->s_qf_names[USRQUOTA]);\n\tgrp_qf_name = rcu_dereference(sbi->s_qf_names[GRPQUOTA]);\n\tif (usr_qf_name)\n\t\tseq_show_option(seq, \"usrjquota\", usr_qf_name);\n\tif (grp_qf_name)\n\t\tseq_show_option(seq, \"grpjquota\", grp_qf_name);\n\trcu_read_unlock();\n#endif\n}\n\nstatic const char *token2str(int token)\n{\n\tconst struct match_token *t;\n\n\tfor (t = tokens; t->token != Opt_err; t++)\n\t\tif (t->token == token && !strchr(t->pattern, '='))\n\t\t\tbreak;\n\treturn t->pattern;\n}\n\n/*\n * Show an option if\n *  - it's set to a non-default value OR\n *  - if the per-sb default is different from the global default\n */\nstatic int _ext4_show_options(struct seq_file *seq, struct super_block *sb,\n\t\t\t      int nodefs)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tint def_errors, def_mount_opt = sbi->s_def_mount_opt;\n\tconst struct mount_opts *m;\n\tchar sep = nodefs ? '\\n' : ',';\n\n#define SEQ_OPTS_PUTS(str) seq_printf(seq, \"%c\" str, sep)\n#define SEQ_OPTS_PRINT(str, arg) seq_printf(seq, \"%c\" str, sep, arg)\n\n\tif (sbi->s_sb_block != 1)\n\t\tSEQ_OPTS_PRINT(\"sb=%llu\", sbi->s_sb_block);\n\n\tfor (m = ext4_mount_opts; m->token != Opt_err; m++) {\n\t\tint want_set = m->flags & MOPT_SET;\n\t\tif (((m->flags & (MOPT_SET|MOPT_CLEAR)) == 0) ||\n\t\t    (m->flags & MOPT_CLEAR_ERR))\n\t\t\tcontinue;\n\t\tif (!nodefs && !(m->mount_opt & (sbi->s_mount_opt ^ def_mount_opt)))\n\t\t\tcontinue; /* skip if same as the default */\n\t\tif ((want_set &&\n\t\t     (sbi->s_mount_opt & m->mount_opt) != m->mount_opt) ||\n\t\t    (!want_set && (sbi->s_mount_opt & m->mount_opt)))\n\t\t\tcontinue; /* select Opt_noFoo vs Opt_Foo */\n\t\tSEQ_OPTS_PRINT(\"%s\", token2str(m->token));\n\t}\n\n\tif (nodefs || !uid_eq(sbi->s_resuid, make_kuid(&init_user_ns, EXT4_DEF_RESUID)) ||\n\t    le16_to_cpu(es->s_def_resuid) != EXT4_DEF_RESUID)\n\t\tSEQ_OPTS_PRINT(\"resuid=%u\",\n\t\t\t\tfrom_kuid_munged(&init_user_ns, sbi->s_resuid));\n\tif (nodefs || !gid_eq(sbi->s_resgid, make_kgid(&init_user_ns, EXT4_DEF_RESGID)) ||\n\t    le16_to_cpu(es->s_def_resgid) != EXT4_DEF_RESGID)\n\t\tSEQ_OPTS_PRINT(\"resgid=%u\",\n\t\t\t\tfrom_kgid_munged(&init_user_ns, sbi->s_resgid));\n\tdef_errors = nodefs ? -1 : le16_to_cpu(es->s_errors);\n\tif (test_opt(sb, ERRORS_RO) && def_errors != EXT4_ERRORS_RO)\n\t\tSEQ_OPTS_PUTS(\"errors=remount-ro\");\n\tif (test_opt(sb, ERRORS_CONT) && def_errors != EXT4_ERRORS_CONTINUE)\n\t\tSEQ_OPTS_PUTS(\"errors=continue\");\n\tif (test_opt(sb, ERRORS_PANIC) && def_errors != EXT4_ERRORS_PANIC)\n\t\tSEQ_OPTS_PUTS(\"errors=panic\");\n\tif (nodefs || sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ)\n\t\tSEQ_OPTS_PRINT(\"commit=%lu\", sbi->s_commit_interval / HZ);\n\tif (nodefs || sbi->s_min_batch_time != EXT4_DEF_MIN_BATCH_TIME)\n\t\tSEQ_OPTS_PRINT(\"min_batch_time=%u\", sbi->s_min_batch_time);\n\tif (nodefs || sbi->s_max_batch_time != EXT4_DEF_MAX_BATCH_TIME)\n\t\tSEQ_OPTS_PRINT(\"max_batch_time=%u\", sbi->s_max_batch_time);\n\tif (sb->s_flags & SB_I_VERSION)\n\t\tSEQ_OPTS_PUTS(\"i_version\");\n\tif (nodefs || sbi->s_stripe)\n\t\tSEQ_OPTS_PRINT(\"stripe=%lu\", sbi->s_stripe);\n\tif (nodefs || EXT4_MOUNT_DATA_FLAGS &\n\t\t\t(sbi->s_mount_opt ^ def_mount_opt)) {\n\t\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\t\tSEQ_OPTS_PUTS(\"data=journal\");\n\t\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\t\tSEQ_OPTS_PUTS(\"data=ordered\");\n\t\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA)\n\t\t\tSEQ_OPTS_PUTS(\"data=writeback\");\n\t}\n\tif (nodefs ||\n\t    sbi->s_inode_readahead_blks != EXT4_DEF_INODE_READAHEAD_BLKS)\n\t\tSEQ_OPTS_PRINT(\"inode_readahead_blks=%u\",\n\t\t\t       sbi->s_inode_readahead_blks);\n\n\tif (test_opt(sb, INIT_INODE_TABLE) && (nodefs ||\n\t\t       (sbi->s_li_wait_mult != EXT4_DEF_LI_WAIT_MULT)))\n\t\tSEQ_OPTS_PRINT(\"init_itable=%u\", sbi->s_li_wait_mult);\n\tif (nodefs || sbi->s_max_dir_size_kb)\n\t\tSEQ_OPTS_PRINT(\"max_dir_size_kb=%u\", sbi->s_max_dir_size_kb);\n\tif (test_opt(sb, DATA_ERR_ABORT))\n\t\tSEQ_OPTS_PUTS(\"data_err=abort\");\n\tif (DUMMY_ENCRYPTION_ENABLED(sbi))\n\t\tSEQ_OPTS_PUTS(\"test_dummy_encryption\");\n\n\text4_show_quota_options(seq, sb);\n\treturn 0;\n}\n\nstatic int ext4_show_options(struct seq_file *seq, struct dentry *root)\n{\n\treturn _ext4_show_options(seq, root->d_sb, 0);\n}\n\nint ext4_seq_options_show(struct seq_file *seq, void *offset)\n{\n\tstruct super_block *sb = seq->private;\n\tint rc;\n\n\tseq_puts(seq, sb_rdonly(sb) ? \"ro\" : \"rw\");\n\trc = _ext4_show_options(seq, sb, 1);\n\tseq_puts(seq, \"\\n\");\n\treturn rc;\n}\n\nstatic int ext4_setup_super(struct super_block *sb, struct ext4_super_block *es,\n\t\t\t    int read_only)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tint err = 0;\n\n\tif (le32_to_cpu(es->s_rev_level) > EXT4_MAX_SUPP_REV) {\n\t\text4_msg(sb, KERN_ERR, \"revision level too high, \"\n\t\t\t \"forcing read-only mode\");\n\t\terr = -EROFS;\n\t}\n\tif (read_only)\n\t\tgoto done;\n\tif (!(sbi->s_mount_state & EXT4_VALID_FS))\n\t\text4_msg(sb, KERN_WARNING, \"warning: mounting unchecked fs, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if (sbi->s_mount_state & EXT4_ERROR_FS)\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: mounting fs with errors, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if ((__s16) le16_to_cpu(es->s_max_mnt_count) > 0 &&\n\t\t le16_to_cpu(es->s_mnt_count) >=\n\t\t (unsigned short) (__s16) le16_to_cpu(es->s_max_mnt_count))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: maximal mount count reached, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if (le32_to_cpu(es->s_checkinterval) &&\n\t\t (ext4_get_tstamp(es, s_lastcheck) +\n\t\t  le32_to_cpu(es->s_checkinterval) <= ktime_get_real_seconds()))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: checktime reached, \"\n\t\t\t \"running e2fsck is recommended\");\n\tif (!sbi->s_journal)\n\t\tes->s_state &= cpu_to_le16(~EXT4_VALID_FS);\n\tif (!(__s16) le16_to_cpu(es->s_max_mnt_count))\n\t\tes->s_max_mnt_count = cpu_to_le16(EXT4_DFL_MAX_MNT_COUNT);\n\tle16_add_cpu(&es->s_mnt_count, 1);\n\text4_update_tstamp(es, s_mtime);\n\tif (sbi->s_journal)\n\t\text4_set_feature_journal_needs_recovery(sb);\n\n\terr = ext4_commit_super(sb, 1);\ndone:\n\tif (test_opt(sb, DEBUG))\n\t\tprintk(KERN_INFO \"[EXT4 FS bs=%lu, gc=%u, \"\n\t\t\t\t\"bpg=%lu, ipg=%lu, mo=%04x, mo2=%04x]\\n\",\n\t\t\tsb->s_blocksize,\n\t\t\tsbi->s_groups_count,\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb),\n\t\t\tEXT4_INODES_PER_GROUP(sb),\n\t\t\tsbi->s_mount_opt, sbi->s_mount_opt2);\n\n\tcleancache_init_fs(sb);\n\treturn err;\n}\n\nint ext4_alloc_flex_bg_array(struct super_block *sb, ext4_group_t ngroup)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct flex_groups *new_groups;\n\tint size;\n\n\tif (!sbi->s_log_groups_per_flex)\n\t\treturn 0;\n\n\tsize = ext4_flex_group(sbi, ngroup - 1) + 1;\n\tif (size <= sbi->s_flex_groups_allocated)\n\t\treturn 0;\n\n\tsize = roundup_pow_of_two(size * sizeof(struct flex_groups));\n\tnew_groups = kvzalloc(size, GFP_KERNEL);\n\tif (!new_groups) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %d flex groups\",\n\t\t\t size / (int) sizeof(struct flex_groups));\n\t\treturn -ENOMEM;\n\t}\n\n\tif (sbi->s_flex_groups) {\n\t\tmemcpy(new_groups, sbi->s_flex_groups,\n\t\t       (sbi->s_flex_groups_allocated *\n\t\t\tsizeof(struct flex_groups)));\n\t\tkvfree(sbi->s_flex_groups);\n\t}\n\tsbi->s_flex_groups = new_groups;\n\tsbi->s_flex_groups_allocated = size / sizeof(struct flex_groups);\n\treturn 0;\n}\n\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group;\n\tint i, err;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\n\terr = ext4_alloc_flex_bg_array(sb, sbi->s_groups_count);\n\tif (err)\n\t\tgoto failed;\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic64_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t     &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}\n\nstatic __le16 ext4_group_desc_csum(struct super_block *sb, __u32 block_group,\n\t\t\t\t   struct ext4_group_desc *gdp)\n{\n\tint offset = offsetof(struct ext4_group_desc, bg_checksum);\n\t__u16 crc = 0;\n\t__le32 le_group = cpu_to_le32(block_group);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (ext4_has_metadata_csum(sbi->s_sb)) {\n\t\t/* Use new metadata_csum algorithm */\n\t\t__u32 csum32;\n\t\t__u16 dummy_csum = 0;\n\n\t\tcsum32 = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&le_group,\n\t\t\t\t     sizeof(le_group));\n\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)gdp, offset);\n\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)&dummy_csum,\n\t\t\t\t     sizeof(dummy_csum));\n\t\toffset += sizeof(dummy_csum);\n\t\tif (offset < sbi->s_desc_size)\n\t\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)gdp + offset,\n\t\t\t\t\t     sbi->s_desc_size - offset);\n\n\t\tcrc = csum32 & 0xFFFF;\n\t\tgoto out;\n\t}\n\n\t/* old crc16 code */\n\tif (!ext4_has_feature_gdt_csum(sb))\n\t\treturn 0;\n\n\tcrc = crc16(~0, sbi->s_es->s_uuid, sizeof(sbi->s_es->s_uuid));\n\tcrc = crc16(crc, (__u8 *)&le_group, sizeof(le_group));\n\tcrc = crc16(crc, (__u8 *)gdp, offset);\n\toffset += sizeof(gdp->bg_checksum); /* skip checksum */\n\t/* for checksum of struct ext4_group_desc do the rest...*/\n\tif (ext4_has_feature_64bit(sb) &&\n\t    offset < le16_to_cpu(sbi->s_es->s_desc_size))\n\t\tcrc = crc16(crc, (__u8 *)gdp + offset,\n\t\t\t    le16_to_cpu(sbi->s_es->s_desc_size) -\n\t\t\t\toffset);\n\nout:\n\treturn cpu_to_le16(crc);\n}\n\nint ext4_group_desc_csum_verify(struct super_block *sb, __u32 block_group,\n\t\t\t\tstruct ext4_group_desc *gdp)\n{\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (gdp->bg_checksum != ext4_group_desc_csum(sb, block_group, gdp)))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nvoid ext4_group_desc_csum_set(struct super_block *sb, __u32 block_group,\n\t\t\t      struct ext4_group_desc *gdp)\n{\n\tif (!ext4_has_group_desc_csum(sb))\n\t\treturn;\n\tgdp->bg_checksum = ext4_group_desc_csum(sb, block_group, gdp);\n}\n\n/* Called at mount-time, super-block is locked */\nstatic int ext4_check_descriptors(struct super_block *sb,\n\t\t\t\t  ext4_fsblk_t sb_block,\n\t\t\t\t  ext4_group_t *first_not_zeroed)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n\text4_fsblk_t last_block;\n\text4_fsblk_t last_bg_block = sb_block + ext4_bg_num_gdb(sb, 0);\n\text4_fsblk_t block_bitmap;\n\text4_fsblk_t inode_bitmap;\n\text4_fsblk_t inode_table;\n\tint flexbg_flag = 0;\n\text4_group_t i, grp = sbi->s_groups_count;\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tflexbg_flag = 1;\n\n\text4_debug(\"Checking group descriptors\");\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tif (i == sbi->s_groups_count - 1 || flexbg_flag)\n\t\t\tlast_block = ext4_blocks_count(sbi->s_es) - 1;\n\t\telse\n\t\t\tlast_block = first_block +\n\t\t\t\t(EXT4_BLOCKS_PER_GROUP(sb) - 1);\n\n\t\tif ((grp == sbi->s_groups_count) &&\n\t\t   !(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tgrp = i;\n\n\t\tblock_bitmap = ext4_block_bitmap(sb, gdp);\n\t\tif (block_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap >= sb_block + 1 &&\n\t\t    block_bitmap <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Block bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, block_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_bitmap = ext4_inode_bitmap(sb, gdp);\n\t\tif (inode_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap >= sb_block + 1 &&\n\t\t    inode_bitmap <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_table = ext4_inode_table(sb, gdp);\n\t\tif (inode_table == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table >= sb_block + 1 &&\n\t\t    inode_table <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table < first_block ||\n\t\t    inode_table + sbi->s_itb_per_group - 1 > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode table for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_table);\n\t\t\treturn 0;\n\t\t}\n\t\text4_lock_group(sb, i);\n\t\tif (!ext4_group_desc_csum_verify(sb, i, gdp)) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Checksum for group %u failed (%u!=%u)\",\n\t\t\t\t i, le16_to_cpu(ext4_group_desc_csum(sb, i,\n\t\t\t\t     gdp)), le16_to_cpu(gdp->bg_checksum));\n\t\t\tif (!sb_rdonly(sb)) {\n\t\t\t\text4_unlock_group(sb, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tif (!flexbg_flag)\n\t\t\tfirst_block += EXT4_BLOCKS_PER_GROUP(sb);\n\t}\n\tif (NULL != first_not_zeroed)\n\t\t*first_not_zeroed = grp;\n\treturn 1;\n}\n\n/* ext4_orphan_cleanup() walks a singly-linked list of inodes (starting at\n * the superblock) which were deleted from all directories, but held open by\n * a process at the time of a crash.  We walk the list and try to delete these\n * inodes at recovery time (only with a read-write filesystem).\n *\n * In order to keep the orphan inode chain consistent during traversal (in\n * case of crash during recovery), we link each inode into the superblock\n * orphan list_head and handle it the same way as an inode deletion during\n * normal operation (which journals the operations for us).\n *\n * We only do an iget() and an iput() on each inode, which is very safe if we\n * accidentally point at an in-use or already deleted inode.  The worst that\n * can happen in this case is that we get a \"bit already cleared\" message from\n * ext4_free_inode().  The only reason we would point at a wrong inode is if\n * e2fsck was run on this filesystem, and it must have already done the orphan\n * inode cleanup for us, so we can safely abort without any further action.\n */\nstatic void ext4_orphan_cleanup(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es)\n{\n\tunsigned int s_flags = sb->s_flags;\n\tint ret, nr_orphans = 0, nr_truncates = 0;\n#ifdef CONFIG_QUOTA\n\tint quota_update = 0;\n\tint i;\n#endif\n\tif (!es->s_last_orphan) {\n\t\tjbd_debug(4, \"no orphan inodes to clean up\\n\");\n\t\treturn;\n\t}\n\n\tif (bdev_read_only(sb->s_bdev)) {\n\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\"unavailable, skipping orphan cleanup\");\n\t\treturn;\n\t}\n\n\t/* Check if feature set would not allow a r/w mount */\n\tif (!ext4_feature_set_ok(sb, 0)) {\n\t\text4_msg(sb, KERN_INFO, \"Skipping orphan cleanup due to \"\n\t\t\t \"unknown ROCOMPAT features\");\n\t\treturn;\n\t}\n\n\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\t/* don't clear list on RO mount w/ errors */\n\t\tif (es->s_last_orphan && !(s_flags & SB_RDONLY)) {\n\t\t\text4_msg(sb, KERN_INFO, \"Errors on filesystem, \"\n\t\t\t\t  \"clearing orphan list.\\n\");\n\t\t\tes->s_last_orphan = 0;\n\t\t}\n\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\treturn;\n\t}\n\n\tif (s_flags & SB_RDONLY) {\n\t\text4_msg(sb, KERN_INFO, \"orphan cleanup on readonly fs\");\n\t\tsb->s_flags &= ~SB_RDONLY;\n\t}\n#ifdef CONFIG_QUOTA\n\t/* Needed for iput() to work correctly and not trash data */\n\tsb->s_flags |= SB_ACTIVE;\n\n\t/*\n\t * Turn on quotas which were not enabled for read-only mounts if\n\t * filesystem has quota feature, so that they are updated correctly.\n\t */\n\tif (ext4_has_feature_quota(sb) && (s_flags & SB_RDONLY)) {\n\t\tint ret = ext4_enable_quotas(sb);\n\n\t\tif (!ret)\n\t\t\tquota_update = 1;\n\t\telse\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"Cannot turn on quotas: error %d\", ret);\n\t}\n\n\t/* Turn on journaled quotas used for old sytle */\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\tif (EXT4_SB(sb)->s_qf_names[i]) {\n\t\t\tint ret = ext4_quota_on_mount(sb, i);\n\n\t\t\tif (!ret)\n\t\t\t\tquota_update = 1;\n\t\t\telse\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"Cannot turn on journaled \"\n\t\t\t\t\t\"quota: type %d: error %d\", i, ret);\n\t\t}\n\t}\n#endif\n\n\twhile (es->s_last_orphan) {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * We may have encountered an error during cleanup; if\n\t\t * so, skip the rest.\n\t\t */\n\t\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tinode = ext4_orphan_get(sb, le32_to_cpu(es->s_last_orphan));\n\t\tif (IS_ERR(inode)) {\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add(&EXT4_I(inode)->i_orphan, &EXT4_SB(sb)->s_orphan);\n\t\tdquot_initialize(inode);\n\t\tif (inode->i_nlink) {\n\t\t\tif (test_opt(sb, DEBUG))\n\t\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\t\"%s: truncating inode %lu to %lld bytes\",\n\t\t\t\t\t__func__, inode->i_ino, inode->i_size);\n\t\t\tjbd_debug(2, \"truncating inode %lu to %lld bytes\\n\",\n\t\t\t\t  inode->i_ino, inode->i_size);\n\t\t\tinode_lock(inode);\n\t\t\ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n\t\t\tret = ext4_truncate(inode);\n\t\t\tif (ret)\n\t\t\t\text4_std_error(inode->i_sb, ret);\n\t\t\tinode_unlock(inode);\n\t\t\tnr_truncates++;\n\t\t} else {\n\t\t\tif (test_opt(sb, DEBUG))\n\t\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\t\"%s: deleting unreferenced inode %lu\",\n\t\t\t\t\t__func__, inode->i_ino);\n\t\t\tjbd_debug(2, \"deleting unreferenced inode %lu\\n\",\n\t\t\t\t  inode->i_ino);\n\t\t\tnr_orphans++;\n\t\t}\n\t\tiput(inode);  /* The delete magic happens here! */\n\t}\n\n#define PLURAL(x) (x), ((x) == 1) ? \"\" : \"s\"\n\n\tif (nr_orphans)\n\t\text4_msg(sb, KERN_INFO, \"%d orphan inode%s deleted\",\n\t\t       PLURAL(nr_orphans));\n\tif (nr_truncates)\n\t\text4_msg(sb, KERN_INFO, \"%d truncate%s cleaned up\",\n\t\t       PLURAL(nr_truncates));\n#ifdef CONFIG_QUOTA\n\t/* Turn off quotas if they were enabled for orphan cleanup */\n\tif (quota_update) {\n\t\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\t\tif (sb_dqopt(sb)->files[i])\n\t\t\t\tdquot_quota_off(sb, i);\n\t\t}\n\t}\n#endif\n\tsb->s_flags = s_flags; /* Restore SB_RDONLY status */\n}\n\n/*\n * Maximal extent format file size.\n * Resulting logical blkno at s_maxbytes must fit in our on-disk\n * extent format containers, within a sector_t, and within i_blocks\n * in the vfs.  ext4 inode has 48 bits of i_block in fsblock units,\n * so that won't be a limiting factor.\n *\n * However there is other limiting factor. We do store extents in the form\n * of starting block and length, hence the resulting length of the extent\n * covering maximum file size must fit into on-disk format containers as\n * well. Given that length is always by 1 unit bigger than max unit (because\n * we count 0 as well) we have to lower the s_maxbytes by one fs block.\n *\n * Note, this does *not* consider any metadata overhead for vfs i_blocks.\n */\nstatic loff_t ext4_max_size(int blkbits, int has_huge_files)\n{\n\tloff_t res;\n\tloff_t upper_limit = MAX_LFS_FILESIZE;\n\n\tBUILD_BUG_ON(sizeof(blkcnt_t) < sizeof(u64));\n\n\tif (!has_huge_files) {\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (blkbits - 9);\n\t\tupper_limit <<= blkbits;\n\t}\n\n\t/*\n\t * 32-bit extent-start container, ee_block. We lower the maxbytes\n\t * by one fs block, so ee_len can cover the extent of maximum file\n\t * size\n\t */\n\tres = (1LL << 32) - 1;\n\tres <<= blkbits;\n\n\t/* Sanity check against vm- & vfs- imposed limits */\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\treturn res;\n}\n\n/*\n * Maximal bitmap file size.  There is a direct, and {,double-,triple-}indirect\n * block limit, and also a limit of (2^48 - 1) 512-byte sectors in i_blocks.\n * We need to be 1 filesystem block less than the 2^48 sector limit.\n */\nstatic loff_t ext4_max_bitmap_size(int bits, int has_huge_files)\n{\n\tloff_t res = EXT4_NDIR_BLOCKS;\n\tint meta_blocks;\n\tloff_t upper_limit;\n\t/* This is calculated to be the largest file size for a dense, block\n\t * mapped file such that the file's total number of 512-byte sectors,\n\t * including data and all indirect blocks, does not exceed (2^48 - 1).\n\t *\n\t * __u32 i_blocks_lo and _u16 i_blocks_high represent the total\n\t * number of 512-byte sectors of the file.\n\t */\n\n\tif (!has_huge_files) {\n\t\t/*\n\t\t * !has_huge_files or implies that the inode i_block field\n\t\t * represents total file blocks in 2^32 512-byte sectors ==\n\t\t * size of vfs inode i_blocks * 8\n\t\t */\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (bits - 9);\n\n\t} else {\n\t\t/*\n\t\t * We use 48 bit ext4_inode i_blocks\n\t\t * With EXT4_HUGE_FILE_FL set the i_blocks\n\t\t * represent total number of blocks in\n\t\t * file system block size\n\t\t */\n\t\tupper_limit = (1LL << 48) - 1;\n\n\t}\n\n\t/* indirect blocks */\n\tmeta_blocks = 1;\n\t/* double indirect blocks */\n\tmeta_blocks += 1 + (1LL << (bits-2));\n\t/* tripple indirect blocks */\n\tmeta_blocks += 1 + (1LL << (bits-2)) + (1LL << (2*(bits-2)));\n\n\tupper_limit -= meta_blocks;\n\tupper_limit <<= bits;\n\n\tres += 1LL << (bits-2);\n\tres += 1LL << (2*(bits-2));\n\tres += 1LL << (3*(bits-2));\n\tres <<= bits;\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\tif (res > MAX_LFS_FILESIZE)\n\t\tres = MAX_LFS_FILESIZE;\n\n\treturn res;\n}\n\nstatic ext4_fsblk_t descriptor_loc(struct super_block *sb,\n\t\t\t\t   ext4_fsblk_t logical_sb_block, int nr)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_group_t bg, first_meta_bg;\n\tint has_super = 0;\n\n\tfirst_meta_bg = le32_to_cpu(sbi->s_es->s_first_meta_bg);\n\n\tif (!ext4_has_feature_meta_bg(sb) || nr < first_meta_bg)\n\t\treturn logical_sb_block + nr + 1;\n\tbg = sbi->s_desc_per_block * nr;\n\tif (ext4_bg_has_super(sb, bg))\n\t\thas_super = 1;\n\n\t/*\n\t * If we have a meta_bg fs with 1k blocks, group 0's GDT is at\n\t * block 2, not 1.  If s_first_data_block == 0 (bigalloc is enabled\n\t * on modern mke2fs or blksize > 1k on older mke2fs) then we must\n\t * compensate.\n\t */\n\tif (sb->s_blocksize == 1024 && nr == 0 &&\n\t    le32_to_cpu(sbi->s_es->s_first_data_block) == 0)\n\t\thas_super++;\n\n\treturn (has_super + ext4_group_first_block_no(sb, bg));\n}\n\n/**\n * ext4_get_stripe_size: Get the stripe size.\n * @sbi: In memory super block info\n *\n * If we have specified it via mount option, then\n * use the mount option value. If the value specified at mount time is\n * greater than the blocks per group use the super block value.\n * If the super block value is greater than blocks per group return 0.\n * Allocator needs it be less than blocks per group.\n *\n */\nstatic unsigned long ext4_get_stripe_size(struct ext4_sb_info *sbi)\n{\n\tunsigned long stride = le16_to_cpu(sbi->s_es->s_raid_stride);\n\tunsigned long stripe_width =\n\t\t\tle32_to_cpu(sbi->s_es->s_raid_stripe_width);\n\tint ret;\n\n\tif (sbi->s_stripe && sbi->s_stripe <= sbi->s_blocks_per_group)\n\t\tret = sbi->s_stripe;\n\telse if (stripe_width && stripe_width <= sbi->s_blocks_per_group)\n\t\tret = stripe_width;\n\telse if (stride && stride <= sbi->s_blocks_per_group)\n\t\tret = stride;\n\telse\n\t\tret = 0;\n\n\t/*\n\t * If the stripe width is 1, this makes no sense and\n\t * we set it to 0 to turn off stripe handling code.\n\t */\n\tif (ret <= 1)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * Check whether this filesystem can be mounted based on\n * the features present and the RDONLY/RDWR mount requested.\n * Returns 1 if this filesystem can be mounted as requested,\n * 0 if it cannot be.\n */\nstatic int ext4_feature_set_ok(struct super_block *sb, int readonly)\n{\n\tif (ext4_has_unknown_ext4_incompat_features(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Couldn't mount because of \"\n\t\t\t\"unsupported optional features (%x)\",\n\t\t\t(le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_incompat) &\n\t\t\t~EXT4_FEATURE_INCOMPAT_SUPP));\n\t\treturn 0;\n\t}\n\n#ifndef CONFIG_UNICODE\n\tif (ext4_has_feature_casefold(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with casefold feature cannot be \"\n\t\t\t \"mounted without CONFIG_UNICODE\");\n\t\treturn 0;\n\t}\n#endif\n\n\tif (readonly)\n\t\treturn 1;\n\n\tif (ext4_has_feature_readonly(sb)) {\n\t\text4_msg(sb, KERN_INFO, \"filesystem is read-only\");\n\t\tsb->s_flags |= SB_RDONLY;\n\t\treturn 1;\n\t}\n\n\t/* Check that feature set is OK for a read-write mount */\n\tif (ext4_has_unknown_ext4_ro_compat_features(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't mount RDWR because of \"\n\t\t\t \"unsupported optional features (%x)\",\n\t\t\t (le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_ro_compat) &\n\t\t\t\t~EXT4_FEATURE_RO_COMPAT_SUPP));\n\t\treturn 0;\n\t}\n\tif (ext4_has_feature_bigalloc(sb) && !ext4_has_feature_extents(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Can't support bigalloc feature without \"\n\t\t\t \"extents feature\\n\");\n\t\treturn 0;\n\t}\n\n#ifndef CONFIG_QUOTA\n\tif (ext4_has_feature_quota(sb) && !readonly) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with quota feature cannot be mounted RDWR \"\n\t\t\t \"without CONFIG_QUOTA\");\n\t\treturn 0;\n\t}\n\tif (ext4_has_feature_project(sb) && !readonly) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with project quota feature cannot be mounted RDWR \"\n\t\t\t \"without CONFIG_QUOTA\");\n\t\treturn 0;\n\t}\n#endif  /* CONFIG_QUOTA */\n\treturn 1;\n}\n\n/*\n * This function is called once a day if we have errors logged\n * on the file system\n */\nstatic void print_daily_error_info(struct timer_list *t)\n{\n\tstruct ext4_sb_info *sbi = from_timer(sbi, t, s_err_report);\n\tstruct super_block *sb = sbi->s_sb;\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\tif (es->s_error_count)\n\t\t/* fsck newer than v1.41.13 is needed to clean this condition. */\n\t\text4_msg(sb, KERN_NOTICE, \"error count since last fsck: %u\",\n\t\t\t le32_to_cpu(es->s_error_count));\n\tif (es->s_first_error_time) {\n\t\tprintk(KERN_NOTICE \"EXT4-fs (%s): initial error at time %llu: %.*s:%d\",\n\t\t       sb->s_id,\n\t\t       ext4_get_tstamp(es, s_first_error_time),\n\t\t       (int) sizeof(es->s_first_error_func),\n\t\t       es->s_first_error_func,\n\t\t       le32_to_cpu(es->s_first_error_line));\n\t\tif (es->s_first_error_ino)\n\t\t\tprintk(KERN_CONT \": inode %u\",\n\t\t\t       le32_to_cpu(es->s_first_error_ino));\n\t\tif (es->s_first_error_block)\n\t\t\tprintk(KERN_CONT \": block %llu\", (unsigned long long)\n\t\t\t       le64_to_cpu(es->s_first_error_block));\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tif (es->s_last_error_time) {\n\t\tprintk(KERN_NOTICE \"EXT4-fs (%s): last error at time %llu: %.*s:%d\",\n\t\t       sb->s_id,\n\t\t       ext4_get_tstamp(es, s_last_error_time),\n\t\t       (int) sizeof(es->s_last_error_func),\n\t\t       es->s_last_error_func,\n\t\t       le32_to_cpu(es->s_last_error_line));\n\t\tif (es->s_last_error_ino)\n\t\t\tprintk(KERN_CONT \": inode %u\",\n\t\t\t       le32_to_cpu(es->s_last_error_ino));\n\t\tif (es->s_last_error_block)\n\t\t\tprintk(KERN_CONT \": block %llu\", (unsigned long long)\n\t\t\t       le64_to_cpu(es->s_last_error_block));\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tmod_timer(&sbi->s_err_report, jiffies + 24*60*60*HZ);  /* Once a day */\n}\n\n/* Find next suitable group and run ext4_init_inode_table */\nstatic int ext4_run_li_request(struct ext4_li_request *elr)\n{\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t group, ngroups;\n\tstruct super_block *sb;\n\tunsigned long timeout = 0;\n\tint ret = 0;\n\n\tsb = elr->lr_super;\n\tngroups = EXT4_SB(sb)->s_groups_count;\n\n\tfor (group = elr->lr_next_group; group < ngroups; group++) {\n\t\tgdp = ext4_get_group_desc(sb, group, NULL);\n\t\tif (!gdp) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tbreak;\n\t}\n\n\tif (group >= ngroups)\n\t\tret = 1;\n\n\tif (!ret) {\n\t\ttimeout = jiffies;\n\t\tret = ext4_init_inode_table(sb, group,\n\t\t\t\t\t    elr->lr_timeout ? 0 : 1);\n\t\tif (elr->lr_timeout == 0) {\n\t\t\ttimeout = (jiffies - timeout) *\n\t\t\t\t  elr->lr_sbi->s_li_wait_mult;\n\t\t\telr->lr_timeout = timeout;\n\t\t}\n\t\telr->lr_next_sched = jiffies + elr->lr_timeout;\n\t\telr->lr_next_group = group + 1;\n\t}\n\treturn ret;\n}\n\n/*\n * Remove lr_request from the list_request and free the\n * request structure. Should be called with li_list_mtx held\n */\nstatic void ext4_remove_li_request(struct ext4_li_request *elr)\n{\n\tstruct ext4_sb_info *sbi;\n\n\tif (!elr)\n\t\treturn;\n\n\tsbi = elr->lr_sbi;\n\n\tlist_del(&elr->lr_request);\n\tsbi->s_li_request = NULL;\n\tkfree(elr);\n}\n\nstatic void ext4_unregister_li_request(struct super_block *sb)\n{\n\tmutex_lock(&ext4_li_mtx);\n\tif (!ext4_li_info) {\n\t\tmutex_unlock(&ext4_li_mtx);\n\t\treturn;\n\t}\n\n\tmutex_lock(&ext4_li_info->li_list_mtx);\n\text4_remove_li_request(EXT4_SB(sb)->s_li_request);\n\tmutex_unlock(&ext4_li_info->li_list_mtx);\n\tmutex_unlock(&ext4_li_mtx);\n}\n\nstatic struct task_struct *ext4_lazyinit_task;\n\n/*\n * This is the function where ext4lazyinit thread lives. It walks\n * through the request list searching for next scheduled filesystem.\n * When such a fs is found, run the lazy initialization request\n * (ext4_rn_li_request) and keep track of the time spend in this\n * function. Based on that time we compute next schedule time of\n * the request. When walking through the list is complete, compute\n * next waking time and put itself into sleep.\n */\nstatic int ext4_lazyinit_thread(void *arg)\n{\n\tstruct ext4_lazy_init *eli = (struct ext4_lazy_init *)arg;\n\tstruct list_head *pos, *n;\n\tstruct ext4_li_request *elr;\n\tunsigned long next_wakeup, cur;\n\n\tBUG_ON(NULL == eli);\n\ncont_thread:\n\twhile (true) {\n\t\tnext_wakeup = MAX_JIFFY_OFFSET;\n\n\t\tmutex_lock(&eli->li_list_mtx);\n\t\tif (list_empty(&eli->li_request_list)) {\n\t\t\tmutex_unlock(&eli->li_list_mtx);\n\t\t\tgoto exit_thread;\n\t\t}\n\t\tlist_for_each_safe(pos, n, &eli->li_request_list) {\n\t\t\tint err = 0;\n\t\t\tint progress = 0;\n\t\t\telr = list_entry(pos, struct ext4_li_request,\n\t\t\t\t\t lr_request);\n\n\t\t\tif (time_before(jiffies, elr->lr_next_sched)) {\n\t\t\t\tif (time_before(elr->lr_next_sched, next_wakeup))\n\t\t\t\t\tnext_wakeup = elr->lr_next_sched;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (down_read_trylock(&elr->lr_super->s_umount)) {\n\t\t\t\tif (sb_start_write_trylock(elr->lr_super)) {\n\t\t\t\t\tprogress = 1;\n\t\t\t\t\t/*\n\t\t\t\t\t * We hold sb->s_umount, sb can not\n\t\t\t\t\t * be removed from the list, it is\n\t\t\t\t\t * now safe to drop li_list_mtx\n\t\t\t\t\t */\n\t\t\t\t\tmutex_unlock(&eli->li_list_mtx);\n\t\t\t\t\terr = ext4_run_li_request(elr);\n\t\t\t\t\tsb_end_write(elr->lr_super);\n\t\t\t\t\tmutex_lock(&eli->li_list_mtx);\n\t\t\t\t\tn = pos->next;\n\t\t\t\t}\n\t\t\t\tup_read((&elr->lr_super->s_umount));\n\t\t\t}\n\t\t\t/* error, remove the lazy_init job */\n\t\t\tif (err) {\n\t\t\t\text4_remove_li_request(elr);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!progress) {\n\t\t\t\telr->lr_next_sched = jiffies +\n\t\t\t\t\t(prandom_u32()\n\t\t\t\t\t % (EXT4_DEF_LI_MAX_START_DELAY * HZ));\n\t\t\t}\n\t\t\tif (time_before(elr->lr_next_sched, next_wakeup))\n\t\t\t\tnext_wakeup = elr->lr_next_sched;\n\t\t}\n\t\tmutex_unlock(&eli->li_list_mtx);\n\n\t\ttry_to_freeze();\n\n\t\tcur = jiffies;\n\t\tif ((time_after_eq(cur, next_wakeup)) ||\n\t\t    (MAX_JIFFY_OFFSET == next_wakeup)) {\n\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t}\n\n\t\tschedule_timeout_interruptible(next_wakeup - cur);\n\n\t\tif (kthread_should_stop()) {\n\t\t\text4_clear_request_list();\n\t\t\tgoto exit_thread;\n\t\t}\n\t}\n\nexit_thread:\n\t/*\n\t * It looks like the request list is empty, but we need\n\t * to check it under the li_list_mtx lock, to prevent any\n\t * additions into it, and of course we should lock ext4_li_mtx\n\t * to atomically free the list and ext4_li_info, because at\n\t * this point another ext4 filesystem could be registering\n\t * new one.\n\t */\n\tmutex_lock(&ext4_li_mtx);\n\tmutex_lock(&eli->li_list_mtx);\n\tif (!list_empty(&eli->li_request_list)) {\n\t\tmutex_unlock(&eli->li_list_mtx);\n\t\tmutex_unlock(&ext4_li_mtx);\n\t\tgoto cont_thread;\n\t}\n\tmutex_unlock(&eli->li_list_mtx);\n\tkfree(ext4_li_info);\n\text4_li_info = NULL;\n\tmutex_unlock(&ext4_li_mtx);\n\n\treturn 0;\n}\n\nstatic void ext4_clear_request_list(void)\n{\n\tstruct list_head *pos, *n;\n\tstruct ext4_li_request *elr;\n\n\tmutex_lock(&ext4_li_info->li_list_mtx);\n\tlist_for_each_safe(pos, n, &ext4_li_info->li_request_list) {\n\t\telr = list_entry(pos, struct ext4_li_request,\n\t\t\t\t lr_request);\n\t\text4_remove_li_request(elr);\n\t}\n\tmutex_unlock(&ext4_li_info->li_list_mtx);\n}\n\nstatic int ext4_run_lazyinit_thread(void)\n{\n\text4_lazyinit_task = kthread_run(ext4_lazyinit_thread,\n\t\t\t\t\t ext4_li_info, \"ext4lazyinit\");\n\tif (IS_ERR(ext4_lazyinit_task)) {\n\t\tint err = PTR_ERR(ext4_lazyinit_task);\n\t\text4_clear_request_list();\n\t\tkfree(ext4_li_info);\n\t\text4_li_info = NULL;\n\t\tprintk(KERN_CRIT \"EXT4-fs: error %d creating inode table \"\n\t\t\t\t \"initialization thread\\n\",\n\t\t\t\t err);\n\t\treturn err;\n\t}\n\text4_li_info->li_state |= EXT4_LAZYINIT_RUNNING;\n\treturn 0;\n}\n\n/*\n * Check whether it make sense to run itable init. thread or not.\n * If there is at least one uninitialized inode table, return\n * corresponding group number, else the loop goes through all\n * groups and return total number of groups.\n */\nstatic ext4_group_t ext4_has_uninit_itable(struct super_block *sb)\n{\n\text4_group_t group, ngroups = EXT4_SB(sb)->s_groups_count;\n\tstruct ext4_group_desc *gdp = NULL;\n\n\tif (!ext4_has_group_desc_csum(sb))\n\t\treturn ngroups;\n\n\tfor (group = 0; group < ngroups; group++) {\n\t\tgdp = ext4_get_group_desc(sb, group, NULL);\n\t\tif (!gdp)\n\t\t\tcontinue;\n\n\t\tif (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tbreak;\n\t}\n\n\treturn group;\n}\n\nstatic int ext4_li_info_new(void)\n{\n\tstruct ext4_lazy_init *eli = NULL;\n\n\teli = kzalloc(sizeof(*eli), GFP_KERNEL);\n\tif (!eli)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&eli->li_request_list);\n\tmutex_init(&eli->li_list_mtx);\n\n\teli->li_state |= EXT4_LAZYINIT_QUIT;\n\n\text4_li_info = eli;\n\n\treturn 0;\n}\n\nstatic struct ext4_li_request *ext4_li_request_new(struct super_block *sb,\n\t\t\t\t\t    ext4_group_t start)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_li_request *elr;\n\n\telr = kzalloc(sizeof(*elr), GFP_KERNEL);\n\tif (!elr)\n\t\treturn NULL;\n\n\telr->lr_super = sb;\n\telr->lr_sbi = sbi;\n\telr->lr_next_group = start;\n\n\t/*\n\t * Randomize first schedule time of the request to\n\t * spread the inode table initialization requests\n\t * better.\n\t */\n\telr->lr_next_sched = jiffies + (prandom_u32() %\n\t\t\t\t(EXT4_DEF_LI_MAX_START_DELAY * HZ));\n\treturn elr;\n}\n\nint ext4_register_li_request(struct super_block *sb,\n\t\t\t     ext4_group_t first_not_zeroed)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_li_request *elr = NULL;\n\text4_group_t ngroups = sbi->s_groups_count;\n\tint ret = 0;\n\n\tmutex_lock(&ext4_li_mtx);\n\tif (sbi->s_li_request != NULL) {\n\t\t/*\n\t\t * Reset timeout so it can be computed again, because\n\t\t * s_li_wait_mult might have changed.\n\t\t */\n\t\tsbi->s_li_request->lr_timeout = 0;\n\t\tgoto out;\n\t}\n\n\tif (first_not_zeroed == ngroups || sb_rdonly(sb) ||\n\t    !test_opt(sb, INIT_INODE_TABLE))\n\t\tgoto out;\n\n\telr = ext4_li_request_new(sb, first_not_zeroed);\n\tif (!elr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (NULL == ext4_li_info) {\n\t\tret = ext4_li_info_new();\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tmutex_lock(&ext4_li_info->li_list_mtx);\n\tlist_add(&elr->lr_request, &ext4_li_info->li_request_list);\n\tmutex_unlock(&ext4_li_info->li_list_mtx);\n\n\tsbi->s_li_request = elr;\n\t/*\n\t * set elr to NULL here since it has been inserted to\n\t * the request_list and the removal and free of it is\n\t * handled by ext4_clear_request_list from now on.\n\t */\n\telr = NULL;\n\n\tif (!(ext4_li_info->li_state & EXT4_LAZYINIT_RUNNING)) {\n\t\tret = ext4_run_lazyinit_thread();\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\nout:\n\tmutex_unlock(&ext4_li_mtx);\n\tif (ret)\n\t\tkfree(elr);\n\treturn ret;\n}\n\n/*\n * We do not need to lock anything since this is called on\n * module unload.\n */\nstatic void ext4_destroy_lazyinit_thread(void)\n{\n\t/*\n\t * If thread exited earlier\n\t * there's nothing to be done.\n\t */\n\tif (!ext4_li_info || !ext4_lazyinit_task)\n\t\treturn;\n\n\tkthread_stop(ext4_lazyinit_task);\n}\n\nstatic int set_journal_csum_feature_set(struct super_block *sb)\n{\n\tint ret = 1;\n\tint compat, incompat;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (ext4_has_metadata_csum(sb)) {\n\t\t/* journal checksum v3 */\n\t\tcompat = 0;\n\t\tincompat = JBD2_FEATURE_INCOMPAT_CSUM_V3;\n\t} else {\n\t\t/* journal checksum v1 */\n\t\tcompat = JBD2_FEATURE_COMPAT_CHECKSUM;\n\t\tincompat = 0;\n\t}\n\n\tjbd2_journal_clear_features(sbi->s_journal,\n\t\t\tJBD2_FEATURE_COMPAT_CHECKSUM, 0,\n\t\t\tJBD2_FEATURE_INCOMPAT_CSUM_V3 |\n\t\t\tJBD2_FEATURE_INCOMPAT_CSUM_V2);\n\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\tret = jbd2_journal_set_features(sbi->s_journal,\n\t\t\t\tcompat, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT |\n\t\t\t\tincompat);\n\t} else if (test_opt(sb, JOURNAL_CHECKSUM)) {\n\t\tret = jbd2_journal_set_features(sbi->s_journal,\n\t\t\t\tcompat, 0,\n\t\t\t\tincompat);\n\t\tjbd2_journal_clear_features(sbi->s_journal, 0, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t} else {\n\t\tjbd2_journal_clear_features(sbi->s_journal, 0, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Note: calculating the overhead so we can be compatible with\n * historical BSD practice is quite difficult in the face of\n * clusters/bigalloc.  This is because multiple metadata blocks from\n * different block group can end up in the same allocation cluster.\n * Calculating the exact overhead in the face of clustered allocation\n * requires either O(all block bitmaps) in memory or O(number of block\n * groups**2) in time.  We will still calculate the superblock for\n * older file systems --- and if we come across with a bigalloc file\n * system with zero in s_overhead_clusters the estimate will be close to\n * correct especially for very large cluster sizes --- but for newer\n * file systems, it's better to calculate this figure once at mkfs\n * time, and store it in the superblock.  If the superblock value is\n * present (even for non-bigalloc file systems), we will use it.\n */\nstatic int count_overhead(struct super_block *sb, ext4_group_t grp,\n\t\t\t  char *buf)\n{\n\tstruct ext4_sb_info\t*sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc\t*gdp;\n\text4_fsblk_t\t\tfirst_block, last_block, b;\n\text4_group_t\t\ti, ngroups = ext4_get_groups_count(sb);\n\tint\t\t\ts, j, count = 0;\n\n\tif (!ext4_has_feature_bigalloc(sb))\n\t\treturn (ext4_bg_has_super(sb, grp) + ext4_bg_num_gdb(sb, grp) +\n\t\t\tsbi->s_itb_per_group + 2);\n\n\tfirst_block = le32_to_cpu(sbi->s_es->s_first_data_block) +\n\t\t(grp * EXT4_BLOCKS_PER_GROUP(sb));\n\tlast_block = first_block + EXT4_BLOCKS_PER_GROUP(sb) - 1;\n\tfor (i = 0; i < ngroups; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\t\tb = ext4_block_bitmap(sb, gdp);\n\t\tif (b >= first_block && b <= last_block) {\n\t\t\text4_set_bit(EXT4_B2C(sbi, b - first_block), buf);\n\t\t\tcount++;\n\t\t}\n\t\tb = ext4_inode_bitmap(sb, gdp);\n\t\tif (b >= first_block && b <= last_block) {\n\t\t\text4_set_bit(EXT4_B2C(sbi, b - first_block), buf);\n\t\t\tcount++;\n\t\t}\n\t\tb = ext4_inode_table(sb, gdp);\n\t\tif (b >= first_block && b + sbi->s_itb_per_group <= last_block)\n\t\t\tfor (j = 0; j < sbi->s_itb_per_group; j++, b++) {\n\t\t\t\tint c = EXT4_B2C(sbi, b - first_block);\n\t\t\t\text4_set_bit(c, buf);\n\t\t\t\tcount++;\n\t\t\t}\n\t\tif (i != grp)\n\t\t\tcontinue;\n\t\ts = 0;\n\t\tif (ext4_bg_has_super(sb, grp)) {\n\t\t\text4_set_bit(s++, buf);\n\t\t\tcount++;\n\t\t}\n\t\tj = ext4_bg_num_gdb(sb, grp);\n\t\tif (s + j > EXT4_BLOCKS_PER_GROUP(sb)) {\n\t\t\text4_error(sb, \"Invalid number of block group \"\n\t\t\t\t   \"descriptor blocks: %d\", j);\n\t\t\tj = EXT4_BLOCKS_PER_GROUP(sb) - s;\n\t\t}\n\t\tcount += j;\n\t\tfor (; j > 0; j--)\n\t\t\text4_set_bit(EXT4_B2C(sbi, s++), buf);\n\t}\n\tif (!count)\n\t\treturn 0;\n\treturn EXT4_CLUSTERS_PER_GROUP(sb) -\n\t\text4_count_free(buf, EXT4_CLUSTERS_PER_GROUP(sb) / 8);\n}\n\n/*\n * Compute the overhead and stash it in sbi->s_overhead\n */\nint ext4_calculate_overhead(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tstruct inode *j_inode;\n\tunsigned int j_blocks, j_inum = le32_to_cpu(es->s_journal_inum);\n\text4_group_t i, ngroups = ext4_get_groups_count(sb);\n\text4_fsblk_t overhead = 0;\n\tchar *buf = (char *) get_zeroed_page(GFP_NOFS);\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Compute the overhead (FS structures).  This is constant\n\t * for a given filesystem unless the number of block groups\n\t * changes so we cache the previous value until it does.\n\t */\n\n\t/*\n\t * All of the blocks before first_data_block are overhead\n\t */\n\toverhead = EXT4_B2C(sbi, le32_to_cpu(es->s_first_data_block));\n\n\t/*\n\t * Add the overhead found in each block group\n\t */\n\tfor (i = 0; i < ngroups; i++) {\n\t\tint blks;\n\n\t\tblks = count_overhead(sb, i, buf);\n\t\toverhead += blks;\n\t\tif (blks)\n\t\t\tmemset(buf, 0, PAGE_SIZE);\n\t\tcond_resched();\n\t}\n\n\t/*\n\t * Add the internal journal blocks whether the journal has been\n\t * loaded or not\n\t */\n\tif (sbi->s_journal && !sbi->journal_bdev)\n\t\toverhead += EXT4_NUM_B2C(sbi, sbi->s_journal->j_maxlen);\n\telse if (ext4_has_feature_journal(sb) && !sbi->s_journal) {\n\t\tj_inode = ext4_get_journal_inode(sb, j_inum);\n\t\tif (j_inode) {\n\t\t\tj_blocks = j_inode->i_size >> sb->s_blocksize_bits;\n\t\t\toverhead += EXT4_NUM_B2C(sbi, j_blocks);\n\t\t\tiput(j_inode);\n\t\t} else {\n\t\t\text4_msg(sb, KERN_ERR, \"can't get journal size\");\n\t\t}\n\t}\n\tsbi->s_overhead = overhead;\n\tsmp_wmb();\n\tfree_page((unsigned long) buf);\n\treturn 0;\n}\n\nstatic void ext4_clamp_want_extra_isize(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\t/* determine the minimum size of new large inodes, if present */\n\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    sbi->s_want_extra_isize == 0) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t     EXT4_GOOD_OLD_INODE_SIZE;\n\t\tif (ext4_has_feature_extra_isize(sb)) {\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_want_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_want_extra_isize);\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_min_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_min_extra_isize);\n\t\t}\n\t}\n\t/* Check if enough inode space is available */\n\tif (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n\t\t\t\t\t\t\tsbi->s_inode_size) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t       EXT4_GOOD_OLD_INODE_SIZE;\n\t\text4_msg(sb, KERN_INFO,\n\t\t\t \"required extra inode space not available\");\n\t}\n}\n\nstatic void ext4_set_resv_clusters(struct super_block *sb)\n{\n\text4_fsblk_t resv_clusters;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\t/*\n\t * There's no need to reserve anything when we aren't using extents.\n\t * The space estimates are exact, there are no unwritten extents,\n\t * hole punching doesn't need new metadata... This is needed especially\n\t * to keep ext2/3 backward compatibility.\n\t */\n\tif (!ext4_has_feature_extents(sb))\n\t\treturn;\n\t/*\n\t * By default we reserve 2% or 4096 clusters, whichever is smaller.\n\t * This should cover the situations where we can not afford to run\n\t * out of space like for example punch hole, or converting\n\t * unwritten extents in delalloc path. In most cases such\n\t * allocation would require 1, or 2 blocks, higher numbers are\n\t * very rare.\n\t */\n\tresv_clusters = (ext4_blocks_count(sbi->s_es) >>\n\t\t\t sbi->s_cluster_bits);\n\n\tdo_div(resv_clusters, 50);\n\tresv_clusters = min_t(ext4_fsblk_t, resv_clusters, 4096);\n\n\tatomic64_set(&sbi->s_resv_clusters, resv_clusters);\n}\n\nstatic int ext4_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct dax_device *dax_dev = fs_dax_get_by_bdev(sb->s_bdev);\n\tchar *orig_data = kstrdup(data, GFP_KERNEL);\n\tstruct buffer_head *bh;\n\tstruct ext4_super_block *es = NULL;\n\tstruct ext4_sb_info *sbi = kzalloc(sizeof(*sbi), GFP_KERNEL);\n\text4_fsblk_t block;\n\text4_fsblk_t sb_block = get_sb_block(&data);\n\text4_fsblk_t logical_sb_block;\n\tunsigned long offset = 0;\n\tunsigned long journal_devnum = 0;\n\tunsigned long def_mount_opts;\n\tstruct inode *root;\n\tconst char *descr;\n\tint ret = -ENOMEM;\n\tint blocksize, clustersize;\n\tunsigned int db_count;\n\tunsigned int i;\n\tint needs_recovery, has_huge_files, has_bigalloc;\n\t__u64 blocks_count;\n\tint err = 0;\n\tunsigned int journal_ioprio = DEFAULT_JOURNAL_IOPRIO;\n\text4_group_t first_not_zeroed;\n\n\tif ((data && !orig_data) || !sbi)\n\t\tgoto out_free_base;\n\n\tsbi->s_daxdev = dax_dev;\n\tsbi->s_blockgroup_lock =\n\t\tkzalloc(sizeof(struct blockgroup_lock), GFP_KERNEL);\n\tif (!sbi->s_blockgroup_lock)\n\t\tgoto out_free_base;\n\n\tsb->s_fs_info = sbi;\n\tsbi->s_sb = sb;\n\tsbi->s_inode_readahead_blks = EXT4_DEF_INODE_READAHEAD_BLKS;\n\tsbi->s_sb_block = sb_block;\n\tif (sb->s_bdev->bd_part)\n\t\tsbi->s_sectors_written_start =\n\t\t\tpart_stat_read(sb->s_bdev->bd_part, sectors[STAT_WRITE]);\n\n\t/* Cleanup superblock name */\n\tstrreplace(sb->s_id, '/', '!');\n\n\t/* -EINVAL is default */\n\tret = -EINVAL;\n\tblocksize = sb_min_blocksize(sb, EXT4_MIN_BLOCK_SIZE);\n\tif (!blocksize) {\n\t\text4_msg(sb, KERN_ERR, \"unable to set blocksize\");\n\t\tgoto out_fail;\n\t}\n\n\t/*\n\t * The ext4 superblock will not be buffer aligned for other than 1kB\n\t * block sizes.  We need to calculate the offset from buffer start.\n\t */\n\tif (blocksize != EXT4_MIN_BLOCK_SIZE) {\n\t\tlogical_sb_block = sb_block * EXT4_MIN_BLOCK_SIZE;\n\t\toffset = do_div(logical_sb_block, blocksize);\n\t} else {\n\t\tlogical_sb_block = sb_block;\n\t}\n\n\tif (!(bh = sb_bread_unmovable(sb, logical_sb_block))) {\n\t\text4_msg(sb, KERN_ERR, \"unable to read superblock\");\n\t\tgoto out_fail;\n\t}\n\t/*\n\t * Note: s_es must be initialized as soon as possible because\n\t *       some ext4 macro-instructions depend on its value\n\t */\n\tes = (struct ext4_super_block *) (bh->b_data + offset);\n\tsbi->s_es = es;\n\tsb->s_magic = le16_to_cpu(es->s_magic);\n\tif (sb->s_magic != EXT4_SUPER_MAGIC)\n\t\tgoto cantfind_ext4;\n\tsbi->s_kbytes_written = le64_to_cpu(es->s_kbytes_written);\n\n\t/* Warn if metadata_csum and gdt_csum are both set. */\n\tif (ext4_has_feature_metadata_csum(sb) &&\n\t    ext4_has_feature_gdt_csum(sb))\n\t\text4_warning(sb, \"metadata_csum and uninit_bg are \"\n\t\t\t     \"redundant flags; please run fsck.\");\n\n\t/* Check for a known checksum algorithm */\n\tif (!ext4_verify_csum_type(sb, es)) {\n\t\text4_msg(sb, KERN_ERR, \"VFS: Found ext4 filesystem with \"\n\t\t\t \"unknown checksum algorithm.\");\n\t\tsilent = 1;\n\t\tgoto cantfind_ext4;\n\t}\n\n\t/* Load the checksum driver */\n\tsbi->s_chksum_driver = crypto_alloc_shash(\"crc32c\", 0, 0);\n\tif (IS_ERR(sbi->s_chksum_driver)) {\n\t\text4_msg(sb, KERN_ERR, \"Cannot load crc32c driver.\");\n\t\tret = PTR_ERR(sbi->s_chksum_driver);\n\t\tsbi->s_chksum_driver = NULL;\n\t\tgoto failed_mount;\n\t}\n\n\t/* Check superblock checksum */\n\tif (!ext4_superblock_csum_verify(sb, es)) {\n\t\text4_msg(sb, KERN_ERR, \"VFS: Found ext4 filesystem with \"\n\t\t\t \"invalid superblock checksum.  Run e2fsck?\");\n\t\tsilent = 1;\n\t\tret = -EFSBADCRC;\n\t\tgoto cantfind_ext4;\n\t}\n\n\t/* Precompute checksum seed for all metadata */\n\tif (ext4_has_feature_csum_seed(sb))\n\t\tsbi->s_csum_seed = le32_to_cpu(es->s_checksum_seed);\n\telse if (ext4_has_metadata_csum(sb) || ext4_has_feature_ea_inode(sb))\n\t\tsbi->s_csum_seed = ext4_chksum(sbi, ~0, es->s_uuid,\n\t\t\t\t\t       sizeof(es->s_uuid));\n\n\t/* Set defaults before we parse the mount options */\n\tdef_mount_opts = le32_to_cpu(es->s_default_mount_opts);\n\tset_opt(sb, INIT_INODE_TABLE);\n\tif (def_mount_opts & EXT4_DEFM_DEBUG)\n\t\tset_opt(sb, DEBUG);\n\tif (def_mount_opts & EXT4_DEFM_BSDGROUPS)\n\t\tset_opt(sb, GRPID);\n\tif (def_mount_opts & EXT4_DEFM_UID16)\n\t\tset_opt(sb, NO_UID32);\n\t/* xattr user namespace & acls are now defaulted on */\n\tset_opt(sb, XATTR_USER);\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\tset_opt(sb, POSIX_ACL);\n#endif\n\t/* don't forget to enable journal_csum when metadata_csum is enabled. */\n\tif (ext4_has_metadata_csum(sb))\n\t\tset_opt(sb, JOURNAL_CHECKSUM);\n\n\tif ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_DATA)\n\t\tset_opt(sb, JOURNAL_DATA);\n\telse if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_ORDERED)\n\t\tset_opt(sb, ORDERED_DATA);\n\telse if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_WBACK)\n\t\tset_opt(sb, WRITEBACK_DATA);\n\n\tif (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_PANIC)\n\t\tset_opt(sb, ERRORS_PANIC);\n\telse if (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_CONTINUE)\n\t\tset_opt(sb, ERRORS_CONT);\n\telse\n\t\tset_opt(sb, ERRORS_RO);\n\t/* block_validity enabled by default; disable with noblock_validity */\n\tset_opt(sb, BLOCK_VALIDITY);\n\tif (def_mount_opts & EXT4_DEFM_DISCARD)\n\t\tset_opt(sb, DISCARD);\n\n\tsbi->s_resuid = make_kuid(&init_user_ns, le16_to_cpu(es->s_def_resuid));\n\tsbi->s_resgid = make_kgid(&init_user_ns, le16_to_cpu(es->s_def_resgid));\n\tsbi->s_commit_interval = JBD2_DEFAULT_MAX_COMMIT_AGE * HZ;\n\tsbi->s_min_batch_time = EXT4_DEF_MIN_BATCH_TIME;\n\tsbi->s_max_batch_time = EXT4_DEF_MAX_BATCH_TIME;\n\n\tif ((def_mount_opts & EXT4_DEFM_NOBARRIER) == 0)\n\t\tset_opt(sb, BARRIER);\n\n\t/*\n\t * enable delayed allocation by default\n\t * Use -o nodelalloc to turn it off\n\t */\n\tif (!IS_EXT3_SB(sb) && !IS_EXT2_SB(sb) &&\n\t    ((def_mount_opts & EXT4_DEFM_NODELALLOC) == 0))\n\t\tset_opt(sb, DELALLOC);\n\n\t/*\n\t * set default s_li_wait_mult for lazyinit, for the case there is\n\t * no mount option specified.\n\t */\n\tsbi->s_li_wait_mult = EXT4_DEF_LI_WAIT_MULT;\n\n\tif (sbi->s_es->s_mount_opts[0]) {\n\t\tchar *s_mount_opts = kstrndup(sbi->s_es->s_mount_opts,\n\t\t\t\t\t      sizeof(sbi->s_es->s_mount_opts),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!s_mount_opts)\n\t\t\tgoto failed_mount;\n\t\tif (!parse_options(s_mount_opts, sb, &journal_devnum,\n\t\t\t\t   &journal_ioprio, 0)) {\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"failed to parse options in superblock: %s\",\n\t\t\t\t s_mount_opts);\n\t\t}\n\t\tkfree(s_mount_opts);\n\t}\n\tsbi->s_def_mount_opt = sbi->s_mount_opt;\n\tif (!parse_options((char *) data, sb, &journal_devnum,\n\t\t\t   &journal_ioprio, 0))\n\t\tgoto failed_mount;\n\n#ifdef CONFIG_UNICODE\n\tif (ext4_has_feature_casefold(sb) && !sbi->s_encoding) {\n\t\tconst struct ext4_sb_encodings *encoding_info;\n\t\tstruct unicode_map *encoding;\n\t\t__u16 encoding_flags;\n\n\t\tif (ext4_has_feature_encrypt(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Can't mount with encoding and encryption\");\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\tif (ext4_sb_read_encoding(es, &encoding_info,\n\t\t\t\t\t  &encoding_flags)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Encoding requested by superblock is unknown\");\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\tencoding = utf8_load(encoding_info->version);\n\t\tif (IS_ERR(encoding)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"can't mount with superblock charset: %s-%s \"\n\t\t\t\t \"not supported by the kernel. flags: 0x%x.\",\n\t\t\t\t encoding_info->name, encoding_info->version,\n\t\t\t\t encoding_flags);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\text4_msg(sb, KERN_INFO,\"Using encoding defined by superblock: \"\n\t\t\t \"%s-%s with flags 0x%hx\", encoding_info->name,\n\t\t\t encoding_info->version?:\"\\b\", encoding_flags);\n\n\t\tsbi->s_encoding = encoding;\n\t\tsbi->s_encoding_flags = encoding_flags;\n\t}\n#endif\n\n\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) {\n\t\tprintk_once(KERN_WARNING \"EXT4-fs: Warning: mounting \"\n\t\t\t    \"with data=journal disables delayed \"\n\t\t\t    \"allocation and O_DIRECT support!\\n\");\n\t\tif (test_opt2(sb, EXPLICIT_DELALLOC)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and delalloc\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (test_opt(sb, DIOREAD_NOLOCK)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and dioread_nolock\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (test_opt(sb, DAX)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and dax\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (ext4_has_feature_encrypt(sb)) {\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"encrypted files will use data=ordered \"\n\t\t\t\t \"instead of data journaling mode\");\n\t\t}\n\t\tif (test_opt(sb, DELALLOC))\n\t\t\tclear_opt(sb, DELALLOC);\n\t} else {\n\t\tsb->s_iflags |= SB_I_CGROUPWB;\n\t}\n\n\tsb->s_flags = (sb->s_flags & ~SB_POSIXACL) |\n\t\t(test_opt(sb, POSIX_ACL) ? SB_POSIXACL : 0);\n\n\tif (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV &&\n\t    (ext4_has_compat_features(sb) ||\n\t     ext4_has_ro_compat_features(sb) ||\n\t     ext4_has_incompat_features(sb)))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t       \"feature flags set on rev 0 fs, \"\n\t\t       \"running e2fsck is recommended\");\n\n\tif (es->s_creator_os == cpu_to_le32(EXT4_OS_HURD)) {\n\t\tset_opt2(sb, HURD_COMPAT);\n\t\tif (ext4_has_feature_64bit(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"The Hurd can't support 64-bit file systems\");\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\t/*\n\t\t * ea_inode feature uses l_i_version field which is not\n\t\t * available in HURD_COMPAT mode.\n\t\t */\n\t\tif (ext4_has_feature_ea_inode(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"ea_inode feature is not supported for Hurd\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\tif (IS_EXT2_SB(sb)) {\n\t\tif (ext2_feature_set_ok(sb))\n\t\t\text4_msg(sb, KERN_INFO, \"mounting ext2 file system \"\n\t\t\t\t \"using the ext4 subsystem\");\n\t\telse {\n\t\t\t/*\n\t\t\t * If we're probing be silent, if this looks like\n\t\t\t * it's actually an ext[34] filesystem.\n\t\t\t */\n\t\t\tif (silent && ext4_feature_set_ok(sb, sb_rdonly(sb)))\n\t\t\t\tgoto failed_mount;\n\t\t\text4_msg(sb, KERN_ERR, \"couldn't mount as ext2 due \"\n\t\t\t\t \"to feature incompatibilities\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\tif (IS_EXT3_SB(sb)) {\n\t\tif (ext3_feature_set_ok(sb))\n\t\t\text4_msg(sb, KERN_INFO, \"mounting ext3 file system \"\n\t\t\t\t \"using the ext4 subsystem\");\n\t\telse {\n\t\t\t/*\n\t\t\t * If we're probing be silent, if this looks like\n\t\t\t * it's actually an ext4 filesystem.\n\t\t\t */\n\t\t\tif (silent && ext4_feature_set_ok(sb, sb_rdonly(sb)))\n\t\t\t\tgoto failed_mount;\n\t\t\text4_msg(sb, KERN_ERR, \"couldn't mount as ext3 due \"\n\t\t\t\t \"to feature incompatibilities\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\t/*\n\t * Check feature flags regardless of the revision level, since we\n\t * previously didn't change the revision level when setting the flags,\n\t * so there is a chance incompat flags are set on a rev 0 filesystem.\n\t */\n\tif (!ext4_feature_set_ok(sb, (sb_rdonly(sb))))\n\t\tgoto failed_mount;\n\n\tblocksize = BLOCK_SIZE << le32_to_cpu(es->s_log_block_size);\n\tif (blocksize < EXT4_MIN_BLOCK_SIZE ||\n\t    blocksize > EXT4_MAX_BLOCK_SIZE) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t       \"Unsupported filesystem blocksize %d (%d log_block_size)\",\n\t\t\t blocksize, le32_to_cpu(es->s_log_block_size));\n\t\tgoto failed_mount;\n\t}\n\tif (le32_to_cpu(es->s_log_block_size) >\n\t    (EXT4_MAX_BLOCK_LOG_SIZE - EXT4_MIN_BLOCK_LOG_SIZE)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Invalid log block size: %u\",\n\t\t\t le32_to_cpu(es->s_log_block_size));\n\t\tgoto failed_mount;\n\t}\n\tif (le32_to_cpu(es->s_log_cluster_size) >\n\t    (EXT4_MAX_CLUSTER_LOG_SIZE - EXT4_MIN_BLOCK_LOG_SIZE)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Invalid log cluster size: %u\",\n\t\t\t le32_to_cpu(es->s_log_cluster_size));\n\t\tgoto failed_mount;\n\t}\n\n\tif (le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) > (blocksize / 4)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Number of reserved GDT blocks insanely large: %d\",\n\t\t\t le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks));\n\t\tgoto failed_mount;\n\t}\n\n\tif (sbi->s_mount_opt & EXT4_MOUNT_DAX) {\n\t\tif (ext4_has_feature_inline_data(sb)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Cannot use DAX on a filesystem\"\n\t\t\t\t\t\" that may contain inline data\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (!bdev_dax_supported(sb->s_bdev, blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"DAX unsupported by block device.\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\tif (ext4_has_feature_encrypt(sb) && es->s_encryption_level) {\n\t\text4_msg(sb, KERN_ERR, \"Unsupported encryption level %d\",\n\t\t\t es->s_encryption_level);\n\t\tgoto failed_mount;\n\t}\n\n\tif (sb->s_blocksize != blocksize) {\n\t\t/* Validate the filesystem blocksize */\n\t\tif (!sb_set_blocksize(sb, blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR, \"bad block size %d\",\n\t\t\t\t\tblocksize);\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\tbrelse(bh);\n\t\tlogical_sb_block = sb_block * EXT4_MIN_BLOCK_SIZE;\n\t\toffset = do_div(logical_sb_block, blocksize);\n\t\tbh = sb_bread_unmovable(sb, logical_sb_block);\n\t\tif (!bh) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Can't read superblock on 2nd try\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tes = (struct ext4_super_block *)(bh->b_data + offset);\n\t\tsbi->s_es = es;\n\t\tif (es->s_magic != cpu_to_le16(EXT4_SUPER_MAGIC)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Magic mismatch, very weird!\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\thas_huge_files = ext4_has_feature_huge_file(sb);\n\tsbi->s_bitmap_maxbytes = ext4_max_bitmap_size(sb->s_blocksize_bits,\n\t\t\t\t\t\t      has_huge_files);\n\tsb->s_maxbytes = ext4_max_size(sb->s_blocksize_bits, has_huge_files);\n\n\tif (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV) {\n\t\tsbi->s_inode_size = EXT4_GOOD_OLD_INODE_SIZE;\n\t\tsbi->s_first_ino = EXT4_GOOD_OLD_FIRST_INO;\n\t} else {\n\t\tsbi->s_inode_size = le16_to_cpu(es->s_inode_size);\n\t\tsbi->s_first_ino = le32_to_cpu(es->s_first_ino);\n\t\tif (sbi->s_first_ino < EXT4_GOOD_OLD_FIRST_INO) {\n\t\t\text4_msg(sb, KERN_ERR, \"invalid first ino: %u\",\n\t\t\t\t sbi->s_first_ino);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif ((sbi->s_inode_size < EXT4_GOOD_OLD_INODE_SIZE) ||\n\t\t    (!is_power_of_2(sbi->s_inode_size)) ||\n\t\t    (sbi->s_inode_size > blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unsupported inode size: %d\",\n\t\t\t       sbi->s_inode_size);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\t/*\n\t\t * i_atime_extra is the last extra field available for [acm]times in\n\t\t * struct ext4_inode. Checking for that field should suffice to ensure\n\t\t * we have extra space for all three.\n\t\t */\n\t\tif (sbi->s_inode_size >= offsetof(struct ext4_inode, i_atime_extra) +\n\t\t\tsizeof(((struct ext4_inode *)0)->i_atime_extra)) {\n\t\t\tsb->s_time_gran = 1;\n\t\t\tsb->s_time_max = EXT4_EXTRA_TIMESTAMP_MAX;\n\t\t} else {\n\t\t\tsb->s_time_gran = NSEC_PER_SEC;\n\t\t\tsb->s_time_max = EXT4_NON_EXTRA_TIMESTAMP_MAX;\n\t\t}\n\n\t\tsb->s_time_min = EXT4_TIMESTAMP_MIN;\n\t}\n\n\tsbi->s_desc_size = le16_to_cpu(es->s_desc_size);\n\tif (ext4_has_feature_64bit(sb)) {\n\t\tif (sbi->s_desc_size < EXT4_MIN_DESC_SIZE_64BIT ||\n\t\t    sbi->s_desc_size > EXT4_MAX_DESC_SIZE ||\n\t\t    !is_power_of_2(sbi->s_desc_size)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unsupported descriptor size %lu\",\n\t\t\t       sbi->s_desc_size);\n\t\t\tgoto failed_mount;\n\t\t}\n\t} else\n\t\tsbi->s_desc_size = EXT4_MIN_DESC_SIZE;\n\n\tsbi->s_blocks_per_group = le32_to_cpu(es->s_blocks_per_group);\n\tsbi->s_inodes_per_group = le32_to_cpu(es->s_inodes_per_group);\n\n\tsbi->s_inodes_per_block = blocksize / EXT4_INODE_SIZE(sb);\n\tif (sbi->s_inodes_per_block == 0)\n\t\tgoto cantfind_ext4;\n\tif (sbi->s_inodes_per_group < sbi->s_inodes_per_block ||\n\t    sbi->s_inodes_per_group > blocksize * 8) {\n\t\text4_msg(sb, KERN_ERR, \"invalid inodes per group: %lu\\n\",\n\t\t\t sbi->s_blocks_per_group);\n\t\tgoto failed_mount;\n\t}\n\tsbi->s_itb_per_group = sbi->s_inodes_per_group /\n\t\t\t\t\tsbi->s_inodes_per_block;\n\tsbi->s_desc_per_block = blocksize / EXT4_DESC_SIZE(sb);\n\tsbi->s_sbh = bh;\n\tsbi->s_mount_state = le16_to_cpu(es->s_state);\n\tsbi->s_addr_per_block_bits = ilog2(EXT4_ADDR_PER_BLOCK(sb));\n\tsbi->s_desc_per_block_bits = ilog2(EXT4_DESC_PER_BLOCK(sb));\n\n\tfor (i = 0; i < 4; i++)\n\t\tsbi->s_hash_seed[i] = le32_to_cpu(es->s_hash_seed[i]);\n\tsbi->s_def_hash_version = es->s_def_hash_version;\n\tif (ext4_has_feature_dir_index(sb)) {\n\t\ti = le32_to_cpu(es->s_flags);\n\t\tif (i & EXT2_FLAGS_UNSIGNED_HASH)\n\t\t\tsbi->s_hash_unsigned = 3;\n\t\telse if ((i & EXT2_FLAGS_SIGNED_HASH) == 0) {\n#ifdef __CHAR_UNSIGNED__\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\tes->s_flags |=\n\t\t\t\t\tcpu_to_le32(EXT2_FLAGS_UNSIGNED_HASH);\n\t\t\tsbi->s_hash_unsigned = 3;\n#else\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\tes->s_flags |=\n\t\t\t\t\tcpu_to_le32(EXT2_FLAGS_SIGNED_HASH);\n#endif\n\t\t}\n\t}\n\n\t/* Handle clustersize */\n\tclustersize = BLOCK_SIZE << le32_to_cpu(es->s_log_cluster_size);\n\thas_bigalloc = ext4_has_feature_bigalloc(sb);\n\tif (has_bigalloc) {\n\t\tif (clustersize < blocksize) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"cluster size (%d) smaller than \"\n\t\t\t\t \"block size (%d)\", clustersize, blocksize);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tsbi->s_cluster_bits = le32_to_cpu(es->s_log_cluster_size) -\n\t\t\tle32_to_cpu(es->s_log_block_size);\n\t\tsbi->s_clusters_per_group =\n\t\t\tle32_to_cpu(es->s_clusters_per_group);\n\t\tif (sbi->s_clusters_per_group > blocksize * 8) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"#clusters per group too big: %lu\",\n\t\t\t\t sbi->s_clusters_per_group);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (sbi->s_blocks_per_group !=\n\t\t    (sbi->s_clusters_per_group * (clustersize / blocksize))) {\n\t\t\text4_msg(sb, KERN_ERR, \"blocks per group (%lu) and \"\n\t\t\t\t \"clusters per group (%lu) inconsistent\",\n\t\t\t\t sbi->s_blocks_per_group,\n\t\t\t\t sbi->s_clusters_per_group);\n\t\t\tgoto failed_mount;\n\t\t}\n\t} else {\n\t\tif (clustersize != blocksize) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"fragment/cluster size (%d) != \"\n\t\t\t\t \"block size (%d)\", clustersize, blocksize);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (sbi->s_blocks_per_group > blocksize * 8) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"#blocks per group too big: %lu\",\n\t\t\t\t sbi->s_blocks_per_group);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tsbi->s_clusters_per_group = sbi->s_blocks_per_group;\n\t\tsbi->s_cluster_bits = 0;\n\t}\n\tsbi->s_cluster_ratio = clustersize / blocksize;\n\n\t/* Do we have standard group size of clustersize * 8 blocks ? */\n\tif (sbi->s_blocks_per_group == clustersize << 3)\n\t\tset_opt2(sb, STD_GROUP_SIZE);\n\n\t/*\n\t * Test whether we have more sectors than will fit in sector_t,\n\t * and whether the max offset is addressable by the page cache.\n\t */\n\terr = generic_check_addressable(sb->s_blocksize_bits,\n\t\t\t\t\text4_blocks_count(es));\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"filesystem\"\n\t\t\t \" too large to mount safely on this system\");\n\t\tgoto failed_mount;\n\t}\n\n\tif (EXT4_BLOCKS_PER_GROUP(sb) == 0)\n\t\tgoto cantfind_ext4;\n\n\t/* check blocks count against device size */\n\tblocks_count = sb->s_bdev->bd_inode->i_size >> sb->s_blocksize_bits;\n\tif (blocks_count && ext4_blocks_count(es) > blocks_count) {\n\t\text4_msg(sb, KERN_WARNING, \"bad geometry: block count %llu \"\n\t\t       \"exceeds size of device (%llu blocks)\",\n\t\t       ext4_blocks_count(es), blocks_count);\n\t\tgoto failed_mount;\n\t}\n\n\t/*\n\t * It makes no sense for the first data block to be beyond the end\n\t * of the filesystem.\n\t */\n\tif (le32_to_cpu(es->s_first_data_block) >= ext4_blocks_count(es)) {\n\t\text4_msg(sb, KERN_WARNING, \"bad geometry: first data \"\n\t\t\t \"block %u is beyond end of filesystem (%llu)\",\n\t\t\t le32_to_cpu(es->s_first_data_block),\n\t\t\t ext4_blocks_count(es));\n\t\tgoto failed_mount;\n\t}\n\tif ((es->s_first_data_block == 0) && (es->s_log_block_size == 0) &&\n\t    (sbi->s_cluster_ratio == 1)) {\n\t\text4_msg(sb, KERN_WARNING, \"bad geometry: first data \"\n\t\t\t \"block is 0 with a 1k block and cluster size\");\n\t\tgoto failed_mount;\n\t}\n\n\tblocks_count = (ext4_blocks_count(es) -\n\t\t\tle32_to_cpu(es->s_first_data_block) +\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb) - 1);\n\tdo_div(blocks_count, EXT4_BLOCKS_PER_GROUP(sb));\n\tif (blocks_count > ((uint64_t)1<<32) - EXT4_DESC_PER_BLOCK(sb)) {\n\t\text4_msg(sb, KERN_WARNING, \"groups count too large: %u \"\n\t\t       \"(block count %llu, first data block %u, \"\n\t\t       \"blocks per group %lu)\", sbi->s_groups_count,\n\t\t       ext4_blocks_count(es),\n\t\t       le32_to_cpu(es->s_first_data_block),\n\t\t       EXT4_BLOCKS_PER_GROUP(sb));\n\t\tgoto failed_mount;\n\t}\n\tsbi->s_groups_count = blocks_count;\n\tsbi->s_blockfile_groups = min_t(ext4_group_t, sbi->s_groups_count,\n\t\t\t(EXT4_MAX_BLOCK_FILE_PHYS / EXT4_BLOCKS_PER_GROUP(sb)));\n\tif (((u64)sbi->s_groups_count * sbi->s_inodes_per_group) !=\n\t    le32_to_cpu(es->s_inodes_count)) {\n\t\text4_msg(sb, KERN_ERR, \"inodes count not valid: %u vs %llu\",\n\t\t\t le32_to_cpu(es->s_inodes_count),\n\t\t\t ((u64)sbi->s_groups_count * sbi->s_inodes_per_group));\n\t\tret = -EINVAL;\n\t\tgoto failed_mount;\n\t}\n\tdb_count = (sbi->s_groups_count + EXT4_DESC_PER_BLOCK(sb) - 1) /\n\t\t   EXT4_DESC_PER_BLOCK(sb);\n\tif (ext4_has_feature_meta_bg(sb)) {\n\t\tif (le32_to_cpu(es->s_first_meta_bg) > db_count) {\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"first meta block group too large: %u \"\n\t\t\t\t \"(group descriptor block count %u)\",\n\t\t\t\t le32_to_cpu(es->s_first_meta_bg), db_count);\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\tsbi->s_group_desc = kvmalloc_array(db_count,\n\t\t\t\t\t   sizeof(struct buffer_head *),\n\t\t\t\t\t   GFP_KERNEL);\n\tif (sbi->s_group_desc == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory\");\n\t\tret = -ENOMEM;\n\t\tgoto failed_mount;\n\t}\n\n\tbgl_lock_init(sbi->s_blockgroup_lock);\n\n\t/* Pre-read the descriptors into the buffer cache */\n\tfor (i = 0; i < db_count; i++) {\n\t\tblock = descriptor_loc(sb, logical_sb_block, i);\n\t\tsb_breadahead(sb, block);\n\t}\n\n\tfor (i = 0; i < db_count; i++) {\n\t\tblock = descriptor_loc(sb, logical_sb_block, i);\n\t\tsbi->s_group_desc[i] = sb_bread_unmovable(sb, block);\n\t\tif (!sbi->s_group_desc[i]) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"can't read group descriptor %d\", i);\n\t\t\tdb_count = i;\n\t\t\tgoto failed_mount2;\n\t\t}\n\t}\n\tsbi->s_gdb_count = db_count;\n\tif (!ext4_check_descriptors(sb, logical_sb_block, &first_not_zeroed)) {\n\t\text4_msg(sb, KERN_ERR, \"group descriptors corrupted!\");\n\t\tret = -EFSCORRUPTED;\n\t\tgoto failed_mount2;\n\t}\n\n\ttimer_setup(&sbi->s_err_report, print_daily_error_info, 0);\n\n\t/* Register extent status tree shrinker */\n\tif (ext4_es_register_shrinker(sbi))\n\t\tgoto failed_mount3;\n\n\tsbi->s_stripe = ext4_get_stripe_size(sbi);\n\tsbi->s_extent_max_zeroout_kb = 32;\n\n\t/*\n\t * set up enough so that it can read an inode\n\t */\n\tsb->s_op = &ext4_sops;\n\tsb->s_export_op = &ext4_export_ops;\n\tsb->s_xattr = ext4_xattr_handlers;\n#ifdef CONFIG_FS_ENCRYPTION\n\tsb->s_cop = &ext4_cryptops;\n#endif\n#ifdef CONFIG_FS_VERITY\n\tsb->s_vop = &ext4_verityops;\n#endif\n#ifdef CONFIG_QUOTA\n\tsb->dq_op = &ext4_quota_operations;\n\tif (ext4_has_feature_quota(sb))\n\t\tsb->s_qcop = &dquot_quotactl_sysfile_ops;\n\telse\n\t\tsb->s_qcop = &ext4_qctl_operations;\n\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ;\n#endif\n\tmemcpy(&sb->s_uuid, es->s_uuid, sizeof(es->s_uuid));\n\n\tINIT_LIST_HEAD(&sbi->s_orphan); /* unlinked but open files */\n\tmutex_init(&sbi->s_orphan_lock);\n\n\tsb->s_root = NULL;\n\n\tneeds_recovery = (es->s_last_orphan != 0 ||\n\t\t\t  ext4_has_feature_journal_needs_recovery(sb));\n\n\tif (ext4_has_feature_mmp(sb) && !sb_rdonly(sb))\n\t\tif (ext4_multi_mount_protect(sb, le64_to_cpu(es->s_mmp_block)))\n\t\t\tgoto failed_mount3a;\n\n\t/*\n\t * The first inode we look at is the journal inode.  Don't try\n\t * root first: it may be modified in the journal!\n\t */\n\tif (!test_opt(sb, NOLOAD) && ext4_has_feature_journal(sb)) {\n\t\terr = ext4_load_journal(sb, es, journal_devnum);\n\t\tif (err)\n\t\t\tgoto failed_mount3a;\n\t} else if (test_opt(sb, NOLOAD) && !sb_rdonly(sb) &&\n\t\t   ext4_has_feature_journal_needs_recovery(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"required journal recovery \"\n\t\t       \"suppressed and not mounted read-only\");\n\t\tgoto failed_mount_wq;\n\t} else {\n\t\t/* Nojournal mode, all journal mount options are illegal */\n\t\tif (test_opt2(sb, EXPLICIT_JOURNAL_CHECKSUM)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"journal_checksum, fs mounted w/o journal\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"journal_async_commit, fs mounted w/o journal\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t\tif (sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"commit=%lu, fs mounted w/o journal\",\n\t\t\t\t sbi->s_commit_interval / HZ);\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t\tif (EXT4_MOUNT_DATA_FLAGS &\n\t\t    (sbi->s_mount_opt ^ sbi->s_def_mount_opt)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"data=, fs mounted w/o journal\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t\tsbi->s_def_mount_opt &= ~EXT4_MOUNT_JOURNAL_CHECKSUM;\n\t\tclear_opt(sb, JOURNAL_CHECKSUM);\n\t\tclear_opt(sb, DATA_FLAGS);\n\t\tsbi->s_journal = NULL;\n\t\tneeds_recovery = 0;\n\t\tgoto no_journal;\n\t}\n\n\tif (ext4_has_feature_64bit(sb) &&\n\t    !jbd2_journal_set_features(EXT4_SB(sb)->s_journal, 0, 0,\n\t\t\t\t       JBD2_FEATURE_INCOMPAT_64BIT)) {\n\t\text4_msg(sb, KERN_ERR, \"Failed to set 64-bit journal feature\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tif (!set_journal_csum_feature_set(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"Failed to set journal checksum \"\n\t\t\t \"feature set\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\t/* We have now updated the journal if required, so we can\n\t * validate the data journaling mode. */\n\tswitch (test_opt(sb, DATA_FLAGS)) {\n\tcase 0:\n\t\t/* No mode set, assume a default based on the journal\n\t\t * capabilities: ORDERED_DATA if the journal can\n\t\t * cope, else JOURNAL_DATA\n\t\t */\n\t\tif (jbd2_journal_check_available_features\n\t\t    (sbi->s_journal, 0, 0, JBD2_FEATURE_INCOMPAT_REVOKE)) {\n\t\t\tset_opt(sb, ORDERED_DATA);\n\t\t\tsbi->s_def_mount_opt |= EXT4_MOUNT_ORDERED_DATA;\n\t\t} else {\n\t\t\tset_opt(sb, JOURNAL_DATA);\n\t\t\tsbi->s_def_mount_opt |= EXT4_MOUNT_JOURNAL_DATA;\n\t\t}\n\t\tbreak;\n\n\tcase EXT4_MOUNT_ORDERED_DATA:\n\tcase EXT4_MOUNT_WRITEBACK_DATA:\n\t\tif (!jbd2_journal_check_available_features\n\t\t    (sbi->s_journal, 0, 0, JBD2_FEATURE_INCOMPAT_REVOKE)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Journal does not support \"\n\t\t\t       \"requested data journaling mode\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA &&\n\t    test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\"journal_async_commit in data=ordered mode\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tset_task_ioprio(sbi->s_journal->j_task, journal_ioprio);\n\n\tsbi->s_journal->j_commit_callback = ext4_journal_commit_callback;\n\nno_journal:\n\tif (!test_opt(sb, NO_MBCACHE)) {\n\t\tsbi->s_ea_block_cache = ext4_xattr_create_cache();\n\t\tif (!sbi->s_ea_block_cache) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Failed to create ea_block_cache\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\n\t\tif (ext4_has_feature_ea_inode(sb)) {\n\t\t\tsbi->s_ea_inode_cache = ext4_xattr_create_cache();\n\t\t\tif (!sbi->s_ea_inode_cache) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t \"Failed to create ea_inode_cache\");\n\t\t\t\tgoto failed_mount_wq;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((DUMMY_ENCRYPTION_ENABLED(sbi) || ext4_has_feature_encrypt(sb)) &&\n\t    (blocksize != PAGE_SIZE)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Unsupported blocksize for fs encryption\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tif (ext4_has_feature_verity(sb) && blocksize != PAGE_SIZE) {\n\t\text4_msg(sb, KERN_ERR, \"Unsupported blocksize for fs-verity\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tif (DUMMY_ENCRYPTION_ENABLED(sbi) && !sb_rdonly(sb) &&\n\t    !ext4_has_feature_encrypt(sb)) {\n\t\text4_set_feature_encrypt(sb);\n\t\text4_commit_super(sb, 1);\n\t}\n\n\t/*\n\t * Get the # of file system overhead blocks from the\n\t * superblock if present.\n\t */\n\tif (es->s_overhead_clusters)\n\t\tsbi->s_overhead = le32_to_cpu(es->s_overhead_clusters);\n\telse {\n\t\terr = ext4_calculate_overhead(sb);\n\t\tif (err)\n\t\t\tgoto failed_mount_wq;\n\t}\n\n\t/*\n\t * The maximum number of concurrent works can be high and\n\t * concurrency isn't really necessary.  Limit it to 1.\n\t */\n\tEXT4_SB(sb)->rsv_conversion_wq =\n\t\talloc_workqueue(\"ext4-rsv-conversion\", WQ_MEM_RECLAIM | WQ_UNBOUND, 1);\n\tif (!EXT4_SB(sb)->rsv_conversion_wq) {\n\t\tprintk(KERN_ERR \"EXT4-fs: failed to create workqueue\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto failed_mount4;\n\t}\n\n\t/*\n\t * The jbd2_journal_load will have done any necessary log recovery,\n\t * so we can safely mount the rest of the filesystem now.\n\t */\n\n\troot = ext4_iget(sb, EXT4_ROOT_INO, EXT4_IGET_SPECIAL);\n\tif (IS_ERR(root)) {\n\t\text4_msg(sb, KERN_ERR, \"get root inode failed\");\n\t\tret = PTR_ERR(root);\n\t\troot = NULL;\n\t\tgoto failed_mount4;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) {\n\t\text4_msg(sb, KERN_ERR, \"corrupt root inode, run e2fsck\");\n\t\tiput(root);\n\t\tgoto failed_mount4;\n\t}\n\n#ifdef CONFIG_UNICODE\n\tif (sbi->s_encoding)\n\t\tsb->s_d_op = &ext4_dentry_ops;\n#endif\n\n\tsb->s_root = d_make_root(root);\n\tif (!sb->s_root) {\n\t\text4_msg(sb, KERN_ERR, \"get root dentry failed\");\n\t\tret = -ENOMEM;\n\t\tgoto failed_mount4;\n\t}\n\n\tret = ext4_setup_super(sb, es, sb_rdonly(sb));\n\tif (ret == -EROFS) {\n\t\tsb->s_flags |= SB_RDONLY;\n\t\tret = 0;\n\t} else if (ret)\n\t\tgoto failed_mount4a;\n\n\text4_clamp_want_extra_isize(sb);\n\n\text4_set_resv_clusters(sb);\n\n\terr = ext4_setup_system_zone(sb);\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"failed to initialize system \"\n\t\t\t \"zone (%d)\", err);\n\t\tgoto failed_mount4a;\n\t}\n\n\text4_ext_init(sb);\n\terr = ext4_mb_init(sb);\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"failed to initialize mballoc (%d)\",\n\t\t\t err);\n\t\tgoto failed_mount5;\n\t}\n\n\tblock = ext4_count_free_clusters(sb);\n\text4_free_blocks_count_set(sbi->s_es, \n\t\t\t\t   EXT4_C2B(sbi, block));\n\text4_superblock_csum_set(sb);\n\terr = percpu_counter_init(&sbi->s_freeclusters_counter, block,\n\t\t\t\t  GFP_KERNEL);\n\tif (!err) {\n\t\tunsigned long freei = ext4_count_free_inodes(sb);\n\t\tsbi->s_es->s_free_inodes_count = cpu_to_le32(freei);\n\t\text4_superblock_csum_set(sb);\n\t\terr = percpu_counter_init(&sbi->s_freeinodes_counter, freei,\n\t\t\t\t\t  GFP_KERNEL);\n\t}\n\tif (!err)\n\t\terr = percpu_counter_init(&sbi->s_dirs_counter,\n\t\t\t\t\t  ext4_count_dirs(sb), GFP_KERNEL);\n\tif (!err)\n\t\terr = percpu_counter_init(&sbi->s_dirtyclusters_counter, 0,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!err)\n\t\terr = percpu_init_rwsem(&sbi->s_journal_flag_rwsem);\n\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"insufficient memory\");\n\t\tgoto failed_mount6;\n\t}\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tif (!ext4_fill_flex_info(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unable to initialize \"\n\t\t\t       \"flex_bg meta info!\");\n\t\t\tgoto failed_mount6;\n\t\t}\n\n\terr = ext4_register_li_request(sb, first_not_zeroed);\n\tif (err)\n\t\tgoto failed_mount6;\n\n\terr = ext4_register_sysfs(sb);\n\tif (err)\n\t\tgoto failed_mount7;\n\n#ifdef CONFIG_QUOTA\n\t/* Enable quota usage during mount. */\n\tif (ext4_has_feature_quota(sb) && !sb_rdonly(sb)) {\n\t\terr = ext4_enable_quotas(sb);\n\t\tif (err)\n\t\t\tgoto failed_mount8;\n\t}\n#endif  /* CONFIG_QUOTA */\n\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ORPHAN_FS;\n\text4_orphan_cleanup(sb, es);\n\tEXT4_SB(sb)->s_mount_state &= ~EXT4_ORPHAN_FS;\n\tif (needs_recovery) {\n\t\text4_msg(sb, KERN_INFO, \"recovery complete\");\n\t\text4_mark_recovery_complete(sb, es);\n\t}\n\tif (EXT4_SB(sb)->s_journal) {\n\t\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\t\tdescr = \" journalled data mode\";\n\t\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\t\tdescr = \" ordered data mode\";\n\t\telse\n\t\t\tdescr = \" writeback data mode\";\n\t} else\n\t\tdescr = \"out journal\";\n\n\tif (test_opt(sb, DISCARD)) {\n\t\tstruct request_queue *q = bdev_get_queue(sb->s_bdev);\n\t\tif (!blk_queue_discard(q))\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"mounting with \\\"discard\\\" option, but \"\n\t\t\t\t \"the device does not support discard\");\n\t}\n\n\tif (___ratelimit(&ext4_mount_msg_ratelimit, \"EXT4-fs mount\"))\n\t\text4_msg(sb, KERN_INFO, \"mounted filesystem with%s. \"\n\t\t\t \"Opts: %.*s%s%s\", descr,\n\t\t\t (int) sizeof(sbi->s_es->s_mount_opts),\n\t\t\t sbi->s_es->s_mount_opts,\n\t\t\t *sbi->s_es->s_mount_opts ? \"; \" : \"\", orig_data);\n\n\tif (es->s_error_count)\n\t\tmod_timer(&sbi->s_err_report, jiffies + 300*HZ); /* 5 minutes */\n\n\t/* Enable message ratelimiting. Default is 10 messages per 5 secs. */\n\tratelimit_state_init(&sbi->s_err_ratelimit_state, 5 * HZ, 10);\n\tratelimit_state_init(&sbi->s_warning_ratelimit_state, 5 * HZ, 10);\n\tratelimit_state_init(&sbi->s_msg_ratelimit_state, 5 * HZ, 10);\n\n\tkfree(orig_data);\n\treturn 0;\n\ncantfind_ext4:\n\tif (!silent)\n\t\text4_msg(sb, KERN_ERR, \"VFS: Can't find ext4 filesystem\");\n\tgoto failed_mount;\n\n#ifdef CONFIG_QUOTA\nfailed_mount8:\n\text4_unregister_sysfs(sb);\n#endif\nfailed_mount7:\n\text4_unregister_li_request(sb);\nfailed_mount6:\n\text4_mb_release(sb);\n\tif (sbi->s_flex_groups)\n\t\tkvfree(sbi->s_flex_groups);\n\tpercpu_counter_destroy(&sbi->s_freeclusters_counter);\n\tpercpu_counter_destroy(&sbi->s_freeinodes_counter);\n\tpercpu_counter_destroy(&sbi->s_dirs_counter);\n\tpercpu_counter_destroy(&sbi->s_dirtyclusters_counter);\n\tpercpu_free_rwsem(&sbi->s_journal_flag_rwsem);\nfailed_mount5:\n\text4_ext_release(sb);\n\text4_release_system_zone(sb);\nfailed_mount4a:\n\tdput(sb->s_root);\n\tsb->s_root = NULL;\nfailed_mount4:\n\text4_msg(sb, KERN_ERR, \"mount failed\");\n\tif (EXT4_SB(sb)->rsv_conversion_wq)\n\t\tdestroy_workqueue(EXT4_SB(sb)->rsv_conversion_wq);\nfailed_mount_wq:\n\text4_xattr_destroy_cache(sbi->s_ea_inode_cache);\n\tsbi->s_ea_inode_cache = NULL;\n\n\text4_xattr_destroy_cache(sbi->s_ea_block_cache);\n\tsbi->s_ea_block_cache = NULL;\n\n\tif (sbi->s_journal) {\n\t\tjbd2_journal_destroy(sbi->s_journal);\n\t\tsbi->s_journal = NULL;\n\t}\nfailed_mount3a:\n\text4_es_unregister_shrinker(sbi);\nfailed_mount3:\n\tdel_timer_sync(&sbi->s_err_report);\n\tif (sbi->s_mmp_tsk)\n\t\tkthread_stop(sbi->s_mmp_tsk);\nfailed_mount2:\n\tfor (i = 0; i < db_count; i++)\n\t\tbrelse(sbi->s_group_desc[i]);\n\tkvfree(sbi->s_group_desc);\nfailed_mount:\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\n#ifdef CONFIG_UNICODE\n\tutf8_unload(sbi->s_encoding);\n#endif\n\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tkfree(get_qf_name(sb, sbi, i));\n#endif\n\text4_blkdev_remove(sbi);\n\tbrelse(bh);\nout_fail:\n\tsb->s_fs_info = NULL;\n\tkfree(sbi->s_blockgroup_lock);\nout_free_base:\n\tkfree(sbi);\n\tkfree(orig_data);\n\tfs_put_dax(dax_dev);\n\treturn err ? err : ret;\n}\n\n/*\n * Setup any per-fs journal parameters now.  We'll do this both on\n * initial mount, once the journal has been initialised but before we've\n * done any recovery; and again on any subsequent remount.\n */\nstatic void ext4_init_journal_params(struct super_block *sb, journal_t *journal)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tjournal->j_commit_interval = sbi->s_commit_interval;\n\tjournal->j_min_batch_time = sbi->s_min_batch_time;\n\tjournal->j_max_batch_time = sbi->s_max_batch_time;\n\n\twrite_lock(&journal->j_state_lock);\n\tif (test_opt(sb, BARRIER))\n\t\tjournal->j_flags |= JBD2_BARRIER;\n\telse\n\t\tjournal->j_flags &= ~JBD2_BARRIER;\n\tif (test_opt(sb, DATA_ERR_ABORT))\n\t\tjournal->j_flags |= JBD2_ABORT_ON_SYNCDATA_ERR;\n\telse\n\t\tjournal->j_flags &= ~JBD2_ABORT_ON_SYNCDATA_ERR;\n\twrite_unlock(&journal->j_state_lock);\n}\n\nstatic struct inode *ext4_get_journal_inode(struct super_block *sb,\n\t\t\t\t\t     unsigned int journal_inum)\n{\n\tstruct inode *journal_inode;\n\n\t/*\n\t * Test for the existence of a valid inode on disk.  Bad things\n\t * happen if we iget() an unused inode, as the subsequent iput()\n\t * will try to delete it.\n\t */\n\tjournal_inode = ext4_iget(sb, journal_inum, EXT4_IGET_SPECIAL);\n\tif (IS_ERR(journal_inode)) {\n\t\text4_msg(sb, KERN_ERR, \"no journal found\");\n\t\treturn NULL;\n\t}\n\tif (!journal_inode->i_nlink) {\n\t\tmake_bad_inode(journal_inode);\n\t\tiput(journal_inode);\n\t\text4_msg(sb, KERN_ERR, \"journal inode is deleted\");\n\t\treturn NULL;\n\t}\n\n\tjbd_debug(2, \"Journal inode found at %p: %lld bytes\\n\",\n\t\t  journal_inode, journal_inode->i_size);\n\tif (!S_ISREG(journal_inode->i_mode)) {\n\t\text4_msg(sb, KERN_ERR, \"invalid journal inode\");\n\t\tiput(journal_inode);\n\t\treturn NULL;\n\t}\n\treturn journal_inode;\n}\n\nstatic journal_t *ext4_get_journal(struct super_block *sb,\n\t\t\t\t   unsigned int journal_inum)\n{\n\tstruct inode *journal_inode;\n\tjournal_t *journal;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tjournal_inode = ext4_get_journal_inode(sb, journal_inum);\n\tif (!journal_inode)\n\t\treturn NULL;\n\n\tjournal = jbd2_journal_init_inode(journal_inode);\n\tif (!journal) {\n\t\text4_msg(sb, KERN_ERR, \"Could not load journal inode\");\n\t\tiput(journal_inode);\n\t\treturn NULL;\n\t}\n\tjournal->j_private = sb;\n\text4_init_journal_params(sb, journal);\n\treturn journal;\n}\n\nstatic journal_t *ext4_get_dev_journal(struct super_block *sb,\n\t\t\t\t       dev_t j_dev)\n{\n\tstruct buffer_head *bh;\n\tjournal_t *journal;\n\text4_fsblk_t start;\n\text4_fsblk_t len;\n\tint hblock, blocksize;\n\text4_fsblk_t sb_block;\n\tunsigned long offset;\n\tstruct ext4_super_block *es;\n\tstruct block_device *bdev;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tbdev = ext4_blkdev_get(j_dev, sb);\n\tif (bdev == NULL)\n\t\treturn NULL;\n\n\tblocksize = sb->s_blocksize;\n\thblock = bdev_logical_block_size(bdev);\n\tif (blocksize < hblock) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"blocksize too small for journal device\");\n\t\tgoto out_bdev;\n\t}\n\n\tsb_block = EXT4_MIN_BLOCK_SIZE / blocksize;\n\toffset = EXT4_MIN_BLOCK_SIZE % blocksize;\n\tset_blocksize(bdev, blocksize);\n\tif (!(bh = __bread(bdev, sb_block, blocksize))) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't read superblock of \"\n\t\t       \"external journal\");\n\t\tgoto out_bdev;\n\t}\n\n\tes = (struct ext4_super_block *) (bh->b_data + offset);\n\tif ((le16_to_cpu(es->s_magic) != EXT4_SUPER_MAGIC) ||\n\t    !(le32_to_cpu(es->s_feature_incompat) &\n\t      EXT4_FEATURE_INCOMPAT_JOURNAL_DEV)) {\n\t\text4_msg(sb, KERN_ERR, \"external journal has \"\n\t\t\t\t\t\"bad superblock\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tif ((le32_to_cpu(es->s_feature_ro_compat) &\n\t     EXT4_FEATURE_RO_COMPAT_METADATA_CSUM) &&\n\t    es->s_checksum != ext4_superblock_csum(sb, es)) {\n\t\text4_msg(sb, KERN_ERR, \"external journal has \"\n\t\t\t\t       \"corrupt superblock\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tif (memcmp(EXT4_SB(sb)->s_es->s_journal_uuid, es->s_uuid, 16)) {\n\t\text4_msg(sb, KERN_ERR, \"journal UUID does not match\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tlen = ext4_blocks_count(es);\n\tstart = sb_block + 1;\n\tbrelse(bh);\t/* we're done with the superblock */\n\n\tjournal = jbd2_journal_init_dev(bdev, sb->s_bdev,\n\t\t\t\t\tstart, len, blocksize);\n\tif (!journal) {\n\t\text4_msg(sb, KERN_ERR, \"failed to create device journal\");\n\t\tgoto out_bdev;\n\t}\n\tjournal->j_private = sb;\n\tll_rw_block(REQ_OP_READ, REQ_META | REQ_PRIO, 1, &journal->j_sb_buffer);\n\twait_on_buffer(journal->j_sb_buffer);\n\tif (!buffer_uptodate(journal->j_sb_buffer)) {\n\t\text4_msg(sb, KERN_ERR, \"I/O error on journal device\");\n\t\tgoto out_journal;\n\t}\n\tif (be32_to_cpu(journal->j_superblock->s_nr_users) != 1) {\n\t\text4_msg(sb, KERN_ERR, \"External journal has more than one \"\n\t\t\t\t\t\"user (unsupported) - %d\",\n\t\t\tbe32_to_cpu(journal->j_superblock->s_nr_users));\n\t\tgoto out_journal;\n\t}\n\tEXT4_SB(sb)->journal_bdev = bdev;\n\text4_init_journal_params(sb, journal);\n\treturn journal;\n\nout_journal:\n\tjbd2_journal_destroy(journal);\nout_bdev:\n\text4_blkdev_put(bdev);\n\treturn NULL;\n}\n\nstatic int ext4_load_journal(struct super_block *sb,\n\t\t\t     struct ext4_super_block *es,\n\t\t\t     unsigned long journal_devnum)\n{\n\tjournal_t *journal;\n\tunsigned int journal_inum = le32_to_cpu(es->s_journal_inum);\n\tdev_t journal_dev;\n\tint err = 0;\n\tint really_read_only;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tif (journal_devnum &&\n\t    journal_devnum != le32_to_cpu(es->s_journal_dev)) {\n\t\text4_msg(sb, KERN_INFO, \"external journal device major/minor \"\n\t\t\t\"numbers have changed\");\n\t\tjournal_dev = new_decode_dev(journal_devnum);\n\t} else\n\t\tjournal_dev = new_decode_dev(le32_to_cpu(es->s_journal_dev));\n\n\treally_read_only = bdev_read_only(sb->s_bdev);\n\n\t/*\n\t * Are we loading a blank journal or performing recovery after a\n\t * crash?  For recovery, we need to check in advance whether we\n\t * can get read-write access to the device.\n\t */\n\tif (ext4_has_feature_journal_needs_recovery(sb)) {\n\t\tif (sb_rdonly(sb)) {\n\t\t\text4_msg(sb, KERN_INFO, \"INFO: recovery \"\n\t\t\t\t\t\"required on readonly filesystem\");\n\t\t\tif (really_read_only) {\n\t\t\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\t\t\"unavailable, cannot proceed \"\n\t\t\t\t\t\"(try mounting with noload)\");\n\t\t\t\treturn -EROFS;\n\t\t\t}\n\t\t\text4_msg(sb, KERN_INFO, \"write access will \"\n\t\t\t       \"be enabled during recovery\");\n\t\t}\n\t}\n\n\tif (journal_inum && journal_dev) {\n\t\text4_msg(sb, KERN_ERR, \"filesystem has both journal \"\n\t\t       \"and inode journals!\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (journal_inum) {\n\t\tif (!(journal = ext4_get_journal(sb, journal_inum)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (!(journal = ext4_get_dev_journal(sb, journal_dev)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!(journal->j_flags & JBD2_BARRIER))\n\t\text4_msg(sb, KERN_INFO, \"barriers disabled\");\n\n\tif (!ext4_has_feature_journal_needs_recovery(sb))\n\t\terr = jbd2_journal_wipe(journal, !really_read_only);\n\tif (!err) {\n\t\tchar *save = kmalloc(EXT4_S_ERR_LEN, GFP_KERNEL);\n\t\tif (save)\n\t\t\tmemcpy(save, ((char *) es) +\n\t\t\t       EXT4_S_ERR_START, EXT4_S_ERR_LEN);\n\t\terr = jbd2_journal_load(journal);\n\t\tif (save)\n\t\t\tmemcpy(((char *) es) + EXT4_S_ERR_START,\n\t\t\t       save, EXT4_S_ERR_LEN);\n\t\tkfree(save);\n\t}\n\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"error loading journal\");\n\t\tjbd2_journal_destroy(journal);\n\t\treturn err;\n\t}\n\n\tEXT4_SB(sb)->s_journal = journal;\n\text4_clear_journal_err(sb, es);\n\n\tif (!really_read_only && journal_devnum &&\n\t    journal_devnum != le32_to_cpu(es->s_journal_dev)) {\n\t\tes->s_journal_dev = cpu_to_le32(journal_devnum);\n\n\t\t/* Make sure we flush the recovery flag to disk. */\n\t\text4_commit_super(sb, 1);\n\t}\n\n\treturn 0;\n}\n\nstatic int ext4_commit_super(struct super_block *sb, int sync)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\tstruct buffer_head *sbh = EXT4_SB(sb)->s_sbh;\n\tint error = 0;\n\n\tif (!sbh || block_device_ejected(sb))\n\t\treturn error;\n\n\t/*\n\t * The superblock bh should be mapped, but it might not be if the\n\t * device was hot-removed. Not much we can do but fail the I/O.\n\t */\n\tif (!buffer_mapped(sbh))\n\t\treturn error;\n\n\t/*\n\t * If the file system is mounted read-only, don't update the\n\t * superblock write time.  This avoids updating the superblock\n\t * write time when we are mounting the root file system\n\t * read/only but we need to replay the journal; at that point,\n\t * for people who are east of GMT and who make their clock\n\t * tick in localtime for Windows bug-for-bug compatibility,\n\t * the clock is set in the future, and this will cause e2fsck\n\t * to complain and force a full file system check.\n\t */\n\tif (!(sb->s_flags & SB_RDONLY))\n\t\text4_update_tstamp(es, s_wtime);\n\tif (sb->s_bdev->bd_part)\n\t\tes->s_kbytes_written =\n\t\t\tcpu_to_le64(EXT4_SB(sb)->s_kbytes_written +\n\t\t\t    ((part_stat_read(sb->s_bdev->bd_part,\n\t\t\t\t\t     sectors[STAT_WRITE]) -\n\t\t\t      EXT4_SB(sb)->s_sectors_written_start) >> 1));\n\telse\n\t\tes->s_kbytes_written =\n\t\t\tcpu_to_le64(EXT4_SB(sb)->s_kbytes_written);\n\tif (percpu_counter_initialized(&EXT4_SB(sb)->s_freeclusters_counter))\n\t\text4_free_blocks_count_set(es,\n\t\t\tEXT4_C2B(EXT4_SB(sb), percpu_counter_sum_positive(\n\t\t\t\t&EXT4_SB(sb)->s_freeclusters_counter)));\n\tif (percpu_counter_initialized(&EXT4_SB(sb)->s_freeinodes_counter))\n\t\tes->s_free_inodes_count =\n\t\t\tcpu_to_le32(percpu_counter_sum_positive(\n\t\t\t\t&EXT4_SB(sb)->s_freeinodes_counter));\n\tBUFFER_TRACE(sbh, \"marking dirty\");\n\text4_superblock_csum_set(sb);\n\tif (sync)\n\t\tlock_buffer(sbh);\n\tif (buffer_write_io_error(sbh) || !buffer_uptodate(sbh)) {\n\t\t/*\n\t\t * Oh, dear.  A previous attempt to write the\n\t\t * superblock failed.  This could happen because the\n\t\t * USB device was yanked out.  Or it could happen to\n\t\t * be a transient write error and maybe the block will\n\t\t * be remapped.  Nothing we can do but to retry the\n\t\t * write and hope for the best.\n\t\t */\n\t\text4_msg(sb, KERN_ERR, \"previous I/O error to \"\n\t\t       \"superblock detected\");\n\t\tclear_buffer_write_io_error(sbh);\n\t\tset_buffer_uptodate(sbh);\n\t}\n\tmark_buffer_dirty(sbh);\n\tif (sync) {\n\t\tunlock_buffer(sbh);\n\t\terror = __sync_dirty_buffer(sbh,\n\t\t\tREQ_SYNC | (test_opt(sb, BARRIER) ? REQ_FUA : 0));\n\t\tif (buffer_write_io_error(sbh)) {\n\t\t\text4_msg(sb, KERN_ERR, \"I/O error while writing \"\n\t\t\t       \"superblock\");\n\t\t\tclear_buffer_write_io_error(sbh);\n\t\t\tset_buffer_uptodate(sbh);\n\t\t}\n\t}\n\treturn error;\n}\n\n/*\n * Have we just finished recovery?  If so, and if we are mounting (or\n * remounting) the filesystem readonly, then we will end up with a\n * consistent fs on disk.  Record that fact.\n */\nstatic void ext4_mark_recovery_complete(struct super_block *sb,\n\t\t\t\t\tstruct ext4_super_block *es)\n{\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\tif (!ext4_has_feature_journal(sb)) {\n\t\tBUG_ON(journal != NULL);\n\t\treturn;\n\t}\n\tjbd2_journal_lock_updates(journal);\n\tif (jbd2_journal_flush(journal) < 0)\n\t\tgoto out;\n\n\tif (ext4_has_feature_journal_needs_recovery(sb) && sb_rdonly(sb)) {\n\t\text4_clear_feature_journal_needs_recovery(sb);\n\t\text4_commit_super(sb, 1);\n\t}\n\nout:\n\tjbd2_journal_unlock_updates(journal);\n}\n\n/*\n * If we are mounting (or read-write remounting) a filesystem whose journal\n * has recorded an error from a previous lifetime, move that error to the\n * main filesystem now.\n */\nstatic void ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es)\n{\n\tjournal_t *journal;\n\tint j_errno;\n\tconst char *errstr;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\t/*\n\t * Now check for any error status which may have been recorded in the\n\t * journal by a prior ext4_error() or ext4_abort()\n\t */\n\n\tj_errno = jbd2_journal_errno(journal);\n\tif (j_errno) {\n\t\tchar nbuf[16];\n\n\t\terrstr = ext4_decode_error(sb, j_errno, nbuf);\n\t\text4_warning(sb, \"Filesystem error recorded \"\n\t\t\t     \"from previous mount: %s\", errstr);\n\t\text4_warning(sb, \"Marking fs in need of filesystem check.\");\n\n\t\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\t\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\t\text4_commit_super(sb, 1);\n\n\t\tjbd2_journal_clear_err(journal);\n\t\tjbd2_journal_update_sb_errno(journal);\n\t}\n}\n\n/*\n * Force the running and committing transactions to commit,\n * and wait on the commit.\n */\nint ext4_force_commit(struct super_block *sb)\n{\n\tjournal_t *journal;\n\n\tif (sb_rdonly(sb))\n\t\treturn 0;\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\treturn ext4_journal_force_commit(journal);\n}\n\nstatic int ext4_sync_fs(struct super_block *sb, int wait)\n{\n\tint ret = 0;\n\ttid_t target;\n\tbool needs_barrier = false;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (unlikely(ext4_forced_shutdown(sbi)))\n\t\treturn 0;\n\n\ttrace_ext4_sync_fs(sb, wait);\n\tflush_workqueue(sbi->rsv_conversion_wq);\n\t/*\n\t * Writeback quota in non-journalled quota case - journalled quota has\n\t * no dirty dquots\n\t */\n\tdquot_writeback_dquots(sb, -1);\n\t/*\n\t * Data writeback is possible w/o journal transaction, so barrier must\n\t * being sent at the end of the function. But we can skip it if\n\t * transaction_commit will do it for us.\n\t */\n\tif (sbi->s_journal) {\n\t\ttarget = jbd2_get_latest_transaction(sbi->s_journal);\n\t\tif (wait && sbi->s_journal->j_flags & JBD2_BARRIER &&\n\t\t    !jbd2_trans_will_send_data_barrier(sbi->s_journal, target))\n\t\t\tneeds_barrier = true;\n\n\t\tif (jbd2_journal_start_commit(sbi->s_journal, &target)) {\n\t\t\tif (wait)\n\t\t\t\tret = jbd2_log_wait_commit(sbi->s_journal,\n\t\t\t\t\t\t\t   target);\n\t\t}\n\t} else if (wait && test_opt(sb, BARRIER))\n\t\tneeds_barrier = true;\n\tif (needs_barrier) {\n\t\tint err;\n\t\terr = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);\n\t\tif (!ret)\n\t\t\tret = err;\n\t}\n\n\treturn ret;\n}\n\n/*\n * LVM calls this function before a (read-only) snapshot is created.  This\n * gives us a chance to flush the journal completely and mark the fs clean.\n *\n * Note that only this function cannot bring a filesystem to be in a clean\n * state independently. It relies on upper layer to stop all data & metadata\n * modifications.\n */\nstatic int ext4_freeze(struct super_block *sb)\n{\n\tint error = 0;\n\tjournal_t *journal;\n\n\tif (sb_rdonly(sb))\n\t\treturn 0;\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\tif (journal) {\n\t\t/* Now we set up the journal barrier. */\n\t\tjbd2_journal_lock_updates(journal);\n\n\t\t/*\n\t\t * Don't clear the needs_recovery flag if we failed to\n\t\t * flush the journal.\n\t\t */\n\t\terror = jbd2_journal_flush(journal);\n\t\tif (error < 0)\n\t\t\tgoto out;\n\n\t\t/* Journal blocked and flushed, clear needs_recovery flag. */\n\t\text4_clear_feature_journal_needs_recovery(sb);\n\t}\n\n\terror = ext4_commit_super(sb, 1);\nout:\n\tif (journal)\n\t\t/* we rely on upper layer to stop further updates */\n\t\tjbd2_journal_unlock_updates(journal);\n\treturn error;\n}\n\n/*\n * Called by LVM after the snapshot is done.  We need to reset the RECOVER\n * flag here, even though the filesystem is not technically dirty yet.\n */\nstatic int ext4_unfreeze(struct super_block *sb)\n{\n\tif (sb_rdonly(sb) || ext4_forced_shutdown(EXT4_SB(sb)))\n\t\treturn 0;\n\n\tif (EXT4_SB(sb)->s_journal) {\n\t\t/* Reset the needs_recovery flag before the fs is unlocked. */\n\t\text4_set_feature_journal_needs_recovery(sb);\n\t}\n\n\text4_commit_super(sb, 1);\n\treturn 0;\n}\n\n/*\n * Structure to save mount options for ext4_remount's benefit\n */\nstruct ext4_mount_options {\n\tunsigned long s_mount_opt;\n\tunsigned long s_mount_opt2;\n\tkuid_t s_resuid;\n\tkgid_t s_resgid;\n\tunsigned long s_commit_interval;\n\tu32 s_min_batch_time, s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\tint s_jquota_fmt;\n\tchar *s_qf_names[EXT4_MAXQUOTAS];\n#endif\n};\n\nstatic int ext4_remount(struct super_block *sb, int *flags, char *data)\n{\n\tstruct ext4_super_block *es;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tunsigned long old_sb_flags;\n\tstruct ext4_mount_options old_opts;\n\tint enable_quota = 0;\n\text4_group_t g;\n\tunsigned int journal_ioprio = DEFAULT_JOURNAL_IOPRIO;\n\tint err = 0;\n#ifdef CONFIG_QUOTA\n\tint i, j;\n\tchar *to_free[EXT4_MAXQUOTAS];\n#endif\n\tchar *orig_data = kstrdup(data, GFP_KERNEL);\n\n\tif (data && !orig_data)\n\t\treturn -ENOMEM;\n\n\t/* Store the original options */\n\told_sb_flags = sb->s_flags;\n\told_opts.s_mount_opt = sbi->s_mount_opt;\n\told_opts.s_mount_opt2 = sbi->s_mount_opt2;\n\told_opts.s_resuid = sbi->s_resuid;\n\told_opts.s_resgid = sbi->s_resgid;\n\told_opts.s_commit_interval = sbi->s_commit_interval;\n\told_opts.s_min_batch_time = sbi->s_min_batch_time;\n\told_opts.s_max_batch_time = sbi->s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\told_opts.s_jquota_fmt = sbi->s_jquota_fmt;\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tif (sbi->s_qf_names[i]) {\n\t\t\tchar *qf_name = get_qf_name(sb, sbi, i);\n\n\t\t\told_opts.s_qf_names[i] = kstrdup(qf_name, GFP_KERNEL);\n\t\t\tif (!old_opts.s_qf_names[i]) {\n\t\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\t\tkfree(old_opts.s_qf_names[j]);\n\t\t\t\tkfree(orig_data);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else\n\t\t\told_opts.s_qf_names[i] = NULL;\n#endif\n\tif (sbi->s_journal && sbi->s_journal->j_task->io_context)\n\t\tjournal_ioprio = sbi->s_journal->j_task->io_context->ioprio;\n\n\tif (!parse_options(data, sb, NULL, &journal_ioprio, 1)) {\n\t\terr = -EINVAL;\n\t\tgoto restore_opts;\n\t}\n\n\text4_clamp_want_extra_isize(sb);\n\n\tif ((old_opts.s_mount_opt & EXT4_MOUNT_JOURNAL_CHECKSUM) ^\n\t    test_opt(sb, JOURNAL_CHECKSUM)) {\n\t\text4_msg(sb, KERN_ERR, \"changing journal_checksum \"\n\t\t\t \"during remount not supported; ignoring\");\n\t\tsbi->s_mount_opt ^= EXT4_MOUNT_JOURNAL_CHECKSUM;\n\t}\n\n\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) {\n\t\tif (test_opt2(sb, EXPLICIT_DELALLOC)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and delalloc\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto restore_opts;\n\t\t}\n\t\tif (test_opt(sb, DIOREAD_NOLOCK)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and dioread_nolock\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto restore_opts;\n\t\t}\n\t\tif (test_opt(sb, DAX)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and dax\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto restore_opts;\n\t\t}\n\t} else if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA) {\n\t\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t\"journal_async_commit in data=ordered mode\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto restore_opts;\n\t\t}\n\t}\n\n\tif ((sbi->s_mount_opt ^ old_opts.s_mount_opt) & EXT4_MOUNT_NO_MBCACHE) {\n\t\text4_msg(sb, KERN_ERR, \"can't enable nombcache during remount\");\n\t\terr = -EINVAL;\n\t\tgoto restore_opts;\n\t}\n\n\tif ((sbi->s_mount_opt ^ old_opts.s_mount_opt) & EXT4_MOUNT_DAX) {\n\t\text4_msg(sb, KERN_WARNING, \"warning: refusing change of \"\n\t\t\t\"dax flag with busy inodes while remounting\");\n\t\tsbi->s_mount_opt ^= EXT4_MOUNT_DAX;\n\t}\n\n\tif (sbi->s_mount_flags & EXT4_MF_FS_ABORTED)\n\t\text4_abort(sb, \"Abort forced by user\");\n\n\tsb->s_flags = (sb->s_flags & ~SB_POSIXACL) |\n\t\t(test_opt(sb, POSIX_ACL) ? SB_POSIXACL : 0);\n\n\tes = sbi->s_es;\n\n\tif (sbi->s_journal) {\n\t\text4_init_journal_params(sb, sbi->s_journal);\n\t\tset_task_ioprio(sbi->s_journal->j_task, journal_ioprio);\n\t}\n\n\tif (*flags & SB_LAZYTIME)\n\t\tsb->s_flags |= SB_LAZYTIME;\n\n\tif ((bool)(*flags & SB_RDONLY) != sb_rdonly(sb)) {\n\t\tif (sbi->s_mount_flags & EXT4_MF_FS_ABORTED) {\n\t\t\terr = -EROFS;\n\t\t\tgoto restore_opts;\n\t\t}\n\n\t\tif (*flags & SB_RDONLY) {\n\t\t\terr = sync_filesystem(sb);\n\t\t\tif (err < 0)\n\t\t\t\tgoto restore_opts;\n\t\t\terr = dquot_suspend(sb, -1);\n\t\t\tif (err < 0)\n\t\t\t\tgoto restore_opts;\n\n\t\t\t/*\n\t\t\t * First of all, the unconditional stuff we have to do\n\t\t\t * to disable replay of the journal when we next remount\n\t\t\t */\n\t\t\tsb->s_flags |= SB_RDONLY;\n\n\t\t\t/*\n\t\t\t * OK, test if we are remounting a valid rw partition\n\t\t\t * readonly, and if so set the rdonly flag and then\n\t\t\t * mark the partition as valid again.\n\t\t\t */\n\t\t\tif (!(es->s_state & cpu_to_le16(EXT4_VALID_FS)) &&\n\t\t\t    (sbi->s_mount_state & EXT4_VALID_FS))\n\t\t\t\tes->s_state = cpu_to_le16(sbi->s_mount_state);\n\n\t\t\tif (sbi->s_journal)\n\t\t\t\text4_mark_recovery_complete(sb, es);\n\t\t\tif (sbi->s_mmp_tsk)\n\t\t\t\tkthread_stop(sbi->s_mmp_tsk);\n\t\t} else {\n\t\t\t/* Make sure we can mount this feature set readwrite */\n\t\t\tif (ext4_has_feature_readonly(sb) ||\n\t\t\t    !ext4_feature_set_ok(sb, 0)) {\n\t\t\t\terr = -EROFS;\n\t\t\t\tgoto restore_opts;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Make sure the group descriptor checksums\n\t\t\t * are sane.  If they aren't, refuse to remount r/w.\n\t\t\t */\n\t\t\tfor (g = 0; g < sbi->s_groups_count; g++) {\n\t\t\t\tstruct ext4_group_desc *gdp =\n\t\t\t\t\text4_get_group_desc(sb, g, NULL);\n\n\t\t\t\tif (!ext4_group_desc_csum_verify(sb, g, gdp)) {\n\t\t\t\t\text4_msg(sb, KERN_ERR,\n\t       \"ext4_remount: Checksum for group %u failed (%u!=%u)\",\n\t\tg, le16_to_cpu(ext4_group_desc_csum(sb, g, gdp)),\n\t\t\t\t\t       le16_to_cpu(gdp->bg_checksum));\n\t\t\t\t\terr = -EFSBADCRC;\n\t\t\t\t\tgoto restore_opts;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If we have an unprocessed orphan list hanging\n\t\t\t * around from a previously readonly bdev mount,\n\t\t\t * require a full umount/remount for now.\n\t\t\t */\n\t\t\tif (es->s_last_orphan) {\n\t\t\t\text4_msg(sb, KERN_WARNING, \"Couldn't \"\n\t\t\t\t       \"remount RDWR because of unprocessed \"\n\t\t\t\t       \"orphan inode list.  Please \"\n\t\t\t\t       \"umount/remount instead\");\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto restore_opts;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Mounting a RDONLY partition read-write, so reread\n\t\t\t * and store the current valid flag.  (It may have\n\t\t\t * been changed by e2fsck since we originally mounted\n\t\t\t * the partition.)\n\t\t\t */\n\t\t\tif (sbi->s_journal)\n\t\t\t\text4_clear_journal_err(sb, es);\n\t\t\tsbi->s_mount_state = le16_to_cpu(es->s_state);\n\n\t\t\terr = ext4_setup_super(sb, es, 0);\n\t\t\tif (err)\n\t\t\t\tgoto restore_opts;\n\n\t\t\tsb->s_flags &= ~SB_RDONLY;\n\t\t\tif (ext4_has_feature_mmp(sb))\n\t\t\t\tif (ext4_multi_mount_protect(sb,\n\t\t\t\t\t\tle64_to_cpu(es->s_mmp_block))) {\n\t\t\t\t\terr = -EROFS;\n\t\t\t\t\tgoto restore_opts;\n\t\t\t\t}\n\t\t\tenable_quota = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Reinitialize lazy itable initialization thread based on\n\t * current settings\n\t */\n\tif (sb_rdonly(sb) || !test_opt(sb, INIT_INODE_TABLE))\n\t\text4_unregister_li_request(sb);\n\telse {\n\t\text4_group_t first_not_zeroed;\n\t\tfirst_not_zeroed = ext4_has_uninit_itable(sb);\n\t\text4_register_li_request(sb, first_not_zeroed);\n\t}\n\n\text4_setup_system_zone(sb);\n\tif (sbi->s_journal == NULL && !(old_sb_flags & SB_RDONLY)) {\n\t\terr = ext4_commit_super(sb, 1);\n\t\tif (err)\n\t\t\tgoto restore_opts;\n\t}\n\n#ifdef CONFIG_QUOTA\n\t/* Release old quota file names */\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tkfree(old_opts.s_qf_names[i]);\n\tif (enable_quota) {\n\t\tif (sb_any_quota_suspended(sb))\n\t\t\tdquot_resume(sb, -1);\n\t\telse if (ext4_has_feature_quota(sb)) {\n\t\t\terr = ext4_enable_quotas(sb);\n\t\t\tif (err)\n\t\t\t\tgoto restore_opts;\n\t\t}\n\t}\n#endif\n\n\t*flags = (*flags & ~SB_LAZYTIME) | (sb->s_flags & SB_LAZYTIME);\n\text4_msg(sb, KERN_INFO, \"re-mounted. Opts: %s\", orig_data);\n\tkfree(orig_data);\n\treturn 0;\n\nrestore_opts:\n\tsb->s_flags = old_sb_flags;\n\tsbi->s_mount_opt = old_opts.s_mount_opt;\n\tsbi->s_mount_opt2 = old_opts.s_mount_opt2;\n\tsbi->s_resuid = old_opts.s_resuid;\n\tsbi->s_resgid = old_opts.s_resgid;\n\tsbi->s_commit_interval = old_opts.s_commit_interval;\n\tsbi->s_min_batch_time = old_opts.s_min_batch_time;\n\tsbi->s_max_batch_time = old_opts.s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\tsbi->s_jquota_fmt = old_opts.s_jquota_fmt;\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\tto_free[i] = get_qf_name(sb, sbi, i);\n\t\trcu_assign_pointer(sbi->s_qf_names[i], old_opts.s_qf_names[i]);\n\t}\n\tsynchronize_rcu();\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tkfree(to_free[i]);\n#endif\n\tkfree(orig_data);\n\treturn err;\n}\n\n#ifdef CONFIG_QUOTA\nstatic int ext4_statfs_project(struct super_block *sb,\n\t\t\t       kprojid_t projid, struct kstatfs *buf)\n{\n\tstruct kqid qid;\n\tstruct dquot *dquot;\n\tu64 limit;\n\tu64 curblock;\n\n\tqid = make_kqid_projid(projid);\n\tdquot = dqget(sb, qid);\n\tif (IS_ERR(dquot))\n\t\treturn PTR_ERR(dquot);\n\tspin_lock(&dquot->dq_dqb_lock);\n\n\tlimit = (dquot->dq_dqb.dqb_bsoftlimit ?\n\t\t dquot->dq_dqb.dqb_bsoftlimit :\n\t\t dquot->dq_dqb.dqb_bhardlimit) >> sb->s_blocksize_bits;\n\tif (limit && buf->f_blocks > limit) {\n\t\tcurblock = (dquot->dq_dqb.dqb_curspace +\n\t\t\t    dquot->dq_dqb.dqb_rsvspace) >> sb->s_blocksize_bits;\n\t\tbuf->f_blocks = limit;\n\t\tbuf->f_bfree = buf->f_bavail =\n\t\t\t(buf->f_blocks > curblock) ?\n\t\t\t (buf->f_blocks - curblock) : 0;\n\t}\n\n\tlimit = dquot->dq_dqb.dqb_isoftlimit ?\n\t\tdquot->dq_dqb.dqb_isoftlimit :\n\t\tdquot->dq_dqb.dqb_ihardlimit;\n\tif (limit && buf->f_files > limit) {\n\t\tbuf->f_files = limit;\n\t\tbuf->f_ffree =\n\t\t\t(buf->f_files > dquot->dq_dqb.dqb_curinodes) ?\n\t\t\t (buf->f_files - dquot->dq_dqb.dqb_curinodes) : 0;\n\t}\n\n\tspin_unlock(&dquot->dq_dqb_lock);\n\tdqput(dquot);\n\treturn 0;\n}\n#endif\n\nstatic int ext4_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct super_block *sb = dentry->d_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\text4_fsblk_t overhead = 0, resv_blocks;\n\tu64 fsid;\n\ts64 bfree;\n\tresv_blocks = EXT4_C2B(sbi, atomic64_read(&sbi->s_resv_clusters));\n\n\tif (!test_opt(sb, MINIX_DF))\n\t\toverhead = sbi->s_overhead;\n\n\tbuf->f_type = EXT4_SUPER_MAGIC;\n\tbuf->f_bsize = sb->s_blocksize;\n\tbuf->f_blocks = ext4_blocks_count(es) - EXT4_C2B(sbi, overhead);\n\tbfree = percpu_counter_sum_positive(&sbi->s_freeclusters_counter) -\n\t\tpercpu_counter_sum_positive(&sbi->s_dirtyclusters_counter);\n\t/* prevent underflow in case that few free space is available */\n\tbuf->f_bfree = EXT4_C2B(sbi, max_t(s64, bfree, 0));\n\tbuf->f_bavail = buf->f_bfree -\n\t\t\t(ext4_r_blocks_count(es) + resv_blocks);\n\tif (buf->f_bfree < (ext4_r_blocks_count(es) + resv_blocks))\n\t\tbuf->f_bavail = 0;\n\tbuf->f_files = le32_to_cpu(es->s_inodes_count);\n\tbuf->f_ffree = percpu_counter_sum_positive(&sbi->s_freeinodes_counter);\n\tbuf->f_namelen = EXT4_NAME_LEN;\n\tfsid = le64_to_cpup((void *)es->s_uuid) ^\n\t       le64_to_cpup((void *)es->s_uuid + sizeof(u64));\n\tbuf->f_fsid.val[0] = fsid & 0xFFFFFFFFUL;\n\tbuf->f_fsid.val[1] = (fsid >> 32) & 0xFFFFFFFFUL;\n\n#ifdef CONFIG_QUOTA\n\tif (ext4_test_inode_flag(dentry->d_inode, EXT4_INODE_PROJINHERIT) &&\n\t    sb_has_quota_limits_enabled(sb, PRJQUOTA))\n\t\text4_statfs_project(sb, EXT4_I(dentry->d_inode)->i_projid, buf);\n#endif\n\treturn 0;\n}\n\n\n#ifdef CONFIG_QUOTA\n\n/*\n * Helper functions so that transaction is started before we acquire dqio_sem\n * to keep correct lock ordering of transaction > dqio_sem\n */\nstatic inline struct inode *dquot_to_inode(struct dquot *dquot)\n{\n\treturn sb_dqopt(dquot->dq_sb)->files[dquot->dq_id.type];\n}\n\nstatic int ext4_write_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\tstruct inode *inode;\n\n\tinode = dquot_to_inode(dquot);\n\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA,\n\t\t\t\t    EXT4_QUOTA_TRANS_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_commit(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_acquire_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(dquot_to_inode(dquot), EXT4_HT_QUOTA,\n\t\t\t\t    EXT4_QUOTA_INIT_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_acquire(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_release_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(dquot_to_inode(dquot), EXT4_HT_QUOTA,\n\t\t\t\t    EXT4_QUOTA_DEL_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle)) {\n\t\t/* Release dquot anyway to avoid endless cycle in dqput() */\n\t\tdquot_release(dquot);\n\t\treturn PTR_ERR(handle);\n\t}\n\tret = dquot_release(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_mark_dquot_dirty(struct dquot *dquot)\n{\n\tstruct super_block *sb = dquot->dq_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\t/* Are we journaling quotas? */\n\tif (ext4_has_feature_quota(sb) ||\n\t    sbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA]) {\n\t\tdquot_mark_dquot_dirty(dquot);\n\t\treturn ext4_write_dquot(dquot);\n\t} else {\n\t\treturn dquot_mark_dquot_dirty(dquot);\n\t}\n}\n\nstatic int ext4_write_info(struct super_block *sb, int type)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\t/* Data block + inode block */\n\thandle = ext4_journal_start(d_inode(sb->s_root), EXT4_HT_QUOTA, 2);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_commit_info(sb, type);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\n/*\n * Turn on quotas during mount time - we need to find\n * the quota file and such...\n */\nstatic int ext4_quota_on_mount(struct super_block *sb, int type)\n{\n\treturn dquot_quota_on_mount(sb, get_qf_name(sb, EXT4_SB(sb), type),\n\t\t\t\t\tEXT4_SB(sb)->s_jquota_fmt, type);\n}\n\nstatic void lockdep_set_quota_inode(struct inode *inode, int subclass)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\t/* The first argument of lockdep_set_subclass has to be\n\t * *exactly* the same as the argument to init_rwsem() --- in\n\t * this case, in init_once() --- or lockdep gets unhappy\n\t * because the name of the lock is set using the\n\t * stringification of the argument to init_rwsem().\n\t */\n\t(void) ei;\t/* shut up clang warning if !CONFIG_LOCKDEP */\n\tlockdep_set_subclass(&ei->i_data_sem, subclass);\n}\n\n/*\n * Standard function to be called on quota_on\n */\nstatic int ext4_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t const struct path *path)\n{\n\tint err;\n\n\tif (!test_opt(sb, QUOTA))\n\t\treturn -EINVAL;\n\n\t/* Quotafile not on the same filesystem? */\n\tif (path->dentry->d_sb != sb)\n\t\treturn -EXDEV;\n\t/* Journaling quota? */\n\tif (EXT4_SB(sb)->s_qf_names[type]) {\n\t\t/* Quotafile not in fs root? */\n\t\tif (path->dentry->d_parent != sb->s_root)\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t\"Quota file not on filesystem root. \"\n\t\t\t\t\"Journaled quota will not work\");\n\t\tsb_dqopt(sb)->flags |= DQUOT_NOLIST_DIRTY;\n\t} else {\n\t\t/*\n\t\t * Clear the flag just in case mount options changed since\n\t\t * last time.\n\t\t */\n\t\tsb_dqopt(sb)->flags &= ~DQUOT_NOLIST_DIRTY;\n\t}\n\n\t/*\n\t * When we journal data on quota file, we have to flush journal to see\n\t * all updates to the file when we bypass pagecache...\n\t */\n\tif (EXT4_SB(sb)->s_journal &&\n\t    ext4_should_journal_data(d_inode(path->dentry))) {\n\t\t/*\n\t\t * We don't need to lock updates but journal_flush() could\n\t\t * otherwise be livelocked...\n\t\t */\n\t\tjbd2_journal_lock_updates(EXT4_SB(sb)->s_journal);\n\t\terr = jbd2_journal_flush(EXT4_SB(sb)->s_journal);\n\t\tjbd2_journal_unlock_updates(EXT4_SB(sb)->s_journal);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tlockdep_set_quota_inode(path->dentry->d_inode, I_DATA_SEM_QUOTA);\n\terr = dquot_quota_on(sb, type, format_id, path);\n\tif (err) {\n\t\tlockdep_set_quota_inode(path->dentry->d_inode,\n\t\t\t\t\t     I_DATA_SEM_NORMAL);\n\t} else {\n\t\tstruct inode *inode = d_inode(path->dentry);\n\t\thandle_t *handle;\n\n\t\t/*\n\t\t * Set inode flags to prevent userspace from messing with quota\n\t\t * files. If this fails, we return success anyway since quotas\n\t\t * are already enabled and this is not a hard failure.\n\t\t */\n\t\tinode_lock(inode);\n\t\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA, 1);\n\t\tif (IS_ERR(handle))\n\t\t\tgoto unlock_inode;\n\t\tEXT4_I(inode)->i_flags |= EXT4_NOATIME_FL | EXT4_IMMUTABLE_FL;\n\t\tinode_set_flags(inode, S_NOATIME | S_IMMUTABLE,\n\t\t\t\tS_NOATIME | S_IMMUTABLE);\n\t\text4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\tunlock_inode:\n\t\tinode_unlock(inode);\n\t}\n\treturn err;\n}\n\nstatic int ext4_quota_enable(struct super_block *sb, int type, int format_id,\n\t\t\t     unsigned int flags)\n{\n\tint err;\n\tstruct inode *qf_inode;\n\tunsigned long qf_inums[EXT4_MAXQUOTAS] = {\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_usr_quota_inum),\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_grp_quota_inum),\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_prj_quota_inum)\n\t};\n\n\tBUG_ON(!ext4_has_feature_quota(sb));\n\n\tif (!qf_inums[type])\n\t\treturn -EPERM;\n\n\tqf_inode = ext4_iget(sb, qf_inums[type], EXT4_IGET_SPECIAL);\n\tif (IS_ERR(qf_inode)) {\n\t\text4_error(sb, \"Bad quota inode # %lu\", qf_inums[type]);\n\t\treturn PTR_ERR(qf_inode);\n\t}\n\n\t/* Don't account quota for quota files to avoid recursion */\n\tqf_inode->i_flags |= S_NOQUOTA;\n\tlockdep_set_quota_inode(qf_inode, I_DATA_SEM_QUOTA);\n\terr = dquot_enable(qf_inode, type, format_id, flags);\n\tif (err)\n\t\tlockdep_set_quota_inode(qf_inode, I_DATA_SEM_NORMAL);\n\tiput(qf_inode);\n\n\treturn err;\n}\n\n/* Enable usage tracking for all quota types. */\nstatic int ext4_enable_quotas(struct super_block *sb)\n{\n\tint type, err = 0;\n\tunsigned long qf_inums[EXT4_MAXQUOTAS] = {\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_usr_quota_inum),\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_grp_quota_inum),\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_prj_quota_inum)\n\t};\n\tbool quota_mopt[EXT4_MAXQUOTAS] = {\n\t\ttest_opt(sb, USRQUOTA),\n\t\ttest_opt(sb, GRPQUOTA),\n\t\ttest_opt(sb, PRJQUOTA),\n\t};\n\n\tsb_dqopt(sb)->flags |= DQUOT_QUOTA_SYS_FILE | DQUOT_NOLIST_DIRTY;\n\tfor (type = 0; type < EXT4_MAXQUOTAS; type++) {\n\t\tif (qf_inums[type]) {\n\t\t\terr = ext4_quota_enable(sb, type, QFMT_VFS_V1,\n\t\t\t\tDQUOT_USAGE_ENABLED |\n\t\t\t\t(quota_mopt[type] ? DQUOT_LIMITS_ENABLED : 0));\n\t\t\tif (err) {\n\t\t\t\text4_warning(sb,\n\t\t\t\t\t\"Failed to enable quota tracking \"\n\t\t\t\t\t\"(type=%d, err=%d). Please run \"\n\t\t\t\t\t\"e2fsck to fix.\", type, err);\n\t\t\t\tfor (type--; type >= 0; type--)\n\t\t\t\t\tdquot_quota_off(sb, type);\n\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int ext4_quota_off(struct super_block *sb, int type)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\thandle_t *handle;\n\tint err;\n\n\t/* Force all delayed allocation blocks to be allocated.\n\t * Caller already holds s_umount sem */\n\tif (test_opt(sb, DELALLOC))\n\t\tsync_filesystem(sb);\n\n\tif (!inode || !igrab(inode))\n\t\tgoto out;\n\n\terr = dquot_quota_off(sb, type);\n\tif (err || ext4_has_feature_quota(sb))\n\t\tgoto out_put;\n\n\tinode_lock(inode);\n\t/*\n\t * Update modification times of quota files when userspace can\n\t * start looking at them. If we fail, we return success anyway since\n\t * this is not a hard failure and quotas are already disabled.\n\t */\n\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA, 1);\n\tif (IS_ERR(handle))\n\t\tgoto out_unlock;\n\tEXT4_I(inode)->i_flags &= ~(EXT4_NOATIME_FL | EXT4_IMMUTABLE_FL);\n\tinode_set_flags(inode, 0, S_NOATIME | S_IMMUTABLE);\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\nout_unlock:\n\tinode_unlock(inode);\nout_put:\n\tlockdep_set_quota_inode(inode, I_DATA_SEM_NORMAL);\n\tiput(inode);\n\treturn err;\nout:\n\treturn dquot_quota_off(sb, type);\n}\n\n/* Read data from quotafile - avoid pagecache and such because we cannot afford\n * acquiring the locks... As quota files are never truncated and quota code\n * itself serializes the operations (and no one else should touch the files)\n * we don't have to be afraid of races */\nstatic ssize_t ext4_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\text4_lblk_t blk = off >> EXT4_BLOCK_SIZE_BITS(sb);\n\tint offset = off & (sb->s_blocksize - 1);\n\tint tocopy;\n\tsize_t toread;\n\tstruct buffer_head *bh;\n\tloff_t i_size = i_size_read(inode);\n\n\tif (off > i_size)\n\t\treturn 0;\n\tif (off+len > i_size)\n\t\tlen = i_size-off;\n\ttoread = len;\n\twhile (toread > 0) {\n\t\ttocopy = sb->s_blocksize - offset < toread ?\n\t\t\t\tsb->s_blocksize - offset : toread;\n\t\tbh = ext4_bread(NULL, inode, blk, 0);\n\t\tif (IS_ERR(bh))\n\t\t\treturn PTR_ERR(bh);\n\t\tif (!bh)\t/* A hole? */\n\t\t\tmemset(data, 0, tocopy);\n\t\telse\n\t\t\tmemcpy(data, bh->b_data+offset, tocopy);\n\t\tbrelse(bh);\n\t\toffset = 0;\n\t\ttoread -= tocopy;\n\t\tdata += tocopy;\n\t\tblk++;\n\t}\n\treturn len;\n}\n\n/* Write to quotafile (we know the transaction is already started and has\n * enough credits) */\nstatic ssize_t ext4_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\text4_lblk_t blk = off >> EXT4_BLOCK_SIZE_BITS(sb);\n\tint err, offset = off & (sb->s_blocksize - 1);\n\tint retries = 0;\n\tstruct buffer_head *bh;\n\thandle_t *handle = journal_current_handle();\n\n\tif (EXT4_SB(sb)->s_journal && !handle) {\n\t\text4_msg(sb, KERN_WARNING, \"Quota write (off=%llu, len=%llu)\"\n\t\t\t\" cancelled because transaction is not started\",\n\t\t\t(unsigned long long)off, (unsigned long long)len);\n\t\treturn -EIO;\n\t}\n\t/*\n\t * Since we account only one data block in transaction credits,\n\t * then it is impossible to cross a block boundary.\n\t */\n\tif (sb->s_blocksize - offset < len) {\n\t\text4_msg(sb, KERN_WARNING, \"Quota write (off=%llu, len=%llu)\"\n\t\t\t\" cancelled because not block aligned\",\n\t\t\t(unsigned long long)off, (unsigned long long)len);\n\t\treturn -EIO;\n\t}\n\n\tdo {\n\t\tbh = ext4_bread(handle, inode, blk,\n\t\t\t\tEXT4_GET_BLOCKS_CREATE |\n\t\t\t\tEXT4_GET_BLOCKS_METADATA_NOFAIL);\n\t} while (IS_ERR(bh) && (PTR_ERR(bh) == -ENOSPC) &&\n\t\t ext4_should_retry_alloc(inode->i_sb, &retries));\n\tif (IS_ERR(bh))\n\t\treturn PTR_ERR(bh);\n\tif (!bh)\n\t\tgoto out;\n\tBUFFER_TRACE(bh, \"get write access\");\n\terr = ext4_journal_get_write_access(handle, bh);\n\tif (err) {\n\t\tbrelse(bh);\n\t\treturn err;\n\t}\n\tlock_buffer(bh);\n\tmemcpy(bh->b_data+offset, data, len);\n\tflush_dcache_page(bh->b_page);\n\tunlock_buffer(bh);\n\terr = ext4_handle_dirty_metadata(handle, NULL, bh);\n\tbrelse(bh);\nout:\n\tif (inode->i_size < off + len) {\n\t\ti_size_write(inode, off + len);\n\t\tEXT4_I(inode)->i_disksize = inode->i_size;\n\t\text4_mark_inode_dirty(handle, inode);\n\t}\n\treturn len;\n}\n\nstatic int ext4_get_next_id(struct super_block *sb, struct kqid *qid)\n{\n\tconst struct quota_format_ops\t*ops;\n\n\tif (!sb_has_quota_loaded(sb, qid->type))\n\t\treturn -ESRCH;\n\tops = sb_dqopt(sb)->ops[qid->type];\n\tif (!ops || !ops->get_next_id)\n\t\treturn -ENOSYS;\n\treturn dquot_get_next_id(sb, qid);\n}\n#endif\n\nstatic struct dentry *ext4_mount(struct file_system_type *fs_type, int flags,\n\t\t       const char *dev_name, void *data)\n{\n\treturn mount_bdev(fs_type, flags, dev_name, data, ext4_fill_super);\n}\n\n#if !defined(CONFIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT2)\nstatic inline void register_as_ext2(void)\n{\n\tint err = register_filesystem(&ext2_fs_type);\n\tif (err)\n\t\tprintk(KERN_WARNING\n\t\t       \"EXT4-fs: Unable to register as ext2 (%d)\\n\", err);\n}\n\nstatic inline void unregister_as_ext2(void)\n{\n\tunregister_filesystem(&ext2_fs_type);\n}\n\nstatic inline int ext2_feature_set_ok(struct super_block *sb)\n{\n\tif (ext4_has_unknown_ext2_incompat_features(sb))\n\t\treturn 0;\n\tif (sb_rdonly(sb))\n\t\treturn 1;\n\tif (ext4_has_unknown_ext2_ro_compat_features(sb))\n\t\treturn 0;\n\treturn 1;\n}\n#else\nstatic inline void register_as_ext2(void) { }\nstatic inline void unregister_as_ext2(void) { }\nstatic inline int ext2_feature_set_ok(struct super_block *sb) { return 0; }\n#endif\n\nstatic inline void register_as_ext3(void)\n{\n\tint err = register_filesystem(&ext3_fs_type);\n\tif (err)\n\t\tprintk(KERN_WARNING\n\t\t       \"EXT4-fs: Unable to register as ext3 (%d)\\n\", err);\n}\n\nstatic inline void unregister_as_ext3(void)\n{\n\tunregister_filesystem(&ext3_fs_type);\n}\n\nstatic inline int ext3_feature_set_ok(struct super_block *sb)\n{\n\tif (ext4_has_unknown_ext3_incompat_features(sb))\n\t\treturn 0;\n\tif (!ext4_has_feature_journal(sb))\n\t\treturn 0;\n\tif (sb_rdonly(sb))\n\t\treturn 1;\n\tif (ext4_has_unknown_ext3_ro_compat_features(sb))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic struct file_system_type ext4_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext4\",\n\t.mount\t\t= ext4_mount,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"ext4\");\n\n/* Shared across all ext4 file systems */\nwait_queue_head_t ext4__ioend_wq[EXT4_WQ_HASH_SZ];\n\nstatic int __init ext4_init_fs(void)\n{\n\tint i, err;\n\n\tratelimit_state_init(&ext4_mount_msg_ratelimit, 30 * HZ, 64);\n\text4_li_info = NULL;\n\tmutex_init(&ext4_li_mtx);\n\n\t/* Build-time check for flags consistency */\n\text4_check_flag_values();\n\n\tfor (i = 0; i < EXT4_WQ_HASH_SZ; i++)\n\t\tinit_waitqueue_head(&ext4__ioend_wq[i]);\n\n\terr = ext4_init_es();\n\tif (err)\n\t\treturn err;\n\n\terr = ext4_init_pending();\n\tif (err)\n\t\tgoto out7;\n\n\terr = ext4_init_post_read_processing();\n\tif (err)\n\t\tgoto out6;\n\n\terr = ext4_init_pageio();\n\tif (err)\n\t\tgoto out5;\n\n\terr = ext4_init_system_zone();\n\tif (err)\n\t\tgoto out4;\n\n\terr = ext4_init_sysfs();\n\tif (err)\n\t\tgoto out3;\n\n\terr = ext4_init_mballoc();\n\tif (err)\n\t\tgoto out2;\n\terr = init_inodecache();\n\tif (err)\n\t\tgoto out1;\n\tregister_as_ext3();\n\tregister_as_ext2();\n\terr = register_filesystem(&ext4_fs_type);\n\tif (err)\n\t\tgoto out;\n\n\treturn 0;\nout:\n\tunregister_as_ext2();\n\tunregister_as_ext3();\n\tdestroy_inodecache();\nout1:\n\text4_exit_mballoc();\nout2:\n\text4_exit_sysfs();\nout3:\n\text4_exit_system_zone();\nout4:\n\text4_exit_pageio();\nout5:\n\text4_exit_post_read_processing();\nout6:\n\text4_exit_pending();\nout7:\n\text4_exit_es();\n\n\treturn err;\n}\n\nstatic void __exit ext4_exit_fs(void)\n{\n\text4_destroy_lazyinit_thread();\n\tunregister_as_ext2();\n\tunregister_as_ext3();\n\tunregister_filesystem(&ext4_fs_type);\n\tdestroy_inodecache();\n\text4_exit_mballoc();\n\text4_exit_sysfs();\n\text4_exit_system_zone();\n\text4_exit_pageio();\n\text4_exit_post_read_processing();\n\text4_exit_es();\n\text4_exit_pending();\n}\n\nMODULE_AUTHOR(\"Remy Card, Stephen Tweedie, Andrew Morton, Andreas Dilger, Theodore Ts'o and others\");\nMODULE_DESCRIPTION(\"Fourth Extended Filesystem\");\nMODULE_LICENSE(\"GPL\");\nMODULE_SOFTDEP(\"pre: crc32c\");\nmodule_init(ext4_init_fs)\nmodule_exit(ext4_exit_fs)\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/ext4/inode.c\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/fs/minix/inode.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  64-bit file support on 64-bit platforms by Jakub Jelinek\n *\t(jj@sunsite.ms.mff.cuni.cz)\n *\n *  Assorted race fixes, rewrite of ext4_get_block() by Al Viro, 2000\n */\n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/dax.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/buffer_head.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/mpage.h>\n#include <linux/namei.h>\n#include <linux/uio.h>\n#include <linux/bio.h>\n#include <linux/workqueue.h>\n#include <linux/kernel.h>\n#include <linux/printk.h>\n#include <linux/slab.h>\n#include <linux/bitops.h>\n#include <linux/iomap.h>\n#include <linux/iversion.h>\n\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n#include \"truncate.h\"\n\n#include <trace/events/ext4.h>\n\n#define MPAGE_DA_EXTENT_TAIL 0x01\n\nstatic __u32 ext4_inode_csum(struct inode *inode, struct ext4_inode *raw,\n\t\t\t      struct ext4_inode_info *ei)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\t__u16 dummy_csum = 0;\n\tint offset = offsetof(struct ext4_inode, i_checksum_lo);\n\tunsigned int csum_size = sizeof(dummy_csum);\n\n\tcsum = ext4_chksum(sbi, ei->i_csum_seed, (__u8 *)raw, offset);\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)&dummy_csum, csum_size);\n\toffset += csum_size;\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)raw + offset,\n\t\t\t   EXT4_GOOD_OLD_INODE_SIZE - offset);\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\toffset = offsetof(struct ext4_inode, i_checksum_hi);\n\t\tcsum = ext4_chksum(sbi, csum, (__u8 *)raw +\n\t\t\t\t   EXT4_GOOD_OLD_INODE_SIZE,\n\t\t\t\t   offset - EXT4_GOOD_OLD_INODE_SIZE);\n\t\tif (EXT4_FITS_IN_INODE(raw, ei, i_checksum_hi)) {\n\t\t\tcsum = ext4_chksum(sbi, csum, (__u8 *)&dummy_csum,\n\t\t\t\t\t   csum_size);\n\t\t\toffset += csum_size;\n\t\t}\n\t\tcsum = ext4_chksum(sbi, csum, (__u8 *)raw + offset,\n\t\t\t\t   EXT4_INODE_SIZE(inode->i_sb) - offset);\n\t}\n\n\treturn csum;\n}\n\nstatic int ext4_inode_csum_verify(struct inode *inode, struct ext4_inode *raw,\n\t\t\t\t  struct ext4_inode_info *ei)\n{\n\t__u32 provided, calculated;\n\n\tif (EXT4_SB(inode->i_sb)->s_es->s_creator_os !=\n\t    cpu_to_le32(EXT4_OS_LINUX) ||\n\t    !ext4_has_metadata_csum(inode->i_sb))\n\t\treturn 1;\n\n\tprovided = le16_to_cpu(raw->i_checksum_lo);\n\tcalculated = ext4_inode_csum(inode, raw, ei);\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw, ei, i_checksum_hi))\n\t\tprovided |= ((__u32)le16_to_cpu(raw->i_checksum_hi)) << 16;\n\telse\n\t\tcalculated &= 0xFFFF;\n\n\treturn provided == calculated;\n}\n\nstatic void ext4_inode_csum_set(struct inode *inode, struct ext4_inode *raw,\n\t\t\t\tstruct ext4_inode_info *ei)\n{\n\t__u32 csum;\n\n\tif (EXT4_SB(inode->i_sb)->s_es->s_creator_os !=\n\t    cpu_to_le32(EXT4_OS_LINUX) ||\n\t    !ext4_has_metadata_csum(inode->i_sb))\n\t\treturn;\n\n\tcsum = ext4_inode_csum(inode, raw, ei);\n\traw->i_checksum_lo = cpu_to_le16(csum & 0xFFFF);\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw, ei, i_checksum_hi))\n\t\traw->i_checksum_hi = cpu_to_le16(csum >> 16);\n}\n\nstatic inline int ext4_begin_ordered_truncate(struct inode *inode,\n\t\t\t\t\t      loff_t new_size)\n{\n\ttrace_ext4_begin_ordered_truncate(inode, new_size);\n\t/*\n\t * If jinode is zero, then we never opened the file for\n\t * writing, so there's no need to call\n\t * jbd2_journal_begin_ordered_truncate() since there's no\n\t * outstanding writes we need to flush.\n\t */\n\tif (!EXT4_I(inode)->jinode)\n\t\treturn 0;\n\treturn jbd2_journal_begin_ordered_truncate(EXT4_JOURNAL(inode),\n\t\t\t\t\t\t   EXT4_I(inode)->jinode,\n\t\t\t\t\t\t   new_size);\n}\n\nstatic void ext4_invalidatepage(struct page *page, unsigned int offset,\n\t\t\t\tunsigned int length);\nstatic int __ext4_journalled_writepage(struct page *page, unsigned int len);\nstatic int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh);\nstatic int ext4_meta_trans_blocks(struct inode *inode, int lblocks,\n\t\t\t\t  int pextents);\n\n/*\n * Test whether an inode is a fast symlink.\n * A fast symlink has its symlink data stored in ext4_inode_info->i_data.\n */\nint ext4_inode_is_fast_symlink(struct inode *inode)\n{\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL)) {\n\t\tint ea_blocks = EXT4_I(inode)->i_file_acl ?\n\t\t\t\tEXT4_CLUSTER_SIZE(inode->i_sb) >> 9 : 0;\n\n\t\tif (ext4_has_inline_data(inode))\n\t\t\treturn 0;\n\n\t\treturn (S_ISLNK(inode->i_mode) && inode->i_blocks - ea_blocks == 0);\n\t}\n\treturn S_ISLNK(inode->i_mode) && inode->i_size &&\n\t       (inode->i_size < EXT4_N_BLOCKS * 4);\n}\n\n/*\n * Called at the last iput() if i_nlink is zero.\n */\nvoid ext4_evict_inode(struct inode *inode)\n{\n\thandle_t *handle;\n\tint err;\n\t/*\n\t * Credits for final inode cleanup and freeing:\n\t * sb + inode (ext4_orphan_del()), block bitmap, group descriptor\n\t * (xattr block freeing), bitmap, group descriptor (inode freeing)\n\t */\n\tint extra_credits = 6;\n\tstruct ext4_xattr_inode_array *ea_inode_array = NULL;\n\n\ttrace_ext4_evict_inode(inode);\n\n\tif (inode->i_nlink) {\n\t\t/*\n\t\t * When journalling data dirty buffers are tracked only in the\n\t\t * journal. So although mm thinks everything is clean and\n\t\t * ready for reaping the inode might still have some pages to\n\t\t * write in the running transaction or waiting to be\n\t\t * checkpointed. Thus calling jbd2_journal_invalidatepage()\n\t\t * (via truncate_inode_pages()) to discard these buffers can\n\t\t * cause data loss. Also even if we did not discard these\n\t\t * buffers, we would have no way to find them after the inode\n\t\t * is reaped and thus user could see stale data if he tries to\n\t\t * read them before the transaction is checkpointed. So be\n\t\t * careful and force everything to disk here... We use\n\t\t * ei->i_datasync_tid to store the newest transaction\n\t\t * containing inode's data.\n\t\t *\n\t\t * Note that directories do not have this problem because they\n\t\t * don't use page cache.\n\t\t */\n\t\tif (inode->i_ino != EXT4_JOURNAL_INO &&\n\t\t    ext4_should_journal_data(inode) &&\n\t\t    (S_ISLNK(inode->i_mode) || S_ISREG(inode->i_mode)) &&\n\t\t    inode->i_data.nrpages) {\n\t\t\tjournal_t *journal = EXT4_SB(inode->i_sb)->s_journal;\n\t\t\ttid_t commit_tid = EXT4_I(inode)->i_datasync_tid;\n\n\t\t\tjbd2_complete_transaction(journal, commit_tid);\n\t\t\tfilemap_write_and_wait(&inode->i_data);\n\t\t}\n\t\ttruncate_inode_pages_final(&inode->i_data);\n\n\t\tgoto no_delete;\n\t}\n\n\tif (is_bad_inode(inode))\n\t\tgoto no_delete;\n\tdquot_initialize(inode);\n\n\tif (ext4_should_order_data(inode))\n\t\text4_begin_ordered_truncate(inode, 0);\n\ttruncate_inode_pages_final(&inode->i_data);\n\n\t/*\n\t * Protect us against freezing - iput() caller didn't have to have any\n\t * protection against it\n\t */\n\tsb_start_intwrite(inode->i_sb);\n\n\tif (!IS_NOQUOTA(inode))\n\t\textra_credits += EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb);\n\n\t/*\n\t * Block bitmap, group descriptor, and inode are accounted in both\n\t * ext4_blocks_for_truncate() and extra_credits. So subtract 3.\n\t */\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE,\n\t\t\t ext4_blocks_for_truncate(inode) + extra_credits - 3);\n\tif (IS_ERR(handle)) {\n\t\text4_std_error(inode->i_sb, PTR_ERR(handle));\n\t\t/*\n\t\t * If we're going to skip the normal cleanup, we still need to\n\t\t * make sure that the in-core orphan linked list is properly\n\t\t * cleaned up.\n\t\t */\n\t\text4_orphan_del(NULL, inode);\n\t\tsb_end_intwrite(inode->i_sb);\n\t\tgoto no_delete;\n\t}\n\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\t/*\n\t * Set inode->i_size to 0 before calling ext4_truncate(). We need\n\t * special handling of symlinks here because i_size is used to\n\t * determine whether ext4_inode_info->i_data contains symlink data or\n\t * block mappings. Setting i_size to 0 will remove its fast symlink\n\t * status. Erase i_data so that it becomes a valid empty block map.\n\t */\n\tif (ext4_inode_is_fast_symlink(inode))\n\t\tmemset(EXT4_I(inode)->i_data, 0, sizeof(EXT4_I(inode)->i_data));\n\tinode->i_size = 0;\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err) {\n\t\text4_warning(inode->i_sb,\n\t\t\t     \"couldn't mark inode dirty (err %d)\", err);\n\t\tgoto stop_handle;\n\t}\n\tif (inode->i_blocks) {\n\t\terr = ext4_truncate(inode);\n\t\tif (err) {\n\t\t\text4_error(inode->i_sb,\n\t\t\t\t   \"couldn't truncate inode %lu (err %d)\",\n\t\t\t\t   inode->i_ino, err);\n\t\t\tgoto stop_handle;\n\t\t}\n\t}\n\n\t/* Remove xattr references. */\n\terr = ext4_xattr_delete_inode(handle, inode, &ea_inode_array,\n\t\t\t\t      extra_credits);\n\tif (err) {\n\t\text4_warning(inode->i_sb, \"xattr delete (err %d)\", err);\nstop_handle:\n\t\text4_journal_stop(handle);\n\t\text4_orphan_del(NULL, inode);\n\t\tsb_end_intwrite(inode->i_sb);\n\t\text4_xattr_inode_array_free(ea_inode_array);\n\t\tgoto no_delete;\n\t}\n\n\t/*\n\t * Kill off the orphan record which ext4_truncate created.\n\t * AKPM: I think this can be inside the above `if'.\n\t * Note that ext4_orphan_del() has to be able to cope with the\n\t * deletion of a non-existent orphan - this is because we don't\n\t * know if ext4_truncate() actually created an orphan record.\n\t * (Well, we could do this if we need to, but heck - it works)\n\t */\n\text4_orphan_del(handle, inode);\n\tEXT4_I(inode)->i_dtime\t= (__u32)ktime_get_real_seconds();\n\n\t/*\n\t * One subtle ordering requirement: if anything has gone wrong\n\t * (transaction abort, IO errors, whatever), then we can still\n\t * do these next steps (the fs will already have been marked as\n\t * having errors), but we can't free the inode if the mark_dirty\n\t * fails.\n\t */\n\tif (ext4_mark_inode_dirty(handle, inode))\n\t\t/* If that failed, just do the required in-core inode clear. */\n\t\text4_clear_inode(inode);\n\telse\n\t\text4_free_inode(handle, inode);\n\text4_journal_stop(handle);\n\tsb_end_intwrite(inode->i_sb);\n\text4_xattr_inode_array_free(ea_inode_array);\n\treturn;\nno_delete:\n\text4_clear_inode(inode);\t/* We must guarantee clearing of inode... */\n}\n\n#ifdef CONFIG_QUOTA\nqsize_t *ext4_get_reserved_space(struct inode *inode)\n{\n\treturn &EXT4_I(inode)->i_reserved_quota;\n}\n#endif\n\n/*\n * Called with i_data_sem down, which is important since we can call\n * ext4_discard_preallocations() from here.\n */\nvoid ext4_da_update_reserve_space(struct inode *inode,\n\t\t\t\t\tint used, int quota_claim)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tspin_lock(&ei->i_block_reservation_lock);\n\ttrace_ext4_da_update_reserve_space(inode, used, quota_claim);\n\tif (unlikely(used > ei->i_reserved_data_blocks)) {\n\t\text4_warning(inode->i_sb, \"%s: ino %lu, used %d \"\n\t\t\t \"with only %d reserved data blocks\",\n\t\t\t __func__, inode->i_ino, used,\n\t\t\t ei->i_reserved_data_blocks);\n\t\tWARN_ON(1);\n\t\tused = ei->i_reserved_data_blocks;\n\t}\n\n\t/* Update per-inode reservations */\n\tei->i_reserved_data_blocks -= used;\n\tpercpu_counter_sub(&sbi->s_dirtyclusters_counter, used);\n\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\t/* Update quota subsystem for data blocks */\n\tif (quota_claim)\n\t\tdquot_claim_block(inode, EXT4_C2B(sbi, used));\n\telse {\n\t\t/*\n\t\t * We did fallocate with an offset that is already delayed\n\t\t * allocated. So on delayed allocated writeback we should\n\t\t * not re-claim the quota for fallocated blocks.\n\t\t */\n\t\tdquot_release_reservation_block(inode, EXT4_C2B(sbi, used));\n\t}\n\n\t/*\n\t * If we have done all the pending block allocations and if\n\t * there aren't any writers on the inode, we can discard the\n\t * inode's preallocations.\n\t */\n\tif ((ei->i_reserved_data_blocks == 0) &&\n\t    !inode_is_open_for_write(inode))\n\t\text4_discard_preallocations(inode);\n}\n\nstatic int __check_block_validity(struct inode *inode, const char *func,\n\t\t\t\tunsigned int line,\n\t\t\t\tstruct ext4_map_blocks *map)\n{\n\tif (ext4_has_feature_journal(inode->i_sb) &&\n\t    (inode->i_ino ==\n\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))\n\t\treturn 0;\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n\t\t\t\t   map->m_len)) {\n\t\text4_error_inode(inode, func, line, map->m_pblk,\n\t\t\t\t \"lblock %lu mapped to illegal pblock %llu \"\n\t\t\t\t \"(length %d)\", (unsigned long) map->m_lblk,\n\t\t\t\t map->m_pblk, map->m_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}\n\nint ext4_issue_zeroout(struct inode *inode, ext4_lblk_t lblk, ext4_fsblk_t pblk,\n\t\t       ext4_lblk_t len)\n{\n\tint ret;\n\n\tif (IS_ENCRYPTED(inode))\n\t\treturn fscrypt_zeroout_range(inode, lblk, pblk, len);\n\n\tret = sb_issue_zeroout(inode->i_sb, pblk, len, GFP_NOFS);\n\tif (ret > 0)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n#define check_block_validity(inode, map)\t\\\n\t__check_block_validity((inode), __func__, __LINE__, (map))\n\n#ifdef ES_AGGRESSIVE_TEST\nstatic void ext4_map_blocks_es_recheck(handle_t *handle,\n\t\t\t\t       struct inode *inode,\n\t\t\t\t       struct ext4_map_blocks *es_map,\n\t\t\t\t       struct ext4_map_blocks *map,\n\t\t\t\t       int flags)\n{\n\tint retval;\n\n\tmap->m_flags = 0;\n\t/*\n\t * There is a race window that the result is not the same.\n\t * e.g. xfstests #223 when dioread_nolock enables.  The reason\n\t * is that we lookup a block mapping in extent status tree with\n\t * out taking i_data_sem.  So at the time the unwritten extent\n\t * could be converted.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\n\t/*\n\t * We don't check m_len because extent will be collpased in status\n\t * tree.  So the m_len might not equal.\n\t */\n\tif (es_map->m_lblk != map->m_lblk ||\n\t    es_map->m_flags != map->m_flags ||\n\t    es_map->m_pblk != map->m_pblk) {\n\t\tprintk(\"ES cache assertion failed for inode: %lu \"\n\t\t       \"es_cached ex [%d/%d/%llu/%x] != \"\n\t\t       \"found ex [%d/%d/%llu/%x] retval %d flags %x\\n\",\n\t\t       inode->i_ino, es_map->m_lblk, es_map->m_len,\n\t\t       es_map->m_pblk, es_map->m_flags, map->m_lblk,\n\t\t       map->m_len, map->m_pblk, map->m_flags,\n\t\t       retval, flags);\n\t}\n}\n#endif /* ES_AGGRESSIVE_TEST */\n\n/*\n * The ext4_map_blocks() function tries to look up the requested blocks,\n * and returns if the blocks are already mapped.\n *\n * Otherwise it takes the write lock of the i_data_sem and allocate blocks\n * and store the allocated blocks in the result buffer head and mark it\n * mapped.\n *\n * If file type is extents based, it will call ext4_ext_map_blocks(),\n * Otherwise, call with ext4_ind_map_blocks() to handle indirect mapping\n * based files\n *\n * On success, it returns the number of blocks being mapped or allocated.  if\n * create==0 and the blocks are pre-allocated and unwritten, the resulting @map\n * is marked as unwritten. If the create == 1, it will mark @map as mapped.\n *\n * It returns 0 if plain look up failed (blocks have not been allocated), in\n * that case, @map is returned as unmapped but we still do fill map->m_len to\n * indicate the length of a hole starting at map->m_lblk.\n *\n * It returns the error in case of allocation failure.\n */\nint ext4_map_blocks(handle_t *handle, struct inode *inode,\n\t\t    struct ext4_map_blocks *map, int flags)\n{\n\tstruct extent_status es;\n\tint retval;\n\tint ret = 0;\n#ifdef ES_AGGRESSIVE_TEST\n\tstruct ext4_map_blocks orig_map;\n\n\tmemcpy(&orig_map, map, sizeof(*map));\n#endif\n\n\tmap->m_flags = 0;\n\text_debug(\"ext4_map_blocks(): inode %lu, flag %d, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, flags, map->m_len,\n\t\t  (unsigned long) map->m_lblk);\n\n\t/*\n\t * ext4_map_blocks returns an int, and m_len is an unsigned int\n\t */\n\tif (unlikely(map->m_len > INT_MAX))\n\t\tmap->m_len = INT_MAX;\n\n\t/* We can handle the block number less than EXT_MAX_BLOCKS */\n\tif (unlikely(map->m_lblk >= EXT_MAX_BLOCKS))\n\t\treturn -EFSCORRUPTED;\n\n\t/* Lookup extent status tree firstly */\n\tif (ext4_es_lookup_extent(inode, map->m_lblk, NULL, &es)) {\n\t\tif (ext4_es_is_written(&es) || ext4_es_is_unwritten(&es)) {\n\t\t\tmap->m_pblk = ext4_es_pblock(&es) +\n\t\t\t\t\tmap->m_lblk - es.es_lblk;\n\t\t\tmap->m_flags |= ext4_es_is_written(&es) ?\n\t\t\t\t\tEXT4_MAP_MAPPED : EXT4_MAP_UNWRITTEN;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t} else if (ext4_es_is_delayed(&es) || ext4_es_is_hole(&es)) {\n\t\t\tmap->m_pblk = 0;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t\tretval = 0;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n#ifdef ES_AGGRESSIVE_TEST\n\t\text4_map_blocks_es_recheck(handle, inode, map,\n\t\t\t\t\t   &orig_map, flags);\n#endif\n\t\tgoto found;\n\t}\n\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t}\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_es_scan_range(inode, &ext4_es_is_delayed, map->m_lblk,\n\t\t\t\t       map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk,\n\t\t\t\t\t    map->m_len, map->m_pblk, status);\n\t\tif (ret < 0)\n\t\t\tretval = ret;\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\nfound:\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t/* If it is only a block(s) look up */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0)\n\t\treturn retval;\n\n\t/*\n\t * Returns if the blocks have already allocated\n\t *\n\t * Note that if blocks have been preallocated\n\t * ext4_ext_get_block() returns the create = 0\n\t * with buffer head unmapped.\n\t */\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED)\n\t\t/*\n\t\t * If we need to convert extent to unwritten\n\t\t * we continue and do the actual work in\n\t\t * ext4_ext_map_blocks()\n\t\t */\n\t\tif (!(flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN))\n\t\t\treturn retval;\n\n\t/*\n\t * Here we clear m_flags because after allocating an new extent,\n\t * it will be set again.\n\t */\n\tmap->m_flags &= ~EXT4_MAP_FLAGS;\n\n\t/*\n\t * New blocks allocate and/or writing to unwritten extent\n\t * will possibly result in updating i_data, so we take\n\t * the write lock of i_data_sem, and call get_block()\n\t * with create == 1 flag.\n\t */\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\n\t/*\n\t * We need to check for EXT4 here because migrate\n\t * could have changed the inode type in between\n\t */\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags);\n\n\t\tif (retval > 0 && map->m_flags & EXT4_MAP_NEW) {\n\t\t\t/*\n\t\t\t * We allocated new blocks which will result in\n\t\t\t * i_data's format changing.  Force the migrate\n\t\t\t * to fail by clearing migrate flags\n\t\t\t */\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);\n\t\t}\n\n\t\t/*\n\t\t * Update reserved blocks/metadata blocks after successful\n\t\t * block allocation which had been deferred till now. We don't\n\t\t * support fallocate for non extent files. So we can update\n\t\t * reserve space here.\n\t\t */\n\t\tif ((retval > 0) &&\n\t\t\t(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))\n\t\t\text4_da_update_reserve_space(inode, retval, 1);\n\t}\n\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\t/*\n\t\t * We have to zeroout blocks before inserting them into extent\n\t\t * status tree. Otherwise someone could look them up there and\n\t\t * use them before they are really zeroed. We also have to\n\t\t * unmap metadata before zeroing as otherwise writeback can\n\t\t * overwrite zeros with stale data from block device.\n\t\t */\n\t\tif (flags & EXT4_GET_BLOCKS_ZERO &&\n\t\t    map->m_flags & EXT4_MAP_MAPPED &&\n\t\t    map->m_flags & EXT4_MAP_NEW) {\n\t\t\tret = ext4_issue_zeroout(inode, map->m_lblk,\n\t\t\t\t\t\t map->m_pblk, map->m_len);\n\t\t\tif (ret) {\n\t\t\t\tretval = ret;\n\t\t\t\tgoto out_sem;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * If the extent has been zeroed out, we don't need to update\n\t\t * extent status tree.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO) &&\n\t\t    ext4_es_lookup_extent(inode, map->m_lblk, NULL, &es)) {\n\t\t\tif (ext4_es_is_written(&es))\n\t\t\t\tgoto out_sem;\n\t\t}\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_es_scan_range(inode, &ext4_es_is_delayed, map->m_lblk,\n\t\t\t\t       map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk, map->m_len,\n\t\t\t\t\t    map->m_pblk, status);\n\t\tif (ret < 0) {\n\t\t\tretval = ret;\n\t\t\tgoto out_sem;\n\t\t}\n\t}\n\nout_sem:\n\tup_write((&EXT4_I(inode)->i_data_sem));\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * Inodes with freshly allocated blocks where contents will be\n\t\t * visible after transaction commit must be on transaction's\n\t\t * ordered data list.\n\t\t */\n\t\tif (map->m_flags & EXT4_MAP_NEW &&\n\t\t    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&\n\t\t    !(flags & EXT4_GET_BLOCKS_ZERO) &&\n\t\t    !ext4_is_quota_file(inode) &&\n\t\t    ext4_should_order_data(inode)) {\n\t\t\tloff_t start_byte =\n\t\t\t\t(loff_t)map->m_lblk << inode->i_blkbits;\n\t\t\tloff_t length = (loff_t)map->m_len << inode->i_blkbits;\n\n\t\t\tif (flags & EXT4_GET_BLOCKS_IO_SUBMIT)\n\t\t\t\tret = ext4_jbd2_inode_add_wait(handle, inode,\n\t\t\t\t\t\tstart_byte, length);\n\t\t\telse\n\t\t\t\tret = ext4_jbd2_inode_add_write(handle, inode,\n\t\t\t\t\t\tstart_byte, length);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\treturn retval;\n}\n\n/*\n * Update EXT4_MAP_FLAGS in bh->b_state. For buffer heads attached to pages\n * we have to be careful as someone else may be manipulating b_state as well.\n */\nstatic void ext4_update_bh_state(struct buffer_head *bh, unsigned long flags)\n{\n\tunsigned long old_state;\n\tunsigned long new_state;\n\n\tflags &= EXT4_MAP_FLAGS;\n\n\t/* Dummy buffer_head? Set non-atomically. */\n\tif (!bh->b_page) {\n\t\tbh->b_state = (bh->b_state & ~EXT4_MAP_FLAGS) | flags;\n\t\treturn;\n\t}\n\t/*\n\t * Someone else may be modifying b_state. Be careful! This is ugly but\n\t * once we get rid of using bh as a container for mapping information\n\t * to pass to / from get_block functions, this can go away.\n\t */\n\tdo {\n\t\told_state = READ_ONCE(bh->b_state);\n\t\tnew_state = (old_state & ~EXT4_MAP_FLAGS) | flags;\n\t} while (unlikely(\n\t\t cmpxchg(&bh->b_state, old_state, new_state) != old_state));\n}\n\nstatic int _ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t\t   struct buffer_head *bh, int flags)\n{\n\tstruct ext4_map_blocks map;\n\tint ret = 0;\n\n\tif (ext4_has_inline_data(inode))\n\t\treturn -ERANGE;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\n\tret = ext4_map_blocks(ext4_journal_current_handle(), inode, &map,\n\t\t\t      flags);\n\tif (ret > 0) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\text4_update_bh_state(bh, map.m_flags);\n\t\tbh->b_size = inode->i_sb->s_blocksize * map.m_len;\n\t\tret = 0;\n\t} else if (ret == 0) {\n\t\t/* hole case, need to fill in bh->b_size */\n\t\tbh->b_size = inode->i_sb->s_blocksize * map.m_len;\n\t}\n\treturn ret;\n}\n\nint ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t   struct buffer_head *bh, int create)\n{\n\treturn _ext4_get_block(inode, iblock, bh,\n\t\t\t       create ? EXT4_GET_BLOCKS_CREATE : 0);\n}\n\n/*\n * Get block function used when preparing for buffered write if we require\n * creating an unwritten extent if blocks haven't been allocated.  The extent\n * will be converted to written after the IO is complete.\n */\nint ext4_get_block_unwritten(struct inode *inode, sector_t iblock,\n\t\t\t     struct buffer_head *bh_result, int create)\n{\n\text4_debug(\"ext4_get_block_unwritten: inode %lu, create flag %d\\n\",\n\t\t   inode->i_ino, create);\n\treturn _ext4_get_block(inode, iblock, bh_result,\n\t\t\t       EXT4_GET_BLOCKS_IO_CREATE_EXT);\n}\n\n/* Maximum number of blocks we map for direct IO at once. */\n#define DIO_MAX_BLOCKS 4096\n\n/*\n * `handle' can be NULL if create is zero\n */\nstruct buffer_head *ext4_getblk(handle_t *handle, struct inode *inode,\n\t\t\t\text4_lblk_t block, int map_flags)\n{\n\tstruct ext4_map_blocks map;\n\tstruct buffer_head *bh;\n\tint create = map_flags & EXT4_GET_BLOCKS_CREATE;\n\tint err;\n\n\tJ_ASSERT(handle != NULL || create == 0);\n\n\tmap.m_lblk = block;\n\tmap.m_len = 1;\n\terr = ext4_map_blocks(handle, inode, &map, map_flags);\n\n\tif (err == 0)\n\t\treturn create ? ERR_PTR(-ENOSPC) : NULL;\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\tbh = sb_getblk(inode->i_sb, map.m_pblk);\n\tif (unlikely(!bh))\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (map.m_flags & EXT4_MAP_NEW) {\n\t\tJ_ASSERT(create != 0);\n\t\tJ_ASSERT(handle != NULL);\n\n\t\t/*\n\t\t * Now that we do not always journal data, we should\n\t\t * keep in mind whether this should always journal the\n\t\t * new buffer as metadata.  For now, regular file\n\t\t * writes use ext4_get_block instead, so it's not a\n\t\t * problem.\n\t\t */\n\t\tlock_buffer(bh);\n\t\tBUFFER_TRACE(bh, \"call get_create_access\");\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (unlikely(err)) {\n\t\t\tunlock_buffer(bh);\n\t\t\tgoto errout;\n\t\t}\n\t\tif (!buffer_uptodate(bh)) {\n\t\t\tmemset(bh->b_data, 0, inode->i_sb->s_blocksize);\n\t\t\tset_buffer_uptodate(bh);\n\t\t}\n\t\tunlock_buffer(bh);\n\t\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (unlikely(err))\n\t\t\tgoto errout;\n\t} else\n\t\tBUFFER_TRACE(bh, \"not a new buffer\");\n\treturn bh;\nerrout:\n\tbrelse(bh);\n\treturn ERR_PTR(err);\n}\n\nstruct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,\n\t\t\t       ext4_lblk_t block, int map_flags)\n{\n\tstruct buffer_head *bh;\n\n\tbh = ext4_getblk(handle, inode, block, map_flags);\n\tif (IS_ERR(bh))\n\t\treturn bh;\n\tif (!bh || ext4_buffer_uptodate(bh))\n\t\treturn bh;\n\tll_rw_block(REQ_OP_READ, REQ_META | REQ_PRIO, 1, &bh);\n\twait_on_buffer(bh);\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tput_bh(bh);\n\treturn ERR_PTR(-EIO);\n}\n\n/* Read a contiguous batch of blocks. */\nint ext4_bread_batch(struct inode *inode, ext4_lblk_t block, int bh_count,\n\t\t     bool wait, struct buffer_head **bhs)\n{\n\tint i, err;\n\n\tfor (i = 0; i < bh_count; i++) {\n\t\tbhs[i] = ext4_getblk(NULL, inode, block + i, 0 /* map_flags */);\n\t\tif (IS_ERR(bhs[i])) {\n\t\t\terr = PTR_ERR(bhs[i]);\n\t\t\tbh_count = i;\n\t\t\tgoto out_brelse;\n\t\t}\n\t}\n\n\tfor (i = 0; i < bh_count; i++)\n\t\t/* Note that NULL bhs[i] is valid because of holes. */\n\t\tif (bhs[i] && !ext4_buffer_uptodate(bhs[i]))\n\t\t\tll_rw_block(REQ_OP_READ, REQ_META | REQ_PRIO, 1,\n\t\t\t\t    &bhs[i]);\n\n\tif (!wait)\n\t\treturn 0;\n\n\tfor (i = 0; i < bh_count; i++)\n\t\tif (bhs[i])\n\t\t\twait_on_buffer(bhs[i]);\n\n\tfor (i = 0; i < bh_count; i++) {\n\t\tif (bhs[i] && !buffer_uptodate(bhs[i])) {\n\t\t\terr = -EIO;\n\t\t\tgoto out_brelse;\n\t\t}\n\t}\n\treturn 0;\n\nout_brelse:\n\tfor (i = 0; i < bh_count; i++) {\n\t\tbrelse(bhs[i]);\n\t\tbhs[i] = NULL;\n\t}\n\treturn err;\n}\n\nint ext4_walk_page_buffers(handle_t *handle,\n\t\t\t   struct buffer_head *head,\n\t\t\t   unsigned from,\n\t\t\t   unsigned to,\n\t\t\t   int *partial,\n\t\t\t   int (*fn)(handle_t *handle,\n\t\t\t\t     struct buffer_head *bh))\n{\n\tstruct buffer_head *bh;\n\tunsigned block_start, block_end;\n\tunsigned blocksize = head->b_size;\n\tint err, ret = 0;\n\tstruct buffer_head *next;\n\n\tfor (bh = head, block_start = 0;\n\t     ret == 0 && (bh != head || !block_start);\n\t     block_start = block_end, bh = next) {\n\t\tnext = bh->b_this_page;\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (partial && !buffer_uptodate(bh))\n\t\t\t\t*partial = 1;\n\t\t\tcontinue;\n\t\t}\n\t\terr = (*fn)(handle, bh);\n\t\tif (!ret)\n\t\t\tret = err;\n\t}\n\treturn ret;\n}\n\n/*\n * To preserve ordering, it is essential that the hole instantiation and\n * the data write be encapsulated in a single transaction.  We cannot\n * close off a transaction and start a new one between the ext4_get_block()\n * and the commit_write().  So doing the jbd2_journal_start at the start of\n * prepare_write() is the right place.\n *\n * Also, this function can nest inside ext4_writepage().  In that case, we\n * *know* that ext4_writepage() has generated enough buffer credits to do the\n * whole page.  So we won't block on the journal in that case, which is good,\n * because the caller may be PF_MEMALLOC.\n *\n * By accident, ext4 can be reentered when a transaction is open via\n * quota file writes.  If we were to commit the transaction while thus\n * reentered, there can be a deadlock - we would be holding a quota\n * lock, and the commit would never complete if another thread had a\n * transaction open and was blocking on the quota lock - a ranking\n * violation.\n *\n * So what we do is to rely on the fact that jbd2_journal_stop/journal_start\n * will _not_ run commit under these circumstances because handle->h_ref\n * is elevated.  We'll still have enough credits for the tiny quotafile\n * write.\n */\nint do_journal_get_write_access(handle_t *handle,\n\t\t\t\tstruct buffer_head *bh)\n{\n\tint dirty = buffer_dirty(bh);\n\tint ret;\n\n\tif (!buffer_mapped(bh) || buffer_freed(bh))\n\t\treturn 0;\n\t/*\n\t * __block_write_begin() could have dirtied some buffers. Clean\n\t * the dirty bit as jbd2_journal_get_write_access() could complain\n\t * otherwise about fs integrity issues. Setting of the dirty bit\n\t * by __block_write_begin() isn't a real problem here as we clear\n\t * the bit before releasing a page lock and thus writeback cannot\n\t * ever write the buffer.\n\t */\n\tif (dirty)\n\t\tclear_buffer_dirty(bh);\n\tBUFFER_TRACE(bh, \"get write access\");\n\tret = ext4_journal_get_write_access(handle, bh);\n\tif (!ret && dirty)\n\t\tret = ext4_handle_dirty_metadata(handle, NULL, bh);\n\treturn ret;\n}\n\n#ifdef CONFIG_FS_ENCRYPTION\nstatic int ext4_block_write_begin(struct page *page, loff_t pos, unsigned len,\n\t\t\t\t  get_block_t *get_block)\n{\n\tunsigned from = pos & (PAGE_SIZE - 1);\n\tunsigned to = from + len;\n\tstruct inode *inode = page->mapping->host;\n\tunsigned block_start, block_end;\n\tsector_t block;\n\tint err = 0;\n\tunsigned blocksize = inode->i_sb->s_blocksize;\n\tunsigned bbits;\n\tstruct buffer_head *bh, *head, *wait[2];\n\tint nr_wait = 0;\n\tint i;\n\n\tBUG_ON(!PageLocked(page));\n\tBUG_ON(from > PAGE_SIZE);\n\tBUG_ON(to > PAGE_SIZE);\n\tBUG_ON(from > to);\n\n\tif (!page_has_buffers(page))\n\t\tcreate_empty_buffers(page, blocksize, 0);\n\thead = page_buffers(page);\n\tbbits = ilog2(blocksize);\n\tblock = (sector_t)page->index << (PAGE_SHIFT - bbits);\n\n\tfor (bh = head, block_start = 0; bh != head || !block_start;\n\t    block++, block_start = block_end, bh = bh->b_this_page) {\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (PageUptodate(page)) {\n\t\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (buffer_new(bh))\n\t\t\tclear_buffer_new(bh);\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tWARN_ON(bh->b_size != blocksize);\n\t\t\terr = get_block(inode, block, bh, 1);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (buffer_new(bh)) {\n\t\t\t\tif (PageUptodate(page)) {\n\t\t\t\t\tclear_buffer_new(bh);\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\t\tmark_buffer_dirty(bh);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (block_end > to || block_start < from)\n\t\t\t\t\tzero_user_segments(page, to, block_end,\n\t\t\t\t\t\t\t   block_start, from);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (PageUptodate(page)) {\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!buffer_uptodate(bh) && !buffer_delay(bh) &&\n\t\t    !buffer_unwritten(bh) &&\n\t\t    (block_start < from || block_end > to)) {\n\t\t\tll_rw_block(REQ_OP_READ, 0, 1, &bh);\n\t\t\twait[nr_wait++] = bh;\n\t\t}\n\t}\n\t/*\n\t * If we issued read requests, let them complete.\n\t */\n\tfor (i = 0; i < nr_wait; i++) {\n\t\twait_on_buffer(wait[i]);\n\t\tif (!buffer_uptodate(wait[i]))\n\t\t\terr = -EIO;\n\t}\n\tif (unlikely(err)) {\n\t\tpage_zero_new_buffers(page, from, to);\n\t} else if (IS_ENCRYPTED(inode) && S_ISREG(inode->i_mode)) {\n\t\tfor (i = 0; i < nr_wait; i++) {\n\t\t\tint err2;\n\n\t\t\terr2 = fscrypt_decrypt_pagecache_blocks(page, blocksize,\n\t\t\t\t\t\t\t\tbh_offset(wait[i]));\n\t\t\tif (err2) {\n\t\t\t\tclear_buffer_uptodate(wait[i]);\n\t\t\t\terr = err2;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn err;\n}\n#endif\n\nstatic int ext4_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t    loff_t pos, unsigned len, unsigned flags,\n\t\t\t    struct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret, needed_blocks;\n\thandle_t *handle;\n\tint retries = 0;\n\tstruct page *page;\n\tpgoff_t index;\n\tunsigned from, to;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\ttrace_ext4_write_begin(inode, pos, len, flags);\n\t/*\n\t * Reserve one block more for addition to orphan list in case\n\t * we allocate blocks but write fails for some reason\n\t */\n\tneeded_blocks = ext4_writepage_trans_blocks(inode) + 1;\n\tindex = pos >> PAGE_SHIFT;\n\tfrom = pos & (PAGE_SIZE - 1);\n\tto = from + len;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) {\n\t\tret = ext4_try_to_write_inline_data(mapping, inode, pos, len,\n\t\t\t\t\t\t    flags, pagep);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (ret == 1)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * grab_cache_page_write_begin() can take a long time if the\n\t * system is thrashing due to memory pressure, or if the page\n\t * is being written back.  So grab it first before we start\n\t * the transaction handle.  This also allows us to allocate\n\t * the page (if needed) without using GFP_NOFS.\n\t */\nretry_grab:\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page)\n\t\treturn -ENOMEM;\n\tunlock_page(page);\n\nretry_journal:\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE, needed_blocks);\n\tif (IS_ERR(handle)) {\n\t\tput_page(page);\n\t\treturn PTR_ERR(handle);\n\t}\n\n\tlock_page(page);\n\tif (page->mapping != mapping) {\n\t\t/* The page got truncated from under us */\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\text4_journal_stop(handle);\n\t\tgoto retry_grab;\n\t}\n\t/* In case writeback began while the page was unlocked */\n\twait_for_stable_page(page);\n\n#ifdef CONFIG_FS_ENCRYPTION\n\tif (ext4_should_dioread_nolock(inode))\n\t\tret = ext4_block_write_begin(page, pos, len,\n\t\t\t\t\t     ext4_get_block_unwritten);\n\telse\n\t\tret = ext4_block_write_begin(page, pos, len,\n\t\t\t\t\t     ext4_get_block);\n#else\n\tif (ext4_should_dioread_nolock(inode))\n\t\tret = __block_write_begin(page, pos, len,\n\t\t\t\t\t  ext4_get_block_unwritten);\n\telse\n\t\tret = __block_write_begin(page, pos, len, ext4_get_block);\n#endif\n\tif (!ret && ext4_should_journal_data(inode)) {\n\t\tret = ext4_walk_page_buffers(handle, page_buffers(page),\n\t\t\t\t\t     from, to, NULL,\n\t\t\t\t\t     do_journal_get_write_access);\n\t}\n\n\tif (ret) {\n\t\tbool extended = (pos + len > inode->i_size) &&\n\t\t\t\t!ext4_verity_in_progress(inode);\n\n\t\tunlock_page(page);\n\t\t/*\n\t\t * __block_write_begin may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again. Don't need\n\t\t * i_size_read because we hold i_mutex.\n\t\t *\n\t\t * Add inode to orphan list in case we crash before\n\t\t * truncate finishes\n\t\t */\n\t\tif (extended && ext4_can_truncate(inode))\n\t\t\text4_orphan_add(handle, inode);\n\n\t\text4_journal_stop(handle);\n\t\tif (extended) {\n\t\t\text4_truncate_failed_write(inode);\n\t\t\t/*\n\t\t\t * If truncate failed early the inode might\n\t\t\t * still be on the orphan list; we need to\n\t\t\t * make sure the inode is removed from the\n\t\t\t * orphan list in that case.\n\t\t\t */\n\t\t\tif (inode->i_nlink)\n\t\t\t\text4_orphan_del(NULL, inode);\n\t\t}\n\n\t\tif (ret == -ENOSPC &&\n\t\t    ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\t\tgoto retry_journal;\n\t\tput_page(page);\n\t\treturn ret;\n\t}\n\t*pagep = page;\n\treturn ret;\n}\n\n/* For write_end() in data=journal mode */\nstatic int write_end_fn(handle_t *handle, struct buffer_head *bh)\n{\n\tint ret;\n\tif (!buffer_mapped(bh) || buffer_freed(bh))\n\t\treturn 0;\n\tset_buffer_uptodate(bh);\n\tret = ext4_handle_dirty_metadata(handle, NULL, bh);\n\tclear_buffer_meta(bh);\n\tclear_buffer_prio(bh);\n\treturn ret;\n}\n\n/*\n * We need to pick up the new inode size which generic_commit_write gave us\n * `file' can be NULL - eg, when called from page_symlink().\n *\n * ext4 never places buffers on inode->i_mapping->private_list.  metadata\n * buffers are managed internally.\n */\nstatic int ext4_write_end(struct file *file,\n\t\t\t  struct address_space *mapping,\n\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t  struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tloff_t old_size = inode->i_size;\n\tint ret = 0, ret2;\n\tint i_size_changed = 0;\n\tint inline_data = ext4_has_inline_data(inode);\n\tbool verity = ext4_verity_in_progress(inode);\n\n\ttrace_ext4_write_end(inode, pos, len, copied);\n\tif (inline_data) {\n\t\tret = ext4_write_inline_data_end(inode, pos, len,\n\t\t\t\t\t\t copied, page);\n\t\tif (ret < 0) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto errout;\n\t\t}\n\t\tcopied = ret;\n\t} else\n\t\tcopied = block_write_end(file, mapping, pos,\n\t\t\t\t\t len, copied, page, fsdata);\n\t/*\n\t * it's important to update i_size while still holding page lock:\n\t * page writeout could otherwise come in and zero beyond i_size.\n\t *\n\t * If FS_IOC_ENABLE_VERITY is running on this inode, then Merkle tree\n\t * blocks are being written past EOF, so skip the i_size update.\n\t */\n\tif (!verity)\n\t\ti_size_changed = ext4_update_inode_size(inode, pos + copied);\n\tunlock_page(page);\n\tput_page(page);\n\n\tif (old_size < pos && !verity)\n\t\tpagecache_isize_extended(inode, old_size, pos);\n\t/*\n\t * Don't mark the inode dirty under page lock. First, it unnecessarily\n\t * makes the holding time of page lock longer. Second, it forces lock\n\t * ordering of page lock and transaction start for journaling\n\t * filesystems.\n\t */\n\tif (i_size_changed || inline_data)\n\t\text4_mark_inode_dirty(handle, inode);\n\n\tif (pos + len > inode->i_size && !verity && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\nerrout:\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\tif (pos + len > inode->i_size && !verity) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}\n\n/*\n * This is a private version of page_zero_new_buffers() which doesn't\n * set the buffer to be dirty, since in data=journalled mode we need\n * to call ext4_handle_dirty_metadata() instead.\n */\nstatic void ext4_journalled_zero_new_buffers(handle_t *handle,\n\t\t\t\t\t    struct page *page,\n\t\t\t\t\t    unsigned from, unsigned to)\n{\n\tunsigned int block_start = 0, block_end;\n\tstruct buffer_head *head, *bh;\n\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tblock_end = block_start + bh->b_size;\n\t\tif (buffer_new(bh)) {\n\t\t\tif (block_end > from && block_start < to) {\n\t\t\t\tif (!PageUptodate(page)) {\n\t\t\t\t\tunsigned start, size;\n\n\t\t\t\t\tstart = max(from, block_start);\n\t\t\t\t\tsize = min(to, block_end) - start;\n\n\t\t\t\t\tzero_user(page, start, size);\n\t\t\t\t\twrite_end_fn(handle, bh);\n\t\t\t\t}\n\t\t\t\tclear_buffer_new(bh);\n\t\t\t}\n\t\t}\n\t\tblock_start = block_end;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n}\n\nstatic int ext4_journalled_write_end(struct file *file,\n\t\t\t\t     struct address_space *mapping,\n\t\t\t\t     loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t     struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tloff_t old_size = inode->i_size;\n\tint ret = 0, ret2;\n\tint partial = 0;\n\tunsigned from, to;\n\tint size_changed = 0;\n\tint inline_data = ext4_has_inline_data(inode);\n\tbool verity = ext4_verity_in_progress(inode);\n\n\ttrace_ext4_journalled_write_end(inode, pos, len, copied);\n\tfrom = pos & (PAGE_SIZE - 1);\n\tto = from + len;\n\n\tBUG_ON(!ext4_handle_valid(handle));\n\n\tif (inline_data) {\n\t\tret = ext4_write_inline_data_end(inode, pos, len,\n\t\t\t\t\t\t copied, page);\n\t\tif (ret < 0) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto errout;\n\t\t}\n\t\tcopied = ret;\n\t} else if (unlikely(copied < len) && !PageUptodate(page)) {\n\t\tcopied = 0;\n\t\text4_journalled_zero_new_buffers(handle, page, from, to);\n\t} else {\n\t\tif (unlikely(copied < len))\n\t\t\text4_journalled_zero_new_buffers(handle, page,\n\t\t\t\t\t\t\t from + copied, to);\n\t\tret = ext4_walk_page_buffers(handle, page_buffers(page), from,\n\t\t\t\t\t     from + copied, &partial,\n\t\t\t\t\t     write_end_fn);\n\t\tif (!partial)\n\t\t\tSetPageUptodate(page);\n\t}\n\tif (!verity)\n\t\tsize_changed = ext4_update_inode_size(inode, pos + copied);\n\text4_set_inode_state(inode, EXT4_STATE_JDATA);\n\tEXT4_I(inode)->i_datasync_tid = handle->h_transaction->t_tid;\n\tunlock_page(page);\n\tput_page(page);\n\n\tif (old_size < pos && !verity)\n\t\tpagecache_isize_extended(inode, old_size, pos);\n\n\tif (size_changed || inline_data) {\n\t\tret2 = ext4_mark_inode_dirty(handle, inode);\n\t\tif (!ret)\n\t\t\tret = ret2;\n\t}\n\n\tif (pos + len > inode->i_size && !verity && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\n\nerrout:\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\tif (pos + len > inode->i_size && !verity) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}\n\n/*\n * Reserve space for a single cluster\n */\nstatic int ext4_da_reserve_space(struct inode *inode)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint ret;\n\n\t/*\n\t * We will charge metadata quota at writeout time; this saves\n\t * us from metadata over-estimation, though we may go over by\n\t * a small amount in the end.  Here we just reserve for data.\n\t */\n\tret = dquot_reserve_block(inode, EXT4_C2B(sbi, 1));\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock(&ei->i_block_reservation_lock);\n\tif (ext4_claim_free_clusters(sbi, 1, 0)) {\n\t\tspin_unlock(&ei->i_block_reservation_lock);\n\t\tdquot_release_reservation_block(inode, EXT4_C2B(sbi, 1));\n\t\treturn -ENOSPC;\n\t}\n\tei->i_reserved_data_blocks++;\n\ttrace_ext4_da_reserve_space(inode);\n\tspin_unlock(&ei->i_block_reservation_lock);\n\n\treturn 0;       /* success */\n}\n\nvoid ext4_da_release_space(struct inode *inode, int to_free)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tif (!to_free)\n\t\treturn;\t\t/* Nothing to release, exit */\n\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\ttrace_ext4_da_release_space(inode, to_free);\n\tif (unlikely(to_free > ei->i_reserved_data_blocks)) {\n\t\t/*\n\t\t * if there aren't enough reserved blocks, then the\n\t\t * counter is messed up somewhere.  Since this\n\t\t * function is called from invalidate page, it's\n\t\t * harmless to return without any action.\n\t\t */\n\t\text4_warning(inode->i_sb, \"ext4_da_release_space: \"\n\t\t\t \"ino %lu, to_free %d with only %d reserved \"\n\t\t\t \"data blocks\", inode->i_ino, to_free,\n\t\t\t ei->i_reserved_data_blocks);\n\t\tWARN_ON(1);\n\t\tto_free = ei->i_reserved_data_blocks;\n\t}\n\tei->i_reserved_data_blocks -= to_free;\n\n\t/* update fs dirty data blocks counter */\n\tpercpu_counter_sub(&sbi->s_dirtyclusters_counter, to_free);\n\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\tdquot_release_reservation_block(inode, EXT4_C2B(sbi, to_free));\n}\n\n/*\n * Delayed allocation stuff\n */\n\nstruct mpage_da_data {\n\tstruct inode *inode;\n\tstruct writeback_control *wbc;\n\n\tpgoff_t first_page;\t/* The first page to write */\n\tpgoff_t next_page;\t/* Current page to examine */\n\tpgoff_t last_page;\t/* Last page to examine */\n\t/*\n\t * Extent to map - this can be after first_page because that can be\n\t * fully mapped. We somewhat abuse m_flags to store whether the extent\n\t * is delalloc or unwritten.\n\t */\n\tstruct ext4_map_blocks map;\n\tstruct ext4_io_submit io_submit;\t/* IO submission data */\n\tunsigned int do_map:1;\n};\n\nstatic void mpage_release_unused_pages(struct mpage_da_data *mpd,\n\t\t\t\t       bool invalidate)\n{\n\tint nr_pages, i;\n\tpgoff_t index, end;\n\tstruct pagevec pvec;\n\tstruct inode *inode = mpd->inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\t/* This is necessary when next_page == 0. */\n\tif (mpd->first_page >= mpd->next_page)\n\t\treturn;\n\n\tindex = mpd->first_page;\n\tend   = mpd->next_page - 1;\n\tif (invalidate) {\n\t\text4_lblk_t start, last;\n\t\tstart = index << (PAGE_SHIFT - inode->i_blkbits);\n\t\tlast = end << (PAGE_SHIFT - inode->i_blkbits);\n\t\text4_es_remove_extent(inode, start, last - start + 1);\n\t}\n\n\tpagevec_init(&pvec);\n\twhile (index <= end) {\n\t\tnr_pages = pagevec_lookup_range(&pvec, mapping, &index, end);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tBUG_ON(!PageLocked(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\tif (invalidate) {\n\t\t\t\tif (page_mapped(page))\n\t\t\t\t\tclear_page_dirty_for_io(page);\n\t\t\t\tblock_invalidatepage(page, 0, PAGE_SIZE);\n\t\t\t\tClearPageUptodate(page);\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n}\n\nstatic void ext4_print_free_blocks(struct inode *inode)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct super_block *sb = inode->i_sb;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\text4_msg(sb, KERN_CRIT, \"Total free blocks count %lld\",\n\t       EXT4_C2B(EXT4_SB(inode->i_sb),\n\t\t\text4_count_free_clusters(sb)));\n\text4_msg(sb, KERN_CRIT, \"Free/Dirty block details\");\n\text4_msg(sb, KERN_CRIT, \"free_blocks=%lld\",\n\t       (long long) EXT4_C2B(EXT4_SB(sb),\n\t\tpercpu_counter_sum(&sbi->s_freeclusters_counter)));\n\text4_msg(sb, KERN_CRIT, \"dirty_blocks=%lld\",\n\t       (long long) EXT4_C2B(EXT4_SB(sb),\n\t\tpercpu_counter_sum(&sbi->s_dirtyclusters_counter)));\n\text4_msg(sb, KERN_CRIT, \"Block reservation details\");\n\text4_msg(sb, KERN_CRIT, \"i_reserved_data_blocks=%u\",\n\t\t ei->i_reserved_data_blocks);\n\treturn;\n}\n\nstatic int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh)\n{\n\treturn (buffer_delay(bh) || buffer_unwritten(bh)) && buffer_dirty(bh);\n}\n\n/*\n * ext4_insert_delayed_block - adds a delayed block to the extents status\n *                             tree, incrementing the reserved cluster/block\n *                             count or making a pending reservation\n *                             where needed\n *\n * @inode - file containing the newly added block\n * @lblk - logical block to be added\n *\n * Returns 0 on success, negative error code on failure.\n */\nstatic int ext4_insert_delayed_block(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint ret;\n\tbool allocated = false;\n\n\t/*\n\t * If the cluster containing lblk is shared with a delayed,\n\t * written, or unwritten extent in a bigalloc file system, it's\n\t * already been accounted for and does not need to be reserved.\n\t * A pending reservation must be made for the cluster if it's\n\t * shared with a written or unwritten extent and doesn't already\n\t * have one.  Written and unwritten extents can be purged from the\n\t * extents status tree if the system is under memory pressure, so\n\t * it's necessary to examine the extent tree if a search of the\n\t * extents status tree doesn't get a match.\n\t */\n\tif (sbi->s_cluster_ratio == 1) {\n\t\tret = ext4_da_reserve_space(inode);\n\t\tif (ret != 0)   /* ENOSPC */\n\t\t\tgoto errout;\n\t} else {   /* bigalloc */\n\t\tif (!ext4_es_scan_clu(inode, &ext4_es_is_delonly, lblk)) {\n\t\t\tif (!ext4_es_scan_clu(inode,\n\t\t\t\t\t      &ext4_es_is_mapped, lblk)) {\n\t\t\t\tret = ext4_clu_mapped(inode,\n\t\t\t\t\t\t      EXT4_B2C(sbi, lblk));\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto errout;\n\t\t\t\tif (ret == 0) {\n\t\t\t\t\tret = ext4_da_reserve_space(inode);\n\t\t\t\t\tif (ret != 0)   /* ENOSPC */\n\t\t\t\t\t\tgoto errout;\n\t\t\t\t} else {\n\t\t\t\t\tallocated = true;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tallocated = true;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = ext4_es_insert_delayed_block(inode, lblk, allocated);\n\nerrout:\n\treturn ret;\n}\n\n/*\n * This function is grabs code from the very beginning of\n * ext4_map_blocks, but assumes that the caller is from delayed write\n * time. This function looks up the requested blocks and sets the\n * buffer delay bit under the protection of i_data_sem.\n */\nstatic int ext4_da_map_blocks(struct inode *inode, sector_t iblock,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      struct buffer_head *bh)\n{\n\tstruct extent_status es;\n\tint retval;\n\tsector_t invalid_block = ~((sector_t) 0xffff);\n#ifdef ES_AGGRESSIVE_TEST\n\tstruct ext4_map_blocks orig_map;\n\n\tmemcpy(&orig_map, map, sizeof(*map));\n#endif\n\n\tif (invalid_block < ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es))\n\t\tinvalid_block = ~0;\n\n\tmap->m_flags = 0;\n\text_debug(\"ext4_da_map_blocks(): inode %lu, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, map->m_len,\n\t\t  (unsigned long) map->m_lblk);\n\n\t/* Lookup extent status tree firstly */\n\tif (ext4_es_lookup_extent(inode, iblock, NULL, &es)) {\n\t\tif (ext4_es_is_hole(&es)) {\n\t\t\tretval = 0;\n\t\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto add_delayed;\n\t\t}\n\n\t\t/*\n\t\t * Delayed extent could be allocated by fallocate.\n\t\t * So we need to check it.\n\t\t */\n\t\tif (ext4_es_is_delayed(&es) && !ext4_es_is_unwritten(&es)) {\n\t\t\tmap_bh(bh, inode->i_sb, invalid_block);\n\t\t\tset_buffer_new(bh);\n\t\t\tset_buffer_delay(bh);\n\t\t\treturn 0;\n\t\t}\n\n\t\tmap->m_pblk = ext4_es_pblock(&es) + iblock - es.es_lblk;\n\t\tretval = es.es_len - (iblock - es.es_lblk);\n\t\tif (retval > map->m_len)\n\t\t\tretval = map->m_len;\n\t\tmap->m_len = retval;\n\t\tif (ext4_es_is_written(&es))\n\t\t\tmap->m_flags |= EXT4_MAP_MAPPED;\n\t\telse if (ext4_es_is_unwritten(&es))\n\t\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\telse\n\t\t\tBUG();\n\n#ifdef ES_AGGRESSIVE_TEST\n\t\text4_map_blocks_es_recheck(NULL, inode, map, &orig_map, 0);\n#endif\n\t\treturn retval;\n\t}\n\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_has_inline_data(inode))\n\t\tretval = 0;\n\telse if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tretval = ext4_ext_map_blocks(NULL, inode, map, 0);\n\telse\n\t\tretval = ext4_ind_map_blocks(NULL, inode, map, 0);\n\nadd_delayed:\n\tif (retval == 0) {\n\t\tint ret;\n\n\t\t/*\n\t\t * XXX: __block_prepare_write() unmaps passed block,\n\t\t * is it OK?\n\t\t */\n\n\t\tret = ext4_insert_delayed_block(inode, map->m_lblk);\n\t\tif (ret != 0) {\n\t\t\tretval = ret;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tmap_bh(bh, inode->i_sb, invalid_block);\n\t\tset_buffer_new(bh);\n\t\tset_buffer_delay(bh);\n\t} else if (retval > 0) {\n\t\tint ret;\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk, map->m_len,\n\t\t\t\t\t    map->m_pblk, status);\n\t\tif (ret != 0)\n\t\t\tretval = ret;\n\t}\n\nout_unlock:\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\n\treturn retval;\n}\n\n/*\n * This is a special get_block_t callback which is used by\n * ext4_da_write_begin().  It will either return mapped block or\n * reserve space for a single block.\n *\n * For delayed buffer_head we have BH_Mapped, BH_New, BH_Delay set.\n * We also have b_blocknr = -1 and b_bdev initialized properly\n *\n * For unwritten buffer_head we have BH_Mapped, BH_New, BH_Unwritten set.\n * We also have b_blocknr = physicalblock mapping unwritten extent and b_bdev\n * initialized properly.\n */\nint ext4_da_get_block_prep(struct inode *inode, sector_t iblock,\n\t\t\t   struct buffer_head *bh, int create)\n{\n\tstruct ext4_map_blocks map;\n\tint ret = 0;\n\n\tBUG_ON(create == 0);\n\tBUG_ON(bh->b_size != inode->i_sb->s_blocksize);\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = 1;\n\n\t/*\n\t * first, we need to know whether the block is allocated already\n\t * preallocated blocks are unmapped but should treated\n\t * the same as allocated blocks.\n\t */\n\tret = ext4_da_map_blocks(inode, iblock, &map, bh);\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\text4_update_bh_state(bh, map.m_flags);\n\n\tif (buffer_unwritten(bh)) {\n\t\t/* A delayed write to unwritten bh should be marked\n\t\t * new and mapped.  Mapped ensures that we don't do\n\t\t * get_block multiple times when we write to the same\n\t\t * offset and new ensures that we do proper zero out\n\t\t * for partial write.\n\t\t */\n\t\tset_buffer_new(bh);\n\t\tset_buffer_mapped(bh);\n\t}\n\treturn 0;\n}\n\nstatic int bget_one(handle_t *handle, struct buffer_head *bh)\n{\n\tget_bh(bh);\n\treturn 0;\n}\n\nstatic int bput_one(handle_t *handle, struct buffer_head *bh)\n{\n\tput_bh(bh);\n\treturn 0;\n}\n\nstatic int __ext4_journalled_writepage(struct page *page,\n\t\t\t\t       unsigned int len)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head *page_bufs = NULL;\n\thandle_t *handle = NULL;\n\tint ret = 0, err = 0;\n\tint inline_data = ext4_has_inline_data(inode);\n\tstruct buffer_head *inode_bh = NULL;\n\n\tClearPageChecked(page);\n\n\tif (inline_data) {\n\t\tBUG_ON(page->index != 0);\n\t\tBUG_ON(len > ext4_get_max_inline_size(inode));\n\t\tinode_bh = ext4_journalled_write_inline_data(inode, len, page);\n\t\tif (inode_bh == NULL)\n\t\t\tgoto out;\n\t} else {\n\t\tpage_bufs = page_buffers(page);\n\t\tif (!page_bufs) {\n\t\t\tBUG();\n\t\t\tgoto out;\n\t\t}\n\t\text4_walk_page_buffers(handle, page_bufs, 0, len,\n\t\t\t\t       NULL, bget_one);\n\t}\n\t/*\n\t * We need to release the page lock before we start the\n\t * journal, so grab a reference so the page won't disappear\n\t * out from under us.\n\t */\n\tget_page(page);\n\tunlock_page(page);\n\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,\n\t\t\t\t    ext4_writepage_trans_blocks(inode));\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tput_page(page);\n\t\tgoto out_no_pagelock;\n\t}\n\tBUG_ON(!ext4_handle_valid(handle));\n\n\tlock_page(page);\n\tput_page(page);\n\tif (page->mapping != mapping) {\n\t\t/* The page got truncated from under us */\n\t\text4_journal_stop(handle);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (inline_data) {\n\t\tret = ext4_mark_inode_dirty(handle, inode);\n\t} else {\n\t\tret = ext4_walk_page_buffers(handle, page_bufs, 0, len, NULL,\n\t\t\t\t\t     do_journal_get_write_access);\n\n\t\terr = ext4_walk_page_buffers(handle, page_bufs, 0, len, NULL,\n\t\t\t\t\t     write_end_fn);\n\t}\n\tif (ret == 0)\n\t\tret = err;\n\tEXT4_I(inode)->i_datasync_tid = handle->h_transaction->t_tid;\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\n\tif (!ext4_has_inline_data(inode))\n\t\text4_walk_page_buffers(NULL, page_bufs, 0, len,\n\t\t\t\t       NULL, bput_one);\n\text4_set_inode_state(inode, EXT4_STATE_JDATA);\nout:\n\tunlock_page(page);\nout_no_pagelock:\n\tbrelse(inode_bh);\n\treturn ret;\n}\n\n/*\n * Note that we don't need to start a transaction unless we're journaling data\n * because we should have holes filled from ext4_page_mkwrite(). We even don't\n * need to file the inode to the transaction's list in ordered mode because if\n * we are writing back data added by write(), the inode is already there and if\n * we are writing back data modified via mmap(), no one guarantees in which\n * transaction the data will hit the disk. In case we are journaling data, we\n * cannot start transaction directly because transaction start ranks above page\n * lock so we have to do some magic.\n *\n * This function can get called via...\n *   - ext4_writepages after taking page lock (have journal handle)\n *   - journal_submit_inode_data_buffers (no journal handle)\n *   - shrink_page_list via the kswapd/direct reclaim (no journal handle)\n *   - grab_page_cache when doing write_begin (have journal handle)\n *\n * We don't do any block allocation in this function. If we have page with\n * multiple blocks we need to write those buffer_heads that are mapped. This\n * is important for mmaped based write. So if we do with blocksize 1K\n * truncate(f, 1024);\n * a = mmap(f, 0, 4096);\n * a[0] = 'a';\n * truncate(f, 4096);\n * we have in the page first buffer_head mapped via page_mkwrite call back\n * but other buffer_heads would be unmapped but dirty (dirty done via the\n * do_wp_page). So writepage should write the first block. If we modify\n * the mmap area beyond 1024 we will again get a page_fault and the\n * page_mkwrite callback will do the block allocation and mark the\n * buffer_heads mapped.\n *\n * We redirty the page if we have any buffer_heads that is either delay or\n * unwritten in the page.\n *\n * We can get recursively called as show below.\n *\n *\text4_writepage() -> kmalloc() -> __alloc_pages() -> page_launder() ->\n *\t\text4_writepage()\n *\n * But since we don't do any block allocation we should not deadlock.\n * Page also have the dirty flag cleared so we don't get recurive page_lock.\n */\nstatic int ext4_writepage(struct page *page,\n\t\t\t  struct writeback_control *wbc)\n{\n\tint ret = 0;\n\tloff_t size;\n\tunsigned int len;\n\tstruct buffer_head *page_bufs = NULL;\n\tstruct inode *inode = page->mapping->host;\n\tstruct ext4_io_submit io_submit;\n\tbool keep_towrite = false;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb)))) {\n\t\text4_invalidatepage(page, 0, PAGE_SIZE);\n\t\tunlock_page(page);\n\t\treturn -EIO;\n\t}\n\n\ttrace_ext4_writepage(page);\n\tsize = i_size_read(inode);\n\tif (page->index == size >> PAGE_SHIFT &&\n\t    !ext4_verity_in_progress(inode))\n\t\tlen = size & ~PAGE_MASK;\n\telse\n\t\tlen = PAGE_SIZE;\n\n\tpage_bufs = page_buffers(page);\n\t/*\n\t * We cannot do block allocation or other extent handling in this\n\t * function. If there are buffers needing that, we have to redirty\n\t * the page. But we may reach here when we do a journal commit via\n\t * journal_submit_inode_data_buffers() and in that case we must write\n\t * allocated buffers to achieve data=ordered mode guarantees.\n\t *\n\t * Also, if there is only one buffer per page (the fs block\n\t * size == the page size), if one buffer needs block\n\t * allocation or needs to modify the extent tree to clear the\n\t * unwritten flag, we know that the page can't be written at\n\t * all, so we might as well refuse the write immediately.\n\t * Unfortunately if the block size != page size, we can't as\n\t * easily detect this case using ext4_walk_page_buffers(), but\n\t * for the extremely common case, this is an optimization that\n\t * skips a useless round trip through ext4_bio_write_page().\n\t */\n\tif (ext4_walk_page_buffers(NULL, page_bufs, 0, len, NULL,\n\t\t\t\t   ext4_bh_delay_or_unwritten)) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tif ((current->flags & PF_MEMALLOC) ||\n\t\t    (inode->i_sb->s_blocksize == PAGE_SIZE)) {\n\t\t\t/*\n\t\t\t * For memory cleaning there's no point in writing only\n\t\t\t * some buffers. So just bail out. Warn if we came here\n\t\t\t * from direct reclaim.\n\t\t\t */\n\t\t\tWARN_ON_ONCE((current->flags & (PF_MEMALLOC|PF_KSWAPD))\n\t\t\t\t\t\t\t== PF_MEMALLOC);\n\t\t\tunlock_page(page);\n\t\t\treturn 0;\n\t\t}\n\t\tkeep_towrite = true;\n\t}\n\n\tif (PageChecked(page) && ext4_should_journal_data(inode))\n\t\t/*\n\t\t * It's mmapped pagecache.  Add buffers and journal it.  There\n\t\t * doesn't seem much point in redirtying the page here.\n\t\t */\n\t\treturn __ext4_journalled_writepage(page, len);\n\n\text4_io_submit_init(&io_submit, wbc);\n\tio_submit.io_end = ext4_init_io_end(inode, GFP_NOFS);\n\tif (!io_submit.io_end) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tunlock_page(page);\n\t\treturn -ENOMEM;\n\t}\n\tret = ext4_bio_write_page(&io_submit, page, len, wbc, keep_towrite);\n\text4_io_submit(&io_submit);\n\t/* Drop io_end reference we got from init */\n\text4_put_io_end_defer(io_submit.io_end);\n\treturn ret;\n}\n\nstatic int mpage_submit_page(struct mpage_da_data *mpd, struct page *page)\n{\n\tint len;\n\tloff_t size;\n\tint err;\n\n\tBUG_ON(page->index != mpd->first_page);\n\tclear_page_dirty_for_io(page);\n\t/*\n\t * We have to be very careful here!  Nothing protects writeback path\n\t * against i_size changes and the page can be writeably mapped into\n\t * page tables. So an application can be growing i_size and writing\n\t * data through mmap while writeback runs. clear_page_dirty_for_io()\n\t * write-protects our page in page tables and the page cannot get\n\t * written to again until we release page lock. So only after\n\t * clear_page_dirty_for_io() we are safe to sample i_size for\n\t * ext4_bio_write_page() to zero-out tail of the written page. We rely\n\t * on the barrier provided by TestClearPageDirty in\n\t * clear_page_dirty_for_io() to make sure i_size is really sampled only\n\t * after page tables are updated.\n\t */\n\tsize = i_size_read(mpd->inode);\n\tif (page->index == size >> PAGE_SHIFT &&\n\t    !ext4_verity_in_progress(mpd->inode))\n\t\tlen = size & ~PAGE_MASK;\n\telse\n\t\tlen = PAGE_SIZE;\n\terr = ext4_bio_write_page(&mpd->io_submit, page, len, mpd->wbc, false);\n\tif (!err)\n\t\tmpd->wbc->nr_to_write--;\n\tmpd->first_page++;\n\n\treturn err;\n}\n\n#define BH_FLAGS ((1 << BH_Unwritten) | (1 << BH_Delay))\n\n/*\n * mballoc gives us at most this number of blocks...\n * XXX: That seems to be only a limitation of ext4_mb_normalize_request().\n * The rest of mballoc seems to handle chunks up to full group size.\n */\n#define MAX_WRITEPAGES_EXTENT_LEN 2048\n\n/*\n * mpage_add_bh_to_extent - try to add bh to extent of blocks to map\n *\n * @mpd - extent of blocks\n * @lblk - logical number of the block in the file\n * @bh - buffer head we want to add to the extent\n *\n * The function is used to collect contig. blocks in the same state. If the\n * buffer doesn't require mapping for writeback and we haven't started the\n * extent of buffers to map yet, the function returns 'true' immediately - the\n * caller can write the buffer right away. Otherwise the function returns true\n * if the block has been added to the extent, false if the block couldn't be\n * added.\n */\nstatic bool mpage_add_bh_to_extent(struct mpage_da_data *mpd, ext4_lblk_t lblk,\n\t\t\t\t   struct buffer_head *bh)\n{\n\tstruct ext4_map_blocks *map = &mpd->map;\n\n\t/* Buffer that doesn't need mapping for writeback? */\n\tif (!buffer_dirty(bh) || !buffer_mapped(bh) ||\n\t    (!buffer_delay(bh) && !buffer_unwritten(bh))) {\n\t\t/* So far no extent to map => we write the buffer right away */\n\t\tif (map->m_len == 0)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\n\t/* First block in the extent? */\n\tif (map->m_len == 0) {\n\t\t/* We cannot map unless handle is started... */\n\t\tif (!mpd->do_map)\n\t\t\treturn false;\n\t\tmap->m_lblk = lblk;\n\t\tmap->m_len = 1;\n\t\tmap->m_flags = bh->b_state & BH_FLAGS;\n\t\treturn true;\n\t}\n\n\t/* Don't go larger than mballoc is willing to allocate */\n\tif (map->m_len >= MAX_WRITEPAGES_EXTENT_LEN)\n\t\treturn false;\n\n\t/* Can we merge the block to our big extent? */\n\tif (lblk == map->m_lblk + map->m_len &&\n\t    (bh->b_state & BH_FLAGS) == map->m_flags) {\n\t\tmap->m_len++;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * mpage_process_page_bufs - submit page buffers for IO or add them to extent\n *\n * @mpd - extent of blocks for mapping\n * @head - the first buffer in the page\n * @bh - buffer we should start processing from\n * @lblk - logical number of the block in the file corresponding to @bh\n *\n * Walk through page buffers from @bh upto @head (exclusive) and either submit\n * the page for IO if all buffers in this page were mapped and there's no\n * accumulated extent of buffers to map or add buffers in the page to the\n * extent of buffers to map. The function returns 1 if the caller can continue\n * by processing the next page, 0 if it should stop adding buffers to the\n * extent to map because we cannot extend it anymore. It can also return value\n * < 0 in case of error during IO submission.\n */\nstatic int mpage_process_page_bufs(struct mpage_da_data *mpd,\n\t\t\t\t   struct buffer_head *head,\n\t\t\t\t   struct buffer_head *bh,\n\t\t\t\t   ext4_lblk_t lblk)\n{\n\tstruct inode *inode = mpd->inode;\n\tint err;\n\text4_lblk_t blocks = (i_size_read(inode) + i_blocksize(inode) - 1)\n\t\t\t\t\t\t\t>> inode->i_blkbits;\n\n\tif (ext4_verity_in_progress(inode))\n\t\tblocks = EXT_MAX_BLOCKS;\n\n\tdo {\n\t\tBUG_ON(buffer_locked(bh));\n\n\t\tif (lblk >= blocks || !mpage_add_bh_to_extent(mpd, lblk, bh)) {\n\t\t\t/* Found extent to map? */\n\t\t\tif (mpd->map.m_len)\n\t\t\t\treturn 0;\n\t\t\t/* Buffer needs mapping and handle is not started? */\n\t\t\tif (!mpd->do_map)\n\t\t\t\treturn 0;\n\t\t\t/* Everything mapped so far and we hit EOF */\n\t\t\tbreak;\n\t\t}\n\t} while (lblk++, (bh = bh->b_this_page) != head);\n\t/* So far everything mapped? Submit the page for IO. */\n\tif (mpd->map.m_len == 0) {\n\t\terr = mpage_submit_page(mpd, head->b_page);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\treturn lblk < blocks;\n}\n\n/*\n * mpage_process_page - update page buffers corresponding to changed extent and\n *\t\t       may submit fully mapped page for IO\n *\n * @mpd\t\t- description of extent to map, on return next extent to map\n * @m_lblk\t- logical block mapping.\n * @m_pblk\t- corresponding physical mapping.\n * @map_bh\t- determines on return whether this page requires any further\n *\t\t  mapping or not.\n * Scan given page buffers corresponding to changed extent and update buffer\n * state according to new extent state.\n * We map delalloc buffers to their physical location, clear unwritten bits.\n * If the given page is not fully mapped, we update @map to the next extent in\n * the given page that needs mapping & return @map_bh as true.\n */\nstatic int mpage_process_page(struct mpage_da_data *mpd, struct page *page,\n\t\t\t      ext4_lblk_t *m_lblk, ext4_fsblk_t *m_pblk,\n\t\t\t      bool *map_bh)\n{\n\tstruct buffer_head *head, *bh;\n\text4_io_end_t *io_end = mpd->io_submit.io_end;\n\text4_lblk_t lblk = *m_lblk;\n\text4_fsblk_t pblock = *m_pblk;\n\tint err = 0;\n\tint blkbits = mpd->inode->i_blkbits;\n\tssize_t io_end_size = 0;\n\tstruct ext4_io_end_vec *io_end_vec = ext4_last_io_end_vec(io_end);\n\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tif (lblk < mpd->map.m_lblk)\n\t\t\tcontinue;\n\t\tif (lblk >= mpd->map.m_lblk + mpd->map.m_len) {\n\t\t\t/*\n\t\t\t * Buffer after end of mapped extent.\n\t\t\t * Find next buffer in the page to map.\n\t\t\t */\n\t\t\tmpd->map.m_len = 0;\n\t\t\tmpd->map.m_flags = 0;\n\t\t\tio_end_vec->size += io_end_size;\n\t\t\tio_end_size = 0;\n\n\t\t\terr = mpage_process_page_bufs(mpd, head, bh, lblk);\n\t\t\tif (err > 0)\n\t\t\t\terr = 0;\n\t\t\tif (!err && mpd->map.m_len && mpd->map.m_lblk > lblk) {\n\t\t\t\tio_end_vec = ext4_alloc_io_end_vec(io_end);\n\t\t\t\tio_end_vec->offset = mpd->map.m_lblk << blkbits;\n\t\t\t}\n\t\t\t*map_bh = true;\n\t\t\tgoto out;\n\t\t}\n\t\tif (buffer_delay(bh)) {\n\t\t\tclear_buffer_delay(bh);\n\t\t\tbh->b_blocknr = pblock++;\n\t\t}\n\t\tclear_buffer_unwritten(bh);\n\t\tio_end_size += (1 << blkbits);\n\t} while (lblk++, (bh = bh->b_this_page) != head);\n\n\tio_end_vec->size += io_end_size;\n\tio_end_size = 0;\n\t*map_bh = false;\nout:\n\t*m_lblk = lblk;\n\t*m_pblk = pblock;\n\treturn err;\n}\n\n/*\n * mpage_map_buffers - update buffers corresponding to changed extent and\n *\t\t       submit fully mapped pages for IO\n *\n * @mpd - description of extent to map, on return next extent to map\n *\n * Scan buffers corresponding to changed extent (we expect corresponding pages\n * to be already locked) and update buffer state according to new extent state.\n * We map delalloc buffers to their physical location, clear unwritten bits,\n * and mark buffers as uninit when we perform writes to unwritten extents\n * and do extent conversion after IO is finished. If the last page is not fully\n * mapped, we update @map to the next extent in the last page that needs\n * mapping. Otherwise we submit the page for IO.\n */\nstatic int mpage_map_and_submit_buffers(struct mpage_da_data *mpd)\n{\n\tstruct pagevec pvec;\n\tint nr_pages, i;\n\tstruct inode *inode = mpd->inode;\n\tint bpp_bits = PAGE_SHIFT - inode->i_blkbits;\n\tpgoff_t start, end;\n\text4_lblk_t lblk;\n\text4_fsblk_t pblock;\n\tint err;\n\tbool map_bh = false;\n\n\tstart = mpd->map.m_lblk >> bpp_bits;\n\tend = (mpd->map.m_lblk + mpd->map.m_len - 1) >> bpp_bits;\n\tlblk = start << bpp_bits;\n\tpblock = mpd->map.m_pblk;\n\n\tpagevec_init(&pvec);\n\twhile (start <= end) {\n\t\tnr_pages = pagevec_lookup_range(&pvec, inode->i_mapping,\n\t\t\t\t\t\t&start, end);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\terr = mpage_process_page(mpd, page, &lblk, &pblock,\n\t\t\t\t\t\t &map_bh);\n\t\t\t/*\n\t\t\t * If map_bh is true, means page may require further bh\n\t\t\t * mapping, or maybe the page was submitted for IO.\n\t\t\t * So we return to call further extent mapping.\n\t\t\t */\n\t\t\tif (err < 0 || map_bh == true)\n\t\t\t\tgoto out;\n\t\t\t/* Page fully mapped - let IO run! */\n\t\t\terr = mpage_submit_page(mpd, page);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n\t/* Extent fully mapped and matches with page boundary. We are done. */\n\tmpd->map.m_len = 0;\n\tmpd->map.m_flags = 0;\n\treturn 0;\nout:\n\tpagevec_release(&pvec);\n\treturn err;\n}\n\nstatic int mpage_map_one_extent(handle_t *handle, struct mpage_da_data *mpd)\n{\n\tstruct inode *inode = mpd->inode;\n\tstruct ext4_map_blocks *map = &mpd->map;\n\tint get_blocks_flags;\n\tint err, dioread_nolock;\n\n\ttrace_ext4_da_write_pages_extent(inode, map);\n\t/*\n\t * Call ext4_map_blocks() to allocate any delayed allocation blocks, or\n\t * to convert an unwritten extent to be initialized (in the case\n\t * where we have written into one or more preallocated blocks).  It is\n\t * possible that we're going to need more metadata blocks than\n\t * previously reserved. However we must not fail because we're in\n\t * writeback and there is nothing we can do about it so it might result\n\t * in data loss.  So use reserved blocks to allocate metadata if\n\t * possible.\n\t *\n\t * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE if\n\t * the blocks in question are delalloc blocks.  This indicates\n\t * that the blocks and quotas has already been checked when\n\t * the data was copied into the page cache.\n\t */\n\tget_blocks_flags = EXT4_GET_BLOCKS_CREATE |\n\t\t\t   EXT4_GET_BLOCKS_METADATA_NOFAIL |\n\t\t\t   EXT4_GET_BLOCKS_IO_SUBMIT;\n\tdioread_nolock = ext4_should_dioread_nolock(inode);\n\tif (dioread_nolock)\n\t\tget_blocks_flags |= EXT4_GET_BLOCKS_IO_CREATE_EXT;\n\tif (map->m_flags & (1 << BH_Delay))\n\t\tget_blocks_flags |= EXT4_GET_BLOCKS_DELALLOC_RESERVE;\n\n\terr = ext4_map_blocks(handle, inode, map, get_blocks_flags);\n\tif (err < 0)\n\t\treturn err;\n\tif (dioread_nolock && (map->m_flags & EXT4_MAP_UNWRITTEN)) {\n\t\tif (!mpd->io_submit.io_end->handle &&\n\t\t    ext4_handle_valid(handle)) {\n\t\t\tmpd->io_submit.io_end->handle = handle->h_rsv_handle;\n\t\t\thandle->h_rsv_handle = NULL;\n\t\t}\n\t\text4_set_io_unwritten_flag(inode, mpd->io_submit.io_end);\n\t}\n\n\tBUG_ON(map->m_len == 0);\n\treturn 0;\n}\n\n/*\n * mpage_map_and_submit_extent - map extent starting at mpd->lblk of length\n *\t\t\t\t mpd->len and submit pages underlying it for IO\n *\n * @handle - handle for journal operations\n * @mpd - extent to map\n * @give_up_on_write - we set this to true iff there is a fatal error and there\n *                     is no hope of writing the data. The caller should discard\n *                     dirty pages to avoid infinite loops.\n *\n * The function maps extent starting at mpd->lblk of length mpd->len. If it is\n * delayed, blocks are allocated, if it is unwritten, we may need to convert\n * them to initialized or split the described range from larger unwritten\n * extent. Note that we need not map all the described range since allocation\n * can return less blocks or the range is covered by more unwritten extents. We\n * cannot map more because we are limited by reserved transaction credits. On\n * the other hand we always make sure that the last touched page is fully\n * mapped so that it can be written out (and thus forward progress is\n * guaranteed). After mapping we submit all mapped pages for IO.\n */\nstatic int mpage_map_and_submit_extent(handle_t *handle,\n\t\t\t\t       struct mpage_da_data *mpd,\n\t\t\t\t       bool *give_up_on_write)\n{\n\tstruct inode *inode = mpd->inode;\n\tstruct ext4_map_blocks *map = &mpd->map;\n\tint err;\n\tloff_t disksize;\n\tint progress = 0;\n\text4_io_end_t *io_end = mpd->io_submit.io_end;\n\tstruct ext4_io_end_vec *io_end_vec = ext4_alloc_io_end_vec(io_end);\n\n\tio_end_vec->offset = ((loff_t)map->m_lblk) << inode->i_blkbits;\n\tdo {\n\t\terr = mpage_map_one_extent(handle, mpd);\n\t\tif (err < 0) {\n\t\t\tstruct super_block *sb = inode->i_sb;\n\n\t\t\tif (ext4_forced_shutdown(EXT4_SB(sb)) ||\n\t\t\t    EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED)\n\t\t\t\tgoto invalidate_dirty_pages;\n\t\t\t/*\n\t\t\t * Let the uper layers retry transient errors.\n\t\t\t * In the case of ENOSPC, if ext4_count_free_blocks()\n\t\t\t * is non-zero, a commit should free up blocks.\n\t\t\t */\n\t\t\tif ((err == -ENOMEM) ||\n\t\t\t    (err == -ENOSPC && ext4_count_free_clusters(sb))) {\n\t\t\t\tif (progress)\n\t\t\t\t\tgoto update_disksize;\n\t\t\t\treturn err;\n\t\t\t}\n\t\t\text4_msg(sb, KERN_CRIT,\n\t\t\t\t \"Delayed block allocation failed for \"\n\t\t\t\t \"inode %lu at logical offset %llu with\"\n\t\t\t\t \" max blocks %u with error %d\",\n\t\t\t\t inode->i_ino,\n\t\t\t\t (unsigned long long)map->m_lblk,\n\t\t\t\t (unsigned)map->m_len, -err);\n\t\t\text4_msg(sb, KERN_CRIT,\n\t\t\t\t \"This should not happen!! Data will \"\n\t\t\t\t \"be lost\\n\");\n\t\t\tif (err == -ENOSPC)\n\t\t\t\text4_print_free_blocks(inode);\n\t\tinvalidate_dirty_pages:\n\t\t\t*give_up_on_write = true;\n\t\t\treturn err;\n\t\t}\n\t\tprogress = 1;\n\t\t/*\n\t\t * Update buffer state, submit mapped pages, and get us new\n\t\t * extent to map\n\t\t */\n\t\terr = mpage_map_and_submit_buffers(mpd);\n\t\tif (err < 0)\n\t\t\tgoto update_disksize;\n\t} while (map->m_len);\n\nupdate_disksize:\n\t/*\n\t * Update on-disk size after IO is submitted.  Races with\n\t * truncate are avoided by checking i_size under i_data_sem.\n\t */\n\tdisksize = ((loff_t)mpd->first_page) << PAGE_SHIFT;\n\tif (disksize > EXT4_I(inode)->i_disksize) {\n\t\tint err2;\n\t\tloff_t i_size;\n\n\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\ti_size = i_size_read(inode);\n\t\tif (disksize > i_size)\n\t\t\tdisksize = i_size;\n\t\tif (disksize > EXT4_I(inode)->i_disksize)\n\t\t\tEXT4_I(inode)->i_disksize = disksize;\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\terr2 = ext4_mark_inode_dirty(handle, inode);\n\t\tif (err2)\n\t\t\text4_error(inode->i_sb,\n\t\t\t\t   \"Failed to mark inode %lu dirty\",\n\t\t\t\t   inode->i_ino);\n\t\tif (!err)\n\t\t\terr = err2;\n\t}\n\treturn err;\n}\n\n/*\n * Calculate the total number of credits to reserve for one writepages\n * iteration. This is called from ext4_writepages(). We map an extent of\n * up to MAX_WRITEPAGES_EXTENT_LEN blocks and then we go on and finish mapping\n * the last partial page. So in total we can map MAX_WRITEPAGES_EXTENT_LEN +\n * bpp - 1 blocks in bpp different extents.\n */\nstatic int ext4_da_writepages_trans_blocks(struct inode *inode)\n{\n\tint bpp = ext4_journal_blocks_per_page(inode);\n\n\treturn ext4_meta_trans_blocks(inode,\n\t\t\t\tMAX_WRITEPAGES_EXTENT_LEN + bpp - 1, bpp);\n}\n\n/*\n * mpage_prepare_extent_to_map - find & lock contiguous range of dirty pages\n * \t\t\t\t and underlying extent to map\n *\n * @mpd - where to look for pages\n *\n * Walk dirty pages in the mapping. If they are fully mapped, submit them for\n * IO immediately. When we find a page which isn't mapped we start accumulating\n * extent of buffers underlying these pages that needs mapping (formed by\n * either delayed or unwritten buffers). We also lock the pages containing\n * these buffers. The extent found is returned in @mpd structure (starting at\n * mpd->lblk with length mpd->len blocks).\n *\n * Note that this function can attach bios to one io_end structure which are\n * neither logically nor physically contiguous. Although it may seem as an\n * unnecessary complication, it is actually inevitable in blocksize < pagesize\n * case as we need to track IO to all buffers underlying a page in one io_end.\n */\nstatic int mpage_prepare_extent_to_map(struct mpage_da_data *mpd)\n{\n\tstruct address_space *mapping = mpd->inode->i_mapping;\n\tstruct pagevec pvec;\n\tunsigned int nr_pages;\n\tlong left = mpd->wbc->nr_to_write;\n\tpgoff_t index = mpd->first_page;\n\tpgoff_t end = mpd->last_page;\n\txa_mark_t tag;\n\tint i, err = 0;\n\tint blkbits = mpd->inode->i_blkbits;\n\text4_lblk_t lblk;\n\tstruct buffer_head *head;\n\n\tif (mpd->wbc->sync_mode == WB_SYNC_ALL || mpd->wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\n\n\tpagevec_init(&pvec);\n\tmpd->map.m_len = 0;\n\tmpd->next_page = index;\n\twhile (index <= end) {\n\t\tnr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,\n\t\t\t\ttag);\n\t\tif (nr_pages == 0)\n\t\t\tgoto out;\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/*\n\t\t\t * Accumulated enough dirty pages? This doesn't apply\n\t\t\t * to WB_SYNC_ALL mode. For integrity sync we have to\n\t\t\t * keep going because someone may be concurrently\n\t\t\t * dirtying pages, and we might have synced a lot of\n\t\t\t * newly appeared dirty pages, but have not synced all\n\t\t\t * of the old dirty pages.\n\t\t\t */\n\t\t\tif (mpd->wbc->sync_mode == WB_SYNC_NONE && left <= 0)\n\t\t\t\tgoto out;\n\n\t\t\t/* If we can't merge this page, we are done. */\n\t\t\tif (mpd->map.m_len > 0 && mpd->next_page != page->index)\n\t\t\t\tgoto out;\n\n\t\t\tlock_page(page);\n\t\t\t/*\n\t\t\t * If the page is no longer dirty, or its mapping no\n\t\t\t * longer corresponds to inode we are writing (which\n\t\t\t * means it has been truncated or invalidated), or the\n\t\t\t * page is already under writeback and we are not doing\n\t\t\t * a data integrity writeback, skip the page\n\t\t\t */\n\t\t\tif (!PageDirty(page) ||\n\t\t\t    (PageWriteback(page) &&\n\t\t\t     (mpd->wbc->sync_mode == WB_SYNC_NONE)) ||\n\t\t\t    unlikely(page->mapping != mapping)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\twait_on_page_writeback(page);\n\t\t\tBUG_ON(PageWriteback(page));\n\n\t\t\tif (mpd->map.m_len == 0)\n\t\t\t\tmpd->first_page = page->index;\n\t\t\tmpd->next_page = page->index + 1;\n\t\t\t/* Add all dirty buffers to mpd */\n\t\t\tlblk = ((ext4_lblk_t)page->index) <<\n\t\t\t\t(PAGE_SHIFT - blkbits);\n\t\t\thead = page_buffers(page);\n\t\t\terr = mpage_process_page_bufs(mpd, head, head, lblk);\n\t\t\tif (err <= 0)\n\t\t\t\tgoto out;\n\t\t\terr = 0;\n\t\t\tleft--;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n\treturn 0;\nout:\n\tpagevec_release(&pvec);\n\treturn err;\n}\n\nstatic int ext4_writepages(struct address_space *mapping,\n\t\t\t   struct writeback_control *wbc)\n{\n\tpgoff_t\twriteback_index = 0;\n\tlong nr_to_write = wbc->nr_to_write;\n\tint range_whole = 0;\n\tint cycled = 1;\n\thandle_t *handle = NULL;\n\tstruct mpage_da_data mpd;\n\tstruct inode *inode = mapping->host;\n\tint needed_blocks, rsv_blocks = 0, ret = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);\n\tbool done;\n\tstruct blk_plug plug;\n\tbool give_up_on_write = false;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tpercpu_down_read(&sbi->s_journal_flag_rwsem);\n\ttrace_ext4_writepages(inode, wbc);\n\n\t/*\n\t * No pages to write? This is mainly a kludge to avoid starting\n\t * a transaction for special inodes like journal inode on last iput()\n\t * because that could violate lock ordering on umount\n\t */\n\tif (!mapping->nrpages || !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\tgoto out_writepages;\n\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = generic_writepages(mapping, wbc);\n\t\tgoto out_writepages;\n\t}\n\n\t/*\n\t * If the filesystem has aborted, it is read-only, so return\n\t * right away instead of dumping stack traces later on that\n\t * will obscure the real source of the problem.  We test\n\t * EXT4_MF_FS_ABORTED instead of sb->s_flag's SB_RDONLY because\n\t * the latter could be true if the filesystem is mounted\n\t * read-only, and in that case, ext4_writepages should\n\t * *never* be called, so if that ever happens, we would want\n\t * the stack trace.\n\t */\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(mapping->host->i_sb)) ||\n\t\t     sbi->s_mount_flags & EXT4_MF_FS_ABORTED)) {\n\t\tret = -EROFS;\n\t\tgoto out_writepages;\n\t}\n\n\t/*\n\t * If we have inline data and arrive here, it means that\n\t * we will soon create the block for the 1st page, so\n\t * we'd better clear the inline data here.\n\t */\n\tif (ext4_has_inline_data(inode)) {\n\t\t/* Just inode will be modified... */\n\t\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 1);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tgoto out_writepages;\n\t\t}\n\t\tBUG_ON(ext4_test_inode_state(inode,\n\t\t\t\tEXT4_STATE_MAY_INLINE_DATA));\n\t\text4_destroy_inline_data(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (ext4_should_dioread_nolock(inode)) {\n\t\t/*\n\t\t * We may need to convert up to one extent per block in\n\t\t * the page and we may dirty the inode.\n\t\t */\n\t\trsv_blocks = 1 + ext4_chunk_trans_blocks(inode,\n\t\t\t\t\t\tPAGE_SIZE >> inode->i_blkbits);\n\t}\n\n\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\trange_whole = 1;\n\n\tif (wbc->range_cyclic) {\n\t\twriteback_index = mapping->writeback_index;\n\t\tif (writeback_index)\n\t\t\tcycled = 0;\n\t\tmpd.first_page = writeback_index;\n\t\tmpd.last_page = -1;\n\t} else {\n\t\tmpd.first_page = wbc->range_start >> PAGE_SHIFT;\n\t\tmpd.last_page = wbc->range_end >> PAGE_SHIFT;\n\t}\n\n\tmpd.inode = inode;\n\tmpd.wbc = wbc;\n\text4_io_submit_init(&mpd.io_submit, wbc);\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, mpd.first_page, mpd.last_page);\n\tdone = false;\n\tblk_start_plug(&plug);\n\n\t/*\n\t * First writeback pages that don't need mapping - we can avoid\n\t * starting a transaction unnecessarily and also avoid being blocked\n\t * in the block layer on device congestion while having transaction\n\t * started.\n\t */\n\tmpd.do_map = 0;\n\tmpd.io_submit.io_end = ext4_init_io_end(inode, GFP_KERNEL);\n\tif (!mpd.io_submit.io_end) {\n\t\tret = -ENOMEM;\n\t\tgoto unplug;\n\t}\n\tret = mpage_prepare_extent_to_map(&mpd);\n\t/* Unlock pages we didn't use */\n\tmpage_release_unused_pages(&mpd, false);\n\t/* Submit prepared bio */\n\text4_io_submit(&mpd.io_submit);\n\text4_put_io_end_defer(mpd.io_submit.io_end);\n\tmpd.io_submit.io_end = NULL;\n\tif (ret < 0)\n\t\tgoto unplug;\n\n\twhile (!done && mpd.first_page <= mpd.last_page) {\n\t\t/* For each extent of pages we use new io_end */\n\t\tmpd.io_submit.io_end = ext4_init_io_end(inode, GFP_KERNEL);\n\t\tif (!mpd.io_submit.io_end) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We have two constraints: We find one extent to map and we\n\t\t * must always write out whole page (makes a difference when\n\t\t * blocksize < pagesize) so that we don't block on IO when we\n\t\t * try to write out the rest of the page. Journalled mode is\n\t\t * not supported by delalloc.\n\t\t */\n\t\tBUG_ON(ext4_should_journal_data(inode));\n\t\tneeded_blocks = ext4_da_writepages_trans_blocks(inode);\n\n\t\t/* start a new transaction */\n\t\thandle = ext4_journal_start_with_reserve(inode,\n\t\t\t\tEXT4_HT_WRITE_PAGE, needed_blocks, rsv_blocks);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\text4_msg(inode->i_sb, KERN_CRIT, \"%s: jbd2_start: \"\n\t\t\t       \"%ld pages, ino %lu; err %d\", __func__,\n\t\t\t\twbc->nr_to_write, inode->i_ino, ret);\n\t\t\t/* Release allocated io_end */\n\t\t\text4_put_io_end(mpd.io_submit.io_end);\n\t\t\tmpd.io_submit.io_end = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tmpd.do_map = 1;\n\n\t\ttrace_ext4_da_write_pages(inode, mpd.first_page, mpd.wbc);\n\t\tret = mpage_prepare_extent_to_map(&mpd);\n\t\tif (!ret) {\n\t\t\tif (mpd.map.m_len)\n\t\t\t\tret = mpage_map_and_submit_extent(handle, &mpd,\n\t\t\t\t\t&give_up_on_write);\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We scanned the whole range (or exhausted\n\t\t\t\t * nr_to_write), submitted what was mapped and\n\t\t\t\t * didn't find anything needing mapping. We are\n\t\t\t\t * done.\n\t\t\t\t */\n\t\t\t\tdone = true;\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Caution: If the handle is synchronous,\n\t\t * ext4_journal_stop() can wait for transaction commit\n\t\t * to finish which may depend on writeback of pages to\n\t\t * complete or on page lock to be released.  In that\n\t\t * case, we have to wait until after after we have\n\t\t * submitted all the IO, released page locks we hold,\n\t\t * and dropped io_end reference (for extent conversion\n\t\t * to be able to complete) before stopping the handle.\n\t\t */\n\t\tif (!ext4_handle_valid(handle) || handle->h_sync == 0) {\n\t\t\text4_journal_stop(handle);\n\t\t\thandle = NULL;\n\t\t\tmpd.do_map = 0;\n\t\t}\n\t\t/* Unlock pages we didn't use */\n\t\tmpage_release_unused_pages(&mpd, give_up_on_write);\n\t\t/* Submit prepared bio */\n\t\text4_io_submit(&mpd.io_submit);\n\n\t\t/*\n\t\t * Drop our io_end reference we got from init. We have\n\t\t * to be careful and use deferred io_end finishing if\n\t\t * we are still holding the transaction as we can\n\t\t * release the last reference to io_end which may end\n\t\t * up doing unwritten extent conversion.\n\t\t */\n\t\tif (handle) {\n\t\t\text4_put_io_end_defer(mpd.io_submit.io_end);\n\t\t\text4_journal_stop(handle);\n\t\t} else\n\t\t\text4_put_io_end(mpd.io_submit.io_end);\n\t\tmpd.io_submit.io_end = NULL;\n\n\t\tif (ret == -ENOSPC && sbi->s_journal) {\n\t\t\t/*\n\t\t\t * Commit the transaction which would\n\t\t\t * free blocks released in the transaction\n\t\t\t * and try again\n\t\t\t */\n\t\t\tjbd2_journal_force_commit_nested(sbi->s_journal);\n\t\t\tret = 0;\n\t\t\tcontinue;\n\t\t}\n\t\t/* Fatal error - ENOMEM, EIO... */\n\t\tif (ret)\n\t\t\tbreak;\n\t}\nunplug:\n\tblk_finish_plug(&plug);\n\tif (!ret && !cycled && wbc->nr_to_write > 0) {\n\t\tcycled = 1;\n\t\tmpd.last_page = writeback_index - 1;\n\t\tmpd.first_page = 0;\n\t\tgoto retry;\n\t}\n\n\t/* Update index */\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\t/*\n\t\t * Set the writeback_index so that range_cyclic\n\t\t * mode will write it back later\n\t\t */\n\t\tmapping->writeback_index = mpd.first_page;\n\nout_writepages:\n\ttrace_ext4_writepages_result(inode, wbc, ret,\n\t\t\t\t     nr_to_write - wbc->nr_to_write);\n\tpercpu_up_read(&sbi->s_journal_flag_rwsem);\n\treturn ret;\n}\n\nstatic int ext4_dax_writepages(struct address_space *mapping,\n\t\t\t       struct writeback_control *wbc)\n{\n\tint ret;\n\tlong nr_to_write = wbc->nr_to_write;\n\tstruct inode *inode = mapping->host;\n\tstruct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tpercpu_down_read(&sbi->s_journal_flag_rwsem);\n\ttrace_ext4_writepages(inode, wbc);\n\n\tret = dax_writeback_mapping_range(mapping, inode->i_sb->s_bdev, wbc);\n\ttrace_ext4_writepages_result(inode, wbc, ret,\n\t\t\t\t     nr_to_write - wbc->nr_to_write);\n\tpercpu_up_read(&sbi->s_journal_flag_rwsem);\n\treturn ret;\n}\n\nstatic int ext4_nonda_switch(struct super_block *sb)\n{\n\ts64 free_clusters, dirty_clusters;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\t/*\n\t * switch to non delalloc mode if we are running low\n\t * on free block. The free block accounting via percpu\n\t * counters can get slightly wrong with percpu_counter_batch getting\n\t * accumulated on each CPU without updating global counters\n\t * Delalloc need an accurate free block accounting. So switch\n\t * to non delalloc when we are near to error range.\n\t */\n\tfree_clusters =\n\t\tpercpu_counter_read_positive(&sbi->s_freeclusters_counter);\n\tdirty_clusters =\n\t\tpercpu_counter_read_positive(&sbi->s_dirtyclusters_counter);\n\t/*\n\t * Start pushing delalloc when 1/2 of free blocks are dirty.\n\t */\n\tif (dirty_clusters && (free_clusters < 2 * dirty_clusters))\n\t\ttry_to_writeback_inodes_sb(sb, WB_REASON_FS_FREE_SPACE);\n\n\tif (2 * free_clusters < 3 * dirty_clusters ||\n\t    free_clusters < (dirty_clusters + EXT4_FREECLUSTERS_WATERMARK)) {\n\t\t/*\n\t\t * free block count is less than 150% of dirty blocks\n\t\t * or free blocks is less than watermark\n\t\t */\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/* We always reserve for an inode update; the superblock could be there too */\nstatic int ext4_da_write_credits(struct inode *inode, loff_t pos, unsigned len)\n{\n\tif (likely(ext4_has_feature_large_file(inode->i_sb)))\n\t\treturn 1;\n\n\tif (pos + len <= 0x7fffffffULL)\n\t\treturn 1;\n\n\t/* We might need to update the superblock to set LARGE_FILE */\n\treturn 2;\n}\n\nstatic int ext4_da_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t       loff_t pos, unsigned len, unsigned flags,\n\t\t\t       struct page **pagep, void **fsdata)\n{\n\tint ret, retries = 0;\n\tstruct page *page;\n\tpgoff_t index;\n\tstruct inode *inode = mapping->host;\n\thandle_t *handle;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tindex = pos >> PAGE_SHIFT;\n\n\tif (ext4_nonda_switch(inode->i_sb) || S_ISLNK(inode->i_mode) ||\n\t    ext4_verity_in_progress(inode)) {\n\t\t*fsdata = (void *)FALL_BACK_TO_NONDELALLOC;\n\t\treturn ext4_write_begin(file, mapping, pos,\n\t\t\t\t\tlen, flags, pagep, fsdata);\n\t}\n\t*fsdata = (void *)0;\n\ttrace_ext4_da_write_begin(inode, pos, len, flags);\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) {\n\t\tret = ext4_da_write_inline_data_begin(mapping, inode,\n\t\t\t\t\t\t      pos, len, flags,\n\t\t\t\t\t\t      pagep, fsdata);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (ret == 1)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * grab_cache_page_write_begin() can take a long time if the\n\t * system is thrashing due to memory pressure, or if the page\n\t * is being written back.  So grab it first before we start\n\t * the transaction handle.  This also allows us to allocate\n\t * the page (if needed) without using GFP_NOFS.\n\t */\nretry_grab:\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page)\n\t\treturn -ENOMEM;\n\tunlock_page(page);\n\n\t/*\n\t * With delayed allocation, we don't log the i_disksize update\n\t * if there is delayed block allocation. But we still need\n\t * to journalling the i_disksize update if writes to the end\n\t * of file which has an already mapped buffer.\n\t */\nretry_journal:\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,\n\t\t\t\text4_da_write_credits(inode, pos, len));\n\tif (IS_ERR(handle)) {\n\t\tput_page(page);\n\t\treturn PTR_ERR(handle);\n\t}\n\n\tlock_page(page);\n\tif (page->mapping != mapping) {\n\t\t/* The page got truncated from under us */\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\text4_journal_stop(handle);\n\t\tgoto retry_grab;\n\t}\n\t/* In case writeback began while the page was unlocked */\n\twait_for_stable_page(page);\n\n#ifdef CONFIG_FS_ENCRYPTION\n\tret = ext4_block_write_begin(page, pos, len,\n\t\t\t\t     ext4_da_get_block_prep);\n#else\n\tret = __block_write_begin(page, pos, len, ext4_da_get_block_prep);\n#endif\n\tif (ret < 0) {\n\t\tunlock_page(page);\n\t\text4_journal_stop(handle);\n\t\t/*\n\t\t * block_write_begin may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again. Don't need\n\t\t * i_size_read because we hold i_mutex.\n\t\t */\n\t\tif (pos + len > inode->i_size)\n\t\t\text4_truncate_failed_write(inode);\n\n\t\tif (ret == -ENOSPC &&\n\t\t    ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\t\tgoto retry_journal;\n\n\t\tput_page(page);\n\t\treturn ret;\n\t}\n\n\t*pagep = page;\n\treturn ret;\n}\n\n/*\n * Check if we should update i_disksize\n * when write to the end of file but not require block allocation\n */\nstatic int ext4_da_should_update_i_disksize(struct page *page,\n\t\t\t\t\t    unsigned long offset)\n{\n\tstruct buffer_head *bh;\n\tstruct inode *inode = page->mapping->host;\n\tunsigned int idx;\n\tint i;\n\n\tbh = page_buffers(page);\n\tidx = offset >> inode->i_blkbits;\n\n\tfor (i = 0; i < idx; i++)\n\t\tbh = bh->b_this_page;\n\n\tif (!buffer_mapped(bh) || (buffer_delay(bh)) || buffer_unwritten(bh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_da_write_end(struct file *file,\n\t\t\t     struct address_space *mapping,\n\t\t\t     loff_t pos, unsigned len, unsigned copied,\n\t\t\t     struct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\thandle_t *handle = ext4_journal_current_handle();\n\tloff_t new_i_size;\n\tunsigned long start, end;\n\tint write_mode = (int)(unsigned long)fsdata;\n\n\tif (write_mode == FALL_BACK_TO_NONDELALLOC)\n\t\treturn ext4_write_end(file, mapping, pos,\n\t\t\t\t      len, copied, page, fsdata);\n\n\ttrace_ext4_da_write_end(inode, pos, len, copied);\n\tstart = pos & (PAGE_SIZE - 1);\n\tend = start + copied - 1;\n\n\t/*\n\t * generic_write_end() will run mark_inode_dirty() if i_size\n\t * changes.  So let's piggyback the i_disksize mark_inode_dirty\n\t * into that.\n\t */\n\tnew_i_size = pos + copied;\n\tif (copied && new_i_size > EXT4_I(inode)->i_disksize) {\n\t\tif (ext4_has_inline_data(inode) ||\n\t\t    ext4_da_should_update_i_disksize(page, end)) {\n\t\t\text4_update_i_disksize(inode, new_i_size);\n\t\t\t/* We need to mark inode dirty even if\n\t\t\t * new_i_size is less that inode->i_size\n\t\t\t * bu greater than i_disksize.(hint delalloc)\n\t\t\t */\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t}\n\t}\n\n\tif (write_mode != CONVERT_INLINE_DATA &&\n\t    ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA) &&\n\t    ext4_has_inline_data(inode))\n\t\tret2 = ext4_da_write_inline_data_end(inode, pos, len, copied,\n\t\t\t\t\t\t     page);\n\telse\n\t\tret2 = generic_write_end(file, mapping, pos, len, copied,\n\t\t\t\t\t\t\tpage, fsdata);\n\n\tcopied = ret2;\n\tif (ret2 < 0)\n\t\tret = ret2;\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\treturn ret ? ret : copied;\n}\n\n/*\n * Force all delayed allocation blocks to be allocated for a given inode.\n */\nint ext4_alloc_da_blocks(struct inode *inode)\n{\n\ttrace_ext4_alloc_da_blocks(inode);\n\n\tif (!EXT4_I(inode)->i_reserved_data_blocks)\n\t\treturn 0;\n\n\t/*\n\t * We do something simple for now.  The filemap_flush() will\n\t * also start triggering a write of the data blocks, which is\n\t * not strictly speaking necessary (and for users of\n\t * laptop_mode, not even desirable).  However, to do otherwise\n\t * would require replicating code paths in:\n\t *\n\t * ext4_writepages() ->\n\t *    write_cache_pages() ---> (via passed in callback function)\n\t *        __mpage_da_writepage() -->\n\t *           mpage_add_bh_to_extent()\n\t *           mpage_da_map_blocks()\n\t *\n\t * The problem is that write_cache_pages(), located in\n\t * mm/page-writeback.c, marks pages clean in preparation for\n\t * doing I/O, which is not desirable if we're not planning on\n\t * doing I/O at all.\n\t *\n\t * We could call write_cache_pages(), and then redirty all of\n\t * the pages by calling redirty_page_for_writepage() but that\n\t * would be ugly in the extreme.  So instead we would need to\n\t * replicate parts of the code in the above functions,\n\t * simplifying them because we wouldn't actually intend to\n\t * write out the pages, but rather only collect contiguous\n\t * logical block extents, call the multi-block allocator, and\n\t * then update the buffer heads with the block allocations.\n\t *\n\t * For now, though, we'll cheat by calling filemap_flush(),\n\t * which will map the blocks, and start the I/O, but not\n\t * actually wait for the I/O to complete.\n\t */\n\treturn filemap_flush(inode->i_mapping);\n}\n\n/*\n * bmap() is special.  It gets used by applications such as lilo and by\n * the swapper to find the on-disk block of a specific piece of data.\n *\n * Naturally, this is dangerous if the block concerned is still in the\n * journal.  If somebody makes a swapfile on an ext4 data-journaling\n * filesystem and enables swap, then they may get a nasty shock when the\n * data getting swapped to that swapfile suddenly gets overwritten by\n * the original zero's written out previously to the journal and\n * awaiting writeback in the kernel's buffer cache.\n *\n * So, if we see any bmap calls here on a modified, data-journaled file,\n * take extra steps to flush any blocks which might be in the cache.\n */\nstatic sector_t ext4_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\tjournal_t *journal;\n\tint err;\n\n\t/*\n\t * We can get here for an inline file via the FIBMAP ioctl\n\t */\n\tif (ext4_has_inline_data(inode))\n\t\treturn 0;\n\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) &&\n\t\t\ttest_opt(inode->i_sb, DELALLOC)) {\n\t\t/*\n\t\t * With delalloc we want to sync the file\n\t\t * so that we can make sure we allocate\n\t\t * blocks for file\n\t\t */\n\t\tfilemap_write_and_wait(mapping);\n\t}\n\n\tif (EXT4_JOURNAL(inode) &&\n\t    ext4_test_inode_state(inode, EXT4_STATE_JDATA)) {\n\t\t/*\n\t\t * This is a REALLY heavyweight approach, but the use of\n\t\t * bmap on dirty files is expected to be extremely rare:\n\t\t * only if we run lilo or swapon on a freshly made file\n\t\t * do we expect this to happen.\n\t\t *\n\t\t * (bmap requires CAP_SYS_RAWIO so this does not\n\t\t * represent an unprivileged user DOS attack --- we'd be\n\t\t * in trouble if mortal users could trigger this path at\n\t\t * will.)\n\t\t *\n\t\t * NB. EXT4_STATE_JDATA is not set on files other than\n\t\t * regular files.  If somebody wants to bmap a directory\n\t\t * or symlink and gets confused because the buffer\n\t\t * hasn't yet been flushed to disk, they deserve\n\t\t * everything they get.\n\t\t */\n\n\t\text4_clear_inode_state(inode, EXT4_STATE_JDATA);\n\t\tjournal = EXT4_JOURNAL(inode);\n\t\tjbd2_journal_lock_updates(journal);\n\t\terr = jbd2_journal_flush(journal);\n\t\tjbd2_journal_unlock_updates(journal);\n\n\t\tif (err)\n\t\t\treturn 0;\n\t}\n\n\treturn generic_block_bmap(mapping, block, ext4_get_block);\n}\n\nstatic int ext4_readpage(struct file *file, struct page *page)\n{\n\tint ret = -EAGAIN;\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_ext4_readpage(page);\n\n\tif (ext4_has_inline_data(inode))\n\t\tret = ext4_readpage_inline(inode, page);\n\n\tif (ret == -EAGAIN)\n\t\treturn ext4_mpage_readpages(page->mapping, NULL, page, 1,\n\t\t\t\t\t\tfalse);\n\n\treturn ret;\n}\n\nstatic int\next4_readpages(struct file *file, struct address_space *mapping,\n\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\tstruct inode *inode = mapping->host;\n\n\t/* If the file has inline data, no need to do readpages. */\n\tif (ext4_has_inline_data(inode))\n\t\treturn 0;\n\n\treturn ext4_mpage_readpages(mapping, pages, NULL, nr_pages, true);\n}\n\nstatic void ext4_invalidatepage(struct page *page, unsigned int offset,\n\t\t\t\tunsigned int length)\n{\n\ttrace_ext4_invalidatepage(page, offset, length);\n\n\t/* No journalling happens on data buffers when this function is used */\n\tWARN_ON(page_has_buffers(page) && buffer_jbd(page_buffers(page)));\n\n\tblock_invalidatepage(page, offset, length);\n}\n\nstatic int __ext4_journalled_invalidatepage(struct page *page,\n\t\t\t\t\t    unsigned int offset,\n\t\t\t\t\t    unsigned int length)\n{\n\tjournal_t *journal = EXT4_JOURNAL(page->mapping->host);\n\n\ttrace_ext4_journalled_invalidatepage(page, offset, length);\n\n\t/*\n\t * If it's a full truncate we just forget about the pending dirtying\n\t */\n\tif (offset == 0 && length == PAGE_SIZE)\n\t\tClearPageChecked(page);\n\n\treturn jbd2_journal_invalidatepage(journal, page, offset, length);\n}\n\n/* Wrapper for aops... */\nstatic void ext4_journalled_invalidatepage(struct page *page,\n\t\t\t\t\t   unsigned int offset,\n\t\t\t\t\t   unsigned int length)\n{\n\tWARN_ON(__ext4_journalled_invalidatepage(page, offset, length) < 0);\n}\n\nstatic int ext4_releasepage(struct page *page, gfp_t wait)\n{\n\tjournal_t *journal = EXT4_JOURNAL(page->mapping->host);\n\n\ttrace_ext4_releasepage(page);\n\n\t/* Page has dirty journalled data -> cannot release */\n\tif (PageChecked(page))\n\t\treturn 0;\n\tif (journal)\n\t\treturn jbd2_journal_try_to_free_buffers(journal, page, wait);\n\telse\n\t\treturn try_to_free_buffers(page);\n}\n\nstatic bool ext4_inode_datasync_dirty(struct inode *inode)\n{\n\tjournal_t *journal = EXT4_SB(inode->i_sb)->s_journal;\n\n\tif (journal)\n\t\treturn !jbd2_transaction_committed(journal,\n\t\t\t\t\tEXT4_I(inode)->i_datasync_tid);\n\t/* Any metadata buffers to write? */\n\tif (!list_empty(&inode->i_mapping->private_list))\n\t\treturn true;\n\treturn inode->i_state & I_DIRTY_DATASYNC;\n}\n\nstatic void ext4_set_iomap(struct inode *inode, struct iomap *iomap,\n\t\t\t   struct ext4_map_blocks *map, loff_t offset,\n\t\t\t   loff_t length)\n{\n\tu8 blkbits = inode->i_blkbits;\n\n\t/*\n\t * Writes that span EOF might trigger an I/O size update on completion,\n\t * so consider them to be dirty for the purpose of O_DSYNC, even if\n\t * there is no other metadata changes being made or are pending.\n\t */\n\tiomap->flags = 0;\n\tif (ext4_inode_datasync_dirty(inode) ||\n\t    offset + length > i_size_read(inode))\n\t\tiomap->flags |= IOMAP_F_DIRTY;\n\n\tif (map->m_flags & EXT4_MAP_NEW)\n\t\tiomap->flags |= IOMAP_F_NEW;\n\n\tiomap->bdev = inode->i_sb->s_bdev;\n\tiomap->dax_dev = EXT4_SB(inode->i_sb)->s_daxdev;\n\tiomap->offset = (u64) map->m_lblk << blkbits;\n\tiomap->length = (u64) map->m_len << blkbits;\n\n\t/*\n\t * Flags passed to ext4_map_blocks() for direct I/O writes can result\n\t * in m_flags having both EXT4_MAP_MAPPED and EXT4_MAP_UNWRITTEN bits\n\t * set. In order for any allocated unwritten extents to be converted\n\t * into written extents correctly within the ->end_io() handler, we\n\t * need to ensure that the iomap->type is set appropriately. Hence, the\n\t * reason why we need to check whether the EXT4_MAP_UNWRITTEN bit has\n\t * been set first.\n\t */\n\tif (map->m_flags & EXT4_MAP_UNWRITTEN) {\n\t\tiomap->type = IOMAP_UNWRITTEN;\n\t\tiomap->addr = (u64) map->m_pblk << blkbits;\n\t} else if (map->m_flags & EXT4_MAP_MAPPED) {\n\t\tiomap->type = IOMAP_MAPPED;\n\t\tiomap->addr = (u64) map->m_pblk << blkbits;\n\t} else {\n\t\tiomap->type = IOMAP_HOLE;\n\t\tiomap->addr = IOMAP_NULL_ADDR;\n\t}\n}\n\nstatic int ext4_iomap_alloc(struct inode *inode, struct ext4_map_blocks *map,\n\t\t\t    unsigned int flags)\n{\n\thandle_t *handle;\n\tu8 blkbits = inode->i_blkbits;\n\tint ret, dio_credits, m_flags = 0, retries = 0;\n\n\t/*\n\t * Trim the mapping request to the maximum value that we can map at\n\t * once for direct I/O.\n\t */\n\tif (map->m_len > DIO_MAX_BLOCKS)\n\t\tmap->m_len = DIO_MAX_BLOCKS;\n\tdio_credits = ext4_chunk_trans_blocks(inode, map->m_len);\n\nretry:\n\t/*\n\t * Either we allocate blocks and then don't get an unwritten extent, so\n\t * in that case we have reserved enough credits. Or, the blocks are\n\t * already allocated and unwritten. In that case, the extent conversion\n\t * fits into the credits as well.\n\t */\n\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS, dio_credits);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\t/*\n\t * DAX and direct I/O are the only two operations that are currently\n\t * supported with IOMAP_WRITE.\n\t */\n\tWARN_ON(!IS_DAX(inode) && !(flags & IOMAP_DIRECT));\n\tif (IS_DAX(inode))\n\t\tm_flags = EXT4_GET_BLOCKS_CREATE_ZERO;\n\t/*\n\t * We use i_size instead of i_disksize here because delalloc writeback\n\t * can complete at any point during the I/O and subsequently push the\n\t * i_disksize out to i_size. This could be beyond where direct I/O is\n\t * happening and thus expose allocated blocks to direct I/O reads.\n\t */\n\telse if ((map->m_lblk * (1 << blkbits)) >= i_size_read(inode))\n\t\tm_flags = EXT4_GET_BLOCKS_CREATE;\n\telse if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tm_flags = EXT4_GET_BLOCKS_IO_CREATE_EXT;\n\n\tret = ext4_map_blocks(handle, inode, map, m_flags);\n\n\t/*\n\t * We cannot fill holes in indirect tree based inodes as that could\n\t * expose stale data in the case of a crash. Use the magic error code\n\t * to fallback to buffered I/O.\n\t */\n\tif (!m_flags && !ret)\n\t\tret = -ENOTBLK;\n\n\text4_journal_stop(handle);\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\n\n\treturn ret;\n}\n\n\nstatic int ext4_iomap_begin(struct inode *inode, loff_t offset, loff_t length,\n\t\tunsigned flags, struct iomap *iomap, struct iomap *srcmap)\n{\n\tint ret;\n\tstruct ext4_map_blocks map;\n\tu8 blkbits = inode->i_blkbits;\n\n\tif ((offset >> blkbits) > EXT4_MAX_LOGICAL_BLOCK)\n\t\treturn -EINVAL;\n\n\tif (WARN_ON_ONCE(ext4_has_inline_data(inode)))\n\t\treturn -ERANGE;\n\n\t/*\n\t * Calculate the first and last logical blocks respectively.\n\t */\n\tmap.m_lblk = offset >> blkbits;\n\tmap.m_len = min_t(loff_t, (offset + length - 1) >> blkbits,\n\t\t\t  EXT4_MAX_LOGICAL_BLOCK) - map.m_lblk + 1;\n\n\tif (flags & IOMAP_WRITE)\n\t\tret = ext4_iomap_alloc(inode, &map, flags);\n\telse\n\t\tret = ext4_map_blocks(NULL, inode, &map, 0);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\text4_set_iomap(inode, iomap, &map, offset, length);\n\n\treturn 0;\n}\n\nstatic int ext4_iomap_end(struct inode *inode, loff_t offset, loff_t length,\n\t\t\t  ssize_t written, unsigned flags, struct iomap *iomap)\n{\n\t/*\n\t * Check to see whether an error occurred while writing out the data to\n\t * the allocated blocks. If so, return the magic error code so that we\n\t * fallback to buffered I/O and attempt to complete the remainder of\n\t * the I/O. Any blocks that may have been allocated in preparation for\n\t * the direct I/O will be reused during buffered I/O.\n\t */\n\tif (flags & (IOMAP_WRITE | IOMAP_DIRECT) && written == 0)\n\t\treturn -ENOTBLK;\n\n\treturn 0;\n}\n\nconst struct iomap_ops ext4_iomap_ops = {\n\t.iomap_begin\t\t= ext4_iomap_begin,\n\t.iomap_end\t\t= ext4_iomap_end,\n};\n\nstatic bool ext4_iomap_is_delalloc(struct inode *inode,\n\t\t\t\t   struct ext4_map_blocks *map)\n{\n\tstruct extent_status es;\n\text4_lblk_t offset = 0, end = map->m_lblk + map->m_len - 1;\n\n\text4_es_find_extent_range(inode, &ext4_es_is_delayed,\n\t\t\t\t  map->m_lblk, end, &es);\n\n\tif (!es.es_len || es.es_lblk > end)\n\t\treturn false;\n\n\tif (es.es_lblk > map->m_lblk) {\n\t\tmap->m_len = es.es_lblk - map->m_lblk;\n\t\treturn false;\n\t}\n\n\toffset = map->m_lblk - es.es_lblk;\n\tmap->m_len = es.es_len - offset;\n\n\treturn true;\n}\n\nstatic int ext4_iomap_begin_report(struct inode *inode, loff_t offset,\n\t\t\t\t   loff_t length, unsigned int flags,\n\t\t\t\t   struct iomap *iomap, struct iomap *srcmap)\n{\n\tint ret;\n\tbool delalloc = false;\n\tstruct ext4_map_blocks map;\n\tu8 blkbits = inode->i_blkbits;\n\n\tif ((offset >> blkbits) > EXT4_MAX_LOGICAL_BLOCK)\n\t\treturn -EINVAL;\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tret = ext4_inline_data_iomap(inode, iomap);\n\t\tif (ret != -EAGAIN) {\n\t\t\tif (ret == 0 && offset >= iomap->length)\n\t\t\t\tret = -ENOENT;\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the first and last logical block respectively.\n\t */\n\tmap.m_lblk = offset >> blkbits;\n\tmap.m_len = min_t(loff_t, (offset + length - 1) >> blkbits,\n\t\t\t  EXT4_MAX_LOGICAL_BLOCK) - map.m_lblk + 1;\n\n\tret = ext4_map_blocks(NULL, inode, &map, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret == 0)\n\t\tdelalloc = ext4_iomap_is_delalloc(inode, &map);\n\n\text4_set_iomap(inode, iomap, &map, offset, length);\n\tif (delalloc && iomap->type == IOMAP_HOLE)\n\t\tiomap->type = IOMAP_DELALLOC;\n\n\treturn 0;\n}\n\nconst struct iomap_ops ext4_iomap_report_ops = {\n\t.iomap_begin = ext4_iomap_begin_report,\n};\n\n/*\n * Pages can be marked dirty completely asynchronously from ext4's journalling\n * activity.  By filemap_sync_pte(), try_to_unmap_one(), etc.  We cannot do\n * much here because ->set_page_dirty is called under VFS locks.  The page is\n * not necessarily locked.\n *\n * We cannot just dirty the page and leave attached buffers clean, because the\n * buffers' dirty state is \"definitive\".  We cannot just set the buffers dirty\n * or jbddirty because all the journalling code will explode.\n *\n * So what we do is to mark the page \"pending dirty\" and next time writepage\n * is called, propagate that into the buffers appropriately.\n */\nstatic int ext4_journalled_set_page_dirty(struct page *page)\n{\n\tSetPageChecked(page);\n\treturn __set_page_dirty_nobuffers(page);\n}\n\nstatic int ext4_set_page_dirty(struct page *page)\n{\n\tWARN_ON_ONCE(!PageLocked(page) && !PageDirty(page));\n\tWARN_ON_ONCE(!page_has_buffers(page));\n\treturn __set_page_dirty_buffers(page);\n}\n\nstatic const struct address_space_operations ext4_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.writepages\t\t= ext4_writepages,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_write_end,\n\t.set_page_dirty\t\t= ext4_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= noop_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_journalled_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.writepages\t\t= ext4_writepages,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_journalled_write_end,\n\t.set_page_dirty\t\t= ext4_journalled_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_journalled_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= noop_direct_IO,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_da_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.writepages\t\t= ext4_writepages,\n\t.write_begin\t\t= ext4_da_write_begin,\n\t.write_end\t\t= ext4_da_write_end,\n\t.set_page_dirty\t\t= ext4_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= noop_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_dax_aops = {\n\t.writepages\t\t= ext4_dax_writepages,\n\t.direct_IO\t\t= noop_direct_IO,\n\t.set_page_dirty\t\t= noop_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= noop_invalidatepage,\n};\n\nvoid ext4_set_aops(struct inode *inode)\n{\n\tswitch (ext4_inode_journal_mode(inode)) {\n\tcase EXT4_INODE_ORDERED_DATA_MODE:\n\tcase EXT4_INODE_WRITEBACK_DATA_MODE:\n\t\tbreak;\n\tcase EXT4_INODE_JOURNAL_DATA_MODE:\n\t\tinode->i_mapping->a_ops = &ext4_journalled_aops;\n\t\treturn;\n\tdefault:\n\t\tBUG();\n\t}\n\tif (IS_DAX(inode))\n\t\tinode->i_mapping->a_ops = &ext4_dax_aops;\n\telse if (test_opt(inode->i_sb, DELALLOC))\n\t\tinode->i_mapping->a_ops = &ext4_da_aops;\n\telse\n\t\tinode->i_mapping->a_ops = &ext4_aops;\n}\n\nstatic int __ext4_block_zero_page_range(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from, loff_t length)\n{\n\text4_fsblk_t index = from >> PAGE_SHIFT;\n\tunsigned offset = from & (PAGE_SIZE-1);\n\tunsigned blocksize, pos;\n\text4_lblk_t iblock;\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head *bh;\n\tstruct page *page;\n\tint err = 0;\n\n\tpage = find_or_create_page(mapping, from >> PAGE_SHIFT,\n\t\t\t\t   mapping_gfp_constraint(mapping, ~__GFP_FS));\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tblocksize = inode->i_sb->s_blocksize;\n\n\tiblock = index << (PAGE_SHIFT - inode->i_sb->s_blocksize_bits);\n\n\tif (!page_has_buffers(page))\n\t\tcreate_empty_buffers(page, blocksize, 0);\n\n\t/* Find the buffer that contains \"offset\" */\n\tbh = page_buffers(page);\n\tpos = blocksize;\n\twhile (offset >= pos) {\n\t\tbh = bh->b_this_page;\n\t\tiblock++;\n\t\tpos += blocksize;\n\t}\n\tif (buffer_freed(bh)) {\n\t\tBUFFER_TRACE(bh, \"freed: skip\");\n\t\tgoto unlock;\n\t}\n\tif (!buffer_mapped(bh)) {\n\t\tBUFFER_TRACE(bh, \"unmapped\");\n\t\text4_get_block(inode, iblock, bh, 0);\n\t\t/* unmapped? It's a hole - nothing to do */\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tBUFFER_TRACE(bh, \"still unmapped\");\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t/* Ok, it's mapped. Make sure it's up-to-date */\n\tif (PageUptodate(page))\n\t\tset_buffer_uptodate(bh);\n\n\tif (!buffer_uptodate(bh)) {\n\t\terr = -EIO;\n\t\tll_rw_block(REQ_OP_READ, 0, 1, &bh);\n\t\twait_on_buffer(bh);\n\t\t/* Uhhuh. Read error. Complain and punt. */\n\t\tif (!buffer_uptodate(bh))\n\t\t\tgoto unlock;\n\t\tif (S_ISREG(inode->i_mode) && IS_ENCRYPTED(inode)) {\n\t\t\t/* We expect the key to be set. */\n\t\t\tBUG_ON(!fscrypt_has_encryption_key(inode));\n\t\t\tWARN_ON_ONCE(fscrypt_decrypt_pagecache_blocks(\n\t\t\t\t\tpage, blocksize, bh_offset(bh)));\n\t\t}\n\t}\n\tif (ext4_should_journal_data(inode)) {\n\t\tBUFFER_TRACE(bh, \"get write access\");\n\t\terr = ext4_journal_get_write_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\tzero_user(page, offset, length);\n\tBUFFER_TRACE(bh, \"zeroed end of block\");\n\n\tif (ext4_should_journal_data(inode)) {\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t} else {\n\t\terr = 0;\n\t\tmark_buffer_dirty(bh);\n\t\tif (ext4_should_order_data(inode))\n\t\t\terr = ext4_jbd2_inode_add_write(handle, inode, from,\n\t\t\t\t\tlength);\n\t}\n\nunlock:\n\tunlock_page(page);\n\tput_page(page);\n\treturn err;\n}\n\n/*\n * ext4_block_zero_page_range() zeros out a mapping of length 'length'\n * starting from file offset 'from'.  The range to be zero'd must\n * be contained with in one block.  If the specified range exceeds\n * the end of the block it will be shortened to end of the block\n * that cooresponds to 'from'\n */\nstatic int ext4_block_zero_page_range(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from, loff_t length)\n{\n\tstruct inode *inode = mapping->host;\n\tunsigned offset = from & (PAGE_SIZE-1);\n\tunsigned blocksize = inode->i_sb->s_blocksize;\n\tunsigned max = blocksize - (offset & (blocksize - 1));\n\n\t/*\n\t * correct length if it does not fall between\n\t * 'from' and the end of the block\n\t */\n\tif (length > max || length < 0)\n\t\tlength = max;\n\n\tif (IS_DAX(inode)) {\n\t\treturn iomap_zero_range(inode, from, length, NULL,\n\t\t\t\t\t&ext4_iomap_ops);\n\t}\n\treturn __ext4_block_zero_page_range(handle, mapping, from, length);\n}\n\n/*\n * ext4_block_truncate_page() zeroes out a mapping from file offset `from'\n * up to the end of the block which corresponds to `from'.\n * This required during truncate. We need to physically zero the tail end\n * of that block so it doesn't yield old data if the file is later grown.\n */\nstatic int ext4_block_truncate_page(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from)\n{\n\tunsigned offset = from & (PAGE_SIZE-1);\n\tunsigned length;\n\tunsigned blocksize;\n\tstruct inode *inode = mapping->host;\n\n\t/* If we are processing an encrypted inode during orphan list handling */\n\tif (IS_ENCRYPTED(inode) && !fscrypt_has_encryption_key(inode))\n\t\treturn 0;\n\n\tblocksize = inode->i_sb->s_blocksize;\n\tlength = blocksize - (offset & (blocksize - 1));\n\n\treturn ext4_block_zero_page_range(handle, mapping, from, length);\n}\n\nint ext4_zero_partial_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     loff_t lstart, loff_t length)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tstruct address_space *mapping = inode->i_mapping;\n\tunsigned partial_start, partial_end;\n\text4_fsblk_t start, end;\n\tloff_t byte_end = (lstart + length - 1);\n\tint err = 0;\n\n\tpartial_start = lstart & (sb->s_blocksize - 1);\n\tpartial_end = byte_end & (sb->s_blocksize - 1);\n\n\tstart = lstart >> sb->s_blocksize_bits;\n\tend = byte_end >> sb->s_blocksize_bits;\n\n\t/* Handle partial zero within the single block */\n\tif (start == end &&\n\t    (partial_start || (partial_end != sb->s_blocksize - 1))) {\n\t\terr = ext4_block_zero_page_range(handle, mapping,\n\t\t\t\t\t\t lstart, length);\n\t\treturn err;\n\t}\n\t/* Handle partial zero out on the start of the range */\n\tif (partial_start) {\n\t\terr = ext4_block_zero_page_range(handle, mapping,\n\t\t\t\t\t\t lstart, sb->s_blocksize);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\t/* Handle partial zero out on the end of the range */\n\tif (partial_end != sb->s_blocksize - 1)\n\t\terr = ext4_block_zero_page_range(handle, mapping,\n\t\t\t\t\t\t byte_end - partial_end,\n\t\t\t\t\t\t partial_end + 1);\n\treturn err;\n}\n\nint ext4_can_truncate(struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode))\n\t\treturn 1;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn 1;\n\tif (S_ISLNK(inode->i_mode))\n\t\treturn !ext4_inode_is_fast_symlink(inode);\n\treturn 0;\n}\n\n/*\n * We have to make sure i_disksize gets properly updated before we truncate\n * page cache due to hole punching or zero range. Otherwise i_disksize update\n * can get lost as it may have been postponed to submission of writeback but\n * that will never happen after we truncate page cache.\n */\nint ext4_update_disksize_before_punch(struct inode *inode, loff_t offset,\n\t\t\t\t      loff_t len)\n{\n\thandle_t *handle;\n\tloff_t size = i_size_read(inode);\n\n\tWARN_ON(!inode_is_locked(inode));\n\tif (offset > size || offset + len < size)\n\t\treturn 0;\n\n\tif (EXT4_I(inode)->i_disksize >= size)\n\t\treturn 0;\n\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\text4_update_i_disksize(inode, size);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n\n\treturn 0;\n}\n\nstatic void ext4_wait_dax_page(struct ext4_inode_info *ei)\n{\n\tup_write(&ei->i_mmap_sem);\n\tschedule();\n\tdown_write(&ei->i_mmap_sem);\n}\n\nint ext4_break_layouts(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct page *page;\n\tint error;\n\n\tif (WARN_ON_ONCE(!rwsem_is_locked(&ei->i_mmap_sem)))\n\t\treturn -EINVAL;\n\n\tdo {\n\t\tpage = dax_layout_busy_page(inode->i_mapping);\n\t\tif (!page)\n\t\t\treturn 0;\n\n\t\terror = ___wait_var_event(&page->_refcount,\n\t\t\t\tatomic_read(&page->_refcount) == 1,\n\t\t\t\tTASK_INTERRUPTIBLE, 0, 0,\n\t\t\t\text4_wait_dax_page(ei));\n\t} while (error == 0);\n\n\treturn error;\n}\n\n/*\n * ext4_punch_hole: punches a hole in a file by releasing the blocks\n * associated with the given offset and length\n *\n * @inode:  File inode\n * @offset: The offset where the hole will begin\n * @len:    The length of the hole\n *\n * Returns: 0 on success or negative on failure\n */\n\nint ext4_punch_hole(struct inode *inode, loff_t offset, loff_t length)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t first_block, stop_block;\n\tstruct address_space *mapping = inode->i_mapping;\n\tloff_t first_block_offset, last_block_offset;\n\thandle_t *handle;\n\tunsigned int credits;\n\tint ret = 0;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_punch_hole(inode, offset, length, 0);\n\n\text4_clear_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\n\tif (ext4_has_inline_data(inode)) {\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\t\tret = ext4_convert_inline_data(inode);\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\tret = filemap_write_and_wait_range(mapping, offset,\n\t\t\t\t\t\t   offset + length - 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tinode_lock(inode);\n\n\t/* No need to punch hole beyond i_size */\n\tif (offset >= inode->i_size)\n\t\tgoto out_mutex;\n\n\t/*\n\t * If the hole extends beyond i_size, set the hole\n\t * to end after the page that contains i_size\n\t */\n\tif (offset + length > inode->i_size) {\n\t\tlength = inode->i_size +\n\t\t   PAGE_SIZE - (inode->i_size & (PAGE_SIZE - 1)) -\n\t\t   offset;\n\t}\n\n\tif (offset & (sb->s_blocksize - 1) ||\n\t    (offset + length) & (sb->s_blocksize - 1)) {\n\t\t/*\n\t\t * Attach jinode to inode for jbd2 if we do any zeroing of\n\t\t * partial block\n\t\t */\n\t\tret = ext4_inode_attach_jinode(inode);\n\t\tif (ret < 0)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\tret = ext4_break_layouts(inode);\n\tif (ret)\n\t\tgoto out_dio;\n\n\tfirst_block_offset = round_up(offset, sb->s_blocksize);\n\tlast_block_offset = round_down((offset + length), sb->s_blocksize) - 1;\n\n\t/* Now release the pages and zero block aligned part of pages*/\n\tif (last_block_offset > first_block_offset) {\n\t\tret = ext4_update_disksize_before_punch(inode, offset, length);\n\t\tif (ret)\n\t\t\tgoto out_dio;\n\t\ttruncate_pagecache_range(inode, first_block_offset,\n\t\t\t\t\t last_block_offset);\n\t}\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\telse\n\t\tcredits = ext4_blocks_for_truncate(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(sb, ret);\n\t\tgoto out_dio;\n\t}\n\n\tret = ext4_zero_partial_blocks(handle, inode, offset,\n\t\t\t\t       length);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tfirst_block = (offset + sb->s_blocksize - 1) >>\n\t\tEXT4_BLOCK_SIZE_BITS(sb);\n\tstop_block = (offset + length) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* If there are blocks to remove, do it */\n\tif (stop_block > first_block) {\n\n\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\text4_discard_preallocations(inode);\n\n\t\tret = ext4_es_remove_extent(inode, first_block,\n\t\t\t\t\t    stop_block - first_block);\n\t\tif (ret) {\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto out_stop;\n\t\t}\n\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\t\tret = ext4_ext_remove_space(inode, first_block,\n\t\t\t\t\t\t    stop_block - 1);\n\t\telse\n\t\t\tret = ext4_ind_remove_space(handle, inode, first_block,\n\t\t\t\t\t\t    stop_block);\n\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t}\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\nout_mutex:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\nint ext4_inode_attach_jinode(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct jbd2_inode *jinode;\n\n\tif (ei->jinode || !EXT4_SB(inode->i_sb)->s_journal)\n\t\treturn 0;\n\n\tjinode = jbd2_alloc_inode(GFP_KERNEL);\n\tspin_lock(&inode->i_lock);\n\tif (!ei->jinode) {\n\t\tif (!jinode) {\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tei->jinode = jinode;\n\t\tjbd2_journal_init_jbd_inode(ei->jinode, inode);\n\t\tjinode = NULL;\n\t}\n\tspin_unlock(&inode->i_lock);\n\tif (unlikely(jinode != NULL))\n\t\tjbd2_free_inode(jinode);\n\treturn 0;\n}\n\n/*\n * ext4_truncate()\n *\n * We block out ext4_get_block() block instantiations across the entire\n * transaction, and VFS/VM ensures that ext4_truncate() cannot run\n * simultaneously on behalf of the same inode.\n *\n * As we work through the truncate and commit bits of it to the journal there\n * is one core, guiding principle: the file's tree must always be consistent on\n * disk.  We must be able to restart the truncate after a crash.\n *\n * The file's tree may be transiently inconsistent in memory (although it\n * probably isn't), but whenever we close off and commit a journal transaction,\n * the contents of (the filesystem + the journal) must be consistent and\n * restartable.  It's pretty simple, really: bottom up, right to left (although\n * left-to-right works OK too).\n *\n * Note that at recovery time, journal replay occurs *before* the restart of\n * truncate against the orphan inode list.\n *\n * The committed inode has the new, desired i_size (which is the same as\n * i_disksize in this case).  After a crash, ext4_orphan_cleanup() will see\n * that this inode's truncate did not complete and it will again call\n * ext4_truncate() to have another go.  So there will be instantiated blocks\n * to the right of the truncation point in a crashed ext4 filesystem.  But\n * that's fine - as long as they are linked from the inode, the post-crash\n * ext4_truncate() run will find them and release them.\n */\nint ext4_truncate(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tunsigned int credits;\n\tint err = 0;\n\thandle_t *handle;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\t/*\n\t * There is a possibility that we're either freeing the inode\n\t * or it's a completely new inode. In those cases we might not\n\t * have i_mutex locked because it's not necessary.\n\t */\n\tif (!(inode->i_state & (I_NEW|I_FREEING)))\n\t\tWARN_ON(!inode_is_locked(inode));\n\ttrace_ext4_truncate_enter(inode);\n\n\tif (!ext4_can_truncate(inode))\n\t\treturn 0;\n\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\n\tif (inode->i_size == 0 && !test_opt(inode->i_sb, NO_AUTO_DA_ALLOC))\n\t\text4_set_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE);\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tint has_inline = 1;\n\n\t\terr = ext4_inline_data_truncate(inode, &has_inline);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (has_inline)\n\t\t\treturn 0;\n\t}\n\n\t/* If we zero-out tail of the page, we have to create jinode for jbd2 */\n\tif (inode->i_size & (inode->i_sb->s_blocksize - 1)) {\n\t\tif (ext4_inode_attach_jinode(inode) < 0)\n\t\t\treturn 0;\n\t}\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\telse\n\t\tcredits = ext4_blocks_for_truncate(inode);\n\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\tif (inode->i_size & (inode->i_sb->s_blocksize - 1))\n\t\text4_block_truncate_page(handle, mapping, inode->i_size);\n\n\t/*\n\t * We add the inode to the orphan list, so that if this\n\t * truncate spans multiple transactions, and we crash, we will\n\t * resume the truncate when the filesystem recovers.  It also\n\t * marks the inode dirty, to catch the new size.\n\t *\n\t * Implication: the file must always be in a sane, consistent\n\t * truncatable state while each transaction commits.\n\t */\n\terr = ext4_orphan_add(handle, inode);\n\tif (err)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\n\text4_discard_preallocations(inode);\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\terr = ext4_ext_truncate(handle, inode);\n\telse\n\t\text4_ind_truncate(handle, inode);\n\n\tup_write(&ei->i_data_sem);\n\tif (err)\n\t\tgoto out_stop;\n\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\t/*\n\t * If this was a simple ftruncate() and the file will remain alive,\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_evict_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n\n\ttrace_ext4_truncate_exit(inode);\n\treturn err;\n}\n\n/*\n * ext4_get_inode_loc returns with an extra refcount against the inode's\n * underlying buffer_head on success. If 'in_mem' is true, we have all\n * data in memory that is needed to recreate the on-disk version of this\n * inode.\n */\nstatic int __ext4_get_inode_loc(struct inode *inode,\n\t\t\t\tstruct ext4_iloc *iloc, int in_mem)\n{\n\tstruct ext4_group_desc\t*gdp;\n\tstruct buffer_head\t*bh;\n\tstruct super_block\t*sb = inode->i_sb;\n\text4_fsblk_t\t\tblock;\n\tstruct blk_plug\t\tplug;\n\tint\t\t\tinodes_per_block, inode_offset;\n\n\tiloc->bh = NULL;\n\tif (inode->i_ino < EXT4_ROOT_INO ||\n\t    inode->i_ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count))\n\t\treturn -EFSCORRUPTED;\n\n\tiloc->block_group = (inode->i_ino - 1) / EXT4_INODES_PER_GROUP(sb);\n\tgdp = ext4_get_group_desc(sb, iloc->block_group, NULL);\n\tif (!gdp)\n\t\treturn -EIO;\n\n\t/*\n\t * Figure out the offset within the block group inode table\n\t */\n\tinodes_per_block = EXT4_SB(sb)->s_inodes_per_block;\n\tinode_offset = ((inode->i_ino - 1) %\n\t\t\tEXT4_INODES_PER_GROUP(sb));\n\tblock = ext4_inode_table(sb, gdp) + (inode_offset / inodes_per_block);\n\tiloc->offset = (inode_offset % inodes_per_block) * EXT4_INODE_SIZE(sb);\n\n\tbh = sb_getblk(sb, block);\n\tif (unlikely(!bh))\n\t\treturn -ENOMEM;\n\tif (!buffer_uptodate(bh)) {\n\t\tlock_buffer(bh);\n\n\t\t/*\n\t\t * If the buffer has the write error flag, we have failed\n\t\t * to write out another inode in the same block.  In this\n\t\t * case, we don't have to read the block because we may\n\t\t * read the old inode data successfully.\n\t\t */\n\t\tif (buffer_write_io_error(bh) && !buffer_uptodate(bh))\n\t\t\tset_buffer_uptodate(bh);\n\n\t\tif (buffer_uptodate(bh)) {\n\t\t\t/* someone brought it uptodate while we waited */\n\t\t\tunlock_buffer(bh);\n\t\t\tgoto has_buffer;\n\t\t}\n\n\t\t/*\n\t\t * If we have all information of the inode in memory and this\n\t\t * is the only valid inode in the block, we need not read the\n\t\t * block.\n\t\t */\n\t\tif (in_mem) {\n\t\t\tstruct buffer_head *bitmap_bh;\n\t\t\tint i, start;\n\n\t\t\tstart = inode_offset & ~(inodes_per_block - 1);\n\n\t\t\t/* Is the inode bitmap in cache? */\n\t\t\tbitmap_bh = sb_getblk(sb, ext4_inode_bitmap(sb, gdp));\n\t\t\tif (unlikely(!bitmap_bh))\n\t\t\t\tgoto make_io;\n\n\t\t\t/*\n\t\t\t * If the inode bitmap isn't in cache then the\n\t\t\t * optimisation may end up performing two reads instead\n\t\t\t * of one, so skip it.\n\t\t\t */\n\t\t\tif (!buffer_uptodate(bitmap_bh)) {\n\t\t\t\tbrelse(bitmap_bh);\n\t\t\t\tgoto make_io;\n\t\t\t}\n\t\t\tfor (i = start; i < start + inodes_per_block; i++) {\n\t\t\t\tif (i == inode_offset)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ext4_test_bit(i, bitmap_bh->b_data))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbrelse(bitmap_bh);\n\t\t\tif (i == start + inodes_per_block) {\n\t\t\t\t/* all other inodes are free, so skip I/O */\n\t\t\t\tmemset(bh->b_data, 0, bh->b_size);\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\tunlock_buffer(bh);\n\t\t\t\tgoto has_buffer;\n\t\t\t}\n\t\t}\n\nmake_io:\n\t\t/*\n\t\t * If we need to do any I/O, try to pre-readahead extra\n\t\t * blocks from the inode table.\n\t\t */\n\t\tblk_start_plug(&plug);\n\t\tif (EXT4_SB(sb)->s_inode_readahead_blks) {\n\t\t\text4_fsblk_t b, end, table;\n\t\t\tunsigned num;\n\t\t\t__u32 ra_blks = EXT4_SB(sb)->s_inode_readahead_blks;\n\n\t\t\ttable = ext4_inode_table(sb, gdp);\n\t\t\t/* s_inode_readahead_blks is always a power of 2 */\n\t\t\tb = block & ~((ext4_fsblk_t) ra_blks - 1);\n\t\t\tif (table > b)\n\t\t\t\tb = table;\n\t\t\tend = b + ra_blks;\n\t\t\tnum = EXT4_INODES_PER_GROUP(sb);\n\t\t\tif (ext4_has_group_desc_csum(sb))\n\t\t\t\tnum -= ext4_itable_unused_count(sb, gdp);\n\t\t\ttable += num / inodes_per_block;\n\t\t\tif (end > table)\n\t\t\t\tend = table;\n\t\t\twhile (b <= end)\n\t\t\t\tsb_breadahead(sb, b++);\n\t\t}\n\n\t\t/*\n\t\t * There are other valid inodes in the buffer, this inode\n\t\t * has in-inode xattrs, or we don't have this inode in memory.\n\t\t * Read the block from disk.\n\t\t */\n\t\ttrace_ext4_load_inode(inode);\n\t\tget_bh(bh);\n\t\tbh->b_end_io = end_buffer_read_sync;\n\t\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\t\tblk_finish_plug(&plug);\n\t\twait_on_buffer(bh);\n\t\tif (!buffer_uptodate(bh)) {\n\t\t\tEXT4_ERROR_INODE_BLOCK(inode, block,\n\t\t\t\t\t       \"unable to read itable block\");\n\t\t\tbrelse(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t}\nhas_buffer:\n\tiloc->bh = bh;\n\treturn 0;\n}\n\nint ext4_get_inode_loc(struct inode *inode, struct ext4_iloc *iloc)\n{\n\t/* We have all inode data except xattrs in memory here. */\n\treturn __ext4_get_inode_loc(inode, iloc,\n\t\t!ext4_test_inode_state(inode, EXT4_STATE_XATTR));\n}\n\nstatic bool ext4_should_use_dax(struct inode *inode)\n{\n\tif (!test_opt(inode->i_sb, DAX))\n\t\treturn false;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn false;\n\tif (ext4_should_journal_data(inode))\n\t\treturn false;\n\tif (ext4_has_inline_data(inode))\n\t\treturn false;\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_ENCRYPT))\n\t\treturn false;\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_VERITY))\n\t\treturn false;\n\treturn true;\n}\n\nvoid ext4_set_inode_flags(struct inode *inode)\n{\n\tunsigned int flags = EXT4_I(inode)->i_flags;\n\tunsigned int new_fl = 0;\n\n\tif (flags & EXT4_SYNC_FL)\n\t\tnew_fl |= S_SYNC;\n\tif (flags & EXT4_APPEND_FL)\n\t\tnew_fl |= S_APPEND;\n\tif (flags & EXT4_IMMUTABLE_FL)\n\t\tnew_fl |= S_IMMUTABLE;\n\tif (flags & EXT4_NOATIME_FL)\n\t\tnew_fl |= S_NOATIME;\n\tif (flags & EXT4_DIRSYNC_FL)\n\t\tnew_fl |= S_DIRSYNC;\n\tif (ext4_should_use_dax(inode))\n\t\tnew_fl |= S_DAX;\n\tif (flags & EXT4_ENCRYPT_FL)\n\t\tnew_fl |= S_ENCRYPTED;\n\tif (flags & EXT4_CASEFOLD_FL)\n\t\tnew_fl |= S_CASEFOLD;\n\tif (flags & EXT4_VERITY_FL)\n\t\tnew_fl |= S_VERITY;\n\tinode_set_flags(inode, new_fl,\n\t\t\tS_SYNC|S_APPEND|S_IMMUTABLE|S_NOATIME|S_DIRSYNC|S_DAX|\n\t\t\tS_ENCRYPTED|S_CASEFOLD|S_VERITY);\n}\n\nstatic blkcnt_t ext4_inode_blocks(struct ext4_inode *raw_inode,\n\t\t\t\t  struct ext4_inode_info *ei)\n{\n\tblkcnt_t i_blocks ;\n\tstruct inode *inode = &(ei->vfs_inode);\n\tstruct super_block *sb = inode->i_sb;\n\n\tif (ext4_has_feature_huge_file(sb)) {\n\t\t/* we are using combined 48 bit field */\n\t\ti_blocks = ((u64)le16_to_cpu(raw_inode->i_blocks_high)) << 32 |\n\t\t\t\t\tle32_to_cpu(raw_inode->i_blocks_lo);\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_HUGE_FILE)) {\n\t\t\t/* i_blocks represent file system block size */\n\t\t\treturn i_blocks  << (inode->i_blkbits - 9);\n\t\t} else {\n\t\t\treturn i_blocks;\n\t\t}\n\t} else {\n\t\treturn le32_to_cpu(raw_inode->i_blocks_lo);\n\t}\n}\n\nstatic inline int ext4_iget_extra_inode(struct inode *inode,\n\t\t\t\t\t struct ext4_inode *raw_inode,\n\t\t\t\t\t struct ext4_inode_info *ei)\n{\n\t__le32 *magic = (void *)raw_inode +\n\t\t\tEXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize;\n\n\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize + sizeof(__le32) <=\n\t    EXT4_INODE_SIZE(inode->i_sb) &&\n\t    *magic == cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t\treturn ext4_find_inline_data_nolock(inode);\n\t} else\n\t\tEXT4_I(inode)->i_inline_off = 0;\n\treturn 0;\n}\n\nint ext4_get_projid(struct inode *inode, kprojid_t *projid)\n{\n\tif (!ext4_has_feature_project(inode->i_sb))\n\t\treturn -EOPNOTSUPP;\n\t*projid = EXT4_I(inode)->i_projid;\n\treturn 0;\n}\n\n/*\n * ext4 has self-managed i_version for ea inodes, it stores the lower 32bit of\n * refcount in i_version, so use raw values if inode has EXT4_EA_INODE_FL flag\n * set.\n */\nstatic inline void ext4_inode_set_iversion_queried(struct inode *inode, u64 val)\n{\n\tif (unlikely(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL))\n\t\tinode_set_iversion_raw(inode, val);\n\telse\n\t\tinode_set_iversion_queried(inode, val);\n}\nstatic inline u64 ext4_inode_peek_iversion(const struct inode *inode)\n{\n\tif (unlikely(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL))\n\t\treturn inode_peek_iversion_raw(inode);\n\telse\n\t\treturn inode_peek_iversion(inode);\n}\n\nstruct inode *__ext4_iget(struct super_block *sb, unsigned long ino,\n\t\t\t  ext4_iget_flags flags, const char *function,\n\t\t\t  unsigned int line)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tloff_t size;\n\tint block;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tif ((!(flags & EXT4_IGET_SPECIAL) &&\n\t     (ino < EXT4_FIRST_INO(sb) && ino != EXT4_ROOT_INO)) ||\n\t    (ino < EXT4_ROOT_INO) ||\n\t    (ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count))) {\n\t\tif (flags & EXT4_IGET_HANDLE)\n\t\t\treturn ERR_PTR(-ESTALE);\n\t\t__ext4_error(sb, function, line,\n\t\t\t     \"inode #%lu: comm %s: iget: illegal inode #\",\n\t\t\t     ino, current->comm);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = NULL;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\n\tif ((ino == EXT4_ROOT_INO) && (raw_inode->i_links_count == 0)) {\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: root inode unallocated\");\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\n\tif ((flags & EXT4_IGET_HANDLE) &&\n\t    (raw_inode->i_links_count == 0) && (raw_inode->i_mode == 0)) {\n\t\tret = -ESTALE;\n\t\tgoto bad_inode;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t\tEXT4_INODE_SIZE(inode->i_sb) ||\n\t\t    (ei->i_extra_isize & 3)) {\n\t\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t\t \"iget: bad extra_isize %u \"\n\t\t\t\t\t \"(inode size %u)\",\n\t\t\t\t\t ei->i_extra_isize,\n\t\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = raw_inode->i_generation;\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\tif (!ext4_inode_csum_verify(inode, raw_inode, ei)) {\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: checksum invalid\");\n\t\tret = -EFSBADCRC;\n\t\tgoto bad_inode;\n\t}\n\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\ti_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\ti_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (ext4_has_feature_project(sb) &&\n\t    EXT4_INODE_SIZE(sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(raw_inode->i_projid);\n\telse\n\t\ti_projid = EXT4_DEF_PROJID;\n\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\ti_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\ti_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\ti_uid_write(inode, i_uid);\n\ti_gid_write(inode, i_gid);\n\tei->i_projid = make_kprojid(&init_user_ns, i_projid);\n\tset_nlink(inode, le16_to_cpu(raw_inode->i_links_count));\n\n\text4_clear_state_flags(ei);\t/* Only relevant on 32-bit archs */\n\tei->i_inline_off = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif ((inode->i_mode == 0 ||\n\t\t     !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) &&\n\t\t    ino != EXT4_BOOT_LOADER_INO) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those.\n\t\t * OR it is the EXT4_BOOT_LOADER_INO which is\n\t\t * not initialized on a new filesystem. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\text4_set_inode_flags(inode);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (ext4_has_feature_64bit(sb))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(sb, raw_inode);\n\tif ((size = i_size_read(inode)) < 0) {\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: bad i_size value: %lld\", size);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tBUILD_BUG_ON(sizeof(struct ext4_inode) & 3);\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\tret = ext4_iget_extra_inode(inode, raw_inode, ei);\n\t\t\tif (ret)\n\t\t\t\tgoto bad_inode;\n\t\t}\n\t}\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = le32_to_cpu(raw_inode->i_disk_version);\n\n\t\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\tivers |=\n\t\t    (__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t\t}\n\t\text4_inode_set_iversion_queried(inode, ivers);\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: bad extended attribute block %llu\",\n\t\t\t\t ei->i_file_acl);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\t/* validate the block references in the inode */\n\t\tif (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\t\t\tret = ext4_ext_check_inode(inode);\n\t\t\telse\n\t\t\t\tret = ext4_ind_check_inode(inode);\n\t\t}\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\t/* VFS does not allow setting these so must be corruption */\n\t\tif (IS_APPEND(inode) || IS_IMMUTABLE(inode)) {\n\t\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t\t \"iget: immutable or append flags \"\n\t\t\t\t\t \"not allowed on symlinks\");\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\tif (IS_ENCRYPTED(inode)) {\n\t\t\tinode->i_op = &ext4_encrypted_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t} else if (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_link = (char *)ei->i_data;\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else if (ino == EXT4_BOOT_LOADER_INO) {\n\t\tmake_bad_inode(inode);\n\t} else {\n\t\tret = -EFSCORRUPTED;\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"iget: bogus i_mode (%o)\", inode->i_mode);\n\t\tgoto bad_inode;\n\t}\n\tif (IS_CASEFOLDED(inode) && !ext4_has_feature_casefold(inode->i_sb))\n\t\text4_error_inode(inode, function, line, 0,\n\t\t\t\t \"casefold flag without casefold feature\");\n\tbrelse(iloc.bh);\n\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}\n\nstatic int ext4_inode_blocks_set(handle_t *handle,\n\t\t\t\tstruct ext4_inode *raw_inode,\n\t\t\t\tstruct ext4_inode_info *ei)\n{\n\tstruct inode *inode = &(ei->vfs_inode);\n\tu64 i_blocks = inode->i_blocks;\n\tstruct super_block *sb = inode->i_sb;\n\n\tif (i_blocks <= ~0U) {\n\t\t/*\n\t\t * i_blocks can be represented in a 32 bit variable\n\t\t * as multiple of 512 bytes\n\t\t */\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = 0;\n\t\text4_clear_inode_flag(inode, EXT4_INODE_HUGE_FILE);\n\t\treturn 0;\n\t}\n\tif (!ext4_has_feature_huge_file(sb))\n\t\treturn -EFBIG;\n\n\tif (i_blocks <= 0xffffffffffffULL) {\n\t\t/*\n\t\t * i_blocks can be represented in a 48 bit variable\n\t\t * as multiple of 512 bytes\n\t\t */\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);\n\t\text4_clear_inode_flag(inode, EXT4_INODE_HUGE_FILE);\n\t} else {\n\t\text4_set_inode_flag(inode, EXT4_INODE_HUGE_FILE);\n\t\t/* i_block is stored in file system block size */\n\t\ti_blocks = i_blocks >> (inode->i_blkbits - 9);\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);\n\t}\n\treturn 0;\n}\n\nstruct other_inode {\n\tunsigned long\t\torig_ino;\n\tstruct ext4_inode\t*raw_inode;\n};\n\nstatic int other_inode_match(struct inode * inode, unsigned long ino,\n\t\t\t     void *data)\n{\n\tstruct other_inode *oi = (struct other_inode *) data;\n\n\tif ((inode->i_ino != ino) ||\n\t    (inode->i_state & (I_FREEING | I_WILL_FREE | I_NEW |\n\t\t\t       I_DIRTY_INODE)) ||\n\t    ((inode->i_state & I_DIRTY_TIME) == 0))\n\t\treturn 0;\n\tspin_lock(&inode->i_lock);\n\tif (((inode->i_state & (I_FREEING | I_WILL_FREE | I_NEW |\n\t\t\t\tI_DIRTY_INODE)) == 0) &&\n\t    (inode->i_state & I_DIRTY_TIME)) {\n\t\tstruct ext4_inode_info\t*ei = EXT4_I(inode);\n\n\t\tinode->i_state &= ~(I_DIRTY_TIME | I_DIRTY_TIME_EXPIRED);\n\t\tspin_unlock(&inode->i_lock);\n\n\t\tspin_lock(&ei->i_raw_lock);\n\t\tEXT4_INODE_SET_XTIME(i_ctime, inode, oi->raw_inode);\n\t\tEXT4_INODE_SET_XTIME(i_mtime, inode, oi->raw_inode);\n\t\tEXT4_INODE_SET_XTIME(i_atime, inode, oi->raw_inode);\n\t\text4_inode_csum_set(inode, oi->raw_inode, ei);\n\t\tspin_unlock(&ei->i_raw_lock);\n\t\ttrace_ext4_other_inode_update_time(inode, oi->orig_ino);\n\t\treturn -1;\n\t}\n\tspin_unlock(&inode->i_lock);\n\treturn -1;\n}\n\n/*\n * Opportunistically update the other time fields for other inodes in\n * the same inode table block.\n */\nstatic void ext4_update_other_inodes_time(struct super_block *sb,\n\t\t\t\t\t  unsigned long orig_ino, char *buf)\n{\n\tstruct other_inode oi;\n\tunsigned long ino;\n\tint i, inodes_per_block = EXT4_SB(sb)->s_inodes_per_block;\n\tint inode_size = EXT4_INODE_SIZE(sb);\n\n\toi.orig_ino = orig_ino;\n\t/*\n\t * Calculate the first inode in the inode table block.  Inode\n\t * numbers are one-based.  That is, the first inode in a block\n\t * (assuming 4k blocks and 256 byte inodes) is (n*16 + 1).\n\t */\n\tino = ((orig_ino - 1) & ~(inodes_per_block - 1)) + 1;\n\tfor (i = 0; i < inodes_per_block; i++, ino++, buf += inode_size) {\n\t\tif (ino == orig_ino)\n\t\t\tcontinue;\n\t\toi.raw_inode = (struct ext4_inode *) buf;\n\t\t(void) find_inode_nowait(sb, ino, other_inode_match, &oi);\n\t}\n}\n\n/*\n * Post the struct inode info into an on-disk inode location in the\n * buffer-cache.  This gobbles the caller's reference to the\n * buffer_head in the inode location struct.\n *\n * The caller must have write access to iloc->bh.\n */\nstatic int ext4_do_update_inode(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_iloc *iloc)\n{\n\tstruct ext4_inode *raw_inode = ext4_raw_inode(iloc);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct buffer_head *bh = iloc->bh;\n\tstruct super_block *sb = inode->i_sb;\n\tint err = 0, rc, block;\n\tint need_datasync = 0, set_large_file = 0;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tspin_lock(&ei->i_raw_lock);\n\n\t/* For fields not tracked in the in-memory inode,\n\t * initialise them to zero for new inodes. */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NEW))\n\t\tmemset(raw_inode, 0, EXT4_SB(inode->i_sb)->s_inode_size);\n\n\traw_inode->i_mode = cpu_to_le16(inode->i_mode);\n\ti_uid = i_uid_read(inode);\n\ti_gid = i_gid_read(inode);\n\ti_projid = from_kprojid(&init_user_ns, ei->i_projid);\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\traw_inode->i_uid_low = cpu_to_le16(low_16_bits(i_uid));\n\t\traw_inode->i_gid_low = cpu_to_le16(low_16_bits(i_gid));\n/*\n * Fix up interoperability with old kernels. Otherwise, old inodes get\n * re-used with the upper 16 bits of the uid/gid intact\n */\n\t\tif (ei->i_dtime && list_empty(&ei->i_orphan)) {\n\t\t\traw_inode->i_uid_high = 0;\n\t\t\traw_inode->i_gid_high = 0;\n\t\t} else {\n\t\t\traw_inode->i_uid_high =\n\t\t\t\tcpu_to_le16(high_16_bits(i_uid));\n\t\t\traw_inode->i_gid_high =\n\t\t\t\tcpu_to_le16(high_16_bits(i_gid));\n\t\t}\n\t} else {\n\t\traw_inode->i_uid_low = cpu_to_le16(fs_high2lowuid(i_uid));\n\t\traw_inode->i_gid_low = cpu_to_le16(fs_high2lowgid(i_gid));\n\t\traw_inode->i_uid_high = 0;\n\t\traw_inode->i_gid_high = 0;\n\t}\n\traw_inode->i_links_count = cpu_to_le16(inode->i_nlink);\n\n\tEXT4_INODE_SET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_SET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_SET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_SET_XTIME(i_crtime, ei, raw_inode);\n\n\terr = ext4_inode_blocks_set(handle, raw_inode, ei);\n\tif (err) {\n\t\tspin_unlock(&ei->i_raw_lock);\n\t\tgoto out_brelse;\n\t}\n\traw_inode->i_dtime = cpu_to_le32(ei->i_dtime);\n\traw_inode->i_flags = cpu_to_le32(ei->i_flags & 0xFFFFFFFF);\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT)))\n\t\traw_inode->i_file_acl_high =\n\t\t\tcpu_to_le16(ei->i_file_acl >> 32);\n\traw_inode->i_file_acl_lo = cpu_to_le32(ei->i_file_acl);\n\tif (ei->i_disksize != ext4_isize(inode->i_sb, raw_inode)) {\n\t\text4_isize_set(raw_inode, ei->i_disksize);\n\t\tneed_datasync = 1;\n\t}\n\tif (ei->i_disksize > 0x7fffffffULL) {\n\t\tif (!ext4_has_feature_large_file(sb) ||\n\t\t\t\tEXT4_SB(sb)->s_es->s_rev_level ==\n\t\t    cpu_to_le32(EXT4_GOOD_OLD_REV))\n\t\t\tset_large_file = 1;\n\t}\n\traw_inode->i_generation = cpu_to_le32(inode->i_generation);\n\tif (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) {\n\t\tif (old_valid_dev(inode->i_rdev)) {\n\t\t\traw_inode->i_block[0] =\n\t\t\t\tcpu_to_le32(old_encode_dev(inode->i_rdev));\n\t\t\traw_inode->i_block[1] = 0;\n\t\t} else {\n\t\t\traw_inode->i_block[0] = 0;\n\t\t\traw_inode->i_block[1] =\n\t\t\t\tcpu_to_le32(new_encode_dev(inode->i_rdev));\n\t\t\traw_inode->i_block[2] = 0;\n\t\t}\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\t\traw_inode->i_block[block] = ei->i_data[block];\n\t}\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = ext4_inode_peek_iversion(inode);\n\n\t\traw_inode->i_disk_version = cpu_to_le32(ivers);\n\t\tif (ei->i_extra_isize) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\traw_inode->i_version_hi =\n\t\t\t\t\tcpu_to_le32(ivers >> 32);\n\t\t\traw_inode->i_extra_isize =\n\t\t\t\tcpu_to_le16(ei->i_extra_isize);\n\t\t}\n\t}\n\n\tBUG_ON(!ext4_has_feature_project(inode->i_sb) &&\n\t       i_projid != EXT4_DEF_PROJID);\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\traw_inode->i_projid = cpu_to_le32(i_projid);\n\n\text4_inode_csum_set(inode, raw_inode, ei);\n\tspin_unlock(&ei->i_raw_lock);\n\tif (inode->i_sb->s_flags & SB_LAZYTIME)\n\t\text4_update_other_inodes_time(inode->i_sb, inode->i_ino,\n\t\t\t\t\t      bh->b_data);\n\n\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\trc = ext4_handle_dirty_metadata(handle, NULL, bh);\n\tif (!err)\n\t\terr = rc;\n\text4_clear_inode_state(inode, EXT4_STATE_NEW);\n\tif (set_large_file) {\n\t\tBUFFER_TRACE(EXT4_SB(sb)->s_sbh, \"get write access\");\n\t\terr = ext4_journal_get_write_access(handle, EXT4_SB(sb)->s_sbh);\n\t\tif (err)\n\t\t\tgoto out_brelse;\n\t\text4_set_feature_large_file(sb);\n\t\text4_handle_sync(handle);\n\t\terr = ext4_handle_dirty_super(handle, sb);\n\t}\n\text4_update_inode_fsync_trans(handle, inode, need_datasync);\nout_brelse:\n\tbrelse(bh);\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n\n/*\n * ext4_write_inode()\n *\n * We are called from a few places:\n *\n * - Within generic_file_aio_write() -> generic_write_sync() for O_SYNC files.\n *   Here, there will be no transaction running. We wait for any running\n *   transaction to commit.\n *\n * - Within flush work (sys_sync(), kupdate and such).\n *   We wait on commit, if told to.\n *\n * - Within iput_final() -> write_inode_now()\n *   We wait on commit, if told to.\n *\n * In all cases it is actually safe for us to return without doing anything,\n * because the inode has been copied into a raw inode buffer in\n * ext4_mark_inode_dirty().  This is a correctness thing for WB_SYNC_ALL\n * writeback.\n *\n * Note that we are absolutely dependent upon all inode dirtiers doing the\n * right thing: they *must* call mark_inode_dirty() after dirtying info in\n * which we are interested.\n *\n * It would be a bug for them to not do this.  The code:\n *\n *\tmark_inode_dirty(inode)\n *\tstuff();\n *\tinode->i_size = expr;\n *\n * is in error because write_inode() could occur while `stuff()' is running,\n * and the new i_size will be lost.  Plus the inode will no longer be on the\n * superblock's dirty inode list.\n */\nint ext4_write_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\tint err;\n\n\tif (WARN_ON_ONCE(current->flags & PF_MEMALLOC) ||\n\t    sb_rdonly(inode->i_sb))\n\t\treturn 0;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tif (EXT4_SB(inode->i_sb)->s_journal) {\n\t\tif (ext4_journal_current_handle()) {\n\t\t\tjbd_debug(1, \"called recursively, non-PF_MEMALLOC!\\n\");\n\t\t\tdump_stack();\n\t\t\treturn -EIO;\n\t\t}\n\n\t\t/*\n\t\t * No need to force transaction in WB_SYNC_NONE mode. Also\n\t\t * ext4_sync_fs() will force the commit after everything is\n\t\t * written.\n\t\t */\n\t\tif (wbc->sync_mode != WB_SYNC_ALL || wbc->for_sync)\n\t\t\treturn 0;\n\n\t\terr = jbd2_complete_transaction(EXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t\t\tEXT4_I(inode)->i_sync_tid);\n\t} else {\n\t\tstruct ext4_iloc iloc;\n\n\t\terr = __ext4_get_inode_loc(inode, &iloc, 0);\n\t\tif (err)\n\t\t\treturn err;\n\t\t/*\n\t\t * sync(2) will flush the whole buffer cache. No need to do\n\t\t * it here separately for each inode.\n\t\t */\n\t\tif (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync)\n\t\t\tsync_dirty_buffer(iloc.bh);\n\t\tif (buffer_req(iloc.bh) && !buffer_uptodate(iloc.bh)) {\n\t\t\tEXT4_ERROR_INODE_BLOCK(inode, iloc.bh->b_blocknr,\n\t\t\t\t\t \"IO error syncing inode\");\n\t\t\terr = -EIO;\n\t\t}\n\t\tbrelse(iloc.bh);\n\t}\n\treturn err;\n}\n\n/*\n * In data=journal mode ext4_journalled_invalidatepage() may fail to invalidate\n * buffers that are attached to a page stradding i_size and are undergoing\n * commit. In that case we have to wait for commit to finish and try again.\n */\nstatic void ext4_wait_for_tail_page_commit(struct inode *inode)\n{\n\tstruct page *page;\n\tunsigned offset;\n\tjournal_t *journal = EXT4_SB(inode->i_sb)->s_journal;\n\ttid_t commit_tid = 0;\n\tint ret;\n\n\toffset = inode->i_size & (PAGE_SIZE - 1);\n\t/*\n\t * All buffers in the last page remain valid? Then there's nothing to\n\t * do. We do the check mainly to optimize the common PAGE_SIZE ==\n\t * blocksize case\n\t */\n\tif (offset > PAGE_SIZE - i_blocksize(inode))\n\t\treturn;\n\twhile (1) {\n\t\tpage = find_lock_page(inode->i_mapping,\n\t\t\t\t      inode->i_size >> PAGE_SHIFT);\n\t\tif (!page)\n\t\t\treturn;\n\t\tret = __ext4_journalled_invalidatepage(page, offset,\n\t\t\t\t\t\tPAGE_SIZE - offset);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret != -EBUSY)\n\t\t\treturn;\n\t\tcommit_tid = 0;\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_committing_transaction)\n\t\t\tcommit_tid = journal->j_committing_transaction->t_tid;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tif (commit_tid)\n\t\t\tjbd2_log_wait_commit(journal, commit_tid);\n\t}\n}\n\n/*\n * ext4_setattr()\n *\n * Called from notify_change.\n *\n * We want to trap VFS attempts to truncate the file as soon as\n * possible.  In particular, we want to make sure that when the VFS\n * shrinks i_size, we put the inode on the orphan list and modify\n * i_disksize immediately, so that during the subsequent flushing of\n * dirty pages and freeing of disk blocks, we can guarantee that any\n * commit will leave the blocks being flushed in an unused state on\n * disk.  (On recovery, the inode will get truncated and the blocks will\n * be freed, so we have a strong guarantee that no future commit will\n * leave these blocks visible to the user.)\n *\n * Another thing we have to assure is that if we are in ordered mode\n * and inode is still attached to the committing transaction, we must\n * we start writeout of all the dirty pages which are being truncated.\n * This way we are sure that all the data written in the previous\n * transaction are already on disk (truncate waits for pages under\n * writeback).\n *\n * Called with inode->i_mutex down.\n */\nint ext4_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tint error, rc = 0;\n\tint orphan = 0;\n\tconst unsigned int ia_valid = attr->ia_valid;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\tif (unlikely(IS_IMMUTABLE(inode)))\n\t\treturn -EPERM;\n\n\tif (unlikely(IS_APPEND(inode) &&\n\t\t     (ia_valid & (ATTR_MODE | ATTR_UID |\n\t\t\t\t  ATTR_GID | ATTR_TIMES_SET))))\n\t\treturn -EPERM;\n\n\terror = setattr_prepare(dentry, attr);\n\tif (error)\n\t\treturn error;\n\n\terror = fscrypt_prepare_setattr(dentry, attr);\n\tif (error)\n\t\treturn error;\n\n\terror = fsverity_prepare_setattr(dentry, attr);\n\tif (error)\n\t\treturn error;\n\n\tif (is_quota_modification(inode, attr)) {\n\t\terror = dquot_initialize(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif ((ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) ||\n\t    (ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid))) {\n\t\thandle_t *handle;\n\n\t\t/* (user+group)*(old+new) structure, inode write (sb,\n\t\t * inode block, ? - but truncate inode update has it) */\n\t\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA,\n\t\t\t(EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb) +\n\t\t\t EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb)) + 3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\n\t\t/* dquot_transfer() calls back ext4_get_inode_usage() which\n\t\t * counts xattr inode references.\n\t\t */\n\t\tdown_read(&EXT4_I(inode)->xattr_sem);\n\t\terror = dquot_transfer(inode, attr);\n\t\tup_read(&EXT4_I(inode)->xattr_sem);\n\n\t\tif (error) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn error;\n\t\t}\n\t\t/* Update corresponding info in inode so that everything is in\n\t\t * one transaction */\n\t\tif (attr->ia_valid & ATTR_UID)\n\t\t\tinode->i_uid = attr->ia_uid;\n\t\tif (attr->ia_valid & ATTR_GID)\n\t\t\tinode->i_gid = attr->ia_gid;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\thandle_t *handle;\n\t\tloff_t oldsize = inode->i_size;\n\t\tint shrink = (attr->ia_size < inode->i_size);\n\n\t\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\t\tif (attr->ia_size > sbi->s_bitmap_maxbytes)\n\t\t\t\treturn -EFBIG;\n\t\t}\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\treturn -EINVAL;\n\n\t\tif (IS_I_VERSION(inode) && attr->ia_size != inode->i_size)\n\t\t\tinode_inc_iversion(inode);\n\n\t\tif (shrink) {\n\t\t\tif (ext4_should_order_data(inode)) {\n\t\t\t\terror = ext4_begin_ordered_truncate(inode,\n\t\t\t\t\t\t\t    attr->ia_size);\n\t\t\t\tif (error)\n\t\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Blocks are going to be removed from the inode. Wait\n\t\t\t * for dio in flight.\n\t\t\t */\n\t\t\tinode_dio_wait(inode);\n\t\t}\n\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\t\trc = ext4_break_layouts(inode);\n\t\tif (rc) {\n\t\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t\treturn rc;\n\t\t}\n\n\t\tif (attr->ia_size != inode->i_size) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 3);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terror = PTR_ERR(handle);\n\t\t\t\tgoto out_mmap_sem;\n\t\t\t}\n\t\t\tif (ext4_handle_valid(handle) && shrink) {\n\t\t\t\terror = ext4_orphan_add(handle, inode);\n\t\t\t\torphan = 1;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Update c/mtime on truncate up, ext4_truncate() will\n\t\t\t * update c/mtime in shrink case below\n\t\t\t */\n\t\t\tif (!shrink) {\n\t\t\t\tinode->i_mtime = current_time(inode);\n\t\t\t\tinode->i_ctime = inode->i_mtime;\n\t\t\t}\n\t\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_I(inode)->i_disksize = attr->ia_size;\n\t\t\trc = ext4_mark_inode_dirty(handle, inode);\n\t\t\tif (!error)\n\t\t\t\terror = rc;\n\t\t\t/*\n\t\t\t * We have to update i_size under i_data_sem together\n\t\t\t * with i_disksize to avoid races with writeback code\n\t\t\t * running ext4_wb_update_i_disksize().\n\t\t\t */\n\t\t\tif (!error)\n\t\t\t\ti_size_write(inode, attr->ia_size);\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\text4_journal_stop(handle);\n\t\t\tif (error)\n\t\t\t\tgoto out_mmap_sem;\n\t\t\tif (!shrink) {\n\t\t\t\tpagecache_isize_extended(inode, oldsize,\n\t\t\t\t\t\t\t inode->i_size);\n\t\t\t} else if (ext4_should_journal_data(inode)) {\n\t\t\t\text4_wait_for_tail_page_commit(inode);\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Truncate pagecache after we've waited for commit\n\t\t * in data=journal mode to make pages freeable.\n\t\t */\n\t\ttruncate_pagecache(inode, inode->i_size);\n\t\t/*\n\t\t * Call ext4_truncate() even if i_size didn't change to\n\t\t * truncate possible preallocated blocks.\n\t\t */\n\t\tif (attr->ia_size <= oldsize) {\n\t\t\trc = ext4_truncate(inode);\n\t\t\tif (rc)\n\t\t\t\terror = rc;\n\t\t}\nout_mmap_sem:\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t}\n\n\tif (!error) {\n\t\tsetattr_copy(inode, attr);\n\t\tmark_inode_dirty(inode);\n\t}\n\n\t/*\n\t * If the call to ext4_truncate failed to get a transaction handle at\n\t * all, we need to clean up the in-core orphan list manually.\n\t */\n\tif (orphan && inode->i_nlink)\n\t\text4_orphan_del(NULL, inode);\n\n\tif (!error && (ia_valid & ATTR_MODE))\n\t\trc = posix_acl_chmod(inode, inode->i_mode);\n\nerr_out:\n\text4_std_error(inode->i_sb, error);\n\tif (!error)\n\t\terror = rc;\n\treturn error;\n}\n\nint ext4_getattr(const struct path *path, struct kstat *stat,\n\t\t u32 request_mask, unsigned int query_flags)\n{\n\tstruct inode *inode = d_inode(path->dentry);\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tunsigned int flags;\n\n\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_crtime)) {\n\t\tstat->result_mask |= STATX_BTIME;\n\t\tstat->btime.tv_sec = ei->i_crtime.tv_sec;\n\t\tstat->btime.tv_nsec = ei->i_crtime.tv_nsec;\n\t}\n\n\tflags = ei->i_flags & EXT4_FL_USER_VISIBLE;\n\tif (flags & EXT4_APPEND_FL)\n\t\tstat->attributes |= STATX_ATTR_APPEND;\n\tif (flags & EXT4_COMPR_FL)\n\t\tstat->attributes |= STATX_ATTR_COMPRESSED;\n\tif (flags & EXT4_ENCRYPT_FL)\n\t\tstat->attributes |= STATX_ATTR_ENCRYPTED;\n\tif (flags & EXT4_IMMUTABLE_FL)\n\t\tstat->attributes |= STATX_ATTR_IMMUTABLE;\n\tif (flags & EXT4_NODUMP_FL)\n\t\tstat->attributes |= STATX_ATTR_NODUMP;\n\n\tstat->attributes_mask |= (STATX_ATTR_APPEND |\n\t\t\t\t  STATX_ATTR_COMPRESSED |\n\t\t\t\t  STATX_ATTR_ENCRYPTED |\n\t\t\t\t  STATX_ATTR_IMMUTABLE |\n\t\t\t\t  STATX_ATTR_NODUMP);\n\n\tgeneric_fillattr(inode, stat);\n\treturn 0;\n}\n\nint ext4_file_getattr(const struct path *path, struct kstat *stat,\n\t\t      u32 request_mask, unsigned int query_flags)\n{\n\tstruct inode *inode = d_inode(path->dentry);\n\tu64 delalloc_blocks;\n\n\text4_getattr(path, stat, request_mask, query_flags);\n\n\t/*\n\t * If there is inline data in the inode, the inode will normally not\n\t * have data blocks allocated (it may have an external xattr block).\n\t * Report at least one sector for such files, so tools like tar, rsync,\n\t * others don't incorrectly think the file is completely sparse.\n\t */\n\tif (unlikely(ext4_has_inline_data(inode)))\n\t\tstat->blocks += (stat->size + 511) >> 9;\n\n\t/*\n\t * We can't update i_blocks if the block allocation is delayed\n\t * otherwise in the case of system crash before the real block\n\t * allocation is done, we will have i_blocks inconsistent with\n\t * on-disk file blocks.\n\t * We always keep i_blocks updated together with real\n\t * allocation. But to not confuse with user, stat\n\t * will return the blocks that include the delayed allocation\n\t * blocks for this file.\n\t */\n\tdelalloc_blocks = EXT4_C2B(EXT4_SB(inode->i_sb),\n\t\t\t\t   EXT4_I(inode)->i_reserved_data_blocks);\n\tstat->blocks += delalloc_blocks << (inode->i_sb->s_blocksize_bits - 9);\n\treturn 0;\n}\n\nstatic int ext4_index_trans_blocks(struct inode *inode, int lblocks,\n\t\t\t\t   int pextents)\n{\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn ext4_ind_trans_blocks(inode, lblocks);\n\treturn ext4_ext_index_trans_blocks(inode, pextents);\n}\n\n/*\n * Account for index blocks, block groups bitmaps and block group\n * descriptor blocks if modify datablocks and index blocks\n * worse case, the indexs blocks spread over different block groups\n *\n * If datablocks are discontiguous, they are possible to spread over\n * different block groups too. If they are contiguous, with flexbg,\n * they could still across block group boundary.\n *\n * Also account for superblock, inode, quota and xattr blocks\n */\nstatic int ext4_meta_trans_blocks(struct inode *inode, int lblocks,\n\t\t\t\t  int pextents)\n{\n\text4_group_t groups, ngroups = ext4_get_groups_count(inode->i_sb);\n\tint gdpblocks;\n\tint idxblocks;\n\tint ret = 0;\n\n\t/*\n\t * How many index blocks need to touch to map @lblocks logical blocks\n\t * to @pextents physical extents?\n\t */\n\tidxblocks = ext4_index_trans_blocks(inode, lblocks, pextents);\n\n\tret = idxblocks;\n\n\t/*\n\t * Now let's see how many group bitmaps and group descriptors need\n\t * to account\n\t */\n\tgroups = idxblocks + pextents;\n\tgdpblocks = groups;\n\tif (groups > ngroups)\n\t\tgroups = ngroups;\n\tif (groups > EXT4_SB(inode->i_sb)->s_gdb_count)\n\t\tgdpblocks = EXT4_SB(inode->i_sb)->s_gdb_count;\n\n\t/* bitmaps and block group descriptor blocks */\n\tret += groups + gdpblocks;\n\n\t/* Blocks for super block, inode, quota and xattr blocks */\n\tret += EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\n\treturn ret;\n}\n\n/*\n * Calculate the total number of credits to reserve to fit\n * the modification of a single pages into a single transaction,\n * which may include multiple chunks of block allocations.\n *\n * This could be called via ext4_write_begin()\n *\n * We need to consider the worse case, when\n * one new block per extent.\n */\nint ext4_writepage_trans_blocks(struct inode *inode)\n{\n\tint bpp = ext4_journal_blocks_per_page(inode);\n\tint ret;\n\n\tret = ext4_meta_trans_blocks(inode, bpp, bpp);\n\n\t/* Account for data blocks for journalled mode */\n\tif (ext4_should_journal_data(inode))\n\t\tret += bpp;\n\treturn ret;\n}\n\n/*\n * Calculate the journal credits for a chunk of data modification.\n *\n * This is called from DIO, fallocate or whoever calling\n * ext4_map_blocks() to map/allocate a chunk of contiguous disk blocks.\n *\n * journal buffers for data blocks are not included here, as DIO\n * and fallocate do no need to journal data buffers.\n */\nint ext4_chunk_trans_blocks(struct inode *inode, int nrblocks)\n{\n\treturn ext4_meta_trans_blocks(inode, nrblocks, 1);\n}\n\n/*\n * The caller must have previously called ext4_reserve_inode_write().\n * Give this, we know that the caller already has write access to iloc->bh.\n */\nint ext4_mark_iloc_dirty(handle_t *handle,\n\t\t\t struct inode *inode, struct ext4_iloc *iloc)\n{\n\tint err = 0;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb)))) {\n\t\tput_bh(iloc->bh);\n\t\treturn -EIO;\n\t}\n\tif (IS_I_VERSION(inode))\n\t\tinode_inc_iversion(inode);\n\n\t/* the do_update_inode consumes one bh->b_count */\n\tget_bh(iloc->bh);\n\n\t/* ext4_do_update_inode() does jbd2_journal_dirty_metadata */\n\terr = ext4_do_update_inode(handle, inode, iloc);\n\tput_bh(iloc->bh);\n\treturn err;\n}\n\n/*\n * On success, We end up with an outstanding reference count against\n * iloc->bh.  This _must_ be cleaned up later.\n */\n\nint\next4_reserve_inode_write(handle_t *handle, struct inode *inode,\n\t\t\t struct ext4_iloc *iloc)\n{\n\tint err;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn -EIO;\n\n\terr = ext4_get_inode_loc(inode, iloc);\n\tif (!err) {\n\t\tBUFFER_TRACE(iloc->bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, iloc->bh);\n\t\tif (err) {\n\t\t\tbrelse(iloc->bh);\n\t\t\tiloc->bh = NULL;\n\t\t}\n\t}\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n\nstatic int __ext4_expand_extra_isize(struct inode *inode,\n\t\t\t\t     unsigned int new_extra_isize,\n\t\t\t\t     struct ext4_iloc *iloc,\n\t\t\t\t     handle_t *handle, int *no_expand)\n{\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tunsigned int inode_size = EXT4_INODE_SIZE(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint error;\n\n\t/* this was checked at iget time, but double check for good measure */\n\tif ((EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > inode_size) ||\n\t    (ei->i_extra_isize & 3)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extra_isize %u (inode size %u)\",\n\t\t\t\t ei->i_extra_isize,\n\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\treturn -EFSCORRUPTED;\n\t}\n\tif ((new_extra_isize < ei->i_extra_isize) ||\n\t    (new_extra_isize < 4) ||\n\t    (new_extra_isize > inode_size - EXT4_GOOD_OLD_INODE_SIZE))\n\t\treturn -EINVAL;\t/* Should never happen */\n\n\traw_inode = ext4_raw_inode(iloc);\n\n\theader = IHDR(inode, raw_inode);\n\n\t/* No extended attributes present */\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||\n\t    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\tmemset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE +\n\t\t       EXT4_I(inode)->i_extra_isize, 0,\n\t\t       new_extra_isize - EXT4_I(inode)->i_extra_isize);\n\t\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\t\treturn 0;\n\t}\n\n\t/* try to expand with EAs present */\n\terror = ext4_expand_extra_isize_ea(inode, new_extra_isize,\n\t\t\t\t\t   raw_inode, handle);\n\tif (error) {\n\t\t/*\n\t\t * Inode size expansion failed; don't try again\n\t\t */\n\t\t*no_expand = 1;\n\t}\n\n\treturn error;\n}\n\n/*\n * Expand an inode by new_extra_isize bytes.\n * Returns 0 on success or negative error number on failure.\n */\nstatic int ext4_try_to_expand_extra_isize(struct inode *inode,\n\t\t\t\t\t  unsigned int new_extra_isize,\n\t\t\t\t\t  struct ext4_iloc iloc,\n\t\t\t\t\t  handle_t *handle)\n{\n\tint no_expand;\n\tint error;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND))\n\t\treturn -EOVERFLOW;\n\n\t/*\n\t * In nojournal mode, we can immediately attempt to expand\n\t * the inode.  When journaled, we first need to obtain extra\n\t * buffer credits since we may write into the EA block\n\t * with this same handle. If journal_extend fails, then it will\n\t * only result in a minor loss of functionality for that inode.\n\t * If this is felt to be critical, then e2fsck should be run to\n\t * force a large enough s_min_extra_isize.\n\t */\n\tif (ext4_journal_extend(handle,\n\t\t\t\tEXT4_DATA_TRANS_BLOCKS(inode->i_sb), 0) != 0)\n\t\treturn -ENOSPC;\n\n\tif (ext4_write_trylock_xattr(inode, &no_expand) == 0)\n\t\treturn -EBUSY;\n\n\terror = __ext4_expand_extra_isize(inode, new_extra_isize, &iloc,\n\t\t\t\t\t  handle, &no_expand);\n\text4_write_unlock_xattr(inode, &no_expand);\n\n\treturn error;\n}\n\nint ext4_expand_extra_isize(struct inode *inode,\n\t\t\t    unsigned int new_extra_isize,\n\t\t\t    struct ext4_iloc *iloc)\n{\n\thandle_t *handle;\n\tint no_expand;\n\tint error, rc;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND)) {\n\t\tbrelse(iloc->bh);\n\t\treturn -EOVERFLOW;\n\t}\n\n\thandle = ext4_journal_start(inode, EXT4_HT_INODE,\n\t\t\t\t    EXT4_DATA_TRANS_BLOCKS(inode->i_sb));\n\tif (IS_ERR(handle)) {\n\t\terror = PTR_ERR(handle);\n\t\tbrelse(iloc->bh);\n\t\treturn error;\n\t}\n\n\text4_write_lock_xattr(inode, &no_expand);\n\n\tBUFFER_TRACE(iloc->bh, \"get_write_access\");\n\terror = ext4_journal_get_write_access(handle, iloc->bh);\n\tif (error) {\n\t\tbrelse(iloc->bh);\n\t\tgoto out_stop;\n\t}\n\n\terror = __ext4_expand_extra_isize(inode, new_extra_isize, iloc,\n\t\t\t\t\t  handle, &no_expand);\n\n\trc = ext4_mark_iloc_dirty(handle, inode, iloc);\n\tif (!error)\n\t\terror = rc;\n\n\text4_write_unlock_xattr(inode, &no_expand);\nout_stop:\n\text4_journal_stop(handle);\n\treturn error;\n}\n\n/*\n * What we do here is to mark the in-core inode as clean with respect to inode\n * dirtiness (it may still be data-dirty).\n * This means that the in-core inode may be reaped by prune_icache\n * without having to perform any I/O.  This is a very good thing,\n * because *any* task may call prune_icache - even ones which\n * have a transaction open against a different journal.\n *\n * Is this cheating?  Not really.  Sure, we haven't written the\n * inode out, but prune_icache isn't a user-visible syncing function.\n * Whenever the user wants stuff synced (sys_sync, sys_msync, sys_fsync)\n * we start and wait on commits.\n */\nint ext4_mark_inode_dirty(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint err;\n\n\tmight_sleep();\n\ttrace_ext4_mark_inode_dirty(inode, _RET_IP_);\n\terr = ext4_reserve_inode_write(handle, inode, &iloc);\n\tif (err)\n\t\treturn err;\n\n\tif (EXT4_I(inode)->i_extra_isize < sbi->s_want_extra_isize)\n\t\text4_try_to_expand_extra_isize(inode, sbi->s_want_extra_isize,\n\t\t\t\t\t       iloc, handle);\n\n\treturn ext4_mark_iloc_dirty(handle, inode, &iloc);\n}\n\n/*\n * ext4_dirty_inode() is called from __mark_inode_dirty()\n *\n * We're really interested in the case where a file is being extended.\n * i_size has been changed by generic_commit_write() and we thus need\n * to include the updated inode in the current transaction.\n *\n * Also, dquot_alloc_block() will always dirty the inode when blocks\n * are allocated to the file.\n *\n * If the inode is marked synchronous, we don't honour that here - doing\n * so would cause a commit on atime updates, which we don't bother doing.\n * We handle synchronous inodes at the highest possible level.\n *\n * If only the I_DIRTY_TIME flag is set, we can skip everything.  If\n * I_DIRTY_TIME and I_DIRTY_SYNC is set, the only inode fields we need\n * to copy into the on-disk inode structure are the timestamp files.\n */\nvoid ext4_dirty_inode(struct inode *inode, int flags)\n{\n\thandle_t *handle;\n\n\tif (flags == I_DIRTY_TIME)\n\t\treturn;\n\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 2);\n\tif (IS_ERR(handle))\n\t\tgoto out;\n\n\text4_mark_inode_dirty(handle, inode);\n\n\text4_journal_stop(handle);\nout:\n\treturn;\n}\n\nint ext4_change_inode_journal_flag(struct inode *inode, int val)\n{\n\tjournal_t *journal;\n\thandle_t *handle;\n\tint err;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t/*\n\t * We have to be very careful here: changing a data block's\n\t * journaling status dynamically is dangerous.  If we write a\n\t * data block to the journal, change the status and then delete\n\t * that block, we risk forgetting to revoke the old log record\n\t * from the journal and so a subsequent replay can corrupt data.\n\t * So, first we make sure that the journal is empty and that\n\t * nobody is changing anything.\n\t */\n\n\tjournal = EXT4_JOURNAL(inode);\n\tif (!journal)\n\t\treturn 0;\n\tif (is_journal_aborted(journal))\n\t\treturn -EROFS;\n\n\t/* Wait for all existing dio workers */\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Before flushing the journal and switching inode's aops, we have\n\t * to flush all dirty data the inode has. There can be outstanding\n\t * delayed allocations, there can be unwritten extents created by\n\t * fallocate or buffered writes in dioread_nolock mode covered by\n\t * dirty data which can be converted only after flushing the dirty\n\t * data (and journalled aops don't know how to handle these cases).\n\t */\n\tif (val) {\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\t\terr = filemap_write_and_wait(inode->i_mapping);\n\t\tif (err < 0) {\n\t\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tpercpu_down_write(&sbi->s_journal_flag_rwsem);\n\tjbd2_journal_lock_updates(journal);\n\n\t/*\n\t * OK, there are no updates running now, and all cached data is\n\t * synced to disk.  We are now in a completely consistent state\n\t * which doesn't have anything in the journal, and we know that\n\t * no filesystem updates are running, so it is safe to modify\n\t * the inode's in-core data-journaling state flag now.\n\t */\n\n\tif (val)\n\t\text4_set_inode_flag(inode, EXT4_INODE_JOURNAL_DATA);\n\telse {\n\t\terr = jbd2_journal_flush(journal);\n\t\tif (err < 0) {\n\t\t\tjbd2_journal_unlock_updates(journal);\n\t\t\tpercpu_up_write(&sbi->s_journal_flag_rwsem);\n\t\t\treturn err;\n\t\t}\n\t\text4_clear_inode_flag(inode, EXT4_INODE_JOURNAL_DATA);\n\t}\n\text4_set_aops(inode);\n\n\tjbd2_journal_unlock_updates(journal);\n\tpercpu_up_write(&sbi->s_journal_flag_rwsem);\n\n\tif (val)\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\n\t/* Finally we can mark the inode as dirty. */\n\n\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\terr = ext4_mark_inode_dirty(handle, inode);\n\text4_handle_sync(handle);\n\text4_journal_stop(handle);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\nstatic int ext4_bh_unmapped(handle_t *handle, struct buffer_head *bh)\n{\n\treturn !buffer_mapped(bh);\n}\n\nvm_fault_t ext4_page_mkwrite(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page = vmf->page;\n\tloff_t size;\n\tunsigned long len;\n\tint err;\n\tvm_fault_t ret;\n\tstruct file *file = vma->vm_file;\n\tstruct inode *inode = file_inode(file);\n\tstruct address_space *mapping = inode->i_mapping;\n\thandle_t *handle;\n\tget_block_t *get_block;\n\tint retries = 0;\n\n\tif (unlikely(IS_IMMUTABLE(inode)))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tsb_start_pagefault(inode->i_sb);\n\tfile_update_time(vma->vm_file);\n\n\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\n\terr = ext4_convert_inline_data(inode);\n\tif (err)\n\t\tgoto out_ret;\n\n\t/* Delalloc case is easy... */\n\tif (test_opt(inode->i_sb, DELALLOC) &&\n\t    !ext4_should_journal_data(inode) &&\n\t    !ext4_nonda_switch(inode->i_sb)) {\n\t\tdo {\n\t\t\terr = block_page_mkwrite(vma, vmf,\n\t\t\t\t\t\t   ext4_da_get_block_prep);\n\t\t} while (err == -ENOSPC &&\n\t\t       ext4_should_retry_alloc(inode->i_sb, &retries));\n\t\tgoto out_ret;\n\t}\n\n\tlock_page(page);\n\tsize = i_size_read(inode);\n\t/* Page got truncated from under us? */\n\tif (page->mapping != mapping || page_offset(page) > size) {\n\t\tunlock_page(page);\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\tif (page->index == size >> PAGE_SHIFT)\n\t\tlen = size & ~PAGE_MASK;\n\telse\n\t\tlen = PAGE_SIZE;\n\t/*\n\t * Return if we have all the buffers mapped. This avoids the need to do\n\t * journal_start/journal_stop which can block and take a long time\n\t */\n\tif (page_has_buffers(page)) {\n\t\tif (!ext4_walk_page_buffers(NULL, page_buffers(page),\n\t\t\t\t\t    0, len, NULL,\n\t\t\t\t\t    ext4_bh_unmapped)) {\n\t\t\t/* Wait so that we don't change page under IO */\n\t\t\twait_for_stable_page(page);\n\t\t\tret = VM_FAULT_LOCKED;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tunlock_page(page);\n\t/* OK, we need to fill the hole... */\n\tif (ext4_should_dioread_nolock(inode))\n\t\tget_block = ext4_get_block_unwritten;\n\telse\n\t\tget_block = ext4_get_block;\nretry_alloc:\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,\n\t\t\t\t    ext4_writepage_trans_blocks(inode));\n\tif (IS_ERR(handle)) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out;\n\t}\n\terr = block_page_mkwrite(vma, vmf, get_block);\n\tif (!err && ext4_should_journal_data(inode)) {\n\t\tif (ext4_walk_page_buffers(handle, page_buffers(page), 0,\n\t\t\t  PAGE_SIZE, NULL, do_journal_get_write_access)) {\n\t\t\tunlock_page(page);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\text4_journal_stop(handle);\n\t\t\tgoto out;\n\t\t}\n\t\text4_set_inode_state(inode, EXT4_STATE_JDATA);\n\t}\n\text4_journal_stop(handle);\n\tif (err == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry_alloc;\nout_ret:\n\tret = block_page_mkwrite_return(err);\nout:\n\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\tsb_end_pagefault(inode->i_sb);\n\treturn ret;\n}\n\nvm_fault_t ext4_filemap_fault(struct vm_fault *vmf)\n{\n\tstruct inode *inode = file_inode(vmf->vma->vm_file);\n\tvm_fault_t ret;\n\n\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\tret = filemap_fault(vmf);\n\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\n\treturn ret;\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/ext4/super.c\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/fs/minix/inode.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  Big-endian to little-endian byte-swapping/bitmaps by\n *        David S. Miller (davem@caip.rutgers.edu), 1995\n */\n\n#include <linux/module.h>\n#include <linux/string.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/parser.h>\n#include <linux/buffer_head.h>\n#include <linux/exportfs.h>\n#include <linux/vfs.h>\n#include <linux/random.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/quotaops.h>\n#include <linux/seq_file.h>\n#include <linux/ctype.h>\n#include <linux/log2.h>\n#include <linux/crc16.h>\n#include <linux/dax.h>\n#include <linux/cleancache.h>\n#include <linux/uaccess.h>\n#include <linux/iversion.h>\n#include <linux/unicode.h>\n\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n\n#include \"ext4.h\"\n#include \"ext4_extents.h\"\t/* Needed for trace points definition */\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n#include \"mballoc.h\"\n#include \"fsmap.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/ext4.h>\n\nstatic struct ext4_lazy_init *ext4_li_info;\nstatic struct mutex ext4_li_mtx;\nstatic struct ratelimit_state ext4_mount_msg_ratelimit;\n\nstatic int ext4_load_journal(struct super_block *, struct ext4_super_block *,\n\t\t\t     unsigned long journal_devnum);\nstatic int ext4_show_options(struct seq_file *seq, struct dentry *root);\nstatic int ext4_commit_super(struct super_block *sb, int sync);\nstatic void ext4_mark_recovery_complete(struct super_block *sb,\n\t\t\t\t\tstruct ext4_super_block *es);\nstatic void ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es);\nstatic int ext4_sync_fs(struct super_block *sb, int wait);\nstatic int ext4_remount(struct super_block *sb, int *flags, char *data);\nstatic int ext4_statfs(struct dentry *dentry, struct kstatfs *buf);\nstatic int ext4_unfreeze(struct super_block *sb);\nstatic int ext4_freeze(struct super_block *sb);\nstatic struct dentry *ext4_mount(struct file_system_type *fs_type, int flags,\n\t\t       const char *dev_name, void *data);\nstatic inline int ext2_feature_set_ok(struct super_block *sb);\nstatic inline int ext3_feature_set_ok(struct super_block *sb);\nstatic int ext4_feature_set_ok(struct super_block *sb, int readonly);\nstatic void ext4_destroy_lazyinit_thread(void);\nstatic void ext4_unregister_li_request(struct super_block *sb);\nstatic void ext4_clear_request_list(void);\nstatic struct inode *ext4_get_journal_inode(struct super_block *sb,\n\t\t\t\t\t    unsigned int journal_inum);\n\n/*\n * Lock ordering\n *\n * Note the difference between i_mmap_sem (EXT4_I(inode)->i_mmap_sem) and\n * i_mmap_rwsem (inode->i_mmap_rwsem)!\n *\n * page fault path:\n * mmap_sem -> sb_start_pagefault -> i_mmap_sem (r) -> transaction start ->\n *   page lock -> i_data_sem (rw)\n *\n * buffered write path:\n * sb_start_write -> i_mutex -> mmap_sem\n * sb_start_write -> i_mutex -> transaction start -> page lock ->\n *   i_data_sem (rw)\n *\n * truncate:\n * sb_start_write -> i_mutex -> i_mmap_sem (w) -> i_mmap_rwsem (w) -> page lock\n * sb_start_write -> i_mutex -> i_mmap_sem (w) -> transaction start ->\n *   i_data_sem (rw)\n *\n * direct IO:\n * sb_start_write -> i_mutex -> mmap_sem\n * sb_start_write -> i_mutex -> transaction start -> i_data_sem (rw)\n *\n * writepages:\n * transaction start -> page lock(s) -> i_data_sem (rw)\n */\n\n#if !defined(CONFIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT2)\nstatic struct file_system_type ext2_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext2\",\n\t.mount\t\t= ext4_mount,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"ext2\");\nMODULE_ALIAS(\"ext2\");\n#define IS_EXT2_SB(sb) ((sb)->s_bdev->bd_holder == &ext2_fs_type)\n#else\n#define IS_EXT2_SB(sb) (0)\n#endif\n\n\nstatic struct file_system_type ext3_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext3\",\n\t.mount\t\t= ext4_mount,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"ext3\");\nMODULE_ALIAS(\"ext3\");\n#define IS_EXT3_SB(sb) ((sb)->s_bdev->bd_holder == &ext3_fs_type)\n\n/*\n * This works like sb_bread() except it uses ERR_PTR for error\n * returns.  Currently with sb_bread it's impossible to distinguish\n * between ENOMEM and EIO situations (since both result in a NULL\n * return.\n */\nstruct buffer_head *\next4_sb_bread(struct super_block *sb, sector_t block, int op_flags)\n{\n\tstruct buffer_head *bh = sb_getblk(sb, block);\n\n\tif (bh == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tll_rw_block(REQ_OP_READ, REQ_META | op_flags, 1, &bh);\n\twait_on_buffer(bh);\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tput_bh(bh);\n\treturn ERR_PTR(-EIO);\n}\n\nstatic int ext4_verify_csum_type(struct super_block *sb,\n\t\t\t\t struct ext4_super_block *es)\n{\n\tif (!ext4_has_feature_metadata_csum(sb))\n\t\treturn 1;\n\n\treturn es->s_checksum_type == EXT4_CRC32C_CHKSUM;\n}\n\nstatic __le32 ext4_superblock_csum(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tint offset = offsetof(struct ext4_super_block, s_checksum);\n\t__u32 csum;\n\n\tcsum = ext4_chksum(sbi, ~0, (char *)es, offset);\n\n\treturn cpu_to_le32(csum);\n}\n\nstatic int ext4_superblock_csum_verify(struct super_block *sb,\n\t\t\t\t       struct ext4_super_block *es)\n{\n\tif (!ext4_has_metadata_csum(sb))\n\t\treturn 1;\n\n\treturn es->s_checksum == ext4_superblock_csum(sb, es);\n}\n\nvoid ext4_superblock_csum_set(struct super_block *sb)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tif (!ext4_has_metadata_csum(sb))\n\t\treturn;\n\n\tes->s_checksum = ext4_superblock_csum(sb, es);\n}\n\nvoid *ext4_kvmalloc(size_t size, gfp_t flags)\n{\n\tvoid *ret;\n\n\tret = kmalloc(size, flags | __GFP_NOWARN);\n\tif (!ret)\n\t\tret = __vmalloc(size, flags, PAGE_KERNEL);\n\treturn ret;\n}\n\nvoid *ext4_kvzalloc(size_t size, gfp_t flags)\n{\n\tvoid *ret;\n\n\tret = kzalloc(size, flags | __GFP_NOWARN);\n\tif (!ret)\n\t\tret = __vmalloc(size, flags | __GFP_ZERO, PAGE_KERNEL);\n\treturn ret;\n}\n\next4_fsblk_t ext4_block_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_block_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_block_bitmap_hi) << 32 : 0);\n}\n\next4_fsblk_t ext4_inode_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_inode_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_bitmap_hi) << 32 : 0);\n}\n\next4_fsblk_t ext4_inode_table(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_inode_table_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_table_hi) << 32 : 0);\n}\n\n__u32 ext4_free_group_clusters(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_free_blocks_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_free_blocks_count_hi) << 16 : 0);\n}\n\n__u32 ext4_free_inodes_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_free_inodes_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_free_inodes_count_hi) << 16 : 0);\n}\n\n__u32 ext4_used_dirs_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_used_dirs_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_used_dirs_count_hi) << 16 : 0);\n}\n\n__u32 ext4_itable_unused_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_itable_unused_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_itable_unused_hi) << 16 : 0);\n}\n\nvoid ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_block_bitmap_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_block_bitmap_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_inode_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_inode_bitmap_lo  = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_inode_bitmap_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_inode_table_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_inode_table_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_inode_table_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_free_group_clusters_set(struct super_block *sb,\n\t\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_free_blocks_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_free_blocks_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_free_inodes_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_free_inodes_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_free_inodes_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_used_dirs_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_used_dirs_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_used_dirs_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_itable_unused_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_itable_unused_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_itable_unused_hi = cpu_to_le16(count >> 16);\n}\n\nstatic void __ext4_update_tstamp(__le32 *lo, __u8 *hi)\n{\n\ttime64_t now = ktime_get_real_seconds();\n\n\tnow = clamp_val(now, 0, (1ull << 40) - 1);\n\n\t*lo = cpu_to_le32(lower_32_bits(now));\n\t*hi = upper_32_bits(now);\n}\n\nstatic time64_t __ext4_get_tstamp(__le32 *lo, __u8 *hi)\n{\n\treturn ((time64_t)(*hi) << 32) + le32_to_cpu(*lo);\n}\n#define ext4_update_tstamp(es, tstamp) \\\n\t__ext4_update_tstamp(&(es)->tstamp, &(es)->tstamp ## _hi)\n#define ext4_get_tstamp(es, tstamp) \\\n\t__ext4_get_tstamp(&(es)->tstamp, &(es)->tstamp ## _hi)\n\nstatic void __save_error_info(struct super_block *sb, const char *func,\n\t\t\t    unsigned int line)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\tif (bdev_read_only(sb->s_bdev))\n\t\treturn;\n\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\text4_update_tstamp(es, s_last_error_time);\n\tstrncpy(es->s_last_error_func, func, sizeof(es->s_last_error_func));\n\tes->s_last_error_line = cpu_to_le32(line);\n\tif (!es->s_first_error_time) {\n\t\tes->s_first_error_time = es->s_last_error_time;\n\t\tes->s_first_error_time_hi = es->s_last_error_time_hi;\n\t\tstrncpy(es->s_first_error_func, func,\n\t\t\tsizeof(es->s_first_error_func));\n\t\tes->s_first_error_line = cpu_to_le32(line);\n\t\tes->s_first_error_ino = es->s_last_error_ino;\n\t\tes->s_first_error_block = es->s_last_error_block;\n\t}\n\t/*\n\t * Start the daily error reporting function if it hasn't been\n\t * started already\n\t */\n\tif (!es->s_error_count)\n\t\tmod_timer(&EXT4_SB(sb)->s_err_report, jiffies + 24*60*60*HZ);\n\tle32_add_cpu(&es->s_error_count, 1);\n}\n\nstatic void save_error_info(struct super_block *sb, const char *func,\n\t\t\t    unsigned int line)\n{\n\t__save_error_info(sb, func, line);\n\text4_commit_super(sb, 1);\n}\n\n/*\n * The del_gendisk() function uninitializes the disk-specific data\n * structures, including the bdi structure, without telling anyone\n * else.  Once this happens, any attempt to call mark_buffer_dirty()\n * (for example, by ext4_commit_super), will cause a kernel OOPS.\n * This is a kludge to prevent these oops until we can put in a proper\n * hook in del_gendisk() to inform the VFS and file system layers.\n */\nstatic int block_device_ejected(struct super_block *sb)\n{\n\tstruct inode *bd_inode = sb->s_bdev->bd_inode;\n\tstruct backing_dev_info *bdi = inode_to_bdi(bd_inode);\n\n\treturn bdi->dev == NULL;\n}\n\nstatic void ext4_journal_commit_callback(journal_t *journal, transaction_t *txn)\n{\n\tstruct super_block\t\t*sb = journal->j_private;\n\tstruct ext4_sb_info\t\t*sbi = EXT4_SB(sb);\n\tint\t\t\t\terror = is_journal_aborted(journal);\n\tstruct ext4_journal_cb_entry\t*jce;\n\n\tBUG_ON(txn->t_state == T_FINISHED);\n\n\text4_process_freed_data(sb, txn->t_tid);\n\n\tspin_lock(&sbi->s_md_lock);\n\twhile (!list_empty(&txn->t_private_list)) {\n\t\tjce = list_entry(txn->t_private_list.next,\n\t\t\t\t struct ext4_journal_cb_entry, jce_list);\n\t\tlist_del_init(&jce->jce_list);\n\t\tspin_unlock(&sbi->s_md_lock);\n\t\tjce->jce_func(sb, jce, error);\n\t\tspin_lock(&sbi->s_md_lock);\n\t}\n\tspin_unlock(&sbi->s_md_lock);\n}\n\nstatic bool system_going_down(void)\n{\n\treturn system_state == SYSTEM_HALT || system_state == SYSTEM_POWER_OFF\n\t\t|| system_state == SYSTEM_RESTART;\n}\n\n/* Deal with the reporting of failure conditions on a filesystem such as\n * inconsistencies detected or read IO failures.\n *\n * On ext2, we can store the error state of the filesystem in the\n * superblock.  That is not possible on ext4, because we may have other\n * write ordering constraints on the superblock which prevent us from\n * writing it out straight away; and given that the journal is about to\n * be aborted, we can't rely on the current, or future, transactions to\n * write out the superblock safely.\n *\n * We'll just use the jbd2_journal_abort() error code to record an error in\n * the journal instead.  On recovery, the journal will complain about\n * that error until we've noted it down and cleared it.\n */\n\nstatic void ext4_handle_error(struct super_block *sb)\n{\n\tif (test_opt(sb, WARN_ON_ERROR))\n\t\tWARN_ON_ONCE(1);\n\n\tif (sb_rdonly(sb))\n\t\treturn;\n\n\tif (!test_opt(sb, ERRORS_CONT)) {\n\t\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\t\tEXT4_SB(sb)->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\tif (journal)\n\t\t\tjbd2_journal_abort(journal, -EIO);\n\t}\n\t/*\n\t * We force ERRORS_RO behavior when system is rebooting. Otherwise we\n\t * could panic during 'reboot -f' as the underlying device got already\n\t * disabled.\n\t */\n\tif (test_opt(sb, ERRORS_RO) || system_going_down()) {\n\t\text4_msg(sb, KERN_CRIT, \"Remounting filesystem read-only\");\n\t\t/*\n\t\t * Make sure updated value of ->s_mount_flags will be visible\n\t\t * before ->s_flags update\n\t\t */\n\t\tsmp_wmb();\n\t\tsb->s_flags |= SB_RDONLY;\n\t} else if (test_opt(sb, ERRORS_PANIC)) {\n\t\tif (EXT4_SB(sb)->s_journal &&\n\t\t  !(EXT4_SB(sb)->s_journal->j_flags & JBD2_REC_ERR))\n\t\t\treturn;\n\t\tpanic(\"EXT4-fs (device %s): panic forced after error\\n\",\n\t\t\tsb->s_id);\n\t}\n}\n\n#define ext4_error_ratelimit(sb)\t\t\t\t\t\\\n\t\t___ratelimit(&(EXT4_SB(sb)->s_err_ratelimit_state),\t\\\n\t\t\t     \"EXT4-fs error\")\n\nvoid __ext4_error(struct super_block *sb, const char *function,\n\t\t  unsigned int line, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\ttrace_ext4_error(sb, function, line);\n\tif (ext4_error_ratelimit(sb)) {\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tprintk(KERN_CRIT\n\t\t       \"EXT4-fs error (device %s): %s:%d: comm %s: %pV\\n\",\n\t\t       sb->s_id, function, line, current->comm, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(sb, function, line);\n\text4_handle_error(sb);\n}\n\nvoid __ext4_error_inode(struct inode *inode, const char *function,\n\t\t\tunsigned int line, ext4_fsblk_t block,\n\t\t\tconst char *fmt, ...)\n{\n\tva_list args;\n\tstruct va_format vaf;\n\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn;\n\n\ttrace_ext4_error(inode->i_sb, function, line);\n\tes->s_last_error_ino = cpu_to_le32(inode->i_ino);\n\tes->s_last_error_block = cpu_to_le64(block);\n\tif (ext4_error_ratelimit(inode->i_sb)) {\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tif (block)\n\t\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s:%d: \"\n\t\t\t       \"inode #%lu: block %llu: comm %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       block, current->comm, &vaf);\n\t\telse\n\t\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s:%d: \"\n\t\t\t       \"inode #%lu: comm %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       current->comm, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(inode->i_sb, function, line);\n\text4_handle_error(inode->i_sb);\n}\n\nvoid __ext4_error_file(struct file *file, const char *function,\n\t\t       unsigned int line, ext4_fsblk_t block,\n\t\t       const char *fmt, ...)\n{\n\tva_list args;\n\tstruct va_format vaf;\n\tstruct ext4_super_block *es;\n\tstruct inode *inode = file_inode(file);\n\tchar pathname[80], *path;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn;\n\n\ttrace_ext4_error(inode->i_sb, function, line);\n\tes = EXT4_SB(inode->i_sb)->s_es;\n\tes->s_last_error_ino = cpu_to_le32(inode->i_ino);\n\tif (ext4_error_ratelimit(inode->i_sb)) {\n\t\tpath = file_path(file, pathname, sizeof(pathname));\n\t\tif (IS_ERR(path))\n\t\t\tpath = \"(unknown)\";\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tif (block)\n\t\t\tprintk(KERN_CRIT\n\t\t\t       \"EXT4-fs error (device %s): %s:%d: inode #%lu: \"\n\t\t\t       \"block %llu: comm %s: path %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       block, current->comm, path, &vaf);\n\t\telse\n\t\t\tprintk(KERN_CRIT\n\t\t\t       \"EXT4-fs error (device %s): %s:%d: inode #%lu: \"\n\t\t\t       \"comm %s: path %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       current->comm, path, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(inode->i_sb, function, line);\n\text4_handle_error(inode->i_sb);\n}\n\nconst char *ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t      char nbuf[16])\n{\n\tchar *errstr = NULL;\n\n\tswitch (errno) {\n\tcase -EFSCORRUPTED:\n\t\terrstr = \"Corrupt filesystem\";\n\t\tbreak;\n\tcase -EFSBADCRC:\n\t\terrstr = \"Filesystem failed CRC\";\n\t\tbreak;\n\tcase -EIO:\n\t\terrstr = \"IO failure\";\n\t\tbreak;\n\tcase -ENOMEM:\n\t\terrstr = \"Out of memory\";\n\t\tbreak;\n\tcase -EROFS:\n\t\tif (!sb || (EXT4_SB(sb)->s_journal &&\n\t\t\t    EXT4_SB(sb)->s_journal->j_flags & JBD2_ABORT))\n\t\t\terrstr = \"Journal has aborted\";\n\t\telse\n\t\t\terrstr = \"Readonly filesystem\";\n\t\tbreak;\n\tdefault:\n\t\t/* If the caller passed in an extra buffer for unknown\n\t\t * errors, textualise them now.  Else we just return\n\t\t * NULL. */\n\t\tif (nbuf) {\n\t\t\t/* Check for truncated error codes... */\n\t\t\tif (snprintf(nbuf, 16, \"error %d\", -errno) >= 0)\n\t\t\t\terrstr = nbuf;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn errstr;\n}\n\n/* __ext4_std_error decodes expected errors from journaling functions\n * automatically and invokes the appropriate error response.  */\n\nvoid __ext4_std_error(struct super_block *sb, const char *function,\n\t\t      unsigned int line, int errno)\n{\n\tchar nbuf[16];\n\tconst char *errstr;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\t/* Special case: if the error is EROFS, and we're not already\n\t * inside a transaction, then there's really no point in logging\n\t * an error. */\n\tif (errno == -EROFS && journal_current_handle() == NULL && sb_rdonly(sb))\n\t\treturn;\n\n\tif (ext4_error_ratelimit(sb)) {\n\t\terrstr = ext4_decode_error(sb, errno, nbuf);\n\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s) in %s:%d: %s\\n\",\n\t\t       sb->s_id, function, line, errstr);\n\t}\n\n\tsave_error_info(sb, function, line);\n\text4_handle_error(sb);\n}\n\n/*\n * ext4_abort is a much stronger failure handler than ext4_error.  The\n * abort function may be used to deal with unrecoverable failures such\n * as journal IO errors or ENOMEM at a critical moment in log management.\n *\n * We unconditionally force the filesystem into an ABORT|READONLY state,\n * unless the error response on the fs has been set to panic in which\n * case we take the easy way out and panic immediately.\n */\n\nvoid __ext4_abort(struct super_block *sb, const char *function,\n\t\tunsigned int line, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\tsave_error_info(sb, function, line);\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s:%d: %pV\\n\",\n\t       sb->s_id, function, line, &vaf);\n\tva_end(args);\n\n\tif (sb_rdonly(sb) == 0) {\n\t\text4_msg(sb, KERN_CRIT, \"Remounting filesystem read-only\");\n\t\tEXT4_SB(sb)->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\t/*\n\t\t * Make sure updated value of ->s_mount_flags will be visible\n\t\t * before ->s_flags update\n\t\t */\n\t\tsmp_wmb();\n\t\tsb->s_flags |= SB_RDONLY;\n\t\tif (EXT4_SB(sb)->s_journal)\n\t\t\tjbd2_journal_abort(EXT4_SB(sb)->s_journal, -EIO);\n\t\tsave_error_info(sb, function, line);\n\t}\n\tif (test_opt(sb, ERRORS_PANIC) && !system_going_down()) {\n\t\tif (EXT4_SB(sb)->s_journal &&\n\t\t  !(EXT4_SB(sb)->s_journal->j_flags & JBD2_REC_ERR))\n\t\t\treturn;\n\t\tpanic(\"EXT4-fs panic from previous error\\n\");\n\t}\n}\n\nvoid __ext4_msg(struct super_block *sb,\n\t\tconst char *prefix, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (!___ratelimit(&(EXT4_SB(sb)->s_msg_ratelimit_state), \"EXT4-fs\"))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(\"%sEXT4-fs (%s): %pV\\n\", prefix, sb->s_id, &vaf);\n\tva_end(args);\n}\n\n#define ext4_warning_ratelimit(sb)\t\t\t\t\t\\\n\t\t___ratelimit(&(EXT4_SB(sb)->s_warning_ratelimit_state),\t\\\n\t\t\t     \"EXT4-fs warning\")\n\nvoid __ext4_warning(struct super_block *sb, const char *function,\n\t\t    unsigned int line, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (!ext4_warning_ratelimit(sb))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(KERN_WARNING \"EXT4-fs warning (device %s): %s:%d: %pV\\n\",\n\t       sb->s_id, function, line, &vaf);\n\tva_end(args);\n}\n\nvoid __ext4_warning_inode(const struct inode *inode, const char *function,\n\t\t\t  unsigned int line, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (!ext4_warning_ratelimit(inode->i_sb))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk(KERN_WARNING \"EXT4-fs warning (device %s): %s:%d: \"\n\t       \"inode #%lu: comm %s: %pV\\n\", inode->i_sb->s_id,\n\t       function, line, inode->i_ino, current->comm, &vaf);\n\tva_end(args);\n}\n\nvoid __ext4_grp_locked_error(const char *function, unsigned int line,\n\t\t\t     struct super_block *sb, ext4_group_t grp,\n\t\t\t     unsigned long ino, ext4_fsblk_t block,\n\t\t\t     const char *fmt, ...)\n__releases(bitlock)\n__acquires(bitlock)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\ttrace_ext4_error(sb, function, line);\n\tes->s_last_error_ino = cpu_to_le32(ino);\n\tes->s_last_error_block = cpu_to_le64(block);\n\t__save_error_info(sb, function, line);\n\n\tif (ext4_error_ratelimit(sb)) {\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s:%d: group %u, \",\n\t\t       sb->s_id, function, line, grp);\n\t\tif (ino)\n\t\t\tprintk(KERN_CONT \"inode %lu: \", ino);\n\t\tif (block)\n\t\t\tprintk(KERN_CONT \"block %llu:\",\n\t\t\t       (unsigned long long) block);\n\t\tprintk(KERN_CONT \"%pV\\n\", &vaf);\n\t\tva_end(args);\n\t}\n\n\tif (test_opt(sb, WARN_ON_ERROR))\n\t\tWARN_ON_ONCE(1);\n\n\tif (test_opt(sb, ERRORS_CONT)) {\n\t\text4_commit_super(sb, 0);\n\t\treturn;\n\t}\n\n\text4_unlock_group(sb, grp);\n\text4_commit_super(sb, 1);\n\text4_handle_error(sb);\n\t/*\n\t * We only get here in the ERRORS_RO case; relocking the group\n\t * may be dangerous, but nothing bad will happen since the\n\t * filesystem will have already been marked read/only and the\n\t * journal has been aborted.  We return 1 as a hint to callers\n\t * who might what to use the return value from\n\t * ext4_grp_locked_error() to distinguish between the\n\t * ERRORS_CONT and ERRORS_RO case, and perhaps return more\n\t * aggressively from the ext4 function in question, with a\n\t * more appropriate error code.\n\t */\n\text4_lock_group(sb, grp);\n\treturn;\n}\n\nvoid ext4_mark_group_bitmap_corrupted(struct super_block *sb,\n\t\t\t\t     ext4_group_t group,\n\t\t\t\t     unsigned int flags)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, group, NULL);\n\tint ret;\n\n\tif (flags & EXT4_GROUP_INFO_BBITMAP_CORRUPT) {\n\t\tret = ext4_test_and_set_bit(EXT4_GROUP_INFO_BBITMAP_CORRUPT_BIT,\n\t\t\t\t\t    &grp->bb_state);\n\t\tif (!ret)\n\t\t\tpercpu_counter_sub(&sbi->s_freeclusters_counter,\n\t\t\t\t\t   grp->bb_free);\n\t}\n\n\tif (flags & EXT4_GROUP_INFO_IBITMAP_CORRUPT) {\n\t\tret = ext4_test_and_set_bit(EXT4_GROUP_INFO_IBITMAP_CORRUPT_BIT,\n\t\t\t\t\t    &grp->bb_state);\n\t\tif (!ret && gdp) {\n\t\t\tint count;\n\n\t\t\tcount = ext4_free_inodes_count(sb, gdp);\n\t\t\tpercpu_counter_sub(&sbi->s_freeinodes_counter,\n\t\t\t\t\t   count);\n\t\t}\n\t}\n}\n\nvoid ext4_update_dynamic_rev(struct super_block *sb)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tif (le32_to_cpu(es->s_rev_level) > EXT4_GOOD_OLD_REV)\n\t\treturn;\n\n\text4_warning(sb,\n\t\t     \"updating to rev %d because of new feature flag, \"\n\t\t     \"running e2fsck is recommended\",\n\t\t     EXT4_DYNAMIC_REV);\n\n\tes->s_first_ino = cpu_to_le32(EXT4_GOOD_OLD_FIRST_INO);\n\tes->s_inode_size = cpu_to_le16(EXT4_GOOD_OLD_INODE_SIZE);\n\tes->s_rev_level = cpu_to_le32(EXT4_DYNAMIC_REV);\n\t/* leave es->s_feature_*compat flags alone */\n\t/* es->s_uuid will be set by e2fsck if empty */\n\n\t/*\n\t * The rest of the superblock fields should be zero, and if not it\n\t * means they are likely already in use, so leave them alone.  We\n\t * can leave it up to e2fsck to clean up any inconsistencies there.\n\t */\n}\n\n/*\n * Open the external journal device\n */\nstatic struct block_device *ext4_blkdev_get(dev_t dev, struct super_block *sb)\n{\n\tstruct block_device *bdev;\n\tchar b[BDEVNAME_SIZE];\n\n\tbdev = blkdev_get_by_dev(dev, FMODE_READ|FMODE_WRITE|FMODE_EXCL, sb);\n\tif (IS_ERR(bdev))\n\t\tgoto fail;\n\treturn bdev;\n\nfail:\n\text4_msg(sb, KERN_ERR, \"failed to open journal device %s: %ld\",\n\t\t\t__bdevname(dev, b), PTR_ERR(bdev));\n\treturn NULL;\n}\n\n/*\n * Release the journal device\n */\nstatic void ext4_blkdev_put(struct block_device *bdev)\n{\n\tblkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);\n}\n\nstatic void ext4_blkdev_remove(struct ext4_sb_info *sbi)\n{\n\tstruct block_device *bdev;\n\tbdev = sbi->journal_bdev;\n\tif (bdev) {\n\t\text4_blkdev_put(bdev);\n\t\tsbi->journal_bdev = NULL;\n\t}\n}\n\nstatic inline struct inode *orphan_list_entry(struct list_head *l)\n{\n\treturn &list_entry(l, struct ext4_inode_info, i_orphan)->vfs_inode;\n}\n\nstatic void dump_orphan_list(struct super_block *sb, struct ext4_sb_info *sbi)\n{\n\tstruct list_head *l;\n\n\text4_msg(sb, KERN_ERR, \"sb orphan head is %d\",\n\t\t le32_to_cpu(sbi->s_es->s_last_orphan));\n\n\tprintk(KERN_ERR \"sb_info orphan list:\\n\");\n\tlist_for_each(l, &sbi->s_orphan) {\n\t\tstruct inode *inode = orphan_list_entry(l);\n\t\tprintk(KERN_ERR \"  \"\n\t\t       \"inode %s:%lu at %p: mode %o, nlink %d, next %d\\n\",\n\t\t       inode->i_sb->s_id, inode->i_ino, inode,\n\t\t       inode->i_mode, inode->i_nlink,\n\t\t       NEXT_ORPHAN(inode));\n\t}\n}\n\n#ifdef CONFIG_QUOTA\nstatic int ext4_quota_off(struct super_block *sb, int type);\n\nstatic inline void ext4_quota_off_umount(struct super_block *sb)\n{\n\tint type;\n\n\t/* Use our quota_off function to clear inode flags etc. */\n\tfor (type = 0; type < EXT4_MAXQUOTAS; type++)\n\t\text4_quota_off(sb, type);\n}\n\n/*\n * This is a helper function which is used in the mount/remount\n * codepaths (which holds s_umount) to fetch the quota file name.\n */\nstatic inline char *get_qf_name(struct super_block *sb,\n\t\t\t\tstruct ext4_sb_info *sbi,\n\t\t\t\tint type)\n{\n\treturn rcu_dereference_protected(sbi->s_qf_names[type],\n\t\t\t\t\t lockdep_is_held(&sb->s_umount));\n}\n#else\nstatic inline void ext4_quota_off_umount(struct super_block *sb)\n{\n}\n#endif\n\nstatic void ext4_put_super(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tint aborted = 0;\n\tint i, err;\n\n\text4_unregister_li_request(sb);\n\text4_quota_off_umount(sb);\n\n\tdestroy_workqueue(sbi->rsv_conversion_wq);\n\n\tif (sbi->s_journal) {\n\t\taborted = is_journal_aborted(sbi->s_journal);\n\t\terr = jbd2_journal_destroy(sbi->s_journal);\n\t\tsbi->s_journal = NULL;\n\t\tif ((err < 0) && !aborted)\n\t\t\text4_abort(sb, \"Couldn't clean up the journal\");\n\t}\n\n\text4_unregister_sysfs(sb);\n\text4_es_unregister_shrinker(sbi);\n\tdel_timer_sync(&sbi->s_err_report);\n\text4_release_system_zone(sb);\n\text4_mb_release(sb);\n\text4_ext_release(sb);\n\n\tif (!sb_rdonly(sb) && !aborted) {\n\t\text4_clear_feature_journal_needs_recovery(sb);\n\t\tes->s_state = cpu_to_le16(sbi->s_mount_state);\n\t}\n\tif (!sb_rdonly(sb))\n\t\text4_commit_super(sb, 1);\n\n\tfor (i = 0; i < sbi->s_gdb_count; i++)\n\t\tbrelse(sbi->s_group_desc[i]);\n\tkvfree(sbi->s_group_desc);\n\tkvfree(sbi->s_flex_groups);\n\tpercpu_counter_destroy(&sbi->s_freeclusters_counter);\n\tpercpu_counter_destroy(&sbi->s_freeinodes_counter);\n\tpercpu_counter_destroy(&sbi->s_dirs_counter);\n\tpercpu_counter_destroy(&sbi->s_dirtyclusters_counter);\n\tpercpu_free_rwsem(&sbi->s_journal_flag_rwsem);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tkfree(get_qf_name(sb, sbi, i));\n#endif\n\n\t/* Debugging code just in case the in-memory inode orphan list\n\t * isn't empty.  The on-disk one can be non-empty if we've\n\t * detected an error and taken the fs readonly, but the\n\t * in-memory list had better be clean by this point. */\n\tif (!list_empty(&sbi->s_orphan))\n\t\tdump_orphan_list(sb, sbi);\n\tJ_ASSERT(list_empty(&sbi->s_orphan));\n\n\tsync_blockdev(sb->s_bdev);\n\tinvalidate_bdev(sb->s_bdev);\n\tif (sbi->journal_bdev && sbi->journal_bdev != sb->s_bdev) {\n\t\t/*\n\t\t * Invalidate the journal device's buffers.  We don't want them\n\t\t * floating about in memory - the physical journal device may\n\t\t * hotswapped, and it breaks the `ro-after' testing code.\n\t\t */\n\t\tsync_blockdev(sbi->journal_bdev);\n\t\tinvalidate_bdev(sbi->journal_bdev);\n\t\text4_blkdev_remove(sbi);\n\t}\n\n\text4_xattr_destroy_cache(sbi->s_ea_inode_cache);\n\tsbi->s_ea_inode_cache = NULL;\n\n\text4_xattr_destroy_cache(sbi->s_ea_block_cache);\n\tsbi->s_ea_block_cache = NULL;\n\n\tif (sbi->s_mmp_tsk)\n\t\tkthread_stop(sbi->s_mmp_tsk);\n\tbrelse(sbi->s_sbh);\n\tsb->s_fs_info = NULL;\n\t/*\n\t * Now that we are completely done shutting down the\n\t * superblock, we need to actually destroy the kobject.\n\t */\n\tkobject_put(&sbi->s_kobj);\n\twait_for_completion(&sbi->s_kobj_unregister);\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi->s_blockgroup_lock);\n\tfs_put_dax(sbi->s_daxdev);\n#ifdef CONFIG_UNICODE\n\tutf8_unload(sbi->s_encoding);\n#endif\n\tkfree(sbi);\n}\n\nstatic struct kmem_cache *ext4_inode_cachep;\n\n/*\n * Called inside transaction, so use GFP_NOFS\n */\nstatic struct inode *ext4_alloc_inode(struct super_block *sb)\n{\n\tstruct ext4_inode_info *ei;\n\n\tei = kmem_cache_alloc(ext4_inode_cachep, GFP_NOFS);\n\tif (!ei)\n\t\treturn NULL;\n\n\tinode_set_iversion(&ei->vfs_inode, 1);\n\tspin_lock_init(&ei->i_raw_lock);\n\tINIT_LIST_HEAD(&ei->i_prealloc_list);\n\tspin_lock_init(&ei->i_prealloc_lock);\n\text4_es_init_tree(&ei->i_es_tree);\n\trwlock_init(&ei->i_es_lock);\n\tINIT_LIST_HEAD(&ei->i_es_list);\n\tei->i_es_all_nr = 0;\n\tei->i_es_shk_nr = 0;\n\tei->i_es_shrink_lblk = 0;\n\tei->i_reserved_data_blocks = 0;\n\tei->i_da_metadata_calc_len = 0;\n\tei->i_da_metadata_calc_last_lblock = 0;\n\tspin_lock_init(&(ei->i_block_reservation_lock));\n\text4_init_pending_tree(&ei->i_pending_tree);\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n\tmemset(&ei->i_dquot, 0, sizeof(ei->i_dquot));\n#endif\n\tei->jinode = NULL;\n\tINIT_LIST_HEAD(&ei->i_rsv_conversion_list);\n\tspin_lock_init(&ei->i_completed_io_lock);\n\tei->i_sync_tid = 0;\n\tei->i_datasync_tid = 0;\n\tatomic_set(&ei->i_unwritten, 0);\n\tINIT_WORK(&ei->i_rsv_conversion_work, ext4_end_io_rsv_work);\n\treturn &ei->vfs_inode;\n}\n\nstatic int ext4_drop_inode(struct inode *inode)\n{\n\tint drop = generic_drop_inode(inode);\n\n\tif (!drop)\n\t\tdrop = fscrypt_drop_inode(inode);\n\n\ttrace_ext4_drop_inode(inode, drop);\n\treturn drop;\n}\n\nstatic void ext4_free_in_core_inode(struct inode *inode)\n{\n\tfscrypt_free_inode(inode);\n\tkmem_cache_free(ext4_inode_cachep, EXT4_I(inode));\n}\n\nstatic void ext4_destroy_inode(struct inode *inode)\n{\n\tif (!list_empty(&(EXT4_I(inode)->i_orphan))) {\n\t\text4_msg(inode->i_sb, KERN_ERR,\n\t\t\t \"Inode %lu (%p): orphan list check failed!\",\n\t\t\t inode->i_ino, EXT4_I(inode));\n\t\tprint_hex_dump(KERN_INFO, \"\", DUMP_PREFIX_ADDRESS, 16, 4,\n\t\t\t\tEXT4_I(inode), sizeof(struct ext4_inode_info),\n\t\t\t\ttrue);\n\t\tdump_stack();\n\t}\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct ext4_inode_info *ei = (struct ext4_inode_info *) foo;\n\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\tinit_rwsem(&ei->xattr_sem);\n\tinit_rwsem(&ei->i_data_sem);\n\tinit_rwsem(&ei->i_mmap_sem);\n\tinode_init_once(&ei->vfs_inode);\n}\n\nstatic int __init init_inodecache(void)\n{\n\text4_inode_cachep = kmem_cache_create_usercopy(\"ext4_inode_cache\",\n\t\t\t\tsizeof(struct ext4_inode_info), 0,\n\t\t\t\t(SLAB_RECLAIM_ACCOUNT|SLAB_MEM_SPREAD|\n\t\t\t\t\tSLAB_ACCOUNT),\n\t\t\t\toffsetof(struct ext4_inode_info, i_data),\n\t\t\t\tsizeof_field(struct ext4_inode_info, i_data),\n\t\t\t\tinit_once);\n\tif (ext4_inode_cachep == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void destroy_inodecache(void)\n{\n\t/*\n\t * Make sure all delayed rcu free inodes are flushed before we\n\t * destroy cache.\n\t */\n\trcu_barrier();\n\tkmem_cache_destroy(ext4_inode_cachep);\n}\n\nvoid ext4_clear_inode(struct inode *inode)\n{\n\tinvalidate_inode_buffers(inode);\n\tclear_inode(inode);\n\tdquot_drop(inode);\n\text4_discard_preallocations(inode);\n\text4_es_remove_extent(inode, 0, EXT_MAX_BLOCKS);\n\tif (EXT4_I(inode)->jinode) {\n\t\tjbd2_journal_release_jbd_inode(EXT4_JOURNAL(inode),\n\t\t\t\t\t       EXT4_I(inode)->jinode);\n\t\tjbd2_free_inode(EXT4_I(inode)->jinode);\n\t\tEXT4_I(inode)->jinode = NULL;\n\t}\n\tfscrypt_put_encryption_info(inode);\n\tfsverity_cleanup_inode(inode);\n}\n\nstatic struct inode *ext4_nfs_get_inode(struct super_block *sb,\n\t\t\t\t\tu64 ino, u32 generation)\n{\n\tstruct inode *inode;\n\n\t/*\n\t * Currently we don't know the generation for parent directory, so\n\t * a generation of 0 means \"accept any\"\n\t */\n\tinode = ext4_iget(sb, ino, EXT4_IGET_HANDLE);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (generation && inode->i_generation != generation) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\n\treturn inode;\n}\n\nstatic struct dentry *ext4_fh_to_dentry(struct super_block *sb, struct fid *fid,\n\t\t\t\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_dentry(sb, fid, fh_len, fh_type,\n\t\t\t\t    ext4_nfs_get_inode);\n}\n\nstatic struct dentry *ext4_fh_to_parent(struct super_block *sb, struct fid *fid,\n\t\t\t\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_parent(sb, fid, fh_len, fh_type,\n\t\t\t\t    ext4_nfs_get_inode);\n}\n\nstatic int ext4_nfs_commit_metadata(struct inode *inode)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_ALL\n\t};\n\n\ttrace_ext4_nfs_commit_metadata(inode);\n\treturn ext4_write_inode(inode, &wbc);\n}\n\n/*\n * Try to release metadata pages (indirect blocks, directories) which are\n * mapped via the block device.  Since these pages could have journal heads\n * which would prevent try_to_free_buffers() from freeing them, we must use\n * jbd2 layer's try_to_free_buffers() function to release them.\n */\nstatic int bdev_try_to_free_page(struct super_block *sb, struct page *page,\n\t\t\t\t gfp_t wait)\n{\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\tWARN_ON(PageChecked(page));\n\tif (!page_has_buffers(page))\n\t\treturn 0;\n\tif (journal)\n\t\treturn jbd2_journal_try_to_free_buffers(journal, page,\n\t\t\t\t\t\twait & ~__GFP_DIRECT_RECLAIM);\n\treturn try_to_free_buffers(page);\n}\n\n#ifdef CONFIG_FS_ENCRYPTION\nstatic int ext4_get_context(struct inode *inode, void *ctx, size_t len)\n{\n\treturn ext4_xattr_get(inode, EXT4_XATTR_INDEX_ENCRYPTION,\n\t\t\t\t EXT4_XATTR_NAME_ENCRYPTION_CONTEXT, ctx, len);\n}\n\nstatic int ext4_set_context(struct inode *inode, const void *ctx, size_t len,\n\t\t\t\t\t\t\tvoid *fs_data)\n{\n\thandle_t *handle = fs_data;\n\tint res, res2, credits, retries = 0;\n\n\t/*\n\t * Encrypting the root directory is not allowed because e2fsck expects\n\t * lost+found to exist and be unencrypted, and encrypting the root\n\t * directory would imply encrypting the lost+found directory as well as\n\t * the filename \"lost+found\" itself.\n\t */\n\tif (inode->i_ino == EXT4_ROOT_INO)\n\t\treturn -EPERM;\n\n\tif (WARN_ON_ONCE(IS_DAX(inode) && i_size_read(inode)))\n\t\treturn -EINVAL;\n\n\tres = ext4_convert_inline_data(inode);\n\tif (res)\n\t\treturn res;\n\n\t/*\n\t * If a journal handle was specified, then the encryption context is\n\t * being set on a new inode via inheritance and is part of a larger\n\t * transaction to create the inode.  Otherwise the encryption context is\n\t * being set on an existing inode in its own transaction.  Only in the\n\t * latter case should the \"retry on ENOSPC\" logic be used.\n\t */\n\n\tif (handle) {\n\t\tres = ext4_xattr_set_handle(handle, inode,\n\t\t\t\t\t    EXT4_XATTR_INDEX_ENCRYPTION,\n\t\t\t\t\t    EXT4_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\t\t    ctx, len, 0);\n\t\tif (!res) {\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_ENCRYPT);\n\t\t\text4_clear_inode_state(inode,\n\t\t\t\t\tEXT4_STATE_MAY_INLINE_DATA);\n\t\t\t/*\n\t\t\t * Update inode->i_flags - S_ENCRYPTED will be enabled,\n\t\t\t * S_DAX may be disabled\n\t\t\t */\n\t\t\text4_set_inode_flags(inode);\n\t\t}\n\t\treturn res;\n\t}\n\n\tres = dquot_initialize(inode);\n\tif (res)\n\t\treturn res;\nretry:\n\tres = ext4_xattr_set_credits(inode, len, false /* is_create */,\n\t\t\t\t     &credits);\n\tif (res)\n\t\treturn res;\n\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, credits);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\tres = ext4_xattr_set_handle(handle, inode, EXT4_XATTR_INDEX_ENCRYPTION,\n\t\t\t\t    EXT4_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\t    ctx, len, 0);\n\tif (!res) {\n\t\text4_set_inode_flag(inode, EXT4_INODE_ENCRYPT);\n\t\t/*\n\t\t * Update inode->i_flags - S_ENCRYPTED will be enabled,\n\t\t * S_DAX may be disabled\n\t\t */\n\t\text4_set_inode_flags(inode);\n\t\tres = ext4_mark_inode_dirty(handle, inode);\n\t\tif (res)\n\t\t\tEXT4_ERROR_INODE(inode, \"Failed to mark inode dirty\");\n\t}\n\tres2 = ext4_journal_stop(handle);\n\n\tif (res == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\n\tif (!res)\n\t\tres = res2;\n\treturn res;\n}\n\nstatic bool ext4_dummy_context(struct inode *inode)\n{\n\treturn DUMMY_ENCRYPTION_ENABLED(EXT4_SB(inode->i_sb));\n}\n\nstatic const struct fscrypt_operations ext4_cryptops = {\n\t.key_prefix\t\t= \"ext4:\",\n\t.get_context\t\t= ext4_get_context,\n\t.set_context\t\t= ext4_set_context,\n\t.dummy_context\t\t= ext4_dummy_context,\n\t.empty_dir\t\t= ext4_empty_dir,\n\t.max_namelen\t\t= EXT4_NAME_LEN,\n};\n#endif\n\n#ifdef CONFIG_QUOTA\nstatic const char * const quotatypes[] = INITQFNAMES;\n#define QTYPE2NAME(t) (quotatypes[t])\n\nstatic int ext4_write_dquot(struct dquot *dquot);\nstatic int ext4_acquire_dquot(struct dquot *dquot);\nstatic int ext4_release_dquot(struct dquot *dquot);\nstatic int ext4_mark_dquot_dirty(struct dquot *dquot);\nstatic int ext4_write_info(struct super_block *sb, int type);\nstatic int ext4_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t const struct path *path);\nstatic int ext4_quota_on_mount(struct super_block *sb, int type);\nstatic ssize_t ext4_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off);\nstatic ssize_t ext4_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off);\nstatic int ext4_quota_enable(struct super_block *sb, int type, int format_id,\n\t\t\t     unsigned int flags);\nstatic int ext4_enable_quotas(struct super_block *sb);\nstatic int ext4_get_next_id(struct super_block *sb, struct kqid *qid);\n\nstatic struct dquot **ext4_get_dquots(struct inode *inode)\n{\n\treturn EXT4_I(inode)->i_dquot;\n}\n\nstatic const struct dquot_operations ext4_quota_operations = {\n\t.get_reserved_space\t= ext4_get_reserved_space,\n\t.write_dquot\t\t= ext4_write_dquot,\n\t.acquire_dquot\t\t= ext4_acquire_dquot,\n\t.release_dquot\t\t= ext4_release_dquot,\n\t.mark_dirty\t\t= ext4_mark_dquot_dirty,\n\t.write_info\t\t= ext4_write_info,\n\t.alloc_dquot\t\t= dquot_alloc,\n\t.destroy_dquot\t\t= dquot_destroy,\n\t.get_projid\t\t= ext4_get_projid,\n\t.get_inode_usage\t= ext4_get_inode_usage,\n\t.get_next_id\t\t= ext4_get_next_id,\n};\n\nstatic const struct quotactl_ops ext4_qctl_operations = {\n\t.quota_on\t= ext4_quota_on,\n\t.quota_off\t= ext4_quota_off,\n\t.quota_sync\t= dquot_quota_sync,\n\t.get_state\t= dquot_get_state,\n\t.set_info\t= dquot_set_dqinfo,\n\t.get_dqblk\t= dquot_get_dqblk,\n\t.set_dqblk\t= dquot_set_dqblk,\n\t.get_nextdqblk\t= dquot_get_next_dqblk,\n};\n#endif\n\nstatic const struct super_operations ext4_sops = {\n\t.alloc_inode\t= ext4_alloc_inode,\n\t.free_inode\t= ext4_free_in_core_inode,\n\t.destroy_inode\t= ext4_destroy_inode,\n\t.write_inode\t= ext4_write_inode,\n\t.dirty_inode\t= ext4_dirty_inode,\n\t.drop_inode\t= ext4_drop_inode,\n\t.evict_inode\t= ext4_evict_inode,\n\t.put_super\t= ext4_put_super,\n\t.sync_fs\t= ext4_sync_fs,\n\t.freeze_fs\t= ext4_freeze,\n\t.unfreeze_fs\t= ext4_unfreeze,\n\t.statfs\t\t= ext4_statfs,\n\t.remount_fs\t= ext4_remount,\n\t.show_options\t= ext4_show_options,\n#ifdef CONFIG_QUOTA\n\t.quota_read\t= ext4_quota_read,\n\t.quota_write\t= ext4_quota_write,\n\t.get_dquots\t= ext4_get_dquots,\n#endif\n\t.bdev_try_to_free_page = bdev_try_to_free_page,\n};\n\nstatic const struct export_operations ext4_export_ops = {\n\t.fh_to_dentry = ext4_fh_to_dentry,\n\t.fh_to_parent = ext4_fh_to_parent,\n\t.get_parent = ext4_get_parent,\n\t.commit_metadata = ext4_nfs_commit_metadata,\n};\n\nenum {\n\tOpt_bsd_df, Opt_minix_df, Opt_grpid, Opt_nogrpid,\n\tOpt_resgid, Opt_resuid, Opt_sb, Opt_err_cont, Opt_err_panic, Opt_err_ro,\n\tOpt_nouid32, Opt_debug, Opt_removed,\n\tOpt_user_xattr, Opt_nouser_xattr, Opt_acl, Opt_noacl,\n\tOpt_auto_da_alloc, Opt_noauto_da_alloc, Opt_noload,\n\tOpt_commit, Opt_min_batch_time, Opt_max_batch_time, Opt_journal_dev,\n\tOpt_journal_path, Opt_journal_checksum, Opt_journal_async_commit,\n\tOpt_abort, Opt_data_journal, Opt_data_ordered, Opt_data_writeback,\n\tOpt_data_err_abort, Opt_data_err_ignore, Opt_test_dummy_encryption,\n\tOpt_usrjquota, Opt_grpjquota, Opt_offusrjquota, Opt_offgrpjquota,\n\tOpt_jqfmt_vfsold, Opt_jqfmt_vfsv0, Opt_jqfmt_vfsv1, Opt_quota,\n\tOpt_noquota, Opt_barrier, Opt_nobarrier, Opt_err,\n\tOpt_usrquota, Opt_grpquota, Opt_prjquota, Opt_i_version, Opt_dax,\n\tOpt_stripe, Opt_delalloc, Opt_nodelalloc, Opt_warn_on_error,\n\tOpt_nowarn_on_error, Opt_mblk_io_submit,\n\tOpt_lazytime, Opt_nolazytime, Opt_debug_want_extra_isize,\n\tOpt_nomblk_io_submit, Opt_block_validity, Opt_noblock_validity,\n\tOpt_inode_readahead_blks, Opt_journal_ioprio,\n\tOpt_dioread_nolock, Opt_dioread_lock,\n\tOpt_discard, Opt_nodiscard, Opt_init_itable, Opt_noinit_itable,\n\tOpt_max_dir_size_kb, Opt_nojournal_checksum, Opt_nombcache,\n};\n\nstatic const match_table_t tokens = {\n\t{Opt_bsd_df, \"bsddf\"},\n\t{Opt_minix_df, \"minixdf\"},\n\t{Opt_grpid, \"grpid\"},\n\t{Opt_grpid, \"bsdgroups\"},\n\t{Opt_nogrpid, \"nogrpid\"},\n\t{Opt_nogrpid, \"sysvgroups\"},\n\t{Opt_resgid, \"resgid=%u\"},\n\t{Opt_resuid, \"resuid=%u\"},\n\t{Opt_sb, \"sb=%u\"},\n\t{Opt_err_cont, \"errors=continue\"},\n\t{Opt_err_panic, \"errors=panic\"},\n\t{Opt_err_ro, \"errors=remount-ro\"},\n\t{Opt_nouid32, \"nouid32\"},\n\t{Opt_debug, \"debug\"},\n\t{Opt_removed, \"oldalloc\"},\n\t{Opt_removed, \"orlov\"},\n\t{Opt_user_xattr, \"user_xattr\"},\n\t{Opt_nouser_xattr, \"nouser_xattr\"},\n\t{Opt_acl, \"acl\"},\n\t{Opt_noacl, \"noacl\"},\n\t{Opt_noload, \"norecovery\"},\n\t{Opt_noload, \"noload\"},\n\t{Opt_removed, \"nobh\"},\n\t{Opt_removed, \"bh\"},\n\t{Opt_commit, \"commit=%u\"},\n\t{Opt_min_batch_time, \"min_batch_time=%u\"},\n\t{Opt_max_batch_time, \"max_batch_time=%u\"},\n\t{Opt_journal_dev, \"journal_dev=%u\"},\n\t{Opt_journal_path, \"journal_path=%s\"},\n\t{Opt_journal_checksum, \"journal_checksum\"},\n\t{Opt_nojournal_checksum, \"nojournal_checksum\"},\n\t{Opt_journal_async_commit, \"journal_async_commit\"},\n\t{Opt_abort, \"abort\"},\n\t{Opt_data_journal, \"data=journal\"},\n\t{Opt_data_ordered, \"data=ordered\"},\n\t{Opt_data_writeback, \"data=writeback\"},\n\t{Opt_data_err_abort, \"data_err=abort\"},\n\t{Opt_data_err_ignore, \"data_err=ignore\"},\n\t{Opt_offusrjquota, \"usrjquota=\"},\n\t{Opt_usrjquota, \"usrjquota=%s\"},\n\t{Opt_offgrpjquota, \"grpjquota=\"},\n\t{Opt_grpjquota, \"grpjquota=%s\"},\n\t{Opt_jqfmt_vfsold, \"jqfmt=vfsold\"},\n\t{Opt_jqfmt_vfsv0, \"jqfmt=vfsv0\"},\n\t{Opt_jqfmt_vfsv1, \"jqfmt=vfsv1\"},\n\t{Opt_grpquota, \"grpquota\"},\n\t{Opt_noquota, \"noquota\"},\n\t{Opt_quota, \"quota\"},\n\t{Opt_usrquota, \"usrquota\"},\n\t{Opt_prjquota, \"prjquota\"},\n\t{Opt_barrier, \"barrier=%u\"},\n\t{Opt_barrier, \"barrier\"},\n\t{Opt_nobarrier, \"nobarrier\"},\n\t{Opt_i_version, \"i_version\"},\n\t{Opt_dax, \"dax\"},\n\t{Opt_stripe, \"stripe=%u\"},\n\t{Opt_delalloc, \"delalloc\"},\n\t{Opt_warn_on_error, \"warn_on_error\"},\n\t{Opt_nowarn_on_error, \"nowarn_on_error\"},\n\t{Opt_lazytime, \"lazytime\"},\n\t{Opt_nolazytime, \"nolazytime\"},\n\t{Opt_debug_want_extra_isize, \"debug_want_extra_isize=%u\"},\n\t{Opt_nodelalloc, \"nodelalloc\"},\n\t{Opt_removed, \"mblk_io_submit\"},\n\t{Opt_removed, \"nomblk_io_submit\"},\n\t{Opt_block_validity, \"block_validity\"},\n\t{Opt_noblock_validity, \"noblock_validity\"},\n\t{Opt_inode_readahead_blks, \"inode_readahead_blks=%u\"},\n\t{Opt_journal_ioprio, \"journal_ioprio=%u\"},\n\t{Opt_auto_da_alloc, \"auto_da_alloc=%u\"},\n\t{Opt_auto_da_alloc, \"auto_da_alloc\"},\n\t{Opt_noauto_da_alloc, \"noauto_da_alloc\"},\n\t{Opt_dioread_nolock, \"dioread_nolock\"},\n\t{Opt_dioread_lock, \"dioread_lock\"},\n\t{Opt_discard, \"discard\"},\n\t{Opt_nodiscard, \"nodiscard\"},\n\t{Opt_init_itable, \"init_itable=%u\"},\n\t{Opt_init_itable, \"init_itable\"},\n\t{Opt_noinit_itable, \"noinit_itable\"},\n\t{Opt_max_dir_size_kb, \"max_dir_size_kb=%u\"},\n\t{Opt_test_dummy_encryption, \"test_dummy_encryption\"},\n\t{Opt_nombcache, \"nombcache\"},\n\t{Opt_nombcache, \"no_mbcache\"},\t/* for backward compatibility */\n\t{Opt_removed, \"check=none\"},\t/* mount option from ext2/3 */\n\t{Opt_removed, \"nocheck\"},\t/* mount option from ext2/3 */\n\t{Opt_removed, \"reservation\"},\t/* mount option from ext2/3 */\n\t{Opt_removed, \"noreservation\"}, /* mount option from ext2/3 */\n\t{Opt_removed, \"journal=%u\"},\t/* mount option from ext2/3 */\n\t{Opt_err, NULL},\n};\n\nstatic ext4_fsblk_t get_sb_block(void **data)\n{\n\text4_fsblk_t\tsb_block;\n\tchar\t\t*options = (char *) *data;\n\n\tif (!options || strncmp(options, \"sb=\", 3) != 0)\n\t\treturn 1;\t/* Default location */\n\n\toptions += 3;\n\t/* TODO: use simple_strtoll with >32bit ext4 */\n\tsb_block = simple_strtoul(options, &options, 0);\n\tif (*options && *options != ',') {\n\t\tprintk(KERN_ERR \"EXT4-fs: Invalid sb specification: %s\\n\",\n\t\t       (char *) *data);\n\t\treturn 1;\n\t}\n\tif (*options == ',')\n\t\toptions++;\n\t*data = (void *) options;\n\n\treturn sb_block;\n}\n\n#define DEFAULT_JOURNAL_IOPRIO (IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, 3))\nstatic const char deprecated_msg[] =\n\t\"Mount option \\\"%s\\\" will be removed by %s\\n\"\n\t\"Contact linux-ext4@vger.kernel.org if you think we should keep it.\\n\";\n\n#ifdef CONFIG_QUOTA\nstatic int set_qf_name(struct super_block *sb, int qtype, substring_t *args)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *qname, *old_qname = get_qf_name(sb, sbi, qtype);\n\tint ret = -1;\n\n\tif (sb_any_quota_loaded(sb) && !old_qname) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Cannot change journaled \"\n\t\t\t\"quota options when quota turned on\");\n\t\treturn -1;\n\t}\n\tif (ext4_has_feature_quota(sb)) {\n\t\text4_msg(sb, KERN_INFO, \"Journaled quota options \"\n\t\t\t \"ignored when QUOTA feature is enabled\");\n\t\treturn 1;\n\t}\n\tqname = match_strdup(args);\n\tif (!qname) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Not enough memory for storing quotafile name\");\n\t\treturn -1;\n\t}\n\tif (old_qname) {\n\t\tif (strcmp(old_qname, qname) == 0)\n\t\t\tret = 1;\n\t\telse\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"%s quota file already specified\",\n\t\t\t\t QTYPE2NAME(qtype));\n\t\tgoto errout;\n\t}\n\tif (strchr(qname, '/')) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"quotafile must be on filesystem root\");\n\t\tgoto errout;\n\t}\n\trcu_assign_pointer(sbi->s_qf_names[qtype], qname);\n\tset_opt(sb, QUOTA);\n\treturn 1;\nerrout:\n\tkfree(qname);\n\treturn ret;\n}\n\nstatic int clear_qf_name(struct super_block *sb, int qtype)\n{\n\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *old_qname = get_qf_name(sb, sbi, qtype);\n\n\tif (sb_any_quota_loaded(sb) && old_qname) {\n\t\text4_msg(sb, KERN_ERR, \"Cannot change journaled quota options\"\n\t\t\t\" when quota turned on\");\n\t\treturn -1;\n\t}\n\trcu_assign_pointer(sbi->s_qf_names[qtype], NULL);\n\tsynchronize_rcu();\n\tkfree(old_qname);\n\treturn 1;\n}\n#endif\n\n#define MOPT_SET\t0x0001\n#define MOPT_CLEAR\t0x0002\n#define MOPT_NOSUPPORT\t0x0004\n#define MOPT_EXPLICIT\t0x0008\n#define MOPT_CLEAR_ERR\t0x0010\n#define MOPT_GTE0\t0x0020\n#ifdef CONFIG_QUOTA\n#define MOPT_Q\t\t0\n#define MOPT_QFMT\t0x0040\n#else\n#define MOPT_Q\t\tMOPT_NOSUPPORT\n#define MOPT_QFMT\tMOPT_NOSUPPORT\n#endif\n#define MOPT_DATAJ\t0x0080\n#define MOPT_NO_EXT2\t0x0100\n#define MOPT_NO_EXT3\t0x0200\n#define MOPT_EXT4_ONLY\t(MOPT_NO_EXT2 | MOPT_NO_EXT3)\n#define MOPT_STRING\t0x0400\n\nstatic const struct mount_opts {\n\tint\ttoken;\n\tint\tmount_opt;\n\tint\tflags;\n} ext4_mount_opts[] = {\n\t{Opt_minix_df, EXT4_MOUNT_MINIX_DF, MOPT_SET},\n\t{Opt_bsd_df, EXT4_MOUNT_MINIX_DF, MOPT_CLEAR},\n\t{Opt_grpid, EXT4_MOUNT_GRPID, MOPT_SET},\n\t{Opt_nogrpid, EXT4_MOUNT_GRPID, MOPT_CLEAR},\n\t{Opt_block_validity, EXT4_MOUNT_BLOCK_VALIDITY, MOPT_SET},\n\t{Opt_noblock_validity, EXT4_MOUNT_BLOCK_VALIDITY, MOPT_CLEAR},\n\t{Opt_dioread_nolock, EXT4_MOUNT_DIOREAD_NOLOCK,\n\t MOPT_EXT4_ONLY | MOPT_SET},\n\t{Opt_dioread_lock, EXT4_MOUNT_DIOREAD_NOLOCK,\n\t MOPT_EXT4_ONLY | MOPT_CLEAR},\n\t{Opt_discard, EXT4_MOUNT_DISCARD, MOPT_SET},\n\t{Opt_nodiscard, EXT4_MOUNT_DISCARD, MOPT_CLEAR},\n\t{Opt_delalloc, EXT4_MOUNT_DELALLOC,\n\t MOPT_EXT4_ONLY | MOPT_SET | MOPT_EXPLICIT},\n\t{Opt_nodelalloc, EXT4_MOUNT_DELALLOC,\n\t MOPT_EXT4_ONLY | MOPT_CLEAR},\n\t{Opt_warn_on_error, EXT4_MOUNT_WARN_ON_ERROR, MOPT_SET},\n\t{Opt_nowarn_on_error, EXT4_MOUNT_WARN_ON_ERROR, MOPT_CLEAR},\n\t{Opt_nojournal_checksum, EXT4_MOUNT_JOURNAL_CHECKSUM,\n\t MOPT_EXT4_ONLY | MOPT_CLEAR},\n\t{Opt_journal_checksum, EXT4_MOUNT_JOURNAL_CHECKSUM,\n\t MOPT_EXT4_ONLY | MOPT_SET | MOPT_EXPLICIT},\n\t{Opt_journal_async_commit, (EXT4_MOUNT_JOURNAL_ASYNC_COMMIT |\n\t\t\t\t    EXT4_MOUNT_JOURNAL_CHECKSUM),\n\t MOPT_EXT4_ONLY | MOPT_SET | MOPT_EXPLICIT},\n\t{Opt_noload, EXT4_MOUNT_NOLOAD, MOPT_NO_EXT2 | MOPT_SET},\n\t{Opt_err_panic, EXT4_MOUNT_ERRORS_PANIC, MOPT_SET | MOPT_CLEAR_ERR},\n\t{Opt_err_ro, EXT4_MOUNT_ERRORS_RO, MOPT_SET | MOPT_CLEAR_ERR},\n\t{Opt_err_cont, EXT4_MOUNT_ERRORS_CONT, MOPT_SET | MOPT_CLEAR_ERR},\n\t{Opt_data_err_abort, EXT4_MOUNT_DATA_ERR_ABORT,\n\t MOPT_NO_EXT2},\n\t{Opt_data_err_ignore, EXT4_MOUNT_DATA_ERR_ABORT,\n\t MOPT_NO_EXT2},\n\t{Opt_barrier, EXT4_MOUNT_BARRIER, MOPT_SET},\n\t{Opt_nobarrier, EXT4_MOUNT_BARRIER, MOPT_CLEAR},\n\t{Opt_noauto_da_alloc, EXT4_MOUNT_NO_AUTO_DA_ALLOC, MOPT_SET},\n\t{Opt_auto_da_alloc, EXT4_MOUNT_NO_AUTO_DA_ALLOC, MOPT_CLEAR},\n\t{Opt_noinit_itable, EXT4_MOUNT_INIT_INODE_TABLE, MOPT_CLEAR},\n\t{Opt_commit, 0, MOPT_GTE0},\n\t{Opt_max_batch_time, 0, MOPT_GTE0},\n\t{Opt_min_batch_time, 0, MOPT_GTE0},\n\t{Opt_inode_readahead_blks, 0, MOPT_GTE0},\n\t{Opt_init_itable, 0, MOPT_GTE0},\n\t{Opt_dax, EXT4_MOUNT_DAX, MOPT_SET},\n\t{Opt_stripe, 0, MOPT_GTE0},\n\t{Opt_resuid, 0, MOPT_GTE0},\n\t{Opt_resgid, 0, MOPT_GTE0},\n\t{Opt_journal_dev, 0, MOPT_NO_EXT2 | MOPT_GTE0},\n\t{Opt_journal_path, 0, MOPT_NO_EXT2 | MOPT_STRING},\n\t{Opt_journal_ioprio, 0, MOPT_NO_EXT2 | MOPT_GTE0},\n\t{Opt_data_journal, EXT4_MOUNT_JOURNAL_DATA, MOPT_NO_EXT2 | MOPT_DATAJ},\n\t{Opt_data_ordered, EXT4_MOUNT_ORDERED_DATA, MOPT_NO_EXT2 | MOPT_DATAJ},\n\t{Opt_data_writeback, EXT4_MOUNT_WRITEBACK_DATA,\n\t MOPT_NO_EXT2 | MOPT_DATAJ},\n\t{Opt_user_xattr, EXT4_MOUNT_XATTR_USER, MOPT_SET},\n\t{Opt_nouser_xattr, EXT4_MOUNT_XATTR_USER, MOPT_CLEAR},\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\t{Opt_acl, EXT4_MOUNT_POSIX_ACL, MOPT_SET},\n\t{Opt_noacl, EXT4_MOUNT_POSIX_ACL, MOPT_CLEAR},\n#else\n\t{Opt_acl, 0, MOPT_NOSUPPORT},\n\t{Opt_noacl, 0, MOPT_NOSUPPORT},\n#endif\n\t{Opt_nouid32, EXT4_MOUNT_NO_UID32, MOPT_SET},\n\t{Opt_debug, EXT4_MOUNT_DEBUG, MOPT_SET},\n\t{Opt_debug_want_extra_isize, 0, MOPT_GTE0},\n\t{Opt_quota, EXT4_MOUNT_QUOTA | EXT4_MOUNT_USRQUOTA, MOPT_SET | MOPT_Q},\n\t{Opt_usrquota, EXT4_MOUNT_QUOTA | EXT4_MOUNT_USRQUOTA,\n\t\t\t\t\t\t\tMOPT_SET | MOPT_Q},\n\t{Opt_grpquota, EXT4_MOUNT_QUOTA | EXT4_MOUNT_GRPQUOTA,\n\t\t\t\t\t\t\tMOPT_SET | MOPT_Q},\n\t{Opt_prjquota, EXT4_MOUNT_QUOTA | EXT4_MOUNT_PRJQUOTA,\n\t\t\t\t\t\t\tMOPT_SET | MOPT_Q},\n\t{Opt_noquota, (EXT4_MOUNT_QUOTA | EXT4_MOUNT_USRQUOTA |\n\t\t       EXT4_MOUNT_GRPQUOTA | EXT4_MOUNT_PRJQUOTA),\n\t\t\t\t\t\t\tMOPT_CLEAR | MOPT_Q},\n\t{Opt_usrjquota, 0, MOPT_Q},\n\t{Opt_grpjquota, 0, MOPT_Q},\n\t{Opt_offusrjquota, 0, MOPT_Q},\n\t{Opt_offgrpjquota, 0, MOPT_Q},\n\t{Opt_jqfmt_vfsold, QFMT_VFS_OLD, MOPT_QFMT},\n\t{Opt_jqfmt_vfsv0, QFMT_VFS_V0, MOPT_QFMT},\n\t{Opt_jqfmt_vfsv1, QFMT_VFS_V1, MOPT_QFMT},\n\t{Opt_max_dir_size_kb, 0, MOPT_GTE0},\n\t{Opt_test_dummy_encryption, 0, MOPT_GTE0},\n\t{Opt_nombcache, EXT4_MOUNT_NO_MBCACHE, MOPT_SET},\n\t{Opt_err, 0, 0}\n};\n\n#ifdef CONFIG_UNICODE\nstatic const struct ext4_sb_encodings {\n\t__u16 magic;\n\tchar *name;\n\tchar *version;\n} ext4_sb_encoding_map[] = {\n\t{EXT4_ENC_UTF8_12_1, \"utf8\", \"12.1.0\"},\n};\n\nstatic int ext4_sb_read_encoding(const struct ext4_super_block *es,\n\t\t\t\t const struct ext4_sb_encodings **encoding,\n\t\t\t\t __u16 *flags)\n{\n\t__u16 magic = le16_to_cpu(es->s_encoding);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ext4_sb_encoding_map); i++)\n\t\tif (magic == ext4_sb_encoding_map[i].magic)\n\t\t\tbreak;\n\n\tif (i >= ARRAY_SIZE(ext4_sb_encoding_map))\n\t\treturn -EINVAL;\n\n\t*encoding = &ext4_sb_encoding_map[i];\n\t*flags = le16_to_cpu(es->s_encoding_flags);\n\n\treturn 0;\n}\n#endif\n\nstatic int handle_mount_opt(struct super_block *sb, char *opt, int token,\n\t\t\t    substring_t *args, unsigned long *journal_devnum,\n\t\t\t    unsigned int *journal_ioprio, int is_remount)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tconst struct mount_opts *m;\n\tkuid_t uid;\n\tkgid_t gid;\n\tint arg = 0;\n\n#ifdef CONFIG_QUOTA\n\tif (token == Opt_usrjquota)\n\t\treturn set_qf_name(sb, USRQUOTA, &args[0]);\n\telse if (token == Opt_grpjquota)\n\t\treturn set_qf_name(sb, GRPQUOTA, &args[0]);\n\telse if (token == Opt_offusrjquota)\n\t\treturn clear_qf_name(sb, USRQUOTA);\n\telse if (token == Opt_offgrpjquota)\n\t\treturn clear_qf_name(sb, GRPQUOTA);\n#endif\n\tswitch (token) {\n\tcase Opt_noacl:\n\tcase Opt_nouser_xattr:\n\t\text4_msg(sb, KERN_WARNING, deprecated_msg, opt, \"3.5\");\n\t\tbreak;\n\tcase Opt_sb:\n\t\treturn 1;\t/* handled by get_sb_block() */\n\tcase Opt_removed:\n\t\text4_msg(sb, KERN_WARNING, \"Ignoring removed %s option\", opt);\n\t\treturn 1;\n\tcase Opt_abort:\n\t\tsbi->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\treturn 1;\n\tcase Opt_i_version:\n\t\tsb->s_flags |= SB_I_VERSION;\n\t\treturn 1;\n\tcase Opt_lazytime:\n\t\tsb->s_flags |= SB_LAZYTIME;\n\t\treturn 1;\n\tcase Opt_nolazytime:\n\t\tsb->s_flags &= ~SB_LAZYTIME;\n\t\treturn 1;\n\t}\n\n\tfor (m = ext4_mount_opts; m->token != Opt_err; m++)\n\t\tif (token == m->token)\n\t\t\tbreak;\n\n\tif (m->token == Opt_err) {\n\t\text4_msg(sb, KERN_ERR, \"Unrecognized mount option \\\"%s\\\" \"\n\t\t\t \"or missing value\", opt);\n\t\treturn -1;\n\t}\n\n\tif ((m->flags & MOPT_NO_EXT2) && IS_EXT2_SB(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Mount option \\\"%s\\\" incompatible with ext2\", opt);\n\t\treturn -1;\n\t}\n\tif ((m->flags & MOPT_NO_EXT3) && IS_EXT3_SB(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Mount option \\\"%s\\\" incompatible with ext3\", opt);\n\t\treturn -1;\n\t}\n\n\tif (args->from && !(m->flags & MOPT_STRING) && match_int(args, &arg))\n\t\treturn -1;\n\tif (args->from && (m->flags & MOPT_GTE0) && (arg < 0))\n\t\treturn -1;\n\tif (m->flags & MOPT_EXPLICIT) {\n\t\tif (m->mount_opt & EXT4_MOUNT_DELALLOC) {\n\t\t\tset_opt2(sb, EXPLICIT_DELALLOC);\n\t\t} else if (m->mount_opt & EXT4_MOUNT_JOURNAL_CHECKSUM) {\n\t\t\tset_opt2(sb, EXPLICIT_JOURNAL_CHECKSUM);\n\t\t} else\n\t\t\treturn -1;\n\t}\n\tif (m->flags & MOPT_CLEAR_ERR)\n\t\tclear_opt(sb, ERRORS_MASK);\n\tif (token == Opt_noquota && sb_any_quota_loaded(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"Cannot change quota \"\n\t\t\t \"options when quota turned on\");\n\t\treturn -1;\n\t}\n\n\tif (m->flags & MOPT_NOSUPPORT) {\n\t\text4_msg(sb, KERN_ERR, \"%s option not supported\", opt);\n\t} else if (token == Opt_commit) {\n\t\tif (arg == 0)\n\t\t\targ = JBD2_DEFAULT_MAX_COMMIT_AGE;\n\t\telse if (arg > INT_MAX / HZ) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Invalid commit interval %d, \"\n\t\t\t\t \"must be smaller than %d\",\n\t\t\t\t arg, INT_MAX / HZ);\n\t\t\treturn -1;\n\t\t}\n\t\tsbi->s_commit_interval = HZ * arg;\n\t} else if (token == Opt_debug_want_extra_isize) {\n\t\tsbi->s_want_extra_isize = arg;\n\t} else if (token == Opt_max_batch_time) {\n\t\tsbi->s_max_batch_time = arg;\n\t} else if (token == Opt_min_batch_time) {\n\t\tsbi->s_min_batch_time = arg;\n\t} else if (token == Opt_inode_readahead_blks) {\n\t\tif (arg && (arg > (1 << 30) || !is_power_of_2(arg))) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"EXT4-fs: inode_readahead_blks must be \"\n\t\t\t\t \"0 or a power of 2 smaller than 2^31\");\n\t\t\treturn -1;\n\t\t}\n\t\tsbi->s_inode_readahead_blks = arg;\n\t} else if (token == Opt_init_itable) {\n\t\tset_opt(sb, INIT_INODE_TABLE);\n\t\tif (!args->from)\n\t\t\targ = EXT4_DEF_LI_WAIT_MULT;\n\t\tsbi->s_li_wait_mult = arg;\n\t} else if (token == Opt_max_dir_size_kb) {\n\t\tsbi->s_max_dir_size_kb = arg;\n\t} else if (token == Opt_stripe) {\n\t\tsbi->s_stripe = arg;\n\t} else if (token == Opt_resuid) {\n\t\tuid = make_kuid(current_user_ns(), arg);\n\t\tif (!uid_valid(uid)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Invalid uid value %d\", arg);\n\t\t\treturn -1;\n\t\t}\n\t\tsbi->s_resuid = uid;\n\t} else if (token == Opt_resgid) {\n\t\tgid = make_kgid(current_user_ns(), arg);\n\t\tif (!gid_valid(gid)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Invalid gid value %d\", arg);\n\t\t\treturn -1;\n\t\t}\n\t\tsbi->s_resgid = gid;\n\t} else if (token == Opt_journal_dev) {\n\t\tif (is_remount) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Cannot specify journal on remount\");\n\t\t\treturn -1;\n\t\t}\n\t\t*journal_devnum = arg;\n\t} else if (token == Opt_journal_path) {\n\t\tchar *journal_path;\n\t\tstruct inode *journal_inode;\n\t\tstruct path path;\n\t\tint error;\n\n\t\tif (is_remount) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Cannot specify journal on remount\");\n\t\t\treturn -1;\n\t\t}\n\t\tjournal_path = match_strdup(&args[0]);\n\t\tif (!journal_path) {\n\t\t\text4_msg(sb, KERN_ERR, \"error: could not dup \"\n\t\t\t\t\"journal device string\");\n\t\t\treturn -1;\n\t\t}\n\n\t\terror = kern_path(journal_path, LOOKUP_FOLLOW, &path);\n\t\tif (error) {\n\t\t\text4_msg(sb, KERN_ERR, \"error: could not find \"\n\t\t\t\t\"journal device path: error %d\", error);\n\t\t\tkfree(journal_path);\n\t\t\treturn -1;\n\t\t}\n\n\t\tjournal_inode = d_inode(path.dentry);\n\t\tif (!S_ISBLK(journal_inode->i_mode)) {\n\t\t\text4_msg(sb, KERN_ERR, \"error: journal path %s \"\n\t\t\t\t\"is not a block device\", journal_path);\n\t\t\tpath_put(&path);\n\t\t\tkfree(journal_path);\n\t\t\treturn -1;\n\t\t}\n\n\t\t*journal_devnum = new_encode_dev(journal_inode->i_rdev);\n\t\tpath_put(&path);\n\t\tkfree(journal_path);\n\t} else if (token == Opt_journal_ioprio) {\n\t\tif (arg > 7) {\n\t\t\text4_msg(sb, KERN_ERR, \"Invalid journal IO priority\"\n\t\t\t\t \" (must be 0-7)\");\n\t\t\treturn -1;\n\t\t}\n\t\t*journal_ioprio =\n\t\t\tIOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, arg);\n\t} else if (token == Opt_test_dummy_encryption) {\n#ifdef CONFIG_FS_ENCRYPTION\n\t\tsbi->s_mount_flags |= EXT4_MF_TEST_DUMMY_ENCRYPTION;\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"Test dummy encryption mode enabled\");\n#else\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"Test dummy encryption mount option ignored\");\n#endif\n\t} else if (m->flags & MOPT_DATAJ) {\n\t\tif (is_remount) {\n\t\t\tif (!sbi->s_journal)\n\t\t\t\text4_msg(sb, KERN_WARNING, \"Remounting file system with no journal so ignoring journalled data option\");\n\t\t\telse if (test_opt(sb, DATA_FLAGS) != m->mount_opt) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t \"Cannot change data mode on remount\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t} else {\n\t\t\tclear_opt(sb, DATA_FLAGS);\n\t\t\tsbi->s_mount_opt |= m->mount_opt;\n\t\t}\n#ifdef CONFIG_QUOTA\n\t} else if (m->flags & MOPT_QFMT) {\n\t\tif (sb_any_quota_loaded(sb) &&\n\t\t    sbi->s_jquota_fmt != m->mount_opt) {\n\t\t\text4_msg(sb, KERN_ERR, \"Cannot change journaled \"\n\t\t\t\t \"quota options when quota turned on\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (ext4_has_feature_quota(sb)) {\n\t\t\text4_msg(sb, KERN_INFO,\n\t\t\t\t \"Quota format mount options ignored \"\n\t\t\t\t \"when QUOTA feature is enabled\");\n\t\t\treturn 1;\n\t\t}\n\t\tsbi->s_jquota_fmt = m->mount_opt;\n#endif\n\t} else if (token == Opt_dax) {\n#ifdef CONFIG_FS_DAX\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\"DAX enabled. Warning: EXPERIMENTAL, use at your own risk\");\n\t\tsbi->s_mount_opt |= m->mount_opt;\n#else\n\t\text4_msg(sb, KERN_INFO, \"dax option not supported\");\n\t\treturn -1;\n#endif\n\t} else if (token == Opt_data_err_abort) {\n\t\tsbi->s_mount_opt |= m->mount_opt;\n\t} else if (token == Opt_data_err_ignore) {\n\t\tsbi->s_mount_opt &= ~m->mount_opt;\n\t} else {\n\t\tif (!args->from)\n\t\t\targ = 1;\n\t\tif (m->flags & MOPT_CLEAR)\n\t\t\targ = !arg;\n\t\telse if (unlikely(!(m->flags & MOPT_SET))) {\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"buggy handling of option %s\", opt);\n\t\t\tWARN_ON(1);\n\t\t\treturn -1;\n\t\t}\n\t\tif (arg != 0)\n\t\t\tsbi->s_mount_opt |= m->mount_opt;\n\t\telse\n\t\t\tsbi->s_mount_opt &= ~m->mount_opt;\n\t}\n\treturn 1;\n}\n\nstatic int parse_options(char *options, struct super_block *sb,\n\t\t\t unsigned long *journal_devnum,\n\t\t\t unsigned int *journal_ioprio,\n\t\t\t int is_remount)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *p, __maybe_unused *usr_qf_name, __maybe_unused *grp_qf_name;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint token;\n\n\tif (!options)\n\t\treturn 1;\n\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tif (!*p)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Initialize args struct so we know whether arg was\n\t\t * found; some options take optional arguments.\n\t\t */\n\t\targs[0].to = args[0].from = NULL;\n\t\ttoken = match_token(p, tokens, args);\n\t\tif (handle_mount_opt(sb, p, token, args, journal_devnum,\n\t\t\t\t     journal_ioprio, is_remount) < 0)\n\t\t\treturn 0;\n\t}\n#ifdef CONFIG_QUOTA\n\t/*\n\t * We do the test below only for project quotas. 'usrquota' and\n\t * 'grpquota' mount options are allowed even without quota feature\n\t * to support legacy quotas in quota files.\n\t */\n\tif (test_opt(sb, PRJQUOTA) && !ext4_has_feature_project(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"Project quota feature not enabled. \"\n\t\t\t \"Cannot enable project quota enforcement.\");\n\t\treturn 0;\n\t}\n\tusr_qf_name = get_qf_name(sb, sbi, USRQUOTA);\n\tgrp_qf_name = get_qf_name(sb, sbi, GRPQUOTA);\n\tif (usr_qf_name || grp_qf_name) {\n\t\tif (test_opt(sb, USRQUOTA) && usr_qf_name)\n\t\t\tclear_opt(sb, USRQUOTA);\n\n\t\tif (test_opt(sb, GRPQUOTA) && grp_qf_name)\n\t\t\tclear_opt(sb, GRPQUOTA);\n\n\t\tif (test_opt(sb, GRPQUOTA) || test_opt(sb, USRQUOTA)) {\n\t\t\text4_msg(sb, KERN_ERR, \"old and new quota \"\n\t\t\t\t\t\"format mixing\");\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!sbi->s_jquota_fmt) {\n\t\t\text4_msg(sb, KERN_ERR, \"journaled quota format \"\n\t\t\t\t\t\"not specified\");\n\t\t\treturn 0;\n\t\t}\n\t}\n#endif\n\treturn 1;\n}\n\nstatic inline void ext4_show_quota_options(struct seq_file *seq,\n\t\t\t\t\t   struct super_block *sb)\n{\n#if defined(CONFIG_QUOTA)\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *usr_qf_name, *grp_qf_name;\n\n\tif (sbi->s_jquota_fmt) {\n\t\tchar *fmtname = \"\";\n\n\t\tswitch (sbi->s_jquota_fmt) {\n\t\tcase QFMT_VFS_OLD:\n\t\t\tfmtname = \"vfsold\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V0:\n\t\t\tfmtname = \"vfsv0\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V1:\n\t\t\tfmtname = \"vfsv1\";\n\t\t\tbreak;\n\t\t}\n\t\tseq_printf(seq, \",jqfmt=%s\", fmtname);\n\t}\n\n\trcu_read_lock();\n\tusr_qf_name = rcu_dereference(sbi->s_qf_names[USRQUOTA]);\n\tgrp_qf_name = rcu_dereference(sbi->s_qf_names[GRPQUOTA]);\n\tif (usr_qf_name)\n\t\tseq_show_option(seq, \"usrjquota\", usr_qf_name);\n\tif (grp_qf_name)\n\t\tseq_show_option(seq, \"grpjquota\", grp_qf_name);\n\trcu_read_unlock();\n#endif\n}\n\nstatic const char *token2str(int token)\n{\n\tconst struct match_token *t;\n\n\tfor (t = tokens; t->token != Opt_err; t++)\n\t\tif (t->token == token && !strchr(t->pattern, '='))\n\t\t\tbreak;\n\treturn t->pattern;\n}\n\n/*\n * Show an option if\n *  - it's set to a non-default value OR\n *  - if the per-sb default is different from the global default\n */\nstatic int _ext4_show_options(struct seq_file *seq, struct super_block *sb,\n\t\t\t      int nodefs)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tint def_errors, def_mount_opt = sbi->s_def_mount_opt;\n\tconst struct mount_opts *m;\n\tchar sep = nodefs ? '\\n' : ',';\n\n#define SEQ_OPTS_PUTS(str) seq_printf(seq, \"%c\" str, sep)\n#define SEQ_OPTS_PRINT(str, arg) seq_printf(seq, \"%c\" str, sep, arg)\n\n\tif (sbi->s_sb_block != 1)\n\t\tSEQ_OPTS_PRINT(\"sb=%llu\", sbi->s_sb_block);\n\n\tfor (m = ext4_mount_opts; m->token != Opt_err; m++) {\n\t\tint want_set = m->flags & MOPT_SET;\n\t\tif (((m->flags & (MOPT_SET|MOPT_CLEAR)) == 0) ||\n\t\t    (m->flags & MOPT_CLEAR_ERR))\n\t\t\tcontinue;\n\t\tif (!nodefs && !(m->mount_opt & (sbi->s_mount_opt ^ def_mount_opt)))\n\t\t\tcontinue; /* skip if same as the default */\n\t\tif ((want_set &&\n\t\t     (sbi->s_mount_opt & m->mount_opt) != m->mount_opt) ||\n\t\t    (!want_set && (sbi->s_mount_opt & m->mount_opt)))\n\t\t\tcontinue; /* select Opt_noFoo vs Opt_Foo */\n\t\tSEQ_OPTS_PRINT(\"%s\", token2str(m->token));\n\t}\n\n\tif (nodefs || !uid_eq(sbi->s_resuid, make_kuid(&init_user_ns, EXT4_DEF_RESUID)) ||\n\t    le16_to_cpu(es->s_def_resuid) != EXT4_DEF_RESUID)\n\t\tSEQ_OPTS_PRINT(\"resuid=%u\",\n\t\t\t\tfrom_kuid_munged(&init_user_ns, sbi->s_resuid));\n\tif (nodefs || !gid_eq(sbi->s_resgid, make_kgid(&init_user_ns, EXT4_DEF_RESGID)) ||\n\t    le16_to_cpu(es->s_def_resgid) != EXT4_DEF_RESGID)\n\t\tSEQ_OPTS_PRINT(\"resgid=%u\",\n\t\t\t\tfrom_kgid_munged(&init_user_ns, sbi->s_resgid));\n\tdef_errors = nodefs ? -1 : le16_to_cpu(es->s_errors);\n\tif (test_opt(sb, ERRORS_RO) && def_errors != EXT4_ERRORS_RO)\n\t\tSEQ_OPTS_PUTS(\"errors=remount-ro\");\n\tif (test_opt(sb, ERRORS_CONT) && def_errors != EXT4_ERRORS_CONTINUE)\n\t\tSEQ_OPTS_PUTS(\"errors=continue\");\n\tif (test_opt(sb, ERRORS_PANIC) && def_errors != EXT4_ERRORS_PANIC)\n\t\tSEQ_OPTS_PUTS(\"errors=panic\");\n\tif (nodefs || sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ)\n\t\tSEQ_OPTS_PRINT(\"commit=%lu\", sbi->s_commit_interval / HZ);\n\tif (nodefs || sbi->s_min_batch_time != EXT4_DEF_MIN_BATCH_TIME)\n\t\tSEQ_OPTS_PRINT(\"min_batch_time=%u\", sbi->s_min_batch_time);\n\tif (nodefs || sbi->s_max_batch_time != EXT4_DEF_MAX_BATCH_TIME)\n\t\tSEQ_OPTS_PRINT(\"max_batch_time=%u\", sbi->s_max_batch_time);\n\tif (sb->s_flags & SB_I_VERSION)\n\t\tSEQ_OPTS_PUTS(\"i_version\");\n\tif (nodefs || sbi->s_stripe)\n\t\tSEQ_OPTS_PRINT(\"stripe=%lu\", sbi->s_stripe);\n\tif (nodefs || EXT4_MOUNT_DATA_FLAGS &\n\t\t\t(sbi->s_mount_opt ^ def_mount_opt)) {\n\t\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\t\tSEQ_OPTS_PUTS(\"data=journal\");\n\t\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\t\tSEQ_OPTS_PUTS(\"data=ordered\");\n\t\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA)\n\t\t\tSEQ_OPTS_PUTS(\"data=writeback\");\n\t}\n\tif (nodefs ||\n\t    sbi->s_inode_readahead_blks != EXT4_DEF_INODE_READAHEAD_BLKS)\n\t\tSEQ_OPTS_PRINT(\"inode_readahead_blks=%u\",\n\t\t\t       sbi->s_inode_readahead_blks);\n\n\tif (test_opt(sb, INIT_INODE_TABLE) && (nodefs ||\n\t\t       (sbi->s_li_wait_mult != EXT4_DEF_LI_WAIT_MULT)))\n\t\tSEQ_OPTS_PRINT(\"init_itable=%u\", sbi->s_li_wait_mult);\n\tif (nodefs || sbi->s_max_dir_size_kb)\n\t\tSEQ_OPTS_PRINT(\"max_dir_size_kb=%u\", sbi->s_max_dir_size_kb);\n\tif (test_opt(sb, DATA_ERR_ABORT))\n\t\tSEQ_OPTS_PUTS(\"data_err=abort\");\n\tif (DUMMY_ENCRYPTION_ENABLED(sbi))\n\t\tSEQ_OPTS_PUTS(\"test_dummy_encryption\");\n\n\text4_show_quota_options(seq, sb);\n\treturn 0;\n}\n\nstatic int ext4_show_options(struct seq_file *seq, struct dentry *root)\n{\n\treturn _ext4_show_options(seq, root->d_sb, 0);\n}\n\nint ext4_seq_options_show(struct seq_file *seq, void *offset)\n{\n\tstruct super_block *sb = seq->private;\n\tint rc;\n\n\tseq_puts(seq, sb_rdonly(sb) ? \"ro\" : \"rw\");\n\trc = _ext4_show_options(seq, sb, 1);\n\tseq_puts(seq, \"\\n\");\n\treturn rc;\n}\n\nstatic int ext4_setup_super(struct super_block *sb, struct ext4_super_block *es,\n\t\t\t    int read_only)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tint err = 0;\n\n\tif (le32_to_cpu(es->s_rev_level) > EXT4_MAX_SUPP_REV) {\n\t\text4_msg(sb, KERN_ERR, \"revision level too high, \"\n\t\t\t \"forcing read-only mode\");\n\t\terr = -EROFS;\n\t}\n\tif (read_only)\n\t\tgoto done;\n\tif (!(sbi->s_mount_state & EXT4_VALID_FS))\n\t\text4_msg(sb, KERN_WARNING, \"warning: mounting unchecked fs, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if (sbi->s_mount_state & EXT4_ERROR_FS)\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: mounting fs with errors, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if ((__s16) le16_to_cpu(es->s_max_mnt_count) > 0 &&\n\t\t le16_to_cpu(es->s_mnt_count) >=\n\t\t (unsigned short) (__s16) le16_to_cpu(es->s_max_mnt_count))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: maximal mount count reached, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if (le32_to_cpu(es->s_checkinterval) &&\n\t\t (ext4_get_tstamp(es, s_lastcheck) +\n\t\t  le32_to_cpu(es->s_checkinterval) <= ktime_get_real_seconds()))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: checktime reached, \"\n\t\t\t \"running e2fsck is recommended\");\n\tif (!sbi->s_journal)\n\t\tes->s_state &= cpu_to_le16(~EXT4_VALID_FS);\n\tif (!(__s16) le16_to_cpu(es->s_max_mnt_count))\n\t\tes->s_max_mnt_count = cpu_to_le16(EXT4_DFL_MAX_MNT_COUNT);\n\tle16_add_cpu(&es->s_mnt_count, 1);\n\text4_update_tstamp(es, s_mtime);\n\tif (sbi->s_journal)\n\t\text4_set_feature_journal_needs_recovery(sb);\n\n\terr = ext4_commit_super(sb, 1);\ndone:\n\tif (test_opt(sb, DEBUG))\n\t\tprintk(KERN_INFO \"[EXT4 FS bs=%lu, gc=%u, \"\n\t\t\t\t\"bpg=%lu, ipg=%lu, mo=%04x, mo2=%04x]\\n\",\n\t\t\tsb->s_blocksize,\n\t\t\tsbi->s_groups_count,\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb),\n\t\t\tEXT4_INODES_PER_GROUP(sb),\n\t\t\tsbi->s_mount_opt, sbi->s_mount_opt2);\n\n\tcleancache_init_fs(sb);\n\treturn err;\n}\n\nint ext4_alloc_flex_bg_array(struct super_block *sb, ext4_group_t ngroup)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct flex_groups *new_groups;\n\tint size;\n\n\tif (!sbi->s_log_groups_per_flex)\n\t\treturn 0;\n\n\tsize = ext4_flex_group(sbi, ngroup - 1) + 1;\n\tif (size <= sbi->s_flex_groups_allocated)\n\t\treturn 0;\n\n\tsize = roundup_pow_of_two(size * sizeof(struct flex_groups));\n\tnew_groups = kvzalloc(size, GFP_KERNEL);\n\tif (!new_groups) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for %d flex groups\",\n\t\t\t size / (int) sizeof(struct flex_groups));\n\t\treturn -ENOMEM;\n\t}\n\n\tif (sbi->s_flex_groups) {\n\t\tmemcpy(new_groups, sbi->s_flex_groups,\n\t\t       (sbi->s_flex_groups_allocated *\n\t\t\tsizeof(struct flex_groups)));\n\t\tkvfree(sbi->s_flex_groups);\n\t}\n\tsbi->s_flex_groups = new_groups;\n\tsbi->s_flex_groups_allocated = size / sizeof(struct flex_groups);\n\treturn 0;\n}\n\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group;\n\tint i, err;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tif (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\n\terr = ext4_alloc_flex_bg_array(sb, sbi->s_groups_count);\n\tif (err)\n\t\tgoto failed;\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic64_add(ext4_free_group_clusters(sb, gdp),\n\t\t\t     &sbi->s_flex_groups[flex_group].free_clusters);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}\n\nstatic __le16 ext4_group_desc_csum(struct super_block *sb, __u32 block_group,\n\t\t\t\t   struct ext4_group_desc *gdp)\n{\n\tint offset = offsetof(struct ext4_group_desc, bg_checksum);\n\t__u16 crc = 0;\n\t__le32 le_group = cpu_to_le32(block_group);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (ext4_has_metadata_csum(sbi->s_sb)) {\n\t\t/* Use new metadata_csum algorithm */\n\t\t__u32 csum32;\n\t\t__u16 dummy_csum = 0;\n\n\t\tcsum32 = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&le_group,\n\t\t\t\t     sizeof(le_group));\n\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)gdp, offset);\n\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)&dummy_csum,\n\t\t\t\t     sizeof(dummy_csum));\n\t\toffset += sizeof(dummy_csum);\n\t\tif (offset < sbi->s_desc_size)\n\t\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)gdp + offset,\n\t\t\t\t\t     sbi->s_desc_size - offset);\n\n\t\tcrc = csum32 & 0xFFFF;\n\t\tgoto out;\n\t}\n\n\t/* old crc16 code */\n\tif (!ext4_has_feature_gdt_csum(sb))\n\t\treturn 0;\n\n\tcrc = crc16(~0, sbi->s_es->s_uuid, sizeof(sbi->s_es->s_uuid));\n\tcrc = crc16(crc, (__u8 *)&le_group, sizeof(le_group));\n\tcrc = crc16(crc, (__u8 *)gdp, offset);\n\toffset += sizeof(gdp->bg_checksum); /* skip checksum */\n\t/* for checksum of struct ext4_group_desc do the rest...*/\n\tif (ext4_has_feature_64bit(sb) &&\n\t    offset < le16_to_cpu(sbi->s_es->s_desc_size))\n\t\tcrc = crc16(crc, (__u8 *)gdp + offset,\n\t\t\t    le16_to_cpu(sbi->s_es->s_desc_size) -\n\t\t\t\toffset);\n\nout:\n\treturn cpu_to_le16(crc);\n}\n\nint ext4_group_desc_csum_verify(struct super_block *sb, __u32 block_group,\n\t\t\t\tstruct ext4_group_desc *gdp)\n{\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (gdp->bg_checksum != ext4_group_desc_csum(sb, block_group, gdp)))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nvoid ext4_group_desc_csum_set(struct super_block *sb, __u32 block_group,\n\t\t\t      struct ext4_group_desc *gdp)\n{\n\tif (!ext4_has_group_desc_csum(sb))\n\t\treturn;\n\tgdp->bg_checksum = ext4_group_desc_csum(sb, block_group, gdp);\n}\n\n/* Called at mount-time, super-block is locked */\nstatic int ext4_check_descriptors(struct super_block *sb,\n\t\t\t\t  ext4_fsblk_t sb_block,\n\t\t\t\t  ext4_group_t *first_not_zeroed)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n\text4_fsblk_t last_block;\n\text4_fsblk_t last_bg_block = sb_block + ext4_bg_num_gdb(sb, 0);\n\text4_fsblk_t block_bitmap;\n\text4_fsblk_t inode_bitmap;\n\text4_fsblk_t inode_table;\n\tint flexbg_flag = 0;\n\text4_group_t i, grp = sbi->s_groups_count;\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tflexbg_flag = 1;\n\n\text4_debug(\"Checking group descriptors\");\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tif (i == sbi->s_groups_count - 1 || flexbg_flag)\n\t\t\tlast_block = ext4_blocks_count(sbi->s_es) - 1;\n\t\telse\n\t\t\tlast_block = first_block +\n\t\t\t\t(EXT4_BLOCKS_PER_GROUP(sb) - 1);\n\n\t\tif ((grp == sbi->s_groups_count) &&\n\t\t   !(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tgrp = i;\n\n\t\tblock_bitmap = ext4_block_bitmap(sb, gdp);\n\t\tif (block_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap >= sb_block + 1 &&\n\t\t    block_bitmap <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Block bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, block_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_bitmap = ext4_inode_bitmap(sb, gdp);\n\t\tif (inode_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap >= sb_block + 1 &&\n\t\t    inode_bitmap <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_table = ext4_inode_table(sb, gdp);\n\t\tif (inode_table == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table >= sb_block + 1 &&\n\t\t    inode_table <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table < first_block ||\n\t\t    inode_table + sbi->s_itb_per_group - 1 > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode table for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_table);\n\t\t\treturn 0;\n\t\t}\n\t\text4_lock_group(sb, i);\n\t\tif (!ext4_group_desc_csum_verify(sb, i, gdp)) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Checksum for group %u failed (%u!=%u)\",\n\t\t\t\t i, le16_to_cpu(ext4_group_desc_csum(sb, i,\n\t\t\t\t     gdp)), le16_to_cpu(gdp->bg_checksum));\n\t\t\tif (!sb_rdonly(sb)) {\n\t\t\t\text4_unlock_group(sb, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tif (!flexbg_flag)\n\t\t\tfirst_block += EXT4_BLOCKS_PER_GROUP(sb);\n\t}\n\tif (NULL != first_not_zeroed)\n\t\t*first_not_zeroed = grp;\n\treturn 1;\n}\n\n/* ext4_orphan_cleanup() walks a singly-linked list of inodes (starting at\n * the superblock) which were deleted from all directories, but held open by\n * a process at the time of a crash.  We walk the list and try to delete these\n * inodes at recovery time (only with a read-write filesystem).\n *\n * In order to keep the orphan inode chain consistent during traversal (in\n * case of crash during recovery), we link each inode into the superblock\n * orphan list_head and handle it the same way as an inode deletion during\n * normal operation (which journals the operations for us).\n *\n * We only do an iget() and an iput() on each inode, which is very safe if we\n * accidentally point at an in-use or already deleted inode.  The worst that\n * can happen in this case is that we get a \"bit already cleared\" message from\n * ext4_free_inode().  The only reason we would point at a wrong inode is if\n * e2fsck was run on this filesystem, and it must have already done the orphan\n * inode cleanup for us, so we can safely abort without any further action.\n */\nstatic void ext4_orphan_cleanup(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es)\n{\n\tunsigned int s_flags = sb->s_flags;\n\tint ret, nr_orphans = 0, nr_truncates = 0;\n#ifdef CONFIG_QUOTA\n\tint quota_update = 0;\n\tint i;\n#endif\n\tif (!es->s_last_orphan) {\n\t\tjbd_debug(4, \"no orphan inodes to clean up\\n\");\n\t\treturn;\n\t}\n\n\tif (bdev_read_only(sb->s_bdev)) {\n\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\"unavailable, skipping orphan cleanup\");\n\t\treturn;\n\t}\n\n\t/* Check if feature set would not allow a r/w mount */\n\tif (!ext4_feature_set_ok(sb, 0)) {\n\t\text4_msg(sb, KERN_INFO, \"Skipping orphan cleanup due to \"\n\t\t\t \"unknown ROCOMPAT features\");\n\t\treturn;\n\t}\n\n\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\t/* don't clear list on RO mount w/ errors */\n\t\tif (es->s_last_orphan && !(s_flags & SB_RDONLY)) {\n\t\t\text4_msg(sb, KERN_INFO, \"Errors on filesystem, \"\n\t\t\t\t  \"clearing orphan list.\\n\");\n\t\t\tes->s_last_orphan = 0;\n\t\t}\n\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\treturn;\n\t}\n\n\tif (s_flags & SB_RDONLY) {\n\t\text4_msg(sb, KERN_INFO, \"orphan cleanup on readonly fs\");\n\t\tsb->s_flags &= ~SB_RDONLY;\n\t}\n#ifdef CONFIG_QUOTA\n\t/* Needed for iput() to work correctly and not trash data */\n\tsb->s_flags |= SB_ACTIVE;\n\n\t/*\n\t * Turn on quotas which were not enabled for read-only mounts if\n\t * filesystem has quota feature, so that they are updated correctly.\n\t */\n\tif (ext4_has_feature_quota(sb) && (s_flags & SB_RDONLY)) {\n\t\tint ret = ext4_enable_quotas(sb);\n\n\t\tif (!ret)\n\t\t\tquota_update = 1;\n\t\telse\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"Cannot turn on quotas: error %d\", ret);\n\t}\n\n\t/* Turn on journaled quotas used for old sytle */\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\tif (EXT4_SB(sb)->s_qf_names[i]) {\n\t\t\tint ret = ext4_quota_on_mount(sb, i);\n\n\t\t\tif (!ret)\n\t\t\t\tquota_update = 1;\n\t\t\telse\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"Cannot turn on journaled \"\n\t\t\t\t\t\"quota: type %d: error %d\", i, ret);\n\t\t}\n\t}\n#endif\n\n\twhile (es->s_last_orphan) {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * We may have encountered an error during cleanup; if\n\t\t * so, skip the rest.\n\t\t */\n\t\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tinode = ext4_orphan_get(sb, le32_to_cpu(es->s_last_orphan));\n\t\tif (IS_ERR(inode)) {\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add(&EXT4_I(inode)->i_orphan, &EXT4_SB(sb)->s_orphan);\n\t\tdquot_initialize(inode);\n\t\tif (inode->i_nlink) {\n\t\t\tif (test_opt(sb, DEBUG))\n\t\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\t\"%s: truncating inode %lu to %lld bytes\",\n\t\t\t\t\t__func__, inode->i_ino, inode->i_size);\n\t\t\tjbd_debug(2, \"truncating inode %lu to %lld bytes\\n\",\n\t\t\t\t  inode->i_ino, inode->i_size);\n\t\t\tinode_lock(inode);\n\t\t\ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n\t\t\tret = ext4_truncate(inode);\n\t\t\tif (ret)\n\t\t\t\text4_std_error(inode->i_sb, ret);\n\t\t\tinode_unlock(inode);\n\t\t\tnr_truncates++;\n\t\t} else {\n\t\t\tif (test_opt(sb, DEBUG))\n\t\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\t\"%s: deleting unreferenced inode %lu\",\n\t\t\t\t\t__func__, inode->i_ino);\n\t\t\tjbd_debug(2, \"deleting unreferenced inode %lu\\n\",\n\t\t\t\t  inode->i_ino);\n\t\t\tnr_orphans++;\n\t\t}\n\t\tiput(inode);  /* The delete magic happens here! */\n\t}\n\n#define PLURAL(x) (x), ((x) == 1) ? \"\" : \"s\"\n\n\tif (nr_orphans)\n\t\text4_msg(sb, KERN_INFO, \"%d orphan inode%s deleted\",\n\t\t       PLURAL(nr_orphans));\n\tif (nr_truncates)\n\t\text4_msg(sb, KERN_INFO, \"%d truncate%s cleaned up\",\n\t\t       PLURAL(nr_truncates));\n#ifdef CONFIG_QUOTA\n\t/* Turn off quotas if they were enabled for orphan cleanup */\n\tif (quota_update) {\n\t\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\t\tif (sb_dqopt(sb)->files[i])\n\t\t\t\tdquot_quota_off(sb, i);\n\t\t}\n\t}\n#endif\n\tsb->s_flags = s_flags; /* Restore SB_RDONLY status */\n}\n\n/*\n * Maximal extent format file size.\n * Resulting logical blkno at s_maxbytes must fit in our on-disk\n * extent format containers, within a sector_t, and within i_blocks\n * in the vfs.  ext4 inode has 48 bits of i_block in fsblock units,\n * so that won't be a limiting factor.\n *\n * However there is other limiting factor. We do store extents in the form\n * of starting block and length, hence the resulting length of the extent\n * covering maximum file size must fit into on-disk format containers as\n * well. Given that length is always by 1 unit bigger than max unit (because\n * we count 0 as well) we have to lower the s_maxbytes by one fs block.\n *\n * Note, this does *not* consider any metadata overhead for vfs i_blocks.\n */\nstatic loff_t ext4_max_size(int blkbits, int has_huge_files)\n{\n\tloff_t res;\n\tloff_t upper_limit = MAX_LFS_FILESIZE;\n\n\tBUILD_BUG_ON(sizeof(blkcnt_t) < sizeof(u64));\n\n\tif (!has_huge_files) {\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (blkbits - 9);\n\t\tupper_limit <<= blkbits;\n\t}\n\n\t/*\n\t * 32-bit extent-start container, ee_block. We lower the maxbytes\n\t * by one fs block, so ee_len can cover the extent of maximum file\n\t * size\n\t */\n\tres = (1LL << 32) - 1;\n\tres <<= blkbits;\n\n\t/* Sanity check against vm- & vfs- imposed limits */\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\treturn res;\n}\n\n/*\n * Maximal bitmap file size.  There is a direct, and {,double-,triple-}indirect\n * block limit, and also a limit of (2^48 - 1) 512-byte sectors in i_blocks.\n * We need to be 1 filesystem block less than the 2^48 sector limit.\n */\nstatic loff_t ext4_max_bitmap_size(int bits, int has_huge_files)\n{\n\tloff_t res = EXT4_NDIR_BLOCKS;\n\tint meta_blocks;\n\tloff_t upper_limit;\n\t/* This is calculated to be the largest file size for a dense, block\n\t * mapped file such that the file's total number of 512-byte sectors,\n\t * including data and all indirect blocks, does not exceed (2^48 - 1).\n\t *\n\t * __u32 i_blocks_lo and _u16 i_blocks_high represent the total\n\t * number of 512-byte sectors of the file.\n\t */\n\n\tif (!has_huge_files) {\n\t\t/*\n\t\t * !has_huge_files or implies that the inode i_block field\n\t\t * represents total file blocks in 2^32 512-byte sectors ==\n\t\t * size of vfs inode i_blocks * 8\n\t\t */\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (bits - 9);\n\n\t} else {\n\t\t/*\n\t\t * We use 48 bit ext4_inode i_blocks\n\t\t * With EXT4_HUGE_FILE_FL set the i_blocks\n\t\t * represent total number of blocks in\n\t\t * file system block size\n\t\t */\n\t\tupper_limit = (1LL << 48) - 1;\n\n\t}\n\n\t/* indirect blocks */\n\tmeta_blocks = 1;\n\t/* double indirect blocks */\n\tmeta_blocks += 1 + (1LL << (bits-2));\n\t/* tripple indirect blocks */\n\tmeta_blocks += 1 + (1LL << (bits-2)) + (1LL << (2*(bits-2)));\n\n\tupper_limit -= meta_blocks;\n\tupper_limit <<= bits;\n\n\tres += 1LL << (bits-2);\n\tres += 1LL << (2*(bits-2));\n\tres += 1LL << (3*(bits-2));\n\tres <<= bits;\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\tif (res > MAX_LFS_FILESIZE)\n\t\tres = MAX_LFS_FILESIZE;\n\n\treturn res;\n}\n\nstatic ext4_fsblk_t descriptor_loc(struct super_block *sb,\n\t\t\t\t   ext4_fsblk_t logical_sb_block, int nr)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_group_t bg, first_meta_bg;\n\tint has_super = 0;\n\n\tfirst_meta_bg = le32_to_cpu(sbi->s_es->s_first_meta_bg);\n\n\tif (!ext4_has_feature_meta_bg(sb) || nr < first_meta_bg)\n\t\treturn logical_sb_block + nr + 1;\n\tbg = sbi->s_desc_per_block * nr;\n\tif (ext4_bg_has_super(sb, bg))\n\t\thas_super = 1;\n\n\t/*\n\t * If we have a meta_bg fs with 1k blocks, group 0's GDT is at\n\t * block 2, not 1.  If s_first_data_block == 0 (bigalloc is enabled\n\t * on modern mke2fs or blksize > 1k on older mke2fs) then we must\n\t * compensate.\n\t */\n\tif (sb->s_blocksize == 1024 && nr == 0 &&\n\t    le32_to_cpu(sbi->s_es->s_first_data_block) == 0)\n\t\thas_super++;\n\n\treturn (has_super + ext4_group_first_block_no(sb, bg));\n}\n\n/**\n * ext4_get_stripe_size: Get the stripe size.\n * @sbi: In memory super block info\n *\n * If we have specified it via mount option, then\n * use the mount option value. If the value specified at mount time is\n * greater than the blocks per group use the super block value.\n * If the super block value is greater than blocks per group return 0.\n * Allocator needs it be less than blocks per group.\n *\n */\nstatic unsigned long ext4_get_stripe_size(struct ext4_sb_info *sbi)\n{\n\tunsigned long stride = le16_to_cpu(sbi->s_es->s_raid_stride);\n\tunsigned long stripe_width =\n\t\t\tle32_to_cpu(sbi->s_es->s_raid_stripe_width);\n\tint ret;\n\n\tif (sbi->s_stripe && sbi->s_stripe <= sbi->s_blocks_per_group)\n\t\tret = sbi->s_stripe;\n\telse if (stripe_width && stripe_width <= sbi->s_blocks_per_group)\n\t\tret = stripe_width;\n\telse if (stride && stride <= sbi->s_blocks_per_group)\n\t\tret = stride;\n\telse\n\t\tret = 0;\n\n\t/*\n\t * If the stripe width is 1, this makes no sense and\n\t * we set it to 0 to turn off stripe handling code.\n\t */\n\tif (ret <= 1)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * Check whether this filesystem can be mounted based on\n * the features present and the RDONLY/RDWR mount requested.\n * Returns 1 if this filesystem can be mounted as requested,\n * 0 if it cannot be.\n */\nstatic int ext4_feature_set_ok(struct super_block *sb, int readonly)\n{\n\tif (ext4_has_unknown_ext4_incompat_features(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Couldn't mount because of \"\n\t\t\t\"unsupported optional features (%x)\",\n\t\t\t(le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_incompat) &\n\t\t\t~EXT4_FEATURE_INCOMPAT_SUPP));\n\t\treturn 0;\n\t}\n\n#ifndef CONFIG_UNICODE\n\tif (ext4_has_feature_casefold(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with casefold feature cannot be \"\n\t\t\t \"mounted without CONFIG_UNICODE\");\n\t\treturn 0;\n\t}\n#endif\n\n\tif (readonly)\n\t\treturn 1;\n\n\tif (ext4_has_feature_readonly(sb)) {\n\t\text4_msg(sb, KERN_INFO, \"filesystem is read-only\");\n\t\tsb->s_flags |= SB_RDONLY;\n\t\treturn 1;\n\t}\n\n\t/* Check that feature set is OK for a read-write mount */\n\tif (ext4_has_unknown_ext4_ro_compat_features(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't mount RDWR because of \"\n\t\t\t \"unsupported optional features (%x)\",\n\t\t\t (le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_ro_compat) &\n\t\t\t\t~EXT4_FEATURE_RO_COMPAT_SUPP));\n\t\treturn 0;\n\t}\n\tif (ext4_has_feature_bigalloc(sb) && !ext4_has_feature_extents(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Can't support bigalloc feature without \"\n\t\t\t \"extents feature\\n\");\n\t\treturn 0;\n\t}\n\n#ifndef CONFIG_QUOTA\n\tif (ext4_has_feature_quota(sb) && !readonly) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with quota feature cannot be mounted RDWR \"\n\t\t\t \"without CONFIG_QUOTA\");\n\t\treturn 0;\n\t}\n\tif (ext4_has_feature_project(sb) && !readonly) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with project quota feature cannot be mounted RDWR \"\n\t\t\t \"without CONFIG_QUOTA\");\n\t\treturn 0;\n\t}\n#endif  /* CONFIG_QUOTA */\n\treturn 1;\n}\n\n/*\n * This function is called once a day if we have errors logged\n * on the file system\n */\nstatic void print_daily_error_info(struct timer_list *t)\n{\n\tstruct ext4_sb_info *sbi = from_timer(sbi, t, s_err_report);\n\tstruct super_block *sb = sbi->s_sb;\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\tif (es->s_error_count)\n\t\t/* fsck newer than v1.41.13 is needed to clean this condition. */\n\t\text4_msg(sb, KERN_NOTICE, \"error count since last fsck: %u\",\n\t\t\t le32_to_cpu(es->s_error_count));\n\tif (es->s_first_error_time) {\n\t\tprintk(KERN_NOTICE \"EXT4-fs (%s): initial error at time %llu: %.*s:%d\",\n\t\t       sb->s_id,\n\t\t       ext4_get_tstamp(es, s_first_error_time),\n\t\t       (int) sizeof(es->s_first_error_func),\n\t\t       es->s_first_error_func,\n\t\t       le32_to_cpu(es->s_first_error_line));\n\t\tif (es->s_first_error_ino)\n\t\t\tprintk(KERN_CONT \": inode %u\",\n\t\t\t       le32_to_cpu(es->s_first_error_ino));\n\t\tif (es->s_first_error_block)\n\t\t\tprintk(KERN_CONT \": block %llu\", (unsigned long long)\n\t\t\t       le64_to_cpu(es->s_first_error_block));\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tif (es->s_last_error_time) {\n\t\tprintk(KERN_NOTICE \"EXT4-fs (%s): last error at time %llu: %.*s:%d\",\n\t\t       sb->s_id,\n\t\t       ext4_get_tstamp(es, s_last_error_time),\n\t\t       (int) sizeof(es->s_last_error_func),\n\t\t       es->s_last_error_func,\n\t\t       le32_to_cpu(es->s_last_error_line));\n\t\tif (es->s_last_error_ino)\n\t\t\tprintk(KERN_CONT \": inode %u\",\n\t\t\t       le32_to_cpu(es->s_last_error_ino));\n\t\tif (es->s_last_error_block)\n\t\t\tprintk(KERN_CONT \": block %llu\", (unsigned long long)\n\t\t\t       le64_to_cpu(es->s_last_error_block));\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tmod_timer(&sbi->s_err_report, jiffies + 24*60*60*HZ);  /* Once a day */\n}\n\n/* Find next suitable group and run ext4_init_inode_table */\nstatic int ext4_run_li_request(struct ext4_li_request *elr)\n{\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t group, ngroups;\n\tstruct super_block *sb;\n\tunsigned long timeout = 0;\n\tint ret = 0;\n\n\tsb = elr->lr_super;\n\tngroups = EXT4_SB(sb)->s_groups_count;\n\n\tfor (group = elr->lr_next_group; group < ngroups; group++) {\n\t\tgdp = ext4_get_group_desc(sb, group, NULL);\n\t\tif (!gdp) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tbreak;\n\t}\n\n\tif (group >= ngroups)\n\t\tret = 1;\n\n\tif (!ret) {\n\t\ttimeout = jiffies;\n\t\tret = ext4_init_inode_table(sb, group,\n\t\t\t\t\t    elr->lr_timeout ? 0 : 1);\n\t\tif (elr->lr_timeout == 0) {\n\t\t\ttimeout = (jiffies - timeout) *\n\t\t\t\t  elr->lr_sbi->s_li_wait_mult;\n\t\t\telr->lr_timeout = timeout;\n\t\t}\n\t\telr->lr_next_sched = jiffies + elr->lr_timeout;\n\t\telr->lr_next_group = group + 1;\n\t}\n\treturn ret;\n}\n\n/*\n * Remove lr_request from the list_request and free the\n * request structure. Should be called with li_list_mtx held\n */\nstatic void ext4_remove_li_request(struct ext4_li_request *elr)\n{\n\tstruct ext4_sb_info *sbi;\n\n\tif (!elr)\n\t\treturn;\n\n\tsbi = elr->lr_sbi;\n\n\tlist_del(&elr->lr_request);\n\tsbi->s_li_request = NULL;\n\tkfree(elr);\n}\n\nstatic void ext4_unregister_li_request(struct super_block *sb)\n{\n\tmutex_lock(&ext4_li_mtx);\n\tif (!ext4_li_info) {\n\t\tmutex_unlock(&ext4_li_mtx);\n\t\treturn;\n\t}\n\n\tmutex_lock(&ext4_li_info->li_list_mtx);\n\text4_remove_li_request(EXT4_SB(sb)->s_li_request);\n\tmutex_unlock(&ext4_li_info->li_list_mtx);\n\tmutex_unlock(&ext4_li_mtx);\n}\n\nstatic struct task_struct *ext4_lazyinit_task;\n\n/*\n * This is the function where ext4lazyinit thread lives. It walks\n * through the request list searching for next scheduled filesystem.\n * When such a fs is found, run the lazy initialization request\n * (ext4_rn_li_request) and keep track of the time spend in this\n * function. Based on that time we compute next schedule time of\n * the request. When walking through the list is complete, compute\n * next waking time and put itself into sleep.\n */\nstatic int ext4_lazyinit_thread(void *arg)\n{\n\tstruct ext4_lazy_init *eli = (struct ext4_lazy_init *)arg;\n\tstruct list_head *pos, *n;\n\tstruct ext4_li_request *elr;\n\tunsigned long next_wakeup, cur;\n\n\tBUG_ON(NULL == eli);\n\ncont_thread:\n\twhile (true) {\n\t\tnext_wakeup = MAX_JIFFY_OFFSET;\n\n\t\tmutex_lock(&eli->li_list_mtx);\n\t\tif (list_empty(&eli->li_request_list)) {\n\t\t\tmutex_unlock(&eli->li_list_mtx);\n\t\t\tgoto exit_thread;\n\t\t}\n\t\tlist_for_each_safe(pos, n, &eli->li_request_list) {\n\t\t\tint err = 0;\n\t\t\tint progress = 0;\n\t\t\telr = list_entry(pos, struct ext4_li_request,\n\t\t\t\t\t lr_request);\n\n\t\t\tif (time_before(jiffies, elr->lr_next_sched)) {\n\t\t\t\tif (time_before(elr->lr_next_sched, next_wakeup))\n\t\t\t\t\tnext_wakeup = elr->lr_next_sched;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (down_read_trylock(&elr->lr_super->s_umount)) {\n\t\t\t\tif (sb_start_write_trylock(elr->lr_super)) {\n\t\t\t\t\tprogress = 1;\n\t\t\t\t\t/*\n\t\t\t\t\t * We hold sb->s_umount, sb can not\n\t\t\t\t\t * be removed from the list, it is\n\t\t\t\t\t * now safe to drop li_list_mtx\n\t\t\t\t\t */\n\t\t\t\t\tmutex_unlock(&eli->li_list_mtx);\n\t\t\t\t\terr = ext4_run_li_request(elr);\n\t\t\t\t\tsb_end_write(elr->lr_super);\n\t\t\t\t\tmutex_lock(&eli->li_list_mtx);\n\t\t\t\t\tn = pos->next;\n\t\t\t\t}\n\t\t\t\tup_read((&elr->lr_super->s_umount));\n\t\t\t}\n\t\t\t/* error, remove the lazy_init job */\n\t\t\tif (err) {\n\t\t\t\text4_remove_li_request(elr);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!progress) {\n\t\t\t\telr->lr_next_sched = jiffies +\n\t\t\t\t\t(prandom_u32()\n\t\t\t\t\t % (EXT4_DEF_LI_MAX_START_DELAY * HZ));\n\t\t\t}\n\t\t\tif (time_before(elr->lr_next_sched, next_wakeup))\n\t\t\t\tnext_wakeup = elr->lr_next_sched;\n\t\t}\n\t\tmutex_unlock(&eli->li_list_mtx);\n\n\t\ttry_to_freeze();\n\n\t\tcur = jiffies;\n\t\tif ((time_after_eq(cur, next_wakeup)) ||\n\t\t    (MAX_JIFFY_OFFSET == next_wakeup)) {\n\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t}\n\n\t\tschedule_timeout_interruptible(next_wakeup - cur);\n\n\t\tif (kthread_should_stop()) {\n\t\t\text4_clear_request_list();\n\t\t\tgoto exit_thread;\n\t\t}\n\t}\n\nexit_thread:\n\t/*\n\t * It looks like the request list is empty, but we need\n\t * to check it under the li_list_mtx lock, to prevent any\n\t * additions into it, and of course we should lock ext4_li_mtx\n\t * to atomically free the list and ext4_li_info, because at\n\t * this point another ext4 filesystem could be registering\n\t * new one.\n\t */\n\tmutex_lock(&ext4_li_mtx);\n\tmutex_lock(&eli->li_list_mtx);\n\tif (!list_empty(&eli->li_request_list)) {\n\t\tmutex_unlock(&eli->li_list_mtx);\n\t\tmutex_unlock(&ext4_li_mtx);\n\t\tgoto cont_thread;\n\t}\n\tmutex_unlock(&eli->li_list_mtx);\n\tkfree(ext4_li_info);\n\text4_li_info = NULL;\n\tmutex_unlock(&ext4_li_mtx);\n\n\treturn 0;\n}\n\nstatic void ext4_clear_request_list(void)\n{\n\tstruct list_head *pos, *n;\n\tstruct ext4_li_request *elr;\n\n\tmutex_lock(&ext4_li_info->li_list_mtx);\n\tlist_for_each_safe(pos, n, &ext4_li_info->li_request_list) {\n\t\telr = list_entry(pos, struct ext4_li_request,\n\t\t\t\t lr_request);\n\t\text4_remove_li_request(elr);\n\t}\n\tmutex_unlock(&ext4_li_info->li_list_mtx);\n}\n\nstatic int ext4_run_lazyinit_thread(void)\n{\n\text4_lazyinit_task = kthread_run(ext4_lazyinit_thread,\n\t\t\t\t\t ext4_li_info, \"ext4lazyinit\");\n\tif (IS_ERR(ext4_lazyinit_task)) {\n\t\tint err = PTR_ERR(ext4_lazyinit_task);\n\t\text4_clear_request_list();\n\t\tkfree(ext4_li_info);\n\t\text4_li_info = NULL;\n\t\tprintk(KERN_CRIT \"EXT4-fs: error %d creating inode table \"\n\t\t\t\t \"initialization thread\\n\",\n\t\t\t\t err);\n\t\treturn err;\n\t}\n\text4_li_info->li_state |= EXT4_LAZYINIT_RUNNING;\n\treturn 0;\n}\n\n/*\n * Check whether it make sense to run itable init. thread or not.\n * If there is at least one uninitialized inode table, return\n * corresponding group number, else the loop goes through all\n * groups and return total number of groups.\n */\nstatic ext4_group_t ext4_has_uninit_itable(struct super_block *sb)\n{\n\text4_group_t group, ngroups = EXT4_SB(sb)->s_groups_count;\n\tstruct ext4_group_desc *gdp = NULL;\n\n\tif (!ext4_has_group_desc_csum(sb))\n\t\treturn ngroups;\n\n\tfor (group = 0; group < ngroups; group++) {\n\t\tgdp = ext4_get_group_desc(sb, group, NULL);\n\t\tif (!gdp)\n\t\t\tcontinue;\n\n\t\tif (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tbreak;\n\t}\n\n\treturn group;\n}\n\nstatic int ext4_li_info_new(void)\n{\n\tstruct ext4_lazy_init *eli = NULL;\n\n\teli = kzalloc(sizeof(*eli), GFP_KERNEL);\n\tif (!eli)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&eli->li_request_list);\n\tmutex_init(&eli->li_list_mtx);\n\n\teli->li_state |= EXT4_LAZYINIT_QUIT;\n\n\text4_li_info = eli;\n\n\treturn 0;\n}\n\nstatic struct ext4_li_request *ext4_li_request_new(struct super_block *sb,\n\t\t\t\t\t    ext4_group_t start)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_li_request *elr;\n\n\telr = kzalloc(sizeof(*elr), GFP_KERNEL);\n\tif (!elr)\n\t\treturn NULL;\n\n\telr->lr_super = sb;\n\telr->lr_sbi = sbi;\n\telr->lr_next_group = start;\n\n\t/*\n\t * Randomize first schedule time of the request to\n\t * spread the inode table initialization requests\n\t * better.\n\t */\n\telr->lr_next_sched = jiffies + (prandom_u32() %\n\t\t\t\t(EXT4_DEF_LI_MAX_START_DELAY * HZ));\n\treturn elr;\n}\n\nint ext4_register_li_request(struct super_block *sb,\n\t\t\t     ext4_group_t first_not_zeroed)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_li_request *elr = NULL;\n\text4_group_t ngroups = sbi->s_groups_count;\n\tint ret = 0;\n\n\tmutex_lock(&ext4_li_mtx);\n\tif (sbi->s_li_request != NULL) {\n\t\t/*\n\t\t * Reset timeout so it can be computed again, because\n\t\t * s_li_wait_mult might have changed.\n\t\t */\n\t\tsbi->s_li_request->lr_timeout = 0;\n\t\tgoto out;\n\t}\n\n\tif (first_not_zeroed == ngroups || sb_rdonly(sb) ||\n\t    !test_opt(sb, INIT_INODE_TABLE))\n\t\tgoto out;\n\n\telr = ext4_li_request_new(sb, first_not_zeroed);\n\tif (!elr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (NULL == ext4_li_info) {\n\t\tret = ext4_li_info_new();\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tmutex_lock(&ext4_li_info->li_list_mtx);\n\tlist_add(&elr->lr_request, &ext4_li_info->li_request_list);\n\tmutex_unlock(&ext4_li_info->li_list_mtx);\n\n\tsbi->s_li_request = elr;\n\t/*\n\t * set elr to NULL here since it has been inserted to\n\t * the request_list and the removal and free of it is\n\t * handled by ext4_clear_request_list from now on.\n\t */\n\telr = NULL;\n\n\tif (!(ext4_li_info->li_state & EXT4_LAZYINIT_RUNNING)) {\n\t\tret = ext4_run_lazyinit_thread();\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\nout:\n\tmutex_unlock(&ext4_li_mtx);\n\tif (ret)\n\t\tkfree(elr);\n\treturn ret;\n}\n\n/*\n * We do not need to lock anything since this is called on\n * module unload.\n */\nstatic void ext4_destroy_lazyinit_thread(void)\n{\n\t/*\n\t * If thread exited earlier\n\t * there's nothing to be done.\n\t */\n\tif (!ext4_li_info || !ext4_lazyinit_task)\n\t\treturn;\n\n\tkthread_stop(ext4_lazyinit_task);\n}\n\nstatic int set_journal_csum_feature_set(struct super_block *sb)\n{\n\tint ret = 1;\n\tint compat, incompat;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (ext4_has_metadata_csum(sb)) {\n\t\t/* journal checksum v3 */\n\t\tcompat = 0;\n\t\tincompat = JBD2_FEATURE_INCOMPAT_CSUM_V3;\n\t} else {\n\t\t/* journal checksum v1 */\n\t\tcompat = JBD2_FEATURE_COMPAT_CHECKSUM;\n\t\tincompat = 0;\n\t}\n\n\tjbd2_journal_clear_features(sbi->s_journal,\n\t\t\tJBD2_FEATURE_COMPAT_CHECKSUM, 0,\n\t\t\tJBD2_FEATURE_INCOMPAT_CSUM_V3 |\n\t\t\tJBD2_FEATURE_INCOMPAT_CSUM_V2);\n\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\tret = jbd2_journal_set_features(sbi->s_journal,\n\t\t\t\tcompat, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT |\n\t\t\t\tincompat);\n\t} else if (test_opt(sb, JOURNAL_CHECKSUM)) {\n\t\tret = jbd2_journal_set_features(sbi->s_journal,\n\t\t\t\tcompat, 0,\n\t\t\t\tincompat);\n\t\tjbd2_journal_clear_features(sbi->s_journal, 0, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t} else {\n\t\tjbd2_journal_clear_features(sbi->s_journal, 0, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Note: calculating the overhead so we can be compatible with\n * historical BSD practice is quite difficult in the face of\n * clusters/bigalloc.  This is because multiple metadata blocks from\n * different block group can end up in the same allocation cluster.\n * Calculating the exact overhead in the face of clustered allocation\n * requires either O(all block bitmaps) in memory or O(number of block\n * groups**2) in time.  We will still calculate the superblock for\n * older file systems --- and if we come across with a bigalloc file\n * system with zero in s_overhead_clusters the estimate will be close to\n * correct especially for very large cluster sizes --- but for newer\n * file systems, it's better to calculate this figure once at mkfs\n * time, and store it in the superblock.  If the superblock value is\n * present (even for non-bigalloc file systems), we will use it.\n */\nstatic int count_overhead(struct super_block *sb, ext4_group_t grp,\n\t\t\t  char *buf)\n{\n\tstruct ext4_sb_info\t*sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc\t*gdp;\n\text4_fsblk_t\t\tfirst_block, last_block, b;\n\text4_group_t\t\ti, ngroups = ext4_get_groups_count(sb);\n\tint\t\t\ts, j, count = 0;\n\n\tif (!ext4_has_feature_bigalloc(sb))\n\t\treturn (ext4_bg_has_super(sb, grp) + ext4_bg_num_gdb(sb, grp) +\n\t\t\tsbi->s_itb_per_group + 2);\n\n\tfirst_block = le32_to_cpu(sbi->s_es->s_first_data_block) +\n\t\t(grp * EXT4_BLOCKS_PER_GROUP(sb));\n\tlast_block = first_block + EXT4_BLOCKS_PER_GROUP(sb) - 1;\n\tfor (i = 0; i < ngroups; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\t\tb = ext4_block_bitmap(sb, gdp);\n\t\tif (b >= first_block && b <= last_block) {\n\t\t\text4_set_bit(EXT4_B2C(sbi, b - first_block), buf);\n\t\t\tcount++;\n\t\t}\n\t\tb = ext4_inode_bitmap(sb, gdp);\n\t\tif (b >= first_block && b <= last_block) {\n\t\t\text4_set_bit(EXT4_B2C(sbi, b - first_block), buf);\n\t\t\tcount++;\n\t\t}\n\t\tb = ext4_inode_table(sb, gdp);\n\t\tif (b >= first_block && b + sbi->s_itb_per_group <= last_block)\n\t\t\tfor (j = 0; j < sbi->s_itb_per_group; j++, b++) {\n\t\t\t\tint c = EXT4_B2C(sbi, b - first_block);\n\t\t\t\text4_set_bit(c, buf);\n\t\t\t\tcount++;\n\t\t\t}\n\t\tif (i != grp)\n\t\t\tcontinue;\n\t\ts = 0;\n\t\tif (ext4_bg_has_super(sb, grp)) {\n\t\t\text4_set_bit(s++, buf);\n\t\t\tcount++;\n\t\t}\n\t\tj = ext4_bg_num_gdb(sb, grp);\n\t\tif (s + j > EXT4_BLOCKS_PER_GROUP(sb)) {\n\t\t\text4_error(sb, \"Invalid number of block group \"\n\t\t\t\t   \"descriptor blocks: %d\", j);\n\t\t\tj = EXT4_BLOCKS_PER_GROUP(sb) - s;\n\t\t}\n\t\tcount += j;\n\t\tfor (; j > 0; j--)\n\t\t\text4_set_bit(EXT4_B2C(sbi, s++), buf);\n\t}\n\tif (!count)\n\t\treturn 0;\n\treturn EXT4_CLUSTERS_PER_GROUP(sb) -\n\t\text4_count_free(buf, EXT4_CLUSTERS_PER_GROUP(sb) / 8);\n}\n\n/*\n * Compute the overhead and stash it in sbi->s_overhead\n */\nint ext4_calculate_overhead(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tstruct inode *j_inode;\n\tunsigned int j_blocks, j_inum = le32_to_cpu(es->s_journal_inum);\n\text4_group_t i, ngroups = ext4_get_groups_count(sb);\n\text4_fsblk_t overhead = 0;\n\tchar *buf = (char *) get_zeroed_page(GFP_NOFS);\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Compute the overhead (FS structures).  This is constant\n\t * for a given filesystem unless the number of block groups\n\t * changes so we cache the previous value until it does.\n\t */\n\n\t/*\n\t * All of the blocks before first_data_block are overhead\n\t */\n\toverhead = EXT4_B2C(sbi, le32_to_cpu(es->s_first_data_block));\n\n\t/*\n\t * Add the overhead found in each block group\n\t */\n\tfor (i = 0; i < ngroups; i++) {\n\t\tint blks;\n\n\t\tblks = count_overhead(sb, i, buf);\n\t\toverhead += blks;\n\t\tif (blks)\n\t\t\tmemset(buf, 0, PAGE_SIZE);\n\t\tcond_resched();\n\t}\n\n\t/*\n\t * Add the internal journal blocks whether the journal has been\n\t * loaded or not\n\t */\n\tif (sbi->s_journal && !sbi->journal_bdev)\n\t\toverhead += EXT4_NUM_B2C(sbi, sbi->s_journal->j_maxlen);\n\telse if (ext4_has_feature_journal(sb) && !sbi->s_journal) {\n\t\tj_inode = ext4_get_journal_inode(sb, j_inum);\n\t\tif (j_inode) {\n\t\t\tj_blocks = j_inode->i_size >> sb->s_blocksize_bits;\n\t\t\toverhead += EXT4_NUM_B2C(sbi, j_blocks);\n\t\t\tiput(j_inode);\n\t\t} else {\n\t\t\text4_msg(sb, KERN_ERR, \"can't get journal size\");\n\t\t}\n\t}\n\tsbi->s_overhead = overhead;\n\tsmp_wmb();\n\tfree_page((unsigned long) buf);\n\treturn 0;\n}\n\nstatic void ext4_clamp_want_extra_isize(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tunsigned def_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\tEXT4_GOOD_OLD_INODE_SIZE;\n\n\tif (sbi->s_inode_size == EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tsbi->s_want_extra_isize = 0;\n\t\treturn;\n\t}\n\tif (sbi->s_want_extra_isize < 4) {\n\t\tsbi->s_want_extra_isize = def_extra_isize;\n\t\tif (ext4_has_feature_extra_isize(sb)) {\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_want_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_want_extra_isize);\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_min_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_min_extra_isize);\n\t\t}\n\t}\n\t/* Check if enough inode space is available */\n\tif ((sbi->s_want_extra_isize > sbi->s_inode_size) ||\n\t    (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n\t\t\t\t\t\t\tsbi->s_inode_size)) {\n\t\tsbi->s_want_extra_isize = def_extra_isize;\n\t\text4_msg(sb, KERN_INFO,\n\t\t\t \"required extra inode space not available\");\n\t}\n}\n\nstatic void ext4_set_resv_clusters(struct super_block *sb)\n{\n\text4_fsblk_t resv_clusters;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\t/*\n\t * There's no need to reserve anything when we aren't using extents.\n\t * The space estimates are exact, there are no unwritten extents,\n\t * hole punching doesn't need new metadata... This is needed especially\n\t * to keep ext2/3 backward compatibility.\n\t */\n\tif (!ext4_has_feature_extents(sb))\n\t\treturn;\n\t/*\n\t * By default we reserve 2% or 4096 clusters, whichever is smaller.\n\t * This should cover the situations where we can not afford to run\n\t * out of space like for example punch hole, or converting\n\t * unwritten extents in delalloc path. In most cases such\n\t * allocation would require 1, or 2 blocks, higher numbers are\n\t * very rare.\n\t */\n\tresv_clusters = (ext4_blocks_count(sbi->s_es) >>\n\t\t\t sbi->s_cluster_bits);\n\n\tdo_div(resv_clusters, 50);\n\tresv_clusters = min_t(ext4_fsblk_t, resv_clusters, 4096);\n\n\tatomic64_set(&sbi->s_resv_clusters, resv_clusters);\n}\n\nstatic int ext4_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct dax_device *dax_dev = fs_dax_get_by_bdev(sb->s_bdev);\n\tchar *orig_data = kstrdup(data, GFP_KERNEL);\n\tstruct buffer_head *bh;\n\tstruct ext4_super_block *es = NULL;\n\tstruct ext4_sb_info *sbi = kzalloc(sizeof(*sbi), GFP_KERNEL);\n\text4_fsblk_t block;\n\text4_fsblk_t sb_block = get_sb_block(&data);\n\text4_fsblk_t logical_sb_block;\n\tunsigned long offset = 0;\n\tunsigned long journal_devnum = 0;\n\tunsigned long def_mount_opts;\n\tstruct inode *root;\n\tconst char *descr;\n\tint ret = -ENOMEM;\n\tint blocksize, clustersize;\n\tunsigned int db_count;\n\tunsigned int i;\n\tint needs_recovery, has_huge_files, has_bigalloc;\n\t__u64 blocks_count;\n\tint err = 0;\n\tunsigned int journal_ioprio = DEFAULT_JOURNAL_IOPRIO;\n\text4_group_t first_not_zeroed;\n\n\tif ((data && !orig_data) || !sbi)\n\t\tgoto out_free_base;\n\n\tsbi->s_daxdev = dax_dev;\n\tsbi->s_blockgroup_lock =\n\t\tkzalloc(sizeof(struct blockgroup_lock), GFP_KERNEL);\n\tif (!sbi->s_blockgroup_lock)\n\t\tgoto out_free_base;\n\n\tsb->s_fs_info = sbi;\n\tsbi->s_sb = sb;\n\tsbi->s_inode_readahead_blks = EXT4_DEF_INODE_READAHEAD_BLKS;\n\tsbi->s_sb_block = sb_block;\n\tif (sb->s_bdev->bd_part)\n\t\tsbi->s_sectors_written_start =\n\t\t\tpart_stat_read(sb->s_bdev->bd_part, sectors[STAT_WRITE]);\n\n\t/* Cleanup superblock name */\n\tstrreplace(sb->s_id, '/', '!');\n\n\t/* -EINVAL is default */\n\tret = -EINVAL;\n\tblocksize = sb_min_blocksize(sb, EXT4_MIN_BLOCK_SIZE);\n\tif (!blocksize) {\n\t\text4_msg(sb, KERN_ERR, \"unable to set blocksize\");\n\t\tgoto out_fail;\n\t}\n\n\t/*\n\t * The ext4 superblock will not be buffer aligned for other than 1kB\n\t * block sizes.  We need to calculate the offset from buffer start.\n\t */\n\tif (blocksize != EXT4_MIN_BLOCK_SIZE) {\n\t\tlogical_sb_block = sb_block * EXT4_MIN_BLOCK_SIZE;\n\t\toffset = do_div(logical_sb_block, blocksize);\n\t} else {\n\t\tlogical_sb_block = sb_block;\n\t}\n\n\tif (!(bh = sb_bread_unmovable(sb, logical_sb_block))) {\n\t\text4_msg(sb, KERN_ERR, \"unable to read superblock\");\n\t\tgoto out_fail;\n\t}\n\t/*\n\t * Note: s_es must be initialized as soon as possible because\n\t *       some ext4 macro-instructions depend on its value\n\t */\n\tes = (struct ext4_super_block *) (bh->b_data + offset);\n\tsbi->s_es = es;\n\tsb->s_magic = le16_to_cpu(es->s_magic);\n\tif (sb->s_magic != EXT4_SUPER_MAGIC)\n\t\tgoto cantfind_ext4;\n\tsbi->s_kbytes_written = le64_to_cpu(es->s_kbytes_written);\n\n\t/* Warn if metadata_csum and gdt_csum are both set. */\n\tif (ext4_has_feature_metadata_csum(sb) &&\n\t    ext4_has_feature_gdt_csum(sb))\n\t\text4_warning(sb, \"metadata_csum and uninit_bg are \"\n\t\t\t     \"redundant flags; please run fsck.\");\n\n\t/* Check for a known checksum algorithm */\n\tif (!ext4_verify_csum_type(sb, es)) {\n\t\text4_msg(sb, KERN_ERR, \"VFS: Found ext4 filesystem with \"\n\t\t\t \"unknown checksum algorithm.\");\n\t\tsilent = 1;\n\t\tgoto cantfind_ext4;\n\t}\n\n\t/* Load the checksum driver */\n\tsbi->s_chksum_driver = crypto_alloc_shash(\"crc32c\", 0, 0);\n\tif (IS_ERR(sbi->s_chksum_driver)) {\n\t\text4_msg(sb, KERN_ERR, \"Cannot load crc32c driver.\");\n\t\tret = PTR_ERR(sbi->s_chksum_driver);\n\t\tsbi->s_chksum_driver = NULL;\n\t\tgoto failed_mount;\n\t}\n\n\t/* Check superblock checksum */\n\tif (!ext4_superblock_csum_verify(sb, es)) {\n\t\text4_msg(sb, KERN_ERR, \"VFS: Found ext4 filesystem with \"\n\t\t\t \"invalid superblock checksum.  Run e2fsck?\");\n\t\tsilent = 1;\n\t\tret = -EFSBADCRC;\n\t\tgoto cantfind_ext4;\n\t}\n\n\t/* Precompute checksum seed for all metadata */\n\tif (ext4_has_feature_csum_seed(sb))\n\t\tsbi->s_csum_seed = le32_to_cpu(es->s_checksum_seed);\n\telse if (ext4_has_metadata_csum(sb) || ext4_has_feature_ea_inode(sb))\n\t\tsbi->s_csum_seed = ext4_chksum(sbi, ~0, es->s_uuid,\n\t\t\t\t\t       sizeof(es->s_uuid));\n\n\t/* Set defaults before we parse the mount options */\n\tdef_mount_opts = le32_to_cpu(es->s_default_mount_opts);\n\tset_opt(sb, INIT_INODE_TABLE);\n\tif (def_mount_opts & EXT4_DEFM_DEBUG)\n\t\tset_opt(sb, DEBUG);\n\tif (def_mount_opts & EXT4_DEFM_BSDGROUPS)\n\t\tset_opt(sb, GRPID);\n\tif (def_mount_opts & EXT4_DEFM_UID16)\n\t\tset_opt(sb, NO_UID32);\n\t/* xattr user namespace & acls are now defaulted on */\n\tset_opt(sb, XATTR_USER);\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\tset_opt(sb, POSIX_ACL);\n#endif\n\t/* don't forget to enable journal_csum when metadata_csum is enabled. */\n\tif (ext4_has_metadata_csum(sb))\n\t\tset_opt(sb, JOURNAL_CHECKSUM);\n\n\tif ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_DATA)\n\t\tset_opt(sb, JOURNAL_DATA);\n\telse if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_ORDERED)\n\t\tset_opt(sb, ORDERED_DATA);\n\telse if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_WBACK)\n\t\tset_opt(sb, WRITEBACK_DATA);\n\n\tif (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_PANIC)\n\t\tset_opt(sb, ERRORS_PANIC);\n\telse if (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_CONTINUE)\n\t\tset_opt(sb, ERRORS_CONT);\n\telse\n\t\tset_opt(sb, ERRORS_RO);\n\t/* block_validity enabled by default; disable with noblock_validity */\n\tset_opt(sb, BLOCK_VALIDITY);\n\tif (def_mount_opts & EXT4_DEFM_DISCARD)\n\t\tset_opt(sb, DISCARD);\n\n\tsbi->s_resuid = make_kuid(&init_user_ns, le16_to_cpu(es->s_def_resuid));\n\tsbi->s_resgid = make_kgid(&init_user_ns, le16_to_cpu(es->s_def_resgid));\n\tsbi->s_commit_interval = JBD2_DEFAULT_MAX_COMMIT_AGE * HZ;\n\tsbi->s_min_batch_time = EXT4_DEF_MIN_BATCH_TIME;\n\tsbi->s_max_batch_time = EXT4_DEF_MAX_BATCH_TIME;\n\n\tif ((def_mount_opts & EXT4_DEFM_NOBARRIER) == 0)\n\t\tset_opt(sb, BARRIER);\n\n\t/*\n\t * enable delayed allocation by default\n\t * Use -o nodelalloc to turn it off\n\t */\n\tif (!IS_EXT3_SB(sb) && !IS_EXT2_SB(sb) &&\n\t    ((def_mount_opts & EXT4_DEFM_NODELALLOC) == 0))\n\t\tset_opt(sb, DELALLOC);\n\n\t/*\n\t * set default s_li_wait_mult for lazyinit, for the case there is\n\t * no mount option specified.\n\t */\n\tsbi->s_li_wait_mult = EXT4_DEF_LI_WAIT_MULT;\n\n\tif (sbi->s_es->s_mount_opts[0]) {\n\t\tchar *s_mount_opts = kstrndup(sbi->s_es->s_mount_opts,\n\t\t\t\t\t      sizeof(sbi->s_es->s_mount_opts),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!s_mount_opts)\n\t\t\tgoto failed_mount;\n\t\tif (!parse_options(s_mount_opts, sb, &journal_devnum,\n\t\t\t\t   &journal_ioprio, 0)) {\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"failed to parse options in superblock: %s\",\n\t\t\t\t s_mount_opts);\n\t\t}\n\t\tkfree(s_mount_opts);\n\t}\n\tsbi->s_def_mount_opt = sbi->s_mount_opt;\n\tif (!parse_options((char *) data, sb, &journal_devnum,\n\t\t\t   &journal_ioprio, 0))\n\t\tgoto failed_mount;\n\n#ifdef CONFIG_UNICODE\n\tif (ext4_has_feature_casefold(sb) && !sbi->s_encoding) {\n\t\tconst struct ext4_sb_encodings *encoding_info;\n\t\tstruct unicode_map *encoding;\n\t\t__u16 encoding_flags;\n\n\t\tif (ext4_has_feature_encrypt(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Can't mount with encoding and encryption\");\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\tif (ext4_sb_read_encoding(es, &encoding_info,\n\t\t\t\t\t  &encoding_flags)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Encoding requested by superblock is unknown\");\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\tencoding = utf8_load(encoding_info->version);\n\t\tif (IS_ERR(encoding)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"can't mount with superblock charset: %s-%s \"\n\t\t\t\t \"not supported by the kernel. flags: 0x%x.\",\n\t\t\t\t encoding_info->name, encoding_info->version,\n\t\t\t\t encoding_flags);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\text4_msg(sb, KERN_INFO,\"Using encoding defined by superblock: \"\n\t\t\t \"%s-%s with flags 0x%hx\", encoding_info->name,\n\t\t\t encoding_info->version?:\"\\b\", encoding_flags);\n\n\t\tsbi->s_encoding = encoding;\n\t\tsbi->s_encoding_flags = encoding_flags;\n\t}\n#endif\n\n\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) {\n\t\tprintk_once(KERN_WARNING \"EXT4-fs: Warning: mounting \"\n\t\t\t    \"with data=journal disables delayed \"\n\t\t\t    \"allocation and O_DIRECT support!\\n\");\n\t\tif (test_opt2(sb, EXPLICIT_DELALLOC)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and delalloc\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (test_opt(sb, DIOREAD_NOLOCK)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and dioread_nolock\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (test_opt(sb, DAX)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and dax\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (ext4_has_feature_encrypt(sb)) {\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"encrypted files will use data=ordered \"\n\t\t\t\t \"instead of data journaling mode\");\n\t\t}\n\t\tif (test_opt(sb, DELALLOC))\n\t\t\tclear_opt(sb, DELALLOC);\n\t} else {\n\t\tsb->s_iflags |= SB_I_CGROUPWB;\n\t}\n\n\tsb->s_flags = (sb->s_flags & ~SB_POSIXACL) |\n\t\t(test_opt(sb, POSIX_ACL) ? SB_POSIXACL : 0);\n\n\tif (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV &&\n\t    (ext4_has_compat_features(sb) ||\n\t     ext4_has_ro_compat_features(sb) ||\n\t     ext4_has_incompat_features(sb)))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t       \"feature flags set on rev 0 fs, \"\n\t\t       \"running e2fsck is recommended\");\n\n\tif (es->s_creator_os == cpu_to_le32(EXT4_OS_HURD)) {\n\t\tset_opt2(sb, HURD_COMPAT);\n\t\tif (ext4_has_feature_64bit(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"The Hurd can't support 64-bit file systems\");\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\t/*\n\t\t * ea_inode feature uses l_i_version field which is not\n\t\t * available in HURD_COMPAT mode.\n\t\t */\n\t\tif (ext4_has_feature_ea_inode(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"ea_inode feature is not supported for Hurd\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\tif (IS_EXT2_SB(sb)) {\n\t\tif (ext2_feature_set_ok(sb))\n\t\t\text4_msg(sb, KERN_INFO, \"mounting ext2 file system \"\n\t\t\t\t \"using the ext4 subsystem\");\n\t\telse {\n\t\t\t/*\n\t\t\t * If we're probing be silent, if this looks like\n\t\t\t * it's actually an ext[34] filesystem.\n\t\t\t */\n\t\t\tif (silent && ext4_feature_set_ok(sb, sb_rdonly(sb)))\n\t\t\t\tgoto failed_mount;\n\t\t\text4_msg(sb, KERN_ERR, \"couldn't mount as ext2 due \"\n\t\t\t\t \"to feature incompatibilities\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\tif (IS_EXT3_SB(sb)) {\n\t\tif (ext3_feature_set_ok(sb))\n\t\t\text4_msg(sb, KERN_INFO, \"mounting ext3 file system \"\n\t\t\t\t \"using the ext4 subsystem\");\n\t\telse {\n\t\t\t/*\n\t\t\t * If we're probing be silent, if this looks like\n\t\t\t * it's actually an ext4 filesystem.\n\t\t\t */\n\t\t\tif (silent && ext4_feature_set_ok(sb, sb_rdonly(sb)))\n\t\t\t\tgoto failed_mount;\n\t\t\text4_msg(sb, KERN_ERR, \"couldn't mount as ext3 due \"\n\t\t\t\t \"to feature incompatibilities\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\t/*\n\t * Check feature flags regardless of the revision level, since we\n\t * previously didn't change the revision level when setting the flags,\n\t * so there is a chance incompat flags are set on a rev 0 filesystem.\n\t */\n\tif (!ext4_feature_set_ok(sb, (sb_rdonly(sb))))\n\t\tgoto failed_mount;\n\n\tblocksize = BLOCK_SIZE << le32_to_cpu(es->s_log_block_size);\n\tif (blocksize < EXT4_MIN_BLOCK_SIZE ||\n\t    blocksize > EXT4_MAX_BLOCK_SIZE) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t       \"Unsupported filesystem blocksize %d (%d log_block_size)\",\n\t\t\t blocksize, le32_to_cpu(es->s_log_block_size));\n\t\tgoto failed_mount;\n\t}\n\tif (le32_to_cpu(es->s_log_block_size) >\n\t    (EXT4_MAX_BLOCK_LOG_SIZE - EXT4_MIN_BLOCK_LOG_SIZE)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Invalid log block size: %u\",\n\t\t\t le32_to_cpu(es->s_log_block_size));\n\t\tgoto failed_mount;\n\t}\n\tif (le32_to_cpu(es->s_log_cluster_size) >\n\t    (EXT4_MAX_CLUSTER_LOG_SIZE - EXT4_MIN_BLOCK_LOG_SIZE)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Invalid log cluster size: %u\",\n\t\t\t le32_to_cpu(es->s_log_cluster_size));\n\t\tgoto failed_mount;\n\t}\n\n\tif (le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) > (blocksize / 4)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Number of reserved GDT blocks insanely large: %d\",\n\t\t\t le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks));\n\t\tgoto failed_mount;\n\t}\n\n\tif (sbi->s_mount_opt & EXT4_MOUNT_DAX) {\n\t\tif (ext4_has_feature_inline_data(sb)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Cannot use DAX on a filesystem\"\n\t\t\t\t\t\" that may contain inline data\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (!bdev_dax_supported(sb->s_bdev, blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"DAX unsupported by block device.\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\tif (ext4_has_feature_encrypt(sb) && es->s_encryption_level) {\n\t\text4_msg(sb, KERN_ERR, \"Unsupported encryption level %d\",\n\t\t\t es->s_encryption_level);\n\t\tgoto failed_mount;\n\t}\n\n\tif (sb->s_blocksize != blocksize) {\n\t\t/* Validate the filesystem blocksize */\n\t\tif (!sb_set_blocksize(sb, blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR, \"bad block size %d\",\n\t\t\t\t\tblocksize);\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\tbrelse(bh);\n\t\tlogical_sb_block = sb_block * EXT4_MIN_BLOCK_SIZE;\n\t\toffset = do_div(logical_sb_block, blocksize);\n\t\tbh = sb_bread_unmovable(sb, logical_sb_block);\n\t\tif (!bh) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Can't read superblock on 2nd try\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tes = (struct ext4_super_block *)(bh->b_data + offset);\n\t\tsbi->s_es = es;\n\t\tif (es->s_magic != cpu_to_le16(EXT4_SUPER_MAGIC)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Magic mismatch, very weird!\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\thas_huge_files = ext4_has_feature_huge_file(sb);\n\tsbi->s_bitmap_maxbytes = ext4_max_bitmap_size(sb->s_blocksize_bits,\n\t\t\t\t\t\t      has_huge_files);\n\tsb->s_maxbytes = ext4_max_size(sb->s_blocksize_bits, has_huge_files);\n\n\tif (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV) {\n\t\tsbi->s_inode_size = EXT4_GOOD_OLD_INODE_SIZE;\n\t\tsbi->s_first_ino = EXT4_GOOD_OLD_FIRST_INO;\n\t} else {\n\t\tsbi->s_inode_size = le16_to_cpu(es->s_inode_size);\n\t\tsbi->s_first_ino = le32_to_cpu(es->s_first_ino);\n\t\tif (sbi->s_first_ino < EXT4_GOOD_OLD_FIRST_INO) {\n\t\t\text4_msg(sb, KERN_ERR, \"invalid first ino: %u\",\n\t\t\t\t sbi->s_first_ino);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif ((sbi->s_inode_size < EXT4_GOOD_OLD_INODE_SIZE) ||\n\t\t    (!is_power_of_2(sbi->s_inode_size)) ||\n\t\t    (sbi->s_inode_size > blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unsupported inode size: %d\",\n\t\t\t       sbi->s_inode_size);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\t/*\n\t\t * i_atime_extra is the last extra field available for [acm]times in\n\t\t * struct ext4_inode. Checking for that field should suffice to ensure\n\t\t * we have extra space for all three.\n\t\t */\n\t\tif (sbi->s_inode_size >= offsetof(struct ext4_inode, i_atime_extra) +\n\t\t\tsizeof(((struct ext4_inode *)0)->i_atime_extra)) {\n\t\t\tsb->s_time_gran = 1;\n\t\t\tsb->s_time_max = EXT4_EXTRA_TIMESTAMP_MAX;\n\t\t} else {\n\t\t\tsb->s_time_gran = NSEC_PER_SEC;\n\t\t\tsb->s_time_max = EXT4_NON_EXTRA_TIMESTAMP_MAX;\n\t\t}\n\n\t\tsb->s_time_min = EXT4_TIMESTAMP_MIN;\n\t}\n\n\tsbi->s_desc_size = le16_to_cpu(es->s_desc_size);\n\tif (ext4_has_feature_64bit(sb)) {\n\t\tif (sbi->s_desc_size < EXT4_MIN_DESC_SIZE_64BIT ||\n\t\t    sbi->s_desc_size > EXT4_MAX_DESC_SIZE ||\n\t\t    !is_power_of_2(sbi->s_desc_size)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unsupported descriptor size %lu\",\n\t\t\t       sbi->s_desc_size);\n\t\t\tgoto failed_mount;\n\t\t}\n\t} else\n\t\tsbi->s_desc_size = EXT4_MIN_DESC_SIZE;\n\n\tsbi->s_blocks_per_group = le32_to_cpu(es->s_blocks_per_group);\n\tsbi->s_inodes_per_group = le32_to_cpu(es->s_inodes_per_group);\n\n\tsbi->s_inodes_per_block = blocksize / EXT4_INODE_SIZE(sb);\n\tif (sbi->s_inodes_per_block == 0)\n\t\tgoto cantfind_ext4;\n\tif (sbi->s_inodes_per_group < sbi->s_inodes_per_block ||\n\t    sbi->s_inodes_per_group > blocksize * 8) {\n\t\text4_msg(sb, KERN_ERR, \"invalid inodes per group: %lu\\n\",\n\t\t\t sbi->s_blocks_per_group);\n\t\tgoto failed_mount;\n\t}\n\tsbi->s_itb_per_group = sbi->s_inodes_per_group /\n\t\t\t\t\tsbi->s_inodes_per_block;\n\tsbi->s_desc_per_block = blocksize / EXT4_DESC_SIZE(sb);\n\tsbi->s_sbh = bh;\n\tsbi->s_mount_state = le16_to_cpu(es->s_state);\n\tsbi->s_addr_per_block_bits = ilog2(EXT4_ADDR_PER_BLOCK(sb));\n\tsbi->s_desc_per_block_bits = ilog2(EXT4_DESC_PER_BLOCK(sb));\n\n\tfor (i = 0; i < 4; i++)\n\t\tsbi->s_hash_seed[i] = le32_to_cpu(es->s_hash_seed[i]);\n\tsbi->s_def_hash_version = es->s_def_hash_version;\n\tif (ext4_has_feature_dir_index(sb)) {\n\t\ti = le32_to_cpu(es->s_flags);\n\t\tif (i & EXT2_FLAGS_UNSIGNED_HASH)\n\t\t\tsbi->s_hash_unsigned = 3;\n\t\telse if ((i & EXT2_FLAGS_SIGNED_HASH) == 0) {\n#ifdef __CHAR_UNSIGNED__\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\tes->s_flags |=\n\t\t\t\t\tcpu_to_le32(EXT2_FLAGS_UNSIGNED_HASH);\n\t\t\tsbi->s_hash_unsigned = 3;\n#else\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\tes->s_flags |=\n\t\t\t\t\tcpu_to_le32(EXT2_FLAGS_SIGNED_HASH);\n#endif\n\t\t}\n\t}\n\n\t/* Handle clustersize */\n\tclustersize = BLOCK_SIZE << le32_to_cpu(es->s_log_cluster_size);\n\thas_bigalloc = ext4_has_feature_bigalloc(sb);\n\tif (has_bigalloc) {\n\t\tif (clustersize < blocksize) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"cluster size (%d) smaller than \"\n\t\t\t\t \"block size (%d)\", clustersize, blocksize);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tsbi->s_cluster_bits = le32_to_cpu(es->s_log_cluster_size) -\n\t\t\tle32_to_cpu(es->s_log_block_size);\n\t\tsbi->s_clusters_per_group =\n\t\t\tle32_to_cpu(es->s_clusters_per_group);\n\t\tif (sbi->s_clusters_per_group > blocksize * 8) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"#clusters per group too big: %lu\",\n\t\t\t\t sbi->s_clusters_per_group);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (sbi->s_blocks_per_group !=\n\t\t    (sbi->s_clusters_per_group * (clustersize / blocksize))) {\n\t\t\text4_msg(sb, KERN_ERR, \"blocks per group (%lu) and \"\n\t\t\t\t \"clusters per group (%lu) inconsistent\",\n\t\t\t\t sbi->s_blocks_per_group,\n\t\t\t\t sbi->s_clusters_per_group);\n\t\t\tgoto failed_mount;\n\t\t}\n\t} else {\n\t\tif (clustersize != blocksize) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"fragment/cluster size (%d) != \"\n\t\t\t\t \"block size (%d)\", clustersize, blocksize);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (sbi->s_blocks_per_group > blocksize * 8) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"#blocks per group too big: %lu\",\n\t\t\t\t sbi->s_blocks_per_group);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tsbi->s_clusters_per_group = sbi->s_blocks_per_group;\n\t\tsbi->s_cluster_bits = 0;\n\t}\n\tsbi->s_cluster_ratio = clustersize / blocksize;\n\n\t/* Do we have standard group size of clustersize * 8 blocks ? */\n\tif (sbi->s_blocks_per_group == clustersize << 3)\n\t\tset_opt2(sb, STD_GROUP_SIZE);\n\n\t/*\n\t * Test whether we have more sectors than will fit in sector_t,\n\t * and whether the max offset is addressable by the page cache.\n\t */\n\terr = generic_check_addressable(sb->s_blocksize_bits,\n\t\t\t\t\text4_blocks_count(es));\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"filesystem\"\n\t\t\t \" too large to mount safely on this system\");\n\t\tgoto failed_mount;\n\t}\n\n\tif (EXT4_BLOCKS_PER_GROUP(sb) == 0)\n\t\tgoto cantfind_ext4;\n\n\t/* check blocks count against device size */\n\tblocks_count = sb->s_bdev->bd_inode->i_size >> sb->s_blocksize_bits;\n\tif (blocks_count && ext4_blocks_count(es) > blocks_count) {\n\t\text4_msg(sb, KERN_WARNING, \"bad geometry: block count %llu \"\n\t\t       \"exceeds size of device (%llu blocks)\",\n\t\t       ext4_blocks_count(es), blocks_count);\n\t\tgoto failed_mount;\n\t}\n\n\t/*\n\t * It makes no sense for the first data block to be beyond the end\n\t * of the filesystem.\n\t */\n\tif (le32_to_cpu(es->s_first_data_block) >= ext4_blocks_count(es)) {\n\t\text4_msg(sb, KERN_WARNING, \"bad geometry: first data \"\n\t\t\t \"block %u is beyond end of filesystem (%llu)\",\n\t\t\t le32_to_cpu(es->s_first_data_block),\n\t\t\t ext4_blocks_count(es));\n\t\tgoto failed_mount;\n\t}\n\tif ((es->s_first_data_block == 0) && (es->s_log_block_size == 0) &&\n\t    (sbi->s_cluster_ratio == 1)) {\n\t\text4_msg(sb, KERN_WARNING, \"bad geometry: first data \"\n\t\t\t \"block is 0 with a 1k block and cluster size\");\n\t\tgoto failed_mount;\n\t}\n\n\tblocks_count = (ext4_blocks_count(es) -\n\t\t\tle32_to_cpu(es->s_first_data_block) +\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb) - 1);\n\tdo_div(blocks_count, EXT4_BLOCKS_PER_GROUP(sb));\n\tif (blocks_count > ((uint64_t)1<<32) - EXT4_DESC_PER_BLOCK(sb)) {\n\t\text4_msg(sb, KERN_WARNING, \"groups count too large: %u \"\n\t\t       \"(block count %llu, first data block %u, \"\n\t\t       \"blocks per group %lu)\", sbi->s_groups_count,\n\t\t       ext4_blocks_count(es),\n\t\t       le32_to_cpu(es->s_first_data_block),\n\t\t       EXT4_BLOCKS_PER_GROUP(sb));\n\t\tgoto failed_mount;\n\t}\n\tsbi->s_groups_count = blocks_count;\n\tsbi->s_blockfile_groups = min_t(ext4_group_t, sbi->s_groups_count,\n\t\t\t(EXT4_MAX_BLOCK_FILE_PHYS / EXT4_BLOCKS_PER_GROUP(sb)));\n\tif (((u64)sbi->s_groups_count * sbi->s_inodes_per_group) !=\n\t    le32_to_cpu(es->s_inodes_count)) {\n\t\text4_msg(sb, KERN_ERR, \"inodes count not valid: %u vs %llu\",\n\t\t\t le32_to_cpu(es->s_inodes_count),\n\t\t\t ((u64)sbi->s_groups_count * sbi->s_inodes_per_group));\n\t\tret = -EINVAL;\n\t\tgoto failed_mount;\n\t}\n\tdb_count = (sbi->s_groups_count + EXT4_DESC_PER_BLOCK(sb) - 1) /\n\t\t   EXT4_DESC_PER_BLOCK(sb);\n\tif (ext4_has_feature_meta_bg(sb)) {\n\t\tif (le32_to_cpu(es->s_first_meta_bg) > db_count) {\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"first meta block group too large: %u \"\n\t\t\t\t \"(group descriptor block count %u)\",\n\t\t\t\t le32_to_cpu(es->s_first_meta_bg), db_count);\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\tsbi->s_group_desc = kvmalloc_array(db_count,\n\t\t\t\t\t   sizeof(struct buffer_head *),\n\t\t\t\t\t   GFP_KERNEL);\n\tif (sbi->s_group_desc == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory\");\n\t\tret = -ENOMEM;\n\t\tgoto failed_mount;\n\t}\n\n\tbgl_lock_init(sbi->s_blockgroup_lock);\n\n\t/* Pre-read the descriptors into the buffer cache */\n\tfor (i = 0; i < db_count; i++) {\n\t\tblock = descriptor_loc(sb, logical_sb_block, i);\n\t\tsb_breadahead(sb, block);\n\t}\n\n\tfor (i = 0; i < db_count; i++) {\n\t\tblock = descriptor_loc(sb, logical_sb_block, i);\n\t\tsbi->s_group_desc[i] = sb_bread_unmovable(sb, block);\n\t\tif (!sbi->s_group_desc[i]) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"can't read group descriptor %d\", i);\n\t\t\tdb_count = i;\n\t\t\tgoto failed_mount2;\n\t\t}\n\t}\n\tsbi->s_gdb_count = db_count;\n\tif (!ext4_check_descriptors(sb, logical_sb_block, &first_not_zeroed)) {\n\t\text4_msg(sb, KERN_ERR, \"group descriptors corrupted!\");\n\t\tret = -EFSCORRUPTED;\n\t\tgoto failed_mount2;\n\t}\n\n\ttimer_setup(&sbi->s_err_report, print_daily_error_info, 0);\n\n\t/* Register extent status tree shrinker */\n\tif (ext4_es_register_shrinker(sbi))\n\t\tgoto failed_mount3;\n\n\tsbi->s_stripe = ext4_get_stripe_size(sbi);\n\tsbi->s_extent_max_zeroout_kb = 32;\n\n\t/*\n\t * set up enough so that it can read an inode\n\t */\n\tsb->s_op = &ext4_sops;\n\tsb->s_export_op = &ext4_export_ops;\n\tsb->s_xattr = ext4_xattr_handlers;\n#ifdef CONFIG_FS_ENCRYPTION\n\tsb->s_cop = &ext4_cryptops;\n#endif\n#ifdef CONFIG_FS_VERITY\n\tsb->s_vop = &ext4_verityops;\n#endif\n#ifdef CONFIG_QUOTA\n\tsb->dq_op = &ext4_quota_operations;\n\tif (ext4_has_feature_quota(sb))\n\t\tsb->s_qcop = &dquot_quotactl_sysfile_ops;\n\telse\n\t\tsb->s_qcop = &ext4_qctl_operations;\n\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ;\n#endif\n\tmemcpy(&sb->s_uuid, es->s_uuid, sizeof(es->s_uuid));\n\n\tINIT_LIST_HEAD(&sbi->s_orphan); /* unlinked but open files */\n\tmutex_init(&sbi->s_orphan_lock);\n\n\tsb->s_root = NULL;\n\n\tneeds_recovery = (es->s_last_orphan != 0 ||\n\t\t\t  ext4_has_feature_journal_needs_recovery(sb));\n\n\tif (ext4_has_feature_mmp(sb) && !sb_rdonly(sb))\n\t\tif (ext4_multi_mount_protect(sb, le64_to_cpu(es->s_mmp_block)))\n\t\t\tgoto failed_mount3a;\n\n\t/*\n\t * The first inode we look at is the journal inode.  Don't try\n\t * root first: it may be modified in the journal!\n\t */\n\tif (!test_opt(sb, NOLOAD) && ext4_has_feature_journal(sb)) {\n\t\terr = ext4_load_journal(sb, es, journal_devnum);\n\t\tif (err)\n\t\t\tgoto failed_mount3a;\n\t} else if (test_opt(sb, NOLOAD) && !sb_rdonly(sb) &&\n\t\t   ext4_has_feature_journal_needs_recovery(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"required journal recovery \"\n\t\t       \"suppressed and not mounted read-only\");\n\t\tgoto failed_mount_wq;\n\t} else {\n\t\t/* Nojournal mode, all journal mount options are illegal */\n\t\tif (test_opt2(sb, EXPLICIT_JOURNAL_CHECKSUM)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"journal_checksum, fs mounted w/o journal\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"journal_async_commit, fs mounted w/o journal\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t\tif (sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"commit=%lu, fs mounted w/o journal\",\n\t\t\t\t sbi->s_commit_interval / HZ);\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t\tif (EXT4_MOUNT_DATA_FLAGS &\n\t\t    (sbi->s_mount_opt ^ sbi->s_def_mount_opt)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"data=, fs mounted w/o journal\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t\tsbi->s_def_mount_opt &= ~EXT4_MOUNT_JOURNAL_CHECKSUM;\n\t\tclear_opt(sb, JOURNAL_CHECKSUM);\n\t\tclear_opt(sb, DATA_FLAGS);\n\t\tsbi->s_journal = NULL;\n\t\tneeds_recovery = 0;\n\t\tgoto no_journal;\n\t}\n\n\tif (ext4_has_feature_64bit(sb) &&\n\t    !jbd2_journal_set_features(EXT4_SB(sb)->s_journal, 0, 0,\n\t\t\t\t       JBD2_FEATURE_INCOMPAT_64BIT)) {\n\t\text4_msg(sb, KERN_ERR, \"Failed to set 64-bit journal feature\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tif (!set_journal_csum_feature_set(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"Failed to set journal checksum \"\n\t\t\t \"feature set\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\t/* We have now updated the journal if required, so we can\n\t * validate the data journaling mode. */\n\tswitch (test_opt(sb, DATA_FLAGS)) {\n\tcase 0:\n\t\t/* No mode set, assume a default based on the journal\n\t\t * capabilities: ORDERED_DATA if the journal can\n\t\t * cope, else JOURNAL_DATA\n\t\t */\n\t\tif (jbd2_journal_check_available_features\n\t\t    (sbi->s_journal, 0, 0, JBD2_FEATURE_INCOMPAT_REVOKE)) {\n\t\t\tset_opt(sb, ORDERED_DATA);\n\t\t\tsbi->s_def_mount_opt |= EXT4_MOUNT_ORDERED_DATA;\n\t\t} else {\n\t\t\tset_opt(sb, JOURNAL_DATA);\n\t\t\tsbi->s_def_mount_opt |= EXT4_MOUNT_JOURNAL_DATA;\n\t\t}\n\t\tbreak;\n\n\tcase EXT4_MOUNT_ORDERED_DATA:\n\tcase EXT4_MOUNT_WRITEBACK_DATA:\n\t\tif (!jbd2_journal_check_available_features\n\t\t    (sbi->s_journal, 0, 0, JBD2_FEATURE_INCOMPAT_REVOKE)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Journal does not support \"\n\t\t\t       \"requested data journaling mode\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA &&\n\t    test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\"journal_async_commit in data=ordered mode\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tset_task_ioprio(sbi->s_journal->j_task, journal_ioprio);\n\n\tsbi->s_journal->j_commit_callback = ext4_journal_commit_callback;\n\nno_journal:\n\tif (!test_opt(sb, NO_MBCACHE)) {\n\t\tsbi->s_ea_block_cache = ext4_xattr_create_cache();\n\t\tif (!sbi->s_ea_block_cache) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"Failed to create ea_block_cache\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\n\t\tif (ext4_has_feature_ea_inode(sb)) {\n\t\t\tsbi->s_ea_inode_cache = ext4_xattr_create_cache();\n\t\t\tif (!sbi->s_ea_inode_cache) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t \"Failed to create ea_inode_cache\");\n\t\t\t\tgoto failed_mount_wq;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((DUMMY_ENCRYPTION_ENABLED(sbi) || ext4_has_feature_encrypt(sb)) &&\n\t    (blocksize != PAGE_SIZE)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Unsupported blocksize for fs encryption\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tif (ext4_has_feature_verity(sb) && blocksize != PAGE_SIZE) {\n\t\text4_msg(sb, KERN_ERR, \"Unsupported blocksize for fs-verity\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tif (DUMMY_ENCRYPTION_ENABLED(sbi) && !sb_rdonly(sb) &&\n\t    !ext4_has_feature_encrypt(sb)) {\n\t\text4_set_feature_encrypt(sb);\n\t\text4_commit_super(sb, 1);\n\t}\n\n\t/*\n\t * Get the # of file system overhead blocks from the\n\t * superblock if present.\n\t */\n\tif (es->s_overhead_clusters)\n\t\tsbi->s_overhead = le32_to_cpu(es->s_overhead_clusters);\n\telse {\n\t\terr = ext4_calculate_overhead(sb);\n\t\tif (err)\n\t\t\tgoto failed_mount_wq;\n\t}\n\n\t/*\n\t * The maximum number of concurrent works can be high and\n\t * concurrency isn't really necessary.  Limit it to 1.\n\t */\n\tEXT4_SB(sb)->rsv_conversion_wq =\n\t\talloc_workqueue(\"ext4-rsv-conversion\", WQ_MEM_RECLAIM | WQ_UNBOUND, 1);\n\tif (!EXT4_SB(sb)->rsv_conversion_wq) {\n\t\tprintk(KERN_ERR \"EXT4-fs: failed to create workqueue\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto failed_mount4;\n\t}\n\n\t/*\n\t * The jbd2_journal_load will have done any necessary log recovery,\n\t * so we can safely mount the rest of the filesystem now.\n\t */\n\n\troot = ext4_iget(sb, EXT4_ROOT_INO, EXT4_IGET_SPECIAL);\n\tif (IS_ERR(root)) {\n\t\text4_msg(sb, KERN_ERR, \"get root inode failed\");\n\t\tret = PTR_ERR(root);\n\t\troot = NULL;\n\t\tgoto failed_mount4;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) {\n\t\text4_msg(sb, KERN_ERR, \"corrupt root inode, run e2fsck\");\n\t\tiput(root);\n\t\tgoto failed_mount4;\n\t}\n\n#ifdef CONFIG_UNICODE\n\tif (sbi->s_encoding)\n\t\tsb->s_d_op = &ext4_dentry_ops;\n#endif\n\n\tsb->s_root = d_make_root(root);\n\tif (!sb->s_root) {\n\t\text4_msg(sb, KERN_ERR, \"get root dentry failed\");\n\t\tret = -ENOMEM;\n\t\tgoto failed_mount4;\n\t}\n\n\tret = ext4_setup_super(sb, es, sb_rdonly(sb));\n\tif (ret == -EROFS) {\n\t\tsb->s_flags |= SB_RDONLY;\n\t\tret = 0;\n\t} else if (ret)\n\t\tgoto failed_mount4a;\n\n\text4_clamp_want_extra_isize(sb);\n\n\text4_set_resv_clusters(sb);\n\n\terr = ext4_setup_system_zone(sb);\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"failed to initialize system \"\n\t\t\t \"zone (%d)\", err);\n\t\tgoto failed_mount4a;\n\t}\n\n\text4_ext_init(sb);\n\terr = ext4_mb_init(sb);\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"failed to initialize mballoc (%d)\",\n\t\t\t err);\n\t\tgoto failed_mount5;\n\t}\n\n\tblock = ext4_count_free_clusters(sb);\n\text4_free_blocks_count_set(sbi->s_es, \n\t\t\t\t   EXT4_C2B(sbi, block));\n\text4_superblock_csum_set(sb);\n\terr = percpu_counter_init(&sbi->s_freeclusters_counter, block,\n\t\t\t\t  GFP_KERNEL);\n\tif (!err) {\n\t\tunsigned long freei = ext4_count_free_inodes(sb);\n\t\tsbi->s_es->s_free_inodes_count = cpu_to_le32(freei);\n\t\text4_superblock_csum_set(sb);\n\t\terr = percpu_counter_init(&sbi->s_freeinodes_counter, freei,\n\t\t\t\t\t  GFP_KERNEL);\n\t}\n\tif (!err)\n\t\terr = percpu_counter_init(&sbi->s_dirs_counter,\n\t\t\t\t\t  ext4_count_dirs(sb), GFP_KERNEL);\n\tif (!err)\n\t\terr = percpu_counter_init(&sbi->s_dirtyclusters_counter, 0,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!err)\n\t\terr = percpu_init_rwsem(&sbi->s_journal_flag_rwsem);\n\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"insufficient memory\");\n\t\tgoto failed_mount6;\n\t}\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tif (!ext4_fill_flex_info(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unable to initialize \"\n\t\t\t       \"flex_bg meta info!\");\n\t\t\tgoto failed_mount6;\n\t\t}\n\n\terr = ext4_register_li_request(sb, first_not_zeroed);\n\tif (err)\n\t\tgoto failed_mount6;\n\n\terr = ext4_register_sysfs(sb);\n\tif (err)\n\t\tgoto failed_mount7;\n\n#ifdef CONFIG_QUOTA\n\t/* Enable quota usage during mount. */\n\tif (ext4_has_feature_quota(sb) && !sb_rdonly(sb)) {\n\t\terr = ext4_enable_quotas(sb);\n\t\tif (err)\n\t\t\tgoto failed_mount8;\n\t}\n#endif  /* CONFIG_QUOTA */\n\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ORPHAN_FS;\n\text4_orphan_cleanup(sb, es);\n\tEXT4_SB(sb)->s_mount_state &= ~EXT4_ORPHAN_FS;\n\tif (needs_recovery) {\n\t\text4_msg(sb, KERN_INFO, \"recovery complete\");\n\t\text4_mark_recovery_complete(sb, es);\n\t}\n\tif (EXT4_SB(sb)->s_journal) {\n\t\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\t\tdescr = \" journalled data mode\";\n\t\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\t\tdescr = \" ordered data mode\";\n\t\telse\n\t\t\tdescr = \" writeback data mode\";\n\t} else\n\t\tdescr = \"out journal\";\n\n\tif (test_opt(sb, DISCARD)) {\n\t\tstruct request_queue *q = bdev_get_queue(sb->s_bdev);\n\t\tif (!blk_queue_discard(q))\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t \"mounting with \\\"discard\\\" option, but \"\n\t\t\t\t \"the device does not support discard\");\n\t}\n\n\tif (___ratelimit(&ext4_mount_msg_ratelimit, \"EXT4-fs mount\"))\n\t\text4_msg(sb, KERN_INFO, \"mounted filesystem with%s. \"\n\t\t\t \"Opts: %.*s%s%s\", descr,\n\t\t\t (int) sizeof(sbi->s_es->s_mount_opts),\n\t\t\t sbi->s_es->s_mount_opts,\n\t\t\t *sbi->s_es->s_mount_opts ? \"; \" : \"\", orig_data);\n\n\tif (es->s_error_count)\n\t\tmod_timer(&sbi->s_err_report, jiffies + 300*HZ); /* 5 minutes */\n\n\t/* Enable message ratelimiting. Default is 10 messages per 5 secs. */\n\tratelimit_state_init(&sbi->s_err_ratelimit_state, 5 * HZ, 10);\n\tratelimit_state_init(&sbi->s_warning_ratelimit_state, 5 * HZ, 10);\n\tratelimit_state_init(&sbi->s_msg_ratelimit_state, 5 * HZ, 10);\n\n\tkfree(orig_data);\n\treturn 0;\n\ncantfind_ext4:\n\tif (!silent)\n\t\text4_msg(sb, KERN_ERR, \"VFS: Can't find ext4 filesystem\");\n\tgoto failed_mount;\n\n#ifdef CONFIG_QUOTA\nfailed_mount8:\n\text4_unregister_sysfs(sb);\n#endif\nfailed_mount7:\n\text4_unregister_li_request(sb);\nfailed_mount6:\n\text4_mb_release(sb);\n\tif (sbi->s_flex_groups)\n\t\tkvfree(sbi->s_flex_groups);\n\tpercpu_counter_destroy(&sbi->s_freeclusters_counter);\n\tpercpu_counter_destroy(&sbi->s_freeinodes_counter);\n\tpercpu_counter_destroy(&sbi->s_dirs_counter);\n\tpercpu_counter_destroy(&sbi->s_dirtyclusters_counter);\n\tpercpu_free_rwsem(&sbi->s_journal_flag_rwsem);\nfailed_mount5:\n\text4_ext_release(sb);\n\text4_release_system_zone(sb);\nfailed_mount4a:\n\tdput(sb->s_root);\n\tsb->s_root = NULL;\nfailed_mount4:\n\text4_msg(sb, KERN_ERR, \"mount failed\");\n\tif (EXT4_SB(sb)->rsv_conversion_wq)\n\t\tdestroy_workqueue(EXT4_SB(sb)->rsv_conversion_wq);\nfailed_mount_wq:\n\text4_xattr_destroy_cache(sbi->s_ea_inode_cache);\n\tsbi->s_ea_inode_cache = NULL;\n\n\text4_xattr_destroy_cache(sbi->s_ea_block_cache);\n\tsbi->s_ea_block_cache = NULL;\n\n\tif (sbi->s_journal) {\n\t\tjbd2_journal_destroy(sbi->s_journal);\n\t\tsbi->s_journal = NULL;\n\t}\nfailed_mount3a:\n\text4_es_unregister_shrinker(sbi);\nfailed_mount3:\n\tdel_timer_sync(&sbi->s_err_report);\n\tif (sbi->s_mmp_tsk)\n\t\tkthread_stop(sbi->s_mmp_tsk);\nfailed_mount2:\n\tfor (i = 0; i < db_count; i++)\n\t\tbrelse(sbi->s_group_desc[i]);\n\tkvfree(sbi->s_group_desc);\nfailed_mount:\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\n#ifdef CONFIG_UNICODE\n\tutf8_unload(sbi->s_encoding);\n#endif\n\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tkfree(get_qf_name(sb, sbi, i));\n#endif\n\text4_blkdev_remove(sbi);\n\tbrelse(bh);\nout_fail:\n\tsb->s_fs_info = NULL;\n\tkfree(sbi->s_blockgroup_lock);\nout_free_base:\n\tkfree(sbi);\n\tkfree(orig_data);\n\tfs_put_dax(dax_dev);\n\treturn err ? err : ret;\n}\n\n/*\n * Setup any per-fs journal parameters now.  We'll do this both on\n * initial mount, once the journal has been initialised but before we've\n * done any recovery; and again on any subsequent remount.\n */\nstatic void ext4_init_journal_params(struct super_block *sb, journal_t *journal)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tjournal->j_commit_interval = sbi->s_commit_interval;\n\tjournal->j_min_batch_time = sbi->s_min_batch_time;\n\tjournal->j_max_batch_time = sbi->s_max_batch_time;\n\n\twrite_lock(&journal->j_state_lock);\n\tif (test_opt(sb, BARRIER))\n\t\tjournal->j_flags |= JBD2_BARRIER;\n\telse\n\t\tjournal->j_flags &= ~JBD2_BARRIER;\n\tif (test_opt(sb, DATA_ERR_ABORT))\n\t\tjournal->j_flags |= JBD2_ABORT_ON_SYNCDATA_ERR;\n\telse\n\t\tjournal->j_flags &= ~JBD2_ABORT_ON_SYNCDATA_ERR;\n\twrite_unlock(&journal->j_state_lock);\n}\n\nstatic struct inode *ext4_get_journal_inode(struct super_block *sb,\n\t\t\t\t\t     unsigned int journal_inum)\n{\n\tstruct inode *journal_inode;\n\n\t/*\n\t * Test for the existence of a valid inode on disk.  Bad things\n\t * happen if we iget() an unused inode, as the subsequent iput()\n\t * will try to delete it.\n\t */\n\tjournal_inode = ext4_iget(sb, journal_inum, EXT4_IGET_SPECIAL);\n\tif (IS_ERR(journal_inode)) {\n\t\text4_msg(sb, KERN_ERR, \"no journal found\");\n\t\treturn NULL;\n\t}\n\tif (!journal_inode->i_nlink) {\n\t\tmake_bad_inode(journal_inode);\n\t\tiput(journal_inode);\n\t\text4_msg(sb, KERN_ERR, \"journal inode is deleted\");\n\t\treturn NULL;\n\t}\n\n\tjbd_debug(2, \"Journal inode found at %p: %lld bytes\\n\",\n\t\t  journal_inode, journal_inode->i_size);\n\tif (!S_ISREG(journal_inode->i_mode)) {\n\t\text4_msg(sb, KERN_ERR, \"invalid journal inode\");\n\t\tiput(journal_inode);\n\t\treturn NULL;\n\t}\n\treturn journal_inode;\n}\n\nstatic journal_t *ext4_get_journal(struct super_block *sb,\n\t\t\t\t   unsigned int journal_inum)\n{\n\tstruct inode *journal_inode;\n\tjournal_t *journal;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tjournal_inode = ext4_get_journal_inode(sb, journal_inum);\n\tif (!journal_inode)\n\t\treturn NULL;\n\n\tjournal = jbd2_journal_init_inode(journal_inode);\n\tif (!journal) {\n\t\text4_msg(sb, KERN_ERR, \"Could not load journal inode\");\n\t\tiput(journal_inode);\n\t\treturn NULL;\n\t}\n\tjournal->j_private = sb;\n\text4_init_journal_params(sb, journal);\n\treturn journal;\n}\n\nstatic journal_t *ext4_get_dev_journal(struct super_block *sb,\n\t\t\t\t       dev_t j_dev)\n{\n\tstruct buffer_head *bh;\n\tjournal_t *journal;\n\text4_fsblk_t start;\n\text4_fsblk_t len;\n\tint hblock, blocksize;\n\text4_fsblk_t sb_block;\n\tunsigned long offset;\n\tstruct ext4_super_block *es;\n\tstruct block_device *bdev;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tbdev = ext4_blkdev_get(j_dev, sb);\n\tif (bdev == NULL)\n\t\treturn NULL;\n\n\tblocksize = sb->s_blocksize;\n\thblock = bdev_logical_block_size(bdev);\n\tif (blocksize < hblock) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"blocksize too small for journal device\");\n\t\tgoto out_bdev;\n\t}\n\n\tsb_block = EXT4_MIN_BLOCK_SIZE / blocksize;\n\toffset = EXT4_MIN_BLOCK_SIZE % blocksize;\n\tset_blocksize(bdev, blocksize);\n\tif (!(bh = __bread(bdev, sb_block, blocksize))) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't read superblock of \"\n\t\t       \"external journal\");\n\t\tgoto out_bdev;\n\t}\n\n\tes = (struct ext4_super_block *) (bh->b_data + offset);\n\tif ((le16_to_cpu(es->s_magic) != EXT4_SUPER_MAGIC) ||\n\t    !(le32_to_cpu(es->s_feature_incompat) &\n\t      EXT4_FEATURE_INCOMPAT_JOURNAL_DEV)) {\n\t\text4_msg(sb, KERN_ERR, \"external journal has \"\n\t\t\t\t\t\"bad superblock\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tif ((le32_to_cpu(es->s_feature_ro_compat) &\n\t     EXT4_FEATURE_RO_COMPAT_METADATA_CSUM) &&\n\t    es->s_checksum != ext4_superblock_csum(sb, es)) {\n\t\text4_msg(sb, KERN_ERR, \"external journal has \"\n\t\t\t\t       \"corrupt superblock\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tif (memcmp(EXT4_SB(sb)->s_es->s_journal_uuid, es->s_uuid, 16)) {\n\t\text4_msg(sb, KERN_ERR, \"journal UUID does not match\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tlen = ext4_blocks_count(es);\n\tstart = sb_block + 1;\n\tbrelse(bh);\t/* we're done with the superblock */\n\n\tjournal = jbd2_journal_init_dev(bdev, sb->s_bdev,\n\t\t\t\t\tstart, len, blocksize);\n\tif (!journal) {\n\t\text4_msg(sb, KERN_ERR, \"failed to create device journal\");\n\t\tgoto out_bdev;\n\t}\n\tjournal->j_private = sb;\n\tll_rw_block(REQ_OP_READ, REQ_META | REQ_PRIO, 1, &journal->j_sb_buffer);\n\twait_on_buffer(journal->j_sb_buffer);\n\tif (!buffer_uptodate(journal->j_sb_buffer)) {\n\t\text4_msg(sb, KERN_ERR, \"I/O error on journal device\");\n\t\tgoto out_journal;\n\t}\n\tif (be32_to_cpu(journal->j_superblock->s_nr_users) != 1) {\n\t\text4_msg(sb, KERN_ERR, \"External journal has more than one \"\n\t\t\t\t\t\"user (unsupported) - %d\",\n\t\t\tbe32_to_cpu(journal->j_superblock->s_nr_users));\n\t\tgoto out_journal;\n\t}\n\tEXT4_SB(sb)->journal_bdev = bdev;\n\text4_init_journal_params(sb, journal);\n\treturn journal;\n\nout_journal:\n\tjbd2_journal_destroy(journal);\nout_bdev:\n\text4_blkdev_put(bdev);\n\treturn NULL;\n}\n\nstatic int ext4_load_journal(struct super_block *sb,\n\t\t\t     struct ext4_super_block *es,\n\t\t\t     unsigned long journal_devnum)\n{\n\tjournal_t *journal;\n\tunsigned int journal_inum = le32_to_cpu(es->s_journal_inum);\n\tdev_t journal_dev;\n\tint err = 0;\n\tint really_read_only;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tif (journal_devnum &&\n\t    journal_devnum != le32_to_cpu(es->s_journal_dev)) {\n\t\text4_msg(sb, KERN_INFO, \"external journal device major/minor \"\n\t\t\t\"numbers have changed\");\n\t\tjournal_dev = new_decode_dev(journal_devnum);\n\t} else\n\t\tjournal_dev = new_decode_dev(le32_to_cpu(es->s_journal_dev));\n\n\treally_read_only = bdev_read_only(sb->s_bdev);\n\n\t/*\n\t * Are we loading a blank journal or performing recovery after a\n\t * crash?  For recovery, we need to check in advance whether we\n\t * can get read-write access to the device.\n\t */\n\tif (ext4_has_feature_journal_needs_recovery(sb)) {\n\t\tif (sb_rdonly(sb)) {\n\t\t\text4_msg(sb, KERN_INFO, \"INFO: recovery \"\n\t\t\t\t\t\"required on readonly filesystem\");\n\t\t\tif (really_read_only) {\n\t\t\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\t\t\"unavailable, cannot proceed \"\n\t\t\t\t\t\"(try mounting with noload)\");\n\t\t\t\treturn -EROFS;\n\t\t\t}\n\t\t\text4_msg(sb, KERN_INFO, \"write access will \"\n\t\t\t       \"be enabled during recovery\");\n\t\t}\n\t}\n\n\tif (journal_inum && journal_dev) {\n\t\text4_msg(sb, KERN_ERR, \"filesystem has both journal \"\n\t\t       \"and inode journals!\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (journal_inum) {\n\t\tif (!(journal = ext4_get_journal(sb, journal_inum)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (!(journal = ext4_get_dev_journal(sb, journal_dev)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!(journal->j_flags & JBD2_BARRIER))\n\t\text4_msg(sb, KERN_INFO, \"barriers disabled\");\n\n\tif (!ext4_has_feature_journal_needs_recovery(sb))\n\t\terr = jbd2_journal_wipe(journal, !really_read_only);\n\tif (!err) {\n\t\tchar *save = kmalloc(EXT4_S_ERR_LEN, GFP_KERNEL);\n\t\tif (save)\n\t\t\tmemcpy(save, ((char *) es) +\n\t\t\t       EXT4_S_ERR_START, EXT4_S_ERR_LEN);\n\t\terr = jbd2_journal_load(journal);\n\t\tif (save)\n\t\t\tmemcpy(((char *) es) + EXT4_S_ERR_START,\n\t\t\t       save, EXT4_S_ERR_LEN);\n\t\tkfree(save);\n\t}\n\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"error loading journal\");\n\t\tjbd2_journal_destroy(journal);\n\t\treturn err;\n\t}\n\n\tEXT4_SB(sb)->s_journal = journal;\n\text4_clear_journal_err(sb, es);\n\n\tif (!really_read_only && journal_devnum &&\n\t    journal_devnum != le32_to_cpu(es->s_journal_dev)) {\n\t\tes->s_journal_dev = cpu_to_le32(journal_devnum);\n\n\t\t/* Make sure we flush the recovery flag to disk. */\n\t\text4_commit_super(sb, 1);\n\t}\n\n\treturn 0;\n}\n\nstatic int ext4_commit_super(struct super_block *sb, int sync)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\tstruct buffer_head *sbh = EXT4_SB(sb)->s_sbh;\n\tint error = 0;\n\n\tif (!sbh || block_device_ejected(sb))\n\t\treturn error;\n\n\t/*\n\t * The superblock bh should be mapped, but it might not be if the\n\t * device was hot-removed. Not much we can do but fail the I/O.\n\t */\n\tif (!buffer_mapped(sbh))\n\t\treturn error;\n\n\t/*\n\t * If the file system is mounted read-only, don't update the\n\t * superblock write time.  This avoids updating the superblock\n\t * write time when we are mounting the root file system\n\t * read/only but we need to replay the journal; at that point,\n\t * for people who are east of GMT and who make their clock\n\t * tick in localtime for Windows bug-for-bug compatibility,\n\t * the clock is set in the future, and this will cause e2fsck\n\t * to complain and force a full file system check.\n\t */\n\tif (!(sb->s_flags & SB_RDONLY))\n\t\text4_update_tstamp(es, s_wtime);\n\tif (sb->s_bdev->bd_part)\n\t\tes->s_kbytes_written =\n\t\t\tcpu_to_le64(EXT4_SB(sb)->s_kbytes_written +\n\t\t\t    ((part_stat_read(sb->s_bdev->bd_part,\n\t\t\t\t\t     sectors[STAT_WRITE]) -\n\t\t\t      EXT4_SB(sb)->s_sectors_written_start) >> 1));\n\telse\n\t\tes->s_kbytes_written =\n\t\t\tcpu_to_le64(EXT4_SB(sb)->s_kbytes_written);\n\tif (percpu_counter_initialized(&EXT4_SB(sb)->s_freeclusters_counter))\n\t\text4_free_blocks_count_set(es,\n\t\t\tEXT4_C2B(EXT4_SB(sb), percpu_counter_sum_positive(\n\t\t\t\t&EXT4_SB(sb)->s_freeclusters_counter)));\n\tif (percpu_counter_initialized(&EXT4_SB(sb)->s_freeinodes_counter))\n\t\tes->s_free_inodes_count =\n\t\t\tcpu_to_le32(percpu_counter_sum_positive(\n\t\t\t\t&EXT4_SB(sb)->s_freeinodes_counter));\n\tBUFFER_TRACE(sbh, \"marking dirty\");\n\text4_superblock_csum_set(sb);\n\tif (sync)\n\t\tlock_buffer(sbh);\n\tif (buffer_write_io_error(sbh) || !buffer_uptodate(sbh)) {\n\t\t/*\n\t\t * Oh, dear.  A previous attempt to write the\n\t\t * superblock failed.  This could happen because the\n\t\t * USB device was yanked out.  Or it could happen to\n\t\t * be a transient write error and maybe the block will\n\t\t * be remapped.  Nothing we can do but to retry the\n\t\t * write and hope for the best.\n\t\t */\n\t\text4_msg(sb, KERN_ERR, \"previous I/O error to \"\n\t\t       \"superblock detected\");\n\t\tclear_buffer_write_io_error(sbh);\n\t\tset_buffer_uptodate(sbh);\n\t}\n\tmark_buffer_dirty(sbh);\n\tif (sync) {\n\t\tunlock_buffer(sbh);\n\t\terror = __sync_dirty_buffer(sbh,\n\t\t\tREQ_SYNC | (test_opt(sb, BARRIER) ? REQ_FUA : 0));\n\t\tif (buffer_write_io_error(sbh)) {\n\t\t\text4_msg(sb, KERN_ERR, \"I/O error while writing \"\n\t\t\t       \"superblock\");\n\t\t\tclear_buffer_write_io_error(sbh);\n\t\t\tset_buffer_uptodate(sbh);\n\t\t}\n\t}\n\treturn error;\n}\n\n/*\n * Have we just finished recovery?  If so, and if we are mounting (or\n * remounting) the filesystem readonly, then we will end up with a\n * consistent fs on disk.  Record that fact.\n */\nstatic void ext4_mark_recovery_complete(struct super_block *sb,\n\t\t\t\t\tstruct ext4_super_block *es)\n{\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\tif (!ext4_has_feature_journal(sb)) {\n\t\tBUG_ON(journal != NULL);\n\t\treturn;\n\t}\n\tjbd2_journal_lock_updates(journal);\n\tif (jbd2_journal_flush(journal) < 0)\n\t\tgoto out;\n\n\tif (ext4_has_feature_journal_needs_recovery(sb) && sb_rdonly(sb)) {\n\t\text4_clear_feature_journal_needs_recovery(sb);\n\t\text4_commit_super(sb, 1);\n\t}\n\nout:\n\tjbd2_journal_unlock_updates(journal);\n}\n\n/*\n * If we are mounting (or read-write remounting) a filesystem whose journal\n * has recorded an error from a previous lifetime, move that error to the\n * main filesystem now.\n */\nstatic void ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es)\n{\n\tjournal_t *journal;\n\tint j_errno;\n\tconst char *errstr;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\t/*\n\t * Now check for any error status which may have been recorded in the\n\t * journal by a prior ext4_error() or ext4_abort()\n\t */\n\n\tj_errno = jbd2_journal_errno(journal);\n\tif (j_errno) {\n\t\tchar nbuf[16];\n\n\t\terrstr = ext4_decode_error(sb, j_errno, nbuf);\n\t\text4_warning(sb, \"Filesystem error recorded \"\n\t\t\t     \"from previous mount: %s\", errstr);\n\t\text4_warning(sb, \"Marking fs in need of filesystem check.\");\n\n\t\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\t\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\t\text4_commit_super(sb, 1);\n\n\t\tjbd2_journal_clear_err(journal);\n\t\tjbd2_journal_update_sb_errno(journal);\n\t}\n}\n\n/*\n * Force the running and committing transactions to commit,\n * and wait on the commit.\n */\nint ext4_force_commit(struct super_block *sb)\n{\n\tjournal_t *journal;\n\n\tif (sb_rdonly(sb))\n\t\treturn 0;\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\treturn ext4_journal_force_commit(journal);\n}\n\nstatic int ext4_sync_fs(struct super_block *sb, int wait)\n{\n\tint ret = 0;\n\ttid_t target;\n\tbool needs_barrier = false;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (unlikely(ext4_forced_shutdown(sbi)))\n\t\treturn 0;\n\n\ttrace_ext4_sync_fs(sb, wait);\n\tflush_workqueue(sbi->rsv_conversion_wq);\n\t/*\n\t * Writeback quota in non-journalled quota case - journalled quota has\n\t * no dirty dquots\n\t */\n\tdquot_writeback_dquots(sb, -1);\n\t/*\n\t * Data writeback is possible w/o journal transaction, so barrier must\n\t * being sent at the end of the function. But we can skip it if\n\t * transaction_commit will do it for us.\n\t */\n\tif (sbi->s_journal) {\n\t\ttarget = jbd2_get_latest_transaction(sbi->s_journal);\n\t\tif (wait && sbi->s_journal->j_flags & JBD2_BARRIER &&\n\t\t    !jbd2_trans_will_send_data_barrier(sbi->s_journal, target))\n\t\t\tneeds_barrier = true;\n\n\t\tif (jbd2_journal_start_commit(sbi->s_journal, &target)) {\n\t\t\tif (wait)\n\t\t\t\tret = jbd2_log_wait_commit(sbi->s_journal,\n\t\t\t\t\t\t\t   target);\n\t\t}\n\t} else if (wait && test_opt(sb, BARRIER))\n\t\tneeds_barrier = true;\n\tif (needs_barrier) {\n\t\tint err;\n\t\terr = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);\n\t\tif (!ret)\n\t\t\tret = err;\n\t}\n\n\treturn ret;\n}\n\n/*\n * LVM calls this function before a (read-only) snapshot is created.  This\n * gives us a chance to flush the journal completely and mark the fs clean.\n *\n * Note that only this function cannot bring a filesystem to be in a clean\n * state independently. It relies on upper layer to stop all data & metadata\n * modifications.\n */\nstatic int ext4_freeze(struct super_block *sb)\n{\n\tint error = 0;\n\tjournal_t *journal;\n\n\tif (sb_rdonly(sb))\n\t\treturn 0;\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\tif (journal) {\n\t\t/* Now we set up the journal barrier. */\n\t\tjbd2_journal_lock_updates(journal);\n\n\t\t/*\n\t\t * Don't clear the needs_recovery flag if we failed to\n\t\t * flush the journal.\n\t\t */\n\t\terror = jbd2_journal_flush(journal);\n\t\tif (error < 0)\n\t\t\tgoto out;\n\n\t\t/* Journal blocked and flushed, clear needs_recovery flag. */\n\t\text4_clear_feature_journal_needs_recovery(sb);\n\t}\n\n\terror = ext4_commit_super(sb, 1);\nout:\n\tif (journal)\n\t\t/* we rely on upper layer to stop further updates */\n\t\tjbd2_journal_unlock_updates(journal);\n\treturn error;\n}\n\n/*\n * Called by LVM after the snapshot is done.  We need to reset the RECOVER\n * flag here, even though the filesystem is not technically dirty yet.\n */\nstatic int ext4_unfreeze(struct super_block *sb)\n{\n\tif (sb_rdonly(sb) || ext4_forced_shutdown(EXT4_SB(sb)))\n\t\treturn 0;\n\n\tif (EXT4_SB(sb)->s_journal) {\n\t\t/* Reset the needs_recovery flag before the fs is unlocked. */\n\t\text4_set_feature_journal_needs_recovery(sb);\n\t}\n\n\text4_commit_super(sb, 1);\n\treturn 0;\n}\n\n/*\n * Structure to save mount options for ext4_remount's benefit\n */\nstruct ext4_mount_options {\n\tunsigned long s_mount_opt;\n\tunsigned long s_mount_opt2;\n\tkuid_t s_resuid;\n\tkgid_t s_resgid;\n\tunsigned long s_commit_interval;\n\tu32 s_min_batch_time, s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\tint s_jquota_fmt;\n\tchar *s_qf_names[EXT4_MAXQUOTAS];\n#endif\n};\n\nstatic int ext4_remount(struct super_block *sb, int *flags, char *data)\n{\n\tstruct ext4_super_block *es;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tunsigned long old_sb_flags;\n\tstruct ext4_mount_options old_opts;\n\tint enable_quota = 0;\n\text4_group_t g;\n\tunsigned int journal_ioprio = DEFAULT_JOURNAL_IOPRIO;\n\tint err = 0;\n#ifdef CONFIG_QUOTA\n\tint i, j;\n\tchar *to_free[EXT4_MAXQUOTAS];\n#endif\n\tchar *orig_data = kstrdup(data, GFP_KERNEL);\n\n\tif (data && !orig_data)\n\t\treturn -ENOMEM;\n\n\t/* Store the original options */\n\told_sb_flags = sb->s_flags;\n\told_opts.s_mount_opt = sbi->s_mount_opt;\n\told_opts.s_mount_opt2 = sbi->s_mount_opt2;\n\told_opts.s_resuid = sbi->s_resuid;\n\told_opts.s_resgid = sbi->s_resgid;\n\told_opts.s_commit_interval = sbi->s_commit_interval;\n\told_opts.s_min_batch_time = sbi->s_min_batch_time;\n\told_opts.s_max_batch_time = sbi->s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\told_opts.s_jquota_fmt = sbi->s_jquota_fmt;\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tif (sbi->s_qf_names[i]) {\n\t\t\tchar *qf_name = get_qf_name(sb, sbi, i);\n\n\t\t\told_opts.s_qf_names[i] = kstrdup(qf_name, GFP_KERNEL);\n\t\t\tif (!old_opts.s_qf_names[i]) {\n\t\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\t\tkfree(old_opts.s_qf_names[j]);\n\t\t\t\tkfree(orig_data);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else\n\t\t\told_opts.s_qf_names[i] = NULL;\n#endif\n\tif (sbi->s_journal && sbi->s_journal->j_task->io_context)\n\t\tjournal_ioprio = sbi->s_journal->j_task->io_context->ioprio;\n\n\tif (!parse_options(data, sb, NULL, &journal_ioprio, 1)) {\n\t\terr = -EINVAL;\n\t\tgoto restore_opts;\n\t}\n\n\text4_clamp_want_extra_isize(sb);\n\n\tif ((old_opts.s_mount_opt & EXT4_MOUNT_JOURNAL_CHECKSUM) ^\n\t    test_opt(sb, JOURNAL_CHECKSUM)) {\n\t\text4_msg(sb, KERN_ERR, \"changing journal_checksum \"\n\t\t\t \"during remount not supported; ignoring\");\n\t\tsbi->s_mount_opt ^= EXT4_MOUNT_JOURNAL_CHECKSUM;\n\t}\n\n\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) {\n\t\tif (test_opt2(sb, EXPLICIT_DELALLOC)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and delalloc\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto restore_opts;\n\t\t}\n\t\tif (test_opt(sb, DIOREAD_NOLOCK)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and dioread_nolock\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto restore_opts;\n\t\t}\n\t\tif (test_opt(sb, DAX)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t \"both data=journal and dax\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto restore_opts;\n\t\t}\n\t} else if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA) {\n\t\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't mount with \"\n\t\t\t\t\"journal_async_commit in data=ordered mode\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto restore_opts;\n\t\t}\n\t}\n\n\tif ((sbi->s_mount_opt ^ old_opts.s_mount_opt) & EXT4_MOUNT_NO_MBCACHE) {\n\t\text4_msg(sb, KERN_ERR, \"can't enable nombcache during remount\");\n\t\terr = -EINVAL;\n\t\tgoto restore_opts;\n\t}\n\n\tif ((sbi->s_mount_opt ^ old_opts.s_mount_opt) & EXT4_MOUNT_DAX) {\n\t\text4_msg(sb, KERN_WARNING, \"warning: refusing change of \"\n\t\t\t\"dax flag with busy inodes while remounting\");\n\t\tsbi->s_mount_opt ^= EXT4_MOUNT_DAX;\n\t}\n\n\tif (sbi->s_mount_flags & EXT4_MF_FS_ABORTED)\n\t\text4_abort(sb, \"Abort forced by user\");\n\n\tsb->s_flags = (sb->s_flags & ~SB_POSIXACL) |\n\t\t(test_opt(sb, POSIX_ACL) ? SB_POSIXACL : 0);\n\n\tes = sbi->s_es;\n\n\tif (sbi->s_journal) {\n\t\text4_init_journal_params(sb, sbi->s_journal);\n\t\tset_task_ioprio(sbi->s_journal->j_task, journal_ioprio);\n\t}\n\n\tif (*flags & SB_LAZYTIME)\n\t\tsb->s_flags |= SB_LAZYTIME;\n\n\tif ((bool)(*flags & SB_RDONLY) != sb_rdonly(sb)) {\n\t\tif (sbi->s_mount_flags & EXT4_MF_FS_ABORTED) {\n\t\t\terr = -EROFS;\n\t\t\tgoto restore_opts;\n\t\t}\n\n\t\tif (*flags & SB_RDONLY) {\n\t\t\terr = sync_filesystem(sb);\n\t\t\tif (err < 0)\n\t\t\t\tgoto restore_opts;\n\t\t\terr = dquot_suspend(sb, -1);\n\t\t\tif (err < 0)\n\t\t\t\tgoto restore_opts;\n\n\t\t\t/*\n\t\t\t * First of all, the unconditional stuff we have to do\n\t\t\t * to disable replay of the journal when we next remount\n\t\t\t */\n\t\t\tsb->s_flags |= SB_RDONLY;\n\n\t\t\t/*\n\t\t\t * OK, test if we are remounting a valid rw partition\n\t\t\t * readonly, and if so set the rdonly flag and then\n\t\t\t * mark the partition as valid again.\n\t\t\t */\n\t\t\tif (!(es->s_state & cpu_to_le16(EXT4_VALID_FS)) &&\n\t\t\t    (sbi->s_mount_state & EXT4_VALID_FS))\n\t\t\t\tes->s_state = cpu_to_le16(sbi->s_mount_state);\n\n\t\t\tif (sbi->s_journal)\n\t\t\t\text4_mark_recovery_complete(sb, es);\n\t\t\tif (sbi->s_mmp_tsk)\n\t\t\t\tkthread_stop(sbi->s_mmp_tsk);\n\t\t} else {\n\t\t\t/* Make sure we can mount this feature set readwrite */\n\t\t\tif (ext4_has_feature_readonly(sb) ||\n\t\t\t    !ext4_feature_set_ok(sb, 0)) {\n\t\t\t\terr = -EROFS;\n\t\t\t\tgoto restore_opts;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Make sure the group descriptor checksums\n\t\t\t * are sane.  If they aren't, refuse to remount r/w.\n\t\t\t */\n\t\t\tfor (g = 0; g < sbi->s_groups_count; g++) {\n\t\t\t\tstruct ext4_group_desc *gdp =\n\t\t\t\t\text4_get_group_desc(sb, g, NULL);\n\n\t\t\t\tif (!ext4_group_desc_csum_verify(sb, g, gdp)) {\n\t\t\t\t\text4_msg(sb, KERN_ERR,\n\t       \"ext4_remount: Checksum for group %u failed (%u!=%u)\",\n\t\tg, le16_to_cpu(ext4_group_desc_csum(sb, g, gdp)),\n\t\t\t\t\t       le16_to_cpu(gdp->bg_checksum));\n\t\t\t\t\terr = -EFSBADCRC;\n\t\t\t\t\tgoto restore_opts;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If we have an unprocessed orphan list hanging\n\t\t\t * around from a previously readonly bdev mount,\n\t\t\t * require a full umount/remount for now.\n\t\t\t */\n\t\t\tif (es->s_last_orphan) {\n\t\t\t\text4_msg(sb, KERN_WARNING, \"Couldn't \"\n\t\t\t\t       \"remount RDWR because of unprocessed \"\n\t\t\t\t       \"orphan inode list.  Please \"\n\t\t\t\t       \"umount/remount instead\");\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto restore_opts;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Mounting a RDONLY partition read-write, so reread\n\t\t\t * and store the current valid flag.  (It may have\n\t\t\t * been changed by e2fsck since we originally mounted\n\t\t\t * the partition.)\n\t\t\t */\n\t\t\tif (sbi->s_journal)\n\t\t\t\text4_clear_journal_err(sb, es);\n\t\t\tsbi->s_mount_state = le16_to_cpu(es->s_state);\n\n\t\t\terr = ext4_setup_super(sb, es, 0);\n\t\t\tif (err)\n\t\t\t\tgoto restore_opts;\n\n\t\t\tsb->s_flags &= ~SB_RDONLY;\n\t\t\tif (ext4_has_feature_mmp(sb))\n\t\t\t\tif (ext4_multi_mount_protect(sb,\n\t\t\t\t\t\tle64_to_cpu(es->s_mmp_block))) {\n\t\t\t\t\terr = -EROFS;\n\t\t\t\t\tgoto restore_opts;\n\t\t\t\t}\n\t\t\tenable_quota = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Reinitialize lazy itable initialization thread based on\n\t * current settings\n\t */\n\tif (sb_rdonly(sb) || !test_opt(sb, INIT_INODE_TABLE))\n\t\text4_unregister_li_request(sb);\n\telse {\n\t\text4_group_t first_not_zeroed;\n\t\tfirst_not_zeroed = ext4_has_uninit_itable(sb);\n\t\text4_register_li_request(sb, first_not_zeroed);\n\t}\n\n\text4_setup_system_zone(sb);\n\tif (sbi->s_journal == NULL && !(old_sb_flags & SB_RDONLY)) {\n\t\terr = ext4_commit_super(sb, 1);\n\t\tif (err)\n\t\t\tgoto restore_opts;\n\t}\n\n#ifdef CONFIG_QUOTA\n\t/* Release old quota file names */\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tkfree(old_opts.s_qf_names[i]);\n\tif (enable_quota) {\n\t\tif (sb_any_quota_suspended(sb))\n\t\t\tdquot_resume(sb, -1);\n\t\telse if (ext4_has_feature_quota(sb)) {\n\t\t\terr = ext4_enable_quotas(sb);\n\t\t\tif (err)\n\t\t\t\tgoto restore_opts;\n\t\t}\n\t}\n#endif\n\n\t*flags = (*flags & ~SB_LAZYTIME) | (sb->s_flags & SB_LAZYTIME);\n\text4_msg(sb, KERN_INFO, \"re-mounted. Opts: %s\", orig_data);\n\tkfree(orig_data);\n\treturn 0;\n\nrestore_opts:\n\tsb->s_flags = old_sb_flags;\n\tsbi->s_mount_opt = old_opts.s_mount_opt;\n\tsbi->s_mount_opt2 = old_opts.s_mount_opt2;\n\tsbi->s_resuid = old_opts.s_resuid;\n\tsbi->s_resgid = old_opts.s_resgid;\n\tsbi->s_commit_interval = old_opts.s_commit_interval;\n\tsbi->s_min_batch_time = old_opts.s_min_batch_time;\n\tsbi->s_max_batch_time = old_opts.s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\tsbi->s_jquota_fmt = old_opts.s_jquota_fmt;\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\tto_free[i] = get_qf_name(sb, sbi, i);\n\t\trcu_assign_pointer(sbi->s_qf_names[i], old_opts.s_qf_names[i]);\n\t}\n\tsynchronize_rcu();\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++)\n\t\tkfree(to_free[i]);\n#endif\n\tkfree(orig_data);\n\treturn err;\n}\n\n#ifdef CONFIG_QUOTA\nstatic int ext4_statfs_project(struct super_block *sb,\n\t\t\t       kprojid_t projid, struct kstatfs *buf)\n{\n\tstruct kqid qid;\n\tstruct dquot *dquot;\n\tu64 limit;\n\tu64 curblock;\n\n\tqid = make_kqid_projid(projid);\n\tdquot = dqget(sb, qid);\n\tif (IS_ERR(dquot))\n\t\treturn PTR_ERR(dquot);\n\tspin_lock(&dquot->dq_dqb_lock);\n\n\tlimit = (dquot->dq_dqb.dqb_bsoftlimit ?\n\t\t dquot->dq_dqb.dqb_bsoftlimit :\n\t\t dquot->dq_dqb.dqb_bhardlimit) >> sb->s_blocksize_bits;\n\tif (limit && buf->f_blocks > limit) {\n\t\tcurblock = (dquot->dq_dqb.dqb_curspace +\n\t\t\t    dquot->dq_dqb.dqb_rsvspace) >> sb->s_blocksize_bits;\n\t\tbuf->f_blocks = limit;\n\t\tbuf->f_bfree = buf->f_bavail =\n\t\t\t(buf->f_blocks > curblock) ?\n\t\t\t (buf->f_blocks - curblock) : 0;\n\t}\n\n\tlimit = dquot->dq_dqb.dqb_isoftlimit ?\n\t\tdquot->dq_dqb.dqb_isoftlimit :\n\t\tdquot->dq_dqb.dqb_ihardlimit;\n\tif (limit && buf->f_files > limit) {\n\t\tbuf->f_files = limit;\n\t\tbuf->f_ffree =\n\t\t\t(buf->f_files > dquot->dq_dqb.dqb_curinodes) ?\n\t\t\t (buf->f_files - dquot->dq_dqb.dqb_curinodes) : 0;\n\t}\n\n\tspin_unlock(&dquot->dq_dqb_lock);\n\tdqput(dquot);\n\treturn 0;\n}\n#endif\n\nstatic int ext4_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct super_block *sb = dentry->d_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\text4_fsblk_t overhead = 0, resv_blocks;\n\tu64 fsid;\n\ts64 bfree;\n\tresv_blocks = EXT4_C2B(sbi, atomic64_read(&sbi->s_resv_clusters));\n\n\tif (!test_opt(sb, MINIX_DF))\n\t\toverhead = sbi->s_overhead;\n\n\tbuf->f_type = EXT4_SUPER_MAGIC;\n\tbuf->f_bsize = sb->s_blocksize;\n\tbuf->f_blocks = ext4_blocks_count(es) - EXT4_C2B(sbi, overhead);\n\tbfree = percpu_counter_sum_positive(&sbi->s_freeclusters_counter) -\n\t\tpercpu_counter_sum_positive(&sbi->s_dirtyclusters_counter);\n\t/* prevent underflow in case that few free space is available */\n\tbuf->f_bfree = EXT4_C2B(sbi, max_t(s64, bfree, 0));\n\tbuf->f_bavail = buf->f_bfree -\n\t\t\t(ext4_r_blocks_count(es) + resv_blocks);\n\tif (buf->f_bfree < (ext4_r_blocks_count(es) + resv_blocks))\n\t\tbuf->f_bavail = 0;\n\tbuf->f_files = le32_to_cpu(es->s_inodes_count);\n\tbuf->f_ffree = percpu_counter_sum_positive(&sbi->s_freeinodes_counter);\n\tbuf->f_namelen = EXT4_NAME_LEN;\n\tfsid = le64_to_cpup((void *)es->s_uuid) ^\n\t       le64_to_cpup((void *)es->s_uuid + sizeof(u64));\n\tbuf->f_fsid.val[0] = fsid & 0xFFFFFFFFUL;\n\tbuf->f_fsid.val[1] = (fsid >> 32) & 0xFFFFFFFFUL;\n\n#ifdef CONFIG_QUOTA\n\tif (ext4_test_inode_flag(dentry->d_inode, EXT4_INODE_PROJINHERIT) &&\n\t    sb_has_quota_limits_enabled(sb, PRJQUOTA))\n\t\text4_statfs_project(sb, EXT4_I(dentry->d_inode)->i_projid, buf);\n#endif\n\treturn 0;\n}\n\n\n#ifdef CONFIG_QUOTA\n\n/*\n * Helper functions so that transaction is started before we acquire dqio_sem\n * to keep correct lock ordering of transaction > dqio_sem\n */\nstatic inline struct inode *dquot_to_inode(struct dquot *dquot)\n{\n\treturn sb_dqopt(dquot->dq_sb)->files[dquot->dq_id.type];\n}\n\nstatic int ext4_write_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\tstruct inode *inode;\n\n\tinode = dquot_to_inode(dquot);\n\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA,\n\t\t\t\t    EXT4_QUOTA_TRANS_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_commit(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_acquire_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(dquot_to_inode(dquot), EXT4_HT_QUOTA,\n\t\t\t\t    EXT4_QUOTA_INIT_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_acquire(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_release_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(dquot_to_inode(dquot), EXT4_HT_QUOTA,\n\t\t\t\t    EXT4_QUOTA_DEL_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle)) {\n\t\t/* Release dquot anyway to avoid endless cycle in dqput() */\n\t\tdquot_release(dquot);\n\t\treturn PTR_ERR(handle);\n\t}\n\tret = dquot_release(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_mark_dquot_dirty(struct dquot *dquot)\n{\n\tstruct super_block *sb = dquot->dq_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\t/* Are we journaling quotas? */\n\tif (ext4_has_feature_quota(sb) ||\n\t    sbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA]) {\n\t\tdquot_mark_dquot_dirty(dquot);\n\t\treturn ext4_write_dquot(dquot);\n\t} else {\n\t\treturn dquot_mark_dquot_dirty(dquot);\n\t}\n}\n\nstatic int ext4_write_info(struct super_block *sb, int type)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\t/* Data block + inode block */\n\thandle = ext4_journal_start(d_inode(sb->s_root), EXT4_HT_QUOTA, 2);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_commit_info(sb, type);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\n/*\n * Turn on quotas during mount time - we need to find\n * the quota file and such...\n */\nstatic int ext4_quota_on_mount(struct super_block *sb, int type)\n{\n\treturn dquot_quota_on_mount(sb, get_qf_name(sb, EXT4_SB(sb), type),\n\t\t\t\t\tEXT4_SB(sb)->s_jquota_fmt, type);\n}\n\nstatic void lockdep_set_quota_inode(struct inode *inode, int subclass)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\t/* The first argument of lockdep_set_subclass has to be\n\t * *exactly* the same as the argument to init_rwsem() --- in\n\t * this case, in init_once() --- or lockdep gets unhappy\n\t * because the name of the lock is set using the\n\t * stringification of the argument to init_rwsem().\n\t */\n\t(void) ei;\t/* shut up clang warning if !CONFIG_LOCKDEP */\n\tlockdep_set_subclass(&ei->i_data_sem, subclass);\n}\n\n/*\n * Standard function to be called on quota_on\n */\nstatic int ext4_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t const struct path *path)\n{\n\tint err;\n\n\tif (!test_opt(sb, QUOTA))\n\t\treturn -EINVAL;\n\n\t/* Quotafile not on the same filesystem? */\n\tif (path->dentry->d_sb != sb)\n\t\treturn -EXDEV;\n\t/* Journaling quota? */\n\tif (EXT4_SB(sb)->s_qf_names[type]) {\n\t\t/* Quotafile not in fs root? */\n\t\tif (path->dentry->d_parent != sb->s_root)\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t\"Quota file not on filesystem root. \"\n\t\t\t\t\"Journaled quota will not work\");\n\t\tsb_dqopt(sb)->flags |= DQUOT_NOLIST_DIRTY;\n\t} else {\n\t\t/*\n\t\t * Clear the flag just in case mount options changed since\n\t\t * last time.\n\t\t */\n\t\tsb_dqopt(sb)->flags &= ~DQUOT_NOLIST_DIRTY;\n\t}\n\n\t/*\n\t * When we journal data on quota file, we have to flush journal to see\n\t * all updates to the file when we bypass pagecache...\n\t */\n\tif (EXT4_SB(sb)->s_journal &&\n\t    ext4_should_journal_data(d_inode(path->dentry))) {\n\t\t/*\n\t\t * We don't need to lock updates but journal_flush() could\n\t\t * otherwise be livelocked...\n\t\t */\n\t\tjbd2_journal_lock_updates(EXT4_SB(sb)->s_journal);\n\t\terr = jbd2_journal_flush(EXT4_SB(sb)->s_journal);\n\t\tjbd2_journal_unlock_updates(EXT4_SB(sb)->s_journal);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tlockdep_set_quota_inode(path->dentry->d_inode, I_DATA_SEM_QUOTA);\n\terr = dquot_quota_on(sb, type, format_id, path);\n\tif (err) {\n\t\tlockdep_set_quota_inode(path->dentry->d_inode,\n\t\t\t\t\t     I_DATA_SEM_NORMAL);\n\t} else {\n\t\tstruct inode *inode = d_inode(path->dentry);\n\t\thandle_t *handle;\n\n\t\t/*\n\t\t * Set inode flags to prevent userspace from messing with quota\n\t\t * files. If this fails, we return success anyway since quotas\n\t\t * are already enabled and this is not a hard failure.\n\t\t */\n\t\tinode_lock(inode);\n\t\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA, 1);\n\t\tif (IS_ERR(handle))\n\t\t\tgoto unlock_inode;\n\t\tEXT4_I(inode)->i_flags |= EXT4_NOATIME_FL | EXT4_IMMUTABLE_FL;\n\t\tinode_set_flags(inode, S_NOATIME | S_IMMUTABLE,\n\t\t\t\tS_NOATIME | S_IMMUTABLE);\n\t\text4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\tunlock_inode:\n\t\tinode_unlock(inode);\n\t}\n\treturn err;\n}\n\nstatic int ext4_quota_enable(struct super_block *sb, int type, int format_id,\n\t\t\t     unsigned int flags)\n{\n\tint err;\n\tstruct inode *qf_inode;\n\tunsigned long qf_inums[EXT4_MAXQUOTAS] = {\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_usr_quota_inum),\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_grp_quota_inum),\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_prj_quota_inum)\n\t};\n\n\tBUG_ON(!ext4_has_feature_quota(sb));\n\n\tif (!qf_inums[type])\n\t\treturn -EPERM;\n\n\tqf_inode = ext4_iget(sb, qf_inums[type], EXT4_IGET_SPECIAL);\n\tif (IS_ERR(qf_inode)) {\n\t\text4_error(sb, \"Bad quota inode # %lu\", qf_inums[type]);\n\t\treturn PTR_ERR(qf_inode);\n\t}\n\n\t/* Don't account quota for quota files to avoid recursion */\n\tqf_inode->i_flags |= S_NOQUOTA;\n\tlockdep_set_quota_inode(qf_inode, I_DATA_SEM_QUOTA);\n\terr = dquot_enable(qf_inode, type, format_id, flags);\n\tif (err)\n\t\tlockdep_set_quota_inode(qf_inode, I_DATA_SEM_NORMAL);\n\tiput(qf_inode);\n\n\treturn err;\n}\n\n/* Enable usage tracking for all quota types. */\nstatic int ext4_enable_quotas(struct super_block *sb)\n{\n\tint type, err = 0;\n\tunsigned long qf_inums[EXT4_MAXQUOTAS] = {\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_usr_quota_inum),\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_grp_quota_inum),\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_prj_quota_inum)\n\t};\n\tbool quota_mopt[EXT4_MAXQUOTAS] = {\n\t\ttest_opt(sb, USRQUOTA),\n\t\ttest_opt(sb, GRPQUOTA),\n\t\ttest_opt(sb, PRJQUOTA),\n\t};\n\n\tsb_dqopt(sb)->flags |= DQUOT_QUOTA_SYS_FILE | DQUOT_NOLIST_DIRTY;\n\tfor (type = 0; type < EXT4_MAXQUOTAS; type++) {\n\t\tif (qf_inums[type]) {\n\t\t\terr = ext4_quota_enable(sb, type, QFMT_VFS_V1,\n\t\t\t\tDQUOT_USAGE_ENABLED |\n\t\t\t\t(quota_mopt[type] ? DQUOT_LIMITS_ENABLED : 0));\n\t\t\tif (err) {\n\t\t\t\text4_warning(sb,\n\t\t\t\t\t\"Failed to enable quota tracking \"\n\t\t\t\t\t\"(type=%d, err=%d). Please run \"\n\t\t\t\t\t\"e2fsck to fix.\", type, err);\n\t\t\t\tfor (type--; type >= 0; type--)\n\t\t\t\t\tdquot_quota_off(sb, type);\n\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int ext4_quota_off(struct super_block *sb, int type)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\thandle_t *handle;\n\tint err;\n\n\t/* Force all delayed allocation blocks to be allocated.\n\t * Caller already holds s_umount sem */\n\tif (test_opt(sb, DELALLOC))\n\t\tsync_filesystem(sb);\n\n\tif (!inode || !igrab(inode))\n\t\tgoto out;\n\n\terr = dquot_quota_off(sb, type);\n\tif (err || ext4_has_feature_quota(sb))\n\t\tgoto out_put;\n\n\tinode_lock(inode);\n\t/*\n\t * Update modification times of quota files when userspace can\n\t * start looking at them. If we fail, we return success anyway since\n\t * this is not a hard failure and quotas are already disabled.\n\t */\n\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA, 1);\n\tif (IS_ERR(handle))\n\t\tgoto out_unlock;\n\tEXT4_I(inode)->i_flags &= ~(EXT4_NOATIME_FL | EXT4_IMMUTABLE_FL);\n\tinode_set_flags(inode, 0, S_NOATIME | S_IMMUTABLE);\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\nout_unlock:\n\tinode_unlock(inode);\nout_put:\n\tlockdep_set_quota_inode(inode, I_DATA_SEM_NORMAL);\n\tiput(inode);\n\treturn err;\nout:\n\treturn dquot_quota_off(sb, type);\n}\n\n/* Read data from quotafile - avoid pagecache and such because we cannot afford\n * acquiring the locks... As quota files are never truncated and quota code\n * itself serializes the operations (and no one else should touch the files)\n * we don't have to be afraid of races */\nstatic ssize_t ext4_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\text4_lblk_t blk = off >> EXT4_BLOCK_SIZE_BITS(sb);\n\tint offset = off & (sb->s_blocksize - 1);\n\tint tocopy;\n\tsize_t toread;\n\tstruct buffer_head *bh;\n\tloff_t i_size = i_size_read(inode);\n\n\tif (off > i_size)\n\t\treturn 0;\n\tif (off+len > i_size)\n\t\tlen = i_size-off;\n\ttoread = len;\n\twhile (toread > 0) {\n\t\ttocopy = sb->s_blocksize - offset < toread ?\n\t\t\t\tsb->s_blocksize - offset : toread;\n\t\tbh = ext4_bread(NULL, inode, blk, 0);\n\t\tif (IS_ERR(bh))\n\t\t\treturn PTR_ERR(bh);\n\t\tif (!bh)\t/* A hole? */\n\t\t\tmemset(data, 0, tocopy);\n\t\telse\n\t\t\tmemcpy(data, bh->b_data+offset, tocopy);\n\t\tbrelse(bh);\n\t\toffset = 0;\n\t\ttoread -= tocopy;\n\t\tdata += tocopy;\n\t\tblk++;\n\t}\n\treturn len;\n}\n\n/* Write to quotafile (we know the transaction is already started and has\n * enough credits) */\nstatic ssize_t ext4_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\text4_lblk_t blk = off >> EXT4_BLOCK_SIZE_BITS(sb);\n\tint err, offset = off & (sb->s_blocksize - 1);\n\tint retries = 0;\n\tstruct buffer_head *bh;\n\thandle_t *handle = journal_current_handle();\n\n\tif (EXT4_SB(sb)->s_journal && !handle) {\n\t\text4_msg(sb, KERN_WARNING, \"Quota write (off=%llu, len=%llu)\"\n\t\t\t\" cancelled because transaction is not started\",\n\t\t\t(unsigned long long)off, (unsigned long long)len);\n\t\treturn -EIO;\n\t}\n\t/*\n\t * Since we account only one data block in transaction credits,\n\t * then it is impossible to cross a block boundary.\n\t */\n\tif (sb->s_blocksize - offset < len) {\n\t\text4_msg(sb, KERN_WARNING, \"Quota write (off=%llu, len=%llu)\"\n\t\t\t\" cancelled because not block aligned\",\n\t\t\t(unsigned long long)off, (unsigned long long)len);\n\t\treturn -EIO;\n\t}\n\n\tdo {\n\t\tbh = ext4_bread(handle, inode, blk,\n\t\t\t\tEXT4_GET_BLOCKS_CREATE |\n\t\t\t\tEXT4_GET_BLOCKS_METADATA_NOFAIL);\n\t} while (IS_ERR(bh) && (PTR_ERR(bh) == -ENOSPC) &&\n\t\t ext4_should_retry_alloc(inode->i_sb, &retries));\n\tif (IS_ERR(bh))\n\t\treturn PTR_ERR(bh);\n\tif (!bh)\n\t\tgoto out;\n\tBUFFER_TRACE(bh, \"get write access\");\n\terr = ext4_journal_get_write_access(handle, bh);\n\tif (err) {\n\t\tbrelse(bh);\n\t\treturn err;\n\t}\n\tlock_buffer(bh);\n\tmemcpy(bh->b_data+offset, data, len);\n\tflush_dcache_page(bh->b_page);\n\tunlock_buffer(bh);\n\terr = ext4_handle_dirty_metadata(handle, NULL, bh);\n\tbrelse(bh);\nout:\n\tif (inode->i_size < off + len) {\n\t\ti_size_write(inode, off + len);\n\t\tEXT4_I(inode)->i_disksize = inode->i_size;\n\t\text4_mark_inode_dirty(handle, inode);\n\t}\n\treturn len;\n}\n\nstatic int ext4_get_next_id(struct super_block *sb, struct kqid *qid)\n{\n\tconst struct quota_format_ops\t*ops;\n\n\tif (!sb_has_quota_loaded(sb, qid->type))\n\t\treturn -ESRCH;\n\tops = sb_dqopt(sb)->ops[qid->type];\n\tif (!ops || !ops->get_next_id)\n\t\treturn -ENOSYS;\n\treturn dquot_get_next_id(sb, qid);\n}\n#endif\n\nstatic struct dentry *ext4_mount(struct file_system_type *fs_type, int flags,\n\t\t       const char *dev_name, void *data)\n{\n\treturn mount_bdev(fs_type, flags, dev_name, data, ext4_fill_super);\n}\n\n#if !defined(CONFIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT2)\nstatic inline void register_as_ext2(void)\n{\n\tint err = register_filesystem(&ext2_fs_type);\n\tif (err)\n\t\tprintk(KERN_WARNING\n\t\t       \"EXT4-fs: Unable to register as ext2 (%d)\\n\", err);\n}\n\nstatic inline void unregister_as_ext2(void)\n{\n\tunregister_filesystem(&ext2_fs_type);\n}\n\nstatic inline int ext2_feature_set_ok(struct super_block *sb)\n{\n\tif (ext4_has_unknown_ext2_incompat_features(sb))\n\t\treturn 0;\n\tif (sb_rdonly(sb))\n\t\treturn 1;\n\tif (ext4_has_unknown_ext2_ro_compat_features(sb))\n\t\treturn 0;\n\treturn 1;\n}\n#else\nstatic inline void register_as_ext2(void) { }\nstatic inline void unregister_as_ext2(void) { }\nstatic inline int ext2_feature_set_ok(struct super_block *sb) { return 0; }\n#endif\n\nstatic inline void register_as_ext3(void)\n{\n\tint err = register_filesystem(&ext3_fs_type);\n\tif (err)\n\t\tprintk(KERN_WARNING\n\t\t       \"EXT4-fs: Unable to register as ext3 (%d)\\n\", err);\n}\n\nstatic inline void unregister_as_ext3(void)\n{\n\tunregister_filesystem(&ext3_fs_type);\n}\n\nstatic inline int ext3_feature_set_ok(struct super_block *sb)\n{\n\tif (ext4_has_unknown_ext3_incompat_features(sb))\n\t\treturn 0;\n\tif (!ext4_has_feature_journal(sb))\n\t\treturn 0;\n\tif (sb_rdonly(sb))\n\t\treturn 1;\n\tif (ext4_has_unknown_ext3_ro_compat_features(sb))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic struct file_system_type ext4_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext4\",\n\t.mount\t\t= ext4_mount,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"ext4\");\n\n/* Shared across all ext4 file systems */\nwait_queue_head_t ext4__ioend_wq[EXT4_WQ_HASH_SZ];\n\nstatic int __init ext4_init_fs(void)\n{\n\tint i, err;\n\n\tratelimit_state_init(&ext4_mount_msg_ratelimit, 30 * HZ, 64);\n\text4_li_info = NULL;\n\tmutex_init(&ext4_li_mtx);\n\n\t/* Build-time check for flags consistency */\n\text4_check_flag_values();\n\n\tfor (i = 0; i < EXT4_WQ_HASH_SZ; i++)\n\t\tinit_waitqueue_head(&ext4__ioend_wq[i]);\n\n\terr = ext4_init_es();\n\tif (err)\n\t\treturn err;\n\n\terr = ext4_init_pending();\n\tif (err)\n\t\tgoto out7;\n\n\terr = ext4_init_post_read_processing();\n\tif (err)\n\t\tgoto out6;\n\n\terr = ext4_init_pageio();\n\tif (err)\n\t\tgoto out5;\n\n\terr = ext4_init_system_zone();\n\tif (err)\n\t\tgoto out4;\n\n\terr = ext4_init_sysfs();\n\tif (err)\n\t\tgoto out3;\n\n\terr = ext4_init_mballoc();\n\tif (err)\n\t\tgoto out2;\n\terr = init_inodecache();\n\tif (err)\n\t\tgoto out1;\n\tregister_as_ext3();\n\tregister_as_ext2();\n\terr = register_filesystem(&ext4_fs_type);\n\tif (err)\n\t\tgoto out;\n\n\treturn 0;\nout:\n\tunregister_as_ext2();\n\tunregister_as_ext3();\n\tdestroy_inodecache();\nout1:\n\text4_exit_mballoc();\nout2:\n\text4_exit_sysfs();\nout3:\n\text4_exit_system_zone();\nout4:\n\text4_exit_pageio();\nout5:\n\text4_exit_post_read_processing();\nout6:\n\text4_exit_pending();\nout7:\n\text4_exit_es();\n\n\treturn err;\n}\n\nstatic void __exit ext4_exit_fs(void)\n{\n\text4_destroy_lazyinit_thread();\n\tunregister_as_ext2();\n\tunregister_as_ext3();\n\tunregister_filesystem(&ext4_fs_type);\n\tdestroy_inodecache();\n\text4_exit_mballoc();\n\text4_exit_sysfs();\n\text4_exit_system_zone();\n\text4_exit_pageio();\n\text4_exit_post_read_processing();\n\text4_exit_es();\n\text4_exit_pending();\n}\n\nMODULE_AUTHOR(\"Remy Card, Stephen Tweedie, Andrew Morton, Andreas Dilger, Theodore Ts'o and others\");\nMODULE_DESCRIPTION(\"Fourth Extended Filesystem\");\nMODULE_LICENSE(\"GPL\");\nMODULE_SOFTDEP(\"pre: crc32c\");\nmodule_init(ext4_init_fs)\nmodule_exit(ext4_exit_fs)\n"], "filenames": ["fs/ext4/inode.c", "fs/ext4/super.c"], "buggy_code_start_loc": [5571, 3548], "buggy_code_end_loc": [5572, 3570], "fixing_code_start_loc": [5572, 3548], "fixing_code_end_loc": [5588, 3573], "type": "CWE-416", "message": "The Linux kernel before 5.4.2 mishandles ext4_expand_extra_isize, as demonstrated by use-after-free errors in __ext4_expand_extra_isize and ext4_xattr_set_entry, related to fs/ext4/inode.c and fs/ext4/super.c, aka CID-4ea99936a163.", "other": {"cve": {"id": "CVE-2019-19767", "sourceIdentifier": "cve@mitre.org", "published": "2019-12-12T20:15:17.583", "lastModified": "2020-01-03T11:15:15.667", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel before 5.4.2 mishandles ext4_expand_extra_isize, as demonstrated by use-after-free errors in __ext4_expand_extra_isize and ext4_xattr_set_entry, related to fs/ext4/inode.c and fs/ext4/super.c, aka CID-4ea99936a163."}, {"lang": "es", "value": "El kernel de Linux versi\u00f3n anterior a 5.4.2 maneja inapropiadamente la funci\u00f3n ext4_expand_extra_isize, como es demostrado por un error de uso de la memoria previamente liberada en las funciones __ext4_expand_extra_isize y ext4_xattr_set_entry, relacionadas con los archivos fs/ext4/inode.c y fs/ext4/super.c, tambi\u00e9n se conoce como CID-4ea99936a163."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.4.2", "matchCriteriaId": "EDA2DEC6-C9B1-4CBB-99ED-EF1EB2832A99"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-03/msg00021.html", "source": "cve@mitre.org"}, {"url": "https://bugzilla.kernel.org/show_bug.cgi?id=205609", "source": "cve@mitre.org", "tags": ["Exploit", "Issue Tracking", "Vendor Advisory"]}, {"url": "https://bugzilla.kernel.org/show_bug.cgi?id=205707", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Vendor Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.4.2", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=4ea99936a1630f51fc3a2d61a58ec4a1c4b7d55a", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/4ea99936a1630f51fc3a2d61a58ec4a1c4b7d55a", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2020/01/msg00013.html", "source": "cve@mitre.org"}, {"url": "https://lists.debian.org/debian-lts-announce/2020/03/msg00001.html", "source": "cve@mitre.org"}, {"url": "https://security.netapp.com/advisory/ntap-20200103-0001/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4258-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4284-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4287-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4287-2/", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/4ea99936a1630f51fc3a2d61a58ec4a1c4b7d55a"}}