{"buggy_code": ["/*\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License, version 2, as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n *\n * Copyright IBM Corp. 2007\n *\n * Authors: Hollis Blanchard <hollisb@us.ibm.com>\n *          Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>\n */\n\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/kvm_host.h>\n#include <linux/vmalloc.h>\n#include <linux/hrtimer.h>\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/module.h>\n#include <linux/irqbypass.h>\n#include <linux/kvm_irqfd.h>\n#include <asm/cputable.h>\n#include <linux/uaccess.h>\n#include <asm/kvm_ppc.h>\n#include <asm/tlbflush.h>\n#include <asm/cputhreads.h>\n#include <asm/irqflags.h>\n#include <asm/iommu.h>\n#include <asm/switch_to.h>\n#include <asm/xive.h>\n\n#include \"timing.h\"\n#include \"irq.h\"\n#include \"../mm/mmu_decl.h\"\n\n#define CREATE_TRACE_POINTS\n#include \"trace.h\"\n\nstruct kvmppc_ops *kvmppc_hv_ops;\nEXPORT_SYMBOL_GPL(kvmppc_hv_ops);\nstruct kvmppc_ops *kvmppc_pr_ops;\nEXPORT_SYMBOL_GPL(kvmppc_pr_ops);\n\n\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *v)\n{\n\treturn !!(v->arch.pending_exceptions) || kvm_request_pending(v);\n}\n\nbool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)\n{\n\treturn false;\n}\n\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\n/*\n * Common checks before entering the guest world.  Call with interrupts\n * disabled.\n *\n * returns:\n *\n * == 1 if we're ready to go into guest state\n * <= 0 if we need to go back to the host with return value\n */\nint kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\n\tWARN_ON(irqs_disabled());\n\thard_irq_disable();\n\n\twhile (true) {\n\t\tif (need_resched()) {\n\t\t\tlocal_irq_enable();\n\t\t\tcond_resched();\n\t\t\thard_irq_disable();\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (signal_pending(current)) {\n\t\t\tkvmppc_account_exit(vcpu, SIGNAL_EXITS);\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_INTR;\n\t\t\tr = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tvcpu->mode = IN_GUEST_MODE;\n\n\t\t/*\n\t\t * Reading vcpu->requests must happen after setting vcpu->mode,\n\t\t * so we don't miss a request because the requester sees\n\t\t * OUTSIDE_GUEST_MODE and assumes we'll be checking requests\n\t\t * before next entering the guest (and thus doesn't IPI).\n\t\t * This also orders the write to mode from any reads\n\t\t * to the page tables done while the VCPU is running.\n\t\t * Please see the comment in kvm_flush_remote_tlbs.\n\t\t */\n\t\tsmp_mb();\n\n\t\tif (kvm_request_pending(vcpu)) {\n\t\t\t/* Make sure we process requests preemptable */\n\t\t\tlocal_irq_enable();\n\t\t\ttrace_kvm_check_requests(vcpu);\n\t\t\tr = kvmppc_core_check_requests(vcpu);\n\t\t\thard_irq_disable();\n\t\t\tif (r > 0)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (kvmppc_core_prepare_to_enter(vcpu)) {\n\t\t\t/* interrupts got enabled in between, so we\n\t\t\t   are back at square 1 */\n\t\t\tcontinue;\n\t\t}\n\n\t\tguest_enter_irqoff();\n\t\treturn 1;\n\t}\n\n\t/* return to host */\n\tlocal_irq_enable();\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvmppc_prepare_to_enter);\n\n#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)\nstatic void kvmppc_swab_shared(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_vcpu_arch_shared *shared = vcpu->arch.shared;\n\tint i;\n\n\tshared->sprg0 = swab64(shared->sprg0);\n\tshared->sprg1 = swab64(shared->sprg1);\n\tshared->sprg2 = swab64(shared->sprg2);\n\tshared->sprg3 = swab64(shared->sprg3);\n\tshared->srr0 = swab64(shared->srr0);\n\tshared->srr1 = swab64(shared->srr1);\n\tshared->dar = swab64(shared->dar);\n\tshared->msr = swab64(shared->msr);\n\tshared->dsisr = swab32(shared->dsisr);\n\tshared->int_pending = swab32(shared->int_pending);\n\tfor (i = 0; i < ARRAY_SIZE(shared->sr); i++)\n\t\tshared->sr[i] = swab32(shared->sr[i]);\n}\n#endif\n\nint kvmppc_kvm_pv(struct kvm_vcpu *vcpu)\n{\n\tint nr = kvmppc_get_gpr(vcpu, 11);\n\tint r;\n\tunsigned long __maybe_unused param1 = kvmppc_get_gpr(vcpu, 3);\n\tunsigned long __maybe_unused param2 = kvmppc_get_gpr(vcpu, 4);\n\tunsigned long __maybe_unused param3 = kvmppc_get_gpr(vcpu, 5);\n\tunsigned long __maybe_unused param4 = kvmppc_get_gpr(vcpu, 6);\n\tunsigned long r2 = 0;\n\n\tif (!(kvmppc_get_msr(vcpu) & MSR_SF)) {\n\t\t/* 32 bit mode */\n\t\tparam1 &= 0xffffffff;\n\t\tparam2 &= 0xffffffff;\n\t\tparam3 &= 0xffffffff;\n\t\tparam4 &= 0xffffffff;\n\t}\n\n\tswitch (nr) {\n\tcase KVM_HCALL_TOKEN(KVM_HC_PPC_MAP_MAGIC_PAGE):\n\t{\n#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)\n\t\t/* Book3S can be little endian, find it out here */\n\t\tint shared_big_endian = true;\n\t\tif (vcpu->arch.intr_msr & MSR_LE)\n\t\t\tshared_big_endian = false;\n\t\tif (shared_big_endian != vcpu->arch.shared_big_endian)\n\t\t\tkvmppc_swab_shared(vcpu);\n\t\tvcpu->arch.shared_big_endian = shared_big_endian;\n#endif\n\n\t\tif (!(param2 & MAGIC_PAGE_FLAG_NOT_MAPPED_NX)) {\n\t\t\t/*\n\t\t\t * Older versions of the Linux magic page code had\n\t\t\t * a bug where they would map their trampoline code\n\t\t\t * NX. If that's the case, remove !PR NX capability.\n\t\t\t */\n\t\t\tvcpu->arch.disable_kernel_nx = true;\n\t\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\t\t}\n\n\t\tvcpu->arch.magic_page_pa = param1 & ~0xfffULL;\n\t\tvcpu->arch.magic_page_ea = param2 & ~0xfffULL;\n\n#ifdef CONFIG_PPC_64K_PAGES\n\t\t/*\n\t\t * Make sure our 4k magic page is in the same window of a 64k\n\t\t * page within the guest and within the host's page.\n\t\t */\n\t\tif ((vcpu->arch.magic_page_pa & 0xf000) !=\n\t\t    ((ulong)vcpu->arch.shared & 0xf000)) {\n\t\t\tvoid *old_shared = vcpu->arch.shared;\n\t\t\tulong shared = (ulong)vcpu->arch.shared;\n\t\t\tvoid *new_shared;\n\n\t\t\tshared &= PAGE_MASK;\n\t\t\tshared |= vcpu->arch.magic_page_pa & 0xf000;\n\t\t\tnew_shared = (void*)shared;\n\t\t\tmemcpy(new_shared, old_shared, 0x1000);\n\t\t\tvcpu->arch.shared = new_shared;\n\t\t}\n#endif\n\n\t\tr2 = KVM_MAGIC_FEAT_SR | KVM_MAGIC_FEAT_MAS0_TO_SPRG7;\n\n\t\tr = EV_SUCCESS;\n\t\tbreak;\n\t}\n\tcase KVM_HCALL_TOKEN(KVM_HC_FEATURES):\n\t\tr = EV_SUCCESS;\n#if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500V2)\n\t\tr2 |= (1 << KVM_FEATURE_MAGIC_PAGE);\n#endif\n\n\t\t/* Second return value is in r4 */\n\t\tbreak;\n\tcase EV_HCALL_TOKEN(EV_IDLE):\n\t\tr = EV_SUCCESS;\n\t\tkvm_vcpu_block(vcpu);\n\t\tkvm_clear_request(KVM_REQ_UNHALT, vcpu);\n\t\tbreak;\n\tdefault:\n\t\tr = EV_UNIMPLEMENTED;\n\t\tbreak;\n\t}\n\n\tkvmppc_set_gpr(vcpu, 4, r2);\n\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvmppc_kvm_pv);\n\nint kvmppc_sanity_check(struct kvm_vcpu *vcpu)\n{\n\tint r = false;\n\n\t/* We have to know what CPU to virtualize */\n\tif (!vcpu->arch.pvr)\n\t\tgoto out;\n\n\t/* PAPR only works with book3s_64 */\n\tif ((vcpu->arch.cpu_type != KVM_CPU_3S_64) && vcpu->arch.papr_enabled)\n\t\tgoto out;\n\n\t/* HV KVM can only do PAPR mode for now */\n\tif (!vcpu->arch.papr_enabled && is_kvmppc_hv_enabled(vcpu->kvm))\n\t\tgoto out;\n\n#ifdef CONFIG_KVM_BOOKE_HV\n\tif (!cpu_has_feature(CPU_FTR_EMB_HV))\n\t\tgoto out;\n#endif\n\n\tr = true;\n\nout:\n\tvcpu->arch.sane = r;\n\treturn r ? 0 : -EINVAL;\n}\nEXPORT_SYMBOL_GPL(kvmppc_sanity_check);\n\nint kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)\n{\n\tenum emulation_result er;\n\tint r;\n\n\ter = kvmppc_emulate_loadstore(vcpu);\n\tswitch (er) {\n\tcase EMULATE_DONE:\n\t\t/* Future optimization: only reload non-volatiles if they were\n\t\t * actually modified. */\n\t\tr = RESUME_GUEST_NV;\n\t\tbreak;\n\tcase EMULATE_AGAIN:\n\t\tr = RESUME_GUEST;\n\t\tbreak;\n\tcase EMULATE_DO_MMIO:\n\t\trun->exit_reason = KVM_EXIT_MMIO;\n\t\t/* We must reload nonvolatiles because \"update\" load/store\n\t\t * instructions modify register state. */\n\t\t/* Future optimization: only reload non-volatiles if they were\n\t\t * actually modified. */\n\t\tr = RESUME_HOST_NV;\n\t\tbreak;\n\tcase EMULATE_FAIL:\n\t{\n\t\tu32 last_inst;\n\n\t\tkvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst);\n\t\t/* XXX Deliver Program interrupt to guest. */\n\t\tpr_emerg(\"%s: emulation failed (%08x)\\n\", __func__, last_inst);\n\t\tr = RESUME_HOST;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tWARN_ON(1);\n\t\tr = RESUME_GUEST;\n\t}\n\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvmppc_emulate_mmio);\n\nint kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,\n\t      bool data)\n{\n\tulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM & PAGE_MASK;\n\tstruct kvmppc_pte pte;\n\tint r;\n\n\tvcpu->stat.st++;\n\n\tr = kvmppc_xlate(vcpu, *eaddr, data ? XLATE_DATA : XLATE_INST,\n\t\t\t XLATE_WRITE, &pte);\n\tif (r < 0)\n\t\treturn r;\n\n\t*eaddr = pte.raddr;\n\n\tif (!pte.may_write)\n\t\treturn -EPERM;\n\n\t/* Magic page override */\n\tif (kvmppc_supports_magic_page(vcpu) && mp_pa &&\n\t    ((pte.raddr & KVM_PAM & PAGE_MASK) == mp_pa) &&\n\t    !(kvmppc_get_msr(vcpu) & MSR_PR)) {\n\t\tvoid *magic = vcpu->arch.shared;\n\t\tmagic += pte.eaddr & 0xfff;\n\t\tmemcpy(magic, ptr, size);\n\t\treturn EMULATE_DONE;\n\t}\n\n\tif (kvm_write_guest(vcpu->kvm, pte.raddr, ptr, size))\n\t\treturn EMULATE_DO_MMIO;\n\n\treturn EMULATE_DONE;\n}\nEXPORT_SYMBOL_GPL(kvmppc_st);\n\nint kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,\n\t\t      bool data)\n{\n\tulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM & PAGE_MASK;\n\tstruct kvmppc_pte pte;\n\tint rc;\n\n\tvcpu->stat.ld++;\n\n\trc = kvmppc_xlate(vcpu, *eaddr, data ? XLATE_DATA : XLATE_INST,\n\t\t\t  XLATE_READ, &pte);\n\tif (rc)\n\t\treturn rc;\n\n\t*eaddr = pte.raddr;\n\n\tif (!pte.may_read)\n\t\treturn -EPERM;\n\n\tif (!data && !pte.may_execute)\n\t\treturn -ENOEXEC;\n\n\t/* Magic page override */\n\tif (kvmppc_supports_magic_page(vcpu) && mp_pa &&\n\t    ((pte.raddr & KVM_PAM & PAGE_MASK) == mp_pa) &&\n\t    !(kvmppc_get_msr(vcpu) & MSR_PR)) {\n\t\tvoid *magic = vcpu->arch.shared;\n\t\tmagic += pte.eaddr & 0xfff;\n\t\tmemcpy(ptr, magic, size);\n\t\treturn EMULATE_DONE;\n\t}\n\n\tif (kvm_read_guest(vcpu->kvm, pte.raddr, ptr, size))\n\t\treturn EMULATE_DO_MMIO;\n\n\treturn EMULATE_DONE;\n}\nEXPORT_SYMBOL_GPL(kvmppc_ld);\n\nint kvm_arch_hardware_enable(void)\n{\n\treturn 0;\n}\n\nint kvm_arch_hardware_setup(void)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_check_processor_compat(void *rtn)\n{\n\t*(int *)rtn = kvmppc_core_check_processor_compat();\n}\n\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\n{\n\tstruct kvmppc_ops *kvm_ops = NULL;\n\t/*\n\t * if we have both HV and PR enabled, default is HV\n\t */\n\tif (type == 0) {\n\t\tif (kvmppc_hv_ops)\n\t\t\tkvm_ops = kvmppc_hv_ops;\n\t\telse\n\t\t\tkvm_ops = kvmppc_pr_ops;\n\t\tif (!kvm_ops)\n\t\t\tgoto err_out;\n\t} else\tif (type == KVM_VM_PPC_HV) {\n\t\tif (!kvmppc_hv_ops)\n\t\t\tgoto err_out;\n\t\tkvm_ops = kvmppc_hv_ops;\n\t} else if (type == KVM_VM_PPC_PR) {\n\t\tif (!kvmppc_pr_ops)\n\t\t\tgoto err_out;\n\t\tkvm_ops = kvmppc_pr_ops;\n\t} else\n\t\tgoto err_out;\n\n\tif (kvm_ops->owner && !try_module_get(kvm_ops->owner))\n\t\treturn -ENOENT;\n\n\tkvm->arch.kvm_ops = kvm_ops;\n\treturn kvmppc_core_init_vm(kvm);\nerr_out:\n\treturn -EINVAL;\n}\n\nbool kvm_arch_has_vcpu_debugfs(void)\n{\n\treturn false;\n}\n\nint kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\n{\n\tunsigned int i;\n\tstruct kvm_vcpu *vcpu;\n\n#ifdef CONFIG_KVM_XICS\n\t/*\n\t * We call kick_all_cpus_sync() to ensure that all\n\t * CPUs have executed any pending IPIs before we\n\t * continue and free VCPUs structures below.\n\t */\n\tif (is_kvmppc_hv_enabled(kvm))\n\t\tkick_all_cpus_sync();\n#endif\n\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_arch_vcpu_free(vcpu);\n\n\tmutex_lock(&kvm->lock);\n\tfor (i = 0; i < atomic_read(&kvm->online_vcpus); i++)\n\t\tkvm->vcpus[i] = NULL;\n\n\tatomic_set(&kvm->online_vcpus, 0);\n\n\tkvmppc_core_destroy_vm(kvm);\n\n\tmutex_unlock(&kvm->lock);\n\n\t/* drop the module reference */\n\tmodule_put(kvm->arch.kvm_ops->owner);\n}\n\nint kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)\n{\n\tint r;\n\t/* Assume we're using HV mode when the HV module is loaded */\n\tint hv_enabled = kvmppc_hv_ops ? 1 : 0;\n\n\tif (kvm) {\n\t\t/*\n\t\t * Hooray - we know which VM type we're running on. Depend on\n\t\t * that rather than the guess above.\n\t\t */\n\t\thv_enabled = is_kvmppc_hv_enabled(kvm);\n\t}\n\n\tswitch (ext) {\n#ifdef CONFIG_BOOKE\n\tcase KVM_CAP_PPC_BOOKE_SREGS:\n\tcase KVM_CAP_PPC_BOOKE_WATCHDOG:\n\tcase KVM_CAP_PPC_EPR:\n#else\n\tcase KVM_CAP_PPC_SEGSTATE:\n\tcase KVM_CAP_PPC_HIOR:\n\tcase KVM_CAP_PPC_PAPR:\n#endif\n\tcase KVM_CAP_PPC_UNSET_IRQ:\n\tcase KVM_CAP_PPC_IRQ_LEVEL:\n\tcase KVM_CAP_ENABLE_CAP:\n\tcase KVM_CAP_ENABLE_CAP_VM:\n\tcase KVM_CAP_ONE_REG:\n\tcase KVM_CAP_IOEVENTFD:\n\tcase KVM_CAP_DEVICE_CTRL:\n\tcase KVM_CAP_IMMEDIATE_EXIT:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_PPC_PAIRED_SINGLES:\n\tcase KVM_CAP_PPC_OSI:\n\tcase KVM_CAP_PPC_GET_PVINFO:\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\n\tcase KVM_CAP_SW_TLB:\n#endif\n\t\t/* We support this only for PR */\n\t\tr = !hv_enabled;\n\t\tbreak;\n#ifdef CONFIG_KVM_MPIC\n\tcase KVM_CAP_IRQ_MPIC:\n\t\tr = 1;\n\t\tbreak;\n#endif\n\n#ifdef CONFIG_PPC_BOOK3S_64\n\tcase KVM_CAP_SPAPR_TCE:\n\tcase KVM_CAP_SPAPR_TCE_64:\n\t\t/* fallthrough */\n\tcase KVM_CAP_SPAPR_TCE_VFIO:\n\tcase KVM_CAP_PPC_RTAS:\n\tcase KVM_CAP_PPC_FIXUP_HCALL:\n\tcase KVM_CAP_PPC_ENABLE_HCALL:\n#ifdef CONFIG_KVM_XICS\n\tcase KVM_CAP_IRQ_XICS:\n#endif\n\t\tr = 1;\n\t\tbreak;\n\n\tcase KVM_CAP_PPC_ALLOC_HTAB:\n\t\tr = hv_enabled;\n\t\tbreak;\n#endif /* CONFIG_PPC_BOOK3S_64 */\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tcase KVM_CAP_PPC_SMT:\n\t\tr = 0;\n\t\tif (kvm) {\n\t\t\tif (kvm->arch.emul_smt_mode > 1)\n\t\t\t\tr = kvm->arch.emul_smt_mode;\n\t\t\telse\n\t\t\t\tr = kvm->arch.smt_mode;\n\t\t} else if (hv_enabled) {\n\t\t\tif (cpu_has_feature(CPU_FTR_ARCH_300))\n\t\t\t\tr = 1;\n\t\t\telse\n\t\t\t\tr = threads_per_subcore;\n\t\t}\n\t\tbreak;\n\tcase KVM_CAP_PPC_SMT_POSSIBLE:\n\t\tr = 1;\n\t\tif (hv_enabled) {\n\t\t\tif (!cpu_has_feature(CPU_FTR_ARCH_300))\n\t\t\t\tr = ((threads_per_subcore << 1) - 1);\n\t\t\telse\n\t\t\t\t/* P9 can emulate dbells, so allow any mode */\n\t\t\t\tr = 8 | 4 | 2 | 1;\n\t\t}\n\t\tbreak;\n\tcase KVM_CAP_PPC_RMA:\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_PPC_HWRNG:\n\t\tr = kvmppc_hwrng_present();\n\t\tbreak;\n\tcase KVM_CAP_PPC_MMU_RADIX:\n\t\tr = !!(hv_enabled && radix_enabled());\n\t\tbreak;\n\tcase KVM_CAP_PPC_MMU_HASH_V3:\n\t\tr = !!(hv_enabled && !radix_enabled() &&\n\t\t       cpu_has_feature(CPU_FTR_ARCH_300));\n\t\tbreak;\n#endif\n\tcase KVM_CAP_SYNC_MMU:\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\t\tr = hv_enabled;\n#elif defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\t\tr = 1;\n#else\n\t\tr = 0;\n#endif\n\t\tbreak;\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tcase KVM_CAP_PPC_HTAB_FD:\n\t\tr = hv_enabled;\n\t\tbreak;\n#endif\n\tcase KVM_CAP_NR_VCPUS:\n\t\t/*\n\t\t * Recommending a number of CPUs is somewhat arbitrary; we\n\t\t * return the number of present CPUs for -HV (since a host\n\t\t * will have secondary threads \"offline\"), and for other KVM\n\t\t * implementations just count online CPUs.\n\t\t */\n\t\tif (hv_enabled)\n\t\t\tr = num_present_cpus();\n\t\telse\n\t\t\tr = num_online_cpus();\n\t\tbreak;\n\tcase KVM_CAP_NR_MEMSLOTS:\n\t\tr = KVM_USER_MEM_SLOTS;\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPUS:\n\t\tr = KVM_MAX_VCPUS;\n\t\tbreak;\n#ifdef CONFIG_PPC_BOOK3S_64\n\tcase KVM_CAP_PPC_GET_SMMU_INFO:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_SPAPR_MULTITCE:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_SPAPR_RESIZE_HPT:\n\t\t/* Disable this on POWER9 until code handles new HPTE format */\n\t\tr = !!hv_enabled && !cpu_has_feature(CPU_FTR_ARCH_300);\n\t\tbreak;\n#endif\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tcase KVM_CAP_PPC_FWNMI:\n\t\tr = hv_enabled;\n\t\tbreak;\n#endif\n\tcase KVM_CAP_PPC_HTM:\n\t\tr = cpu_has_feature(CPU_FTR_TM_COMP) &&\n\t\t    is_kvmppc_hv_enabled(kvm);\n\t\tbreak;\n\tdefault:\n\t\tr = 0;\n\t\tbreak;\n\t}\n\treturn r;\n\n}\n\nlong kvm_arch_dev_ioctl(struct file *filp,\n                        unsigned int ioctl, unsigned long arg)\n{\n\treturn -EINVAL;\n}\n\nvoid kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,\n\t\t\t   struct kvm_memory_slot *dont)\n{\n\tkvmppc_core_free_memslot(kvm, free, dont);\n}\n\nint kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,\n\t\t\t    unsigned long npages)\n{\n\treturn kvmppc_core_create_memslot(kvm, slot, npages);\n}\n\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *memslot,\n\t\t\t\t   const struct kvm_userspace_memory_region *mem,\n\t\t\t\t   enum kvm_mr_change change)\n{\n\treturn kvmppc_core_prepare_memory_region(kvm, memslot, mem);\n}\n\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\n\t\t\t\t   const struct kvm_userspace_memory_region *mem,\n\t\t\t\t   const struct kvm_memory_slot *old,\n\t\t\t\t   const struct kvm_memory_slot *new,\n\t\t\t\t   enum kvm_mr_change change)\n{\n\tkvmppc_core_commit_memory_region(kvm, mem, old, new);\n}\n\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot)\n{\n\tkvmppc_core_flush_memslot(kvm, slot);\n}\n\nstruct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)\n{\n\tstruct kvm_vcpu *vcpu;\n\tvcpu = kvmppc_core_vcpu_create(kvm, id);\n\tif (!IS_ERR(vcpu)) {\n\t\tvcpu->arch.wqp = &vcpu->wq;\n\t\tkvmppc_create_vcpu_debugfs(vcpu, id);\n\t}\n\treturn vcpu;\n}\n\nvoid kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\n{\n}\n\nvoid kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)\n{\n\t/* Make sure we're not using the vcpu anymore */\n\thrtimer_cancel(&vcpu->arch.dec_timer);\n\n\tkvmppc_remove_vcpu_debugfs(vcpu);\n\n\tswitch (vcpu->arch.irq_type) {\n\tcase KVMPPC_IRQ_MPIC:\n\t\tkvmppc_mpic_disconnect_vcpu(vcpu->arch.mpic, vcpu);\n\t\tbreak;\n\tcase KVMPPC_IRQ_XICS:\n\t\tif (xive_enabled())\n\t\t\tkvmppc_xive_cleanup_vcpu(vcpu);\n\t\telse\n\t\t\tkvmppc_xics_free_icp(vcpu);\n\t\tbreak;\n\t}\n\n\tkvmppc_core_vcpu_free(vcpu);\n}\n\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tkvm_arch_vcpu_free(vcpu);\n}\n\nint kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)\n{\n\treturn kvmppc_core_pending_dec(vcpu);\n}\n\nstatic enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)\n{\n\tstruct kvm_vcpu *vcpu;\n\n\tvcpu = container_of(timer, struct kvm_vcpu, arch.dec_timer);\n\tkvmppc_decrementer_func(vcpu);\n\n\treturn HRTIMER_NORESTART;\n}\n\nint kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\thrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);\n\tvcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;\n\tvcpu->arch.dec_expires = ~(u64)0;\n\n#ifdef CONFIG_KVM_EXIT_TIMING\n\tmutex_init(&vcpu->arch.exit_timing_lock);\n#endif\n\tret = kvmppc_subarch_vcpu_init(vcpu);\n\treturn ret;\n}\n\nvoid kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)\n{\n\tkvmppc_mmu_destroy(vcpu);\n\tkvmppc_subarch_vcpu_uninit(vcpu);\n}\n\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n#ifdef CONFIG_BOOKE\n\t/*\n\t * vrsave (formerly usprg0) isn't used by Linux, but may\n\t * be used by the guest.\n\t *\n\t * On non-booke this is associated with Altivec and\n\t * is handled by code in book3s.c.\n\t */\n\tmtspr(SPRN_VRSAVE, vcpu->arch.vrsave);\n#endif\n\tkvmppc_core_vcpu_load(vcpu, cpu);\n}\n\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tkvmppc_core_vcpu_put(vcpu);\n#ifdef CONFIG_BOOKE\n\tvcpu->arch.vrsave = mfspr(SPRN_VRSAVE);\n#endif\n}\n\n/*\n * irq_bypass_add_producer and irq_bypass_del_producer are only\n * useful if the architecture supports PCI passthrough.\n * irq_bypass_stop and irq_bypass_start are not needed and so\n * kvm_ops are not defined for them.\n */\nbool kvm_arch_has_irq_bypass(void)\n{\n\treturn ((kvmppc_hv_ops && kvmppc_hv_ops->irq_bypass_add_producer) ||\n\t\t(kvmppc_pr_ops && kvmppc_pr_ops->irq_bypass_add_producer));\n}\n\nint kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t     struct irq_bypass_producer *prod)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\tstruct kvm *kvm = irqfd->kvm;\n\n\tif (kvm->arch.kvm_ops->irq_bypass_add_producer)\n\t\treturn kvm->arch.kvm_ops->irq_bypass_add_producer(cons, prod);\n\n\treturn 0;\n}\n\nvoid kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t      struct irq_bypass_producer *prod)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\tstruct kvm *kvm = irqfd->kvm;\n\n\tif (kvm->arch.kvm_ops->irq_bypass_del_producer)\n\t\tkvm->arch.kvm_ops->irq_bypass_del_producer(cons, prod);\n}\n\n#ifdef CONFIG_VSX\nstatic inline int kvmppc_get_vsr_dword_offset(int index)\n{\n\tint offset;\n\n\tif ((index != 0) && (index != 1))\n\t\treturn -1;\n\n#ifdef __BIG_ENDIAN\n\toffset =  index;\n#else\n\toffset = 1 - index;\n#endif\n\n\treturn offset;\n}\n\nstatic inline int kvmppc_get_vsr_word_offset(int index)\n{\n\tint offset;\n\n\tif ((index > 3) || (index < 0))\n\t\treturn -1;\n\n#ifdef __BIG_ENDIAN\n\toffset = index;\n#else\n\toffset = 3 - index;\n#endif\n\treturn offset;\n}\n\nstatic inline void kvmppc_set_vsr_dword(struct kvm_vcpu *vcpu,\n\tu64 gpr)\n{\n\tunion kvmppc_one_reg val;\n\tint offset = kvmppc_get_vsr_dword_offset(vcpu->arch.mmio_vsx_offset);\n\tint index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;\n\n\tif (offset == -1)\n\t\treturn;\n\n\tif (vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\tval.vval = VCPU_VSX_VR(vcpu, index);\n\t\tval.vsxval[offset] = gpr;\n\t\tVCPU_VSX_VR(vcpu, index) = val.vval;\n\t} else {\n\t\tVCPU_VSX_FPR(vcpu, index, offset) = gpr;\n\t}\n}\n\nstatic inline void kvmppc_set_vsr_dword_dump(struct kvm_vcpu *vcpu,\n\tu64 gpr)\n{\n\tunion kvmppc_one_reg val;\n\tint index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;\n\n\tif (vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\tval.vval = VCPU_VSX_VR(vcpu, index);\n\t\tval.vsxval[0] = gpr;\n\t\tval.vsxval[1] = gpr;\n\t\tVCPU_VSX_VR(vcpu, index) = val.vval;\n\t} else {\n\t\tVCPU_VSX_FPR(vcpu, index, 0) = gpr;\n\t\tVCPU_VSX_FPR(vcpu, index, 1) = gpr;\n\t}\n}\n\nstatic inline void kvmppc_set_vsr_word(struct kvm_vcpu *vcpu,\n\tu32 gpr32)\n{\n\tunion kvmppc_one_reg val;\n\tint offset = kvmppc_get_vsr_word_offset(vcpu->arch.mmio_vsx_offset);\n\tint index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;\n\tint dword_offset, word_offset;\n\n\tif (offset == -1)\n\t\treturn;\n\n\tif (vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\tval.vval = VCPU_VSX_VR(vcpu, index);\n\t\tval.vsx32val[offset] = gpr32;\n\t\tVCPU_VSX_VR(vcpu, index) = val.vval;\n\t} else {\n\t\tdword_offset = offset / 2;\n\t\tword_offset = offset % 2;\n\t\tval.vsxval[0] = VCPU_VSX_FPR(vcpu, index, dword_offset);\n\t\tval.vsx32val[word_offset] = gpr32;\n\t\tVCPU_VSX_FPR(vcpu, index, dword_offset) = val.vsxval[0];\n\t}\n}\n#endif /* CONFIG_VSX */\n\n#ifdef CONFIG_PPC_FPU\nstatic inline u64 sp_to_dp(u32 fprs)\n{\n\tu64 fprd;\n\n\tpreempt_disable();\n\tenable_kernel_fp();\n\tasm (\"lfs%U1%X1 0,%1; stfd%U0%X0 0,%0\" : \"=m\" (fprd) : \"m\" (fprs)\n\t     : \"fr0\");\n\tpreempt_enable();\n\treturn fprd;\n}\n\nstatic inline u32 dp_to_sp(u64 fprd)\n{\n\tu32 fprs;\n\n\tpreempt_disable();\n\tenable_kernel_fp();\n\tasm (\"lfd%U1%X1 0,%1; stfs%U0%X0 0,%0\" : \"=m\" (fprs) : \"m\" (fprd)\n\t     : \"fr0\");\n\tpreempt_enable();\n\treturn fprs;\n}\n\n#else\n#define sp_to_dp(x)\t(x)\n#define dp_to_sp(x)\t(x)\n#endif /* CONFIG_PPC_FPU */\n\nstatic void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,\n                                      struct kvm_run *run)\n{\n\tu64 uninitialized_var(gpr);\n\n\tif (run->mmio.len > sizeof(gpr)) {\n\t\tprintk(KERN_ERR \"bad MMIO length: %d\\n\", run->mmio.len);\n\t\treturn;\n\t}\n\n\tif (!vcpu->arch.mmio_host_swabbed) {\n\t\tswitch (run->mmio.len) {\n\t\tcase 8: gpr = *(u64 *)run->mmio.data; break;\n\t\tcase 4: gpr = *(u32 *)run->mmio.data; break;\n\t\tcase 2: gpr = *(u16 *)run->mmio.data; break;\n\t\tcase 1: gpr = *(u8 *)run->mmio.data; break;\n\t\t}\n\t} else {\n\t\tswitch (run->mmio.len) {\n\t\tcase 8: gpr = swab64(*(u64 *)run->mmio.data); break;\n\t\tcase 4: gpr = swab32(*(u32 *)run->mmio.data); break;\n\t\tcase 2: gpr = swab16(*(u16 *)run->mmio.data); break;\n\t\tcase 1: gpr = *(u8 *)run->mmio.data; break;\n\t\t}\n\t}\n\n\t/* conversion between single and double precision */\n\tif ((vcpu->arch.mmio_sp64_extend) && (run->mmio.len == 4))\n\t\tgpr = sp_to_dp(gpr);\n\n\tif (vcpu->arch.mmio_sign_extend) {\n\t\tswitch (run->mmio.len) {\n#ifdef CONFIG_PPC64\n\t\tcase 4:\n\t\t\tgpr = (s64)(s32)gpr;\n\t\t\tbreak;\n#endif\n\t\tcase 2:\n\t\t\tgpr = (s64)(s16)gpr;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tgpr = (s64)(s8)gpr;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tswitch (vcpu->arch.io_gpr & KVM_MMIO_REG_EXT_MASK) {\n\tcase KVM_MMIO_REG_GPR:\n\t\tkvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);\n\t\tbreak;\n\tcase KVM_MMIO_REG_FPR:\n\t\tVCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;\n\t\tbreak;\n#ifdef CONFIG_PPC_BOOK3S\n\tcase KVM_MMIO_REG_QPR:\n\t\tvcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;\n\t\tbreak;\n\tcase KVM_MMIO_REG_FQPR:\n\t\tVCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;\n\t\tvcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;\n\t\tbreak;\n#endif\n#ifdef CONFIG_VSX\n\tcase KVM_MMIO_REG_VSX:\n\t\tif (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_DWORD)\n\t\t\tkvmppc_set_vsr_dword(vcpu, gpr);\n\t\telse if (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_WORD)\n\t\t\tkvmppc_set_vsr_word(vcpu, gpr);\n\t\telse if (vcpu->arch.mmio_vsx_copy_type ==\n\t\t\t\tKVMPPC_VSX_COPY_DWORD_LOAD_DUMP)\n\t\t\tkvmppc_set_vsr_dword_dump(vcpu, gpr);\n\t\tbreak;\n#endif\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int __kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\t\tunsigned int rt, unsigned int bytes,\n\t\t\t\tint is_default_endian, int sign_extend)\n{\n\tint idx, ret;\n\tbool host_swabbed;\n\n\t/* Pity C doesn't have a logical XOR operator */\n\tif (kvmppc_need_byteswap(vcpu)) {\n\t\thost_swabbed = is_default_endian;\n\t} else {\n\t\thost_swabbed = !is_default_endian;\n\t}\n\n\tif (bytes > sizeof(run->mmio.data)) {\n\t\tprintk(KERN_ERR \"%s: bad MMIO length: %d\\n\", __func__,\n\t\t       run->mmio.len);\n\t}\n\n\trun->mmio.phys_addr = vcpu->arch.paddr_accessed;\n\trun->mmio.len = bytes;\n\trun->mmio.is_write = 0;\n\n\tvcpu->arch.io_gpr = rt;\n\tvcpu->arch.mmio_host_swabbed = host_swabbed;\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_is_write = 0;\n\tvcpu->arch.mmio_sign_extend = sign_extend;\n\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\n\tret = kvm_io_bus_read(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,\n\t\t\t      bytes, &run->mmio.data);\n\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\n\tif (!ret) {\n\t\tkvmppc_complete_mmio_load(vcpu, run);\n\t\tvcpu->mmio_needed = 0;\n\t\treturn EMULATE_DONE;\n\t}\n\n\treturn EMULATE_DO_MMIO;\n}\n\nint kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t       unsigned int rt, unsigned int bytes,\n\t\t       int is_default_endian)\n{\n\treturn __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 0);\n}\nEXPORT_SYMBOL_GPL(kvmppc_handle_load);\n\n/* Same as above, but sign extends */\nint kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\tunsigned int rt, unsigned int bytes,\n\t\t\tint is_default_endian)\n{\n\treturn __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 1);\n}\n\n#ifdef CONFIG_VSX\nint kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\tunsigned int rt, unsigned int bytes,\n\t\t\tint is_default_endian, int mmio_sign_extend)\n{\n\tenum emulation_result emulated = EMULATE_DONE;\n\n\t/* Currently, mmio_vsx_copy_nums only allowed to be less than 4 */\n\tif ( (vcpu->arch.mmio_vsx_copy_nums > 4) ||\n\t\t(vcpu->arch.mmio_vsx_copy_nums < 0) ) {\n\t\treturn EMULATE_FAIL;\n\t}\n\n\twhile (vcpu->arch.mmio_vsx_copy_nums) {\n\t\temulated = __kvmppc_handle_load(run, vcpu, rt, bytes,\n\t\t\tis_default_endian, mmio_sign_extend);\n\n\t\tif (emulated != EMULATE_DONE)\n\t\t\tbreak;\n\n\t\tvcpu->arch.paddr_accessed += run->mmio.len;\n\n\t\tvcpu->arch.mmio_vsx_copy_nums--;\n\t\tvcpu->arch.mmio_vsx_offset++;\n\t}\n\treturn emulated;\n}\n#endif /* CONFIG_VSX */\n\nint kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\tu64 val, unsigned int bytes, int is_default_endian)\n{\n\tvoid *data = run->mmio.data;\n\tint idx, ret;\n\tbool host_swabbed;\n\n\t/* Pity C doesn't have a logical XOR operator */\n\tif (kvmppc_need_byteswap(vcpu)) {\n\t\thost_swabbed = is_default_endian;\n\t} else {\n\t\thost_swabbed = !is_default_endian;\n\t}\n\n\tif (bytes > sizeof(run->mmio.data)) {\n\t\tprintk(KERN_ERR \"%s: bad MMIO length: %d\\n\", __func__,\n\t\t       run->mmio.len);\n\t}\n\n\trun->mmio.phys_addr = vcpu->arch.paddr_accessed;\n\trun->mmio.len = bytes;\n\trun->mmio.is_write = 1;\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_is_write = 1;\n\n\tif ((vcpu->arch.mmio_sp64_extend) && (bytes == 4))\n\t\tval = dp_to_sp(val);\n\n\t/* Store the value at the lowest bytes in 'data'. */\n\tif (!host_swabbed) {\n\t\tswitch (bytes) {\n\t\tcase 8: *(u64 *)data = val; break;\n\t\tcase 4: *(u32 *)data = val; break;\n\t\tcase 2: *(u16 *)data = val; break;\n\t\tcase 1: *(u8  *)data = val; break;\n\t\t}\n\t} else {\n\t\tswitch (bytes) {\n\t\tcase 8: *(u64 *)data = swab64(val); break;\n\t\tcase 4: *(u32 *)data = swab32(val); break;\n\t\tcase 2: *(u16 *)data = swab16(val); break;\n\t\tcase 1: *(u8  *)data = val; break;\n\t\t}\n\t}\n\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\n\tret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,\n\t\t\t       bytes, &run->mmio.data);\n\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\n\tif (!ret) {\n\t\tvcpu->mmio_needed = 0;\n\t\treturn EMULATE_DONE;\n\t}\n\n\treturn EMULATE_DO_MMIO;\n}\nEXPORT_SYMBOL_GPL(kvmppc_handle_store);\n\n#ifdef CONFIG_VSX\nstatic inline int kvmppc_get_vsr_data(struct kvm_vcpu *vcpu, int rs, u64 *val)\n{\n\tu32 dword_offset, word_offset;\n\tunion kvmppc_one_reg reg;\n\tint vsx_offset = 0;\n\tint copy_type = vcpu->arch.mmio_vsx_copy_type;\n\tint result = 0;\n\n\tswitch (copy_type) {\n\tcase KVMPPC_VSX_COPY_DWORD:\n\t\tvsx_offset =\n\t\t\tkvmppc_get_vsr_dword_offset(vcpu->arch.mmio_vsx_offset);\n\n\t\tif (vsx_offset == -1) {\n\t\t\tresult = -1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\t\t*val = VCPU_VSX_FPR(vcpu, rs, vsx_offset);\n\t\t} else {\n\t\t\treg.vval = VCPU_VSX_VR(vcpu, rs);\n\t\t\t*val = reg.vsxval[vsx_offset];\n\t\t}\n\t\tbreak;\n\n\tcase KVMPPC_VSX_COPY_WORD:\n\t\tvsx_offset =\n\t\t\tkvmppc_get_vsr_word_offset(vcpu->arch.mmio_vsx_offset);\n\n\t\tif (vsx_offset == -1) {\n\t\t\tresult = -1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\t\tdword_offset = vsx_offset / 2;\n\t\t\tword_offset = vsx_offset % 2;\n\t\t\treg.vsxval[0] = VCPU_VSX_FPR(vcpu, rs, dword_offset);\n\t\t\t*val = reg.vsx32val[word_offset];\n\t\t} else {\n\t\t\treg.vval = VCPU_VSX_VR(vcpu, rs);\n\t\t\t*val = reg.vsx32val[vsx_offset];\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tresult = -1;\n\t\tbreak;\n\t}\n\n\treturn result;\n}\n\nint kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\tint rs, unsigned int bytes, int is_default_endian)\n{\n\tu64 val;\n\tenum emulation_result emulated = EMULATE_DONE;\n\n\tvcpu->arch.io_gpr = rs;\n\n\t/* Currently, mmio_vsx_copy_nums only allowed to be less than 4 */\n\tif ( (vcpu->arch.mmio_vsx_copy_nums > 4) ||\n\t\t(vcpu->arch.mmio_vsx_copy_nums < 0) ) {\n\t\treturn EMULATE_FAIL;\n\t}\n\n\twhile (vcpu->arch.mmio_vsx_copy_nums) {\n\t\tif (kvmppc_get_vsr_data(vcpu, rs, &val) == -1)\n\t\t\treturn EMULATE_FAIL;\n\n\t\temulated = kvmppc_handle_store(run, vcpu,\n\t\t\t val, bytes, is_default_endian);\n\n\t\tif (emulated != EMULATE_DONE)\n\t\t\tbreak;\n\n\t\tvcpu->arch.paddr_accessed += run->mmio.len;\n\n\t\tvcpu->arch.mmio_vsx_copy_nums--;\n\t\tvcpu->arch.mmio_vsx_offset++;\n\t}\n\n\treturn emulated;\n}\n\nstatic int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu,\n\t\t\tstruct kvm_run *run)\n{\n\tenum emulation_result emulated = EMULATE_FAIL;\n\tint r;\n\n\tvcpu->arch.paddr_accessed += run->mmio.len;\n\n\tif (!vcpu->mmio_is_write) {\n\t\temulated = kvmppc_handle_vsx_load(run, vcpu, vcpu->arch.io_gpr,\n\t\t\t run->mmio.len, 1, vcpu->arch.mmio_sign_extend);\n\t} else {\n\t\temulated = kvmppc_handle_vsx_store(run, vcpu,\n\t\t\t vcpu->arch.io_gpr, run->mmio.len, 1);\n\t}\n\n\tswitch (emulated) {\n\tcase EMULATE_DO_MMIO:\n\t\trun->exit_reason = KVM_EXIT_MMIO;\n\t\tr = RESUME_HOST;\n\t\tbreak;\n\tcase EMULATE_FAIL:\n\t\tpr_info(\"KVM: MMIO emulation failed (VSX repeat)\\n\");\n\t\trun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\trun->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tr = RESUME_HOST;\n\t\tbreak;\n\tdefault:\n\t\tr = RESUME_GUEST;\n\t\tbreak;\n\t}\n\treturn r;\n}\n#endif /* CONFIG_VSX */\n\nint kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)\n{\n\tint r = 0;\n\tunion kvmppc_one_reg val;\n\tint size;\n\n\tsize = one_reg_size(reg->id);\n\tif (size > sizeof(val))\n\t\treturn -EINVAL;\n\n\tr = kvmppc_get_one_reg(vcpu, reg->id, &val);\n\tif (r == -EINVAL) {\n\t\tr = 0;\n\t\tswitch (reg->id) {\n#ifdef CONFIG_ALTIVEC\n\t\tcase KVM_REG_PPC_VR0 ... KVM_REG_PPC_VR31:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tval.vval = vcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0];\n\t\t\tbreak;\n\t\tcase KVM_REG_PPC_VSCR:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tval = get_reg_val(reg->id, vcpu->arch.vr.vscr.u[3]);\n\t\t\tbreak;\n\t\tcase KVM_REG_PPC_VRSAVE:\n\t\t\tval = get_reg_val(reg->id, vcpu->arch.vrsave);\n\t\t\tbreak;\n#endif /* CONFIG_ALTIVEC */\n\t\tdefault:\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (r)\n\t\treturn r;\n\n\tif (copy_to_user((char __user *)(unsigned long)reg->addr, &val, size))\n\t\tr = -EFAULT;\n\n\treturn r;\n}\n\nint kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)\n{\n\tint r;\n\tunion kvmppc_one_reg val;\n\tint size;\n\n\tsize = one_reg_size(reg->id);\n\tif (size > sizeof(val))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&val, (char __user *)(unsigned long)reg->addr, size))\n\t\treturn -EFAULT;\n\n\tr = kvmppc_set_one_reg(vcpu, reg->id, &val);\n\tif (r == -EINVAL) {\n\t\tr = 0;\n\t\tswitch (reg->id) {\n#ifdef CONFIG_ALTIVEC\n\t\tcase KVM_REG_PPC_VR0 ... KVM_REG_PPC_VR31:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0] = val.vval;\n\t\t\tbreak;\n\t\tcase KVM_REG_PPC_VSCR:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvcpu->arch.vr.vscr.u[3] = set_reg_val(reg->id, val);\n\t\t\tbreak;\n\t\tcase KVM_REG_PPC_VRSAVE:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvcpu->arch.vrsave = set_reg_val(reg->id, val);\n\t\t\tbreak;\n#endif /* CONFIG_ALTIVEC */\n\t\tdefault:\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn r;\n}\n\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tint r;\n\tsigset_t sigsaved;\n\n\tif (vcpu->mmio_needed) {\n\t\tvcpu->mmio_needed = 0;\n\t\tif (!vcpu->mmio_is_write)\n\t\t\tkvmppc_complete_mmio_load(vcpu, run);\n#ifdef CONFIG_VSX\n\t\tif (vcpu->arch.mmio_vsx_copy_nums > 0) {\n\t\t\tvcpu->arch.mmio_vsx_copy_nums--;\n\t\t\tvcpu->arch.mmio_vsx_offset++;\n\t\t}\n\n\t\tif (vcpu->arch.mmio_vsx_copy_nums > 0) {\n\t\t\tr = kvmppc_emulate_mmio_vsx_loadstore(vcpu, run);\n\t\t\tif (r == RESUME_HOST) {\n\t\t\t\tvcpu->mmio_needed = 1;\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n#endif\n\t} else if (vcpu->arch.osi_needed) {\n\t\tu64 *gprs = run->osi.gprs;\n\t\tint i;\n\n\t\tfor (i = 0; i < 32; i++)\n\t\t\tkvmppc_set_gpr(vcpu, i, gprs[i]);\n\t\tvcpu->arch.osi_needed = 0;\n\t} else if (vcpu->arch.hcall_needed) {\n\t\tint i;\n\n\t\tkvmppc_set_gpr(vcpu, 3, run->papr_hcall.ret);\n\t\tfor (i = 0; i < 9; ++i)\n\t\t\tkvmppc_set_gpr(vcpu, 4 + i, run->papr_hcall.args[i]);\n\t\tvcpu->arch.hcall_needed = 0;\n#ifdef CONFIG_BOOKE\n\t} else if (vcpu->arch.epr_needed) {\n\t\tkvmppc_set_epr(vcpu, run->epr.epr);\n\t\tvcpu->arch.epr_needed = 0;\n#endif\n\t}\n\n\tif (vcpu->sigset_active)\n\t\tsigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);\n\n\tif (run->immediate_exit)\n\t\tr = -EINTR;\n\telse\n\t\tr = kvmppc_vcpu_run(run, vcpu);\n\n\tif (vcpu->sigset_active)\n\t\tsigprocmask(SIG_SETMASK, &sigsaved, NULL);\n\n\treturn r;\n}\n\nint kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)\n{\n\tif (irq->irq == KVM_INTERRUPT_UNSET) {\n\t\tkvmppc_core_dequeue_external(vcpu);\n\t\treturn 0;\n\t}\n\n\tkvmppc_core_queue_external(vcpu, irq);\n\n\tkvm_vcpu_kick(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_enable_cap *cap)\n{\n\tint r;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n\tcase KVM_CAP_PPC_OSI:\n\t\tr = 0;\n\t\tvcpu->arch.osi_enabled = true;\n\t\tbreak;\n\tcase KVM_CAP_PPC_PAPR:\n\t\tr = 0;\n\t\tvcpu->arch.papr_enabled = true;\n\t\tbreak;\n\tcase KVM_CAP_PPC_EPR:\n\t\tr = 0;\n\t\tif (cap->args[0])\n\t\t\tvcpu->arch.epr_flags |= KVMPPC_EPR_USER;\n\t\telse\n\t\t\tvcpu->arch.epr_flags &= ~KVMPPC_EPR_USER;\n\t\tbreak;\n#ifdef CONFIG_BOOKE\n\tcase KVM_CAP_PPC_BOOKE_WATCHDOG:\n\t\tr = 0;\n\t\tvcpu->arch.watchdog_enabled = true;\n\t\tbreak;\n#endif\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\n\tcase KVM_CAP_SW_TLB: {\n\t\tstruct kvm_config_tlb cfg;\n\t\tvoid __user *user_ptr = (void __user *)(uintptr_t)cap->args[0];\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cfg, user_ptr, sizeof(cfg)))\n\t\t\tbreak;\n\n\t\tr = kvm_vcpu_ioctl_config_tlb(vcpu, &cfg);\n\t\tbreak;\n\t}\n#endif\n#ifdef CONFIG_KVM_MPIC\n\tcase KVM_CAP_IRQ_MPIC: {\n\t\tstruct fd f;\n\t\tstruct kvm_device *dev;\n\n\t\tr = -EBADF;\n\t\tf = fdget(cap->args[0]);\n\t\tif (!f.file)\n\t\t\tbreak;\n\n\t\tr = -EPERM;\n\t\tdev = kvm_device_from_filp(f.file);\n\t\tif (dev)\n\t\t\tr = kvmppc_mpic_connect_vcpu(dev, vcpu, cap->args[1]);\n\n\t\tfdput(f);\n\t\tbreak;\n\t}\n#endif\n#ifdef CONFIG_KVM_XICS\n\tcase KVM_CAP_IRQ_XICS: {\n\t\tstruct fd f;\n\t\tstruct kvm_device *dev;\n\n\t\tr = -EBADF;\n\t\tf = fdget(cap->args[0]);\n\t\tif (!f.file)\n\t\t\tbreak;\n\n\t\tr = -EPERM;\n\t\tdev = kvm_device_from_filp(f.file);\n\t\tif (dev) {\n\t\t\tif (xive_enabled())\n\t\t\t\tr = kvmppc_xive_connect_vcpu(dev, vcpu, cap->args[1]);\n\t\t\telse\n\t\t\t\tr = kvmppc_xics_connect_vcpu(dev, vcpu, cap->args[1]);\n\t\t}\n\n\t\tfdput(f);\n\t\tbreak;\n\t}\n#endif /* CONFIG_KVM_XICS */\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tcase KVM_CAP_PPC_FWNMI:\n\t\tr = -EINVAL;\n\t\tif (!is_kvmppc_hv_enabled(vcpu->kvm))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tvcpu->kvm->arch.fwnmi_enabled = true;\n\t\tbreak;\n#endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (!r)\n\t\tr = kvmppc_sanity_check(vcpu);\n\n\treturn r;\n}\n\nbool kvm_arch_intc_initialized(struct kvm *kvm)\n{\n#ifdef CONFIG_KVM_MPIC\n\tif (kvm->arch.mpic)\n\t\treturn true;\n#endif\n#ifdef CONFIG_KVM_XICS\n\tif (kvm->arch.xics || kvm->arch.xive)\n\t\treturn true;\n#endif\n\treturn false;\n}\n\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\n                                    struct kvm_mp_state *mp_state)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\n                                    struct kvm_mp_state *mp_state)\n{\n\treturn -EINVAL;\n}\n\nlong kvm_arch_vcpu_ioctl(struct file *filp,\n                         unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tlong r;\n\n\tswitch (ioctl) {\n\tcase KVM_INTERRUPT: {\n\t\tstruct kvm_interrupt irq;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&irq, argp, sizeof(irq)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_interrupt(vcpu, &irq);\n\t\tgoto out;\n\t}\n\n\tcase KVM_ENABLE_CAP:\n\t{\n\t\tstruct kvm_enable_cap cap;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cap, argp, sizeof(cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);\n\t\tbreak;\n\t}\n\n\tcase KVM_SET_ONE_REG:\n\tcase KVM_GET_ONE_REG:\n\t{\n\t\tstruct kvm_one_reg reg;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&reg, argp, sizeof(reg)))\n\t\t\tgoto out;\n\t\tif (ioctl == KVM_SET_ONE_REG)\n\t\t\tr = kvm_vcpu_ioctl_set_one_reg(vcpu, &reg);\n\t\telse\n\t\t\tr = kvm_vcpu_ioctl_get_one_reg(vcpu, &reg);\n\t\tbreak;\n\t}\n\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\n\tcase KVM_DIRTY_TLB: {\n\t\tstruct kvm_dirty_tlb dirty;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dirty, argp, sizeof(dirty)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_dirty_tlb(vcpu, &dirty);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tr = -EINVAL;\n\t}\n\nout:\n\treturn r;\n}\n\nint kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)\n{\n\tu32 inst_nop = 0x60000000;\n#ifdef CONFIG_KVM_BOOKE_HV\n\tu32 inst_sc1 = 0x44000022;\n\tpvinfo->hcall[0] = cpu_to_be32(inst_sc1);\n\tpvinfo->hcall[1] = cpu_to_be32(inst_nop);\n\tpvinfo->hcall[2] = cpu_to_be32(inst_nop);\n\tpvinfo->hcall[3] = cpu_to_be32(inst_nop);\n#else\n\tu32 inst_lis = 0x3c000000;\n\tu32 inst_ori = 0x60000000;\n\tu32 inst_sc = 0x44000002;\n\tu32 inst_imm_mask = 0xffff;\n\n\t/*\n\t * The hypercall to get into KVM from within guest context is as\n\t * follows:\n\t *\n\t *    lis r0, r0, KVM_SC_MAGIC_R0@h\n\t *    ori r0, KVM_SC_MAGIC_R0@l\n\t *    sc\n\t *    nop\n\t */\n\tpvinfo->hcall[0] = cpu_to_be32(inst_lis | ((KVM_SC_MAGIC_R0 >> 16) & inst_imm_mask));\n\tpvinfo->hcall[1] = cpu_to_be32(inst_ori | (KVM_SC_MAGIC_R0 & inst_imm_mask));\n\tpvinfo->hcall[2] = cpu_to_be32(inst_sc);\n\tpvinfo->hcall[3] = cpu_to_be32(inst_nop);\n#endif\n\n\tpvinfo->flags = KVM_PPC_PVINFO_FLAGS_EV_IDLE;\n\n\treturn 0;\n}\n\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,\n\t\t\t  bool line_status)\n{\n\tif (!irqchip_in_kernel(kvm))\n\t\treturn -ENXIO;\n\n\tirq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,\n\t\t\t\t\tirq_event->irq, irq_event->level,\n\t\t\t\t\tline_status);\n\treturn 0;\n}\n\n\nstatic int kvm_vm_ioctl_enable_cap(struct kvm *kvm,\n\t\t\t\t   struct kvm_enable_cap *cap)\n{\n\tint r;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n#ifdef CONFIG_KVM_BOOK3S_64_HANDLER\n\tcase KVM_CAP_PPC_ENABLE_HCALL: {\n\t\tunsigned long hcall = cap->args[0];\n\n\t\tr = -EINVAL;\n\t\tif (hcall > MAX_HCALL_OPCODE || (hcall & 3) ||\n\t\t    cap->args[1] > 1)\n\t\t\tbreak;\n\t\tif (!kvmppc_book3s_hcall_implemented(kvm, hcall))\n\t\t\tbreak;\n\t\tif (cap->args[1])\n\t\t\tset_bit(hcall / 4, kvm->arch.enabled_hcalls);\n\t\telse\n\t\t\tclear_bit(hcall / 4, kvm->arch.enabled_hcalls);\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_CAP_PPC_SMT: {\n\t\tunsigned long mode = cap->args[0];\n\t\tunsigned long flags = cap->args[1];\n\n\t\tr = -EINVAL;\n\t\tif (kvm->arch.kvm_ops->set_smt_mode)\n\t\t\tr = kvm->arch.kvm_ops->set_smt_mode(kvm, mode, flags);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nlong kvm_arch_vm_ioctl(struct file *filp,\n                       unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm __maybe_unused = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tlong r;\n\n\tswitch (ioctl) {\n\tcase KVM_PPC_GET_PVINFO: {\n\t\tstruct kvm_ppc_pvinfo pvinfo;\n\t\tmemset(&pvinfo, 0, sizeof(pvinfo));\n\t\tr = kvm_vm_ioctl_get_pvinfo(&pvinfo);\n\t\tif (copy_to_user(argp, &pvinfo, sizeof(pvinfo))) {\n\t\t\tr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbreak;\n\t}\n\tcase KVM_ENABLE_CAP:\n\t{\n\t\tstruct kvm_enable_cap cap;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cap, argp, sizeof(cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_enable_cap(kvm, &cap);\n\t\tbreak;\n\t}\n#ifdef CONFIG_SPAPR_TCE_IOMMU\n\tcase KVM_CREATE_SPAPR_TCE_64: {\n\t\tstruct kvm_create_spapr_tce_64 create_tce_64;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&create_tce_64, argp, sizeof(create_tce_64)))\n\t\t\tgoto out;\n\t\tif (create_tce_64.flags) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tr = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce_64);\n\t\tgoto out;\n\t}\n\tcase KVM_CREATE_SPAPR_TCE: {\n\t\tstruct kvm_create_spapr_tce create_tce;\n\t\tstruct kvm_create_spapr_tce_64 create_tce_64;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&create_tce, argp, sizeof(create_tce)))\n\t\t\tgoto out;\n\n\t\tcreate_tce_64.liobn = create_tce.liobn;\n\t\tcreate_tce_64.page_shift = IOMMU_PAGE_SHIFT_4K;\n\t\tcreate_tce_64.offset = 0;\n\t\tcreate_tce_64.size = create_tce.window_size >>\n\t\t\t\tIOMMU_PAGE_SHIFT_4K;\n\t\tcreate_tce_64.flags = 0;\n\t\tr = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce_64);\n\t\tgoto out;\n\t}\n#endif\n#ifdef CONFIG_PPC_BOOK3S_64\n\tcase KVM_PPC_GET_SMMU_INFO: {\n\t\tstruct kvm_ppc_smmu_info info;\n\t\tstruct kvm *kvm = filp->private_data;\n\n\t\tmemset(&info, 0, sizeof(info));\n\t\tr = kvm->arch.kvm_ops->get_smmu_info(kvm, &info);\n\t\tif (r >= 0 && copy_to_user(argp, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase KVM_PPC_RTAS_DEFINE_TOKEN: {\n\t\tstruct kvm *kvm = filp->private_data;\n\n\t\tr = kvm_vm_ioctl_rtas_define_token(kvm, argp);\n\t\tbreak;\n\t}\n\tcase KVM_PPC_CONFIGURE_V3_MMU: {\n\t\tstruct kvm *kvm = filp->private_data;\n\t\tstruct kvm_ppc_mmuv3_cfg cfg;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm->arch.kvm_ops->configure_mmu)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cfg, argp, sizeof(cfg)))\n\t\t\tgoto out;\n\t\tr = kvm->arch.kvm_ops->configure_mmu(kvm, &cfg);\n\t\tbreak;\n\t}\n\tcase KVM_PPC_GET_RMMU_INFO: {\n\t\tstruct kvm *kvm = filp->private_data;\n\t\tstruct kvm_ppc_rmmu_info info;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm->arch.kvm_ops->get_rmmu_info)\n\t\t\tgoto out;\n\t\tr = kvm->arch.kvm_ops->get_rmmu_info(kvm, &info);\n\t\tif (r >= 0 && copy_to_user(argp, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault: {\n\t\tstruct kvm *kvm = filp->private_data;\n\t\tr = kvm->arch.kvm_ops->arch_vm_ioctl(filp, ioctl, arg);\n\t}\n#else /* CONFIG_PPC_BOOK3S_64 */\n\tdefault:\n\t\tr = -ENOTTY;\n#endif\n\t}\nout:\n\treturn r;\n}\n\nstatic unsigned long lpid_inuse[BITS_TO_LONGS(KVMPPC_NR_LPIDS)];\nstatic unsigned long nr_lpids;\n\nlong kvmppc_alloc_lpid(void)\n{\n\tlong lpid;\n\n\tdo {\n\t\tlpid = find_first_zero_bit(lpid_inuse, KVMPPC_NR_LPIDS);\n\t\tif (lpid >= nr_lpids) {\n\t\t\tpr_err(\"%s: No LPIDs free\\n\", __func__);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t} while (test_and_set_bit(lpid, lpid_inuse));\n\n\treturn lpid;\n}\nEXPORT_SYMBOL_GPL(kvmppc_alloc_lpid);\n\nvoid kvmppc_claim_lpid(long lpid)\n{\n\tset_bit(lpid, lpid_inuse);\n}\nEXPORT_SYMBOL_GPL(kvmppc_claim_lpid);\n\nvoid kvmppc_free_lpid(long lpid)\n{\n\tclear_bit(lpid, lpid_inuse);\n}\nEXPORT_SYMBOL_GPL(kvmppc_free_lpid);\n\nvoid kvmppc_init_lpid(unsigned long nr_lpids_param)\n{\n\tnr_lpids = min_t(unsigned long, KVMPPC_NR_LPIDS, nr_lpids_param);\n\tmemset(lpid_inuse, 0, sizeof(lpid_inuse));\n}\nEXPORT_SYMBOL_GPL(kvmppc_init_lpid);\n\nint kvm_arch_init(void *opaque)\n{\n\treturn 0;\n}\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_ppc_instr);\n"], "fixing_code": ["/*\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License, version 2, as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n *\n * Copyright IBM Corp. 2007\n *\n * Authors: Hollis Blanchard <hollisb@us.ibm.com>\n *          Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>\n */\n\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/kvm_host.h>\n#include <linux/vmalloc.h>\n#include <linux/hrtimer.h>\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/module.h>\n#include <linux/irqbypass.h>\n#include <linux/kvm_irqfd.h>\n#include <asm/cputable.h>\n#include <linux/uaccess.h>\n#include <asm/kvm_ppc.h>\n#include <asm/tlbflush.h>\n#include <asm/cputhreads.h>\n#include <asm/irqflags.h>\n#include <asm/iommu.h>\n#include <asm/switch_to.h>\n#include <asm/xive.h>\n\n#include \"timing.h\"\n#include \"irq.h\"\n#include \"../mm/mmu_decl.h\"\n\n#define CREATE_TRACE_POINTS\n#include \"trace.h\"\n\nstruct kvmppc_ops *kvmppc_hv_ops;\nEXPORT_SYMBOL_GPL(kvmppc_hv_ops);\nstruct kvmppc_ops *kvmppc_pr_ops;\nEXPORT_SYMBOL_GPL(kvmppc_pr_ops);\n\n\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *v)\n{\n\treturn !!(v->arch.pending_exceptions) || kvm_request_pending(v);\n}\n\nbool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)\n{\n\treturn false;\n}\n\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\n/*\n * Common checks before entering the guest world.  Call with interrupts\n * disabled.\n *\n * returns:\n *\n * == 1 if we're ready to go into guest state\n * <= 0 if we need to go back to the host with return value\n */\nint kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\n\tWARN_ON(irqs_disabled());\n\thard_irq_disable();\n\n\twhile (true) {\n\t\tif (need_resched()) {\n\t\t\tlocal_irq_enable();\n\t\t\tcond_resched();\n\t\t\thard_irq_disable();\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (signal_pending(current)) {\n\t\t\tkvmppc_account_exit(vcpu, SIGNAL_EXITS);\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_INTR;\n\t\t\tr = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tvcpu->mode = IN_GUEST_MODE;\n\n\t\t/*\n\t\t * Reading vcpu->requests must happen after setting vcpu->mode,\n\t\t * so we don't miss a request because the requester sees\n\t\t * OUTSIDE_GUEST_MODE and assumes we'll be checking requests\n\t\t * before next entering the guest (and thus doesn't IPI).\n\t\t * This also orders the write to mode from any reads\n\t\t * to the page tables done while the VCPU is running.\n\t\t * Please see the comment in kvm_flush_remote_tlbs.\n\t\t */\n\t\tsmp_mb();\n\n\t\tif (kvm_request_pending(vcpu)) {\n\t\t\t/* Make sure we process requests preemptable */\n\t\t\tlocal_irq_enable();\n\t\t\ttrace_kvm_check_requests(vcpu);\n\t\t\tr = kvmppc_core_check_requests(vcpu);\n\t\t\thard_irq_disable();\n\t\t\tif (r > 0)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (kvmppc_core_prepare_to_enter(vcpu)) {\n\t\t\t/* interrupts got enabled in between, so we\n\t\t\t   are back at square 1 */\n\t\t\tcontinue;\n\t\t}\n\n\t\tguest_enter_irqoff();\n\t\treturn 1;\n\t}\n\n\t/* return to host */\n\tlocal_irq_enable();\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvmppc_prepare_to_enter);\n\n#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)\nstatic void kvmppc_swab_shared(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_vcpu_arch_shared *shared = vcpu->arch.shared;\n\tint i;\n\n\tshared->sprg0 = swab64(shared->sprg0);\n\tshared->sprg1 = swab64(shared->sprg1);\n\tshared->sprg2 = swab64(shared->sprg2);\n\tshared->sprg3 = swab64(shared->sprg3);\n\tshared->srr0 = swab64(shared->srr0);\n\tshared->srr1 = swab64(shared->srr1);\n\tshared->dar = swab64(shared->dar);\n\tshared->msr = swab64(shared->msr);\n\tshared->dsisr = swab32(shared->dsisr);\n\tshared->int_pending = swab32(shared->int_pending);\n\tfor (i = 0; i < ARRAY_SIZE(shared->sr); i++)\n\t\tshared->sr[i] = swab32(shared->sr[i]);\n}\n#endif\n\nint kvmppc_kvm_pv(struct kvm_vcpu *vcpu)\n{\n\tint nr = kvmppc_get_gpr(vcpu, 11);\n\tint r;\n\tunsigned long __maybe_unused param1 = kvmppc_get_gpr(vcpu, 3);\n\tunsigned long __maybe_unused param2 = kvmppc_get_gpr(vcpu, 4);\n\tunsigned long __maybe_unused param3 = kvmppc_get_gpr(vcpu, 5);\n\tunsigned long __maybe_unused param4 = kvmppc_get_gpr(vcpu, 6);\n\tunsigned long r2 = 0;\n\n\tif (!(kvmppc_get_msr(vcpu) & MSR_SF)) {\n\t\t/* 32 bit mode */\n\t\tparam1 &= 0xffffffff;\n\t\tparam2 &= 0xffffffff;\n\t\tparam3 &= 0xffffffff;\n\t\tparam4 &= 0xffffffff;\n\t}\n\n\tswitch (nr) {\n\tcase KVM_HCALL_TOKEN(KVM_HC_PPC_MAP_MAGIC_PAGE):\n\t{\n#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)\n\t\t/* Book3S can be little endian, find it out here */\n\t\tint shared_big_endian = true;\n\t\tif (vcpu->arch.intr_msr & MSR_LE)\n\t\t\tshared_big_endian = false;\n\t\tif (shared_big_endian != vcpu->arch.shared_big_endian)\n\t\t\tkvmppc_swab_shared(vcpu);\n\t\tvcpu->arch.shared_big_endian = shared_big_endian;\n#endif\n\n\t\tif (!(param2 & MAGIC_PAGE_FLAG_NOT_MAPPED_NX)) {\n\t\t\t/*\n\t\t\t * Older versions of the Linux magic page code had\n\t\t\t * a bug where they would map their trampoline code\n\t\t\t * NX. If that's the case, remove !PR NX capability.\n\t\t\t */\n\t\t\tvcpu->arch.disable_kernel_nx = true;\n\t\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\t\t}\n\n\t\tvcpu->arch.magic_page_pa = param1 & ~0xfffULL;\n\t\tvcpu->arch.magic_page_ea = param2 & ~0xfffULL;\n\n#ifdef CONFIG_PPC_64K_PAGES\n\t\t/*\n\t\t * Make sure our 4k magic page is in the same window of a 64k\n\t\t * page within the guest and within the host's page.\n\t\t */\n\t\tif ((vcpu->arch.magic_page_pa & 0xf000) !=\n\t\t    ((ulong)vcpu->arch.shared & 0xf000)) {\n\t\t\tvoid *old_shared = vcpu->arch.shared;\n\t\t\tulong shared = (ulong)vcpu->arch.shared;\n\t\t\tvoid *new_shared;\n\n\t\t\tshared &= PAGE_MASK;\n\t\t\tshared |= vcpu->arch.magic_page_pa & 0xf000;\n\t\t\tnew_shared = (void*)shared;\n\t\t\tmemcpy(new_shared, old_shared, 0x1000);\n\t\t\tvcpu->arch.shared = new_shared;\n\t\t}\n#endif\n\n\t\tr2 = KVM_MAGIC_FEAT_SR | KVM_MAGIC_FEAT_MAS0_TO_SPRG7;\n\n\t\tr = EV_SUCCESS;\n\t\tbreak;\n\t}\n\tcase KVM_HCALL_TOKEN(KVM_HC_FEATURES):\n\t\tr = EV_SUCCESS;\n#if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500V2)\n\t\tr2 |= (1 << KVM_FEATURE_MAGIC_PAGE);\n#endif\n\n\t\t/* Second return value is in r4 */\n\t\tbreak;\n\tcase EV_HCALL_TOKEN(EV_IDLE):\n\t\tr = EV_SUCCESS;\n\t\tkvm_vcpu_block(vcpu);\n\t\tkvm_clear_request(KVM_REQ_UNHALT, vcpu);\n\t\tbreak;\n\tdefault:\n\t\tr = EV_UNIMPLEMENTED;\n\t\tbreak;\n\t}\n\n\tkvmppc_set_gpr(vcpu, 4, r2);\n\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvmppc_kvm_pv);\n\nint kvmppc_sanity_check(struct kvm_vcpu *vcpu)\n{\n\tint r = false;\n\n\t/* We have to know what CPU to virtualize */\n\tif (!vcpu->arch.pvr)\n\t\tgoto out;\n\n\t/* PAPR only works with book3s_64 */\n\tif ((vcpu->arch.cpu_type != KVM_CPU_3S_64) && vcpu->arch.papr_enabled)\n\t\tgoto out;\n\n\t/* HV KVM can only do PAPR mode for now */\n\tif (!vcpu->arch.papr_enabled && is_kvmppc_hv_enabled(vcpu->kvm))\n\t\tgoto out;\n\n#ifdef CONFIG_KVM_BOOKE_HV\n\tif (!cpu_has_feature(CPU_FTR_EMB_HV))\n\t\tgoto out;\n#endif\n\n\tr = true;\n\nout:\n\tvcpu->arch.sane = r;\n\treturn r ? 0 : -EINVAL;\n}\nEXPORT_SYMBOL_GPL(kvmppc_sanity_check);\n\nint kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)\n{\n\tenum emulation_result er;\n\tint r;\n\n\ter = kvmppc_emulate_loadstore(vcpu);\n\tswitch (er) {\n\tcase EMULATE_DONE:\n\t\t/* Future optimization: only reload non-volatiles if they were\n\t\t * actually modified. */\n\t\tr = RESUME_GUEST_NV;\n\t\tbreak;\n\tcase EMULATE_AGAIN:\n\t\tr = RESUME_GUEST;\n\t\tbreak;\n\tcase EMULATE_DO_MMIO:\n\t\trun->exit_reason = KVM_EXIT_MMIO;\n\t\t/* We must reload nonvolatiles because \"update\" load/store\n\t\t * instructions modify register state. */\n\t\t/* Future optimization: only reload non-volatiles if they were\n\t\t * actually modified. */\n\t\tr = RESUME_HOST_NV;\n\t\tbreak;\n\tcase EMULATE_FAIL:\n\t{\n\t\tu32 last_inst;\n\n\t\tkvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst);\n\t\t/* XXX Deliver Program interrupt to guest. */\n\t\tpr_emerg(\"%s: emulation failed (%08x)\\n\", __func__, last_inst);\n\t\tr = RESUME_HOST;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tWARN_ON(1);\n\t\tr = RESUME_GUEST;\n\t}\n\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvmppc_emulate_mmio);\n\nint kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,\n\t      bool data)\n{\n\tulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM & PAGE_MASK;\n\tstruct kvmppc_pte pte;\n\tint r;\n\n\tvcpu->stat.st++;\n\n\tr = kvmppc_xlate(vcpu, *eaddr, data ? XLATE_DATA : XLATE_INST,\n\t\t\t XLATE_WRITE, &pte);\n\tif (r < 0)\n\t\treturn r;\n\n\t*eaddr = pte.raddr;\n\n\tif (!pte.may_write)\n\t\treturn -EPERM;\n\n\t/* Magic page override */\n\tif (kvmppc_supports_magic_page(vcpu) && mp_pa &&\n\t    ((pte.raddr & KVM_PAM & PAGE_MASK) == mp_pa) &&\n\t    !(kvmppc_get_msr(vcpu) & MSR_PR)) {\n\t\tvoid *magic = vcpu->arch.shared;\n\t\tmagic += pte.eaddr & 0xfff;\n\t\tmemcpy(magic, ptr, size);\n\t\treturn EMULATE_DONE;\n\t}\n\n\tif (kvm_write_guest(vcpu->kvm, pte.raddr, ptr, size))\n\t\treturn EMULATE_DO_MMIO;\n\n\treturn EMULATE_DONE;\n}\nEXPORT_SYMBOL_GPL(kvmppc_st);\n\nint kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,\n\t\t      bool data)\n{\n\tulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM & PAGE_MASK;\n\tstruct kvmppc_pte pte;\n\tint rc;\n\n\tvcpu->stat.ld++;\n\n\trc = kvmppc_xlate(vcpu, *eaddr, data ? XLATE_DATA : XLATE_INST,\n\t\t\t  XLATE_READ, &pte);\n\tif (rc)\n\t\treturn rc;\n\n\t*eaddr = pte.raddr;\n\n\tif (!pte.may_read)\n\t\treturn -EPERM;\n\n\tif (!data && !pte.may_execute)\n\t\treturn -ENOEXEC;\n\n\t/* Magic page override */\n\tif (kvmppc_supports_magic_page(vcpu) && mp_pa &&\n\t    ((pte.raddr & KVM_PAM & PAGE_MASK) == mp_pa) &&\n\t    !(kvmppc_get_msr(vcpu) & MSR_PR)) {\n\t\tvoid *magic = vcpu->arch.shared;\n\t\tmagic += pte.eaddr & 0xfff;\n\t\tmemcpy(ptr, magic, size);\n\t\treturn EMULATE_DONE;\n\t}\n\n\tif (kvm_read_guest(vcpu->kvm, pte.raddr, ptr, size))\n\t\treturn EMULATE_DO_MMIO;\n\n\treturn EMULATE_DONE;\n}\nEXPORT_SYMBOL_GPL(kvmppc_ld);\n\nint kvm_arch_hardware_enable(void)\n{\n\treturn 0;\n}\n\nint kvm_arch_hardware_setup(void)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_check_processor_compat(void *rtn)\n{\n\t*(int *)rtn = kvmppc_core_check_processor_compat();\n}\n\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\n{\n\tstruct kvmppc_ops *kvm_ops = NULL;\n\t/*\n\t * if we have both HV and PR enabled, default is HV\n\t */\n\tif (type == 0) {\n\t\tif (kvmppc_hv_ops)\n\t\t\tkvm_ops = kvmppc_hv_ops;\n\t\telse\n\t\t\tkvm_ops = kvmppc_pr_ops;\n\t\tif (!kvm_ops)\n\t\t\tgoto err_out;\n\t} else\tif (type == KVM_VM_PPC_HV) {\n\t\tif (!kvmppc_hv_ops)\n\t\t\tgoto err_out;\n\t\tkvm_ops = kvmppc_hv_ops;\n\t} else if (type == KVM_VM_PPC_PR) {\n\t\tif (!kvmppc_pr_ops)\n\t\t\tgoto err_out;\n\t\tkvm_ops = kvmppc_pr_ops;\n\t} else\n\t\tgoto err_out;\n\n\tif (kvm_ops->owner && !try_module_get(kvm_ops->owner))\n\t\treturn -ENOENT;\n\n\tkvm->arch.kvm_ops = kvm_ops;\n\treturn kvmppc_core_init_vm(kvm);\nerr_out:\n\treturn -EINVAL;\n}\n\nbool kvm_arch_has_vcpu_debugfs(void)\n{\n\treturn false;\n}\n\nint kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\n{\n\tunsigned int i;\n\tstruct kvm_vcpu *vcpu;\n\n#ifdef CONFIG_KVM_XICS\n\t/*\n\t * We call kick_all_cpus_sync() to ensure that all\n\t * CPUs have executed any pending IPIs before we\n\t * continue and free VCPUs structures below.\n\t */\n\tif (is_kvmppc_hv_enabled(kvm))\n\t\tkick_all_cpus_sync();\n#endif\n\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_arch_vcpu_free(vcpu);\n\n\tmutex_lock(&kvm->lock);\n\tfor (i = 0; i < atomic_read(&kvm->online_vcpus); i++)\n\t\tkvm->vcpus[i] = NULL;\n\n\tatomic_set(&kvm->online_vcpus, 0);\n\n\tkvmppc_core_destroy_vm(kvm);\n\n\tmutex_unlock(&kvm->lock);\n\n\t/* drop the module reference */\n\tmodule_put(kvm->arch.kvm_ops->owner);\n}\n\nint kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)\n{\n\tint r;\n\t/* Assume we're using HV mode when the HV module is loaded */\n\tint hv_enabled = kvmppc_hv_ops ? 1 : 0;\n\n\tif (kvm) {\n\t\t/*\n\t\t * Hooray - we know which VM type we're running on. Depend on\n\t\t * that rather than the guess above.\n\t\t */\n\t\thv_enabled = is_kvmppc_hv_enabled(kvm);\n\t}\n\n\tswitch (ext) {\n#ifdef CONFIG_BOOKE\n\tcase KVM_CAP_PPC_BOOKE_SREGS:\n\tcase KVM_CAP_PPC_BOOKE_WATCHDOG:\n\tcase KVM_CAP_PPC_EPR:\n#else\n\tcase KVM_CAP_PPC_SEGSTATE:\n\tcase KVM_CAP_PPC_HIOR:\n\tcase KVM_CAP_PPC_PAPR:\n#endif\n\tcase KVM_CAP_PPC_UNSET_IRQ:\n\tcase KVM_CAP_PPC_IRQ_LEVEL:\n\tcase KVM_CAP_ENABLE_CAP:\n\tcase KVM_CAP_ENABLE_CAP_VM:\n\tcase KVM_CAP_ONE_REG:\n\tcase KVM_CAP_IOEVENTFD:\n\tcase KVM_CAP_DEVICE_CTRL:\n\tcase KVM_CAP_IMMEDIATE_EXIT:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_PPC_PAIRED_SINGLES:\n\tcase KVM_CAP_PPC_OSI:\n\tcase KVM_CAP_PPC_GET_PVINFO:\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\n\tcase KVM_CAP_SW_TLB:\n#endif\n\t\t/* We support this only for PR */\n\t\tr = !hv_enabled;\n\t\tbreak;\n#ifdef CONFIG_KVM_MPIC\n\tcase KVM_CAP_IRQ_MPIC:\n\t\tr = 1;\n\t\tbreak;\n#endif\n\n#ifdef CONFIG_PPC_BOOK3S_64\n\tcase KVM_CAP_SPAPR_TCE:\n\tcase KVM_CAP_SPAPR_TCE_64:\n\t\t/* fallthrough */\n\tcase KVM_CAP_SPAPR_TCE_VFIO:\n\tcase KVM_CAP_PPC_RTAS:\n\tcase KVM_CAP_PPC_FIXUP_HCALL:\n\tcase KVM_CAP_PPC_ENABLE_HCALL:\n#ifdef CONFIG_KVM_XICS\n\tcase KVM_CAP_IRQ_XICS:\n#endif\n\t\tr = 1;\n\t\tbreak;\n\n\tcase KVM_CAP_PPC_ALLOC_HTAB:\n\t\tr = hv_enabled;\n\t\tbreak;\n#endif /* CONFIG_PPC_BOOK3S_64 */\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tcase KVM_CAP_PPC_SMT:\n\t\tr = 0;\n\t\tif (kvm) {\n\t\t\tif (kvm->arch.emul_smt_mode > 1)\n\t\t\t\tr = kvm->arch.emul_smt_mode;\n\t\t\telse\n\t\t\t\tr = kvm->arch.smt_mode;\n\t\t} else if (hv_enabled) {\n\t\t\tif (cpu_has_feature(CPU_FTR_ARCH_300))\n\t\t\t\tr = 1;\n\t\t\telse\n\t\t\t\tr = threads_per_subcore;\n\t\t}\n\t\tbreak;\n\tcase KVM_CAP_PPC_SMT_POSSIBLE:\n\t\tr = 1;\n\t\tif (hv_enabled) {\n\t\t\tif (!cpu_has_feature(CPU_FTR_ARCH_300))\n\t\t\t\tr = ((threads_per_subcore << 1) - 1);\n\t\t\telse\n\t\t\t\t/* P9 can emulate dbells, so allow any mode */\n\t\t\t\tr = 8 | 4 | 2 | 1;\n\t\t}\n\t\tbreak;\n\tcase KVM_CAP_PPC_RMA:\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_PPC_HWRNG:\n\t\tr = kvmppc_hwrng_present();\n\t\tbreak;\n\tcase KVM_CAP_PPC_MMU_RADIX:\n\t\tr = !!(hv_enabled && radix_enabled());\n\t\tbreak;\n\tcase KVM_CAP_PPC_MMU_HASH_V3:\n\t\tr = !!(hv_enabled && !radix_enabled() &&\n\t\t       cpu_has_feature(CPU_FTR_ARCH_300));\n\t\tbreak;\n#endif\n\tcase KVM_CAP_SYNC_MMU:\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\t\tr = hv_enabled;\n#elif defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\t\tr = 1;\n#else\n\t\tr = 0;\n#endif\n\t\tbreak;\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tcase KVM_CAP_PPC_HTAB_FD:\n\t\tr = hv_enabled;\n\t\tbreak;\n#endif\n\tcase KVM_CAP_NR_VCPUS:\n\t\t/*\n\t\t * Recommending a number of CPUs is somewhat arbitrary; we\n\t\t * return the number of present CPUs for -HV (since a host\n\t\t * will have secondary threads \"offline\"), and for other KVM\n\t\t * implementations just count online CPUs.\n\t\t */\n\t\tif (hv_enabled)\n\t\t\tr = num_present_cpus();\n\t\telse\n\t\t\tr = num_online_cpus();\n\t\tbreak;\n\tcase KVM_CAP_NR_MEMSLOTS:\n\t\tr = KVM_USER_MEM_SLOTS;\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPUS:\n\t\tr = KVM_MAX_VCPUS;\n\t\tbreak;\n#ifdef CONFIG_PPC_BOOK3S_64\n\tcase KVM_CAP_PPC_GET_SMMU_INFO:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_SPAPR_MULTITCE:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_SPAPR_RESIZE_HPT:\n\t\t/* Disable this on POWER9 until code handles new HPTE format */\n\t\tr = !!hv_enabled && !cpu_has_feature(CPU_FTR_ARCH_300);\n\t\tbreak;\n#endif\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tcase KVM_CAP_PPC_FWNMI:\n\t\tr = hv_enabled;\n\t\tbreak;\n#endif\n\tcase KVM_CAP_PPC_HTM:\n\t\tr = cpu_has_feature(CPU_FTR_TM_COMP) && hv_enabled;\n\t\tbreak;\n\tdefault:\n\t\tr = 0;\n\t\tbreak;\n\t}\n\treturn r;\n\n}\n\nlong kvm_arch_dev_ioctl(struct file *filp,\n                        unsigned int ioctl, unsigned long arg)\n{\n\treturn -EINVAL;\n}\n\nvoid kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,\n\t\t\t   struct kvm_memory_slot *dont)\n{\n\tkvmppc_core_free_memslot(kvm, free, dont);\n}\n\nint kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,\n\t\t\t    unsigned long npages)\n{\n\treturn kvmppc_core_create_memslot(kvm, slot, npages);\n}\n\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *memslot,\n\t\t\t\t   const struct kvm_userspace_memory_region *mem,\n\t\t\t\t   enum kvm_mr_change change)\n{\n\treturn kvmppc_core_prepare_memory_region(kvm, memslot, mem);\n}\n\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\n\t\t\t\t   const struct kvm_userspace_memory_region *mem,\n\t\t\t\t   const struct kvm_memory_slot *old,\n\t\t\t\t   const struct kvm_memory_slot *new,\n\t\t\t\t   enum kvm_mr_change change)\n{\n\tkvmppc_core_commit_memory_region(kvm, mem, old, new);\n}\n\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot)\n{\n\tkvmppc_core_flush_memslot(kvm, slot);\n}\n\nstruct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)\n{\n\tstruct kvm_vcpu *vcpu;\n\tvcpu = kvmppc_core_vcpu_create(kvm, id);\n\tif (!IS_ERR(vcpu)) {\n\t\tvcpu->arch.wqp = &vcpu->wq;\n\t\tkvmppc_create_vcpu_debugfs(vcpu, id);\n\t}\n\treturn vcpu;\n}\n\nvoid kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\n{\n}\n\nvoid kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)\n{\n\t/* Make sure we're not using the vcpu anymore */\n\thrtimer_cancel(&vcpu->arch.dec_timer);\n\n\tkvmppc_remove_vcpu_debugfs(vcpu);\n\n\tswitch (vcpu->arch.irq_type) {\n\tcase KVMPPC_IRQ_MPIC:\n\t\tkvmppc_mpic_disconnect_vcpu(vcpu->arch.mpic, vcpu);\n\t\tbreak;\n\tcase KVMPPC_IRQ_XICS:\n\t\tif (xive_enabled())\n\t\t\tkvmppc_xive_cleanup_vcpu(vcpu);\n\t\telse\n\t\t\tkvmppc_xics_free_icp(vcpu);\n\t\tbreak;\n\t}\n\n\tkvmppc_core_vcpu_free(vcpu);\n}\n\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tkvm_arch_vcpu_free(vcpu);\n}\n\nint kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)\n{\n\treturn kvmppc_core_pending_dec(vcpu);\n}\n\nstatic enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)\n{\n\tstruct kvm_vcpu *vcpu;\n\n\tvcpu = container_of(timer, struct kvm_vcpu, arch.dec_timer);\n\tkvmppc_decrementer_func(vcpu);\n\n\treturn HRTIMER_NORESTART;\n}\n\nint kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\thrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);\n\tvcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;\n\tvcpu->arch.dec_expires = ~(u64)0;\n\n#ifdef CONFIG_KVM_EXIT_TIMING\n\tmutex_init(&vcpu->arch.exit_timing_lock);\n#endif\n\tret = kvmppc_subarch_vcpu_init(vcpu);\n\treturn ret;\n}\n\nvoid kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)\n{\n\tkvmppc_mmu_destroy(vcpu);\n\tkvmppc_subarch_vcpu_uninit(vcpu);\n}\n\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n#ifdef CONFIG_BOOKE\n\t/*\n\t * vrsave (formerly usprg0) isn't used by Linux, but may\n\t * be used by the guest.\n\t *\n\t * On non-booke this is associated with Altivec and\n\t * is handled by code in book3s.c.\n\t */\n\tmtspr(SPRN_VRSAVE, vcpu->arch.vrsave);\n#endif\n\tkvmppc_core_vcpu_load(vcpu, cpu);\n}\n\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tkvmppc_core_vcpu_put(vcpu);\n#ifdef CONFIG_BOOKE\n\tvcpu->arch.vrsave = mfspr(SPRN_VRSAVE);\n#endif\n}\n\n/*\n * irq_bypass_add_producer and irq_bypass_del_producer are only\n * useful if the architecture supports PCI passthrough.\n * irq_bypass_stop and irq_bypass_start are not needed and so\n * kvm_ops are not defined for them.\n */\nbool kvm_arch_has_irq_bypass(void)\n{\n\treturn ((kvmppc_hv_ops && kvmppc_hv_ops->irq_bypass_add_producer) ||\n\t\t(kvmppc_pr_ops && kvmppc_pr_ops->irq_bypass_add_producer));\n}\n\nint kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t     struct irq_bypass_producer *prod)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\tstruct kvm *kvm = irqfd->kvm;\n\n\tif (kvm->arch.kvm_ops->irq_bypass_add_producer)\n\t\treturn kvm->arch.kvm_ops->irq_bypass_add_producer(cons, prod);\n\n\treturn 0;\n}\n\nvoid kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t      struct irq_bypass_producer *prod)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\tstruct kvm *kvm = irqfd->kvm;\n\n\tif (kvm->arch.kvm_ops->irq_bypass_del_producer)\n\t\tkvm->arch.kvm_ops->irq_bypass_del_producer(cons, prod);\n}\n\n#ifdef CONFIG_VSX\nstatic inline int kvmppc_get_vsr_dword_offset(int index)\n{\n\tint offset;\n\n\tif ((index != 0) && (index != 1))\n\t\treturn -1;\n\n#ifdef __BIG_ENDIAN\n\toffset =  index;\n#else\n\toffset = 1 - index;\n#endif\n\n\treturn offset;\n}\n\nstatic inline int kvmppc_get_vsr_word_offset(int index)\n{\n\tint offset;\n\n\tif ((index > 3) || (index < 0))\n\t\treturn -1;\n\n#ifdef __BIG_ENDIAN\n\toffset = index;\n#else\n\toffset = 3 - index;\n#endif\n\treturn offset;\n}\n\nstatic inline void kvmppc_set_vsr_dword(struct kvm_vcpu *vcpu,\n\tu64 gpr)\n{\n\tunion kvmppc_one_reg val;\n\tint offset = kvmppc_get_vsr_dword_offset(vcpu->arch.mmio_vsx_offset);\n\tint index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;\n\n\tif (offset == -1)\n\t\treturn;\n\n\tif (vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\tval.vval = VCPU_VSX_VR(vcpu, index);\n\t\tval.vsxval[offset] = gpr;\n\t\tVCPU_VSX_VR(vcpu, index) = val.vval;\n\t} else {\n\t\tVCPU_VSX_FPR(vcpu, index, offset) = gpr;\n\t}\n}\n\nstatic inline void kvmppc_set_vsr_dword_dump(struct kvm_vcpu *vcpu,\n\tu64 gpr)\n{\n\tunion kvmppc_one_reg val;\n\tint index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;\n\n\tif (vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\tval.vval = VCPU_VSX_VR(vcpu, index);\n\t\tval.vsxval[0] = gpr;\n\t\tval.vsxval[1] = gpr;\n\t\tVCPU_VSX_VR(vcpu, index) = val.vval;\n\t} else {\n\t\tVCPU_VSX_FPR(vcpu, index, 0) = gpr;\n\t\tVCPU_VSX_FPR(vcpu, index, 1) = gpr;\n\t}\n}\n\nstatic inline void kvmppc_set_vsr_word(struct kvm_vcpu *vcpu,\n\tu32 gpr32)\n{\n\tunion kvmppc_one_reg val;\n\tint offset = kvmppc_get_vsr_word_offset(vcpu->arch.mmio_vsx_offset);\n\tint index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;\n\tint dword_offset, word_offset;\n\n\tif (offset == -1)\n\t\treturn;\n\n\tif (vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\tval.vval = VCPU_VSX_VR(vcpu, index);\n\t\tval.vsx32val[offset] = gpr32;\n\t\tVCPU_VSX_VR(vcpu, index) = val.vval;\n\t} else {\n\t\tdword_offset = offset / 2;\n\t\tword_offset = offset % 2;\n\t\tval.vsxval[0] = VCPU_VSX_FPR(vcpu, index, dword_offset);\n\t\tval.vsx32val[word_offset] = gpr32;\n\t\tVCPU_VSX_FPR(vcpu, index, dword_offset) = val.vsxval[0];\n\t}\n}\n#endif /* CONFIG_VSX */\n\n#ifdef CONFIG_PPC_FPU\nstatic inline u64 sp_to_dp(u32 fprs)\n{\n\tu64 fprd;\n\n\tpreempt_disable();\n\tenable_kernel_fp();\n\tasm (\"lfs%U1%X1 0,%1; stfd%U0%X0 0,%0\" : \"=m\" (fprd) : \"m\" (fprs)\n\t     : \"fr0\");\n\tpreempt_enable();\n\treturn fprd;\n}\n\nstatic inline u32 dp_to_sp(u64 fprd)\n{\n\tu32 fprs;\n\n\tpreempt_disable();\n\tenable_kernel_fp();\n\tasm (\"lfd%U1%X1 0,%1; stfs%U0%X0 0,%0\" : \"=m\" (fprs) : \"m\" (fprd)\n\t     : \"fr0\");\n\tpreempt_enable();\n\treturn fprs;\n}\n\n#else\n#define sp_to_dp(x)\t(x)\n#define dp_to_sp(x)\t(x)\n#endif /* CONFIG_PPC_FPU */\n\nstatic void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,\n                                      struct kvm_run *run)\n{\n\tu64 uninitialized_var(gpr);\n\n\tif (run->mmio.len > sizeof(gpr)) {\n\t\tprintk(KERN_ERR \"bad MMIO length: %d\\n\", run->mmio.len);\n\t\treturn;\n\t}\n\n\tif (!vcpu->arch.mmio_host_swabbed) {\n\t\tswitch (run->mmio.len) {\n\t\tcase 8: gpr = *(u64 *)run->mmio.data; break;\n\t\tcase 4: gpr = *(u32 *)run->mmio.data; break;\n\t\tcase 2: gpr = *(u16 *)run->mmio.data; break;\n\t\tcase 1: gpr = *(u8 *)run->mmio.data; break;\n\t\t}\n\t} else {\n\t\tswitch (run->mmio.len) {\n\t\tcase 8: gpr = swab64(*(u64 *)run->mmio.data); break;\n\t\tcase 4: gpr = swab32(*(u32 *)run->mmio.data); break;\n\t\tcase 2: gpr = swab16(*(u16 *)run->mmio.data); break;\n\t\tcase 1: gpr = *(u8 *)run->mmio.data; break;\n\t\t}\n\t}\n\n\t/* conversion between single and double precision */\n\tif ((vcpu->arch.mmio_sp64_extend) && (run->mmio.len == 4))\n\t\tgpr = sp_to_dp(gpr);\n\n\tif (vcpu->arch.mmio_sign_extend) {\n\t\tswitch (run->mmio.len) {\n#ifdef CONFIG_PPC64\n\t\tcase 4:\n\t\t\tgpr = (s64)(s32)gpr;\n\t\t\tbreak;\n#endif\n\t\tcase 2:\n\t\t\tgpr = (s64)(s16)gpr;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tgpr = (s64)(s8)gpr;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tswitch (vcpu->arch.io_gpr & KVM_MMIO_REG_EXT_MASK) {\n\tcase KVM_MMIO_REG_GPR:\n\t\tkvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);\n\t\tbreak;\n\tcase KVM_MMIO_REG_FPR:\n\t\tVCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;\n\t\tbreak;\n#ifdef CONFIG_PPC_BOOK3S\n\tcase KVM_MMIO_REG_QPR:\n\t\tvcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;\n\t\tbreak;\n\tcase KVM_MMIO_REG_FQPR:\n\t\tVCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;\n\t\tvcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;\n\t\tbreak;\n#endif\n#ifdef CONFIG_VSX\n\tcase KVM_MMIO_REG_VSX:\n\t\tif (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_DWORD)\n\t\t\tkvmppc_set_vsr_dword(vcpu, gpr);\n\t\telse if (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_WORD)\n\t\t\tkvmppc_set_vsr_word(vcpu, gpr);\n\t\telse if (vcpu->arch.mmio_vsx_copy_type ==\n\t\t\t\tKVMPPC_VSX_COPY_DWORD_LOAD_DUMP)\n\t\t\tkvmppc_set_vsr_dword_dump(vcpu, gpr);\n\t\tbreak;\n#endif\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int __kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\t\tunsigned int rt, unsigned int bytes,\n\t\t\t\tint is_default_endian, int sign_extend)\n{\n\tint idx, ret;\n\tbool host_swabbed;\n\n\t/* Pity C doesn't have a logical XOR operator */\n\tif (kvmppc_need_byteswap(vcpu)) {\n\t\thost_swabbed = is_default_endian;\n\t} else {\n\t\thost_swabbed = !is_default_endian;\n\t}\n\n\tif (bytes > sizeof(run->mmio.data)) {\n\t\tprintk(KERN_ERR \"%s: bad MMIO length: %d\\n\", __func__,\n\t\t       run->mmio.len);\n\t}\n\n\trun->mmio.phys_addr = vcpu->arch.paddr_accessed;\n\trun->mmio.len = bytes;\n\trun->mmio.is_write = 0;\n\n\tvcpu->arch.io_gpr = rt;\n\tvcpu->arch.mmio_host_swabbed = host_swabbed;\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_is_write = 0;\n\tvcpu->arch.mmio_sign_extend = sign_extend;\n\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\n\tret = kvm_io_bus_read(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,\n\t\t\t      bytes, &run->mmio.data);\n\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\n\tif (!ret) {\n\t\tkvmppc_complete_mmio_load(vcpu, run);\n\t\tvcpu->mmio_needed = 0;\n\t\treturn EMULATE_DONE;\n\t}\n\n\treturn EMULATE_DO_MMIO;\n}\n\nint kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t       unsigned int rt, unsigned int bytes,\n\t\t       int is_default_endian)\n{\n\treturn __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 0);\n}\nEXPORT_SYMBOL_GPL(kvmppc_handle_load);\n\n/* Same as above, but sign extends */\nint kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\tunsigned int rt, unsigned int bytes,\n\t\t\tint is_default_endian)\n{\n\treturn __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 1);\n}\n\n#ifdef CONFIG_VSX\nint kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\tunsigned int rt, unsigned int bytes,\n\t\t\tint is_default_endian, int mmio_sign_extend)\n{\n\tenum emulation_result emulated = EMULATE_DONE;\n\n\t/* Currently, mmio_vsx_copy_nums only allowed to be less than 4 */\n\tif ( (vcpu->arch.mmio_vsx_copy_nums > 4) ||\n\t\t(vcpu->arch.mmio_vsx_copy_nums < 0) ) {\n\t\treturn EMULATE_FAIL;\n\t}\n\n\twhile (vcpu->arch.mmio_vsx_copy_nums) {\n\t\temulated = __kvmppc_handle_load(run, vcpu, rt, bytes,\n\t\t\tis_default_endian, mmio_sign_extend);\n\n\t\tif (emulated != EMULATE_DONE)\n\t\t\tbreak;\n\n\t\tvcpu->arch.paddr_accessed += run->mmio.len;\n\n\t\tvcpu->arch.mmio_vsx_copy_nums--;\n\t\tvcpu->arch.mmio_vsx_offset++;\n\t}\n\treturn emulated;\n}\n#endif /* CONFIG_VSX */\n\nint kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\tu64 val, unsigned int bytes, int is_default_endian)\n{\n\tvoid *data = run->mmio.data;\n\tint idx, ret;\n\tbool host_swabbed;\n\n\t/* Pity C doesn't have a logical XOR operator */\n\tif (kvmppc_need_byteswap(vcpu)) {\n\t\thost_swabbed = is_default_endian;\n\t} else {\n\t\thost_swabbed = !is_default_endian;\n\t}\n\n\tif (bytes > sizeof(run->mmio.data)) {\n\t\tprintk(KERN_ERR \"%s: bad MMIO length: %d\\n\", __func__,\n\t\t       run->mmio.len);\n\t}\n\n\trun->mmio.phys_addr = vcpu->arch.paddr_accessed;\n\trun->mmio.len = bytes;\n\trun->mmio.is_write = 1;\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_is_write = 1;\n\n\tif ((vcpu->arch.mmio_sp64_extend) && (bytes == 4))\n\t\tval = dp_to_sp(val);\n\n\t/* Store the value at the lowest bytes in 'data'. */\n\tif (!host_swabbed) {\n\t\tswitch (bytes) {\n\t\tcase 8: *(u64 *)data = val; break;\n\t\tcase 4: *(u32 *)data = val; break;\n\t\tcase 2: *(u16 *)data = val; break;\n\t\tcase 1: *(u8  *)data = val; break;\n\t\t}\n\t} else {\n\t\tswitch (bytes) {\n\t\tcase 8: *(u64 *)data = swab64(val); break;\n\t\tcase 4: *(u32 *)data = swab32(val); break;\n\t\tcase 2: *(u16 *)data = swab16(val); break;\n\t\tcase 1: *(u8  *)data = val; break;\n\t\t}\n\t}\n\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\n\tret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,\n\t\t\t       bytes, &run->mmio.data);\n\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\n\tif (!ret) {\n\t\tvcpu->mmio_needed = 0;\n\t\treturn EMULATE_DONE;\n\t}\n\n\treturn EMULATE_DO_MMIO;\n}\nEXPORT_SYMBOL_GPL(kvmppc_handle_store);\n\n#ifdef CONFIG_VSX\nstatic inline int kvmppc_get_vsr_data(struct kvm_vcpu *vcpu, int rs, u64 *val)\n{\n\tu32 dword_offset, word_offset;\n\tunion kvmppc_one_reg reg;\n\tint vsx_offset = 0;\n\tint copy_type = vcpu->arch.mmio_vsx_copy_type;\n\tint result = 0;\n\n\tswitch (copy_type) {\n\tcase KVMPPC_VSX_COPY_DWORD:\n\t\tvsx_offset =\n\t\t\tkvmppc_get_vsr_dword_offset(vcpu->arch.mmio_vsx_offset);\n\n\t\tif (vsx_offset == -1) {\n\t\t\tresult = -1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\t\t*val = VCPU_VSX_FPR(vcpu, rs, vsx_offset);\n\t\t} else {\n\t\t\treg.vval = VCPU_VSX_VR(vcpu, rs);\n\t\t\t*val = reg.vsxval[vsx_offset];\n\t\t}\n\t\tbreak;\n\n\tcase KVMPPC_VSX_COPY_WORD:\n\t\tvsx_offset =\n\t\t\tkvmppc_get_vsr_word_offset(vcpu->arch.mmio_vsx_offset);\n\n\t\tif (vsx_offset == -1) {\n\t\t\tresult = -1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!vcpu->arch.mmio_vsx_tx_sx_enabled) {\n\t\t\tdword_offset = vsx_offset / 2;\n\t\t\tword_offset = vsx_offset % 2;\n\t\t\treg.vsxval[0] = VCPU_VSX_FPR(vcpu, rs, dword_offset);\n\t\t\t*val = reg.vsx32val[word_offset];\n\t\t} else {\n\t\t\treg.vval = VCPU_VSX_VR(vcpu, rs);\n\t\t\t*val = reg.vsx32val[vsx_offset];\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tresult = -1;\n\t\tbreak;\n\t}\n\n\treturn result;\n}\n\nint kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,\n\t\t\tint rs, unsigned int bytes, int is_default_endian)\n{\n\tu64 val;\n\tenum emulation_result emulated = EMULATE_DONE;\n\n\tvcpu->arch.io_gpr = rs;\n\n\t/* Currently, mmio_vsx_copy_nums only allowed to be less than 4 */\n\tif ( (vcpu->arch.mmio_vsx_copy_nums > 4) ||\n\t\t(vcpu->arch.mmio_vsx_copy_nums < 0) ) {\n\t\treturn EMULATE_FAIL;\n\t}\n\n\twhile (vcpu->arch.mmio_vsx_copy_nums) {\n\t\tif (kvmppc_get_vsr_data(vcpu, rs, &val) == -1)\n\t\t\treturn EMULATE_FAIL;\n\n\t\temulated = kvmppc_handle_store(run, vcpu,\n\t\t\t val, bytes, is_default_endian);\n\n\t\tif (emulated != EMULATE_DONE)\n\t\t\tbreak;\n\n\t\tvcpu->arch.paddr_accessed += run->mmio.len;\n\n\t\tvcpu->arch.mmio_vsx_copy_nums--;\n\t\tvcpu->arch.mmio_vsx_offset++;\n\t}\n\n\treturn emulated;\n}\n\nstatic int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu,\n\t\t\tstruct kvm_run *run)\n{\n\tenum emulation_result emulated = EMULATE_FAIL;\n\tint r;\n\n\tvcpu->arch.paddr_accessed += run->mmio.len;\n\n\tif (!vcpu->mmio_is_write) {\n\t\temulated = kvmppc_handle_vsx_load(run, vcpu, vcpu->arch.io_gpr,\n\t\t\t run->mmio.len, 1, vcpu->arch.mmio_sign_extend);\n\t} else {\n\t\temulated = kvmppc_handle_vsx_store(run, vcpu,\n\t\t\t vcpu->arch.io_gpr, run->mmio.len, 1);\n\t}\n\n\tswitch (emulated) {\n\tcase EMULATE_DO_MMIO:\n\t\trun->exit_reason = KVM_EXIT_MMIO;\n\t\tr = RESUME_HOST;\n\t\tbreak;\n\tcase EMULATE_FAIL:\n\t\tpr_info(\"KVM: MMIO emulation failed (VSX repeat)\\n\");\n\t\trun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\trun->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tr = RESUME_HOST;\n\t\tbreak;\n\tdefault:\n\t\tr = RESUME_GUEST;\n\t\tbreak;\n\t}\n\treturn r;\n}\n#endif /* CONFIG_VSX */\n\nint kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)\n{\n\tint r = 0;\n\tunion kvmppc_one_reg val;\n\tint size;\n\n\tsize = one_reg_size(reg->id);\n\tif (size > sizeof(val))\n\t\treturn -EINVAL;\n\n\tr = kvmppc_get_one_reg(vcpu, reg->id, &val);\n\tif (r == -EINVAL) {\n\t\tr = 0;\n\t\tswitch (reg->id) {\n#ifdef CONFIG_ALTIVEC\n\t\tcase KVM_REG_PPC_VR0 ... KVM_REG_PPC_VR31:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tval.vval = vcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0];\n\t\t\tbreak;\n\t\tcase KVM_REG_PPC_VSCR:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tval = get_reg_val(reg->id, vcpu->arch.vr.vscr.u[3]);\n\t\t\tbreak;\n\t\tcase KVM_REG_PPC_VRSAVE:\n\t\t\tval = get_reg_val(reg->id, vcpu->arch.vrsave);\n\t\t\tbreak;\n#endif /* CONFIG_ALTIVEC */\n\t\tdefault:\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (r)\n\t\treturn r;\n\n\tif (copy_to_user((char __user *)(unsigned long)reg->addr, &val, size))\n\t\tr = -EFAULT;\n\n\treturn r;\n}\n\nint kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)\n{\n\tint r;\n\tunion kvmppc_one_reg val;\n\tint size;\n\n\tsize = one_reg_size(reg->id);\n\tif (size > sizeof(val))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&val, (char __user *)(unsigned long)reg->addr, size))\n\t\treturn -EFAULT;\n\n\tr = kvmppc_set_one_reg(vcpu, reg->id, &val);\n\tif (r == -EINVAL) {\n\t\tr = 0;\n\t\tswitch (reg->id) {\n#ifdef CONFIG_ALTIVEC\n\t\tcase KVM_REG_PPC_VR0 ... KVM_REG_PPC_VR31:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0] = val.vval;\n\t\t\tbreak;\n\t\tcase KVM_REG_PPC_VSCR:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvcpu->arch.vr.vscr.u[3] = set_reg_val(reg->id, val);\n\t\t\tbreak;\n\t\tcase KVM_REG_PPC_VRSAVE:\n\t\t\tif (!cpu_has_feature(CPU_FTR_ALTIVEC)) {\n\t\t\t\tr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvcpu->arch.vrsave = set_reg_val(reg->id, val);\n\t\t\tbreak;\n#endif /* CONFIG_ALTIVEC */\n\t\tdefault:\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn r;\n}\n\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tint r;\n\tsigset_t sigsaved;\n\n\tif (vcpu->mmio_needed) {\n\t\tvcpu->mmio_needed = 0;\n\t\tif (!vcpu->mmio_is_write)\n\t\t\tkvmppc_complete_mmio_load(vcpu, run);\n#ifdef CONFIG_VSX\n\t\tif (vcpu->arch.mmio_vsx_copy_nums > 0) {\n\t\t\tvcpu->arch.mmio_vsx_copy_nums--;\n\t\t\tvcpu->arch.mmio_vsx_offset++;\n\t\t}\n\n\t\tif (vcpu->arch.mmio_vsx_copy_nums > 0) {\n\t\t\tr = kvmppc_emulate_mmio_vsx_loadstore(vcpu, run);\n\t\t\tif (r == RESUME_HOST) {\n\t\t\t\tvcpu->mmio_needed = 1;\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n#endif\n\t} else if (vcpu->arch.osi_needed) {\n\t\tu64 *gprs = run->osi.gprs;\n\t\tint i;\n\n\t\tfor (i = 0; i < 32; i++)\n\t\t\tkvmppc_set_gpr(vcpu, i, gprs[i]);\n\t\tvcpu->arch.osi_needed = 0;\n\t} else if (vcpu->arch.hcall_needed) {\n\t\tint i;\n\n\t\tkvmppc_set_gpr(vcpu, 3, run->papr_hcall.ret);\n\t\tfor (i = 0; i < 9; ++i)\n\t\t\tkvmppc_set_gpr(vcpu, 4 + i, run->papr_hcall.args[i]);\n\t\tvcpu->arch.hcall_needed = 0;\n#ifdef CONFIG_BOOKE\n\t} else if (vcpu->arch.epr_needed) {\n\t\tkvmppc_set_epr(vcpu, run->epr.epr);\n\t\tvcpu->arch.epr_needed = 0;\n#endif\n\t}\n\n\tif (vcpu->sigset_active)\n\t\tsigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);\n\n\tif (run->immediate_exit)\n\t\tr = -EINTR;\n\telse\n\t\tr = kvmppc_vcpu_run(run, vcpu);\n\n\tif (vcpu->sigset_active)\n\t\tsigprocmask(SIG_SETMASK, &sigsaved, NULL);\n\n\treturn r;\n}\n\nint kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)\n{\n\tif (irq->irq == KVM_INTERRUPT_UNSET) {\n\t\tkvmppc_core_dequeue_external(vcpu);\n\t\treturn 0;\n\t}\n\n\tkvmppc_core_queue_external(vcpu, irq);\n\n\tkvm_vcpu_kick(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_enable_cap *cap)\n{\n\tint r;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n\tcase KVM_CAP_PPC_OSI:\n\t\tr = 0;\n\t\tvcpu->arch.osi_enabled = true;\n\t\tbreak;\n\tcase KVM_CAP_PPC_PAPR:\n\t\tr = 0;\n\t\tvcpu->arch.papr_enabled = true;\n\t\tbreak;\n\tcase KVM_CAP_PPC_EPR:\n\t\tr = 0;\n\t\tif (cap->args[0])\n\t\t\tvcpu->arch.epr_flags |= KVMPPC_EPR_USER;\n\t\telse\n\t\t\tvcpu->arch.epr_flags &= ~KVMPPC_EPR_USER;\n\t\tbreak;\n#ifdef CONFIG_BOOKE\n\tcase KVM_CAP_PPC_BOOKE_WATCHDOG:\n\t\tr = 0;\n\t\tvcpu->arch.watchdog_enabled = true;\n\t\tbreak;\n#endif\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\n\tcase KVM_CAP_SW_TLB: {\n\t\tstruct kvm_config_tlb cfg;\n\t\tvoid __user *user_ptr = (void __user *)(uintptr_t)cap->args[0];\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cfg, user_ptr, sizeof(cfg)))\n\t\t\tbreak;\n\n\t\tr = kvm_vcpu_ioctl_config_tlb(vcpu, &cfg);\n\t\tbreak;\n\t}\n#endif\n#ifdef CONFIG_KVM_MPIC\n\tcase KVM_CAP_IRQ_MPIC: {\n\t\tstruct fd f;\n\t\tstruct kvm_device *dev;\n\n\t\tr = -EBADF;\n\t\tf = fdget(cap->args[0]);\n\t\tif (!f.file)\n\t\t\tbreak;\n\n\t\tr = -EPERM;\n\t\tdev = kvm_device_from_filp(f.file);\n\t\tif (dev)\n\t\t\tr = kvmppc_mpic_connect_vcpu(dev, vcpu, cap->args[1]);\n\n\t\tfdput(f);\n\t\tbreak;\n\t}\n#endif\n#ifdef CONFIG_KVM_XICS\n\tcase KVM_CAP_IRQ_XICS: {\n\t\tstruct fd f;\n\t\tstruct kvm_device *dev;\n\n\t\tr = -EBADF;\n\t\tf = fdget(cap->args[0]);\n\t\tif (!f.file)\n\t\t\tbreak;\n\n\t\tr = -EPERM;\n\t\tdev = kvm_device_from_filp(f.file);\n\t\tif (dev) {\n\t\t\tif (xive_enabled())\n\t\t\t\tr = kvmppc_xive_connect_vcpu(dev, vcpu, cap->args[1]);\n\t\t\telse\n\t\t\t\tr = kvmppc_xics_connect_vcpu(dev, vcpu, cap->args[1]);\n\t\t}\n\n\t\tfdput(f);\n\t\tbreak;\n\t}\n#endif /* CONFIG_KVM_XICS */\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tcase KVM_CAP_PPC_FWNMI:\n\t\tr = -EINVAL;\n\t\tif (!is_kvmppc_hv_enabled(vcpu->kvm))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tvcpu->kvm->arch.fwnmi_enabled = true;\n\t\tbreak;\n#endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (!r)\n\t\tr = kvmppc_sanity_check(vcpu);\n\n\treturn r;\n}\n\nbool kvm_arch_intc_initialized(struct kvm *kvm)\n{\n#ifdef CONFIG_KVM_MPIC\n\tif (kvm->arch.mpic)\n\t\treturn true;\n#endif\n#ifdef CONFIG_KVM_XICS\n\tif (kvm->arch.xics || kvm->arch.xive)\n\t\treturn true;\n#endif\n\treturn false;\n}\n\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\n                                    struct kvm_mp_state *mp_state)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\n                                    struct kvm_mp_state *mp_state)\n{\n\treturn -EINVAL;\n}\n\nlong kvm_arch_vcpu_ioctl(struct file *filp,\n                         unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tlong r;\n\n\tswitch (ioctl) {\n\tcase KVM_INTERRUPT: {\n\t\tstruct kvm_interrupt irq;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&irq, argp, sizeof(irq)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_interrupt(vcpu, &irq);\n\t\tgoto out;\n\t}\n\n\tcase KVM_ENABLE_CAP:\n\t{\n\t\tstruct kvm_enable_cap cap;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cap, argp, sizeof(cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);\n\t\tbreak;\n\t}\n\n\tcase KVM_SET_ONE_REG:\n\tcase KVM_GET_ONE_REG:\n\t{\n\t\tstruct kvm_one_reg reg;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&reg, argp, sizeof(reg)))\n\t\t\tgoto out;\n\t\tif (ioctl == KVM_SET_ONE_REG)\n\t\t\tr = kvm_vcpu_ioctl_set_one_reg(vcpu, &reg);\n\t\telse\n\t\t\tr = kvm_vcpu_ioctl_get_one_reg(vcpu, &reg);\n\t\tbreak;\n\t}\n\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\n\tcase KVM_DIRTY_TLB: {\n\t\tstruct kvm_dirty_tlb dirty;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dirty, argp, sizeof(dirty)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_dirty_tlb(vcpu, &dirty);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tr = -EINVAL;\n\t}\n\nout:\n\treturn r;\n}\n\nint kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)\n{\n\tu32 inst_nop = 0x60000000;\n#ifdef CONFIG_KVM_BOOKE_HV\n\tu32 inst_sc1 = 0x44000022;\n\tpvinfo->hcall[0] = cpu_to_be32(inst_sc1);\n\tpvinfo->hcall[1] = cpu_to_be32(inst_nop);\n\tpvinfo->hcall[2] = cpu_to_be32(inst_nop);\n\tpvinfo->hcall[3] = cpu_to_be32(inst_nop);\n#else\n\tu32 inst_lis = 0x3c000000;\n\tu32 inst_ori = 0x60000000;\n\tu32 inst_sc = 0x44000002;\n\tu32 inst_imm_mask = 0xffff;\n\n\t/*\n\t * The hypercall to get into KVM from within guest context is as\n\t * follows:\n\t *\n\t *    lis r0, r0, KVM_SC_MAGIC_R0@h\n\t *    ori r0, KVM_SC_MAGIC_R0@l\n\t *    sc\n\t *    nop\n\t */\n\tpvinfo->hcall[0] = cpu_to_be32(inst_lis | ((KVM_SC_MAGIC_R0 >> 16) & inst_imm_mask));\n\tpvinfo->hcall[1] = cpu_to_be32(inst_ori | (KVM_SC_MAGIC_R0 & inst_imm_mask));\n\tpvinfo->hcall[2] = cpu_to_be32(inst_sc);\n\tpvinfo->hcall[3] = cpu_to_be32(inst_nop);\n#endif\n\n\tpvinfo->flags = KVM_PPC_PVINFO_FLAGS_EV_IDLE;\n\n\treturn 0;\n}\n\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,\n\t\t\t  bool line_status)\n{\n\tif (!irqchip_in_kernel(kvm))\n\t\treturn -ENXIO;\n\n\tirq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,\n\t\t\t\t\tirq_event->irq, irq_event->level,\n\t\t\t\t\tline_status);\n\treturn 0;\n}\n\n\nstatic int kvm_vm_ioctl_enable_cap(struct kvm *kvm,\n\t\t\t\t   struct kvm_enable_cap *cap)\n{\n\tint r;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n#ifdef CONFIG_KVM_BOOK3S_64_HANDLER\n\tcase KVM_CAP_PPC_ENABLE_HCALL: {\n\t\tunsigned long hcall = cap->args[0];\n\n\t\tr = -EINVAL;\n\t\tif (hcall > MAX_HCALL_OPCODE || (hcall & 3) ||\n\t\t    cap->args[1] > 1)\n\t\t\tbreak;\n\t\tif (!kvmppc_book3s_hcall_implemented(kvm, hcall))\n\t\t\tbreak;\n\t\tif (cap->args[1])\n\t\t\tset_bit(hcall / 4, kvm->arch.enabled_hcalls);\n\t\telse\n\t\t\tclear_bit(hcall / 4, kvm->arch.enabled_hcalls);\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_CAP_PPC_SMT: {\n\t\tunsigned long mode = cap->args[0];\n\t\tunsigned long flags = cap->args[1];\n\n\t\tr = -EINVAL;\n\t\tif (kvm->arch.kvm_ops->set_smt_mode)\n\t\t\tr = kvm->arch.kvm_ops->set_smt_mode(kvm, mode, flags);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nlong kvm_arch_vm_ioctl(struct file *filp,\n                       unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm __maybe_unused = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tlong r;\n\n\tswitch (ioctl) {\n\tcase KVM_PPC_GET_PVINFO: {\n\t\tstruct kvm_ppc_pvinfo pvinfo;\n\t\tmemset(&pvinfo, 0, sizeof(pvinfo));\n\t\tr = kvm_vm_ioctl_get_pvinfo(&pvinfo);\n\t\tif (copy_to_user(argp, &pvinfo, sizeof(pvinfo))) {\n\t\t\tr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbreak;\n\t}\n\tcase KVM_ENABLE_CAP:\n\t{\n\t\tstruct kvm_enable_cap cap;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cap, argp, sizeof(cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_enable_cap(kvm, &cap);\n\t\tbreak;\n\t}\n#ifdef CONFIG_SPAPR_TCE_IOMMU\n\tcase KVM_CREATE_SPAPR_TCE_64: {\n\t\tstruct kvm_create_spapr_tce_64 create_tce_64;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&create_tce_64, argp, sizeof(create_tce_64)))\n\t\t\tgoto out;\n\t\tif (create_tce_64.flags) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tr = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce_64);\n\t\tgoto out;\n\t}\n\tcase KVM_CREATE_SPAPR_TCE: {\n\t\tstruct kvm_create_spapr_tce create_tce;\n\t\tstruct kvm_create_spapr_tce_64 create_tce_64;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&create_tce, argp, sizeof(create_tce)))\n\t\t\tgoto out;\n\n\t\tcreate_tce_64.liobn = create_tce.liobn;\n\t\tcreate_tce_64.page_shift = IOMMU_PAGE_SHIFT_4K;\n\t\tcreate_tce_64.offset = 0;\n\t\tcreate_tce_64.size = create_tce.window_size >>\n\t\t\t\tIOMMU_PAGE_SHIFT_4K;\n\t\tcreate_tce_64.flags = 0;\n\t\tr = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce_64);\n\t\tgoto out;\n\t}\n#endif\n#ifdef CONFIG_PPC_BOOK3S_64\n\tcase KVM_PPC_GET_SMMU_INFO: {\n\t\tstruct kvm_ppc_smmu_info info;\n\t\tstruct kvm *kvm = filp->private_data;\n\n\t\tmemset(&info, 0, sizeof(info));\n\t\tr = kvm->arch.kvm_ops->get_smmu_info(kvm, &info);\n\t\tif (r >= 0 && copy_to_user(argp, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase KVM_PPC_RTAS_DEFINE_TOKEN: {\n\t\tstruct kvm *kvm = filp->private_data;\n\n\t\tr = kvm_vm_ioctl_rtas_define_token(kvm, argp);\n\t\tbreak;\n\t}\n\tcase KVM_PPC_CONFIGURE_V3_MMU: {\n\t\tstruct kvm *kvm = filp->private_data;\n\t\tstruct kvm_ppc_mmuv3_cfg cfg;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm->arch.kvm_ops->configure_mmu)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cfg, argp, sizeof(cfg)))\n\t\t\tgoto out;\n\t\tr = kvm->arch.kvm_ops->configure_mmu(kvm, &cfg);\n\t\tbreak;\n\t}\n\tcase KVM_PPC_GET_RMMU_INFO: {\n\t\tstruct kvm *kvm = filp->private_data;\n\t\tstruct kvm_ppc_rmmu_info info;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm->arch.kvm_ops->get_rmmu_info)\n\t\t\tgoto out;\n\t\tr = kvm->arch.kvm_ops->get_rmmu_info(kvm, &info);\n\t\tif (r >= 0 && copy_to_user(argp, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault: {\n\t\tstruct kvm *kvm = filp->private_data;\n\t\tr = kvm->arch.kvm_ops->arch_vm_ioctl(filp, ioctl, arg);\n\t}\n#else /* CONFIG_PPC_BOOK3S_64 */\n\tdefault:\n\t\tr = -ENOTTY;\n#endif\n\t}\nout:\n\treturn r;\n}\n\nstatic unsigned long lpid_inuse[BITS_TO_LONGS(KVMPPC_NR_LPIDS)];\nstatic unsigned long nr_lpids;\n\nlong kvmppc_alloc_lpid(void)\n{\n\tlong lpid;\n\n\tdo {\n\t\tlpid = find_first_zero_bit(lpid_inuse, KVMPPC_NR_LPIDS);\n\t\tif (lpid >= nr_lpids) {\n\t\t\tpr_err(\"%s: No LPIDs free\\n\", __func__);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t} while (test_and_set_bit(lpid, lpid_inuse));\n\n\treturn lpid;\n}\nEXPORT_SYMBOL_GPL(kvmppc_alloc_lpid);\n\nvoid kvmppc_claim_lpid(long lpid)\n{\n\tset_bit(lpid, lpid_inuse);\n}\nEXPORT_SYMBOL_GPL(kvmppc_claim_lpid);\n\nvoid kvmppc_free_lpid(long lpid)\n{\n\tclear_bit(lpid, lpid_inuse);\n}\nEXPORT_SYMBOL_GPL(kvmppc_free_lpid);\n\nvoid kvmppc_init_lpid(unsigned long nr_lpids_param)\n{\n\tnr_lpids = min_t(unsigned long, KVMPPC_NR_LPIDS, nr_lpids_param);\n\tmemset(lpid_inuse, 0, sizeof(lpid_inuse));\n}\nEXPORT_SYMBOL_GPL(kvmppc_init_lpid);\n\nint kvm_arch_init(void *opaque)\n{\n\treturn 0;\n}\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_ppc_instr);\n"], "filenames": ["arch/powerpc/kvm/powerpc.c"], "buggy_code_start_loc": [647], "buggy_code_end_loc": [649], "fixing_code_start_loc": [647], "fixing_code_end_loc": [648], "type": "CWE-476", "message": "The kvm_vm_ioctl_check_extension function in arch/powerpc/kvm/powerpc.c in the Linux kernel before 4.13.11 allows local users to cause a denial of service (NULL pointer dereference and system crash) via a KVM_CHECK_EXTENSION KVM_CAP_PPC_HTM ioctl call to /dev/kvm.", "other": {"cve": {"id": "CVE-2017-15306", "sourceIdentifier": "cve@mitre.org", "published": "2017-11-06T18:29:00.233", "lastModified": "2017-11-28T18:48:43.903", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The kvm_vm_ioctl_check_extension function in arch/powerpc/kvm/powerpc.c in the Linux kernel before 4.13.11 allows local users to cause a denial of service (NULL pointer dereference and system crash) via a KVM_CHECK_EXTENSION KVM_CAP_PPC_HTM ioctl call to /dev/kvm."}, {"lang": "es", "value": "La funci\u00f3n kvm_vm_ioctl_check_extension en arch/powerpc/kvm/powerpc.c en el kernel de Linux, en versiones anteriores a la 4.13.11, permite que los usuarios locales provoquen una denegaci\u00f3n de servicio (desreferencia de puntero NULL y cierre inesperado del sistema) mediante una llamada ioctl KVM_CHECK_EXTENSION KVM_CAP_PPC_HTM a /dev/kvm."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.13.10", "matchCriteriaId": "161E8E77-A2DB-482D-95FB-5DB45ABDECB3"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=ac64115a66c18c01745bbd3c47a36b124e5fd8c0", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "http://openwall.com/lists/oss-security/2017/11/06/6", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.13.11", "source": "cve@mitre.org", "tags": ["Release Notes"]}, {"url": "http://www.securityfocus.com/bid/101693", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/ac64115a66c18c01745bbd3c47a36b124e5fd8c0", "source": "cve@mitre.org", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/ac64115a66c18c01745bbd3c47a36b124e5fd8c0"}}