{"buggy_code": ["/*\n *  Fast Userspace Mutexes (which I call \"Futexes!\").\n *  (C) Rusty Russell, IBM 2002\n *\n *  Generalized futexes, futex requeueing, misc fixes by Ingo Molnar\n *  (C) Copyright 2003 Red Hat Inc, All Rights Reserved\n *\n *  Removed page pinning, fix privately mapped COW pages and other cleanups\n *  (C) Copyright 2003, 2004 Jamie Lokier\n *\n *  Robust futex support started by Ingo Molnar\n *  (C) Copyright 2006 Red Hat Inc, All Rights Reserved\n *  Thanks to Thomas Gleixner for suggestions, analysis and fixes.\n *\n *  PI-futex support started by Ingo Molnar and Thomas Gleixner\n *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>\n *  Copyright (C) 2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>\n *\n *  PRIVATE futexes by Eric Dumazet\n *  Copyright (C) 2007 Eric Dumazet <dada1@cosmosbay.com>\n *\n *  Requeue-PI support by Darren Hart <dvhltc@us.ibm.com>\n *  Copyright (C) IBM Corporation, 2009\n *  Thanks to Thomas Gleixner for conceptual design and careful reviews.\n *\n *  Thanks to Ben LaHaise for yelling \"hashed waitqueues\" loudly\n *  enough at me, Linus for the original (flawed) idea, Matthew\n *  Kirkwood for proof-of-concept implementation.\n *\n *  \"The futexes are also cursed.\"\n *  \"But they come in a choice of three flavours!\"\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 2 of the License, or\n *  (at your option) any later version.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License\n *  along with this program; if not, write to the Free Software\n *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n */\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/jhash.h>\n#include <linux/init.h>\n#include <linux/futex.h>\n#include <linux/mount.h>\n#include <linux/pagemap.h>\n#include <linux/syscalls.h>\n#include <linux/signal.h>\n#include <linux/export.h>\n#include <linux/magic.h>\n#include <linux/pid.h>\n#include <linux/nsproxy.h>\n#include <linux/ptrace.h>\n#include <linux/sched/rt.h>\n#include <linux/hugetlb.h>\n#include <linux/freezer.h>\n#include <linux/bootmem.h>\n\n#include <asm/futex.h>\n\n#include \"locking/rtmutex_common.h\"\n\n/*\n * READ this before attempting to hack on futexes!\n *\n * Basic futex operation and ordering guarantees\n * =============================================\n *\n * The waiter reads the futex value in user space and calls\n * futex_wait(). This function computes the hash bucket and acquires\n * the hash bucket lock. After that it reads the futex user space value\n * again and verifies that the data has not changed. If it has not changed\n * it enqueues itself into the hash bucket, releases the hash bucket lock\n * and schedules.\n *\n * The waker side modifies the user space value of the futex and calls\n * futex_wake(). This function computes the hash bucket and acquires the\n * hash bucket lock. Then it looks for waiters on that futex in the hash\n * bucket and wakes them.\n *\n * In futex wake up scenarios where no tasks are blocked on a futex, taking\n * the hb spinlock can be avoided and simply return. In order for this\n * optimization to work, ordering guarantees must exist so that the waiter\n * being added to the list is acknowledged when the list is concurrently being\n * checked by the waker, avoiding scenarios like the following:\n *\n * CPU 0                               CPU 1\n * val = *futex;\n * sys_futex(WAIT, futex, val);\n *   futex_wait(futex, val);\n *   uval = *futex;\n *                                     *futex = newval;\n *                                     sys_futex(WAKE, futex);\n *                                       futex_wake(futex);\n *                                       if (queue_empty())\n *                                         return;\n *   if (uval == val)\n *      lock(hash_bucket(futex));\n *      queue();\n *     unlock(hash_bucket(futex));\n *     schedule();\n *\n * This would cause the waiter on CPU 0 to wait forever because it\n * missed the transition of the user space value from val to newval\n * and the waker did not find the waiter in the hash bucket queue.\n *\n * The correct serialization ensures that a waiter either observes\n * the changed user space value before blocking or is woken by a\n * concurrent waker:\n *\n * CPU 0                                 CPU 1\n * val = *futex;\n * sys_futex(WAIT, futex, val);\n *   futex_wait(futex, val);\n *\n *   waiters++; (a)\n *   mb(); (A) <-- paired with -.\n *                              |\n *   lock(hash_bucket(futex));  |\n *                              |\n *   uval = *futex;             |\n *                              |        *futex = newval;\n *                              |        sys_futex(WAKE, futex);\n *                              |          futex_wake(futex);\n *                              |\n *                              `------->  mb(); (B)\n *   if (uval == val)\n *     queue();\n *     unlock(hash_bucket(futex));\n *     schedule();                         if (waiters)\n *                                           lock(hash_bucket(futex));\n *   else                                    wake_waiters(futex);\n *     waiters--; (b)                        unlock(hash_bucket(futex));\n *\n * Where (A) orders the waiters increment and the futex value read through\n * atomic operations (see hb_waiters_inc) and where (B) orders the write\n * to futex and the waiters read -- this is done by the barriers in\n * get_futex_key_refs(), through either ihold or atomic_inc, depending on the\n * futex type.\n *\n * This yields the following case (where X:=waiters, Y:=futex):\n *\n *\tX = Y = 0\n *\n *\tw[X]=1\t\tw[Y]=1\n *\tMB\t\tMB\n *\tr[Y]=y\t\tr[X]=x\n *\n * Which guarantees that x==0 && y==0 is impossible; which translates back into\n * the guarantee that we cannot both miss the futex variable change and the\n * enqueue.\n *\n * Note that a new waiter is accounted for in (a) even when it is possible that\n * the wait call can return error, in which case we backtrack from it in (b).\n * Refer to the comment in queue_lock().\n *\n * Similarly, in order to account for waiters being requeued on another\n * address we always increment the waiters for the destination bucket before\n * acquiring the lock. It then decrements them again  after releasing it -\n * the code that actually moves the futex(es) between hash buckets (requeue_futex)\n * will do the additional required waiter count housekeeping. This is done for\n * double_lock_hb() and double_unlock_hb(), respectively.\n */\n\n#ifndef CONFIG_HAVE_FUTEX_CMPXCHG\nint __read_mostly futex_cmpxchg_enabled;\n#endif\n\n/*\n * Futex flags used to encode options to functions and preserve them across\n * restarts.\n */\n#define FLAGS_SHARED\t\t0x01\n#define FLAGS_CLOCKRT\t\t0x02\n#define FLAGS_HAS_TIMEOUT\t0x04\n\n/*\n * Priority Inheritance state:\n */\nstruct futex_pi_state {\n\t/*\n\t * list of 'owned' pi_state instances - these have to be\n\t * cleaned up in do_exit() if the task exits prematurely:\n\t */\n\tstruct list_head list;\n\n\t/*\n\t * The PI object:\n\t */\n\tstruct rt_mutex pi_mutex;\n\n\tstruct task_struct *owner;\n\tatomic_t refcount;\n\n\tunion futex_key key;\n};\n\n/**\n * struct futex_q - The hashed futex queue entry, one per waiting task\n * @list:\t\tpriority-sorted list of tasks waiting on this futex\n * @task:\t\tthe task waiting on the futex\n * @lock_ptr:\t\tthe hash bucket lock\n * @key:\t\tthe key the futex is hashed on\n * @pi_state:\t\toptional priority inheritance state\n * @rt_waiter:\t\trt_waiter storage for use with requeue_pi\n * @requeue_pi_key:\tthe requeue_pi target futex key\n * @bitset:\t\tbitset for the optional bitmasked wakeup\n *\n * We use this hashed waitqueue, instead of a normal wait_queue_t, so\n * we can wake only the relevant ones (hashed queues may be shared).\n *\n * A futex_q has a woken state, just like tasks have TASK_RUNNING.\n * It is considered woken when plist_node_empty(&q->list) || q->lock_ptr == 0.\n * The order of wakeup is always to make the first condition true, then\n * the second.\n *\n * PI futexes are typically woken before they are removed from the hash list via\n * the rt_mutex code. See unqueue_me_pi().\n */\nstruct futex_q {\n\tstruct plist_node list;\n\n\tstruct task_struct *task;\n\tspinlock_t *lock_ptr;\n\tunion futex_key key;\n\tstruct futex_pi_state *pi_state;\n\tstruct rt_mutex_waiter *rt_waiter;\n\tunion futex_key *requeue_pi_key;\n\tu32 bitset;\n};\n\nstatic const struct futex_q futex_q_init = {\n\t/* list gets initialized in queue_me()*/\n\t.key = FUTEX_KEY_INIT,\n\t.bitset = FUTEX_BITSET_MATCH_ANY\n};\n\n/*\n * Hash buckets are shared by all the futex_keys that hash to the same\n * location.  Each key may have multiple futex_q structures, one for each task\n * waiting on a futex.\n */\nstruct futex_hash_bucket {\n\tatomic_t waiters;\n\tspinlock_t lock;\n\tstruct plist_head chain;\n} ____cacheline_aligned_in_smp;\n\nstatic unsigned long __read_mostly futex_hashsize;\n\nstatic struct futex_hash_bucket *futex_queues;\n\nstatic inline void futex_get_mm(union futex_key *key)\n{\n\tatomic_inc(&key->private.mm->mm_count);\n\t/*\n\t * Ensure futex_get_mm() implies a full barrier such that\n\t * get_futex_key() implies a full barrier. This is relied upon\n\t * as full barrier (B), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic_inc();\n}\n\n/*\n * Reflects a new waiter being added to the waitqueue.\n */\nstatic inline void hb_waiters_inc(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\tatomic_inc(&hb->waiters);\n\t/*\n\t * Full barrier (A), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic_inc();\n#endif\n}\n\n/*\n * Reflects a waiter being removed from the waitqueue by wakeup\n * paths.\n */\nstatic inline void hb_waiters_dec(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\tatomic_dec(&hb->waiters);\n#endif\n}\n\nstatic inline int hb_waiters_pending(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\treturn atomic_read(&hb->waiters);\n#else\n\treturn 1;\n#endif\n}\n\n/*\n * We hash on the keys returned from get_futex_key (see below).\n */\nstatic struct futex_hash_bucket *hash_futex(union futex_key *key)\n{\n\tu32 hash = jhash2((u32*)&key->both.word,\n\t\t\t  (sizeof(key->both.word)+sizeof(key->both.ptr))/4,\n\t\t\t  key->both.offset);\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}\n\n/*\n * Return 1 if two futex_keys are equal, 0 otherwise.\n */\nstatic inline int match_futex(union futex_key *key1, union futex_key *key2)\n{\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}\n\n/*\n * Take a reference to the resource addressed by a key.\n * Can be called while holding spinlocks.\n *\n */\nstatic void get_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr)\n\t\treturn;\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tihold(key->shared.inode); /* implies MB (B) */\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tfutex_get_mm(key); /* implies MB (B) */\n\t\tbreak;\n\t}\n}\n\n/*\n * Drop a reference to the resource addressed by a key.\n * The hash bucket spinlock must not be held.\n */\nstatic void drop_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr) {\n\t\t/* If we're here then we tried to put a key we failed to get */\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tiput(key->shared.inode);\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tmmdrop(key->private.mm);\n\t\tbreak;\n\t}\n}\n\n/**\n * get_futex_key() - Get parameters which are the keys for a futex\n * @uaddr:\tvirtual address of the futex\n * @fshared:\t0 for a PROCESS_PRIVATE futex, 1 for PROCESS_SHARED\n * @key:\taddress where result is stored.\n * @rw:\t\tmapping needs to be read/write (values: VERIFY_READ,\n *              VERIFY_WRITE)\n *\n * Return: a negative error code or 0\n *\n * The key words are stored in *key on success.\n *\n * For shared mappings, it's (page->index, file_inode(vma->vm_file),\n * offset_within_page).  For private mappings, it's (uaddr, current->mm).\n * We can usually work out the index without swapping in the page.\n *\n * lock_page() might sleep, the caller should not hold a spinlock.\n */\nstatic int\nget_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *page_head;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(rw, uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\tget_futex_key_refs(key);  /* implies MB (B) */\n\t\treturn 0;\n\t}\n\nagain:\n\terr = get_user_pages_fast(address, 1, 1, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == VERIFY_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tpage_head = page;\n\tif (unlikely(PageTail(page))) {\n\t\tput_page(page);\n\t\t/* serialize against __split_huge_page_splitting() */\n\t\tlocal_irq_disable();\n\t\tif (likely(__get_user_pages_fast(address, 1, !ro, &page) == 1)) {\n\t\t\tpage_head = compound_head(page);\n\t\t\t/*\n\t\t\t * page_head is valid pointer but we must pin\n\t\t\t * it before taking the PG_lock and/or\n\t\t\t * PG_compound_lock. The moment we re-enable\n\t\t\t * irqs __split_huge_page_splitting() can\n\t\t\t * return and the head page can be freed from\n\t\t\t * under us. We can't take the PG_lock and/or\n\t\t\t * PG_compound_lock on a page that could be\n\t\t\t * freed from under us.\n\t\t\t */\n\t\t\tif (page != page_head) {\n\t\t\t\tget_page(page_head);\n\t\t\t\tput_page(page);\n\t\t\t}\n\t\t\tlocal_irq_enable();\n\t\t} else {\n\t\t\tlocal_irq_enable();\n\t\t\tgoto again;\n\t\t}\n\t}\n#else\n\tpage_head = compound_head(page);\n\tif (page != page_head) {\n\t\tget_page(page_head);\n\t\tput_page(page);\n\t}\n#endif\n\n\tlock_page(page_head);\n\n\t/*\n\t * If page_head->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page_head->mapping.\n\t */\n\tif (!page_head->mapping) {\n\t\tint shmem_swizzled = PageSwapCache(page_head);\n\t\tunlock_page(page_head);\n\t\tput_page(page_head);\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page_head)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t} else {\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.inode = page_head->mapping->host;\n\t\tkey->shared.pgoff = basepage_index(page);\n\t}\n\n\tget_futex_key_refs(key); /* implies MB (B) */\n\nout:\n\tunlock_page(page_head);\n\tput_page(page_head);\n\treturn err;\n}\n\nstatic inline void put_futex_key(union futex_key *key)\n{\n\tdrop_futex_key_refs(key);\n}\n\n/**\n * fault_in_user_writeable() - Fault in user address and verify RW access\n * @uaddr:\tpointer to faulting user space address\n *\n * Slow path to fixup the fault we just took in the atomic write\n * access to @uaddr.\n *\n * We have no generic implementation of a non-destructive write to the\n * user address. We know that we faulted in the atomic pagefault\n * disabled section so we can as well avoid the #PF overhead by\n * calling get_user_pages() right away.\n */\nstatic int fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tdown_read(&mm->mmap_sem);\n\tret = fixup_user_fault(current, mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE);\n\tup_read(&mm->mmap_sem);\n\n\treturn ret < 0 ? ret : 0;\n}\n\n/**\n * futex_top_waiter() - Return the highest priority waiter on a futex\n * @hb:\t\tthe hash bucket the futex_q's reside in\n * @key:\tthe futex key (to distinguish it from other futex futex_q's)\n *\n * Must be called with the hb lock held.\n */\nstatic struct futex_q *futex_top_waiter(struct futex_hash_bucket *hb,\n\t\t\t\t\tunion futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (match_futex(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}\n\nstatic int cmpxchg_futex_value_locked(u32 *curval, u32 __user *uaddr,\n\t\t\t\t      u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}\n\nstatic int get_futex_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __copy_from_user_inatomic(dest, from, sizeof(u32));\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}\n\n\n/*\n * PI code:\n */\nstatic int refill_pi_state_cache(void)\n{\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\tatomic_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}\n\nstatic struct futex_pi_state * alloc_pi_state(void)\n{\n\tstruct futex_pi_state *pi_state = current->pi_state_cache;\n\n\tWARN_ON(!pi_state);\n\tcurrent->pi_state_cache = NULL;\n\n\treturn pi_state;\n}\n\nstatic void free_pi_state(struct futex_pi_state *pi_state)\n{\n\tif (!atomic_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\traw_spin_lock_irq(&pi_state->owner->pi_lock);\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock_irq(&pi_state->owner->pi_lock);\n\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex, pi_state->owner);\n\t}\n\n\tif (current->pi_state_cache)\n\t\tkfree(pi_state);\n\telse {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\tatomic_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}\n\n/*\n * Look up the task based on what TID userspace gave us.\n * We dont trust it.\n */\nstatic struct task_struct * futex_find_get_task(pid_t pid)\n{\n\tstruct task_struct *p;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p)\n\t\tget_task_struct(p);\n\n\trcu_read_unlock();\n\n\treturn p;\n}\n\n/*\n * This task is holding PI mutexes at exit time => bad.\n * Kernel cleans up PI-state, but userspace is likely hosed.\n * (Robust-futex cleanup is separate and might save the day for userspace.)\n */\nvoid exit_pi_state_list(struct task_struct *curr)\n{\n\tstruct list_head *next, *head = &curr->pi_state_list;\n\tstruct futex_pi_state *pi_state;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn;\n\t/*\n\t * We are a ZOMBIE and nobody can enqueue itself on\n\t * pi_state_list anymore, but we have to be careful\n\t * versus waiters unqueueing themselves:\n\t */\n\traw_spin_lock_irq(&curr->pi_lock);\n\twhile (!list_empty(head)) {\n\n\t\tnext = head->next;\n\t\tpi_state = list_entry(next, struct futex_pi_state, list);\n\t\tkey = pi_state->key;\n\t\thb = hash_futex(&key);\n\t\traw_spin_unlock_irq(&curr->pi_lock);\n\n\t\tspin_lock(&hb->lock);\n\n\t\traw_spin_lock_irq(&curr->pi_lock);\n\t\t/*\n\t\t * We dropped the pi-lock, so re-check whether this\n\t\t * task still owns the PI-state:\n\t\t */\n\t\tif (head->next != next) {\n\t\t\tspin_unlock(&hb->lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON(pi_state->owner != curr);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\tpi_state->owner = NULL;\n\t\traw_spin_unlock_irq(&curr->pi_lock);\n\n\t\trt_mutex_unlock(&pi_state->pi_mutex);\n\n\t\tspin_unlock(&hb->lock);\n\n\t\traw_spin_lock_irq(&curr->pi_lock);\n\t}\n\traw_spin_unlock_irq(&curr->pi_lock);\n}\n\nstatic int\nlookup_pi_state(u32 uval, struct futex_hash_bucket *hb,\n\t\tunion futex_key *key, struct futex_pi_state **ps,\n\t\tstruct task_struct *task)\n{\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_q *this, *next;\n\tstruct task_struct *p;\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (match_futex(&this->key, key)) {\n\t\t\t/*\n\t\t\t * Another waiter already exists - bump up\n\t\t\t * the refcount and return its pi_state:\n\t\t\t */\n\t\t\tpi_state = this->pi_state;\n\t\t\t/*\n\t\t\t * Userspace might have messed up non-PI and PI futexes\n\t\t\t */\n\t\t\tif (unlikely(!pi_state))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tWARN_ON(!atomic_read(&pi_state->refcount));\n\n\t\t\t/*\n\t\t\t * When pi_state->owner is NULL then the owner died\n\t\t\t * and another waiter is on the fly. pi_state->owner\n\t\t\t * is fixed up by the task which acquires\n\t\t\t * pi_state->rt_mutex.\n\t\t\t *\n\t\t\t * We do not check for pid == 0 which can happen when\n\t\t\t * the owner died and robust_list_exit() cleared the\n\t\t\t * TID.\n\t\t\t */\n\t\t\tif (pid && pi_state->owner) {\n\t\t\t\t/*\n\t\t\t\t * Bail out if user space manipulated the\n\t\t\t\t * futex value.\n\t\t\t\t */\n\t\t\t\tif (pid != task_pid_vnr(pi_state->owner))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Protect against a corrupted uval. If uval\n\t\t\t * is 0x80000000 then pid is 0 and the waiter\n\t\t\t * bit is set. So the deadlock check in the\n\t\t\t * calling code has failed and we did not fall\n\t\t\t * into the check above due to !pid.\n\t\t\t */\n\t\t\tif (task && pi_state->owner == task)\n\t\t\t\treturn -EDEADLK;\n\n\t\t\tatomic_inc(&pi_state->refcount);\n\t\t\t*ps = pi_state;\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0\n\t */\n\tif (!pid)\n\t\treturn -ESRCH;\n\tp = futex_find_get_task(pid);\n\tif (!p)\n\t\treturn -ESRCH;\n\n\tif (!p->mm) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state flags to figure out,\n\t * whether the task is exiting. To protect against the do_exit\n\t * change of the task flags, we do this protected by\n\t * p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->flags & PF_EXITING)) {\n\t\t/*\n\t\t * The task is on the way out. When PF_EXITPIDONE is\n\t\t * set, we know that the task has finished the\n\t\t * cleanup:\n\t\t */\n\t\tint ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\tpi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make 'p'\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\tpi_state->owner = p;\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\t*ps = pi_state;\n\n\treturn 0;\n}\n\n/**\n * futex_lock_pi_atomic() - Atomic work required to acquire a pi aware futex\n * @uaddr:\t\tthe pi futex user address\n * @hb:\t\t\tthe pi futex hash bucket\n * @key:\t\tthe futex key associated with uaddr and hb\n * @ps:\t\t\tthe pi_state pointer where we store the result of the\n *\t\t\tlookup\n * @task:\t\tthe task to perform the atomic lock work for.  This will\n *\t\t\tbe \"current\" except in the case of requeue pi.\n * @set_waiters:\tforce setting the FUTEX_WAITERS bit (1) or not (0)\n *\n * Return:\n *  0 - ready to wait;\n *  1 - acquired the lock;\n * <0 - error\n *\n * The hb->lock and futex_key refs shall be held by the caller.\n */\nstatic int futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,\n\t\t\t\tunion futex_key *key,\n\t\t\t\tstruct futex_pi_state **ps,\n\t\t\t\tstruct task_struct *task, int set_waiters)\n{\n\tint lock_taken, ret, force_take = 0;\n\tu32 uval, newval, curval, vpid = task_pid_vnr(task);\n\nretry:\n\tret = lock_taken = 0;\n\n\t/*\n\t * To avoid races, we attempt to take the lock here again\n\t * (by doing a 0 -> TID atomic cmpxchg), while holding all\n\t * the locks. It will most likely not succeed.\n\t */\n\tnewval = vpid;\n\tif (set_waiters)\n\t\tnewval |= FUTEX_WAITERS;\n\n\tif (unlikely(cmpxchg_futex_value_locked(&curval, uaddr, 0, newval)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Detect deadlocks.\n\t */\n\tif ((unlikely((curval & FUTEX_TID_MASK) == vpid)))\n\t\treturn -EDEADLK;\n\n\t/*\n\t * Surprise - we got the lock. Just return to userspace:\n\t */\n\tif (unlikely(!curval))\n\t\treturn 1;\n\n\tuval = curval;\n\n\t/*\n\t * Set the FUTEX_WAITERS flag, so the owner will know it has someone\n\t * to wake at the next unlock.\n\t */\n\tnewval = curval | FUTEX_WAITERS;\n\n\t/*\n\t * Should we force take the futex? See below.\n\t */\n\tif (unlikely(force_take)) {\n\t\t/*\n\t\t * Keep the OWNER_DIED and the WAITERS bit and set the\n\t\t * new TID value.\n\t\t */\n\t\tnewval = (curval & ~FUTEX_TID_MASK) | vpid;\n\t\tforce_take = 0;\n\t\tlock_taken = 1;\n\t}\n\n\tif (unlikely(cmpxchg_futex_value_locked(&curval, uaddr, uval, newval)))\n\t\treturn -EFAULT;\n\tif (unlikely(curval != uval))\n\t\tgoto retry;\n\n\t/*\n\t * We took the lock due to forced take over.\n\t */\n\tif (unlikely(lock_taken))\n\t\treturn 1;\n\n\t/*\n\t * We dont have the lock. Look up the PI state (or create it if\n\t * we are the first waiter):\n\t */\n\tret = lookup_pi_state(uval, hb, key, ps, task);\n\n\tif (unlikely(ret)) {\n\t\tswitch (ret) {\n\t\tcase -ESRCH:\n\t\t\t/*\n\t\t\t * We failed to find an owner for this\n\t\t\t * futex. So we have no pi_state to block\n\t\t\t * on. This can happen in two cases:\n\t\t\t *\n\t\t\t * 1) The owner died\n\t\t\t * 2) A stale FUTEX_WAITERS bit\n\t\t\t *\n\t\t\t * Re-read the futex value.\n\t\t\t */\n\t\t\tif (get_futex_value_locked(&curval, uaddr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\t/*\n\t\t\t * If the owner died or we have a stale\n\t\t\t * WAITERS bit the owner TID in the user space\n\t\t\t * futex is 0.\n\t\t\t */\n\t\t\tif (!(curval & FUTEX_TID_MASK)) {\n\t\t\t\tforce_take = 1;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/**\n * __unqueue_futex() - Remove the futex_q from its futex_hash_bucket\n * @q:\tThe futex_q to unqueue\n *\n * The q->lock_ptr must not be NULL and must be held by the caller.\n */\nstatic void __unqueue_futex(struct futex_q *q)\n{\n\tstruct futex_hash_bucket *hb;\n\n\tif (WARN_ON_SMP(!q->lock_ptr || !spin_is_locked(q->lock_ptr))\n\t    || WARN_ON(plist_node_empty(&q->list)))\n\t\treturn;\n\n\thb = container_of(q->lock_ptr, struct futex_hash_bucket, lock);\n\tplist_del(&q->list, &hb->chain);\n\thb_waiters_dec(hb);\n}\n\n/*\n * The hash bucket lock must be held when this is called.\n * Afterwards, the futex_q must not be accessed.\n */\nstatic void wake_futex(struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\t/*\n\t * We set q->lock_ptr = NULL _before_ we wake up the task. If\n\t * a non-futex wake up happens on another CPU then the task\n\t * might exit and p would dereference a non-existing task\n\t * struct. Prevent this by holding a reference on p across the\n\t * wake up.\n\t */\n\tget_task_struct(p);\n\n\t__unqueue_futex(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as\n\t * q->lock_ptr = NULL is written, without taking any locks. A\n\t * memory barrier is required here to prevent the following\n\t * store to lock_ptr from getting ahead of the plist_del.\n\t */\n\tsmp_wmb();\n\tq->lock_ptr = NULL;\n\n\twake_up_state(p, TASK_NORMAL);\n\tput_task_struct(p);\n}\n\nstatic int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_q *this)\n{\n\tstruct task_struct *new_owner;\n\tstruct futex_pi_state *pi_state = this->pi_state;\n\tu32 uninitialized_var(curval), newval;\n\n\tif (!pi_state)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If current does not own the pi_state then the futex is\n\t * inconsistent and user space fiddled with the futex value.\n\t */\n\tif (pi_state->owner != current)\n\t\treturn -EINVAL;\n\n\traw_spin_lock(&pi_state->pi_mutex.wait_lock);\n\tnew_owner = rt_mutex_next_owner(&pi_state->pi_mutex);\n\n\t/*\n\t * It is possible that the next waiter (the one that brought\n\t * this owner to the kernel) timed out and is no longer\n\t * waiting on the lock.\n\t */\n\tif (!new_owner)\n\t\tnew_owner = this->task;\n\n\t/*\n\t * We pass it to the next owner. (The WAITERS bit is always\n\t * kept enabled while there is PI state around. We must also\n\t * preserve the owner died bit.)\n\t */\n\tif (!(uval & FUTEX_OWNER_DIED)) {\n\t\tint ret = 0;\n\n\t\tnewval = FUTEX_WAITERS | task_pid_vnr(new_owner);\n\n\t\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval))\n\t\t\tret = -EFAULT;\n\t\telse if (curval != uval)\n\t\t\tret = -EINVAL;\n\t\tif (ret) {\n\t\t\traw_spin_unlock(&pi_state->pi_mutex.wait_lock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\traw_spin_lock_irq(&pi_state->owner->pi_lock);\n\tWARN_ON(list_empty(&pi_state->list));\n\tlist_del_init(&pi_state->list);\n\traw_spin_unlock_irq(&pi_state->owner->pi_lock);\n\n\traw_spin_lock_irq(&new_owner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\tpi_state->owner = new_owner;\n\traw_spin_unlock_irq(&new_owner->pi_lock);\n\n\traw_spin_unlock(&pi_state->pi_mutex.wait_lock);\n\trt_mutex_unlock(&pi_state->pi_mutex);\n\n\treturn 0;\n}\n\nstatic int unlock_futex_pi(u32 __user *uaddr, u32 uval)\n{\n\tu32 uninitialized_var(oldval);\n\n\t/*\n\t * There is no waiter, so we unlock the futex. The owner died\n\t * bit has not to be preserved here. We are the owner:\n\t */\n\tif (cmpxchg_futex_value_locked(&oldval, uaddr, uval, 0))\n\t\treturn -EFAULT;\n\tif (oldval != uval)\n\t\treturn -EAGAIN;\n\n\treturn 0;\n}\n\n/*\n * Express the locking dependencies for lockdep:\n */\nstatic inline void\ndouble_lock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tif (hb1 <= hb2) {\n\t\tspin_lock(&hb1->lock);\n\t\tif (hb1 < hb2)\n\t\t\tspin_lock_nested(&hb2->lock, SINGLE_DEPTH_NESTING);\n\t} else { /* hb1 > hb2 */\n\t\tspin_lock(&hb2->lock);\n\t\tspin_lock_nested(&hb1->lock, SINGLE_DEPTH_NESTING);\n\t}\n}\n\nstatic inline void\ndouble_unlock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tspin_unlock(&hb1->lock);\n\tif (hb1 != hb2)\n\t\tspin_unlock(&hb2->lock);\n}\n\n/*\n * Wake up waiters matching bitset queued on this futex (uaddr).\n */\nstatic int\nfutex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\thb = hash_futex(&key);\n\n\t/* Make sure we really have tasks to wakeup */\n\tif (!hb_waiters_pending(hb))\n\t\tgoto out_put_key;\n\n\tspin_lock(&hb->lock);\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (match_futex (&this->key, &key)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check if one of the bits is set in both bitsets */\n\t\t\tif (!(this->bitset & bitset))\n\t\t\t\tcontinue;\n\n\t\t\twake_futex(this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock(&hb->lock);\nout_put_key:\n\tput_futex_key(&key);\nout:\n\treturn ret;\n}\n\n/*\n * Wake up all waiters hashed on the physical page that is mapped\n * to this virtual address:\n */\nstatic int\nfutex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,\n\t      int nr_wake, int nr_wake2, int op)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\tint ret, op_ret;\n\nretry:\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out_put_key1;\n\n\thb1 = hash_futex(&key1);\n\thb2 = hash_futex(&key2);\n\nretry_private:\n\tdouble_lock_hb(hb1, hb2);\n\top_ret = futex_atomic_op_inuser(op, uaddr2);\n\tif (unlikely(op_ret < 0)) {\n\n\t\tdouble_unlock_hb(hb1, hb2);\n\n#ifndef CONFIG_MMU\n\t\t/*\n\t\t * we don't get EFAULT from MMU faults if we don't have an MMU,\n\t\t * but we might get them from range checking\n\t\t */\n\t\tret = op_ret;\n\t\tgoto out_put_keys;\n#endif\n\n\t\tif (unlikely(op_ret != -EFAULT)) {\n\t\t\tret = op_ret;\n\t\t\tgoto out_put_keys;\n\t\t}\n\n\t\tret = fault_in_user_writeable(uaddr2);\n\t\tif (ret)\n\t\t\tgoto out_put_keys;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tput_futex_key(&key2);\n\t\tput_futex_key(&key1);\n\t\tgoto retry;\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (match_futex (&this->key, &key1)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\twake_futex(this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (op_ret > 0) {\n\t\top_ret = 0;\n\t\tplist_for_each_entry_safe(this, next, &hb2->chain, list) {\n\t\t\tif (match_futex (&this->key, &key2)) {\n\t\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t\twake_futex(this);\n\t\t\t\tif (++op_ret >= nr_wake2)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret += op_ret;\n\t}\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\nout_put_keys:\n\tput_futex_key(&key2);\nout_put_key1:\n\tput_futex_key(&key1);\nout:\n\treturn ret;\n}\n\n/**\n * requeue_futex() - Requeue a futex_q from one hb to another\n * @q:\t\tthe futex_q to requeue\n * @hb1:\tthe source hash_bucket\n * @hb2:\tthe target hash_bucket\n * @key2:\tthe new key for the requeued futex_q\n */\nstatic inline\nvoid requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,\n\t\t   struct futex_hash_bucket *hb2, union futex_key *key2)\n{\n\n\t/*\n\t * If key1 and key2 hash to the same bucket, no need to\n\t * requeue.\n\t */\n\tif (likely(&hb1->chain != &hb2->chain)) {\n\t\tplist_del(&q->list, &hb1->chain);\n\t\thb_waiters_dec(hb1);\n\t\tplist_add(&q->list, &hb2->chain);\n\t\thb_waiters_inc(hb2);\n\t\tq->lock_ptr = &hb2->lock;\n\t}\n\tget_futex_key_refs(key2);\n\tq->key = *key2;\n}\n\n/**\n * requeue_pi_wake_futex() - Wake a task that acquired the lock during requeue\n * @q:\t\tthe futex_q\n * @key:\tthe key of the requeue target futex\n * @hb:\t\tthe hash_bucket of the requeue target futex\n *\n * During futex_requeue, with requeue_pi=1, it is possible to acquire the\n * target futex if it is uncontended or via a lock steal.  Set the futex_q key\n * to the requeue target futex so the waiter can detect the wakeup on the right\n * futex, but remove it from the hb and NULL the rt_waiter so it can detect\n * atomic lock acquisition.  Set the q->lock_ptr to the requeue target hb->lock\n * to protect access to the pi_state to fixup the owner later.  Must be called\n * with both q->lock_ptr and hb->lock held.\n */\nstatic inline\nvoid requeue_pi_wake_futex(struct futex_q *q, union futex_key *key,\n\t\t\t   struct futex_hash_bucket *hb)\n{\n\tget_futex_key_refs(key);\n\tq->key = *key;\n\n\t__unqueue_futex(q);\n\n\tWARN_ON(!q->rt_waiter);\n\tq->rt_waiter = NULL;\n\n\tq->lock_ptr = &hb->lock;\n\n\twake_up_state(q->task, TASK_NORMAL);\n}\n\n/**\n * futex_proxy_trylock_atomic() - Attempt an atomic lock for the top waiter\n * @pifutex:\t\tthe user address of the to futex\n * @hb1:\t\tthe from futex hash bucket, must be locked by the caller\n * @hb2:\t\tthe to futex hash bucket, must be locked by the caller\n * @key1:\t\tthe from futex key\n * @key2:\t\tthe to futex key\n * @ps:\t\t\taddress to store the pi_state pointer\n * @set_waiters:\tforce setting the FUTEX_WAITERS bit (1) or not (0)\n *\n * Try and get the lock on behalf of the top waiter if we can do it atomically.\n * Wake the top waiter if we succeed.  If the caller specified set_waiters,\n * then direct futex_lock_pi_atomic() to force setting the FUTEX_WAITERS bit.\n * hb1 and hb2 must be held by the caller.\n *\n * Return:\n *  0 - failed to acquire the lock atomically;\n * >0 - acquired the lock, return value is vpid of the top_waiter\n * <0 - error\n */\nstatic int futex_proxy_trylock_atomic(u32 __user *pifutex,\n\t\t\t\t struct futex_hash_bucket *hb1,\n\t\t\t\t struct futex_hash_bucket *hb2,\n\t\t\t\t union futex_key *key1, union futex_key *key2,\n\t\t\t\t struct futex_pi_state **ps, int set_waiters)\n{\n\tstruct futex_q *top_waiter = NULL;\n\tu32 curval;\n\tint ret, vpid;\n\n\tif (get_futex_value_locked(&curval, pifutex))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Find the top_waiter and determine if there are additional waiters.\n\t * If the caller intends to requeue more than 1 waiter to pifutex,\n\t * force futex_lock_pi_atomic() to set the FUTEX_WAITERS bit now,\n\t * as we have means to handle the possible fault.  If not, don't set\n\t * the bit unecessarily as it will force the subsequent unlock to enter\n\t * the kernel.\n\t */\n\ttop_waiter = futex_top_waiter(hb1, key1);\n\n\t/* There are no waiters, nothing for us to do. */\n\tif (!top_waiter)\n\t\treturn 0;\n\n\t/* Ensure we requeue to the expected futex. */\n\tif (!match_futex(top_waiter->requeue_pi_key, key2))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Try to take the lock for top_waiter.  Set the FUTEX_WAITERS bit in\n\t * the contended case or if set_waiters is 1.  The pi_state is returned\n\t * in ps in contended cases.\n\t */\n\tvpid = task_pid_vnr(top_waiter->task);\n\tret = futex_lock_pi_atomic(pifutex, hb2, key2, ps, top_waiter->task,\n\t\t\t\t   set_waiters);\n\tif (ret == 1) {\n\t\trequeue_pi_wake_futex(top_waiter, key2, hb2);\n\t\treturn vpid;\n\t}\n\treturn ret;\n}\n\n/**\n * futex_requeue() - Requeue waiters from uaddr1 to uaddr2\n * @uaddr1:\tsource futex user address\n * @flags:\tfutex flags (FLAGS_SHARED, etc.)\n * @uaddr2:\ttarget futex user address\n * @nr_wake:\tnumber of waiters to wake (must be 1 for requeue_pi)\n * @nr_requeue:\tnumber of waiters to requeue (0-INT_MAX)\n * @cmpval:\t@uaddr1 expected value (or %NULL)\n * @requeue_pi:\tif we are attempting to requeue from a non-pi futex to a\n *\t\tpi futex (pi to pi requeue is not supported)\n *\n * Requeue waiters on uaddr1 to uaddr2. In the requeue_pi case, try to acquire\n * uaddr2 atomically on behalf of the top waiter.\n *\n * Return:\n * >=0 - on success, the number of tasks requeued or woken;\n *  <0 - on error\n */\nstatic int futex_requeue(u32 __user *uaddr1, unsigned int flags,\n\t\t\t u32 __user *uaddr2, int nr_wake, int nr_requeue,\n\t\t\t u32 *cmpval, int requeue_pi)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tint drop_count = 0, task_count = 0, ret;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\n\tif (requeue_pi) {\n\t\t/*\n\t\t * requeue_pi requires a pi_state, try to allocate it now\n\t\t * without any locks in case it fails.\n\t\t */\n\t\tif (refill_pi_state_cache())\n\t\t\treturn -ENOMEM;\n\t\t/*\n\t\t * requeue_pi must wake as many tasks as it can, up to nr_wake\n\t\t * + nr_requeue, since it acquires the rt_mutex prior to\n\t\t * returning to userspace, so as to not leave the rt_mutex with\n\t\t * waiters and no owner.  However, second and third wake-ups\n\t\t * cannot be predicted as they involve race conditions with the\n\t\t * first wake and a fault while looking up the pi_state.  Both\n\t\t * pthread_cond_signal() and pthread_cond_broadcast() should\n\t\t * use nr_wake=1.\n\t\t */\n\t\tif (nr_wake != 1)\n\t\t\treturn -EINVAL;\n\t}\n\nretry:\n\tif (pi_state != NULL) {\n\t\t/*\n\t\t * We will have to lookup the pi_state again, so free this one\n\t\t * to keep the accounting correct.\n\t\t */\n\t\tfree_pi_state(pi_state);\n\t\tpi_state = NULL;\n\t}\n\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2,\n\t\t\t    requeue_pi ? VERIFY_WRITE : VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out_put_key1;\n\n\thb1 = hash_futex(&key1);\n\thb2 = hash_futex(&key2);\n\nretry_private:\n\thb_waiters_inc(hb2);\n\tdouble_lock_hb(hb1, hb2);\n\n\tif (likely(cmpval != NULL)) {\n\t\tu32 curval;\n\n\t\tret = get_futex_value_locked(&curval, uaddr1);\n\n\t\tif (unlikely(ret)) {\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\n\t\t\tret = get_user(curval, uaddr1);\n\t\t\tif (ret)\n\t\t\t\tgoto out_put_keys;\n\n\t\t\tif (!(flags & FLAGS_SHARED))\n\t\t\t\tgoto retry_private;\n\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tgoto retry;\n\t\t}\n\t\tif (curval != *cmpval) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (requeue_pi && (task_count - nr_wake < nr_requeue)) {\n\t\t/*\n\t\t * Attempt to acquire uaddr2 and wake the top waiter. If we\n\t\t * intend to requeue waiters, force setting the FUTEX_WAITERS\n\t\t * bit.  We force this here where we are able to easily handle\n\t\t * faults rather in the requeue loop below.\n\t\t */\n\t\tret = futex_proxy_trylock_atomic(uaddr2, hb1, hb2, &key1,\n\t\t\t\t\t\t &key2, &pi_state, nr_requeue);\n\n\t\t/*\n\t\t * At this point the top_waiter has either taken uaddr2 or is\n\t\t * waiting on it.  If the former, then the pi_state will not\n\t\t * exist yet, look it up one more time to ensure we have a\n\t\t * reference to it. If the lock was taken, ret contains the\n\t\t * vpid of the top waiter task.\n\t\t */\n\t\tif (ret > 0) {\n\t\t\tWARN_ON(pi_state);\n\t\t\tdrop_count++;\n\t\t\ttask_count++;\n\t\t\t/*\n\t\t\t * If we acquired the lock, then the user\n\t\t\t * space value of uaddr2 should be vpid. It\n\t\t\t * cannot be changed by the top waiter as it\n\t\t\t * is blocked on hb2 lock if it tries to do\n\t\t\t * so. If something fiddled with it behind our\n\t\t\t * back the pi state lookup might unearth\n\t\t\t * it. So we rather use the known value than\n\t\t\t * rereading and handing potential crap to\n\t\t\t * lookup_pi_state.\n\t\t\t */\n\t\t\tret = lookup_pi_state(ret, hb2, &key2, &pi_state, NULL);\n\t\t}\n\n\t\tswitch (ret) {\n\t\tcase 0:\n\t\t\tbreak;\n\t\tcase -EFAULT:\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tret = fault_in_user_writeable(uaddr2);\n\t\t\tif (!ret)\n\t\t\t\tgoto retry;\n\t\t\tgoto out;\n\t\tcase -EAGAIN:\n\t\t\t/* The owner was exiting, try again. */\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (task_count - nr_wake >= nr_requeue)\n\t\t\tbreak;\n\n\t\tif (!match_futex(&this->key, &key1))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * FUTEX_WAIT_REQEUE_PI and FUTEX_CMP_REQUEUE_PI should always\n\t\t * be paired with each other and no other futex ops.\n\t\t *\n\t\t * We should never be requeueing a futex_q with a pi_state,\n\t\t * which is awaiting a futex_unlock_pi().\n\t\t */\n\t\tif ((requeue_pi && !this->rt_waiter) ||\n\t\t    (!requeue_pi && this->rt_waiter) ||\n\t\t    this->pi_state) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Wake nr_wake waiters.  For requeue_pi, if we acquired the\n\t\t * lock, we already woke the top_waiter.  If not, it will be\n\t\t * woken by futex_unlock_pi().\n\t\t */\n\t\tif (++task_count <= nr_wake && !requeue_pi) {\n\t\t\twake_futex(this);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Ensure we requeue to the expected futex for requeue_pi. */\n\t\tif (requeue_pi && !match_futex(this->requeue_pi_key, &key2)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Requeue nr_requeue waiters and possibly one more in the case\n\t\t * of requeue_pi if we couldn't acquire the lock atomically.\n\t\t */\n\t\tif (requeue_pi) {\n\t\t\t/* Prepare the waiter to take the rt_mutex. */\n\t\t\tatomic_inc(&pi_state->refcount);\n\t\t\tthis->pi_state = pi_state;\n\t\t\tret = rt_mutex_start_proxy_lock(&pi_state->pi_mutex,\n\t\t\t\t\t\t\tthis->rt_waiter,\n\t\t\t\t\t\t\tthis->task, 1);\n\t\t\tif (ret == 1) {\n\t\t\t\t/* We got the lock. */\n\t\t\t\trequeue_pi_wake_futex(this, &key2, hb2);\n\t\t\t\tdrop_count++;\n\t\t\t\tcontinue;\n\t\t\t} else if (ret) {\n\t\t\t\t/* -EDEADLK */\n\t\t\t\tthis->pi_state = NULL;\n\t\t\t\tfree_pi_state(pi_state);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t\trequeue_futex(this, hb1, hb2, &key2);\n\t\tdrop_count++;\n\t}\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\n\thb_waiters_dec(hb2);\n\n\t/*\n\t * drop_futex_key_refs() must be called outside the spinlocks. During\n\t * the requeue we moved futex_q's from the hash bucket at key1 to the\n\t * one at key2 and updated their key pointer.  We no longer need to\n\t * hold the references to key1.\n\t */\n\twhile (--drop_count >= 0)\n\t\tdrop_futex_key_refs(&key1);\n\nout_put_keys:\n\tput_futex_key(&key2);\nout_put_key1:\n\tput_futex_key(&key1);\nout:\n\tif (pi_state != NULL)\n\t\tfree_pi_state(pi_state);\n\treturn ret ? ret : task_count;\n}\n\n/* The key must be already stored in q->key. */\nstatic inline struct futex_hash_bucket *queue_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = hash_futex(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all queue_lock()\n\t * users end up calling queue_me(). Similarly, for housekeeping,\n\t * decrement the counter at queue_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\thb_waiters_inc(hb);\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock); /* implies MB (A) */\n\treturn hb;\n}\n\nstatic inline void\nqueue_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\thb_waiters_dec(hb);\n}\n\n/**\n * queue_me() - Enqueue the futex_q on the futex_hash_bucket\n * @q:\tThe futex_q to enqueue\n * @hb:\tThe destination hash bucket\n *\n * The hb->lock must be held by the caller, and is released here. A call to\n * queue_me() is typically paired with exactly one call to unqueue_me().  The\n * exceptions involve the PI related operations, which may use unqueue_me_pi()\n * or nothing if the unqueue is done as part of the wake process and the unqueue\n * state is implicit in the state of woken task (see futex_wait_requeue_pi() for\n * an example).\n */\nstatic inline void queue_me(struct futex_q *q, struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n\tspin_unlock(&hb->lock);\n}\n\n/**\n * unqueue_me() - Remove the futex_q from its futex_hash_bucket\n * @q:\tThe futex_q to unqueue\n *\n * The q->lock_ptr must not be held by the caller. A call to unqueue_me() must\n * be paired with exactly one earlier call to queue_me().\n *\n * Return:\n *   1 - if the futex_q was still queued (and we removed unqueued it);\n *   0 - if the futex_q was already removed by the waking thread\n */\nstatic int unqueue_me(struct futex_q *q)\n{\n\tspinlock_t *lock_ptr;\n\tint ret = 0;\n\n\t/* In the common case we don't take the spinlock, which is nice. */\nretry:\n\tlock_ptr = q->lock_ptr;\n\tbarrier();\n\tif (lock_ptr != NULL) {\n\t\tspin_lock(lock_ptr);\n\t\t/*\n\t\t * q->lock_ptr can change between reading it and\n\t\t * spin_lock(), causing us to take the wrong lock.  This\n\t\t * corrects the race condition.\n\t\t *\n\t\t * Reasoning goes like this: if we have the wrong lock,\n\t\t * q->lock_ptr must have changed (maybe several times)\n\t\t * between reading it and the spin_lock().  It can\n\t\t * change again after the spin_lock() but only if it was\n\t\t * already changed before the spin_lock().  It cannot,\n\t\t * however, change back to the original value.  Therefore\n\t\t * we can detect whether we acquired the correct lock.\n\t\t */\n\t\tif (unlikely(lock_ptr != q->lock_ptr)) {\n\t\t\tspin_unlock(lock_ptr);\n\t\t\tgoto retry;\n\t\t}\n\t\t__unqueue_futex(q);\n\n\t\tBUG_ON(q->pi_state);\n\n\t\tspin_unlock(lock_ptr);\n\t\tret = 1;\n\t}\n\n\tdrop_futex_key_refs(&q->key);\n\treturn ret;\n}\n\n/*\n * PI futexes can not be requeued and must remove themself from the\n * hash bucket. The hash bucket lock (i.e. lock_ptr) is held on entry\n * and dropped here.\n */\nstatic void unqueue_me_pi(struct futex_q *q)\n\t__releases(q->lock_ptr)\n{\n\t__unqueue_futex(q);\n\n\tBUG_ON(!q->pi_state);\n\tfree_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n\n\tspin_unlock(q->lock_ptr);\n}\n\n/*\n * Fixup the pi_state owner with the new owner.\n *\n * Must be called with hash bucket lock held and mm->sem held for non\n * private futexes.\n */\nstatic int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *newowner)\n{\n\tu32 newtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tstruct task_struct *oldowner = pi_state->owner;\n\tu32 uval, uninitialized_var(curval), newval;\n\tint ret;\n\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\t/*\n\t * We are here either because we stole the rtmutex from the\n\t * previous highest priority waiter or we are the highest priority\n\t * waiter but failed to get the rtmutex the first time.\n\t * We have to replace the newowner TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would\n\t * leave the pi_state in an inconsistent state when we fault\n\t * here, because we need to drop the hash bucket lock to\n\t * handle the fault. This might be observed in the PID check\n\t * in lookup_pi_state.\n\t */\nretry:\n\tif (get_futex_value_locked(&uval, uaddr))\n\t\tgoto handle_fault;\n\n\twhile (1) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval))\n\t\t\tgoto handle_fault;\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tif (pi_state->owner != NULL) {\n\t\traw_spin_lock_irq(&pi_state->owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock_irq(&pi_state->owner->pi_lock);\n\t}\n\n\tpi_state->owner = newowner;\n\n\traw_spin_lock_irq(&newowner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &newowner->pi_state_list);\n\traw_spin_unlock_irq(&newowner->pi_lock);\n\treturn 0;\n\n\t/*\n\t * To handle the page fault we need to drop the hash bucket\n\t * lock here. That gives the other task (either the highest priority\n\t * waiter itself or the task which stole the rtmutex) the\n\t * chance to try the fixup of the pi_state. So once we are\n\t * back from handling the fault we need to check the pi_state\n\t * after reacquiring the hash bucket lock and before trying to\n\t * do another fixup. When the fixup has been done already we\n\t * simply return.\n\t */\nhandle_fault:\n\tspin_unlock(q->lock_ptr);\n\n\tret = fault_in_user_writeable(uaddr);\n\n\tspin_lock(q->lock_ptr);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner)\n\t\treturn 0;\n\n\tif (ret)\n\t\treturn ret;\n\n\tgoto retry;\n}\n\nstatic long futex_wait_restart(struct restart_block *restart);\n\n/**\n * fixup_owner() - Post lock pi_state and corner case management\n * @uaddr:\tuser address of the futex\n * @q:\t\tfutex_q (contains pi_state and access to the rt_mutex)\n * @locked:\tif the attempt to take the rt_mutex succeeded (1) or not (0)\n *\n * After attempting to lock an rt_mutex, this function is called to cleanup\n * the pi_state owner as well as handle race conditions that may allow us to\n * acquire the lock. Must be called with the hb lock held.\n *\n * Return:\n *  1 - success, lock taken;\n *  0 - success, lock not taken;\n * <0 - on error (-EFAULT)\n */\nstatic int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Catch the rare case, where the lock was released when we were on the\n\t * way back before we locked the hash bucket.\n\t */\n\tif (q->pi_state->owner == current) {\n\t\t/*\n\t\t * Try to get the rt_mutex now. This might fail as some other\n\t\t * task acquired the rt_mutex after we removed ourself from the\n\t\t * rt_mutex waiters list.\n\t\t */\n\t\tif (rt_mutex_trylock(&q->pi_state->pi_mutex)) {\n\t\t\tlocked = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * pi_state is incorrect, some other task did a lock steal and\n\t\t * we returned due to timeout or signal without taking the\n\t\t * rt_mutex. Too late.\n\t\t */\n\t\traw_spin_lock(&q->pi_state->pi_mutex.wait_lock);\n\t\towner = rt_mutex_owner(&q->pi_state->pi_mutex);\n\t\tif (!owner)\n\t\t\towner = rt_mutex_next_owner(&q->pi_state->pi_mutex);\n\t\traw_spin_unlock(&q->pi_state->pi_mutex.wait_lock);\n\t\tret = fixup_pi_state_owner(uaddr, q, owner);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current)\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\nout:\n\treturn ret ? ret : locked;\n}\n\n/**\n * futex_wait_queue_me() - queue_me() and wait for wakeup, timeout, or signal\n * @hb:\t\tthe futex hash bucket, must be locked by the caller\n * @q:\t\tthe futex_q to queue up on\n * @timeout:\tthe prepared hrtimer_sleeper, or null for no timeout\n */\nstatic void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,\n\t\t\t\tstruct hrtimer_sleeper *timeout)\n{\n\t/*\n\t * The task state is guaranteed to be set before another task can\n\t * wake it. set_current_state() is implemented using set_mb() and\n\t * queue_me() calls spin_unlock() upon completion, both serializing\n\t * access to the hash list and forcing another memory barrier.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tqueue_me(q, hb);\n\n\t/* Arm the timer */\n\tif (timeout) {\n\t\thrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);\n\t\tif (!hrtimer_active(&timeout->timer))\n\t\t\ttimeout->task = NULL;\n\t}\n\n\t/*\n\t * If we have been removed from the hash list, then another task\n\t * has tried to wake us, and we can skip the call to schedule().\n\t */\n\tif (likely(!plist_node_empty(&q->list))) {\n\t\t/*\n\t\t * If the timer has already expired, current will already be\n\t\t * flagged for rescheduling. Only call schedule if there\n\t\t * is no timeout, or if it has yet to expire.\n\t\t */\n\t\tif (!timeout || timeout->task)\n\t\t\tfreezable_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}\n\n/**\n * futex_wait_setup() - Prepare to wait on a futex\n * @uaddr:\tthe futex userspace address\n * @val:\tthe expected value\n * @flags:\tfutex flags (FLAGS_SHARED, etc.)\n * @q:\t\tthe associated futex_q\n * @hb:\t\tstorage for hash_bucket pointer to be returned to caller\n *\n * Setup the futex_q and locate the hash_bucket.  Get the futex value and\n * compare it with the expected value.  Handle atomic faults internally.\n * Return with the hb lock held and a q.key reference on success, and unlocked\n * with no q.key reference on failure.\n *\n * Return:\n *  0 - uaddr contains val and hb has been locked;\n * <1 - -EFAULT or -EWOULDBLOCK (uaddr does not contain val) and hb is unlocked\n */\nstatic int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,\n\t\t\t   struct futex_q *q, struct futex_hash_bucket **hb)\n{\n\tu32 uval;\n\tint ret;\n\n\t/*\n\t * Access the page AFTER the hash-bucket is locked.\n\t * Order is important:\n\t *\n\t *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);\n\t *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }\n\t *\n\t * The basic logical guarantee of a futex is that it blocks ONLY\n\t * if cond(var) is known to be true at the time of blocking, for\n\t * any cond.  If we locked the hash-bucket after testing *uaddr, that\n\t * would open a race condition where we could block indefinitely with\n\t * cond(var) false, which would violate the guarantee.\n\t *\n\t * On the other hand, we insert q and release the hash-bucket only\n\t * after testing *uaddr.  This guarantees that futex_wait() will NOT\n\t * absorb a wakeup if *uaddr does not match the desired values\n\t * while the syscall executes.\n\t */\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\nretry_private:\n\t*hb = queue_lock(q);\n\n\tret = get_futex_value_locked(&uval, uaddr);\n\n\tif (ret) {\n\t\tqueue_unlock(*hb);\n\n\t\tret = get_user(uval, uaddr);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tput_futex_key(&q->key);\n\t\tgoto retry;\n\t}\n\n\tif (uval != val) {\n\t\tqueue_unlock(*hb);\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout:\n\tif (ret)\n\t\tput_futex_key(&q->key);\n\treturn ret;\n}\n\nstatic int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,\n\t\t      ktime_t *abs_time, u32 bitset)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, holds hb lock and increments\n\t * q.key refs.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* queue_me and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\t/* unqueue_me() drops q.key ref */\n\tif (!unqueue_me(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current_thread_info()->restart_block;\n\trestart->fn = futex_wait_restart;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = abs_time->tv64;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = -ERESTART_RESTARTBLOCK;\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}\n\n\nstatic long futex_wait_restart(struct restart_block *restart)\n{\n\tu32 __user *uaddr = restart->futex.uaddr;\n\tktime_t t, *tp = NULL;\n\n\tif (restart->futex.flags & FLAGS_HAS_TIMEOUT) {\n\t\tt.tv64 = restart->futex.time;\n\t\ttp = &t;\n\t}\n\trestart->fn = do_no_restart_syscall;\n\n\treturn (long)futex_wait(uaddr, restart->futex.flags,\n\t\t\t\trestart->futex.val, tp, restart->futex.bitset);\n}\n\n\n/*\n * Userspace tried a 0 -> TID atomic transition of the futex value\n * and failed. The kernel side here does the whole locking operation:\n * if there are waiters then it will block, it does PI, etc. (Due to\n * races the kernel might see a 0 value of the futex too.)\n */\nstatic int futex_lock_pi(u32 __user *uaddr, unsigned int flags, int detect,\n\t\t\t ktime_t *time, int trylock)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (refill_pi_state_cache())\n\t\treturn -ENOMEM;\n\n\tif (time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, CLOCK_REALTIME,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires(&to->timer, *time);\n\t}\n\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\nretry_private:\n\thb = queue_lock(&q);\n\n\tret = futex_lock_pi_atomic(uaddr, hb, &q.key, &q.pi_state, current, 0);\n\tif (unlikely(ret)) {\n\t\tswitch (ret) {\n\t\tcase 1:\n\t\t\t/* We got the lock. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_put_key;\n\t\tcase -EFAULT:\n\t\t\tgoto uaddr_faulted;\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * Task is exiting and we just wait for the\n\t\t\t * exit to complete.\n\t\t\t */\n\t\t\tqueue_unlock(hb);\n\t\t\tput_futex_key(&q.key);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock_put_key;\n\t\t}\n\t}\n\n\t/*\n\t * Only actually queue now that the atomic ops are done:\n\t */\n\tqueue_me(&q, hb);\n\n\tWARN_ON(!q.pi_state);\n\t/*\n\t * Block on the PI mutex:\n\t */\n\tif (!trylock)\n\t\tret = rt_mutex_timed_lock(&q.pi_state->pi_mutex, to, 1);\n\telse {\n\t\tret = rt_mutex_trylock(&q.pi_state->pi_mutex);\n\t\t/* Fixup the trylock return value: */\n\t\tret = ret ? 0 : -EWOULDBLOCK;\n\t}\n\n\tspin_lock(q.lock_ptr);\n\t/*\n\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t * haven't already.\n\t */\n\tres = fixup_owner(uaddr, &q, !ret);\n\t/*\n\t * If fixup_owner() returned an error, proprogate that.  If it acquired\n\t * the lock, clear our -ETIMEDOUT or -EINTR.\n\t */\n\tif (res)\n\t\tret = (res < 0) ? res : 0;\n\n\t/*\n\t * If fixup_owner() faulted and was unable to handle the fault, unlock\n\t * it and return the fault to userspace.\n\t */\n\tif (ret && (rt_mutex_owner(&q.pi_state->pi_mutex) == current))\n\t\trt_mutex_unlock(&q.pi_state->pi_mutex);\n\n\t/* Unqueue and drop the lock */\n\tunqueue_me_pi(&q);\n\n\tgoto out_put_key;\n\nout_unlock_put_key:\n\tqueue_unlock(hb);\n\nout_put_key:\n\tput_futex_key(&q.key);\nout:\n\tif (to)\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\treturn ret != -EINTR ? ret : -ERESTARTNOINTR;\n\nuaddr_faulted:\n\tqueue_unlock(hb);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (ret)\n\t\tgoto out_put_key;\n\n\tif (!(flags & FLAGS_SHARED))\n\t\tgoto retry_private;\n\n\tput_futex_key(&q.key);\n\tgoto retry;\n}\n\n/*\n * Userspace attempted a TID -> 0 atomic transition, and failed.\n * This is the in-kernel slowpath: we look up the PI state (if any),\n * and do the rt-mutex unlock.\n */\nstatic int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tu32 uval, vpid = task_pid_vnr(current);\n\tint ret;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -EFAULT;\n\t/*\n\t * We release only a lock we actually own:\n\t */\n\tif ((uval & FUTEX_TID_MASK) != vpid)\n\t\treturn -EPERM;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\thb = hash_futex(&key);\n\tspin_lock(&hb->lock);\n\n\t/*\n\t * To avoid races, try to do the TID -> 0 atomic transition\n\t * again. If it succeeds then we can return without waking\n\t * anyone else up:\n\t */\n\tif (!(uval & FUTEX_OWNER_DIED) &&\n\t    cmpxchg_futex_value_locked(&uval, uaddr, vpid, 0))\n\t\tgoto pi_faulted;\n\t/*\n\t * Rare case: we managed to release the lock atomically,\n\t * no need to wake anyone else up:\n\t */\n\tif (unlikely(uval == vpid))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Ok, other tasks may need to be woken up - check waiters\n\t * and do the wakeup if necessary:\n\t */\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (!match_futex (&this->key, &key))\n\t\t\tcontinue;\n\t\tret = wake_futex_pi(uaddr, uval, this);\n\t\t/*\n\t\t * The atomic access to the futex value\n\t\t * generated a pagefault, so retry the\n\t\t * user-access and the wakeup:\n\t\t */\n\t\tif (ret == -EFAULT)\n\t\t\tgoto pi_faulted;\n\t\tgoto out_unlock;\n\t}\n\t/*\n\t * No waiters - kernel unlocks the futex:\n\t */\n\tif (!(uval & FUTEX_OWNER_DIED)) {\n\t\tret = unlock_futex_pi(uaddr, uval);\n\t\tif (ret == -EFAULT)\n\t\t\tgoto pi_faulted;\n\t}\n\nout_unlock:\n\tspin_unlock(&hb->lock);\n\tput_futex_key(&key);\n\nout:\n\treturn ret;\n\npi_faulted:\n\tspin_unlock(&hb->lock);\n\tput_futex_key(&key);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (!ret)\n\t\tgoto retry;\n\n\treturn ret;\n}\n\n/**\n * handle_early_requeue_pi_wakeup() - Detect early wakeup on the initial futex\n * @hb:\t\tthe hash_bucket futex_q was original enqueued on\n * @q:\t\tthe futex_q woken while waiting to be requeued\n * @key2:\tthe futex_key of the requeue target futex\n * @timeout:\tthe timeout associated with the wait (NULL if none)\n *\n * Detect if the task was woken on the initial futex as opposed to the requeue\n * target futex.  If so, determine if it was a timeout or a signal that caused\n * the wakeup and return the appropriate error code to the caller.  Must be\n * called with the hb lock held.\n *\n * Return:\n *  0 = no early wakeup detected;\n * <0 = -ETIMEDOUT or -ERESTARTNOINTR\n */\nstatic inline\nint handle_early_requeue_pi_wakeup(struct futex_hash_bucket *hb,\n\t\t\t\t   struct futex_q *q, union futex_key *key2,\n\t\t\t\t   struct hrtimer_sleeper *timeout)\n{\n\tint ret = 0;\n\n\t/*\n\t * With the hb lock held, we avoid races while we process the wakeup.\n\t * We only need to hold hb (and not hb2) to ensure atomicity as the\n\t * wakeup code can't change q.key from uaddr to uaddr2 if we hold hb.\n\t * It can't be requeued from uaddr2 to something else since we don't\n\t * support a PI aware source futex for requeue.\n\t */\n\tif (!match_futex(&q->key, key2)) {\n\t\tWARN_ON(q->lock_ptr && (&hb->lock != q->lock_ptr));\n\t\t/*\n\t\t * We were woken prior to requeue by a timeout or a signal.\n\t\t * Unqueue the futex_q and determine which it was.\n\t\t */\n\t\tplist_del(&q->list, &hb->chain);\n\t\thb_waiters_dec(hb);\n\n\t\t/* Handle spurious wakeups gracefully */\n\t\tret = -EWOULDBLOCK;\n\t\tif (timeout && !timeout->task)\n\t\t\tret = -ETIMEDOUT;\n\t\telse if (signal_pending(current))\n\t\t\tret = -ERESTARTNOINTR;\n\t}\n\treturn ret;\n}\n\n/**\n * futex_wait_requeue_pi() - Wait on uaddr and take uaddr2\n * @uaddr:\tthe futex we initially wait on (non-pi)\n * @flags:\tfutex flags (FLAGS_SHARED, FLAGS_CLOCKRT, etc.), they must be\n * \t\tthe same type, no requeueing from private to shared, etc.\n * @val:\tthe expected value of uaddr\n * @abs_time:\tabsolute timeout\n * @bitset:\t32 bit wakeup bitset set by userspace, defaults to all\n * @uaddr2:\tthe pi futex we will take prior to returning to user-space\n *\n * The caller will wait on uaddr and will be requeued by futex_requeue() to\n * uaddr2 which must be PI aware and unique from uaddr.  Normal wakeup will wake\n * on uaddr2 and complete the acquisition of the rt_mutex prior to returning to\n * userspace.  This ensures the rt_mutex maintains an owner when it has waiters;\n * without one, the pi logic would not know which task to boost/deboost, if\n * there was a need to.\n *\n * We call schedule in futex_wait_queue_me() when we enqueue and return there\n * via the following--\n * 1) wakeup on uaddr2 after an atomic lock acquisition by futex_requeue()\n * 2) wakeup on uaddr2 after a requeue\n * 3) signal\n * 4) timeout\n *\n * If 3, cleanup and return -ERESTARTNOINTR.\n *\n * If 2, we may then block on trying to take the rt_mutex and return via:\n * 5) successful lock\n * 6) signal\n * 7) timeout\n * 8) other lock acquisition failure\n *\n * If 6, return -EWOULDBLOCK (restarting the syscall would do the same).\n *\n * If 4 or 7, we cleanup and return with -ETIMEDOUT.\n *\n * Return:\n *  0 - On success;\n * <0 - On error\n */\nstatic int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t\t u32 val, ktime_t *abs_time, u32 bitset,\n\t\t\t\t u32 __user *uaddr2)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct rt_mutex *pi_mutex = NULL;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key2 = FUTEX_KEY_INIT;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (uaddr == uaddr2)\n\t\treturn -EINVAL;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\n\t/*\n\t * The waiter is allocated on our stack, manipulated by the requeue\n\t * code while we sleep on uaddr.\n\t */\n\tdebug_rt_mutex_init_waiter(&rt_waiter);\n\tRB_CLEAR_NODE(&rt_waiter.pi_tree_entry);\n\tRB_CLEAR_NODE(&rt_waiter.tree_entry);\n\trt_waiter.task = NULL;\n\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tq.bitset = bitset;\n\tq.rt_waiter = &rt_waiter;\n\tq.requeue_pi_key = &key2;\n\n\t/*\n\t * Prepare to wait on uaddr. On success, increments q.key (key1) ref\n\t * count.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out_key2;\n\n\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\tspin_lock(&hb->lock);\n\tret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);\n\tspin_unlock(&hb->lock);\n\tif (ret)\n\t\tgoto out_put_keys;\n\n\t/*\n\t * In order for us to be here, we know our q.key == key2, and since\n\t * we took the hb->lock above, we also know that futex_requeue() has\n\t * completed and we no longer have to concern ourselves with a wakeup\n\t * race with the atomic proxy lock acquisition by the requeue code. The\n\t * futex_requeue dropped our key1 reference and incremented our key2\n\t * reference count.\n\t */\n\n\t/* Check if the requeue code acquired the second futex for us. */\n\tif (!q.rt_waiter) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case.\n\t\t */\n\t\tif (q.pi_state && (q.pi_state->owner != current)) {\n\t\t\tspin_lock(q.lock_ptr);\n\t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n\t\t\tspin_unlock(q.lock_ptr);\n\t\t}\n\t} else {\n\t\t/*\n\t\t * We have been woken up by futex_unlock_pi(), a timeout, or a\n\t\t * signal.  futex_unlock_pi() will not destroy the lock_ptr nor\n\t\t * the pi_state.\n\t\t */\n\t\tWARN_ON(!q.pi_state);\n\t\tpi_mutex = &q.pi_state->pi_mutex;\n\t\tret = rt_mutex_finish_proxy_lock(pi_mutex, to, &rt_waiter, 1);\n\t\tdebug_rt_mutex_free_waiter(&rt_waiter);\n\n\t\tspin_lock(q.lock_ptr);\n\t\t/*\n\t\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t\t * haven't already.\n\t\t */\n\t\tres = fixup_owner(uaddr2, &q, !ret);\n\t\t/*\n\t\t * If fixup_owner() returned an error, proprogate that.  If it\n\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.\n\t\t */\n\t\tif (res)\n\t\t\tret = (res < 0) ? res : 0;\n\n\t\t/* Unqueue and drop the lock. */\n\t\tunqueue_me_pi(&q);\n\t}\n\n\t/*\n\t * If fixup_pi_state_owner() faulted and was unable to handle the\n\t * fault, unlock the rt_mutex and return the fault to userspace.\n\t */\n\tif (ret == -EFAULT) {\n\t\tif (pi_mutex && rt_mutex_owner(pi_mutex) == current)\n\t\t\trt_mutex_unlock(pi_mutex);\n\t} else if (ret == -EINTR) {\n\t\t/*\n\t\t * We've already been requeued, but cannot restart by calling\n\t\t * futex_lock_pi() directly. We could restart this syscall, but\n\t\t * it would detect that the user space \"val\" changed and return\n\t\t * -EWOULDBLOCK.  Save the overhead of the restart and return\n\t\t * -EWOULDBLOCK directly.\n\t\t */\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout_put_keys:\n\tput_futex_key(&q.key);\nout_key2:\n\tput_futex_key(&key2);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}\n\n/*\n * Support for robust futexes: the kernel cleans up held futexes at\n * thread exit time.\n *\n * Implementation: user-space maintains a per-thread list of locks it\n * is holding. Upon do_exit(), the kernel carefully walks this list,\n * and marks all locks that are owned by this thread with the\n * FUTEX_OWNER_DIED bit, and wakes up a waiter (if any). The list is\n * always manipulated with the lock held, so the list is private and\n * per-thread. Userspace also maintains a per-thread 'list_op_pending'\n * field, to allow the kernel to clean up if the thread dies after\n * acquiring the lock, but just before it could have added itself to\n * the list. There can only be one such pending lock.\n */\n\n/**\n * sys_set_robust_list() - Set the robust-futex list head of a task\n * @head:\tpointer to the list-head\n * @len:\tlength of the list-head, as userspace expects\n */\nSYSCALL_DEFINE2(set_robust_list, struct robust_list_head __user *, head,\n\t\tsize_t, len)\n{\n\tif (!futex_cmpxchg_enabled)\n\t\treturn -ENOSYS;\n\t/*\n\t * The kernel knows only one size for now:\n\t */\n\tif (unlikely(len != sizeof(*head)))\n\t\treturn -EINVAL;\n\n\tcurrent->robust_list = head;\n\n\treturn 0;\n}\n\n/**\n * sys_get_robust_list() - Get the robust-futex list head of a task\n * @pid:\tpid of the process [zero for current task]\n * @head_ptr:\tpointer to a list-head pointer, the kernel fills it in\n * @len_ptr:\tpointer to a length field, the kernel fills in the header size\n */\nSYSCALL_DEFINE3(get_robust_list, int, pid,\n\t\tstruct robust_list_head __user * __user *, head_ptr,\n\t\tsize_t __user *, len_ptr)\n{\n\tstruct robust_list_head __user *head;\n\tunsigned long ret;\n\tstruct task_struct *p;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn -ENOSYS;\n\n\trcu_read_lock();\n\n\tret = -ESRCH;\n\tif (!pid)\n\t\tp = current;\n\telse {\n\t\tp = find_task_by_vpid(pid);\n\t\tif (!p)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = -EPERM;\n\tif (!ptrace_may_access(p, PTRACE_MODE_READ))\n\t\tgoto err_unlock;\n\n\thead = p->robust_list;\n\trcu_read_unlock();\n\n\tif (put_user(sizeof(*head), len_ptr))\n\t\treturn -EFAULT;\n\treturn put_user(head, head_ptr);\n\nerr_unlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Process a futex-list entry, check whether it's owned by the\n * dying task, and do notification if so:\n */\nint handle_futex_death(u32 __user *uaddr, struct task_struct *curr, int pi)\n{\n\tu32 uval, uninitialized_var(nval), mval;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -1;\n\n\tif ((uval & FUTEX_TID_MASK) == task_pid_vnr(curr)) {\n\t\t/*\n\t\t * Ok, this dying thread is truly holding a futex\n\t\t * of interest. Set the OWNER_DIED bit atomically\n\t\t * via cmpxchg, and if the value had FUTEX_WAITERS\n\t\t * set, wake up a waiter (if any). (We have to do a\n\t\t * futex_wake() even if OWNER_DIED is already set -\n\t\t * to handle the rare but possible case of recursive\n\t\t * thread-death.) The rest of the cleanup is done in\n\t\t * userspace.\n\t\t */\n\t\tmval = (uval & FUTEX_WAITERS) | FUTEX_OWNER_DIED;\n\t\t/*\n\t\t * We are not holding a lock here, but we want to have\n\t\t * the pagefault_disable/enable() protection because\n\t\t * we want to handle the fault gracefully. If the\n\t\t * access fails we try to fault in the futex with R/W\n\t\t * verification via get_user_pages. get_user() above\n\t\t * does not guarantee R/W access. If that fails we\n\t\t * give up and leave the futex locked.\n\t\t */\n\t\tif (cmpxchg_futex_value_locked(&nval, uaddr, uval, mval)) {\n\t\t\tif (fault_in_user_writeable(uaddr))\n\t\t\t\treturn -1;\n\t\t\tgoto retry;\n\t\t}\n\t\tif (nval != uval)\n\t\t\tgoto retry;\n\n\t\t/*\n\t\t * Wake robust non-PI futexes here. The wakeup of\n\t\t * PI futexes happens in exit_pi_state():\n\t\t */\n\t\tif (!pi && (uval & FUTEX_WAITERS))\n\t\t\tfutex_wake(uaddr, 1, 1, FUTEX_BITSET_MATCH_ANY);\n\t}\n\treturn 0;\n}\n\n/*\n * Fetch a robust-list pointer. Bit 0 signals PI futexes:\n */\nstatic inline int fetch_robust_entry(struct robust_list __user **entry,\n\t\t\t\t     struct robust_list __user * __user *head,\n\t\t\t\t     unsigned int *pi)\n{\n\tunsigned long uentry;\n\n\tif (get_user(uentry, (unsigned long __user *)head))\n\t\treturn -EFAULT;\n\n\t*entry = (void __user *)(uentry & ~1UL);\n\t*pi = uentry & 1;\n\n\treturn 0;\n}\n\n/*\n * Walk curr->robust_list (very carefully, it's a userspace list!)\n * and mark any locks found there dead, and notify any waiters.\n *\n * We silently return on any sign of list-walking problem.\n */\nvoid exit_robust_list(struct task_struct *curr)\n{\n\tstruct robust_list_head __user *head = curr->robust_list;\n\tstruct robust_list __user *entry, *next_entry, *pending;\n\tunsigned int limit = ROBUST_LIST_LIMIT, pi, pip;\n\tunsigned int uninitialized_var(next_pi);\n\tunsigned long futex_offset;\n\tint rc;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn;\n\n\t/*\n\t * Fetch the list head (which was registered earlier, via\n\t * sys_set_robust_list()):\n\t */\n\tif (fetch_robust_entry(&entry, &head->list.next, &pi))\n\t\treturn;\n\t/*\n\t * Fetch the relative futex offset:\n\t */\n\tif (get_user(futex_offset, &head->futex_offset))\n\t\treturn;\n\t/*\n\t * Fetch any possibly pending lock-add first, and handle it\n\t * if it exists:\n\t */\n\tif (fetch_robust_entry(&pending, &head->list_op_pending, &pip))\n\t\treturn;\n\n\tnext_entry = NULL;\t/* avoid warning with gcc */\n\twhile (entry != &head->list) {\n\t\t/*\n\t\t * Fetch the next entry in the list before calling\n\t\t * handle_futex_death:\n\t\t */\n\t\trc = fetch_robust_entry(&next_entry, &entry->next, &next_pi);\n\t\t/*\n\t\t * A pending lock might already be on the list, so\n\t\t * don't process it twice:\n\t\t */\n\t\tif (entry != pending)\n\t\t\tif (handle_futex_death((void __user *)entry + futex_offset,\n\t\t\t\t\t\tcurr, pi))\n\t\t\t\treturn;\n\t\tif (rc)\n\t\t\treturn;\n\t\tentry = next_entry;\n\t\tpi = next_pi;\n\t\t/*\n\t\t * Avoid excessively long or circular lists:\n\t\t */\n\t\tif (!--limit)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (pending)\n\t\thandle_futex_death((void __user *)pending + futex_offset,\n\t\t\t\t   curr, pip);\n}\n\nlong do_futex(u32 __user *uaddr, int op, u32 val, ktime_t *timeout,\n\t\tu32 __user *uaddr2, u32 val2, u32 val3)\n{\n\tint cmd = op & FUTEX_CMD_MASK;\n\tunsigned int flags = 0;\n\n\tif (!(op & FUTEX_PRIVATE_FLAG))\n\t\tflags |= FLAGS_SHARED;\n\n\tif (op & FUTEX_CLOCK_REALTIME) {\n\t\tflags |= FLAGS_CLOCKRT;\n\t\tif (cmd != FUTEX_WAIT_BITSET && cmd != FUTEX_WAIT_REQUEUE_PI)\n\t\t\treturn -ENOSYS;\n\t}\n\n\tswitch (cmd) {\n\tcase FUTEX_LOCK_PI:\n\tcase FUTEX_UNLOCK_PI:\n\tcase FUTEX_TRYLOCK_PI:\n\tcase FUTEX_WAIT_REQUEUE_PI:\n\tcase FUTEX_CMP_REQUEUE_PI:\n\t\tif (!futex_cmpxchg_enabled)\n\t\t\treturn -ENOSYS;\n\t}\n\n\tswitch (cmd) {\n\tcase FUTEX_WAIT:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\tcase FUTEX_WAIT_BITSET:\n\t\treturn futex_wait(uaddr, flags, val, timeout, val3);\n\tcase FUTEX_WAKE:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\tcase FUTEX_WAKE_BITSET:\n\t\treturn futex_wake(uaddr, flags, val, val3);\n\tcase FUTEX_REQUEUE:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, NULL, 0);\n\tcase FUTEX_CMP_REQUEUE:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 0);\n\tcase FUTEX_WAKE_OP:\n\t\treturn futex_wake_op(uaddr, flags, uaddr2, val, val2, val3);\n\tcase FUTEX_LOCK_PI:\n\t\treturn futex_lock_pi(uaddr, flags, val, timeout, 0);\n\tcase FUTEX_UNLOCK_PI:\n\t\treturn futex_unlock_pi(uaddr, flags);\n\tcase FUTEX_TRYLOCK_PI:\n\t\treturn futex_lock_pi(uaddr, flags, 0, timeout, 1);\n\tcase FUTEX_WAIT_REQUEUE_PI:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\t\treturn futex_wait_requeue_pi(uaddr, flags, val, timeout, val3,\n\t\t\t\t\t     uaddr2);\n\tcase FUTEX_CMP_REQUEUE_PI:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 1);\n\t}\n\treturn -ENOSYS;\n}\n\n\nSYSCALL_DEFINE6(futex, u32 __user *, uaddr, int, op, u32, val,\n\t\tstruct timespec __user *, utime, u32 __user *, uaddr2,\n\t\tu32, val3)\n{\n\tstruct timespec ts;\n\tktime_t t, *tp = NULL;\n\tu32 val2 = 0;\n\tint cmd = op & FUTEX_CMD_MASK;\n\n\tif (utime && (cmd == FUTEX_WAIT || cmd == FUTEX_LOCK_PI ||\n\t\t      cmd == FUTEX_WAIT_BITSET ||\n\t\t      cmd == FUTEX_WAIT_REQUEUE_PI)) {\n\t\tif (copy_from_user(&ts, utime, sizeof(ts)) != 0)\n\t\t\treturn -EFAULT;\n\t\tif (!timespec_valid(&ts))\n\t\t\treturn -EINVAL;\n\n\t\tt = timespec_to_ktime(ts);\n\t\tif (cmd == FUTEX_WAIT)\n\t\t\tt = ktime_add_safe(ktime_get(), t);\n\t\ttp = &t;\n\t}\n\t/*\n\t * requeue parameter in 'utime' if cmd == FUTEX_*_REQUEUE_*.\n\t * number of waiters to wake in 'utime' if cmd == FUTEX_WAKE_OP.\n\t */\n\tif (cmd == FUTEX_REQUEUE || cmd == FUTEX_CMP_REQUEUE ||\n\t    cmd == FUTEX_CMP_REQUEUE_PI || cmd == FUTEX_WAKE_OP)\n\t\tval2 = (u32) (unsigned long) utime;\n\n\treturn do_futex(uaddr, op, val, tp, uaddr2, val2, val3);\n}\n\nstatic void __init futex_detect_cmpxchg(void)\n{\n#ifndef CONFIG_HAVE_FUTEX_CMPXCHG\n\tu32 curval;\n\n\t/*\n\t * This will fail and we want it. Some arch implementations do\n\t * runtime detection of the futex_atomic_cmpxchg_inatomic()\n\t * functionality. We want to know that before we call in any\n\t * of the complex code paths. Also we want to prevent\n\t * registration of robust lists in that case. NULL is\n\t * guaranteed to fault and we get -EFAULT on functional\n\t * implementation, the non-functional ones will return\n\t * -ENOSYS.\n\t */\n\tif (cmpxchg_futex_value_locked(&curval, NULL, 0, 0) == -EFAULT)\n\t\tfutex_cmpxchg_enabled = 1;\n#endif\n}\n\nstatic int __init futex_init(void)\n{\n\tunsigned int futex_shift;\n\tunsigned long i;\n\n#if CONFIG_BASE_SMALL\n\tfutex_hashsize = 16;\n#else\n\tfutex_hashsize = roundup_pow_of_two(256 * num_possible_cpus());\n#endif\n\n\tfutex_queues = alloc_large_system_hash(\"futex\", sizeof(*futex_queues),\n\t\t\t\t\t       futex_hashsize, 0,\n\t\t\t\t\t       futex_hashsize < 256 ? HASH_SMALL : 0,\n\t\t\t\t\t       &futex_shift, NULL,\n\t\t\t\t\t       futex_hashsize, futex_hashsize);\n\tfutex_hashsize = 1UL << futex_shift;\n\n\tfutex_detect_cmpxchg();\n\n\tfor (i = 0; i < futex_hashsize; i++) {\n\t\tatomic_set(&futex_queues[i].waiters, 0);\n\t\tplist_head_init(&futex_queues[i].chain);\n\t\tspin_lock_init(&futex_queues[i].lock);\n\t}\n\n\treturn 0;\n}\n__initcall(futex_init);\n"], "fixing_code": ["/*\n *  Fast Userspace Mutexes (which I call \"Futexes!\").\n *  (C) Rusty Russell, IBM 2002\n *\n *  Generalized futexes, futex requeueing, misc fixes by Ingo Molnar\n *  (C) Copyright 2003 Red Hat Inc, All Rights Reserved\n *\n *  Removed page pinning, fix privately mapped COW pages and other cleanups\n *  (C) Copyright 2003, 2004 Jamie Lokier\n *\n *  Robust futex support started by Ingo Molnar\n *  (C) Copyright 2006 Red Hat Inc, All Rights Reserved\n *  Thanks to Thomas Gleixner for suggestions, analysis and fixes.\n *\n *  PI-futex support started by Ingo Molnar and Thomas Gleixner\n *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>\n *  Copyright (C) 2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>\n *\n *  PRIVATE futexes by Eric Dumazet\n *  Copyright (C) 2007 Eric Dumazet <dada1@cosmosbay.com>\n *\n *  Requeue-PI support by Darren Hart <dvhltc@us.ibm.com>\n *  Copyright (C) IBM Corporation, 2009\n *  Thanks to Thomas Gleixner for conceptual design and careful reviews.\n *\n *  Thanks to Ben LaHaise for yelling \"hashed waitqueues\" loudly\n *  enough at me, Linus for the original (flawed) idea, Matthew\n *  Kirkwood for proof-of-concept implementation.\n *\n *  \"The futexes are also cursed.\"\n *  \"But they come in a choice of three flavours!\"\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 2 of the License, or\n *  (at your option) any later version.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License\n *  along with this program; if not, write to the Free Software\n *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n */\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/jhash.h>\n#include <linux/init.h>\n#include <linux/futex.h>\n#include <linux/mount.h>\n#include <linux/pagemap.h>\n#include <linux/syscalls.h>\n#include <linux/signal.h>\n#include <linux/export.h>\n#include <linux/magic.h>\n#include <linux/pid.h>\n#include <linux/nsproxy.h>\n#include <linux/ptrace.h>\n#include <linux/sched/rt.h>\n#include <linux/hugetlb.h>\n#include <linux/freezer.h>\n#include <linux/bootmem.h>\n\n#include <asm/futex.h>\n\n#include \"locking/rtmutex_common.h\"\n\n/*\n * READ this before attempting to hack on futexes!\n *\n * Basic futex operation and ordering guarantees\n * =============================================\n *\n * The waiter reads the futex value in user space and calls\n * futex_wait(). This function computes the hash bucket and acquires\n * the hash bucket lock. After that it reads the futex user space value\n * again and verifies that the data has not changed. If it has not changed\n * it enqueues itself into the hash bucket, releases the hash bucket lock\n * and schedules.\n *\n * The waker side modifies the user space value of the futex and calls\n * futex_wake(). This function computes the hash bucket and acquires the\n * hash bucket lock. Then it looks for waiters on that futex in the hash\n * bucket and wakes them.\n *\n * In futex wake up scenarios where no tasks are blocked on a futex, taking\n * the hb spinlock can be avoided and simply return. In order for this\n * optimization to work, ordering guarantees must exist so that the waiter\n * being added to the list is acknowledged when the list is concurrently being\n * checked by the waker, avoiding scenarios like the following:\n *\n * CPU 0                               CPU 1\n * val = *futex;\n * sys_futex(WAIT, futex, val);\n *   futex_wait(futex, val);\n *   uval = *futex;\n *                                     *futex = newval;\n *                                     sys_futex(WAKE, futex);\n *                                       futex_wake(futex);\n *                                       if (queue_empty())\n *                                         return;\n *   if (uval == val)\n *      lock(hash_bucket(futex));\n *      queue();\n *     unlock(hash_bucket(futex));\n *     schedule();\n *\n * This would cause the waiter on CPU 0 to wait forever because it\n * missed the transition of the user space value from val to newval\n * and the waker did not find the waiter in the hash bucket queue.\n *\n * The correct serialization ensures that a waiter either observes\n * the changed user space value before blocking or is woken by a\n * concurrent waker:\n *\n * CPU 0                                 CPU 1\n * val = *futex;\n * sys_futex(WAIT, futex, val);\n *   futex_wait(futex, val);\n *\n *   waiters++; (a)\n *   mb(); (A) <-- paired with -.\n *                              |\n *   lock(hash_bucket(futex));  |\n *                              |\n *   uval = *futex;             |\n *                              |        *futex = newval;\n *                              |        sys_futex(WAKE, futex);\n *                              |          futex_wake(futex);\n *                              |\n *                              `------->  mb(); (B)\n *   if (uval == val)\n *     queue();\n *     unlock(hash_bucket(futex));\n *     schedule();                         if (waiters)\n *                                           lock(hash_bucket(futex));\n *   else                                    wake_waiters(futex);\n *     waiters--; (b)                        unlock(hash_bucket(futex));\n *\n * Where (A) orders the waiters increment and the futex value read through\n * atomic operations (see hb_waiters_inc) and where (B) orders the write\n * to futex and the waiters read -- this is done by the barriers in\n * get_futex_key_refs(), through either ihold or atomic_inc, depending on the\n * futex type.\n *\n * This yields the following case (where X:=waiters, Y:=futex):\n *\n *\tX = Y = 0\n *\n *\tw[X]=1\t\tw[Y]=1\n *\tMB\t\tMB\n *\tr[Y]=y\t\tr[X]=x\n *\n * Which guarantees that x==0 && y==0 is impossible; which translates back into\n * the guarantee that we cannot both miss the futex variable change and the\n * enqueue.\n *\n * Note that a new waiter is accounted for in (a) even when it is possible that\n * the wait call can return error, in which case we backtrack from it in (b).\n * Refer to the comment in queue_lock().\n *\n * Similarly, in order to account for waiters being requeued on another\n * address we always increment the waiters for the destination bucket before\n * acquiring the lock. It then decrements them again  after releasing it -\n * the code that actually moves the futex(es) between hash buckets (requeue_futex)\n * will do the additional required waiter count housekeeping. This is done for\n * double_lock_hb() and double_unlock_hb(), respectively.\n */\n\n#ifndef CONFIG_HAVE_FUTEX_CMPXCHG\nint __read_mostly futex_cmpxchg_enabled;\n#endif\n\n/*\n * Futex flags used to encode options to functions and preserve them across\n * restarts.\n */\n#define FLAGS_SHARED\t\t0x01\n#define FLAGS_CLOCKRT\t\t0x02\n#define FLAGS_HAS_TIMEOUT\t0x04\n\n/*\n * Priority Inheritance state:\n */\nstruct futex_pi_state {\n\t/*\n\t * list of 'owned' pi_state instances - these have to be\n\t * cleaned up in do_exit() if the task exits prematurely:\n\t */\n\tstruct list_head list;\n\n\t/*\n\t * The PI object:\n\t */\n\tstruct rt_mutex pi_mutex;\n\n\tstruct task_struct *owner;\n\tatomic_t refcount;\n\n\tunion futex_key key;\n};\n\n/**\n * struct futex_q - The hashed futex queue entry, one per waiting task\n * @list:\t\tpriority-sorted list of tasks waiting on this futex\n * @task:\t\tthe task waiting on the futex\n * @lock_ptr:\t\tthe hash bucket lock\n * @key:\t\tthe key the futex is hashed on\n * @pi_state:\t\toptional priority inheritance state\n * @rt_waiter:\t\trt_waiter storage for use with requeue_pi\n * @requeue_pi_key:\tthe requeue_pi target futex key\n * @bitset:\t\tbitset for the optional bitmasked wakeup\n *\n * We use this hashed waitqueue, instead of a normal wait_queue_t, so\n * we can wake only the relevant ones (hashed queues may be shared).\n *\n * A futex_q has a woken state, just like tasks have TASK_RUNNING.\n * It is considered woken when plist_node_empty(&q->list) || q->lock_ptr == 0.\n * The order of wakeup is always to make the first condition true, then\n * the second.\n *\n * PI futexes are typically woken before they are removed from the hash list via\n * the rt_mutex code. See unqueue_me_pi().\n */\nstruct futex_q {\n\tstruct plist_node list;\n\n\tstruct task_struct *task;\n\tspinlock_t *lock_ptr;\n\tunion futex_key key;\n\tstruct futex_pi_state *pi_state;\n\tstruct rt_mutex_waiter *rt_waiter;\n\tunion futex_key *requeue_pi_key;\n\tu32 bitset;\n};\n\nstatic const struct futex_q futex_q_init = {\n\t/* list gets initialized in queue_me()*/\n\t.key = FUTEX_KEY_INIT,\n\t.bitset = FUTEX_BITSET_MATCH_ANY\n};\n\n/*\n * Hash buckets are shared by all the futex_keys that hash to the same\n * location.  Each key may have multiple futex_q structures, one for each task\n * waiting on a futex.\n */\nstruct futex_hash_bucket {\n\tatomic_t waiters;\n\tspinlock_t lock;\n\tstruct plist_head chain;\n} ____cacheline_aligned_in_smp;\n\nstatic unsigned long __read_mostly futex_hashsize;\n\nstatic struct futex_hash_bucket *futex_queues;\n\nstatic inline void futex_get_mm(union futex_key *key)\n{\n\tatomic_inc(&key->private.mm->mm_count);\n\t/*\n\t * Ensure futex_get_mm() implies a full barrier such that\n\t * get_futex_key() implies a full barrier. This is relied upon\n\t * as full barrier (B), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic_inc();\n}\n\n/*\n * Reflects a new waiter being added to the waitqueue.\n */\nstatic inline void hb_waiters_inc(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\tatomic_inc(&hb->waiters);\n\t/*\n\t * Full barrier (A), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic_inc();\n#endif\n}\n\n/*\n * Reflects a waiter being removed from the waitqueue by wakeup\n * paths.\n */\nstatic inline void hb_waiters_dec(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\tatomic_dec(&hb->waiters);\n#endif\n}\n\nstatic inline int hb_waiters_pending(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\treturn atomic_read(&hb->waiters);\n#else\n\treturn 1;\n#endif\n}\n\n/*\n * We hash on the keys returned from get_futex_key (see below).\n */\nstatic struct futex_hash_bucket *hash_futex(union futex_key *key)\n{\n\tu32 hash = jhash2((u32*)&key->both.word,\n\t\t\t  (sizeof(key->both.word)+sizeof(key->both.ptr))/4,\n\t\t\t  key->both.offset);\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}\n\n/*\n * Return 1 if two futex_keys are equal, 0 otherwise.\n */\nstatic inline int match_futex(union futex_key *key1, union futex_key *key2)\n{\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}\n\n/*\n * Take a reference to the resource addressed by a key.\n * Can be called while holding spinlocks.\n *\n */\nstatic void get_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr)\n\t\treturn;\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tihold(key->shared.inode); /* implies MB (B) */\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tfutex_get_mm(key); /* implies MB (B) */\n\t\tbreak;\n\t}\n}\n\n/*\n * Drop a reference to the resource addressed by a key.\n * The hash bucket spinlock must not be held.\n */\nstatic void drop_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr) {\n\t\t/* If we're here then we tried to put a key we failed to get */\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tiput(key->shared.inode);\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tmmdrop(key->private.mm);\n\t\tbreak;\n\t}\n}\n\n/**\n * get_futex_key() - Get parameters which are the keys for a futex\n * @uaddr:\tvirtual address of the futex\n * @fshared:\t0 for a PROCESS_PRIVATE futex, 1 for PROCESS_SHARED\n * @key:\taddress where result is stored.\n * @rw:\t\tmapping needs to be read/write (values: VERIFY_READ,\n *              VERIFY_WRITE)\n *\n * Return: a negative error code or 0\n *\n * The key words are stored in *key on success.\n *\n * For shared mappings, it's (page->index, file_inode(vma->vm_file),\n * offset_within_page).  For private mappings, it's (uaddr, current->mm).\n * We can usually work out the index without swapping in the page.\n *\n * lock_page() might sleep, the caller should not hold a spinlock.\n */\nstatic int\nget_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *page_head;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(rw, uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\tget_futex_key_refs(key);  /* implies MB (B) */\n\t\treturn 0;\n\t}\n\nagain:\n\terr = get_user_pages_fast(address, 1, 1, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == VERIFY_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tpage_head = page;\n\tif (unlikely(PageTail(page))) {\n\t\tput_page(page);\n\t\t/* serialize against __split_huge_page_splitting() */\n\t\tlocal_irq_disable();\n\t\tif (likely(__get_user_pages_fast(address, 1, !ro, &page) == 1)) {\n\t\t\tpage_head = compound_head(page);\n\t\t\t/*\n\t\t\t * page_head is valid pointer but we must pin\n\t\t\t * it before taking the PG_lock and/or\n\t\t\t * PG_compound_lock. The moment we re-enable\n\t\t\t * irqs __split_huge_page_splitting() can\n\t\t\t * return and the head page can be freed from\n\t\t\t * under us. We can't take the PG_lock and/or\n\t\t\t * PG_compound_lock on a page that could be\n\t\t\t * freed from under us.\n\t\t\t */\n\t\t\tif (page != page_head) {\n\t\t\t\tget_page(page_head);\n\t\t\t\tput_page(page);\n\t\t\t}\n\t\t\tlocal_irq_enable();\n\t\t} else {\n\t\t\tlocal_irq_enable();\n\t\t\tgoto again;\n\t\t}\n\t}\n#else\n\tpage_head = compound_head(page);\n\tif (page != page_head) {\n\t\tget_page(page_head);\n\t\tput_page(page);\n\t}\n#endif\n\n\tlock_page(page_head);\n\n\t/*\n\t * If page_head->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page_head->mapping.\n\t */\n\tif (!page_head->mapping) {\n\t\tint shmem_swizzled = PageSwapCache(page_head);\n\t\tunlock_page(page_head);\n\t\tput_page(page_head);\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page_head)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t} else {\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.inode = page_head->mapping->host;\n\t\tkey->shared.pgoff = basepage_index(page);\n\t}\n\n\tget_futex_key_refs(key); /* implies MB (B) */\n\nout:\n\tunlock_page(page_head);\n\tput_page(page_head);\n\treturn err;\n}\n\nstatic inline void put_futex_key(union futex_key *key)\n{\n\tdrop_futex_key_refs(key);\n}\n\n/**\n * fault_in_user_writeable() - Fault in user address and verify RW access\n * @uaddr:\tpointer to faulting user space address\n *\n * Slow path to fixup the fault we just took in the atomic write\n * access to @uaddr.\n *\n * We have no generic implementation of a non-destructive write to the\n * user address. We know that we faulted in the atomic pagefault\n * disabled section so we can as well avoid the #PF overhead by\n * calling get_user_pages() right away.\n */\nstatic int fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tdown_read(&mm->mmap_sem);\n\tret = fixup_user_fault(current, mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE);\n\tup_read(&mm->mmap_sem);\n\n\treturn ret < 0 ? ret : 0;\n}\n\n/**\n * futex_top_waiter() - Return the highest priority waiter on a futex\n * @hb:\t\tthe hash bucket the futex_q's reside in\n * @key:\tthe futex key (to distinguish it from other futex futex_q's)\n *\n * Must be called with the hb lock held.\n */\nstatic struct futex_q *futex_top_waiter(struct futex_hash_bucket *hb,\n\t\t\t\t\tunion futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (match_futex(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}\n\nstatic int cmpxchg_futex_value_locked(u32 *curval, u32 __user *uaddr,\n\t\t\t\t      u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}\n\nstatic int get_futex_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __copy_from_user_inatomic(dest, from, sizeof(u32));\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}\n\n\n/*\n * PI code:\n */\nstatic int refill_pi_state_cache(void)\n{\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\tatomic_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}\n\nstatic struct futex_pi_state * alloc_pi_state(void)\n{\n\tstruct futex_pi_state *pi_state = current->pi_state_cache;\n\n\tWARN_ON(!pi_state);\n\tcurrent->pi_state_cache = NULL;\n\n\treturn pi_state;\n}\n\nstatic void free_pi_state(struct futex_pi_state *pi_state)\n{\n\tif (!atomic_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\traw_spin_lock_irq(&pi_state->owner->pi_lock);\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock_irq(&pi_state->owner->pi_lock);\n\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex, pi_state->owner);\n\t}\n\n\tif (current->pi_state_cache)\n\t\tkfree(pi_state);\n\telse {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\tatomic_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}\n\n/*\n * Look up the task based on what TID userspace gave us.\n * We dont trust it.\n */\nstatic struct task_struct * futex_find_get_task(pid_t pid)\n{\n\tstruct task_struct *p;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p)\n\t\tget_task_struct(p);\n\n\trcu_read_unlock();\n\n\treturn p;\n}\n\n/*\n * This task is holding PI mutexes at exit time => bad.\n * Kernel cleans up PI-state, but userspace is likely hosed.\n * (Robust-futex cleanup is separate and might save the day for userspace.)\n */\nvoid exit_pi_state_list(struct task_struct *curr)\n{\n\tstruct list_head *next, *head = &curr->pi_state_list;\n\tstruct futex_pi_state *pi_state;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn;\n\t/*\n\t * We are a ZOMBIE and nobody can enqueue itself on\n\t * pi_state_list anymore, but we have to be careful\n\t * versus waiters unqueueing themselves:\n\t */\n\traw_spin_lock_irq(&curr->pi_lock);\n\twhile (!list_empty(head)) {\n\n\t\tnext = head->next;\n\t\tpi_state = list_entry(next, struct futex_pi_state, list);\n\t\tkey = pi_state->key;\n\t\thb = hash_futex(&key);\n\t\traw_spin_unlock_irq(&curr->pi_lock);\n\n\t\tspin_lock(&hb->lock);\n\n\t\traw_spin_lock_irq(&curr->pi_lock);\n\t\t/*\n\t\t * We dropped the pi-lock, so re-check whether this\n\t\t * task still owns the PI-state:\n\t\t */\n\t\tif (head->next != next) {\n\t\t\tspin_unlock(&hb->lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON(pi_state->owner != curr);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\tpi_state->owner = NULL;\n\t\traw_spin_unlock_irq(&curr->pi_lock);\n\n\t\trt_mutex_unlock(&pi_state->pi_mutex);\n\n\t\tspin_unlock(&hb->lock);\n\n\t\traw_spin_lock_irq(&curr->pi_lock);\n\t}\n\traw_spin_unlock_irq(&curr->pi_lock);\n}\n\nstatic int\nlookup_pi_state(u32 uval, struct futex_hash_bucket *hb,\n\t\tunion futex_key *key, struct futex_pi_state **ps,\n\t\tstruct task_struct *task)\n{\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_q *this, *next;\n\tstruct task_struct *p;\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (match_futex(&this->key, key)) {\n\t\t\t/*\n\t\t\t * Another waiter already exists - bump up\n\t\t\t * the refcount and return its pi_state:\n\t\t\t */\n\t\t\tpi_state = this->pi_state;\n\t\t\t/*\n\t\t\t * Userspace might have messed up non-PI and PI futexes\n\t\t\t */\n\t\t\tif (unlikely(!pi_state))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tWARN_ON(!atomic_read(&pi_state->refcount));\n\n\t\t\t/*\n\t\t\t * When pi_state->owner is NULL then the owner died\n\t\t\t * and another waiter is on the fly. pi_state->owner\n\t\t\t * is fixed up by the task which acquires\n\t\t\t * pi_state->rt_mutex.\n\t\t\t *\n\t\t\t * We do not check for pid == 0 which can happen when\n\t\t\t * the owner died and robust_list_exit() cleared the\n\t\t\t * TID.\n\t\t\t */\n\t\t\tif (pid && pi_state->owner) {\n\t\t\t\t/*\n\t\t\t\t * Bail out if user space manipulated the\n\t\t\t\t * futex value.\n\t\t\t\t */\n\t\t\t\tif (pid != task_pid_vnr(pi_state->owner))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Protect against a corrupted uval. If uval\n\t\t\t * is 0x80000000 then pid is 0 and the waiter\n\t\t\t * bit is set. So the deadlock check in the\n\t\t\t * calling code has failed and we did not fall\n\t\t\t * into the check above due to !pid.\n\t\t\t */\n\t\t\tif (task && pi_state->owner == task)\n\t\t\t\treturn -EDEADLK;\n\n\t\t\tatomic_inc(&pi_state->refcount);\n\t\t\t*ps = pi_state;\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0\n\t */\n\tif (!pid)\n\t\treturn -ESRCH;\n\tp = futex_find_get_task(pid);\n\tif (!p)\n\t\treturn -ESRCH;\n\n\tif (!p->mm) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state flags to figure out,\n\t * whether the task is exiting. To protect against the do_exit\n\t * change of the task flags, we do this protected by\n\t * p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->flags & PF_EXITING)) {\n\t\t/*\n\t\t * The task is on the way out. When PF_EXITPIDONE is\n\t\t * set, we know that the task has finished the\n\t\t * cleanup:\n\t\t */\n\t\tint ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\tpi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make 'p'\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\tpi_state->owner = p;\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\t*ps = pi_state;\n\n\treturn 0;\n}\n\n/**\n * futex_lock_pi_atomic() - Atomic work required to acquire a pi aware futex\n * @uaddr:\t\tthe pi futex user address\n * @hb:\t\t\tthe pi futex hash bucket\n * @key:\t\tthe futex key associated with uaddr and hb\n * @ps:\t\t\tthe pi_state pointer where we store the result of the\n *\t\t\tlookup\n * @task:\t\tthe task to perform the atomic lock work for.  This will\n *\t\t\tbe \"current\" except in the case of requeue pi.\n * @set_waiters:\tforce setting the FUTEX_WAITERS bit (1) or not (0)\n *\n * Return:\n *  0 - ready to wait;\n *  1 - acquired the lock;\n * <0 - error\n *\n * The hb->lock and futex_key refs shall be held by the caller.\n */\nstatic int futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,\n\t\t\t\tunion futex_key *key,\n\t\t\t\tstruct futex_pi_state **ps,\n\t\t\t\tstruct task_struct *task, int set_waiters)\n{\n\tint lock_taken, ret, force_take = 0;\n\tu32 uval, newval, curval, vpid = task_pid_vnr(task);\n\nretry:\n\tret = lock_taken = 0;\n\n\t/*\n\t * To avoid races, we attempt to take the lock here again\n\t * (by doing a 0 -> TID atomic cmpxchg), while holding all\n\t * the locks. It will most likely not succeed.\n\t */\n\tnewval = vpid;\n\tif (set_waiters)\n\t\tnewval |= FUTEX_WAITERS;\n\n\tif (unlikely(cmpxchg_futex_value_locked(&curval, uaddr, 0, newval)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Detect deadlocks.\n\t */\n\tif ((unlikely((curval & FUTEX_TID_MASK) == vpid)))\n\t\treturn -EDEADLK;\n\n\t/*\n\t * Surprise - we got the lock. Just return to userspace:\n\t */\n\tif (unlikely(!curval))\n\t\treturn 1;\n\n\tuval = curval;\n\n\t/*\n\t * Set the FUTEX_WAITERS flag, so the owner will know it has someone\n\t * to wake at the next unlock.\n\t */\n\tnewval = curval | FUTEX_WAITERS;\n\n\t/*\n\t * Should we force take the futex? See below.\n\t */\n\tif (unlikely(force_take)) {\n\t\t/*\n\t\t * Keep the OWNER_DIED and the WAITERS bit and set the\n\t\t * new TID value.\n\t\t */\n\t\tnewval = (curval & ~FUTEX_TID_MASK) | vpid;\n\t\tforce_take = 0;\n\t\tlock_taken = 1;\n\t}\n\n\tif (unlikely(cmpxchg_futex_value_locked(&curval, uaddr, uval, newval)))\n\t\treturn -EFAULT;\n\tif (unlikely(curval != uval))\n\t\tgoto retry;\n\n\t/*\n\t * We took the lock due to forced take over.\n\t */\n\tif (unlikely(lock_taken))\n\t\treturn 1;\n\n\t/*\n\t * We dont have the lock. Look up the PI state (or create it if\n\t * we are the first waiter):\n\t */\n\tret = lookup_pi_state(uval, hb, key, ps, task);\n\n\tif (unlikely(ret)) {\n\t\tswitch (ret) {\n\t\tcase -ESRCH:\n\t\t\t/*\n\t\t\t * We failed to find an owner for this\n\t\t\t * futex. So we have no pi_state to block\n\t\t\t * on. This can happen in two cases:\n\t\t\t *\n\t\t\t * 1) The owner died\n\t\t\t * 2) A stale FUTEX_WAITERS bit\n\t\t\t *\n\t\t\t * Re-read the futex value.\n\t\t\t */\n\t\t\tif (get_futex_value_locked(&curval, uaddr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\t/*\n\t\t\t * If the owner died or we have a stale\n\t\t\t * WAITERS bit the owner TID in the user space\n\t\t\t * futex is 0.\n\t\t\t */\n\t\t\tif (!(curval & FUTEX_TID_MASK)) {\n\t\t\t\tforce_take = 1;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/**\n * __unqueue_futex() - Remove the futex_q from its futex_hash_bucket\n * @q:\tThe futex_q to unqueue\n *\n * The q->lock_ptr must not be NULL and must be held by the caller.\n */\nstatic void __unqueue_futex(struct futex_q *q)\n{\n\tstruct futex_hash_bucket *hb;\n\n\tif (WARN_ON_SMP(!q->lock_ptr || !spin_is_locked(q->lock_ptr))\n\t    || WARN_ON(plist_node_empty(&q->list)))\n\t\treturn;\n\n\thb = container_of(q->lock_ptr, struct futex_hash_bucket, lock);\n\tplist_del(&q->list, &hb->chain);\n\thb_waiters_dec(hb);\n}\n\n/*\n * The hash bucket lock must be held when this is called.\n * Afterwards, the futex_q must not be accessed.\n */\nstatic void wake_futex(struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\t/*\n\t * We set q->lock_ptr = NULL _before_ we wake up the task. If\n\t * a non-futex wake up happens on another CPU then the task\n\t * might exit and p would dereference a non-existing task\n\t * struct. Prevent this by holding a reference on p across the\n\t * wake up.\n\t */\n\tget_task_struct(p);\n\n\t__unqueue_futex(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as\n\t * q->lock_ptr = NULL is written, without taking any locks. A\n\t * memory barrier is required here to prevent the following\n\t * store to lock_ptr from getting ahead of the plist_del.\n\t */\n\tsmp_wmb();\n\tq->lock_ptr = NULL;\n\n\twake_up_state(p, TASK_NORMAL);\n\tput_task_struct(p);\n}\n\nstatic int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_q *this)\n{\n\tstruct task_struct *new_owner;\n\tstruct futex_pi_state *pi_state = this->pi_state;\n\tu32 uninitialized_var(curval), newval;\n\n\tif (!pi_state)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If current does not own the pi_state then the futex is\n\t * inconsistent and user space fiddled with the futex value.\n\t */\n\tif (pi_state->owner != current)\n\t\treturn -EINVAL;\n\n\traw_spin_lock(&pi_state->pi_mutex.wait_lock);\n\tnew_owner = rt_mutex_next_owner(&pi_state->pi_mutex);\n\n\t/*\n\t * It is possible that the next waiter (the one that brought\n\t * this owner to the kernel) timed out and is no longer\n\t * waiting on the lock.\n\t */\n\tif (!new_owner)\n\t\tnew_owner = this->task;\n\n\t/*\n\t * We pass it to the next owner. (The WAITERS bit is always\n\t * kept enabled while there is PI state around. We must also\n\t * preserve the owner died bit.)\n\t */\n\tif (!(uval & FUTEX_OWNER_DIED)) {\n\t\tint ret = 0;\n\n\t\tnewval = FUTEX_WAITERS | task_pid_vnr(new_owner);\n\n\t\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval))\n\t\t\tret = -EFAULT;\n\t\telse if (curval != uval)\n\t\t\tret = -EINVAL;\n\t\tif (ret) {\n\t\t\traw_spin_unlock(&pi_state->pi_mutex.wait_lock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\traw_spin_lock_irq(&pi_state->owner->pi_lock);\n\tWARN_ON(list_empty(&pi_state->list));\n\tlist_del_init(&pi_state->list);\n\traw_spin_unlock_irq(&pi_state->owner->pi_lock);\n\n\traw_spin_lock_irq(&new_owner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\tpi_state->owner = new_owner;\n\traw_spin_unlock_irq(&new_owner->pi_lock);\n\n\traw_spin_unlock(&pi_state->pi_mutex.wait_lock);\n\trt_mutex_unlock(&pi_state->pi_mutex);\n\n\treturn 0;\n}\n\nstatic int unlock_futex_pi(u32 __user *uaddr, u32 uval)\n{\n\tu32 uninitialized_var(oldval);\n\n\t/*\n\t * There is no waiter, so we unlock the futex. The owner died\n\t * bit has not to be preserved here. We are the owner:\n\t */\n\tif (cmpxchg_futex_value_locked(&oldval, uaddr, uval, 0))\n\t\treturn -EFAULT;\n\tif (oldval != uval)\n\t\treturn -EAGAIN;\n\n\treturn 0;\n}\n\n/*\n * Express the locking dependencies for lockdep:\n */\nstatic inline void\ndouble_lock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tif (hb1 <= hb2) {\n\t\tspin_lock(&hb1->lock);\n\t\tif (hb1 < hb2)\n\t\t\tspin_lock_nested(&hb2->lock, SINGLE_DEPTH_NESTING);\n\t} else { /* hb1 > hb2 */\n\t\tspin_lock(&hb2->lock);\n\t\tspin_lock_nested(&hb1->lock, SINGLE_DEPTH_NESTING);\n\t}\n}\n\nstatic inline void\ndouble_unlock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tspin_unlock(&hb1->lock);\n\tif (hb1 != hb2)\n\t\tspin_unlock(&hb2->lock);\n}\n\n/*\n * Wake up waiters matching bitset queued on this futex (uaddr).\n */\nstatic int\nfutex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\thb = hash_futex(&key);\n\n\t/* Make sure we really have tasks to wakeup */\n\tif (!hb_waiters_pending(hb))\n\t\tgoto out_put_key;\n\n\tspin_lock(&hb->lock);\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (match_futex (&this->key, &key)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check if one of the bits is set in both bitsets */\n\t\t\tif (!(this->bitset & bitset))\n\t\t\t\tcontinue;\n\n\t\t\twake_futex(this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock(&hb->lock);\nout_put_key:\n\tput_futex_key(&key);\nout:\n\treturn ret;\n}\n\n/*\n * Wake up all waiters hashed on the physical page that is mapped\n * to this virtual address:\n */\nstatic int\nfutex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,\n\t      int nr_wake, int nr_wake2, int op)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\tint ret, op_ret;\n\nretry:\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out_put_key1;\n\n\thb1 = hash_futex(&key1);\n\thb2 = hash_futex(&key2);\n\nretry_private:\n\tdouble_lock_hb(hb1, hb2);\n\top_ret = futex_atomic_op_inuser(op, uaddr2);\n\tif (unlikely(op_ret < 0)) {\n\n\t\tdouble_unlock_hb(hb1, hb2);\n\n#ifndef CONFIG_MMU\n\t\t/*\n\t\t * we don't get EFAULT from MMU faults if we don't have an MMU,\n\t\t * but we might get them from range checking\n\t\t */\n\t\tret = op_ret;\n\t\tgoto out_put_keys;\n#endif\n\n\t\tif (unlikely(op_ret != -EFAULT)) {\n\t\t\tret = op_ret;\n\t\t\tgoto out_put_keys;\n\t\t}\n\n\t\tret = fault_in_user_writeable(uaddr2);\n\t\tif (ret)\n\t\t\tgoto out_put_keys;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tput_futex_key(&key2);\n\t\tput_futex_key(&key1);\n\t\tgoto retry;\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (match_futex (&this->key, &key1)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\twake_futex(this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (op_ret > 0) {\n\t\top_ret = 0;\n\t\tplist_for_each_entry_safe(this, next, &hb2->chain, list) {\n\t\t\tif (match_futex (&this->key, &key2)) {\n\t\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t\twake_futex(this);\n\t\t\t\tif (++op_ret >= nr_wake2)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret += op_ret;\n\t}\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\nout_put_keys:\n\tput_futex_key(&key2);\nout_put_key1:\n\tput_futex_key(&key1);\nout:\n\treturn ret;\n}\n\n/**\n * requeue_futex() - Requeue a futex_q from one hb to another\n * @q:\t\tthe futex_q to requeue\n * @hb1:\tthe source hash_bucket\n * @hb2:\tthe target hash_bucket\n * @key2:\tthe new key for the requeued futex_q\n */\nstatic inline\nvoid requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,\n\t\t   struct futex_hash_bucket *hb2, union futex_key *key2)\n{\n\n\t/*\n\t * If key1 and key2 hash to the same bucket, no need to\n\t * requeue.\n\t */\n\tif (likely(&hb1->chain != &hb2->chain)) {\n\t\tplist_del(&q->list, &hb1->chain);\n\t\thb_waiters_dec(hb1);\n\t\tplist_add(&q->list, &hb2->chain);\n\t\thb_waiters_inc(hb2);\n\t\tq->lock_ptr = &hb2->lock;\n\t}\n\tget_futex_key_refs(key2);\n\tq->key = *key2;\n}\n\n/**\n * requeue_pi_wake_futex() - Wake a task that acquired the lock during requeue\n * @q:\t\tthe futex_q\n * @key:\tthe key of the requeue target futex\n * @hb:\t\tthe hash_bucket of the requeue target futex\n *\n * During futex_requeue, with requeue_pi=1, it is possible to acquire the\n * target futex if it is uncontended or via a lock steal.  Set the futex_q key\n * to the requeue target futex so the waiter can detect the wakeup on the right\n * futex, but remove it from the hb and NULL the rt_waiter so it can detect\n * atomic lock acquisition.  Set the q->lock_ptr to the requeue target hb->lock\n * to protect access to the pi_state to fixup the owner later.  Must be called\n * with both q->lock_ptr and hb->lock held.\n */\nstatic inline\nvoid requeue_pi_wake_futex(struct futex_q *q, union futex_key *key,\n\t\t\t   struct futex_hash_bucket *hb)\n{\n\tget_futex_key_refs(key);\n\tq->key = *key;\n\n\t__unqueue_futex(q);\n\n\tWARN_ON(!q->rt_waiter);\n\tq->rt_waiter = NULL;\n\n\tq->lock_ptr = &hb->lock;\n\n\twake_up_state(q->task, TASK_NORMAL);\n}\n\n/**\n * futex_proxy_trylock_atomic() - Attempt an atomic lock for the top waiter\n * @pifutex:\t\tthe user address of the to futex\n * @hb1:\t\tthe from futex hash bucket, must be locked by the caller\n * @hb2:\t\tthe to futex hash bucket, must be locked by the caller\n * @key1:\t\tthe from futex key\n * @key2:\t\tthe to futex key\n * @ps:\t\t\taddress to store the pi_state pointer\n * @set_waiters:\tforce setting the FUTEX_WAITERS bit (1) or not (0)\n *\n * Try and get the lock on behalf of the top waiter if we can do it atomically.\n * Wake the top waiter if we succeed.  If the caller specified set_waiters,\n * then direct futex_lock_pi_atomic() to force setting the FUTEX_WAITERS bit.\n * hb1 and hb2 must be held by the caller.\n *\n * Return:\n *  0 - failed to acquire the lock atomically;\n * >0 - acquired the lock, return value is vpid of the top_waiter\n * <0 - error\n */\nstatic int futex_proxy_trylock_atomic(u32 __user *pifutex,\n\t\t\t\t struct futex_hash_bucket *hb1,\n\t\t\t\t struct futex_hash_bucket *hb2,\n\t\t\t\t union futex_key *key1, union futex_key *key2,\n\t\t\t\t struct futex_pi_state **ps, int set_waiters)\n{\n\tstruct futex_q *top_waiter = NULL;\n\tu32 curval;\n\tint ret, vpid;\n\n\tif (get_futex_value_locked(&curval, pifutex))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Find the top_waiter and determine if there are additional waiters.\n\t * If the caller intends to requeue more than 1 waiter to pifutex,\n\t * force futex_lock_pi_atomic() to set the FUTEX_WAITERS bit now,\n\t * as we have means to handle the possible fault.  If not, don't set\n\t * the bit unecessarily as it will force the subsequent unlock to enter\n\t * the kernel.\n\t */\n\ttop_waiter = futex_top_waiter(hb1, key1);\n\n\t/* There are no waiters, nothing for us to do. */\n\tif (!top_waiter)\n\t\treturn 0;\n\n\t/* Ensure we requeue to the expected futex. */\n\tif (!match_futex(top_waiter->requeue_pi_key, key2))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Try to take the lock for top_waiter.  Set the FUTEX_WAITERS bit in\n\t * the contended case or if set_waiters is 1.  The pi_state is returned\n\t * in ps in contended cases.\n\t */\n\tvpid = task_pid_vnr(top_waiter->task);\n\tret = futex_lock_pi_atomic(pifutex, hb2, key2, ps, top_waiter->task,\n\t\t\t\t   set_waiters);\n\tif (ret == 1) {\n\t\trequeue_pi_wake_futex(top_waiter, key2, hb2);\n\t\treturn vpid;\n\t}\n\treturn ret;\n}\n\n/**\n * futex_requeue() - Requeue waiters from uaddr1 to uaddr2\n * @uaddr1:\tsource futex user address\n * @flags:\tfutex flags (FLAGS_SHARED, etc.)\n * @uaddr2:\ttarget futex user address\n * @nr_wake:\tnumber of waiters to wake (must be 1 for requeue_pi)\n * @nr_requeue:\tnumber of waiters to requeue (0-INT_MAX)\n * @cmpval:\t@uaddr1 expected value (or %NULL)\n * @requeue_pi:\tif we are attempting to requeue from a non-pi futex to a\n *\t\tpi futex (pi to pi requeue is not supported)\n *\n * Requeue waiters on uaddr1 to uaddr2. In the requeue_pi case, try to acquire\n * uaddr2 atomically on behalf of the top waiter.\n *\n * Return:\n * >=0 - on success, the number of tasks requeued or woken;\n *  <0 - on error\n */\nstatic int futex_requeue(u32 __user *uaddr1, unsigned int flags,\n\t\t\t u32 __user *uaddr2, int nr_wake, int nr_requeue,\n\t\t\t u32 *cmpval, int requeue_pi)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tint drop_count = 0, task_count = 0, ret;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\n\tif (requeue_pi) {\n\t\t/*\n\t\t * Requeue PI only works on two distinct uaddrs. This\n\t\t * check is only valid for private futexes. See below.\n\t\t */\n\t\tif (uaddr1 == uaddr2)\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * requeue_pi requires a pi_state, try to allocate it now\n\t\t * without any locks in case it fails.\n\t\t */\n\t\tif (refill_pi_state_cache())\n\t\t\treturn -ENOMEM;\n\t\t/*\n\t\t * requeue_pi must wake as many tasks as it can, up to nr_wake\n\t\t * + nr_requeue, since it acquires the rt_mutex prior to\n\t\t * returning to userspace, so as to not leave the rt_mutex with\n\t\t * waiters and no owner.  However, second and third wake-ups\n\t\t * cannot be predicted as they involve race conditions with the\n\t\t * first wake and a fault while looking up the pi_state.  Both\n\t\t * pthread_cond_signal() and pthread_cond_broadcast() should\n\t\t * use nr_wake=1.\n\t\t */\n\t\tif (nr_wake != 1)\n\t\t\treturn -EINVAL;\n\t}\n\nretry:\n\tif (pi_state != NULL) {\n\t\t/*\n\t\t * We will have to lookup the pi_state again, so free this one\n\t\t * to keep the accounting correct.\n\t\t */\n\t\tfree_pi_state(pi_state);\n\t\tpi_state = NULL;\n\t}\n\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2,\n\t\t\t    requeue_pi ? VERIFY_WRITE : VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out_put_key1;\n\n\t/*\n\t * The check above which compares uaddrs is not sufficient for\n\t * shared futexes. We need to compare the keys:\n\t */\n\tif (requeue_pi && match_futex(&key1, &key2)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_keys;\n\t}\n\n\thb1 = hash_futex(&key1);\n\thb2 = hash_futex(&key2);\n\nretry_private:\n\thb_waiters_inc(hb2);\n\tdouble_lock_hb(hb1, hb2);\n\n\tif (likely(cmpval != NULL)) {\n\t\tu32 curval;\n\n\t\tret = get_futex_value_locked(&curval, uaddr1);\n\n\t\tif (unlikely(ret)) {\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\n\t\t\tret = get_user(curval, uaddr1);\n\t\t\tif (ret)\n\t\t\t\tgoto out_put_keys;\n\n\t\t\tif (!(flags & FLAGS_SHARED))\n\t\t\t\tgoto retry_private;\n\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tgoto retry;\n\t\t}\n\t\tif (curval != *cmpval) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (requeue_pi && (task_count - nr_wake < nr_requeue)) {\n\t\t/*\n\t\t * Attempt to acquire uaddr2 and wake the top waiter. If we\n\t\t * intend to requeue waiters, force setting the FUTEX_WAITERS\n\t\t * bit.  We force this here where we are able to easily handle\n\t\t * faults rather in the requeue loop below.\n\t\t */\n\t\tret = futex_proxy_trylock_atomic(uaddr2, hb1, hb2, &key1,\n\t\t\t\t\t\t &key2, &pi_state, nr_requeue);\n\n\t\t/*\n\t\t * At this point the top_waiter has either taken uaddr2 or is\n\t\t * waiting on it.  If the former, then the pi_state will not\n\t\t * exist yet, look it up one more time to ensure we have a\n\t\t * reference to it. If the lock was taken, ret contains the\n\t\t * vpid of the top waiter task.\n\t\t */\n\t\tif (ret > 0) {\n\t\t\tWARN_ON(pi_state);\n\t\t\tdrop_count++;\n\t\t\ttask_count++;\n\t\t\t/*\n\t\t\t * If we acquired the lock, then the user\n\t\t\t * space value of uaddr2 should be vpid. It\n\t\t\t * cannot be changed by the top waiter as it\n\t\t\t * is blocked on hb2 lock if it tries to do\n\t\t\t * so. If something fiddled with it behind our\n\t\t\t * back the pi state lookup might unearth\n\t\t\t * it. So we rather use the known value than\n\t\t\t * rereading and handing potential crap to\n\t\t\t * lookup_pi_state.\n\t\t\t */\n\t\t\tret = lookup_pi_state(ret, hb2, &key2, &pi_state, NULL);\n\t\t}\n\n\t\tswitch (ret) {\n\t\tcase 0:\n\t\t\tbreak;\n\t\tcase -EFAULT:\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tret = fault_in_user_writeable(uaddr2);\n\t\t\tif (!ret)\n\t\t\t\tgoto retry;\n\t\t\tgoto out;\n\t\tcase -EAGAIN:\n\t\t\t/* The owner was exiting, try again. */\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (task_count - nr_wake >= nr_requeue)\n\t\t\tbreak;\n\n\t\tif (!match_futex(&this->key, &key1))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * FUTEX_WAIT_REQEUE_PI and FUTEX_CMP_REQUEUE_PI should always\n\t\t * be paired with each other and no other futex ops.\n\t\t *\n\t\t * We should never be requeueing a futex_q with a pi_state,\n\t\t * which is awaiting a futex_unlock_pi().\n\t\t */\n\t\tif ((requeue_pi && !this->rt_waiter) ||\n\t\t    (!requeue_pi && this->rt_waiter) ||\n\t\t    this->pi_state) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Wake nr_wake waiters.  For requeue_pi, if we acquired the\n\t\t * lock, we already woke the top_waiter.  If not, it will be\n\t\t * woken by futex_unlock_pi().\n\t\t */\n\t\tif (++task_count <= nr_wake && !requeue_pi) {\n\t\t\twake_futex(this);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Ensure we requeue to the expected futex for requeue_pi. */\n\t\tif (requeue_pi && !match_futex(this->requeue_pi_key, &key2)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Requeue nr_requeue waiters and possibly one more in the case\n\t\t * of requeue_pi if we couldn't acquire the lock atomically.\n\t\t */\n\t\tif (requeue_pi) {\n\t\t\t/* Prepare the waiter to take the rt_mutex. */\n\t\t\tatomic_inc(&pi_state->refcount);\n\t\t\tthis->pi_state = pi_state;\n\t\t\tret = rt_mutex_start_proxy_lock(&pi_state->pi_mutex,\n\t\t\t\t\t\t\tthis->rt_waiter,\n\t\t\t\t\t\t\tthis->task, 1);\n\t\t\tif (ret == 1) {\n\t\t\t\t/* We got the lock. */\n\t\t\t\trequeue_pi_wake_futex(this, &key2, hb2);\n\t\t\t\tdrop_count++;\n\t\t\t\tcontinue;\n\t\t\t} else if (ret) {\n\t\t\t\t/* -EDEADLK */\n\t\t\t\tthis->pi_state = NULL;\n\t\t\t\tfree_pi_state(pi_state);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t\trequeue_futex(this, hb1, hb2, &key2);\n\t\tdrop_count++;\n\t}\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\n\thb_waiters_dec(hb2);\n\n\t/*\n\t * drop_futex_key_refs() must be called outside the spinlocks. During\n\t * the requeue we moved futex_q's from the hash bucket at key1 to the\n\t * one at key2 and updated their key pointer.  We no longer need to\n\t * hold the references to key1.\n\t */\n\twhile (--drop_count >= 0)\n\t\tdrop_futex_key_refs(&key1);\n\nout_put_keys:\n\tput_futex_key(&key2);\nout_put_key1:\n\tput_futex_key(&key1);\nout:\n\tif (pi_state != NULL)\n\t\tfree_pi_state(pi_state);\n\treturn ret ? ret : task_count;\n}\n\n/* The key must be already stored in q->key. */\nstatic inline struct futex_hash_bucket *queue_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = hash_futex(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all queue_lock()\n\t * users end up calling queue_me(). Similarly, for housekeeping,\n\t * decrement the counter at queue_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\thb_waiters_inc(hb);\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock); /* implies MB (A) */\n\treturn hb;\n}\n\nstatic inline void\nqueue_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\thb_waiters_dec(hb);\n}\n\n/**\n * queue_me() - Enqueue the futex_q on the futex_hash_bucket\n * @q:\tThe futex_q to enqueue\n * @hb:\tThe destination hash bucket\n *\n * The hb->lock must be held by the caller, and is released here. A call to\n * queue_me() is typically paired with exactly one call to unqueue_me().  The\n * exceptions involve the PI related operations, which may use unqueue_me_pi()\n * or nothing if the unqueue is done as part of the wake process and the unqueue\n * state is implicit in the state of woken task (see futex_wait_requeue_pi() for\n * an example).\n */\nstatic inline void queue_me(struct futex_q *q, struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n\tspin_unlock(&hb->lock);\n}\n\n/**\n * unqueue_me() - Remove the futex_q from its futex_hash_bucket\n * @q:\tThe futex_q to unqueue\n *\n * The q->lock_ptr must not be held by the caller. A call to unqueue_me() must\n * be paired with exactly one earlier call to queue_me().\n *\n * Return:\n *   1 - if the futex_q was still queued (and we removed unqueued it);\n *   0 - if the futex_q was already removed by the waking thread\n */\nstatic int unqueue_me(struct futex_q *q)\n{\n\tspinlock_t *lock_ptr;\n\tint ret = 0;\n\n\t/* In the common case we don't take the spinlock, which is nice. */\nretry:\n\tlock_ptr = q->lock_ptr;\n\tbarrier();\n\tif (lock_ptr != NULL) {\n\t\tspin_lock(lock_ptr);\n\t\t/*\n\t\t * q->lock_ptr can change between reading it and\n\t\t * spin_lock(), causing us to take the wrong lock.  This\n\t\t * corrects the race condition.\n\t\t *\n\t\t * Reasoning goes like this: if we have the wrong lock,\n\t\t * q->lock_ptr must have changed (maybe several times)\n\t\t * between reading it and the spin_lock().  It can\n\t\t * change again after the spin_lock() but only if it was\n\t\t * already changed before the spin_lock().  It cannot,\n\t\t * however, change back to the original value.  Therefore\n\t\t * we can detect whether we acquired the correct lock.\n\t\t */\n\t\tif (unlikely(lock_ptr != q->lock_ptr)) {\n\t\t\tspin_unlock(lock_ptr);\n\t\t\tgoto retry;\n\t\t}\n\t\t__unqueue_futex(q);\n\n\t\tBUG_ON(q->pi_state);\n\n\t\tspin_unlock(lock_ptr);\n\t\tret = 1;\n\t}\n\n\tdrop_futex_key_refs(&q->key);\n\treturn ret;\n}\n\n/*\n * PI futexes can not be requeued and must remove themself from the\n * hash bucket. The hash bucket lock (i.e. lock_ptr) is held on entry\n * and dropped here.\n */\nstatic void unqueue_me_pi(struct futex_q *q)\n\t__releases(q->lock_ptr)\n{\n\t__unqueue_futex(q);\n\n\tBUG_ON(!q->pi_state);\n\tfree_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n\n\tspin_unlock(q->lock_ptr);\n}\n\n/*\n * Fixup the pi_state owner with the new owner.\n *\n * Must be called with hash bucket lock held and mm->sem held for non\n * private futexes.\n */\nstatic int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *newowner)\n{\n\tu32 newtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tstruct task_struct *oldowner = pi_state->owner;\n\tu32 uval, uninitialized_var(curval), newval;\n\tint ret;\n\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\t/*\n\t * We are here either because we stole the rtmutex from the\n\t * previous highest priority waiter or we are the highest priority\n\t * waiter but failed to get the rtmutex the first time.\n\t * We have to replace the newowner TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would\n\t * leave the pi_state in an inconsistent state when we fault\n\t * here, because we need to drop the hash bucket lock to\n\t * handle the fault. This might be observed in the PID check\n\t * in lookup_pi_state.\n\t */\nretry:\n\tif (get_futex_value_locked(&uval, uaddr))\n\t\tgoto handle_fault;\n\n\twhile (1) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval))\n\t\t\tgoto handle_fault;\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tif (pi_state->owner != NULL) {\n\t\traw_spin_lock_irq(&pi_state->owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock_irq(&pi_state->owner->pi_lock);\n\t}\n\n\tpi_state->owner = newowner;\n\n\traw_spin_lock_irq(&newowner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &newowner->pi_state_list);\n\traw_spin_unlock_irq(&newowner->pi_lock);\n\treturn 0;\n\n\t/*\n\t * To handle the page fault we need to drop the hash bucket\n\t * lock here. That gives the other task (either the highest priority\n\t * waiter itself or the task which stole the rtmutex) the\n\t * chance to try the fixup of the pi_state. So once we are\n\t * back from handling the fault we need to check the pi_state\n\t * after reacquiring the hash bucket lock and before trying to\n\t * do another fixup. When the fixup has been done already we\n\t * simply return.\n\t */\nhandle_fault:\n\tspin_unlock(q->lock_ptr);\n\n\tret = fault_in_user_writeable(uaddr);\n\n\tspin_lock(q->lock_ptr);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner)\n\t\treturn 0;\n\n\tif (ret)\n\t\treturn ret;\n\n\tgoto retry;\n}\n\nstatic long futex_wait_restart(struct restart_block *restart);\n\n/**\n * fixup_owner() - Post lock pi_state and corner case management\n * @uaddr:\tuser address of the futex\n * @q:\t\tfutex_q (contains pi_state and access to the rt_mutex)\n * @locked:\tif the attempt to take the rt_mutex succeeded (1) or not (0)\n *\n * After attempting to lock an rt_mutex, this function is called to cleanup\n * the pi_state owner as well as handle race conditions that may allow us to\n * acquire the lock. Must be called with the hb lock held.\n *\n * Return:\n *  1 - success, lock taken;\n *  0 - success, lock not taken;\n * <0 - on error (-EFAULT)\n */\nstatic int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Catch the rare case, where the lock was released when we were on the\n\t * way back before we locked the hash bucket.\n\t */\n\tif (q->pi_state->owner == current) {\n\t\t/*\n\t\t * Try to get the rt_mutex now. This might fail as some other\n\t\t * task acquired the rt_mutex after we removed ourself from the\n\t\t * rt_mutex waiters list.\n\t\t */\n\t\tif (rt_mutex_trylock(&q->pi_state->pi_mutex)) {\n\t\t\tlocked = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * pi_state is incorrect, some other task did a lock steal and\n\t\t * we returned due to timeout or signal without taking the\n\t\t * rt_mutex. Too late.\n\t\t */\n\t\traw_spin_lock(&q->pi_state->pi_mutex.wait_lock);\n\t\towner = rt_mutex_owner(&q->pi_state->pi_mutex);\n\t\tif (!owner)\n\t\t\towner = rt_mutex_next_owner(&q->pi_state->pi_mutex);\n\t\traw_spin_unlock(&q->pi_state->pi_mutex.wait_lock);\n\t\tret = fixup_pi_state_owner(uaddr, q, owner);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current)\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\nout:\n\treturn ret ? ret : locked;\n}\n\n/**\n * futex_wait_queue_me() - queue_me() and wait for wakeup, timeout, or signal\n * @hb:\t\tthe futex hash bucket, must be locked by the caller\n * @q:\t\tthe futex_q to queue up on\n * @timeout:\tthe prepared hrtimer_sleeper, or null for no timeout\n */\nstatic void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,\n\t\t\t\tstruct hrtimer_sleeper *timeout)\n{\n\t/*\n\t * The task state is guaranteed to be set before another task can\n\t * wake it. set_current_state() is implemented using set_mb() and\n\t * queue_me() calls spin_unlock() upon completion, both serializing\n\t * access to the hash list and forcing another memory barrier.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tqueue_me(q, hb);\n\n\t/* Arm the timer */\n\tif (timeout) {\n\t\thrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);\n\t\tif (!hrtimer_active(&timeout->timer))\n\t\t\ttimeout->task = NULL;\n\t}\n\n\t/*\n\t * If we have been removed from the hash list, then another task\n\t * has tried to wake us, and we can skip the call to schedule().\n\t */\n\tif (likely(!plist_node_empty(&q->list))) {\n\t\t/*\n\t\t * If the timer has already expired, current will already be\n\t\t * flagged for rescheduling. Only call schedule if there\n\t\t * is no timeout, or if it has yet to expire.\n\t\t */\n\t\tif (!timeout || timeout->task)\n\t\t\tfreezable_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}\n\n/**\n * futex_wait_setup() - Prepare to wait on a futex\n * @uaddr:\tthe futex userspace address\n * @val:\tthe expected value\n * @flags:\tfutex flags (FLAGS_SHARED, etc.)\n * @q:\t\tthe associated futex_q\n * @hb:\t\tstorage for hash_bucket pointer to be returned to caller\n *\n * Setup the futex_q and locate the hash_bucket.  Get the futex value and\n * compare it with the expected value.  Handle atomic faults internally.\n * Return with the hb lock held and a q.key reference on success, and unlocked\n * with no q.key reference on failure.\n *\n * Return:\n *  0 - uaddr contains val and hb has been locked;\n * <1 - -EFAULT or -EWOULDBLOCK (uaddr does not contain val) and hb is unlocked\n */\nstatic int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,\n\t\t\t   struct futex_q *q, struct futex_hash_bucket **hb)\n{\n\tu32 uval;\n\tint ret;\n\n\t/*\n\t * Access the page AFTER the hash-bucket is locked.\n\t * Order is important:\n\t *\n\t *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);\n\t *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }\n\t *\n\t * The basic logical guarantee of a futex is that it blocks ONLY\n\t * if cond(var) is known to be true at the time of blocking, for\n\t * any cond.  If we locked the hash-bucket after testing *uaddr, that\n\t * would open a race condition where we could block indefinitely with\n\t * cond(var) false, which would violate the guarantee.\n\t *\n\t * On the other hand, we insert q and release the hash-bucket only\n\t * after testing *uaddr.  This guarantees that futex_wait() will NOT\n\t * absorb a wakeup if *uaddr does not match the desired values\n\t * while the syscall executes.\n\t */\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\nretry_private:\n\t*hb = queue_lock(q);\n\n\tret = get_futex_value_locked(&uval, uaddr);\n\n\tif (ret) {\n\t\tqueue_unlock(*hb);\n\n\t\tret = get_user(uval, uaddr);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tput_futex_key(&q->key);\n\t\tgoto retry;\n\t}\n\n\tif (uval != val) {\n\t\tqueue_unlock(*hb);\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout:\n\tif (ret)\n\t\tput_futex_key(&q->key);\n\treturn ret;\n}\n\nstatic int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,\n\t\t      ktime_t *abs_time, u32 bitset)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, holds hb lock and increments\n\t * q.key refs.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* queue_me and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\t/* unqueue_me() drops q.key ref */\n\tif (!unqueue_me(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current_thread_info()->restart_block;\n\trestart->fn = futex_wait_restart;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = abs_time->tv64;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = -ERESTART_RESTARTBLOCK;\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}\n\n\nstatic long futex_wait_restart(struct restart_block *restart)\n{\n\tu32 __user *uaddr = restart->futex.uaddr;\n\tktime_t t, *tp = NULL;\n\n\tif (restart->futex.flags & FLAGS_HAS_TIMEOUT) {\n\t\tt.tv64 = restart->futex.time;\n\t\ttp = &t;\n\t}\n\trestart->fn = do_no_restart_syscall;\n\n\treturn (long)futex_wait(uaddr, restart->futex.flags,\n\t\t\t\trestart->futex.val, tp, restart->futex.bitset);\n}\n\n\n/*\n * Userspace tried a 0 -> TID atomic transition of the futex value\n * and failed. The kernel side here does the whole locking operation:\n * if there are waiters then it will block, it does PI, etc. (Due to\n * races the kernel might see a 0 value of the futex too.)\n */\nstatic int futex_lock_pi(u32 __user *uaddr, unsigned int flags, int detect,\n\t\t\t ktime_t *time, int trylock)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (refill_pi_state_cache())\n\t\treturn -ENOMEM;\n\n\tif (time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, CLOCK_REALTIME,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires(&to->timer, *time);\n\t}\n\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\nretry_private:\n\thb = queue_lock(&q);\n\n\tret = futex_lock_pi_atomic(uaddr, hb, &q.key, &q.pi_state, current, 0);\n\tif (unlikely(ret)) {\n\t\tswitch (ret) {\n\t\tcase 1:\n\t\t\t/* We got the lock. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_put_key;\n\t\tcase -EFAULT:\n\t\t\tgoto uaddr_faulted;\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * Task is exiting and we just wait for the\n\t\t\t * exit to complete.\n\t\t\t */\n\t\t\tqueue_unlock(hb);\n\t\t\tput_futex_key(&q.key);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock_put_key;\n\t\t}\n\t}\n\n\t/*\n\t * Only actually queue now that the atomic ops are done:\n\t */\n\tqueue_me(&q, hb);\n\n\tWARN_ON(!q.pi_state);\n\t/*\n\t * Block on the PI mutex:\n\t */\n\tif (!trylock)\n\t\tret = rt_mutex_timed_lock(&q.pi_state->pi_mutex, to, 1);\n\telse {\n\t\tret = rt_mutex_trylock(&q.pi_state->pi_mutex);\n\t\t/* Fixup the trylock return value: */\n\t\tret = ret ? 0 : -EWOULDBLOCK;\n\t}\n\n\tspin_lock(q.lock_ptr);\n\t/*\n\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t * haven't already.\n\t */\n\tres = fixup_owner(uaddr, &q, !ret);\n\t/*\n\t * If fixup_owner() returned an error, proprogate that.  If it acquired\n\t * the lock, clear our -ETIMEDOUT or -EINTR.\n\t */\n\tif (res)\n\t\tret = (res < 0) ? res : 0;\n\n\t/*\n\t * If fixup_owner() faulted and was unable to handle the fault, unlock\n\t * it and return the fault to userspace.\n\t */\n\tif (ret && (rt_mutex_owner(&q.pi_state->pi_mutex) == current))\n\t\trt_mutex_unlock(&q.pi_state->pi_mutex);\n\n\t/* Unqueue and drop the lock */\n\tunqueue_me_pi(&q);\n\n\tgoto out_put_key;\n\nout_unlock_put_key:\n\tqueue_unlock(hb);\n\nout_put_key:\n\tput_futex_key(&q.key);\nout:\n\tif (to)\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\treturn ret != -EINTR ? ret : -ERESTARTNOINTR;\n\nuaddr_faulted:\n\tqueue_unlock(hb);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (ret)\n\t\tgoto out_put_key;\n\n\tif (!(flags & FLAGS_SHARED))\n\t\tgoto retry_private;\n\n\tput_futex_key(&q.key);\n\tgoto retry;\n}\n\n/*\n * Userspace attempted a TID -> 0 atomic transition, and failed.\n * This is the in-kernel slowpath: we look up the PI state (if any),\n * and do the rt-mutex unlock.\n */\nstatic int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tu32 uval, vpid = task_pid_vnr(current);\n\tint ret;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -EFAULT;\n\t/*\n\t * We release only a lock we actually own:\n\t */\n\tif ((uval & FUTEX_TID_MASK) != vpid)\n\t\treturn -EPERM;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\thb = hash_futex(&key);\n\tspin_lock(&hb->lock);\n\n\t/*\n\t * To avoid races, try to do the TID -> 0 atomic transition\n\t * again. If it succeeds then we can return without waking\n\t * anyone else up:\n\t */\n\tif (!(uval & FUTEX_OWNER_DIED) &&\n\t    cmpxchg_futex_value_locked(&uval, uaddr, vpid, 0))\n\t\tgoto pi_faulted;\n\t/*\n\t * Rare case: we managed to release the lock atomically,\n\t * no need to wake anyone else up:\n\t */\n\tif (unlikely(uval == vpid))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Ok, other tasks may need to be woken up - check waiters\n\t * and do the wakeup if necessary:\n\t */\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (!match_futex (&this->key, &key))\n\t\t\tcontinue;\n\t\tret = wake_futex_pi(uaddr, uval, this);\n\t\t/*\n\t\t * The atomic access to the futex value\n\t\t * generated a pagefault, so retry the\n\t\t * user-access and the wakeup:\n\t\t */\n\t\tif (ret == -EFAULT)\n\t\t\tgoto pi_faulted;\n\t\tgoto out_unlock;\n\t}\n\t/*\n\t * No waiters - kernel unlocks the futex:\n\t */\n\tif (!(uval & FUTEX_OWNER_DIED)) {\n\t\tret = unlock_futex_pi(uaddr, uval);\n\t\tif (ret == -EFAULT)\n\t\t\tgoto pi_faulted;\n\t}\n\nout_unlock:\n\tspin_unlock(&hb->lock);\n\tput_futex_key(&key);\n\nout:\n\treturn ret;\n\npi_faulted:\n\tspin_unlock(&hb->lock);\n\tput_futex_key(&key);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (!ret)\n\t\tgoto retry;\n\n\treturn ret;\n}\n\n/**\n * handle_early_requeue_pi_wakeup() - Detect early wakeup on the initial futex\n * @hb:\t\tthe hash_bucket futex_q was original enqueued on\n * @q:\t\tthe futex_q woken while waiting to be requeued\n * @key2:\tthe futex_key of the requeue target futex\n * @timeout:\tthe timeout associated with the wait (NULL if none)\n *\n * Detect if the task was woken on the initial futex as opposed to the requeue\n * target futex.  If so, determine if it was a timeout or a signal that caused\n * the wakeup and return the appropriate error code to the caller.  Must be\n * called with the hb lock held.\n *\n * Return:\n *  0 = no early wakeup detected;\n * <0 = -ETIMEDOUT or -ERESTARTNOINTR\n */\nstatic inline\nint handle_early_requeue_pi_wakeup(struct futex_hash_bucket *hb,\n\t\t\t\t   struct futex_q *q, union futex_key *key2,\n\t\t\t\t   struct hrtimer_sleeper *timeout)\n{\n\tint ret = 0;\n\n\t/*\n\t * With the hb lock held, we avoid races while we process the wakeup.\n\t * We only need to hold hb (and not hb2) to ensure atomicity as the\n\t * wakeup code can't change q.key from uaddr to uaddr2 if we hold hb.\n\t * It can't be requeued from uaddr2 to something else since we don't\n\t * support a PI aware source futex for requeue.\n\t */\n\tif (!match_futex(&q->key, key2)) {\n\t\tWARN_ON(q->lock_ptr && (&hb->lock != q->lock_ptr));\n\t\t/*\n\t\t * We were woken prior to requeue by a timeout or a signal.\n\t\t * Unqueue the futex_q and determine which it was.\n\t\t */\n\t\tplist_del(&q->list, &hb->chain);\n\t\thb_waiters_dec(hb);\n\n\t\t/* Handle spurious wakeups gracefully */\n\t\tret = -EWOULDBLOCK;\n\t\tif (timeout && !timeout->task)\n\t\t\tret = -ETIMEDOUT;\n\t\telse if (signal_pending(current))\n\t\t\tret = -ERESTARTNOINTR;\n\t}\n\treturn ret;\n}\n\n/**\n * futex_wait_requeue_pi() - Wait on uaddr and take uaddr2\n * @uaddr:\tthe futex we initially wait on (non-pi)\n * @flags:\tfutex flags (FLAGS_SHARED, FLAGS_CLOCKRT, etc.), they must be\n * \t\tthe same type, no requeueing from private to shared, etc.\n * @val:\tthe expected value of uaddr\n * @abs_time:\tabsolute timeout\n * @bitset:\t32 bit wakeup bitset set by userspace, defaults to all\n * @uaddr2:\tthe pi futex we will take prior to returning to user-space\n *\n * The caller will wait on uaddr and will be requeued by futex_requeue() to\n * uaddr2 which must be PI aware and unique from uaddr.  Normal wakeup will wake\n * on uaddr2 and complete the acquisition of the rt_mutex prior to returning to\n * userspace.  This ensures the rt_mutex maintains an owner when it has waiters;\n * without one, the pi logic would not know which task to boost/deboost, if\n * there was a need to.\n *\n * We call schedule in futex_wait_queue_me() when we enqueue and return there\n * via the following--\n * 1) wakeup on uaddr2 after an atomic lock acquisition by futex_requeue()\n * 2) wakeup on uaddr2 after a requeue\n * 3) signal\n * 4) timeout\n *\n * If 3, cleanup and return -ERESTARTNOINTR.\n *\n * If 2, we may then block on trying to take the rt_mutex and return via:\n * 5) successful lock\n * 6) signal\n * 7) timeout\n * 8) other lock acquisition failure\n *\n * If 6, return -EWOULDBLOCK (restarting the syscall would do the same).\n *\n * If 4 or 7, we cleanup and return with -ETIMEDOUT.\n *\n * Return:\n *  0 - On success;\n * <0 - On error\n */\nstatic int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t\t u32 val, ktime_t *abs_time, u32 bitset,\n\t\t\t\t u32 __user *uaddr2)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct rt_mutex *pi_mutex = NULL;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key2 = FUTEX_KEY_INIT;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (uaddr == uaddr2)\n\t\treturn -EINVAL;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\n\t/*\n\t * The waiter is allocated on our stack, manipulated by the requeue\n\t * code while we sleep on uaddr.\n\t */\n\tdebug_rt_mutex_init_waiter(&rt_waiter);\n\tRB_CLEAR_NODE(&rt_waiter.pi_tree_entry);\n\tRB_CLEAR_NODE(&rt_waiter.tree_entry);\n\trt_waiter.task = NULL;\n\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tq.bitset = bitset;\n\tq.rt_waiter = &rt_waiter;\n\tq.requeue_pi_key = &key2;\n\n\t/*\n\t * Prepare to wait on uaddr. On success, increments q.key (key1) ref\n\t * count.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out_key2;\n\n\t/*\n\t * The check above which compares uaddrs is not sufficient for\n\t * shared futexes. We need to compare the keys:\n\t */\n\tif (match_futex(&q.key, &key2)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_keys;\n\t}\n\n\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\tspin_lock(&hb->lock);\n\tret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);\n\tspin_unlock(&hb->lock);\n\tif (ret)\n\t\tgoto out_put_keys;\n\n\t/*\n\t * In order for us to be here, we know our q.key == key2, and since\n\t * we took the hb->lock above, we also know that futex_requeue() has\n\t * completed and we no longer have to concern ourselves with a wakeup\n\t * race with the atomic proxy lock acquisition by the requeue code. The\n\t * futex_requeue dropped our key1 reference and incremented our key2\n\t * reference count.\n\t */\n\n\t/* Check if the requeue code acquired the second futex for us. */\n\tif (!q.rt_waiter) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case.\n\t\t */\n\t\tif (q.pi_state && (q.pi_state->owner != current)) {\n\t\t\tspin_lock(q.lock_ptr);\n\t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n\t\t\tspin_unlock(q.lock_ptr);\n\t\t}\n\t} else {\n\t\t/*\n\t\t * We have been woken up by futex_unlock_pi(), a timeout, or a\n\t\t * signal.  futex_unlock_pi() will not destroy the lock_ptr nor\n\t\t * the pi_state.\n\t\t */\n\t\tWARN_ON(!q.pi_state);\n\t\tpi_mutex = &q.pi_state->pi_mutex;\n\t\tret = rt_mutex_finish_proxy_lock(pi_mutex, to, &rt_waiter, 1);\n\t\tdebug_rt_mutex_free_waiter(&rt_waiter);\n\n\t\tspin_lock(q.lock_ptr);\n\t\t/*\n\t\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t\t * haven't already.\n\t\t */\n\t\tres = fixup_owner(uaddr2, &q, !ret);\n\t\t/*\n\t\t * If fixup_owner() returned an error, proprogate that.  If it\n\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.\n\t\t */\n\t\tif (res)\n\t\t\tret = (res < 0) ? res : 0;\n\n\t\t/* Unqueue and drop the lock. */\n\t\tunqueue_me_pi(&q);\n\t}\n\n\t/*\n\t * If fixup_pi_state_owner() faulted and was unable to handle the\n\t * fault, unlock the rt_mutex and return the fault to userspace.\n\t */\n\tif (ret == -EFAULT) {\n\t\tif (pi_mutex && rt_mutex_owner(pi_mutex) == current)\n\t\t\trt_mutex_unlock(pi_mutex);\n\t} else if (ret == -EINTR) {\n\t\t/*\n\t\t * We've already been requeued, but cannot restart by calling\n\t\t * futex_lock_pi() directly. We could restart this syscall, but\n\t\t * it would detect that the user space \"val\" changed and return\n\t\t * -EWOULDBLOCK.  Save the overhead of the restart and return\n\t\t * -EWOULDBLOCK directly.\n\t\t */\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout_put_keys:\n\tput_futex_key(&q.key);\nout_key2:\n\tput_futex_key(&key2);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}\n\n/*\n * Support for robust futexes: the kernel cleans up held futexes at\n * thread exit time.\n *\n * Implementation: user-space maintains a per-thread list of locks it\n * is holding. Upon do_exit(), the kernel carefully walks this list,\n * and marks all locks that are owned by this thread with the\n * FUTEX_OWNER_DIED bit, and wakes up a waiter (if any). The list is\n * always manipulated with the lock held, so the list is private and\n * per-thread. Userspace also maintains a per-thread 'list_op_pending'\n * field, to allow the kernel to clean up if the thread dies after\n * acquiring the lock, but just before it could have added itself to\n * the list. There can only be one such pending lock.\n */\n\n/**\n * sys_set_robust_list() - Set the robust-futex list head of a task\n * @head:\tpointer to the list-head\n * @len:\tlength of the list-head, as userspace expects\n */\nSYSCALL_DEFINE2(set_robust_list, struct robust_list_head __user *, head,\n\t\tsize_t, len)\n{\n\tif (!futex_cmpxchg_enabled)\n\t\treturn -ENOSYS;\n\t/*\n\t * The kernel knows only one size for now:\n\t */\n\tif (unlikely(len != sizeof(*head)))\n\t\treturn -EINVAL;\n\n\tcurrent->robust_list = head;\n\n\treturn 0;\n}\n\n/**\n * sys_get_robust_list() - Get the robust-futex list head of a task\n * @pid:\tpid of the process [zero for current task]\n * @head_ptr:\tpointer to a list-head pointer, the kernel fills it in\n * @len_ptr:\tpointer to a length field, the kernel fills in the header size\n */\nSYSCALL_DEFINE3(get_robust_list, int, pid,\n\t\tstruct robust_list_head __user * __user *, head_ptr,\n\t\tsize_t __user *, len_ptr)\n{\n\tstruct robust_list_head __user *head;\n\tunsigned long ret;\n\tstruct task_struct *p;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn -ENOSYS;\n\n\trcu_read_lock();\n\n\tret = -ESRCH;\n\tif (!pid)\n\t\tp = current;\n\telse {\n\t\tp = find_task_by_vpid(pid);\n\t\tif (!p)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = -EPERM;\n\tif (!ptrace_may_access(p, PTRACE_MODE_READ))\n\t\tgoto err_unlock;\n\n\thead = p->robust_list;\n\trcu_read_unlock();\n\n\tif (put_user(sizeof(*head), len_ptr))\n\t\treturn -EFAULT;\n\treturn put_user(head, head_ptr);\n\nerr_unlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Process a futex-list entry, check whether it's owned by the\n * dying task, and do notification if so:\n */\nint handle_futex_death(u32 __user *uaddr, struct task_struct *curr, int pi)\n{\n\tu32 uval, uninitialized_var(nval), mval;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -1;\n\n\tif ((uval & FUTEX_TID_MASK) == task_pid_vnr(curr)) {\n\t\t/*\n\t\t * Ok, this dying thread is truly holding a futex\n\t\t * of interest. Set the OWNER_DIED bit atomically\n\t\t * via cmpxchg, and if the value had FUTEX_WAITERS\n\t\t * set, wake up a waiter (if any). (We have to do a\n\t\t * futex_wake() even if OWNER_DIED is already set -\n\t\t * to handle the rare but possible case of recursive\n\t\t * thread-death.) The rest of the cleanup is done in\n\t\t * userspace.\n\t\t */\n\t\tmval = (uval & FUTEX_WAITERS) | FUTEX_OWNER_DIED;\n\t\t/*\n\t\t * We are not holding a lock here, but we want to have\n\t\t * the pagefault_disable/enable() protection because\n\t\t * we want to handle the fault gracefully. If the\n\t\t * access fails we try to fault in the futex with R/W\n\t\t * verification via get_user_pages. get_user() above\n\t\t * does not guarantee R/W access. If that fails we\n\t\t * give up and leave the futex locked.\n\t\t */\n\t\tif (cmpxchg_futex_value_locked(&nval, uaddr, uval, mval)) {\n\t\t\tif (fault_in_user_writeable(uaddr))\n\t\t\t\treturn -1;\n\t\t\tgoto retry;\n\t\t}\n\t\tif (nval != uval)\n\t\t\tgoto retry;\n\n\t\t/*\n\t\t * Wake robust non-PI futexes here. The wakeup of\n\t\t * PI futexes happens in exit_pi_state():\n\t\t */\n\t\tif (!pi && (uval & FUTEX_WAITERS))\n\t\t\tfutex_wake(uaddr, 1, 1, FUTEX_BITSET_MATCH_ANY);\n\t}\n\treturn 0;\n}\n\n/*\n * Fetch a robust-list pointer. Bit 0 signals PI futexes:\n */\nstatic inline int fetch_robust_entry(struct robust_list __user **entry,\n\t\t\t\t     struct robust_list __user * __user *head,\n\t\t\t\t     unsigned int *pi)\n{\n\tunsigned long uentry;\n\n\tif (get_user(uentry, (unsigned long __user *)head))\n\t\treturn -EFAULT;\n\n\t*entry = (void __user *)(uentry & ~1UL);\n\t*pi = uentry & 1;\n\n\treturn 0;\n}\n\n/*\n * Walk curr->robust_list (very carefully, it's a userspace list!)\n * and mark any locks found there dead, and notify any waiters.\n *\n * We silently return on any sign of list-walking problem.\n */\nvoid exit_robust_list(struct task_struct *curr)\n{\n\tstruct robust_list_head __user *head = curr->robust_list;\n\tstruct robust_list __user *entry, *next_entry, *pending;\n\tunsigned int limit = ROBUST_LIST_LIMIT, pi, pip;\n\tunsigned int uninitialized_var(next_pi);\n\tunsigned long futex_offset;\n\tint rc;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn;\n\n\t/*\n\t * Fetch the list head (which was registered earlier, via\n\t * sys_set_robust_list()):\n\t */\n\tif (fetch_robust_entry(&entry, &head->list.next, &pi))\n\t\treturn;\n\t/*\n\t * Fetch the relative futex offset:\n\t */\n\tif (get_user(futex_offset, &head->futex_offset))\n\t\treturn;\n\t/*\n\t * Fetch any possibly pending lock-add first, and handle it\n\t * if it exists:\n\t */\n\tif (fetch_robust_entry(&pending, &head->list_op_pending, &pip))\n\t\treturn;\n\n\tnext_entry = NULL;\t/* avoid warning with gcc */\n\twhile (entry != &head->list) {\n\t\t/*\n\t\t * Fetch the next entry in the list before calling\n\t\t * handle_futex_death:\n\t\t */\n\t\trc = fetch_robust_entry(&next_entry, &entry->next, &next_pi);\n\t\t/*\n\t\t * A pending lock might already be on the list, so\n\t\t * don't process it twice:\n\t\t */\n\t\tif (entry != pending)\n\t\t\tif (handle_futex_death((void __user *)entry + futex_offset,\n\t\t\t\t\t\tcurr, pi))\n\t\t\t\treturn;\n\t\tif (rc)\n\t\t\treturn;\n\t\tentry = next_entry;\n\t\tpi = next_pi;\n\t\t/*\n\t\t * Avoid excessively long or circular lists:\n\t\t */\n\t\tif (!--limit)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (pending)\n\t\thandle_futex_death((void __user *)pending + futex_offset,\n\t\t\t\t   curr, pip);\n}\n\nlong do_futex(u32 __user *uaddr, int op, u32 val, ktime_t *timeout,\n\t\tu32 __user *uaddr2, u32 val2, u32 val3)\n{\n\tint cmd = op & FUTEX_CMD_MASK;\n\tunsigned int flags = 0;\n\n\tif (!(op & FUTEX_PRIVATE_FLAG))\n\t\tflags |= FLAGS_SHARED;\n\n\tif (op & FUTEX_CLOCK_REALTIME) {\n\t\tflags |= FLAGS_CLOCKRT;\n\t\tif (cmd != FUTEX_WAIT_BITSET && cmd != FUTEX_WAIT_REQUEUE_PI)\n\t\t\treturn -ENOSYS;\n\t}\n\n\tswitch (cmd) {\n\tcase FUTEX_LOCK_PI:\n\tcase FUTEX_UNLOCK_PI:\n\tcase FUTEX_TRYLOCK_PI:\n\tcase FUTEX_WAIT_REQUEUE_PI:\n\tcase FUTEX_CMP_REQUEUE_PI:\n\t\tif (!futex_cmpxchg_enabled)\n\t\t\treturn -ENOSYS;\n\t}\n\n\tswitch (cmd) {\n\tcase FUTEX_WAIT:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\tcase FUTEX_WAIT_BITSET:\n\t\treturn futex_wait(uaddr, flags, val, timeout, val3);\n\tcase FUTEX_WAKE:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\tcase FUTEX_WAKE_BITSET:\n\t\treturn futex_wake(uaddr, flags, val, val3);\n\tcase FUTEX_REQUEUE:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, NULL, 0);\n\tcase FUTEX_CMP_REQUEUE:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 0);\n\tcase FUTEX_WAKE_OP:\n\t\treturn futex_wake_op(uaddr, flags, uaddr2, val, val2, val3);\n\tcase FUTEX_LOCK_PI:\n\t\treturn futex_lock_pi(uaddr, flags, val, timeout, 0);\n\tcase FUTEX_UNLOCK_PI:\n\t\treturn futex_unlock_pi(uaddr, flags);\n\tcase FUTEX_TRYLOCK_PI:\n\t\treturn futex_lock_pi(uaddr, flags, 0, timeout, 1);\n\tcase FUTEX_WAIT_REQUEUE_PI:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\t\treturn futex_wait_requeue_pi(uaddr, flags, val, timeout, val3,\n\t\t\t\t\t     uaddr2);\n\tcase FUTEX_CMP_REQUEUE_PI:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 1);\n\t}\n\treturn -ENOSYS;\n}\n\n\nSYSCALL_DEFINE6(futex, u32 __user *, uaddr, int, op, u32, val,\n\t\tstruct timespec __user *, utime, u32 __user *, uaddr2,\n\t\tu32, val3)\n{\n\tstruct timespec ts;\n\tktime_t t, *tp = NULL;\n\tu32 val2 = 0;\n\tint cmd = op & FUTEX_CMD_MASK;\n\n\tif (utime && (cmd == FUTEX_WAIT || cmd == FUTEX_LOCK_PI ||\n\t\t      cmd == FUTEX_WAIT_BITSET ||\n\t\t      cmd == FUTEX_WAIT_REQUEUE_PI)) {\n\t\tif (copy_from_user(&ts, utime, sizeof(ts)) != 0)\n\t\t\treturn -EFAULT;\n\t\tif (!timespec_valid(&ts))\n\t\t\treturn -EINVAL;\n\n\t\tt = timespec_to_ktime(ts);\n\t\tif (cmd == FUTEX_WAIT)\n\t\t\tt = ktime_add_safe(ktime_get(), t);\n\t\ttp = &t;\n\t}\n\t/*\n\t * requeue parameter in 'utime' if cmd == FUTEX_*_REQUEUE_*.\n\t * number of waiters to wake in 'utime' if cmd == FUTEX_WAKE_OP.\n\t */\n\tif (cmd == FUTEX_REQUEUE || cmd == FUTEX_CMP_REQUEUE ||\n\t    cmd == FUTEX_CMP_REQUEUE_PI || cmd == FUTEX_WAKE_OP)\n\t\tval2 = (u32) (unsigned long) utime;\n\n\treturn do_futex(uaddr, op, val, tp, uaddr2, val2, val3);\n}\n\nstatic void __init futex_detect_cmpxchg(void)\n{\n#ifndef CONFIG_HAVE_FUTEX_CMPXCHG\n\tu32 curval;\n\n\t/*\n\t * This will fail and we want it. Some arch implementations do\n\t * runtime detection of the futex_atomic_cmpxchg_inatomic()\n\t * functionality. We want to know that before we call in any\n\t * of the complex code paths. Also we want to prevent\n\t * registration of robust lists in that case. NULL is\n\t * guaranteed to fault and we get -EFAULT on functional\n\t * implementation, the non-functional ones will return\n\t * -ENOSYS.\n\t */\n\tif (cmpxchg_futex_value_locked(&curval, NULL, 0, 0) == -EFAULT)\n\t\tfutex_cmpxchg_enabled = 1;\n#endif\n}\n\nstatic int __init futex_init(void)\n{\n\tunsigned int futex_shift;\n\tunsigned long i;\n\n#if CONFIG_BASE_SMALL\n\tfutex_hashsize = 16;\n#else\n\tfutex_hashsize = roundup_pow_of_two(256 * num_possible_cpus());\n#endif\n\n\tfutex_queues = alloc_large_system_hash(\"futex\", sizeof(*futex_queues),\n\t\t\t\t\t       futex_hashsize, 0,\n\t\t\t\t\t       futex_hashsize < 256 ? HASH_SMALL : 0,\n\t\t\t\t\t       &futex_shift, NULL,\n\t\t\t\t\t       futex_hashsize, futex_hashsize);\n\tfutex_hashsize = 1UL << futex_shift;\n\n\tfutex_detect_cmpxchg();\n\n\tfor (i = 0; i < futex_hashsize; i++) {\n\t\tatomic_set(&futex_queues[i].waiters, 0);\n\t\tplist_head_init(&futex_queues[i].chain);\n\t\tspin_lock_init(&futex_queues[i].lock);\n\t}\n\n\treturn 0;\n}\n__initcall(futex_init);\n"], "filenames": ["kernel/futex.c"], "buggy_code_start_loc": [1444], "buggy_code_end_loc": [2526], "fixing_code_start_loc": [1445], "fixing_code_end_loc": [2552], "type": "CWE-269", "message": "The futex_requeue function in kernel/futex.c in the Linux kernel through 3.14.5 does not ensure that calls have two different futex addresses, which allows local users to gain privileges via a crafted FUTEX_REQUEUE command that facilitates unsafe waiter modification.", "other": {"cve": {"id": "CVE-2014-3153", "sourceIdentifier": "cve-coordination@google.com", "published": "2014-06-07T14:55:27.240", "lastModified": "2021-02-08T16:25:59.053", "vulnStatus": "Analyzed", "cisaExploitAdd": "2022-05-25", "cisaActionDue": "2022-06-15", "cisaRequiredAction": "Apply updates per vendor instructions.", "cisaVulnerabilityName": "Linux Kernel Privilege Escalation Vulnerability", "descriptions": [{"lang": "en", "value": "The futex_requeue function in kernel/futex.c in the Linux kernel through 3.14.5 does not ensure that calls have two different futex addresses, which allows local users to gain privileges via a crafted FUTEX_REQUEUE command that facilitates unsafe waiter modification."}, {"lang": "es", "value": "La funci\u00f3n futex_requeue en kernel/futex.c en el kernel de Linux hasta 3.14.5 no asegura que las llamadas tengan dos direcciones futex diferentes, lo que permite a usuarios locales ganar privilegios a trav\u00e9s de un comando FUTEX_REQUEUE manipulado que facilita la modificaci\u00f3n insegura del objeto o funci\u00f3n a la espera."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-269"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.14.5", "matchCriteriaId": "64AC2C28-4430-4AE4-A540-C4AF96D94B53"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:6.2:*:*:*:*:*:*:*", "matchCriteriaId": "AD6D0378-F0F4-4AAA-80AF-8287C790EC96"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:opensuse:11.4:*:*:*:*:*:*:*", "matchCriteriaId": "DE554781-1EB9-446E-911F-6C11970C47F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_desktop:11:sp3:*:*:*:*:*:*", "matchCriteriaId": "3ED68ADD-BBDA-4485-BC76-58F011D72311"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_high_availability_extension:11:sp3:*:*:*:*:*:*", "matchCriteriaId": "A3A907A3-2A3A-46D4-8D75-914649877B65"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_real_time_extension:11:sp3:*:*:*:*:*:*", "matchCriteriaId": "3DB41B45-D94D-4A58-88B0-B3EC3EC350E2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_server:11:-:*:*:*:*:*:*", "matchCriteriaId": "F13F07CC-739B-465C-9184-0E9D708BD4C7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_server:11:sp2:*:*:ltss:*:*:*", "matchCriteriaId": "CB6476C7-03F2-4939-AB85-69AA524516D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_server:11:sp3:*:*:*:-:*:*", "matchCriteriaId": "E534C201-BCC5-473C-AAA7-AAB97CEB5437"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_server:11:sp3:*:*:*:vmware:*:*", "matchCriteriaId": "2470C6E8-2024-4CF5-9982-CFF50E88EAE9"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=e9c243a5a6de0be8e584c604d353412584b592f8", "source": "cve-coordination@google.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-0771.html", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-3037.html", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-3038.html", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-3039.html", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2014-06/msg00014.html", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2014-06/msg00018.html", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2014-06/msg00025.html", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2014-07/msg00006.html", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2014-10/msg00006.html", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2014-10/msg00007.html", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://openwall.com/lists/oss-security/2014/06/05/24", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://openwall.com/lists/oss-security/2014/06/06/20", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2014-0800.html", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.debian.org/security/2014/dsa-2949", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.exploit-db.com/exploits/35370", "source": "cve-coordination@google.com", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/06/05/22", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2021/02/01/4", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/67906", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1030451", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.ubuntu.com/usn/USN-2237-1", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2240-1", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1103626", "source": "cve-coordination@google.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://elongl.github.io/exploitation/2021/01/08/cve-2014-3153.html", "source": "cve-coordination@google.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=13fbca4c6ecd96ec1a1cfa2e4f2ce191fe928a5e", "source": "cve-coordination@google.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=54a217887a7b658e2650c3feff22756ab80c7339", "source": "cve-coordination@google.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b3eaa9fc5cd0a4d74b18f6b8dc617aeaf1873270", "source": "cve-coordination@google.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/elongl/CVE-2014-3153", "source": "cve-coordination@google.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/e9c243a5a6de0be8e584c604d353412584b592f8", "source": "cve-coordination@google.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.openwall.com/lists/oss-security/2021/02/01/4", "source": "cve-coordination@google.com", "tags": ["Mailing List", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/e9c243a5a6de0be8e584c604d353412584b592f8"}}