{"buggy_code": ["#!/usr/bin/env python3\n\n# Copyright (C) 2012 Canonical Ltd.\n# Copyright (C) 2012 Hewlett-Packard Development Company, L.P.\n# Copyright (C) 2012 Yahoo! Inc.\n# Copyright (C) 2017 Amazon.com, Inc. or its affiliates\n#\n# Author: Scott Moser <scott.moser@canonical.com>\n# Author: Juerg Haefliger <juerg.haefliger@hp.com>\n# Author: Joshua Harlow <harlowja@yahoo-inc.com>\n# Author: Andrew Jorgensen <ajorgens@amazon.com>\n#\n# This file is part of cloud-init. See LICENSE file for license information.\n\n# Skip isort on this file because of the patch that comes between imports\n# isort: skip_file\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nimport traceback\n\nfrom cloudinit import patcher\nfrom cloudinit.config.modules import Modules\n\npatcher.patch_logging()\n\nfrom cloudinit.config.schema import validate_cloudconfig_schema\nfrom cloudinit import log as logging\nfrom cloudinit import netinfo\nfrom cloudinit import signal_handler\nfrom cloudinit import sources\nfrom cloudinit import stages\nfrom cloudinit import url_helper\nfrom cloudinit import util\nfrom cloudinit import version\nfrom cloudinit import warnings\n\nfrom cloudinit import reporting\nfrom cloudinit.reporting import events\n\nfrom cloudinit.settings import PER_INSTANCE, PER_ALWAYS, PER_ONCE, CLOUD_CONFIG\n\nfrom cloudinit import atomic_helper\n\nfrom cloudinit.config import cc_set_hostname\nfrom cloudinit import dhclient_hook\nfrom cloudinit.cmd.devel import read_cfg_paths\n\n\n# Welcome message template\nWELCOME_MSG_TPL = (\n    \"Cloud-init v. {version} running '{action}' at \"\n    \"{timestamp}. Up {uptime} seconds.\"\n)\n\n# Module section template\nMOD_SECTION_TPL = \"cloud_%s_modules\"\n\n# Frequency shortname to full name\n# (so users don't have to remember the full name...)\nFREQ_SHORT_NAMES = {\n    \"instance\": PER_INSTANCE,\n    \"always\": PER_ALWAYS,\n    \"once\": PER_ONCE,\n}\n\nLOG = logging.getLogger()\n\n\n# Used for when a logger may not be active\n# and we still want to print exceptions...\ndef print_exc(msg=\"\"):\n    if msg:\n        sys.stderr.write(\"%s\\n\" % (msg))\n    sys.stderr.write(\"-\" * 60)\n    sys.stderr.write(\"\\n\")\n    traceback.print_exc(file=sys.stderr)\n    sys.stderr.write(\"-\" * 60)\n    sys.stderr.write(\"\\n\")\n\n\ndef welcome(action, msg=None):\n    if not msg:\n        msg = welcome_format(action)\n    util.multi_log(\"%s\\n\" % (msg), console=False, stderr=True, log=LOG)\n    return msg\n\n\ndef welcome_format(action):\n    return WELCOME_MSG_TPL.format(\n        version=version.version_string(),\n        uptime=util.uptime(),\n        timestamp=util.time_rfc2822(),\n        action=action,\n    )\n\n\ndef extract_fns(args):\n    # Files are already opened so lets just pass that along\n    # since it would of broke if it couldn't have\n    # read that file already...\n    fn_cfgs = []\n    if args.files:\n        for fh in args.files:\n            # The realpath is more useful in logging\n            # so lets resolve to that...\n            fn_cfgs.append(os.path.realpath(fh.name))\n    return fn_cfgs\n\n\ndef run_module_section(mods: Modules, action_name, section):\n    full_section_name = MOD_SECTION_TPL % (section)\n    (which_ran, failures) = mods.run_section(full_section_name)\n    total_attempted = len(which_ran) + len(failures)\n    if total_attempted == 0:\n        msg = \"No '%s' modules to run under section '%s'\" % (\n            action_name,\n            full_section_name,\n        )\n        sys.stderr.write(\"%s\\n\" % (msg))\n        LOG.debug(msg)\n        return []\n    else:\n        LOG.debug(\n            \"Ran %s modules with %s failures\", len(which_ran), len(failures)\n        )\n        return failures\n\n\ndef apply_reporting_cfg(cfg):\n    if cfg.get(\"reporting\"):\n        reporting.update_configuration(cfg.get(\"reporting\"))\n\n\ndef parse_cmdline_url(cmdline, names=(\"cloud-config-url\", \"url\")):\n    data = util.keyval_str_to_dict(cmdline)\n    for key in names:\n        if key in data:\n            return key, data[key]\n    raise KeyError(\"No keys (%s) found in string '%s'\" % (cmdline, names))\n\n\ndef attempt_cmdline_url(path, network=True, cmdline=None):\n    \"\"\"Write data from url referenced in command line to path.\n\n    path: a file to write content to if downloaded.\n    network: should network access be assumed.\n    cmdline: the cmdline to parse for cloud-config-url.\n\n    This is used in MAAS datasource, in \"ephemeral\" (read-only root)\n    environment where the instance netboots to iscsi ro root.\n    and the entity that controls the pxe config has to configure\n    the maas datasource.\n\n    An attempt is made on network urls even in local datasource\n    for case of network set up in initramfs.\n\n    Return value is a tuple of a logger function (logging.DEBUG)\n    and a message indicating what happened.\n    \"\"\"\n\n    if cmdline is None:\n        cmdline = util.get_cmdline()\n\n    try:\n        cmdline_name, url = parse_cmdline_url(cmdline)\n    except KeyError:\n        return (logging.DEBUG, \"No kernel command line url found.\")\n\n    path_is_local = url.startswith(\"file://\") or url.startswith(\"/\")\n\n    if path_is_local and os.path.exists(path):\n        if network:\n            m = (\n                \"file '%s' existed, possibly from local stage download\"\n                \" of command line url '%s'. Not re-writing.\" % (path, url)\n            )\n            level = logging.INFO\n            if path_is_local:\n                level = logging.DEBUG\n        else:\n            m = (\n                \"file '%s' existed, possibly from previous boot download\"\n                \" of command line url '%s'. Not re-writing.\" % (path, url)\n            )\n            level = logging.WARN\n\n        return (level, m)\n\n    kwargs = {\"url\": url, \"timeout\": 10, \"retries\": 2}\n    if network or path_is_local:\n        level = logging.WARN\n        kwargs[\"sec_between\"] = 1\n    else:\n        level = logging.DEBUG\n        kwargs[\"sec_between\"] = 0.1\n\n    data = None\n    header = b\"#cloud-config\"\n    try:\n        resp = url_helper.read_file_or_url(**kwargs)\n        if resp.ok():\n            data = resp.contents\n            if not resp.contents.startswith(header):\n                if cmdline_name == \"cloud-config-url\":\n                    level = logging.WARN\n                else:\n                    level = logging.INFO\n                return (\n                    level,\n                    \"contents of '%s' did not start with %s\" % (url, header),\n                )\n        else:\n            return (\n                level,\n                \"url '%s' returned code %s. Ignoring.\" % (url, resp.code),\n            )\n\n    except url_helper.UrlError as e:\n        return (level, \"retrieving url '%s' failed: %s\" % (url, e))\n\n    util.write_file(path, data, mode=0o600)\n    return (\n        logging.INFO,\n        \"wrote cloud-config data from %s='%s' to %s\"\n        % (cmdline_name, url, path),\n    )\n\n\ndef purge_cache_on_python_version_change(init):\n    \"\"\"Purge the cache if python version changed on us.\n\n    There could be changes not represented in our cache (obj.pkl) after we\n    upgrade to a new version of python, so at that point clear the cache\n    \"\"\"\n    current_python_version = \"%d.%d\" % (\n        sys.version_info.major,\n        sys.version_info.minor,\n    )\n    python_version_path = os.path.join(\n        init.paths.get_cpath(\"data\"), \"python-version\"\n    )\n    if os.path.exists(python_version_path):\n        cached_python_version = open(python_version_path).read()\n        # The Python version has changed out from under us, anything that was\n        # pickled previously is likely useless due to API changes.\n        if cached_python_version != current_python_version:\n            LOG.debug(\"Python version change detected. Purging cache\")\n            init.purge_cache(True)\n            util.write_file(python_version_path, current_python_version)\n    else:\n        if os.path.exists(init.paths.get_ipath_cur(\"obj_pkl\")):\n            LOG.info(\n                \"Writing python-version file. \"\n                \"Cache compatibility status is currently unknown.\"\n            )\n        util.write_file(python_version_path, current_python_version)\n\n\ndef _should_bring_up_interfaces(init, args):\n    if util.get_cfg_option_bool(init.cfg, \"disable_network_activation\"):\n        return False\n    return not args.local\n\n\ndef main_init(name, args):\n    deps = [sources.DEP_FILESYSTEM, sources.DEP_NETWORK]\n    if args.local:\n        deps = [sources.DEP_FILESYSTEM]\n\n    early_logs = [\n        attempt_cmdline_url(\n            path=os.path.join(\n                \"%s.d\" % CLOUD_CONFIG, \"91_kernel_cmdline_url.cfg\"\n            ),\n            network=not args.local,\n        )\n    ]\n\n    # Cloud-init 'init' stage is broken up into the following sub-stages\n    # 1. Ensure that the init object fetches its config without errors\n    # 2. Setup logging/output redirections with resultant config (if any)\n    # 3. Initialize the cloud-init filesystem\n    # 4. Check if we can stop early by looking for various files\n    # 5. Fetch the datasource\n    # 6. Connect to the current instance location + update the cache\n    # 7. Consume the userdata (handlers get activated here)\n    # 8. Construct the modules object\n    # 9. Adjust any subsequent logging/output redirections using the modules\n    #    objects config as it may be different from init object\n    # 10. Run the modules for the 'init' stage\n    # 11. Done!\n    if not args.local:\n        w_msg = welcome_format(name)\n    else:\n        w_msg = welcome_format(\"%s-local\" % (name))\n    init = stages.Init(ds_deps=deps, reporter=args.reporter)\n    # Stage 1\n    init.read_cfg(extract_fns(args))\n    # Stage 2\n    outfmt = None\n    errfmt = None\n    try:\n        early_logs.append((logging.DEBUG, \"Closing stdin.\"))\n        util.close_stdin()\n        (outfmt, errfmt) = util.fixup_output(init.cfg, name)\n    except Exception:\n        msg = \"Failed to setup output redirection!\"\n        util.logexc(LOG, msg)\n        print_exc(msg)\n        early_logs.append((logging.WARN, msg))\n    if args.debug:\n        # Reset so that all the debug handlers are closed out\n        LOG.debug(\n            \"Logging being reset, this logger may no longer be active shortly\"\n        )\n        logging.resetLogging()\n    logging.setupLogging(init.cfg)\n    apply_reporting_cfg(init.cfg)\n\n    # Any log usage prior to setupLogging above did not have local user log\n    # config applied.  We send the welcome message now, as stderr/out have\n    # been redirected and log now configured.\n    welcome(name, msg=w_msg)\n\n    # re-play early log messages before logging was setup\n    for lvl, msg in early_logs:\n        LOG.log(lvl, msg)\n\n    # Stage 3\n    try:\n        init.initialize()\n    except Exception:\n        util.logexc(LOG, \"Failed to initialize, likely bad things to come!\")\n    # Stage 4\n    path_helper = init.paths\n    purge_cache_on_python_version_change(init)\n    mode = sources.DSMODE_LOCAL if args.local else sources.DSMODE_NETWORK\n\n    if mode == sources.DSMODE_NETWORK:\n        existing = \"trust\"\n        sys.stderr.write(\"%s\\n\" % (netinfo.debug_info()))\n    else:\n        existing = \"check\"\n        mcfg = util.get_cfg_option_bool(init.cfg, \"manual_cache_clean\", False)\n        if mcfg:\n            LOG.debug(\"manual cache clean set from config\")\n            existing = \"trust\"\n        else:\n            mfile = path_helper.get_ipath_cur(\"manual_clean_marker\")\n            if os.path.exists(mfile):\n                LOG.debug(\"manual cache clean found from marker: %s\", mfile)\n                existing = \"trust\"\n\n        init.purge_cache()\n\n    # Stage 5\n    bring_up_interfaces = _should_bring_up_interfaces(init, args)\n    try:\n        init.fetch(existing=existing)\n        # if in network mode, and the datasource is local\n        # then work was done at that stage.\n        if mode == sources.DSMODE_NETWORK and init.datasource.dsmode != mode:\n            LOG.debug(\n                \"[%s] Exiting. datasource %s in local mode\",\n                mode,\n                init.datasource,\n            )\n            return (None, [])\n    except sources.DataSourceNotFoundException:\n        # In the case of 'cloud-init init' without '--local' it is a bit\n        # more likely that the user would consider it failure if nothing was\n        # found.\n        if mode == sources.DSMODE_LOCAL:\n            LOG.debug(\"No local datasource found\")\n        else:\n            util.logexc(\n                LOG, \"No instance datasource found! Likely bad things to come!\"\n            )\n        if not args.force:\n            init.apply_network_config(bring_up=bring_up_interfaces)\n            LOG.debug(\"[%s] Exiting without datasource\", mode)\n            if mode == sources.DSMODE_LOCAL:\n                return (None, [])\n            else:\n                return (None, [\"No instance datasource found.\"])\n        else:\n            LOG.debug(\n                \"[%s] barreling on in force mode without datasource\", mode\n            )\n\n    _maybe_persist_instance_data(init)\n    # Stage 6\n    iid = init.instancify()\n    LOG.debug(\n        \"[%s] %s will now be targeting instance id: %s. new=%s\",\n        mode,\n        name,\n        iid,\n        init.is_new_instance(),\n    )\n\n    if mode == sources.DSMODE_LOCAL:\n        # Before network comes up, set any configured hostname to allow\n        # dhcp clients to advertize this hostname to any DDNS services\n        # LP: #1746455.\n        _maybe_set_hostname(init, stage=\"local\", retry_stage=\"network\")\n    init.apply_network_config(bring_up=bring_up_interfaces)\n\n    if mode == sources.DSMODE_LOCAL:\n        if init.datasource.dsmode != mode:\n            LOG.debug(\n                \"[%s] Exiting. datasource %s not in local mode.\",\n                mode,\n                init.datasource,\n            )\n            return (init.datasource, [])\n        else:\n            LOG.debug(\n                \"[%s] %s is in local mode, will apply init modules now.\",\n                mode,\n                init.datasource,\n            )\n\n    # Give the datasource a chance to use network resources.\n    # This is used on Azure to communicate with the fabric over network.\n    init.setup_datasource()\n    # update fully realizes user-data (pulling in #include if necessary)\n    init.update()\n    _maybe_set_hostname(init, stage=\"init-net\", retry_stage=\"modules:config\")\n    # Stage 7\n    try:\n        # Attempt to consume the data per instance.\n        # This may run user-data handlers and/or perform\n        # url downloads and such as needed.\n        (ran, _results) = init.cloudify().run(\n            \"consume_data\",\n            init.consume_data,\n            args=[PER_INSTANCE],\n            freq=PER_INSTANCE,\n        )\n        if not ran:\n            # Just consume anything that is set to run per-always\n            # if nothing ran in the per-instance code\n            #\n            # See: https://bugs.launchpad.net/bugs/819507 for a little\n            # reason behind this...\n            init.consume_data(PER_ALWAYS)\n    except Exception:\n        util.logexc(LOG, \"Consuming user data failed!\")\n        return (init.datasource, [\"Consuming user data failed!\"])\n\n    # Validate user-data adheres to schema definition\n    if os.path.exists(init.paths.get_ipath_cur(\"userdata_raw\")):\n        validate_cloudconfig_schema(config=init.cfg, strict=False)\n    else:\n        LOG.debug(\"Skipping user-data validation. No user-data found.\")\n\n    apply_reporting_cfg(init.cfg)\n\n    # Stage 8 - re-read and apply relevant cloud-config to include user-data\n    mods = Modules(init, extract_fns(args), reporter=args.reporter)\n    # Stage 9\n    try:\n        outfmt_orig = outfmt\n        errfmt_orig = errfmt\n        (outfmt, errfmt) = util.get_output_cfg(mods.cfg, name)\n        if outfmt_orig != outfmt or errfmt_orig != errfmt:\n            LOG.warning(\"Stdout, stderr changing to (%s, %s)\", outfmt, errfmt)\n            (outfmt, errfmt) = util.fixup_output(mods.cfg, name)\n    except Exception:\n        util.logexc(LOG, \"Failed to re-adjust output redirection!\")\n    logging.setupLogging(mods.cfg)\n\n    # give the activated datasource a chance to adjust\n    init.activate_datasource()\n\n    di_report_warn(datasource=init.datasource, cfg=init.cfg)\n\n    # Stage 10\n    return (init.datasource, run_module_section(mods, name, name))\n\n\ndef di_report_warn(datasource, cfg):\n    if \"di_report\" not in cfg:\n        LOG.debug(\"no di_report found in config.\")\n        return\n\n    dicfg = cfg[\"di_report\"]\n    if dicfg is None:\n        # ds-identify may write 'di_report:\\n #comment\\n'\n        # which reads as {'di_report': None}\n        LOG.debug(\"di_report was None.\")\n        return\n\n    if not isinstance(dicfg, dict):\n        LOG.warning(\"di_report config not a dictionary: %s\", dicfg)\n        return\n\n    dslist = dicfg.get(\"datasource_list\")\n    if dslist is None:\n        LOG.warning(\"no 'datasource_list' found in di_report.\")\n        return\n    elif not isinstance(dslist, list):\n        LOG.warning(\"di_report/datasource_list not a list: %s\", dslist)\n        return\n\n    # ds.__module__ is like cloudinit.sources.DataSourceName\n    # where Name is the thing that shows up in datasource_list.\n    modname = datasource.__module__.rpartition(\".\")[2]\n    if modname.startswith(sources.DS_PREFIX):\n        modname = modname[len(sources.DS_PREFIX) :]\n    else:\n        LOG.warning(\n            \"Datasource '%s' came from unexpected module '%s'.\",\n            datasource,\n            modname,\n        )\n\n    if modname in dslist:\n        LOG.debug(\n            \"used datasource '%s' from '%s' was in di_report's list: %s\",\n            datasource,\n            modname,\n            dslist,\n        )\n        return\n\n    warnings.show_warning(\n        \"dsid_missing_source\", cfg, source=modname, dslist=str(dslist)\n    )\n\n\ndef main_modules(action_name, args):\n    name = args.mode\n    # Cloud-init 'modules' stages are broken up into the following sub-stages\n    # 1. Ensure that the init object fetches its config without errors\n    # 2. Get the datasource from the init object, if it does\n    #    not exist then that means the main_init stage never\n    #    worked, and thus this stage can not run.\n    # 3. Construct the modules object\n    # 4. Adjust any subsequent logging/output redirections using\n    #    the modules objects configuration\n    # 5. Run the modules for the given stage name\n    # 6. Done!\n    w_msg = welcome_format(\"%s:%s\" % (action_name, name))\n    init = stages.Init(ds_deps=[], reporter=args.reporter)\n    # Stage 1\n    init.read_cfg(extract_fns(args))\n    # Stage 2\n    try:\n        init.fetch(existing=\"trust\")\n    except sources.DataSourceNotFoundException:\n        # There was no datasource found, theres nothing to do\n        msg = (\n            \"Can not apply stage %s, no datasource found! Likely bad \"\n            \"things to come!\" % name\n        )\n        util.logexc(LOG, msg)\n        print_exc(msg)\n        if not args.force:\n            return [(msg)]\n    _maybe_persist_instance_data(init)\n    # Stage 3\n    mods = Modules(init, extract_fns(args), reporter=args.reporter)\n    # Stage 4\n    try:\n        LOG.debug(\"Closing stdin\")\n        util.close_stdin()\n        util.fixup_output(mods.cfg, name)\n    except Exception:\n        util.logexc(LOG, \"Failed to setup output redirection!\")\n    if args.debug:\n        # Reset so that all the debug handlers are closed out\n        LOG.debug(\n            \"Logging being reset, this logger may no longer be active shortly\"\n        )\n        logging.resetLogging()\n    logging.setupLogging(mods.cfg)\n    apply_reporting_cfg(init.cfg)\n\n    # now that logging is setup and stdout redirected, send welcome\n    welcome(name, msg=w_msg)\n\n    # Stage 5\n    return run_module_section(mods, name, name)\n\n\ndef main_single(name, args):\n    # Cloud-init single stage is broken up into the following sub-stages\n    # 1. Ensure that the init object fetches its config without errors\n    # 2. Attempt to fetch the datasource (warn if it doesn't work)\n    # 3. Construct the modules object\n    # 4. Adjust any subsequent logging/output redirections using\n    #    the modules objects configuration\n    # 5. Run the single module\n    # 6. Done!\n    mod_name = args.name\n    w_msg = welcome_format(name)\n    init = stages.Init(ds_deps=[], reporter=args.reporter)\n    # Stage 1\n    init.read_cfg(extract_fns(args))\n    # Stage 2\n    try:\n        init.fetch(existing=\"trust\")\n    except sources.DataSourceNotFoundException:\n        # There was no datasource found,\n        # that might be bad (or ok) depending on\n        # the module being ran (so continue on)\n        util.logexc(\n            LOG, \"Failed to fetch your datasource, likely bad things to come!\"\n        )\n        print_exc(\n            \"Failed to fetch your datasource, likely bad things to come!\"\n        )\n        if not args.force:\n            return 1\n    _maybe_persist_instance_data(init)\n    # Stage 3\n    mods = Modules(init, extract_fns(args), reporter=args.reporter)\n    mod_args = args.module_args\n    if mod_args:\n        LOG.debug(\"Using passed in arguments %s\", mod_args)\n    mod_freq = args.frequency\n    if mod_freq:\n        LOG.debug(\"Using passed in frequency %s\", mod_freq)\n        mod_freq = FREQ_SHORT_NAMES.get(mod_freq)\n    # Stage 4\n    try:\n        LOG.debug(\"Closing stdin\")\n        util.close_stdin()\n        util.fixup_output(mods.cfg, None)\n    except Exception:\n        util.logexc(LOG, \"Failed to setup output redirection!\")\n    if args.debug:\n        # Reset so that all the debug handlers are closed out\n        LOG.debug(\n            \"Logging being reset, this logger may no longer be active shortly\"\n        )\n        logging.resetLogging()\n    logging.setupLogging(mods.cfg)\n    apply_reporting_cfg(init.cfg)\n\n    # now that logging is setup and stdout redirected, send welcome\n    welcome(name, msg=w_msg)\n\n    # Stage 5\n    (which_ran, failures) = mods.run_single(mod_name, mod_args, mod_freq)\n    if failures:\n        LOG.warning(\"Ran %s but it failed!\", mod_name)\n        return 1\n    elif not which_ran:\n        LOG.warning(\"Did not run %s, does it exist?\", mod_name)\n        return 1\n    else:\n        # Guess it worked\n        return 0\n\n\ndef status_wrapper(name, args, data_d=None, link_d=None):\n    if data_d is None:\n        paths = read_cfg_paths()\n        data_d = paths.get_cpath(\"data\")\n    if link_d is None:\n        link_d = os.path.normpath(\"/run/cloud-init\")\n\n    status_path = os.path.join(data_d, \"status.json\")\n    status_link = os.path.join(link_d, \"status.json\")\n    result_path = os.path.join(data_d, \"result.json\")\n    result_link = os.path.join(link_d, \"result.json\")\n\n    util.ensure_dirs(\n        (\n            data_d,\n            link_d,\n        )\n    )\n\n    (_name, functor) = args.action\n\n    if name == \"init\":\n        if args.local:\n            mode = \"init-local\"\n        else:\n            mode = \"init\"\n    elif name == \"modules\":\n        mode = \"modules-%s\" % args.mode\n    else:\n        raise ValueError(\"unknown name: %s\" % name)\n\n    modes = (\n        \"init\",\n        \"init-local\",\n        \"modules-init\",\n        \"modules-config\",\n        \"modules-final\",\n    )\n    if mode not in modes:\n        raise ValueError(\n            \"Invalid cloud init mode specified '{0}'\".format(mode)\n        )\n\n    status = None\n    if mode == \"init-local\":\n        for f in (status_link, result_link, status_path, result_path):\n            util.del_file(f)\n    else:\n        try:\n            status = json.loads(util.load_file(status_path))\n        except Exception:\n            pass\n\n    nullstatus = {\n        \"errors\": [],\n        \"start\": None,\n        \"finished\": None,\n    }\n\n    if status is None:\n        status = {\"v1\": {}}\n        status[\"v1\"][\"datasource\"] = None\n\n    for m in modes:\n        if m not in status[\"v1\"]:\n            status[\"v1\"][m] = nullstatus.copy()\n\n    v1 = status[\"v1\"]\n    v1[\"stage\"] = mode\n    v1[mode][\"start\"] = time.time()\n\n    atomic_helper.write_json(status_path, status)\n    util.sym_link(\n        os.path.relpath(status_path, link_d), status_link, force=True\n    )\n\n    try:\n        ret = functor(name, args)\n        if mode in (\"init\", \"init-local\"):\n            (datasource, errors) = ret\n            if datasource is not None:\n                v1[\"datasource\"] = str(datasource)\n        else:\n            errors = ret\n\n        v1[mode][\"errors\"] = [str(e) for e in errors]\n\n    except Exception as e:\n        util.logexc(LOG, \"failed stage %s\", mode)\n        print_exc(\"failed run of stage %s\" % mode)\n        v1[mode][\"errors\"] = [str(e)]\n\n    v1[mode][\"finished\"] = time.time()\n    v1[\"stage\"] = None\n\n    atomic_helper.write_json(status_path, status)\n\n    if mode == \"modules-final\":\n        # write the 'finished' file\n        errors = []\n        for m in modes:\n            if v1[m][\"errors\"]:\n                errors.extend(v1[m].get(\"errors\", []))\n\n        atomic_helper.write_json(\n            result_path,\n            {\"v1\": {\"datasource\": v1[\"datasource\"], \"errors\": errors}},\n        )\n        util.sym_link(\n            os.path.relpath(result_path, link_d), result_link, force=True\n        )\n\n    return len(v1[mode][\"errors\"])\n\n\ndef _maybe_persist_instance_data(init):\n    \"\"\"Write instance-data.json file if absent and datasource is restored.\"\"\"\n    if init.ds_restored:\n        instance_data_file = os.path.join(\n            init.paths.run_dir, sources.INSTANCE_JSON_FILE\n        )\n        if not os.path.exists(instance_data_file):\n            init.datasource.persist_instance_data()\n\n\ndef _maybe_set_hostname(init, stage, retry_stage):\n    \"\"\"Call set-hostname if metadata, vendordata or userdata provides it.\n\n    @param stage: String representing current stage in which we are running.\n    @param retry_stage: String represented logs upon error setting hostname.\n    \"\"\"\n    cloud = init.cloudify()\n    (hostname, _fqdn, _) = util.get_hostname_fqdn(\n        init.cfg, cloud, metadata_only=True\n    )\n    if hostname:  # meta-data or user-data hostname content\n        try:\n            cc_set_hostname.handle(\"set-hostname\", init.cfg, cloud, LOG, None)\n        except cc_set_hostname.SetHostnameError as e:\n            LOG.debug(\n                \"Failed setting hostname in %s stage. Will\"\n                \" retry in %s stage. Error: %s.\",\n                stage,\n                retry_stage,\n                str(e),\n            )\n\n\ndef main_features(name, args):\n    sys.stdout.write(\"\\n\".join(sorted(version.FEATURES)) + \"\\n\")\n\n\ndef main(sysv_args=None):\n    if not sysv_args:\n        sysv_args = sys.argv\n    parser = argparse.ArgumentParser(prog=sysv_args.pop(0))\n\n    # Top level args\n    parser.add_argument(\n        \"--version\",\n        \"-v\",\n        action=\"version\",\n        version=\"%(prog)s \" + (version.version_string()),\n        help=\"Show program's version number and exit.\",\n    )\n    parser.add_argument(\n        \"--file\",\n        \"-f\",\n        action=\"append\",\n        dest=\"files\",\n        help=\"Use additional yaml configuration files.\",\n        type=argparse.FileType(\"rb\"),\n    )\n    parser.add_argument(\n        \"--debug\",\n        \"-d\",\n        action=\"store_true\",\n        help=\"Show additional pre-action logging (default: %(default)s).\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--force\",\n        action=\"store_true\",\n        help=(\n            \"Force running even if no datasource is\"\n            \" found (use at your own risk).\"\n        ),\n        dest=\"force\",\n        default=False,\n    )\n\n    parser.set_defaults(reporter=None)\n    subparsers = parser.add_subparsers(title=\"Subcommands\", dest=\"subcommand\")\n    subparsers.required = True\n\n    # Each action and its sub-options (if any)\n    parser_init = subparsers.add_parser(\n        \"init\", help=\"Initialize cloud-init and perform initial modules.\"\n    )\n    parser_init.add_argument(\n        \"--local\",\n        \"-l\",\n        action=\"store_true\",\n        help=\"Start in local mode (default: %(default)s).\",\n        default=False,\n    )\n    # This is used so that we can know which action is selected +\n    # the functor to use to run this subcommand\n    parser_init.set_defaults(action=(\"init\", main_init))\n\n    # These settings are used for the 'config' and 'final' stages\n    parser_mod = subparsers.add_parser(\n        \"modules\", help=\"Activate modules using a given configuration key.\"\n    )\n    parser_mod.add_argument(\n        \"--mode\",\n        \"-m\",\n        action=\"store\",\n        help=\"Module configuration name to use (default: %(default)s).\",\n        default=\"config\",\n        choices=(\"init\", \"config\", \"final\"),\n    )\n    parser_mod.set_defaults(action=(\"modules\", main_modules))\n\n    # This subcommand allows you to run a single module\n    parser_single = subparsers.add_parser(\n        \"single\", help=\"Run a single module.\"\n    )\n    parser_single.add_argument(\n        \"--name\",\n        \"-n\",\n        action=\"store\",\n        help=\"module name to run\",\n        required=True,\n    )\n    parser_single.add_argument(\n        \"--frequency\",\n        action=\"store\",\n        help=\"Set module frequency.\",\n        required=False,\n        choices=list(FREQ_SHORT_NAMES.keys()),\n    )\n    parser_single.add_argument(\n        \"--report\",\n        action=\"store_true\",\n        help=\"Enable reporting.\",\n        required=False,\n    )\n    parser_single.add_argument(\n        \"module_args\",\n        nargs=\"*\",\n        metavar=\"argument\",\n        help=\"Any additional arguments to pass to this module.\",\n    )\n    parser_single.set_defaults(action=(\"single\", main_single))\n\n    parser_query = subparsers.add_parser(\n        \"query\",\n        help=\"Query standardized instance metadata from the command line.\",\n    )\n\n    parser_dhclient = subparsers.add_parser(\n        dhclient_hook.NAME, help=dhclient_hook.__doc__\n    )\n    dhclient_hook.get_parser(parser_dhclient)\n\n    parser_features = subparsers.add_parser(\n        \"features\", help=\"List defined features.\"\n    )\n    parser_features.set_defaults(action=(\"features\", main_features))\n\n    parser_analyze = subparsers.add_parser(\n        \"analyze\", help=\"Devel tool: Analyze cloud-init logs and data.\"\n    )\n\n    parser_devel = subparsers.add_parser(\n        \"devel\", help=\"Run development tools.\"\n    )\n\n    parser_collect_logs = subparsers.add_parser(\n        \"collect-logs\", help=\"Collect and tar all cloud-init debug info.\"\n    )\n\n    parser_clean = subparsers.add_parser(\n        \"clean\", help=\"Remove logs and artifacts so cloud-init can re-run.\"\n    )\n\n    parser_status = subparsers.add_parser(\n        \"status\", help=\"Report cloud-init status or wait on completion.\"\n    )\n\n    parser_schema = subparsers.add_parser(\n        \"schema\", help=\"Validate cloud-config files using jsonschema.\"\n    )\n\n    if sysv_args:\n        # Only load subparsers if subcommand is specified to avoid load cost\n        subcommand = sysv_args[0]\n        if subcommand == \"analyze\":\n            from cloudinit.analyze.__main__ import get_parser as analyze_parser\n\n            # Construct analyze subcommand parser\n            analyze_parser(parser_analyze)\n        elif subcommand == \"devel\":\n            from cloudinit.cmd.devel.parser import get_parser as devel_parser\n\n            # Construct devel subcommand parser\n            devel_parser(parser_devel)\n        elif subcommand == \"collect-logs\":\n            from cloudinit.cmd.devel.logs import (\n                get_parser as logs_parser,\n                handle_collect_logs_args,\n            )\n\n            logs_parser(parser_collect_logs)\n            parser_collect_logs.set_defaults(\n                action=(\"collect-logs\", handle_collect_logs_args)\n            )\n        elif subcommand == \"clean\":\n            from cloudinit.cmd.clean import (\n                get_parser as clean_parser,\n                handle_clean_args,\n            )\n\n            clean_parser(parser_clean)\n            parser_clean.set_defaults(action=(\"clean\", handle_clean_args))\n        elif subcommand == \"query\":\n            from cloudinit.cmd.query import (\n                get_parser as query_parser,\n                handle_args as handle_query_args,\n            )\n\n            query_parser(parser_query)\n            parser_query.set_defaults(action=(\"render\", handle_query_args))\n        elif subcommand == \"schema\":\n            from cloudinit.config.schema import (\n                get_parser as schema_parser,\n                handle_schema_args,\n            )\n\n            schema_parser(parser_schema)\n            parser_schema.set_defaults(action=(\"schema\", handle_schema_args))\n        elif subcommand == \"status\":\n            from cloudinit.cmd.status import (\n                get_parser as status_parser,\n                handle_status_args,\n            )\n\n            status_parser(parser_status)\n            parser_status.set_defaults(action=(\"status\", handle_status_args))\n\n    args = parser.parse_args(args=sysv_args)\n\n    # Subparsers.required = True and each subparser sets action=(name, functor)\n    (name, functor) = args.action\n\n    # Setup basic logging to start (until reinitialized)\n    # iff in debug mode.\n    if args.debug:\n        logging.setupBasicLogging()\n\n    # Setup signal handlers before running\n    signal_handler.attach_handlers()\n\n    if name in (\"modules\", \"init\"):\n        functor = status_wrapper\n\n    rname = None\n    report_on = True\n    if name == \"init\":\n        if args.local:\n            rname, rdesc = (\"init-local\", \"searching for local datasources\")\n        else:\n            rname, rdesc = (\n                \"init-network\",\n                \"searching for network datasources\",\n            )\n    elif name == \"modules\":\n        rname, rdesc = (\n            \"modules-%s\" % args.mode,\n            \"running modules for %s\" % args.mode,\n        )\n    elif name == \"single\":\n        rname, rdesc = (\n            \"single/%s\" % args.name,\n            \"running single module %s\" % args.name,\n        )\n        report_on = args.report\n    else:\n        rname = name\n        rdesc = \"running 'cloud-init %s'\" % name\n        report_on = False\n\n    args.reporter = events.ReportEventStack(\n        rname, rdesc, reporting_enabled=report_on\n    )\n\n    with args.reporter:\n        retval = util.log_time(\n            logfunc=LOG.debug,\n            msg=\"cloud-init mode '%s'\" % name,\n            get_uptime=True,\n            func=functor,\n            args=(name, args),\n        )\n        reporting.flush_events()\n        return retval\n\n\nif __name__ == \"__main__\":\n    if \"TZ\" not in os.environ:\n        os.environ[\"TZ\"] = \":/etc/localtime\"\n    return_value = main(sys.argv)\n    if return_value:\n        sys.exit(return_value)\n", "# This file is part of cloud-init. See LICENSE file for license information.\n\"\"\"schema.py: Set of module functions for processing cloud-config schema.\"\"\"\n\nimport argparse\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport textwrap\nimport typing\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom functools import partial\nfrom typing import Optional, Tuple, cast\n\nimport yaml\n\nfrom cloudinit import importer, safeyaml\nfrom cloudinit.cmd.devel import read_cfg_paths\nfrom cloudinit.util import error, get_modules_from_dir, load_file\n\nerror = partial(error, sys_exit=True)\nLOG = logging.getLogger(__name__)\n\nVERSIONED_USERDATA_SCHEMA_FILE = \"versions.schema.cloud-config.json\"\n# Bump this file when introducing incompatible schema changes.\n# Also add new version definition to versions.schema.json.\nUSERDATA_SCHEMA_FILE = \"schema-cloud-config-v1.json\"\n_YAML_MAP = {True: \"true\", False: \"false\", None: \"null\"}\nCLOUD_CONFIG_HEADER = b\"#cloud-config\"\nSCHEMA_DOC_TMPL = \"\"\"\n{name}\n{title_underbar}\n**Summary:** {title}\n\n{description}\n\n**Internal name:** ``{id}``\n\n**Module frequency:** {frequency}\n\n**Supported distros:** {distros}\n\n{property_header}\n{property_doc}\n\n{examples}\n\"\"\"\nSCHEMA_PROPERTY_HEADER = \"**Config schema**:\"\nSCHEMA_PROPERTY_TMPL = \"{prefix}**{prop_name}:** ({prop_type}){description}\"\nSCHEMA_LIST_ITEM_TMPL = (\n    \"{prefix}Each object in **{prop_name}** list supports the following keys:\"\n)\nSCHEMA_EXAMPLES_HEADER = \"**Examples**::\\n\\n\"\nSCHEMA_EXAMPLES_SPACER_TEMPLATE = \"\\n    # --- Example{0} ---\"\n\n\n# annotations add value for development, but don't break old versions\n# pyver: 3.6 -> 3.8\n# pylint: disable=E1101\nif sys.version_info >= (3, 8):\n\n    class MetaSchema(typing.TypedDict):\n        name: str\n        id: str\n        title: str\n        description: str\n        distros: typing.List[str]\n        examples: typing.List[str]\n        frequency: str\n\nelse:\n    MetaSchema = dict\n# pylint: enable=E1101\n\n\nclass SchemaValidationError(ValueError):\n    \"\"\"Raised when validating a cloud-config file against a schema.\"\"\"\n\n    def __init__(self, schema_errors=()):\n        \"\"\"Init the exception an n-tuple of schema errors.\n\n        @param schema_errors: An n-tuple of the format:\n            ((flat.config.key, msg),)\n        \"\"\"\n        self.schema_errors = schema_errors\n        error_messages = [\n            \"{0}: {1}\".format(config_key, message)\n            for config_key, message in schema_errors\n        ]\n        message = \"Cloud config schema errors: {0}\".format(\n            \", \".join(error_messages)\n        )\n        super(SchemaValidationError, self).__init__(message)\n\n\ndef is_schema_byte_string(checker, instance):\n    \"\"\"TYPE_CHECKER override allowing bytes for string type\n\n    For jsonschema v. 3.0.0+\n    \"\"\"\n    try:\n        from jsonschema import Draft4Validator\n    except ImportError:\n        return False\n    return Draft4Validator.TYPE_CHECKER.is_type(\n        instance, \"string\"\n    ) or isinstance(instance, (bytes,))\n\n\ndef get_jsonschema_validator():\n    \"\"\"Get metaschema validator and format checker\n\n    Older versions of jsonschema require some compatibility changes.\n\n    @returns: Tuple: (jsonschema.Validator, FormatChecker)\n    @raises: ImportError when jsonschema is not present\n    \"\"\"\n    from jsonschema import Draft4Validator, FormatChecker\n    from jsonschema.validators import create\n\n    # Allow for bytes to be presented as an acceptable valid value for string\n    # type jsonschema attributes in cloud-init's schema.\n    # This allows #cloud-config to provide valid yaml \"content: !!binary | ...\"\n\n    strict_metaschema = deepcopy(Draft4Validator.META_SCHEMA)\n    strict_metaschema[\"additionalProperties\"] = False\n\n    # This additional label allows us to specify a different name\n    # than the property key when generating docs.\n    # This is especially useful when using a \"patternProperties\" regex,\n    # otherwise the property label in the generated docs will be a\n    # regular expression.\n    # http://json-schema.org/understanding-json-schema/reference/object.html#pattern-properties\n    strict_metaschema[\"properties\"][\"label\"] = {\"type\": \"string\"}\n\n    if hasattr(Draft4Validator, \"TYPE_CHECKER\"):  # jsonschema 3.0+\n        type_checker = Draft4Validator.TYPE_CHECKER.redefine(\n            \"string\", is_schema_byte_string\n        )\n        cloudinitValidator = create(\n            meta_schema=strict_metaschema,\n            validators=Draft4Validator.VALIDATORS,\n            version=\"draft4\",\n            type_checker=type_checker,\n        )\n    else:  # jsonschema 2.6 workaround\n        types = Draft4Validator.DEFAULT_TYPES  # pylint: disable=E1101\n        # Allow bytes as well as string (and disable a spurious unsupported\n        # assignment-operation pylint warning which appears because this\n        # code path isn't written against the latest jsonschema).\n        types[\"string\"] = (str, bytes)  # pylint: disable=E1137\n        cloudinitValidator = create(  # pylint: disable=E1123\n            meta_schema=strict_metaschema,\n            validators=Draft4Validator.VALIDATORS,\n            version=\"draft4\",\n            default_types=types,\n        )\n    return (cloudinitValidator, FormatChecker)\n\n\ndef validate_cloudconfig_metaschema(validator, schema: dict, throw=True):\n    \"\"\"Validate provided schema meets the metaschema definition. Return strict\n    Validator and FormatChecker for use in validation\n    @param validator: Draft4Validator instance used to validate the schema\n    @param schema: schema to validate\n    @param throw: Sometimes the validator and checker are required, even if\n        the schema is invalid. Toggle for whether to raise\n        SchemaValidationError or log warnings.\n\n    @raises: ImportError when jsonschema is not present\n    @raises: SchemaValidationError when the schema is invalid\n    \"\"\"\n\n    from jsonschema.exceptions import SchemaError\n\n    try:\n        validator.check_schema(schema)\n    except SchemaError as err:\n        # Raise SchemaValidationError to avoid jsonschema imports at call\n        # sites\n        if throw:\n            raise SchemaValidationError(\n                schema_errors=(\n                    (\".\".join([str(p) for p in err.path]), err.message),\n                )\n            ) from err\n        LOG.warning(\n            \"Meta-schema validation failed, attempting to validate config \"\n            \"anyway: %s\",\n            err,\n        )\n\n\ndef validate_cloudconfig_schema(\n    config: dict,\n    schema: dict = None,\n    strict: bool = False,\n    strict_metaschema: bool = False,\n):\n    \"\"\"Validate provided config meets the schema definition.\n\n    @param config: Dict of cloud configuration settings validated against\n        schema. Ignored if strict_metaschema=True\n    @param schema: jsonschema dict describing the supported schema definition\n       for the cloud config module (config.cc_*). If None, validate against\n       global schema.\n    @param strict: Boolean, when True raise SchemaValidationErrors instead of\n       logging warnings.\n    @param strict_metaschema: Boolean, when True validates schema using strict\n       metaschema definition at runtime (currently unused)\n\n    @raises: SchemaValidationError when provided config does not validate\n        against the provided schema.\n    @raises: RuntimeError when provided config sourced from YAML is not a dict.\n    \"\"\"\n    if schema is None:\n        schema = get_schema()\n    try:\n        (cloudinitValidator, FormatChecker) = get_jsonschema_validator()\n        if strict_metaschema:\n            validate_cloudconfig_metaschema(\n                cloudinitValidator, schema, throw=False\n            )\n    except ImportError:\n        LOG.debug(\"Ignoring schema validation. jsonschema is not present\")\n        return\n\n    validator = cloudinitValidator(schema, format_checker=FormatChecker())\n    errors: Tuple[Tuple[str, str], ...] = ()\n    for error in sorted(validator.iter_errors(config), key=lambda e: e.path):\n        path = \".\".join([str(p) for p in error.path])\n        errors += ((path, error.message),)\n    if errors:\n        if strict:\n            raise SchemaValidationError(errors)\n        else:\n            messages = [\"{0}: {1}\".format(k, msg) for k, msg in errors]\n            LOG.warning(\n                \"Invalid cloud-config provided:\\n%s\", \"\\n\".join(messages)\n            )\n\n\ndef annotated_cloudconfig_file(\n    cloudconfig, original_content, schema_errors, schemamarks\n):\n    \"\"\"Return contents of the cloud-config file annotated with schema errors.\n\n    @param cloudconfig: YAML-loaded dict from the original_content or empty\n        dict if unparseable.\n    @param original_content: The contents of a cloud-config file\n    @param schema_errors: List of tuples from a JSONSchemaValidationError. The\n        tuples consist of (schemapath, error_message).\n    \"\"\"\n    if not schema_errors:\n        return original_content\n    errors_by_line = defaultdict(list)\n    error_footer = []\n    error_header = \"# Errors: -------------\\n{0}\\n\\n\"\n    annotated_content = []\n    lines = original_content.decode().split(\"\\n\")\n    if not isinstance(cloudconfig, dict):\n        # Return a meaningful message on empty cloud-config\n        return \"\\n\".join(\n            lines\n            + [error_header.format(\"# E1: Cloud-config is not a YAML dict.\")]\n        )\n    for path, msg in schema_errors:\n        match = re.match(r\"format-l(?P<line>\\d+)\\.c(?P<col>\\d+).*\", path)\n        if match:\n            line, col = match.groups()\n            errors_by_line[int(line)].append(msg)\n        else:\n            col = None\n            errors_by_line[schemamarks[path]].append(msg)\n        if col is not None:\n            msg = \"Line {line} column {col}: {msg}\".format(\n                line=line, col=col, msg=msg\n            )\n    error_index = 1\n    for line_number, line in enumerate(lines, 1):\n        errors = errors_by_line[line_number]\n        if errors:\n            error_label = []\n            for error in errors:\n                error_label.append(\"E{0}\".format(error_index))\n                error_footer.append(\"# E{0}: {1}\".format(error_index, error))\n                error_index += 1\n            annotated_content.append(line + \"\\t\\t# \" + \",\".join(error_label))\n\n        else:\n            annotated_content.append(line)\n    annotated_content.append(error_header.format(\"\\n\".join(error_footer)))\n    return \"\\n\".join(annotated_content)\n\n\ndef validate_cloudconfig_file(config_path, schema, annotate=False):\n    \"\"\"Validate cloudconfig file adheres to a specific jsonschema.\n\n    @param config_path: Path to the yaml cloud-config file to parse, or None\n        to default to system userdata from Paths object.\n    @param schema: Dict describing a valid jsonschema to validate against.\n    @param annotate: Boolean set True to print original config file with error\n        annotations on the offending lines.\n\n    @raises SchemaValidationError containing any of schema_errors encountered.\n    @raises RuntimeError when config_path does not exist.\n    \"\"\"\n    if config_path is None:\n        # Use system's raw userdata path\n        if os.getuid() != 0:\n            raise RuntimeError(\n                \"Unable to read system userdata as non-root user.\"\n                \" Try using sudo\"\n            )\n        paths = read_cfg_paths()\n        user_data_file = paths.get_ipath_cur(\"userdata_raw\")\n        content = load_file(user_data_file, decode=False)\n    else:\n        if not os.path.exists(config_path):\n            raise RuntimeError(\n                \"Configfile {0} does not exist\".format(config_path)\n            )\n        content = load_file(config_path, decode=False)\n    if not content.startswith(CLOUD_CONFIG_HEADER):\n        errors = (\n            (\n                \"format-l1.c1\",\n                'File {0} needs to begin with \"{1}\"'.format(\n                    config_path, CLOUD_CONFIG_HEADER.decode()\n                ),\n            ),\n        )\n        error = SchemaValidationError(errors)\n        if annotate:\n            print(\n                annotated_cloudconfig_file(\n                    {}, content, error.schema_errors, {}\n                )\n            )\n        raise error\n    try:\n        if annotate:\n            cloudconfig, marks = safeyaml.load_with_marks(content)\n        else:\n            cloudconfig = safeyaml.load(content)\n            marks = {}\n    except (yaml.YAMLError) as e:\n        line = column = 1\n        mark = None\n        if hasattr(e, \"context_mark\") and getattr(e, \"context_mark\"):\n            mark = getattr(e, \"context_mark\")\n        elif hasattr(e, \"problem_mark\") and getattr(e, \"problem_mark\"):\n            mark = getattr(e, \"problem_mark\")\n        if mark:\n            line = mark.line + 1\n            column = mark.column + 1\n        errors = (\n            (\n                \"format-l{line}.c{col}\".format(line=line, col=column),\n                \"File {0} is not valid yaml. {1}\".format(config_path, str(e)),\n            ),\n        )\n        error = SchemaValidationError(errors)\n        if annotate:\n            print(\n                annotated_cloudconfig_file(\n                    {}, content, error.schema_errors, {}\n                )\n            )\n        raise error from e\n    if not isinstance(cloudconfig, dict):\n        # Return a meaningful message on empty cloud-config\n        if not annotate:\n            raise RuntimeError(\"Cloud-config is not a YAML dict.\")\n    try:\n        validate_cloudconfig_schema(cloudconfig, schema, strict=True)\n    except SchemaValidationError as e:\n        if annotate:\n            print(\n                annotated_cloudconfig_file(\n                    cloudconfig, content, e.schema_errors, marks\n                )\n            )\n        raise\n\n\ndef _sort_property_order(value):\n    \"\"\"Provide a sorting weight for documentation of property types.\n\n    Weight values ensure 'array' sorted after 'object' which is sorted\n    after anything else which remains unsorted.\n    \"\"\"\n    if value == \"array\":\n        return 2\n    elif value == \"object\":\n        return 1\n    return 0\n\n\ndef _get_property_type(property_dict: dict, defs: dict) -> str:\n    \"\"\"Return a string representing a property type from a given\n    jsonschema.\n    \"\"\"\n    _flatten_schema_refs(property_dict, defs)\n    property_types = property_dict.get(\"type\", [])\n    if not isinstance(property_types, list):\n        property_types = [property_types]\n    if property_dict.get(\"enum\"):\n        property_types = [\n            f\"``{_YAML_MAP.get(k, k)}``\" for k in property_dict[\"enum\"]\n        ]\n    elif property_dict.get(\"oneOf\"):\n        property_types.extend(\n            [\n                subschema[\"type\"]\n                for subschema in property_dict.get(\"oneOf\", {})\n                if subschema.get(\"type\")\n            ]\n        )\n    if len(property_types) == 1:\n        property_type = property_types[0]\n    else:\n        property_types.sort(key=_sort_property_order)\n        property_type = \"/\".join(property_types)\n    items = property_dict.get(\"items\", {})\n    sub_property_types = items.get(\"type\", [])\n    if not isinstance(sub_property_types, list):\n        sub_property_types = [sub_property_types]\n    # Collect each item type\n    for sub_item in items.get(\"oneOf\", {}):\n        sub_property_types.append(_get_property_type(sub_item, defs))\n    if sub_property_types:\n        if len(sub_property_types) == 1:\n            return f\"{property_type} of {sub_property_types[0]}\"\n        sub_property_types.sort(key=_sort_property_order)\n        sub_property_doc = f\"({'/'.join(sub_property_types)})\"\n        return f\"{property_type} of {sub_property_doc}\"\n    return property_type or \"UNDEFINED\"\n\n\ndef _parse_description(description, prefix) -> str:\n    \"\"\"Parse description from the meta in a format that we can better\n    display in our docs. This parser does three things:\n\n    - Guarantee that a paragraph will be in a single line\n    - Guarantee that each new paragraph will be aligned with\n      the first paragraph\n    - Proper align lists of items\n\n    @param description: The original description in the meta.\n    @param prefix: The number of spaces used to align the current description\n    \"\"\"\n    list_paragraph = prefix * 3\n    description = re.sub(r\"(\\S)\\n(\\S)\", r\"\\1 \\2\", description)\n    description = re.sub(r\"\\n\\n\", r\"\\n\\n{}\".format(prefix), description)\n    description = re.sub(\n        r\"\\n( +)-\", r\"\\n{}-\".format(list_paragraph), description\n    )\n\n    return description\n\n\ndef _flatten_schema_refs(src_cfg: dict, defs: dict):\n    \"\"\"Flatten schema: replace $refs in src_cfg with definitions from $defs.\"\"\"\n    if \"$ref\" in src_cfg:\n        reference = src_cfg.pop(\"$ref\").replace(\"#/$defs/\", \"\")\n        # Update the defined references in subschema for doc rendering\n        src_cfg.update(defs[reference])\n    if \"items\" in src_cfg:\n        if \"$ref\" in src_cfg[\"items\"]:\n            reference = src_cfg[\"items\"].pop(\"$ref\").replace(\"#/$defs/\", \"\")\n            # Update the references in subschema for doc rendering\n            src_cfg[\"items\"].update(defs[reference])\n        if \"oneOf\" in src_cfg[\"items\"]:\n            for alt_schema in src_cfg[\"items\"][\"oneOf\"]:\n                if \"$ref\" in alt_schema:\n                    reference = alt_schema.pop(\"$ref\").replace(\"#/$defs/\", \"\")\n                    alt_schema.update(defs[reference])\n    for alt_schema in src_cfg.get(\"oneOf\", []):\n        if \"$ref\" in alt_schema:\n            reference = alt_schema.pop(\"$ref\").replace(\"#/$defs/\", \"\")\n            alt_schema.update(defs[reference])\n\n\ndef _get_property_doc(schema: dict, defs: dict, prefix=\"    \") -> str:\n    \"\"\"Return restructured text describing the supported schema properties.\"\"\"\n    new_prefix = prefix + \"    \"\n    properties = []\n    if schema.get(\"hidden\") is True:\n        return \"\"  # no docs for this schema\n    property_keys = [\n        key\n        for key in (\"properties\", \"patternProperties\")\n        if \"hidden\" not in schema or key not in schema[\"hidden\"]\n    ]\n    property_schemas = [schema.get(key, {}) for key in property_keys]\n\n    for prop_schema in property_schemas:\n        for prop_key, prop_config in prop_schema.items():\n            _flatten_schema_refs(prop_config, defs)\n            if prop_config.get(\"hidden\") is True:\n                continue  # document nothing for this property\n            # Define prop_name and description for SCHEMA_PROPERTY_TMPL\n            description = prop_config.get(\"description\", \"\")\n            if description:\n                description = \" \" + description\n\n            # Define prop_name and description for SCHEMA_PROPERTY_TMPL\n            label = prop_config.get(\"label\", prop_key)\n            properties.append(\n                SCHEMA_PROPERTY_TMPL.format(\n                    prefix=prefix,\n                    prop_name=label,\n                    description=_parse_description(description, prefix),\n                    prop_type=_get_property_type(prop_config, defs),\n                )\n            )\n            items = prop_config.get(\"items\")\n            if items:\n                _flatten_schema_refs(items, defs)\n                if items.get(\"properties\") or items.get(\"patternProperties\"):\n                    properties.append(\n                        SCHEMA_LIST_ITEM_TMPL.format(\n                            prefix=new_prefix, prop_name=label\n                        )\n                    )\n                    new_prefix += \"    \"\n                    properties.append(\n                        _get_property_doc(items, defs=defs, prefix=new_prefix)\n                    )\n                for alt_schema in items.get(\"oneOf\", []):\n                    if alt_schema.get(\"properties\") or alt_schema.get(\n                        \"patternProperties\"\n                    ):\n                        properties.append(\n                            SCHEMA_LIST_ITEM_TMPL.format(\n                                prefix=new_prefix, prop_name=label\n                            )\n                        )\n                        new_prefix += \"    \"\n                        properties.append(\n                            _get_property_doc(\n                                alt_schema, defs=defs, prefix=new_prefix\n                            )\n                        )\n            if (\n                \"properties\" in prop_config\n                or \"patternProperties\" in prop_config\n            ):\n                properties.append(\n                    _get_property_doc(\n                        prop_config, defs=defs, prefix=new_prefix\n                    )\n                )\n    return \"\\n\\n\".join(properties)\n\n\ndef _get_examples(meta: MetaSchema) -> str:\n    \"\"\"Return restructured text describing the meta examples if present.\"\"\"\n    examples = meta.get(\"examples\")\n    if not examples:\n        return \"\"\n    rst_content = SCHEMA_EXAMPLES_HEADER\n    for count, example in enumerate(examples):\n        indented_lines = textwrap.indent(example, \"    \").split(\"\\n\")\n        if rst_content != SCHEMA_EXAMPLES_HEADER:\n            indented_lines.insert(\n                0, SCHEMA_EXAMPLES_SPACER_TEMPLATE.format(count + 1)\n            )\n        rst_content += \"\\n\".join(indented_lines)\n    return rst_content\n\n\ndef get_meta_doc(meta: MetaSchema, schema: Optional[dict] = None) -> str:\n    \"\"\"Return reStructured text rendering the provided metadata.\n\n    @param meta: Dict of metadata to render.\n    @param schema: Optional module schema, if absent, read global schema.\n    @raise KeyError: If metadata lacks an expected key.\n    \"\"\"\n\n    if schema is None:\n        schema = get_schema()\n    if not meta or not schema:\n        raise ValueError(\"Expected non-empty meta and schema\")\n    keys = set(meta.keys())\n    expected = set(\n        {\n            \"id\",\n            \"title\",\n            \"examples\",\n            \"frequency\",\n            \"distros\",\n            \"description\",\n            \"name\",\n        }\n    )\n    error_message = \"\"\n    if expected - keys:\n        error_message = \"Missing expected keys in module meta: {}\".format(\n            expected - keys\n        )\n    elif keys - expected:\n        error_message = (\n            \"Additional unexpected keys found in module meta: {}\".format(\n                keys - expected\n            )\n        )\n    if error_message:\n        raise KeyError(error_message)\n\n    # cast away type annotation\n    meta_copy = dict(deepcopy(meta))\n    meta_copy[\"property_header\"] = \"\"\n    defs = schema.get(\"$defs\", {})\n    if defs.get(meta[\"id\"]):\n        schema = defs.get(meta[\"id\"], {})\n        schema = cast(dict, schema)\n    try:\n        meta_copy[\"property_doc\"] = _get_property_doc(schema, defs=defs)\n    except AttributeError:\n        LOG.warning(\"Unable to render property_doc due to invalid schema\")\n        meta_copy[\"property_doc\"] = \"\"\n    if meta_copy[\"property_doc\"]:\n        meta_copy[\"property_header\"] = SCHEMA_PROPERTY_HEADER\n    meta_copy[\"examples\"] = _get_examples(meta)\n    meta_copy[\"distros\"] = \", \".join(meta[\"distros\"])\n    # Need an underbar of the same length as the name\n    meta_copy[\"title_underbar\"] = re.sub(r\".\", \"-\", meta[\"name\"])\n    template = SCHEMA_DOC_TMPL.format(**meta_copy)\n    return template\n\n\ndef get_modules() -> dict:\n    configs_dir = os.path.dirname(os.path.abspath(__file__))\n    return get_modules_from_dir(configs_dir)\n\n\ndef load_doc(requested_modules: list) -> str:\n    \"\"\"Load module docstrings\n\n    Docstrings are generated on module load. Reduce, reuse, recycle.\n    \"\"\"\n    docs = \"\"\n    all_modules = list(get_modules().values()) + [\"all\"]\n    invalid_docs = set(requested_modules).difference(set(all_modules))\n    if invalid_docs:\n        error(\n            \"Invalid --docs value {}. Must be one of: {}\".format(\n                list(invalid_docs),\n                \", \".join(all_modules),\n            )\n        )\n    for mod_name in all_modules:\n        if \"all\" in requested_modules or mod_name in requested_modules:\n            (mod_locs, _) = importer.find_module(\n                mod_name, [\"cloudinit.config\"], [\"meta\"]\n            )\n            if mod_locs:\n                mod = importer.import_module(mod_locs[0])\n                docs += mod.__doc__ or \"\"\n    return docs\n\n\ndef get_schema_dir() -> str:\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"schemas\")\n\n\ndef get_schema() -> dict:\n    \"\"\"Return jsonschema coalesced from all cc_* cloud-config modules.\"\"\"\n    # Note versions.schema.json is publicly consumed by schemastore.org.\n    # If we change the location of versions.schema.json in github, we need\n    # to provide an updated PR to\n    # https://github.com/SchemaStore/schemastore.\n\n    # When bumping schema version due to incompatible changes:\n    # 1. Add a new schema-cloud-config-v#.json\n    # 2. change the USERDATA_SCHEMA_FILE to cloud-init-schema-v#.json\n    # 3. Add the new version definition to versions.schema.cloud-config.json\n    schema_file = os.path.join(get_schema_dir(), USERDATA_SCHEMA_FILE)\n    full_schema = None\n    try:\n        full_schema = json.loads(load_file(schema_file))\n    except Exception as e:\n        LOG.warning(\"Cannot parse JSON schema file %s. %s\", schema_file, e)\n    if not full_schema:\n        LOG.warning(\n            \"No base JSON schema files found at %s.\"\n            \" Setting default empty schema\",\n            schema_file,\n        )\n        full_schema = {\n            \"$defs\": {},\n            \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n            \"allOf\": [],\n        }\n    return full_schema\n\n\ndef get_meta() -> dict:\n    \"\"\"Return metadata coalesced from all cc_* cloud-config module.\"\"\"\n    full_meta = dict()\n    for (_, mod_name) in get_modules().items():\n        mod_locs, _ = importer.find_module(\n            mod_name, [\"cloudinit.config\"], [\"meta\"]\n        )\n        if mod_locs:\n            mod = importer.import_module(mod_locs[0])\n            full_meta[mod.meta[\"id\"]] = mod.meta\n    return full_meta\n\n\ndef get_parser(parser=None):\n    \"\"\"Return a parser for supported cmdline arguments.\"\"\"\n    if not parser:\n        parser = argparse.ArgumentParser(\n            prog=\"cloudconfig-schema\",\n            description=\"Validate cloud-config files or document schema\",\n        )\n    parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        help=\"Path of the cloud-config yaml file to validate\",\n    )\n    parser.add_argument(\n        \"--system\",\n        action=\"store_true\",\n        default=False,\n        help=\"Validate the system cloud-config userdata\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--docs\",\n        nargs=\"+\",\n        help=(\n            \"Print schema module docs. Choices: all or\"\n            \" space-delimited cc_names.\"\n        ),\n    )\n    parser.add_argument(\n        \"--annotate\",\n        action=\"store_true\",\n        default=False,\n        help=\"Annotate existing cloud-config file with errors\",\n    )\n    return parser\n\n\ndef handle_schema_args(name, args):\n    \"\"\"Handle provided schema args and perform the appropriate actions.\"\"\"\n    exclusive_args = [args.config_file, args.docs, args.system]\n    if len([arg for arg in exclusive_args if arg]) != 1:\n        error(\"Expected one of --config-file, --system or --docs arguments\")\n    if args.annotate and args.docs:\n        error(\"Invalid flag combination. Cannot use --annotate with --docs\")\n    full_schema = get_schema()\n    if args.config_file or args.system:\n        try:\n            validate_cloudconfig_file(\n                args.config_file, full_schema, args.annotate\n            )\n        except SchemaValidationError as e:\n            if not args.annotate:\n                error(str(e))\n        except RuntimeError as e:\n            error(str(e))\n        else:\n            if args.config_file is None:\n                cfg_name = \"system userdata\"\n            else:\n                cfg_name = args.config_file\n            print(\"Valid cloud-config:\", cfg_name)\n    elif args.docs:\n        print(load_doc(args.docs))\n\n\ndef main():\n    \"\"\"Tool to validate schema of a cloud-config file or print schema docs.\"\"\"\n    parser = get_parser()\n    handle_schema_args(\"cloudconfig-schema\", parser.parse_args())\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\n# vi: ts=4 expandtab\n", "\"\"\"Integration tests for CLI functionality\n\nThese would be for behavior manually invoked by user from the command line\n\"\"\"\n\nimport pytest\n\nfrom tests.integration_tests.instances import IntegrationInstance\n\nVALID_USER_DATA = \"\"\"\\\n#cloud-config\nruncmd:\n  - echo 'hi' > /var/tmp/test\n\"\"\"\n\nINVALID_USER_DATA_HEADER = \"\"\"\\\nruncmd:\n  - echo 'hi' > /var/tmp/test\n\"\"\"\n\nINVALID_USER_DATA_SCHEMA = \"\"\"\\\n#cloud-config\nupdates:\n notnetwork: -1\napt_pipelining: bogus\n\"\"\"\n\n\n@pytest.mark.user_data(VALID_USER_DATA)\ndef test_valid_userdata(client: IntegrationInstance):\n    \"\"\"Test `cloud-init schema` with valid userdata.\n\n    PR #575\n    \"\"\"\n    result = client.execute(\"cloud-init schema --system\")\n    assert result.ok\n    assert \"Valid cloud-config: system userdata\" == result.stdout.strip()\n    result = client.execute(\"cloud-init status --long\")\n    if not result.ok:\n        raise AssertionError(\n            f\"Unexpected error from cloud-init status: {result}\"\n        )\n\n\n@pytest.mark.user_data(INVALID_USER_DATA_HEADER)\ndef test_invalid_userdata(client: IntegrationInstance):\n    \"\"\"Test `cloud-init schema` with invalid userdata.\n\n    PR #575\n    \"\"\"\n    result = client.execute(\"cloud-init schema --system\")\n    assert not result.ok\n    assert \"Cloud config schema errors\" in result.stderr\n    assert 'needs to begin with \"#cloud-config\"' in result.stderr\n    result = client.execute(\"cloud-init status --long\")\n    if not result.ok:\n        raise AssertionError(\n            f\"Unexpected error from cloud-init status: {result}\"\n        )\n\n\n@pytest.mark.user_data(INVALID_USER_DATA_SCHEMA)\ndef test_invalid_userdata_schema(client: IntegrationInstance):\n    \"\"\"Test invalid schema represented as Warnings, not fatal\n\n    PR #1175\n    \"\"\"\n    result = client.execute(\"cloud-init status --long\")\n    assert result.ok\n    log = client.read_from_file(\"/var/log/cloud-init.log\")\n    warning = (\n        \"[WARNING]: Invalid cloud-config provided:\\napt_pipelining: 'bogus'\"\n        \" is not valid under any of the given schemas\\nupdates: Additional\"\n        \" properties are not allowed ('notnetwork' was unexpected)\"\n    )\n    assert warning in log\n    result = client.execute(\"cloud-init status --long\")\n    if not result.ok:\n        raise AssertionError(\n            f\"Unexpected error from cloud-init status: {result}\"\n        )\n", "# This file is part of cloud-init. See LICENSE file for license information.\n\n\nimport importlib\nimport inspect\nimport itertools\nimport json\nimport logging\nimport os\nimport re\nimport sys\nfrom copy import copy\nfrom pathlib import Path\nfrom textwrap import dedent\nfrom types import ModuleType\nfrom typing import List\n\nimport pytest\n\nfrom cloudinit.config.schema import (\n    CLOUD_CONFIG_HEADER,\n    VERSIONED_USERDATA_SCHEMA_FILE,\n    MetaSchema,\n    SchemaValidationError,\n    annotated_cloudconfig_file,\n    get_jsonschema_validator,\n    get_meta_doc,\n    get_schema,\n    get_schema_dir,\n    load_doc,\n    main,\n    validate_cloudconfig_file,\n    validate_cloudconfig_metaschema,\n    validate_cloudconfig_schema,\n)\nfrom cloudinit.distros import OSFAMILIES\nfrom cloudinit.safeyaml import load, load_with_marks\nfrom cloudinit.settings import FREQUENCIES\nfrom cloudinit.util import load_file, write_file\nfrom tests.unittests.helpers import (\n    CiTestCase,\n    cloud_init_project_dir,\n    mock,\n    skipUnlessJsonSchema,\n)\n\n\ndef get_schemas() -> dict:\n    \"\"\"Return all legacy module schemas\n\n    Assumes that module schemas have the variable name \"schema\"\n    \"\"\"\n    return get_module_variable(\"schema\")\n\n\ndef get_metas() -> dict:\n    \"\"\"Return all module metas\n\n    Assumes that module schemas have the variable name \"schema\"\n    \"\"\"\n    return get_module_variable(\"meta\")\n\n\ndef get_module_names() -> List[str]:\n    \"\"\"Return list of module names in cloudinit/config\"\"\"\n    files = list(\n        Path(cloud_init_project_dir(\"cloudinit/config/\")).glob(\"cc_*.py\")\n    )\n\n    return [mod.stem for mod in files]\n\n\ndef get_modules() -> List[ModuleType]:\n    \"\"\"Return list of modules in cloudinit/config\"\"\"\n    return [\n        importlib.import_module(f\"cloudinit.config.{module}\")\n        for module in get_module_names()\n    ]\n\n\ndef get_module_variable(var_name) -> dict:\n    \"\"\"Inspect modules and get variable from module matching var_name\"\"\"\n    schemas: dict = {}\n    get_modules()\n    for k, v in sys.modules.items():\n        path = Path(k)\n        if \"cloudinit.config\" == path.stem and path.suffix[1:4] == \"cc_\":\n            module_name = path.suffix[1:]\n            members = inspect.getmembers(v)\n            schemas[module_name] = None\n            for name, value in members:\n                if name == var_name:\n                    schemas[module_name] = value\n                    break\n    return schemas\n\n\nclass TestVersionedSchemas:\n    @pytest.mark.parametrize(\n        \"schema,error_msg\",\n        (\n            ({}, None),\n            ({\"version\": \"v1\"}, None),\n            ({\"version\": \"v2\"}, \"is not valid\"),\n            ({\"version\": \"v1\", \"final_message\": -1}, \"is not valid\"),\n            ({\"version\": \"v1\", \"final_message\": \"some msg\"}, None),\n        ),\n    )\n    def test_versioned_cloud_config_schema_is_valid_json(\n        self, schema, error_msg\n    ):\n        schema_dir = get_schema_dir()\n        version_schemafile = os.path.join(\n            schema_dir, VERSIONED_USERDATA_SCHEMA_FILE\n        )\n        # Point to local schema files avoid JSON resolver trying to pull the\n        # reference from our upstream raw file in github.\n        version_schema = json.loads(\n            re.sub(\n                r\"https:\\/\\/raw.githubusercontent.com\\/canonical\\/\"\n                r\"cloud-init\\/main\\/cloudinit\\/config\\/schemas\\/\",\n                f\"file://{schema_dir}/\",\n                load_file(version_schemafile),\n            )\n        )\n        if error_msg:\n            with pytest.raises(SchemaValidationError) as context_mgr:\n                validate_cloudconfig_schema(\n                    schema, schema=version_schema, strict=True\n                )\n            assert error_msg in str(context_mgr.value)\n        else:\n            validate_cloudconfig_schema(\n                schema, schema=version_schema, strict=True\n            )\n\n\nclass TestGetSchema:\n    def test_static_schema_file_is_valid(self, caplog):\n        with caplog.at_level(logging.WARNING):\n            get_schema()\n        # Assert no warnings parsing our packaged schema file\n        warnings = [msg for (_, _, msg) in caplog.record_tuples]\n        assert [] == warnings\n\n    def test_get_schema_coalesces_known_schema(self):\n        \"\"\"Every cloudconfig module with schema is listed in allOf keyword.\"\"\"\n        schema = get_schema()\n        assert sorted(get_module_names()) == sorted(\n            [meta[\"id\"] for meta in get_metas().values() if meta is not None]\n        )\n        assert \"http://json-schema.org/draft-04/schema#\" == schema[\"$schema\"]\n        assert [\"$defs\", \"$schema\", \"allOf\"] == sorted(list(schema.keys()))\n        # New style schema should be defined in static schema file in $defs\n        expected_subschema_defs = [\n            {\"$ref\": \"#/$defs/cc_apk_configure\"},\n            {\"$ref\": \"#/$defs/cc_apt_configure\"},\n            {\"$ref\": \"#/$defs/cc_apt_pipelining\"},\n            {\"$ref\": \"#/$defs/cc_bootcmd\"},\n            {\"$ref\": \"#/$defs/cc_byobu\"},\n            {\"$ref\": \"#/$defs/cc_ca_certs\"},\n            {\"$ref\": \"#/$defs/cc_chef\"},\n            {\"$ref\": \"#/$defs/cc_debug\"},\n            {\"$ref\": \"#/$defs/cc_disable_ec2_metadata\"},\n            {\"$ref\": \"#/$defs/cc_disk_setup\"},\n            {\"$ref\": \"#/$defs/cc_fan\"},\n            {\"$ref\": \"#/$defs/cc_final_message\"},\n            {\"$ref\": \"#/$defs/cc_growpart\"},\n            {\"$ref\": \"#/$defs/cc_grub_dpkg\"},\n            {\"$ref\": \"#/$defs/cc_install_hotplug\"},\n            {\"$ref\": \"#/$defs/cc_keyboard\"},\n            {\"$ref\": \"#/$defs/cc_keys_to_console\"},\n            {\"$ref\": \"#/$defs/cc_landscape\"},\n            {\"$ref\": \"#/$defs/cc_locale\"},\n            {\"$ref\": \"#/$defs/cc_lxd\"},\n            {\"$ref\": \"#/$defs/cc_mcollective\"},\n            {\"$ref\": \"#/$defs/cc_migrator\"},\n            {\"$ref\": \"#/$defs/cc_mounts\"},\n            {\"$ref\": \"#/$defs/cc_ntp\"},\n            {\"$ref\": \"#/$defs/cc_package_update_upgrade_install\"},\n            {\"$ref\": \"#/$defs/cc_phone_home\"},\n            {\"$ref\": \"#/$defs/cc_power_state_change\"},\n            {\"$ref\": \"#/$defs/cc_puppet\"},\n            {\"$ref\": \"#/$defs/cc_resizefs\"},\n            {\"$ref\": \"#/$defs/cc_resolv_conf\"},\n            {\"$ref\": \"#/$defs/cc_rh_subscription\"},\n            {\"$ref\": \"#/$defs/cc_rsyslog\"},\n            {\"$ref\": \"#/$defs/cc_runcmd\"},\n            {\"$ref\": \"#/$defs/cc_salt_minion\"},\n            {\"$ref\": \"#/$defs/cc_scripts_vendor\"},\n            {\"$ref\": \"#/$defs/cc_seed_random\"},\n            {\"$ref\": \"#/$defs/cc_set_hostname\"},\n            {\"$ref\": \"#/$defs/cc_set_passwords\"},\n            {\"$ref\": \"#/$defs/cc_snap\"},\n            {\"$ref\": \"#/$defs/cc_spacewalk\"},\n            {\"$ref\": \"#/$defs/cc_ssh_authkey_fingerprints\"},\n            {\"$ref\": \"#/$defs/cc_ssh_import_id\"},\n            {\"$ref\": \"#/$defs/cc_ssh\"},\n            {\"$ref\": \"#/$defs/cc_timezone\"},\n            {\"$ref\": \"#/$defs/cc_ubuntu_advantage\"},\n            {\"$ref\": \"#/$defs/cc_ubuntu_drivers\"},\n            {\"$ref\": \"#/$defs/cc_update_etc_hosts\"},\n            {\"$ref\": \"#/$defs/cc_update_hostname\"},\n            {\"$ref\": \"#/$defs/cc_users_groups\"},\n            {\"$ref\": \"#/$defs/cc_write_files\"},\n            {\"$ref\": \"#/$defs/cc_yum_add_repo\"},\n            {\"$ref\": \"#/$defs/cc_zypper_add_repo\"},\n            {\"$ref\": \"#/$defs/reporting_config\"},\n        ]\n        found_subschema_defs = []\n        legacy_schema_keys = []\n        for subschema in schema[\"allOf\"]:\n            if \"$ref\" in subschema:\n                found_subschema_defs.append(subschema)\n            else:  # Legacy subschema sourced from cc_* module 'schema' attr\n                legacy_schema_keys.extend(subschema[\"properties\"].keys())\n\n        assert expected_subschema_defs == found_subschema_defs\n        # This list should remain empty unless we induct new modules with\n        # legacy schema attributes defined within the cc_module.\n        assert [] == sorted(legacy_schema_keys)\n\n\nclass TestLoadDoc:\n\n    docs = get_module_variable(\"__doc__\")\n\n    @pytest.mark.parametrize(\n        \"module_name\",\n        (\"cc_apt_pipelining\",),  # new style composite schema file\n    )\n    def test_report_docs_consolidated_schema(self, module_name):\n        doc = load_doc([module_name])\n        assert doc, \"Unexpected empty docs for {}\".format(module_name)\n        assert self.docs[module_name] == doc\n\n\nclass SchemaValidationErrorTest(CiTestCase):\n    \"\"\"Test validate_cloudconfig_schema\"\"\"\n\n    def test_schema_validation_error_expects_schema_errors(self):\n        \"\"\"SchemaValidationError is initialized from schema_errors.\"\"\"\n        errors = (\n            (\"key.path\", 'unexpected key \"junk\"'),\n            (\"key2.path\", '\"-123\" is not a valid \"hostname\" format'),\n        )\n        exception = SchemaValidationError(schema_errors=errors)\n        self.assertIsInstance(exception, Exception)\n        self.assertEqual(exception.schema_errors, errors)\n        self.assertEqual(\n            'Cloud config schema errors: key.path: unexpected key \"junk\", '\n            'key2.path: \"-123\" is not a valid \"hostname\" format',\n            str(exception),\n        )\n        self.assertTrue(isinstance(exception, ValueError))\n\n\nclass TestValidateCloudConfigSchema:\n    \"\"\"Tests for validate_cloudconfig_schema.\"\"\"\n\n    with_logs = True\n\n    @pytest.mark.parametrize(\n        \"schema, call_count\",\n        ((None, 1), ({\"properties\": {\"p1\": {\"type\": \"string\"}}}, 0)),\n    )\n    @skipUnlessJsonSchema()\n    @mock.patch(\"cloudinit.config.schema.get_schema\")\n    def test_validateconfig_schema_use_full_schema_when_no_schema_param(\n        self, get_schema, schema, call_count\n    ):\n        \"\"\"Use full schema when schema param is absent.\"\"\"\n        get_schema.return_value = {\"properties\": {\"p1\": {\"type\": \"string\"}}}\n        kwargs = {\"config\": {\"p1\": \"valid\"}}\n        if schema:\n            kwargs[\"schema\"] = schema\n        validate_cloudconfig_schema(**kwargs)\n        assert call_count == get_schema.call_count\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_non_strict_emits_warnings(self, caplog):\n        \"\"\"When strict is False validate_cloudconfig_schema emits warnings.\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\"}}}\n        validate_cloudconfig_schema({\"p1\": -1}, schema, strict=False)\n        [(module, log_level, log_msg)] = caplog.record_tuples\n        assert \"cloudinit.config.schema\" == module\n        assert logging.WARNING == log_level\n        assert (\n            \"Invalid cloud-config provided:\\np1: -1 is not of type 'string'\"\n            == log_msg\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_emits_warning_on_missing_jsonschema(\n        self, caplog\n    ):\n        \"\"\"Warning from validate_cloudconfig_schema when missing jsonschema.\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\"}}}\n        with mock.patch.dict(\"sys.modules\", **{\"jsonschema\": ImportError()}):\n            validate_cloudconfig_schema({\"p1\": -1}, schema, strict=True)\n        assert \"Ignoring schema validation. jsonschema is not present\" in (\n            caplog.text\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_strict_raises_errors(self):\n        \"\"\"When strict is True validate_cloudconfig_schema raises errors.\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\"}}}\n        with pytest.raises(SchemaValidationError) as context_mgr:\n            validate_cloudconfig_schema({\"p1\": -1}, schema, strict=True)\n        assert (\n            \"Cloud config schema errors: p1: -1 is not of type 'string'\"\n            == (str(context_mgr.value))\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_honors_formats(self):\n        \"\"\"With strict True, validate_cloudconfig_schema errors on format.\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\", \"format\": \"email\"}}}\n        with pytest.raises(SchemaValidationError) as context_mgr:\n            validate_cloudconfig_schema({\"p1\": \"-1\"}, schema, strict=True)\n        assert \"Cloud config schema errors: p1: '-1' is not a 'email'\" == (\n            str(context_mgr.value)\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_honors_formats_strict_metaschema(self):\n        \"\"\"With strict and strict_metaschema True, ensure errors on format\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\", \"format\": \"email\"}}}\n        with pytest.raises(SchemaValidationError) as context_mgr:\n            validate_cloudconfig_schema(\n                {\"p1\": \"-1\"}, schema, strict=True, strict_metaschema=True\n            )\n        assert \"Cloud config schema errors: p1: '-1' is not a 'email'\" == str(\n            context_mgr.value\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_strict_metaschema_do_not_raise_exception(\n        self, caplog\n    ):\n        \"\"\"With strict_metaschema=True, do not raise exceptions.\n\n        This flag is currently unused, but is intended for run-time validation.\n        This should warn, but not raise.\n        \"\"\"\n        schema = {\"properties\": {\"p1\": {\"types\": \"string\", \"format\": \"email\"}}}\n        validate_cloudconfig_schema(\n            {\"p1\": \"-1\"}, schema, strict_metaschema=True\n        )\n        assert (\n            \"Meta-schema validation failed, attempting to validate config\"\n            in caplog.text\n        )\n\n\nclass TestCloudConfigExamples:\n    metas = get_metas()\n    params = [\n        (meta[\"id\"], example)\n        for meta in metas.values()\n        if meta and meta.get(\"examples\")\n        for example in meta.get(\"examples\")\n    ]\n\n    @pytest.mark.parametrize(\"schema_id, example\", params)\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_of_example(self, schema_id, example):\n        \"\"\"For a given example in a config module we test if it is valid\n        according to the unified schema of all config modules\n        \"\"\"\n        schema = get_schema()\n        config_load = load(example)\n        # cloud-init-schema-v1 is permissive of additionalProperties at the\n        # top-level.\n        # To validate specific schemas against known documented examples\n        # we need to only define the specific module schema and supply\n        # strict=True.\n        # TODO(Drop to pop/update once full schema is strict)\n        schema.pop(\"allOf\")\n        schema.update(schema[\"$defs\"][schema_id])\n        schema[\"additionalProperties\"] = False\n        # Some module examples reference keys defined in multiple schemas\n        supplemental_schemas = {\n            \"cc_ubuntu_advantage\": [\"cc_power_state_change\"],\n            \"cc_update_hostname\": [\"cc_set_hostname\"],\n            \"cc_users_groups\": [\"cc_ssh_import_id\"],\n            \"cc_disk_setup\": [\"cc_mounts\"],\n        }\n        for supplement_id in supplemental_schemas.get(schema_id, []):\n            supplemental_props = dict(\n                [\n                    (key, value)\n                    for key, value in schema[\"$defs\"][supplement_id][\n                        \"properties\"\n                    ].items()\n                ]\n            )\n            schema[\"properties\"].update(supplemental_props)\n        validate_cloudconfig_schema(config_load, schema, strict=True)\n\n\nclass TestValidateCloudConfigFile:\n    \"\"\"Tests for validate_cloudconfig_file.\"\"\"\n\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_error_on_absent_file(self, annotate):\n        \"\"\"On absent config_path, validate_cloudconfig_file errors.\"\"\"\n        with pytest.raises(\n            RuntimeError, match=\"Configfile /not/here does not exist\"\n        ):\n            validate_cloudconfig_file(\"/not/here\", {}, annotate)\n\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_error_on_invalid_header(\n        self, annotate, tmpdir\n    ):\n        \"\"\"On invalid header, validate_cloudconfig_file errors.\n\n        A SchemaValidationError is raised when the file doesn't begin with\n        CLOUD_CONFIG_HEADER.\n        \"\"\"\n        config_file = tmpdir.join(\"my.yaml\")\n        config_file.write(\"#junk\")\n        error_msg = (\n            \"Cloud config schema errors: format-l1.c1: File\"\n            f\" {config_file} needs to begin with\"\n            f' \"{CLOUD_CONFIG_HEADER.decode()}\"'\n        )\n        with pytest.raises(SchemaValidationError, match=error_msg):\n            validate_cloudconfig_file(config_file.strpath, {}, annotate)\n\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_error_on_non_yaml_scanner_error(\n        self, annotate, tmpdir\n    ):\n        \"\"\"On non-yaml scan issues, validate_cloudconfig_file errors.\"\"\"\n        # Generate a scanner error by providing text on a single line with\n        # improper indent.\n        config_file = tmpdir.join(\"my.yaml\")\n        config_file.write(\"#cloud-config\\nasdf:\\nasdf\")\n        error_msg = (\n            f\".*errors: format-l3.c1: File {config_file} is not valid yaml.*\"\n        )\n        with pytest.raises(SchemaValidationError, match=error_msg):\n            validate_cloudconfig_file(config_file.strpath, {}, annotate)\n\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_error_on_non_yaml_parser_error(\n        self, annotate, tmpdir\n    ):\n        \"\"\"On non-yaml parser issues, validate_cloudconfig_file errors.\"\"\"\n        config_file = tmpdir.join(\"my.yaml\")\n        config_file.write(\"#cloud-config\\n{}}\")\n        error_msg = (\n            f\"errors: format-l2.c3: File {config_file} is not valid yaml.\"\n        )\n        with pytest.raises(SchemaValidationError, match=error_msg):\n            validate_cloudconfig_file(config_file.strpath, {}, annotate)\n\n    @skipUnlessJsonSchema()\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_sctrictly_validates_schema(\n        self, annotate, tmpdir\n    ):\n        \"\"\"validate_cloudconfig_file raises errors on invalid schema.\"\"\"\n        config_file = tmpdir.join(\"my.yaml\")\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\", \"format\": \"string\"}}}\n        config_file.write(\"#cloud-config\\np1: -1\")\n        error_msg = (\n            \"Cloud config schema errors: p1: -1 is not of type 'string'\"\n        )\n        with pytest.raises(SchemaValidationError, match=error_msg):\n            validate_cloudconfig_file(config_file.strpath, schema, annotate)\n\n\nclass TestSchemaDocMarkdown:\n    \"\"\"Tests for get_meta_doc.\"\"\"\n\n    required_schema = {\n        \"title\": \"title\",\n        \"description\": \"description\",\n        \"id\": \"id\",\n        \"name\": \"name\",\n        \"frequency\": \"frequency\",\n        \"distros\": [\"debian\", \"rhel\"],\n    }\n    meta: MetaSchema = {\n        \"title\": \"title\",\n        \"description\": \"description\",\n        \"id\": \"id\",\n        \"name\": \"name\",\n        \"frequency\": \"frequency\",\n        \"distros\": [\"debian\", \"rhel\"],\n        \"examples\": [\n            'ex1:\\n    [don\\'t, expand, \"this\"]',\n            \"ex2: true\",\n        ],\n    }\n\n    def test_get_meta_doc_returns_restructured_text(self):\n        \"\"\"get_meta_doc returns restructured text for a cloudinit schema.\"\"\"\n        full_schema = copy(self.required_schema)\n        full_schema.update(\n            {\n                \"properties\": {\n                    \"prop1\": {\n                        \"type\": \"array\",\n                        \"description\": \"prop-description\",\n                        \"items\": {\"type\": \"integer\"},\n                    }\n                }\n            }\n        )\n\n        doc = get_meta_doc(self.meta, full_schema)\n        assert (\n            dedent(\n                \"\"\"\n            name\n            ----\n            **Summary:** title\n\n            description\n\n            **Internal name:** ``id``\n\n            **Module frequency:** frequency\n\n            **Supported distros:** debian, rhel\n\n            **Config schema**:\n                **prop1:** (array of integer) prop-description\n\n            **Examples**::\n\n                ex1:\n                    [don't, expand, \"this\"]\n                # --- Example2 ---\n                ex2: true\n        \"\"\"\n            )\n            == doc\n        )\n\n    def test_get_meta_doc_handles_multiple_types(self):\n        \"\"\"get_meta_doc delimits multiple property types with a '/'.\"\"\"\n        schema = {\"properties\": {\"prop1\": {\"type\": [\"string\", \"integer\"]}}}\n        assert \"**prop1:** (string/integer)\" in get_meta_doc(self.meta, schema)\n\n    def test_references_are_flattened_in_schema_docs(self):\n        \"\"\"get_meta_doc flattens and renders full schema definitions.\"\"\"\n        schema = {\n            \"$defs\": {\n                \"flattenit\": {\n                    \"type\": [\"object\", \"string\"],\n                    \"description\": \"Objects support the following keys:\",\n                    \"patternProperties\": {\n                        \"^.+$\": {\n                            \"label\": \"<opaque_label>\",\n                            \"description\": \"List of cool strings\",\n                            \"type\": \"array\",\n                            \"items\": {\"type\": \"string\"},\n                            \"minItems\": 1,\n                        }\n                    },\n                }\n            },\n            \"properties\": {\"prop1\": {\"$ref\": \"#/$defs/flattenit\"}},\n        }\n        assert (\n            dedent(\n                \"\"\"\\\n            **prop1:** (string/object) Objects support the following keys:\n\n                    **<opaque_label>:** (array of string) List of cool strings\n            \"\"\"\n            )\n            in get_meta_doc(self.meta, schema)\n        )\n\n    @pytest.mark.parametrize(\n        \"sub_schema,expected\",\n        (\n            (\n                {\"enum\": [True, False, \"stuff\"]},\n                \"**prop1:** (``true``/``false``/``stuff``)\",\n            ),\n            # When type: string and enum, document enum values\n            (\n                {\"type\": \"string\", \"enum\": [\"a\", \"b\"]},\n                \"**prop1:** (``a``/``b``)\",\n            ),\n        ),\n    )\n    def test_get_meta_doc_handles_enum_types(self, sub_schema, expected):\n        \"\"\"get_meta_doc converts enum types to yaml and delimits with '/'.\"\"\"\n        schema = {\"properties\": {\"prop1\": sub_schema}}\n        assert expected in get_meta_doc(self.meta, schema)\n\n    @pytest.mark.parametrize(\n        \"schema,expected\",\n        (\n            (  # Hide top-level keys like 'properties'\n                {\n                    \"hidden\": [\"properties\"],\n                    \"properties\": {\n                        \"p1\": {\"type\": \"string\"},\n                        \"p2\": {\"type\": \"boolean\"},\n                    },\n                    \"patternProperties\": {\n                        \"^.*$\": {\n                            \"type\": \"string\",\n                            \"label\": \"label2\",\n                        }\n                    },\n                },\n                dedent(\n                    \"\"\"\n                **Config schema**:\n                    **label2:** (string)\n                \"\"\"\n                ),\n            ),\n            (  # Hide nested individual keys with a bool\n                {\n                    \"properties\": {\n                        \"p1\": {\"type\": \"string\", \"hidden\": True},\n                        \"p2\": {\"type\": \"boolean\"},\n                    }\n                },\n                dedent(\n                    \"\"\"\n                **Config schema**:\n                    **p2:** (boolean)\n                \"\"\"\n                ),\n            ),\n        ),\n    )\n    def test_get_meta_doc_hidden_hides_specific_properties_from_docs(\n        self, schema, expected\n    ):\n        \"\"\"Docs are hidden for any property in the hidden list.\n\n        Useful for hiding deprecated key schema.\n        \"\"\"\n        assert expected in get_meta_doc(self.meta, schema)\n\n    def test_get_meta_doc_handles_nested_oneof_property_types(self):\n        \"\"\"get_meta_doc describes array items oneOf declarations in type.\"\"\"\n        schema = {\n            \"properties\": {\n                \"prop1\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"oneOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]\n                    },\n                }\n            }\n        }\n        assert \"**prop1:** (array of (string/integer))\" in get_meta_doc(\n            self.meta, schema\n        )\n\n    def test_get_meta_doc_handles_types_as_list(self):\n        \"\"\"get_meta_doc renders types which have a list value.\"\"\"\n        schema = {\n            \"properties\": {\n                \"prop1\": {\n                    \"type\": [\"boolean\", \"array\"],\n                    \"items\": {\n                        \"oneOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]\n                    },\n                }\n            }\n        }\n        assert (\n            \"**prop1:** (boolean/array of (string/integer))\"\n            in get_meta_doc(self.meta, schema)\n        )\n\n    def test_get_meta_doc_handles_flattening_defs(self):\n        \"\"\"get_meta_doc renders $defs.\"\"\"\n        schema = {\n            \"$defs\": {\n                \"prop1object\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"subprop\": {\"type\": \"string\"}},\n                }\n            },\n            \"properties\": {\"prop1\": {\"$ref\": \"#/$defs/prop1object\"}},\n        }\n        assert (\n            \"**prop1:** (object)\\n\\n        **subprop:** (string)\\n\"\n            in get_meta_doc(self.meta, schema)\n        )\n\n    def test_get_meta_doc_handles_string_examples(self):\n        \"\"\"get_meta_doc properly indented examples as a list of strings.\"\"\"\n        full_schema = copy(self.required_schema)\n        full_schema.update(\n            {\n                \"examples\": [\n                    'ex1:\\n    [don\\'t, expand, \"this\"]',\n                    \"ex2: true\",\n                ],\n                \"properties\": {\n                    \"prop1\": {\n                        \"type\": \"array\",\n                        \"description\": \"prop-description\",\n                        \"items\": {\"type\": \"integer\"},\n                    }\n                },\n            }\n        )\n        assert (\n            dedent(\n                \"\"\"\n            **Config schema**:\n                **prop1:** (array of integer) prop-description\n\n            **Examples**::\n\n                ex1:\n                    [don't, expand, \"this\"]\n                # --- Example2 ---\n                ex2: true\n            \"\"\"\n            )\n            in get_meta_doc(self.meta, full_schema)\n        )\n\n    def test_get_meta_doc_properly_parse_description(self):\n        \"\"\"get_meta_doc description properly formatted\"\"\"\n        schema = {\n            \"properties\": {\n                \"p1\": {\n                    \"type\": \"string\",\n                    \"description\": dedent(\n                        \"\"\"\\\n                        This item\n                        has the\n                        following options:\n\n                          - option1\n                          - option2\n                          - option3\n\n                        The default value is\n                        option1\"\"\"\n                    ),\n                }\n            }\n        }\n\n        assert (\n            dedent(\n                \"\"\"\n            **Config schema**:\n                **p1:** (string) This item has the following options:\n\n                        - option1\n                        - option2\n                        - option3\n\n                The default value is option1\n\n        \"\"\"\n            )\n            in get_meta_doc(self.meta, schema)\n        )\n\n    def test_get_meta_doc_raises_key_errors(self):\n        \"\"\"get_meta_doc raises KeyErrors on missing keys.\"\"\"\n        schema = {\n            \"properties\": {\n                \"prop1\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"oneOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]\n                    },\n                }\n            }\n        }\n        for key in self.meta:\n            invalid_meta = copy(self.meta)\n            invalid_meta.pop(key)\n            with pytest.raises(KeyError) as context_mgr:\n                get_meta_doc(invalid_meta, schema)\n            assert key in str(context_mgr.value)\n\n    def test_label_overrides_property_name(self):\n        \"\"\"get_meta_doc overrides property name with label.\"\"\"\n        schema = {\n            \"properties\": {\n                \"prop1\": {\n                    \"type\": \"string\",\n                    \"label\": \"label1\",\n                },\n                \"prop_no_label\": {\n                    \"type\": \"string\",\n                },\n                \"prop_array\": {\n                    \"label\": \"array_label\",\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"some_prop\": {\"type\": \"number\"},\n                        },\n                    },\n                },\n            },\n            \"patternProperties\": {\n                \"^.*$\": {\n                    \"type\": \"string\",\n                    \"label\": \"label2\",\n                }\n            },\n        }\n        meta_doc = get_meta_doc(self.meta, schema)\n        assert \"**label1:** (string)\" in meta_doc\n        assert \"**label2:** (string\" in meta_doc\n        assert \"**prop_no_label:** (string)\" in meta_doc\n        assert \"Each object in **array_label** list\" in meta_doc\n\n        assert \"prop1\" not in meta_doc\n        assert \".*\" not in meta_doc\n\n\nclass TestAnnotatedCloudconfigFile:\n    def test_annotated_cloudconfig_file_no_schema_errors(self):\n        \"\"\"With no schema_errors, print the original content.\"\"\"\n        content = b\"ntp:\\n  pools: [ntp1.pools.com]\\n\"\n        parse_cfg, schemamarks = load_with_marks(content)\n        assert content == annotated_cloudconfig_file(\n            parse_cfg, content, schema_errors=[], schemamarks=schemamarks\n        )\n\n    def test_annotated_cloudconfig_file_with_non_dict_cloud_config(self):\n        \"\"\"Error when empty non-dict cloud-config is provided.\n\n        OurJSON validation when user-data is None type generates a bunch\n        schema validation errors of the format:\n        ('', \"None is not of type 'object'\"). Ignore those symptoms and\n        report the general problem instead.\n        \"\"\"\n        content = b\"\\n\\n\\n\"\n        expected = \"\\n\".join(\n            [\n                content.decode(),\n                \"# Errors: -------------\",\n                \"# E1: Cloud-config is not a YAML dict.\\n\\n\",\n            ]\n        )\n        assert expected == annotated_cloudconfig_file(\n            None,\n            content,\n            schema_errors=[(\"\", \"None is not of type 'object'\")],\n            schemamarks={},\n        )\n\n    def test_annotated_cloudconfig_file_schema_annotates_and_adds_footer(self):\n        \"\"\"With schema_errors, error lines are annotated and a footer added.\"\"\"\n        content = dedent(\n            \"\"\"\\\n            #cloud-config\n            # comment\n            ntp:\n              pools: [-99, 75]\n            \"\"\"\n        ).encode()\n        expected = dedent(\n            \"\"\"\\\n            #cloud-config\n            # comment\n            ntp:\t\t# E1\n              pools: [-99, 75]\t\t# E2,E3\n\n            # Errors: -------------\n            # E1: Some type error\n            # E2: -99 is not a string\n            # E3: 75 is not a string\n\n            \"\"\"\n        )\n        parsed_config, schemamarks = load_with_marks(content[13:])\n        schema_errors = [\n            (\"ntp\", \"Some type error\"),\n            (\"ntp.pools.0\", \"-99 is not a string\"),\n            (\"ntp.pools.1\", \"75 is not a string\"),\n        ]\n        assert expected == annotated_cloudconfig_file(\n            parsed_config, content, schema_errors, schemamarks=schemamarks\n        )\n\n    def test_annotated_cloudconfig_file_annotates_separate_line_items(self):\n        \"\"\"Errors are annotated for lists with items on separate lines.\"\"\"\n        content = dedent(\n            \"\"\"\\\n            #cloud-config\n            # comment\n            ntp:\n              pools:\n                - -99\n                - 75\n            \"\"\"\n        ).encode()\n        expected = dedent(\n            \"\"\"\\\n            ntp:\n              pools:\n                - -99\t\t# E1\n                - 75\t\t# E2\n            \"\"\"\n        )\n        parsed_config, schemamarks = load_with_marks(content[13:])\n        schema_errors = [\n            (\"ntp.pools.0\", \"-99 is not a string\"),\n            (\"ntp.pools.1\", \"75 is not a string\"),\n        ]\n        assert expected in annotated_cloudconfig_file(\n            parsed_config, content, schema_errors, schemamarks=schemamarks\n        )\n\n\nclass TestMain:\n\n    exclusive_combinations = itertools.combinations(\n        [\"--system\", \"--docs all\", \"--config-file something\"], 2\n    )\n\n    @pytest.mark.parametrize(\"params\", exclusive_combinations)\n    def test_main_exclusive_args(self, params, capsys):\n        \"\"\"Main exits non-zero and error on required exclusive args.\"\"\"\n        params = list(itertools.chain(*[a.split() for a in params]))\n        with mock.patch(\"sys.argv\", [\"mycmd\"] + params):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n\n        _out, err = capsys.readouterr()\n        expected = (\n            \"Error:\\n\"\n            \"Expected one of --config-file, --system or --docs arguments\\n\"\n        )\n        assert expected == err\n\n    def test_main_missing_args(self, capsys):\n        \"\"\"Main exits non-zero and reports an error on missing parameters.\"\"\"\n        with mock.patch(\"sys.argv\", [\"mycmd\"]):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n\n        _out, err = capsys.readouterr()\n        expected = (\n            \"Error:\\n\"\n            \"Expected one of --config-file, --system or --docs arguments\\n\"\n        )\n        assert expected == err\n\n    def test_main_absent_config_file(self, capsys):\n        \"\"\"Main exits non-zero when config file is absent.\"\"\"\n        myargs = [\"mycmd\", \"--annotate\", \"--config-file\", \"NOT_A_FILE\"]\n        with mock.patch(\"sys.argv\", myargs):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n        _out, err = capsys.readouterr()\n        assert \"Error:\\nConfigfile NOT_A_FILE does not exist\\n\" == err\n\n    def test_main_invalid_flag_combo(self, capsys):\n        \"\"\"Main exits non-zero when invalid flag combo used.\"\"\"\n        myargs = [\"mycmd\", \"--annotate\", \"--docs\", \"DOES_NOT_MATTER\"]\n        with mock.patch(\"sys.argv\", myargs):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n        _, err = capsys.readouterr()\n        assert (\n            \"Error:\\nInvalid flag combination. \"\n            \"Cannot use --annotate with --docs\\n\" == err\n        )\n\n    def test_main_prints_docs(self, capsys):\n        \"\"\"When --docs parameter is provided, main generates documentation.\"\"\"\n        myargs = [\"mycmd\", \"--docs\", \"all\"]\n        with mock.patch(\"sys.argv\", myargs):\n            assert 0 == main(), \"Expected 0 exit code\"\n        out, _err = capsys.readouterr()\n        assert \"\\nNTP\\n---\\n\" in out\n        assert \"\\nRuncmd\\n------\\n\" in out\n\n    def test_main_validates_config_file(self, tmpdir, capsys):\n        \"\"\"When --config-file parameter is provided, main validates schema.\"\"\"\n        myyaml = tmpdir.join(\"my.yaml\")\n        myargs = [\"mycmd\", \"--config-file\", myyaml.strpath]\n        myyaml.write(b\"#cloud-config\\nntp:\")  # shortest ntp schema\n        with mock.patch(\"sys.argv\", myargs):\n            assert 0 == main(), \"Expected 0 exit code\"\n        out, _err = capsys.readouterr()\n        assert \"Valid cloud-config: {0}\\n\".format(myyaml) == out\n\n    @mock.patch(\"cloudinit.config.schema.read_cfg_paths\")\n    @mock.patch(\"cloudinit.config.schema.os.getuid\", return_value=0)\n    def test_main_validates_system_userdata(\n        self, m_getuid, m_read_cfg_paths, capsys, paths\n    ):\n        \"\"\"When --system is provided, main validates system userdata.\"\"\"\n        m_read_cfg_paths.return_value = paths\n        ud_file = paths.get_ipath_cur(\"userdata_raw\")\n        write_file(ud_file, b\"#cloud-config\\nntp:\")\n        myargs = [\"mycmd\", \"--system\"]\n        with mock.patch(\"sys.argv\", myargs):\n            assert 0 == main(), \"Expected 0 exit code\"\n        out, _err = capsys.readouterr()\n        assert \"Valid cloud-config: system userdata\\n\" == out\n\n    @mock.patch(\"cloudinit.config.schema.os.getuid\", return_value=1000)\n    def test_main_system_userdata_requires_root(self, m_getuid, capsys, paths):\n        \"\"\"Non-root user can't use --system param\"\"\"\n        myargs = [\"mycmd\", \"--system\"]\n        with mock.patch(\"sys.argv\", myargs):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n        _out, err = capsys.readouterr()\n        expected = (\n            \"Error:\\nUnable to read system userdata as non-root user. \"\n            \"Try using sudo\\n\"\n        )\n        assert expected == err\n\n\ndef _get_meta_doc_examples():\n    examples_dir = Path(cloud_init_project_dir(\"doc/examples\"))\n    assert examples_dir.is_dir()\n\n    return (\n        str(f)\n        for f in examples_dir.glob(\"cloud-config*.txt\")\n        if not f.name.startswith(\"cloud-config-archive\")\n    )\n\n\nclass TestSchemaDocExamples:\n    schema = get_schema()\n\n    @pytest.mark.parametrize(\"example_path\", _get_meta_doc_examples())\n    @skipUnlessJsonSchema()\n    def test_schema_doc_examples(self, example_path):\n        validate_cloudconfig_file(example_path, self.schema)\n\n\nclass TestStrictMetaschema:\n    \"\"\"Validate that schemas follow a stricter metaschema definition than\n    the default. This disallows arbitrary key/value pairs.\n    \"\"\"\n\n    @skipUnlessJsonSchema()\n    def test_modules(self):\n        \"\"\"Validate all modules with a stricter metaschema\"\"\"\n        (validator, _) = get_jsonschema_validator()\n        for (name, value) in get_schemas().items():\n            if value:\n                validate_cloudconfig_metaschema(validator, value)\n            else:\n                logging.warning(\"module %s has no schema definition\", name)\n\n    @skipUnlessJsonSchema()\n    def test_validate_bad_module(self):\n        \"\"\"Throw exception by default, don't throw if throw=False\n\n        item should be 'items' and is therefore interpreted as an additional\n        property which is invalid with a strict metaschema\n        \"\"\"\n        (validator, _) = get_jsonschema_validator()\n        schema = {\n            \"type\": \"array\",\n            \"item\": {\n                \"type\": \"object\",\n            },\n        }\n        with pytest.raises(\n            SchemaValidationError,\n            match=r\"Additional properties are not allowed.*\",\n        ):\n\n            validate_cloudconfig_metaschema(validator, schema)\n\n        validate_cloudconfig_metaschema(validator, schema, throw=False)\n\n\nclass TestMeta:\n    def test_valid_meta_for_every_module(self):\n        all_distros = {\n            name for distro in OSFAMILIES.values() for name in distro\n        }\n        all_distros.add(\"all\")\n        for module in get_modules():\n            assert \"frequency\" in module.meta\n            assert \"distros\" in module.meta\n            assert {module.meta[\"frequency\"]}.issubset(FREQUENCIES)\n            assert set(module.meta[\"distros\"]).issubset(all_distros)\n"], "fixing_code": ["#!/usr/bin/env python3\n\n# Copyright (C) 2012 Canonical Ltd.\n# Copyright (C) 2012 Hewlett-Packard Development Company, L.P.\n# Copyright (C) 2012 Yahoo! Inc.\n# Copyright (C) 2017 Amazon.com, Inc. or its affiliates\n#\n# Author: Scott Moser <scott.moser@canonical.com>\n# Author: Juerg Haefliger <juerg.haefliger@hp.com>\n# Author: Joshua Harlow <harlowja@yahoo-inc.com>\n# Author: Andrew Jorgensen <ajorgens@amazon.com>\n#\n# This file is part of cloud-init. See LICENSE file for license information.\n\n# Skip isort on this file because of the patch that comes between imports\n# isort: skip_file\n\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nimport traceback\n\nfrom cloudinit import patcher\nfrom cloudinit.config.modules import Modules\n\npatcher.patch_logging()\n\nfrom cloudinit.config.schema import validate_cloudconfig_schema\nfrom cloudinit import log as logging\nfrom cloudinit import netinfo\nfrom cloudinit import signal_handler\nfrom cloudinit import sources\nfrom cloudinit import stages\nfrom cloudinit import url_helper\nfrom cloudinit import util\nfrom cloudinit import version\nfrom cloudinit import warnings\n\nfrom cloudinit import reporting\nfrom cloudinit.reporting import events\n\nfrom cloudinit.settings import PER_INSTANCE, PER_ALWAYS, PER_ONCE, CLOUD_CONFIG\n\nfrom cloudinit import atomic_helper\n\nfrom cloudinit.config import cc_set_hostname\nfrom cloudinit import dhclient_hook\nfrom cloudinit.cmd.devel import read_cfg_paths\n\n\n# Welcome message template\nWELCOME_MSG_TPL = (\n    \"Cloud-init v. {version} running '{action}' at \"\n    \"{timestamp}. Up {uptime} seconds.\"\n)\n\n# Module section template\nMOD_SECTION_TPL = \"cloud_%s_modules\"\n\n# Frequency shortname to full name\n# (so users don't have to remember the full name...)\nFREQ_SHORT_NAMES = {\n    \"instance\": PER_INSTANCE,\n    \"always\": PER_ALWAYS,\n    \"once\": PER_ONCE,\n}\n\nLOG = logging.getLogger()\n\n\n# Used for when a logger may not be active\n# and we still want to print exceptions...\ndef print_exc(msg=\"\"):\n    if msg:\n        sys.stderr.write(\"%s\\n\" % (msg))\n    sys.stderr.write(\"-\" * 60)\n    sys.stderr.write(\"\\n\")\n    traceback.print_exc(file=sys.stderr)\n    sys.stderr.write(\"-\" * 60)\n    sys.stderr.write(\"\\n\")\n\n\ndef welcome(action, msg=None):\n    if not msg:\n        msg = welcome_format(action)\n    util.multi_log(\"%s\\n\" % (msg), console=False, stderr=True, log=LOG)\n    return msg\n\n\ndef welcome_format(action):\n    return WELCOME_MSG_TPL.format(\n        version=version.version_string(),\n        uptime=util.uptime(),\n        timestamp=util.time_rfc2822(),\n        action=action,\n    )\n\n\ndef extract_fns(args):\n    # Files are already opened so lets just pass that along\n    # since it would of broke if it couldn't have\n    # read that file already...\n    fn_cfgs = []\n    if args.files:\n        for fh in args.files:\n            # The realpath is more useful in logging\n            # so lets resolve to that...\n            fn_cfgs.append(os.path.realpath(fh.name))\n    return fn_cfgs\n\n\ndef run_module_section(mods: Modules, action_name, section):\n    full_section_name = MOD_SECTION_TPL % (section)\n    (which_ran, failures) = mods.run_section(full_section_name)\n    total_attempted = len(which_ran) + len(failures)\n    if total_attempted == 0:\n        msg = \"No '%s' modules to run under section '%s'\" % (\n            action_name,\n            full_section_name,\n        )\n        sys.stderr.write(\"%s\\n\" % (msg))\n        LOG.debug(msg)\n        return []\n    else:\n        LOG.debug(\n            \"Ran %s modules with %s failures\", len(which_ran), len(failures)\n        )\n        return failures\n\n\ndef apply_reporting_cfg(cfg):\n    if cfg.get(\"reporting\"):\n        reporting.update_configuration(cfg.get(\"reporting\"))\n\n\ndef parse_cmdline_url(cmdline, names=(\"cloud-config-url\", \"url\")):\n    data = util.keyval_str_to_dict(cmdline)\n    for key in names:\n        if key in data:\n            return key, data[key]\n    raise KeyError(\"No keys (%s) found in string '%s'\" % (cmdline, names))\n\n\ndef attempt_cmdline_url(path, network=True, cmdline=None):\n    \"\"\"Write data from url referenced in command line to path.\n\n    path: a file to write content to if downloaded.\n    network: should network access be assumed.\n    cmdline: the cmdline to parse for cloud-config-url.\n\n    This is used in MAAS datasource, in \"ephemeral\" (read-only root)\n    environment where the instance netboots to iscsi ro root.\n    and the entity that controls the pxe config has to configure\n    the maas datasource.\n\n    An attempt is made on network urls even in local datasource\n    for case of network set up in initramfs.\n\n    Return value is a tuple of a logger function (logging.DEBUG)\n    and a message indicating what happened.\n    \"\"\"\n\n    if cmdline is None:\n        cmdline = util.get_cmdline()\n\n    try:\n        cmdline_name, url = parse_cmdline_url(cmdline)\n    except KeyError:\n        return (logging.DEBUG, \"No kernel command line url found.\")\n\n    path_is_local = url.startswith(\"file://\") or url.startswith(\"/\")\n\n    if path_is_local and os.path.exists(path):\n        if network:\n            m = (\n                \"file '%s' existed, possibly from local stage download\"\n                \" of command line url '%s'. Not re-writing.\" % (path, url)\n            )\n            level = logging.INFO\n            if path_is_local:\n                level = logging.DEBUG\n        else:\n            m = (\n                \"file '%s' existed, possibly from previous boot download\"\n                \" of command line url '%s'. Not re-writing.\" % (path, url)\n            )\n            level = logging.WARN\n\n        return (level, m)\n\n    kwargs = {\"url\": url, \"timeout\": 10, \"retries\": 2}\n    if network or path_is_local:\n        level = logging.WARN\n        kwargs[\"sec_between\"] = 1\n    else:\n        level = logging.DEBUG\n        kwargs[\"sec_between\"] = 0.1\n\n    data = None\n    header = b\"#cloud-config\"\n    try:\n        resp = url_helper.read_file_or_url(**kwargs)\n        if resp.ok():\n            data = resp.contents\n            if not resp.contents.startswith(header):\n                if cmdline_name == \"cloud-config-url\":\n                    level = logging.WARN\n                else:\n                    level = logging.INFO\n                return (\n                    level,\n                    \"contents of '%s' did not start with %s\" % (url, header),\n                )\n        else:\n            return (\n                level,\n                \"url '%s' returned code %s. Ignoring.\" % (url, resp.code),\n            )\n\n    except url_helper.UrlError as e:\n        return (level, \"retrieving url '%s' failed: %s\" % (url, e))\n\n    util.write_file(path, data, mode=0o600)\n    return (\n        logging.INFO,\n        \"wrote cloud-config data from %s='%s' to %s\"\n        % (cmdline_name, url, path),\n    )\n\n\ndef purge_cache_on_python_version_change(init):\n    \"\"\"Purge the cache if python version changed on us.\n\n    There could be changes not represented in our cache (obj.pkl) after we\n    upgrade to a new version of python, so at that point clear the cache\n    \"\"\"\n    current_python_version = \"%d.%d\" % (\n        sys.version_info.major,\n        sys.version_info.minor,\n    )\n    python_version_path = os.path.join(\n        init.paths.get_cpath(\"data\"), \"python-version\"\n    )\n    if os.path.exists(python_version_path):\n        cached_python_version = open(python_version_path).read()\n        # The Python version has changed out from under us, anything that was\n        # pickled previously is likely useless due to API changes.\n        if cached_python_version != current_python_version:\n            LOG.debug(\"Python version change detected. Purging cache\")\n            init.purge_cache(True)\n            util.write_file(python_version_path, current_python_version)\n    else:\n        if os.path.exists(init.paths.get_ipath_cur(\"obj_pkl\")):\n            LOG.info(\n                \"Writing python-version file. \"\n                \"Cache compatibility status is currently unknown.\"\n            )\n        util.write_file(python_version_path, current_python_version)\n\n\ndef _should_bring_up_interfaces(init, args):\n    if util.get_cfg_option_bool(init.cfg, \"disable_network_activation\"):\n        return False\n    return not args.local\n\n\ndef main_init(name, args):\n    deps = [sources.DEP_FILESYSTEM, sources.DEP_NETWORK]\n    if args.local:\n        deps = [sources.DEP_FILESYSTEM]\n\n    early_logs = [\n        attempt_cmdline_url(\n            path=os.path.join(\n                \"%s.d\" % CLOUD_CONFIG, \"91_kernel_cmdline_url.cfg\"\n            ),\n            network=not args.local,\n        )\n    ]\n\n    # Cloud-init 'init' stage is broken up into the following sub-stages\n    # 1. Ensure that the init object fetches its config without errors\n    # 2. Setup logging/output redirections with resultant config (if any)\n    # 3. Initialize the cloud-init filesystem\n    # 4. Check if we can stop early by looking for various files\n    # 5. Fetch the datasource\n    # 6. Connect to the current instance location + update the cache\n    # 7. Consume the userdata (handlers get activated here)\n    # 8. Construct the modules object\n    # 9. Adjust any subsequent logging/output redirections using the modules\n    #    objects config as it may be different from init object\n    # 10. Run the modules for the 'init' stage\n    # 11. Done!\n    if not args.local:\n        w_msg = welcome_format(name)\n    else:\n        w_msg = welcome_format(\"%s-local\" % (name))\n    init = stages.Init(ds_deps=deps, reporter=args.reporter)\n    # Stage 1\n    init.read_cfg(extract_fns(args))\n    # Stage 2\n    outfmt = None\n    errfmt = None\n    try:\n        early_logs.append((logging.DEBUG, \"Closing stdin.\"))\n        util.close_stdin()\n        (outfmt, errfmt) = util.fixup_output(init.cfg, name)\n    except Exception:\n        msg = \"Failed to setup output redirection!\"\n        util.logexc(LOG, msg)\n        print_exc(msg)\n        early_logs.append((logging.WARN, msg))\n    if args.debug:\n        # Reset so that all the debug handlers are closed out\n        LOG.debug(\n            \"Logging being reset, this logger may no longer be active shortly\"\n        )\n        logging.resetLogging()\n    logging.setupLogging(init.cfg)\n    apply_reporting_cfg(init.cfg)\n\n    # Any log usage prior to setupLogging above did not have local user log\n    # config applied.  We send the welcome message now, as stderr/out have\n    # been redirected and log now configured.\n    welcome(name, msg=w_msg)\n\n    # re-play early log messages before logging was setup\n    for lvl, msg in early_logs:\n        LOG.log(lvl, msg)\n\n    # Stage 3\n    try:\n        init.initialize()\n    except Exception:\n        util.logexc(LOG, \"Failed to initialize, likely bad things to come!\")\n    # Stage 4\n    path_helper = init.paths\n    purge_cache_on_python_version_change(init)\n    mode = sources.DSMODE_LOCAL if args.local else sources.DSMODE_NETWORK\n\n    if mode == sources.DSMODE_NETWORK:\n        existing = \"trust\"\n        sys.stderr.write(\"%s\\n\" % (netinfo.debug_info()))\n    else:\n        existing = \"check\"\n        mcfg = util.get_cfg_option_bool(init.cfg, \"manual_cache_clean\", False)\n        if mcfg:\n            LOG.debug(\"manual cache clean set from config\")\n            existing = \"trust\"\n        else:\n            mfile = path_helper.get_ipath_cur(\"manual_clean_marker\")\n            if os.path.exists(mfile):\n                LOG.debug(\"manual cache clean found from marker: %s\", mfile)\n                existing = \"trust\"\n\n        init.purge_cache()\n\n    # Stage 5\n    bring_up_interfaces = _should_bring_up_interfaces(init, args)\n    try:\n        init.fetch(existing=existing)\n        # if in network mode, and the datasource is local\n        # then work was done at that stage.\n        if mode == sources.DSMODE_NETWORK and init.datasource.dsmode != mode:\n            LOG.debug(\n                \"[%s] Exiting. datasource %s in local mode\",\n                mode,\n                init.datasource,\n            )\n            return (None, [])\n    except sources.DataSourceNotFoundException:\n        # In the case of 'cloud-init init' without '--local' it is a bit\n        # more likely that the user would consider it failure if nothing was\n        # found.\n        if mode == sources.DSMODE_LOCAL:\n            LOG.debug(\"No local datasource found\")\n        else:\n            util.logexc(\n                LOG, \"No instance datasource found! Likely bad things to come!\"\n            )\n        if not args.force:\n            init.apply_network_config(bring_up=bring_up_interfaces)\n            LOG.debug(\"[%s] Exiting without datasource\", mode)\n            if mode == sources.DSMODE_LOCAL:\n                return (None, [])\n            else:\n                return (None, [\"No instance datasource found.\"])\n        else:\n            LOG.debug(\n                \"[%s] barreling on in force mode without datasource\", mode\n            )\n\n    _maybe_persist_instance_data(init)\n    # Stage 6\n    iid = init.instancify()\n    LOG.debug(\n        \"[%s] %s will now be targeting instance id: %s. new=%s\",\n        mode,\n        name,\n        iid,\n        init.is_new_instance(),\n    )\n\n    if mode == sources.DSMODE_LOCAL:\n        # Before network comes up, set any configured hostname to allow\n        # dhcp clients to advertize this hostname to any DDNS services\n        # LP: #1746455.\n        _maybe_set_hostname(init, stage=\"local\", retry_stage=\"network\")\n    init.apply_network_config(bring_up=bring_up_interfaces)\n\n    if mode == sources.DSMODE_LOCAL:\n        if init.datasource.dsmode != mode:\n            LOG.debug(\n                \"[%s] Exiting. datasource %s not in local mode.\",\n                mode,\n                init.datasource,\n            )\n            return (init.datasource, [])\n        else:\n            LOG.debug(\n                \"[%s] %s is in local mode, will apply init modules now.\",\n                mode,\n                init.datasource,\n            )\n\n    # Give the datasource a chance to use network resources.\n    # This is used on Azure to communicate with the fabric over network.\n    init.setup_datasource()\n    # update fully realizes user-data (pulling in #include if necessary)\n    init.update()\n    _maybe_set_hostname(init, stage=\"init-net\", retry_stage=\"modules:config\")\n    # Stage 7\n    try:\n        # Attempt to consume the data per instance.\n        # This may run user-data handlers and/or perform\n        # url downloads and such as needed.\n        (ran, _results) = init.cloudify().run(\n            \"consume_data\",\n            init.consume_data,\n            args=[PER_INSTANCE],\n            freq=PER_INSTANCE,\n        )\n        if not ran:\n            # Just consume anything that is set to run per-always\n            # if nothing ran in the per-instance code\n            #\n            # See: https://bugs.launchpad.net/bugs/819507 for a little\n            # reason behind this...\n            init.consume_data(PER_ALWAYS)\n    except Exception:\n        util.logexc(LOG, \"Consuming user data failed!\")\n        return (init.datasource, [\"Consuming user data failed!\"])\n\n    # Validate user-data adheres to schema definition\n    if os.path.exists(init.paths.get_ipath_cur(\"userdata_raw\")):\n        validate_cloudconfig_schema(\n            config=init.cfg, strict=False, log_details=False\n        )\n    else:\n        LOG.debug(\"Skipping user-data validation. No user-data found.\")\n\n    apply_reporting_cfg(init.cfg)\n\n    # Stage 8 - re-read and apply relevant cloud-config to include user-data\n    mods = Modules(init, extract_fns(args), reporter=args.reporter)\n    # Stage 9\n    try:\n        outfmt_orig = outfmt\n        errfmt_orig = errfmt\n        (outfmt, errfmt) = util.get_output_cfg(mods.cfg, name)\n        if outfmt_orig != outfmt or errfmt_orig != errfmt:\n            LOG.warning(\"Stdout, stderr changing to (%s, %s)\", outfmt, errfmt)\n            (outfmt, errfmt) = util.fixup_output(mods.cfg, name)\n    except Exception:\n        util.logexc(LOG, \"Failed to re-adjust output redirection!\")\n    logging.setupLogging(mods.cfg)\n\n    # give the activated datasource a chance to adjust\n    init.activate_datasource()\n\n    di_report_warn(datasource=init.datasource, cfg=init.cfg)\n\n    # Stage 10\n    return (init.datasource, run_module_section(mods, name, name))\n\n\ndef di_report_warn(datasource, cfg):\n    if \"di_report\" not in cfg:\n        LOG.debug(\"no di_report found in config.\")\n        return\n\n    dicfg = cfg[\"di_report\"]\n    if dicfg is None:\n        # ds-identify may write 'di_report:\\n #comment\\n'\n        # which reads as {'di_report': None}\n        LOG.debug(\"di_report was None.\")\n        return\n\n    if not isinstance(dicfg, dict):\n        LOG.warning(\"di_report config not a dictionary: %s\", dicfg)\n        return\n\n    dslist = dicfg.get(\"datasource_list\")\n    if dslist is None:\n        LOG.warning(\"no 'datasource_list' found in di_report.\")\n        return\n    elif not isinstance(dslist, list):\n        LOG.warning(\"di_report/datasource_list not a list: %s\", dslist)\n        return\n\n    # ds.__module__ is like cloudinit.sources.DataSourceName\n    # where Name is the thing that shows up in datasource_list.\n    modname = datasource.__module__.rpartition(\".\")[2]\n    if modname.startswith(sources.DS_PREFIX):\n        modname = modname[len(sources.DS_PREFIX) :]\n    else:\n        LOG.warning(\n            \"Datasource '%s' came from unexpected module '%s'.\",\n            datasource,\n            modname,\n        )\n\n    if modname in dslist:\n        LOG.debug(\n            \"used datasource '%s' from '%s' was in di_report's list: %s\",\n            datasource,\n            modname,\n            dslist,\n        )\n        return\n\n    warnings.show_warning(\n        \"dsid_missing_source\", cfg, source=modname, dslist=str(dslist)\n    )\n\n\ndef main_modules(action_name, args):\n    name = args.mode\n    # Cloud-init 'modules' stages are broken up into the following sub-stages\n    # 1. Ensure that the init object fetches its config without errors\n    # 2. Get the datasource from the init object, if it does\n    #    not exist then that means the main_init stage never\n    #    worked, and thus this stage can not run.\n    # 3. Construct the modules object\n    # 4. Adjust any subsequent logging/output redirections using\n    #    the modules objects configuration\n    # 5. Run the modules for the given stage name\n    # 6. Done!\n    w_msg = welcome_format(\"%s:%s\" % (action_name, name))\n    init = stages.Init(ds_deps=[], reporter=args.reporter)\n    # Stage 1\n    init.read_cfg(extract_fns(args))\n    # Stage 2\n    try:\n        init.fetch(existing=\"trust\")\n    except sources.DataSourceNotFoundException:\n        # There was no datasource found, theres nothing to do\n        msg = (\n            \"Can not apply stage %s, no datasource found! Likely bad \"\n            \"things to come!\" % name\n        )\n        util.logexc(LOG, msg)\n        print_exc(msg)\n        if not args.force:\n            return [(msg)]\n    _maybe_persist_instance_data(init)\n    # Stage 3\n    mods = Modules(init, extract_fns(args), reporter=args.reporter)\n    # Stage 4\n    try:\n        LOG.debug(\"Closing stdin\")\n        util.close_stdin()\n        util.fixup_output(mods.cfg, name)\n    except Exception:\n        util.logexc(LOG, \"Failed to setup output redirection!\")\n    if args.debug:\n        # Reset so that all the debug handlers are closed out\n        LOG.debug(\n            \"Logging being reset, this logger may no longer be active shortly\"\n        )\n        logging.resetLogging()\n    logging.setupLogging(mods.cfg)\n    apply_reporting_cfg(init.cfg)\n\n    # now that logging is setup and stdout redirected, send welcome\n    welcome(name, msg=w_msg)\n\n    # Stage 5\n    return run_module_section(mods, name, name)\n\n\ndef main_single(name, args):\n    # Cloud-init single stage is broken up into the following sub-stages\n    # 1. Ensure that the init object fetches its config without errors\n    # 2. Attempt to fetch the datasource (warn if it doesn't work)\n    # 3. Construct the modules object\n    # 4. Adjust any subsequent logging/output redirections using\n    #    the modules objects configuration\n    # 5. Run the single module\n    # 6. Done!\n    mod_name = args.name\n    w_msg = welcome_format(name)\n    init = stages.Init(ds_deps=[], reporter=args.reporter)\n    # Stage 1\n    init.read_cfg(extract_fns(args))\n    # Stage 2\n    try:\n        init.fetch(existing=\"trust\")\n    except sources.DataSourceNotFoundException:\n        # There was no datasource found,\n        # that might be bad (or ok) depending on\n        # the module being ran (so continue on)\n        util.logexc(\n            LOG, \"Failed to fetch your datasource, likely bad things to come!\"\n        )\n        print_exc(\n            \"Failed to fetch your datasource, likely bad things to come!\"\n        )\n        if not args.force:\n            return 1\n    _maybe_persist_instance_data(init)\n    # Stage 3\n    mods = Modules(init, extract_fns(args), reporter=args.reporter)\n    mod_args = args.module_args\n    if mod_args:\n        LOG.debug(\"Using passed in arguments %s\", mod_args)\n    mod_freq = args.frequency\n    if mod_freq:\n        LOG.debug(\"Using passed in frequency %s\", mod_freq)\n        mod_freq = FREQ_SHORT_NAMES.get(mod_freq)\n    # Stage 4\n    try:\n        LOG.debug(\"Closing stdin\")\n        util.close_stdin()\n        util.fixup_output(mods.cfg, None)\n    except Exception:\n        util.logexc(LOG, \"Failed to setup output redirection!\")\n    if args.debug:\n        # Reset so that all the debug handlers are closed out\n        LOG.debug(\n            \"Logging being reset, this logger may no longer be active shortly\"\n        )\n        logging.resetLogging()\n    logging.setupLogging(mods.cfg)\n    apply_reporting_cfg(init.cfg)\n\n    # now that logging is setup and stdout redirected, send welcome\n    welcome(name, msg=w_msg)\n\n    # Stage 5\n    (which_ran, failures) = mods.run_single(mod_name, mod_args, mod_freq)\n    if failures:\n        LOG.warning(\"Ran %s but it failed!\", mod_name)\n        return 1\n    elif not which_ran:\n        LOG.warning(\"Did not run %s, does it exist?\", mod_name)\n        return 1\n    else:\n        # Guess it worked\n        return 0\n\n\ndef status_wrapper(name, args, data_d=None, link_d=None):\n    if data_d is None:\n        paths = read_cfg_paths()\n        data_d = paths.get_cpath(\"data\")\n    if link_d is None:\n        link_d = os.path.normpath(\"/run/cloud-init\")\n\n    status_path = os.path.join(data_d, \"status.json\")\n    status_link = os.path.join(link_d, \"status.json\")\n    result_path = os.path.join(data_d, \"result.json\")\n    result_link = os.path.join(link_d, \"result.json\")\n\n    util.ensure_dirs(\n        (\n            data_d,\n            link_d,\n        )\n    )\n\n    (_name, functor) = args.action\n\n    if name == \"init\":\n        if args.local:\n            mode = \"init-local\"\n        else:\n            mode = \"init\"\n    elif name == \"modules\":\n        mode = \"modules-%s\" % args.mode\n    else:\n        raise ValueError(\"unknown name: %s\" % name)\n\n    modes = (\n        \"init\",\n        \"init-local\",\n        \"modules-init\",\n        \"modules-config\",\n        \"modules-final\",\n    )\n    if mode not in modes:\n        raise ValueError(\n            \"Invalid cloud init mode specified '{0}'\".format(mode)\n        )\n\n    status = None\n    if mode == \"init-local\":\n        for f in (status_link, result_link, status_path, result_path):\n            util.del_file(f)\n    else:\n        try:\n            status = json.loads(util.load_file(status_path))\n        except Exception:\n            pass\n\n    nullstatus = {\n        \"errors\": [],\n        \"start\": None,\n        \"finished\": None,\n    }\n\n    if status is None:\n        status = {\"v1\": {}}\n        status[\"v1\"][\"datasource\"] = None\n\n    for m in modes:\n        if m not in status[\"v1\"]:\n            status[\"v1\"][m] = nullstatus.copy()\n\n    v1 = status[\"v1\"]\n    v1[\"stage\"] = mode\n    v1[mode][\"start\"] = time.time()\n\n    atomic_helper.write_json(status_path, status)\n    util.sym_link(\n        os.path.relpath(status_path, link_d), status_link, force=True\n    )\n\n    try:\n        ret = functor(name, args)\n        if mode in (\"init\", \"init-local\"):\n            (datasource, errors) = ret\n            if datasource is not None:\n                v1[\"datasource\"] = str(datasource)\n        else:\n            errors = ret\n\n        v1[mode][\"errors\"] = [str(e) for e in errors]\n\n    except Exception as e:\n        util.logexc(LOG, \"failed stage %s\", mode)\n        print_exc(\"failed run of stage %s\" % mode)\n        v1[mode][\"errors\"] = [str(e)]\n\n    v1[mode][\"finished\"] = time.time()\n    v1[\"stage\"] = None\n\n    atomic_helper.write_json(status_path, status)\n\n    if mode == \"modules-final\":\n        # write the 'finished' file\n        errors = []\n        for m in modes:\n            if v1[m][\"errors\"]:\n                errors.extend(v1[m].get(\"errors\", []))\n\n        atomic_helper.write_json(\n            result_path,\n            {\"v1\": {\"datasource\": v1[\"datasource\"], \"errors\": errors}},\n        )\n        util.sym_link(\n            os.path.relpath(result_path, link_d), result_link, force=True\n        )\n\n    return len(v1[mode][\"errors\"])\n\n\ndef _maybe_persist_instance_data(init):\n    \"\"\"Write instance-data.json file if absent and datasource is restored.\"\"\"\n    if init.ds_restored:\n        instance_data_file = os.path.join(\n            init.paths.run_dir, sources.INSTANCE_JSON_FILE\n        )\n        if not os.path.exists(instance_data_file):\n            init.datasource.persist_instance_data()\n\n\ndef _maybe_set_hostname(init, stage, retry_stage):\n    \"\"\"Call set-hostname if metadata, vendordata or userdata provides it.\n\n    @param stage: String representing current stage in which we are running.\n    @param retry_stage: String represented logs upon error setting hostname.\n    \"\"\"\n    cloud = init.cloudify()\n    (hostname, _fqdn, _) = util.get_hostname_fqdn(\n        init.cfg, cloud, metadata_only=True\n    )\n    if hostname:  # meta-data or user-data hostname content\n        try:\n            cc_set_hostname.handle(\"set-hostname\", init.cfg, cloud, LOG, None)\n        except cc_set_hostname.SetHostnameError as e:\n            LOG.debug(\n                \"Failed setting hostname in %s stage. Will\"\n                \" retry in %s stage. Error: %s.\",\n                stage,\n                retry_stage,\n                str(e),\n            )\n\n\ndef main_features(name, args):\n    sys.stdout.write(\"\\n\".join(sorted(version.FEATURES)) + \"\\n\")\n\n\ndef main(sysv_args=None):\n    if not sysv_args:\n        sysv_args = sys.argv\n    parser = argparse.ArgumentParser(prog=sysv_args.pop(0))\n\n    # Top level args\n    parser.add_argument(\n        \"--version\",\n        \"-v\",\n        action=\"version\",\n        version=\"%(prog)s \" + (version.version_string()),\n        help=\"Show program's version number and exit.\",\n    )\n    parser.add_argument(\n        \"--file\",\n        \"-f\",\n        action=\"append\",\n        dest=\"files\",\n        help=\"Use additional yaml configuration files.\",\n        type=argparse.FileType(\"rb\"),\n    )\n    parser.add_argument(\n        \"--debug\",\n        \"-d\",\n        action=\"store_true\",\n        help=\"Show additional pre-action logging (default: %(default)s).\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--force\",\n        action=\"store_true\",\n        help=(\n            \"Force running even if no datasource is\"\n            \" found (use at your own risk).\"\n        ),\n        dest=\"force\",\n        default=False,\n    )\n\n    parser.set_defaults(reporter=None)\n    subparsers = parser.add_subparsers(title=\"Subcommands\", dest=\"subcommand\")\n    subparsers.required = True\n\n    # Each action and its sub-options (if any)\n    parser_init = subparsers.add_parser(\n        \"init\", help=\"Initialize cloud-init and perform initial modules.\"\n    )\n    parser_init.add_argument(\n        \"--local\",\n        \"-l\",\n        action=\"store_true\",\n        help=\"Start in local mode (default: %(default)s).\",\n        default=False,\n    )\n    # This is used so that we can know which action is selected +\n    # the functor to use to run this subcommand\n    parser_init.set_defaults(action=(\"init\", main_init))\n\n    # These settings are used for the 'config' and 'final' stages\n    parser_mod = subparsers.add_parser(\n        \"modules\", help=\"Activate modules using a given configuration key.\"\n    )\n    parser_mod.add_argument(\n        \"--mode\",\n        \"-m\",\n        action=\"store\",\n        help=\"Module configuration name to use (default: %(default)s).\",\n        default=\"config\",\n        choices=(\"init\", \"config\", \"final\"),\n    )\n    parser_mod.set_defaults(action=(\"modules\", main_modules))\n\n    # This subcommand allows you to run a single module\n    parser_single = subparsers.add_parser(\n        \"single\", help=\"Run a single module.\"\n    )\n    parser_single.add_argument(\n        \"--name\",\n        \"-n\",\n        action=\"store\",\n        help=\"module name to run\",\n        required=True,\n    )\n    parser_single.add_argument(\n        \"--frequency\",\n        action=\"store\",\n        help=\"Set module frequency.\",\n        required=False,\n        choices=list(FREQ_SHORT_NAMES.keys()),\n    )\n    parser_single.add_argument(\n        \"--report\",\n        action=\"store_true\",\n        help=\"Enable reporting.\",\n        required=False,\n    )\n    parser_single.add_argument(\n        \"module_args\",\n        nargs=\"*\",\n        metavar=\"argument\",\n        help=\"Any additional arguments to pass to this module.\",\n    )\n    parser_single.set_defaults(action=(\"single\", main_single))\n\n    parser_query = subparsers.add_parser(\n        \"query\",\n        help=\"Query standardized instance metadata from the command line.\",\n    )\n\n    parser_dhclient = subparsers.add_parser(\n        dhclient_hook.NAME, help=dhclient_hook.__doc__\n    )\n    dhclient_hook.get_parser(parser_dhclient)\n\n    parser_features = subparsers.add_parser(\n        \"features\", help=\"List defined features.\"\n    )\n    parser_features.set_defaults(action=(\"features\", main_features))\n\n    parser_analyze = subparsers.add_parser(\n        \"analyze\", help=\"Devel tool: Analyze cloud-init logs and data.\"\n    )\n\n    parser_devel = subparsers.add_parser(\n        \"devel\", help=\"Run development tools.\"\n    )\n\n    parser_collect_logs = subparsers.add_parser(\n        \"collect-logs\", help=\"Collect and tar all cloud-init debug info.\"\n    )\n\n    parser_clean = subparsers.add_parser(\n        \"clean\", help=\"Remove logs and artifacts so cloud-init can re-run.\"\n    )\n\n    parser_status = subparsers.add_parser(\n        \"status\", help=\"Report cloud-init status or wait on completion.\"\n    )\n\n    parser_schema = subparsers.add_parser(\n        \"schema\", help=\"Validate cloud-config files using jsonschema.\"\n    )\n\n    if sysv_args:\n        # Only load subparsers if subcommand is specified to avoid load cost\n        subcommand = sysv_args[0]\n        if subcommand == \"analyze\":\n            from cloudinit.analyze.__main__ import get_parser as analyze_parser\n\n            # Construct analyze subcommand parser\n            analyze_parser(parser_analyze)\n        elif subcommand == \"devel\":\n            from cloudinit.cmd.devel.parser import get_parser as devel_parser\n\n            # Construct devel subcommand parser\n            devel_parser(parser_devel)\n        elif subcommand == \"collect-logs\":\n            from cloudinit.cmd.devel.logs import (\n                get_parser as logs_parser,\n                handle_collect_logs_args,\n            )\n\n            logs_parser(parser_collect_logs)\n            parser_collect_logs.set_defaults(\n                action=(\"collect-logs\", handle_collect_logs_args)\n            )\n        elif subcommand == \"clean\":\n            from cloudinit.cmd.clean import (\n                get_parser as clean_parser,\n                handle_clean_args,\n            )\n\n            clean_parser(parser_clean)\n            parser_clean.set_defaults(action=(\"clean\", handle_clean_args))\n        elif subcommand == \"query\":\n            from cloudinit.cmd.query import (\n                get_parser as query_parser,\n                handle_args as handle_query_args,\n            )\n\n            query_parser(parser_query)\n            parser_query.set_defaults(action=(\"render\", handle_query_args))\n        elif subcommand == \"schema\":\n            from cloudinit.config.schema import (\n                get_parser as schema_parser,\n                handle_schema_args,\n            )\n\n            schema_parser(parser_schema)\n            parser_schema.set_defaults(action=(\"schema\", handle_schema_args))\n        elif subcommand == \"status\":\n            from cloudinit.cmd.status import (\n                get_parser as status_parser,\n                handle_status_args,\n            )\n\n            status_parser(parser_status)\n            parser_status.set_defaults(action=(\"status\", handle_status_args))\n\n    args = parser.parse_args(args=sysv_args)\n\n    # Subparsers.required = True and each subparser sets action=(name, functor)\n    (name, functor) = args.action\n\n    # Setup basic logging to start (until reinitialized)\n    # iff in debug mode.\n    if args.debug:\n        logging.setupBasicLogging()\n\n    # Setup signal handlers before running\n    signal_handler.attach_handlers()\n\n    if name in (\"modules\", \"init\"):\n        functor = status_wrapper\n\n    rname = None\n    report_on = True\n    if name == \"init\":\n        if args.local:\n            rname, rdesc = (\"init-local\", \"searching for local datasources\")\n        else:\n            rname, rdesc = (\n                \"init-network\",\n                \"searching for network datasources\",\n            )\n    elif name == \"modules\":\n        rname, rdesc = (\n            \"modules-%s\" % args.mode,\n            \"running modules for %s\" % args.mode,\n        )\n    elif name == \"single\":\n        rname, rdesc = (\n            \"single/%s\" % args.name,\n            \"running single module %s\" % args.name,\n        )\n        report_on = args.report\n    else:\n        rname = name\n        rdesc = \"running 'cloud-init %s'\" % name\n        report_on = False\n\n    args.reporter = events.ReportEventStack(\n        rname, rdesc, reporting_enabled=report_on\n    )\n\n    with args.reporter:\n        retval = util.log_time(\n            logfunc=LOG.debug,\n            msg=\"cloud-init mode '%s'\" % name,\n            get_uptime=True,\n            func=functor,\n            args=(name, args),\n        )\n        reporting.flush_events()\n        return retval\n\n\nif __name__ == \"__main__\":\n    if \"TZ\" not in os.environ:\n        os.environ[\"TZ\"] = \":/etc/localtime\"\n    return_value = main(sys.argv)\n    if return_value:\n        sys.exit(return_value)\n", "# This file is part of cloud-init. See LICENSE file for license information.\n\"\"\"schema.py: Set of module functions for processing cloud-config schema.\"\"\"\n\nimport argparse\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport textwrap\nimport typing\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom functools import partial\nfrom typing import Optional, Tuple, cast\n\nimport yaml\n\nfrom cloudinit import importer, safeyaml\nfrom cloudinit.cmd.devel import read_cfg_paths\nfrom cloudinit.util import error, get_modules_from_dir, load_file\n\nerror = partial(error, sys_exit=True)\nLOG = logging.getLogger(__name__)\n\nVERSIONED_USERDATA_SCHEMA_FILE = \"versions.schema.cloud-config.json\"\n# Bump this file when introducing incompatible schema changes.\n# Also add new version definition to versions.schema.json.\nUSERDATA_SCHEMA_FILE = \"schema-cloud-config-v1.json\"\n_YAML_MAP = {True: \"true\", False: \"false\", None: \"null\"}\nCLOUD_CONFIG_HEADER = b\"#cloud-config\"\nSCHEMA_DOC_TMPL = \"\"\"\n{name}\n{title_underbar}\n**Summary:** {title}\n\n{description}\n\n**Internal name:** ``{id}``\n\n**Module frequency:** {frequency}\n\n**Supported distros:** {distros}\n\n{property_header}\n{property_doc}\n\n{examples}\n\"\"\"\nSCHEMA_PROPERTY_HEADER = \"**Config schema**:\"\nSCHEMA_PROPERTY_TMPL = \"{prefix}**{prop_name}:** ({prop_type}){description}\"\nSCHEMA_LIST_ITEM_TMPL = (\n    \"{prefix}Each object in **{prop_name}** list supports the following keys:\"\n)\nSCHEMA_EXAMPLES_HEADER = \"**Examples**::\\n\\n\"\nSCHEMA_EXAMPLES_SPACER_TEMPLATE = \"\\n    # --- Example{0} ---\"\n\n\n# annotations add value for development, but don't break old versions\n# pyver: 3.6 -> 3.8\n# pylint: disable=E1101\nif sys.version_info >= (3, 8):\n\n    class MetaSchema(typing.TypedDict):\n        name: str\n        id: str\n        title: str\n        description: str\n        distros: typing.List[str]\n        examples: typing.List[str]\n        frequency: str\n\nelse:\n    MetaSchema = dict\n# pylint: enable=E1101\n\n\nclass SchemaValidationError(ValueError):\n    \"\"\"Raised when validating a cloud-config file against a schema.\"\"\"\n\n    def __init__(self, schema_errors=()):\n        \"\"\"Init the exception an n-tuple of schema errors.\n\n        @param schema_errors: An n-tuple of the format:\n            ((flat.config.key, msg),)\n        \"\"\"\n        self.schema_errors = schema_errors\n        error_messages = [\n            \"{0}: {1}\".format(config_key, message)\n            for config_key, message in schema_errors\n        ]\n        message = \"Cloud config schema errors: {0}\".format(\n            \", \".join(error_messages)\n        )\n        super(SchemaValidationError, self).__init__(message)\n\n\ndef is_schema_byte_string(checker, instance):\n    \"\"\"TYPE_CHECKER override allowing bytes for string type\n\n    For jsonschema v. 3.0.0+\n    \"\"\"\n    try:\n        from jsonschema import Draft4Validator\n    except ImportError:\n        return False\n    return Draft4Validator.TYPE_CHECKER.is_type(\n        instance, \"string\"\n    ) or isinstance(instance, (bytes,))\n\n\ndef get_jsonschema_validator():\n    \"\"\"Get metaschema validator and format checker\n\n    Older versions of jsonschema require some compatibility changes.\n\n    @returns: Tuple: (jsonschema.Validator, FormatChecker)\n    @raises: ImportError when jsonschema is not present\n    \"\"\"\n    from jsonschema import Draft4Validator, FormatChecker\n    from jsonschema.validators import create\n\n    # Allow for bytes to be presented as an acceptable valid value for string\n    # type jsonschema attributes in cloud-init's schema.\n    # This allows #cloud-config to provide valid yaml \"content: !!binary | ...\"\n\n    strict_metaschema = deepcopy(Draft4Validator.META_SCHEMA)\n    strict_metaschema[\"additionalProperties\"] = False\n\n    # This additional label allows us to specify a different name\n    # than the property key when generating docs.\n    # This is especially useful when using a \"patternProperties\" regex,\n    # otherwise the property label in the generated docs will be a\n    # regular expression.\n    # http://json-schema.org/understanding-json-schema/reference/object.html#pattern-properties\n    strict_metaschema[\"properties\"][\"label\"] = {\"type\": \"string\"}\n\n    if hasattr(Draft4Validator, \"TYPE_CHECKER\"):  # jsonschema 3.0+\n        type_checker = Draft4Validator.TYPE_CHECKER.redefine(\n            \"string\", is_schema_byte_string\n        )\n        cloudinitValidator = create(\n            meta_schema=strict_metaschema,\n            validators=Draft4Validator.VALIDATORS,\n            version=\"draft4\",\n            type_checker=type_checker,\n        )\n    else:  # jsonschema 2.6 workaround\n        types = Draft4Validator.DEFAULT_TYPES  # pylint: disable=E1101\n        # Allow bytes as well as string (and disable a spurious unsupported\n        # assignment-operation pylint warning which appears because this\n        # code path isn't written against the latest jsonschema).\n        types[\"string\"] = (str, bytes)  # pylint: disable=E1137\n        cloudinitValidator = create(  # pylint: disable=E1123\n            meta_schema=strict_metaschema,\n            validators=Draft4Validator.VALIDATORS,\n            version=\"draft4\",\n            default_types=types,\n        )\n    return (cloudinitValidator, FormatChecker)\n\n\ndef validate_cloudconfig_metaschema(validator, schema: dict, throw=True):\n    \"\"\"Validate provided schema meets the metaschema definition. Return strict\n    Validator and FormatChecker for use in validation\n    @param validator: Draft4Validator instance used to validate the schema\n    @param schema: schema to validate\n    @param throw: Sometimes the validator and checker are required, even if\n        the schema is invalid. Toggle for whether to raise\n        SchemaValidationError or log warnings.\n\n    @raises: ImportError when jsonschema is not present\n    @raises: SchemaValidationError when the schema is invalid\n    \"\"\"\n\n    from jsonschema.exceptions import SchemaError\n\n    try:\n        validator.check_schema(schema)\n    except SchemaError as err:\n        # Raise SchemaValidationError to avoid jsonschema imports at call\n        # sites\n        if throw:\n            raise SchemaValidationError(\n                schema_errors=(\n                    (\".\".join([str(p) for p in err.path]), err.message),\n                )\n            ) from err\n        LOG.warning(\n            \"Meta-schema validation failed, attempting to validate config \"\n            \"anyway: %s\",\n            err,\n        )\n\n\ndef validate_cloudconfig_schema(\n    config: dict,\n    schema: dict = None,\n    strict: bool = False,\n    strict_metaschema: bool = False,\n    log_details: bool = True,\n):\n    \"\"\"Validate provided config meets the schema definition.\n\n    @param config: Dict of cloud configuration settings validated against\n        schema. Ignored if strict_metaschema=True\n    @param schema: jsonschema dict describing the supported schema definition\n       for the cloud config module (config.cc_*). If None, validate against\n       global schema.\n    @param strict: Boolean, when True raise SchemaValidationErrors instead of\n       logging warnings.\n    @param strict_metaschema: Boolean, when True validates schema using strict\n       metaschema definition at runtime (currently unused)\n    @param log_details: Boolean, when True logs details of validation errors.\n       If there are concerns about logging sensitive userdata, this should\n       be set to False.\n\n    @raises: SchemaValidationError when provided config does not validate\n        against the provided schema.\n    @raises: RuntimeError when provided config sourced from YAML is not a dict.\n    \"\"\"\n    if schema is None:\n        schema = get_schema()\n    try:\n        (cloudinitValidator, FormatChecker) = get_jsonschema_validator()\n        if strict_metaschema:\n            validate_cloudconfig_metaschema(\n                cloudinitValidator, schema, throw=False\n            )\n    except ImportError:\n        LOG.debug(\"Ignoring schema validation. jsonschema is not present\")\n        return\n\n    validator = cloudinitValidator(schema, format_checker=FormatChecker())\n    errors: Tuple[Tuple[str, str], ...] = ()\n    for error in sorted(validator.iter_errors(config), key=lambda e: e.path):\n        path = \".\".join([str(p) for p in error.path])\n        errors += ((path, error.message),)\n    if errors:\n        if strict:\n            # This could output/log sensitive data\n            raise SchemaValidationError(errors)\n        if log_details:\n            messages = [\"{0}: {1}\".format(k, msg) for k, msg in errors]\n            details = \"\\n\" + \"\\n\".join(messages)\n        else:\n            details = (\n                \"Please run 'sudo cloud-init schema --system' to \"\n                \"see the schema errors.\"\n            )\n        LOG.warning(\"Invalid cloud-config provided: %s\", details)\n\n\ndef annotated_cloudconfig_file(\n    cloudconfig, original_content, schema_errors, schemamarks\n):\n    \"\"\"Return contents of the cloud-config file annotated with schema errors.\n\n    @param cloudconfig: YAML-loaded dict from the original_content or empty\n        dict if unparseable.\n    @param original_content: The contents of a cloud-config file\n    @param schema_errors: List of tuples from a JSONSchemaValidationError. The\n        tuples consist of (schemapath, error_message).\n    \"\"\"\n    if not schema_errors:\n        return original_content\n    errors_by_line = defaultdict(list)\n    error_footer = []\n    error_header = \"# Errors: -------------\\n{0}\\n\\n\"\n    annotated_content = []\n    lines = original_content.decode().split(\"\\n\")\n    if not isinstance(cloudconfig, dict):\n        # Return a meaningful message on empty cloud-config\n        return \"\\n\".join(\n            lines\n            + [error_header.format(\"# E1: Cloud-config is not a YAML dict.\")]\n        )\n    for path, msg in schema_errors:\n        match = re.match(r\"format-l(?P<line>\\d+)\\.c(?P<col>\\d+).*\", path)\n        if match:\n            line, col = match.groups()\n            errors_by_line[int(line)].append(msg)\n        else:\n            col = None\n            errors_by_line[schemamarks[path]].append(msg)\n        if col is not None:\n            msg = \"Line {line} column {col}: {msg}\".format(\n                line=line, col=col, msg=msg\n            )\n    error_index = 1\n    for line_number, line in enumerate(lines, 1):\n        errors = errors_by_line[line_number]\n        if errors:\n            error_label = []\n            for error in errors:\n                error_label.append(\"E{0}\".format(error_index))\n                error_footer.append(\"# E{0}: {1}\".format(error_index, error))\n                error_index += 1\n            annotated_content.append(line + \"\\t\\t# \" + \",\".join(error_label))\n\n        else:\n            annotated_content.append(line)\n    annotated_content.append(error_header.format(\"\\n\".join(error_footer)))\n    return \"\\n\".join(annotated_content)\n\n\ndef validate_cloudconfig_file(config_path, schema, annotate=False):\n    \"\"\"Validate cloudconfig file adheres to a specific jsonschema.\n\n    @param config_path: Path to the yaml cloud-config file to parse, or None\n        to default to system userdata from Paths object.\n    @param schema: Dict describing a valid jsonschema to validate against.\n    @param annotate: Boolean set True to print original config file with error\n        annotations on the offending lines.\n\n    @raises SchemaValidationError containing any of schema_errors encountered.\n    @raises RuntimeError when config_path does not exist.\n    \"\"\"\n    if config_path is None:\n        # Use system's raw userdata path\n        if os.getuid() != 0:\n            raise RuntimeError(\n                \"Unable to read system userdata as non-root user.\"\n                \" Try using sudo\"\n            )\n        paths = read_cfg_paths()\n        user_data_file = paths.get_ipath_cur(\"userdata_raw\")\n        content = load_file(user_data_file, decode=False)\n    else:\n        if not os.path.exists(config_path):\n            raise RuntimeError(\n                \"Configfile {0} does not exist\".format(config_path)\n            )\n        content = load_file(config_path, decode=False)\n    if not content.startswith(CLOUD_CONFIG_HEADER):\n        errors = (\n            (\n                \"format-l1.c1\",\n                'File {0} needs to begin with \"{1}\"'.format(\n                    config_path, CLOUD_CONFIG_HEADER.decode()\n                ),\n            ),\n        )\n        error = SchemaValidationError(errors)\n        if annotate:\n            print(\n                annotated_cloudconfig_file(\n                    {}, content, error.schema_errors, {}\n                )\n            )\n        raise error\n    try:\n        if annotate:\n            cloudconfig, marks = safeyaml.load_with_marks(content)\n        else:\n            cloudconfig = safeyaml.load(content)\n            marks = {}\n    except (yaml.YAMLError) as e:\n        line = column = 1\n        mark = None\n        if hasattr(e, \"context_mark\") and getattr(e, \"context_mark\"):\n            mark = getattr(e, \"context_mark\")\n        elif hasattr(e, \"problem_mark\") and getattr(e, \"problem_mark\"):\n            mark = getattr(e, \"problem_mark\")\n        if mark:\n            line = mark.line + 1\n            column = mark.column + 1\n        errors = (\n            (\n                \"format-l{line}.c{col}\".format(line=line, col=column),\n                \"File {0} is not valid yaml. {1}\".format(config_path, str(e)),\n            ),\n        )\n        error = SchemaValidationError(errors)\n        if annotate:\n            print(\n                annotated_cloudconfig_file(\n                    {}, content, error.schema_errors, {}\n                )\n            )\n        raise error from e\n    if not isinstance(cloudconfig, dict):\n        # Return a meaningful message on empty cloud-config\n        if not annotate:\n            raise RuntimeError(\"Cloud-config is not a YAML dict.\")\n    try:\n        validate_cloudconfig_schema(cloudconfig, schema, strict=True)\n    except SchemaValidationError as e:\n        if annotate:\n            print(\n                annotated_cloudconfig_file(\n                    cloudconfig, content, e.schema_errors, marks\n                )\n            )\n        raise\n\n\ndef _sort_property_order(value):\n    \"\"\"Provide a sorting weight for documentation of property types.\n\n    Weight values ensure 'array' sorted after 'object' which is sorted\n    after anything else which remains unsorted.\n    \"\"\"\n    if value == \"array\":\n        return 2\n    elif value == \"object\":\n        return 1\n    return 0\n\n\ndef _get_property_type(property_dict: dict, defs: dict) -> str:\n    \"\"\"Return a string representing a property type from a given\n    jsonschema.\n    \"\"\"\n    _flatten_schema_refs(property_dict, defs)\n    property_types = property_dict.get(\"type\", [])\n    if not isinstance(property_types, list):\n        property_types = [property_types]\n    if property_dict.get(\"enum\"):\n        property_types = [\n            f\"``{_YAML_MAP.get(k, k)}``\" for k in property_dict[\"enum\"]\n        ]\n    elif property_dict.get(\"oneOf\"):\n        property_types.extend(\n            [\n                subschema[\"type\"]\n                for subschema in property_dict.get(\"oneOf\", {})\n                if subschema.get(\"type\")\n            ]\n        )\n    if len(property_types) == 1:\n        property_type = property_types[0]\n    else:\n        property_types.sort(key=_sort_property_order)\n        property_type = \"/\".join(property_types)\n    items = property_dict.get(\"items\", {})\n    sub_property_types = items.get(\"type\", [])\n    if not isinstance(sub_property_types, list):\n        sub_property_types = [sub_property_types]\n    # Collect each item type\n    for sub_item in items.get(\"oneOf\", {}):\n        sub_property_types.append(_get_property_type(sub_item, defs))\n    if sub_property_types:\n        if len(sub_property_types) == 1:\n            return f\"{property_type} of {sub_property_types[0]}\"\n        sub_property_types.sort(key=_sort_property_order)\n        sub_property_doc = f\"({'/'.join(sub_property_types)})\"\n        return f\"{property_type} of {sub_property_doc}\"\n    return property_type or \"UNDEFINED\"\n\n\ndef _parse_description(description, prefix) -> str:\n    \"\"\"Parse description from the meta in a format that we can better\n    display in our docs. This parser does three things:\n\n    - Guarantee that a paragraph will be in a single line\n    - Guarantee that each new paragraph will be aligned with\n      the first paragraph\n    - Proper align lists of items\n\n    @param description: The original description in the meta.\n    @param prefix: The number of spaces used to align the current description\n    \"\"\"\n    list_paragraph = prefix * 3\n    description = re.sub(r\"(\\S)\\n(\\S)\", r\"\\1 \\2\", description)\n    description = re.sub(r\"\\n\\n\", r\"\\n\\n{}\".format(prefix), description)\n    description = re.sub(\n        r\"\\n( +)-\", r\"\\n{}-\".format(list_paragraph), description\n    )\n\n    return description\n\n\ndef _flatten_schema_refs(src_cfg: dict, defs: dict):\n    \"\"\"Flatten schema: replace $refs in src_cfg with definitions from $defs.\"\"\"\n    if \"$ref\" in src_cfg:\n        reference = src_cfg.pop(\"$ref\").replace(\"#/$defs/\", \"\")\n        # Update the defined references in subschema for doc rendering\n        src_cfg.update(defs[reference])\n    if \"items\" in src_cfg:\n        if \"$ref\" in src_cfg[\"items\"]:\n            reference = src_cfg[\"items\"].pop(\"$ref\").replace(\"#/$defs/\", \"\")\n            # Update the references in subschema for doc rendering\n            src_cfg[\"items\"].update(defs[reference])\n        if \"oneOf\" in src_cfg[\"items\"]:\n            for alt_schema in src_cfg[\"items\"][\"oneOf\"]:\n                if \"$ref\" in alt_schema:\n                    reference = alt_schema.pop(\"$ref\").replace(\"#/$defs/\", \"\")\n                    alt_schema.update(defs[reference])\n    for alt_schema in src_cfg.get(\"oneOf\", []):\n        if \"$ref\" in alt_schema:\n            reference = alt_schema.pop(\"$ref\").replace(\"#/$defs/\", \"\")\n            alt_schema.update(defs[reference])\n\n\ndef _get_property_doc(schema: dict, defs: dict, prefix=\"    \") -> str:\n    \"\"\"Return restructured text describing the supported schema properties.\"\"\"\n    new_prefix = prefix + \"    \"\n    properties = []\n    if schema.get(\"hidden\") is True:\n        return \"\"  # no docs for this schema\n    property_keys = [\n        key\n        for key in (\"properties\", \"patternProperties\")\n        if \"hidden\" not in schema or key not in schema[\"hidden\"]\n    ]\n    property_schemas = [schema.get(key, {}) for key in property_keys]\n\n    for prop_schema in property_schemas:\n        for prop_key, prop_config in prop_schema.items():\n            _flatten_schema_refs(prop_config, defs)\n            if prop_config.get(\"hidden\") is True:\n                continue  # document nothing for this property\n            # Define prop_name and description for SCHEMA_PROPERTY_TMPL\n            description = prop_config.get(\"description\", \"\")\n            if description:\n                description = \" \" + description\n\n            # Define prop_name and description for SCHEMA_PROPERTY_TMPL\n            label = prop_config.get(\"label\", prop_key)\n            properties.append(\n                SCHEMA_PROPERTY_TMPL.format(\n                    prefix=prefix,\n                    prop_name=label,\n                    description=_parse_description(description, prefix),\n                    prop_type=_get_property_type(prop_config, defs),\n                )\n            )\n            items = prop_config.get(\"items\")\n            if items:\n                _flatten_schema_refs(items, defs)\n                if items.get(\"properties\") or items.get(\"patternProperties\"):\n                    properties.append(\n                        SCHEMA_LIST_ITEM_TMPL.format(\n                            prefix=new_prefix, prop_name=label\n                        )\n                    )\n                    new_prefix += \"    \"\n                    properties.append(\n                        _get_property_doc(items, defs=defs, prefix=new_prefix)\n                    )\n                for alt_schema in items.get(\"oneOf\", []):\n                    if alt_schema.get(\"properties\") or alt_schema.get(\n                        \"patternProperties\"\n                    ):\n                        properties.append(\n                            SCHEMA_LIST_ITEM_TMPL.format(\n                                prefix=new_prefix, prop_name=label\n                            )\n                        )\n                        new_prefix += \"    \"\n                        properties.append(\n                            _get_property_doc(\n                                alt_schema, defs=defs, prefix=new_prefix\n                            )\n                        )\n            if (\n                \"properties\" in prop_config\n                or \"patternProperties\" in prop_config\n            ):\n                properties.append(\n                    _get_property_doc(\n                        prop_config, defs=defs, prefix=new_prefix\n                    )\n                )\n    return \"\\n\\n\".join(properties)\n\n\ndef _get_examples(meta: MetaSchema) -> str:\n    \"\"\"Return restructured text describing the meta examples if present.\"\"\"\n    examples = meta.get(\"examples\")\n    if not examples:\n        return \"\"\n    rst_content = SCHEMA_EXAMPLES_HEADER\n    for count, example in enumerate(examples):\n        indented_lines = textwrap.indent(example, \"    \").split(\"\\n\")\n        if rst_content != SCHEMA_EXAMPLES_HEADER:\n            indented_lines.insert(\n                0, SCHEMA_EXAMPLES_SPACER_TEMPLATE.format(count + 1)\n            )\n        rst_content += \"\\n\".join(indented_lines)\n    return rst_content\n\n\ndef get_meta_doc(meta: MetaSchema, schema: Optional[dict] = None) -> str:\n    \"\"\"Return reStructured text rendering the provided metadata.\n\n    @param meta: Dict of metadata to render.\n    @param schema: Optional module schema, if absent, read global schema.\n    @raise KeyError: If metadata lacks an expected key.\n    \"\"\"\n\n    if schema is None:\n        schema = get_schema()\n    if not meta or not schema:\n        raise ValueError(\"Expected non-empty meta and schema\")\n    keys = set(meta.keys())\n    expected = set(\n        {\n            \"id\",\n            \"title\",\n            \"examples\",\n            \"frequency\",\n            \"distros\",\n            \"description\",\n            \"name\",\n        }\n    )\n    error_message = \"\"\n    if expected - keys:\n        error_message = \"Missing expected keys in module meta: {}\".format(\n            expected - keys\n        )\n    elif keys - expected:\n        error_message = (\n            \"Additional unexpected keys found in module meta: {}\".format(\n                keys - expected\n            )\n        )\n    if error_message:\n        raise KeyError(error_message)\n\n    # cast away type annotation\n    meta_copy = dict(deepcopy(meta))\n    meta_copy[\"property_header\"] = \"\"\n    defs = schema.get(\"$defs\", {})\n    if defs.get(meta[\"id\"]):\n        schema = defs.get(meta[\"id\"], {})\n        schema = cast(dict, schema)\n    try:\n        meta_copy[\"property_doc\"] = _get_property_doc(schema, defs=defs)\n    except AttributeError:\n        LOG.warning(\"Unable to render property_doc due to invalid schema\")\n        meta_copy[\"property_doc\"] = \"\"\n    if meta_copy[\"property_doc\"]:\n        meta_copy[\"property_header\"] = SCHEMA_PROPERTY_HEADER\n    meta_copy[\"examples\"] = _get_examples(meta)\n    meta_copy[\"distros\"] = \", \".join(meta[\"distros\"])\n    # Need an underbar of the same length as the name\n    meta_copy[\"title_underbar\"] = re.sub(r\".\", \"-\", meta[\"name\"])\n    template = SCHEMA_DOC_TMPL.format(**meta_copy)\n    return template\n\n\ndef get_modules() -> dict:\n    configs_dir = os.path.dirname(os.path.abspath(__file__))\n    return get_modules_from_dir(configs_dir)\n\n\ndef load_doc(requested_modules: list) -> str:\n    \"\"\"Load module docstrings\n\n    Docstrings are generated on module load. Reduce, reuse, recycle.\n    \"\"\"\n    docs = \"\"\n    all_modules = list(get_modules().values()) + [\"all\"]\n    invalid_docs = set(requested_modules).difference(set(all_modules))\n    if invalid_docs:\n        error(\n            \"Invalid --docs value {}. Must be one of: {}\".format(\n                list(invalid_docs),\n                \", \".join(all_modules),\n            )\n        )\n    for mod_name in all_modules:\n        if \"all\" in requested_modules or mod_name in requested_modules:\n            (mod_locs, _) = importer.find_module(\n                mod_name, [\"cloudinit.config\"], [\"meta\"]\n            )\n            if mod_locs:\n                mod = importer.import_module(mod_locs[0])\n                docs += mod.__doc__ or \"\"\n    return docs\n\n\ndef get_schema_dir() -> str:\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"schemas\")\n\n\ndef get_schema() -> dict:\n    \"\"\"Return jsonschema coalesced from all cc_* cloud-config modules.\"\"\"\n    # Note versions.schema.json is publicly consumed by schemastore.org.\n    # If we change the location of versions.schema.json in github, we need\n    # to provide an updated PR to\n    # https://github.com/SchemaStore/schemastore.\n\n    # When bumping schema version due to incompatible changes:\n    # 1. Add a new schema-cloud-config-v#.json\n    # 2. change the USERDATA_SCHEMA_FILE to cloud-init-schema-v#.json\n    # 3. Add the new version definition to versions.schema.cloud-config.json\n    schema_file = os.path.join(get_schema_dir(), USERDATA_SCHEMA_FILE)\n    full_schema = None\n    try:\n        full_schema = json.loads(load_file(schema_file))\n    except Exception as e:\n        LOG.warning(\"Cannot parse JSON schema file %s. %s\", schema_file, e)\n    if not full_schema:\n        LOG.warning(\n            \"No base JSON schema files found at %s.\"\n            \" Setting default empty schema\",\n            schema_file,\n        )\n        full_schema = {\n            \"$defs\": {},\n            \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n            \"allOf\": [],\n        }\n    return full_schema\n\n\ndef get_meta() -> dict:\n    \"\"\"Return metadata coalesced from all cc_* cloud-config module.\"\"\"\n    full_meta = dict()\n    for (_, mod_name) in get_modules().items():\n        mod_locs, _ = importer.find_module(\n            mod_name, [\"cloudinit.config\"], [\"meta\"]\n        )\n        if mod_locs:\n            mod = importer.import_module(mod_locs[0])\n            full_meta[mod.meta[\"id\"]] = mod.meta\n    return full_meta\n\n\ndef get_parser(parser=None):\n    \"\"\"Return a parser for supported cmdline arguments.\"\"\"\n    if not parser:\n        parser = argparse.ArgumentParser(\n            prog=\"cloudconfig-schema\",\n            description=\"Validate cloud-config files or document schema\",\n        )\n    parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        help=\"Path of the cloud-config yaml file to validate\",\n    )\n    parser.add_argument(\n        \"--system\",\n        action=\"store_true\",\n        default=False,\n        help=\"Validate the system cloud-config userdata\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--docs\",\n        nargs=\"+\",\n        help=(\n            \"Print schema module docs. Choices: all or\"\n            \" space-delimited cc_names.\"\n        ),\n    )\n    parser.add_argument(\n        \"--annotate\",\n        action=\"store_true\",\n        default=False,\n        help=\"Annotate existing cloud-config file with errors\",\n    )\n    return parser\n\n\ndef handle_schema_args(name, args):\n    \"\"\"Handle provided schema args and perform the appropriate actions.\"\"\"\n    exclusive_args = [args.config_file, args.docs, args.system]\n    if len([arg for arg in exclusive_args if arg]) != 1:\n        error(\"Expected one of --config-file, --system or --docs arguments\")\n    if args.annotate and args.docs:\n        error(\"Invalid flag combination. Cannot use --annotate with --docs\")\n    full_schema = get_schema()\n    if args.config_file or args.system:\n        try:\n            validate_cloudconfig_file(\n                args.config_file, full_schema, args.annotate\n            )\n        except SchemaValidationError as e:\n            if not args.annotate:\n                error(str(e))\n        except RuntimeError as e:\n            error(str(e))\n        else:\n            if args.config_file is None:\n                cfg_name = \"system userdata\"\n            else:\n                cfg_name = args.config_file\n            print(\"Valid cloud-config:\", cfg_name)\n    elif args.docs:\n        print(load_doc(args.docs))\n\n\ndef main():\n    \"\"\"Tool to validate schema of a cloud-config file or print schema docs.\"\"\"\n    parser = get_parser()\n    handle_schema_args(\"cloudconfig-schema\", parser.parse_args())\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\n# vi: ts=4 expandtab\n", "\"\"\"Integration tests for CLI functionality\n\nThese would be for behavior manually invoked by user from the command line\n\"\"\"\n\nimport pytest\n\nfrom tests.integration_tests.instances import IntegrationInstance\n\nVALID_USER_DATA = \"\"\"\\\n#cloud-config\nruncmd:\n  - echo 'hi' > /var/tmp/test\n\"\"\"\n\nINVALID_USER_DATA_HEADER = \"\"\"\\\nruncmd:\n  - echo 'hi' > /var/tmp/test\n\"\"\"\n\n# The '-' in 'hashed-password' fails schema validation\nINVALID_USER_DATA_SCHEMA = \"\"\"\\\n#cloud-config\nusers:\n  - default\n  - name: newsuper\n    gecos: Big Stuff\n    groups: users, admin\n    sudo: ALL=(ALL) NOPASSWD:ALL\n    hashed-password: asdfasdf\n    shell: /bin/bash\n    lock_passwd: true\n\"\"\"\n\n\n@pytest.mark.user_data(VALID_USER_DATA)\ndef test_valid_userdata(client: IntegrationInstance):\n    \"\"\"Test `cloud-init schema` with valid userdata.\n\n    PR #575\n    \"\"\"\n    result = client.execute(\"cloud-init schema --system\")\n    assert result.ok\n    assert \"Valid cloud-config: system userdata\" == result.stdout.strip()\n    result = client.execute(\"cloud-init status --long\")\n    if not result.ok:\n        raise AssertionError(\n            f\"Unexpected error from cloud-init status: {result}\"\n        )\n\n\n@pytest.mark.user_data(INVALID_USER_DATA_HEADER)\ndef test_invalid_userdata(client: IntegrationInstance):\n    \"\"\"Test `cloud-init schema` with invalid userdata.\n\n    PR #575\n    \"\"\"\n    result = client.execute(\"cloud-init schema --system\")\n    assert not result.ok\n    assert \"Cloud config schema errors\" in result.stderr\n    assert 'needs to begin with \"#cloud-config\"' in result.stderr\n    result = client.execute(\"cloud-init status --long\")\n    if not result.ok:\n        raise AssertionError(\n            f\"Unexpected error from cloud-init status: {result}\"\n        )\n\n\n@pytest.mark.user_data(INVALID_USER_DATA_SCHEMA)\ndef test_invalid_userdata_schema(client: IntegrationInstance):\n    \"\"\"Test invalid schema represented as Warnings, not fatal\n\n    PR #1175\n    \"\"\"\n    result = client.execute(\"cloud-init status --long\")\n    assert result.ok\n    log = client.read_from_file(\"/var/log/cloud-init.log\")\n    warning = (\n        \"[WARNING]: Invalid cloud-config provided: Please run \"\n        \"'sudo cloud-init schema --system' to see the schema errors.\"\n    )\n    assert warning in log\n    assert \"asdfasdf\" not in log\n\n    result = client.execute(\"cloud-init status --long\")\n    if not result.ok:\n        raise AssertionError(\n            f\"Unexpected error from cloud-init status: {result}\"\n        )\n", "# This file is part of cloud-init. See LICENSE file for license information.\n\n\nimport importlib\nimport inspect\nimport itertools\nimport json\nimport logging\nimport os\nimport re\nimport sys\nfrom copy import copy\nfrom pathlib import Path\nfrom textwrap import dedent\nfrom types import ModuleType\nfrom typing import List\n\nimport pytest\n\nfrom cloudinit.config.schema import (\n    CLOUD_CONFIG_HEADER,\n    VERSIONED_USERDATA_SCHEMA_FILE,\n    MetaSchema,\n    SchemaValidationError,\n    annotated_cloudconfig_file,\n    get_jsonschema_validator,\n    get_meta_doc,\n    get_schema,\n    get_schema_dir,\n    load_doc,\n    main,\n    validate_cloudconfig_file,\n    validate_cloudconfig_metaschema,\n    validate_cloudconfig_schema,\n)\nfrom cloudinit.distros import OSFAMILIES\nfrom cloudinit.safeyaml import load, load_with_marks\nfrom cloudinit.settings import FREQUENCIES\nfrom cloudinit.util import load_file, write_file\nfrom tests.unittests.helpers import (\n    CiTestCase,\n    cloud_init_project_dir,\n    mock,\n    skipUnlessJsonSchema,\n)\n\n\ndef get_schemas() -> dict:\n    \"\"\"Return all legacy module schemas\n\n    Assumes that module schemas have the variable name \"schema\"\n    \"\"\"\n    return get_module_variable(\"schema\")\n\n\ndef get_metas() -> dict:\n    \"\"\"Return all module metas\n\n    Assumes that module schemas have the variable name \"schema\"\n    \"\"\"\n    return get_module_variable(\"meta\")\n\n\ndef get_module_names() -> List[str]:\n    \"\"\"Return list of module names in cloudinit/config\"\"\"\n    files = list(\n        Path(cloud_init_project_dir(\"cloudinit/config/\")).glob(\"cc_*.py\")\n    )\n\n    return [mod.stem for mod in files]\n\n\ndef get_modules() -> List[ModuleType]:\n    \"\"\"Return list of modules in cloudinit/config\"\"\"\n    return [\n        importlib.import_module(f\"cloudinit.config.{module}\")\n        for module in get_module_names()\n    ]\n\n\ndef get_module_variable(var_name) -> dict:\n    \"\"\"Inspect modules and get variable from module matching var_name\"\"\"\n    schemas: dict = {}\n    get_modules()\n    for k, v in sys.modules.items():\n        path = Path(k)\n        if \"cloudinit.config\" == path.stem and path.suffix[1:4] == \"cc_\":\n            module_name = path.suffix[1:]\n            members = inspect.getmembers(v)\n            schemas[module_name] = None\n            for name, value in members:\n                if name == var_name:\n                    schemas[module_name] = value\n                    break\n    return schemas\n\n\nclass TestVersionedSchemas:\n    @pytest.mark.parametrize(\n        \"schema,error_msg\",\n        (\n            ({}, None),\n            ({\"version\": \"v1\"}, None),\n            ({\"version\": \"v2\"}, \"is not valid\"),\n            ({\"version\": \"v1\", \"final_message\": -1}, \"is not valid\"),\n            ({\"version\": \"v1\", \"final_message\": \"some msg\"}, None),\n        ),\n    )\n    def test_versioned_cloud_config_schema_is_valid_json(\n        self, schema, error_msg\n    ):\n        schema_dir = get_schema_dir()\n        version_schemafile = os.path.join(\n            schema_dir, VERSIONED_USERDATA_SCHEMA_FILE\n        )\n        # Point to local schema files avoid JSON resolver trying to pull the\n        # reference from our upstream raw file in github.\n        version_schema = json.loads(\n            re.sub(\n                r\"https:\\/\\/raw.githubusercontent.com\\/canonical\\/\"\n                r\"cloud-init\\/main\\/cloudinit\\/config\\/schemas\\/\",\n                f\"file://{schema_dir}/\",\n                load_file(version_schemafile),\n            )\n        )\n        if error_msg:\n            with pytest.raises(SchemaValidationError) as context_mgr:\n                validate_cloudconfig_schema(\n                    schema, schema=version_schema, strict=True\n                )\n            assert error_msg in str(context_mgr.value)\n        else:\n            validate_cloudconfig_schema(\n                schema, schema=version_schema, strict=True\n            )\n\n\nclass TestGetSchema:\n    def test_static_schema_file_is_valid(self, caplog):\n        with caplog.at_level(logging.WARNING):\n            get_schema()\n        # Assert no warnings parsing our packaged schema file\n        warnings = [msg for (_, _, msg) in caplog.record_tuples]\n        assert [] == warnings\n\n    def test_get_schema_coalesces_known_schema(self):\n        \"\"\"Every cloudconfig module with schema is listed in allOf keyword.\"\"\"\n        schema = get_schema()\n        assert sorted(get_module_names()) == sorted(\n            [meta[\"id\"] for meta in get_metas().values() if meta is not None]\n        )\n        assert \"http://json-schema.org/draft-04/schema#\" == schema[\"$schema\"]\n        assert [\"$defs\", \"$schema\", \"allOf\"] == sorted(list(schema.keys()))\n        # New style schema should be defined in static schema file in $defs\n        expected_subschema_defs = [\n            {\"$ref\": \"#/$defs/cc_apk_configure\"},\n            {\"$ref\": \"#/$defs/cc_apt_configure\"},\n            {\"$ref\": \"#/$defs/cc_apt_pipelining\"},\n            {\"$ref\": \"#/$defs/cc_bootcmd\"},\n            {\"$ref\": \"#/$defs/cc_byobu\"},\n            {\"$ref\": \"#/$defs/cc_ca_certs\"},\n            {\"$ref\": \"#/$defs/cc_chef\"},\n            {\"$ref\": \"#/$defs/cc_debug\"},\n            {\"$ref\": \"#/$defs/cc_disable_ec2_metadata\"},\n            {\"$ref\": \"#/$defs/cc_disk_setup\"},\n            {\"$ref\": \"#/$defs/cc_fan\"},\n            {\"$ref\": \"#/$defs/cc_final_message\"},\n            {\"$ref\": \"#/$defs/cc_growpart\"},\n            {\"$ref\": \"#/$defs/cc_grub_dpkg\"},\n            {\"$ref\": \"#/$defs/cc_install_hotplug\"},\n            {\"$ref\": \"#/$defs/cc_keyboard\"},\n            {\"$ref\": \"#/$defs/cc_keys_to_console\"},\n            {\"$ref\": \"#/$defs/cc_landscape\"},\n            {\"$ref\": \"#/$defs/cc_locale\"},\n            {\"$ref\": \"#/$defs/cc_lxd\"},\n            {\"$ref\": \"#/$defs/cc_mcollective\"},\n            {\"$ref\": \"#/$defs/cc_migrator\"},\n            {\"$ref\": \"#/$defs/cc_mounts\"},\n            {\"$ref\": \"#/$defs/cc_ntp\"},\n            {\"$ref\": \"#/$defs/cc_package_update_upgrade_install\"},\n            {\"$ref\": \"#/$defs/cc_phone_home\"},\n            {\"$ref\": \"#/$defs/cc_power_state_change\"},\n            {\"$ref\": \"#/$defs/cc_puppet\"},\n            {\"$ref\": \"#/$defs/cc_resizefs\"},\n            {\"$ref\": \"#/$defs/cc_resolv_conf\"},\n            {\"$ref\": \"#/$defs/cc_rh_subscription\"},\n            {\"$ref\": \"#/$defs/cc_rsyslog\"},\n            {\"$ref\": \"#/$defs/cc_runcmd\"},\n            {\"$ref\": \"#/$defs/cc_salt_minion\"},\n            {\"$ref\": \"#/$defs/cc_scripts_vendor\"},\n            {\"$ref\": \"#/$defs/cc_seed_random\"},\n            {\"$ref\": \"#/$defs/cc_set_hostname\"},\n            {\"$ref\": \"#/$defs/cc_set_passwords\"},\n            {\"$ref\": \"#/$defs/cc_snap\"},\n            {\"$ref\": \"#/$defs/cc_spacewalk\"},\n            {\"$ref\": \"#/$defs/cc_ssh_authkey_fingerprints\"},\n            {\"$ref\": \"#/$defs/cc_ssh_import_id\"},\n            {\"$ref\": \"#/$defs/cc_ssh\"},\n            {\"$ref\": \"#/$defs/cc_timezone\"},\n            {\"$ref\": \"#/$defs/cc_ubuntu_advantage\"},\n            {\"$ref\": \"#/$defs/cc_ubuntu_drivers\"},\n            {\"$ref\": \"#/$defs/cc_update_etc_hosts\"},\n            {\"$ref\": \"#/$defs/cc_update_hostname\"},\n            {\"$ref\": \"#/$defs/cc_users_groups\"},\n            {\"$ref\": \"#/$defs/cc_write_files\"},\n            {\"$ref\": \"#/$defs/cc_yum_add_repo\"},\n            {\"$ref\": \"#/$defs/cc_zypper_add_repo\"},\n            {\"$ref\": \"#/$defs/reporting_config\"},\n        ]\n        found_subschema_defs = []\n        legacy_schema_keys = []\n        for subschema in schema[\"allOf\"]:\n            if \"$ref\" in subschema:\n                found_subschema_defs.append(subschema)\n            else:  # Legacy subschema sourced from cc_* module 'schema' attr\n                legacy_schema_keys.extend(subschema[\"properties\"].keys())\n\n        assert expected_subschema_defs == found_subschema_defs\n        # This list should remain empty unless we induct new modules with\n        # legacy schema attributes defined within the cc_module.\n        assert [] == sorted(legacy_schema_keys)\n\n\nclass TestLoadDoc:\n\n    docs = get_module_variable(\"__doc__\")\n\n    @pytest.mark.parametrize(\n        \"module_name\",\n        (\"cc_apt_pipelining\",),  # new style composite schema file\n    )\n    def test_report_docs_consolidated_schema(self, module_name):\n        doc = load_doc([module_name])\n        assert doc, \"Unexpected empty docs for {}\".format(module_name)\n        assert self.docs[module_name] == doc\n\n\nclass SchemaValidationErrorTest(CiTestCase):\n    \"\"\"Test validate_cloudconfig_schema\"\"\"\n\n    def test_schema_validation_error_expects_schema_errors(self):\n        \"\"\"SchemaValidationError is initialized from schema_errors.\"\"\"\n        errors = (\n            (\"key.path\", 'unexpected key \"junk\"'),\n            (\"key2.path\", '\"-123\" is not a valid \"hostname\" format'),\n        )\n        exception = SchemaValidationError(schema_errors=errors)\n        self.assertIsInstance(exception, Exception)\n        self.assertEqual(exception.schema_errors, errors)\n        self.assertEqual(\n            'Cloud config schema errors: key.path: unexpected key \"junk\", '\n            'key2.path: \"-123\" is not a valid \"hostname\" format',\n            str(exception),\n        )\n        self.assertTrue(isinstance(exception, ValueError))\n\n\nclass TestValidateCloudConfigSchema:\n    \"\"\"Tests for validate_cloudconfig_schema.\"\"\"\n\n    with_logs = True\n\n    @pytest.mark.parametrize(\n        \"schema, call_count\",\n        ((None, 1), ({\"properties\": {\"p1\": {\"type\": \"string\"}}}, 0)),\n    )\n    @skipUnlessJsonSchema()\n    @mock.patch(\"cloudinit.config.schema.get_schema\")\n    def test_validateconfig_schema_use_full_schema_when_no_schema_param(\n        self, get_schema, schema, call_count\n    ):\n        \"\"\"Use full schema when schema param is absent.\"\"\"\n        get_schema.return_value = {\"properties\": {\"p1\": {\"type\": \"string\"}}}\n        kwargs = {\"config\": {\"p1\": \"valid\"}}\n        if schema:\n            kwargs[\"schema\"] = schema\n        validate_cloudconfig_schema(**kwargs)\n        assert call_count == get_schema.call_count\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_non_strict_emits_warnings(self, caplog):\n        \"\"\"When strict is False validate_cloudconfig_schema emits warnings.\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\"}}}\n        validate_cloudconfig_schema({\"p1\": -1}, schema, strict=False)\n        [(module, log_level, log_msg)] = caplog.record_tuples\n        assert \"cloudinit.config.schema\" == module\n        assert logging.WARNING == log_level\n        assert (\n            \"Invalid cloud-config provided: \\np1: -1 is not of type 'string'\"\n            == log_msg\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_sensitive(self, caplog):\n        \"\"\"When log_details=False, ensure details are omitted\"\"\"\n        schema = {\n            \"properties\": {\"hashed_password\": {\"type\": \"string\"}},\n            \"additionalProperties\": False,\n        }\n        validate_cloudconfig_schema(\n            {\"hashed-password\": \"secret\"},\n            schema,\n            strict=False,\n            log_details=False,\n        )\n        [(module, log_level, log_msg)] = caplog.record_tuples\n        assert \"cloudinit.config.schema\" == module\n        assert logging.WARNING == log_level\n        assert (\n            \"Invalid cloud-config provided: Please run 'sudo cloud-init \"\n            \"schema --system' to see the schema errors.\" == log_msg\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_emits_warning_on_missing_jsonschema(\n        self, caplog\n    ):\n        \"\"\"Warning from validate_cloudconfig_schema when missing jsonschema.\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\"}}}\n        with mock.patch.dict(\"sys.modules\", **{\"jsonschema\": ImportError()}):\n            validate_cloudconfig_schema({\"p1\": -1}, schema, strict=True)\n        assert \"Ignoring schema validation. jsonschema is not present\" in (\n            caplog.text\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_strict_raises_errors(self):\n        \"\"\"When strict is True validate_cloudconfig_schema raises errors.\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\"}}}\n        with pytest.raises(SchemaValidationError) as context_mgr:\n            validate_cloudconfig_schema({\"p1\": -1}, schema, strict=True)\n        assert (\n            \"Cloud config schema errors: p1: -1 is not of type 'string'\"\n            == (str(context_mgr.value))\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_honors_formats(self):\n        \"\"\"With strict True, validate_cloudconfig_schema errors on format.\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\", \"format\": \"email\"}}}\n        with pytest.raises(SchemaValidationError) as context_mgr:\n            validate_cloudconfig_schema({\"p1\": \"-1\"}, schema, strict=True)\n        assert \"Cloud config schema errors: p1: '-1' is not a 'email'\" == (\n            str(context_mgr.value)\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_honors_formats_strict_metaschema(self):\n        \"\"\"With strict and strict_metaschema True, ensure errors on format\"\"\"\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\", \"format\": \"email\"}}}\n        with pytest.raises(SchemaValidationError) as context_mgr:\n            validate_cloudconfig_schema(\n                {\"p1\": \"-1\"}, schema, strict=True, strict_metaschema=True\n            )\n        assert \"Cloud config schema errors: p1: '-1' is not a 'email'\" == str(\n            context_mgr.value\n        )\n\n    @skipUnlessJsonSchema()\n    def test_validateconfig_strict_metaschema_do_not_raise_exception(\n        self, caplog\n    ):\n        \"\"\"With strict_metaschema=True, do not raise exceptions.\n\n        This flag is currently unused, but is intended for run-time validation.\n        This should warn, but not raise.\n        \"\"\"\n        schema = {\"properties\": {\"p1\": {\"types\": \"string\", \"format\": \"email\"}}}\n        validate_cloudconfig_schema(\n            {\"p1\": \"-1\"}, schema, strict_metaschema=True\n        )\n        assert (\n            \"Meta-schema validation failed, attempting to validate config\"\n            in caplog.text\n        )\n\n\nclass TestCloudConfigExamples:\n    metas = get_metas()\n    params = [\n        (meta[\"id\"], example)\n        for meta in metas.values()\n        if meta and meta.get(\"examples\")\n        for example in meta.get(\"examples\")\n    ]\n\n    @pytest.mark.parametrize(\"schema_id, example\", params)\n    @skipUnlessJsonSchema()\n    def test_validateconfig_schema_of_example(self, schema_id, example):\n        \"\"\"For a given example in a config module we test if it is valid\n        according to the unified schema of all config modules\n        \"\"\"\n        schema = get_schema()\n        config_load = load(example)\n        # cloud-init-schema-v1 is permissive of additionalProperties at the\n        # top-level.\n        # To validate specific schemas against known documented examples\n        # we need to only define the specific module schema and supply\n        # strict=True.\n        # TODO(Drop to pop/update once full schema is strict)\n        schema.pop(\"allOf\")\n        schema.update(schema[\"$defs\"][schema_id])\n        schema[\"additionalProperties\"] = False\n        # Some module examples reference keys defined in multiple schemas\n        supplemental_schemas = {\n            \"cc_ubuntu_advantage\": [\"cc_power_state_change\"],\n            \"cc_update_hostname\": [\"cc_set_hostname\"],\n            \"cc_users_groups\": [\"cc_ssh_import_id\"],\n            \"cc_disk_setup\": [\"cc_mounts\"],\n        }\n        for supplement_id in supplemental_schemas.get(schema_id, []):\n            supplemental_props = dict(\n                [\n                    (key, value)\n                    for key, value in schema[\"$defs\"][supplement_id][\n                        \"properties\"\n                    ].items()\n                ]\n            )\n            schema[\"properties\"].update(supplemental_props)\n        validate_cloudconfig_schema(config_load, schema, strict=True)\n\n\nclass TestValidateCloudConfigFile:\n    \"\"\"Tests for validate_cloudconfig_file.\"\"\"\n\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_error_on_absent_file(self, annotate):\n        \"\"\"On absent config_path, validate_cloudconfig_file errors.\"\"\"\n        with pytest.raises(\n            RuntimeError, match=\"Configfile /not/here does not exist\"\n        ):\n            validate_cloudconfig_file(\"/not/here\", {}, annotate)\n\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_error_on_invalid_header(\n        self, annotate, tmpdir\n    ):\n        \"\"\"On invalid header, validate_cloudconfig_file errors.\n\n        A SchemaValidationError is raised when the file doesn't begin with\n        CLOUD_CONFIG_HEADER.\n        \"\"\"\n        config_file = tmpdir.join(\"my.yaml\")\n        config_file.write(\"#junk\")\n        error_msg = (\n            \"Cloud config schema errors: format-l1.c1: File\"\n            f\" {config_file} needs to begin with\"\n            f' \"{CLOUD_CONFIG_HEADER.decode()}\"'\n        )\n        with pytest.raises(SchemaValidationError, match=error_msg):\n            validate_cloudconfig_file(config_file.strpath, {}, annotate)\n\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_error_on_non_yaml_scanner_error(\n        self, annotate, tmpdir\n    ):\n        \"\"\"On non-yaml scan issues, validate_cloudconfig_file errors.\"\"\"\n        # Generate a scanner error by providing text on a single line with\n        # improper indent.\n        config_file = tmpdir.join(\"my.yaml\")\n        config_file.write(\"#cloud-config\\nasdf:\\nasdf\")\n        error_msg = (\n            f\".*errors: format-l3.c1: File {config_file} is not valid yaml.*\"\n        )\n        with pytest.raises(SchemaValidationError, match=error_msg):\n            validate_cloudconfig_file(config_file.strpath, {}, annotate)\n\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_error_on_non_yaml_parser_error(\n        self, annotate, tmpdir\n    ):\n        \"\"\"On non-yaml parser issues, validate_cloudconfig_file errors.\"\"\"\n        config_file = tmpdir.join(\"my.yaml\")\n        config_file.write(\"#cloud-config\\n{}}\")\n        error_msg = (\n            f\"errors: format-l2.c3: File {config_file} is not valid yaml.\"\n        )\n        with pytest.raises(SchemaValidationError, match=error_msg):\n            validate_cloudconfig_file(config_file.strpath, {}, annotate)\n\n    @skipUnlessJsonSchema()\n    @pytest.mark.parametrize(\"annotate\", (True, False))\n    def test_validateconfig_file_sctrictly_validates_schema(\n        self, annotate, tmpdir\n    ):\n        \"\"\"validate_cloudconfig_file raises errors on invalid schema.\"\"\"\n        config_file = tmpdir.join(\"my.yaml\")\n        schema = {\"properties\": {\"p1\": {\"type\": \"string\", \"format\": \"string\"}}}\n        config_file.write(\"#cloud-config\\np1: -1\")\n        error_msg = (\n            \"Cloud config schema errors: p1: -1 is not of type 'string'\"\n        )\n        with pytest.raises(SchemaValidationError, match=error_msg):\n            validate_cloudconfig_file(config_file.strpath, schema, annotate)\n\n\nclass TestSchemaDocMarkdown:\n    \"\"\"Tests for get_meta_doc.\"\"\"\n\n    required_schema = {\n        \"title\": \"title\",\n        \"description\": \"description\",\n        \"id\": \"id\",\n        \"name\": \"name\",\n        \"frequency\": \"frequency\",\n        \"distros\": [\"debian\", \"rhel\"],\n    }\n    meta: MetaSchema = {\n        \"title\": \"title\",\n        \"description\": \"description\",\n        \"id\": \"id\",\n        \"name\": \"name\",\n        \"frequency\": \"frequency\",\n        \"distros\": [\"debian\", \"rhel\"],\n        \"examples\": [\n            'ex1:\\n    [don\\'t, expand, \"this\"]',\n            \"ex2: true\",\n        ],\n    }\n\n    def test_get_meta_doc_returns_restructured_text(self):\n        \"\"\"get_meta_doc returns restructured text for a cloudinit schema.\"\"\"\n        full_schema = copy(self.required_schema)\n        full_schema.update(\n            {\n                \"properties\": {\n                    \"prop1\": {\n                        \"type\": \"array\",\n                        \"description\": \"prop-description\",\n                        \"items\": {\"type\": \"integer\"},\n                    }\n                }\n            }\n        )\n\n        doc = get_meta_doc(self.meta, full_schema)\n        assert (\n            dedent(\n                \"\"\"\n            name\n            ----\n            **Summary:** title\n\n            description\n\n            **Internal name:** ``id``\n\n            **Module frequency:** frequency\n\n            **Supported distros:** debian, rhel\n\n            **Config schema**:\n                **prop1:** (array of integer) prop-description\n\n            **Examples**::\n\n                ex1:\n                    [don't, expand, \"this\"]\n                # --- Example2 ---\n                ex2: true\n        \"\"\"\n            )\n            == doc\n        )\n\n    def test_get_meta_doc_handles_multiple_types(self):\n        \"\"\"get_meta_doc delimits multiple property types with a '/'.\"\"\"\n        schema = {\"properties\": {\"prop1\": {\"type\": [\"string\", \"integer\"]}}}\n        assert \"**prop1:** (string/integer)\" in get_meta_doc(self.meta, schema)\n\n    def test_references_are_flattened_in_schema_docs(self):\n        \"\"\"get_meta_doc flattens and renders full schema definitions.\"\"\"\n        schema = {\n            \"$defs\": {\n                \"flattenit\": {\n                    \"type\": [\"object\", \"string\"],\n                    \"description\": \"Objects support the following keys:\",\n                    \"patternProperties\": {\n                        \"^.+$\": {\n                            \"label\": \"<opaque_label>\",\n                            \"description\": \"List of cool strings\",\n                            \"type\": \"array\",\n                            \"items\": {\"type\": \"string\"},\n                            \"minItems\": 1,\n                        }\n                    },\n                }\n            },\n            \"properties\": {\"prop1\": {\"$ref\": \"#/$defs/flattenit\"}},\n        }\n        assert (\n            dedent(\n                \"\"\"\\\n            **prop1:** (string/object) Objects support the following keys:\n\n                    **<opaque_label>:** (array of string) List of cool strings\n            \"\"\"\n            )\n            in get_meta_doc(self.meta, schema)\n        )\n\n    @pytest.mark.parametrize(\n        \"sub_schema,expected\",\n        (\n            (\n                {\"enum\": [True, False, \"stuff\"]},\n                \"**prop1:** (``true``/``false``/``stuff``)\",\n            ),\n            # When type: string and enum, document enum values\n            (\n                {\"type\": \"string\", \"enum\": [\"a\", \"b\"]},\n                \"**prop1:** (``a``/``b``)\",\n            ),\n        ),\n    )\n    def test_get_meta_doc_handles_enum_types(self, sub_schema, expected):\n        \"\"\"get_meta_doc converts enum types to yaml and delimits with '/'.\"\"\"\n        schema = {\"properties\": {\"prop1\": sub_schema}}\n        assert expected in get_meta_doc(self.meta, schema)\n\n    @pytest.mark.parametrize(\n        \"schema,expected\",\n        (\n            (  # Hide top-level keys like 'properties'\n                {\n                    \"hidden\": [\"properties\"],\n                    \"properties\": {\n                        \"p1\": {\"type\": \"string\"},\n                        \"p2\": {\"type\": \"boolean\"},\n                    },\n                    \"patternProperties\": {\n                        \"^.*$\": {\n                            \"type\": \"string\",\n                            \"label\": \"label2\",\n                        }\n                    },\n                },\n                dedent(\n                    \"\"\"\n                **Config schema**:\n                    **label2:** (string)\n                \"\"\"\n                ),\n            ),\n            (  # Hide nested individual keys with a bool\n                {\n                    \"properties\": {\n                        \"p1\": {\"type\": \"string\", \"hidden\": True},\n                        \"p2\": {\"type\": \"boolean\"},\n                    }\n                },\n                dedent(\n                    \"\"\"\n                **Config schema**:\n                    **p2:** (boolean)\n                \"\"\"\n                ),\n            ),\n        ),\n    )\n    def test_get_meta_doc_hidden_hides_specific_properties_from_docs(\n        self, schema, expected\n    ):\n        \"\"\"Docs are hidden for any property in the hidden list.\n\n        Useful for hiding deprecated key schema.\n        \"\"\"\n        assert expected in get_meta_doc(self.meta, schema)\n\n    def test_get_meta_doc_handles_nested_oneof_property_types(self):\n        \"\"\"get_meta_doc describes array items oneOf declarations in type.\"\"\"\n        schema = {\n            \"properties\": {\n                \"prop1\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"oneOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]\n                    },\n                }\n            }\n        }\n        assert \"**prop1:** (array of (string/integer))\" in get_meta_doc(\n            self.meta, schema\n        )\n\n    def test_get_meta_doc_handles_types_as_list(self):\n        \"\"\"get_meta_doc renders types which have a list value.\"\"\"\n        schema = {\n            \"properties\": {\n                \"prop1\": {\n                    \"type\": [\"boolean\", \"array\"],\n                    \"items\": {\n                        \"oneOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]\n                    },\n                }\n            }\n        }\n        assert (\n            \"**prop1:** (boolean/array of (string/integer))\"\n            in get_meta_doc(self.meta, schema)\n        )\n\n    def test_get_meta_doc_handles_flattening_defs(self):\n        \"\"\"get_meta_doc renders $defs.\"\"\"\n        schema = {\n            \"$defs\": {\n                \"prop1object\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"subprop\": {\"type\": \"string\"}},\n                }\n            },\n            \"properties\": {\"prop1\": {\"$ref\": \"#/$defs/prop1object\"}},\n        }\n        assert (\n            \"**prop1:** (object)\\n\\n        **subprop:** (string)\\n\"\n            in get_meta_doc(self.meta, schema)\n        )\n\n    def test_get_meta_doc_handles_string_examples(self):\n        \"\"\"get_meta_doc properly indented examples as a list of strings.\"\"\"\n        full_schema = copy(self.required_schema)\n        full_schema.update(\n            {\n                \"examples\": [\n                    'ex1:\\n    [don\\'t, expand, \"this\"]',\n                    \"ex2: true\",\n                ],\n                \"properties\": {\n                    \"prop1\": {\n                        \"type\": \"array\",\n                        \"description\": \"prop-description\",\n                        \"items\": {\"type\": \"integer\"},\n                    }\n                },\n            }\n        )\n        assert (\n            dedent(\n                \"\"\"\n            **Config schema**:\n                **prop1:** (array of integer) prop-description\n\n            **Examples**::\n\n                ex1:\n                    [don't, expand, \"this\"]\n                # --- Example2 ---\n                ex2: true\n            \"\"\"\n            )\n            in get_meta_doc(self.meta, full_schema)\n        )\n\n    def test_get_meta_doc_properly_parse_description(self):\n        \"\"\"get_meta_doc description properly formatted\"\"\"\n        schema = {\n            \"properties\": {\n                \"p1\": {\n                    \"type\": \"string\",\n                    \"description\": dedent(\n                        \"\"\"\\\n                        This item\n                        has the\n                        following options:\n\n                          - option1\n                          - option2\n                          - option3\n\n                        The default value is\n                        option1\"\"\"\n                    ),\n                }\n            }\n        }\n\n        assert (\n            dedent(\n                \"\"\"\n            **Config schema**:\n                **p1:** (string) This item has the following options:\n\n                        - option1\n                        - option2\n                        - option3\n\n                The default value is option1\n\n        \"\"\"\n            )\n            in get_meta_doc(self.meta, schema)\n        )\n\n    def test_get_meta_doc_raises_key_errors(self):\n        \"\"\"get_meta_doc raises KeyErrors on missing keys.\"\"\"\n        schema = {\n            \"properties\": {\n                \"prop1\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"oneOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]\n                    },\n                }\n            }\n        }\n        for key in self.meta:\n            invalid_meta = copy(self.meta)\n            invalid_meta.pop(key)\n            with pytest.raises(KeyError) as context_mgr:\n                get_meta_doc(invalid_meta, schema)\n            assert key in str(context_mgr.value)\n\n    def test_label_overrides_property_name(self):\n        \"\"\"get_meta_doc overrides property name with label.\"\"\"\n        schema = {\n            \"properties\": {\n                \"prop1\": {\n                    \"type\": \"string\",\n                    \"label\": \"label1\",\n                },\n                \"prop_no_label\": {\n                    \"type\": \"string\",\n                },\n                \"prop_array\": {\n                    \"label\": \"array_label\",\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"some_prop\": {\"type\": \"number\"},\n                        },\n                    },\n                },\n            },\n            \"patternProperties\": {\n                \"^.*$\": {\n                    \"type\": \"string\",\n                    \"label\": \"label2\",\n                }\n            },\n        }\n        meta_doc = get_meta_doc(self.meta, schema)\n        assert \"**label1:** (string)\" in meta_doc\n        assert \"**label2:** (string\" in meta_doc\n        assert \"**prop_no_label:** (string)\" in meta_doc\n        assert \"Each object in **array_label** list\" in meta_doc\n\n        assert \"prop1\" not in meta_doc\n        assert \".*\" not in meta_doc\n\n\nclass TestAnnotatedCloudconfigFile:\n    def test_annotated_cloudconfig_file_no_schema_errors(self):\n        \"\"\"With no schema_errors, print the original content.\"\"\"\n        content = b\"ntp:\\n  pools: [ntp1.pools.com]\\n\"\n        parse_cfg, schemamarks = load_with_marks(content)\n        assert content == annotated_cloudconfig_file(\n            parse_cfg, content, schema_errors=[], schemamarks=schemamarks\n        )\n\n    def test_annotated_cloudconfig_file_with_non_dict_cloud_config(self):\n        \"\"\"Error when empty non-dict cloud-config is provided.\n\n        OurJSON validation when user-data is None type generates a bunch\n        schema validation errors of the format:\n        ('', \"None is not of type 'object'\"). Ignore those symptoms and\n        report the general problem instead.\n        \"\"\"\n        content = b\"\\n\\n\\n\"\n        expected = \"\\n\".join(\n            [\n                content.decode(),\n                \"# Errors: -------------\",\n                \"# E1: Cloud-config is not a YAML dict.\\n\\n\",\n            ]\n        )\n        assert expected == annotated_cloudconfig_file(\n            None,\n            content,\n            schema_errors=[(\"\", \"None is not of type 'object'\")],\n            schemamarks={},\n        )\n\n    def test_annotated_cloudconfig_file_schema_annotates_and_adds_footer(self):\n        \"\"\"With schema_errors, error lines are annotated and a footer added.\"\"\"\n        content = dedent(\n            \"\"\"\\\n            #cloud-config\n            # comment\n            ntp:\n              pools: [-99, 75]\n            \"\"\"\n        ).encode()\n        expected = dedent(\n            \"\"\"\\\n            #cloud-config\n            # comment\n            ntp:\t\t# E1\n              pools: [-99, 75]\t\t# E2,E3\n\n            # Errors: -------------\n            # E1: Some type error\n            # E2: -99 is not a string\n            # E3: 75 is not a string\n\n            \"\"\"\n        )\n        parsed_config, schemamarks = load_with_marks(content[13:])\n        schema_errors = [\n            (\"ntp\", \"Some type error\"),\n            (\"ntp.pools.0\", \"-99 is not a string\"),\n            (\"ntp.pools.1\", \"75 is not a string\"),\n        ]\n        assert expected == annotated_cloudconfig_file(\n            parsed_config, content, schema_errors, schemamarks=schemamarks\n        )\n\n    def test_annotated_cloudconfig_file_annotates_separate_line_items(self):\n        \"\"\"Errors are annotated for lists with items on separate lines.\"\"\"\n        content = dedent(\n            \"\"\"\\\n            #cloud-config\n            # comment\n            ntp:\n              pools:\n                - -99\n                - 75\n            \"\"\"\n        ).encode()\n        expected = dedent(\n            \"\"\"\\\n            ntp:\n              pools:\n                - -99\t\t# E1\n                - 75\t\t# E2\n            \"\"\"\n        )\n        parsed_config, schemamarks = load_with_marks(content[13:])\n        schema_errors = [\n            (\"ntp.pools.0\", \"-99 is not a string\"),\n            (\"ntp.pools.1\", \"75 is not a string\"),\n        ]\n        assert expected in annotated_cloudconfig_file(\n            parsed_config, content, schema_errors, schemamarks=schemamarks\n        )\n\n\nclass TestMain:\n\n    exclusive_combinations = itertools.combinations(\n        [\"--system\", \"--docs all\", \"--config-file something\"], 2\n    )\n\n    @pytest.mark.parametrize(\"params\", exclusive_combinations)\n    def test_main_exclusive_args(self, params, capsys):\n        \"\"\"Main exits non-zero and error on required exclusive args.\"\"\"\n        params = list(itertools.chain(*[a.split() for a in params]))\n        with mock.patch(\"sys.argv\", [\"mycmd\"] + params):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n\n        _out, err = capsys.readouterr()\n        expected = (\n            \"Error:\\n\"\n            \"Expected one of --config-file, --system or --docs arguments\\n\"\n        )\n        assert expected == err\n\n    def test_main_missing_args(self, capsys):\n        \"\"\"Main exits non-zero and reports an error on missing parameters.\"\"\"\n        with mock.patch(\"sys.argv\", [\"mycmd\"]):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n\n        _out, err = capsys.readouterr()\n        expected = (\n            \"Error:\\n\"\n            \"Expected one of --config-file, --system or --docs arguments\\n\"\n        )\n        assert expected == err\n\n    def test_main_absent_config_file(self, capsys):\n        \"\"\"Main exits non-zero when config file is absent.\"\"\"\n        myargs = [\"mycmd\", \"--annotate\", \"--config-file\", \"NOT_A_FILE\"]\n        with mock.patch(\"sys.argv\", myargs):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n        _out, err = capsys.readouterr()\n        assert \"Error:\\nConfigfile NOT_A_FILE does not exist\\n\" == err\n\n    def test_main_invalid_flag_combo(self, capsys):\n        \"\"\"Main exits non-zero when invalid flag combo used.\"\"\"\n        myargs = [\"mycmd\", \"--annotate\", \"--docs\", \"DOES_NOT_MATTER\"]\n        with mock.patch(\"sys.argv\", myargs):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n        _, err = capsys.readouterr()\n        assert (\n            \"Error:\\nInvalid flag combination. \"\n            \"Cannot use --annotate with --docs\\n\" == err\n        )\n\n    def test_main_prints_docs(self, capsys):\n        \"\"\"When --docs parameter is provided, main generates documentation.\"\"\"\n        myargs = [\"mycmd\", \"--docs\", \"all\"]\n        with mock.patch(\"sys.argv\", myargs):\n            assert 0 == main(), \"Expected 0 exit code\"\n        out, _err = capsys.readouterr()\n        assert \"\\nNTP\\n---\\n\" in out\n        assert \"\\nRuncmd\\n------\\n\" in out\n\n    def test_main_validates_config_file(self, tmpdir, capsys):\n        \"\"\"When --config-file parameter is provided, main validates schema.\"\"\"\n        myyaml = tmpdir.join(\"my.yaml\")\n        myargs = [\"mycmd\", \"--config-file\", myyaml.strpath]\n        myyaml.write(b\"#cloud-config\\nntp:\")  # shortest ntp schema\n        with mock.patch(\"sys.argv\", myargs):\n            assert 0 == main(), \"Expected 0 exit code\"\n        out, _err = capsys.readouterr()\n        assert \"Valid cloud-config: {0}\\n\".format(myyaml) == out\n\n    @mock.patch(\"cloudinit.config.schema.read_cfg_paths\")\n    @mock.patch(\"cloudinit.config.schema.os.getuid\", return_value=0)\n    def test_main_validates_system_userdata(\n        self, m_getuid, m_read_cfg_paths, capsys, paths\n    ):\n        \"\"\"When --system is provided, main validates system userdata.\"\"\"\n        m_read_cfg_paths.return_value = paths\n        ud_file = paths.get_ipath_cur(\"userdata_raw\")\n        write_file(ud_file, b\"#cloud-config\\nntp:\")\n        myargs = [\"mycmd\", \"--system\"]\n        with mock.patch(\"sys.argv\", myargs):\n            assert 0 == main(), \"Expected 0 exit code\"\n        out, _err = capsys.readouterr()\n        assert \"Valid cloud-config: system userdata\\n\" == out\n\n    @mock.patch(\"cloudinit.config.schema.os.getuid\", return_value=1000)\n    def test_main_system_userdata_requires_root(self, m_getuid, capsys, paths):\n        \"\"\"Non-root user can't use --system param\"\"\"\n        myargs = [\"mycmd\", \"--system\"]\n        with mock.patch(\"sys.argv\", myargs):\n            with pytest.raises(SystemExit) as context_manager:\n                main()\n        assert 1 == context_manager.value.code\n        _out, err = capsys.readouterr()\n        expected = (\n            \"Error:\\nUnable to read system userdata as non-root user. \"\n            \"Try using sudo\\n\"\n        )\n        assert expected == err\n\n\ndef _get_meta_doc_examples():\n    examples_dir = Path(cloud_init_project_dir(\"doc/examples\"))\n    assert examples_dir.is_dir()\n\n    return (\n        str(f)\n        for f in examples_dir.glob(\"cloud-config*.txt\")\n        if not f.name.startswith(\"cloud-config-archive\")\n    )\n\n\nclass TestSchemaDocExamples:\n    schema = get_schema()\n\n    @pytest.mark.parametrize(\"example_path\", _get_meta_doc_examples())\n    @skipUnlessJsonSchema()\n    def test_schema_doc_examples(self, example_path):\n        validate_cloudconfig_file(example_path, self.schema)\n\n\nclass TestStrictMetaschema:\n    \"\"\"Validate that schemas follow a stricter metaschema definition than\n    the default. This disallows arbitrary key/value pairs.\n    \"\"\"\n\n    @skipUnlessJsonSchema()\n    def test_modules(self):\n        \"\"\"Validate all modules with a stricter metaschema\"\"\"\n        (validator, _) = get_jsonschema_validator()\n        for (name, value) in get_schemas().items():\n            if value:\n                validate_cloudconfig_metaschema(validator, value)\n            else:\n                logging.warning(\"module %s has no schema definition\", name)\n\n    @skipUnlessJsonSchema()\n    def test_validate_bad_module(self):\n        \"\"\"Throw exception by default, don't throw if throw=False\n\n        item should be 'items' and is therefore interpreted as an additional\n        property which is invalid with a strict metaschema\n        \"\"\"\n        (validator, _) = get_jsonschema_validator()\n        schema = {\n            \"type\": \"array\",\n            \"item\": {\n                \"type\": \"object\",\n            },\n        }\n        with pytest.raises(\n            SchemaValidationError,\n            match=r\"Additional properties are not allowed.*\",\n        ):\n\n            validate_cloudconfig_metaschema(validator, schema)\n\n        validate_cloudconfig_metaschema(validator, schema, throw=False)\n\n\nclass TestMeta:\n    def test_valid_meta_for_every_module(self):\n        all_distros = {\n            name for distro in OSFAMILIES.values() for name in distro\n        }\n        all_distros.add(\"all\")\n        for module in get_modules():\n            assert \"frequency\" in module.meta\n            assert \"distros\" in module.meta\n            assert {module.meta[\"frequency\"]}.issubset(FREQUENCIES)\n            assert set(module.meta[\"distros\"]).issubset(all_distros)\n"], "filenames": ["cloudinit/cmd/main.py", "cloudinit/config/schema.py", "tests/integration_tests/modules/test_cli.py", "tests/unittests/config/test_schema.py"], "buggy_code_start_loc": [458, 200, 20, 289], "buggy_code_end_loc": [459, 243, 76, 290], "fixing_code_start_loc": [458, 201, 21, 289], "fixing_code_end_loc": [461, 252, 85, 312], "type": "CWE-532", "message": "Sensitive data could be exposed in world readable logs of cloud-init before version 22.3 when schema failures are reported. This leak could include hashed passwords.", "other": {"cve": {"id": "CVE-2022-2084", "sourceIdentifier": "security@ubuntu.com", "published": "2023-04-19T22:15:10.207", "lastModified": "2023-05-01T17:39:10.673", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Sensitive data could be exposed in world readable logs of cloud-init before version 22.3 when schema failures are reported. This leak could include hashed passwords."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security@ubuntu.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-532"}]}, {"source": "security@ubuntu.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-532"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:canonical:cloud-init:*:*:*:*:*:*:*:*", "versionEndExcluding": "22.3", "matchCriteriaId": "40F3D313-A853-4C22-B9C6-774118D80091"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:20.04:*:*:*:lts:*:*:*", "matchCriteriaId": "902B8056-9E37-443B-8905-8AA93E2447FB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:21.10:*:*:*:*:*:*:*", "matchCriteriaId": "AAE4D2D0-CEEB-416F-8BC5-A7987DF56190"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:22.04:*:*:*:lts:*:*:*", "matchCriteriaId": "359012F1-2C63-415A-88B8-6726A87830DE"}]}]}], "references": [{"url": "https://github.com/canonical/cloud-init/commit/4d467b14363d800b2185b89790d57871f11ea88c", "source": "security@ubuntu.com", "tags": ["Patch"]}, {"url": "https://ubuntu.com/security/notices/USN-5496-1", "source": "security@ubuntu.com", "tags": ["Broken Link"]}]}, "github_commit_url": "https://github.com/canonical/cloud-init/commit/4d467b14363d800b2185b89790d57871f11ea88c"}}