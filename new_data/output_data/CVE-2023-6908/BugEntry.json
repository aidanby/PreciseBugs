{"buggy_code": ["#/usr/bin/env python2\r\n# -*- coding: utf-8 -*-\r\n\r\nimport json\r\nimport os\r\nimport urllib\r\nimport shutil\r\nimport yaml \r\nimport zipfile\r\nimport hashlib\r\nimport base64\r\nimport tarfile\r\n\r\nfrom datetime import datetime\r\n\r\nfrom flask import Flask\r\nfrom flask import request, redirect, render_template, url_for, flash\r\nfrom flask import jsonify\r\nfrom flask import send_file\r\n\r\nfrom app import app\r\n\r\nimport parser_management\r\n\r\nfrom app.database.dbstuff import *\r\nfrom app.database.elkdb import *\r\n\r\nfrom werkzeug.utils import secure_filename\r\nfrom bson import json_util\r\nfrom bson.json_util import dumps\r\n\r\n\r\nSIDEBAR = {\r\n    \"sidebar\"           : app.config['FLASK_CASE_SIDEBAR'],\r\n    \"open\"              : app.config['SIDEBAR_OPEN'],\r\n    'current_version'   : app.config['GIT_KUIPER_VERSION']\r\n}\r\n\r\nRemoveRawFiles = app.config['FLASK_REMOVE_RAW_FILES']  # remove the uploaded raw files after unzip it to the machine files\r\n\r\n# =================================================\r\n#               Helper Functions\r\n# =================================================\r\n\r\n# ================================ create folder\r\n# create folder if not exists, used to create case upload folder \r\ndef create_folders(path):\r\n    try:\r\n        os.makedirs(path)\r\n        return [True, \"Folder [\"+path+\"] created\"]\r\n    except OSError as e:\r\n        if \"File exists\" in str(e):\r\n            return [True , \"Folder [\"+path+\"] already exists\" ]\r\n        else:\r\n            return [False , str(e) ]\r\n    except Exception as e:\r\n        return [False , str(e) ]\r\n\r\n# ================================ remove folder\r\ndef remove_folder(path):\r\n    try:\r\n        shutil.rmtree(path)\r\n        return [True, \"Folder [\"+path+\"] removed\"]\r\n    except Exception as e:\r\n        return [False, \"Error: \" + str(e)] \r\n\r\n\r\n# ================================ remove file\r\ndef remove_file(path):\r\n    try:\r\n        os.remove(path)\r\n        return [True, 'File ['+path+'] removed']\r\n    except Exception as e:\r\n        return [False, \"Error: \" + str(e)]\r\n\r\n\r\n# ================================ is file exists\r\ndef is_file_exists(path):\r\n    return os.path.isfile(path)\r\n\r\n\r\n# ================================ MD5 for file\r\ndef md5(fname):\r\n    try:\r\n        hash_md5 = hashlib.md5()\r\n        with open(fname, \"rb\") as f:\r\n            for chunk in iter(lambda: f.read(4096), b\"\"):\r\n                hash_md5.update(chunk)\r\n        return [True, hash_md5.hexdigest()]\r\n    except Exception as e:\r\n        return [False, str(e)]\r\n\r\n\r\n# ================================ decompress file\r\n# decompress the provided file to the dst_path\r\ndef decompress_file(archive_path,dst_path):\r\n    try:\r\n        createfolders = create_folders(dst_path)\r\n        if createfolders[0] == False:\r\n            return createfolders\r\n        \r\n        # decompress archive based on archive type\r\n        if zipfile.is_zipfile(archive_path):\r\n            unzip_file(archive_path, dst_path)\r\n        elif tarfile.is_tarfile(archive_path):\r\n            untar_file(archive_path, dst_path)\r\n        else:\r\n            raise Exception(\"File [\" + archive_path + \"] is not an archive\")\r\n        return [True , \"All files of [\"+archive_path+\"] extracted to [\"+dst_path+\"]\"]\r\n\r\n    except UnicodeDecodeError as e:\r\n        #handle unicode errors, like utf-8 codec issues\r\n        return [True , \"All files of [\"+archive_path+\"] partially extracted to [\"+dst_path+\"]\"]\r\n    except Exception as e:\r\n        return [False, \"Error extracting the archive content: \" + str(e)]\r\n\r\n\r\n# ================================ unzip file\r\n# unzip the provided file to the dst_path\r\ndef unzip_file(zip_path, dst_path):\r\n    with zipfile.ZipFile(zip_path , mode='r') as zfile:\r\n        zfile.extractall(path=dst_path)\r\n\r\n\r\n# ================================ untar file\r\n# untar the provided file to the dst_path\r\ndef untar_file(tar_path, dst_path):\r\n    with tarfile.open(tar_path , mode='r') as tfile:\r\n        tfile.extractall(path=dst_path)\r\n\r\n\r\n# ================================ list zip file content\r\n# list zip file content\r\ndef list_zip_file(zip_path):\r\n    try:\r\n        zip_ref = zipfile.ZipFile(zip_path, 'r')\r\n        zip_content = []\r\n        for z in zip_ref.namelist():\r\n            # skip folders \r\n            if z.endswith('/'):\r\n                continue\r\n            zip_content.append(z)\r\n        return [True, zip_content]\r\n    except Exception as e:\r\n        return [False, str(e)]\r\n\r\n\r\n# ================================ json beautifier\r\n# return json in a beautifier\r\ndef json_beautifier(js):\r\n    return json.dumps(js, indent=4, sort_keys=True)\r\n\r\n\r\n# ================================ get important fields\r\n# return json of all parsers and the important fields (field , json path)\r\ndef get_CASE_FIELDS():\r\n    parsers_details = db_parsers.get_parser()\r\n    if parsers_details[0] == False:\r\n        return parsers_details # if failed return the error message\r\n\r\n    case_fields = {}\r\n    for p in parsers_details[1]:\r\n        case_fields[ p['name'] ] = []\r\n        if 'important_field' in p.keys():\r\n            for f in p['important_field']:\r\n                case_fields[ p['name'] ].append( [ f['name'] , \"_source.Data.\" + f['path'] ] )\r\n    return [True, case_fields]\r\n\r\n\r\n# ================================ get machine name\r\n# return machine name based on the filename\r\ndef get_machine_name(filename):\r\n    machine_name = os.path.splitext(filename)[0]\r\n    if machine_name.endswith('.tar'):\r\n        machine_name = os.path.splitext(machine_name)[0]\r\n    return machine_name.replace(\"/\" , \"_\")\r\n\r\n\r\n# ============================= upload files\r\n# this function handle uploaded files, decompress it, create machine if machine uploaded, etc.\r\ndef upload_file(file , case_id , machine=None , base64_name=None):\r\n    \r\n    source_filename = secure_filename(file.filename) if base64_name is None else base64.b64decode(base64_name)\r\n    isUploadMachine = True if machine is None else False \r\n    \r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Handle uploaded file [\"+source_filename+\"]\")\r\n    # ====== prepare the variables \r\n    \r\n    temp_filename   = str(datetime.now() ).split('.')[0].replace(' ' , 'T') + \"-\" + source_filename\r\n    \r\n    if isUploadMachine:\r\n        # if the option is upload machine\r\n        machine_name    = get_machine_name(source_filename)\r\n        machine_id      = case_id + \"_\" + machine_name\r\n    else:\r\n        # if upload artifacts\r\n        machine_id      = machine\r\n    \r\n    \r\n    try: \r\n        # ======= validate machine exists or not  \r\n        # check if machine already exists\r\n        machine_info = db_cases.get_machine_by_id(machine_id)\r\n        if machine_info[0] == False:\r\n            # if there was exception when checking if the machine exists\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed checking if the machine [\"+machine_id+\"] already exists\", reason=machine_info[1])\r\n            return [False, {'result' : False , 'filename' : source_filename , 'message' : 'Failed checking if the machine already exists: ' + str(machine_info[1])}]\r\n\r\n\r\n        if isUploadMachine:\r\n            # if upload machine, then make sure there is no other machine already exists\r\n            if machine_info[0] == True and machine_info[1] is not None:\r\n                # if the machine already exists\r\n                logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading machine\" , reason=\"Machine [\"+machine_id+\"] already exists\")\r\n                return [ False, {'result' : False , 'filename' : source_filename , 'message' : 'Machine already exists'}]\r\n        else:\r\n            # if upload artifacts, then make sure there is machine to upload the artifacts to it\r\n            if machine_info[0] == True and machine_info[1] is None:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading artifacts\" , reason=\"Machine [\"+machine_id+\"] not found\")\r\n                return [ False, {'result' : False , 'filename' : source_filename , 'message' : 'Machine not exists'}]\r\n\r\n\r\n        # ======= create folders \r\n        # create the machine folder in files folder\r\n        files_folder    = app.config[\"UPLOADED_FILES_DEST\"]     + \"/\" + case_id + \"/\" + machine_id + \"/\"\r\n        raw_folder      = app.config[\"UPLOADED_FILES_DEST_RAW\"] + \"/\" + case_id + \"/\" + machine_id + \"/\"\r\n            \r\n        create_files_folder = create_folders( files_folder )  # create the folder for the case in files if not exists\r\n        create_raw_folder   = create_folders( raw_folder )    # create the folder for the case in raw folder if not exists\r\n        if create_files_folder[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating the folder [\"+files_folder+\"]\", reason=create_files_folder[1])\r\n            return [False, {'result' : False , 'filename' : source_filename , 'message' : \"Failed creating the folder [\"+files_folder+\"], \" + create_files_folder[1]}]\r\n        if create_raw_folder[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating the folder [\"+raw_folder+\"]\", reason=create_raw_folder[1])\r\n            return [False, {'result' : False , 'filename' : source_filename , 'message' : \"Failed creating the folder [\"+raw_folder+\"], \" + create_raw_folder[1]}]\r\n\r\n        \r\n                \r\n\r\n        # ====== save the file\r\n        # save the file to the raw data\r\n        try:\r\n            file.save(raw_folder + temp_filename)   \r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed saving the file [\"+source_filename+\"]\", reason=str(e))\r\n            return jsonify({'result' : False , 'filename' : source_filename , 'message' : 'Failed saving the file ['+source_filename+']: ' + str(e)})\r\n        \r\n\r\n        # ======= check hash if exists        \r\n        file_hash       = md5(raw_folder + temp_filename)\r\n        # get hash values for all files for this machine\r\n        for (dirpath, dirnames, filenames) in os.walk(raw_folder):\r\n            for f in filenames:\r\n                if md5(dirpath + f) == file_hash and f != temp_filename:\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading file [\"+temp_filename+\"]\", reason=\"Match same hash of file [\"+f+\"]\")\r\n                    return [False, {'result': False , 'filename': source_filename , 'message' : 'The following two files has same MD5 hash<br > - Uploaded: ' + temp_filename + '<br > - Exists: '+f}]\r\n        \r\n\r\n        # ====== decompress zip file or move it to files folder\r\n        if zipfile.is_zipfile(raw_folder + temp_filename) or tarfile.is_tarfile(raw_folder + temp_filename):\r\n            # if archive\r\n            # decompress the file to machine files\r\n            try:\r\n                unzip_fun = decompress_file(raw_folder + temp_filename, files_folder + temp_filename + \"/\")\r\n\r\n                if unzip_fun[0] == True:\r\n                    if RemoveRawFiles:\r\n                        # remove the raw file\r\n                        remove_raw_files = remove_file(raw_folder + temp_filename)\r\n                        if remove_raw_files[0] == False:\r\n                            logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing raw file [\"+raw_folder + temp_filename+\"]\", reason=remove_raw_files[1])\r\n\r\n\r\n                else:\r\n                    remove_raw_files = remove_file(raw_folder +  temp_filename) # remove file if exists\r\n                    if remove_raw_files[0] == False:\r\n                        logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing raw file [\"+raw_folder + temp_filename+\"]\", reason=remove_raw_files[1])\r\n\r\n                    remove_files_folder = remove_folder(files_folder + temp_filename + \"/\") # remove file if exists\r\n                    if remove_files_folder[0] == False:\r\n                        logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing files folder [\"+files_folder + temp_filename + \"/\"+\"]\", reason=remove_files_folder[1])\r\n                    \r\n\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed decompressing the file [\"+raw_folder + temp_filename+\"]\", reason=unzip_fun[1])\r\n                    return [False, {'result' : False , 'filename' : source_filename , 'message' : unzip_fun[1]}]\r\n            \r\n            # if unzip failed\r\n            except Exception as e:\r\n                if 'password required' in str(e):\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed decompressing the file [\"+raw_folder + temp_filename+\"]\", reason='password required')\r\n                    return [False, {'result' : False , 'filename' : source_filename , 'message' : 'File require password'}]\r\n                else:\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed decompressing the file [\"+raw_folder + temp_filename+\"]\", reason=str(e))\r\n                    return [False, {'result' : False , 'filename' : source_filename , 'message' : 'Failed to unzip the file: ' + str(e)}]\r\n        else:\r\n            # if not archive\r\n\r\n            try:\r\n                # create folder in files folder to include the file \r\n                create_files_folder   = create_folders( files_folder + temp_filename + \"/\" )    # create the folder which will include the uploaded file (we are using folder to keep the original file name untouched)\r\n                if create_files_folder[0] == False:\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating the folder [\"+files_folder+\"]\", reason=create_files_folder[1])\r\n                    return [False, {'result' : False , 'filename' : source_filename , 'message' : \"Failed creating the folder [\"+files_folder+\"], \" + create_files_folder[1]}]\r\n\r\n                shutil.copy(raw_folder + temp_filename, files_folder + temp_filename + \"/\" + source_filename)  \r\n            except Exception as e:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed moving the file [\"+raw_folder + temp_filename+\"] to files folder\", reason=str(e))\r\n                return [False, {'result' : False , 'filename' : source_filename , 'message' : 'Failed moving the file to files folder: ' + str(e)}]\r\n\r\n\r\n        # ====== create machine\r\n        if isUploadMachine:\r\n            create_m = db_cases.add_machine({\r\n                'main_case'     : case_id,\r\n                'machinename'   : machine_name.replace(\"/\" , \"_\")\r\n            })\r\n\r\n            if create_m[0] == False: # if creating the machine failed\r\n                remove_file(raw_folder +  temp_filename) # remove file if exists\r\n                remove_folder(files_folder + temp_filename + \"/\") # remove file if exists\r\n\r\n                if remove_file[0] == False:\r\n                    logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing files folder [\"+raw_folder +  temp_filename + \"/\"+\"]\", reason=remove_file[1])\r\n\r\n                if remove_folder[0] == False:\r\n                    logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing files folder [\"+files_folder + temp_filename + \"/\"+\"]\", reason=remove_folder[1])\r\n\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading the machine [\"+machine_id+\"]\", reason=create_m[1])\r\n                return jsonify({'result' : False , 'filename' : source_filename , 'message' : create_m[1]}) \r\n\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Machine [\"+machine_id+\"] uploaded\")\r\n        else:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: File [\"+source_filename+\"] uploaded to machine [\"+machine_id+\"]\")\r\n\r\n        return [True, {'result': True , 'filename' : source_filename, 'message': source_filename}]\r\n\r\n    except Exception as e:\r\n        if isUploadMachine:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading the machine\", reason=str(e))\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading the artifacts for machine [\"+machine_id+\"]\", reason=str(e))\r\n        return [False, {'result' : False , 'filename' : source_filename , 'message' : \"Failed uploading the artifacts for machine [\"+machine_id+\"]: \" + str(e)}]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# export list of results as a response based on the case_id and body, \r\n# if fields specified the output will be a csv file, if not specified it will be a json file\r\ndef export_stream_es(case_id , body, chunk_size , fields = None):\r\n    body['size'] = chunk_size\r\n    body[\"from\"] = 0  \r\n    scroll_id = None\r\n    while True:\r\n        # request the data from elasticsearch\r\n        try:\r\n            if scroll_id is None: \r\n                res = db_es.query( case_id, body , scroll=\"5m\") \r\n            else:\r\n                res = db_es.query_scroll( scroll_id=scroll_id, scroll=\"5m\" ) \r\n \r\n\r\n            if res[0] == False: \r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed export query artifacts from dataabase\", reason=res[1])\r\n                yield json.dumps({'success' : False  , 'message' : res[1]} )\r\n \r\n            buff        = ','.join(fields) if fields is not None and scroll_id is None else ''\r\n            scroll_id   = res[1]['_scroll_id'] \r\n            results     = res[1]['hits']['hits']\r\n            if len(results):     \r\n                # print the csv header if there is a fields specified\r\n                for rec in results: \r\n                    if fields is not None:   \r\n                        # if the fields specified, export csv files\r\n                        vals = []   \r\n                        for f in fields:\r\n                            v = json_get_val_by_path(rec['_source'] , f)\r\n                            val = v[1] if v[0] else \"\"\r\n                            vals.append( val )\r\n                        buff += '\\n' + ','.join(vals)\r\n                    else:\r\n                        # if fields not specified, export it is a json\r\n                        buff += json.dumps(rec['_source'])\r\n                    \r\n                yield buff\r\n            else:\r\n                break\r\n\r\n        except Exception as e:\r\n            yield json.dumps({'success' : False  , 'message' : str(e)} )\r\n\r\n\r\n# =================================================\r\n#               Flask Functions\r\n# =================================================\r\n# - Dashboard \r\n# - Machines\r\n# - Artifacts\r\n# - Timeline\r\n# - Alerts\r\n# - graph\r\n\r\n\r\n# =================== Dashboard =======================\r\n\r\n# ================================ dashboard page\r\n@app.route('/case/<case_id>/dashboard', methods=['GET'])\r\ndef case_dashboard(case_id):\r\n\r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case details\", reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=case[1])\r\n\r\n    \r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n\r\n\r\n    # get rules information\r\n    all_rules = db_rules.get_rules()\r\n    if all_rules[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting rules\", reason=all_rules[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=all_rules[1])\r\n\r\n    \r\n    alerts = {\r\n        \"Critical\"  : 0,\r\n        \"High\"      : 0,\r\n        \"Medium\"    : 0,\r\n        \"Low\"       : 0,\r\n        \"Whitelist\" : 0\r\n    }\r\n\r\n    # for each rule get total count of each rule and increament the severity of it\r\n    for rule in all_rules[1]:\r\n        body = {\r\n                \"query\":{\r\n                    \"query_string\":{\r\n                        \"query\" : rule['rule'],\r\n                        \"default_field\": \"catch_all\"\r\n                    }\r\n                },\r\n                \"size\":0\r\n        }\r\n        res = db_es.query( case_id, body )\r\n        if res[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting rules\", reason=all_rules[1])\r\n            continue\r\n\r\n        alerts[rule['rule_severity']] += res[1]['hits']['total']['value']\r\n\r\n\r\n    # get machines information\r\n    machines = db_cases.get_machines(case_id)\r\n    if machines[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case machines\", reason=machines[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=machines[1])\r\n\r\n    # dashboard info to be pushed\r\n    dashboard_info = {\r\n        'alerts' : alerts,\r\n        'machines' : machines[1]\r\n    }\r\n\r\n    return render_template('case/dashboard.html',case_details=case[1] ,SIDEBAR=SIDEBAR , dashboard_info=dashboard_info)\r\n\r\n\r\n\r\n# =================== Machines =======================\r\n\r\n# ================================ list machines\r\n# list all machines in the case\r\n@app.route(\"/case/<case_id>/\" , defaults={'group_name': None})\r\n@app.route(\"/case/<case_id>/<group_name>\")\r\ndef all_machines(case_id , group_name):\r\n\r\n\r\n    # check messages or errors \r\n    message = None if 'message' not in request.args else request.args['message']\r\n    err_msg = None if 'err_msg' not in request.args else request.args['err_msg']\r\n    # collect information \r\n    machines        = db_cases.get_machines(case_id , group_name)\r\n    case            = db_cases.get_case_by_id(case_id)\r\n    parsers_details = db_parsers.get_parser()\r\n    groups_list     = db_groups.get_groups(case_id)\r\n    # check if there is error getting the information  \r\n    error = None\r\n    if machines[0] == False:    \r\n        error = [\"Case[\"+case_id+\"]: Failed getting case machines\" , machines[1]]\r\n    elif case[0] == False:\r\n        error = [\"Case[\"+case_id+\"]: Failed getting case information\" , case[1]]\r\n    elif case[1] is None:\r\n        error = [\"Case[\"+case_id+\"]: Failed getting case information\" , 'Index not found']\r\n    elif parsers_details[0] == False:\r\n        error = [\"Case[\"+case_id+\"]: Failed getting parsers information\" , parsers_details[1]]\r\n    elif groups_list[0] == False:\r\n        error = [\"Case[\"+case_id+\"]: Failed getting groups information\" , groups_list[1]]\r\n\r\n\r\n    # add specifiy the selected group in the list\r\n    if group_name is not None:\r\n        for i in groups_list[1]:\r\n            if i['_id'] == case_id + \"_\" + group_name:\r\n                i['selected'] = True \r\n\r\n \r\n    if error is not None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=error[0], reason=error[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=error[0] + \"<br />\" + error[1])\r\n\r\n    return render_template('case/machines.html',case_details=case[1] , all_machines=machines[1],SIDEBAR=SIDEBAR ,parsers_details =parsers_details[1] , groups_list=groups_list[1] , message = message , err_msg=err_msg)\r\n\r\n\r\n\r\n# ================================ get processing progress\r\n@app.route(\"/case/<case_id>/progress\" , methods=['POST'])\r\ndef all_machines_progress(case_id):\r\n    if request.method == 'POST':\r\n        ajax_str            =  urllib.unquote(request.data).decode('utf8')\r\n        machines_list       = json.loads(ajax_str)['machines_list']\r\n        machines_progress   = []\r\n        for machine in machines_list:\r\n            \r\n            machines        = db_files.get_parsing_progress(machine)\r\n            if machines[0]:\r\n\r\n                # calculate progress\r\n                machine_static = {\r\n                        'pending' : 0,\r\n                        'parsing' : 0,\r\n                        'done'    : 0,\r\n                        'queued'  : 0,\r\n                        'error'   : 0\r\n                }\r\n                for parser in machines[1].keys():\r\n                    for f in machines[1][parser]:\r\n                        machine_static[f['status']] += 1\r\n\r\n\r\n                machines_progress.append( {'machine': machine , 'progress' : machine_static } )\r\n            else:\r\n                logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the process progress for machine [\"+machine+\"]\", reason=machines[1])\r\n                return redirect(url_for('all_machines',case_id=case_id , err_msg=machine[1]))\r\n        return json.dumps(machines_progress)\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ machine files status \r\n# this will list all files and their parser and show their status (done, parsing, pending, etc)\r\n@app.route(\"/case/<case_id>/machine_files/<machine_id>\" , methods=['GET'])\r\ndef case_machine_files_status(case_id , machine_id):\r\n    # upload machine page\r\n    if request.method == 'GET':\r\n\r\n        case        = db_cases.get_case_by_id(case_id)\r\n        CASE_FIELDS = get_CASE_FIELDS()\r\n\r\n        # if there is no case exist\r\n        if case[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason=case[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />\" + case[1])\r\n        if case[0] == True and case[1] is None:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='case not found')\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />case not found\")\r\n\r\n        # get files\r\n        files = db_files.get_by_machine(machine_id)\r\n        if files[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting machine files\", reason=files[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting machine files<br />\" + files[1])\r\n        \r\n        return render_template('case/machine_file_status.html',case_details=case[1] ,SIDEBAR=SIDEBAR, machine_id=machine_id , db_files=files[1]['files'] )\r\n\r\n\r\n\r\n\r\n# ================================ disable/enable file processing\r\n# this function disable/enable a selected file on machine for specific parser, so it will exclude it from parsing\r\n# for example disable windows events security file from being parsing\r\n@app.route(\"/case/<case_id>/disable_enable_selected_files/<machine_id>\" , methods=['POST'])\r\ndef case_disable_enable_selected_files(case_id , machine_id):\r\n    # upload machine page\r\n    if request.method == 'POST':\r\n        \r\n        ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n        ajax_data = json.loads(ajax_str)['data']\r\n        case = db_cases.get_case_by_id(case_id)\r\n        # if there is no case exist\r\n        if case[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]\", reason=case[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]<br />\" + case[1])\r\n        elif case[0] == True and case[1] is None:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]\", reason='case not found')\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]<br />case not found\")\r\n\r\n\r\n        disable = True\r\n        if ajax_data['action'] == 'enable':\r\n            disable = False\r\n        up = db_files.disable_enable_file(machine_id , ajax_data['path'] , ajax_data['parser'] , disable)\r\n        \r\n        if up[0] == True:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: File [\"+ajax_data['path']+\"] \"+ ajax_data['action'])\r\n            ajax_data['result'] = True \r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]\", reason=up[1])\r\n            ajax_data['result'] = False\r\n            \r\n        return jsonify(ajax_data)\r\n\r\n\r\n# ================================ Upload machines\r\n# upload machine page to upload files to the artifacts file upload,\r\n# this is the page allow to upload multiple zip file, each considered as machine\r\n@app.route(\"/case/<case_id>/upload_machine\" , methods=['POST' , 'GET'])\r\ndef case_upload_machine(case_id):\r\n    # upload machine page\r\n    if request.method == 'GET':\r\n\r\n        CASE_FIELDS = get_CASE_FIELDS()\r\n        if CASE_FIELDS[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n            \r\n        case = db_cases.get_case_by_id(case_id)\r\n        if case[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case [\"+case_id+\"] information\", reason=case[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />\" + case[1])\r\n        elif case[0] == True and case[1] is None:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case [\"+case_id+\"] information\", reason='Index not found')\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case [\"+case_id+\"] information<br />Index not found\")\r\n        \r\n\r\n        return render_template('case/upload_machines.html',case_details=case[1] ,SIDEBAR=SIDEBAR )\r\n\r\n    # upload the machine files ajax\r\n    elif request.method == 'POST':\r\n        # get file\r\n        file = request.files['files[]']\r\n        # if there is a file to upload\r\n        if file:\r\n            base64_name = None if 'base64_name' not in request.form else request.form['base64_name'] \r\n            # start handling uploading the file\r\n            uf = upload_file(file , case_id , base64_name=base64_name)\r\n            return json.dumps(uf[1])\r\n\r\n        return json.dumps({'result' : False , 'filename' : '' , 'message' : 'There is no file selected'})\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n        \r\n\r\n# ================================ Upload Artifacts\r\n# upload artifacts for specific machine\r\n@app.route('/case/<main_case_id>/uploadartifacts/<machine_case_id>', methods=['POST'])\r\ndef main_upload_artifacts(main_case_id,machine_case_id):\r\n\r\n    if request.method == 'POST':\r\n        try:\r\n               \r\n            # get file\r\n            file = request.files['files[]']\r\n\r\n\r\n            # if there is a file to upload\r\n            if file:\r\n                base64_name = None if 'base64_name' not in request.form else request.form['base64_name']       \r\n\r\n                # start handling uploading the file\r\n                uf = upload_file(file , main_case_id , machine_case_id , base64_name=base64_name)\r\n                return json.dumps(uf[1])\r\n\r\n            return json.dumps({'result' : False , 'filename' : '' , 'message' : 'There is no file selected'})\r\n\r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+main_case_id+\"]: Failed upload artifacts\" , reason=str(e))\r\n            return jsonify({'result':'error' , 'message':str(e)})\r\n            \r\n    else:\r\n        return redirect(url_for('home_page'))\r\n        \r\n\r\n\r\n# ================================ add machine\r\n# add a machine to the case\r\n@app.route(\"/case/<case_id>/add_machine/\" , methods=['POST','GET'])\r\ndef add_machine(case_id):\r\n    if request.method == 'POST':\r\n        machine_details = {\r\n            'machinename'   :request.form['machinename'].replace(\"/\" , \"_\"),\r\n            'main_case'     :case_id,\r\n            'ip'            :request.form['ip'],\r\n\r\n        }\r\n\r\n        machine = db_cases.add_machine(machine_details)\r\n        if machine[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Machine [\"+machine_details['machinename']+\"] created\")\r\n            return redirect(url_for('all_machines',case_id=case_id , message= \"Machine [\" + machine[1].lstrip(case_id + \"_\") + \"] created\"))\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating machine [\"+machine_details['machinename']+\"]\", reason=machine[1])\r\n            return redirect(url_for('all_machines',case_id=case_id , err_msg=machine[1]))\r\n\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ delete machine\r\n# delete machine from case\r\n@app.route(\"/case/<case_id>/delete_machine/<machines_list>\" , methods=['GET'])\r\ndef delete_machine(case_id , machines_list):\r\n    if request.method == 'GET':\r\n\r\n        logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Delete machines\" , reason=machines_list)\r\n        # delete machine from mongo db\r\n        for machine in machines_list.split(','):\r\n            db_machine = db_cases.delete_machine(machine)\r\n            if db_machine[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting machine [\"+machine+\"]\" , reason=db_machine[1])\r\n                continue\r\n\r\n            # delete machines records from elasticsearch\r\n            q = {\"query\": {\r\n                    \"query_string\": {\r\n                        \"query\": \"(machine.keyword:\\\"\"+machine.replace('-' , '\\\\-')+\"\\\")\"\r\n                    }\r\n                }\r\n            } \r\n            es_machine = db_es.del_record_by_query(case_id , q)\r\n            if es_machine:\r\n                logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Machine [\"+machine+\"] deleted\")\r\n\r\n            # delete all files\r\n            try:\r\n                files_folder    = app.config[\"UPLOADED_FILES_DEST\"]     + \"/\" + case_id + \"/\" + machine + \"/\"\r\n                raw_folder      = app.config[\"UPLOADED_FILES_DEST_RAW\"] + \"/\" + case_id + \"/\" + machine + \"/\"\r\n                shutil.rmtree(files_folder, ignore_errors=True)\r\n                shutil.rmtree(raw_folder, ignore_errors=True)\r\n                logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: deleted machine folders [\"+machine+\"]\" , reason=files_folder + \",\" + raw_folder)\r\n            except Exception as e:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting machine folders [\"+machine+\"]\" , reason=str(e))\r\n\r\n        return redirect(url_for('all_machines',case_id=case_id))\r\n\r\n\r\n\r\n\r\n# ================================ process artifacts\r\n# run the selected parser for the machines specified\r\n@app.route('/case/<main_case_id>/processartifacts/<machine_case_id>/<parser_name>', methods=['GET'])\r\ndef main_process_artifacts(main_case_id,machine_case_id,parser_name):\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+main_case_id+\"]: Start processing machine [\"+machine_case_id+\"], parsers [\"+parser_name+\"]\")\r\n\r\n    parsers = parser_name.split(',')\r\n    task = parser_management.run_parsers.apply_async((main_case_id,machine_case_id,parsers) , link_error=parser_management.task_error_handler.s())\r\n    #task = parser_management.run_parsers.apply_async((main_case_id,machine_case_id,parsers))\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+main_case_id+\"]: task ID [\"+task.id+\"]\")\r\n    return jsonify({'data': 'started processing'})\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# ================================ add group\r\n# add a group to the case\r\n@app.route(\"/case/<case_id>/add_group\" , methods=['POST','GET'])\r\ndef add_group(case_id):\r\n    \r\n    if request.method == 'POST':\r\n        group_name  = request.form['group_name']\r\n        group       = db_groups.add_group(case_id , group_name)\r\n\r\n        if group[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Group [\"+group_name+\"] created\")\r\n            return redirect(url_for('all_machines',case_id=case_id , message= \"Group [\"+group_name+\"] created\"))\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating group [\"+group_name+\"]\", reason=group[1])\r\n            return redirect(url_for('all_machines',case_id=case_id , err_msg=group[1]))\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n\r\n\r\n\r\n\r\n# ================================ assign to group \r\n# assign the selected machines to group\r\n@app.route(\"/case/<case_id>/assign_to_group/\" , methods=['POST'])\r\ndef assign_to_group(case_id):\r\n    \r\n    if request.method == 'POST':\r\n        ajax_str        =  urllib.unquote(request.data).decode('utf8')\r\n        machines_list   = json.loads(ajax_str)['machines_list']\r\n        group_name      = json.loads(ajax_str)['group_name']\r\n        res       = db_cases.assign_to_group(machines_list , group_name)\r\n\r\n        if res[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: assigned machines to group [\"+group_name+\"]\" , reason=\",\".join(machines_list))\r\n            return json.dumps({'result' : 'successful' , 'message' : 'assigned to groups'})\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed assigning  machines to group [\"+group_name+\"]\", reason=res[1])\r\n            return json.dumps({'result' : 'failed' , 'message' : res[1]})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n\r\n\r\n# ================================ deassign from group \r\n# deassign the selected machines from group\r\n@app.route(\"/case/<case_id>/deassign_from_group/\" , methods=['POST'])\r\ndef deassign_from_group(case_id):\r\n    \r\n    if request.method == 'POST':\r\n        ajax_str        =  urllib.unquote(request.data).decode('utf8')\r\n        machines_list   = json.loads(ajax_str)['machines_list']\r\n        group_name      = json.loads(ajax_str)['group_name']\r\n        res       = db_cases.deassign_from_group(machines_list , group_name)\r\n\r\n        if res[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: deassigned machines from group [\"+group_name+\"]\" , reason=\",\".join(machines_list))\r\n            return json.dumps({'result' : 'successful' , 'message' : 'deassigned from group'})\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deassigning  machines from group [\"+group_name+\"]\", reason=res[1])\r\n            return json.dumps({'result' : 'failed' , 'message' : res[1]})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n\r\n\r\n\r\n\r\n# ================================ deassign to group \r\n# deassign the selected machines from group\r\n@app.route(\"/case/<case_id>/delete_group/<group_name>\" , methods=['GET'])\r\ndef delete_group(case_id , group_name):\r\n    \r\n    if request.method == 'GET':\r\n        res       = db_groups.delete_group(case_id , group_name)\r\n\r\n        if res[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Group [\"+group_name+\"] deleted\")\r\n            return json.dumps({'result' : 'successful' , 'message' : 'Group deleted'})\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting group [\"+group_name+\"]\", reason=res[1])\r\n            return json.dumps({'result' : 'failed' , 'message' : res[1]})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# =================== Artifacts =======================\r\n\r\n\r\n# ================================ get artifacts data types\r\n# get all the list of artifacts for this case\r\n@app.route('/case/<case_id>/browse_artifacts_list_ajax', methods=['POST'])\r\ndef browse_artifacts_list_ajax(case_id):\r\n\r\n    if request.method == \"POST\":\r\n        try:\r\n            ajax_str    =  urllib.unquote(request.data).decode('utf8')\r\n            ajax_data   = json.loads(ajax_str)['data']\r\n            body = {\r\n                \"size\":30,\r\n \r\n            }\r\n            body = {\r\n                \"query\": {\r\n                    \"query_string\" : {\r\n                        \"query\" : ajax_data['query'],\r\n                        \"default_field\" : \"catch_all\"\r\n                    }\r\n                } ,\r\n                \"size\":0\r\n            }\r\n            \r\n            body[\"aggs\"] = {\r\n                    \"data_type\": {\r\n                        \"terms\" : {\r\n                            \"field\" : \"data_type.keyword\",\r\n                            \"size\" : 500,\r\n                            \"order\": {\r\n                                \"_key\": \"asc\"\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Query artifacts list\", reason=json.dumps(body))\r\n            res = db_es.query( case_id, body )\r\n            if res[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed query artifacts list from dataabase\", reason=res[1])\r\n                return json.dumps({'res_total' : 0 , 'res_records' : [] , 'aggs' : []})\r\n\r\n            aggs_records = res[1][\"aggregations\"][\"data_type\"][\"buckets\"]\r\n            res_records = res[1]['hits']['hits']\r\n            res_total   = res[1]['hits']['total']['value']\r\n             \r\n            return json.dumps({\"res_total\" : res_total , \"res_records\" : res_records , 'aggs' : aggs_records})\r\n  \r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed query artifacts list from dataabase\", reason=str(e))\r\n            return json.dumps({'res_total' : 0 , 'res_records' : [] , 'aggs' : []})\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ get artifacts records ajax\r\n# retrive all artifacts records using ajax\r\n@app.route('/case/<case_id>/browse_artifacts_ajax', methods=[\"POST\"])\r\ndef case_browse_artifacts_ajax(case_id):\r\n    if request.method == \"POST\": \r\n        try:\r\n            records_per_page = 30\r\n            ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n            ajax_data = json.loads(ajax_str)['data']\r\n            body = {\r\n                \"from\": int(ajax_data[\"wanted_page\"]) * records_per_page,\r\n                \"size\":records_per_page,\r\n            }\r\n            if \"query\" in ajax_data.keys() and ajax_data['query'] != \"None\":\r\n                body[\"query\"] = {\r\n                    \"query_string\" : {\r\n                        \"query\" : ajax_data['query'],\r\n                        \"default_field\" : \"catch_all\"\r\n                    }\r\n                } \r\n\r\n            if \"sort_by\" in ajax_data.keys() and ajax_data['sort_by'] != \"None\":\r\n                order = \"asc\" if ajax_data['sort_by']['order'] == 0 else \"desc\"\r\n                body[\"sort\"] = {\r\n                    ajax_data['sort_by']['name'] : {\"order\" : order}\r\n                } \r\n               \r\n            if \"group_by\" in ajax_data.keys() and ajax_data['group_by'] != \"None\" and \"fields\" in ajax_data['group_by'].keys() and len(ajax_data['group_by']['fields']):\r\n                body['size'] = 0 # if the data came from aggs, then no need for the actual data\r\n                  \r\n                # specifiy whether it is multi fields or single field \r\n                isSingleField = len(ajax_data['group_by'][\"fields\"]) == 1\r\n                \r\n                # sort buckets\r\n                sort_by_order = \"asc\" if \"sort_count\" in ajax_data['group_by'].keys() and ajax_data['group_by'][\"sort_count\"] == \"asc\" else \"desc\"\r\n\r\n                # agg terms\r\n                aggs_dict = {\r\n                    \"size\"                      : 1000 ,    \r\n                    \"show_term_doc_count_error\" : True , \r\n                    \"order\"                     : {\"_count\" : sort_by_order}\r\n                }\r\n                if isSingleField:\r\n                    terms                   = ajax_data['group_by'][\"fields\"][0] + \".keyword\"\r\n                    aggs_type               = \"terms\"\r\n                    aggs_dict[\"field\"]      = terms\r\n                    aggs_dict[\"missing\"]    = \"\"\r\n                else: \r\n                    terms                   = []\r\n                    for t in ajax_data['group_by'][\"fields\"]:\r\n                        terms.append( { \"field\" : t + \".keyword\" , \"missing\" : \"\"} )     \r\n                    aggs_type               = \"multi_terms\" \r\n                    aggs_dict[\"terms\"]      = terms\r\n                  \r\n                body[\"aggs\"] = {     \r\n                    \"group_by\" : {\r\n                        aggs_type : aggs_dict,  \r\n                        \"aggs\" : {\r\n                            \"my_buckets\" : { \r\n                                \"bucket_sort\" : {   \r\n                                    \"from\" : int(ajax_data[\"wanted_page\"]) * records_per_page,\r\n                                    \"size\" : records_per_page,\r\n                                }, \r\n                            }, \r\n                            \"group_by_top_hits\": {\r\n                                \"top_hits\" : {\r\n                                    \"size\" : ajax_data['group_by']['size'],     \r\n                                },       \r\n                            }, \r\n                        },  \r\n                    },      \r\n                    \"get_total\": {\r\n                        aggs_type : aggs_dict\r\n                    },   \r\n                    \"get_total_stats\": {\r\n                        \"stats_bucket\": {\r\n                            \"buckets_path\": \"get_total._count\" \r\n                        } \r\n                    }\r\n                } \r\n            logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Query artifacts\", reason=json.dumps(body))\r\n            res = db_es.query( case_id, body )\r\n            if res[0] == False:\r\n                print res[1]   \r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed query artifacts from dataabase\", reason=res[1])\r\n                return json.dumps({'res_total' : 0 , 'res_records' : [] , 'aggs' : []})\r\n\r\n            res_records = res[1]['hits']['hits']\r\n            res_total   = res[1]['hits']['total']['value']\r\n            aggs_records= res[1][\"aggregations\"][\"group_by\"][\"buckets\"] if \"aggregations\" in res[1].keys() else []\r\n    \r\n            # this function get the record and retrive the machine name from the database and enrich it\r\n            def get_machine_by_id(record):\r\n                if \"machine\" in record['_source'].keys():\r\n                    machine = db_cases.get_machine_by_id(record['_source']['machine'])\r\n                    if machine[0] == True and machine[1] is not None:\r\n                        record['_source']['machinename'] = machine[1]['machinename']\r\n                    else:\r\n                        record['_source']['machinename'] = record['_source']['machine']\r\n                        logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the machine name\", reason=machine[1])\r\n\r\n\r\n            for i in range( 0  , len(res_records) ):    \r\n                get_machine_by_id(res_records[i])\r\n            \r\n            if len(aggs_records):   \r\n                res_total = res[1][\"aggregations\"][\"get_total_stats\"]['count']\r\n                \"\"\"\r\n                res_records = []\r\n                for i in range(0 , len(aggs_records)):\r\n                    res_records.append( aggs_records[i][\"key\"] )\r\n                \"\"\"    \r\n                res_records = []\r\n                 \r\n                for i in range(0 , len(aggs_records)):\r\n                    for r in range(0 , len(aggs_records[i]['group_by_top_hits']['hits']['hits'])):\r\n                        get_machine_by_id(aggs_records[i]['group_by_top_hits']['hits']['hits'][r])\r\n                    \r\n                    rec = aggs_records[i]['group_by_top_hits']['hits']['hits'][0]\r\n                    rec['group_by'] = {\r\n                        'key'       : aggs_records[i]['key'],\r\n                        'doc_count' : aggs_records[i]['doc_count']\r\n                    } \r\n \r\n                    # if used a single field, then include it on a list to match the multi-fields group by\r\n                    if isSingleField:\r\n                        rec['group_by']['key'] = [rec['group_by']['key']]\r\n                    res_records.append( rec )\r\n                \r\n\r\n\r\n            ajax_res = {\"res_total\" : res_total , \"res_records\" : res_records , 'aggs' : ajax_data['group_by'][\"fields\"]}\r\n\r\n            return json.dumps(ajax_res)\r\n   \r\n \r\n        except Exception as e:\r\n            print str(e)\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the browser artifacts\", reason=str(e))\r\n            return json.dumps({\"res_total\" : 0 , \"res_records\" : [], 'aggs' : None})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n# ================================ get artifacts data types\r\n# get all artifacts for case\r\n@app.route('/case/<case_id>/browse_artifacts', methods=['GET'])\r\ndef case_browse_artifacts(case_id):  \r\n   \r\n    CASE_FIELDS = get_CASE_FIELDS()\r\n    if CASE_FIELDS[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n            \r\n\r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\" , reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information, <br />\"+case[1])\r\n    \r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n    \r\n    # get all fields from elasticsearch\r\n    # used for advanced search to list all fields when searching\r\n    fields_mapping = db_es.get_mapping_fields(case_id)\r\n    if fields_mapping[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the mapping fields \" , reason=fields_mapping[1])\r\n        return render_template('case/error_page.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting the mapping fields for case [\"+case_id+\"], <br />\"+fields_mapping[1])\r\n        \r\n    \r\n    query = {\r\n            \"AND\" : []\r\n            }\r\n\r\n    if 'q' in request.args:\r\n        try:\r\n            query = json.loads(urllib.unquote(request.args['q']).decode('utf-8'))\r\n        except Exception as e:\r\n            pass\r\n   \r\n    if 'machine' in request.args:\r\n        query[\"AND\"].append({'==machine' : request.args['machine']})\r\n\r\n    if 'rule' in request.args:\r\n        query['AND'].append({'==rule' : request.args['rule']})\r\n\r\n \r\n\r\n    group = None \r\n    if 'group' in request.args:\r\n        try:\r\n            machines        = db_cases.get_machines(case_id , request.args['group'])\r\n            if machines[0] == False:    \r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case machines\" , reason=machines[1])\r\n            else:  \r\n                group = {\r\n                    'group' : request.args['group'],\r\n                    'machines' : []\r\n                }\r\n                for m in machines[1]:  \r\n                    group['machines'].append(case_id + \"_\" + m['machinename'])\r\n                      \r\n                    \r\n        except Exception as e:  \r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case machines\" , reason=machines[1])\r\n\r\n  \r\n    # get all rules\r\n    all_rules = db_rules.get_rules()\r\n    if all_rules[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the rules information\" , reason=all_rules[1])\r\n        return render_template('case/error_page.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting the rules information, <br />\" + all_rules[1])\r\n \r\n    q = json.dumps(query)   \r\n    return render_template('case/browse_artifacts.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , search_query = q , fields_mapping=fields_mapping[1] , rules = all_rules[1] , group=group)\r\n\r\n\r\n\r\n  \r\n@app.route('/case/<case_id>/browse_artifacts_export' , methods=['POST'])\r\ndef browse_artifacts_export(case_id):\r\n    if request.method == \"POST\":\r\n        try:\r\n            \r\n            request_str =  urllib.unquote(request.data).decode('utf8')\r\n            request_json = json.loads(request_str)['data']\r\n                \r\n            logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Browse artifacts export\", reason=json.dumps(request_json))\r\n\r\n            # == from - to\r\n            body = {}\r\n            # == query   \r\n            query = '*' \r\n            if 'query' in request_json.keys() and request_json['query'] != None:\r\n                request_json['query'] = request_json['query'].strip()\r\n                query = '*' if request_json['query'] == \"\" or request_json['query'] is None else request_json['query']\r\n                body[\"query\"] = {\r\n                    \"query_string\" : {\r\n                        \"query\" : '!(data_type:\\\"tag\\\") AND ' + query,\r\n                        \"default_field\" : \"catch_all\"\r\n                    }\r\n                }\r\n            # == sort  \r\n            if 'sort_by' in request_json.keys() and request_json['sort_by'] != None:\r\n                order = \"asc\" if request_json['sort_by']['order'] == 0 else \"desc\"\r\n                body[\"sort\"] = {request_json['sort_by']['name'] : {\"order\" : order}}\r\n            else:\r\n                body[\"sort\"] = {'Data.@timestamp' : {\"order\" : 'asc'}}\r\n            \r\n\r\n            # == fields\r\n            fields = None\r\n            if 'fields' in request_json.keys() and request_json['fields'] != None :\r\n                fields = request_json['fields']\r\n                body['_source'] = {}\r\n                body['_source']['includes'] = fields \r\n  \r\n   \r\n            logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Export Query artifacts\", reason=json.dumps(body))\r\n\r\n            from flask import Response  \r\n\r\n            # get total number of records\r\n            body['size'] = 0\r\n            res = db_es.query( case_id, body) \r\n\r\n            if res[0] == False: \r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed export query artifacts from dataabase\", reason=res[1])\r\n                return json.dumps({\"res_total\" : 0 , \"res_records\" : []})\r\n            total_records = res[1]['hits']['total']['value']\r\n\r\n            return Response( \r\n                        export_stream_es(case_id=case_id, body=body , chunk_size=30 , fields=fields),\r\n                        mimetype='text/csv',\r\n                        headers= {\"total\" : str(total_records)}\r\n                    )\r\n                      \r\n   \r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed exporting the artifacts\", reason=str(e))\r\n            return json.dumps({\"res_total\" : 0 , \"res_records\" : []})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n# =================== Timeline =======================\r\n\r\n# ================================ timeline page\r\n# get the time line page\r\n@app.route('/case/<case_id>/timeline', methods=['GET'])\r\ndef case_timeline(case_id):\r\n\r\n    \r\n    CASE_FIELDS = get_CASE_FIELDS()\r\n    if CASE_FIELDS[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n            \r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />\" + case[1])\r\n    \r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n    return render_template('case/timeline.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1])\r\n\r\n\r\n# ================================ get tags ajax\r\n# get all tags for the case via ajax\r\n@app.route('/case/<case_id>/timeline_ajax', methods=['POST'])\r\ndef case_timeline_ajax(case_id ):\r\n    if request.method == \"POST\":\r\n        try:\r\n            ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n            ajax_data = json.loads(ajax_str)['data']\r\n\r\n\r\n            body = {\r\n                \"query\": {\r\n                    \"query_string\" : {\r\n                        \"query\" : 'data_type:tag'\r\n                    }\r\n                },\r\n                \"sort\":{\r\n                    \"Data.@timestamp\" : {\"order\" : \"asc\"}\r\n                },\r\n                \"size\": 2000\r\n            }\r\n            res = db_es.query( case_id, body )\r\n\r\n            if res[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed retriving timeline from database\", reason=res[1])\r\n                return json.dumps( {'result': 'failed' , 'tags' : res[1]} )\r\n\r\n\r\n            total_tags  = res[1]['hits']['total']['value']\r\n            tags        =  res[1]['hits']['hits']\r\n\r\n\r\n            for t in range(0 , len(tags)):\r\n                if 'record_id' in tags[t]['_source']['Data'].keys():\r\n                    record_id = tags[t]['_source']['Data']['record_id']\r\n                    rec = db_es.get_record_by_id(case_id , record_id)\r\n                    if rec[0] == False:\r\n                        logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting record details in timeline\", reason=rec[1])\r\n\r\n                    if rec[0] == True and rec[1] != False:\r\n                        tags[t]['_source']['Data']['record_details'] = rec[1]\r\n\r\n\r\n            return json.dumps({ 'result' : 'successful' , \"tags\" : tags})\r\n\r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed retriving timeline\", reason=str(e))\r\n            return json.dumps( {'result': 'failed' , 'tags' : \"Failed retriving timeline\" + str(e)} )\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ delete tag ajax\r\n# delete a specific tag by its ID\r\n@app.route('/case/<case_id>/timeline_delete_tag_ajax', methods=['POST'])\r\ndef case_timeline_delete_tag(case_id ):\r\n    if request.method == \"POST\":\r\n        ajax_str    =  urllib.unquote(request.data).decode('utf8')\r\n        ajax_data   = json.loads(ajax_str)['data']\r\n        logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Delete tag\", reason=json.dumps(ajax_data))\r\n\r\n        tag_id      =  ajax_data['tag_id']\r\n        record_id   = None if 'record_id' not in ajax_data.keys() else ajax_data['record_id']\r\n\r\n        # delete the tag record\r\n        delete = db_es.del_record_by_id( case_id = case_id , record_id = tag_id)\r\n        if delete[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting tag\", reason=delete[1])\r\n            return json.dumps({'result' : 'failed' , 'data': 'Failed:' + delete[1]})\r\n        \r\n        # if the tag associated with artifact record\r\n        if record_id is not None:\r\n            # delete the tag record from record\r\n            update_field = db_es.update_field( {\"script\": \"ctx._source.remove(\\\"tag_id\\\");ctx._source.remove(\\\"tag_type\\\")\"}  , record_id , case_id)\r\n            if update_field[1] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed adding tag_id to artifact record\", reason=update_field[1])\r\n                return json.dumps({'result' : 'failed' , 'data': 'Failed deleting tag_id from artifact record: ' + update_field[1]})\r\n\r\n\r\n\r\n        logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: tag [\"+tag_id+\"] deleted\")\r\n        return json.dumps({'result' : 'successful'})\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ add tag ajax\r\n# add tag to a specifc record\r\n@app.route('/case/<case_id>/add_tag_ajax', methods=[\"POST\"])\r\ndef case_add_tag_ajax(case_id):\r\n    if request.method == \"POST\":\r\n        ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n        record_id = None\r\n        ajax_data = json.loads(ajax_str)['data']\r\n\r\n        Data = {\r\n            \"tag\"           : ajax_data['tag'] ,\r\n            \"@timestamp\"    : ajax_data['time'],\r\n            'tag_type'      : ajax_data['tag_type']\r\n        }\r\n\r\n        if 'doc_id' in ajax_data.keys():\r\n            Data['record_id']   = ajax_data['doc_id']\r\n            record_id           = ajax_data['doc_id']\r\n        if 'message' in ajax_data.keys():\r\n            Data['message']     = ajax_data['message']\r\n\r\n        logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Add tag\", reason=json.dumps(Data))\r\n\r\n        # add new record tag\r\n        record = {\r\n            \"Data\"          :Data, \r\n            \"data_source\"   :None, \r\n            \"data_type\"     :'tag', \r\n        }\r\n        up = db_es.bulk_queue_push( [record] , case_id , chunk_size=500) \r\n        #db_es.es_add_tag(data = { \"Data\" : Data  , \"data_type\" : 'tag' } , case_id = case_id )\r\n        if up[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed adding tag\", reason=up[1])\r\n            return json.dumps({'result' : 'failed' , 'data': 'Failed adding tag: ' + up[1]})\r\n\r\n        # update the tag_id to artifact record\r\n        else:\r\n            \r\n            if record_id is not None and len(up[3]) != 0:\r\n \r\n                # for each successful tag added\r\n                for tag_id in up[3]: \r\n                    update_field = db_es.update_field( {'doc': {'tag_id' : tag_id , 'tag_type' : record['Data']['tag_type'] }}  , record_id , case_id)\r\n                    \r\n                    if update_field[0] == False:\r\n                        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed adding tag_id to artifact record\", reason=update_field[1])\r\n                        return json.dumps({'result' : 'failed' , 'data': 'Failed adding tag_id to artifact record: ' + update_field[1]})\r\n\r\n                logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Tag created\")\r\n                return json.dumps({\"result\" : 'successful' , 'tag_id' : up[3][0] , 'tag_type' : record['Data']['tag_type']})\r\n            return json.dumps({\"result\" : 'successful' , 'tag_id' : up[3][0] , 'tag_type' : record['Data']['tag_type']})\r\n\r\n \r\n\r\n# ================================ generate timeline\r\n# get all tags for the case via ajax\r\n@app.route('/case/<case_id>/timeline_build_ajax', methods=['GET'])\r\ndef case_timeline_build_ajax(case_id):\r\n    if request.method == \"GET\":\r\n        try:\r\n            # create case timeline folder if not exists  \r\n            dest_timeline_folder = os.path.join( app.config['TIMELINE_FOLDER'] , case_id)\r\n            if not os.path.isdir(dest_timeline_folder):\r\n                create_folders(dest_timeline_folder)\r\n\r\n            # get if there is a previous version already exists for the case\r\n            src_filename        = 'timeline.xlsx'\r\n            dest_filename       = src_filename.rstrip(\".xlsx\") + \"_v0.xlsx\"\r\n\r\n            # get the latest timetime version\r\n            latest_version = 0\r\n            for previous_file in os.listdir(dest_timeline_folder): \r\n                try:\r\n                    if os.path.isfile(os.path.join(dest_timeline_folder, previous_file)) and previous_file.endswith(\".xlsx\"):    \r\n                        latest_version      = int(previous_file.split(\"_v\")[1].rstrip(\".xlsx\")) if int(previous_file.split(\"_v\")[1].rstrip(\".xlsx\")) > latest_version else latest_version\r\n                except Exception as e:\r\n                    pass \r\n\r\n            \r\n\r\n            # create instance for timeline builder          \r\n            views_folder        = app.config['TIMELINE_VIEWS_FOLDER']   \r\n            new_version         = latest_version + 1\r\n            dest_filename       = src_filename.rstrip(\".xlsx\") + \"_v\"+str(new_version)+\".xlsx\"\r\n            src_filename        = src_filename.rstrip(\".xlsx\") + \"_v\"+str(latest_version)+\".xlsx\" if latest_version > 0 else src_filename\r\n            dest_timeline       = os.path.join( dest_timeline_folder , dest_filename )\r\n            src_timeline        = os.path.join(dest_timeline_folder , src_filename) if latest_version > 0 else os.path.join( app.config['Timeline_Templates'] , src_filename)\r\n            t                   = buildTimeline.BuildTimeline(views_folder=views_folder , fname= src_timeline)\r\n            export_date         = str(datetime.now())\r\n\r\n\r\n            # get all yaml views from timeline views\r\n            all_rules           = t.get_views(views_folder)\r\n            requests            = []\r\n            sheet_timeline      =\"Timeline\"  \r\n            sheet_kuiper_id_col =\"KuiperID\"\r\n\r\n\r\n            all_active_rules    = []\r\n            default_rule        = None \r\n\r\n            for rule in range( 0 , len(all_rules)):\r\n                if 'default' in all_rules[rule]['condition'].keys() and all_rules[rule]['condition']['default'] == True:\r\n                    default_rule = all_rules[rule]\r\n                    continue\r\n                if 'active' in all_rules[rule]['condition'].keys() and all_rules[rule]['condition']['active'] == False:\r\n                    continue\r\n                all_active_rules.append(all_rules[rule])  \r\n\r\n\r\n            for rule in range( 0 , len(all_active_rules)):\r\n                requests.append({\r\n                        \"query\":{\r\n                            \"query_string\":{\r\n                                \"query\" : \"tag_id:* AND (\" + all_active_rules[rule][\"condition\"][\"query\"] + \")\",\r\n                                \"default_field\": \"catch_all\"\r\n                            }\r\n                        },\r\n                        \"size\": 2000\r\n                })\r\n            \r\n            requests.append({\r\n                \"query\":{\r\n                    \"query_string\":{\r\n                        \"query\" : \"tag_id:*\",\r\n                        \"default_field\": \"catch_all\"\r\n                    }\r\n                },\r\n                \"size\": 2000 \r\n            })\r\n            if len(requests):\r\n                res = db_es.multiqueries(case_id, requests)\r\n            else:\r\n                res = [True, []] # if there is no rules\r\n\r\n            if res[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting timeline generator search\", reason=res[1])\r\n                return json.dumps({ 'result' : 'failed' , \"message\" : res[1]})\r\n\r\n            # this list contains the ID of already added records to the sheet, so in the default records will not be added\r\n            added_records           = []\r\n            old_records             = t.get_values_by_column(sheet_timeline , sheet_kuiper_id_col)\r\n            to_be_removed_records   = []\r\n\r\n\r\n\r\n            for r in range(0 , len(res[1])):\r\n                if r < len(res[1])-1:\r\n                    \r\n                    # if the results belongs to a search query\r\n                    for data in res[1][r]['hits']['hits']: \r\n                        if data['_id'] in old_records:\r\n                            added_records.append(data['_id'])\r\n                        if data['_id'] in added_records: continue \r\n                        # add extra data related to the record itself\r\n                        data['_source']['_id']              = data['_id']\r\n                        data['_source']['_Export_Version']  = \"V_\" + str(new_version)\r\n                        data['_source']['_Export_Date']     = export_date \r\n\r\n \r\n                        fields_data = t.merge_data_and_fields(fields = all_active_rules[r]['fields'].copy(), data= data['_source'])\r\n                        add_res = t.add_data_to_sheet(sheet_timeline, fields_data)\r\n                        if add_res[0]:\r\n                            added_records.append(data['_id']) \r\n                        else:    \r\n                            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed adding the record [\"+data['_id']+\"] to xlsx timeline\", reason=add_res[1])\r\n                elif default_rule is not None:\r\n                    # if the results does not belongs to a search query \r\n                    for data in res[1][r]['hits']['hits']:\r\n                        if data['_id'] in old_records:\r\n                            added_records.append(data['_id'])\r\n                        if data['_id'] in added_records: continue\r\n                        \r\n                        # add extra data related to the record itself\r\n                        data['_source']['_id'] = data['_id'] \r\n                        data['_source']['_Export_Version']  = \"V_\" + str(new_version)\r\n                        data['_source']['_Export_Date']     = export_date\r\n                        \r\n                        fields_data = t.merge_data_and_fields(fields = default_rule['fields'].copy(), data= data['_source'])\r\n                        t.add_data_to_sheet(sheet_timeline, fields_data) \r\n            \r\n            # if there is a record on the old version of timeline has been untagged, remove it from the timeline new version\r\n            for r in old_records:\r\n                if r not in added_records:\r\n                    to_be_removed_records.append(r)\r\n            \r\n            for record in to_be_removed_records:\r\n                if not t.delete_row_from_sheet_by_kuiperID(sheet_timeline , record):\r\n                    logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting the record [\"+record+\"]\", reason=\"\")\r\n                    \r\n  \r\n            t.save(dest_timeline)\r\n             \r\n            return send_file(dest_timeline, as_attachment=True) \r\n\r\n        except Exception as e: \r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed retriving timeline\", reason=str(e))\r\n            return json.dumps( {'result': 'failed' , 'message' : \"Failed retriving timeline: \" + str(e)} )\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n \r\n\r\n# =================== Alerts =======================\r\n\r\n\r\n# ================================ get all alerts for case\r\n@app.route('/case/<case_id>/alerts/', defaults={'machinename': None, 'allMachines': False}, methods=['GET'])\r\n@app.route('/case/<case_id>/alerts/all/', defaults={'machinename': None, 'allMachines': True}, methods=['GET'])\r\n@app.route('/case/<case_id>/alerts/<machinename>', defaults={'allMachines': False}, methods=['GET'])\r\ndef case_alerts(case_id, machinename, allMachines):\r\n\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Open alerts page\")\r\n\r\n\r\n    CASE_FIELDS = get_CASE_FIELDS()\r\n    if CASE_FIELDS[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n            \r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />\" + case[1])\r\n    \r\n\r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n    machines = db_cases.get_machines(case_id)\r\n\r\n    if machinename is not None:\r\n        # validate machine exists\r\n        machine_id = case_id + \"_\" + machinename\r\n        machine_info = db_cases.get_machine_by_id(machine_id)\r\n        if machine_info[0] is False or machine_info[1] is None: # machine_info[0] in case of exception False. machine_info[1] is machine or None (not found) or exception-string\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed checking if the machine [\"+machinename+\"] exists\", reason=\"\")\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed checking if the machine exists<br />\" )\r\n        currentmachinename = machinename\r\n    elif allMachines is True:\r\n        currentmachinename = \"All\"\r\n        machine_id = None\r\n    else:\r\n        return render_template('case/alerts.html',case_details=case[1] ,SIDEBAR=SIDEBAR, all_rules=[], rhaegal_hits=[], machines=machines[1], currentmachinename=\"\", browse_alert_link_query=\"\", machine_id=\"\")\r\n\r\n    all_rules = db_rules.get_rules()\r\n    if all_rules[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting rules information\", reason=all_rules[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=all_rules[1])\r\n\r\n    requests = []\r\n    \r\n    for rule in all_rules[1]:\r\n        if allMachines is True:\r\n            qrule = rule['rule']\r\n        else: #elif machine_id is not None: for sure\r\n            qrule = \"machine:\" + machine_id + \" AND (\" + rule['rule'] +\")\"\r\n        requests.append({\r\n                \"query\":{\r\n                    \"query_string\":{\r\n                        \"query\" : qrule,\r\n                        \"default_field\": \"catch_all\"\r\n                    }\r\n                },\r\n                \"size\":0\r\n        })\r\n    \r\n    \r\n    if len(requests):\r\n        res = db_es.multiqueries(case_id, requests)\r\n    else:\r\n        res = [True, []] # if there is no rules\r\n\r\n    if res[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting total hits of rules from database\", reason=res[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=res[1])\r\n\r\n    #prepare browse-machine-alerts link\r\n    browse_alert_link_query = \"\"\r\n    if allMachines is False and machine_id is not None:\r\n        json_query = {\"AND\" : [{\"==machine\": machine_id}]}\r\n        browse_alert_link_query = json.dumps(json_query)\r\n\r\n    for r in range(0 , len(res[1])):\r\n        all_rules[1][r]['hits'] =  res[1][r]['hits']['total']['value']\r\n    \r\n    # build the query to get all rhaegal hits\r\n    \r\n    if allMachines is True:\r\n        qrhaegal = \"Data.rhaegal.name:*\"\r\n    else: #elif machine_id is not None: for sure anyway\r\n        qrhaegal = \"machine:\" + machine_id + \" AND (Data.rhaegal.name:*)\"\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: get Rhaegal hits\")\r\n    Rhaegal_query = {\r\n                \"query\":{\r\n                    \"query_string\":{\r\n                        \"query\" : qrhaegal,\r\n                        \"default_field\": \"catch_all\"\r\n                    }\r\n                },\r\n                \"size\":0,\r\n                \"aggs\" : {\r\n                        \"rhaegal\": {\r\n                            \"terms\" : {\r\n                                \"field\" : \"Data.rhaegal.score.keyword\",\r\n                                \"size\" : 100,\r\n                                \"order\": {\r\n                                    \"_key\": \"asc\"\r\n                                }\r\n                            },\r\n                            \"aggs\" :{\r\n                                \"names\" : {\r\n                                    \"terms\" : {\r\n                                        \"field\" : \"Data.rhaegal.name.keyword\",\r\n                                        \"size\" : 2000,\r\n                                    },\r\n                                    \"aggs\" : {\r\n                                        \"first_record\" : {\r\n                                            \"top_hits\" : {\r\n                                                \"size\" : 1\r\n                                            },\r\n                                        }\r\n                                    }\r\n                                }\r\n                            }\r\n                        }\r\n                    }\r\n        }\r\n\r\n    res = db_es.query(case_id , Rhaegal_query)\r\n    if res[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the Rhaegal hits\", reason=res[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=res[1])\r\n\r\n\r\n    Rhaegal_hits = {\r\n        \"names\" : [] , \r\n        \"scores\" : []\r\n    }\r\n    for score in res[1][\"aggregations\"][\"rhaegal\"][\"buckets\"]:\r\n        Rhaegal_hits[\"scores\"].append({\r\n            \"score\" : score[\"key\"],\r\n            \"count\" : score[\"doc_count\"]\r\n        })\r\n        \r\n        for name in score[\"names\"][\"buckets\"]:\r\n            tmp_meta = name[\"first_record\"][\"hits\"][\"hits\"][0][\"_source\"][\"Data\"][\"rhaegal\"]\r\n            tmp_meta[\"count\"] = name[\"doc_count\"]\r\n            Rhaegal_hits[\"names\"].append(tmp_meta)\r\n\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: hits [\"+str(len(Rhaegal_hits[\"names\"]))+\"]\")\r\n\r\n    return render_template('case/alerts.html',case_details=case[1] ,SIDEBAR=SIDEBAR , all_rules= all_rules[1] , rhaegal_hits=Rhaegal_hits, machines=machines[1], currentmachinename=currentmachinename,browse_alert_link_query=browse_alert_link_query, machine_id=machine_id)\r\n\r\n\r\n\r\n# =================== Graph =======================\r\n\r\n\r\n# ================================ show the graph page\r\n@app.route('/case/<case_id>/graph/<record_id>')\r\ndef graph_display(case_id , record_id):\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Open graph for record [\"+record_id+\"]\")\r\n\r\n    CASE_FIELDS = get_CASE_FIELDS()\r\n    if CASE_FIELDS[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n       \r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case details\", reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=case[1])\r\n\r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n\r\n\r\n    record = []\r\n    if record_id is not None:\r\n        query = {\r\n            \"query\": {\r\n                \"terms\": {\r\n                    \"_id\": [record_id]\r\n                }\r\n            }\r\n        }\r\n\r\n        record = db_es.query( case_id, query )\r\n        if record[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting record information for graph\", reason=record[1])\r\n            return {'result' : False , 'data' : record[1]}\r\n        record = record[1]['hits']['hits']\r\n    \r\n    return render_template('case/graph.html',case_details=case[1] , SIDEBAR=SIDEBAR, init_records = record , page_header=\"Graph\")\r\n\r\n\r\n\r\n\r\n\r\n# ================================ expand the graph nodes (search)\r\n# retrive requested nodes to be added to the graph\r\n@app.route('/case/<case_id>/expand_graph', methods=[\"POST\"])\r\ndef graph_expand(case_id):\r\n    if request.method == \"POST\":\r\n\r\n\r\n        ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n        ajax_data = json.loads(ajax_str)['data']\r\n        field = ajax_data['field']\r\n        value = ajax_data['value']\r\n        logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Expand search [\"+field+\":\"+value+\"]\")\r\n\r\n\r\n\r\n        special_chars = [ '\\\\' , '/' , ':' , '-' , '{' , '}' , '(', ')' , ' ' , '@' ]\r\n        for sc in special_chars:\r\n            value = value.replace(sc , '\\\\' + sc)\r\n\r\n        body = {\r\n            \"query\": {\r\n                \"query_string\":{\r\n                        \"query\" : \"*\" + str(value) + \"*\",\r\n                        \"default_field\" : \"catch_all\"\r\n                    }\r\n            },\r\n            \"size\": 500\r\n        }\r\n\r\n        try:\r\n            res = db_es.query( case_id, body )\r\n            if res[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting record information for graph\", reason=res[1])\r\n                ajax_res = {'response' : 'error' , 'data' : str(res[1])}\r\n\r\n            res_total = res[1]['hits']['total']['value']\r\n            res_records = res[1]['hits']['hits']\r\n            ajax_res = {'response' : 'OK' , \"res_total\" : res_total , \"res_records\" : res_records}\r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting record information for graph\", reason=str(e))\r\n            ajax_res = {'response' : 'error'}\r\n            \r\n\r\n\r\n        return json.dumps(ajax_res)\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n"], "fixing_code": ["#/usr/bin/env python2\r\n# -*- coding: utf-8 -*-\r\n\r\nimport json\r\nimport os\r\nimport urllib\r\nimport shutil\r\nimport yaml \r\nimport zipfile\r\nimport hashlib\r\nimport base64\r\nimport tarfile\r\n\r\nfrom datetime import datetime\r\n\r\nfrom flask import Flask\r\nfrom flask import request, redirect, render_template, url_for, flash\r\nfrom flask import jsonify\r\nfrom flask import send_file\r\n\r\nfrom app import app\r\n\r\nimport parser_management\r\n\r\nfrom app.database.dbstuff import *\r\nfrom app.database.elkdb import *\r\n\r\nfrom werkzeug.utils import secure_filename\r\nfrom bson import json_util\r\nfrom bson.json_util import dumps\r\n\r\n\r\nSIDEBAR = {\r\n    \"sidebar\"           : app.config['FLASK_CASE_SIDEBAR'],\r\n    \"open\"              : app.config['SIDEBAR_OPEN'],\r\n    'current_version'   : app.config['GIT_KUIPER_VERSION']\r\n}\r\n\r\nRemoveRawFiles = app.config['FLASK_REMOVE_RAW_FILES']  # remove the uploaded raw files after unzip it to the machine files\r\n\r\n# =================================================\r\n#               Helper Functions\r\n# =================================================\r\n\r\n# ================================ create folder\r\n# create folder if not exists, used to create case upload folder \r\ndef create_folders(path):\r\n    try:\r\n        os.makedirs(path)\r\n        return [True, \"Folder [\"+path+\"] created\"]\r\n    except OSError as e:\r\n        if \"File exists\" in str(e):\r\n            return [True , \"Folder [\"+path+\"] already exists\" ]\r\n        else:\r\n            return [False , str(e) ]\r\n    except Exception as e:\r\n        return [False , str(e) ]\r\n\r\n# ================================ remove folder\r\ndef remove_folder(path):\r\n    try:\r\n        shutil.rmtree(path)\r\n        return [True, \"Folder [\"+path+\"] removed\"]\r\n    except Exception as e:\r\n        return [False, \"Error: \" + str(e)] \r\n\r\n\r\n# ================================ remove file\r\ndef remove_file(path):\r\n    try:\r\n        os.remove(path)\r\n        return [True, 'File ['+path+'] removed']\r\n    except Exception as e:\r\n        return [False, \"Error: \" + str(e)]\r\n\r\n\r\n# ================================ is file exists\r\ndef is_file_exists(path):\r\n    return os.path.isfile(path)\r\n\r\n\r\n# ================================ MD5 for file\r\ndef md5(fname):\r\n    try:\r\n        hash_md5 = hashlib.md5()\r\n        with open(fname, \"rb\") as f:\r\n            for chunk in iter(lambda: f.read(4096), b\"\"):\r\n                hash_md5.update(chunk)\r\n        return [True, hash_md5.hexdigest()]\r\n    except Exception as e:\r\n        return [False, str(e)]\r\n\r\n\r\n# ================================ decompress file\r\n# decompress the provided file to the dst_path\r\ndef decompress_file(archive_path,dst_path):\r\n    try:\r\n        createfolders = create_folders(dst_path)\r\n        if createfolders[0] == False:\r\n            return createfolders\r\n        \r\n        # decompress archive based on archive type\r\n        if zipfile.is_zipfile(archive_path):\r\n            unzip_file(archive_path, dst_path)\r\n        elif tarfile.is_tarfile(archive_path):\r\n            untar_file(archive_path, dst_path)\r\n        else:\r\n            raise Exception(\"File [\" + archive_path + \"] is not an archive\")\r\n        return [True , \"All files of [\"+archive_path+\"] extracted to [\"+dst_path+\"]\"]\r\n\r\n    except UnicodeDecodeError as e:\r\n        #handle unicode errors, like utf-8 codec issues\r\n        return [True , \"All files of [\"+archive_path+\"] partially extracted to [\"+dst_path+\"]\"]\r\n    except Exception as e:\r\n        return [False, \"Error extracting the archive content: \" + str(e)]\r\n\r\n\r\n# ================================ unzip file\r\n# unzip the provided file to the dst_path\r\ndef unzip_file(zip_path, dst_path):\r\n    with zipfile.ZipFile(zip_path , mode='r') as zfile:\r\n        zfile.extractall(path=dst_path)\r\n\r\n\r\n# ================================ is within directory\r\n# check if target is within a given directory\r\ndef is_within_directory(directory, target):\r\n    abs_directory = os.path.abspath(directory)\r\n    abs_target = os.path.abspath(target)\r\n\r\n    prefix = os.path.commonprefix([abs_directory, abs_target])\r\n\r\n    return prefix == abs_directory\r\n\r\n\r\n# ================================ safe tar extraction\r\n# Safe extraction of tar file to avoid path traversal vulnerability (CVE-2007-4559)\r\n# Patch taken from https://github.com/dbt-labs/dbt-core/pull/5981/files\r\ndef safe_tar_extract(tar, dst_path):\r\n    for member in tar.getmembers():\r\n        member_path = os.path.join(dst_path, member.name)\r\n        if not is_within_directory(directory=dst_path, target=member_path):\r\n            raise Exception(\"Attempted Path Traversal in Tar File\")\r\n\r\n    tar.extractall(path=dst_path)\r\n\r\n\r\n# ================================ untar file\r\n# untar the provided file to the dst_path\r\ndef untar_file(tar_path, dst_path):\r\n    with tarfile.open(tar_path , mode='r') as tfile:\r\n        safe_tar_extract(tfile, dst_path)\r\n\r\n\r\n# ================================ list zip file content\r\n# list zip file content\r\ndef list_zip_file(zip_path):\r\n    try:\r\n        zip_ref = zipfile.ZipFile(zip_path, 'r')\r\n        zip_content = []\r\n        for z in zip_ref.namelist():\r\n            # skip folders \r\n            if z.endswith('/'):\r\n                continue\r\n            zip_content.append(z)\r\n        return [True, zip_content]\r\n    except Exception as e:\r\n        return [False, str(e)]\r\n\r\n\r\n# ================================ json beautifier\r\n# return json in a beautifier\r\ndef json_beautifier(js):\r\n    return json.dumps(js, indent=4, sort_keys=True)\r\n\r\n\r\n# ================================ get important fields\r\n# return json of all parsers and the important fields (field , json path)\r\ndef get_CASE_FIELDS():\r\n    parsers_details = db_parsers.get_parser()\r\n    if parsers_details[0] == False:\r\n        return parsers_details # if failed return the error message\r\n\r\n    case_fields = {}\r\n    for p in parsers_details[1]:\r\n        case_fields[ p['name'] ] = []\r\n        if 'important_field' in p.keys():\r\n            for f in p['important_field']:\r\n                case_fields[ p['name'] ].append( [ f['name'] , \"_source.Data.\" + f['path'] ] )\r\n    return [True, case_fields]\r\n\r\n\r\n# ================================ get machine name\r\n# return machine name based on the filename\r\ndef get_machine_name(filename):\r\n    machine_name = os.path.splitext(filename)[0]\r\n    if machine_name.endswith('.tar'):\r\n        machine_name = os.path.splitext(machine_name)[0]\r\n    return machine_name.replace(\"/\" , \"_\")\r\n\r\n\r\n# ============================= upload files\r\n# this function handle uploaded files, decompress it, create machine if machine uploaded, etc.\r\ndef upload_file(file , case_id , machine=None , base64_name=None):\r\n    \r\n    source_filename = secure_filename(file.filename) if base64_name is None else base64.b64decode(base64_name)\r\n    isUploadMachine = True if machine is None else False \r\n    \r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Handle uploaded file [\"+source_filename+\"]\")\r\n    # ====== prepare the variables \r\n    \r\n    temp_filename   = str(datetime.now() ).split('.')[0].replace(' ' , 'T') + \"-\" + source_filename\r\n    \r\n    if isUploadMachine:\r\n        # if the option is upload machine\r\n        machine_name    = get_machine_name(source_filename)\r\n        machine_id      = case_id + \"_\" + machine_name\r\n    else:\r\n        # if upload artifacts\r\n        machine_id      = machine\r\n    \r\n    \r\n    try: \r\n        # ======= validate machine exists or not  \r\n        # check if machine already exists\r\n        machine_info = db_cases.get_machine_by_id(machine_id)\r\n        if machine_info[0] == False:\r\n            # if there was exception when checking if the machine exists\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed checking if the machine [\"+machine_id+\"] already exists\", reason=machine_info[1])\r\n            return [False, {'result' : False , 'filename' : source_filename , 'message' : 'Failed checking if the machine already exists: ' + str(machine_info[1])}]\r\n\r\n\r\n        if isUploadMachine:\r\n            # if upload machine, then make sure there is no other machine already exists\r\n            if machine_info[0] == True and machine_info[1] is not None:\r\n                # if the machine already exists\r\n                logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading machine\" , reason=\"Machine [\"+machine_id+\"] already exists\")\r\n                return [ False, {'result' : False , 'filename' : source_filename , 'message' : 'Machine already exists'}]\r\n        else:\r\n            # if upload artifacts, then make sure there is machine to upload the artifacts to it\r\n            if machine_info[0] == True and machine_info[1] is None:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading artifacts\" , reason=\"Machine [\"+machine_id+\"] not found\")\r\n                return [ False, {'result' : False , 'filename' : source_filename , 'message' : 'Machine not exists'}]\r\n\r\n\r\n        # ======= create folders \r\n        # create the machine folder in files folder\r\n        files_folder    = app.config[\"UPLOADED_FILES_DEST\"]     + \"/\" + case_id + \"/\" + machine_id + \"/\"\r\n        raw_folder      = app.config[\"UPLOADED_FILES_DEST_RAW\"] + \"/\" + case_id + \"/\" + machine_id + \"/\"\r\n            \r\n        create_files_folder = create_folders( files_folder )  # create the folder for the case in files if not exists\r\n        create_raw_folder   = create_folders( raw_folder )    # create the folder for the case in raw folder if not exists\r\n        if create_files_folder[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating the folder [\"+files_folder+\"]\", reason=create_files_folder[1])\r\n            return [False, {'result' : False , 'filename' : source_filename , 'message' : \"Failed creating the folder [\"+files_folder+\"], \" + create_files_folder[1]}]\r\n        if create_raw_folder[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating the folder [\"+raw_folder+\"]\", reason=create_raw_folder[1])\r\n            return [False, {'result' : False , 'filename' : source_filename , 'message' : \"Failed creating the folder [\"+raw_folder+\"], \" + create_raw_folder[1]}]\r\n\r\n        \r\n                \r\n\r\n        # ====== save the file\r\n        # save the file to the raw data\r\n        try:\r\n            file.save(raw_folder + temp_filename)   \r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed saving the file [\"+source_filename+\"]\", reason=str(e))\r\n            return jsonify({'result' : False , 'filename' : source_filename , 'message' : 'Failed saving the file ['+source_filename+']: ' + str(e)})\r\n        \r\n\r\n        # ======= check hash if exists        \r\n        file_hash       = md5(raw_folder + temp_filename)\r\n        # get hash values for all files for this machine\r\n        for (dirpath, dirnames, filenames) in os.walk(raw_folder):\r\n            for f in filenames:\r\n                if md5(dirpath + f) == file_hash and f != temp_filename:\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading file [\"+temp_filename+\"]\", reason=\"Match same hash of file [\"+f+\"]\")\r\n                    return [False, {'result': False , 'filename': source_filename , 'message' : 'The following two files has same MD5 hash<br > - Uploaded: ' + temp_filename + '<br > - Exists: '+f}]\r\n        \r\n\r\n        # ====== decompress zip file or move it to files folder\r\n        if zipfile.is_zipfile(raw_folder + temp_filename) or tarfile.is_tarfile(raw_folder + temp_filename):\r\n            # if archive\r\n            # decompress the file to machine files\r\n            try:\r\n                unzip_fun = decompress_file(raw_folder + temp_filename, files_folder + temp_filename + \"/\")\r\n\r\n                if unzip_fun[0] == True:\r\n                    if RemoveRawFiles:\r\n                        # remove the raw file\r\n                        remove_raw_files = remove_file(raw_folder + temp_filename)\r\n                        if remove_raw_files[0] == False:\r\n                            logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing raw file [\"+raw_folder + temp_filename+\"]\", reason=remove_raw_files[1])\r\n\r\n\r\n                else:\r\n                    remove_raw_files = remove_file(raw_folder +  temp_filename) # remove file if exists\r\n                    if remove_raw_files[0] == False:\r\n                        logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing raw file [\"+raw_folder + temp_filename+\"]\", reason=remove_raw_files[1])\r\n\r\n                    remove_files_folder = remove_folder(files_folder + temp_filename + \"/\") # remove file if exists\r\n                    if remove_files_folder[0] == False:\r\n                        logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing files folder [\"+files_folder + temp_filename + \"/\"+\"]\", reason=remove_files_folder[1])\r\n                    \r\n\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed decompressing the file [\"+raw_folder + temp_filename+\"]\", reason=unzip_fun[1])\r\n                    return [False, {'result' : False , 'filename' : source_filename , 'message' : unzip_fun[1]}]\r\n            \r\n            # if unzip failed\r\n            except Exception as e:\r\n                if 'password required' in str(e):\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed decompressing the file [\"+raw_folder + temp_filename+\"]\", reason='password required')\r\n                    return [False, {'result' : False , 'filename' : source_filename , 'message' : 'File require password'}]\r\n                else:\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed decompressing the file [\"+raw_folder + temp_filename+\"]\", reason=str(e))\r\n                    return [False, {'result' : False , 'filename' : source_filename , 'message' : 'Failed to unzip the file: ' + str(e)}]\r\n        else:\r\n            # if not archive\r\n\r\n            try:\r\n                # create folder in files folder to include the file \r\n                create_files_folder   = create_folders( files_folder + temp_filename + \"/\" )    # create the folder which will include the uploaded file (we are using folder to keep the original file name untouched)\r\n                if create_files_folder[0] == False:\r\n                    logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating the folder [\"+files_folder+\"]\", reason=create_files_folder[1])\r\n                    return [False, {'result' : False , 'filename' : source_filename , 'message' : \"Failed creating the folder [\"+files_folder+\"], \" + create_files_folder[1]}]\r\n\r\n                shutil.copy(raw_folder + temp_filename, files_folder + temp_filename + \"/\" + source_filename)  \r\n            except Exception as e:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed moving the file [\"+raw_folder + temp_filename+\"] to files folder\", reason=str(e))\r\n                return [False, {'result' : False , 'filename' : source_filename , 'message' : 'Failed moving the file to files folder: ' + str(e)}]\r\n\r\n\r\n        # ====== create machine\r\n        if isUploadMachine:\r\n            create_m = db_cases.add_machine({\r\n                'main_case'     : case_id,\r\n                'machinename'   : machine_name.replace(\"/\" , \"_\")\r\n            })\r\n\r\n            if create_m[0] == False: # if creating the machine failed\r\n                remove_file(raw_folder +  temp_filename) # remove file if exists\r\n                remove_folder(files_folder + temp_filename + \"/\") # remove file if exists\r\n\r\n                if remove_file[0] == False:\r\n                    logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing files folder [\"+raw_folder +  temp_filename + \"/\"+\"]\", reason=remove_file[1])\r\n\r\n                if remove_folder[0] == False:\r\n                    logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed removing files folder [\"+files_folder + temp_filename + \"/\"+\"]\", reason=remove_folder[1])\r\n\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading the machine [\"+machine_id+\"]\", reason=create_m[1])\r\n                return jsonify({'result' : False , 'filename' : source_filename , 'message' : create_m[1]}) \r\n\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Machine [\"+machine_id+\"] uploaded\")\r\n        else:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: File [\"+source_filename+\"] uploaded to machine [\"+machine_id+\"]\")\r\n\r\n        return [True, {'result': True , 'filename' : source_filename, 'message': source_filename}]\r\n\r\n    except Exception as e:\r\n        if isUploadMachine:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading the machine\", reason=str(e))\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed uploading the artifacts for machine [\"+machine_id+\"]\", reason=str(e))\r\n        return [False, {'result' : False , 'filename' : source_filename , 'message' : \"Failed uploading the artifacts for machine [\"+machine_id+\"]: \" + str(e)}]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# export list of results as a response based on the case_id and body, \r\n# if fields specified the output will be a csv file, if not specified it will be a json file\r\ndef export_stream_es(case_id , body, chunk_size , fields = None):\r\n    body['size'] = chunk_size\r\n    body[\"from\"] = 0  \r\n    scroll_id = None\r\n    while True:\r\n        # request the data from elasticsearch\r\n        try:\r\n            if scroll_id is None: \r\n                res = db_es.query( case_id, body , scroll=\"5m\") \r\n            else:\r\n                res = db_es.query_scroll( scroll_id=scroll_id, scroll=\"5m\" ) \r\n \r\n\r\n            if res[0] == False: \r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed export query artifacts from dataabase\", reason=res[1])\r\n                yield json.dumps({'success' : False  , 'message' : res[1]} )\r\n \r\n            buff        = ','.join(fields) if fields is not None and scroll_id is None else ''\r\n            scroll_id   = res[1]['_scroll_id'] \r\n            results     = res[1]['hits']['hits']\r\n            if len(results):     \r\n                # print the csv header if there is a fields specified\r\n                for rec in results: \r\n                    if fields is not None:   \r\n                        # if the fields specified, export csv files\r\n                        vals = []   \r\n                        for f in fields:\r\n                            v = json_get_val_by_path(rec['_source'] , f)\r\n                            val = v[1] if v[0] else \"\"\r\n                            vals.append( val )\r\n                        buff += '\\n' + ','.join(vals)\r\n                    else:\r\n                        # if fields not specified, export it is a json\r\n                        buff += json.dumps(rec['_source'])\r\n                    \r\n                yield buff\r\n            else:\r\n                break\r\n\r\n        except Exception as e:\r\n            yield json.dumps({'success' : False  , 'message' : str(e)} )\r\n\r\n\r\n# =================================================\r\n#               Flask Functions\r\n# =================================================\r\n# - Dashboard \r\n# - Machines\r\n# - Artifacts\r\n# - Timeline\r\n# - Alerts\r\n# - graph\r\n\r\n\r\n# =================== Dashboard =======================\r\n\r\n# ================================ dashboard page\r\n@app.route('/case/<case_id>/dashboard', methods=['GET'])\r\ndef case_dashboard(case_id):\r\n\r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case details\", reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=case[1])\r\n\r\n    \r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n\r\n\r\n    # get rules information\r\n    all_rules = db_rules.get_rules()\r\n    if all_rules[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting rules\", reason=all_rules[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=all_rules[1])\r\n\r\n    \r\n    alerts = {\r\n        \"Critical\"  : 0,\r\n        \"High\"      : 0,\r\n        \"Medium\"    : 0,\r\n        \"Low\"       : 0,\r\n        \"Whitelist\" : 0\r\n    }\r\n\r\n    # for each rule get total count of each rule and increament the severity of it\r\n    for rule in all_rules[1]:\r\n        body = {\r\n                \"query\":{\r\n                    \"query_string\":{\r\n                        \"query\" : rule['rule'],\r\n                        \"default_field\": \"catch_all\"\r\n                    }\r\n                },\r\n                \"size\":0\r\n        }\r\n        res = db_es.query( case_id, body )\r\n        if res[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting rules\", reason=all_rules[1])\r\n            continue\r\n\r\n        alerts[rule['rule_severity']] += res[1]['hits']['total']['value']\r\n\r\n\r\n    # get machines information\r\n    machines = db_cases.get_machines(case_id)\r\n    if machines[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case machines\", reason=machines[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=machines[1])\r\n\r\n    # dashboard info to be pushed\r\n    dashboard_info = {\r\n        'alerts' : alerts,\r\n        'machines' : machines[1]\r\n    }\r\n\r\n    return render_template('case/dashboard.html',case_details=case[1] ,SIDEBAR=SIDEBAR , dashboard_info=dashboard_info)\r\n\r\n\r\n\r\n# =================== Machines =======================\r\n\r\n# ================================ list machines\r\n# list all machines in the case\r\n@app.route(\"/case/<case_id>/\" , defaults={'group_name': None})\r\n@app.route(\"/case/<case_id>/<group_name>\")\r\ndef all_machines(case_id , group_name):\r\n\r\n\r\n    # check messages or errors \r\n    message = None if 'message' not in request.args else request.args['message']\r\n    err_msg = None if 'err_msg' not in request.args else request.args['err_msg']\r\n    # collect information \r\n    machines        = db_cases.get_machines(case_id , group_name)\r\n    case            = db_cases.get_case_by_id(case_id)\r\n    parsers_details = db_parsers.get_parser()\r\n    groups_list     = db_groups.get_groups(case_id)\r\n    # check if there is error getting the information  \r\n    error = None\r\n    if machines[0] == False:    \r\n        error = [\"Case[\"+case_id+\"]: Failed getting case machines\" , machines[1]]\r\n    elif case[0] == False:\r\n        error = [\"Case[\"+case_id+\"]: Failed getting case information\" , case[1]]\r\n    elif case[1] is None:\r\n        error = [\"Case[\"+case_id+\"]: Failed getting case information\" , 'Index not found']\r\n    elif parsers_details[0] == False:\r\n        error = [\"Case[\"+case_id+\"]: Failed getting parsers information\" , parsers_details[1]]\r\n    elif groups_list[0] == False:\r\n        error = [\"Case[\"+case_id+\"]: Failed getting groups information\" , groups_list[1]]\r\n\r\n\r\n    # add specifiy the selected group in the list\r\n    if group_name is not None:\r\n        for i in groups_list[1]:\r\n            if i['_id'] == case_id + \"_\" + group_name:\r\n                i['selected'] = True \r\n\r\n \r\n    if error is not None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=error[0], reason=error[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=error[0] + \"<br />\" + error[1])\r\n\r\n    return render_template('case/machines.html',case_details=case[1] , all_machines=machines[1],SIDEBAR=SIDEBAR ,parsers_details =parsers_details[1] , groups_list=groups_list[1] , message = message , err_msg=err_msg)\r\n\r\n\r\n\r\n# ================================ get processing progress\r\n@app.route(\"/case/<case_id>/progress\" , methods=['POST'])\r\ndef all_machines_progress(case_id):\r\n    if request.method == 'POST':\r\n        ajax_str            =  urllib.unquote(request.data).decode('utf8')\r\n        machines_list       = json.loads(ajax_str)['machines_list']\r\n        machines_progress   = []\r\n        for machine in machines_list:\r\n            \r\n            machines        = db_files.get_parsing_progress(machine)\r\n            if machines[0]:\r\n\r\n                # calculate progress\r\n                machine_static = {\r\n                        'pending' : 0,\r\n                        'parsing' : 0,\r\n                        'done'    : 0,\r\n                        'queued'  : 0,\r\n                        'error'   : 0\r\n                }\r\n                for parser in machines[1].keys():\r\n                    for f in machines[1][parser]:\r\n                        machine_static[f['status']] += 1\r\n\r\n\r\n                machines_progress.append( {'machine': machine , 'progress' : machine_static } )\r\n            else:\r\n                logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the process progress for machine [\"+machine+\"]\", reason=machines[1])\r\n                return redirect(url_for('all_machines',case_id=case_id , err_msg=machine[1]))\r\n        return json.dumps(machines_progress)\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ machine files status \r\n# this will list all files and their parser and show their status (done, parsing, pending, etc)\r\n@app.route(\"/case/<case_id>/machine_files/<machine_id>\" , methods=['GET'])\r\ndef case_machine_files_status(case_id , machine_id):\r\n    # upload machine page\r\n    if request.method == 'GET':\r\n\r\n        case        = db_cases.get_case_by_id(case_id)\r\n        CASE_FIELDS = get_CASE_FIELDS()\r\n\r\n        # if there is no case exist\r\n        if case[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason=case[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />\" + case[1])\r\n        if case[0] == True and case[1] is None:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='case not found')\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />case not found\")\r\n\r\n        # get files\r\n        files = db_files.get_by_machine(machine_id)\r\n        if files[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting machine files\", reason=files[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting machine files<br />\" + files[1])\r\n        \r\n        return render_template('case/machine_file_status.html',case_details=case[1] ,SIDEBAR=SIDEBAR, machine_id=machine_id , db_files=files[1]['files'] )\r\n\r\n\r\n\r\n\r\n# ================================ disable/enable file processing\r\n# this function disable/enable a selected file on machine for specific parser, so it will exclude it from parsing\r\n# for example disable windows events security file from being parsing\r\n@app.route(\"/case/<case_id>/disable_enable_selected_files/<machine_id>\" , methods=['POST'])\r\ndef case_disable_enable_selected_files(case_id , machine_id):\r\n    # upload machine page\r\n    if request.method == 'POST':\r\n        \r\n        ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n        ajax_data = json.loads(ajax_str)['data']\r\n        case = db_cases.get_case_by_id(case_id)\r\n        # if there is no case exist\r\n        if case[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]\", reason=case[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]<br />\" + case[1])\r\n        elif case[0] == True and case[1] is None:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]\", reason='case not found')\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]<br />case not found\")\r\n\r\n\r\n        disable = True\r\n        if ajax_data['action'] == 'enable':\r\n            disable = False\r\n        up = db_files.disable_enable_file(machine_id , ajax_data['path'] , ajax_data['parser'] , disable)\r\n        \r\n        if up[0] == True:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: File [\"+ajax_data['path']+\"] \"+ ajax_data['action'])\r\n            ajax_data['result'] = True \r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed \"+ ajax_data['action']+ \" file [\"+ajax_data['path']+\"]\", reason=up[1])\r\n            ajax_data['result'] = False\r\n            \r\n        return jsonify(ajax_data)\r\n\r\n\r\n# ================================ Upload machines\r\n# upload machine page to upload files to the artifacts file upload,\r\n# this is the page allow to upload multiple zip file, each considered as machine\r\n@app.route(\"/case/<case_id>/upload_machine\" , methods=['POST' , 'GET'])\r\ndef case_upload_machine(case_id):\r\n    # upload machine page\r\n    if request.method == 'GET':\r\n\r\n        CASE_FIELDS = get_CASE_FIELDS()\r\n        if CASE_FIELDS[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n            \r\n        case = db_cases.get_case_by_id(case_id)\r\n        if case[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case [\"+case_id+\"] information\", reason=case[1])\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />\" + case[1])\r\n        elif case[0] == True and case[1] is None:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case [\"+case_id+\"] information\", reason='Index not found')\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case [\"+case_id+\"] information<br />Index not found\")\r\n        \r\n\r\n        return render_template('case/upload_machines.html',case_details=case[1] ,SIDEBAR=SIDEBAR )\r\n\r\n    # upload the machine files ajax\r\n    elif request.method == 'POST':\r\n        # get file\r\n        file = request.files['files[]']\r\n        # if there is a file to upload\r\n        if file:\r\n            base64_name = None if 'base64_name' not in request.form else request.form['base64_name'] \r\n            # start handling uploading the file\r\n            uf = upload_file(file , case_id , base64_name=base64_name)\r\n            return json.dumps(uf[1])\r\n\r\n        return json.dumps({'result' : False , 'filename' : '' , 'message' : 'There is no file selected'})\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n        \r\n\r\n# ================================ Upload Artifacts\r\n# upload artifacts for specific machine\r\n@app.route('/case/<main_case_id>/uploadartifacts/<machine_case_id>', methods=['POST'])\r\ndef main_upload_artifacts(main_case_id,machine_case_id):\r\n\r\n    if request.method == 'POST':\r\n        try:\r\n               \r\n            # get file\r\n            file = request.files['files[]']\r\n\r\n\r\n            # if there is a file to upload\r\n            if file:\r\n                base64_name = None if 'base64_name' not in request.form else request.form['base64_name']       \r\n\r\n                # start handling uploading the file\r\n                uf = upload_file(file , main_case_id , machine_case_id , base64_name=base64_name)\r\n                return json.dumps(uf[1])\r\n\r\n            return json.dumps({'result' : False , 'filename' : '' , 'message' : 'There is no file selected'})\r\n\r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+main_case_id+\"]: Failed upload artifacts\" , reason=str(e))\r\n            return jsonify({'result':'error' , 'message':str(e)})\r\n            \r\n    else:\r\n        return redirect(url_for('home_page'))\r\n        \r\n\r\n\r\n# ================================ add machine\r\n# add a machine to the case\r\n@app.route(\"/case/<case_id>/add_machine/\" , methods=['POST','GET'])\r\ndef add_machine(case_id):\r\n    if request.method == 'POST':\r\n        machine_details = {\r\n            'machinename'   :request.form['machinename'].replace(\"/\" , \"_\"),\r\n            'main_case'     :case_id,\r\n            'ip'            :request.form['ip'],\r\n\r\n        }\r\n\r\n        machine = db_cases.add_machine(machine_details)\r\n        if machine[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Machine [\"+machine_details['machinename']+\"] created\")\r\n            return redirect(url_for('all_machines',case_id=case_id , message= \"Machine [\" + machine[1].lstrip(case_id + \"_\") + \"] created\"))\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating machine [\"+machine_details['machinename']+\"]\", reason=machine[1])\r\n            return redirect(url_for('all_machines',case_id=case_id , err_msg=machine[1]))\r\n\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ delete machine\r\n# delete machine from case\r\n@app.route(\"/case/<case_id>/delete_machine/<machines_list>\" , methods=['GET'])\r\ndef delete_machine(case_id , machines_list):\r\n    if request.method == 'GET':\r\n\r\n        logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Delete machines\" , reason=machines_list)\r\n        # delete machine from mongo db\r\n        for machine in machines_list.split(','):\r\n            db_machine = db_cases.delete_machine(machine)\r\n            if db_machine[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting machine [\"+machine+\"]\" , reason=db_machine[1])\r\n                continue\r\n\r\n            # delete machines records from elasticsearch\r\n            q = {\"query\": {\r\n                    \"query_string\": {\r\n                        \"query\": \"(machine.keyword:\\\"\"+machine.replace('-' , '\\\\-')+\"\\\")\"\r\n                    }\r\n                }\r\n            } \r\n            es_machine = db_es.del_record_by_query(case_id , q)\r\n            if es_machine:\r\n                logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Machine [\"+machine+\"] deleted\")\r\n\r\n            # delete all files\r\n            try:\r\n                files_folder    = app.config[\"UPLOADED_FILES_DEST\"]     + \"/\" + case_id + \"/\" + machine + \"/\"\r\n                raw_folder      = app.config[\"UPLOADED_FILES_DEST_RAW\"] + \"/\" + case_id + \"/\" + machine + \"/\"\r\n                shutil.rmtree(files_folder, ignore_errors=True)\r\n                shutil.rmtree(raw_folder, ignore_errors=True)\r\n                logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: deleted machine folders [\"+machine+\"]\" , reason=files_folder + \",\" + raw_folder)\r\n            except Exception as e:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting machine folders [\"+machine+\"]\" , reason=str(e))\r\n\r\n        return redirect(url_for('all_machines',case_id=case_id))\r\n\r\n\r\n\r\n\r\n# ================================ process artifacts\r\n# run the selected parser for the machines specified\r\n@app.route('/case/<main_case_id>/processartifacts/<machine_case_id>/<parser_name>', methods=['GET'])\r\ndef main_process_artifacts(main_case_id,machine_case_id,parser_name):\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+main_case_id+\"]: Start processing machine [\"+machine_case_id+\"], parsers [\"+parser_name+\"]\")\r\n\r\n    parsers = parser_name.split(',')\r\n    task = parser_management.run_parsers.apply_async((main_case_id,machine_case_id,parsers) , link_error=parser_management.task_error_handler.s())\r\n    #task = parser_management.run_parsers.apply_async((main_case_id,machine_case_id,parsers))\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+main_case_id+\"]: task ID [\"+task.id+\"]\")\r\n    return jsonify({'data': 'started processing'})\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# ================================ add group\r\n# add a group to the case\r\n@app.route(\"/case/<case_id>/add_group\" , methods=['POST','GET'])\r\ndef add_group(case_id):\r\n    \r\n    if request.method == 'POST':\r\n        group_name  = request.form['group_name']\r\n        group       = db_groups.add_group(case_id , group_name)\r\n\r\n        if group[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Group [\"+group_name+\"] created\")\r\n            return redirect(url_for('all_machines',case_id=case_id , message= \"Group [\"+group_name+\"] created\"))\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed creating group [\"+group_name+\"]\", reason=group[1])\r\n            return redirect(url_for('all_machines',case_id=case_id , err_msg=group[1]))\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n\r\n\r\n\r\n\r\n# ================================ assign to group \r\n# assign the selected machines to group\r\n@app.route(\"/case/<case_id>/assign_to_group/\" , methods=['POST'])\r\ndef assign_to_group(case_id):\r\n    \r\n    if request.method == 'POST':\r\n        ajax_str        =  urllib.unquote(request.data).decode('utf8')\r\n        machines_list   = json.loads(ajax_str)['machines_list']\r\n        group_name      = json.loads(ajax_str)['group_name']\r\n        res       = db_cases.assign_to_group(machines_list , group_name)\r\n\r\n        if res[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: assigned machines to group [\"+group_name+\"]\" , reason=\",\".join(machines_list))\r\n            return json.dumps({'result' : 'successful' , 'message' : 'assigned to groups'})\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed assigning  machines to group [\"+group_name+\"]\", reason=res[1])\r\n            return json.dumps({'result' : 'failed' , 'message' : res[1]})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n\r\n\r\n# ================================ deassign from group \r\n# deassign the selected machines from group\r\n@app.route(\"/case/<case_id>/deassign_from_group/\" , methods=['POST'])\r\ndef deassign_from_group(case_id):\r\n    \r\n    if request.method == 'POST':\r\n        ajax_str        =  urllib.unquote(request.data).decode('utf8')\r\n        machines_list   = json.loads(ajax_str)['machines_list']\r\n        group_name      = json.loads(ajax_str)['group_name']\r\n        res       = db_cases.deassign_from_group(machines_list , group_name)\r\n\r\n        if res[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: deassigned machines from group [\"+group_name+\"]\" , reason=\",\".join(machines_list))\r\n            return json.dumps({'result' : 'successful' , 'message' : 'deassigned from group'})\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deassigning  machines from group [\"+group_name+\"]\", reason=res[1])\r\n            return json.dumps({'result' : 'failed' , 'message' : res[1]})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n\r\n\r\n\r\n\r\n# ================================ deassign to group \r\n# deassign the selected machines from group\r\n@app.route(\"/case/<case_id>/delete_group/<group_name>\" , methods=['GET'])\r\ndef delete_group(case_id , group_name):\r\n    \r\n    if request.method == 'GET':\r\n        res       = db_groups.delete_group(case_id , group_name)\r\n\r\n        if res[0]:\r\n            logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Group [\"+group_name+\"] deleted\")\r\n            return json.dumps({'result' : 'successful' , 'message' : 'Group deleted'})\r\n        else:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting group [\"+group_name+\"]\", reason=res[1])\r\n            return json.dumps({'result' : 'failed' , 'message' : res[1]})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# =================== Artifacts =======================\r\n\r\n\r\n# ================================ get artifacts data types\r\n# get all the list of artifacts for this case\r\n@app.route('/case/<case_id>/browse_artifacts_list_ajax', methods=['POST'])\r\ndef browse_artifacts_list_ajax(case_id):\r\n\r\n    if request.method == \"POST\":\r\n        try:\r\n            ajax_str    =  urllib.unquote(request.data).decode('utf8')\r\n            ajax_data   = json.loads(ajax_str)['data']\r\n            body = {\r\n                \"size\":30,\r\n \r\n            }\r\n            body = {\r\n                \"query\": {\r\n                    \"query_string\" : {\r\n                        \"query\" : ajax_data['query'],\r\n                        \"default_field\" : \"catch_all\"\r\n                    }\r\n                } ,\r\n                \"size\":0\r\n            }\r\n            \r\n            body[\"aggs\"] = {\r\n                    \"data_type\": {\r\n                        \"terms\" : {\r\n                            \"field\" : \"data_type.keyword\",\r\n                            \"size\" : 500,\r\n                            \"order\": {\r\n                                \"_key\": \"asc\"\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Query artifacts list\", reason=json.dumps(body))\r\n            res = db_es.query( case_id, body )\r\n            if res[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed query artifacts list from dataabase\", reason=res[1])\r\n                return json.dumps({'res_total' : 0 , 'res_records' : [] , 'aggs' : []})\r\n\r\n            aggs_records = res[1][\"aggregations\"][\"data_type\"][\"buckets\"]\r\n            res_records = res[1]['hits']['hits']\r\n            res_total   = res[1]['hits']['total']['value']\r\n             \r\n            return json.dumps({\"res_total\" : res_total , \"res_records\" : res_records , 'aggs' : aggs_records})\r\n  \r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed query artifacts list from dataabase\", reason=str(e))\r\n            return json.dumps({'res_total' : 0 , 'res_records' : [] , 'aggs' : []})\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ get artifacts records ajax\r\n# retrive all artifacts records using ajax\r\n@app.route('/case/<case_id>/browse_artifacts_ajax', methods=[\"POST\"])\r\ndef case_browse_artifacts_ajax(case_id):\r\n    if request.method == \"POST\": \r\n        try:\r\n            records_per_page = 30\r\n            ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n            ajax_data = json.loads(ajax_str)['data']\r\n            body = {\r\n                \"from\": int(ajax_data[\"wanted_page\"]) * records_per_page,\r\n                \"size\":records_per_page,\r\n            }\r\n            if \"query\" in ajax_data.keys() and ajax_data['query'] != \"None\":\r\n                body[\"query\"] = {\r\n                    \"query_string\" : {\r\n                        \"query\" : ajax_data['query'],\r\n                        \"default_field\" : \"catch_all\"\r\n                    }\r\n                } \r\n\r\n            if \"sort_by\" in ajax_data.keys() and ajax_data['sort_by'] != \"None\":\r\n                order = \"asc\" if ajax_data['sort_by']['order'] == 0 else \"desc\"\r\n                body[\"sort\"] = {\r\n                    ajax_data['sort_by']['name'] : {\"order\" : order}\r\n                } \r\n               \r\n            if \"group_by\" in ajax_data.keys() and ajax_data['group_by'] != \"None\" and \"fields\" in ajax_data['group_by'].keys() and len(ajax_data['group_by']['fields']):\r\n                body['size'] = 0 # if the data came from aggs, then no need for the actual data\r\n                  \r\n                # specifiy whether it is multi fields or single field \r\n                isSingleField = len(ajax_data['group_by'][\"fields\"]) == 1\r\n                \r\n                # sort buckets\r\n                sort_by_order = \"asc\" if \"sort_count\" in ajax_data['group_by'].keys() and ajax_data['group_by'][\"sort_count\"] == \"asc\" else \"desc\"\r\n\r\n                # agg terms\r\n                aggs_dict = {\r\n                    \"size\"                      : 1000 ,    \r\n                    \"show_term_doc_count_error\" : True , \r\n                    \"order\"                     : {\"_count\" : sort_by_order}\r\n                }\r\n                if isSingleField:\r\n                    terms                   = ajax_data['group_by'][\"fields\"][0] + \".keyword\"\r\n                    aggs_type               = \"terms\"\r\n                    aggs_dict[\"field\"]      = terms\r\n                    aggs_dict[\"missing\"]    = \"\"\r\n                else: \r\n                    terms                   = []\r\n                    for t in ajax_data['group_by'][\"fields\"]:\r\n                        terms.append( { \"field\" : t + \".keyword\" , \"missing\" : \"\"} )     \r\n                    aggs_type               = \"multi_terms\" \r\n                    aggs_dict[\"terms\"]      = terms\r\n                  \r\n                body[\"aggs\"] = {     \r\n                    \"group_by\" : {\r\n                        aggs_type : aggs_dict,  \r\n                        \"aggs\" : {\r\n                            \"my_buckets\" : { \r\n                                \"bucket_sort\" : {   \r\n                                    \"from\" : int(ajax_data[\"wanted_page\"]) * records_per_page,\r\n                                    \"size\" : records_per_page,\r\n                                }, \r\n                            }, \r\n                            \"group_by_top_hits\": {\r\n                                \"top_hits\" : {\r\n                                    \"size\" : ajax_data['group_by']['size'],     \r\n                                },       \r\n                            }, \r\n                        },  \r\n                    },      \r\n                    \"get_total\": {\r\n                        aggs_type : aggs_dict\r\n                    },   \r\n                    \"get_total_stats\": {\r\n                        \"stats_bucket\": {\r\n                            \"buckets_path\": \"get_total._count\" \r\n                        } \r\n                    }\r\n                } \r\n            logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Query artifacts\", reason=json.dumps(body))\r\n            res = db_es.query( case_id, body )\r\n            if res[0] == False:\r\n                print res[1]   \r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed query artifacts from dataabase\", reason=res[1])\r\n                return json.dumps({'res_total' : 0 , 'res_records' : [] , 'aggs' : []})\r\n\r\n            res_records = res[1]['hits']['hits']\r\n            res_total   = res[1]['hits']['total']['value']\r\n            aggs_records= res[1][\"aggregations\"][\"group_by\"][\"buckets\"] if \"aggregations\" in res[1].keys() else []\r\n    \r\n            # this function get the record and retrive the machine name from the database and enrich it\r\n            def get_machine_by_id(record):\r\n                if \"machine\" in record['_source'].keys():\r\n                    machine = db_cases.get_machine_by_id(record['_source']['machine'])\r\n                    if machine[0] == True and machine[1] is not None:\r\n                        record['_source']['machinename'] = machine[1]['machinename']\r\n                    else:\r\n                        record['_source']['machinename'] = record['_source']['machine']\r\n                        logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the machine name\", reason=machine[1])\r\n\r\n\r\n            for i in range( 0  , len(res_records) ):    \r\n                get_machine_by_id(res_records[i])\r\n            \r\n            if len(aggs_records):   \r\n                res_total = res[1][\"aggregations\"][\"get_total_stats\"]['count']\r\n                \"\"\"\r\n                res_records = []\r\n                for i in range(0 , len(aggs_records)):\r\n                    res_records.append( aggs_records[i][\"key\"] )\r\n                \"\"\"    \r\n                res_records = []\r\n                 \r\n                for i in range(0 , len(aggs_records)):\r\n                    for r in range(0 , len(aggs_records[i]['group_by_top_hits']['hits']['hits'])):\r\n                        get_machine_by_id(aggs_records[i]['group_by_top_hits']['hits']['hits'][r])\r\n                    \r\n                    rec = aggs_records[i]['group_by_top_hits']['hits']['hits'][0]\r\n                    rec['group_by'] = {\r\n                        'key'       : aggs_records[i]['key'],\r\n                        'doc_count' : aggs_records[i]['doc_count']\r\n                    } \r\n \r\n                    # if used a single field, then include it on a list to match the multi-fields group by\r\n                    if isSingleField:\r\n                        rec['group_by']['key'] = [rec['group_by']['key']]\r\n                    res_records.append( rec )\r\n                \r\n\r\n\r\n            ajax_res = {\"res_total\" : res_total , \"res_records\" : res_records , 'aggs' : ajax_data['group_by'][\"fields\"]}\r\n\r\n            return json.dumps(ajax_res)\r\n   \r\n \r\n        except Exception as e:\r\n            print str(e)\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the browser artifacts\", reason=str(e))\r\n            return json.dumps({\"res_total\" : 0 , \"res_records\" : [], 'aggs' : None})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n# ================================ get artifacts data types\r\n# get all artifacts for case\r\n@app.route('/case/<case_id>/browse_artifacts', methods=['GET'])\r\ndef case_browse_artifacts(case_id):  \r\n   \r\n    CASE_FIELDS = get_CASE_FIELDS()\r\n    if CASE_FIELDS[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n            \r\n\r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\" , reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information, <br />\"+case[1])\r\n    \r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n    \r\n    # get all fields from elasticsearch\r\n    # used for advanced search to list all fields when searching\r\n    fields_mapping = db_es.get_mapping_fields(case_id)\r\n    if fields_mapping[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the mapping fields \" , reason=fields_mapping[1])\r\n        return render_template('case/error_page.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting the mapping fields for case [\"+case_id+\"], <br />\"+fields_mapping[1])\r\n        \r\n    \r\n    query = {\r\n            \"AND\" : []\r\n            }\r\n\r\n    if 'q' in request.args:\r\n        try:\r\n            query = json.loads(urllib.unquote(request.args['q']).decode('utf-8'))\r\n        except Exception as e:\r\n            pass\r\n   \r\n    if 'machine' in request.args:\r\n        query[\"AND\"].append({'==machine' : request.args['machine']})\r\n\r\n    if 'rule' in request.args:\r\n        query['AND'].append({'==rule' : request.args['rule']})\r\n\r\n \r\n\r\n    group = None \r\n    if 'group' in request.args:\r\n        try:\r\n            machines        = db_cases.get_machines(case_id , request.args['group'])\r\n            if machines[0] == False:    \r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case machines\" , reason=machines[1])\r\n            else:  \r\n                group = {\r\n                    'group' : request.args['group'],\r\n                    'machines' : []\r\n                }\r\n                for m in machines[1]:  \r\n                    group['machines'].append(case_id + \"_\" + m['machinename'])\r\n                      \r\n                    \r\n        except Exception as e:  \r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case machines\" , reason=machines[1])\r\n\r\n  \r\n    # get all rules\r\n    all_rules = db_rules.get_rules()\r\n    if all_rules[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the rules information\" , reason=all_rules[1])\r\n        return render_template('case/error_page.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting the rules information, <br />\" + all_rules[1])\r\n \r\n    q = json.dumps(query)   \r\n    return render_template('case/browse_artifacts.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , search_query = q , fields_mapping=fields_mapping[1] , rules = all_rules[1] , group=group)\r\n\r\n\r\n\r\n  \r\n@app.route('/case/<case_id>/browse_artifacts_export' , methods=['POST'])\r\ndef browse_artifacts_export(case_id):\r\n    if request.method == \"POST\":\r\n        try:\r\n            \r\n            request_str =  urllib.unquote(request.data).decode('utf8')\r\n            request_json = json.loads(request_str)['data']\r\n                \r\n            logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Browse artifacts export\", reason=json.dumps(request_json))\r\n\r\n            # == from - to\r\n            body = {}\r\n            # == query   \r\n            query = '*' \r\n            if 'query' in request_json.keys() and request_json['query'] != None:\r\n                request_json['query'] = request_json['query'].strip()\r\n                query = '*' if request_json['query'] == \"\" or request_json['query'] is None else request_json['query']\r\n                body[\"query\"] = {\r\n                    \"query_string\" : {\r\n                        \"query\" : '!(data_type:\\\"tag\\\") AND ' + query,\r\n                        \"default_field\" : \"catch_all\"\r\n                    }\r\n                }\r\n            # == sort  \r\n            if 'sort_by' in request_json.keys() and request_json['sort_by'] != None:\r\n                order = \"asc\" if request_json['sort_by']['order'] == 0 else \"desc\"\r\n                body[\"sort\"] = {request_json['sort_by']['name'] : {\"order\" : order}}\r\n            else:\r\n                body[\"sort\"] = {'Data.@timestamp' : {\"order\" : 'asc'}}\r\n            \r\n\r\n            # == fields\r\n            fields = None\r\n            if 'fields' in request_json.keys() and request_json['fields'] != None :\r\n                fields = request_json['fields']\r\n                body['_source'] = {}\r\n                body['_source']['includes'] = fields \r\n  \r\n   \r\n            logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Export Query artifacts\", reason=json.dumps(body))\r\n\r\n            from flask import Response  \r\n\r\n            # get total number of records\r\n            body['size'] = 0\r\n            res = db_es.query( case_id, body) \r\n\r\n            if res[0] == False: \r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed export query artifacts from dataabase\", reason=res[1])\r\n                return json.dumps({\"res_total\" : 0 , \"res_records\" : []})\r\n            total_records = res[1]['hits']['total']['value']\r\n\r\n            return Response( \r\n                        export_stream_es(case_id=case_id, body=body , chunk_size=30 , fields=fields),\r\n                        mimetype='text/csv',\r\n                        headers= {\"total\" : str(total_records)}\r\n                    )\r\n                      \r\n   \r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed exporting the artifacts\", reason=str(e))\r\n            return json.dumps({\"res_total\" : 0 , \"res_records\" : []})\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n\r\n# =================== Timeline =======================\r\n\r\n# ================================ timeline page\r\n# get the time line page\r\n@app.route('/case/<case_id>/timeline', methods=['GET'])\r\ndef case_timeline(case_id):\r\n\r\n    \r\n    CASE_FIELDS = get_CASE_FIELDS()\r\n    if CASE_FIELDS[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n            \r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />\" + case[1])\r\n    \r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n    return render_template('case/timeline.html',case_details=case[1] ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1])\r\n\r\n\r\n# ================================ get tags ajax\r\n# get all tags for the case via ajax\r\n@app.route('/case/<case_id>/timeline_ajax', methods=['POST'])\r\ndef case_timeline_ajax(case_id ):\r\n    if request.method == \"POST\":\r\n        try:\r\n            ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n            ajax_data = json.loads(ajax_str)['data']\r\n\r\n\r\n            body = {\r\n                \"query\": {\r\n                    \"query_string\" : {\r\n                        \"query\" : 'data_type:tag'\r\n                    }\r\n                },\r\n                \"sort\":{\r\n                    \"Data.@timestamp\" : {\"order\" : \"asc\"}\r\n                },\r\n                \"size\": 2000\r\n            }\r\n            res = db_es.query( case_id, body )\r\n\r\n            if res[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed retriving timeline from database\", reason=res[1])\r\n                return json.dumps( {'result': 'failed' , 'tags' : res[1]} )\r\n\r\n\r\n            total_tags  = res[1]['hits']['total']['value']\r\n            tags        =  res[1]['hits']['hits']\r\n\r\n\r\n            for t in range(0 , len(tags)):\r\n                if 'record_id' in tags[t]['_source']['Data'].keys():\r\n                    record_id = tags[t]['_source']['Data']['record_id']\r\n                    rec = db_es.get_record_by_id(case_id , record_id)\r\n                    if rec[0] == False:\r\n                        logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting record details in timeline\", reason=rec[1])\r\n\r\n                    if rec[0] == True and rec[1] != False:\r\n                        tags[t]['_source']['Data']['record_details'] = rec[1]\r\n\r\n\r\n            return json.dumps({ 'result' : 'successful' , \"tags\" : tags})\r\n\r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed retriving timeline\", reason=str(e))\r\n            return json.dumps( {'result': 'failed' , 'tags' : \"Failed retriving timeline\" + str(e)} )\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ delete tag ajax\r\n# delete a specific tag by its ID\r\n@app.route('/case/<case_id>/timeline_delete_tag_ajax', methods=['POST'])\r\ndef case_timeline_delete_tag(case_id ):\r\n    if request.method == \"POST\":\r\n        ajax_str    =  urllib.unquote(request.data).decode('utf8')\r\n        ajax_data   = json.loads(ajax_str)['data']\r\n        logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Delete tag\", reason=json.dumps(ajax_data))\r\n\r\n        tag_id      =  ajax_data['tag_id']\r\n        record_id   = None if 'record_id' not in ajax_data.keys() else ajax_data['record_id']\r\n\r\n        # delete the tag record\r\n        delete = db_es.del_record_by_id( case_id = case_id , record_id = tag_id)\r\n        if delete[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting tag\", reason=delete[1])\r\n            return json.dumps({'result' : 'failed' , 'data': 'Failed:' + delete[1]})\r\n        \r\n        # if the tag associated with artifact record\r\n        if record_id is not None:\r\n            # delete the tag record from record\r\n            update_field = db_es.update_field( {\"script\": \"ctx._source.remove(\\\"tag_id\\\");ctx._source.remove(\\\"tag_type\\\")\"}  , record_id , case_id)\r\n            if update_field[1] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed adding tag_id to artifact record\", reason=update_field[1])\r\n                return json.dumps({'result' : 'failed' , 'data': 'Failed deleting tag_id from artifact record: ' + update_field[1]})\r\n\r\n\r\n\r\n        logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: tag [\"+tag_id+\"] deleted\")\r\n        return json.dumps({'result' : 'successful'})\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n# ================================ add tag ajax\r\n# add tag to a specifc record\r\n@app.route('/case/<case_id>/add_tag_ajax', methods=[\"POST\"])\r\ndef case_add_tag_ajax(case_id):\r\n    if request.method == \"POST\":\r\n        ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n        record_id = None\r\n        ajax_data = json.loads(ajax_str)['data']\r\n\r\n        Data = {\r\n            \"tag\"           : ajax_data['tag'] ,\r\n            \"@timestamp\"    : ajax_data['time'],\r\n            'tag_type'      : ajax_data['tag_type']\r\n        }\r\n\r\n        if 'doc_id' in ajax_data.keys():\r\n            Data['record_id']   = ajax_data['doc_id']\r\n            record_id           = ajax_data['doc_id']\r\n        if 'message' in ajax_data.keys():\r\n            Data['message']     = ajax_data['message']\r\n\r\n        logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Add tag\", reason=json.dumps(Data))\r\n\r\n        # add new record tag\r\n        record = {\r\n            \"Data\"          :Data, \r\n            \"data_source\"   :None, \r\n            \"data_type\"     :'tag', \r\n        }\r\n        up = db_es.bulk_queue_push( [record] , case_id , chunk_size=500) \r\n        #db_es.es_add_tag(data = { \"Data\" : Data  , \"data_type\" : 'tag' } , case_id = case_id )\r\n        if up[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed adding tag\", reason=up[1])\r\n            return json.dumps({'result' : 'failed' , 'data': 'Failed adding tag: ' + up[1]})\r\n\r\n        # update the tag_id to artifact record\r\n        else:\r\n            \r\n            if record_id is not None and len(up[3]) != 0:\r\n \r\n                # for each successful tag added\r\n                for tag_id in up[3]: \r\n                    update_field = db_es.update_field( {'doc': {'tag_id' : tag_id , 'tag_type' : record['Data']['tag_type'] }}  , record_id , case_id)\r\n                    \r\n                    if update_field[0] == False:\r\n                        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed adding tag_id to artifact record\", reason=update_field[1])\r\n                        return json.dumps({'result' : 'failed' , 'data': 'Failed adding tag_id to artifact record: ' + update_field[1]})\r\n\r\n                logger.logger(level=logger.INFO , type=\"case\", message=\"Case[\"+case_id+\"]: Tag created\")\r\n                return json.dumps({\"result\" : 'successful' , 'tag_id' : up[3][0] , 'tag_type' : record['Data']['tag_type']})\r\n            return json.dumps({\"result\" : 'successful' , 'tag_id' : up[3][0] , 'tag_type' : record['Data']['tag_type']})\r\n\r\n \r\n\r\n# ================================ generate timeline\r\n# get all tags for the case via ajax\r\n@app.route('/case/<case_id>/timeline_build_ajax', methods=['GET'])\r\ndef case_timeline_build_ajax(case_id):\r\n    if request.method == \"GET\":\r\n        try:\r\n            # create case timeline folder if not exists  \r\n            dest_timeline_folder = os.path.join( app.config['TIMELINE_FOLDER'] , case_id)\r\n            if not os.path.isdir(dest_timeline_folder):\r\n                create_folders(dest_timeline_folder)\r\n\r\n            # get if there is a previous version already exists for the case\r\n            src_filename        = 'timeline.xlsx'\r\n            dest_filename       = src_filename.rstrip(\".xlsx\") + \"_v0.xlsx\"\r\n\r\n            # get the latest timetime version\r\n            latest_version = 0\r\n            for previous_file in os.listdir(dest_timeline_folder): \r\n                try:\r\n                    if os.path.isfile(os.path.join(dest_timeline_folder, previous_file)) and previous_file.endswith(\".xlsx\"):    \r\n                        latest_version      = int(previous_file.split(\"_v\")[1].rstrip(\".xlsx\")) if int(previous_file.split(\"_v\")[1].rstrip(\".xlsx\")) > latest_version else latest_version\r\n                except Exception as e:\r\n                    pass \r\n\r\n            \r\n\r\n            # create instance for timeline builder          \r\n            views_folder        = app.config['TIMELINE_VIEWS_FOLDER']   \r\n            new_version         = latest_version + 1\r\n            dest_filename       = src_filename.rstrip(\".xlsx\") + \"_v\"+str(new_version)+\".xlsx\"\r\n            src_filename        = src_filename.rstrip(\".xlsx\") + \"_v\"+str(latest_version)+\".xlsx\" if latest_version > 0 else src_filename\r\n            dest_timeline       = os.path.join( dest_timeline_folder , dest_filename )\r\n            src_timeline        = os.path.join(dest_timeline_folder , src_filename) if latest_version > 0 else os.path.join( app.config['Timeline_Templates'] , src_filename)\r\n            t                   = buildTimeline.BuildTimeline(views_folder=views_folder , fname= src_timeline)\r\n            export_date         = str(datetime.now())\r\n\r\n\r\n            # get all yaml views from timeline views\r\n            all_rules           = t.get_views(views_folder)\r\n            requests            = []\r\n            sheet_timeline      =\"Timeline\"  \r\n            sheet_kuiper_id_col =\"KuiperID\"\r\n\r\n\r\n            all_active_rules    = []\r\n            default_rule        = None \r\n\r\n            for rule in range( 0 , len(all_rules)):\r\n                if 'default' in all_rules[rule]['condition'].keys() and all_rules[rule]['condition']['default'] == True:\r\n                    default_rule = all_rules[rule]\r\n                    continue\r\n                if 'active' in all_rules[rule]['condition'].keys() and all_rules[rule]['condition']['active'] == False:\r\n                    continue\r\n                all_active_rules.append(all_rules[rule])  \r\n\r\n\r\n            for rule in range( 0 , len(all_active_rules)):\r\n                requests.append({\r\n                        \"query\":{\r\n                            \"query_string\":{\r\n                                \"query\" : \"tag_id:* AND (\" + all_active_rules[rule][\"condition\"][\"query\"] + \")\",\r\n                                \"default_field\": \"catch_all\"\r\n                            }\r\n                        },\r\n                        \"size\": 2000\r\n                })\r\n            \r\n            requests.append({\r\n                \"query\":{\r\n                    \"query_string\":{\r\n                        \"query\" : \"tag_id:*\",\r\n                        \"default_field\": \"catch_all\"\r\n                    }\r\n                },\r\n                \"size\": 2000 \r\n            })\r\n            if len(requests):\r\n                res = db_es.multiqueries(case_id, requests)\r\n            else:\r\n                res = [True, []] # if there is no rules\r\n\r\n            if res[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting timeline generator search\", reason=res[1])\r\n                return json.dumps({ 'result' : 'failed' , \"message\" : res[1]})\r\n\r\n            # this list contains the ID of already added records to the sheet, so in the default records will not be added\r\n            added_records           = []\r\n            old_records             = t.get_values_by_column(sheet_timeline , sheet_kuiper_id_col)\r\n            to_be_removed_records   = []\r\n\r\n\r\n\r\n            for r in range(0 , len(res[1])):\r\n                if r < len(res[1])-1:\r\n                    \r\n                    # if the results belongs to a search query\r\n                    for data in res[1][r]['hits']['hits']: \r\n                        if data['_id'] in old_records:\r\n                            added_records.append(data['_id'])\r\n                        if data['_id'] in added_records: continue \r\n                        # add extra data related to the record itself\r\n                        data['_source']['_id']              = data['_id']\r\n                        data['_source']['_Export_Version']  = \"V_\" + str(new_version)\r\n                        data['_source']['_Export_Date']     = export_date \r\n\r\n \r\n                        fields_data = t.merge_data_and_fields(fields = all_active_rules[r]['fields'].copy(), data= data['_source'])\r\n                        add_res = t.add_data_to_sheet(sheet_timeline, fields_data)\r\n                        if add_res[0]:\r\n                            added_records.append(data['_id']) \r\n                        else:    \r\n                            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed adding the record [\"+data['_id']+\"] to xlsx timeline\", reason=add_res[1])\r\n                elif default_rule is not None:\r\n                    # if the results does not belongs to a search query \r\n                    for data in res[1][r]['hits']['hits']:\r\n                        if data['_id'] in old_records:\r\n                            added_records.append(data['_id'])\r\n                        if data['_id'] in added_records: continue\r\n                        \r\n                        # add extra data related to the record itself\r\n                        data['_source']['_id'] = data['_id'] \r\n                        data['_source']['_Export_Version']  = \"V_\" + str(new_version)\r\n                        data['_source']['_Export_Date']     = export_date\r\n                        \r\n                        fields_data = t.merge_data_and_fields(fields = default_rule['fields'].copy(), data= data['_source'])\r\n                        t.add_data_to_sheet(sheet_timeline, fields_data) \r\n            \r\n            # if there is a record on the old version of timeline has been untagged, remove it from the timeline new version\r\n            for r in old_records:\r\n                if r not in added_records:\r\n                    to_be_removed_records.append(r)\r\n            \r\n            for record in to_be_removed_records:\r\n                if not t.delete_row_from_sheet_by_kuiperID(sheet_timeline , record):\r\n                    logger.logger(level=logger.WARNING , type=\"case\", message=\"Case[\"+case_id+\"]: Failed deleting the record [\"+record+\"]\", reason=\"\")\r\n                    \r\n  \r\n            t.save(dest_timeline)\r\n             \r\n            return send_file(dest_timeline, as_attachment=True) \r\n\r\n        except Exception as e: \r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed retriving timeline\", reason=str(e))\r\n            return json.dumps( {'result': 'failed' , 'message' : \"Failed retriving timeline: \" + str(e)} )\r\n\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n\r\n \r\n\r\n# =================== Alerts =======================\r\n\r\n\r\n# ================================ get all alerts for case\r\n@app.route('/case/<case_id>/alerts/', defaults={'machinename': None, 'allMachines': False}, methods=['GET'])\r\n@app.route('/case/<case_id>/alerts/all/', defaults={'machinename': None, 'allMachines': True}, methods=['GET'])\r\n@app.route('/case/<case_id>/alerts/<machinename>', defaults={'allMachines': False}, methods=['GET'])\r\ndef case_alerts(case_id, machinename, allMachines):\r\n\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Open alerts page\")\r\n\r\n\r\n    CASE_FIELDS = get_CASE_FIELDS()\r\n    if CASE_FIELDS[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n            \r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting case information<br />\" + case[1])\r\n    \r\n\r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n    machines = db_cases.get_machines(case_id)\r\n\r\n    if machinename is not None:\r\n        # validate machine exists\r\n        machine_id = case_id + \"_\" + machinename\r\n        machine_info = db_cases.get_machine_by_id(machine_id)\r\n        if machine_info[0] is False or machine_info[1] is None: # machine_info[0] in case of exception False. machine_info[1] is machine or None (not found) or exception-string\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed checking if the machine [\"+machinename+\"] exists\", reason=\"\")\r\n            return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed checking if the machine exists<br />\" )\r\n        currentmachinename = machinename\r\n    elif allMachines is True:\r\n        currentmachinename = \"All\"\r\n        machine_id = None\r\n    else:\r\n        return render_template('case/alerts.html',case_details=case[1] ,SIDEBAR=SIDEBAR, all_rules=[], rhaegal_hits=[], machines=machines[1], currentmachinename=\"\", browse_alert_link_query=\"\", machine_id=\"\")\r\n\r\n    all_rules = db_rules.get_rules()\r\n    if all_rules[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting rules information\", reason=all_rules[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=all_rules[1])\r\n\r\n    requests = []\r\n    \r\n    for rule in all_rules[1]:\r\n        if allMachines is True:\r\n            qrule = rule['rule']\r\n        else: #elif machine_id is not None: for sure\r\n            qrule = \"machine:\" + machine_id + \" AND (\" + rule['rule'] +\")\"\r\n        requests.append({\r\n                \"query\":{\r\n                    \"query_string\":{\r\n                        \"query\" : qrule,\r\n                        \"default_field\": \"catch_all\"\r\n                    }\r\n                },\r\n                \"size\":0\r\n        })\r\n    \r\n    \r\n    if len(requests):\r\n        res = db_es.multiqueries(case_id, requests)\r\n    else:\r\n        res = [True, []] # if there is no rules\r\n\r\n    if res[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting total hits of rules from database\", reason=res[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=res[1])\r\n\r\n    #prepare browse-machine-alerts link\r\n    browse_alert_link_query = \"\"\r\n    if allMachines is False and machine_id is not None:\r\n        json_query = {\"AND\" : [{\"==machine\": machine_id}]}\r\n        browse_alert_link_query = json.dumps(json_query)\r\n\r\n    for r in range(0 , len(res[1])):\r\n        all_rules[1][r]['hits'] =  res[1][r]['hits']['total']['value']\r\n    \r\n    # build the query to get all rhaegal hits\r\n    \r\n    if allMachines is True:\r\n        qrhaegal = \"Data.rhaegal.name:*\"\r\n    else: #elif machine_id is not None: for sure anyway\r\n        qrhaegal = \"machine:\" + machine_id + \" AND (Data.rhaegal.name:*)\"\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: get Rhaegal hits\")\r\n    Rhaegal_query = {\r\n                \"query\":{\r\n                    \"query_string\":{\r\n                        \"query\" : qrhaegal,\r\n                        \"default_field\": \"catch_all\"\r\n                    }\r\n                },\r\n                \"size\":0,\r\n                \"aggs\" : {\r\n                        \"rhaegal\": {\r\n                            \"terms\" : {\r\n                                \"field\" : \"Data.rhaegal.score.keyword\",\r\n                                \"size\" : 100,\r\n                                \"order\": {\r\n                                    \"_key\": \"asc\"\r\n                                }\r\n                            },\r\n                            \"aggs\" :{\r\n                                \"names\" : {\r\n                                    \"terms\" : {\r\n                                        \"field\" : \"Data.rhaegal.name.keyword\",\r\n                                        \"size\" : 2000,\r\n                                    },\r\n                                    \"aggs\" : {\r\n                                        \"first_record\" : {\r\n                                            \"top_hits\" : {\r\n                                                \"size\" : 1\r\n                                            },\r\n                                        }\r\n                                    }\r\n                                }\r\n                            }\r\n                        }\r\n                    }\r\n        }\r\n\r\n    res = db_es.query(case_id , Rhaegal_query)\r\n    if res[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting the Rhaegal hits\", reason=res[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=res[1])\r\n\r\n\r\n    Rhaegal_hits = {\r\n        \"names\" : [] , \r\n        \"scores\" : []\r\n    }\r\n    for score in res[1][\"aggregations\"][\"rhaegal\"][\"buckets\"]:\r\n        Rhaegal_hits[\"scores\"].append({\r\n            \"score\" : score[\"key\"],\r\n            \"count\" : score[\"doc_count\"]\r\n        })\r\n        \r\n        for name in score[\"names\"][\"buckets\"]:\r\n            tmp_meta = name[\"first_record\"][\"hits\"][\"hits\"][0][\"_source\"][\"Data\"][\"rhaegal\"]\r\n            tmp_meta[\"count\"] = name[\"doc_count\"]\r\n            Rhaegal_hits[\"names\"].append(tmp_meta)\r\n\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: hits [\"+str(len(Rhaegal_hits[\"names\"]))+\"]\")\r\n\r\n    return render_template('case/alerts.html',case_details=case[1] ,SIDEBAR=SIDEBAR , all_rules= all_rules[1] , rhaegal_hits=Rhaegal_hits, machines=machines[1], currentmachinename=currentmachinename,browse_alert_link_query=browse_alert_link_query, machine_id=machine_id)\r\n\r\n\r\n\r\n# =================== Graph =======================\r\n\r\n\r\n# ================================ show the graph page\r\n@app.route('/case/<case_id>/graph/<record_id>')\r\ndef graph_display(case_id , record_id):\r\n    logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Open graph for record [\"+record_id+\"]\")\r\n\r\n    CASE_FIELDS = get_CASE_FIELDS()\r\n    if CASE_FIELDS[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting parsers important fields\", reason=CASE_FIELDS[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Failed getting parsers important fields<br />\" + CASE_FIELDS[1])\r\n       \r\n\r\n    case = db_cases.get_case_by_id(case_id)\r\n    if case[0] == False:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case details\", reason=case[1])\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=case[1])\r\n\r\n    if case[1] is None:\r\n        logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting case information\", reason='Index not found')\r\n        return render_template('case/error_page.html',case_details=case_id ,SIDEBAR=SIDEBAR , CASE_FIELDS=CASE_FIELDS[1] , message=\"Case[\"+case_id+\"]: Failed getting case information<br />Index not found\")\r\n\r\n\r\n\r\n    record = []\r\n    if record_id is not None:\r\n        query = {\r\n            \"query\": {\r\n                \"terms\": {\r\n                    \"_id\": [record_id]\r\n                }\r\n            }\r\n        }\r\n\r\n        record = db_es.query( case_id, query )\r\n        if record[0] == False:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting record information for graph\", reason=record[1])\r\n            return {'result' : False , 'data' : record[1]}\r\n        record = record[1]['hits']['hits']\r\n    \r\n    return render_template('case/graph.html',case_details=case[1] , SIDEBAR=SIDEBAR, init_records = record , page_header=\"Graph\")\r\n\r\n\r\n\r\n\r\n\r\n# ================================ expand the graph nodes (search)\r\n# retrive requested nodes to be added to the graph\r\n@app.route('/case/<case_id>/expand_graph', methods=[\"POST\"])\r\ndef graph_expand(case_id):\r\n    if request.method == \"POST\":\r\n\r\n\r\n        ajax_str =  urllib.unquote(request.data).decode('utf8')\r\n        ajax_data = json.loads(ajax_str)['data']\r\n        field = ajax_data['field']\r\n        value = ajax_data['value']\r\n        logger.logger(level=logger.DEBUG , type=\"case\", message=\"Case[\"+case_id+\"]: Expand search [\"+field+\":\"+value+\"]\")\r\n\r\n\r\n\r\n        special_chars = [ '\\\\' , '/' , ':' , '-' , '{' , '}' , '(', ')' , ' ' , '@' ]\r\n        for sc in special_chars:\r\n            value = value.replace(sc , '\\\\' + sc)\r\n\r\n        body = {\r\n            \"query\": {\r\n                \"query_string\":{\r\n                        \"query\" : \"*\" + str(value) + \"*\",\r\n                        \"default_field\" : \"catch_all\"\r\n                    }\r\n            },\r\n            \"size\": 500\r\n        }\r\n\r\n        try:\r\n            res = db_es.query( case_id, body )\r\n            if res[0] == False:\r\n                logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting record information for graph\", reason=res[1])\r\n                ajax_res = {'response' : 'error' , 'data' : str(res[1])}\r\n\r\n            res_total = res[1]['hits']['total']['value']\r\n            res_records = res[1]['hits']['hits']\r\n            ajax_res = {'response' : 'OK' , \"res_total\" : res_total , \"res_records\" : res_records}\r\n        except Exception as e:\r\n            logger.logger(level=logger.ERROR , type=\"case\", message=\"Case[\"+case_id+\"]: Failed getting record information for graph\", reason=str(e))\r\n            ajax_res = {'response' : 'error'}\r\n            \r\n\r\n\r\n        return json.dumps(ajax_res)\r\n    else:\r\n        return redirect(url_for('home_page'))\r\n"], "filenames": ["kuiper/app/controllers/case_management.py"], "buggy_code_start_loc": [124], "buggy_code_end_loc": [130], "fixing_code_start_loc": [125], "fixing_code_end_loc": [153], "type": "CWE-22", "message": "A vulnerability, which was classified as problematic, was found in DFIRKuiper Kuiper 2.3.4. This affects the function unzip_file of the file kuiper/app/controllers/case_management.py of the component TAR Archive Handler. The manipulation of the argument dst_path leads to path traversal. It is possible to initiate the attack remotely. The complexity of an attack is rather high. The exploitability is told to be difficult. Upgrading to version 2.3.5 is able to address this issue. The identifier of the patch is 94fa135153002f651f5526c55a7240e083db8d73. It is recommended to upgrade the affected component. The identifier VDB-248277 was assigned to this vulnerability.", "other": {"cve": {"id": "CVE-2023-6908", "sourceIdentifier": "cna@vuldb.com", "published": "2023-12-18T04:15:52.013", "lastModified": "2024-02-29T01:42:48.797", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "A vulnerability, which was classified as problematic, was found in DFIRKuiper Kuiper 2.3.4. This affects the function unzip_file of the file kuiper/app/controllers/case_management.py of the component TAR Archive Handler. The manipulation of the argument dst_path leads to path traversal. It is possible to initiate the attack remotely. The complexity of an attack is rather high. The exploitability is told to be difficult. Upgrading to version 2.3.5 is able to address this issue. The identifier of the patch is 94fa135153002f651f5526c55a7240e083db8d73. It is recommended to upgrade the affected component. The identifier VDB-248277 was assigned to this vulnerability."}, {"lang": "es", "value": "Una vulnerabilidad fue encontrada en DFIRKuiper Kuiper 2.3.4 y clasificada como problem\u00e1tica. Esto afecta a la funci\u00f3n unzip_file del archivo kuiper/app/controllers/case_management.py del componente TAR Archive Handler. La manipulaci\u00f3n del argumento dst_path conduce a path traversal. Es posible iniciar el ataque de forma remota. La complejidad de un ataque es bastante alta. Se dice que la explotabilidad es dif\u00edcil. La actualizaci\u00f3n a la versi\u00f3n 2.3.5 puede solucionar este problema. El identificador del parche es 94fa135153002f651f5526c55a7240e083db8d73. Se recomienda actualizar el componente afectado. A esta vulnerabilidad se le asign\u00f3 el identificador VDB-248277."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}, {"source": "cna@vuldb.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:N/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 3.1, "baseSeverity": "LOW"}, "exploitabilityScore": 1.6, "impactScore": 1.4}], "cvssMetricV2": [{"source": "cna@vuldb.com", "type": "Secondary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:H/Au:N/C:N/I:P/A:N", "accessVector": "NETWORK", "accessComplexity": "HIGH", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 2.6}, "baseSeverity": "LOW", "exploitabilityScore": 4.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "cna@vuldb.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-22"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:dfirkuiper:kuiper:2.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "840CD615-EE1A-4741-9688-A4A0DCFCEFE0"}]}]}], "references": [{"url": "https://github.com/DFIRKuiper/Kuiper/commit/94fa135153002f651f5526c55a7240e083db8d73", "source": "cna@vuldb.com", "tags": ["Patch"]}, {"url": "https://github.com/DFIRKuiper/Kuiper/pull/106", "source": "cna@vuldb.com", "tags": ["Patch"]}, {"url": "https://github.com/DFIRKuiper/Kuiper/releases/tag/v2.3.5", "source": "cna@vuldb.com", "tags": ["Release Notes"]}, {"url": "https://vuldb.com/?ctiid.248277", "source": "cna@vuldb.com", "tags": ["Permissions Required", "Third Party Advisory"]}, {"url": "https://vuldb.com/?id.248277", "source": "cna@vuldb.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/DFIRKuiper/Kuiper/commit/94fa135153002f651f5526c55a7240e083db8d73"}}