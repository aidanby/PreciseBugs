{"buggy_code": ["/*\n * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <net/sock.h>\n#include <linux/in.h>\n#include <linux/export.h>\n#include <linux/time.h>\n#include <linux/rds.h>\n\n#include \"rds.h\"\n\nvoid rds_inc_init(struct rds_incoming *inc, struct rds_connection *conn,\n\t\t struct in6_addr *saddr)\n{\n\trefcount_set(&inc->i_refcount, 1);\n\tINIT_LIST_HEAD(&inc->i_item);\n\tinc->i_conn = conn;\n\tinc->i_saddr = *saddr;\n\tinc->i_rdma_cookie = 0;\n\tinc->i_rx_tstamp = ktime_set(0, 0);\n\n\tmemset(inc->i_rx_lat_trace, 0, sizeof(inc->i_rx_lat_trace));\n}\nEXPORT_SYMBOL_GPL(rds_inc_init);\n\nvoid rds_inc_path_init(struct rds_incoming *inc, struct rds_conn_path *cp,\n\t\t       struct in6_addr  *saddr)\n{\n\trefcount_set(&inc->i_refcount, 1);\n\tINIT_LIST_HEAD(&inc->i_item);\n\tinc->i_conn = cp->cp_conn;\n\tinc->i_conn_path = cp;\n\tinc->i_saddr = *saddr;\n\tinc->i_rdma_cookie = 0;\n\tinc->i_rx_tstamp = ktime_set(0, 0);\n}\nEXPORT_SYMBOL_GPL(rds_inc_path_init);\n\nstatic void rds_inc_addref(struct rds_incoming *inc)\n{\n\trdsdebug(\"addref inc %p ref %d\\n\", inc, refcount_read(&inc->i_refcount));\n\trefcount_inc(&inc->i_refcount);\n}\n\nvoid rds_inc_put(struct rds_incoming *inc)\n{\n\trdsdebug(\"put inc %p ref %d\\n\", inc, refcount_read(&inc->i_refcount));\n\tif (refcount_dec_and_test(&inc->i_refcount)) {\n\t\tBUG_ON(!list_empty(&inc->i_item));\n\n\t\tinc->i_conn->c_trans->inc_free(inc);\n\t}\n}\nEXPORT_SYMBOL_GPL(rds_inc_put);\n\nstatic void rds_recv_rcvbuf_delta(struct rds_sock *rs, struct sock *sk,\n\t\t\t\t  struct rds_cong_map *map,\n\t\t\t\t  int delta, __be16 port)\n{\n\tint now_congested;\n\n\tif (delta == 0)\n\t\treturn;\n\n\trs->rs_rcv_bytes += delta;\n\tif (delta > 0)\n\t\trds_stats_add(s_recv_bytes_added_to_socket, delta);\n\telse\n\t\trds_stats_add(s_recv_bytes_removed_from_socket, -delta);\n\n\t/* loop transport doesn't send/recv congestion updates */\n\tif (rs->rs_transport->t_type == RDS_TRANS_LOOP)\n\t\treturn;\n\n\tnow_congested = rs->rs_rcv_bytes > rds_sk_rcvbuf(rs);\n\n\trdsdebug(\"rs %p (%pI6c:%u) recv bytes %d buf %d \"\n\t  \"now_cong %d delta %d\\n\",\n\t  rs, &rs->rs_bound_addr,\n\t  ntohs(rs->rs_bound_port), rs->rs_rcv_bytes,\n\t  rds_sk_rcvbuf(rs), now_congested, delta);\n\n\t/* wasn't -> am congested */\n\tif (!rs->rs_congested && now_congested) {\n\t\trs->rs_congested = 1;\n\t\trds_cong_set_bit(map, port);\n\t\trds_cong_queue_updates(map);\n\t}\n\t/* was -> aren't congested */\n\t/* Require more free space before reporting uncongested to prevent\n\t   bouncing cong/uncong state too often */\n\telse if (rs->rs_congested && (rs->rs_rcv_bytes < (rds_sk_rcvbuf(rs)/2))) {\n\t\trs->rs_congested = 0;\n\t\trds_cong_clear_bit(map, port);\n\t\trds_cong_queue_updates(map);\n\t}\n\n\t/* do nothing if no change in cong state */\n}\n\nstatic void rds_conn_peer_gen_update(struct rds_connection *conn,\n\t\t\t\t     u32 peer_gen_num)\n{\n\tint i;\n\tstruct rds_message *rm, *tmp;\n\tunsigned long flags;\n\n\tWARN_ON(conn->c_trans->t_type != RDS_TRANS_TCP);\n\tif (peer_gen_num != 0) {\n\t\tif (conn->c_peer_gen_num != 0 &&\n\t\t    peer_gen_num != conn->c_peer_gen_num) {\n\t\t\tfor (i = 0; i < RDS_MPATH_WORKERS; i++) {\n\t\t\t\tstruct rds_conn_path *cp;\n\n\t\t\t\tcp = &conn->c_path[i];\n\t\t\t\tspin_lock_irqsave(&cp->cp_lock, flags);\n\t\t\t\tcp->cp_next_tx_seq = 1;\n\t\t\t\tcp->cp_next_rx_seq = 0;\n\t\t\t\tlist_for_each_entry_safe(rm, tmp,\n\t\t\t\t\t\t\t &cp->cp_retrans,\n\t\t\t\t\t\t\t m_conn_item) {\n\t\t\t\t\tset_bit(RDS_MSG_FLUSH, &rm->m_flags);\n\t\t\t\t}\n\t\t\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\t\t\t}\n\t\t}\n\t\tconn->c_peer_gen_num = peer_gen_num;\n\t}\n}\n\n/*\n * Process all extension headers that come with this message.\n */\nstatic void rds_recv_incoming_exthdrs(struct rds_incoming *inc, struct rds_sock *rs)\n{\n\tstruct rds_header *hdr = &inc->i_hdr;\n\tunsigned int pos = 0, type, len;\n\tunion {\n\t\tstruct rds_ext_header_version version;\n\t\tstruct rds_ext_header_rdma rdma;\n\t\tstruct rds_ext_header_rdma_dest rdma_dest;\n\t} buffer;\n\n\twhile (1) {\n\t\tlen = sizeof(buffer);\n\t\ttype = rds_message_next_extension(hdr, &pos, &buffer, &len);\n\t\tif (type == RDS_EXTHDR_NONE)\n\t\t\tbreak;\n\t\t/* Process extension header here */\n\t\tswitch (type) {\n\t\tcase RDS_EXTHDR_RDMA:\n\t\t\trds_rdma_unuse(rs, be32_to_cpu(buffer.rdma.h_rdma_rkey), 0);\n\t\t\tbreak;\n\n\t\tcase RDS_EXTHDR_RDMA_DEST:\n\t\t\t/* We ignore the size for now. We could stash it\n\t\t\t * somewhere and use it for error checking. */\n\t\t\tinc->i_rdma_cookie = rds_rdma_make_cookie(\n\t\t\t\t\tbe32_to_cpu(buffer.rdma_dest.h_rdma_rkey),\n\t\t\t\t\tbe32_to_cpu(buffer.rdma_dest.h_rdma_offset));\n\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void rds_recv_hs_exthdrs(struct rds_header *hdr,\n\t\t\t\tstruct rds_connection *conn)\n{\n\tunsigned int pos = 0, type, len;\n\tunion {\n\t\tstruct rds_ext_header_version version;\n\t\tu16 rds_npaths;\n\t\tu32 rds_gen_num;\n\t} buffer;\n\tu32 new_peer_gen_num = 0;\n\n\twhile (1) {\n\t\tlen = sizeof(buffer);\n\t\ttype = rds_message_next_extension(hdr, &pos, &buffer, &len);\n\t\tif (type == RDS_EXTHDR_NONE)\n\t\t\tbreak;\n\t\t/* Process extension header here */\n\t\tswitch (type) {\n\t\tcase RDS_EXTHDR_NPATHS:\n\t\t\tconn->c_npaths = min_t(int, RDS_MPATH_WORKERS,\n\t\t\t\t\t       be16_to_cpu(buffer.rds_npaths));\n\t\t\tbreak;\n\t\tcase RDS_EXTHDR_GEN_NUM:\n\t\t\tnew_peer_gen_num = be32_to_cpu(buffer.rds_gen_num);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_warn_ratelimited(\"ignoring unknown exthdr type \"\n\t\t\t\t\t     \"0x%x\\n\", type);\n\t\t}\n\t}\n\t/* if RDS_EXTHDR_NPATHS was not found, default to a single-path */\n\tconn->c_npaths = max_t(int, conn->c_npaths, 1);\n\tconn->c_ping_triggered = 0;\n\trds_conn_peer_gen_update(conn, new_peer_gen_num);\n}\n\n/* rds_start_mprds() will synchronously start multiple paths when appropriate.\n * The scheme is based on the following rules:\n *\n * 1. rds_sendmsg on first connect attempt sends the probe ping, with the\n *    sender's npaths (s_npaths)\n * 2. rcvr of probe-ping knows the mprds_paths = min(s_npaths, r_npaths). It\n *    sends back a probe-pong with r_npaths. After that, if rcvr is the\n *    smaller ip addr, it starts rds_conn_path_connect_if_down on all\n *    mprds_paths.\n * 3. sender gets woken up, and can move to rds_conn_path_connect_if_down.\n *    If it is the smaller ipaddr, rds_conn_path_connect_if_down can be\n *    called after reception of the probe-pong on all mprds_paths.\n *    Otherwise (sender of probe-ping is not the smaller ip addr): just call\n *    rds_conn_path_connect_if_down on the hashed path. (see rule 4)\n * 4. rds_connect_worker must only trigger a connection if laddr < faddr.\n * 5. sender may end up queuing the packet on the cp. will get sent out later.\n *    when connection is completed.\n */\nstatic void rds_start_mprds(struct rds_connection *conn)\n{\n\tint i;\n\tstruct rds_conn_path *cp;\n\n\tif (conn->c_npaths > 1 &&\n\t    rds_addr_cmp(&conn->c_laddr, &conn->c_faddr) < 0) {\n\t\tfor (i = 0; i < conn->c_npaths; i++) {\n\t\t\tcp = &conn->c_path[i];\n\t\t\trds_conn_path_connect_if_down(cp);\n\t\t}\n\t}\n}\n\n/*\n * The transport must make sure that this is serialized against other\n * rx and conn reset on this specific conn.\n *\n * We currently assert that only one fragmented message will be sent\n * down a connection at a time.  This lets us reassemble in the conn\n * instead of per-flow which means that we don't have to go digging through\n * flows to tear down partial reassembly progress on conn failure and\n * we save flow lookup and locking for each frag arrival.  It does mean\n * that small messages will wait behind large ones.  Fragmenting at all\n * is only to reduce the memory consumption of pre-posted buffers.\n *\n * The caller passes in saddr and daddr instead of us getting it from the\n * conn.  This lets loopback, who only has one conn for both directions,\n * tell us which roles the addrs in the conn are playing for this message.\n */\nvoid rds_recv_incoming(struct rds_connection *conn, struct in6_addr *saddr,\n\t\t       struct in6_addr *daddr,\n\t\t       struct rds_incoming *inc, gfp_t gfp)\n{\n\tstruct rds_sock *rs = NULL;\n\tstruct sock *sk;\n\tunsigned long flags;\n\tstruct rds_conn_path *cp;\n\n\tinc->i_conn = conn;\n\tinc->i_rx_jiffies = jiffies;\n\tif (conn->c_trans->t_mp_capable)\n\t\tcp = inc->i_conn_path;\n\telse\n\t\tcp = &conn->c_path[0];\n\n\trdsdebug(\"conn %p next %llu inc %p seq %llu len %u sport %u dport %u \"\n\t\t \"flags 0x%x rx_jiffies %lu\\n\", conn,\n\t\t (unsigned long long)cp->cp_next_rx_seq,\n\t\t inc,\n\t\t (unsigned long long)be64_to_cpu(inc->i_hdr.h_sequence),\n\t\t be32_to_cpu(inc->i_hdr.h_len),\n\t\t be16_to_cpu(inc->i_hdr.h_sport),\n\t\t be16_to_cpu(inc->i_hdr.h_dport),\n\t\t inc->i_hdr.h_flags,\n\t\t inc->i_rx_jiffies);\n\n\t/*\n\t * Sequence numbers should only increase.  Messages get their\n\t * sequence number as they're queued in a sending conn.  They\n\t * can be dropped, though, if the sending socket is closed before\n\t * they hit the wire.  So sequence numbers can skip forward\n\t * under normal operation.  They can also drop back in the conn\n\t * failover case as previously sent messages are resent down the\n\t * new instance of a conn.  We drop those, otherwise we have\n\t * to assume that the next valid seq does not come after a\n\t * hole in the fragment stream.\n\t *\n\t * The headers don't give us a way to realize if fragments of\n\t * a message have been dropped.  We assume that frags that arrive\n\t * to a flow are part of the current message on the flow that is\n\t * being reassembled.  This means that senders can't drop messages\n\t * from the sending conn until all their frags are sent.\n\t *\n\t * XXX we could spend more on the wire to get more robust failure\n\t * detection, arguably worth it to avoid data corruption.\n\t */\n\tif (be64_to_cpu(inc->i_hdr.h_sequence) < cp->cp_next_rx_seq &&\n\t    (inc->i_hdr.h_flags & RDS_FLAG_RETRANSMITTED)) {\n\t\trds_stats_inc(s_recv_drop_old_seq);\n\t\tgoto out;\n\t}\n\tcp->cp_next_rx_seq = be64_to_cpu(inc->i_hdr.h_sequence) + 1;\n\n\tif (rds_sysctl_ping_enable && inc->i_hdr.h_dport == 0) {\n\t\tif (inc->i_hdr.h_sport == 0) {\n\t\t\trdsdebug(\"ignore ping with 0 sport from %pI6c\\n\",\n\t\t\t\t saddr);\n\t\t\tgoto out;\n\t\t}\n\t\trds_stats_inc(s_recv_ping);\n\t\trds_send_pong(cp, inc->i_hdr.h_sport);\n\t\t/* if this is a handshake ping, start multipath if necessary */\n\t\tif (RDS_HS_PROBE(be16_to_cpu(inc->i_hdr.h_sport),\n\t\t\t\t be16_to_cpu(inc->i_hdr.h_dport))) {\n\t\t\trds_recv_hs_exthdrs(&inc->i_hdr, cp->cp_conn);\n\t\t\trds_start_mprds(cp->cp_conn);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (be16_to_cpu(inc->i_hdr.h_dport) ==  RDS_FLAG_PROBE_PORT &&\n\t    inc->i_hdr.h_sport == 0) {\n\t\trds_recv_hs_exthdrs(&inc->i_hdr, cp->cp_conn);\n\t\t/* if this is a handshake pong, start multipath if necessary */\n\t\trds_start_mprds(cp->cp_conn);\n\t\twake_up(&cp->cp_conn->c_hs_waitq);\n\t\tgoto out;\n\t}\n\n\trs = rds_find_bound(daddr, inc->i_hdr.h_dport, conn->c_bound_if);\n\tif (!rs) {\n\t\trds_stats_inc(s_recv_drop_no_sock);\n\t\tgoto out;\n\t}\n\n\t/* Process extension headers */\n\trds_recv_incoming_exthdrs(inc, rs);\n\n\t/* We can be racing with rds_release() which marks the socket dead. */\n\tsk = rds_rs_to_sk(rs);\n\n\t/* serialize with rds_release -> sock_orphan */\n\twrite_lock_irqsave(&rs->rs_recv_lock, flags);\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\trdsdebug(\"adding inc %p to rs %p's recv queue\\n\", inc, rs);\n\t\trds_stats_inc(s_recv_queued);\n\t\trds_recv_rcvbuf_delta(rs, sk, inc->i_conn->c_lcong,\n\t\t\t\t      be32_to_cpu(inc->i_hdr.h_len),\n\t\t\t\t      inc->i_hdr.h_dport);\n\t\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t\tinc->i_rx_tstamp = ktime_get_real();\n\t\trds_inc_addref(inc);\n\t\tinc->i_rx_lat_trace[RDS_MSG_RX_END] = local_clock();\n\t\tlist_add_tail(&inc->i_item, &rs->rs_recv_queue);\n\t\t__rds_wake_sk_sleep(sk);\n\t} else {\n\t\trds_stats_inc(s_recv_drop_dead_sock);\n\t}\n\twrite_unlock_irqrestore(&rs->rs_recv_lock, flags);\n\nout:\n\tif (rs)\n\t\trds_sock_put(rs);\n}\nEXPORT_SYMBOL_GPL(rds_recv_incoming);\n\n/*\n * be very careful here.  This is being called as the condition in\n * wait_event_*() needs to cope with being called many times.\n */\nstatic int rds_next_incoming(struct rds_sock *rs, struct rds_incoming **inc)\n{\n\tunsigned long flags;\n\n\tif (!*inc) {\n\t\tread_lock_irqsave(&rs->rs_recv_lock, flags);\n\t\tif (!list_empty(&rs->rs_recv_queue)) {\n\t\t\t*inc = list_entry(rs->rs_recv_queue.next,\n\t\t\t\t\t  struct rds_incoming,\n\t\t\t\t\t  i_item);\n\t\t\trds_inc_addref(*inc);\n\t\t}\n\t\tread_unlock_irqrestore(&rs->rs_recv_lock, flags);\n\t}\n\n\treturn *inc != NULL;\n}\n\nstatic int rds_still_queued(struct rds_sock *rs, struct rds_incoming *inc,\n\t\t\t    int drop)\n{\n\tstruct sock *sk = rds_rs_to_sk(rs);\n\tint ret = 0;\n\tunsigned long flags;\n\n\twrite_lock_irqsave(&rs->rs_recv_lock, flags);\n\tif (!list_empty(&inc->i_item)) {\n\t\tret = 1;\n\t\tif (drop) {\n\t\t\t/* XXX make sure this i_conn is reliable */\n\t\t\trds_recv_rcvbuf_delta(rs, sk, inc->i_conn->c_lcong,\n\t\t\t\t\t      -be32_to_cpu(inc->i_hdr.h_len),\n\t\t\t\t\t      inc->i_hdr.h_dport);\n\t\t\tlist_del_init(&inc->i_item);\n\t\t\trds_inc_put(inc);\n\t\t}\n\t}\n\twrite_unlock_irqrestore(&rs->rs_recv_lock, flags);\n\n\trdsdebug(\"inc %p rs %p still %d dropped %d\\n\", inc, rs, ret, drop);\n\treturn ret;\n}\n\n/*\n * Pull errors off the error queue.\n * If msghdr is NULL, we will just purge the error queue.\n */\nint rds_notify_queue_get(struct rds_sock *rs, struct msghdr *msghdr)\n{\n\tstruct rds_notifier *notifier;\n\tstruct rds_rdma_notify cmsg = { 0 }; /* fill holes with zero */\n\tunsigned int count = 0, max_messages = ~0U;\n\tunsigned long flags;\n\tLIST_HEAD(copy);\n\tint err = 0;\n\n\n\t/* put_cmsg copies to user space and thus may sleep. We can't do this\n\t * with rs_lock held, so first grab as many notifications as we can stuff\n\t * in the user provided cmsg buffer. We don't try to copy more, to avoid\n\t * losing notifications - except when the buffer is so small that it wouldn't\n\t * even hold a single notification. Then we give him as much of this single\n\t * msg as we can squeeze in, and set MSG_CTRUNC.\n\t */\n\tif (msghdr) {\n\t\tmax_messages = msghdr->msg_controllen / CMSG_SPACE(sizeof(cmsg));\n\t\tif (!max_messages)\n\t\t\tmax_messages = 1;\n\t}\n\n\tspin_lock_irqsave(&rs->rs_lock, flags);\n\twhile (!list_empty(&rs->rs_notify_queue) && count < max_messages) {\n\t\tnotifier = list_entry(rs->rs_notify_queue.next,\n\t\t\t\tstruct rds_notifier, n_list);\n\t\tlist_move(&notifier->n_list, &copy);\n\t\tcount++;\n\t}\n\tspin_unlock_irqrestore(&rs->rs_lock, flags);\n\n\tif (!count)\n\t\treturn 0;\n\n\twhile (!list_empty(&copy)) {\n\t\tnotifier = list_entry(copy.next, struct rds_notifier, n_list);\n\n\t\tif (msghdr) {\n\t\t\tcmsg.user_token = notifier->n_user_token;\n\t\t\tcmsg.status = notifier->n_status;\n\n\t\t\terr = put_cmsg(msghdr, SOL_RDS, RDS_CMSG_RDMA_STATUS,\n\t\t\t\t       sizeof(cmsg), &cmsg);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tlist_del_init(&notifier->n_list);\n\t\tkfree(notifier);\n\t}\n\n\t/* If we bailed out because of an error in put_cmsg,\n\t * we may be left with one or more notifications that we\n\t * didn't process. Return them to the head of the list. */\n\tif (!list_empty(&copy)) {\n\t\tspin_lock_irqsave(&rs->rs_lock, flags);\n\t\tlist_splice(&copy, &rs->rs_notify_queue);\n\t\tspin_unlock_irqrestore(&rs->rs_lock, flags);\n\t}\n\n\treturn err;\n}\n\n/*\n * Queue a congestion notification\n */\nstatic int rds_notify_cong(struct rds_sock *rs, struct msghdr *msghdr)\n{\n\tuint64_t notify = rs->rs_cong_notify;\n\tunsigned long flags;\n\tint err;\n\n\terr = put_cmsg(msghdr, SOL_RDS, RDS_CMSG_CONG_UPDATE,\n\t\t\tsizeof(notify), &notify);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock_irqsave(&rs->rs_lock, flags);\n\trs->rs_cong_notify &= ~notify;\n\tspin_unlock_irqrestore(&rs->rs_lock, flags);\n\n\treturn 0;\n}\n\n/*\n * Receive any control messages.\n */\nstatic int rds_cmsg_recv(struct rds_incoming *inc, struct msghdr *msg,\n\t\t\t struct rds_sock *rs)\n{\n\tint ret = 0;\n\n\tif (inc->i_rdma_cookie) {\n\t\tret = put_cmsg(msg, SOL_RDS, RDS_CMSG_RDMA_DEST,\n\t\t\t\tsizeof(inc->i_rdma_cookie), &inc->i_rdma_cookie);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif ((inc->i_rx_tstamp != 0) &&\n\t    sock_flag(rds_rs_to_sk(rs), SOCK_RCVTSTAMP)) {\n\t\tstruct __kernel_old_timeval tv = ns_to_kernel_old_timeval(inc->i_rx_tstamp);\n\n\t\tif (!sock_flag(rds_rs_to_sk(rs), SOCK_TSTAMP_NEW)) {\n\t\t\tret = put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP_OLD,\n\t\t\t\t       sizeof(tv), &tv);\n\t\t} else {\n\t\t\tstruct __kernel_sock_timeval sk_tv;\n\n\t\t\tsk_tv.tv_sec = tv.tv_sec;\n\t\t\tsk_tv.tv_usec = tv.tv_usec;\n\n\t\t\tret = put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP_NEW,\n\t\t\t\t       sizeof(sk_tv), &sk_tv);\n\t\t}\n\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (rs->rs_rx_traces) {\n\t\tstruct rds_cmsg_rx_trace t;\n\t\tint i, j;\n\n\t\tmemset(&t, 0, sizeof(t));\n\t\tinc->i_rx_lat_trace[RDS_MSG_RX_CMSG] = local_clock();\n\t\tt.rx_traces =  rs->rs_rx_traces;\n\t\tfor (i = 0; i < rs->rs_rx_traces; i++) {\n\t\t\tj = rs->rs_rx_trace[i];\n\t\t\tt.rx_trace_pos[i] = j;\n\t\t\tt.rx_trace[i] = inc->i_rx_lat_trace[j + 1] -\n\t\t\t\t\t  inc->i_rx_lat_trace[j];\n\t\t}\n\n\t\tret = put_cmsg(msg, SOL_RDS, RDS_CMSG_RXPATH_LATENCY,\n\t\t\t       sizeof(t), &t);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic bool rds_recvmsg_zcookie(struct rds_sock *rs, struct msghdr *msg)\n{\n\tstruct rds_msg_zcopy_queue *q = &rs->rs_zcookie_queue;\n\tstruct rds_msg_zcopy_info *info = NULL;\n\tstruct rds_zcopy_cookies *done;\n\tunsigned long flags;\n\n\tif (!msg->msg_control)\n\t\treturn false;\n\n\tif (!sock_flag(rds_rs_to_sk(rs), SOCK_ZEROCOPY) ||\n\t    msg->msg_controllen < CMSG_SPACE(sizeof(*done)))\n\t\treturn false;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tif (!list_empty(&q->zcookie_head)) {\n\t\tinfo = list_entry(q->zcookie_head.next,\n\t\t\t\t  struct rds_msg_zcopy_info, rs_zcookie_next);\n\t\tlist_del(&info->rs_zcookie_next);\n\t}\n\tspin_unlock_irqrestore(&q->lock, flags);\n\tif (!info)\n\t\treturn false;\n\tdone = &info->zcookies;\n\tif (put_cmsg(msg, SOL_RDS, RDS_CMSG_ZCOPY_COMPLETION, sizeof(*done),\n\t\t     done)) {\n\t\tspin_lock_irqsave(&q->lock, flags);\n\t\tlist_add(&info->rs_zcookie_next, &q->zcookie_head);\n\t\tspin_unlock_irqrestore(&q->lock, flags);\n\t\treturn false;\n\t}\n\tkfree(info);\n\treturn true;\n}\n\nint rds_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\tint msg_flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tlong timeo;\n\tint ret = 0, nonblock = msg_flags & MSG_DONTWAIT;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, msg->msg_name);\n\tstruct rds_incoming *inc = NULL;\n\n\t/* udp_recvmsg()->sock_recvtimeo() gets away without locking too.. */\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\trdsdebug(\"size %zu flags 0x%x timeo %ld\\n\", size, msg_flags, timeo);\n\n\tif (msg_flags & MSG_OOB)\n\t\tgoto out;\n\tif (msg_flags & MSG_ERRQUEUE)\n\t\treturn sock_recv_errqueue(sk, msg, size, SOL_IP, IP_RECVERR);\n\n\twhile (1) {\n\t\t/* If there are pending notifications, do those - and nothing else */\n\t\tif (!list_empty(&rs->rs_notify_queue)) {\n\t\t\tret = rds_notify_queue_get(rs, msg);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rs->rs_cong_notify) {\n\t\t\tret = rds_notify_cong(rs, msg);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!rds_next_incoming(rs, &inc)) {\n\t\t\tif (nonblock) {\n\t\t\t\tbool reaped = rds_recvmsg_zcookie(rs, msg);\n\n\t\t\t\tret = reaped ?  0 : -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t(!list_empty(&rs->rs_notify_queue) ||\n\t\t\t\t\t rs->rs_cong_notify ||\n\t\t\t\t\t rds_next_incoming(rs, &inc)), timeo);\n\t\t\trdsdebug(\"recvmsg woke inc %p timeo %ld\\n\", inc,\n\t\t\t\t timeo);\n\t\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\t\tcontinue;\n\n\t\t\tret = timeo;\n\t\t\tif (ret == 0)\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\n\t\trdsdebug(\"copying inc %p from %pI6c:%u to user\\n\", inc,\n\t\t\t &inc->i_conn->c_faddr,\n\t\t\t ntohs(inc->i_hdr.h_sport));\n\t\tret = inc->i_conn->c_trans->inc_copy_to_user(inc, &msg->msg_iter);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * if the message we just copied isn't at the head of the\n\t\t * recv queue then someone else raced us to return it, try\n\t\t * to get the next message.\n\t\t */\n\t\tif (!rds_still_queued(rs, inc, !(msg_flags & MSG_PEEK))) {\n\t\t\trds_inc_put(inc);\n\t\t\tinc = NULL;\n\t\t\trds_stats_inc(s_recv_deliver_raced);\n\t\t\tiov_iter_revert(&msg->msg_iter, ret);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ret < be32_to_cpu(inc->i_hdr.h_len)) {\n\t\t\tif (msg_flags & MSG_TRUNC)\n\t\t\t\tret = be32_to_cpu(inc->i_hdr.h_len);\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\t}\n\n\t\tif (rds_cmsg_recv(inc, msg, rs)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\trds_recvmsg_zcookie(rs, msg);\n\n\t\trds_stats_inc(s_recv_delivered);\n\n\t\tif (msg->msg_name) {\n\t\t\tif (ipv6_addr_v4mapped(&inc->i_saddr)) {\n\t\t\t\tsin = (struct sockaddr_in *)msg->msg_name;\n\n\t\t\t\tsin->sin_family = AF_INET;\n\t\t\t\tsin->sin_port = inc->i_hdr.h_sport;\n\t\t\t\tsin->sin_addr.s_addr =\n\t\t\t\t    inc->i_saddr.s6_addr32[3];\n\t\t\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t\t\tmsg->msg_namelen = sizeof(*sin);\n\t\t\t} else {\n\t\t\t\tsin6 = (struct sockaddr_in6 *)msg->msg_name;\n\n\t\t\t\tsin6->sin6_family = AF_INET6;\n\t\t\t\tsin6->sin6_port = inc->i_hdr.h_sport;\n\t\t\t\tsin6->sin6_addr = inc->i_saddr;\n\t\t\t\tsin6->sin6_flowinfo = 0;\n\t\t\t\tsin6->sin6_scope_id = rs->rs_bound_scope_id;\n\t\t\t\tmsg->msg_namelen = sizeof(*sin6);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (inc)\n\t\trds_inc_put(inc);\n\nout:\n\treturn ret;\n}\n\n/*\n * The socket is being shut down and we're asked to drop messages that were\n * queued for recvmsg.  The caller has unbound the socket so the receive path\n * won't queue any more incoming fragments or messages on the socket.\n */\nvoid rds_clear_recv_queue(struct rds_sock *rs)\n{\n\tstruct sock *sk = rds_rs_to_sk(rs);\n\tstruct rds_incoming *inc, *tmp;\n\tunsigned long flags;\n\n\twrite_lock_irqsave(&rs->rs_recv_lock, flags);\n\tlist_for_each_entry_safe(inc, tmp, &rs->rs_recv_queue, i_item) {\n\t\trds_recv_rcvbuf_delta(rs, sk, inc->i_conn->c_lcong,\n\t\t\t\t      -be32_to_cpu(inc->i_hdr.h_len),\n\t\t\t\t      inc->i_hdr.h_dport);\n\t\tlist_del_init(&inc->i_item);\n\t\trds_inc_put(inc);\n\t}\n\twrite_unlock_irqrestore(&rs->rs_recv_lock, flags);\n}\n\n/*\n * inc->i_saddr isn't used here because it is only set in the receive\n * path.\n */\nvoid rds_inc_info_copy(struct rds_incoming *inc,\n\t\t       struct rds_info_iterator *iter,\n\t\t       __be32 saddr, __be32 daddr, int flip)\n{\n\tstruct rds_info_message minfo;\n\n\tminfo.seq = be64_to_cpu(inc->i_hdr.h_sequence);\n\tminfo.len = be32_to_cpu(inc->i_hdr.h_len);\n\tminfo.tos = inc->i_conn->c_tos;\n\n\tif (flip) {\n\t\tminfo.laddr = daddr;\n\t\tminfo.faddr = saddr;\n\t\tminfo.lport = inc->i_hdr.h_dport;\n\t\tminfo.fport = inc->i_hdr.h_sport;\n\t} else {\n\t\tminfo.laddr = saddr;\n\t\tminfo.faddr = daddr;\n\t\tminfo.lport = inc->i_hdr.h_sport;\n\t\tminfo.fport = inc->i_hdr.h_dport;\n\t}\n\n\tminfo.flags = 0;\n\n\trds_info_copy(iter, &minfo, sizeof(minfo));\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nvoid rds6_inc_info_copy(struct rds_incoming *inc,\n\t\t\tstruct rds_info_iterator *iter,\n\t\t\tstruct in6_addr *saddr, struct in6_addr *daddr,\n\t\t\tint flip)\n{\n\tstruct rds6_info_message minfo6;\n\n\tminfo6.seq = be64_to_cpu(inc->i_hdr.h_sequence);\n\tminfo6.len = be32_to_cpu(inc->i_hdr.h_len);\n\n\tif (flip) {\n\t\tminfo6.laddr = *daddr;\n\t\tminfo6.faddr = *saddr;\n\t\tminfo6.lport = inc->i_hdr.h_dport;\n\t\tminfo6.fport = inc->i_hdr.h_sport;\n\t} else {\n\t\tminfo6.laddr = *saddr;\n\t\tminfo6.faddr = *daddr;\n\t\tminfo6.lport = inc->i_hdr.h_sport;\n\t\tminfo6.fport = inc->i_hdr.h_dport;\n\t}\n\n\trds_info_copy(iter, &minfo6, sizeof(minfo6));\n}\n#endif\n"], "fixing_code": ["/*\n * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <net/sock.h>\n#include <linux/in.h>\n#include <linux/export.h>\n#include <linux/time.h>\n#include <linux/rds.h>\n\n#include \"rds.h\"\n\nvoid rds_inc_init(struct rds_incoming *inc, struct rds_connection *conn,\n\t\t struct in6_addr *saddr)\n{\n\trefcount_set(&inc->i_refcount, 1);\n\tINIT_LIST_HEAD(&inc->i_item);\n\tinc->i_conn = conn;\n\tinc->i_saddr = *saddr;\n\tinc->i_rdma_cookie = 0;\n\tinc->i_rx_tstamp = ktime_set(0, 0);\n\n\tmemset(inc->i_rx_lat_trace, 0, sizeof(inc->i_rx_lat_trace));\n}\nEXPORT_SYMBOL_GPL(rds_inc_init);\n\nvoid rds_inc_path_init(struct rds_incoming *inc, struct rds_conn_path *cp,\n\t\t       struct in6_addr  *saddr)\n{\n\trefcount_set(&inc->i_refcount, 1);\n\tINIT_LIST_HEAD(&inc->i_item);\n\tinc->i_conn = cp->cp_conn;\n\tinc->i_conn_path = cp;\n\tinc->i_saddr = *saddr;\n\tinc->i_rdma_cookie = 0;\n\tinc->i_rx_tstamp = ktime_set(0, 0);\n}\nEXPORT_SYMBOL_GPL(rds_inc_path_init);\n\nstatic void rds_inc_addref(struct rds_incoming *inc)\n{\n\trdsdebug(\"addref inc %p ref %d\\n\", inc, refcount_read(&inc->i_refcount));\n\trefcount_inc(&inc->i_refcount);\n}\n\nvoid rds_inc_put(struct rds_incoming *inc)\n{\n\trdsdebug(\"put inc %p ref %d\\n\", inc, refcount_read(&inc->i_refcount));\n\tif (refcount_dec_and_test(&inc->i_refcount)) {\n\t\tBUG_ON(!list_empty(&inc->i_item));\n\n\t\tinc->i_conn->c_trans->inc_free(inc);\n\t}\n}\nEXPORT_SYMBOL_GPL(rds_inc_put);\n\nstatic void rds_recv_rcvbuf_delta(struct rds_sock *rs, struct sock *sk,\n\t\t\t\t  struct rds_cong_map *map,\n\t\t\t\t  int delta, __be16 port)\n{\n\tint now_congested;\n\n\tif (delta == 0)\n\t\treturn;\n\n\trs->rs_rcv_bytes += delta;\n\tif (delta > 0)\n\t\trds_stats_add(s_recv_bytes_added_to_socket, delta);\n\telse\n\t\trds_stats_add(s_recv_bytes_removed_from_socket, -delta);\n\n\t/* loop transport doesn't send/recv congestion updates */\n\tif (rs->rs_transport->t_type == RDS_TRANS_LOOP)\n\t\treturn;\n\n\tnow_congested = rs->rs_rcv_bytes > rds_sk_rcvbuf(rs);\n\n\trdsdebug(\"rs %p (%pI6c:%u) recv bytes %d buf %d \"\n\t  \"now_cong %d delta %d\\n\",\n\t  rs, &rs->rs_bound_addr,\n\t  ntohs(rs->rs_bound_port), rs->rs_rcv_bytes,\n\t  rds_sk_rcvbuf(rs), now_congested, delta);\n\n\t/* wasn't -> am congested */\n\tif (!rs->rs_congested && now_congested) {\n\t\trs->rs_congested = 1;\n\t\trds_cong_set_bit(map, port);\n\t\trds_cong_queue_updates(map);\n\t}\n\t/* was -> aren't congested */\n\t/* Require more free space before reporting uncongested to prevent\n\t   bouncing cong/uncong state too often */\n\telse if (rs->rs_congested && (rs->rs_rcv_bytes < (rds_sk_rcvbuf(rs)/2))) {\n\t\trs->rs_congested = 0;\n\t\trds_cong_clear_bit(map, port);\n\t\trds_cong_queue_updates(map);\n\t}\n\n\t/* do nothing if no change in cong state */\n}\n\nstatic void rds_conn_peer_gen_update(struct rds_connection *conn,\n\t\t\t\t     u32 peer_gen_num)\n{\n\tint i;\n\tstruct rds_message *rm, *tmp;\n\tunsigned long flags;\n\n\tWARN_ON(conn->c_trans->t_type != RDS_TRANS_TCP);\n\tif (peer_gen_num != 0) {\n\t\tif (conn->c_peer_gen_num != 0 &&\n\t\t    peer_gen_num != conn->c_peer_gen_num) {\n\t\t\tfor (i = 0; i < RDS_MPATH_WORKERS; i++) {\n\t\t\t\tstruct rds_conn_path *cp;\n\n\t\t\t\tcp = &conn->c_path[i];\n\t\t\t\tspin_lock_irqsave(&cp->cp_lock, flags);\n\t\t\t\tcp->cp_next_tx_seq = 1;\n\t\t\t\tcp->cp_next_rx_seq = 0;\n\t\t\t\tlist_for_each_entry_safe(rm, tmp,\n\t\t\t\t\t\t\t &cp->cp_retrans,\n\t\t\t\t\t\t\t m_conn_item) {\n\t\t\t\t\tset_bit(RDS_MSG_FLUSH, &rm->m_flags);\n\t\t\t\t}\n\t\t\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\t\t\t}\n\t\t}\n\t\tconn->c_peer_gen_num = peer_gen_num;\n\t}\n}\n\n/*\n * Process all extension headers that come with this message.\n */\nstatic void rds_recv_incoming_exthdrs(struct rds_incoming *inc, struct rds_sock *rs)\n{\n\tstruct rds_header *hdr = &inc->i_hdr;\n\tunsigned int pos = 0, type, len;\n\tunion {\n\t\tstruct rds_ext_header_version version;\n\t\tstruct rds_ext_header_rdma rdma;\n\t\tstruct rds_ext_header_rdma_dest rdma_dest;\n\t} buffer;\n\n\twhile (1) {\n\t\tlen = sizeof(buffer);\n\t\ttype = rds_message_next_extension(hdr, &pos, &buffer, &len);\n\t\tif (type == RDS_EXTHDR_NONE)\n\t\t\tbreak;\n\t\t/* Process extension header here */\n\t\tswitch (type) {\n\t\tcase RDS_EXTHDR_RDMA:\n\t\t\trds_rdma_unuse(rs, be32_to_cpu(buffer.rdma.h_rdma_rkey), 0);\n\t\t\tbreak;\n\n\t\tcase RDS_EXTHDR_RDMA_DEST:\n\t\t\t/* We ignore the size for now. We could stash it\n\t\t\t * somewhere and use it for error checking. */\n\t\t\tinc->i_rdma_cookie = rds_rdma_make_cookie(\n\t\t\t\t\tbe32_to_cpu(buffer.rdma_dest.h_rdma_rkey),\n\t\t\t\t\tbe32_to_cpu(buffer.rdma_dest.h_rdma_offset));\n\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void rds_recv_hs_exthdrs(struct rds_header *hdr,\n\t\t\t\tstruct rds_connection *conn)\n{\n\tunsigned int pos = 0, type, len;\n\tunion {\n\t\tstruct rds_ext_header_version version;\n\t\tu16 rds_npaths;\n\t\tu32 rds_gen_num;\n\t} buffer;\n\tu32 new_peer_gen_num = 0;\n\n\twhile (1) {\n\t\tlen = sizeof(buffer);\n\t\ttype = rds_message_next_extension(hdr, &pos, &buffer, &len);\n\t\tif (type == RDS_EXTHDR_NONE)\n\t\t\tbreak;\n\t\t/* Process extension header here */\n\t\tswitch (type) {\n\t\tcase RDS_EXTHDR_NPATHS:\n\t\t\tconn->c_npaths = min_t(int, RDS_MPATH_WORKERS,\n\t\t\t\t\t       be16_to_cpu(buffer.rds_npaths));\n\t\t\tbreak;\n\t\tcase RDS_EXTHDR_GEN_NUM:\n\t\t\tnew_peer_gen_num = be32_to_cpu(buffer.rds_gen_num);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_warn_ratelimited(\"ignoring unknown exthdr type \"\n\t\t\t\t\t     \"0x%x\\n\", type);\n\t\t}\n\t}\n\t/* if RDS_EXTHDR_NPATHS was not found, default to a single-path */\n\tconn->c_npaths = max_t(int, conn->c_npaths, 1);\n\tconn->c_ping_triggered = 0;\n\trds_conn_peer_gen_update(conn, new_peer_gen_num);\n}\n\n/* rds_start_mprds() will synchronously start multiple paths when appropriate.\n * The scheme is based on the following rules:\n *\n * 1. rds_sendmsg on first connect attempt sends the probe ping, with the\n *    sender's npaths (s_npaths)\n * 2. rcvr of probe-ping knows the mprds_paths = min(s_npaths, r_npaths). It\n *    sends back a probe-pong with r_npaths. After that, if rcvr is the\n *    smaller ip addr, it starts rds_conn_path_connect_if_down on all\n *    mprds_paths.\n * 3. sender gets woken up, and can move to rds_conn_path_connect_if_down.\n *    If it is the smaller ipaddr, rds_conn_path_connect_if_down can be\n *    called after reception of the probe-pong on all mprds_paths.\n *    Otherwise (sender of probe-ping is not the smaller ip addr): just call\n *    rds_conn_path_connect_if_down on the hashed path. (see rule 4)\n * 4. rds_connect_worker must only trigger a connection if laddr < faddr.\n * 5. sender may end up queuing the packet on the cp. will get sent out later.\n *    when connection is completed.\n */\nstatic void rds_start_mprds(struct rds_connection *conn)\n{\n\tint i;\n\tstruct rds_conn_path *cp;\n\n\tif (conn->c_npaths > 1 &&\n\t    rds_addr_cmp(&conn->c_laddr, &conn->c_faddr) < 0) {\n\t\tfor (i = 0; i < conn->c_npaths; i++) {\n\t\t\tcp = &conn->c_path[i];\n\t\t\trds_conn_path_connect_if_down(cp);\n\t\t}\n\t}\n}\n\n/*\n * The transport must make sure that this is serialized against other\n * rx and conn reset on this specific conn.\n *\n * We currently assert that only one fragmented message will be sent\n * down a connection at a time.  This lets us reassemble in the conn\n * instead of per-flow which means that we don't have to go digging through\n * flows to tear down partial reassembly progress on conn failure and\n * we save flow lookup and locking for each frag arrival.  It does mean\n * that small messages will wait behind large ones.  Fragmenting at all\n * is only to reduce the memory consumption of pre-posted buffers.\n *\n * The caller passes in saddr and daddr instead of us getting it from the\n * conn.  This lets loopback, who only has one conn for both directions,\n * tell us which roles the addrs in the conn are playing for this message.\n */\nvoid rds_recv_incoming(struct rds_connection *conn, struct in6_addr *saddr,\n\t\t       struct in6_addr *daddr,\n\t\t       struct rds_incoming *inc, gfp_t gfp)\n{\n\tstruct rds_sock *rs = NULL;\n\tstruct sock *sk;\n\tunsigned long flags;\n\tstruct rds_conn_path *cp;\n\n\tinc->i_conn = conn;\n\tinc->i_rx_jiffies = jiffies;\n\tif (conn->c_trans->t_mp_capable)\n\t\tcp = inc->i_conn_path;\n\telse\n\t\tcp = &conn->c_path[0];\n\n\trdsdebug(\"conn %p next %llu inc %p seq %llu len %u sport %u dport %u \"\n\t\t \"flags 0x%x rx_jiffies %lu\\n\", conn,\n\t\t (unsigned long long)cp->cp_next_rx_seq,\n\t\t inc,\n\t\t (unsigned long long)be64_to_cpu(inc->i_hdr.h_sequence),\n\t\t be32_to_cpu(inc->i_hdr.h_len),\n\t\t be16_to_cpu(inc->i_hdr.h_sport),\n\t\t be16_to_cpu(inc->i_hdr.h_dport),\n\t\t inc->i_hdr.h_flags,\n\t\t inc->i_rx_jiffies);\n\n\t/*\n\t * Sequence numbers should only increase.  Messages get their\n\t * sequence number as they're queued in a sending conn.  They\n\t * can be dropped, though, if the sending socket is closed before\n\t * they hit the wire.  So sequence numbers can skip forward\n\t * under normal operation.  They can also drop back in the conn\n\t * failover case as previously sent messages are resent down the\n\t * new instance of a conn.  We drop those, otherwise we have\n\t * to assume that the next valid seq does not come after a\n\t * hole in the fragment stream.\n\t *\n\t * The headers don't give us a way to realize if fragments of\n\t * a message have been dropped.  We assume that frags that arrive\n\t * to a flow are part of the current message on the flow that is\n\t * being reassembled.  This means that senders can't drop messages\n\t * from the sending conn until all their frags are sent.\n\t *\n\t * XXX we could spend more on the wire to get more robust failure\n\t * detection, arguably worth it to avoid data corruption.\n\t */\n\tif (be64_to_cpu(inc->i_hdr.h_sequence) < cp->cp_next_rx_seq &&\n\t    (inc->i_hdr.h_flags & RDS_FLAG_RETRANSMITTED)) {\n\t\trds_stats_inc(s_recv_drop_old_seq);\n\t\tgoto out;\n\t}\n\tcp->cp_next_rx_seq = be64_to_cpu(inc->i_hdr.h_sequence) + 1;\n\n\tif (rds_sysctl_ping_enable && inc->i_hdr.h_dport == 0) {\n\t\tif (inc->i_hdr.h_sport == 0) {\n\t\t\trdsdebug(\"ignore ping with 0 sport from %pI6c\\n\",\n\t\t\t\t saddr);\n\t\t\tgoto out;\n\t\t}\n\t\trds_stats_inc(s_recv_ping);\n\t\trds_send_pong(cp, inc->i_hdr.h_sport);\n\t\t/* if this is a handshake ping, start multipath if necessary */\n\t\tif (RDS_HS_PROBE(be16_to_cpu(inc->i_hdr.h_sport),\n\t\t\t\t be16_to_cpu(inc->i_hdr.h_dport))) {\n\t\t\trds_recv_hs_exthdrs(&inc->i_hdr, cp->cp_conn);\n\t\t\trds_start_mprds(cp->cp_conn);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (be16_to_cpu(inc->i_hdr.h_dport) ==  RDS_FLAG_PROBE_PORT &&\n\t    inc->i_hdr.h_sport == 0) {\n\t\trds_recv_hs_exthdrs(&inc->i_hdr, cp->cp_conn);\n\t\t/* if this is a handshake pong, start multipath if necessary */\n\t\trds_start_mprds(cp->cp_conn);\n\t\twake_up(&cp->cp_conn->c_hs_waitq);\n\t\tgoto out;\n\t}\n\n\trs = rds_find_bound(daddr, inc->i_hdr.h_dport, conn->c_bound_if);\n\tif (!rs) {\n\t\trds_stats_inc(s_recv_drop_no_sock);\n\t\tgoto out;\n\t}\n\n\t/* Process extension headers */\n\trds_recv_incoming_exthdrs(inc, rs);\n\n\t/* We can be racing with rds_release() which marks the socket dead. */\n\tsk = rds_rs_to_sk(rs);\n\n\t/* serialize with rds_release -> sock_orphan */\n\twrite_lock_irqsave(&rs->rs_recv_lock, flags);\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\trdsdebug(\"adding inc %p to rs %p's recv queue\\n\", inc, rs);\n\t\trds_stats_inc(s_recv_queued);\n\t\trds_recv_rcvbuf_delta(rs, sk, inc->i_conn->c_lcong,\n\t\t\t\t      be32_to_cpu(inc->i_hdr.h_len),\n\t\t\t\t      inc->i_hdr.h_dport);\n\t\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t\tinc->i_rx_tstamp = ktime_get_real();\n\t\trds_inc_addref(inc);\n\t\tinc->i_rx_lat_trace[RDS_MSG_RX_END] = local_clock();\n\t\tlist_add_tail(&inc->i_item, &rs->rs_recv_queue);\n\t\t__rds_wake_sk_sleep(sk);\n\t} else {\n\t\trds_stats_inc(s_recv_drop_dead_sock);\n\t}\n\twrite_unlock_irqrestore(&rs->rs_recv_lock, flags);\n\nout:\n\tif (rs)\n\t\trds_sock_put(rs);\n}\nEXPORT_SYMBOL_GPL(rds_recv_incoming);\n\n/*\n * be very careful here.  This is being called as the condition in\n * wait_event_*() needs to cope with being called many times.\n */\nstatic int rds_next_incoming(struct rds_sock *rs, struct rds_incoming **inc)\n{\n\tunsigned long flags;\n\n\tif (!*inc) {\n\t\tread_lock_irqsave(&rs->rs_recv_lock, flags);\n\t\tif (!list_empty(&rs->rs_recv_queue)) {\n\t\t\t*inc = list_entry(rs->rs_recv_queue.next,\n\t\t\t\t\t  struct rds_incoming,\n\t\t\t\t\t  i_item);\n\t\t\trds_inc_addref(*inc);\n\t\t}\n\t\tread_unlock_irqrestore(&rs->rs_recv_lock, flags);\n\t}\n\n\treturn *inc != NULL;\n}\n\nstatic int rds_still_queued(struct rds_sock *rs, struct rds_incoming *inc,\n\t\t\t    int drop)\n{\n\tstruct sock *sk = rds_rs_to_sk(rs);\n\tint ret = 0;\n\tunsigned long flags;\n\n\twrite_lock_irqsave(&rs->rs_recv_lock, flags);\n\tif (!list_empty(&inc->i_item)) {\n\t\tret = 1;\n\t\tif (drop) {\n\t\t\t/* XXX make sure this i_conn is reliable */\n\t\t\trds_recv_rcvbuf_delta(rs, sk, inc->i_conn->c_lcong,\n\t\t\t\t\t      -be32_to_cpu(inc->i_hdr.h_len),\n\t\t\t\t\t      inc->i_hdr.h_dport);\n\t\t\tlist_del_init(&inc->i_item);\n\t\t\trds_inc_put(inc);\n\t\t}\n\t}\n\twrite_unlock_irqrestore(&rs->rs_recv_lock, flags);\n\n\trdsdebug(\"inc %p rs %p still %d dropped %d\\n\", inc, rs, ret, drop);\n\treturn ret;\n}\n\n/*\n * Pull errors off the error queue.\n * If msghdr is NULL, we will just purge the error queue.\n */\nint rds_notify_queue_get(struct rds_sock *rs, struct msghdr *msghdr)\n{\n\tstruct rds_notifier *notifier;\n\tstruct rds_rdma_notify cmsg = { 0 }; /* fill holes with zero */\n\tunsigned int count = 0, max_messages = ~0U;\n\tunsigned long flags;\n\tLIST_HEAD(copy);\n\tint err = 0;\n\n\n\t/* put_cmsg copies to user space and thus may sleep. We can't do this\n\t * with rs_lock held, so first grab as many notifications as we can stuff\n\t * in the user provided cmsg buffer. We don't try to copy more, to avoid\n\t * losing notifications - except when the buffer is so small that it wouldn't\n\t * even hold a single notification. Then we give him as much of this single\n\t * msg as we can squeeze in, and set MSG_CTRUNC.\n\t */\n\tif (msghdr) {\n\t\tmax_messages = msghdr->msg_controllen / CMSG_SPACE(sizeof(cmsg));\n\t\tif (!max_messages)\n\t\t\tmax_messages = 1;\n\t}\n\n\tspin_lock_irqsave(&rs->rs_lock, flags);\n\twhile (!list_empty(&rs->rs_notify_queue) && count < max_messages) {\n\t\tnotifier = list_entry(rs->rs_notify_queue.next,\n\t\t\t\tstruct rds_notifier, n_list);\n\t\tlist_move(&notifier->n_list, &copy);\n\t\tcount++;\n\t}\n\tspin_unlock_irqrestore(&rs->rs_lock, flags);\n\n\tif (!count)\n\t\treturn 0;\n\n\twhile (!list_empty(&copy)) {\n\t\tnotifier = list_entry(copy.next, struct rds_notifier, n_list);\n\n\t\tif (msghdr) {\n\t\t\tcmsg.user_token = notifier->n_user_token;\n\t\t\tcmsg.status = notifier->n_status;\n\n\t\t\terr = put_cmsg(msghdr, SOL_RDS, RDS_CMSG_RDMA_STATUS,\n\t\t\t\t       sizeof(cmsg), &cmsg);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tlist_del_init(&notifier->n_list);\n\t\tkfree(notifier);\n\t}\n\n\t/* If we bailed out because of an error in put_cmsg,\n\t * we may be left with one or more notifications that we\n\t * didn't process. Return them to the head of the list. */\n\tif (!list_empty(&copy)) {\n\t\tspin_lock_irqsave(&rs->rs_lock, flags);\n\t\tlist_splice(&copy, &rs->rs_notify_queue);\n\t\tspin_unlock_irqrestore(&rs->rs_lock, flags);\n\t}\n\n\treturn err;\n}\n\n/*\n * Queue a congestion notification\n */\nstatic int rds_notify_cong(struct rds_sock *rs, struct msghdr *msghdr)\n{\n\tuint64_t notify = rs->rs_cong_notify;\n\tunsigned long flags;\n\tint err;\n\n\terr = put_cmsg(msghdr, SOL_RDS, RDS_CMSG_CONG_UPDATE,\n\t\t\tsizeof(notify), &notify);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock_irqsave(&rs->rs_lock, flags);\n\trs->rs_cong_notify &= ~notify;\n\tspin_unlock_irqrestore(&rs->rs_lock, flags);\n\n\treturn 0;\n}\n\n/*\n * Receive any control messages.\n */\nstatic int rds_cmsg_recv(struct rds_incoming *inc, struct msghdr *msg,\n\t\t\t struct rds_sock *rs)\n{\n\tint ret = 0;\n\n\tif (inc->i_rdma_cookie) {\n\t\tret = put_cmsg(msg, SOL_RDS, RDS_CMSG_RDMA_DEST,\n\t\t\t\tsizeof(inc->i_rdma_cookie), &inc->i_rdma_cookie);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif ((inc->i_rx_tstamp != 0) &&\n\t    sock_flag(rds_rs_to_sk(rs), SOCK_RCVTSTAMP)) {\n\t\tstruct __kernel_old_timeval tv = ns_to_kernel_old_timeval(inc->i_rx_tstamp);\n\n\t\tif (!sock_flag(rds_rs_to_sk(rs), SOCK_TSTAMP_NEW)) {\n\t\t\tret = put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP_OLD,\n\t\t\t\t       sizeof(tv), &tv);\n\t\t} else {\n\t\t\tstruct __kernel_sock_timeval sk_tv;\n\n\t\t\tsk_tv.tv_sec = tv.tv_sec;\n\t\t\tsk_tv.tv_usec = tv.tv_usec;\n\n\t\t\tret = put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP_NEW,\n\t\t\t\t       sizeof(sk_tv), &sk_tv);\n\t\t}\n\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (rs->rs_rx_traces) {\n\t\tstruct rds_cmsg_rx_trace t;\n\t\tint i, j;\n\n\t\tmemset(&t, 0, sizeof(t));\n\t\tinc->i_rx_lat_trace[RDS_MSG_RX_CMSG] = local_clock();\n\t\tt.rx_traces =  rs->rs_rx_traces;\n\t\tfor (i = 0; i < rs->rs_rx_traces; i++) {\n\t\t\tj = rs->rs_rx_trace[i];\n\t\t\tt.rx_trace_pos[i] = j;\n\t\t\tt.rx_trace[i] = inc->i_rx_lat_trace[j + 1] -\n\t\t\t\t\t  inc->i_rx_lat_trace[j];\n\t\t}\n\n\t\tret = put_cmsg(msg, SOL_RDS, RDS_CMSG_RXPATH_LATENCY,\n\t\t\t       sizeof(t), &t);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic bool rds_recvmsg_zcookie(struct rds_sock *rs, struct msghdr *msg)\n{\n\tstruct rds_msg_zcopy_queue *q = &rs->rs_zcookie_queue;\n\tstruct rds_msg_zcopy_info *info = NULL;\n\tstruct rds_zcopy_cookies *done;\n\tunsigned long flags;\n\n\tif (!msg->msg_control)\n\t\treturn false;\n\n\tif (!sock_flag(rds_rs_to_sk(rs), SOCK_ZEROCOPY) ||\n\t    msg->msg_controllen < CMSG_SPACE(sizeof(*done)))\n\t\treturn false;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tif (!list_empty(&q->zcookie_head)) {\n\t\tinfo = list_entry(q->zcookie_head.next,\n\t\t\t\t  struct rds_msg_zcopy_info, rs_zcookie_next);\n\t\tlist_del(&info->rs_zcookie_next);\n\t}\n\tspin_unlock_irqrestore(&q->lock, flags);\n\tif (!info)\n\t\treturn false;\n\tdone = &info->zcookies;\n\tif (put_cmsg(msg, SOL_RDS, RDS_CMSG_ZCOPY_COMPLETION, sizeof(*done),\n\t\t     done)) {\n\t\tspin_lock_irqsave(&q->lock, flags);\n\t\tlist_add(&info->rs_zcookie_next, &q->zcookie_head);\n\t\tspin_unlock_irqrestore(&q->lock, flags);\n\t\treturn false;\n\t}\n\tkfree(info);\n\treturn true;\n}\n\nint rds_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\tint msg_flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tlong timeo;\n\tint ret = 0, nonblock = msg_flags & MSG_DONTWAIT;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, msg->msg_name);\n\tstruct rds_incoming *inc = NULL;\n\n\t/* udp_recvmsg()->sock_recvtimeo() gets away without locking too.. */\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\trdsdebug(\"size %zu flags 0x%x timeo %ld\\n\", size, msg_flags, timeo);\n\n\tif (msg_flags & MSG_OOB)\n\t\tgoto out;\n\tif (msg_flags & MSG_ERRQUEUE)\n\t\treturn sock_recv_errqueue(sk, msg, size, SOL_IP, IP_RECVERR);\n\n\twhile (1) {\n\t\t/* If there are pending notifications, do those - and nothing else */\n\t\tif (!list_empty(&rs->rs_notify_queue)) {\n\t\t\tret = rds_notify_queue_get(rs, msg);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rs->rs_cong_notify) {\n\t\t\tret = rds_notify_cong(rs, msg);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!rds_next_incoming(rs, &inc)) {\n\t\t\tif (nonblock) {\n\t\t\t\tbool reaped = rds_recvmsg_zcookie(rs, msg);\n\n\t\t\t\tret = reaped ?  0 : -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t(!list_empty(&rs->rs_notify_queue) ||\n\t\t\t\t\t rs->rs_cong_notify ||\n\t\t\t\t\t rds_next_incoming(rs, &inc)), timeo);\n\t\t\trdsdebug(\"recvmsg woke inc %p timeo %ld\\n\", inc,\n\t\t\t\t timeo);\n\t\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\t\tcontinue;\n\n\t\t\tret = timeo;\n\t\t\tif (ret == 0)\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\n\t\trdsdebug(\"copying inc %p from %pI6c:%u to user\\n\", inc,\n\t\t\t &inc->i_conn->c_faddr,\n\t\t\t ntohs(inc->i_hdr.h_sport));\n\t\tret = inc->i_conn->c_trans->inc_copy_to_user(inc, &msg->msg_iter);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * if the message we just copied isn't at the head of the\n\t\t * recv queue then someone else raced us to return it, try\n\t\t * to get the next message.\n\t\t */\n\t\tif (!rds_still_queued(rs, inc, !(msg_flags & MSG_PEEK))) {\n\t\t\trds_inc_put(inc);\n\t\t\tinc = NULL;\n\t\t\trds_stats_inc(s_recv_deliver_raced);\n\t\t\tiov_iter_revert(&msg->msg_iter, ret);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ret < be32_to_cpu(inc->i_hdr.h_len)) {\n\t\t\tif (msg_flags & MSG_TRUNC)\n\t\t\t\tret = be32_to_cpu(inc->i_hdr.h_len);\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\t}\n\n\t\tif (rds_cmsg_recv(inc, msg, rs)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\trds_recvmsg_zcookie(rs, msg);\n\n\t\trds_stats_inc(s_recv_delivered);\n\n\t\tif (msg->msg_name) {\n\t\t\tif (ipv6_addr_v4mapped(&inc->i_saddr)) {\n\t\t\t\tsin = (struct sockaddr_in *)msg->msg_name;\n\n\t\t\t\tsin->sin_family = AF_INET;\n\t\t\t\tsin->sin_port = inc->i_hdr.h_sport;\n\t\t\t\tsin->sin_addr.s_addr =\n\t\t\t\t    inc->i_saddr.s6_addr32[3];\n\t\t\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t\t\tmsg->msg_namelen = sizeof(*sin);\n\t\t\t} else {\n\t\t\t\tsin6 = (struct sockaddr_in6 *)msg->msg_name;\n\n\t\t\t\tsin6->sin6_family = AF_INET6;\n\t\t\t\tsin6->sin6_port = inc->i_hdr.h_sport;\n\t\t\t\tsin6->sin6_addr = inc->i_saddr;\n\t\t\t\tsin6->sin6_flowinfo = 0;\n\t\t\t\tsin6->sin6_scope_id = rs->rs_bound_scope_id;\n\t\t\t\tmsg->msg_namelen = sizeof(*sin6);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (inc)\n\t\trds_inc_put(inc);\n\nout:\n\treturn ret;\n}\n\n/*\n * The socket is being shut down and we're asked to drop messages that were\n * queued for recvmsg.  The caller has unbound the socket so the receive path\n * won't queue any more incoming fragments or messages on the socket.\n */\nvoid rds_clear_recv_queue(struct rds_sock *rs)\n{\n\tstruct sock *sk = rds_rs_to_sk(rs);\n\tstruct rds_incoming *inc, *tmp;\n\tunsigned long flags;\n\n\twrite_lock_irqsave(&rs->rs_recv_lock, flags);\n\tlist_for_each_entry_safe(inc, tmp, &rs->rs_recv_queue, i_item) {\n\t\trds_recv_rcvbuf_delta(rs, sk, inc->i_conn->c_lcong,\n\t\t\t\t      -be32_to_cpu(inc->i_hdr.h_len),\n\t\t\t\t      inc->i_hdr.h_dport);\n\t\tlist_del_init(&inc->i_item);\n\t\trds_inc_put(inc);\n\t}\n\twrite_unlock_irqrestore(&rs->rs_recv_lock, flags);\n}\n\n/*\n * inc->i_saddr isn't used here because it is only set in the receive\n * path.\n */\nvoid rds_inc_info_copy(struct rds_incoming *inc,\n\t\t       struct rds_info_iterator *iter,\n\t\t       __be32 saddr, __be32 daddr, int flip)\n{\n\tstruct rds_info_message minfo;\n\n\tminfo.seq = be64_to_cpu(inc->i_hdr.h_sequence);\n\tminfo.len = be32_to_cpu(inc->i_hdr.h_len);\n\tminfo.tos = inc->i_conn->c_tos;\n\n\tif (flip) {\n\t\tminfo.laddr = daddr;\n\t\tminfo.faddr = saddr;\n\t\tminfo.lport = inc->i_hdr.h_dport;\n\t\tminfo.fport = inc->i_hdr.h_sport;\n\t} else {\n\t\tminfo.laddr = saddr;\n\t\tminfo.faddr = daddr;\n\t\tminfo.lport = inc->i_hdr.h_sport;\n\t\tminfo.fport = inc->i_hdr.h_dport;\n\t}\n\n\tminfo.flags = 0;\n\n\trds_info_copy(iter, &minfo, sizeof(minfo));\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nvoid rds6_inc_info_copy(struct rds_incoming *inc,\n\t\t\tstruct rds_info_iterator *iter,\n\t\t\tstruct in6_addr *saddr, struct in6_addr *daddr,\n\t\t\tint flip)\n{\n\tstruct rds6_info_message minfo6;\n\n\tminfo6.seq = be64_to_cpu(inc->i_hdr.h_sequence);\n\tminfo6.len = be32_to_cpu(inc->i_hdr.h_len);\n\tminfo6.tos = inc->i_conn->c_tos;\n\n\tif (flip) {\n\t\tminfo6.laddr = *daddr;\n\t\tminfo6.faddr = *saddr;\n\t\tminfo6.lport = inc->i_hdr.h_dport;\n\t\tminfo6.fport = inc->i_hdr.h_sport;\n\t} else {\n\t\tminfo6.laddr = *saddr;\n\t\tminfo6.faddr = *daddr;\n\t\tminfo6.lport = inc->i_hdr.h_sport;\n\t\tminfo6.fport = inc->i_hdr.h_dport;\n\t}\n\n\tminfo6.flags = 0;\n\n\trds_info_copy(iter, &minfo6, sizeof(minfo6));\n}\n#endif\n"], "filenames": ["net/rds/recv.c"], "buggy_code_start_loc": [2], "buggy_code_end_loc": [826], "fixing_code_start_loc": [2], "fixing_code_end_loc": [830], "type": "CWE-909", "message": "In the Linux kernel before 5.2.14, rds6_inc_info_copy in net/rds/recv.c allows attackers to obtain sensitive information from kernel stack memory because tos and flags fields are not initialized.", "other": {"cve": {"id": "CVE-2019-16714", "sourceIdentifier": "cve@mitre.org", "published": "2019-09-23T12:15:10.847", "lastModified": "2022-03-31T17:45:06.220", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the Linux kernel before 5.2.14, rds6_inc_info_copy in net/rds/recv.c allows attackers to obtain sensitive information from kernel stack memory because tos and flags fields are not initialized."}, {"lang": "es", "value": "En el kernel de Linux versiones anteriores a 5.2.14, la funci\u00f3n rds6_inc_info_copy en el archivo net/rds/recv.c permite a atacantes obtener informaci\u00f3n confidencial de la memoria de la pila del kernel porque los campos tos y flags no est\u00e1n inicializados."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-909"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.2.14", "matchCriteriaId": "E0C02F8E-5352-428C-AF29-EEC37F1C9FC9"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:19.04:*:*:*:*:*:*:*", "matchCriteriaId": "CD783B0C-9246-47D9-A937-6144FE8BFF0F"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:f5:traffix_signaling_delivery_controller:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.0.0", "versionEndIncluding": "5.1.0", "matchCriteriaId": "4E52F91D-3F39-4D89-8069-EC422FB1F700"}]}]}], "references": [{"url": "http://www.openwall.com/lists/oss-security/2019/09/24/2", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2019/09/25/1", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.2.14", "source": "cve@mitre.org", "tags": ["Mailing List", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/7d0a06586b2686ba80c4a2da5f91cb10ffbea736", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20191031-0005/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://support.f5.com/csp/article/K48351130?utm_source=f5support&amp;utm_medium=RSS", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4157-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4157-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/7d0a06586b2686ba80c4a2da5f91cb10ffbea736"}}