{"buggy_code": ["/*\n * Common Block IO controller cgroup interface\n *\n * Based on ideas and code from CFQ, CFS and BFQ:\n * Copyright (C) 2003 Jens Axboe <axboe@kernel.dk>\n *\n * Copyright (C) 2008 Fabio Checconi <fabio@gandalf.sssup.it>\n *\t\t      Paolo Valente <paolo.valente@unimore.it>\n *\n * Copyright (C) 2009 Vivek Goyal <vgoyal@redhat.com>\n * \t              Nauman Rafique <nauman@google.com>\n *\n * For policy-specific per-blkcg data:\n * Copyright (C) 2015 Paolo Valente <paolo.valente@unimore.it>\n *                    Arianna Avanzini <avanzini.arianna@gmail.com>\n */\n#include <linux/ioprio.h>\n#include <linux/kdev_t.h>\n#include <linux/module.h>\n#include <linux/err.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/slab.h>\n#include <linux/genhd.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/ctype.h>\n#include <linux/blk-cgroup.h>\n#include \"blk.h\"\n\n#define MAX_KEY_LEN 100\n\n/*\n * blkcg_pol_mutex protects blkcg_policy[] and policy [de]activation.\n * blkcg_pol_register_mutex nests outside of it and synchronizes entire\n * policy [un]register operations including cgroup file additions /\n * removals.  Putting cgroup file registration outside blkcg_pol_mutex\n * allows grabbing it from cgroup callbacks.\n */\nstatic DEFINE_MUTEX(blkcg_pol_register_mutex);\nstatic DEFINE_MUTEX(blkcg_pol_mutex);\n\nstruct blkcg blkcg_root;\nEXPORT_SYMBOL_GPL(blkcg_root);\n\nstruct cgroup_subsys_state * const blkcg_root_css = &blkcg_root.css;\n\nstatic struct blkcg_policy *blkcg_policy[BLKCG_MAX_POLS];\n\nstatic LIST_HEAD(all_blkcgs);\t\t/* protected by blkcg_pol_mutex */\n\nstatic bool blkcg_policy_enabled(struct request_queue *q,\n\t\t\t\t const struct blkcg_policy *pol)\n{\n\treturn pol && test_bit(pol->plid, q->blkcg_pols);\n}\n\n/**\n * blkg_free - free a blkg\n * @blkg: blkg to free\n *\n * Free @blkg which may be partially allocated.\n */\nstatic void blkg_free(struct blkcg_gq *blkg)\n{\n\tint i;\n\n\tif (!blkg)\n\t\treturn;\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (blkg->pd[i])\n\t\t\tblkcg_policy[i]->pd_free_fn(blkg->pd[i]);\n\n\tif (blkg->blkcg != &blkcg_root)\n\t\tblk_exit_rl(&blkg->rl);\n\n\tblkg_rwstat_exit(&blkg->stat_ios);\n\tblkg_rwstat_exit(&blkg->stat_bytes);\n\tkfree(blkg);\n}\n\n/**\n * blkg_alloc - allocate a blkg\n * @blkcg: block cgroup the new blkg is associated with\n * @q: request_queue the new blkg is associated with\n * @gfp_mask: allocation mask to use\n *\n * Allocate a new blkg assocating @blkcg and @q.\n */\nstatic struct blkcg_gq *blkg_alloc(struct blkcg *blkcg, struct request_queue *q,\n\t\t\t\t   gfp_t gfp_mask)\n{\n\tstruct blkcg_gq *blkg;\n\tint i;\n\n\t/* alloc and init base part */\n\tblkg = kzalloc_node(sizeof(*blkg), gfp_mask, q->node);\n\tif (!blkg)\n\t\treturn NULL;\n\n\tif (blkg_rwstat_init(&blkg->stat_bytes, gfp_mask) ||\n\t    blkg_rwstat_init(&blkg->stat_ios, gfp_mask))\n\t\tgoto err_free;\n\n\tblkg->q = q;\n\tINIT_LIST_HEAD(&blkg->q_node);\n\tblkg->blkcg = blkcg;\n\tatomic_set(&blkg->refcnt, 1);\n\n\t/* root blkg uses @q->root_rl, init rl only for !root blkgs */\n\tif (blkcg != &blkcg_root) {\n\t\tif (blk_init_rl(&blkg->rl, q, gfp_mask))\n\t\t\tgoto err_free;\n\t\tblkg->rl.blkg = blkg;\n\t}\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\t\tstruct blkg_policy_data *pd;\n\n\t\tif (!blkcg_policy_enabled(q, pol))\n\t\t\tcontinue;\n\n\t\t/* alloc per-policy data and attach it to blkg */\n\t\tpd = pol->pd_alloc_fn(gfp_mask, q->node);\n\t\tif (!pd)\n\t\t\tgoto err_free;\n\n\t\tblkg->pd[i] = pd;\n\t\tpd->blkg = blkg;\n\t\tpd->plid = i;\n\t}\n\n\treturn blkg;\n\nerr_free:\n\tblkg_free(blkg);\n\treturn NULL;\n}\n\nstruct blkcg_gq *blkg_lookup_slowpath(struct blkcg *blkcg,\n\t\t\t\t      struct request_queue *q, bool update_hint)\n{\n\tstruct blkcg_gq *blkg;\n\n\t/*\n\t * Hint didn't match.  Look up from the radix tree.  Note that the\n\t * hint can only be updated under queue_lock as otherwise @blkg\n\t * could have already been removed from blkg_tree.  The caller is\n\t * responsible for grabbing queue_lock if @update_hint.\n\t */\n\tblkg = radix_tree_lookup(&blkcg->blkg_tree, q->id);\n\tif (blkg && blkg->q == q) {\n\t\tif (update_hint) {\n\t\t\tlockdep_assert_held(q->queue_lock);\n\t\t\trcu_assign_pointer(blkcg->blkg_hint, blkg);\n\t\t}\n\t\treturn blkg;\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(blkg_lookup_slowpath);\n\n/*\n * If @new_blkg is %NULL, this function tries to allocate a new one as\n * necessary using %GFP_NOWAIT.  @new_blkg is always consumed on return.\n */\nstatic struct blkcg_gq *blkg_create(struct blkcg *blkcg,\n\t\t\t\t    struct request_queue *q,\n\t\t\t\t    struct blkcg_gq *new_blkg)\n{\n\tstruct blkcg_gq *blkg;\n\tstruct bdi_writeback_congested *wb_congested;\n\tint i, ret;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\tlockdep_assert_held(q->queue_lock);\n\n\t/* blkg holds a reference to blkcg */\n\tif (!css_tryget_online(&blkcg->css)) {\n\t\tret = -ENODEV;\n\t\tgoto err_free_blkg;\n\t}\n\n\twb_congested = wb_congested_get_create(q->backing_dev_info,\n\t\t\t\t\t       blkcg->css.id,\n\t\t\t\t\t       GFP_NOWAIT | __GFP_NOWARN);\n\tif (!wb_congested) {\n\t\tret = -ENOMEM;\n\t\tgoto err_put_css;\n\t}\n\n\t/* allocate */\n\tif (!new_blkg) {\n\t\tnew_blkg = blkg_alloc(blkcg, q, GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (unlikely(!new_blkg)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_put_congested;\n\t\t}\n\t}\n\tblkg = new_blkg;\n\tblkg->wb_congested = wb_congested;\n\n\t/* link parent */\n\tif (blkcg_parent(blkcg)) {\n\t\tblkg->parent = __blkg_lookup(blkcg_parent(blkcg), q, false);\n\t\tif (WARN_ON_ONCE(!blkg->parent)) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto err_put_congested;\n\t\t}\n\t\tblkg_get(blkg->parent);\n\t}\n\n\t/* invoke per-policy init */\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\tif (blkg->pd[i] && pol->pd_init_fn)\n\t\t\tpol->pd_init_fn(blkg->pd[i]);\n\t}\n\n\t/* insert */\n\tspin_lock(&blkcg->lock);\n\tret = radix_tree_insert(&blkcg->blkg_tree, q->id, blkg);\n\tif (likely(!ret)) {\n\t\thlist_add_head_rcu(&blkg->blkcg_node, &blkcg->blkg_list);\n\t\tlist_add(&blkg->q_node, &q->blkg_list);\n\n\t\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\t\tif (blkg->pd[i] && pol->pd_online_fn)\n\t\t\t\tpol->pd_online_fn(blkg->pd[i]);\n\t\t}\n\t}\n\tblkg->online = true;\n\tspin_unlock(&blkcg->lock);\n\n\tif (!ret)\n\t\treturn blkg;\n\n\t/* @blkg failed fully initialized, use the usual release path */\n\tblkg_put(blkg);\n\treturn ERR_PTR(ret);\n\nerr_put_congested:\n\twb_congested_put(wb_congested);\nerr_put_css:\n\tcss_put(&blkcg->css);\nerr_free_blkg:\n\tblkg_free(new_blkg);\n\treturn ERR_PTR(ret);\n}\n\n/**\n * blkg_lookup_create - lookup blkg, try to create one if not there\n * @blkcg: blkcg of interest\n * @q: request_queue of interest\n *\n * Lookup blkg for the @blkcg - @q pair.  If it doesn't exist, try to\n * create one.  blkg creation is performed recursively from blkcg_root such\n * that all non-root blkg's have access to the parent blkg.  This function\n * should be called under RCU read lock and @q->queue_lock.\n *\n * Returns pointer to the looked up or created blkg on success, ERR_PTR()\n * value on error.  If @q is dead, returns ERR_PTR(-EINVAL).  If @q is not\n * dead and bypassing, returns ERR_PTR(-EBUSY).\n */\nstruct blkcg_gq *blkg_lookup_create(struct blkcg *blkcg,\n\t\t\t\t    struct request_queue *q)\n{\n\tstruct blkcg_gq *blkg;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\tlockdep_assert_held(q->queue_lock);\n\n\t/*\n\t * This could be the first entry point of blkcg implementation and\n\t * we shouldn't allow anything to go through for a bypassing queue.\n\t */\n\tif (unlikely(blk_queue_bypass(q)))\n\t\treturn ERR_PTR(blk_queue_dying(q) ? -ENODEV : -EBUSY);\n\n\tblkg = __blkg_lookup(blkcg, q, true);\n\tif (blkg)\n\t\treturn blkg;\n\n\t/*\n\t * Create blkgs walking down from blkcg_root to @blkcg, so that all\n\t * non-root blkgs have access to their parents.\n\t */\n\twhile (true) {\n\t\tstruct blkcg *pos = blkcg;\n\t\tstruct blkcg *parent = blkcg_parent(blkcg);\n\n\t\twhile (parent && !__blkg_lookup(parent, q, false)) {\n\t\t\tpos = parent;\n\t\t\tparent = blkcg_parent(parent);\n\t\t}\n\n\t\tblkg = blkg_create(pos, q, NULL);\n\t\tif (pos == blkcg || IS_ERR(blkg))\n\t\t\treturn blkg;\n\t}\n}\n\nstatic void blkg_destroy(struct blkcg_gq *blkg)\n{\n\tstruct blkcg *blkcg = blkg->blkcg;\n\tstruct blkcg_gq *parent = blkg->parent;\n\tint i;\n\n\tlockdep_assert_held(blkg->q->queue_lock);\n\tlockdep_assert_held(&blkcg->lock);\n\n\t/* Something wrong if we are trying to remove same group twice */\n\tWARN_ON_ONCE(list_empty(&blkg->q_node));\n\tWARN_ON_ONCE(hlist_unhashed(&blkg->blkcg_node));\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\tif (blkg->pd[i] && pol->pd_offline_fn)\n\t\t\tpol->pd_offline_fn(blkg->pd[i]);\n\t}\n\n\tif (parent) {\n\t\tblkg_rwstat_add_aux(&parent->stat_bytes, &blkg->stat_bytes);\n\t\tblkg_rwstat_add_aux(&parent->stat_ios, &blkg->stat_ios);\n\t}\n\n\tblkg->online = false;\n\n\tradix_tree_delete(&blkcg->blkg_tree, blkg->q->id);\n\tlist_del_init(&blkg->q_node);\n\thlist_del_init_rcu(&blkg->blkcg_node);\n\n\t/*\n\t * Both setting lookup hint to and clearing it from @blkg are done\n\t * under queue_lock.  If it's not pointing to @blkg now, it never\n\t * will.  Hint assignment itself can race safely.\n\t */\n\tif (rcu_access_pointer(blkcg->blkg_hint) == blkg)\n\t\trcu_assign_pointer(blkcg->blkg_hint, NULL);\n\n\t/*\n\t * Put the reference taken at the time of creation so that when all\n\t * queues are gone, group can be destroyed.\n\t */\n\tblkg_put(blkg);\n}\n\n/**\n * blkg_destroy_all - destroy all blkgs associated with a request_queue\n * @q: request_queue of interest\n *\n * Destroy all blkgs associated with @q.\n */\nstatic void blkg_destroy_all(struct request_queue *q)\n{\n\tstruct blkcg_gq *blkg, *n;\n\n\tlockdep_assert_held(q->queue_lock);\n\n\tlist_for_each_entry_safe(blkg, n, &q->blkg_list, q_node) {\n\t\tstruct blkcg *blkcg = blkg->blkcg;\n\n\t\tspin_lock(&blkcg->lock);\n\t\tblkg_destroy(blkg);\n\t\tspin_unlock(&blkcg->lock);\n\t}\n\n\tq->root_blkg = NULL;\n\tq->root_rl.blkg = NULL;\n}\n\n/*\n * A group is RCU protected, but having an rcu lock does not mean that one\n * can access all the fields of blkg and assume these are valid.  For\n * example, don't try to follow throtl_data and request queue links.\n *\n * Having a reference to blkg under an rcu allows accesses to only values\n * local to groups like group stats and group rate limits.\n */\nvoid __blkg_release_rcu(struct rcu_head *rcu_head)\n{\n\tstruct blkcg_gq *blkg = container_of(rcu_head, struct blkcg_gq, rcu_head);\n\n\t/* release the blkcg and parent blkg refs this blkg has been holding */\n\tcss_put(&blkg->blkcg->css);\n\tif (blkg->parent)\n\t\tblkg_put(blkg->parent);\n\n\twb_congested_put(blkg->wb_congested);\n\n\tblkg_free(blkg);\n}\nEXPORT_SYMBOL_GPL(__blkg_release_rcu);\n\n/*\n * The next function used by blk_queue_for_each_rl().  It's a bit tricky\n * because the root blkg uses @q->root_rl instead of its own rl.\n */\nstruct request_list *__blk_queue_next_rl(struct request_list *rl,\n\t\t\t\t\t struct request_queue *q)\n{\n\tstruct list_head *ent;\n\tstruct blkcg_gq *blkg;\n\n\t/*\n\t * Determine the current blkg list_head.  The first entry is\n\t * root_rl which is off @q->blkg_list and mapped to the head.\n\t */\n\tif (rl == &q->root_rl) {\n\t\tent = &q->blkg_list;\n\t\t/* There are no more block groups, hence no request lists */\n\t\tif (list_empty(ent))\n\t\t\treturn NULL;\n\t} else {\n\t\tblkg = container_of(rl, struct blkcg_gq, rl);\n\t\tent = &blkg->q_node;\n\t}\n\n\t/* walk to the next list_head, skip root blkcg */\n\tent = ent->next;\n\tif (ent == &q->root_blkg->q_node)\n\t\tent = ent->next;\n\tif (ent == &q->blkg_list)\n\t\treturn NULL;\n\n\tblkg = container_of(ent, struct blkcg_gq, q_node);\n\treturn &blkg->rl;\n}\n\nstatic int blkcg_reset_stats(struct cgroup_subsys_state *css,\n\t\t\t     struct cftype *cftype, u64 val)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(css);\n\tstruct blkcg_gq *blkg;\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\tspin_lock_irq(&blkcg->lock);\n\n\t/*\n\t * Note that stat reset is racy - it doesn't synchronize against\n\t * stat updates.  This is a debug feature which shouldn't exist\n\t * anyway.  If you get hit by a race, retry.\n\t */\n\thlist_for_each_entry(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tblkg_rwstat_reset(&blkg->stat_bytes);\n\t\tblkg_rwstat_reset(&blkg->stat_ios);\n\n\t\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\t\tif (blkg->pd[i] && pol->pd_reset_stats_fn)\n\t\t\t\tpol->pd_reset_stats_fn(blkg->pd[i]);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&blkcg->lock);\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn 0;\n}\n\nconst char *blkg_dev_name(struct blkcg_gq *blkg)\n{\n\t/* some drivers (floppy) instantiate a queue w/o disk registered */\n\tif (blkg->q->backing_dev_info->dev)\n\t\treturn dev_name(blkg->q->backing_dev_info->dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(blkg_dev_name);\n\n/**\n * blkcg_print_blkgs - helper for printing per-blkg data\n * @sf: seq_file to print to\n * @blkcg: blkcg of interest\n * @prfill: fill function to print out a blkg\n * @pol: policy in question\n * @data: data to be passed to @prfill\n * @show_total: to print out sum of prfill return values or not\n *\n * This function invokes @prfill on each blkg of @blkcg if pd for the\n * policy specified by @pol exists.  @prfill is invoked with @sf, the\n * policy data and @data and the matching queue lock held.  If @show_total\n * is %true, the sum of the return values from @prfill is printed with\n * \"Total\" label at the end.\n *\n * This is to be used to construct print functions for\n * cftype->read_seq_string method.\n */\nvoid blkcg_print_blkgs(struct seq_file *sf, struct blkcg *blkcg,\n\t\t       u64 (*prfill)(struct seq_file *,\n\t\t\t\t     struct blkg_policy_data *, int),\n\t\t       const struct blkcg_policy *pol, int data,\n\t\t       bool show_total)\n{\n\tstruct blkcg_gq *blkg;\n\tu64 total = 0;\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tspin_lock_irq(blkg->q->queue_lock);\n\t\tif (blkcg_policy_enabled(blkg->q, pol))\n\t\t\ttotal += prfill(sf, blkg->pd[pol->plid], data);\n\t\tspin_unlock_irq(blkg->q->queue_lock);\n\t}\n\trcu_read_unlock();\n\n\tif (show_total)\n\t\tseq_printf(sf, \"Total %llu\\n\", (unsigned long long)total);\n}\nEXPORT_SYMBOL_GPL(blkcg_print_blkgs);\n\n/**\n * __blkg_prfill_u64 - prfill helper for a single u64 value\n * @sf: seq_file to print to\n * @pd: policy private data of interest\n * @v: value to print\n *\n * Print @v to @sf for the device assocaited with @pd.\n */\nu64 __blkg_prfill_u64(struct seq_file *sf, struct blkg_policy_data *pd, u64 v)\n{\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\n\tif (!dname)\n\t\treturn 0;\n\n\tseq_printf(sf, \"%s %llu\\n\", dname, (unsigned long long)v);\n\treturn v;\n}\nEXPORT_SYMBOL_GPL(__blkg_prfill_u64);\n\n/**\n * __blkg_prfill_rwstat - prfill helper for a blkg_rwstat\n * @sf: seq_file to print to\n * @pd: policy private data of interest\n * @rwstat: rwstat to print\n *\n * Print @rwstat to @sf for the device assocaited with @pd.\n */\nu64 __blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t\t const struct blkg_rwstat *rwstat)\n{\n\tstatic const char *rwstr[] = {\n\t\t[BLKG_RWSTAT_READ]\t= \"Read\",\n\t\t[BLKG_RWSTAT_WRITE]\t= \"Write\",\n\t\t[BLKG_RWSTAT_SYNC]\t= \"Sync\",\n\t\t[BLKG_RWSTAT_ASYNC]\t= \"Async\",\n\t};\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\tu64 v;\n\tint i;\n\n\tif (!dname)\n\t\treturn 0;\n\n\tfor (i = 0; i < BLKG_RWSTAT_NR; i++)\n\t\tseq_printf(sf, \"%s %s %llu\\n\", dname, rwstr[i],\n\t\t\t   (unsigned long long)atomic64_read(&rwstat->aux_cnt[i]));\n\n\tv = atomic64_read(&rwstat->aux_cnt[BLKG_RWSTAT_READ]) +\n\t\tatomic64_read(&rwstat->aux_cnt[BLKG_RWSTAT_WRITE]);\n\tseq_printf(sf, \"%s Total %llu\\n\", dname, (unsigned long long)v);\n\treturn v;\n}\nEXPORT_SYMBOL_GPL(__blkg_prfill_rwstat);\n\n/**\n * blkg_prfill_stat - prfill callback for blkg_stat\n * @sf: seq_file to print to\n * @pd: policy private data of interest\n * @off: offset to the blkg_stat in @pd\n *\n * prfill callback for printing a blkg_stat.\n */\nu64 blkg_prfill_stat(struct seq_file *sf, struct blkg_policy_data *pd, int off)\n{\n\treturn __blkg_prfill_u64(sf, pd, blkg_stat_read((void *)pd + off));\n}\nEXPORT_SYMBOL_GPL(blkg_prfill_stat);\n\n/**\n * blkg_prfill_rwstat - prfill callback for blkg_rwstat\n * @sf: seq_file to print to\n * @pd: policy private data of interest\n * @off: offset to the blkg_rwstat in @pd\n *\n * prfill callback for printing a blkg_rwstat.\n */\nu64 blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t       int off)\n{\n\tstruct blkg_rwstat rwstat = blkg_rwstat_read((void *)pd + off);\n\n\treturn __blkg_prfill_rwstat(sf, pd, &rwstat);\n}\nEXPORT_SYMBOL_GPL(blkg_prfill_rwstat);\n\nstatic u64 blkg_prfill_rwstat_field(struct seq_file *sf,\n\t\t\t\t    struct blkg_policy_data *pd, int off)\n{\n\tstruct blkg_rwstat rwstat = blkg_rwstat_read((void *)pd->blkg + off);\n\n\treturn __blkg_prfill_rwstat(sf, pd, &rwstat);\n}\n\n/**\n * blkg_print_stat_bytes - seq_show callback for blkg->stat_bytes\n * @sf: seq_file to print to\n * @v: unused\n *\n * To be used as cftype->seq_show to print blkg->stat_bytes.\n * cftype->private must be set to the blkcg_policy.\n */\nint blkg_print_stat_bytes(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat_field, (void *)seq_cft(sf)->private,\n\t\t\t  offsetof(struct blkcg_gq, stat_bytes), true);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_print_stat_bytes);\n\n/**\n * blkg_print_stat_bytes - seq_show callback for blkg->stat_ios\n * @sf: seq_file to print to\n * @v: unused\n *\n * To be used as cftype->seq_show to print blkg->stat_ios.  cftype->private\n * must be set to the blkcg_policy.\n */\nint blkg_print_stat_ios(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat_field, (void *)seq_cft(sf)->private,\n\t\t\t  offsetof(struct blkcg_gq, stat_ios), true);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_print_stat_ios);\n\nstatic u64 blkg_prfill_rwstat_field_recursive(struct seq_file *sf,\n\t\t\t\t\t      struct blkg_policy_data *pd,\n\t\t\t\t\t      int off)\n{\n\tstruct blkg_rwstat rwstat = blkg_rwstat_recursive_sum(pd->blkg,\n\t\t\t\t\t\t\t      NULL, off);\n\treturn __blkg_prfill_rwstat(sf, pd, &rwstat);\n}\n\n/**\n * blkg_print_stat_bytes_recursive - recursive version of blkg_print_stat_bytes\n * @sf: seq_file to print to\n * @v: unused\n */\nint blkg_print_stat_bytes_recursive(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat_field_recursive,\n\t\t\t  (void *)seq_cft(sf)->private,\n\t\t\t  offsetof(struct blkcg_gq, stat_bytes), true);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_print_stat_bytes_recursive);\n\n/**\n * blkg_print_stat_ios_recursive - recursive version of blkg_print_stat_ios\n * @sf: seq_file to print to\n * @v: unused\n */\nint blkg_print_stat_ios_recursive(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat_field_recursive,\n\t\t\t  (void *)seq_cft(sf)->private,\n\t\t\t  offsetof(struct blkcg_gq, stat_ios), true);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_print_stat_ios_recursive);\n\n/**\n * blkg_stat_recursive_sum - collect hierarchical blkg_stat\n * @blkg: blkg of interest\n * @pol: blkcg_policy which contains the blkg_stat\n * @off: offset to the blkg_stat in blkg_policy_data or @blkg\n *\n * Collect the blkg_stat specified by @blkg, @pol and @off and all its\n * online descendants and their aux counts.  The caller must be holding the\n * queue lock for online tests.\n *\n * If @pol is NULL, blkg_stat is at @off bytes into @blkg; otherwise, it is\n * at @off bytes into @blkg's blkg_policy_data of the policy.\n */\nu64 blkg_stat_recursive_sum(struct blkcg_gq *blkg,\n\t\t\t    struct blkcg_policy *pol, int off)\n{\n\tstruct blkcg_gq *pos_blkg;\n\tstruct cgroup_subsys_state *pos_css;\n\tu64 sum = 0;\n\n\tlockdep_assert_held(blkg->q->queue_lock);\n\n\trcu_read_lock();\n\tblkg_for_each_descendant_pre(pos_blkg, pos_css, blkg) {\n\t\tstruct blkg_stat *stat;\n\n\t\tif (!pos_blkg->online)\n\t\t\tcontinue;\n\n\t\tif (pol)\n\t\t\tstat = (void *)blkg_to_pd(pos_blkg, pol) + off;\n\t\telse\n\t\t\tstat = (void *)blkg + off;\n\n\t\tsum += blkg_stat_read(stat) + atomic64_read(&stat->aux_cnt);\n\t}\n\trcu_read_unlock();\n\n\treturn sum;\n}\nEXPORT_SYMBOL_GPL(blkg_stat_recursive_sum);\n\n/**\n * blkg_rwstat_recursive_sum - collect hierarchical blkg_rwstat\n * @blkg: blkg of interest\n * @pol: blkcg_policy which contains the blkg_rwstat\n * @off: offset to the blkg_rwstat in blkg_policy_data or @blkg\n *\n * Collect the blkg_rwstat specified by @blkg, @pol and @off and all its\n * online descendants and their aux counts.  The caller must be holding the\n * queue lock for online tests.\n *\n * If @pol is NULL, blkg_rwstat is at @off bytes into @blkg; otherwise, it\n * is at @off bytes into @blkg's blkg_policy_data of the policy.\n */\nstruct blkg_rwstat blkg_rwstat_recursive_sum(struct blkcg_gq *blkg,\n\t\t\t\t\t     struct blkcg_policy *pol, int off)\n{\n\tstruct blkcg_gq *pos_blkg;\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct blkg_rwstat sum = { };\n\tint i;\n\n\tlockdep_assert_held(blkg->q->queue_lock);\n\n\trcu_read_lock();\n\tblkg_for_each_descendant_pre(pos_blkg, pos_css, blkg) {\n\t\tstruct blkg_rwstat *rwstat;\n\n\t\tif (!pos_blkg->online)\n\t\t\tcontinue;\n\n\t\tif (pol)\n\t\t\trwstat = (void *)blkg_to_pd(pos_blkg, pol) + off;\n\t\telse\n\t\t\trwstat = (void *)pos_blkg + off;\n\n\t\tfor (i = 0; i < BLKG_RWSTAT_NR; i++)\n\t\t\tatomic64_add(atomic64_read(&rwstat->aux_cnt[i]) +\n\t\t\t\tpercpu_counter_sum_positive(&rwstat->cpu_cnt[i]),\n\t\t\t\t&sum.aux_cnt[i]);\n\t}\n\trcu_read_unlock();\n\n\treturn sum;\n}\nEXPORT_SYMBOL_GPL(blkg_rwstat_recursive_sum);\n\n/**\n * blkg_conf_prep - parse and prepare for per-blkg config update\n * @blkcg: target block cgroup\n * @pol: target policy\n * @input: input string\n * @ctx: blkg_conf_ctx to be filled\n *\n * Parse per-blkg config update from @input and initialize @ctx with the\n * result.  @ctx->blkg points to the blkg to be updated and @ctx->body the\n * part of @input following MAJ:MIN.  This function returns with RCU read\n * lock and queue lock held and must be paired with blkg_conf_finish().\n */\nint blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,\n\t\t   char *input, struct blkg_conf_ctx *ctx)\n\t__acquires(rcu) __acquires(disk->queue->queue_lock)\n{\n\tstruct gendisk *disk;\n\tstruct blkcg_gq *blkg;\n\tstruct module *owner;\n\tunsigned int major, minor;\n\tint key_len, part, ret;\n\tchar *body;\n\n\tif (sscanf(input, \"%u:%u%n\", &major, &minor, &key_len) != 2)\n\t\treturn -EINVAL;\n\n\tbody = input + key_len;\n\tif (!isspace(*body))\n\t\treturn -EINVAL;\n\tbody = skip_spaces(body);\n\n\tdisk = get_gendisk(MKDEV(major, minor), &part);\n\tif (!disk)\n\t\treturn -ENODEV;\n\tif (part) {\n\t\towner = disk->fops->owner;\n\t\tput_disk(disk);\n\t\tmodule_put(owner);\n\t\treturn -ENODEV;\n\t}\n\n\trcu_read_lock();\n\tspin_lock_irq(disk->queue->queue_lock);\n\n\tif (blkcg_policy_enabled(disk->queue, pol))\n\t\tblkg = blkg_lookup_create(blkcg, disk->queue);\n\telse\n\t\tblkg = ERR_PTR(-EOPNOTSUPP);\n\n\tif (IS_ERR(blkg)) {\n\t\tret = PTR_ERR(blkg);\n\t\trcu_read_unlock();\n\t\tspin_unlock_irq(disk->queue->queue_lock);\n\t\towner = disk->fops->owner;\n\t\tput_disk(disk);\n\t\tmodule_put(owner);\n\t\t/*\n\t\t * If queue was bypassing, we should retry.  Do so after a\n\t\t * short msleep().  It isn't strictly necessary but queue\n\t\t * can be bypassing for some time and it's always nice to\n\t\t * avoid busy looping.\n\t\t */\n\t\tif (ret == -EBUSY) {\n\t\t\tmsleep(10);\n\t\t\tret = restart_syscall();\n\t\t}\n\t\treturn ret;\n\t}\n\n\tctx->disk = disk;\n\tctx->blkg = blkg;\n\tctx->body = body;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_conf_prep);\n\n/**\n * blkg_conf_finish - finish up per-blkg config update\n * @ctx: blkg_conf_ctx intiailized by blkg_conf_prep()\n *\n * Finish up after per-blkg config update.  This function must be paired\n * with blkg_conf_prep().\n */\nvoid blkg_conf_finish(struct blkg_conf_ctx *ctx)\n\t__releases(ctx->disk->queue->queue_lock) __releases(rcu)\n{\n\tstruct module *owner;\n\n\tspin_unlock_irq(ctx->disk->queue->queue_lock);\n\trcu_read_unlock();\n\towner = ctx->disk->fops->owner;\n\tput_disk(ctx->disk);\n\tmodule_put(owner);\n}\nEXPORT_SYMBOL_GPL(blkg_conf_finish);\n\nstatic int blkcg_print_stat(struct seq_file *sf, void *v)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\n\tstruct blkcg_gq *blkg;\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tconst char *dname;\n\t\tstruct blkg_rwstat rwstat;\n\t\tu64 rbytes, wbytes, rios, wios;\n\n\t\tdname = blkg_dev_name(blkg);\n\t\tif (!dname)\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(blkg->q->queue_lock);\n\n\t\trwstat = blkg_rwstat_recursive_sum(blkg, NULL,\n\t\t\t\t\toffsetof(struct blkcg_gq, stat_bytes));\n\t\trbytes = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_READ]);\n\t\twbytes = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_WRITE]);\n\n\t\trwstat = blkg_rwstat_recursive_sum(blkg, NULL,\n\t\t\t\t\toffsetof(struct blkcg_gq, stat_ios));\n\t\trios = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_READ]);\n\t\twios = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_WRITE]);\n\n\t\tspin_unlock_irq(blkg->q->queue_lock);\n\n\t\tif (rbytes || wbytes || rios || wios)\n\t\t\tseq_printf(sf, \"%s rbytes=%llu wbytes=%llu rios=%llu wios=%llu\\n\",\n\t\t\t\t   dname, rbytes, wbytes, rios, wios);\n\t}\n\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic struct cftype blkcg_files[] = {\n\t{\n\t\t.name = \"stat\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = blkcg_print_stat,\n\t},\n\t{ }\t/* terminate */\n};\n\nstatic struct cftype blkcg_legacy_files[] = {\n\t{\n\t\t.name = \"reset_stats\",\n\t\t.write_u64 = blkcg_reset_stats,\n\t},\n\t{ }\t/* terminate */\n};\n\n/**\n * blkcg_css_offline - cgroup css_offline callback\n * @css: css of interest\n *\n * This function is called when @css is about to go away and responsible\n * for shooting down all blkgs associated with @css.  blkgs should be\n * removed while holding both q and blkcg locks.  As blkcg lock is nested\n * inside q lock, this function performs reverse double lock dancing.\n *\n * This is the blkcg counterpart of ioc_release_fn().\n */\nstatic void blkcg_css_offline(struct cgroup_subsys_state *css)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(css);\n\n\tspin_lock_irq(&blkcg->lock);\n\n\twhile (!hlist_empty(&blkcg->blkg_list)) {\n\t\tstruct blkcg_gq *blkg = hlist_entry(blkcg->blkg_list.first,\n\t\t\t\t\t\tstruct blkcg_gq, blkcg_node);\n\t\tstruct request_queue *q = blkg->q;\n\n\t\tif (spin_trylock(q->queue_lock)) {\n\t\t\tblkg_destroy(blkg);\n\t\t\tspin_unlock(q->queue_lock);\n\t\t} else {\n\t\t\tspin_unlock_irq(&blkcg->lock);\n\t\t\tcpu_relax();\n\t\t\tspin_lock_irq(&blkcg->lock);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&blkcg->lock);\n\n\twb_blkcg_offline(blkcg);\n}\n\nstatic void blkcg_css_free(struct cgroup_subsys_state *css)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(css);\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tlist_del(&blkcg->all_blkcgs_node);\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (blkcg->cpd[i])\n\t\t\tblkcg_policy[i]->cpd_free_fn(blkcg->cpd[i]);\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\n\tkfree(blkcg);\n}\n\nstatic struct cgroup_subsys_state *\nblkcg_css_alloc(struct cgroup_subsys_state *parent_css)\n{\n\tstruct blkcg *blkcg;\n\tstruct cgroup_subsys_state *ret;\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tif (!parent_css) {\n\t\tblkcg = &blkcg_root;\n\t} else {\n\t\tblkcg = kzalloc(sizeof(*blkcg), GFP_KERNEL);\n\t\tif (!blkcg) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto free_blkcg;\n\t\t}\n\t}\n\n\tfor (i = 0; i < BLKCG_MAX_POLS ; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\t\tstruct blkcg_policy_data *cpd;\n\n\t\t/*\n\t\t * If the policy hasn't been attached yet, wait for it\n\t\t * to be attached before doing anything else. Otherwise,\n\t\t * check if the policy requires any specific per-cgroup\n\t\t * data: if it does, allocate and initialize it.\n\t\t */\n\t\tif (!pol || !pol->cpd_alloc_fn)\n\t\t\tcontinue;\n\n\t\tcpd = pol->cpd_alloc_fn(GFP_KERNEL);\n\t\tif (!cpd) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto free_pd_blkcg;\n\t\t}\n\t\tblkcg->cpd[i] = cpd;\n\t\tcpd->blkcg = blkcg;\n\t\tcpd->plid = i;\n\t\tif (pol->cpd_init_fn)\n\t\t\tpol->cpd_init_fn(cpd);\n\t}\n\n\tspin_lock_init(&blkcg->lock);\n\tINIT_RADIX_TREE(&blkcg->blkg_tree, GFP_NOWAIT | __GFP_NOWARN);\n\tINIT_HLIST_HEAD(&blkcg->blkg_list);\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tINIT_LIST_HEAD(&blkcg->cgwb_list);\n#endif\n\tlist_add_tail(&blkcg->all_blkcgs_node, &all_blkcgs);\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn &blkcg->css;\n\nfree_pd_blkcg:\n\tfor (i--; i >= 0; i--)\n\t\tif (blkcg->cpd[i])\n\t\t\tblkcg_policy[i]->cpd_free_fn(blkcg->cpd[i]);\nfree_blkcg:\n\tkfree(blkcg);\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn ret;\n}\n\n/**\n * blkcg_init_queue - initialize blkcg part of request queue\n * @q: request_queue to initialize\n *\n * Called from blk_alloc_queue_node(). Responsible for initializing blkcg\n * part of new request_queue @q.\n *\n * RETURNS:\n * 0 on success, -errno on failure.\n */\nint blkcg_init_queue(struct request_queue *q)\n{\n\tstruct blkcg_gq *new_blkg, *blkg;\n\tbool preloaded;\n\tint ret;\n\n\tnew_blkg = blkg_alloc(&blkcg_root, q, GFP_KERNEL);\n\tif (!new_blkg)\n\t\treturn -ENOMEM;\n\n\tpreloaded = !radix_tree_preload(GFP_KERNEL);\n\n\t/*\n\t * Make sure the root blkg exists and count the existing blkgs.  As\n\t * @q is bypassing at this point, blkg_lookup_create() can't be\n\t * used.  Open code insertion.\n\t */\n\trcu_read_lock();\n\tspin_lock_irq(q->queue_lock);\n\tblkg = blkg_create(&blkcg_root, q, new_blkg);\n\tspin_unlock_irq(q->queue_lock);\n\trcu_read_unlock();\n\n\tif (preloaded)\n\t\tradix_tree_preload_end();\n\n\tif (IS_ERR(blkg)) {\n\t\tblkg_free(new_blkg);\n\t\treturn PTR_ERR(blkg);\n\t}\n\n\tq->root_blkg = blkg;\n\tq->root_rl.blkg = blkg;\n\n\tret = blk_throtl_init(q);\n\tif (ret) {\n\t\tspin_lock_irq(q->queue_lock);\n\t\tblkg_destroy_all(q);\n\t\tspin_unlock_irq(q->queue_lock);\n\t}\n\treturn ret;\n}\n\n/**\n * blkcg_drain_queue - drain blkcg part of request_queue\n * @q: request_queue to drain\n *\n * Called from blk_drain_queue().  Responsible for draining blkcg part.\n */\nvoid blkcg_drain_queue(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\n\t/*\n\t * @q could be exiting and already have destroyed all blkgs as\n\t * indicated by NULL root_blkg.  If so, don't confuse policies.\n\t */\n\tif (!q->root_blkg)\n\t\treturn;\n\n\tblk_throtl_drain(q);\n}\n\n/**\n * blkcg_exit_queue - exit and release blkcg part of request_queue\n * @q: request_queue being released\n *\n * Called from blk_release_queue().  Responsible for exiting blkcg part.\n */\nvoid blkcg_exit_queue(struct request_queue *q)\n{\n\tspin_lock_irq(q->queue_lock);\n\tblkg_destroy_all(q);\n\tspin_unlock_irq(q->queue_lock);\n\n\tblk_throtl_exit(q);\n}\n\n/*\n * We cannot support shared io contexts, as we have no mean to support\n * two tasks with the same ioc in two different groups without major rework\n * of the main cic data structures.  For now we allow a task to change\n * its cgroup only if it's the only owner of its ioc.\n */\nstatic int blkcg_can_attach(struct cgroup_taskset *tset)\n{\n\tstruct task_struct *task;\n\tstruct cgroup_subsys_state *dst_css;\n\tstruct io_context *ioc;\n\tint ret = 0;\n\n\t/* task_lock() is needed to avoid races with exit_io_context() */\n\tcgroup_taskset_for_each(task, dst_css, tset) {\n\t\ttask_lock(task);\n\t\tioc = task->io_context;\n\t\tif (ioc && atomic_read(&ioc->nr_tasks) > 1)\n\t\t\tret = -EINVAL;\n\t\ttask_unlock(task);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic void blkcg_bind(struct cgroup_subsys_state *root_css)\n{\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\t\tstruct blkcg *blkcg;\n\n\t\tif (!pol || !pol->cpd_bind_fn)\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node)\n\t\t\tif (blkcg->cpd[pol->plid])\n\t\t\t\tpol->cpd_bind_fn(blkcg->cpd[pol->plid]);\n\t}\n\tmutex_unlock(&blkcg_pol_mutex);\n}\n\nstruct cgroup_subsys io_cgrp_subsys = {\n\t.css_alloc = blkcg_css_alloc,\n\t.css_offline = blkcg_css_offline,\n\t.css_free = blkcg_css_free,\n\t.can_attach = blkcg_can_attach,\n\t.bind = blkcg_bind,\n\t.dfl_cftypes = blkcg_files,\n\t.legacy_cftypes = blkcg_legacy_files,\n\t.legacy_name = \"blkio\",\n#ifdef CONFIG_MEMCG\n\t/*\n\t * This ensures that, if available, memcg is automatically enabled\n\t * together on the default hierarchy so that the owner cgroup can\n\t * be retrieved from writeback pages.\n\t */\n\t.depends_on = 1 << memory_cgrp_id,\n#endif\n};\nEXPORT_SYMBOL_GPL(io_cgrp_subsys);\n\n/**\n * blkcg_activate_policy - activate a blkcg policy on a request_queue\n * @q: request_queue of interest\n * @pol: blkcg policy to activate\n *\n * Activate @pol on @q.  Requires %GFP_KERNEL context.  @q goes through\n * bypass mode to populate its blkgs with policy_data for @pol.\n *\n * Activation happens with @q bypassed, so nobody would be accessing blkgs\n * from IO path.  Update of each blkg is protected by both queue and blkcg\n * locks so that holding either lock and testing blkcg_policy_enabled() is\n * always enough for dereferencing policy data.\n *\n * The caller is responsible for synchronizing [de]activations and policy\n * [un]registerations.  Returns 0 on success, -errno on failure.\n */\nint blkcg_activate_policy(struct request_queue *q,\n\t\t\t  const struct blkcg_policy *pol)\n{\n\tstruct blkg_policy_data *pd_prealloc = NULL;\n\tstruct blkcg_gq *blkg;\n\tint ret;\n\n\tif (blkcg_policy_enabled(q, pol))\n\t\treturn 0;\n\n\tif (q->mq_ops)\n\t\tblk_mq_freeze_queue(q);\n\telse\n\t\tblk_queue_bypass_start(q);\npd_prealloc:\n\tif (!pd_prealloc) {\n\t\tpd_prealloc = pol->pd_alloc_fn(GFP_KERNEL, q->node);\n\t\tif (!pd_prealloc) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_bypass_end;\n\t\t}\n\t}\n\n\tspin_lock_irq(q->queue_lock);\n\n\tlist_for_each_entry(blkg, &q->blkg_list, q_node) {\n\t\tstruct blkg_policy_data *pd;\n\n\t\tif (blkg->pd[pol->plid])\n\t\t\tcontinue;\n\n\t\tpd = pol->pd_alloc_fn(GFP_NOWAIT | __GFP_NOWARN, q->node);\n\t\tif (!pd)\n\t\t\tswap(pd, pd_prealloc);\n\t\tif (!pd) {\n\t\t\tspin_unlock_irq(q->queue_lock);\n\t\t\tgoto pd_prealloc;\n\t\t}\n\n\t\tblkg->pd[pol->plid] = pd;\n\t\tpd->blkg = blkg;\n\t\tpd->plid = pol->plid;\n\t\tif (pol->pd_init_fn)\n\t\t\tpol->pd_init_fn(pd);\n\t}\n\n\t__set_bit(pol->plid, q->blkcg_pols);\n\tret = 0;\n\n\tspin_unlock_irq(q->queue_lock);\nout_bypass_end:\n\tif (q->mq_ops)\n\t\tblk_mq_unfreeze_queue(q);\n\telse\n\t\tblk_queue_bypass_end(q);\n\tif (pd_prealloc)\n\t\tpol->pd_free_fn(pd_prealloc);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blkcg_activate_policy);\n\n/**\n * blkcg_deactivate_policy - deactivate a blkcg policy on a request_queue\n * @q: request_queue of interest\n * @pol: blkcg policy to deactivate\n *\n * Deactivate @pol on @q.  Follows the same synchronization rules as\n * blkcg_activate_policy().\n */\nvoid blkcg_deactivate_policy(struct request_queue *q,\n\t\t\t     const struct blkcg_policy *pol)\n{\n\tstruct blkcg_gq *blkg;\n\n\tif (!blkcg_policy_enabled(q, pol))\n\t\treturn;\n\n\tif (q->mq_ops)\n\t\tblk_mq_freeze_queue(q);\n\telse\n\t\tblk_queue_bypass_start(q);\n\n\tspin_lock_irq(q->queue_lock);\n\n\t__clear_bit(pol->plid, q->blkcg_pols);\n\n\tlist_for_each_entry(blkg, &q->blkg_list, q_node) {\n\t\t/* grab blkcg lock too while removing @pd from @blkg */\n\t\tspin_lock(&blkg->blkcg->lock);\n\n\t\tif (blkg->pd[pol->plid]) {\n\t\t\tif (pol->pd_offline_fn)\n\t\t\t\tpol->pd_offline_fn(blkg->pd[pol->plid]);\n\t\t\tpol->pd_free_fn(blkg->pd[pol->plid]);\n\t\t\tblkg->pd[pol->plid] = NULL;\n\t\t}\n\n\t\tspin_unlock(&blkg->blkcg->lock);\n\t}\n\n\tspin_unlock_irq(q->queue_lock);\n\n\tif (q->mq_ops)\n\t\tblk_mq_unfreeze_queue(q);\n\telse\n\t\tblk_queue_bypass_end(q);\n}\nEXPORT_SYMBOL_GPL(blkcg_deactivate_policy);\n\n/**\n * blkcg_policy_register - register a blkcg policy\n * @pol: blkcg policy to register\n *\n * Register @pol with blkcg core.  Might sleep and @pol may be modified on\n * successful registration.  Returns 0 on success and -errno on failure.\n */\nint blkcg_policy_register(struct blkcg_policy *pol)\n{\n\tstruct blkcg *blkcg;\n\tint i, ret;\n\n\tmutex_lock(&blkcg_pol_register_mutex);\n\tmutex_lock(&blkcg_pol_mutex);\n\n\t/* find an empty slot */\n\tret = -ENOSPC;\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (!blkcg_policy[i])\n\t\t\tbreak;\n\tif (i >= BLKCG_MAX_POLS)\n\t\tgoto err_unlock;\n\n\t/* register @pol */\n\tpol->plid = i;\n\tblkcg_policy[pol->plid] = pol;\n\n\t/* allocate and install cpd's */\n\tif (pol->cpd_alloc_fn) {\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\n\t\t\tstruct blkcg_policy_data *cpd;\n\n\t\t\tcpd = pol->cpd_alloc_fn(GFP_KERNEL);\n\t\t\tif (!cpd)\n\t\t\t\tgoto err_free_cpds;\n\n\t\t\tblkcg->cpd[pol->plid] = cpd;\n\t\t\tcpd->blkcg = blkcg;\n\t\t\tcpd->plid = pol->plid;\n\t\t\tpol->cpd_init_fn(cpd);\n\t\t}\n\t}\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\n\t/* everything is in place, add intf files for the new policy */\n\tif (pol->dfl_cftypes)\n\t\tWARN_ON(cgroup_add_dfl_cftypes(&io_cgrp_subsys,\n\t\t\t\t\t       pol->dfl_cftypes));\n\tif (pol->legacy_cftypes)\n\t\tWARN_ON(cgroup_add_legacy_cftypes(&io_cgrp_subsys,\n\t\t\t\t\t\t  pol->legacy_cftypes));\n\tmutex_unlock(&blkcg_pol_register_mutex);\n\treturn 0;\n\nerr_free_cpds:\n\tif (pol->cpd_alloc_fn) {\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\n\t\t\tif (blkcg->cpd[pol->plid]) {\n\t\t\t\tpol->cpd_free_fn(blkcg->cpd[pol->plid]);\n\t\t\t\tblkcg->cpd[pol->plid] = NULL;\n\t\t\t}\n\t\t}\n\t}\n\tblkcg_policy[pol->plid] = NULL;\nerr_unlock:\n\tmutex_unlock(&blkcg_pol_mutex);\n\tmutex_unlock(&blkcg_pol_register_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blkcg_policy_register);\n\n/**\n * blkcg_policy_unregister - unregister a blkcg policy\n * @pol: blkcg policy to unregister\n *\n * Undo blkcg_policy_register(@pol).  Might sleep.\n */\nvoid blkcg_policy_unregister(struct blkcg_policy *pol)\n{\n\tstruct blkcg *blkcg;\n\n\tmutex_lock(&blkcg_pol_register_mutex);\n\n\tif (WARN_ON(blkcg_policy[pol->plid] != pol))\n\t\tgoto out_unlock;\n\n\t/* kill the intf files first */\n\tif (pol->dfl_cftypes)\n\t\tcgroup_rm_cftypes(pol->dfl_cftypes);\n\tif (pol->legacy_cftypes)\n\t\tcgroup_rm_cftypes(pol->legacy_cftypes);\n\n\t/* remove cpds and unregister */\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tif (pol->cpd_alloc_fn) {\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\n\t\t\tif (blkcg->cpd[pol->plid]) {\n\t\t\t\tpol->cpd_free_fn(blkcg->cpd[pol->plid]);\n\t\t\t\tblkcg->cpd[pol->plid] = NULL;\n\t\t\t}\n\t\t}\n\t}\n\tblkcg_policy[pol->plid] = NULL;\n\n\tmutex_unlock(&blkcg_pol_mutex);\nout_unlock:\n\tmutex_unlock(&blkcg_pol_register_mutex);\n}\nEXPORT_SYMBOL_GPL(blkcg_policy_unregister);\n"], "fixing_code": ["/*\n * Common Block IO controller cgroup interface\n *\n * Based on ideas and code from CFQ, CFS and BFQ:\n * Copyright (C) 2003 Jens Axboe <axboe@kernel.dk>\n *\n * Copyright (C) 2008 Fabio Checconi <fabio@gandalf.sssup.it>\n *\t\t      Paolo Valente <paolo.valente@unimore.it>\n *\n * Copyright (C) 2009 Vivek Goyal <vgoyal@redhat.com>\n * \t              Nauman Rafique <nauman@google.com>\n *\n * For policy-specific per-blkcg data:\n * Copyright (C) 2015 Paolo Valente <paolo.valente@unimore.it>\n *                    Arianna Avanzini <avanzini.arianna@gmail.com>\n */\n#include <linux/ioprio.h>\n#include <linux/kdev_t.h>\n#include <linux/module.h>\n#include <linux/err.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/slab.h>\n#include <linux/genhd.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/ctype.h>\n#include <linux/blk-cgroup.h>\n#include \"blk.h\"\n\n#define MAX_KEY_LEN 100\n\n/*\n * blkcg_pol_mutex protects blkcg_policy[] and policy [de]activation.\n * blkcg_pol_register_mutex nests outside of it and synchronizes entire\n * policy [un]register operations including cgroup file additions /\n * removals.  Putting cgroup file registration outside blkcg_pol_mutex\n * allows grabbing it from cgroup callbacks.\n */\nstatic DEFINE_MUTEX(blkcg_pol_register_mutex);\nstatic DEFINE_MUTEX(blkcg_pol_mutex);\n\nstruct blkcg blkcg_root;\nEXPORT_SYMBOL_GPL(blkcg_root);\n\nstruct cgroup_subsys_state * const blkcg_root_css = &blkcg_root.css;\n\nstatic struct blkcg_policy *blkcg_policy[BLKCG_MAX_POLS];\n\nstatic LIST_HEAD(all_blkcgs);\t\t/* protected by blkcg_pol_mutex */\n\nstatic bool blkcg_policy_enabled(struct request_queue *q,\n\t\t\t\t const struct blkcg_policy *pol)\n{\n\treturn pol && test_bit(pol->plid, q->blkcg_pols);\n}\n\n/**\n * blkg_free - free a blkg\n * @blkg: blkg to free\n *\n * Free @blkg which may be partially allocated.\n */\nstatic void blkg_free(struct blkcg_gq *blkg)\n{\n\tint i;\n\n\tif (!blkg)\n\t\treturn;\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (blkg->pd[i])\n\t\t\tblkcg_policy[i]->pd_free_fn(blkg->pd[i]);\n\n\tif (blkg->blkcg != &blkcg_root)\n\t\tblk_exit_rl(&blkg->rl);\n\n\tblkg_rwstat_exit(&blkg->stat_ios);\n\tblkg_rwstat_exit(&blkg->stat_bytes);\n\tkfree(blkg);\n}\n\n/**\n * blkg_alloc - allocate a blkg\n * @blkcg: block cgroup the new blkg is associated with\n * @q: request_queue the new blkg is associated with\n * @gfp_mask: allocation mask to use\n *\n * Allocate a new blkg assocating @blkcg and @q.\n */\nstatic struct blkcg_gq *blkg_alloc(struct blkcg *blkcg, struct request_queue *q,\n\t\t\t\t   gfp_t gfp_mask)\n{\n\tstruct blkcg_gq *blkg;\n\tint i;\n\n\t/* alloc and init base part */\n\tblkg = kzalloc_node(sizeof(*blkg), gfp_mask, q->node);\n\tif (!blkg)\n\t\treturn NULL;\n\n\tif (blkg_rwstat_init(&blkg->stat_bytes, gfp_mask) ||\n\t    blkg_rwstat_init(&blkg->stat_ios, gfp_mask))\n\t\tgoto err_free;\n\n\tblkg->q = q;\n\tINIT_LIST_HEAD(&blkg->q_node);\n\tblkg->blkcg = blkcg;\n\tatomic_set(&blkg->refcnt, 1);\n\n\t/* root blkg uses @q->root_rl, init rl only for !root blkgs */\n\tif (blkcg != &blkcg_root) {\n\t\tif (blk_init_rl(&blkg->rl, q, gfp_mask))\n\t\t\tgoto err_free;\n\t\tblkg->rl.blkg = blkg;\n\t}\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\t\tstruct blkg_policy_data *pd;\n\n\t\tif (!blkcg_policy_enabled(q, pol))\n\t\t\tcontinue;\n\n\t\t/* alloc per-policy data and attach it to blkg */\n\t\tpd = pol->pd_alloc_fn(gfp_mask, q->node);\n\t\tif (!pd)\n\t\t\tgoto err_free;\n\n\t\tblkg->pd[i] = pd;\n\t\tpd->blkg = blkg;\n\t\tpd->plid = i;\n\t}\n\n\treturn blkg;\n\nerr_free:\n\tblkg_free(blkg);\n\treturn NULL;\n}\n\nstruct blkcg_gq *blkg_lookup_slowpath(struct blkcg *blkcg,\n\t\t\t\t      struct request_queue *q, bool update_hint)\n{\n\tstruct blkcg_gq *blkg;\n\n\t/*\n\t * Hint didn't match.  Look up from the radix tree.  Note that the\n\t * hint can only be updated under queue_lock as otherwise @blkg\n\t * could have already been removed from blkg_tree.  The caller is\n\t * responsible for grabbing queue_lock if @update_hint.\n\t */\n\tblkg = radix_tree_lookup(&blkcg->blkg_tree, q->id);\n\tif (blkg && blkg->q == q) {\n\t\tif (update_hint) {\n\t\t\tlockdep_assert_held(q->queue_lock);\n\t\t\trcu_assign_pointer(blkcg->blkg_hint, blkg);\n\t\t}\n\t\treturn blkg;\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(blkg_lookup_slowpath);\n\n/*\n * If @new_blkg is %NULL, this function tries to allocate a new one as\n * necessary using %GFP_NOWAIT.  @new_blkg is always consumed on return.\n */\nstatic struct blkcg_gq *blkg_create(struct blkcg *blkcg,\n\t\t\t\t    struct request_queue *q,\n\t\t\t\t    struct blkcg_gq *new_blkg)\n{\n\tstruct blkcg_gq *blkg;\n\tstruct bdi_writeback_congested *wb_congested;\n\tint i, ret;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\tlockdep_assert_held(q->queue_lock);\n\n\t/* blkg holds a reference to blkcg */\n\tif (!css_tryget_online(&blkcg->css)) {\n\t\tret = -ENODEV;\n\t\tgoto err_free_blkg;\n\t}\n\n\twb_congested = wb_congested_get_create(q->backing_dev_info,\n\t\t\t\t\t       blkcg->css.id,\n\t\t\t\t\t       GFP_NOWAIT | __GFP_NOWARN);\n\tif (!wb_congested) {\n\t\tret = -ENOMEM;\n\t\tgoto err_put_css;\n\t}\n\n\t/* allocate */\n\tif (!new_blkg) {\n\t\tnew_blkg = blkg_alloc(blkcg, q, GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (unlikely(!new_blkg)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_put_congested;\n\t\t}\n\t}\n\tblkg = new_blkg;\n\tblkg->wb_congested = wb_congested;\n\n\t/* link parent */\n\tif (blkcg_parent(blkcg)) {\n\t\tblkg->parent = __blkg_lookup(blkcg_parent(blkcg), q, false);\n\t\tif (WARN_ON_ONCE(!blkg->parent)) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto err_put_congested;\n\t\t}\n\t\tblkg_get(blkg->parent);\n\t}\n\n\t/* invoke per-policy init */\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\tif (blkg->pd[i] && pol->pd_init_fn)\n\t\t\tpol->pd_init_fn(blkg->pd[i]);\n\t}\n\n\t/* insert */\n\tspin_lock(&blkcg->lock);\n\tret = radix_tree_insert(&blkcg->blkg_tree, q->id, blkg);\n\tif (likely(!ret)) {\n\t\thlist_add_head_rcu(&blkg->blkcg_node, &blkcg->blkg_list);\n\t\tlist_add(&blkg->q_node, &q->blkg_list);\n\n\t\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\t\tif (blkg->pd[i] && pol->pd_online_fn)\n\t\t\t\tpol->pd_online_fn(blkg->pd[i]);\n\t\t}\n\t}\n\tblkg->online = true;\n\tspin_unlock(&blkcg->lock);\n\n\tif (!ret)\n\t\treturn blkg;\n\n\t/* @blkg failed fully initialized, use the usual release path */\n\tblkg_put(blkg);\n\treturn ERR_PTR(ret);\n\nerr_put_congested:\n\twb_congested_put(wb_congested);\nerr_put_css:\n\tcss_put(&blkcg->css);\nerr_free_blkg:\n\tblkg_free(new_blkg);\n\treturn ERR_PTR(ret);\n}\n\n/**\n * blkg_lookup_create - lookup blkg, try to create one if not there\n * @blkcg: blkcg of interest\n * @q: request_queue of interest\n *\n * Lookup blkg for the @blkcg - @q pair.  If it doesn't exist, try to\n * create one.  blkg creation is performed recursively from blkcg_root such\n * that all non-root blkg's have access to the parent blkg.  This function\n * should be called under RCU read lock and @q->queue_lock.\n *\n * Returns pointer to the looked up or created blkg on success, ERR_PTR()\n * value on error.  If @q is dead, returns ERR_PTR(-EINVAL).  If @q is not\n * dead and bypassing, returns ERR_PTR(-EBUSY).\n */\nstruct blkcg_gq *blkg_lookup_create(struct blkcg *blkcg,\n\t\t\t\t    struct request_queue *q)\n{\n\tstruct blkcg_gq *blkg;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\tlockdep_assert_held(q->queue_lock);\n\n\t/*\n\t * This could be the first entry point of blkcg implementation and\n\t * we shouldn't allow anything to go through for a bypassing queue.\n\t */\n\tif (unlikely(blk_queue_bypass(q)))\n\t\treturn ERR_PTR(blk_queue_dying(q) ? -ENODEV : -EBUSY);\n\n\tblkg = __blkg_lookup(blkcg, q, true);\n\tif (blkg)\n\t\treturn blkg;\n\n\t/*\n\t * Create blkgs walking down from blkcg_root to @blkcg, so that all\n\t * non-root blkgs have access to their parents.\n\t */\n\twhile (true) {\n\t\tstruct blkcg *pos = blkcg;\n\t\tstruct blkcg *parent = blkcg_parent(blkcg);\n\n\t\twhile (parent && !__blkg_lookup(parent, q, false)) {\n\t\t\tpos = parent;\n\t\t\tparent = blkcg_parent(parent);\n\t\t}\n\n\t\tblkg = blkg_create(pos, q, NULL);\n\t\tif (pos == blkcg || IS_ERR(blkg))\n\t\t\treturn blkg;\n\t}\n}\n\nstatic void blkg_destroy(struct blkcg_gq *blkg)\n{\n\tstruct blkcg *blkcg = blkg->blkcg;\n\tstruct blkcg_gq *parent = blkg->parent;\n\tint i;\n\n\tlockdep_assert_held(blkg->q->queue_lock);\n\tlockdep_assert_held(&blkcg->lock);\n\n\t/* Something wrong if we are trying to remove same group twice */\n\tWARN_ON_ONCE(list_empty(&blkg->q_node));\n\tWARN_ON_ONCE(hlist_unhashed(&blkg->blkcg_node));\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\tif (blkg->pd[i] && pol->pd_offline_fn)\n\t\t\tpol->pd_offline_fn(blkg->pd[i]);\n\t}\n\n\tif (parent) {\n\t\tblkg_rwstat_add_aux(&parent->stat_bytes, &blkg->stat_bytes);\n\t\tblkg_rwstat_add_aux(&parent->stat_ios, &blkg->stat_ios);\n\t}\n\n\tblkg->online = false;\n\n\tradix_tree_delete(&blkcg->blkg_tree, blkg->q->id);\n\tlist_del_init(&blkg->q_node);\n\thlist_del_init_rcu(&blkg->blkcg_node);\n\n\t/*\n\t * Both setting lookup hint to and clearing it from @blkg are done\n\t * under queue_lock.  If it's not pointing to @blkg now, it never\n\t * will.  Hint assignment itself can race safely.\n\t */\n\tif (rcu_access_pointer(blkcg->blkg_hint) == blkg)\n\t\trcu_assign_pointer(blkcg->blkg_hint, NULL);\n\n\t/*\n\t * Put the reference taken at the time of creation so that when all\n\t * queues are gone, group can be destroyed.\n\t */\n\tblkg_put(blkg);\n}\n\n/**\n * blkg_destroy_all - destroy all blkgs associated with a request_queue\n * @q: request_queue of interest\n *\n * Destroy all blkgs associated with @q.\n */\nstatic void blkg_destroy_all(struct request_queue *q)\n{\n\tstruct blkcg_gq *blkg, *n;\n\n\tlockdep_assert_held(q->queue_lock);\n\n\tlist_for_each_entry_safe(blkg, n, &q->blkg_list, q_node) {\n\t\tstruct blkcg *blkcg = blkg->blkcg;\n\n\t\tspin_lock(&blkcg->lock);\n\t\tblkg_destroy(blkg);\n\t\tspin_unlock(&blkcg->lock);\n\t}\n\n\tq->root_blkg = NULL;\n\tq->root_rl.blkg = NULL;\n}\n\n/*\n * A group is RCU protected, but having an rcu lock does not mean that one\n * can access all the fields of blkg and assume these are valid.  For\n * example, don't try to follow throtl_data and request queue links.\n *\n * Having a reference to blkg under an rcu allows accesses to only values\n * local to groups like group stats and group rate limits.\n */\nvoid __blkg_release_rcu(struct rcu_head *rcu_head)\n{\n\tstruct blkcg_gq *blkg = container_of(rcu_head, struct blkcg_gq, rcu_head);\n\n\t/* release the blkcg and parent blkg refs this blkg has been holding */\n\tcss_put(&blkg->blkcg->css);\n\tif (blkg->parent)\n\t\tblkg_put(blkg->parent);\n\n\twb_congested_put(blkg->wb_congested);\n\n\tblkg_free(blkg);\n}\nEXPORT_SYMBOL_GPL(__blkg_release_rcu);\n\n/*\n * The next function used by blk_queue_for_each_rl().  It's a bit tricky\n * because the root blkg uses @q->root_rl instead of its own rl.\n */\nstruct request_list *__blk_queue_next_rl(struct request_list *rl,\n\t\t\t\t\t struct request_queue *q)\n{\n\tstruct list_head *ent;\n\tstruct blkcg_gq *blkg;\n\n\t/*\n\t * Determine the current blkg list_head.  The first entry is\n\t * root_rl which is off @q->blkg_list and mapped to the head.\n\t */\n\tif (rl == &q->root_rl) {\n\t\tent = &q->blkg_list;\n\t\t/* There are no more block groups, hence no request lists */\n\t\tif (list_empty(ent))\n\t\t\treturn NULL;\n\t} else {\n\t\tblkg = container_of(rl, struct blkcg_gq, rl);\n\t\tent = &blkg->q_node;\n\t}\n\n\t/* walk to the next list_head, skip root blkcg */\n\tent = ent->next;\n\tif (ent == &q->root_blkg->q_node)\n\t\tent = ent->next;\n\tif (ent == &q->blkg_list)\n\t\treturn NULL;\n\n\tblkg = container_of(ent, struct blkcg_gq, q_node);\n\treturn &blkg->rl;\n}\n\nstatic int blkcg_reset_stats(struct cgroup_subsys_state *css,\n\t\t\t     struct cftype *cftype, u64 val)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(css);\n\tstruct blkcg_gq *blkg;\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\tspin_lock_irq(&blkcg->lock);\n\n\t/*\n\t * Note that stat reset is racy - it doesn't synchronize against\n\t * stat updates.  This is a debug feature which shouldn't exist\n\t * anyway.  If you get hit by a race, retry.\n\t */\n\thlist_for_each_entry(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tblkg_rwstat_reset(&blkg->stat_bytes);\n\t\tblkg_rwstat_reset(&blkg->stat_ios);\n\n\t\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\t\tif (blkg->pd[i] && pol->pd_reset_stats_fn)\n\t\t\t\tpol->pd_reset_stats_fn(blkg->pd[i]);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&blkcg->lock);\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn 0;\n}\n\nconst char *blkg_dev_name(struct blkcg_gq *blkg)\n{\n\t/* some drivers (floppy) instantiate a queue w/o disk registered */\n\tif (blkg->q->backing_dev_info->dev)\n\t\treturn dev_name(blkg->q->backing_dev_info->dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(blkg_dev_name);\n\n/**\n * blkcg_print_blkgs - helper for printing per-blkg data\n * @sf: seq_file to print to\n * @blkcg: blkcg of interest\n * @prfill: fill function to print out a blkg\n * @pol: policy in question\n * @data: data to be passed to @prfill\n * @show_total: to print out sum of prfill return values or not\n *\n * This function invokes @prfill on each blkg of @blkcg if pd for the\n * policy specified by @pol exists.  @prfill is invoked with @sf, the\n * policy data and @data and the matching queue lock held.  If @show_total\n * is %true, the sum of the return values from @prfill is printed with\n * \"Total\" label at the end.\n *\n * This is to be used to construct print functions for\n * cftype->read_seq_string method.\n */\nvoid blkcg_print_blkgs(struct seq_file *sf, struct blkcg *blkcg,\n\t\t       u64 (*prfill)(struct seq_file *,\n\t\t\t\t     struct blkg_policy_data *, int),\n\t\t       const struct blkcg_policy *pol, int data,\n\t\t       bool show_total)\n{\n\tstruct blkcg_gq *blkg;\n\tu64 total = 0;\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tspin_lock_irq(blkg->q->queue_lock);\n\t\tif (blkcg_policy_enabled(blkg->q, pol))\n\t\t\ttotal += prfill(sf, blkg->pd[pol->plid], data);\n\t\tspin_unlock_irq(blkg->q->queue_lock);\n\t}\n\trcu_read_unlock();\n\n\tif (show_total)\n\t\tseq_printf(sf, \"Total %llu\\n\", (unsigned long long)total);\n}\nEXPORT_SYMBOL_GPL(blkcg_print_blkgs);\n\n/**\n * __blkg_prfill_u64 - prfill helper for a single u64 value\n * @sf: seq_file to print to\n * @pd: policy private data of interest\n * @v: value to print\n *\n * Print @v to @sf for the device assocaited with @pd.\n */\nu64 __blkg_prfill_u64(struct seq_file *sf, struct blkg_policy_data *pd, u64 v)\n{\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\n\tif (!dname)\n\t\treturn 0;\n\n\tseq_printf(sf, \"%s %llu\\n\", dname, (unsigned long long)v);\n\treturn v;\n}\nEXPORT_SYMBOL_GPL(__blkg_prfill_u64);\n\n/**\n * __blkg_prfill_rwstat - prfill helper for a blkg_rwstat\n * @sf: seq_file to print to\n * @pd: policy private data of interest\n * @rwstat: rwstat to print\n *\n * Print @rwstat to @sf for the device assocaited with @pd.\n */\nu64 __blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t\t const struct blkg_rwstat *rwstat)\n{\n\tstatic const char *rwstr[] = {\n\t\t[BLKG_RWSTAT_READ]\t= \"Read\",\n\t\t[BLKG_RWSTAT_WRITE]\t= \"Write\",\n\t\t[BLKG_RWSTAT_SYNC]\t= \"Sync\",\n\t\t[BLKG_RWSTAT_ASYNC]\t= \"Async\",\n\t};\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\tu64 v;\n\tint i;\n\n\tif (!dname)\n\t\treturn 0;\n\n\tfor (i = 0; i < BLKG_RWSTAT_NR; i++)\n\t\tseq_printf(sf, \"%s %s %llu\\n\", dname, rwstr[i],\n\t\t\t   (unsigned long long)atomic64_read(&rwstat->aux_cnt[i]));\n\n\tv = atomic64_read(&rwstat->aux_cnt[BLKG_RWSTAT_READ]) +\n\t\tatomic64_read(&rwstat->aux_cnt[BLKG_RWSTAT_WRITE]);\n\tseq_printf(sf, \"%s Total %llu\\n\", dname, (unsigned long long)v);\n\treturn v;\n}\nEXPORT_SYMBOL_GPL(__blkg_prfill_rwstat);\n\n/**\n * blkg_prfill_stat - prfill callback for blkg_stat\n * @sf: seq_file to print to\n * @pd: policy private data of interest\n * @off: offset to the blkg_stat in @pd\n *\n * prfill callback for printing a blkg_stat.\n */\nu64 blkg_prfill_stat(struct seq_file *sf, struct blkg_policy_data *pd, int off)\n{\n\treturn __blkg_prfill_u64(sf, pd, blkg_stat_read((void *)pd + off));\n}\nEXPORT_SYMBOL_GPL(blkg_prfill_stat);\n\n/**\n * blkg_prfill_rwstat - prfill callback for blkg_rwstat\n * @sf: seq_file to print to\n * @pd: policy private data of interest\n * @off: offset to the blkg_rwstat in @pd\n *\n * prfill callback for printing a blkg_rwstat.\n */\nu64 blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t       int off)\n{\n\tstruct blkg_rwstat rwstat = blkg_rwstat_read((void *)pd + off);\n\n\treturn __blkg_prfill_rwstat(sf, pd, &rwstat);\n}\nEXPORT_SYMBOL_GPL(blkg_prfill_rwstat);\n\nstatic u64 blkg_prfill_rwstat_field(struct seq_file *sf,\n\t\t\t\t    struct blkg_policy_data *pd, int off)\n{\n\tstruct blkg_rwstat rwstat = blkg_rwstat_read((void *)pd->blkg + off);\n\n\treturn __blkg_prfill_rwstat(sf, pd, &rwstat);\n}\n\n/**\n * blkg_print_stat_bytes - seq_show callback for blkg->stat_bytes\n * @sf: seq_file to print to\n * @v: unused\n *\n * To be used as cftype->seq_show to print blkg->stat_bytes.\n * cftype->private must be set to the blkcg_policy.\n */\nint blkg_print_stat_bytes(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat_field, (void *)seq_cft(sf)->private,\n\t\t\t  offsetof(struct blkcg_gq, stat_bytes), true);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_print_stat_bytes);\n\n/**\n * blkg_print_stat_bytes - seq_show callback for blkg->stat_ios\n * @sf: seq_file to print to\n * @v: unused\n *\n * To be used as cftype->seq_show to print blkg->stat_ios.  cftype->private\n * must be set to the blkcg_policy.\n */\nint blkg_print_stat_ios(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat_field, (void *)seq_cft(sf)->private,\n\t\t\t  offsetof(struct blkcg_gq, stat_ios), true);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_print_stat_ios);\n\nstatic u64 blkg_prfill_rwstat_field_recursive(struct seq_file *sf,\n\t\t\t\t\t      struct blkg_policy_data *pd,\n\t\t\t\t\t      int off)\n{\n\tstruct blkg_rwstat rwstat = blkg_rwstat_recursive_sum(pd->blkg,\n\t\t\t\t\t\t\t      NULL, off);\n\treturn __blkg_prfill_rwstat(sf, pd, &rwstat);\n}\n\n/**\n * blkg_print_stat_bytes_recursive - recursive version of blkg_print_stat_bytes\n * @sf: seq_file to print to\n * @v: unused\n */\nint blkg_print_stat_bytes_recursive(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat_field_recursive,\n\t\t\t  (void *)seq_cft(sf)->private,\n\t\t\t  offsetof(struct blkcg_gq, stat_bytes), true);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_print_stat_bytes_recursive);\n\n/**\n * blkg_print_stat_ios_recursive - recursive version of blkg_print_stat_ios\n * @sf: seq_file to print to\n * @v: unused\n */\nint blkg_print_stat_ios_recursive(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat_field_recursive,\n\t\t\t  (void *)seq_cft(sf)->private,\n\t\t\t  offsetof(struct blkcg_gq, stat_ios), true);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_print_stat_ios_recursive);\n\n/**\n * blkg_stat_recursive_sum - collect hierarchical blkg_stat\n * @blkg: blkg of interest\n * @pol: blkcg_policy which contains the blkg_stat\n * @off: offset to the blkg_stat in blkg_policy_data or @blkg\n *\n * Collect the blkg_stat specified by @blkg, @pol and @off and all its\n * online descendants and their aux counts.  The caller must be holding the\n * queue lock for online tests.\n *\n * If @pol is NULL, blkg_stat is at @off bytes into @blkg; otherwise, it is\n * at @off bytes into @blkg's blkg_policy_data of the policy.\n */\nu64 blkg_stat_recursive_sum(struct blkcg_gq *blkg,\n\t\t\t    struct blkcg_policy *pol, int off)\n{\n\tstruct blkcg_gq *pos_blkg;\n\tstruct cgroup_subsys_state *pos_css;\n\tu64 sum = 0;\n\n\tlockdep_assert_held(blkg->q->queue_lock);\n\n\trcu_read_lock();\n\tblkg_for_each_descendant_pre(pos_blkg, pos_css, blkg) {\n\t\tstruct blkg_stat *stat;\n\n\t\tif (!pos_blkg->online)\n\t\t\tcontinue;\n\n\t\tif (pol)\n\t\t\tstat = (void *)blkg_to_pd(pos_blkg, pol) + off;\n\t\telse\n\t\t\tstat = (void *)blkg + off;\n\n\t\tsum += blkg_stat_read(stat) + atomic64_read(&stat->aux_cnt);\n\t}\n\trcu_read_unlock();\n\n\treturn sum;\n}\nEXPORT_SYMBOL_GPL(blkg_stat_recursive_sum);\n\n/**\n * blkg_rwstat_recursive_sum - collect hierarchical blkg_rwstat\n * @blkg: blkg of interest\n * @pol: blkcg_policy which contains the blkg_rwstat\n * @off: offset to the blkg_rwstat in blkg_policy_data or @blkg\n *\n * Collect the blkg_rwstat specified by @blkg, @pol and @off and all its\n * online descendants and their aux counts.  The caller must be holding the\n * queue lock for online tests.\n *\n * If @pol is NULL, blkg_rwstat is at @off bytes into @blkg; otherwise, it\n * is at @off bytes into @blkg's blkg_policy_data of the policy.\n */\nstruct blkg_rwstat blkg_rwstat_recursive_sum(struct blkcg_gq *blkg,\n\t\t\t\t\t     struct blkcg_policy *pol, int off)\n{\n\tstruct blkcg_gq *pos_blkg;\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct blkg_rwstat sum = { };\n\tint i;\n\n\tlockdep_assert_held(blkg->q->queue_lock);\n\n\trcu_read_lock();\n\tblkg_for_each_descendant_pre(pos_blkg, pos_css, blkg) {\n\t\tstruct blkg_rwstat *rwstat;\n\n\t\tif (!pos_blkg->online)\n\t\t\tcontinue;\n\n\t\tif (pol)\n\t\t\trwstat = (void *)blkg_to_pd(pos_blkg, pol) + off;\n\t\telse\n\t\t\trwstat = (void *)pos_blkg + off;\n\n\t\tfor (i = 0; i < BLKG_RWSTAT_NR; i++)\n\t\t\tatomic64_add(atomic64_read(&rwstat->aux_cnt[i]) +\n\t\t\t\tpercpu_counter_sum_positive(&rwstat->cpu_cnt[i]),\n\t\t\t\t&sum.aux_cnt[i]);\n\t}\n\trcu_read_unlock();\n\n\treturn sum;\n}\nEXPORT_SYMBOL_GPL(blkg_rwstat_recursive_sum);\n\n/**\n * blkg_conf_prep - parse and prepare for per-blkg config update\n * @blkcg: target block cgroup\n * @pol: target policy\n * @input: input string\n * @ctx: blkg_conf_ctx to be filled\n *\n * Parse per-blkg config update from @input and initialize @ctx with the\n * result.  @ctx->blkg points to the blkg to be updated and @ctx->body the\n * part of @input following MAJ:MIN.  This function returns with RCU read\n * lock and queue lock held and must be paired with blkg_conf_finish().\n */\nint blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,\n\t\t   char *input, struct blkg_conf_ctx *ctx)\n\t__acquires(rcu) __acquires(disk->queue->queue_lock)\n{\n\tstruct gendisk *disk;\n\tstruct blkcg_gq *blkg;\n\tstruct module *owner;\n\tunsigned int major, minor;\n\tint key_len, part, ret;\n\tchar *body;\n\n\tif (sscanf(input, \"%u:%u%n\", &major, &minor, &key_len) != 2)\n\t\treturn -EINVAL;\n\n\tbody = input + key_len;\n\tif (!isspace(*body))\n\t\treturn -EINVAL;\n\tbody = skip_spaces(body);\n\n\tdisk = get_gendisk(MKDEV(major, minor), &part);\n\tif (!disk)\n\t\treturn -ENODEV;\n\tif (part) {\n\t\towner = disk->fops->owner;\n\t\tput_disk(disk);\n\t\tmodule_put(owner);\n\t\treturn -ENODEV;\n\t}\n\n\trcu_read_lock();\n\tspin_lock_irq(disk->queue->queue_lock);\n\n\tif (blkcg_policy_enabled(disk->queue, pol))\n\t\tblkg = blkg_lookup_create(blkcg, disk->queue);\n\telse\n\t\tblkg = ERR_PTR(-EOPNOTSUPP);\n\n\tif (IS_ERR(blkg)) {\n\t\tret = PTR_ERR(blkg);\n\t\trcu_read_unlock();\n\t\tspin_unlock_irq(disk->queue->queue_lock);\n\t\towner = disk->fops->owner;\n\t\tput_disk(disk);\n\t\tmodule_put(owner);\n\t\t/*\n\t\t * If queue was bypassing, we should retry.  Do so after a\n\t\t * short msleep().  It isn't strictly necessary but queue\n\t\t * can be bypassing for some time and it's always nice to\n\t\t * avoid busy looping.\n\t\t */\n\t\tif (ret == -EBUSY) {\n\t\t\tmsleep(10);\n\t\t\tret = restart_syscall();\n\t\t}\n\t\treturn ret;\n\t}\n\n\tctx->disk = disk;\n\tctx->blkg = blkg;\n\tctx->body = body;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blkg_conf_prep);\n\n/**\n * blkg_conf_finish - finish up per-blkg config update\n * @ctx: blkg_conf_ctx intiailized by blkg_conf_prep()\n *\n * Finish up after per-blkg config update.  This function must be paired\n * with blkg_conf_prep().\n */\nvoid blkg_conf_finish(struct blkg_conf_ctx *ctx)\n\t__releases(ctx->disk->queue->queue_lock) __releases(rcu)\n{\n\tstruct module *owner;\n\n\tspin_unlock_irq(ctx->disk->queue->queue_lock);\n\trcu_read_unlock();\n\towner = ctx->disk->fops->owner;\n\tput_disk(ctx->disk);\n\tmodule_put(owner);\n}\nEXPORT_SYMBOL_GPL(blkg_conf_finish);\n\nstatic int blkcg_print_stat(struct seq_file *sf, void *v)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\n\tstruct blkcg_gq *blkg;\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tconst char *dname;\n\t\tstruct blkg_rwstat rwstat;\n\t\tu64 rbytes, wbytes, rios, wios;\n\n\t\tdname = blkg_dev_name(blkg);\n\t\tif (!dname)\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(blkg->q->queue_lock);\n\n\t\trwstat = blkg_rwstat_recursive_sum(blkg, NULL,\n\t\t\t\t\toffsetof(struct blkcg_gq, stat_bytes));\n\t\trbytes = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_READ]);\n\t\twbytes = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_WRITE]);\n\n\t\trwstat = blkg_rwstat_recursive_sum(blkg, NULL,\n\t\t\t\t\toffsetof(struct blkcg_gq, stat_ios));\n\t\trios = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_READ]);\n\t\twios = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_WRITE]);\n\n\t\tspin_unlock_irq(blkg->q->queue_lock);\n\n\t\tif (rbytes || wbytes || rios || wios)\n\t\t\tseq_printf(sf, \"%s rbytes=%llu wbytes=%llu rios=%llu wios=%llu\\n\",\n\t\t\t\t   dname, rbytes, wbytes, rios, wios);\n\t}\n\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic struct cftype blkcg_files[] = {\n\t{\n\t\t.name = \"stat\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = blkcg_print_stat,\n\t},\n\t{ }\t/* terminate */\n};\n\nstatic struct cftype blkcg_legacy_files[] = {\n\t{\n\t\t.name = \"reset_stats\",\n\t\t.write_u64 = blkcg_reset_stats,\n\t},\n\t{ }\t/* terminate */\n};\n\n/**\n * blkcg_css_offline - cgroup css_offline callback\n * @css: css of interest\n *\n * This function is called when @css is about to go away and responsible\n * for shooting down all blkgs associated with @css.  blkgs should be\n * removed while holding both q and blkcg locks.  As blkcg lock is nested\n * inside q lock, this function performs reverse double lock dancing.\n *\n * This is the blkcg counterpart of ioc_release_fn().\n */\nstatic void blkcg_css_offline(struct cgroup_subsys_state *css)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(css);\n\n\tspin_lock_irq(&blkcg->lock);\n\n\twhile (!hlist_empty(&blkcg->blkg_list)) {\n\t\tstruct blkcg_gq *blkg = hlist_entry(blkcg->blkg_list.first,\n\t\t\t\t\t\tstruct blkcg_gq, blkcg_node);\n\t\tstruct request_queue *q = blkg->q;\n\n\t\tif (spin_trylock(q->queue_lock)) {\n\t\t\tblkg_destroy(blkg);\n\t\t\tspin_unlock(q->queue_lock);\n\t\t} else {\n\t\t\tspin_unlock_irq(&blkcg->lock);\n\t\t\tcpu_relax();\n\t\t\tspin_lock_irq(&blkcg->lock);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&blkcg->lock);\n\n\twb_blkcg_offline(blkcg);\n}\n\nstatic void blkcg_css_free(struct cgroup_subsys_state *css)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(css);\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tlist_del(&blkcg->all_blkcgs_node);\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (blkcg->cpd[i])\n\t\t\tblkcg_policy[i]->cpd_free_fn(blkcg->cpd[i]);\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\n\tkfree(blkcg);\n}\n\nstatic struct cgroup_subsys_state *\nblkcg_css_alloc(struct cgroup_subsys_state *parent_css)\n{\n\tstruct blkcg *blkcg;\n\tstruct cgroup_subsys_state *ret;\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tif (!parent_css) {\n\t\tblkcg = &blkcg_root;\n\t} else {\n\t\tblkcg = kzalloc(sizeof(*blkcg), GFP_KERNEL);\n\t\tif (!blkcg) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto free_blkcg;\n\t\t}\n\t}\n\n\tfor (i = 0; i < BLKCG_MAX_POLS ; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\t\tstruct blkcg_policy_data *cpd;\n\n\t\t/*\n\t\t * If the policy hasn't been attached yet, wait for it\n\t\t * to be attached before doing anything else. Otherwise,\n\t\t * check if the policy requires any specific per-cgroup\n\t\t * data: if it does, allocate and initialize it.\n\t\t */\n\t\tif (!pol || !pol->cpd_alloc_fn)\n\t\t\tcontinue;\n\n\t\tcpd = pol->cpd_alloc_fn(GFP_KERNEL);\n\t\tif (!cpd) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto free_pd_blkcg;\n\t\t}\n\t\tblkcg->cpd[i] = cpd;\n\t\tcpd->blkcg = blkcg;\n\t\tcpd->plid = i;\n\t\tif (pol->cpd_init_fn)\n\t\t\tpol->cpd_init_fn(cpd);\n\t}\n\n\tspin_lock_init(&blkcg->lock);\n\tINIT_RADIX_TREE(&blkcg->blkg_tree, GFP_NOWAIT | __GFP_NOWARN);\n\tINIT_HLIST_HEAD(&blkcg->blkg_list);\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tINIT_LIST_HEAD(&blkcg->cgwb_list);\n#endif\n\tlist_add_tail(&blkcg->all_blkcgs_node, &all_blkcgs);\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn &blkcg->css;\n\nfree_pd_blkcg:\n\tfor (i--; i >= 0; i--)\n\t\tif (blkcg->cpd[i])\n\t\t\tblkcg_policy[i]->cpd_free_fn(blkcg->cpd[i]);\nfree_blkcg:\n\tkfree(blkcg);\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn ret;\n}\n\n/**\n * blkcg_init_queue - initialize blkcg part of request queue\n * @q: request_queue to initialize\n *\n * Called from blk_alloc_queue_node(). Responsible for initializing blkcg\n * part of new request_queue @q.\n *\n * RETURNS:\n * 0 on success, -errno on failure.\n */\nint blkcg_init_queue(struct request_queue *q)\n{\n\tstruct blkcg_gq *new_blkg, *blkg;\n\tbool preloaded;\n\tint ret;\n\n\tnew_blkg = blkg_alloc(&blkcg_root, q, GFP_KERNEL);\n\tif (!new_blkg)\n\t\treturn -ENOMEM;\n\n\tpreloaded = !radix_tree_preload(GFP_KERNEL);\n\n\t/*\n\t * Make sure the root blkg exists and count the existing blkgs.  As\n\t * @q is bypassing at this point, blkg_lookup_create() can't be\n\t * used.  Open code insertion.\n\t */\n\trcu_read_lock();\n\tspin_lock_irq(q->queue_lock);\n\tblkg = blkg_create(&blkcg_root, q, new_blkg);\n\tspin_unlock_irq(q->queue_lock);\n\trcu_read_unlock();\n\n\tif (preloaded)\n\t\tradix_tree_preload_end();\n\n\tif (IS_ERR(blkg))\n\t\treturn PTR_ERR(blkg);\n\n\tq->root_blkg = blkg;\n\tq->root_rl.blkg = blkg;\n\n\tret = blk_throtl_init(q);\n\tif (ret) {\n\t\tspin_lock_irq(q->queue_lock);\n\t\tblkg_destroy_all(q);\n\t\tspin_unlock_irq(q->queue_lock);\n\t}\n\treturn ret;\n}\n\n/**\n * blkcg_drain_queue - drain blkcg part of request_queue\n * @q: request_queue to drain\n *\n * Called from blk_drain_queue().  Responsible for draining blkcg part.\n */\nvoid blkcg_drain_queue(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\n\t/*\n\t * @q could be exiting and already have destroyed all blkgs as\n\t * indicated by NULL root_blkg.  If so, don't confuse policies.\n\t */\n\tif (!q->root_blkg)\n\t\treturn;\n\n\tblk_throtl_drain(q);\n}\n\n/**\n * blkcg_exit_queue - exit and release blkcg part of request_queue\n * @q: request_queue being released\n *\n * Called from blk_release_queue().  Responsible for exiting blkcg part.\n */\nvoid blkcg_exit_queue(struct request_queue *q)\n{\n\tspin_lock_irq(q->queue_lock);\n\tblkg_destroy_all(q);\n\tspin_unlock_irq(q->queue_lock);\n\n\tblk_throtl_exit(q);\n}\n\n/*\n * We cannot support shared io contexts, as we have no mean to support\n * two tasks with the same ioc in two different groups without major rework\n * of the main cic data structures.  For now we allow a task to change\n * its cgroup only if it's the only owner of its ioc.\n */\nstatic int blkcg_can_attach(struct cgroup_taskset *tset)\n{\n\tstruct task_struct *task;\n\tstruct cgroup_subsys_state *dst_css;\n\tstruct io_context *ioc;\n\tint ret = 0;\n\n\t/* task_lock() is needed to avoid races with exit_io_context() */\n\tcgroup_taskset_for_each(task, dst_css, tset) {\n\t\ttask_lock(task);\n\t\tioc = task->io_context;\n\t\tif (ioc && atomic_read(&ioc->nr_tasks) > 1)\n\t\t\tret = -EINVAL;\n\t\ttask_unlock(task);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic void blkcg_bind(struct cgroup_subsys_state *root_css)\n{\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\t\tstruct blkcg *blkcg;\n\n\t\tif (!pol || !pol->cpd_bind_fn)\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node)\n\t\t\tif (blkcg->cpd[pol->plid])\n\t\t\t\tpol->cpd_bind_fn(blkcg->cpd[pol->plid]);\n\t}\n\tmutex_unlock(&blkcg_pol_mutex);\n}\n\nstruct cgroup_subsys io_cgrp_subsys = {\n\t.css_alloc = blkcg_css_alloc,\n\t.css_offline = blkcg_css_offline,\n\t.css_free = blkcg_css_free,\n\t.can_attach = blkcg_can_attach,\n\t.bind = blkcg_bind,\n\t.dfl_cftypes = blkcg_files,\n\t.legacy_cftypes = blkcg_legacy_files,\n\t.legacy_name = \"blkio\",\n#ifdef CONFIG_MEMCG\n\t/*\n\t * This ensures that, if available, memcg is automatically enabled\n\t * together on the default hierarchy so that the owner cgroup can\n\t * be retrieved from writeback pages.\n\t */\n\t.depends_on = 1 << memory_cgrp_id,\n#endif\n};\nEXPORT_SYMBOL_GPL(io_cgrp_subsys);\n\n/**\n * blkcg_activate_policy - activate a blkcg policy on a request_queue\n * @q: request_queue of interest\n * @pol: blkcg policy to activate\n *\n * Activate @pol on @q.  Requires %GFP_KERNEL context.  @q goes through\n * bypass mode to populate its blkgs with policy_data for @pol.\n *\n * Activation happens with @q bypassed, so nobody would be accessing blkgs\n * from IO path.  Update of each blkg is protected by both queue and blkcg\n * locks so that holding either lock and testing blkcg_policy_enabled() is\n * always enough for dereferencing policy data.\n *\n * The caller is responsible for synchronizing [de]activations and policy\n * [un]registerations.  Returns 0 on success, -errno on failure.\n */\nint blkcg_activate_policy(struct request_queue *q,\n\t\t\t  const struct blkcg_policy *pol)\n{\n\tstruct blkg_policy_data *pd_prealloc = NULL;\n\tstruct blkcg_gq *blkg;\n\tint ret;\n\n\tif (blkcg_policy_enabled(q, pol))\n\t\treturn 0;\n\n\tif (q->mq_ops)\n\t\tblk_mq_freeze_queue(q);\n\telse\n\t\tblk_queue_bypass_start(q);\npd_prealloc:\n\tif (!pd_prealloc) {\n\t\tpd_prealloc = pol->pd_alloc_fn(GFP_KERNEL, q->node);\n\t\tif (!pd_prealloc) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_bypass_end;\n\t\t}\n\t}\n\n\tspin_lock_irq(q->queue_lock);\n\n\tlist_for_each_entry(blkg, &q->blkg_list, q_node) {\n\t\tstruct blkg_policy_data *pd;\n\n\t\tif (blkg->pd[pol->plid])\n\t\t\tcontinue;\n\n\t\tpd = pol->pd_alloc_fn(GFP_NOWAIT | __GFP_NOWARN, q->node);\n\t\tif (!pd)\n\t\t\tswap(pd, pd_prealloc);\n\t\tif (!pd) {\n\t\t\tspin_unlock_irq(q->queue_lock);\n\t\t\tgoto pd_prealloc;\n\t\t}\n\n\t\tblkg->pd[pol->plid] = pd;\n\t\tpd->blkg = blkg;\n\t\tpd->plid = pol->plid;\n\t\tif (pol->pd_init_fn)\n\t\t\tpol->pd_init_fn(pd);\n\t}\n\n\t__set_bit(pol->plid, q->blkcg_pols);\n\tret = 0;\n\n\tspin_unlock_irq(q->queue_lock);\nout_bypass_end:\n\tif (q->mq_ops)\n\t\tblk_mq_unfreeze_queue(q);\n\telse\n\t\tblk_queue_bypass_end(q);\n\tif (pd_prealloc)\n\t\tpol->pd_free_fn(pd_prealloc);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blkcg_activate_policy);\n\n/**\n * blkcg_deactivate_policy - deactivate a blkcg policy on a request_queue\n * @q: request_queue of interest\n * @pol: blkcg policy to deactivate\n *\n * Deactivate @pol on @q.  Follows the same synchronization rules as\n * blkcg_activate_policy().\n */\nvoid blkcg_deactivate_policy(struct request_queue *q,\n\t\t\t     const struct blkcg_policy *pol)\n{\n\tstruct blkcg_gq *blkg;\n\n\tif (!blkcg_policy_enabled(q, pol))\n\t\treturn;\n\n\tif (q->mq_ops)\n\t\tblk_mq_freeze_queue(q);\n\telse\n\t\tblk_queue_bypass_start(q);\n\n\tspin_lock_irq(q->queue_lock);\n\n\t__clear_bit(pol->plid, q->blkcg_pols);\n\n\tlist_for_each_entry(blkg, &q->blkg_list, q_node) {\n\t\t/* grab blkcg lock too while removing @pd from @blkg */\n\t\tspin_lock(&blkg->blkcg->lock);\n\n\t\tif (blkg->pd[pol->plid]) {\n\t\t\tif (pol->pd_offline_fn)\n\t\t\t\tpol->pd_offline_fn(blkg->pd[pol->plid]);\n\t\t\tpol->pd_free_fn(blkg->pd[pol->plid]);\n\t\t\tblkg->pd[pol->plid] = NULL;\n\t\t}\n\n\t\tspin_unlock(&blkg->blkcg->lock);\n\t}\n\n\tspin_unlock_irq(q->queue_lock);\n\n\tif (q->mq_ops)\n\t\tblk_mq_unfreeze_queue(q);\n\telse\n\t\tblk_queue_bypass_end(q);\n}\nEXPORT_SYMBOL_GPL(blkcg_deactivate_policy);\n\n/**\n * blkcg_policy_register - register a blkcg policy\n * @pol: blkcg policy to register\n *\n * Register @pol with blkcg core.  Might sleep and @pol may be modified on\n * successful registration.  Returns 0 on success and -errno on failure.\n */\nint blkcg_policy_register(struct blkcg_policy *pol)\n{\n\tstruct blkcg *blkcg;\n\tint i, ret;\n\n\tmutex_lock(&blkcg_pol_register_mutex);\n\tmutex_lock(&blkcg_pol_mutex);\n\n\t/* find an empty slot */\n\tret = -ENOSPC;\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (!blkcg_policy[i])\n\t\t\tbreak;\n\tif (i >= BLKCG_MAX_POLS)\n\t\tgoto err_unlock;\n\n\t/* register @pol */\n\tpol->plid = i;\n\tblkcg_policy[pol->plid] = pol;\n\n\t/* allocate and install cpd's */\n\tif (pol->cpd_alloc_fn) {\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\n\t\t\tstruct blkcg_policy_data *cpd;\n\n\t\t\tcpd = pol->cpd_alloc_fn(GFP_KERNEL);\n\t\t\tif (!cpd)\n\t\t\t\tgoto err_free_cpds;\n\n\t\t\tblkcg->cpd[pol->plid] = cpd;\n\t\t\tcpd->blkcg = blkcg;\n\t\t\tcpd->plid = pol->plid;\n\t\t\tpol->cpd_init_fn(cpd);\n\t\t}\n\t}\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\n\t/* everything is in place, add intf files for the new policy */\n\tif (pol->dfl_cftypes)\n\t\tWARN_ON(cgroup_add_dfl_cftypes(&io_cgrp_subsys,\n\t\t\t\t\t       pol->dfl_cftypes));\n\tif (pol->legacy_cftypes)\n\t\tWARN_ON(cgroup_add_legacy_cftypes(&io_cgrp_subsys,\n\t\t\t\t\t\t  pol->legacy_cftypes));\n\tmutex_unlock(&blkcg_pol_register_mutex);\n\treturn 0;\n\nerr_free_cpds:\n\tif (pol->cpd_alloc_fn) {\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\n\t\t\tif (blkcg->cpd[pol->plid]) {\n\t\t\t\tpol->cpd_free_fn(blkcg->cpd[pol->plid]);\n\t\t\t\tblkcg->cpd[pol->plid] = NULL;\n\t\t\t}\n\t\t}\n\t}\n\tblkcg_policy[pol->plid] = NULL;\nerr_unlock:\n\tmutex_unlock(&blkcg_pol_mutex);\n\tmutex_unlock(&blkcg_pol_register_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blkcg_policy_register);\n\n/**\n * blkcg_policy_unregister - unregister a blkcg policy\n * @pol: blkcg policy to unregister\n *\n * Undo blkcg_policy_register(@pol).  Might sleep.\n */\nvoid blkcg_policy_unregister(struct blkcg_policy *pol)\n{\n\tstruct blkcg *blkcg;\n\n\tmutex_lock(&blkcg_pol_register_mutex);\n\n\tif (WARN_ON(blkcg_policy[pol->plid] != pol))\n\t\tgoto out_unlock;\n\n\t/* kill the intf files first */\n\tif (pol->dfl_cftypes)\n\t\tcgroup_rm_cftypes(pol->dfl_cftypes);\n\tif (pol->legacy_cftypes)\n\t\tcgroup_rm_cftypes(pol->legacy_cftypes);\n\n\t/* remove cpds and unregister */\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tif (pol->cpd_alloc_fn) {\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\n\t\t\tif (blkcg->cpd[pol->plid]) {\n\t\t\t\tpol->cpd_free_fn(blkcg->cpd[pol->plid]);\n\t\t\t\tblkcg->cpd[pol->plid] = NULL;\n\t\t\t}\n\t\t}\n\t}\n\tblkcg_policy[pol->plid] = NULL;\n\n\tmutex_unlock(&blkcg_pol_mutex);\nout_unlock:\n\tmutex_unlock(&blkcg_pol_register_mutex);\n}\nEXPORT_SYMBOL_GPL(blkcg_policy_unregister);\n"], "filenames": ["block/blk-cgroup.c"], "buggy_code_start_loc": [1082], "buggy_code_end_loc": [1086], "fixing_code_start_loc": [1082], "fixing_code_end_loc": [1083], "type": "CWE-415", "message": "The blkcg_init_queue function in block/blk-cgroup.c in the Linux kernel before 4.11 allows local users to cause a denial of service (double free) or possibly have unspecified other impact by triggering a creation failure.", "other": {"cve": {"id": "CVE-2018-7480", "sourceIdentifier": "cve@mitre.org", "published": "2018-02-25T20:29:00.217", "lastModified": "2023-02-24T18:33:17.673", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The blkcg_init_queue function in block/blk-cgroup.c in the Linux kernel before 4.11 allows local users to cause a denial of service (double free) or possibly have unspecified other impact by triggering a creation failure."}, {"lang": "es", "value": "La funci\u00f3n blkcg_init_queue en block/blk-cgroup.c en el kernel de Linux, en versiones anteriores a la 4.11, permite que los usuarios locales provoquen una denegaci\u00f3n de servicio (doble liberaci\u00f3n) o, posiblemente, causen otros impactos no especificados desencadenando un fallo de creaci\u00f3n."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-415"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.1.41", "versionEndExcluding": "4.1.51", "matchCriteriaId": "DA03C683-29ED-4558-9DBB-5B68A64C6CF5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.3", "versionEndExcluding": "4.4.123", "matchCriteriaId": "DB409D6C-E41F-4783-BE9B-963DBB96B6B6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.89", "matchCriteriaId": "69FDF1F8-791A-40C2-8951-0622B6E8666F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.11", "matchCriteriaId": "28FFE753-2608-40BE-A218-483B3D8C0241"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=9b54d816e00425c3a517514e0d677bb3cec49258", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/9b54d816e00425c3a517514e0d677bb3cec49258", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://usn.ubuntu.com/3654-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3654-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3656-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4188", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/9b54d816e00425c3a517514e0d677bb3cec49258"}}