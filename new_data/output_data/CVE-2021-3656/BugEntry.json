{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * AMD SVM support\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *   Avi Kivity   <avi@qumranet.com>\n */\n\n#define pr_fmt(fmt) \"SVM: \" fmt\n\n#include <linux/kvm_types.h>\n#include <linux/kvm_host.h>\n#include <linux/kernel.h>\n\n#include <asm/msr-index.h>\n#include <asm/debugreg.h>\n\n#include \"kvm_emulate.h\"\n#include \"trace.h\"\n#include \"mmu.h\"\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"lapic.h\"\n#include \"svm.h\"\n\n#define CC KVM_NESTED_VMENTER_CONSISTENCY_CHECK\n\nstatic void nested_svm_inject_npf_exit(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct x86_exception *fault)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (svm->vmcb->control.exit_code != SVM_EXIT_NPF) {\n\t\t/*\n\t\t * TODO: track the cause of the nested page fault, and\n\t\t * correctly fill in the high bits of exit_info_1.\n\t\t */\n\t\tsvm->vmcb->control.exit_code = SVM_EXIT_NPF;\n\t\tsvm->vmcb->control.exit_code_hi = 0;\n\t\tsvm->vmcb->control.exit_info_1 = (1ULL << 32);\n\t\tsvm->vmcb->control.exit_info_2 = fault->address;\n\t}\n\n\tsvm->vmcb->control.exit_info_1 &= ~0xffffffffULL;\n\tsvm->vmcb->control.exit_info_1 |= fault->error_code;\n\n\tnested_svm_vmexit(svm);\n}\n\nstatic void svm_inject_page_fault_nested(struct kvm_vcpu *vcpu, struct x86_exception *fault)\n{\n       struct vcpu_svm *svm = to_svm(vcpu);\n       WARN_ON(!is_guest_mode(vcpu));\n\n       if (vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_EXCEPTION_OFFSET + PF_VECTOR) &&\n\t   !svm->nested.nested_run_pending) {\n               svm->vmcb->control.exit_code = SVM_EXIT_EXCP_BASE + PF_VECTOR;\n               svm->vmcb->control.exit_code_hi = 0;\n               svm->vmcb->control.exit_info_1 = fault->error_code;\n               svm->vmcb->control.exit_info_2 = fault->address;\n               nested_svm_vmexit(svm);\n       } else {\n               kvm_inject_page_fault(vcpu, fault);\n       }\n}\n\nstatic u64 nested_svm_get_tdp_pdptr(struct kvm_vcpu *vcpu, int index)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu64 cr3 = svm->nested.ctl.nested_cr3;\n\tu64 pdpte;\n\tint ret;\n\n\tret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(cr3), &pdpte,\n\t\t\t\t       offset_in_page(cr3) + index * 8, 8);\n\tif (ret)\n\t\treturn 0;\n\treturn pdpte;\n}\n\nstatic unsigned long nested_svm_get_tdp_cr3(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\treturn svm->nested.ctl.nested_cr3;\n}\n\nstatic void nested_svm_init_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tWARN_ON(mmu_is_nested(vcpu));\n\n\tvcpu->arch.mmu = &vcpu->arch.guest_mmu;\n\n\t/*\n\t * The NPT format depends on L1's CR4 and EFER, which is in vmcb01.  Note,\n\t * when called via KVM_SET_NESTED_STATE, that state may _not_ match current\n\t * vCPU state.  CR0.WP is explicitly ignored, while CR0.PG is required.\n\t */\n\tkvm_init_shadow_npt_mmu(vcpu, X86_CR0_PG, svm->vmcb01.ptr->save.cr4,\n\t\t\t\tsvm->vmcb01.ptr->save.efer,\n\t\t\t\tsvm->nested.ctl.nested_cr3);\n\tvcpu->arch.mmu->get_guest_pgd     = nested_svm_get_tdp_cr3;\n\tvcpu->arch.mmu->get_pdptr         = nested_svm_get_tdp_pdptr;\n\tvcpu->arch.mmu->inject_page_fault = nested_svm_inject_npf_exit;\n\tvcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;\n}\n\nstatic void nested_svm_uninit_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.mmu = &vcpu->arch.root_mmu;\n\tvcpu->arch.walk_mmu = &vcpu->arch.root_mmu;\n}\n\nvoid recalc_intercepts(struct vcpu_svm *svm)\n{\n\tstruct vmcb_control_area *c, *h, *g;\n\tunsigned int i;\n\n\tvmcb_mark_dirty(svm->vmcb, VMCB_INTERCEPTS);\n\n\tif (!is_guest_mode(&svm->vcpu))\n\t\treturn;\n\n\tc = &svm->vmcb->control;\n\th = &svm->vmcb01.ptr->control;\n\tg = &svm->nested.ctl;\n\n\tfor (i = 0; i < MAX_INTERCEPT; i++)\n\t\tc->intercepts[i] = h->intercepts[i];\n\n\tif (g->int_ctl & V_INTR_MASKING_MASK) {\n\t\t/* We only want the cr8 intercept bits of L1 */\n\t\tvmcb_clr_intercept(c, INTERCEPT_CR8_READ);\n\t\tvmcb_clr_intercept(c, INTERCEPT_CR8_WRITE);\n\n\t\t/*\n\t\t * Once running L2 with HF_VINTR_MASK, EFLAGS.IF does not\n\t\t * affect any interrupt we may want to inject; therefore,\n\t\t * interrupt window vmexits are irrelevant to L0.\n\t\t */\n\t\tvmcb_clr_intercept(c, INTERCEPT_VINTR);\n\t}\n\n\t/* We don't want to see VMMCALLs from a nested guest */\n\tvmcb_clr_intercept(c, INTERCEPT_VMMCALL);\n\n\tfor (i = 0; i < MAX_INTERCEPT; i++)\n\t\tc->intercepts[i] |= g->intercepts[i];\n\n\t/* If SMI is not intercepted, ignore guest SMI intercept as well  */\n\tif (!intercept_smi)\n\t\tvmcb_clr_intercept(c, INTERCEPT_SMI);\n}\n\nstatic void copy_vmcb_control_area(struct vmcb_control_area *dst,\n\t\t\t\t   struct vmcb_control_area *from)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < MAX_INTERCEPT; i++)\n\t\tdst->intercepts[i] = from->intercepts[i];\n\n\tdst->iopm_base_pa         = from->iopm_base_pa;\n\tdst->msrpm_base_pa        = from->msrpm_base_pa;\n\tdst->tsc_offset           = from->tsc_offset;\n\t/* asid not copied, it is handled manually for svm->vmcb.  */\n\tdst->tlb_ctl              = from->tlb_ctl;\n\tdst->int_ctl              = from->int_ctl;\n\tdst->int_vector           = from->int_vector;\n\tdst->int_state            = from->int_state;\n\tdst->exit_code            = from->exit_code;\n\tdst->exit_code_hi         = from->exit_code_hi;\n\tdst->exit_info_1          = from->exit_info_1;\n\tdst->exit_info_2          = from->exit_info_2;\n\tdst->exit_int_info        = from->exit_int_info;\n\tdst->exit_int_info_err    = from->exit_int_info_err;\n\tdst->nested_ctl           = from->nested_ctl;\n\tdst->event_inj            = from->event_inj;\n\tdst->event_inj_err        = from->event_inj_err;\n\tdst->nested_cr3           = from->nested_cr3;\n\tdst->virt_ext              = from->virt_ext;\n\tdst->pause_filter_count   = from->pause_filter_count;\n\tdst->pause_filter_thresh  = from->pause_filter_thresh;\n}\n\nstatic bool nested_svm_vmrun_msrpm(struct vcpu_svm *svm)\n{\n\t/*\n\t * This function merges the msr permission bitmaps of kvm and the\n\t * nested vmcb. It is optimized in that it only merges the parts where\n\t * the kvm msr permission bitmap may contain zero bits\n\t */\n\tint i;\n\n\tif (!(vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_MSR_PROT)))\n\t\treturn true;\n\n\tfor (i = 0; i < MSRPM_OFFSETS; i++) {\n\t\tu32 value, p;\n\t\tu64 offset;\n\n\t\tif (msrpm_offsets[i] == 0xffffffff)\n\t\t\tbreak;\n\n\t\tp      = msrpm_offsets[i];\n\t\toffset = svm->nested.ctl.msrpm_base_pa + (p * 4);\n\n\t\tif (kvm_vcpu_read_guest(&svm->vcpu, offset, &value, 4))\n\t\t\treturn false;\n\n\t\tsvm->nested.msrpm[p] = svm->msrpm[p] | value;\n\t}\n\n\tsvm->vmcb->control.msrpm_base_pa = __sme_set(__pa(svm->nested.msrpm));\n\n\treturn true;\n}\n\n/*\n * Bits 11:0 of bitmap address are ignored by hardware\n */\nstatic bool nested_svm_check_bitmap_pa(struct kvm_vcpu *vcpu, u64 pa, u32 size)\n{\n\tu64 addr = PAGE_ALIGN(pa);\n\n\treturn kvm_vcpu_is_legal_gpa(vcpu, addr) &&\n\t    kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);\n}\n\nstatic bool nested_vmcb_check_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcb_control_area *control)\n{\n\tif (CC(!vmcb_is_intercept(control, INTERCEPT_VMRUN)))\n\t\treturn false;\n\n\tif (CC(control->asid == 0))\n\t\treturn false;\n\n\tif (CC((control->nested_ctl & SVM_NESTED_CTL_NP_ENABLE) && !npt_enabled))\n\t\treturn false;\n\n\tif (CC(!nested_svm_check_bitmap_pa(vcpu, control->msrpm_base_pa,\n\t\t\t\t\t   MSRPM_SIZE)))\n\t\treturn false;\n\tif (CC(!nested_svm_check_bitmap_pa(vcpu, control->iopm_base_pa,\n\t\t\t\t\t   IOPM_SIZE)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool nested_vmcb_check_cr3_cr4(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct vmcb_save_area *save)\n{\n\t/*\n\t * These checks are also performed by KVM_SET_SREGS,\n\t * except that EFER.LMA is not checked by SVM against\n\t * CR0.PG && EFER.LME.\n\t */\n\tif ((save->efer & EFER_LME) && (save->cr0 & X86_CR0_PG)) {\n\t\tif (CC(!(save->cr4 & X86_CR4_PAE)) ||\n\t\t    CC(!(save->cr0 & X86_CR0_PE)) ||\n\t\t    CC(kvm_vcpu_is_illegal_gpa(vcpu, save->cr3)))\n\t\t\treturn false;\n\t}\n\n\tif (CC(!kvm_is_valid_cr4(vcpu, save->cr4)))\n\t\treturn false;\n\n\treturn true;\n}\n\n/* Common checks that apply to both L1 and L2 state.  */\nstatic bool nested_vmcb_valid_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct vmcb_save_area *save)\n{\n\t/*\n\t * FIXME: these should be done after copying the fields,\n\t * to avoid TOC/TOU races.  For these save area checks\n\t * the possible damage is limited since kvm_set_cr0 and\n\t * kvm_set_cr4 handle failure; EFER_SVME is an exception\n\t * so it is force-set later in nested_prepare_vmcb_save.\n\t */\n\tif (CC(!(save->efer & EFER_SVME)))\n\t\treturn false;\n\n\tif (CC((save->cr0 & X86_CR0_CD) == 0 && (save->cr0 & X86_CR0_NW)) ||\n\t    CC(save->cr0 & ~0xffffffffULL))\n\t\treturn false;\n\n\tif (CC(!kvm_dr6_valid(save->dr6)) || CC(!kvm_dr7_valid(save->dr7)))\n\t\treturn false;\n\n\tif (!nested_vmcb_check_cr3_cr4(vcpu, save))\n\t\treturn false;\n\n\tif (CC(!kvm_valid_efer(vcpu, save->efer)))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid nested_load_control_from_vmcb12(struct vcpu_svm *svm,\n\t\t\t\t     struct vmcb_control_area *control)\n{\n\tcopy_vmcb_control_area(&svm->nested.ctl, control);\n\n\t/* Copy it here because nested_svm_check_controls will check it.  */\n\tsvm->nested.ctl.asid           = control->asid;\n\tsvm->nested.ctl.msrpm_base_pa &= ~0x0fffULL;\n\tsvm->nested.ctl.iopm_base_pa  &= ~0x0fffULL;\n}\n\n/*\n * Synchronize fields that are written by the processor, so that\n * they can be copied back into the vmcb12.\n */\nvoid nested_sync_control_from_vmcb02(struct vcpu_svm *svm)\n{\n\tu32 mask;\n\tsvm->nested.ctl.event_inj      = svm->vmcb->control.event_inj;\n\tsvm->nested.ctl.event_inj_err  = svm->vmcb->control.event_inj_err;\n\n\t/* Only a few fields of int_ctl are written by the processor.  */\n\tmask = V_IRQ_MASK | V_TPR_MASK;\n\tif (!(svm->nested.ctl.int_ctl & V_INTR_MASKING_MASK) &&\n\t    svm_is_intercept(svm, INTERCEPT_VINTR)) {\n\t\t/*\n\t\t * In order to request an interrupt window, L0 is usurping\n\t\t * svm->vmcb->control.int_ctl and possibly setting V_IRQ\n\t\t * even if it was clear in L1's VMCB.  Restoring it would be\n\t\t * wrong.  However, in this case V_IRQ will remain true until\n\t\t * interrupt_window_interception calls svm_clear_vintr and\n\t\t * restores int_ctl.  We can just leave it aside.\n\t\t */\n\t\tmask &= ~V_IRQ_MASK;\n\t}\n\tsvm->nested.ctl.int_ctl        &= ~mask;\n\tsvm->nested.ctl.int_ctl        |= svm->vmcb->control.int_ctl & mask;\n}\n\n/*\n * Transfer any event that L0 or L1 wanted to inject into L2 to\n * EXIT_INT_INFO.\n */\nstatic void nested_save_pending_event_to_vmcb12(struct vcpu_svm *svm,\n\t\t\t\t\t\tstruct vmcb *vmcb12)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tu32 exit_int_info = 0;\n\tunsigned int nr;\n\n\tif (vcpu->arch.exception.injected) {\n\t\tnr = vcpu->arch.exception.nr;\n\t\texit_int_info = nr | SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_EXEPT;\n\n\t\tif (vcpu->arch.exception.has_error_code) {\n\t\t\texit_int_info |= SVM_EVTINJ_VALID_ERR;\n\t\t\tvmcb12->control.exit_int_info_err =\n\t\t\t\tvcpu->arch.exception.error_code;\n\t\t}\n\n\t} else if (vcpu->arch.nmi_injected) {\n\t\texit_int_info = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;\n\n\t} else if (vcpu->arch.interrupt.injected) {\n\t\tnr = vcpu->arch.interrupt.nr;\n\t\texit_int_info = nr | SVM_EVTINJ_VALID;\n\n\t\tif (vcpu->arch.interrupt.soft)\n\t\t\texit_int_info |= SVM_EVTINJ_TYPE_SOFT;\n\t\telse\n\t\t\texit_int_info |= SVM_EVTINJ_TYPE_INTR;\n\t}\n\n\tvmcb12->control.exit_int_info = exit_int_info;\n}\n\nstatic inline bool nested_npt_enabled(struct vcpu_svm *svm)\n{\n\treturn svm->nested.ctl.nested_ctl & SVM_NESTED_CTL_NP_ENABLE;\n}\n\nstatic void nested_svm_transition_tlb_flush(struct kvm_vcpu *vcpu)\n{\n\t/*\n\t * TODO: optimize unconditional TLB flush/MMU sync.  A partial list of\n\t * things to fix before this can be conditional:\n\t *\n\t *  - Flush TLBs for both L1 and L2 remote TLB flush\n\t *  - Honor L1's request to flush an ASID on nested VMRUN\n\t *  - Sync nested NPT MMU on VMRUN that flushes L2's ASID[*]\n\t *  - Don't crush a pending TLB flush in vmcb02 on nested VMRUN\n\t *  - Flush L1's ASID on KVM_REQ_TLB_FLUSH_GUEST\n\t *\n\t * [*] Unlike nested EPT, SVM's ASID management can invalidate nested\n\t *     NPT guest-physical mappings on VMRUN.\n\t */\n\tkvm_make_request(KVM_REQ_MMU_SYNC, vcpu);\n\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n}\n\n/*\n * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true\n * if we are emulating VM-Entry into a guest with NPT enabled.\n */\nstatic int nested_svm_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3,\n\t\t\t       bool nested_npt, bool reload_pdptrs)\n{\n\tif (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3)))\n\t\treturn -EINVAL;\n\n\tif (reload_pdptrs && !nested_npt && is_pae_paging(vcpu) &&\n\t    CC(!load_pdptrs(vcpu, vcpu->arch.walk_mmu, cr3)))\n\t\treturn -EINVAL;\n\n\tif (!nested_npt)\n\t\tkvm_mmu_new_pgd(vcpu, cr3);\n\n\tvcpu->arch.cr3 = cr3;\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_CR3);\n\n\t/* Re-initialize the MMU, e.g. to pick up CR4 MMU role changes. */\n\tkvm_init_mmu(vcpu);\n\n\treturn 0;\n}\n\nvoid nested_vmcb02_compute_g_pat(struct vcpu_svm *svm)\n{\n\tif (!svm->nested.vmcb02.ptr)\n\t\treturn;\n\n\t/* FIXME: merge g_pat from vmcb01 and vmcb12.  */\n\tsvm->nested.vmcb02.ptr->save.g_pat = svm->vmcb01.ptr->save.g_pat;\n}\n\nstatic void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12)\n{\n\tbool new_vmcb12 = false;\n\n\tnested_vmcb02_compute_g_pat(svm);\n\n\t/* Load the nested guest state */\n\tif (svm->nested.vmcb12_gpa != svm->nested.last_vmcb12_gpa) {\n\t\tnew_vmcb12 = true;\n\t\tsvm->nested.last_vmcb12_gpa = svm->nested.vmcb12_gpa;\n\t}\n\n\tif (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_SEG))) {\n\t\tsvm->vmcb->save.es = vmcb12->save.es;\n\t\tsvm->vmcb->save.cs = vmcb12->save.cs;\n\t\tsvm->vmcb->save.ss = vmcb12->save.ss;\n\t\tsvm->vmcb->save.ds = vmcb12->save.ds;\n\t\tsvm->vmcb->save.cpl = vmcb12->save.cpl;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_SEG);\n\t}\n\n\tif (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_DT))) {\n\t\tsvm->vmcb->save.gdtr = vmcb12->save.gdtr;\n\t\tsvm->vmcb->save.idtr = vmcb12->save.idtr;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_DT);\n\t}\n\n\tkvm_set_rflags(&svm->vcpu, vmcb12->save.rflags | X86_EFLAGS_FIXED);\n\n\t/*\n\t * Force-set EFER_SVME even though it is checked earlier on the\n\t * VMCB12, because the guest can flip the bit between the check\n\t * and now.  Clearing EFER_SVME would call svm_free_nested.\n\t */\n\tsvm_set_efer(&svm->vcpu, vmcb12->save.efer | EFER_SVME);\n\n\tsvm_set_cr0(&svm->vcpu, vmcb12->save.cr0);\n\tsvm_set_cr4(&svm->vcpu, vmcb12->save.cr4);\n\n\tsvm->vcpu.arch.cr2 = vmcb12->save.cr2;\n\n\tkvm_rax_write(&svm->vcpu, vmcb12->save.rax);\n\tkvm_rsp_write(&svm->vcpu, vmcb12->save.rsp);\n\tkvm_rip_write(&svm->vcpu, vmcb12->save.rip);\n\n\t/* In case we don't even reach vcpu_run, the fields are not updated */\n\tsvm->vmcb->save.rax = vmcb12->save.rax;\n\tsvm->vmcb->save.rsp = vmcb12->save.rsp;\n\tsvm->vmcb->save.rip = vmcb12->save.rip;\n\n\t/* These bits will be set properly on the first execution when new_vmc12 is true */\n\tif (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_DR))) {\n\t\tsvm->vmcb->save.dr7 = vmcb12->save.dr7 | DR7_FIXED_1;\n\t\tsvm->vcpu.arch.dr6  = vmcb12->save.dr6 | DR6_ACTIVE_LOW;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_DR);\n\t}\n}\n\nstatic void nested_vmcb02_prepare_control(struct vcpu_svm *svm)\n{\n\tconst u32 int_ctl_vmcb01_bits =\n\t\tV_INTR_MASKING_MASK | V_GIF_MASK | V_GIF_ENABLE_MASK;\n\n\tconst u32 int_ctl_vmcb12_bits = V_TPR_MASK | V_IRQ_INJECTION_BITS_MASK;\n\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\n\t/*\n\t * Filled at exit: exit_code, exit_code_hi, exit_info_1, exit_info_2,\n\t * exit_int_info, exit_int_info_err, next_rip, insn_len, insn_bytes.\n\t */\n\n\t/*\n\t * Also covers avic_vapic_bar, avic_backing_page, avic_logical_id,\n\t * avic_physical_id.\n\t */\n\tWARN_ON(kvm_apicv_activated(svm->vcpu.kvm));\n\n\t/* Copied from vmcb01.  msrpm_base can be overwritten later.  */\n\tsvm->vmcb->control.nested_ctl = svm->vmcb01.ptr->control.nested_ctl;\n\tsvm->vmcb->control.iopm_base_pa = svm->vmcb01.ptr->control.iopm_base_pa;\n\tsvm->vmcb->control.msrpm_base_pa = svm->vmcb01.ptr->control.msrpm_base_pa;\n\n\t/* Done at vmrun: asid.  */\n\n\t/* Also overwritten later if necessary.  */\n\tsvm->vmcb->control.tlb_ctl = TLB_CONTROL_DO_NOTHING;\n\n\t/* nested_cr3.  */\n\tif (nested_npt_enabled(svm))\n\t\tnested_svm_init_mmu_context(vcpu);\n\n\tsvm->vmcb->control.tsc_offset = vcpu->arch.tsc_offset =\n\t\tvcpu->arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;\n\n\tsvm->vmcb->control.int_ctl             =\n\t\t(svm->nested.ctl.int_ctl & int_ctl_vmcb12_bits) |\n\t\t(svm->vmcb01.ptr->control.int_ctl & int_ctl_vmcb01_bits);\n\n\tsvm->vmcb->control.virt_ext            = svm->nested.ctl.virt_ext;\n\tsvm->vmcb->control.int_vector          = svm->nested.ctl.int_vector;\n\tsvm->vmcb->control.int_state           = svm->nested.ctl.int_state;\n\tsvm->vmcb->control.event_inj           = svm->nested.ctl.event_inj;\n\tsvm->vmcb->control.event_inj_err       = svm->nested.ctl.event_inj_err;\n\n\tsvm->vmcb->control.pause_filter_count  = svm->nested.ctl.pause_filter_count;\n\tsvm->vmcb->control.pause_filter_thresh = svm->nested.ctl.pause_filter_thresh;\n\n\tnested_svm_transition_tlb_flush(vcpu);\n\n\t/* Enter Guest-Mode */\n\tenter_guest_mode(vcpu);\n\n\t/*\n\t * Merge guest and host intercepts - must be called with vcpu in\n\t * guest-mode to take effect.\n\t */\n\trecalc_intercepts(svm);\n}\n\nstatic void nested_svm_copy_common_state(struct vmcb *from_vmcb, struct vmcb *to_vmcb)\n{\n\t/*\n\t * Some VMCB state is shared between L1 and L2 and thus has to be\n\t * moved at the time of nested vmrun and vmexit.\n\t *\n\t * VMLOAD/VMSAVE state would also belong in this category, but KVM\n\t * always performs VMLOAD and VMSAVE from the VMCB01.\n\t */\n\tto_vmcb->save.spec_ctrl = from_vmcb->save.spec_ctrl;\n}\n\nint enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint ret;\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\n\tWARN_ON(svm->vmcb == svm->nested.vmcb02.ptr);\n\n\tnested_svm_copy_common_state(svm->vmcb01.ptr, svm->nested.vmcb02.ptr);\n\n\tsvm_switch_vmcb(svm, &svm->nested.vmcb02);\n\tnested_vmcb02_prepare_control(svm);\n\tnested_vmcb02_prepare_save(svm, vmcb12);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm), true);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!npt_enabled)\n\t\tvcpu->arch.mmu->inject_page_fault = svm_inject_page_fault_nested;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}\n\nint nested_svm_vmrun(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (!svm->nested.hsave_msr) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tif (is_smm(vcpu)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tnested_load_control_from_vmcb12(svm, &vmcb12->control);\n\n\tif (!nested_vmcb_valid_sregs(vcpu, &vmcb12->save) ||\n\t    !nested_vmcb_check_controls(vcpu, &svm->nested.ctl)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\t/*\n\t * Since vmcb01 is not in use, we can use it to store some of the L1\n\t * state.\n\t */\n\tsvm->vmcb01.ptr->save.efer   = vcpu->arch.efer;\n\tsvm->vmcb01.ptr->save.cr0    = kvm_read_cr0(vcpu);\n\tsvm->vmcb01.ptr->save.cr4    = vcpu->arch.cr4;\n\tsvm->vmcb01.ptr->save.rflags = kvm_get_rflags(vcpu);\n\tsvm->vmcb01.ptr->save.rip    = kvm_rip_read(vcpu);\n\n\tif (!npt_enabled)\n\t\tsvm->vmcb01.ptr->save.cr3 = kvm_read_cr3(vcpu);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(vcpu, &map, true);\n\n\treturn ret;\n}\n\n/* Copy state save area fields which are handled by VMRUN */\nvoid svm_copy_vmrun_state(struct vmcb_save_area *to_save,\n\t\t\t  struct vmcb_save_area *from_save)\n{\n\tto_save->es = from_save->es;\n\tto_save->cs = from_save->cs;\n\tto_save->ss = from_save->ss;\n\tto_save->ds = from_save->ds;\n\tto_save->gdtr = from_save->gdtr;\n\tto_save->idtr = from_save->idtr;\n\tto_save->rflags = from_save->rflags | X86_EFLAGS_FIXED;\n\tto_save->efer = from_save->efer;\n\tto_save->cr0 = from_save->cr0;\n\tto_save->cr3 = from_save->cr3;\n\tto_save->cr4 = from_save->cr4;\n\tto_save->rax = from_save->rax;\n\tto_save->rsp = from_save->rsp;\n\tto_save->rip = from_save->rip;\n\tto_save->cpl = 0;\n}\n\nvoid svm_copy_vmloadsave_state(struct vmcb *to_vmcb, struct vmcb *from_vmcb)\n{\n\tto_vmcb->save.fs = from_vmcb->save.fs;\n\tto_vmcb->save.gs = from_vmcb->save.gs;\n\tto_vmcb->save.tr = from_vmcb->save.tr;\n\tto_vmcb->save.ldtr = from_vmcb->save.ldtr;\n\tto_vmcb->save.kernel_gs_base = from_vmcb->save.kernel_gs_base;\n\tto_vmcb->save.star = from_vmcb->save.star;\n\tto_vmcb->save.lstar = from_vmcb->save.lstar;\n\tto_vmcb->save.cstar = from_vmcb->save.cstar;\n\tto_vmcb->save.sfmask = from_vmcb->save.sfmask;\n\tto_vmcb->save.sysenter_cs = from_vmcb->save.sysenter_cs;\n\tto_vmcb->save.sysenter_esp = from_vmcb->save.sysenter_esp;\n\tto_vmcb->save.sysenter_eip = from_vmcb->save.sysenter_eip;\n}\n\nint nested_svm_vmexit(struct vcpu_svm *svm)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tint rc;\n\n\t/* Triple faults in L2 should never escape. */\n\tWARN_ON_ONCE(kvm_check_request(KVM_REQ_TRIPLE_FAULT, vcpu));\n\n\trc = kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.vmcb12_gpa), &map);\n\tif (rc) {\n\t\tif (rc == -EINVAL)\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tvmcb12 = map.hva;\n\n\t/* Exit Guest-Mode */\n\tleave_guest_mode(vcpu);\n\tsvm->nested.vmcb12_gpa = 0;\n\tWARN_ON_ONCE(svm->nested.nested_run_pending);\n\n\tkvm_clear_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n\n\t/* in case we halted in L2 */\n\tsvm->vcpu.arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\t/* Give the current vmcb to the guest */\n\n\tvmcb12->save.es     = vmcb->save.es;\n\tvmcb12->save.cs     = vmcb->save.cs;\n\tvmcb12->save.ss     = vmcb->save.ss;\n\tvmcb12->save.ds     = vmcb->save.ds;\n\tvmcb12->save.gdtr   = vmcb->save.gdtr;\n\tvmcb12->save.idtr   = vmcb->save.idtr;\n\tvmcb12->save.efer   = svm->vcpu.arch.efer;\n\tvmcb12->save.cr0    = kvm_read_cr0(vcpu);\n\tvmcb12->save.cr3    = kvm_read_cr3(vcpu);\n\tvmcb12->save.cr2    = vmcb->save.cr2;\n\tvmcb12->save.cr4    = svm->vcpu.arch.cr4;\n\tvmcb12->save.rflags = kvm_get_rflags(vcpu);\n\tvmcb12->save.rip    = kvm_rip_read(vcpu);\n\tvmcb12->save.rsp    = kvm_rsp_read(vcpu);\n\tvmcb12->save.rax    = kvm_rax_read(vcpu);\n\tvmcb12->save.dr7    = vmcb->save.dr7;\n\tvmcb12->save.dr6    = svm->vcpu.arch.dr6;\n\tvmcb12->save.cpl    = vmcb->save.cpl;\n\n\tvmcb12->control.int_state         = vmcb->control.int_state;\n\tvmcb12->control.exit_code         = vmcb->control.exit_code;\n\tvmcb12->control.exit_code_hi      = vmcb->control.exit_code_hi;\n\tvmcb12->control.exit_info_1       = vmcb->control.exit_info_1;\n\tvmcb12->control.exit_info_2       = vmcb->control.exit_info_2;\n\n\tif (vmcb12->control.exit_code != SVM_EXIT_ERR)\n\t\tnested_save_pending_event_to_vmcb12(svm, vmcb12);\n\n\tif (svm->nrips_enabled)\n\t\tvmcb12->control.next_rip  = vmcb->control.next_rip;\n\n\tvmcb12->control.int_ctl           = svm->nested.ctl.int_ctl;\n\tvmcb12->control.tlb_ctl           = svm->nested.ctl.tlb_ctl;\n\tvmcb12->control.event_inj         = svm->nested.ctl.event_inj;\n\tvmcb12->control.event_inj_err     = svm->nested.ctl.event_inj_err;\n\n\tvmcb12->control.pause_filter_count =\n\t\tsvm->vmcb->control.pause_filter_count;\n\tvmcb12->control.pause_filter_thresh =\n\t\tsvm->vmcb->control.pause_filter_thresh;\n\n\tnested_svm_copy_common_state(svm->nested.vmcb02.ptr, svm->vmcb01.ptr);\n\n\tsvm_switch_vmcb(svm, &svm->vmcb01);\n\n\t/*\n\t * On vmexit the  GIF is set to false and\n\t * no event can be injected in L1.\n\t */\n\tsvm_set_gif(svm, false);\n\tsvm->vmcb->control.exit_int_info = 0;\n\n\tsvm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;\n\tif (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {\n\t\tsvm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_INTERCEPTS);\n\t}\n\n\tsvm->nested.ctl.nested_cr3 = 0;\n\n\t/*\n\t * Restore processor state that had been saved in vmcb01\n\t */\n\tkvm_set_rflags(vcpu, svm->vmcb->save.rflags);\n\tsvm_set_efer(vcpu, svm->vmcb->save.efer);\n\tsvm_set_cr0(vcpu, svm->vmcb->save.cr0 | X86_CR0_PE);\n\tsvm_set_cr4(vcpu, svm->vmcb->save.cr4);\n\tkvm_rax_write(vcpu, svm->vmcb->save.rax);\n\tkvm_rsp_write(vcpu, svm->vmcb->save.rsp);\n\tkvm_rip_write(vcpu, svm->vmcb->save.rip);\n\n\tsvm->vcpu.arch.dr7 = DR7_FIXED_1;\n\tkvm_update_dr7(&svm->vcpu);\n\n\ttrace_kvm_nested_vmexit_inject(vmcb12->control.exit_code,\n\t\t\t\t       vmcb12->control.exit_info_1,\n\t\t\t\t       vmcb12->control.exit_info_2,\n\t\t\t\t       vmcb12->control.exit_int_info,\n\t\t\t\t       vmcb12->control.exit_int_info_err,\n\t\t\t\t       KVM_ISA_SVM);\n\n\tkvm_vcpu_unmap(vcpu, &map, true);\n\n\tnested_svm_transition_tlb_flush(vcpu);\n\n\tnested_svm_uninit_mmu_context(vcpu);\n\n\trc = nested_svm_load_cr3(vcpu, svm->vmcb->save.cr3, false, true);\n\tif (rc)\n\t\treturn 1;\n\n\t/*\n\t * Drop what we picked up for L2 via svm_complete_interrupts() so it\n\t * doesn't end up in L1.\n\t */\n\tsvm->vcpu.arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\t/*\n\t * If we are here following the completion of a VMRUN that\n\t * is being single-stepped, queue the pending #DB intercept\n\t * right now so that it an be accounted for before we execute\n\t * L1's next instruction.\n\t */\n\tif (unlikely(svm->vmcb->save.rflags & X86_EFLAGS_TF))\n\t\tkvm_queue_exception(&(svm->vcpu), DB_VECTOR);\n\n\treturn 0;\n}\n\nstatic void nested_svm_triple_fault(struct kvm_vcpu *vcpu)\n{\n\tnested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);\n}\n\nint svm_allocate_nested(struct vcpu_svm *svm)\n{\n\tstruct page *vmcb02_page;\n\n\tif (svm->nested.initialized)\n\t\treturn 0;\n\n\tvmcb02_page = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\tif (!vmcb02_page)\n\t\treturn -ENOMEM;\n\tsvm->nested.vmcb02.ptr = page_address(vmcb02_page);\n\tsvm->nested.vmcb02.pa = __sme_set(page_to_pfn(vmcb02_page) << PAGE_SHIFT);\n\n\tsvm->nested.msrpm = svm_vcpu_alloc_msrpm();\n\tif (!svm->nested.msrpm)\n\t\tgoto err_free_vmcb02;\n\tsvm_vcpu_init_msrpm(&svm->vcpu, svm->nested.msrpm);\n\n\tsvm->nested.initialized = true;\n\treturn 0;\n\nerr_free_vmcb02:\n\t__free_page(vmcb02_page);\n\treturn -ENOMEM;\n}\n\nvoid svm_free_nested(struct vcpu_svm *svm)\n{\n\tif (!svm->nested.initialized)\n\t\treturn;\n\n\tsvm_vcpu_free_msrpm(svm->nested.msrpm);\n\tsvm->nested.msrpm = NULL;\n\n\t__free_page(virt_to_page(svm->nested.vmcb02.ptr));\n\tsvm->nested.vmcb02.ptr = NULL;\n\n\t/*\n\t * When last_vmcb12_gpa matches the current vmcb12 gpa,\n\t * some vmcb12 fields are not loaded if they are marked clean\n\t * in the vmcb12, since in this case they are up to date already.\n\t *\n\t * When the vmcb02 is freed, this optimization becomes invalid.\n\t */\n\tsvm->nested.last_vmcb12_gpa = INVALID_GPA;\n\n\tsvm->nested.initialized = false;\n}\n\n/*\n * Forcibly leave nested mode in order to be able to reset the VCPU later on.\n */\nvoid svm_leave_nested(struct vcpu_svm *svm)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tsvm->nested.nested_run_pending = 0;\n\t\tsvm->nested.vmcb12_gpa = INVALID_GPA;\n\n\t\tleave_guest_mode(vcpu);\n\n\t\tsvm_switch_vmcb(svm, &svm->vmcb01);\n\n\t\tnested_svm_uninit_mmu_context(vcpu);\n\t\tvmcb_mark_all_dirty(svm->vmcb);\n\t}\n\n\tkvm_clear_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n}\n\nstatic int nested_svm_exit_handled_msr(struct vcpu_svm *svm)\n{\n\tu32 offset, msr, value;\n\tint write, mask;\n\n\tif (!(vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_MSR_PROT)))\n\t\treturn NESTED_EXIT_HOST;\n\n\tmsr    = svm->vcpu.arch.regs[VCPU_REGS_RCX];\n\toffset = svm_msrpm_offset(msr);\n\twrite  = svm->vmcb->control.exit_info_1 & 1;\n\tmask   = 1 << ((2 * (msr & 0xf)) + write);\n\n\tif (offset == MSR_INVALID)\n\t\treturn NESTED_EXIT_DONE;\n\n\t/* Offset is in 32 bit units but need in 8 bit units */\n\toffset *= 4;\n\n\tif (kvm_vcpu_read_guest(&svm->vcpu, svm->nested.ctl.msrpm_base_pa + offset, &value, 4))\n\t\treturn NESTED_EXIT_DONE;\n\n\treturn (value & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;\n}\n\nstatic int nested_svm_intercept_ioio(struct vcpu_svm *svm)\n{\n\tunsigned port, size, iopm_len;\n\tu16 val, mask;\n\tu8 start_bit;\n\tu64 gpa;\n\n\tif (!(vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_IOIO_PROT)))\n\t\treturn NESTED_EXIT_HOST;\n\n\tport = svm->vmcb->control.exit_info_1 >> 16;\n\tsize = (svm->vmcb->control.exit_info_1 & SVM_IOIO_SIZE_MASK) >>\n\t\tSVM_IOIO_SIZE_SHIFT;\n\tgpa  = svm->nested.ctl.iopm_base_pa + (port / 8);\n\tstart_bit = port % 8;\n\tiopm_len = (start_bit + size > 8) ? 2 : 1;\n\tmask = (0xf >> (4 - size)) << start_bit;\n\tval = 0;\n\n\tif (kvm_vcpu_read_guest(&svm->vcpu, gpa, &val, iopm_len))\n\t\treturn NESTED_EXIT_DONE;\n\n\treturn (val & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;\n}\n\nstatic int nested_svm_intercept(struct vcpu_svm *svm)\n{\n\tu32 exit_code = svm->vmcb->control.exit_code;\n\tint vmexit = NESTED_EXIT_HOST;\n\n\tswitch (exit_code) {\n\tcase SVM_EXIT_MSR:\n\t\tvmexit = nested_svm_exit_handled_msr(svm);\n\t\tbreak;\n\tcase SVM_EXIT_IOIO:\n\t\tvmexit = nested_svm_intercept_ioio(svm);\n\t\tbreak;\n\tcase SVM_EXIT_READ_CR0 ... SVM_EXIT_WRITE_CR8: {\n\t\tif (vmcb_is_intercept(&svm->nested.ctl, exit_code))\n\t\t\tvmexit = NESTED_EXIT_DONE;\n\t\tbreak;\n\t}\n\tcase SVM_EXIT_READ_DR0 ... SVM_EXIT_WRITE_DR7: {\n\t\tif (vmcb_is_intercept(&svm->nested.ctl, exit_code))\n\t\t\tvmexit = NESTED_EXIT_DONE;\n\t\tbreak;\n\t}\n\tcase SVM_EXIT_EXCP_BASE ... SVM_EXIT_EXCP_BASE + 0x1f: {\n\t\t/*\n\t\t * Host-intercepted exceptions have been checked already in\n\t\t * nested_svm_exit_special.  There is nothing to do here,\n\t\t * the vmexit is injected by svm_check_nested_events.\n\t\t */\n\t\tvmexit = NESTED_EXIT_DONE;\n\t\tbreak;\n\t}\n\tcase SVM_EXIT_ERR: {\n\t\tvmexit = NESTED_EXIT_DONE;\n\t\tbreak;\n\t}\n\tdefault: {\n\t\tif (vmcb_is_intercept(&svm->nested.ctl, exit_code))\n\t\t\tvmexit = NESTED_EXIT_DONE;\n\t}\n\t}\n\n\treturn vmexit;\n}\n\nint nested_svm_exit_handled(struct vcpu_svm *svm)\n{\n\tint vmexit;\n\n\tvmexit = nested_svm_intercept(svm);\n\n\tif (vmexit == NESTED_EXIT_DONE)\n\t\tnested_svm_vmexit(svm);\n\n\treturn vmexit;\n}\n\nint nested_svm_check_permissions(struct kvm_vcpu *vcpu)\n{\n\tif (!(vcpu->arch.efer & EFER_SVME) || !is_paging(vcpu)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (to_svm(vcpu)->vmcb->save.cpl) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic bool nested_exit_on_exception(struct vcpu_svm *svm)\n{\n\tunsigned int nr = svm->vcpu.arch.exception.nr;\n\n\treturn (svm->nested.ctl.intercepts[INTERCEPT_EXCEPTION] & BIT(nr));\n}\n\nstatic void nested_svm_inject_exception_vmexit(struct vcpu_svm *svm)\n{\n\tunsigned int nr = svm->vcpu.arch.exception.nr;\n\n\tsvm->vmcb->control.exit_code = SVM_EXIT_EXCP_BASE + nr;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\n\tif (svm->vcpu.arch.exception.has_error_code)\n\t\tsvm->vmcb->control.exit_info_1 = svm->vcpu.arch.exception.error_code;\n\n\t/*\n\t * EXITINFO2 is undefined for all exception intercepts other\n\t * than #PF.\n\t */\n\tif (nr == PF_VECTOR) {\n\t\tif (svm->vcpu.arch.exception.nested_apf)\n\t\t\tsvm->vmcb->control.exit_info_2 = svm->vcpu.arch.apf.nested_apf_token;\n\t\telse if (svm->vcpu.arch.exception.has_payload)\n\t\t\tsvm->vmcb->control.exit_info_2 = svm->vcpu.arch.exception.payload;\n\t\telse\n\t\t\tsvm->vmcb->control.exit_info_2 = svm->vcpu.arch.cr2;\n\t} else if (nr == DB_VECTOR) {\n\t\t/* See inject_pending_event.  */\n\t\tkvm_deliver_exception_payload(&svm->vcpu);\n\t\tif (svm->vcpu.arch.dr7 & DR7_GD) {\n\t\t\tsvm->vcpu.arch.dr7 &= ~DR7_GD;\n\t\t\tkvm_update_dr7(&svm->vcpu);\n\t\t}\n\t} else\n\t\tWARN_ON(svm->vcpu.arch.exception.has_payload);\n\n\tnested_svm_vmexit(svm);\n}\n\nstatic inline bool nested_exit_on_init(struct vcpu_svm *svm)\n{\n\treturn vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_INIT);\n}\n\nstatic int svm_check_nested_events(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tbool block_nested_events =\n\t\tkvm_event_needs_reinjection(vcpu) || svm->nested.nested_run_pending;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (lapic_in_kernel(vcpu) &&\n\t    test_bit(KVM_APIC_INIT, &apic->pending_events)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_init(svm))\n\t\t\treturn 0;\n\t\tnested_svm_simple_vmexit(svm, SVM_EXIT_INIT);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.exception.pending) {\n\t\t/*\n\t\t * Only a pending nested run can block a pending exception.\n\t\t * Otherwise an injected NMI/interrupt should either be\n\t\t * lost or delivered to the nested hypervisor in the EXITINTINFO\n\t\t * vmcb field, while delivering the pending exception.\n\t\t */\n\t\tif (svm->nested.nested_run_pending)\n                        return -EBUSY;\n\t\tif (!nested_exit_on_exception(svm))\n\t\t\treturn 0;\n\t\tnested_svm_inject_exception_vmexit(svm);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.smi_pending && !svm_smi_blocked(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_smi(svm))\n\t\t\treturn 0;\n\t\tnested_svm_simple_vmexit(svm, SVM_EXIT_SMI);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.nmi_pending && !svm_nmi_blocked(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_nmi(svm))\n\t\t\treturn 0;\n\t\tnested_svm_simple_vmexit(svm, SVM_EXIT_NMI);\n\t\treturn 0;\n\t}\n\n\tif (kvm_cpu_has_interrupt(vcpu) && !svm_interrupt_blocked(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_intr(svm))\n\t\t\treturn 0;\n\t\ttrace_kvm_nested_intr_vmexit(svm->vmcb->save.rip);\n\t\tnested_svm_simple_vmexit(svm, SVM_EXIT_INTR);\n\t\treturn 0;\n\t}\n\n\treturn 0;\n}\n\nint nested_svm_exit_special(struct vcpu_svm *svm)\n{\n\tu32 exit_code = svm->vmcb->control.exit_code;\n\n\tswitch (exit_code) {\n\tcase SVM_EXIT_INTR:\n\tcase SVM_EXIT_NMI:\n\tcase SVM_EXIT_NPF:\n\t\treturn NESTED_EXIT_HOST;\n\tcase SVM_EXIT_EXCP_BASE ... SVM_EXIT_EXCP_BASE + 0x1f: {\n\t\tu32 excp_bits = 1 << (exit_code - SVM_EXIT_EXCP_BASE);\n\n\t\tif (svm->vmcb01.ptr->control.intercepts[INTERCEPT_EXCEPTION] &\n\t\t    excp_bits)\n\t\t\treturn NESTED_EXIT_HOST;\n\t\telse if (exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR &&\n\t\t\t svm->vcpu.arch.apf.host_apf_flags)\n\t\t\t/* Trap async PF even if not shadowing */\n\t\t\treturn NESTED_EXIT_HOST;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NESTED_EXIT_CONTINUE;\n}\n\nstatic int svm_get_nested_state(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_nested_state __user *user_kvm_nested_state,\n\t\t\t\tu32 user_data_size)\n{\n\tstruct vcpu_svm *svm;\n\tstruct kvm_nested_state kvm_state = {\n\t\t.flags = 0,\n\t\t.format = KVM_STATE_NESTED_FORMAT_SVM,\n\t\t.size = sizeof(kvm_state),\n\t};\n\tstruct vmcb __user *user_vmcb = (struct vmcb __user *)\n\t\t&user_kvm_nested_state->data.svm[0];\n\n\tif (!vcpu)\n\t\treturn kvm_state.size + KVM_STATE_NESTED_SVM_VMCB_SIZE;\n\n\tsvm = to_svm(vcpu);\n\n\tif (user_data_size < kvm_state.size)\n\t\tgoto out;\n\n\t/* First fill in the header and copy it out.  */\n\tif (is_guest_mode(vcpu)) {\n\t\tkvm_state.hdr.svm.vmcb_pa = svm->nested.vmcb12_gpa;\n\t\tkvm_state.size += KVM_STATE_NESTED_SVM_VMCB_SIZE;\n\t\tkvm_state.flags |= KVM_STATE_NESTED_GUEST_MODE;\n\n\t\tif (svm->nested.nested_run_pending)\n\t\t\tkvm_state.flags |= KVM_STATE_NESTED_RUN_PENDING;\n\t}\n\n\tif (gif_set(svm))\n\t\tkvm_state.flags |= KVM_STATE_NESTED_GIF_SET;\n\n\tif (copy_to_user(user_kvm_nested_state, &kvm_state, sizeof(kvm_state)))\n\t\treturn -EFAULT;\n\n\tif (!is_guest_mode(vcpu))\n\t\tgoto out;\n\n\t/*\n\t * Copy over the full size of the VMCB rather than just the size\n\t * of the structs.\n\t */\n\tif (clear_user(user_vmcb, KVM_STATE_NESTED_SVM_VMCB_SIZE))\n\t\treturn -EFAULT;\n\tif (copy_to_user(&user_vmcb->control, &svm->nested.ctl,\n\t\t\t sizeof(user_vmcb->control)))\n\t\treturn -EFAULT;\n\tif (copy_to_user(&user_vmcb->save, &svm->vmcb01.ptr->save,\n\t\t\t sizeof(user_vmcb->save)))\n\t\treturn -EFAULT;\nout:\n\treturn kvm_state.size;\n}\n\nstatic int svm_set_nested_state(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_nested_state __user *user_kvm_nested_state,\n\t\t\t\tstruct kvm_nested_state *kvm_state)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb __user *user_vmcb = (struct vmcb __user *)\n\t\t&user_kvm_nested_state->data.svm[0];\n\tstruct vmcb_control_area *ctl;\n\tstruct vmcb_save_area *save;\n\tunsigned long cr0;\n\tint ret;\n\n\tBUILD_BUG_ON(sizeof(struct vmcb_control_area) + sizeof(struct vmcb_save_area) >\n\t\t     KVM_STATE_NESTED_SVM_VMCB_SIZE);\n\n\tif (kvm_state->format != KVM_STATE_NESTED_FORMAT_SVM)\n\t\treturn -EINVAL;\n\n\tif (kvm_state->flags & ~(KVM_STATE_NESTED_GUEST_MODE |\n\t\t\t\t KVM_STATE_NESTED_RUN_PENDING |\n\t\t\t\t KVM_STATE_NESTED_GIF_SET))\n\t\treturn -EINVAL;\n\n\t/*\n\t * If in guest mode, vcpu->arch.efer actually refers to the L2 guest's\n\t * EFER.SVME, but EFER.SVME still has to be 1 for VMRUN to succeed.\n\t */\n\tif (!(vcpu->arch.efer & EFER_SVME)) {\n\t\t/* GIF=1 and no guest mode are required if SVME=0.  */\n\t\tif (kvm_state->flags != KVM_STATE_NESTED_GIF_SET)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* SMM temporarily disables SVM, so we cannot be in guest mode.  */\n\tif (is_smm(vcpu) && (kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE))\n\t\treturn -EINVAL;\n\n\tif (!(kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE)) {\n\t\tsvm_leave_nested(svm);\n\t\tsvm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));\n\t\treturn 0;\n\t}\n\n\tif (!page_address_valid(vcpu, kvm_state->hdr.svm.vmcb_pa))\n\t\treturn -EINVAL;\n\tif (kvm_state->size < sizeof(*kvm_state) + KVM_STATE_NESTED_SVM_VMCB_SIZE)\n\t\treturn -EINVAL;\n\n\tret  = -ENOMEM;\n\tctl  = kzalloc(sizeof(*ctl),  GFP_KERNEL_ACCOUNT);\n\tsave = kzalloc(sizeof(*save), GFP_KERNEL_ACCOUNT);\n\tif (!ctl || !save)\n\t\tgoto out_free;\n\n\tret = -EFAULT;\n\tif (copy_from_user(ctl, &user_vmcb->control, sizeof(*ctl)))\n\t\tgoto out_free;\n\tif (copy_from_user(save, &user_vmcb->save, sizeof(*save)))\n\t\tgoto out_free;\n\n\tret = -EINVAL;\n\tif (!nested_vmcb_check_controls(vcpu, ctl))\n\t\tgoto out_free;\n\n\t/*\n\t * Processor state contains L2 state.  Check that it is\n\t * valid for guest mode (see nested_vmcb_check_save).\n\t */\n\tcr0 = kvm_read_cr0(vcpu);\n        if (((cr0 & X86_CR0_CD) == 0) && (cr0 & X86_CR0_NW))\n\t\tgoto out_free;\n\n\t/*\n\t * Validate host state saved from before VMRUN (see\n\t * nested_svm_check_permissions).\n\t */\n\tif (!(save->cr0 & X86_CR0_PG) ||\n\t    !(save->cr0 & X86_CR0_PE) ||\n\t    (save->rflags & X86_EFLAGS_VM) ||\n\t    !nested_vmcb_valid_sregs(vcpu, save))\n\t\tgoto out_free;\n\n\t/*\n\t * While the nested guest CR3 is already checked and set by\n\t * KVM_SET_SREGS, it was set when nested state was yet loaded,\n\t * thus MMU might not be initialized correctly.\n\t * Set it again to fix this.\n\t */\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3,\n\t\t\t\t  nested_npt_enabled(svm), false);\n\tif (WARN_ON_ONCE(ret))\n\t\tgoto out_free;\n\n\n\t/*\n\t * All checks done, we can enter guest mode. Userspace provides\n\t * vmcb12.control, which will be combined with L1 and stored into\n\t * vmcb02, and the L1 save state which we store in vmcb01.\n\t * L2 registers if needed are moved from the current VMCB to VMCB02.\n\t */\n\n\tif (is_guest_mode(vcpu))\n\t\tsvm_leave_nested(svm);\n\telse\n\t\tsvm->nested.vmcb02.ptr->save = svm->vmcb01.ptr->save;\n\n\tsvm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));\n\n\tsvm->nested.nested_run_pending =\n\t\t!!(kvm_state->flags & KVM_STATE_NESTED_RUN_PENDING);\n\n\tsvm->nested.vmcb12_gpa = kvm_state->hdr.svm.vmcb_pa;\n\n\tsvm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);\n\tnested_load_control_from_vmcb12(svm, ctl);\n\n\tsvm_switch_vmcb(svm, &svm->nested.vmcb02);\n\tnested_vmcb02_prepare_control(svm);\n\tkvm_make_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n\tret = 0;\nout_free:\n\tkfree(save);\n\tkfree(ctl);\n\n\treturn ret;\n}\n\nstatic bool svm_get_nested_state_pages(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (WARN_ON(!is_guest_mode(vcpu)))\n\t\treturn true;\n\n\tif (!vcpu->arch.pdptrs_from_userspace &&\n\t    !nested_npt_enabled(svm) && is_pae_paging(vcpu))\n\t\t/*\n\t\t * Reload the guest's PDPTRs since after a migration\n\t\t * the guest CR3 might be restored prior to setting the nested\n\t\t * state which can lead to a load of wrong PDPTRs.\n\t\t */\n\t\tif (CC(!load_pdptrs(vcpu, vcpu->arch.walk_mmu, vcpu->arch.cr3)))\n\t\t\treturn false;\n\n\tif (!nested_svm_vmrun_msrpm(svm)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror =\n\t\t\tKVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstruct kvm_x86_nested_ops svm_nested_ops = {\n\t.check_events = svm_check_nested_events,\n\t.triple_fault = nested_svm_triple_fault,\n\t.get_nested_state_pages = svm_get_nested_state_pages,\n\t.get_state = svm_get_nested_state,\n\t.set_state = svm_set_nested_state,\n};\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * AMD SVM support\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *   Avi Kivity   <avi@qumranet.com>\n */\n\n#define pr_fmt(fmt) \"SVM: \" fmt\n\n#include <linux/kvm_types.h>\n#include <linux/kvm_host.h>\n#include <linux/kernel.h>\n\n#include <asm/msr-index.h>\n#include <asm/debugreg.h>\n\n#include \"kvm_emulate.h\"\n#include \"trace.h\"\n#include \"mmu.h\"\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"lapic.h\"\n#include \"svm.h\"\n\n#define CC KVM_NESTED_VMENTER_CONSISTENCY_CHECK\n\nstatic void nested_svm_inject_npf_exit(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct x86_exception *fault)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (svm->vmcb->control.exit_code != SVM_EXIT_NPF) {\n\t\t/*\n\t\t * TODO: track the cause of the nested page fault, and\n\t\t * correctly fill in the high bits of exit_info_1.\n\t\t */\n\t\tsvm->vmcb->control.exit_code = SVM_EXIT_NPF;\n\t\tsvm->vmcb->control.exit_code_hi = 0;\n\t\tsvm->vmcb->control.exit_info_1 = (1ULL << 32);\n\t\tsvm->vmcb->control.exit_info_2 = fault->address;\n\t}\n\n\tsvm->vmcb->control.exit_info_1 &= ~0xffffffffULL;\n\tsvm->vmcb->control.exit_info_1 |= fault->error_code;\n\n\tnested_svm_vmexit(svm);\n}\n\nstatic void svm_inject_page_fault_nested(struct kvm_vcpu *vcpu, struct x86_exception *fault)\n{\n       struct vcpu_svm *svm = to_svm(vcpu);\n       WARN_ON(!is_guest_mode(vcpu));\n\n       if (vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_EXCEPTION_OFFSET + PF_VECTOR) &&\n\t   !svm->nested.nested_run_pending) {\n               svm->vmcb->control.exit_code = SVM_EXIT_EXCP_BASE + PF_VECTOR;\n               svm->vmcb->control.exit_code_hi = 0;\n               svm->vmcb->control.exit_info_1 = fault->error_code;\n               svm->vmcb->control.exit_info_2 = fault->address;\n               nested_svm_vmexit(svm);\n       } else {\n               kvm_inject_page_fault(vcpu, fault);\n       }\n}\n\nstatic u64 nested_svm_get_tdp_pdptr(struct kvm_vcpu *vcpu, int index)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu64 cr3 = svm->nested.ctl.nested_cr3;\n\tu64 pdpte;\n\tint ret;\n\n\tret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(cr3), &pdpte,\n\t\t\t\t       offset_in_page(cr3) + index * 8, 8);\n\tif (ret)\n\t\treturn 0;\n\treturn pdpte;\n}\n\nstatic unsigned long nested_svm_get_tdp_cr3(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\treturn svm->nested.ctl.nested_cr3;\n}\n\nstatic void nested_svm_init_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tWARN_ON(mmu_is_nested(vcpu));\n\n\tvcpu->arch.mmu = &vcpu->arch.guest_mmu;\n\n\t/*\n\t * The NPT format depends on L1's CR4 and EFER, which is in vmcb01.  Note,\n\t * when called via KVM_SET_NESTED_STATE, that state may _not_ match current\n\t * vCPU state.  CR0.WP is explicitly ignored, while CR0.PG is required.\n\t */\n\tkvm_init_shadow_npt_mmu(vcpu, X86_CR0_PG, svm->vmcb01.ptr->save.cr4,\n\t\t\t\tsvm->vmcb01.ptr->save.efer,\n\t\t\t\tsvm->nested.ctl.nested_cr3);\n\tvcpu->arch.mmu->get_guest_pgd     = nested_svm_get_tdp_cr3;\n\tvcpu->arch.mmu->get_pdptr         = nested_svm_get_tdp_pdptr;\n\tvcpu->arch.mmu->inject_page_fault = nested_svm_inject_npf_exit;\n\tvcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;\n}\n\nstatic void nested_svm_uninit_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.mmu = &vcpu->arch.root_mmu;\n\tvcpu->arch.walk_mmu = &vcpu->arch.root_mmu;\n}\n\nvoid recalc_intercepts(struct vcpu_svm *svm)\n{\n\tstruct vmcb_control_area *c, *h, *g;\n\tunsigned int i;\n\n\tvmcb_mark_dirty(svm->vmcb, VMCB_INTERCEPTS);\n\n\tif (!is_guest_mode(&svm->vcpu))\n\t\treturn;\n\n\tc = &svm->vmcb->control;\n\th = &svm->vmcb01.ptr->control;\n\tg = &svm->nested.ctl;\n\n\tfor (i = 0; i < MAX_INTERCEPT; i++)\n\t\tc->intercepts[i] = h->intercepts[i];\n\n\tif (g->int_ctl & V_INTR_MASKING_MASK) {\n\t\t/* We only want the cr8 intercept bits of L1 */\n\t\tvmcb_clr_intercept(c, INTERCEPT_CR8_READ);\n\t\tvmcb_clr_intercept(c, INTERCEPT_CR8_WRITE);\n\n\t\t/*\n\t\t * Once running L2 with HF_VINTR_MASK, EFLAGS.IF does not\n\t\t * affect any interrupt we may want to inject; therefore,\n\t\t * interrupt window vmexits are irrelevant to L0.\n\t\t */\n\t\tvmcb_clr_intercept(c, INTERCEPT_VINTR);\n\t}\n\n\t/* We don't want to see VMMCALLs from a nested guest */\n\tvmcb_clr_intercept(c, INTERCEPT_VMMCALL);\n\n\tfor (i = 0; i < MAX_INTERCEPT; i++)\n\t\tc->intercepts[i] |= g->intercepts[i];\n\n\t/* If SMI is not intercepted, ignore guest SMI intercept as well  */\n\tif (!intercept_smi)\n\t\tvmcb_clr_intercept(c, INTERCEPT_SMI);\n\n\tvmcb_set_intercept(c, INTERCEPT_VMLOAD);\n\tvmcb_set_intercept(c, INTERCEPT_VMSAVE);\n}\n\nstatic void copy_vmcb_control_area(struct vmcb_control_area *dst,\n\t\t\t\t   struct vmcb_control_area *from)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < MAX_INTERCEPT; i++)\n\t\tdst->intercepts[i] = from->intercepts[i];\n\n\tdst->iopm_base_pa         = from->iopm_base_pa;\n\tdst->msrpm_base_pa        = from->msrpm_base_pa;\n\tdst->tsc_offset           = from->tsc_offset;\n\t/* asid not copied, it is handled manually for svm->vmcb.  */\n\tdst->tlb_ctl              = from->tlb_ctl;\n\tdst->int_ctl              = from->int_ctl;\n\tdst->int_vector           = from->int_vector;\n\tdst->int_state            = from->int_state;\n\tdst->exit_code            = from->exit_code;\n\tdst->exit_code_hi         = from->exit_code_hi;\n\tdst->exit_info_1          = from->exit_info_1;\n\tdst->exit_info_2          = from->exit_info_2;\n\tdst->exit_int_info        = from->exit_int_info;\n\tdst->exit_int_info_err    = from->exit_int_info_err;\n\tdst->nested_ctl           = from->nested_ctl;\n\tdst->event_inj            = from->event_inj;\n\tdst->event_inj_err        = from->event_inj_err;\n\tdst->nested_cr3           = from->nested_cr3;\n\tdst->virt_ext              = from->virt_ext;\n\tdst->pause_filter_count   = from->pause_filter_count;\n\tdst->pause_filter_thresh  = from->pause_filter_thresh;\n}\n\nstatic bool nested_svm_vmrun_msrpm(struct vcpu_svm *svm)\n{\n\t/*\n\t * This function merges the msr permission bitmaps of kvm and the\n\t * nested vmcb. It is optimized in that it only merges the parts where\n\t * the kvm msr permission bitmap may contain zero bits\n\t */\n\tint i;\n\n\tif (!(vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_MSR_PROT)))\n\t\treturn true;\n\n\tfor (i = 0; i < MSRPM_OFFSETS; i++) {\n\t\tu32 value, p;\n\t\tu64 offset;\n\n\t\tif (msrpm_offsets[i] == 0xffffffff)\n\t\t\tbreak;\n\n\t\tp      = msrpm_offsets[i];\n\t\toffset = svm->nested.ctl.msrpm_base_pa + (p * 4);\n\n\t\tif (kvm_vcpu_read_guest(&svm->vcpu, offset, &value, 4))\n\t\t\treturn false;\n\n\t\tsvm->nested.msrpm[p] = svm->msrpm[p] | value;\n\t}\n\n\tsvm->vmcb->control.msrpm_base_pa = __sme_set(__pa(svm->nested.msrpm));\n\n\treturn true;\n}\n\n/*\n * Bits 11:0 of bitmap address are ignored by hardware\n */\nstatic bool nested_svm_check_bitmap_pa(struct kvm_vcpu *vcpu, u64 pa, u32 size)\n{\n\tu64 addr = PAGE_ALIGN(pa);\n\n\treturn kvm_vcpu_is_legal_gpa(vcpu, addr) &&\n\t    kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);\n}\n\nstatic bool nested_vmcb_check_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcb_control_area *control)\n{\n\tif (CC(!vmcb_is_intercept(control, INTERCEPT_VMRUN)))\n\t\treturn false;\n\n\tif (CC(control->asid == 0))\n\t\treturn false;\n\n\tif (CC((control->nested_ctl & SVM_NESTED_CTL_NP_ENABLE) && !npt_enabled))\n\t\treturn false;\n\n\tif (CC(!nested_svm_check_bitmap_pa(vcpu, control->msrpm_base_pa,\n\t\t\t\t\t   MSRPM_SIZE)))\n\t\treturn false;\n\tif (CC(!nested_svm_check_bitmap_pa(vcpu, control->iopm_base_pa,\n\t\t\t\t\t   IOPM_SIZE)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool nested_vmcb_check_cr3_cr4(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct vmcb_save_area *save)\n{\n\t/*\n\t * These checks are also performed by KVM_SET_SREGS,\n\t * except that EFER.LMA is not checked by SVM against\n\t * CR0.PG && EFER.LME.\n\t */\n\tif ((save->efer & EFER_LME) && (save->cr0 & X86_CR0_PG)) {\n\t\tif (CC(!(save->cr4 & X86_CR4_PAE)) ||\n\t\t    CC(!(save->cr0 & X86_CR0_PE)) ||\n\t\t    CC(kvm_vcpu_is_illegal_gpa(vcpu, save->cr3)))\n\t\t\treturn false;\n\t}\n\n\tif (CC(!kvm_is_valid_cr4(vcpu, save->cr4)))\n\t\treturn false;\n\n\treturn true;\n}\n\n/* Common checks that apply to both L1 and L2 state.  */\nstatic bool nested_vmcb_valid_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct vmcb_save_area *save)\n{\n\t/*\n\t * FIXME: these should be done after copying the fields,\n\t * to avoid TOC/TOU races.  For these save area checks\n\t * the possible damage is limited since kvm_set_cr0 and\n\t * kvm_set_cr4 handle failure; EFER_SVME is an exception\n\t * so it is force-set later in nested_prepare_vmcb_save.\n\t */\n\tif (CC(!(save->efer & EFER_SVME)))\n\t\treturn false;\n\n\tif (CC((save->cr0 & X86_CR0_CD) == 0 && (save->cr0 & X86_CR0_NW)) ||\n\t    CC(save->cr0 & ~0xffffffffULL))\n\t\treturn false;\n\n\tif (CC(!kvm_dr6_valid(save->dr6)) || CC(!kvm_dr7_valid(save->dr7)))\n\t\treturn false;\n\n\tif (!nested_vmcb_check_cr3_cr4(vcpu, save))\n\t\treturn false;\n\n\tif (CC(!kvm_valid_efer(vcpu, save->efer)))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid nested_load_control_from_vmcb12(struct vcpu_svm *svm,\n\t\t\t\t     struct vmcb_control_area *control)\n{\n\tcopy_vmcb_control_area(&svm->nested.ctl, control);\n\n\t/* Copy it here because nested_svm_check_controls will check it.  */\n\tsvm->nested.ctl.asid           = control->asid;\n\tsvm->nested.ctl.msrpm_base_pa &= ~0x0fffULL;\n\tsvm->nested.ctl.iopm_base_pa  &= ~0x0fffULL;\n}\n\n/*\n * Synchronize fields that are written by the processor, so that\n * they can be copied back into the vmcb12.\n */\nvoid nested_sync_control_from_vmcb02(struct vcpu_svm *svm)\n{\n\tu32 mask;\n\tsvm->nested.ctl.event_inj      = svm->vmcb->control.event_inj;\n\tsvm->nested.ctl.event_inj_err  = svm->vmcb->control.event_inj_err;\n\n\t/* Only a few fields of int_ctl are written by the processor.  */\n\tmask = V_IRQ_MASK | V_TPR_MASK;\n\tif (!(svm->nested.ctl.int_ctl & V_INTR_MASKING_MASK) &&\n\t    svm_is_intercept(svm, INTERCEPT_VINTR)) {\n\t\t/*\n\t\t * In order to request an interrupt window, L0 is usurping\n\t\t * svm->vmcb->control.int_ctl and possibly setting V_IRQ\n\t\t * even if it was clear in L1's VMCB.  Restoring it would be\n\t\t * wrong.  However, in this case V_IRQ will remain true until\n\t\t * interrupt_window_interception calls svm_clear_vintr and\n\t\t * restores int_ctl.  We can just leave it aside.\n\t\t */\n\t\tmask &= ~V_IRQ_MASK;\n\t}\n\tsvm->nested.ctl.int_ctl        &= ~mask;\n\tsvm->nested.ctl.int_ctl        |= svm->vmcb->control.int_ctl & mask;\n}\n\n/*\n * Transfer any event that L0 or L1 wanted to inject into L2 to\n * EXIT_INT_INFO.\n */\nstatic void nested_save_pending_event_to_vmcb12(struct vcpu_svm *svm,\n\t\t\t\t\t\tstruct vmcb *vmcb12)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tu32 exit_int_info = 0;\n\tunsigned int nr;\n\n\tif (vcpu->arch.exception.injected) {\n\t\tnr = vcpu->arch.exception.nr;\n\t\texit_int_info = nr | SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_EXEPT;\n\n\t\tif (vcpu->arch.exception.has_error_code) {\n\t\t\texit_int_info |= SVM_EVTINJ_VALID_ERR;\n\t\t\tvmcb12->control.exit_int_info_err =\n\t\t\t\tvcpu->arch.exception.error_code;\n\t\t}\n\n\t} else if (vcpu->arch.nmi_injected) {\n\t\texit_int_info = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;\n\n\t} else if (vcpu->arch.interrupt.injected) {\n\t\tnr = vcpu->arch.interrupt.nr;\n\t\texit_int_info = nr | SVM_EVTINJ_VALID;\n\n\t\tif (vcpu->arch.interrupt.soft)\n\t\t\texit_int_info |= SVM_EVTINJ_TYPE_SOFT;\n\t\telse\n\t\t\texit_int_info |= SVM_EVTINJ_TYPE_INTR;\n\t}\n\n\tvmcb12->control.exit_int_info = exit_int_info;\n}\n\nstatic inline bool nested_npt_enabled(struct vcpu_svm *svm)\n{\n\treturn svm->nested.ctl.nested_ctl & SVM_NESTED_CTL_NP_ENABLE;\n}\n\nstatic void nested_svm_transition_tlb_flush(struct kvm_vcpu *vcpu)\n{\n\t/*\n\t * TODO: optimize unconditional TLB flush/MMU sync.  A partial list of\n\t * things to fix before this can be conditional:\n\t *\n\t *  - Flush TLBs for both L1 and L2 remote TLB flush\n\t *  - Honor L1's request to flush an ASID on nested VMRUN\n\t *  - Sync nested NPT MMU on VMRUN that flushes L2's ASID[*]\n\t *  - Don't crush a pending TLB flush in vmcb02 on nested VMRUN\n\t *  - Flush L1's ASID on KVM_REQ_TLB_FLUSH_GUEST\n\t *\n\t * [*] Unlike nested EPT, SVM's ASID management can invalidate nested\n\t *     NPT guest-physical mappings on VMRUN.\n\t */\n\tkvm_make_request(KVM_REQ_MMU_SYNC, vcpu);\n\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n}\n\n/*\n * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true\n * if we are emulating VM-Entry into a guest with NPT enabled.\n */\nstatic int nested_svm_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3,\n\t\t\t       bool nested_npt, bool reload_pdptrs)\n{\n\tif (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3)))\n\t\treturn -EINVAL;\n\n\tif (reload_pdptrs && !nested_npt && is_pae_paging(vcpu) &&\n\t    CC(!load_pdptrs(vcpu, vcpu->arch.walk_mmu, cr3)))\n\t\treturn -EINVAL;\n\n\tif (!nested_npt)\n\t\tkvm_mmu_new_pgd(vcpu, cr3);\n\n\tvcpu->arch.cr3 = cr3;\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_CR3);\n\n\t/* Re-initialize the MMU, e.g. to pick up CR4 MMU role changes. */\n\tkvm_init_mmu(vcpu);\n\n\treturn 0;\n}\n\nvoid nested_vmcb02_compute_g_pat(struct vcpu_svm *svm)\n{\n\tif (!svm->nested.vmcb02.ptr)\n\t\treturn;\n\n\t/* FIXME: merge g_pat from vmcb01 and vmcb12.  */\n\tsvm->nested.vmcb02.ptr->save.g_pat = svm->vmcb01.ptr->save.g_pat;\n}\n\nstatic void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12)\n{\n\tbool new_vmcb12 = false;\n\n\tnested_vmcb02_compute_g_pat(svm);\n\n\t/* Load the nested guest state */\n\tif (svm->nested.vmcb12_gpa != svm->nested.last_vmcb12_gpa) {\n\t\tnew_vmcb12 = true;\n\t\tsvm->nested.last_vmcb12_gpa = svm->nested.vmcb12_gpa;\n\t}\n\n\tif (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_SEG))) {\n\t\tsvm->vmcb->save.es = vmcb12->save.es;\n\t\tsvm->vmcb->save.cs = vmcb12->save.cs;\n\t\tsvm->vmcb->save.ss = vmcb12->save.ss;\n\t\tsvm->vmcb->save.ds = vmcb12->save.ds;\n\t\tsvm->vmcb->save.cpl = vmcb12->save.cpl;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_SEG);\n\t}\n\n\tif (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_DT))) {\n\t\tsvm->vmcb->save.gdtr = vmcb12->save.gdtr;\n\t\tsvm->vmcb->save.idtr = vmcb12->save.idtr;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_DT);\n\t}\n\n\tkvm_set_rflags(&svm->vcpu, vmcb12->save.rflags | X86_EFLAGS_FIXED);\n\n\t/*\n\t * Force-set EFER_SVME even though it is checked earlier on the\n\t * VMCB12, because the guest can flip the bit between the check\n\t * and now.  Clearing EFER_SVME would call svm_free_nested.\n\t */\n\tsvm_set_efer(&svm->vcpu, vmcb12->save.efer | EFER_SVME);\n\n\tsvm_set_cr0(&svm->vcpu, vmcb12->save.cr0);\n\tsvm_set_cr4(&svm->vcpu, vmcb12->save.cr4);\n\n\tsvm->vcpu.arch.cr2 = vmcb12->save.cr2;\n\n\tkvm_rax_write(&svm->vcpu, vmcb12->save.rax);\n\tkvm_rsp_write(&svm->vcpu, vmcb12->save.rsp);\n\tkvm_rip_write(&svm->vcpu, vmcb12->save.rip);\n\n\t/* In case we don't even reach vcpu_run, the fields are not updated */\n\tsvm->vmcb->save.rax = vmcb12->save.rax;\n\tsvm->vmcb->save.rsp = vmcb12->save.rsp;\n\tsvm->vmcb->save.rip = vmcb12->save.rip;\n\n\t/* These bits will be set properly on the first execution when new_vmc12 is true */\n\tif (unlikely(new_vmcb12 || vmcb_is_dirty(vmcb12, VMCB_DR))) {\n\t\tsvm->vmcb->save.dr7 = vmcb12->save.dr7 | DR7_FIXED_1;\n\t\tsvm->vcpu.arch.dr6  = vmcb12->save.dr6 | DR6_ACTIVE_LOW;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_DR);\n\t}\n}\n\nstatic void nested_vmcb02_prepare_control(struct vcpu_svm *svm)\n{\n\tconst u32 int_ctl_vmcb01_bits =\n\t\tV_INTR_MASKING_MASK | V_GIF_MASK | V_GIF_ENABLE_MASK;\n\n\tconst u32 int_ctl_vmcb12_bits = V_TPR_MASK | V_IRQ_INJECTION_BITS_MASK;\n\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\n\t/*\n\t * Filled at exit: exit_code, exit_code_hi, exit_info_1, exit_info_2,\n\t * exit_int_info, exit_int_info_err, next_rip, insn_len, insn_bytes.\n\t */\n\n\t/*\n\t * Also covers avic_vapic_bar, avic_backing_page, avic_logical_id,\n\t * avic_physical_id.\n\t */\n\tWARN_ON(kvm_apicv_activated(svm->vcpu.kvm));\n\n\t/* Copied from vmcb01.  msrpm_base can be overwritten later.  */\n\tsvm->vmcb->control.nested_ctl = svm->vmcb01.ptr->control.nested_ctl;\n\tsvm->vmcb->control.iopm_base_pa = svm->vmcb01.ptr->control.iopm_base_pa;\n\tsvm->vmcb->control.msrpm_base_pa = svm->vmcb01.ptr->control.msrpm_base_pa;\n\n\t/* Done at vmrun: asid.  */\n\n\t/* Also overwritten later if necessary.  */\n\tsvm->vmcb->control.tlb_ctl = TLB_CONTROL_DO_NOTHING;\n\n\t/* nested_cr3.  */\n\tif (nested_npt_enabled(svm))\n\t\tnested_svm_init_mmu_context(vcpu);\n\n\tsvm->vmcb->control.tsc_offset = vcpu->arch.tsc_offset =\n\t\tvcpu->arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;\n\n\tsvm->vmcb->control.int_ctl             =\n\t\t(svm->nested.ctl.int_ctl & int_ctl_vmcb12_bits) |\n\t\t(svm->vmcb01.ptr->control.int_ctl & int_ctl_vmcb01_bits);\n\n\tsvm->vmcb->control.virt_ext            = svm->nested.ctl.virt_ext;\n\tsvm->vmcb->control.int_vector          = svm->nested.ctl.int_vector;\n\tsvm->vmcb->control.int_state           = svm->nested.ctl.int_state;\n\tsvm->vmcb->control.event_inj           = svm->nested.ctl.event_inj;\n\tsvm->vmcb->control.event_inj_err       = svm->nested.ctl.event_inj_err;\n\n\tsvm->vmcb->control.pause_filter_count  = svm->nested.ctl.pause_filter_count;\n\tsvm->vmcb->control.pause_filter_thresh = svm->nested.ctl.pause_filter_thresh;\n\n\tnested_svm_transition_tlb_flush(vcpu);\n\n\t/* Enter Guest-Mode */\n\tenter_guest_mode(vcpu);\n\n\t/*\n\t * Merge guest and host intercepts - must be called with vcpu in\n\t * guest-mode to take effect.\n\t */\n\trecalc_intercepts(svm);\n}\n\nstatic void nested_svm_copy_common_state(struct vmcb *from_vmcb, struct vmcb *to_vmcb)\n{\n\t/*\n\t * Some VMCB state is shared between L1 and L2 and thus has to be\n\t * moved at the time of nested vmrun and vmexit.\n\t *\n\t * VMLOAD/VMSAVE state would also belong in this category, but KVM\n\t * always performs VMLOAD and VMSAVE from the VMCB01.\n\t */\n\tto_vmcb->save.spec_ctrl = from_vmcb->save.spec_ctrl;\n}\n\nint enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint ret;\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\n\tWARN_ON(svm->vmcb == svm->nested.vmcb02.ptr);\n\n\tnested_svm_copy_common_state(svm->vmcb01.ptr, svm->nested.vmcb02.ptr);\n\n\tsvm_switch_vmcb(svm, &svm->nested.vmcb02);\n\tnested_vmcb02_prepare_control(svm);\n\tnested_vmcb02_prepare_save(svm, vmcb12);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm), true);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!npt_enabled)\n\t\tvcpu->arch.mmu->inject_page_fault = svm_inject_page_fault_nested;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}\n\nint nested_svm_vmrun(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (!svm->nested.hsave_msr) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tif (is_smm(vcpu)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tnested_load_control_from_vmcb12(svm, &vmcb12->control);\n\n\tif (!nested_vmcb_valid_sregs(vcpu, &vmcb12->save) ||\n\t    !nested_vmcb_check_controls(vcpu, &svm->nested.ctl)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\t/*\n\t * Since vmcb01 is not in use, we can use it to store some of the L1\n\t * state.\n\t */\n\tsvm->vmcb01.ptr->save.efer   = vcpu->arch.efer;\n\tsvm->vmcb01.ptr->save.cr0    = kvm_read_cr0(vcpu);\n\tsvm->vmcb01.ptr->save.cr4    = vcpu->arch.cr4;\n\tsvm->vmcb01.ptr->save.rflags = kvm_get_rflags(vcpu);\n\tsvm->vmcb01.ptr->save.rip    = kvm_rip_read(vcpu);\n\n\tif (!npt_enabled)\n\t\tsvm->vmcb01.ptr->save.cr3 = kvm_read_cr3(vcpu);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(vcpu, &map, true);\n\n\treturn ret;\n}\n\n/* Copy state save area fields which are handled by VMRUN */\nvoid svm_copy_vmrun_state(struct vmcb_save_area *to_save,\n\t\t\t  struct vmcb_save_area *from_save)\n{\n\tto_save->es = from_save->es;\n\tto_save->cs = from_save->cs;\n\tto_save->ss = from_save->ss;\n\tto_save->ds = from_save->ds;\n\tto_save->gdtr = from_save->gdtr;\n\tto_save->idtr = from_save->idtr;\n\tto_save->rflags = from_save->rflags | X86_EFLAGS_FIXED;\n\tto_save->efer = from_save->efer;\n\tto_save->cr0 = from_save->cr0;\n\tto_save->cr3 = from_save->cr3;\n\tto_save->cr4 = from_save->cr4;\n\tto_save->rax = from_save->rax;\n\tto_save->rsp = from_save->rsp;\n\tto_save->rip = from_save->rip;\n\tto_save->cpl = 0;\n}\n\nvoid svm_copy_vmloadsave_state(struct vmcb *to_vmcb, struct vmcb *from_vmcb)\n{\n\tto_vmcb->save.fs = from_vmcb->save.fs;\n\tto_vmcb->save.gs = from_vmcb->save.gs;\n\tto_vmcb->save.tr = from_vmcb->save.tr;\n\tto_vmcb->save.ldtr = from_vmcb->save.ldtr;\n\tto_vmcb->save.kernel_gs_base = from_vmcb->save.kernel_gs_base;\n\tto_vmcb->save.star = from_vmcb->save.star;\n\tto_vmcb->save.lstar = from_vmcb->save.lstar;\n\tto_vmcb->save.cstar = from_vmcb->save.cstar;\n\tto_vmcb->save.sfmask = from_vmcb->save.sfmask;\n\tto_vmcb->save.sysenter_cs = from_vmcb->save.sysenter_cs;\n\tto_vmcb->save.sysenter_esp = from_vmcb->save.sysenter_esp;\n\tto_vmcb->save.sysenter_eip = from_vmcb->save.sysenter_eip;\n}\n\nint nested_svm_vmexit(struct vcpu_svm *svm)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tint rc;\n\n\t/* Triple faults in L2 should never escape. */\n\tWARN_ON_ONCE(kvm_check_request(KVM_REQ_TRIPLE_FAULT, vcpu));\n\n\trc = kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.vmcb12_gpa), &map);\n\tif (rc) {\n\t\tif (rc == -EINVAL)\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tvmcb12 = map.hva;\n\n\t/* Exit Guest-Mode */\n\tleave_guest_mode(vcpu);\n\tsvm->nested.vmcb12_gpa = 0;\n\tWARN_ON_ONCE(svm->nested.nested_run_pending);\n\n\tkvm_clear_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n\n\t/* in case we halted in L2 */\n\tsvm->vcpu.arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\t/* Give the current vmcb to the guest */\n\n\tvmcb12->save.es     = vmcb->save.es;\n\tvmcb12->save.cs     = vmcb->save.cs;\n\tvmcb12->save.ss     = vmcb->save.ss;\n\tvmcb12->save.ds     = vmcb->save.ds;\n\tvmcb12->save.gdtr   = vmcb->save.gdtr;\n\tvmcb12->save.idtr   = vmcb->save.idtr;\n\tvmcb12->save.efer   = svm->vcpu.arch.efer;\n\tvmcb12->save.cr0    = kvm_read_cr0(vcpu);\n\tvmcb12->save.cr3    = kvm_read_cr3(vcpu);\n\tvmcb12->save.cr2    = vmcb->save.cr2;\n\tvmcb12->save.cr4    = svm->vcpu.arch.cr4;\n\tvmcb12->save.rflags = kvm_get_rflags(vcpu);\n\tvmcb12->save.rip    = kvm_rip_read(vcpu);\n\tvmcb12->save.rsp    = kvm_rsp_read(vcpu);\n\tvmcb12->save.rax    = kvm_rax_read(vcpu);\n\tvmcb12->save.dr7    = vmcb->save.dr7;\n\tvmcb12->save.dr6    = svm->vcpu.arch.dr6;\n\tvmcb12->save.cpl    = vmcb->save.cpl;\n\n\tvmcb12->control.int_state         = vmcb->control.int_state;\n\tvmcb12->control.exit_code         = vmcb->control.exit_code;\n\tvmcb12->control.exit_code_hi      = vmcb->control.exit_code_hi;\n\tvmcb12->control.exit_info_1       = vmcb->control.exit_info_1;\n\tvmcb12->control.exit_info_2       = vmcb->control.exit_info_2;\n\n\tif (vmcb12->control.exit_code != SVM_EXIT_ERR)\n\t\tnested_save_pending_event_to_vmcb12(svm, vmcb12);\n\n\tif (svm->nrips_enabled)\n\t\tvmcb12->control.next_rip  = vmcb->control.next_rip;\n\n\tvmcb12->control.int_ctl           = svm->nested.ctl.int_ctl;\n\tvmcb12->control.tlb_ctl           = svm->nested.ctl.tlb_ctl;\n\tvmcb12->control.event_inj         = svm->nested.ctl.event_inj;\n\tvmcb12->control.event_inj_err     = svm->nested.ctl.event_inj_err;\n\n\tvmcb12->control.pause_filter_count =\n\t\tsvm->vmcb->control.pause_filter_count;\n\tvmcb12->control.pause_filter_thresh =\n\t\tsvm->vmcb->control.pause_filter_thresh;\n\n\tnested_svm_copy_common_state(svm->nested.vmcb02.ptr, svm->vmcb01.ptr);\n\n\tsvm_switch_vmcb(svm, &svm->vmcb01);\n\n\t/*\n\t * On vmexit the  GIF is set to false and\n\t * no event can be injected in L1.\n\t */\n\tsvm_set_gif(svm, false);\n\tsvm->vmcb->control.exit_int_info = 0;\n\n\tsvm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;\n\tif (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {\n\t\tsvm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_INTERCEPTS);\n\t}\n\n\tsvm->nested.ctl.nested_cr3 = 0;\n\n\t/*\n\t * Restore processor state that had been saved in vmcb01\n\t */\n\tkvm_set_rflags(vcpu, svm->vmcb->save.rflags);\n\tsvm_set_efer(vcpu, svm->vmcb->save.efer);\n\tsvm_set_cr0(vcpu, svm->vmcb->save.cr0 | X86_CR0_PE);\n\tsvm_set_cr4(vcpu, svm->vmcb->save.cr4);\n\tkvm_rax_write(vcpu, svm->vmcb->save.rax);\n\tkvm_rsp_write(vcpu, svm->vmcb->save.rsp);\n\tkvm_rip_write(vcpu, svm->vmcb->save.rip);\n\n\tsvm->vcpu.arch.dr7 = DR7_FIXED_1;\n\tkvm_update_dr7(&svm->vcpu);\n\n\ttrace_kvm_nested_vmexit_inject(vmcb12->control.exit_code,\n\t\t\t\t       vmcb12->control.exit_info_1,\n\t\t\t\t       vmcb12->control.exit_info_2,\n\t\t\t\t       vmcb12->control.exit_int_info,\n\t\t\t\t       vmcb12->control.exit_int_info_err,\n\t\t\t\t       KVM_ISA_SVM);\n\n\tkvm_vcpu_unmap(vcpu, &map, true);\n\n\tnested_svm_transition_tlb_flush(vcpu);\n\n\tnested_svm_uninit_mmu_context(vcpu);\n\n\trc = nested_svm_load_cr3(vcpu, svm->vmcb->save.cr3, false, true);\n\tif (rc)\n\t\treturn 1;\n\n\t/*\n\t * Drop what we picked up for L2 via svm_complete_interrupts() so it\n\t * doesn't end up in L1.\n\t */\n\tsvm->vcpu.arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\t/*\n\t * If we are here following the completion of a VMRUN that\n\t * is being single-stepped, queue the pending #DB intercept\n\t * right now so that it an be accounted for before we execute\n\t * L1's next instruction.\n\t */\n\tif (unlikely(svm->vmcb->save.rflags & X86_EFLAGS_TF))\n\t\tkvm_queue_exception(&(svm->vcpu), DB_VECTOR);\n\n\treturn 0;\n}\n\nstatic void nested_svm_triple_fault(struct kvm_vcpu *vcpu)\n{\n\tnested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);\n}\n\nint svm_allocate_nested(struct vcpu_svm *svm)\n{\n\tstruct page *vmcb02_page;\n\n\tif (svm->nested.initialized)\n\t\treturn 0;\n\n\tvmcb02_page = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\tif (!vmcb02_page)\n\t\treturn -ENOMEM;\n\tsvm->nested.vmcb02.ptr = page_address(vmcb02_page);\n\tsvm->nested.vmcb02.pa = __sme_set(page_to_pfn(vmcb02_page) << PAGE_SHIFT);\n\n\tsvm->nested.msrpm = svm_vcpu_alloc_msrpm();\n\tif (!svm->nested.msrpm)\n\t\tgoto err_free_vmcb02;\n\tsvm_vcpu_init_msrpm(&svm->vcpu, svm->nested.msrpm);\n\n\tsvm->nested.initialized = true;\n\treturn 0;\n\nerr_free_vmcb02:\n\t__free_page(vmcb02_page);\n\treturn -ENOMEM;\n}\n\nvoid svm_free_nested(struct vcpu_svm *svm)\n{\n\tif (!svm->nested.initialized)\n\t\treturn;\n\n\tsvm_vcpu_free_msrpm(svm->nested.msrpm);\n\tsvm->nested.msrpm = NULL;\n\n\t__free_page(virt_to_page(svm->nested.vmcb02.ptr));\n\tsvm->nested.vmcb02.ptr = NULL;\n\n\t/*\n\t * When last_vmcb12_gpa matches the current vmcb12 gpa,\n\t * some vmcb12 fields are not loaded if they are marked clean\n\t * in the vmcb12, since in this case they are up to date already.\n\t *\n\t * When the vmcb02 is freed, this optimization becomes invalid.\n\t */\n\tsvm->nested.last_vmcb12_gpa = INVALID_GPA;\n\n\tsvm->nested.initialized = false;\n}\n\n/*\n * Forcibly leave nested mode in order to be able to reset the VCPU later on.\n */\nvoid svm_leave_nested(struct vcpu_svm *svm)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tsvm->nested.nested_run_pending = 0;\n\t\tsvm->nested.vmcb12_gpa = INVALID_GPA;\n\n\t\tleave_guest_mode(vcpu);\n\n\t\tsvm_switch_vmcb(svm, &svm->vmcb01);\n\n\t\tnested_svm_uninit_mmu_context(vcpu);\n\t\tvmcb_mark_all_dirty(svm->vmcb);\n\t}\n\n\tkvm_clear_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n}\n\nstatic int nested_svm_exit_handled_msr(struct vcpu_svm *svm)\n{\n\tu32 offset, msr, value;\n\tint write, mask;\n\n\tif (!(vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_MSR_PROT)))\n\t\treturn NESTED_EXIT_HOST;\n\n\tmsr    = svm->vcpu.arch.regs[VCPU_REGS_RCX];\n\toffset = svm_msrpm_offset(msr);\n\twrite  = svm->vmcb->control.exit_info_1 & 1;\n\tmask   = 1 << ((2 * (msr & 0xf)) + write);\n\n\tif (offset == MSR_INVALID)\n\t\treturn NESTED_EXIT_DONE;\n\n\t/* Offset is in 32 bit units but need in 8 bit units */\n\toffset *= 4;\n\n\tif (kvm_vcpu_read_guest(&svm->vcpu, svm->nested.ctl.msrpm_base_pa + offset, &value, 4))\n\t\treturn NESTED_EXIT_DONE;\n\n\treturn (value & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;\n}\n\nstatic int nested_svm_intercept_ioio(struct vcpu_svm *svm)\n{\n\tunsigned port, size, iopm_len;\n\tu16 val, mask;\n\tu8 start_bit;\n\tu64 gpa;\n\n\tif (!(vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_IOIO_PROT)))\n\t\treturn NESTED_EXIT_HOST;\n\n\tport = svm->vmcb->control.exit_info_1 >> 16;\n\tsize = (svm->vmcb->control.exit_info_1 & SVM_IOIO_SIZE_MASK) >>\n\t\tSVM_IOIO_SIZE_SHIFT;\n\tgpa  = svm->nested.ctl.iopm_base_pa + (port / 8);\n\tstart_bit = port % 8;\n\tiopm_len = (start_bit + size > 8) ? 2 : 1;\n\tmask = (0xf >> (4 - size)) << start_bit;\n\tval = 0;\n\n\tif (kvm_vcpu_read_guest(&svm->vcpu, gpa, &val, iopm_len))\n\t\treturn NESTED_EXIT_DONE;\n\n\treturn (val & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;\n}\n\nstatic int nested_svm_intercept(struct vcpu_svm *svm)\n{\n\tu32 exit_code = svm->vmcb->control.exit_code;\n\tint vmexit = NESTED_EXIT_HOST;\n\n\tswitch (exit_code) {\n\tcase SVM_EXIT_MSR:\n\t\tvmexit = nested_svm_exit_handled_msr(svm);\n\t\tbreak;\n\tcase SVM_EXIT_IOIO:\n\t\tvmexit = nested_svm_intercept_ioio(svm);\n\t\tbreak;\n\tcase SVM_EXIT_READ_CR0 ... SVM_EXIT_WRITE_CR8: {\n\t\tif (vmcb_is_intercept(&svm->nested.ctl, exit_code))\n\t\t\tvmexit = NESTED_EXIT_DONE;\n\t\tbreak;\n\t}\n\tcase SVM_EXIT_READ_DR0 ... SVM_EXIT_WRITE_DR7: {\n\t\tif (vmcb_is_intercept(&svm->nested.ctl, exit_code))\n\t\t\tvmexit = NESTED_EXIT_DONE;\n\t\tbreak;\n\t}\n\tcase SVM_EXIT_EXCP_BASE ... SVM_EXIT_EXCP_BASE + 0x1f: {\n\t\t/*\n\t\t * Host-intercepted exceptions have been checked already in\n\t\t * nested_svm_exit_special.  There is nothing to do here,\n\t\t * the vmexit is injected by svm_check_nested_events.\n\t\t */\n\t\tvmexit = NESTED_EXIT_DONE;\n\t\tbreak;\n\t}\n\tcase SVM_EXIT_ERR: {\n\t\tvmexit = NESTED_EXIT_DONE;\n\t\tbreak;\n\t}\n\tdefault: {\n\t\tif (vmcb_is_intercept(&svm->nested.ctl, exit_code))\n\t\t\tvmexit = NESTED_EXIT_DONE;\n\t}\n\t}\n\n\treturn vmexit;\n}\n\nint nested_svm_exit_handled(struct vcpu_svm *svm)\n{\n\tint vmexit;\n\n\tvmexit = nested_svm_intercept(svm);\n\n\tif (vmexit == NESTED_EXIT_DONE)\n\t\tnested_svm_vmexit(svm);\n\n\treturn vmexit;\n}\n\nint nested_svm_check_permissions(struct kvm_vcpu *vcpu)\n{\n\tif (!(vcpu->arch.efer & EFER_SVME) || !is_paging(vcpu)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (to_svm(vcpu)->vmcb->save.cpl) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic bool nested_exit_on_exception(struct vcpu_svm *svm)\n{\n\tunsigned int nr = svm->vcpu.arch.exception.nr;\n\n\treturn (svm->nested.ctl.intercepts[INTERCEPT_EXCEPTION] & BIT(nr));\n}\n\nstatic void nested_svm_inject_exception_vmexit(struct vcpu_svm *svm)\n{\n\tunsigned int nr = svm->vcpu.arch.exception.nr;\n\n\tsvm->vmcb->control.exit_code = SVM_EXIT_EXCP_BASE + nr;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\n\tif (svm->vcpu.arch.exception.has_error_code)\n\t\tsvm->vmcb->control.exit_info_1 = svm->vcpu.arch.exception.error_code;\n\n\t/*\n\t * EXITINFO2 is undefined for all exception intercepts other\n\t * than #PF.\n\t */\n\tif (nr == PF_VECTOR) {\n\t\tif (svm->vcpu.arch.exception.nested_apf)\n\t\t\tsvm->vmcb->control.exit_info_2 = svm->vcpu.arch.apf.nested_apf_token;\n\t\telse if (svm->vcpu.arch.exception.has_payload)\n\t\t\tsvm->vmcb->control.exit_info_2 = svm->vcpu.arch.exception.payload;\n\t\telse\n\t\t\tsvm->vmcb->control.exit_info_2 = svm->vcpu.arch.cr2;\n\t} else if (nr == DB_VECTOR) {\n\t\t/* See inject_pending_event.  */\n\t\tkvm_deliver_exception_payload(&svm->vcpu);\n\t\tif (svm->vcpu.arch.dr7 & DR7_GD) {\n\t\t\tsvm->vcpu.arch.dr7 &= ~DR7_GD;\n\t\t\tkvm_update_dr7(&svm->vcpu);\n\t\t}\n\t} else\n\t\tWARN_ON(svm->vcpu.arch.exception.has_payload);\n\n\tnested_svm_vmexit(svm);\n}\n\nstatic inline bool nested_exit_on_init(struct vcpu_svm *svm)\n{\n\treturn vmcb_is_intercept(&svm->nested.ctl, INTERCEPT_INIT);\n}\n\nstatic int svm_check_nested_events(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tbool block_nested_events =\n\t\tkvm_event_needs_reinjection(vcpu) || svm->nested.nested_run_pending;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (lapic_in_kernel(vcpu) &&\n\t    test_bit(KVM_APIC_INIT, &apic->pending_events)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_init(svm))\n\t\t\treturn 0;\n\t\tnested_svm_simple_vmexit(svm, SVM_EXIT_INIT);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.exception.pending) {\n\t\t/*\n\t\t * Only a pending nested run can block a pending exception.\n\t\t * Otherwise an injected NMI/interrupt should either be\n\t\t * lost or delivered to the nested hypervisor in the EXITINTINFO\n\t\t * vmcb field, while delivering the pending exception.\n\t\t */\n\t\tif (svm->nested.nested_run_pending)\n                        return -EBUSY;\n\t\tif (!nested_exit_on_exception(svm))\n\t\t\treturn 0;\n\t\tnested_svm_inject_exception_vmexit(svm);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.smi_pending && !svm_smi_blocked(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_smi(svm))\n\t\t\treturn 0;\n\t\tnested_svm_simple_vmexit(svm, SVM_EXIT_SMI);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.nmi_pending && !svm_nmi_blocked(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_nmi(svm))\n\t\t\treturn 0;\n\t\tnested_svm_simple_vmexit(svm, SVM_EXIT_NMI);\n\t\treturn 0;\n\t}\n\n\tif (kvm_cpu_has_interrupt(vcpu) && !svm_interrupt_blocked(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_intr(svm))\n\t\t\treturn 0;\n\t\ttrace_kvm_nested_intr_vmexit(svm->vmcb->save.rip);\n\t\tnested_svm_simple_vmexit(svm, SVM_EXIT_INTR);\n\t\treturn 0;\n\t}\n\n\treturn 0;\n}\n\nint nested_svm_exit_special(struct vcpu_svm *svm)\n{\n\tu32 exit_code = svm->vmcb->control.exit_code;\n\n\tswitch (exit_code) {\n\tcase SVM_EXIT_INTR:\n\tcase SVM_EXIT_NMI:\n\tcase SVM_EXIT_NPF:\n\t\treturn NESTED_EXIT_HOST;\n\tcase SVM_EXIT_EXCP_BASE ... SVM_EXIT_EXCP_BASE + 0x1f: {\n\t\tu32 excp_bits = 1 << (exit_code - SVM_EXIT_EXCP_BASE);\n\n\t\tif (svm->vmcb01.ptr->control.intercepts[INTERCEPT_EXCEPTION] &\n\t\t    excp_bits)\n\t\t\treturn NESTED_EXIT_HOST;\n\t\telse if (exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR &&\n\t\t\t svm->vcpu.arch.apf.host_apf_flags)\n\t\t\t/* Trap async PF even if not shadowing */\n\t\t\treturn NESTED_EXIT_HOST;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NESTED_EXIT_CONTINUE;\n}\n\nstatic int svm_get_nested_state(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_nested_state __user *user_kvm_nested_state,\n\t\t\t\tu32 user_data_size)\n{\n\tstruct vcpu_svm *svm;\n\tstruct kvm_nested_state kvm_state = {\n\t\t.flags = 0,\n\t\t.format = KVM_STATE_NESTED_FORMAT_SVM,\n\t\t.size = sizeof(kvm_state),\n\t};\n\tstruct vmcb __user *user_vmcb = (struct vmcb __user *)\n\t\t&user_kvm_nested_state->data.svm[0];\n\n\tif (!vcpu)\n\t\treturn kvm_state.size + KVM_STATE_NESTED_SVM_VMCB_SIZE;\n\n\tsvm = to_svm(vcpu);\n\n\tif (user_data_size < kvm_state.size)\n\t\tgoto out;\n\n\t/* First fill in the header and copy it out.  */\n\tif (is_guest_mode(vcpu)) {\n\t\tkvm_state.hdr.svm.vmcb_pa = svm->nested.vmcb12_gpa;\n\t\tkvm_state.size += KVM_STATE_NESTED_SVM_VMCB_SIZE;\n\t\tkvm_state.flags |= KVM_STATE_NESTED_GUEST_MODE;\n\n\t\tif (svm->nested.nested_run_pending)\n\t\t\tkvm_state.flags |= KVM_STATE_NESTED_RUN_PENDING;\n\t}\n\n\tif (gif_set(svm))\n\t\tkvm_state.flags |= KVM_STATE_NESTED_GIF_SET;\n\n\tif (copy_to_user(user_kvm_nested_state, &kvm_state, sizeof(kvm_state)))\n\t\treturn -EFAULT;\n\n\tif (!is_guest_mode(vcpu))\n\t\tgoto out;\n\n\t/*\n\t * Copy over the full size of the VMCB rather than just the size\n\t * of the structs.\n\t */\n\tif (clear_user(user_vmcb, KVM_STATE_NESTED_SVM_VMCB_SIZE))\n\t\treturn -EFAULT;\n\tif (copy_to_user(&user_vmcb->control, &svm->nested.ctl,\n\t\t\t sizeof(user_vmcb->control)))\n\t\treturn -EFAULT;\n\tif (copy_to_user(&user_vmcb->save, &svm->vmcb01.ptr->save,\n\t\t\t sizeof(user_vmcb->save)))\n\t\treturn -EFAULT;\nout:\n\treturn kvm_state.size;\n}\n\nstatic int svm_set_nested_state(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_nested_state __user *user_kvm_nested_state,\n\t\t\t\tstruct kvm_nested_state *kvm_state)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb __user *user_vmcb = (struct vmcb __user *)\n\t\t&user_kvm_nested_state->data.svm[0];\n\tstruct vmcb_control_area *ctl;\n\tstruct vmcb_save_area *save;\n\tunsigned long cr0;\n\tint ret;\n\n\tBUILD_BUG_ON(sizeof(struct vmcb_control_area) + sizeof(struct vmcb_save_area) >\n\t\t     KVM_STATE_NESTED_SVM_VMCB_SIZE);\n\n\tif (kvm_state->format != KVM_STATE_NESTED_FORMAT_SVM)\n\t\treturn -EINVAL;\n\n\tif (kvm_state->flags & ~(KVM_STATE_NESTED_GUEST_MODE |\n\t\t\t\t KVM_STATE_NESTED_RUN_PENDING |\n\t\t\t\t KVM_STATE_NESTED_GIF_SET))\n\t\treturn -EINVAL;\n\n\t/*\n\t * If in guest mode, vcpu->arch.efer actually refers to the L2 guest's\n\t * EFER.SVME, but EFER.SVME still has to be 1 for VMRUN to succeed.\n\t */\n\tif (!(vcpu->arch.efer & EFER_SVME)) {\n\t\t/* GIF=1 and no guest mode are required if SVME=0.  */\n\t\tif (kvm_state->flags != KVM_STATE_NESTED_GIF_SET)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* SMM temporarily disables SVM, so we cannot be in guest mode.  */\n\tif (is_smm(vcpu) && (kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE))\n\t\treturn -EINVAL;\n\n\tif (!(kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE)) {\n\t\tsvm_leave_nested(svm);\n\t\tsvm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));\n\t\treturn 0;\n\t}\n\n\tif (!page_address_valid(vcpu, kvm_state->hdr.svm.vmcb_pa))\n\t\treturn -EINVAL;\n\tif (kvm_state->size < sizeof(*kvm_state) + KVM_STATE_NESTED_SVM_VMCB_SIZE)\n\t\treturn -EINVAL;\n\n\tret  = -ENOMEM;\n\tctl  = kzalloc(sizeof(*ctl),  GFP_KERNEL_ACCOUNT);\n\tsave = kzalloc(sizeof(*save), GFP_KERNEL_ACCOUNT);\n\tif (!ctl || !save)\n\t\tgoto out_free;\n\n\tret = -EFAULT;\n\tif (copy_from_user(ctl, &user_vmcb->control, sizeof(*ctl)))\n\t\tgoto out_free;\n\tif (copy_from_user(save, &user_vmcb->save, sizeof(*save)))\n\t\tgoto out_free;\n\n\tret = -EINVAL;\n\tif (!nested_vmcb_check_controls(vcpu, ctl))\n\t\tgoto out_free;\n\n\t/*\n\t * Processor state contains L2 state.  Check that it is\n\t * valid for guest mode (see nested_vmcb_check_save).\n\t */\n\tcr0 = kvm_read_cr0(vcpu);\n        if (((cr0 & X86_CR0_CD) == 0) && (cr0 & X86_CR0_NW))\n\t\tgoto out_free;\n\n\t/*\n\t * Validate host state saved from before VMRUN (see\n\t * nested_svm_check_permissions).\n\t */\n\tif (!(save->cr0 & X86_CR0_PG) ||\n\t    !(save->cr0 & X86_CR0_PE) ||\n\t    (save->rflags & X86_EFLAGS_VM) ||\n\t    !nested_vmcb_valid_sregs(vcpu, save))\n\t\tgoto out_free;\n\n\t/*\n\t * While the nested guest CR3 is already checked and set by\n\t * KVM_SET_SREGS, it was set when nested state was yet loaded,\n\t * thus MMU might not be initialized correctly.\n\t * Set it again to fix this.\n\t */\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3,\n\t\t\t\t  nested_npt_enabled(svm), false);\n\tif (WARN_ON_ONCE(ret))\n\t\tgoto out_free;\n\n\n\t/*\n\t * All checks done, we can enter guest mode. Userspace provides\n\t * vmcb12.control, which will be combined with L1 and stored into\n\t * vmcb02, and the L1 save state which we store in vmcb01.\n\t * L2 registers if needed are moved from the current VMCB to VMCB02.\n\t */\n\n\tif (is_guest_mode(vcpu))\n\t\tsvm_leave_nested(svm);\n\telse\n\t\tsvm->nested.vmcb02.ptr->save = svm->vmcb01.ptr->save;\n\n\tsvm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));\n\n\tsvm->nested.nested_run_pending =\n\t\t!!(kvm_state->flags & KVM_STATE_NESTED_RUN_PENDING);\n\n\tsvm->nested.vmcb12_gpa = kvm_state->hdr.svm.vmcb_pa;\n\n\tsvm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);\n\tnested_load_control_from_vmcb12(svm, ctl);\n\n\tsvm_switch_vmcb(svm, &svm->nested.vmcb02);\n\tnested_vmcb02_prepare_control(svm);\n\tkvm_make_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n\tret = 0;\nout_free:\n\tkfree(save);\n\tkfree(ctl);\n\n\treturn ret;\n}\n\nstatic bool svm_get_nested_state_pages(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (WARN_ON(!is_guest_mode(vcpu)))\n\t\treturn true;\n\n\tif (!vcpu->arch.pdptrs_from_userspace &&\n\t    !nested_npt_enabled(svm) && is_pae_paging(vcpu))\n\t\t/*\n\t\t * Reload the guest's PDPTRs since after a migration\n\t\t * the guest CR3 might be restored prior to setting the nested\n\t\t * state which can lead to a load of wrong PDPTRs.\n\t\t */\n\t\tif (CC(!load_pdptrs(vcpu, vcpu->arch.walk_mmu, vcpu->arch.cr3)))\n\t\t\treturn false;\n\n\tif (!nested_svm_vmrun_msrpm(svm)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror =\n\t\t\tKVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstruct kvm_x86_nested_ops svm_nested_ops = {\n\t.check_events = svm_check_nested_events,\n\t.triple_fault = nested_svm_triple_fault,\n\t.get_nested_state_pages = svm_get_nested_state_pages,\n\t.get_state = svm_get_nested_state,\n\t.set_state = svm_set_nested_state,\n};\n"], "filenames": ["arch/x86/kvm/svm/nested.c"], "buggy_code_start_loc": [160], "buggy_code_end_loc": [160], "fixing_code_start_loc": [161], "fixing_code_end_loc": [164], "type": "CWE-862", "message": "A flaw was found in the KVM's AMD code for supporting SVM nested virtualization. The flaw occurs when processing the VMCB (virtual machine control block) provided by the L1 guest to spawn/handle a nested guest (L2). Due to improper validation of the \"virt_ext\" field, this issue could allow a malicious L1 to disable both VMLOAD/VMSAVE intercepts and VLS (Virtual VMLOAD/VMSAVE) for the L2 guest. As a result, the L2 guest would be allowed to read/write physical pages of the host, resulting in a crash of the entire system, leak of sensitive data or potential guest-to-host escape.", "other": {"cve": {"id": "CVE-2021-3656", "sourceIdentifier": "secalert@redhat.com", "published": "2022-03-04T19:15:08.677", "lastModified": "2023-01-19T15:53:14.633", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A flaw was found in the KVM's AMD code for supporting SVM nested virtualization. The flaw occurs when processing the VMCB (virtual machine control block) provided by the L1 guest to spawn/handle a nested guest (L2). Due to improper validation of the \"virt_ext\" field, this issue could allow a malicious L1 to disable both VMLOAD/VMSAVE intercepts and VLS (Virtual VMLOAD/VMSAVE) for the L2 guest. As a result, the L2 guest would be allowed to read/write physical pages of the host, resulting in a crash of the entire system, leak of sensitive data or potential guest-to-host escape."}, {"lang": "es", "value": "Se ha encontrado un fallo en el c\u00f3digo AMD de KVM para soportar la virtualizaci\u00f3n anidada SVM. El fallo es producido cuando es procesado el VMCB (bloque de control de la m\u00e1quina virtual) proporcionado por el hu\u00e9sped L1 para generar/manejar un hu\u00e9sped anidado (L2). Debido a que no es comprobado apropiadamente el campo \"virt_ext\", este problema podr\u00eda permitir a un L1 malicioso deshabilitar tanto las intercepciones VMLOAD/VMSAVE como el VLS (Virtual VMLOAD/VMSAVE) para el hu\u00e9sped L2. Como resultado, el invitado L2 podr\u00eda leer/escribir p\u00e1ginas f\u00edsicas del anfitri\u00f3n, resultando en un bloqueo de todo el sistema, un filtrado de datos confidenciales o un potencial escape del invitado al anfitri\u00f3n"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.0, "impactScore": 6.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-862"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-862"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.13", "versionEndExcluding": "4.14.245", "matchCriteriaId": "46647E49-211F-401B-B550-1C33058B2150"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.15", "versionEndExcluding": "4.19.205", "matchCriteriaId": "5DFB089B-C0CD-422B-9182-497E5451AD10"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.20", "versionEndExcluding": "5.4.142", "matchCriteriaId": "BABFD545-0405-4B3C-89BF-B7B0A9A5DCDF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.5", "versionEndExcluding": "5.10.60", "matchCriteriaId": "58120FFF-3B1B-4287-A7D3-657641443823"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.11", "versionEndExcluding": "5.13.12", "matchCriteriaId": "C31610D4-4A14-453C-8ECC-AFF86AC4D24D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:-:*:*:*:*:*:*", "matchCriteriaId": "6A05198E-F8FA-4517-8D0E-8C95066AED38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc1:*:*:*:*:*:*", "matchCriteriaId": "71268287-21A8-4488-AA4F-23C473153131"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc2:*:*:*:*:*:*", "matchCriteriaId": "23B9E5C6-FAB5-4A02-9E39-27C8787B0991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc3:*:*:*:*:*:*", "matchCriteriaId": "D185CF67-7E4A-4154-93DB-CE379C67DB56"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc4:*:*:*:*:*:*", "matchCriteriaId": "D1DA0AF6-02F4-47C7-A318-8C006ED0C665"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc5:*:*:*:*:*:*", "matchCriteriaId": "49DD30B1-8C99-4C38-A66B-CAB3827BEE8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc6:*:*:*:*:*:*", "matchCriteriaId": "15013998-4AF0-4CDC-AB13-829ECD8A8E66"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:33:*:*:*:*:*:*:*", "matchCriteriaId": "E460AA51-FCDA-46B9-AE97-E6676AA5E194"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:34:*:*:*:*:*:*:*", "matchCriteriaId": "A930E247-0B43-43CB-98FF-6CE7B8189835"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:software_collections:-:*:*:*:*:*:*:*", "matchCriteriaId": "749804DA-4B27-492A-9ABA-6BB562A6B3AC"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:openstack:13:*:*:*:*:*:*:*", "matchCriteriaId": "704CFA1A-953E-4105-BFBE-406034B83DED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "33C068A4-3780-4EAB-A937-6082DF847564"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.1:*:*:*:*:*:*:*", "matchCriteriaId": "92BC9265-6959-4D37-BE5E-8C45E98992F8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "831F0F47-3565-4763-B16F-C87B1FF2035E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "0E3F09B5-569F-4C58-9FCA-3C0953D107B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_ibm_z_systems:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "566507B6-AC95-47F7-A3FB-C6F414E45F51"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_ibm_z_systems:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "87C21FE1-EA5C-498F-9C6C-D05F91A88217"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_ibm_z_systems_eus:8.1:*:*:*:*:*:*:*", "matchCriteriaId": "280D547B-F204-4848-9262-A103176B740C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_ibm_z_systems_eus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "0AB105EC-19F9-424A-86F1-305A6FD74A9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_ibm_z_systems_eus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "8C9BD9AE-46FC-4609-8D99-A3CFE91D58D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_big_endian:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "1CDCFF34-6F1D-45A1-BE37-6A0E17B04801"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "B4A684C7-88FD-43C4-9BDB-AE337FCBD0AB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "47811209-5CE5-4375-8391-B0A7F6A0E420"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian_eus:8.1:*:*:*:*:*:*:*", "matchCriteriaId": "8EB6F417-25D0-4A28-B7BA-D21929EAA9E9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian_eus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "E5C80DB2-4A78-4EC9-B2A8-1E4D902C4834"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian_eus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "983533DD-3970-4A37-9A9C-582BD48AA1E5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time:7:*:*:*:*:*:*:*", "matchCriteriaId": "C2B15608-BABC-4663-A58F-B74BD2D1A734"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time:8:*:*:*:*:*:*:*", "matchCriteriaId": "CBF9BCF3-187F-410A-96CA-9C47D3ED6924"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv:7:*:*:*:*:*:*:*", "matchCriteriaId": "36E85B24-30F2-42AB-9F68-8668C0FCC5E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv:8:*:*:*:*:*:*:*", "matchCriteriaId": "E5CB3640-F55B-4127-875A-2F52D873D179"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv_tus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "77C61DDC-81F3-4E2D-9CAA-17A256C85443"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv_tus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "B6B0DA79-DF12-4418-B075-F048C9E2979A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_tus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "B92409A9-0D6B-4B7E-8847-1B63837D201F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_tus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "C5C5860E-9FEB-4259-92FD-A85911E2F99E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_scientific_computing:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "37CE1DC7-72C5-483C-8921-0B462C8284D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "B353CE99-D57C-465B-AAB0-73EF581127D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.7:*:*:*:*:*:*:*", "matchCriteriaId": "7431ABC1-9252-419E-8CC1-311B41360078"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "6897676D-53F9-45B3-B27F-7FF9A4C58D33"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "E28F226A-CBC7-4A32-BE58-398FA5B42481"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_for_power_little_endian_update_services_for_sap_solutions:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "57B5CF5A-D48E-4AD0-91E2-F5BDD44B7A66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_for_power_little_endian_update_services_for_sap_solutions:8.1:*:*:*:*:*:*:*", "matchCriteriaId": "4DF2B9A2-8CA6-4EDF-9975-07265E363ED2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_for_power_little_endian_update_services_for_sap_solutions:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "7DA6A5AF-2EBE-4ED9-B312-DCD9D150D031"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_for_power_little_endian_update_services_for_sap_solutions:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "22D095ED-9247-4133-A133-73B7668565E4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "B76AA310-FEC7-497F-AF04-C3EC1E76C4CC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.7:*:*:*:*:*:*:*", "matchCriteriaId": "17F256A9-D3B9-4C72-B013-4EFD878BFEA8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "B09ACF2D-D83F-4A86-8185-9569605D8EE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "AC10D919-57FD-4725-B8D2-39ECB476902F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_update_services_for_sap_solutions:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "5C450C83-695F-4408-8B4F-0E7D6DDAE345"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_update_services_for_sap_solutions:7.7:*:*:*:*:*:*:*", "matchCriteriaId": "3707B08D-8A78-48CB-914C-33A753D13FC7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_update_services_for_sap_solutions:8.1:*:*:*:*:*:*:*", "matchCriteriaId": "48C2E003-A71C-4D06-B8B3-F93160568182"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_update_services_for_sap_solutions:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "3921C1CF-A16D-4727-99AD-03EFFA7C91CA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_update_services_for_sap_solutions:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "BC6DD887-9744-43EA-8B3C-44C6B6339590"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "825ECE2D-E232-46E0-A047-074B34DB1E97"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:3scale_api_management:2.0:*:*:*:*:*:*:*", "matchCriteriaId": "C5434CC8-66E0-4378-AAB3-B2FECDDE61BB"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:codeready_linux_builder:-:*:*:*:*:*:*:*", "matchCriteriaId": "1CD81C46-328B-412D-AF4E-68A2AD2F1A73"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.1:*:*:*:*:*:*:*", "matchCriteriaId": "92BC9265-6959-4D37-BE5E-8C45E98992F8"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "831F0F47-3565-4763-B16F-C87B1FF2035E"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "0E3F09B5-569F-4C58-9FCA-3C0953D107B5"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "47811209-5CE5-4375-8391-B0A7F6A0E420"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian_eus:8.1:*:*:*:*:*:*:*", "matchCriteriaId": "8EB6F417-25D0-4A28-B7BA-D21929EAA9E9"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian_eus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "E5C80DB2-4A78-4EC9-B2A8-1E4D902C4834"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian_eus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "983533DD-3970-4A37-9A9C-582BD48AA1E5"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:virtualization_host:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "BB28F9AF-3D06-4532-B397-96D7E4792503"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1983988", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://git.kernel.org/pub/scm/virt/kvm/kvm.git/commit/?id=c7dfa4009965a9b2d7b329ee970eb8da0d32f0bc", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/c7dfa4009965a9b2d7b329ee970eb8da0d32f0bc", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.openwall.com/lists/oss-security/2021/08/16/1", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c7dfa4009965a9b2d7b329ee970eb8da0d32f0bc"}}