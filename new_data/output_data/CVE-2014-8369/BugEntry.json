{"buggy_code": ["/*\n * Copyright (c) 2006, Intel Corporation.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms and conditions of the GNU General Public License,\n * version 2, as published by the Free Software Foundation.\n *\n * This program is distributed in the hope it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n * more details.\n *\n * You should have received a copy of the GNU General Public License along with\n * this program; if not, write to the Free Software Foundation, Inc., 59 Temple\n * Place - Suite 330, Boston, MA 02111-1307 USA.\n *\n * Copyright (C) 2006-2008 Intel Corporation\n * Copyright IBM Corporation, 2008\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Author: Allen M. Kay <allen.m.kay@intel.com>\n * Author: Weidong Han <weidong.han@intel.com>\n * Author: Ben-Ami Yassour <benami@il.ibm.com>\n */\n\n#include <linux/list.h>\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/stat.h>\n#include <linux/dmar.h>\n#include <linux/iommu.h>\n#include <linux/intel-iommu.h>\n\nstatic bool allow_unsafe_assigned_interrupts;\nmodule_param_named(allow_unsafe_assigned_interrupts,\n\t\t   allow_unsafe_assigned_interrupts, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(allow_unsafe_assigned_interrupts,\n \"Enable device assignment on platforms without interrupt remapping support.\");\n\nstatic int kvm_iommu_unmap_memslots(struct kvm *kvm);\nstatic void kvm_iommu_put_pages(struct kvm *kvm,\n\t\t\t\tgfn_t base_gfn, unsigned long npages);\n\nstatic pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t   unsigned long size)\n{\n\tgfn_t end_gfn;\n\tpfn_t pfn;\n\n\tpfn     = gfn_to_pfn_memslot(slot, gfn);\n\tend_gfn = gfn + (size >> PAGE_SHIFT);\n\tgfn    += 1;\n\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn pfn;\n\n\twhile (gfn < end_gfn)\n\t\tgfn_to_pfn_memslot(slot, gfn++);\n\n\treturn pfn;\n}\n\nstatic void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < npages; ++i)\n\t\tkvm_release_pfn_clean(pfn + i);\n}\n\nint kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tgfn_t gfn, end_gfn;\n\tpfn_t pfn;\n\tint r = 0;\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tint flags;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tgfn     = slot->base_gfn;\n\tend_gfn = gfn + slot->npages;\n\n\tflags = IOMMU_READ;\n\tif (!(slot->flags & KVM_MEM_READONLY))\n\t\tflags |= IOMMU_WRITE;\n\tif (!kvm->arch.iommu_noncoherent)\n\t\tflags |= IOMMU_CACHE;\n\n\n\twhile (gfn < end_gfn) {\n\t\tunsigned long page_size;\n\n\t\t/* Check if already mapped */\n\t\tif (iommu_iova_to_phys(domain, gfn_to_gpa(gfn))) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get the page size we could use to map */\n\t\tpage_size = kvm_host_page_size(kvm, gfn);\n\n\t\t/* Make sure the page_size does not exceed the memslot */\n\t\twhile ((gfn + (page_size >> PAGE_SHIFT)) > end_gfn)\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure gfn is aligned to the page size we want to map */\n\t\twhile ((gfn << PAGE_SHIFT) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure hva is aligned to the page size we want to map */\n\t\twhile (__gfn_to_hva_memslot(slot, gfn) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/*\n\t\t * Pin all pages we are about to map in memory. This is\n\t\t * important because we unmap and unpin in 4kb steps later.\n\t\t */\n\t\tpfn = kvm_pin_pages(slot, gfn, page_size);\n\t\tif (is_error_noslot_pfn(pfn)) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Map into IO address space */\n\t\tr = iommu_map(domain, gfn_to_gpa(gfn), pfn_to_hpa(pfn),\n\t\t\t      page_size, flags);\n\t\tif (r) {\n\t\t\tprintk(KERN_ERR \"kvm_iommu_map_address:\"\n\t\t\t       \"iommu failed to map pfn=%llx\\n\", pfn);\n\t\t\tkvm_unpin_pages(kvm, pfn, page_size);\n\t\t\tgoto unmap_pages;\n\t\t}\n\n\t\tgfn += page_size >> PAGE_SHIFT;\n\n\n\t}\n\n\treturn 0;\n\nunmap_pages:\n\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn - slot->base_gfn);\n\treturn r;\n}\n\nstatic int kvm_iommu_map_memslots(struct kvm *kvm)\n{\n\tint idx, r = 0;\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memory_slot *memslot;\n\n\tif (kvm->arch.iommu_noncoherent)\n\t\tkvm_arch_register_noncoherent_dma(kvm);\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tslots = kvm_memslots(kvm);\n\n\tkvm_for_each_memslot(memslot, slots) {\n\t\tr = kvm_iommu_map_pages(kvm, memslot);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn r;\n}\n\nint kvm_assign_device(struct kvm *kvm,\n\t\t      struct kvm_assigned_dev_kernel *assigned_dev)\n{\n\tstruct pci_dev *pdev = NULL;\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tint r;\n\tbool noncoherent;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tpdev = assigned_dev->dev;\n\tif (pdev == NULL)\n\t\treturn -ENODEV;\n\n\tr = iommu_attach_device(domain, &pdev->dev);\n\tif (r) {\n\t\tdev_err(&pdev->dev, \"kvm assign device failed ret %d\", r);\n\t\treturn r;\n\t}\n\n\tnoncoherent = !iommu_capable(&pci_bus_type, IOMMU_CAP_CACHE_COHERENCY);\n\n\t/* Check if need to update IOMMU page table for guest memory */\n\tif (noncoherent != kvm->arch.iommu_noncoherent) {\n\t\tkvm_iommu_unmap_memslots(kvm);\n\t\tkvm->arch.iommu_noncoherent = noncoherent;\n\t\tr = kvm_iommu_map_memslots(kvm);\n\t\tif (r)\n\t\t\tgoto out_unmap;\n\t}\n\n\tpci_set_dev_assigned(pdev);\n\n\tdev_info(&pdev->dev, \"kvm assign device\\n\");\n\n\treturn 0;\nout_unmap:\n\tkvm_iommu_unmap_memslots(kvm);\n\treturn r;\n}\n\nint kvm_deassign_device(struct kvm *kvm,\n\t\t\tstruct kvm_assigned_dev_kernel *assigned_dev)\n{\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tstruct pci_dev *pdev = NULL;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tpdev = assigned_dev->dev;\n\tif (pdev == NULL)\n\t\treturn -ENODEV;\n\n\tiommu_detach_device(domain, &pdev->dev);\n\n\tpci_clear_dev_assigned(pdev);\n\n\tdev_info(&pdev->dev, \"kvm deassign device\\n\");\n\n\treturn 0;\n}\n\nint kvm_iommu_map_guest(struct kvm *kvm)\n{\n\tint r;\n\n\tif (!iommu_present(&pci_bus_type)) {\n\t\tprintk(KERN_ERR \"%s: iommu not found\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm->arch.iommu_domain = iommu_domain_alloc(&pci_bus_type);\n\tif (!kvm->arch.iommu_domain) {\n\t\tr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!allow_unsafe_assigned_interrupts &&\n\t    !iommu_capable(&pci_bus_type, IOMMU_CAP_INTR_REMAP)) {\n\t\tprintk(KERN_WARNING \"%s: No interrupt remapping support,\"\n\t\t       \" disallowing device assignment.\"\n\t\t       \" Re-enble with \\\"allow_unsafe_assigned_interrupts=1\\\"\"\n\t\t       \" module option.\\n\", __func__);\n\t\tiommu_domain_free(kvm->arch.iommu_domain);\n\t\tkvm->arch.iommu_domain = NULL;\n\t\tr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tr = kvm_iommu_map_memslots(kvm);\n\tif (r)\n\t\tkvm_iommu_unmap_memslots(kvm);\n\nout_unlock:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\n\nstatic void kvm_iommu_put_pages(struct kvm *kvm,\n\t\t\t\tgfn_t base_gfn, unsigned long npages)\n{\n\tstruct iommu_domain *domain;\n\tgfn_t end_gfn, gfn;\n\tpfn_t pfn;\n\tu64 phys;\n\n\tdomain  = kvm->arch.iommu_domain;\n\tend_gfn = base_gfn + npages;\n\tgfn     = base_gfn;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn;\n\n\twhile (gfn < end_gfn) {\n\t\tunsigned long unmap_pages;\n\t\tsize_t size;\n\n\t\t/* Get physical address */\n\t\tphys = iommu_iova_to_phys(domain, gfn_to_gpa(gfn));\n\n\t\tif (!phys) {\n\t\t\tgfn++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpfn  = phys >> PAGE_SHIFT;\n\n\t\t/* Unmap address from IO address space */\n\t\tsize       = iommu_unmap(domain, gfn_to_gpa(gfn), PAGE_SIZE);\n\t\tunmap_pages = 1ULL << get_order(size);\n\n\t\t/* Unpin all pages we just unmapped to not leak any memory */\n\t\tkvm_unpin_pages(kvm, pfn, unmap_pages);\n\n\t\tgfn += unmap_pages;\n\t}\n}\n\nvoid kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tkvm_iommu_put_pages(kvm, slot->base_gfn, slot->npages);\n}\n\nstatic int kvm_iommu_unmap_memslots(struct kvm *kvm)\n{\n\tint idx;\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memory_slot *memslot;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tslots = kvm_memslots(kvm);\n\n\tkvm_for_each_memslot(memslot, slots)\n\t\tkvm_iommu_unmap_pages(kvm, memslot);\n\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\tif (kvm->arch.iommu_noncoherent)\n\t\tkvm_arch_unregister_noncoherent_dma(kvm);\n\n\treturn 0;\n}\n\nint kvm_iommu_unmap_guest(struct kvm *kvm)\n{\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tmutex_lock(&kvm->slots_lock);\n\tkvm_iommu_unmap_memslots(kvm);\n\tkvm->arch.iommu_domain = NULL;\n\tkvm->arch.iommu_noncoherent = false;\n\tmutex_unlock(&kvm->slots_lock);\n\n\tiommu_domain_free(domain);\n\treturn 0;\n}\n"], "fixing_code": ["/*\n * Copyright (c) 2006, Intel Corporation.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms and conditions of the GNU General Public License,\n * version 2, as published by the Free Software Foundation.\n *\n * This program is distributed in the hope it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n * more details.\n *\n * You should have received a copy of the GNU General Public License along with\n * this program; if not, write to the Free Software Foundation, Inc., 59 Temple\n * Place - Suite 330, Boston, MA 02111-1307 USA.\n *\n * Copyright (C) 2006-2008 Intel Corporation\n * Copyright IBM Corporation, 2008\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Author: Allen M. Kay <allen.m.kay@intel.com>\n * Author: Weidong Han <weidong.han@intel.com>\n * Author: Ben-Ami Yassour <benami@il.ibm.com>\n */\n\n#include <linux/list.h>\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/stat.h>\n#include <linux/dmar.h>\n#include <linux/iommu.h>\n#include <linux/intel-iommu.h>\n\nstatic bool allow_unsafe_assigned_interrupts;\nmodule_param_named(allow_unsafe_assigned_interrupts,\n\t\t   allow_unsafe_assigned_interrupts, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(allow_unsafe_assigned_interrupts,\n \"Enable device assignment on platforms without interrupt remapping support.\");\n\nstatic int kvm_iommu_unmap_memslots(struct kvm *kvm);\nstatic void kvm_iommu_put_pages(struct kvm *kvm,\n\t\t\t\tgfn_t base_gfn, unsigned long npages);\n\nstatic pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t   unsigned long npages)\n{\n\tgfn_t end_gfn;\n\tpfn_t pfn;\n\n\tpfn     = gfn_to_pfn_memslot(slot, gfn);\n\tend_gfn = gfn + npages;\n\tgfn    += 1;\n\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn pfn;\n\n\twhile (gfn < end_gfn)\n\t\tgfn_to_pfn_memslot(slot, gfn++);\n\n\treturn pfn;\n}\n\nstatic void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < npages; ++i)\n\t\tkvm_release_pfn_clean(pfn + i);\n}\n\nint kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tgfn_t gfn, end_gfn;\n\tpfn_t pfn;\n\tint r = 0;\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tint flags;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tgfn     = slot->base_gfn;\n\tend_gfn = gfn + slot->npages;\n\n\tflags = IOMMU_READ;\n\tif (!(slot->flags & KVM_MEM_READONLY))\n\t\tflags |= IOMMU_WRITE;\n\tif (!kvm->arch.iommu_noncoherent)\n\t\tflags |= IOMMU_CACHE;\n\n\n\twhile (gfn < end_gfn) {\n\t\tunsigned long page_size;\n\n\t\t/* Check if already mapped */\n\t\tif (iommu_iova_to_phys(domain, gfn_to_gpa(gfn))) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get the page size we could use to map */\n\t\tpage_size = kvm_host_page_size(kvm, gfn);\n\n\t\t/* Make sure the page_size does not exceed the memslot */\n\t\twhile ((gfn + (page_size >> PAGE_SHIFT)) > end_gfn)\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure gfn is aligned to the page size we want to map */\n\t\twhile ((gfn << PAGE_SHIFT) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/* Make sure hva is aligned to the page size we want to map */\n\t\twhile (__gfn_to_hva_memslot(slot, gfn) & (page_size - 1))\n\t\t\tpage_size >>= 1;\n\n\t\t/*\n\t\t * Pin all pages we are about to map in memory. This is\n\t\t * important because we unmap and unpin in 4kb steps later.\n\t\t */\n\t\tpfn = kvm_pin_pages(slot, gfn, page_size >> PAGE_SHIFT);\n\t\tif (is_error_noslot_pfn(pfn)) {\n\t\t\tgfn += 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Map into IO address space */\n\t\tr = iommu_map(domain, gfn_to_gpa(gfn), pfn_to_hpa(pfn),\n\t\t\t      page_size, flags);\n\t\tif (r) {\n\t\t\tprintk(KERN_ERR \"kvm_iommu_map_address:\"\n\t\t\t       \"iommu failed to map pfn=%llx\\n\", pfn);\n\t\t\tkvm_unpin_pages(kvm, pfn, page_size >> PAGE_SHIFT);\n\t\t\tgoto unmap_pages;\n\t\t}\n\n\t\tgfn += page_size >> PAGE_SHIFT;\n\n\n\t}\n\n\treturn 0;\n\nunmap_pages:\n\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn - slot->base_gfn);\n\treturn r;\n}\n\nstatic int kvm_iommu_map_memslots(struct kvm *kvm)\n{\n\tint idx, r = 0;\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memory_slot *memslot;\n\n\tif (kvm->arch.iommu_noncoherent)\n\t\tkvm_arch_register_noncoherent_dma(kvm);\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tslots = kvm_memslots(kvm);\n\n\tkvm_for_each_memslot(memslot, slots) {\n\t\tr = kvm_iommu_map_pages(kvm, memslot);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn r;\n}\n\nint kvm_assign_device(struct kvm *kvm,\n\t\t      struct kvm_assigned_dev_kernel *assigned_dev)\n{\n\tstruct pci_dev *pdev = NULL;\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tint r;\n\tbool noncoherent;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tpdev = assigned_dev->dev;\n\tif (pdev == NULL)\n\t\treturn -ENODEV;\n\n\tr = iommu_attach_device(domain, &pdev->dev);\n\tif (r) {\n\t\tdev_err(&pdev->dev, \"kvm assign device failed ret %d\", r);\n\t\treturn r;\n\t}\n\n\tnoncoherent = !iommu_capable(&pci_bus_type, IOMMU_CAP_CACHE_COHERENCY);\n\n\t/* Check if need to update IOMMU page table for guest memory */\n\tif (noncoherent != kvm->arch.iommu_noncoherent) {\n\t\tkvm_iommu_unmap_memslots(kvm);\n\t\tkvm->arch.iommu_noncoherent = noncoherent;\n\t\tr = kvm_iommu_map_memslots(kvm);\n\t\tif (r)\n\t\t\tgoto out_unmap;\n\t}\n\n\tpci_set_dev_assigned(pdev);\n\n\tdev_info(&pdev->dev, \"kvm assign device\\n\");\n\n\treturn 0;\nout_unmap:\n\tkvm_iommu_unmap_memslots(kvm);\n\treturn r;\n}\n\nint kvm_deassign_device(struct kvm *kvm,\n\t\t\tstruct kvm_assigned_dev_kernel *assigned_dev)\n{\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\tstruct pci_dev *pdev = NULL;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tpdev = assigned_dev->dev;\n\tif (pdev == NULL)\n\t\treturn -ENODEV;\n\n\tiommu_detach_device(domain, &pdev->dev);\n\n\tpci_clear_dev_assigned(pdev);\n\n\tdev_info(&pdev->dev, \"kvm deassign device\\n\");\n\n\treturn 0;\n}\n\nint kvm_iommu_map_guest(struct kvm *kvm)\n{\n\tint r;\n\n\tif (!iommu_present(&pci_bus_type)) {\n\t\tprintk(KERN_ERR \"%s: iommu not found\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm->arch.iommu_domain = iommu_domain_alloc(&pci_bus_type);\n\tif (!kvm->arch.iommu_domain) {\n\t\tr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!allow_unsafe_assigned_interrupts &&\n\t    !iommu_capable(&pci_bus_type, IOMMU_CAP_INTR_REMAP)) {\n\t\tprintk(KERN_WARNING \"%s: No interrupt remapping support,\"\n\t\t       \" disallowing device assignment.\"\n\t\t       \" Re-enble with \\\"allow_unsafe_assigned_interrupts=1\\\"\"\n\t\t       \" module option.\\n\", __func__);\n\t\tiommu_domain_free(kvm->arch.iommu_domain);\n\t\tkvm->arch.iommu_domain = NULL;\n\t\tr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tr = kvm_iommu_map_memslots(kvm);\n\tif (r)\n\t\tkvm_iommu_unmap_memslots(kvm);\n\nout_unlock:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\n\nstatic void kvm_iommu_put_pages(struct kvm *kvm,\n\t\t\t\tgfn_t base_gfn, unsigned long npages)\n{\n\tstruct iommu_domain *domain;\n\tgfn_t end_gfn, gfn;\n\tpfn_t pfn;\n\tu64 phys;\n\n\tdomain  = kvm->arch.iommu_domain;\n\tend_gfn = base_gfn + npages;\n\tgfn     = base_gfn;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn;\n\n\twhile (gfn < end_gfn) {\n\t\tunsigned long unmap_pages;\n\t\tsize_t size;\n\n\t\t/* Get physical address */\n\t\tphys = iommu_iova_to_phys(domain, gfn_to_gpa(gfn));\n\n\t\tif (!phys) {\n\t\t\tgfn++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpfn  = phys >> PAGE_SHIFT;\n\n\t\t/* Unmap address from IO address space */\n\t\tsize       = iommu_unmap(domain, gfn_to_gpa(gfn), PAGE_SIZE);\n\t\tunmap_pages = 1ULL << get_order(size);\n\n\t\t/* Unpin all pages we just unmapped to not leak any memory */\n\t\tkvm_unpin_pages(kvm, pfn, unmap_pages);\n\n\t\tgfn += unmap_pages;\n\t}\n}\n\nvoid kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tkvm_iommu_put_pages(kvm, slot->base_gfn, slot->npages);\n}\n\nstatic int kvm_iommu_unmap_memslots(struct kvm *kvm)\n{\n\tint idx;\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memory_slot *memslot;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tslots = kvm_memslots(kvm);\n\n\tkvm_for_each_memslot(memslot, slots)\n\t\tkvm_iommu_unmap_pages(kvm, memslot);\n\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\tif (kvm->arch.iommu_noncoherent)\n\t\tkvm_arch_unregister_noncoherent_dma(kvm);\n\n\treturn 0;\n}\n\nint kvm_iommu_unmap_guest(struct kvm *kvm)\n{\n\tstruct iommu_domain *domain = kvm->arch.iommu_domain;\n\n\t/* check if iommu exists and in use */\n\tif (!domain)\n\t\treturn 0;\n\n\tmutex_lock(&kvm->slots_lock);\n\tkvm_iommu_unmap_memslots(kvm);\n\tkvm->arch.iommu_domain = NULL;\n\tkvm->arch.iommu_noncoherent = false;\n\tmutex_unlock(&kvm->slots_lock);\n\n\tiommu_domain_free(domain);\n\treturn 0;\n}\n"], "filenames": ["virt/kvm/iommu.c"], "buggy_code_start_loc": [46], "buggy_code_end_loc": [135], "fixing_code_start_loc": [46], "fixing_code_end_loc": [135], "type": "CWE-119", "message": "The kvm_iommu_map_pages function in virt/kvm/iommu.c in the Linux kernel through 3.17.2 miscalculates the number of pages during the handling of a mapping failure, which allows guest OS users to cause a denial of service (host OS page unpinning) or possibly have unspecified other impact by leveraging guest OS privileges.  NOTE: this vulnerability exists because of an incorrect fix for CVE-2014-3601.", "other": {"cve": {"id": "CVE-2014-8369", "sourceIdentifier": "cve@mitre.org", "published": "2014-11-10T11:55:08.737", "lastModified": "2020-08-13T19:37:01.047", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The kvm_iommu_map_pages function in virt/kvm/iommu.c in the Linux kernel through 3.17.2 miscalculates the number of pages during the handling of a mapping failure, which allows guest OS users to cause a denial of service (host OS page unpinning) or possibly have unspecified other impact by leveraging guest OS privileges.  NOTE: this vulnerability exists because of an incorrect fix for CVE-2014-3601."}, {"lang": "es", "value": "La funci\u00f3n kvm_iommu_map_pages en virt/kvm/iommu.c en el kernel de Linux hasta 3.17.2 calcula mal el n\u00famero de p\u00e1ginas durante el manejo de fallo en el mapeo, lo que permite a usuarios del sistema operativo invitado causar una denegaci\u00f3n de servicio ( liberaci\u00f3n de p\u00e1gina del sistema operativo anfitri\u00f3n) o posiblemente tener otro impacto no especificado mediante el aprovechamiento de los privilegios del sistema operativo invitado. NOTA: esta vulnerabilidad existe debido a una soluci\u00f3n incorrecta para CVE-2014-3601."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.17.2", "matchCriteriaId": "D5362594-2AE6-4AFD-A1FB-FCB55482F71E"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "16F59A04-14CF-49E2-9973-645477EA09DA"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:evergreen:11.4:*:*:*:*:*:*:*", "matchCriteriaId": "CCE4D64E-8C4B-4F21-A9B0-90637C85C1D0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_real_time_extension:11:sp3:*:*:*:*:*:*", "matchCriteriaId": "3DB41B45-D94D-4A58-88B0-B3EC3EC350E2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:suse_linux_enterprise_server:11:sp2:*:*:ltss:*:*:*", "matchCriteriaId": "C202F75B-221A-40BB-8A0D-451335B39937"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=3d32e4dbe71374a6780eaf51d719d76f9a9bf22f", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00010.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00025.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-04/msg00015.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2015-0674.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.debian.org/security/2014/dsa-3093", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/10/24/7", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/70747", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/bid/70749", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1156518", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/3d32e4dbe71374a6780eaf51d719d76f9a9bf22f", "source": "cve@mitre.org", "tags": ["Exploit", "Patch", "Third Party Advisory"]}, {"url": "https://lkml.org/lkml/2014/10/24/460", "source": "cve@mitre.org", "tags": ["Exploit", "Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/3d32e4dbe71374a6780eaf51d719d76f9a9bf22f"}}