{"buggy_code": ["#!/usr/bin/env bash\n\nset -ex\n\nexpect_1()\n{\n  set -x\n  set +e\n  \"$@\"\n  if [ $? == 1 ]; then return 0; else return 1; fi\n}\n\n\nkey=`ceph auth get-or-create-key client.poolaccess1 mon 'allow r' osd 'allow *'`\nrados --id poolaccess1 --key $key -p rbd ls\n\nkey=`ceph auth get-or-create-key client.poolaccess2 mon 'allow r' osd 'allow * pool=nopool'`\nexpect_1 rados --id poolaccess2 --key $key -p rbd ls\n\nkey=`ceph auth get-or-create-key client.poolaccess3 mon 'allow r' osd 'allow rw pool=nopool'`\nexpect_1 rados --id poolaccess3 --key $key -p rbd ls\n\necho OK\n", "#!/usr/bin/env bash\nset -ex\n\nIMAGE_FEATURES=\"layering,exclusive-lock,object-map,fast-diff\"\n\nclone_v2_enabled() {\n    image_spec=$1\n    rbd info $image_spec | grep \"clone-parent\"\n}\n\ncreate_pools() {\n    ceph osd pool create images 32\n    rbd pool init images\n    ceph osd pool create volumes 32\n    rbd pool init volumes\n}\n\ndelete_pools() {\n    (ceph osd pool delete images images --yes-i-really-really-mean-it || true) >/dev/null 2>&1\n    (ceph osd pool delete volumes volumes --yes-i-really-really-mean-it || true) >/dev/null 2>&1\n\n}\n\nrecreate_pools() {\n    delete_pools\n    create_pools\n}\n\ndelete_users() {\n    (ceph auth del client.volumes || true) >/dev/null 2>&1\n    (ceph auth del client.images || true) >/dev/null 2>&1\n}\n\ncreate_users() {\n    ceph auth get-or-create client.volumes mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd-read-only pool=images' >> $KEYRING\n    ceph auth get-or-create client.images mon 'profile rbd' osd 'profile rbd pool=images' >> $KEYRING\n}\n\nexpect() {\n\n  set +e\n\n  local expected_ret=$1\n  local ret\n\n  shift\n  cmd=$@\n\n  eval $cmd\n  ret=$?\n\n  set -e\n\n  if [[ $ret -ne $expected_ret ]]; then\n    echo \"ERROR: running \\'$cmd\\': expected $expected_ret got $ret\"\n    return 1\n  fi\n\n  return 0\n}\n\ntest_images_access() {\n    rbd -k $KEYRING --id images create --image-format 2 --image-feature $IMAGE_FEATURES -s 1 images/foo\n    rbd -k $KEYRING --id images snap create images/foo@snap\n    rbd -k $KEYRING --id images snap protect images/foo@snap\n    rbd -k $KEYRING --id images snap unprotect images/foo@snap\n    rbd -k $KEYRING --id images snap protect images/foo@snap\n    rbd -k $KEYRING --id images export images/foo@snap - >/dev/null\n    expect 16 rbd -k $KEYRING --id images snap rm images/foo@snap\n\n    rbd -k $KEYRING --id volumes clone --image-feature $IMAGE_FEATURES images/foo@snap volumes/child\n\n    if ! clone_v2_enabled images/foo; then\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n    fi\n\n    expect 1 rbd -k $KEYRING --id volumes snap unprotect images/foo@snap\n    expect 1 rbd -k $KEYRING --id images flatten volumes/child\n    rbd -k $KEYRING --id volumes flatten volumes/child\n    expect 1 rbd -k $KEYRING --id volumes snap unprotect images/foo@snap\n    rbd -k $KEYRING --id images snap unprotect images/foo@snap\n\n    expect 39 rbd -k $KEYRING --id images rm images/foo\n    rbd -k $KEYRING --id images snap rm images/foo@snap\n    rbd -k $KEYRING --id images rm images/foo\n    rbd -k $KEYRING --id volumes rm volumes/child\n}\n\ntest_volumes_access() {\n    rbd -k $KEYRING --id images create --image-format 2 --image-feature $IMAGE_FEATURES -s 1 images/foo\n    rbd -k $KEYRING --id images snap create images/foo@snap\n    rbd -k $KEYRING --id images snap protect images/foo@snap\n\n    # commands that work with read-only access\n    rbd -k $KEYRING --id volumes info images/foo@snap\n    rbd -k $KEYRING --id volumes snap ls images/foo\n    rbd -k $KEYRING --id volumes export images/foo - >/dev/null\n    rbd -k $KEYRING --id volumes cp images/foo volumes/foo_copy\n    rbd -k $KEYRING --id volumes rm volumes/foo_copy\n    rbd -k $KEYRING --id volumes children images/foo@snap\n    rbd -k $KEYRING --id volumes lock list images/foo\n\n    # commands that fail with read-only access\n    expect 1 rbd -k $KEYRING --id volumes resize -s 2 images/foo --allow-shrink\n    expect 1 rbd -k $KEYRING --id volumes snap create images/foo@2\n    expect 1 rbd -k $KEYRING --id volumes snap rollback images/foo@snap\n    expect 1 rbd -k $KEYRING --id volumes snap remove images/foo@snap\n    expect 1 rbd -k $KEYRING --id volumes snap purge images/foo\n    expect 1 rbd -k $KEYRING --id volumes snap unprotect images/foo@snap\n    expect 1 rbd -k $KEYRING --id volumes flatten images/foo\n    expect 1 rbd -k $KEYRING --id volumes lock add images/foo test\n    expect 1 rbd -k $KEYRING --id volumes lock remove images/foo test locker\n    expect 1 rbd -k $KEYRING --id volumes ls rbd\n\n    # create clone and snapshot\n    rbd -k $KEYRING --id volumes clone --image-feature $IMAGE_FEATURES images/foo@snap volumes/child\n    rbd -k $KEYRING --id volumes snap create volumes/child@snap1\n    rbd -k $KEYRING --id volumes snap protect volumes/child@snap1\n    rbd -k $KEYRING --id volumes snap create volumes/child@snap2\n\n    # make sure original snapshot stays protected\n    if clone_v2_enabled images/foo; then\n        rbd -k $KEYRING --id volumes flatten volumes/child\n        rbd -k $KEYRING --id volumes snap rm volumes/child@snap2\n        rbd -k $KEYRING --id volumes snap unprotect volumes/child@snap1\n    else\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n        rbd -k $KEYRING --id volumes flatten volumes/child\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n        rbd -k $KEYRING --id volumes snap rm volumes/child@snap2\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n        expect 2 rbd -k $KEYRING --id volumes snap rm volumes/child@snap2\n        rbd -k $KEYRING --id volumes snap unprotect volumes/child@snap1\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n    fi\n\n    # clean up\n    rbd -k $KEYRING --id volumes snap rm volumes/child@snap1\n    rbd -k $KEYRING --id images snap unprotect images/foo@snap\n    rbd -k $KEYRING --id images snap rm images/foo@snap\n    rbd -k $KEYRING --id images rm images/foo\n    rbd -k $KEYRING --id volumes rm volumes/child\n}\n\ncleanup() {\n    rm -f $KEYRING\n}\nKEYRING=$(mktemp)\ntrap cleanup EXIT ERR HUP INT QUIT\n\ndelete_users\ncreate_users\n\nrecreate_pools\ntest_images_access\n\nrecreate_pools\ntest_volumes_access\n\ndelete_pools\ndelete_users\n\necho OK\nexit 0\n", "// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-\n// vim: ts=8 sw=2 smarttab\n/*\n * Ceph - scalable distributed file system\n *\n * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>\n * Copyright (C) 2013,2014 Cloudwatt <libre.licensing@cloudwatt.com>\n * Copyright (C) 2014 Red Hat <contact@redhat.com>\n *\n * Author: Loic Dachary <loic@dachary.org>\n *\n * This is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License version 2.1, as published by the Free Software\n * Foundation.  See file COPYING.\n *\n */\n\n#include <algorithm>\n#include <boost/algorithm/string.hpp>\n#include <locale>\n#include <sstream>\n\n#include \"mon/OSDMonitor.h\"\n#include \"mon/Monitor.h\"\n#include \"mon/MDSMonitor.h\"\n#include \"mon/MgrStatMonitor.h\"\n#include \"mon/AuthMonitor.h\"\n#include \"mon/ConfigKeyService.h\"\n\n#include \"mon/MonitorDBStore.h\"\n#include \"mon/Session.h\"\n\n#include \"crush/CrushWrapper.h\"\n#include \"crush/CrushTester.h\"\n#include \"crush/CrushTreeDumper.h\"\n\n#include \"messages/MOSDBeacon.h\"\n#include \"messages/MOSDFailure.h\"\n#include \"messages/MOSDMarkMeDown.h\"\n#include \"messages/MOSDFull.h\"\n#include \"messages/MOSDMap.h\"\n#include \"messages/MMonGetOSDMap.h\"\n#include \"messages/MOSDBoot.h\"\n#include \"messages/MOSDAlive.h\"\n#include \"messages/MPoolOp.h\"\n#include \"messages/MPoolOpReply.h\"\n#include \"messages/MOSDPGCreate.h\"\n#include \"messages/MOSDPGCreate2.h\"\n#include \"messages/MOSDPGCreated.h\"\n#include \"messages/MOSDPGTemp.h\"\n#include \"messages/MMonCommand.h\"\n#include \"messages/MRemoveSnaps.h\"\n#include \"messages/MOSDScrub.h\"\n#include \"messages/MRoute.h\"\n\n#include \"common/TextTable.h\"\n#include \"common/Timer.h\"\n#include \"common/ceph_argparse.h\"\n#include \"common/perf_counters.h\"\n#include \"common/strtol.h\"\n\n#include \"common/config.h\"\n#include \"common/errno.h\"\n\n#include \"erasure-code/ErasureCodePlugin.h\"\n#include \"compressor/Compressor.h\"\n#include \"common/Checksummer.h\"\n\n#include \"include/compat.h\"\n#include \"include/assert.h\"\n#include \"include/stringify.h\"\n#include \"include/util.h\"\n#include \"common/cmdparse.h\"\n#include \"include/str_list.h\"\n#include \"include/str_map.h\"\n#include \"include/scope_guard.h\"\n\n#include \"json_spirit/json_spirit_reader.h\"\n\n#include <boost/algorithm/string/predicate.hpp>\n\n#define dout_subsys ceph_subsys_mon\nstatic const string OSD_PG_CREATING_PREFIX(\"osd_pg_creating\");\nstatic const string OSD_METADATA_PREFIX(\"osd_metadata\");\nstatic const string OSD_SNAP_PREFIX(\"osd_snap\");\n\nnamespace {\n\nconst uint32_t MAX_POOL_APPLICATIONS = 4;\nconst uint32_t MAX_POOL_APPLICATION_KEYS = 64;\nconst uint32_t MAX_POOL_APPLICATION_LENGTH = 128;\n\n} // anonymous namespace\n\nvoid LastEpochClean::Lec::report(ps_t ps, epoch_t last_epoch_clean)\n{\n  if (epoch_by_pg.size() <= ps) {\n    epoch_by_pg.resize(ps + 1, 0);\n  }\n  const auto old_lec = epoch_by_pg[ps];\n  if (old_lec >= last_epoch_clean) {\n    // stale lec\n    return;\n  }\n  epoch_by_pg[ps] = last_epoch_clean;\n  if (last_epoch_clean < floor) {\n    floor = last_epoch_clean;\n  } else if (last_epoch_clean > floor) {\n    if (old_lec == floor) {\n      // probably should increase floor?\n      auto new_floor = std::min_element(std::begin(epoch_by_pg),\n\t\t\t\t\tstd::end(epoch_by_pg));\n      floor = *new_floor;\n    }\n  }\n  if (ps != next_missing) {\n    return;\n  }\n  for (; next_missing < epoch_by_pg.size(); next_missing++) {\n    if (epoch_by_pg[next_missing] == 0) {\n      break;\n    }\n  }\n}\n\nvoid LastEpochClean::remove_pool(uint64_t pool)\n{\n  report_by_pool.erase(pool);\n}\n\nvoid LastEpochClean::report(const pg_t& pg, epoch_t last_epoch_clean)\n{\n  auto& lec = report_by_pool[pg.pool()];\n  return lec.report(pg.ps(), last_epoch_clean);\n}\n\nepoch_t LastEpochClean::get_lower_bound(const OSDMap& latest) const\n{\n  auto floor = latest.get_epoch();\n  for (auto& pool : latest.get_pools()) {\n    auto reported = report_by_pool.find(pool.first);\n    if (reported == report_by_pool.end()) {\n      return 0;\n    }\n    if (reported->second.next_missing < pool.second.get_pg_num()) {\n      return 0;\n    }\n    if (reported->second.floor < floor) {\n      floor = reported->second.floor;\n    }\n  }\n  return floor;\n}\n\n\nclass C_UpdateCreatingPGs : public Context {\npublic:\n  OSDMonitor *osdmon;\n  utime_t start;\n  epoch_t epoch;\n  C_UpdateCreatingPGs(OSDMonitor *osdmon, epoch_t e) :\n    osdmon(osdmon), start(ceph_clock_now()), epoch(e) {}\n  void finish(int r) override {\n    if (r >= 0) {\n      utime_t end = ceph_clock_now();\n      dout(10) << \"osdmap epoch \" << epoch << \" mapping took \"\n\t       << (end - start) << \" seconds\" << dendl;\n      osdmon->update_creating_pgs();\n      osdmon->check_pg_creates_subs();\n    }\n  }\n};\n\n#undef dout_prefix\n#define dout_prefix _prefix(_dout, mon, osdmap)\nstatic ostream& _prefix(std::ostream *_dout, Monitor *mon, const OSDMap& osdmap) {\n  return *_dout << \"mon.\" << mon->name << \"@\" << mon->rank\n\t\t<< \"(\" << mon->get_state_name()\n\t\t<< \").osd e\" << osdmap.get_epoch() << \" \";\n}\n\nOSDMonitor::OSDMonitor(\n  CephContext *cct,\n  Monitor *mn,\n  Paxos *p,\n  const string& service_name)\n : PaxosService(mn, p, service_name),\n   cct(cct),\n   inc_osd_cache(g_conf->mon_osd_cache_size),\n   full_osd_cache(g_conf->mon_osd_cache_size),\n   has_osdmap_manifest(false),\n   mapper(mn->cct, &mn->cpu_tp)\n{}\n\nbool OSDMonitor::_have_pending_crush()\n{\n  return pending_inc.crush.length() > 0;\n}\n\nCrushWrapper &OSDMonitor::_get_stable_crush()\n{\n  return *osdmap.crush;\n}\n\nvoid OSDMonitor::_get_pending_crush(CrushWrapper& newcrush)\n{\n  bufferlist bl;\n  if (pending_inc.crush.length())\n    bl = pending_inc.crush;\n  else\n    osdmap.crush->encode(bl, CEPH_FEATURES_SUPPORTED_DEFAULT);\n\n  auto p = bl.cbegin();\n  newcrush.decode(p);\n}\n\nvoid OSDMonitor::create_initial()\n{\n  dout(10) << \"create_initial for \" << mon->monmap->fsid << dendl;\n\n  OSDMap newmap;\n\n  bufferlist bl;\n  mon->store->get(\"mkfs\", \"osdmap\", bl);\n\n  if (bl.length()) {\n    newmap.decode(bl);\n    newmap.set_fsid(mon->monmap->fsid);\n  } else {\n    newmap.build_simple(cct, 0, mon->monmap->fsid, 0);\n  }\n  newmap.set_epoch(1);\n  newmap.created = newmap.modified = ceph_clock_now();\n\n  // new clusters should sort bitwise by default.\n  newmap.set_flag(CEPH_OSDMAP_SORTBITWISE);\n\n  newmap.flags |=\n    CEPH_OSDMAP_RECOVERY_DELETES |\n    CEPH_OSDMAP_PURGED_SNAPDIRS;\n  newmap.full_ratio = g_conf->mon_osd_full_ratio;\n  if (newmap.full_ratio > 1.0) newmap.full_ratio /= 100;\n  newmap.backfillfull_ratio = g_conf->mon_osd_backfillfull_ratio;\n  if (newmap.backfillfull_ratio > 1.0) newmap.backfillfull_ratio /= 100;\n  newmap.nearfull_ratio = g_conf->mon_osd_nearfull_ratio;\n  if (newmap.nearfull_ratio > 1.0) newmap.nearfull_ratio /= 100;\n\n  // new cluster should require latest by default\n  if (g_conf->get_val<bool>(\"mon_debug_no_require_nautilus\")) {\n    if (g_conf->mon_debug_no_require_mimic) {\n      derr << __func__ << \" mon_debug_no_require_mimic=true and nautilus=true\" << dendl;\n      newmap.require_osd_release = CEPH_RELEASE_LUMINOUS;\n    } else {\n      derr << __func__ << \" mon_debug_no_require_nautilus=true\" << dendl;\n      newmap.require_osd_release = CEPH_RELEASE_MIMIC;\n    }\n  } else {\n    newmap.require_osd_release = CEPH_RELEASE_NAUTILUS;\n    int r = ceph_release_from_name(\n      g_conf->mon_osd_initial_require_min_compat_client.c_str());\n    if (r <= 0) {\n      assert(0 == \"mon_osd_initial_require_min_compat_client is not valid\");\n    }\n    newmap.require_min_compat_client = r;\n  }\n\n  // encode into pending incremental\n  uint64_t features = newmap.get_encoding_features();\n  newmap.encode(pending_inc.fullmap,\n                features | CEPH_FEATURE_RESERVED);\n  pending_inc.full_crc = newmap.get_crc();\n  dout(20) << \" full crc \" << pending_inc.full_crc << dendl;\n}\n\nvoid OSDMonitor::get_store_prefixes(std::set<string>& s) const\n{\n  s.insert(service_name);\n  s.insert(OSD_PG_CREATING_PREFIX);\n  s.insert(OSD_METADATA_PREFIX);\n  s.insert(OSD_SNAP_PREFIX);\n}\n\nvoid OSDMonitor::update_from_paxos(bool *need_bootstrap)\n{\n  // we really don't care if the version has been updated, because we may\n  // have trimmed without having increased the last committed; yet, we may\n  // need to update the in-memory manifest.\n  load_osdmap_manifest();\n\n  version_t version = get_last_committed();\n  if (version == osdmap.epoch)\n    return;\n  assert(version > osdmap.epoch);\n\n  dout(15) << \"update_from_paxos paxos e \" << version\n\t   << \", my e \" << osdmap.epoch << dendl;\n\n  if (mapping_job) {\n    if (!mapping_job->is_done()) {\n      dout(1) << __func__ << \" mapping job \"\n\t      << mapping_job.get() << \" did not complete, \"\n\t      << mapping_job->shards << \" left, canceling\" << dendl;\n      mapping_job->abort();\n    }\n    mapping_job.reset();\n  }\n\n  load_health();\n\n  /*\n   * We will possibly have a stashed latest that *we* wrote, and we will\n   * always be sure to have the oldest full map in the first..last range\n   * due to encode_trim_extra(), which includes the oldest full map in the trim\n   * transaction.\n   *\n   * encode_trim_extra() does not however write the full map's\n   * version to 'full_latest'.  This is only done when we are building the\n   * full maps from the incremental versions.  But don't panic!  We make sure\n   * that the following conditions find whichever full map version is newer.\n   */\n  version_t latest_full = get_version_latest_full();\n  if (latest_full == 0 && get_first_committed() > 1)\n    latest_full = get_first_committed();\n\n  if (get_first_committed() > 1 &&\n      latest_full < get_first_committed()) {\n    // the monitor could be just sync'ed with its peer, and the latest_full key\n    // is not encoded in the paxos commits in encode_pending(), so we need to\n    // make sure we get it pointing to a proper version.\n    version_t lc = get_last_committed();\n    version_t fc = get_first_committed();\n\n    dout(10) << __func__ << \" looking for valid full map in interval\"\n\t     << \" [\" << fc << \", \" << lc << \"]\" << dendl;\n\n    latest_full = 0;\n    for (version_t v = lc; v >= fc; v--) {\n      string full_key = \"full_\" + stringify(v);\n      if (mon->store->exists(get_service_name(), full_key)) {\n        dout(10) << __func__ << \" found latest full map v \" << v << dendl;\n        latest_full = v;\n        break;\n      }\n    }\n\n    assert(latest_full > 0);\n    auto t(std::make_shared<MonitorDBStore::Transaction>());\n    put_version_latest_full(t, latest_full);\n    mon->store->apply_transaction(t);\n    dout(10) << __func__ << \" updated the on-disk full map version to \"\n             << latest_full << dendl;\n  }\n\n  if ((latest_full > 0) && (latest_full > osdmap.epoch)) {\n    bufferlist latest_bl;\n    get_version_full(latest_full, latest_bl);\n    assert(latest_bl.length() != 0);\n    dout(7) << __func__ << \" loading latest full map e\" << latest_full << dendl;\n    osdmap = OSDMap();\n    osdmap.decode(latest_bl);\n  }\n\n  bufferlist bl;\n  if (!mon->store->get(OSD_PG_CREATING_PREFIX, \"creating\", bl)) {\n    auto p = bl.cbegin();\n    std::lock_guard<std::mutex> l(creating_pgs_lock);\n    creating_pgs.decode(p);\n    dout(7) << __func__ << \" loading creating_pgs last_scan_epoch \"\n\t    << creating_pgs.last_scan_epoch\n\t    << \" with \" << creating_pgs.pgs.size() << \" pgs\" << dendl;\n  } else {\n    dout(1) << __func__ << \" missing creating pgs; upgrade from post-kraken?\"\n\t    << dendl;\n  }\n\n  // walk through incrementals\n  MonitorDBStore::TransactionRef t;\n  size_t tx_size = 0;\n  while (version > osdmap.epoch) {\n    bufferlist inc_bl;\n    int err = get_version(osdmap.epoch+1, inc_bl);\n    assert(err == 0);\n    assert(inc_bl.length());\n\n    dout(7) << \"update_from_paxos  applying incremental \" << osdmap.epoch+1\n\t    << dendl;\n    OSDMap::Incremental inc(inc_bl);\n    err = osdmap.apply_incremental(inc);\n    assert(err == 0);\n\n    if (!t)\n      t.reset(new MonitorDBStore::Transaction);\n\n    // Write out the full map for all past epochs.  Encode the full\n    // map with the same features as the incremental.  If we don't\n    // know, use the quorum features.  If we don't know those either,\n    // encode with all features.\n    uint64_t f = inc.encode_features;\n    if (!f)\n      f = mon->get_quorum_con_features();\n    if (!f)\n      f = -1;\n    bufferlist full_bl;\n    osdmap.encode(full_bl, f | CEPH_FEATURE_RESERVED);\n    tx_size += full_bl.length();\n\n    bufferlist orig_full_bl;\n    get_version_full(osdmap.epoch, orig_full_bl);\n    if (orig_full_bl.length()) {\n      // the primary provided the full map\n      assert(inc.have_crc);\n      if (inc.full_crc != osdmap.crc) {\n\t// This will happen if the mons were running mixed versions in\n\t// the past or some other circumstance made the full encoded\n\t// maps divergent.  Reloading here will bring us back into\n\t// sync with the primary for this and all future maps.  OSDs\n\t// will also be brought back into sync when they discover the\n\t// crc mismatch and request a full map from a mon.\n\tderr << __func__ << \" full map CRC mismatch, resetting to canonical\"\n\t     << dendl;\n\n\tdout(20) << __func__ << \" my (bad) full osdmap:\\n\";\n\tJSONFormatter jf(true);\n\tjf.dump_object(\"osdmap\", osdmap);\n\tjf.flush(*_dout);\n\t*_dout << \"\\nhexdump:\\n\";\n\tfull_bl.hexdump(*_dout);\n\t*_dout << dendl;\n\n\tosdmap = OSDMap();\n\tosdmap.decode(orig_full_bl);\n\n\tdout(20) << __func__ << \" canonical full osdmap:\\n\";\n\tJSONFormatter jf(true);\n\tjf.dump_object(\"osdmap\", osdmap);\n\tjf.flush(*_dout);\n\t*_dout << \"\\nhexdump:\\n\";\n\torig_full_bl.hexdump(*_dout);\n\t*_dout << dendl;\n      }\n    } else {\n      assert(!inc.have_crc);\n      put_version_full(t, osdmap.epoch, full_bl);\n    }\n    put_version_latest_full(t, osdmap.epoch);\n\n    // share\n    dout(1) << osdmap << dendl;\n\n    if (osdmap.epoch == 1) {\n      t->erase(\"mkfs\", \"osdmap\");\n    }\n\n    if (tx_size > g_conf->mon_sync_max_payload_size*2) {\n      mon->store->apply_transaction(t);\n      t = MonitorDBStore::TransactionRef();\n      tx_size = 0;\n    }\n    for (const auto &osd_state : inc.new_state) {\n      if (osd_state.second & CEPH_OSD_UP) {\n\t// could be marked up *or* down, but we're too lazy to check which\n\tlast_osd_report.erase(osd_state.first);\n      }\n      if (osd_state.second & CEPH_OSD_EXISTS) {\n\t// could be created *or* destroyed, but we can safely drop it\n\tosd_epochs.erase(osd_state.first);\n      }\n    }\n  }\n\n  if (t) {\n    mon->store->apply_transaction(t);\n  }\n\n  for (int o = 0; o < osdmap.get_max_osd(); o++) {\n    if (osdmap.is_out(o))\n      continue;\n    auto found = down_pending_out.find(o);\n    if (osdmap.is_down(o)) {\n      // populate down -> out map\n      if (found == down_pending_out.end()) {\n        dout(10) << \" adding osd.\" << o << \" to down_pending_out map\" << dendl;\n        down_pending_out[o] = ceph_clock_now();\n      }\n    } else {\n      if (found != down_pending_out.end()) {\n        dout(10) << \" removing osd.\" << o << \" from down_pending_out map\" << dendl;\n        down_pending_out.erase(found);\n      }\n    }\n  }\n  // XXX: need to trim MonSession connected with a osd whose id > max_osd?\n\n  check_osdmap_subs();\n  check_pg_creates_subs();\n\n  share_map_with_random_osd();\n  update_logger();\n\n  process_failures();\n\n  // make sure our feature bits reflect the latest map\n  update_msgr_features();\n\n  if (!mon->is_leader()) {\n    // will be called by on_active() on the leader, avoid doing so twice\n    start_mapping();\n  }\n}\n\nvoid OSDMonitor::start_mapping()\n{\n  // initiate mapping job\n  if (mapping_job) {\n    dout(10) << __func__ << \" canceling previous mapping_job \" << mapping_job.get()\n\t     << dendl;\n    mapping_job->abort();\n  }\n  if (!osdmap.get_pools().empty()) {\n    auto fin = new C_UpdateCreatingPGs(this, osdmap.get_epoch());\n    mapping_job = mapping.start_update(osdmap, mapper,\n\t\t\t\t       g_conf->mon_osd_mapping_pgs_per_chunk);\n    dout(10) << __func__ << \" started mapping job \" << mapping_job.get()\n\t     << \" at \" << fin->start << dendl;\n    mapping_job->set_finish_event(fin);\n  } else {\n    dout(10) << __func__ << \" no pools, no mapping job\" << dendl;\n    mapping_job = nullptr;\n  }\n}\n\nvoid OSDMonitor::update_msgr_features()\n{\n  set<int> types;\n  types.insert((int)entity_name_t::TYPE_OSD);\n  types.insert((int)entity_name_t::TYPE_CLIENT);\n  types.insert((int)entity_name_t::TYPE_MDS);\n  types.insert((int)entity_name_t::TYPE_MON);\n  for (set<int>::iterator q = types.begin(); q != types.end(); ++q) {\n    uint64_t mask;\n    uint64_t features = osdmap.get_features(*q, &mask);\n    if ((mon->messenger->get_policy(*q).features_required & mask) != features) {\n      dout(0) << \"crush map has features \" << features << \", adjusting msgr requires\" << dendl;\n      ceph::net::Policy p = mon->messenger->get_policy(*q);\n      p.features_required = (p.features_required & ~mask) | features;\n      mon->messenger->set_policy(*q, p);\n    }\n  }\n}\n\nvoid OSDMonitor::on_active()\n{\n  update_logger();\n\n  if (mon->is_leader()) {\n    mon->clog->debug() << \"osdmap \" << osdmap;\n  } else {\n    list<MonOpRequestRef> ls;\n    take_all_failures(ls);\n    while (!ls.empty()) {\n      MonOpRequestRef op = ls.front();\n      op->mark_osdmon_event(__func__);\n      dispatch(op);\n      ls.pop_front();\n    }\n  }\n  start_mapping();\n}\n\nvoid OSDMonitor::on_restart()\n{\n  last_osd_report.clear();\n}\n\nvoid OSDMonitor::on_shutdown()\n{\n  dout(10) << __func__ << dendl;\n  if (mapping_job) {\n    dout(10) << __func__ << \" canceling previous mapping_job \" << mapping_job.get()\n\t     << dendl;\n    mapping_job->abort();\n  }\n\n  // discard failure info, waiters\n  list<MonOpRequestRef> ls;\n  take_all_failures(ls);\n  ls.clear();\n}\n\nvoid OSDMonitor::update_logger()\n{\n  dout(10) << \"update_logger\" << dendl;\n\n  mon->cluster_logger->set(l_cluster_num_osd, osdmap.get_num_osds());\n  mon->cluster_logger->set(l_cluster_num_osd_up, osdmap.get_num_up_osds());\n  mon->cluster_logger->set(l_cluster_num_osd_in, osdmap.get_num_in_osds());\n  mon->cluster_logger->set(l_cluster_osd_epoch, osdmap.get_epoch());\n}\n\nvoid OSDMonitor::create_pending()\n{\n  pending_inc = OSDMap::Incremental(osdmap.epoch+1);\n  pending_inc.fsid = mon->monmap->fsid;\n  pending_metadata.clear();\n  pending_metadata_rm.clear();\n\n  dout(10) << \"create_pending e \" << pending_inc.epoch << dendl;\n\n  // clean up pg_temp, primary_temp\n  OSDMap::clean_temps(cct, osdmap, &pending_inc);\n  dout(10) << \"create_pending  did clean_temps\" << dendl;\n\n  // safety checks (this shouldn't really happen)\n  {\n    if (osdmap.backfillfull_ratio <= 0) {\n      pending_inc.new_backfillfull_ratio = g_conf->mon_osd_backfillfull_ratio;\n      if (pending_inc.new_backfillfull_ratio > 1.0)\n\tpending_inc.new_backfillfull_ratio /= 100;\n      dout(1) << __func__ << \" setting backfillfull_ratio = \"\n\t      << pending_inc.new_backfillfull_ratio << dendl;\n    }\n    if (osdmap.full_ratio <= 0) {\n      pending_inc.new_full_ratio = g_conf->mon_osd_full_ratio;\n      if (pending_inc.new_full_ratio > 1.0)\n        pending_inc.new_full_ratio /= 100;\n      dout(1) << __func__ << \" setting full_ratio = \"\n\t      << pending_inc.new_full_ratio << dendl;\n    }\n    if (osdmap.nearfull_ratio <= 0) {\n      pending_inc.new_nearfull_ratio = g_conf->mon_osd_nearfull_ratio;\n      if (pending_inc.new_nearfull_ratio > 1.0)\n        pending_inc.new_nearfull_ratio /= 100;\n      dout(1) << __func__ << \" setting nearfull_ratio = \"\n\t      << pending_inc.new_nearfull_ratio << dendl;\n    }\n  }\n\n  // Rewrite CRUSH rule IDs if they are using legacy \"ruleset\"\n  // structure.\n  if (osdmap.crush->has_legacy_rule_ids()) {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    // First, for all pools, work out which rule they really used\n    // by resolving ruleset to rule.\n    for (const auto &i : osdmap.get_pools()) {\n      const auto pool_id = i.first;\n      const auto &pool = i.second;\n      int new_rule_id = newcrush.find_rule(pool.crush_rule,\n\t\t\t\t\t   pool.type, pool.size);\n\n      dout(1) << __func__ << \" rewriting pool \"\n\t      << osdmap.get_pool_name(pool_id) << \" crush ruleset \"\n\t      << pool.crush_rule << \" -> rule id \" << new_rule_id << dendl;\n      if (pending_inc.new_pools.count(pool_id) == 0) {\n\tpending_inc.new_pools[pool_id] = pool;\n      }\n      pending_inc.new_pools[pool_id].crush_rule = new_rule_id;\n    }\n\n    // Now, go ahead and renumber all the rules so that their\n    // rule_id field corresponds to their position in the array\n    auto old_to_new = newcrush.renumber_rules();\n    dout(1) << __func__ << \" Rewrote \" << old_to_new << \" crush IDs:\" << dendl;\n    for (const auto &i : old_to_new) {\n      dout(1) << __func__ << \" \" << i.first << \" -> \" << i.second << dendl;\n    }\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n  }\n}\n\ncreating_pgs_t\nOSDMonitor::update_pending_pgs(const OSDMap::Incremental& inc,\n\t\t\t       const OSDMap& nextmap)\n{\n  dout(10) << __func__ << dendl;\n  creating_pgs_t pending_creatings;\n  {\n    std::lock_guard<std::mutex> l(creating_pgs_lock);\n    pending_creatings = creating_pgs;\n  }\n  // check for new or old pools\n  if (pending_creatings.last_scan_epoch < inc.epoch) {\n    unsigned queued = 0;\n    queued += scan_for_creating_pgs(osdmap.get_pools(),\n\t\t\t\t    inc.old_pools,\n\t\t\t\t    inc.modified,\n\t\t\t\t    &pending_creatings);\n    queued += scan_for_creating_pgs(inc.new_pools,\n\t\t\t\t    inc.old_pools,\n\t\t\t\t    inc.modified,\n\t\t\t\t    &pending_creatings);\n    dout(10) << __func__ << \" \" << queued << \" pools queued\" << dendl;\n    for (auto deleted_pool : inc.old_pools) {\n      auto removed = pending_creatings.remove_pool(deleted_pool);\n      dout(10) << __func__ << \" \" << removed\n               << \" pg removed because containing pool deleted: \"\n               << deleted_pool << dendl;\n      last_epoch_clean.remove_pool(deleted_pool);\n    }\n    // pgmon updates its creating_pgs in check_osd_map() which is called by\n    // on_active() and check_osd_map() could be delayed if lease expires, so its\n    // creating_pgs could be stale in comparison with the one of osdmon. let's\n    // trim them here. otherwise, they will be added back after being erased.\n    unsigned removed = 0;\n    for (auto& pg : pending_created_pgs) {\n      dout(20) << __func__ << \" noting created pg \" << pg << dendl;\n      pending_creatings.created_pools.insert(pg.pool());\n      removed += pending_creatings.pgs.erase(pg);\n    }\n    pending_created_pgs.clear();\n    dout(10) << __func__ << \" \" << removed\n\t     << \" pgs removed because they're created\" << dendl;\n    pending_creatings.last_scan_epoch = osdmap.get_epoch();\n  }\n\n  // filter out any pgs that shouldn't exist.\n  {\n    auto i = pending_creatings.pgs.begin();\n    while (i != pending_creatings.pgs.end()) {\n      if (!nextmap.pg_exists(i->first)) {\n\tdout(10) << __func__ << \" removing pg \" << i->first\n\t\t << \" which should not exist\" << dendl;\n\ti = pending_creatings.pgs.erase(i);\n      } else {\n\t++i;\n      }\n    }\n  }\n\n  // process queue\n  unsigned max = std::max<int64_t>(1, g_conf->mon_osd_max_creating_pgs);\n  const auto total = pending_creatings.pgs.size();\n  while (pending_creatings.pgs.size() < max &&\n\t !pending_creatings.queue.empty()) {\n    auto p = pending_creatings.queue.begin();\n    int64_t poolid = p->first;\n    dout(10) << __func__ << \" pool \" << poolid\n\t     << \" created \" << p->second.created\n\t     << \" modified \" << p->second.modified\n\t     << \" [\" << p->second.start << \"-\" << p->second.end << \")\"\n\t     << dendl;\n    int n = std::min(max - pending_creatings.pgs.size(),\n\t\tp->second.end - p->second.start);\n    ps_t first = p->second.start;\n    ps_t end = first + n;\n    for (ps_t ps = first; ps < end; ++ps) {\n      const pg_t pgid{ps, static_cast<uint64_t>(poolid)};\n      // NOTE: use the *current* epoch as the PG creation epoch so that the\n      // OSD does not have to generate a long set of PastIntervals.\n      pending_creatings.pgs.emplace(pgid, make_pair(inc.epoch,\n\t\t\t\t\t\t    p->second.modified));\n      dout(10) << __func__ << \" adding \" << pgid << dendl;\n    }\n    p->second.start = end;\n    if (p->second.done()) {\n      dout(10) << __func__ << \" done with queue for \" << poolid << dendl;\n      pending_creatings.queue.erase(p);\n    } else {\n      dout(10) << __func__ << \" pool \" << poolid\n\t       << \" now [\" << p->second.start << \"-\" << p->second.end << \")\"\n\t       << dendl;\n    }\n  }\n  dout(10) << __func__ << \" queue remaining: \" << pending_creatings.queue.size()\n\t   << \" pools\" << dendl;\n  dout(10) << __func__\n\t   << \" \" << (pending_creatings.pgs.size() - total)\n\t   << \"/\" << pending_creatings.pgs.size()\n\t   << \" pgs added from queued pools\" << dendl;\n  return pending_creatings;\n}\n\nvoid OSDMonitor::maybe_prime_pg_temp()\n{\n  bool all = false;\n  if (pending_inc.crush.length()) {\n    dout(10) << __func__ << \" new crush map, all\" << dendl;\n    all = true;\n  }\n\n  if (!pending_inc.new_up_client.empty()) {\n    dout(10) << __func__ << \" new up osds, all\" << dendl;\n    all = true;\n  }\n\n  // check for interesting OSDs\n  set<int> osds;\n  for (auto p = pending_inc.new_state.begin();\n       !all && p != pending_inc.new_state.end();\n       ++p) {\n    if ((p->second & CEPH_OSD_UP) &&\n\tosdmap.is_up(p->first)) {\n      osds.insert(p->first);\n    }\n  }\n  for (map<int32_t,uint32_t>::iterator p = pending_inc.new_weight.begin();\n       !all && p != pending_inc.new_weight.end();\n       ++p) {\n    if (p->second < osdmap.get_weight(p->first)) {\n      // weight reduction\n      osds.insert(p->first);\n    } else {\n      dout(10) << __func__ << \" osd.\" << p->first << \" weight increase, all\"\n\t       << dendl;\n      all = true;\n    }\n  }\n\n  if (!all && osds.empty())\n    return;\n\n  if (!all) {\n    unsigned estimate =\n      mapping.get_osd_acting_pgs(*osds.begin()).size() * osds.size();\n    if (estimate > mapping.get_num_pgs() *\n\tg_conf->mon_osd_prime_pg_temp_max_estimate) {\n      dout(10) << __func__ << \" estimate \" << estimate << \" pgs on \"\n\t       << osds.size() << \" osds >= \"\n\t       << g_conf->mon_osd_prime_pg_temp_max_estimate << \" of total \"\n\t       << mapping.get_num_pgs() << \" pgs, all\"\n\t       << dendl;\n      all = true;\n    } else {\n      dout(10) << __func__ << \" estimate \" << estimate << \" pgs on \"\n\t       << osds.size() << \" osds\" << dendl;\n    }\n  }\n\n  OSDMap next;\n  next.deepish_copy_from(osdmap);\n  next.apply_incremental(pending_inc);\n\n  if (next.get_pools().empty()) {\n    dout(10) << __func__ << \" no pools, no pg_temp priming\" << dendl;\n  } else if (all) {\n    PrimeTempJob job(next, this);\n    mapper.queue(&job, g_conf->mon_osd_mapping_pgs_per_chunk);\n    if (job.wait_for(g_conf->mon_osd_prime_pg_temp_max_time)) {\n      dout(10) << __func__ << \" done in \" << job.get_duration() << dendl;\n    } else {\n      dout(10) << __func__ << \" did not finish in \"\n\t       << g_conf->mon_osd_prime_pg_temp_max_time\n\t       << \", stopping\" << dendl;\n      job.abort();\n    }\n  } else {\n    dout(10) << __func__ << \" \" << osds.size() << \" interesting osds\" << dendl;\n    utime_t stop = ceph_clock_now();\n    stop += g_conf->mon_osd_prime_pg_temp_max_time;\n    const int chunk = 1000;\n    int n = chunk;\n    std::unordered_set<pg_t> did_pgs;\n    for (auto osd : osds) {\n      auto& pgs = mapping.get_osd_acting_pgs(osd);\n      dout(20) << __func__ << \" osd.\" << osd << \" \" << pgs << dendl;\n      for (auto pgid : pgs) {\n\tif (!did_pgs.insert(pgid).second) {\n\t  continue;\n\t}\n\tprime_pg_temp(next, pgid);\n\tif (--n <= 0) {\n\t  n = chunk;\n\t  if (ceph_clock_now() > stop) {\n\t    dout(10) << __func__ << \" consumed more than \"\n\t\t     << g_conf->mon_osd_prime_pg_temp_max_time\n\t\t     << \" seconds, stopping\"\n\t\t     << dendl;\n\t    return;\n\t  }\n\t}\n      }\n    }\n  }\n}\n\nvoid OSDMonitor::prime_pg_temp(\n  const OSDMap& next,\n  pg_t pgid)\n{\n  // TODO: remove this creating_pgs direct access?\n  if (creating_pgs.pgs.count(pgid)) {\n    return;\n  }\n  if (!osdmap.pg_exists(pgid)) {\n    return;\n  }\n\n  vector<int> up, acting;\n  mapping.get(pgid, &up, nullptr, &acting, nullptr);\n\n  vector<int> next_up, next_acting;\n  int next_up_primary, next_acting_primary;\n  next.pg_to_up_acting_osds(pgid, &next_up, &next_up_primary,\n\t\t\t    &next_acting, &next_acting_primary);\n  if (acting == next_acting && next_up != next_acting)\n    return;  // no change since last epoch\n\n  if (acting.empty())\n    return;  // if previously empty now we can be no worse off\n  const pg_pool_t *pool = next.get_pg_pool(pgid.pool());\n  if (pool && acting.size() < pool->min_size)\n    return;  // can be no worse off than before\n\n  if (next_up == next_acting) {\n    acting.clear();\n    dout(20) << __func__ << \" next_up == next_acting now, clear pg_temp\"\n\t     << dendl;\n  }\n\n  dout(20) << __func__ << \" \" << pgid << \" \" << up << \"/\" << acting\n\t   << \" -> \" << next_up << \"/\" << next_acting\n\t   << \", priming \" << acting\n\t   << dendl;\n  {\n    Mutex::Locker l(prime_pg_temp_lock);\n    // do not touch a mapping if a change is pending\n    pending_inc.new_pg_temp.emplace(\n      pgid,\n      mempool::osdmap::vector<int>(acting.begin(), acting.end()));\n  }\n}\n\n/**\n * @note receiving a transaction in this function gives a fair amount of\n * freedom to the service implementation if it does need it. It shouldn't.\n */\nvoid OSDMonitor::encode_pending(MonitorDBStore::TransactionRef t)\n{\n  dout(10) << \"encode_pending e \" << pending_inc.epoch\n\t   << dendl;\n\n  if (do_prune(t)) {\n    dout(1) << __func__ << \" osdmap full prune encoded e\"\n            << pending_inc.epoch << dendl;\n  }\n\n  // finalize up pending_inc\n  pending_inc.modified = ceph_clock_now();\n\n  int r = pending_inc.propagate_snaps_to_tiers(cct, osdmap);\n  assert(r == 0);\n\n  if (mapping_job) {\n    if (!mapping_job->is_done()) {\n      dout(1) << __func__ << \" skipping prime_pg_temp; mapping job \"\n\t      << mapping_job.get() << \" did not complete, \"\n\t      << mapping_job->shards << \" left\" << dendl;\n      mapping_job->abort();\n    } else if (mapping.get_epoch() < osdmap.get_epoch()) {\n      dout(1) << __func__ << \" skipping prime_pg_temp; mapping job \"\n\t      << mapping_job.get() << \" is prior epoch \"\n\t      << mapping.get_epoch() << dendl;\n    } else {\n      if (g_conf->mon_osd_prime_pg_temp) {\n\tmaybe_prime_pg_temp();\n      }\n    } \n  } else if (g_conf->mon_osd_prime_pg_temp) {\n    dout(1) << __func__ << \" skipping prime_pg_temp; mapping job did not start\"\n\t    << dendl;\n  }\n  mapping_job.reset();\n\n  // ensure we don't have blank new_state updates.  these are interrpeted as\n  // CEPH_OSD_UP (and almost certainly not what we want!).\n  auto p = pending_inc.new_state.begin();\n  while (p != pending_inc.new_state.end()) {\n    if (p->second == 0) {\n      dout(10) << \"new_state for osd.\" << p->first << \" is 0, removing\" << dendl;\n      p = pending_inc.new_state.erase(p);\n    } else {\n      ++p;\n    }\n  }\n\n  {\n    OSDMap tmp;\n    tmp.deepish_copy_from(osdmap);\n    tmp.apply_incremental(pending_inc);\n\n    // remove any legacy osdmap nearfull/full flags\n    {\n      if (tmp.test_flag(CEPH_OSDMAP_FULL | CEPH_OSDMAP_NEARFULL)) {\n\tdout(10) << __func__ << \" clearing legacy osdmap nearfull/full flag\"\n\t\t << dendl;\n\tremove_flag(CEPH_OSDMAP_NEARFULL);\n\tremove_flag(CEPH_OSDMAP_FULL);\n      }\n    }\n    // collect which pools are currently affected by\n    // the near/backfill/full osd(s),\n    // and set per-pool near/backfill/full flag instead\n    set<int64_t> full_pool_ids;\n    set<int64_t> backfillfull_pool_ids;\n    set<int64_t> nearfull_pool_ids;\n    tmp.get_full_pools(cct,\n\t\t       &full_pool_ids,\n\t\t       &backfillfull_pool_ids,\n                         &nearfull_pool_ids);\n    if (full_pool_ids.empty() ||\n\tbackfillfull_pool_ids.empty() ||\n\tnearfull_pool_ids.empty()) {\n      // normal case - no nearfull, backfillfull or full osds\n        // try cancel any improper nearfull/backfillfull/full pool\n        // flags first\n      for (auto &pool: tmp.get_pools()) {\n\tauto p = pool.first;\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_NEARFULL) &&\n\t    nearfull_pool_ids.empty()) {\n\t  dout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t   << \"'s nearfull flag\" << dendl;\n\t  if (pending_inc.new_pools.count(p) == 0) {\n\t    // load original pool info first!\n\t    pending_inc.new_pools[p] = pool.second;\n\t  }\n\t  pending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_NEARFULL;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_BACKFILLFULL) &&\n\t    backfillfull_pool_ids.empty()) {\n\t  dout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t   << \"'s backfillfull flag\" << dendl;\n\t  if (pending_inc.new_pools.count(p) == 0) {\n\t    pending_inc.new_pools[p] = pool.second;\n\t  }\n\t  pending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_BACKFILLFULL;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL) &&\n\t    full_pool_ids.empty()) {\n\t  if (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n\t    // set by EQUOTA, skipping\n\t    continue;\n\t  }\n\t  dout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t   << \"'s full flag\" << dendl;\n\t  if (pending_inc.new_pools.count(p) == 0) {\n\t    pending_inc.new_pools[p] = pool.second;\n\t  }\n\t  pending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_FULL;\n\t}\n      }\n    }\n    if (!full_pool_ids.empty()) {\n      dout(10) << __func__ << \" marking pool(s) \" << full_pool_ids\n\t       << \" as full\" << dendl;\n      for (auto &p: full_pool_ids) {\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL)) {\n\t  continue;\n\t}\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = tmp.pools[p];\n\t}\n\tpending_inc.new_pools[p].flags |= pg_pool_t::FLAG_FULL;\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_BACKFILLFULL;\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_NEARFULL;\n      }\n      // cancel FLAG_FULL for pools which are no longer full too\n      for (auto &pool: tmp.get_pools()) {\n\tauto p = pool.first;\n\tif (full_pool_ids.count(p)) {\n\t  // skip pools we have just marked as full above\n\t  continue;\n\t}\n\tif (!tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL) ||\n\t    tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n\t  // don't touch if currently is not full\n\t  // or is running out of quota (and hence considered as full)\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t << \"'s full flag\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = pool.second;\n\t}\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_FULL;\n      }\n    }\n    if (!backfillfull_pool_ids.empty()) {\n      for (auto &p: backfillfull_pool_ids) {\n\tif (full_pool_ids.count(p)) {\n\t  // skip pools we have already considered as full above\n\t  continue;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n\t  // make sure FLAG_FULL is truly set, so we are safe not\n\t  // to set a extra (redundant) FLAG_BACKFILLFULL flag\n\t  assert(tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL));\n\t  continue;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_BACKFILLFULL)) {\n\t  // don't bother if pool is already marked as backfillfull\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" marking pool '\" << tmp.pool_name[p]\n\t\t << \"'s as backfillfull\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = tmp.pools[p];\n\t}\n\tpending_inc.new_pools[p].flags |= pg_pool_t::FLAG_BACKFILLFULL;\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_NEARFULL;\n      }\n      // cancel FLAG_BACKFILLFULL for pools\n      // which are no longer backfillfull too\n      for (auto &pool: tmp.get_pools()) {\n\tauto p = pool.first;\n\tif (full_pool_ids.count(p) || backfillfull_pool_ids.count(p)) {\n\t  // skip pools we have just marked as backfillfull/full above\n\t  continue;\n\t}\n\tif (!tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_BACKFILLFULL)) {\n\t  // and don't touch if currently is not backfillfull\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t << \"'s backfillfull flag\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = pool.second;\n\t}\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_BACKFILLFULL;\n      }\n    }\n    if (!nearfull_pool_ids.empty()) {\n      for (auto &p: nearfull_pool_ids) {\n\tif (full_pool_ids.count(p) || backfillfull_pool_ids.count(p)) {\n\t  continue;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n\t  // make sure FLAG_FULL is truly set, so we are safe not\n\t  // to set a extra (redundant) FLAG_NEARFULL flag\n\t  assert(tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL));\n\t  continue;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_NEARFULL)) {\n\t  // don't bother if pool is already marked as nearfull\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" marking pool '\" << tmp.pool_name[p]\n\t\t << \"'s as nearfull\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = tmp.pools[p];\n\t}\n\tpending_inc.new_pools[p].flags |= pg_pool_t::FLAG_NEARFULL;\n      }\n      // cancel FLAG_NEARFULL for pools\n      // which are no longer nearfull too\n      for (auto &pool: tmp.get_pools()) {\n\tauto p = pool.first;\n\tif (full_pool_ids.count(p) ||\n\t    backfillfull_pool_ids.count(p) ||\n\t    nearfull_pool_ids.count(p)) {\n\t  // skip pools we have just marked as\n\t  // nearfull/backfillfull/full above\n\t  continue;\n\t}\n\tif (!tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_NEARFULL)) {\n\t  // and don't touch if currently is not nearfull\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t << \"'s nearfull flag\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = pool.second;\n\t}\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_NEARFULL;\n      }\n    }\n\n    // min_compat_client?\n    if (tmp.require_min_compat_client == 0) {\n      auto mv = tmp.get_min_compat_client();\n      dout(1) << __func__ << \" setting require_min_compat_client to currently \"\n\t      << \"required \" << ceph_release_name(mv) << dendl;\n      mon->clog->info() << \"setting require_min_compat_client to currently \"\n\t\t\t<< \"required \" << ceph_release_name(mv);\n      pending_inc.new_require_min_compat_client = mv;\n    }\n\n    // upgrade to mimic?\n    if (osdmap.require_osd_release < CEPH_RELEASE_MIMIC &&\n\ttmp.require_osd_release >= CEPH_RELEASE_MIMIC) {\n      dout(10) << __func__ << \" first mimic+ epoch\" << dendl;\n      // record this epoch as the deletion for all legacy removed_snaps\n      for (auto& p : tmp.get_pools()) {\n\t// update every pool\n\tif (pending_inc.new_pools.count(p.first) == 0) {\n\t  pending_inc.new_pools[p.first] = p.second;\n\t}\n\tauto& pi = pending_inc.new_pools[p.first];\n\tif (pi.snap_seq == 0) {\n\t  // no snaps on this pool\n\t  continue;\n\t}\n\tif ((pi.flags & (pg_pool_t::FLAG_SELFMANAGED_SNAPS |\n\t\t\t pg_pool_t::FLAG_POOL_SNAPS)) == 0) {\n\t  if (!pi.removed_snaps.empty()) {\n\t    pi.flags |= pg_pool_t::FLAG_SELFMANAGED_SNAPS;\n\t  } else {\n\t    pi.flags |= pg_pool_t::FLAG_POOL_SNAPS;\n\t  }\n\t}\n\n\t// Make all previously removed snaps appear to be removed in this\n\t// epoch.  this populates removed_snaps_queue.  The OSD will subtract\n\t// off its purged_snaps, as before, and this set will shrink over the\n\t// following epochs as the purged snaps are reported back through the\n\t// mgr.\n\tOSDMap::snap_interval_set_t removed;\n\tif (!p.second.removed_snaps.empty()) {\n\t  // different flavor of interval_set :(\n\t  for (auto q = p.second.removed_snaps.begin();\n\t       q != p.second.removed_snaps.end();\n\t       ++q) {\n\t    removed.insert(q.get_start(), q.get_len());\n\t  }\n\t} else {\n\t  for (snapid_t s = 1; s <= pi.get_snap_seq(); s = s + 1) {\n\t    if (pi.snaps.count(s) == 0) {\n\t      removed.insert(s);\n\t    }\n\t  }\n\t}\n\tpending_inc.new_removed_snaps[p.first].union_of(removed);\n\n\tdout(10) << __func__ << \" converting pool \" << p.first\n\t\t << \" with \" << p.second.removed_snaps.size()\n\t\t << \" legacy removed_snaps\" << dendl;\n\tstring k = make_snap_epoch_key(p.first, pending_inc.epoch);\n\tbufferlist v;\n\tencode(p.second.removed_snaps, v);\n\tt->put(OSD_SNAP_PREFIX, k, v);\n\tfor (auto q = p.second.removed_snaps.begin();\n\t     q != p.second.removed_snaps.end();\n\t     ++q) {\n\t  bufferlist v;\n\t  string k = make_snap_key_value(p.first, q.get_start(),\n\t\t\t\t\t q.get_len(), pending_inc.epoch, &v);\n\t  t->put(OSD_SNAP_PREFIX, k, v);\n\t}\n      }\n    }\n    if (osdmap.require_osd_release < CEPH_RELEASE_NAUTILUS &&\n\ttmp.require_osd_release >= CEPH_RELEASE_NAUTILUS) {\n      dout(10) << __func__ << \" first nautilus+ epoch\" << dendl;\n    }\n  }\n\n  // tell me about it\n  for (auto i = pending_inc.new_state.begin();\n       i != pending_inc.new_state.end();\n       ++i) {\n    int s = i->second ? i->second : CEPH_OSD_UP;\n    if (s & CEPH_OSD_UP)\n      dout(2) << \" osd.\" << i->first << \" DOWN\" << dendl;\n    if (s & CEPH_OSD_EXISTS)\n      dout(2) << \" osd.\" << i->first << \" DNE\" << dendl;\n  }\n  for (auto i = pending_inc.new_up_client.begin();\n       i != pending_inc.new_up_client.end();\n       ++i) {\n    //FIXME: insert cluster addresses too\n    dout(2) << \" osd.\" << i->first << \" UP \" << i->second << dendl;\n  }\n  for (map<int32_t,uint32_t>::iterator i = pending_inc.new_weight.begin();\n       i != pending_inc.new_weight.end();\n       ++i) {\n    if (i->second == CEPH_OSD_OUT) {\n      dout(2) << \" osd.\" << i->first << \" OUT\" << dendl;\n    } else if (i->second == CEPH_OSD_IN) {\n      dout(2) << \" osd.\" << i->first << \" IN\" << dendl;\n    } else {\n      dout(2) << \" osd.\" << i->first << \" WEIGHT \" << hex << i->second << dec << dendl;\n    }\n  }\n\n  // clean inappropriate pg_upmap/pg_upmap_items (if any)\n  osdmap.maybe_remove_pg_upmaps(cct, osdmap, &pending_inc);\n\n  // features for osdmap and its incremental\n  uint64_t features;\n\n  // encode full map and determine its crc\n  OSDMap tmp;\n  {\n    tmp.deepish_copy_from(osdmap);\n    tmp.apply_incremental(pending_inc);\n\n    // determine appropriate features\n    features = tmp.get_encoding_features();\n    dout(10) << __func__ << \" encoding full map with \"\n\t     << ceph_release_name(tmp.require_osd_release)\n\t     << \" features \" << features << dendl;\n\n    // the features should be a subset of the mon quorum's features!\n    assert((features & ~mon->get_quorum_con_features()) == 0);\n\n    bufferlist fullbl;\n    encode(tmp, fullbl, features | CEPH_FEATURE_RESERVED);\n    pending_inc.full_crc = tmp.get_crc();\n\n    // include full map in the txn.  note that old monitors will\n    // overwrite this.  new ones will now skip the local full map\n    // encode and reload from this.\n    put_version_full(t, pending_inc.epoch, fullbl);\n  }\n\n  // encode\n  assert(get_last_committed() + 1 == pending_inc.epoch);\n  bufferlist bl;\n  encode(pending_inc, bl, features | CEPH_FEATURE_RESERVED);\n\n  dout(20) << \" full_crc \" << tmp.get_crc()\n\t   << \" inc_crc \" << pending_inc.inc_crc << dendl;\n\n  /* put everything in the transaction */\n  put_version(t, pending_inc.epoch, bl);\n  put_last_committed(t, pending_inc.epoch);\n\n  // metadata, too!\n  for (map<int,bufferlist>::iterator p = pending_metadata.begin();\n       p != pending_metadata.end();\n       ++p)\n    t->put(OSD_METADATA_PREFIX, stringify(p->first), p->second);\n  for (set<int>::iterator p = pending_metadata_rm.begin();\n       p != pending_metadata_rm.end();\n       ++p)\n    t->erase(OSD_METADATA_PREFIX, stringify(*p));\n  pending_metadata.clear();\n  pending_metadata_rm.clear();\n\n  // and pg creating, also!\n  auto pending_creatings = update_pending_pgs(pending_inc, tmp);\n  bufferlist creatings_bl;\n  encode(pending_creatings, creatings_bl);\n  t->put(OSD_PG_CREATING_PREFIX, \"creating\", creatings_bl);\n\n  // removed_snaps\n  if (tmp.require_osd_release >= CEPH_RELEASE_MIMIC) {\n    for (auto& i : pending_inc.new_removed_snaps) {\n      {\n\t// all snaps removed this epoch\n\tstring k = make_snap_epoch_key(i.first, pending_inc.epoch);\n\tbufferlist v;\n\tencode(i.second, v);\n\tt->put(OSD_SNAP_PREFIX, k, v);\n      }\n      for (auto q = i.second.begin();\n\t   q != i.second.end();\n\t   ++q) {\n\tbufferlist v;\n\tstring k = make_snap_key_value(i.first, q.get_start(),\n\t\t\t\t       q.get_len(), pending_inc.epoch, &v);\n\tt->put(OSD_SNAP_PREFIX, k, v);\n      }\n    }\n    for (auto& i : pending_inc.new_purged_snaps) {\n      for (auto q = i.second.begin();\n\t   q != i.second.end();\n\t   ++q) {\n\tbufferlist v;\n\tstring k = make_snap_purged_key_value(i.first, q.get_start(),\n\t\t\t\t\t      q.get_len(), pending_inc.epoch,\n\t\t\t\t\t      &v);\n\tt->put(OSD_SNAP_PREFIX, k, v);\n      }\n    }\n  }\n\n  // health\n  health_check_map_t next;\n  tmp.check_health(&next);\n  encode_health(next, t);\n}\n\nint OSDMonitor::load_metadata(int osd, map<string, string>& m, ostream *err)\n{\n  bufferlist bl;\n  int r = mon->store->get(OSD_METADATA_PREFIX, stringify(osd), bl);\n  if (r < 0)\n    return r;\n  try {\n    auto p = bl.cbegin();\n    decode(m, p);\n  }\n  catch (buffer::error& e) {\n    if (err)\n      *err << \"osd.\" << osd << \" metadata is corrupt\";\n    return -EIO;\n  }\n  return 0;\n}\n\nvoid OSDMonitor::count_metadata(const string& field, map<string,int> *out)\n{\n  for (int osd = 0; osd < osdmap.get_max_osd(); ++osd) {\n    if (osdmap.is_up(osd)) {\n      map<string,string> meta;\n      load_metadata(osd, meta, nullptr);\n      auto p = meta.find(field);\n      if (p == meta.end()) {\n\t(*out)[\"unknown\"]++;\n      } else {\n\t(*out)[p->second]++;\n      }\n    }\n  }\n}\n\nvoid OSDMonitor::count_metadata(const string& field, Formatter *f)\n{\n  map<string,int> by_val;\n  count_metadata(field, &by_val);\n  f->open_object_section(field.c_str());\n  for (auto& p : by_val) {\n    f->dump_int(p.first.c_str(), p.second);\n  }\n  f->close_section();\n}\n\nint OSDMonitor::get_osd_objectstore_type(int osd, string *type)\n{\n  map<string, string> metadata;\n  int r = load_metadata(osd, metadata, nullptr);\n  if (r < 0)\n    return r;\n\n  auto it = metadata.find(\"osd_objectstore\");\n  if (it == metadata.end())\n    return -ENOENT;\n  *type = it->second;\n  return 0;\n}\n\nbool OSDMonitor::is_pool_currently_all_bluestore(int64_t pool_id,\n\t\t\t\t\t\t const pg_pool_t &pool,\n\t\t\t\t\t\t ostream *err)\n{\n  // just check a few pgs for efficiency - this can't give a guarantee anyway,\n  // since filestore osds could always join the pool later\n  set<int> checked_osds;\n  for (unsigned ps = 0; ps < std::min(8u, pool.get_pg_num()); ++ps) {\n    vector<int> up, acting;\n    pg_t pgid(ps, pool_id);\n    osdmap.pg_to_up_acting_osds(pgid, up, acting);\n    for (int osd : up) {\n      if (checked_osds.find(osd) != checked_osds.end())\n\tcontinue;\n      string objectstore_type;\n      int r = get_osd_objectstore_type(osd, &objectstore_type);\n      // allow with missing metadata, e.g. due to an osd never booting yet\n      if (r < 0 || objectstore_type == \"bluestore\") {\n\tchecked_osds.insert(osd);\n\tcontinue;\n      }\n      *err << \"osd.\" << osd << \" uses \" << objectstore_type;\n      return false;\n    }\n  }\n  return true;\n}\n\nint OSDMonitor::dump_osd_metadata(int osd, Formatter *f, ostream *err)\n{\n  map<string,string> m;\n  if (int r = load_metadata(osd, m, err))\n    return r;\n  for (map<string,string>::iterator p = m.begin(); p != m.end(); ++p)\n    f->dump_string(p->first.c_str(), p->second);\n  return 0;\n}\n\nvoid OSDMonitor::print_nodes(Formatter *f)\n{\n  // group OSDs by their hosts\n  map<string, list<int> > osds; // hostname => osd\n  for (int osd = 0; osd < osdmap.get_max_osd(); osd++) {\n    map<string, string> m;\n    if (load_metadata(osd, m, NULL)) {\n      continue;\n    }\n    map<string, string>::iterator hostname = m.find(\"hostname\");\n    if (hostname == m.end()) {\n      // not likely though\n      continue;\n    }\n    osds[hostname->second].push_back(osd);\n  }\n\n  dump_services(f, osds, \"osd\");\n}\n\nvoid OSDMonitor::share_map_with_random_osd()\n{\n  if (osdmap.get_num_up_osds() == 0) {\n    dout(10) << __func__ << \" no up osds, don't share with anyone\" << dendl;\n    return;\n  }\n\n  MonSession *s = mon->session_map.get_random_osd_session(&osdmap);\n  if (!s) {\n    dout(10) << __func__ << \" no up osd on our session map\" << dendl;\n    return;\n  }\n\n  dout(10) << \"committed, telling random \" << s->name\n\t   << \" all about it\" << dendl;\n\n  // get feature of the peer\n  // use quorum_con_features, if it's an anonymous connection.\n  uint64_t features = s->con_features ? s->con_features :\n                                        mon->get_quorum_con_features();\n  // whatev, they'll request more if they need it\n  MOSDMap *m = build_incremental(osdmap.get_epoch() - 1, osdmap.get_epoch(), features);\n  s->con->send_message(m);\n  // NOTE: do *not* record osd has up to this epoch (as we do\n  // elsewhere) as they may still need to request older values.\n}\n\nversion_t OSDMonitor::get_trim_to() const\n{\n  if (mon->get_quorum().empty()) {\n    dout(10) << __func__ << \": quorum not formed\" << dendl;\n    return 0;\n  }\n\n  {\n    std::lock_guard<std::mutex> l(creating_pgs_lock);\n    if (!creating_pgs.pgs.empty()) {\n      return 0;\n    }\n  }\n\n  if (g_conf->get_val<bool>(\"mon_debug_block_osdmap_trim\")) {\n    dout(0) << __func__\n            << \" blocking osdmap trim\"\n               \" ('mon_debug_block_osdmap_trim' set to 'true')\"\n            << dendl;\n    return 0;\n  }\n\n  {\n    epoch_t floor = get_min_last_epoch_clean();\n    dout(10) << \" min_last_epoch_clean \" << floor << dendl;\n    if (g_conf->mon_osd_force_trim_to > 0 &&\n\tg_conf->mon_osd_force_trim_to < (int)get_last_committed()) {\n      floor = g_conf->mon_osd_force_trim_to;\n      dout(10) << \" explicit mon_osd_force_trim_to = \" << floor << dendl;\n    }\n    unsigned min = g_conf->mon_min_osdmap_epochs;\n    if (floor + min > get_last_committed()) {\n      if (min < get_last_committed())\n\tfloor = get_last_committed() - min;\n      else\n\tfloor = 0;\n    }\n    if (floor > get_first_committed())\n      return floor;\n  }\n  return 0;\n}\n\nepoch_t OSDMonitor::get_min_last_epoch_clean() const\n{\n  auto floor = last_epoch_clean.get_lower_bound(osdmap);\n  // also scan osd epochs\n  // don't trim past the oldest reported osd epoch\n  for (auto& osd_epoch : osd_epochs) {\n    if (osd_epoch.second < floor) {\n      floor = osd_epoch.second;\n    }\n  }\n  return floor;\n}\n\nvoid OSDMonitor::encode_trim_extra(MonitorDBStore::TransactionRef tx,\n\t\t\t\t   version_t first)\n{\n  dout(10) << __func__ << \" including full map for e \" << first << dendl;\n  bufferlist bl;\n  get_version_full(first, bl);\n  put_version_full(tx, first, bl);\n\n  if (has_osdmap_manifest &&\n      first > osdmap_manifest.get_first_pinned()) {\n    _prune_update_trimmed(tx, first);\n  }\n}\n\n\n/* full osdmap prune\n *\n * for more information, please refer to doc/dev/mon-osdmap-prune.rst\n */\n\nvoid OSDMonitor::load_osdmap_manifest()\n{\n  bool store_has_manifest =\n    mon->store->exists(get_service_name(), \"osdmap_manifest\");\n\n  if (!store_has_manifest) {\n    if (!has_osdmap_manifest) {\n      return;\n    }\n\n    dout(20) << __func__\n             << \" dropping osdmap manifest from memory.\" << dendl;\n    osdmap_manifest = osdmap_manifest_t();\n    has_osdmap_manifest = false;\n    return;\n  }\n\n  dout(20) << __func__\n           << \" osdmap manifest detected in store; reload.\" << dendl;\n\n  bufferlist manifest_bl;\n  int r = get_value(\"osdmap_manifest\", manifest_bl);\n  if (r < 0) {\n    derr << __func__ << \" unable to read osdmap version manifest\" << dendl;\n    ceph_assert(0 == \"error reading manifest\");\n  }\n  osdmap_manifest.decode(manifest_bl);\n  has_osdmap_manifest = true;\n\n  dout(10) << __func__ << \" store osdmap manifest pinned (\"\n           << osdmap_manifest.get_first_pinned()\n           << \" .. \"\n           << osdmap_manifest.get_last_pinned()\n           << \")\"\n           << dendl;\n}\n\nbool OSDMonitor::should_prune() const\n{\n  version_t first = get_first_committed();\n  version_t last = get_last_committed();\n  version_t min_osdmap_epochs =\n    g_conf->get_val<int64_t>(\"mon_min_osdmap_epochs\");\n  version_t prune_min =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_min\");\n  version_t prune_interval =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_interval\");\n  version_t last_pinned = osdmap_manifest.get_last_pinned();\n  version_t last_to_pin = last - min_osdmap_epochs;\n\n  // Make it or break it constraints.\n  //\n  // If any of these conditions fails, we will not prune, regardless of\n  // whether we have an on-disk manifest with an on-going pruning state.\n  //\n  if ((last - first) <= min_osdmap_epochs) {\n    // between the first and last committed epochs, we don't have\n    // enough epochs to trim, much less to prune.\n    dout(10) << __func__\n             << \" currently holding only \" << (last - first)\n             << \" epochs (min osdmap epochs: \" << min_osdmap_epochs\n             << \"); do not prune.\"\n             << dendl;\n    return false;\n\n  } else if ((last_to_pin - first) < prune_min) {\n    // between the first committed epoch and the last epoch we would prune,\n    // we simply don't have enough versions over the minimum to prune maps.\n    dout(10) << __func__\n             << \" could only prune \" << (last_to_pin - first)\n             << \" epochs (\" << first << \"..\" << last_to_pin << \"), which\"\n                \" is less than the required minimum (\" << prune_min << \")\"\n             << dendl;\n    return false;\n\n  } else if (has_osdmap_manifest && last_pinned >= last_to_pin) {\n    dout(10) << __func__\n             << \" we have pruned as far as we can; do not prune.\"\n             << dendl;\n    return false;\n\n  } else if (last_pinned + prune_interval > last_to_pin) {\n    dout(10) << __func__\n             << \" not enough epochs to form an interval (last pinned: \"\n             << last_pinned << \", last to pin: \"\n             << last_to_pin << \", interval: \" << prune_interval << \")\"\n             << dendl;\n    return false;\n  }\n\n  dout(15) << __func__\n           << \" should prune (\" << last_pinned << \"..\" << last_to_pin << \")\"\n           << \" lc (\" << first << \"..\" << last << \")\"\n           << dendl;\n  return true;\n}\n\nvoid OSDMonitor::_prune_update_trimmed(\n    MonitorDBStore::TransactionRef tx,\n    version_t first)\n{\n  dout(10) << __func__\n           << \" first \" << first\n           << \" last_pinned \" << osdmap_manifest.get_last_pinned()\n           << \" last_pinned \" << osdmap_manifest.get_last_pinned()\n           << dendl;\n\n  if (!osdmap_manifest.is_pinned(first)) {\n    osdmap_manifest.pin(first);\n  }\n\n  set<version_t>::iterator p_end = osdmap_manifest.pinned.find(first);\n  set<version_t>::iterator p = osdmap_manifest.pinned.begin();\n  osdmap_manifest.pinned.erase(p, p_end);\n  ceph_assert(osdmap_manifest.get_first_pinned() == first);\n\n  if (osdmap_manifest.get_last_pinned() == first+1 ||\n      osdmap_manifest.pinned.size() == 1) {\n    // we reached the end of the line, as pinned maps go; clean up our\n    // manifest, and let `should_prune()` decide whether we should prune\n    // again.\n    tx->erase(get_service_name(), \"osdmap_manifest\");\n    return;\n  }\n\n  bufferlist bl;\n  osdmap_manifest.encode(bl);\n  tx->put(get_service_name(), \"osdmap_manifest\", bl);\n}\n\nvoid OSDMonitor::prune_init()\n{\n  dout(1) << __func__ << dendl;\n\n  version_t pin_first;\n\n  if (!has_osdmap_manifest) {\n    // we must have never pruned, OR if we pruned the state must no longer\n    // be relevant (i.e., the state must have been removed alongside with\n    // the trim that *must* have removed past the last pinned map in a\n    // previous prune).\n    ceph_assert(osdmap_manifest.pinned.empty());\n    ceph_assert(!mon->store->exists(get_service_name(), \"osdmap_manifest\"));\n    pin_first = get_first_committed();\n\n  } else {\n    // we must have pruned in the past AND its state is still relevant\n    // (i.e., even if we trimmed, we still hold pinned maps in the manifest,\n    // and thus we still hold a manifest in the store).\n    ceph_assert(!osdmap_manifest.pinned.empty());\n    ceph_assert(osdmap_manifest.get_first_pinned() == get_first_committed());\n    ceph_assert(osdmap_manifest.get_last_pinned() < get_last_committed());\n\n    dout(10) << __func__\n             << \" first_pinned \" << osdmap_manifest.get_first_pinned()\n             << \" last_pinned \" << osdmap_manifest.get_last_pinned()\n             << dendl;\n\n    pin_first = osdmap_manifest.get_last_pinned();\n  }\n\n  osdmap_manifest.pin(pin_first);\n}\n\nbool OSDMonitor::_prune_sanitize_options() const\n{\n  uint64_t prune_interval =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_interval\");\n  uint64_t prune_min =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_min\");\n  uint64_t txsize =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_txsize\");\n\n  bool r = true;\n\n  if (prune_interval == 0) {\n    derr << __func__\n         << \" prune is enabled BUT prune interval is zero; abort.\"\n         << dendl;\n    r = false;\n  } else if (prune_interval == 1) {\n    derr << __func__\n         << \" prune interval is equal to one, which essentially means\"\n            \" no pruning; abort.\"\n         << dendl;\n    r = false;\n  }\n  if (prune_min == 0) {\n    derr << __func__\n         << \" prune is enabled BUT prune min is zero; abort.\"\n         << dendl;\n    r = false;\n  }\n  if (prune_interval > prune_min) {\n    derr << __func__\n         << \" impossible to ascertain proper prune interval because\"\n         << \" it is greater than the minimum prune epochs\"\n         << \" (min: \" << prune_min << \", interval: \" << prune_interval << \")\"\n         << dendl;\n    r = false;\n  }\n\n  if (txsize < prune_interval - 1) {\n    derr << __func__\n         << \"'mon_osdmap_full_prune_txsize' (\" << txsize\n         << \") < 'mon_osdmap_full_prune_interval-1' (\" << prune_interval - 1\n         << \"); abort.\" << dendl;\n    r = false;\n  }\n  return r;\n}\n\nbool OSDMonitor::is_prune_enabled() const {\n  return g_conf->get_val<bool>(\"mon_osdmap_full_prune_enabled\");\n}\n\nbool OSDMonitor::is_prune_supported() const {\n  return mon->get_required_mon_features().contains_any(\n      ceph::features::mon::FEATURE_OSDMAP_PRUNE);\n}\n\n/** do_prune\n *\n * @returns true if has side-effects; false otherwise.\n */\nbool OSDMonitor::do_prune(MonitorDBStore::TransactionRef tx)\n{\n  bool enabled = is_prune_enabled();\n\n  dout(1) << __func__ << \" osdmap full prune \"\n          << ( enabled ? \"enabled\" : \"disabled\")\n          << dendl;\n\n  if (!enabled || !_prune_sanitize_options() || !should_prune()) {\n    return false;\n  }\n\n  // we are beyond the minimum prune versions, we need to remove maps because\n  // otherwise the store will grow unbounded and we may end up having issues\n  // with available disk space or store hangs.\n\n  // we will not pin all versions. We will leave a buffer number of versions.\n  // this allows us the monitor to trim maps without caring too much about\n  // pinned maps, and then allow us to use another ceph-mon without these\n  // capabilities, without having to repair the store.\n\n  version_t first = get_first_committed();\n  version_t last = get_last_committed();\n\n  version_t last_to_pin = last - g_conf->mon_min_osdmap_epochs;\n  version_t last_pinned = osdmap_manifest.get_last_pinned();\n  uint64_t prune_interval =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_interval\");\n  uint64_t txsize =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_txsize\");\n\n  prune_init();\n\n  // we need to get rid of some osdmaps\n\n  dout(5) << __func__\n          << \" lc (\" << first << \" .. \" << last << \")\"\n          << \" last_pinned \" << last_pinned\n          << \" interval \" << prune_interval\n          << \" last_to_pin \" << last_to_pin\n          << dendl;\n\n  // We will be erasing maps as we go.\n  //\n  // We will erase all maps between `last_pinned` and the `next_to_pin`.\n  //\n  // If `next_to_pin` happens to be greater than `last_to_pin`, then\n  // we stop pruning. We could prune the maps between `next_to_pin` and\n  // `last_to_pin`, but by not doing it we end up with neater pruned\n  // intervals, aligned with `prune_interval`. Besides, this should not be a\n  // problem as long as `prune_interval` is set to a sane value, instead of\n  // hundreds or thousands of maps.\n\n  auto map_exists = [this](version_t v) {\n    string k = mon->store->combine_strings(\"full\", v);\n    return mon->store->exists(get_service_name(), k);\n  };\n\n  // 'interval' represents the number of maps from the last pinned\n  // i.e., if we pinned version 1 and have an interval of 10, we're pinning\n  // version 11 next; all intermediate versions will be removed.\n  //\n  // 'txsize' represents the maximum number of versions we'll be removing in\n  // this iteration. If 'txsize' is large enough to perform multiple passes\n  // pinning and removing maps, we will do so; if not, we'll do at least one\n  // pass. We are quite relaxed about honouring 'txsize', but we'll always\n  // ensure that we never go *over* the maximum.\n\n  // e.g., if we pin 1 and 11, we're removing versions [2..10]; i.e., 9 maps.\n  uint64_t removal_interval = prune_interval - 1;\n\n  if (txsize < removal_interval) {\n    dout(5) << __func__\n\t    << \" setting txsize to removal interval size (\"\n\t    << removal_interval << \" versions\"\n\t    << dendl;\n    txsize = removal_interval;\n  }\n  ceph_assert(removal_interval > 0);\n\n  uint64_t num_pruned = 0;\n  while (num_pruned + removal_interval <= txsize) { \n    last_pinned = osdmap_manifest.get_last_pinned();\n\n    if (last_pinned + prune_interval > last_to_pin) {\n      break;\n    }\n    ceph_assert(last_pinned < last_to_pin);\n\n    version_t next_pinned = last_pinned + prune_interval;\n    ceph_assert(next_pinned <= last_to_pin);\n    osdmap_manifest.pin(next_pinned);\n\n    dout(20) << __func__\n\t     << \" last_pinned \" << last_pinned\n\t     << \" next_pinned \" << next_pinned\n\t     << \" num_pruned \" << num_pruned\n\t     << \" removal interval (\" << (last_pinned+1)\n\t     << \"..\" << (next_pinned-1) << \")\"\n\t     << \" txsize \" << txsize << dendl;\n\n    ceph_assert(map_exists(last_pinned));\n    ceph_assert(map_exists(next_pinned));\n\n    for (version_t v = last_pinned+1; v < next_pinned; ++v) {\n      ceph_assert(!osdmap_manifest.is_pinned(v));\n\n      dout(20) << __func__ << \"   pruning full osdmap e\" << v << dendl;\n      string full_key = mon->store->combine_strings(\"full\", v);\n      tx->erase(get_service_name(), full_key);\n      ++num_pruned;\n    }\n  }\n\n  ceph_assert(num_pruned > 0);\n\n  bufferlist bl;\n  osdmap_manifest.encode(bl);\n  tx->put(get_service_name(), \"osdmap_manifest\", bl);\n\n  return true;\n}\n\n\n// -------------\n\nbool OSDMonitor::preprocess_query(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  Message *m = op->get_req();\n  dout(10) << \"preprocess_query \" << *m << \" from \" << m->get_orig_source_inst() << dendl;\n\n  switch (m->get_type()) {\n    // READs\n  case MSG_MON_COMMAND:\n    return preprocess_command(op);\n  case CEPH_MSG_MON_GET_OSDMAP:\n    return preprocess_get_osdmap(op);\n\n    // damp updates\n  case MSG_OSD_MARK_ME_DOWN:\n    return preprocess_mark_me_down(op);\n  case MSG_OSD_FULL:\n    return preprocess_full(op);\n  case MSG_OSD_FAILURE:\n    return preprocess_failure(op);\n  case MSG_OSD_BOOT:\n    return preprocess_boot(op);\n  case MSG_OSD_ALIVE:\n    return preprocess_alive(op);\n  case MSG_OSD_PG_CREATED:\n    return preprocess_pg_created(op);\n  case MSG_OSD_PGTEMP:\n    return preprocess_pgtemp(op);\n  case MSG_OSD_BEACON:\n    return preprocess_beacon(op);\n\n  case CEPH_MSG_POOLOP:\n    return preprocess_pool_op(op);\n\n  case MSG_REMOVE_SNAPS:\n    return preprocess_remove_snaps(op);\n\n  default:\n    ceph_abort();\n    return true;\n  }\n}\n\nbool OSDMonitor::prepare_update(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  Message *m = op->get_req();\n  dout(7) << \"prepare_update \" << *m << \" from \" << m->get_orig_source_inst() << dendl;\n\n  switch (m->get_type()) {\n    // damp updates\n  case MSG_OSD_MARK_ME_DOWN:\n    return prepare_mark_me_down(op);\n  case MSG_OSD_FULL:\n    return prepare_full(op);\n  case MSG_OSD_FAILURE:\n    return prepare_failure(op);\n  case MSG_OSD_BOOT:\n    return prepare_boot(op);\n  case MSG_OSD_ALIVE:\n    return prepare_alive(op);\n  case MSG_OSD_PG_CREATED:\n    return prepare_pg_created(op);\n  case MSG_OSD_PGTEMP:\n    return prepare_pgtemp(op);\n  case MSG_OSD_BEACON:\n    return prepare_beacon(op);\n\n  case MSG_MON_COMMAND:\n    return prepare_command(op);\n\n  case CEPH_MSG_POOLOP:\n    return prepare_pool_op(op);\n\n  case MSG_REMOVE_SNAPS:\n    return prepare_remove_snaps(op);\n\n\n  default:\n    ceph_abort();\n  }\n\n  return false;\n}\n\nbool OSDMonitor::should_propose(double& delay)\n{\n  dout(10) << \"should_propose\" << dendl;\n\n  // if full map, propose immediately!  any subsequent changes will be clobbered.\n  if (pending_inc.fullmap.length())\n    return true;\n\n  // adjust osd weights?\n  if (!osd_weight.empty() &&\n      osd_weight.size() == (unsigned)osdmap.get_max_osd()) {\n    dout(0) << \" adjusting osd weights based on \" << osd_weight << dendl;\n    osdmap.adjust_osd_weights(osd_weight, pending_inc);\n    delay = 0.0;\n    osd_weight.clear();\n    return true;\n  }\n\n  return PaxosService::should_propose(delay);\n}\n\n\n\n// ---------------------------\n// READs\n\nbool OSDMonitor::preprocess_get_osdmap(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MMonGetOSDMap *m = static_cast<MMonGetOSDMap*>(op->get_req());\n\n  uint64_t features = mon->get_quorum_con_features();\n  if (m->get_session() && m->get_session()->con_features)\n    features = m->get_session()->con_features;\n\n  dout(10) << __func__ << \" \" << *m << dendl;\n  MOSDMap *reply = new MOSDMap(mon->monmap->fsid, features);\n  epoch_t first = get_first_committed();\n  epoch_t last = osdmap.get_epoch();\n  int max = g_conf->osd_map_message_max;\n  for (epoch_t e = std::max(first, m->get_full_first());\n       e <= std::min(last, m->get_full_last()) && max > 0;\n       ++e, --max) {\n    int r = get_version_full(e, features, reply->maps[e]);\n    assert(r >= 0);\n  }\n  for (epoch_t e = std::max(first, m->get_inc_first());\n       e <= std::min(last, m->get_inc_last()) && max > 0;\n       ++e, --max) {\n    int r = get_version(e, features, reply->incremental_maps[e]);\n    assert(r >= 0);\n  }\n  reply->oldest_map = first;\n  reply->newest_map = last;\n  mon->send_reply(op, reply);\n  return true;\n}\n\n\n// ---------------------------\n// UPDATEs\n\n// failure --\n\nbool OSDMonitor::check_source(PaxosServiceMessage *m, uuid_d fsid) {\n  // check permissions\n  MonSession *session = m->get_session();\n  if (!session)\n    return true;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"got MOSDFailure from entity with insufficient caps \"\n\t    << session->caps << dendl;\n    return true;\n  }\n  if (fsid != mon->monmap->fsid) {\n    dout(0) << \"check_source: on fsid \" << fsid\n\t    << \" != \" << mon->monmap->fsid << dendl;\n    return true;\n  }\n  return false;\n}\n\n\nbool OSDMonitor::preprocess_failure(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDFailure *m = static_cast<MOSDFailure*>(op->get_req());\n  // who is target_osd\n  int badboy = m->get_target_osd();\n\n  // check permissions\n  if (check_source(m, m->fsid))\n    goto didit;\n\n  // first, verify the reporting host is valid\n  if (m->get_orig_source().is_osd()) {\n    int from = m->get_orig_source().num();\n    if (!osdmap.exists(from) ||\n\tosdmap.get_addrs(from) != m->get_orig_source_addrs() ||\n\t(osdmap.is_down(from) && m->if_osd_failed())) {\n      dout(5) << \"preprocess_failure from dead osd.\" << from\n\t      << \", ignoring\" << dendl;\n      send_incremental(op, m->get_epoch()+1);\n      goto didit;\n    }\n  }\n\n\n  // weird?\n  if (osdmap.is_down(badboy)) {\n    dout(5) << \"preprocess_failure dne(/dup?): osd.\" << m->get_target_osd()\n\t    << \" \" << m->get_target_addrs()\n\t    << \", from \" << m->get_orig_source() << dendl;\n    if (m->get_epoch() < osdmap.get_epoch())\n      send_incremental(op, m->get_epoch()+1);\n    goto didit;\n  }\n  if (osdmap.get_addrs(badboy) != m->get_target_addrs()) {\n    dout(5) << \"preprocess_failure wrong osd: report osd.\" << m->get_target_osd()\n\t    << \" \" << m->get_target_addrs()\n\t    << \" != map's \" << osdmap.get_addrs(badboy)\n\t    << \", from \" << m->get_orig_source() << dendl;\n    if (m->get_epoch() < osdmap.get_epoch())\n      send_incremental(op, m->get_epoch()+1);\n    goto didit;\n  }\n\n  // already reported?\n  if (osdmap.is_down(badboy) ||\n      osdmap.get_up_from(badboy) > m->get_epoch()) {\n    dout(5) << \"preprocess_failure dup/old: osd.\" << m->get_target_osd()\n\t    << \" \" << m->get_target_addrs()\n\t    << \", from \" << m->get_orig_source() << dendl;\n    if (m->get_epoch() < osdmap.get_epoch())\n      send_incremental(op, m->get_epoch()+1);\n    goto didit;\n  }\n\n  if (!can_mark_down(badboy)) {\n    dout(5) << \"preprocess_failure ignoring report of osd.\"\n\t    << m->get_target_osd() << \" \" << m->get_target_addrs()\n\t    << \" from \" << m->get_orig_source() << dendl;\n    goto didit;\n  }\n\n  dout(10) << \"preprocess_failure new: osd.\" << m->get_target_osd()\n\t   << \" \" << m->get_target_addrs()\n\t   << \", from \" << m->get_orig_source() << dendl;\n  return false;\n\n didit:\n  mon->no_reply(op);\n  return true;\n}\n\nclass C_AckMarkedDown : public C_MonOp {\n  OSDMonitor *osdmon;\npublic:\n  C_AckMarkedDown(\n    OSDMonitor *osdmon,\n    MonOpRequestRef op)\n    : C_MonOp(op), osdmon(osdmon) {}\n\n  void _finish(int) override {\n    MOSDMarkMeDown *m = static_cast<MOSDMarkMeDown*>(op->get_req());\n    osdmon->mon->send_reply(\n      op,\n      new MOSDMarkMeDown(\n\tm->fsid,\n\tm->target_osd,\n\tm->target_addrs,\n\tm->get_epoch(),\n\tfalse));   // ACK itself does not request an ack\n  }\n  ~C_AckMarkedDown() override {\n  }\n};\n\nbool OSDMonitor::preprocess_mark_me_down(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDMarkMeDown *m = static_cast<MOSDMarkMeDown*>(op->get_req());\n  int from = m->target_osd;\n\n  // check permissions\n  if (check_source(m, m->fsid))\n    goto reply;\n\n  // first, verify the reporting host is valid\n  if (!m->get_orig_source().is_osd())\n    goto reply;\n\n  if (!osdmap.exists(from) ||\n      osdmap.is_down(from) ||\n      osdmap.get_addrs(from) != m->target_addrs) {\n    dout(5) << \"preprocess_mark_me_down from dead osd.\"\n\t    << from << \", ignoring\" << dendl;\n    send_incremental(op, m->get_epoch()+1);\n    goto reply;\n  }\n\n  // no down might be set\n  if (!can_mark_down(from))\n    goto reply;\n\n  dout(10) << \"MOSDMarkMeDown for: \" << m->get_orig_source()\n\t   << \" \" << m->target_addrs << dendl;\n  return false;\n\n reply:\n  if (m->request_ack) {\n    Context *c(new C_AckMarkedDown(this, op));\n    c->complete(0);\n  }\n  return true;\n}\n\nbool OSDMonitor::prepare_mark_me_down(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDMarkMeDown *m = static_cast<MOSDMarkMeDown*>(op->get_req());\n  int target_osd = m->target_osd;\n\n  assert(osdmap.is_up(target_osd));\n  assert(osdmap.get_addrs(target_osd) == m->target_addrs);\n\n  mon->clog->info() << \"osd.\" << target_osd << \" marked itself down\";\n  pending_inc.new_state[target_osd] = CEPH_OSD_UP;\n  if (m->request_ack)\n    wait_for_finished_proposal(op, new C_AckMarkedDown(this, op));\n  return true;\n}\n\nbool OSDMonitor::can_mark_down(int i)\n{\n  if (osdmap.test_flag(CEPH_OSDMAP_NODOWN)) {\n    dout(5) << __func__ << \" NODOWN flag set, will not mark osd.\" << i\n            << \" down\" << dendl;\n    return false;\n  }\n\n  if (osdmap.is_nodown(i)) {\n    dout(5) << __func__ << \" osd.\" << i << \" is marked as nodown, \"\n            << \"will not mark it down\" << dendl;\n    return false;\n  }\n\n  int num_osds = osdmap.get_num_osds();\n  if (num_osds == 0) {\n    dout(5) << __func__ << \" no osds\" << dendl;\n    return false;\n  }\n  int up = osdmap.get_num_up_osds() - pending_inc.get_net_marked_down(&osdmap);\n  float up_ratio = (float)up / (float)num_osds;\n  if (up_ratio < g_conf->mon_osd_min_up_ratio) {\n    dout(2) << __func__ << \" current up_ratio \" << up_ratio << \" < min \"\n\t    << g_conf->mon_osd_min_up_ratio\n\t    << \", will not mark osd.\" << i << \" down\" << dendl;\n    return false;\n  }\n  return true;\n}\n\nbool OSDMonitor::can_mark_up(int i)\n{\n  if (osdmap.test_flag(CEPH_OSDMAP_NOUP)) {\n    dout(5) << __func__ << \" NOUP flag set, will not mark osd.\" << i\n            << \" up\" << dendl;\n    return false;\n  }\n\n  if (osdmap.is_noup(i)) {\n    dout(5) << __func__ << \" osd.\" << i << \" is marked as noup, \"\n            << \"will not mark it up\" << dendl;\n    return false;\n  }\n\n  return true;\n}\n\n/**\n * @note the parameter @p i apparently only exists here so we can output the\n *\t osd's id on messages.\n */\nbool OSDMonitor::can_mark_out(int i)\n{\n  if (osdmap.test_flag(CEPH_OSDMAP_NOOUT)) {\n    dout(5) << __func__ << \" NOOUT flag set, will not mark osds out\" << dendl;\n    return false;\n  }\n\n  if (osdmap.is_noout(i)) {\n    dout(5) << __func__ << \" osd.\" << i << \" is marked as noout, \"\n            << \"will not mark it out\" << dendl;\n    return false;\n  }\n\n  int num_osds = osdmap.get_num_osds();\n  if (num_osds == 0) {\n    dout(5) << __func__ << \" no osds\" << dendl;\n    return false;\n  }\n  int in = osdmap.get_num_in_osds() - pending_inc.get_net_marked_out(&osdmap);\n  float in_ratio = (float)in / (float)num_osds;\n  if (in_ratio < g_conf->mon_osd_min_in_ratio) {\n    if (i >= 0)\n      dout(5) << __func__ << \" current in_ratio \" << in_ratio << \" < min \"\n\t      << g_conf->mon_osd_min_in_ratio\n\t      << \", will not mark osd.\" << i << \" out\" << dendl;\n    else\n      dout(5) << __func__ << \" current in_ratio \" << in_ratio << \" < min \"\n\t      << g_conf->mon_osd_min_in_ratio\n\t      << \", will not mark osds out\" << dendl;\n    return false;\n  }\n\n  return true;\n}\n\nbool OSDMonitor::can_mark_in(int i)\n{\n  if (osdmap.test_flag(CEPH_OSDMAP_NOIN)) {\n    dout(5) << __func__ << \" NOIN flag set, will not mark osd.\" << i\n            << \" in\" << dendl;\n    return false;\n  }\n\n  if (osdmap.is_noin(i)) {\n    dout(5) << __func__ << \" osd.\" << i << \" is marked as noin, \"\n            << \"will not mark it in\" << dendl;\n    return false;\n  }\n\n  return true;\n}\n\nbool OSDMonitor::check_failures(utime_t now)\n{\n  bool found_failure = false;\n  for (map<int,failure_info_t>::iterator p = failure_info.begin();\n       p != failure_info.end();\n       ++p) {\n    if (can_mark_down(p->first)) {\n      found_failure |= check_failure(now, p->first, p->second);\n    }\n  }\n  return found_failure;\n}\n\nbool OSDMonitor::check_failure(utime_t now, int target_osd, failure_info_t& fi)\n{\n  // already pending failure?\n  if (pending_inc.new_state.count(target_osd) &&\n      pending_inc.new_state[target_osd] & CEPH_OSD_UP) {\n    dout(10) << \" already pending failure\" << dendl;\n    return true;\n  }\n\n  set<string> reporters_by_subtree;\n  auto reporter_subtree_level = g_conf->get_val<string>(\"mon_osd_reporter_subtree_level\");\n  utime_t orig_grace(g_conf->osd_heartbeat_grace, 0);\n  utime_t max_failed_since = fi.get_failed_since();\n  utime_t failed_for = now - max_failed_since;\n\n  utime_t grace = orig_grace;\n  double my_grace = 0, peer_grace = 0;\n  double decay_k = 0;\n  if (g_conf->mon_osd_adjust_heartbeat_grace) {\n    double halflife = (double)g_conf->mon_osd_laggy_halflife;\n    decay_k = ::log(.5) / halflife;\n\n    // scale grace period based on historical probability of 'lagginess'\n    // (false positive failures due to slowness).\n    const osd_xinfo_t& xi = osdmap.get_xinfo(target_osd);\n    double decay = exp((double)failed_for * decay_k);\n    dout(20) << \" halflife \" << halflife << \" decay_k \" << decay_k\n\t     << \" failed_for \" << failed_for << \" decay \" << decay << dendl;\n    my_grace = decay * (double)xi.laggy_interval * xi.laggy_probability;\n    grace += my_grace;\n  }\n\n  // consider the peers reporting a failure a proxy for a potential\n  // 'subcluster' over the overall cluster that is similarly\n  // laggy.  this is clearly not true in all cases, but will sometimes\n  // help us localize the grace correction to a subset of the system\n  // (say, a rack with a bad switch) that is unhappy.\n  assert(fi.reporters.size());\n  for (map<int,failure_reporter_t>::iterator p = fi.reporters.begin();\n\tp != fi.reporters.end();\n\t++p) {\n    // get the parent bucket whose type matches with \"reporter_subtree_level\".\n    // fall back to OSD if the level doesn't exist.\n    map<string, string> reporter_loc = osdmap.crush->get_full_location(p->first);\n    map<string, string>::iterator iter = reporter_loc.find(reporter_subtree_level);\n    if (iter == reporter_loc.end()) {\n      reporters_by_subtree.insert(\"osd.\" + to_string(p->first));\n    } else {\n      reporters_by_subtree.insert(iter->second);\n    }\n    if (g_conf->mon_osd_adjust_heartbeat_grace) {\n      const osd_xinfo_t& xi = osdmap.get_xinfo(p->first);\n      utime_t elapsed = now - xi.down_stamp;\n      double decay = exp((double)elapsed * decay_k);\n      peer_grace += decay * (double)xi.laggy_interval * xi.laggy_probability;\n    }\n  }\n  \n  if (g_conf->mon_osd_adjust_heartbeat_grace) {\n    peer_grace /= (double)fi.reporters.size();\n    grace += peer_grace;\n  }\n\n  dout(10) << \" osd.\" << target_osd << \" has \"\n\t   << fi.reporters.size() << \" reporters, \"\n\t   << grace << \" grace (\" << orig_grace << \" + \" << my_grace\n\t   << \" + \" << peer_grace << \"), max_failed_since \" << max_failed_since\n\t   << dendl;\n\n  if (failed_for >= grace &&\n      reporters_by_subtree.size() >= g_conf->get_val<uint64_t>(\"mon_osd_min_down_reporters\")) {\n    dout(1) << \" we have enough reporters to mark osd.\" << target_osd\n\t    << \" down\" << dendl;\n    pending_inc.new_state[target_osd] = CEPH_OSD_UP;\n\n    mon->clog->info() << \"osd.\" << target_osd << \" failed (\"\n\t\t      << osdmap.crush->get_full_location_ordered_string(\n\t\t\ttarget_osd)\n\t\t      << \") (\"\n\t\t      << (int)reporters_by_subtree.size()\n\t\t      << \" reporters from different \"\n\t\t      << reporter_subtree_level << \" after \"\n\t\t      << failed_for << \" >= grace \" << grace << \")\";\n    return true;\n  }\n  return false;\n}\n\nvoid OSDMonitor::force_failure(int target_osd, int by)\n{\n  // already pending failure?\n  if (pending_inc.new_state.count(target_osd) &&\n      pending_inc.new_state[target_osd] & CEPH_OSD_UP) {\n    dout(10) << \" already pending failure\" << dendl;\n    return;\n  }\n\n  dout(1) << \" we're forcing failure of osd.\" << target_osd << dendl;\n  pending_inc.new_state[target_osd] = CEPH_OSD_UP;\n\n  mon->clog->info() << \"osd.\" << target_osd << \" failed (\"\n\t\t    << osdmap.crush->get_full_location_ordered_string(target_osd)\n\t\t    << \") (connection refused reported by osd.\" << by << \")\";\n  return;\n}\n\nbool OSDMonitor::prepare_failure(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDFailure *m = static_cast<MOSDFailure*>(op->get_req());\n  dout(1) << \"prepare_failure osd.\" << m->get_target_osd()\n\t  << \" \" << m->get_target_addrs()\n\t  << \" from \" << m->get_orig_source()\n          << \" is reporting failure:\" << m->if_osd_failed() << dendl;\n\n  int target_osd = m->get_target_osd();\n  int reporter = m->get_orig_source().num();\n  assert(osdmap.is_up(target_osd));\n  assert(osdmap.get_addrs(target_osd) == m->get_target_addrs());\n\n  if (m->if_osd_failed()) {\n    // calculate failure time\n    utime_t now = ceph_clock_now();\n    utime_t failed_since =\n      m->get_recv_stamp() - utime_t(m->failed_for, 0);\n\n    // add a report\n    if (m->is_immediate()) {\n      mon->clog->debug() << \"osd.\" << m->get_target_osd()\n\t\t\t << \" reported immediately failed by \"\n\t\t\t << m->get_orig_source();\n      force_failure(target_osd, reporter);\n      mon->no_reply(op);\n      return true;\n    }\n    mon->clog->debug() << \"osd.\" << m->get_target_osd() << \" reported failed by \"\n\t\t      << m->get_orig_source();\n\n    failure_info_t& fi = failure_info[target_osd];\n    MonOpRequestRef old_op = fi.add_report(reporter, failed_since, op);\n    if (old_op) {\n      mon->no_reply(old_op);\n    }\n\n    return check_failure(now, target_osd, fi);\n  } else {\n    // remove the report\n    mon->clog->debug() << \"osd.\" << m->get_target_osd()\n\t\t       << \" failure report canceled by \"\n\t\t       << m->get_orig_source();\n    if (failure_info.count(target_osd)) {\n      failure_info_t& fi = failure_info[target_osd];\n      MonOpRequestRef report_op = fi.cancel_report(reporter);\n      if (report_op) {\n        mon->no_reply(report_op);\n      }\n      if (fi.reporters.empty()) {\n\tdout(10) << \" removing last failure_info for osd.\" << target_osd\n\t\t << dendl;\n\tfailure_info.erase(target_osd);\n      } else {\n\tdout(10) << \" failure_info for osd.\" << target_osd << \" now \"\n\t\t << fi.reporters.size() << \" reporters\" << dendl;\n      }\n    } else {\n      dout(10) << \" no failure_info for osd.\" << target_osd << dendl;\n    }\n    mon->no_reply(op);\n  }\n\n  return false;\n}\n\nvoid OSDMonitor::process_failures()\n{\n  map<int,failure_info_t>::iterator p = failure_info.begin();\n  while (p != failure_info.end()) {\n    if (osdmap.is_up(p->first)) {\n      ++p;\n    } else {\n      dout(10) << \"process_failures osd.\" << p->first << dendl;\n      list<MonOpRequestRef> ls;\n      p->second.take_report_messages(ls);\n      failure_info.erase(p++);\n\n      while (!ls.empty()) {\n        MonOpRequestRef o = ls.front();\n        if (o) {\n          o->mark_event(__func__);\n          MOSDFailure *m = o->get_req<MOSDFailure>();\n          send_latest(o, m->get_epoch());\n\t  mon->no_reply(o);\n        }\n\tls.pop_front();\n      }\n    }\n  }\n}\n\nvoid OSDMonitor::take_all_failures(list<MonOpRequestRef>& ls)\n{\n  dout(10) << __func__ << \" on \" << failure_info.size() << \" osds\" << dendl;\n\n  for (map<int,failure_info_t>::iterator p = failure_info.begin();\n       p != failure_info.end();\n       ++p) {\n    p->second.take_report_messages(ls);\n  }\n  failure_info.clear();\n}\n\n\n// boot --\n\nbool OSDMonitor::preprocess_boot(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDBoot *m = static_cast<MOSDBoot*>(op->get_req());\n  int from = m->get_orig_source_inst().name.num();\n\n  // check permissions, ignore if failed (no response expected)\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"got preprocess_boot message from entity with insufficient caps\"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  if (m->sb.cluster_fsid != mon->monmap->fsid) {\n    dout(0) << \"preprocess_boot on fsid \" << m->sb.cluster_fsid\n\t    << \" != \" << mon->monmap->fsid << dendl;\n    goto ignore;\n  }\n\n  if (m->get_orig_source_inst().addr.is_blank_ip()) {\n    dout(0) << \"preprocess_boot got blank addr for \" << m->get_orig_source_inst() << dendl;\n    goto ignore;\n  }\n\n  assert(m->get_orig_source_inst().name.is_osd());\n\n  // check if osd has required features to boot\n  if (osdmap.require_osd_release >= CEPH_RELEASE_LUMINOUS &&\n      !HAVE_FEATURE(m->osd_features, SERVER_LUMINOUS)) {\n    mon->clog->info() << \"disallowing boot of OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because the osdmap requires\"\n\t\t      << \" CEPH_FEATURE_SERVER_LUMINOUS\"\n\t\t      << \" but the osd lacks CEPH_FEATURE_SERVER_LUMINOUS\";\n    goto ignore;\n  }\n\n  if (osdmap.require_osd_release >= CEPH_RELEASE_JEWEL &&\n      !(m->osd_features & CEPH_FEATURE_SERVER_JEWEL)) {\n    mon->clog->info() << \"disallowing boot of OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because the osdmap requires\"\n\t\t      << \" CEPH_FEATURE_SERVER_JEWEL\"\n\t\t      << \" but the osd lacks CEPH_FEATURE_SERVER_JEWEL\";\n    goto ignore;\n  }\n\n  if (osdmap.require_osd_release >= CEPH_RELEASE_KRAKEN &&\n      !HAVE_FEATURE(m->osd_features, SERVER_KRAKEN)) {\n    mon->clog->info() << \"disallowing boot of OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because the osdmap requires\"\n\t\t      << \" CEPH_FEATURE_SERVER_KRAKEN\"\n\t\t      << \" but the osd lacks CEPH_FEATURE_SERVER_KRAKEN\";\n    goto ignore;\n  }\n\n  if (osdmap.test_flag(CEPH_OSDMAP_RECOVERY_DELETES) &&\n      !(m->osd_features & CEPH_FEATURE_OSD_RECOVERY_DELETES)) {\n    mon->clog->info() << \"disallowing boot of OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because 'recovery_deletes' osdmap flag is set and OSD lacks the OSD_RECOVERY_DELETES feature\";\n    goto ignore;\n  }\n\n  // make sure upgrades stop at nautilus\n  if (HAVE_FEATURE(m->osd_features, SERVER_O) &&\n      osdmap.require_osd_release < CEPH_RELEASE_NAUTILUS) {\n    mon->clog->info() << \"disallowing boot of post-nautilus OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because require_osd_release < nautilus\";\n    goto ignore;\n  }\n\n  // already booted?\n  if (osdmap.is_up(from) &&\n      osdmap.get_addrs(from) == m->get_orig_source_addrs() &&\n      osdmap.get_cluster_addrs(from) == m->cluster_addrs) {\n    // yup.\n    dout(7) << \"preprocess_boot dup from \" << m->get_orig_source()\n\t    << \" \" << m->get_orig_source_addrs()\n\t    << \" == \" << osdmap.get_addrs(from) << dendl;\n    _booted(op, false);\n    return true;\n  }\n\n  if (osdmap.exists(from) &&\n      !osdmap.get_uuid(from).is_zero() &&\n      osdmap.get_uuid(from) != m->sb.osd_fsid) {\n    dout(7) << __func__ << \" from \" << m->get_orig_source_inst()\n            << \" clashes with existing osd: different fsid\"\n            << \" (ours: \" << osdmap.get_uuid(from)\n            << \" ; theirs: \" << m->sb.osd_fsid << \")\" << dendl;\n    goto ignore;\n  }\n\n  if (osdmap.exists(from) &&\n      osdmap.get_info(from).up_from > m->version &&\n      osdmap.get_most_recent_addrs(from) == m->get_orig_source_addrs()) {\n    dout(7) << \"prepare_boot msg from before last up_from, ignoring\" << dendl;\n    send_latest(op, m->sb.current_epoch+1);\n    return true;\n  }\n\n  // noup?\n  if (!can_mark_up(from)) {\n    dout(7) << \"preprocess_boot ignoring boot from \" << m->get_orig_source_inst() << dendl;\n    send_latest(op, m->sb.current_epoch+1);\n    return true;\n  }\n\n  dout(10) << \"preprocess_boot from \" << m->get_orig_source_inst() << dendl;\n  return false;\n\n ignore:\n  return true;\n}\n\nbool OSDMonitor::prepare_boot(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDBoot *m = static_cast<MOSDBoot*>(op->get_req());\n  dout(7) << __func__ << \" from \" << m->get_source()\n\t  << \" sb \" << m->sb\n\t  << \" client_addrs\" << m->get_connection()->get_peer_addrs()\n\t  << \" cluster_addrs \" << m->cluster_addrs\n\t  << \" hb_back_addrs \" << m->hb_back_addrs\n\t  << \" hb_front_addrs \" << m->hb_front_addrs\n\t  << dendl;\n\n  assert(m->get_orig_source().is_osd());\n  int from = m->get_orig_source().num();\n\n  // does this osd exist?\n  if (from >= osdmap.get_max_osd()) {\n    dout(1) << \"boot from osd.\" << from << \" >= max_osd \"\n\t    << osdmap.get_max_osd() << dendl;\n    return false;\n  }\n\n  int oldstate = osdmap.exists(from) ? osdmap.get_state(from) : CEPH_OSD_NEW;\n  if (pending_inc.new_state.count(from))\n    oldstate ^= pending_inc.new_state[from];\n\n  // already up?  mark down first?\n  if (osdmap.is_up(from)) {\n    dout(7) << __func__ << \" was up, first marking down osd.\" << from << \" \"\n\t    << osdmap.get_addrs(from) << dendl;\n    // preprocess should have caught these;  if not, assert.\n    assert(osdmap.get_addrs(from) != m->get_orig_source_addrs() ||\n           osdmap.get_cluster_addrs(from) != m->cluster_addrs);\n    assert(osdmap.get_uuid(from) == m->sb.osd_fsid);\n\n    if (pending_inc.new_state.count(from) == 0 ||\n\t(pending_inc.new_state[from] & CEPH_OSD_UP) == 0) {\n      // mark previous guy down\n      pending_inc.new_state[from] = CEPH_OSD_UP;\n    }\n    wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n  } else if (pending_inc.new_up_client.count(from)) {\n    // already prepared, just wait\n    dout(7) << __func__ << \" already prepared, waiting on \"\n\t    << m->get_orig_source_addr() << dendl;\n    wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n  } else {\n    // mark new guy up.\n    pending_inc.new_up_client[from] = m->get_orig_source_addrs();\n    pending_inc.new_up_cluster[from] = m->cluster_addrs;\n    pending_inc.new_hb_back_up[from] = m->hb_back_addrs;\n    pending_inc.new_hb_front_up[from] = m->hb_front_addrs;\n\n    down_pending_out.erase(from);  // if any\n\n    if (m->sb.weight)\n      osd_weight[from] = m->sb.weight;\n\n    // set uuid?\n    dout(10) << \" setting osd.\" << from << \" uuid to \" << m->sb.osd_fsid\n\t     << dendl;\n    if (!osdmap.exists(from) || osdmap.get_uuid(from) != m->sb.osd_fsid) {\n      // preprocess should have caught this;  if not, assert.\n      assert(!osdmap.exists(from) || osdmap.get_uuid(from).is_zero());\n      pending_inc.new_uuid[from] = m->sb.osd_fsid;\n    }\n\n    // fresh osd?\n    if (m->sb.newest_map == 0 && osdmap.exists(from)) {\n      const osd_info_t& i = osdmap.get_info(from);\n      if (i.up_from > i.lost_at) {\n\tdout(10) << \" fresh osd; marking lost_at too\" << dendl;\n\tpending_inc.new_lost[from] = osdmap.get_epoch();\n      }\n    }\n\n    // metadata\n    bufferlist osd_metadata;\n    encode(m->metadata, osd_metadata);\n    pending_metadata[from] = osd_metadata;\n    pending_metadata_rm.erase(from);\n\n    // adjust last clean unmount epoch?\n    const osd_info_t& info = osdmap.get_info(from);\n    dout(10) << \" old osd_info: \" << info << dendl;\n    if (m->sb.mounted > info.last_clean_begin ||\n\t(m->sb.mounted == info.last_clean_begin &&\n\t m->sb.clean_thru > info.last_clean_end)) {\n      epoch_t begin = m->sb.mounted;\n      epoch_t end = m->sb.clean_thru;\n\n      dout(10) << __func__ << \" osd.\" << from << \" last_clean_interval \"\n\t       << \"[\" << info.last_clean_begin << \",\" << info.last_clean_end\n\t       << \") -> [\" << begin << \"-\" << end << \")\"\n\t       << dendl;\n      pending_inc.new_last_clean_interval[from] =\n\tpair<epoch_t,epoch_t>(begin, end);\n    }\n\n    osd_xinfo_t xi = osdmap.get_xinfo(from);\n    if (m->boot_epoch == 0) {\n      xi.laggy_probability *= (1.0 - g_conf->mon_osd_laggy_weight);\n      xi.laggy_interval *= (1.0 - g_conf->mon_osd_laggy_weight);\n      dout(10) << \" not laggy, new xi \" << xi << dendl;\n    } else {\n      if (xi.down_stamp.sec()) {\n        int interval = ceph_clock_now().sec() -\n\t  xi.down_stamp.sec();\n        if (g_conf->mon_osd_laggy_max_interval &&\n\t    (interval > g_conf->mon_osd_laggy_max_interval)) {\n          interval =  g_conf->mon_osd_laggy_max_interval;\n        }\n        xi.laggy_interval =\n\t  interval * g_conf->mon_osd_laggy_weight +\n\t  xi.laggy_interval * (1.0 - g_conf->mon_osd_laggy_weight);\n      }\n      xi.laggy_probability =\n\tg_conf->mon_osd_laggy_weight +\n\txi.laggy_probability * (1.0 - g_conf->mon_osd_laggy_weight);\n      dout(10) << \" laggy, now xi \" << xi << dendl;\n    }\n\n    // set features shared by the osd\n    if (m->osd_features)\n      xi.features = m->osd_features;\n    else\n      xi.features = m->get_connection()->get_features();\n\n    // mark in?\n    if ((g_conf->mon_osd_auto_mark_auto_out_in &&\n\t (oldstate & CEPH_OSD_AUTOOUT)) ||\n\t(g_conf->mon_osd_auto_mark_new_in && (oldstate & CEPH_OSD_NEW)) ||\n\t(g_conf->mon_osd_auto_mark_in)) {\n      if (can_mark_in(from)) {\n\tif (osdmap.osd_xinfo[from].old_weight > 0) {\n\t  pending_inc.new_weight[from] = osdmap.osd_xinfo[from].old_weight;\n\t  xi.old_weight = 0;\n\t} else {\n\t  pending_inc.new_weight[from] = CEPH_OSD_IN;\n\t}\n      } else {\n\tdout(7) << __func__ << \" NOIN set, will not mark in \"\n\t\t<< m->get_orig_source_addr() << dendl;\n      }\n    }\n\n    pending_inc.new_xinfo[from] = xi;\n\n    // wait\n    wait_for_finished_proposal(op, new C_Booted(this, op));\n  }\n  return true;\n}\n\nvoid OSDMonitor::_booted(MonOpRequestRef op, bool logit)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDBoot *m = static_cast<MOSDBoot*>(op->get_req());\n  dout(7) << \"_booted \" << m->get_orig_source_inst() \n\t  << \" w \" << m->sb.weight << \" from \" << m->sb.current_epoch << dendl;\n\n  if (logit) {\n    mon->clog->info() << m->get_orig_source_inst() << \" boot\";\n  }\n\n  send_latest(op, m->sb.current_epoch+1);\n}\n\n\n// -------------\n// full\n\nbool OSDMonitor::preprocess_full(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDFull *m = static_cast<MOSDFull*>(op->get_req());\n  int from = m->get_orig_source().num();\n  set<string> state;\n  unsigned mask = CEPH_OSD_NEARFULL | CEPH_OSD_BACKFILLFULL | CEPH_OSD_FULL;\n\n  // check permissions, ignore if failed\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"MOSDFull from entity with insufficient privileges:\"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  // ignore a full message from the osd instance that already went down\n  if (!osdmap.exists(from)) {\n    dout(7) << __func__ << \" ignoring full message from nonexistent \"\n\t    << m->get_orig_source_inst() << dendl;\n    goto ignore;\n  }\n  if ((!osdmap.is_up(from) &&\n       osdmap.get_most_recent_addrs(from) == m->get_orig_source_addrs()) ||\n      (osdmap.is_up(from) &&\n       osdmap.get_addrs(from) != m->get_orig_source_addrs())) {\n    dout(7) << __func__ << \" ignoring full message from down \"\n\t    << m->get_orig_source_inst() << dendl;\n    goto ignore;\n  }\n\n  OSDMap::calc_state_set(osdmap.get_state(from), state);\n\n  if ((osdmap.get_state(from) & mask) == m->state) {\n    dout(7) << __func__ << \" state already \" << state << \" for osd.\" << from\n\t    << \" \" << m->get_orig_source_inst() << dendl;\n    _reply_map(op, m->version);\n    goto ignore;\n  }\n\n  dout(10) << __func__ << \" want state \" << state << \" for osd.\" << from\n\t   << \" \" << m->get_orig_source_inst() << dendl;\n  return false;\n\n ignore:\n  return true;\n}\n\nbool OSDMonitor::prepare_full(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  const MOSDFull *m = static_cast<MOSDFull*>(op->get_req());\n  const int from = m->get_orig_source().num();\n\n  const unsigned mask = CEPH_OSD_NEARFULL | CEPH_OSD_BACKFILLFULL | CEPH_OSD_FULL;\n  const unsigned want_state = m->state & mask;  // safety first\n\n  unsigned cur_state = osdmap.get_state(from);\n  auto p = pending_inc.new_state.find(from);\n  if (p != pending_inc.new_state.end()) {\n    cur_state ^= p->second;\n  }\n  cur_state &= mask;\n\n  set<string> want_state_set, cur_state_set;\n  OSDMap::calc_state_set(want_state, want_state_set);\n  OSDMap::calc_state_set(cur_state, cur_state_set);\n\n  if (cur_state != want_state) {\n    if (p != pending_inc.new_state.end()) {\n      p->second &= ~mask;\n    } else {\n      pending_inc.new_state[from] = 0;\n    }\n    pending_inc.new_state[from] |= (osdmap.get_state(from) & mask) ^ want_state;\n    dout(7) << __func__ << \" osd.\" << from << \" \" << cur_state_set\n\t    << \" -> \" << want_state_set << dendl;\n  } else {\n    dout(7) << __func__ << \" osd.\" << from << \" \" << cur_state_set\n\t    << \" = wanted \" << want_state_set << \", just waiting\" << dendl;\n  }\n\n  wait_for_finished_proposal(op, new C_ReplyMap(this, op, m->version));\n  return true;\n}\n\n// -------------\n// alive\n\nbool OSDMonitor::preprocess_alive(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDAlive *m = static_cast<MOSDAlive*>(op->get_req());\n  int from = m->get_orig_source().num();\n\n  // check permissions, ignore if failed\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"attempt to send MOSDAlive from entity with insufficient privileges:\"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  if (!osdmap.is_up(from) ||\n      osdmap.get_addrs(from) != m->get_orig_source_addrs()) {\n    dout(7) << \"preprocess_alive ignoring alive message from down \"\n\t    << m->get_orig_source() << \" \" << m->get_orig_source_addrs()\n\t    << dendl;\n    goto ignore;\n  }\n\n  if (osdmap.get_up_thru(from) >= m->want) {\n    // yup.\n    dout(7) << \"preprocess_alive want up_thru \" << m->want << \" dup from \" << m->get_orig_source_inst() << dendl;\n    _reply_map(op, m->version);\n    return true;\n  }\n\n  dout(10) << \"preprocess_alive want up_thru \" << m->want\n\t   << \" from \" << m->get_orig_source_inst() << dendl;\n  return false;\n\n ignore:\n  return true;\n}\n\nbool OSDMonitor::prepare_alive(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDAlive *m = static_cast<MOSDAlive*>(op->get_req());\n  int from = m->get_orig_source().num();\n\n  if (0) {  // we probably don't care much about these\n    mon->clog->debug() << m->get_orig_source_inst() << \" alive\";\n  }\n\n  dout(7) << \"prepare_alive want up_thru \" << m->want << \" have \" << m->version\n\t  << \" from \" << m->get_orig_source_inst() << dendl;\n\n  update_up_thru(from, m->version); // set to the latest map the OSD has\n  wait_for_finished_proposal(op, new C_ReplyMap(this, op, m->version));\n  return true;\n}\n\nvoid OSDMonitor::_reply_map(MonOpRequestRef op, epoch_t e)\n{\n  op->mark_osdmon_event(__func__);\n  dout(7) << \"_reply_map \" << e\n\t  << \" from \" << op->get_req()->get_orig_source_inst()\n\t  << dendl;\n  send_latest(op, e);\n}\n\n// pg_created\nbool OSDMonitor::preprocess_pg_created(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  auto m = static_cast<MOSDPGCreated*>(op->get_req());\n  dout(10) << __func__ << \" \" << *m << dendl;\n  auto session = m->get_session();\n  mon->no_reply(op);\n  if (!session) {\n    dout(10) << __func__ << \": no monitor session!\" << dendl;\n    return true;\n  }\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    derr << __func__ << \" received from entity \"\n         << \"with insufficient privileges \" << session->caps << dendl;\n    return true;\n  }\n  // always forward the \"created!\" to the leader\n  return false;\n}\n\nbool OSDMonitor::prepare_pg_created(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  auto m = static_cast<MOSDPGCreated*>(op->get_req());\n  dout(10) << __func__ << \" \" << *m << dendl;\n  auto src = m->get_orig_source();\n  auto from = src.num();\n  if (!src.is_osd() ||\n      !mon->osdmon()->osdmap.is_up(from) ||\n      m->get_orig_source_addrs() != mon->osdmon()->osdmap.get_addrs(from)) {\n    dout(1) << __func__ << \" ignoring stats from non-active osd.\" << dendl;\n    return false;\n  }\n  pending_created_pgs.push_back(m->pgid);\n  return true;\n}\n\n// -------------\n// pg_temp changes\n\nbool OSDMonitor::preprocess_pgtemp(MonOpRequestRef op)\n{\n  MOSDPGTemp *m = static_cast<MOSDPGTemp*>(op->get_req());\n  dout(10) << \"preprocess_pgtemp \" << *m << dendl;\n  mempool::osdmap::vector<int> empty;\n  int from = m->get_orig_source().num();\n  size_t ignore_cnt = 0;\n\n  // check caps\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"attempt to send MOSDPGTemp from entity with insufficient caps \"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  if (!osdmap.is_up(from) ||\n      osdmap.get_addrs(from) != m->get_orig_source_addrs()) {\n    dout(7) << \"ignoring pgtemp message from down \"\n\t    << m->get_orig_source() << \" \" << m->get_orig_source_addrs()\n\t    << dendl;\n    goto ignore;\n  }\n\n  if (m->forced) {\n    return false;\n  }\n\n  for (auto p = m->pg_temp.begin(); p != m->pg_temp.end(); ++p) {\n    dout(20) << \" \" << p->first\n\t     << (osdmap.pg_temp->count(p->first) ? osdmap.pg_temp->get(p->first) : empty)\n             << \" -> \" << p->second << dendl;\n\n    // does the pool exist?\n    if (!osdmap.have_pg_pool(p->first.pool())) {\n      /*\n       * 1. If the osdmap does not have the pool, it means the pool has been\n       *    removed in-between the osd sending this message and us handling it.\n       * 2. If osdmap doesn't have the pool, it is safe to assume the pool does\n       *    not exist in the pending either, as the osds would not send a\n       *    message about a pool they know nothing about (yet).\n       * 3. However, if the pool does exist in the pending, then it must be a\n       *    new pool, and not relevant to this message (see 1).\n       */\n      dout(10) << __func__ << \" ignore \" << p->first << \" -> \" << p->second\n               << \": pool has been removed\" << dendl;\n      ignore_cnt++;\n      continue;\n    }\n\n    int acting_primary = -1;\n    osdmap.pg_to_up_acting_osds(\n      p->first, nullptr, nullptr, nullptr, &acting_primary);\n    if (acting_primary != from) {\n      /* If the source isn't the primary based on the current osdmap, we know\n       * that the interval changed and that we can discard this message.\n       * Indeed, we must do so to avoid 16127 since we can't otherwise determine\n       * which of two pg temp mappings on the same pg is more recent.\n       */\n      dout(10) << __func__ << \" ignore \" << p->first << \" -> \" << p->second\n\t       << \": primary has changed\" << dendl;\n      ignore_cnt++;\n      continue;\n    }\n\n    // removal?\n    if (p->second.empty() && (osdmap.pg_temp->count(p->first) ||\n\t\t\t      osdmap.primary_temp->count(p->first)))\n      return false;\n    // change?\n    //  NOTE: we assume that this will clear pg_primary, so consider\n    //        an existing pg_primary field to imply a change\n    if (p->second.size() &&\n\t(osdmap.pg_temp->count(p->first) == 0 ||\n\t osdmap.pg_temp->get(p->first) != p->second ||\n\t osdmap.primary_temp->count(p->first)))\n      return false;\n  }\n\n  // should we ignore all the pgs?\n  if (ignore_cnt == m->pg_temp.size())\n    goto ignore;\n\n  dout(7) << \"preprocess_pgtemp e\" << m->map_epoch << \" no changes from \" << m->get_orig_source_inst() << dendl;\n  _reply_map(op, m->map_epoch);\n  return true;\n\n ignore:\n  return true;\n}\n\nvoid OSDMonitor::update_up_thru(int from, epoch_t up_thru)\n{\n  epoch_t old_up_thru = osdmap.get_up_thru(from);\n  auto ut = pending_inc.new_up_thru.find(from);\n  if (ut != pending_inc.new_up_thru.end()) {\n    old_up_thru = ut->second;\n  }\n  if (up_thru > old_up_thru) {\n    // set up_thru too, so the osd doesn't have to ask again\n    pending_inc.new_up_thru[from] = up_thru;\n  }\n}\n\nbool OSDMonitor::prepare_pgtemp(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDPGTemp *m = static_cast<MOSDPGTemp*>(op->get_req());\n  int from = m->get_orig_source().num();\n  dout(7) << \"prepare_pgtemp e\" << m->map_epoch << \" from \" << m->get_orig_source_inst() << dendl;\n  for (map<pg_t,vector<int32_t> >::iterator p = m->pg_temp.begin(); p != m->pg_temp.end(); ++p) {\n    uint64_t pool = p->first.pool();\n    if (pending_inc.old_pools.count(pool)) {\n      dout(10) << __func__ << \" ignore \" << p->first << \" -> \" << p->second\n               << \": pool pending removal\" << dendl;\n      continue;\n    }\n    if (!osdmap.have_pg_pool(pool)) {\n      dout(10) << __func__ << \" ignore \" << p->first << \" -> \" << p->second\n               << \": pool has been removed\" << dendl;\n      continue;\n    }\n    pending_inc.new_pg_temp[p->first] =\n      mempool::osdmap::vector<int>(p->second.begin(), p->second.end());\n\n    // unconditionally clear pg_primary (until this message can encode\n    // a change for that, too.. at which point we need to also fix\n    // preprocess_pg_temp)\n    if (osdmap.primary_temp->count(p->first) ||\n\tpending_inc.new_primary_temp.count(p->first))\n      pending_inc.new_primary_temp[p->first] = -1;\n  }\n\n  // set up_thru too, so the osd doesn't have to ask again\n  update_up_thru(from, m->map_epoch);\n\n  wait_for_finished_proposal(op, new C_ReplyMap(this, op, m->map_epoch));\n  return true;\n}\n\n\n// ---\n\nbool OSDMonitor::preprocess_remove_snaps(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MRemoveSnaps *m = static_cast<MRemoveSnaps*>(op->get_req());\n  dout(7) << \"preprocess_remove_snaps \" << *m << dendl;\n\n  // check privilege, ignore if failed\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->caps.is_capable(\n\tcct,\n\tCEPH_ENTITY_TYPE_MON,\n\tsession->entity_name,\n        \"osd\", \"osd pool rmsnap\", {}, true, true, false)) {\n    dout(0) << \"got preprocess_remove_snaps from entity with insufficient caps \"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  for (map<int, vector<snapid_t> >::iterator q = m->snaps.begin();\n       q != m->snaps.end();\n       ++q) {\n    if (!osdmap.have_pg_pool(q->first)) {\n      dout(10) << \" ignoring removed_snaps \" << q->second << \" on non-existent pool \" << q->first << dendl;\n      continue;\n    }\n    const pg_pool_t *pi = osdmap.get_pg_pool(q->first);\n    for (vector<snapid_t>::iterator p = q->second.begin();\n\t p != q->second.end();\n\t ++p) {\n      if (*p > pi->get_snap_seq() ||\n\t  !pi->removed_snaps.contains(*p))\n\treturn false;\n    }\n  }\n\n ignore:\n  return true;\n}\n\nbool OSDMonitor::prepare_remove_snaps(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MRemoveSnaps *m = static_cast<MRemoveSnaps*>(op->get_req());\n  dout(7) << \"prepare_remove_snaps \" << *m << dendl;\n\n  for (map<int, vector<snapid_t> >::iterator p = m->snaps.begin();\n       p != m->snaps.end();\n       ++p) {\n\n    if (!osdmap.have_pg_pool(p->first)) {\n      dout(10) << \" ignoring removed_snaps \" << p->second << \" on non-existent pool \" << p->first << dendl;\n      continue;\n    }\n\n    pg_pool_t& pi = osdmap.pools[p->first];\n    for (vector<snapid_t>::iterator q = p->second.begin();\n\t q != p->second.end();\n\t ++q) {\n      if (!pi.removed_snaps.contains(*q) &&\n\t  (!pending_inc.new_pools.count(p->first) ||\n\t   !pending_inc.new_pools[p->first].removed_snaps.contains(*q))) {\n\tpg_pool_t *newpi = pending_inc.get_new_pool(p->first, &pi);\n\tnewpi->removed_snaps.insert(*q);\n\tnewpi->flags |= pg_pool_t::FLAG_SELFMANAGED_SNAPS;\n\tdout(10) << \" pool \" << p->first << \" removed_snaps added \" << *q\n\t\t << \" (now \" << newpi->removed_snaps << \")\" << dendl;\n\tif (*q > newpi->get_snap_seq()) {\n\t  dout(10) << \" pool \" << p->first << \" snap_seq \"\n\t\t   << newpi->get_snap_seq() << \" -> \" << *q << dendl;\n\t  newpi->set_snap_seq(*q);\n\t}\n\tnewpi->set_snap_epoch(pending_inc.epoch);\n\tpending_inc.new_removed_snaps[p->first].insert(*q);\n      }\n    }\n  }\n  return true;\n}\n\n// osd beacon\nbool OSDMonitor::preprocess_beacon(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  auto beacon = static_cast<MOSDBeacon*>(op->get_req());\n  // check caps\n  auto session = beacon->get_session();\n  mon->no_reply(op);\n  if (!session) {\n    dout(10) << __func__ << \" no monitor session!\" << dendl;\n    return true;\n  }\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    derr << __func__ << \" received from entity \"\n         << \"with insufficient privileges \" << session->caps << dendl;\n    return true;\n  }\n  // Always forward the beacon to the leader, even if they are the same as\n  // the old one. The leader will mark as down osds that haven't sent\n  // beacon for a few minutes.\n  return false;\n}\n\nbool OSDMonitor::prepare_beacon(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  const auto beacon = static_cast<MOSDBeacon*>(op->get_req());\n  const auto src = beacon->get_orig_source();\n  dout(10) << __func__ << \" \" << *beacon\n\t   << \" from \" << src << dendl;\n  int from = src.num();\n\n  if (!src.is_osd() ||\n      !osdmap.is_up(from) ||\n      beacon->get_orig_source_addrs() != osdmap.get_addrs(from)) {\n    dout(1) << \" ignoring beacon from non-active osd.\" << from << dendl;\n    return false;\n  }\n\n  last_osd_report[from] = ceph_clock_now();\n  osd_epochs[from] = beacon->version;\n\n  for (const auto& pg : beacon->pgs) {\n    last_epoch_clean.report(pg, beacon->min_last_epoch_clean);\n  }\n  return false;\n}\n\n// ---------------\n// map helpers\n\nvoid OSDMonitor::send_latest(MonOpRequestRef op, epoch_t start)\n{\n  op->mark_osdmon_event(__func__);\n  dout(5) << \"send_latest to \" << op->get_req()->get_orig_source_inst()\n\t  << \" start \" << start << dendl;\n  if (start == 0)\n    send_full(op);\n  else\n    send_incremental(op, start);\n}\n\n\nMOSDMap *OSDMonitor::build_latest_full(uint64_t features)\n{\n  MOSDMap *r = new MOSDMap(mon->monmap->fsid, features);\n  get_version_full(osdmap.get_epoch(), features, r->maps[osdmap.get_epoch()]);\n  r->oldest_map = get_first_committed();\n  r->newest_map = osdmap.get_epoch();\n  return r;\n}\n\nMOSDMap *OSDMonitor::build_incremental(epoch_t from, epoch_t to, uint64_t features)\n{\n  dout(10) << \"build_incremental [\" << from << \"..\" << to << \"] with features \"\n\t   << std::hex << features << std::dec << dendl;\n  MOSDMap *m = new MOSDMap(mon->monmap->fsid, features);\n  m->oldest_map = get_first_committed();\n  m->newest_map = osdmap.get_epoch();\n\n  for (epoch_t e = to; e >= from && e > 0; e--) {\n    bufferlist bl;\n    int err = get_version(e, features, bl);\n    if (err == 0) {\n      assert(bl.length());\n      // if (get_version(e, bl) > 0) {\n      dout(20) << \"build_incremental    inc \" << e << \" \"\n\t       << bl.length() << \" bytes\" << dendl;\n      m->incremental_maps[e] = bl;\n    } else {\n      assert(err == -ENOENT);\n      assert(!bl.length());\n      get_version_full(e, features, bl);\n      if (bl.length() > 0) {\n      //else if (get_version(\"full\", e, bl) > 0) {\n      dout(20) << \"build_incremental   full \" << e << \" \"\n\t       << bl.length() << \" bytes\" << dendl;\n      m->maps[e] = bl;\n      } else {\n\tceph_abort();  // we should have all maps.\n      }\n    }\n  }\n  return m;\n}\n\nvoid OSDMonitor::send_full(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  dout(5) << \"send_full to \" << op->get_req()->get_orig_source_inst() << dendl;\n  mon->send_reply(op, build_latest_full(op->get_session()->con_features));\n}\n\nvoid OSDMonitor::send_incremental(MonOpRequestRef op, epoch_t first)\n{\n  op->mark_osdmon_event(__func__);\n\n  MonSession *s = op->get_session();\n  assert(s);\n\n  if (s->proxy_con) {\n    // oh, we can tell the other mon to do it\n    dout(10) << __func__ << \" asking proxying mon to send_incremental from \"\n\t     << first << dendl;\n    MRoute *r = new MRoute(s->proxy_tid, NULL);\n    r->send_osdmap_first = first;\n    s->proxy_con->send_message(r);\n    op->mark_event(\"reply: send routed send_osdmap_first reply\");\n  } else {\n    // do it ourselves\n    send_incremental(first, s, false, op);\n  }\n}\n\nvoid OSDMonitor::send_incremental(epoch_t first,\n\t\t\t\t  MonSession *session,\n\t\t\t\t  bool onetime,\n\t\t\t\t  MonOpRequestRef req)\n{\n  dout(5) << \"send_incremental [\" << first << \"..\" << osdmap.get_epoch() << \"]\"\n\t  << \" to \" << session->name << dendl;\n\n  // get feature of the peer\n  // use quorum_con_features, if it's an anonymous connection.\n  uint64_t features = session->con_features ? session->con_features :\n    mon->get_quorum_con_features();\n\n  if (first <= session->osd_epoch) {\n    dout(10) << __func__ << \" \" << session->name << \" should already have epoch \"\n\t     << session->osd_epoch << dendl;\n    first = session->osd_epoch + 1;\n  }\n\n  if (first < get_first_committed()) {\n    MOSDMap *m = new MOSDMap(osdmap.get_fsid(), features);\n    m->oldest_map = get_first_committed();\n    m->newest_map = osdmap.get_epoch();\n\n    // share removed snaps during the gap\n    get_removed_snaps_range(first, m->oldest_map, &m->gap_removed_snaps);\n\n    first = get_first_committed();\n    bufferlist bl;\n    int err = get_version_full(first, features, bl);\n    assert(err == 0);\n    assert(bl.length());\n    dout(20) << \"send_incremental starting with base full \"\n\t     << first << \" \" << bl.length() << \" bytes\" << dendl;\n    m->maps[first] = bl;\n\n    if (req) {\n      mon->send_reply(req, m);\n      session->osd_epoch = first;\n      return;\n    } else {\n      session->con->send_message(m);\n      session->osd_epoch = first;\n    }\n    first++;\n  }\n\n  while (first <= osdmap.get_epoch()) {\n    epoch_t last = std::min<epoch_t>(first + g_conf->osd_map_message_max - 1,\n\t\t\t\t     osdmap.get_epoch());\n    MOSDMap *m = build_incremental(first, last, features);\n\n    if (req) {\n      // send some maps.  it may not be all of them, but it will get them\n      // started.\n      mon->send_reply(req, m);\n    } else {\n      session->con->send_message(m);\n      first = last + 1;\n    }\n    session->osd_epoch = last;\n    if (onetime || req)\n      break;\n  }\n}\n\nvoid OSDMonitor::get_removed_snaps_range(\n  epoch_t start, epoch_t end,\n  mempool::osdmap::map<int64_t,OSDMap::snap_interval_set_t> *gap_removed_snaps)\n{\n  // we only care about pools that exist now.\n  for (auto& p : osdmap.get_pools()) {\n    auto& t = (*gap_removed_snaps)[p.first];\n    for (epoch_t epoch = start; epoch < end; ++epoch) {\n      string k = make_snap_epoch_key(p.first, epoch);\n      bufferlist v;\n      mon->store->get(OSD_SNAP_PREFIX, k, v);\n      if (v.length()) {\n\tauto q = v.cbegin();\n\tOSDMap::snap_interval_set_t snaps;\n\tdecode(snaps, q);\n\tt.union_of(snaps);\n      }\n    }\n    dout(10) << __func__ << \" \" << p.first << \" \" << t << dendl;\n  }\n}\n\nint OSDMonitor::get_version(version_t ver, bufferlist& bl)\n{\n  return get_version(ver, mon->get_quorum_con_features(), bl);\n}\n\nvoid OSDMonitor::reencode_incremental_map(bufferlist& bl, uint64_t features)\n{\n  OSDMap::Incremental inc;\n  auto q = bl.cbegin();\n  inc.decode(q);\n  // always encode with subset of osdmap's canonical features\n  uint64_t f = features & inc.encode_features;\n  dout(20) << __func__ << \" \" << inc.epoch << \" with features \" << f\n\t   << dendl;\n  bl.clear();\n  if (inc.fullmap.length()) {\n    // embedded full map?\n    OSDMap m;\n    m.decode(inc.fullmap);\n    inc.fullmap.clear();\n    m.encode(inc.fullmap, f | CEPH_FEATURE_RESERVED);\n  }\n  if (inc.crush.length()) {\n    // embedded crush map\n    CrushWrapper c;\n    auto p = inc.crush.cbegin();\n    c.decode(p);\n    inc.crush.clear();\n    c.encode(inc.crush, f);\n  }\n  inc.encode(bl, f | CEPH_FEATURE_RESERVED);\n}\n\nvoid OSDMonitor::reencode_full_map(bufferlist& bl, uint64_t features)\n{\n  OSDMap m;\n  auto q = bl.cbegin();\n  m.decode(q);\n  // always encode with subset of osdmap's canonical features\n  uint64_t f = features & m.get_encoding_features();\n  dout(20) << __func__ << \" \" << m.get_epoch() << \" with features \" << f\n\t   << dendl;\n  bl.clear();\n  m.encode(bl, f | CEPH_FEATURE_RESERVED);\n}\n\nint OSDMonitor::get_version(version_t ver, uint64_t features, bufferlist& bl)\n{\n  uint64_t significant_features = OSDMap::get_significant_features(features);\n  if (inc_osd_cache.lookup({ver, significant_features}, &bl)) {\n    return 0;\n  }\n  int ret = PaxosService::get_version(ver, bl);\n  if (ret < 0) {\n    return ret;\n  }\n  // NOTE: this check is imprecise; the OSDMap encoding features may\n  // be a subset of the latest mon quorum features, but worst case we\n  // reencode once and then cache the (identical) result under both\n  // feature masks.\n  if (significant_features !=\n      OSDMap::get_significant_features(mon->get_quorum_con_features())) {\n    reencode_incremental_map(bl, features);\n  }\n  inc_osd_cache.add({ver, significant_features}, bl);\n  return 0;\n}\n\nint OSDMonitor::get_inc(version_t ver, OSDMap::Incremental& inc)\n{\n  bufferlist inc_bl;\n  int err = get_version(ver, inc_bl);\n  ceph_assert(err == 0);\n  ceph_assert(inc_bl.length());\n\n  auto p = inc_bl.cbegin();\n  inc.decode(p);\n  dout(10) << __func__ << \"     \"\n           << \" epoch \" << inc.epoch\n           << \" inc_crc \" << inc.inc_crc\n           << \" full_crc \" << inc.full_crc\n           << \" encode_features \" << inc.encode_features << dendl;\n  return 0;\n}\n\nint OSDMonitor::get_full_from_pinned_map(version_t ver, bufferlist& bl)\n{\n  dout(10) << __func__ << \" ver \" << ver << dendl;\n\n  version_t closest_pinned = osdmap_manifest.get_lower_closest_pinned(ver);\n  if (closest_pinned == 0) {\n    return -ENOENT;\n  }\n  if (closest_pinned > ver) {\n    dout(0) << __func__ << \" pinned: \" << osdmap_manifest.pinned << dendl;\n  }\n  ceph_assert(closest_pinned <= ver);\n\n  dout(10) << __func__ << \" closest pinned ver \" << closest_pinned << dendl;\n\n  // get osdmap incremental maps and apply on top of this one.\n  bufferlist osdm_bl;\n  bool has_cached_osdmap = false;\n  for (version_t v = ver-1; v >= closest_pinned; --v) {\n    if (full_osd_cache.lookup({v, mon->get_quorum_con_features()},\n                                &osdm_bl)) {\n      dout(10) << __func__ << \" found map in cache ver \" << v << dendl;\n      closest_pinned = v;\n      has_cached_osdmap = true;\n      break;\n    }\n  }\n\n  if (!has_cached_osdmap) {\n    int err = PaxosService::get_version_full(closest_pinned, osdm_bl);\n    if (err != 0) {\n      derr << __func__ << \" closest pinned map ver \" << closest_pinned\n           << \" not available! error: \" << cpp_strerror(err) << dendl;\n    }\n    ceph_assert(err == 0);\n  }\n\n  ceph_assert(osdm_bl.length());\n\n  OSDMap osdm;\n  osdm.decode(osdm_bl);\n\n  dout(10) << __func__ << \" loaded osdmap epoch \" << closest_pinned\n           << \" e\" << osdm.epoch\n           << \" crc \" << osdm.get_crc()\n           << \" -- applying incremental maps.\" << dendl;\n\n  uint64_t encode_features = 0;\n  for (version_t v = closest_pinned + 1; v <= ver; ++v) {\n    dout(20) << __func__ << \"    applying inc epoch \" << v << dendl;\n\n    OSDMap::Incremental inc;\n    int err = get_inc(v, inc);\n    ceph_assert(err == 0);\n\n    encode_features = inc.encode_features;\n\n    err = osdm.apply_incremental(inc);\n    ceph_assert(err == 0);\n\n    // this block performs paranoid checks on map retrieval\n    if (g_conf->get_val<bool>(\"mon_debug_extra_checks\") &&\n        inc.full_crc != 0) {\n\n      uint64_t f = encode_features;\n      if (!f) {\n        f = (mon->quorum_con_features ? mon->quorum_con_features : -1);\n      }\n\n      // encode osdmap to force calculating crcs\n      bufferlist tbl;\n      osdm.encode(tbl, f | CEPH_FEATURE_RESERVED);\n      // decode osdmap to compare crcs with what's expected by incremental\n      OSDMap tosdm;\n      tosdm.decode(tbl);\n\n      if (tosdm.get_crc() != inc.full_crc) {\n        derr << __func__\n             << \"    osdmap crc mismatch! (osdmap crc \" << tosdm.get_crc()\n             << \", expected \" << inc.full_crc << \")\" << dendl;\n        ceph_assert(0 == \"osdmap crc mismatch\");\n      }\n    }\n\n    // note: we cannot add the recently computed map to the cache, as is,\n    // because we have not encoded the map into a bl.\n  }\n\n  if (!encode_features) {\n    dout(10) << __func__\n             << \" last incremental map didn't have features;\"\n             << \" defaulting to quorum's or all\" << dendl;\n    encode_features =\n      (mon->quorum_con_features ? mon->quorum_con_features : -1);\n  }\n  osdm.encode(bl, encode_features | CEPH_FEATURE_RESERVED);\n\n  return 0;\n}\n\nint OSDMonitor::get_version_full(version_t ver, bufferlist& bl)\n{\n  return get_version_full(ver, mon->get_quorum_con_features(), bl);\n}\n\nint OSDMonitor::get_version_full(version_t ver, uint64_t features,\n\t\t\t\t bufferlist& bl)\n{\n  uint64_t significant_features = OSDMap::get_significant_features(features);\n  if (full_osd_cache.lookup({ver, significant_features}, &bl)) {\n    return 0;\n  }\n  int ret = PaxosService::get_version_full(ver, bl);\n  if (ret == -ENOENT) {\n    // build map?\n    ret = get_full_from_pinned_map(ver, bl);\n  }\n  if (ret < 0) {\n    return ret;\n  }\n  // NOTE: this check is imprecise; the OSDMap encoding features may\n  // be a subset of the latest mon quorum features, but worst case we\n  // reencode once and then cache the (identical) result under both\n  // feature masks.\n  if (significant_features !=\n      OSDMap::get_significant_features(mon->get_quorum_con_features())) {\n    reencode_full_map(bl, features);\n  }\n  full_osd_cache.add({ver, significant_features}, bl);\n  return 0;\n}\n\nepoch_t OSDMonitor::blacklist(const entity_addrvec_t& av, utime_t until)\n{\n  dout(10) << \"blacklist \" << av << \" until \" << until << dendl;\n  for (auto& a : av.v) {\n    pending_inc.new_blacklist[a] = until;\n  }\n  return pending_inc.epoch;\n}\n\nepoch_t OSDMonitor::blacklist(const entity_addr_t& a, utime_t until)\n{\n  dout(10) << \"blacklist \" << a << \" until \" << until << dendl;\n  pending_inc.new_blacklist[a] = until;\n  return pending_inc.epoch;\n}\n\n\nvoid OSDMonitor::check_osdmap_subs()\n{\n  dout(10) << __func__ << dendl;\n  if (!osdmap.get_epoch()) {\n    return;\n  }\n  auto osdmap_subs = mon->session_map.subs.find(\"osdmap\");\n  if (osdmap_subs == mon->session_map.subs.end()) {\n    return;\n  }\n  auto p = osdmap_subs->second->begin();\n  while (!p.end()) {\n    auto sub = *p;\n    ++p;\n    check_osdmap_sub(sub);\n  }\n}\n\nvoid OSDMonitor::check_osdmap_sub(Subscription *sub)\n{\n  dout(10) << __func__ << \" \" << sub << \" next \" << sub->next\n\t   << (sub->onetime ? \" (onetime)\":\" (ongoing)\") << dendl;\n  if (sub->next <= osdmap.get_epoch()) {\n    if (sub->next >= 1)\n      send_incremental(sub->next, sub->session, sub->incremental_onetime);\n    else\n      sub->session->con->send_message(build_latest_full(sub->session->con_features));\n    if (sub->onetime)\n      mon->session_map.remove_sub(sub);\n    else\n      sub->next = osdmap.get_epoch() + 1;\n  }\n}\n\nvoid OSDMonitor::check_pg_creates_subs()\n{\n  if (!osdmap.get_num_up_osds()) {\n    return;\n  }\n  assert(osdmap.get_up_osd_features() & CEPH_FEATURE_MON_STATEFUL_SUB);\n  mon->with_session_map([this](const MonSessionMap& session_map) {\n      auto pg_creates_subs = session_map.subs.find(\"osd_pg_creates\");\n      if (pg_creates_subs == session_map.subs.end()) {\n\treturn;\n      }\n      for (auto sub : *pg_creates_subs->second) {\n\tcheck_pg_creates_sub(sub);\n      }\n    });\n}\n\nvoid OSDMonitor::check_pg_creates_sub(Subscription *sub)\n{\n  dout(20) << __func__ << \" .. \" << sub->session->name << dendl;\n  assert(sub->type == \"osd_pg_creates\");\n  // only send these if the OSD is up.  we will check_subs() when they do\n  // come up so they will get the creates then.\n  if (sub->session->name.is_osd() &&\n      mon->osdmon()->osdmap.is_up(sub->session->name.num())) {\n    sub->next = send_pg_creates(sub->session->name.num(),\n\t\t\t\tsub->session->con.get(),\n\t\t\t\tsub->next);\n  }\n}\n\nvoid OSDMonitor::do_application_enable(int64_t pool_id,\n                                       const std::string &app_name,\n\t\t\t\t       const std::string &app_key,\n\t\t\t\t       const std::string &app_value)\n{\n  assert(paxos->is_plugged() && is_writeable());\n\n  dout(20) << __func__ << \": pool_id=\" << pool_id << \", app_name=\" << app_name\n           << dendl;\n\n  assert(osdmap.require_osd_release >= CEPH_RELEASE_LUMINOUS);\n\n  auto pp = osdmap.get_pg_pool(pool_id);\n  assert(pp != nullptr);\n\n  pg_pool_t p = *pp;\n  if (pending_inc.new_pools.count(pool_id)) {\n    p = pending_inc.new_pools[pool_id];\n  }\n\n  if (app_key.empty()) {\n    p.application_metadata.insert({app_name, {}});\n  } else {\n    p.application_metadata.insert({app_name, {{app_key, app_value}}});\n  }\n  p.last_change = pending_inc.epoch;\n  pending_inc.new_pools[pool_id] = p;\n}\n\nunsigned OSDMonitor::scan_for_creating_pgs(\n  const mempool::osdmap::map<int64_t,pg_pool_t>& pools,\n  const mempool::osdmap::set<int64_t>& removed_pools,\n  utime_t modified,\n  creating_pgs_t* creating_pgs) const\n{\n  unsigned queued = 0;\n  for (auto& p : pools) {\n    int64_t poolid = p.first;\n    const pg_pool_t& pool = p.second;\n    int ruleno = osdmap.crush->find_rule(pool.get_crush_rule(),\n\t\t\t\t\t pool.get_type(), pool.get_size());\n    if (ruleno < 0 || !osdmap.crush->rule_exists(ruleno))\n      continue;\n\n    const auto last_scan_epoch = creating_pgs->last_scan_epoch;\n    const auto created = pool.get_last_change();\n    if (last_scan_epoch && created <= last_scan_epoch) {\n      dout(10) << __func__ << \" no change in pool \" << poolid\n\t       << \" \" << pool << dendl;\n      continue;\n    }\n    if (removed_pools.count(poolid)) {\n      dout(10) << __func__ << \" pool is being removed: \" << poolid\n\t       << \" \" << pool << dendl;\n      continue;\n    }\n    dout(10) << __func__ << \" queueing pool create for \" << poolid\n\t     << \" \" << pool << dendl;\n    if (creating_pgs->create_pool(poolid, pool.get_pg_num(),\n\t\t\t\t  created, modified)) {\n      queued++;\n    }\n  }\n  return queued;\n}\n\nvoid OSDMonitor::update_creating_pgs()\n{\n  dout(10) << __func__ << \" \" << creating_pgs.pgs.size() << \" pgs creating, \"\n\t   << creating_pgs.queue.size() << \" pools in queue\" << dendl;\n  decltype(creating_pgs_by_osd_epoch) new_pgs_by_osd_epoch;\n  std::lock_guard<std::mutex> l(creating_pgs_lock);\n  for (const auto& pg : creating_pgs.pgs) {\n    int acting_primary = -1;\n    auto pgid = pg.first;\n    if (!osdmap.pg_exists(pgid)) {\n      dout(20) << __func__ << \" ignoring \" << pgid << \" which should not exist\"\n\t       << dendl;\n      continue;\n    }\n    auto mapped = pg.second.first;\n    dout(20) << __func__ << \" looking up \" << pgid << \"@\" << mapped << dendl;\n    spg_t spgid(pgid);\n    mapping.get_primary_and_shard(pgid, &acting_primary, &spgid);\n    // check the previous creating_pgs, look for the target to whom the pg was\n    // previously mapped\n    for (const auto& pgs_by_epoch : creating_pgs_by_osd_epoch) {\n      const auto last_acting_primary = pgs_by_epoch.first;\n      for (auto& pgs: pgs_by_epoch.second) {\n\tif (pgs.second.count(spgid)) {\n\t  if (last_acting_primary == acting_primary) {\n\t    mapped = pgs.first;\n\t  } else {\n\t    dout(20) << __func__ << \" \" << pgid << \" \"\n\t\t     << \" acting_primary:\" << last_acting_primary\n\t\t     << \" -> \" << acting_primary << dendl;\n\t    // note epoch if the target of the create message changed.\n\t    mapped = mapping.get_epoch();\n          }\n          break;\n        } else {\n\t  // newly creating\n\t  mapped = mapping.get_epoch();\n\t}\n      }\n    }\n    dout(10) << __func__ << \" will instruct osd.\" << acting_primary\n\t     << \" to create \" << pgid << \"@\" << mapped << dendl;\n    new_pgs_by_osd_epoch[acting_primary][mapped].insert(spgid);\n  }\n  creating_pgs_by_osd_epoch = std::move(new_pgs_by_osd_epoch);\n  creating_pgs_epoch = mapping.get_epoch();\n}\n\nepoch_t OSDMonitor::send_pg_creates(int osd, Connection *con, epoch_t next) const\n{\n  dout(30) << __func__ << \" osd.\" << osd << \" next=\" << next\n\t   << \" \" << creating_pgs_by_osd_epoch << dendl;\n  std::lock_guard<std::mutex> l(creating_pgs_lock);\n  if (creating_pgs_epoch <= creating_pgs.last_scan_epoch) {\n    dout(20) << __func__\n\t     << \" not using stale creating_pgs@\" << creating_pgs_epoch << dendl;\n    // the subscribers will be updated when the mapping is completed anyway\n    return next;\n  }\n  auto creating_pgs_by_epoch = creating_pgs_by_osd_epoch.find(osd);\n  if (creating_pgs_by_epoch == creating_pgs_by_osd_epoch.end())\n    return next;\n  assert(!creating_pgs_by_epoch->second.empty());\n\n  MOSDPGCreate *oldm = nullptr; // for pre-mimic OSD compat\n  MOSDPGCreate2 *m = nullptr;\n\n  // for now, keep sending legacy creates.  Until we sort out how to address\n  // racing mon create resends and splits, we are better off with the less\n  // drastic impacts of http://tracker.ceph.com/issues/22165.  The legacy\n  // create message handling path in the OSD still does the old thing where\n  // the pg history is pregenerated and it's instantiated at the latest osdmap\n  // epoch; child pgs are simply not created.\n  bool old = true; // !HAVE_FEATURE(con->get_features(), SERVER_NAUTILUS);\n\n  epoch_t last = 0;\n  for (auto epoch_pgs = creating_pgs_by_epoch->second.lower_bound(next);\n       epoch_pgs != creating_pgs_by_epoch->second.end(); ++epoch_pgs) {\n    auto epoch = epoch_pgs->first;\n    auto& pgs = epoch_pgs->second;\n    dout(20) << __func__ << \" osd.\" << osd << \" from \" << next\n             << \" : epoch \" << epoch << \" \" << pgs.size() << \" pgs\" << dendl;\n    last = epoch;\n    for (auto& pg : pgs) {\n      // Need the create time from the monitor using its clock to set\n      // last_scrub_stamp upon pg creation.\n      auto create = creating_pgs.pgs.find(pg.pgid);\n      assert(create != creating_pgs.pgs.end());\n      if (old) {\n\tif (!oldm) {\n\t  oldm = new MOSDPGCreate(creating_pgs_epoch);\n\t}\n\toldm->mkpg.emplace(pg.pgid,\n\t\t\t   pg_create_t{create->second.first, pg.pgid, 0});\n\toldm->ctimes.emplace(pg.pgid, create->second.second);\n      } else {\n\tif (!m) {\n\t  m = new MOSDPGCreate2(creating_pgs_epoch);\n\t}\n\tm->pgs.emplace(pg, create->second);\n      }\n      dout(20) << __func__ << \" will create \" << pg\n\t       << \" at \" << create->second.first << dendl;\n    }\n  }\n  if (m) {\n    con->send_message(m);\n  } else if (oldm) {\n    con->send_message(oldm);\n  } else {\n    dout(20) << __func__ << \" osd.\" << osd << \" from \" << next\n             << \" has nothing to send\" << dendl;\n    return next;\n  }\n\n  // sub is current through last + 1\n  return last + 1;\n}\n\n// TICK\n\n\nvoid OSDMonitor::tick()\n{\n  if (!is_active()) return;\n\n  dout(10) << osdmap << dendl;\n\n  // always update osdmap manifest, regardless of being the leader.\n  load_osdmap_manifest();\n\n  if (!mon->is_leader()) return;\n\n  bool do_propose = false;\n  utime_t now = ceph_clock_now();\n\n  if (handle_osd_timeouts(now, last_osd_report)) {\n    do_propose = true;\n  }\n\n  // mark osds down?\n  if (check_failures(now)) {\n    do_propose = true;\n  }\n\n  // Force a proposal if we need to prune; pruning is performed on\n  // ``encode_pending()``, hence why we need to regularly trigger a proposal\n  // even if there's nothing going on.\n  if (is_prune_enabled() && should_prune()) {\n    do_propose = true;\n  }\n\n  // mark down osds out?\n\n  /* can_mark_out() checks if we can mark osds as being out. The -1 has no\n   * influence at all. The decision is made based on the ratio of \"in\" osds,\n   * and the function returns false if this ratio is lower that the minimum\n   * ratio set by g_conf->mon_osd_min_in_ratio. So it's not really up to us.\n   */\n  if (can_mark_out(-1)) {\n    set<int> down_cache;  // quick cache of down subtrees\n\n    map<int,utime_t>::iterator i = down_pending_out.begin();\n    while (i != down_pending_out.end()) {\n      int o = i->first;\n      utime_t down = now;\n      down -= i->second;\n      ++i;\n\n      if (osdmap.is_down(o) &&\n\t  osdmap.is_in(o) &&\n\t  can_mark_out(o)) {\n\tutime_t orig_grace(g_conf->mon_osd_down_out_interval, 0);\n\tutime_t grace = orig_grace;\n\tdouble my_grace = 0.0;\n\n\tif (g_conf->mon_osd_adjust_down_out_interval) {\n\t  // scale grace period the same way we do the heartbeat grace.\n\t  const osd_xinfo_t& xi = osdmap.get_xinfo(o);\n\t  double halflife = (double)g_conf->mon_osd_laggy_halflife;\n\t  double decay_k = ::log(.5) / halflife;\n\t  double decay = exp((double)down * decay_k);\n\t  dout(20) << \"osd.\" << o << \" laggy halflife \" << halflife << \" decay_k \" << decay_k\n\t\t   << \" down for \" << down << \" decay \" << decay << dendl;\n\t  my_grace = decay * (double)xi.laggy_interval * xi.laggy_probability;\n\t  grace += my_grace;\n\t}\n\n\t// is this an entire large subtree down?\n\tif (g_conf->mon_osd_down_out_subtree_limit.length()) {\n\t  int type = osdmap.crush->get_type_id(g_conf->mon_osd_down_out_subtree_limit);\n\t  if (type > 0) {\n\t    if (osdmap.containing_subtree_is_down(cct, o, type, &down_cache)) {\n\t      dout(10) << \"tick entire containing \" << g_conf->mon_osd_down_out_subtree_limit\n\t\t       << \" subtree for osd.\" << o << \" is down; resetting timer\" << dendl;\n\t      // reset timer, too.\n\t      down_pending_out[o] = now;\n\t      continue;\n\t    }\n\t  }\n\t}\n\n        bool down_out = !osdmap.is_destroyed(o) &&\n          g_conf->mon_osd_down_out_interval > 0 && down.sec() >= grace;\n        bool destroyed_out = osdmap.is_destroyed(o) &&\n          g_conf->mon_osd_destroyed_out_interval > 0 &&\n        // this is not precise enough as we did not make a note when this osd\n        // was marked as destroyed, but let's not bother with that\n        // complexity for now.\n          down.sec() >= g_conf->mon_osd_destroyed_out_interval;\n        if (down_out || destroyed_out) {\n\t  dout(10) << \"tick marking osd.\" << o << \" OUT after \" << down\n\t\t   << \" sec (target \" << grace << \" = \" << orig_grace << \" + \" << my_grace << \")\" << dendl;\n\t  pending_inc.new_weight[o] = CEPH_OSD_OUT;\n\n\t  // set the AUTOOUT bit.\n\t  if (pending_inc.new_state.count(o) == 0)\n\t    pending_inc.new_state[o] = 0;\n\t  pending_inc.new_state[o] |= CEPH_OSD_AUTOOUT;\n\n\t  // remember previous weight\n\t  if (pending_inc.new_xinfo.count(o) == 0)\n\t    pending_inc.new_xinfo[o] = osdmap.osd_xinfo[o];\n\t  pending_inc.new_xinfo[o].old_weight = osdmap.osd_weight[o];\n\n\t  do_propose = true;\n\n\t  mon->clog->info() << \"Marking osd.\" << o << \" out (has been down for \"\n                            << int(down.sec()) << \" seconds)\";\n\t} else\n\t  continue;\n      }\n\n      down_pending_out.erase(o);\n    }\n  } else {\n    dout(10) << \"tick NOOUT flag set, not checking down osds\" << dendl;\n  }\n\n  // expire blacklisted items?\n  for (ceph::unordered_map<entity_addr_t,utime_t>::iterator p = osdmap.blacklist.begin();\n       p != osdmap.blacklist.end();\n       ++p) {\n    if (p->second < now) {\n      dout(10) << \"expiring blacklist item \" << p->first << \" expired \" << p->second << \" < now \" << now << dendl;\n      pending_inc.old_blacklist.push_back(p->first);\n      do_propose = true;\n    }\n  }\n\n  if (try_prune_purged_snaps()) {\n    do_propose = true;\n  }\n\n  if (update_pools_status())\n    do_propose = true;\n\n  if (do_propose ||\n      !pending_inc.new_pg_temp.empty())  // also propose if we adjusted pg_temp\n    propose_pending();\n}\n\nbool OSDMonitor::handle_osd_timeouts(const utime_t &now,\n\t\t\t\t     std::map<int,utime_t> &last_osd_report)\n{\n  utime_t timeo(g_conf->mon_osd_report_timeout, 0);\n  if (now - mon->get_leader_since() < timeo) {\n    // We haven't been the leader for long enough to consider OSD timeouts\n    return false;\n  }\n\n  int max_osd = osdmap.get_max_osd();\n  bool new_down = false;\n\n  for (int i=0; i < max_osd; ++i) {\n    dout(30) << __func__ << \": checking up on osd \" << i << dendl;\n    if (!osdmap.exists(i)) {\n      last_osd_report.erase(i); // if any\n      continue;\n    }\n    if (!osdmap.is_up(i))\n      continue;\n    const std::map<int,utime_t>::const_iterator t = last_osd_report.find(i);\n    if (t == last_osd_report.end()) {\n      // it wasn't in the map; start the timer.\n      last_osd_report[i] = now;\n    } else if (can_mark_down(i)) {\n      utime_t diff = now - t->second;\n      if (diff > timeo) {\n\tmon->clog->info() << \"osd.\" << i << \" marked down after no beacon for \"\n\t\t\t  << diff << \" seconds\";\n\tderr << \"no beacon from osd.\" << i << \" since \" << t->second\n\t     << \", \" << diff << \" seconds ago.  marking down\" << dendl;\n\tpending_inc.new_state[i] = CEPH_OSD_UP;\n\tnew_down = true;\n      }\n    }\n  }\n  return new_down;\n}\n\nvoid OSDMonitor::dump_info(Formatter *f)\n{\n  f->open_object_section(\"osdmap\");\n  osdmap.dump(f);\n  f->close_section();\n\n  f->open_array_section(\"osd_metadata\");\n  for (int i=0; i<osdmap.get_max_osd(); ++i) {\n    if (osdmap.exists(i)) {\n      f->open_object_section(\"osd\");\n      f->dump_unsigned(\"id\", i);\n      dump_osd_metadata(i, f, NULL);\n      f->close_section();\n    }\n  }\n  f->close_section();\n\n  f->dump_unsigned(\"osdmap_first_committed\", get_first_committed());\n  f->dump_unsigned(\"osdmap_last_committed\", get_last_committed());\n\n  f->open_object_section(\"crushmap\");\n  osdmap.crush->dump(f);\n  f->close_section();\n\n  if (has_osdmap_manifest) {\n    f->open_object_section(\"osdmap_manifest\");\n    osdmap_manifest.dump(f);\n    f->close_section();\n  }\n}\n\nnamespace {\n  enum osd_pool_get_choices {\n    SIZE, MIN_SIZE,\n    PG_NUM, PGP_NUM, CRUSH_RULE, HASHPSPOOL, EC_OVERWRITES,\n    NODELETE, NOPGCHANGE, NOSIZECHANGE,\n    WRITE_FADVISE_DONTNEED, NOSCRUB, NODEEP_SCRUB,\n    HIT_SET_TYPE, HIT_SET_PERIOD, HIT_SET_COUNT, HIT_SET_FPP,\n    USE_GMT_HITSET, AUID, TARGET_MAX_OBJECTS, TARGET_MAX_BYTES,\n    CACHE_TARGET_DIRTY_RATIO, CACHE_TARGET_DIRTY_HIGH_RATIO,\n    CACHE_TARGET_FULL_RATIO,\n    CACHE_MIN_FLUSH_AGE, CACHE_MIN_EVICT_AGE,\n    ERASURE_CODE_PROFILE, MIN_READ_RECENCY_FOR_PROMOTE,\n    MIN_WRITE_RECENCY_FOR_PROMOTE, FAST_READ,\n    HIT_SET_GRADE_DECAY_RATE, HIT_SET_SEARCH_LAST_N,\n    SCRUB_MIN_INTERVAL, SCRUB_MAX_INTERVAL, DEEP_SCRUB_INTERVAL,\n    RECOVERY_PRIORITY, RECOVERY_OP_PRIORITY, SCRUB_PRIORITY,\n    COMPRESSION_MODE, COMPRESSION_ALGORITHM, COMPRESSION_REQUIRED_RATIO,\n    COMPRESSION_MAX_BLOB_SIZE, COMPRESSION_MIN_BLOB_SIZE,\n    CSUM_TYPE, CSUM_MAX_BLOCK, CSUM_MIN_BLOCK };\n\n  std::set<osd_pool_get_choices>\n    subtract_second_from_first(const std::set<osd_pool_get_choices>& first,\n\t\t\t\tconst std::set<osd_pool_get_choices>& second)\n    {\n      std::set<osd_pool_get_choices> result;\n      std::set_difference(first.begin(), first.end(),\n\t\t\t  second.begin(), second.end(),\n\t\t\t  std::inserter(result, result.end()));\n      return result;\n    }\n}\n\n\nbool OSDMonitor::preprocess_command(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MMonCommand *m = static_cast<MMonCommand*>(op->get_req());\n  int r = 0;\n  bufferlist rdata;\n  stringstream ss, ds;\n\n  cmdmap_t cmdmap;\n  if (!cmdmap_from_json(m->cmd, &cmdmap, ss)) {\n    string rs = ss.str();\n    mon->reply_command(op, -EINVAL, rs, get_last_committed());\n    return true;\n  }\n\n  MonSession *session = m->get_session();\n  if (!session) {\n    mon->reply_command(op, -EACCES, \"access denied\", get_last_committed());\n    return true;\n  }\n\n  string prefix;\n  cmd_getval(cct, cmdmap, \"prefix\", prefix);\n\n  string format;\n  cmd_getval(cct, cmdmap, \"format\", format, string(\"plain\"));\n  boost::scoped_ptr<Formatter> f(Formatter::create(format));\n\n  if (prefix == \"osd stat\") {\n    osdmap.print_summary(f.get(), ds, \"\", true);\n    if (f)\n      f->flush(rdata);\n    else\n      rdata.append(ds);\n  }\n  else if (prefix == \"osd dump\" ||\n\t   prefix == \"osd tree\" ||\n\t   prefix == \"osd tree-from\" ||\n\t   prefix == \"osd ls\" ||\n\t   prefix == \"osd getmap\" ||\n\t   prefix == \"osd getcrushmap\" ||\n\t   prefix == \"osd ls-tree\") {\n    string val;\n\n    epoch_t epoch = 0;\n    int64_t epochnum;\n    cmd_getval(cct, cmdmap, \"epoch\", epochnum, (int64_t)osdmap.get_epoch());\n    epoch = epochnum;\n    \n    bufferlist osdmap_bl;\n    int err = get_version_full(epoch, osdmap_bl);\n    if (err == -ENOENT) {\n      r = -ENOENT;\n      ss << \"there is no map for epoch \" << epoch;\n      goto reply;\n    }\n    assert(err == 0);\n    assert(osdmap_bl.length());\n\n    OSDMap *p;\n    if (epoch == osdmap.get_epoch()) {\n      p = &osdmap;\n    } else {\n      p = new OSDMap;\n      p->decode(osdmap_bl);\n    }\n\n    auto sg = make_scope_guard([&] {\n      if (p != &osdmap) {\n        delete p;\n      }\n    });\n\n    if (prefix == \"osd dump\") {\n      stringstream ds;\n      if (f) {\n\tf->open_object_section(\"osdmap\");\n\tp->dump(f.get());\n\tf->close_section();\n\tf->flush(ds);\n      } else {\n\tp->print(ds);\n      }\n      rdata.append(ds);\n      if (!f)\n\tds << \" \";\n    } else if (prefix == \"osd ls\") {\n      if (f) {\n\tf->open_array_section(\"osds\");\n\tfor (int i = 0; i < osdmap.get_max_osd(); i++) {\n\t  if (osdmap.exists(i)) {\n\t    f->dump_int(\"osd\", i);\n\t  }\n\t}\n\tf->close_section();\n\tf->flush(ds);\n      } else {\n\tbool first = true;\n\tfor (int i = 0; i < osdmap.get_max_osd(); i++) {\n\t  if (osdmap.exists(i)) {\n\t    if (!first)\n\t      ds << \"\\n\";\n\t    first = false;\n\t    ds << i;\n\t  }\n\t}\n      }\n      rdata.append(ds);\n    } else if (prefix == \"osd tree\" || prefix == \"osd tree-from\") {\n      string bucket;\n      if (prefix == \"osd tree-from\") {\n        cmd_getval(cct, cmdmap, \"bucket\", bucket);\n        if (!osdmap.crush->name_exists(bucket)) {\n          ss << \"bucket '\" << bucket << \"' does not exist\";\n          r = -ENOENT;\n          goto reply;\n        }\n        int id = osdmap.crush->get_item_id(bucket);\n        if (id >= 0) {\n          ss << \"\\\"\" << bucket << \"\\\" is not a bucket\";\n          r = -EINVAL;\n          goto reply;\n        }\n      }\n\n      vector<string> states;\n      cmd_getval(cct, cmdmap, \"states\", states);\n      unsigned filter = 0;\n      for (auto& s : states) {\n\tif (s == \"up\") {\n\t  filter |= OSDMap::DUMP_UP;\n\t} else if (s == \"down\") {\n\t  filter |= OSDMap::DUMP_DOWN;\n\t} else if (s == \"in\") {\n\t  filter |= OSDMap::DUMP_IN;\n\t} else if (s == \"out\") {\n\t  filter |= OSDMap::DUMP_OUT;\n\t} else if (s == \"destroyed\") {\n\t  filter |= OSDMap::DUMP_DESTROYED;\n\t} else {\n\t  ss << \"unrecognized state '\" << s << \"'\";\n\t  r = -EINVAL;\n\t  goto reply;\n\t}\n      }\n      if ((filter & (OSDMap::DUMP_IN|OSDMap::DUMP_OUT)) ==\n\t  (OSDMap::DUMP_IN|OSDMap::DUMP_OUT)) {\n        ss << \"cannot specify both 'in' and 'out'\";\n        r = -EINVAL;\n        goto reply;\n      }\n      if (((filter & (OSDMap::DUMP_UP|OSDMap::DUMP_DOWN)) ==\n\t   (OSDMap::DUMP_UP|OSDMap::DUMP_DOWN)) ||\n           ((filter & (OSDMap::DUMP_UP|OSDMap::DUMP_DESTROYED)) ==\n           (OSDMap::DUMP_UP|OSDMap::DUMP_DESTROYED)) ||\n           ((filter & (OSDMap::DUMP_DOWN|OSDMap::DUMP_DESTROYED)) ==\n           (OSDMap::DUMP_DOWN|OSDMap::DUMP_DESTROYED))) {\n\tss << \"can specify only one of 'up', 'down' and 'destroyed'\";\n\tr = -EINVAL;\n\tgoto reply;\n      }\n      if (f) {\n\tf->open_object_section(\"tree\");\n\tp->print_tree(f.get(), NULL, filter, bucket);\n\tf->close_section();\n\tf->flush(ds);\n      } else {\n\tp->print_tree(NULL, &ds, filter, bucket);\n      }\n      rdata.append(ds);\n    } else if (prefix == \"osd getmap\") {\n      rdata.append(osdmap_bl);\n      ss << \"got osdmap epoch \" << p->get_epoch();\n    } else if (prefix == \"osd getcrushmap\") {\n      p->crush->encode(rdata, mon->get_quorum_con_features());\n      ss << p->get_crush_version();\n    } else if (prefix == \"osd ls-tree\") {\n      string bucket_name;\n      cmd_getval(cct, cmdmap, \"name\", bucket_name);\n      set<int> osds;\n      r = p->get_osds_by_bucket_name(bucket_name, &osds);\n      if (r == -ENOENT) {\n        ss << \"\\\"\" << bucket_name << \"\\\" does not exist\";\n        goto reply;\n      } else if (r < 0) {\n        ss << \"can not parse bucket name:\\\"\" << bucket_name << \"\\\"\";\n        goto reply;\n      }\n\n      if (f) {\n        f->open_array_section(\"osds\");\n        for (auto &i : osds) {\n          if (osdmap.exists(i)) {\n            f->dump_int(\"osd\", i);\n          }\n        }\n        f->close_section();\n        f->flush(ds);\n      } else {\n        bool first = true;\n        for (auto &i : osds) {\n          if (osdmap.exists(i)) {\n            if (!first)\n              ds << \"\\n\";\n            first = false;\n            ds << i;\n          }\n        }\n      }\n\n      rdata.append(ds);\n    }\n  } else if (prefix == \"osd getmaxosd\") {\n    if (f) {\n      f->open_object_section(\"getmaxosd\");\n      f->dump_unsigned(\"epoch\", osdmap.get_epoch());\n      f->dump_int(\"max_osd\", osdmap.get_max_osd());\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ds << \"max_osd = \" << osdmap.get_max_osd() << \" in epoch \" << osdmap.get_epoch();\n      rdata.append(ds);\n    }\n  } else if (prefix == \"osd utilization\") {\n    string out;\n    osdmap.summarize_mapping_stats(NULL, NULL, &out, f.get());\n    if (f)\n      f->flush(rdata);\n    else\n      rdata.append(out);\n    r = 0;\n    goto reply;\n  } else if (prefix  == \"osd find\") {\n    int64_t osd;\n    if (!cmd_getval(cct, cmdmap, \"id\", osd)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap[\"id\"]) << \"'\";\n      r = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.exists(osd)) {\n      ss << \"osd.\" << osd << \" does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    f->open_object_section(\"osd_location\");\n    f->dump_int(\"osd\", osd);\n    f->dump_object(\"addrs\", osdmap.get_addrs(osd));\n    f->open_object_section(\"crush_location\");\n    map<string,string> loc = osdmap.crush->get_full_location(osd);\n    for (map<string,string>::iterator p = loc.begin(); p != loc.end(); ++p)\n      f->dump_string(p->first.c_str(), p->second);\n    f->close_section();\n    f->close_section();\n    f->flush(rdata);\n  } else if (prefix == \"osd metadata\") {\n    int64_t osd = -1;\n    if (cmd_vartype_stringify(cmdmap[\"id\"]).size() &&\n        !cmd_getval(cct, cmdmap, \"id\", osd)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap[\"id\"]) << \"'\";\n      r = -EINVAL;\n      goto reply;\n    }\n    if (osd >= 0 && !osdmap.exists(osd)) {\n      ss << \"osd.\" << osd << \" does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    if (osd >= 0) {\n      f->open_object_section(\"osd_metadata\");\n      f->dump_unsigned(\"id\", osd);\n      r = dump_osd_metadata(osd, f.get(), &ss);\n      if (r < 0)\n        goto reply;\n      f->close_section();\n    } else {\n      r = 0;\n      f->open_array_section(\"osd_metadata\");\n      for (int i=0; i<osdmap.get_max_osd(); ++i) {\n        if (osdmap.exists(i)) {\n          f->open_object_section(\"osd\");\n          f->dump_unsigned(\"id\", i);\n          r = dump_osd_metadata(i, f.get(), NULL);\n          if (r == -EINVAL || r == -ENOENT) {\n            // Drop error, continue to get other daemons' metadata\n            dout(4) << \"No metadata for osd.\" << i << dendl;\n            r = 0;\n          } else if (r < 0) {\n            // Unexpected error\n            goto reply;\n          }\n          f->close_section();\n        }\n      }\n      f->close_section();\n    }\n    f->flush(rdata);\n  } else if (prefix == \"osd versions\") {\n    if (!f)\n      f.reset(Formatter::create(\"json-pretty\"));\n    count_metadata(\"ceph_version\", f.get());\n    f->flush(rdata);\n    r = 0;\n  } else if (prefix == \"osd count-metadata\") {\n    if (!f)\n      f.reset(Formatter::create(\"json-pretty\"));\n    string field;\n    cmd_getval(cct, cmdmap, \"property\", field);\n    count_metadata(field, f.get());\n    f->flush(rdata);\n    r = 0;\n  } else if (prefix == \"osd map\") {\n    string poolstr, objstr, namespacestr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    cmd_getval(cct, cmdmap, \"object\", objstr);\n    cmd_getval(cct, cmdmap, \"nspace\", namespacestr);\n\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"pool \" << poolstr << \" does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    object_locator_t oloc(pool, namespacestr);\n    object_t oid(objstr);\n    pg_t pgid = osdmap.object_locator_to_pg(oid, oloc);\n    pg_t mpgid = osdmap.raw_pg_to_pg(pgid);\n    vector<int> up, acting;\n    int up_p, acting_p;\n    osdmap.pg_to_up_acting_osds(mpgid, &up, &up_p, &acting, &acting_p);\n\n    string fullobjname;\n    if (!namespacestr.empty())\n      fullobjname = namespacestr + string(\"/\") + oid.name;\n    else\n      fullobjname = oid.name;\n    if (f) {\n      f->open_object_section(\"osd_map\");\n      f->dump_unsigned(\"epoch\", osdmap.get_epoch());\n      f->dump_string(\"pool\", poolstr);\n      f->dump_int(\"pool_id\", pool);\n      f->dump_stream(\"objname\") << fullobjname;\n      f->dump_stream(\"raw_pgid\") << pgid;\n      f->dump_stream(\"pgid\") << mpgid;\n      f->open_array_section(\"up\");\n      for (vector<int>::iterator p = up.begin(); p != up.end(); ++p)\n        f->dump_int(\"osd\", *p);\n      f->close_section();\n      f->dump_int(\"up_primary\", up_p);\n      f->open_array_section(\"acting\");\n      for (vector<int>::iterator p = acting.begin(); p != acting.end(); ++p)\n        f->dump_int(\"osd\", *p);\n      f->close_section();\n      f->dump_int(\"acting_primary\", acting_p);\n      f->close_section(); // osd_map\n      f->flush(rdata);\n    } else {\n      ds << \"osdmap e\" << osdmap.get_epoch()\n        << \" pool '\" << poolstr << \"' (\" << pool << \")\"\n        << \" object '\" << fullobjname << \"' ->\"\n        << \" pg \" << pgid << \" (\" << mpgid << \")\"\n        << \" -> up (\" << pg_vector_string(up) << \", p\" << up_p << \") acting (\"\n        << pg_vector_string(acting) << \", p\" << acting_p << \")\";\n      rdata.append(ds);\n    }\n\n  } else if (prefix == \"pg map\") {\n    pg_t pgid;\n    string pgidstr;\n    cmd_getval(cct, cmdmap, \"pgid\", pgidstr);\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      r = -EINVAL;\n      goto reply;\n    }\n    vector<int> up, acting;\n    if (!osdmap.have_pg_pool(pgid.pool())) {\n      ss << \"pg '\" << pgidstr << \"' does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    pg_t mpgid = osdmap.raw_pg_to_pg(pgid);\n    osdmap.pg_to_up_acting_osds(pgid, up, acting);\n    if (f) {\n      f->open_object_section(\"pg_map\");\n      f->dump_unsigned(\"epoch\", osdmap.get_epoch());\n      f->dump_stream(\"raw_pgid\") << pgid;\n      f->dump_stream(\"pgid\") << mpgid;\n      f->open_array_section(\"up\");\n      for (auto osd : up) {\n\tf->dump_int(\"up_osd\", osd);\n      }\n      f->close_section();\n      f->open_array_section(\"acting\");\n      for (auto osd : acting) {\n\tf->dump_int(\"acting_osd\", osd);\n      }\n      f->close_section();\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ds << \"osdmap e\" << osdmap.get_epoch()\n         << \" pg \" << pgid << \" (\" << mpgid << \")\"\n         << \" -> up \" << up << \" acting \" << acting;\n      rdata.append(ds);\n    }\n    goto reply;\n\n  } else if (prefix == \"osd lspools\") {\n    int64_t auid;\n    cmd_getval(cct, cmdmap, \"auid\", auid, int64_t(0));\n    if (f)\n      f->open_array_section(\"pools\");\n    for (map<int64_t, pg_pool_t>::iterator p = osdmap.pools.begin();\n\t p != osdmap.pools.end();\n\t ++p) {\n      if (!auid || p->second.auid == (uint64_t)auid) {\n\tif (f) {\n\t  f->open_object_section(\"pool\");\n\t  f->dump_int(\"poolnum\", p->first);\n\t  f->dump_string(\"poolname\", osdmap.pool_name[p->first]);\n\t  f->close_section();\n\t} else {\n\t  ds << p->first << ' ' << osdmap.pool_name[p->first];\n\t  if (next(p) != osdmap.pools.end()) {\n\t    ds << '\\n';\n\t  }\n\t}\n      }\n    }\n    if (f) {\n      f->close_section();\n      f->flush(ds);\n    }\n    rdata.append(ds);\n  } else if (prefix == \"osd blacklist ls\") {\n    if (f)\n      f->open_array_section(\"blacklist\");\n\n    for (ceph::unordered_map<entity_addr_t,utime_t>::iterator p = osdmap.blacklist.begin();\n\t p != osdmap.blacklist.end();\n\t ++p) {\n      if (f) {\n\tf->open_object_section(\"entry\");\n\tf->dump_stream(\"addr\") << p->first;\n\tf->dump_stream(\"until\") << p->second;\n\tf->close_section();\n      } else {\n\tstringstream ss;\n\tstring s;\n\tss << p->first << \" \" << p->second;\n\tgetline(ss, s);\n\ts += \"\\n\";\n\trdata.append(s);\n      }\n    }\n    if (f) {\n      f->close_section();\n      f->flush(rdata);\n    }\n    ss << \"listed \" << osdmap.blacklist.size() << \" entries\";\n\n  } else if (prefix == \"osd pool ls\") {\n    string detail;\n    cmd_getval(cct, cmdmap, \"detail\", detail);\n    if (!f && detail == \"detail\") {\n      ostringstream ss;\n      osdmap.print_pools(ss);\n      rdata.append(ss.str());\n    } else {\n      if (f)\n\tf->open_array_section(\"pools\");\n      for (map<int64_t,pg_pool_t>::const_iterator it = osdmap.get_pools().begin();\n\t   it != osdmap.get_pools().end();\n\t   ++it) {\n\tif (f) {\n\t  if (detail == \"detail\") {\n\t    f->open_object_section(\"pool\");\n\t    f->dump_string(\"pool_name\", osdmap.get_pool_name(it->first));\n\t    it->second.dump(f.get());\n\t    f->close_section();\n\t  } else {\n\t    f->dump_string(\"pool_name\", osdmap.get_pool_name(it->first));\n\t  }\n\t} else {\n\t  rdata.append(osdmap.get_pool_name(it->first) + \"\\n\");\n\t}\n      }\n      if (f) {\n\tf->close_section();\n\tf->flush(rdata);\n      }\n    }\n\n  } else if (prefix == \"osd crush get-tunable\") {\n    string tunable;\n    cmd_getval(cct, cmdmap, \"tunable\", tunable);\n    ostringstream rss;\n    if (f)\n      f->open_object_section(\"tunable\");\n    if (tunable == \"straw_calc_version\") {\n      if (f)\n\tf->dump_int(tunable.c_str(), osdmap.crush->get_straw_calc_version());\n      else\n\trss << osdmap.crush->get_straw_calc_version() << \"\\n\";\n    } else {\n      r = -EINVAL;\n      goto reply;\n    }\n    if (f) {\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      rdata.append(rss.str());\n    }\n    r = 0;\n\n  } else if (prefix == \"osd pool get\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      r = -ENOENT;\n      goto reply;\n    }\n\n    const pg_pool_t *p = osdmap.get_pg_pool(pool);\n    string var;\n    cmd_getval(cct, cmdmap, \"var\", var);\n\n    typedef std::map<std::string, osd_pool_get_choices> choices_map_t;\n    const choices_map_t ALL_CHOICES = {\n      {\"size\", SIZE},\n      {\"min_size\", MIN_SIZE},\n      {\"pg_num\", PG_NUM}, {\"pgp_num\", PGP_NUM},\n      {\"crush_rule\", CRUSH_RULE}, {\"hashpspool\", HASHPSPOOL},\n      {\"allow_ec_overwrites\", EC_OVERWRITES}, {\"nodelete\", NODELETE},\n      {\"nopgchange\", NOPGCHANGE}, {\"nosizechange\", NOSIZECHANGE},\n      {\"noscrub\", NOSCRUB}, {\"nodeep-scrub\", NODEEP_SCRUB},\n      {\"write_fadvise_dontneed\", WRITE_FADVISE_DONTNEED},\n      {\"hit_set_type\", HIT_SET_TYPE}, {\"hit_set_period\", HIT_SET_PERIOD},\n      {\"hit_set_count\", HIT_SET_COUNT}, {\"hit_set_fpp\", HIT_SET_FPP},\n      {\"use_gmt_hitset\", USE_GMT_HITSET},\n      {\"auid\", AUID}, {\"target_max_objects\", TARGET_MAX_OBJECTS},\n      {\"target_max_bytes\", TARGET_MAX_BYTES},\n      {\"cache_target_dirty_ratio\", CACHE_TARGET_DIRTY_RATIO},\n      {\"cache_target_dirty_high_ratio\", CACHE_TARGET_DIRTY_HIGH_RATIO},\n      {\"cache_target_full_ratio\", CACHE_TARGET_FULL_RATIO},\n      {\"cache_min_flush_age\", CACHE_MIN_FLUSH_AGE},\n      {\"cache_min_evict_age\", CACHE_MIN_EVICT_AGE},\n      {\"erasure_code_profile\", ERASURE_CODE_PROFILE},\n      {\"min_read_recency_for_promote\", MIN_READ_RECENCY_FOR_PROMOTE},\n      {\"min_write_recency_for_promote\", MIN_WRITE_RECENCY_FOR_PROMOTE},\n      {\"fast_read\", FAST_READ},\n      {\"hit_set_grade_decay_rate\", HIT_SET_GRADE_DECAY_RATE},\n      {\"hit_set_search_last_n\", HIT_SET_SEARCH_LAST_N},\n      {\"scrub_min_interval\", SCRUB_MIN_INTERVAL},\n      {\"scrub_max_interval\", SCRUB_MAX_INTERVAL},\n      {\"deep_scrub_interval\", DEEP_SCRUB_INTERVAL},\n      {\"recovery_priority\", RECOVERY_PRIORITY},\n      {\"recovery_op_priority\", RECOVERY_OP_PRIORITY},\n      {\"scrub_priority\", SCRUB_PRIORITY},\n      {\"compression_mode\", COMPRESSION_MODE},\n      {\"compression_algorithm\", COMPRESSION_ALGORITHM},\n      {\"compression_required_ratio\", COMPRESSION_REQUIRED_RATIO},\n      {\"compression_max_blob_size\", COMPRESSION_MAX_BLOB_SIZE},\n      {\"compression_min_blob_size\", COMPRESSION_MIN_BLOB_SIZE},\n      {\"csum_type\", CSUM_TYPE},\n      {\"csum_max_block\", CSUM_MAX_BLOCK},\n      {\"csum_min_block\", CSUM_MIN_BLOCK},\n    };\n\n    typedef std::set<osd_pool_get_choices> choices_set_t;\n\n    const choices_set_t ONLY_TIER_CHOICES = {\n      HIT_SET_TYPE, HIT_SET_PERIOD, HIT_SET_COUNT, HIT_SET_FPP,\n      TARGET_MAX_OBJECTS, TARGET_MAX_BYTES, CACHE_TARGET_FULL_RATIO,\n      CACHE_TARGET_DIRTY_RATIO, CACHE_TARGET_DIRTY_HIGH_RATIO,\n      CACHE_MIN_FLUSH_AGE, CACHE_MIN_EVICT_AGE,\n      MIN_READ_RECENCY_FOR_PROMOTE,\n      MIN_WRITE_RECENCY_FOR_PROMOTE,\n      HIT_SET_GRADE_DECAY_RATE, HIT_SET_SEARCH_LAST_N\n    };\n    const choices_set_t ONLY_ERASURE_CHOICES = {\n      EC_OVERWRITES, ERASURE_CODE_PROFILE\n    };\n\n    choices_set_t selected_choices;\n    if (var == \"all\") {\n      for(choices_map_t::const_iterator it = ALL_CHOICES.begin();\n\t  it != ALL_CHOICES.end(); ++it) {\n\tselected_choices.insert(it->second);\n      }\n\n      if(!p->is_tier()) {\n\tselected_choices = subtract_second_from_first(selected_choices,\n\t\t\t\t\t\t      ONLY_TIER_CHOICES);\n      }\n\n      if(!p->is_erasure()) {\n\tselected_choices = subtract_second_from_first(selected_choices,\n\t\t\t\t\t\t      ONLY_ERASURE_CHOICES);\n      }\n    } else /* var != \"all\" */  {\n      choices_map_t::const_iterator found = ALL_CHOICES.find(var);\n      osd_pool_get_choices selected = found->second;\n\n      if (!p->is_tier() &&\n\t  ONLY_TIER_CHOICES.find(selected) != ONLY_TIER_CHOICES.end()) {\n\tss << \"pool '\" << poolstr\n\t   << \"' is not a tier pool: variable not applicable\";\n\tr = -EACCES;\n\tgoto reply;\n      }\n\n      if (!p->is_erasure() &&\n\t  ONLY_ERASURE_CHOICES.find(selected)\n\t  != ONLY_ERASURE_CHOICES.end()) {\n\tss << \"pool '\" << poolstr\n\t   << \"' is not a erasure pool: variable not applicable\";\n\tr = -EACCES;\n\tgoto reply;\n      }\n\n      if (pool_opts_t::is_opt_name(var) &&\n\t  !p->opts.is_set(pool_opts_t::get_opt_desc(var).key)) {\n\tss << \"option '\" << var << \"' is not set on pool '\" << poolstr << \"'\";\n\tr = -ENOENT;\n\tgoto reply;\n      }\n\n      selected_choices.insert(selected);\n    }\n\n    if (f) {\n      f->open_object_section(\"pool\");\n      f->dump_string(\"pool\", poolstr);\n      f->dump_int(\"pool_id\", pool);\n      for(choices_set_t::const_iterator it = selected_choices.begin();\n\t  it != selected_choices.end(); ++it) {\n\tchoices_map_t::const_iterator i;\n        for (i = ALL_CHOICES.begin(); i != ALL_CHOICES.end(); ++i) {\n          if (i->second == *it) {\n            break;\n          }\n        }\n        assert(i != ALL_CHOICES.end());\n\tswitch(*it) {\n\t  case PG_NUM:\n\t    f->dump_int(\"pg_num\", p->get_pg_num());\n\t    break;\n\t  case PGP_NUM:\n\t    f->dump_int(\"pgp_num\", p->get_pgp_num());\n\t    break;\n\t  case AUID:\n\t    f->dump_int(\"auid\", p->get_auid());\n\t    break;\n\t  case SIZE:\n\t    f->dump_int(\"size\", p->get_size());\n\t    break;\n\t  case MIN_SIZE:\n\t    f->dump_int(\"min_size\", p->get_min_size());\n\t    break;\n\t  case CRUSH_RULE:\n\t    if (osdmap.crush->rule_exists(p->get_crush_rule())) {\n\t      f->dump_string(\"crush_rule\", osdmap.crush->get_rule_name(\n\t\t\t       p->get_crush_rule()));\n\t    } else {\n\t      f->dump_string(\"crush_rule\", stringify(p->get_crush_rule()));\n\t    }\n\t    break;\n\t  case EC_OVERWRITES:\n\t    f->dump_bool(\"allow_ec_overwrites\",\n                         p->has_flag(pg_pool_t::FLAG_EC_OVERWRITES));\n\t    break;\n\t  case HASHPSPOOL:\n\t  case NODELETE:\n\t  case NOPGCHANGE:\n\t  case NOSIZECHANGE:\n\t  case WRITE_FADVISE_DONTNEED:\n\t  case NOSCRUB:\n\t  case NODEEP_SCRUB:\n\t    f->dump_bool(i->first.c_str(),\n\t\t\t   p->has_flag(pg_pool_t::get_flag_by_name(i->first)));\n\t    break;\n\t  case HIT_SET_PERIOD:\n\t    f->dump_int(\"hit_set_period\", p->hit_set_period);\n\t    break;\n\t  case HIT_SET_COUNT:\n\t    f->dump_int(\"hit_set_count\", p->hit_set_count);\n\t    break;\n\t  case HIT_SET_TYPE:\n\t    f->dump_string(\"hit_set_type\",\n\t\t\t   HitSet::get_type_name(p->hit_set_params.get_type()));\n\t    break;\n\t  case HIT_SET_FPP:\n\t    {\n\t      if (p->hit_set_params.get_type() == HitSet::TYPE_BLOOM) {\n\t\tBloomHitSet::Params *bloomp =\n\t\t  static_cast<BloomHitSet::Params*>(p->hit_set_params.impl.get());\n\t\tf->dump_float(\"hit_set_fpp\", bloomp->get_fpp());\n\t      } else if(var != \"all\") {\n\t\tf->close_section();\n\t\tss << \"hit set is not of type Bloom; \" <<\n\t\t  \"invalid to get a false positive rate!\";\n\t\tr = -EINVAL;\n\t\tgoto reply;\n\t      }\n\t    }\n\t    break;\n\t  case USE_GMT_HITSET:\n\t    f->dump_bool(\"use_gmt_hitset\", p->use_gmt_hitset);\n\t    break;\n\t  case TARGET_MAX_OBJECTS:\n\t    f->dump_unsigned(\"target_max_objects\", p->target_max_objects);\n\t    break;\n\t  case TARGET_MAX_BYTES:\n\t    f->dump_unsigned(\"target_max_bytes\", p->target_max_bytes);\n\t    break;\n\t  case CACHE_TARGET_DIRTY_RATIO:\n\t    f->dump_unsigned(\"cache_target_dirty_ratio_micro\",\n\t\t\t     p->cache_target_dirty_ratio_micro);\n\t    f->dump_float(\"cache_target_dirty_ratio\",\n\t\t\t  ((float)p->cache_target_dirty_ratio_micro/1000000));\n\t    break;\n\t  case CACHE_TARGET_DIRTY_HIGH_RATIO:\n\t    f->dump_unsigned(\"cache_target_dirty_high_ratio_micro\",\n\t\t\t     p->cache_target_dirty_high_ratio_micro);\n\t    f->dump_float(\"cache_target_dirty_high_ratio\",\n\t\t\t  ((float)p->cache_target_dirty_high_ratio_micro/1000000));\n\t    break;\n\t  case CACHE_TARGET_FULL_RATIO:\n\t    f->dump_unsigned(\"cache_target_full_ratio_micro\",\n\t\t\t     p->cache_target_full_ratio_micro);\n\t    f->dump_float(\"cache_target_full_ratio\",\n\t\t\t  ((float)p->cache_target_full_ratio_micro/1000000));\n\t    break;\n\t  case CACHE_MIN_FLUSH_AGE:\n\t    f->dump_unsigned(\"cache_min_flush_age\", p->cache_min_flush_age);\n\t    break;\n\t  case CACHE_MIN_EVICT_AGE:\n\t    f->dump_unsigned(\"cache_min_evict_age\", p->cache_min_evict_age);\n\t    break;\n\t  case ERASURE_CODE_PROFILE:\n\t    f->dump_string(\"erasure_code_profile\", p->erasure_code_profile);\n\t    break;\n\t  case MIN_READ_RECENCY_FOR_PROMOTE:\n\t    f->dump_int(\"min_read_recency_for_promote\",\n\t\t\tp->min_read_recency_for_promote);\n\t    break;\n\t  case MIN_WRITE_RECENCY_FOR_PROMOTE:\n\t    f->dump_int(\"min_write_recency_for_promote\",\n\t\t\tp->min_write_recency_for_promote);\n\t    break;\n          case FAST_READ:\n            f->dump_int(\"fast_read\", p->fast_read);\n            break;\n\t  case HIT_SET_GRADE_DECAY_RATE:\n\t    f->dump_int(\"hit_set_grade_decay_rate\",\n\t\t\tp->hit_set_grade_decay_rate);\n\t    break;\n\t  case HIT_SET_SEARCH_LAST_N:\n\t    f->dump_int(\"hit_set_search_last_n\",\n\t\t\tp->hit_set_search_last_n);\n\t    break;\n\t  case SCRUB_MIN_INTERVAL:\n\t  case SCRUB_MAX_INTERVAL:\n\t  case DEEP_SCRUB_INTERVAL:\n          case RECOVERY_PRIORITY:\n          case RECOVERY_OP_PRIORITY:\n          case SCRUB_PRIORITY:\n\t  case COMPRESSION_MODE:\n\t  case COMPRESSION_ALGORITHM:\n\t  case COMPRESSION_REQUIRED_RATIO:\n\t  case COMPRESSION_MAX_BLOB_SIZE:\n\t  case COMPRESSION_MIN_BLOB_SIZE:\n\t  case CSUM_TYPE:\n\t  case CSUM_MAX_BLOCK:\n\t  case CSUM_MIN_BLOCK:\n            pool_opts_t::key_t key = pool_opts_t::get_opt_desc(i->first).key;\n            if (p->opts.is_set(key)) {\n              if(*it == CSUM_TYPE) {\n                int val;\n                p->opts.get(pool_opts_t::CSUM_TYPE, &val);\n                f->dump_string(i->first.c_str(), Checksummer::get_csum_type_string(val));\n              } else {\n                p->opts.dump(i->first, f.get());\n              }\n\t    }\n            break;\n\t}\n      }\n      f->close_section();\n      f->flush(rdata);\n    } else /* !f */ {\n      for(choices_set_t::const_iterator it = selected_choices.begin();\n\t  it != selected_choices.end(); ++it) {\n\tchoices_map_t::const_iterator i;\n\tswitch(*it) {\n\t  case PG_NUM:\n\t    ss << \"pg_num: \" << p->get_pg_num() << \"\\n\";\n\t    break;\n\t  case PGP_NUM:\n\t    ss << \"pgp_num: \" << p->get_pgp_num() << \"\\n\";\n\t    break;\n\t  case AUID:\n\t    ss << \"auid: \" << p->get_auid() << \"\\n\";\n\t    break;\n\t  case SIZE:\n\t    ss << \"size: \" << p->get_size() << \"\\n\";\n\t    break;\n\t  case MIN_SIZE:\n\t    ss << \"min_size: \" << p->get_min_size() << \"\\n\";\n\t    break;\n\t  case CRUSH_RULE:\n\t    if (osdmap.crush->rule_exists(p->get_crush_rule())) {\n\t      ss << \"crush_rule: \" << osdmap.crush->get_rule_name(\n\t\tp->get_crush_rule()) << \"\\n\";\n\t    } else {\n\t      ss << \"crush_rule: \" << p->get_crush_rule() << \"\\n\";\n\t    }\n\t    break;\n\t  case HIT_SET_PERIOD:\n\t    ss << \"hit_set_period: \" << p->hit_set_period << \"\\n\";\n\t    break;\n\t  case HIT_SET_COUNT:\n\t    ss << \"hit_set_count: \" << p->hit_set_count << \"\\n\";\n\t    break;\n\t  case HIT_SET_TYPE:\n\t    ss << \"hit_set_type: \" <<\n\t      HitSet::get_type_name(p->hit_set_params.get_type()) << \"\\n\";\n\t    break;\n\t  case HIT_SET_FPP:\n\t    {\n\t      if (p->hit_set_params.get_type() == HitSet::TYPE_BLOOM) {\n\t\tBloomHitSet::Params *bloomp =\n\t\t  static_cast<BloomHitSet::Params*>(p->hit_set_params.impl.get());\n\t\tss << \"hit_set_fpp: \" << bloomp->get_fpp() << \"\\n\";\n\t      } else if(var != \"all\") {\n\t\tss << \"hit set is not of type Bloom; \" <<\n\t\t  \"invalid to get a false positive rate!\";\n\t\tr = -EINVAL;\n\t\tgoto reply;\n\t      }\n\t    }\n\t    break;\n\t  case USE_GMT_HITSET:\n\t    ss << \"use_gmt_hitset: \" << p->use_gmt_hitset << \"\\n\";\n\t    break;\n\t  case TARGET_MAX_OBJECTS:\n\t    ss << \"target_max_objects: \" << p->target_max_objects << \"\\n\";\n\t    break;\n\t  case TARGET_MAX_BYTES:\n\t    ss << \"target_max_bytes: \" << p->target_max_bytes << \"\\n\";\n\t    break;\n\t  case CACHE_TARGET_DIRTY_RATIO:\n\t    ss << \"cache_target_dirty_ratio: \"\n\t       << ((float)p->cache_target_dirty_ratio_micro/1000000) << \"\\n\";\n\t    break;\n\t  case CACHE_TARGET_DIRTY_HIGH_RATIO:\n\t    ss << \"cache_target_dirty_high_ratio: \"\n\t       << ((float)p->cache_target_dirty_high_ratio_micro/1000000) << \"\\n\";\n\t    break;\n\t  case CACHE_TARGET_FULL_RATIO:\n\t    ss << \"cache_target_full_ratio: \"\n\t       << ((float)p->cache_target_full_ratio_micro/1000000) << \"\\n\";\n\t    break;\n\t  case CACHE_MIN_FLUSH_AGE:\n\t    ss << \"cache_min_flush_age: \" << p->cache_min_flush_age << \"\\n\";\n\t    break;\n\t  case CACHE_MIN_EVICT_AGE:\n\t    ss << \"cache_min_evict_age: \" << p->cache_min_evict_age << \"\\n\";\n\t    break;\n\t  case ERASURE_CODE_PROFILE:\n\t    ss << \"erasure_code_profile: \" << p->erasure_code_profile << \"\\n\";\n\t    break;\n\t  case MIN_READ_RECENCY_FOR_PROMOTE:\n\t    ss << \"min_read_recency_for_promote: \" <<\n\t      p->min_read_recency_for_promote << \"\\n\";\n\t    break;\n\t  case HIT_SET_GRADE_DECAY_RATE:\n\t    ss << \"hit_set_grade_decay_rate: \" <<\n\t      p->hit_set_grade_decay_rate << \"\\n\";\n\t    break;\n\t  case HIT_SET_SEARCH_LAST_N:\n\t    ss << \"hit_set_search_last_n: \" <<\n\t      p->hit_set_search_last_n << \"\\n\";\n\t    break;\n\t  case EC_OVERWRITES:\n\t    ss << \"allow_ec_overwrites: \" <<\n\t      (p->has_flag(pg_pool_t::FLAG_EC_OVERWRITES) ? \"true\" : \"false\") <<\n\t      \"\\n\";\n\t    break;\n\t  case HASHPSPOOL:\n\t  case NODELETE:\n\t  case NOPGCHANGE:\n\t  case NOSIZECHANGE:\n\t  case WRITE_FADVISE_DONTNEED:\n\t  case NOSCRUB:\n\t  case NODEEP_SCRUB:\n\t    for (i = ALL_CHOICES.begin(); i != ALL_CHOICES.end(); ++i) {\n\t      if (i->second == *it)\n\t\tbreak;\n\t    }\n\t    assert(i != ALL_CHOICES.end());\n\t    ss << i->first << \": \" <<\n\t      (p->has_flag(pg_pool_t::get_flag_by_name(i->first)) ?\n\t       \"true\" : \"false\") << \"\\n\";\n\t    break;\n\t  case MIN_WRITE_RECENCY_FOR_PROMOTE:\n\t    ss << \"min_write_recency_for_promote: \" <<\n\t      p->min_write_recency_for_promote << \"\\n\";\n\t    break;\n          case FAST_READ:\n            ss << \"fast_read: \" << p->fast_read << \"\\n\";\n            break;\n\t  case SCRUB_MIN_INTERVAL:\n\t  case SCRUB_MAX_INTERVAL:\n\t  case DEEP_SCRUB_INTERVAL:\n          case RECOVERY_PRIORITY:\n          case RECOVERY_OP_PRIORITY:\n          case SCRUB_PRIORITY:\n\t  case COMPRESSION_MODE:\n\t  case COMPRESSION_ALGORITHM:\n\t  case COMPRESSION_REQUIRED_RATIO:\n\t  case COMPRESSION_MAX_BLOB_SIZE:\n\t  case COMPRESSION_MIN_BLOB_SIZE:\n\t  case CSUM_TYPE:\n\t  case CSUM_MAX_BLOCK:\n\t  case CSUM_MIN_BLOCK:\n\t    for (i = ALL_CHOICES.begin(); i != ALL_CHOICES.end(); ++i) {\n\t      if (i->second == *it)\n\t\tbreak;\n\t    }\n\t    assert(i != ALL_CHOICES.end());\n\t    {\n\t      pool_opts_t::key_t key = pool_opts_t::get_opt_desc(i->first).key;\n\t      if (p->opts.is_set(key)) {\n                if(key == pool_opts_t::CSUM_TYPE) {\n                  int val;\n                  p->opts.get(key, &val);\n  \t\t  ss << i->first << \": \" << Checksummer::get_csum_type_string(val) << \"\\n\";\n                } else {\n  \t\t  ss << i->first << \": \" << p->opts.get(key) << \"\\n\";\n                }\n\t      }\n\t    }\n\t    break;\n\t}\n\trdata.append(ss.str());\n\tss.str(\"\");\n      }\n    }\n    r = 0;\n  } else if (prefix == \"osd pool get-quota\") {\n    string pool_name;\n    cmd_getval(cct, cmdmap, \"pool\", pool_name);\n\n    int64_t poolid = osdmap.lookup_pg_pool_name(pool_name);\n    if (poolid < 0) {\n      assert(poolid == -ENOENT);\n      ss << \"unrecognized pool '\" << pool_name << \"'\";\n      r = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(poolid);\n\n    if (f) {\n      f->open_object_section(\"pool_quotas\");\n      f->dump_string(\"pool_name\", pool_name);\n      f->dump_unsigned(\"pool_id\", poolid);\n      f->dump_unsigned(\"quota_max_objects\", p->quota_max_objects);\n      f->dump_unsigned(\"quota_max_bytes\", p->quota_max_bytes);\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      stringstream rs;\n      rs << \"quotas for pool '\" << pool_name << \"':\\n\"\n         << \"  max objects: \";\n      if (p->quota_max_objects == 0)\n        rs << \"N/A\";\n      else\n        rs << si_u_t(p->quota_max_objects) << \" objects\";\n      rs << \"\\n\"\n         << \"  max bytes  : \";\n      if (p->quota_max_bytes == 0)\n        rs << \"N/A\";\n      else\n        rs << byte_u_t(p->quota_max_bytes);\n      rdata.append(rs.str());\n    }\n    rdata.append(\"\\n\");\n    r = 0;\n  } else if (prefix == \"osd crush rule list\" ||\n\t     prefix == \"osd crush rule ls\") {\n    if (f) {\n      f->open_array_section(\"rules\");\n      osdmap.crush->list_rules(f.get());\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ostringstream ss;\n      osdmap.crush->list_rules(&ss);\n      rdata.append(ss.str());\n    }\n  } else if (prefix == \"osd crush rule ls-by-class\") {\n    string class_name;\n    cmd_getval(cct, cmdmap, \"class\", class_name);\n    if (class_name.empty()) {\n      ss << \"no class specified\";\n      r = -EINVAL;\n      goto reply;\n    }\n    set<int> rules;\n    r = osdmap.crush->get_rules_by_class(class_name, &rules);\n    if (r < 0) {\n      ss << \"failed to get rules by class '\" << class_name << \"'\";\n      goto reply;\n    }\n    if (f) {\n      f->open_array_section(\"rules\");\n      for (auto &rule: rules) {\n        f->dump_string(\"name\", osdmap.crush->get_rule_name(rule));\n      }\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ostringstream rs;\n      for (auto &rule: rules) {\n        rs << osdmap.crush->get_rule_name(rule) << \"\\n\";\n      }\n      rdata.append(rs.str());\n    }\n  } else if (prefix == \"osd crush rule dump\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    if (name == \"\") {\n      f->open_array_section(\"rules\");\n      osdmap.crush->dump_rules(f.get());\n      f->close_section();\n    } else {\n      int ruleno = osdmap.crush->get_rule_id(name);\n      if (ruleno < 0) {\n\tss << \"unknown crush rule '\" << name << \"'\";\n\tr = ruleno;\n\tgoto reply;\n      }\n      osdmap.crush->dump_rule(ruleno, f.get());\n    }\n    ostringstream rs;\n    f->flush(rs);\n    rs << \"\\n\";\n    rdata.append(rs.str());\n  } else if (prefix == \"osd crush dump\") {\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    f->open_object_section(\"crush_map\");\n    osdmap.crush->dump(f.get());\n    f->close_section();\n    ostringstream rs;\n    f->flush(rs);\n    rs << \"\\n\";\n    rdata.append(rs.str());\n  } else if (prefix == \"osd crush show-tunables\") {\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    f->open_object_section(\"crush_map_tunables\");\n    osdmap.crush->dump_tunables(f.get());\n    f->close_section();\n    ostringstream rs;\n    f->flush(rs);\n    rs << \"\\n\";\n    rdata.append(rs.str());\n  } else if (prefix == \"osd crush tree\") {\n    string shadow;\n    cmd_getval(cct, cmdmap, \"shadow\", shadow);\n    bool show_shadow = shadow == \"--show-shadow\";\n    boost::scoped_ptr<Formatter> f(Formatter::create(format));\n    if (f) {\n      osdmap.crush->dump_tree(nullptr,\n                              f.get(),\n                              osdmap.get_pool_names(),\n                              show_shadow);\n      f->flush(rdata);\n    } else {\n      ostringstream ss;\n      osdmap.crush->dump_tree(&ss,\n                              nullptr,\n                              osdmap.get_pool_names(),\n                              show_shadow);\n      rdata.append(ss.str());\n    }\n  } else if (prefix == \"osd crush ls\") {\n    string name;\n    if (!cmd_getval(cct, cmdmap, \"node\", name)) {\n      ss << \"no node specified\";\n      r = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.crush->name_exists(name)) {\n      ss << \"node '\" << name << \"' does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    int id = osdmap.crush->get_item_id(name);\n    list<int> result;\n    if (id >= 0) {\n      result.push_back(id);\n    } else {\n      int num = osdmap.crush->get_bucket_size(id);\n      for (int i = 0; i < num; ++i) {\n\tresult.push_back(osdmap.crush->get_bucket_item(id, i));\n      }\n    }\n    if (f) {\n      f->open_array_section(\"items\");\n      for (auto i : result) {\n\tf->dump_string(\"item\", osdmap.crush->get_item_name(i));\n      }\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ostringstream ss;\n      for (auto i : result) {\n\tss << osdmap.crush->get_item_name(i) << \"\\n\";\n      }\n      rdata.append(ss.str());\n    }\n    r = 0;\n  } else if (prefix == \"osd crush class ls\") {\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    f->open_array_section(\"crush_classes\");\n    for (auto i : osdmap.crush->class_name)\n      f->dump_string(\"class\", i.second);\n    f->close_section();\n    f->flush(rdata);\n  } else if (prefix == \"osd crush class ls-osd\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"class\", name);\n    set<int> osds;\n    osdmap.crush->get_devices_by_class(name, &osds);\n    if (f) {\n      f->open_array_section(\"osds\");\n      for (auto &osd: osds)\n        f->dump_int(\"osd\", osd);\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      bool first = true;\n      for (auto &osd : osds) {\n        if (!first)\n          ds << \"\\n\";\n        first = false;\n        ds << osd;\n      }\n      rdata.append(ds);\n    }\n  } else if (prefix == \"osd erasure-code-profile ls\") {\n    const auto &profiles = osdmap.get_erasure_code_profiles();\n    if (f)\n      f->open_array_section(\"erasure-code-profiles\");\n    for (auto i = profiles.begin(); i != profiles.end(); ++i) {\n      if (f)\n        f->dump_string(\"profile\", i->first.c_str());\n      else\n\trdata.append(i->first + \"\\n\");\n    }\n    if (f) {\n      f->close_section();\n      ostringstream rs;\n      f->flush(rs);\n      rs << \"\\n\";\n      rdata.append(rs.str());\n    }\n  } else if (prefix == \"osd crush weight-set ls\") {\n    boost::scoped_ptr<Formatter> f(Formatter::create(format));\n    if (f) {\n      f->open_array_section(\"weight_sets\");\n      if (osdmap.crush->have_choose_args(CrushWrapper::DEFAULT_CHOOSE_ARGS)) {\n\tf->dump_string(\"pool\", \"(compat)\");\n      }\n      for (auto& i : osdmap.crush->choose_args) {\n\tif (i.first >= 0) {\n\t  f->dump_string(\"pool\", osdmap.get_pool_name(i.first));\n\t}\n      }\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ostringstream rs;\n      if (osdmap.crush->have_choose_args(CrushWrapper::DEFAULT_CHOOSE_ARGS)) {\n\trs << \"(compat)\\n\";\n      }\n      for (auto& i : osdmap.crush->choose_args) {\n\tif (i.first >= 0) {\n\t  rs << osdmap.get_pool_name(i.first) << \"\\n\";\n\t}\n      }\n      rdata.append(rs.str());\n    }\n  } else if (prefix == \"osd crush weight-set dump\") {\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\",\n\t\t\t\t\t\t     \"json-pretty\"));\n    osdmap.crush->dump_choose_args(f.get());\n    f->flush(rdata);\n  } else if (prefix == \"osd erasure-code-profile get\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    if (!osdmap.has_erasure_code_profile(name)) {\n      ss << \"unknown erasure code profile '\" << name << \"'\";\n      r = -ENOENT;\n      goto reply;\n    }\n    const map<string,string> &profile = osdmap.get_erasure_code_profile(name);\n    if (f)\n      f->open_object_section(\"profile\");\n    for (map<string,string>::const_iterator i = profile.begin();\n\t i != profile.end();\n\t ++i) {\n      if (f)\n        f->dump_string(i->first.c_str(), i->second.c_str());\n      else\n\trdata.append(i->first + \"=\" + i->second + \"\\n\");\n    }\n    if (f) {\n      f->close_section();\n      ostringstream rs;\n      f->flush(rs);\n      rs << \"\\n\";\n      rdata.append(rs.str());\n    }\n  } else if (prefix == \"osd pool application get\") {\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\",\n                                                     \"json-pretty\"));\n    string pool_name;\n    cmd_getval(cct, cmdmap, \"pool\", pool_name);\n    string app;\n    cmd_getval(cct, cmdmap, \"app\", app);\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n\n    if (pool_name.empty()) {\n      // all\n      f->open_object_section(\"pools\");\n      for (const auto &pool : osdmap.pools) {\n        std::string name(\"<unknown>\");\n        const auto &pni = osdmap.pool_name.find(pool.first);\n        if (pni != osdmap.pool_name.end())\n          name = pni->second;\n        f->open_object_section(name.c_str());\n        for (auto &app_pair : pool.second.application_metadata) {\n          f->open_object_section(app_pair.first.c_str());\n          for (auto &kv_pair : app_pair.second) {\n            f->dump_string(kv_pair.first.c_str(), kv_pair.second);\n          }\n          f->close_section();\n        }\n        f->close_section(); // name\n      }\n      f->close_section(); // pools\n      f->flush(rdata);\n    } else {\n      int64_t pool = osdmap.lookup_pg_pool_name(pool_name.c_str());\n      if (pool < 0) {\n        ss << \"unrecognized pool '\" << pool_name << \"'\";\n        r = -ENOENT;\n        goto reply;\n      }\n      auto p = osdmap.get_pg_pool(pool);\n      // filter by pool\n      if (app.empty()) {\n        f->open_object_section(pool_name.c_str());\n        for (auto &app_pair : p->application_metadata) {\n          f->open_object_section(app_pair.first.c_str());\n          for (auto &kv_pair : app_pair.second) {\n            f->dump_string(kv_pair.first.c_str(), kv_pair.second);\n          }\n          f->close_section(); // application\n        }\n        f->close_section(); // pool_name\n        f->flush(rdata);\n        goto reply;\n      }\n\n      auto app_it = p->application_metadata.find(app);\n      if (app_it == p->application_metadata.end()) {\n        ss << \"pool '\" << pool_name << \"' has no application '\" << app << \"'\";\n        r = -ENOENT;\n        goto reply;\n      }\n      // filter by pool + app\n      if (key.empty()) {\n        f->open_object_section(app_it->first.c_str());\n        for (auto &kv_pair : app_it->second) {\n          f->dump_string(kv_pair.first.c_str(), kv_pair.second);\n        }\n        f->close_section(); // application\n        f->flush(rdata);\n        goto reply;\n      }\n      // filter by pool + app + key\n      auto key_it = app_it->second.find(key);\n      if (key_it == app_it->second.end()) {\n        ss << \"application '\" << app << \"' on pool '\" << pool_name\n           << \"' does not have key '\" << key << \"'\";\n        r = -ENOENT;\n        goto reply;\n      }\n      ss << key_it->second << \"\\n\";\n      rdata.append(ss.str());\n      ss.str(\"\");\n    }\n  } else if (prefix == \"osd get-require-min-compat-client\") {\n    ss << ceph_release_name(osdmap.require_min_compat_client) << std::endl;\n    rdata.append(ss.str());\n    ss.str(\"\");\n    goto reply;\n  } else {\n    // try prepare update\n    return false;\n  }\n\n reply:\n  string rs;\n  getline(ss, rs);\n  mon->reply_command(op, r, rs, rdata, get_last_committed());\n  return true;\n}\n\nvoid OSDMonitor::set_pool_flags(int64_t pool_id, uint64_t flags)\n{\n  pg_pool_t *pool = pending_inc.get_new_pool(pool_id,\n    osdmap.get_pg_pool(pool_id));\n  assert(pool);\n  pool->set_flag(flags);\n}\n\nvoid OSDMonitor::clear_pool_flags(int64_t pool_id, uint64_t flags)\n{\n  pg_pool_t *pool = pending_inc.get_new_pool(pool_id,\n    osdmap.get_pg_pool(pool_id));\n  assert(pool);\n  pool->unset_flag(flags);\n}\n\nstring OSDMonitor::make_snap_epoch_key(int64_t pool, epoch_t epoch)\n{\n  char k[80];\n  snprintf(k, sizeof(k), \"removed_epoch_%llu_%08lx\",\n\t   (unsigned long long)pool, (unsigned long)epoch);\n  return k;\n}\n\nstring OSDMonitor::make_snap_key(int64_t pool, snapid_t snap)\n{\n  char k[80];\n  snprintf(k, sizeof(k), \"removed_snap_%llu_%016llx\",\n\t   (unsigned long long)pool, (unsigned long long)snap);\n  return k;\n}\n\n\nstring OSDMonitor::make_snap_key_value(\n  int64_t pool, snapid_t snap, snapid_t num,\n  epoch_t epoch, bufferlist *v)\n{\n  // encode the *last* epoch in the key so that we can use forward\n  // iteration only to search for an epoch in an interval.\n  encode(snap, *v);\n  encode(snap + num, *v);\n  encode(epoch, *v);\n  return make_snap_key(pool, snap + num - 1);\n}\n\nstring OSDMonitor::make_snap_purged_key(int64_t pool, snapid_t snap)\n{\n  char k[80];\n  snprintf(k, sizeof(k), \"purged_snap_%llu_%016llx\",\n\t   (unsigned long long)pool, (unsigned long long)snap);\n  return k;\n}\nstring OSDMonitor::make_snap_purged_key_value(\n  int64_t pool, snapid_t snap, snapid_t num,\n  epoch_t epoch, bufferlist *v)\n{\n  // encode the *last* epoch in the key so that we can use forward\n  // iteration only to search for an epoch in an interval.\n  encode(snap, *v);\n  encode(snap + num, *v);\n  encode(epoch, *v);\n  return make_snap_purged_key(pool, snap + num - 1);\n}\n\nint OSDMonitor::lookup_pruned_snap(int64_t pool, snapid_t snap,\n\t\t\t\t   snapid_t *begin, snapid_t *end)\n{\n  string k = make_snap_key(pool, snap);\n  auto it = mon->store->get_iterator(OSD_SNAP_PREFIX);\n  it->lower_bound(k);\n  if (!it->valid()) {\n    return -ENOENT;\n  }\n  if (it->key().find(OSD_SNAP_PREFIX) != 0) {\n    return -ENOENT;\n  }\n  bufferlist v = it->value();\n  auto p = v.cbegin();\n  decode(*begin, p);\n  decode(*end, p);\n  if (snap < *begin || snap >= *end) {\n    return -ENOENT;\n  }\n  return 0;\n}\n\nbool OSDMonitor::try_prune_purged_snaps()\n{\n  if (!mon->mgrstatmon()->is_readable()) {\n    return false;\n  }\n  if (osdmap.require_osd_release < CEPH_RELEASE_MIMIC) {\n    return false;\n  }\n  if (!pending_inc.new_purged_snaps.empty()) {\n    return false;  // we already pruned for this epoch\n  }\n\n  unsigned max_prune = cct->_conf->get_val<uint64_t>(\n    \"mon_max_snap_prune_per_epoch\");\n  if (!max_prune) {\n    max_prune = 100000;\n  }\n  dout(10) << __func__ << \" max_prune \" << max_prune << dendl;\n\n  unsigned actually_pruned = 0;\n  auto& purged_snaps = mon->mgrstatmon()->get_digest().purged_snaps;\n  for (auto& p : osdmap.get_pools()) {\n    auto q = purged_snaps.find(p.first);\n    if (q == purged_snaps.end()) {\n      continue;\n    }\n    auto& purged = q->second;\n    if (purged.empty()) {\n      dout(20) << __func__ << \" \" << p.first << \" nothing purged\" << dendl;\n      continue;\n    }\n    dout(20) << __func__ << \" pool \" << p.first << \" purged \" << purged << dendl;\n    OSDMap::snap_interval_set_t to_prune;\n    unsigned maybe_pruned = actually_pruned;\n    for (auto i = purged.begin(); i != purged.end(); ++i) {\n      snapid_t begin = i.get_start();\n      auto end = i.get_start() + i.get_len();\n      snapid_t pbegin = 0, pend = 0;\n      int r = lookup_pruned_snap(p.first, begin, &pbegin, &pend);\n      if (r == 0) {\n\t// already purged.\n\t// be a bit aggressive about backing off here, because the mon may\n\t// do a lot of work going through this set, and if we know the\n\t// purged set from the OSDs is at least *partly* stale we may as\n\t// well wait for it to be fresh.\n\tdout(20) << __func__ << \"  we've already pruned \" << pbegin\n\t\t << \"~\" << (pend - pbegin) << dendl;\n\tbreak;  // next pool\n      }\n      if (pbegin && pbegin < end) {\n\t// the tail of [begin,end) is purged; shorten the range\n\tassert(pbegin > begin);\n\tend = pbegin;\n      }\n      to_prune.insert(begin, end - begin);\n      maybe_pruned += end - begin;\n      if (maybe_pruned >= max_prune) {\n\tbreak;\n      }\n    }\n    if (!to_prune.empty()) {\n      // PGs may still be reporting things as purged that we have already\n      // pruned from removed_snaps_queue.\n      OSDMap::snap_interval_set_t actual;\n      auto r = osdmap.removed_snaps_queue.find(p.first);\n      if (r != osdmap.removed_snaps_queue.end()) {\n\tactual.intersection_of(to_prune, r->second);\n      }\n      actually_pruned += actual.size();\n      dout(10) << __func__ << \" pool \" << p.first << \" reports pruned \" << to_prune\n\t       << \", actual pruned \" << actual << dendl;\n      if (!actual.empty()) {\n\tpending_inc.new_purged_snaps[p.first].swap(actual);\n      }\n    }\n    if (actually_pruned >= max_prune) {\n      break;\n    }\n  }\n  dout(10) << __func__ << \" actually pruned \" << actually_pruned << dendl;\n  return !!actually_pruned;\n}\n\nbool OSDMonitor::update_pools_status()\n{\n  if (!mon->mgrstatmon()->is_readable())\n    return false;\n\n  bool ret = false;\n\n  auto& pools = osdmap.get_pools();\n  for (auto it = pools.begin(); it != pools.end(); ++it) {\n    const pool_stat_t *pstat = mon->mgrstatmon()->get_pool_stat(it->first);\n    if (!pstat)\n      continue;\n    const object_stat_sum_t& sum = pstat->stats.sum;\n    const pg_pool_t &pool = it->second;\n    const string& pool_name = osdmap.get_pool_name(it->first);\n\n    bool pool_is_full =\n      (pool.quota_max_bytes > 0 && (uint64_t)sum.num_bytes >= pool.quota_max_bytes) ||\n      (pool.quota_max_objects > 0 && (uint64_t)sum.num_objects >= pool.quota_max_objects);\n\n    if (pool.has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n      if (pool_is_full)\n        continue;\n\n      mon->clog->info() << \"pool '\" << pool_name\n                       << \"' no longer out of quota; removing NO_QUOTA flag\";\n      // below we cancel FLAG_FULL too, we'll set it again in\n      // OSDMonitor::encode_pending if it still fails the osd-full checking.\n      clear_pool_flags(it->first,\n                       pg_pool_t::FLAG_FULL_QUOTA | pg_pool_t::FLAG_FULL);\n      ret = true;\n    } else {\n      if (!pool_is_full)\n\tcontinue;\n\n      if (pool.quota_max_bytes > 0 &&\n          (uint64_t)sum.num_bytes >= pool.quota_max_bytes) {\n        mon->clog->warn() << \"pool '\" << pool_name << \"' is full\"\n                         << \" (reached quota's max_bytes: \"\n                         << byte_u_t(pool.quota_max_bytes) << \")\";\n      }\n      if (pool.quota_max_objects > 0 &&\n\t\t (uint64_t)sum.num_objects >= pool.quota_max_objects) {\n        mon->clog->warn() << \"pool '\" << pool_name << \"' is full\"\n                         << \" (reached quota's max_objects: \"\n                         << pool.quota_max_objects << \")\";\n      }\n      // set both FLAG_FULL_QUOTA and FLAG_FULL\n      // note that below we try to cancel FLAG_BACKFILLFULL/NEARFULL too\n      // since FLAG_FULL should always take precedence\n      set_pool_flags(it->first,\n                     pg_pool_t::FLAG_FULL_QUOTA | pg_pool_t::FLAG_FULL);\n      clear_pool_flags(it->first,\n                       pg_pool_t::FLAG_NEARFULL |\n                       pg_pool_t::FLAG_BACKFILLFULL);\n      ret = true;\n    }\n  }\n  return ret;\n}\n\nint OSDMonitor::prepare_new_pool(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  dout(10) << \"prepare_new_pool from \" << m->get_connection() << dendl;\n  MonSession *session = m->get_session();\n  if (!session)\n    return -EPERM;\n  string erasure_code_profile;\n  stringstream ss;\n  string rule_name;\n  int ret = 0;\n  if (m->auid)\n    ret =  prepare_new_pool(m->name, m->auid, m->crush_rule, rule_name,\n\t\t\t    0, 0,\n                            erasure_code_profile,\n\t\t\t    pg_pool_t::TYPE_REPLICATED, 0, FAST_READ_OFF, &ss);\n  else\n    ret = prepare_new_pool(m->name, session->auid, m->crush_rule, rule_name,\n\t\t\t    0, 0,\n                            erasure_code_profile,\n\t\t\t    pg_pool_t::TYPE_REPLICATED, 0, FAST_READ_OFF, &ss);\n\n  if (ret < 0) {\n    dout(10) << __func__ << \" got \" << ret << \" \" << ss.str() << dendl;\n  }\n  return ret;\n}\n\nint OSDMonitor::crush_rename_bucket(const string& srcname,\n\t\t\t\t    const string& dstname,\n\t\t\t\t    ostream *ss)\n{\n  int ret;\n  //\n  // Avoid creating a pending crush if it does not already exists and\n  // the rename would fail.\n  //\n  if (!_have_pending_crush()) {\n    ret = _get_stable_crush().can_rename_bucket(srcname,\n\t\t\t\t\t\tdstname,\n\t\t\t\t\t\tss);\n    if (ret)\n      return ret;\n  }\n\n  CrushWrapper newcrush;\n  _get_pending_crush(newcrush);\n\n  ret = newcrush.rename_bucket(srcname,\n\t\t\t       dstname,\n\t\t\t       ss);\n  if (ret)\n    return ret;\n\n  pending_inc.crush.clear();\n  newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n  *ss << \"renamed bucket \" << srcname << \" into \" << dstname;\t\n  return 0;\n}\n\nvoid OSDMonitor::check_legacy_ec_plugin(const string& plugin, const string& profile) const\n{\n  string replacement = \"\";\n\n  if (plugin == \"jerasure_generic\" || \n      plugin == \"jerasure_sse3\" ||\n      plugin == \"jerasure_sse4\" ||\n      plugin == \"jerasure_neon\") {\n    replacement = \"jerasure\";\n  } else if (plugin == \"shec_generic\" ||\n\t     plugin == \"shec_sse3\" ||\n\t     plugin == \"shec_sse4\" ||\n             plugin == \"shec_neon\") {\n    replacement = \"shec\";\n  }\n\n  if (replacement != \"\") {\n    dout(0) << \"WARNING: erasure coding profile \" << profile << \" uses plugin \"\n\t    << plugin << \" that has been deprecated. Please use \" \n\t    << replacement << \" instead.\" << dendl;\n  }\n}\n\nint OSDMonitor::normalize_profile(const string& profilename,\n\t\t\t\t  ErasureCodeProfile &profile,\n\t\t\t\t  bool force,\n\t\t\t\t  ostream *ss)\n{\n  ErasureCodeInterfaceRef erasure_code;\n  ErasureCodePluginRegistry &instance = ErasureCodePluginRegistry::instance();\n  ErasureCodeProfile::const_iterator plugin = profile.find(\"plugin\");\n  check_legacy_ec_plugin(plugin->second, profilename);\n  int err = instance.factory(plugin->second,\n\t\t\t     g_conf->get_val<std::string>(\"erasure_code_dir\"),\n\t\t\t     profile, &erasure_code, ss);\n  if (err) {\n    return err;\n  }\n\n  err = erasure_code->init(profile, ss);\n  if (err) {\n    return err;\n  }\n\n  auto it = profile.find(\"stripe_unit\");\n  if (it != profile.end()) {\n    string err_str;\n    uint32_t stripe_unit = strict_iecstrtoll(it->second.c_str(), &err_str);\n    if (!err_str.empty()) {\n      *ss << \"could not parse stripe_unit '\" << it->second\n\t  << \"': \" << err_str << std::endl;\n      return -EINVAL;\n    }\n    uint32_t data_chunks = erasure_code->get_data_chunk_count();\n    uint32_t chunk_size = erasure_code->get_chunk_size(stripe_unit * data_chunks);\n    if (chunk_size != stripe_unit) {\n      *ss << \"stripe_unit \" << stripe_unit << \" does not match ec profile \"\n\t  << \"alignment. Would be padded to \" << chunk_size\n\t  << std::endl;\n      return -EINVAL;\n    }\n    if ((stripe_unit % 4096) != 0 && !force) {\n      *ss << \"stripe_unit should be a multiple of 4096 bytes for best performance.\"\n\t  << \"use --force to override this check\" << std::endl;\n      return -EINVAL;\n    }\n  }\n  return 0;\n}\n\nint OSDMonitor::crush_rule_create_erasure(const string &name,\n\t\t\t\t\t     const string &profile,\n\t\t\t\t\t     int *rule,\n\t\t\t\t\t     ostream *ss)\n{\n  int ruleid = osdmap.crush->get_rule_id(name);\n  if (ruleid != -ENOENT) {\n    *rule = osdmap.crush->get_rule_mask_ruleset(ruleid);\n    return -EEXIST;\n  }\n\n  CrushWrapper newcrush;\n  _get_pending_crush(newcrush);\n\n  ruleid = newcrush.get_rule_id(name);\n  if (ruleid != -ENOENT) {\n    *rule = newcrush.get_rule_mask_ruleset(ruleid);\n    return -EALREADY;\n  } else {\n    ErasureCodeInterfaceRef erasure_code;\n    int err = get_erasure_code(profile, &erasure_code, ss);\n    if (err) {\n      *ss << \"failed to load plugin using profile \" << profile << std::endl;\n      return err;\n    }\n\n    err = erasure_code->create_rule(name, newcrush, ss);\n    erasure_code.reset();\n    if (err < 0)\n      return err;\n    *rule = err;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    return 0;\n  }\n}\n\nint OSDMonitor::get_erasure_code(const string &erasure_code_profile,\n\t\t\t\t ErasureCodeInterfaceRef *erasure_code,\n\t\t\t\t ostream *ss) const\n{\n  if (pending_inc.has_erasure_code_profile(erasure_code_profile))\n    return -EAGAIN;\n  ErasureCodeProfile profile =\n    osdmap.get_erasure_code_profile(erasure_code_profile);\n  ErasureCodeProfile::const_iterator plugin =\n    profile.find(\"plugin\");\n  if (plugin == profile.end()) {\n    *ss << \"cannot determine the erasure code plugin\"\n\t<< \" because there is no 'plugin' entry in the erasure_code_profile \"\n\t<< profile << std::endl;\n    return -EINVAL;\n  }\n  check_legacy_ec_plugin(plugin->second, erasure_code_profile);\n  ErasureCodePluginRegistry &instance = ErasureCodePluginRegistry::instance();\n  return instance.factory(plugin->second,\n\t\t\t  g_conf->get_val<std::string>(\"erasure_code_dir\"),\n\t\t\t  profile, erasure_code, ss);\n}\n\nint OSDMonitor::check_cluster_features(uint64_t features,\n\t\t\t\t       stringstream &ss)\n{\n  stringstream unsupported_ss;\n  int unsupported_count = 0;\n  if ((mon->get_quorum_con_features() & features) != features) {\n    unsupported_ss << \"the monitor cluster\";\n    ++unsupported_count;\n  }\n\n  set<int32_t> up_osds;\n  osdmap.get_up_osds(up_osds);\n  for (set<int32_t>::iterator it = up_osds.begin();\n       it != up_osds.end(); ++it) {\n    const osd_xinfo_t &xi = osdmap.get_xinfo(*it);\n    if ((xi.features & features) != features) {\n      if (unsupported_count > 0)\n\tunsupported_ss << \", \";\n      unsupported_ss << \"osd.\" << *it;\n      unsupported_count ++;\n    }\n  }\n\n  if (unsupported_count > 0) {\n    ss << \"features \" << features << \" unsupported by: \"\n       << unsupported_ss.str();\n    return -ENOTSUP;\n  }\n\n  // check pending osd state, too!\n  for (map<int32_t,osd_xinfo_t>::const_iterator p =\n\t pending_inc.new_xinfo.begin();\n       p != pending_inc.new_xinfo.end(); ++p) {\n    const osd_xinfo_t &xi = p->second;\n    if ((xi.features & features) != features) {\n      dout(10) << __func__ << \" pending osd.\" << p->first\n\t       << \" features are insufficient; retry\" << dendl;\n      return -EAGAIN;\n    }\n  }\n\n  return 0;\n}\n\nbool OSDMonitor::validate_crush_against_features(const CrushWrapper *newcrush,\n                                                 stringstream& ss)\n{\n  OSDMap::Incremental new_pending = pending_inc;\n  encode(*newcrush, new_pending.crush, mon->get_quorum_con_features());\n  OSDMap newmap;\n  newmap.deepish_copy_from(osdmap);\n  newmap.apply_incremental(new_pending);\n\n  // client compat\n  if (newmap.require_min_compat_client > 0) {\n    auto mv = newmap.get_min_compat_client();\n    if (mv > newmap.require_min_compat_client) {\n      ss << \"new crush map requires client version \" << ceph_release_name(mv)\n\t << \" but require_min_compat_client is \"\n\t << ceph_release_name(newmap.require_min_compat_client);\n      return false;\n    }\n  }\n\n  // osd compat\n  uint64_t features =\n    newmap.get_features(CEPH_ENTITY_TYPE_MON, NULL) |\n    newmap.get_features(CEPH_ENTITY_TYPE_OSD, NULL);\n  stringstream features_ss;\n  int r = check_cluster_features(features, features_ss);\n  if (r) {\n    ss << \"Could not change CRUSH: \" << features_ss.str();\n    return false;\n  }\n\n  return true;\n}\n\nbool OSDMonitor::erasure_code_profile_in_use(\n  const mempool::osdmap::map<int64_t, pg_pool_t> &pools,\n  const string &profile,\n  ostream *ss)\n{\n  bool found = false;\n  for (map<int64_t, pg_pool_t>::const_iterator p = pools.begin();\n       p != pools.end();\n       ++p) {\n    if (p->second.erasure_code_profile == profile && p->second.is_erasure()) {\n      *ss << osdmap.pool_name[p->first] << \" \";\n      found = true;\n    }\n  }\n  if (found) {\n    *ss << \"pool(s) are using the erasure code profile '\" << profile << \"'\";\n  }\n  return found;\n}\n\nint OSDMonitor::parse_erasure_code_profile(const vector<string> &erasure_code_profile,\n\t\t\t\t\t   map<string,string> *erasure_code_profile_map,\n\t\t\t\t\t   ostream *ss)\n{\n  int r = g_conf->with_val<string>(\"osd_pool_default_erasure_code_profile\",\n\t\t\t\t   get_json_str_map,\n\t\t\t\t   *ss,\n\t\t\t\t   erasure_code_profile_map,\n\t\t\t\t   true);\n  if (r)\n    return r;\n  assert((*erasure_code_profile_map).count(\"plugin\"));\n  string default_plugin = (*erasure_code_profile_map)[\"plugin\"];\n  map<string,string> user_map;\n  for (vector<string>::const_iterator i = erasure_code_profile.begin();\n       i != erasure_code_profile.end();\n       ++i) {\n    size_t equal = i->find('=');\n    if (equal == string::npos) {\n      user_map[*i] = string();\n      (*erasure_code_profile_map)[*i] = string();\n    } else {\n      const string key = i->substr(0, equal);\n      equal++;\n      const string value = i->substr(equal);\n      if (key.find(\"ruleset-\") == 0) {\n\t*ss << \"property '\" << key << \"' is no longer supported; try \"\n\t    << \"'crush-\" << key.substr(8) << \"' instead\";\n\treturn -EINVAL;\n      }\n      user_map[key] = value;\n      (*erasure_code_profile_map)[key] = value;\n    }\n  }\n\n  if (user_map.count(\"plugin\") && user_map[\"plugin\"] != default_plugin)\n    (*erasure_code_profile_map) = user_map;\n\n  return 0;\n}\n\nint OSDMonitor::prepare_pool_size(const unsigned pool_type,\n\t\t\t\t  const string &erasure_code_profile,\n\t\t\t\t  unsigned *size, unsigned *min_size,\n\t\t\t\t  ostream *ss)\n{\n  int err = 0;\n  switch (pool_type) {\n  case pg_pool_t::TYPE_REPLICATED:\n    *size = g_conf->get_val<uint64_t>(\"osd_pool_default_size\");\n    *min_size = g_conf->get_osd_pool_default_min_size();\n    break;\n  case pg_pool_t::TYPE_ERASURE:\n    {\n      ErasureCodeInterfaceRef erasure_code;\n      err = get_erasure_code(erasure_code_profile, &erasure_code, ss);\n      if (err == 0) {\n\t*size = erasure_code->get_chunk_count();\n\t*min_size = std::min(erasure_code->get_data_chunk_count() + 1, *size);\n      }\n    }\n    break;\n  default:\n    *ss << \"prepare_pool_size: \" << pool_type << \" is not a known pool type\";\n    err = -EINVAL;\n    break;\n  }\n  return err;\n}\n\nint OSDMonitor::prepare_pool_stripe_width(const unsigned pool_type,\n\t\t\t\t\t  const string &erasure_code_profile,\n\t\t\t\t\t  uint32_t *stripe_width,\n\t\t\t\t\t  ostream *ss)\n{\n  int err = 0;\n  switch (pool_type) {\n  case pg_pool_t::TYPE_REPLICATED:\n    // ignored\n    break;\n  case pg_pool_t::TYPE_ERASURE:\n    {\n      ErasureCodeProfile profile =\n\tosdmap.get_erasure_code_profile(erasure_code_profile);\n      ErasureCodeInterfaceRef erasure_code;\n      err = get_erasure_code(erasure_code_profile, &erasure_code, ss);\n      if (err)\n\tbreak;\n      uint32_t data_chunks = erasure_code->get_data_chunk_count();\n      uint32_t stripe_unit = g_conf->get_val<Option::size_t>(\"osd_pool_erasure_code_stripe_unit\");\n      auto it = profile.find(\"stripe_unit\");\n      if (it != profile.end()) {\n\tstring err_str;\n\tstripe_unit = strict_iecstrtoll(it->second.c_str(), &err_str);\n\tassert(err_str.empty());\n      }\n      *stripe_width = data_chunks *\n\terasure_code->get_chunk_size(stripe_unit * data_chunks);\n    }\n    break;\n  default:\n    *ss << \"prepare_pool_stripe_width: \"\n       << pool_type << \" is not a known pool type\";\n    err = -EINVAL;\n    break;\n  }\n  return err;\n}\n\nint OSDMonitor::prepare_pool_crush_rule(const unsigned pool_type,\n\t\t\t\t\tconst string &erasure_code_profile,\n\t\t\t\t\tconst string &rule_name,\n\t\t\t\t\tint *crush_rule,\n\t\t\t\t\tostream *ss)\n{\n\n  if (*crush_rule < 0) {\n    switch (pool_type) {\n    case pg_pool_t::TYPE_REPLICATED:\n      {\n\tif (rule_name == \"\") {\n\t  // Use default rule\n\t  *crush_rule = osdmap.crush->get_osd_pool_default_crush_replicated_ruleset(cct);\n\t  if (*crush_rule < 0) {\n\t    // Errors may happen e.g. if no valid rule is available\n\t    *ss << \"No suitable CRUSH rule exists, check \"\n                << \"'osd pool default crush *' config options\";\n\t    return -ENOENT;\n\t  }\n\t} else {\n\t  return get_crush_rule(rule_name, crush_rule, ss);\n\t}\n      }\n      break;\n    case pg_pool_t::TYPE_ERASURE:\n      {\n\tint err = crush_rule_create_erasure(rule_name,\n\t\t\t\t\t       erasure_code_profile,\n\t\t\t\t\t       crush_rule, ss);\n\tswitch (err) {\n\tcase -EALREADY:\n\t  dout(20) << \"prepare_pool_crush_rule: rule \"\n\t\t   << rule_name << \" try again\" << dendl;\n\t  // fall through\n\tcase 0:\n\t  // need to wait for the crush rule to be proposed before proceeding\n\t  err = -EAGAIN;\n\t  break;\n\tcase -EEXIST:\n\t  err = 0;\n\t  break;\n \t}\n\treturn err;\n      }\n      break;\n    default:\n      *ss << \"prepare_pool_crush_rule: \" << pool_type\n\t << \" is not a known pool type\";\n      return -EINVAL;\n      break;\n    }\n  } else {\n    if (!osdmap.crush->ruleset_exists(*crush_rule)) {\n      *ss << \"CRUSH rule \" << *crush_rule << \" not found\";\n      return -ENOENT;\n    }\n  }\n\n  return 0;\n}\n\nint OSDMonitor::get_crush_rule(const string &rule_name,\n\t\t\t       int *crush_rule,\n\t\t\t       ostream *ss)\n{\n  int ret;\n  ret = osdmap.crush->get_rule_id(rule_name);\n  if (ret != -ENOENT) {\n    // found it, use it\n    *crush_rule = ret;\n  } else {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    ret = newcrush.get_rule_id(rule_name);\n    if (ret != -ENOENT) {\n      // found it, wait for it to be proposed\n      dout(20) << __func__ << \": rule \" << rule_name\n\t       << \" try again\" << dendl;\n      return -EAGAIN;\n    } else {\n      // Cannot find it , return error\n      *ss << \"specified rule \" << rule_name << \" doesn't exist\";\n      return ret;\n    }\n  }\n  return 0;\n}\n\nint OSDMonitor::check_pg_num(int64_t pool, int pg_num, int size, ostream *ss)\n{\n  auto max_pgs_per_osd = g_conf->get_val<uint64_t>(\"mon_max_pg_per_osd\");\n  auto num_osds = std::max(osdmap.get_num_in_osds(), 3u);   // assume min cluster size 3\n  auto max_pgs = max_pgs_per_osd * num_osds;\n  uint64_t projected = 0;\n  if (pool < 0) {\n    projected += pg_num * size;\n  }\n  for (const auto& i : osdmap.get_pools()) {\n    if (i.first == pool) {\n      projected += pg_num * size;\n    } else {\n      projected += i.second.get_pg_num() * i.second.get_size();\n    }\n  }\n  if (projected > max_pgs) {\n    if (pool >= 0) {\n      *ss << \"pool id \" << pool;\n    }\n    *ss << \" pg_num \" << pg_num << \" size \" << size\n\t<< \" would mean \" << projected\n\t<< \" total pgs, which exceeds max \" << max_pgs\n\t<< \" (mon_max_pg_per_osd \" << max_pgs_per_osd\n\t<< \" * num_in_osds \" << num_osds << \")\";\n    return -ERANGE;\n  }\n  return 0;\n}\n\n/**\n * @param name The name of the new pool\n * @param auid The auid of the pool owner. Can be -1\n * @param crush_rule The crush rule to use. If <0, will use the system default\n * @param crush_rule_name The crush rule to use, if crush_rulset <0\n * @param pg_num The pg_num to use. If set to 0, will use the system default\n * @param pgp_num The pgp_num to use. If set to 0, will use the system default\n * @param erasure_code_profile The profile name in OSDMap to be used for erasure code\n * @param pool_type TYPE_ERASURE, or TYPE_REP\n * @param expected_num_objects expected number of objects on the pool\n * @param fast_read fast read type. \n * @param ss human readable error message, if any.\n *\n * @return 0 on success, negative errno on failure.\n */\nint OSDMonitor::prepare_new_pool(string& name, uint64_t auid,\n\t\t\t\t int crush_rule,\n\t\t\t\t const string &crush_rule_name,\n                                 unsigned pg_num, unsigned pgp_num,\n\t\t\t\t const string &erasure_code_profile,\n                                 const unsigned pool_type,\n                                 const uint64_t expected_num_objects,\n                                 FastReadType fast_read,\n\t\t\t\t ostream *ss)\n{\n  if (name.length() == 0)\n    return -EINVAL;\n  if (pg_num == 0)\n    pg_num = g_conf->get_val<uint64_t>(\"osd_pool_default_pg_num\");\n  if (pgp_num == 0)\n    pgp_num = g_conf->get_val<uint64_t>(\"osd_pool_default_pgp_num\");\n  if (pg_num > g_conf->get_val<uint64_t>(\"mon_max_pool_pg_num\")) {\n    *ss << \"'pg_num' must be greater than 0 and less than or equal to \"\n        << g_conf->get_val<uint64_t>(\"mon_max_pool_pg_num\")\n        << \" (you may adjust 'mon max pool pg num' for higher values)\";\n    return -ERANGE;\n  }\n  if (pgp_num > pg_num) {\n    *ss << \"'pgp_num' must be greater than 0 and lower or equal than 'pg_num'\"\n        << \", which in this case is \" << pg_num;\n    return -ERANGE;\n  }\n  if (pool_type == pg_pool_t::TYPE_REPLICATED && fast_read == FAST_READ_ON) {\n    *ss << \"'fast_read' can only apply to erasure coding pool\";\n    return -EINVAL;\n  }\n  int r;\n  r = prepare_pool_crush_rule(pool_type, erasure_code_profile,\n\t\t\t\t crush_rule_name, &crush_rule, ss);\n  if (r) {\n    dout(10) << \"prepare_pool_crush_rule returns \" << r << dendl;\n    return r;\n  }\n  if (g_conf->mon_osd_crush_smoke_test) {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    ostringstream err;\n    CrushTester tester(newcrush, err);\n    tester.set_min_x(0);\n    tester.set_max_x(50);\n    tester.set_rule(crush_rule);\n    auto start = ceph::coarse_mono_clock::now();\n    r = tester.test_with_fork(g_conf->mon_lease);\n    auto duration = ceph::coarse_mono_clock::now() - start;\n    if (r < 0) {\n      dout(10) << \"tester.test_with_fork returns \" << r\n\t       << \": \" << err.str() << dendl;\n      *ss << \"crush test failed with \" << r << \": \" << err.str();\n      return r;\n    }\n    dout(10) << __func__ << \" crush smoke test duration: \"\n             << duration << dendl;\n  }\n  unsigned size, min_size;\n  r = prepare_pool_size(pool_type, erasure_code_profile, &size, &min_size, ss);\n  if (r) {\n    dout(10) << \"prepare_pool_size returns \" << r << dendl;\n    return r;\n  }\n  r = check_pg_num(-1, pg_num, size, ss);\n  if (r) {\n    dout(10) << \"check_pg_num returns \" << r << dendl;\n    return r;\n  }\n\n  if (!osdmap.crush->check_crush_rule(crush_rule, pool_type, size, *ss)) {\n    return -EINVAL;\n  }\n\n  uint32_t stripe_width = 0;\n  r = prepare_pool_stripe_width(pool_type, erasure_code_profile, &stripe_width, ss);\n  if (r) {\n    dout(10) << \"prepare_pool_stripe_width returns \" << r << dendl;\n    return r;\n  }\n  \n  bool fread = false;\n  if (pool_type == pg_pool_t::TYPE_ERASURE) {\n    switch (fast_read) {\n      case FAST_READ_OFF:\n        fread = false;\n        break;\n      case FAST_READ_ON:\n        fread = true;\n        break;\n      case FAST_READ_DEFAULT:\n        fread = g_conf->mon_osd_pool_ec_fast_read;\n        break;\n      default:\n        *ss << \"invalid fast_read setting: \" << fast_read;\n        return -EINVAL;\n    }\n  }\n\n  for (map<int64_t,string>::iterator p = pending_inc.new_pool_names.begin();\n       p != pending_inc.new_pool_names.end();\n       ++p) {\n    if (p->second == name)\n      return 0;\n  }\n\n  if (-1 == pending_inc.new_pool_max)\n    pending_inc.new_pool_max = osdmap.pool_max;\n  int64_t pool = ++pending_inc.new_pool_max;\n  pg_pool_t empty;\n  pg_pool_t *pi = pending_inc.get_new_pool(pool, &empty);\n  pi->create_time = ceph_clock_now();\n  pi->type = pool_type;\n  pi->fast_read = fread; \n  pi->flags = g_conf->osd_pool_default_flags;\n  if (g_conf->osd_pool_default_flag_hashpspool)\n    pi->set_flag(pg_pool_t::FLAG_HASHPSPOOL);\n  if (g_conf->osd_pool_default_flag_nodelete)\n    pi->set_flag(pg_pool_t::FLAG_NODELETE);\n  if (g_conf->osd_pool_default_flag_nopgchange)\n    pi->set_flag(pg_pool_t::FLAG_NOPGCHANGE);\n  if (g_conf->osd_pool_default_flag_nosizechange)\n    pi->set_flag(pg_pool_t::FLAG_NOSIZECHANGE);\n  if (g_conf->osd_pool_use_gmt_hitset)\n    pi->use_gmt_hitset = true;\n  else\n    pi->use_gmt_hitset = false;\n\n  pi->size = size;\n  pi->min_size = min_size;\n  pi->crush_rule = crush_rule;\n  pi->expected_num_objects = expected_num_objects;\n  pi->object_hash = CEPH_STR_HASH_RJENKINS;\n  pi->set_pg_num(pg_num);\n  pi->set_pgp_num(pgp_num);\n  pi->last_change = pending_inc.epoch;\n  pi->auid = auid;\n  if (pool_type == pg_pool_t::TYPE_ERASURE) {\n      pi->erasure_code_profile = erasure_code_profile;\n  } else {\n      pi->erasure_code_profile = \"\";\n  }\n  pi->stripe_width = stripe_width;\n  pi->cache_target_dirty_ratio_micro =\n    g_conf->osd_pool_default_cache_target_dirty_ratio * 1000000;\n  pi->cache_target_dirty_high_ratio_micro =\n    g_conf->osd_pool_default_cache_target_dirty_high_ratio * 1000000;\n  pi->cache_target_full_ratio_micro =\n    g_conf->osd_pool_default_cache_target_full_ratio * 1000000;\n  pi->cache_min_flush_age = g_conf->osd_pool_default_cache_min_flush_age;\n  pi->cache_min_evict_age = g_conf->osd_pool_default_cache_min_evict_age;\n  pending_inc.new_pool_names[pool] = name;\n  return 0;\n}\n\nbool OSDMonitor::prepare_set_flag(MonOpRequestRef op, int flag)\n{\n  op->mark_osdmon_event(__func__);\n  ostringstream ss;\n  if (pending_inc.new_flags < 0)\n    pending_inc.new_flags = osdmap.get_flags();\n  pending_inc.new_flags |= flag;\n  ss << OSDMap::get_flag_string(flag) << \" is set\";\n  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t\t    get_last_committed() + 1));\n  return true;\n}\n\nbool OSDMonitor::prepare_unset_flag(MonOpRequestRef op, int flag)\n{\n  op->mark_osdmon_event(__func__);\n  ostringstream ss;\n  if (pending_inc.new_flags < 0)\n    pending_inc.new_flags = osdmap.get_flags();\n  pending_inc.new_flags &= ~flag;\n  ss << OSDMap::get_flag_string(flag) << \" is unset\";\n  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t\t    get_last_committed() + 1));\n  return true;\n}\n\nint OSDMonitor::prepare_command_pool_set(const cmdmap_t& cmdmap,\n                                         stringstream& ss)\n{\n  string poolstr;\n  cmd_getval(cct, cmdmap, \"pool\", poolstr);\n  int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n  if (pool < 0) {\n    ss << \"unrecognized pool '\" << poolstr << \"'\";\n    return -ENOENT;\n  }\n  string var;\n  cmd_getval(cct, cmdmap, \"var\", var);\n\n  pg_pool_t p = *osdmap.get_pg_pool(pool);\n  if (pending_inc.new_pools.count(pool))\n    p = pending_inc.new_pools[pool];\n\n  // accept val as a json string in the normal case (current\n  // generation monitor).  parse out int or float values from the\n  // string as needed.  however, if it is not a string, try to pull\n  // out an int, in case an older monitor with an older json schema is\n  // forwarding a request.\n  string val;\n  string interr, floaterr;\n  int64_t n = 0;\n  double f = 0;\n  int64_t uf = 0;  // micro-f\n  if (!cmd_getval(cct, cmdmap, \"val\", val)) {\n    // wasn't a string; maybe an older mon forwarded json with an int?\n    if (!cmd_getval(cct, cmdmap, \"val\", n))\n      return -EINVAL;  // no value!\n  } else {\n    // we got a string.  see if it contains an int.\n    n = strict_strtoll(val.c_str(), 10, &interr);\n    // or a float\n    f = strict_strtod(val.c_str(), &floaterr);\n    uf = llrintl(f * (double)1000000.0);\n  }\n\n  if (!p.is_tier() &&\n      (var == \"hit_set_type\" || var == \"hit_set_period\" ||\n       var == \"hit_set_count\" || var == \"hit_set_fpp\" ||\n       var == \"target_max_objects\" || var == \"target_max_bytes\" ||\n       var == \"cache_target_full_ratio\" || var == \"cache_target_dirty_ratio\" ||\n       var == \"cache_target_dirty_high_ratio\" || var == \"use_gmt_hitset\" ||\n       var == \"cache_min_flush_age\" || var == \"cache_min_evict_age\" ||\n       var == \"hit_set_grade_decay_rate\" || var == \"hit_set_search_last_n\" ||\n       var == \"min_read_recency_for_promote\" || var == \"min_write_recency_for_promote\")) {\n    return -EACCES;\n  }\n\n  if (var == \"size\") {\n    if (p.has_flag(pg_pool_t::FLAG_NOSIZECHANGE)) {\n      ss << \"pool size change is disabled; you must unset nosizechange flag for the pool first\";\n      return -EPERM;\n    }\n    if (p.type == pg_pool_t::TYPE_ERASURE) {\n      ss << \"can not change the size of an erasure-coded pool\";\n      return -ENOTSUP;\n    }\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n <= 0 || n > 10) {\n      ss << \"pool size must be between 1 and 10\";\n      return -EINVAL;\n    }\n    int r = check_pg_num(pool, p.get_pg_num(), n, &ss);\n    if (r < 0) {\n      return r;\n    }\n    p.size = n;\n    if (n < p.min_size)\n      p.min_size = n;\n  } else if (var == \"min_size\") {\n    if (p.has_flag(pg_pool_t::FLAG_NOSIZECHANGE)) {\n      ss << \"pool min size change is disabled; you must unset nosizechange flag for the pool first\";\n      return -EPERM;\n    }\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n\n    if (p.type != pg_pool_t::TYPE_ERASURE) {\n      if (n < 1 || n > p.size) {\n\tss << \"pool min_size must be between 1 and \" << (int)p.size;\n\treturn -EINVAL;\n      }\n    } else {\n       ErasureCodeInterfaceRef erasure_code;\n       int k;\n       stringstream tmp;\n       int err = get_erasure_code(p.erasure_code_profile, &erasure_code, &tmp);\n       if (err == 0) {\n\t k = erasure_code->get_data_chunk_count();\n       } else {\n\t ss << __func__ << \" get_erasure_code failed: \" << tmp.str();\n\t return err;\n       }\n\n       if (n < k || n > p.size) {\n\t ss << \"pool min_size must be between \" << k << \" and \" << (int)p.size;\n\t return -EINVAL;\n       }\n    }\n    p.min_size = n;\n  } else if (var == \"auid\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.auid = n;\n  } else if (var == \"pg_num\") {\n    if (p.has_flag(pg_pool_t::FLAG_NOPGCHANGE)) {\n      ss << \"pool pg_num change is disabled; you must unset nopgchange flag for the pool first\";\n      return -EPERM;\n    }\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n <= (int)p.get_pg_num()) {\n      ss << \"specified pg_num \" << n << \" <= current \" << p.get_pg_num();\n      if (n < (int)p.get_pg_num())\n\treturn -EEXIST;\n      return 0;\n    }\n    if (static_cast<uint64_t>(n) > g_conf->get_val<uint64_t>(\"mon_max_pool_pg_num\")) {\n      ss << \"'pg_num' must be greater than 0 and less than or equal to \"\n         << g_conf->get_val<uint64_t>(\"mon_max_pool_pg_num\")\n         << \" (you may adjust 'mon max pool pg num' for higher values)\";\n      return -ERANGE;\n    }\n    int r = check_pg_num(pool, n, p.get_size(), &ss);\n    if (r) {\n      return r;\n    }\n    string force;\n    cmd_getval(cct,cmdmap, \"force\", force);\n    if (p.cache_mode != pg_pool_t::CACHEMODE_NONE &&\n\tforce != \"--yes-i-really-mean-it\") {\n      ss << \"splits in cache pools must be followed by scrubs and leave sufficient free space to avoid overfilling.  use --yes-i-really-mean-it to force.\";\n      return -EPERM;\n    }\n    int expected_osds = std::min(p.get_pg_num(), osdmap.get_num_osds());\n    int64_t new_pgs = n - p.get_pg_num();\n    if (new_pgs > g_conf->mon_osd_max_split_count * expected_osds) {\n      ss << \"specified pg_num \" << n << \" is too large (creating \"\n\t << new_pgs << \" new PGs on ~\" << expected_osds\n\t << \" OSDs exceeds per-OSD max with mon_osd_max_split_count of \"\n         << g_conf->mon_osd_max_split_count << ')';\n      return -E2BIG;\n    }\n    p.set_pg_num(n);\n    // force pre-luminous clients to resend their ops, since they\n    // don't understand that split PGs now form a new interval.\n    p.last_force_op_resend_preluminous = pending_inc.epoch;\n  } else if (var == \"pgp_num\") {\n    if (p.has_flag(pg_pool_t::FLAG_NOPGCHANGE)) {\n      ss << \"pool pgp_num change is disabled; you must unset nopgchange flag for the pool first\";\n      return -EPERM;\n    }\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n <= 0) {\n      ss << \"specified pgp_num must > 0, but you set to \" << n;\n      return -EINVAL;\n    }\n    if (n > (int)p.get_pg_num()) {\n      ss << \"specified pgp_num \" << n << \" > pg_num \" << p.get_pg_num();\n      return -EINVAL;\n    }\n    p.set_pgp_num(n);\n  } else if (var == \"crush_rule\") {\n    int id = osdmap.crush->get_rule_id(val);\n    if (id == -ENOENT) {\n      ss << \"crush rule \" << val << \" does not exist\";\n      return -ENOENT;\n    }\n    if (id < 0) {\n      ss << cpp_strerror(id);\n      return -ENOENT;\n    }\n    if (!osdmap.crush->check_crush_rule(id, p.get_type(), p.get_size(), ss)) {\n      return -EINVAL;\n    }\n    p.crush_rule = id;\n  } else if (var == \"nodelete\" || var == \"nopgchange\" ||\n\t     var == \"nosizechange\" || var == \"write_fadvise_dontneed\" ||\n\t     var == \"noscrub\" || var == \"nodeep-scrub\") {\n    uint64_t flag = pg_pool_t::get_flag_by_name(var);\n    // make sure we only compare against 'n' if we didn't receive a string\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n      p.set_flag(flag);\n    } else if (val == \"false\" || (interr.empty() && n == 0)) {\n      p.unset_flag(flag);\n    } else {\n      ss << \"expecting value 'true', 'false', '0', or '1'\";\n      return -EINVAL;\n    }\n  } else if (var == \"hashpspool\") {\n    uint64_t flag = pg_pool_t::get_flag_by_name(var);\n    string force;\n    cmd_getval(cct, cmdmap, \"force\", force);\n    if (force != \"--yes-i-really-mean-it\") {\n      ss << \"are you SURE?  this will remap all placement groups in this pool,\"\n\t    \" this triggers large data movement,\"\n\t    \" pass --yes-i-really-mean-it if you really do.\";\n      return -EPERM;\n    }\n    // make sure we only compare against 'n' if we didn't receive a string\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n      p.set_flag(flag);\n    } else if (val == \"false\" || (interr.empty() && n == 0)) {\n      p.unset_flag(flag);\n    } else {\n      ss << \"expecting value 'true', 'false', '0', or '1'\";\n      return -EINVAL;\n    }\n  } else if (var == \"hit_set_type\") {\n    if (val == \"none\")\n      p.hit_set_params = HitSet::Params();\n    else {\n      int err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n      if (err)\n\treturn err;\n      if (val == \"bloom\") {\n\tBloomHitSet::Params *bsp = new BloomHitSet::Params;\n\tbsp->set_fpp(g_conf->get_val<double>(\"osd_pool_default_hit_set_bloom_fpp\"));\n\tp.hit_set_params = HitSet::Params(bsp);\n      } else if (val == \"explicit_hash\")\n\tp.hit_set_params = HitSet::Params(new ExplicitHashHitSet::Params);\n      else if (val == \"explicit_object\")\n\tp.hit_set_params = HitSet::Params(new ExplicitObjectHitSet::Params);\n      else {\n\tss << \"unrecognized hit_set type '\" << val << \"'\";\n\treturn -EINVAL;\n      }\n    }\n  } else if (var == \"hit_set_period\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.hit_set_period = n;\n  } else if (var == \"hit_set_count\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.hit_set_count = n;\n  } else if (var == \"hit_set_fpp\") {\n    if (floaterr.length()) {\n      ss << \"error parsing floating point value '\" << val << \"': \" << floaterr;\n      return -EINVAL;\n    }\n    if (p.hit_set_params.get_type() != HitSet::TYPE_BLOOM) {\n      ss << \"hit set is not of type Bloom; invalid to set a false positive rate!\";\n      return -EINVAL;\n    }\n    BloomHitSet::Params *bloomp = static_cast<BloomHitSet::Params*>(p.hit_set_params.impl.get());\n    bloomp->set_fpp(f);\n  } else if (var == \"use_gmt_hitset\") {\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n      p.use_gmt_hitset = true;\n    } else {\n      ss << \"expecting value 'true' or '1'\";\n      return -EINVAL;\n    }\n  } else if (var == \"allow_ec_overwrites\") {\n    if (!p.is_erasure()) {\n      ss << \"ec overwrites can only be enabled for an erasure coded pool\";\n      return -EINVAL;\n    }\n    stringstream err;\n    if (!g_conf->mon_debug_no_require_bluestore_for_ec_overwrites &&\n\t!is_pool_currently_all_bluestore(pool, p, &err)) {\n      ss << \"pool must only be stored on bluestore for scrubbing to work: \" << err.str();\n      return -EINVAL;\n    }\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n\tp.flags |= pg_pool_t::FLAG_EC_OVERWRITES;\n    } else if (val == \"false\" || (interr.empty() && n == 0)) {\n      ss << \"ec overwrites cannot be disabled once enabled\";\n      return -EINVAL;\n    } else {\n      ss << \"expecting value 'true', 'false', '0', or '1'\";\n      return -EINVAL;\n    }\n  } else if (var == \"target_max_objects\") {\n    if (interr.length()) {\n      ss << \"error parsing int '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.target_max_objects = n;\n  } else if (var == \"target_max_bytes\") {\n    if (interr.length()) {\n      ss << \"error parsing int '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.target_max_bytes = n;\n  } else if (var == \"cache_target_dirty_ratio\") {\n    if (floaterr.length()) {\n      ss << \"error parsing float '\" << val << \"': \" << floaterr;\n      return -EINVAL;\n    }\n    if (f < 0 || f > 1.0) {\n      ss << \"value must be in the range 0..1\";\n      return -ERANGE;\n    }\n    p.cache_target_dirty_ratio_micro = uf;\n  } else if (var == \"cache_target_dirty_high_ratio\") {\n    if (floaterr.length()) {\n      ss << \"error parsing float '\" << val << \"': \" << floaterr;\n      return -EINVAL;\n    }\n    if (f < 0 || f > 1.0) {\n      ss << \"value must be in the range 0..1\";\n      return -ERANGE;\n    }\n    p.cache_target_dirty_high_ratio_micro = uf;\n  } else if (var == \"cache_target_full_ratio\") {\n    if (floaterr.length()) {\n      ss << \"error parsing float '\" << val << \"': \" << floaterr;\n      return -EINVAL;\n    }\n    if (f < 0 || f > 1.0) {\n      ss << \"value must be in the range 0..1\";\n      return -ERANGE;\n    }\n    p.cache_target_full_ratio_micro = uf;\n  } else if (var == \"cache_min_flush_age\") {\n    if (interr.length()) {\n      ss << \"error parsing int '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.cache_min_flush_age = n;\n  } else if (var == \"cache_min_evict_age\") {\n    if (interr.length()) {\n      ss << \"error parsing int '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.cache_min_evict_age = n;\n  } else if (var == \"min_read_recency_for_promote\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.min_read_recency_for_promote = n;\n  } else if (var == \"hit_set_grade_decay_rate\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n > 100 || n < 0) {\n      ss << \"value out of range,valid range is 0 - 100\";\n      return -EINVAL;\n    }\n    p.hit_set_grade_decay_rate = n;\n  } else if (var == \"hit_set_search_last_n\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n > p.hit_set_count || n < 0) {\n      ss << \"value out of range,valid range is 0 - hit_set_count\";\n      return -EINVAL;\n    }\n    p.hit_set_search_last_n = n;\n  } else if (var == \"min_write_recency_for_promote\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.min_write_recency_for_promote = n;\n  } else if (var == \"fast_read\") {\n    if (p.is_replicated()) {\n        ss << \"fast read is not supported in replication pool\";\n        return -EINVAL;\n    }\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n      p.fast_read = true;\n    } else if (val == \"false\" || (interr.empty() && n == 0)) {\n      p.fast_read = false;\n    } else {\n      ss << \"expecting value 'true', 'false', '0', or '1'\";\n      return -EINVAL;\n    }\n  } else if (pool_opts_t::is_opt_name(var)) {\n    bool unset = val == \"unset\";\n    if (var == \"compression_mode\") {\n      if (!unset) {\n        auto cmode = Compressor::get_comp_mode_type(val);\n        if (!cmode) {\n\t  ss << \"unrecognized compression mode '\" << val << \"'\";\n\t  return -EINVAL;\n        }\n      }\n    } else if (var == \"compression_algorithm\") {\n      if (!unset) {\n        auto alg = Compressor::get_comp_alg_type(val);\n        if (!alg) {\n          ss << \"unrecognized compression_algorithm '\" << val << \"'\";\n\t  return -EINVAL;\n        }\n      }\n    } else if (var == \"compression_required_ratio\") {\n      if (floaterr.length()) {\n        ss << \"error parsing float value '\" << val << \"': \" << floaterr;\n        return -EINVAL;\n      }\n      if (f < 0 || f > 1) {\n        ss << \"compression_required_ratio is out of range (0-1): '\" << val << \"'\";\n\treturn -EINVAL;\n      }\n    } else if (var == \"csum_type\") {\n      auto t = unset ? 0 : Checksummer::get_csum_string_type(val);\n      if (t < 0 ) {\n        ss << \"unrecognized csum_type '\" << val << \"'\";\n\treturn -EINVAL;\n      }\n      //preserve csum_type numeric value\n      n = t;\n      interr.clear(); \n    } else if (var == \"compression_max_blob_size\" ||\n               var == \"compression_min_blob_size\" ||\n               var == \"csum_max_block\" ||\n               var == \"csum_min_block\") {\n      if (interr.length()) {\n        ss << \"error parsing int value '\" << val << \"': \" << interr;\n        return -EINVAL;\n      }\n    }\n\n    pool_opts_t::opt_desc_t desc = pool_opts_t::get_opt_desc(var);\n    switch (desc.type) {\n    case pool_opts_t::STR:\n      if (unset) {\n\tp.opts.unset(desc.key);\n      } else {\n\tp.opts.set(desc.key, static_cast<std::string>(val));\n      }\n      break;\n    case pool_opts_t::INT:\n      if (interr.length()) {\n\tss << \"error parsing integer value '\" << val << \"': \" << interr;\n\treturn -EINVAL;\n      }\n      if (n == 0) {\n\tp.opts.unset(desc.key);\n      } else {\n\tp.opts.set(desc.key, static_cast<int>(n));\n      }\n      break;\n    case pool_opts_t::DOUBLE:\n      if (floaterr.length()) {\n\tss << \"error parsing floating point value '\" << val << \"': \" << floaterr;\n\treturn -EINVAL;\n      }\n      if (f == 0) {\n\tp.opts.unset(desc.key);\n      } else {\n\tp.opts.set(desc.key, static_cast<double>(f));\n      }\n      break;\n    default:\n      assert(!\"unknown type\");\n    }\n  } else {\n    ss << \"unrecognized variable '\" << var << \"'\";\n    return -EINVAL;\n  }\n  if (val != \"unset\") {\n    ss << \"set pool \" << pool << \" \" << var << \" to \" << val;\n  } else {\n    ss << \"unset pool \" << pool << \" \" << var;\n  }\n  p.last_change = pending_inc.epoch;\n  pending_inc.new_pools[pool] = p;\n  return 0;\n}\n\nint OSDMonitor::prepare_command_pool_application(const string &prefix,\n                                                 const cmdmap_t& cmdmap,\n                                                 stringstream& ss)\n{\n  string pool_name;\n  cmd_getval(cct, cmdmap, \"pool\", pool_name);\n  int64_t pool = osdmap.lookup_pg_pool_name(pool_name.c_str());\n  if (pool < 0) {\n    ss << \"unrecognized pool '\" << pool_name << \"'\";\n    return -ENOENT;\n  }\n\n  pg_pool_t p = *osdmap.get_pg_pool(pool);\n  if (pending_inc.new_pools.count(pool)) {\n    p = pending_inc.new_pools[pool];\n  }\n\n  string app;\n  cmd_getval(cct, cmdmap, \"app\", app);\n  bool app_exists = (p.application_metadata.count(app) > 0);\n\n  string key;\n  cmd_getval(cct, cmdmap, \"key\", key);\n  if (key == \"all\") {\n    ss << \"key cannot be 'all'\";\n    return -EINVAL;\n  }\n\n  string value;\n  cmd_getval(cct, cmdmap, \"value\", value);\n  if (value == \"all\") {\n    ss << \"value cannot be 'all'\";\n    return -EINVAL;\n  }\n\n  if (boost::algorithm::ends_with(prefix, \"enable\")) {\n    if (app.empty()) {\n      ss << \"application name must be provided\";\n      return -EINVAL;\n    }\n\n    if (p.is_tier()) {\n      ss << \"application must be enabled on base tier\";\n      return -EINVAL;\n    }\n\n    string force;\n    cmd_getval(cct, cmdmap, \"force\", force);\n\n    if (!app_exists && !p.application_metadata.empty() &&\n        force != \"--yes-i-really-mean-it\") {\n      ss << \"Are you SURE? Pool '\" << pool_name << \"' already has an enabled \"\n         << \"application; pass --yes-i-really-mean-it to proceed anyway\";\n      return -EPERM;\n    }\n\n    if (!app_exists && p.application_metadata.size() >= MAX_POOL_APPLICATIONS) {\n      ss << \"too many enabled applications on pool '\" << pool_name << \"'; \"\n         << \"max \" << MAX_POOL_APPLICATIONS;\n      return -EINVAL;\n    }\n\n    if (app.length() > MAX_POOL_APPLICATION_LENGTH) {\n      ss << \"application name '\" << app << \"' too long; max length \"\n         << MAX_POOL_APPLICATION_LENGTH;\n      return -EINVAL;\n    }\n\n    if (!app_exists) {\n      p.application_metadata[app] = {};\n    }\n    ss << \"enabled application '\" << app << \"' on pool '\" << pool_name << \"'\";\n\n  } else if (boost::algorithm::ends_with(prefix, \"disable\")) {\n    string force;\n    cmd_getval(cct, cmdmap, \"force\", force);\n\n    if (force != \"--yes-i-really-mean-it\") {\n      ss << \"Are you SURE? Disabling an application within a pool might result \"\n         << \"in loss of application functionality; pass \"\n         << \"--yes-i-really-mean-it to proceed anyway\";\n      return -EPERM;\n    }\n\n    if (!app_exists) {\n      ss << \"application '\" << app << \"' is not enabled on pool '\" << pool_name\n         << \"'\";\n      return 0; // idempotent\n    }\n\n    p.application_metadata.erase(app);\n    ss << \"disable application '\" << app << \"' on pool '\" << pool_name << \"'\";\n\n  } else if (boost::algorithm::ends_with(prefix, \"set\")) {\n    if (p.is_tier()) {\n      ss << \"application metadata must be set on base tier\";\n      return -EINVAL;\n    }\n\n    if (!app_exists) {\n      ss << \"application '\" << app << \"' is not enabled on pool '\" << pool_name\n         << \"'\";\n      return -ENOENT;\n    }\n\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n\n    if (key.empty()) {\n      ss << \"key must be provided\";\n      return -EINVAL;\n    }\n\n    auto &app_keys = p.application_metadata[app];\n    if (app_keys.count(key) == 0 &&\n        app_keys.size() >= MAX_POOL_APPLICATION_KEYS) {\n      ss << \"too many keys set for application '\" << app << \"' on pool '\"\n         << pool_name << \"'; max \" << MAX_POOL_APPLICATION_KEYS;\n      return -EINVAL;\n    }\n\n    if (key.length() > MAX_POOL_APPLICATION_LENGTH) {\n      ss << \"key '\" << app << \"' too long; max length \"\n         << MAX_POOL_APPLICATION_LENGTH;\n      return -EINVAL;\n    }\n\n    string value;\n    cmd_getval(cct, cmdmap, \"value\", value);\n    if (value.length() > MAX_POOL_APPLICATION_LENGTH) {\n      ss << \"value '\" << value << \"' too long; max length \"\n         << MAX_POOL_APPLICATION_LENGTH;\n      return -EINVAL;\n    }\n\n    p.application_metadata[app][key] = value;\n    ss << \"set application '\" << app << \"' key '\" << key << \"' to '\"\n       << value << \"' on pool '\" << pool_name << \"'\";\n  } else if (boost::algorithm::ends_with(prefix, \"rm\")) {\n    if (!app_exists) {\n      ss << \"application '\" << app << \"' is not enabled on pool '\" << pool_name\n         << \"'\";\n      return -ENOENT;\n    }\n\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n    auto it = p.application_metadata[app].find(key);\n    if (it == p.application_metadata[app].end()) {\n      ss << \"application '\" << app << \"' on pool '\" << pool_name\n         << \"' does not have key '\" << key << \"'\";\n      return 0; // idempotent\n    }\n\n    p.application_metadata[app].erase(it);\n    ss << \"removed application '\" << app << \"' key '\" << key << \"' on pool '\"\n       << pool_name << \"'\";\n  } else {\n    ceph_abort();\n  }\n\n  p.last_change = pending_inc.epoch;\n  pending_inc.new_pools[pool] = p;\n  return 0;\n}\n\nint OSDMonitor::_prepare_command_osd_crush_remove(\n    CrushWrapper &newcrush,\n    int32_t id,\n    int32_t ancestor,\n    bool has_ancestor,\n    bool unlink_only)\n{\n  int err = 0;\n\n  if (has_ancestor) {\n    err = newcrush.remove_item_under(cct, id, ancestor,\n        unlink_only);\n  } else {\n    err = newcrush.remove_item(cct, id, unlink_only);\n  }\n  return err;\n}\n\nvoid OSDMonitor::do_osd_crush_remove(CrushWrapper& newcrush)\n{\n  pending_inc.crush.clear();\n  newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n}\n\nint OSDMonitor::prepare_command_osd_crush_remove(\n    CrushWrapper &newcrush,\n    int32_t id,\n    int32_t ancestor,\n    bool has_ancestor,\n    bool unlink_only)\n{\n  int err = _prepare_command_osd_crush_remove(\n      newcrush, id, ancestor,\n      has_ancestor, unlink_only);\n\n  if (err < 0)\n    return err;\n\n  assert(err == 0);\n  do_osd_crush_remove(newcrush);\n\n  return 0;\n}\n\nint OSDMonitor::prepare_command_osd_remove(int32_t id)\n{\n  if (osdmap.is_up(id)) {\n    return -EBUSY;\n  }\n\n  pending_inc.new_state[id] = osdmap.get_state(id);\n  pending_inc.new_uuid[id] = uuid_d();\n  pending_metadata_rm.insert(id);\n  pending_metadata.erase(id);\n\n  return 0;\n}\n\nint32_t OSDMonitor::_allocate_osd_id(int32_t* existing_id)\n{\n  assert(existing_id);\n  *existing_id = -1;\n\n  for (int32_t i = 0; i < osdmap.get_max_osd(); ++i) {\n    if (!osdmap.exists(i) &&\n        pending_inc.new_up_client.count(i) == 0 &&\n        (pending_inc.new_state.count(i) == 0 ||\n         (pending_inc.new_state[i] & CEPH_OSD_EXISTS) == 0)) {\n      *existing_id = i;\n      return -1;\n    }\n  }\n\n  if (pending_inc.new_max_osd < 0) {\n    return osdmap.get_max_osd();\n  }\n  return pending_inc.new_max_osd;\n}\n\nvoid OSDMonitor::do_osd_create(\n    const int32_t id,\n    const uuid_d& uuid,\n    const string& device_class,\n    int32_t* new_id)\n{\n  dout(10) << __func__ << \" uuid \" << uuid << dendl;\n  assert(new_id);\n\n  // We presume validation has been performed prior to calling this\n  // function. We assert with prejudice.\n\n  int32_t allocated_id = -1; // declare here so we can jump\n  int32_t existing_id = -1;\n  if (!uuid.is_zero()) {\n    existing_id = osdmap.identify_osd(uuid);\n    if (existing_id >= 0) {\n      assert(id < 0 || id == existing_id);\n      *new_id = existing_id;\n      goto out;\n    } else if (id >= 0) {\n      // uuid does not exist, and id has been provided, so just create\n      // the new osd.id\n      *new_id = id;\n      goto out;\n    }\n  }\n\n  // allocate a new id\n  allocated_id = _allocate_osd_id(&existing_id);\n  dout(10) << __func__ << \" allocated id \" << allocated_id\n           << \" existing id \" << existing_id << dendl;\n  if (existing_id >= 0) {\n    assert(existing_id < osdmap.get_max_osd());\n    assert(allocated_id < 0);\n    pending_inc.new_weight[existing_id] = CEPH_OSD_OUT;\n    *new_id = existing_id;\n  } else if (allocated_id >= 0) {\n    assert(existing_id < 0);\n    // raise max_osd\n    if (pending_inc.new_max_osd < 0) {\n      pending_inc.new_max_osd = osdmap.get_max_osd() + 1;\n    } else {\n      ++pending_inc.new_max_osd;\n    }\n    *new_id = pending_inc.new_max_osd - 1;\n    assert(*new_id == allocated_id);\n  } else {\n    assert(0 == \"unexpected condition\");\n  }\n\nout:\n  if (device_class.size()) {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    if (newcrush.get_max_devices() < *new_id + 1) {\n      newcrush.set_max_devices(*new_id + 1);\n    }\n    string name = string(\"osd.\") + stringify(*new_id);\n    if (!newcrush.item_exists(*new_id)) {\n      newcrush.set_item_name(*new_id, name);\n    }\n    ostringstream ss;\n    int r = newcrush.update_device_class(*new_id, device_class, name, &ss);\n    if (r < 0) {\n      derr << __func__ << \" failed to set \" << name << \" device_class \"\n\t   << device_class << \": \" << cpp_strerror(r) << \" - \" << ss.str()\n\t   << dendl;\n      // non-fatal... this might be a replay and we want to be idempotent.\n    } else {\n      dout(20) << __func__ << \" set \" << name << \" device_class \" << device_class\n\t       << dendl;\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    }\n  } else {\n    dout(20) << __func__ << \" no device_class\" << dendl;\n  }\n\n  dout(10) << __func__ << \" using id \" << *new_id << dendl;\n  if (osdmap.get_max_osd() <= *new_id && pending_inc.new_max_osd <= *new_id) {\n    pending_inc.new_max_osd = *new_id + 1;\n  }\n\n  pending_inc.new_state[*new_id] |= CEPH_OSD_EXISTS | CEPH_OSD_NEW;\n  if (!uuid.is_zero())\n    pending_inc.new_uuid[*new_id] = uuid;\n}\n\nint OSDMonitor::validate_osd_create(\n    const int32_t id,\n    const uuid_d& uuid,\n    const bool check_osd_exists,\n    int32_t* existing_id,\n    stringstream& ss)\n{\n\n  dout(10) << __func__ << \" id \" << id << \" uuid \" << uuid\n           << \" check_osd_exists \" << check_osd_exists << dendl;\n\n  assert(existing_id);\n\n  if (id < 0 && uuid.is_zero()) {\n    // we have nothing to validate\n    *existing_id = -1;\n    return 0;\n  } else if (uuid.is_zero()) {\n    // we have an id but we will ignore it - because that's what\n    // `osd create` does.\n    return 0;\n  }\n\n  /*\n   * This function will be used to validate whether we are able to\n   * create a new osd when the `uuid` is specified.\n   *\n   * It will be used by both `osd create` and `osd new`, as the checks\n   * are basically the same when it pertains to osd id and uuid validation.\n   * However, `osd create` presumes an `uuid` is optional, for legacy\n   * reasons, while `osd new` requires the `uuid` to be provided. This\n   * means that `osd create` will not be idempotent if an `uuid` is not\n   * provided, but we will always guarantee the idempotency of `osd new`.\n   */\n\n  assert(!uuid.is_zero());\n  if (pending_inc.identify_osd(uuid) >= 0) {\n    // osd is about to exist\n    return -EAGAIN;\n  }\n\n  int32_t i = osdmap.identify_osd(uuid);\n  if (i >= 0) {\n    // osd already exists\n    if (id >= 0 && i != id) {\n      ss << \"uuid \" << uuid << \" already in use for different id \" << i;\n      return -EEXIST;\n    }\n    // return a positive errno to distinguish between a blocking error\n    // and an error we consider to not be a problem (i.e., this would be\n    // an idempotent operation).\n    *existing_id = i;\n    return EEXIST;\n  }\n  // i < 0\n  if (id >= 0) {\n    if (pending_inc.new_state.count(id)) {\n      // osd is about to exist\n      return -EAGAIN;\n    }\n    // we may not care if an osd exists if we are recreating a previously\n    // destroyed osd.\n    if (check_osd_exists && osdmap.exists(id)) {\n      ss << \"id \" << id << \" already in use and does not match uuid \"\n         << uuid;\n      return -EINVAL;\n    }\n  }\n  return 0;\n}\n\nint OSDMonitor::prepare_command_osd_create(\n    const int32_t id,\n    const uuid_d& uuid,\n    int32_t* existing_id,\n    stringstream& ss)\n{\n  dout(10) << __func__ << \" id \" << id << \" uuid \" << uuid << dendl;\n  assert(existing_id);\n  if (osdmap.is_destroyed(id)) {\n    ss << \"ceph osd create has been deprecated. Please use ceph osd new \"\n          \"instead.\";\n    return -EINVAL;\n  }\n\n  if (uuid.is_zero()) {\n    dout(10) << __func__ << \" no uuid; assuming legacy `osd create`\" << dendl;\n  }\n\n  return validate_osd_create(id, uuid, true, existing_id, ss);\n}\n\nint OSDMonitor::prepare_command_osd_new(\n    MonOpRequestRef op,\n    const cmdmap_t& cmdmap,\n    const map<string,string>& params,\n    stringstream &ss,\n    Formatter *f)\n{\n  uuid_d uuid;\n  string uuidstr;\n  int64_t id = -1;\n\n  assert(paxos->is_plugged());\n\n  dout(10) << __func__ << \" \" << op << dendl;\n\n  /* validate command. abort now if something's wrong. */\n\n  /* `osd new` will expect a `uuid` to be supplied; `id` is optional.\n   *\n   * If `id` is not specified, we will identify any existing osd based\n   * on `uuid`. Operation will be idempotent iff secrets match.\n   *\n   * If `id` is specified, we will identify any existing osd based on\n   * `uuid` and match against `id`. If they match, operation will be\n   * idempotent iff secrets match.\n   *\n   * `-i secrets.json` will be optional. If supplied, will be used\n   * to check for idempotency when `id` and `uuid` match.\n   *\n   * If `id` is not specified, and `uuid` does not exist, an id will\n   * be found or allocated for the osd.\n   *\n   * If `id` is specified, and the osd has been previously marked\n   * as destroyed, then the `id` will be reused.\n   */\n  if (!cmd_getval(cct, cmdmap, \"uuid\", uuidstr)) {\n    ss << \"requires the OSD's UUID to be specified.\";\n    return -EINVAL;\n  } else if (!uuid.parse(uuidstr.c_str())) {\n    ss << \"invalid UUID value '\" << uuidstr << \"'.\";\n    return -EINVAL;\n  }\n\n  if (cmd_getval(cct, cmdmap, \"id\", id) &&\n      (id < 0)) {\n    ss << \"invalid OSD id; must be greater or equal than zero.\";\n    return -EINVAL;\n  }\n\n  // are we running an `osd create`-like command, or recreating\n  // a previously destroyed osd?\n\n  bool is_recreate_destroyed = (id >= 0 && osdmap.is_destroyed(id));\n\n  // we will care about `id` to assess whether osd is `destroyed`, or\n  // to create a new osd.\n  // we will need an `id` by the time we reach auth.\n\n  int32_t existing_id = -1;\n  int err = validate_osd_create(id, uuid, !is_recreate_destroyed,\n                                &existing_id, ss);\n\n  bool may_be_idempotent = false;\n  if (err == EEXIST) {\n    // this is idempotent from the osdmon's point-of-view\n    may_be_idempotent = true;\n    assert(existing_id >= 0);\n    id = existing_id;\n  } else if (err < 0) {\n    return err;\n  }\n\n  if (!may_be_idempotent) {\n    // idempotency is out of the window. We are either creating a new\n    // osd or recreating a destroyed osd.\n    //\n    // We now need to figure out if we have an `id` (and if it's valid),\n    // of find an `id` if we don't have one.\n\n    // NOTE: we need to consider the case where the `id` is specified for\n    // `osd create`, and we must honor it. So this means checking if\n    // the `id` is destroyed, and if so assume the destroy; otherwise,\n    // check if it `exists` - in which case we complain about not being\n    // `destroyed`. In the end, if nothing fails, we must allow the\n    // creation, so that we are compatible with `create`.\n    if (id >= 0 && osdmap.exists(id) && !osdmap.is_destroyed(id)) {\n      dout(10) << __func__ << \" osd.\" << id << \" isn't destroyed\" << dendl;\n      ss << \"OSD \" << id << \" has not yet been destroyed\";\n      return -EINVAL;\n    } else if (id < 0) {\n      // find an `id`\n      id = _allocate_osd_id(&existing_id);\n      if (id < 0) {\n        assert(existing_id >= 0);\n        id = existing_id;\n      }\n      dout(10) << __func__ << \" found id \" << id << \" to use\" << dendl;\n    } else if (id >= 0 && osdmap.is_destroyed(id)) {\n      dout(10) << __func__ << \" recreating osd.\" << id << dendl;\n    } else {\n      dout(10) << __func__ << \" creating new osd.\" << id << dendl;\n    }\n  } else {\n    assert(id >= 0);\n    assert(osdmap.exists(id));\n  }\n\n  // we are now able to either create a brand new osd or reuse an existing\n  // osd that has been previously destroyed.\n\n  dout(10) << __func__ << \" id \" << id << \" uuid \" << uuid << dendl;\n\n  if (may_be_idempotent && params.empty()) {\n    // nothing to do, really.\n    dout(10) << __func__ << \" idempotent and no params -- no op.\" << dendl;\n    assert(id >= 0);\n    if (f) {\n      f->open_object_section(\"created_osd\");\n      f->dump_int(\"osdid\", id);\n      f->close_section();\n    } else {\n      ss << id;\n    }\n    return EEXIST;\n  }\n\n  string device_class;\n  auto p = params.find(\"crush_device_class\");\n  if (p != params.end()) {\n    device_class = p->second;\n    dout(20) << __func__ << \" device_class will be \" << device_class << dendl;\n  }\n  string cephx_secret, lockbox_secret, dmcrypt_key;\n  bool has_lockbox = false;\n  bool has_secrets = params.count(\"cephx_secret\")\n    || params.count(\"cephx_lockbox_secret\")\n    || params.count(\"dmcrypt_key\");\n\n  ConfigKeyService *svc = nullptr;\n  AuthMonitor::auth_entity_t cephx_entity, lockbox_entity;\n\n  if (has_secrets) {\n    if (params.count(\"cephx_secret\") == 0) {\n      ss << \"requires a cephx secret.\";\n      return -EINVAL;\n    }\n    cephx_secret = params.at(\"cephx_secret\");\n\n    bool has_lockbox_secret = (params.count(\"cephx_lockbox_secret\") > 0);\n    bool has_dmcrypt_key = (params.count(\"dmcrypt_key\") > 0);\n\n    dout(10) << __func__ << \" has lockbox \" << has_lockbox_secret\n             << \" dmcrypt \" << has_dmcrypt_key << dendl;\n\n    if (has_lockbox_secret && has_dmcrypt_key) {\n      has_lockbox = true;\n      lockbox_secret = params.at(\"cephx_lockbox_secret\");\n      dmcrypt_key = params.at(\"dmcrypt_key\");\n    } else if (!has_lockbox_secret != !has_dmcrypt_key) {\n      ss << \"requires both a cephx lockbox secret and a dm-crypt key.\";\n      return -EINVAL;\n    }\n\n    dout(10) << __func__ << \" validate secrets using osd id \" << id << dendl;\n\n    err = mon->authmon()->validate_osd_new(id, uuid,\n        cephx_secret,\n        lockbox_secret,\n        cephx_entity,\n        lockbox_entity,\n        ss);\n    if (err < 0) {\n      return err;\n    } else if (may_be_idempotent && err != EEXIST) {\n      // for this to be idempotent, `id` should already be >= 0; no need\n      // to use validate_id.\n      assert(id >= 0);\n      ss << \"osd.\" << id << \" exists but secrets do not match\";\n      return -EEXIST;\n    }\n\n    if (has_lockbox) {\n      svc = (ConfigKeyService*)mon->config_key_service;\n      err = svc->validate_osd_new(uuid, dmcrypt_key, ss);\n      if (err < 0) {\n        return err;\n      } else if (may_be_idempotent && err != EEXIST) {\n        assert(id >= 0);\n        ss << \"osd.\" << id << \" exists but dm-crypt key does not match.\";\n        return -EEXIST;\n      }\n    }\n  }\n  assert(!has_secrets || !cephx_secret.empty());\n  assert(!has_lockbox || !lockbox_secret.empty());\n\n  if (may_be_idempotent) {\n    // we have nothing to do for either the osdmon or the authmon,\n    // and we have no lockbox - so the config key service will not be\n    // touched. This is therefore an idempotent operation, and we can\n    // just return right away.\n    dout(10) << __func__ << \" idempotent -- no op.\" << dendl;\n    assert(id >= 0);\n    if (f) {\n      f->open_object_section(\"created_osd\");\n      f->dump_int(\"osdid\", id);\n      f->close_section();\n    } else {\n      ss << id;\n    }\n    return EEXIST;\n  }\n  assert(!may_be_idempotent);\n\n  // perform updates.\n  if (has_secrets) {\n    assert(!cephx_secret.empty());\n    assert((lockbox_secret.empty() && dmcrypt_key.empty()) ||\n           (!lockbox_secret.empty() && !dmcrypt_key.empty()));\n\n    err = mon->authmon()->do_osd_new(cephx_entity,\n        lockbox_entity,\n        has_lockbox);\n    assert(0 == err);\n\n    if (has_lockbox) {\n      assert(nullptr != svc);\n      svc->do_osd_new(uuid, dmcrypt_key);\n    }\n  }\n\n  if (is_recreate_destroyed) {\n    assert(id >= 0);\n    assert(osdmap.is_destroyed(id));\n    pending_inc.new_weight[id] = CEPH_OSD_OUT;\n    pending_inc.new_state[id] |= CEPH_OSD_DESTROYED;\n    if ((osdmap.get_state(id) & CEPH_OSD_NEW) == 0) {\n      pending_inc.new_state[id] |= CEPH_OSD_NEW;\n    }\n    if (osdmap.get_state(id) & CEPH_OSD_UP) {\n      // due to http://tracker.ceph.com/issues/20751 some clusters may\n      // have UP set for non-existent OSDs; make sure it is cleared\n      // for a newly created osd.\n      pending_inc.new_state[id] |= CEPH_OSD_UP;\n    }\n    pending_inc.new_uuid[id] = uuid;\n  } else {\n    assert(id >= 0);\n    int32_t new_id = -1;\n    do_osd_create(id, uuid, device_class, &new_id);\n    assert(new_id >= 0);\n    assert(id == new_id);\n  }\n\n  if (f) {\n    f->open_object_section(\"created_osd\");\n    f->dump_int(\"osdid\", id);\n    f->close_section();\n  } else {\n    ss << id;\n  }\n\n  return 0;\n}\n\nbool OSDMonitor::prepare_command(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MMonCommand *m = static_cast<MMonCommand*>(op->get_req());\n  stringstream ss;\n  cmdmap_t cmdmap;\n  if (!cmdmap_from_json(m->cmd, &cmdmap, ss)) {\n    string rs = ss.str();\n    mon->reply_command(op, -EINVAL, rs, get_last_committed());\n    return true;\n  }\n\n  MonSession *session = m->get_session();\n  if (!session) {\n    mon->reply_command(op, -EACCES, \"access denied\", get_last_committed());\n    return true;\n  }\n\n  return prepare_command_impl(op, cmdmap);\n}\n\nstatic int parse_reweights(CephContext *cct,\n\t\t\t   const cmdmap_t& cmdmap,\n\t\t\t   const OSDMap& osdmap,\n\t\t\t   map<int32_t, uint32_t>* weights)\n{\n  string weights_str;\n  if (!cmd_getval(cct, cmdmap, \"weights\", weights_str)) {\n    return -EINVAL;\n  }\n  std::replace(begin(weights_str), end(weights_str), '\\'', '\"');\n  json_spirit::mValue json_value;\n  if (!json_spirit::read(weights_str, json_value)) {\n    return -EINVAL;\n  }\n  if (json_value.type() != json_spirit::obj_type) {\n    return -EINVAL;\n  }\n  const auto obj = json_value.get_obj();\n  try {\n    for (auto& osd_weight : obj) {\n      auto osd_id = std::stoi(osd_weight.first);\n      if (!osdmap.exists(osd_id)) {\n\treturn -ENOENT;\n      }\n      if (osd_weight.second.type() != json_spirit::str_type) {\n\treturn -EINVAL;\n      }\n      auto weight = std::stoul(osd_weight.second.get_str());\n      weights->insert({osd_id, weight});\n    }\n  } catch (const std::logic_error& e) {\n    return -EINVAL;\n  }\n  return 0;\n}\n\nint OSDMonitor::prepare_command_osd_destroy(\n    int32_t id,\n    stringstream& ss)\n{\n  assert(paxos->is_plugged());\n\n  // we check if the osd exists for the benefit of `osd purge`, which may\n  // have previously removed the osd. If the osd does not exist, return\n  // -ENOENT to convey this, and let the caller deal with it.\n  //\n  // we presume that all auth secrets and config keys were removed prior\n  // to this command being called. if they exist by now, we also assume\n  // they must have been created by some other command and do not pertain\n  // to this non-existent osd.\n  if (!osdmap.exists(id)) {\n    dout(10) << __func__ << \" osd.\" << id << \" does not exist.\" << dendl;\n    return -ENOENT;\n  }\n\n  uuid_d uuid = osdmap.get_uuid(id);\n  dout(10) << __func__ << \" destroying osd.\" << id\n           << \" uuid \" << uuid << dendl;\n\n  // if it has been destroyed, we assume our work here is done.\n  if (osdmap.is_destroyed(id)) {\n    ss << \"destroyed osd.\" << id;\n    return 0;\n  }\n\n  EntityName cephx_entity, lockbox_entity;\n  bool idempotent_auth = false, idempotent_cks = false;\n\n  int err = mon->authmon()->validate_osd_destroy(id, uuid,\n                                                 cephx_entity,\n                                                 lockbox_entity,\n                                                 ss);\n  if (err < 0) {\n    if (err == -ENOENT) {\n      idempotent_auth = true;\n    } else {\n      return err;\n    }\n  }\n\n  ConfigKeyService *svc = (ConfigKeyService*)mon->config_key_service;\n  err = svc->validate_osd_destroy(id, uuid);\n  if (err < 0) {\n    assert(err == -ENOENT);\n    err = 0;\n    idempotent_cks = true;\n  }\n\n  if (!idempotent_auth) {\n    err = mon->authmon()->do_osd_destroy(cephx_entity, lockbox_entity);\n    assert(0 == err);\n  }\n\n  if (!idempotent_cks) {\n    svc->do_osd_destroy(id, uuid);\n  }\n\n  pending_inc.new_state[id] = CEPH_OSD_DESTROYED;\n  pending_inc.new_uuid[id] = uuid_d();\n\n  // we can only propose_pending() once per service, otherwise we'll be\n  // defying PaxosService and all laws of nature. Therefore, as we may\n  // be used during 'osd purge', let's keep the caller responsible for\n  // proposing.\n  assert(err == 0);\n  return 0;\n}\n\nint OSDMonitor::prepare_command_osd_purge(\n    int32_t id,\n    stringstream& ss)\n{\n  assert(paxos->is_plugged());\n  dout(10) << __func__ << \" purging osd.\" << id << dendl;\n\n  assert(!osdmap.is_up(id));\n\n  /*\n   * This may look a bit weird, but this is what's going to happen:\n   *\n   *  1. we make sure that removing from crush works\n   *  2. we call `prepare_command_osd_destroy()`. If it returns an\n   *     error, then we abort the whole operation, as no updates\n   *     have been made. However, we this function will have\n   *     side-effects, thus we need to make sure that all operations\n   *     performed henceforth will *always* succeed.\n   *  3. we call `prepare_command_osd_remove()`. Although this\n   *     function can return an error, it currently only checks if the\n   *     osd is up - and we have made sure that it is not so, so there\n   *     is no conflict, and it is effectively an update.\n   *  4. finally, we call `do_osd_crush_remove()`, which will perform\n   *     the crush update we delayed from before.\n   */\n\n  CrushWrapper newcrush;\n  _get_pending_crush(newcrush);\n\n  bool may_be_idempotent = false;\n\n  int err = _prepare_command_osd_crush_remove(newcrush, id, 0, false, false);\n  if (err == -ENOENT) {\n    err = 0;\n    may_be_idempotent = true;\n  } else if (err < 0) {\n    ss << \"error removing osd.\" << id << \" from crush\";\n    return err;\n  }\n\n  // no point destroying the osd again if it has already been marked destroyed\n  if (!osdmap.is_destroyed(id)) {\n    err = prepare_command_osd_destroy(id, ss);\n    if (err < 0) {\n      if (err == -ENOENT) {\n        err = 0;\n      } else {\n        return err;\n      }\n    } else {\n      may_be_idempotent = false;\n    }\n  }\n  assert(0 == err);\n\n  if (may_be_idempotent && !osdmap.exists(id)) {\n    dout(10) << __func__ << \" osd.\" << id << \" does not exist and \"\n             << \"we are idempotent.\" << dendl;\n    return -ENOENT;\n  }\n\n  err = prepare_command_osd_remove(id);\n  // we should not be busy, as we should have made sure this id is not up.\n  assert(0 == err);\n\n  do_osd_crush_remove(newcrush);\n  return 0;\n}\n\nbool OSDMonitor::prepare_command_impl(MonOpRequestRef op,\n\t\t\t\t      const cmdmap_t& cmdmap)\n{\n  op->mark_osdmon_event(__func__);\n  MMonCommand *m = static_cast<MMonCommand*>(op->get_req());\n  bool ret = false;\n  stringstream ss;\n  string rs;\n  bufferlist rdata;\n  int err = 0;\n\n  string format;\n  cmd_getval(cct, cmdmap, \"format\", format, string(\"plain\"));\n  boost::scoped_ptr<Formatter> f(Formatter::create(format));\n\n  string prefix;\n  cmd_getval(cct, cmdmap, \"prefix\", prefix);\n\n  int64_t osdid;\n  string osd_name;\n  bool osdid_present = false;\n  if (prefix != \"osd pg-temp\" &&\n      prefix != \"osd pg-upmap\" &&\n      prefix != \"osd pg-upmap-items\") {  // avoid commands with non-int id arg\n    osdid_present = cmd_getval(cct, cmdmap, \"id\", osdid);\n  }\n  if (osdid_present) {\n    ostringstream oss;\n    oss << \"osd.\" << osdid;\n    osd_name = oss.str();\n  }\n\n  // Even if there's a pending state with changes that could affect\n  // a command, considering that said state isn't yet committed, we\n  // just don't care about those changes if the command currently being\n  // handled acts as a no-op against the current committed state.\n  // In a nutshell, we assume this command  happens *before*.\n  //\n  // Let me make this clearer:\n  //\n  //   - If we have only one client, and that client issues some\n  //     operation that would conflict with this operation  but is\n  //     still on the pending state, then we would be sure that said\n  //     operation wouldn't have returned yet, so the client wouldn't\n  //     issue this operation (unless the client didn't wait for the\n  //     operation to finish, and that would be the client's own fault).\n  //\n  //   - If we have more than one client, each client will observe\n  //     whatever is the state at the moment of the commit.  So, if we\n  //     have two clients, one issuing an unlink and another issuing a\n  //     link, and if the link happens while the unlink is still on the\n  //     pending state, from the link's point-of-view this is a no-op.\n  //     If different clients are issuing conflicting operations and\n  //     they care about that, then the clients should make sure they\n  //     enforce some kind of concurrency mechanism -- from our\n  //     perspective that's what Douglas Adams would call an SEP.\n  //\n  // This should be used as a general guideline for most commands handled\n  // in this function.  Adapt as you see fit, but please bear in mind that\n  // this is the expected behavior.\n   \n \n  if (prefix == \"osd setcrushmap\" ||\n      (prefix == \"osd crush set\" && !osdid_present)) {\n    if (pending_inc.crush.length()) {\n      dout(10) << __func__ << \" waiting for pending crush update \" << dendl;\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    dout(10) << \"prepare_command setting new crush map\" << dendl;\n    bufferlist data(m->get_data());\n    CrushWrapper crush;\n    try {\n      auto bl = data.cbegin();\n      crush.decode(bl);\n    }\n    catch (const std::exception &e) {\n      err = -EINVAL;\n      ss << \"Failed to parse crushmap: \" << e.what();\n      goto reply;\n    }\n  \n    int64_t prior_version = 0;\n    if (cmd_getval(cct, cmdmap, \"prior_version\", prior_version)) {\n      if (prior_version == osdmap.get_crush_version() - 1) {\n\t// see if we are a resend of the last update.  this is imperfect\n\t// (multiple racing updaters may not both get reliable success)\n\t// but we expect crush updaters (via this interface) to be rare-ish.\n\tbufferlist current, proposed;\n\tosdmap.crush->encode(current, mon->get_quorum_con_features());\n\tcrush.encode(proposed, mon->get_quorum_con_features());\n\tif (current.contents_equal(proposed)) {\n\t  dout(10) << __func__\n\t\t   << \" proposed matches current and version equals previous\"\n\t\t   << dendl;\n\t  err = 0;\n\t  ss << osdmap.get_crush_version();\n\t  goto reply;\n\t}\n      }\n      if (prior_version != osdmap.get_crush_version()) {\n\terr = -EPERM;\n\tss << \"prior_version \" << prior_version << \" != crush version \"\n\t   << osdmap.get_crush_version();\n\tgoto reply;\n      }\n    }\n\n    if (crush.has_legacy_rule_ids()) {\n      err = -EINVAL;\n      ss << \"crush maps with ruleset != ruleid are no longer allowed\";\n      goto reply;\n    }\n    if (!validate_crush_against_features(&crush, ss)) {\n      err = -EINVAL;\n      goto reply;\n    }\n\n    err = osdmap.validate_crush_rules(&crush, &ss);\n    if (err < 0) {\n      goto reply;\n    }\n\n    if (g_conf->mon_osd_crush_smoke_test) {\n      // sanity check: test some inputs to make sure this map isn't\n      // totally broken\n      dout(10) << \" testing map\" << dendl;\n      stringstream ess;\n      CrushTester tester(crush, ess);\n      tester.set_min_x(0);\n      tester.set_max_x(50);\n      auto start = ceph::coarse_mono_clock::now();\n      int r = tester.test_with_fork(g_conf->mon_lease);\n      auto duration = ceph::coarse_mono_clock::now() - start;\n      if (r < 0) {\n\tdout(10) << \" tester.test_with_fork returns \" << r\n\t\t << \": \" << ess.str() << dendl;\n\tss << \"crush smoke test failed with \" << r << \": \" << ess.str();\n\terr = r;\n\tgoto reply;\n      }\n      dout(10) << __func__ << \" crush somke test duration: \"\n               << duration << \", result: \" << ess.str() << dendl;\n    }\n\n    pending_inc.crush = data;\n    ss << osdmap.get_crush_version() + 1;\n    goto update;\n\n  } else if (prefix == \"osd crush set-all-straw-buckets-to-straw2\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    for (int b = 0; b < newcrush.get_max_buckets(); ++b) {\n      int bid = -1 - b;\n      if (newcrush.bucket_exists(bid) &&\n\t  newcrush.get_bucket_alg(bid) == CRUSH_BUCKET_STRAW) {\n\tdout(20) << \" bucket \" << bid << \" is straw, can convert\" << dendl;\n\tnewcrush.bucket_set_alg(bid, CRUSH_BUCKET_STRAW2);\n      }\n    }\n    if (!validate_crush_against_features(&newcrush, ss)) {\n      err = -EINVAL;\n      goto reply;\n    }\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush set-device-class\") {\n    string device_class;\n    if (!cmd_getval(cct, cmdmap, \"class\", device_class)) {\n      err = -EINVAL; // no value!\n      goto reply;\n    }\n\n    bool stop = false;\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    set<int> updated;\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n      set<int> osds;\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n        osdmap.get_all_osds(osds);\n        stop = true;\n      } else {\n        // try traditional single osd way\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          // ss has reason for failure\n          ss << \", unable to parse osd id:\\\"\" << idvec[j] << \"\\\". \";\n          err = -EINVAL;\n          continue;\n        }\n        osds.insert(osd);\n      }\n\n      for (auto &osd : osds) {\n        if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        ostringstream oss;\n        oss << \"osd.\" << osd;\n        string name = oss.str();\n\n\tif (newcrush.get_max_devices() < osd + 1) {\n\t  newcrush.set_max_devices(osd + 1);\n\t}\n        string action;\n        if (newcrush.item_exists(osd)) {\n          action = \"updating\";\n        } else {\n          action = \"creating\";\n          newcrush.set_item_name(osd, name);\n        }\n\n        dout(5) << action << \" crush item id \" << osd << \" name '\" << name\n                << \"' device_class '\" << device_class << \"'\"\n                << dendl;\n        err = newcrush.update_device_class(osd, device_class, name, &ss);\n        if (err < 0) {\n          goto reply;\n        }\n        if (err == 0 && !_have_pending_crush()) {\n          if (!stop) {\n            // for single osd only, wildcard makes too much noise\n            ss << \"set-device-class item id \" << osd << \" name '\" << name\n               << \"' device_class '\" << device_class << \"': no change. \";\n          }\n        } else {\n          updated.insert(osd);\n        }\n      }\n    }\n\n    if (!updated.empty()) {\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n      ss << \"set osd(s) \" << updated << \" to class '\" << device_class << \"'\";\n      getline(ss, rs);\n      wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon,op, 0, rs, get_last_committed() + 1));\n      return true;\n    }\n\n } else if (prefix == \"osd crush rm-device-class\") {\n    bool stop = false;\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    set<int> updated;\n\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n      set<int> osds;\n\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n        osdmap.get_all_osds(osds);\n        stop = true;\n      } else {\n        // try traditional single osd way\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          // ss has reason for failure\n          ss << \", unable to parse osd id:\\\"\" << idvec[j] << \"\\\". \";\n          err = -EINVAL;\n          goto reply;\n        }\n        osds.insert(osd);\n      }\n\n      for (auto &osd : osds) {\n        if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        auto class_name = newcrush.get_item_class(osd);\n        if (!class_name) {\n          ss << \"osd.\" << osd << \" belongs to no class, \";\n          continue;\n        }\n        // note that we do not verify if class_is_in_use here\n        // in case the device is misclassified and user wants\n        // to overridely reset...\n\n        err = newcrush.remove_device_class(cct, osd, &ss);\n        if (err < 0) {\n          // ss has reason for failure\n          goto reply;\n        }\n        updated.insert(osd);\n      }\n    }\n\n    if (!updated.empty()) {\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n      ss << \"done removing class of osd(s): \" << updated;\n      getline(ss, rs);\n      wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon,op, 0, rs, get_last_committed() + 1));\n      return true;\n    }\n  } else if (prefix == \"osd crush class rename\") {\n    string srcname, dstname;\n    if (!cmd_getval(cct, cmdmap, \"srcname\", srcname)) {\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!cmd_getval(cct, cmdmap, \"dstname\", dstname)) {\n      err = -EINVAL;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    if (!newcrush.class_exists(srcname) && newcrush.class_exists(dstname)) {\n      // suppose this is a replay and return success\n      // so command is idempotent\n      ss << \"already renamed to '\" << dstname << \"'\";\n      err = 0;\n      goto reply;\n    }\n\n    err = newcrush.rename_class(srcname, dstname);\n    if (err < 0) {\n      ss << \"fail to rename '\" << srcname << \"' to '\" << dstname << \"' : \"\n         << cpp_strerror(err);\n      goto reply;\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"rename class '\" << srcname << \"' to '\" << dstname << \"'\";\n    goto update;\n  } else if (prefix == \"osd crush add-bucket\") {\n    // os crush add-bucket <name> <type>\n    string name, typestr;\n    vector<string> argvec;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    cmd_getval(cct, cmdmap, \"type\", typestr);\n    cmd_getval(cct, cmdmap, \"args\", argvec);\n    map<string,string> loc;\n    if (!argvec.empty()) {\n      CrushWrapper::parse_loc_map(argvec, &loc);\n      dout(0) << \"will create and move bucket '\" << name\n              << \"' to location \" << loc << dendl;\n    }\n\n    if (!_have_pending_crush() &&\n\t_get_stable_crush().name_exists(name)) {\n      ss << \"bucket '\" << name << \"' already exists\";\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (newcrush.name_exists(name)) {\n      ss << \"bucket '\" << name << \"' already exists\";\n      goto update;\n    }\n    int type = newcrush.get_type_id(typestr);\n    if (type < 0) {\n      ss << \"type '\" << typestr << \"' does not exist\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (type == 0) {\n      ss << \"type '\" << typestr << \"' is for devices, not buckets\";\n      err = -EINVAL;\n      goto reply;\n    }\n    int bucketno;\n    err = newcrush.add_bucket(0, 0,\n\t\t\t      CRUSH_HASH_DEFAULT, type, 0, NULL,\n\t\t\t      NULL, &bucketno);\n    if (err < 0) {\n      ss << \"add_bucket error: '\" << cpp_strerror(err) << \"'\";\n      goto reply;\n    }\n    err = newcrush.set_item_name(bucketno, name);\n    if (err < 0) {\n      ss << \"error setting bucket name to '\" << name << \"'\";\n      goto reply;\n    }\n\n    if (!loc.empty()) {\n      if (!newcrush.check_item_loc(cct, bucketno, loc,\n          (int *)NULL)) {\n        err = newcrush.move_bucket(cct, bucketno, loc);\n        if (err < 0) {\n          ss << \"error moving bucket '\" << name << \"' to location \" << loc;\n          goto reply;\n        }\n      } else {\n        ss << \"no need to move item id \" << bucketno << \" name '\" << name\n           << \"' to location \" << loc << \" in crush map\";\n      }\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    if (loc.empty()) {\n      ss << \"added bucket \" << name << \" type \" << typestr\n         << \" to crush map\";\n    } else {\n      ss << \"added bucket \" << name << \" type \" << typestr\n         << \" to location \" << loc;\n    }\n    goto update;\n  } else if (prefix == \"osd crush rename-bucket\") {\n    string srcname, dstname;\n    cmd_getval(cct, cmdmap, \"srcname\", srcname);\n    cmd_getval(cct, cmdmap, \"dstname\", dstname);\n\n    err = crush_rename_bucket(srcname, dstname, &ss);\n    if (err == -EALREADY) // equivalent to success for idempotency\n      err = 0;\n    if (err)\n      goto reply;\n    else\n      goto update;\n  } else if (prefix == \"osd crush weight-set create\" ||\n\t     prefix == \"osd crush weight-set create-compat\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    int64_t pool;\n    int positions;\n    if (newcrush.has_non_straw2_buckets()) {\n      ss << \"crush map contains one or more bucket(s) that are not straw2\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (prefix == \"osd crush weight-set create\") {\n      if (osdmap.require_min_compat_client > 0 &&\n\t  osdmap.require_min_compat_client < CEPH_RELEASE_LUMINOUS) {\n\tss << \"require_min_compat_client \"\n\t   << ceph_release_name(osdmap.require_min_compat_client)\n\t   << \" < luminous, which is required for per-pool weight-sets. \"\n           << \"Try 'ceph osd set-require-min-compat-client luminous' \"\n           << \"before using the new interface\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n      string poolname, mode;\n      cmd_getval(cct, cmdmap, \"pool\", poolname);\n      pool = osdmap.lookup_pg_pool_name(poolname.c_str());\n      if (pool < 0) {\n\tss << \"pool '\" << poolname << \"' not found\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n      cmd_getval(cct, cmdmap, \"mode\", mode);\n      if (mode != \"flat\" && mode != \"positional\") {\n\tss << \"unrecognized weight-set mode '\" << mode << \"'\";\n\terr = -EINVAL;\n\tgoto reply;\n      }\n      positions = mode == \"flat\" ? 1 : osdmap.get_pg_pool(pool)->get_size();\n    } else {\n      pool = CrushWrapper::DEFAULT_CHOOSE_ARGS;\n      positions = 1;\n    }\n    if (!newcrush.create_choose_args(pool, positions)) {\n      if (pool == CrushWrapper::DEFAULT_CHOOSE_ARGS) {\n        ss << \"compat weight-set already created\";\n      } else {\n        ss << \"weight-set for pool '\" << osdmap.get_pool_name(pool)\n           << \"' already created\";\n      }\n      goto reply;\n    }\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    goto update;\n\n  } else if (prefix == \"osd crush weight-set rm\" ||\n\t     prefix == \"osd crush weight-set rm-compat\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    int64_t pool;\n    if (prefix == \"osd crush weight-set rm\") {\n      string poolname;\n      cmd_getval(cct, cmdmap, \"pool\", poolname);\n      pool = osdmap.lookup_pg_pool_name(poolname.c_str());\n      if (pool < 0) {\n\tss << \"pool '\" << poolname << \"' not found\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n    } else {\n      pool = CrushWrapper::DEFAULT_CHOOSE_ARGS;\n    }\n    newcrush.rm_choose_args(pool);\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    goto update;\n\n  } else if (prefix == \"osd crush weight-set reweight\" ||\n\t     prefix == \"osd crush weight-set reweight-compat\") {\n    string poolname, item;\n    vector<double> weight;\n    cmd_getval(cct, cmdmap, \"pool\", poolname);\n    cmd_getval(cct, cmdmap, \"item\", item);\n    cmd_getval(cct, cmdmap, \"weight\", weight);\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    int64_t pool;\n    if (prefix == \"osd crush weight-set reweight\") {\n      pool = osdmap.lookup_pg_pool_name(poolname.c_str());\n      if (pool < 0) {\n\tss << \"pool '\" << poolname << \"' not found\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n      if (!newcrush.have_choose_args(pool)) {\n\tss << \"no weight-set for pool '\" << poolname << \"'\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n      auto arg_map = newcrush.choose_args_get(pool);\n      int positions = newcrush.get_choose_args_positions(arg_map);\n      if (weight.size() != (size_t)positions) {\n         ss << \"must specify exact \" << positions << \" weight values\";\n         err = -EINVAL;\n         goto reply;\n      }\n    } else {\n      pool = CrushWrapper::DEFAULT_CHOOSE_ARGS;\n      if (!newcrush.have_choose_args(pool)) {\n\tss << \"no backward-compatible weight-set\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n    }\n    if (!newcrush.name_exists(item)) {\n      ss << \"item '\" << item << \"' does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    err = newcrush.choose_args_adjust_item_weightf(\n      cct,\n      newcrush.choose_args_get(pool),\n      newcrush.get_item_id(item),\n      weight,\n      &ss);\n    if (err < 0) {\n      goto reply;\n    }\n    err = 0;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    goto update;\n  } else if (osdid_present &&\n\t     (prefix == \"osd crush set\" || prefix == \"osd crush add\")) {\n    // <OsdName> is 'osd.<id>' or '<id>', passed as int64_t id\n    // osd crush set <OsdName> <weight> <loc1> [<loc2> ...]\n    // osd crush add <OsdName> <weight> <loc1> [<loc2> ...]\n\n    if (!osdmap.exists(osdid)) {\n      err = -ENOENT;\n      ss << osd_name\n\t << \" does not exist. Create it before updating the crush map\";\n      goto reply;\n    }\n\n    double weight;\n    if (!cmd_getval(cct, cmdmap, \"weight\", weight)) {\n      ss << \"unable to parse weight value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    string args;\n    vector<string> argvec;\n    cmd_getval(cct, cmdmap, \"args\", argvec);\n    map<string,string> loc;\n    CrushWrapper::parse_loc_map(argvec, &loc);\n\n    if (prefix == \"osd crush set\"\n        && !_get_stable_crush().item_exists(osdid)) {\n      err = -ENOENT;\n      ss << \"unable to set item id \" << osdid << \" name '\" << osd_name\n         << \"' weight \" << weight << \" at location \" << loc\n         << \": does not exist\";\n      goto reply;\n    }\n\n    dout(5) << \"adding/updating crush item id \" << osdid << \" name '\"\n      << osd_name << \"' weight \" << weight << \" at location \"\n      << loc << dendl;\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    string action;\n    if (prefix == \"osd crush set\" ||\n        newcrush.check_item_loc(cct, osdid, loc, (int *)NULL)) {\n      action = \"set\";\n      err = newcrush.update_item(cct, osdid, weight, osd_name, loc);\n    } else {\n      action = \"add\";\n      err = newcrush.insert_item(cct, osdid, weight, osd_name, loc);\n      if (err == 0)\n        err = 1;\n    }\n\n    if (err < 0)\n      goto reply;\n\n    if (err == 0 && !_have_pending_crush()) {\n      ss << action << \" item id \" << osdid << \" name '\" << osd_name\n\t << \"' weight \" << weight << \" at location \" << loc << \": no change\";\n      goto reply;\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << action << \" item id \" << osdid << \" name '\" << osd_name << \"' weight \"\n       << weight << \" at location \" << loc << \" to crush map\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush create-or-move\") {\n    do {\n      // osd crush create-or-move <OsdName> <initial_weight> <loc1> [<loc2> ...]\n      if (!osdmap.exists(osdid)) {\n\terr = -ENOENT;\n\tss << osd_name\n\t   << \" does not exist.  create it before updating the crush map\";\n\tgoto reply;\n      }\n\n      double weight;\n      if (!cmd_getval(cct, cmdmap, \"weight\", weight)) {\n        ss << \"unable to parse weight value '\"\n           << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n        err = -EINVAL;\n        goto reply;\n      }\n\n      string args;\n      vector<string> argvec;\n      cmd_getval(cct, cmdmap, \"args\", argvec);\n      map<string,string> loc;\n      CrushWrapper::parse_loc_map(argvec, &loc);\n\n      dout(0) << \"create-or-move crush item name '\" << osd_name\n\t      << \"' initial_weight \" << weight << \" at location \" << loc\n\t      << dendl;\n\n      CrushWrapper newcrush;\n      _get_pending_crush(newcrush);\n\n      err = newcrush.create_or_move_item(cct, osdid, weight, osd_name, loc);\n      if (err == 0) {\n\tss << \"create-or-move updated item name '\" << osd_name\n\t   << \"' weight \" << weight\n\t   << \" at location \" << loc << \" to crush map\";\n\tbreak;\n      }\n      if (err > 0) {\n\tpending_inc.crush.clear();\n\tnewcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n\tss << \"create-or-move updating item name '\" << osd_name\n\t   << \"' weight \" << weight\n\t   << \" at location \" << loc << \" to crush map\";\n\tgetline(ss, rs);\n\twait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n\treturn true;\n      }\n    } while (false);\n\n  } else if (prefix == \"osd crush move\") {\n    do {\n      // osd crush move <name> <loc1> [<loc2> ...]\n      string name;\n      vector<string> argvec;\n      cmd_getval(cct, cmdmap, \"name\", name);\n      cmd_getval(cct, cmdmap, \"args\", argvec);\n      map<string,string> loc;\n      CrushWrapper::parse_loc_map(argvec, &loc);\n\n      dout(0) << \"moving crush item name '\" << name << \"' to location \" << loc << dendl;\n      CrushWrapper newcrush;\n      _get_pending_crush(newcrush);\n\n      if (!newcrush.name_exists(name)) {\n\terr = -ENOENT;\n\tss << \"item \" << name << \" does not exist\";\n\tbreak;\n      }\n      int id = newcrush.get_item_id(name);\n\n      if (!newcrush.check_item_loc(cct, id, loc, (int *)NULL)) {\n\tif (id >= 0) {\n\t  err = newcrush.create_or_move_item(cct, id, 0, name, loc);\n\t} else {\n\t  err = newcrush.move_bucket(cct, id, loc);\n\t}\n\tif (err >= 0) {\n\t  ss << \"moved item id \" << id << \" name '\" << name << \"' to location \" << loc << \" in crush map\";\n\t  pending_inc.crush.clear();\n\t  newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n\t  getline(ss, rs);\n\t  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t   get_last_committed() + 1));\n\t  return true;\n\t}\n      } else {\n\tss << \"no need to move item id \" << id << \" name '\" << name << \"' to location \" << loc << \" in crush map\";\n\terr = 0;\n      }\n    } while (false);\n  } else if (prefix == \"osd crush swap-bucket\") {\n    string source, dest, force;\n    cmd_getval(cct, cmdmap, \"source\", source);\n    cmd_getval(cct, cmdmap, \"dest\", dest);\n    cmd_getval(cct, cmdmap, \"force\", force);\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    if (!newcrush.name_exists(source)) {\n      ss << \"source item \" << source << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    if (!newcrush.name_exists(dest)) {\n      ss << \"dest item \" << dest << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    int sid = newcrush.get_item_id(source);\n    int did = newcrush.get_item_id(dest);\n    int sparent;\n    if (newcrush.get_immediate_parent_id(sid, &sparent) == 0 &&\n\tforce != \"--yes-i-really-mean-it\") {\n      ss << \"source item \" << source << \" is not an orphan bucket; pass --yes-i-really-mean-it to proceed anyway\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (newcrush.get_bucket_alg(sid) != newcrush.get_bucket_alg(did) &&\n\tforce != \"--yes-i-really-mean-it\") {\n      ss << \"source bucket alg \" << crush_alg_name(newcrush.get_bucket_alg(sid)) << \" != \"\n\t << \"dest bucket alg \" << crush_alg_name(newcrush.get_bucket_alg(did))\n\t << \"; pass --yes-i-really-mean-it to proceed anyway\";\n      err = -EPERM;\n      goto reply;\n    }\n    int r = newcrush.swap_bucket(cct, sid, did);\n    if (r < 0) {\n      ss << \"failed to swap bucket contents: \" << cpp_strerror(r);\n      err = r;\n      goto reply;\n    }\n    ss << \"swapped bucket of \" << source << \" to \" << dest;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    wait_for_finished_proposal(op,\n\t\t\t       new Monitor::C_Command(mon, op, err, ss.str(),\n\t\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush link\") {\n    // osd crush link <name> <loc1> [<loc2> ...]\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    vector<string> argvec;\n    cmd_getval(cct, cmdmap, \"args\", argvec);\n    map<string,string> loc;\n    CrushWrapper::parse_loc_map(argvec, &loc);\n\n    // Need an explicit check for name_exists because get_item_id returns\n    // 0 on unfound.\n    int id = osdmap.crush->get_item_id(name);\n    if (!osdmap.crush->name_exists(name)) {\n      err = -ENOENT;\n      ss << \"item \" << name << \" does not exist\";\n      goto reply;\n    } else {\n      dout(5) << \"resolved crush name '\" << name << \"' to id \" << id << dendl;\n    }\n    if (osdmap.crush->check_item_loc(cct, id, loc, (int*) NULL)) {\n      ss << \"no need to move item id \" << id << \" name '\" << name\n\t << \"' to location \" << loc << \" in crush map\";\n      err = 0;\n      goto reply;\n    }\n\n    dout(5) << \"linking crush item name '\" << name << \"' at location \" << loc << dendl;\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (!newcrush.name_exists(name)) {\n      err = -ENOENT;\n      ss << \"item \" << name << \" does not exist\";\n      goto reply;\n    } else {\n      int id = newcrush.get_item_id(name);\n      if (!newcrush.check_item_loc(cct, id, loc, (int *)NULL)) {\n\terr = newcrush.link_bucket(cct, id, loc);\n\tif (err >= 0) {\n\t  ss << \"linked item id \" << id << \" name '\" << name\n             << \"' to location \" << loc << \" in crush map\";\n\t  pending_inc.crush.clear();\n\t  newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n\t} else {\n\t  ss << \"cannot link item id \" << id << \" name '\" << name\n             << \"' to location \" << loc;\n          goto reply;\n\t}\n      } else {\n\tss << \"no need to move item id \" << id << \" name '\" << name\n           << \"' to location \" << loc << \" in crush map\";\n\terr = 0;\n      }\n    }\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, err, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush rm\" ||\n\t     prefix == \"osd crush remove\" ||\n\t     prefix == \"osd crush unlink\") {\n    do {\n      // osd crush rm <id> [ancestor]\n      CrushWrapper newcrush;\n      _get_pending_crush(newcrush);\n\n      string name;\n      cmd_getval(cct, cmdmap, \"name\", name);\n\n      if (!osdmap.crush->name_exists(name)) {\n\terr = 0;\n\tss << \"device '\" << name << \"' does not appear in the crush map\";\n\tbreak;\n      }\n      if (!newcrush.name_exists(name)) {\n\terr = 0;\n\tss << \"device '\" << name << \"' does not appear in the crush map\";\n\tgetline(ss, rs);\n\twait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n\treturn true;\n      }\n      int id = newcrush.get_item_id(name);\n      int ancestor = 0;\n\n      bool unlink_only = prefix == \"osd crush unlink\";\n      string ancestor_str;\n      if (cmd_getval(cct, cmdmap, \"ancestor\", ancestor_str)) {\n\tif (!newcrush.name_exists(ancestor_str)) {\n\t  err = -ENOENT;\n\t  ss << \"ancestor item '\" << ancestor_str\n\t     << \"' does not appear in the crush map\";\n\t  break;\n\t}\n        ancestor = newcrush.get_item_id(ancestor_str);\n      }\n\n      err = prepare_command_osd_crush_remove(\n          newcrush,\n          id, ancestor,\n          (ancestor < 0), unlink_only);\n\n      if (err == -ENOENT) {\n\tss << \"item \" << id << \" does not appear in that position\";\n\terr = 0;\n\tbreak;\n      }\n      if (err == 0) {\n\tss << \"removed item id \" << id << \" name '\" << name << \"' from crush map\";\n\tgetline(ss, rs);\n\twait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n\treturn true;\n      }\n    } while (false);\n\n  } else if (prefix == \"osd crush reweight-all\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    newcrush.reweight(cct);\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"reweighted crush hierarchy\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush reweight\") {\n    // osd crush reweight <name> <weight>\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    if (!newcrush.name_exists(name)) {\n      err = -ENOENT;\n      ss << \"device '\" << name << \"' does not appear in the crush map\";\n      goto reply;\n    }\n\n    int id = newcrush.get_item_id(name);\n    if (id < 0) {\n      ss << \"device '\" << name << \"' is not a leaf in the crush map\";\n      err = -EINVAL;\n      goto reply;\n    }\n    double w;\n    if (!cmd_getval(cct, cmdmap, \"weight\", w)) {\n      ss << \"unable to parse weight value '\"\n\t << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    err = newcrush.adjust_item_weightf(cct, id, w);\n    if (err < 0)\n      goto reply;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"reweighted item id \" << id << \" name '\" << name << \"' to \" << w\n       << \" in crush map\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush reweight-subtree\") {\n    // osd crush reweight <name> <weight>\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    if (!newcrush.name_exists(name)) {\n      err = -ENOENT;\n      ss << \"device '\" << name << \"' does not appear in the crush map\";\n      goto reply;\n    }\n\n    int id = newcrush.get_item_id(name);\n    if (id >= 0) {\n      ss << \"device '\" << name << \"' is not a subtree in the crush map\";\n      err = -EINVAL;\n      goto reply;\n    }\n    double w;\n    if (!cmd_getval(cct, cmdmap, \"weight\", w)) {\n      ss << \"unable to parse weight value '\"\n\t << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    err = newcrush.adjust_subtree_weightf(cct, id, w);\n    if (err < 0)\n      goto reply;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"reweighted subtree id \" << id << \" name '\" << name << \"' to \" << w\n       << \" in crush map\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush tunables\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    err = 0;\n    string profile;\n    cmd_getval(cct, cmdmap, \"profile\", profile);\n    if (profile == \"legacy\" || profile == \"argonaut\") {\n      newcrush.set_tunables_legacy();\n    } else if (profile == \"bobtail\") {\n      newcrush.set_tunables_bobtail();\n    } else if (profile == \"firefly\") {\n      newcrush.set_tunables_firefly();\n    } else if (profile == \"hammer\") {\n      newcrush.set_tunables_hammer();\n    } else if (profile == \"jewel\") {\n      newcrush.set_tunables_jewel();\n    } else if (profile == \"optimal\") {\n      newcrush.set_tunables_optimal();\n    } else if (profile == \"default\") {\n      newcrush.set_tunables_default();\n    } else {\n      ss << \"unrecognized profile '\" << profile << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    if (!validate_crush_against_features(&newcrush, ss)) {\n      err = -EINVAL;\n      goto reply;\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"adjusted tunables profile to \" << profile;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush set-tunable\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    err = 0;\n    string tunable;\n    cmd_getval(cct, cmdmap, \"tunable\", tunable);\n\n    int64_t value = -1;\n    if (!cmd_getval(cct, cmdmap, \"value\", value)) {\n      err = -EINVAL;\n      ss << \"failed to parse integer value \"\n\t << cmd_vartype_stringify(cmdmap.at(\"value\"));\n      goto reply;\n    }\n\n    if (tunable == \"straw_calc_version\") {\n      if (value != 0 && value != 1) {\n\tss << \"value must be 0 or 1; got \" << value;\n\terr = -EINVAL;\n\tgoto reply;\n      }\n      newcrush.set_straw_calc_version(value);\n    } else {\n      ss << \"unrecognized tunable '\" << tunable << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    if (!validate_crush_against_features(&newcrush, ss)) {\n      err = -EINVAL;\n      goto reply;\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"adjusted tunable \" << tunable << \" to \" << value;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule create-simple\") {\n    string name, root, type, mode;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    cmd_getval(cct, cmdmap, \"root\", root);\n    cmd_getval(cct, cmdmap, \"type\", type);\n    cmd_getval(cct, cmdmap, \"mode\", mode);\n    if (mode == \"\")\n      mode = \"firstn\";\n\n    if (osdmap.crush->rule_exists(name)) {\n      // The name is uniquely associated to a ruleid and the rule it contains\n      // From the user point of view, the rule is more meaningfull.\n      ss << \"rule \" << name << \" already exists\";\n      err = 0;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (newcrush.rule_exists(name)) {\n      // The name is uniquely associated to a ruleid and the rule it contains\n      // From the user point of view, the rule is more meaningfull.\n      ss << \"rule \" << name << \" already exists\";\n      err = 0;\n    } else {\n      int ruleno = newcrush.add_simple_rule(name, root, type, \"\", mode,\n\t\t\t\t\t       pg_pool_t::TYPE_REPLICATED, &ss);\n      if (ruleno < 0) {\n\terr = ruleno;\n\tgoto reply;\n      }\n\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule create-replicated\") {\n    string name, root, type, device_class;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    cmd_getval(cct, cmdmap, \"root\", root);\n    cmd_getval(cct, cmdmap, \"type\", type);\n    cmd_getval(cct, cmdmap, \"class\", device_class);\n\n    if (osdmap.crush->rule_exists(name)) {\n      // The name is uniquely associated to a ruleid and the rule it contains\n      // From the user point of view, the rule is more meaningfull.\n      ss << \"rule \" << name << \" already exists\";\n      err = 0;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (newcrush.rule_exists(name)) {\n      // The name is uniquely associated to a ruleid and the rule it contains\n      // From the user point of view, the rule is more meaningfull.\n      ss << \"rule \" << name << \" already exists\";\n      err = 0;\n    } else {\n      int ruleno = newcrush.add_simple_rule(\n\tname, root, type, device_class,\n\t\"firstn\", pg_pool_t::TYPE_REPLICATED, &ss);\n      if (ruleno < 0) {\n\terr = ruleno;\n\tgoto reply;\n      }\n\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd erasure-code-profile rm\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n\n    if (erasure_code_profile_in_use(pending_inc.new_pools, name, &ss))\n      goto wait;\n\n    if (erasure_code_profile_in_use(osdmap.pools, name, &ss)) {\n      err = -EBUSY;\n      goto reply;\n    }\n\n    if (osdmap.has_erasure_code_profile(name) ||\n\tpending_inc.new_erasure_code_profiles.count(name)) {\n      if (osdmap.has_erasure_code_profile(name)) {\n\tpending_inc.old_erasure_code_profiles.push_back(name);\n      } else {\n\tdout(20) << \"erasure code profile rm \" << name << \": creation canceled\" << dendl;\n\tpending_inc.new_erasure_code_profiles.erase(name);\n      }\n\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t\tget_last_committed() + 1));\n      return true;\n    } else {\n      ss << \"erasure-code-profile \" << name << \" does not exist\";\n      err = 0;\n      goto reply;\n    }\n\n  } else if (prefix == \"osd erasure-code-profile set\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    vector<string> profile;\n    cmd_getval(cct, cmdmap, \"profile\", profile);\n    bool force;\n    if (profile.size() > 0 && profile.back() == \"--force\") {\n      profile.pop_back();\n      force = true;\n    } else {\n      force = false;\n    }\n    map<string,string> profile_map;\n    err = parse_erasure_code_profile(profile, &profile_map, &ss);\n    if (err)\n      goto reply;\n    if (profile_map.find(\"plugin\") == profile_map.end()) {\n      ss << \"erasure-code-profile \" << profile_map\n\t << \" must contain a plugin entry\" << std::endl;\n      err = -EINVAL;\n      goto reply;\n    }\n    string plugin = profile_map[\"plugin\"];\n\n    if (pending_inc.has_erasure_code_profile(name)) {\n      dout(20) << \"erasure code profile \" << name << \" try again\" << dendl;\n      goto wait;\n    } else {\n      err = normalize_profile(name, profile_map, force, &ss);\n      if (err)\n\tgoto reply;\n\n      if (osdmap.has_erasure_code_profile(name)) {\n\tErasureCodeProfile existing_profile_map =\n\t  osdmap.get_erasure_code_profile(name);\n\terr = normalize_profile(name, existing_profile_map, force, &ss);\n\tif (err)\n\t  goto reply;\n\n\tif (existing_profile_map == profile_map) {\n\t  err = 0;\n\t  goto reply;\n\t}\n\tif (!force) {\n\t  err = -EPERM;\n\t  ss << \"will not override erasure code profile \" << name\n\t     << \" because the existing profile \"\n\t     << existing_profile_map\n\t     << \" is different from the proposed profile \"\n\t     << profile_map;\n\t  goto reply;\n\t}\n      }\n\n      dout(20) << \"erasure code profile set \" << name << \"=\"\n\t       << profile_map << dendl;\n      pending_inc.set_erasure_code_profile(name, profile_map);\n    }\n\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                                                      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule create-erasure\") {\n    err = check_cluster_features(CEPH_FEATURE_CRUSH_V2, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string name, poolstr;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    string profile;\n    cmd_getval(cct, cmdmap, \"profile\", profile);\n    if (profile == \"\")\n      profile = \"default\";\n    if (profile == \"default\") {\n      if (!osdmap.has_erasure_code_profile(profile)) {\n\tif (pending_inc.has_erasure_code_profile(profile)) {\n\t  dout(20) << \"erasure code profile \" << profile << \" already pending\" << dendl;\n\t  goto wait;\n\t}\n\n\tmap<string,string> profile_map;\n\terr = osdmap.get_erasure_code_profile_default(cct,\n\t\t\t\t\t\t      profile_map,\n\t\t\t\t\t\t      &ss);\n\tif (err)\n\t  goto reply;\n\terr = normalize_profile(name, profile_map, true, &ss);\n\tif (err)\n\t  goto reply;\n\tdout(20) << \"erasure code profile set \" << profile << \"=\"\n\t\t << profile_map << dendl;\n\tpending_inc.set_erasure_code_profile(profile, profile_map);\n\tgoto wait;\n      }\n    }\n\n    int rule;\n    err = crush_rule_create_erasure(name, profile, &rule, &ss);\n    if (err < 0) {\n      switch(err) {\n      case -EEXIST: // return immediately\n\tss << \"rule \" << name << \" already exists\";\n\terr = 0;\n\tgoto reply;\n\tbreak;\n      case -EALREADY: // wait for pending to be proposed\n\tss << \"rule \" << name << \" already exists\";\n\terr = 0;\n\tbreak;\n      default: // non recoverable error\n \tgoto reply;\n\tbreak;\n      }\n    } else {\n      ss << \"created rule \" << name << \" at \" << rule;\n    }\n\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                                                      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule rm\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n\n    if (!osdmap.crush->rule_exists(name)) {\n      ss << \"rule \" << name << \" does not exist\";\n      err = 0;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (!newcrush.rule_exists(name)) {\n      ss << \"rule \" << name << \" does not exist\";\n      err = 0;\n    } else {\n      int ruleno = newcrush.get_rule_id(name);\n      assert(ruleno >= 0);\n\n      // make sure it is not in use.\n      // FIXME: this is ok in some situations, but let's not bother with that\n      // complexity now.\n      int ruleset = newcrush.get_rule_mask_ruleset(ruleno);\n      if (osdmap.crush_rule_in_use(ruleset)) {\n\tss << \"crush ruleset \" << name << \" \" << ruleset << \" is in use\";\n\terr = -EBUSY;\n\tgoto reply;\n      }\n\n      err = newcrush.remove_rule(ruleno);\n      if (err < 0) {\n\tgoto reply;\n      }\n\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule rename\") {\n    string srcname;\n    string dstname;\n    cmd_getval(cct, cmdmap, \"srcname\", srcname);\n    cmd_getval(cct, cmdmap, \"dstname\", dstname);\n    if (srcname.empty() || dstname.empty()) {\n      ss << \"must specify both source rule name and destination rule name\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (srcname == dstname) {\n      ss << \"destination rule name is equal to source rule name\";\n      err = 0;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    if (!newcrush.rule_exists(srcname) && newcrush.rule_exists(dstname)) {\n      // srcname does not exist and dstname already exists\n      // suppose this is a replay and return success\n      // (so this command is idempotent)\n      ss << \"already renamed to '\" << dstname << \"'\";\n      err = 0;\n      goto reply;\n    }\n\n    err = newcrush.rename_rule(srcname, dstname, &ss);\n    if (err < 0) {\n      // ss has reason for failure\n      goto reply;\n    }\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                               get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd setmaxosd\") {\n    int64_t newmax;\n    if (!cmd_getval(cct, cmdmap, \"newmax\", newmax)) {\n      ss << \"unable to parse 'newmax' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"newmax\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    if (newmax > g_conf->mon_max_osd) {\n      err = -ERANGE;\n      ss << \"cannot set max_osd to \" << newmax << \" which is > conf.mon_max_osd (\"\n\t << g_conf->mon_max_osd << \")\";\n      goto reply;\n    }\n\n    // Don't allow shrinking OSD number as this will cause data loss\n    // and may cause kernel crashes.\n    // Note: setmaxosd sets the maximum OSD number and not the number of OSDs\n    if (newmax < osdmap.get_max_osd()) {\n      // Check if the OSDs exist between current max and new value.\n      // If there are any OSDs exist, then don't allow shrinking number\n      // of OSDs.\n      for (int i = newmax; i < osdmap.get_max_osd(); i++) {\n        if (osdmap.exists(i)) {\n          err = -EBUSY;\n          ss << \"cannot shrink max_osd to \" << newmax\n             << \" because osd.\" << i << \" (and possibly others) still in use\";\n          goto reply;\n        }\n      }\n    }\n\n    pending_inc.new_max_osd = newmax;\n    ss << \"set new max_osd = \" << pending_inc.new_max_osd;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd set-full-ratio\" ||\n\t     prefix == \"osd set-backfillfull-ratio\" ||\n             prefix == \"osd set-nearfull-ratio\") {\n    double n;\n    if (!cmd_getval(cct, cmdmap, \"ratio\", n)) {\n      ss << \"unable to parse 'ratio' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"ratio\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (prefix == \"osd set-full-ratio\")\n      pending_inc.new_full_ratio = n;\n    else if (prefix == \"osd set-backfillfull-ratio\")\n      pending_inc.new_backfillfull_ratio = n;\n    else if (prefix == \"osd set-nearfull-ratio\")\n      pending_inc.new_nearfull_ratio = n;\n    ss << prefix << \" \" << n;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd set-require-min-compat-client\") {\n    string v;\n    cmd_getval(cct, cmdmap, \"version\", v);\n    int vno = ceph_release_from_name(v.c_str());\n    if (vno <= 0) {\n      ss << \"version \" << v << \" is not recognized\";\n      err = -EINVAL;\n      goto reply;\n    }\n    OSDMap newmap;\n    newmap.deepish_copy_from(osdmap);\n    newmap.apply_incremental(pending_inc);\n    newmap.require_min_compat_client = vno;\n    auto mvno = newmap.get_min_compat_client();\n    if (vno < mvno) {\n      ss << \"osdmap current utilizes features that require \"\n\t << ceph_release_name(mvno)\n\t << \"; cannot set require_min_compat_client below that to \"\n\t << ceph_release_name(vno);\n      err = -EPERM;\n      goto reply;\n    }\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    if (sure != \"--yes-i-really-mean-it\") {\n      FeatureMap m;\n      mon->get_combined_feature_map(&m);\n      uint64_t features = ceph_release_features(vno);\n      bool first = true;\n      bool ok = true;\n      for (int type : {\n\t    CEPH_ENTITY_TYPE_CLIENT,\n\t    CEPH_ENTITY_TYPE_MDS,\n\t    CEPH_ENTITY_TYPE_MGR }) {\n\tauto p = m.m.find(type);\n\tif (p == m.m.end()) {\n\t  continue;\n\t}\n\tfor (auto& q : p->second) {\n\t  uint64_t missing = ~q.first & features;\n\t  if (missing) {\n\t    if (first) {\n\t      ss << \"cannot set require_min_compat_client to \" << v << \": \";\n\t    } else {\n\t      ss << \"; \";\n\t    }\n\t    first = false;\n\t    ss << q.second << \" connected \" << ceph_entity_type_name(type)\n\t       << \"(s) look like \" << ceph_release_name(\n\t\t ceph_release_from_features(q.first))\n\t       << \" (missing 0x\" << std::hex << missing << std::dec << \")\";\n\t    ok = false;\n\t  }\n\t}\n      }\n      if (!ok) {\n\tss << \"; add --yes-i-really-mean-it to do it anyway\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n    }\n    ss << \"set require_min_compat_client to \" << ceph_release_name(vno);\n    pending_inc.new_require_min_compat_client = vno;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t\t  get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pause\") {\n    return prepare_set_flag(op, CEPH_OSDMAP_PAUSERD | CEPH_OSDMAP_PAUSEWR);\n\n  } else if (prefix == \"osd unpause\") {\n    return prepare_unset_flag(op, CEPH_OSDMAP_PAUSERD | CEPH_OSDMAP_PAUSEWR);\n\n  } else if (prefix == \"osd set\") {\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n    if (key == \"full\")\n      return prepare_set_flag(op, CEPH_OSDMAP_FULL);\n    else if (key == \"pause\")\n      return prepare_set_flag(op, CEPH_OSDMAP_PAUSERD | CEPH_OSDMAP_PAUSEWR);\n    else if (key == \"noup\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOUP);\n    else if (key == \"nodown\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NODOWN);\n    else if (key == \"noout\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOOUT);\n    else if (key == \"noin\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOIN);\n    else if (key == \"nobackfill\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOBACKFILL);\n    else if (key == \"norebalance\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOREBALANCE);\n    else if (key == \"norecover\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NORECOVER);\n    else if (key == \"noscrub\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOSCRUB);\n    else if (key == \"nodeep-scrub\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NODEEP_SCRUB);\n    else if (key == \"notieragent\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOTIERAGENT);\n    else if (key == \"nosnaptrim\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOSNAPTRIM);\n    else if (key == \"sortbitwise\") {\n      return prepare_set_flag(op, CEPH_OSDMAP_SORTBITWISE);\n    } else if (key == \"recovery_deletes\") {\n      if (!osdmap.get_num_up_osds() && sure != \"--yes-i-really-mean-it\") {\n        ss << \"Not advisable to continue since no OSDs are up. Pass \"\n           << \"--yes-i-really-mean-it if you really wish to continue.\";\n        err = -EPERM;\n        goto reply;\n      }\n      if (HAVE_FEATURE(osdmap.get_up_osd_features(), OSD_RECOVERY_DELETES)\n          || sure == \"--yes-i-really-mean-it\") {\n\treturn prepare_set_flag(op, CEPH_OSDMAP_RECOVERY_DELETES);\n      } else {\n\tss << \"not all up OSDs have OSD_RECOVERY_DELETES feature\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n    } else if (key == \"require_jewel_osds\") {\n      if (!osdmap.get_num_up_osds() && sure != \"--yes-i-really-mean-it\") {\n        ss << \"Not advisable to continue since no OSDs are up. Pass \"\n           << \"--yes-i-really-mean-it if you really wish to continue.\";\n        err = -EPERM;\n        goto reply;\n      }\n      if (!osdmap.test_flag(CEPH_OSDMAP_SORTBITWISE)) {\n\tss << \"the sortbitwise flag must be set before require_jewel_osds\";\n\terr = -EPERM;\n\tgoto reply;\n      } else if (osdmap.require_osd_release >= CEPH_RELEASE_JEWEL) {\n\tss << \"require_osd_release is already >= jewel\";\n\terr = 0;\n\tgoto reply;\n      } else if (HAVE_FEATURE(osdmap.get_up_osd_features(), SERVER_JEWEL)\n                 || sure == \"--yes-i-really-mean-it\") {\n\treturn prepare_set_flag(op, CEPH_OSDMAP_REQUIRE_JEWEL);\n      } else {\n\tss << \"not all up OSDs have CEPH_FEATURE_SERVER_JEWEL feature\";\n\terr = -EPERM;\n      }\n    } else if (key == \"require_kraken_osds\") {\n      if (!osdmap.get_num_up_osds() && sure != \"--yes-i-really-mean-it\") {\n        ss << \"Not advisable to continue since no OSDs are up. Pass \"\n           << \"--yes-i-really-mean-it if you really wish to continue.\";\n        err = -EPERM;\n        goto reply;\n      }\n      if (!osdmap.test_flag(CEPH_OSDMAP_SORTBITWISE)) {\n\tss << \"the sortbitwise flag must be set before require_kraken_osds\";\n\terr = -EPERM;\n\tgoto reply;\n      } else if (osdmap.require_osd_release >= CEPH_RELEASE_KRAKEN) {\n\tss << \"require_osd_release is already >= kraken\";\n\terr = 0;\n\tgoto reply;\n      } else if (HAVE_FEATURE(osdmap.get_up_osd_features(), SERVER_KRAKEN)\n                 || sure == \"--yes-i-really-mean-it\") {\n\tbool r = prepare_set_flag(op, CEPH_OSDMAP_REQUIRE_KRAKEN);\n\t// ensure JEWEL is also set\n\tpending_inc.new_flags |= CEPH_OSDMAP_REQUIRE_JEWEL;\n\treturn r;\n      } else {\n\tss << \"not all up OSDs have CEPH_FEATURE_SERVER_KRAKEN feature\";\n\terr = -EPERM;\n      }\n    } else {\n      ss << \"unrecognized flag '\" << key << \"'\";\n      err = -EINVAL;\n    }\n\n  } else if (prefix == \"osd unset\") {\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n    if (key == \"full\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_FULL);\n    else if (key == \"pause\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_PAUSERD | CEPH_OSDMAP_PAUSEWR);\n    else if (key == \"noup\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOUP);\n    else if (key == \"nodown\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NODOWN);\n    else if (key == \"noout\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOOUT);\n    else if (key == \"noin\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOIN);\n    else if (key == \"nobackfill\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOBACKFILL);\n    else if (key == \"norebalance\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOREBALANCE);\n    else if (key == \"norecover\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NORECOVER);\n    else if (key == \"noscrub\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOSCRUB);\n    else if (key == \"nodeep-scrub\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NODEEP_SCRUB);\n    else if (key == \"notieragent\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOTIERAGENT);\n    else if (key == \"nosnaptrim\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOSNAPTRIM);\n    else {\n      ss << \"unrecognized flag '\" << key << \"'\";\n      err = -EINVAL;\n    }\n\n  } else if (prefix == \"osd require-osd-release\") {\n    string release;\n    cmd_getval(cct, cmdmap, \"release\", release);\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    if (!osdmap.test_flag(CEPH_OSDMAP_SORTBITWISE)) {\n      ss << \"the sortbitwise flag must be set first\";\n      err = -EPERM;\n      goto reply;\n    }\n    int rel = ceph_release_from_name(release.c_str());\n    if (rel <= 0) {\n      ss << \"unrecognized release \" << release;\n      err = -EINVAL;\n      goto reply;\n    }\n    if (rel < CEPH_RELEASE_LUMINOUS) {\n      ss << \"use this command only for luminous and later\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (rel == osdmap.require_osd_release) {\n      // idempotent\n      err = 0;\n      goto reply;\n    }\n    assert(osdmap.require_osd_release >= CEPH_RELEASE_LUMINOUS);\n    if (!osdmap.get_num_up_osds() && sure != \"--yes-i-really-mean-it\") {\n      ss << \"Not advisable to continue since no OSDs are up. Pass \"\n\t << \"--yes-i-really-mean-it if you really wish to continue.\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (rel == CEPH_RELEASE_MIMIC) {\n      if ((!HAVE_FEATURE(osdmap.get_up_osd_features(), SERVER_MIMIC))\n           && sure != \"--yes-i-really-mean-it\") {\n\tss << \"not all up OSDs have CEPH_FEATURE_SERVER_MIMIC feature\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n    } else if (rel == CEPH_RELEASE_NAUTILUS) {\n      if ((!HAVE_FEATURE(osdmap.get_up_osd_features(), SERVER_NAUTILUS))\n           && sure != \"--yes-i-really-mean-it\") {\n\tss << \"not all up OSDs have CEPH_FEATURE_SERVER_NAUTILUS feature\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n    } else {\n      ss << \"not supported for this release yet\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (rel < osdmap.require_osd_release) {\n      ss << \"require_osd_release cannot be lowered once it has been set\";\n      err = -EPERM;\n      goto reply;\n    }\n    pending_inc.new_require_osd_release = rel;\n    goto update;\n  } else if (prefix == \"osd cluster_snap\") {\n    // ** DISABLE THIS FOR NOW **\n    ss << \"cluster snapshot currently disabled (broken implementation)\";\n    // ** DISABLE THIS FOR NOW **\n\n  } else if (prefix == \"osd down\" ||\n\t     prefix == \"osd out\" ||\n\t     prefix == \"osd in\" ||\n\t     prefix == \"osd rm\") {\n\n    bool any = false;\n    bool stop = false;\n    bool verbose = true;\n\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n      set<int> osds;\n\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n        if (prefix == \"osd in\") {\n          // touch out osds only\n          osdmap.get_out_osds(osds);\n        } else {\n          osdmap.get_all_osds(osds);\n        }\n        stop = true;\n        verbose = false; // so the output is less noisy.\n      } else {\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          ss << \"invalid osd id\" << osd;\n          err = -EINVAL;\n          continue;\n        } else if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        osds.insert(osd);\n      }\n\n      for (auto &osd : osds) {\n        if (prefix == \"osd down\") {\n\t  if (osdmap.is_down(osd)) {\n            if (verbose)\n\t      ss << \"osd.\" << osd << \" is already down. \";\n\t  } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_UP);\n\t    ss << \"marked down osd.\" << osd << \". \";\n\t    any = true;\n\t  }\n        } else if (prefix == \"osd out\") {\n\t  if (osdmap.is_out(osd)) {\n            if (verbose)\n\t      ss << \"osd.\" << osd << \" is already out. \";\n\t  } else {\n\t    pending_inc.new_weight[osd] = CEPH_OSD_OUT;\n\t    if (osdmap.osd_weight[osd]) {\n\t      if (pending_inc.new_xinfo.count(osd) == 0) {\n\t        pending_inc.new_xinfo[osd] = osdmap.osd_xinfo[osd];\n\t      }\n\t      pending_inc.new_xinfo[osd].old_weight = osdmap.osd_weight[osd];\n\t    }\n\t    ss << \"marked out osd.\" << osd << \". \";\n            std::ostringstream msg;\n            msg << \"Client \" << op->get_session()->entity_name\n                << \" marked osd.\" << osd << \" out\";\n            if (osdmap.is_up(osd)) {\n              msg << \", while it was still marked up\";\n            } else {\n              auto period = ceph_clock_now() - down_pending_out[osd];\n              msg << \", after it was down for \" << int(period.sec())\n                  << \" seconds\";\n            }\n\n            mon->clog->info() << msg.str();\n\t    any = true;\n\t  }\n        } else if (prefix == \"osd in\") {\n\t  if (osdmap.is_in(osd)) {\n            if (verbose)\n\t      ss << \"osd.\" << osd << \" is already in. \";\n\t  } else {\n\t    if (osdmap.osd_xinfo[osd].old_weight > 0) {\n\t      pending_inc.new_weight[osd] = osdmap.osd_xinfo[osd].old_weight;\n\t      if (pending_inc.new_xinfo.count(osd) == 0) {\n\t        pending_inc.new_xinfo[osd] = osdmap.osd_xinfo[osd];\n\t      }\n\t      pending_inc.new_xinfo[osd].old_weight = 0;\n\t    } else {\n\t      pending_inc.new_weight[osd] = CEPH_OSD_IN;\n\t    }\n\t    ss << \"marked in osd.\" << osd << \". \";\n\t    any = true;\n\t  }\n        } else if (prefix == \"osd rm\") {\n          err = prepare_command_osd_remove(osd);\n\n          if (err == -EBUSY) {\n\t    if (any)\n\t      ss << \", \";\n            ss << \"osd.\" << osd << \" is still up; must be down before removal. \";\n\t  } else {\n            assert(err == 0);\n\t    if (any) {\n\t      ss << \", osd.\" << osd;\n            } else {\n\t      ss << \"removed osd.\" << osd;\n            }\n\t    any = true;\n\t  }\n        }\n      }\n    }\n    if (any) {\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, err, rs,\n\t\t\t\t\t\tget_last_committed() + 1));\n      return true;\n    }\n  } else if (prefix == \"osd add-noup\" ||\n             prefix == \"osd add-nodown\" ||\n             prefix == \"osd add-noin\" ||\n             prefix == \"osd add-noout\") {\n\n    enum {\n      OP_NOUP,\n      OP_NODOWN,\n      OP_NOIN,\n      OP_NOOUT,\n    } option;\n\n    if (prefix == \"osd add-noup\") {\n      option = OP_NOUP;\n    } else if (prefix == \"osd add-nodown\") {\n      option = OP_NODOWN;\n    } else if (prefix == \"osd add-noin\") {\n      option = OP_NOIN;\n    } else {\n      option = OP_NOOUT;\n    }\n\n    bool any = false;\n    bool stop = false;\n\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n\n      set<int> osds;\n\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n        osdmap.get_all_osds(osds);\n        stop = true;\n      } else {\n        // try traditional single osd way\n\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          // ss has reason for failure\n          ss << \", unable to parse osd id:\\\"\" << idvec[j] << \"\\\". \";\n          err = -EINVAL;\n          continue;\n        }\n\n        osds.insert(osd);\n      }\n\n      for (auto &osd : osds) {\n\n        if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        switch (option) {\n        case OP_NOUP:\n          if (osdmap.is_up(osd)) {\n            ss << \"osd.\" << osd << \" is already up. \";\n            continue;\n          }\n\n          if (osdmap.is_noup(osd)) {\n            if (pending_inc.pending_osd_state_clear(osd, CEPH_OSD_NOUP))\n              any = true;\n          } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOUP);\n            any = true;\n          }\n\n          break;\n\n        case OP_NODOWN:\n          if (osdmap.is_down(osd)) {\n            ss << \"osd.\" << osd << \" is already down. \";\n            continue;\n          }\n\n          if (osdmap.is_nodown(osd)) {\n            if (pending_inc.pending_osd_state_clear(osd, CEPH_OSD_NODOWN))\n              any = true;\n          } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_NODOWN);\n            any = true;\n          }\n\n          break;\n\n        case OP_NOIN:\n          if (osdmap.is_in(osd)) {\n            ss << \"osd.\" << osd << \" is already in. \";\n            continue;\n          }\n\n          if (osdmap.is_noin(osd)) {\n            if (pending_inc.pending_osd_state_clear(osd, CEPH_OSD_NOIN))\n              any = true;\n          } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOIN);\n            any = true;\n          }\n\n          break;\n\n        case OP_NOOUT:\n          if (osdmap.is_out(osd)) {\n            ss << \"osd.\" << osd << \" is already out. \";\n            continue;\n          }\n\n          if (osdmap.is_noout(osd)) {\n            if (pending_inc.pending_osd_state_clear(osd, CEPH_OSD_NOOUT))\n              any = true;\n          } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOOUT);\n            any = true;\n          }\n\n          break;\n\n        default:\n\t  assert(0 == \"invalid option\");\n        }\n      }\n    }\n\n    if (any) {\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, err, rs,\n                                 get_last_committed() + 1));\n      return true;\n    }\n  } else if (prefix == \"osd rm-noup\" ||\n             prefix == \"osd rm-nodown\" ||\n             prefix == \"osd rm-noin\" ||\n             prefix == \"osd rm-noout\") {\n\n    enum {\n      OP_NOUP,\n      OP_NODOWN,\n      OP_NOIN,\n      OP_NOOUT,\n    } option;\n\n    if (prefix == \"osd rm-noup\") {\n      option = OP_NOUP;\n    } else if (prefix == \"osd rm-nodown\") {\n      option = OP_NODOWN;\n    } else if (prefix == \"osd rm-noin\") {\n      option = OP_NOIN;\n    } else {\n      option = OP_NOOUT;\n    }\n\n    bool any = false;\n    bool stop = false;\n\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n\n      vector<int> osds;\n\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n\n        // touch previous noup/nodown/noin/noout osds only\n        switch (option) {\n        case OP_NOUP:\n          osdmap.get_noup_osds(&osds);\n          break;\n        case OP_NODOWN:\n          osdmap.get_nodown_osds(&osds);\n          break;\n        case OP_NOIN:\n          osdmap.get_noin_osds(&osds);\n          break;\n        case OP_NOOUT:\n          osdmap.get_noout_osds(&osds);\n          break;\n        default:\n          assert(0 == \"invalid option\");\n        }\n\n        // cancel any pending noup/nodown/noin/noout requests too\n        vector<int> pending_state_osds;\n        (void) pending_inc.get_pending_state_osds(&pending_state_osds);\n        for (auto &p : pending_state_osds) {\n\n          switch (option) {\n          case OP_NOUP:\n            if (!osdmap.is_noup(p) &&\n                pending_inc.pending_osd_state_clear(p, CEPH_OSD_NOUP)) {\n              any = true;\n            }\n            break;\n\n          case OP_NODOWN:\n            if (!osdmap.is_nodown(p) &&\n                pending_inc.pending_osd_state_clear(p, CEPH_OSD_NODOWN)) {\n              any = true;\n            }\n            break;\n\n          case OP_NOIN:\n            if (!osdmap.is_noin(p) &&\n                pending_inc.pending_osd_state_clear(p, CEPH_OSD_NOIN)) {\n              any = true;\n            }\n            break;\n\n          case OP_NOOUT:\n            if (!osdmap.is_noout(p) &&\n                pending_inc.pending_osd_state_clear(p, CEPH_OSD_NOOUT)) {\n              any = true;\n            }\n            break;\n\n          default:\n            assert(0 == \"invalid option\");\n          }\n        }\n\n        stop = true;\n      } else {\n        // try traditional single osd way\n\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          // ss has reason for failure\n          ss << \", unable to parse osd id:\\\"\" << idvec[j] << \"\\\". \";\n          err = -EINVAL;\n          continue;\n        }\n\n        osds.push_back(osd);\n      }\n\n      for (auto &osd : osds) {\n\n        if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        switch (option) {\n          case OP_NOUP:\n            if (osdmap.is_noup(osd)) {\n              pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOUP);\n              any = true;\n            } else if (pending_inc.pending_osd_state_clear(\n              osd, CEPH_OSD_NOUP)) {\n              any = true;\n            }\n            break;\n\n          case OP_NODOWN:\n            if (osdmap.is_nodown(osd)) {\n              pending_inc.pending_osd_state_set(osd, CEPH_OSD_NODOWN);\n              any = true;\n            } else if (pending_inc.pending_osd_state_clear(\n              osd, CEPH_OSD_NODOWN)) {\n              any = true;\n            }\n            break;\n\n          case OP_NOIN:\n            if (osdmap.is_noin(osd)) {\n              pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOIN);\n              any = true;\n            } else if (pending_inc.pending_osd_state_clear(\n              osd, CEPH_OSD_NOIN)) {\n              any = true;\n            }\n            break;\n\n          case OP_NOOUT:\n            if (osdmap.is_noout(osd)) {\n              pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOOUT);\n              any = true;\n            } else if (pending_inc.pending_osd_state_clear(\n              osd, CEPH_OSD_NOOUT)) {\n              any = true;\n            }\n            break;\n\n          default:\n            assert(0 == \"invalid option\");\n        }\n      }\n    }\n\n    if (any) {\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, err, rs,\n                                 get_last_committed() + 1));\n      return true;\n    }\n  } else if (prefix == \"osd pg-temp\") {\n    string pgidstr;\n    if (!cmd_getval(cct, cmdmap, \"pgid\", pgidstr)) {\n      ss << \"unable to parse 'pgid' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"pgid\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    pg_t pgid;\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.pg_exists(pgid)) {\n      ss << \"pg \" << pgid << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    if (pending_inc.new_pg_temp.count(pgid)) {\n      dout(10) << __func__ << \" waiting for pending update on \" << pgid << dendl;\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n\n    vector<int64_t> id_vec;\n    vector<int32_t> new_pg_temp;\n    cmd_getval(cct, cmdmap, \"id\", id_vec);\n    if (id_vec.empty())  {\n      pending_inc.new_pg_temp[pgid] = mempool::osdmap::vector<int>();\n      ss << \"done cleaning up pg_temp of \" << pgid;\n      goto update;\n    }\n    for (auto osd : id_vec) {\n      if (!osdmap.exists(osd)) {\n        ss << \"osd.\" << osd << \" does not exist\";\n        err = -ENOENT;\n        goto reply;\n      }\n      new_pg_temp.push_back(osd);\n    }\n\n    int pool_min_size = osdmap.get_pg_pool_min_size(pgid);\n    if ((int)new_pg_temp.size() < pool_min_size) {\n      ss << \"num of osds (\" << new_pg_temp.size() <<\") < pool min size (\"\n         << pool_min_size << \")\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    int pool_size = osdmap.get_pg_pool_size(pgid);\n    if ((int)new_pg_temp.size() > pool_size) {\n      ss << \"num of osds (\" << new_pg_temp.size() <<\") > pool size (\"\n         << pool_size << \")\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    pending_inc.new_pg_temp[pgid] = mempool::osdmap::vector<int>(\n      new_pg_temp.begin(), new_pg_temp.end());\n    ss << \"set \" << pgid << \" pg_temp mapping to \" << new_pg_temp;\n    goto update;\n  } else if (prefix == \"osd primary-temp\") {\n    string pgidstr;\n    if (!cmd_getval(cct, cmdmap, \"pgid\", pgidstr)) {\n      ss << \"unable to parse 'pgid' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"pgid\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    pg_t pgid;\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.pg_exists(pgid)) {\n      ss << \"pg \" << pgid << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n\n    int64_t osd;\n    if (!cmd_getval(cct, cmdmap, \"id\", osd)) {\n      ss << \"unable to parse 'id' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (osd != -1 && !osdmap.exists(osd)) {\n      ss << \"osd.\" << osd << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n\n    if (osdmap.require_min_compat_client > 0 &&\n\tosdmap.require_min_compat_client < CEPH_RELEASE_FIREFLY) {\n      ss << \"require_min_compat_client \"\n\t << ceph_release_name(osdmap.require_min_compat_client)\n\t << \" < firefly, which is required for primary-temp\";\n      err = -EPERM;\n      goto reply;\n    }\n\n    pending_inc.new_primary_temp[pgid] = osd;\n    ss << \"set \" << pgid << \" primary_temp mapping to \" << osd;\n    goto update;\n  } else if (prefix == \"osd pg-upmap\" ||\n             prefix == \"osd rm-pg-upmap\" ||\n             prefix == \"osd pg-upmap-items\" ||\n             prefix == \"osd rm-pg-upmap-items\") {\n    if (osdmap.require_min_compat_client < CEPH_RELEASE_LUMINOUS) {\n      ss << \"min_compat_client \"\n\t << ceph_release_name(osdmap.require_min_compat_client)\n\t << \" < luminous, which is required for pg-upmap. \"\n         << \"Try 'ceph osd set-require-min-compat-client luminous' \"\n         << \"before using the new interface\";\n      err = -EPERM;\n      goto reply;\n    }\n    err = check_cluster_features(CEPH_FEATUREMASK_OSDMAP_PG_UPMAP, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err < 0)\n      goto reply;\n    string pgidstr;\n    if (!cmd_getval(cct, cmdmap, \"pgid\", pgidstr)) {\n      ss << \"unable to parse 'pgid' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"pgid\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    pg_t pgid;\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.pg_exists(pgid)) {\n      ss << \"pg \" << pgid << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    if (pending_inc.old_pools.count(pgid.pool())) {\n      ss << \"pool of \" << pgid << \" is pending removal\";\n      err = -ENOENT;\n      getline(ss, rs);\n      wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon, op, err, rs, get_last_committed() + 1));\n      return true;\n    }\n\n    enum {\n      OP_PG_UPMAP,\n      OP_RM_PG_UPMAP,\n      OP_PG_UPMAP_ITEMS,\n      OP_RM_PG_UPMAP_ITEMS,\n    } option;\n\n    if (prefix == \"osd pg-upmap\") {\n      option = OP_PG_UPMAP;\n    } else if (prefix == \"osd rm-pg-upmap\") {\n      option = OP_RM_PG_UPMAP;\n    } else if (prefix == \"osd pg-upmap-items\") {\n      option = OP_PG_UPMAP_ITEMS;\n    } else {\n      option = OP_RM_PG_UPMAP_ITEMS;\n    }\n\n    // check pending upmap changes\n    switch (option) {\n    case OP_PG_UPMAP: // fall through\n    case OP_RM_PG_UPMAP:\n      if (pending_inc.new_pg_upmap.count(pgid) ||\n          pending_inc.old_pg_upmap.count(pgid)) {\n        dout(10) << __func__ << \" waiting for pending update on \"\n                 << pgid << dendl;\n        wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n        return true;\n      }\n      break;\n\n    case OP_PG_UPMAP_ITEMS: // fall through\n    case OP_RM_PG_UPMAP_ITEMS:\n      if (pending_inc.new_pg_upmap_items.count(pgid) ||\n          pending_inc.old_pg_upmap_items.count(pgid)) {\n        dout(10) << __func__ << \" waiting for pending update on \"\n                 << pgid << dendl;\n        wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n        return true;\n      }\n      break;\n\n    default:\n      assert(0 == \"invalid option\");\n    }\n\n    switch (option) {\n    case OP_PG_UPMAP:\n      {\n        vector<int64_t> id_vec;\n        if (!cmd_getval(cct, cmdmap, \"id\", id_vec)) {\n          ss << \"unable to parse 'id' value(s) '\"\n             << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        int pool_min_size = osdmap.get_pg_pool_min_size(pgid);\n        if ((int)id_vec.size() < pool_min_size) {\n          ss << \"num of osds (\" << id_vec.size() <<\") < pool min size (\"\n             << pool_min_size << \")\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        int pool_size = osdmap.get_pg_pool_size(pgid);\n        if ((int)id_vec.size() > pool_size) {\n          ss << \"num of osds (\" << id_vec.size() <<\") > pool size (\"\n             << pool_size << \")\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        vector<int32_t> new_pg_upmap;\n        for (auto osd : id_vec) {\n          if (osd != CRUSH_ITEM_NONE && !osdmap.exists(osd)) {\n            ss << \"osd.\" << osd << \" does not exist\";\n            err = -ENOENT;\n            goto reply;\n          }\n          auto it = std::find(new_pg_upmap.begin(), new_pg_upmap.end(), osd);\n          if (it != new_pg_upmap.end()) {\n            ss << \"osd.\" << osd << \" already exists, \";\n            continue;\n          }\n          new_pg_upmap.push_back(osd);\n        }\n\n        if (new_pg_upmap.empty()) {\n          ss << \"no valid upmap items(pairs) is specified\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        pending_inc.new_pg_upmap[pgid] = mempool::osdmap::vector<int32_t>(\n          new_pg_upmap.begin(), new_pg_upmap.end());\n        ss << \"set \" << pgid << \" pg_upmap mapping to \" << new_pg_upmap;\n      }\n      break;\n\n    case OP_RM_PG_UPMAP:\n      {\n        pending_inc.old_pg_upmap.insert(pgid);\n        ss << \"clear \" << pgid << \" pg_upmap mapping\";\n      }\n      break;\n\n    case OP_PG_UPMAP_ITEMS:\n      {\n        vector<int64_t> id_vec;\n        if (!cmd_getval(cct, cmdmap, \"id\", id_vec)) {\n          ss << \"unable to parse 'id' value(s) '\"\n             << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        if (id_vec.size() % 2) {\n          ss << \"you must specify pairs of osd ids to be remapped\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        int pool_size = osdmap.get_pg_pool_size(pgid);\n        if ((int)(id_vec.size() / 2) > pool_size) {\n          ss << \"num of osd pairs (\" << id_vec.size() / 2 <<\") > pool size (\"\n             << pool_size << \")\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        vector<pair<int32_t,int32_t>> new_pg_upmap_items;\n        ostringstream items;\n        items << \"[\";\n        for (auto p = id_vec.begin(); p != id_vec.end(); ++p) {\n          int from = *p++;\n          int to = *p;\n          if (from == to) {\n            ss << \"from osd.\" << from << \" == to osd.\" << to << \", \";\n            continue;\n          }\n          if (!osdmap.exists(from)) {\n            ss << \"osd.\" << from << \" does not exist\";\n            err = -ENOENT;\n            goto reply;\n          }\n          if (to != CRUSH_ITEM_NONE && !osdmap.exists(to)) {\n            ss << \"osd.\" << to << \" does not exist\";\n            err = -ENOENT;\n            goto reply;\n          }\n          pair<int32_t,int32_t> entry = make_pair(from, to);\n          auto it = std::find(new_pg_upmap_items.begin(),\n            new_pg_upmap_items.end(), entry);\n          if (it != new_pg_upmap_items.end()) {\n            ss << \"osd.\" << from << \" -> osd.\" << to << \" already exists, \";\n            continue;\n          }\n          new_pg_upmap_items.push_back(entry);\n          items << from << \"->\" << to << \",\";\n        }\n        string out(items.str());\n        out.resize(out.size() - 1); // drop last ','\n        out += \"]\";\n\n        if (new_pg_upmap_items.empty()) {\n          ss << \"no valid upmap items(pairs) is specified\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        pending_inc.new_pg_upmap_items[pgid] =\n          mempool::osdmap::vector<pair<int32_t,int32_t>>(\n          new_pg_upmap_items.begin(), new_pg_upmap_items.end());\n        ss << \"set \" << pgid << \" pg_upmap_items mapping to \" << out;\n      }\n      break;\n\n    case OP_RM_PG_UPMAP_ITEMS:\n      {\n        pending_inc.old_pg_upmap_items.insert(pgid);\n        ss << \"clear \" << pgid << \" pg_upmap_items mapping\";\n      }\n      break;\n\n    default:\n      assert(0 == \"invalid option\");\n    }\n\n    goto update;\n  } else if (prefix == \"osd primary-affinity\") {\n    int64_t id;\n    if (!cmd_getval(cct, cmdmap, \"id\", id)) {\n      ss << \"invalid osd id value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    double w;\n    if (!cmd_getval(cct, cmdmap, \"weight\", w)) {\n      ss << \"unable to parse 'weight' value '\"\n\t << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    long ww = (int)((double)CEPH_OSD_MAX_PRIMARY_AFFINITY*w);\n    if (ww < 0L) {\n      ss << \"weight must be >= 0\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (osdmap.require_min_compat_client > 0 &&\n\tosdmap.require_min_compat_client < CEPH_RELEASE_FIREFLY) {\n      ss << \"require_min_compat_client \"\n\t << ceph_release_name(osdmap.require_min_compat_client)\n\t << \" < firefly, which is required for primary-affinity\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (osdmap.exists(id)) {\n      pending_inc.new_primary_affinity[id] = ww;\n      ss << \"set osd.\" << id << \" primary-affinity to \" << w << \" (\" << ios::hex << ww << ios::dec << \")\";\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                                                get_last_committed() + 1));\n      return true;\n    } else {\n      ss << \"osd.\" << id << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n  } else if (prefix == \"osd reweight\") {\n    int64_t id;\n    if (!cmd_getval(cct, cmdmap, \"id\", id)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    double w;\n    if (!cmd_getval(cct, cmdmap, \"weight\", w)) {\n      ss << \"unable to parse weight value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    long ww = (int)((double)CEPH_OSD_IN*w);\n    if (ww < 0L) {\n      ss << \"weight must be >= 0\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (osdmap.exists(id)) {\n      pending_inc.new_weight[id] = ww;\n      ss << \"reweighted osd.\" << id << \" to \" << w << \" (\" << std::hex << ww << std::dec << \")\";\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\tget_last_committed() + 1));\n      return true;\n    } else {\n      ss << \"osd.\" << id << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n  } else if (prefix == \"osd reweightn\") {\n    map<int32_t, uint32_t> weights;\n    err = parse_reweights(cct, cmdmap, osdmap, &weights);\n    if (err) {\n      ss << \"unable to parse 'weights' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"weights\")) << \"'\";\n      goto reply;\n    }\n    pending_inc.new_weight.insert(weights.begin(), weights.end());\n    wait_for_finished_proposal(\n\top,\n\tnew Monitor::C_Command(mon, op, 0, rs, rdata, get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd lost\") {\n    int64_t id;\n    if (!cmd_getval(cct, cmdmap, \"id\", id)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    string sure;\n    if (!cmd_getval(cct, cmdmap, \"sure\", sure) || sure != \"--yes-i-really-mean-it\") {\n      ss << \"are you SURE?  this might mean real, permanent data loss.  pass \"\n\t    \"--yes-i-really-mean-it if you really do.\";\n      err = -EPERM;\n      goto reply;\n    } else if (!osdmap.exists(id)) {\n      ss << \"osd.\" << id << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    } else if (!osdmap.is_down(id)) {\n      ss << \"osd.\" << id << \" is not down\";\n      err = -EBUSY;\n      goto reply;\n    } else {\n      epoch_t e = osdmap.get_info(id).down_at;\n      pending_inc.new_lost[id] = e;\n      ss << \"marked osd lost in epoch \" << e;\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\tget_last_committed() + 1));\n      return true;\n    }\n\n  } else if (prefix == \"osd destroy\" ||\n\t     prefix == \"osd purge\" ||\n\t     prefix == \"osd purge-new\") {\n    /* Destroying an OSD means that we don't expect to further make use of\n     * the OSDs data (which may even become unreadable after this operation),\n     * and that we are okay with scrubbing all its cephx keys and config-key\n     * data (which may include lockbox keys, thus rendering the osd's data\n     * unreadable).\n     *\n     * The OSD will not be removed. Instead, we will mark it as destroyed,\n     * such that a subsequent call to `create` will not reuse the osd id.\n     * This will play into being able to recreate the OSD, at the same\n     * crush location, with minimal data movement.\n     */\n\n    // make sure authmon is writeable.\n    if (!mon->authmon()->is_writeable()) {\n      dout(10) << __func__ << \" waiting for auth mon to be writeable for \"\n               << \"osd destroy\" << dendl;\n      mon->authmon()->wait_for_writeable(op, new C_RetryMessage(this, op));\n      return false;\n    }\n\n    int64_t id;\n    if (!cmd_getval(cct, cmdmap, \"id\", id)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    bool is_destroy = (prefix == \"osd destroy\");\n    if (!is_destroy) {\n      assert(\"osd purge\" == prefix ||\n\t     \"osd purge-new\" == prefix);\n    }\n\n    string sure;\n    if (!cmd_getval(cct, cmdmap, \"sure\", sure) ||\n        sure != \"--yes-i-really-mean-it\") {\n      ss << \"Are you SURE? This will mean real, permanent data loss, as well \"\n         << \"as cephx and lockbox keys. Pass --yes-i-really-mean-it if you \"\n         << \"really do.\";\n      err = -EPERM;\n      goto reply;\n    } else if (!osdmap.exists(id)) {\n      ss << \"osd.\" << id << \" does not exist\";\n      err = 0; // idempotent\n      goto reply;\n    } else if (osdmap.is_up(id)) {\n      ss << \"osd.\" << id << \" is not `down`.\";\n      err = -EBUSY;\n      goto reply;\n    } else if (is_destroy && osdmap.is_destroyed(id)) {\n      ss << \"destroyed osd.\" << id;\n      err = 0;\n      goto reply;\n    }\n\n    if (prefix == \"osd purge-new\" &&\n\t(osdmap.get_state(id) & CEPH_OSD_NEW) == 0) {\n      ss << \"osd.\" << id << \" is not new\";\n      err = -EPERM;\n      goto reply;\n    }\n\n    bool goto_reply = false;\n\n    paxos->plug();\n    if (is_destroy) {\n      err = prepare_command_osd_destroy(id, ss);\n      // we checked above that it should exist.\n      assert(err != -ENOENT);\n    } else {\n      err = prepare_command_osd_purge(id, ss);\n      if (err == -ENOENT) {\n        err = 0;\n        ss << \"osd.\" << id << \" does not exist.\";\n        goto_reply = true;\n      }\n    }\n    paxos->unplug();\n\n    if (err < 0 || goto_reply) {\n      goto reply;\n    }\n\n    if (is_destroy) {\n      ss << \"destroyed osd.\" << id;\n    } else {\n      ss << \"purged osd.\" << id;\n    }\n\n    getline(ss, rs);\n    wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon, op, 0, rs, get_last_committed() + 1));\n    force_immediate_propose();\n    return true;\n\n  } else if (prefix == \"osd new\") {\n\n    // make sure authmon is writeable.\n    if (!mon->authmon()->is_writeable()) {\n      dout(10) << __func__ << \" waiting for auth mon to be writeable for \"\n               << \"osd new\" << dendl;\n      mon->authmon()->wait_for_writeable(op, new C_RetryMessage(this, op));\n      return false;\n    }\n\n    map<string,string> param_map;\n\n    bufferlist bl = m->get_data();\n    string param_json = bl.to_str();\n    dout(20) << __func__ << \" osd new json = \" << param_json << dendl;\n\n    err = get_json_str_map(param_json, ss, &param_map);\n    if (err < 0)\n      goto reply;\n\n    dout(20) << __func__ << \" osd new params \" << param_map << dendl;\n\n    paxos->plug();\n    err = prepare_command_osd_new(op, cmdmap, param_map, ss, f.get());\n    paxos->unplug();\n\n    if (err < 0) {\n      goto reply;\n    }\n\n    if (f) {\n      f->flush(rdata);\n    } else {\n      rdata.append(ss);\n    }\n\n    if (err == EEXIST) {\n      // idempotent operation\n      err = 0;\n      goto reply;\n    }\n\n    wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon, op, 0, rs, rdata,\n                               get_last_committed() + 1));\n    force_immediate_propose();\n    return true;\n\n  } else if (prefix == \"osd create\") {\n\n    // optional id provided?\n    int64_t id = -1, cmd_id = -1;\n    if (cmd_getval(cct, cmdmap, \"id\", cmd_id)) {\n      if (cmd_id < 0) {\n\tss << \"invalid osd id value '\" << cmd_id << \"'\";\n\terr = -EINVAL;\n\tgoto reply;\n      }\n      dout(10) << \" osd create got id \" << cmd_id << dendl;\n    }\n\n    uuid_d uuid;\n    string uuidstr;\n    if (cmd_getval(cct, cmdmap, \"uuid\", uuidstr)) {\n      if (!uuid.parse(uuidstr.c_str())) {\n        ss << \"invalid uuid value '\" << uuidstr << \"'\";\n        err = -EINVAL;\n        goto reply;\n      }\n      // we only care about the id if we also have the uuid, to\n      // ensure the operation's idempotency.\n      id = cmd_id;\n    }\n\n    int32_t new_id = -1;\n    err = prepare_command_osd_create(id, uuid, &new_id, ss);\n    if (err < 0) {\n      if (err == -EAGAIN) {\n        wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n        return true;\n      }\n      // a check has failed; reply to the user.\n      goto reply;\n\n    } else if (err == EEXIST) {\n      // this is an idempotent operation; we can go ahead and reply.\n      if (f) {\n        f->open_object_section(\"created_osd\");\n        f->dump_int(\"osdid\", new_id);\n        f->close_section();\n        f->flush(rdata);\n      } else {\n        ss << new_id;\n        rdata.append(ss);\n      }\n      err = 0;\n      goto reply;\n    }\n\n    string empty_device_class;\n    do_osd_create(id, uuid, empty_device_class, &new_id);\n\n    if (f) {\n      f->open_object_section(\"created_osd\");\n      f->dump_int(\"osdid\", new_id);\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ss << new_id;\n      rdata.append(ss);\n    }\n    wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon, op, 0, rs, rdata,\n                               get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd blacklist clear\") {\n    pending_inc.new_blacklist.clear();\n    std::list<std::pair<entity_addr_t,utime_t > > blacklist;\n    osdmap.get_blacklist(&blacklist);\n    for (const auto &entry : blacklist) {\n      pending_inc.old_blacklist.push_back(entry.first);\n    }\n    ss << \" removed all blacklist entries\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                                              get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd blacklist\") {\n    string addrstr;\n    cmd_getval(cct, cmdmap, \"addr\", addrstr);\n    entity_addr_t addr;\n    if (!addr.parse(addrstr.c_str(), 0)) {\n      ss << \"unable to parse address \" << addrstr;\n      err = -EINVAL;\n      goto reply;\n    }\n    else {\n      string blacklistop;\n      cmd_getval(cct, cmdmap, \"blacklistop\", blacklistop);\n      if (blacklistop == \"add\") {\n\tutime_t expires = ceph_clock_now();\n\tdouble d;\n\t// default one hour\n\tcmd_getval(cct, cmdmap, \"expire\", d,\n          g_conf->mon_osd_blacklist_default_expire);\n\texpires += d;\n\n\tpending_inc.new_blacklist[addr] = expires;\n\n        {\n          // cancel any pending un-blacklisting request too\n          auto it = std::find(pending_inc.old_blacklist.begin(),\n            pending_inc.old_blacklist.end(), addr);\n          if (it != pending_inc.old_blacklist.end()) {\n            pending_inc.old_blacklist.erase(it);\n          }\n        }\n\n\tss << \"blacklisting \" << addr << \" until \" << expires << \" (\" << d << \" sec)\";\n\tgetline(ss, rs);\n\twait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n\treturn true;\n      } else if (blacklistop == \"rm\") {\n\tif (osdmap.is_blacklisted(addr) ||\n\t    pending_inc.new_blacklist.count(addr)) {\n\t  if (osdmap.is_blacklisted(addr))\n\t    pending_inc.old_blacklist.push_back(addr);\n\t  else\n\t    pending_inc.new_blacklist.erase(addr);\n\t  ss << \"un-blacklisting \" << addr;\n\t  getline(ss, rs);\n\t  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t    get_last_committed() + 1));\n\t  return true;\n\t}\n\tss << addr << \" isn't blacklisted\";\n\terr = 0;\n\tgoto reply;\n      }\n    }\n  } else if (prefix == \"osd pool mksnap\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string snapname;\n    cmd_getval(cct, cmdmap, \"snap\", snapname);\n    const pg_pool_t *p = osdmap.get_pg_pool(pool);\n    if (p->is_unmanaged_snaps_mode()) {\n      ss << \"pool \" << poolstr << \" is in unmanaged snaps mode\";\n      err = -EINVAL;\n      goto reply;\n    } else if (p->snap_exists(snapname.c_str())) {\n      ss << \"pool \" << poolstr << \" snap \" << snapname << \" already exists\";\n      err = 0;\n      goto reply;\n    } else if (p->is_tier()) {\n      ss << \"pool \" << poolstr << \" is a cache tier\";\n      err = -EINVAL;\n      goto reply;\n    }\n    pg_pool_t *pp = 0;\n    if (pending_inc.new_pools.count(pool))\n      pp = &pending_inc.new_pools[pool];\n    if (!pp) {\n      pp = &pending_inc.new_pools[pool];\n      *pp = *p;\n    }\n    if (pp->snap_exists(snapname.c_str())) {\n      ss << \"pool \" << poolstr << \" snap \" << snapname << \" already exists\";\n    } else {\n      pp->add_snap(snapname.c_str(), ceph_clock_now());\n      pp->set_snap_epoch(pending_inc.epoch);\n      ss << \"created pool \" << poolstr << \" snap \" << snapname;\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pool rmsnap\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string snapname;\n    cmd_getval(cct, cmdmap, \"snap\", snapname);\n    const pg_pool_t *p = osdmap.get_pg_pool(pool);\n    if (p->is_unmanaged_snaps_mode()) {\n      ss << \"pool \" << poolstr << \" is in unmanaged snaps mode\";\n      err = -EINVAL;\n      goto reply;\n    } else if (!p->snap_exists(snapname.c_str())) {\n      ss << \"pool \" << poolstr << \" snap \" << snapname << \" does not exist\";\n      err = 0;\n      goto reply;\n    }\n    pg_pool_t *pp = 0;\n    if (pending_inc.new_pools.count(pool))\n      pp = &pending_inc.new_pools[pool];\n    if (!pp) {\n      pp = &pending_inc.new_pools[pool];\n      *pp = *p;\n    }\n    snapid_t sn = pp->snap_exists(snapname.c_str());\n    if (sn) {\n      pp->remove_snap(sn);\n      pp->set_snap_epoch(pending_inc.epoch);\n      ss << \"removed pool \" << poolstr << \" snap \" << snapname;\n    } else {\n      ss << \"already removed pool \" << poolstr << \" snap \" << snapname;\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pool create\") {\n    int64_t  pg_num;\n    int64_t pgp_num;\n    cmd_getval(cct, cmdmap, \"pg_num\", pg_num, int64_t(0));\n    cmd_getval(cct, cmdmap, \"pgp_num\", pgp_num, pg_num);\n\n    string pool_type_str;\n    cmd_getval(cct, cmdmap, \"pool_type\", pool_type_str);\n    if (pool_type_str.empty())\n      pool_type_str = g_conf->get_val<string>(\"osd_pool_default_type\");\n\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id >= 0) {\n      const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n      if (pool_type_str != p->get_type_name()) {\n\tss << \"pool '\" << poolstr << \"' cannot change to type \" << pool_type_str;\n \terr = -EINVAL;\n      } else {\n\tss << \"pool '\" << poolstr << \"' already exists\";\n\terr = 0;\n      }\n      goto reply;\n    }\n\n    int pool_type;\n    if (pool_type_str == \"replicated\") {\n      pool_type = pg_pool_t::TYPE_REPLICATED;\n    } else if (pool_type_str == \"erasure\") {\n      pool_type = pg_pool_t::TYPE_ERASURE;\n    } else {\n      ss << \"unknown pool type '\" << pool_type_str << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    bool implicit_rule_creation = false;\n    int64_t expected_num_objects = 0;\n    string rule_name;\n    cmd_getval(cct, cmdmap, \"rule\", rule_name);\n    string erasure_code_profile;\n    cmd_getval(cct, cmdmap, \"erasure_code_profile\", erasure_code_profile);\n\n    if (pool_type == pg_pool_t::TYPE_ERASURE) {\n      if (erasure_code_profile == \"\")\n\terasure_code_profile = \"default\";\n      //handle the erasure code profile\n      if (erasure_code_profile == \"default\") {\n\tif (!osdmap.has_erasure_code_profile(erasure_code_profile)) {\n\t  if (pending_inc.has_erasure_code_profile(erasure_code_profile)) {\n\t    dout(20) << \"erasure code profile \" << erasure_code_profile << \" already pending\" << dendl;\n\t    goto wait;\n\t  }\n\n\t  map<string,string> profile_map;\n\t  err = osdmap.get_erasure_code_profile_default(cct,\n\t\t\t\t\t\t      profile_map,\n\t\t\t\t\t\t      &ss);\n\t  if (err)\n\t    goto reply;\n\t  dout(20) << \"erasure code profile \" << erasure_code_profile << \" set\" << dendl;\n\t  pending_inc.set_erasure_code_profile(erasure_code_profile, profile_map);\n\t  goto wait;\n\t}\n      }\n      if (rule_name == \"\") {\n\timplicit_rule_creation = true;\n\tif (erasure_code_profile == \"default\") {\n\t  rule_name = \"erasure-code\";\n\t} else {\n\t  dout(1) << \"implicitly use rule named after the pool: \"\n\t\t<< poolstr << dendl;\n\t  rule_name = poolstr;\n\t}\n      }\n      cmd_getval(g_ceph_context, cmdmap, \"expected_num_objects\",\n                 expected_num_objects, int64_t(0));\n    } else {\n      //NOTE:for replicated pool,cmd_map will put rule_name to erasure_code_profile field\n      //     and put expected_num_objects to rule field\n      if (erasure_code_profile != \"\") { // cmd is from CLI\n        if (rule_name != \"\") {\n          string interr;\n          expected_num_objects = strict_strtoll(rule_name.c_str(), 10, &interr);\n          if (interr.length()) {\n            ss << \"error parsing integer value '\" << rule_name << \"': \" << interr;\n            err = -EINVAL;\n            goto reply;\n          }\n        }\n        rule_name = erasure_code_profile;\n      } else { // cmd is well-formed\n        cmd_getval(g_ceph_context, cmdmap, \"expected_num_objects\",\n                   expected_num_objects, int64_t(0));\n      }\n    }\n\n    if (!implicit_rule_creation && rule_name != \"\") {\n      int rule;\n      err = get_crush_rule(rule_name, &rule, &ss);\n      if (err == -EAGAIN) {\n\twait_for_finished_proposal(op, new C_RetryMessage(this, op));\n\treturn true;\n      }\n      if (err)\n\tgoto reply;\n    }\n\n    if (expected_num_objects < 0) {\n      ss << \"'expected_num_objects' must be non-negative\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    int64_t fast_read_param;\n    cmd_getval(cct, cmdmap, \"fast_read\", fast_read_param, int64_t(-1));\n    FastReadType fast_read = FAST_READ_DEFAULT;\n    if (fast_read_param == 0)\n      fast_read = FAST_READ_OFF;\n    else if (fast_read_param > 0)\n      fast_read = FAST_READ_ON;\n    \n    err = prepare_new_pool(poolstr, 0, // auid=0 for admin created pool\n\t\t\t   -1, // default crush rule\n\t\t\t   rule_name,\n\t\t\t   pg_num, pgp_num,\n\t\t\t   erasure_code_profile, pool_type,\n                           (uint64_t)expected_num_objects,\n                           fast_read,\n\t\t\t   &ss);\n    if (err < 0) {\n      switch(err) {\n      case -EEXIST:\n\tss << \"pool '\" << poolstr << \"' already exists\";\n\tbreak;\n      case -EAGAIN:\n\twait_for_finished_proposal(op, new C_RetryMessage(this, op));\n\treturn true;\n      case -ERANGE:\n        goto reply;\n      default:\n\tgoto reply;\n\tbreak;\n      }\n    } else {\n      ss << \"pool '\" << poolstr << \"' created\";\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd pool delete\" ||\n             prefix == \"osd pool rm\") {\n    // osd pool delete/rm <poolname> <poolname again> --yes-i-really-really-mean-it\n    string poolstr, poolstr2, sure;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    cmd_getval(cct, cmdmap, \"pool2\", poolstr2);\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"pool '\" << poolstr << \"' does not exist\";\n      err = 0;\n      goto reply;\n    }\n\n    bool force_no_fake = sure == \"--yes-i-really-really-mean-it-not-faking\";\n    if (poolstr2 != poolstr ||\n\t(sure != \"--yes-i-really-really-mean-it\" && !force_no_fake)) {\n      ss << \"WARNING: this will *PERMANENTLY DESTROY* all data stored in pool \" << poolstr\n\t << \".  If you are *ABSOLUTELY CERTAIN* that is what you want, pass the pool name *twice*, \"\n\t << \"followed by --yes-i-really-really-mean-it.\";\n      err = -EPERM;\n      goto reply;\n    }\n    err = _prepare_remove_pool(pool, &ss, force_no_fake);\n    if (err == -EAGAIN) {\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    if (err < 0)\n      goto reply;\n    goto update;\n  } else if (prefix == \"osd pool rename\") {\n    string srcpoolstr, destpoolstr;\n    cmd_getval(cct, cmdmap, \"srcpool\", srcpoolstr);\n    cmd_getval(cct, cmdmap, \"destpool\", destpoolstr);\n    int64_t pool_src = osdmap.lookup_pg_pool_name(srcpoolstr.c_str());\n    int64_t pool_dst = osdmap.lookup_pg_pool_name(destpoolstr.c_str());\n\n    if (pool_src < 0) {\n      if (pool_dst >= 0) {\n        // src pool doesn't exist, dst pool does exist: to ensure idempotency\n        // of operations, assume this rename succeeded, as it is not changing\n        // the current state.  Make sure we output something understandable\n        // for whoever is issuing the command, if they are paying attention,\n        // in case it was not intentional; or to avoid a \"wtf?\" and a bug\n        // report in case it was intentional, while expecting a failure.\n        ss << \"pool '\" << srcpoolstr << \"' does not exist; pool '\"\n          << destpoolstr << \"' does -- assuming successful rename\";\n        err = 0;\n      } else {\n        ss << \"unrecognized pool '\" << srcpoolstr << \"'\";\n        err = -ENOENT;\n      }\n      goto reply;\n    } else if (pool_dst >= 0) {\n      // source pool exists and so does the destination pool\n      ss << \"pool '\" << destpoolstr << \"' already exists\";\n      err = -EEXIST;\n      goto reply;\n    }\n\n    int ret = _prepare_rename_pool(pool_src, destpoolstr);\n    if (ret == 0) {\n      ss << \"pool '\" << srcpoolstr << \"' renamed to '\" << destpoolstr << \"'\";\n    } else {\n      ss << \"failed to rename pool '\" << srcpoolstr << \"' to '\" << destpoolstr << \"': \"\n        << cpp_strerror(ret);\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, ret, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd pool set\") {\n    err = prepare_command_pool_set(cmdmap, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err < 0)\n      goto reply;\n\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t   get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier add\") {\n    err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string tierpoolstr;\n    cmd_getval(cct, cmdmap, \"tierpool\", tierpoolstr);\n    int64_t tierpool_id = osdmap.lookup_pg_pool_name(tierpoolstr);\n    if (tierpool_id < 0) {\n      ss << \"unrecognized pool '\" << tierpoolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    const pg_pool_t *tp = osdmap.get_pg_pool(tierpool_id);\n    assert(tp);\n\n    if (!_check_become_tier(tierpool_id, tp, pool_id, p, &err, &ss)) {\n      goto reply;\n    }\n\n    // make sure new tier is empty\n    string force_nonempty;\n    cmd_getval(cct, cmdmap, \"force_nonempty\", force_nonempty);\n    const pool_stat_t *pstats = mon->mgrstatmon()->get_pool_stat(tierpool_id);\n    if (pstats && pstats->stats.sum.num_objects != 0 &&\n\tforce_nonempty != \"--force-nonempty\") {\n      ss << \"tier pool '\" << tierpoolstr << \"' is not empty; --force-nonempty to force\";\n      err = -ENOTEMPTY;\n      goto reply;\n    }\n    if (tp->is_erasure()) {\n      ss << \"tier pool '\" << tierpoolstr\n\t << \"' is an ec pool, which cannot be a tier\";\n      err = -ENOTSUP;\n      goto reply;\n    }\n    if ((!tp->removed_snaps.empty() || !tp->snaps.empty()) &&\n\t((force_nonempty != \"--force-nonempty\") ||\n\t (!g_conf->mon_debug_unsafe_allow_tier_with_nonempty_snaps))) {\n      ss << \"tier pool '\" << tierpoolstr << \"' has snapshot state; it cannot be added as a tier without breaking the pool\";\n      err = -ENOTEMPTY;\n      goto reply;\n    }\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    pg_pool_t *ntp = pending_inc.get_new_pool(tierpool_id, tp);\n    if (np->tiers.count(tierpool_id) || ntp->is_tier()) {\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    np->tiers.insert(tierpool_id);\n    np->set_snap_epoch(pending_inc.epoch); // tier will update to our snap info\n    ntp->tier_of = pool_id;\n    ss << \"pool '\" << tierpoolstr << \"' is now (or already was) a tier of '\" << poolstr << \"'\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier remove\" ||\n             prefix == \"osd tier rm\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string tierpoolstr;\n    cmd_getval(cct, cmdmap, \"tierpool\", tierpoolstr);\n    int64_t tierpool_id = osdmap.lookup_pg_pool_name(tierpoolstr);\n    if (tierpool_id < 0) {\n      ss << \"unrecognized pool '\" << tierpoolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    const pg_pool_t *tp = osdmap.get_pg_pool(tierpool_id);\n    assert(tp);\n\n    if (!_check_remove_tier(pool_id, p, tp, &err, &ss)) {\n      goto reply;\n    }\n\n    if (p->tiers.count(tierpool_id) == 0) {\n      ss << \"pool '\" << tierpoolstr << \"' is now (or already was) not a tier of '\" << poolstr << \"'\";\n      err = 0;\n      goto reply;\n    }\n    if (tp->tier_of != pool_id) {\n      ss << \"tier pool '\" << tierpoolstr << \"' is a tier of '\"\n         << osdmap.get_pool_name(tp->tier_of) << \"': \"\n         // be scary about it; this is an inconsistency and bells must go off\n         << \"THIS SHOULD NOT HAVE HAPPENED AT ALL\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (p->read_tier == tierpool_id) {\n      ss << \"tier pool '\" << tierpoolstr << \"' is the overlay for '\" << poolstr << \"'; please remove-overlay first\";\n      err = -EBUSY;\n      goto reply;\n    }\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    pg_pool_t *ntp = pending_inc.get_new_pool(tierpool_id, tp);\n    if (np->tiers.count(tierpool_id) == 0 ||\n\tntp->tier_of != pool_id ||\n\tnp->read_tier == tierpool_id) {\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    np->tiers.erase(tierpool_id);\n    ntp->clear_tier();\n    ss << \"pool '\" << tierpoolstr << \"' is now (or already was) not a tier of '\" << poolstr << \"'\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier set-overlay\") {\n    err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string overlaypoolstr;\n    cmd_getval(cct, cmdmap, \"overlaypool\", overlaypoolstr);\n    int64_t overlaypool_id = osdmap.lookup_pg_pool_name(overlaypoolstr);\n    if (overlaypool_id < 0) {\n      ss << \"unrecognized pool '\" << overlaypoolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    const pg_pool_t *overlay_p = osdmap.get_pg_pool(overlaypool_id);\n    assert(overlay_p);\n    if (p->tiers.count(overlaypool_id) == 0) {\n      ss << \"tier pool '\" << overlaypoolstr << \"' is not a tier of '\" << poolstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (p->read_tier == overlaypool_id) {\n      err = 0;\n      ss << \"overlay for '\" << poolstr << \"' is now (or already was) '\" << overlaypoolstr << \"'\";\n      goto reply;\n    }\n    if (p->has_read_tier()) {\n      ss << \"pool '\" << poolstr << \"' has overlay '\"\n\t << osdmap.get_pool_name(p->read_tier)\n\t << \"'; please remove-overlay first\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    np->read_tier = overlaypool_id;\n    np->write_tier = overlaypool_id;\n    np->set_last_force_op_resend(pending_inc.epoch);\n    pg_pool_t *noverlay_p = pending_inc.get_new_pool(overlaypool_id, overlay_p);\n    noverlay_p->set_last_force_op_resend(pending_inc.epoch);\n    ss << \"overlay for '\" << poolstr << \"' is now (or already was) '\" << overlaypoolstr << \"'\";\n    if (overlay_p->cache_mode == pg_pool_t::CACHEMODE_NONE)\n      ss <<\" (WARNING: overlay pool cache_mode is still NONE)\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier remove-overlay\" ||\n             prefix == \"osd tier rm-overlay\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    if (!p->has_read_tier()) {\n      err = 0;\n      ss << \"there is now (or already was) no overlay for '\" << poolstr << \"'\";\n      goto reply;\n    }\n\n    if (!_check_remove_tier(pool_id, p, NULL, &err, &ss)) {\n      goto reply;\n    }\n\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    if (np->has_read_tier()) {\n      const pg_pool_t *op = osdmap.get_pg_pool(np->read_tier);\n      pg_pool_t *nop = pending_inc.get_new_pool(np->read_tier,op);\n      nop->set_last_force_op_resend(pending_inc.epoch);\n    }\n    if (np->has_write_tier()) {\n      const pg_pool_t *op = osdmap.get_pg_pool(np->write_tier);\n      pg_pool_t *nop = pending_inc.get_new_pool(np->write_tier, op);\n      nop->set_last_force_op_resend(pending_inc.epoch);\n    }\n    np->clear_read_tier();\n    np->clear_write_tier();\n    np->set_last_force_op_resend(pending_inc.epoch);\n    ss << \"there is now (or already was) no overlay for '\" << poolstr << \"'\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier cache-mode\") {\n    err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    if (!p->is_tier()) {\n      ss << \"pool '\" << poolstr << \"' is not a tier\";\n      err = -EINVAL;\n      goto reply;\n    }\n    string modestr;\n    cmd_getval(cct, cmdmap, \"mode\", modestr);\n    pg_pool_t::cache_mode_t mode = pg_pool_t::get_cache_mode_from_str(modestr);\n    if (mode < 0) {\n      ss << \"'\" << modestr << \"' is not a valid cache mode\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    if ((mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t mode != pg_pool_t::CACHEMODE_NONE &&\n\t mode != pg_pool_t::CACHEMODE_PROXY &&\n\t mode != pg_pool_t::CACHEMODE_READPROXY) &&\n\tsure != \"--yes-i-really-mean-it\") {\n      ss << \"'\" << modestr << \"' is not a well-supported cache mode and may \"\n\t << \"corrupt your data.  pass --yes-i-really-mean-it to force.\";\n      err = -EPERM;\n      goto reply;\n    }\n\n    // pool already has this cache-mode set and there are no pending changes\n    if (p->cache_mode == mode &&\n\t(pending_inc.new_pools.count(pool_id) == 0 ||\n\t pending_inc.new_pools[pool_id].cache_mode == p->cache_mode)) {\n      ss << \"set cache-mode for pool '\" << poolstr << \"'\"\n         << \" to \" << pg_pool_t::get_cache_mode_name(mode);\n      err = 0;\n      goto reply;\n    }\n\n    /* Mode description:\n     *\n     *  none:       No cache-mode defined\n     *  forward:    Forward all reads and writes to base pool\n     *  writeback:  Cache writes, promote reads from base pool\n     *  readonly:   Forward writes to base pool\n     *  readforward: Writes are in writeback mode, Reads are in forward mode\n     *  proxy:       Proxy all reads and writes to base pool\n     *  readproxy:   Writes are in writeback mode, Reads are in proxy mode\n     *\n     * Hence, these are the allowed transitions:\n     *\n     *  none -> any\n     *  forward -> proxy || readforward || readproxy || writeback || any IF num_objects_dirty == 0\n     *  proxy -> forward || readforward || readproxy || writeback || any IF num_objects_dirty == 0\n     *  readforward -> forward || proxy || readproxy || writeback || any IF num_objects_dirty == 0\n     *  readproxy -> forward || proxy || readforward || writeback || any IF num_objects_dirty == 0\n     *  writeback -> readforward || readproxy || forward || proxy\n     *  readonly -> any\n     */\n\n    // We check if the transition is valid against the current pool mode, as\n    // it is the only committed state thus far.  We will blantly squash\n    // whatever mode is on the pending state.\n\n    if (p->cache_mode == pg_pool_t::CACHEMODE_WRITEBACK &&\n        (mode != pg_pool_t::CACHEMODE_FORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_PROXY &&\n\t  mode != pg_pool_t::CACHEMODE_READFORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_READPROXY)) {\n      ss << \"unable to set cache-mode '\" << pg_pool_t::get_cache_mode_name(mode)\n         << \"' on a '\" << pg_pool_t::get_cache_mode_name(p->cache_mode)\n         << \"' pool; only '\"\n         << pg_pool_t::get_cache_mode_name(pg_pool_t::CACHEMODE_FORWARD)\n\t << \"','\"\n         << pg_pool_t::get_cache_mode_name(pg_pool_t::CACHEMODE_PROXY)\n\t << \"','\"\n         << pg_pool_t::get_cache_mode_name(pg_pool_t::CACHEMODE_READFORWARD)\n\t << \"','\"\n         << pg_pool_t::get_cache_mode_name(pg_pool_t::CACHEMODE_READPROXY)\n        << \"' allowed.\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if ((p->cache_mode == pg_pool_t::CACHEMODE_READFORWARD &&\n        (mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t  mode != pg_pool_t::CACHEMODE_FORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_PROXY &&\n\t  mode != pg_pool_t::CACHEMODE_READPROXY)) ||\n\n        (p->cache_mode == pg_pool_t::CACHEMODE_READPROXY &&\n        (mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t  mode != pg_pool_t::CACHEMODE_FORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_READFORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_PROXY)) ||\n\n        (p->cache_mode == pg_pool_t::CACHEMODE_PROXY &&\n        (mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t  mode != pg_pool_t::CACHEMODE_FORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_READFORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_READPROXY)) ||\n\n        (p->cache_mode == pg_pool_t::CACHEMODE_FORWARD &&\n        (mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t  mode != pg_pool_t::CACHEMODE_READFORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_PROXY &&\n\t  mode != pg_pool_t::CACHEMODE_READPROXY))) {\n\n      const pool_stat_t* pstats =\n        mon->mgrstatmon()->get_pool_stat(pool_id);\n\n      if (pstats && pstats->stats.sum.num_objects_dirty > 0) {\n        ss << \"unable to set cache-mode '\"\n           << pg_pool_t::get_cache_mode_name(mode) << \"' on pool '\" << poolstr\n           << \"': dirty objects found\";\n        err = -EBUSY;\n        goto reply;\n      }\n    }\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    np->cache_mode = mode;\n    // set this both when moving to and from cache_mode NONE.  this is to\n    // capture legacy pools that were set up before this flag existed.\n    np->flags |= pg_pool_t::FLAG_INCOMPLETE_CLONES;\n    ss << \"set cache-mode for pool '\" << poolstr\n\t<< \"' to \" << pg_pool_t::get_cache_mode_name(mode);\n    if (mode == pg_pool_t::CACHEMODE_NONE) {\n      const pg_pool_t *base_pool = osdmap.get_pg_pool(np->tier_of);\n      assert(base_pool);\n      if (base_pool->read_tier == pool_id ||\n\t  base_pool->write_tier == pool_id)\n\tss <<\" (WARNING: pool is still configured as read or write tier)\";\n    }\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier add-cache\") {\n    err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string tierpoolstr;\n    cmd_getval(cct, cmdmap, \"tierpool\", tierpoolstr);\n    int64_t tierpool_id = osdmap.lookup_pg_pool_name(tierpoolstr);\n    if (tierpool_id < 0) {\n      ss << \"unrecognized pool '\" << tierpoolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    const pg_pool_t *tp = osdmap.get_pg_pool(tierpool_id);\n    assert(tp);\n\n    if (!_check_become_tier(tierpool_id, tp, pool_id, p, &err, &ss)) {\n      goto reply;\n    }\n\n    int64_t size = 0;\n    if (!cmd_getval(cct, cmdmap, \"size\", size)) {\n      ss << \"unable to parse 'size' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"size\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    // make sure new tier is empty\n    const pool_stat_t *pstats =\n      mon->mgrstatmon()->get_pool_stat(tierpool_id);\n    if (pstats && pstats->stats.sum.num_objects != 0) {\n      ss << \"tier pool '\" << tierpoolstr << \"' is not empty\";\n      err = -ENOTEMPTY;\n      goto reply;\n    }\n    auto& modestr = g_conf->get_val<string>(\"osd_tier_default_cache_mode\");\n    pg_pool_t::cache_mode_t mode = pg_pool_t::get_cache_mode_from_str(modestr);\n    if (mode < 0) {\n      ss << \"osd tier cache default mode '\" << modestr << \"' is not a valid cache mode\";\n      err = -EINVAL;\n      goto reply;\n    }\n    HitSet::Params hsp;\n    auto& cache_hit_set_type =\n      g_conf->get_val<string>(\"osd_tier_default_cache_hit_set_type\");\n    if (cache_hit_set_type == \"bloom\") {\n      BloomHitSet::Params *bsp = new BloomHitSet::Params;\n      bsp->set_fpp(g_conf->get_val<double>(\"osd_pool_default_hit_set_bloom_fpp\"));\n      hsp = HitSet::Params(bsp);\n    } else if (cache_hit_set_type == \"explicit_hash\") {\n      hsp = HitSet::Params(new ExplicitHashHitSet::Params);\n    } else if (cache_hit_set_type == \"explicit_object\") {\n      hsp = HitSet::Params(new ExplicitObjectHitSet::Params);\n    } else {\n      ss << \"osd tier cache default hit set type '\"\n\t << cache_hit_set_type << \"' is not a known type\";\n      err = -EINVAL;\n      goto reply;\n    }\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    pg_pool_t *ntp = pending_inc.get_new_pool(tierpool_id, tp);\n    if (np->tiers.count(tierpool_id) || ntp->is_tier()) {\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    np->tiers.insert(tierpool_id);\n    np->read_tier = np->write_tier = tierpool_id;\n    np->set_snap_epoch(pending_inc.epoch); // tier will update to our snap info\n    np->set_last_force_op_resend(pending_inc.epoch);\n    ntp->set_last_force_op_resend(pending_inc.epoch);\n    ntp->tier_of = pool_id;\n    ntp->cache_mode = mode;\n    ntp->hit_set_count = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_hit_set_count\");\n    ntp->hit_set_period = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_hit_set_period\");\n    ntp->min_read_recency_for_promote = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_min_read_recency_for_promote\");\n    ntp->min_write_recency_for_promote = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_min_write_recency_for_promote\");\n    ntp->hit_set_grade_decay_rate = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_hit_set_grade_decay_rate\");\n    ntp->hit_set_search_last_n = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_hit_set_search_last_n\");\n    ntp->hit_set_params = hsp;\n    ntp->target_max_bytes = size;\n    ss << \"pool '\" << tierpoolstr << \"' is now (or already was) a cache tier of '\" << poolstr << \"'\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pool set-quota\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n\n    string field;\n    cmd_getval(cct, cmdmap, \"field\", field);\n    if (field != \"max_objects\" && field != \"max_bytes\") {\n      ss << \"unrecognized field '\" << field << \"'; should be 'max_bytes' or 'max_objects'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    // val could contain unit designations, so we treat as a string\n    string val;\n    cmd_getval(cct, cmdmap, \"val\", val);\n    string tss;\n    int64_t value;\n    if (field == \"max_objects\") {\n      value = strict_sistrtoll(val.c_str(), &tss);\n    } else if (field == \"max_bytes\") {\n      value = strict_iecstrtoll(val.c_str(), &tss);\n    } else {\n      assert(0 == \"unrecognized option\");\n    }\n    if (!tss.empty()) {\n      ss << \"error parsing value '\" << val << \"': \" << tss;\n      err = -EINVAL;\n      goto reply;\n    }\n\n    pg_pool_t *pi = pending_inc.get_new_pool(pool_id, osdmap.get_pg_pool(pool_id));\n    if (field == \"max_objects\") {\n      pi->quota_max_objects = value;\n    } else if (field == \"max_bytes\") {\n      pi->quota_max_bytes = value;\n    } else {\n      assert(0 == \"unrecognized option\");\n    }\n    ss << \"set-quota \" << field << \" = \" << value << \" for pool \" << poolstr;\n    rs = ss.str();\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pool application enable\" ||\n             prefix == \"osd pool application disable\" ||\n             prefix == \"osd pool application set\" ||\n             prefix == \"osd pool application rm\") {\n    err = prepare_command_pool_application(prefix, cmdmap, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err < 0)\n      goto reply;\n\n    getline(ss, rs);\n    wait_for_finished_proposal(\n      op, new Monitor::C_Command(mon, op, 0, rs, get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd force-create-pg\") {\n    pg_t pgid;\n    string pgidstr;\n    cmd_getval(cct, cmdmap, \"pgid\", pgidstr);\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.pg_exists(pgid)) {\n      ss << \"pg \" << pgid << \" should not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    if (sure != \"--yes-i-really-mean-it\") {\n      ss << \"This command will recreate a lost (as in data lost) PG with data in it, such that the cluster will give up ever trying to recover the lost data.  Do this only if you are certain that all copies of the PG are in fact lost and you are willing to accept that the data is permanently destroyed.  Pass --yes-i-really-mean-it to proceed.\";\n      err = -EPERM;\n      goto reply;\n    }\n    bool creating_now;\n    {\n      std::lock_guard<std::mutex> l(creating_pgs_lock);\n      auto emplaced = creating_pgs.pgs.emplace(pgid,\n\t\t\t\t\t       make_pair(osdmap.get_epoch(),\n\t\t\t\t\t\t\t ceph_clock_now()));\n      creating_now = emplaced.second;\n    }\n    if (creating_now) {\n      ss << \"pg \" << pgidstr << \" now creating, ok\";\n      err = 0;\n      goto update;\n    } else {\n      ss << \"pg \" << pgid << \" already creating\";\n      err = 0;\n      goto reply;\n    }\n  } else {\n    err = -EINVAL;\n  }\n\n reply:\n  getline(ss, rs);\n  if (err < 0 && rs.length() == 0)\n    rs = cpp_strerror(err);\n  mon->reply_command(op, err, rs, rdata, get_last_committed());\n  return ret;\n\n update:\n  getline(ss, rs);\n  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t    get_last_committed() + 1));\n  return true;\n\n wait:\n  wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n  return true;\n}\n\nbool OSDMonitor::preprocess_pool_op(MonOpRequestRef op) \n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  \n  if (m->fsid != mon->monmap->fsid) {\n    dout(0) << __func__ << \" drop message on fsid \" << m->fsid\n            << \" != \" << mon->monmap->fsid << \" for \" << *m << dendl;\n    _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n    return true;\n  }\n\n  if (m->op == POOL_OP_CREATE)\n    return preprocess_pool_op_create(op);\n\n  const pg_pool_t *p = osdmap.get_pg_pool(m->pool);\n  if (p == nullptr) {\n    dout(10) << \"attempt to operate on non-existent pool id \" << m->pool << dendl;\n    if (m->op == POOL_OP_DELETE) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n    } else {\n      _pool_op_reply(op, -ENOENT, osdmap.get_epoch());\n    }\n    return true;\n  }\n\n  // check if the snap and snapname exist\n  bool snap_exists = false;\n  if (p->snap_exists(m->name.c_str()))\n    snap_exists = true;\n\n  switch (m->op) {\n  case POOL_OP_CREATE_SNAP:\n    if (p->is_unmanaged_snaps_mode() || p->is_tier()) {\n      _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n      return true;\n    }\n    if (snap_exists) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_CREATE_UNMANAGED_SNAP:\n    if (p->is_pool_snaps_mode()) {\n      _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_DELETE_SNAP:\n    if (p->is_unmanaged_snaps_mode()) {\n      _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n      return true;\n    }\n    if (!snap_exists) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_DELETE_UNMANAGED_SNAP:\n    if (p->is_pool_snaps_mode()) {\n      _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n      return true;\n    }\n    if (p->is_removed_snap(m->snapid)) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_DELETE:\n    if (osdmap.lookup_pg_pool_name(m->name.c_str()) >= 0) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_AUID_CHANGE:\n    return false;\n  default:\n    ceph_abort();\n    break;\n  }\n\n  return false;\n}\n\nbool OSDMonitor::preprocess_pool_op_create(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  MonSession *session = m->get_session();\n  if (!session) {\n    _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n    return true;\n  }\n  if (!session->is_capable(\"osd\", MON_CAP_W)) {\n    dout(5) << \"attempt to create new pool without sufficient auid privileges!\"\n\t    << \"message: \" << *m  << std::endl\n\t    << \"caps: \" << session->caps << dendl;\n    _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n    return true;\n  }\n\n  int64_t pool = osdmap.lookup_pg_pool_name(m->name.c_str());\n  if (pool >= 0) {\n    _pool_op_reply(op, 0, osdmap.get_epoch());\n    return true;\n  }\n\n  return false;\n}\n\nbool OSDMonitor::prepare_pool_op(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  dout(10) << \"prepare_pool_op \" << *m << dendl;\n  if (m->op == POOL_OP_CREATE) {\n    return prepare_pool_op_create(op);\n  } else if (m->op == POOL_OP_DELETE) {\n    return prepare_pool_op_delete(op);\n  }\n\n  int ret = 0;\n  bool changed = false;\n\n  if (!osdmap.have_pg_pool(m->pool)) {\n    _pool_op_reply(op, -ENOENT, osdmap.get_epoch());\n    return false;\n  }\n\n  const pg_pool_t *pool = osdmap.get_pg_pool(m->pool);\n\n  switch (m->op) {\n    case POOL_OP_CREATE_SNAP:\n      if (pool->is_tier()) {\n        ret = -EINVAL;\n        _pool_op_reply(op, ret, osdmap.get_epoch());\n        return false;\n      }  // else, fall through\n    case POOL_OP_DELETE_SNAP:\n      if (!pool->is_unmanaged_snaps_mode()) {\n        bool snap_exists = pool->snap_exists(m->name.c_str());\n        if ((m->op == POOL_OP_CREATE_SNAP && snap_exists)\n          || (m->op == POOL_OP_DELETE_SNAP && !snap_exists)) {\n          ret = 0;\n        } else {\n          break;\n        }\n      } else {\n        ret = -EINVAL;\n      }\n      _pool_op_reply(op, ret, osdmap.get_epoch());\n      return false;\n\n    case POOL_OP_DELETE_UNMANAGED_SNAP:\n      // we won't allow removal of an unmanaged snapshot from a pool\n      // not in unmanaged snaps mode.\n      if (!pool->is_unmanaged_snaps_mode()) {\n        _pool_op_reply(op, -ENOTSUP, osdmap.get_epoch());\n        return false;\n      }\n      /* fall-thru */\n    case POOL_OP_CREATE_UNMANAGED_SNAP:\n      // but we will allow creating an unmanaged snapshot on any pool\n      // as long as it is not in 'pool' snaps mode.\n      if (pool->is_pool_snaps_mode()) {\n        _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n        return false;\n      }\n  }\n\n  // projected pool info\n  pg_pool_t pp;\n  if (pending_inc.new_pools.count(m->pool))\n    pp = pending_inc.new_pools[m->pool];\n  else\n    pp = *osdmap.get_pg_pool(m->pool);\n\n  bufferlist reply_data;\n\n  // pool snaps vs unmanaged snaps are mutually exclusive\n  switch (m->op) {\n  case POOL_OP_CREATE_SNAP:\n  case POOL_OP_DELETE_SNAP:\n    if (pp.is_unmanaged_snaps_mode()) {\n      ret = -EINVAL;\n      goto out;\n    }\n    break;\n\n  case POOL_OP_CREATE_UNMANAGED_SNAP:\n  case POOL_OP_DELETE_UNMANAGED_SNAP:\n    if (pp.is_pool_snaps_mode()) {\n      ret = -EINVAL;\n      goto out;\n    }\n  }\n\n  switch (m->op) {\n  case POOL_OP_CREATE_SNAP:\n    if (!pp.snap_exists(m->name.c_str())) {\n      pp.add_snap(m->name.c_str(), ceph_clock_now());\n      dout(10) << \"create snap in pool \" << m->pool << \" \" << m->name\n\t       << \" seq \" << pp.get_snap_epoch() << dendl;\n      changed = true;\n    }\n    break;\n\n  case POOL_OP_DELETE_SNAP:\n    {\n      snapid_t s = pp.snap_exists(m->name.c_str());\n      if (s) {\n\tpp.remove_snap(s);\n\tpending_inc.new_removed_snaps[m->pool].insert(s);\n\tchanged = true;\n      }\n    }\n    break;\n\n  case POOL_OP_CREATE_UNMANAGED_SNAP:\n    {\n      uint64_t snapid;\n      pp.add_unmanaged_snap(snapid);\n      encode(snapid, reply_data);\n      changed = true;\n    }\n    break;\n\n  case POOL_OP_DELETE_UNMANAGED_SNAP:\n    if (!pp.is_removed_snap(m->snapid)) {\n      if (m->snapid > pp.get_snap_seq()) {\n        _pool_op_reply(op, -ENOENT, osdmap.get_epoch());\n        return false;\n      }\n      pp.remove_unmanaged_snap(m->snapid);\n      pending_inc.new_removed_snaps[m->pool].insert(m->snapid);\n      changed = true;\n    }\n    break;\n\n  case POOL_OP_AUID_CHANGE:\n    if (pp.auid != m->auid) {\n      pp.auid = m->auid;\n      changed = true;\n    }\n    break;\n\n  default:\n    ceph_abort();\n    break;\n  }\n\n  if (changed) {\n    pp.set_snap_epoch(pending_inc.epoch);\n    pending_inc.new_pools[m->pool] = pp;\n  }\n\n out:\n  wait_for_finished_proposal(op, new OSDMonitor::C_PoolOp(this, op, ret, pending_inc.epoch, &reply_data));\n  return true;\n}\n\nbool OSDMonitor::prepare_pool_op_create(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  int err = prepare_new_pool(op);\n  wait_for_finished_proposal(op, new OSDMonitor::C_PoolOp(this, op, err, pending_inc.epoch));\n  return true;\n}\n\nint OSDMonitor::_check_remove_pool(int64_t pool_id, const pg_pool_t& pool,\n\t\t\t\t   ostream *ss)\n{\n  const string& poolstr = osdmap.get_pool_name(pool_id);\n\n  // If the Pool is in use by CephFS, refuse to delete it\n  FSMap const &pending_fsmap = mon->mdsmon()->get_pending_fsmap();\n  if (pending_fsmap.pool_in_use(pool_id)) {\n    *ss << \"pool '\" << poolstr << \"' is in use by CephFS\";\n    return -EBUSY;\n  }\n\n  if (pool.tier_of >= 0) {\n    *ss << \"pool '\" << poolstr << \"' is a tier of '\"\n\t<< osdmap.get_pool_name(pool.tier_of) << \"'\";\n    return -EBUSY;\n  }\n  if (!pool.tiers.empty()) {\n    *ss << \"pool '\" << poolstr << \"' has tiers\";\n    for(auto tier : pool.tiers) {\n      *ss << \" \" << osdmap.get_pool_name(tier);\n    }\n    return -EBUSY;\n  }\n\n  if (!g_conf->mon_allow_pool_delete) {\n    *ss << \"pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool\";\n    return -EPERM;\n  }\n\n  if (pool.has_flag(pg_pool_t::FLAG_NODELETE)) {\n    *ss << \"pool deletion is disabled; you must unset nodelete flag for the pool first\";\n    return -EPERM;\n  }\n\n  *ss << \"pool '\" << poolstr << \"' removed\";\n  return 0;\n}\n\n/**\n * Check if it is safe to add a tier to a base pool\n *\n * @return\n * True if the operation should proceed, false if we should abort here\n * (abort doesn't necessarily mean error, could be idempotency)\n */\nbool OSDMonitor::_check_become_tier(\n    const int64_t tier_pool_id, const pg_pool_t *tier_pool,\n    const int64_t base_pool_id, const pg_pool_t *base_pool,\n    int *err,\n    ostream *ss) const\n{\n  const std::string &tier_pool_name = osdmap.get_pool_name(tier_pool_id);\n  const std::string &base_pool_name = osdmap.get_pool_name(base_pool_id);\n\n  const FSMap &pending_fsmap = mon->mdsmon()->get_pending_fsmap();\n  if (pending_fsmap.pool_in_use(tier_pool_id)) {\n    *ss << \"pool '\" << tier_pool_name << \"' is in use by CephFS\";\n    *err = -EBUSY;\n    return false;\n  }\n\n  if (base_pool->tiers.count(tier_pool_id)) {\n    assert(tier_pool->tier_of == base_pool_id);\n    *err = 0;\n    *ss << \"pool '\" << tier_pool_name << \"' is now (or already was) a tier of '\"\n      << base_pool_name << \"'\";\n    return false;\n  }\n\n  if (base_pool->is_tier()) {\n    *ss << \"pool '\" << base_pool_name << \"' is already a tier of '\"\n      << osdmap.get_pool_name(base_pool->tier_of) << \"', \"\n      << \"multiple tiers are not yet supported.\";\n    *err = -EINVAL;\n    return false;\n  }\n\n  if (tier_pool->has_tiers()) {\n    *ss << \"pool '\" << tier_pool_name << \"' has following tier(s) already:\";\n    for (set<uint64_t>::iterator it = tier_pool->tiers.begin();\n         it != tier_pool->tiers.end(); ++it)\n      *ss << \"'\" << osdmap.get_pool_name(*it) << \"',\";\n    *ss << \" multiple tiers are not yet supported.\";\n    *err = -EINVAL;\n    return false;\n  }\n\n  if (tier_pool->is_tier()) {\n    *ss << \"tier pool '\" << tier_pool_name << \"' is already a tier of '\"\n       << osdmap.get_pool_name(tier_pool->tier_of) << \"'\";\n    *err = -EINVAL;\n    return false;\n  }\n\n  *err = 0;\n  return true;\n}\n\n\n/**\n * Check if it is safe to remove a tier from this base pool\n *\n * @return\n * True if the operation should proceed, false if we should abort here\n * (abort doesn't necessarily mean error, could be idempotency)\n */\nbool OSDMonitor::_check_remove_tier(\n    const int64_t base_pool_id, const pg_pool_t *base_pool,\n    const pg_pool_t *tier_pool,\n    int *err, ostream *ss) const\n{\n  const std::string &base_pool_name = osdmap.get_pool_name(base_pool_id);\n\n  // Apply CephFS-specific checks\n  const FSMap &pending_fsmap = mon->mdsmon()->get_pending_fsmap();\n  if (pending_fsmap.pool_in_use(base_pool_id)) {\n    if (base_pool->is_erasure() && !base_pool->allows_ecoverwrites()) {\n      // If the underlying pool is erasure coded and does not allow EC\n      // overwrites, we can't permit the removal of the replicated tier that\n      // CephFS relies on to access it\n      *ss << \"pool '\" << base_pool_name <<\n          \"' does not allow EC overwrites and is in use by CephFS\"\n          \" via its tier\";\n      *err = -EBUSY;\n      return false;\n    }\n\n    if (tier_pool && tier_pool->cache_mode == pg_pool_t::CACHEMODE_WRITEBACK) {\n      *ss << \"pool '\" << base_pool_name << \"' is in use by CephFS, and this \"\n             \"tier is still in use as a writeback cache.  Change the cache \"\n             \"mode and flush the cache before removing it\";\n      *err = -EBUSY;\n      return false;\n    }\n  }\n\n  *err = 0;\n  return true;\n}\n\nint OSDMonitor::_prepare_remove_pool(\n  int64_t pool, ostream *ss, bool no_fake)\n{\n  dout(10) << __func__ << \" \" << pool << dendl;\n  const pg_pool_t *p = osdmap.get_pg_pool(pool);\n  int r = _check_remove_pool(pool, *p, ss);\n  if (r < 0)\n    return r;\n\n  auto new_pool = pending_inc.new_pools.find(pool);\n  if (new_pool != pending_inc.new_pools.end()) {\n    // if there is a problem with the pending info, wait and retry\n    // this op.\n    const auto& p = new_pool->second;\n    int r = _check_remove_pool(pool, p, ss);\n    if (r < 0)\n      return -EAGAIN;\n  }\n\n  if (pending_inc.old_pools.count(pool)) {\n    dout(10) << __func__ << \" \" << pool << \" already pending removal\"\n\t     << dendl;\n    return 0;\n  }\n\n  if (g_conf->mon_fake_pool_delete && !no_fake) {\n    string old_name = osdmap.get_pool_name(pool);\n    string new_name = old_name + \".\" + stringify(pool) + \".DELETED\";\n    dout(1) << __func__ << \" faking pool deletion: renaming \" << pool << \" \"\n\t    << old_name << \" -> \" << new_name << dendl;\n    pending_inc.new_pool_names[pool] = new_name;\n    return 0;\n  }\n\n  // remove\n  pending_inc.old_pools.insert(pool);\n\n  // remove any pg_temp mappings for this pool\n  for (auto p = osdmap.pg_temp->begin();\n       p != osdmap.pg_temp->end();\n       ++p) {\n    if (p->first.pool() == pool) {\n      dout(10) << __func__ << \" \" << pool << \" removing obsolete pg_temp \"\n\t       << p->first << dendl;\n      pending_inc.new_pg_temp[p->first].clear();\n    }\n  }\n  // remove any primary_temp mappings for this pool\n  for (auto p = osdmap.primary_temp->begin();\n      p != osdmap.primary_temp->end();\n      ++p) {\n    if (p->first.pool() == pool) {\n      dout(10) << __func__ << \" \" << pool\n               << \" removing obsolete primary_temp\" << p->first << dendl;\n      pending_inc.new_primary_temp[p->first] = -1;\n    }\n  }\n  // remove any pg_upmap mappings for this pool\n  for (auto& p : osdmap.pg_upmap) {\n    if (p.first.pool() == pool) {\n      dout(10) << __func__ << \" \" << pool\n               << \" removing obsolete pg_upmap \"\n               << p.first << dendl;\n      pending_inc.old_pg_upmap.insert(p.first);\n    }\n  }\n  // remove any pending pg_upmap mappings for this pool\n  {\n    auto it = pending_inc.new_pg_upmap.begin();\n    while (it != pending_inc.new_pg_upmap.end()) {\n      if (it->first.pool() == pool) {\n        dout(10) << __func__ << \" \" << pool\n                 << \" removing pending pg_upmap \"\n                 << it->first << dendl;\n        it = pending_inc.new_pg_upmap.erase(it);\n      } else {\n        it++;\n      }\n    }\n  }\n  // remove any pg_upmap_items mappings for this pool\n  for (auto& p : osdmap.pg_upmap_items) {\n    if (p.first.pool() == pool) {\n      dout(10) << __func__ << \" \" << pool\n               << \" removing obsolete pg_upmap_items \" << p.first\n               << dendl;\n      pending_inc.old_pg_upmap_items.insert(p.first);\n    }\n  }\n  // remove any pending pg_upmap mappings for this pool\n  {\n    auto it = pending_inc.new_pg_upmap_items.begin();\n    while (it != pending_inc.new_pg_upmap_items.end()) {\n      if (it->first.pool() == pool) {\n        dout(10) << __func__ << \" \" << pool\n                 << \" removing pending pg_upmap_items \"\n                 << it->first << dendl;\n        it = pending_inc.new_pg_upmap_items.erase(it);\n      } else {\n        it++;\n      }\n    }\n  }\n\n  // remove any choose_args for this pool\n  CrushWrapper newcrush;\n  _get_pending_crush(newcrush);\n  if (newcrush.have_choose_args(pool)) {\n    dout(10) << __func__ << \" removing choose_args for pool \" << pool << dendl;\n    newcrush.rm_choose_args(pool);\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n  }\n  return 0;\n}\n\nint OSDMonitor::_prepare_rename_pool(int64_t pool, string newname)\n{\n  dout(10) << \"_prepare_rename_pool \" << pool << dendl;\n  if (pending_inc.old_pools.count(pool)) {\n    dout(10) << \"_prepare_rename_pool \" << pool << \" pending removal\" << dendl;\n    return -ENOENT;\n  }\n  for (map<int64_t,string>::iterator p = pending_inc.new_pool_names.begin();\n       p != pending_inc.new_pool_names.end();\n       ++p) {\n    if (p->second == newname && p->first != pool) {\n      return -EEXIST;\n    }\n  }\n\n  pending_inc.new_pool_names[pool] = newname;\n  return 0;\n}\n\nbool OSDMonitor::prepare_pool_op_delete(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  ostringstream ss;\n  int ret = _prepare_remove_pool(m->pool, &ss, false);\n  if (ret == -EAGAIN) {\n    wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n    return true;\n  }\n  if (ret < 0)\n    dout(10) << __func__ << \" got \" << ret << \" \" << ss.str() << dendl;\n  wait_for_finished_proposal(op, new OSDMonitor::C_PoolOp(this, op, ret,\n\t\t\t\t\t\t      pending_inc.epoch));\n  return true;\n}\n\nvoid OSDMonitor::_pool_op_reply(MonOpRequestRef op,\n                                int ret, epoch_t epoch, bufferlist *blp)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  dout(20) << \"_pool_op_reply \" << ret << dendl;\n  MPoolOpReply *reply = new MPoolOpReply(m->fsid, m->get_tid(),\n\t\t\t\t\t ret, epoch, get_last_committed(), blp);\n  mon->send_reply(op, reply);\n}\n", "// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- \n// vim: ts=8 sw=2 smarttab\n/*\n * Ceph - scalable distributed file system\n *\n * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>\n * Copyright (C) 2013,2014 Cloudwatt <libre.licensing@cloudwatt.com>\n *\n * Author: Loic Dachary <loic@dachary.org>\n *\n * This is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License version 2.1, as published by the Free Software \n * Foundation.  See file COPYING.\n * \n */\n\n/* Object Store Device (OSD) Monitor\n */\n\n#ifndef CEPH_OSDMONITOR_H\n#define CEPH_OSDMONITOR_H\n\n#include <map>\n#include <set>\n\n#include \"include/types.h\"\n#include \"include/encoding.h\"\n#include \"common/simple_cache.hpp\"\n#include \"msg/Messenger.h\"\n\n#include \"osd/OSDMap.h\"\n#include \"osd/OSDMapMapping.h\"\n\n#include \"CreatingPGs.h\"\n#include \"PaxosService.h\"\n\nclass Monitor;\nclass PGMap;\nclass MonSession;\nclass MOSDMap;\n\n#include \"erasure-code/ErasureCodeInterface.h\"\n#include \"mon/MonOpRequest.h\"\n#include <boost/functional/hash.hpp>\n// re-include our assert to clobber the system one; fix dout:\n#include \"include/assert.h\"\n\n/// information about a particular peer's failure reports for one osd\nstruct failure_reporter_t {\n  utime_t failed_since;     ///< when they think it failed\n  MonOpRequestRef op;       ///< failure op request\n\n  failure_reporter_t() {}\n  explicit failure_reporter_t(utime_t s) : failed_since(s) {}\n  ~failure_reporter_t() { }\n};\n\n/// information about all failure reports for one osd\nstruct failure_info_t {\n  map<int, failure_reporter_t> reporters;  ///< reporter -> failed_since etc\n  utime_t max_failed_since;                ///< most recent failed_since\n\n  failure_info_t() {}\n\n  utime_t get_failed_since() {\n    if (max_failed_since == utime_t() && !reporters.empty()) {\n      // the old max must have canceled; recalculate.\n      for (map<int, failure_reporter_t>::iterator p = reporters.begin();\n\t   p != reporters.end();\n\t   ++p)\n\tif (p->second.failed_since > max_failed_since)\n\t  max_failed_since = p->second.failed_since;\n    }\n    return max_failed_since;\n  }\n\n  // set the message for the latest report.  return any old op request we had,\n  // if any, so we can discard it.\n  MonOpRequestRef add_report(int who, utime_t failed_since,\n\t\t\t     MonOpRequestRef op) {\n    map<int, failure_reporter_t>::iterator p = reporters.find(who);\n    if (p == reporters.end()) {\n      if (max_failed_since < failed_since)\n\tmax_failed_since = failed_since;\n      p = reporters.insert(map<int, failure_reporter_t>::value_type(who, failure_reporter_t(failed_since))).first;\n    }\n\n    MonOpRequestRef ret = p->second.op;\n    p->second.op = op;\n    return ret;\n  }\n\n  void take_report_messages(list<MonOpRequestRef>& ls) {\n    for (map<int, failure_reporter_t>::iterator p = reporters.begin();\n\t p != reporters.end();\n\t ++p) {\n      if (p->second.op) {\n\tls.push_back(p->second.op);\n        p->second.op.reset();\n      }\n    }\n  }\n\n  MonOpRequestRef cancel_report(int who) {\n    map<int, failure_reporter_t>::iterator p = reporters.find(who);\n    if (p == reporters.end())\n      return MonOpRequestRef();\n    MonOpRequestRef ret = p->second.op;\n    reporters.erase(p);\n    return ret;\n  }\n};\n\n\nclass LastEpochClean {\n  struct Lec {\n    vector<epoch_t> epoch_by_pg;\n    ps_t next_missing = 0;\n    epoch_t floor = std::numeric_limits<epoch_t>::max();\n    void report(ps_t pg, epoch_t last_epoch_clean);\n  };\n  std::map<uint64_t, Lec> report_by_pool;\npublic:\n  void report(const pg_t& pg, epoch_t last_epoch_clean);\n  void remove_pool(uint64_t pool);\n  epoch_t get_lower_bound(const OSDMap& latest) const;\n};\n\n\nstruct osdmap_manifest_t {\n  // all the maps we have pinned -- i.e., won't be removed unless\n  // they are inside a trim interval.\n  set<version_t> pinned;\n\n  osdmap_manifest_t() {}\n\n  version_t get_last_pinned() const\n  {\n    set<version_t>::const_reverse_iterator it = pinned.crbegin();\n    if (it == pinned.crend()) {\n      return 0;\n    }\n    return *it;\n  }\n\n  version_t get_first_pinned() const\n  {\n    set<version_t>::const_iterator it = pinned.cbegin();\n    if (it == pinned.cend()) {\n      return 0;\n    }\n    return *it;\n  }\n\n  bool is_pinned(version_t v) const\n  {\n    return pinned.find(v) != pinned.end();\n  }\n\n  void pin(version_t v)\n  {\n    pinned.insert(v);\n  }\n\n  version_t get_lower_closest_pinned(version_t v) const {\n    set<version_t>::const_iterator p = pinned.lower_bound(v);\n    if (p == pinned.cend()) {\n      return 0;\n    } else if (*p > v) {\n      if (p == pinned.cbegin()) {\n        return 0;\n      }\n      --p;\n    }\n    return *p;\n  }\n\n  void encode(bufferlist& bl) const\n  {\n    ENCODE_START(1, 1, bl);\n    encode(pinned, bl);\n    ENCODE_FINISH(bl);\n  }\n\n  void decode(bufferlist::const_iterator& bl)\n  {\n    DECODE_START(1, bl);\n    decode(pinned, bl);\n    DECODE_FINISH(bl);\n  }\n\n  void decode(bufferlist& bl) {\n    auto p = bl.cbegin();\n    decode(p);\n  }\n\n  void dump(Formatter *f) {\n    f->dump_unsigned(\"first_pinned\", get_first_pinned());\n    f->dump_unsigned(\"last_pinned\", get_last_pinned());\n    f->open_array_section(\"pinned_maps\");\n    for (auto& i : pinned) {\n      f->dump_unsigned(\"epoch\", i);\n    }\n    f->close_section();\n }\n};\nWRITE_CLASS_ENCODER(osdmap_manifest_t);\n\nclass OSDMonitor : public PaxosService {\n  CephContext *cct;\n\npublic:\n  OSDMap osdmap;\n\n  // [leader]\n  OSDMap::Incremental pending_inc;\n  map<int, bufferlist> pending_metadata;\n  set<int>             pending_metadata_rm;\n  map<int, failure_info_t> failure_info;\n  map<int,utime_t>    down_pending_out;  // osd down -> out\n\n  map<int,double> osd_weight;\n\n  using osdmap_key_t = std::pair<version_t, uint64_t>;\n  using osdmap_cache_t = SimpleLRU<osdmap_key_t,\n                                   bufferlist,\n                                   std::less<osdmap_key_t>,\n                                   boost::hash<osdmap_key_t>>;\n  osdmap_cache_t inc_osd_cache;\n  osdmap_cache_t full_osd_cache;\n\n  bool has_osdmap_manifest;\n  osdmap_manifest_t osdmap_manifest;\n\n  bool check_failures(utime_t now);\n  bool check_failure(utime_t now, int target_osd, failure_info_t& fi);\n  void force_failure(int target_osd, int by);\n\n  bool _have_pending_crush();\n  CrushWrapper &_get_stable_crush();\n  void _get_pending_crush(CrushWrapper& newcrush);\n\n  enum FastReadType {\n    FAST_READ_OFF,\n    FAST_READ_ON,\n    FAST_READ_DEFAULT\n  };\n\n  // svc\npublic:\n  void create_initial() override;\n  void get_store_prefixes(std::set<string>& s) const override;\n\nprivate:\n  void update_from_paxos(bool *need_bootstrap) override;\n  void create_pending() override;  // prepare a new pending\n  void encode_pending(MonitorDBStore::TransactionRef t) override;\n  void on_active() override;\n  void on_restart() override;\n  void on_shutdown() override;\n\n  /* osdmap full map prune */\n  void load_osdmap_manifest();\n  bool should_prune() const;\n  void _prune_update_trimmed(\n      MonitorDBStore::TransactionRef tx,\n      version_t first);\n  void prune_init();\n  bool _prune_sanitize_options() const;\n  bool is_prune_enabled() const;\n  bool is_prune_supported() const;\n  bool do_prune(MonitorDBStore::TransactionRef tx);\n\n  /**\n   * we haven't delegated full version stashing to paxosservice for some time\n   * now, making this function useless in current context.\n   */\n  void encode_full(MonitorDBStore::TransactionRef t) override { }\n  /**\n   * do not let paxosservice periodically stash full osdmaps, or we will break our\n   * locally-managed full maps.  (update_from_paxos loads the latest and writes them\n   * out going forward from there, but if we just synced that may mean we skip some.)\n   */\n  bool should_stash_full() override {\n    return false;\n  }\n\n  /**\n   * hook into trim to include the oldest full map in the trim transaction\n   *\n   * This ensures that anyone post-sync will have enough to rebuild their\n   * full osdmaps.\n   */\n  void encode_trim_extra(MonitorDBStore::TransactionRef tx, version_t first) override;\n\n  void update_msgr_features();\n  int check_cluster_features(uint64_t features, stringstream &ss);\n  /**\n   * check if the cluster supports the features required by the\n   * given crush map. Outputs the daemons which don't support it\n   * to the stringstream.\n   *\n   * @returns true if the map is passable, false otherwise\n   */\n  bool validate_crush_against_features(const CrushWrapper *newcrush,\n                                      stringstream &ss);\n  void check_osdmap_subs();\n  void share_map_with_random_osd();\n\n  Mutex prime_pg_temp_lock = {\"OSDMonitor::prime_pg_temp_lock\"};\n  struct PrimeTempJob : public ParallelPGMapper::Job {\n    OSDMonitor *osdmon;\n    PrimeTempJob(const OSDMap& om, OSDMonitor *m)\n      : ParallelPGMapper::Job(&om), osdmon(m) {}\n    void process(int64_t pool, unsigned ps_begin, unsigned ps_end) override {\n      for (unsigned ps = ps_begin; ps < ps_end; ++ps) {\n\tpg_t pgid(ps, pool);\n\tosdmon->prime_pg_temp(*osdmap, pgid);\n      }\n    }\n    void complete() override {}\n  };\n  void maybe_prime_pg_temp();\n  void prime_pg_temp(const OSDMap& next, pg_t pgid);\n\n  ParallelPGMapper mapper;                        ///< for background pg work\n  OSDMapMapping mapping;                          ///< pg <-> osd mappings\n  unique_ptr<ParallelPGMapper::Job> mapping_job;  ///< background mapping job\n  void start_mapping();\n\n  void update_logger();\n\n  void handle_query(PaxosServiceMessage *m);\n  bool preprocess_query(MonOpRequestRef op) override;  // true if processed.\n  bool prepare_update(MonOpRequestRef op) override;\n  bool should_propose(double &delay) override;\n\n  version_t get_trim_to() const override;\n\n  bool can_mark_down(int o);\n  bool can_mark_up(int o);\n  bool can_mark_out(int o);\n  bool can_mark_in(int o);\n\n  // ...\n  MOSDMap *build_latest_full(uint64_t features);\n  MOSDMap *build_incremental(epoch_t first, epoch_t last, uint64_t features);\n  void send_full(MonOpRequestRef op);\n  void send_incremental(MonOpRequestRef op, epoch_t first);\npublic:\n  // @param req an optional op request, if the osdmaps are replies to it. so\n  //            @c Monitor::send_reply() can mark_event with it.\n  void send_incremental(epoch_t first, MonSession *session, bool onetime,\n\t\t\tMonOpRequestRef req = MonOpRequestRef());\n\nprivate:\n  void print_utilization(ostream &out, Formatter *f, bool tree) const;\n\n  bool check_source(PaxosServiceMessage *m, uuid_d fsid);\n \n  bool preprocess_get_osdmap(MonOpRequestRef op);\n\n  bool preprocess_mark_me_down(MonOpRequestRef op);\n\n  friend class C_AckMarkedDown;\n  bool preprocess_failure(MonOpRequestRef op);\n  bool prepare_failure(MonOpRequestRef op);\n  bool prepare_mark_me_down(MonOpRequestRef op);\n  void process_failures();\n  void take_all_failures(list<MonOpRequestRef>& ls);\n\n  bool preprocess_full(MonOpRequestRef op);\n  bool prepare_full(MonOpRequestRef op);\n\n  bool preprocess_boot(MonOpRequestRef op);\n  bool prepare_boot(MonOpRequestRef op);\n  void _booted(MonOpRequestRef op, bool logit);\n\n  void update_up_thru(int from, epoch_t up_thru);\n  bool preprocess_alive(MonOpRequestRef op);\n  bool prepare_alive(MonOpRequestRef op);\n  void _reply_map(MonOpRequestRef op, epoch_t e);\n\n  bool preprocess_pgtemp(MonOpRequestRef op);\n  bool prepare_pgtemp(MonOpRequestRef op);\n\n  bool preprocess_pg_created(MonOpRequestRef op);\n  bool prepare_pg_created(MonOpRequestRef op);\n\n  int _check_remove_pool(int64_t pool_id, const pg_pool_t &pool, ostream *ss);\n  bool _check_become_tier(\n      int64_t tier_pool_id, const pg_pool_t *tier_pool,\n      int64_t base_pool_id, const pg_pool_t *base_pool,\n      int *err, ostream *ss) const;\n  bool _check_remove_tier(\n      int64_t base_pool_id, const pg_pool_t *base_pool, const pg_pool_t *tier_pool,\n      int *err, ostream *ss) const;\n\n  int _prepare_remove_pool(int64_t pool, ostream *ss, bool no_fake);\n  int _prepare_rename_pool(int64_t pool, string newname);\n\n  bool preprocess_pool_op (MonOpRequestRef op);\n  bool preprocess_pool_op_create (MonOpRequestRef op);\n  bool prepare_pool_op (MonOpRequestRef op);\n  bool prepare_pool_op_create (MonOpRequestRef op);\n  bool prepare_pool_op_delete(MonOpRequestRef op);\n  int crush_rename_bucket(const string& srcname,\n\t\t\t  const string& dstname,\n\t\t\t  ostream *ss);\n  void check_legacy_ec_plugin(const string& plugin, \n\t\t\t      const string& profile) const;\n  int normalize_profile(const string& profilename, \n\t\t\tErasureCodeProfile &profile,\n\t\t\tbool force,\n\t\t\tostream *ss);\n  int crush_rule_create_erasure(const string &name,\n\t\t\t\tconst string &profile,\n\t\t\t\tint *rule,\n\t\t\t\tostream *ss);\n  int get_crush_rule(const string &rule_name,\n\t\t\tint *crush_rule,\n\t\t\tostream *ss);\n  int get_erasure_code(const string &erasure_code_profile,\n\t\t       ErasureCodeInterfaceRef *erasure_code,\n\t\t       ostream *ss) const;\n  int prepare_pool_crush_rule(const unsigned pool_type,\n\t\t\t\t const string &erasure_code_profile,\n\t\t\t\t const string &rule_name,\n\t\t\t\t int *crush_rule,\n\t\t\t\t ostream *ss);\n  bool erasure_code_profile_in_use(\n    const mempool::osdmap::map<int64_t, pg_pool_t> &pools,\n    const string &profile,\n    ostream *ss);\n  int parse_erasure_code_profile(const vector<string> &erasure_code_profile,\n\t\t\t\t map<string,string> *erasure_code_profile_map,\n\t\t\t\t ostream *ss);\n  int prepare_pool_size(const unsigned pool_type,\n\t\t\tconst string &erasure_code_profile,\n\t\t\tunsigned *size, unsigned *min_size,\n\t\t\tostream *ss);\n  int prepare_pool_stripe_width(const unsigned pool_type,\n\t\t\t\tconst string &erasure_code_profile,\n\t\t\t\tunsigned *stripe_width,\n\t\t\t\tostream *ss);\n  int check_pg_num(int64_t pool, int pg_num, int size, ostream* ss);\n  int prepare_new_pool(string& name, uint64_t auid,\n\t\t       int crush_rule,\n\t\t       const string &crush_rule_name,\n                       unsigned pg_num, unsigned pgp_num,\n\t\t       const string &erasure_code_profile,\n                       const unsigned pool_type,\n                       const uint64_t expected_num_objects,\n                       FastReadType fast_read,\n\t\t       ostream *ss);\n  int prepare_new_pool(MonOpRequestRef op);\n\n  void set_pool_flags(int64_t pool_id, uint64_t flags);\n  void clear_pool_flags(int64_t pool_id, uint64_t flags);\n  bool update_pools_status();\n\n  string make_snap_epoch_key(int64_t pool, epoch_t epoch);\n  string make_snap_key(int64_t pool, snapid_t snap);\n  string make_snap_key_value(int64_t pool, snapid_t snap, snapid_t num,\n\t\t\t     epoch_t epoch, bufferlist *v);\n  string make_snap_purged_key(int64_t pool, snapid_t snap);\n  string make_snap_purged_key_value(int64_t pool, snapid_t snap, snapid_t num,\n\t\t\t\t    epoch_t epoch, bufferlist *v);\n  bool try_prune_purged_snaps();\n  int lookup_pruned_snap(int64_t pool, snapid_t snap,\n\t\t\t snapid_t *begin, snapid_t *end);\n\n  bool prepare_set_flag(MonOpRequestRef op, int flag);\n  bool prepare_unset_flag(MonOpRequestRef op, int flag);\n\n  void _pool_op_reply(MonOpRequestRef op,\n                      int ret, epoch_t epoch, bufferlist *blp=NULL);\n\n  struct C_Booted : public C_MonOp {\n    OSDMonitor *cmon;\n    bool logit;\n    C_Booted(OSDMonitor *cm, MonOpRequestRef op_, bool l=true) :\n      C_MonOp(op_), cmon(cm), logit(l) {}\n    void _finish(int r) override {\n      if (r >= 0)\n\tcmon->_booted(op, logit);\n      else if (r == -ECANCELED)\n        return;\n      else if (r == -EAGAIN)\n        cmon->dispatch(op);\n      else\n\tassert(0 == \"bad C_Booted return value\");\n    }\n  };\n\n  struct C_ReplyMap : public C_MonOp {\n    OSDMonitor *osdmon;\n    epoch_t e;\n    C_ReplyMap(OSDMonitor *o, MonOpRequestRef op_, epoch_t ee)\n      : C_MonOp(op_), osdmon(o), e(ee) {}\n    void _finish(int r) override {\n      if (r >= 0)\n\tosdmon->_reply_map(op, e);\n      else if (r == -ECANCELED)\n        return;\n      else if (r == -EAGAIN)\n\tosdmon->dispatch(op);\n      else\n\tassert(0 == \"bad C_ReplyMap return value\");\n    }    \n  };\n  struct C_PoolOp : public C_MonOp {\n    OSDMonitor *osdmon;\n    int replyCode;\n    int epoch;\n    bufferlist reply_data;\n    C_PoolOp(OSDMonitor * osd, MonOpRequestRef op_, int rc, int e, bufferlist *rd=NULL) :\n      C_MonOp(op_), osdmon(osd), replyCode(rc), epoch(e) {\n      if (rd)\n\treply_data = *rd;\n    }\n    void _finish(int r) override {\n      if (r >= 0)\n\tosdmon->_pool_op_reply(op, replyCode, epoch, &reply_data);\n      else if (r == -ECANCELED)\n        return;\n      else if (r == -EAGAIN)\n\tosdmon->dispatch(op);\n      else\n\tassert(0 == \"bad C_PoolOp return value\");\n    }\n  };\n\n  bool preprocess_remove_snaps(MonOpRequestRef op);\n  bool prepare_remove_snaps(MonOpRequestRef op);\n\n  int load_metadata(int osd, map<string, string>& m, ostream *err);\n  void count_metadata(const string& field, Formatter *f);\n\n  void reencode_incremental_map(bufferlist& bl, uint64_t features);\n  void reencode_full_map(bufferlist& bl, uint64_t features);\npublic:\n  void count_metadata(const string& field, map<string,int> *out);\nprotected:\n  int get_osd_objectstore_type(int osd, std::string *type);\n  bool is_pool_currently_all_bluestore(int64_t pool_id, const pg_pool_t &pool,\n\t\t\t\t       ostream *err);\n\n  // when we last received PG stats from each osd\n  map<int,utime_t> last_osd_report;\n  // TODO: use last_osd_report to store the osd report epochs, once we don't\n  //       need to upgrade from pre-luminous releases.\n  map<int,epoch_t> osd_epochs;\n  LastEpochClean last_epoch_clean;\n  bool preprocess_beacon(MonOpRequestRef op);\n  bool prepare_beacon(MonOpRequestRef op);\n  epoch_t get_min_last_epoch_clean() const;\n\n  friend class C_UpdateCreatingPGs;\n  std::map<int, std::map<epoch_t, std::set<spg_t>>> creating_pgs_by_osd_epoch;\n  std::vector<pg_t> pending_created_pgs;\n  // the epoch when the pg mapping was calculated\n  epoch_t creating_pgs_epoch = 0;\n  creating_pgs_t creating_pgs;\n  mutable std::mutex creating_pgs_lock;\n\n  creating_pgs_t update_pending_pgs(const OSDMap::Incremental& inc,\n\t\t\t\t    const OSDMap& nextmap);\n  unsigned scan_for_creating_pgs(\n    const mempool::osdmap::map<int64_t,pg_pool_t>& pools,\n    const mempool::osdmap::set<int64_t>& removed_pools,\n    utime_t modified,\n    creating_pgs_t* creating_pgs) const;\n  pair<int32_t, pg_t> get_parent_pg(pg_t pgid) const;\n  void update_creating_pgs();\n  void check_pg_creates_subs();\n  epoch_t send_pg_creates(int osd, Connection *con, epoch_t next) const;\n\n  int32_t _allocate_osd_id(int32_t* existing_id);\n\npublic:\n  OSDMonitor(CephContext *cct, Monitor *mn, Paxos *p, const string& service_name);\n\n  void tick() override;  // check state, take actions\n\n  bool preprocess_command(MonOpRequestRef op);\n  bool prepare_command(MonOpRequestRef op);\n  bool prepare_command_impl(MonOpRequestRef op, const cmdmap_t& cmdmap);\n\n  int validate_osd_create(\n      const int32_t id,\n      const uuid_d& uuid,\n      const bool check_osd_exists,\n      int32_t* existing_id,\n      stringstream& ss);\n  int prepare_command_osd_create(\n      const int32_t id,\n      const uuid_d& uuid,\n      int32_t* existing_id,\n      stringstream& ss);\n  void do_osd_create(const int32_t id, const uuid_d& uuid,\n\t\t     const string& device_class,\n\t\t     int32_t* new_id);\n  int prepare_command_osd_purge(int32_t id, stringstream& ss);\n  int prepare_command_osd_destroy(int32_t id, stringstream& ss);\n  int _prepare_command_osd_crush_remove(\n      CrushWrapper &newcrush,\n      int32_t id,\n      int32_t ancestor,\n      bool has_ancestor,\n      bool unlink_only);\n  void do_osd_crush_remove(CrushWrapper& newcrush);\n  int prepare_command_osd_crush_remove(\n      CrushWrapper &newcrush,\n      int32_t id,\n      int32_t ancestor,\n      bool has_ancestor,\n      bool unlink_only);\n  int prepare_command_osd_remove(int32_t id);\n  int prepare_command_osd_new(\n      MonOpRequestRef op,\n      const cmdmap_t& cmdmap,\n      const map<string,string>& secrets,\n      stringstream &ss,\n      Formatter *f);\n\n  int prepare_command_pool_set(const cmdmap_t& cmdmap,\n                               stringstream& ss);\n  int prepare_command_pool_application(const string &prefix,\n                                       const cmdmap_t& cmdmap,\n                                       stringstream& ss);\n\n  bool handle_osd_timeouts(const utime_t &now,\n\t\t\t   std::map<int,utime_t> &last_osd_report);\n\n  void send_latest(MonOpRequestRef op, epoch_t start=0);\n  void send_latest_now_nodelete(MonOpRequestRef op, epoch_t start=0) {\n    op->mark_osdmon_event(__func__);\n    send_incremental(op, start);\n  }\n\n  void get_removed_snaps_range(\n    epoch_t start, epoch_t end,\n    mempool::osdmap::map<int64_t,OSDMap::snap_interval_set_t> *gap_removed_snaps);\n\n  int get_version(version_t ver, bufferlist& bl) override;\n  int get_version(version_t ver, uint64_t feature, bufferlist& bl);\n\n  int get_version_full(version_t ver, uint64_t feature, bufferlist& bl);\n  int get_version_full(version_t ver, bufferlist& bl) override;\n  int get_inc(version_t ver, OSDMap::Incremental& inc);\n  int get_full_from_pinned_map(version_t ver, bufferlist& bl);\n\n  epoch_t blacklist(const entity_addrvec_t& av, utime_t until);\n  epoch_t blacklist(const entity_addr_t& a, utime_t until);\n\n  void dump_info(Formatter *f);\n  int dump_osd_metadata(int osd, Formatter *f, ostream *err);\n  void print_nodes(Formatter *f);\n\n  void check_osdmap_sub(Subscription *sub);\n  void check_pg_creates_sub(Subscription *sub);\n\n  void do_application_enable(int64_t pool_id, const std::string &app_name,\n\t\t\t     const std::string &app_key=\"\",\n\t\t\t     const std::string &app_value=\"\");\n\n  void add_flag(int flag) {\n    if (!(osdmap.flags & flag)) {\n      if (pending_inc.new_flags < 0)\n\tpending_inc.new_flags = osdmap.flags;\n      pending_inc.new_flags |= flag;\n    }\n  }\n\n  void remove_flag(int flag) {\n    if(osdmap.flags & flag) {\n      if (pending_inc.new_flags < 0)\n\tpending_inc.new_flags = osdmap.flags;\n      pending_inc.new_flags &= ~flag;\n    }\n  }\n};\n\n#endif\n", "# cython: embedsignature=True\n\"\"\"\nThis module is a thin wrapper around librados.\n\nError codes from librados are turned into exceptions that subclass\n:class:`Error`. Almost all methods may raise :class:`Error(the base class of all rados exceptions), :class:`PermissionError`\n(the base class of all rados exceptions), :class:`PermissionError`\nand :class:`IOError`, in addition to those documented for the\nmethod.\n\"\"\"\n# Copyright 2011 Josh Durgin\n# Copyright 2011, Hannu Valtonen <hannu.valtonen@ormod.com>\n# Copyright 2015 Hector Martin <marcan@marcan.st>\n# Copyright 2016 Mehdi Abaakouk <sileht@redhat.com>\n\nfrom cpython cimport PyObject, ref\nfrom cpython.pycapsule cimport *\nfrom libc cimport errno\nfrom libc.stdint cimport *\nfrom libc.stdlib cimport malloc, realloc, free\n\nimport sys\nimport threading\nimport time\n\nfrom collections import Callable\nfrom datetime import datetime\nfrom functools import partial, wraps\nfrom itertools import chain\n\n# Are we running Python 2.x\nif sys.version_info[0] < 3:\n    str_type = basestring\nelse:\n    str_type = str\n\n\ncdef extern from \"Python.h\":\n    # These are in cpython/string.pxd, but use \"object\" types instead of\n    # PyObject*, which invokes assumptions in cpython that we need to\n    # legitimately break to implement zero-copy string buffers in Ioctx.read().\n    # This is valid use of the Python API and documented as a special case.\n    PyObject *PyBytes_FromStringAndSize(char *v, Py_ssize_t len) except NULL\n    char* PyBytes_AsString(PyObject *string) except NULL\n    int _PyBytes_Resize(PyObject **string, Py_ssize_t newsize) except -1\n    void PyEval_InitThreads()\n\n\ncdef extern from \"time.h\":\n    ctypedef long int time_t\n    ctypedef long int suseconds_t\n\n\ncdef extern from \"sys/time.h\":\n    cdef struct timeval:\n        time_t tv_sec\n        suseconds_t tv_usec\n\n\ncdef extern from \"rados/rados_types.h\" nogil:\n    cdef char* _LIBRADOS_ALL_NSPACES \"LIBRADOS_ALL_NSPACES\"\n\n\ncdef extern from \"rados/librados.h\" nogil:\n    enum:\n        _LIBRADOS_OP_FLAG_EXCL \"LIBRADOS_OP_FLAG_EXCL\"\n        _LIBRADOS_OP_FLAG_FAILOK \"LIBRADOS_OP_FLAG_FAILOK\"\n        _LIBRADOS_OP_FLAG_FADVISE_RANDOM \"LIBRADOS_OP_FLAG_FADVISE_RANDOM\"\n        _LIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL \"LIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL\"\n        _LIBRADOS_OP_FLAG_FADVISE_WILLNEED \"LIBRADOS_OP_FLAG_FADVISE_WILLNEED\"\n        _LIBRADOS_OP_FLAG_FADVISE_DONTNEED \"LIBRADOS_OP_FLAG_FADVISE_DONTNEED\"\n        _LIBRADOS_OP_FLAG_FADVISE_NOCACHE \"LIBRADOS_OP_FLAG_FADVISE_NOCACHE\"\n\n\n    enum:\n        _LIBRADOS_OPERATION_NOFLAG \"LIBRADOS_OPERATION_NOFLAG\"\n        _LIBRADOS_OPERATION_BALANCE_READS \"LIBRADOS_OPERATION_BALANCE_READS\"\n        _LIBRADOS_OPERATION_LOCALIZE_READS \"LIBRADOS_OPERATION_LOCALIZE_READS\"\n        _LIBRADOS_OPERATION_ORDER_READS_WRITES \"LIBRADOS_OPERATION_ORDER_READS_WRITES\"\n        _LIBRADOS_OPERATION_IGNORE_CACHE \"LIBRADOS_OPERATION_IGNORE_CACHE\"\n        _LIBRADOS_OPERATION_SKIPRWLOCKS \"LIBRADOS_OPERATION_SKIPRWLOCKS\"\n        _LIBRADOS_OPERATION_IGNORE_OVERLAY \"LIBRADOS_OPERATION_IGNORE_OVERLAY\"\n        _LIBRADOS_CREATE_EXCLUSIVE \"LIBRADOS_CREATE_EXCLUSIVE\"\n        _LIBRADOS_CREATE_IDEMPOTENT \"LIBRADOS_CREATE_IDEMPOTENT\"\n\n    cdef uint64_t _LIBRADOS_SNAP_HEAD \"LIBRADOS_SNAP_HEAD\"\n\n    ctypedef void* rados_t\n    ctypedef void* rados_config_t\n    ctypedef void* rados_ioctx_t\n    ctypedef void* rados_xattrs_iter_t\n    ctypedef void* rados_omap_iter_t\n    ctypedef void* rados_list_ctx_t\n    ctypedef uint64_t rados_snap_t\n    ctypedef void *rados_write_op_t\n    ctypedef void *rados_read_op_t\n    ctypedef void *rados_completion_t\n    ctypedef void (*rados_callback_t)(rados_completion_t cb, void *arg)\n    ctypedef void (*rados_log_callback_t)(void *arg, const char *line, const char *who,\n                                          uint64_t sec, uint64_t nsec, uint64_t seq, const char *level, const char *msg)\n    ctypedef void (*rados_log_callback2_t)(void *arg, const char *line, const char *channel, const char *who, const char *name,\n                                          uint64_t sec, uint64_t nsec, uint64_t seq, const char *level, const char *msg)\n\n\n    cdef struct rados_cluster_stat_t:\n        uint64_t kb\n        uint64_t kb_used\n        uint64_t kb_avail\n        uint64_t num_objects\n\n    cdef struct rados_pool_stat_t:\n        uint64_t num_bytes\n        uint64_t num_kb\n        uint64_t num_objects\n        uint64_t num_object_clones\n        uint64_t num_object_copies\n        uint64_t num_objects_missing_on_primary\n        uint64_t num_objects_unfound\n        uint64_t num_objects_degraded\n        uint64_t num_rd\n        uint64_t num_rd_kb\n        uint64_t num_wr\n        uint64_t num_wr_kb\n\n    void rados_buffer_free(char *buf)\n\n    void rados_version(int *major, int *minor, int *extra)\n    int rados_create2(rados_t *pcluster, const char *const clustername,\n                      const char * const name, uint64_t flags)\n    int rados_create_with_context(rados_t *cluster, rados_config_t cct)\n    int rados_connect(rados_t cluster)\n    void rados_shutdown(rados_t cluster)\n    int rados_conf_read_file(rados_t cluster, const char *path)\n    int rados_conf_parse_argv_remainder(rados_t cluster, int argc, const char **argv, const char **remargv)\n    int rados_conf_parse_env(rados_t cluster, const char *var)\n    int rados_conf_set(rados_t cluster, char *option, const char *value)\n    int rados_conf_get(rados_t cluster, char *option, char *buf, size_t len)\n\n    int rados_ioctx_pool_stat(rados_ioctx_t io, rados_pool_stat_t *stats)\n    int64_t rados_pool_lookup(rados_t cluster, const char *pool_name)\n    int rados_pool_reverse_lookup(rados_t cluster, int64_t id, char *buf, size_t maxlen)\n    int rados_pool_create(rados_t cluster, const char *pool_name)\n    int rados_pool_create_with_auid(rados_t cluster, const char *pool_name, uint64_t auid)\n    int rados_pool_create_with_crush_rule(rados_t cluster, const char *pool_name, uint8_t crush_rule_num)\n    int rados_pool_create_with_all(rados_t cluster, const char *pool_name, uint64_t auid, uint8_t crush_rule_num)\n    int rados_pool_get_base_tier(rados_t cluster, int64_t pool, int64_t *base_tier)\n    int rados_pool_list(rados_t cluster, char *buf, size_t len)\n    int rados_pool_delete(rados_t cluster, const char *pool_name)\n    int rados_inconsistent_pg_list(rados_t cluster, int64_t pool, char *buf, size_t len)\n\n    int rados_cluster_stat(rados_t cluster, rados_cluster_stat_t *result)\n    int rados_cluster_fsid(rados_t cluster, char *buf, size_t len)\n    int rados_blacklist_add(rados_t cluster, char *client_address, uint32_t expire_seconds)\n    int rados_application_enable(rados_ioctx_t io, const char *app_name,\n                                 int force)\n    void rados_set_osdmap_full_try(rados_ioctx_t io)\n    void rados_unset_osdmap_full_try(rados_ioctx_t io)\n    int rados_application_list(rados_ioctx_t io, char *values,\n                             size_t *values_len)\n    int rados_application_metadata_get(rados_ioctx_t io, const char *app_name,\n                                       const char *key, char *value,\n                                       size_t *value_len)\n    int rados_application_metadata_set(rados_ioctx_t io, const char *app_name,\n                                       const char *key, const char *value)\n    int rados_application_metadata_remove(rados_ioctx_t io,\n                                          const char *app_name, const char *key)\n    int rados_application_metadata_list(rados_ioctx_t io,\n                                        const char *app_name, char *keys,\n                                        size_t *key_len, char *values,\n                                        size_t *value_len)\n    int rados_ping_monitor(rados_t cluster, const char *mon_id, char **outstr, size_t *outstrlen)\n    int rados_mon_command(rados_t cluster, const char **cmd, size_t cmdlen,\n                          const char *inbuf, size_t inbuflen,\n                          char **outbuf, size_t *outbuflen,\n                          char **outs, size_t *outslen)\n    int rados_mgr_command(rados_t cluster, const char **cmd, size_t cmdlen,\n                          const char *inbuf, size_t inbuflen,\n                          char **outbuf, size_t *outbuflen,\n                          char **outs, size_t *outslen)\n    int rados_mon_command_target(rados_t cluster, const char *name, const char **cmd, size_t cmdlen,\n                                 const char *inbuf, size_t inbuflen,\n                                 char **outbuf, size_t *outbuflen,\n                                 char **outs, size_t *outslen)\n    int rados_osd_command(rados_t cluster, int osdid, const char **cmd, size_t cmdlen,\n                          const char *inbuf, size_t inbuflen,\n                          char **outbuf, size_t *outbuflen,\n                          char **outs, size_t *outslen)\n    int rados_pg_command(rados_t cluster, const char *pgstr, const char **cmd, size_t cmdlen,\n                         const char *inbuf, size_t inbuflen,\n                         char **outbuf, size_t *outbuflen,\n                         char **outs, size_t *outslen)\n    int rados_monitor_log(rados_t cluster, const char *level, rados_log_callback_t cb, void *arg)\n    int rados_monitor_log2(rados_t cluster, const char *level, rados_log_callback2_t cb, void *arg)\n\n    int rados_wait_for_latest_osdmap(rados_t cluster)\n\n    int rados_service_register(rados_t cluster, const char *service, const char *daemon, const char *metadata_dict)\n    int rados_service_update_status(rados_t cluster, const char *status_dict)\n\n    int rados_ioctx_create(rados_t cluster, const char *pool_name, rados_ioctx_t *ioctx)\n    int rados_ioctx_create2(rados_t cluster, int64_t pool_id, rados_ioctx_t *ioctx)\n    void rados_ioctx_destroy(rados_ioctx_t io)\n    int rados_ioctx_pool_set_auid(rados_ioctx_t io, uint64_t auid)\n    void rados_ioctx_locator_set_key(rados_ioctx_t io, const char *key)\n    void rados_ioctx_set_namespace(rados_ioctx_t io, const char * nspace)\n\n    uint64_t rados_get_last_version(rados_ioctx_t io)\n    int rados_stat(rados_ioctx_t io, const char *o, uint64_t *psize, time_t *pmtime)\n    int rados_write(rados_ioctx_t io, const char *oid, const char *buf, size_t len, uint64_t off)\n    int rados_write_full(rados_ioctx_t io, const char *oid, const char *buf, size_t len)\n    int rados_append(rados_ioctx_t io, const char *oid, const char *buf, size_t len)\n    int rados_read(rados_ioctx_t io, const char *oid, char *buf, size_t len, uint64_t off)\n    int rados_remove(rados_ioctx_t io, const char *oid)\n    int rados_trunc(rados_ioctx_t io, const char *oid, uint64_t size)\n    int rados_getxattr(rados_ioctx_t io, const char *o, const char *name, char *buf, size_t len)\n    int rados_setxattr(rados_ioctx_t io, const char *o, const char *name, const char *buf, size_t len)\n    int rados_rmxattr(rados_ioctx_t io, const char *o, const char *name)\n    int rados_getxattrs(rados_ioctx_t io, const char *oid, rados_xattrs_iter_t *iter)\n    int rados_getxattrs_next(rados_xattrs_iter_t iter, const char **name, const char **val, size_t *len)\n    void rados_getxattrs_end(rados_xattrs_iter_t iter)\n\n    int rados_nobjects_list_open(rados_ioctx_t io, rados_list_ctx_t *ctx)\n    int rados_nobjects_list_next(rados_list_ctx_t ctx, const char **entry, const char **key, const char **nspace)\n    void rados_nobjects_list_close(rados_list_ctx_t ctx)\n\n    int rados_ioctx_pool_requires_alignment2(rados_ioctx_t io, int * requires)\n    int rados_ioctx_pool_required_alignment2(rados_ioctx_t io, uint64_t * alignment)\n\n    int rados_ioctx_snap_rollback(rados_ioctx_t io, const char * oid, const char * snapname)\n    int rados_ioctx_snap_create(rados_ioctx_t io, const char * snapname)\n    int rados_ioctx_snap_remove(rados_ioctx_t io, const char * snapname)\n    int rados_ioctx_snap_lookup(rados_ioctx_t io, const char * name, rados_snap_t * id)\n    int rados_ioctx_snap_get_name(rados_ioctx_t io, rados_snap_t id, char * name, int maxlen)\n    void rados_ioctx_snap_set_read(rados_ioctx_t io, rados_snap_t snap)\n    int rados_ioctx_snap_list(rados_ioctx_t io, rados_snap_t * snaps, int maxlen)\n    int rados_ioctx_snap_get_stamp(rados_ioctx_t io, rados_snap_t id, time_t * t)\n\n    int rados_lock_exclusive(rados_ioctx_t io, const char * oid, const char * name,\n                             const char * cookie, const char * desc,\n                             timeval * duration, uint8_t flags)\n    int rados_lock_shared(rados_ioctx_t io, const char * o, const char * name,\n                          const char * cookie, const char * tag, const char * desc,\n                          timeval * duration, uint8_t flags)\n    int rados_unlock(rados_ioctx_t io, const char * o, const char * name, const char * cookie)\n\n    rados_write_op_t rados_create_write_op()\n    void rados_release_write_op(rados_write_op_t write_op)\n\n    rados_read_op_t rados_create_read_op()\n    void rados_release_read_op(rados_read_op_t read_op)\n\n    int rados_aio_create_completion(void * cb_arg, rados_callback_t cb_complete, rados_callback_t cb_safe, rados_completion_t * pc)\n    void rados_aio_release(rados_completion_t c)\n    int rados_aio_stat(rados_ioctx_t io, const char *oid, rados_completion_t completion, uint64_t *psize, time_t *pmtime)\n    int rados_aio_write(rados_ioctx_t io, const char * oid, rados_completion_t completion, const char * buf, size_t len, uint64_t off)\n    int rados_aio_append(rados_ioctx_t io, const char * oid, rados_completion_t completion, const char * buf, size_t len)\n    int rados_aio_write_full(rados_ioctx_t io, const char * oid, rados_completion_t completion, const char * buf, size_t len)\n    int rados_aio_remove(rados_ioctx_t io, const char * oid, rados_completion_t completion)\n    int rados_aio_read(rados_ioctx_t io, const char * oid, rados_completion_t completion, char * buf, size_t len, uint64_t off)\n    int rados_aio_flush(rados_ioctx_t io)\n\n    int rados_aio_get_return_value(rados_completion_t c)\n    int rados_aio_wait_for_complete_and_cb(rados_completion_t c)\n    int rados_aio_wait_for_safe_and_cb(rados_completion_t c)\n    int rados_aio_wait_for_complete(rados_completion_t c)\n    int rados_aio_wait_for_safe(rados_completion_t c)\n    int rados_aio_is_complete(rados_completion_t c)\n    int rados_aio_is_safe(rados_completion_t c)\n\n    int rados_exec(rados_ioctx_t io, const char * oid, const char * cls, const char * method,\n                   const char * in_buf, size_t in_len, char * buf, size_t out_len)\n    int rados_aio_exec(rados_ioctx_t io, const char * oid, rados_completion_t completion, const char * cls, const char * method,\n                       const char * in_buf, size_t in_len, char * buf, size_t out_len)\n\n    int rados_write_op_operate(rados_write_op_t write_op, rados_ioctx_t io, const char * oid, time_t * mtime, int flags)\n    int rados_aio_write_op_operate(rados_write_op_t write_op, rados_ioctx_t io, rados_completion_t completion, const char *oid, time_t *mtime, int flags)\n    void rados_write_op_omap_set(rados_write_op_t write_op, const char * const* keys, const char * const* vals, const size_t * lens, size_t num)\n    void rados_write_op_omap_rm_keys(rados_write_op_t write_op, const char * const* keys, size_t keys_len)\n    void rados_write_op_omap_clear(rados_write_op_t write_op)\n    void rados_write_op_set_flags(rados_write_op_t write_op, int flags)\n\n    void rados_write_op_create(rados_write_op_t write_op, int exclusive, const char *category)\n    void rados_write_op_append(rados_write_op_t write_op, const char *buffer, size_t len)\n    void rados_write_op_write_full(rados_write_op_t write_op, const char *buffer, size_t len)\n    void rados_write_op_write(rados_write_op_t write_op, const char *buffer, size_t len, uint64_t offset)\n    void rados_write_op_remove(rados_write_op_t write_op)\n    void rados_write_op_truncate(rados_write_op_t write_op, uint64_t offset)\n    void rados_write_op_zero(rados_write_op_t write_op, uint64_t offset, uint64_t len)\n\n    void rados_read_op_omap_get_vals2(rados_read_op_t read_op, const char * start_after, const char * filter_prefix, uint64_t max_return, rados_omap_iter_t * iter, unsigned char *pmore, int * prval)\n    void rados_read_op_omap_get_keys2(rados_read_op_t read_op, const char * start_after, uint64_t max_return, rados_omap_iter_t * iter, unsigned char *pmore, int * prval)\n    void rados_read_op_omap_get_vals_by_keys(rados_read_op_t read_op, const char * const* keys, size_t keys_len, rados_omap_iter_t * iter, int * prval)\n    int rados_read_op_operate(rados_read_op_t read_op, rados_ioctx_t io, const char * oid, int flags)\n    int rados_aio_read_op_operate(rados_read_op_t read_op, rados_ioctx_t io, rados_completion_t completion, const char *oid, int flags)\n    void rados_read_op_set_flags(rados_read_op_t read_op, int flags)\n    int rados_omap_get_next(rados_omap_iter_t iter, const char * const* key, const char * const* val, size_t * len)\n    void rados_omap_get_end(rados_omap_iter_t iter)\n\n\nLIBRADOS_OP_FLAG_EXCL = _LIBRADOS_OP_FLAG_EXCL\nLIBRADOS_OP_FLAG_FAILOK = _LIBRADOS_OP_FLAG_FAILOK\nLIBRADOS_OP_FLAG_FADVISE_RANDOM = _LIBRADOS_OP_FLAG_FADVISE_RANDOM\nLIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL = _LIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL\nLIBRADOS_OP_FLAG_FADVISE_WILLNEED = _LIBRADOS_OP_FLAG_FADVISE_WILLNEED\nLIBRADOS_OP_FLAG_FADVISE_DONTNEED = _LIBRADOS_OP_FLAG_FADVISE_DONTNEED\nLIBRADOS_OP_FLAG_FADVISE_NOCACHE = _LIBRADOS_OP_FLAG_FADVISE_NOCACHE\n\nLIBRADOS_SNAP_HEAD = _LIBRADOS_SNAP_HEAD\n\nLIBRADOS_OPERATION_NOFLAG = _LIBRADOS_OPERATION_NOFLAG\nLIBRADOS_OPERATION_BALANCE_READS = _LIBRADOS_OPERATION_BALANCE_READS\nLIBRADOS_OPERATION_LOCALIZE_READS = _LIBRADOS_OPERATION_LOCALIZE_READS\nLIBRADOS_OPERATION_ORDER_READS_WRITES = _LIBRADOS_OPERATION_ORDER_READS_WRITES\nLIBRADOS_OPERATION_IGNORE_CACHE = _LIBRADOS_OPERATION_IGNORE_CACHE\nLIBRADOS_OPERATION_SKIPRWLOCKS = _LIBRADOS_OPERATION_SKIPRWLOCKS\nLIBRADOS_OPERATION_IGNORE_OVERLAY = _LIBRADOS_OPERATION_IGNORE_OVERLAY\n\nLIBRADOS_ALL_NSPACES = _LIBRADOS_ALL_NSPACES.decode('utf-8')\n\nLIBRADOS_CREATE_EXCLUSIVE = _LIBRADOS_CREATE_EXCLUSIVE\nLIBRADOS_CREATE_IDEMPOTENT = _LIBRADOS_CREATE_IDEMPOTENT\n\nANONYMOUS_AUID = 0xffffffffffffffff\nADMIN_AUID = 0\n\n\nclass Error(Exception):\n    \"\"\" `Error` class, derived from `Exception` \"\"\"\n    def __init__(self, message, errno=None):\n        super(Exception, self).__init__(message)\n        self.errno = errno\n\n    def __str__(self):\n        msg = super(Exception, self).__str__()\n        if self.errno is None:\n            return msg\n        return '[errno {0}] {1}'.format(self.errno, msg)\n\n    def __reduce__(self):\n        return (self.__class__, (self.message, self.errno))\n\nclass InvalidArgumentError(Error):\n    pass\n\nclass OSError(Error):\n    \"\"\" `OSError` class, derived from `Error` \"\"\"\n    pass\n\nclass InterruptedOrTimeoutError(OSError):\n    \"\"\" `InterruptedOrTimeoutError` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass PermissionError(OSError):\n    \"\"\" `PermissionError` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass PermissionDeniedError(OSError):\n    \"\"\" deal with EACCES related. \"\"\"\n    pass\n\n\nclass ObjectNotFound(OSError):\n    \"\"\" `ObjectNotFound` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass NoData(OSError):\n    \"\"\" `NoData` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass ObjectExists(OSError):\n    \"\"\" `ObjectExists` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass ObjectBusy(OSError):\n    \"\"\" `ObjectBusy` class, derived from `IOError` \"\"\"\n    pass\n\n\nclass IOError(OSError):\n    \"\"\" `ObjectBusy` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass NoSpace(OSError):\n    \"\"\" `NoSpace` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass RadosStateError(Error):\n    \"\"\" `RadosStateError` class, derived from `Error` \"\"\"\n    pass\n\n\nclass IoctxStateError(Error):\n    \"\"\" `IoctxStateError` class, derived from `Error` \"\"\"\n    pass\n\n\nclass ObjectStateError(Error):\n    \"\"\" `ObjectStateError` class, derived from `Error` \"\"\"\n    pass\n\n\nclass LogicError(Error):\n    \"\"\" `` class, derived from `Error` \"\"\"\n    pass\n\n\nclass TimedOut(OSError):\n    \"\"\" `TimedOut` class, derived from `OSError` \"\"\"\n    pass\n\n\nIF UNAME_SYSNAME == \"FreeBSD\":\n    cdef errno_to_exception = {\n        errno.EPERM     : PermissionError,\n        errno.ENOENT    : ObjectNotFound,\n        errno.EIO       : IOError,\n        errno.ENOSPC    : NoSpace,\n        errno.EEXIST    : ObjectExists,\n        errno.EBUSY     : ObjectBusy,\n        errno.ENOATTR   : NoData,\n        errno.EINTR     : InterruptedOrTimeoutError,\n        errno.ETIMEDOUT : TimedOut,\n        errno.EACCES    : PermissionDeniedError,\n        errno.EINVAL    : InvalidArgumentError,\n    }\nELSE:\n    cdef errno_to_exception = {\n        errno.EPERM     : PermissionError,\n        errno.ENOENT    : ObjectNotFound,\n        errno.EIO       : IOError,\n        errno.ENOSPC    : NoSpace,\n        errno.EEXIST    : ObjectExists,\n        errno.EBUSY     : ObjectBusy,\n        errno.ENODATA   : NoData,\n        errno.EINTR     : InterruptedOrTimeoutError,\n        errno.ETIMEDOUT : TimedOut,\n        errno.EACCES    : PermissionDeniedError,\n        errno.EINVAL    : InvalidArgumentError,\n    }\n\n\ncdef make_ex(ret, msg):\n    \"\"\"\n    Translate a librados return code into an exception.\n\n    :param ret: the return code\n    :type ret: int\n    :param msg: the error message to use\n    :type msg: str\n    :returns: a subclass of :class:`Error`\n    \"\"\"\n    ret = abs(ret)\n    if ret in errno_to_exception:\n        return errno_to_exception[ret](msg, errno=ret)\n    else:\n        return OSError(msg, errno=ret)\n\n\n# helper to specify an optional argument, where in addition to `cls`, `None`\n# is also acceptable\ndef opt(cls):\n    return (cls, None)\n\n\n# validate argument types of an instance method\n# kwargs is an un-ordered dict, so use args instead\ndef requires(*types):\n    def is_type_of(v, t):\n        if t is None:\n            return v is None\n        else:\n            return isinstance(v, t)\n\n    def check_type(val, arg_name, arg_type):\n        if isinstance(arg_type, tuple):\n            if any(is_type_of(val, t) for t in arg_type):\n                return\n            type_names = ' or '.join('None' if t is None else t.__name__\n                                     for t in arg_type)\n            raise TypeError('%s must be %s' % (arg_name, type_names))\n        else:\n            if is_type_of(val, arg_type):\n                return\n            assert(arg_type is not None)\n            raise TypeError('%s must be %s' % (arg_name, arg_type.__name__))\n\n    def wrapper(f):\n        # FIXME(sileht): this stop with\n        # AttributeError: 'method_descriptor' object has no attribute '__module__'\n        # @wraps(f)\n        def validate_func(*args, **kwargs):\n            # ignore the `self` arg\n            pos_args = zip(args[1:], types)\n            named_args = ((kwargs[name], (name, spec)) for name, spec in types\n                          if name in kwargs)\n            for arg_val, (arg_name, arg_type) in chain(pos_args, named_args):\n                check_type(arg_val, arg_name, arg_type)\n            return f(*args, **kwargs)\n        return validate_func\n    return wrapper\n\n\ndef cstr(val, name, encoding=\"utf-8\", opt=False):\n    \"\"\"\n    Create a byte string from a Python string\n\n    :param basestring val: Python string\n    :param str name: Name of the string parameter, for exceptions\n    :param str encoding: Encoding to use\n    :param bool opt: If True, None is allowed\n    :rtype: bytes\n    :raises: :class:`InvalidArgument`\n    \"\"\"\n    if opt and val is None:\n        return None\n    if isinstance(val, bytes):\n        return val\n    elif isinstance(val, unicode):\n        return val.encode(encoding)\n    else:\n        raise TypeError('%s must be a string' % name)\n\n\ndef cstr_list(list_str, name, encoding=\"utf-8\"):\n    return [cstr(s, name) for s in list_str]\n\n\ndef decode_cstr(val, encoding=\"utf-8\"):\n    \"\"\"\n    Decode a byte string into a Python string.\n\n    :param bytes val: byte string\n    :rtype: unicode or None\n    \"\"\"\n    if val is None:\n        return None\n\n    return val.decode(encoding)\n\n\ncdef char* opt_str(s) except? NULL:\n    if s is None:\n        return NULL\n    return s\n\n\ncdef void* realloc_chk(void* ptr, size_t size) except NULL:\n    cdef void *ret = realloc(ptr, size)\n    if ret == NULL:\n        raise MemoryError(\"realloc failed\")\n    return ret\n\n\ncdef size_t * to_csize_t_array(list_int):\n    cdef size_t *ret = <size_t *>malloc(len(list_int) * sizeof(size_t))\n    if ret == NULL:\n        raise MemoryError(\"malloc failed\")\n    for i in xrange(len(list_int)):\n        ret[i] = <size_t>list_int[i]\n    return ret\n\n\ncdef char ** to_bytes_array(list_bytes):\n    cdef char **ret = <char **>malloc(len(list_bytes) * sizeof(char *))\n    if ret == NULL:\n        raise MemoryError(\"malloc failed\")\n    for i in xrange(len(list_bytes)):\n        ret[i] = <char *>list_bytes[i]\n    return ret\n\n\n\ncdef int __monitor_callback(void *arg, const char *line, const char *who,\n                             uint64_t sec, uint64_t nsec, uint64_t seq,\n                             const char *level, const char *msg) with gil:\n    cdef object cb_info = <object>arg\n    cb_info[0](cb_info[1], line, who, sec, nsec, seq, level, msg)\n    return 0\n\ncdef int __monitor_callback2(void *arg, const char *line, const char *channel,\n                             const char *who,\n                             const char *name,\n                             uint64_t sec, uint64_t nsec, uint64_t seq,\n                             const char *level, const char *msg) with gil:\n    cdef object cb_info = <object>arg\n    cb_info[0](cb_info[1], line, channel, name, who, sec, nsec, seq, level, msg)\n    return 0\n\n\nclass Version(object):\n    \"\"\" Version information \"\"\"\n    def __init__(self, major, minor, extra):\n        self.major = major\n        self.minor = minor\n        self.extra = extra\n\n    def __str__(self):\n        return \"%d.%d.%d\" % (self.major, self.minor, self.extra)\n\n\ncdef class Rados(object):\n    \"\"\"This class wraps librados functions\"\"\"\n    # NOTE(sileht): attributes declared in .pyd\n\n    def __init__(self, *args, **kwargs):\n        PyEval_InitThreads()\n        self.__setup(*args, **kwargs)\n\n    @requires(('rados_id', opt(str_type)), ('name', opt(str_type)), ('clustername', opt(str_type)),\n              ('conffile', opt(str_type)))\n    def __setup(self, rados_id=None, name=None, clustername=None,\n                conf_defaults=None, conffile=None, conf=None, flags=0,\n                context=None):\n        self.monitor_callback = None\n        self.monitor_callback2 = None\n        self.parsed_args = []\n        self.conf_defaults = conf_defaults\n        self.conffile = conffile\n        self.rados_id = rados_id\n\n        if rados_id and name:\n            raise Error(\"Rados(): can't supply both rados_id and name\")\n        elif rados_id:\n            name = 'client.' + rados_id\n        elif name is None:\n            name = 'client.admin'\n        if clustername is None:\n            clustername = ''\n\n        name = cstr(name, 'name')\n        clustername = cstr(clustername, 'clustername')\n        cdef:\n            char *_name = name\n            char *_clustername = clustername\n            int _flags = flags\n            int ret\n\n        if context:\n            # Unpack void* (aka rados_config_t) from capsule\n            rados_config = <rados_config_t> PyCapsule_GetPointer(context, NULL)\n            with nogil:\n                ret = rados_create_with_context(&self.cluster, rados_config)\n        else:\n            with nogil:\n                ret = rados_create2(&self.cluster, _clustername, _name, _flags)\n        if ret != 0:\n            raise Error(\"rados_initialize failed with error code: %d\" % ret)\n\n        self.state = \"configuring\"\n        # order is important: conf_defaults, then conffile, then conf\n        if conf_defaults:\n            for key, value in conf_defaults.items():\n                self.conf_set(key, value)\n        if conffile is not None:\n            # read the default conf file when '' is given\n            if conffile == '':\n                conffile = None\n            self.conf_read_file(conffile)\n        if conf:\n            for key, value in conf.items():\n                self.conf_set(key, value)\n\n    def require_state(self, *args):\n        \"\"\"\n        Checks if the Rados object is in a special state\n\n        :raises: :class:`RadosStateError`\n        \"\"\"\n        if self.state in args:\n            return\n        raise RadosStateError(\"You cannot perform that operation on a \\\nRados object in state %s.\" % self.state)\n\n    def shutdown(self):\n        \"\"\"\n        Disconnects from the cluster.  Call this explicitly when a\n        Rados.connect()ed object is no longer used.\n        \"\"\"\n        if self.state != \"shutdown\":\n            with nogil:\n                rados_shutdown(self.cluster)\n            self.state = \"shutdown\"\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        self.shutdown()\n        return False\n\n    def version(self):\n        \"\"\"\n        Get the version number of the ``librados`` C library.\n\n        :returns: a tuple of ``(major, minor, extra)`` components of the\n                  librados version\n        \"\"\"\n        cdef int major = 0\n        cdef int minor = 0\n        cdef int extra = 0\n        with nogil:\n            rados_version(&major, &minor, &extra)\n        return Version(major, minor, extra)\n\n    @requires(('path', opt(str_type)))\n    def conf_read_file(self, path=None):\n        \"\"\"\n        Configure the cluster handle using a Ceph config file.\n\n        :param path: path to the config file\n        :type path: str\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        path = cstr(path, 'path', opt=True)\n        cdef:\n            char *_path = opt_str(path)\n        with nogil:\n            ret = rados_conf_read_file(self.cluster, _path)\n        if ret != 0:\n            raise make_ex(ret, \"error calling conf_read_file\")\n\n    def conf_parse_argv(self, args):\n        \"\"\"\n        Parse known arguments from args, and remove; returned\n        args contain only those unknown to ceph\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        if not args:\n            return\n\n        cargs = cstr_list(args, 'args')\n        cdef:\n            int _argc = len(args)\n            char **_argv = to_bytes_array(cargs)\n            char **_remargv = NULL\n\n        try:\n            _remargv = <char **>malloc(_argc * sizeof(char *))\n            with nogil:\n                ret = rados_conf_parse_argv_remainder(self.cluster, _argc,\n                                                      <const char**>_argv,\n                                                      <const char**>_remargv)\n            if ret:\n                raise make_ex(ret, \"error calling conf_parse_argv_remainder\")\n\n            # _remargv was allocated with fixed argc; collapse return\n            # list to eliminate any missing args\n            retargs = [decode_cstr(a) for a in _remargv[:_argc]\n                       if a != NULL]\n            self.parsed_args = args\n            return retargs\n        finally:\n            free(_argv)\n            free(_remargv)\n\n    def conf_parse_env(self, var='CEPH_ARGS'):\n        \"\"\"\n        Parse known arguments from an environment variable, normally\n        CEPH_ARGS.\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        if not var:\n            return\n\n        var = cstr(var, 'var')\n        cdef:\n            char *_var = var\n        with nogil:\n            ret = rados_conf_parse_env(self.cluster, _var)\n        if ret != 0:\n            raise make_ex(ret, \"error calling conf_parse_env\")\n\n    @requires(('option', str_type))\n    def conf_get(self, option):\n        \"\"\"\n        Get the value of a configuration option\n\n        :param option: which option to read\n        :type option: str\n\n        :returns: str - value of the option or None\n        :raises: :class:`TypeError`\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        option = cstr(option, 'option')\n        cdef:\n            char *_option = option\n            size_t length = 20\n            char *ret_buf = NULL\n\n        try:\n            while True:\n                ret_buf = <char *>realloc_chk(ret_buf, length)\n                with nogil:\n                    ret = rados_conf_get(self.cluster, _option, ret_buf, length)\n                if ret == 0:\n                    return decode_cstr(ret_buf)\n                elif ret == -errno.ENAMETOOLONG:\n                    length = length * 2\n                elif ret == -errno.ENOENT:\n                    return None\n                else:\n                    raise make_ex(ret, \"error calling conf_get\")\n        finally:\n            free(ret_buf)\n\n    @requires(('option', str_type), ('val', str_type))\n    def conf_set(self, option, val):\n        \"\"\"\n        Set the value of a configuration option\n\n        :param option: which option to set\n        :type option: str\n        :param option: value of the option\n        :type option: str\n\n        :raises: :class:`TypeError`, :class:`ObjectNotFound`\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        option = cstr(option, 'option')\n        val = cstr(val, 'val')\n        cdef:\n            char *_option = option\n            char *_val = val\n\n        with nogil:\n            ret = rados_conf_set(self.cluster, _option, _val)\n        if ret != 0:\n            raise make_ex(ret, \"error calling conf_set\")\n\n    def ping_monitor(self, mon_id):\n        \"\"\"\n        Ping a monitor to assess liveness\n\n        May be used as a simply way to assess liveness, or to obtain\n        information about the monitor in a simple way even in the\n        absence of quorum.\n\n        :param mon_id: the ID portion of the monitor's name (i.e., mon.<ID>)\n        :type mon_id: str\n        :returns: the string reply from the monitor\n        \"\"\"\n\n        self.require_state(\"configuring\", \"connected\")\n\n        mon_id = cstr(mon_id, 'mon_id')\n        cdef:\n            char *_mon_id = mon_id\n            size_t outstrlen = 0\n            char *outstr\n\n        with nogil:\n            ret = rados_ping_monitor(self.cluster, _mon_id, &outstr, &outstrlen)\n\n        if ret != 0:\n            raise make_ex(ret, \"error calling ping_monitor\")\n\n        if outstrlen:\n            my_outstr = outstr[:outstrlen]\n            rados_buffer_free(outstr)\n            return decode_cstr(my_outstr)\n\n    def connect(self, timeout=0):\n        \"\"\"\n        Connect to the cluster.  Use shutdown() to release resources.\n        \"\"\"\n        self.require_state(\"configuring\")\n        # NOTE(sileht): timeout was supported by old python API,\n        # but this is not something available in C API, so ignore\n        # for now and remove it later\n        with nogil:\n            ret = rados_connect(self.cluster)\n        if ret != 0:\n            raise make_ex(ret, \"error connecting to the cluster\")\n        self.state = \"connected\"\n\n    def get_cluster_stats(self):\n        \"\"\"\n        Read usage info about the cluster\n\n        This tells you total space, space used, space available, and number\n        of objects. These are not updated immediately when data is written,\n        they are eventually consistent.\n\n        :returns: dict - contains the following keys:\n\n            - ``kb`` (int) - total space\n\n            - ``kb_used`` (int) - space used\n\n            - ``kb_avail`` (int) - free space available\n\n            - ``num_objects`` (int) - number of objects\n\n        \"\"\"\n        cdef:\n            rados_cluster_stat_t stats\n\n        with nogil:\n            ret = rados_cluster_stat(self.cluster, &stats)\n\n        if ret < 0:\n            raise make_ex(\n                ret, \"Rados.get_cluster_stats(%s): get_stats failed\" % self.rados_id)\n        return {'kb': stats.kb,\n                'kb_used': stats.kb_used,\n                'kb_avail': stats.kb_avail,\n                'num_objects': stats.num_objects}\n\n    @requires(('pool_name', str_type))\n    def pool_exists(self, pool_name):\n        \"\"\"\n        Checks if a given pool exists.\n\n        :param pool_name: name of the pool to check\n        :type pool_name: str\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: bool - whether the pool exists, false otherwise.\n        \"\"\"\n        self.require_state(\"connected\")\n\n        pool_name = cstr(pool_name, 'pool_name')\n        cdef:\n            char *_pool_name = pool_name\n\n        with nogil:\n            ret = rados_pool_lookup(self.cluster, _pool_name)\n        if ret >= 0:\n            return True\n        elif ret == -errno.ENOENT:\n            return False\n        else:\n            raise make_ex(ret, \"error looking up pool '%s'\" % pool_name)\n\n    @requires(('pool_name', str_type))\n    def pool_lookup(self, pool_name):\n        \"\"\"\n        Returns a pool's ID based on its name.\n\n        :param pool_name: name of the pool to look up\n        :type pool_name: str\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: int - pool ID, or None if it doesn't exist\n        \"\"\"\n        self.require_state(\"connected\")\n        pool_name = cstr(pool_name, 'pool_name')\n        cdef:\n            char *_pool_name = pool_name\n\n        with nogil:\n            ret = rados_pool_lookup(self.cluster, _pool_name)\n        if ret >= 0:\n            return int(ret)\n        elif ret == -errno.ENOENT:\n            return None\n        else:\n            raise make_ex(ret, \"error looking up pool '%s'\" % pool_name)\n\n    @requires(('pool_id', int))\n    def pool_reverse_lookup(self, pool_id):\n        \"\"\"\n        Returns a pool's name based on its ID.\n\n        :param pool_id: ID of the pool to look up\n        :type pool_id: int\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: string - pool name, or None if it doesn't exist\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            int64_t _pool_id = pool_id\n            size_t size = 512\n            char *name = NULL\n\n        try:\n            while True:\n                name = <char *>realloc_chk(name, size)\n                with nogil:\n                    ret = rados_pool_reverse_lookup(self.cluster, _pool_id, name, size)\n                if ret >= 0:\n                    break\n                elif ret != -errno.ERANGE and size <= 4096:\n                    size *= 2\n                elif ret == -errno.ENOENT:\n                    return None\n                elif ret < 0:\n                    raise make_ex(ret, \"error reverse looking up pool '%s'\" % pool_id)\n\n            return decode_cstr(name)\n\n        finally:\n            free(name)\n\n    @requires(('pool_name', str_type), ('auid', opt(int)), ('crush_rule', opt(int)))\n    def create_pool(self, pool_name, auid=None, crush_rule=None):\n        \"\"\"\n        Create a pool:\n        - with default settings: if auid=None and crush_rule=None\n        - owned by a specific auid: auid given and crush_rule=None\n        - with a specific CRUSH rule: if auid=None and crush_rule given\n        - with a specific CRUSH rule and auid: if auid and crush_rule given\n\n        :param pool_name: name of the pool to create\n        :type pool_name: str\n        :param auid: the id of the owner of the new pool\n        :type auid: int\n        :param crush_rule: rule to use for placement in the new pool\n        :type crush_rule: int\n\n        :raises: :class:`TypeError`, :class:`Error`\n        \"\"\"\n        self.require_state(\"connected\")\n\n        pool_name = cstr(pool_name, 'pool_name')\n        cdef:\n            char *_pool_name = pool_name\n            uint8_t _crush_rule\n            uint64_t _auid\n\n        if auid is None and crush_rule is None:\n            with nogil:\n                ret = rados_pool_create(self.cluster, _pool_name)\n        elif auid is None:\n            _crush_rule = crush_rule\n            with nogil:\n                ret = rados_pool_create_with_crush_rule(self.cluster, _pool_name, _crush_rule)\n        elif crush_rule is None:\n            _auid = auid\n            with nogil:\n                ret = rados_pool_create_with_auid(self.cluster, _pool_name, _auid)\n        else:\n            _auid = auid\n            _crush_rule = crush_rule\n            with nogil:\n                ret = rados_pool_create_with_all(self.cluster, _pool_name, _auid, _crush_rule)\n        if ret < 0:\n            raise make_ex(ret, \"error creating pool '%s'\" % pool_name)\n\n    @requires(('pool_id', int))\n    def get_pool_base_tier(self, pool_id):\n        \"\"\"\n        Get base pool\n\n        :returns: base pool, or pool_id if tiering is not configured for the pool\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            int64_t base_tier = 0\n            int64_t _pool_id = pool_id\n\n        with nogil:\n            ret = rados_pool_get_base_tier(self.cluster, _pool_id, &base_tier)\n        if ret < 0:\n            raise make_ex(ret, \"get_pool_base_tier(%d)\" % pool_id)\n        return int(base_tier)\n\n    @requires(('pool_name', str_type))\n    def delete_pool(self, pool_name):\n        \"\"\"\n        Delete a pool and all data inside it.\n\n        The pool is removed from the cluster immediately,\n        but the actual data is deleted in the background.\n\n        :param pool_name: name of the pool to delete\n        :type pool_name: str\n\n        :raises: :class:`TypeError`, :class:`Error`\n        \"\"\"\n        self.require_state(\"connected\")\n\n        pool_name = cstr(pool_name, 'pool_name')\n        cdef:\n            char *_pool_name = pool_name\n\n        with nogil:\n            ret = rados_pool_delete(self.cluster, _pool_name)\n        if ret < 0:\n            raise make_ex(ret, \"error deleting pool '%s'\" % pool_name)\n\n    @requires(('pool_id', int))\n    def get_inconsistent_pgs(self, pool_id):\n        \"\"\"\n        List inconsistent placement groups in the given pool\n\n        :param pool_id: ID of the pool in which PGs are listed\n        :type pool_id: int\n        :returns: list - inconsistent placement groups\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            int64_t pool = pool_id\n            size_t size = 512\n            char *pgs = NULL\n\n        try:\n            while True:\n                pgs = <char *>realloc_chk(pgs, size);\n                with nogil:\n                    ret = rados_inconsistent_pg_list(self.cluster, pool,\n                                                     pgs, size)\n                if ret > <int>size:\n                    size *= 2\n                elif ret >= 0:\n                    break\n                else:\n                    raise make_ex(ret, \"error calling inconsistent_pg_list\")\n            return [pg for pg in decode_cstr(pgs[:ret]).split('\\0') if pg]\n        finally:\n            free(pgs)\n\n    def list_pools(self):\n        \"\"\"\n        Gets a list of pool names.\n\n        :returns: list - of pool names.\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            size_t size = 512\n            char *c_names = NULL\n\n        try:\n            while True:\n                c_names = <char *>realloc_chk(c_names, size)\n                with nogil:\n                    ret = rados_pool_list(self.cluster, c_names, size)\n                if ret > <int>size:\n                    size *= 2\n                elif ret >= 0:\n                    break\n            return [name for name in decode_cstr(c_names[:ret]).split('\\0')\n                    if name]\n        finally:\n            free(c_names)\n\n    def get_fsid(self):\n        \"\"\"\n        Get the fsid of the cluster as a hexadecimal string.\n\n        :raises: :class:`Error`\n        :returns: str - cluster fsid\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            char *ret_buf\n            size_t buf_len = 37\n            PyObject* ret_s = NULL\n\n        ret_s = PyBytes_FromStringAndSize(NULL, buf_len)\n        try:\n            ret_buf = PyBytes_AsString(ret_s)\n            with nogil:\n                ret = rados_cluster_fsid(self.cluster, ret_buf, buf_len)\n            if ret < 0:\n                raise make_ex(ret, \"error getting cluster fsid\")\n            if ret != <int>buf_len:\n                _PyBytes_Resize(&ret_s, ret)\n            return <object>ret_s\n        finally:\n            # We DECREF unconditionally: the cast to object above will have\n            # INCREFed if necessary. This also takes care of exceptions,\n            # including if _PyString_Resize fails (that will free the string\n            # itself and set ret_s to NULL, hence XDECREF).\n            ref.Py_XDECREF(ret_s)\n\n    @requires(('ioctx_name', str_type))\n    def open_ioctx(self, ioctx_name):\n        \"\"\"\n        Create an io context\n\n        The io context allows you to perform operations within a particular\n        pool.\n\n        :param ioctx_name: name of the pool\n        :type ioctx_name: str\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: Ioctx - Rados Ioctx object\n        \"\"\"\n        self.require_state(\"connected\")\n        ioctx_name = cstr(ioctx_name, 'ioctx_name')\n        cdef:\n            rados_ioctx_t ioctx\n            char *_ioctx_name = ioctx_name\n        with nogil:\n            ret = rados_ioctx_create(self.cluster, _ioctx_name, &ioctx)\n        if ret < 0:\n            raise make_ex(ret, \"error opening pool '%s'\" % ioctx_name)\n        io = Ioctx(ioctx_name)\n        io.io = ioctx\n        return io\n\n    @requires(('pool_id', int))\n    def open_ioctx2(self, pool_id):\n        \"\"\"\n        Create an io context\n\n        The io context allows you to perform operations within a particular\n        pool.\n\n        :param pool_id: ID of the pool\n        :type pool_id: int\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: Ioctx - Rados Ioctx object\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            rados_ioctx_t ioctx\n            int64_t _pool_id = pool_id\n        with nogil:\n            ret = rados_ioctx_create2(self.cluster, _pool_id, &ioctx)\n        if ret < 0:\n            raise make_ex(ret, \"error opening pool id '%s'\" % pool_id)\n        io = Ioctx(str(pool_id))\n        io.io = ioctx\n        return io\n\n    def mon_command(self, cmd, inbuf, timeout=0, target=None):\n        \"\"\"\n        mon_command[_target](cmd, inbuf, outbuf, outbuflen, outs, outslen)\n        returns (int ret, string outbuf, string outs)\n        \"\"\"\n        # NOTE(sileht): timeout is ignored because C API doesn't provide\n        # timeout argument, but we keep it for backward compat with old python binding\n\n        self.require_state(\"connected\")\n        cmd = cstr_list(cmd, 'c')\n\n        if isinstance(target, int):\n        # NOTE(sileht): looks weird but test_monmap_dump pass int\n            target = str(target)\n\n        target = cstr(target, 'target', opt=True)\n        inbuf = cstr(inbuf, 'inbuf')\n\n        cdef:\n            char *_target = opt_str(target)\n            char **_cmd = to_bytes_array(cmd)\n            size_t _cmdlen = len(cmd)\n\n            char *_inbuf = inbuf\n            size_t _inbuf_len = len(inbuf)\n\n            char *_outbuf\n            size_t _outbuf_len\n            char *_outs\n            size_t _outs_len\n\n        try:\n            if target:\n                with nogil:\n                    ret = rados_mon_command_target(self.cluster, _target,\n                                                <const char **>_cmd, _cmdlen,\n                                                <const char*>_inbuf, _inbuf_len,\n                                                &_outbuf, &_outbuf_len,\n                                                &_outs, &_outs_len)\n            else:\n                with nogil:\n                    ret = rados_mon_command(self.cluster,\n                                            <const char **>_cmd, _cmdlen,\n                                            <const char*>_inbuf, _inbuf_len,\n                                            &_outbuf, &_outbuf_len,\n                                            &_outs, &_outs_len)\n\n            my_outs = decode_cstr(_outs[:_outs_len])\n            my_outbuf = _outbuf[:_outbuf_len]\n            if _outs_len:\n                rados_buffer_free(_outs)\n            if _outbuf_len:\n                rados_buffer_free(_outbuf)\n            return (ret, my_outbuf, my_outs)\n        finally:\n            free(_cmd)\n\n    def osd_command(self, osdid, cmd, inbuf, timeout=0):\n        \"\"\"\n        osd_command(osdid, cmd, inbuf, outbuf, outbuflen, outs, outslen)\n        returns (int ret, string outbuf, string outs)\n        \"\"\"\n        # NOTE(sileht): timeout is ignored because C API doesn't provide\n        # timeout argument, but we keep it for backward compat with old python binding\n        self.require_state(\"connected\")\n\n        cmd = cstr_list(cmd, 'cmd')\n        inbuf = cstr(inbuf, 'inbuf')\n\n        cdef:\n            int _osdid = osdid\n            char **_cmd = to_bytes_array(cmd)\n            size_t _cmdlen = len(cmd)\n\n            char *_inbuf = inbuf\n            size_t _inbuf_len = len(inbuf)\n\n            char *_outbuf\n            size_t _outbuf_len\n            char *_outs\n            size_t _outs_len\n\n        try:\n            with nogil:\n                ret = rados_osd_command(self.cluster, _osdid,\n                                        <const char **>_cmd, _cmdlen,\n                                        <const char*>_inbuf, _inbuf_len,\n                                        &_outbuf, &_outbuf_len,\n                                        &_outs, &_outs_len)\n\n            my_outs = decode_cstr(_outs[:_outs_len])\n            my_outbuf = _outbuf[:_outbuf_len]\n            if _outs_len:\n                rados_buffer_free(_outs)\n            if _outbuf_len:\n                rados_buffer_free(_outbuf)\n            return (ret, my_outbuf, my_outs)\n        finally:\n            free(_cmd)\n\n    def mgr_command(self, cmd, inbuf, timeout=0):\n        \"\"\"\n        returns (int ret, string outbuf, string outs)\n        \"\"\"\n        # NOTE(sileht): timeout is ignored because C API doesn't provide\n        # timeout argument, but we keep it for backward compat with old python binding\n        self.require_state(\"connected\")\n\n        cmd = cstr_list(cmd, 'cmd')\n        inbuf = cstr(inbuf, 'inbuf')\n\n        cdef:\n            char **_cmd = to_bytes_array(cmd)\n            size_t _cmdlen = len(cmd)\n\n            char *_inbuf = inbuf\n            size_t _inbuf_len = len(inbuf)\n\n            char *_outbuf\n            size_t _outbuf_len\n            char *_outs\n            size_t _outs_len\n\n        try:\n            with nogil:\n                ret = rados_mgr_command(self.cluster,\n                                        <const char **>_cmd, _cmdlen,\n                                        <const char*>_inbuf, _inbuf_len,\n                                        &_outbuf, &_outbuf_len,\n                                        &_outs, &_outs_len)\n\n            my_outs = decode_cstr(_outs[:_outs_len])\n            my_outbuf = _outbuf[:_outbuf_len]\n            if _outs_len:\n                rados_buffer_free(_outs)\n            if _outbuf_len:\n                rados_buffer_free(_outbuf)\n            return (ret, my_outbuf, my_outs)\n        finally:\n            free(_cmd)\n\n    def pg_command(self, pgid, cmd, inbuf, timeout=0):\n        \"\"\"\n        pg_command(pgid, cmd, inbuf, outbuf, outbuflen, outs, outslen)\n        returns (int ret, string outbuf, string outs)\n        \"\"\"\n        # NOTE(sileht): timeout is ignored because C API doesn't provide\n        # timeout argument, but we keep it for backward compat with old python binding\n        self.require_state(\"connected\")\n\n        pgid = cstr(pgid, 'pgid')\n        cmd = cstr_list(cmd, 'cmd')\n        inbuf = cstr(inbuf, 'inbuf')\n\n        cdef:\n            char *_pgid = pgid\n            char **_cmd = to_bytes_array(cmd)\n            size_t _cmdlen = len(cmd)\n\n            char *_inbuf = inbuf\n            size_t _inbuf_len = len(inbuf)\n\n            char *_outbuf\n            size_t _outbuf_len\n            char *_outs\n            size_t _outs_len\n\n        try:\n            with nogil:\n                ret = rados_pg_command(self.cluster, _pgid,\n                                       <const char **>_cmd, _cmdlen,\n                                       <const char *>_inbuf, _inbuf_len,\n                                       &_outbuf, &_outbuf_len,\n                                       &_outs, &_outs_len)\n\n            my_outs = decode_cstr(_outs[:_outs_len])\n            my_outbuf = _outbuf[:_outbuf_len]\n            if _outs_len:\n                rados_buffer_free(_outs)\n            if _outbuf_len:\n                rados_buffer_free(_outbuf)\n            return (ret, my_outbuf, my_outs)\n        finally:\n            free(_cmd)\n\n    def wait_for_latest_osdmap(self):\n        self.require_state(\"connected\")\n        with nogil:\n            ret = rados_wait_for_latest_osdmap(self.cluster)\n        return ret\n\n    def blacklist_add(self, client_address, expire_seconds=0):\n        \"\"\"\n        Blacklist a client from the OSDs\n\n        :param client_address: client address\n        :type client_address: str\n        :param expire_seconds: number of seconds to blacklist\n        :type expire_seconds: int\n\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_state(\"connected\")\n        client_address =  cstr(client_address, 'client_address')\n        cdef:\n            uint32_t _expire_seconds = expire_seconds\n            char *_client_address = client_address\n\n        with nogil:\n            ret = rados_blacklist_add(self.cluster, _client_address, _expire_seconds)\n        if ret < 0:\n            raise make_ex(ret, \"error blacklisting client '%s'\" % client_address)\n\n    def monitor_log(self, level, callback, arg):\n        if level not in MONITOR_LEVELS:\n            raise LogicError(\"invalid monitor level \" + level)\n        if callback is not None and not callable(callback):\n            raise LogicError(\"callback must be a callable function or None\")\n\n        level = cstr(level, 'level')\n        cdef char *_level = level\n\n        if callback is None:\n            with nogil:\n                r = rados_monitor_log(self.cluster, <const char*>_level, NULL, NULL)\n            self.monitor_callback = None\n            self.monitor_callback2 = None\n            return\n\n        cb = (callback, arg)\n        cdef PyObject* _arg = <PyObject*>cb\n        with nogil:\n            r = rados_monitor_log(self.cluster, <const char*>_level,\n                                  <rados_log_callback_t>&__monitor_callback, _arg)\n\n        if r:\n            raise make_ex(r, 'error calling rados_monitor_log')\n        # NOTE(sileht): Prevents the callback method from being garbage collected\n        self.monitor_callback = cb\n        self.monitor_callback2 = None\n\n    def monitor_log2(self, level, callback, arg):\n        if level not in MONITOR_LEVELS:\n            raise LogicError(\"invalid monitor level \" + level)\n        if callback is not None and not callable(callback):\n            raise LogicError(\"callback must be a callable function or None\")\n\n        level = cstr(level, 'level')\n        cdef char *_level = level\n\n        if callback is None:\n            with nogil:\n                r = rados_monitor_log2(self.cluster, <const char*>_level, NULL, NULL)\n            self.monitor_callback = None\n            self.monitor_callback2 = None\n            return\n\n        cb = (callback, arg)\n        cdef PyObject* _arg = <PyObject*>cb\n        with nogil:\n            r = rados_monitor_log2(self.cluster, <const char*>_level,\n                                  <rados_log_callback2_t>&__monitor_callback2, _arg)\n\n        if r:\n            raise make_ex(r, 'error calling rados_monitor_log')\n        # NOTE(sileht): Prevents the callback method from being garbage collected\n        self.monitor_callback = None\n        self.monitor_callback2 = cb\n\n    @requires(('service', str_type), ('daemon', str_type), ('metadata', dict))\n    def service_daemon_register(self, service, daemon, metadata):\n        \"\"\"\n        :param str service: service name (e.g. \"rgw\")\n        :param str daemon: daemon name (e.g. \"gwfoo\")\n        :param dict metadata: static metadata about the register daemon\n               (e.g., the version of Ceph, the kernel version.)\n        \"\"\"\n        service = cstr(service, 'service')\n        daemon = cstr(daemon, 'daemon')\n        metadata_dict = '\\0'.join(chain.from_iterable(metadata.items()))\n        metadata_dict += '\\0'\n        cdef:\n            char *_service = service\n            char *_daemon = daemon\n            char *_metadata = metadata_dict\n\n        with nogil:\n            ret = rados_service_register(self.cluster, _service, _daemon, _metadata)\n        if ret != 0:\n            raise make_ex(ret, \"error calling service_register()\")\n\n    @requires(('metadata', dict))\n    def service_daemon_update(self, status):\n        status_dict = '\\0'.join(chain.from_iterable(status.items()))\n        status_dict += '\\0'\n        cdef:\n            char *_status = status_dict\n\n        with nogil:\n            ret = rados_service_update_status(self.cluster, _status)\n        if ret != 0:\n            raise make_ex(ret, \"error calling service_daemon_update()\")\n\n\ncdef class OmapIterator(object):\n    \"\"\"Omap iterator\"\"\"\n\n    cdef public Ioctx ioctx\n    cdef rados_omap_iter_t ctx\n\n    def __cinit__(self, Ioctx ioctx):\n        self.ioctx = ioctx\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next key-value pair in the object\n        :returns: next rados.OmapItem\n        \"\"\"\n        cdef:\n            char *key_ = NULL\n            char *val_ = NULL\n            size_t len_\n\n        with nogil:\n            ret = rados_omap_get_next(self.ctx, &key_, &val_, &len_)\n\n        if ret != 0:\n            raise make_ex(ret, \"error iterating over the omap\")\n        if key_ == NULL:\n            raise StopIteration()\n        key = decode_cstr(key_)\n        val = None\n        if val_ != NULL:\n            val = val_[:len_]\n        return (key, val)\n\n    def __dealloc__(self):\n        with nogil:\n            rados_omap_get_end(self.ctx)\n\n\ncdef class ObjectIterator(object):\n    \"\"\"rados.Ioctx Object iterator\"\"\"\n\n    cdef rados_list_ctx_t ctx\n\n    cdef public object ioctx\n\n    def __cinit__(self, Ioctx ioctx):\n        self.ioctx = ioctx\n\n        with nogil:\n            ret = rados_nobjects_list_open(ioctx.io, &self.ctx)\n        if ret < 0:\n            raise make_ex(ret, \"error iterating over the objects in ioctx '%s'\"\n                          % self.ioctx.name)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next object name and locator in the pool\n\n        :raises: StopIteration\n        :returns: next rados.Ioctx Object\n        \"\"\"\n        cdef:\n            const char *key_ = NULL\n            const char *locator_ = NULL\n            const char *nspace_ = NULL\n\n        with nogil:\n            ret = rados_nobjects_list_next(self.ctx, &key_, &locator_, &nspace_)\n\n        if ret < 0:\n            raise StopIteration()\n\n        key = decode_cstr(key_)\n        locator = decode_cstr(locator_) if locator_ != NULL else None\n        nspace = decode_cstr(nspace_) if nspace_ != NULL else None\n        return Object(self.ioctx, key, locator, nspace)\n\n    def __dealloc__(self):\n        with nogil:\n            rados_nobjects_list_close(self.ctx)\n\n\ncdef class XattrIterator(object):\n    \"\"\"Extended attribute iterator\"\"\"\n\n    cdef rados_xattrs_iter_t it\n    cdef char* _oid\n\n    cdef public Ioctx ioctx\n    cdef public object oid\n\n    def __cinit__(self, Ioctx ioctx, oid):\n        self.ioctx = ioctx\n        self.oid = cstr(oid, 'oid')\n        self._oid = self.oid\n\n        with nogil:\n            ret = rados_getxattrs(ioctx.io,  self._oid, &self.it)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to get rados xattrs for object %r\" % oid)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next xattr on the object\n\n        :raises: StopIteration\n        :returns: pair - of name and value of the next Xattr\n        \"\"\"\n        cdef:\n            const char *name_ = NULL\n            const char *val_ = NULL\n            size_t len_ = 0\n\n        with nogil:\n            ret = rados_getxattrs_next(self.it, &name_, &val_, &len_)\n        if ret != 0:\n            raise make_ex(ret, \"error iterating over the extended attributes \\\nin '%s'\" % self.oid)\n        if name_ == NULL:\n            raise StopIteration()\n        name = decode_cstr(name_)\n        val = val_[:len_]\n        return (name, val)\n\n    def __dealloc__(self):\n        with nogil:\n            rados_getxattrs_end(self.it)\n\n\ncdef class SnapIterator(object):\n    \"\"\"Snapshot iterator\"\"\"\n\n    cdef public Ioctx ioctx\n\n    cdef rados_snap_t *snaps\n    cdef int max_snap\n    cdef int cur_snap\n\n    def __cinit__(self, Ioctx ioctx):\n        self.ioctx = ioctx\n        # We don't know how big a buffer we need until we've called the\n        # function. So use the exponential doubling strategy.\n        cdef int num_snaps = 10\n        while True:\n            self.snaps = <rados_snap_t*>realloc_chk(self.snaps,\n                                                    num_snaps *\n                                                    sizeof(rados_snap_t))\n\n            with nogil:\n                ret = rados_ioctx_snap_list(ioctx.io, self.snaps, num_snaps)\n            if ret >= 0:\n                self.max_snap = ret\n                break\n            elif ret != -errno.ERANGE:\n                raise make_ex(ret, \"error calling rados_snap_list for \\\nioctx '%s'\" % self.ioctx.name)\n            num_snaps = num_snaps * 2\n        self.cur_snap = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next Snapshot\n\n        :raises: :class:`Error`, StopIteration\n        :returns: Snap - next snapshot\n        \"\"\"\n        if self.cur_snap >= self.max_snap:\n            raise StopIteration\n\n        cdef:\n            rados_snap_t snap_id = self.snaps[self.cur_snap]\n            int name_len = 10\n            char *name = NULL\n\n        try:\n            while True:\n                name = <char *>realloc_chk(name, name_len)\n                with nogil:\n                    ret = rados_ioctx_snap_get_name(self.ioctx.io, snap_id, name, name_len)\n                if ret == 0:\n                    break\n                elif ret != -errno.ERANGE:\n                    raise make_ex(ret, \"rados_snap_get_name error\")\n                else:\n                    name_len = name_len * 2\n\n            snap = Snap(self.ioctx, decode_cstr(name[:name_len]).rstrip('\\0'), snap_id)\n            self.cur_snap = self.cur_snap + 1\n            return snap\n        finally:\n            free(name)\n\n\ncdef class Snap(object):\n    \"\"\"Snapshot object\"\"\"\n    cdef public Ioctx ioctx\n    cdef public object name\n\n    # NOTE(sileht): old API was storing the ctypes object\n    # instead of the value ....\n    cdef public rados_snap_t snap_id\n\n    def __cinit__(self, Ioctx ioctx, object name, rados_snap_t snap_id):\n        self.ioctx = ioctx\n        self.name = name\n        self.snap_id = snap_id\n\n    def __str__(self):\n        return \"rados.Snap(ioctx=%s,name=%s,snap_id=%d)\" \\\n            % (str(self.ioctx), self.name, self.snap_id)\n\n    def get_timestamp(self):\n        \"\"\"\n        Find when a snapshot in the current pool occurred\n\n        :raises: :class:`Error`\n        :returns: datetime - the data and time the snapshot was created\n        \"\"\"\n        cdef time_t snap_time\n\n        with nogil:\n            ret = rados_ioctx_snap_get_stamp(self.ioctx.io, self.snap_id, &snap_time)\n        if ret != 0:\n            raise make_ex(ret, \"rados_ioctx_snap_get_stamp error\")\n        return datetime.fromtimestamp(snap_time)\n\n\ncdef class Completion(object):\n    \"\"\"completion object\"\"\"\n\n    cdef public:\n         Ioctx ioctx\n         object oncomplete\n         object onsafe\n\n    cdef:\n         rados_callback_t complete_cb\n         rados_callback_t safe_cb\n         rados_completion_t rados_comp\n         PyObject* buf\n\n    def __cinit__(self, Ioctx ioctx, object oncomplete, object onsafe):\n        self.oncomplete = oncomplete\n        self.onsafe = onsafe\n        self.ioctx = ioctx\n\n    def is_safe(self):\n        \"\"\"\n        Is an asynchronous operation safe?\n\n        This does not imply that the safe callback has finished.\n\n        :returns: True if the operation is safe\n        \"\"\"\n        with nogil:\n            ret = rados_aio_is_safe(self.rados_comp)\n        return ret == 1\n\n    def is_complete(self):\n        \"\"\"\n        Has an asynchronous operation completed?\n\n        This does not imply that the safe callback has finished.\n\n        :returns: True if the operation is completed\n        \"\"\"\n        with nogil:\n            ret = rados_aio_is_complete(self.rados_comp)\n        return ret == 1\n\n    def wait_for_safe(self):\n        \"\"\"\n        Wait for an asynchronous operation to be marked safe\n\n        This does not imply that the safe callback has finished.\n        \"\"\"\n        with nogil:\n            rados_aio_wait_for_safe(self.rados_comp)\n\n    def wait_for_complete(self):\n        \"\"\"\n        Wait for an asynchronous operation to complete\n\n        This does not imply that the complete callback has finished.\n        \"\"\"\n        with nogil:\n            rados_aio_wait_for_complete(self.rados_comp)\n\n    def wait_for_safe_and_cb(self):\n        \"\"\"\n        Wait for an asynchronous operation to be marked safe and for\n        the safe callback to have returned\n        \"\"\"\n        with nogil:\n            rados_aio_wait_for_safe_and_cb(self.rados_comp)\n\n    def wait_for_complete_and_cb(self):\n        \"\"\"\n        Wait for an asynchronous operation to complete and for the\n        complete callback to have returned\n\n        :returns:  whether the operation is completed\n        \"\"\"\n        with nogil:\n            ret = rados_aio_wait_for_complete_and_cb(self.rados_comp)\n        return ret\n\n    def get_return_value(self):\n        \"\"\"\n        Get the return value of an asychronous operation\n\n        The return value is set when the operation is complete or safe,\n        whichever comes first.\n\n        :returns: int - return value of the operation\n        \"\"\"\n        with nogil:\n            ret = rados_aio_get_return_value(self.rados_comp)\n        return ret\n\n    def __dealloc__(self):\n        \"\"\"\n        Release a completion\n\n        Call this when you no longer need the completion. It may not be\n        freed immediately if the operation is not acked and committed.\n        \"\"\"\n        ref.Py_XDECREF(self.buf)\n        self.buf = NULL\n        if self.rados_comp != NULL:\n            with nogil:\n                rados_aio_release(self.rados_comp)\n                self.rados_comp = NULL\n\n    def _complete(self):\n        self.oncomplete(self)\n        with self.ioctx.lock:\n            if self.oncomplete:\n                self.ioctx.complete_completions.remove(self)\n\n    def _safe(self):\n        self.onsafe(self)\n        with self.ioctx.lock:\n            if self.onsafe:\n                self.ioctx.safe_completions.remove(self)\n\n    def _cleanup(self):\n        with self.ioctx.lock:\n            if self.oncomplete:\n                self.ioctx.complete_completions.remove(self)\n            if self.onsafe:\n                self.ioctx.safe_completions.remove(self)\n\n\nclass OpCtx(object):\n    def __enter__(self):\n        return self.create()\n\n    def __exit__(self, type, msg, traceback):\n        self.release()\n\n\ncdef class WriteOp(object):\n    cdef rados_write_op_t write_op\n\n    def create(self):\n        with nogil:\n            self.write_op = rados_create_write_op()\n        return self\n\n    def release(self):\n        with nogil:\n            rados_release_write_op(self.write_op)\n\n    @requires(('exclusive', opt(int)))\n    def new(self, exclusive=None):\n        \"\"\"\n        Create the object.\n        \"\"\"\n\n        cdef:\n            int _exclusive = exclusive\n\n        with nogil:\n            rados_write_op_create(self.write_op, _exclusive, NULL)\n\n\n    def remove(self):\n        \"\"\"\n        Remove object.\n        \"\"\"\n        with nogil:\n            rados_write_op_remove(self.write_op)\n\n    @requires(('flags', int))\n    def set_flags(self, flags=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        Set flags for the last operation added to this write_op.\n        :para flags: flags to apply to the last operation\n        :type flags: int\n        \"\"\"\n\n        cdef:\n            int _flags = flags\n\n        with nogil:\n            rados_write_op_set_flags(self.write_op, _flags)\n\n    @requires(('to_write', bytes))\n    def append(self, to_write):\n        \"\"\"\n        Append data to an object synchronously\n        :param to_write: data to write\n        :type to_write: bytes\n        \"\"\"\n\n        cdef:\n            char *_to_write = to_write\n            size_t length = len(to_write)\n\n        with nogil:\n            rados_write_op_append(self.write_op, _to_write, length)\n\n    @requires(('to_write', bytes))\n    def write_full(self, to_write):\n        \"\"\"\n        Write whole object, atomically replacing it.\n        :param to_write: data to write\n        :type to_write: bytes\n        \"\"\"\n\n        cdef:\n            char *_to_write = to_write\n            size_t length = len(to_write)\n\n        with nogil:\n            rados_write_op_write_full(self.write_op, _to_write, length)\n\n    @requires(('to_write', bytes), ('offset', int))\n    def write(self, to_write, offset=0):\n        \"\"\"\n        Write to offset.\n        :param to_write: data to write\n        :type to_write: bytes\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n        \"\"\"\n\n        cdef:\n            char *_to_write = to_write\n            size_t length = len(to_write)\n            uint64_t _offset = offset\n\n        with nogil:\n            rados_write_op_write(self.write_op, _to_write, length, _offset)\n\n    @requires(('offset', int), ('length', int))\n    def zero(self, offset, length):\n        \"\"\"\n        Zero part of an object.\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n        :param offset: number of zero to write\n        :type offset: int\n        \"\"\"\n\n        cdef:\n            size_t _length = length\n            uint64_t _offset = offset\n\n        with nogil:\n            rados_write_op_zero(self.write_op, _length, _offset)\n\n    @requires(('offset', int))\n    def truncate(self, offset):\n        \"\"\"\n        Truncate an object.\n        :param offset: byte offset in the object to begin truncating at\n        :type offset: int\n        \"\"\"\n\n        cdef:\n            uint64_t _offset = offset\n\n        with nogil:\n            rados_write_op_truncate(self.write_op,  _offset)\n\n\nclass WriteOpCtx(WriteOp, OpCtx):\n    \"\"\"write operation context manager\"\"\"\n\n\ncdef class ReadOp(object):\n    cdef rados_read_op_t read_op\n\n    def create(self):\n        with nogil:\n            self.read_op = rados_create_read_op()\n        return self\n\n    def release(self):\n        with nogil:\n            rados_release_read_op(self.read_op)\n\n    @requires(('flags', int))\n    def set_flags(self, flags=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        Set flags for the last operation added to this read_op.\n        :para flags: flags to apply to the last operation\n        :type flags: int\n        \"\"\"\n\n        cdef:\n            int _flags = flags\n\n        with nogil:\n            rados_read_op_set_flags(self.read_op, _flags)\n\n\nclass ReadOpCtx(ReadOp, OpCtx):\n    \"\"\"read operation context manager\"\"\"\n\n\ncdef int __aio_safe_cb(rados_completion_t completion, void *args) with gil:\n    \"\"\"\n    Callback to onsafe() for asynchronous operations\n    \"\"\"\n    cdef object cb = <object>args\n    cb._safe()\n    return 0\n\n\ncdef int __aio_complete_cb(rados_completion_t completion, void *args) with gil:\n    \"\"\"\n    Callback to oncomplete() for asynchronous operations\n    \"\"\"\n    cdef object cb = <object>args\n    cb._complete()\n    return 0\n\n\ncdef class Ioctx(object):\n    \"\"\"rados.Ioctx object\"\"\"\n    # NOTE(sileht): attributes declared in .pyd\n\n    def __init__(self, name):\n        self.name = name\n        self.state = \"open\"\n\n        self.locator_key = \"\"\n        self.nspace = \"\"\n        self.lock = threading.Lock()\n        self.safe_completions = []\n        self.complete_completions = []\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        self.close()\n        return False\n\n    def __dealloc__(self):\n        self.close()\n\n    def __track_completion(self, completion_obj):\n        if completion_obj.oncomplete:\n            with self.lock:\n                self.complete_completions.append(completion_obj)\n        if completion_obj.onsafe:\n            with self.lock:\n                self.safe_completions.append(completion_obj)\n\n    def __get_completion(self, oncomplete, onsafe):\n        \"\"\"\n        Constructs a completion to use with asynchronous operations\n\n        :param oncomplete: what to do when the write is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the write is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        completion_obj = Completion(self, oncomplete, onsafe)\n\n        cdef:\n            rados_callback_t complete_cb = NULL\n            rados_callback_t safe_cb = NULL\n            rados_completion_t completion\n            PyObject* p_completion_obj= <PyObject*>completion_obj\n\n        if oncomplete:\n            complete_cb = <rados_callback_t>&__aio_complete_cb\n        if onsafe:\n            safe_cb = <rados_callback_t>&__aio_safe_cb\n\n        with nogil:\n            ret = rados_aio_create_completion(p_completion_obj, complete_cb, safe_cb,\n                                              &completion)\n        if ret < 0:\n            raise make_ex(ret, \"error getting a completion\")\n\n        completion_obj.rados_comp = completion\n        return completion_obj\n\n    @requires(('object_name', str_type), ('oncomplete', opt(Callable)))\n    def aio_stat(self, object_name, oncomplete):\n        \"\"\"\n        Asynchronously get object stats (size/mtime)\n\n        oncomplete will be called with the returned size and mtime\n        as well as the completion:\n\n        oncomplete(completion, size, mtime)\n\n        :param object_name: the name of the object to get stats from\n        :type object_name: str\n        :param oncomplete: what to do when the stat is complete\n        :type oncomplete: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char *_object_name = object_name\n            uint64_t psize\n            time_t pmtime\n\n        def oncomplete_(completion_v):\n            cdef Completion _completion_v = completion_v\n            return_value = _completion_v.get_return_value()\n            if return_value >= 0:\n                return oncomplete(_completion_v, psize, time.localtime(pmtime))\n            else:\n                return oncomplete(_completion_v, None, None)\n\n        completion = self.__get_completion(oncomplete_, None)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_stat(self.io, _object_name, completion.rados_comp,\n                                 &psize, &pmtime)\n\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error stating %s\" % object_name)\n        return completion\n\n    @requires(('object_name', str_type), ('to_write', bytes), ('offset', int),\n              ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)))\n    def aio_write(self, object_name, to_write, offset=0,\n                  oncomplete=None, onsafe=None):\n        \"\"\"\n        Write data to an object asynchronously\n\n        Queues the write and returns.\n\n        :param object_name: name of the object\n        :type object_name: str\n        :param to_write: data to write\n        :type to_write: bytes\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n        :param oncomplete: what to do when the write is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the write is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n            char* _to_write = to_write\n            size_t size = len(to_write)\n            uint64_t _offset = offset\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_write(self.io, _object_name, completion.rados_comp,\n                                _to_write, size, _offset)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error writing object %s\" % object_name)\n        return completion\n\n    @requires(('object_name', str_type), ('to_write', bytes), ('oncomplete', opt(Callable)),\n              ('onsafe', opt(Callable)))\n    def aio_write_full(self, object_name, to_write,\n                       oncomplete=None, onsafe=None):\n        \"\"\"\n        Asychronously write an entire object\n\n        The object is filled with the provided data. If the object exists,\n        it is atomically truncated and then written.\n        Queues the write and returns.\n\n        :param object_name: name of the object\n        :type object_name: str\n        :param to_write: data to write\n        :type to_write: str\n        :param oncomplete: what to do when the write is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the write is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n            char* _to_write = to_write\n            size_t size = len(to_write)\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_write_full(self.io, _object_name,\n                                    completion.rados_comp,\n                                    _to_write, size)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error writing object %s\" % object_name)\n        return completion\n\n    @requires(('object_name', str_type), ('to_append', bytes), ('oncomplete', opt(Callable)),\n              ('onsafe', opt(Callable)))\n    def aio_append(self, object_name, to_append, oncomplete=None, onsafe=None):\n        \"\"\"\n        Asychronously append data to an object\n\n        Queues the write and returns.\n\n        :param object_name: name of the object\n        :type object_name: str\n        :param to_append: data to append\n        :type to_append: str\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n        :param oncomplete: what to do when the write is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the write is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n            char* _to_append = to_append\n            size_t size = len(to_append)\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_append(self.io, _object_name,\n                                completion.rados_comp,\n                                _to_append, size)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error appending object %s\" % object_name)\n        return completion\n\n    def aio_flush(self):\n        \"\"\"\n        Block until all pending writes in an io context are safe\n\n        :raises: :class:`Error`\n        \"\"\"\n        with nogil:\n            ret = rados_aio_flush(self.io)\n        if ret < 0:\n            raise make_ex(ret, \"error flushing\")\n\n    @requires(('object_name', str_type), ('length', int), ('offset', int),\n              ('oncomplete', opt(Callable)))\n    def aio_read(self, object_name, length, offset, oncomplete):\n        \"\"\"\n        Asychronously read data from an object\n\n        oncomplete will be called with the returned read value as\n        well as the completion:\n\n        oncomplete(completion, data_read)\n\n        :param object_name: name of the object to read from\n        :type object_name: str\n        :param length: the number of bytes to read\n        :type length: int\n        :param offset: byte offset in the object to begin reading from\n        :type offset: int\n        :param oncomplete: what to do when the read is complete\n        :type oncomplete: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n            uint64_t _offset = offset\n\n            char *ref_buf\n            size_t _length = length\n\n        def oncomplete_(completion_v):\n            cdef Completion _completion_v = completion_v\n            return_value = _completion_v.get_return_value()\n            if return_value > 0 and return_value != length:\n                _PyBytes_Resize(&_completion_v.buf, return_value)\n            return oncomplete(_completion_v, <object>_completion_v.buf if return_value >= 0 else None)\n\n        completion = self.__get_completion(oncomplete_, None)\n        completion.buf = PyBytes_FromStringAndSize(NULL, length)\n        ret_buf = PyBytes_AsString(completion.buf)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_read(self.io, _object_name, completion.rados_comp,\n                                ret_buf, _length, _offset)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error reading %s\" % object_name)\n        return completion\n\n    @requires(('object_name', str_type), ('cls', str_type), ('method', str_type),\n              ('data', bytes), ('length', int),\n              ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)))\n    def aio_execute(self, object_name, cls, method, data,\n                    length=8192, oncomplete=None, onsafe=None):\n        \"\"\"\n        Asynchronously execute an OSD class method on an object.\n\n        oncomplete and onsafe will be called with the data returned from\n        the plugin as well as the completion:\n\n        oncomplete(completion, data)\n        onsafe(completion, data)\n\n        :param object_name: name of the object\n        :type object_name: str\n        :param cls: name of the object class\n        :type cls: str\n        :param method: name of the method\n        :type method: str\n        :param data: input data\n        :type data: bytes\n        :param length: size of output buffer in bytes (default=8192)\n        :type length: int\n        :param oncomplete: what to do when the execution is complete\n        :type oncomplete: completion\n        :param onsafe:  what to do when the execution is safe and complete\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n        cls = cstr(cls, 'cls')\n        method = cstr(method, 'method')\n        cdef:\n            Completion completion\n            char *_object_name = object_name\n            char *_cls = cls\n            char *_method = method\n            char *_data = data\n            size_t _data_len = len(data)\n\n            char *ref_buf\n            size_t _length = length\n\n        def oncomplete_(completion_v):\n            cdef Completion _completion_v = completion_v\n            return_value = _completion_v.get_return_value()\n            if return_value > 0 and return_value != length:\n                _PyBytes_Resize(&_completion_v.buf, return_value)\n            return oncomplete(_completion_v, <object>_completion_v.buf if return_value >= 0 else None)\n\n        def onsafe_(completion_v):\n            cdef Completion _completion_v = completion_v\n            return_value = _completion_v.get_return_value()\n            return onsafe(_completion_v, <object>_completion_v.buf if return_value >= 0 else None)\n\n        completion = self.__get_completion(oncomplete_ if oncomplete else None, onsafe_ if onsafe else None)\n        completion.buf = PyBytes_FromStringAndSize(NULL, length)\n        ret_buf = PyBytes_AsString(completion.buf)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_exec(self.io, _object_name, completion.rados_comp,\n                                 _cls, _method, _data, _data_len, ret_buf, _length)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error executing %s::%s on %s\" % (cls, method, object_name))\n        return completion\n\n    @requires(('object_name', str_type), ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)))\n    def aio_remove(self, object_name, oncomplete=None, onsafe=None):\n        \"\"\"\n        Asychronously remove an object\n\n        :param object_name: name of the object to remove\n        :type object_name: str\n        :param oncomplete: what to do when the remove is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the remove is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_remove(self.io, _object_name,\n                                completion.rados_comp)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error removing %s\" % object_name)\n        return completion\n\n    def require_ioctx_open(self):\n        \"\"\"\n        Checks if the rados.Ioctx object state is 'open'\n\n        :raises: IoctxStateError\n        \"\"\"\n        if self.state != \"open\":\n            raise IoctxStateError(\"The pool is %s\" % self.state)\n\n    def change_auid(self, auid):\n        \"\"\"\n        Attempt to change an io context's associated auid \"owner.\"\n\n        Requires that you have write permission on both the current and new\n        auid.\n\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n\n        cdef:\n            uint64_t _auid = auid\n\n        with nogil:\n            ret = rados_ioctx_pool_set_auid(self.io, _auid)\n        if ret < 0:\n            raise make_ex(ret, \"error changing auid of '%s' to %d\"\n                          % (self.name, auid))\n\n    @requires(('loc_key', str_type))\n    def set_locator_key(self, loc_key):\n        \"\"\"\n        Set the key for mapping objects to pgs within an io context.\n\n        The key is used instead of the object name to determine which\n        placement groups an object is put in. This affects all subsequent\n        operations of the io context - until a different locator key is\n        set, all objects in this io context will be placed in the same pg.\n\n        :param loc_key: the key to use as the object locator, or NULL to discard\n            any previously set key\n        :type loc_key: str\n\n        :raises: :class:`TypeError`\n        \"\"\"\n        self.require_ioctx_open()\n        cloc_key = cstr(loc_key, 'loc_key')\n        cdef char *_loc_key = cloc_key\n        with nogil:\n            rados_ioctx_locator_set_key(self.io, _loc_key)\n        self.locator_key = loc_key\n\n    def get_locator_key(self):\n        \"\"\"\n        Get the locator_key of context\n\n        :returns: locator_key\n        \"\"\"\n        return self.locator_key\n\n    @requires(('snap_id', long))\n    def set_read(self, snap_id):\n        \"\"\"\n        Set the snapshot for reading objects.\n\n        To stop to read from snapshot, use set_read(LIBRADOS_SNAP_HEAD)\n\n        :param snap_id: the snapshot Id\n        :type snap_id: int\n\n        :raises: :class:`TypeError`\n        \"\"\"\n        self.require_ioctx_open()\n        cdef rados_snap_t _snap_id = snap_id\n        with nogil:\n            rados_ioctx_snap_set_read(self.io, _snap_id)\n\n    @requires(('nspace', str_type))\n    def set_namespace(self, nspace):\n        \"\"\"\n        Set the namespace for objects within an io context.\n\n        The namespace in addition to the object name fully identifies\n        an object. This affects all subsequent operations of the io context\n        - until a different namespace is set, all objects in this io context\n        will be placed in the same namespace.\n\n        :param nspace: the namespace to use, or None/\"\" for the default namespace\n        :type nspace: str\n\n        :raises: :class:`TypeError`\n        \"\"\"\n        self.require_ioctx_open()\n        if nspace is None:\n            nspace = \"\"\n        cnspace = cstr(nspace, 'nspace')\n        cdef char *_nspace = cnspace\n        with nogil:\n            rados_ioctx_set_namespace(self.io, _nspace)\n        self.nspace = nspace\n\n    def get_namespace(self):\n        \"\"\"\n        Get the namespace of context\n\n        :returns: namespace\n        \"\"\"\n        return self.nspace\n\n    def close(self):\n        \"\"\"\n        Close a rados.Ioctx object.\n\n        This just tells librados that you no longer need to use the io context.\n        It may not be freed immediately if there are pending asynchronous\n        requests on it, but you should not use an io context again after\n        calling this function on it.\n        \"\"\"\n        if self.state == \"open\":\n            self.require_ioctx_open()\n            with nogil:\n                rados_ioctx_destroy(self.io)\n            self.state = \"closed\"\n\n\n    @requires(('key', str_type), ('data', bytes))\n    def write(self, key, data, offset=0):\n        \"\"\"\n        Write data to an object synchronously\n\n        :param key: name of the object\n        :type key: str\n        :param data: data to write\n        :type data: bytes\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`LogicError`\n        :returns: int - 0 on success\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            char *_data = data\n            size_t length = len(data)\n            uint64_t _offset = offset\n\n        with nogil:\n            ret = rados_write(self.io, _key, _data, length, _offset)\n        if ret == 0:\n            return ret\n        elif ret < 0:\n            raise make_ex(ret, \"Ioctx.write(%s): failed to write %s\"\n                          % (self.name, key))\n        else:\n            raise LogicError(\"Ioctx.write(%s): rados_write \\\nreturned %d, but should return zero on success.\" % (self.name, ret))\n\n    @requires(('key', str_type), ('data', bytes))\n    def write_full(self, key, data):\n        \"\"\"\n        Write an entire object synchronously.\n\n        The object is filled with the provided data. If the object exists,\n        it is atomically truncated and then written.\n\n        :param key: name of the object\n        :type key: str\n        :param data: data to write\n        :type data: bytes\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: int - 0 on success\n        \"\"\"\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            char *_data = data\n            size_t length = len(data)\n\n        with nogil:\n            ret = rados_write_full(self.io, _key, _data, length)\n        if ret == 0:\n            return ret\n        elif ret < 0:\n            raise make_ex(ret, \"Ioctx.write_full(%s): failed to write %s\"\n                          % (self.name, key))\n        else:\n            raise LogicError(\"Ioctx.write_full(%s): rados_write_full \\\nreturned %d, but should return zero on success.\" % (self.name, ret))\n\n    @requires(('key', str_type), ('data', bytes))\n    def append(self, key, data):\n        \"\"\"\n        Append data to an object synchronously\n\n        :param key: name of the object\n        :type key: str\n        :param data: data to write\n        :type data: bytes\n\n        :raises: :class:`TypeError`\n        :raises: :class:`LogicError`\n        :returns: int - 0 on success\n        \"\"\"\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            char *_data = data\n            size_t length = len(data)\n\n        with nogil:\n            ret = rados_append(self.io, _key, _data, length)\n        if ret == 0:\n            return ret\n        elif ret < 0:\n            raise make_ex(ret, \"Ioctx.append(%s): failed to append %s\"\n                          % (self.name, key))\n        else:\n            raise LogicError(\"Ioctx.append(%s): rados_append \\\nreturned %d, but should return zero on success.\" % (self.name, ret))\n\n    @requires(('key', str_type))\n    def read(self, key, length=8192, offset=0):\n        \"\"\"\n        Read data from an object synchronously\n\n        :param key: name of the object\n        :type key: str\n        :param length: the number of bytes to read (default=8192)\n        :type length: int\n        :param offset: byte offset in the object to begin reading at\n        :type offset: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: str - data read from object\n        \"\"\"\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            char *ret_buf\n            uint64_t _offset = offset\n            size_t _length = length\n            PyObject* ret_s = NULL\n\n        ret_s = PyBytes_FromStringAndSize(NULL, length)\n        try:\n            ret_buf = PyBytes_AsString(ret_s)\n            with nogil:\n                ret = rados_read(self.io, _key, ret_buf, _length, _offset)\n            if ret < 0:\n                raise make_ex(ret, \"Ioctx.read(%s): failed to read %s\" % (self.name, key))\n\n            if ret != length:\n                _PyBytes_Resize(&ret_s, ret)\n\n            return <object>ret_s\n        finally:\n            # We DECREF unconditionally: the cast to object above will have\n            # INCREFed if necessary. This also takes care of exceptions,\n            # including if _PyString_Resize fails (that will free the string\n            # itself and set ret_s to NULL, hence XDECREF).\n            ref.Py_XDECREF(ret_s)\n\n    @requires(('key', str_type), ('cls', str_type), ('method', str_type), ('data', bytes))\n    def execute(self, key, cls, method, data, length=8192):\n        \"\"\"\n        Execute an OSD class method on an object.\n\n        :param key: name of the object\n        :type key: str\n        :param cls: name of the object class\n        :type cls: str\n        :param method: name of the method\n        :type method: str\n        :param data: input data\n        :type data: bytes\n        :param length: size of output buffer in bytes (default=8192)\n        :type length: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: (ret, method output)\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        cls = cstr(cls, 'cls')\n        method = cstr(method, 'method')\n        cdef:\n            char *_key = key\n            char *_cls = cls\n            char *_method = method\n            char *_data = data\n            size_t _data_len = len(data)\n\n            char *ref_buf\n            size_t _length = length\n            PyObject* ret_s = NULL\n\n        ret_s = PyBytes_FromStringAndSize(NULL, length)\n        try:\n            ret_buf = PyBytes_AsString(ret_s)\n            with nogil:\n                ret = rados_exec(self.io, _key, _cls, _method, _data,\n                                 _data_len, ret_buf, _length)\n            if ret < 0:\n                raise make_ex(ret, \"Ioctx.read(%s): failed to read %s\" % (self.name, key))\n\n            if ret != length:\n                _PyBytes_Resize(&ret_s, ret)\n\n            return ret, <object>ret_s\n        finally:\n            # We DECREF unconditionally: the cast to object above will have\n            # INCREFed if necessary. This also takes care of exceptions,\n            # including if _PyString_Resize fails (that will free the string\n            # itself and set ret_s to NULL, hence XDECREF).\n            ref.Py_XDECREF(ret_s)\n\n    def get_stats(self):\n        \"\"\"\n        Get pool usage statistics\n\n        :returns: dict - contains the following keys:\n\n            - ``num_bytes`` (int) - size of pool in bytes\n\n            - ``num_kb`` (int) - size of pool in kbytes\n\n            - ``num_objects`` (int) - number of objects in the pool\n\n            - ``num_object_clones`` (int) - number of object clones\n\n            - ``num_object_copies`` (int) - number of object copies\n\n            - ``num_objects_missing_on_primary`` (int) - number of objets\n                missing on primary\n\n            - ``num_objects_unfound`` (int) - number of unfound objects\n\n            - ``num_objects_degraded`` (int) - number of degraded objects\n\n            - ``num_rd`` (int) - bytes read\n\n            - ``num_rd_kb`` (int) - kbytes read\n\n            - ``num_wr`` (int) - bytes written\n\n            - ``num_wr_kb`` (int) - kbytes written\n        \"\"\"\n        self.require_ioctx_open()\n        cdef rados_pool_stat_t stats\n        with nogil:\n            ret = rados_ioctx_pool_stat(self.io, &stats)\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.get_stats(%s): get_stats failed\" % self.name)\n        return {'num_bytes': stats.num_bytes,\n                'num_kb': stats.num_kb,\n                'num_objects': stats.num_objects,\n                'num_object_clones': stats.num_object_clones,\n                'num_object_copies': stats.num_object_copies,\n                \"num_objects_missing_on_primary\": stats.num_objects_missing_on_primary,\n                \"num_objects_unfound\": stats.num_objects_unfound,\n                \"num_objects_degraded\": stats.num_objects_degraded,\n                \"num_rd\": stats.num_rd,\n                \"num_rd_kb\": stats.num_rd_kb,\n                \"num_wr\": stats.num_wr,\n                \"num_wr_kb\": stats.num_wr_kb}\n\n    @requires(('key', str_type))\n    def remove_object(self, key):\n        \"\"\"\n        Delete an object\n\n        This does not delete any snapshots of the object.\n\n        :param key: the name of the object to delete\n        :type key: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: bool - True on success\n        \"\"\"\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n\n        with nogil:\n            ret = rados_remove(self.io, _key)\n        if ret < 0:\n            raise make_ex(ret, \"Failed to remove '%s'\" % key)\n        return True\n\n    @requires(('key', str_type))\n    def trunc(self, key, size):\n        \"\"\"\n        Resize an object\n\n        If this enlarges the object, the new area is logically filled with\n        zeroes. If this shrinks the object, the excess data is removed.\n\n        :param key: the name of the object to resize\n        :type key: str\n        :param size: the new size of the object in bytes\n        :type size: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: int - 0 on success, otherwise raises error\n        \"\"\"\n\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            uint64_t _size = size\n\n        with nogil:\n            ret = rados_trunc(self.io, _key, _size)\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.trunc(%s): failed to truncate %s\" % (self.name, key))\n        return ret\n\n    @requires(('key', str_type))\n    def stat(self, key):\n        \"\"\"\n        Get object stats (size/mtime)\n\n        :param key: the name of the object to get stats from\n        :type key: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: (size,timestamp)\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            uint64_t psize\n            time_t pmtime\n\n        with nogil:\n            ret = rados_stat(self.io, _key, &psize, &pmtime)\n        if ret < 0:\n            raise make_ex(ret, \"Failed to stat %r\" % key)\n        return psize, time.localtime(pmtime)\n\n    @requires(('key', str_type), ('xattr_name', str_type))\n    def get_xattr(self, key, xattr_name):\n        \"\"\"\n        Get the value of an extended attribute on an object.\n\n        :param key: the name of the object to get xattr from\n        :type key: str\n        :param xattr_name: which extended attribute to read\n        :type xattr_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: str - value of the xattr\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        xattr_name = cstr(xattr_name, 'xattr_name')\n        cdef:\n            char *_key = key\n            char *_xattr_name = xattr_name\n            size_t ret_length = 4096\n            char *ret_buf = NULL\n\n        try:\n            while ret_length < 4096 * 1024 * 1024:\n                ret_buf = <char *>realloc_chk(ret_buf, ret_length)\n                with nogil:\n                    ret = rados_getxattr(self.io, _key, _xattr_name, ret_buf, ret_length)\n                if ret == -errno.ERANGE:\n                    ret_length *= 2\n                elif ret < 0:\n                    raise make_ex(ret, \"Failed to get xattr %r\" % xattr_name)\n                else:\n                    break\n            return ret_buf[:ret]\n        finally:\n            free(ret_buf)\n\n    @requires(('oid', str_type))\n    def get_xattrs(self, oid):\n        \"\"\"\n        Start iterating over xattrs on an object.\n\n        :param oid: the name of the object to get xattrs from\n        :type oid: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: XattrIterator\n        \"\"\"\n        self.require_ioctx_open()\n        return XattrIterator(self, oid)\n\n    @requires(('key', str_type), ('xattr_name', str_type), ('xattr_value', bytes))\n    def set_xattr(self, key, xattr_name, xattr_value):\n        \"\"\"\n        Set an extended attribute on an object.\n\n        :param key: the name of the object to set xattr to\n        :type key: str\n        :param xattr_name: which extended attribute to set\n        :type xattr_name: str\n        :param xattr_value: the value of the  extended attribute\n        :type xattr_value: bytes\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: bool - True on success, otherwise raise an error\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        xattr_name = cstr(xattr_name, 'xattr_name')\n        cdef:\n            char *_key = key\n            char *_xattr_name = xattr_name\n            char *_xattr_value = xattr_value\n            size_t _xattr_value_len = len(xattr_value)\n\n        with nogil:\n            ret = rados_setxattr(self.io, _key, _xattr_name,\n                                 _xattr_value, _xattr_value_len)\n        if ret < 0:\n            raise make_ex(ret, \"Failed to set xattr %r\" % xattr_name)\n        return True\n\n    @requires(('key', str_type), ('xattr_name', str_type))\n    def rm_xattr(self, key, xattr_name):\n        \"\"\"\n        Removes an extended attribute on from an object.\n\n        :param key: the name of the object to remove xattr from\n        :type key: str\n        :param xattr_name: which extended attribute to remove\n        :type xattr_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: bool - True on success, otherwise raise an error\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        xattr_name = cstr(xattr_name, 'xattr_name')\n        cdef:\n            char *_key = key\n            char *_xattr_name = xattr_name\n\n        with nogil:\n            ret = rados_rmxattr(self.io, _key, _xattr_name)\n        if ret < 0:\n            raise make_ex(ret, \"Failed to delete key %r xattr %r\" %\n                          (key, xattr_name))\n        return True\n\n    def list_objects(self):\n        \"\"\"\n        Get ObjectIterator on rados.Ioctx object.\n\n        :returns: ObjectIterator\n        \"\"\"\n        self.require_ioctx_open()\n        return ObjectIterator(self)\n\n    def list_snaps(self):\n        \"\"\"\n        Get SnapIterator on rados.Ioctx object.\n\n        :returns: SnapIterator\n        \"\"\"\n        self.require_ioctx_open()\n        return SnapIterator(self)\n\n    @requires(('snap_name', str_type))\n    def create_snap(self, snap_name):\n        \"\"\"\n        Create a pool-wide snapshot\n\n        :param snap_name: the name of the snapshot\n        :type snap_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        snap_name = cstr(snap_name, 'snap_name')\n        cdef char *_snap_name = snap_name\n\n        with nogil:\n            ret = rados_ioctx_snap_create(self.io, _snap_name)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to create snap %s\" % snap_name)\n\n    @requires(('snap_name', str_type))\n    def remove_snap(self, snap_name):\n        \"\"\"\n        Removes a pool-wide snapshot\n\n        :param snap_name: the name of the snapshot\n        :type snap_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        snap_name = cstr(snap_name, 'snap_name')\n        cdef char *_snap_name = snap_name\n\n        with nogil:\n            ret = rados_ioctx_snap_remove(self.io, _snap_name)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to remove snap %s\" % snap_name)\n\n    @requires(('snap_name', str_type))\n    def lookup_snap(self, snap_name):\n        \"\"\"\n        Get the id of a pool snapshot\n\n        :param snap_name: the name of the snapshot to lookop\n        :type snap_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: Snap - on success\n        \"\"\"\n        self.require_ioctx_open()\n        csnap_name = cstr(snap_name, 'snap_name')\n        cdef:\n            char *_snap_name = csnap_name\n            rados_snap_t snap_id\n\n        with nogil:\n            ret = rados_ioctx_snap_lookup(self.io, _snap_name, &snap_id)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to lookup snap %s\" % snap_name)\n        return Snap(self, snap_name, int(snap_id))\n\n    @requires(('oid', str_type), ('snap_name', str_type))\n    def snap_rollback(self, oid, snap_name):\n        \"\"\"\n        Rollback an object to a snapshot\n\n        :param oid: the name of the object\n        :type oid: str\n        :param snap_name: the name of the snapshot\n        :type snap_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        oid = cstr(oid, 'oid')\n        snap_name = cstr(snap_name, 'snap_name')\n        cdef:\n            char *_snap_name = snap_name\n            char *_oid = oid\n\n        with nogil:\n            ret = rados_ioctx_snap_rollback(self.io, _oid, _snap_name)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to rollback %s\" % oid)\n\n    def get_last_version(self):\n        \"\"\"\n        Return the version of the last object read or written to.\n\n        This exposes the internal version number of the last object read or\n        written via this io context\n\n        :returns: version of the last object used\n        \"\"\"\n        self.require_ioctx_open()\n        with nogil:\n            ret = rados_get_last_version(self.io)\n        return int(ret)\n\n    def create_write_op(self):\n        \"\"\"\n        create write operation object.\n        need call release_write_op after use\n        \"\"\"\n        return WriteOp().create()\n\n    def create_read_op(self):\n        \"\"\"\n        create read operation object.\n        need call release_read_op after use\n        \"\"\"\n        return ReadOp().create()\n\n    def release_write_op(self, write_op):\n        \"\"\"\n        release memory alloc by create_write_op\n        \"\"\"\n        write_op.release()\n\n    def release_read_op(self, read_op):\n        \"\"\"\n        release memory alloc by create_read_op\n        :para read_op: read_op object\n        :type: int\n        \"\"\"\n        read_op.release()\n\n    @requires(('write_op', WriteOp), ('keys', tuple), ('values', tuple))\n    def set_omap(self, write_op, keys, values):\n        \"\"\"\n        set keys values to write_op\n        :para write_op: write_operation object\n        :type write_op: WriteOp\n        :para keys: a tuple of keys\n        :type keys: tuple\n        :para values: a tuple of values\n        :type values: tuple\n        \"\"\"\n\n        if len(keys) != len(values):\n            raise Error(\"Rados(): keys and values must have the same number of items\")\n\n        keys = cstr_list(keys, 'keys')\n        cdef:\n            WriteOp _write_op = write_op\n            size_t key_num = len(keys)\n            char **_keys = to_bytes_array(keys)\n            char **_values = to_bytes_array(values)\n            size_t *_lens = to_csize_t_array([len(v) for v in values])\n\n        try:\n            with nogil:\n                rados_write_op_omap_set(_write_op.write_op,\n                                        <const char**>_keys,\n                                        <const char**>_values,\n                                        <const size_t*>_lens, key_num)\n        finally:\n            free(_keys)\n            free(_values)\n            free(_lens)\n\n    @requires(('write_op', WriteOp), ('oid', str_type), ('mtime', opt(int)), ('flags', opt(int)))\n    def operate_write_op(self, write_op, oid, mtime=0, flags=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        execute the real write operation\n        :para write_op: write operation object\n        :type write_op: WriteOp\n        :para oid: object name\n        :type oid: str\n        :para mtime: the time to set the mtime to, 0 for the current time\n        :type mtime: int\n        :para flags: flags to apply to the entire operation\n        :type flags: int\n        \"\"\"\n\n        oid = cstr(oid, 'oid')\n        cdef:\n            WriteOp _write_op = write_op\n            char *_oid = oid\n            time_t _mtime = mtime\n            int _flags = flags\n\n        with nogil:\n            ret = rados_write_op_operate(_write_op.write_op, self.io, _oid, &_mtime, _flags)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to operate write op for oid %s\" % oid)\n\n    @requires(('write_op', WriteOp), ('oid', str_type), ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)), ('mtime', opt(int)), ('flags', opt(int)))\n    def operate_aio_write_op(self, write_op, oid, oncomplete=None, onsafe=None, mtime=0, flags=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        execute the real write operation asynchronously\n        :para write_op: write operation object\n        :type write_op: WriteOp\n        :para oid: object name\n        :type oid: str\n        :param oncomplete: what to do when the remove is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the remove is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n        :para mtime: the time to set the mtime to, 0 for the current time\n        :type mtime: int\n        :para flags: flags to apply to the entire operation\n        :type flags: int\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        oid = cstr(oid, 'oid')\n        cdef:\n            WriteOp _write_op = write_op\n            char *_oid = oid\n            Completion completion\n            time_t _mtime = mtime\n            int _flags = flags\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n\n        with nogil:\n            ret = rados_aio_write_op_operate(_write_op.write_op, self.io, completion.rados_comp, _oid,\n                                             &_mtime, _flags)\n        if ret != 0:\n            completion._cleanup()\n            raise make_ex(ret, \"Failed to operate aio write op for oid %s\" % oid)\n        return completion\n\n    @requires(('read_op', ReadOp), ('oid', str_type), ('flag', opt(int)))\n    def operate_read_op(self, read_op, oid, flag=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        execute the real read operation\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para oid: object name\n        :type oid: str\n        :para flag: flags to apply to the entire operation\n        :type flag: int\n        \"\"\"\n        oid = cstr(oid, 'oid')\n        cdef:\n            ReadOp _read_op = read_op\n            char *_oid = oid\n            int _flag = flag\n\n        with nogil:\n            ret = rados_read_op_operate(_read_op.read_op, self.io, _oid, _flag)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to operate read op for oid %s\" % oid)\n\n    @requires(('read_op', ReadOp), ('oid', str_type), ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)), ('flag', opt(int)))\n    def operate_aio_read_op(self, read_op, oid, oncomplete=None, onsafe=None, flag=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        execute the real read operation\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para oid: object name\n        :type oid: str\n        :param oncomplete: what to do when the remove is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the remove is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n        :para flag: flags to apply to the entire operation\n        :type flag: int\n        \"\"\"\n        oid = cstr(oid, 'oid')\n        cdef:\n            ReadOp _read_op = read_op\n            char *_oid = oid\n            Completion completion\n            int _flag = flag\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n\n        with nogil:\n            ret = rados_aio_read_op_operate(_read_op.read_op, self.io, completion.rados_comp, _oid, _flag)\n        if ret != 0:\n            completion._cleanup()\n            raise make_ex(ret, \"Failed to operate aio read op for oid %s\" % oid)\n        return completion\n\n    @requires(('read_op', ReadOp), ('start_after', str_type), ('filter_prefix', str_type), ('max_return', int))\n    def get_omap_vals(self, read_op, start_after, filter_prefix, max_return):\n        \"\"\"\n        get the omap values\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para start_after: list keys starting after start_after\n        :type start_after: str\n        :para filter_prefix: list only keys beginning with filter_prefix\n        :type filter_prefix: str\n        :para max_return: list no more than max_return key/value pairs\n        :type max_return: int\n        :returns: an iterator over the requested omap values, return value from this action\n        \"\"\"\n\n        start_after = cstr(start_after, 'start_after') if start_after else None\n        filter_prefix = cstr(filter_prefix, 'filter_prefix') if filter_prefix else None\n        cdef:\n            char *_start_after = opt_str(start_after)\n            char *_filter_prefix = opt_str(filter_prefix)\n            ReadOp _read_op = read_op\n            rados_omap_iter_t iter_addr = NULL\n            int _max_return = max_return\n            int prval = 0\n\n        with nogil:\n            rados_read_op_omap_get_vals2(_read_op.read_op, _start_after, _filter_prefix,\n                                         _max_return, &iter_addr, NULL, &prval)\n        it = OmapIterator(self)\n        it.ctx = iter_addr\n        return it, int(prval)\n\n    @requires(('read_op', ReadOp), ('start_after', str_type), ('max_return', int))\n    def get_omap_keys(self, read_op, start_after, max_return):\n        \"\"\"\n        get the omap keys\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para start_after: list keys starting after start_after\n        :type start_after: str\n        :para max_return: list no more than max_return key/value pairs\n        :type max_return: int\n        :returns: an iterator over the requested omap values, return value from this action\n        \"\"\"\n        start_after = cstr(start_after, 'start_after') if start_after else None\n        cdef:\n            char *_start_after = opt_str(start_after)\n            ReadOp _read_op = read_op\n            rados_omap_iter_t iter_addr = NULL\n            int _max_return = max_return\n            int prval = 0\n\n        with nogil:\n            rados_read_op_omap_get_keys2(_read_op.read_op, _start_after,\n                                         _max_return, &iter_addr, NULL, &prval)\n        it = OmapIterator(self)\n        it.ctx = iter_addr\n        return it, int(prval)\n\n    @requires(('read_op', ReadOp), ('keys', tuple))\n    def get_omap_vals_by_keys(self, read_op, keys):\n        \"\"\"\n        get the omap values by keys\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para keys: input key tuple\n        :type keys: tuple\n        :returns: an iterator over the requested omap values, return value from this action\n        \"\"\"\n        keys = cstr_list(keys, 'keys')\n        cdef:\n            ReadOp _read_op = read_op\n            rados_omap_iter_t iter_addr\n            char **_keys = to_bytes_array(keys)\n            size_t key_num = len(keys)\n            int prval = 0\n\n        try:\n            with nogil:\n                rados_read_op_omap_get_vals_by_keys(_read_op.read_op,\n                                                    <const char**>_keys,\n                                                    key_num, &iter_addr,  &prval)\n            it = OmapIterator(self)\n            it.ctx = iter_addr\n            return it, int(prval)\n        finally:\n            free(_keys)\n\n    @requires(('write_op', WriteOp), ('keys', tuple))\n    def remove_omap_keys(self, write_op, keys):\n        \"\"\"\n        remove omap keys specifiled\n        :para write_op: write operation object\n        :type write_op: WriteOp\n        :para keys: input key tuple\n        :type keys: tuple\n        \"\"\"\n\n        keys = cstr_list(keys, 'keys')\n        cdef:\n            WriteOp _write_op = write_op\n            size_t key_num = len(keys)\n            char **_keys = to_bytes_array(keys)\n\n        try:\n            with nogil:\n                rados_write_op_omap_rm_keys(_write_op.write_op, <const char**>_keys, key_num)\n        finally:\n            free(_keys)\n\n    @requires(('write_op', WriteOp))\n    def clear_omap(self, write_op):\n        \"\"\"\n        Remove all key/value pairs from an object\n        :para write_op: write operation object\n        :type write_op: WriteOp\n        \"\"\"\n\n        cdef:\n            WriteOp _write_op = write_op\n\n        with nogil:\n            rados_write_op_omap_clear(_write_op.write_op)\n\n    @requires(('key', str_type), ('name', str_type), ('cookie', str_type), ('desc', str_type),\n              ('duration', opt(int)), ('flags', int))\n    def lock_exclusive(self, key, name, cookie, desc=\"\", duration=None, flags=0):\n\n        \"\"\"\n        Take an exclusive lock on an object\n\n        :param key: name of the object\n        :type key: str\n        :param name: name of the lock\n        :type name: str\n        :param cookie: cookie of the lock\n        :type cookie: str\n        :param desc: description of the lock\n        :type desc: str\n        :param duration: duration of the lock in seconds\n        :type duration: int\n        :param flags: flags\n        :type flags: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        name = cstr(name, 'name')\n        cookie = cstr(cookie, 'cookie')\n        desc = cstr(desc, 'desc')\n\n        cdef:\n            char* _key = key\n            char* _name = name\n            char* _cookie = cookie\n            char* _desc = desc\n            uint8_t _flags = flags\n            timeval _duration\n\n        if duration is None:\n            with nogil:\n                ret = rados_lock_exclusive(self.io, _key, _name, _cookie, _desc,\n                                           NULL, _flags)\n        else:\n            _duration.tv_sec = duration\n            with nogil:\n                ret = rados_lock_exclusive(self.io, _key, _name, _cookie, _desc,\n                                           &_duration, _flags)\n\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.rados_lock_exclusive(%s): failed to set lock %s on %s\" % (self.name, name, key))\n\n    @requires(('key', str_type), ('name', str_type), ('cookie', str_type), ('tag', str_type),\n              ('desc', str_type), ('duration', opt(int)), ('flags', int))\n    def lock_shared(self, key, name, cookie, tag, desc=\"\", duration=None, flags=0):\n\n        \"\"\"\n        Take a shared lock on an object\n\n        :param key: name of the object\n        :type key: str\n        :param name: name of the lock\n        :type name: str\n        :param cookie: cookie of the lock\n        :type cookie: str\n        :param tag: tag of the lock\n        :type tag: str\n        :param desc: description of the lock\n        :type desc: str\n        :param duration: duration of the lock in seconds\n        :type duration: int\n        :param flags: flags\n        :type flags: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        tag = cstr(tag, 'tag')\n        name = cstr(name, 'name')\n        cookie = cstr(cookie, 'cookie')\n        desc = cstr(desc, 'desc')\n\n        cdef:\n            char* _key = key\n            char* _tag = tag\n            char* _name = name\n            char* _cookie = cookie\n            char* _desc = desc\n            uint8_t _flags = flags\n            timeval _duration\n\n        if duration is None:\n            with nogil:\n                ret = rados_lock_shared(self.io, _key, _name, _cookie, _tag, _desc,\n                                        NULL, _flags)\n        else:\n            _duration.tv_sec = duration\n            with nogil:\n                ret = rados_lock_shared(self.io, _key, _name, _cookie, _tag, _desc,\n                                        &_duration, _flags)\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.rados_lock_exclusive(%s): failed to set lock %s on %s\" % (self.name, name, key))\n\n    @requires(('key', str_type), ('name', str_type), ('cookie', str_type))\n    def unlock(self, key, name, cookie):\n\n        \"\"\"\n        Release a shared or exclusive lock on an object\n\n        :param key: name of the object\n        :type key: str\n        :param name: name of the lock\n        :type name: str\n        :param cookie: cookie of the lock\n        :type cookie: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        name = cstr(name, 'name')\n        cookie = cstr(cookie, 'cookie')\n\n        cdef:\n            char* _key = key\n            char* _name = name\n            char* _cookie = cookie\n\n        with nogil:\n            ret = rados_unlock(self.io, _key, _name, _cookie)\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.rados_lock_exclusive(%s): failed to set lock %s on %s\" % (self.name, name, key))\n\n    def set_osdmap_full_try(self):\n        \"\"\"\n        Set global osdmap_full_try label to true\n        \"\"\"\n        with nogil:\n            rados_set_osdmap_full_try(self.io)\n\n    def unset_osdmap_full_try(self):\n        \"\"\"\n        Unset\n        \"\"\"\n        with nogil:\n            rados_unset_osdmap_full_try(self.io)\n\n    def application_enable(self, app_name, force=False):\n        \"\"\"\n        Enable an application on an OSD pool\n\n        :param app_name: application name\n        :type app_name: str\n        :param force: False if only a single app should exist per pool\n        :type expire_seconds: boool\n\n        :raises: :class:`Error`\n        \"\"\"\n        app_name =  cstr(app_name, 'app_name')\n        cdef:\n            char *_app_name = app_name\n            int _force = (1 if force else 0)\n\n        with nogil:\n            ret = rados_application_enable(self.io, _app_name, _force)\n        if ret < 0:\n            raise make_ex(ret, \"error enabling application\")\n\n    def application_list(self):\n        \"\"\"\n        Returns a list of enabled applications\n\n        :returns: list of app name string\n        \"\"\"\n        cdef:\n            size_t length = 128\n            char *apps = NULL\n\n        try:\n            while True:\n                apps = <char *>realloc_chk(apps, length)\n                with nogil:\n                    ret = rados_application_list(self.io, apps, &length)\n                if ret == 0:\n                    return [decode_cstr(app) for app in\n                                apps[:length].split(b'\\0') if app]\n                elif ret == -errno.ENOENT:\n                    return None\n                elif ret == -errno.ERANGE:\n                    pass\n                else:\n                    raise make_ex(ret, \"error listing applications\")\n        finally:\n            free(apps)\n\n    def application_metadata_set(self, app_name, key, value):\n        \"\"\"\n        Sets application metadata on an OSD pool\n\n        :param app_name: application name\n        :type app_name: str\n        :param key: metadata key\n        :type key: str\n        :param value: metadata value\n        :type value: str\n\n        :raises: :class:`Error`\n        \"\"\"\n        app_name =  cstr(app_name, 'app_name')\n        key =  cstr(key, 'key')\n        value =  cstr(value, 'value')\n        cdef:\n            char *_app_name = app_name\n            char *_key = key\n            char *_value = value\n\n        with nogil:\n            ret = rados_application_metadata_set(self.io, _app_name, _key,\n                                                 _value)\n        if ret < 0:\n            raise make_ex(ret, \"error setting application metadata\")\n\n    def application_metadata_remove(self, app_name, key):\n        \"\"\"\n        Remove application metadata from an OSD pool\n\n        :param app_name: application name\n        :type app_name: str\n        :param key: metadata key\n        :type key: str\n\n        :raises: :class:`Error`\n        \"\"\"\n        app_name =  cstr(app_name, 'app_name')\n        key =  cstr(key, 'key')\n        cdef:\n            char *_app_name = app_name\n            char *_key = key\n\n        with nogil:\n            ret = rados_application_metadata_remove(self.io, _app_name, _key)\n        if ret < 0:\n            raise make_ex(ret, \"error removing application metadata\")\n\n    def application_metadata_list(self, app_name):\n        \"\"\"\n        Returns a list of enabled applications\n\n        :param app_name: application name\n        :type app_name: str\n        :returns: list of key/value tuples\n        \"\"\"\n        app_name =  cstr(app_name, 'app_name')\n        cdef:\n            char *_app_name = app_name\n            size_t key_length = 128\n            size_t val_length = 128\n            char *c_keys = NULL\n            char *c_vals = NULL\n\n        try:\n            while True:\n                c_keys = <char *>realloc_chk(c_keys, key_length)\n                c_vals = <char *>realloc_chk(c_vals, val_length)\n                with nogil:\n                    ret = rados_application_metadata_list(self.io, _app_name,\n                                                          c_keys, &key_length,\n                                                          c_vals, &val_length)\n                if ret == 0:\n                    keys = [decode_cstr(key) for key in\n                                c_keys[:key_length].split(b'\\0') if key]\n                    vals = [decode_cstr(val) for val in\n                                c_vals[:val_length].split(b'\\0') if val]\n                    return zip(keys, vals)\n                elif ret == -errno.ERANGE:\n                    pass\n                else:\n                    raise make_ex(ret, \"error listing application metadata\")\n        finally:\n            free(c_keys)\n            free(c_vals)\n\n    def alignment(self):\n        \"\"\"\n        Returns pool alignment\n\n        :returns:\n            Number of alignment bytes required by the current pool, or None if\n            alignment is not required.\n        \"\"\"\n        cdef:\n            int requires = 0\n            uint64_t _alignment\n\n        with nogil:\n            ret = rados_ioctx_pool_requires_alignment2(self.io, &requires)\n        if ret != 0:\n            raise make_ex(ret, \"error checking alignment\")\n\n        alignment = None\n        if requires:\n            with nogil:\n                ret = rados_ioctx_pool_required_alignment2(self.io, &_alignment)\n            if ret != 0:\n                raise make_ex(ret, \"error querying alignment\")\n            alignment = _alignment\n        return alignment\n\n\ndef set_object_locator(func):\n    def retfunc(self, *args, **kwargs):\n        if self.locator_key is not None:\n            old_locator = self.ioctx.get_locator_key()\n            self.ioctx.set_locator_key(self.locator_key)\n            retval = func(self, *args, **kwargs)\n            self.ioctx.set_locator_key(old_locator)\n            return retval\n        else:\n            return func(self, *args, **kwargs)\n    return retfunc\n\n\ndef set_object_namespace(func):\n    def retfunc(self, *args, **kwargs):\n        if self.nspace is None:\n            raise LogicError(\"Namespace not set properly in context\")\n        old_nspace = self.ioctx.get_namespace()\n        self.ioctx.set_namespace(self.nspace)\n        retval = func(self, *args, **kwargs)\n        self.ioctx.set_namespace(old_nspace)\n        return retval\n    return retfunc\n\n\nclass Object(object):\n    \"\"\"Rados object wrapper, makes the object look like a file\"\"\"\n    def __init__(self, ioctx, key, locator_key=None, nspace=None):\n        self.key = key\n        self.ioctx = ioctx\n        self.offset = 0\n        self.state = \"exists\"\n        self.locator_key = locator_key\n        self.nspace = \"\" if nspace is None else nspace\n\n    def __str__(self):\n        return \"rados.Object(ioctx=%s,key=%s,nspace=%s,locator=%s)\" % \\\n            (str(self.ioctx), self.key, \"--default--\"\n             if self.nspace is \"\" else self.nspace, self.locator_key)\n\n    def require_object_exists(self):\n        if self.state != \"exists\":\n            raise ObjectStateError(\"The object is %s\" % self.state)\n\n    @set_object_locator\n    @set_object_namespace\n    def read(self, length=1024 * 1024):\n        self.require_object_exists()\n        ret = self.ioctx.read(self.key, length, self.offset)\n        self.offset += len(ret)\n        return ret\n\n    @set_object_locator\n    @set_object_namespace\n    def write(self, string_to_write):\n        self.require_object_exists()\n        ret = self.ioctx.write(self.key, string_to_write, self.offset)\n        if ret == 0:\n            self.offset += len(string_to_write)\n        return ret\n\n    @set_object_locator\n    @set_object_namespace\n    def remove(self):\n        self.require_object_exists()\n        self.ioctx.remove_object(self.key)\n        self.state = \"removed\"\n\n    @set_object_locator\n    @set_object_namespace\n    def stat(self):\n        self.require_object_exists()\n        return self.ioctx.stat(self.key)\n\n    def seek(self, position):\n        self.require_object_exists()\n        self.offset = position\n\n    @set_object_locator\n    @set_object_namespace\n    def get_xattr(self, xattr_name):\n        self.require_object_exists()\n        return self.ioctx.get_xattr(self.key, xattr_name)\n\n    @set_object_locator\n    @set_object_namespace\n    def get_xattrs(self):\n        self.require_object_exists()\n        return self.ioctx.get_xattrs(self.key)\n\n    @set_object_locator\n    @set_object_namespace\n    def set_xattr(self, xattr_name, xattr_value):\n        self.require_object_exists()\n        return self.ioctx.set_xattr(self.key, xattr_name, xattr_value)\n\n    @set_object_locator\n    @set_object_namespace\n    def rm_xattr(self, xattr_name):\n        self.require_object_exists()\n        return self.ioctx.rm_xattr(self.key, xattr_name)\n\nMONITOR_LEVELS = [\n    \"debug\",\n    \"info\",\n    \"warn\", \"warning\",\n    \"err\", \"error\",\n    \"sec\",\n    ]\n\n\nclass MonitorLog(object):\n    # NOTE(sileht): Keep this class for backward compat\n    # method moved to Rados.monitor_log()\n    \"\"\"\n    For watching cluster log messages.  Instantiate an object and keep\n    it around while callback is periodically called.  Construct with\n    'level' to monitor 'level' messages (one of MONITOR_LEVELS).\n    arg will be passed to the callback.\n\n    callback will be called with:\n        arg (given to __init__)\n        line (the full line, including timestamp, who, level, msg)\n        who (which entity issued the log message)\n        timestamp_sec (sec of a struct timespec)\n        timestamp_nsec (sec of a struct timespec)\n        seq (sequence number)\n        level (string representing the level of the log message)\n        msg (the message itself)\n    callback's return value is ignored\n    \"\"\"\n    def __init__(self, cluster, level, callback, arg):\n        self.level = level\n        self.callback = callback\n        self.arg = arg\n        self.cluster = cluster\n        self.cluster.monitor_log(level, callback, arg)\n\n", "from __future__ import print_function\nfrom nose import SkipTest\nfrom nose.tools import eq_ as eq, ok_ as ok, assert_raises\nfrom rados import (Rados, Error, RadosStateError, Object, ObjectExists,\n                   ObjectNotFound, ObjectBusy, requires, opt,\n                   ANONYMOUS_AUID, ADMIN_AUID, LIBRADOS_ALL_NSPACES, WriteOpCtx, ReadOpCtx,\n                   LIBRADOS_SNAP_HEAD, LIBRADOS_OPERATION_BALANCE_READS, LIBRADOS_OPERATION_SKIPRWLOCKS, MonitorLog)\nimport time\nimport threading\nimport json\nimport errno\nimport os\nimport sys\n\n# Are we running Python 2.x\n_python2 = sys.version_info[0] < 3\n\ndef test_rados_init_error():\n    assert_raises(Error, Rados, conffile='', rados_id='admin',\n                  name='client.admin')\n    assert_raises(Error, Rados, conffile='', name='invalid')\n    assert_raises(Error, Rados, conffile='', name='bad.invalid')\n\ndef test_rados_init():\n    with Rados(conffile='', rados_id='admin'):\n        pass\n    with Rados(conffile='', name='client.admin'):\n        pass\n    with Rados(conffile='', name='client.admin'):\n        pass\n    with Rados(conffile='', name='client.admin'):\n        pass\n\ndef test_ioctx_context_manager():\n    with Rados(conffile='', rados_id='admin') as conn:\n        with conn.open_ioctx('rbd') as ioctx:\n            pass\n\ndef test_parse_argv():\n    args = ['osd', 'pool', 'delete', 'foobar', 'foobar', '--yes-i-really-really-mean-it']\n    r = Rados()\n    eq(args, r.conf_parse_argv(args))\n\ndef test_parse_argv_empty_str():\n    args = ['']\n    r = Rados()\n    eq(args, r.conf_parse_argv(args))\n\nclass TestRequires(object):\n    @requires(('foo', str), ('bar', int), ('baz', int))\n    def _method_plain(self, foo, bar, baz):\n        ok(isinstance(foo, str))\n        ok(isinstance(bar, int))\n        ok(isinstance(baz, int))\n        return (foo, bar, baz)\n\n    def test_method_plain(self):\n        assert_raises(TypeError, self._method_plain, 42, 42, 42)\n        assert_raises(TypeError, self._method_plain, '42', '42', '42')\n        assert_raises(TypeError, self._method_plain, foo='42', bar='42', baz='42')\n        eq(self._method_plain('42', 42, 42), ('42', 42, 42))\n        eq(self._method_plain(foo='42', bar=42, baz=42), ('42', 42, 42))\n\n    @requires(('opt_foo', opt(str)), ('opt_bar', opt(int)), ('baz', int))\n    def _method_with_opt_arg(self, foo, bar, baz):\n        ok(isinstance(foo, str) or foo is None)\n        ok(isinstance(bar, int) or bar is None)\n        ok(isinstance(baz, int))\n        return (foo, bar, baz)\n\n    def test_method_with_opt_args(self):\n        assert_raises(TypeError, self._method_with_opt_arg, 42, 42, 42)\n        assert_raises(TypeError, self._method_with_opt_arg, '42', '42', 42)\n        assert_raises(TypeError, self._method_with_opt_arg, None, None, None)\n        eq(self._method_with_opt_arg(None, 42, 42), (None, 42, 42))\n        eq(self._method_with_opt_arg('42', None, 42), ('42', None, 42))\n        eq(self._method_with_opt_arg(None, None, 42), (None, None, 42))\n\n\nclass TestRadosStateError(object):\n    def _requires_configuring(self, rados):\n        assert_raises(RadosStateError, rados.connect)\n\n    def _requires_configuring_or_connected(self, rados):\n        assert_raises(RadosStateError, rados.conf_read_file)\n        assert_raises(RadosStateError, rados.conf_parse_argv, None)\n        assert_raises(RadosStateError, rados.conf_parse_env)\n        assert_raises(RadosStateError, rados.conf_get, 'opt')\n        assert_raises(RadosStateError, rados.conf_set, 'opt', 'val')\n        assert_raises(RadosStateError, rados.ping_monitor, 0)\n\n    def _requires_connected(self, rados):\n        assert_raises(RadosStateError, rados.pool_exists, 'foo')\n        assert_raises(RadosStateError, rados.pool_lookup, 'foo')\n        assert_raises(RadosStateError, rados.pool_reverse_lookup, 0)\n        assert_raises(RadosStateError, rados.create_pool, 'foo')\n        assert_raises(RadosStateError, rados.get_pool_base_tier, 0)\n        assert_raises(RadosStateError, rados.delete_pool, 'foo')\n        assert_raises(RadosStateError, rados.list_pools)\n        assert_raises(RadosStateError, rados.get_fsid)\n        assert_raises(RadosStateError, rados.open_ioctx, 'foo')\n        assert_raises(RadosStateError, rados.mon_command, '', b'')\n        assert_raises(RadosStateError, rados.osd_command, 0, '', b'')\n        assert_raises(RadosStateError, rados.pg_command, '', '', b'')\n        assert_raises(RadosStateError, rados.wait_for_latest_osdmap)\n        assert_raises(RadosStateError, rados.blacklist_add, '127.0.0.1/123', 0)\n\n    def test_configuring(self):\n        rados = Rados(conffile='')\n        eq('configuring', rados.state)\n        self._requires_connected(rados)\n\n    def test_connected(self):\n        rados = Rados(conffile='')\n        with rados:\n            eq('connected', rados.state)\n            self._requires_configuring(rados)\n\n    def test_shutdown(self):\n        rados = Rados(conffile='')\n        with rados:\n            pass\n        eq('shutdown', rados.state)\n        self._requires_configuring(rados)\n        self._requires_configuring_or_connected(rados)\n        self._requires_connected(rados)\n\n\nclass TestRados(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.conf_parse_env('FOO_DOES_NOT_EXIST_BLAHBLAH')\n        self.rados.conf_parse_env()\n        self.rados.connect()\n\n        # Assume any pre-existing pools are the cluster's defaults\n        self.default_pools = self.rados.list_pools()\n\n    def tearDown(self):\n        self.rados.shutdown()\n\n    def test_ping_monitor(self):\n        assert_raises(ObjectNotFound, self.rados.ping_monitor, 'not_exists_monitor')\n        cmd = {'prefix': 'mon dump', 'format':'json'}\n        ret, buf, out = self.rados.mon_command(json.dumps(cmd), b'')\n        for mon in json.loads(buf.decode('utf8'))['mons']:\n            while True:\n                output = self.rados.ping_monitor(mon['name'])\n                if output is None:\n                    continue\n                buf = json.loads(output)\n                if buf.get('health'):\n                    break\n\n    def test_create(self):\n        self.rados.create_pool('foo')\n        self.rados.delete_pool('foo')\n\n    def test_create_utf8(self):\n        if _python2:\n            # Use encoded bytestring\n            poolname = b\"\\351\\273\\204\"\n        else:\n            poolname = \"\\u9ec4\"\n        self.rados.create_pool(poolname)\n        assert self.rados.pool_exists(u\"\\u9ec4\")\n        self.rados.delete_pool(poolname)\n\n    def test_pool_lookup_utf8(self):\n        if _python2:\n            poolname = u'\\u9ec4'\n        else:\n            poolname = '\\u9ec4'\n        self.rados.create_pool(poolname)\n        try:\n            poolid = self.rados.pool_lookup(poolname)\n            eq(poolname, self.rados.pool_reverse_lookup(poolid))\n        finally:\n            self.rados.delete_pool(poolname)\n\n    def test_create_auid(self):\n        self.rados.create_pool('foo', 100)\n        assert self.rados.pool_exists('foo')\n        self.rados.delete_pool('foo')\n\n    def test_eexist(self):\n        self.rados.create_pool('foo')\n        assert_raises(ObjectExists, self.rados.create_pool, 'foo')\n        self.rados.delete_pool('foo')\n\n    def list_non_default_pools(self):\n        pools = self.rados.list_pools()\n        for p in self.default_pools:\n            pools.remove(p)\n        return set(pools)\n\n    def test_list_pools(self):\n        eq(set(), self.list_non_default_pools())\n        self.rados.create_pool('foo')\n        eq(set(['foo']), self.list_non_default_pools())\n        self.rados.create_pool('bar')\n        eq(set(['foo', 'bar']), self.list_non_default_pools())\n        self.rados.create_pool('baz')\n        eq(set(['foo', 'bar', 'baz']), self.list_non_default_pools())\n        self.rados.delete_pool('foo')\n        eq(set(['bar', 'baz']), self.list_non_default_pools())\n        self.rados.delete_pool('baz')\n        eq(set(['bar']), self.list_non_default_pools())\n        self.rados.delete_pool('bar')\n        eq(set(), self.list_non_default_pools())\n        self.rados.create_pool('a' * 500)\n        eq(set(['a' * 500]), self.list_non_default_pools())\n        self.rados.delete_pool('a' * 500)\n\n    def test_get_pool_base_tier(self):\n        self.rados.create_pool('foo')\n        try:\n            self.rados.create_pool('foo-cache')\n            try:\n                pool_id = self.rados.pool_lookup('foo')\n                tier_pool_id = self.rados.pool_lookup('foo-cache')\n\n                cmd = {\"prefix\":\"osd tier add\", \"pool\":\"foo\", \"tierpool\":\"foo-cache\", \"force_nonempty\":\"\"}\n                ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n                eq(ret, 0)\n\n                try:\n                    cmd = {\"prefix\":\"osd tier cache-mode\", \"pool\":\"foo-cache\", \"tierpool\":\"foo-cache\", \"mode\":\"readonly\", \"sure\":\"--yes-i-really-mean-it\"}\n                    ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n                    eq(ret, 0)\n\n                    eq(self.rados.wait_for_latest_osdmap(), 0)\n\n                    eq(pool_id, self.rados.get_pool_base_tier(pool_id))\n                    eq(pool_id, self.rados.get_pool_base_tier(tier_pool_id))\n                finally:\n                    cmd = {\"prefix\":\"osd tier remove\", \"pool\":\"foo\", \"tierpool\":\"foo-cache\"}\n                    ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n                    eq(ret, 0)\n            finally:\n                self.rados.delete_pool('foo-cache')\n        finally:\n            self.rados.delete_pool('foo')\n\n    def test_get_fsid(self):\n        fsid = self.rados.get_fsid()\n        eq(len(fsid), 36)\n\n    def test_blacklist_add(self):\n        self.rados.blacklist_add(\"1.2.3.4/123\", 1)\n\n    def test_get_cluster_stats(self):\n        stats = self.rados.get_cluster_stats()\n        assert stats['kb'] > 0\n        assert stats['kb_avail'] > 0\n        assert stats['kb_used'] > 0\n        assert stats['num_objects'] >= 0\n\n    def test_monitor_log(self):\n        lock = threading.Condition()\n        def cb(arg, line, who, sec, nsec, seq, level, msg):\n            # NOTE(sileht): the old pyrados API was received the pointer as int\n            # instead of the value of arg\n            eq(arg, \"arg\")\n            with lock:\n                lock.notify()\n            return 0\n\n        # NOTE(sileht): force don't save the monitor into local var\n        # to ensure all references are correctly tracked into the lib\n        MonitorLog(self.rados, \"debug\", cb, \"arg\")\n        with lock:\n            lock.wait()\n        MonitorLog(self.rados, \"debug\", None, None)\n        eq(None, self.rados.monitor_callback)\n\nclass TestIoctx(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.rados.create_pool('test_pool')\n        assert self.rados.pool_exists('test_pool')\n        self.ioctx = self.rados.open_ioctx('test_pool')\n\n    def tearDown(self):\n        cmd = {\"prefix\":\"osd unset\", \"key\":\"noup\"}\n        self.rados.mon_command(json.dumps(cmd), b'')\n        self.ioctx.close()\n        self.rados.delete_pool('test_pool')\n        self.rados.shutdown()\n\n    def test_get_last_version(self):\n        version = self.ioctx.get_last_version()\n        assert version >= 0\n\n    def test_get_stats(self):\n        stats = self.ioctx.get_stats()\n        eq(stats, {'num_objects_unfound': 0,\n                   'num_objects_missing_on_primary': 0,\n                   'num_object_clones': 0,\n                   'num_objects': 0,\n                   'num_object_copies': 0,\n                   'num_bytes': 0,\n                   'num_rd_kb': 0,\n                   'num_wr_kb': 0,\n                   'num_kb': 0,\n                   'num_wr': 0,\n                   'num_objects_degraded': 0,\n                   'num_rd': 0})\n\n    def test_change_auid(self):\n        self.ioctx.change_auid(ANONYMOUS_AUID)\n        self.ioctx.change_auid(ADMIN_AUID)\n\n    def test_write(self):\n        self.ioctx.write('abc', b'abc')\n        eq(self.ioctx.read('abc'), b'abc')\n\n    def test_write_full(self):\n        self.ioctx.write('abc', b'abc')\n        eq(self.ioctx.read('abc'), b'abc')\n        self.ioctx.write_full('abc', b'd')\n        eq(self.ioctx.read('abc'), b'd')\n\n    def test_append(self):\n        self.ioctx.write('abc', b'a')\n        self.ioctx.append('abc', b'b')\n        self.ioctx.append('abc', b'c')\n        eq(self.ioctx.read('abc'), b'abc')\n\n    def test_write_zeros(self):\n        self.ioctx.write('abc', b'a\\0b\\0c')\n        eq(self.ioctx.read('abc'), b'a\\0b\\0c')\n\n    def test_trunc(self):\n        self.ioctx.write('abc', b'abc')\n        self.ioctx.trunc('abc', 2)\n        eq(self.ioctx.read('abc'), b'ab')\n        size = self.ioctx.stat('abc')[0]\n        eq(size, 2)\n\n    def test_list_objects_empty(self):\n        eq(list(self.ioctx.list_objects()), [])\n\n    def test_list_objects(self):\n        self.ioctx.write('a', b'')\n        self.ioctx.write('b', b'foo')\n        self.ioctx.write_full('c', b'bar')\n        self.ioctx.append('d', b'jazz')\n        object_names = [obj.key for obj in self.ioctx.list_objects()]\n        eq(sorted(object_names), ['a', 'b', 'c', 'd'])\n\n    def test_list_ns_objects(self):\n        self.ioctx.write('a', b'')\n        self.ioctx.write('b', b'foo')\n        self.ioctx.write_full('c', b'bar')\n        self.ioctx.append('d', b'jazz')\n        self.ioctx.set_namespace(\"ns1\")\n        self.ioctx.write('ns1-a', b'')\n        self.ioctx.write('ns1-b', b'foo')\n        self.ioctx.write_full('ns1-c', b'bar')\n        self.ioctx.append('ns1-d', b'jazz')\n        self.ioctx.append('d', b'jazz')\n        self.ioctx.set_namespace(LIBRADOS_ALL_NSPACES)\n        object_names = [(obj.nspace, obj.key) for obj in self.ioctx.list_objects()]\n        eq(sorted(object_names), [('', 'a'), ('','b'), ('','c'), ('','d'),\\\n                ('ns1', 'd'), ('ns1', 'ns1-a'), ('ns1', 'ns1-b'),\\\n                ('ns1', 'ns1-c'), ('ns1', 'ns1-d')])\n\n    def test_xattrs(self):\n        xattrs = dict(a=b'1', b=b'2', c=b'3', d=b'a\\0b', e=b'\\0', f='')\n        self.ioctx.write('abc', b'')\n        for key, value in xattrs.items():\n            self.ioctx.set_xattr('abc', key, value)\n            eq(self.ioctx.get_xattr('abc', key), value)\n        stored_xattrs = {}\n        for key, value in self.ioctx.get_xattrs('abc'):\n            stored_xattrs[key] = value\n        eq(stored_xattrs, xattrs)\n\n    def test_obj_xattrs(self):\n        xattrs = dict(a=b'1', b=b'2', c=b'3', d=b'a\\0b', e=b'\\0', f='')\n        self.ioctx.write('abc', b'')\n        obj = list(self.ioctx.list_objects())[0]\n        for key, value in xattrs.items():\n            obj.set_xattr(key, value)\n            eq(obj.get_xattr(key), value)\n        stored_xattrs = {}\n        for key, value in obj.get_xattrs():\n            stored_xattrs[key] = value\n        eq(stored_xattrs, xattrs)\n\n    def test_create_snap(self):\n        assert_raises(ObjectNotFound, self.ioctx.remove_snap, 'foo')\n        self.ioctx.create_snap('foo')\n        self.ioctx.remove_snap('foo')\n\n    def test_list_snaps_empty(self):\n        eq(list(self.ioctx.list_snaps()), [])\n\n    def test_list_snaps(self):\n        snaps = ['snap1', 'snap2', 'snap3']\n        for snap in snaps:\n            self.ioctx.create_snap(snap)\n        listed_snaps = [snap.name for snap in self.ioctx.list_snaps()]\n        eq(snaps, listed_snaps)\n\n    def test_lookup_snap(self):\n        self.ioctx.create_snap('foo')\n        snap = self.ioctx.lookup_snap('foo')\n        eq(snap.name, 'foo')\n\n    def test_snap_timestamp(self):\n        self.ioctx.create_snap('foo')\n        snap = self.ioctx.lookup_snap('foo')\n        snap.get_timestamp()\n\n    def test_remove_snap(self):\n        self.ioctx.create_snap('foo')\n        (snap,) = self.ioctx.list_snaps()\n        eq(snap.name, 'foo')\n        self.ioctx.remove_snap('foo')\n        eq(list(self.ioctx.list_snaps()), [])\n\n    def test_snap_rollback(self):\n        self.ioctx.write(\"insnap\", b\"contents1\")\n        self.ioctx.create_snap(\"snap1\")\n        self.ioctx.remove_object(\"insnap\")\n        self.ioctx.snap_rollback(\"insnap\", \"snap1\")\n        eq(self.ioctx.read(\"insnap\"), b\"contents1\")\n        self.ioctx.remove_snap(\"snap1\")\n        self.ioctx.remove_object(\"insnap\")\n\n    def test_snap_read(self):\n        self.ioctx.write(\"insnap\", b\"contents1\")\n        self.ioctx.create_snap(\"snap1\")\n        self.ioctx.remove_object(\"insnap\")\n        snap = self.ioctx.lookup_snap(\"snap1\")\n        self.ioctx.set_read(snap.snap_id)\n        eq(self.ioctx.read(\"insnap\"), b\"contents1\")\n        self.ioctx.set_read(LIBRADOS_SNAP_HEAD)\n        self.ioctx.write(\"inhead\", b\"contents2\")\n        eq(self.ioctx.read(\"inhead\"), b\"contents2\")\n        self.ioctx.remove_snap(\"snap1\")\n        self.ioctx.remove_object(\"inhead\")\n\n    def test_set_omap(self):\n        keys = (\"1\", \"2\", \"3\", \"4\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\", b\"\\x04\\x04\\x04\\x04\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            write_op.set_flags(LIBRADOS_OPERATION_SKIPRWLOCKS)\n            self.ioctx.operate_write_op(write_op, \"hw\")\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals(read_op, \"\", \"\", 4)\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            next(iter)\n            eq(list(iter), [(\"2\", b\"bbb\"), (\"3\", b\"ccc\"), (\"4\", b\"\\x04\\x04\\x04\\x04\")])\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals(read_op, \"2\", \"\", 4)\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq((\"3\", b\"ccc\"), next(iter))\n            eq(list(iter), [(\"4\", b\"\\x04\\x04\\x04\\x04\")])\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals(read_op, \"\", \"2\", 4)\n            eq(ret, 0)\n            read_op.set_flags(LIBRADOS_OPERATION_BALANCE_READS)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq(list(iter), [(\"2\", b\"bbb\")])\n\n    def test_set_omap_aio(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n\n        keys = (\"1\", \"2\", \"3\", \"4\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\", b\"\\x04\\x04\\x04\\x04\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            comp = self.ioctx.operate_aio_write_op(write_op, \"hw\", cb, cb)\n            comp.wait_for_complete()\n            comp.wait_for_safe()\n            with lock:\n                while count[0] < 2:\n                    lock.wait()\n            eq(comp.get_return_value(), 0)\n\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals(read_op, \"\", \"\", 4)\n            eq(ret, 0)\n            comp = self.ioctx.operate_aio_read_op(read_op, \"hw\", cb, cb)\n            comp.wait_for_complete()\n            comp.wait_for_safe()\n            with lock:\n                while count[0] < 4:\n                    lock.wait()\n            eq(comp.get_return_value(), 0)\n            next(iter)\n            eq(list(iter), [(\"2\", b\"bbb\"), (\"3\", b\"ccc\"), (\"4\", b\"\\x04\\x04\\x04\\x04\")])\n\n    def test_write_ops(self):\n        with WriteOpCtx(self.ioctx) as write_op:\n            write_op.new(0)\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'')\n            write_op.write_full(b'1')\n            write_op.append(b'2')\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'12')\n            write_op.write_full(b'12345')\n            write_op.write(b'x', 2)\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'12x45')\n            write_op.write_full(b'12345')\n            write_op.zero(2, 2)\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'12\\x00\\x005')\n            write_op.write_full(b'12345')\n            write_op.truncate(2)\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'12')\n            write_op.remove()\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            with assert_raises(ObjectNotFound):\n                self.ioctx.read('write_ops')\n\n    def test_get_omap_vals_by_keys(self):\n        keys = (\"1\", \"2\", \"3\", \"4\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\", b\"\\x04\\x04\\x04\\x04\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            self.ioctx.operate_write_op(write_op, \"hw\")\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals_by_keys(read_op,(\"3\",\"4\",))\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq(list(iter), [(\"3\", b\"ccc\"), (\"4\", b\"\\x04\\x04\\x04\\x04\")])\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals_by_keys(read_op,(\"3\",\"4\",))\n            eq(ret, 0)\n            with assert_raises(ObjectNotFound):\n                self.ioctx.operate_read_op(read_op, \"no_such\")\n\n    def test_get_omap_keys(self):\n        keys = (\"1\", \"2\", \"3\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            self.ioctx.operate_write_op(write_op, \"hw\")\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_keys(read_op,\"\",2)\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq(list(iter), [(\"1\", None), (\"2\", None)])\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_keys(read_op,\"\",2)\n            eq(ret, 0)\n            with assert_raises(ObjectNotFound):\n                self.ioctx.operate_read_op(read_op, \"no_such\")\n\n    def test_clear_omap(self):\n        keys = (\"1\", \"2\", \"3\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            self.ioctx.operate_write_op(write_op, \"hw\")\n        with WriteOpCtx(self.ioctx) as write_op_1:\n            self.ioctx.clear_omap(write_op_1)\n            self.ioctx.operate_write_op(write_op_1, \"hw\")\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals_by_keys(read_op,(\"1\",))\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq(list(iter), [])\n\n    def test_locator(self):\n        self.ioctx.set_locator_key(\"bar\")\n        self.ioctx.write('foo', b'contents1')\n        objects = [i for i in self.ioctx.list_objects()]\n        eq(len(objects), 1)\n        eq(self.ioctx.get_locator_key(), \"bar\")\n        self.ioctx.set_locator_key(\"\")\n        objects[0].seek(0)\n        objects[0].write(b\"contents2\")\n        eq(self.ioctx.get_locator_key(), \"\")\n        self.ioctx.set_locator_key(\"bar\")\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"contents2\")\n        eq(self.ioctx.get_locator_key(), \"bar\")\n        objects[0].remove()\n        objects = [i for i in self.ioctx.list_objects()]\n        eq(objects, [])\n        self.ioctx.set_locator_key(\"\")\n\n    def test_aio_write(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n        comp = self.ioctx.aio_write(\"foo\", b\"bar\", 0, cb, cb)\n        comp.wait_for_complete()\n        comp.wait_for_safe()\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        eq(comp.get_return_value(), 0)\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"bar\")\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_aio_write_no_comp_ref(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n        # NOTE(sileht): force don't save the comp into local var\n        # to ensure all references are correctly tracked into the lib\n        self.ioctx.aio_write(\"foo\", b\"bar\", 0, cb, cb)\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"bar\")\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_aio_append(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n        comp = self.ioctx.aio_write(\"foo\", b\"bar\", 0, cb, cb)\n        comp2 = self.ioctx.aio_append(\"foo\", b\"baz\", cb, cb)\n        comp.wait_for_complete()\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"barbaz\")\n        with lock:\n            while count[0] < 4:\n                lock.wait()\n        eq(comp.get_return_value(), 0)\n        eq(comp2.get_return_value(), 0)\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_aio_write_full(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n        self.ioctx.aio_write(\"foo\", b\"barbaz\", 0, cb, cb)\n        comp = self.ioctx.aio_write_full(\"foo\", b\"bar\", cb, cb)\n        comp.wait_for_complete()\n        comp.wait_for_safe()\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        eq(comp.get_return_value(), 0)\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"bar\")\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_aio_stat(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(_, size, mtime):\n            with lock:\n                count[0] += 1\n                lock.notify()\n\n        comp = self.ioctx.aio_stat(\"foo\", cb)\n        comp.wait_for_complete()\n        with lock:\n            while count[0] < 1:\n                lock.wait()\n        eq(comp.get_return_value(), -2)\n\n        self.ioctx.write(\"foo\", b\"bar\")\n\n        comp = self.ioctx.aio_stat(\"foo\", cb)\n        comp.wait_for_complete()\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        eq(comp.get_return_value(), 0)\n\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def _take_down_acting_set(self, pool, objectname):\n        # find acting_set for pool:objectname and take it down; used to\n        # verify that async reads don't complete while acting set is missing\n        cmd = {\n            \"prefix\":\"osd map\",\n            \"pool\":pool,\n            \"object\":objectname,\n            \"format\":\"json\",\n        }\n        r, jsonout, _ = self.rados.mon_command(json.dumps(cmd), b'')\n        objmap = json.loads(jsonout.decode(\"utf-8\"))\n        acting_set = objmap['acting']\n        cmd = {\"prefix\":\"osd set\", \"key\":\"noup\"}\n        r, _, _ = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(r, 0)\n        cmd = {\"prefix\":\"osd down\", \"ids\":[str(i) for i in acting_set]}\n        r, _, _ = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(r, 0)\n\n        # wait for OSDs to acknowledge the down\n        eq(self.rados.wait_for_latest_osdmap(), 0)\n\n    def _let_osds_back_up(self):\n        cmd = {\"prefix\":\"osd unset\", \"key\":\"noup\"}\n        r, _, _ = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(r, 0)\n\n    def test_aio_read(self):\n        # this is a list so that the local cb() can modify it\n        retval = [None]\n        lock = threading.Condition()\n        def cb(_, buf):\n            with lock:\n                retval[0] = buf\n                lock.notify()\n        payload = b\"bar\\000frob\"\n        self.ioctx.write(\"foo\", payload)\n\n        # test1: use wait_for_complete() and wait for cb by\n        # watching retval[0]\n        self._take_down_acting_set('test_pool', 'foo')\n        comp = self.ioctx.aio_read(\"foo\", len(payload), 0, cb)\n        eq(False, comp.is_complete())\n        time.sleep(3)\n        eq(False, comp.is_complete())\n        with lock:\n            eq(None, retval[0])\n        self._let_osds_back_up()\n        comp.wait_for_complete()\n        loops = 0\n        with lock:\n            while retval[0] is None and loops <= 10:\n                lock.wait(timeout=5)\n                loops += 1\n        assert(loops <= 10)\n\n        eq(retval[0], payload)\n        eq(sys.getrefcount(comp), 2)\n\n        # test2: use wait_for_complete_and_cb(), verify retval[0] is\n        # set by the time we regain control\n\n        retval[0] = None\n        self._take_down_acting_set('test_pool', 'foo')\n        comp = self.ioctx.aio_read(\"foo\", len(payload), 0, cb)\n        eq(False, comp.is_complete())\n        time.sleep(3)\n        eq(False, comp.is_complete())\n        with lock:\n            eq(None, retval[0])\n        self._let_osds_back_up()\n\n        comp.wait_for_complete_and_cb()\n        assert(retval[0] is not None)\n        eq(retval[0], payload)\n        eq(sys.getrefcount(comp), 2)\n\n        # test3: error case, use wait_for_complete_and_cb(), verify retval[0] is\n        # set by the time we regain control\n\n        retval[0] = 1\n        self._take_down_acting_set('test_pool', 'bar')\n        comp = self.ioctx.aio_read(\"bar\", len(payload), 0, cb)\n        eq(False, comp.is_complete())\n        time.sleep(3)\n        eq(False, comp.is_complete())\n        with lock:\n            eq(1, retval[0])\n        self._let_osds_back_up()\n\n        comp.wait_for_complete_and_cb()\n        eq(None, retval[0])\n        assert(comp.get_return_value() < 0)\n        eq(sys.getrefcount(comp), 2)\n\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_lock(self):\n        self.ioctx.lock_exclusive(\"foo\", \"lock\", \"locker\", \"desc_lock\",\n                                  10000, 0)\n        assert_raises(ObjectExists,\n                      self.ioctx.lock_exclusive,\n                      \"foo\", \"lock\", \"locker\", \"desc_lock\", 10000, 0)\n        self.ioctx.unlock(\"foo\", \"lock\", \"locker\")\n        assert_raises(ObjectNotFound, self.ioctx.unlock, \"foo\", \"lock\", \"locker\")\n\n        self.ioctx.lock_shared(\"foo\", \"lock\", \"locker1\", \"tag\", \"desc_lock\",\n                               10000, 0)\n        self.ioctx.lock_shared(\"foo\", \"lock\", \"locker2\", \"tag\", \"desc_lock\",\n                               10000, 0)\n        assert_raises(ObjectBusy,\n                      self.ioctx.lock_exclusive,\n                      \"foo\", \"lock\", \"locker3\", \"desc_lock\", 10000, 0)\n        self.ioctx.unlock(\"foo\", \"lock\", \"locker1\")\n        self.ioctx.unlock(\"foo\", \"lock\", \"locker2\")\n        assert_raises(ObjectNotFound, self.ioctx.unlock, \"foo\", \"lock\", \"locker1\")\n        assert_raises(ObjectNotFound, self.ioctx.unlock, \"foo\", \"lock\", \"locker2\")\n\n    def test_execute(self):\n        self.ioctx.write(\"foo\", b\"\") # ensure object exists\n\n        ret, buf = self.ioctx.execute(\"foo\", \"hello\", \"say_hello\", b\"\")\n        eq(buf, b\"Hello, world!\")\n\n        ret, buf = self.ioctx.execute(\"foo\", \"hello\", \"say_hello\", b\"nose\")\n        eq(buf, b\"Hello, nose!\")\n\n    def test_aio_execute(self):\n        count = [0]\n        retval = [None]\n        lock = threading.Condition()\n        def cb(_, buf):\n            with lock:\n                if retval[0] is None:\n                    retval[0] = buf\n                count[0] += 1\n                lock.notify()\n        self.ioctx.write(\"foo\", b\"\") # ensure object exists\n\n        comp = self.ioctx.aio_execute(\"foo\", \"hello\", \"say_hello\", b\"\", 32, cb, cb)\n        comp.wait_for_complete()\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        eq(comp.get_return_value(), 13)\n        eq(retval[0], b\"Hello, world!\")\n\n        retval[0] = None\n        comp = self.ioctx.aio_execute(\"foo\", \"hello\", \"say_hello\", b\"nose\", 32, cb, cb)\n        comp.wait_for_complete()\n        with lock:\n            while count[0] < 4:\n                lock.wait()\n        eq(comp.get_return_value(), 12)\n        eq(retval[0], b\"Hello, nose!\")\n\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_applications(self):\n        cmd = {\"prefix\":\"osd dump\", \"format\":\"json\"}\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(ret, 0)\n        assert len(buf) > 0\n        release = json.loads(buf.decode(\"utf-8\")).get(\"require_osd_release\",\n                                                      None)\n        if not release or release[0] < 'l':\n            raise SkipTest\n\n        eq([], self.ioctx.application_list())\n\n        self.ioctx.application_enable(\"app1\")\n        assert_raises(Error, self.ioctx.application_enable, \"app2\")\n        self.ioctx.application_enable(\"app2\", True)\n\n        assert_raises(Error, self.ioctx.application_metadata_list, \"dne\")\n        eq([], self.ioctx.application_metadata_list(\"app1\"))\n\n        assert_raises(Error, self.ioctx.application_metadata_set, \"dne\", \"key\",\n                      \"key\")\n        self.ioctx.application_metadata_set(\"app1\", \"key1\", \"val1\")\n        self.ioctx.application_metadata_set(\"app1\", \"key2\", \"val2\")\n        self.ioctx.application_metadata_set(\"app2\", \"key1\", \"val1\")\n\n        eq([(\"key1\", \"val1\"), (\"key2\", \"val2\")],\n           self.ioctx.application_metadata_list(\"app1\"))\n\n        self.ioctx.application_metadata_remove(\"app1\", \"key1\")\n        eq([(\"key2\", \"val2\")], self.ioctx.application_metadata_list(\"app1\"))\n\n    def test_service_daemon(self):\n        name = \"pid-\" + str(os.getpid())\n        metadata = {'version': '3.14', 'memory': '42'}\n        self.rados.service_daemon_register(\"laundry\", name, metadata)\n        status = {'result': 'unknown', 'test': 'running'}\n        self.rados.service_daemon_update(status)\n\n    def test_alignment(self):\n        eq(self.ioctx.alignment(), None)\n\n\nclass TestIoctxEc(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.pool = 'test-ec'\n        self.profile = 'testprofile-%s' % self.pool\n        cmd = {\"prefix\": \"osd erasure-code-profile set\", \n               \"name\": self.profile, \"profile\": [\"k=2\", \"m=1\", \"crush-failure-domain=osd\"]}\n        ret, buf, out = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, 0, msg=out)\n        # create ec pool with profile created above\n        cmd = {'prefix': 'osd pool create', 'pg_num': 8, 'pgp_num': 8,\n               'pool': self.pool, 'pool_type': 'erasure', \n               'erasure_code_profile': self.profile}\n        ret, buf, out = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, 0, msg=out)\n        assert self.rados.pool_exists(self.pool)\n        self.ioctx = self.rados.open_ioctx(self.pool)\n\n    def tearDown(self):\n        cmd = {\"prefix\": \"osd unset\", \"key\": \"noup\"}\n        self.rados.mon_command(json.dumps(cmd), b'')\n        self.ioctx.close()\n        self.rados.delete_pool(self.pool)\n        self.rados.shutdown()\n\n    def test_alignment(self):\n        eq(self.ioctx.alignment(), 8192)\n\n\nclass TestIoctx2(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.rados.create_pool('test_pool')\n        assert self.rados.pool_exists('test_pool')\n        pool_id = self.rados.pool_lookup('test_pool')\n        assert pool_id > 0\n        self.ioctx2 = self.rados.open_ioctx2(pool_id)\n\n    def tearDown(self):\n        cmd = {\"prefix\": \"osd unset\", \"key\": \"noup\"}\n        self.rados.mon_command(json.dumps(cmd), b'')\n        self.ioctx2.close()\n        self.rados.delete_pool('test_pool')\n        self.rados.shutdown()\n\n    def test_get_last_version(self):\n        version = self.ioctx2.get_last_version()\n        assert version >= 0\n\n    def test_get_stats(self):\n        stats = self.ioctx2.get_stats()\n        eq(stats, {'num_objects_unfound': 0,\n                   'num_objects_missing_on_primary': 0,\n                   'num_object_clones': 0,\n                   'num_objects': 0,\n                   'num_object_copies': 0,\n                   'num_bytes': 0,\n                   'num_rd_kb': 0,\n                   'num_wr_kb': 0,\n                   'num_kb': 0,\n                   'num_wr': 0,\n                   'num_objects_degraded': 0,\n                   'num_rd': 0})\n\n    def test_change_auid(self):\n        self.ioctx2.change_auid(ANONYMOUS_AUID)\n        self.ioctx2.change_auid(ADMIN_AUID)\n\n\nclass TestObject(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.rados.create_pool('test_pool')\n        assert self.rados.pool_exists('test_pool')\n        self.ioctx = self.rados.open_ioctx('test_pool')\n        self.ioctx.write('foo', b'bar')\n        self.object = Object(self.ioctx, 'foo')\n\n    def tearDown(self):\n        self.ioctx.close()\n        self.ioctx = None\n        self.rados.delete_pool('test_pool')\n        self.rados.shutdown()\n        self.rados = None\n\n    def test_read(self):\n        eq(self.object.read(3), b'bar')\n        eq(self.object.read(100), b'')\n\n    def test_seek(self):\n        self.object.write(b'blah')\n        self.object.seek(0)\n        eq(self.object.read(4), b'blah')\n        self.object.seek(1)\n        eq(self.object.read(3), b'lah')\n\n    def test_write(self):\n        self.object.write(b'barbaz')\n        self.object.seek(0)\n        eq(self.object.read(3), b'bar')\n        eq(self.object.read(3), b'baz')\n\nclass TestCommand(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n\n    def tearDown(self):\n        self.rados.shutdown()\n\n    def test_monmap_dump(self):\n\n        # check for success and some plain output with epoch in it\n        cmd = {\"prefix\":\"mon dump\"}\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, 0)\n        assert len(buf) > 0\n        assert(b'epoch' in buf)\n\n        # JSON, and grab current epoch\n        cmd['format'] = 'json'\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, 0)\n        assert len(buf) > 0\n        d = json.loads(buf.decode(\"utf-8\"))\n        assert('epoch' in d)\n        epoch = d['epoch']\n\n        # assume epoch + 1000 does not exist; test for ENOENT\n        cmd['epoch'] = epoch + 1000\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, -errno.ENOENT)\n        eq(len(buf), 0)\n        del cmd['epoch']\n\n        # send to specific target by name\n        target = d['mons'][0]['name']\n        print(target)\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30,\n                                                target=target)\n        eq(ret, 0)\n        assert len(buf) > 0\n        d = json.loads(buf.decode(\"utf-8\"))\n        assert('epoch' in d)\n\n        # and by rank\n        target = d['mons'][0]['rank']\n        print(target)\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30,\n                                                target=target)\n        eq(ret, 0)\n        assert len(buf) > 0\n        d = json.loads(buf.decode(\"utf-8\"))\n        assert('epoch' in d)\n\n    def test_osd_bench(self):\n        cmd = dict(prefix='bench', size=4096, count=8192)\n        ret, buf, err = self.rados.osd_command(0, json.dumps(cmd), b'',\n                                               timeout=30)\n        eq(ret, 0)\n        assert len(buf) > 0\n        out = json.loads(buf.decode('utf-8'))\n        eq(out['blocksize'], cmd['size'])\n        eq(out['bytes_written'], cmd['count'])\n\n    def test_ceph_osd_pool_create_utf8(self):\n        if _python2:\n            # Use encoded bytestring\n            poolname = b\"\\351\\273\\205\"\n        else:\n            poolname = \"\\u9ec5\"\n\n        cmd = {\"prefix\": \"osd pool create\", \"pg_num\": 16, \"pool\": poolname}\n        ret, buf, out = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(ret, 0)\n        assert len(out) > 0\n        eq(u\"pool '\\u9ec5' created\", out)\n"], "fixing_code": ["#!/usr/bin/env bash\n\nset -ex\n\nKEYRING=$(mktemp)\ntrap cleanup EXIT ERR HUP INT QUIT\n\ncleanup() {\n    (ceph auth del client.mon_read || true) >/dev/null 2>&1\n    (ceph auth del client.mon_write || true) >/dev/null 2>&1\n\n    rm -f $KEYRING\n}\n\nexpect_false()\n{\n\tset -x\n\tif \"$@\"; then return 1; else return 0; fi\n}\n\ncreate_pool_op() {\n  ID=$1\n  POOL=$2\n\n  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\nimport rados\n\ncluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\ncluster.connect()\ncluster.create_pool(\"${POOL}\")\nEOF\n}\n\ndelete_pool_op() {\n  ID=$1\n  POOL=$2\n\n  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\nimport rados\n\ncluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\ncluster.connect()\ncluster.delete_pool(\"${POOL}\")\nEOF\n}\n\ncreate_pool_snap_op() {\n  ID=$1\n  POOL=$2\n  SNAP=$3\n\n  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\nimport rados\n\ncluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\ncluster.connect()\nioctx = cluster.open_ioctx(\"${POOL}\")\n\nioctx.create_snap(\"${SNAP}\")\nEOF\n}\n\nremove_pool_snap_op() {\n  ID=$1\n  POOL=$2\n  SNAP=$3\n\n  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\nimport rados\n\ncluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\ncluster.connect()\nioctx = cluster.open_ioctx(\"${POOL}\")\n\nioctx.remove_snap(\"${SNAP}\")\nEOF\n}\n\ntest_pool_op()\n{\n    ceph auth get-or-create client.mon_read mon 'allow r' >> $KEYRING\n    ceph auth get-or-create client.mon_write mon 'allow *' >> $KEYRING\n\n    expect_false create_pool_op mon_read pool1\n    create_pool_op mon_write pool1\n\n    expect_false create_pool_snap_op mon_read pool1 snap1\n    create_pool_snap_op mon_write pool1 snap1\n\n    expect_false remove_pool_snap_op mon_read pool1 snap1\n    remove_pool_snap_op mon_write pool1 snap1\n\n    expect_false delete_pool_op mon_read pool1\n    delete_pool_op mon_write pool1\n}\n\nkey=`ceph auth get-or-create-key client.poolaccess1 mon 'allow r' osd 'allow *'`\nrados --id poolaccess1 --key $key -p rbd ls\n\nkey=`ceph auth get-or-create-key client.poolaccess2 mon 'allow r' osd 'allow * pool=nopool'`\nexpect_false rados --id poolaccess2 --key $key -p rbd ls\n\nkey=`ceph auth get-or-create-key client.poolaccess3 mon 'allow r' osd 'allow rw pool=nopool'`\nexpect_false rados --id poolaccess3 --key $key -p rbd ls\n\ntest_pool_op\n\necho OK\n", "#!/usr/bin/env bash\nset -ex\n\nIMAGE_FEATURES=\"layering,exclusive-lock,object-map,fast-diff\"\n\nclone_v2_enabled() {\n    image_spec=$1\n    rbd info $image_spec | grep \"clone-parent\"\n}\n\ncreate_pools() {\n    ceph osd pool create images 32\n    rbd pool init images\n    ceph osd pool create volumes 32\n    rbd pool init volumes\n}\n\ndelete_pools() {\n    (ceph osd pool delete images images --yes-i-really-really-mean-it || true) >/dev/null 2>&1\n    (ceph osd pool delete volumes volumes --yes-i-really-really-mean-it || true) >/dev/null 2>&1\n\n}\n\nrecreate_pools() {\n    delete_pools\n    create_pools\n}\n\ndelete_users() {\n    (ceph auth del client.volumes || true) >/dev/null 2>&1\n    (ceph auth del client.images || true) >/dev/null 2>&1\n\n    (ceph auth del client.snap_none || true) >/dev/null 2>&1\n    (ceph auth del client.snap_all || true) >/dev/null 2>&1\n    (ceph auth del client.snap_pool || true) >/dev/null 2>&1\n    (ceph auth del client.snap_profile_all || true) >/dev/null 2>&1\n    (ceph auth del client.snap_profile_pool || true) >/dev/null 2>&1\n\n    (ceph auth del client.mon_write || true) >/dev/null 2>&1\n}\n\ncreate_users() {\n    ceph auth get-or-create client.volumes mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd-read-only pool=images' >> $KEYRING\n    ceph auth get-or-create client.images mon 'profile rbd' osd 'profile rbd pool=images' >> $KEYRING\n\n    ceph auth get-or-create client.snap_none mon 'allow r' >> $KEYRING\n    ceph auth get-or-create client.snap_all mon 'allow r' osd 'allow w' >> $KEYRING\n    ceph auth get-or-create client.snap_pool mon 'allow r' osd 'allow w pool=images' >> $KEYRING\n    ceph auth get-or-create client.snap_profile_all mon 'allow r' osd 'profile rbd' >> $KEYRING\n    ceph auth get-or-create client.snap_profile_pool mon 'allow r' osd 'profile rbd pool=images' >> $KEYRING\n\n    ceph auth get-or-create client.mon_write mon 'allow *' >> $KEYRING\n}\n\nexpect() {\n\n  set +e\n\n  local expected_ret=$1\n  local ret\n\n  shift\n  cmd=$@\n\n  eval $cmd\n  ret=$?\n\n  set -e\n\n  if [[ $ret -ne $expected_ret ]]; then\n    echo \"ERROR: running \\'$cmd\\': expected $expected_ret got $ret\"\n    return 1\n  fi\n\n  return 0\n}\n\ntest_images_access() {\n    rbd -k $KEYRING --id images create --image-format 2 --image-feature $IMAGE_FEATURES -s 1 images/foo\n    rbd -k $KEYRING --id images snap create images/foo@snap\n    rbd -k $KEYRING --id images snap protect images/foo@snap\n    rbd -k $KEYRING --id images snap unprotect images/foo@snap\n    rbd -k $KEYRING --id images snap protect images/foo@snap\n    rbd -k $KEYRING --id images export images/foo@snap - >/dev/null\n    expect 16 rbd -k $KEYRING --id images snap rm images/foo@snap\n\n    rbd -k $KEYRING --id volumes clone --image-feature $IMAGE_FEATURES images/foo@snap volumes/child\n\n    if ! clone_v2_enabled images/foo; then\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n    fi\n\n    expect 1 rbd -k $KEYRING --id volumes snap unprotect images/foo@snap\n    expect 1 rbd -k $KEYRING --id images flatten volumes/child\n    rbd -k $KEYRING --id volumes flatten volumes/child\n    expect 1 rbd -k $KEYRING --id volumes snap unprotect images/foo@snap\n    rbd -k $KEYRING --id images snap unprotect images/foo@snap\n\n    expect 39 rbd -k $KEYRING --id images rm images/foo\n    rbd -k $KEYRING --id images snap rm images/foo@snap\n    rbd -k $KEYRING --id images rm images/foo\n    rbd -k $KEYRING --id volumes rm volumes/child\n}\n\ntest_volumes_access() {\n    rbd -k $KEYRING --id images create --image-format 2 --image-feature $IMAGE_FEATURES -s 1 images/foo\n    rbd -k $KEYRING --id images snap create images/foo@snap\n    rbd -k $KEYRING --id images snap protect images/foo@snap\n\n    # commands that work with read-only access\n    rbd -k $KEYRING --id volumes info images/foo@snap\n    rbd -k $KEYRING --id volumes snap ls images/foo\n    rbd -k $KEYRING --id volumes export images/foo - >/dev/null\n    rbd -k $KEYRING --id volumes cp images/foo volumes/foo_copy\n    rbd -k $KEYRING --id volumes rm volumes/foo_copy\n    rbd -k $KEYRING --id volumes children images/foo@snap\n    rbd -k $KEYRING --id volumes lock list images/foo\n\n    # commands that fail with read-only access\n    expect 1 rbd -k $KEYRING --id volumes resize -s 2 images/foo --allow-shrink\n    expect 1 rbd -k $KEYRING --id volumes snap create images/foo@2\n    expect 1 rbd -k $KEYRING --id volumes snap rollback images/foo@snap\n    expect 1 rbd -k $KEYRING --id volumes snap remove images/foo@snap\n    expect 1 rbd -k $KEYRING --id volumes snap purge images/foo\n    expect 1 rbd -k $KEYRING --id volumes snap unprotect images/foo@snap\n    expect 1 rbd -k $KEYRING --id volumes flatten images/foo\n    expect 1 rbd -k $KEYRING --id volumes lock add images/foo test\n    expect 1 rbd -k $KEYRING --id volumes lock remove images/foo test locker\n    expect 1 rbd -k $KEYRING --id volumes ls rbd\n\n    # create clone and snapshot\n    rbd -k $KEYRING --id volumes clone --image-feature $IMAGE_FEATURES images/foo@snap volumes/child\n    rbd -k $KEYRING --id volumes snap create volumes/child@snap1\n    rbd -k $KEYRING --id volumes snap protect volumes/child@snap1\n    rbd -k $KEYRING --id volumes snap create volumes/child@snap2\n\n    # make sure original snapshot stays protected\n    if clone_v2_enabled images/foo; then\n        rbd -k $KEYRING --id volumes flatten volumes/child\n        rbd -k $KEYRING --id volumes snap rm volumes/child@snap2\n        rbd -k $KEYRING --id volumes snap unprotect volumes/child@snap1\n    else\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n        rbd -k $KEYRING --id volumes flatten volumes/child\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n        rbd -k $KEYRING --id volumes snap rm volumes/child@snap2\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n        expect 2 rbd -k $KEYRING --id volumes snap rm volumes/child@snap2\n        rbd -k $KEYRING --id volumes snap unprotect volumes/child@snap1\n        expect 16 rbd -k $KEYRING --id images snap unprotect images/foo@snap\n    fi\n\n    # clean up\n    rbd -k $KEYRING --id volumes snap rm volumes/child@snap1\n    rbd -k $KEYRING --id images snap unprotect images/foo@snap\n    rbd -k $KEYRING --id images snap rm images/foo@snap\n    rbd -k $KEYRING --id images rm images/foo\n    rbd -k $KEYRING --id volumes rm volumes/child\n}\n\ncreate_self_managed_snapshot() {\n  ID=$1\n  POOL=$2\n\n  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\nimport rados\n\ncluster = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\ncluster.connect()\nioctx = cluster.open_ioctx(\"${POOL}\")\n\nsnap_id = ioctx.create_self_managed_snap()\nprint (\"Created snap id {}\".format(snap_id))\nEOF\n}\n\nremove_self_managed_snapshot() {\n  ID=$1\n  POOL=$2\n\n  cat << EOF | CEPH_ARGS=\"-k $KEYRING\" python\nimport rados\n\ncluster1 = rados.Rados(conffile=\"\", rados_id=\"mon_write\")\ncluster1.connect()\nioctx1 = cluster1.open_ioctx(\"${POOL}\")\n\nsnap_id = ioctx1.create_self_managed_snap()\nprint (\"Created snap id {}\".format(snap_id))\n\ncluster2 = rados.Rados(conffile=\"\", rados_id=\"${ID}\")\ncluster2.connect()\nioctx2 = cluster2.open_ioctx(\"${POOL}\")\n\nioctx2.remove_self_managed_snap(snap_id)\nprint (\"Removed snap id {}\".format(snap_id))\nEOF\n}\n\ntest_remove_self_managed_snapshots() {\n    # Ensure users cannot create self-managed snapshots w/o permissions\n    expect 1 create_self_managed_snapshot snap_none images\n    expect 1 create_self_managed_snapshot snap_none volumes\n\n    create_self_managed_snapshot snap_all images\n    create_self_managed_snapshot snap_all volumes\n\n    create_self_managed_snapshot snap_pool images\n    expect 1 create_self_managed_snapshot snap_pool volumes\n\n    create_self_managed_snapshot snap_profile_all images\n    create_self_managed_snapshot snap_profile_all volumes\n\n    create_self_managed_snapshot snap_profile_pool images\n    expect 1 create_self_managed_snapshot snap_profile_pool volumes\n\n    # Ensure users cannot delete self-managed snapshots w/o permissions\n    expect 1 remove_self_managed_snapshot snap_none images\n    expect 1 remove_self_managed_snapshot snap_none volumes\n\n    remove_self_managed_snapshot snap_all images\n    remove_self_managed_snapshot snap_all volumes\n\n    remove_self_managed_snapshot snap_pool images\n    expect 1 remove_self_managed_snapshot snap_pool volumes\n\n    remove_self_managed_snapshot snap_profile_all images\n    remove_self_managed_snapshot snap_profile_all volumes\n\n    remove_self_managed_snapshot snap_profile_pool images\n    expect 1 remove_self_managed_snapshot snap_profile_pool volumes\n}\n\ncleanup() {\n    rm -f $KEYRING\n}\n\nKEYRING=$(mktemp)\ntrap cleanup EXIT ERR HUP INT QUIT\n\ndelete_users\ncreate_users\n\nrecreate_pools\ntest_images_access\n\nrecreate_pools\ntest_volumes_access\n\ntest_remove_self_managed_snapshots\n\ndelete_pools\ndelete_users\n\necho OK\nexit 0\n", "// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-\n// vim: ts=8 sw=2 smarttab\n/*\n * Ceph - scalable distributed file system\n *\n * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>\n * Copyright (C) 2013,2014 Cloudwatt <libre.licensing@cloudwatt.com>\n * Copyright (C) 2014 Red Hat <contact@redhat.com>\n *\n * Author: Loic Dachary <loic@dachary.org>\n *\n * This is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License version 2.1, as published by the Free Software\n * Foundation.  See file COPYING.\n *\n */\n\n#include <algorithm>\n#include <boost/algorithm/string.hpp>\n#include <locale>\n#include <sstream>\n\n#include \"mon/OSDMonitor.h\"\n#include \"mon/Monitor.h\"\n#include \"mon/MDSMonitor.h\"\n#include \"mon/MgrStatMonitor.h\"\n#include \"mon/AuthMonitor.h\"\n#include \"mon/ConfigKeyService.h\"\n\n#include \"mon/MonitorDBStore.h\"\n#include \"mon/Session.h\"\n\n#include \"crush/CrushWrapper.h\"\n#include \"crush/CrushTester.h\"\n#include \"crush/CrushTreeDumper.h\"\n\n#include \"messages/MOSDBeacon.h\"\n#include \"messages/MOSDFailure.h\"\n#include \"messages/MOSDMarkMeDown.h\"\n#include \"messages/MOSDFull.h\"\n#include \"messages/MOSDMap.h\"\n#include \"messages/MMonGetOSDMap.h\"\n#include \"messages/MOSDBoot.h\"\n#include \"messages/MOSDAlive.h\"\n#include \"messages/MPoolOp.h\"\n#include \"messages/MPoolOpReply.h\"\n#include \"messages/MOSDPGCreate.h\"\n#include \"messages/MOSDPGCreate2.h\"\n#include \"messages/MOSDPGCreated.h\"\n#include \"messages/MOSDPGTemp.h\"\n#include \"messages/MMonCommand.h\"\n#include \"messages/MRemoveSnaps.h\"\n#include \"messages/MOSDScrub.h\"\n#include \"messages/MRoute.h\"\n\n#include \"common/TextTable.h\"\n#include \"common/Timer.h\"\n#include \"common/ceph_argparse.h\"\n#include \"common/perf_counters.h\"\n#include \"common/strtol.h\"\n\n#include \"common/config.h\"\n#include \"common/errno.h\"\n\n#include \"erasure-code/ErasureCodePlugin.h\"\n#include \"compressor/Compressor.h\"\n#include \"common/Checksummer.h\"\n\n#include \"include/compat.h\"\n#include \"include/assert.h\"\n#include \"include/stringify.h\"\n#include \"include/util.h\"\n#include \"common/cmdparse.h\"\n#include \"include/str_list.h\"\n#include \"include/str_map.h\"\n#include \"include/scope_guard.h\"\n\n#include \"auth/cephx/CephxKeyServer.h\"\n#include \"osd/OSDCap.h\"\n\n#include \"json_spirit/json_spirit_reader.h\"\n\n#include <boost/algorithm/string/predicate.hpp>\n\n#define dout_subsys ceph_subsys_mon\nstatic const string OSD_PG_CREATING_PREFIX(\"osd_pg_creating\");\nstatic const string OSD_METADATA_PREFIX(\"osd_metadata\");\nstatic const string OSD_SNAP_PREFIX(\"osd_snap\");\n\nnamespace {\n\nconst uint32_t MAX_POOL_APPLICATIONS = 4;\nconst uint32_t MAX_POOL_APPLICATION_KEYS = 64;\nconst uint32_t MAX_POOL_APPLICATION_LENGTH = 128;\n\nbool is_osd_writable(const OSDCapGrant& grant, const std::string* pool_name) {\n  // Note: this doesn't include support for the application tag match\n  if ((grant.spec.allow & OSD_CAP_W) != 0) {\n    auto& match = grant.match;\n    if (match.is_match_all()) {\n      return true;\n    } else if (pool_name != nullptr && match.auid < 0 &&\n               !match.pool_namespace.pool_name.empty() &&\n               match.pool_namespace.pool_name == *pool_name) {\n      return true;\n    }\n  }\n  return false;\n}\n\nbool is_unmanaged_snap_op_permitted(CephContext* cct,\n                                    const KeyServer& key_server,\n                                    const EntityName& entity_name,\n                                    const MonCap& mon_caps,\n                                    const std::string* pool_name)\n{\n  typedef std::map<std::string, std::string> CommandArgs;\n\n  if (mon_caps.is_capable(cct, CEPH_ENTITY_TYPE_MON,\n                               entity_name, \"osd\",\n                               \"osd pool op unmanaged-snap\",\n                               (pool_name == nullptr ?\n                                  CommandArgs{} /* pool DNE, require unrestricted cap */ :\n                                  CommandArgs{{\"poolname\", *pool_name}}),\n                                false, true, false)) {\n    return true;\n  }\n\n  AuthCapsInfo caps_info;\n  if (!key_server.get_service_caps(entity_name, CEPH_ENTITY_TYPE_OSD,\n                                   caps_info)) {\n    dout(10) << \"unable to locate OSD cap data for \" << entity_name\n             << \" in auth db\" << dendl;\n    return false;\n  }\n\n  string caps_str;\n  if (caps_info.caps.length() > 0) {\n    auto p = caps_info.caps.cbegin();\n    try {\n      decode(caps_str, p);\n    } catch (const buffer::error &err) {\n      derr << \"corrupt OSD cap data for \" << entity_name << \" in auth db\"\n           << dendl;\n      return false;\n    }\n  }\n\n  OSDCap osd_cap;\n  if (!osd_cap.parse(caps_str, nullptr)) {\n    dout(10) << \"unable to parse OSD cap data for \" << entity_name\n             << \" in auth db\" << dendl;\n    return false;\n  }\n\n  // if the entity has write permissions in one or all pools, permit\n  // usage of unmanaged-snapshots\n  if (osd_cap.allow_all()) {\n    return true;\n  }\n\n  for (auto& grant : osd_cap.grants) {\n    if (grant.profile.is_valid()) {\n      for (auto& profile_grant : grant.profile_grants) {\n        if (is_osd_writable(profile_grant, pool_name)) {\n          return true;\n        }\n      }\n    } else if (is_osd_writable(grant, pool_name)) {\n      return true;\n    }\n  }\n\n  return false;\n}\n\n} // anonymous namespace\n\nvoid LastEpochClean::Lec::report(ps_t ps, epoch_t last_epoch_clean)\n{\n  if (epoch_by_pg.size() <= ps) {\n    epoch_by_pg.resize(ps + 1, 0);\n  }\n  const auto old_lec = epoch_by_pg[ps];\n  if (old_lec >= last_epoch_clean) {\n    // stale lec\n    return;\n  }\n  epoch_by_pg[ps] = last_epoch_clean;\n  if (last_epoch_clean < floor) {\n    floor = last_epoch_clean;\n  } else if (last_epoch_clean > floor) {\n    if (old_lec == floor) {\n      // probably should increase floor?\n      auto new_floor = std::min_element(std::begin(epoch_by_pg),\n\t\t\t\t\tstd::end(epoch_by_pg));\n      floor = *new_floor;\n    }\n  }\n  if (ps != next_missing) {\n    return;\n  }\n  for (; next_missing < epoch_by_pg.size(); next_missing++) {\n    if (epoch_by_pg[next_missing] == 0) {\n      break;\n    }\n  }\n}\n\nvoid LastEpochClean::remove_pool(uint64_t pool)\n{\n  report_by_pool.erase(pool);\n}\n\nvoid LastEpochClean::report(const pg_t& pg, epoch_t last_epoch_clean)\n{\n  auto& lec = report_by_pool[pg.pool()];\n  return lec.report(pg.ps(), last_epoch_clean);\n}\n\nepoch_t LastEpochClean::get_lower_bound(const OSDMap& latest) const\n{\n  auto floor = latest.get_epoch();\n  for (auto& pool : latest.get_pools()) {\n    auto reported = report_by_pool.find(pool.first);\n    if (reported == report_by_pool.end()) {\n      return 0;\n    }\n    if (reported->second.next_missing < pool.second.get_pg_num()) {\n      return 0;\n    }\n    if (reported->second.floor < floor) {\n      floor = reported->second.floor;\n    }\n  }\n  return floor;\n}\n\n\nclass C_UpdateCreatingPGs : public Context {\npublic:\n  OSDMonitor *osdmon;\n  utime_t start;\n  epoch_t epoch;\n  C_UpdateCreatingPGs(OSDMonitor *osdmon, epoch_t e) :\n    osdmon(osdmon), start(ceph_clock_now()), epoch(e) {}\n  void finish(int r) override {\n    if (r >= 0) {\n      utime_t end = ceph_clock_now();\n      dout(10) << \"osdmap epoch \" << epoch << \" mapping took \"\n\t       << (end - start) << \" seconds\" << dendl;\n      osdmon->update_creating_pgs();\n      osdmon->check_pg_creates_subs();\n    }\n  }\n};\n\n#undef dout_prefix\n#define dout_prefix _prefix(_dout, mon, osdmap)\nstatic ostream& _prefix(std::ostream *_dout, Monitor *mon, const OSDMap& osdmap) {\n  return *_dout << \"mon.\" << mon->name << \"@\" << mon->rank\n\t\t<< \"(\" << mon->get_state_name()\n\t\t<< \").osd e\" << osdmap.get_epoch() << \" \";\n}\n\nOSDMonitor::OSDMonitor(\n  CephContext *cct,\n  Monitor *mn,\n  Paxos *p,\n  const string& service_name)\n : PaxosService(mn, p, service_name),\n   cct(cct),\n   inc_osd_cache(g_conf->mon_osd_cache_size),\n   full_osd_cache(g_conf->mon_osd_cache_size),\n   has_osdmap_manifest(false),\n   mapper(mn->cct, &mn->cpu_tp)\n{}\n\nbool OSDMonitor::_have_pending_crush()\n{\n  return pending_inc.crush.length() > 0;\n}\n\nCrushWrapper &OSDMonitor::_get_stable_crush()\n{\n  return *osdmap.crush;\n}\n\nvoid OSDMonitor::_get_pending_crush(CrushWrapper& newcrush)\n{\n  bufferlist bl;\n  if (pending_inc.crush.length())\n    bl = pending_inc.crush;\n  else\n    osdmap.crush->encode(bl, CEPH_FEATURES_SUPPORTED_DEFAULT);\n\n  auto p = bl.cbegin();\n  newcrush.decode(p);\n}\n\nvoid OSDMonitor::create_initial()\n{\n  dout(10) << \"create_initial for \" << mon->monmap->fsid << dendl;\n\n  OSDMap newmap;\n\n  bufferlist bl;\n  mon->store->get(\"mkfs\", \"osdmap\", bl);\n\n  if (bl.length()) {\n    newmap.decode(bl);\n    newmap.set_fsid(mon->monmap->fsid);\n  } else {\n    newmap.build_simple(cct, 0, mon->monmap->fsid, 0);\n  }\n  newmap.set_epoch(1);\n  newmap.created = newmap.modified = ceph_clock_now();\n\n  // new clusters should sort bitwise by default.\n  newmap.set_flag(CEPH_OSDMAP_SORTBITWISE);\n\n  newmap.flags |=\n    CEPH_OSDMAP_RECOVERY_DELETES |\n    CEPH_OSDMAP_PURGED_SNAPDIRS;\n  newmap.full_ratio = g_conf->mon_osd_full_ratio;\n  if (newmap.full_ratio > 1.0) newmap.full_ratio /= 100;\n  newmap.backfillfull_ratio = g_conf->mon_osd_backfillfull_ratio;\n  if (newmap.backfillfull_ratio > 1.0) newmap.backfillfull_ratio /= 100;\n  newmap.nearfull_ratio = g_conf->mon_osd_nearfull_ratio;\n  if (newmap.nearfull_ratio > 1.0) newmap.nearfull_ratio /= 100;\n\n  // new cluster should require latest by default\n  if (g_conf->get_val<bool>(\"mon_debug_no_require_nautilus\")) {\n    if (g_conf->mon_debug_no_require_mimic) {\n      derr << __func__ << \" mon_debug_no_require_mimic=true and nautilus=true\" << dendl;\n      newmap.require_osd_release = CEPH_RELEASE_LUMINOUS;\n    } else {\n      derr << __func__ << \" mon_debug_no_require_nautilus=true\" << dendl;\n      newmap.require_osd_release = CEPH_RELEASE_MIMIC;\n    }\n  } else {\n    newmap.require_osd_release = CEPH_RELEASE_NAUTILUS;\n    int r = ceph_release_from_name(\n      g_conf->mon_osd_initial_require_min_compat_client.c_str());\n    if (r <= 0) {\n      assert(0 == \"mon_osd_initial_require_min_compat_client is not valid\");\n    }\n    newmap.require_min_compat_client = r;\n  }\n\n  // encode into pending incremental\n  uint64_t features = newmap.get_encoding_features();\n  newmap.encode(pending_inc.fullmap,\n                features | CEPH_FEATURE_RESERVED);\n  pending_inc.full_crc = newmap.get_crc();\n  dout(20) << \" full crc \" << pending_inc.full_crc << dendl;\n}\n\nvoid OSDMonitor::get_store_prefixes(std::set<string>& s) const\n{\n  s.insert(service_name);\n  s.insert(OSD_PG_CREATING_PREFIX);\n  s.insert(OSD_METADATA_PREFIX);\n  s.insert(OSD_SNAP_PREFIX);\n}\n\nvoid OSDMonitor::update_from_paxos(bool *need_bootstrap)\n{\n  // we really don't care if the version has been updated, because we may\n  // have trimmed without having increased the last committed; yet, we may\n  // need to update the in-memory manifest.\n  load_osdmap_manifest();\n\n  version_t version = get_last_committed();\n  if (version == osdmap.epoch)\n    return;\n  assert(version > osdmap.epoch);\n\n  dout(15) << \"update_from_paxos paxos e \" << version\n\t   << \", my e \" << osdmap.epoch << dendl;\n\n  if (mapping_job) {\n    if (!mapping_job->is_done()) {\n      dout(1) << __func__ << \" mapping job \"\n\t      << mapping_job.get() << \" did not complete, \"\n\t      << mapping_job->shards << \" left, canceling\" << dendl;\n      mapping_job->abort();\n    }\n    mapping_job.reset();\n  }\n\n  load_health();\n\n  /*\n   * We will possibly have a stashed latest that *we* wrote, and we will\n   * always be sure to have the oldest full map in the first..last range\n   * due to encode_trim_extra(), which includes the oldest full map in the trim\n   * transaction.\n   *\n   * encode_trim_extra() does not however write the full map's\n   * version to 'full_latest'.  This is only done when we are building the\n   * full maps from the incremental versions.  But don't panic!  We make sure\n   * that the following conditions find whichever full map version is newer.\n   */\n  version_t latest_full = get_version_latest_full();\n  if (latest_full == 0 && get_first_committed() > 1)\n    latest_full = get_first_committed();\n\n  if (get_first_committed() > 1 &&\n      latest_full < get_first_committed()) {\n    // the monitor could be just sync'ed with its peer, and the latest_full key\n    // is not encoded in the paxos commits in encode_pending(), so we need to\n    // make sure we get it pointing to a proper version.\n    version_t lc = get_last_committed();\n    version_t fc = get_first_committed();\n\n    dout(10) << __func__ << \" looking for valid full map in interval\"\n\t     << \" [\" << fc << \", \" << lc << \"]\" << dendl;\n\n    latest_full = 0;\n    for (version_t v = lc; v >= fc; v--) {\n      string full_key = \"full_\" + stringify(v);\n      if (mon->store->exists(get_service_name(), full_key)) {\n        dout(10) << __func__ << \" found latest full map v \" << v << dendl;\n        latest_full = v;\n        break;\n      }\n    }\n\n    assert(latest_full > 0);\n    auto t(std::make_shared<MonitorDBStore::Transaction>());\n    put_version_latest_full(t, latest_full);\n    mon->store->apply_transaction(t);\n    dout(10) << __func__ << \" updated the on-disk full map version to \"\n             << latest_full << dendl;\n  }\n\n  if ((latest_full > 0) && (latest_full > osdmap.epoch)) {\n    bufferlist latest_bl;\n    get_version_full(latest_full, latest_bl);\n    assert(latest_bl.length() != 0);\n    dout(7) << __func__ << \" loading latest full map e\" << latest_full << dendl;\n    osdmap = OSDMap();\n    osdmap.decode(latest_bl);\n  }\n\n  bufferlist bl;\n  if (!mon->store->get(OSD_PG_CREATING_PREFIX, \"creating\", bl)) {\n    auto p = bl.cbegin();\n    std::lock_guard<std::mutex> l(creating_pgs_lock);\n    creating_pgs.decode(p);\n    dout(7) << __func__ << \" loading creating_pgs last_scan_epoch \"\n\t    << creating_pgs.last_scan_epoch\n\t    << \" with \" << creating_pgs.pgs.size() << \" pgs\" << dendl;\n  } else {\n    dout(1) << __func__ << \" missing creating pgs; upgrade from post-kraken?\"\n\t    << dendl;\n  }\n\n  // walk through incrementals\n  MonitorDBStore::TransactionRef t;\n  size_t tx_size = 0;\n  while (version > osdmap.epoch) {\n    bufferlist inc_bl;\n    int err = get_version(osdmap.epoch+1, inc_bl);\n    assert(err == 0);\n    assert(inc_bl.length());\n\n    dout(7) << \"update_from_paxos  applying incremental \" << osdmap.epoch+1\n\t    << dendl;\n    OSDMap::Incremental inc(inc_bl);\n    err = osdmap.apply_incremental(inc);\n    assert(err == 0);\n\n    if (!t)\n      t.reset(new MonitorDBStore::Transaction);\n\n    // Write out the full map for all past epochs.  Encode the full\n    // map with the same features as the incremental.  If we don't\n    // know, use the quorum features.  If we don't know those either,\n    // encode with all features.\n    uint64_t f = inc.encode_features;\n    if (!f)\n      f = mon->get_quorum_con_features();\n    if (!f)\n      f = -1;\n    bufferlist full_bl;\n    osdmap.encode(full_bl, f | CEPH_FEATURE_RESERVED);\n    tx_size += full_bl.length();\n\n    bufferlist orig_full_bl;\n    get_version_full(osdmap.epoch, orig_full_bl);\n    if (orig_full_bl.length()) {\n      // the primary provided the full map\n      assert(inc.have_crc);\n      if (inc.full_crc != osdmap.crc) {\n\t// This will happen if the mons were running mixed versions in\n\t// the past or some other circumstance made the full encoded\n\t// maps divergent.  Reloading here will bring us back into\n\t// sync with the primary for this and all future maps.  OSDs\n\t// will also be brought back into sync when they discover the\n\t// crc mismatch and request a full map from a mon.\n\tderr << __func__ << \" full map CRC mismatch, resetting to canonical\"\n\t     << dendl;\n\n\tdout(20) << __func__ << \" my (bad) full osdmap:\\n\";\n\tJSONFormatter jf(true);\n\tjf.dump_object(\"osdmap\", osdmap);\n\tjf.flush(*_dout);\n\t*_dout << \"\\nhexdump:\\n\";\n\tfull_bl.hexdump(*_dout);\n\t*_dout << dendl;\n\n\tosdmap = OSDMap();\n\tosdmap.decode(orig_full_bl);\n\n\tdout(20) << __func__ << \" canonical full osdmap:\\n\";\n\tJSONFormatter jf(true);\n\tjf.dump_object(\"osdmap\", osdmap);\n\tjf.flush(*_dout);\n\t*_dout << \"\\nhexdump:\\n\";\n\torig_full_bl.hexdump(*_dout);\n\t*_dout << dendl;\n      }\n    } else {\n      assert(!inc.have_crc);\n      put_version_full(t, osdmap.epoch, full_bl);\n    }\n    put_version_latest_full(t, osdmap.epoch);\n\n    // share\n    dout(1) << osdmap << dendl;\n\n    if (osdmap.epoch == 1) {\n      t->erase(\"mkfs\", \"osdmap\");\n    }\n\n    if (tx_size > g_conf->mon_sync_max_payload_size*2) {\n      mon->store->apply_transaction(t);\n      t = MonitorDBStore::TransactionRef();\n      tx_size = 0;\n    }\n    for (const auto &osd_state : inc.new_state) {\n      if (osd_state.second & CEPH_OSD_UP) {\n\t// could be marked up *or* down, but we're too lazy to check which\n\tlast_osd_report.erase(osd_state.first);\n      }\n      if (osd_state.second & CEPH_OSD_EXISTS) {\n\t// could be created *or* destroyed, but we can safely drop it\n\tosd_epochs.erase(osd_state.first);\n      }\n    }\n  }\n\n  if (t) {\n    mon->store->apply_transaction(t);\n  }\n\n  for (int o = 0; o < osdmap.get_max_osd(); o++) {\n    if (osdmap.is_out(o))\n      continue;\n    auto found = down_pending_out.find(o);\n    if (osdmap.is_down(o)) {\n      // populate down -> out map\n      if (found == down_pending_out.end()) {\n        dout(10) << \" adding osd.\" << o << \" to down_pending_out map\" << dendl;\n        down_pending_out[o] = ceph_clock_now();\n      }\n    } else {\n      if (found != down_pending_out.end()) {\n        dout(10) << \" removing osd.\" << o << \" from down_pending_out map\" << dendl;\n        down_pending_out.erase(found);\n      }\n    }\n  }\n  // XXX: need to trim MonSession connected with a osd whose id > max_osd?\n\n  check_osdmap_subs();\n  check_pg_creates_subs();\n\n  share_map_with_random_osd();\n  update_logger();\n\n  process_failures();\n\n  // make sure our feature bits reflect the latest map\n  update_msgr_features();\n\n  if (!mon->is_leader()) {\n    // will be called by on_active() on the leader, avoid doing so twice\n    start_mapping();\n  }\n}\n\nvoid OSDMonitor::start_mapping()\n{\n  // initiate mapping job\n  if (mapping_job) {\n    dout(10) << __func__ << \" canceling previous mapping_job \" << mapping_job.get()\n\t     << dendl;\n    mapping_job->abort();\n  }\n  if (!osdmap.get_pools().empty()) {\n    auto fin = new C_UpdateCreatingPGs(this, osdmap.get_epoch());\n    mapping_job = mapping.start_update(osdmap, mapper,\n\t\t\t\t       g_conf->mon_osd_mapping_pgs_per_chunk);\n    dout(10) << __func__ << \" started mapping job \" << mapping_job.get()\n\t     << \" at \" << fin->start << dendl;\n    mapping_job->set_finish_event(fin);\n  } else {\n    dout(10) << __func__ << \" no pools, no mapping job\" << dendl;\n    mapping_job = nullptr;\n  }\n}\n\nvoid OSDMonitor::update_msgr_features()\n{\n  set<int> types;\n  types.insert((int)entity_name_t::TYPE_OSD);\n  types.insert((int)entity_name_t::TYPE_CLIENT);\n  types.insert((int)entity_name_t::TYPE_MDS);\n  types.insert((int)entity_name_t::TYPE_MON);\n  for (set<int>::iterator q = types.begin(); q != types.end(); ++q) {\n    uint64_t mask;\n    uint64_t features = osdmap.get_features(*q, &mask);\n    if ((mon->messenger->get_policy(*q).features_required & mask) != features) {\n      dout(0) << \"crush map has features \" << features << \", adjusting msgr requires\" << dendl;\n      ceph::net::Policy p = mon->messenger->get_policy(*q);\n      p.features_required = (p.features_required & ~mask) | features;\n      mon->messenger->set_policy(*q, p);\n    }\n  }\n}\n\nvoid OSDMonitor::on_active()\n{\n  update_logger();\n\n  if (mon->is_leader()) {\n    mon->clog->debug() << \"osdmap \" << osdmap;\n  } else {\n    list<MonOpRequestRef> ls;\n    take_all_failures(ls);\n    while (!ls.empty()) {\n      MonOpRequestRef op = ls.front();\n      op->mark_osdmon_event(__func__);\n      dispatch(op);\n      ls.pop_front();\n    }\n  }\n  start_mapping();\n}\n\nvoid OSDMonitor::on_restart()\n{\n  last_osd_report.clear();\n}\n\nvoid OSDMonitor::on_shutdown()\n{\n  dout(10) << __func__ << dendl;\n  if (mapping_job) {\n    dout(10) << __func__ << \" canceling previous mapping_job \" << mapping_job.get()\n\t     << dendl;\n    mapping_job->abort();\n  }\n\n  // discard failure info, waiters\n  list<MonOpRequestRef> ls;\n  take_all_failures(ls);\n  ls.clear();\n}\n\nvoid OSDMonitor::update_logger()\n{\n  dout(10) << \"update_logger\" << dendl;\n\n  mon->cluster_logger->set(l_cluster_num_osd, osdmap.get_num_osds());\n  mon->cluster_logger->set(l_cluster_num_osd_up, osdmap.get_num_up_osds());\n  mon->cluster_logger->set(l_cluster_num_osd_in, osdmap.get_num_in_osds());\n  mon->cluster_logger->set(l_cluster_osd_epoch, osdmap.get_epoch());\n}\n\nvoid OSDMonitor::create_pending()\n{\n  pending_inc = OSDMap::Incremental(osdmap.epoch+1);\n  pending_inc.fsid = mon->monmap->fsid;\n  pending_metadata.clear();\n  pending_metadata_rm.clear();\n\n  dout(10) << \"create_pending e \" << pending_inc.epoch << dendl;\n\n  // clean up pg_temp, primary_temp\n  OSDMap::clean_temps(cct, osdmap, &pending_inc);\n  dout(10) << \"create_pending  did clean_temps\" << dendl;\n\n  // safety checks (this shouldn't really happen)\n  {\n    if (osdmap.backfillfull_ratio <= 0) {\n      pending_inc.new_backfillfull_ratio = g_conf->mon_osd_backfillfull_ratio;\n      if (pending_inc.new_backfillfull_ratio > 1.0)\n\tpending_inc.new_backfillfull_ratio /= 100;\n      dout(1) << __func__ << \" setting backfillfull_ratio = \"\n\t      << pending_inc.new_backfillfull_ratio << dendl;\n    }\n    if (osdmap.full_ratio <= 0) {\n      pending_inc.new_full_ratio = g_conf->mon_osd_full_ratio;\n      if (pending_inc.new_full_ratio > 1.0)\n        pending_inc.new_full_ratio /= 100;\n      dout(1) << __func__ << \" setting full_ratio = \"\n\t      << pending_inc.new_full_ratio << dendl;\n    }\n    if (osdmap.nearfull_ratio <= 0) {\n      pending_inc.new_nearfull_ratio = g_conf->mon_osd_nearfull_ratio;\n      if (pending_inc.new_nearfull_ratio > 1.0)\n        pending_inc.new_nearfull_ratio /= 100;\n      dout(1) << __func__ << \" setting nearfull_ratio = \"\n\t      << pending_inc.new_nearfull_ratio << dendl;\n    }\n  }\n\n  // Rewrite CRUSH rule IDs if they are using legacy \"ruleset\"\n  // structure.\n  if (osdmap.crush->has_legacy_rule_ids()) {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    // First, for all pools, work out which rule they really used\n    // by resolving ruleset to rule.\n    for (const auto &i : osdmap.get_pools()) {\n      const auto pool_id = i.first;\n      const auto &pool = i.second;\n      int new_rule_id = newcrush.find_rule(pool.crush_rule,\n\t\t\t\t\t   pool.type, pool.size);\n\n      dout(1) << __func__ << \" rewriting pool \"\n\t      << osdmap.get_pool_name(pool_id) << \" crush ruleset \"\n\t      << pool.crush_rule << \" -> rule id \" << new_rule_id << dendl;\n      if (pending_inc.new_pools.count(pool_id) == 0) {\n\tpending_inc.new_pools[pool_id] = pool;\n      }\n      pending_inc.new_pools[pool_id].crush_rule = new_rule_id;\n    }\n\n    // Now, go ahead and renumber all the rules so that their\n    // rule_id field corresponds to their position in the array\n    auto old_to_new = newcrush.renumber_rules();\n    dout(1) << __func__ << \" Rewrote \" << old_to_new << \" crush IDs:\" << dendl;\n    for (const auto &i : old_to_new) {\n      dout(1) << __func__ << \" \" << i.first << \" -> \" << i.second << dendl;\n    }\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n  }\n}\n\ncreating_pgs_t\nOSDMonitor::update_pending_pgs(const OSDMap::Incremental& inc,\n\t\t\t       const OSDMap& nextmap)\n{\n  dout(10) << __func__ << dendl;\n  creating_pgs_t pending_creatings;\n  {\n    std::lock_guard<std::mutex> l(creating_pgs_lock);\n    pending_creatings = creating_pgs;\n  }\n  // check for new or old pools\n  if (pending_creatings.last_scan_epoch < inc.epoch) {\n    unsigned queued = 0;\n    queued += scan_for_creating_pgs(osdmap.get_pools(),\n\t\t\t\t    inc.old_pools,\n\t\t\t\t    inc.modified,\n\t\t\t\t    &pending_creatings);\n    queued += scan_for_creating_pgs(inc.new_pools,\n\t\t\t\t    inc.old_pools,\n\t\t\t\t    inc.modified,\n\t\t\t\t    &pending_creatings);\n    dout(10) << __func__ << \" \" << queued << \" pools queued\" << dendl;\n    for (auto deleted_pool : inc.old_pools) {\n      auto removed = pending_creatings.remove_pool(deleted_pool);\n      dout(10) << __func__ << \" \" << removed\n               << \" pg removed because containing pool deleted: \"\n               << deleted_pool << dendl;\n      last_epoch_clean.remove_pool(deleted_pool);\n    }\n    // pgmon updates its creating_pgs in check_osd_map() which is called by\n    // on_active() and check_osd_map() could be delayed if lease expires, so its\n    // creating_pgs could be stale in comparison with the one of osdmon. let's\n    // trim them here. otherwise, they will be added back after being erased.\n    unsigned removed = 0;\n    for (auto& pg : pending_created_pgs) {\n      dout(20) << __func__ << \" noting created pg \" << pg << dendl;\n      pending_creatings.created_pools.insert(pg.pool());\n      removed += pending_creatings.pgs.erase(pg);\n    }\n    pending_created_pgs.clear();\n    dout(10) << __func__ << \" \" << removed\n\t     << \" pgs removed because they're created\" << dendl;\n    pending_creatings.last_scan_epoch = osdmap.get_epoch();\n  }\n\n  // filter out any pgs that shouldn't exist.\n  {\n    auto i = pending_creatings.pgs.begin();\n    while (i != pending_creatings.pgs.end()) {\n      if (!nextmap.pg_exists(i->first)) {\n\tdout(10) << __func__ << \" removing pg \" << i->first\n\t\t << \" which should not exist\" << dendl;\n\ti = pending_creatings.pgs.erase(i);\n      } else {\n\t++i;\n      }\n    }\n  }\n\n  // process queue\n  unsigned max = std::max<int64_t>(1, g_conf->mon_osd_max_creating_pgs);\n  const auto total = pending_creatings.pgs.size();\n  while (pending_creatings.pgs.size() < max &&\n\t !pending_creatings.queue.empty()) {\n    auto p = pending_creatings.queue.begin();\n    int64_t poolid = p->first;\n    dout(10) << __func__ << \" pool \" << poolid\n\t     << \" created \" << p->second.created\n\t     << \" modified \" << p->second.modified\n\t     << \" [\" << p->second.start << \"-\" << p->second.end << \")\"\n\t     << dendl;\n    int n = std::min(max - pending_creatings.pgs.size(),\n\t\tp->second.end - p->second.start);\n    ps_t first = p->second.start;\n    ps_t end = first + n;\n    for (ps_t ps = first; ps < end; ++ps) {\n      const pg_t pgid{ps, static_cast<uint64_t>(poolid)};\n      // NOTE: use the *current* epoch as the PG creation epoch so that the\n      // OSD does not have to generate a long set of PastIntervals.\n      pending_creatings.pgs.emplace(pgid, make_pair(inc.epoch,\n\t\t\t\t\t\t    p->second.modified));\n      dout(10) << __func__ << \" adding \" << pgid << dendl;\n    }\n    p->second.start = end;\n    if (p->second.done()) {\n      dout(10) << __func__ << \" done with queue for \" << poolid << dendl;\n      pending_creatings.queue.erase(p);\n    } else {\n      dout(10) << __func__ << \" pool \" << poolid\n\t       << \" now [\" << p->second.start << \"-\" << p->second.end << \")\"\n\t       << dendl;\n    }\n  }\n  dout(10) << __func__ << \" queue remaining: \" << pending_creatings.queue.size()\n\t   << \" pools\" << dendl;\n  dout(10) << __func__\n\t   << \" \" << (pending_creatings.pgs.size() - total)\n\t   << \"/\" << pending_creatings.pgs.size()\n\t   << \" pgs added from queued pools\" << dendl;\n  return pending_creatings;\n}\n\nvoid OSDMonitor::maybe_prime_pg_temp()\n{\n  bool all = false;\n  if (pending_inc.crush.length()) {\n    dout(10) << __func__ << \" new crush map, all\" << dendl;\n    all = true;\n  }\n\n  if (!pending_inc.new_up_client.empty()) {\n    dout(10) << __func__ << \" new up osds, all\" << dendl;\n    all = true;\n  }\n\n  // check for interesting OSDs\n  set<int> osds;\n  for (auto p = pending_inc.new_state.begin();\n       !all && p != pending_inc.new_state.end();\n       ++p) {\n    if ((p->second & CEPH_OSD_UP) &&\n\tosdmap.is_up(p->first)) {\n      osds.insert(p->first);\n    }\n  }\n  for (map<int32_t,uint32_t>::iterator p = pending_inc.new_weight.begin();\n       !all && p != pending_inc.new_weight.end();\n       ++p) {\n    if (p->second < osdmap.get_weight(p->first)) {\n      // weight reduction\n      osds.insert(p->first);\n    } else {\n      dout(10) << __func__ << \" osd.\" << p->first << \" weight increase, all\"\n\t       << dendl;\n      all = true;\n    }\n  }\n\n  if (!all && osds.empty())\n    return;\n\n  if (!all) {\n    unsigned estimate =\n      mapping.get_osd_acting_pgs(*osds.begin()).size() * osds.size();\n    if (estimate > mapping.get_num_pgs() *\n\tg_conf->mon_osd_prime_pg_temp_max_estimate) {\n      dout(10) << __func__ << \" estimate \" << estimate << \" pgs on \"\n\t       << osds.size() << \" osds >= \"\n\t       << g_conf->mon_osd_prime_pg_temp_max_estimate << \" of total \"\n\t       << mapping.get_num_pgs() << \" pgs, all\"\n\t       << dendl;\n      all = true;\n    } else {\n      dout(10) << __func__ << \" estimate \" << estimate << \" pgs on \"\n\t       << osds.size() << \" osds\" << dendl;\n    }\n  }\n\n  OSDMap next;\n  next.deepish_copy_from(osdmap);\n  next.apply_incremental(pending_inc);\n\n  if (next.get_pools().empty()) {\n    dout(10) << __func__ << \" no pools, no pg_temp priming\" << dendl;\n  } else if (all) {\n    PrimeTempJob job(next, this);\n    mapper.queue(&job, g_conf->mon_osd_mapping_pgs_per_chunk);\n    if (job.wait_for(g_conf->mon_osd_prime_pg_temp_max_time)) {\n      dout(10) << __func__ << \" done in \" << job.get_duration() << dendl;\n    } else {\n      dout(10) << __func__ << \" did not finish in \"\n\t       << g_conf->mon_osd_prime_pg_temp_max_time\n\t       << \", stopping\" << dendl;\n      job.abort();\n    }\n  } else {\n    dout(10) << __func__ << \" \" << osds.size() << \" interesting osds\" << dendl;\n    utime_t stop = ceph_clock_now();\n    stop += g_conf->mon_osd_prime_pg_temp_max_time;\n    const int chunk = 1000;\n    int n = chunk;\n    std::unordered_set<pg_t> did_pgs;\n    for (auto osd : osds) {\n      auto& pgs = mapping.get_osd_acting_pgs(osd);\n      dout(20) << __func__ << \" osd.\" << osd << \" \" << pgs << dendl;\n      for (auto pgid : pgs) {\n\tif (!did_pgs.insert(pgid).second) {\n\t  continue;\n\t}\n\tprime_pg_temp(next, pgid);\n\tif (--n <= 0) {\n\t  n = chunk;\n\t  if (ceph_clock_now() > stop) {\n\t    dout(10) << __func__ << \" consumed more than \"\n\t\t     << g_conf->mon_osd_prime_pg_temp_max_time\n\t\t     << \" seconds, stopping\"\n\t\t     << dendl;\n\t    return;\n\t  }\n\t}\n      }\n    }\n  }\n}\n\nvoid OSDMonitor::prime_pg_temp(\n  const OSDMap& next,\n  pg_t pgid)\n{\n  // TODO: remove this creating_pgs direct access?\n  if (creating_pgs.pgs.count(pgid)) {\n    return;\n  }\n  if (!osdmap.pg_exists(pgid)) {\n    return;\n  }\n\n  vector<int> up, acting;\n  mapping.get(pgid, &up, nullptr, &acting, nullptr);\n\n  vector<int> next_up, next_acting;\n  int next_up_primary, next_acting_primary;\n  next.pg_to_up_acting_osds(pgid, &next_up, &next_up_primary,\n\t\t\t    &next_acting, &next_acting_primary);\n  if (acting == next_acting && next_up != next_acting)\n    return;  // no change since last epoch\n\n  if (acting.empty())\n    return;  // if previously empty now we can be no worse off\n  const pg_pool_t *pool = next.get_pg_pool(pgid.pool());\n  if (pool && acting.size() < pool->min_size)\n    return;  // can be no worse off than before\n\n  if (next_up == next_acting) {\n    acting.clear();\n    dout(20) << __func__ << \" next_up == next_acting now, clear pg_temp\"\n\t     << dendl;\n  }\n\n  dout(20) << __func__ << \" \" << pgid << \" \" << up << \"/\" << acting\n\t   << \" -> \" << next_up << \"/\" << next_acting\n\t   << \", priming \" << acting\n\t   << dendl;\n  {\n    Mutex::Locker l(prime_pg_temp_lock);\n    // do not touch a mapping if a change is pending\n    pending_inc.new_pg_temp.emplace(\n      pgid,\n      mempool::osdmap::vector<int>(acting.begin(), acting.end()));\n  }\n}\n\n/**\n * @note receiving a transaction in this function gives a fair amount of\n * freedom to the service implementation if it does need it. It shouldn't.\n */\nvoid OSDMonitor::encode_pending(MonitorDBStore::TransactionRef t)\n{\n  dout(10) << \"encode_pending e \" << pending_inc.epoch\n\t   << dendl;\n\n  if (do_prune(t)) {\n    dout(1) << __func__ << \" osdmap full prune encoded e\"\n            << pending_inc.epoch << dendl;\n  }\n\n  // finalize up pending_inc\n  pending_inc.modified = ceph_clock_now();\n\n  int r = pending_inc.propagate_snaps_to_tiers(cct, osdmap);\n  assert(r == 0);\n\n  if (mapping_job) {\n    if (!mapping_job->is_done()) {\n      dout(1) << __func__ << \" skipping prime_pg_temp; mapping job \"\n\t      << mapping_job.get() << \" did not complete, \"\n\t      << mapping_job->shards << \" left\" << dendl;\n      mapping_job->abort();\n    } else if (mapping.get_epoch() < osdmap.get_epoch()) {\n      dout(1) << __func__ << \" skipping prime_pg_temp; mapping job \"\n\t      << mapping_job.get() << \" is prior epoch \"\n\t      << mapping.get_epoch() << dendl;\n    } else {\n      if (g_conf->mon_osd_prime_pg_temp) {\n\tmaybe_prime_pg_temp();\n      }\n    } \n  } else if (g_conf->mon_osd_prime_pg_temp) {\n    dout(1) << __func__ << \" skipping prime_pg_temp; mapping job did not start\"\n\t    << dendl;\n  }\n  mapping_job.reset();\n\n  // ensure we don't have blank new_state updates.  these are interrpeted as\n  // CEPH_OSD_UP (and almost certainly not what we want!).\n  auto p = pending_inc.new_state.begin();\n  while (p != pending_inc.new_state.end()) {\n    if (p->second == 0) {\n      dout(10) << \"new_state for osd.\" << p->first << \" is 0, removing\" << dendl;\n      p = pending_inc.new_state.erase(p);\n    } else {\n      ++p;\n    }\n  }\n\n  {\n    OSDMap tmp;\n    tmp.deepish_copy_from(osdmap);\n    tmp.apply_incremental(pending_inc);\n\n    // remove any legacy osdmap nearfull/full flags\n    {\n      if (tmp.test_flag(CEPH_OSDMAP_FULL | CEPH_OSDMAP_NEARFULL)) {\n\tdout(10) << __func__ << \" clearing legacy osdmap nearfull/full flag\"\n\t\t << dendl;\n\tremove_flag(CEPH_OSDMAP_NEARFULL);\n\tremove_flag(CEPH_OSDMAP_FULL);\n      }\n    }\n    // collect which pools are currently affected by\n    // the near/backfill/full osd(s),\n    // and set per-pool near/backfill/full flag instead\n    set<int64_t> full_pool_ids;\n    set<int64_t> backfillfull_pool_ids;\n    set<int64_t> nearfull_pool_ids;\n    tmp.get_full_pools(cct,\n\t\t       &full_pool_ids,\n\t\t       &backfillfull_pool_ids,\n                         &nearfull_pool_ids);\n    if (full_pool_ids.empty() ||\n\tbackfillfull_pool_ids.empty() ||\n\tnearfull_pool_ids.empty()) {\n      // normal case - no nearfull, backfillfull or full osds\n        // try cancel any improper nearfull/backfillfull/full pool\n        // flags first\n      for (auto &pool: tmp.get_pools()) {\n\tauto p = pool.first;\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_NEARFULL) &&\n\t    nearfull_pool_ids.empty()) {\n\t  dout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t   << \"'s nearfull flag\" << dendl;\n\t  if (pending_inc.new_pools.count(p) == 0) {\n\t    // load original pool info first!\n\t    pending_inc.new_pools[p] = pool.second;\n\t  }\n\t  pending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_NEARFULL;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_BACKFILLFULL) &&\n\t    backfillfull_pool_ids.empty()) {\n\t  dout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t   << \"'s backfillfull flag\" << dendl;\n\t  if (pending_inc.new_pools.count(p) == 0) {\n\t    pending_inc.new_pools[p] = pool.second;\n\t  }\n\t  pending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_BACKFILLFULL;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL) &&\n\t    full_pool_ids.empty()) {\n\t  if (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n\t    // set by EQUOTA, skipping\n\t    continue;\n\t  }\n\t  dout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t   << \"'s full flag\" << dendl;\n\t  if (pending_inc.new_pools.count(p) == 0) {\n\t    pending_inc.new_pools[p] = pool.second;\n\t  }\n\t  pending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_FULL;\n\t}\n      }\n    }\n    if (!full_pool_ids.empty()) {\n      dout(10) << __func__ << \" marking pool(s) \" << full_pool_ids\n\t       << \" as full\" << dendl;\n      for (auto &p: full_pool_ids) {\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL)) {\n\t  continue;\n\t}\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = tmp.pools[p];\n\t}\n\tpending_inc.new_pools[p].flags |= pg_pool_t::FLAG_FULL;\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_BACKFILLFULL;\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_NEARFULL;\n      }\n      // cancel FLAG_FULL for pools which are no longer full too\n      for (auto &pool: tmp.get_pools()) {\n\tauto p = pool.first;\n\tif (full_pool_ids.count(p)) {\n\t  // skip pools we have just marked as full above\n\t  continue;\n\t}\n\tif (!tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL) ||\n\t    tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n\t  // don't touch if currently is not full\n\t  // or is running out of quota (and hence considered as full)\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t << \"'s full flag\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = pool.second;\n\t}\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_FULL;\n      }\n    }\n    if (!backfillfull_pool_ids.empty()) {\n      for (auto &p: backfillfull_pool_ids) {\n\tif (full_pool_ids.count(p)) {\n\t  // skip pools we have already considered as full above\n\t  continue;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n\t  // make sure FLAG_FULL is truly set, so we are safe not\n\t  // to set a extra (redundant) FLAG_BACKFILLFULL flag\n\t  assert(tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL));\n\t  continue;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_BACKFILLFULL)) {\n\t  // don't bother if pool is already marked as backfillfull\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" marking pool '\" << tmp.pool_name[p]\n\t\t << \"'s as backfillfull\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = tmp.pools[p];\n\t}\n\tpending_inc.new_pools[p].flags |= pg_pool_t::FLAG_BACKFILLFULL;\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_NEARFULL;\n      }\n      // cancel FLAG_BACKFILLFULL for pools\n      // which are no longer backfillfull too\n      for (auto &pool: tmp.get_pools()) {\n\tauto p = pool.first;\n\tif (full_pool_ids.count(p) || backfillfull_pool_ids.count(p)) {\n\t  // skip pools we have just marked as backfillfull/full above\n\t  continue;\n\t}\n\tif (!tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_BACKFILLFULL)) {\n\t  // and don't touch if currently is not backfillfull\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t << \"'s backfillfull flag\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = pool.second;\n\t}\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_BACKFILLFULL;\n      }\n    }\n    if (!nearfull_pool_ids.empty()) {\n      for (auto &p: nearfull_pool_ids) {\n\tif (full_pool_ids.count(p) || backfillfull_pool_ids.count(p)) {\n\t  continue;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n\t  // make sure FLAG_FULL is truly set, so we are safe not\n\t  // to set a extra (redundant) FLAG_NEARFULL flag\n\t  assert(tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_FULL));\n\t  continue;\n\t}\n\tif (tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_NEARFULL)) {\n\t  // don't bother if pool is already marked as nearfull\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" marking pool '\" << tmp.pool_name[p]\n\t\t << \"'s as nearfull\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = tmp.pools[p];\n\t}\n\tpending_inc.new_pools[p].flags |= pg_pool_t::FLAG_NEARFULL;\n      }\n      // cancel FLAG_NEARFULL for pools\n      // which are no longer nearfull too\n      for (auto &pool: tmp.get_pools()) {\n\tauto p = pool.first;\n\tif (full_pool_ids.count(p) ||\n\t    backfillfull_pool_ids.count(p) ||\n\t    nearfull_pool_ids.count(p)) {\n\t  // skip pools we have just marked as\n\t  // nearfull/backfillfull/full above\n\t  continue;\n\t}\n\tif (!tmp.get_pg_pool(p)->has_flag(pg_pool_t::FLAG_NEARFULL)) {\n\t  // and don't touch if currently is not nearfull\n\t  continue;\n\t}\n\tdout(10) << __func__ << \" clearing pool '\" << tmp.pool_name[p]\n\t\t << \"'s nearfull flag\" << dendl;\n\tif (pending_inc.new_pools.count(p) == 0) {\n\t  pending_inc.new_pools[p] = pool.second;\n\t}\n\tpending_inc.new_pools[p].flags &= ~pg_pool_t::FLAG_NEARFULL;\n      }\n    }\n\n    // min_compat_client?\n    if (tmp.require_min_compat_client == 0) {\n      auto mv = tmp.get_min_compat_client();\n      dout(1) << __func__ << \" setting require_min_compat_client to currently \"\n\t      << \"required \" << ceph_release_name(mv) << dendl;\n      mon->clog->info() << \"setting require_min_compat_client to currently \"\n\t\t\t<< \"required \" << ceph_release_name(mv);\n      pending_inc.new_require_min_compat_client = mv;\n    }\n\n    // upgrade to mimic?\n    if (osdmap.require_osd_release < CEPH_RELEASE_MIMIC &&\n\ttmp.require_osd_release >= CEPH_RELEASE_MIMIC) {\n      dout(10) << __func__ << \" first mimic+ epoch\" << dendl;\n      // record this epoch as the deletion for all legacy removed_snaps\n      for (auto& p : tmp.get_pools()) {\n\t// update every pool\n\tif (pending_inc.new_pools.count(p.first) == 0) {\n\t  pending_inc.new_pools[p.first] = p.second;\n\t}\n\tauto& pi = pending_inc.new_pools[p.first];\n\tif (pi.snap_seq == 0) {\n\t  // no snaps on this pool\n\t  continue;\n\t}\n\tif ((pi.flags & (pg_pool_t::FLAG_SELFMANAGED_SNAPS |\n\t\t\t pg_pool_t::FLAG_POOL_SNAPS)) == 0) {\n\t  if (!pi.removed_snaps.empty()) {\n\t    pi.flags |= pg_pool_t::FLAG_SELFMANAGED_SNAPS;\n\t  } else {\n\t    pi.flags |= pg_pool_t::FLAG_POOL_SNAPS;\n\t  }\n\t}\n\n\t// Make all previously removed snaps appear to be removed in this\n\t// epoch.  this populates removed_snaps_queue.  The OSD will subtract\n\t// off its purged_snaps, as before, and this set will shrink over the\n\t// following epochs as the purged snaps are reported back through the\n\t// mgr.\n\tOSDMap::snap_interval_set_t removed;\n\tif (!p.second.removed_snaps.empty()) {\n\t  // different flavor of interval_set :(\n\t  for (auto q = p.second.removed_snaps.begin();\n\t       q != p.second.removed_snaps.end();\n\t       ++q) {\n\t    removed.insert(q.get_start(), q.get_len());\n\t  }\n\t} else {\n\t  for (snapid_t s = 1; s <= pi.get_snap_seq(); s = s + 1) {\n\t    if (pi.snaps.count(s) == 0) {\n\t      removed.insert(s);\n\t    }\n\t  }\n\t}\n\tpending_inc.new_removed_snaps[p.first].union_of(removed);\n\n\tdout(10) << __func__ << \" converting pool \" << p.first\n\t\t << \" with \" << p.second.removed_snaps.size()\n\t\t << \" legacy removed_snaps\" << dendl;\n\tstring k = make_snap_epoch_key(p.first, pending_inc.epoch);\n\tbufferlist v;\n\tencode(p.second.removed_snaps, v);\n\tt->put(OSD_SNAP_PREFIX, k, v);\n\tfor (auto q = p.second.removed_snaps.begin();\n\t     q != p.second.removed_snaps.end();\n\t     ++q) {\n\t  bufferlist v;\n\t  string k = make_snap_key_value(p.first, q.get_start(),\n\t\t\t\t\t q.get_len(), pending_inc.epoch, &v);\n\t  t->put(OSD_SNAP_PREFIX, k, v);\n\t}\n      }\n    }\n    if (osdmap.require_osd_release < CEPH_RELEASE_NAUTILUS &&\n\ttmp.require_osd_release >= CEPH_RELEASE_NAUTILUS) {\n      dout(10) << __func__ << \" first nautilus+ epoch\" << dendl;\n    }\n  }\n\n  // tell me about it\n  for (auto i = pending_inc.new_state.begin();\n       i != pending_inc.new_state.end();\n       ++i) {\n    int s = i->second ? i->second : CEPH_OSD_UP;\n    if (s & CEPH_OSD_UP)\n      dout(2) << \" osd.\" << i->first << \" DOWN\" << dendl;\n    if (s & CEPH_OSD_EXISTS)\n      dout(2) << \" osd.\" << i->first << \" DNE\" << dendl;\n  }\n  for (auto i = pending_inc.new_up_client.begin();\n       i != pending_inc.new_up_client.end();\n       ++i) {\n    //FIXME: insert cluster addresses too\n    dout(2) << \" osd.\" << i->first << \" UP \" << i->second << dendl;\n  }\n  for (map<int32_t,uint32_t>::iterator i = pending_inc.new_weight.begin();\n       i != pending_inc.new_weight.end();\n       ++i) {\n    if (i->second == CEPH_OSD_OUT) {\n      dout(2) << \" osd.\" << i->first << \" OUT\" << dendl;\n    } else if (i->second == CEPH_OSD_IN) {\n      dout(2) << \" osd.\" << i->first << \" IN\" << dendl;\n    } else {\n      dout(2) << \" osd.\" << i->first << \" WEIGHT \" << hex << i->second << dec << dendl;\n    }\n  }\n\n  // clean inappropriate pg_upmap/pg_upmap_items (if any)\n  osdmap.maybe_remove_pg_upmaps(cct, osdmap, &pending_inc);\n\n  // features for osdmap and its incremental\n  uint64_t features;\n\n  // encode full map and determine its crc\n  OSDMap tmp;\n  {\n    tmp.deepish_copy_from(osdmap);\n    tmp.apply_incremental(pending_inc);\n\n    // determine appropriate features\n    features = tmp.get_encoding_features();\n    dout(10) << __func__ << \" encoding full map with \"\n\t     << ceph_release_name(tmp.require_osd_release)\n\t     << \" features \" << features << dendl;\n\n    // the features should be a subset of the mon quorum's features!\n    assert((features & ~mon->get_quorum_con_features()) == 0);\n\n    bufferlist fullbl;\n    encode(tmp, fullbl, features | CEPH_FEATURE_RESERVED);\n    pending_inc.full_crc = tmp.get_crc();\n\n    // include full map in the txn.  note that old monitors will\n    // overwrite this.  new ones will now skip the local full map\n    // encode and reload from this.\n    put_version_full(t, pending_inc.epoch, fullbl);\n  }\n\n  // encode\n  assert(get_last_committed() + 1 == pending_inc.epoch);\n  bufferlist bl;\n  encode(pending_inc, bl, features | CEPH_FEATURE_RESERVED);\n\n  dout(20) << \" full_crc \" << tmp.get_crc()\n\t   << \" inc_crc \" << pending_inc.inc_crc << dendl;\n\n  /* put everything in the transaction */\n  put_version(t, pending_inc.epoch, bl);\n  put_last_committed(t, pending_inc.epoch);\n\n  // metadata, too!\n  for (map<int,bufferlist>::iterator p = pending_metadata.begin();\n       p != pending_metadata.end();\n       ++p)\n    t->put(OSD_METADATA_PREFIX, stringify(p->first), p->second);\n  for (set<int>::iterator p = pending_metadata_rm.begin();\n       p != pending_metadata_rm.end();\n       ++p)\n    t->erase(OSD_METADATA_PREFIX, stringify(*p));\n  pending_metadata.clear();\n  pending_metadata_rm.clear();\n\n  // and pg creating, also!\n  auto pending_creatings = update_pending_pgs(pending_inc, tmp);\n  bufferlist creatings_bl;\n  encode(pending_creatings, creatings_bl);\n  t->put(OSD_PG_CREATING_PREFIX, \"creating\", creatings_bl);\n\n  // removed_snaps\n  if (tmp.require_osd_release >= CEPH_RELEASE_MIMIC) {\n    for (auto& i : pending_inc.new_removed_snaps) {\n      {\n\t// all snaps removed this epoch\n\tstring k = make_snap_epoch_key(i.first, pending_inc.epoch);\n\tbufferlist v;\n\tencode(i.second, v);\n\tt->put(OSD_SNAP_PREFIX, k, v);\n      }\n      for (auto q = i.second.begin();\n\t   q != i.second.end();\n\t   ++q) {\n\tbufferlist v;\n\tstring k = make_snap_key_value(i.first, q.get_start(),\n\t\t\t\t       q.get_len(), pending_inc.epoch, &v);\n\tt->put(OSD_SNAP_PREFIX, k, v);\n      }\n    }\n    for (auto& i : pending_inc.new_purged_snaps) {\n      for (auto q = i.second.begin();\n\t   q != i.second.end();\n\t   ++q) {\n\tbufferlist v;\n\tstring k = make_snap_purged_key_value(i.first, q.get_start(),\n\t\t\t\t\t      q.get_len(), pending_inc.epoch,\n\t\t\t\t\t      &v);\n\tt->put(OSD_SNAP_PREFIX, k, v);\n      }\n    }\n  }\n\n  // health\n  health_check_map_t next;\n  tmp.check_health(&next);\n  encode_health(next, t);\n}\n\nint OSDMonitor::load_metadata(int osd, map<string, string>& m, ostream *err)\n{\n  bufferlist bl;\n  int r = mon->store->get(OSD_METADATA_PREFIX, stringify(osd), bl);\n  if (r < 0)\n    return r;\n  try {\n    auto p = bl.cbegin();\n    decode(m, p);\n  }\n  catch (buffer::error& e) {\n    if (err)\n      *err << \"osd.\" << osd << \" metadata is corrupt\";\n    return -EIO;\n  }\n  return 0;\n}\n\nvoid OSDMonitor::count_metadata(const string& field, map<string,int> *out)\n{\n  for (int osd = 0; osd < osdmap.get_max_osd(); ++osd) {\n    if (osdmap.is_up(osd)) {\n      map<string,string> meta;\n      load_metadata(osd, meta, nullptr);\n      auto p = meta.find(field);\n      if (p == meta.end()) {\n\t(*out)[\"unknown\"]++;\n      } else {\n\t(*out)[p->second]++;\n      }\n    }\n  }\n}\n\nvoid OSDMonitor::count_metadata(const string& field, Formatter *f)\n{\n  map<string,int> by_val;\n  count_metadata(field, &by_val);\n  f->open_object_section(field.c_str());\n  for (auto& p : by_val) {\n    f->dump_int(p.first.c_str(), p.second);\n  }\n  f->close_section();\n}\n\nint OSDMonitor::get_osd_objectstore_type(int osd, string *type)\n{\n  map<string, string> metadata;\n  int r = load_metadata(osd, metadata, nullptr);\n  if (r < 0)\n    return r;\n\n  auto it = metadata.find(\"osd_objectstore\");\n  if (it == metadata.end())\n    return -ENOENT;\n  *type = it->second;\n  return 0;\n}\n\nbool OSDMonitor::is_pool_currently_all_bluestore(int64_t pool_id,\n\t\t\t\t\t\t const pg_pool_t &pool,\n\t\t\t\t\t\t ostream *err)\n{\n  // just check a few pgs for efficiency - this can't give a guarantee anyway,\n  // since filestore osds could always join the pool later\n  set<int> checked_osds;\n  for (unsigned ps = 0; ps < std::min(8u, pool.get_pg_num()); ++ps) {\n    vector<int> up, acting;\n    pg_t pgid(ps, pool_id);\n    osdmap.pg_to_up_acting_osds(pgid, up, acting);\n    for (int osd : up) {\n      if (checked_osds.find(osd) != checked_osds.end())\n\tcontinue;\n      string objectstore_type;\n      int r = get_osd_objectstore_type(osd, &objectstore_type);\n      // allow with missing metadata, e.g. due to an osd never booting yet\n      if (r < 0 || objectstore_type == \"bluestore\") {\n\tchecked_osds.insert(osd);\n\tcontinue;\n      }\n      *err << \"osd.\" << osd << \" uses \" << objectstore_type;\n      return false;\n    }\n  }\n  return true;\n}\n\nint OSDMonitor::dump_osd_metadata(int osd, Formatter *f, ostream *err)\n{\n  map<string,string> m;\n  if (int r = load_metadata(osd, m, err))\n    return r;\n  for (map<string,string>::iterator p = m.begin(); p != m.end(); ++p)\n    f->dump_string(p->first.c_str(), p->second);\n  return 0;\n}\n\nvoid OSDMonitor::print_nodes(Formatter *f)\n{\n  // group OSDs by their hosts\n  map<string, list<int> > osds; // hostname => osd\n  for (int osd = 0; osd < osdmap.get_max_osd(); osd++) {\n    map<string, string> m;\n    if (load_metadata(osd, m, NULL)) {\n      continue;\n    }\n    map<string, string>::iterator hostname = m.find(\"hostname\");\n    if (hostname == m.end()) {\n      // not likely though\n      continue;\n    }\n    osds[hostname->second].push_back(osd);\n  }\n\n  dump_services(f, osds, \"osd\");\n}\n\nvoid OSDMonitor::share_map_with_random_osd()\n{\n  if (osdmap.get_num_up_osds() == 0) {\n    dout(10) << __func__ << \" no up osds, don't share with anyone\" << dendl;\n    return;\n  }\n\n  MonSession *s = mon->session_map.get_random_osd_session(&osdmap);\n  if (!s) {\n    dout(10) << __func__ << \" no up osd on our session map\" << dendl;\n    return;\n  }\n\n  dout(10) << \"committed, telling random \" << s->name\n\t   << \" all about it\" << dendl;\n\n  // get feature of the peer\n  // use quorum_con_features, if it's an anonymous connection.\n  uint64_t features = s->con_features ? s->con_features :\n                                        mon->get_quorum_con_features();\n  // whatev, they'll request more if they need it\n  MOSDMap *m = build_incremental(osdmap.get_epoch() - 1, osdmap.get_epoch(), features);\n  s->con->send_message(m);\n  // NOTE: do *not* record osd has up to this epoch (as we do\n  // elsewhere) as they may still need to request older values.\n}\n\nversion_t OSDMonitor::get_trim_to() const\n{\n  if (mon->get_quorum().empty()) {\n    dout(10) << __func__ << \": quorum not formed\" << dendl;\n    return 0;\n  }\n\n  {\n    std::lock_guard<std::mutex> l(creating_pgs_lock);\n    if (!creating_pgs.pgs.empty()) {\n      return 0;\n    }\n  }\n\n  if (g_conf->get_val<bool>(\"mon_debug_block_osdmap_trim\")) {\n    dout(0) << __func__\n            << \" blocking osdmap trim\"\n               \" ('mon_debug_block_osdmap_trim' set to 'true')\"\n            << dendl;\n    return 0;\n  }\n\n  {\n    epoch_t floor = get_min_last_epoch_clean();\n    dout(10) << \" min_last_epoch_clean \" << floor << dendl;\n    if (g_conf->mon_osd_force_trim_to > 0 &&\n\tg_conf->mon_osd_force_trim_to < (int)get_last_committed()) {\n      floor = g_conf->mon_osd_force_trim_to;\n      dout(10) << \" explicit mon_osd_force_trim_to = \" << floor << dendl;\n    }\n    unsigned min = g_conf->mon_min_osdmap_epochs;\n    if (floor + min > get_last_committed()) {\n      if (min < get_last_committed())\n\tfloor = get_last_committed() - min;\n      else\n\tfloor = 0;\n    }\n    if (floor > get_first_committed())\n      return floor;\n  }\n  return 0;\n}\n\nepoch_t OSDMonitor::get_min_last_epoch_clean() const\n{\n  auto floor = last_epoch_clean.get_lower_bound(osdmap);\n  // also scan osd epochs\n  // don't trim past the oldest reported osd epoch\n  for (auto& osd_epoch : osd_epochs) {\n    if (osd_epoch.second < floor) {\n      floor = osd_epoch.second;\n    }\n  }\n  return floor;\n}\n\nvoid OSDMonitor::encode_trim_extra(MonitorDBStore::TransactionRef tx,\n\t\t\t\t   version_t first)\n{\n  dout(10) << __func__ << \" including full map for e \" << first << dendl;\n  bufferlist bl;\n  get_version_full(first, bl);\n  put_version_full(tx, first, bl);\n\n  if (has_osdmap_manifest &&\n      first > osdmap_manifest.get_first_pinned()) {\n    _prune_update_trimmed(tx, first);\n  }\n}\n\n\n/* full osdmap prune\n *\n * for more information, please refer to doc/dev/mon-osdmap-prune.rst\n */\n\nvoid OSDMonitor::load_osdmap_manifest()\n{\n  bool store_has_manifest =\n    mon->store->exists(get_service_name(), \"osdmap_manifest\");\n\n  if (!store_has_manifest) {\n    if (!has_osdmap_manifest) {\n      return;\n    }\n\n    dout(20) << __func__\n             << \" dropping osdmap manifest from memory.\" << dendl;\n    osdmap_manifest = osdmap_manifest_t();\n    has_osdmap_manifest = false;\n    return;\n  }\n\n  dout(20) << __func__\n           << \" osdmap manifest detected in store; reload.\" << dendl;\n\n  bufferlist manifest_bl;\n  int r = get_value(\"osdmap_manifest\", manifest_bl);\n  if (r < 0) {\n    derr << __func__ << \" unable to read osdmap version manifest\" << dendl;\n    ceph_assert(0 == \"error reading manifest\");\n  }\n  osdmap_manifest.decode(manifest_bl);\n  has_osdmap_manifest = true;\n\n  dout(10) << __func__ << \" store osdmap manifest pinned (\"\n           << osdmap_manifest.get_first_pinned()\n           << \" .. \"\n           << osdmap_manifest.get_last_pinned()\n           << \")\"\n           << dendl;\n}\n\nbool OSDMonitor::should_prune() const\n{\n  version_t first = get_first_committed();\n  version_t last = get_last_committed();\n  version_t min_osdmap_epochs =\n    g_conf->get_val<int64_t>(\"mon_min_osdmap_epochs\");\n  version_t prune_min =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_min\");\n  version_t prune_interval =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_interval\");\n  version_t last_pinned = osdmap_manifest.get_last_pinned();\n  version_t last_to_pin = last - min_osdmap_epochs;\n\n  // Make it or break it constraints.\n  //\n  // If any of these conditions fails, we will not prune, regardless of\n  // whether we have an on-disk manifest with an on-going pruning state.\n  //\n  if ((last - first) <= min_osdmap_epochs) {\n    // between the first and last committed epochs, we don't have\n    // enough epochs to trim, much less to prune.\n    dout(10) << __func__\n             << \" currently holding only \" << (last - first)\n             << \" epochs (min osdmap epochs: \" << min_osdmap_epochs\n             << \"); do not prune.\"\n             << dendl;\n    return false;\n\n  } else if ((last_to_pin - first) < prune_min) {\n    // between the first committed epoch and the last epoch we would prune,\n    // we simply don't have enough versions over the minimum to prune maps.\n    dout(10) << __func__\n             << \" could only prune \" << (last_to_pin - first)\n             << \" epochs (\" << first << \"..\" << last_to_pin << \"), which\"\n                \" is less than the required minimum (\" << prune_min << \")\"\n             << dendl;\n    return false;\n\n  } else if (has_osdmap_manifest && last_pinned >= last_to_pin) {\n    dout(10) << __func__\n             << \" we have pruned as far as we can; do not prune.\"\n             << dendl;\n    return false;\n\n  } else if (last_pinned + prune_interval > last_to_pin) {\n    dout(10) << __func__\n             << \" not enough epochs to form an interval (last pinned: \"\n             << last_pinned << \", last to pin: \"\n             << last_to_pin << \", interval: \" << prune_interval << \")\"\n             << dendl;\n    return false;\n  }\n\n  dout(15) << __func__\n           << \" should prune (\" << last_pinned << \"..\" << last_to_pin << \")\"\n           << \" lc (\" << first << \"..\" << last << \")\"\n           << dendl;\n  return true;\n}\n\nvoid OSDMonitor::_prune_update_trimmed(\n    MonitorDBStore::TransactionRef tx,\n    version_t first)\n{\n  dout(10) << __func__\n           << \" first \" << first\n           << \" last_pinned \" << osdmap_manifest.get_last_pinned()\n           << \" last_pinned \" << osdmap_manifest.get_last_pinned()\n           << dendl;\n\n  if (!osdmap_manifest.is_pinned(first)) {\n    osdmap_manifest.pin(first);\n  }\n\n  set<version_t>::iterator p_end = osdmap_manifest.pinned.find(first);\n  set<version_t>::iterator p = osdmap_manifest.pinned.begin();\n  osdmap_manifest.pinned.erase(p, p_end);\n  ceph_assert(osdmap_manifest.get_first_pinned() == first);\n\n  if (osdmap_manifest.get_last_pinned() == first+1 ||\n      osdmap_manifest.pinned.size() == 1) {\n    // we reached the end of the line, as pinned maps go; clean up our\n    // manifest, and let `should_prune()` decide whether we should prune\n    // again.\n    tx->erase(get_service_name(), \"osdmap_manifest\");\n    return;\n  }\n\n  bufferlist bl;\n  osdmap_manifest.encode(bl);\n  tx->put(get_service_name(), \"osdmap_manifest\", bl);\n}\n\nvoid OSDMonitor::prune_init()\n{\n  dout(1) << __func__ << dendl;\n\n  version_t pin_first;\n\n  if (!has_osdmap_manifest) {\n    // we must have never pruned, OR if we pruned the state must no longer\n    // be relevant (i.e., the state must have been removed alongside with\n    // the trim that *must* have removed past the last pinned map in a\n    // previous prune).\n    ceph_assert(osdmap_manifest.pinned.empty());\n    ceph_assert(!mon->store->exists(get_service_name(), \"osdmap_manifest\"));\n    pin_first = get_first_committed();\n\n  } else {\n    // we must have pruned in the past AND its state is still relevant\n    // (i.e., even if we trimmed, we still hold pinned maps in the manifest,\n    // and thus we still hold a manifest in the store).\n    ceph_assert(!osdmap_manifest.pinned.empty());\n    ceph_assert(osdmap_manifest.get_first_pinned() == get_first_committed());\n    ceph_assert(osdmap_manifest.get_last_pinned() < get_last_committed());\n\n    dout(10) << __func__\n             << \" first_pinned \" << osdmap_manifest.get_first_pinned()\n             << \" last_pinned \" << osdmap_manifest.get_last_pinned()\n             << dendl;\n\n    pin_first = osdmap_manifest.get_last_pinned();\n  }\n\n  osdmap_manifest.pin(pin_first);\n}\n\nbool OSDMonitor::_prune_sanitize_options() const\n{\n  uint64_t prune_interval =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_interval\");\n  uint64_t prune_min =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_min\");\n  uint64_t txsize =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_txsize\");\n\n  bool r = true;\n\n  if (prune_interval == 0) {\n    derr << __func__\n         << \" prune is enabled BUT prune interval is zero; abort.\"\n         << dendl;\n    r = false;\n  } else if (prune_interval == 1) {\n    derr << __func__\n         << \" prune interval is equal to one, which essentially means\"\n            \" no pruning; abort.\"\n         << dendl;\n    r = false;\n  }\n  if (prune_min == 0) {\n    derr << __func__\n         << \" prune is enabled BUT prune min is zero; abort.\"\n         << dendl;\n    r = false;\n  }\n  if (prune_interval > prune_min) {\n    derr << __func__\n         << \" impossible to ascertain proper prune interval because\"\n         << \" it is greater than the minimum prune epochs\"\n         << \" (min: \" << prune_min << \", interval: \" << prune_interval << \")\"\n         << dendl;\n    r = false;\n  }\n\n  if (txsize < prune_interval - 1) {\n    derr << __func__\n         << \"'mon_osdmap_full_prune_txsize' (\" << txsize\n         << \") < 'mon_osdmap_full_prune_interval-1' (\" << prune_interval - 1\n         << \"); abort.\" << dendl;\n    r = false;\n  }\n  return r;\n}\n\nbool OSDMonitor::is_prune_enabled() const {\n  return g_conf->get_val<bool>(\"mon_osdmap_full_prune_enabled\");\n}\n\nbool OSDMonitor::is_prune_supported() const {\n  return mon->get_required_mon_features().contains_any(\n      ceph::features::mon::FEATURE_OSDMAP_PRUNE);\n}\n\n/** do_prune\n *\n * @returns true if has side-effects; false otherwise.\n */\nbool OSDMonitor::do_prune(MonitorDBStore::TransactionRef tx)\n{\n  bool enabled = is_prune_enabled();\n\n  dout(1) << __func__ << \" osdmap full prune \"\n          << ( enabled ? \"enabled\" : \"disabled\")\n          << dendl;\n\n  if (!enabled || !_prune_sanitize_options() || !should_prune()) {\n    return false;\n  }\n\n  // we are beyond the minimum prune versions, we need to remove maps because\n  // otherwise the store will grow unbounded and we may end up having issues\n  // with available disk space or store hangs.\n\n  // we will not pin all versions. We will leave a buffer number of versions.\n  // this allows us the monitor to trim maps without caring too much about\n  // pinned maps, and then allow us to use another ceph-mon without these\n  // capabilities, without having to repair the store.\n\n  version_t first = get_first_committed();\n  version_t last = get_last_committed();\n\n  version_t last_to_pin = last - g_conf->mon_min_osdmap_epochs;\n  version_t last_pinned = osdmap_manifest.get_last_pinned();\n  uint64_t prune_interval =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_interval\");\n  uint64_t txsize =\n    g_conf->get_val<uint64_t>(\"mon_osdmap_full_prune_txsize\");\n\n  prune_init();\n\n  // we need to get rid of some osdmaps\n\n  dout(5) << __func__\n          << \" lc (\" << first << \" .. \" << last << \")\"\n          << \" last_pinned \" << last_pinned\n          << \" interval \" << prune_interval\n          << \" last_to_pin \" << last_to_pin\n          << dendl;\n\n  // We will be erasing maps as we go.\n  //\n  // We will erase all maps between `last_pinned` and the `next_to_pin`.\n  //\n  // If `next_to_pin` happens to be greater than `last_to_pin`, then\n  // we stop pruning. We could prune the maps between `next_to_pin` and\n  // `last_to_pin`, but by not doing it we end up with neater pruned\n  // intervals, aligned with `prune_interval`. Besides, this should not be a\n  // problem as long as `prune_interval` is set to a sane value, instead of\n  // hundreds or thousands of maps.\n\n  auto map_exists = [this](version_t v) {\n    string k = mon->store->combine_strings(\"full\", v);\n    return mon->store->exists(get_service_name(), k);\n  };\n\n  // 'interval' represents the number of maps from the last pinned\n  // i.e., if we pinned version 1 and have an interval of 10, we're pinning\n  // version 11 next; all intermediate versions will be removed.\n  //\n  // 'txsize' represents the maximum number of versions we'll be removing in\n  // this iteration. If 'txsize' is large enough to perform multiple passes\n  // pinning and removing maps, we will do so; if not, we'll do at least one\n  // pass. We are quite relaxed about honouring 'txsize', but we'll always\n  // ensure that we never go *over* the maximum.\n\n  // e.g., if we pin 1 and 11, we're removing versions [2..10]; i.e., 9 maps.\n  uint64_t removal_interval = prune_interval - 1;\n\n  if (txsize < removal_interval) {\n    dout(5) << __func__\n\t    << \" setting txsize to removal interval size (\"\n\t    << removal_interval << \" versions\"\n\t    << dendl;\n    txsize = removal_interval;\n  }\n  ceph_assert(removal_interval > 0);\n\n  uint64_t num_pruned = 0;\n  while (num_pruned + removal_interval <= txsize) { \n    last_pinned = osdmap_manifest.get_last_pinned();\n\n    if (last_pinned + prune_interval > last_to_pin) {\n      break;\n    }\n    ceph_assert(last_pinned < last_to_pin);\n\n    version_t next_pinned = last_pinned + prune_interval;\n    ceph_assert(next_pinned <= last_to_pin);\n    osdmap_manifest.pin(next_pinned);\n\n    dout(20) << __func__\n\t     << \" last_pinned \" << last_pinned\n\t     << \" next_pinned \" << next_pinned\n\t     << \" num_pruned \" << num_pruned\n\t     << \" removal interval (\" << (last_pinned+1)\n\t     << \"..\" << (next_pinned-1) << \")\"\n\t     << \" txsize \" << txsize << dendl;\n\n    ceph_assert(map_exists(last_pinned));\n    ceph_assert(map_exists(next_pinned));\n\n    for (version_t v = last_pinned+1; v < next_pinned; ++v) {\n      ceph_assert(!osdmap_manifest.is_pinned(v));\n\n      dout(20) << __func__ << \"   pruning full osdmap e\" << v << dendl;\n      string full_key = mon->store->combine_strings(\"full\", v);\n      tx->erase(get_service_name(), full_key);\n      ++num_pruned;\n    }\n  }\n\n  ceph_assert(num_pruned > 0);\n\n  bufferlist bl;\n  osdmap_manifest.encode(bl);\n  tx->put(get_service_name(), \"osdmap_manifest\", bl);\n\n  return true;\n}\n\n\n// -------------\n\nbool OSDMonitor::preprocess_query(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  Message *m = op->get_req();\n  dout(10) << \"preprocess_query \" << *m << \" from \" << m->get_orig_source_inst() << dendl;\n\n  switch (m->get_type()) {\n    // READs\n  case MSG_MON_COMMAND:\n    return preprocess_command(op);\n  case CEPH_MSG_MON_GET_OSDMAP:\n    return preprocess_get_osdmap(op);\n\n    // damp updates\n  case MSG_OSD_MARK_ME_DOWN:\n    return preprocess_mark_me_down(op);\n  case MSG_OSD_FULL:\n    return preprocess_full(op);\n  case MSG_OSD_FAILURE:\n    return preprocess_failure(op);\n  case MSG_OSD_BOOT:\n    return preprocess_boot(op);\n  case MSG_OSD_ALIVE:\n    return preprocess_alive(op);\n  case MSG_OSD_PG_CREATED:\n    return preprocess_pg_created(op);\n  case MSG_OSD_PGTEMP:\n    return preprocess_pgtemp(op);\n  case MSG_OSD_BEACON:\n    return preprocess_beacon(op);\n\n  case CEPH_MSG_POOLOP:\n    return preprocess_pool_op(op);\n\n  case MSG_REMOVE_SNAPS:\n    return preprocess_remove_snaps(op);\n\n  default:\n    ceph_abort();\n    return true;\n  }\n}\n\nbool OSDMonitor::prepare_update(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  Message *m = op->get_req();\n  dout(7) << \"prepare_update \" << *m << \" from \" << m->get_orig_source_inst() << dendl;\n\n  switch (m->get_type()) {\n    // damp updates\n  case MSG_OSD_MARK_ME_DOWN:\n    return prepare_mark_me_down(op);\n  case MSG_OSD_FULL:\n    return prepare_full(op);\n  case MSG_OSD_FAILURE:\n    return prepare_failure(op);\n  case MSG_OSD_BOOT:\n    return prepare_boot(op);\n  case MSG_OSD_ALIVE:\n    return prepare_alive(op);\n  case MSG_OSD_PG_CREATED:\n    return prepare_pg_created(op);\n  case MSG_OSD_PGTEMP:\n    return prepare_pgtemp(op);\n  case MSG_OSD_BEACON:\n    return prepare_beacon(op);\n\n  case MSG_MON_COMMAND:\n    return prepare_command(op);\n\n  case CEPH_MSG_POOLOP:\n    return prepare_pool_op(op);\n\n  case MSG_REMOVE_SNAPS:\n    return prepare_remove_snaps(op);\n\n\n  default:\n    ceph_abort();\n  }\n\n  return false;\n}\n\nbool OSDMonitor::should_propose(double& delay)\n{\n  dout(10) << \"should_propose\" << dendl;\n\n  // if full map, propose immediately!  any subsequent changes will be clobbered.\n  if (pending_inc.fullmap.length())\n    return true;\n\n  // adjust osd weights?\n  if (!osd_weight.empty() &&\n      osd_weight.size() == (unsigned)osdmap.get_max_osd()) {\n    dout(0) << \" adjusting osd weights based on \" << osd_weight << dendl;\n    osdmap.adjust_osd_weights(osd_weight, pending_inc);\n    delay = 0.0;\n    osd_weight.clear();\n    return true;\n  }\n\n  return PaxosService::should_propose(delay);\n}\n\n\n\n// ---------------------------\n// READs\n\nbool OSDMonitor::preprocess_get_osdmap(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MMonGetOSDMap *m = static_cast<MMonGetOSDMap*>(op->get_req());\n\n  uint64_t features = mon->get_quorum_con_features();\n  if (m->get_session() && m->get_session()->con_features)\n    features = m->get_session()->con_features;\n\n  dout(10) << __func__ << \" \" << *m << dendl;\n  MOSDMap *reply = new MOSDMap(mon->monmap->fsid, features);\n  epoch_t first = get_first_committed();\n  epoch_t last = osdmap.get_epoch();\n  int max = g_conf->osd_map_message_max;\n  for (epoch_t e = std::max(first, m->get_full_first());\n       e <= std::min(last, m->get_full_last()) && max > 0;\n       ++e, --max) {\n    int r = get_version_full(e, features, reply->maps[e]);\n    assert(r >= 0);\n  }\n  for (epoch_t e = std::max(first, m->get_inc_first());\n       e <= std::min(last, m->get_inc_last()) && max > 0;\n       ++e, --max) {\n    int r = get_version(e, features, reply->incremental_maps[e]);\n    assert(r >= 0);\n  }\n  reply->oldest_map = first;\n  reply->newest_map = last;\n  mon->send_reply(op, reply);\n  return true;\n}\n\n\n// ---------------------------\n// UPDATEs\n\n// failure --\n\nbool OSDMonitor::check_source(PaxosServiceMessage *m, uuid_d fsid) {\n  // check permissions\n  MonSession *session = m->get_session();\n  if (!session)\n    return true;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"got MOSDFailure from entity with insufficient caps \"\n\t    << session->caps << dendl;\n    return true;\n  }\n  if (fsid != mon->monmap->fsid) {\n    dout(0) << \"check_source: on fsid \" << fsid\n\t    << \" != \" << mon->monmap->fsid << dendl;\n    return true;\n  }\n  return false;\n}\n\n\nbool OSDMonitor::preprocess_failure(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDFailure *m = static_cast<MOSDFailure*>(op->get_req());\n  // who is target_osd\n  int badboy = m->get_target_osd();\n\n  // check permissions\n  if (check_source(m, m->fsid))\n    goto didit;\n\n  // first, verify the reporting host is valid\n  if (m->get_orig_source().is_osd()) {\n    int from = m->get_orig_source().num();\n    if (!osdmap.exists(from) ||\n\tosdmap.get_addrs(from) != m->get_orig_source_addrs() ||\n\t(osdmap.is_down(from) && m->if_osd_failed())) {\n      dout(5) << \"preprocess_failure from dead osd.\" << from\n\t      << \", ignoring\" << dendl;\n      send_incremental(op, m->get_epoch()+1);\n      goto didit;\n    }\n  }\n\n\n  // weird?\n  if (osdmap.is_down(badboy)) {\n    dout(5) << \"preprocess_failure dne(/dup?): osd.\" << m->get_target_osd()\n\t    << \" \" << m->get_target_addrs()\n\t    << \", from \" << m->get_orig_source() << dendl;\n    if (m->get_epoch() < osdmap.get_epoch())\n      send_incremental(op, m->get_epoch()+1);\n    goto didit;\n  }\n  if (osdmap.get_addrs(badboy) != m->get_target_addrs()) {\n    dout(5) << \"preprocess_failure wrong osd: report osd.\" << m->get_target_osd()\n\t    << \" \" << m->get_target_addrs()\n\t    << \" != map's \" << osdmap.get_addrs(badboy)\n\t    << \", from \" << m->get_orig_source() << dendl;\n    if (m->get_epoch() < osdmap.get_epoch())\n      send_incremental(op, m->get_epoch()+1);\n    goto didit;\n  }\n\n  // already reported?\n  if (osdmap.is_down(badboy) ||\n      osdmap.get_up_from(badboy) > m->get_epoch()) {\n    dout(5) << \"preprocess_failure dup/old: osd.\" << m->get_target_osd()\n\t    << \" \" << m->get_target_addrs()\n\t    << \", from \" << m->get_orig_source() << dendl;\n    if (m->get_epoch() < osdmap.get_epoch())\n      send_incremental(op, m->get_epoch()+1);\n    goto didit;\n  }\n\n  if (!can_mark_down(badboy)) {\n    dout(5) << \"preprocess_failure ignoring report of osd.\"\n\t    << m->get_target_osd() << \" \" << m->get_target_addrs()\n\t    << \" from \" << m->get_orig_source() << dendl;\n    goto didit;\n  }\n\n  dout(10) << \"preprocess_failure new: osd.\" << m->get_target_osd()\n\t   << \" \" << m->get_target_addrs()\n\t   << \", from \" << m->get_orig_source() << dendl;\n  return false;\n\n didit:\n  mon->no_reply(op);\n  return true;\n}\n\nclass C_AckMarkedDown : public C_MonOp {\n  OSDMonitor *osdmon;\npublic:\n  C_AckMarkedDown(\n    OSDMonitor *osdmon,\n    MonOpRequestRef op)\n    : C_MonOp(op), osdmon(osdmon) {}\n\n  void _finish(int) override {\n    MOSDMarkMeDown *m = static_cast<MOSDMarkMeDown*>(op->get_req());\n    osdmon->mon->send_reply(\n      op,\n      new MOSDMarkMeDown(\n\tm->fsid,\n\tm->target_osd,\n\tm->target_addrs,\n\tm->get_epoch(),\n\tfalse));   // ACK itself does not request an ack\n  }\n  ~C_AckMarkedDown() override {\n  }\n};\n\nbool OSDMonitor::preprocess_mark_me_down(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDMarkMeDown *m = static_cast<MOSDMarkMeDown*>(op->get_req());\n  int from = m->target_osd;\n\n  // check permissions\n  if (check_source(m, m->fsid))\n    goto reply;\n\n  // first, verify the reporting host is valid\n  if (!m->get_orig_source().is_osd())\n    goto reply;\n\n  if (!osdmap.exists(from) ||\n      osdmap.is_down(from) ||\n      osdmap.get_addrs(from) != m->target_addrs) {\n    dout(5) << \"preprocess_mark_me_down from dead osd.\"\n\t    << from << \", ignoring\" << dendl;\n    send_incremental(op, m->get_epoch()+1);\n    goto reply;\n  }\n\n  // no down might be set\n  if (!can_mark_down(from))\n    goto reply;\n\n  dout(10) << \"MOSDMarkMeDown for: \" << m->get_orig_source()\n\t   << \" \" << m->target_addrs << dendl;\n  return false;\n\n reply:\n  if (m->request_ack) {\n    Context *c(new C_AckMarkedDown(this, op));\n    c->complete(0);\n  }\n  return true;\n}\n\nbool OSDMonitor::prepare_mark_me_down(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDMarkMeDown *m = static_cast<MOSDMarkMeDown*>(op->get_req());\n  int target_osd = m->target_osd;\n\n  assert(osdmap.is_up(target_osd));\n  assert(osdmap.get_addrs(target_osd) == m->target_addrs);\n\n  mon->clog->info() << \"osd.\" << target_osd << \" marked itself down\";\n  pending_inc.new_state[target_osd] = CEPH_OSD_UP;\n  if (m->request_ack)\n    wait_for_finished_proposal(op, new C_AckMarkedDown(this, op));\n  return true;\n}\n\nbool OSDMonitor::can_mark_down(int i)\n{\n  if (osdmap.test_flag(CEPH_OSDMAP_NODOWN)) {\n    dout(5) << __func__ << \" NODOWN flag set, will not mark osd.\" << i\n            << \" down\" << dendl;\n    return false;\n  }\n\n  if (osdmap.is_nodown(i)) {\n    dout(5) << __func__ << \" osd.\" << i << \" is marked as nodown, \"\n            << \"will not mark it down\" << dendl;\n    return false;\n  }\n\n  int num_osds = osdmap.get_num_osds();\n  if (num_osds == 0) {\n    dout(5) << __func__ << \" no osds\" << dendl;\n    return false;\n  }\n  int up = osdmap.get_num_up_osds() - pending_inc.get_net_marked_down(&osdmap);\n  float up_ratio = (float)up / (float)num_osds;\n  if (up_ratio < g_conf->mon_osd_min_up_ratio) {\n    dout(2) << __func__ << \" current up_ratio \" << up_ratio << \" < min \"\n\t    << g_conf->mon_osd_min_up_ratio\n\t    << \", will not mark osd.\" << i << \" down\" << dendl;\n    return false;\n  }\n  return true;\n}\n\nbool OSDMonitor::can_mark_up(int i)\n{\n  if (osdmap.test_flag(CEPH_OSDMAP_NOUP)) {\n    dout(5) << __func__ << \" NOUP flag set, will not mark osd.\" << i\n            << \" up\" << dendl;\n    return false;\n  }\n\n  if (osdmap.is_noup(i)) {\n    dout(5) << __func__ << \" osd.\" << i << \" is marked as noup, \"\n            << \"will not mark it up\" << dendl;\n    return false;\n  }\n\n  return true;\n}\n\n/**\n * @note the parameter @p i apparently only exists here so we can output the\n *\t osd's id on messages.\n */\nbool OSDMonitor::can_mark_out(int i)\n{\n  if (osdmap.test_flag(CEPH_OSDMAP_NOOUT)) {\n    dout(5) << __func__ << \" NOOUT flag set, will not mark osds out\" << dendl;\n    return false;\n  }\n\n  if (osdmap.is_noout(i)) {\n    dout(5) << __func__ << \" osd.\" << i << \" is marked as noout, \"\n            << \"will not mark it out\" << dendl;\n    return false;\n  }\n\n  int num_osds = osdmap.get_num_osds();\n  if (num_osds == 0) {\n    dout(5) << __func__ << \" no osds\" << dendl;\n    return false;\n  }\n  int in = osdmap.get_num_in_osds() - pending_inc.get_net_marked_out(&osdmap);\n  float in_ratio = (float)in / (float)num_osds;\n  if (in_ratio < g_conf->mon_osd_min_in_ratio) {\n    if (i >= 0)\n      dout(5) << __func__ << \" current in_ratio \" << in_ratio << \" < min \"\n\t      << g_conf->mon_osd_min_in_ratio\n\t      << \", will not mark osd.\" << i << \" out\" << dendl;\n    else\n      dout(5) << __func__ << \" current in_ratio \" << in_ratio << \" < min \"\n\t      << g_conf->mon_osd_min_in_ratio\n\t      << \", will not mark osds out\" << dendl;\n    return false;\n  }\n\n  return true;\n}\n\nbool OSDMonitor::can_mark_in(int i)\n{\n  if (osdmap.test_flag(CEPH_OSDMAP_NOIN)) {\n    dout(5) << __func__ << \" NOIN flag set, will not mark osd.\" << i\n            << \" in\" << dendl;\n    return false;\n  }\n\n  if (osdmap.is_noin(i)) {\n    dout(5) << __func__ << \" osd.\" << i << \" is marked as noin, \"\n            << \"will not mark it in\" << dendl;\n    return false;\n  }\n\n  return true;\n}\n\nbool OSDMonitor::check_failures(utime_t now)\n{\n  bool found_failure = false;\n  for (map<int,failure_info_t>::iterator p = failure_info.begin();\n       p != failure_info.end();\n       ++p) {\n    if (can_mark_down(p->first)) {\n      found_failure |= check_failure(now, p->first, p->second);\n    }\n  }\n  return found_failure;\n}\n\nbool OSDMonitor::check_failure(utime_t now, int target_osd, failure_info_t& fi)\n{\n  // already pending failure?\n  if (pending_inc.new_state.count(target_osd) &&\n      pending_inc.new_state[target_osd] & CEPH_OSD_UP) {\n    dout(10) << \" already pending failure\" << dendl;\n    return true;\n  }\n\n  set<string> reporters_by_subtree;\n  auto reporter_subtree_level = g_conf->get_val<string>(\"mon_osd_reporter_subtree_level\");\n  utime_t orig_grace(g_conf->osd_heartbeat_grace, 0);\n  utime_t max_failed_since = fi.get_failed_since();\n  utime_t failed_for = now - max_failed_since;\n\n  utime_t grace = orig_grace;\n  double my_grace = 0, peer_grace = 0;\n  double decay_k = 0;\n  if (g_conf->mon_osd_adjust_heartbeat_grace) {\n    double halflife = (double)g_conf->mon_osd_laggy_halflife;\n    decay_k = ::log(.5) / halflife;\n\n    // scale grace period based on historical probability of 'lagginess'\n    // (false positive failures due to slowness).\n    const osd_xinfo_t& xi = osdmap.get_xinfo(target_osd);\n    double decay = exp((double)failed_for * decay_k);\n    dout(20) << \" halflife \" << halflife << \" decay_k \" << decay_k\n\t     << \" failed_for \" << failed_for << \" decay \" << decay << dendl;\n    my_grace = decay * (double)xi.laggy_interval * xi.laggy_probability;\n    grace += my_grace;\n  }\n\n  // consider the peers reporting a failure a proxy for a potential\n  // 'subcluster' over the overall cluster that is similarly\n  // laggy.  this is clearly not true in all cases, but will sometimes\n  // help us localize the grace correction to a subset of the system\n  // (say, a rack with a bad switch) that is unhappy.\n  assert(fi.reporters.size());\n  for (map<int,failure_reporter_t>::iterator p = fi.reporters.begin();\n\tp != fi.reporters.end();\n\t++p) {\n    // get the parent bucket whose type matches with \"reporter_subtree_level\".\n    // fall back to OSD if the level doesn't exist.\n    map<string, string> reporter_loc = osdmap.crush->get_full_location(p->first);\n    map<string, string>::iterator iter = reporter_loc.find(reporter_subtree_level);\n    if (iter == reporter_loc.end()) {\n      reporters_by_subtree.insert(\"osd.\" + to_string(p->first));\n    } else {\n      reporters_by_subtree.insert(iter->second);\n    }\n    if (g_conf->mon_osd_adjust_heartbeat_grace) {\n      const osd_xinfo_t& xi = osdmap.get_xinfo(p->first);\n      utime_t elapsed = now - xi.down_stamp;\n      double decay = exp((double)elapsed * decay_k);\n      peer_grace += decay * (double)xi.laggy_interval * xi.laggy_probability;\n    }\n  }\n  \n  if (g_conf->mon_osd_adjust_heartbeat_grace) {\n    peer_grace /= (double)fi.reporters.size();\n    grace += peer_grace;\n  }\n\n  dout(10) << \" osd.\" << target_osd << \" has \"\n\t   << fi.reporters.size() << \" reporters, \"\n\t   << grace << \" grace (\" << orig_grace << \" + \" << my_grace\n\t   << \" + \" << peer_grace << \"), max_failed_since \" << max_failed_since\n\t   << dendl;\n\n  if (failed_for >= grace &&\n      reporters_by_subtree.size() >= g_conf->get_val<uint64_t>(\"mon_osd_min_down_reporters\")) {\n    dout(1) << \" we have enough reporters to mark osd.\" << target_osd\n\t    << \" down\" << dendl;\n    pending_inc.new_state[target_osd] = CEPH_OSD_UP;\n\n    mon->clog->info() << \"osd.\" << target_osd << \" failed (\"\n\t\t      << osdmap.crush->get_full_location_ordered_string(\n\t\t\ttarget_osd)\n\t\t      << \") (\"\n\t\t      << (int)reporters_by_subtree.size()\n\t\t      << \" reporters from different \"\n\t\t      << reporter_subtree_level << \" after \"\n\t\t      << failed_for << \" >= grace \" << grace << \")\";\n    return true;\n  }\n  return false;\n}\n\nvoid OSDMonitor::force_failure(int target_osd, int by)\n{\n  // already pending failure?\n  if (pending_inc.new_state.count(target_osd) &&\n      pending_inc.new_state[target_osd] & CEPH_OSD_UP) {\n    dout(10) << \" already pending failure\" << dendl;\n    return;\n  }\n\n  dout(1) << \" we're forcing failure of osd.\" << target_osd << dendl;\n  pending_inc.new_state[target_osd] = CEPH_OSD_UP;\n\n  mon->clog->info() << \"osd.\" << target_osd << \" failed (\"\n\t\t    << osdmap.crush->get_full_location_ordered_string(target_osd)\n\t\t    << \") (connection refused reported by osd.\" << by << \")\";\n  return;\n}\n\nbool OSDMonitor::prepare_failure(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDFailure *m = static_cast<MOSDFailure*>(op->get_req());\n  dout(1) << \"prepare_failure osd.\" << m->get_target_osd()\n\t  << \" \" << m->get_target_addrs()\n\t  << \" from \" << m->get_orig_source()\n          << \" is reporting failure:\" << m->if_osd_failed() << dendl;\n\n  int target_osd = m->get_target_osd();\n  int reporter = m->get_orig_source().num();\n  assert(osdmap.is_up(target_osd));\n  assert(osdmap.get_addrs(target_osd) == m->get_target_addrs());\n\n  if (m->if_osd_failed()) {\n    // calculate failure time\n    utime_t now = ceph_clock_now();\n    utime_t failed_since =\n      m->get_recv_stamp() - utime_t(m->failed_for, 0);\n\n    // add a report\n    if (m->is_immediate()) {\n      mon->clog->debug() << \"osd.\" << m->get_target_osd()\n\t\t\t << \" reported immediately failed by \"\n\t\t\t << m->get_orig_source();\n      force_failure(target_osd, reporter);\n      mon->no_reply(op);\n      return true;\n    }\n    mon->clog->debug() << \"osd.\" << m->get_target_osd() << \" reported failed by \"\n\t\t      << m->get_orig_source();\n\n    failure_info_t& fi = failure_info[target_osd];\n    MonOpRequestRef old_op = fi.add_report(reporter, failed_since, op);\n    if (old_op) {\n      mon->no_reply(old_op);\n    }\n\n    return check_failure(now, target_osd, fi);\n  } else {\n    // remove the report\n    mon->clog->debug() << \"osd.\" << m->get_target_osd()\n\t\t       << \" failure report canceled by \"\n\t\t       << m->get_orig_source();\n    if (failure_info.count(target_osd)) {\n      failure_info_t& fi = failure_info[target_osd];\n      MonOpRequestRef report_op = fi.cancel_report(reporter);\n      if (report_op) {\n        mon->no_reply(report_op);\n      }\n      if (fi.reporters.empty()) {\n\tdout(10) << \" removing last failure_info for osd.\" << target_osd\n\t\t << dendl;\n\tfailure_info.erase(target_osd);\n      } else {\n\tdout(10) << \" failure_info for osd.\" << target_osd << \" now \"\n\t\t << fi.reporters.size() << \" reporters\" << dendl;\n      }\n    } else {\n      dout(10) << \" no failure_info for osd.\" << target_osd << dendl;\n    }\n    mon->no_reply(op);\n  }\n\n  return false;\n}\n\nvoid OSDMonitor::process_failures()\n{\n  map<int,failure_info_t>::iterator p = failure_info.begin();\n  while (p != failure_info.end()) {\n    if (osdmap.is_up(p->first)) {\n      ++p;\n    } else {\n      dout(10) << \"process_failures osd.\" << p->first << dendl;\n      list<MonOpRequestRef> ls;\n      p->second.take_report_messages(ls);\n      failure_info.erase(p++);\n\n      while (!ls.empty()) {\n        MonOpRequestRef o = ls.front();\n        if (o) {\n          o->mark_event(__func__);\n          MOSDFailure *m = o->get_req<MOSDFailure>();\n          send_latest(o, m->get_epoch());\n\t  mon->no_reply(o);\n        }\n\tls.pop_front();\n      }\n    }\n  }\n}\n\nvoid OSDMonitor::take_all_failures(list<MonOpRequestRef>& ls)\n{\n  dout(10) << __func__ << \" on \" << failure_info.size() << \" osds\" << dendl;\n\n  for (map<int,failure_info_t>::iterator p = failure_info.begin();\n       p != failure_info.end();\n       ++p) {\n    p->second.take_report_messages(ls);\n  }\n  failure_info.clear();\n}\n\n\n// boot --\n\nbool OSDMonitor::preprocess_boot(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDBoot *m = static_cast<MOSDBoot*>(op->get_req());\n  int from = m->get_orig_source_inst().name.num();\n\n  // check permissions, ignore if failed (no response expected)\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"got preprocess_boot message from entity with insufficient caps\"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  if (m->sb.cluster_fsid != mon->monmap->fsid) {\n    dout(0) << \"preprocess_boot on fsid \" << m->sb.cluster_fsid\n\t    << \" != \" << mon->monmap->fsid << dendl;\n    goto ignore;\n  }\n\n  if (m->get_orig_source_inst().addr.is_blank_ip()) {\n    dout(0) << \"preprocess_boot got blank addr for \" << m->get_orig_source_inst() << dendl;\n    goto ignore;\n  }\n\n  assert(m->get_orig_source_inst().name.is_osd());\n\n  // check if osd has required features to boot\n  if (osdmap.require_osd_release >= CEPH_RELEASE_LUMINOUS &&\n      !HAVE_FEATURE(m->osd_features, SERVER_LUMINOUS)) {\n    mon->clog->info() << \"disallowing boot of OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because the osdmap requires\"\n\t\t      << \" CEPH_FEATURE_SERVER_LUMINOUS\"\n\t\t      << \" but the osd lacks CEPH_FEATURE_SERVER_LUMINOUS\";\n    goto ignore;\n  }\n\n  if (osdmap.require_osd_release >= CEPH_RELEASE_JEWEL &&\n      !(m->osd_features & CEPH_FEATURE_SERVER_JEWEL)) {\n    mon->clog->info() << \"disallowing boot of OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because the osdmap requires\"\n\t\t      << \" CEPH_FEATURE_SERVER_JEWEL\"\n\t\t      << \" but the osd lacks CEPH_FEATURE_SERVER_JEWEL\";\n    goto ignore;\n  }\n\n  if (osdmap.require_osd_release >= CEPH_RELEASE_KRAKEN &&\n      !HAVE_FEATURE(m->osd_features, SERVER_KRAKEN)) {\n    mon->clog->info() << \"disallowing boot of OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because the osdmap requires\"\n\t\t      << \" CEPH_FEATURE_SERVER_KRAKEN\"\n\t\t      << \" but the osd lacks CEPH_FEATURE_SERVER_KRAKEN\";\n    goto ignore;\n  }\n\n  if (osdmap.test_flag(CEPH_OSDMAP_RECOVERY_DELETES) &&\n      !(m->osd_features & CEPH_FEATURE_OSD_RECOVERY_DELETES)) {\n    mon->clog->info() << \"disallowing boot of OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because 'recovery_deletes' osdmap flag is set and OSD lacks the OSD_RECOVERY_DELETES feature\";\n    goto ignore;\n  }\n\n  // make sure upgrades stop at nautilus\n  if (HAVE_FEATURE(m->osd_features, SERVER_O) &&\n      osdmap.require_osd_release < CEPH_RELEASE_NAUTILUS) {\n    mon->clog->info() << \"disallowing boot of post-nautilus OSD \"\n\t\t      << m->get_orig_source_inst()\n\t\t      << \" because require_osd_release < nautilus\";\n    goto ignore;\n  }\n\n  // already booted?\n  if (osdmap.is_up(from) &&\n      osdmap.get_addrs(from) == m->get_orig_source_addrs() &&\n      osdmap.get_cluster_addrs(from) == m->cluster_addrs) {\n    // yup.\n    dout(7) << \"preprocess_boot dup from \" << m->get_orig_source()\n\t    << \" \" << m->get_orig_source_addrs()\n\t    << \" == \" << osdmap.get_addrs(from) << dendl;\n    _booted(op, false);\n    return true;\n  }\n\n  if (osdmap.exists(from) &&\n      !osdmap.get_uuid(from).is_zero() &&\n      osdmap.get_uuid(from) != m->sb.osd_fsid) {\n    dout(7) << __func__ << \" from \" << m->get_orig_source_inst()\n            << \" clashes with existing osd: different fsid\"\n            << \" (ours: \" << osdmap.get_uuid(from)\n            << \" ; theirs: \" << m->sb.osd_fsid << \")\" << dendl;\n    goto ignore;\n  }\n\n  if (osdmap.exists(from) &&\n      osdmap.get_info(from).up_from > m->version &&\n      osdmap.get_most_recent_addrs(from) == m->get_orig_source_addrs()) {\n    dout(7) << \"prepare_boot msg from before last up_from, ignoring\" << dendl;\n    send_latest(op, m->sb.current_epoch+1);\n    return true;\n  }\n\n  // noup?\n  if (!can_mark_up(from)) {\n    dout(7) << \"preprocess_boot ignoring boot from \" << m->get_orig_source_inst() << dendl;\n    send_latest(op, m->sb.current_epoch+1);\n    return true;\n  }\n\n  dout(10) << \"preprocess_boot from \" << m->get_orig_source_inst() << dendl;\n  return false;\n\n ignore:\n  return true;\n}\n\nbool OSDMonitor::prepare_boot(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDBoot *m = static_cast<MOSDBoot*>(op->get_req());\n  dout(7) << __func__ << \" from \" << m->get_source()\n\t  << \" sb \" << m->sb\n\t  << \" client_addrs\" << m->get_connection()->get_peer_addrs()\n\t  << \" cluster_addrs \" << m->cluster_addrs\n\t  << \" hb_back_addrs \" << m->hb_back_addrs\n\t  << \" hb_front_addrs \" << m->hb_front_addrs\n\t  << dendl;\n\n  assert(m->get_orig_source().is_osd());\n  int from = m->get_orig_source().num();\n\n  // does this osd exist?\n  if (from >= osdmap.get_max_osd()) {\n    dout(1) << \"boot from osd.\" << from << \" >= max_osd \"\n\t    << osdmap.get_max_osd() << dendl;\n    return false;\n  }\n\n  int oldstate = osdmap.exists(from) ? osdmap.get_state(from) : CEPH_OSD_NEW;\n  if (pending_inc.new_state.count(from))\n    oldstate ^= pending_inc.new_state[from];\n\n  // already up?  mark down first?\n  if (osdmap.is_up(from)) {\n    dout(7) << __func__ << \" was up, first marking down osd.\" << from << \" \"\n\t    << osdmap.get_addrs(from) << dendl;\n    // preprocess should have caught these;  if not, assert.\n    assert(osdmap.get_addrs(from) != m->get_orig_source_addrs() ||\n           osdmap.get_cluster_addrs(from) != m->cluster_addrs);\n    assert(osdmap.get_uuid(from) == m->sb.osd_fsid);\n\n    if (pending_inc.new_state.count(from) == 0 ||\n\t(pending_inc.new_state[from] & CEPH_OSD_UP) == 0) {\n      // mark previous guy down\n      pending_inc.new_state[from] = CEPH_OSD_UP;\n    }\n    wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n  } else if (pending_inc.new_up_client.count(from)) {\n    // already prepared, just wait\n    dout(7) << __func__ << \" already prepared, waiting on \"\n\t    << m->get_orig_source_addr() << dendl;\n    wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n  } else {\n    // mark new guy up.\n    pending_inc.new_up_client[from] = m->get_orig_source_addrs();\n    pending_inc.new_up_cluster[from] = m->cluster_addrs;\n    pending_inc.new_hb_back_up[from] = m->hb_back_addrs;\n    pending_inc.new_hb_front_up[from] = m->hb_front_addrs;\n\n    down_pending_out.erase(from);  // if any\n\n    if (m->sb.weight)\n      osd_weight[from] = m->sb.weight;\n\n    // set uuid?\n    dout(10) << \" setting osd.\" << from << \" uuid to \" << m->sb.osd_fsid\n\t     << dendl;\n    if (!osdmap.exists(from) || osdmap.get_uuid(from) != m->sb.osd_fsid) {\n      // preprocess should have caught this;  if not, assert.\n      assert(!osdmap.exists(from) || osdmap.get_uuid(from).is_zero());\n      pending_inc.new_uuid[from] = m->sb.osd_fsid;\n    }\n\n    // fresh osd?\n    if (m->sb.newest_map == 0 && osdmap.exists(from)) {\n      const osd_info_t& i = osdmap.get_info(from);\n      if (i.up_from > i.lost_at) {\n\tdout(10) << \" fresh osd; marking lost_at too\" << dendl;\n\tpending_inc.new_lost[from] = osdmap.get_epoch();\n      }\n    }\n\n    // metadata\n    bufferlist osd_metadata;\n    encode(m->metadata, osd_metadata);\n    pending_metadata[from] = osd_metadata;\n    pending_metadata_rm.erase(from);\n\n    // adjust last clean unmount epoch?\n    const osd_info_t& info = osdmap.get_info(from);\n    dout(10) << \" old osd_info: \" << info << dendl;\n    if (m->sb.mounted > info.last_clean_begin ||\n\t(m->sb.mounted == info.last_clean_begin &&\n\t m->sb.clean_thru > info.last_clean_end)) {\n      epoch_t begin = m->sb.mounted;\n      epoch_t end = m->sb.clean_thru;\n\n      dout(10) << __func__ << \" osd.\" << from << \" last_clean_interval \"\n\t       << \"[\" << info.last_clean_begin << \",\" << info.last_clean_end\n\t       << \") -> [\" << begin << \"-\" << end << \")\"\n\t       << dendl;\n      pending_inc.new_last_clean_interval[from] =\n\tpair<epoch_t,epoch_t>(begin, end);\n    }\n\n    osd_xinfo_t xi = osdmap.get_xinfo(from);\n    if (m->boot_epoch == 0) {\n      xi.laggy_probability *= (1.0 - g_conf->mon_osd_laggy_weight);\n      xi.laggy_interval *= (1.0 - g_conf->mon_osd_laggy_weight);\n      dout(10) << \" not laggy, new xi \" << xi << dendl;\n    } else {\n      if (xi.down_stamp.sec()) {\n        int interval = ceph_clock_now().sec() -\n\t  xi.down_stamp.sec();\n        if (g_conf->mon_osd_laggy_max_interval &&\n\t    (interval > g_conf->mon_osd_laggy_max_interval)) {\n          interval =  g_conf->mon_osd_laggy_max_interval;\n        }\n        xi.laggy_interval =\n\t  interval * g_conf->mon_osd_laggy_weight +\n\t  xi.laggy_interval * (1.0 - g_conf->mon_osd_laggy_weight);\n      }\n      xi.laggy_probability =\n\tg_conf->mon_osd_laggy_weight +\n\txi.laggy_probability * (1.0 - g_conf->mon_osd_laggy_weight);\n      dout(10) << \" laggy, now xi \" << xi << dendl;\n    }\n\n    // set features shared by the osd\n    if (m->osd_features)\n      xi.features = m->osd_features;\n    else\n      xi.features = m->get_connection()->get_features();\n\n    // mark in?\n    if ((g_conf->mon_osd_auto_mark_auto_out_in &&\n\t (oldstate & CEPH_OSD_AUTOOUT)) ||\n\t(g_conf->mon_osd_auto_mark_new_in && (oldstate & CEPH_OSD_NEW)) ||\n\t(g_conf->mon_osd_auto_mark_in)) {\n      if (can_mark_in(from)) {\n\tif (osdmap.osd_xinfo[from].old_weight > 0) {\n\t  pending_inc.new_weight[from] = osdmap.osd_xinfo[from].old_weight;\n\t  xi.old_weight = 0;\n\t} else {\n\t  pending_inc.new_weight[from] = CEPH_OSD_IN;\n\t}\n      } else {\n\tdout(7) << __func__ << \" NOIN set, will not mark in \"\n\t\t<< m->get_orig_source_addr() << dendl;\n      }\n    }\n\n    pending_inc.new_xinfo[from] = xi;\n\n    // wait\n    wait_for_finished_proposal(op, new C_Booted(this, op));\n  }\n  return true;\n}\n\nvoid OSDMonitor::_booted(MonOpRequestRef op, bool logit)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDBoot *m = static_cast<MOSDBoot*>(op->get_req());\n  dout(7) << \"_booted \" << m->get_orig_source_inst() \n\t  << \" w \" << m->sb.weight << \" from \" << m->sb.current_epoch << dendl;\n\n  if (logit) {\n    mon->clog->info() << m->get_orig_source_inst() << \" boot\";\n  }\n\n  send_latest(op, m->sb.current_epoch+1);\n}\n\n\n// -------------\n// full\n\nbool OSDMonitor::preprocess_full(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDFull *m = static_cast<MOSDFull*>(op->get_req());\n  int from = m->get_orig_source().num();\n  set<string> state;\n  unsigned mask = CEPH_OSD_NEARFULL | CEPH_OSD_BACKFILLFULL | CEPH_OSD_FULL;\n\n  // check permissions, ignore if failed\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"MOSDFull from entity with insufficient privileges:\"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  // ignore a full message from the osd instance that already went down\n  if (!osdmap.exists(from)) {\n    dout(7) << __func__ << \" ignoring full message from nonexistent \"\n\t    << m->get_orig_source_inst() << dendl;\n    goto ignore;\n  }\n  if ((!osdmap.is_up(from) &&\n       osdmap.get_most_recent_addrs(from) == m->get_orig_source_addrs()) ||\n      (osdmap.is_up(from) &&\n       osdmap.get_addrs(from) != m->get_orig_source_addrs())) {\n    dout(7) << __func__ << \" ignoring full message from down \"\n\t    << m->get_orig_source_inst() << dendl;\n    goto ignore;\n  }\n\n  OSDMap::calc_state_set(osdmap.get_state(from), state);\n\n  if ((osdmap.get_state(from) & mask) == m->state) {\n    dout(7) << __func__ << \" state already \" << state << \" for osd.\" << from\n\t    << \" \" << m->get_orig_source_inst() << dendl;\n    _reply_map(op, m->version);\n    goto ignore;\n  }\n\n  dout(10) << __func__ << \" want state \" << state << \" for osd.\" << from\n\t   << \" \" << m->get_orig_source_inst() << dendl;\n  return false;\n\n ignore:\n  return true;\n}\n\nbool OSDMonitor::prepare_full(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  const MOSDFull *m = static_cast<MOSDFull*>(op->get_req());\n  const int from = m->get_orig_source().num();\n\n  const unsigned mask = CEPH_OSD_NEARFULL | CEPH_OSD_BACKFILLFULL | CEPH_OSD_FULL;\n  const unsigned want_state = m->state & mask;  // safety first\n\n  unsigned cur_state = osdmap.get_state(from);\n  auto p = pending_inc.new_state.find(from);\n  if (p != pending_inc.new_state.end()) {\n    cur_state ^= p->second;\n  }\n  cur_state &= mask;\n\n  set<string> want_state_set, cur_state_set;\n  OSDMap::calc_state_set(want_state, want_state_set);\n  OSDMap::calc_state_set(cur_state, cur_state_set);\n\n  if (cur_state != want_state) {\n    if (p != pending_inc.new_state.end()) {\n      p->second &= ~mask;\n    } else {\n      pending_inc.new_state[from] = 0;\n    }\n    pending_inc.new_state[from] |= (osdmap.get_state(from) & mask) ^ want_state;\n    dout(7) << __func__ << \" osd.\" << from << \" \" << cur_state_set\n\t    << \" -> \" << want_state_set << dendl;\n  } else {\n    dout(7) << __func__ << \" osd.\" << from << \" \" << cur_state_set\n\t    << \" = wanted \" << want_state_set << \", just waiting\" << dendl;\n  }\n\n  wait_for_finished_proposal(op, new C_ReplyMap(this, op, m->version));\n  return true;\n}\n\n// -------------\n// alive\n\nbool OSDMonitor::preprocess_alive(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDAlive *m = static_cast<MOSDAlive*>(op->get_req());\n  int from = m->get_orig_source().num();\n\n  // check permissions, ignore if failed\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"attempt to send MOSDAlive from entity with insufficient privileges:\"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  if (!osdmap.is_up(from) ||\n      osdmap.get_addrs(from) != m->get_orig_source_addrs()) {\n    dout(7) << \"preprocess_alive ignoring alive message from down \"\n\t    << m->get_orig_source() << \" \" << m->get_orig_source_addrs()\n\t    << dendl;\n    goto ignore;\n  }\n\n  if (osdmap.get_up_thru(from) >= m->want) {\n    // yup.\n    dout(7) << \"preprocess_alive want up_thru \" << m->want << \" dup from \" << m->get_orig_source_inst() << dendl;\n    _reply_map(op, m->version);\n    return true;\n  }\n\n  dout(10) << \"preprocess_alive want up_thru \" << m->want\n\t   << \" from \" << m->get_orig_source_inst() << dendl;\n  return false;\n\n ignore:\n  return true;\n}\n\nbool OSDMonitor::prepare_alive(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDAlive *m = static_cast<MOSDAlive*>(op->get_req());\n  int from = m->get_orig_source().num();\n\n  if (0) {  // we probably don't care much about these\n    mon->clog->debug() << m->get_orig_source_inst() << \" alive\";\n  }\n\n  dout(7) << \"prepare_alive want up_thru \" << m->want << \" have \" << m->version\n\t  << \" from \" << m->get_orig_source_inst() << dendl;\n\n  update_up_thru(from, m->version); // set to the latest map the OSD has\n  wait_for_finished_proposal(op, new C_ReplyMap(this, op, m->version));\n  return true;\n}\n\nvoid OSDMonitor::_reply_map(MonOpRequestRef op, epoch_t e)\n{\n  op->mark_osdmon_event(__func__);\n  dout(7) << \"_reply_map \" << e\n\t  << \" from \" << op->get_req()->get_orig_source_inst()\n\t  << dendl;\n  send_latest(op, e);\n}\n\n// pg_created\nbool OSDMonitor::preprocess_pg_created(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  auto m = static_cast<MOSDPGCreated*>(op->get_req());\n  dout(10) << __func__ << \" \" << *m << dendl;\n  auto session = m->get_session();\n  mon->no_reply(op);\n  if (!session) {\n    dout(10) << __func__ << \": no monitor session!\" << dendl;\n    return true;\n  }\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    derr << __func__ << \" received from entity \"\n         << \"with insufficient privileges \" << session->caps << dendl;\n    return true;\n  }\n  // always forward the \"created!\" to the leader\n  return false;\n}\n\nbool OSDMonitor::prepare_pg_created(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  auto m = static_cast<MOSDPGCreated*>(op->get_req());\n  dout(10) << __func__ << \" \" << *m << dendl;\n  auto src = m->get_orig_source();\n  auto from = src.num();\n  if (!src.is_osd() ||\n      !mon->osdmon()->osdmap.is_up(from) ||\n      m->get_orig_source_addrs() != mon->osdmon()->osdmap.get_addrs(from)) {\n    dout(1) << __func__ << \" ignoring stats from non-active osd.\" << dendl;\n    return false;\n  }\n  pending_created_pgs.push_back(m->pgid);\n  return true;\n}\n\n// -------------\n// pg_temp changes\n\nbool OSDMonitor::preprocess_pgtemp(MonOpRequestRef op)\n{\n  MOSDPGTemp *m = static_cast<MOSDPGTemp*>(op->get_req());\n  dout(10) << \"preprocess_pgtemp \" << *m << dendl;\n  mempool::osdmap::vector<int> empty;\n  int from = m->get_orig_source().num();\n  size_t ignore_cnt = 0;\n\n  // check caps\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    dout(0) << \"attempt to send MOSDPGTemp from entity with insufficient caps \"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  if (!osdmap.is_up(from) ||\n      osdmap.get_addrs(from) != m->get_orig_source_addrs()) {\n    dout(7) << \"ignoring pgtemp message from down \"\n\t    << m->get_orig_source() << \" \" << m->get_orig_source_addrs()\n\t    << dendl;\n    goto ignore;\n  }\n\n  if (m->forced) {\n    return false;\n  }\n\n  for (auto p = m->pg_temp.begin(); p != m->pg_temp.end(); ++p) {\n    dout(20) << \" \" << p->first\n\t     << (osdmap.pg_temp->count(p->first) ? osdmap.pg_temp->get(p->first) : empty)\n             << \" -> \" << p->second << dendl;\n\n    // does the pool exist?\n    if (!osdmap.have_pg_pool(p->first.pool())) {\n      /*\n       * 1. If the osdmap does not have the pool, it means the pool has been\n       *    removed in-between the osd sending this message and us handling it.\n       * 2. If osdmap doesn't have the pool, it is safe to assume the pool does\n       *    not exist in the pending either, as the osds would not send a\n       *    message about a pool they know nothing about (yet).\n       * 3. However, if the pool does exist in the pending, then it must be a\n       *    new pool, and not relevant to this message (see 1).\n       */\n      dout(10) << __func__ << \" ignore \" << p->first << \" -> \" << p->second\n               << \": pool has been removed\" << dendl;\n      ignore_cnt++;\n      continue;\n    }\n\n    int acting_primary = -1;\n    osdmap.pg_to_up_acting_osds(\n      p->first, nullptr, nullptr, nullptr, &acting_primary);\n    if (acting_primary != from) {\n      /* If the source isn't the primary based on the current osdmap, we know\n       * that the interval changed and that we can discard this message.\n       * Indeed, we must do so to avoid 16127 since we can't otherwise determine\n       * which of two pg temp mappings on the same pg is more recent.\n       */\n      dout(10) << __func__ << \" ignore \" << p->first << \" -> \" << p->second\n\t       << \": primary has changed\" << dendl;\n      ignore_cnt++;\n      continue;\n    }\n\n    // removal?\n    if (p->second.empty() && (osdmap.pg_temp->count(p->first) ||\n\t\t\t      osdmap.primary_temp->count(p->first)))\n      return false;\n    // change?\n    //  NOTE: we assume that this will clear pg_primary, so consider\n    //        an existing pg_primary field to imply a change\n    if (p->second.size() &&\n\t(osdmap.pg_temp->count(p->first) == 0 ||\n\t osdmap.pg_temp->get(p->first) != p->second ||\n\t osdmap.primary_temp->count(p->first)))\n      return false;\n  }\n\n  // should we ignore all the pgs?\n  if (ignore_cnt == m->pg_temp.size())\n    goto ignore;\n\n  dout(7) << \"preprocess_pgtemp e\" << m->map_epoch << \" no changes from \" << m->get_orig_source_inst() << dendl;\n  _reply_map(op, m->map_epoch);\n  return true;\n\n ignore:\n  return true;\n}\n\nvoid OSDMonitor::update_up_thru(int from, epoch_t up_thru)\n{\n  epoch_t old_up_thru = osdmap.get_up_thru(from);\n  auto ut = pending_inc.new_up_thru.find(from);\n  if (ut != pending_inc.new_up_thru.end()) {\n    old_up_thru = ut->second;\n  }\n  if (up_thru > old_up_thru) {\n    // set up_thru too, so the osd doesn't have to ask again\n    pending_inc.new_up_thru[from] = up_thru;\n  }\n}\n\nbool OSDMonitor::prepare_pgtemp(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MOSDPGTemp *m = static_cast<MOSDPGTemp*>(op->get_req());\n  int from = m->get_orig_source().num();\n  dout(7) << \"prepare_pgtemp e\" << m->map_epoch << \" from \" << m->get_orig_source_inst() << dendl;\n  for (map<pg_t,vector<int32_t> >::iterator p = m->pg_temp.begin(); p != m->pg_temp.end(); ++p) {\n    uint64_t pool = p->first.pool();\n    if (pending_inc.old_pools.count(pool)) {\n      dout(10) << __func__ << \" ignore \" << p->first << \" -> \" << p->second\n               << \": pool pending removal\" << dendl;\n      continue;\n    }\n    if (!osdmap.have_pg_pool(pool)) {\n      dout(10) << __func__ << \" ignore \" << p->first << \" -> \" << p->second\n               << \": pool has been removed\" << dendl;\n      continue;\n    }\n    pending_inc.new_pg_temp[p->first] =\n      mempool::osdmap::vector<int>(p->second.begin(), p->second.end());\n\n    // unconditionally clear pg_primary (until this message can encode\n    // a change for that, too.. at which point we need to also fix\n    // preprocess_pg_temp)\n    if (osdmap.primary_temp->count(p->first) ||\n\tpending_inc.new_primary_temp.count(p->first))\n      pending_inc.new_primary_temp[p->first] = -1;\n  }\n\n  // set up_thru too, so the osd doesn't have to ask again\n  update_up_thru(from, m->map_epoch);\n\n  wait_for_finished_proposal(op, new C_ReplyMap(this, op, m->map_epoch));\n  return true;\n}\n\n\n// ---\n\nbool OSDMonitor::preprocess_remove_snaps(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MRemoveSnaps *m = static_cast<MRemoveSnaps*>(op->get_req());\n  dout(7) << \"preprocess_remove_snaps \" << *m << dendl;\n\n  // check privilege, ignore if failed\n  MonSession *session = m->get_session();\n  if (!session)\n    goto ignore;\n  if (!session->caps.is_capable(\n\tcct,\n\tCEPH_ENTITY_TYPE_MON,\n\tsession->entity_name,\n        \"osd\", \"osd pool rmsnap\", {}, true, true, false)) {\n    dout(0) << \"got preprocess_remove_snaps from entity with insufficient caps \"\n\t    << session->caps << dendl;\n    goto ignore;\n  }\n\n  for (map<int, vector<snapid_t> >::iterator q = m->snaps.begin();\n       q != m->snaps.end();\n       ++q) {\n    if (!osdmap.have_pg_pool(q->first)) {\n      dout(10) << \" ignoring removed_snaps \" << q->second << \" on non-existent pool \" << q->first << dendl;\n      continue;\n    }\n    const pg_pool_t *pi = osdmap.get_pg_pool(q->first);\n    for (vector<snapid_t>::iterator p = q->second.begin();\n\t p != q->second.end();\n\t ++p) {\n      if (*p > pi->get_snap_seq() ||\n\t  !pi->removed_snaps.contains(*p))\n\treturn false;\n    }\n  }\n\n ignore:\n  return true;\n}\n\nbool OSDMonitor::prepare_remove_snaps(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MRemoveSnaps *m = static_cast<MRemoveSnaps*>(op->get_req());\n  dout(7) << \"prepare_remove_snaps \" << *m << dendl;\n\n  for (map<int, vector<snapid_t> >::iterator p = m->snaps.begin();\n       p != m->snaps.end();\n       ++p) {\n\n    if (!osdmap.have_pg_pool(p->first)) {\n      dout(10) << \" ignoring removed_snaps \" << p->second << \" on non-existent pool \" << p->first << dendl;\n      continue;\n    }\n\n    pg_pool_t& pi = osdmap.pools[p->first];\n    for (vector<snapid_t>::iterator q = p->second.begin();\n\t q != p->second.end();\n\t ++q) {\n      if (!pi.removed_snaps.contains(*q) &&\n\t  (!pending_inc.new_pools.count(p->first) ||\n\t   !pending_inc.new_pools[p->first].removed_snaps.contains(*q))) {\n\tpg_pool_t *newpi = pending_inc.get_new_pool(p->first, &pi);\n\tnewpi->removed_snaps.insert(*q);\n\tnewpi->flags |= pg_pool_t::FLAG_SELFMANAGED_SNAPS;\n\tdout(10) << \" pool \" << p->first << \" removed_snaps added \" << *q\n\t\t << \" (now \" << newpi->removed_snaps << \")\" << dendl;\n\tif (*q > newpi->get_snap_seq()) {\n\t  dout(10) << \" pool \" << p->first << \" snap_seq \"\n\t\t   << newpi->get_snap_seq() << \" -> \" << *q << dendl;\n\t  newpi->set_snap_seq(*q);\n\t}\n\tnewpi->set_snap_epoch(pending_inc.epoch);\n\tpending_inc.new_removed_snaps[p->first].insert(*q);\n      }\n    }\n  }\n  return true;\n}\n\n// osd beacon\nbool OSDMonitor::preprocess_beacon(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  auto beacon = static_cast<MOSDBeacon*>(op->get_req());\n  // check caps\n  auto session = beacon->get_session();\n  mon->no_reply(op);\n  if (!session) {\n    dout(10) << __func__ << \" no monitor session!\" << dendl;\n    return true;\n  }\n  if (!session->is_capable(\"osd\", MON_CAP_X)) {\n    derr << __func__ << \" received from entity \"\n         << \"with insufficient privileges \" << session->caps << dendl;\n    return true;\n  }\n  // Always forward the beacon to the leader, even if they are the same as\n  // the old one. The leader will mark as down osds that haven't sent\n  // beacon for a few minutes.\n  return false;\n}\n\nbool OSDMonitor::prepare_beacon(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  const auto beacon = static_cast<MOSDBeacon*>(op->get_req());\n  const auto src = beacon->get_orig_source();\n  dout(10) << __func__ << \" \" << *beacon\n\t   << \" from \" << src << dendl;\n  int from = src.num();\n\n  if (!src.is_osd() ||\n      !osdmap.is_up(from) ||\n      beacon->get_orig_source_addrs() != osdmap.get_addrs(from)) {\n    dout(1) << \" ignoring beacon from non-active osd.\" << from << dendl;\n    return false;\n  }\n\n  last_osd_report[from] = ceph_clock_now();\n  osd_epochs[from] = beacon->version;\n\n  for (const auto& pg : beacon->pgs) {\n    last_epoch_clean.report(pg, beacon->min_last_epoch_clean);\n  }\n  return false;\n}\n\n// ---------------\n// map helpers\n\nvoid OSDMonitor::send_latest(MonOpRequestRef op, epoch_t start)\n{\n  op->mark_osdmon_event(__func__);\n  dout(5) << \"send_latest to \" << op->get_req()->get_orig_source_inst()\n\t  << \" start \" << start << dendl;\n  if (start == 0)\n    send_full(op);\n  else\n    send_incremental(op, start);\n}\n\n\nMOSDMap *OSDMonitor::build_latest_full(uint64_t features)\n{\n  MOSDMap *r = new MOSDMap(mon->monmap->fsid, features);\n  get_version_full(osdmap.get_epoch(), features, r->maps[osdmap.get_epoch()]);\n  r->oldest_map = get_first_committed();\n  r->newest_map = osdmap.get_epoch();\n  return r;\n}\n\nMOSDMap *OSDMonitor::build_incremental(epoch_t from, epoch_t to, uint64_t features)\n{\n  dout(10) << \"build_incremental [\" << from << \"..\" << to << \"] with features \"\n\t   << std::hex << features << std::dec << dendl;\n  MOSDMap *m = new MOSDMap(mon->monmap->fsid, features);\n  m->oldest_map = get_first_committed();\n  m->newest_map = osdmap.get_epoch();\n\n  for (epoch_t e = to; e >= from && e > 0; e--) {\n    bufferlist bl;\n    int err = get_version(e, features, bl);\n    if (err == 0) {\n      assert(bl.length());\n      // if (get_version(e, bl) > 0) {\n      dout(20) << \"build_incremental    inc \" << e << \" \"\n\t       << bl.length() << \" bytes\" << dendl;\n      m->incremental_maps[e] = bl;\n    } else {\n      assert(err == -ENOENT);\n      assert(!bl.length());\n      get_version_full(e, features, bl);\n      if (bl.length() > 0) {\n      //else if (get_version(\"full\", e, bl) > 0) {\n      dout(20) << \"build_incremental   full \" << e << \" \"\n\t       << bl.length() << \" bytes\" << dendl;\n      m->maps[e] = bl;\n      } else {\n\tceph_abort();  // we should have all maps.\n      }\n    }\n  }\n  return m;\n}\n\nvoid OSDMonitor::send_full(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  dout(5) << \"send_full to \" << op->get_req()->get_orig_source_inst() << dendl;\n  mon->send_reply(op, build_latest_full(op->get_session()->con_features));\n}\n\nvoid OSDMonitor::send_incremental(MonOpRequestRef op, epoch_t first)\n{\n  op->mark_osdmon_event(__func__);\n\n  MonSession *s = op->get_session();\n  assert(s);\n\n  if (s->proxy_con) {\n    // oh, we can tell the other mon to do it\n    dout(10) << __func__ << \" asking proxying mon to send_incremental from \"\n\t     << first << dendl;\n    MRoute *r = new MRoute(s->proxy_tid, NULL);\n    r->send_osdmap_first = first;\n    s->proxy_con->send_message(r);\n    op->mark_event(\"reply: send routed send_osdmap_first reply\");\n  } else {\n    // do it ourselves\n    send_incremental(first, s, false, op);\n  }\n}\n\nvoid OSDMonitor::send_incremental(epoch_t first,\n\t\t\t\t  MonSession *session,\n\t\t\t\t  bool onetime,\n\t\t\t\t  MonOpRequestRef req)\n{\n  dout(5) << \"send_incremental [\" << first << \"..\" << osdmap.get_epoch() << \"]\"\n\t  << \" to \" << session->name << dendl;\n\n  // get feature of the peer\n  // use quorum_con_features, if it's an anonymous connection.\n  uint64_t features = session->con_features ? session->con_features :\n    mon->get_quorum_con_features();\n\n  if (first <= session->osd_epoch) {\n    dout(10) << __func__ << \" \" << session->name << \" should already have epoch \"\n\t     << session->osd_epoch << dendl;\n    first = session->osd_epoch + 1;\n  }\n\n  if (first < get_first_committed()) {\n    MOSDMap *m = new MOSDMap(osdmap.get_fsid(), features);\n    m->oldest_map = get_first_committed();\n    m->newest_map = osdmap.get_epoch();\n\n    // share removed snaps during the gap\n    get_removed_snaps_range(first, m->oldest_map, &m->gap_removed_snaps);\n\n    first = get_first_committed();\n    bufferlist bl;\n    int err = get_version_full(first, features, bl);\n    assert(err == 0);\n    assert(bl.length());\n    dout(20) << \"send_incremental starting with base full \"\n\t     << first << \" \" << bl.length() << \" bytes\" << dendl;\n    m->maps[first] = bl;\n\n    if (req) {\n      mon->send_reply(req, m);\n      session->osd_epoch = first;\n      return;\n    } else {\n      session->con->send_message(m);\n      session->osd_epoch = first;\n    }\n    first++;\n  }\n\n  while (first <= osdmap.get_epoch()) {\n    epoch_t last = std::min<epoch_t>(first + g_conf->osd_map_message_max - 1,\n\t\t\t\t     osdmap.get_epoch());\n    MOSDMap *m = build_incremental(first, last, features);\n\n    if (req) {\n      // send some maps.  it may not be all of them, but it will get them\n      // started.\n      mon->send_reply(req, m);\n    } else {\n      session->con->send_message(m);\n      first = last + 1;\n    }\n    session->osd_epoch = last;\n    if (onetime || req)\n      break;\n  }\n}\n\nvoid OSDMonitor::get_removed_snaps_range(\n  epoch_t start, epoch_t end,\n  mempool::osdmap::map<int64_t,OSDMap::snap_interval_set_t> *gap_removed_snaps)\n{\n  // we only care about pools that exist now.\n  for (auto& p : osdmap.get_pools()) {\n    auto& t = (*gap_removed_snaps)[p.first];\n    for (epoch_t epoch = start; epoch < end; ++epoch) {\n      string k = make_snap_epoch_key(p.first, epoch);\n      bufferlist v;\n      mon->store->get(OSD_SNAP_PREFIX, k, v);\n      if (v.length()) {\n\tauto q = v.cbegin();\n\tOSDMap::snap_interval_set_t snaps;\n\tdecode(snaps, q);\n\tt.union_of(snaps);\n      }\n    }\n    dout(10) << __func__ << \" \" << p.first << \" \" << t << dendl;\n  }\n}\n\nint OSDMonitor::get_version(version_t ver, bufferlist& bl)\n{\n  return get_version(ver, mon->get_quorum_con_features(), bl);\n}\n\nvoid OSDMonitor::reencode_incremental_map(bufferlist& bl, uint64_t features)\n{\n  OSDMap::Incremental inc;\n  auto q = bl.cbegin();\n  inc.decode(q);\n  // always encode with subset of osdmap's canonical features\n  uint64_t f = features & inc.encode_features;\n  dout(20) << __func__ << \" \" << inc.epoch << \" with features \" << f\n\t   << dendl;\n  bl.clear();\n  if (inc.fullmap.length()) {\n    // embedded full map?\n    OSDMap m;\n    m.decode(inc.fullmap);\n    inc.fullmap.clear();\n    m.encode(inc.fullmap, f | CEPH_FEATURE_RESERVED);\n  }\n  if (inc.crush.length()) {\n    // embedded crush map\n    CrushWrapper c;\n    auto p = inc.crush.cbegin();\n    c.decode(p);\n    inc.crush.clear();\n    c.encode(inc.crush, f);\n  }\n  inc.encode(bl, f | CEPH_FEATURE_RESERVED);\n}\n\nvoid OSDMonitor::reencode_full_map(bufferlist& bl, uint64_t features)\n{\n  OSDMap m;\n  auto q = bl.cbegin();\n  m.decode(q);\n  // always encode with subset of osdmap's canonical features\n  uint64_t f = features & m.get_encoding_features();\n  dout(20) << __func__ << \" \" << m.get_epoch() << \" with features \" << f\n\t   << dendl;\n  bl.clear();\n  m.encode(bl, f | CEPH_FEATURE_RESERVED);\n}\n\nint OSDMonitor::get_version(version_t ver, uint64_t features, bufferlist& bl)\n{\n  uint64_t significant_features = OSDMap::get_significant_features(features);\n  if (inc_osd_cache.lookup({ver, significant_features}, &bl)) {\n    return 0;\n  }\n  int ret = PaxosService::get_version(ver, bl);\n  if (ret < 0) {\n    return ret;\n  }\n  // NOTE: this check is imprecise; the OSDMap encoding features may\n  // be a subset of the latest mon quorum features, but worst case we\n  // reencode once and then cache the (identical) result under both\n  // feature masks.\n  if (significant_features !=\n      OSDMap::get_significant_features(mon->get_quorum_con_features())) {\n    reencode_incremental_map(bl, features);\n  }\n  inc_osd_cache.add({ver, significant_features}, bl);\n  return 0;\n}\n\nint OSDMonitor::get_inc(version_t ver, OSDMap::Incremental& inc)\n{\n  bufferlist inc_bl;\n  int err = get_version(ver, inc_bl);\n  ceph_assert(err == 0);\n  ceph_assert(inc_bl.length());\n\n  auto p = inc_bl.cbegin();\n  inc.decode(p);\n  dout(10) << __func__ << \"     \"\n           << \" epoch \" << inc.epoch\n           << \" inc_crc \" << inc.inc_crc\n           << \" full_crc \" << inc.full_crc\n           << \" encode_features \" << inc.encode_features << dendl;\n  return 0;\n}\n\nint OSDMonitor::get_full_from_pinned_map(version_t ver, bufferlist& bl)\n{\n  dout(10) << __func__ << \" ver \" << ver << dendl;\n\n  version_t closest_pinned = osdmap_manifest.get_lower_closest_pinned(ver);\n  if (closest_pinned == 0) {\n    return -ENOENT;\n  }\n  if (closest_pinned > ver) {\n    dout(0) << __func__ << \" pinned: \" << osdmap_manifest.pinned << dendl;\n  }\n  ceph_assert(closest_pinned <= ver);\n\n  dout(10) << __func__ << \" closest pinned ver \" << closest_pinned << dendl;\n\n  // get osdmap incremental maps and apply on top of this one.\n  bufferlist osdm_bl;\n  bool has_cached_osdmap = false;\n  for (version_t v = ver-1; v >= closest_pinned; --v) {\n    if (full_osd_cache.lookup({v, mon->get_quorum_con_features()},\n                                &osdm_bl)) {\n      dout(10) << __func__ << \" found map in cache ver \" << v << dendl;\n      closest_pinned = v;\n      has_cached_osdmap = true;\n      break;\n    }\n  }\n\n  if (!has_cached_osdmap) {\n    int err = PaxosService::get_version_full(closest_pinned, osdm_bl);\n    if (err != 0) {\n      derr << __func__ << \" closest pinned map ver \" << closest_pinned\n           << \" not available! error: \" << cpp_strerror(err) << dendl;\n    }\n    ceph_assert(err == 0);\n  }\n\n  ceph_assert(osdm_bl.length());\n\n  OSDMap osdm;\n  osdm.decode(osdm_bl);\n\n  dout(10) << __func__ << \" loaded osdmap epoch \" << closest_pinned\n           << \" e\" << osdm.epoch\n           << \" crc \" << osdm.get_crc()\n           << \" -- applying incremental maps.\" << dendl;\n\n  uint64_t encode_features = 0;\n  for (version_t v = closest_pinned + 1; v <= ver; ++v) {\n    dout(20) << __func__ << \"    applying inc epoch \" << v << dendl;\n\n    OSDMap::Incremental inc;\n    int err = get_inc(v, inc);\n    ceph_assert(err == 0);\n\n    encode_features = inc.encode_features;\n\n    err = osdm.apply_incremental(inc);\n    ceph_assert(err == 0);\n\n    // this block performs paranoid checks on map retrieval\n    if (g_conf->get_val<bool>(\"mon_debug_extra_checks\") &&\n        inc.full_crc != 0) {\n\n      uint64_t f = encode_features;\n      if (!f) {\n        f = (mon->quorum_con_features ? mon->quorum_con_features : -1);\n      }\n\n      // encode osdmap to force calculating crcs\n      bufferlist tbl;\n      osdm.encode(tbl, f | CEPH_FEATURE_RESERVED);\n      // decode osdmap to compare crcs with what's expected by incremental\n      OSDMap tosdm;\n      tosdm.decode(tbl);\n\n      if (tosdm.get_crc() != inc.full_crc) {\n        derr << __func__\n             << \"    osdmap crc mismatch! (osdmap crc \" << tosdm.get_crc()\n             << \", expected \" << inc.full_crc << \")\" << dendl;\n        ceph_assert(0 == \"osdmap crc mismatch\");\n      }\n    }\n\n    // note: we cannot add the recently computed map to the cache, as is,\n    // because we have not encoded the map into a bl.\n  }\n\n  if (!encode_features) {\n    dout(10) << __func__\n             << \" last incremental map didn't have features;\"\n             << \" defaulting to quorum's or all\" << dendl;\n    encode_features =\n      (mon->quorum_con_features ? mon->quorum_con_features : -1);\n  }\n  osdm.encode(bl, encode_features | CEPH_FEATURE_RESERVED);\n\n  return 0;\n}\n\nint OSDMonitor::get_version_full(version_t ver, bufferlist& bl)\n{\n  return get_version_full(ver, mon->get_quorum_con_features(), bl);\n}\n\nint OSDMonitor::get_version_full(version_t ver, uint64_t features,\n\t\t\t\t bufferlist& bl)\n{\n  uint64_t significant_features = OSDMap::get_significant_features(features);\n  if (full_osd_cache.lookup({ver, significant_features}, &bl)) {\n    return 0;\n  }\n  int ret = PaxosService::get_version_full(ver, bl);\n  if (ret == -ENOENT) {\n    // build map?\n    ret = get_full_from_pinned_map(ver, bl);\n  }\n  if (ret < 0) {\n    return ret;\n  }\n  // NOTE: this check is imprecise; the OSDMap encoding features may\n  // be a subset of the latest mon quorum features, but worst case we\n  // reencode once and then cache the (identical) result under both\n  // feature masks.\n  if (significant_features !=\n      OSDMap::get_significant_features(mon->get_quorum_con_features())) {\n    reencode_full_map(bl, features);\n  }\n  full_osd_cache.add({ver, significant_features}, bl);\n  return 0;\n}\n\nepoch_t OSDMonitor::blacklist(const entity_addrvec_t& av, utime_t until)\n{\n  dout(10) << \"blacklist \" << av << \" until \" << until << dendl;\n  for (auto& a : av.v) {\n    pending_inc.new_blacklist[a] = until;\n  }\n  return pending_inc.epoch;\n}\n\nepoch_t OSDMonitor::blacklist(const entity_addr_t& a, utime_t until)\n{\n  dout(10) << \"blacklist \" << a << \" until \" << until << dendl;\n  pending_inc.new_blacklist[a] = until;\n  return pending_inc.epoch;\n}\n\n\nvoid OSDMonitor::check_osdmap_subs()\n{\n  dout(10) << __func__ << dendl;\n  if (!osdmap.get_epoch()) {\n    return;\n  }\n  auto osdmap_subs = mon->session_map.subs.find(\"osdmap\");\n  if (osdmap_subs == mon->session_map.subs.end()) {\n    return;\n  }\n  auto p = osdmap_subs->second->begin();\n  while (!p.end()) {\n    auto sub = *p;\n    ++p;\n    check_osdmap_sub(sub);\n  }\n}\n\nvoid OSDMonitor::check_osdmap_sub(Subscription *sub)\n{\n  dout(10) << __func__ << \" \" << sub << \" next \" << sub->next\n\t   << (sub->onetime ? \" (onetime)\":\" (ongoing)\") << dendl;\n  if (sub->next <= osdmap.get_epoch()) {\n    if (sub->next >= 1)\n      send_incremental(sub->next, sub->session, sub->incremental_onetime);\n    else\n      sub->session->con->send_message(build_latest_full(sub->session->con_features));\n    if (sub->onetime)\n      mon->session_map.remove_sub(sub);\n    else\n      sub->next = osdmap.get_epoch() + 1;\n  }\n}\n\nvoid OSDMonitor::check_pg_creates_subs()\n{\n  if (!osdmap.get_num_up_osds()) {\n    return;\n  }\n  assert(osdmap.get_up_osd_features() & CEPH_FEATURE_MON_STATEFUL_SUB);\n  mon->with_session_map([this](const MonSessionMap& session_map) {\n      auto pg_creates_subs = session_map.subs.find(\"osd_pg_creates\");\n      if (pg_creates_subs == session_map.subs.end()) {\n\treturn;\n      }\n      for (auto sub : *pg_creates_subs->second) {\n\tcheck_pg_creates_sub(sub);\n      }\n    });\n}\n\nvoid OSDMonitor::check_pg_creates_sub(Subscription *sub)\n{\n  dout(20) << __func__ << \" .. \" << sub->session->name << dendl;\n  assert(sub->type == \"osd_pg_creates\");\n  // only send these if the OSD is up.  we will check_subs() when they do\n  // come up so they will get the creates then.\n  if (sub->session->name.is_osd() &&\n      mon->osdmon()->osdmap.is_up(sub->session->name.num())) {\n    sub->next = send_pg_creates(sub->session->name.num(),\n\t\t\t\tsub->session->con.get(),\n\t\t\t\tsub->next);\n  }\n}\n\nvoid OSDMonitor::do_application_enable(int64_t pool_id,\n                                       const std::string &app_name,\n\t\t\t\t       const std::string &app_key,\n\t\t\t\t       const std::string &app_value)\n{\n  assert(paxos->is_plugged() && is_writeable());\n\n  dout(20) << __func__ << \": pool_id=\" << pool_id << \", app_name=\" << app_name\n           << dendl;\n\n  assert(osdmap.require_osd_release >= CEPH_RELEASE_LUMINOUS);\n\n  auto pp = osdmap.get_pg_pool(pool_id);\n  assert(pp != nullptr);\n\n  pg_pool_t p = *pp;\n  if (pending_inc.new_pools.count(pool_id)) {\n    p = pending_inc.new_pools[pool_id];\n  }\n\n  if (app_key.empty()) {\n    p.application_metadata.insert({app_name, {}});\n  } else {\n    p.application_metadata.insert({app_name, {{app_key, app_value}}});\n  }\n  p.last_change = pending_inc.epoch;\n  pending_inc.new_pools[pool_id] = p;\n}\n\nunsigned OSDMonitor::scan_for_creating_pgs(\n  const mempool::osdmap::map<int64_t,pg_pool_t>& pools,\n  const mempool::osdmap::set<int64_t>& removed_pools,\n  utime_t modified,\n  creating_pgs_t* creating_pgs) const\n{\n  unsigned queued = 0;\n  for (auto& p : pools) {\n    int64_t poolid = p.first;\n    const pg_pool_t& pool = p.second;\n    int ruleno = osdmap.crush->find_rule(pool.get_crush_rule(),\n\t\t\t\t\t pool.get_type(), pool.get_size());\n    if (ruleno < 0 || !osdmap.crush->rule_exists(ruleno))\n      continue;\n\n    const auto last_scan_epoch = creating_pgs->last_scan_epoch;\n    const auto created = pool.get_last_change();\n    if (last_scan_epoch && created <= last_scan_epoch) {\n      dout(10) << __func__ << \" no change in pool \" << poolid\n\t       << \" \" << pool << dendl;\n      continue;\n    }\n    if (removed_pools.count(poolid)) {\n      dout(10) << __func__ << \" pool is being removed: \" << poolid\n\t       << \" \" << pool << dendl;\n      continue;\n    }\n    dout(10) << __func__ << \" queueing pool create for \" << poolid\n\t     << \" \" << pool << dendl;\n    if (creating_pgs->create_pool(poolid, pool.get_pg_num(),\n\t\t\t\t  created, modified)) {\n      queued++;\n    }\n  }\n  return queued;\n}\n\nvoid OSDMonitor::update_creating_pgs()\n{\n  dout(10) << __func__ << \" \" << creating_pgs.pgs.size() << \" pgs creating, \"\n\t   << creating_pgs.queue.size() << \" pools in queue\" << dendl;\n  decltype(creating_pgs_by_osd_epoch) new_pgs_by_osd_epoch;\n  std::lock_guard<std::mutex> l(creating_pgs_lock);\n  for (const auto& pg : creating_pgs.pgs) {\n    int acting_primary = -1;\n    auto pgid = pg.first;\n    if (!osdmap.pg_exists(pgid)) {\n      dout(20) << __func__ << \" ignoring \" << pgid << \" which should not exist\"\n\t       << dendl;\n      continue;\n    }\n    auto mapped = pg.second.first;\n    dout(20) << __func__ << \" looking up \" << pgid << \"@\" << mapped << dendl;\n    spg_t spgid(pgid);\n    mapping.get_primary_and_shard(pgid, &acting_primary, &spgid);\n    // check the previous creating_pgs, look for the target to whom the pg was\n    // previously mapped\n    for (const auto& pgs_by_epoch : creating_pgs_by_osd_epoch) {\n      const auto last_acting_primary = pgs_by_epoch.first;\n      for (auto& pgs: pgs_by_epoch.second) {\n\tif (pgs.second.count(spgid)) {\n\t  if (last_acting_primary == acting_primary) {\n\t    mapped = pgs.first;\n\t  } else {\n\t    dout(20) << __func__ << \" \" << pgid << \" \"\n\t\t     << \" acting_primary:\" << last_acting_primary\n\t\t     << \" -> \" << acting_primary << dendl;\n\t    // note epoch if the target of the create message changed.\n\t    mapped = mapping.get_epoch();\n          }\n          break;\n        } else {\n\t  // newly creating\n\t  mapped = mapping.get_epoch();\n\t}\n      }\n    }\n    dout(10) << __func__ << \" will instruct osd.\" << acting_primary\n\t     << \" to create \" << pgid << \"@\" << mapped << dendl;\n    new_pgs_by_osd_epoch[acting_primary][mapped].insert(spgid);\n  }\n  creating_pgs_by_osd_epoch = std::move(new_pgs_by_osd_epoch);\n  creating_pgs_epoch = mapping.get_epoch();\n}\n\nepoch_t OSDMonitor::send_pg_creates(int osd, Connection *con, epoch_t next) const\n{\n  dout(30) << __func__ << \" osd.\" << osd << \" next=\" << next\n\t   << \" \" << creating_pgs_by_osd_epoch << dendl;\n  std::lock_guard<std::mutex> l(creating_pgs_lock);\n  if (creating_pgs_epoch <= creating_pgs.last_scan_epoch) {\n    dout(20) << __func__\n\t     << \" not using stale creating_pgs@\" << creating_pgs_epoch << dendl;\n    // the subscribers will be updated when the mapping is completed anyway\n    return next;\n  }\n  auto creating_pgs_by_epoch = creating_pgs_by_osd_epoch.find(osd);\n  if (creating_pgs_by_epoch == creating_pgs_by_osd_epoch.end())\n    return next;\n  assert(!creating_pgs_by_epoch->second.empty());\n\n  MOSDPGCreate *oldm = nullptr; // for pre-mimic OSD compat\n  MOSDPGCreate2 *m = nullptr;\n\n  // for now, keep sending legacy creates.  Until we sort out how to address\n  // racing mon create resends and splits, we are better off with the less\n  // drastic impacts of http://tracker.ceph.com/issues/22165.  The legacy\n  // create message handling path in the OSD still does the old thing where\n  // the pg history is pregenerated and it's instantiated at the latest osdmap\n  // epoch; child pgs are simply not created.\n  bool old = true; // !HAVE_FEATURE(con->get_features(), SERVER_NAUTILUS);\n\n  epoch_t last = 0;\n  for (auto epoch_pgs = creating_pgs_by_epoch->second.lower_bound(next);\n       epoch_pgs != creating_pgs_by_epoch->second.end(); ++epoch_pgs) {\n    auto epoch = epoch_pgs->first;\n    auto& pgs = epoch_pgs->second;\n    dout(20) << __func__ << \" osd.\" << osd << \" from \" << next\n             << \" : epoch \" << epoch << \" \" << pgs.size() << \" pgs\" << dendl;\n    last = epoch;\n    for (auto& pg : pgs) {\n      // Need the create time from the monitor using its clock to set\n      // last_scrub_stamp upon pg creation.\n      auto create = creating_pgs.pgs.find(pg.pgid);\n      assert(create != creating_pgs.pgs.end());\n      if (old) {\n\tif (!oldm) {\n\t  oldm = new MOSDPGCreate(creating_pgs_epoch);\n\t}\n\toldm->mkpg.emplace(pg.pgid,\n\t\t\t   pg_create_t{create->second.first, pg.pgid, 0});\n\toldm->ctimes.emplace(pg.pgid, create->second.second);\n      } else {\n\tif (!m) {\n\t  m = new MOSDPGCreate2(creating_pgs_epoch);\n\t}\n\tm->pgs.emplace(pg, create->second);\n      }\n      dout(20) << __func__ << \" will create \" << pg\n\t       << \" at \" << create->second.first << dendl;\n    }\n  }\n  if (m) {\n    con->send_message(m);\n  } else if (oldm) {\n    con->send_message(oldm);\n  } else {\n    dout(20) << __func__ << \" osd.\" << osd << \" from \" << next\n             << \" has nothing to send\" << dendl;\n    return next;\n  }\n\n  // sub is current through last + 1\n  return last + 1;\n}\n\n// TICK\n\n\nvoid OSDMonitor::tick()\n{\n  if (!is_active()) return;\n\n  dout(10) << osdmap << dendl;\n\n  // always update osdmap manifest, regardless of being the leader.\n  load_osdmap_manifest();\n\n  if (!mon->is_leader()) return;\n\n  bool do_propose = false;\n  utime_t now = ceph_clock_now();\n\n  if (handle_osd_timeouts(now, last_osd_report)) {\n    do_propose = true;\n  }\n\n  // mark osds down?\n  if (check_failures(now)) {\n    do_propose = true;\n  }\n\n  // Force a proposal if we need to prune; pruning is performed on\n  // ``encode_pending()``, hence why we need to regularly trigger a proposal\n  // even if there's nothing going on.\n  if (is_prune_enabled() && should_prune()) {\n    do_propose = true;\n  }\n\n  // mark down osds out?\n\n  /* can_mark_out() checks if we can mark osds as being out. The -1 has no\n   * influence at all. The decision is made based on the ratio of \"in\" osds,\n   * and the function returns false if this ratio is lower that the minimum\n   * ratio set by g_conf->mon_osd_min_in_ratio. So it's not really up to us.\n   */\n  if (can_mark_out(-1)) {\n    set<int> down_cache;  // quick cache of down subtrees\n\n    map<int,utime_t>::iterator i = down_pending_out.begin();\n    while (i != down_pending_out.end()) {\n      int o = i->first;\n      utime_t down = now;\n      down -= i->second;\n      ++i;\n\n      if (osdmap.is_down(o) &&\n\t  osdmap.is_in(o) &&\n\t  can_mark_out(o)) {\n\tutime_t orig_grace(g_conf->mon_osd_down_out_interval, 0);\n\tutime_t grace = orig_grace;\n\tdouble my_grace = 0.0;\n\n\tif (g_conf->mon_osd_adjust_down_out_interval) {\n\t  // scale grace period the same way we do the heartbeat grace.\n\t  const osd_xinfo_t& xi = osdmap.get_xinfo(o);\n\t  double halflife = (double)g_conf->mon_osd_laggy_halflife;\n\t  double decay_k = ::log(.5) / halflife;\n\t  double decay = exp((double)down * decay_k);\n\t  dout(20) << \"osd.\" << o << \" laggy halflife \" << halflife << \" decay_k \" << decay_k\n\t\t   << \" down for \" << down << \" decay \" << decay << dendl;\n\t  my_grace = decay * (double)xi.laggy_interval * xi.laggy_probability;\n\t  grace += my_grace;\n\t}\n\n\t// is this an entire large subtree down?\n\tif (g_conf->mon_osd_down_out_subtree_limit.length()) {\n\t  int type = osdmap.crush->get_type_id(g_conf->mon_osd_down_out_subtree_limit);\n\t  if (type > 0) {\n\t    if (osdmap.containing_subtree_is_down(cct, o, type, &down_cache)) {\n\t      dout(10) << \"tick entire containing \" << g_conf->mon_osd_down_out_subtree_limit\n\t\t       << \" subtree for osd.\" << o << \" is down; resetting timer\" << dendl;\n\t      // reset timer, too.\n\t      down_pending_out[o] = now;\n\t      continue;\n\t    }\n\t  }\n\t}\n\n        bool down_out = !osdmap.is_destroyed(o) &&\n          g_conf->mon_osd_down_out_interval > 0 && down.sec() >= grace;\n        bool destroyed_out = osdmap.is_destroyed(o) &&\n          g_conf->mon_osd_destroyed_out_interval > 0 &&\n        // this is not precise enough as we did not make a note when this osd\n        // was marked as destroyed, but let's not bother with that\n        // complexity for now.\n          down.sec() >= g_conf->mon_osd_destroyed_out_interval;\n        if (down_out || destroyed_out) {\n\t  dout(10) << \"tick marking osd.\" << o << \" OUT after \" << down\n\t\t   << \" sec (target \" << grace << \" = \" << orig_grace << \" + \" << my_grace << \")\" << dendl;\n\t  pending_inc.new_weight[o] = CEPH_OSD_OUT;\n\n\t  // set the AUTOOUT bit.\n\t  if (pending_inc.new_state.count(o) == 0)\n\t    pending_inc.new_state[o] = 0;\n\t  pending_inc.new_state[o] |= CEPH_OSD_AUTOOUT;\n\n\t  // remember previous weight\n\t  if (pending_inc.new_xinfo.count(o) == 0)\n\t    pending_inc.new_xinfo[o] = osdmap.osd_xinfo[o];\n\t  pending_inc.new_xinfo[o].old_weight = osdmap.osd_weight[o];\n\n\t  do_propose = true;\n\n\t  mon->clog->info() << \"Marking osd.\" << o << \" out (has been down for \"\n                            << int(down.sec()) << \" seconds)\";\n\t} else\n\t  continue;\n      }\n\n      down_pending_out.erase(o);\n    }\n  } else {\n    dout(10) << \"tick NOOUT flag set, not checking down osds\" << dendl;\n  }\n\n  // expire blacklisted items?\n  for (ceph::unordered_map<entity_addr_t,utime_t>::iterator p = osdmap.blacklist.begin();\n       p != osdmap.blacklist.end();\n       ++p) {\n    if (p->second < now) {\n      dout(10) << \"expiring blacklist item \" << p->first << \" expired \" << p->second << \" < now \" << now << dendl;\n      pending_inc.old_blacklist.push_back(p->first);\n      do_propose = true;\n    }\n  }\n\n  if (try_prune_purged_snaps()) {\n    do_propose = true;\n  }\n\n  if (update_pools_status())\n    do_propose = true;\n\n  if (do_propose ||\n      !pending_inc.new_pg_temp.empty())  // also propose if we adjusted pg_temp\n    propose_pending();\n}\n\nbool OSDMonitor::handle_osd_timeouts(const utime_t &now,\n\t\t\t\t     std::map<int,utime_t> &last_osd_report)\n{\n  utime_t timeo(g_conf->mon_osd_report_timeout, 0);\n  if (now - mon->get_leader_since() < timeo) {\n    // We haven't been the leader for long enough to consider OSD timeouts\n    return false;\n  }\n\n  int max_osd = osdmap.get_max_osd();\n  bool new_down = false;\n\n  for (int i=0; i < max_osd; ++i) {\n    dout(30) << __func__ << \": checking up on osd \" << i << dendl;\n    if (!osdmap.exists(i)) {\n      last_osd_report.erase(i); // if any\n      continue;\n    }\n    if (!osdmap.is_up(i))\n      continue;\n    const std::map<int,utime_t>::const_iterator t = last_osd_report.find(i);\n    if (t == last_osd_report.end()) {\n      // it wasn't in the map; start the timer.\n      last_osd_report[i] = now;\n    } else if (can_mark_down(i)) {\n      utime_t diff = now - t->second;\n      if (diff > timeo) {\n\tmon->clog->info() << \"osd.\" << i << \" marked down after no beacon for \"\n\t\t\t  << diff << \" seconds\";\n\tderr << \"no beacon from osd.\" << i << \" since \" << t->second\n\t     << \", \" << diff << \" seconds ago.  marking down\" << dendl;\n\tpending_inc.new_state[i] = CEPH_OSD_UP;\n\tnew_down = true;\n      }\n    }\n  }\n  return new_down;\n}\n\nvoid OSDMonitor::dump_info(Formatter *f)\n{\n  f->open_object_section(\"osdmap\");\n  osdmap.dump(f);\n  f->close_section();\n\n  f->open_array_section(\"osd_metadata\");\n  for (int i=0; i<osdmap.get_max_osd(); ++i) {\n    if (osdmap.exists(i)) {\n      f->open_object_section(\"osd\");\n      f->dump_unsigned(\"id\", i);\n      dump_osd_metadata(i, f, NULL);\n      f->close_section();\n    }\n  }\n  f->close_section();\n\n  f->dump_unsigned(\"osdmap_first_committed\", get_first_committed());\n  f->dump_unsigned(\"osdmap_last_committed\", get_last_committed());\n\n  f->open_object_section(\"crushmap\");\n  osdmap.crush->dump(f);\n  f->close_section();\n\n  if (has_osdmap_manifest) {\n    f->open_object_section(\"osdmap_manifest\");\n    osdmap_manifest.dump(f);\n    f->close_section();\n  }\n}\n\nnamespace {\n  enum osd_pool_get_choices {\n    SIZE, MIN_SIZE,\n    PG_NUM, PGP_NUM, CRUSH_RULE, HASHPSPOOL, EC_OVERWRITES,\n    NODELETE, NOPGCHANGE, NOSIZECHANGE,\n    WRITE_FADVISE_DONTNEED, NOSCRUB, NODEEP_SCRUB,\n    HIT_SET_TYPE, HIT_SET_PERIOD, HIT_SET_COUNT, HIT_SET_FPP,\n    USE_GMT_HITSET, AUID, TARGET_MAX_OBJECTS, TARGET_MAX_BYTES,\n    CACHE_TARGET_DIRTY_RATIO, CACHE_TARGET_DIRTY_HIGH_RATIO,\n    CACHE_TARGET_FULL_RATIO,\n    CACHE_MIN_FLUSH_AGE, CACHE_MIN_EVICT_AGE,\n    ERASURE_CODE_PROFILE, MIN_READ_RECENCY_FOR_PROMOTE,\n    MIN_WRITE_RECENCY_FOR_PROMOTE, FAST_READ,\n    HIT_SET_GRADE_DECAY_RATE, HIT_SET_SEARCH_LAST_N,\n    SCRUB_MIN_INTERVAL, SCRUB_MAX_INTERVAL, DEEP_SCRUB_INTERVAL,\n    RECOVERY_PRIORITY, RECOVERY_OP_PRIORITY, SCRUB_PRIORITY,\n    COMPRESSION_MODE, COMPRESSION_ALGORITHM, COMPRESSION_REQUIRED_RATIO,\n    COMPRESSION_MAX_BLOB_SIZE, COMPRESSION_MIN_BLOB_SIZE,\n    CSUM_TYPE, CSUM_MAX_BLOCK, CSUM_MIN_BLOCK };\n\n  std::set<osd_pool_get_choices>\n    subtract_second_from_first(const std::set<osd_pool_get_choices>& first,\n\t\t\t\tconst std::set<osd_pool_get_choices>& second)\n    {\n      std::set<osd_pool_get_choices> result;\n      std::set_difference(first.begin(), first.end(),\n\t\t\t  second.begin(), second.end(),\n\t\t\t  std::inserter(result, result.end()));\n      return result;\n    }\n}\n\n\nbool OSDMonitor::preprocess_command(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MMonCommand *m = static_cast<MMonCommand*>(op->get_req());\n  int r = 0;\n  bufferlist rdata;\n  stringstream ss, ds;\n\n  cmdmap_t cmdmap;\n  if (!cmdmap_from_json(m->cmd, &cmdmap, ss)) {\n    string rs = ss.str();\n    mon->reply_command(op, -EINVAL, rs, get_last_committed());\n    return true;\n  }\n\n  MonSession *session = m->get_session();\n  if (!session) {\n    mon->reply_command(op, -EACCES, \"access denied\", get_last_committed());\n    return true;\n  }\n\n  string prefix;\n  cmd_getval(cct, cmdmap, \"prefix\", prefix);\n\n  string format;\n  cmd_getval(cct, cmdmap, \"format\", format, string(\"plain\"));\n  boost::scoped_ptr<Formatter> f(Formatter::create(format));\n\n  if (prefix == \"osd stat\") {\n    osdmap.print_summary(f.get(), ds, \"\", true);\n    if (f)\n      f->flush(rdata);\n    else\n      rdata.append(ds);\n  }\n  else if (prefix == \"osd dump\" ||\n\t   prefix == \"osd tree\" ||\n\t   prefix == \"osd tree-from\" ||\n\t   prefix == \"osd ls\" ||\n\t   prefix == \"osd getmap\" ||\n\t   prefix == \"osd getcrushmap\" ||\n\t   prefix == \"osd ls-tree\") {\n    string val;\n\n    epoch_t epoch = 0;\n    int64_t epochnum;\n    cmd_getval(cct, cmdmap, \"epoch\", epochnum, (int64_t)osdmap.get_epoch());\n    epoch = epochnum;\n    \n    bufferlist osdmap_bl;\n    int err = get_version_full(epoch, osdmap_bl);\n    if (err == -ENOENT) {\n      r = -ENOENT;\n      ss << \"there is no map for epoch \" << epoch;\n      goto reply;\n    }\n    assert(err == 0);\n    assert(osdmap_bl.length());\n\n    OSDMap *p;\n    if (epoch == osdmap.get_epoch()) {\n      p = &osdmap;\n    } else {\n      p = new OSDMap;\n      p->decode(osdmap_bl);\n    }\n\n    auto sg = make_scope_guard([&] {\n      if (p != &osdmap) {\n        delete p;\n      }\n    });\n\n    if (prefix == \"osd dump\") {\n      stringstream ds;\n      if (f) {\n\tf->open_object_section(\"osdmap\");\n\tp->dump(f.get());\n\tf->close_section();\n\tf->flush(ds);\n      } else {\n\tp->print(ds);\n      }\n      rdata.append(ds);\n      if (!f)\n\tds << \" \";\n    } else if (prefix == \"osd ls\") {\n      if (f) {\n\tf->open_array_section(\"osds\");\n\tfor (int i = 0; i < osdmap.get_max_osd(); i++) {\n\t  if (osdmap.exists(i)) {\n\t    f->dump_int(\"osd\", i);\n\t  }\n\t}\n\tf->close_section();\n\tf->flush(ds);\n      } else {\n\tbool first = true;\n\tfor (int i = 0; i < osdmap.get_max_osd(); i++) {\n\t  if (osdmap.exists(i)) {\n\t    if (!first)\n\t      ds << \"\\n\";\n\t    first = false;\n\t    ds << i;\n\t  }\n\t}\n      }\n      rdata.append(ds);\n    } else if (prefix == \"osd tree\" || prefix == \"osd tree-from\") {\n      string bucket;\n      if (prefix == \"osd tree-from\") {\n        cmd_getval(cct, cmdmap, \"bucket\", bucket);\n        if (!osdmap.crush->name_exists(bucket)) {\n          ss << \"bucket '\" << bucket << \"' does not exist\";\n          r = -ENOENT;\n          goto reply;\n        }\n        int id = osdmap.crush->get_item_id(bucket);\n        if (id >= 0) {\n          ss << \"\\\"\" << bucket << \"\\\" is not a bucket\";\n          r = -EINVAL;\n          goto reply;\n        }\n      }\n\n      vector<string> states;\n      cmd_getval(cct, cmdmap, \"states\", states);\n      unsigned filter = 0;\n      for (auto& s : states) {\n\tif (s == \"up\") {\n\t  filter |= OSDMap::DUMP_UP;\n\t} else if (s == \"down\") {\n\t  filter |= OSDMap::DUMP_DOWN;\n\t} else if (s == \"in\") {\n\t  filter |= OSDMap::DUMP_IN;\n\t} else if (s == \"out\") {\n\t  filter |= OSDMap::DUMP_OUT;\n\t} else if (s == \"destroyed\") {\n\t  filter |= OSDMap::DUMP_DESTROYED;\n\t} else {\n\t  ss << \"unrecognized state '\" << s << \"'\";\n\t  r = -EINVAL;\n\t  goto reply;\n\t}\n      }\n      if ((filter & (OSDMap::DUMP_IN|OSDMap::DUMP_OUT)) ==\n\t  (OSDMap::DUMP_IN|OSDMap::DUMP_OUT)) {\n        ss << \"cannot specify both 'in' and 'out'\";\n        r = -EINVAL;\n        goto reply;\n      }\n      if (((filter & (OSDMap::DUMP_UP|OSDMap::DUMP_DOWN)) ==\n\t   (OSDMap::DUMP_UP|OSDMap::DUMP_DOWN)) ||\n           ((filter & (OSDMap::DUMP_UP|OSDMap::DUMP_DESTROYED)) ==\n           (OSDMap::DUMP_UP|OSDMap::DUMP_DESTROYED)) ||\n           ((filter & (OSDMap::DUMP_DOWN|OSDMap::DUMP_DESTROYED)) ==\n           (OSDMap::DUMP_DOWN|OSDMap::DUMP_DESTROYED))) {\n\tss << \"can specify only one of 'up', 'down' and 'destroyed'\";\n\tr = -EINVAL;\n\tgoto reply;\n      }\n      if (f) {\n\tf->open_object_section(\"tree\");\n\tp->print_tree(f.get(), NULL, filter, bucket);\n\tf->close_section();\n\tf->flush(ds);\n      } else {\n\tp->print_tree(NULL, &ds, filter, bucket);\n      }\n      rdata.append(ds);\n    } else if (prefix == \"osd getmap\") {\n      rdata.append(osdmap_bl);\n      ss << \"got osdmap epoch \" << p->get_epoch();\n    } else if (prefix == \"osd getcrushmap\") {\n      p->crush->encode(rdata, mon->get_quorum_con_features());\n      ss << p->get_crush_version();\n    } else if (prefix == \"osd ls-tree\") {\n      string bucket_name;\n      cmd_getval(cct, cmdmap, \"name\", bucket_name);\n      set<int> osds;\n      r = p->get_osds_by_bucket_name(bucket_name, &osds);\n      if (r == -ENOENT) {\n        ss << \"\\\"\" << bucket_name << \"\\\" does not exist\";\n        goto reply;\n      } else if (r < 0) {\n        ss << \"can not parse bucket name:\\\"\" << bucket_name << \"\\\"\";\n        goto reply;\n      }\n\n      if (f) {\n        f->open_array_section(\"osds\");\n        for (auto &i : osds) {\n          if (osdmap.exists(i)) {\n            f->dump_int(\"osd\", i);\n          }\n        }\n        f->close_section();\n        f->flush(ds);\n      } else {\n        bool first = true;\n        for (auto &i : osds) {\n          if (osdmap.exists(i)) {\n            if (!first)\n              ds << \"\\n\";\n            first = false;\n            ds << i;\n          }\n        }\n      }\n\n      rdata.append(ds);\n    }\n  } else if (prefix == \"osd getmaxosd\") {\n    if (f) {\n      f->open_object_section(\"getmaxosd\");\n      f->dump_unsigned(\"epoch\", osdmap.get_epoch());\n      f->dump_int(\"max_osd\", osdmap.get_max_osd());\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ds << \"max_osd = \" << osdmap.get_max_osd() << \" in epoch \" << osdmap.get_epoch();\n      rdata.append(ds);\n    }\n  } else if (prefix == \"osd utilization\") {\n    string out;\n    osdmap.summarize_mapping_stats(NULL, NULL, &out, f.get());\n    if (f)\n      f->flush(rdata);\n    else\n      rdata.append(out);\n    r = 0;\n    goto reply;\n  } else if (prefix  == \"osd find\") {\n    int64_t osd;\n    if (!cmd_getval(cct, cmdmap, \"id\", osd)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap[\"id\"]) << \"'\";\n      r = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.exists(osd)) {\n      ss << \"osd.\" << osd << \" does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    f->open_object_section(\"osd_location\");\n    f->dump_int(\"osd\", osd);\n    f->dump_object(\"addrs\", osdmap.get_addrs(osd));\n    f->open_object_section(\"crush_location\");\n    map<string,string> loc = osdmap.crush->get_full_location(osd);\n    for (map<string,string>::iterator p = loc.begin(); p != loc.end(); ++p)\n      f->dump_string(p->first.c_str(), p->second);\n    f->close_section();\n    f->close_section();\n    f->flush(rdata);\n  } else if (prefix == \"osd metadata\") {\n    int64_t osd = -1;\n    if (cmd_vartype_stringify(cmdmap[\"id\"]).size() &&\n        !cmd_getval(cct, cmdmap, \"id\", osd)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap[\"id\"]) << \"'\";\n      r = -EINVAL;\n      goto reply;\n    }\n    if (osd >= 0 && !osdmap.exists(osd)) {\n      ss << \"osd.\" << osd << \" does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    if (osd >= 0) {\n      f->open_object_section(\"osd_metadata\");\n      f->dump_unsigned(\"id\", osd);\n      r = dump_osd_metadata(osd, f.get(), &ss);\n      if (r < 0)\n        goto reply;\n      f->close_section();\n    } else {\n      r = 0;\n      f->open_array_section(\"osd_metadata\");\n      for (int i=0; i<osdmap.get_max_osd(); ++i) {\n        if (osdmap.exists(i)) {\n          f->open_object_section(\"osd\");\n          f->dump_unsigned(\"id\", i);\n          r = dump_osd_metadata(i, f.get(), NULL);\n          if (r == -EINVAL || r == -ENOENT) {\n            // Drop error, continue to get other daemons' metadata\n            dout(4) << \"No metadata for osd.\" << i << dendl;\n            r = 0;\n          } else if (r < 0) {\n            // Unexpected error\n            goto reply;\n          }\n          f->close_section();\n        }\n      }\n      f->close_section();\n    }\n    f->flush(rdata);\n  } else if (prefix == \"osd versions\") {\n    if (!f)\n      f.reset(Formatter::create(\"json-pretty\"));\n    count_metadata(\"ceph_version\", f.get());\n    f->flush(rdata);\n    r = 0;\n  } else if (prefix == \"osd count-metadata\") {\n    if (!f)\n      f.reset(Formatter::create(\"json-pretty\"));\n    string field;\n    cmd_getval(cct, cmdmap, \"property\", field);\n    count_metadata(field, f.get());\n    f->flush(rdata);\n    r = 0;\n  } else if (prefix == \"osd map\") {\n    string poolstr, objstr, namespacestr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    cmd_getval(cct, cmdmap, \"object\", objstr);\n    cmd_getval(cct, cmdmap, \"nspace\", namespacestr);\n\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"pool \" << poolstr << \" does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    object_locator_t oloc(pool, namespacestr);\n    object_t oid(objstr);\n    pg_t pgid = osdmap.object_locator_to_pg(oid, oloc);\n    pg_t mpgid = osdmap.raw_pg_to_pg(pgid);\n    vector<int> up, acting;\n    int up_p, acting_p;\n    osdmap.pg_to_up_acting_osds(mpgid, &up, &up_p, &acting, &acting_p);\n\n    string fullobjname;\n    if (!namespacestr.empty())\n      fullobjname = namespacestr + string(\"/\") + oid.name;\n    else\n      fullobjname = oid.name;\n    if (f) {\n      f->open_object_section(\"osd_map\");\n      f->dump_unsigned(\"epoch\", osdmap.get_epoch());\n      f->dump_string(\"pool\", poolstr);\n      f->dump_int(\"pool_id\", pool);\n      f->dump_stream(\"objname\") << fullobjname;\n      f->dump_stream(\"raw_pgid\") << pgid;\n      f->dump_stream(\"pgid\") << mpgid;\n      f->open_array_section(\"up\");\n      for (vector<int>::iterator p = up.begin(); p != up.end(); ++p)\n        f->dump_int(\"osd\", *p);\n      f->close_section();\n      f->dump_int(\"up_primary\", up_p);\n      f->open_array_section(\"acting\");\n      for (vector<int>::iterator p = acting.begin(); p != acting.end(); ++p)\n        f->dump_int(\"osd\", *p);\n      f->close_section();\n      f->dump_int(\"acting_primary\", acting_p);\n      f->close_section(); // osd_map\n      f->flush(rdata);\n    } else {\n      ds << \"osdmap e\" << osdmap.get_epoch()\n        << \" pool '\" << poolstr << \"' (\" << pool << \")\"\n        << \" object '\" << fullobjname << \"' ->\"\n        << \" pg \" << pgid << \" (\" << mpgid << \")\"\n        << \" -> up (\" << pg_vector_string(up) << \", p\" << up_p << \") acting (\"\n        << pg_vector_string(acting) << \", p\" << acting_p << \")\";\n      rdata.append(ds);\n    }\n\n  } else if (prefix == \"pg map\") {\n    pg_t pgid;\n    string pgidstr;\n    cmd_getval(cct, cmdmap, \"pgid\", pgidstr);\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      r = -EINVAL;\n      goto reply;\n    }\n    vector<int> up, acting;\n    if (!osdmap.have_pg_pool(pgid.pool())) {\n      ss << \"pg '\" << pgidstr << \"' does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    pg_t mpgid = osdmap.raw_pg_to_pg(pgid);\n    osdmap.pg_to_up_acting_osds(pgid, up, acting);\n    if (f) {\n      f->open_object_section(\"pg_map\");\n      f->dump_unsigned(\"epoch\", osdmap.get_epoch());\n      f->dump_stream(\"raw_pgid\") << pgid;\n      f->dump_stream(\"pgid\") << mpgid;\n      f->open_array_section(\"up\");\n      for (auto osd : up) {\n\tf->dump_int(\"up_osd\", osd);\n      }\n      f->close_section();\n      f->open_array_section(\"acting\");\n      for (auto osd : acting) {\n\tf->dump_int(\"acting_osd\", osd);\n      }\n      f->close_section();\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ds << \"osdmap e\" << osdmap.get_epoch()\n         << \" pg \" << pgid << \" (\" << mpgid << \")\"\n         << \" -> up \" << up << \" acting \" << acting;\n      rdata.append(ds);\n    }\n    goto reply;\n\n  } else if (prefix == \"osd lspools\") {\n    int64_t auid;\n    cmd_getval(cct, cmdmap, \"auid\", auid, int64_t(0));\n    if (f)\n      f->open_array_section(\"pools\");\n    for (map<int64_t, pg_pool_t>::iterator p = osdmap.pools.begin();\n\t p != osdmap.pools.end();\n\t ++p) {\n      if (!auid || p->second.auid == (uint64_t)auid) {\n\tif (f) {\n\t  f->open_object_section(\"pool\");\n\t  f->dump_int(\"poolnum\", p->first);\n\t  f->dump_string(\"poolname\", osdmap.pool_name[p->first]);\n\t  f->close_section();\n\t} else {\n\t  ds << p->first << ' ' << osdmap.pool_name[p->first];\n\t  if (next(p) != osdmap.pools.end()) {\n\t    ds << '\\n';\n\t  }\n\t}\n      }\n    }\n    if (f) {\n      f->close_section();\n      f->flush(ds);\n    }\n    rdata.append(ds);\n  } else if (prefix == \"osd blacklist ls\") {\n    if (f)\n      f->open_array_section(\"blacklist\");\n\n    for (ceph::unordered_map<entity_addr_t,utime_t>::iterator p = osdmap.blacklist.begin();\n\t p != osdmap.blacklist.end();\n\t ++p) {\n      if (f) {\n\tf->open_object_section(\"entry\");\n\tf->dump_stream(\"addr\") << p->first;\n\tf->dump_stream(\"until\") << p->second;\n\tf->close_section();\n      } else {\n\tstringstream ss;\n\tstring s;\n\tss << p->first << \" \" << p->second;\n\tgetline(ss, s);\n\ts += \"\\n\";\n\trdata.append(s);\n      }\n    }\n    if (f) {\n      f->close_section();\n      f->flush(rdata);\n    }\n    ss << \"listed \" << osdmap.blacklist.size() << \" entries\";\n\n  } else if (prefix == \"osd pool ls\") {\n    string detail;\n    cmd_getval(cct, cmdmap, \"detail\", detail);\n    if (!f && detail == \"detail\") {\n      ostringstream ss;\n      osdmap.print_pools(ss);\n      rdata.append(ss.str());\n    } else {\n      if (f)\n\tf->open_array_section(\"pools\");\n      for (map<int64_t,pg_pool_t>::const_iterator it = osdmap.get_pools().begin();\n\t   it != osdmap.get_pools().end();\n\t   ++it) {\n\tif (f) {\n\t  if (detail == \"detail\") {\n\t    f->open_object_section(\"pool\");\n\t    f->dump_string(\"pool_name\", osdmap.get_pool_name(it->first));\n\t    it->second.dump(f.get());\n\t    f->close_section();\n\t  } else {\n\t    f->dump_string(\"pool_name\", osdmap.get_pool_name(it->first));\n\t  }\n\t} else {\n\t  rdata.append(osdmap.get_pool_name(it->first) + \"\\n\");\n\t}\n      }\n      if (f) {\n\tf->close_section();\n\tf->flush(rdata);\n      }\n    }\n\n  } else if (prefix == \"osd crush get-tunable\") {\n    string tunable;\n    cmd_getval(cct, cmdmap, \"tunable\", tunable);\n    ostringstream rss;\n    if (f)\n      f->open_object_section(\"tunable\");\n    if (tunable == \"straw_calc_version\") {\n      if (f)\n\tf->dump_int(tunable.c_str(), osdmap.crush->get_straw_calc_version());\n      else\n\trss << osdmap.crush->get_straw_calc_version() << \"\\n\";\n    } else {\n      r = -EINVAL;\n      goto reply;\n    }\n    if (f) {\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      rdata.append(rss.str());\n    }\n    r = 0;\n\n  } else if (prefix == \"osd pool get\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      r = -ENOENT;\n      goto reply;\n    }\n\n    const pg_pool_t *p = osdmap.get_pg_pool(pool);\n    string var;\n    cmd_getval(cct, cmdmap, \"var\", var);\n\n    typedef std::map<std::string, osd_pool_get_choices> choices_map_t;\n    const choices_map_t ALL_CHOICES = {\n      {\"size\", SIZE},\n      {\"min_size\", MIN_SIZE},\n      {\"pg_num\", PG_NUM}, {\"pgp_num\", PGP_NUM},\n      {\"crush_rule\", CRUSH_RULE}, {\"hashpspool\", HASHPSPOOL},\n      {\"allow_ec_overwrites\", EC_OVERWRITES}, {\"nodelete\", NODELETE},\n      {\"nopgchange\", NOPGCHANGE}, {\"nosizechange\", NOSIZECHANGE},\n      {\"noscrub\", NOSCRUB}, {\"nodeep-scrub\", NODEEP_SCRUB},\n      {\"write_fadvise_dontneed\", WRITE_FADVISE_DONTNEED},\n      {\"hit_set_type\", HIT_SET_TYPE}, {\"hit_set_period\", HIT_SET_PERIOD},\n      {\"hit_set_count\", HIT_SET_COUNT}, {\"hit_set_fpp\", HIT_SET_FPP},\n      {\"use_gmt_hitset\", USE_GMT_HITSET},\n      {\"auid\", AUID}, {\"target_max_objects\", TARGET_MAX_OBJECTS},\n      {\"target_max_bytes\", TARGET_MAX_BYTES},\n      {\"cache_target_dirty_ratio\", CACHE_TARGET_DIRTY_RATIO},\n      {\"cache_target_dirty_high_ratio\", CACHE_TARGET_DIRTY_HIGH_RATIO},\n      {\"cache_target_full_ratio\", CACHE_TARGET_FULL_RATIO},\n      {\"cache_min_flush_age\", CACHE_MIN_FLUSH_AGE},\n      {\"cache_min_evict_age\", CACHE_MIN_EVICT_AGE},\n      {\"erasure_code_profile\", ERASURE_CODE_PROFILE},\n      {\"min_read_recency_for_promote\", MIN_READ_RECENCY_FOR_PROMOTE},\n      {\"min_write_recency_for_promote\", MIN_WRITE_RECENCY_FOR_PROMOTE},\n      {\"fast_read\", FAST_READ},\n      {\"hit_set_grade_decay_rate\", HIT_SET_GRADE_DECAY_RATE},\n      {\"hit_set_search_last_n\", HIT_SET_SEARCH_LAST_N},\n      {\"scrub_min_interval\", SCRUB_MIN_INTERVAL},\n      {\"scrub_max_interval\", SCRUB_MAX_INTERVAL},\n      {\"deep_scrub_interval\", DEEP_SCRUB_INTERVAL},\n      {\"recovery_priority\", RECOVERY_PRIORITY},\n      {\"recovery_op_priority\", RECOVERY_OP_PRIORITY},\n      {\"scrub_priority\", SCRUB_PRIORITY},\n      {\"compression_mode\", COMPRESSION_MODE},\n      {\"compression_algorithm\", COMPRESSION_ALGORITHM},\n      {\"compression_required_ratio\", COMPRESSION_REQUIRED_RATIO},\n      {\"compression_max_blob_size\", COMPRESSION_MAX_BLOB_SIZE},\n      {\"compression_min_blob_size\", COMPRESSION_MIN_BLOB_SIZE},\n      {\"csum_type\", CSUM_TYPE},\n      {\"csum_max_block\", CSUM_MAX_BLOCK},\n      {\"csum_min_block\", CSUM_MIN_BLOCK},\n    };\n\n    typedef std::set<osd_pool_get_choices> choices_set_t;\n\n    const choices_set_t ONLY_TIER_CHOICES = {\n      HIT_SET_TYPE, HIT_SET_PERIOD, HIT_SET_COUNT, HIT_SET_FPP,\n      TARGET_MAX_OBJECTS, TARGET_MAX_BYTES, CACHE_TARGET_FULL_RATIO,\n      CACHE_TARGET_DIRTY_RATIO, CACHE_TARGET_DIRTY_HIGH_RATIO,\n      CACHE_MIN_FLUSH_AGE, CACHE_MIN_EVICT_AGE,\n      MIN_READ_RECENCY_FOR_PROMOTE,\n      MIN_WRITE_RECENCY_FOR_PROMOTE,\n      HIT_SET_GRADE_DECAY_RATE, HIT_SET_SEARCH_LAST_N\n    };\n    const choices_set_t ONLY_ERASURE_CHOICES = {\n      EC_OVERWRITES, ERASURE_CODE_PROFILE\n    };\n\n    choices_set_t selected_choices;\n    if (var == \"all\") {\n      for(choices_map_t::const_iterator it = ALL_CHOICES.begin();\n\t  it != ALL_CHOICES.end(); ++it) {\n\tselected_choices.insert(it->second);\n      }\n\n      if(!p->is_tier()) {\n\tselected_choices = subtract_second_from_first(selected_choices,\n\t\t\t\t\t\t      ONLY_TIER_CHOICES);\n      }\n\n      if(!p->is_erasure()) {\n\tselected_choices = subtract_second_from_first(selected_choices,\n\t\t\t\t\t\t      ONLY_ERASURE_CHOICES);\n      }\n    } else /* var != \"all\" */  {\n      choices_map_t::const_iterator found = ALL_CHOICES.find(var);\n      osd_pool_get_choices selected = found->second;\n\n      if (!p->is_tier() &&\n\t  ONLY_TIER_CHOICES.find(selected) != ONLY_TIER_CHOICES.end()) {\n\tss << \"pool '\" << poolstr\n\t   << \"' is not a tier pool: variable not applicable\";\n\tr = -EACCES;\n\tgoto reply;\n      }\n\n      if (!p->is_erasure() &&\n\t  ONLY_ERASURE_CHOICES.find(selected)\n\t  != ONLY_ERASURE_CHOICES.end()) {\n\tss << \"pool '\" << poolstr\n\t   << \"' is not a erasure pool: variable not applicable\";\n\tr = -EACCES;\n\tgoto reply;\n      }\n\n      if (pool_opts_t::is_opt_name(var) &&\n\t  !p->opts.is_set(pool_opts_t::get_opt_desc(var).key)) {\n\tss << \"option '\" << var << \"' is not set on pool '\" << poolstr << \"'\";\n\tr = -ENOENT;\n\tgoto reply;\n      }\n\n      selected_choices.insert(selected);\n    }\n\n    if (f) {\n      f->open_object_section(\"pool\");\n      f->dump_string(\"pool\", poolstr);\n      f->dump_int(\"pool_id\", pool);\n      for(choices_set_t::const_iterator it = selected_choices.begin();\n\t  it != selected_choices.end(); ++it) {\n\tchoices_map_t::const_iterator i;\n        for (i = ALL_CHOICES.begin(); i != ALL_CHOICES.end(); ++i) {\n          if (i->second == *it) {\n            break;\n          }\n        }\n        assert(i != ALL_CHOICES.end());\n\tswitch(*it) {\n\t  case PG_NUM:\n\t    f->dump_int(\"pg_num\", p->get_pg_num());\n\t    break;\n\t  case PGP_NUM:\n\t    f->dump_int(\"pgp_num\", p->get_pgp_num());\n\t    break;\n\t  case AUID:\n\t    f->dump_int(\"auid\", p->get_auid());\n\t    break;\n\t  case SIZE:\n\t    f->dump_int(\"size\", p->get_size());\n\t    break;\n\t  case MIN_SIZE:\n\t    f->dump_int(\"min_size\", p->get_min_size());\n\t    break;\n\t  case CRUSH_RULE:\n\t    if (osdmap.crush->rule_exists(p->get_crush_rule())) {\n\t      f->dump_string(\"crush_rule\", osdmap.crush->get_rule_name(\n\t\t\t       p->get_crush_rule()));\n\t    } else {\n\t      f->dump_string(\"crush_rule\", stringify(p->get_crush_rule()));\n\t    }\n\t    break;\n\t  case EC_OVERWRITES:\n\t    f->dump_bool(\"allow_ec_overwrites\",\n                         p->has_flag(pg_pool_t::FLAG_EC_OVERWRITES));\n\t    break;\n\t  case HASHPSPOOL:\n\t  case NODELETE:\n\t  case NOPGCHANGE:\n\t  case NOSIZECHANGE:\n\t  case WRITE_FADVISE_DONTNEED:\n\t  case NOSCRUB:\n\t  case NODEEP_SCRUB:\n\t    f->dump_bool(i->first.c_str(),\n\t\t\t   p->has_flag(pg_pool_t::get_flag_by_name(i->first)));\n\t    break;\n\t  case HIT_SET_PERIOD:\n\t    f->dump_int(\"hit_set_period\", p->hit_set_period);\n\t    break;\n\t  case HIT_SET_COUNT:\n\t    f->dump_int(\"hit_set_count\", p->hit_set_count);\n\t    break;\n\t  case HIT_SET_TYPE:\n\t    f->dump_string(\"hit_set_type\",\n\t\t\t   HitSet::get_type_name(p->hit_set_params.get_type()));\n\t    break;\n\t  case HIT_SET_FPP:\n\t    {\n\t      if (p->hit_set_params.get_type() == HitSet::TYPE_BLOOM) {\n\t\tBloomHitSet::Params *bloomp =\n\t\t  static_cast<BloomHitSet::Params*>(p->hit_set_params.impl.get());\n\t\tf->dump_float(\"hit_set_fpp\", bloomp->get_fpp());\n\t      } else if(var != \"all\") {\n\t\tf->close_section();\n\t\tss << \"hit set is not of type Bloom; \" <<\n\t\t  \"invalid to get a false positive rate!\";\n\t\tr = -EINVAL;\n\t\tgoto reply;\n\t      }\n\t    }\n\t    break;\n\t  case USE_GMT_HITSET:\n\t    f->dump_bool(\"use_gmt_hitset\", p->use_gmt_hitset);\n\t    break;\n\t  case TARGET_MAX_OBJECTS:\n\t    f->dump_unsigned(\"target_max_objects\", p->target_max_objects);\n\t    break;\n\t  case TARGET_MAX_BYTES:\n\t    f->dump_unsigned(\"target_max_bytes\", p->target_max_bytes);\n\t    break;\n\t  case CACHE_TARGET_DIRTY_RATIO:\n\t    f->dump_unsigned(\"cache_target_dirty_ratio_micro\",\n\t\t\t     p->cache_target_dirty_ratio_micro);\n\t    f->dump_float(\"cache_target_dirty_ratio\",\n\t\t\t  ((float)p->cache_target_dirty_ratio_micro/1000000));\n\t    break;\n\t  case CACHE_TARGET_DIRTY_HIGH_RATIO:\n\t    f->dump_unsigned(\"cache_target_dirty_high_ratio_micro\",\n\t\t\t     p->cache_target_dirty_high_ratio_micro);\n\t    f->dump_float(\"cache_target_dirty_high_ratio\",\n\t\t\t  ((float)p->cache_target_dirty_high_ratio_micro/1000000));\n\t    break;\n\t  case CACHE_TARGET_FULL_RATIO:\n\t    f->dump_unsigned(\"cache_target_full_ratio_micro\",\n\t\t\t     p->cache_target_full_ratio_micro);\n\t    f->dump_float(\"cache_target_full_ratio\",\n\t\t\t  ((float)p->cache_target_full_ratio_micro/1000000));\n\t    break;\n\t  case CACHE_MIN_FLUSH_AGE:\n\t    f->dump_unsigned(\"cache_min_flush_age\", p->cache_min_flush_age);\n\t    break;\n\t  case CACHE_MIN_EVICT_AGE:\n\t    f->dump_unsigned(\"cache_min_evict_age\", p->cache_min_evict_age);\n\t    break;\n\t  case ERASURE_CODE_PROFILE:\n\t    f->dump_string(\"erasure_code_profile\", p->erasure_code_profile);\n\t    break;\n\t  case MIN_READ_RECENCY_FOR_PROMOTE:\n\t    f->dump_int(\"min_read_recency_for_promote\",\n\t\t\tp->min_read_recency_for_promote);\n\t    break;\n\t  case MIN_WRITE_RECENCY_FOR_PROMOTE:\n\t    f->dump_int(\"min_write_recency_for_promote\",\n\t\t\tp->min_write_recency_for_promote);\n\t    break;\n          case FAST_READ:\n            f->dump_int(\"fast_read\", p->fast_read);\n            break;\n\t  case HIT_SET_GRADE_DECAY_RATE:\n\t    f->dump_int(\"hit_set_grade_decay_rate\",\n\t\t\tp->hit_set_grade_decay_rate);\n\t    break;\n\t  case HIT_SET_SEARCH_LAST_N:\n\t    f->dump_int(\"hit_set_search_last_n\",\n\t\t\tp->hit_set_search_last_n);\n\t    break;\n\t  case SCRUB_MIN_INTERVAL:\n\t  case SCRUB_MAX_INTERVAL:\n\t  case DEEP_SCRUB_INTERVAL:\n          case RECOVERY_PRIORITY:\n          case RECOVERY_OP_PRIORITY:\n          case SCRUB_PRIORITY:\n\t  case COMPRESSION_MODE:\n\t  case COMPRESSION_ALGORITHM:\n\t  case COMPRESSION_REQUIRED_RATIO:\n\t  case COMPRESSION_MAX_BLOB_SIZE:\n\t  case COMPRESSION_MIN_BLOB_SIZE:\n\t  case CSUM_TYPE:\n\t  case CSUM_MAX_BLOCK:\n\t  case CSUM_MIN_BLOCK:\n            pool_opts_t::key_t key = pool_opts_t::get_opt_desc(i->first).key;\n            if (p->opts.is_set(key)) {\n              if(*it == CSUM_TYPE) {\n                int val;\n                p->opts.get(pool_opts_t::CSUM_TYPE, &val);\n                f->dump_string(i->first.c_str(), Checksummer::get_csum_type_string(val));\n              } else {\n                p->opts.dump(i->first, f.get());\n              }\n\t    }\n            break;\n\t}\n      }\n      f->close_section();\n      f->flush(rdata);\n    } else /* !f */ {\n      for(choices_set_t::const_iterator it = selected_choices.begin();\n\t  it != selected_choices.end(); ++it) {\n\tchoices_map_t::const_iterator i;\n\tswitch(*it) {\n\t  case PG_NUM:\n\t    ss << \"pg_num: \" << p->get_pg_num() << \"\\n\";\n\t    break;\n\t  case PGP_NUM:\n\t    ss << \"pgp_num: \" << p->get_pgp_num() << \"\\n\";\n\t    break;\n\t  case AUID:\n\t    ss << \"auid: \" << p->get_auid() << \"\\n\";\n\t    break;\n\t  case SIZE:\n\t    ss << \"size: \" << p->get_size() << \"\\n\";\n\t    break;\n\t  case MIN_SIZE:\n\t    ss << \"min_size: \" << p->get_min_size() << \"\\n\";\n\t    break;\n\t  case CRUSH_RULE:\n\t    if (osdmap.crush->rule_exists(p->get_crush_rule())) {\n\t      ss << \"crush_rule: \" << osdmap.crush->get_rule_name(\n\t\tp->get_crush_rule()) << \"\\n\";\n\t    } else {\n\t      ss << \"crush_rule: \" << p->get_crush_rule() << \"\\n\";\n\t    }\n\t    break;\n\t  case HIT_SET_PERIOD:\n\t    ss << \"hit_set_period: \" << p->hit_set_period << \"\\n\";\n\t    break;\n\t  case HIT_SET_COUNT:\n\t    ss << \"hit_set_count: \" << p->hit_set_count << \"\\n\";\n\t    break;\n\t  case HIT_SET_TYPE:\n\t    ss << \"hit_set_type: \" <<\n\t      HitSet::get_type_name(p->hit_set_params.get_type()) << \"\\n\";\n\t    break;\n\t  case HIT_SET_FPP:\n\t    {\n\t      if (p->hit_set_params.get_type() == HitSet::TYPE_BLOOM) {\n\t\tBloomHitSet::Params *bloomp =\n\t\t  static_cast<BloomHitSet::Params*>(p->hit_set_params.impl.get());\n\t\tss << \"hit_set_fpp: \" << bloomp->get_fpp() << \"\\n\";\n\t      } else if(var != \"all\") {\n\t\tss << \"hit set is not of type Bloom; \" <<\n\t\t  \"invalid to get a false positive rate!\";\n\t\tr = -EINVAL;\n\t\tgoto reply;\n\t      }\n\t    }\n\t    break;\n\t  case USE_GMT_HITSET:\n\t    ss << \"use_gmt_hitset: \" << p->use_gmt_hitset << \"\\n\";\n\t    break;\n\t  case TARGET_MAX_OBJECTS:\n\t    ss << \"target_max_objects: \" << p->target_max_objects << \"\\n\";\n\t    break;\n\t  case TARGET_MAX_BYTES:\n\t    ss << \"target_max_bytes: \" << p->target_max_bytes << \"\\n\";\n\t    break;\n\t  case CACHE_TARGET_DIRTY_RATIO:\n\t    ss << \"cache_target_dirty_ratio: \"\n\t       << ((float)p->cache_target_dirty_ratio_micro/1000000) << \"\\n\";\n\t    break;\n\t  case CACHE_TARGET_DIRTY_HIGH_RATIO:\n\t    ss << \"cache_target_dirty_high_ratio: \"\n\t       << ((float)p->cache_target_dirty_high_ratio_micro/1000000) << \"\\n\";\n\t    break;\n\t  case CACHE_TARGET_FULL_RATIO:\n\t    ss << \"cache_target_full_ratio: \"\n\t       << ((float)p->cache_target_full_ratio_micro/1000000) << \"\\n\";\n\t    break;\n\t  case CACHE_MIN_FLUSH_AGE:\n\t    ss << \"cache_min_flush_age: \" << p->cache_min_flush_age << \"\\n\";\n\t    break;\n\t  case CACHE_MIN_EVICT_AGE:\n\t    ss << \"cache_min_evict_age: \" << p->cache_min_evict_age << \"\\n\";\n\t    break;\n\t  case ERASURE_CODE_PROFILE:\n\t    ss << \"erasure_code_profile: \" << p->erasure_code_profile << \"\\n\";\n\t    break;\n\t  case MIN_READ_RECENCY_FOR_PROMOTE:\n\t    ss << \"min_read_recency_for_promote: \" <<\n\t      p->min_read_recency_for_promote << \"\\n\";\n\t    break;\n\t  case HIT_SET_GRADE_DECAY_RATE:\n\t    ss << \"hit_set_grade_decay_rate: \" <<\n\t      p->hit_set_grade_decay_rate << \"\\n\";\n\t    break;\n\t  case HIT_SET_SEARCH_LAST_N:\n\t    ss << \"hit_set_search_last_n: \" <<\n\t      p->hit_set_search_last_n << \"\\n\";\n\t    break;\n\t  case EC_OVERWRITES:\n\t    ss << \"allow_ec_overwrites: \" <<\n\t      (p->has_flag(pg_pool_t::FLAG_EC_OVERWRITES) ? \"true\" : \"false\") <<\n\t      \"\\n\";\n\t    break;\n\t  case HASHPSPOOL:\n\t  case NODELETE:\n\t  case NOPGCHANGE:\n\t  case NOSIZECHANGE:\n\t  case WRITE_FADVISE_DONTNEED:\n\t  case NOSCRUB:\n\t  case NODEEP_SCRUB:\n\t    for (i = ALL_CHOICES.begin(); i != ALL_CHOICES.end(); ++i) {\n\t      if (i->second == *it)\n\t\tbreak;\n\t    }\n\t    assert(i != ALL_CHOICES.end());\n\t    ss << i->first << \": \" <<\n\t      (p->has_flag(pg_pool_t::get_flag_by_name(i->first)) ?\n\t       \"true\" : \"false\") << \"\\n\";\n\t    break;\n\t  case MIN_WRITE_RECENCY_FOR_PROMOTE:\n\t    ss << \"min_write_recency_for_promote: \" <<\n\t      p->min_write_recency_for_promote << \"\\n\";\n\t    break;\n          case FAST_READ:\n            ss << \"fast_read: \" << p->fast_read << \"\\n\";\n            break;\n\t  case SCRUB_MIN_INTERVAL:\n\t  case SCRUB_MAX_INTERVAL:\n\t  case DEEP_SCRUB_INTERVAL:\n          case RECOVERY_PRIORITY:\n          case RECOVERY_OP_PRIORITY:\n          case SCRUB_PRIORITY:\n\t  case COMPRESSION_MODE:\n\t  case COMPRESSION_ALGORITHM:\n\t  case COMPRESSION_REQUIRED_RATIO:\n\t  case COMPRESSION_MAX_BLOB_SIZE:\n\t  case COMPRESSION_MIN_BLOB_SIZE:\n\t  case CSUM_TYPE:\n\t  case CSUM_MAX_BLOCK:\n\t  case CSUM_MIN_BLOCK:\n\t    for (i = ALL_CHOICES.begin(); i != ALL_CHOICES.end(); ++i) {\n\t      if (i->second == *it)\n\t\tbreak;\n\t    }\n\t    assert(i != ALL_CHOICES.end());\n\t    {\n\t      pool_opts_t::key_t key = pool_opts_t::get_opt_desc(i->first).key;\n\t      if (p->opts.is_set(key)) {\n                if(key == pool_opts_t::CSUM_TYPE) {\n                  int val;\n                  p->opts.get(key, &val);\n  \t\t  ss << i->first << \": \" << Checksummer::get_csum_type_string(val) << \"\\n\";\n                } else {\n  \t\t  ss << i->first << \": \" << p->opts.get(key) << \"\\n\";\n                }\n\t      }\n\t    }\n\t    break;\n\t}\n\trdata.append(ss.str());\n\tss.str(\"\");\n      }\n    }\n    r = 0;\n  } else if (prefix == \"osd pool get-quota\") {\n    string pool_name;\n    cmd_getval(cct, cmdmap, \"pool\", pool_name);\n\n    int64_t poolid = osdmap.lookup_pg_pool_name(pool_name);\n    if (poolid < 0) {\n      assert(poolid == -ENOENT);\n      ss << \"unrecognized pool '\" << pool_name << \"'\";\n      r = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(poolid);\n\n    if (f) {\n      f->open_object_section(\"pool_quotas\");\n      f->dump_string(\"pool_name\", pool_name);\n      f->dump_unsigned(\"pool_id\", poolid);\n      f->dump_unsigned(\"quota_max_objects\", p->quota_max_objects);\n      f->dump_unsigned(\"quota_max_bytes\", p->quota_max_bytes);\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      stringstream rs;\n      rs << \"quotas for pool '\" << pool_name << \"':\\n\"\n         << \"  max objects: \";\n      if (p->quota_max_objects == 0)\n        rs << \"N/A\";\n      else\n        rs << si_u_t(p->quota_max_objects) << \" objects\";\n      rs << \"\\n\"\n         << \"  max bytes  : \";\n      if (p->quota_max_bytes == 0)\n        rs << \"N/A\";\n      else\n        rs << byte_u_t(p->quota_max_bytes);\n      rdata.append(rs.str());\n    }\n    rdata.append(\"\\n\");\n    r = 0;\n  } else if (prefix == \"osd crush rule list\" ||\n\t     prefix == \"osd crush rule ls\") {\n    if (f) {\n      f->open_array_section(\"rules\");\n      osdmap.crush->list_rules(f.get());\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ostringstream ss;\n      osdmap.crush->list_rules(&ss);\n      rdata.append(ss.str());\n    }\n  } else if (prefix == \"osd crush rule ls-by-class\") {\n    string class_name;\n    cmd_getval(cct, cmdmap, \"class\", class_name);\n    if (class_name.empty()) {\n      ss << \"no class specified\";\n      r = -EINVAL;\n      goto reply;\n    }\n    set<int> rules;\n    r = osdmap.crush->get_rules_by_class(class_name, &rules);\n    if (r < 0) {\n      ss << \"failed to get rules by class '\" << class_name << \"'\";\n      goto reply;\n    }\n    if (f) {\n      f->open_array_section(\"rules\");\n      for (auto &rule: rules) {\n        f->dump_string(\"name\", osdmap.crush->get_rule_name(rule));\n      }\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ostringstream rs;\n      for (auto &rule: rules) {\n        rs << osdmap.crush->get_rule_name(rule) << \"\\n\";\n      }\n      rdata.append(rs.str());\n    }\n  } else if (prefix == \"osd crush rule dump\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    if (name == \"\") {\n      f->open_array_section(\"rules\");\n      osdmap.crush->dump_rules(f.get());\n      f->close_section();\n    } else {\n      int ruleno = osdmap.crush->get_rule_id(name);\n      if (ruleno < 0) {\n\tss << \"unknown crush rule '\" << name << \"'\";\n\tr = ruleno;\n\tgoto reply;\n      }\n      osdmap.crush->dump_rule(ruleno, f.get());\n    }\n    ostringstream rs;\n    f->flush(rs);\n    rs << \"\\n\";\n    rdata.append(rs.str());\n  } else if (prefix == \"osd crush dump\") {\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    f->open_object_section(\"crush_map\");\n    osdmap.crush->dump(f.get());\n    f->close_section();\n    ostringstream rs;\n    f->flush(rs);\n    rs << \"\\n\";\n    rdata.append(rs.str());\n  } else if (prefix == \"osd crush show-tunables\") {\n    string format;\n    cmd_getval(cct, cmdmap, \"format\", format);\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    f->open_object_section(\"crush_map_tunables\");\n    osdmap.crush->dump_tunables(f.get());\n    f->close_section();\n    ostringstream rs;\n    f->flush(rs);\n    rs << \"\\n\";\n    rdata.append(rs.str());\n  } else if (prefix == \"osd crush tree\") {\n    string shadow;\n    cmd_getval(cct, cmdmap, \"shadow\", shadow);\n    bool show_shadow = shadow == \"--show-shadow\";\n    boost::scoped_ptr<Formatter> f(Formatter::create(format));\n    if (f) {\n      osdmap.crush->dump_tree(nullptr,\n                              f.get(),\n                              osdmap.get_pool_names(),\n                              show_shadow);\n      f->flush(rdata);\n    } else {\n      ostringstream ss;\n      osdmap.crush->dump_tree(&ss,\n                              nullptr,\n                              osdmap.get_pool_names(),\n                              show_shadow);\n      rdata.append(ss.str());\n    }\n  } else if (prefix == \"osd crush ls\") {\n    string name;\n    if (!cmd_getval(cct, cmdmap, \"node\", name)) {\n      ss << \"no node specified\";\n      r = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.crush->name_exists(name)) {\n      ss << \"node '\" << name << \"' does not exist\";\n      r = -ENOENT;\n      goto reply;\n    }\n    int id = osdmap.crush->get_item_id(name);\n    list<int> result;\n    if (id >= 0) {\n      result.push_back(id);\n    } else {\n      int num = osdmap.crush->get_bucket_size(id);\n      for (int i = 0; i < num; ++i) {\n\tresult.push_back(osdmap.crush->get_bucket_item(id, i));\n      }\n    }\n    if (f) {\n      f->open_array_section(\"items\");\n      for (auto i : result) {\n\tf->dump_string(\"item\", osdmap.crush->get_item_name(i));\n      }\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ostringstream ss;\n      for (auto i : result) {\n\tss << osdmap.crush->get_item_name(i) << \"\\n\";\n      }\n      rdata.append(ss.str());\n    }\n    r = 0;\n  } else if (prefix == \"osd crush class ls\") {\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\", \"json-pretty\"));\n    f->open_array_section(\"crush_classes\");\n    for (auto i : osdmap.crush->class_name)\n      f->dump_string(\"class\", i.second);\n    f->close_section();\n    f->flush(rdata);\n  } else if (prefix == \"osd crush class ls-osd\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"class\", name);\n    set<int> osds;\n    osdmap.crush->get_devices_by_class(name, &osds);\n    if (f) {\n      f->open_array_section(\"osds\");\n      for (auto &osd: osds)\n        f->dump_int(\"osd\", osd);\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      bool first = true;\n      for (auto &osd : osds) {\n        if (!first)\n          ds << \"\\n\";\n        first = false;\n        ds << osd;\n      }\n      rdata.append(ds);\n    }\n  } else if (prefix == \"osd erasure-code-profile ls\") {\n    const auto &profiles = osdmap.get_erasure_code_profiles();\n    if (f)\n      f->open_array_section(\"erasure-code-profiles\");\n    for (auto i = profiles.begin(); i != profiles.end(); ++i) {\n      if (f)\n        f->dump_string(\"profile\", i->first.c_str());\n      else\n\trdata.append(i->first + \"\\n\");\n    }\n    if (f) {\n      f->close_section();\n      ostringstream rs;\n      f->flush(rs);\n      rs << \"\\n\";\n      rdata.append(rs.str());\n    }\n  } else if (prefix == \"osd crush weight-set ls\") {\n    boost::scoped_ptr<Formatter> f(Formatter::create(format));\n    if (f) {\n      f->open_array_section(\"weight_sets\");\n      if (osdmap.crush->have_choose_args(CrushWrapper::DEFAULT_CHOOSE_ARGS)) {\n\tf->dump_string(\"pool\", \"(compat)\");\n      }\n      for (auto& i : osdmap.crush->choose_args) {\n\tif (i.first >= 0) {\n\t  f->dump_string(\"pool\", osdmap.get_pool_name(i.first));\n\t}\n      }\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ostringstream rs;\n      if (osdmap.crush->have_choose_args(CrushWrapper::DEFAULT_CHOOSE_ARGS)) {\n\trs << \"(compat)\\n\";\n      }\n      for (auto& i : osdmap.crush->choose_args) {\n\tif (i.first >= 0) {\n\t  rs << osdmap.get_pool_name(i.first) << \"\\n\";\n\t}\n      }\n      rdata.append(rs.str());\n    }\n  } else if (prefix == \"osd crush weight-set dump\") {\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\",\n\t\t\t\t\t\t     \"json-pretty\"));\n    osdmap.crush->dump_choose_args(f.get());\n    f->flush(rdata);\n  } else if (prefix == \"osd erasure-code-profile get\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    if (!osdmap.has_erasure_code_profile(name)) {\n      ss << \"unknown erasure code profile '\" << name << \"'\";\n      r = -ENOENT;\n      goto reply;\n    }\n    const map<string,string> &profile = osdmap.get_erasure_code_profile(name);\n    if (f)\n      f->open_object_section(\"profile\");\n    for (map<string,string>::const_iterator i = profile.begin();\n\t i != profile.end();\n\t ++i) {\n      if (f)\n        f->dump_string(i->first.c_str(), i->second.c_str());\n      else\n\trdata.append(i->first + \"=\" + i->second + \"\\n\");\n    }\n    if (f) {\n      f->close_section();\n      ostringstream rs;\n      f->flush(rs);\n      rs << \"\\n\";\n      rdata.append(rs.str());\n    }\n  } else if (prefix == \"osd pool application get\") {\n    boost::scoped_ptr<Formatter> f(Formatter::create(format, \"json-pretty\",\n                                                     \"json-pretty\"));\n    string pool_name;\n    cmd_getval(cct, cmdmap, \"pool\", pool_name);\n    string app;\n    cmd_getval(cct, cmdmap, \"app\", app);\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n\n    if (pool_name.empty()) {\n      // all\n      f->open_object_section(\"pools\");\n      for (const auto &pool : osdmap.pools) {\n        std::string name(\"<unknown>\");\n        const auto &pni = osdmap.pool_name.find(pool.first);\n        if (pni != osdmap.pool_name.end())\n          name = pni->second;\n        f->open_object_section(name.c_str());\n        for (auto &app_pair : pool.second.application_metadata) {\n          f->open_object_section(app_pair.first.c_str());\n          for (auto &kv_pair : app_pair.second) {\n            f->dump_string(kv_pair.first.c_str(), kv_pair.second);\n          }\n          f->close_section();\n        }\n        f->close_section(); // name\n      }\n      f->close_section(); // pools\n      f->flush(rdata);\n    } else {\n      int64_t pool = osdmap.lookup_pg_pool_name(pool_name.c_str());\n      if (pool < 0) {\n        ss << \"unrecognized pool '\" << pool_name << \"'\";\n        r = -ENOENT;\n        goto reply;\n      }\n      auto p = osdmap.get_pg_pool(pool);\n      // filter by pool\n      if (app.empty()) {\n        f->open_object_section(pool_name.c_str());\n        for (auto &app_pair : p->application_metadata) {\n          f->open_object_section(app_pair.first.c_str());\n          for (auto &kv_pair : app_pair.second) {\n            f->dump_string(kv_pair.first.c_str(), kv_pair.second);\n          }\n          f->close_section(); // application\n        }\n        f->close_section(); // pool_name\n        f->flush(rdata);\n        goto reply;\n      }\n\n      auto app_it = p->application_metadata.find(app);\n      if (app_it == p->application_metadata.end()) {\n        ss << \"pool '\" << pool_name << \"' has no application '\" << app << \"'\";\n        r = -ENOENT;\n        goto reply;\n      }\n      // filter by pool + app\n      if (key.empty()) {\n        f->open_object_section(app_it->first.c_str());\n        for (auto &kv_pair : app_it->second) {\n          f->dump_string(kv_pair.first.c_str(), kv_pair.second);\n        }\n        f->close_section(); // application\n        f->flush(rdata);\n        goto reply;\n      }\n      // filter by pool + app + key\n      auto key_it = app_it->second.find(key);\n      if (key_it == app_it->second.end()) {\n        ss << \"application '\" << app << \"' on pool '\" << pool_name\n           << \"' does not have key '\" << key << \"'\";\n        r = -ENOENT;\n        goto reply;\n      }\n      ss << key_it->second << \"\\n\";\n      rdata.append(ss.str());\n      ss.str(\"\");\n    }\n  } else if (prefix == \"osd get-require-min-compat-client\") {\n    ss << ceph_release_name(osdmap.require_min_compat_client) << std::endl;\n    rdata.append(ss.str());\n    ss.str(\"\");\n    goto reply;\n  } else {\n    // try prepare update\n    return false;\n  }\n\n reply:\n  string rs;\n  getline(ss, rs);\n  mon->reply_command(op, r, rs, rdata, get_last_committed());\n  return true;\n}\n\nvoid OSDMonitor::set_pool_flags(int64_t pool_id, uint64_t flags)\n{\n  pg_pool_t *pool = pending_inc.get_new_pool(pool_id,\n    osdmap.get_pg_pool(pool_id));\n  assert(pool);\n  pool->set_flag(flags);\n}\n\nvoid OSDMonitor::clear_pool_flags(int64_t pool_id, uint64_t flags)\n{\n  pg_pool_t *pool = pending_inc.get_new_pool(pool_id,\n    osdmap.get_pg_pool(pool_id));\n  assert(pool);\n  pool->unset_flag(flags);\n}\n\nstring OSDMonitor::make_snap_epoch_key(int64_t pool, epoch_t epoch)\n{\n  char k[80];\n  snprintf(k, sizeof(k), \"removed_epoch_%llu_%08lx\",\n\t   (unsigned long long)pool, (unsigned long)epoch);\n  return k;\n}\n\nstring OSDMonitor::make_snap_key(int64_t pool, snapid_t snap)\n{\n  char k[80];\n  snprintf(k, sizeof(k), \"removed_snap_%llu_%016llx\",\n\t   (unsigned long long)pool, (unsigned long long)snap);\n  return k;\n}\n\n\nstring OSDMonitor::make_snap_key_value(\n  int64_t pool, snapid_t snap, snapid_t num,\n  epoch_t epoch, bufferlist *v)\n{\n  // encode the *last* epoch in the key so that we can use forward\n  // iteration only to search for an epoch in an interval.\n  encode(snap, *v);\n  encode(snap + num, *v);\n  encode(epoch, *v);\n  return make_snap_key(pool, snap + num - 1);\n}\n\nstring OSDMonitor::make_snap_purged_key(int64_t pool, snapid_t snap)\n{\n  char k[80];\n  snprintf(k, sizeof(k), \"purged_snap_%llu_%016llx\",\n\t   (unsigned long long)pool, (unsigned long long)snap);\n  return k;\n}\nstring OSDMonitor::make_snap_purged_key_value(\n  int64_t pool, snapid_t snap, snapid_t num,\n  epoch_t epoch, bufferlist *v)\n{\n  // encode the *last* epoch in the key so that we can use forward\n  // iteration only to search for an epoch in an interval.\n  encode(snap, *v);\n  encode(snap + num, *v);\n  encode(epoch, *v);\n  return make_snap_purged_key(pool, snap + num - 1);\n}\n\nint OSDMonitor::lookup_pruned_snap(int64_t pool, snapid_t snap,\n\t\t\t\t   snapid_t *begin, snapid_t *end)\n{\n  string k = make_snap_key(pool, snap);\n  auto it = mon->store->get_iterator(OSD_SNAP_PREFIX);\n  it->lower_bound(k);\n  if (!it->valid()) {\n    return -ENOENT;\n  }\n  if (it->key().find(OSD_SNAP_PREFIX) != 0) {\n    return -ENOENT;\n  }\n  bufferlist v = it->value();\n  auto p = v.cbegin();\n  decode(*begin, p);\n  decode(*end, p);\n  if (snap < *begin || snap >= *end) {\n    return -ENOENT;\n  }\n  return 0;\n}\n\nbool OSDMonitor::try_prune_purged_snaps()\n{\n  if (!mon->mgrstatmon()->is_readable()) {\n    return false;\n  }\n  if (osdmap.require_osd_release < CEPH_RELEASE_MIMIC) {\n    return false;\n  }\n  if (!pending_inc.new_purged_snaps.empty()) {\n    return false;  // we already pruned for this epoch\n  }\n\n  unsigned max_prune = cct->_conf->get_val<uint64_t>(\n    \"mon_max_snap_prune_per_epoch\");\n  if (!max_prune) {\n    max_prune = 100000;\n  }\n  dout(10) << __func__ << \" max_prune \" << max_prune << dendl;\n\n  unsigned actually_pruned = 0;\n  auto& purged_snaps = mon->mgrstatmon()->get_digest().purged_snaps;\n  for (auto& p : osdmap.get_pools()) {\n    auto q = purged_snaps.find(p.first);\n    if (q == purged_snaps.end()) {\n      continue;\n    }\n    auto& purged = q->second;\n    if (purged.empty()) {\n      dout(20) << __func__ << \" \" << p.first << \" nothing purged\" << dendl;\n      continue;\n    }\n    dout(20) << __func__ << \" pool \" << p.first << \" purged \" << purged << dendl;\n    OSDMap::snap_interval_set_t to_prune;\n    unsigned maybe_pruned = actually_pruned;\n    for (auto i = purged.begin(); i != purged.end(); ++i) {\n      snapid_t begin = i.get_start();\n      auto end = i.get_start() + i.get_len();\n      snapid_t pbegin = 0, pend = 0;\n      int r = lookup_pruned_snap(p.first, begin, &pbegin, &pend);\n      if (r == 0) {\n\t// already purged.\n\t// be a bit aggressive about backing off here, because the mon may\n\t// do a lot of work going through this set, and if we know the\n\t// purged set from the OSDs is at least *partly* stale we may as\n\t// well wait for it to be fresh.\n\tdout(20) << __func__ << \"  we've already pruned \" << pbegin\n\t\t << \"~\" << (pend - pbegin) << dendl;\n\tbreak;  // next pool\n      }\n      if (pbegin && pbegin < end) {\n\t// the tail of [begin,end) is purged; shorten the range\n\tassert(pbegin > begin);\n\tend = pbegin;\n      }\n      to_prune.insert(begin, end - begin);\n      maybe_pruned += end - begin;\n      if (maybe_pruned >= max_prune) {\n\tbreak;\n      }\n    }\n    if (!to_prune.empty()) {\n      // PGs may still be reporting things as purged that we have already\n      // pruned from removed_snaps_queue.\n      OSDMap::snap_interval_set_t actual;\n      auto r = osdmap.removed_snaps_queue.find(p.first);\n      if (r != osdmap.removed_snaps_queue.end()) {\n\tactual.intersection_of(to_prune, r->second);\n      }\n      actually_pruned += actual.size();\n      dout(10) << __func__ << \" pool \" << p.first << \" reports pruned \" << to_prune\n\t       << \", actual pruned \" << actual << dendl;\n      if (!actual.empty()) {\n\tpending_inc.new_purged_snaps[p.first].swap(actual);\n      }\n    }\n    if (actually_pruned >= max_prune) {\n      break;\n    }\n  }\n  dout(10) << __func__ << \" actually pruned \" << actually_pruned << dendl;\n  return !!actually_pruned;\n}\n\nbool OSDMonitor::update_pools_status()\n{\n  if (!mon->mgrstatmon()->is_readable())\n    return false;\n\n  bool ret = false;\n\n  auto& pools = osdmap.get_pools();\n  for (auto it = pools.begin(); it != pools.end(); ++it) {\n    const pool_stat_t *pstat = mon->mgrstatmon()->get_pool_stat(it->first);\n    if (!pstat)\n      continue;\n    const object_stat_sum_t& sum = pstat->stats.sum;\n    const pg_pool_t &pool = it->second;\n    const string& pool_name = osdmap.get_pool_name(it->first);\n\n    bool pool_is_full =\n      (pool.quota_max_bytes > 0 && (uint64_t)sum.num_bytes >= pool.quota_max_bytes) ||\n      (pool.quota_max_objects > 0 && (uint64_t)sum.num_objects >= pool.quota_max_objects);\n\n    if (pool.has_flag(pg_pool_t::FLAG_FULL_QUOTA)) {\n      if (pool_is_full)\n        continue;\n\n      mon->clog->info() << \"pool '\" << pool_name\n                       << \"' no longer out of quota; removing NO_QUOTA flag\";\n      // below we cancel FLAG_FULL too, we'll set it again in\n      // OSDMonitor::encode_pending if it still fails the osd-full checking.\n      clear_pool_flags(it->first,\n                       pg_pool_t::FLAG_FULL_QUOTA | pg_pool_t::FLAG_FULL);\n      ret = true;\n    } else {\n      if (!pool_is_full)\n\tcontinue;\n\n      if (pool.quota_max_bytes > 0 &&\n          (uint64_t)sum.num_bytes >= pool.quota_max_bytes) {\n        mon->clog->warn() << \"pool '\" << pool_name << \"' is full\"\n                         << \" (reached quota's max_bytes: \"\n                         << byte_u_t(pool.quota_max_bytes) << \")\";\n      }\n      if (pool.quota_max_objects > 0 &&\n\t\t (uint64_t)sum.num_objects >= pool.quota_max_objects) {\n        mon->clog->warn() << \"pool '\" << pool_name << \"' is full\"\n                         << \" (reached quota's max_objects: \"\n                         << pool.quota_max_objects << \")\";\n      }\n      // set both FLAG_FULL_QUOTA and FLAG_FULL\n      // note that below we try to cancel FLAG_BACKFILLFULL/NEARFULL too\n      // since FLAG_FULL should always take precedence\n      set_pool_flags(it->first,\n                     pg_pool_t::FLAG_FULL_QUOTA | pg_pool_t::FLAG_FULL);\n      clear_pool_flags(it->first,\n                       pg_pool_t::FLAG_NEARFULL |\n                       pg_pool_t::FLAG_BACKFILLFULL);\n      ret = true;\n    }\n  }\n  return ret;\n}\n\nint OSDMonitor::prepare_new_pool(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  dout(10) << \"prepare_new_pool from \" << m->get_connection() << dendl;\n  MonSession *session = m->get_session();\n  if (!session)\n    return -EPERM;\n  string erasure_code_profile;\n  stringstream ss;\n  string rule_name;\n  int ret = 0;\n  if (m->auid)\n    ret =  prepare_new_pool(m->name, m->auid, m->crush_rule, rule_name,\n\t\t\t    0, 0,\n                            erasure_code_profile,\n\t\t\t    pg_pool_t::TYPE_REPLICATED, 0, FAST_READ_OFF, &ss);\n  else\n    ret = prepare_new_pool(m->name, session->auid, m->crush_rule, rule_name,\n\t\t\t    0, 0,\n                            erasure_code_profile,\n\t\t\t    pg_pool_t::TYPE_REPLICATED, 0, FAST_READ_OFF, &ss);\n\n  if (ret < 0) {\n    dout(10) << __func__ << \" got \" << ret << \" \" << ss.str() << dendl;\n  }\n  return ret;\n}\n\nint OSDMonitor::crush_rename_bucket(const string& srcname,\n\t\t\t\t    const string& dstname,\n\t\t\t\t    ostream *ss)\n{\n  int ret;\n  //\n  // Avoid creating a pending crush if it does not already exists and\n  // the rename would fail.\n  //\n  if (!_have_pending_crush()) {\n    ret = _get_stable_crush().can_rename_bucket(srcname,\n\t\t\t\t\t\tdstname,\n\t\t\t\t\t\tss);\n    if (ret)\n      return ret;\n  }\n\n  CrushWrapper newcrush;\n  _get_pending_crush(newcrush);\n\n  ret = newcrush.rename_bucket(srcname,\n\t\t\t       dstname,\n\t\t\t       ss);\n  if (ret)\n    return ret;\n\n  pending_inc.crush.clear();\n  newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n  *ss << \"renamed bucket \" << srcname << \" into \" << dstname;\t\n  return 0;\n}\n\nvoid OSDMonitor::check_legacy_ec_plugin(const string& plugin, const string& profile) const\n{\n  string replacement = \"\";\n\n  if (plugin == \"jerasure_generic\" || \n      plugin == \"jerasure_sse3\" ||\n      plugin == \"jerasure_sse4\" ||\n      plugin == \"jerasure_neon\") {\n    replacement = \"jerasure\";\n  } else if (plugin == \"shec_generic\" ||\n\t     plugin == \"shec_sse3\" ||\n\t     plugin == \"shec_sse4\" ||\n             plugin == \"shec_neon\") {\n    replacement = \"shec\";\n  }\n\n  if (replacement != \"\") {\n    dout(0) << \"WARNING: erasure coding profile \" << profile << \" uses plugin \"\n\t    << plugin << \" that has been deprecated. Please use \" \n\t    << replacement << \" instead.\" << dendl;\n  }\n}\n\nint OSDMonitor::normalize_profile(const string& profilename,\n\t\t\t\t  ErasureCodeProfile &profile,\n\t\t\t\t  bool force,\n\t\t\t\t  ostream *ss)\n{\n  ErasureCodeInterfaceRef erasure_code;\n  ErasureCodePluginRegistry &instance = ErasureCodePluginRegistry::instance();\n  ErasureCodeProfile::const_iterator plugin = profile.find(\"plugin\");\n  check_legacy_ec_plugin(plugin->second, profilename);\n  int err = instance.factory(plugin->second,\n\t\t\t     g_conf->get_val<std::string>(\"erasure_code_dir\"),\n\t\t\t     profile, &erasure_code, ss);\n  if (err) {\n    return err;\n  }\n\n  err = erasure_code->init(profile, ss);\n  if (err) {\n    return err;\n  }\n\n  auto it = profile.find(\"stripe_unit\");\n  if (it != profile.end()) {\n    string err_str;\n    uint32_t stripe_unit = strict_iecstrtoll(it->second.c_str(), &err_str);\n    if (!err_str.empty()) {\n      *ss << \"could not parse stripe_unit '\" << it->second\n\t  << \"': \" << err_str << std::endl;\n      return -EINVAL;\n    }\n    uint32_t data_chunks = erasure_code->get_data_chunk_count();\n    uint32_t chunk_size = erasure_code->get_chunk_size(stripe_unit * data_chunks);\n    if (chunk_size != stripe_unit) {\n      *ss << \"stripe_unit \" << stripe_unit << \" does not match ec profile \"\n\t  << \"alignment. Would be padded to \" << chunk_size\n\t  << std::endl;\n      return -EINVAL;\n    }\n    if ((stripe_unit % 4096) != 0 && !force) {\n      *ss << \"stripe_unit should be a multiple of 4096 bytes for best performance.\"\n\t  << \"use --force to override this check\" << std::endl;\n      return -EINVAL;\n    }\n  }\n  return 0;\n}\n\nint OSDMonitor::crush_rule_create_erasure(const string &name,\n\t\t\t\t\t     const string &profile,\n\t\t\t\t\t     int *rule,\n\t\t\t\t\t     ostream *ss)\n{\n  int ruleid = osdmap.crush->get_rule_id(name);\n  if (ruleid != -ENOENT) {\n    *rule = osdmap.crush->get_rule_mask_ruleset(ruleid);\n    return -EEXIST;\n  }\n\n  CrushWrapper newcrush;\n  _get_pending_crush(newcrush);\n\n  ruleid = newcrush.get_rule_id(name);\n  if (ruleid != -ENOENT) {\n    *rule = newcrush.get_rule_mask_ruleset(ruleid);\n    return -EALREADY;\n  } else {\n    ErasureCodeInterfaceRef erasure_code;\n    int err = get_erasure_code(profile, &erasure_code, ss);\n    if (err) {\n      *ss << \"failed to load plugin using profile \" << profile << std::endl;\n      return err;\n    }\n\n    err = erasure_code->create_rule(name, newcrush, ss);\n    erasure_code.reset();\n    if (err < 0)\n      return err;\n    *rule = err;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    return 0;\n  }\n}\n\nint OSDMonitor::get_erasure_code(const string &erasure_code_profile,\n\t\t\t\t ErasureCodeInterfaceRef *erasure_code,\n\t\t\t\t ostream *ss) const\n{\n  if (pending_inc.has_erasure_code_profile(erasure_code_profile))\n    return -EAGAIN;\n  ErasureCodeProfile profile =\n    osdmap.get_erasure_code_profile(erasure_code_profile);\n  ErasureCodeProfile::const_iterator plugin =\n    profile.find(\"plugin\");\n  if (plugin == profile.end()) {\n    *ss << \"cannot determine the erasure code plugin\"\n\t<< \" because there is no 'plugin' entry in the erasure_code_profile \"\n\t<< profile << std::endl;\n    return -EINVAL;\n  }\n  check_legacy_ec_plugin(plugin->second, erasure_code_profile);\n  ErasureCodePluginRegistry &instance = ErasureCodePluginRegistry::instance();\n  return instance.factory(plugin->second,\n\t\t\t  g_conf->get_val<std::string>(\"erasure_code_dir\"),\n\t\t\t  profile, erasure_code, ss);\n}\n\nint OSDMonitor::check_cluster_features(uint64_t features,\n\t\t\t\t       stringstream &ss)\n{\n  stringstream unsupported_ss;\n  int unsupported_count = 0;\n  if ((mon->get_quorum_con_features() & features) != features) {\n    unsupported_ss << \"the monitor cluster\";\n    ++unsupported_count;\n  }\n\n  set<int32_t> up_osds;\n  osdmap.get_up_osds(up_osds);\n  for (set<int32_t>::iterator it = up_osds.begin();\n       it != up_osds.end(); ++it) {\n    const osd_xinfo_t &xi = osdmap.get_xinfo(*it);\n    if ((xi.features & features) != features) {\n      if (unsupported_count > 0)\n\tunsupported_ss << \", \";\n      unsupported_ss << \"osd.\" << *it;\n      unsupported_count ++;\n    }\n  }\n\n  if (unsupported_count > 0) {\n    ss << \"features \" << features << \" unsupported by: \"\n       << unsupported_ss.str();\n    return -ENOTSUP;\n  }\n\n  // check pending osd state, too!\n  for (map<int32_t,osd_xinfo_t>::const_iterator p =\n\t pending_inc.new_xinfo.begin();\n       p != pending_inc.new_xinfo.end(); ++p) {\n    const osd_xinfo_t &xi = p->second;\n    if ((xi.features & features) != features) {\n      dout(10) << __func__ << \" pending osd.\" << p->first\n\t       << \" features are insufficient; retry\" << dendl;\n      return -EAGAIN;\n    }\n  }\n\n  return 0;\n}\n\nbool OSDMonitor::validate_crush_against_features(const CrushWrapper *newcrush,\n                                                 stringstream& ss)\n{\n  OSDMap::Incremental new_pending = pending_inc;\n  encode(*newcrush, new_pending.crush, mon->get_quorum_con_features());\n  OSDMap newmap;\n  newmap.deepish_copy_from(osdmap);\n  newmap.apply_incremental(new_pending);\n\n  // client compat\n  if (newmap.require_min_compat_client > 0) {\n    auto mv = newmap.get_min_compat_client();\n    if (mv > newmap.require_min_compat_client) {\n      ss << \"new crush map requires client version \" << ceph_release_name(mv)\n\t << \" but require_min_compat_client is \"\n\t << ceph_release_name(newmap.require_min_compat_client);\n      return false;\n    }\n  }\n\n  // osd compat\n  uint64_t features =\n    newmap.get_features(CEPH_ENTITY_TYPE_MON, NULL) |\n    newmap.get_features(CEPH_ENTITY_TYPE_OSD, NULL);\n  stringstream features_ss;\n  int r = check_cluster_features(features, features_ss);\n  if (r) {\n    ss << \"Could not change CRUSH: \" << features_ss.str();\n    return false;\n  }\n\n  return true;\n}\n\nbool OSDMonitor::erasure_code_profile_in_use(\n  const mempool::osdmap::map<int64_t, pg_pool_t> &pools,\n  const string &profile,\n  ostream *ss)\n{\n  bool found = false;\n  for (map<int64_t, pg_pool_t>::const_iterator p = pools.begin();\n       p != pools.end();\n       ++p) {\n    if (p->second.erasure_code_profile == profile && p->second.is_erasure()) {\n      *ss << osdmap.pool_name[p->first] << \" \";\n      found = true;\n    }\n  }\n  if (found) {\n    *ss << \"pool(s) are using the erasure code profile '\" << profile << \"'\";\n  }\n  return found;\n}\n\nint OSDMonitor::parse_erasure_code_profile(const vector<string> &erasure_code_profile,\n\t\t\t\t\t   map<string,string> *erasure_code_profile_map,\n\t\t\t\t\t   ostream *ss)\n{\n  int r = g_conf->with_val<string>(\"osd_pool_default_erasure_code_profile\",\n\t\t\t\t   get_json_str_map,\n\t\t\t\t   *ss,\n\t\t\t\t   erasure_code_profile_map,\n\t\t\t\t   true);\n  if (r)\n    return r;\n  assert((*erasure_code_profile_map).count(\"plugin\"));\n  string default_plugin = (*erasure_code_profile_map)[\"plugin\"];\n  map<string,string> user_map;\n  for (vector<string>::const_iterator i = erasure_code_profile.begin();\n       i != erasure_code_profile.end();\n       ++i) {\n    size_t equal = i->find('=');\n    if (equal == string::npos) {\n      user_map[*i] = string();\n      (*erasure_code_profile_map)[*i] = string();\n    } else {\n      const string key = i->substr(0, equal);\n      equal++;\n      const string value = i->substr(equal);\n      if (key.find(\"ruleset-\") == 0) {\n\t*ss << \"property '\" << key << \"' is no longer supported; try \"\n\t    << \"'crush-\" << key.substr(8) << \"' instead\";\n\treturn -EINVAL;\n      }\n      user_map[key] = value;\n      (*erasure_code_profile_map)[key] = value;\n    }\n  }\n\n  if (user_map.count(\"plugin\") && user_map[\"plugin\"] != default_plugin)\n    (*erasure_code_profile_map) = user_map;\n\n  return 0;\n}\n\nint OSDMonitor::prepare_pool_size(const unsigned pool_type,\n\t\t\t\t  const string &erasure_code_profile,\n\t\t\t\t  unsigned *size, unsigned *min_size,\n\t\t\t\t  ostream *ss)\n{\n  int err = 0;\n  switch (pool_type) {\n  case pg_pool_t::TYPE_REPLICATED:\n    *size = g_conf->get_val<uint64_t>(\"osd_pool_default_size\");\n    *min_size = g_conf->get_osd_pool_default_min_size();\n    break;\n  case pg_pool_t::TYPE_ERASURE:\n    {\n      ErasureCodeInterfaceRef erasure_code;\n      err = get_erasure_code(erasure_code_profile, &erasure_code, ss);\n      if (err == 0) {\n\t*size = erasure_code->get_chunk_count();\n\t*min_size = std::min(erasure_code->get_data_chunk_count() + 1, *size);\n      }\n    }\n    break;\n  default:\n    *ss << \"prepare_pool_size: \" << pool_type << \" is not a known pool type\";\n    err = -EINVAL;\n    break;\n  }\n  return err;\n}\n\nint OSDMonitor::prepare_pool_stripe_width(const unsigned pool_type,\n\t\t\t\t\t  const string &erasure_code_profile,\n\t\t\t\t\t  uint32_t *stripe_width,\n\t\t\t\t\t  ostream *ss)\n{\n  int err = 0;\n  switch (pool_type) {\n  case pg_pool_t::TYPE_REPLICATED:\n    // ignored\n    break;\n  case pg_pool_t::TYPE_ERASURE:\n    {\n      ErasureCodeProfile profile =\n\tosdmap.get_erasure_code_profile(erasure_code_profile);\n      ErasureCodeInterfaceRef erasure_code;\n      err = get_erasure_code(erasure_code_profile, &erasure_code, ss);\n      if (err)\n\tbreak;\n      uint32_t data_chunks = erasure_code->get_data_chunk_count();\n      uint32_t stripe_unit = g_conf->get_val<Option::size_t>(\"osd_pool_erasure_code_stripe_unit\");\n      auto it = profile.find(\"stripe_unit\");\n      if (it != profile.end()) {\n\tstring err_str;\n\tstripe_unit = strict_iecstrtoll(it->second.c_str(), &err_str);\n\tassert(err_str.empty());\n      }\n      *stripe_width = data_chunks *\n\terasure_code->get_chunk_size(stripe_unit * data_chunks);\n    }\n    break;\n  default:\n    *ss << \"prepare_pool_stripe_width: \"\n       << pool_type << \" is not a known pool type\";\n    err = -EINVAL;\n    break;\n  }\n  return err;\n}\n\nint OSDMonitor::prepare_pool_crush_rule(const unsigned pool_type,\n\t\t\t\t\tconst string &erasure_code_profile,\n\t\t\t\t\tconst string &rule_name,\n\t\t\t\t\tint *crush_rule,\n\t\t\t\t\tostream *ss)\n{\n\n  if (*crush_rule < 0) {\n    switch (pool_type) {\n    case pg_pool_t::TYPE_REPLICATED:\n      {\n\tif (rule_name == \"\") {\n\t  // Use default rule\n\t  *crush_rule = osdmap.crush->get_osd_pool_default_crush_replicated_ruleset(cct);\n\t  if (*crush_rule < 0) {\n\t    // Errors may happen e.g. if no valid rule is available\n\t    *ss << \"No suitable CRUSH rule exists, check \"\n                << \"'osd pool default crush *' config options\";\n\t    return -ENOENT;\n\t  }\n\t} else {\n\t  return get_crush_rule(rule_name, crush_rule, ss);\n\t}\n      }\n      break;\n    case pg_pool_t::TYPE_ERASURE:\n      {\n\tint err = crush_rule_create_erasure(rule_name,\n\t\t\t\t\t       erasure_code_profile,\n\t\t\t\t\t       crush_rule, ss);\n\tswitch (err) {\n\tcase -EALREADY:\n\t  dout(20) << \"prepare_pool_crush_rule: rule \"\n\t\t   << rule_name << \" try again\" << dendl;\n\t  // fall through\n\tcase 0:\n\t  // need to wait for the crush rule to be proposed before proceeding\n\t  err = -EAGAIN;\n\t  break;\n\tcase -EEXIST:\n\t  err = 0;\n\t  break;\n \t}\n\treturn err;\n      }\n      break;\n    default:\n      *ss << \"prepare_pool_crush_rule: \" << pool_type\n\t << \" is not a known pool type\";\n      return -EINVAL;\n      break;\n    }\n  } else {\n    if (!osdmap.crush->ruleset_exists(*crush_rule)) {\n      *ss << \"CRUSH rule \" << *crush_rule << \" not found\";\n      return -ENOENT;\n    }\n  }\n\n  return 0;\n}\n\nint OSDMonitor::get_crush_rule(const string &rule_name,\n\t\t\t       int *crush_rule,\n\t\t\t       ostream *ss)\n{\n  int ret;\n  ret = osdmap.crush->get_rule_id(rule_name);\n  if (ret != -ENOENT) {\n    // found it, use it\n    *crush_rule = ret;\n  } else {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    ret = newcrush.get_rule_id(rule_name);\n    if (ret != -ENOENT) {\n      // found it, wait for it to be proposed\n      dout(20) << __func__ << \": rule \" << rule_name\n\t       << \" try again\" << dendl;\n      return -EAGAIN;\n    } else {\n      // Cannot find it , return error\n      *ss << \"specified rule \" << rule_name << \" doesn't exist\";\n      return ret;\n    }\n  }\n  return 0;\n}\n\nint OSDMonitor::check_pg_num(int64_t pool, int pg_num, int size, ostream *ss)\n{\n  auto max_pgs_per_osd = g_conf->get_val<uint64_t>(\"mon_max_pg_per_osd\");\n  auto num_osds = std::max(osdmap.get_num_in_osds(), 3u);   // assume min cluster size 3\n  auto max_pgs = max_pgs_per_osd * num_osds;\n  uint64_t projected = 0;\n  if (pool < 0) {\n    projected += pg_num * size;\n  }\n  for (const auto& i : osdmap.get_pools()) {\n    if (i.first == pool) {\n      projected += pg_num * size;\n    } else {\n      projected += i.second.get_pg_num() * i.second.get_size();\n    }\n  }\n  if (projected > max_pgs) {\n    if (pool >= 0) {\n      *ss << \"pool id \" << pool;\n    }\n    *ss << \" pg_num \" << pg_num << \" size \" << size\n\t<< \" would mean \" << projected\n\t<< \" total pgs, which exceeds max \" << max_pgs\n\t<< \" (mon_max_pg_per_osd \" << max_pgs_per_osd\n\t<< \" * num_in_osds \" << num_osds << \")\";\n    return -ERANGE;\n  }\n  return 0;\n}\n\n/**\n * @param name The name of the new pool\n * @param auid The auid of the pool owner. Can be -1\n * @param crush_rule The crush rule to use. If <0, will use the system default\n * @param crush_rule_name The crush rule to use, if crush_rulset <0\n * @param pg_num The pg_num to use. If set to 0, will use the system default\n * @param pgp_num The pgp_num to use. If set to 0, will use the system default\n * @param erasure_code_profile The profile name in OSDMap to be used for erasure code\n * @param pool_type TYPE_ERASURE, or TYPE_REP\n * @param expected_num_objects expected number of objects on the pool\n * @param fast_read fast read type. \n * @param ss human readable error message, if any.\n *\n * @return 0 on success, negative errno on failure.\n */\nint OSDMonitor::prepare_new_pool(string& name, uint64_t auid,\n\t\t\t\t int crush_rule,\n\t\t\t\t const string &crush_rule_name,\n                                 unsigned pg_num, unsigned pgp_num,\n\t\t\t\t const string &erasure_code_profile,\n                                 const unsigned pool_type,\n                                 const uint64_t expected_num_objects,\n                                 FastReadType fast_read,\n\t\t\t\t ostream *ss)\n{\n  if (name.length() == 0)\n    return -EINVAL;\n  if (pg_num == 0)\n    pg_num = g_conf->get_val<uint64_t>(\"osd_pool_default_pg_num\");\n  if (pgp_num == 0)\n    pgp_num = g_conf->get_val<uint64_t>(\"osd_pool_default_pgp_num\");\n  if (pg_num > g_conf->get_val<uint64_t>(\"mon_max_pool_pg_num\")) {\n    *ss << \"'pg_num' must be greater than 0 and less than or equal to \"\n        << g_conf->get_val<uint64_t>(\"mon_max_pool_pg_num\")\n        << \" (you may adjust 'mon max pool pg num' for higher values)\";\n    return -ERANGE;\n  }\n  if (pgp_num > pg_num) {\n    *ss << \"'pgp_num' must be greater than 0 and lower or equal than 'pg_num'\"\n        << \", which in this case is \" << pg_num;\n    return -ERANGE;\n  }\n  if (pool_type == pg_pool_t::TYPE_REPLICATED && fast_read == FAST_READ_ON) {\n    *ss << \"'fast_read' can only apply to erasure coding pool\";\n    return -EINVAL;\n  }\n  int r;\n  r = prepare_pool_crush_rule(pool_type, erasure_code_profile,\n\t\t\t\t crush_rule_name, &crush_rule, ss);\n  if (r) {\n    dout(10) << \"prepare_pool_crush_rule returns \" << r << dendl;\n    return r;\n  }\n  if (g_conf->mon_osd_crush_smoke_test) {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    ostringstream err;\n    CrushTester tester(newcrush, err);\n    tester.set_min_x(0);\n    tester.set_max_x(50);\n    tester.set_rule(crush_rule);\n    auto start = ceph::coarse_mono_clock::now();\n    r = tester.test_with_fork(g_conf->mon_lease);\n    auto duration = ceph::coarse_mono_clock::now() - start;\n    if (r < 0) {\n      dout(10) << \"tester.test_with_fork returns \" << r\n\t       << \": \" << err.str() << dendl;\n      *ss << \"crush test failed with \" << r << \": \" << err.str();\n      return r;\n    }\n    dout(10) << __func__ << \" crush smoke test duration: \"\n             << duration << dendl;\n  }\n  unsigned size, min_size;\n  r = prepare_pool_size(pool_type, erasure_code_profile, &size, &min_size, ss);\n  if (r) {\n    dout(10) << \"prepare_pool_size returns \" << r << dendl;\n    return r;\n  }\n  r = check_pg_num(-1, pg_num, size, ss);\n  if (r) {\n    dout(10) << \"check_pg_num returns \" << r << dendl;\n    return r;\n  }\n\n  if (!osdmap.crush->check_crush_rule(crush_rule, pool_type, size, *ss)) {\n    return -EINVAL;\n  }\n\n  uint32_t stripe_width = 0;\n  r = prepare_pool_stripe_width(pool_type, erasure_code_profile, &stripe_width, ss);\n  if (r) {\n    dout(10) << \"prepare_pool_stripe_width returns \" << r << dendl;\n    return r;\n  }\n  \n  bool fread = false;\n  if (pool_type == pg_pool_t::TYPE_ERASURE) {\n    switch (fast_read) {\n      case FAST_READ_OFF:\n        fread = false;\n        break;\n      case FAST_READ_ON:\n        fread = true;\n        break;\n      case FAST_READ_DEFAULT:\n        fread = g_conf->mon_osd_pool_ec_fast_read;\n        break;\n      default:\n        *ss << \"invalid fast_read setting: \" << fast_read;\n        return -EINVAL;\n    }\n  }\n\n  for (map<int64_t,string>::iterator p = pending_inc.new_pool_names.begin();\n       p != pending_inc.new_pool_names.end();\n       ++p) {\n    if (p->second == name)\n      return 0;\n  }\n\n  if (-1 == pending_inc.new_pool_max)\n    pending_inc.new_pool_max = osdmap.pool_max;\n  int64_t pool = ++pending_inc.new_pool_max;\n  pg_pool_t empty;\n  pg_pool_t *pi = pending_inc.get_new_pool(pool, &empty);\n  pi->create_time = ceph_clock_now();\n  pi->type = pool_type;\n  pi->fast_read = fread; \n  pi->flags = g_conf->osd_pool_default_flags;\n  if (g_conf->osd_pool_default_flag_hashpspool)\n    pi->set_flag(pg_pool_t::FLAG_HASHPSPOOL);\n  if (g_conf->osd_pool_default_flag_nodelete)\n    pi->set_flag(pg_pool_t::FLAG_NODELETE);\n  if (g_conf->osd_pool_default_flag_nopgchange)\n    pi->set_flag(pg_pool_t::FLAG_NOPGCHANGE);\n  if (g_conf->osd_pool_default_flag_nosizechange)\n    pi->set_flag(pg_pool_t::FLAG_NOSIZECHANGE);\n  if (g_conf->osd_pool_use_gmt_hitset)\n    pi->use_gmt_hitset = true;\n  else\n    pi->use_gmt_hitset = false;\n\n  pi->size = size;\n  pi->min_size = min_size;\n  pi->crush_rule = crush_rule;\n  pi->expected_num_objects = expected_num_objects;\n  pi->object_hash = CEPH_STR_HASH_RJENKINS;\n  pi->set_pg_num(pg_num);\n  pi->set_pgp_num(pgp_num);\n  pi->last_change = pending_inc.epoch;\n  pi->auid = auid;\n  if (pool_type == pg_pool_t::TYPE_ERASURE) {\n      pi->erasure_code_profile = erasure_code_profile;\n  } else {\n      pi->erasure_code_profile = \"\";\n  }\n  pi->stripe_width = stripe_width;\n  pi->cache_target_dirty_ratio_micro =\n    g_conf->osd_pool_default_cache_target_dirty_ratio * 1000000;\n  pi->cache_target_dirty_high_ratio_micro =\n    g_conf->osd_pool_default_cache_target_dirty_high_ratio * 1000000;\n  pi->cache_target_full_ratio_micro =\n    g_conf->osd_pool_default_cache_target_full_ratio * 1000000;\n  pi->cache_min_flush_age = g_conf->osd_pool_default_cache_min_flush_age;\n  pi->cache_min_evict_age = g_conf->osd_pool_default_cache_min_evict_age;\n  pending_inc.new_pool_names[pool] = name;\n  return 0;\n}\n\nbool OSDMonitor::prepare_set_flag(MonOpRequestRef op, int flag)\n{\n  op->mark_osdmon_event(__func__);\n  ostringstream ss;\n  if (pending_inc.new_flags < 0)\n    pending_inc.new_flags = osdmap.get_flags();\n  pending_inc.new_flags |= flag;\n  ss << OSDMap::get_flag_string(flag) << \" is set\";\n  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t\t    get_last_committed() + 1));\n  return true;\n}\n\nbool OSDMonitor::prepare_unset_flag(MonOpRequestRef op, int flag)\n{\n  op->mark_osdmon_event(__func__);\n  ostringstream ss;\n  if (pending_inc.new_flags < 0)\n    pending_inc.new_flags = osdmap.get_flags();\n  pending_inc.new_flags &= ~flag;\n  ss << OSDMap::get_flag_string(flag) << \" is unset\";\n  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t\t    get_last_committed() + 1));\n  return true;\n}\n\nint OSDMonitor::prepare_command_pool_set(const cmdmap_t& cmdmap,\n                                         stringstream& ss)\n{\n  string poolstr;\n  cmd_getval(cct, cmdmap, \"pool\", poolstr);\n  int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n  if (pool < 0) {\n    ss << \"unrecognized pool '\" << poolstr << \"'\";\n    return -ENOENT;\n  }\n  string var;\n  cmd_getval(cct, cmdmap, \"var\", var);\n\n  pg_pool_t p = *osdmap.get_pg_pool(pool);\n  if (pending_inc.new_pools.count(pool))\n    p = pending_inc.new_pools[pool];\n\n  // accept val as a json string in the normal case (current\n  // generation monitor).  parse out int or float values from the\n  // string as needed.  however, if it is not a string, try to pull\n  // out an int, in case an older monitor with an older json schema is\n  // forwarding a request.\n  string val;\n  string interr, floaterr;\n  int64_t n = 0;\n  double f = 0;\n  int64_t uf = 0;  // micro-f\n  if (!cmd_getval(cct, cmdmap, \"val\", val)) {\n    // wasn't a string; maybe an older mon forwarded json with an int?\n    if (!cmd_getval(cct, cmdmap, \"val\", n))\n      return -EINVAL;  // no value!\n  } else {\n    // we got a string.  see if it contains an int.\n    n = strict_strtoll(val.c_str(), 10, &interr);\n    // or a float\n    f = strict_strtod(val.c_str(), &floaterr);\n    uf = llrintl(f * (double)1000000.0);\n  }\n\n  if (!p.is_tier() &&\n      (var == \"hit_set_type\" || var == \"hit_set_period\" ||\n       var == \"hit_set_count\" || var == \"hit_set_fpp\" ||\n       var == \"target_max_objects\" || var == \"target_max_bytes\" ||\n       var == \"cache_target_full_ratio\" || var == \"cache_target_dirty_ratio\" ||\n       var == \"cache_target_dirty_high_ratio\" || var == \"use_gmt_hitset\" ||\n       var == \"cache_min_flush_age\" || var == \"cache_min_evict_age\" ||\n       var == \"hit_set_grade_decay_rate\" || var == \"hit_set_search_last_n\" ||\n       var == \"min_read_recency_for_promote\" || var == \"min_write_recency_for_promote\")) {\n    return -EACCES;\n  }\n\n  if (var == \"size\") {\n    if (p.has_flag(pg_pool_t::FLAG_NOSIZECHANGE)) {\n      ss << \"pool size change is disabled; you must unset nosizechange flag for the pool first\";\n      return -EPERM;\n    }\n    if (p.type == pg_pool_t::TYPE_ERASURE) {\n      ss << \"can not change the size of an erasure-coded pool\";\n      return -ENOTSUP;\n    }\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n <= 0 || n > 10) {\n      ss << \"pool size must be between 1 and 10\";\n      return -EINVAL;\n    }\n    int r = check_pg_num(pool, p.get_pg_num(), n, &ss);\n    if (r < 0) {\n      return r;\n    }\n    p.size = n;\n    if (n < p.min_size)\n      p.min_size = n;\n  } else if (var == \"min_size\") {\n    if (p.has_flag(pg_pool_t::FLAG_NOSIZECHANGE)) {\n      ss << \"pool min size change is disabled; you must unset nosizechange flag for the pool first\";\n      return -EPERM;\n    }\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n\n    if (p.type != pg_pool_t::TYPE_ERASURE) {\n      if (n < 1 || n > p.size) {\n\tss << \"pool min_size must be between 1 and \" << (int)p.size;\n\treturn -EINVAL;\n      }\n    } else {\n       ErasureCodeInterfaceRef erasure_code;\n       int k;\n       stringstream tmp;\n       int err = get_erasure_code(p.erasure_code_profile, &erasure_code, &tmp);\n       if (err == 0) {\n\t k = erasure_code->get_data_chunk_count();\n       } else {\n\t ss << __func__ << \" get_erasure_code failed: \" << tmp.str();\n\t return err;\n       }\n\n       if (n < k || n > p.size) {\n\t ss << \"pool min_size must be between \" << k << \" and \" << (int)p.size;\n\t return -EINVAL;\n       }\n    }\n    p.min_size = n;\n  } else if (var == \"auid\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.auid = n;\n  } else if (var == \"pg_num\") {\n    if (p.has_flag(pg_pool_t::FLAG_NOPGCHANGE)) {\n      ss << \"pool pg_num change is disabled; you must unset nopgchange flag for the pool first\";\n      return -EPERM;\n    }\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n <= (int)p.get_pg_num()) {\n      ss << \"specified pg_num \" << n << \" <= current \" << p.get_pg_num();\n      if (n < (int)p.get_pg_num())\n\treturn -EEXIST;\n      return 0;\n    }\n    if (static_cast<uint64_t>(n) > g_conf->get_val<uint64_t>(\"mon_max_pool_pg_num\")) {\n      ss << \"'pg_num' must be greater than 0 and less than or equal to \"\n         << g_conf->get_val<uint64_t>(\"mon_max_pool_pg_num\")\n         << \" (you may adjust 'mon max pool pg num' for higher values)\";\n      return -ERANGE;\n    }\n    int r = check_pg_num(pool, n, p.get_size(), &ss);\n    if (r) {\n      return r;\n    }\n    string force;\n    cmd_getval(cct,cmdmap, \"force\", force);\n    if (p.cache_mode != pg_pool_t::CACHEMODE_NONE &&\n\tforce != \"--yes-i-really-mean-it\") {\n      ss << \"splits in cache pools must be followed by scrubs and leave sufficient free space to avoid overfilling.  use --yes-i-really-mean-it to force.\";\n      return -EPERM;\n    }\n    int expected_osds = std::min(p.get_pg_num(), osdmap.get_num_osds());\n    int64_t new_pgs = n - p.get_pg_num();\n    if (new_pgs > g_conf->mon_osd_max_split_count * expected_osds) {\n      ss << \"specified pg_num \" << n << \" is too large (creating \"\n\t << new_pgs << \" new PGs on ~\" << expected_osds\n\t << \" OSDs exceeds per-OSD max with mon_osd_max_split_count of \"\n         << g_conf->mon_osd_max_split_count << ')';\n      return -E2BIG;\n    }\n    p.set_pg_num(n);\n    // force pre-luminous clients to resend their ops, since they\n    // don't understand that split PGs now form a new interval.\n    p.last_force_op_resend_preluminous = pending_inc.epoch;\n  } else if (var == \"pgp_num\") {\n    if (p.has_flag(pg_pool_t::FLAG_NOPGCHANGE)) {\n      ss << \"pool pgp_num change is disabled; you must unset nopgchange flag for the pool first\";\n      return -EPERM;\n    }\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n <= 0) {\n      ss << \"specified pgp_num must > 0, but you set to \" << n;\n      return -EINVAL;\n    }\n    if (n > (int)p.get_pg_num()) {\n      ss << \"specified pgp_num \" << n << \" > pg_num \" << p.get_pg_num();\n      return -EINVAL;\n    }\n    p.set_pgp_num(n);\n  } else if (var == \"crush_rule\") {\n    int id = osdmap.crush->get_rule_id(val);\n    if (id == -ENOENT) {\n      ss << \"crush rule \" << val << \" does not exist\";\n      return -ENOENT;\n    }\n    if (id < 0) {\n      ss << cpp_strerror(id);\n      return -ENOENT;\n    }\n    if (!osdmap.crush->check_crush_rule(id, p.get_type(), p.get_size(), ss)) {\n      return -EINVAL;\n    }\n    p.crush_rule = id;\n  } else if (var == \"nodelete\" || var == \"nopgchange\" ||\n\t     var == \"nosizechange\" || var == \"write_fadvise_dontneed\" ||\n\t     var == \"noscrub\" || var == \"nodeep-scrub\") {\n    uint64_t flag = pg_pool_t::get_flag_by_name(var);\n    // make sure we only compare against 'n' if we didn't receive a string\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n      p.set_flag(flag);\n    } else if (val == \"false\" || (interr.empty() && n == 0)) {\n      p.unset_flag(flag);\n    } else {\n      ss << \"expecting value 'true', 'false', '0', or '1'\";\n      return -EINVAL;\n    }\n  } else if (var == \"hashpspool\") {\n    uint64_t flag = pg_pool_t::get_flag_by_name(var);\n    string force;\n    cmd_getval(cct, cmdmap, \"force\", force);\n    if (force != \"--yes-i-really-mean-it\") {\n      ss << \"are you SURE?  this will remap all placement groups in this pool,\"\n\t    \" this triggers large data movement,\"\n\t    \" pass --yes-i-really-mean-it if you really do.\";\n      return -EPERM;\n    }\n    // make sure we only compare against 'n' if we didn't receive a string\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n      p.set_flag(flag);\n    } else if (val == \"false\" || (interr.empty() && n == 0)) {\n      p.unset_flag(flag);\n    } else {\n      ss << \"expecting value 'true', 'false', '0', or '1'\";\n      return -EINVAL;\n    }\n  } else if (var == \"hit_set_type\") {\n    if (val == \"none\")\n      p.hit_set_params = HitSet::Params();\n    else {\n      int err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n      if (err)\n\treturn err;\n      if (val == \"bloom\") {\n\tBloomHitSet::Params *bsp = new BloomHitSet::Params;\n\tbsp->set_fpp(g_conf->get_val<double>(\"osd_pool_default_hit_set_bloom_fpp\"));\n\tp.hit_set_params = HitSet::Params(bsp);\n      } else if (val == \"explicit_hash\")\n\tp.hit_set_params = HitSet::Params(new ExplicitHashHitSet::Params);\n      else if (val == \"explicit_object\")\n\tp.hit_set_params = HitSet::Params(new ExplicitObjectHitSet::Params);\n      else {\n\tss << \"unrecognized hit_set type '\" << val << \"'\";\n\treturn -EINVAL;\n      }\n    }\n  } else if (var == \"hit_set_period\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.hit_set_period = n;\n  } else if (var == \"hit_set_count\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.hit_set_count = n;\n  } else if (var == \"hit_set_fpp\") {\n    if (floaterr.length()) {\n      ss << \"error parsing floating point value '\" << val << \"': \" << floaterr;\n      return -EINVAL;\n    }\n    if (p.hit_set_params.get_type() != HitSet::TYPE_BLOOM) {\n      ss << \"hit set is not of type Bloom; invalid to set a false positive rate!\";\n      return -EINVAL;\n    }\n    BloomHitSet::Params *bloomp = static_cast<BloomHitSet::Params*>(p.hit_set_params.impl.get());\n    bloomp->set_fpp(f);\n  } else if (var == \"use_gmt_hitset\") {\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n      p.use_gmt_hitset = true;\n    } else {\n      ss << \"expecting value 'true' or '1'\";\n      return -EINVAL;\n    }\n  } else if (var == \"allow_ec_overwrites\") {\n    if (!p.is_erasure()) {\n      ss << \"ec overwrites can only be enabled for an erasure coded pool\";\n      return -EINVAL;\n    }\n    stringstream err;\n    if (!g_conf->mon_debug_no_require_bluestore_for_ec_overwrites &&\n\t!is_pool_currently_all_bluestore(pool, p, &err)) {\n      ss << \"pool must only be stored on bluestore for scrubbing to work: \" << err.str();\n      return -EINVAL;\n    }\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n\tp.flags |= pg_pool_t::FLAG_EC_OVERWRITES;\n    } else if (val == \"false\" || (interr.empty() && n == 0)) {\n      ss << \"ec overwrites cannot be disabled once enabled\";\n      return -EINVAL;\n    } else {\n      ss << \"expecting value 'true', 'false', '0', or '1'\";\n      return -EINVAL;\n    }\n  } else if (var == \"target_max_objects\") {\n    if (interr.length()) {\n      ss << \"error parsing int '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.target_max_objects = n;\n  } else if (var == \"target_max_bytes\") {\n    if (interr.length()) {\n      ss << \"error parsing int '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.target_max_bytes = n;\n  } else if (var == \"cache_target_dirty_ratio\") {\n    if (floaterr.length()) {\n      ss << \"error parsing float '\" << val << \"': \" << floaterr;\n      return -EINVAL;\n    }\n    if (f < 0 || f > 1.0) {\n      ss << \"value must be in the range 0..1\";\n      return -ERANGE;\n    }\n    p.cache_target_dirty_ratio_micro = uf;\n  } else if (var == \"cache_target_dirty_high_ratio\") {\n    if (floaterr.length()) {\n      ss << \"error parsing float '\" << val << \"': \" << floaterr;\n      return -EINVAL;\n    }\n    if (f < 0 || f > 1.0) {\n      ss << \"value must be in the range 0..1\";\n      return -ERANGE;\n    }\n    p.cache_target_dirty_high_ratio_micro = uf;\n  } else if (var == \"cache_target_full_ratio\") {\n    if (floaterr.length()) {\n      ss << \"error parsing float '\" << val << \"': \" << floaterr;\n      return -EINVAL;\n    }\n    if (f < 0 || f > 1.0) {\n      ss << \"value must be in the range 0..1\";\n      return -ERANGE;\n    }\n    p.cache_target_full_ratio_micro = uf;\n  } else if (var == \"cache_min_flush_age\") {\n    if (interr.length()) {\n      ss << \"error parsing int '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.cache_min_flush_age = n;\n  } else if (var == \"cache_min_evict_age\") {\n    if (interr.length()) {\n      ss << \"error parsing int '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.cache_min_evict_age = n;\n  } else if (var == \"min_read_recency_for_promote\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.min_read_recency_for_promote = n;\n  } else if (var == \"hit_set_grade_decay_rate\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n > 100 || n < 0) {\n      ss << \"value out of range,valid range is 0 - 100\";\n      return -EINVAL;\n    }\n    p.hit_set_grade_decay_rate = n;\n  } else if (var == \"hit_set_search_last_n\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    if (n > p.hit_set_count || n < 0) {\n      ss << \"value out of range,valid range is 0 - hit_set_count\";\n      return -EINVAL;\n    }\n    p.hit_set_search_last_n = n;\n  } else if (var == \"min_write_recency_for_promote\") {\n    if (interr.length()) {\n      ss << \"error parsing integer value '\" << val << \"': \" << interr;\n      return -EINVAL;\n    }\n    p.min_write_recency_for_promote = n;\n  } else if (var == \"fast_read\") {\n    if (p.is_replicated()) {\n        ss << \"fast read is not supported in replication pool\";\n        return -EINVAL;\n    }\n    if (val == \"true\" || (interr.empty() && n == 1)) {\n      p.fast_read = true;\n    } else if (val == \"false\" || (interr.empty() && n == 0)) {\n      p.fast_read = false;\n    } else {\n      ss << \"expecting value 'true', 'false', '0', or '1'\";\n      return -EINVAL;\n    }\n  } else if (pool_opts_t::is_opt_name(var)) {\n    bool unset = val == \"unset\";\n    if (var == \"compression_mode\") {\n      if (!unset) {\n        auto cmode = Compressor::get_comp_mode_type(val);\n        if (!cmode) {\n\t  ss << \"unrecognized compression mode '\" << val << \"'\";\n\t  return -EINVAL;\n        }\n      }\n    } else if (var == \"compression_algorithm\") {\n      if (!unset) {\n        auto alg = Compressor::get_comp_alg_type(val);\n        if (!alg) {\n          ss << \"unrecognized compression_algorithm '\" << val << \"'\";\n\t  return -EINVAL;\n        }\n      }\n    } else if (var == \"compression_required_ratio\") {\n      if (floaterr.length()) {\n        ss << \"error parsing float value '\" << val << \"': \" << floaterr;\n        return -EINVAL;\n      }\n      if (f < 0 || f > 1) {\n        ss << \"compression_required_ratio is out of range (0-1): '\" << val << \"'\";\n\treturn -EINVAL;\n      }\n    } else if (var == \"csum_type\") {\n      auto t = unset ? 0 : Checksummer::get_csum_string_type(val);\n      if (t < 0 ) {\n        ss << \"unrecognized csum_type '\" << val << \"'\";\n\treturn -EINVAL;\n      }\n      //preserve csum_type numeric value\n      n = t;\n      interr.clear(); \n    } else if (var == \"compression_max_blob_size\" ||\n               var == \"compression_min_blob_size\" ||\n               var == \"csum_max_block\" ||\n               var == \"csum_min_block\") {\n      if (interr.length()) {\n        ss << \"error parsing int value '\" << val << \"': \" << interr;\n        return -EINVAL;\n      }\n    }\n\n    pool_opts_t::opt_desc_t desc = pool_opts_t::get_opt_desc(var);\n    switch (desc.type) {\n    case pool_opts_t::STR:\n      if (unset) {\n\tp.opts.unset(desc.key);\n      } else {\n\tp.opts.set(desc.key, static_cast<std::string>(val));\n      }\n      break;\n    case pool_opts_t::INT:\n      if (interr.length()) {\n\tss << \"error parsing integer value '\" << val << \"': \" << interr;\n\treturn -EINVAL;\n      }\n      if (n == 0) {\n\tp.opts.unset(desc.key);\n      } else {\n\tp.opts.set(desc.key, static_cast<int>(n));\n      }\n      break;\n    case pool_opts_t::DOUBLE:\n      if (floaterr.length()) {\n\tss << \"error parsing floating point value '\" << val << \"': \" << floaterr;\n\treturn -EINVAL;\n      }\n      if (f == 0) {\n\tp.opts.unset(desc.key);\n      } else {\n\tp.opts.set(desc.key, static_cast<double>(f));\n      }\n      break;\n    default:\n      assert(!\"unknown type\");\n    }\n  } else {\n    ss << \"unrecognized variable '\" << var << \"'\";\n    return -EINVAL;\n  }\n  if (val != \"unset\") {\n    ss << \"set pool \" << pool << \" \" << var << \" to \" << val;\n  } else {\n    ss << \"unset pool \" << pool << \" \" << var;\n  }\n  p.last_change = pending_inc.epoch;\n  pending_inc.new_pools[pool] = p;\n  return 0;\n}\n\nint OSDMonitor::prepare_command_pool_application(const string &prefix,\n                                                 const cmdmap_t& cmdmap,\n                                                 stringstream& ss)\n{\n  string pool_name;\n  cmd_getval(cct, cmdmap, \"pool\", pool_name);\n  int64_t pool = osdmap.lookup_pg_pool_name(pool_name.c_str());\n  if (pool < 0) {\n    ss << \"unrecognized pool '\" << pool_name << \"'\";\n    return -ENOENT;\n  }\n\n  pg_pool_t p = *osdmap.get_pg_pool(pool);\n  if (pending_inc.new_pools.count(pool)) {\n    p = pending_inc.new_pools[pool];\n  }\n\n  string app;\n  cmd_getval(cct, cmdmap, \"app\", app);\n  bool app_exists = (p.application_metadata.count(app) > 0);\n\n  string key;\n  cmd_getval(cct, cmdmap, \"key\", key);\n  if (key == \"all\") {\n    ss << \"key cannot be 'all'\";\n    return -EINVAL;\n  }\n\n  string value;\n  cmd_getval(cct, cmdmap, \"value\", value);\n  if (value == \"all\") {\n    ss << \"value cannot be 'all'\";\n    return -EINVAL;\n  }\n\n  if (boost::algorithm::ends_with(prefix, \"enable\")) {\n    if (app.empty()) {\n      ss << \"application name must be provided\";\n      return -EINVAL;\n    }\n\n    if (p.is_tier()) {\n      ss << \"application must be enabled on base tier\";\n      return -EINVAL;\n    }\n\n    string force;\n    cmd_getval(cct, cmdmap, \"force\", force);\n\n    if (!app_exists && !p.application_metadata.empty() &&\n        force != \"--yes-i-really-mean-it\") {\n      ss << \"Are you SURE? Pool '\" << pool_name << \"' already has an enabled \"\n         << \"application; pass --yes-i-really-mean-it to proceed anyway\";\n      return -EPERM;\n    }\n\n    if (!app_exists && p.application_metadata.size() >= MAX_POOL_APPLICATIONS) {\n      ss << \"too many enabled applications on pool '\" << pool_name << \"'; \"\n         << \"max \" << MAX_POOL_APPLICATIONS;\n      return -EINVAL;\n    }\n\n    if (app.length() > MAX_POOL_APPLICATION_LENGTH) {\n      ss << \"application name '\" << app << \"' too long; max length \"\n         << MAX_POOL_APPLICATION_LENGTH;\n      return -EINVAL;\n    }\n\n    if (!app_exists) {\n      p.application_metadata[app] = {};\n    }\n    ss << \"enabled application '\" << app << \"' on pool '\" << pool_name << \"'\";\n\n  } else if (boost::algorithm::ends_with(prefix, \"disable\")) {\n    string force;\n    cmd_getval(cct, cmdmap, \"force\", force);\n\n    if (force != \"--yes-i-really-mean-it\") {\n      ss << \"Are you SURE? Disabling an application within a pool might result \"\n         << \"in loss of application functionality; pass \"\n         << \"--yes-i-really-mean-it to proceed anyway\";\n      return -EPERM;\n    }\n\n    if (!app_exists) {\n      ss << \"application '\" << app << \"' is not enabled on pool '\" << pool_name\n         << \"'\";\n      return 0; // idempotent\n    }\n\n    p.application_metadata.erase(app);\n    ss << \"disable application '\" << app << \"' on pool '\" << pool_name << \"'\";\n\n  } else if (boost::algorithm::ends_with(prefix, \"set\")) {\n    if (p.is_tier()) {\n      ss << \"application metadata must be set on base tier\";\n      return -EINVAL;\n    }\n\n    if (!app_exists) {\n      ss << \"application '\" << app << \"' is not enabled on pool '\" << pool_name\n         << \"'\";\n      return -ENOENT;\n    }\n\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n\n    if (key.empty()) {\n      ss << \"key must be provided\";\n      return -EINVAL;\n    }\n\n    auto &app_keys = p.application_metadata[app];\n    if (app_keys.count(key) == 0 &&\n        app_keys.size() >= MAX_POOL_APPLICATION_KEYS) {\n      ss << \"too many keys set for application '\" << app << \"' on pool '\"\n         << pool_name << \"'; max \" << MAX_POOL_APPLICATION_KEYS;\n      return -EINVAL;\n    }\n\n    if (key.length() > MAX_POOL_APPLICATION_LENGTH) {\n      ss << \"key '\" << app << \"' too long; max length \"\n         << MAX_POOL_APPLICATION_LENGTH;\n      return -EINVAL;\n    }\n\n    string value;\n    cmd_getval(cct, cmdmap, \"value\", value);\n    if (value.length() > MAX_POOL_APPLICATION_LENGTH) {\n      ss << \"value '\" << value << \"' too long; max length \"\n         << MAX_POOL_APPLICATION_LENGTH;\n      return -EINVAL;\n    }\n\n    p.application_metadata[app][key] = value;\n    ss << \"set application '\" << app << \"' key '\" << key << \"' to '\"\n       << value << \"' on pool '\" << pool_name << \"'\";\n  } else if (boost::algorithm::ends_with(prefix, \"rm\")) {\n    if (!app_exists) {\n      ss << \"application '\" << app << \"' is not enabled on pool '\" << pool_name\n         << \"'\";\n      return -ENOENT;\n    }\n\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n    auto it = p.application_metadata[app].find(key);\n    if (it == p.application_metadata[app].end()) {\n      ss << \"application '\" << app << \"' on pool '\" << pool_name\n         << \"' does not have key '\" << key << \"'\";\n      return 0; // idempotent\n    }\n\n    p.application_metadata[app].erase(it);\n    ss << \"removed application '\" << app << \"' key '\" << key << \"' on pool '\"\n       << pool_name << \"'\";\n  } else {\n    ceph_abort();\n  }\n\n  p.last_change = pending_inc.epoch;\n  pending_inc.new_pools[pool] = p;\n  return 0;\n}\n\nint OSDMonitor::_prepare_command_osd_crush_remove(\n    CrushWrapper &newcrush,\n    int32_t id,\n    int32_t ancestor,\n    bool has_ancestor,\n    bool unlink_only)\n{\n  int err = 0;\n\n  if (has_ancestor) {\n    err = newcrush.remove_item_under(cct, id, ancestor,\n        unlink_only);\n  } else {\n    err = newcrush.remove_item(cct, id, unlink_only);\n  }\n  return err;\n}\n\nvoid OSDMonitor::do_osd_crush_remove(CrushWrapper& newcrush)\n{\n  pending_inc.crush.clear();\n  newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n}\n\nint OSDMonitor::prepare_command_osd_crush_remove(\n    CrushWrapper &newcrush,\n    int32_t id,\n    int32_t ancestor,\n    bool has_ancestor,\n    bool unlink_only)\n{\n  int err = _prepare_command_osd_crush_remove(\n      newcrush, id, ancestor,\n      has_ancestor, unlink_only);\n\n  if (err < 0)\n    return err;\n\n  assert(err == 0);\n  do_osd_crush_remove(newcrush);\n\n  return 0;\n}\n\nint OSDMonitor::prepare_command_osd_remove(int32_t id)\n{\n  if (osdmap.is_up(id)) {\n    return -EBUSY;\n  }\n\n  pending_inc.new_state[id] = osdmap.get_state(id);\n  pending_inc.new_uuid[id] = uuid_d();\n  pending_metadata_rm.insert(id);\n  pending_metadata.erase(id);\n\n  return 0;\n}\n\nint32_t OSDMonitor::_allocate_osd_id(int32_t* existing_id)\n{\n  assert(existing_id);\n  *existing_id = -1;\n\n  for (int32_t i = 0; i < osdmap.get_max_osd(); ++i) {\n    if (!osdmap.exists(i) &&\n        pending_inc.new_up_client.count(i) == 0 &&\n        (pending_inc.new_state.count(i) == 0 ||\n         (pending_inc.new_state[i] & CEPH_OSD_EXISTS) == 0)) {\n      *existing_id = i;\n      return -1;\n    }\n  }\n\n  if (pending_inc.new_max_osd < 0) {\n    return osdmap.get_max_osd();\n  }\n  return pending_inc.new_max_osd;\n}\n\nvoid OSDMonitor::do_osd_create(\n    const int32_t id,\n    const uuid_d& uuid,\n    const string& device_class,\n    int32_t* new_id)\n{\n  dout(10) << __func__ << \" uuid \" << uuid << dendl;\n  assert(new_id);\n\n  // We presume validation has been performed prior to calling this\n  // function. We assert with prejudice.\n\n  int32_t allocated_id = -1; // declare here so we can jump\n  int32_t existing_id = -1;\n  if (!uuid.is_zero()) {\n    existing_id = osdmap.identify_osd(uuid);\n    if (existing_id >= 0) {\n      assert(id < 0 || id == existing_id);\n      *new_id = existing_id;\n      goto out;\n    } else if (id >= 0) {\n      // uuid does not exist, and id has been provided, so just create\n      // the new osd.id\n      *new_id = id;\n      goto out;\n    }\n  }\n\n  // allocate a new id\n  allocated_id = _allocate_osd_id(&existing_id);\n  dout(10) << __func__ << \" allocated id \" << allocated_id\n           << \" existing id \" << existing_id << dendl;\n  if (existing_id >= 0) {\n    assert(existing_id < osdmap.get_max_osd());\n    assert(allocated_id < 0);\n    pending_inc.new_weight[existing_id] = CEPH_OSD_OUT;\n    *new_id = existing_id;\n  } else if (allocated_id >= 0) {\n    assert(existing_id < 0);\n    // raise max_osd\n    if (pending_inc.new_max_osd < 0) {\n      pending_inc.new_max_osd = osdmap.get_max_osd() + 1;\n    } else {\n      ++pending_inc.new_max_osd;\n    }\n    *new_id = pending_inc.new_max_osd - 1;\n    assert(*new_id == allocated_id);\n  } else {\n    assert(0 == \"unexpected condition\");\n  }\n\nout:\n  if (device_class.size()) {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    if (newcrush.get_max_devices() < *new_id + 1) {\n      newcrush.set_max_devices(*new_id + 1);\n    }\n    string name = string(\"osd.\") + stringify(*new_id);\n    if (!newcrush.item_exists(*new_id)) {\n      newcrush.set_item_name(*new_id, name);\n    }\n    ostringstream ss;\n    int r = newcrush.update_device_class(*new_id, device_class, name, &ss);\n    if (r < 0) {\n      derr << __func__ << \" failed to set \" << name << \" device_class \"\n\t   << device_class << \": \" << cpp_strerror(r) << \" - \" << ss.str()\n\t   << dendl;\n      // non-fatal... this might be a replay and we want to be idempotent.\n    } else {\n      dout(20) << __func__ << \" set \" << name << \" device_class \" << device_class\n\t       << dendl;\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    }\n  } else {\n    dout(20) << __func__ << \" no device_class\" << dendl;\n  }\n\n  dout(10) << __func__ << \" using id \" << *new_id << dendl;\n  if (osdmap.get_max_osd() <= *new_id && pending_inc.new_max_osd <= *new_id) {\n    pending_inc.new_max_osd = *new_id + 1;\n  }\n\n  pending_inc.new_state[*new_id] |= CEPH_OSD_EXISTS | CEPH_OSD_NEW;\n  if (!uuid.is_zero())\n    pending_inc.new_uuid[*new_id] = uuid;\n}\n\nint OSDMonitor::validate_osd_create(\n    const int32_t id,\n    const uuid_d& uuid,\n    const bool check_osd_exists,\n    int32_t* existing_id,\n    stringstream& ss)\n{\n\n  dout(10) << __func__ << \" id \" << id << \" uuid \" << uuid\n           << \" check_osd_exists \" << check_osd_exists << dendl;\n\n  assert(existing_id);\n\n  if (id < 0 && uuid.is_zero()) {\n    // we have nothing to validate\n    *existing_id = -1;\n    return 0;\n  } else if (uuid.is_zero()) {\n    // we have an id but we will ignore it - because that's what\n    // `osd create` does.\n    return 0;\n  }\n\n  /*\n   * This function will be used to validate whether we are able to\n   * create a new osd when the `uuid` is specified.\n   *\n   * It will be used by both `osd create` and `osd new`, as the checks\n   * are basically the same when it pertains to osd id and uuid validation.\n   * However, `osd create` presumes an `uuid` is optional, for legacy\n   * reasons, while `osd new` requires the `uuid` to be provided. This\n   * means that `osd create` will not be idempotent if an `uuid` is not\n   * provided, but we will always guarantee the idempotency of `osd new`.\n   */\n\n  assert(!uuid.is_zero());\n  if (pending_inc.identify_osd(uuid) >= 0) {\n    // osd is about to exist\n    return -EAGAIN;\n  }\n\n  int32_t i = osdmap.identify_osd(uuid);\n  if (i >= 0) {\n    // osd already exists\n    if (id >= 0 && i != id) {\n      ss << \"uuid \" << uuid << \" already in use for different id \" << i;\n      return -EEXIST;\n    }\n    // return a positive errno to distinguish between a blocking error\n    // and an error we consider to not be a problem (i.e., this would be\n    // an idempotent operation).\n    *existing_id = i;\n    return EEXIST;\n  }\n  // i < 0\n  if (id >= 0) {\n    if (pending_inc.new_state.count(id)) {\n      // osd is about to exist\n      return -EAGAIN;\n    }\n    // we may not care if an osd exists if we are recreating a previously\n    // destroyed osd.\n    if (check_osd_exists && osdmap.exists(id)) {\n      ss << \"id \" << id << \" already in use and does not match uuid \"\n         << uuid;\n      return -EINVAL;\n    }\n  }\n  return 0;\n}\n\nint OSDMonitor::prepare_command_osd_create(\n    const int32_t id,\n    const uuid_d& uuid,\n    int32_t* existing_id,\n    stringstream& ss)\n{\n  dout(10) << __func__ << \" id \" << id << \" uuid \" << uuid << dendl;\n  assert(existing_id);\n  if (osdmap.is_destroyed(id)) {\n    ss << \"ceph osd create has been deprecated. Please use ceph osd new \"\n          \"instead.\";\n    return -EINVAL;\n  }\n\n  if (uuid.is_zero()) {\n    dout(10) << __func__ << \" no uuid; assuming legacy `osd create`\" << dendl;\n  }\n\n  return validate_osd_create(id, uuid, true, existing_id, ss);\n}\n\nint OSDMonitor::prepare_command_osd_new(\n    MonOpRequestRef op,\n    const cmdmap_t& cmdmap,\n    const map<string,string>& params,\n    stringstream &ss,\n    Formatter *f)\n{\n  uuid_d uuid;\n  string uuidstr;\n  int64_t id = -1;\n\n  assert(paxos->is_plugged());\n\n  dout(10) << __func__ << \" \" << op << dendl;\n\n  /* validate command. abort now if something's wrong. */\n\n  /* `osd new` will expect a `uuid` to be supplied; `id` is optional.\n   *\n   * If `id` is not specified, we will identify any existing osd based\n   * on `uuid`. Operation will be idempotent iff secrets match.\n   *\n   * If `id` is specified, we will identify any existing osd based on\n   * `uuid` and match against `id`. If they match, operation will be\n   * idempotent iff secrets match.\n   *\n   * `-i secrets.json` will be optional. If supplied, will be used\n   * to check for idempotency when `id` and `uuid` match.\n   *\n   * If `id` is not specified, and `uuid` does not exist, an id will\n   * be found or allocated for the osd.\n   *\n   * If `id` is specified, and the osd has been previously marked\n   * as destroyed, then the `id` will be reused.\n   */\n  if (!cmd_getval(cct, cmdmap, \"uuid\", uuidstr)) {\n    ss << \"requires the OSD's UUID to be specified.\";\n    return -EINVAL;\n  } else if (!uuid.parse(uuidstr.c_str())) {\n    ss << \"invalid UUID value '\" << uuidstr << \"'.\";\n    return -EINVAL;\n  }\n\n  if (cmd_getval(cct, cmdmap, \"id\", id) &&\n      (id < 0)) {\n    ss << \"invalid OSD id; must be greater or equal than zero.\";\n    return -EINVAL;\n  }\n\n  // are we running an `osd create`-like command, or recreating\n  // a previously destroyed osd?\n\n  bool is_recreate_destroyed = (id >= 0 && osdmap.is_destroyed(id));\n\n  // we will care about `id` to assess whether osd is `destroyed`, or\n  // to create a new osd.\n  // we will need an `id` by the time we reach auth.\n\n  int32_t existing_id = -1;\n  int err = validate_osd_create(id, uuid, !is_recreate_destroyed,\n                                &existing_id, ss);\n\n  bool may_be_idempotent = false;\n  if (err == EEXIST) {\n    // this is idempotent from the osdmon's point-of-view\n    may_be_idempotent = true;\n    assert(existing_id >= 0);\n    id = existing_id;\n  } else if (err < 0) {\n    return err;\n  }\n\n  if (!may_be_idempotent) {\n    // idempotency is out of the window. We are either creating a new\n    // osd or recreating a destroyed osd.\n    //\n    // We now need to figure out if we have an `id` (and if it's valid),\n    // of find an `id` if we don't have one.\n\n    // NOTE: we need to consider the case where the `id` is specified for\n    // `osd create`, and we must honor it. So this means checking if\n    // the `id` is destroyed, and if so assume the destroy; otherwise,\n    // check if it `exists` - in which case we complain about not being\n    // `destroyed`. In the end, if nothing fails, we must allow the\n    // creation, so that we are compatible with `create`.\n    if (id >= 0 && osdmap.exists(id) && !osdmap.is_destroyed(id)) {\n      dout(10) << __func__ << \" osd.\" << id << \" isn't destroyed\" << dendl;\n      ss << \"OSD \" << id << \" has not yet been destroyed\";\n      return -EINVAL;\n    } else if (id < 0) {\n      // find an `id`\n      id = _allocate_osd_id(&existing_id);\n      if (id < 0) {\n        assert(existing_id >= 0);\n        id = existing_id;\n      }\n      dout(10) << __func__ << \" found id \" << id << \" to use\" << dendl;\n    } else if (id >= 0 && osdmap.is_destroyed(id)) {\n      dout(10) << __func__ << \" recreating osd.\" << id << dendl;\n    } else {\n      dout(10) << __func__ << \" creating new osd.\" << id << dendl;\n    }\n  } else {\n    assert(id >= 0);\n    assert(osdmap.exists(id));\n  }\n\n  // we are now able to either create a brand new osd or reuse an existing\n  // osd that has been previously destroyed.\n\n  dout(10) << __func__ << \" id \" << id << \" uuid \" << uuid << dendl;\n\n  if (may_be_idempotent && params.empty()) {\n    // nothing to do, really.\n    dout(10) << __func__ << \" idempotent and no params -- no op.\" << dendl;\n    assert(id >= 0);\n    if (f) {\n      f->open_object_section(\"created_osd\");\n      f->dump_int(\"osdid\", id);\n      f->close_section();\n    } else {\n      ss << id;\n    }\n    return EEXIST;\n  }\n\n  string device_class;\n  auto p = params.find(\"crush_device_class\");\n  if (p != params.end()) {\n    device_class = p->second;\n    dout(20) << __func__ << \" device_class will be \" << device_class << dendl;\n  }\n  string cephx_secret, lockbox_secret, dmcrypt_key;\n  bool has_lockbox = false;\n  bool has_secrets = params.count(\"cephx_secret\")\n    || params.count(\"cephx_lockbox_secret\")\n    || params.count(\"dmcrypt_key\");\n\n  ConfigKeyService *svc = nullptr;\n  AuthMonitor::auth_entity_t cephx_entity, lockbox_entity;\n\n  if (has_secrets) {\n    if (params.count(\"cephx_secret\") == 0) {\n      ss << \"requires a cephx secret.\";\n      return -EINVAL;\n    }\n    cephx_secret = params.at(\"cephx_secret\");\n\n    bool has_lockbox_secret = (params.count(\"cephx_lockbox_secret\") > 0);\n    bool has_dmcrypt_key = (params.count(\"dmcrypt_key\") > 0);\n\n    dout(10) << __func__ << \" has lockbox \" << has_lockbox_secret\n             << \" dmcrypt \" << has_dmcrypt_key << dendl;\n\n    if (has_lockbox_secret && has_dmcrypt_key) {\n      has_lockbox = true;\n      lockbox_secret = params.at(\"cephx_lockbox_secret\");\n      dmcrypt_key = params.at(\"dmcrypt_key\");\n    } else if (!has_lockbox_secret != !has_dmcrypt_key) {\n      ss << \"requires both a cephx lockbox secret and a dm-crypt key.\";\n      return -EINVAL;\n    }\n\n    dout(10) << __func__ << \" validate secrets using osd id \" << id << dendl;\n\n    err = mon->authmon()->validate_osd_new(id, uuid,\n        cephx_secret,\n        lockbox_secret,\n        cephx_entity,\n        lockbox_entity,\n        ss);\n    if (err < 0) {\n      return err;\n    } else if (may_be_idempotent && err != EEXIST) {\n      // for this to be idempotent, `id` should already be >= 0; no need\n      // to use validate_id.\n      assert(id >= 0);\n      ss << \"osd.\" << id << \" exists but secrets do not match\";\n      return -EEXIST;\n    }\n\n    if (has_lockbox) {\n      svc = (ConfigKeyService*)mon->config_key_service;\n      err = svc->validate_osd_new(uuid, dmcrypt_key, ss);\n      if (err < 0) {\n        return err;\n      } else if (may_be_idempotent && err != EEXIST) {\n        assert(id >= 0);\n        ss << \"osd.\" << id << \" exists but dm-crypt key does not match.\";\n        return -EEXIST;\n      }\n    }\n  }\n  assert(!has_secrets || !cephx_secret.empty());\n  assert(!has_lockbox || !lockbox_secret.empty());\n\n  if (may_be_idempotent) {\n    // we have nothing to do for either the osdmon or the authmon,\n    // and we have no lockbox - so the config key service will not be\n    // touched. This is therefore an idempotent operation, and we can\n    // just return right away.\n    dout(10) << __func__ << \" idempotent -- no op.\" << dendl;\n    assert(id >= 0);\n    if (f) {\n      f->open_object_section(\"created_osd\");\n      f->dump_int(\"osdid\", id);\n      f->close_section();\n    } else {\n      ss << id;\n    }\n    return EEXIST;\n  }\n  assert(!may_be_idempotent);\n\n  // perform updates.\n  if (has_secrets) {\n    assert(!cephx_secret.empty());\n    assert((lockbox_secret.empty() && dmcrypt_key.empty()) ||\n           (!lockbox_secret.empty() && !dmcrypt_key.empty()));\n\n    err = mon->authmon()->do_osd_new(cephx_entity,\n        lockbox_entity,\n        has_lockbox);\n    assert(0 == err);\n\n    if (has_lockbox) {\n      assert(nullptr != svc);\n      svc->do_osd_new(uuid, dmcrypt_key);\n    }\n  }\n\n  if (is_recreate_destroyed) {\n    assert(id >= 0);\n    assert(osdmap.is_destroyed(id));\n    pending_inc.new_weight[id] = CEPH_OSD_OUT;\n    pending_inc.new_state[id] |= CEPH_OSD_DESTROYED;\n    if ((osdmap.get_state(id) & CEPH_OSD_NEW) == 0) {\n      pending_inc.new_state[id] |= CEPH_OSD_NEW;\n    }\n    if (osdmap.get_state(id) & CEPH_OSD_UP) {\n      // due to http://tracker.ceph.com/issues/20751 some clusters may\n      // have UP set for non-existent OSDs; make sure it is cleared\n      // for a newly created osd.\n      pending_inc.new_state[id] |= CEPH_OSD_UP;\n    }\n    pending_inc.new_uuid[id] = uuid;\n  } else {\n    assert(id >= 0);\n    int32_t new_id = -1;\n    do_osd_create(id, uuid, device_class, &new_id);\n    assert(new_id >= 0);\n    assert(id == new_id);\n  }\n\n  if (f) {\n    f->open_object_section(\"created_osd\");\n    f->dump_int(\"osdid\", id);\n    f->close_section();\n  } else {\n    ss << id;\n  }\n\n  return 0;\n}\n\nbool OSDMonitor::prepare_command(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MMonCommand *m = static_cast<MMonCommand*>(op->get_req());\n  stringstream ss;\n  cmdmap_t cmdmap;\n  if (!cmdmap_from_json(m->cmd, &cmdmap, ss)) {\n    string rs = ss.str();\n    mon->reply_command(op, -EINVAL, rs, get_last_committed());\n    return true;\n  }\n\n  MonSession *session = m->get_session();\n  if (!session) {\n    mon->reply_command(op, -EACCES, \"access denied\", get_last_committed());\n    return true;\n  }\n\n  return prepare_command_impl(op, cmdmap);\n}\n\nstatic int parse_reweights(CephContext *cct,\n\t\t\t   const cmdmap_t& cmdmap,\n\t\t\t   const OSDMap& osdmap,\n\t\t\t   map<int32_t, uint32_t>* weights)\n{\n  string weights_str;\n  if (!cmd_getval(cct, cmdmap, \"weights\", weights_str)) {\n    return -EINVAL;\n  }\n  std::replace(begin(weights_str), end(weights_str), '\\'', '\"');\n  json_spirit::mValue json_value;\n  if (!json_spirit::read(weights_str, json_value)) {\n    return -EINVAL;\n  }\n  if (json_value.type() != json_spirit::obj_type) {\n    return -EINVAL;\n  }\n  const auto obj = json_value.get_obj();\n  try {\n    for (auto& osd_weight : obj) {\n      auto osd_id = std::stoi(osd_weight.first);\n      if (!osdmap.exists(osd_id)) {\n\treturn -ENOENT;\n      }\n      if (osd_weight.second.type() != json_spirit::str_type) {\n\treturn -EINVAL;\n      }\n      auto weight = std::stoul(osd_weight.second.get_str());\n      weights->insert({osd_id, weight});\n    }\n  } catch (const std::logic_error& e) {\n    return -EINVAL;\n  }\n  return 0;\n}\n\nint OSDMonitor::prepare_command_osd_destroy(\n    int32_t id,\n    stringstream& ss)\n{\n  assert(paxos->is_plugged());\n\n  // we check if the osd exists for the benefit of `osd purge`, which may\n  // have previously removed the osd. If the osd does not exist, return\n  // -ENOENT to convey this, and let the caller deal with it.\n  //\n  // we presume that all auth secrets and config keys were removed prior\n  // to this command being called. if they exist by now, we also assume\n  // they must have been created by some other command and do not pertain\n  // to this non-existent osd.\n  if (!osdmap.exists(id)) {\n    dout(10) << __func__ << \" osd.\" << id << \" does not exist.\" << dendl;\n    return -ENOENT;\n  }\n\n  uuid_d uuid = osdmap.get_uuid(id);\n  dout(10) << __func__ << \" destroying osd.\" << id\n           << \" uuid \" << uuid << dendl;\n\n  // if it has been destroyed, we assume our work here is done.\n  if (osdmap.is_destroyed(id)) {\n    ss << \"destroyed osd.\" << id;\n    return 0;\n  }\n\n  EntityName cephx_entity, lockbox_entity;\n  bool idempotent_auth = false, idempotent_cks = false;\n\n  int err = mon->authmon()->validate_osd_destroy(id, uuid,\n                                                 cephx_entity,\n                                                 lockbox_entity,\n                                                 ss);\n  if (err < 0) {\n    if (err == -ENOENT) {\n      idempotent_auth = true;\n    } else {\n      return err;\n    }\n  }\n\n  ConfigKeyService *svc = (ConfigKeyService*)mon->config_key_service;\n  err = svc->validate_osd_destroy(id, uuid);\n  if (err < 0) {\n    assert(err == -ENOENT);\n    err = 0;\n    idempotent_cks = true;\n  }\n\n  if (!idempotent_auth) {\n    err = mon->authmon()->do_osd_destroy(cephx_entity, lockbox_entity);\n    assert(0 == err);\n  }\n\n  if (!idempotent_cks) {\n    svc->do_osd_destroy(id, uuid);\n  }\n\n  pending_inc.new_state[id] = CEPH_OSD_DESTROYED;\n  pending_inc.new_uuid[id] = uuid_d();\n\n  // we can only propose_pending() once per service, otherwise we'll be\n  // defying PaxosService and all laws of nature. Therefore, as we may\n  // be used during 'osd purge', let's keep the caller responsible for\n  // proposing.\n  assert(err == 0);\n  return 0;\n}\n\nint OSDMonitor::prepare_command_osd_purge(\n    int32_t id,\n    stringstream& ss)\n{\n  assert(paxos->is_plugged());\n  dout(10) << __func__ << \" purging osd.\" << id << dendl;\n\n  assert(!osdmap.is_up(id));\n\n  /*\n   * This may look a bit weird, but this is what's going to happen:\n   *\n   *  1. we make sure that removing from crush works\n   *  2. we call `prepare_command_osd_destroy()`. If it returns an\n   *     error, then we abort the whole operation, as no updates\n   *     have been made. However, we this function will have\n   *     side-effects, thus we need to make sure that all operations\n   *     performed henceforth will *always* succeed.\n   *  3. we call `prepare_command_osd_remove()`. Although this\n   *     function can return an error, it currently only checks if the\n   *     osd is up - and we have made sure that it is not so, so there\n   *     is no conflict, and it is effectively an update.\n   *  4. finally, we call `do_osd_crush_remove()`, which will perform\n   *     the crush update we delayed from before.\n   */\n\n  CrushWrapper newcrush;\n  _get_pending_crush(newcrush);\n\n  bool may_be_idempotent = false;\n\n  int err = _prepare_command_osd_crush_remove(newcrush, id, 0, false, false);\n  if (err == -ENOENT) {\n    err = 0;\n    may_be_idempotent = true;\n  } else if (err < 0) {\n    ss << \"error removing osd.\" << id << \" from crush\";\n    return err;\n  }\n\n  // no point destroying the osd again if it has already been marked destroyed\n  if (!osdmap.is_destroyed(id)) {\n    err = prepare_command_osd_destroy(id, ss);\n    if (err < 0) {\n      if (err == -ENOENT) {\n        err = 0;\n      } else {\n        return err;\n      }\n    } else {\n      may_be_idempotent = false;\n    }\n  }\n  assert(0 == err);\n\n  if (may_be_idempotent && !osdmap.exists(id)) {\n    dout(10) << __func__ << \" osd.\" << id << \" does not exist and \"\n             << \"we are idempotent.\" << dendl;\n    return -ENOENT;\n  }\n\n  err = prepare_command_osd_remove(id);\n  // we should not be busy, as we should have made sure this id is not up.\n  assert(0 == err);\n\n  do_osd_crush_remove(newcrush);\n  return 0;\n}\n\nbool OSDMonitor::prepare_command_impl(MonOpRequestRef op,\n\t\t\t\t      const cmdmap_t& cmdmap)\n{\n  op->mark_osdmon_event(__func__);\n  MMonCommand *m = static_cast<MMonCommand*>(op->get_req());\n  bool ret = false;\n  stringstream ss;\n  string rs;\n  bufferlist rdata;\n  int err = 0;\n\n  string format;\n  cmd_getval(cct, cmdmap, \"format\", format, string(\"plain\"));\n  boost::scoped_ptr<Formatter> f(Formatter::create(format));\n\n  string prefix;\n  cmd_getval(cct, cmdmap, \"prefix\", prefix);\n\n  int64_t osdid;\n  string osd_name;\n  bool osdid_present = false;\n  if (prefix != \"osd pg-temp\" &&\n      prefix != \"osd pg-upmap\" &&\n      prefix != \"osd pg-upmap-items\") {  // avoid commands with non-int id arg\n    osdid_present = cmd_getval(cct, cmdmap, \"id\", osdid);\n  }\n  if (osdid_present) {\n    ostringstream oss;\n    oss << \"osd.\" << osdid;\n    osd_name = oss.str();\n  }\n\n  // Even if there's a pending state with changes that could affect\n  // a command, considering that said state isn't yet committed, we\n  // just don't care about those changes if the command currently being\n  // handled acts as a no-op against the current committed state.\n  // In a nutshell, we assume this command  happens *before*.\n  //\n  // Let me make this clearer:\n  //\n  //   - If we have only one client, and that client issues some\n  //     operation that would conflict with this operation  but is\n  //     still on the pending state, then we would be sure that said\n  //     operation wouldn't have returned yet, so the client wouldn't\n  //     issue this operation (unless the client didn't wait for the\n  //     operation to finish, and that would be the client's own fault).\n  //\n  //   - If we have more than one client, each client will observe\n  //     whatever is the state at the moment of the commit.  So, if we\n  //     have two clients, one issuing an unlink and another issuing a\n  //     link, and if the link happens while the unlink is still on the\n  //     pending state, from the link's point-of-view this is a no-op.\n  //     If different clients are issuing conflicting operations and\n  //     they care about that, then the clients should make sure they\n  //     enforce some kind of concurrency mechanism -- from our\n  //     perspective that's what Douglas Adams would call an SEP.\n  //\n  // This should be used as a general guideline for most commands handled\n  // in this function.  Adapt as you see fit, but please bear in mind that\n  // this is the expected behavior.\n   \n \n  if (prefix == \"osd setcrushmap\" ||\n      (prefix == \"osd crush set\" && !osdid_present)) {\n    if (pending_inc.crush.length()) {\n      dout(10) << __func__ << \" waiting for pending crush update \" << dendl;\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    dout(10) << \"prepare_command setting new crush map\" << dendl;\n    bufferlist data(m->get_data());\n    CrushWrapper crush;\n    try {\n      auto bl = data.cbegin();\n      crush.decode(bl);\n    }\n    catch (const std::exception &e) {\n      err = -EINVAL;\n      ss << \"Failed to parse crushmap: \" << e.what();\n      goto reply;\n    }\n  \n    int64_t prior_version = 0;\n    if (cmd_getval(cct, cmdmap, \"prior_version\", prior_version)) {\n      if (prior_version == osdmap.get_crush_version() - 1) {\n\t// see if we are a resend of the last update.  this is imperfect\n\t// (multiple racing updaters may not both get reliable success)\n\t// but we expect crush updaters (via this interface) to be rare-ish.\n\tbufferlist current, proposed;\n\tosdmap.crush->encode(current, mon->get_quorum_con_features());\n\tcrush.encode(proposed, mon->get_quorum_con_features());\n\tif (current.contents_equal(proposed)) {\n\t  dout(10) << __func__\n\t\t   << \" proposed matches current and version equals previous\"\n\t\t   << dendl;\n\t  err = 0;\n\t  ss << osdmap.get_crush_version();\n\t  goto reply;\n\t}\n      }\n      if (prior_version != osdmap.get_crush_version()) {\n\terr = -EPERM;\n\tss << \"prior_version \" << prior_version << \" != crush version \"\n\t   << osdmap.get_crush_version();\n\tgoto reply;\n      }\n    }\n\n    if (crush.has_legacy_rule_ids()) {\n      err = -EINVAL;\n      ss << \"crush maps with ruleset != ruleid are no longer allowed\";\n      goto reply;\n    }\n    if (!validate_crush_against_features(&crush, ss)) {\n      err = -EINVAL;\n      goto reply;\n    }\n\n    err = osdmap.validate_crush_rules(&crush, &ss);\n    if (err < 0) {\n      goto reply;\n    }\n\n    if (g_conf->mon_osd_crush_smoke_test) {\n      // sanity check: test some inputs to make sure this map isn't\n      // totally broken\n      dout(10) << \" testing map\" << dendl;\n      stringstream ess;\n      CrushTester tester(crush, ess);\n      tester.set_min_x(0);\n      tester.set_max_x(50);\n      auto start = ceph::coarse_mono_clock::now();\n      int r = tester.test_with_fork(g_conf->mon_lease);\n      auto duration = ceph::coarse_mono_clock::now() - start;\n      if (r < 0) {\n\tdout(10) << \" tester.test_with_fork returns \" << r\n\t\t << \": \" << ess.str() << dendl;\n\tss << \"crush smoke test failed with \" << r << \": \" << ess.str();\n\terr = r;\n\tgoto reply;\n      }\n      dout(10) << __func__ << \" crush somke test duration: \"\n               << duration << \", result: \" << ess.str() << dendl;\n    }\n\n    pending_inc.crush = data;\n    ss << osdmap.get_crush_version() + 1;\n    goto update;\n\n  } else if (prefix == \"osd crush set-all-straw-buckets-to-straw2\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    for (int b = 0; b < newcrush.get_max_buckets(); ++b) {\n      int bid = -1 - b;\n      if (newcrush.bucket_exists(bid) &&\n\t  newcrush.get_bucket_alg(bid) == CRUSH_BUCKET_STRAW) {\n\tdout(20) << \" bucket \" << bid << \" is straw, can convert\" << dendl;\n\tnewcrush.bucket_set_alg(bid, CRUSH_BUCKET_STRAW2);\n      }\n    }\n    if (!validate_crush_against_features(&newcrush, ss)) {\n      err = -EINVAL;\n      goto reply;\n    }\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush set-device-class\") {\n    string device_class;\n    if (!cmd_getval(cct, cmdmap, \"class\", device_class)) {\n      err = -EINVAL; // no value!\n      goto reply;\n    }\n\n    bool stop = false;\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    set<int> updated;\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n      set<int> osds;\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n        osdmap.get_all_osds(osds);\n        stop = true;\n      } else {\n        // try traditional single osd way\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          // ss has reason for failure\n          ss << \", unable to parse osd id:\\\"\" << idvec[j] << \"\\\". \";\n          err = -EINVAL;\n          continue;\n        }\n        osds.insert(osd);\n      }\n\n      for (auto &osd : osds) {\n        if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        ostringstream oss;\n        oss << \"osd.\" << osd;\n        string name = oss.str();\n\n\tif (newcrush.get_max_devices() < osd + 1) {\n\t  newcrush.set_max_devices(osd + 1);\n\t}\n        string action;\n        if (newcrush.item_exists(osd)) {\n          action = \"updating\";\n        } else {\n          action = \"creating\";\n          newcrush.set_item_name(osd, name);\n        }\n\n        dout(5) << action << \" crush item id \" << osd << \" name '\" << name\n                << \"' device_class '\" << device_class << \"'\"\n                << dendl;\n        err = newcrush.update_device_class(osd, device_class, name, &ss);\n        if (err < 0) {\n          goto reply;\n        }\n        if (err == 0 && !_have_pending_crush()) {\n          if (!stop) {\n            // for single osd only, wildcard makes too much noise\n            ss << \"set-device-class item id \" << osd << \" name '\" << name\n               << \"' device_class '\" << device_class << \"': no change. \";\n          }\n        } else {\n          updated.insert(osd);\n        }\n      }\n    }\n\n    if (!updated.empty()) {\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n      ss << \"set osd(s) \" << updated << \" to class '\" << device_class << \"'\";\n      getline(ss, rs);\n      wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon,op, 0, rs, get_last_committed() + 1));\n      return true;\n    }\n\n } else if (prefix == \"osd crush rm-device-class\") {\n    bool stop = false;\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    set<int> updated;\n\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n      set<int> osds;\n\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n        osdmap.get_all_osds(osds);\n        stop = true;\n      } else {\n        // try traditional single osd way\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          // ss has reason for failure\n          ss << \", unable to parse osd id:\\\"\" << idvec[j] << \"\\\". \";\n          err = -EINVAL;\n          goto reply;\n        }\n        osds.insert(osd);\n      }\n\n      for (auto &osd : osds) {\n        if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        auto class_name = newcrush.get_item_class(osd);\n        if (!class_name) {\n          ss << \"osd.\" << osd << \" belongs to no class, \";\n          continue;\n        }\n        // note that we do not verify if class_is_in_use here\n        // in case the device is misclassified and user wants\n        // to overridely reset...\n\n        err = newcrush.remove_device_class(cct, osd, &ss);\n        if (err < 0) {\n          // ss has reason for failure\n          goto reply;\n        }\n        updated.insert(osd);\n      }\n    }\n\n    if (!updated.empty()) {\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n      ss << \"done removing class of osd(s): \" << updated;\n      getline(ss, rs);\n      wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon,op, 0, rs, get_last_committed() + 1));\n      return true;\n    }\n  } else if (prefix == \"osd crush class rename\") {\n    string srcname, dstname;\n    if (!cmd_getval(cct, cmdmap, \"srcname\", srcname)) {\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!cmd_getval(cct, cmdmap, \"dstname\", dstname)) {\n      err = -EINVAL;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    if (!newcrush.class_exists(srcname) && newcrush.class_exists(dstname)) {\n      // suppose this is a replay and return success\n      // so command is idempotent\n      ss << \"already renamed to '\" << dstname << \"'\";\n      err = 0;\n      goto reply;\n    }\n\n    err = newcrush.rename_class(srcname, dstname);\n    if (err < 0) {\n      ss << \"fail to rename '\" << srcname << \"' to '\" << dstname << \"' : \"\n         << cpp_strerror(err);\n      goto reply;\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"rename class '\" << srcname << \"' to '\" << dstname << \"'\";\n    goto update;\n  } else if (prefix == \"osd crush add-bucket\") {\n    // os crush add-bucket <name> <type>\n    string name, typestr;\n    vector<string> argvec;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    cmd_getval(cct, cmdmap, \"type\", typestr);\n    cmd_getval(cct, cmdmap, \"args\", argvec);\n    map<string,string> loc;\n    if (!argvec.empty()) {\n      CrushWrapper::parse_loc_map(argvec, &loc);\n      dout(0) << \"will create and move bucket '\" << name\n              << \"' to location \" << loc << dendl;\n    }\n\n    if (!_have_pending_crush() &&\n\t_get_stable_crush().name_exists(name)) {\n      ss << \"bucket '\" << name << \"' already exists\";\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (newcrush.name_exists(name)) {\n      ss << \"bucket '\" << name << \"' already exists\";\n      goto update;\n    }\n    int type = newcrush.get_type_id(typestr);\n    if (type < 0) {\n      ss << \"type '\" << typestr << \"' does not exist\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (type == 0) {\n      ss << \"type '\" << typestr << \"' is for devices, not buckets\";\n      err = -EINVAL;\n      goto reply;\n    }\n    int bucketno;\n    err = newcrush.add_bucket(0, 0,\n\t\t\t      CRUSH_HASH_DEFAULT, type, 0, NULL,\n\t\t\t      NULL, &bucketno);\n    if (err < 0) {\n      ss << \"add_bucket error: '\" << cpp_strerror(err) << \"'\";\n      goto reply;\n    }\n    err = newcrush.set_item_name(bucketno, name);\n    if (err < 0) {\n      ss << \"error setting bucket name to '\" << name << \"'\";\n      goto reply;\n    }\n\n    if (!loc.empty()) {\n      if (!newcrush.check_item_loc(cct, bucketno, loc,\n          (int *)NULL)) {\n        err = newcrush.move_bucket(cct, bucketno, loc);\n        if (err < 0) {\n          ss << \"error moving bucket '\" << name << \"' to location \" << loc;\n          goto reply;\n        }\n      } else {\n        ss << \"no need to move item id \" << bucketno << \" name '\" << name\n           << \"' to location \" << loc << \" in crush map\";\n      }\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    if (loc.empty()) {\n      ss << \"added bucket \" << name << \" type \" << typestr\n         << \" to crush map\";\n    } else {\n      ss << \"added bucket \" << name << \" type \" << typestr\n         << \" to location \" << loc;\n    }\n    goto update;\n  } else if (prefix == \"osd crush rename-bucket\") {\n    string srcname, dstname;\n    cmd_getval(cct, cmdmap, \"srcname\", srcname);\n    cmd_getval(cct, cmdmap, \"dstname\", dstname);\n\n    err = crush_rename_bucket(srcname, dstname, &ss);\n    if (err == -EALREADY) // equivalent to success for idempotency\n      err = 0;\n    if (err)\n      goto reply;\n    else\n      goto update;\n  } else if (prefix == \"osd crush weight-set create\" ||\n\t     prefix == \"osd crush weight-set create-compat\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    int64_t pool;\n    int positions;\n    if (newcrush.has_non_straw2_buckets()) {\n      ss << \"crush map contains one or more bucket(s) that are not straw2\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (prefix == \"osd crush weight-set create\") {\n      if (osdmap.require_min_compat_client > 0 &&\n\t  osdmap.require_min_compat_client < CEPH_RELEASE_LUMINOUS) {\n\tss << \"require_min_compat_client \"\n\t   << ceph_release_name(osdmap.require_min_compat_client)\n\t   << \" < luminous, which is required for per-pool weight-sets. \"\n           << \"Try 'ceph osd set-require-min-compat-client luminous' \"\n           << \"before using the new interface\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n      string poolname, mode;\n      cmd_getval(cct, cmdmap, \"pool\", poolname);\n      pool = osdmap.lookup_pg_pool_name(poolname.c_str());\n      if (pool < 0) {\n\tss << \"pool '\" << poolname << \"' not found\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n      cmd_getval(cct, cmdmap, \"mode\", mode);\n      if (mode != \"flat\" && mode != \"positional\") {\n\tss << \"unrecognized weight-set mode '\" << mode << \"'\";\n\terr = -EINVAL;\n\tgoto reply;\n      }\n      positions = mode == \"flat\" ? 1 : osdmap.get_pg_pool(pool)->get_size();\n    } else {\n      pool = CrushWrapper::DEFAULT_CHOOSE_ARGS;\n      positions = 1;\n    }\n    if (!newcrush.create_choose_args(pool, positions)) {\n      if (pool == CrushWrapper::DEFAULT_CHOOSE_ARGS) {\n        ss << \"compat weight-set already created\";\n      } else {\n        ss << \"weight-set for pool '\" << osdmap.get_pool_name(pool)\n           << \"' already created\";\n      }\n      goto reply;\n    }\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    goto update;\n\n  } else if (prefix == \"osd crush weight-set rm\" ||\n\t     prefix == \"osd crush weight-set rm-compat\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    int64_t pool;\n    if (prefix == \"osd crush weight-set rm\") {\n      string poolname;\n      cmd_getval(cct, cmdmap, \"pool\", poolname);\n      pool = osdmap.lookup_pg_pool_name(poolname.c_str());\n      if (pool < 0) {\n\tss << \"pool '\" << poolname << \"' not found\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n    } else {\n      pool = CrushWrapper::DEFAULT_CHOOSE_ARGS;\n    }\n    newcrush.rm_choose_args(pool);\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    goto update;\n\n  } else if (prefix == \"osd crush weight-set reweight\" ||\n\t     prefix == \"osd crush weight-set reweight-compat\") {\n    string poolname, item;\n    vector<double> weight;\n    cmd_getval(cct, cmdmap, \"pool\", poolname);\n    cmd_getval(cct, cmdmap, \"item\", item);\n    cmd_getval(cct, cmdmap, \"weight\", weight);\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    int64_t pool;\n    if (prefix == \"osd crush weight-set reweight\") {\n      pool = osdmap.lookup_pg_pool_name(poolname.c_str());\n      if (pool < 0) {\n\tss << \"pool '\" << poolname << \"' not found\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n      if (!newcrush.have_choose_args(pool)) {\n\tss << \"no weight-set for pool '\" << poolname << \"'\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n      auto arg_map = newcrush.choose_args_get(pool);\n      int positions = newcrush.get_choose_args_positions(arg_map);\n      if (weight.size() != (size_t)positions) {\n         ss << \"must specify exact \" << positions << \" weight values\";\n         err = -EINVAL;\n         goto reply;\n      }\n    } else {\n      pool = CrushWrapper::DEFAULT_CHOOSE_ARGS;\n      if (!newcrush.have_choose_args(pool)) {\n\tss << \"no backward-compatible weight-set\";\n\terr = -ENOENT;\n\tgoto reply;\n      }\n    }\n    if (!newcrush.name_exists(item)) {\n      ss << \"item '\" << item << \"' does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    err = newcrush.choose_args_adjust_item_weightf(\n      cct,\n      newcrush.choose_args_get(pool),\n      newcrush.get_item_id(item),\n      weight,\n      &ss);\n    if (err < 0) {\n      goto reply;\n    }\n    err = 0;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    goto update;\n  } else if (osdid_present &&\n\t     (prefix == \"osd crush set\" || prefix == \"osd crush add\")) {\n    // <OsdName> is 'osd.<id>' or '<id>', passed as int64_t id\n    // osd crush set <OsdName> <weight> <loc1> [<loc2> ...]\n    // osd crush add <OsdName> <weight> <loc1> [<loc2> ...]\n\n    if (!osdmap.exists(osdid)) {\n      err = -ENOENT;\n      ss << osd_name\n\t << \" does not exist. Create it before updating the crush map\";\n      goto reply;\n    }\n\n    double weight;\n    if (!cmd_getval(cct, cmdmap, \"weight\", weight)) {\n      ss << \"unable to parse weight value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    string args;\n    vector<string> argvec;\n    cmd_getval(cct, cmdmap, \"args\", argvec);\n    map<string,string> loc;\n    CrushWrapper::parse_loc_map(argvec, &loc);\n\n    if (prefix == \"osd crush set\"\n        && !_get_stable_crush().item_exists(osdid)) {\n      err = -ENOENT;\n      ss << \"unable to set item id \" << osdid << \" name '\" << osd_name\n         << \"' weight \" << weight << \" at location \" << loc\n         << \": does not exist\";\n      goto reply;\n    }\n\n    dout(5) << \"adding/updating crush item id \" << osdid << \" name '\"\n      << osd_name << \"' weight \" << weight << \" at location \"\n      << loc << dendl;\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    string action;\n    if (prefix == \"osd crush set\" ||\n        newcrush.check_item_loc(cct, osdid, loc, (int *)NULL)) {\n      action = \"set\";\n      err = newcrush.update_item(cct, osdid, weight, osd_name, loc);\n    } else {\n      action = \"add\";\n      err = newcrush.insert_item(cct, osdid, weight, osd_name, loc);\n      if (err == 0)\n        err = 1;\n    }\n\n    if (err < 0)\n      goto reply;\n\n    if (err == 0 && !_have_pending_crush()) {\n      ss << action << \" item id \" << osdid << \" name '\" << osd_name\n\t << \"' weight \" << weight << \" at location \" << loc << \": no change\";\n      goto reply;\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << action << \" item id \" << osdid << \" name '\" << osd_name << \"' weight \"\n       << weight << \" at location \" << loc << \" to crush map\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush create-or-move\") {\n    do {\n      // osd crush create-or-move <OsdName> <initial_weight> <loc1> [<loc2> ...]\n      if (!osdmap.exists(osdid)) {\n\terr = -ENOENT;\n\tss << osd_name\n\t   << \" does not exist.  create it before updating the crush map\";\n\tgoto reply;\n      }\n\n      double weight;\n      if (!cmd_getval(cct, cmdmap, \"weight\", weight)) {\n        ss << \"unable to parse weight value '\"\n           << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n        err = -EINVAL;\n        goto reply;\n      }\n\n      string args;\n      vector<string> argvec;\n      cmd_getval(cct, cmdmap, \"args\", argvec);\n      map<string,string> loc;\n      CrushWrapper::parse_loc_map(argvec, &loc);\n\n      dout(0) << \"create-or-move crush item name '\" << osd_name\n\t      << \"' initial_weight \" << weight << \" at location \" << loc\n\t      << dendl;\n\n      CrushWrapper newcrush;\n      _get_pending_crush(newcrush);\n\n      err = newcrush.create_or_move_item(cct, osdid, weight, osd_name, loc);\n      if (err == 0) {\n\tss << \"create-or-move updated item name '\" << osd_name\n\t   << \"' weight \" << weight\n\t   << \" at location \" << loc << \" to crush map\";\n\tbreak;\n      }\n      if (err > 0) {\n\tpending_inc.crush.clear();\n\tnewcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n\tss << \"create-or-move updating item name '\" << osd_name\n\t   << \"' weight \" << weight\n\t   << \" at location \" << loc << \" to crush map\";\n\tgetline(ss, rs);\n\twait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n\treturn true;\n      }\n    } while (false);\n\n  } else if (prefix == \"osd crush move\") {\n    do {\n      // osd crush move <name> <loc1> [<loc2> ...]\n      string name;\n      vector<string> argvec;\n      cmd_getval(cct, cmdmap, \"name\", name);\n      cmd_getval(cct, cmdmap, \"args\", argvec);\n      map<string,string> loc;\n      CrushWrapper::parse_loc_map(argvec, &loc);\n\n      dout(0) << \"moving crush item name '\" << name << \"' to location \" << loc << dendl;\n      CrushWrapper newcrush;\n      _get_pending_crush(newcrush);\n\n      if (!newcrush.name_exists(name)) {\n\terr = -ENOENT;\n\tss << \"item \" << name << \" does not exist\";\n\tbreak;\n      }\n      int id = newcrush.get_item_id(name);\n\n      if (!newcrush.check_item_loc(cct, id, loc, (int *)NULL)) {\n\tif (id >= 0) {\n\t  err = newcrush.create_or_move_item(cct, id, 0, name, loc);\n\t} else {\n\t  err = newcrush.move_bucket(cct, id, loc);\n\t}\n\tif (err >= 0) {\n\t  ss << \"moved item id \" << id << \" name '\" << name << \"' to location \" << loc << \" in crush map\";\n\t  pending_inc.crush.clear();\n\t  newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n\t  getline(ss, rs);\n\t  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t   get_last_committed() + 1));\n\t  return true;\n\t}\n      } else {\n\tss << \"no need to move item id \" << id << \" name '\" << name << \"' to location \" << loc << \" in crush map\";\n\terr = 0;\n      }\n    } while (false);\n  } else if (prefix == \"osd crush swap-bucket\") {\n    string source, dest, force;\n    cmd_getval(cct, cmdmap, \"source\", source);\n    cmd_getval(cct, cmdmap, \"dest\", dest);\n    cmd_getval(cct, cmdmap, \"force\", force);\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    if (!newcrush.name_exists(source)) {\n      ss << \"source item \" << source << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    if (!newcrush.name_exists(dest)) {\n      ss << \"dest item \" << dest << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    int sid = newcrush.get_item_id(source);\n    int did = newcrush.get_item_id(dest);\n    int sparent;\n    if (newcrush.get_immediate_parent_id(sid, &sparent) == 0 &&\n\tforce != \"--yes-i-really-mean-it\") {\n      ss << \"source item \" << source << \" is not an orphan bucket; pass --yes-i-really-mean-it to proceed anyway\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (newcrush.get_bucket_alg(sid) != newcrush.get_bucket_alg(did) &&\n\tforce != \"--yes-i-really-mean-it\") {\n      ss << \"source bucket alg \" << crush_alg_name(newcrush.get_bucket_alg(sid)) << \" != \"\n\t << \"dest bucket alg \" << crush_alg_name(newcrush.get_bucket_alg(did))\n\t << \"; pass --yes-i-really-mean-it to proceed anyway\";\n      err = -EPERM;\n      goto reply;\n    }\n    int r = newcrush.swap_bucket(cct, sid, did);\n    if (r < 0) {\n      ss << \"failed to swap bucket contents: \" << cpp_strerror(r);\n      err = r;\n      goto reply;\n    }\n    ss << \"swapped bucket of \" << source << \" to \" << dest;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    wait_for_finished_proposal(op,\n\t\t\t       new Monitor::C_Command(mon, op, err, ss.str(),\n\t\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush link\") {\n    // osd crush link <name> <loc1> [<loc2> ...]\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    vector<string> argvec;\n    cmd_getval(cct, cmdmap, \"args\", argvec);\n    map<string,string> loc;\n    CrushWrapper::parse_loc_map(argvec, &loc);\n\n    // Need an explicit check for name_exists because get_item_id returns\n    // 0 on unfound.\n    int id = osdmap.crush->get_item_id(name);\n    if (!osdmap.crush->name_exists(name)) {\n      err = -ENOENT;\n      ss << \"item \" << name << \" does not exist\";\n      goto reply;\n    } else {\n      dout(5) << \"resolved crush name '\" << name << \"' to id \" << id << dendl;\n    }\n    if (osdmap.crush->check_item_loc(cct, id, loc, (int*) NULL)) {\n      ss << \"no need to move item id \" << id << \" name '\" << name\n\t << \"' to location \" << loc << \" in crush map\";\n      err = 0;\n      goto reply;\n    }\n\n    dout(5) << \"linking crush item name '\" << name << \"' at location \" << loc << dendl;\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (!newcrush.name_exists(name)) {\n      err = -ENOENT;\n      ss << \"item \" << name << \" does not exist\";\n      goto reply;\n    } else {\n      int id = newcrush.get_item_id(name);\n      if (!newcrush.check_item_loc(cct, id, loc, (int *)NULL)) {\n\terr = newcrush.link_bucket(cct, id, loc);\n\tif (err >= 0) {\n\t  ss << \"linked item id \" << id << \" name '\" << name\n             << \"' to location \" << loc << \" in crush map\";\n\t  pending_inc.crush.clear();\n\t  newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n\t} else {\n\t  ss << \"cannot link item id \" << id << \" name '\" << name\n             << \"' to location \" << loc;\n          goto reply;\n\t}\n      } else {\n\tss << \"no need to move item id \" << id << \" name '\" << name\n           << \"' to location \" << loc << \" in crush map\";\n\terr = 0;\n      }\n    }\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, err, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush rm\" ||\n\t     prefix == \"osd crush remove\" ||\n\t     prefix == \"osd crush unlink\") {\n    do {\n      // osd crush rm <id> [ancestor]\n      CrushWrapper newcrush;\n      _get_pending_crush(newcrush);\n\n      string name;\n      cmd_getval(cct, cmdmap, \"name\", name);\n\n      if (!osdmap.crush->name_exists(name)) {\n\terr = 0;\n\tss << \"device '\" << name << \"' does not appear in the crush map\";\n\tbreak;\n      }\n      if (!newcrush.name_exists(name)) {\n\terr = 0;\n\tss << \"device '\" << name << \"' does not appear in the crush map\";\n\tgetline(ss, rs);\n\twait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n\treturn true;\n      }\n      int id = newcrush.get_item_id(name);\n      int ancestor = 0;\n\n      bool unlink_only = prefix == \"osd crush unlink\";\n      string ancestor_str;\n      if (cmd_getval(cct, cmdmap, \"ancestor\", ancestor_str)) {\n\tif (!newcrush.name_exists(ancestor_str)) {\n\t  err = -ENOENT;\n\t  ss << \"ancestor item '\" << ancestor_str\n\t     << \"' does not appear in the crush map\";\n\t  break;\n\t}\n        ancestor = newcrush.get_item_id(ancestor_str);\n      }\n\n      err = prepare_command_osd_crush_remove(\n          newcrush,\n          id, ancestor,\n          (ancestor < 0), unlink_only);\n\n      if (err == -ENOENT) {\n\tss << \"item \" << id << \" does not appear in that position\";\n\terr = 0;\n\tbreak;\n      }\n      if (err == 0) {\n\tss << \"removed item id \" << id << \" name '\" << name << \"' from crush map\";\n\tgetline(ss, rs);\n\twait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n\treturn true;\n      }\n    } while (false);\n\n  } else if (prefix == \"osd crush reweight-all\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    newcrush.reweight(cct);\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"reweighted crush hierarchy\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush reweight\") {\n    // osd crush reweight <name> <weight>\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    if (!newcrush.name_exists(name)) {\n      err = -ENOENT;\n      ss << \"device '\" << name << \"' does not appear in the crush map\";\n      goto reply;\n    }\n\n    int id = newcrush.get_item_id(name);\n    if (id < 0) {\n      ss << \"device '\" << name << \"' is not a leaf in the crush map\";\n      err = -EINVAL;\n      goto reply;\n    }\n    double w;\n    if (!cmd_getval(cct, cmdmap, \"weight\", w)) {\n      ss << \"unable to parse weight value '\"\n\t << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    err = newcrush.adjust_item_weightf(cct, id, w);\n    if (err < 0)\n      goto reply;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"reweighted item id \" << id << \" name '\" << name << \"' to \" << w\n       << \" in crush map\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush reweight-subtree\") {\n    // osd crush reweight <name> <weight>\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    if (!newcrush.name_exists(name)) {\n      err = -ENOENT;\n      ss << \"device '\" << name << \"' does not appear in the crush map\";\n      goto reply;\n    }\n\n    int id = newcrush.get_item_id(name);\n    if (id >= 0) {\n      ss << \"device '\" << name << \"' is not a subtree in the crush map\";\n      err = -EINVAL;\n      goto reply;\n    }\n    double w;\n    if (!cmd_getval(cct, cmdmap, \"weight\", w)) {\n      ss << \"unable to parse weight value '\"\n\t << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    err = newcrush.adjust_subtree_weightf(cct, id, w);\n    if (err < 0)\n      goto reply;\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"reweighted subtree id \" << id << \" name '\" << name << \"' to \" << w\n       << \" in crush map\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush tunables\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    err = 0;\n    string profile;\n    cmd_getval(cct, cmdmap, \"profile\", profile);\n    if (profile == \"legacy\" || profile == \"argonaut\") {\n      newcrush.set_tunables_legacy();\n    } else if (profile == \"bobtail\") {\n      newcrush.set_tunables_bobtail();\n    } else if (profile == \"firefly\") {\n      newcrush.set_tunables_firefly();\n    } else if (profile == \"hammer\") {\n      newcrush.set_tunables_hammer();\n    } else if (profile == \"jewel\") {\n      newcrush.set_tunables_jewel();\n    } else if (profile == \"optimal\") {\n      newcrush.set_tunables_optimal();\n    } else if (profile == \"default\") {\n      newcrush.set_tunables_default();\n    } else {\n      ss << \"unrecognized profile '\" << profile << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    if (!validate_crush_against_features(&newcrush, ss)) {\n      err = -EINVAL;\n      goto reply;\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"adjusted tunables profile to \" << profile;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd crush set-tunable\") {\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    err = 0;\n    string tunable;\n    cmd_getval(cct, cmdmap, \"tunable\", tunable);\n\n    int64_t value = -1;\n    if (!cmd_getval(cct, cmdmap, \"value\", value)) {\n      err = -EINVAL;\n      ss << \"failed to parse integer value \"\n\t << cmd_vartype_stringify(cmdmap.at(\"value\"));\n      goto reply;\n    }\n\n    if (tunable == \"straw_calc_version\") {\n      if (value != 0 && value != 1) {\n\tss << \"value must be 0 or 1; got \" << value;\n\terr = -EINVAL;\n\tgoto reply;\n      }\n      newcrush.set_straw_calc_version(value);\n    } else {\n      ss << \"unrecognized tunable '\" << tunable << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    if (!validate_crush_against_features(&newcrush, ss)) {\n      err = -EINVAL;\n      goto reply;\n    }\n\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    ss << \"adjusted tunable \" << tunable << \" to \" << value;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule create-simple\") {\n    string name, root, type, mode;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    cmd_getval(cct, cmdmap, \"root\", root);\n    cmd_getval(cct, cmdmap, \"type\", type);\n    cmd_getval(cct, cmdmap, \"mode\", mode);\n    if (mode == \"\")\n      mode = \"firstn\";\n\n    if (osdmap.crush->rule_exists(name)) {\n      // The name is uniquely associated to a ruleid and the rule it contains\n      // From the user point of view, the rule is more meaningfull.\n      ss << \"rule \" << name << \" already exists\";\n      err = 0;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (newcrush.rule_exists(name)) {\n      // The name is uniquely associated to a ruleid and the rule it contains\n      // From the user point of view, the rule is more meaningfull.\n      ss << \"rule \" << name << \" already exists\";\n      err = 0;\n    } else {\n      int ruleno = newcrush.add_simple_rule(name, root, type, \"\", mode,\n\t\t\t\t\t       pg_pool_t::TYPE_REPLICATED, &ss);\n      if (ruleno < 0) {\n\terr = ruleno;\n\tgoto reply;\n      }\n\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule create-replicated\") {\n    string name, root, type, device_class;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    cmd_getval(cct, cmdmap, \"root\", root);\n    cmd_getval(cct, cmdmap, \"type\", type);\n    cmd_getval(cct, cmdmap, \"class\", device_class);\n\n    if (osdmap.crush->rule_exists(name)) {\n      // The name is uniquely associated to a ruleid and the rule it contains\n      // From the user point of view, the rule is more meaningfull.\n      ss << \"rule \" << name << \" already exists\";\n      err = 0;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (newcrush.rule_exists(name)) {\n      // The name is uniquely associated to a ruleid and the rule it contains\n      // From the user point of view, the rule is more meaningfull.\n      ss << \"rule \" << name << \" already exists\";\n      err = 0;\n    } else {\n      int ruleno = newcrush.add_simple_rule(\n\tname, root, type, device_class,\n\t\"firstn\", pg_pool_t::TYPE_REPLICATED, &ss);\n      if (ruleno < 0) {\n\terr = ruleno;\n\tgoto reply;\n      }\n\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd erasure-code-profile rm\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n\n    if (erasure_code_profile_in_use(pending_inc.new_pools, name, &ss))\n      goto wait;\n\n    if (erasure_code_profile_in_use(osdmap.pools, name, &ss)) {\n      err = -EBUSY;\n      goto reply;\n    }\n\n    if (osdmap.has_erasure_code_profile(name) ||\n\tpending_inc.new_erasure_code_profiles.count(name)) {\n      if (osdmap.has_erasure_code_profile(name)) {\n\tpending_inc.old_erasure_code_profiles.push_back(name);\n      } else {\n\tdout(20) << \"erasure code profile rm \" << name << \": creation canceled\" << dendl;\n\tpending_inc.new_erasure_code_profiles.erase(name);\n      }\n\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t\tget_last_committed() + 1));\n      return true;\n    } else {\n      ss << \"erasure-code-profile \" << name << \" does not exist\";\n      err = 0;\n      goto reply;\n    }\n\n  } else if (prefix == \"osd erasure-code-profile set\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    vector<string> profile;\n    cmd_getval(cct, cmdmap, \"profile\", profile);\n    bool force;\n    if (profile.size() > 0 && profile.back() == \"--force\") {\n      profile.pop_back();\n      force = true;\n    } else {\n      force = false;\n    }\n    map<string,string> profile_map;\n    err = parse_erasure_code_profile(profile, &profile_map, &ss);\n    if (err)\n      goto reply;\n    if (profile_map.find(\"plugin\") == profile_map.end()) {\n      ss << \"erasure-code-profile \" << profile_map\n\t << \" must contain a plugin entry\" << std::endl;\n      err = -EINVAL;\n      goto reply;\n    }\n    string plugin = profile_map[\"plugin\"];\n\n    if (pending_inc.has_erasure_code_profile(name)) {\n      dout(20) << \"erasure code profile \" << name << \" try again\" << dendl;\n      goto wait;\n    } else {\n      err = normalize_profile(name, profile_map, force, &ss);\n      if (err)\n\tgoto reply;\n\n      if (osdmap.has_erasure_code_profile(name)) {\n\tErasureCodeProfile existing_profile_map =\n\t  osdmap.get_erasure_code_profile(name);\n\terr = normalize_profile(name, existing_profile_map, force, &ss);\n\tif (err)\n\t  goto reply;\n\n\tif (existing_profile_map == profile_map) {\n\t  err = 0;\n\t  goto reply;\n\t}\n\tif (!force) {\n\t  err = -EPERM;\n\t  ss << \"will not override erasure code profile \" << name\n\t     << \" because the existing profile \"\n\t     << existing_profile_map\n\t     << \" is different from the proposed profile \"\n\t     << profile_map;\n\t  goto reply;\n\t}\n      }\n\n      dout(20) << \"erasure code profile set \" << name << \"=\"\n\t       << profile_map << dendl;\n      pending_inc.set_erasure_code_profile(name, profile_map);\n    }\n\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                                                      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule create-erasure\") {\n    err = check_cluster_features(CEPH_FEATURE_CRUSH_V2, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string name, poolstr;\n    cmd_getval(cct, cmdmap, \"name\", name);\n    string profile;\n    cmd_getval(cct, cmdmap, \"profile\", profile);\n    if (profile == \"\")\n      profile = \"default\";\n    if (profile == \"default\") {\n      if (!osdmap.has_erasure_code_profile(profile)) {\n\tif (pending_inc.has_erasure_code_profile(profile)) {\n\t  dout(20) << \"erasure code profile \" << profile << \" already pending\" << dendl;\n\t  goto wait;\n\t}\n\n\tmap<string,string> profile_map;\n\terr = osdmap.get_erasure_code_profile_default(cct,\n\t\t\t\t\t\t      profile_map,\n\t\t\t\t\t\t      &ss);\n\tif (err)\n\t  goto reply;\n\terr = normalize_profile(name, profile_map, true, &ss);\n\tif (err)\n\t  goto reply;\n\tdout(20) << \"erasure code profile set \" << profile << \"=\"\n\t\t << profile_map << dendl;\n\tpending_inc.set_erasure_code_profile(profile, profile_map);\n\tgoto wait;\n      }\n    }\n\n    int rule;\n    err = crush_rule_create_erasure(name, profile, &rule, &ss);\n    if (err < 0) {\n      switch(err) {\n      case -EEXIST: // return immediately\n\tss << \"rule \" << name << \" already exists\";\n\terr = 0;\n\tgoto reply;\n\tbreak;\n      case -EALREADY: // wait for pending to be proposed\n\tss << \"rule \" << name << \" already exists\";\n\terr = 0;\n\tbreak;\n      default: // non recoverable error\n \tgoto reply;\n\tbreak;\n      }\n    } else {\n      ss << \"created rule \" << name << \" at \" << rule;\n    }\n\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                                                      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule rm\") {\n    string name;\n    cmd_getval(cct, cmdmap, \"name\", name);\n\n    if (!osdmap.crush->rule_exists(name)) {\n      ss << \"rule \" << name << \" does not exist\";\n      err = 0;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n\n    if (!newcrush.rule_exists(name)) {\n      ss << \"rule \" << name << \" does not exist\";\n      err = 0;\n    } else {\n      int ruleno = newcrush.get_rule_id(name);\n      assert(ruleno >= 0);\n\n      // make sure it is not in use.\n      // FIXME: this is ok in some situations, but let's not bother with that\n      // complexity now.\n      int ruleset = newcrush.get_rule_mask_ruleset(ruleno);\n      if (osdmap.crush_rule_in_use(ruleset)) {\n\tss << \"crush ruleset \" << name << \" \" << ruleset << \" is in use\";\n\terr = -EBUSY;\n\tgoto reply;\n      }\n\n      err = newcrush.remove_rule(ruleno);\n      if (err < 0) {\n\tgoto reply;\n      }\n\n      pending_inc.crush.clear();\n      newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd crush rule rename\") {\n    string srcname;\n    string dstname;\n    cmd_getval(cct, cmdmap, \"srcname\", srcname);\n    cmd_getval(cct, cmdmap, \"dstname\", dstname);\n    if (srcname.empty() || dstname.empty()) {\n      ss << \"must specify both source rule name and destination rule name\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (srcname == dstname) {\n      ss << \"destination rule name is equal to source rule name\";\n      err = 0;\n      goto reply;\n    }\n\n    CrushWrapper newcrush;\n    _get_pending_crush(newcrush);\n    if (!newcrush.rule_exists(srcname) && newcrush.rule_exists(dstname)) {\n      // srcname does not exist and dstname already exists\n      // suppose this is a replay and return success\n      // (so this command is idempotent)\n      ss << \"already renamed to '\" << dstname << \"'\";\n      err = 0;\n      goto reply;\n    }\n\n    err = newcrush.rename_rule(srcname, dstname, &ss);\n    if (err < 0) {\n      // ss has reason for failure\n      goto reply;\n    }\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                               get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd setmaxosd\") {\n    int64_t newmax;\n    if (!cmd_getval(cct, cmdmap, \"newmax\", newmax)) {\n      ss << \"unable to parse 'newmax' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"newmax\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    if (newmax > g_conf->mon_max_osd) {\n      err = -ERANGE;\n      ss << \"cannot set max_osd to \" << newmax << \" which is > conf.mon_max_osd (\"\n\t << g_conf->mon_max_osd << \")\";\n      goto reply;\n    }\n\n    // Don't allow shrinking OSD number as this will cause data loss\n    // and may cause kernel crashes.\n    // Note: setmaxosd sets the maximum OSD number and not the number of OSDs\n    if (newmax < osdmap.get_max_osd()) {\n      // Check if the OSDs exist between current max and new value.\n      // If there are any OSDs exist, then don't allow shrinking number\n      // of OSDs.\n      for (int i = newmax; i < osdmap.get_max_osd(); i++) {\n        if (osdmap.exists(i)) {\n          err = -EBUSY;\n          ss << \"cannot shrink max_osd to \" << newmax\n             << \" because osd.\" << i << \" (and possibly others) still in use\";\n          goto reply;\n        }\n      }\n    }\n\n    pending_inc.new_max_osd = newmax;\n    ss << \"set new max_osd = \" << pending_inc.new_max_osd;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd set-full-ratio\" ||\n\t     prefix == \"osd set-backfillfull-ratio\" ||\n             prefix == \"osd set-nearfull-ratio\") {\n    double n;\n    if (!cmd_getval(cct, cmdmap, \"ratio\", n)) {\n      ss << \"unable to parse 'ratio' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"ratio\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (prefix == \"osd set-full-ratio\")\n      pending_inc.new_full_ratio = n;\n    else if (prefix == \"osd set-backfillfull-ratio\")\n      pending_inc.new_backfillfull_ratio = n;\n    else if (prefix == \"osd set-nearfull-ratio\")\n      pending_inc.new_nearfull_ratio = n;\n    ss << prefix << \" \" << n;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd set-require-min-compat-client\") {\n    string v;\n    cmd_getval(cct, cmdmap, \"version\", v);\n    int vno = ceph_release_from_name(v.c_str());\n    if (vno <= 0) {\n      ss << \"version \" << v << \" is not recognized\";\n      err = -EINVAL;\n      goto reply;\n    }\n    OSDMap newmap;\n    newmap.deepish_copy_from(osdmap);\n    newmap.apply_incremental(pending_inc);\n    newmap.require_min_compat_client = vno;\n    auto mvno = newmap.get_min_compat_client();\n    if (vno < mvno) {\n      ss << \"osdmap current utilizes features that require \"\n\t << ceph_release_name(mvno)\n\t << \"; cannot set require_min_compat_client below that to \"\n\t << ceph_release_name(vno);\n      err = -EPERM;\n      goto reply;\n    }\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    if (sure != \"--yes-i-really-mean-it\") {\n      FeatureMap m;\n      mon->get_combined_feature_map(&m);\n      uint64_t features = ceph_release_features(vno);\n      bool first = true;\n      bool ok = true;\n      for (int type : {\n\t    CEPH_ENTITY_TYPE_CLIENT,\n\t    CEPH_ENTITY_TYPE_MDS,\n\t    CEPH_ENTITY_TYPE_MGR }) {\n\tauto p = m.m.find(type);\n\tif (p == m.m.end()) {\n\t  continue;\n\t}\n\tfor (auto& q : p->second) {\n\t  uint64_t missing = ~q.first & features;\n\t  if (missing) {\n\t    if (first) {\n\t      ss << \"cannot set require_min_compat_client to \" << v << \": \";\n\t    } else {\n\t      ss << \"; \";\n\t    }\n\t    first = false;\n\t    ss << q.second << \" connected \" << ceph_entity_type_name(type)\n\t       << \"(s) look like \" << ceph_release_name(\n\t\t ceph_release_from_features(q.first))\n\t       << \" (missing 0x\" << std::hex << missing << std::dec << \")\";\n\t    ok = false;\n\t  }\n\t}\n      }\n      if (!ok) {\n\tss << \"; add --yes-i-really-mean-it to do it anyway\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n    }\n    ss << \"set require_min_compat_client to \" << ceph_release_name(vno);\n    pending_inc.new_require_min_compat_client = vno;\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t\t  get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pause\") {\n    return prepare_set_flag(op, CEPH_OSDMAP_PAUSERD | CEPH_OSDMAP_PAUSEWR);\n\n  } else if (prefix == \"osd unpause\") {\n    return prepare_unset_flag(op, CEPH_OSDMAP_PAUSERD | CEPH_OSDMAP_PAUSEWR);\n\n  } else if (prefix == \"osd set\") {\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n    if (key == \"full\")\n      return prepare_set_flag(op, CEPH_OSDMAP_FULL);\n    else if (key == \"pause\")\n      return prepare_set_flag(op, CEPH_OSDMAP_PAUSERD | CEPH_OSDMAP_PAUSEWR);\n    else if (key == \"noup\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOUP);\n    else if (key == \"nodown\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NODOWN);\n    else if (key == \"noout\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOOUT);\n    else if (key == \"noin\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOIN);\n    else if (key == \"nobackfill\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOBACKFILL);\n    else if (key == \"norebalance\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOREBALANCE);\n    else if (key == \"norecover\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NORECOVER);\n    else if (key == \"noscrub\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOSCRUB);\n    else if (key == \"nodeep-scrub\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NODEEP_SCRUB);\n    else if (key == \"notieragent\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOTIERAGENT);\n    else if (key == \"nosnaptrim\")\n      return prepare_set_flag(op, CEPH_OSDMAP_NOSNAPTRIM);\n    else if (key == \"sortbitwise\") {\n      return prepare_set_flag(op, CEPH_OSDMAP_SORTBITWISE);\n    } else if (key == \"recovery_deletes\") {\n      if (!osdmap.get_num_up_osds() && sure != \"--yes-i-really-mean-it\") {\n        ss << \"Not advisable to continue since no OSDs are up. Pass \"\n           << \"--yes-i-really-mean-it if you really wish to continue.\";\n        err = -EPERM;\n        goto reply;\n      }\n      if (HAVE_FEATURE(osdmap.get_up_osd_features(), OSD_RECOVERY_DELETES)\n          || sure == \"--yes-i-really-mean-it\") {\n\treturn prepare_set_flag(op, CEPH_OSDMAP_RECOVERY_DELETES);\n      } else {\n\tss << \"not all up OSDs have OSD_RECOVERY_DELETES feature\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n    } else if (key == \"require_jewel_osds\") {\n      if (!osdmap.get_num_up_osds() && sure != \"--yes-i-really-mean-it\") {\n        ss << \"Not advisable to continue since no OSDs are up. Pass \"\n           << \"--yes-i-really-mean-it if you really wish to continue.\";\n        err = -EPERM;\n        goto reply;\n      }\n      if (!osdmap.test_flag(CEPH_OSDMAP_SORTBITWISE)) {\n\tss << \"the sortbitwise flag must be set before require_jewel_osds\";\n\terr = -EPERM;\n\tgoto reply;\n      } else if (osdmap.require_osd_release >= CEPH_RELEASE_JEWEL) {\n\tss << \"require_osd_release is already >= jewel\";\n\terr = 0;\n\tgoto reply;\n      } else if (HAVE_FEATURE(osdmap.get_up_osd_features(), SERVER_JEWEL)\n                 || sure == \"--yes-i-really-mean-it\") {\n\treturn prepare_set_flag(op, CEPH_OSDMAP_REQUIRE_JEWEL);\n      } else {\n\tss << \"not all up OSDs have CEPH_FEATURE_SERVER_JEWEL feature\";\n\terr = -EPERM;\n      }\n    } else if (key == \"require_kraken_osds\") {\n      if (!osdmap.get_num_up_osds() && sure != \"--yes-i-really-mean-it\") {\n        ss << \"Not advisable to continue since no OSDs are up. Pass \"\n           << \"--yes-i-really-mean-it if you really wish to continue.\";\n        err = -EPERM;\n        goto reply;\n      }\n      if (!osdmap.test_flag(CEPH_OSDMAP_SORTBITWISE)) {\n\tss << \"the sortbitwise flag must be set before require_kraken_osds\";\n\terr = -EPERM;\n\tgoto reply;\n      } else if (osdmap.require_osd_release >= CEPH_RELEASE_KRAKEN) {\n\tss << \"require_osd_release is already >= kraken\";\n\terr = 0;\n\tgoto reply;\n      } else if (HAVE_FEATURE(osdmap.get_up_osd_features(), SERVER_KRAKEN)\n                 || sure == \"--yes-i-really-mean-it\") {\n\tbool r = prepare_set_flag(op, CEPH_OSDMAP_REQUIRE_KRAKEN);\n\t// ensure JEWEL is also set\n\tpending_inc.new_flags |= CEPH_OSDMAP_REQUIRE_JEWEL;\n\treturn r;\n      } else {\n\tss << \"not all up OSDs have CEPH_FEATURE_SERVER_KRAKEN feature\";\n\terr = -EPERM;\n      }\n    } else {\n      ss << \"unrecognized flag '\" << key << \"'\";\n      err = -EINVAL;\n    }\n\n  } else if (prefix == \"osd unset\") {\n    string key;\n    cmd_getval(cct, cmdmap, \"key\", key);\n    if (key == \"full\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_FULL);\n    else if (key == \"pause\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_PAUSERD | CEPH_OSDMAP_PAUSEWR);\n    else if (key == \"noup\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOUP);\n    else if (key == \"nodown\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NODOWN);\n    else if (key == \"noout\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOOUT);\n    else if (key == \"noin\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOIN);\n    else if (key == \"nobackfill\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOBACKFILL);\n    else if (key == \"norebalance\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOREBALANCE);\n    else if (key == \"norecover\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NORECOVER);\n    else if (key == \"noscrub\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOSCRUB);\n    else if (key == \"nodeep-scrub\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NODEEP_SCRUB);\n    else if (key == \"notieragent\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOTIERAGENT);\n    else if (key == \"nosnaptrim\")\n      return prepare_unset_flag(op, CEPH_OSDMAP_NOSNAPTRIM);\n    else {\n      ss << \"unrecognized flag '\" << key << \"'\";\n      err = -EINVAL;\n    }\n\n  } else if (prefix == \"osd require-osd-release\") {\n    string release;\n    cmd_getval(cct, cmdmap, \"release\", release);\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    if (!osdmap.test_flag(CEPH_OSDMAP_SORTBITWISE)) {\n      ss << \"the sortbitwise flag must be set first\";\n      err = -EPERM;\n      goto reply;\n    }\n    int rel = ceph_release_from_name(release.c_str());\n    if (rel <= 0) {\n      ss << \"unrecognized release \" << release;\n      err = -EINVAL;\n      goto reply;\n    }\n    if (rel < CEPH_RELEASE_LUMINOUS) {\n      ss << \"use this command only for luminous and later\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (rel == osdmap.require_osd_release) {\n      // idempotent\n      err = 0;\n      goto reply;\n    }\n    assert(osdmap.require_osd_release >= CEPH_RELEASE_LUMINOUS);\n    if (!osdmap.get_num_up_osds() && sure != \"--yes-i-really-mean-it\") {\n      ss << \"Not advisable to continue since no OSDs are up. Pass \"\n\t << \"--yes-i-really-mean-it if you really wish to continue.\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (rel == CEPH_RELEASE_MIMIC) {\n      if ((!HAVE_FEATURE(osdmap.get_up_osd_features(), SERVER_MIMIC))\n           && sure != \"--yes-i-really-mean-it\") {\n\tss << \"not all up OSDs have CEPH_FEATURE_SERVER_MIMIC feature\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n    } else if (rel == CEPH_RELEASE_NAUTILUS) {\n      if ((!HAVE_FEATURE(osdmap.get_up_osd_features(), SERVER_NAUTILUS))\n           && sure != \"--yes-i-really-mean-it\") {\n\tss << \"not all up OSDs have CEPH_FEATURE_SERVER_NAUTILUS feature\";\n\terr = -EPERM;\n\tgoto reply;\n      }\n    } else {\n      ss << \"not supported for this release yet\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (rel < osdmap.require_osd_release) {\n      ss << \"require_osd_release cannot be lowered once it has been set\";\n      err = -EPERM;\n      goto reply;\n    }\n    pending_inc.new_require_osd_release = rel;\n    goto update;\n  } else if (prefix == \"osd cluster_snap\") {\n    // ** DISABLE THIS FOR NOW **\n    ss << \"cluster snapshot currently disabled (broken implementation)\";\n    // ** DISABLE THIS FOR NOW **\n\n  } else if (prefix == \"osd down\" ||\n\t     prefix == \"osd out\" ||\n\t     prefix == \"osd in\" ||\n\t     prefix == \"osd rm\") {\n\n    bool any = false;\n    bool stop = false;\n    bool verbose = true;\n\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n      set<int> osds;\n\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n        if (prefix == \"osd in\") {\n          // touch out osds only\n          osdmap.get_out_osds(osds);\n        } else {\n          osdmap.get_all_osds(osds);\n        }\n        stop = true;\n        verbose = false; // so the output is less noisy.\n      } else {\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          ss << \"invalid osd id\" << osd;\n          err = -EINVAL;\n          continue;\n        } else if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        osds.insert(osd);\n      }\n\n      for (auto &osd : osds) {\n        if (prefix == \"osd down\") {\n\t  if (osdmap.is_down(osd)) {\n            if (verbose)\n\t      ss << \"osd.\" << osd << \" is already down. \";\n\t  } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_UP);\n\t    ss << \"marked down osd.\" << osd << \". \";\n\t    any = true;\n\t  }\n        } else if (prefix == \"osd out\") {\n\t  if (osdmap.is_out(osd)) {\n            if (verbose)\n\t      ss << \"osd.\" << osd << \" is already out. \";\n\t  } else {\n\t    pending_inc.new_weight[osd] = CEPH_OSD_OUT;\n\t    if (osdmap.osd_weight[osd]) {\n\t      if (pending_inc.new_xinfo.count(osd) == 0) {\n\t        pending_inc.new_xinfo[osd] = osdmap.osd_xinfo[osd];\n\t      }\n\t      pending_inc.new_xinfo[osd].old_weight = osdmap.osd_weight[osd];\n\t    }\n\t    ss << \"marked out osd.\" << osd << \". \";\n            std::ostringstream msg;\n            msg << \"Client \" << op->get_session()->entity_name\n                << \" marked osd.\" << osd << \" out\";\n            if (osdmap.is_up(osd)) {\n              msg << \", while it was still marked up\";\n            } else {\n              auto period = ceph_clock_now() - down_pending_out[osd];\n              msg << \", after it was down for \" << int(period.sec())\n                  << \" seconds\";\n            }\n\n            mon->clog->info() << msg.str();\n\t    any = true;\n\t  }\n        } else if (prefix == \"osd in\") {\n\t  if (osdmap.is_in(osd)) {\n            if (verbose)\n\t      ss << \"osd.\" << osd << \" is already in. \";\n\t  } else {\n\t    if (osdmap.osd_xinfo[osd].old_weight > 0) {\n\t      pending_inc.new_weight[osd] = osdmap.osd_xinfo[osd].old_weight;\n\t      if (pending_inc.new_xinfo.count(osd) == 0) {\n\t        pending_inc.new_xinfo[osd] = osdmap.osd_xinfo[osd];\n\t      }\n\t      pending_inc.new_xinfo[osd].old_weight = 0;\n\t    } else {\n\t      pending_inc.new_weight[osd] = CEPH_OSD_IN;\n\t    }\n\t    ss << \"marked in osd.\" << osd << \". \";\n\t    any = true;\n\t  }\n        } else if (prefix == \"osd rm\") {\n          err = prepare_command_osd_remove(osd);\n\n          if (err == -EBUSY) {\n\t    if (any)\n\t      ss << \", \";\n            ss << \"osd.\" << osd << \" is still up; must be down before removal. \";\n\t  } else {\n            assert(err == 0);\n\t    if (any) {\n\t      ss << \", osd.\" << osd;\n            } else {\n\t      ss << \"removed osd.\" << osd;\n            }\n\t    any = true;\n\t  }\n        }\n      }\n    }\n    if (any) {\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, err, rs,\n\t\t\t\t\t\tget_last_committed() + 1));\n      return true;\n    }\n  } else if (prefix == \"osd add-noup\" ||\n             prefix == \"osd add-nodown\" ||\n             prefix == \"osd add-noin\" ||\n             prefix == \"osd add-noout\") {\n\n    enum {\n      OP_NOUP,\n      OP_NODOWN,\n      OP_NOIN,\n      OP_NOOUT,\n    } option;\n\n    if (prefix == \"osd add-noup\") {\n      option = OP_NOUP;\n    } else if (prefix == \"osd add-nodown\") {\n      option = OP_NODOWN;\n    } else if (prefix == \"osd add-noin\") {\n      option = OP_NOIN;\n    } else {\n      option = OP_NOOUT;\n    }\n\n    bool any = false;\n    bool stop = false;\n\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n\n      set<int> osds;\n\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n        osdmap.get_all_osds(osds);\n        stop = true;\n      } else {\n        // try traditional single osd way\n\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          // ss has reason for failure\n          ss << \", unable to parse osd id:\\\"\" << idvec[j] << \"\\\". \";\n          err = -EINVAL;\n          continue;\n        }\n\n        osds.insert(osd);\n      }\n\n      for (auto &osd : osds) {\n\n        if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        switch (option) {\n        case OP_NOUP:\n          if (osdmap.is_up(osd)) {\n            ss << \"osd.\" << osd << \" is already up. \";\n            continue;\n          }\n\n          if (osdmap.is_noup(osd)) {\n            if (pending_inc.pending_osd_state_clear(osd, CEPH_OSD_NOUP))\n              any = true;\n          } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOUP);\n            any = true;\n          }\n\n          break;\n\n        case OP_NODOWN:\n          if (osdmap.is_down(osd)) {\n            ss << \"osd.\" << osd << \" is already down. \";\n            continue;\n          }\n\n          if (osdmap.is_nodown(osd)) {\n            if (pending_inc.pending_osd_state_clear(osd, CEPH_OSD_NODOWN))\n              any = true;\n          } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_NODOWN);\n            any = true;\n          }\n\n          break;\n\n        case OP_NOIN:\n          if (osdmap.is_in(osd)) {\n            ss << \"osd.\" << osd << \" is already in. \";\n            continue;\n          }\n\n          if (osdmap.is_noin(osd)) {\n            if (pending_inc.pending_osd_state_clear(osd, CEPH_OSD_NOIN))\n              any = true;\n          } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOIN);\n            any = true;\n          }\n\n          break;\n\n        case OP_NOOUT:\n          if (osdmap.is_out(osd)) {\n            ss << \"osd.\" << osd << \" is already out. \";\n            continue;\n          }\n\n          if (osdmap.is_noout(osd)) {\n            if (pending_inc.pending_osd_state_clear(osd, CEPH_OSD_NOOUT))\n              any = true;\n          } else {\n            pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOOUT);\n            any = true;\n          }\n\n          break;\n\n        default:\n\t  assert(0 == \"invalid option\");\n        }\n      }\n    }\n\n    if (any) {\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, err, rs,\n                                 get_last_committed() + 1));\n      return true;\n    }\n  } else if (prefix == \"osd rm-noup\" ||\n             prefix == \"osd rm-nodown\" ||\n             prefix == \"osd rm-noin\" ||\n             prefix == \"osd rm-noout\") {\n\n    enum {\n      OP_NOUP,\n      OP_NODOWN,\n      OP_NOIN,\n      OP_NOOUT,\n    } option;\n\n    if (prefix == \"osd rm-noup\") {\n      option = OP_NOUP;\n    } else if (prefix == \"osd rm-nodown\") {\n      option = OP_NODOWN;\n    } else if (prefix == \"osd rm-noin\") {\n      option = OP_NOIN;\n    } else {\n      option = OP_NOOUT;\n    }\n\n    bool any = false;\n    bool stop = false;\n\n    vector<string> idvec;\n    cmd_getval(cct, cmdmap, \"ids\", idvec);\n\n    for (unsigned j = 0; j < idvec.size() && !stop; j++) {\n\n      vector<int> osds;\n\n      // wildcard?\n      if (j == 0 &&\n          (idvec[0] == \"any\" || idvec[0] == \"all\" || idvec[0] == \"*\")) {\n\n        // touch previous noup/nodown/noin/noout osds only\n        switch (option) {\n        case OP_NOUP:\n          osdmap.get_noup_osds(&osds);\n          break;\n        case OP_NODOWN:\n          osdmap.get_nodown_osds(&osds);\n          break;\n        case OP_NOIN:\n          osdmap.get_noin_osds(&osds);\n          break;\n        case OP_NOOUT:\n          osdmap.get_noout_osds(&osds);\n          break;\n        default:\n          assert(0 == \"invalid option\");\n        }\n\n        // cancel any pending noup/nodown/noin/noout requests too\n        vector<int> pending_state_osds;\n        (void) pending_inc.get_pending_state_osds(&pending_state_osds);\n        for (auto &p : pending_state_osds) {\n\n          switch (option) {\n          case OP_NOUP:\n            if (!osdmap.is_noup(p) &&\n                pending_inc.pending_osd_state_clear(p, CEPH_OSD_NOUP)) {\n              any = true;\n            }\n            break;\n\n          case OP_NODOWN:\n            if (!osdmap.is_nodown(p) &&\n                pending_inc.pending_osd_state_clear(p, CEPH_OSD_NODOWN)) {\n              any = true;\n            }\n            break;\n\n          case OP_NOIN:\n            if (!osdmap.is_noin(p) &&\n                pending_inc.pending_osd_state_clear(p, CEPH_OSD_NOIN)) {\n              any = true;\n            }\n            break;\n\n          case OP_NOOUT:\n            if (!osdmap.is_noout(p) &&\n                pending_inc.pending_osd_state_clear(p, CEPH_OSD_NOOUT)) {\n              any = true;\n            }\n            break;\n\n          default:\n            assert(0 == \"invalid option\");\n          }\n        }\n\n        stop = true;\n      } else {\n        // try traditional single osd way\n\n        long osd = parse_osd_id(idvec[j].c_str(), &ss);\n        if (osd < 0) {\n          // ss has reason for failure\n          ss << \", unable to parse osd id:\\\"\" << idvec[j] << \"\\\". \";\n          err = -EINVAL;\n          continue;\n        }\n\n        osds.push_back(osd);\n      }\n\n      for (auto &osd : osds) {\n\n        if (!osdmap.exists(osd)) {\n          ss << \"osd.\" << osd << \" does not exist. \";\n          continue;\n        }\n\n        switch (option) {\n          case OP_NOUP:\n            if (osdmap.is_noup(osd)) {\n              pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOUP);\n              any = true;\n            } else if (pending_inc.pending_osd_state_clear(\n              osd, CEPH_OSD_NOUP)) {\n              any = true;\n            }\n            break;\n\n          case OP_NODOWN:\n            if (osdmap.is_nodown(osd)) {\n              pending_inc.pending_osd_state_set(osd, CEPH_OSD_NODOWN);\n              any = true;\n            } else if (pending_inc.pending_osd_state_clear(\n              osd, CEPH_OSD_NODOWN)) {\n              any = true;\n            }\n            break;\n\n          case OP_NOIN:\n            if (osdmap.is_noin(osd)) {\n              pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOIN);\n              any = true;\n            } else if (pending_inc.pending_osd_state_clear(\n              osd, CEPH_OSD_NOIN)) {\n              any = true;\n            }\n            break;\n\n          case OP_NOOUT:\n            if (osdmap.is_noout(osd)) {\n              pending_inc.pending_osd_state_set(osd, CEPH_OSD_NOOUT);\n              any = true;\n            } else if (pending_inc.pending_osd_state_clear(\n              osd, CEPH_OSD_NOOUT)) {\n              any = true;\n            }\n            break;\n\n          default:\n            assert(0 == \"invalid option\");\n        }\n      }\n    }\n\n    if (any) {\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, err, rs,\n                                 get_last_committed() + 1));\n      return true;\n    }\n  } else if (prefix == \"osd pg-temp\") {\n    string pgidstr;\n    if (!cmd_getval(cct, cmdmap, \"pgid\", pgidstr)) {\n      ss << \"unable to parse 'pgid' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"pgid\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    pg_t pgid;\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.pg_exists(pgid)) {\n      ss << \"pg \" << pgid << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    if (pending_inc.new_pg_temp.count(pgid)) {\n      dout(10) << __func__ << \" waiting for pending update on \" << pgid << dendl;\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n\n    vector<int64_t> id_vec;\n    vector<int32_t> new_pg_temp;\n    cmd_getval(cct, cmdmap, \"id\", id_vec);\n    if (id_vec.empty())  {\n      pending_inc.new_pg_temp[pgid] = mempool::osdmap::vector<int>();\n      ss << \"done cleaning up pg_temp of \" << pgid;\n      goto update;\n    }\n    for (auto osd : id_vec) {\n      if (!osdmap.exists(osd)) {\n        ss << \"osd.\" << osd << \" does not exist\";\n        err = -ENOENT;\n        goto reply;\n      }\n      new_pg_temp.push_back(osd);\n    }\n\n    int pool_min_size = osdmap.get_pg_pool_min_size(pgid);\n    if ((int)new_pg_temp.size() < pool_min_size) {\n      ss << \"num of osds (\" << new_pg_temp.size() <<\") < pool min size (\"\n         << pool_min_size << \")\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    int pool_size = osdmap.get_pg_pool_size(pgid);\n    if ((int)new_pg_temp.size() > pool_size) {\n      ss << \"num of osds (\" << new_pg_temp.size() <<\") > pool size (\"\n         << pool_size << \")\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    pending_inc.new_pg_temp[pgid] = mempool::osdmap::vector<int>(\n      new_pg_temp.begin(), new_pg_temp.end());\n    ss << \"set \" << pgid << \" pg_temp mapping to \" << new_pg_temp;\n    goto update;\n  } else if (prefix == \"osd primary-temp\") {\n    string pgidstr;\n    if (!cmd_getval(cct, cmdmap, \"pgid\", pgidstr)) {\n      ss << \"unable to parse 'pgid' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"pgid\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    pg_t pgid;\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.pg_exists(pgid)) {\n      ss << \"pg \" << pgid << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n\n    int64_t osd;\n    if (!cmd_getval(cct, cmdmap, \"id\", osd)) {\n      ss << \"unable to parse 'id' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (osd != -1 && !osdmap.exists(osd)) {\n      ss << \"osd.\" << osd << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n\n    if (osdmap.require_min_compat_client > 0 &&\n\tosdmap.require_min_compat_client < CEPH_RELEASE_FIREFLY) {\n      ss << \"require_min_compat_client \"\n\t << ceph_release_name(osdmap.require_min_compat_client)\n\t << \" < firefly, which is required for primary-temp\";\n      err = -EPERM;\n      goto reply;\n    }\n\n    pending_inc.new_primary_temp[pgid] = osd;\n    ss << \"set \" << pgid << \" primary_temp mapping to \" << osd;\n    goto update;\n  } else if (prefix == \"osd pg-upmap\" ||\n             prefix == \"osd rm-pg-upmap\" ||\n             prefix == \"osd pg-upmap-items\" ||\n             prefix == \"osd rm-pg-upmap-items\") {\n    if (osdmap.require_min_compat_client < CEPH_RELEASE_LUMINOUS) {\n      ss << \"min_compat_client \"\n\t << ceph_release_name(osdmap.require_min_compat_client)\n\t << \" < luminous, which is required for pg-upmap. \"\n         << \"Try 'ceph osd set-require-min-compat-client luminous' \"\n         << \"before using the new interface\";\n      err = -EPERM;\n      goto reply;\n    }\n    err = check_cluster_features(CEPH_FEATUREMASK_OSDMAP_PG_UPMAP, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err < 0)\n      goto reply;\n    string pgidstr;\n    if (!cmd_getval(cct, cmdmap, \"pgid\", pgidstr)) {\n      ss << \"unable to parse 'pgid' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"pgid\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    pg_t pgid;\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.pg_exists(pgid)) {\n      ss << \"pg \" << pgid << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    if (pending_inc.old_pools.count(pgid.pool())) {\n      ss << \"pool of \" << pgid << \" is pending removal\";\n      err = -ENOENT;\n      getline(ss, rs);\n      wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon, op, err, rs, get_last_committed() + 1));\n      return true;\n    }\n\n    enum {\n      OP_PG_UPMAP,\n      OP_RM_PG_UPMAP,\n      OP_PG_UPMAP_ITEMS,\n      OP_RM_PG_UPMAP_ITEMS,\n    } option;\n\n    if (prefix == \"osd pg-upmap\") {\n      option = OP_PG_UPMAP;\n    } else if (prefix == \"osd rm-pg-upmap\") {\n      option = OP_RM_PG_UPMAP;\n    } else if (prefix == \"osd pg-upmap-items\") {\n      option = OP_PG_UPMAP_ITEMS;\n    } else {\n      option = OP_RM_PG_UPMAP_ITEMS;\n    }\n\n    // check pending upmap changes\n    switch (option) {\n    case OP_PG_UPMAP: // fall through\n    case OP_RM_PG_UPMAP:\n      if (pending_inc.new_pg_upmap.count(pgid) ||\n          pending_inc.old_pg_upmap.count(pgid)) {\n        dout(10) << __func__ << \" waiting for pending update on \"\n                 << pgid << dendl;\n        wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n        return true;\n      }\n      break;\n\n    case OP_PG_UPMAP_ITEMS: // fall through\n    case OP_RM_PG_UPMAP_ITEMS:\n      if (pending_inc.new_pg_upmap_items.count(pgid) ||\n          pending_inc.old_pg_upmap_items.count(pgid)) {\n        dout(10) << __func__ << \" waiting for pending update on \"\n                 << pgid << dendl;\n        wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n        return true;\n      }\n      break;\n\n    default:\n      assert(0 == \"invalid option\");\n    }\n\n    switch (option) {\n    case OP_PG_UPMAP:\n      {\n        vector<int64_t> id_vec;\n        if (!cmd_getval(cct, cmdmap, \"id\", id_vec)) {\n          ss << \"unable to parse 'id' value(s) '\"\n             << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        int pool_min_size = osdmap.get_pg_pool_min_size(pgid);\n        if ((int)id_vec.size() < pool_min_size) {\n          ss << \"num of osds (\" << id_vec.size() <<\") < pool min size (\"\n             << pool_min_size << \")\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        int pool_size = osdmap.get_pg_pool_size(pgid);\n        if ((int)id_vec.size() > pool_size) {\n          ss << \"num of osds (\" << id_vec.size() <<\") > pool size (\"\n             << pool_size << \")\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        vector<int32_t> new_pg_upmap;\n        for (auto osd : id_vec) {\n          if (osd != CRUSH_ITEM_NONE && !osdmap.exists(osd)) {\n            ss << \"osd.\" << osd << \" does not exist\";\n            err = -ENOENT;\n            goto reply;\n          }\n          auto it = std::find(new_pg_upmap.begin(), new_pg_upmap.end(), osd);\n          if (it != new_pg_upmap.end()) {\n            ss << \"osd.\" << osd << \" already exists, \";\n            continue;\n          }\n          new_pg_upmap.push_back(osd);\n        }\n\n        if (new_pg_upmap.empty()) {\n          ss << \"no valid upmap items(pairs) is specified\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        pending_inc.new_pg_upmap[pgid] = mempool::osdmap::vector<int32_t>(\n          new_pg_upmap.begin(), new_pg_upmap.end());\n        ss << \"set \" << pgid << \" pg_upmap mapping to \" << new_pg_upmap;\n      }\n      break;\n\n    case OP_RM_PG_UPMAP:\n      {\n        pending_inc.old_pg_upmap.insert(pgid);\n        ss << \"clear \" << pgid << \" pg_upmap mapping\";\n      }\n      break;\n\n    case OP_PG_UPMAP_ITEMS:\n      {\n        vector<int64_t> id_vec;\n        if (!cmd_getval(cct, cmdmap, \"id\", id_vec)) {\n          ss << \"unable to parse 'id' value(s) '\"\n             << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        if (id_vec.size() % 2) {\n          ss << \"you must specify pairs of osd ids to be remapped\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        int pool_size = osdmap.get_pg_pool_size(pgid);\n        if ((int)(id_vec.size() / 2) > pool_size) {\n          ss << \"num of osd pairs (\" << id_vec.size() / 2 <<\") > pool size (\"\n             << pool_size << \")\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        vector<pair<int32_t,int32_t>> new_pg_upmap_items;\n        ostringstream items;\n        items << \"[\";\n        for (auto p = id_vec.begin(); p != id_vec.end(); ++p) {\n          int from = *p++;\n          int to = *p;\n          if (from == to) {\n            ss << \"from osd.\" << from << \" == to osd.\" << to << \", \";\n            continue;\n          }\n          if (!osdmap.exists(from)) {\n            ss << \"osd.\" << from << \" does not exist\";\n            err = -ENOENT;\n            goto reply;\n          }\n          if (to != CRUSH_ITEM_NONE && !osdmap.exists(to)) {\n            ss << \"osd.\" << to << \" does not exist\";\n            err = -ENOENT;\n            goto reply;\n          }\n          pair<int32_t,int32_t> entry = make_pair(from, to);\n          auto it = std::find(new_pg_upmap_items.begin(),\n            new_pg_upmap_items.end(), entry);\n          if (it != new_pg_upmap_items.end()) {\n            ss << \"osd.\" << from << \" -> osd.\" << to << \" already exists, \";\n            continue;\n          }\n          new_pg_upmap_items.push_back(entry);\n          items << from << \"->\" << to << \",\";\n        }\n        string out(items.str());\n        out.resize(out.size() - 1); // drop last ','\n        out += \"]\";\n\n        if (new_pg_upmap_items.empty()) {\n          ss << \"no valid upmap items(pairs) is specified\";\n          err = -EINVAL;\n          goto reply;\n        }\n\n        pending_inc.new_pg_upmap_items[pgid] =\n          mempool::osdmap::vector<pair<int32_t,int32_t>>(\n          new_pg_upmap_items.begin(), new_pg_upmap_items.end());\n        ss << \"set \" << pgid << \" pg_upmap_items mapping to \" << out;\n      }\n      break;\n\n    case OP_RM_PG_UPMAP_ITEMS:\n      {\n        pending_inc.old_pg_upmap_items.insert(pgid);\n        ss << \"clear \" << pgid << \" pg_upmap_items mapping\";\n      }\n      break;\n\n    default:\n      assert(0 == \"invalid option\");\n    }\n\n    goto update;\n  } else if (prefix == \"osd primary-affinity\") {\n    int64_t id;\n    if (!cmd_getval(cct, cmdmap, \"id\", id)) {\n      ss << \"invalid osd id value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    double w;\n    if (!cmd_getval(cct, cmdmap, \"weight\", w)) {\n      ss << \"unable to parse 'weight' value '\"\n\t << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    long ww = (int)((double)CEPH_OSD_MAX_PRIMARY_AFFINITY*w);\n    if (ww < 0L) {\n      ss << \"weight must be >= 0\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (osdmap.require_min_compat_client > 0 &&\n\tosdmap.require_min_compat_client < CEPH_RELEASE_FIREFLY) {\n      ss << \"require_min_compat_client \"\n\t << ceph_release_name(osdmap.require_min_compat_client)\n\t << \" < firefly, which is required for primary-affinity\";\n      err = -EPERM;\n      goto reply;\n    }\n    if (osdmap.exists(id)) {\n      pending_inc.new_primary_affinity[id] = ww;\n      ss << \"set osd.\" << id << \" primary-affinity to \" << w << \" (\" << ios::hex << ww << ios::dec << \")\";\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                                                get_last_committed() + 1));\n      return true;\n    } else {\n      ss << \"osd.\" << id << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n  } else if (prefix == \"osd reweight\") {\n    int64_t id;\n    if (!cmd_getval(cct, cmdmap, \"id\", id)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    double w;\n    if (!cmd_getval(cct, cmdmap, \"weight\", w)) {\n      ss << \"unable to parse weight value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"weight\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    long ww = (int)((double)CEPH_OSD_IN*w);\n    if (ww < 0L) {\n      ss << \"weight must be >= 0\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (osdmap.exists(id)) {\n      pending_inc.new_weight[id] = ww;\n      ss << \"reweighted osd.\" << id << \" to \" << w << \" (\" << std::hex << ww << std::dec << \")\";\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\tget_last_committed() + 1));\n      return true;\n    } else {\n      ss << \"osd.\" << id << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n  } else if (prefix == \"osd reweightn\") {\n    map<int32_t, uint32_t> weights;\n    err = parse_reweights(cct, cmdmap, osdmap, &weights);\n    if (err) {\n      ss << \"unable to parse 'weights' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"weights\")) << \"'\";\n      goto reply;\n    }\n    pending_inc.new_weight.insert(weights.begin(), weights.end());\n    wait_for_finished_proposal(\n\top,\n\tnew Monitor::C_Command(mon, op, 0, rs, rdata, get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd lost\") {\n    int64_t id;\n    if (!cmd_getval(cct, cmdmap, \"id\", id)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    string sure;\n    if (!cmd_getval(cct, cmdmap, \"sure\", sure) || sure != \"--yes-i-really-mean-it\") {\n      ss << \"are you SURE?  this might mean real, permanent data loss.  pass \"\n\t    \"--yes-i-really-mean-it if you really do.\";\n      err = -EPERM;\n      goto reply;\n    } else if (!osdmap.exists(id)) {\n      ss << \"osd.\" << id << \" does not exist\";\n      err = -ENOENT;\n      goto reply;\n    } else if (!osdmap.is_down(id)) {\n      ss << \"osd.\" << id << \" is not down\";\n      err = -EBUSY;\n      goto reply;\n    } else {\n      epoch_t e = osdmap.get_info(id).down_at;\n      pending_inc.new_lost[id] = e;\n      ss << \"marked osd lost in epoch \" << e;\n      getline(ss, rs);\n      wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\tget_last_committed() + 1));\n      return true;\n    }\n\n  } else if (prefix == \"osd destroy\" ||\n\t     prefix == \"osd purge\" ||\n\t     prefix == \"osd purge-new\") {\n    /* Destroying an OSD means that we don't expect to further make use of\n     * the OSDs data (which may even become unreadable after this operation),\n     * and that we are okay with scrubbing all its cephx keys and config-key\n     * data (which may include lockbox keys, thus rendering the osd's data\n     * unreadable).\n     *\n     * The OSD will not be removed. Instead, we will mark it as destroyed,\n     * such that a subsequent call to `create` will not reuse the osd id.\n     * This will play into being able to recreate the OSD, at the same\n     * crush location, with minimal data movement.\n     */\n\n    // make sure authmon is writeable.\n    if (!mon->authmon()->is_writeable()) {\n      dout(10) << __func__ << \" waiting for auth mon to be writeable for \"\n               << \"osd destroy\" << dendl;\n      mon->authmon()->wait_for_writeable(op, new C_RetryMessage(this, op));\n      return false;\n    }\n\n    int64_t id;\n    if (!cmd_getval(cct, cmdmap, \"id\", id)) {\n      ss << \"unable to parse osd id value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"id\")) << \"\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    bool is_destroy = (prefix == \"osd destroy\");\n    if (!is_destroy) {\n      assert(\"osd purge\" == prefix ||\n\t     \"osd purge-new\" == prefix);\n    }\n\n    string sure;\n    if (!cmd_getval(cct, cmdmap, \"sure\", sure) ||\n        sure != \"--yes-i-really-mean-it\") {\n      ss << \"Are you SURE? This will mean real, permanent data loss, as well \"\n         << \"as cephx and lockbox keys. Pass --yes-i-really-mean-it if you \"\n         << \"really do.\";\n      err = -EPERM;\n      goto reply;\n    } else if (!osdmap.exists(id)) {\n      ss << \"osd.\" << id << \" does not exist\";\n      err = 0; // idempotent\n      goto reply;\n    } else if (osdmap.is_up(id)) {\n      ss << \"osd.\" << id << \" is not `down`.\";\n      err = -EBUSY;\n      goto reply;\n    } else if (is_destroy && osdmap.is_destroyed(id)) {\n      ss << \"destroyed osd.\" << id;\n      err = 0;\n      goto reply;\n    }\n\n    if (prefix == \"osd purge-new\" &&\n\t(osdmap.get_state(id) & CEPH_OSD_NEW) == 0) {\n      ss << \"osd.\" << id << \" is not new\";\n      err = -EPERM;\n      goto reply;\n    }\n\n    bool goto_reply = false;\n\n    paxos->plug();\n    if (is_destroy) {\n      err = prepare_command_osd_destroy(id, ss);\n      // we checked above that it should exist.\n      assert(err != -ENOENT);\n    } else {\n      err = prepare_command_osd_purge(id, ss);\n      if (err == -ENOENT) {\n        err = 0;\n        ss << \"osd.\" << id << \" does not exist.\";\n        goto_reply = true;\n      }\n    }\n    paxos->unplug();\n\n    if (err < 0 || goto_reply) {\n      goto reply;\n    }\n\n    if (is_destroy) {\n      ss << \"destroyed osd.\" << id;\n    } else {\n      ss << \"purged osd.\" << id;\n    }\n\n    getline(ss, rs);\n    wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon, op, 0, rs, get_last_committed() + 1));\n    force_immediate_propose();\n    return true;\n\n  } else if (prefix == \"osd new\") {\n\n    // make sure authmon is writeable.\n    if (!mon->authmon()->is_writeable()) {\n      dout(10) << __func__ << \" waiting for auth mon to be writeable for \"\n               << \"osd new\" << dendl;\n      mon->authmon()->wait_for_writeable(op, new C_RetryMessage(this, op));\n      return false;\n    }\n\n    map<string,string> param_map;\n\n    bufferlist bl = m->get_data();\n    string param_json = bl.to_str();\n    dout(20) << __func__ << \" osd new json = \" << param_json << dendl;\n\n    err = get_json_str_map(param_json, ss, &param_map);\n    if (err < 0)\n      goto reply;\n\n    dout(20) << __func__ << \" osd new params \" << param_map << dendl;\n\n    paxos->plug();\n    err = prepare_command_osd_new(op, cmdmap, param_map, ss, f.get());\n    paxos->unplug();\n\n    if (err < 0) {\n      goto reply;\n    }\n\n    if (f) {\n      f->flush(rdata);\n    } else {\n      rdata.append(ss);\n    }\n\n    if (err == EEXIST) {\n      // idempotent operation\n      err = 0;\n      goto reply;\n    }\n\n    wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon, op, 0, rs, rdata,\n                               get_last_committed() + 1));\n    force_immediate_propose();\n    return true;\n\n  } else if (prefix == \"osd create\") {\n\n    // optional id provided?\n    int64_t id = -1, cmd_id = -1;\n    if (cmd_getval(cct, cmdmap, \"id\", cmd_id)) {\n      if (cmd_id < 0) {\n\tss << \"invalid osd id value '\" << cmd_id << \"'\";\n\terr = -EINVAL;\n\tgoto reply;\n      }\n      dout(10) << \" osd create got id \" << cmd_id << dendl;\n    }\n\n    uuid_d uuid;\n    string uuidstr;\n    if (cmd_getval(cct, cmdmap, \"uuid\", uuidstr)) {\n      if (!uuid.parse(uuidstr.c_str())) {\n        ss << \"invalid uuid value '\" << uuidstr << \"'\";\n        err = -EINVAL;\n        goto reply;\n      }\n      // we only care about the id if we also have the uuid, to\n      // ensure the operation's idempotency.\n      id = cmd_id;\n    }\n\n    int32_t new_id = -1;\n    err = prepare_command_osd_create(id, uuid, &new_id, ss);\n    if (err < 0) {\n      if (err == -EAGAIN) {\n        wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n        return true;\n      }\n      // a check has failed; reply to the user.\n      goto reply;\n\n    } else if (err == EEXIST) {\n      // this is an idempotent operation; we can go ahead and reply.\n      if (f) {\n        f->open_object_section(\"created_osd\");\n        f->dump_int(\"osdid\", new_id);\n        f->close_section();\n        f->flush(rdata);\n      } else {\n        ss << new_id;\n        rdata.append(ss);\n      }\n      err = 0;\n      goto reply;\n    }\n\n    string empty_device_class;\n    do_osd_create(id, uuid, empty_device_class, &new_id);\n\n    if (f) {\n      f->open_object_section(\"created_osd\");\n      f->dump_int(\"osdid\", new_id);\n      f->close_section();\n      f->flush(rdata);\n    } else {\n      ss << new_id;\n      rdata.append(ss);\n    }\n    wait_for_finished_proposal(op,\n        new Monitor::C_Command(mon, op, 0, rs, rdata,\n                               get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd blacklist clear\") {\n    pending_inc.new_blacklist.clear();\n    std::list<std::pair<entity_addr_t,utime_t > > blacklist;\n    osdmap.get_blacklist(&blacklist);\n    for (const auto &entry : blacklist) {\n      pending_inc.old_blacklist.push_back(entry.first);\n    }\n    ss << \" removed all blacklist entries\";\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n                                              get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd blacklist\") {\n    string addrstr;\n    cmd_getval(cct, cmdmap, \"addr\", addrstr);\n    entity_addr_t addr;\n    if (!addr.parse(addrstr.c_str(), 0)) {\n      ss << \"unable to parse address \" << addrstr;\n      err = -EINVAL;\n      goto reply;\n    }\n    else {\n      string blacklistop;\n      cmd_getval(cct, cmdmap, \"blacklistop\", blacklistop);\n      if (blacklistop == \"add\") {\n\tutime_t expires = ceph_clock_now();\n\tdouble d;\n\t// default one hour\n\tcmd_getval(cct, cmdmap, \"expire\", d,\n          g_conf->mon_osd_blacklist_default_expire);\n\texpires += d;\n\n\tpending_inc.new_blacklist[addr] = expires;\n\n        {\n          // cancel any pending un-blacklisting request too\n          auto it = std::find(pending_inc.old_blacklist.begin(),\n            pending_inc.old_blacklist.end(), addr);\n          if (it != pending_inc.old_blacklist.end()) {\n            pending_inc.old_blacklist.erase(it);\n          }\n        }\n\n\tss << \"blacklisting \" << addr << \" until \" << expires << \" (\" << d << \" sec)\";\n\tgetline(ss, rs);\n\twait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t  get_last_committed() + 1));\n\treturn true;\n      } else if (blacklistop == \"rm\") {\n\tif (osdmap.is_blacklisted(addr) ||\n\t    pending_inc.new_blacklist.count(addr)) {\n\t  if (osdmap.is_blacklisted(addr))\n\t    pending_inc.old_blacklist.push_back(addr);\n\t  else\n\t    pending_inc.new_blacklist.erase(addr);\n\t  ss << \"un-blacklisting \" << addr;\n\t  getline(ss, rs);\n\t  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t    get_last_committed() + 1));\n\t  return true;\n\t}\n\tss << addr << \" isn't blacklisted\";\n\terr = 0;\n\tgoto reply;\n      }\n    }\n  } else if (prefix == \"osd pool mksnap\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string snapname;\n    cmd_getval(cct, cmdmap, \"snap\", snapname);\n    const pg_pool_t *p = osdmap.get_pg_pool(pool);\n    if (p->is_unmanaged_snaps_mode()) {\n      ss << \"pool \" << poolstr << \" is in unmanaged snaps mode\";\n      err = -EINVAL;\n      goto reply;\n    } else if (p->snap_exists(snapname.c_str())) {\n      ss << \"pool \" << poolstr << \" snap \" << snapname << \" already exists\";\n      err = 0;\n      goto reply;\n    } else if (p->is_tier()) {\n      ss << \"pool \" << poolstr << \" is a cache tier\";\n      err = -EINVAL;\n      goto reply;\n    }\n    pg_pool_t *pp = 0;\n    if (pending_inc.new_pools.count(pool))\n      pp = &pending_inc.new_pools[pool];\n    if (!pp) {\n      pp = &pending_inc.new_pools[pool];\n      *pp = *p;\n    }\n    if (pp->snap_exists(snapname.c_str())) {\n      ss << \"pool \" << poolstr << \" snap \" << snapname << \" already exists\";\n    } else {\n      pp->add_snap(snapname.c_str(), ceph_clock_now());\n      pp->set_snap_epoch(pending_inc.epoch);\n      ss << \"created pool \" << poolstr << \" snap \" << snapname;\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pool rmsnap\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string snapname;\n    cmd_getval(cct, cmdmap, \"snap\", snapname);\n    const pg_pool_t *p = osdmap.get_pg_pool(pool);\n    if (p->is_unmanaged_snaps_mode()) {\n      ss << \"pool \" << poolstr << \" is in unmanaged snaps mode\";\n      err = -EINVAL;\n      goto reply;\n    } else if (!p->snap_exists(snapname.c_str())) {\n      ss << \"pool \" << poolstr << \" snap \" << snapname << \" does not exist\";\n      err = 0;\n      goto reply;\n    }\n    pg_pool_t *pp = 0;\n    if (pending_inc.new_pools.count(pool))\n      pp = &pending_inc.new_pools[pool];\n    if (!pp) {\n      pp = &pending_inc.new_pools[pool];\n      *pp = *p;\n    }\n    snapid_t sn = pp->snap_exists(snapname.c_str());\n    if (sn) {\n      pp->remove_snap(sn);\n      pp->set_snap_epoch(pending_inc.epoch);\n      ss << \"removed pool \" << poolstr << \" snap \" << snapname;\n    } else {\n      ss << \"already removed pool \" << poolstr << \" snap \" << snapname;\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pool create\") {\n    int64_t  pg_num;\n    int64_t pgp_num;\n    cmd_getval(cct, cmdmap, \"pg_num\", pg_num, int64_t(0));\n    cmd_getval(cct, cmdmap, \"pgp_num\", pgp_num, pg_num);\n\n    string pool_type_str;\n    cmd_getval(cct, cmdmap, \"pool_type\", pool_type_str);\n    if (pool_type_str.empty())\n      pool_type_str = g_conf->get_val<string>(\"osd_pool_default_type\");\n\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id >= 0) {\n      const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n      if (pool_type_str != p->get_type_name()) {\n\tss << \"pool '\" << poolstr << \"' cannot change to type \" << pool_type_str;\n \terr = -EINVAL;\n      } else {\n\tss << \"pool '\" << poolstr << \"' already exists\";\n\terr = 0;\n      }\n      goto reply;\n    }\n\n    int pool_type;\n    if (pool_type_str == \"replicated\") {\n      pool_type = pg_pool_t::TYPE_REPLICATED;\n    } else if (pool_type_str == \"erasure\") {\n      pool_type = pg_pool_t::TYPE_ERASURE;\n    } else {\n      ss << \"unknown pool type '\" << pool_type_str << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    bool implicit_rule_creation = false;\n    int64_t expected_num_objects = 0;\n    string rule_name;\n    cmd_getval(cct, cmdmap, \"rule\", rule_name);\n    string erasure_code_profile;\n    cmd_getval(cct, cmdmap, \"erasure_code_profile\", erasure_code_profile);\n\n    if (pool_type == pg_pool_t::TYPE_ERASURE) {\n      if (erasure_code_profile == \"\")\n\terasure_code_profile = \"default\";\n      //handle the erasure code profile\n      if (erasure_code_profile == \"default\") {\n\tif (!osdmap.has_erasure_code_profile(erasure_code_profile)) {\n\t  if (pending_inc.has_erasure_code_profile(erasure_code_profile)) {\n\t    dout(20) << \"erasure code profile \" << erasure_code_profile << \" already pending\" << dendl;\n\t    goto wait;\n\t  }\n\n\t  map<string,string> profile_map;\n\t  err = osdmap.get_erasure_code_profile_default(cct,\n\t\t\t\t\t\t      profile_map,\n\t\t\t\t\t\t      &ss);\n\t  if (err)\n\t    goto reply;\n\t  dout(20) << \"erasure code profile \" << erasure_code_profile << \" set\" << dendl;\n\t  pending_inc.set_erasure_code_profile(erasure_code_profile, profile_map);\n\t  goto wait;\n\t}\n      }\n      if (rule_name == \"\") {\n\timplicit_rule_creation = true;\n\tif (erasure_code_profile == \"default\") {\n\t  rule_name = \"erasure-code\";\n\t} else {\n\t  dout(1) << \"implicitly use rule named after the pool: \"\n\t\t<< poolstr << dendl;\n\t  rule_name = poolstr;\n\t}\n      }\n      cmd_getval(g_ceph_context, cmdmap, \"expected_num_objects\",\n                 expected_num_objects, int64_t(0));\n    } else {\n      //NOTE:for replicated pool,cmd_map will put rule_name to erasure_code_profile field\n      //     and put expected_num_objects to rule field\n      if (erasure_code_profile != \"\") { // cmd is from CLI\n        if (rule_name != \"\") {\n          string interr;\n          expected_num_objects = strict_strtoll(rule_name.c_str(), 10, &interr);\n          if (interr.length()) {\n            ss << \"error parsing integer value '\" << rule_name << \"': \" << interr;\n            err = -EINVAL;\n            goto reply;\n          }\n        }\n        rule_name = erasure_code_profile;\n      } else { // cmd is well-formed\n        cmd_getval(g_ceph_context, cmdmap, \"expected_num_objects\",\n                   expected_num_objects, int64_t(0));\n      }\n    }\n\n    if (!implicit_rule_creation && rule_name != \"\") {\n      int rule;\n      err = get_crush_rule(rule_name, &rule, &ss);\n      if (err == -EAGAIN) {\n\twait_for_finished_proposal(op, new C_RetryMessage(this, op));\n\treturn true;\n      }\n      if (err)\n\tgoto reply;\n    }\n\n    if (expected_num_objects < 0) {\n      ss << \"'expected_num_objects' must be non-negative\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    int64_t fast_read_param;\n    cmd_getval(cct, cmdmap, \"fast_read\", fast_read_param, int64_t(-1));\n    FastReadType fast_read = FAST_READ_DEFAULT;\n    if (fast_read_param == 0)\n      fast_read = FAST_READ_OFF;\n    else if (fast_read_param > 0)\n      fast_read = FAST_READ_ON;\n    \n    err = prepare_new_pool(poolstr, 0, // auid=0 for admin created pool\n\t\t\t   -1, // default crush rule\n\t\t\t   rule_name,\n\t\t\t   pg_num, pgp_num,\n\t\t\t   erasure_code_profile, pool_type,\n                           (uint64_t)expected_num_objects,\n                           fast_read,\n\t\t\t   &ss);\n    if (err < 0) {\n      switch(err) {\n      case -EEXIST:\n\tss << \"pool '\" << poolstr << \"' already exists\";\n\tbreak;\n      case -EAGAIN:\n\twait_for_finished_proposal(op, new C_RetryMessage(this, op));\n\treturn true;\n      case -ERANGE:\n        goto reply;\n      default:\n\tgoto reply;\n\tbreak;\n      }\n    } else {\n      ss << \"pool '\" << poolstr << \"' created\";\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd pool delete\" ||\n             prefix == \"osd pool rm\") {\n    // osd pool delete/rm <poolname> <poolname again> --yes-i-really-really-mean-it\n    string poolstr, poolstr2, sure;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    cmd_getval(cct, cmdmap, \"pool2\", poolstr2);\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    int64_t pool = osdmap.lookup_pg_pool_name(poolstr.c_str());\n    if (pool < 0) {\n      ss << \"pool '\" << poolstr << \"' does not exist\";\n      err = 0;\n      goto reply;\n    }\n\n    bool force_no_fake = sure == \"--yes-i-really-really-mean-it-not-faking\";\n    if (poolstr2 != poolstr ||\n\t(sure != \"--yes-i-really-really-mean-it\" && !force_no_fake)) {\n      ss << \"WARNING: this will *PERMANENTLY DESTROY* all data stored in pool \" << poolstr\n\t << \".  If you are *ABSOLUTELY CERTAIN* that is what you want, pass the pool name *twice*, \"\n\t << \"followed by --yes-i-really-really-mean-it.\";\n      err = -EPERM;\n      goto reply;\n    }\n    err = _prepare_remove_pool(pool, &ss, force_no_fake);\n    if (err == -EAGAIN) {\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    if (err < 0)\n      goto reply;\n    goto update;\n  } else if (prefix == \"osd pool rename\") {\n    string srcpoolstr, destpoolstr;\n    cmd_getval(cct, cmdmap, \"srcpool\", srcpoolstr);\n    cmd_getval(cct, cmdmap, \"destpool\", destpoolstr);\n    int64_t pool_src = osdmap.lookup_pg_pool_name(srcpoolstr.c_str());\n    int64_t pool_dst = osdmap.lookup_pg_pool_name(destpoolstr.c_str());\n\n    if (pool_src < 0) {\n      if (pool_dst >= 0) {\n        // src pool doesn't exist, dst pool does exist: to ensure idempotency\n        // of operations, assume this rename succeeded, as it is not changing\n        // the current state.  Make sure we output something understandable\n        // for whoever is issuing the command, if they are paying attention,\n        // in case it was not intentional; or to avoid a \"wtf?\" and a bug\n        // report in case it was intentional, while expecting a failure.\n        ss << \"pool '\" << srcpoolstr << \"' does not exist; pool '\"\n          << destpoolstr << \"' does -- assuming successful rename\";\n        err = 0;\n      } else {\n        ss << \"unrecognized pool '\" << srcpoolstr << \"'\";\n        err = -ENOENT;\n      }\n      goto reply;\n    } else if (pool_dst >= 0) {\n      // source pool exists and so does the destination pool\n      ss << \"pool '\" << destpoolstr << \"' already exists\";\n      err = -EEXIST;\n      goto reply;\n    }\n\n    int ret = _prepare_rename_pool(pool_src, destpoolstr);\n    if (ret == 0) {\n      ss << \"pool '\" << srcpoolstr << \"' renamed to '\" << destpoolstr << \"'\";\n    } else {\n      ss << \"failed to rename pool '\" << srcpoolstr << \"' to '\" << destpoolstr << \"': \"\n        << cpp_strerror(ret);\n    }\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, ret, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n\n  } else if (prefix == \"osd pool set\") {\n    err = prepare_command_pool_set(cmdmap, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err < 0)\n      goto reply;\n\n    getline(ss, rs);\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t\t   get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier add\") {\n    err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string tierpoolstr;\n    cmd_getval(cct, cmdmap, \"tierpool\", tierpoolstr);\n    int64_t tierpool_id = osdmap.lookup_pg_pool_name(tierpoolstr);\n    if (tierpool_id < 0) {\n      ss << \"unrecognized pool '\" << tierpoolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    const pg_pool_t *tp = osdmap.get_pg_pool(tierpool_id);\n    assert(tp);\n\n    if (!_check_become_tier(tierpool_id, tp, pool_id, p, &err, &ss)) {\n      goto reply;\n    }\n\n    // make sure new tier is empty\n    string force_nonempty;\n    cmd_getval(cct, cmdmap, \"force_nonempty\", force_nonempty);\n    const pool_stat_t *pstats = mon->mgrstatmon()->get_pool_stat(tierpool_id);\n    if (pstats && pstats->stats.sum.num_objects != 0 &&\n\tforce_nonempty != \"--force-nonempty\") {\n      ss << \"tier pool '\" << tierpoolstr << \"' is not empty; --force-nonempty to force\";\n      err = -ENOTEMPTY;\n      goto reply;\n    }\n    if (tp->is_erasure()) {\n      ss << \"tier pool '\" << tierpoolstr\n\t << \"' is an ec pool, which cannot be a tier\";\n      err = -ENOTSUP;\n      goto reply;\n    }\n    if ((!tp->removed_snaps.empty() || !tp->snaps.empty()) &&\n\t((force_nonempty != \"--force-nonempty\") ||\n\t (!g_conf->mon_debug_unsafe_allow_tier_with_nonempty_snaps))) {\n      ss << \"tier pool '\" << tierpoolstr << \"' has snapshot state; it cannot be added as a tier without breaking the pool\";\n      err = -ENOTEMPTY;\n      goto reply;\n    }\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    pg_pool_t *ntp = pending_inc.get_new_pool(tierpool_id, tp);\n    if (np->tiers.count(tierpool_id) || ntp->is_tier()) {\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    np->tiers.insert(tierpool_id);\n    np->set_snap_epoch(pending_inc.epoch); // tier will update to our snap info\n    ntp->tier_of = pool_id;\n    ss << \"pool '\" << tierpoolstr << \"' is now (or already was) a tier of '\" << poolstr << \"'\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier remove\" ||\n             prefix == \"osd tier rm\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string tierpoolstr;\n    cmd_getval(cct, cmdmap, \"tierpool\", tierpoolstr);\n    int64_t tierpool_id = osdmap.lookup_pg_pool_name(tierpoolstr);\n    if (tierpool_id < 0) {\n      ss << \"unrecognized pool '\" << tierpoolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    const pg_pool_t *tp = osdmap.get_pg_pool(tierpool_id);\n    assert(tp);\n\n    if (!_check_remove_tier(pool_id, p, tp, &err, &ss)) {\n      goto reply;\n    }\n\n    if (p->tiers.count(tierpool_id) == 0) {\n      ss << \"pool '\" << tierpoolstr << \"' is now (or already was) not a tier of '\" << poolstr << \"'\";\n      err = 0;\n      goto reply;\n    }\n    if (tp->tier_of != pool_id) {\n      ss << \"tier pool '\" << tierpoolstr << \"' is a tier of '\"\n         << osdmap.get_pool_name(tp->tier_of) << \"': \"\n         // be scary about it; this is an inconsistency and bells must go off\n         << \"THIS SHOULD NOT HAVE HAPPENED AT ALL\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (p->read_tier == tierpool_id) {\n      ss << \"tier pool '\" << tierpoolstr << \"' is the overlay for '\" << poolstr << \"'; please remove-overlay first\";\n      err = -EBUSY;\n      goto reply;\n    }\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    pg_pool_t *ntp = pending_inc.get_new_pool(tierpool_id, tp);\n    if (np->tiers.count(tierpool_id) == 0 ||\n\tntp->tier_of != pool_id ||\n\tnp->read_tier == tierpool_id) {\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    np->tiers.erase(tierpool_id);\n    ntp->clear_tier();\n    ss << \"pool '\" << tierpoolstr << \"' is now (or already was) not a tier of '\" << poolstr << \"'\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier set-overlay\") {\n    err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string overlaypoolstr;\n    cmd_getval(cct, cmdmap, \"overlaypool\", overlaypoolstr);\n    int64_t overlaypool_id = osdmap.lookup_pg_pool_name(overlaypoolstr);\n    if (overlaypool_id < 0) {\n      ss << \"unrecognized pool '\" << overlaypoolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    const pg_pool_t *overlay_p = osdmap.get_pg_pool(overlaypool_id);\n    assert(overlay_p);\n    if (p->tiers.count(overlaypool_id) == 0) {\n      ss << \"tier pool '\" << overlaypoolstr << \"' is not a tier of '\" << poolstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (p->read_tier == overlaypool_id) {\n      err = 0;\n      ss << \"overlay for '\" << poolstr << \"' is now (or already was) '\" << overlaypoolstr << \"'\";\n      goto reply;\n    }\n    if (p->has_read_tier()) {\n      ss << \"pool '\" << poolstr << \"' has overlay '\"\n\t << osdmap.get_pool_name(p->read_tier)\n\t << \"'; please remove-overlay first\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    np->read_tier = overlaypool_id;\n    np->write_tier = overlaypool_id;\n    np->set_last_force_op_resend(pending_inc.epoch);\n    pg_pool_t *noverlay_p = pending_inc.get_new_pool(overlaypool_id, overlay_p);\n    noverlay_p->set_last_force_op_resend(pending_inc.epoch);\n    ss << \"overlay for '\" << poolstr << \"' is now (or already was) '\" << overlaypoolstr << \"'\";\n    if (overlay_p->cache_mode == pg_pool_t::CACHEMODE_NONE)\n      ss <<\" (WARNING: overlay pool cache_mode is still NONE)\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier remove-overlay\" ||\n             prefix == \"osd tier rm-overlay\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    if (!p->has_read_tier()) {\n      err = 0;\n      ss << \"there is now (or already was) no overlay for '\" << poolstr << \"'\";\n      goto reply;\n    }\n\n    if (!_check_remove_tier(pool_id, p, NULL, &err, &ss)) {\n      goto reply;\n    }\n\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    if (np->has_read_tier()) {\n      const pg_pool_t *op = osdmap.get_pg_pool(np->read_tier);\n      pg_pool_t *nop = pending_inc.get_new_pool(np->read_tier,op);\n      nop->set_last_force_op_resend(pending_inc.epoch);\n    }\n    if (np->has_write_tier()) {\n      const pg_pool_t *op = osdmap.get_pg_pool(np->write_tier);\n      pg_pool_t *nop = pending_inc.get_new_pool(np->write_tier, op);\n      nop->set_last_force_op_resend(pending_inc.epoch);\n    }\n    np->clear_read_tier();\n    np->clear_write_tier();\n    np->set_last_force_op_resend(pending_inc.epoch);\n    ss << \"there is now (or already was) no overlay for '\" << poolstr << \"'\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier cache-mode\") {\n    err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    if (!p->is_tier()) {\n      ss << \"pool '\" << poolstr << \"' is not a tier\";\n      err = -EINVAL;\n      goto reply;\n    }\n    string modestr;\n    cmd_getval(cct, cmdmap, \"mode\", modestr);\n    pg_pool_t::cache_mode_t mode = pg_pool_t::get_cache_mode_from_str(modestr);\n    if (mode < 0) {\n      ss << \"'\" << modestr << \"' is not a valid cache mode\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    if ((mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t mode != pg_pool_t::CACHEMODE_NONE &&\n\t mode != pg_pool_t::CACHEMODE_PROXY &&\n\t mode != pg_pool_t::CACHEMODE_READPROXY) &&\n\tsure != \"--yes-i-really-mean-it\") {\n      ss << \"'\" << modestr << \"' is not a well-supported cache mode and may \"\n\t << \"corrupt your data.  pass --yes-i-really-mean-it to force.\";\n      err = -EPERM;\n      goto reply;\n    }\n\n    // pool already has this cache-mode set and there are no pending changes\n    if (p->cache_mode == mode &&\n\t(pending_inc.new_pools.count(pool_id) == 0 ||\n\t pending_inc.new_pools[pool_id].cache_mode == p->cache_mode)) {\n      ss << \"set cache-mode for pool '\" << poolstr << \"'\"\n         << \" to \" << pg_pool_t::get_cache_mode_name(mode);\n      err = 0;\n      goto reply;\n    }\n\n    /* Mode description:\n     *\n     *  none:       No cache-mode defined\n     *  forward:    Forward all reads and writes to base pool\n     *  writeback:  Cache writes, promote reads from base pool\n     *  readonly:   Forward writes to base pool\n     *  readforward: Writes are in writeback mode, Reads are in forward mode\n     *  proxy:       Proxy all reads and writes to base pool\n     *  readproxy:   Writes are in writeback mode, Reads are in proxy mode\n     *\n     * Hence, these are the allowed transitions:\n     *\n     *  none -> any\n     *  forward -> proxy || readforward || readproxy || writeback || any IF num_objects_dirty == 0\n     *  proxy -> forward || readforward || readproxy || writeback || any IF num_objects_dirty == 0\n     *  readforward -> forward || proxy || readproxy || writeback || any IF num_objects_dirty == 0\n     *  readproxy -> forward || proxy || readforward || writeback || any IF num_objects_dirty == 0\n     *  writeback -> readforward || readproxy || forward || proxy\n     *  readonly -> any\n     */\n\n    // We check if the transition is valid against the current pool mode, as\n    // it is the only committed state thus far.  We will blantly squash\n    // whatever mode is on the pending state.\n\n    if (p->cache_mode == pg_pool_t::CACHEMODE_WRITEBACK &&\n        (mode != pg_pool_t::CACHEMODE_FORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_PROXY &&\n\t  mode != pg_pool_t::CACHEMODE_READFORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_READPROXY)) {\n      ss << \"unable to set cache-mode '\" << pg_pool_t::get_cache_mode_name(mode)\n         << \"' on a '\" << pg_pool_t::get_cache_mode_name(p->cache_mode)\n         << \"' pool; only '\"\n         << pg_pool_t::get_cache_mode_name(pg_pool_t::CACHEMODE_FORWARD)\n\t << \"','\"\n         << pg_pool_t::get_cache_mode_name(pg_pool_t::CACHEMODE_PROXY)\n\t << \"','\"\n         << pg_pool_t::get_cache_mode_name(pg_pool_t::CACHEMODE_READFORWARD)\n\t << \"','\"\n         << pg_pool_t::get_cache_mode_name(pg_pool_t::CACHEMODE_READPROXY)\n        << \"' allowed.\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if ((p->cache_mode == pg_pool_t::CACHEMODE_READFORWARD &&\n        (mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t  mode != pg_pool_t::CACHEMODE_FORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_PROXY &&\n\t  mode != pg_pool_t::CACHEMODE_READPROXY)) ||\n\n        (p->cache_mode == pg_pool_t::CACHEMODE_READPROXY &&\n        (mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t  mode != pg_pool_t::CACHEMODE_FORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_READFORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_PROXY)) ||\n\n        (p->cache_mode == pg_pool_t::CACHEMODE_PROXY &&\n        (mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t  mode != pg_pool_t::CACHEMODE_FORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_READFORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_READPROXY)) ||\n\n        (p->cache_mode == pg_pool_t::CACHEMODE_FORWARD &&\n        (mode != pg_pool_t::CACHEMODE_WRITEBACK &&\n\t  mode != pg_pool_t::CACHEMODE_READFORWARD &&\n\t  mode != pg_pool_t::CACHEMODE_PROXY &&\n\t  mode != pg_pool_t::CACHEMODE_READPROXY))) {\n\n      const pool_stat_t* pstats =\n        mon->mgrstatmon()->get_pool_stat(pool_id);\n\n      if (pstats && pstats->stats.sum.num_objects_dirty > 0) {\n        ss << \"unable to set cache-mode '\"\n           << pg_pool_t::get_cache_mode_name(mode) << \"' on pool '\" << poolstr\n           << \"': dirty objects found\";\n        err = -EBUSY;\n        goto reply;\n      }\n    }\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    np->cache_mode = mode;\n    // set this both when moving to and from cache_mode NONE.  this is to\n    // capture legacy pools that were set up before this flag existed.\n    np->flags |= pg_pool_t::FLAG_INCOMPLETE_CLONES;\n    ss << \"set cache-mode for pool '\" << poolstr\n\t<< \"' to \" << pg_pool_t::get_cache_mode_name(mode);\n    if (mode == pg_pool_t::CACHEMODE_NONE) {\n      const pg_pool_t *base_pool = osdmap.get_pg_pool(np->tier_of);\n      assert(base_pool);\n      if (base_pool->read_tier == pool_id ||\n\t  base_pool->write_tier == pool_id)\n\tss <<\" (WARNING: pool is still configured as read or write tier)\";\n    }\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd tier add-cache\") {\n    err = check_cluster_features(CEPH_FEATURE_OSD_CACHEPOOL, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err)\n      goto reply;\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string tierpoolstr;\n    cmd_getval(cct, cmdmap, \"tierpool\", tierpoolstr);\n    int64_t tierpool_id = osdmap.lookup_pg_pool_name(tierpoolstr);\n    if (tierpool_id < 0) {\n      ss << \"unrecognized pool '\" << tierpoolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n    const pg_pool_t *p = osdmap.get_pg_pool(pool_id);\n    assert(p);\n    const pg_pool_t *tp = osdmap.get_pg_pool(tierpool_id);\n    assert(tp);\n\n    if (!_check_become_tier(tierpool_id, tp, pool_id, p, &err, &ss)) {\n      goto reply;\n    }\n\n    int64_t size = 0;\n    if (!cmd_getval(cct, cmdmap, \"size\", size)) {\n      ss << \"unable to parse 'size' value '\"\n         << cmd_vartype_stringify(cmdmap.at(\"size\")) << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    // make sure new tier is empty\n    const pool_stat_t *pstats =\n      mon->mgrstatmon()->get_pool_stat(tierpool_id);\n    if (pstats && pstats->stats.sum.num_objects != 0) {\n      ss << \"tier pool '\" << tierpoolstr << \"' is not empty\";\n      err = -ENOTEMPTY;\n      goto reply;\n    }\n    auto& modestr = g_conf->get_val<string>(\"osd_tier_default_cache_mode\");\n    pg_pool_t::cache_mode_t mode = pg_pool_t::get_cache_mode_from_str(modestr);\n    if (mode < 0) {\n      ss << \"osd tier cache default mode '\" << modestr << \"' is not a valid cache mode\";\n      err = -EINVAL;\n      goto reply;\n    }\n    HitSet::Params hsp;\n    auto& cache_hit_set_type =\n      g_conf->get_val<string>(\"osd_tier_default_cache_hit_set_type\");\n    if (cache_hit_set_type == \"bloom\") {\n      BloomHitSet::Params *bsp = new BloomHitSet::Params;\n      bsp->set_fpp(g_conf->get_val<double>(\"osd_pool_default_hit_set_bloom_fpp\"));\n      hsp = HitSet::Params(bsp);\n    } else if (cache_hit_set_type == \"explicit_hash\") {\n      hsp = HitSet::Params(new ExplicitHashHitSet::Params);\n    } else if (cache_hit_set_type == \"explicit_object\") {\n      hsp = HitSet::Params(new ExplicitObjectHitSet::Params);\n    } else {\n      ss << \"osd tier cache default hit set type '\"\n\t << cache_hit_set_type << \"' is not a known type\";\n      err = -EINVAL;\n      goto reply;\n    }\n    // go\n    pg_pool_t *np = pending_inc.get_new_pool(pool_id, p);\n    pg_pool_t *ntp = pending_inc.get_new_pool(tierpool_id, tp);\n    if (np->tiers.count(tierpool_id) || ntp->is_tier()) {\n      wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n      return true;\n    }\n    np->tiers.insert(tierpool_id);\n    np->read_tier = np->write_tier = tierpool_id;\n    np->set_snap_epoch(pending_inc.epoch); // tier will update to our snap info\n    np->set_last_force_op_resend(pending_inc.epoch);\n    ntp->set_last_force_op_resend(pending_inc.epoch);\n    ntp->tier_of = pool_id;\n    ntp->cache_mode = mode;\n    ntp->hit_set_count = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_hit_set_count\");\n    ntp->hit_set_period = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_hit_set_period\");\n    ntp->min_read_recency_for_promote = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_min_read_recency_for_promote\");\n    ntp->min_write_recency_for_promote = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_min_write_recency_for_promote\");\n    ntp->hit_set_grade_decay_rate = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_hit_set_grade_decay_rate\");\n    ntp->hit_set_search_last_n = g_conf->get_val<uint64_t>(\"osd_tier_default_cache_hit_set_search_last_n\");\n    ntp->hit_set_params = hsp;\n    ntp->target_max_bytes = size;\n    ss << \"pool '\" << tierpoolstr << \"' is now (or already was) a cache tier of '\" << poolstr << \"'\";\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(),\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pool set-quota\") {\n    string poolstr;\n    cmd_getval(cct, cmdmap, \"pool\", poolstr);\n    int64_t pool_id = osdmap.lookup_pg_pool_name(poolstr);\n    if (pool_id < 0) {\n      ss << \"unrecognized pool '\" << poolstr << \"'\";\n      err = -ENOENT;\n      goto reply;\n    }\n\n    string field;\n    cmd_getval(cct, cmdmap, \"field\", field);\n    if (field != \"max_objects\" && field != \"max_bytes\") {\n      ss << \"unrecognized field '\" << field << \"'; should be 'max_bytes' or 'max_objects'\";\n      err = -EINVAL;\n      goto reply;\n    }\n\n    // val could contain unit designations, so we treat as a string\n    string val;\n    cmd_getval(cct, cmdmap, \"val\", val);\n    string tss;\n    int64_t value;\n    if (field == \"max_objects\") {\n      value = strict_sistrtoll(val.c_str(), &tss);\n    } else if (field == \"max_bytes\") {\n      value = strict_iecstrtoll(val.c_str(), &tss);\n    } else {\n      assert(0 == \"unrecognized option\");\n    }\n    if (!tss.empty()) {\n      ss << \"error parsing value '\" << val << \"': \" << tss;\n      err = -EINVAL;\n      goto reply;\n    }\n\n    pg_pool_t *pi = pending_inc.get_new_pool(pool_id, osdmap.get_pg_pool(pool_id));\n    if (field == \"max_objects\") {\n      pi->quota_max_objects = value;\n    } else if (field == \"max_bytes\") {\n      pi->quota_max_bytes = value;\n    } else {\n      assert(0 == \"unrecognized option\");\n    }\n    ss << \"set-quota \" << field << \" = \" << value << \" for pool \" << poolstr;\n    rs = ss.str();\n    wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t      get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd pool application enable\" ||\n             prefix == \"osd pool application disable\" ||\n             prefix == \"osd pool application set\" ||\n             prefix == \"osd pool application rm\") {\n    err = prepare_command_pool_application(prefix, cmdmap, ss);\n    if (err == -EAGAIN)\n      goto wait;\n    if (err < 0)\n      goto reply;\n\n    getline(ss, rs);\n    wait_for_finished_proposal(\n      op, new Monitor::C_Command(mon, op, 0, rs, get_last_committed() + 1));\n    return true;\n  } else if (prefix == \"osd force-create-pg\") {\n    pg_t pgid;\n    string pgidstr;\n    cmd_getval(cct, cmdmap, \"pgid\", pgidstr);\n    if (!pgid.parse(pgidstr.c_str())) {\n      ss << \"invalid pgid '\" << pgidstr << \"'\";\n      err = -EINVAL;\n      goto reply;\n    }\n    if (!osdmap.pg_exists(pgid)) {\n      ss << \"pg \" << pgid << \" should not exist\";\n      err = -ENOENT;\n      goto reply;\n    }\n    string sure;\n    cmd_getval(cct, cmdmap, \"sure\", sure);\n    if (sure != \"--yes-i-really-mean-it\") {\n      ss << \"This command will recreate a lost (as in data lost) PG with data in it, such that the cluster will give up ever trying to recover the lost data.  Do this only if you are certain that all copies of the PG are in fact lost and you are willing to accept that the data is permanently destroyed.  Pass --yes-i-really-mean-it to proceed.\";\n      err = -EPERM;\n      goto reply;\n    }\n    bool creating_now;\n    {\n      std::lock_guard<std::mutex> l(creating_pgs_lock);\n      auto emplaced = creating_pgs.pgs.emplace(pgid,\n\t\t\t\t\t       make_pair(osdmap.get_epoch(),\n\t\t\t\t\t\t\t ceph_clock_now()));\n      creating_now = emplaced.second;\n    }\n    if (creating_now) {\n      ss << \"pg \" << pgidstr << \" now creating, ok\";\n      err = 0;\n      goto update;\n    } else {\n      ss << \"pg \" << pgid << \" already creating\";\n      err = 0;\n      goto reply;\n    }\n  } else {\n    err = -EINVAL;\n  }\n\n reply:\n  getline(ss, rs);\n  if (err < 0 && rs.length() == 0)\n    rs = cpp_strerror(err);\n  mon->reply_command(op, err, rs, rdata, get_last_committed());\n  return ret;\n\n update:\n  getline(ss, rs);\n  wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, rs,\n\t\t\t\t\t    get_last_committed() + 1));\n  return true;\n\n wait:\n  wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n  return true;\n}\n\nbool OSDMonitor::enforce_pool_op_caps(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  MonSession *session = m->get_session();\n  if (!session) {\n    _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n    return true;\n  }\n\n  switch (m->op) {\n  case POOL_OP_CREATE_UNMANAGED_SNAP:\n  case POOL_OP_DELETE_UNMANAGED_SNAP:\n    {\n      const std::string* pool_name = nullptr;\n      const pg_pool_t *pg_pool = osdmap.get_pg_pool(m->pool);\n      if (pg_pool != nullptr) {\n        pool_name = &osdmap.get_pool_name(m->pool);\n      }\n\n      if (!is_unmanaged_snap_op_permitted(cct, mon->key_server,\n                                          session->entity_name, session->caps,\n                                          pool_name)) {\n        dout(0) << \"got unmanaged-snap pool op from entity with insufficient \"\n                << \"privileges. message: \" << *m  << std::endl\n                << \"caps: \" << session->caps << dendl;\n        _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n        return true;\n      }\n    }\n    break;\n  default:\n    if (!session->is_capable(\"osd\", MON_CAP_W)) {\n      dout(0) << \"got pool op from entity with insufficient privileges. \"\n              << \"message: \" << *m  << std::endl\n              << \"caps: \" << session->caps << dendl;\n      _pool_op_reply(op, -EPERM, osdmap.get_epoch());\n      return true;\n    }\n    break;\n  }\n\n  return false;\n}\n\nbool OSDMonitor::preprocess_pool_op(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n\n  if (enforce_pool_op_caps(op)) {\n    return true;\n  }\n\n  if (m->fsid != mon->monmap->fsid) {\n    dout(0) << __func__ << \" drop message on fsid \" << m->fsid\n            << \" != \" << mon->monmap->fsid << \" for \" << *m << dendl;\n    _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n    return true;\n  }\n\n  if (m->op == POOL_OP_CREATE)\n    return preprocess_pool_op_create(op);\n\n  const pg_pool_t *p = osdmap.get_pg_pool(m->pool);\n  if (p == nullptr) {\n    dout(10) << \"attempt to operate on non-existent pool id \" << m->pool << dendl;\n    if (m->op == POOL_OP_DELETE) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n    } else {\n      _pool_op_reply(op, -ENOENT, osdmap.get_epoch());\n    }\n    return true;\n  }\n\n  // check if the snap and snapname exist\n  bool snap_exists = false;\n  if (p->snap_exists(m->name.c_str()))\n    snap_exists = true;\n\n  switch (m->op) {\n  case POOL_OP_CREATE_SNAP:\n    if (p->is_unmanaged_snaps_mode() || p->is_tier()) {\n      _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n      return true;\n    }\n    if (snap_exists) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_CREATE_UNMANAGED_SNAP:\n    if (p->is_pool_snaps_mode()) {\n      _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_DELETE_SNAP:\n    if (p->is_unmanaged_snaps_mode()) {\n      _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n      return true;\n    }\n    if (!snap_exists) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_DELETE_UNMANAGED_SNAP:\n    if (p->is_pool_snaps_mode()) {\n      _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n      return true;\n    }\n    if (p->is_removed_snap(m->snapid)) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_DELETE:\n    if (osdmap.lookup_pg_pool_name(m->name.c_str()) >= 0) {\n      _pool_op_reply(op, 0, osdmap.get_epoch());\n      return true;\n    }\n    return false;\n  case POOL_OP_AUID_CHANGE:\n    return false;\n  default:\n    ceph_abort();\n    break;\n  }\n\n  return false;\n}\n\nbool OSDMonitor::preprocess_pool_op_create(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  int64_t pool = osdmap.lookup_pg_pool_name(m->name.c_str());\n  if (pool >= 0) {\n    _pool_op_reply(op, 0, osdmap.get_epoch());\n    return true;\n  }\n\n  return false;\n}\n\nbool OSDMonitor::prepare_pool_op(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  dout(10) << \"prepare_pool_op \" << *m << dendl;\n  if (m->op == POOL_OP_CREATE) {\n    return prepare_pool_op_create(op);\n  } else if (m->op == POOL_OP_DELETE) {\n    return prepare_pool_op_delete(op);\n  }\n\n  int ret = 0;\n  bool changed = false;\n\n  if (!osdmap.have_pg_pool(m->pool)) {\n    _pool_op_reply(op, -ENOENT, osdmap.get_epoch());\n    return false;\n  }\n\n  const pg_pool_t *pool = osdmap.get_pg_pool(m->pool);\n\n  switch (m->op) {\n    case POOL_OP_CREATE_SNAP:\n      if (pool->is_tier()) {\n        ret = -EINVAL;\n        _pool_op_reply(op, ret, osdmap.get_epoch());\n        return false;\n      }  // else, fall through\n    case POOL_OP_DELETE_SNAP:\n      if (!pool->is_unmanaged_snaps_mode()) {\n        bool snap_exists = pool->snap_exists(m->name.c_str());\n        if ((m->op == POOL_OP_CREATE_SNAP && snap_exists)\n          || (m->op == POOL_OP_DELETE_SNAP && !snap_exists)) {\n          ret = 0;\n        } else {\n          break;\n        }\n      } else {\n        ret = -EINVAL;\n      }\n      _pool_op_reply(op, ret, osdmap.get_epoch());\n      return false;\n\n    case POOL_OP_DELETE_UNMANAGED_SNAP:\n      // we won't allow removal of an unmanaged snapshot from a pool\n      // not in unmanaged snaps mode.\n      if (!pool->is_unmanaged_snaps_mode()) {\n        _pool_op_reply(op, -ENOTSUP, osdmap.get_epoch());\n        return false;\n      }\n      /* fall-thru */\n    case POOL_OP_CREATE_UNMANAGED_SNAP:\n      // but we will allow creating an unmanaged snapshot on any pool\n      // as long as it is not in 'pool' snaps mode.\n      if (pool->is_pool_snaps_mode()) {\n        _pool_op_reply(op, -EINVAL, osdmap.get_epoch());\n        return false;\n      }\n  }\n\n  // projected pool info\n  pg_pool_t pp;\n  if (pending_inc.new_pools.count(m->pool))\n    pp = pending_inc.new_pools[m->pool];\n  else\n    pp = *osdmap.get_pg_pool(m->pool);\n\n  bufferlist reply_data;\n\n  // pool snaps vs unmanaged snaps are mutually exclusive\n  switch (m->op) {\n  case POOL_OP_CREATE_SNAP:\n  case POOL_OP_DELETE_SNAP:\n    if (pp.is_unmanaged_snaps_mode()) {\n      ret = -EINVAL;\n      goto out;\n    }\n    break;\n\n  case POOL_OP_CREATE_UNMANAGED_SNAP:\n  case POOL_OP_DELETE_UNMANAGED_SNAP:\n    if (pp.is_pool_snaps_mode()) {\n      ret = -EINVAL;\n      goto out;\n    }\n  }\n\n  switch (m->op) {\n  case POOL_OP_CREATE_SNAP:\n    if (!pp.snap_exists(m->name.c_str())) {\n      pp.add_snap(m->name.c_str(), ceph_clock_now());\n      dout(10) << \"create snap in pool \" << m->pool << \" \" << m->name\n\t       << \" seq \" << pp.get_snap_epoch() << dendl;\n      changed = true;\n    }\n    break;\n\n  case POOL_OP_DELETE_SNAP:\n    {\n      snapid_t s = pp.snap_exists(m->name.c_str());\n      if (s) {\n\tpp.remove_snap(s);\n\tpending_inc.new_removed_snaps[m->pool].insert(s);\n\tchanged = true;\n      }\n    }\n    break;\n\n  case POOL_OP_CREATE_UNMANAGED_SNAP:\n    {\n      uint64_t snapid;\n      pp.add_unmanaged_snap(snapid);\n      encode(snapid, reply_data);\n      changed = true;\n    }\n    break;\n\n  case POOL_OP_DELETE_UNMANAGED_SNAP:\n    if (!pp.is_removed_snap(m->snapid)) {\n      if (m->snapid > pp.get_snap_seq()) {\n        _pool_op_reply(op, -ENOENT, osdmap.get_epoch());\n        return false;\n      }\n      pp.remove_unmanaged_snap(m->snapid);\n      pending_inc.new_removed_snaps[m->pool].insert(m->snapid);\n      changed = true;\n    }\n    break;\n\n  case POOL_OP_AUID_CHANGE:\n    if (pp.auid != m->auid) {\n      pp.auid = m->auid;\n      changed = true;\n    }\n    break;\n\n  default:\n    ceph_abort();\n    break;\n  }\n\n  if (changed) {\n    pp.set_snap_epoch(pending_inc.epoch);\n    pending_inc.new_pools[m->pool] = pp;\n  }\n\n out:\n  wait_for_finished_proposal(op, new OSDMonitor::C_PoolOp(this, op, ret, pending_inc.epoch, &reply_data));\n  return true;\n}\n\nbool OSDMonitor::prepare_pool_op_create(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  int err = prepare_new_pool(op);\n  wait_for_finished_proposal(op, new OSDMonitor::C_PoolOp(this, op, err, pending_inc.epoch));\n  return true;\n}\n\nint OSDMonitor::_check_remove_pool(int64_t pool_id, const pg_pool_t& pool,\n\t\t\t\t   ostream *ss)\n{\n  const string& poolstr = osdmap.get_pool_name(pool_id);\n\n  // If the Pool is in use by CephFS, refuse to delete it\n  FSMap const &pending_fsmap = mon->mdsmon()->get_pending_fsmap();\n  if (pending_fsmap.pool_in_use(pool_id)) {\n    *ss << \"pool '\" << poolstr << \"' is in use by CephFS\";\n    return -EBUSY;\n  }\n\n  if (pool.tier_of >= 0) {\n    *ss << \"pool '\" << poolstr << \"' is a tier of '\"\n\t<< osdmap.get_pool_name(pool.tier_of) << \"'\";\n    return -EBUSY;\n  }\n  if (!pool.tiers.empty()) {\n    *ss << \"pool '\" << poolstr << \"' has tiers\";\n    for(auto tier : pool.tiers) {\n      *ss << \" \" << osdmap.get_pool_name(tier);\n    }\n    return -EBUSY;\n  }\n\n  if (!g_conf->mon_allow_pool_delete) {\n    *ss << \"pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool\";\n    return -EPERM;\n  }\n\n  if (pool.has_flag(pg_pool_t::FLAG_NODELETE)) {\n    *ss << \"pool deletion is disabled; you must unset nodelete flag for the pool first\";\n    return -EPERM;\n  }\n\n  *ss << \"pool '\" << poolstr << \"' removed\";\n  return 0;\n}\n\n/**\n * Check if it is safe to add a tier to a base pool\n *\n * @return\n * True if the operation should proceed, false if we should abort here\n * (abort doesn't necessarily mean error, could be idempotency)\n */\nbool OSDMonitor::_check_become_tier(\n    const int64_t tier_pool_id, const pg_pool_t *tier_pool,\n    const int64_t base_pool_id, const pg_pool_t *base_pool,\n    int *err,\n    ostream *ss) const\n{\n  const std::string &tier_pool_name = osdmap.get_pool_name(tier_pool_id);\n  const std::string &base_pool_name = osdmap.get_pool_name(base_pool_id);\n\n  const FSMap &pending_fsmap = mon->mdsmon()->get_pending_fsmap();\n  if (pending_fsmap.pool_in_use(tier_pool_id)) {\n    *ss << \"pool '\" << tier_pool_name << \"' is in use by CephFS\";\n    *err = -EBUSY;\n    return false;\n  }\n\n  if (base_pool->tiers.count(tier_pool_id)) {\n    assert(tier_pool->tier_of == base_pool_id);\n    *err = 0;\n    *ss << \"pool '\" << tier_pool_name << \"' is now (or already was) a tier of '\"\n      << base_pool_name << \"'\";\n    return false;\n  }\n\n  if (base_pool->is_tier()) {\n    *ss << \"pool '\" << base_pool_name << \"' is already a tier of '\"\n      << osdmap.get_pool_name(base_pool->tier_of) << \"', \"\n      << \"multiple tiers are not yet supported.\";\n    *err = -EINVAL;\n    return false;\n  }\n\n  if (tier_pool->has_tiers()) {\n    *ss << \"pool '\" << tier_pool_name << \"' has following tier(s) already:\";\n    for (set<uint64_t>::iterator it = tier_pool->tiers.begin();\n         it != tier_pool->tiers.end(); ++it)\n      *ss << \"'\" << osdmap.get_pool_name(*it) << \"',\";\n    *ss << \" multiple tiers are not yet supported.\";\n    *err = -EINVAL;\n    return false;\n  }\n\n  if (tier_pool->is_tier()) {\n    *ss << \"tier pool '\" << tier_pool_name << \"' is already a tier of '\"\n       << osdmap.get_pool_name(tier_pool->tier_of) << \"'\";\n    *err = -EINVAL;\n    return false;\n  }\n\n  *err = 0;\n  return true;\n}\n\n\n/**\n * Check if it is safe to remove a tier from this base pool\n *\n * @return\n * True if the operation should proceed, false if we should abort here\n * (abort doesn't necessarily mean error, could be idempotency)\n */\nbool OSDMonitor::_check_remove_tier(\n    const int64_t base_pool_id, const pg_pool_t *base_pool,\n    const pg_pool_t *tier_pool,\n    int *err, ostream *ss) const\n{\n  const std::string &base_pool_name = osdmap.get_pool_name(base_pool_id);\n\n  // Apply CephFS-specific checks\n  const FSMap &pending_fsmap = mon->mdsmon()->get_pending_fsmap();\n  if (pending_fsmap.pool_in_use(base_pool_id)) {\n    if (base_pool->is_erasure() && !base_pool->allows_ecoverwrites()) {\n      // If the underlying pool is erasure coded and does not allow EC\n      // overwrites, we can't permit the removal of the replicated tier that\n      // CephFS relies on to access it\n      *ss << \"pool '\" << base_pool_name <<\n          \"' does not allow EC overwrites and is in use by CephFS\"\n          \" via its tier\";\n      *err = -EBUSY;\n      return false;\n    }\n\n    if (tier_pool && tier_pool->cache_mode == pg_pool_t::CACHEMODE_WRITEBACK) {\n      *ss << \"pool '\" << base_pool_name << \"' is in use by CephFS, and this \"\n             \"tier is still in use as a writeback cache.  Change the cache \"\n             \"mode and flush the cache before removing it\";\n      *err = -EBUSY;\n      return false;\n    }\n  }\n\n  *err = 0;\n  return true;\n}\n\nint OSDMonitor::_prepare_remove_pool(\n  int64_t pool, ostream *ss, bool no_fake)\n{\n  dout(10) << __func__ << \" \" << pool << dendl;\n  const pg_pool_t *p = osdmap.get_pg_pool(pool);\n  int r = _check_remove_pool(pool, *p, ss);\n  if (r < 0)\n    return r;\n\n  auto new_pool = pending_inc.new_pools.find(pool);\n  if (new_pool != pending_inc.new_pools.end()) {\n    // if there is a problem with the pending info, wait and retry\n    // this op.\n    const auto& p = new_pool->second;\n    int r = _check_remove_pool(pool, p, ss);\n    if (r < 0)\n      return -EAGAIN;\n  }\n\n  if (pending_inc.old_pools.count(pool)) {\n    dout(10) << __func__ << \" \" << pool << \" already pending removal\"\n\t     << dendl;\n    return 0;\n  }\n\n  if (g_conf->mon_fake_pool_delete && !no_fake) {\n    string old_name = osdmap.get_pool_name(pool);\n    string new_name = old_name + \".\" + stringify(pool) + \".DELETED\";\n    dout(1) << __func__ << \" faking pool deletion: renaming \" << pool << \" \"\n\t    << old_name << \" -> \" << new_name << dendl;\n    pending_inc.new_pool_names[pool] = new_name;\n    return 0;\n  }\n\n  // remove\n  pending_inc.old_pools.insert(pool);\n\n  // remove any pg_temp mappings for this pool\n  for (auto p = osdmap.pg_temp->begin();\n       p != osdmap.pg_temp->end();\n       ++p) {\n    if (p->first.pool() == pool) {\n      dout(10) << __func__ << \" \" << pool << \" removing obsolete pg_temp \"\n\t       << p->first << dendl;\n      pending_inc.new_pg_temp[p->first].clear();\n    }\n  }\n  // remove any primary_temp mappings for this pool\n  for (auto p = osdmap.primary_temp->begin();\n      p != osdmap.primary_temp->end();\n      ++p) {\n    if (p->first.pool() == pool) {\n      dout(10) << __func__ << \" \" << pool\n               << \" removing obsolete primary_temp\" << p->first << dendl;\n      pending_inc.new_primary_temp[p->first] = -1;\n    }\n  }\n  // remove any pg_upmap mappings for this pool\n  for (auto& p : osdmap.pg_upmap) {\n    if (p.first.pool() == pool) {\n      dout(10) << __func__ << \" \" << pool\n               << \" removing obsolete pg_upmap \"\n               << p.first << dendl;\n      pending_inc.old_pg_upmap.insert(p.first);\n    }\n  }\n  // remove any pending pg_upmap mappings for this pool\n  {\n    auto it = pending_inc.new_pg_upmap.begin();\n    while (it != pending_inc.new_pg_upmap.end()) {\n      if (it->first.pool() == pool) {\n        dout(10) << __func__ << \" \" << pool\n                 << \" removing pending pg_upmap \"\n                 << it->first << dendl;\n        it = pending_inc.new_pg_upmap.erase(it);\n      } else {\n        it++;\n      }\n    }\n  }\n  // remove any pg_upmap_items mappings for this pool\n  for (auto& p : osdmap.pg_upmap_items) {\n    if (p.first.pool() == pool) {\n      dout(10) << __func__ << \" \" << pool\n               << \" removing obsolete pg_upmap_items \" << p.first\n               << dendl;\n      pending_inc.old_pg_upmap_items.insert(p.first);\n    }\n  }\n  // remove any pending pg_upmap mappings for this pool\n  {\n    auto it = pending_inc.new_pg_upmap_items.begin();\n    while (it != pending_inc.new_pg_upmap_items.end()) {\n      if (it->first.pool() == pool) {\n        dout(10) << __func__ << \" \" << pool\n                 << \" removing pending pg_upmap_items \"\n                 << it->first << dendl;\n        it = pending_inc.new_pg_upmap_items.erase(it);\n      } else {\n        it++;\n      }\n    }\n  }\n\n  // remove any choose_args for this pool\n  CrushWrapper newcrush;\n  _get_pending_crush(newcrush);\n  if (newcrush.have_choose_args(pool)) {\n    dout(10) << __func__ << \" removing choose_args for pool \" << pool << dendl;\n    newcrush.rm_choose_args(pool);\n    pending_inc.crush.clear();\n    newcrush.encode(pending_inc.crush, mon->get_quorum_con_features());\n  }\n  return 0;\n}\n\nint OSDMonitor::_prepare_rename_pool(int64_t pool, string newname)\n{\n  dout(10) << \"_prepare_rename_pool \" << pool << dendl;\n  if (pending_inc.old_pools.count(pool)) {\n    dout(10) << \"_prepare_rename_pool \" << pool << \" pending removal\" << dendl;\n    return -ENOENT;\n  }\n  for (map<int64_t,string>::iterator p = pending_inc.new_pool_names.begin();\n       p != pending_inc.new_pool_names.end();\n       ++p) {\n    if (p->second == newname && p->first != pool) {\n      return -EEXIST;\n    }\n  }\n\n  pending_inc.new_pool_names[pool] = newname;\n  return 0;\n}\n\nbool OSDMonitor::prepare_pool_op_delete(MonOpRequestRef op)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  ostringstream ss;\n  int ret = _prepare_remove_pool(m->pool, &ss, false);\n  if (ret == -EAGAIN) {\n    wait_for_finished_proposal(op, new C_RetryMessage(this, op));\n    return true;\n  }\n  if (ret < 0)\n    dout(10) << __func__ << \" got \" << ret << \" \" << ss.str() << dendl;\n  wait_for_finished_proposal(op, new OSDMonitor::C_PoolOp(this, op, ret,\n\t\t\t\t\t\t      pending_inc.epoch));\n  return true;\n}\n\nvoid OSDMonitor::_pool_op_reply(MonOpRequestRef op,\n                                int ret, epoch_t epoch, bufferlist *blp)\n{\n  op->mark_osdmon_event(__func__);\n  MPoolOp *m = static_cast<MPoolOp*>(op->get_req());\n  dout(20) << \"_pool_op_reply \" << ret << dendl;\n  MPoolOpReply *reply = new MPoolOpReply(m->fsid, m->get_tid(),\n\t\t\t\t\t ret, epoch, get_last_committed(), blp);\n  mon->send_reply(op, reply);\n}\n", "// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- \n// vim: ts=8 sw=2 smarttab\n/*\n * Ceph - scalable distributed file system\n *\n * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>\n * Copyright (C) 2013,2014 Cloudwatt <libre.licensing@cloudwatt.com>\n *\n * Author: Loic Dachary <loic@dachary.org>\n *\n * This is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License version 2.1, as published by the Free Software \n * Foundation.  See file COPYING.\n * \n */\n\n/* Object Store Device (OSD) Monitor\n */\n\n#ifndef CEPH_OSDMONITOR_H\n#define CEPH_OSDMONITOR_H\n\n#include <map>\n#include <set>\n\n#include \"include/types.h\"\n#include \"include/encoding.h\"\n#include \"common/simple_cache.hpp\"\n#include \"msg/Messenger.h\"\n\n#include \"osd/OSDMap.h\"\n#include \"osd/OSDMapMapping.h\"\n\n#include \"CreatingPGs.h\"\n#include \"PaxosService.h\"\n\nclass Monitor;\nclass PGMap;\nclass MonSession;\nclass MOSDMap;\n\n#include \"erasure-code/ErasureCodeInterface.h\"\n#include \"mon/MonOpRequest.h\"\n#include <boost/functional/hash.hpp>\n// re-include our assert to clobber the system one; fix dout:\n#include \"include/assert.h\"\n\n/// information about a particular peer's failure reports for one osd\nstruct failure_reporter_t {\n  utime_t failed_since;     ///< when they think it failed\n  MonOpRequestRef op;       ///< failure op request\n\n  failure_reporter_t() {}\n  explicit failure_reporter_t(utime_t s) : failed_since(s) {}\n  ~failure_reporter_t() { }\n};\n\n/// information about all failure reports for one osd\nstruct failure_info_t {\n  map<int, failure_reporter_t> reporters;  ///< reporter -> failed_since etc\n  utime_t max_failed_since;                ///< most recent failed_since\n\n  failure_info_t() {}\n\n  utime_t get_failed_since() {\n    if (max_failed_since == utime_t() && !reporters.empty()) {\n      // the old max must have canceled; recalculate.\n      for (map<int, failure_reporter_t>::iterator p = reporters.begin();\n\t   p != reporters.end();\n\t   ++p)\n\tif (p->second.failed_since > max_failed_since)\n\t  max_failed_since = p->second.failed_since;\n    }\n    return max_failed_since;\n  }\n\n  // set the message for the latest report.  return any old op request we had,\n  // if any, so we can discard it.\n  MonOpRequestRef add_report(int who, utime_t failed_since,\n\t\t\t     MonOpRequestRef op) {\n    map<int, failure_reporter_t>::iterator p = reporters.find(who);\n    if (p == reporters.end()) {\n      if (max_failed_since < failed_since)\n\tmax_failed_since = failed_since;\n      p = reporters.insert(map<int, failure_reporter_t>::value_type(who, failure_reporter_t(failed_since))).first;\n    }\n\n    MonOpRequestRef ret = p->second.op;\n    p->second.op = op;\n    return ret;\n  }\n\n  void take_report_messages(list<MonOpRequestRef>& ls) {\n    for (map<int, failure_reporter_t>::iterator p = reporters.begin();\n\t p != reporters.end();\n\t ++p) {\n      if (p->second.op) {\n\tls.push_back(p->second.op);\n        p->second.op.reset();\n      }\n    }\n  }\n\n  MonOpRequestRef cancel_report(int who) {\n    map<int, failure_reporter_t>::iterator p = reporters.find(who);\n    if (p == reporters.end())\n      return MonOpRequestRef();\n    MonOpRequestRef ret = p->second.op;\n    reporters.erase(p);\n    return ret;\n  }\n};\n\n\nclass LastEpochClean {\n  struct Lec {\n    vector<epoch_t> epoch_by_pg;\n    ps_t next_missing = 0;\n    epoch_t floor = std::numeric_limits<epoch_t>::max();\n    void report(ps_t pg, epoch_t last_epoch_clean);\n  };\n  std::map<uint64_t, Lec> report_by_pool;\npublic:\n  void report(const pg_t& pg, epoch_t last_epoch_clean);\n  void remove_pool(uint64_t pool);\n  epoch_t get_lower_bound(const OSDMap& latest) const;\n};\n\n\nstruct osdmap_manifest_t {\n  // all the maps we have pinned -- i.e., won't be removed unless\n  // they are inside a trim interval.\n  set<version_t> pinned;\n\n  osdmap_manifest_t() {}\n\n  version_t get_last_pinned() const\n  {\n    set<version_t>::const_reverse_iterator it = pinned.crbegin();\n    if (it == pinned.crend()) {\n      return 0;\n    }\n    return *it;\n  }\n\n  version_t get_first_pinned() const\n  {\n    set<version_t>::const_iterator it = pinned.cbegin();\n    if (it == pinned.cend()) {\n      return 0;\n    }\n    return *it;\n  }\n\n  bool is_pinned(version_t v) const\n  {\n    return pinned.find(v) != pinned.end();\n  }\n\n  void pin(version_t v)\n  {\n    pinned.insert(v);\n  }\n\n  version_t get_lower_closest_pinned(version_t v) const {\n    set<version_t>::const_iterator p = pinned.lower_bound(v);\n    if (p == pinned.cend()) {\n      return 0;\n    } else if (*p > v) {\n      if (p == pinned.cbegin()) {\n        return 0;\n      }\n      --p;\n    }\n    return *p;\n  }\n\n  void encode(bufferlist& bl) const\n  {\n    ENCODE_START(1, 1, bl);\n    encode(pinned, bl);\n    ENCODE_FINISH(bl);\n  }\n\n  void decode(bufferlist::const_iterator& bl)\n  {\n    DECODE_START(1, bl);\n    decode(pinned, bl);\n    DECODE_FINISH(bl);\n  }\n\n  void decode(bufferlist& bl) {\n    auto p = bl.cbegin();\n    decode(p);\n  }\n\n  void dump(Formatter *f) {\n    f->dump_unsigned(\"first_pinned\", get_first_pinned());\n    f->dump_unsigned(\"last_pinned\", get_last_pinned());\n    f->open_array_section(\"pinned_maps\");\n    for (auto& i : pinned) {\n      f->dump_unsigned(\"epoch\", i);\n    }\n    f->close_section();\n }\n};\nWRITE_CLASS_ENCODER(osdmap_manifest_t);\n\nclass OSDMonitor : public PaxosService {\n  CephContext *cct;\n\npublic:\n  OSDMap osdmap;\n\n  // [leader]\n  OSDMap::Incremental pending_inc;\n  map<int, bufferlist> pending_metadata;\n  set<int>             pending_metadata_rm;\n  map<int, failure_info_t> failure_info;\n  map<int,utime_t>    down_pending_out;  // osd down -> out\n\n  map<int,double> osd_weight;\n\n  using osdmap_key_t = std::pair<version_t, uint64_t>;\n  using osdmap_cache_t = SimpleLRU<osdmap_key_t,\n                                   bufferlist,\n                                   std::less<osdmap_key_t>,\n                                   boost::hash<osdmap_key_t>>;\n  osdmap_cache_t inc_osd_cache;\n  osdmap_cache_t full_osd_cache;\n\n  bool has_osdmap_manifest;\n  osdmap_manifest_t osdmap_manifest;\n\n  bool check_failures(utime_t now);\n  bool check_failure(utime_t now, int target_osd, failure_info_t& fi);\n  void force_failure(int target_osd, int by);\n\n  bool _have_pending_crush();\n  CrushWrapper &_get_stable_crush();\n  void _get_pending_crush(CrushWrapper& newcrush);\n\n  enum FastReadType {\n    FAST_READ_OFF,\n    FAST_READ_ON,\n    FAST_READ_DEFAULT\n  };\n\n  // svc\npublic:\n  void create_initial() override;\n  void get_store_prefixes(std::set<string>& s) const override;\n\nprivate:\n  void update_from_paxos(bool *need_bootstrap) override;\n  void create_pending() override;  // prepare a new pending\n  void encode_pending(MonitorDBStore::TransactionRef t) override;\n  void on_active() override;\n  void on_restart() override;\n  void on_shutdown() override;\n\n  /* osdmap full map prune */\n  void load_osdmap_manifest();\n  bool should_prune() const;\n  void _prune_update_trimmed(\n      MonitorDBStore::TransactionRef tx,\n      version_t first);\n  void prune_init();\n  bool _prune_sanitize_options() const;\n  bool is_prune_enabled() const;\n  bool is_prune_supported() const;\n  bool do_prune(MonitorDBStore::TransactionRef tx);\n\n  /**\n   * we haven't delegated full version stashing to paxosservice for some time\n   * now, making this function useless in current context.\n   */\n  void encode_full(MonitorDBStore::TransactionRef t) override { }\n  /**\n   * do not let paxosservice periodically stash full osdmaps, or we will break our\n   * locally-managed full maps.  (update_from_paxos loads the latest and writes them\n   * out going forward from there, but if we just synced that may mean we skip some.)\n   */\n  bool should_stash_full() override {\n    return false;\n  }\n\n  /**\n   * hook into trim to include the oldest full map in the trim transaction\n   *\n   * This ensures that anyone post-sync will have enough to rebuild their\n   * full osdmaps.\n   */\n  void encode_trim_extra(MonitorDBStore::TransactionRef tx, version_t first) override;\n\n  void update_msgr_features();\n  int check_cluster_features(uint64_t features, stringstream &ss);\n  /**\n   * check if the cluster supports the features required by the\n   * given crush map. Outputs the daemons which don't support it\n   * to the stringstream.\n   *\n   * @returns true if the map is passable, false otherwise\n   */\n  bool validate_crush_against_features(const CrushWrapper *newcrush,\n                                      stringstream &ss);\n  void check_osdmap_subs();\n  void share_map_with_random_osd();\n\n  Mutex prime_pg_temp_lock = {\"OSDMonitor::prime_pg_temp_lock\"};\n  struct PrimeTempJob : public ParallelPGMapper::Job {\n    OSDMonitor *osdmon;\n    PrimeTempJob(const OSDMap& om, OSDMonitor *m)\n      : ParallelPGMapper::Job(&om), osdmon(m) {}\n    void process(int64_t pool, unsigned ps_begin, unsigned ps_end) override {\n      for (unsigned ps = ps_begin; ps < ps_end; ++ps) {\n\tpg_t pgid(ps, pool);\n\tosdmon->prime_pg_temp(*osdmap, pgid);\n      }\n    }\n    void complete() override {}\n  };\n  void maybe_prime_pg_temp();\n  void prime_pg_temp(const OSDMap& next, pg_t pgid);\n\n  ParallelPGMapper mapper;                        ///< for background pg work\n  OSDMapMapping mapping;                          ///< pg <-> osd mappings\n  unique_ptr<ParallelPGMapper::Job> mapping_job;  ///< background mapping job\n  void start_mapping();\n\n  void update_logger();\n\n  void handle_query(PaxosServiceMessage *m);\n  bool preprocess_query(MonOpRequestRef op) override;  // true if processed.\n  bool prepare_update(MonOpRequestRef op) override;\n  bool should_propose(double &delay) override;\n\n  version_t get_trim_to() const override;\n\n  bool can_mark_down(int o);\n  bool can_mark_up(int o);\n  bool can_mark_out(int o);\n  bool can_mark_in(int o);\n\n  // ...\n  MOSDMap *build_latest_full(uint64_t features);\n  MOSDMap *build_incremental(epoch_t first, epoch_t last, uint64_t features);\n  void send_full(MonOpRequestRef op);\n  void send_incremental(MonOpRequestRef op, epoch_t first);\npublic:\n  // @param req an optional op request, if the osdmaps are replies to it. so\n  //            @c Monitor::send_reply() can mark_event with it.\n  void send_incremental(epoch_t first, MonSession *session, bool onetime,\n\t\t\tMonOpRequestRef req = MonOpRequestRef());\n\nprivate:\n  void print_utilization(ostream &out, Formatter *f, bool tree) const;\n\n  bool check_source(PaxosServiceMessage *m, uuid_d fsid);\n \n  bool preprocess_get_osdmap(MonOpRequestRef op);\n\n  bool preprocess_mark_me_down(MonOpRequestRef op);\n\n  friend class C_AckMarkedDown;\n  bool preprocess_failure(MonOpRequestRef op);\n  bool prepare_failure(MonOpRequestRef op);\n  bool prepare_mark_me_down(MonOpRequestRef op);\n  void process_failures();\n  void take_all_failures(list<MonOpRequestRef>& ls);\n\n  bool preprocess_full(MonOpRequestRef op);\n  bool prepare_full(MonOpRequestRef op);\n\n  bool preprocess_boot(MonOpRequestRef op);\n  bool prepare_boot(MonOpRequestRef op);\n  void _booted(MonOpRequestRef op, bool logit);\n\n  void update_up_thru(int from, epoch_t up_thru);\n  bool preprocess_alive(MonOpRequestRef op);\n  bool prepare_alive(MonOpRequestRef op);\n  void _reply_map(MonOpRequestRef op, epoch_t e);\n\n  bool preprocess_pgtemp(MonOpRequestRef op);\n  bool prepare_pgtemp(MonOpRequestRef op);\n\n  bool preprocess_pg_created(MonOpRequestRef op);\n  bool prepare_pg_created(MonOpRequestRef op);\n\n  int _check_remove_pool(int64_t pool_id, const pg_pool_t &pool, ostream *ss);\n  bool _check_become_tier(\n      int64_t tier_pool_id, const pg_pool_t *tier_pool,\n      int64_t base_pool_id, const pg_pool_t *base_pool,\n      int *err, ostream *ss) const;\n  bool _check_remove_tier(\n      int64_t base_pool_id, const pg_pool_t *base_pool, const pg_pool_t *tier_pool,\n      int *err, ostream *ss) const;\n\n  int _prepare_remove_pool(int64_t pool, ostream *ss, bool no_fake);\n  int _prepare_rename_pool(int64_t pool, string newname);\n\n  bool enforce_pool_op_caps(MonOpRequestRef op);\n  bool preprocess_pool_op (MonOpRequestRef op);\n  bool preprocess_pool_op_create (MonOpRequestRef op);\n  bool prepare_pool_op (MonOpRequestRef op);\n  bool prepare_pool_op_create (MonOpRequestRef op);\n  bool prepare_pool_op_delete(MonOpRequestRef op);\n  int crush_rename_bucket(const string& srcname,\n\t\t\t  const string& dstname,\n\t\t\t  ostream *ss);\n  void check_legacy_ec_plugin(const string& plugin, \n\t\t\t      const string& profile) const;\n  int normalize_profile(const string& profilename, \n\t\t\tErasureCodeProfile &profile,\n\t\t\tbool force,\n\t\t\tostream *ss);\n  int crush_rule_create_erasure(const string &name,\n\t\t\t\tconst string &profile,\n\t\t\t\tint *rule,\n\t\t\t\tostream *ss);\n  int get_crush_rule(const string &rule_name,\n\t\t\tint *crush_rule,\n\t\t\tostream *ss);\n  int get_erasure_code(const string &erasure_code_profile,\n\t\t       ErasureCodeInterfaceRef *erasure_code,\n\t\t       ostream *ss) const;\n  int prepare_pool_crush_rule(const unsigned pool_type,\n\t\t\t\t const string &erasure_code_profile,\n\t\t\t\t const string &rule_name,\n\t\t\t\t int *crush_rule,\n\t\t\t\t ostream *ss);\n  bool erasure_code_profile_in_use(\n    const mempool::osdmap::map<int64_t, pg_pool_t> &pools,\n    const string &profile,\n    ostream *ss);\n  int parse_erasure_code_profile(const vector<string> &erasure_code_profile,\n\t\t\t\t map<string,string> *erasure_code_profile_map,\n\t\t\t\t ostream *ss);\n  int prepare_pool_size(const unsigned pool_type,\n\t\t\tconst string &erasure_code_profile,\n\t\t\tunsigned *size, unsigned *min_size,\n\t\t\tostream *ss);\n  int prepare_pool_stripe_width(const unsigned pool_type,\n\t\t\t\tconst string &erasure_code_profile,\n\t\t\t\tunsigned *stripe_width,\n\t\t\t\tostream *ss);\n  int check_pg_num(int64_t pool, int pg_num, int size, ostream* ss);\n  int prepare_new_pool(string& name, uint64_t auid,\n\t\t       int crush_rule,\n\t\t       const string &crush_rule_name,\n                       unsigned pg_num, unsigned pgp_num,\n\t\t       const string &erasure_code_profile,\n                       const unsigned pool_type,\n                       const uint64_t expected_num_objects,\n                       FastReadType fast_read,\n\t\t       ostream *ss);\n  int prepare_new_pool(MonOpRequestRef op);\n\n  void set_pool_flags(int64_t pool_id, uint64_t flags);\n  void clear_pool_flags(int64_t pool_id, uint64_t flags);\n  bool update_pools_status();\n\n  string make_snap_epoch_key(int64_t pool, epoch_t epoch);\n  string make_snap_key(int64_t pool, snapid_t snap);\n  string make_snap_key_value(int64_t pool, snapid_t snap, snapid_t num,\n\t\t\t     epoch_t epoch, bufferlist *v);\n  string make_snap_purged_key(int64_t pool, snapid_t snap);\n  string make_snap_purged_key_value(int64_t pool, snapid_t snap, snapid_t num,\n\t\t\t\t    epoch_t epoch, bufferlist *v);\n  bool try_prune_purged_snaps();\n  int lookup_pruned_snap(int64_t pool, snapid_t snap,\n\t\t\t snapid_t *begin, snapid_t *end);\n\n  bool prepare_set_flag(MonOpRequestRef op, int flag);\n  bool prepare_unset_flag(MonOpRequestRef op, int flag);\n\n  void _pool_op_reply(MonOpRequestRef op,\n                      int ret, epoch_t epoch, bufferlist *blp=NULL);\n\n  struct C_Booted : public C_MonOp {\n    OSDMonitor *cmon;\n    bool logit;\n    C_Booted(OSDMonitor *cm, MonOpRequestRef op_, bool l=true) :\n      C_MonOp(op_), cmon(cm), logit(l) {}\n    void _finish(int r) override {\n      if (r >= 0)\n\tcmon->_booted(op, logit);\n      else if (r == -ECANCELED)\n        return;\n      else if (r == -EAGAIN)\n        cmon->dispatch(op);\n      else\n\tassert(0 == \"bad C_Booted return value\");\n    }\n  };\n\n  struct C_ReplyMap : public C_MonOp {\n    OSDMonitor *osdmon;\n    epoch_t e;\n    C_ReplyMap(OSDMonitor *o, MonOpRequestRef op_, epoch_t ee)\n      : C_MonOp(op_), osdmon(o), e(ee) {}\n    void _finish(int r) override {\n      if (r >= 0)\n\tosdmon->_reply_map(op, e);\n      else if (r == -ECANCELED)\n        return;\n      else if (r == -EAGAIN)\n\tosdmon->dispatch(op);\n      else\n\tassert(0 == \"bad C_ReplyMap return value\");\n    }    \n  };\n  struct C_PoolOp : public C_MonOp {\n    OSDMonitor *osdmon;\n    int replyCode;\n    int epoch;\n    bufferlist reply_data;\n    C_PoolOp(OSDMonitor * osd, MonOpRequestRef op_, int rc, int e, bufferlist *rd=NULL) :\n      C_MonOp(op_), osdmon(osd), replyCode(rc), epoch(e) {\n      if (rd)\n\treply_data = *rd;\n    }\n    void _finish(int r) override {\n      if (r >= 0)\n\tosdmon->_pool_op_reply(op, replyCode, epoch, &reply_data);\n      else if (r == -ECANCELED)\n        return;\n      else if (r == -EAGAIN)\n\tosdmon->dispatch(op);\n      else\n\tassert(0 == \"bad C_PoolOp return value\");\n    }\n  };\n\n  bool preprocess_remove_snaps(MonOpRequestRef op);\n  bool prepare_remove_snaps(MonOpRequestRef op);\n\n  int load_metadata(int osd, map<string, string>& m, ostream *err);\n  void count_metadata(const string& field, Formatter *f);\n\n  void reencode_incremental_map(bufferlist& bl, uint64_t features);\n  void reencode_full_map(bufferlist& bl, uint64_t features);\npublic:\n  void count_metadata(const string& field, map<string,int> *out);\nprotected:\n  int get_osd_objectstore_type(int osd, std::string *type);\n  bool is_pool_currently_all_bluestore(int64_t pool_id, const pg_pool_t &pool,\n\t\t\t\t       ostream *err);\n\n  // when we last received PG stats from each osd\n  map<int,utime_t> last_osd_report;\n  // TODO: use last_osd_report to store the osd report epochs, once we don't\n  //       need to upgrade from pre-luminous releases.\n  map<int,epoch_t> osd_epochs;\n  LastEpochClean last_epoch_clean;\n  bool preprocess_beacon(MonOpRequestRef op);\n  bool prepare_beacon(MonOpRequestRef op);\n  epoch_t get_min_last_epoch_clean() const;\n\n  friend class C_UpdateCreatingPGs;\n  std::map<int, std::map<epoch_t, std::set<spg_t>>> creating_pgs_by_osd_epoch;\n  std::vector<pg_t> pending_created_pgs;\n  // the epoch when the pg mapping was calculated\n  epoch_t creating_pgs_epoch = 0;\n  creating_pgs_t creating_pgs;\n  mutable std::mutex creating_pgs_lock;\n\n  creating_pgs_t update_pending_pgs(const OSDMap::Incremental& inc,\n\t\t\t\t    const OSDMap& nextmap);\n  unsigned scan_for_creating_pgs(\n    const mempool::osdmap::map<int64_t,pg_pool_t>& pools,\n    const mempool::osdmap::set<int64_t>& removed_pools,\n    utime_t modified,\n    creating_pgs_t* creating_pgs) const;\n  pair<int32_t, pg_t> get_parent_pg(pg_t pgid) const;\n  void update_creating_pgs();\n  void check_pg_creates_subs();\n  epoch_t send_pg_creates(int osd, Connection *con, epoch_t next) const;\n\n  int32_t _allocate_osd_id(int32_t* existing_id);\n\npublic:\n  OSDMonitor(CephContext *cct, Monitor *mn, Paxos *p, const string& service_name);\n\n  void tick() override;  // check state, take actions\n\n  bool preprocess_command(MonOpRequestRef op);\n  bool prepare_command(MonOpRequestRef op);\n  bool prepare_command_impl(MonOpRequestRef op, const cmdmap_t& cmdmap);\n\n  int validate_osd_create(\n      const int32_t id,\n      const uuid_d& uuid,\n      const bool check_osd_exists,\n      int32_t* existing_id,\n      stringstream& ss);\n  int prepare_command_osd_create(\n      const int32_t id,\n      const uuid_d& uuid,\n      int32_t* existing_id,\n      stringstream& ss);\n  void do_osd_create(const int32_t id, const uuid_d& uuid,\n\t\t     const string& device_class,\n\t\t     int32_t* new_id);\n  int prepare_command_osd_purge(int32_t id, stringstream& ss);\n  int prepare_command_osd_destroy(int32_t id, stringstream& ss);\n  int _prepare_command_osd_crush_remove(\n      CrushWrapper &newcrush,\n      int32_t id,\n      int32_t ancestor,\n      bool has_ancestor,\n      bool unlink_only);\n  void do_osd_crush_remove(CrushWrapper& newcrush);\n  int prepare_command_osd_crush_remove(\n      CrushWrapper &newcrush,\n      int32_t id,\n      int32_t ancestor,\n      bool has_ancestor,\n      bool unlink_only);\n  int prepare_command_osd_remove(int32_t id);\n  int prepare_command_osd_new(\n      MonOpRequestRef op,\n      const cmdmap_t& cmdmap,\n      const map<string,string>& secrets,\n      stringstream &ss,\n      Formatter *f);\n\n  int prepare_command_pool_set(const cmdmap_t& cmdmap,\n                               stringstream& ss);\n  int prepare_command_pool_application(const string &prefix,\n                                       const cmdmap_t& cmdmap,\n                                       stringstream& ss);\n\n  bool handle_osd_timeouts(const utime_t &now,\n\t\t\t   std::map<int,utime_t> &last_osd_report);\n\n  void send_latest(MonOpRequestRef op, epoch_t start=0);\n  void send_latest_now_nodelete(MonOpRequestRef op, epoch_t start=0) {\n    op->mark_osdmon_event(__func__);\n    send_incremental(op, start);\n  }\n\n  void get_removed_snaps_range(\n    epoch_t start, epoch_t end,\n    mempool::osdmap::map<int64_t,OSDMap::snap_interval_set_t> *gap_removed_snaps);\n\n  int get_version(version_t ver, bufferlist& bl) override;\n  int get_version(version_t ver, uint64_t feature, bufferlist& bl);\n\n  int get_version_full(version_t ver, uint64_t feature, bufferlist& bl);\n  int get_version_full(version_t ver, bufferlist& bl) override;\n  int get_inc(version_t ver, OSDMap::Incremental& inc);\n  int get_full_from_pinned_map(version_t ver, bufferlist& bl);\n\n  epoch_t blacklist(const entity_addrvec_t& av, utime_t until);\n  epoch_t blacklist(const entity_addr_t& a, utime_t until);\n\n  void dump_info(Formatter *f);\n  int dump_osd_metadata(int osd, Formatter *f, ostream *err);\n  void print_nodes(Formatter *f);\n\n  void check_osdmap_sub(Subscription *sub);\n  void check_pg_creates_sub(Subscription *sub);\n\n  void do_application_enable(int64_t pool_id, const std::string &app_name,\n\t\t\t     const std::string &app_key=\"\",\n\t\t\t     const std::string &app_value=\"\");\n\n  void add_flag(int flag) {\n    if (!(osdmap.flags & flag)) {\n      if (pending_inc.new_flags < 0)\n\tpending_inc.new_flags = osdmap.flags;\n      pending_inc.new_flags |= flag;\n    }\n  }\n\n  void remove_flag(int flag) {\n    if(osdmap.flags & flag) {\n      if (pending_inc.new_flags < 0)\n\tpending_inc.new_flags = osdmap.flags;\n      pending_inc.new_flags &= ~flag;\n    }\n  }\n};\n\n#endif\n", "# cython: embedsignature=True\n\"\"\"\nThis module is a thin wrapper around librados.\n\nError codes from librados are turned into exceptions that subclass\n:class:`Error`. Almost all methods may raise :class:`Error(the base class of all rados exceptions), :class:`PermissionError`\n(the base class of all rados exceptions), :class:`PermissionError`\nand :class:`IOError`, in addition to those documented for the\nmethod.\n\"\"\"\n# Copyright 2011 Josh Durgin\n# Copyright 2011, Hannu Valtonen <hannu.valtonen@ormod.com>\n# Copyright 2015 Hector Martin <marcan@marcan.st>\n# Copyright 2016 Mehdi Abaakouk <sileht@redhat.com>\n\nfrom cpython cimport PyObject, ref\nfrom cpython.pycapsule cimport *\nfrom libc cimport errno\nfrom libc.stdint cimport *\nfrom libc.stdlib cimport malloc, realloc, free\n\nimport sys\nimport threading\nimport time\n\nfrom collections import Callable\nfrom datetime import datetime\nfrom functools import partial, wraps\nfrom itertools import chain\n\n# Are we running Python 2.x\nif sys.version_info[0] < 3:\n    str_type = basestring\nelse:\n    str_type = str\n\n\ncdef extern from \"Python.h\":\n    # These are in cpython/string.pxd, but use \"object\" types instead of\n    # PyObject*, which invokes assumptions in cpython that we need to\n    # legitimately break to implement zero-copy string buffers in Ioctx.read().\n    # This is valid use of the Python API and documented as a special case.\n    PyObject *PyBytes_FromStringAndSize(char *v, Py_ssize_t len) except NULL\n    char* PyBytes_AsString(PyObject *string) except NULL\n    int _PyBytes_Resize(PyObject **string, Py_ssize_t newsize) except -1\n    void PyEval_InitThreads()\n\n\ncdef extern from \"time.h\":\n    ctypedef long int time_t\n    ctypedef long int suseconds_t\n\n\ncdef extern from \"sys/time.h\":\n    cdef struct timeval:\n        time_t tv_sec\n        suseconds_t tv_usec\n\n\ncdef extern from \"rados/rados_types.h\" nogil:\n    cdef char* _LIBRADOS_ALL_NSPACES \"LIBRADOS_ALL_NSPACES\"\n\n\ncdef extern from \"rados/librados.h\" nogil:\n    enum:\n        _LIBRADOS_OP_FLAG_EXCL \"LIBRADOS_OP_FLAG_EXCL\"\n        _LIBRADOS_OP_FLAG_FAILOK \"LIBRADOS_OP_FLAG_FAILOK\"\n        _LIBRADOS_OP_FLAG_FADVISE_RANDOM \"LIBRADOS_OP_FLAG_FADVISE_RANDOM\"\n        _LIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL \"LIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL\"\n        _LIBRADOS_OP_FLAG_FADVISE_WILLNEED \"LIBRADOS_OP_FLAG_FADVISE_WILLNEED\"\n        _LIBRADOS_OP_FLAG_FADVISE_DONTNEED \"LIBRADOS_OP_FLAG_FADVISE_DONTNEED\"\n        _LIBRADOS_OP_FLAG_FADVISE_NOCACHE \"LIBRADOS_OP_FLAG_FADVISE_NOCACHE\"\n\n\n    enum:\n        _LIBRADOS_OPERATION_NOFLAG \"LIBRADOS_OPERATION_NOFLAG\"\n        _LIBRADOS_OPERATION_BALANCE_READS \"LIBRADOS_OPERATION_BALANCE_READS\"\n        _LIBRADOS_OPERATION_LOCALIZE_READS \"LIBRADOS_OPERATION_LOCALIZE_READS\"\n        _LIBRADOS_OPERATION_ORDER_READS_WRITES \"LIBRADOS_OPERATION_ORDER_READS_WRITES\"\n        _LIBRADOS_OPERATION_IGNORE_CACHE \"LIBRADOS_OPERATION_IGNORE_CACHE\"\n        _LIBRADOS_OPERATION_SKIPRWLOCKS \"LIBRADOS_OPERATION_SKIPRWLOCKS\"\n        _LIBRADOS_OPERATION_IGNORE_OVERLAY \"LIBRADOS_OPERATION_IGNORE_OVERLAY\"\n        _LIBRADOS_CREATE_EXCLUSIVE \"LIBRADOS_CREATE_EXCLUSIVE\"\n        _LIBRADOS_CREATE_IDEMPOTENT \"LIBRADOS_CREATE_IDEMPOTENT\"\n\n    cdef uint64_t _LIBRADOS_SNAP_HEAD \"LIBRADOS_SNAP_HEAD\"\n\n    ctypedef void* rados_t\n    ctypedef void* rados_config_t\n    ctypedef void* rados_ioctx_t\n    ctypedef void* rados_xattrs_iter_t\n    ctypedef void* rados_omap_iter_t\n    ctypedef void* rados_list_ctx_t\n    ctypedef uint64_t rados_snap_t\n    ctypedef void *rados_write_op_t\n    ctypedef void *rados_read_op_t\n    ctypedef void *rados_completion_t\n    ctypedef void (*rados_callback_t)(rados_completion_t cb, void *arg)\n    ctypedef void (*rados_log_callback_t)(void *arg, const char *line, const char *who,\n                                          uint64_t sec, uint64_t nsec, uint64_t seq, const char *level, const char *msg)\n    ctypedef void (*rados_log_callback2_t)(void *arg, const char *line, const char *channel, const char *who, const char *name,\n                                          uint64_t sec, uint64_t nsec, uint64_t seq, const char *level, const char *msg)\n\n\n    cdef struct rados_cluster_stat_t:\n        uint64_t kb\n        uint64_t kb_used\n        uint64_t kb_avail\n        uint64_t num_objects\n\n    cdef struct rados_pool_stat_t:\n        uint64_t num_bytes\n        uint64_t num_kb\n        uint64_t num_objects\n        uint64_t num_object_clones\n        uint64_t num_object_copies\n        uint64_t num_objects_missing_on_primary\n        uint64_t num_objects_unfound\n        uint64_t num_objects_degraded\n        uint64_t num_rd\n        uint64_t num_rd_kb\n        uint64_t num_wr\n        uint64_t num_wr_kb\n\n    void rados_buffer_free(char *buf)\n\n    void rados_version(int *major, int *minor, int *extra)\n    int rados_create2(rados_t *pcluster, const char *const clustername,\n                      const char * const name, uint64_t flags)\n    int rados_create_with_context(rados_t *cluster, rados_config_t cct)\n    int rados_connect(rados_t cluster)\n    void rados_shutdown(rados_t cluster)\n    int rados_conf_read_file(rados_t cluster, const char *path)\n    int rados_conf_parse_argv_remainder(rados_t cluster, int argc, const char **argv, const char **remargv)\n    int rados_conf_parse_env(rados_t cluster, const char *var)\n    int rados_conf_set(rados_t cluster, char *option, const char *value)\n    int rados_conf_get(rados_t cluster, char *option, char *buf, size_t len)\n\n    int rados_ioctx_pool_stat(rados_ioctx_t io, rados_pool_stat_t *stats)\n    int64_t rados_pool_lookup(rados_t cluster, const char *pool_name)\n    int rados_pool_reverse_lookup(rados_t cluster, int64_t id, char *buf, size_t maxlen)\n    int rados_pool_create(rados_t cluster, const char *pool_name)\n    int rados_pool_create_with_auid(rados_t cluster, const char *pool_name, uint64_t auid)\n    int rados_pool_create_with_crush_rule(rados_t cluster, const char *pool_name, uint8_t crush_rule_num)\n    int rados_pool_create_with_all(rados_t cluster, const char *pool_name, uint64_t auid, uint8_t crush_rule_num)\n    int rados_pool_get_base_tier(rados_t cluster, int64_t pool, int64_t *base_tier)\n    int rados_pool_list(rados_t cluster, char *buf, size_t len)\n    int rados_pool_delete(rados_t cluster, const char *pool_name)\n    int rados_inconsistent_pg_list(rados_t cluster, int64_t pool, char *buf, size_t len)\n\n    int rados_cluster_stat(rados_t cluster, rados_cluster_stat_t *result)\n    int rados_cluster_fsid(rados_t cluster, char *buf, size_t len)\n    int rados_blacklist_add(rados_t cluster, char *client_address, uint32_t expire_seconds)\n    int rados_application_enable(rados_ioctx_t io, const char *app_name,\n                                 int force)\n    void rados_set_osdmap_full_try(rados_ioctx_t io)\n    void rados_unset_osdmap_full_try(rados_ioctx_t io)\n    int rados_application_list(rados_ioctx_t io, char *values,\n                             size_t *values_len)\n    int rados_application_metadata_get(rados_ioctx_t io, const char *app_name,\n                                       const char *key, char *value,\n                                       size_t *value_len)\n    int rados_application_metadata_set(rados_ioctx_t io, const char *app_name,\n                                       const char *key, const char *value)\n    int rados_application_metadata_remove(rados_ioctx_t io,\n                                          const char *app_name, const char *key)\n    int rados_application_metadata_list(rados_ioctx_t io,\n                                        const char *app_name, char *keys,\n                                        size_t *key_len, char *values,\n                                        size_t *value_len)\n    int rados_ping_monitor(rados_t cluster, const char *mon_id, char **outstr, size_t *outstrlen)\n    int rados_mon_command(rados_t cluster, const char **cmd, size_t cmdlen,\n                          const char *inbuf, size_t inbuflen,\n                          char **outbuf, size_t *outbuflen,\n                          char **outs, size_t *outslen)\n    int rados_mgr_command(rados_t cluster, const char **cmd, size_t cmdlen,\n                          const char *inbuf, size_t inbuflen,\n                          char **outbuf, size_t *outbuflen,\n                          char **outs, size_t *outslen)\n    int rados_mon_command_target(rados_t cluster, const char *name, const char **cmd, size_t cmdlen,\n                                 const char *inbuf, size_t inbuflen,\n                                 char **outbuf, size_t *outbuflen,\n                                 char **outs, size_t *outslen)\n    int rados_osd_command(rados_t cluster, int osdid, const char **cmd, size_t cmdlen,\n                          const char *inbuf, size_t inbuflen,\n                          char **outbuf, size_t *outbuflen,\n                          char **outs, size_t *outslen)\n    int rados_pg_command(rados_t cluster, const char *pgstr, const char **cmd, size_t cmdlen,\n                         const char *inbuf, size_t inbuflen,\n                         char **outbuf, size_t *outbuflen,\n                         char **outs, size_t *outslen)\n    int rados_monitor_log(rados_t cluster, const char *level, rados_log_callback_t cb, void *arg)\n    int rados_monitor_log2(rados_t cluster, const char *level, rados_log_callback2_t cb, void *arg)\n\n    int rados_wait_for_latest_osdmap(rados_t cluster)\n\n    int rados_service_register(rados_t cluster, const char *service, const char *daemon, const char *metadata_dict)\n    int rados_service_update_status(rados_t cluster, const char *status_dict)\n\n    int rados_ioctx_create(rados_t cluster, const char *pool_name, rados_ioctx_t *ioctx)\n    int rados_ioctx_create2(rados_t cluster, int64_t pool_id, rados_ioctx_t *ioctx)\n    void rados_ioctx_destroy(rados_ioctx_t io)\n    int rados_ioctx_pool_set_auid(rados_ioctx_t io, uint64_t auid)\n    void rados_ioctx_locator_set_key(rados_ioctx_t io, const char *key)\n    void rados_ioctx_set_namespace(rados_ioctx_t io, const char * nspace)\n\n    uint64_t rados_get_last_version(rados_ioctx_t io)\n    int rados_stat(rados_ioctx_t io, const char *o, uint64_t *psize, time_t *pmtime)\n    int rados_write(rados_ioctx_t io, const char *oid, const char *buf, size_t len, uint64_t off)\n    int rados_write_full(rados_ioctx_t io, const char *oid, const char *buf, size_t len)\n    int rados_append(rados_ioctx_t io, const char *oid, const char *buf, size_t len)\n    int rados_read(rados_ioctx_t io, const char *oid, char *buf, size_t len, uint64_t off)\n    int rados_remove(rados_ioctx_t io, const char *oid)\n    int rados_trunc(rados_ioctx_t io, const char *oid, uint64_t size)\n    int rados_getxattr(rados_ioctx_t io, const char *o, const char *name, char *buf, size_t len)\n    int rados_setxattr(rados_ioctx_t io, const char *o, const char *name, const char *buf, size_t len)\n    int rados_rmxattr(rados_ioctx_t io, const char *o, const char *name)\n    int rados_getxattrs(rados_ioctx_t io, const char *oid, rados_xattrs_iter_t *iter)\n    int rados_getxattrs_next(rados_xattrs_iter_t iter, const char **name, const char **val, size_t *len)\n    void rados_getxattrs_end(rados_xattrs_iter_t iter)\n\n    int rados_nobjects_list_open(rados_ioctx_t io, rados_list_ctx_t *ctx)\n    int rados_nobjects_list_next(rados_list_ctx_t ctx, const char **entry, const char **key, const char **nspace)\n    void rados_nobjects_list_close(rados_list_ctx_t ctx)\n\n    int rados_ioctx_pool_requires_alignment2(rados_ioctx_t io, int * requires)\n    int rados_ioctx_pool_required_alignment2(rados_ioctx_t io, uint64_t * alignment)\n\n    int rados_ioctx_snap_rollback(rados_ioctx_t io, const char * oid, const char * snapname)\n    int rados_ioctx_snap_create(rados_ioctx_t io, const char * snapname)\n    int rados_ioctx_snap_remove(rados_ioctx_t io, const char * snapname)\n    int rados_ioctx_snap_lookup(rados_ioctx_t io, const char * name, rados_snap_t * id)\n    int rados_ioctx_snap_get_name(rados_ioctx_t io, rados_snap_t id, char * name, int maxlen)\n    void rados_ioctx_snap_set_read(rados_ioctx_t io, rados_snap_t snap)\n    int rados_ioctx_snap_list(rados_ioctx_t io, rados_snap_t * snaps, int maxlen)\n    int rados_ioctx_snap_get_stamp(rados_ioctx_t io, rados_snap_t id, time_t * t)\n\n    int rados_ioctx_selfmanaged_snap_create(rados_ioctx_t io,\n                                            rados_snap_t *snapid)\n    int rados_ioctx_selfmanaged_snap_remove(rados_ioctx_t io,\n                                            rados_snap_t snapid)\n    int rados_ioctx_selfmanaged_snap_set_write_ctx(rados_ioctx_t io,\n                                                   rados_snap_t snap_seq,\n                                                   rados_snap_t *snap,\n                                                   int num_snaps)\n    int rados_ioctx_selfmanaged_snap_rollback(rados_ioctx_t io, const char *oid,\n                                              rados_snap_t snapid)\n\n    int rados_lock_exclusive(rados_ioctx_t io, const char * oid, const char * name,\n                             const char * cookie, const char * desc,\n                             timeval * duration, uint8_t flags)\n    int rados_lock_shared(rados_ioctx_t io, const char * o, const char * name,\n                          const char * cookie, const char * tag, const char * desc,\n                          timeval * duration, uint8_t flags)\n    int rados_unlock(rados_ioctx_t io, const char * o, const char * name, const char * cookie)\n\n    rados_write_op_t rados_create_write_op()\n    void rados_release_write_op(rados_write_op_t write_op)\n\n    rados_read_op_t rados_create_read_op()\n    void rados_release_read_op(rados_read_op_t read_op)\n\n    int rados_aio_create_completion(void * cb_arg, rados_callback_t cb_complete, rados_callback_t cb_safe, rados_completion_t * pc)\n    void rados_aio_release(rados_completion_t c)\n    int rados_aio_stat(rados_ioctx_t io, const char *oid, rados_completion_t completion, uint64_t *psize, time_t *pmtime)\n    int rados_aio_write(rados_ioctx_t io, const char * oid, rados_completion_t completion, const char * buf, size_t len, uint64_t off)\n    int rados_aio_append(rados_ioctx_t io, const char * oid, rados_completion_t completion, const char * buf, size_t len)\n    int rados_aio_write_full(rados_ioctx_t io, const char * oid, rados_completion_t completion, const char * buf, size_t len)\n    int rados_aio_remove(rados_ioctx_t io, const char * oid, rados_completion_t completion)\n    int rados_aio_read(rados_ioctx_t io, const char * oid, rados_completion_t completion, char * buf, size_t len, uint64_t off)\n    int rados_aio_flush(rados_ioctx_t io)\n\n    int rados_aio_get_return_value(rados_completion_t c)\n    int rados_aio_wait_for_complete_and_cb(rados_completion_t c)\n    int rados_aio_wait_for_safe_and_cb(rados_completion_t c)\n    int rados_aio_wait_for_complete(rados_completion_t c)\n    int rados_aio_wait_for_safe(rados_completion_t c)\n    int rados_aio_is_complete(rados_completion_t c)\n    int rados_aio_is_safe(rados_completion_t c)\n\n    int rados_exec(rados_ioctx_t io, const char * oid, const char * cls, const char * method,\n                   const char * in_buf, size_t in_len, char * buf, size_t out_len)\n    int rados_aio_exec(rados_ioctx_t io, const char * oid, rados_completion_t completion, const char * cls, const char * method,\n                       const char * in_buf, size_t in_len, char * buf, size_t out_len)\n\n    int rados_write_op_operate(rados_write_op_t write_op, rados_ioctx_t io, const char * oid, time_t * mtime, int flags)\n    int rados_aio_write_op_operate(rados_write_op_t write_op, rados_ioctx_t io, rados_completion_t completion, const char *oid, time_t *mtime, int flags)\n    void rados_write_op_omap_set(rados_write_op_t write_op, const char * const* keys, const char * const* vals, const size_t * lens, size_t num)\n    void rados_write_op_omap_rm_keys(rados_write_op_t write_op, const char * const* keys, size_t keys_len)\n    void rados_write_op_omap_clear(rados_write_op_t write_op)\n    void rados_write_op_set_flags(rados_write_op_t write_op, int flags)\n\n    void rados_write_op_create(rados_write_op_t write_op, int exclusive, const char *category)\n    void rados_write_op_append(rados_write_op_t write_op, const char *buffer, size_t len)\n    void rados_write_op_write_full(rados_write_op_t write_op, const char *buffer, size_t len)\n    void rados_write_op_write(rados_write_op_t write_op, const char *buffer, size_t len, uint64_t offset)\n    void rados_write_op_remove(rados_write_op_t write_op)\n    void rados_write_op_truncate(rados_write_op_t write_op, uint64_t offset)\n    void rados_write_op_zero(rados_write_op_t write_op, uint64_t offset, uint64_t len)\n\n    void rados_read_op_omap_get_vals2(rados_read_op_t read_op, const char * start_after, const char * filter_prefix, uint64_t max_return, rados_omap_iter_t * iter, unsigned char *pmore, int * prval)\n    void rados_read_op_omap_get_keys2(rados_read_op_t read_op, const char * start_after, uint64_t max_return, rados_omap_iter_t * iter, unsigned char *pmore, int * prval)\n    void rados_read_op_omap_get_vals_by_keys(rados_read_op_t read_op, const char * const* keys, size_t keys_len, rados_omap_iter_t * iter, int * prval)\n    int rados_read_op_operate(rados_read_op_t read_op, rados_ioctx_t io, const char * oid, int flags)\n    int rados_aio_read_op_operate(rados_read_op_t read_op, rados_ioctx_t io, rados_completion_t completion, const char *oid, int flags)\n    void rados_read_op_set_flags(rados_read_op_t read_op, int flags)\n    int rados_omap_get_next(rados_omap_iter_t iter, const char * const* key, const char * const* val, size_t * len)\n    void rados_omap_get_end(rados_omap_iter_t iter)\n\n\nLIBRADOS_OP_FLAG_EXCL = _LIBRADOS_OP_FLAG_EXCL\nLIBRADOS_OP_FLAG_FAILOK = _LIBRADOS_OP_FLAG_FAILOK\nLIBRADOS_OP_FLAG_FADVISE_RANDOM = _LIBRADOS_OP_FLAG_FADVISE_RANDOM\nLIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL = _LIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL\nLIBRADOS_OP_FLAG_FADVISE_WILLNEED = _LIBRADOS_OP_FLAG_FADVISE_WILLNEED\nLIBRADOS_OP_FLAG_FADVISE_DONTNEED = _LIBRADOS_OP_FLAG_FADVISE_DONTNEED\nLIBRADOS_OP_FLAG_FADVISE_NOCACHE = _LIBRADOS_OP_FLAG_FADVISE_NOCACHE\n\nLIBRADOS_SNAP_HEAD = _LIBRADOS_SNAP_HEAD\n\nLIBRADOS_OPERATION_NOFLAG = _LIBRADOS_OPERATION_NOFLAG\nLIBRADOS_OPERATION_BALANCE_READS = _LIBRADOS_OPERATION_BALANCE_READS\nLIBRADOS_OPERATION_LOCALIZE_READS = _LIBRADOS_OPERATION_LOCALIZE_READS\nLIBRADOS_OPERATION_ORDER_READS_WRITES = _LIBRADOS_OPERATION_ORDER_READS_WRITES\nLIBRADOS_OPERATION_IGNORE_CACHE = _LIBRADOS_OPERATION_IGNORE_CACHE\nLIBRADOS_OPERATION_SKIPRWLOCKS = _LIBRADOS_OPERATION_SKIPRWLOCKS\nLIBRADOS_OPERATION_IGNORE_OVERLAY = _LIBRADOS_OPERATION_IGNORE_OVERLAY\n\nLIBRADOS_ALL_NSPACES = _LIBRADOS_ALL_NSPACES.decode('utf-8')\n\nLIBRADOS_CREATE_EXCLUSIVE = _LIBRADOS_CREATE_EXCLUSIVE\nLIBRADOS_CREATE_IDEMPOTENT = _LIBRADOS_CREATE_IDEMPOTENT\n\nANONYMOUS_AUID = 0xffffffffffffffff\nADMIN_AUID = 0\n\n\nclass Error(Exception):\n    \"\"\" `Error` class, derived from `Exception` \"\"\"\n    def __init__(self, message, errno=None):\n        super(Exception, self).__init__(message)\n        self.errno = errno\n\n    def __str__(self):\n        msg = super(Exception, self).__str__()\n        if self.errno is None:\n            return msg\n        return '[errno {0}] {1}'.format(self.errno, msg)\n\n    def __reduce__(self):\n        return (self.__class__, (self.message, self.errno))\n\nclass InvalidArgumentError(Error):\n    pass\n\nclass OSError(Error):\n    \"\"\" `OSError` class, derived from `Error` \"\"\"\n    pass\n\nclass InterruptedOrTimeoutError(OSError):\n    \"\"\" `InterruptedOrTimeoutError` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass PermissionError(OSError):\n    \"\"\" `PermissionError` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass PermissionDeniedError(OSError):\n    \"\"\" deal with EACCES related. \"\"\"\n    pass\n\n\nclass ObjectNotFound(OSError):\n    \"\"\" `ObjectNotFound` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass NoData(OSError):\n    \"\"\" `NoData` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass ObjectExists(OSError):\n    \"\"\" `ObjectExists` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass ObjectBusy(OSError):\n    \"\"\" `ObjectBusy` class, derived from `IOError` \"\"\"\n    pass\n\n\nclass IOError(OSError):\n    \"\"\" `ObjectBusy` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass NoSpace(OSError):\n    \"\"\" `NoSpace` class, derived from `OSError` \"\"\"\n    pass\n\n\nclass RadosStateError(Error):\n    \"\"\" `RadosStateError` class, derived from `Error` \"\"\"\n    pass\n\n\nclass IoctxStateError(Error):\n    \"\"\" `IoctxStateError` class, derived from `Error` \"\"\"\n    pass\n\n\nclass ObjectStateError(Error):\n    \"\"\" `ObjectStateError` class, derived from `Error` \"\"\"\n    pass\n\n\nclass LogicError(Error):\n    \"\"\" `` class, derived from `Error` \"\"\"\n    pass\n\n\nclass TimedOut(OSError):\n    \"\"\" `TimedOut` class, derived from `OSError` \"\"\"\n    pass\n\n\nIF UNAME_SYSNAME == \"FreeBSD\":\n    cdef errno_to_exception = {\n        errno.EPERM     : PermissionError,\n        errno.ENOENT    : ObjectNotFound,\n        errno.EIO       : IOError,\n        errno.ENOSPC    : NoSpace,\n        errno.EEXIST    : ObjectExists,\n        errno.EBUSY     : ObjectBusy,\n        errno.ENOATTR   : NoData,\n        errno.EINTR     : InterruptedOrTimeoutError,\n        errno.ETIMEDOUT : TimedOut,\n        errno.EACCES    : PermissionDeniedError,\n        errno.EINVAL    : InvalidArgumentError,\n    }\nELSE:\n    cdef errno_to_exception = {\n        errno.EPERM     : PermissionError,\n        errno.ENOENT    : ObjectNotFound,\n        errno.EIO       : IOError,\n        errno.ENOSPC    : NoSpace,\n        errno.EEXIST    : ObjectExists,\n        errno.EBUSY     : ObjectBusy,\n        errno.ENODATA   : NoData,\n        errno.EINTR     : InterruptedOrTimeoutError,\n        errno.ETIMEDOUT : TimedOut,\n        errno.EACCES    : PermissionDeniedError,\n        errno.EINVAL    : InvalidArgumentError,\n    }\n\n\ncdef make_ex(ret, msg):\n    \"\"\"\n    Translate a librados return code into an exception.\n\n    :param ret: the return code\n    :type ret: int\n    :param msg: the error message to use\n    :type msg: str\n    :returns: a subclass of :class:`Error`\n    \"\"\"\n    ret = abs(ret)\n    if ret in errno_to_exception:\n        return errno_to_exception[ret](msg, errno=ret)\n    else:\n        return OSError(msg, errno=ret)\n\n\n# helper to specify an optional argument, where in addition to `cls`, `None`\n# is also acceptable\ndef opt(cls):\n    return (cls, None)\n\n\n# validate argument types of an instance method\n# kwargs is an un-ordered dict, so use args instead\ndef requires(*types):\n    def is_type_of(v, t):\n        if t is None:\n            return v is None\n        else:\n            return isinstance(v, t)\n\n    def check_type(val, arg_name, arg_type):\n        if isinstance(arg_type, tuple):\n            if any(is_type_of(val, t) for t in arg_type):\n                return\n            type_names = ' or '.join('None' if t is None else t.__name__\n                                     for t in arg_type)\n            raise TypeError('%s must be %s' % (arg_name, type_names))\n        else:\n            if is_type_of(val, arg_type):\n                return\n            assert(arg_type is not None)\n            raise TypeError('%s must be %s' % (arg_name, arg_type.__name__))\n\n    def wrapper(f):\n        # FIXME(sileht): this stop with\n        # AttributeError: 'method_descriptor' object has no attribute '__module__'\n        # @wraps(f)\n        def validate_func(*args, **kwargs):\n            # ignore the `self` arg\n            pos_args = zip(args[1:], types)\n            named_args = ((kwargs[name], (name, spec)) for name, spec in types\n                          if name in kwargs)\n            for arg_val, (arg_name, arg_type) in chain(pos_args, named_args):\n                check_type(arg_val, arg_name, arg_type)\n            return f(*args, **kwargs)\n        return validate_func\n    return wrapper\n\n\ndef cstr(val, name, encoding=\"utf-8\", opt=False):\n    \"\"\"\n    Create a byte string from a Python string\n\n    :param basestring val: Python string\n    :param str name: Name of the string parameter, for exceptions\n    :param str encoding: Encoding to use\n    :param bool opt: If True, None is allowed\n    :rtype: bytes\n    :raises: :class:`InvalidArgument`\n    \"\"\"\n    if opt and val is None:\n        return None\n    if isinstance(val, bytes):\n        return val\n    elif isinstance(val, unicode):\n        return val.encode(encoding)\n    else:\n        raise TypeError('%s must be a string' % name)\n\n\ndef cstr_list(list_str, name, encoding=\"utf-8\"):\n    return [cstr(s, name) for s in list_str]\n\n\ndef decode_cstr(val, encoding=\"utf-8\"):\n    \"\"\"\n    Decode a byte string into a Python string.\n\n    :param bytes val: byte string\n    :rtype: unicode or None\n    \"\"\"\n    if val is None:\n        return None\n\n    return val.decode(encoding)\n\n\ncdef char* opt_str(s) except? NULL:\n    if s is None:\n        return NULL\n    return s\n\n\ncdef void* realloc_chk(void* ptr, size_t size) except NULL:\n    cdef void *ret = realloc(ptr, size)\n    if ret == NULL:\n        raise MemoryError(\"realloc failed\")\n    return ret\n\n\ncdef size_t * to_csize_t_array(list_int):\n    cdef size_t *ret = <size_t *>malloc(len(list_int) * sizeof(size_t))\n    if ret == NULL:\n        raise MemoryError(\"malloc failed\")\n    for i in xrange(len(list_int)):\n        ret[i] = <size_t>list_int[i]\n    return ret\n\n\ncdef char ** to_bytes_array(list_bytes):\n    cdef char **ret = <char **>malloc(len(list_bytes) * sizeof(char *))\n    if ret == NULL:\n        raise MemoryError(\"malloc failed\")\n    for i in xrange(len(list_bytes)):\n        ret[i] = <char *>list_bytes[i]\n    return ret\n\n\n\ncdef int __monitor_callback(void *arg, const char *line, const char *who,\n                             uint64_t sec, uint64_t nsec, uint64_t seq,\n                             const char *level, const char *msg) with gil:\n    cdef object cb_info = <object>arg\n    cb_info[0](cb_info[1], line, who, sec, nsec, seq, level, msg)\n    return 0\n\ncdef int __monitor_callback2(void *arg, const char *line, const char *channel,\n                             const char *who,\n                             const char *name,\n                             uint64_t sec, uint64_t nsec, uint64_t seq,\n                             const char *level, const char *msg) with gil:\n    cdef object cb_info = <object>arg\n    cb_info[0](cb_info[1], line, channel, name, who, sec, nsec, seq, level, msg)\n    return 0\n\n\nclass Version(object):\n    \"\"\" Version information \"\"\"\n    def __init__(self, major, minor, extra):\n        self.major = major\n        self.minor = minor\n        self.extra = extra\n\n    def __str__(self):\n        return \"%d.%d.%d\" % (self.major, self.minor, self.extra)\n\n\ncdef class Rados(object):\n    \"\"\"This class wraps librados functions\"\"\"\n    # NOTE(sileht): attributes declared in .pyd\n\n    def __init__(self, *args, **kwargs):\n        PyEval_InitThreads()\n        self.__setup(*args, **kwargs)\n\n    @requires(('rados_id', opt(str_type)), ('name', opt(str_type)), ('clustername', opt(str_type)),\n              ('conffile', opt(str_type)))\n    def __setup(self, rados_id=None, name=None, clustername=None,\n                conf_defaults=None, conffile=None, conf=None, flags=0,\n                context=None):\n        self.monitor_callback = None\n        self.monitor_callback2 = None\n        self.parsed_args = []\n        self.conf_defaults = conf_defaults\n        self.conffile = conffile\n        self.rados_id = rados_id\n\n        if rados_id and name:\n            raise Error(\"Rados(): can't supply both rados_id and name\")\n        elif rados_id:\n            name = 'client.' + rados_id\n        elif name is None:\n            name = 'client.admin'\n        if clustername is None:\n            clustername = ''\n\n        name = cstr(name, 'name')\n        clustername = cstr(clustername, 'clustername')\n        cdef:\n            char *_name = name\n            char *_clustername = clustername\n            int _flags = flags\n            int ret\n\n        if context:\n            # Unpack void* (aka rados_config_t) from capsule\n            rados_config = <rados_config_t> PyCapsule_GetPointer(context, NULL)\n            with nogil:\n                ret = rados_create_with_context(&self.cluster, rados_config)\n        else:\n            with nogil:\n                ret = rados_create2(&self.cluster, _clustername, _name, _flags)\n        if ret != 0:\n            raise Error(\"rados_initialize failed with error code: %d\" % ret)\n\n        self.state = \"configuring\"\n        # order is important: conf_defaults, then conffile, then conf\n        if conf_defaults:\n            for key, value in conf_defaults.items():\n                self.conf_set(key, value)\n        if conffile is not None:\n            # read the default conf file when '' is given\n            if conffile == '':\n                conffile = None\n            self.conf_read_file(conffile)\n        if conf:\n            for key, value in conf.items():\n                self.conf_set(key, value)\n\n    def require_state(self, *args):\n        \"\"\"\n        Checks if the Rados object is in a special state\n\n        :raises: :class:`RadosStateError`\n        \"\"\"\n        if self.state in args:\n            return\n        raise RadosStateError(\"You cannot perform that operation on a \\\nRados object in state %s.\" % self.state)\n\n    def shutdown(self):\n        \"\"\"\n        Disconnects from the cluster.  Call this explicitly when a\n        Rados.connect()ed object is no longer used.\n        \"\"\"\n        if self.state != \"shutdown\":\n            with nogil:\n                rados_shutdown(self.cluster)\n            self.state = \"shutdown\"\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        self.shutdown()\n        return False\n\n    def version(self):\n        \"\"\"\n        Get the version number of the ``librados`` C library.\n\n        :returns: a tuple of ``(major, minor, extra)`` components of the\n                  librados version\n        \"\"\"\n        cdef int major = 0\n        cdef int minor = 0\n        cdef int extra = 0\n        with nogil:\n            rados_version(&major, &minor, &extra)\n        return Version(major, minor, extra)\n\n    @requires(('path', opt(str_type)))\n    def conf_read_file(self, path=None):\n        \"\"\"\n        Configure the cluster handle using a Ceph config file.\n\n        :param path: path to the config file\n        :type path: str\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        path = cstr(path, 'path', opt=True)\n        cdef:\n            char *_path = opt_str(path)\n        with nogil:\n            ret = rados_conf_read_file(self.cluster, _path)\n        if ret != 0:\n            raise make_ex(ret, \"error calling conf_read_file\")\n\n    def conf_parse_argv(self, args):\n        \"\"\"\n        Parse known arguments from args, and remove; returned\n        args contain only those unknown to ceph\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        if not args:\n            return\n\n        cargs = cstr_list(args, 'args')\n        cdef:\n            int _argc = len(args)\n            char **_argv = to_bytes_array(cargs)\n            char **_remargv = NULL\n\n        try:\n            _remargv = <char **>malloc(_argc * sizeof(char *))\n            with nogil:\n                ret = rados_conf_parse_argv_remainder(self.cluster, _argc,\n                                                      <const char**>_argv,\n                                                      <const char**>_remargv)\n            if ret:\n                raise make_ex(ret, \"error calling conf_parse_argv_remainder\")\n\n            # _remargv was allocated with fixed argc; collapse return\n            # list to eliminate any missing args\n            retargs = [decode_cstr(a) for a in _remargv[:_argc]\n                       if a != NULL]\n            self.parsed_args = args\n            return retargs\n        finally:\n            free(_argv)\n            free(_remargv)\n\n    def conf_parse_env(self, var='CEPH_ARGS'):\n        \"\"\"\n        Parse known arguments from an environment variable, normally\n        CEPH_ARGS.\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        if not var:\n            return\n\n        var = cstr(var, 'var')\n        cdef:\n            char *_var = var\n        with nogil:\n            ret = rados_conf_parse_env(self.cluster, _var)\n        if ret != 0:\n            raise make_ex(ret, \"error calling conf_parse_env\")\n\n    @requires(('option', str_type))\n    def conf_get(self, option):\n        \"\"\"\n        Get the value of a configuration option\n\n        :param option: which option to read\n        :type option: str\n\n        :returns: str - value of the option or None\n        :raises: :class:`TypeError`\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        option = cstr(option, 'option')\n        cdef:\n            char *_option = option\n            size_t length = 20\n            char *ret_buf = NULL\n\n        try:\n            while True:\n                ret_buf = <char *>realloc_chk(ret_buf, length)\n                with nogil:\n                    ret = rados_conf_get(self.cluster, _option, ret_buf, length)\n                if ret == 0:\n                    return decode_cstr(ret_buf)\n                elif ret == -errno.ENAMETOOLONG:\n                    length = length * 2\n                elif ret == -errno.ENOENT:\n                    return None\n                else:\n                    raise make_ex(ret, \"error calling conf_get\")\n        finally:\n            free(ret_buf)\n\n    @requires(('option', str_type), ('val', str_type))\n    def conf_set(self, option, val):\n        \"\"\"\n        Set the value of a configuration option\n\n        :param option: which option to set\n        :type option: str\n        :param option: value of the option\n        :type option: str\n\n        :raises: :class:`TypeError`, :class:`ObjectNotFound`\n        \"\"\"\n        self.require_state(\"configuring\", \"connected\")\n        option = cstr(option, 'option')\n        val = cstr(val, 'val')\n        cdef:\n            char *_option = option\n            char *_val = val\n\n        with nogil:\n            ret = rados_conf_set(self.cluster, _option, _val)\n        if ret != 0:\n            raise make_ex(ret, \"error calling conf_set\")\n\n    def ping_monitor(self, mon_id):\n        \"\"\"\n        Ping a monitor to assess liveness\n\n        May be used as a simply way to assess liveness, or to obtain\n        information about the monitor in a simple way even in the\n        absence of quorum.\n\n        :param mon_id: the ID portion of the monitor's name (i.e., mon.<ID>)\n        :type mon_id: str\n        :returns: the string reply from the monitor\n        \"\"\"\n\n        self.require_state(\"configuring\", \"connected\")\n\n        mon_id = cstr(mon_id, 'mon_id')\n        cdef:\n            char *_mon_id = mon_id\n            size_t outstrlen = 0\n            char *outstr\n\n        with nogil:\n            ret = rados_ping_monitor(self.cluster, _mon_id, &outstr, &outstrlen)\n\n        if ret != 0:\n            raise make_ex(ret, \"error calling ping_monitor\")\n\n        if outstrlen:\n            my_outstr = outstr[:outstrlen]\n            rados_buffer_free(outstr)\n            return decode_cstr(my_outstr)\n\n    def connect(self, timeout=0):\n        \"\"\"\n        Connect to the cluster.  Use shutdown() to release resources.\n        \"\"\"\n        self.require_state(\"configuring\")\n        # NOTE(sileht): timeout was supported by old python API,\n        # but this is not something available in C API, so ignore\n        # for now and remove it later\n        with nogil:\n            ret = rados_connect(self.cluster)\n        if ret != 0:\n            raise make_ex(ret, \"error connecting to the cluster\")\n        self.state = \"connected\"\n\n    def get_cluster_stats(self):\n        \"\"\"\n        Read usage info about the cluster\n\n        This tells you total space, space used, space available, and number\n        of objects. These are not updated immediately when data is written,\n        they are eventually consistent.\n\n        :returns: dict - contains the following keys:\n\n            - ``kb`` (int) - total space\n\n            - ``kb_used`` (int) - space used\n\n            - ``kb_avail`` (int) - free space available\n\n            - ``num_objects`` (int) - number of objects\n\n        \"\"\"\n        cdef:\n            rados_cluster_stat_t stats\n\n        with nogil:\n            ret = rados_cluster_stat(self.cluster, &stats)\n\n        if ret < 0:\n            raise make_ex(\n                ret, \"Rados.get_cluster_stats(%s): get_stats failed\" % self.rados_id)\n        return {'kb': stats.kb,\n                'kb_used': stats.kb_used,\n                'kb_avail': stats.kb_avail,\n                'num_objects': stats.num_objects}\n\n    @requires(('pool_name', str_type))\n    def pool_exists(self, pool_name):\n        \"\"\"\n        Checks if a given pool exists.\n\n        :param pool_name: name of the pool to check\n        :type pool_name: str\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: bool - whether the pool exists, false otherwise.\n        \"\"\"\n        self.require_state(\"connected\")\n\n        pool_name = cstr(pool_name, 'pool_name')\n        cdef:\n            char *_pool_name = pool_name\n\n        with nogil:\n            ret = rados_pool_lookup(self.cluster, _pool_name)\n        if ret >= 0:\n            return True\n        elif ret == -errno.ENOENT:\n            return False\n        else:\n            raise make_ex(ret, \"error looking up pool '%s'\" % pool_name)\n\n    @requires(('pool_name', str_type))\n    def pool_lookup(self, pool_name):\n        \"\"\"\n        Returns a pool's ID based on its name.\n\n        :param pool_name: name of the pool to look up\n        :type pool_name: str\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: int - pool ID, or None if it doesn't exist\n        \"\"\"\n        self.require_state(\"connected\")\n        pool_name = cstr(pool_name, 'pool_name')\n        cdef:\n            char *_pool_name = pool_name\n\n        with nogil:\n            ret = rados_pool_lookup(self.cluster, _pool_name)\n        if ret >= 0:\n            return int(ret)\n        elif ret == -errno.ENOENT:\n            return None\n        else:\n            raise make_ex(ret, \"error looking up pool '%s'\" % pool_name)\n\n    @requires(('pool_id', int))\n    def pool_reverse_lookup(self, pool_id):\n        \"\"\"\n        Returns a pool's name based on its ID.\n\n        :param pool_id: ID of the pool to look up\n        :type pool_id: int\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: string - pool name, or None if it doesn't exist\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            int64_t _pool_id = pool_id\n            size_t size = 512\n            char *name = NULL\n\n        try:\n            while True:\n                name = <char *>realloc_chk(name, size)\n                with nogil:\n                    ret = rados_pool_reverse_lookup(self.cluster, _pool_id, name, size)\n                if ret >= 0:\n                    break\n                elif ret != -errno.ERANGE and size <= 4096:\n                    size *= 2\n                elif ret == -errno.ENOENT:\n                    return None\n                elif ret < 0:\n                    raise make_ex(ret, \"error reverse looking up pool '%s'\" % pool_id)\n\n            return decode_cstr(name)\n\n        finally:\n            free(name)\n\n    @requires(('pool_name', str_type), ('auid', opt(int)), ('crush_rule', opt(int)))\n    def create_pool(self, pool_name, auid=None, crush_rule=None):\n        \"\"\"\n        Create a pool:\n        - with default settings: if auid=None and crush_rule=None\n        - owned by a specific auid: auid given and crush_rule=None\n        - with a specific CRUSH rule: if auid=None and crush_rule given\n        - with a specific CRUSH rule and auid: if auid and crush_rule given\n\n        :param pool_name: name of the pool to create\n        :type pool_name: str\n        :param auid: the id of the owner of the new pool\n        :type auid: int\n        :param crush_rule: rule to use for placement in the new pool\n        :type crush_rule: int\n\n        :raises: :class:`TypeError`, :class:`Error`\n        \"\"\"\n        self.require_state(\"connected\")\n\n        pool_name = cstr(pool_name, 'pool_name')\n        cdef:\n            char *_pool_name = pool_name\n            uint8_t _crush_rule\n            uint64_t _auid\n\n        if auid is None and crush_rule is None:\n            with nogil:\n                ret = rados_pool_create(self.cluster, _pool_name)\n        elif auid is None:\n            _crush_rule = crush_rule\n            with nogil:\n                ret = rados_pool_create_with_crush_rule(self.cluster, _pool_name, _crush_rule)\n        elif crush_rule is None:\n            _auid = auid\n            with nogil:\n                ret = rados_pool_create_with_auid(self.cluster, _pool_name, _auid)\n        else:\n            _auid = auid\n            _crush_rule = crush_rule\n            with nogil:\n                ret = rados_pool_create_with_all(self.cluster, _pool_name, _auid, _crush_rule)\n        if ret < 0:\n            raise make_ex(ret, \"error creating pool '%s'\" % pool_name)\n\n    @requires(('pool_id', int))\n    def get_pool_base_tier(self, pool_id):\n        \"\"\"\n        Get base pool\n\n        :returns: base pool, or pool_id if tiering is not configured for the pool\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            int64_t base_tier = 0\n            int64_t _pool_id = pool_id\n\n        with nogil:\n            ret = rados_pool_get_base_tier(self.cluster, _pool_id, &base_tier)\n        if ret < 0:\n            raise make_ex(ret, \"get_pool_base_tier(%d)\" % pool_id)\n        return int(base_tier)\n\n    @requires(('pool_name', str_type))\n    def delete_pool(self, pool_name):\n        \"\"\"\n        Delete a pool and all data inside it.\n\n        The pool is removed from the cluster immediately,\n        but the actual data is deleted in the background.\n\n        :param pool_name: name of the pool to delete\n        :type pool_name: str\n\n        :raises: :class:`TypeError`, :class:`Error`\n        \"\"\"\n        self.require_state(\"connected\")\n\n        pool_name = cstr(pool_name, 'pool_name')\n        cdef:\n            char *_pool_name = pool_name\n\n        with nogil:\n            ret = rados_pool_delete(self.cluster, _pool_name)\n        if ret < 0:\n            raise make_ex(ret, \"error deleting pool '%s'\" % pool_name)\n\n    @requires(('pool_id', int))\n    def get_inconsistent_pgs(self, pool_id):\n        \"\"\"\n        List inconsistent placement groups in the given pool\n\n        :param pool_id: ID of the pool in which PGs are listed\n        :type pool_id: int\n        :returns: list - inconsistent placement groups\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            int64_t pool = pool_id\n            size_t size = 512\n            char *pgs = NULL\n\n        try:\n            while True:\n                pgs = <char *>realloc_chk(pgs, size);\n                with nogil:\n                    ret = rados_inconsistent_pg_list(self.cluster, pool,\n                                                     pgs, size)\n                if ret > <int>size:\n                    size *= 2\n                elif ret >= 0:\n                    break\n                else:\n                    raise make_ex(ret, \"error calling inconsistent_pg_list\")\n            return [pg for pg in decode_cstr(pgs[:ret]).split('\\0') if pg]\n        finally:\n            free(pgs)\n\n    def list_pools(self):\n        \"\"\"\n        Gets a list of pool names.\n\n        :returns: list - of pool names.\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            size_t size = 512\n            char *c_names = NULL\n\n        try:\n            while True:\n                c_names = <char *>realloc_chk(c_names, size)\n                with nogil:\n                    ret = rados_pool_list(self.cluster, c_names, size)\n                if ret > <int>size:\n                    size *= 2\n                elif ret >= 0:\n                    break\n            return [name for name in decode_cstr(c_names[:ret]).split('\\0')\n                    if name]\n        finally:\n            free(c_names)\n\n    def get_fsid(self):\n        \"\"\"\n        Get the fsid of the cluster as a hexadecimal string.\n\n        :raises: :class:`Error`\n        :returns: str - cluster fsid\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            char *ret_buf\n            size_t buf_len = 37\n            PyObject* ret_s = NULL\n\n        ret_s = PyBytes_FromStringAndSize(NULL, buf_len)\n        try:\n            ret_buf = PyBytes_AsString(ret_s)\n            with nogil:\n                ret = rados_cluster_fsid(self.cluster, ret_buf, buf_len)\n            if ret < 0:\n                raise make_ex(ret, \"error getting cluster fsid\")\n            if ret != <int>buf_len:\n                _PyBytes_Resize(&ret_s, ret)\n            return <object>ret_s\n        finally:\n            # We DECREF unconditionally: the cast to object above will have\n            # INCREFed if necessary. This also takes care of exceptions,\n            # including if _PyString_Resize fails (that will free the string\n            # itself and set ret_s to NULL, hence XDECREF).\n            ref.Py_XDECREF(ret_s)\n\n    @requires(('ioctx_name', str_type))\n    def open_ioctx(self, ioctx_name):\n        \"\"\"\n        Create an io context\n\n        The io context allows you to perform operations within a particular\n        pool.\n\n        :param ioctx_name: name of the pool\n        :type ioctx_name: str\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: Ioctx - Rados Ioctx object\n        \"\"\"\n        self.require_state(\"connected\")\n        ioctx_name = cstr(ioctx_name, 'ioctx_name')\n        cdef:\n            rados_ioctx_t ioctx\n            char *_ioctx_name = ioctx_name\n        with nogil:\n            ret = rados_ioctx_create(self.cluster, _ioctx_name, &ioctx)\n        if ret < 0:\n            raise make_ex(ret, \"error opening pool '%s'\" % ioctx_name)\n        io = Ioctx(ioctx_name)\n        io.io = ioctx\n        return io\n\n    @requires(('pool_id', int))\n    def open_ioctx2(self, pool_id):\n        \"\"\"\n        Create an io context\n\n        The io context allows you to perform operations within a particular\n        pool.\n\n        :param pool_id: ID of the pool\n        :type pool_id: int\n\n        :raises: :class:`TypeError`, :class:`Error`\n        :returns: Ioctx - Rados Ioctx object\n        \"\"\"\n        self.require_state(\"connected\")\n        cdef:\n            rados_ioctx_t ioctx\n            int64_t _pool_id = pool_id\n        with nogil:\n            ret = rados_ioctx_create2(self.cluster, _pool_id, &ioctx)\n        if ret < 0:\n            raise make_ex(ret, \"error opening pool id '%s'\" % pool_id)\n        io = Ioctx(str(pool_id))\n        io.io = ioctx\n        return io\n\n    def mon_command(self, cmd, inbuf, timeout=0, target=None):\n        \"\"\"\n        mon_command[_target](cmd, inbuf, outbuf, outbuflen, outs, outslen)\n        returns (int ret, string outbuf, string outs)\n        \"\"\"\n        # NOTE(sileht): timeout is ignored because C API doesn't provide\n        # timeout argument, but we keep it for backward compat with old python binding\n\n        self.require_state(\"connected\")\n        cmd = cstr_list(cmd, 'c')\n\n        if isinstance(target, int):\n        # NOTE(sileht): looks weird but test_monmap_dump pass int\n            target = str(target)\n\n        target = cstr(target, 'target', opt=True)\n        inbuf = cstr(inbuf, 'inbuf')\n\n        cdef:\n            char *_target = opt_str(target)\n            char **_cmd = to_bytes_array(cmd)\n            size_t _cmdlen = len(cmd)\n\n            char *_inbuf = inbuf\n            size_t _inbuf_len = len(inbuf)\n\n            char *_outbuf\n            size_t _outbuf_len\n            char *_outs\n            size_t _outs_len\n\n        try:\n            if target:\n                with nogil:\n                    ret = rados_mon_command_target(self.cluster, _target,\n                                                <const char **>_cmd, _cmdlen,\n                                                <const char*>_inbuf, _inbuf_len,\n                                                &_outbuf, &_outbuf_len,\n                                                &_outs, &_outs_len)\n            else:\n                with nogil:\n                    ret = rados_mon_command(self.cluster,\n                                            <const char **>_cmd, _cmdlen,\n                                            <const char*>_inbuf, _inbuf_len,\n                                            &_outbuf, &_outbuf_len,\n                                            &_outs, &_outs_len)\n\n            my_outs = decode_cstr(_outs[:_outs_len])\n            my_outbuf = _outbuf[:_outbuf_len]\n            if _outs_len:\n                rados_buffer_free(_outs)\n            if _outbuf_len:\n                rados_buffer_free(_outbuf)\n            return (ret, my_outbuf, my_outs)\n        finally:\n            free(_cmd)\n\n    def osd_command(self, osdid, cmd, inbuf, timeout=0):\n        \"\"\"\n        osd_command(osdid, cmd, inbuf, outbuf, outbuflen, outs, outslen)\n        returns (int ret, string outbuf, string outs)\n        \"\"\"\n        # NOTE(sileht): timeout is ignored because C API doesn't provide\n        # timeout argument, but we keep it for backward compat with old python binding\n        self.require_state(\"connected\")\n\n        cmd = cstr_list(cmd, 'cmd')\n        inbuf = cstr(inbuf, 'inbuf')\n\n        cdef:\n            int _osdid = osdid\n            char **_cmd = to_bytes_array(cmd)\n            size_t _cmdlen = len(cmd)\n\n            char *_inbuf = inbuf\n            size_t _inbuf_len = len(inbuf)\n\n            char *_outbuf\n            size_t _outbuf_len\n            char *_outs\n            size_t _outs_len\n\n        try:\n            with nogil:\n                ret = rados_osd_command(self.cluster, _osdid,\n                                        <const char **>_cmd, _cmdlen,\n                                        <const char*>_inbuf, _inbuf_len,\n                                        &_outbuf, &_outbuf_len,\n                                        &_outs, &_outs_len)\n\n            my_outs = decode_cstr(_outs[:_outs_len])\n            my_outbuf = _outbuf[:_outbuf_len]\n            if _outs_len:\n                rados_buffer_free(_outs)\n            if _outbuf_len:\n                rados_buffer_free(_outbuf)\n            return (ret, my_outbuf, my_outs)\n        finally:\n            free(_cmd)\n\n    def mgr_command(self, cmd, inbuf, timeout=0):\n        \"\"\"\n        returns (int ret, string outbuf, string outs)\n        \"\"\"\n        # NOTE(sileht): timeout is ignored because C API doesn't provide\n        # timeout argument, but we keep it for backward compat with old python binding\n        self.require_state(\"connected\")\n\n        cmd = cstr_list(cmd, 'cmd')\n        inbuf = cstr(inbuf, 'inbuf')\n\n        cdef:\n            char **_cmd = to_bytes_array(cmd)\n            size_t _cmdlen = len(cmd)\n\n            char *_inbuf = inbuf\n            size_t _inbuf_len = len(inbuf)\n\n            char *_outbuf\n            size_t _outbuf_len\n            char *_outs\n            size_t _outs_len\n\n        try:\n            with nogil:\n                ret = rados_mgr_command(self.cluster,\n                                        <const char **>_cmd, _cmdlen,\n                                        <const char*>_inbuf, _inbuf_len,\n                                        &_outbuf, &_outbuf_len,\n                                        &_outs, &_outs_len)\n\n            my_outs = decode_cstr(_outs[:_outs_len])\n            my_outbuf = _outbuf[:_outbuf_len]\n            if _outs_len:\n                rados_buffer_free(_outs)\n            if _outbuf_len:\n                rados_buffer_free(_outbuf)\n            return (ret, my_outbuf, my_outs)\n        finally:\n            free(_cmd)\n\n    def pg_command(self, pgid, cmd, inbuf, timeout=0):\n        \"\"\"\n        pg_command(pgid, cmd, inbuf, outbuf, outbuflen, outs, outslen)\n        returns (int ret, string outbuf, string outs)\n        \"\"\"\n        # NOTE(sileht): timeout is ignored because C API doesn't provide\n        # timeout argument, but we keep it for backward compat with old python binding\n        self.require_state(\"connected\")\n\n        pgid = cstr(pgid, 'pgid')\n        cmd = cstr_list(cmd, 'cmd')\n        inbuf = cstr(inbuf, 'inbuf')\n\n        cdef:\n            char *_pgid = pgid\n            char **_cmd = to_bytes_array(cmd)\n            size_t _cmdlen = len(cmd)\n\n            char *_inbuf = inbuf\n            size_t _inbuf_len = len(inbuf)\n\n            char *_outbuf\n            size_t _outbuf_len\n            char *_outs\n            size_t _outs_len\n\n        try:\n            with nogil:\n                ret = rados_pg_command(self.cluster, _pgid,\n                                       <const char **>_cmd, _cmdlen,\n                                       <const char *>_inbuf, _inbuf_len,\n                                       &_outbuf, &_outbuf_len,\n                                       &_outs, &_outs_len)\n\n            my_outs = decode_cstr(_outs[:_outs_len])\n            my_outbuf = _outbuf[:_outbuf_len]\n            if _outs_len:\n                rados_buffer_free(_outs)\n            if _outbuf_len:\n                rados_buffer_free(_outbuf)\n            return (ret, my_outbuf, my_outs)\n        finally:\n            free(_cmd)\n\n    def wait_for_latest_osdmap(self):\n        self.require_state(\"connected\")\n        with nogil:\n            ret = rados_wait_for_latest_osdmap(self.cluster)\n        return ret\n\n    def blacklist_add(self, client_address, expire_seconds=0):\n        \"\"\"\n        Blacklist a client from the OSDs\n\n        :param client_address: client address\n        :type client_address: str\n        :param expire_seconds: number of seconds to blacklist\n        :type expire_seconds: int\n\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_state(\"connected\")\n        client_address =  cstr(client_address, 'client_address')\n        cdef:\n            uint32_t _expire_seconds = expire_seconds\n            char *_client_address = client_address\n\n        with nogil:\n            ret = rados_blacklist_add(self.cluster, _client_address, _expire_seconds)\n        if ret < 0:\n            raise make_ex(ret, \"error blacklisting client '%s'\" % client_address)\n\n    def monitor_log(self, level, callback, arg):\n        if level not in MONITOR_LEVELS:\n            raise LogicError(\"invalid monitor level \" + level)\n        if callback is not None and not callable(callback):\n            raise LogicError(\"callback must be a callable function or None\")\n\n        level = cstr(level, 'level')\n        cdef char *_level = level\n\n        if callback is None:\n            with nogil:\n                r = rados_monitor_log(self.cluster, <const char*>_level, NULL, NULL)\n            self.monitor_callback = None\n            self.monitor_callback2 = None\n            return\n\n        cb = (callback, arg)\n        cdef PyObject* _arg = <PyObject*>cb\n        with nogil:\n            r = rados_monitor_log(self.cluster, <const char*>_level,\n                                  <rados_log_callback_t>&__monitor_callback, _arg)\n\n        if r:\n            raise make_ex(r, 'error calling rados_monitor_log')\n        # NOTE(sileht): Prevents the callback method from being garbage collected\n        self.monitor_callback = cb\n        self.monitor_callback2 = None\n\n    def monitor_log2(self, level, callback, arg):\n        if level not in MONITOR_LEVELS:\n            raise LogicError(\"invalid monitor level \" + level)\n        if callback is not None and not callable(callback):\n            raise LogicError(\"callback must be a callable function or None\")\n\n        level = cstr(level, 'level')\n        cdef char *_level = level\n\n        if callback is None:\n            with nogil:\n                r = rados_monitor_log2(self.cluster, <const char*>_level, NULL, NULL)\n            self.monitor_callback = None\n            self.monitor_callback2 = None\n            return\n\n        cb = (callback, arg)\n        cdef PyObject* _arg = <PyObject*>cb\n        with nogil:\n            r = rados_monitor_log2(self.cluster, <const char*>_level,\n                                  <rados_log_callback2_t>&__monitor_callback2, _arg)\n\n        if r:\n            raise make_ex(r, 'error calling rados_monitor_log')\n        # NOTE(sileht): Prevents the callback method from being garbage collected\n        self.monitor_callback = None\n        self.monitor_callback2 = cb\n\n    @requires(('service', str_type), ('daemon', str_type), ('metadata', dict))\n    def service_daemon_register(self, service, daemon, metadata):\n        \"\"\"\n        :param str service: service name (e.g. \"rgw\")\n        :param str daemon: daemon name (e.g. \"gwfoo\")\n        :param dict metadata: static metadata about the register daemon\n               (e.g., the version of Ceph, the kernel version.)\n        \"\"\"\n        service = cstr(service, 'service')\n        daemon = cstr(daemon, 'daemon')\n        metadata_dict = '\\0'.join(chain.from_iterable(metadata.items()))\n        metadata_dict += '\\0'\n        cdef:\n            char *_service = service\n            char *_daemon = daemon\n            char *_metadata = metadata_dict\n\n        with nogil:\n            ret = rados_service_register(self.cluster, _service, _daemon, _metadata)\n        if ret != 0:\n            raise make_ex(ret, \"error calling service_register()\")\n\n    @requires(('metadata', dict))\n    def service_daemon_update(self, status):\n        status_dict = '\\0'.join(chain.from_iterable(status.items()))\n        status_dict += '\\0'\n        cdef:\n            char *_status = status_dict\n\n        with nogil:\n            ret = rados_service_update_status(self.cluster, _status)\n        if ret != 0:\n            raise make_ex(ret, \"error calling service_daemon_update()\")\n\n\ncdef class OmapIterator(object):\n    \"\"\"Omap iterator\"\"\"\n\n    cdef public Ioctx ioctx\n    cdef rados_omap_iter_t ctx\n\n    def __cinit__(self, Ioctx ioctx):\n        self.ioctx = ioctx\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next key-value pair in the object\n        :returns: next rados.OmapItem\n        \"\"\"\n        cdef:\n            char *key_ = NULL\n            char *val_ = NULL\n            size_t len_\n\n        with nogil:\n            ret = rados_omap_get_next(self.ctx, &key_, &val_, &len_)\n\n        if ret != 0:\n            raise make_ex(ret, \"error iterating over the omap\")\n        if key_ == NULL:\n            raise StopIteration()\n        key = decode_cstr(key_)\n        val = None\n        if val_ != NULL:\n            val = val_[:len_]\n        return (key, val)\n\n    def __dealloc__(self):\n        with nogil:\n            rados_omap_get_end(self.ctx)\n\n\ncdef class ObjectIterator(object):\n    \"\"\"rados.Ioctx Object iterator\"\"\"\n\n    cdef rados_list_ctx_t ctx\n\n    cdef public object ioctx\n\n    def __cinit__(self, Ioctx ioctx):\n        self.ioctx = ioctx\n\n        with nogil:\n            ret = rados_nobjects_list_open(ioctx.io, &self.ctx)\n        if ret < 0:\n            raise make_ex(ret, \"error iterating over the objects in ioctx '%s'\"\n                          % self.ioctx.name)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next object name and locator in the pool\n\n        :raises: StopIteration\n        :returns: next rados.Ioctx Object\n        \"\"\"\n        cdef:\n            const char *key_ = NULL\n            const char *locator_ = NULL\n            const char *nspace_ = NULL\n\n        with nogil:\n            ret = rados_nobjects_list_next(self.ctx, &key_, &locator_, &nspace_)\n\n        if ret < 0:\n            raise StopIteration()\n\n        key = decode_cstr(key_)\n        locator = decode_cstr(locator_) if locator_ != NULL else None\n        nspace = decode_cstr(nspace_) if nspace_ != NULL else None\n        return Object(self.ioctx, key, locator, nspace)\n\n    def __dealloc__(self):\n        with nogil:\n            rados_nobjects_list_close(self.ctx)\n\n\ncdef class XattrIterator(object):\n    \"\"\"Extended attribute iterator\"\"\"\n\n    cdef rados_xattrs_iter_t it\n    cdef char* _oid\n\n    cdef public Ioctx ioctx\n    cdef public object oid\n\n    def __cinit__(self, Ioctx ioctx, oid):\n        self.ioctx = ioctx\n        self.oid = cstr(oid, 'oid')\n        self._oid = self.oid\n\n        with nogil:\n            ret = rados_getxattrs(ioctx.io,  self._oid, &self.it)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to get rados xattrs for object %r\" % oid)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next xattr on the object\n\n        :raises: StopIteration\n        :returns: pair - of name and value of the next Xattr\n        \"\"\"\n        cdef:\n            const char *name_ = NULL\n            const char *val_ = NULL\n            size_t len_ = 0\n\n        with nogil:\n            ret = rados_getxattrs_next(self.it, &name_, &val_, &len_)\n        if ret != 0:\n            raise make_ex(ret, \"error iterating over the extended attributes \\\nin '%s'\" % self.oid)\n        if name_ == NULL:\n            raise StopIteration()\n        name = decode_cstr(name_)\n        val = val_[:len_]\n        return (name, val)\n\n    def __dealloc__(self):\n        with nogil:\n            rados_getxattrs_end(self.it)\n\n\ncdef class SnapIterator(object):\n    \"\"\"Snapshot iterator\"\"\"\n\n    cdef public Ioctx ioctx\n\n    cdef rados_snap_t *snaps\n    cdef int max_snap\n    cdef int cur_snap\n\n    def __cinit__(self, Ioctx ioctx):\n        self.ioctx = ioctx\n        # We don't know how big a buffer we need until we've called the\n        # function. So use the exponential doubling strategy.\n        cdef int num_snaps = 10\n        while True:\n            self.snaps = <rados_snap_t*>realloc_chk(self.snaps,\n                                                    num_snaps *\n                                                    sizeof(rados_snap_t))\n\n            with nogil:\n                ret = rados_ioctx_snap_list(ioctx.io, self.snaps, num_snaps)\n            if ret >= 0:\n                self.max_snap = ret\n                break\n            elif ret != -errno.ERANGE:\n                raise make_ex(ret, \"error calling rados_snap_list for \\\nioctx '%s'\" % self.ioctx.name)\n            num_snaps = num_snaps * 2\n        self.cur_snap = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next Snapshot\n\n        :raises: :class:`Error`, StopIteration\n        :returns: Snap - next snapshot\n        \"\"\"\n        if self.cur_snap >= self.max_snap:\n            raise StopIteration\n\n        cdef:\n            rados_snap_t snap_id = self.snaps[self.cur_snap]\n            int name_len = 10\n            char *name = NULL\n\n        try:\n            while True:\n                name = <char *>realloc_chk(name, name_len)\n                with nogil:\n                    ret = rados_ioctx_snap_get_name(self.ioctx.io, snap_id, name, name_len)\n                if ret == 0:\n                    break\n                elif ret != -errno.ERANGE:\n                    raise make_ex(ret, \"rados_snap_get_name error\")\n                else:\n                    name_len = name_len * 2\n\n            snap = Snap(self.ioctx, decode_cstr(name[:name_len]).rstrip('\\0'), snap_id)\n            self.cur_snap = self.cur_snap + 1\n            return snap\n        finally:\n            free(name)\n\n\ncdef class Snap(object):\n    \"\"\"Snapshot object\"\"\"\n    cdef public Ioctx ioctx\n    cdef public object name\n\n    # NOTE(sileht): old API was storing the ctypes object\n    # instead of the value ....\n    cdef public rados_snap_t snap_id\n\n    def __cinit__(self, Ioctx ioctx, object name, rados_snap_t snap_id):\n        self.ioctx = ioctx\n        self.name = name\n        self.snap_id = snap_id\n\n    def __str__(self):\n        return \"rados.Snap(ioctx=%s,name=%s,snap_id=%d)\" \\\n            % (str(self.ioctx), self.name, self.snap_id)\n\n    def get_timestamp(self):\n        \"\"\"\n        Find when a snapshot in the current pool occurred\n\n        :raises: :class:`Error`\n        :returns: datetime - the data and time the snapshot was created\n        \"\"\"\n        cdef time_t snap_time\n\n        with nogil:\n            ret = rados_ioctx_snap_get_stamp(self.ioctx.io, self.snap_id, &snap_time)\n        if ret != 0:\n            raise make_ex(ret, \"rados_ioctx_snap_get_stamp error\")\n        return datetime.fromtimestamp(snap_time)\n\n\ncdef class Completion(object):\n    \"\"\"completion object\"\"\"\n\n    cdef public:\n         Ioctx ioctx\n         object oncomplete\n         object onsafe\n\n    cdef:\n         rados_callback_t complete_cb\n         rados_callback_t safe_cb\n         rados_completion_t rados_comp\n         PyObject* buf\n\n    def __cinit__(self, Ioctx ioctx, object oncomplete, object onsafe):\n        self.oncomplete = oncomplete\n        self.onsafe = onsafe\n        self.ioctx = ioctx\n\n    def is_safe(self):\n        \"\"\"\n        Is an asynchronous operation safe?\n\n        This does not imply that the safe callback has finished.\n\n        :returns: True if the operation is safe\n        \"\"\"\n        with nogil:\n            ret = rados_aio_is_safe(self.rados_comp)\n        return ret == 1\n\n    def is_complete(self):\n        \"\"\"\n        Has an asynchronous operation completed?\n\n        This does not imply that the safe callback has finished.\n\n        :returns: True if the operation is completed\n        \"\"\"\n        with nogil:\n            ret = rados_aio_is_complete(self.rados_comp)\n        return ret == 1\n\n    def wait_for_safe(self):\n        \"\"\"\n        Wait for an asynchronous operation to be marked safe\n\n        This does not imply that the safe callback has finished.\n        \"\"\"\n        with nogil:\n            rados_aio_wait_for_safe(self.rados_comp)\n\n    def wait_for_complete(self):\n        \"\"\"\n        Wait for an asynchronous operation to complete\n\n        This does not imply that the complete callback has finished.\n        \"\"\"\n        with nogil:\n            rados_aio_wait_for_complete(self.rados_comp)\n\n    def wait_for_safe_and_cb(self):\n        \"\"\"\n        Wait for an asynchronous operation to be marked safe and for\n        the safe callback to have returned\n        \"\"\"\n        with nogil:\n            rados_aio_wait_for_safe_and_cb(self.rados_comp)\n\n    def wait_for_complete_and_cb(self):\n        \"\"\"\n        Wait for an asynchronous operation to complete and for the\n        complete callback to have returned\n\n        :returns:  whether the operation is completed\n        \"\"\"\n        with nogil:\n            ret = rados_aio_wait_for_complete_and_cb(self.rados_comp)\n        return ret\n\n    def get_return_value(self):\n        \"\"\"\n        Get the return value of an asychronous operation\n\n        The return value is set when the operation is complete or safe,\n        whichever comes first.\n\n        :returns: int - return value of the operation\n        \"\"\"\n        with nogil:\n            ret = rados_aio_get_return_value(self.rados_comp)\n        return ret\n\n    def __dealloc__(self):\n        \"\"\"\n        Release a completion\n\n        Call this when you no longer need the completion. It may not be\n        freed immediately if the operation is not acked and committed.\n        \"\"\"\n        ref.Py_XDECREF(self.buf)\n        self.buf = NULL\n        if self.rados_comp != NULL:\n            with nogil:\n                rados_aio_release(self.rados_comp)\n                self.rados_comp = NULL\n\n    def _complete(self):\n        self.oncomplete(self)\n        with self.ioctx.lock:\n            if self.oncomplete:\n                self.ioctx.complete_completions.remove(self)\n\n    def _safe(self):\n        self.onsafe(self)\n        with self.ioctx.lock:\n            if self.onsafe:\n                self.ioctx.safe_completions.remove(self)\n\n    def _cleanup(self):\n        with self.ioctx.lock:\n            if self.oncomplete:\n                self.ioctx.complete_completions.remove(self)\n            if self.onsafe:\n                self.ioctx.safe_completions.remove(self)\n\n\nclass OpCtx(object):\n    def __enter__(self):\n        return self.create()\n\n    def __exit__(self, type, msg, traceback):\n        self.release()\n\n\ncdef class WriteOp(object):\n    cdef rados_write_op_t write_op\n\n    def create(self):\n        with nogil:\n            self.write_op = rados_create_write_op()\n        return self\n\n    def release(self):\n        with nogil:\n            rados_release_write_op(self.write_op)\n\n    @requires(('exclusive', opt(int)))\n    def new(self, exclusive=None):\n        \"\"\"\n        Create the object.\n        \"\"\"\n\n        cdef:\n            int _exclusive = exclusive\n\n        with nogil:\n            rados_write_op_create(self.write_op, _exclusive, NULL)\n\n\n    def remove(self):\n        \"\"\"\n        Remove object.\n        \"\"\"\n        with nogil:\n            rados_write_op_remove(self.write_op)\n\n    @requires(('flags', int))\n    def set_flags(self, flags=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        Set flags for the last operation added to this write_op.\n        :para flags: flags to apply to the last operation\n        :type flags: int\n        \"\"\"\n\n        cdef:\n            int _flags = flags\n\n        with nogil:\n            rados_write_op_set_flags(self.write_op, _flags)\n\n    @requires(('to_write', bytes))\n    def append(self, to_write):\n        \"\"\"\n        Append data to an object synchronously\n        :param to_write: data to write\n        :type to_write: bytes\n        \"\"\"\n\n        cdef:\n            char *_to_write = to_write\n            size_t length = len(to_write)\n\n        with nogil:\n            rados_write_op_append(self.write_op, _to_write, length)\n\n    @requires(('to_write', bytes))\n    def write_full(self, to_write):\n        \"\"\"\n        Write whole object, atomically replacing it.\n        :param to_write: data to write\n        :type to_write: bytes\n        \"\"\"\n\n        cdef:\n            char *_to_write = to_write\n            size_t length = len(to_write)\n\n        with nogil:\n            rados_write_op_write_full(self.write_op, _to_write, length)\n\n    @requires(('to_write', bytes), ('offset', int))\n    def write(self, to_write, offset=0):\n        \"\"\"\n        Write to offset.\n        :param to_write: data to write\n        :type to_write: bytes\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n        \"\"\"\n\n        cdef:\n            char *_to_write = to_write\n            size_t length = len(to_write)\n            uint64_t _offset = offset\n\n        with nogil:\n            rados_write_op_write(self.write_op, _to_write, length, _offset)\n\n    @requires(('offset', int), ('length', int))\n    def zero(self, offset, length):\n        \"\"\"\n        Zero part of an object.\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n        :param offset: number of zero to write\n        :type offset: int\n        \"\"\"\n\n        cdef:\n            size_t _length = length\n            uint64_t _offset = offset\n\n        with nogil:\n            rados_write_op_zero(self.write_op, _length, _offset)\n\n    @requires(('offset', int))\n    def truncate(self, offset):\n        \"\"\"\n        Truncate an object.\n        :param offset: byte offset in the object to begin truncating at\n        :type offset: int\n        \"\"\"\n\n        cdef:\n            uint64_t _offset = offset\n\n        with nogil:\n            rados_write_op_truncate(self.write_op,  _offset)\n\n\nclass WriteOpCtx(WriteOp, OpCtx):\n    \"\"\"write operation context manager\"\"\"\n\n\ncdef class ReadOp(object):\n    cdef rados_read_op_t read_op\n\n    def create(self):\n        with nogil:\n            self.read_op = rados_create_read_op()\n        return self\n\n    def release(self):\n        with nogil:\n            rados_release_read_op(self.read_op)\n\n    @requires(('flags', int))\n    def set_flags(self, flags=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        Set flags for the last operation added to this read_op.\n        :para flags: flags to apply to the last operation\n        :type flags: int\n        \"\"\"\n\n        cdef:\n            int _flags = flags\n\n        with nogil:\n            rados_read_op_set_flags(self.read_op, _flags)\n\n\nclass ReadOpCtx(ReadOp, OpCtx):\n    \"\"\"read operation context manager\"\"\"\n\n\ncdef int __aio_safe_cb(rados_completion_t completion, void *args) with gil:\n    \"\"\"\n    Callback to onsafe() for asynchronous operations\n    \"\"\"\n    cdef object cb = <object>args\n    cb._safe()\n    return 0\n\n\ncdef int __aio_complete_cb(rados_completion_t completion, void *args) with gil:\n    \"\"\"\n    Callback to oncomplete() for asynchronous operations\n    \"\"\"\n    cdef object cb = <object>args\n    cb._complete()\n    return 0\n\n\ncdef class Ioctx(object):\n    \"\"\"rados.Ioctx object\"\"\"\n    # NOTE(sileht): attributes declared in .pyd\n\n    def __init__(self, name):\n        self.name = name\n        self.state = \"open\"\n\n        self.locator_key = \"\"\n        self.nspace = \"\"\n        self.lock = threading.Lock()\n        self.safe_completions = []\n        self.complete_completions = []\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        self.close()\n        return False\n\n    def __dealloc__(self):\n        self.close()\n\n    def __track_completion(self, completion_obj):\n        if completion_obj.oncomplete:\n            with self.lock:\n                self.complete_completions.append(completion_obj)\n        if completion_obj.onsafe:\n            with self.lock:\n                self.safe_completions.append(completion_obj)\n\n    def __get_completion(self, oncomplete, onsafe):\n        \"\"\"\n        Constructs a completion to use with asynchronous operations\n\n        :param oncomplete: what to do when the write is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the write is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        completion_obj = Completion(self, oncomplete, onsafe)\n\n        cdef:\n            rados_callback_t complete_cb = NULL\n            rados_callback_t safe_cb = NULL\n            rados_completion_t completion\n            PyObject* p_completion_obj= <PyObject*>completion_obj\n\n        if oncomplete:\n            complete_cb = <rados_callback_t>&__aio_complete_cb\n        if onsafe:\n            safe_cb = <rados_callback_t>&__aio_safe_cb\n\n        with nogil:\n            ret = rados_aio_create_completion(p_completion_obj, complete_cb, safe_cb,\n                                              &completion)\n        if ret < 0:\n            raise make_ex(ret, \"error getting a completion\")\n\n        completion_obj.rados_comp = completion\n        return completion_obj\n\n    @requires(('object_name', str_type), ('oncomplete', opt(Callable)))\n    def aio_stat(self, object_name, oncomplete):\n        \"\"\"\n        Asynchronously get object stats (size/mtime)\n\n        oncomplete will be called with the returned size and mtime\n        as well as the completion:\n\n        oncomplete(completion, size, mtime)\n\n        :param object_name: the name of the object to get stats from\n        :type object_name: str\n        :param oncomplete: what to do when the stat is complete\n        :type oncomplete: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char *_object_name = object_name\n            uint64_t psize\n            time_t pmtime\n\n        def oncomplete_(completion_v):\n            cdef Completion _completion_v = completion_v\n            return_value = _completion_v.get_return_value()\n            if return_value >= 0:\n                return oncomplete(_completion_v, psize, time.localtime(pmtime))\n            else:\n                return oncomplete(_completion_v, None, None)\n\n        completion = self.__get_completion(oncomplete_, None)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_stat(self.io, _object_name, completion.rados_comp,\n                                 &psize, &pmtime)\n\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error stating %s\" % object_name)\n        return completion\n\n    @requires(('object_name', str_type), ('to_write', bytes), ('offset', int),\n              ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)))\n    def aio_write(self, object_name, to_write, offset=0,\n                  oncomplete=None, onsafe=None):\n        \"\"\"\n        Write data to an object asynchronously\n\n        Queues the write and returns.\n\n        :param object_name: name of the object\n        :type object_name: str\n        :param to_write: data to write\n        :type to_write: bytes\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n        :param oncomplete: what to do when the write is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the write is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n            char* _to_write = to_write\n            size_t size = len(to_write)\n            uint64_t _offset = offset\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_write(self.io, _object_name, completion.rados_comp,\n                                _to_write, size, _offset)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error writing object %s\" % object_name)\n        return completion\n\n    @requires(('object_name', str_type), ('to_write', bytes), ('oncomplete', opt(Callable)),\n              ('onsafe', opt(Callable)))\n    def aio_write_full(self, object_name, to_write,\n                       oncomplete=None, onsafe=None):\n        \"\"\"\n        Asychronously write an entire object\n\n        The object is filled with the provided data. If the object exists,\n        it is atomically truncated and then written.\n        Queues the write and returns.\n\n        :param object_name: name of the object\n        :type object_name: str\n        :param to_write: data to write\n        :type to_write: str\n        :param oncomplete: what to do when the write is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the write is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n            char* _to_write = to_write\n            size_t size = len(to_write)\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_write_full(self.io, _object_name,\n                                    completion.rados_comp,\n                                    _to_write, size)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error writing object %s\" % object_name)\n        return completion\n\n    @requires(('object_name', str_type), ('to_append', bytes), ('oncomplete', opt(Callable)),\n              ('onsafe', opt(Callable)))\n    def aio_append(self, object_name, to_append, oncomplete=None, onsafe=None):\n        \"\"\"\n        Asychronously append data to an object\n\n        Queues the write and returns.\n\n        :param object_name: name of the object\n        :type object_name: str\n        :param to_append: data to append\n        :type to_append: str\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n        :param oncomplete: what to do when the write is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the write is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n            char* _to_append = to_append\n            size_t size = len(to_append)\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_append(self.io, _object_name,\n                                completion.rados_comp,\n                                _to_append, size)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error appending object %s\" % object_name)\n        return completion\n\n    def aio_flush(self):\n        \"\"\"\n        Block until all pending writes in an io context are safe\n\n        :raises: :class:`Error`\n        \"\"\"\n        with nogil:\n            ret = rados_aio_flush(self.io)\n        if ret < 0:\n            raise make_ex(ret, \"error flushing\")\n\n    @requires(('object_name', str_type), ('length', int), ('offset', int),\n              ('oncomplete', opt(Callable)))\n    def aio_read(self, object_name, length, offset, oncomplete):\n        \"\"\"\n        Asychronously read data from an object\n\n        oncomplete will be called with the returned read value as\n        well as the completion:\n\n        oncomplete(completion, data_read)\n\n        :param object_name: name of the object to read from\n        :type object_name: str\n        :param length: the number of bytes to read\n        :type length: int\n        :param offset: byte offset in the object to begin reading from\n        :type offset: int\n        :param oncomplete: what to do when the read is complete\n        :type oncomplete: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n            uint64_t _offset = offset\n\n            char *ref_buf\n            size_t _length = length\n\n        def oncomplete_(completion_v):\n            cdef Completion _completion_v = completion_v\n            return_value = _completion_v.get_return_value()\n            if return_value > 0 and return_value != length:\n                _PyBytes_Resize(&_completion_v.buf, return_value)\n            return oncomplete(_completion_v, <object>_completion_v.buf if return_value >= 0 else None)\n\n        completion = self.__get_completion(oncomplete_, None)\n        completion.buf = PyBytes_FromStringAndSize(NULL, length)\n        ret_buf = PyBytes_AsString(completion.buf)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_read(self.io, _object_name, completion.rados_comp,\n                                ret_buf, _length, _offset)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error reading %s\" % object_name)\n        return completion\n\n    @requires(('object_name', str_type), ('cls', str_type), ('method', str_type),\n              ('data', bytes), ('length', int),\n              ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)))\n    def aio_execute(self, object_name, cls, method, data,\n                    length=8192, oncomplete=None, onsafe=None):\n        \"\"\"\n        Asynchronously execute an OSD class method on an object.\n\n        oncomplete and onsafe will be called with the data returned from\n        the plugin as well as the completion:\n\n        oncomplete(completion, data)\n        onsafe(completion, data)\n\n        :param object_name: name of the object\n        :type object_name: str\n        :param cls: name of the object class\n        :type cls: str\n        :param method: name of the method\n        :type method: str\n        :param data: input data\n        :type data: bytes\n        :param length: size of output buffer in bytes (default=8192)\n        :type length: int\n        :param oncomplete: what to do when the execution is complete\n        :type oncomplete: completion\n        :param onsafe:  what to do when the execution is safe and complete\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        object_name = cstr(object_name, 'object_name')\n        cls = cstr(cls, 'cls')\n        method = cstr(method, 'method')\n        cdef:\n            Completion completion\n            char *_object_name = object_name\n            char *_cls = cls\n            char *_method = method\n            char *_data = data\n            size_t _data_len = len(data)\n\n            char *ref_buf\n            size_t _length = length\n\n        def oncomplete_(completion_v):\n            cdef Completion _completion_v = completion_v\n            return_value = _completion_v.get_return_value()\n            if return_value > 0 and return_value != length:\n                _PyBytes_Resize(&_completion_v.buf, return_value)\n            return oncomplete(_completion_v, <object>_completion_v.buf if return_value >= 0 else None)\n\n        def onsafe_(completion_v):\n            cdef Completion _completion_v = completion_v\n            return_value = _completion_v.get_return_value()\n            return onsafe(_completion_v, <object>_completion_v.buf if return_value >= 0 else None)\n\n        completion = self.__get_completion(oncomplete_ if oncomplete else None, onsafe_ if onsafe else None)\n        completion.buf = PyBytes_FromStringAndSize(NULL, length)\n        ret_buf = PyBytes_AsString(completion.buf)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_exec(self.io, _object_name, completion.rados_comp,\n                                 _cls, _method, _data, _data_len, ret_buf, _length)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error executing %s::%s on %s\" % (cls, method, object_name))\n        return completion\n\n    @requires(('object_name', str_type), ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)))\n    def aio_remove(self, object_name, oncomplete=None, onsafe=None):\n        \"\"\"\n        Asychronously remove an object\n\n        :param object_name: name of the object to remove\n        :type object_name: str\n        :param oncomplete: what to do when the remove is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the remove is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n        object_name = cstr(object_name, 'object_name')\n\n        cdef:\n            Completion completion\n            char* _object_name = object_name\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n        with nogil:\n            ret = rados_aio_remove(self.io, _object_name,\n                                completion.rados_comp)\n        if ret < 0:\n            completion._cleanup()\n            raise make_ex(ret, \"error removing %s\" % object_name)\n        return completion\n\n    def require_ioctx_open(self):\n        \"\"\"\n        Checks if the rados.Ioctx object state is 'open'\n\n        :raises: IoctxStateError\n        \"\"\"\n        if self.state != \"open\":\n            raise IoctxStateError(\"The pool is %s\" % self.state)\n\n    def change_auid(self, auid):\n        \"\"\"\n        Attempt to change an io context's associated auid \"owner.\"\n\n        Requires that you have write permission on both the current and new\n        auid.\n\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n\n        cdef:\n            uint64_t _auid = auid\n\n        with nogil:\n            ret = rados_ioctx_pool_set_auid(self.io, _auid)\n        if ret < 0:\n            raise make_ex(ret, \"error changing auid of '%s' to %d\"\n                          % (self.name, auid))\n\n    @requires(('loc_key', str_type))\n    def set_locator_key(self, loc_key):\n        \"\"\"\n        Set the key for mapping objects to pgs within an io context.\n\n        The key is used instead of the object name to determine which\n        placement groups an object is put in. This affects all subsequent\n        operations of the io context - until a different locator key is\n        set, all objects in this io context will be placed in the same pg.\n\n        :param loc_key: the key to use as the object locator, or NULL to discard\n            any previously set key\n        :type loc_key: str\n\n        :raises: :class:`TypeError`\n        \"\"\"\n        self.require_ioctx_open()\n        cloc_key = cstr(loc_key, 'loc_key')\n        cdef char *_loc_key = cloc_key\n        with nogil:\n            rados_ioctx_locator_set_key(self.io, _loc_key)\n        self.locator_key = loc_key\n\n    def get_locator_key(self):\n        \"\"\"\n        Get the locator_key of context\n\n        :returns: locator_key\n        \"\"\"\n        return self.locator_key\n\n    @requires(('snap_id', long))\n    def set_read(self, snap_id):\n        \"\"\"\n        Set the snapshot for reading objects.\n\n        To stop to read from snapshot, use set_read(LIBRADOS_SNAP_HEAD)\n\n        :param snap_id: the snapshot Id\n        :type snap_id: int\n\n        :raises: :class:`TypeError`\n        \"\"\"\n        self.require_ioctx_open()\n        cdef rados_snap_t _snap_id = snap_id\n        with nogil:\n            rados_ioctx_snap_set_read(self.io, _snap_id)\n\n    @requires(('nspace', str_type))\n    def set_namespace(self, nspace):\n        \"\"\"\n        Set the namespace for objects within an io context.\n\n        The namespace in addition to the object name fully identifies\n        an object. This affects all subsequent operations of the io context\n        - until a different namespace is set, all objects in this io context\n        will be placed in the same namespace.\n\n        :param nspace: the namespace to use, or None/\"\" for the default namespace\n        :type nspace: str\n\n        :raises: :class:`TypeError`\n        \"\"\"\n        self.require_ioctx_open()\n        if nspace is None:\n            nspace = \"\"\n        cnspace = cstr(nspace, 'nspace')\n        cdef char *_nspace = cnspace\n        with nogil:\n            rados_ioctx_set_namespace(self.io, _nspace)\n        self.nspace = nspace\n\n    def get_namespace(self):\n        \"\"\"\n        Get the namespace of context\n\n        :returns: namespace\n        \"\"\"\n        return self.nspace\n\n    def close(self):\n        \"\"\"\n        Close a rados.Ioctx object.\n\n        This just tells librados that you no longer need to use the io context.\n        It may not be freed immediately if there are pending asynchronous\n        requests on it, but you should not use an io context again after\n        calling this function on it.\n        \"\"\"\n        if self.state == \"open\":\n            self.require_ioctx_open()\n            with nogil:\n                rados_ioctx_destroy(self.io)\n            self.state = \"closed\"\n\n\n    @requires(('key', str_type), ('data', bytes))\n    def write(self, key, data, offset=0):\n        \"\"\"\n        Write data to an object synchronously\n\n        :param key: name of the object\n        :type key: str\n        :param data: data to write\n        :type data: bytes\n        :param offset: byte offset in the object to begin writing at\n        :type offset: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`LogicError`\n        :returns: int - 0 on success\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            char *_data = data\n            size_t length = len(data)\n            uint64_t _offset = offset\n\n        with nogil:\n            ret = rados_write(self.io, _key, _data, length, _offset)\n        if ret == 0:\n            return ret\n        elif ret < 0:\n            raise make_ex(ret, \"Ioctx.write(%s): failed to write %s\"\n                          % (self.name, key))\n        else:\n            raise LogicError(\"Ioctx.write(%s): rados_write \\\nreturned %d, but should return zero on success.\" % (self.name, ret))\n\n    @requires(('key', str_type), ('data', bytes))\n    def write_full(self, key, data):\n        \"\"\"\n        Write an entire object synchronously.\n\n        The object is filled with the provided data. If the object exists,\n        it is atomically truncated and then written.\n\n        :param key: name of the object\n        :type key: str\n        :param data: data to write\n        :type data: bytes\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: int - 0 on success\n        \"\"\"\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            char *_data = data\n            size_t length = len(data)\n\n        with nogil:\n            ret = rados_write_full(self.io, _key, _data, length)\n        if ret == 0:\n            return ret\n        elif ret < 0:\n            raise make_ex(ret, \"Ioctx.write_full(%s): failed to write %s\"\n                          % (self.name, key))\n        else:\n            raise LogicError(\"Ioctx.write_full(%s): rados_write_full \\\nreturned %d, but should return zero on success.\" % (self.name, ret))\n\n    @requires(('key', str_type), ('data', bytes))\n    def append(self, key, data):\n        \"\"\"\n        Append data to an object synchronously\n\n        :param key: name of the object\n        :type key: str\n        :param data: data to write\n        :type data: bytes\n\n        :raises: :class:`TypeError`\n        :raises: :class:`LogicError`\n        :returns: int - 0 on success\n        \"\"\"\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            char *_data = data\n            size_t length = len(data)\n\n        with nogil:\n            ret = rados_append(self.io, _key, _data, length)\n        if ret == 0:\n            return ret\n        elif ret < 0:\n            raise make_ex(ret, \"Ioctx.append(%s): failed to append %s\"\n                          % (self.name, key))\n        else:\n            raise LogicError(\"Ioctx.append(%s): rados_append \\\nreturned %d, but should return zero on success.\" % (self.name, ret))\n\n    @requires(('key', str_type))\n    def read(self, key, length=8192, offset=0):\n        \"\"\"\n        Read data from an object synchronously\n\n        :param key: name of the object\n        :type key: str\n        :param length: the number of bytes to read (default=8192)\n        :type length: int\n        :param offset: byte offset in the object to begin reading at\n        :type offset: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: str - data read from object\n        \"\"\"\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            char *ret_buf\n            uint64_t _offset = offset\n            size_t _length = length\n            PyObject* ret_s = NULL\n\n        ret_s = PyBytes_FromStringAndSize(NULL, length)\n        try:\n            ret_buf = PyBytes_AsString(ret_s)\n            with nogil:\n                ret = rados_read(self.io, _key, ret_buf, _length, _offset)\n            if ret < 0:\n                raise make_ex(ret, \"Ioctx.read(%s): failed to read %s\" % (self.name, key))\n\n            if ret != length:\n                _PyBytes_Resize(&ret_s, ret)\n\n            return <object>ret_s\n        finally:\n            # We DECREF unconditionally: the cast to object above will have\n            # INCREFed if necessary. This also takes care of exceptions,\n            # including if _PyString_Resize fails (that will free the string\n            # itself and set ret_s to NULL, hence XDECREF).\n            ref.Py_XDECREF(ret_s)\n\n    @requires(('key', str_type), ('cls', str_type), ('method', str_type), ('data', bytes))\n    def execute(self, key, cls, method, data, length=8192):\n        \"\"\"\n        Execute an OSD class method on an object.\n\n        :param key: name of the object\n        :type key: str\n        :param cls: name of the object class\n        :type cls: str\n        :param method: name of the method\n        :type method: str\n        :param data: input data\n        :type data: bytes\n        :param length: size of output buffer in bytes (default=8192)\n        :type length: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: (ret, method output)\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        cls = cstr(cls, 'cls')\n        method = cstr(method, 'method')\n        cdef:\n            char *_key = key\n            char *_cls = cls\n            char *_method = method\n            char *_data = data\n            size_t _data_len = len(data)\n\n            char *ref_buf\n            size_t _length = length\n            PyObject* ret_s = NULL\n\n        ret_s = PyBytes_FromStringAndSize(NULL, length)\n        try:\n            ret_buf = PyBytes_AsString(ret_s)\n            with nogil:\n                ret = rados_exec(self.io, _key, _cls, _method, _data,\n                                 _data_len, ret_buf, _length)\n            if ret < 0:\n                raise make_ex(ret, \"Ioctx.read(%s): failed to read %s\" % (self.name, key))\n\n            if ret != length:\n                _PyBytes_Resize(&ret_s, ret)\n\n            return ret, <object>ret_s\n        finally:\n            # We DECREF unconditionally: the cast to object above will have\n            # INCREFed if necessary. This also takes care of exceptions,\n            # including if _PyString_Resize fails (that will free the string\n            # itself and set ret_s to NULL, hence XDECREF).\n            ref.Py_XDECREF(ret_s)\n\n    def get_stats(self):\n        \"\"\"\n        Get pool usage statistics\n\n        :returns: dict - contains the following keys:\n\n            - ``num_bytes`` (int) - size of pool in bytes\n\n            - ``num_kb`` (int) - size of pool in kbytes\n\n            - ``num_objects`` (int) - number of objects in the pool\n\n            - ``num_object_clones`` (int) - number of object clones\n\n            - ``num_object_copies`` (int) - number of object copies\n\n            - ``num_objects_missing_on_primary`` (int) - number of objets\n                missing on primary\n\n            - ``num_objects_unfound`` (int) - number of unfound objects\n\n            - ``num_objects_degraded`` (int) - number of degraded objects\n\n            - ``num_rd`` (int) - bytes read\n\n            - ``num_rd_kb`` (int) - kbytes read\n\n            - ``num_wr`` (int) - bytes written\n\n            - ``num_wr_kb`` (int) - kbytes written\n        \"\"\"\n        self.require_ioctx_open()\n        cdef rados_pool_stat_t stats\n        with nogil:\n            ret = rados_ioctx_pool_stat(self.io, &stats)\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.get_stats(%s): get_stats failed\" % self.name)\n        return {'num_bytes': stats.num_bytes,\n                'num_kb': stats.num_kb,\n                'num_objects': stats.num_objects,\n                'num_object_clones': stats.num_object_clones,\n                'num_object_copies': stats.num_object_copies,\n                \"num_objects_missing_on_primary\": stats.num_objects_missing_on_primary,\n                \"num_objects_unfound\": stats.num_objects_unfound,\n                \"num_objects_degraded\": stats.num_objects_degraded,\n                \"num_rd\": stats.num_rd,\n                \"num_rd_kb\": stats.num_rd_kb,\n                \"num_wr\": stats.num_wr,\n                \"num_wr_kb\": stats.num_wr_kb}\n\n    @requires(('key', str_type))\n    def remove_object(self, key):\n        \"\"\"\n        Delete an object\n\n        This does not delete any snapshots of the object.\n\n        :param key: the name of the object to delete\n        :type key: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: bool - True on success\n        \"\"\"\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n\n        with nogil:\n            ret = rados_remove(self.io, _key)\n        if ret < 0:\n            raise make_ex(ret, \"Failed to remove '%s'\" % key)\n        return True\n\n    @requires(('key', str_type))\n    def trunc(self, key, size):\n        \"\"\"\n        Resize an object\n\n        If this enlarges the object, the new area is logically filled with\n        zeroes. If this shrinks the object, the excess data is removed.\n\n        :param key: the name of the object to resize\n        :type key: str\n        :param size: the new size of the object in bytes\n        :type size: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: int - 0 on success, otherwise raises error\n        \"\"\"\n\n        self.require_ioctx_open()\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            uint64_t _size = size\n\n        with nogil:\n            ret = rados_trunc(self.io, _key, _size)\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.trunc(%s): failed to truncate %s\" % (self.name, key))\n        return ret\n\n    @requires(('key', str_type))\n    def stat(self, key):\n        \"\"\"\n        Get object stats (size/mtime)\n\n        :param key: the name of the object to get stats from\n        :type key: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: (size,timestamp)\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        cdef:\n            char *_key = key\n            uint64_t psize\n            time_t pmtime\n\n        with nogil:\n            ret = rados_stat(self.io, _key, &psize, &pmtime)\n        if ret < 0:\n            raise make_ex(ret, \"Failed to stat %r\" % key)\n        return psize, time.localtime(pmtime)\n\n    @requires(('key', str_type), ('xattr_name', str_type))\n    def get_xattr(self, key, xattr_name):\n        \"\"\"\n        Get the value of an extended attribute on an object.\n\n        :param key: the name of the object to get xattr from\n        :type key: str\n        :param xattr_name: which extended attribute to read\n        :type xattr_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: str - value of the xattr\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        xattr_name = cstr(xattr_name, 'xattr_name')\n        cdef:\n            char *_key = key\n            char *_xattr_name = xattr_name\n            size_t ret_length = 4096\n            char *ret_buf = NULL\n\n        try:\n            while ret_length < 4096 * 1024 * 1024:\n                ret_buf = <char *>realloc_chk(ret_buf, ret_length)\n                with nogil:\n                    ret = rados_getxattr(self.io, _key, _xattr_name, ret_buf, ret_length)\n                if ret == -errno.ERANGE:\n                    ret_length *= 2\n                elif ret < 0:\n                    raise make_ex(ret, \"Failed to get xattr %r\" % xattr_name)\n                else:\n                    break\n            return ret_buf[:ret]\n        finally:\n            free(ret_buf)\n\n    @requires(('oid', str_type))\n    def get_xattrs(self, oid):\n        \"\"\"\n        Start iterating over xattrs on an object.\n\n        :param oid: the name of the object to get xattrs from\n        :type oid: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: XattrIterator\n        \"\"\"\n        self.require_ioctx_open()\n        return XattrIterator(self, oid)\n\n    @requires(('key', str_type), ('xattr_name', str_type), ('xattr_value', bytes))\n    def set_xattr(self, key, xattr_name, xattr_value):\n        \"\"\"\n        Set an extended attribute on an object.\n\n        :param key: the name of the object to set xattr to\n        :type key: str\n        :param xattr_name: which extended attribute to set\n        :type xattr_name: str\n        :param xattr_value: the value of the  extended attribute\n        :type xattr_value: bytes\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: bool - True on success, otherwise raise an error\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        xattr_name = cstr(xattr_name, 'xattr_name')\n        cdef:\n            char *_key = key\n            char *_xattr_name = xattr_name\n            char *_xattr_value = xattr_value\n            size_t _xattr_value_len = len(xattr_value)\n\n        with nogil:\n            ret = rados_setxattr(self.io, _key, _xattr_name,\n                                 _xattr_value, _xattr_value_len)\n        if ret < 0:\n            raise make_ex(ret, \"Failed to set xattr %r\" % xattr_name)\n        return True\n\n    @requires(('key', str_type), ('xattr_name', str_type))\n    def rm_xattr(self, key, xattr_name):\n        \"\"\"\n        Removes an extended attribute on from an object.\n\n        :param key: the name of the object to remove xattr from\n        :type key: str\n        :param xattr_name: which extended attribute to remove\n        :type xattr_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: bool - True on success, otherwise raise an error\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        xattr_name = cstr(xattr_name, 'xattr_name')\n        cdef:\n            char *_key = key\n            char *_xattr_name = xattr_name\n\n        with nogil:\n            ret = rados_rmxattr(self.io, _key, _xattr_name)\n        if ret < 0:\n            raise make_ex(ret, \"Failed to delete key %r xattr %r\" %\n                          (key, xattr_name))\n        return True\n\n    def list_objects(self):\n        \"\"\"\n        Get ObjectIterator on rados.Ioctx object.\n\n        :returns: ObjectIterator\n        \"\"\"\n        self.require_ioctx_open()\n        return ObjectIterator(self)\n\n    def list_snaps(self):\n        \"\"\"\n        Get SnapIterator on rados.Ioctx object.\n\n        :returns: SnapIterator\n        \"\"\"\n        self.require_ioctx_open()\n        return SnapIterator(self)\n\n    @requires(('snap_name', str_type))\n    def create_snap(self, snap_name):\n        \"\"\"\n        Create a pool-wide snapshot\n\n        :param snap_name: the name of the snapshot\n        :type snap_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        snap_name = cstr(snap_name, 'snap_name')\n        cdef char *_snap_name = snap_name\n\n        with nogil:\n            ret = rados_ioctx_snap_create(self.io, _snap_name)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to create snap %s\" % snap_name)\n\n    @requires(('snap_name', str_type))\n    def remove_snap(self, snap_name):\n        \"\"\"\n        Removes a pool-wide snapshot\n\n        :param snap_name: the name of the snapshot\n        :type snap_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        snap_name = cstr(snap_name, 'snap_name')\n        cdef char *_snap_name = snap_name\n\n        with nogil:\n            ret = rados_ioctx_snap_remove(self.io, _snap_name)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to remove snap %s\" % snap_name)\n\n    @requires(('snap_name', str_type))\n    def lookup_snap(self, snap_name):\n        \"\"\"\n        Get the id of a pool snapshot\n\n        :param snap_name: the name of the snapshot to lookop\n        :type snap_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        :returns: Snap - on success\n        \"\"\"\n        self.require_ioctx_open()\n        csnap_name = cstr(snap_name, 'snap_name')\n        cdef:\n            char *_snap_name = csnap_name\n            rados_snap_t snap_id\n\n        with nogil:\n            ret = rados_ioctx_snap_lookup(self.io, _snap_name, &snap_id)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to lookup snap %s\" % snap_name)\n        return Snap(self, snap_name, int(snap_id))\n\n    @requires(('oid', str_type), ('snap_name', str_type))\n    def snap_rollback(self, oid, snap_name):\n        \"\"\"\n        Rollback an object to a snapshot\n\n        :param oid: the name of the object\n        :type oid: str\n        :param snap_name: the name of the snapshot\n        :type snap_name: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        oid = cstr(oid, 'oid')\n        snap_name = cstr(snap_name, 'snap_name')\n        cdef:\n            char *_snap_name = snap_name\n            char *_oid = oid\n\n        with nogil:\n            ret = rados_ioctx_snap_rollback(self.io, _oid, _snap_name)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to rollback %s\" % oid)\n\n    def create_self_managed_snap(self):\n        \"\"\"\n        Creates a self-managed snapshot\n\n        :returns: snap id on success\n\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        cdef:\n            rados_snap_t _snap_id\n        with nogil:\n            ret = rados_ioctx_selfmanaged_snap_create(self.io, &_snap_id)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to create self-managed snapshot\")\n        return int(_snap_id)\n\n    @requires(('snap_id', int))\n    def remove_self_managed_snap(self, snap_id):\n        \"\"\"\n        Removes a self-managed snapshot\n\n        :param snap_id: the name of the snapshot\n        :type snap_id: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        cdef:\n            rados_snap_t _snap_id = snap_id\n        with nogil:\n            ret = rados_ioctx_selfmanaged_snap_remove(self.io, _snap_id)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to remove self-managed snapshot\")\n\n    def set_self_managed_snap_write(self, snaps):\n        \"\"\"\n        Updates the write context to the specified self-managed\n        snapshot ids.\n\n        :param snaps: all associated self-managed snapshot ids\n        :type snaps: list\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        sorted_snaps = []\n        snap_seq = 0\n        if snaps:\n            sorted_snaps = sorted([int(x) for x in snaps], reverse=True)\n            snap_seq = sorted_snaps[0]\n\n        cdef:\n            rados_snap_t _snap_seq = snap_seq\n            rados_snap_t *_snaps = NULL\n            int _num_snaps = len(sorted_snaps)\n        try:\n            _snaps = <rados_snap_t *>malloc(_num_snaps * sizeof(rados_snap_t))\n            for i in range(len(sorted_snaps)):\n                _snaps[i] = sorted_snaps[i]\n            with nogil:\n                ret = rados_ioctx_selfmanaged_snap_set_write_ctx(self.io,\n                                                                 _snap_seq,\n                                                                 _snaps,\n                                                                 _num_snaps)\n            if ret != 0:\n                raise make_ex(ret, \"Failed to update snapshot write context\")\n        finally:\n            free(_snaps)\n\n    @requires(('oid', str_type), ('snap_id', int))\n    def rollback_self_managed_snap(self, oid, snap_id):\n        \"\"\"\n        Rolls an specific object back to a self-managed snapshot revision\n\n        :param oid: the name of the object\n        :type oid: str\n        :param snap_id: the name of the snapshot\n        :type snap_id: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n        oid = cstr(oid, 'oid')\n        cdef:\n            char *_oid = oid\n            rados_snap_t _snap_id = snap_id\n        with nogil:\n            ret = rados_ioctx_selfmanaged_snap_rollback(self.io, _oid, _snap_id)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to rollback %s\" % oid)\n\n    def get_last_version(self):\n        \"\"\"\n        Return the version of the last object read or written to.\n\n        This exposes the internal version number of the last object read or\n        written via this io context\n\n        :returns: version of the last object used\n        \"\"\"\n        self.require_ioctx_open()\n        with nogil:\n            ret = rados_get_last_version(self.io)\n        return int(ret)\n\n    def create_write_op(self):\n        \"\"\"\n        create write operation object.\n        need call release_write_op after use\n        \"\"\"\n        return WriteOp().create()\n\n    def create_read_op(self):\n        \"\"\"\n        create read operation object.\n        need call release_read_op after use\n        \"\"\"\n        return ReadOp().create()\n\n    def release_write_op(self, write_op):\n        \"\"\"\n        release memory alloc by create_write_op\n        \"\"\"\n        write_op.release()\n\n    def release_read_op(self, read_op):\n        \"\"\"\n        release memory alloc by create_read_op\n        :para read_op: read_op object\n        :type: int\n        \"\"\"\n        read_op.release()\n\n    @requires(('write_op', WriteOp), ('keys', tuple), ('values', tuple))\n    def set_omap(self, write_op, keys, values):\n        \"\"\"\n        set keys values to write_op\n        :para write_op: write_operation object\n        :type write_op: WriteOp\n        :para keys: a tuple of keys\n        :type keys: tuple\n        :para values: a tuple of values\n        :type values: tuple\n        \"\"\"\n\n        if len(keys) != len(values):\n            raise Error(\"Rados(): keys and values must have the same number of items\")\n\n        keys = cstr_list(keys, 'keys')\n        cdef:\n            WriteOp _write_op = write_op\n            size_t key_num = len(keys)\n            char **_keys = to_bytes_array(keys)\n            char **_values = to_bytes_array(values)\n            size_t *_lens = to_csize_t_array([len(v) for v in values])\n\n        try:\n            with nogil:\n                rados_write_op_omap_set(_write_op.write_op,\n                                        <const char**>_keys,\n                                        <const char**>_values,\n                                        <const size_t*>_lens, key_num)\n        finally:\n            free(_keys)\n            free(_values)\n            free(_lens)\n\n    @requires(('write_op', WriteOp), ('oid', str_type), ('mtime', opt(int)), ('flags', opt(int)))\n    def operate_write_op(self, write_op, oid, mtime=0, flags=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        execute the real write operation\n        :para write_op: write operation object\n        :type write_op: WriteOp\n        :para oid: object name\n        :type oid: str\n        :para mtime: the time to set the mtime to, 0 for the current time\n        :type mtime: int\n        :para flags: flags to apply to the entire operation\n        :type flags: int\n        \"\"\"\n\n        oid = cstr(oid, 'oid')\n        cdef:\n            WriteOp _write_op = write_op\n            char *_oid = oid\n            time_t _mtime = mtime\n            int _flags = flags\n\n        with nogil:\n            ret = rados_write_op_operate(_write_op.write_op, self.io, _oid, &_mtime, _flags)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to operate write op for oid %s\" % oid)\n\n    @requires(('write_op', WriteOp), ('oid', str_type), ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)), ('mtime', opt(int)), ('flags', opt(int)))\n    def operate_aio_write_op(self, write_op, oid, oncomplete=None, onsafe=None, mtime=0, flags=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        execute the real write operation asynchronously\n        :para write_op: write operation object\n        :type write_op: WriteOp\n        :para oid: object name\n        :type oid: str\n        :param oncomplete: what to do when the remove is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the remove is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n        :para mtime: the time to set the mtime to, 0 for the current time\n        :type mtime: int\n        :para flags: flags to apply to the entire operation\n        :type flags: int\n\n        :raises: :class:`Error`\n        :returns: completion object\n        \"\"\"\n\n        oid = cstr(oid, 'oid')\n        cdef:\n            WriteOp _write_op = write_op\n            char *_oid = oid\n            Completion completion\n            time_t _mtime = mtime\n            int _flags = flags\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n\n        with nogil:\n            ret = rados_aio_write_op_operate(_write_op.write_op, self.io, completion.rados_comp, _oid,\n                                             &_mtime, _flags)\n        if ret != 0:\n            completion._cleanup()\n            raise make_ex(ret, \"Failed to operate aio write op for oid %s\" % oid)\n        return completion\n\n    @requires(('read_op', ReadOp), ('oid', str_type), ('flag', opt(int)))\n    def operate_read_op(self, read_op, oid, flag=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        execute the real read operation\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para oid: object name\n        :type oid: str\n        :para flag: flags to apply to the entire operation\n        :type flag: int\n        \"\"\"\n        oid = cstr(oid, 'oid')\n        cdef:\n            ReadOp _read_op = read_op\n            char *_oid = oid\n            int _flag = flag\n\n        with nogil:\n            ret = rados_read_op_operate(_read_op.read_op, self.io, _oid, _flag)\n        if ret != 0:\n            raise make_ex(ret, \"Failed to operate read op for oid %s\" % oid)\n\n    @requires(('read_op', ReadOp), ('oid', str_type), ('oncomplete', opt(Callable)), ('onsafe', opt(Callable)), ('flag', opt(int)))\n    def operate_aio_read_op(self, read_op, oid, oncomplete=None, onsafe=None, flag=LIBRADOS_OPERATION_NOFLAG):\n        \"\"\"\n        execute the real read operation\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para oid: object name\n        :type oid: str\n        :param oncomplete: what to do when the remove is safe and complete in memory\n            on all replicas\n        :type oncomplete: completion\n        :param onsafe:  what to do when the remove is safe and complete on storage\n            on all replicas\n        :type onsafe: completion\n        :para flag: flags to apply to the entire operation\n        :type flag: int\n        \"\"\"\n        oid = cstr(oid, 'oid')\n        cdef:\n            ReadOp _read_op = read_op\n            char *_oid = oid\n            Completion completion\n            int _flag = flag\n\n        completion = self.__get_completion(oncomplete, onsafe)\n        self.__track_completion(completion)\n\n        with nogil:\n            ret = rados_aio_read_op_operate(_read_op.read_op, self.io, completion.rados_comp, _oid, _flag)\n        if ret != 0:\n            completion._cleanup()\n            raise make_ex(ret, \"Failed to operate aio read op for oid %s\" % oid)\n        return completion\n\n    @requires(('read_op', ReadOp), ('start_after', str_type), ('filter_prefix', str_type), ('max_return', int))\n    def get_omap_vals(self, read_op, start_after, filter_prefix, max_return):\n        \"\"\"\n        get the omap values\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para start_after: list keys starting after start_after\n        :type start_after: str\n        :para filter_prefix: list only keys beginning with filter_prefix\n        :type filter_prefix: str\n        :para max_return: list no more than max_return key/value pairs\n        :type max_return: int\n        :returns: an iterator over the requested omap values, return value from this action\n        \"\"\"\n\n        start_after = cstr(start_after, 'start_after') if start_after else None\n        filter_prefix = cstr(filter_prefix, 'filter_prefix') if filter_prefix else None\n        cdef:\n            char *_start_after = opt_str(start_after)\n            char *_filter_prefix = opt_str(filter_prefix)\n            ReadOp _read_op = read_op\n            rados_omap_iter_t iter_addr = NULL\n            int _max_return = max_return\n            int prval = 0\n\n        with nogil:\n            rados_read_op_omap_get_vals2(_read_op.read_op, _start_after, _filter_prefix,\n                                         _max_return, &iter_addr, NULL, &prval)\n        it = OmapIterator(self)\n        it.ctx = iter_addr\n        return it, int(prval)\n\n    @requires(('read_op', ReadOp), ('start_after', str_type), ('max_return', int))\n    def get_omap_keys(self, read_op, start_after, max_return):\n        \"\"\"\n        get the omap keys\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para start_after: list keys starting after start_after\n        :type start_after: str\n        :para max_return: list no more than max_return key/value pairs\n        :type max_return: int\n        :returns: an iterator over the requested omap values, return value from this action\n        \"\"\"\n        start_after = cstr(start_after, 'start_after') if start_after else None\n        cdef:\n            char *_start_after = opt_str(start_after)\n            ReadOp _read_op = read_op\n            rados_omap_iter_t iter_addr = NULL\n            int _max_return = max_return\n            int prval = 0\n\n        with nogil:\n            rados_read_op_omap_get_keys2(_read_op.read_op, _start_after,\n                                         _max_return, &iter_addr, NULL, &prval)\n        it = OmapIterator(self)\n        it.ctx = iter_addr\n        return it, int(prval)\n\n    @requires(('read_op', ReadOp), ('keys', tuple))\n    def get_omap_vals_by_keys(self, read_op, keys):\n        \"\"\"\n        get the omap values by keys\n        :para read_op: read operation object\n        :type read_op: ReadOp\n        :para keys: input key tuple\n        :type keys: tuple\n        :returns: an iterator over the requested omap values, return value from this action\n        \"\"\"\n        keys = cstr_list(keys, 'keys')\n        cdef:\n            ReadOp _read_op = read_op\n            rados_omap_iter_t iter_addr\n            char **_keys = to_bytes_array(keys)\n            size_t key_num = len(keys)\n            int prval = 0\n\n        try:\n            with nogil:\n                rados_read_op_omap_get_vals_by_keys(_read_op.read_op,\n                                                    <const char**>_keys,\n                                                    key_num, &iter_addr,  &prval)\n            it = OmapIterator(self)\n            it.ctx = iter_addr\n            return it, int(prval)\n        finally:\n            free(_keys)\n\n    @requires(('write_op', WriteOp), ('keys', tuple))\n    def remove_omap_keys(self, write_op, keys):\n        \"\"\"\n        remove omap keys specifiled\n        :para write_op: write operation object\n        :type write_op: WriteOp\n        :para keys: input key tuple\n        :type keys: tuple\n        \"\"\"\n\n        keys = cstr_list(keys, 'keys')\n        cdef:\n            WriteOp _write_op = write_op\n            size_t key_num = len(keys)\n            char **_keys = to_bytes_array(keys)\n\n        try:\n            with nogil:\n                rados_write_op_omap_rm_keys(_write_op.write_op, <const char**>_keys, key_num)\n        finally:\n            free(_keys)\n\n    @requires(('write_op', WriteOp))\n    def clear_omap(self, write_op):\n        \"\"\"\n        Remove all key/value pairs from an object\n        :para write_op: write operation object\n        :type write_op: WriteOp\n        \"\"\"\n\n        cdef:\n            WriteOp _write_op = write_op\n\n        with nogil:\n            rados_write_op_omap_clear(_write_op.write_op)\n\n    @requires(('key', str_type), ('name', str_type), ('cookie', str_type), ('desc', str_type),\n              ('duration', opt(int)), ('flags', int))\n    def lock_exclusive(self, key, name, cookie, desc=\"\", duration=None, flags=0):\n\n        \"\"\"\n        Take an exclusive lock on an object\n\n        :param key: name of the object\n        :type key: str\n        :param name: name of the lock\n        :type name: str\n        :param cookie: cookie of the lock\n        :type cookie: str\n        :param desc: description of the lock\n        :type desc: str\n        :param duration: duration of the lock in seconds\n        :type duration: int\n        :param flags: flags\n        :type flags: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        name = cstr(name, 'name')\n        cookie = cstr(cookie, 'cookie')\n        desc = cstr(desc, 'desc')\n\n        cdef:\n            char* _key = key\n            char* _name = name\n            char* _cookie = cookie\n            char* _desc = desc\n            uint8_t _flags = flags\n            timeval _duration\n\n        if duration is None:\n            with nogil:\n                ret = rados_lock_exclusive(self.io, _key, _name, _cookie, _desc,\n                                           NULL, _flags)\n        else:\n            _duration.tv_sec = duration\n            with nogil:\n                ret = rados_lock_exclusive(self.io, _key, _name, _cookie, _desc,\n                                           &_duration, _flags)\n\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.rados_lock_exclusive(%s): failed to set lock %s on %s\" % (self.name, name, key))\n\n    @requires(('key', str_type), ('name', str_type), ('cookie', str_type), ('tag', str_type),\n              ('desc', str_type), ('duration', opt(int)), ('flags', int))\n    def lock_shared(self, key, name, cookie, tag, desc=\"\", duration=None, flags=0):\n\n        \"\"\"\n        Take a shared lock on an object\n\n        :param key: name of the object\n        :type key: str\n        :param name: name of the lock\n        :type name: str\n        :param cookie: cookie of the lock\n        :type cookie: str\n        :param tag: tag of the lock\n        :type tag: str\n        :param desc: description of the lock\n        :type desc: str\n        :param duration: duration of the lock in seconds\n        :type duration: int\n        :param flags: flags\n        :type flags: int\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        tag = cstr(tag, 'tag')\n        name = cstr(name, 'name')\n        cookie = cstr(cookie, 'cookie')\n        desc = cstr(desc, 'desc')\n\n        cdef:\n            char* _key = key\n            char* _tag = tag\n            char* _name = name\n            char* _cookie = cookie\n            char* _desc = desc\n            uint8_t _flags = flags\n            timeval _duration\n\n        if duration is None:\n            with nogil:\n                ret = rados_lock_shared(self.io, _key, _name, _cookie, _tag, _desc,\n                                        NULL, _flags)\n        else:\n            _duration.tv_sec = duration\n            with nogil:\n                ret = rados_lock_shared(self.io, _key, _name, _cookie, _tag, _desc,\n                                        &_duration, _flags)\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.rados_lock_exclusive(%s): failed to set lock %s on %s\" % (self.name, name, key))\n\n    @requires(('key', str_type), ('name', str_type), ('cookie', str_type))\n    def unlock(self, key, name, cookie):\n\n        \"\"\"\n        Release a shared or exclusive lock on an object\n\n        :param key: name of the object\n        :type key: str\n        :param name: name of the lock\n        :type name: str\n        :param cookie: cookie of the lock\n        :type cookie: str\n\n        :raises: :class:`TypeError`\n        :raises: :class:`Error`\n        \"\"\"\n        self.require_ioctx_open()\n\n        key = cstr(key, 'key')\n        name = cstr(name, 'name')\n        cookie = cstr(cookie, 'cookie')\n\n        cdef:\n            char* _key = key\n            char* _name = name\n            char* _cookie = cookie\n\n        with nogil:\n            ret = rados_unlock(self.io, _key, _name, _cookie)\n        if ret < 0:\n            raise make_ex(ret, \"Ioctx.rados_lock_exclusive(%s): failed to set lock %s on %s\" % (self.name, name, key))\n\n    def set_osdmap_full_try(self):\n        \"\"\"\n        Set global osdmap_full_try label to true\n        \"\"\"\n        with nogil:\n            rados_set_osdmap_full_try(self.io)\n\n    def unset_osdmap_full_try(self):\n        \"\"\"\n        Unset\n        \"\"\"\n        with nogil:\n            rados_unset_osdmap_full_try(self.io)\n\n    def application_enable(self, app_name, force=False):\n        \"\"\"\n        Enable an application on an OSD pool\n\n        :param app_name: application name\n        :type app_name: str\n        :param force: False if only a single app should exist per pool\n        :type expire_seconds: boool\n\n        :raises: :class:`Error`\n        \"\"\"\n        app_name =  cstr(app_name, 'app_name')\n        cdef:\n            char *_app_name = app_name\n            int _force = (1 if force else 0)\n\n        with nogil:\n            ret = rados_application_enable(self.io, _app_name, _force)\n        if ret < 0:\n            raise make_ex(ret, \"error enabling application\")\n\n    def application_list(self):\n        \"\"\"\n        Returns a list of enabled applications\n\n        :returns: list of app name string\n        \"\"\"\n        cdef:\n            size_t length = 128\n            char *apps = NULL\n\n        try:\n            while True:\n                apps = <char *>realloc_chk(apps, length)\n                with nogil:\n                    ret = rados_application_list(self.io, apps, &length)\n                if ret == 0:\n                    return [decode_cstr(app) for app in\n                                apps[:length].split(b'\\0') if app]\n                elif ret == -errno.ENOENT:\n                    return None\n                elif ret == -errno.ERANGE:\n                    pass\n                else:\n                    raise make_ex(ret, \"error listing applications\")\n        finally:\n            free(apps)\n\n    def application_metadata_set(self, app_name, key, value):\n        \"\"\"\n        Sets application metadata on an OSD pool\n\n        :param app_name: application name\n        :type app_name: str\n        :param key: metadata key\n        :type key: str\n        :param value: metadata value\n        :type value: str\n\n        :raises: :class:`Error`\n        \"\"\"\n        app_name =  cstr(app_name, 'app_name')\n        key =  cstr(key, 'key')\n        value =  cstr(value, 'value')\n        cdef:\n            char *_app_name = app_name\n            char *_key = key\n            char *_value = value\n\n        with nogil:\n            ret = rados_application_metadata_set(self.io, _app_name, _key,\n                                                 _value)\n        if ret < 0:\n            raise make_ex(ret, \"error setting application metadata\")\n\n    def application_metadata_remove(self, app_name, key):\n        \"\"\"\n        Remove application metadata from an OSD pool\n\n        :param app_name: application name\n        :type app_name: str\n        :param key: metadata key\n        :type key: str\n\n        :raises: :class:`Error`\n        \"\"\"\n        app_name =  cstr(app_name, 'app_name')\n        key =  cstr(key, 'key')\n        cdef:\n            char *_app_name = app_name\n            char *_key = key\n\n        with nogil:\n            ret = rados_application_metadata_remove(self.io, _app_name, _key)\n        if ret < 0:\n            raise make_ex(ret, \"error removing application metadata\")\n\n    def application_metadata_list(self, app_name):\n        \"\"\"\n        Returns a list of enabled applications\n\n        :param app_name: application name\n        :type app_name: str\n        :returns: list of key/value tuples\n        \"\"\"\n        app_name =  cstr(app_name, 'app_name')\n        cdef:\n            char *_app_name = app_name\n            size_t key_length = 128\n            size_t val_length = 128\n            char *c_keys = NULL\n            char *c_vals = NULL\n\n        try:\n            while True:\n                c_keys = <char *>realloc_chk(c_keys, key_length)\n                c_vals = <char *>realloc_chk(c_vals, val_length)\n                with nogil:\n                    ret = rados_application_metadata_list(self.io, _app_name,\n                                                          c_keys, &key_length,\n                                                          c_vals, &val_length)\n                if ret == 0:\n                    keys = [decode_cstr(key) for key in\n                                c_keys[:key_length].split(b'\\0') if key]\n                    vals = [decode_cstr(val) for val in\n                                c_vals[:val_length].split(b'\\0') if val]\n                    return zip(keys, vals)\n                elif ret == -errno.ERANGE:\n                    pass\n                else:\n                    raise make_ex(ret, \"error listing application metadata\")\n        finally:\n            free(c_keys)\n            free(c_vals)\n\n    def alignment(self):\n        \"\"\"\n        Returns pool alignment\n\n        :returns:\n            Number of alignment bytes required by the current pool, or None if\n            alignment is not required.\n        \"\"\"\n        cdef:\n            int requires = 0\n            uint64_t _alignment\n\n        with nogil:\n            ret = rados_ioctx_pool_requires_alignment2(self.io, &requires)\n        if ret != 0:\n            raise make_ex(ret, \"error checking alignment\")\n\n        alignment = None\n        if requires:\n            with nogil:\n                ret = rados_ioctx_pool_required_alignment2(self.io, &_alignment)\n            if ret != 0:\n                raise make_ex(ret, \"error querying alignment\")\n            alignment = _alignment\n        return alignment\n\n\ndef set_object_locator(func):\n    def retfunc(self, *args, **kwargs):\n        if self.locator_key is not None:\n            old_locator = self.ioctx.get_locator_key()\n            self.ioctx.set_locator_key(self.locator_key)\n            retval = func(self, *args, **kwargs)\n            self.ioctx.set_locator_key(old_locator)\n            return retval\n        else:\n            return func(self, *args, **kwargs)\n    return retfunc\n\n\ndef set_object_namespace(func):\n    def retfunc(self, *args, **kwargs):\n        if self.nspace is None:\n            raise LogicError(\"Namespace not set properly in context\")\n        old_nspace = self.ioctx.get_namespace()\n        self.ioctx.set_namespace(self.nspace)\n        retval = func(self, *args, **kwargs)\n        self.ioctx.set_namespace(old_nspace)\n        return retval\n    return retfunc\n\n\nclass Object(object):\n    \"\"\"Rados object wrapper, makes the object look like a file\"\"\"\n    def __init__(self, ioctx, key, locator_key=None, nspace=None):\n        self.key = key\n        self.ioctx = ioctx\n        self.offset = 0\n        self.state = \"exists\"\n        self.locator_key = locator_key\n        self.nspace = \"\" if nspace is None else nspace\n\n    def __str__(self):\n        return \"rados.Object(ioctx=%s,key=%s,nspace=%s,locator=%s)\" % \\\n            (str(self.ioctx), self.key, \"--default--\"\n             if self.nspace is \"\" else self.nspace, self.locator_key)\n\n    def require_object_exists(self):\n        if self.state != \"exists\":\n            raise ObjectStateError(\"The object is %s\" % self.state)\n\n    @set_object_locator\n    @set_object_namespace\n    def read(self, length=1024 * 1024):\n        self.require_object_exists()\n        ret = self.ioctx.read(self.key, length, self.offset)\n        self.offset += len(ret)\n        return ret\n\n    @set_object_locator\n    @set_object_namespace\n    def write(self, string_to_write):\n        self.require_object_exists()\n        ret = self.ioctx.write(self.key, string_to_write, self.offset)\n        if ret == 0:\n            self.offset += len(string_to_write)\n        return ret\n\n    @set_object_locator\n    @set_object_namespace\n    def remove(self):\n        self.require_object_exists()\n        self.ioctx.remove_object(self.key)\n        self.state = \"removed\"\n\n    @set_object_locator\n    @set_object_namespace\n    def stat(self):\n        self.require_object_exists()\n        return self.ioctx.stat(self.key)\n\n    def seek(self, position):\n        self.require_object_exists()\n        self.offset = position\n\n    @set_object_locator\n    @set_object_namespace\n    def get_xattr(self, xattr_name):\n        self.require_object_exists()\n        return self.ioctx.get_xattr(self.key, xattr_name)\n\n    @set_object_locator\n    @set_object_namespace\n    def get_xattrs(self):\n        self.require_object_exists()\n        return self.ioctx.get_xattrs(self.key)\n\n    @set_object_locator\n    @set_object_namespace\n    def set_xattr(self, xattr_name, xattr_value):\n        self.require_object_exists()\n        return self.ioctx.set_xattr(self.key, xattr_name, xattr_value)\n\n    @set_object_locator\n    @set_object_namespace\n    def rm_xattr(self, xattr_name):\n        self.require_object_exists()\n        return self.ioctx.rm_xattr(self.key, xattr_name)\n\nMONITOR_LEVELS = [\n    \"debug\",\n    \"info\",\n    \"warn\", \"warning\",\n    \"err\", \"error\",\n    \"sec\",\n    ]\n\n\nclass MonitorLog(object):\n    # NOTE(sileht): Keep this class for backward compat\n    # method moved to Rados.monitor_log()\n    \"\"\"\n    For watching cluster log messages.  Instantiate an object and keep\n    it around while callback is periodically called.  Construct with\n    'level' to monitor 'level' messages (one of MONITOR_LEVELS).\n    arg will be passed to the callback.\n\n    callback will be called with:\n        arg (given to __init__)\n        line (the full line, including timestamp, who, level, msg)\n        who (which entity issued the log message)\n        timestamp_sec (sec of a struct timespec)\n        timestamp_nsec (sec of a struct timespec)\n        seq (sequence number)\n        level (string representing the level of the log message)\n        msg (the message itself)\n    callback's return value is ignored\n    \"\"\"\n    def __init__(self, cluster, level, callback, arg):\n        self.level = level\n        self.callback = callback\n        self.arg = arg\n        self.cluster = cluster\n        self.cluster.monitor_log(level, callback, arg)\n\n", "from __future__ import print_function\nfrom nose import SkipTest\nfrom nose.tools import eq_ as eq, ok_ as ok, assert_raises\nfrom rados import (Rados, Error, RadosStateError, Object, ObjectExists,\n                   ObjectNotFound, ObjectBusy, requires, opt,\n                   ANONYMOUS_AUID, ADMIN_AUID, LIBRADOS_ALL_NSPACES, WriteOpCtx, ReadOpCtx,\n                   LIBRADOS_SNAP_HEAD, LIBRADOS_OPERATION_BALANCE_READS, LIBRADOS_OPERATION_SKIPRWLOCKS, MonitorLog)\nimport time\nimport threading\nimport json\nimport errno\nimport os\nimport sys\n\n# Are we running Python 2.x\n_python2 = sys.version_info[0] < 3\n\ndef test_rados_init_error():\n    assert_raises(Error, Rados, conffile='', rados_id='admin',\n                  name='client.admin')\n    assert_raises(Error, Rados, conffile='', name='invalid')\n    assert_raises(Error, Rados, conffile='', name='bad.invalid')\n\ndef test_rados_init():\n    with Rados(conffile='', rados_id='admin'):\n        pass\n    with Rados(conffile='', name='client.admin'):\n        pass\n    with Rados(conffile='', name='client.admin'):\n        pass\n    with Rados(conffile='', name='client.admin'):\n        pass\n\ndef test_ioctx_context_manager():\n    with Rados(conffile='', rados_id='admin') as conn:\n        with conn.open_ioctx('rbd') as ioctx:\n            pass\n\ndef test_parse_argv():\n    args = ['osd', 'pool', 'delete', 'foobar', 'foobar', '--yes-i-really-really-mean-it']\n    r = Rados()\n    eq(args, r.conf_parse_argv(args))\n\ndef test_parse_argv_empty_str():\n    args = ['']\n    r = Rados()\n    eq(args, r.conf_parse_argv(args))\n\nclass TestRequires(object):\n    @requires(('foo', str), ('bar', int), ('baz', int))\n    def _method_plain(self, foo, bar, baz):\n        ok(isinstance(foo, str))\n        ok(isinstance(bar, int))\n        ok(isinstance(baz, int))\n        return (foo, bar, baz)\n\n    def test_method_plain(self):\n        assert_raises(TypeError, self._method_plain, 42, 42, 42)\n        assert_raises(TypeError, self._method_plain, '42', '42', '42')\n        assert_raises(TypeError, self._method_plain, foo='42', bar='42', baz='42')\n        eq(self._method_plain('42', 42, 42), ('42', 42, 42))\n        eq(self._method_plain(foo='42', bar=42, baz=42), ('42', 42, 42))\n\n    @requires(('opt_foo', opt(str)), ('opt_bar', opt(int)), ('baz', int))\n    def _method_with_opt_arg(self, foo, bar, baz):\n        ok(isinstance(foo, str) or foo is None)\n        ok(isinstance(bar, int) or bar is None)\n        ok(isinstance(baz, int))\n        return (foo, bar, baz)\n\n    def test_method_with_opt_args(self):\n        assert_raises(TypeError, self._method_with_opt_arg, 42, 42, 42)\n        assert_raises(TypeError, self._method_with_opt_arg, '42', '42', 42)\n        assert_raises(TypeError, self._method_with_opt_arg, None, None, None)\n        eq(self._method_with_opt_arg(None, 42, 42), (None, 42, 42))\n        eq(self._method_with_opt_arg('42', None, 42), ('42', None, 42))\n        eq(self._method_with_opt_arg(None, None, 42), (None, None, 42))\n\n\nclass TestRadosStateError(object):\n    def _requires_configuring(self, rados):\n        assert_raises(RadosStateError, rados.connect)\n\n    def _requires_configuring_or_connected(self, rados):\n        assert_raises(RadosStateError, rados.conf_read_file)\n        assert_raises(RadosStateError, rados.conf_parse_argv, None)\n        assert_raises(RadosStateError, rados.conf_parse_env)\n        assert_raises(RadosStateError, rados.conf_get, 'opt')\n        assert_raises(RadosStateError, rados.conf_set, 'opt', 'val')\n        assert_raises(RadosStateError, rados.ping_monitor, 0)\n\n    def _requires_connected(self, rados):\n        assert_raises(RadosStateError, rados.pool_exists, 'foo')\n        assert_raises(RadosStateError, rados.pool_lookup, 'foo')\n        assert_raises(RadosStateError, rados.pool_reverse_lookup, 0)\n        assert_raises(RadosStateError, rados.create_pool, 'foo')\n        assert_raises(RadosStateError, rados.get_pool_base_tier, 0)\n        assert_raises(RadosStateError, rados.delete_pool, 'foo')\n        assert_raises(RadosStateError, rados.list_pools)\n        assert_raises(RadosStateError, rados.get_fsid)\n        assert_raises(RadosStateError, rados.open_ioctx, 'foo')\n        assert_raises(RadosStateError, rados.mon_command, '', b'')\n        assert_raises(RadosStateError, rados.osd_command, 0, '', b'')\n        assert_raises(RadosStateError, rados.pg_command, '', '', b'')\n        assert_raises(RadosStateError, rados.wait_for_latest_osdmap)\n        assert_raises(RadosStateError, rados.blacklist_add, '127.0.0.1/123', 0)\n\n    def test_configuring(self):\n        rados = Rados(conffile='')\n        eq('configuring', rados.state)\n        self._requires_connected(rados)\n\n    def test_connected(self):\n        rados = Rados(conffile='')\n        with rados:\n            eq('connected', rados.state)\n            self._requires_configuring(rados)\n\n    def test_shutdown(self):\n        rados = Rados(conffile='')\n        with rados:\n            pass\n        eq('shutdown', rados.state)\n        self._requires_configuring(rados)\n        self._requires_configuring_or_connected(rados)\n        self._requires_connected(rados)\n\n\nclass TestRados(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.conf_parse_env('FOO_DOES_NOT_EXIST_BLAHBLAH')\n        self.rados.conf_parse_env()\n        self.rados.connect()\n\n        # Assume any pre-existing pools are the cluster's defaults\n        self.default_pools = self.rados.list_pools()\n\n    def tearDown(self):\n        self.rados.shutdown()\n\n    def test_ping_monitor(self):\n        assert_raises(ObjectNotFound, self.rados.ping_monitor, 'not_exists_monitor')\n        cmd = {'prefix': 'mon dump', 'format':'json'}\n        ret, buf, out = self.rados.mon_command(json.dumps(cmd), b'')\n        for mon in json.loads(buf.decode('utf8'))['mons']:\n            while True:\n                output = self.rados.ping_monitor(mon['name'])\n                if output is None:\n                    continue\n                buf = json.loads(output)\n                if buf.get('health'):\n                    break\n\n    def test_create(self):\n        self.rados.create_pool('foo')\n        self.rados.delete_pool('foo')\n\n    def test_create_utf8(self):\n        if _python2:\n            # Use encoded bytestring\n            poolname = b\"\\351\\273\\204\"\n        else:\n            poolname = \"\\u9ec4\"\n        self.rados.create_pool(poolname)\n        assert self.rados.pool_exists(u\"\\u9ec4\")\n        self.rados.delete_pool(poolname)\n\n    def test_pool_lookup_utf8(self):\n        if _python2:\n            poolname = u'\\u9ec4'\n        else:\n            poolname = '\\u9ec4'\n        self.rados.create_pool(poolname)\n        try:\n            poolid = self.rados.pool_lookup(poolname)\n            eq(poolname, self.rados.pool_reverse_lookup(poolid))\n        finally:\n            self.rados.delete_pool(poolname)\n\n    def test_create_auid(self):\n        self.rados.create_pool('foo', 100)\n        assert self.rados.pool_exists('foo')\n        self.rados.delete_pool('foo')\n\n    def test_eexist(self):\n        self.rados.create_pool('foo')\n        assert_raises(ObjectExists, self.rados.create_pool, 'foo')\n        self.rados.delete_pool('foo')\n\n    def list_non_default_pools(self):\n        pools = self.rados.list_pools()\n        for p in self.default_pools:\n            pools.remove(p)\n        return set(pools)\n\n    def test_list_pools(self):\n        eq(set(), self.list_non_default_pools())\n        self.rados.create_pool('foo')\n        eq(set(['foo']), self.list_non_default_pools())\n        self.rados.create_pool('bar')\n        eq(set(['foo', 'bar']), self.list_non_default_pools())\n        self.rados.create_pool('baz')\n        eq(set(['foo', 'bar', 'baz']), self.list_non_default_pools())\n        self.rados.delete_pool('foo')\n        eq(set(['bar', 'baz']), self.list_non_default_pools())\n        self.rados.delete_pool('baz')\n        eq(set(['bar']), self.list_non_default_pools())\n        self.rados.delete_pool('bar')\n        eq(set(), self.list_non_default_pools())\n        self.rados.create_pool('a' * 500)\n        eq(set(['a' * 500]), self.list_non_default_pools())\n        self.rados.delete_pool('a' * 500)\n\n    def test_get_pool_base_tier(self):\n        self.rados.create_pool('foo')\n        try:\n            self.rados.create_pool('foo-cache')\n            try:\n                pool_id = self.rados.pool_lookup('foo')\n                tier_pool_id = self.rados.pool_lookup('foo-cache')\n\n                cmd = {\"prefix\":\"osd tier add\", \"pool\":\"foo\", \"tierpool\":\"foo-cache\", \"force_nonempty\":\"\"}\n                ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n                eq(ret, 0)\n\n                try:\n                    cmd = {\"prefix\":\"osd tier cache-mode\", \"pool\":\"foo-cache\", \"tierpool\":\"foo-cache\", \"mode\":\"readonly\", \"sure\":\"--yes-i-really-mean-it\"}\n                    ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n                    eq(ret, 0)\n\n                    eq(self.rados.wait_for_latest_osdmap(), 0)\n\n                    eq(pool_id, self.rados.get_pool_base_tier(pool_id))\n                    eq(pool_id, self.rados.get_pool_base_tier(tier_pool_id))\n                finally:\n                    cmd = {\"prefix\":\"osd tier remove\", \"pool\":\"foo\", \"tierpool\":\"foo-cache\"}\n                    ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n                    eq(ret, 0)\n            finally:\n                self.rados.delete_pool('foo-cache')\n        finally:\n            self.rados.delete_pool('foo')\n\n    def test_get_fsid(self):\n        fsid = self.rados.get_fsid()\n        eq(len(fsid), 36)\n\n    def test_blacklist_add(self):\n        self.rados.blacklist_add(\"1.2.3.4/123\", 1)\n\n    def test_get_cluster_stats(self):\n        stats = self.rados.get_cluster_stats()\n        assert stats['kb'] > 0\n        assert stats['kb_avail'] > 0\n        assert stats['kb_used'] > 0\n        assert stats['num_objects'] >= 0\n\n    def test_monitor_log(self):\n        lock = threading.Condition()\n        def cb(arg, line, who, sec, nsec, seq, level, msg):\n            # NOTE(sileht): the old pyrados API was received the pointer as int\n            # instead of the value of arg\n            eq(arg, \"arg\")\n            with lock:\n                lock.notify()\n            return 0\n\n        # NOTE(sileht): force don't save the monitor into local var\n        # to ensure all references are correctly tracked into the lib\n        MonitorLog(self.rados, \"debug\", cb, \"arg\")\n        with lock:\n            lock.wait()\n        MonitorLog(self.rados, \"debug\", None, None)\n        eq(None, self.rados.monitor_callback)\n\nclass TestIoctx(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.rados.create_pool('test_pool')\n        assert self.rados.pool_exists('test_pool')\n        self.ioctx = self.rados.open_ioctx('test_pool')\n\n    def tearDown(self):\n        cmd = {\"prefix\":\"osd unset\", \"key\":\"noup\"}\n        self.rados.mon_command(json.dumps(cmd), b'')\n        self.ioctx.close()\n        self.rados.delete_pool('test_pool')\n        self.rados.shutdown()\n\n    def test_get_last_version(self):\n        version = self.ioctx.get_last_version()\n        assert version >= 0\n\n    def test_get_stats(self):\n        stats = self.ioctx.get_stats()\n        eq(stats, {'num_objects_unfound': 0,\n                   'num_objects_missing_on_primary': 0,\n                   'num_object_clones': 0,\n                   'num_objects': 0,\n                   'num_object_copies': 0,\n                   'num_bytes': 0,\n                   'num_rd_kb': 0,\n                   'num_wr_kb': 0,\n                   'num_kb': 0,\n                   'num_wr': 0,\n                   'num_objects_degraded': 0,\n                   'num_rd': 0})\n\n    def test_change_auid(self):\n        self.ioctx.change_auid(ANONYMOUS_AUID)\n        self.ioctx.change_auid(ADMIN_AUID)\n\n    def test_write(self):\n        self.ioctx.write('abc', b'abc')\n        eq(self.ioctx.read('abc'), b'abc')\n\n    def test_write_full(self):\n        self.ioctx.write('abc', b'abc')\n        eq(self.ioctx.read('abc'), b'abc')\n        self.ioctx.write_full('abc', b'd')\n        eq(self.ioctx.read('abc'), b'd')\n\n    def test_append(self):\n        self.ioctx.write('abc', b'a')\n        self.ioctx.append('abc', b'b')\n        self.ioctx.append('abc', b'c')\n        eq(self.ioctx.read('abc'), b'abc')\n\n    def test_write_zeros(self):\n        self.ioctx.write('abc', b'a\\0b\\0c')\n        eq(self.ioctx.read('abc'), b'a\\0b\\0c')\n\n    def test_trunc(self):\n        self.ioctx.write('abc', b'abc')\n        self.ioctx.trunc('abc', 2)\n        eq(self.ioctx.read('abc'), b'ab')\n        size = self.ioctx.stat('abc')[0]\n        eq(size, 2)\n\n    def test_list_objects_empty(self):\n        eq(list(self.ioctx.list_objects()), [])\n\n    def test_list_objects(self):\n        self.ioctx.write('a', b'')\n        self.ioctx.write('b', b'foo')\n        self.ioctx.write_full('c', b'bar')\n        self.ioctx.append('d', b'jazz')\n        object_names = [obj.key for obj in self.ioctx.list_objects()]\n        eq(sorted(object_names), ['a', 'b', 'c', 'd'])\n\n    def test_list_ns_objects(self):\n        self.ioctx.write('a', b'')\n        self.ioctx.write('b', b'foo')\n        self.ioctx.write_full('c', b'bar')\n        self.ioctx.append('d', b'jazz')\n        self.ioctx.set_namespace(\"ns1\")\n        self.ioctx.write('ns1-a', b'')\n        self.ioctx.write('ns1-b', b'foo')\n        self.ioctx.write_full('ns1-c', b'bar')\n        self.ioctx.append('ns1-d', b'jazz')\n        self.ioctx.append('d', b'jazz')\n        self.ioctx.set_namespace(LIBRADOS_ALL_NSPACES)\n        object_names = [(obj.nspace, obj.key) for obj in self.ioctx.list_objects()]\n        eq(sorted(object_names), [('', 'a'), ('','b'), ('','c'), ('','d'),\\\n                ('ns1', 'd'), ('ns1', 'ns1-a'), ('ns1', 'ns1-b'),\\\n                ('ns1', 'ns1-c'), ('ns1', 'ns1-d')])\n\n    def test_xattrs(self):\n        xattrs = dict(a=b'1', b=b'2', c=b'3', d=b'a\\0b', e=b'\\0', f='')\n        self.ioctx.write('abc', b'')\n        for key, value in xattrs.items():\n            self.ioctx.set_xattr('abc', key, value)\n            eq(self.ioctx.get_xattr('abc', key), value)\n        stored_xattrs = {}\n        for key, value in self.ioctx.get_xattrs('abc'):\n            stored_xattrs[key] = value\n        eq(stored_xattrs, xattrs)\n\n    def test_obj_xattrs(self):\n        xattrs = dict(a=b'1', b=b'2', c=b'3', d=b'a\\0b', e=b'\\0', f='')\n        self.ioctx.write('abc', b'')\n        obj = list(self.ioctx.list_objects())[0]\n        for key, value in xattrs.items():\n            obj.set_xattr(key, value)\n            eq(obj.get_xattr(key), value)\n        stored_xattrs = {}\n        for key, value in obj.get_xattrs():\n            stored_xattrs[key] = value\n        eq(stored_xattrs, xattrs)\n\n    def test_create_snap(self):\n        assert_raises(ObjectNotFound, self.ioctx.remove_snap, 'foo')\n        self.ioctx.create_snap('foo')\n        self.ioctx.remove_snap('foo')\n\n    def test_list_snaps_empty(self):\n        eq(list(self.ioctx.list_snaps()), [])\n\n    def test_list_snaps(self):\n        snaps = ['snap1', 'snap2', 'snap3']\n        for snap in snaps:\n            self.ioctx.create_snap(snap)\n        listed_snaps = [snap.name for snap in self.ioctx.list_snaps()]\n        eq(snaps, listed_snaps)\n\n    def test_lookup_snap(self):\n        self.ioctx.create_snap('foo')\n        snap = self.ioctx.lookup_snap('foo')\n        eq(snap.name, 'foo')\n\n    def test_snap_timestamp(self):\n        self.ioctx.create_snap('foo')\n        snap = self.ioctx.lookup_snap('foo')\n        snap.get_timestamp()\n\n    def test_remove_snap(self):\n        self.ioctx.create_snap('foo')\n        (snap,) = self.ioctx.list_snaps()\n        eq(snap.name, 'foo')\n        self.ioctx.remove_snap('foo')\n        eq(list(self.ioctx.list_snaps()), [])\n\n    def test_snap_rollback(self):\n        self.ioctx.write(\"insnap\", b\"contents1\")\n        self.ioctx.create_snap(\"snap1\")\n        self.ioctx.remove_object(\"insnap\")\n        self.ioctx.snap_rollback(\"insnap\", \"snap1\")\n        eq(self.ioctx.read(\"insnap\"), b\"contents1\")\n        self.ioctx.remove_snap(\"snap1\")\n        self.ioctx.remove_object(\"insnap\")\n\n    def test_snap_read(self):\n        self.ioctx.write(\"insnap\", b\"contents1\")\n        self.ioctx.create_snap(\"snap1\")\n        self.ioctx.remove_object(\"insnap\")\n        snap = self.ioctx.lookup_snap(\"snap1\")\n        self.ioctx.set_read(snap.snap_id)\n        eq(self.ioctx.read(\"insnap\"), b\"contents1\")\n        self.ioctx.set_read(LIBRADOS_SNAP_HEAD)\n        self.ioctx.write(\"inhead\", b\"contents2\")\n        eq(self.ioctx.read(\"inhead\"), b\"contents2\")\n        self.ioctx.remove_snap(\"snap1\")\n        self.ioctx.remove_object(\"inhead\")\n\n    def test_set_omap(self):\n        keys = (\"1\", \"2\", \"3\", \"4\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\", b\"\\x04\\x04\\x04\\x04\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            write_op.set_flags(LIBRADOS_OPERATION_SKIPRWLOCKS)\n            self.ioctx.operate_write_op(write_op, \"hw\")\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals(read_op, \"\", \"\", 4)\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            next(iter)\n            eq(list(iter), [(\"2\", b\"bbb\"), (\"3\", b\"ccc\"), (\"4\", b\"\\x04\\x04\\x04\\x04\")])\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals(read_op, \"2\", \"\", 4)\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq((\"3\", b\"ccc\"), next(iter))\n            eq(list(iter), [(\"4\", b\"\\x04\\x04\\x04\\x04\")])\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals(read_op, \"\", \"2\", 4)\n            eq(ret, 0)\n            read_op.set_flags(LIBRADOS_OPERATION_BALANCE_READS)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq(list(iter), [(\"2\", b\"bbb\")])\n\n    def test_set_omap_aio(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n\n        keys = (\"1\", \"2\", \"3\", \"4\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\", b\"\\x04\\x04\\x04\\x04\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            comp = self.ioctx.operate_aio_write_op(write_op, \"hw\", cb, cb)\n            comp.wait_for_complete()\n            comp.wait_for_safe()\n            with lock:\n                while count[0] < 2:\n                    lock.wait()\n            eq(comp.get_return_value(), 0)\n\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals(read_op, \"\", \"\", 4)\n            eq(ret, 0)\n            comp = self.ioctx.operate_aio_read_op(read_op, \"hw\", cb, cb)\n            comp.wait_for_complete()\n            comp.wait_for_safe()\n            with lock:\n                while count[0] < 4:\n                    lock.wait()\n            eq(comp.get_return_value(), 0)\n            next(iter)\n            eq(list(iter), [(\"2\", b\"bbb\"), (\"3\", b\"ccc\"), (\"4\", b\"\\x04\\x04\\x04\\x04\")])\n\n    def test_write_ops(self):\n        with WriteOpCtx(self.ioctx) as write_op:\n            write_op.new(0)\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'')\n            write_op.write_full(b'1')\n            write_op.append(b'2')\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'12')\n            write_op.write_full(b'12345')\n            write_op.write(b'x', 2)\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'12x45')\n            write_op.write_full(b'12345')\n            write_op.zero(2, 2)\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'12\\x00\\x005')\n            write_op.write_full(b'12345')\n            write_op.truncate(2)\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            eq(self.ioctx.read('write_ops'), b'12')\n            write_op.remove()\n            self.ioctx.operate_write_op(write_op, \"write_ops\")\n            with assert_raises(ObjectNotFound):\n                self.ioctx.read('write_ops')\n\n    def test_get_omap_vals_by_keys(self):\n        keys = (\"1\", \"2\", \"3\", \"4\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\", b\"\\x04\\x04\\x04\\x04\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            self.ioctx.operate_write_op(write_op, \"hw\")\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals_by_keys(read_op,(\"3\",\"4\",))\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq(list(iter), [(\"3\", b\"ccc\"), (\"4\", b\"\\x04\\x04\\x04\\x04\")])\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals_by_keys(read_op,(\"3\",\"4\",))\n            eq(ret, 0)\n            with assert_raises(ObjectNotFound):\n                self.ioctx.operate_read_op(read_op, \"no_such\")\n\n    def test_get_omap_keys(self):\n        keys = (\"1\", \"2\", \"3\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            self.ioctx.operate_write_op(write_op, \"hw\")\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_keys(read_op,\"\",2)\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq(list(iter), [(\"1\", None), (\"2\", None)])\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_keys(read_op,\"\",2)\n            eq(ret, 0)\n            with assert_raises(ObjectNotFound):\n                self.ioctx.operate_read_op(read_op, \"no_such\")\n\n    def test_clear_omap(self):\n        keys = (\"1\", \"2\", \"3\")\n        values = (b\"aaa\", b\"bbb\", b\"ccc\")\n        with WriteOpCtx(self.ioctx) as write_op:\n            self.ioctx.set_omap(write_op, keys, values)\n            self.ioctx.operate_write_op(write_op, \"hw\")\n        with WriteOpCtx(self.ioctx) as write_op_1:\n            self.ioctx.clear_omap(write_op_1)\n            self.ioctx.operate_write_op(write_op_1, \"hw\")\n        with ReadOpCtx(self.ioctx) as read_op:\n            iter, ret = self.ioctx.get_omap_vals_by_keys(read_op,(\"1\",))\n            eq(ret, 0)\n            self.ioctx.operate_read_op(read_op, \"hw\")\n            eq(list(iter), [])\n\n    def test_locator(self):\n        self.ioctx.set_locator_key(\"bar\")\n        self.ioctx.write('foo', b'contents1')\n        objects = [i for i in self.ioctx.list_objects()]\n        eq(len(objects), 1)\n        eq(self.ioctx.get_locator_key(), \"bar\")\n        self.ioctx.set_locator_key(\"\")\n        objects[0].seek(0)\n        objects[0].write(b\"contents2\")\n        eq(self.ioctx.get_locator_key(), \"\")\n        self.ioctx.set_locator_key(\"bar\")\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"contents2\")\n        eq(self.ioctx.get_locator_key(), \"bar\")\n        objects[0].remove()\n        objects = [i for i in self.ioctx.list_objects()]\n        eq(objects, [])\n        self.ioctx.set_locator_key(\"\")\n\n    def test_aio_write(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n        comp = self.ioctx.aio_write(\"foo\", b\"bar\", 0, cb, cb)\n        comp.wait_for_complete()\n        comp.wait_for_safe()\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        eq(comp.get_return_value(), 0)\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"bar\")\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_aio_write_no_comp_ref(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n        # NOTE(sileht): force don't save the comp into local var\n        # to ensure all references are correctly tracked into the lib\n        self.ioctx.aio_write(\"foo\", b\"bar\", 0, cb, cb)\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"bar\")\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_aio_append(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n        comp = self.ioctx.aio_write(\"foo\", b\"bar\", 0, cb, cb)\n        comp2 = self.ioctx.aio_append(\"foo\", b\"baz\", cb, cb)\n        comp.wait_for_complete()\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"barbaz\")\n        with lock:\n            while count[0] < 4:\n                lock.wait()\n        eq(comp.get_return_value(), 0)\n        eq(comp2.get_return_value(), 0)\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_aio_write_full(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(blah):\n            with lock:\n                count[0] += 1\n                lock.notify()\n            return 0\n        self.ioctx.aio_write(\"foo\", b\"barbaz\", 0, cb, cb)\n        comp = self.ioctx.aio_write_full(\"foo\", b\"bar\", cb, cb)\n        comp.wait_for_complete()\n        comp.wait_for_safe()\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        eq(comp.get_return_value(), 0)\n        contents = self.ioctx.read(\"foo\")\n        eq(contents, b\"bar\")\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_aio_stat(self):\n        lock = threading.Condition()\n        count = [0]\n        def cb(_, size, mtime):\n            with lock:\n                count[0] += 1\n                lock.notify()\n\n        comp = self.ioctx.aio_stat(\"foo\", cb)\n        comp.wait_for_complete()\n        with lock:\n            while count[0] < 1:\n                lock.wait()\n        eq(comp.get_return_value(), -2)\n\n        self.ioctx.write(\"foo\", b\"bar\")\n\n        comp = self.ioctx.aio_stat(\"foo\", cb)\n        comp.wait_for_complete()\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        eq(comp.get_return_value(), 0)\n\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def _take_down_acting_set(self, pool, objectname):\n        # find acting_set for pool:objectname and take it down; used to\n        # verify that async reads don't complete while acting set is missing\n        cmd = {\n            \"prefix\":\"osd map\",\n            \"pool\":pool,\n            \"object\":objectname,\n            \"format\":\"json\",\n        }\n        r, jsonout, _ = self.rados.mon_command(json.dumps(cmd), b'')\n        objmap = json.loads(jsonout.decode(\"utf-8\"))\n        acting_set = objmap['acting']\n        cmd = {\"prefix\":\"osd set\", \"key\":\"noup\"}\n        r, _, _ = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(r, 0)\n        cmd = {\"prefix\":\"osd down\", \"ids\":[str(i) for i in acting_set]}\n        r, _, _ = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(r, 0)\n\n        # wait for OSDs to acknowledge the down\n        eq(self.rados.wait_for_latest_osdmap(), 0)\n\n    def _let_osds_back_up(self):\n        cmd = {\"prefix\":\"osd unset\", \"key\":\"noup\"}\n        r, _, _ = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(r, 0)\n\n    def test_aio_read(self):\n        # this is a list so that the local cb() can modify it\n        retval = [None]\n        lock = threading.Condition()\n        def cb(_, buf):\n            with lock:\n                retval[0] = buf\n                lock.notify()\n        payload = b\"bar\\000frob\"\n        self.ioctx.write(\"foo\", payload)\n\n        # test1: use wait_for_complete() and wait for cb by\n        # watching retval[0]\n        self._take_down_acting_set('test_pool', 'foo')\n        comp = self.ioctx.aio_read(\"foo\", len(payload), 0, cb)\n        eq(False, comp.is_complete())\n        time.sleep(3)\n        eq(False, comp.is_complete())\n        with lock:\n            eq(None, retval[0])\n        self._let_osds_back_up()\n        comp.wait_for_complete()\n        loops = 0\n        with lock:\n            while retval[0] is None and loops <= 10:\n                lock.wait(timeout=5)\n                loops += 1\n        assert(loops <= 10)\n\n        eq(retval[0], payload)\n        eq(sys.getrefcount(comp), 2)\n\n        # test2: use wait_for_complete_and_cb(), verify retval[0] is\n        # set by the time we regain control\n\n        retval[0] = None\n        self._take_down_acting_set('test_pool', 'foo')\n        comp = self.ioctx.aio_read(\"foo\", len(payload), 0, cb)\n        eq(False, comp.is_complete())\n        time.sleep(3)\n        eq(False, comp.is_complete())\n        with lock:\n            eq(None, retval[0])\n        self._let_osds_back_up()\n\n        comp.wait_for_complete_and_cb()\n        assert(retval[0] is not None)\n        eq(retval[0], payload)\n        eq(sys.getrefcount(comp), 2)\n\n        # test3: error case, use wait_for_complete_and_cb(), verify retval[0] is\n        # set by the time we regain control\n\n        retval[0] = 1\n        self._take_down_acting_set('test_pool', 'bar')\n        comp = self.ioctx.aio_read(\"bar\", len(payload), 0, cb)\n        eq(False, comp.is_complete())\n        time.sleep(3)\n        eq(False, comp.is_complete())\n        with lock:\n            eq(1, retval[0])\n        self._let_osds_back_up()\n\n        comp.wait_for_complete_and_cb()\n        eq(None, retval[0])\n        assert(comp.get_return_value() < 0)\n        eq(sys.getrefcount(comp), 2)\n\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_lock(self):\n        self.ioctx.lock_exclusive(\"foo\", \"lock\", \"locker\", \"desc_lock\",\n                                  10000, 0)\n        assert_raises(ObjectExists,\n                      self.ioctx.lock_exclusive,\n                      \"foo\", \"lock\", \"locker\", \"desc_lock\", 10000, 0)\n        self.ioctx.unlock(\"foo\", \"lock\", \"locker\")\n        assert_raises(ObjectNotFound, self.ioctx.unlock, \"foo\", \"lock\", \"locker\")\n\n        self.ioctx.lock_shared(\"foo\", \"lock\", \"locker1\", \"tag\", \"desc_lock\",\n                               10000, 0)\n        self.ioctx.lock_shared(\"foo\", \"lock\", \"locker2\", \"tag\", \"desc_lock\",\n                               10000, 0)\n        assert_raises(ObjectBusy,\n                      self.ioctx.lock_exclusive,\n                      \"foo\", \"lock\", \"locker3\", \"desc_lock\", 10000, 0)\n        self.ioctx.unlock(\"foo\", \"lock\", \"locker1\")\n        self.ioctx.unlock(\"foo\", \"lock\", \"locker2\")\n        assert_raises(ObjectNotFound, self.ioctx.unlock, \"foo\", \"lock\", \"locker1\")\n        assert_raises(ObjectNotFound, self.ioctx.unlock, \"foo\", \"lock\", \"locker2\")\n\n    def test_execute(self):\n        self.ioctx.write(\"foo\", b\"\") # ensure object exists\n\n        ret, buf = self.ioctx.execute(\"foo\", \"hello\", \"say_hello\", b\"\")\n        eq(buf, b\"Hello, world!\")\n\n        ret, buf = self.ioctx.execute(\"foo\", \"hello\", \"say_hello\", b\"nose\")\n        eq(buf, b\"Hello, nose!\")\n\n    def test_aio_execute(self):\n        count = [0]\n        retval = [None]\n        lock = threading.Condition()\n        def cb(_, buf):\n            with lock:\n                if retval[0] is None:\n                    retval[0] = buf\n                count[0] += 1\n                lock.notify()\n        self.ioctx.write(\"foo\", b\"\") # ensure object exists\n\n        comp = self.ioctx.aio_execute(\"foo\", \"hello\", \"say_hello\", b\"\", 32, cb, cb)\n        comp.wait_for_complete()\n        with lock:\n            while count[0] < 2:\n                lock.wait()\n        eq(comp.get_return_value(), 13)\n        eq(retval[0], b\"Hello, world!\")\n\n        retval[0] = None\n        comp = self.ioctx.aio_execute(\"foo\", \"hello\", \"say_hello\", b\"nose\", 32, cb, cb)\n        comp.wait_for_complete()\n        with lock:\n            while count[0] < 4:\n                lock.wait()\n        eq(comp.get_return_value(), 12)\n        eq(retval[0], b\"Hello, nose!\")\n\n        [i.remove() for i in self.ioctx.list_objects()]\n\n    def test_applications(self):\n        cmd = {\"prefix\":\"osd dump\", \"format\":\"json\"}\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(ret, 0)\n        assert len(buf) > 0\n        release = json.loads(buf.decode(\"utf-8\")).get(\"require_osd_release\",\n                                                      None)\n        if not release or release[0] < 'l':\n            raise SkipTest\n\n        eq([], self.ioctx.application_list())\n\n        self.ioctx.application_enable(\"app1\")\n        assert_raises(Error, self.ioctx.application_enable, \"app2\")\n        self.ioctx.application_enable(\"app2\", True)\n\n        assert_raises(Error, self.ioctx.application_metadata_list, \"dne\")\n        eq([], self.ioctx.application_metadata_list(\"app1\"))\n\n        assert_raises(Error, self.ioctx.application_metadata_set, \"dne\", \"key\",\n                      \"key\")\n        self.ioctx.application_metadata_set(\"app1\", \"key1\", \"val1\")\n        self.ioctx.application_metadata_set(\"app1\", \"key2\", \"val2\")\n        self.ioctx.application_metadata_set(\"app2\", \"key1\", \"val1\")\n\n        eq([(\"key1\", \"val1\"), (\"key2\", \"val2\")],\n           self.ioctx.application_metadata_list(\"app1\"))\n\n        self.ioctx.application_metadata_remove(\"app1\", \"key1\")\n        eq([(\"key2\", \"val2\")], self.ioctx.application_metadata_list(\"app1\"))\n\n    def test_service_daemon(self):\n        name = \"pid-\" + str(os.getpid())\n        metadata = {'version': '3.14', 'memory': '42'}\n        self.rados.service_daemon_register(\"laundry\", name, metadata)\n        status = {'result': 'unknown', 'test': 'running'}\n        self.rados.service_daemon_update(status)\n\n    def test_alignment(self):\n        eq(self.ioctx.alignment(), None)\n\n\nclass TestIoctxEc(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.pool = 'test-ec'\n        self.profile = 'testprofile-%s' % self.pool\n        cmd = {\"prefix\": \"osd erasure-code-profile set\", \n               \"name\": self.profile, \"profile\": [\"k=2\", \"m=1\", \"crush-failure-domain=osd\"]}\n        ret, buf, out = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, 0, msg=out)\n        # create ec pool with profile created above\n        cmd = {'prefix': 'osd pool create', 'pg_num': 8, 'pgp_num': 8,\n               'pool': self.pool, 'pool_type': 'erasure', \n               'erasure_code_profile': self.profile}\n        ret, buf, out = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, 0, msg=out)\n        assert self.rados.pool_exists(self.pool)\n        self.ioctx = self.rados.open_ioctx(self.pool)\n\n    def tearDown(self):\n        cmd = {\"prefix\": \"osd unset\", \"key\": \"noup\"}\n        self.rados.mon_command(json.dumps(cmd), b'')\n        self.ioctx.close()\n        self.rados.delete_pool(self.pool)\n        self.rados.shutdown()\n\n    def test_alignment(self):\n        eq(self.ioctx.alignment(), 8192)\n\n\nclass TestIoctx2(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.rados.create_pool('test_pool')\n        assert self.rados.pool_exists('test_pool')\n        pool_id = self.rados.pool_lookup('test_pool')\n        assert pool_id > 0\n        self.ioctx2 = self.rados.open_ioctx2(pool_id)\n\n    def tearDown(self):\n        cmd = {\"prefix\": \"osd unset\", \"key\": \"noup\"}\n        self.rados.mon_command(json.dumps(cmd), b'')\n        self.ioctx2.close()\n        self.rados.delete_pool('test_pool')\n        self.rados.shutdown()\n\n    def test_get_last_version(self):\n        version = self.ioctx2.get_last_version()\n        assert version >= 0\n\n    def test_get_stats(self):\n        stats = self.ioctx2.get_stats()\n        eq(stats, {'num_objects_unfound': 0,\n                   'num_objects_missing_on_primary': 0,\n                   'num_object_clones': 0,\n                   'num_objects': 0,\n                   'num_object_copies': 0,\n                   'num_bytes': 0,\n                   'num_rd_kb': 0,\n                   'num_wr_kb': 0,\n                   'num_kb': 0,\n                   'num_wr': 0,\n                   'num_objects_degraded': 0,\n                   'num_rd': 0})\n\n    def test_change_auid(self):\n        self.ioctx2.change_auid(ANONYMOUS_AUID)\n        self.ioctx2.change_auid(ADMIN_AUID)\n\n\nclass TestObject(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.rados.create_pool('test_pool')\n        assert self.rados.pool_exists('test_pool')\n        self.ioctx = self.rados.open_ioctx('test_pool')\n        self.ioctx.write('foo', b'bar')\n        self.object = Object(self.ioctx, 'foo')\n\n    def tearDown(self):\n        self.ioctx.close()\n        self.ioctx = None\n        self.rados.delete_pool('test_pool')\n        self.rados.shutdown()\n        self.rados = None\n\n    def test_read(self):\n        eq(self.object.read(3), b'bar')\n        eq(self.object.read(100), b'')\n\n    def test_seek(self):\n        self.object.write(b'blah')\n        self.object.seek(0)\n        eq(self.object.read(4), b'blah')\n        self.object.seek(1)\n        eq(self.object.read(3), b'lah')\n\n    def test_write(self):\n        self.object.write(b'barbaz')\n        self.object.seek(0)\n        eq(self.object.read(3), b'bar')\n        eq(self.object.read(3), b'baz')\n\nclass TestIoCtxSelfManagedSnaps(object):\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n        self.rados.create_pool('test_pool')\n        assert self.rados.pool_exists('test_pool')\n        self.ioctx = self.rados.open_ioctx('test_pool')\n\n    def tearDown(self):\n        cmd = {\"prefix\":\"osd unset\", \"key\":\"noup\"}\n        self.rados.mon_command(json.dumps(cmd), b'')\n        self.ioctx.close()\n        self.rados.delete_pool('test_pool')\n        self.rados.shutdown()\n\n    def test(self):\n        # cannot mix-and-match pool and self-managed snapshot mode\n        self.ioctx.set_self_managed_snap_write([])\n        self.ioctx.write('abc', b'abc')\n        snap_id_1 = self.ioctx.create_self_managed_snap()\n        self.ioctx.set_self_managed_snap_write([snap_id_1])\n\n        self.ioctx.write('abc', b'def')\n        snap_id_2 = self.ioctx.create_self_managed_snap()\n        self.ioctx.set_self_managed_snap_write([snap_id_1, snap_id_2])\n\n        self.ioctx.write('abc', b'ghi')\n\n        self.ioctx.rollback_self_managed_snap('abc', snap_id_1)\n        eq(self.ioctx.read('abc'), b'abc')\n\n        self.ioctx.rollback_self_managed_snap('abc', snap_id_2)\n        eq(self.ioctx.read('abc'), b'def')\n\n        self.ioctx.remove_self_managed_snap(snap_id_1)\n        self.ioctx.remove_self_managed_snap(snap_id_2)\n\nclass TestCommand(object):\n\n    def setUp(self):\n        self.rados = Rados(conffile='')\n        self.rados.connect()\n\n    def tearDown(self):\n        self.rados.shutdown()\n\n    def test_monmap_dump(self):\n\n        # check for success and some plain output with epoch in it\n        cmd = {\"prefix\":\"mon dump\"}\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, 0)\n        assert len(buf) > 0\n        assert(b'epoch' in buf)\n\n        # JSON, and grab current epoch\n        cmd['format'] = 'json'\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, 0)\n        assert len(buf) > 0\n        d = json.loads(buf.decode(\"utf-8\"))\n        assert('epoch' in d)\n        epoch = d['epoch']\n\n        # assume epoch + 1000 does not exist; test for ENOENT\n        cmd['epoch'] = epoch + 1000\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30)\n        eq(ret, -errno.ENOENT)\n        eq(len(buf), 0)\n        del cmd['epoch']\n\n        # send to specific target by name\n        target = d['mons'][0]['name']\n        print(target)\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30,\n                                                target=target)\n        eq(ret, 0)\n        assert len(buf) > 0\n        d = json.loads(buf.decode(\"utf-8\"))\n        assert('epoch' in d)\n\n        # and by rank\n        target = d['mons'][0]['rank']\n        print(target)\n        ret, buf, errs = self.rados.mon_command(json.dumps(cmd), b'', timeout=30,\n                                                target=target)\n        eq(ret, 0)\n        assert len(buf) > 0\n        d = json.loads(buf.decode(\"utf-8\"))\n        assert('epoch' in d)\n\n    def test_osd_bench(self):\n        cmd = dict(prefix='bench', size=4096, count=8192)\n        ret, buf, err = self.rados.osd_command(0, json.dumps(cmd), b'',\n                                               timeout=30)\n        eq(ret, 0)\n        assert len(buf) > 0\n        out = json.loads(buf.decode('utf-8'))\n        eq(out['blocksize'], cmd['size'])\n        eq(out['bytes_written'], cmd['count'])\n\n    def test_ceph_osd_pool_create_utf8(self):\n        if _python2:\n            # Use encoded bytestring\n            poolname = b\"\\351\\273\\205\"\n        else:\n            poolname = \"\\u9ec5\"\n\n        cmd = {\"prefix\": \"osd pool create\", \"pg_num\": 16, \"pool\": poolname}\n        ret, buf, out = self.rados.mon_command(json.dumps(cmd), b'')\n        eq(ret, 0)\n        assert len(out) > 0\n        eq(u\"pool '\\u9ec5' created\", out)\n"], "filenames": ["qa/workunits/rados/test_pool_access.sh", "qa/workunits/rbd/permissions.sh", "src/mon/OSDMonitor.cc", "src/mon/OSDMonitor.h", "src/pybind/rados/rados.pyx", "src/test/pybind/test_rados.py"], "buggy_code_start_loc": [5, 31, 78, 402, 237, 1013], "buggy_code_end_loc": [22, 159, 11811, 402, 3184, 1013], "fixing_code_start_loc": [5, 32, 79, 403, 238, 1014], "fixing_code_end_loc": [107, 252, 11931, 404, 3291, 1051], "type": "CWE-287", "message": "A flaw was found in the way ceph mon handles user requests. Any authenticated ceph user having read access to ceph can delete, create ceph storage pools and corrupt snapshot images. Ceph branches master, mimic, luminous and jewel are believed to be affected.", "other": {"cve": {"id": "CVE-2018-10861", "sourceIdentifier": "secalert@redhat.com", "published": "2018-07-10T14:29:00.213", "lastModified": "2019-10-09T23:33:05.587", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "A flaw was found in the way ceph mon handles user requests. Any authenticated ceph user having read access to ceph can delete, create ceph storage pools and corrupt snapshot images. Ceph branches master, mimic, luminous and jewel are believed to be affected."}, {"lang": "es", "value": "Se ha encontrado un error en la forma en la que ceph mon maneja las peticiones de usuario. Cualquier usuario de ceph autenticado que tenga acceso de lectura en ceph puede eliminar, crear pools de almacenamiento de ceph y corromper im\u00e1genes instant\u00e1neas. Se cree que las ramas de ceph master, mimic, luminous y jewel se han visto afectadas."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:N/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 5.5}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-287"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-285"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.0:*:*:*:*:*:*:*", "matchCriteriaId": "8901022A-8A84-494A-A5BF-358F2CBBDFFF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "76788B0A-7776-4D0C-B0D7-C855E9A0231E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "7A925DB4-83DC-45D1-A48B-1675A111213B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "D22BA440-CB28-445C-A7F8-CBD6E8965B2E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "A503C653-AFEB-4E5A-872B-AD033C0E2259"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "7C00462A-A1B8-42A7-9336-DE1BF5510B6B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "3505D4E2-4EA8-40A4-A57C-46CCA9922EF3"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "09EC481B-79F0-41DB-B95F-D1A221C96F4B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "31F159B5-AF02-48BE-B994-749F21B9D362"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "D9684039-7938-405D-B833-4C54BFBD6476"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "8FAE4350-8F39-4E78-AB25-17DE76FD57AF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:10.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "3B2369D2-4413-447C-A0A8-84CA37B1F5B8"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:12.2.0:*:*:*:*:*:*:*", "matchCriteriaId": "3515BF53-4921-462F-820E-B842BB3FF066"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:12.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "48067E54-26F5-4020-BCEA-A65C2536618B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:12.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "F9A86B91-78C3-4D02-B7C8-11AAFB1CCCEC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:12.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "CDBD084F-4A0B-4231-8465-61F8BE5E57F6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:12.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "0885F67A-E01B-4BF2-A760-D452B55C5F69"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:12.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "DB9D95E9-52F3-459C-89AD-6FCA6A975085"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:12.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "087C6821-9A77-4CC8-8AA0-2C51414D9B58"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:12.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "A667C6AF-76D4-4192-A8BF-395F368EFAE4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:13.2.0:*:*:*:*:*:*:*", "matchCriteriaId": "13BF6806-6E69-4172-9260-2E97FB253339"}, {"vulnerable": true, "criteria": "cpe:2.3:a:ceph:ceph:13.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "DCAE0EE4-BBE9-4DBD-84CC-9A72E97E73E6"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:ceph_storage:3:*:*:*:*:*:*:*", "matchCriteriaId": "E9184616-421F-4EA9-AC1A-A4C95BBAAC99"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:ceph_storage_mon:2:*:*:*:*:*:*:*", "matchCriteriaId": "8C2EBAD9-F0D5-4176-9C4D-001B230E699E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:ceph_storage_mon:3:*:*:*:*:*:*:*", "matchCriteriaId": "CD2F9BA8-FE0A-43DE-A756-C35A24C3D96E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:ceph_storage_osd:2:*:*:*:*:*:*:*", "matchCriteriaId": "AA5F5227-DBDA-4C01-BF7C-4D53F455404F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:ceph_storage_osd:3:*:*:*:*:*:*:*", "matchCriteriaId": "A80BACB5-7A56-4BC6-9261-58A3860F4E8C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "33C068A4-3780-4EAB-A937-6082DF847564"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "825ECE2D-E232-46E0-A047-074B34DB1E97"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.0:*:*:*:*:*:*:*", "matchCriteriaId": "F1E78106-58E6-4D59-990F-75DA575BFAD9"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-04/msg00100.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://tracker.ceph.com/issues/24838", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/104742", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2177", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2179", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2261", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2274", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1593308", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/ceph/ceph/commit/975528f632f73fbffa3f1fee304e3bbe3296cffc", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4339", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/ceph/ceph/commit/975528f632f73fbffa3f1fee304e3bbe3296cffc"}}