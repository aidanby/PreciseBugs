{"buggy_code": ["/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#ifndef __BTRFS_CTREE__\n#define __BTRFS_CTREE__\n\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/fs.h>\n#include <linux/rwsem.h>\n#include <linux/completion.h>\n#include <linux/backing-dev.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n#include <linux/kobject.h>\n#include <trace/events/btrfs.h>\n#include <asm/kmap_types.h>\n#include <linux/pagemap.h>\n#include \"extent_io.h\"\n#include \"extent_map.h\"\n#include \"async-thread.h\"\n#include \"ioctl.h\"\n\nstruct btrfs_trans_handle;\nstruct btrfs_transaction;\nstruct btrfs_pending_snapshot;\nextern struct kmem_cache *btrfs_trans_handle_cachep;\nextern struct kmem_cache *btrfs_transaction_cachep;\nextern struct kmem_cache *btrfs_bit_radix_cachep;\nextern struct kmem_cache *btrfs_path_cachep;\nextern struct kmem_cache *btrfs_free_space_cachep;\nstruct btrfs_ordered_sum;\n\n#define BTRFS_MAGIC \"_BHRfS_M\"\n\n#define BTRFS_MAX_MIRRORS 3\n\n#define BTRFS_MAX_LEVEL 8\n\n#define BTRFS_COMPAT_EXTENT_TREE_V0\n\n/*\n * files bigger than this get some pre-flushing when they are added\n * to the ordered operations list.  That way we limit the total\n * work done by the commit\n */\n#define BTRFS_ORDERED_OPERATIONS_FLUSH_LIMIT (8 * 1024 * 1024)\n\n/* holds pointers to all of the tree roots */\n#define BTRFS_ROOT_TREE_OBJECTID 1ULL\n\n/* stores information about which extents are in use, and reference counts */\n#define BTRFS_EXTENT_TREE_OBJECTID 2ULL\n\n/*\n * chunk tree stores translations from logical -> physical block numbering\n * the super block points to the chunk tree\n */\n#define BTRFS_CHUNK_TREE_OBJECTID 3ULL\n\n/*\n * stores information about which areas of a given device are in use.\n * one per device.  The tree of tree roots points to the device tree\n */\n#define BTRFS_DEV_TREE_OBJECTID 4ULL\n\n/* one per subvolume, storing files and directories */\n#define BTRFS_FS_TREE_OBJECTID 5ULL\n\n/* directory objectid inside the root tree */\n#define BTRFS_ROOT_TREE_DIR_OBJECTID 6ULL\n\n/* holds checksums of all the data extents */\n#define BTRFS_CSUM_TREE_OBJECTID 7ULL\n\n/* for storing balance parameters in the root tree */\n#define BTRFS_BALANCE_OBJECTID -4ULL\n\n/* holds quota configuration and tracking */\n#define BTRFS_QUOTA_TREE_OBJECTID 8ULL\n\n/* orhpan objectid for tracking unlinked/truncated files */\n#define BTRFS_ORPHAN_OBJECTID -5ULL\n\n/* does write ahead logging to speed up fsyncs */\n#define BTRFS_TREE_LOG_OBJECTID -6ULL\n#define BTRFS_TREE_LOG_FIXUP_OBJECTID -7ULL\n\n/* for space balancing */\n#define BTRFS_TREE_RELOC_OBJECTID -8ULL\n#define BTRFS_DATA_RELOC_TREE_OBJECTID -9ULL\n\n/*\n * extent checksums all have this objectid\n * this allows them to share the logging tree\n * for fsyncs\n */\n#define BTRFS_EXTENT_CSUM_OBJECTID -10ULL\n\n/* For storing free space cache */\n#define BTRFS_FREE_SPACE_OBJECTID -11ULL\n\n/*\n * The inode number assigned to the special inode for storing\n * free ino cache\n */\n#define BTRFS_FREE_INO_OBJECTID -12ULL\n\n/* dummy objectid represents multiple objectids */\n#define BTRFS_MULTIPLE_OBJECTIDS -255ULL\n\n/*\n * All files have objectids in this range.\n */\n#define BTRFS_FIRST_FREE_OBJECTID 256ULL\n#define BTRFS_LAST_FREE_OBJECTID -256ULL\n#define BTRFS_FIRST_CHUNK_TREE_OBJECTID 256ULL\n\n\n/*\n * the device items go into the chunk tree.  The key is in the form\n * [ 1 BTRFS_DEV_ITEM_KEY device_id ]\n */\n#define BTRFS_DEV_ITEMS_OBJECTID 1ULL\n\n#define BTRFS_BTREE_INODE_OBJECTID 1\n\n#define BTRFS_EMPTY_SUBVOL_DIR_OBJECTID 2\n\n#define BTRFS_DEV_REPLACE_DEVID 0\n\n/*\n * the max metadata block size.  This limit is somewhat artificial,\n * but the memmove costs go through the roof for larger blocks.\n */\n#define BTRFS_MAX_METADATA_BLOCKSIZE 65536\n\n/*\n * we can actually store much bigger names, but lets not confuse the rest\n * of linux\n */\n#define BTRFS_NAME_LEN 255\n\n/*\n * Theoretical limit is larger, but we keep this down to a sane\n * value. That should limit greatly the possibility of collisions on\n * inode ref items.\n */\n#define BTRFS_LINK_MAX 65535U\n\n/* 32 bytes in various csum fields */\n#define BTRFS_CSUM_SIZE 32\n\n/* csum types */\n#define BTRFS_CSUM_TYPE_CRC32\t0\n\nstatic int btrfs_csum_sizes[] = { 4, 0 };\n\n/* four bytes for CRC32 */\n#define BTRFS_EMPTY_DIR_SIZE 0\n\n/* spefic to btrfs_map_block(), therefore not in include/linux/blk_types.h */\n#define REQ_GET_READ_MIRRORS\t(1 << 30)\n\n#define BTRFS_FT_UNKNOWN\t0\n#define BTRFS_FT_REG_FILE\t1\n#define BTRFS_FT_DIR\t\t2\n#define BTRFS_FT_CHRDEV\t\t3\n#define BTRFS_FT_BLKDEV\t\t4\n#define BTRFS_FT_FIFO\t\t5\n#define BTRFS_FT_SOCK\t\t6\n#define BTRFS_FT_SYMLINK\t7\n#define BTRFS_FT_XATTR\t\t8\n#define BTRFS_FT_MAX\t\t9\n\n/* ioprio of readahead is set to idle */\n#define BTRFS_IOPRIO_READA (IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0))\n\n/*\n * The key defines the order in the tree, and so it also defines (optimal)\n * block layout.\n *\n * objectid corresponds to the inode number.\n *\n * type tells us things about the object, and is a kind of stream selector.\n * so for a given inode, keys with type of 1 might refer to the inode data,\n * type of 2 may point to file data in the btree and type == 3 may point to\n * extents.\n *\n * offset is the starting byte offset for this key in the stream.\n *\n * btrfs_disk_key is in disk byte order.  struct btrfs_key is always\n * in cpu native order.  Otherwise they are identical and their sizes\n * should be the same (ie both packed)\n */\nstruct btrfs_disk_key {\n\t__le64 objectid;\n\tu8 type;\n\t__le64 offset;\n} __attribute__ ((__packed__));\n\nstruct btrfs_key {\n\tu64 objectid;\n\tu8 type;\n\tu64 offset;\n} __attribute__ ((__packed__));\n\nstruct btrfs_mapping_tree {\n\tstruct extent_map_tree map_tree;\n};\n\nstruct btrfs_dev_item {\n\t/* the internal btrfs device id */\n\t__le64 devid;\n\n\t/* size of the device */\n\t__le64 total_bytes;\n\n\t/* bytes used */\n\t__le64 bytes_used;\n\n\t/* optimal io alignment for this device */\n\t__le32 io_align;\n\n\t/* optimal io width for this device */\n\t__le32 io_width;\n\n\t/* minimal io size for this device */\n\t__le32 sector_size;\n\n\t/* type and info about this device */\n\t__le64 type;\n\n\t/* expected generation for this device */\n\t__le64 generation;\n\n\t/*\n\t * starting byte of this partition on the device,\n\t * to allow for stripe alignment in the future\n\t */\n\t__le64 start_offset;\n\n\t/* grouping information for allocation decisions */\n\t__le32 dev_group;\n\n\t/* seek speed 0-100 where 100 is fastest */\n\tu8 seek_speed;\n\n\t/* bandwidth 0-100 where 100 is fastest */\n\tu8 bandwidth;\n\n\t/* btrfs generated uuid for this device */\n\tu8 uuid[BTRFS_UUID_SIZE];\n\n\t/* uuid of FS who owns this device */\n\tu8 fsid[BTRFS_UUID_SIZE];\n} __attribute__ ((__packed__));\n\nstruct btrfs_stripe {\n\t__le64 devid;\n\t__le64 offset;\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n} __attribute__ ((__packed__));\n\nstruct btrfs_chunk {\n\t/* size of this chunk in bytes */\n\t__le64 length;\n\n\t/* objectid of the root referencing this chunk */\n\t__le64 owner;\n\n\t__le64 stripe_len;\n\t__le64 type;\n\n\t/* optimal io alignment for this chunk */\n\t__le32 io_align;\n\n\t/* optimal io width for this chunk */\n\t__le32 io_width;\n\n\t/* minimal io size for this chunk */\n\t__le32 sector_size;\n\n\t/* 2^16 stripes is quite a lot, a second limit is the size of a single\n\t * item in the btree\n\t */\n\t__le16 num_stripes;\n\n\t/* sub stripes only matter for raid10 */\n\t__le16 sub_stripes;\n\tstruct btrfs_stripe stripe;\n\t/* additional stripes go here */\n} __attribute__ ((__packed__));\n\n#define BTRFS_FREE_SPACE_EXTENT\t1\n#define BTRFS_FREE_SPACE_BITMAP\t2\n\nstruct btrfs_free_space_entry {\n\t__le64 offset;\n\t__le64 bytes;\n\tu8 type;\n} __attribute__ ((__packed__));\n\nstruct btrfs_free_space_header {\n\tstruct btrfs_disk_key location;\n\t__le64 generation;\n\t__le64 num_entries;\n\t__le64 num_bitmaps;\n} __attribute__ ((__packed__));\n\nstatic inline unsigned long btrfs_chunk_item_size(int num_stripes)\n{\n\tBUG_ON(num_stripes == 0);\n\treturn sizeof(struct btrfs_chunk) +\n\t\tsizeof(struct btrfs_stripe) * (num_stripes - 1);\n}\n\n#define BTRFS_HEADER_FLAG_WRITTEN\t(1ULL << 0)\n#define BTRFS_HEADER_FLAG_RELOC\t\t(1ULL << 1)\n\n/*\n * File system states\n */\n\n/* Errors detected */\n#define BTRFS_SUPER_FLAG_ERROR\t\t(1ULL << 2)\n\n#define BTRFS_SUPER_FLAG_SEEDING\t(1ULL << 32)\n#define BTRFS_SUPER_FLAG_METADUMP\t(1ULL << 33)\n\n#define BTRFS_BACKREF_REV_MAX\t\t256\n#define BTRFS_BACKREF_REV_SHIFT\t\t56\n#define BTRFS_BACKREF_REV_MASK\t\t(((u64)BTRFS_BACKREF_REV_MAX - 1) << \\\n\t\t\t\t\t BTRFS_BACKREF_REV_SHIFT)\n\n#define BTRFS_OLD_BACKREF_REV\t\t0\n#define BTRFS_MIXED_BACKREF_REV\t\t1\n\n/*\n * every tree block (leaf or node) starts with this header.\n */\nstruct btrfs_header {\n\t/* these first four must match the super block */\n\tu8 csum[BTRFS_CSUM_SIZE];\n\tu8 fsid[BTRFS_FSID_SIZE]; /* FS specific uuid */\n\t__le64 bytenr; /* which block this node is supposed to live in */\n\t__le64 flags;\n\n\t/* allowed to be different from the super from here on down */\n\tu8 chunk_tree_uuid[BTRFS_UUID_SIZE];\n\t__le64 generation;\n\t__le64 owner;\n\t__le32 nritems;\n\tu8 level;\n} __attribute__ ((__packed__));\n\n#define BTRFS_NODEPTRS_PER_BLOCK(r) (((r)->nodesize - \\\n\t\t\t\t      sizeof(struct btrfs_header)) / \\\n\t\t\t\t     sizeof(struct btrfs_key_ptr))\n#define __BTRFS_LEAF_DATA_SIZE(bs) ((bs) - sizeof(struct btrfs_header))\n#define BTRFS_LEAF_DATA_SIZE(r) (__BTRFS_LEAF_DATA_SIZE(r->leafsize))\n#define BTRFS_MAX_INLINE_DATA_SIZE(r) (BTRFS_LEAF_DATA_SIZE(r) - \\\n\t\t\t\t\tsizeof(struct btrfs_item) - \\\n\t\t\t\t\tsizeof(struct btrfs_file_extent_item))\n#define BTRFS_MAX_XATTR_SIZE(r)\t(BTRFS_LEAF_DATA_SIZE(r) - \\\n\t\t\t\t sizeof(struct btrfs_item) -\\\n\t\t\t\t sizeof(struct btrfs_dir_item))\n\n\n/*\n * this is a very generous portion of the super block, giving us\n * room to translate 14 chunks with 3 stripes each.\n */\n#define BTRFS_SYSTEM_CHUNK_ARRAY_SIZE 2048\n#define BTRFS_LABEL_SIZE 256\n\n/*\n * just in case we somehow lose the roots and are not able to mount,\n * we store an array of the roots from previous transactions\n * in the super.\n */\n#define BTRFS_NUM_BACKUP_ROOTS 4\nstruct btrfs_root_backup {\n\t__le64 tree_root;\n\t__le64 tree_root_gen;\n\n\t__le64 chunk_root;\n\t__le64 chunk_root_gen;\n\n\t__le64 extent_root;\n\t__le64 extent_root_gen;\n\n\t__le64 fs_root;\n\t__le64 fs_root_gen;\n\n\t__le64 dev_root;\n\t__le64 dev_root_gen;\n\n\t__le64 csum_root;\n\t__le64 csum_root_gen;\n\n\t__le64 total_bytes;\n\t__le64 bytes_used;\n\t__le64 num_devices;\n\t/* future */\n\t__le64 unused_64[4];\n\n\tu8 tree_root_level;\n\tu8 chunk_root_level;\n\tu8 extent_root_level;\n\tu8 fs_root_level;\n\tu8 dev_root_level;\n\tu8 csum_root_level;\n\t/* future and to align */\n\tu8 unused_8[10];\n} __attribute__ ((__packed__));\n\n/*\n * the super block basically lists the main trees of the FS\n * it currently lacks any block count etc etc\n */\nstruct btrfs_super_block {\n\tu8 csum[BTRFS_CSUM_SIZE];\n\t/* the first 4 fields must match struct btrfs_header */\n\tu8 fsid[BTRFS_FSID_SIZE];    /* FS specific uuid */\n\t__le64 bytenr; /* this block number */\n\t__le64 flags;\n\n\t/* allowed to be different from the btrfs_header from here own down */\n\t__le64 magic;\n\t__le64 generation;\n\t__le64 root;\n\t__le64 chunk_root;\n\t__le64 log_root;\n\n\t/* this will help find the new super based on the log root */\n\t__le64 log_root_transid;\n\t__le64 total_bytes;\n\t__le64 bytes_used;\n\t__le64 root_dir_objectid;\n\t__le64 num_devices;\n\t__le32 sectorsize;\n\t__le32 nodesize;\n\t__le32 leafsize;\n\t__le32 stripesize;\n\t__le32 sys_chunk_array_size;\n\t__le64 chunk_root_generation;\n\t__le64 compat_flags;\n\t__le64 compat_ro_flags;\n\t__le64 incompat_flags;\n\t__le16 csum_type;\n\tu8 root_level;\n\tu8 chunk_root_level;\n\tu8 log_root_level;\n\tstruct btrfs_dev_item dev_item;\n\n\tchar label[BTRFS_LABEL_SIZE];\n\n\t__le64 cache_generation;\n\n\t/* future expansion */\n\t__le64 reserved[31];\n\tu8 sys_chunk_array[BTRFS_SYSTEM_CHUNK_ARRAY_SIZE];\n\tstruct btrfs_root_backup super_roots[BTRFS_NUM_BACKUP_ROOTS];\n} __attribute__ ((__packed__));\n\n/*\n * Compat flags that we support.  If any incompat flags are set other than the\n * ones specified below then we will fail to mount\n */\n#define BTRFS_FEATURE_INCOMPAT_MIXED_BACKREF\t(1ULL << 0)\n#define BTRFS_FEATURE_INCOMPAT_DEFAULT_SUBVOL\t(1ULL << 1)\n#define BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS\t(1ULL << 2)\n#define BTRFS_FEATURE_INCOMPAT_COMPRESS_LZO\t(1ULL << 3)\n/*\n * some patches floated around with a second compression method\n * lets save that incompat here for when they do get in\n * Note we don't actually support it, we're just reserving the\n * number\n */\n#define BTRFS_FEATURE_INCOMPAT_COMPRESS_LZOv2\t(1ULL << 4)\n\n/*\n * older kernels tried to do bigger metadata blocks, but the\n * code was pretty buggy.  Lets not let them try anymore.\n */\n#define BTRFS_FEATURE_INCOMPAT_BIG_METADATA\t(1ULL << 5)\n\n#define BTRFS_FEATURE_INCOMPAT_EXTENDED_IREF\t(1ULL << 6)\n\n#define BTRFS_FEATURE_COMPAT_SUPP\t\t0ULL\n#define BTRFS_FEATURE_COMPAT_RO_SUPP\t\t0ULL\n#define BTRFS_FEATURE_INCOMPAT_SUPP\t\t\t\\\n\t(BTRFS_FEATURE_INCOMPAT_MIXED_BACKREF |\t\t\\\n\t BTRFS_FEATURE_INCOMPAT_DEFAULT_SUBVOL |\t\\\n\t BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS |\t\t\\\n\t BTRFS_FEATURE_INCOMPAT_BIG_METADATA |\t\t\\\n\t BTRFS_FEATURE_INCOMPAT_COMPRESS_LZO |\t\t\\\n\t BTRFS_FEATURE_INCOMPAT_EXTENDED_IREF)\n\n/*\n * A leaf is full of items. offset and size tell us where to find\n * the item in the leaf (relative to the start of the data area)\n */\nstruct btrfs_item {\n\tstruct btrfs_disk_key key;\n\t__le32 offset;\n\t__le32 size;\n} __attribute__ ((__packed__));\n\n/*\n * leaves have an item area and a data area:\n * [item0, item1....itemN] [free space] [dataN...data1, data0]\n *\n * The data is separate from the items to get the keys closer together\n * during searches.\n */\nstruct btrfs_leaf {\n\tstruct btrfs_header header;\n\tstruct btrfs_item items[];\n} __attribute__ ((__packed__));\n\n/*\n * all non-leaf blocks are nodes, they hold only keys and pointers to\n * other blocks\n */\nstruct btrfs_key_ptr {\n\tstruct btrfs_disk_key key;\n\t__le64 blockptr;\n\t__le64 generation;\n} __attribute__ ((__packed__));\n\nstruct btrfs_node {\n\tstruct btrfs_header header;\n\tstruct btrfs_key_ptr ptrs[];\n} __attribute__ ((__packed__));\n\n/*\n * btrfs_paths remember the path taken from the root down to the leaf.\n * level 0 is always the leaf, and nodes[1...BTRFS_MAX_LEVEL] will point\n * to any other levels that are present.\n *\n * The slots array records the index of the item or block pointer\n * used while walking the tree.\n */\nstruct btrfs_path {\n\tstruct extent_buffer *nodes[BTRFS_MAX_LEVEL];\n\tint slots[BTRFS_MAX_LEVEL];\n\t/* if there is real range locking, this locks field will change */\n\tint locks[BTRFS_MAX_LEVEL];\n\tint reada;\n\t/* keep some upper locks as we walk down */\n\tint lowest_level;\n\n\t/*\n\t * set by btrfs_split_item, tells search_slot to keep all locks\n\t * and to force calls to keep space in the nodes\n\t */\n\tunsigned int search_for_split:1;\n\tunsigned int keep_locks:1;\n\tunsigned int skip_locking:1;\n\tunsigned int leave_spinning:1;\n\tunsigned int search_commit_root:1;\n\tunsigned int really_keep_locks:1;\n};\n\n/*\n * items in the extent btree are used to record the objectid of the\n * owner of the block and the number of references\n */\n\nstruct btrfs_extent_item {\n\t__le64 refs;\n\t__le64 generation;\n\t__le64 flags;\n} __attribute__ ((__packed__));\n\nstruct btrfs_extent_item_v0 {\n\t__le32 refs;\n} __attribute__ ((__packed__));\n\n#define BTRFS_MAX_EXTENT_ITEM_SIZE(r) ((BTRFS_LEAF_DATA_SIZE(r) >> 4) - \\\n\t\t\t\t\tsizeof(struct btrfs_item))\n\n#define BTRFS_EXTENT_FLAG_DATA\t\t(1ULL << 0)\n#define BTRFS_EXTENT_FLAG_TREE_BLOCK\t(1ULL << 1)\n\n/* following flags only apply to tree blocks */\n\n/* use full backrefs for extent pointers in the block */\n#define BTRFS_BLOCK_FLAG_FULL_BACKREF\t(1ULL << 8)\n\n/*\n * this flag is only used internally by scrub and may be changed at any time\n * it is only declared here to avoid collisions\n */\n#define BTRFS_EXTENT_FLAG_SUPER\t\t(1ULL << 48)\n\nstruct btrfs_tree_block_info {\n\tstruct btrfs_disk_key key;\n\tu8 level;\n} __attribute__ ((__packed__));\n\nstruct btrfs_extent_data_ref {\n\t__le64 root;\n\t__le64 objectid;\n\t__le64 offset;\n\t__le32 count;\n} __attribute__ ((__packed__));\n\nstruct btrfs_shared_data_ref {\n\t__le32 count;\n} __attribute__ ((__packed__));\n\nstruct btrfs_extent_inline_ref {\n\tu8 type;\n\t__le64 offset;\n} __attribute__ ((__packed__));\n\n/* old style backrefs item */\nstruct btrfs_extent_ref_v0 {\n\t__le64 root;\n\t__le64 generation;\n\t__le64 objectid;\n\t__le32 count;\n} __attribute__ ((__packed__));\n\n\n/* dev extents record free space on individual devices.  The owner\n * field points back to the chunk allocation mapping tree that allocated\n * the extent.  The chunk tree uuid field is a way to double check the owner\n */\nstruct btrfs_dev_extent {\n\t__le64 chunk_tree;\n\t__le64 chunk_objectid;\n\t__le64 chunk_offset;\n\t__le64 length;\n\tu8 chunk_tree_uuid[BTRFS_UUID_SIZE];\n} __attribute__ ((__packed__));\n\nstruct btrfs_inode_ref {\n\t__le64 index;\n\t__le16 name_len;\n\t/* name goes here */\n} __attribute__ ((__packed__));\n\nstruct btrfs_inode_extref {\n\t__le64 parent_objectid;\n\t__le64 index;\n\t__le16 name_len;\n\t__u8   name[0];\n\t/* name goes here */\n} __attribute__ ((__packed__));\n\nstruct btrfs_timespec {\n\t__le64 sec;\n\t__le32 nsec;\n} __attribute__ ((__packed__));\n\nenum btrfs_compression_type {\n\tBTRFS_COMPRESS_NONE  = 0,\n\tBTRFS_COMPRESS_ZLIB  = 1,\n\tBTRFS_COMPRESS_LZO   = 2,\n\tBTRFS_COMPRESS_TYPES = 2,\n\tBTRFS_COMPRESS_LAST  = 3,\n};\n\nstruct btrfs_inode_item {\n\t/* nfs style generation number */\n\t__le64 generation;\n\t/* transid that last touched this inode */\n\t__le64 transid;\n\t__le64 size;\n\t__le64 nbytes;\n\t__le64 block_group;\n\t__le32 nlink;\n\t__le32 uid;\n\t__le32 gid;\n\t__le32 mode;\n\t__le64 rdev;\n\t__le64 flags;\n\n\t/* modification sequence number for NFS */\n\t__le64 sequence;\n\n\t/*\n\t * a little future expansion, for more than this we can\n\t * just grow the inode item and version it\n\t */\n\t__le64 reserved[4];\n\tstruct btrfs_timespec atime;\n\tstruct btrfs_timespec ctime;\n\tstruct btrfs_timespec mtime;\n\tstruct btrfs_timespec otime;\n} __attribute__ ((__packed__));\n\nstruct btrfs_dir_log_item {\n\t__le64 end;\n} __attribute__ ((__packed__));\n\nstruct btrfs_dir_item {\n\tstruct btrfs_disk_key location;\n\t__le64 transid;\n\t__le16 data_len;\n\t__le16 name_len;\n\tu8 type;\n} __attribute__ ((__packed__));\n\n#define BTRFS_ROOT_SUBVOL_RDONLY\t(1ULL << 0)\n\nstruct btrfs_root_item {\n\tstruct btrfs_inode_item inode;\n\t__le64 generation;\n\t__le64 root_dirid;\n\t__le64 bytenr;\n\t__le64 byte_limit;\n\t__le64 bytes_used;\n\t__le64 last_snapshot;\n\t__le64 flags;\n\t__le32 refs;\n\tstruct btrfs_disk_key drop_progress;\n\tu8 drop_level;\n\tu8 level;\n\n\t/*\n\t * The following fields appear after subvol_uuids+subvol_times\n\t * were introduced.\n\t */\n\n\t/*\n\t * This generation number is used to test if the new fields are valid\n\t * and up to date while reading the root item. Everytime the root item\n\t * is written out, the \"generation\" field is copied into this field. If\n\t * anyone ever mounted the fs with an older kernel, we will have\n\t * mismatching generation values here and thus must invalidate the\n\t * new fields. See btrfs_update_root and btrfs_find_last_root for\n\t * details.\n\t * the offset of generation_v2 is also used as the start for the memset\n\t * when invalidating the fields.\n\t */\n\t__le64 generation_v2;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tu8 parent_uuid[BTRFS_UUID_SIZE];\n\tu8 received_uuid[BTRFS_UUID_SIZE];\n\t__le64 ctransid; /* updated when an inode changes */\n\t__le64 otransid; /* trans when created */\n\t__le64 stransid; /* trans when sent. non-zero for received subvol */\n\t__le64 rtransid; /* trans when received. non-zero for received subvol */\n\tstruct btrfs_timespec ctime;\n\tstruct btrfs_timespec otime;\n\tstruct btrfs_timespec stime;\n\tstruct btrfs_timespec rtime;\n\t__le64 reserved[8]; /* for future */\n} __attribute__ ((__packed__));\n\n/*\n * this is used for both forward and backward root refs\n */\nstruct btrfs_root_ref {\n\t__le64 dirid;\n\t__le64 sequence;\n\t__le16 name_len;\n} __attribute__ ((__packed__));\n\nstruct btrfs_disk_balance_args {\n\t/*\n\t * profiles to operate on, single is denoted by\n\t * BTRFS_AVAIL_ALLOC_BIT_SINGLE\n\t */\n\t__le64 profiles;\n\n\t/* usage filter */\n\t__le64 usage;\n\n\t/* devid filter */\n\t__le64 devid;\n\n\t/* devid subset filter [pstart..pend) */\n\t__le64 pstart;\n\t__le64 pend;\n\n\t/* btrfs virtual address space subset filter [vstart..vend) */\n\t__le64 vstart;\n\t__le64 vend;\n\n\t/*\n\t * profile to convert to, single is denoted by\n\t * BTRFS_AVAIL_ALLOC_BIT_SINGLE\n\t */\n\t__le64 target;\n\n\t/* BTRFS_BALANCE_ARGS_* */\n\t__le64 flags;\n\n\t__le64 unused[8];\n} __attribute__ ((__packed__));\n\n/*\n * store balance parameters to disk so that balance can be properly\n * resumed after crash or unmount\n */\nstruct btrfs_balance_item {\n\t/* BTRFS_BALANCE_* */\n\t__le64 flags;\n\n\tstruct btrfs_disk_balance_args data;\n\tstruct btrfs_disk_balance_args meta;\n\tstruct btrfs_disk_balance_args sys;\n\n\t__le64 unused[4];\n} __attribute__ ((__packed__));\n\n#define BTRFS_FILE_EXTENT_INLINE 0\n#define BTRFS_FILE_EXTENT_REG 1\n#define BTRFS_FILE_EXTENT_PREALLOC 2\n\nstruct btrfs_file_extent_item {\n\t/*\n\t * transaction id that created this extent\n\t */\n\t__le64 generation;\n\t/*\n\t * max number of bytes to hold this extent in ram\n\t * when we split a compressed extent we can't know how big\n\t * each of the resulting pieces will be.  So, this is\n\t * an upper limit on the size of the extent in ram instead of\n\t * an exact limit.\n\t */\n\t__le64 ram_bytes;\n\n\t/*\n\t * 32 bits for the various ways we might encode the data,\n\t * including compression and encryption.  If any of these\n\t * are set to something a given disk format doesn't understand\n\t * it is treated like an incompat flag for reading and writing,\n\t * but not for stat.\n\t */\n\tu8 compression;\n\tu8 encryption;\n\t__le16 other_encoding; /* spare for later use */\n\n\t/* are we inline data or a real extent? */\n\tu8 type;\n\n\t/*\n\t * disk space consumed by the extent, checksum blocks are included\n\t * in these numbers\n\t */\n\t__le64 disk_bytenr;\n\t__le64 disk_num_bytes;\n\t/*\n\t * the logical offset in file blocks (no csums)\n\t * this extent record is for.  This allows a file extent to point\n\t * into the middle of an existing extent on disk, sharing it\n\t * between two snapshots (useful if some bytes in the middle of the\n\t * extent have changed\n\t */\n\t__le64 offset;\n\t/*\n\t * the logical number of file blocks (no csums included).  This\n\t * always reflects the size uncompressed and without encoding.\n\t */\n\t__le64 num_bytes;\n\n} __attribute__ ((__packed__));\n\nstruct btrfs_csum_item {\n\tu8 csum;\n} __attribute__ ((__packed__));\n\nstruct btrfs_dev_stats_item {\n\t/*\n\t * grow this item struct at the end for future enhancements and keep\n\t * the existing values unchanged\n\t */\n\t__le64 values[BTRFS_DEV_STAT_VALUES_MAX];\n} __attribute__ ((__packed__));\n\n#define BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_ALWAYS\t0\n#define BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_AVOID\t1\n#define BTRFS_DEV_REPLACE_ITEM_STATE_NEVER_STARTED\t0\n#define BTRFS_DEV_REPLACE_ITEM_STATE_STARTED\t\t1\n#define BTRFS_DEV_REPLACE_ITEM_STATE_SUSPENDED\t\t2\n#define BTRFS_DEV_REPLACE_ITEM_STATE_FINISHED\t\t3\n#define BTRFS_DEV_REPLACE_ITEM_STATE_CANCELED\t\t4\n\nstruct btrfs_dev_replace {\n\tu64 replace_state;\t/* see #define above */\n\tu64 time_started;\t/* seconds since 1-Jan-1970 */\n\tu64 time_stopped;\t/* seconds since 1-Jan-1970 */\n\tatomic64_t num_write_errors;\n\tatomic64_t num_uncorrectable_read_errors;\n\n\tu64 cursor_left;\n\tu64 committed_cursor_left;\n\tu64 cursor_left_last_write_of_item;\n\tu64 cursor_right;\n\n\tu64 cont_reading_from_srcdev_mode;\t/* see #define above */\n\n\tint is_valid;\n\tint item_needs_writeback;\n\tstruct btrfs_device *srcdev;\n\tstruct btrfs_device *tgtdev;\n\n\tpid_t lock_owner;\n\tatomic_t nesting_level;\n\tstruct mutex lock_finishing_cancel_unmount;\n\tstruct mutex lock_management_lock;\n\tstruct mutex lock;\n\n\tstruct btrfs_scrub_progress scrub_progress;\n};\n\nstruct btrfs_dev_replace_item {\n\t/*\n\t * grow this item struct at the end for future enhancements and keep\n\t * the existing values unchanged\n\t */\n\t__le64 src_devid;\n\t__le64 cursor_left;\n\t__le64 cursor_right;\n\t__le64 cont_reading_from_srcdev_mode;\n\n\t__le64 replace_state;\n\t__le64 time_started;\n\t__le64 time_stopped;\n\t__le64 num_write_errors;\n\t__le64 num_uncorrectable_read_errors;\n} __attribute__ ((__packed__));\n\n/* different types of block groups (and chunks) */\n#define BTRFS_BLOCK_GROUP_DATA\t\t(1ULL << 0)\n#define BTRFS_BLOCK_GROUP_SYSTEM\t(1ULL << 1)\n#define BTRFS_BLOCK_GROUP_METADATA\t(1ULL << 2)\n#define BTRFS_BLOCK_GROUP_RAID0\t\t(1ULL << 3)\n#define BTRFS_BLOCK_GROUP_RAID1\t\t(1ULL << 4)\n#define BTRFS_BLOCK_GROUP_DUP\t\t(1ULL << 5)\n#define BTRFS_BLOCK_GROUP_RAID10\t(1ULL << 6)\n#define BTRFS_BLOCK_GROUP_RESERVED\tBTRFS_AVAIL_ALLOC_BIT_SINGLE\n#define BTRFS_NR_RAID_TYPES\t\t5\n\n#define BTRFS_BLOCK_GROUP_TYPE_MASK\t(BTRFS_BLOCK_GROUP_DATA |    \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_SYSTEM |  \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_METADATA)\n\n#define BTRFS_BLOCK_GROUP_PROFILE_MASK\t(BTRFS_BLOCK_GROUP_RAID0 |   \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_RAID1 |   \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_DUP |     \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_RAID10)\n/*\n * We need a bit for restriper to be able to tell when chunks of type\n * SINGLE are available.  This \"extended\" profile format is used in\n * fs_info->avail_*_alloc_bits (in-memory) and balance item fields\n * (on-disk).  The corresponding on-disk bit in chunk.type is reserved\n * to avoid remappings between two formats in future.\n */\n#define BTRFS_AVAIL_ALLOC_BIT_SINGLE\t(1ULL << 48)\n\n#define BTRFS_EXTENDED_PROFILE_MASK\t(BTRFS_BLOCK_GROUP_PROFILE_MASK | \\\n\t\t\t\t\t BTRFS_AVAIL_ALLOC_BIT_SINGLE)\n\nstatic inline u64 chunk_to_extended(u64 flags)\n{\n\tif ((flags & BTRFS_BLOCK_GROUP_PROFILE_MASK) == 0)\n\t\tflags |= BTRFS_AVAIL_ALLOC_BIT_SINGLE;\n\n\treturn flags;\n}\nstatic inline u64 extended_to_chunk(u64 flags)\n{\n\treturn flags & ~BTRFS_AVAIL_ALLOC_BIT_SINGLE;\n}\n\nstruct btrfs_block_group_item {\n\t__le64 used;\n\t__le64 chunk_objectid;\n\t__le64 flags;\n} __attribute__ ((__packed__));\n\n/*\n * is subvolume quota turned on?\n */\n#define BTRFS_QGROUP_STATUS_FLAG_ON\t\t(1ULL << 0)\n/*\n * SCANNING is set during the initialization phase\n */\n#define BTRFS_QGROUP_STATUS_FLAG_SCANNING\t(1ULL << 1)\n/*\n * Some qgroup entries are known to be out of date,\n * either because the configuration has changed in a way that\n * makes a rescan necessary, or because the fs has been mounted\n * with a non-qgroup-aware version.\n * Turning qouta off and on again makes it inconsistent, too.\n */\n#define BTRFS_QGROUP_STATUS_FLAG_INCONSISTENT\t(1ULL << 2)\n\n#define BTRFS_QGROUP_STATUS_VERSION        1\n\nstruct btrfs_qgroup_status_item {\n\t__le64 version;\n\t/*\n\t * the generation is updated during every commit. As older\n\t * versions of btrfs are not aware of qgroups, it will be\n\t * possible to detect inconsistencies by checking the\n\t * generation on mount time\n\t */\n\t__le64 generation;\n\n\t/* flag definitions see above */\n\t__le64 flags;\n\n\t/*\n\t * only used during scanning to record the progress\n\t * of the scan. It contains a logical address\n\t */\n\t__le64 scan;\n} __attribute__ ((__packed__));\n\nstruct btrfs_qgroup_info_item {\n\t__le64 generation;\n\t__le64 rfer;\n\t__le64 rfer_cmpr;\n\t__le64 excl;\n\t__le64 excl_cmpr;\n} __attribute__ ((__packed__));\n\n/* flags definition for qgroup limits */\n#define BTRFS_QGROUP_LIMIT_MAX_RFER\t(1ULL << 0)\n#define BTRFS_QGROUP_LIMIT_MAX_EXCL\t(1ULL << 1)\n#define BTRFS_QGROUP_LIMIT_RSV_RFER\t(1ULL << 2)\n#define BTRFS_QGROUP_LIMIT_RSV_EXCL\t(1ULL << 3)\n#define BTRFS_QGROUP_LIMIT_RFER_CMPR\t(1ULL << 4)\n#define BTRFS_QGROUP_LIMIT_EXCL_CMPR\t(1ULL << 5)\n\nstruct btrfs_qgroup_limit_item {\n\t/*\n\t * only updated when any of the other values change\n\t */\n\t__le64 flags;\n\t__le64 max_rfer;\n\t__le64 max_excl;\n\t__le64 rsv_rfer;\n\t__le64 rsv_excl;\n} __attribute__ ((__packed__));\n\nstruct btrfs_space_info {\n\tu64 flags;\n\n\tu64 total_bytes;\t/* total bytes in the space,\n\t\t\t\t   this doesn't take mirrors into account */\n\tu64 bytes_used;\t\t/* total bytes used,\n\t\t\t\t   this doesn't take mirrors into account */\n\tu64 bytes_pinned;\t/* total bytes pinned, will be freed when the\n\t\t\t\t   transaction finishes */\n\tu64 bytes_reserved;\t/* total bytes the allocator has reserved for\n\t\t\t\t   current allocations */\n\tu64 bytes_readonly;\t/* total bytes that are read only */\n\n\tu64 bytes_may_use;\t/* number of bytes that may be used for\n\t\t\t\t   delalloc/allocations */\n\tu64 disk_used;\t\t/* total bytes used on disk */\n\tu64 disk_total;\t\t/* total bytes on disk, takes mirrors into\n\t\t\t\t   account */\n\n\t/*\n\t * we bump reservation progress every time we decrement\n\t * bytes_reserved.  This way people waiting for reservations\n\t * know something good has happened and they can check\n\t * for progress.  The number here isn't to be trusted, it\n\t * just shows reclaim activity\n\t */\n\tunsigned long reservation_progress;\n\n\tunsigned int full:1;\t/* indicates that we cannot allocate any more\n\t\t\t\t   chunks for this space */\n\tunsigned int chunk_alloc:1;\t/* set if we are allocating a chunk */\n\n\tunsigned int flush:1;\t\t/* set if we are trying to make space */\n\n\tunsigned int force_alloc;\t/* set if we need to force a chunk\n\t\t\t\t\t   alloc for this space */\n\n\tstruct list_head list;\n\n\t/* for block groups in our same type */\n\tstruct list_head block_groups[BTRFS_NR_RAID_TYPES];\n\tspinlock_t lock;\n\tstruct rw_semaphore groups_sem;\n\twait_queue_head_t wait;\n};\n\n#define\tBTRFS_BLOCK_RSV_GLOBAL\t\t1\n#define\tBTRFS_BLOCK_RSV_DELALLOC\t2\n#define\tBTRFS_BLOCK_RSV_TRANS\t\t3\n#define\tBTRFS_BLOCK_RSV_CHUNK\t\t4\n#define\tBTRFS_BLOCK_RSV_DELOPS\t\t5\n#define\tBTRFS_BLOCK_RSV_EMPTY\t\t6\n#define\tBTRFS_BLOCK_RSV_TEMP\t\t7\n\nstruct btrfs_block_rsv {\n\tu64 size;\n\tu64 reserved;\n\tstruct btrfs_space_info *space_info;\n\tspinlock_t lock;\n\tunsigned short full;\n\tunsigned short type;\n\tunsigned short failfast;\n};\n\n/*\n * free clusters are used to claim free space in relatively large chunks,\n * allowing us to do less seeky writes.  They are used for all metadata\n * allocations and data allocations in ssd mode.\n */\nstruct btrfs_free_cluster {\n\tspinlock_t lock;\n\tspinlock_t refill_lock;\n\tstruct rb_root root;\n\n\t/* largest extent in this cluster */\n\tu64 max_size;\n\n\t/* first extent starting offset */\n\tu64 window_start;\n\n\tstruct btrfs_block_group_cache *block_group;\n\t/*\n\t * when a cluster is allocated from a block group, we put the\n\t * cluster onto a list in the block group so that it can\n\t * be freed before the block group is freed.\n\t */\n\tstruct list_head block_group_list;\n};\n\nenum btrfs_caching_type {\n\tBTRFS_CACHE_NO\t\t= 0,\n\tBTRFS_CACHE_STARTED\t= 1,\n\tBTRFS_CACHE_FAST\t= 2,\n\tBTRFS_CACHE_FINISHED\t= 3,\n};\n\nenum btrfs_disk_cache_state {\n\tBTRFS_DC_WRITTEN\t= 0,\n\tBTRFS_DC_ERROR\t\t= 1,\n\tBTRFS_DC_CLEAR\t\t= 2,\n\tBTRFS_DC_SETUP\t\t= 3,\n\tBTRFS_DC_NEED_WRITE\t= 4,\n};\n\nstruct btrfs_caching_control {\n\tstruct list_head list;\n\tstruct mutex mutex;\n\twait_queue_head_t wait;\n\tstruct btrfs_work work;\n\tstruct btrfs_block_group_cache *block_group;\n\tu64 progress;\n\tatomic_t count;\n};\n\nstruct btrfs_block_group_cache {\n\tstruct btrfs_key key;\n\tstruct btrfs_block_group_item item;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct inode *inode;\n\tspinlock_t lock;\n\tu64 pinned;\n\tu64 reserved;\n\tu64 bytes_super;\n\tu64 flags;\n\tu64 sectorsize;\n\tu64 cache_generation;\n\tunsigned int ro:1;\n\tunsigned int dirty:1;\n\tunsigned int iref:1;\n\n\tint disk_cache_state;\n\n\t/* cache tracking stuff */\n\tint cached;\n\tstruct btrfs_caching_control *caching_ctl;\n\tu64 last_byte_to_unpin;\n\n\tstruct btrfs_space_info *space_info;\n\n\t/* free space cache stuff */\n\tstruct btrfs_free_space_ctl *free_space_ctl;\n\n\t/* block group cache stuff */\n\tstruct rb_node cache_node;\n\n\t/* for block groups in the same raid type */\n\tstruct list_head list;\n\n\t/* usage count */\n\tatomic_t count;\n\n\t/* List of struct btrfs_free_clusters for this block group.\n\t * Today it will only have one thing on it, but that may change\n\t */\n\tstruct list_head cluster_list;\n\n\t/* For delayed block group creation */\n\tstruct list_head new_bg_list;\n};\n\n/* delayed seq elem */\nstruct seq_list {\n\tstruct list_head list;\n\tu64 seq;\n};\n\n/* fs_info */\nstruct reloc_control;\nstruct btrfs_device;\nstruct btrfs_fs_devices;\nstruct btrfs_balance_control;\nstruct btrfs_delayed_root;\nstruct btrfs_fs_info {\n\tu8 fsid[BTRFS_FSID_SIZE];\n\tu8 chunk_tree_uuid[BTRFS_UUID_SIZE];\n\tstruct btrfs_root *extent_root;\n\tstruct btrfs_root *tree_root;\n\tstruct btrfs_root *chunk_root;\n\tstruct btrfs_root *dev_root;\n\tstruct btrfs_root *fs_root;\n\tstruct btrfs_root *csum_root;\n\tstruct btrfs_root *quota_root;\n\n\t/* the log root tree is a directory of all the other log roots */\n\tstruct btrfs_root *log_root_tree;\n\n\tspinlock_t fs_roots_radix_lock;\n\tstruct radix_tree_root fs_roots_radix;\n\n\t/* block group cache stuff */\n\tspinlock_t block_group_cache_lock;\n\tstruct rb_root block_group_cache_tree;\n\n\t/* keep track of unallocated space */\n\tspinlock_t free_chunk_lock;\n\tu64 free_chunk_space;\n\n\tstruct extent_io_tree freed_extents[2];\n\tstruct extent_io_tree *pinned_extents;\n\n\t/* logical->physical extent mapping */\n\tstruct btrfs_mapping_tree mapping_tree;\n\n\t/*\n\t * block reservation for extent, checksum, root tree and\n\t * delayed dir index item\n\t */\n\tstruct btrfs_block_rsv global_block_rsv;\n\t/* block reservation for delay allocation */\n\tstruct btrfs_block_rsv delalloc_block_rsv;\n\t/* block reservation for metadata operations */\n\tstruct btrfs_block_rsv trans_block_rsv;\n\t/* block reservation for chunk tree */\n\tstruct btrfs_block_rsv chunk_block_rsv;\n\t/* block reservation for delayed operations */\n\tstruct btrfs_block_rsv delayed_block_rsv;\n\n\tstruct btrfs_block_rsv empty_block_rsv;\n\n\tu64 generation;\n\tu64 last_trans_committed;\n\n\t/*\n\t * this is updated to the current trans every time a full commit\n\t * is required instead of the faster short fsync log commits\n\t */\n\tu64 last_trans_log_full_commit;\n\tunsigned long mount_opt;\n\tunsigned long compress_type:4;\n\tu64 max_inline;\n\tu64 alloc_start;\n\tstruct btrfs_transaction *running_transaction;\n\twait_queue_head_t transaction_throttle;\n\twait_queue_head_t transaction_wait;\n\twait_queue_head_t transaction_blocked_wait;\n\twait_queue_head_t async_submit_wait;\n\n\tstruct btrfs_super_block *super_copy;\n\tstruct btrfs_super_block *super_for_commit;\n\tstruct block_device *__bdev;\n\tstruct super_block *sb;\n\tstruct inode *btree_inode;\n\tstruct backing_dev_info bdi;\n\tstruct mutex tree_log_mutex;\n\tstruct mutex transaction_kthread_mutex;\n\tstruct mutex cleaner_mutex;\n\tstruct mutex chunk_mutex;\n\tstruct mutex volume_mutex;\n\t/*\n\t * this protects the ordered operations list only while we are\n\t * processing all of the entries on it.  This way we make\n\t * sure the commit code doesn't find the list temporarily empty\n\t * because another function happens to be doing non-waiting preflush\n\t * before jumping into the main commit.\n\t */\n\tstruct mutex ordered_operations_mutex;\n\tstruct rw_semaphore extent_commit_sem;\n\n\tstruct rw_semaphore cleanup_work_sem;\n\n\tstruct rw_semaphore subvol_sem;\n\tstruct srcu_struct subvol_srcu;\n\n\tspinlock_t trans_lock;\n\t/*\n\t * the reloc mutex goes with the trans lock, it is taken\n\t * during commit to protect us from the relocation code\n\t */\n\tstruct mutex reloc_mutex;\n\n\tstruct list_head trans_list;\n\tstruct list_head dead_roots;\n\tstruct list_head caching_block_groups;\n\n\tspinlock_t delayed_iput_lock;\n\tstruct list_head delayed_iputs;\n\n\t/* this protects tree_mod_seq_list */\n\tspinlock_t tree_mod_seq_lock;\n\tatomic_t tree_mod_seq;\n\tstruct list_head tree_mod_seq_list;\n\tstruct seq_list tree_mod_seq_elem;\n\n\t/* this protects tree_mod_log */\n\trwlock_t tree_mod_log_lock;\n\tstruct rb_root tree_mod_log;\n\n\tatomic_t nr_async_submits;\n\tatomic_t async_submit_draining;\n\tatomic_t nr_async_bios;\n\tatomic_t async_delalloc_pages;\n\tatomic_t open_ioctl_trans;\n\n\t/*\n\t * this is used by the balancing code to wait for all the pending\n\t * ordered extents\n\t */\n\tspinlock_t ordered_extent_lock;\n\n\t/*\n\t * all of the data=ordered extents pending writeback\n\t * these can span multiple transactions and basically include\n\t * every dirty data page that isn't from nodatacow\n\t */\n\tstruct list_head ordered_extents;\n\n\t/*\n\t * all of the inodes that have delalloc bytes.  It is possible for\n\t * this list to be empty even when there is still dirty data=ordered\n\t * extents waiting to finish IO.\n\t */\n\tstruct list_head delalloc_inodes;\n\n\t/*\n\t * special rename and truncate targets that must be on disk before\n\t * we're allowed to commit.  This is basically the ext3 style\n\t * data=ordered list.\n\t */\n\tstruct list_head ordered_operations;\n\n\t/*\n\t * there is a pool of worker threads for checksumming during writes\n\t * and a pool for checksumming after reads.  This is because readers\n\t * can run with FS locks held, and the writers may be waiting for\n\t * those locks.  We don't want ordering in the pending list to cause\n\t * deadlocks, and so the two are serviced separately.\n\t *\n\t * A third pool does submit_bio to avoid deadlocking with the other\n\t * two\n\t */\n\tstruct btrfs_workers generic_worker;\n\tstruct btrfs_workers workers;\n\tstruct btrfs_workers delalloc_workers;\n\tstruct btrfs_workers flush_workers;\n\tstruct btrfs_workers endio_workers;\n\tstruct btrfs_workers endio_meta_workers;\n\tstruct btrfs_workers endio_meta_write_workers;\n\tstruct btrfs_workers endio_write_workers;\n\tstruct btrfs_workers endio_freespace_worker;\n\tstruct btrfs_workers submit_workers;\n\tstruct btrfs_workers caching_workers;\n\tstruct btrfs_workers readahead_workers;\n\n\t/*\n\t * fixup workers take dirty pages that didn't properly go through\n\t * the cow mechanism and make them safe to write.  It happens\n\t * for the sys_munmap function call path\n\t */\n\tstruct btrfs_workers fixup_workers;\n\tstruct btrfs_workers delayed_workers;\n\tstruct task_struct *transaction_kthread;\n\tstruct task_struct *cleaner_kthread;\n\tint thread_pool_size;\n\n\tstruct kobject super_kobj;\n\tstruct completion kobj_unregister;\n\tint do_barriers;\n\tint closing;\n\tint log_root_recovering;\n\tint enospc_unlink;\n\tint trans_no_join;\n\n\tu64 total_pinned;\n\n\t/* protected by the delalloc lock, used to keep from writing\n\t * metadata until there is a nice batch\n\t */\n\tu64 dirty_metadata_bytes;\n\tstruct list_head dirty_cowonly_roots;\n\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * the space_info list is almost entirely read only.  It only changes\n\t * when we add a new raid type to the FS, and that happens\n\t * very rarely.  RCU is used to protect it.\n\t */\n\tstruct list_head space_info;\n\n\tstruct btrfs_space_info *data_sinfo;\n\n\tstruct reloc_control *reloc_ctl;\n\n\tspinlock_t delalloc_lock;\n\tu64 delalloc_bytes;\n\n\t/* data_alloc_cluster is only used in ssd mode */\n\tstruct btrfs_free_cluster data_alloc_cluster;\n\n\t/* all metadata allocations go through this cluster */\n\tstruct btrfs_free_cluster meta_alloc_cluster;\n\n\t/* auto defrag inodes go here */\n\tspinlock_t defrag_inodes_lock;\n\tstruct rb_root defrag_inodes;\n\tatomic_t defrag_running;\n\n\t/*\n\t * these three are in extended format (availability of single\n\t * chunks is denoted by BTRFS_AVAIL_ALLOC_BIT_SINGLE bit, other\n\t * types are denoted by corresponding BTRFS_BLOCK_GROUP_* bits)\n\t */\n\tu64 avail_data_alloc_bits;\n\tu64 avail_metadata_alloc_bits;\n\tu64 avail_system_alloc_bits;\n\n\t/* restriper state */\n\tspinlock_t balance_lock;\n\tstruct mutex balance_mutex;\n\tatomic_t balance_running;\n\tatomic_t balance_pause_req;\n\tatomic_t balance_cancel_req;\n\tstruct btrfs_balance_control *balance_ctl;\n\twait_queue_head_t balance_wait_q;\n\n\tunsigned data_chunk_allocations;\n\tunsigned metadata_ratio;\n\n\tvoid *bdev_holder;\n\n\t/* private scrub information */\n\tstruct mutex scrub_lock;\n\tatomic_t scrubs_running;\n\tatomic_t scrub_pause_req;\n\tatomic_t scrubs_paused;\n\tatomic_t scrub_cancel_req;\n\twait_queue_head_t scrub_pause_wait;\n\tstruct rw_semaphore scrub_super_lock;\n\tint scrub_workers_refcnt;\n\tstruct btrfs_workers scrub_workers;\n\tstruct btrfs_workers scrub_wr_completion_workers;\n\tstruct btrfs_workers scrub_nocow_workers;\n\n#ifdef CONFIG_BTRFS_FS_CHECK_INTEGRITY\n\tu32 check_integrity_print_mask;\n#endif\n\t/*\n\t * quota information\n\t */\n\tunsigned int quota_enabled:1;\n\n\t/*\n\t * quota_enabled only changes state after a commit. This holds the\n\t * next state.\n\t */\n\tunsigned int pending_quota_state:1;\n\n\t/* is qgroup tracking in a consistent state? */\n\tu64 qgroup_flags;\n\n\t/* holds configuration and tracking. Protected by qgroup_lock */\n\tstruct rb_root qgroup_tree;\n\tspinlock_t qgroup_lock;\n\n\t/* list of dirty qgroups to be written at next commit */\n\tstruct list_head dirty_qgroups;\n\n\t/* used by btrfs_qgroup_record_ref for an efficient tree traversal */\n\tu64 qgroup_seq;\n\n\t/* filesystem state */\n\tu64 fs_state;\n\n\tstruct btrfs_delayed_root *delayed_root;\n\n\t/* readahead tree */\n\tspinlock_t reada_lock;\n\tstruct radix_tree_root reada_tree;\n\n\t/* next backup root to be overwritten */\n\tint backup_root_index;\n\n\tint num_tolerated_disk_barrier_failures;\n\n\t/* device replace state */\n\tstruct btrfs_dev_replace dev_replace;\n\n\tatomic_t mutually_exclusive_operation_running;\n};\n\n/*\n * in ram representation of the tree.  extent_root is used for all allocations\n * and for the extent tree extent_root root.\n */\nstruct btrfs_root {\n\tstruct extent_buffer *node;\n\n\tstruct extent_buffer *commit_root;\n\tstruct btrfs_root *log_root;\n\tstruct btrfs_root *reloc_root;\n\n\tstruct btrfs_root_item root_item;\n\tstruct btrfs_key root_key;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct extent_io_tree dirty_log_pages;\n\n\tstruct kobject root_kobj;\n\tstruct completion kobj_unregister;\n\tstruct mutex objectid_mutex;\n\n\tspinlock_t accounting_lock;\n\tstruct btrfs_block_rsv *block_rsv;\n\n\t/* free ino cache stuff */\n\tstruct mutex fs_commit_mutex;\n\tstruct btrfs_free_space_ctl *free_ino_ctl;\n\tenum btrfs_caching_type cached;\n\tspinlock_t cache_lock;\n\twait_queue_head_t cache_wait;\n\tstruct btrfs_free_space_ctl *free_ino_pinned;\n\tu64 cache_progress;\n\tstruct inode *cache_inode;\n\n\tstruct mutex log_mutex;\n\twait_queue_head_t log_writer_wait;\n\twait_queue_head_t log_commit_wait[2];\n\tatomic_t log_writers;\n\tatomic_t log_commit[2];\n\tatomic_t log_batch;\n\tunsigned long log_transid;\n\tunsigned long last_log_commit;\n\tpid_t log_start_pid;\n\tbool log_multiple_pids;\n\n\tu64 objectid;\n\tu64 last_trans;\n\n\t/* data allocations are done in sectorsize units */\n\tu32 sectorsize;\n\n\t/* node allocations are done in nodesize units */\n\tu32 nodesize;\n\n\t/* leaf allocations are done in leafsize units */\n\tu32 leafsize;\n\n\tu32 stripesize;\n\n\tu32 type;\n\n\tu64 highest_objectid;\n\n\t/* btrfs_record_root_in_trans is a multi-step process,\n\t * and it can race with the balancing code.   But the\n\t * race is very small, and only the first time the root\n\t * is added to each transaction.  So in_trans_setup\n\t * is used to tell us when more checks are required\n\t */\n\tunsigned long in_trans_setup;\n\tint ref_cows;\n\tint track_dirty;\n\tint in_radix;\n\n\tu64 defrag_trans_start;\n\tstruct btrfs_key defrag_progress;\n\tstruct btrfs_key defrag_max;\n\tint defrag_running;\n\tchar *name;\n\n\t/* the dirty list is only used by non-reference counted roots */\n\tstruct list_head dirty_list;\n\n\tstruct list_head root_list;\n\n\tspinlock_t orphan_lock;\n\tatomic_t orphan_inodes;\n\tstruct btrfs_block_rsv *orphan_block_rsv;\n\tint orphan_item_inserted;\n\tint orphan_cleanup_state;\n\n\tspinlock_t inode_lock;\n\t/* red-black tree that keeps track of in-memory inodes */\n\tstruct rb_root inode_tree;\n\n\t/*\n\t * radix tree that keeps track of delayed nodes of every inode,\n\t * protected by inode_lock\n\t */\n\tstruct radix_tree_root delayed_nodes_tree;\n\t/*\n\t * right now this just gets used so that a root has its own devid\n\t * for stat.  It may be used for more later\n\t */\n\tdev_t anon_dev;\n\n\tint force_cow;\n\n\tspinlock_t root_item_lock;\n};\n\nstruct btrfs_ioctl_defrag_range_args {\n\t/* start of the defrag operation */\n\t__u64 start;\n\n\t/* number of bytes to defrag, use (u64)-1 to say all */\n\t__u64 len;\n\n\t/*\n\t * flags for the operation, which can include turning\n\t * on compression for this one defrag\n\t */\n\t__u64 flags;\n\n\t/*\n\t * any extent bigger than this will be considered\n\t * already defragged.  Use 0 to take the kernel default\n\t * Use 1 to say every single extent must be rewritten\n\t */\n\t__u32 extent_thresh;\n\n\t/*\n\t * which compression method to use if turning on compression\n\t * for this defrag operation.  If unspecified, zlib will\n\t * be used\n\t */\n\t__u32 compress_type;\n\n\t/* spare for later */\n\t__u32 unused[4];\n};\n\n\n/*\n * inode items have the data typically returned from stat and store other\n * info about object characteristics.  There is one for every file and dir in\n * the FS\n */\n#define BTRFS_INODE_ITEM_KEY\t\t1\n#define BTRFS_INODE_REF_KEY\t\t12\n#define BTRFS_INODE_EXTREF_KEY\t\t13\n#define BTRFS_XATTR_ITEM_KEY\t\t24\n#define BTRFS_ORPHAN_ITEM_KEY\t\t48\n/* reserve 2-15 close to the inode for later flexibility */\n\n/*\n * dir items are the name -> inode pointers in a directory.  There is one\n * for every name in a directory.\n */\n#define BTRFS_DIR_LOG_ITEM_KEY  60\n#define BTRFS_DIR_LOG_INDEX_KEY 72\n#define BTRFS_DIR_ITEM_KEY\t84\n#define BTRFS_DIR_INDEX_KEY\t96\n/*\n * extent data is for file data\n */\n#define BTRFS_EXTENT_DATA_KEY\t108\n\n/*\n * extent csums are stored in a separate tree and hold csums for\n * an entire extent on disk.\n */\n#define BTRFS_EXTENT_CSUM_KEY\t128\n\n/*\n * root items point to tree roots.  They are typically in the root\n * tree used by the super block to find all the other trees\n */\n#define BTRFS_ROOT_ITEM_KEY\t132\n\n/*\n * root backrefs tie subvols and snapshots to the directory entries that\n * reference them\n */\n#define BTRFS_ROOT_BACKREF_KEY\t144\n\n/*\n * root refs make a fast index for listing all of the snapshots and\n * subvolumes referenced by a given root.  They point directly to the\n * directory item in the root that references the subvol\n */\n#define BTRFS_ROOT_REF_KEY\t156\n\n/*\n * extent items are in the extent map tree.  These record which blocks\n * are used, and how many references there are to each block\n */\n#define BTRFS_EXTENT_ITEM_KEY\t168\n\n#define BTRFS_TREE_BLOCK_REF_KEY\t176\n\n#define BTRFS_EXTENT_DATA_REF_KEY\t178\n\n#define BTRFS_EXTENT_REF_V0_KEY\t\t180\n\n#define BTRFS_SHARED_BLOCK_REF_KEY\t182\n\n#define BTRFS_SHARED_DATA_REF_KEY\t184\n\n/*\n * block groups give us hints into the extent allocation trees.  Which\n * blocks are free etc etc\n */\n#define BTRFS_BLOCK_GROUP_ITEM_KEY 192\n\n#define BTRFS_DEV_EXTENT_KEY\t204\n#define BTRFS_DEV_ITEM_KEY\t216\n#define BTRFS_CHUNK_ITEM_KEY\t228\n\n/*\n * Records the overall state of the qgroups.\n * There's only one instance of this key present,\n * (0, BTRFS_QGROUP_STATUS_KEY, 0)\n */\n#define BTRFS_QGROUP_STATUS_KEY         240\n/*\n * Records the currently used space of the qgroup.\n * One key per qgroup, (0, BTRFS_QGROUP_INFO_KEY, qgroupid).\n */\n#define BTRFS_QGROUP_INFO_KEY           242\n/*\n * Contains the user configured limits for the qgroup.\n * One key per qgroup, (0, BTRFS_QGROUP_LIMIT_KEY, qgroupid).\n */\n#define BTRFS_QGROUP_LIMIT_KEY          244\n/*\n * Records the child-parent relationship of qgroups. For\n * each relation, 2 keys are present:\n * (childid, BTRFS_QGROUP_RELATION_KEY, parentid)\n * (parentid, BTRFS_QGROUP_RELATION_KEY, childid)\n */\n#define BTRFS_QGROUP_RELATION_KEY       246\n\n#define BTRFS_BALANCE_ITEM_KEY\t248\n\n/*\n * Persistantly stores the io stats in the device tree.\n * One key for all stats, (0, BTRFS_DEV_STATS_KEY, devid).\n */\n#define BTRFS_DEV_STATS_KEY\t249\n\n/*\n * Persistantly stores the device replace state in the device tree.\n * The key is built like this: (0, BTRFS_DEV_REPLACE_KEY, 0).\n */\n#define BTRFS_DEV_REPLACE_KEY\t250\n\n/*\n * string items are for debugging.  They just store a short string of\n * data in the FS\n */\n#define BTRFS_STRING_ITEM_KEY\t253\n\n/*\n * Flags for mount options.\n *\n * Note: don't forget to add new options to btrfs_show_options()\n */\n#define BTRFS_MOUNT_NODATASUM\t\t(1 << 0)\n#define BTRFS_MOUNT_NODATACOW\t\t(1 << 1)\n#define BTRFS_MOUNT_NOBARRIER\t\t(1 << 2)\n#define BTRFS_MOUNT_SSD\t\t\t(1 << 3)\n#define BTRFS_MOUNT_DEGRADED\t\t(1 << 4)\n#define BTRFS_MOUNT_COMPRESS\t\t(1 << 5)\n#define BTRFS_MOUNT_NOTREELOG           (1 << 6)\n#define BTRFS_MOUNT_FLUSHONCOMMIT       (1 << 7)\n#define BTRFS_MOUNT_SSD_SPREAD\t\t(1 << 8)\n#define BTRFS_MOUNT_NOSSD\t\t(1 << 9)\n#define BTRFS_MOUNT_DISCARD\t\t(1 << 10)\n#define BTRFS_MOUNT_FORCE_COMPRESS      (1 << 11)\n#define BTRFS_MOUNT_SPACE_CACHE\t\t(1 << 12)\n#define BTRFS_MOUNT_CLEAR_CACHE\t\t(1 << 13)\n#define BTRFS_MOUNT_USER_SUBVOL_RM_ALLOWED (1 << 14)\n#define BTRFS_MOUNT_ENOSPC_DEBUG\t (1 << 15)\n#define BTRFS_MOUNT_AUTO_DEFRAG\t\t(1 << 16)\n#define BTRFS_MOUNT_INODE_MAP_CACHE\t(1 << 17)\n#define BTRFS_MOUNT_RECOVERY\t\t(1 << 18)\n#define BTRFS_MOUNT_SKIP_BALANCE\t(1 << 19)\n#define BTRFS_MOUNT_CHECK_INTEGRITY\t(1 << 20)\n#define BTRFS_MOUNT_CHECK_INTEGRITY_INCLUDING_EXTENT_DATA (1 << 21)\n#define BTRFS_MOUNT_PANIC_ON_FATAL_ERROR\t(1 << 22)\n\n#define btrfs_clear_opt(o, opt)\t\t((o) &= ~BTRFS_MOUNT_##opt)\n#define btrfs_set_opt(o, opt)\t\t((o) |= BTRFS_MOUNT_##opt)\n#define btrfs_test_opt(root, opt)\t((root)->fs_info->mount_opt & \\\n\t\t\t\t\t BTRFS_MOUNT_##opt)\n/*\n * Inode flags\n */\n#define BTRFS_INODE_NODATASUM\t\t(1 << 0)\n#define BTRFS_INODE_NODATACOW\t\t(1 << 1)\n#define BTRFS_INODE_READONLY\t\t(1 << 2)\n#define BTRFS_INODE_NOCOMPRESS\t\t(1 << 3)\n#define BTRFS_INODE_PREALLOC\t\t(1 << 4)\n#define BTRFS_INODE_SYNC\t\t(1 << 5)\n#define BTRFS_INODE_IMMUTABLE\t\t(1 << 6)\n#define BTRFS_INODE_APPEND\t\t(1 << 7)\n#define BTRFS_INODE_NODUMP\t\t(1 << 8)\n#define BTRFS_INODE_NOATIME\t\t(1 << 9)\n#define BTRFS_INODE_DIRSYNC\t\t(1 << 10)\n#define BTRFS_INODE_COMPRESS\t\t(1 << 11)\n\n#define BTRFS_INODE_ROOT_ITEM_INIT\t(1 << 31)\n\nstruct btrfs_map_token {\n\tstruct extent_buffer *eb;\n\tchar *kaddr;\n\tunsigned long offset;\n};\n\nstatic inline void btrfs_init_map_token (struct btrfs_map_token *token)\n{\n\ttoken->kaddr = NULL;\n}\n\n/* some macros to generate set/get funcs for the struct fields.  This\n * assumes there is a lefoo_to_cpu for every type, so lets make a simple\n * one for u8:\n */\n#define le8_to_cpu(v) (v)\n#define cpu_to_le8(v) (v)\n#define __le8 u8\n\n#define read_eb_member(eb, ptr, type, member, result) (\t\t\t\\\n\tread_extent_buffer(eb, (char *)(result),\t\t\t\\\n\t\t\t   ((unsigned long)(ptr)) +\t\t\t\\\n\t\t\t    offsetof(type, member),\t\t\t\\\n\t\t\t   sizeof(((type *)0)->member)))\n\n#define write_eb_member(eb, ptr, type, member, result) (\t\t\\\n\twrite_extent_buffer(eb, (char *)(result),\t\t\t\\\n\t\t\t   ((unsigned long)(ptr)) +\t\t\t\\\n\t\t\t    offsetof(type, member),\t\t\t\\\n\t\t\t   sizeof(((type *)0)->member)))\n\n#define DECLARE_BTRFS_SETGET_BITS(bits)\t\t\t\t\t\\\nu##bits btrfs_get_token_##bits(struct extent_buffer *eb, void *ptr,\t\\\n\t\t\t       unsigned long off,\t\t\t\\\n                              struct btrfs_map_token *token);\t\t\\\nvoid btrfs_set_token_##bits(struct extent_buffer *eb, void *ptr,\t\\\n\t\t\t    unsigned long off, u##bits val,\t\t\\\n\t\t\t    struct btrfs_map_token *token);\t\t\\\nstatic inline u##bits btrfs_get_##bits(struct extent_buffer *eb, void *ptr, \\\n\t\t\t\t       unsigned long off)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn btrfs_get_token_##bits(eb, ptr, off, NULL);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_##bits(struct extent_buffer *eb, void *ptr, \\\n\t\t\t\t    unsigned long off, u##bits val)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n       btrfs_set_token_##bits(eb, ptr, off, val, NULL);\t\t\t\\\n}\n\nDECLARE_BTRFS_SETGET_BITS(8)\nDECLARE_BTRFS_SETGET_BITS(16)\nDECLARE_BTRFS_SETGET_BITS(32)\nDECLARE_BTRFS_SETGET_BITS(64)\n\n#define BTRFS_SETGET_FUNCS(name, type, member, bits)\t\t\t\\\nstatic inline u##bits btrfs_##name(struct extent_buffer *eb, type *s)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);\t\\\n\treturn btrfs_get_##bits(eb, s, offsetof(type, member));\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_##name(struct extent_buffer *eb, type *s,\t\\\n\t\t\t\t    u##bits val)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);\t\\\n\tbtrfs_set_##bits(eb, s, offsetof(type, member), val);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline u##bits btrfs_token_##name(struct extent_buffer *eb, type *s, \\\n\t\t\t\t\t struct btrfs_map_token *token)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);\t\\\n\treturn btrfs_get_token_##bits(eb, s, offsetof(type, member), token); \\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_token_##name(struct extent_buffer *eb,\t\\\n\t\t\t\t\t  type *s, u##bits val,\t\t\\\n                                         struct btrfs_map_token *token)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);\t\\\n\tbtrfs_set_token_##bits(eb, s, offsetof(type, member), val, token); \\\n}\n\n#define BTRFS_SETGET_HEADER_FUNCS(name, type, member, bits)\t\t\\\nstatic inline u##bits btrfs_##name(struct extent_buffer *eb)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *p = page_address(eb->pages[0]);\t\t\t\t\\\n\tu##bits res = le##bits##_to_cpu(p->member);\t\t\t\\\n\treturn res;\t\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_##name(struct extent_buffer *eb,\t\t\\\n\t\t\t\t    u##bits val)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *p = page_address(eb->pages[0]);\t\t\t\t\\\n\tp->member = cpu_to_le##bits(val);\t\t\t\t\\\n}\n\n#define BTRFS_SETGET_STACK_FUNCS(name, type, member, bits)\t\t\\\nstatic inline u##bits btrfs_##name(type *s)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn le##bits##_to_cpu(s->member);\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_##name(type *s, u##bits val)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ts->member = cpu_to_le##bits(val);\t\t\t\t\\\n}\n\nBTRFS_SETGET_FUNCS(device_type, struct btrfs_dev_item, type, 64);\nBTRFS_SETGET_FUNCS(device_total_bytes, struct btrfs_dev_item, total_bytes, 64);\nBTRFS_SETGET_FUNCS(device_bytes_used, struct btrfs_dev_item, bytes_used, 64);\nBTRFS_SETGET_FUNCS(device_io_align, struct btrfs_dev_item, io_align, 32);\nBTRFS_SETGET_FUNCS(device_io_width, struct btrfs_dev_item, io_width, 32);\nBTRFS_SETGET_FUNCS(device_start_offset, struct btrfs_dev_item,\n\t\t   start_offset, 64);\nBTRFS_SETGET_FUNCS(device_sector_size, struct btrfs_dev_item, sector_size, 32);\nBTRFS_SETGET_FUNCS(device_id, struct btrfs_dev_item, devid, 64);\nBTRFS_SETGET_FUNCS(device_group, struct btrfs_dev_item, dev_group, 32);\nBTRFS_SETGET_FUNCS(device_seek_speed, struct btrfs_dev_item, seek_speed, 8);\nBTRFS_SETGET_FUNCS(device_bandwidth, struct btrfs_dev_item, bandwidth, 8);\nBTRFS_SETGET_FUNCS(device_generation, struct btrfs_dev_item, generation, 64);\n\nBTRFS_SETGET_STACK_FUNCS(stack_device_type, struct btrfs_dev_item, type, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_device_total_bytes, struct btrfs_dev_item,\n\t\t\t total_bytes, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_device_bytes_used, struct btrfs_dev_item,\n\t\t\t bytes_used, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_device_io_align, struct btrfs_dev_item,\n\t\t\t io_align, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_device_io_width, struct btrfs_dev_item,\n\t\t\t io_width, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_device_sector_size, struct btrfs_dev_item,\n\t\t\t sector_size, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_device_id, struct btrfs_dev_item, devid, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_device_group, struct btrfs_dev_item,\n\t\t\t dev_group, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_device_seek_speed, struct btrfs_dev_item,\n\t\t\t seek_speed, 8);\nBTRFS_SETGET_STACK_FUNCS(stack_device_bandwidth, struct btrfs_dev_item,\n\t\t\t bandwidth, 8);\nBTRFS_SETGET_STACK_FUNCS(stack_device_generation, struct btrfs_dev_item,\n\t\t\t generation, 64);\n\nstatic inline char *btrfs_device_uuid(struct btrfs_dev_item *d)\n{\n\treturn (char *)d + offsetof(struct btrfs_dev_item, uuid);\n}\n\nstatic inline char *btrfs_device_fsid(struct btrfs_dev_item *d)\n{\n\treturn (char *)d + offsetof(struct btrfs_dev_item, fsid);\n}\n\nBTRFS_SETGET_FUNCS(chunk_length, struct btrfs_chunk, length, 64);\nBTRFS_SETGET_FUNCS(chunk_owner, struct btrfs_chunk, owner, 64);\nBTRFS_SETGET_FUNCS(chunk_stripe_len, struct btrfs_chunk, stripe_len, 64);\nBTRFS_SETGET_FUNCS(chunk_io_align, struct btrfs_chunk, io_align, 32);\nBTRFS_SETGET_FUNCS(chunk_io_width, struct btrfs_chunk, io_width, 32);\nBTRFS_SETGET_FUNCS(chunk_sector_size, struct btrfs_chunk, sector_size, 32);\nBTRFS_SETGET_FUNCS(chunk_type, struct btrfs_chunk, type, 64);\nBTRFS_SETGET_FUNCS(chunk_num_stripes, struct btrfs_chunk, num_stripes, 16);\nBTRFS_SETGET_FUNCS(chunk_sub_stripes, struct btrfs_chunk, sub_stripes, 16);\nBTRFS_SETGET_FUNCS(stripe_devid, struct btrfs_stripe, devid, 64);\nBTRFS_SETGET_FUNCS(stripe_offset, struct btrfs_stripe, offset, 64);\n\nstatic inline char *btrfs_stripe_dev_uuid(struct btrfs_stripe *s)\n{\n\treturn (char *)s + offsetof(struct btrfs_stripe, dev_uuid);\n}\n\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_length, struct btrfs_chunk, length, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_owner, struct btrfs_chunk, owner, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_stripe_len, struct btrfs_chunk,\n\t\t\t stripe_len, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_io_align, struct btrfs_chunk,\n\t\t\t io_align, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_io_width, struct btrfs_chunk,\n\t\t\t io_width, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_sector_size, struct btrfs_chunk,\n\t\t\t sector_size, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_type, struct btrfs_chunk, type, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_num_stripes, struct btrfs_chunk,\n\t\t\t num_stripes, 16);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_sub_stripes, struct btrfs_chunk,\n\t\t\t sub_stripes, 16);\nBTRFS_SETGET_STACK_FUNCS(stack_stripe_devid, struct btrfs_stripe, devid, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_stripe_offset, struct btrfs_stripe, offset, 64);\n\nstatic inline struct btrfs_stripe *btrfs_stripe_nr(struct btrfs_chunk *c,\n\t\t\t\t\t\t   int nr)\n{\n\tunsigned long offset = (unsigned long)c;\n\toffset += offsetof(struct btrfs_chunk, stripe);\n\toffset += nr * sizeof(struct btrfs_stripe);\n\treturn (struct btrfs_stripe *)offset;\n}\n\nstatic inline char *btrfs_stripe_dev_uuid_nr(struct btrfs_chunk *c, int nr)\n{\n\treturn btrfs_stripe_dev_uuid(btrfs_stripe_nr(c, nr));\n}\n\nstatic inline u64 btrfs_stripe_offset_nr(struct extent_buffer *eb,\n\t\t\t\t\t struct btrfs_chunk *c, int nr)\n{\n\treturn btrfs_stripe_offset(eb, btrfs_stripe_nr(c, nr));\n}\n\nstatic inline u64 btrfs_stripe_devid_nr(struct extent_buffer *eb,\n\t\t\t\t\t struct btrfs_chunk *c, int nr)\n{\n\treturn btrfs_stripe_devid(eb, btrfs_stripe_nr(c, nr));\n}\n\n/* struct btrfs_block_group_item */\nBTRFS_SETGET_STACK_FUNCS(block_group_used, struct btrfs_block_group_item,\n\t\t\t used, 64);\nBTRFS_SETGET_FUNCS(disk_block_group_used, struct btrfs_block_group_item,\n\t\t\t used, 64);\nBTRFS_SETGET_STACK_FUNCS(block_group_chunk_objectid,\n\t\t\tstruct btrfs_block_group_item, chunk_objectid, 64);\n\nBTRFS_SETGET_FUNCS(disk_block_group_chunk_objectid,\n\t\t   struct btrfs_block_group_item, chunk_objectid, 64);\nBTRFS_SETGET_FUNCS(disk_block_group_flags,\n\t\t   struct btrfs_block_group_item, flags, 64);\nBTRFS_SETGET_STACK_FUNCS(block_group_flags,\n\t\t\tstruct btrfs_block_group_item, flags, 64);\n\n/* struct btrfs_inode_ref */\nBTRFS_SETGET_FUNCS(inode_ref_name_len, struct btrfs_inode_ref, name_len, 16);\nBTRFS_SETGET_FUNCS(inode_ref_index, struct btrfs_inode_ref, index, 64);\n\n/* struct btrfs_inode_extref */\nBTRFS_SETGET_FUNCS(inode_extref_parent, struct btrfs_inode_extref,\n\t\t   parent_objectid, 64);\nBTRFS_SETGET_FUNCS(inode_extref_name_len, struct btrfs_inode_extref,\n\t\t   name_len, 16);\nBTRFS_SETGET_FUNCS(inode_extref_index, struct btrfs_inode_extref, index, 64);\n\n/* struct btrfs_inode_item */\nBTRFS_SETGET_FUNCS(inode_generation, struct btrfs_inode_item, generation, 64);\nBTRFS_SETGET_FUNCS(inode_sequence, struct btrfs_inode_item, sequence, 64);\nBTRFS_SETGET_FUNCS(inode_transid, struct btrfs_inode_item, transid, 64);\nBTRFS_SETGET_FUNCS(inode_size, struct btrfs_inode_item, size, 64);\nBTRFS_SETGET_FUNCS(inode_nbytes, struct btrfs_inode_item, nbytes, 64);\nBTRFS_SETGET_FUNCS(inode_block_group, struct btrfs_inode_item, block_group, 64);\nBTRFS_SETGET_FUNCS(inode_nlink, struct btrfs_inode_item, nlink, 32);\nBTRFS_SETGET_FUNCS(inode_uid, struct btrfs_inode_item, uid, 32);\nBTRFS_SETGET_FUNCS(inode_gid, struct btrfs_inode_item, gid, 32);\nBTRFS_SETGET_FUNCS(inode_mode, struct btrfs_inode_item, mode, 32);\nBTRFS_SETGET_FUNCS(inode_rdev, struct btrfs_inode_item, rdev, 64);\nBTRFS_SETGET_FUNCS(inode_flags, struct btrfs_inode_item, flags, 64);\n\nstatic inline struct btrfs_timespec *\nbtrfs_inode_atime(struct btrfs_inode_item *inode_item)\n{\n\tunsigned long ptr = (unsigned long)inode_item;\n\tptr += offsetof(struct btrfs_inode_item, atime);\n\treturn (struct btrfs_timespec *)ptr;\n}\n\nstatic inline struct btrfs_timespec *\nbtrfs_inode_mtime(struct btrfs_inode_item *inode_item)\n{\n\tunsigned long ptr = (unsigned long)inode_item;\n\tptr += offsetof(struct btrfs_inode_item, mtime);\n\treturn (struct btrfs_timespec *)ptr;\n}\n\nstatic inline struct btrfs_timespec *\nbtrfs_inode_ctime(struct btrfs_inode_item *inode_item)\n{\n\tunsigned long ptr = (unsigned long)inode_item;\n\tptr += offsetof(struct btrfs_inode_item, ctime);\n\treturn (struct btrfs_timespec *)ptr;\n}\n\nBTRFS_SETGET_FUNCS(timespec_sec, struct btrfs_timespec, sec, 64);\nBTRFS_SETGET_FUNCS(timespec_nsec, struct btrfs_timespec, nsec, 32);\n\n/* struct btrfs_dev_extent */\nBTRFS_SETGET_FUNCS(dev_extent_chunk_tree, struct btrfs_dev_extent,\n\t\t   chunk_tree, 64);\nBTRFS_SETGET_FUNCS(dev_extent_chunk_objectid, struct btrfs_dev_extent,\n\t\t   chunk_objectid, 64);\nBTRFS_SETGET_FUNCS(dev_extent_chunk_offset, struct btrfs_dev_extent,\n\t\t   chunk_offset, 64);\nBTRFS_SETGET_FUNCS(dev_extent_length, struct btrfs_dev_extent, length, 64);\n\nstatic inline u8 *btrfs_dev_extent_chunk_tree_uuid(struct btrfs_dev_extent *dev)\n{\n\tunsigned long ptr = offsetof(struct btrfs_dev_extent, chunk_tree_uuid);\n\treturn (u8 *)((unsigned long)dev + ptr);\n}\n\nBTRFS_SETGET_FUNCS(extent_refs, struct btrfs_extent_item, refs, 64);\nBTRFS_SETGET_FUNCS(extent_generation, struct btrfs_extent_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(extent_flags, struct btrfs_extent_item, flags, 64);\n\nBTRFS_SETGET_FUNCS(extent_refs_v0, struct btrfs_extent_item_v0, refs, 32);\n\n\nBTRFS_SETGET_FUNCS(tree_block_level, struct btrfs_tree_block_info, level, 8);\n\nstatic inline void btrfs_tree_block_key(struct extent_buffer *eb,\n\t\t\t\t\tstruct btrfs_tree_block_info *item,\n\t\t\t\t\tstruct btrfs_disk_key *key)\n{\n\tread_eb_member(eb, item, struct btrfs_tree_block_info, key, key);\n}\n\nstatic inline void btrfs_set_tree_block_key(struct extent_buffer *eb,\n\t\t\t\t\t    struct btrfs_tree_block_info *item,\n\t\t\t\t\t    struct btrfs_disk_key *key)\n{\n\twrite_eb_member(eb, item, struct btrfs_tree_block_info, key, key);\n}\n\nBTRFS_SETGET_FUNCS(extent_data_ref_root, struct btrfs_extent_data_ref,\n\t\t   root, 64);\nBTRFS_SETGET_FUNCS(extent_data_ref_objectid, struct btrfs_extent_data_ref,\n\t\t   objectid, 64);\nBTRFS_SETGET_FUNCS(extent_data_ref_offset, struct btrfs_extent_data_ref,\n\t\t   offset, 64);\nBTRFS_SETGET_FUNCS(extent_data_ref_count, struct btrfs_extent_data_ref,\n\t\t   count, 32);\n\nBTRFS_SETGET_FUNCS(shared_data_ref_count, struct btrfs_shared_data_ref,\n\t\t   count, 32);\n\nBTRFS_SETGET_FUNCS(extent_inline_ref_type, struct btrfs_extent_inline_ref,\n\t\t   type, 8);\nBTRFS_SETGET_FUNCS(extent_inline_ref_offset, struct btrfs_extent_inline_ref,\n\t\t   offset, 64);\n\nstatic inline u32 btrfs_extent_inline_ref_size(int type)\n{\n\tif (type == BTRFS_TREE_BLOCK_REF_KEY ||\n\t    type == BTRFS_SHARED_BLOCK_REF_KEY)\n\t\treturn sizeof(struct btrfs_extent_inline_ref);\n\tif (type == BTRFS_SHARED_DATA_REF_KEY)\n\t\treturn sizeof(struct btrfs_shared_data_ref) +\n\t\t       sizeof(struct btrfs_extent_inline_ref);\n\tif (type == BTRFS_EXTENT_DATA_REF_KEY)\n\t\treturn sizeof(struct btrfs_extent_data_ref) +\n\t\t       offsetof(struct btrfs_extent_inline_ref, offset);\n\tBUG();\n\treturn 0;\n}\n\nBTRFS_SETGET_FUNCS(ref_root_v0, struct btrfs_extent_ref_v0, root, 64);\nBTRFS_SETGET_FUNCS(ref_generation_v0, struct btrfs_extent_ref_v0,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(ref_objectid_v0, struct btrfs_extent_ref_v0, objectid, 64);\nBTRFS_SETGET_FUNCS(ref_count_v0, struct btrfs_extent_ref_v0, count, 32);\n\n/* struct btrfs_node */\nBTRFS_SETGET_FUNCS(key_blockptr, struct btrfs_key_ptr, blockptr, 64);\nBTRFS_SETGET_FUNCS(key_generation, struct btrfs_key_ptr, generation, 64);\n\nstatic inline u64 btrfs_node_blockptr(struct extent_buffer *eb, int nr)\n{\n\tunsigned long ptr;\n\tptr = offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n\treturn btrfs_key_blockptr(eb, (struct btrfs_key_ptr *)ptr);\n}\n\nstatic inline void btrfs_set_node_blockptr(struct extent_buffer *eb,\n\t\t\t\t\t   int nr, u64 val)\n{\n\tunsigned long ptr;\n\tptr = offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n\tbtrfs_set_key_blockptr(eb, (struct btrfs_key_ptr *)ptr, val);\n}\n\nstatic inline u64 btrfs_node_ptr_generation(struct extent_buffer *eb, int nr)\n{\n\tunsigned long ptr;\n\tptr = offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n\treturn btrfs_key_generation(eb, (struct btrfs_key_ptr *)ptr);\n}\n\nstatic inline void btrfs_set_node_ptr_generation(struct extent_buffer *eb,\n\t\t\t\t\t\t int nr, u64 val)\n{\n\tunsigned long ptr;\n\tptr = offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n\tbtrfs_set_key_generation(eb, (struct btrfs_key_ptr *)ptr, val);\n}\n\nstatic inline unsigned long btrfs_node_key_ptr_offset(int nr)\n{\n\treturn offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n}\n\nvoid btrfs_node_key(struct extent_buffer *eb,\n\t\t    struct btrfs_disk_key *disk_key, int nr);\n\nstatic inline void btrfs_set_node_key(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_disk_key *disk_key, int nr)\n{\n\tunsigned long ptr;\n\tptr = btrfs_node_key_ptr_offset(nr);\n\twrite_eb_member(eb, (struct btrfs_key_ptr *)ptr,\n\t\t       struct btrfs_key_ptr, key, disk_key);\n}\n\n/* struct btrfs_item */\nBTRFS_SETGET_FUNCS(item_offset, struct btrfs_item, offset, 32);\nBTRFS_SETGET_FUNCS(item_size, struct btrfs_item, size, 32);\n\nstatic inline unsigned long btrfs_item_nr_offset(int nr)\n{\n\treturn offsetof(struct btrfs_leaf, items) +\n\t\tsizeof(struct btrfs_item) * nr;\n}\n\nstatic inline struct btrfs_item *btrfs_item_nr(struct extent_buffer *eb,\n\t\t\t\t\t       int nr)\n{\n\treturn (struct btrfs_item *)btrfs_item_nr_offset(nr);\n}\n\nstatic inline u32 btrfs_item_end(struct extent_buffer *eb,\n\t\t\t\t struct btrfs_item *item)\n{\n\treturn btrfs_item_offset(eb, item) + btrfs_item_size(eb, item);\n}\n\nstatic inline u32 btrfs_item_end_nr(struct extent_buffer *eb, int nr)\n{\n\treturn btrfs_item_end(eb, btrfs_item_nr(eb, nr));\n}\n\nstatic inline u32 btrfs_item_offset_nr(struct extent_buffer *eb, int nr)\n{\n\treturn btrfs_item_offset(eb, btrfs_item_nr(eb, nr));\n}\n\nstatic inline u32 btrfs_item_size_nr(struct extent_buffer *eb, int nr)\n{\n\treturn btrfs_item_size(eb, btrfs_item_nr(eb, nr));\n}\n\nstatic inline void btrfs_item_key(struct extent_buffer *eb,\n\t\t\t   struct btrfs_disk_key *disk_key, int nr)\n{\n\tstruct btrfs_item *item = btrfs_item_nr(eb, nr);\n\tread_eb_member(eb, item, struct btrfs_item, key, disk_key);\n}\n\nstatic inline void btrfs_set_item_key(struct extent_buffer *eb,\n\t\t\t       struct btrfs_disk_key *disk_key, int nr)\n{\n\tstruct btrfs_item *item = btrfs_item_nr(eb, nr);\n\twrite_eb_member(eb, item, struct btrfs_item, key, disk_key);\n}\n\nBTRFS_SETGET_FUNCS(dir_log_end, struct btrfs_dir_log_item, end, 64);\n\n/*\n * struct btrfs_root_ref\n */\nBTRFS_SETGET_FUNCS(root_ref_dirid, struct btrfs_root_ref, dirid, 64);\nBTRFS_SETGET_FUNCS(root_ref_sequence, struct btrfs_root_ref, sequence, 64);\nBTRFS_SETGET_FUNCS(root_ref_name_len, struct btrfs_root_ref, name_len, 16);\n\n/* struct btrfs_dir_item */\nBTRFS_SETGET_FUNCS(dir_data_len, struct btrfs_dir_item, data_len, 16);\nBTRFS_SETGET_FUNCS(dir_type, struct btrfs_dir_item, type, 8);\nBTRFS_SETGET_FUNCS(dir_name_len, struct btrfs_dir_item, name_len, 16);\nBTRFS_SETGET_FUNCS(dir_transid, struct btrfs_dir_item, transid, 64);\n\nstatic inline void btrfs_dir_item_key(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_dir_item *item,\n\t\t\t\t      struct btrfs_disk_key *key)\n{\n\tread_eb_member(eb, item, struct btrfs_dir_item, location, key);\n}\n\nstatic inline void btrfs_set_dir_item_key(struct extent_buffer *eb,\n\t\t\t\t\t  struct btrfs_dir_item *item,\n\t\t\t\t\t  struct btrfs_disk_key *key)\n{\n\twrite_eb_member(eb, item, struct btrfs_dir_item, location, key);\n}\n\nBTRFS_SETGET_FUNCS(free_space_entries, struct btrfs_free_space_header,\n\t\t   num_entries, 64);\nBTRFS_SETGET_FUNCS(free_space_bitmaps, struct btrfs_free_space_header,\n\t\t   num_bitmaps, 64);\nBTRFS_SETGET_FUNCS(free_space_generation, struct btrfs_free_space_header,\n\t\t   generation, 64);\n\nstatic inline void btrfs_free_space_key(struct extent_buffer *eb,\n\t\t\t\t\tstruct btrfs_free_space_header *h,\n\t\t\t\t\tstruct btrfs_disk_key *key)\n{\n\tread_eb_member(eb, h, struct btrfs_free_space_header, location, key);\n}\n\nstatic inline void btrfs_set_free_space_key(struct extent_buffer *eb,\n\t\t\t\t\t    struct btrfs_free_space_header *h,\n\t\t\t\t\t    struct btrfs_disk_key *key)\n{\n\twrite_eb_member(eb, h, struct btrfs_free_space_header, location, key);\n}\n\n/* struct btrfs_disk_key */\nBTRFS_SETGET_STACK_FUNCS(disk_key_objectid, struct btrfs_disk_key,\n\t\t\t objectid, 64);\nBTRFS_SETGET_STACK_FUNCS(disk_key_offset, struct btrfs_disk_key, offset, 64);\nBTRFS_SETGET_STACK_FUNCS(disk_key_type, struct btrfs_disk_key, type, 8);\n\nstatic inline void btrfs_disk_key_to_cpu(struct btrfs_key *cpu,\n\t\t\t\t\t struct btrfs_disk_key *disk)\n{\n\tcpu->offset = le64_to_cpu(disk->offset);\n\tcpu->type = disk->type;\n\tcpu->objectid = le64_to_cpu(disk->objectid);\n}\n\nstatic inline void btrfs_cpu_key_to_disk(struct btrfs_disk_key *disk,\n\t\t\t\t\t struct btrfs_key *cpu)\n{\n\tdisk->offset = cpu_to_le64(cpu->offset);\n\tdisk->type = cpu->type;\n\tdisk->objectid = cpu_to_le64(cpu->objectid);\n}\n\nstatic inline void btrfs_node_key_to_cpu(struct extent_buffer *eb,\n\t\t\t\t  struct btrfs_key *key, int nr)\n{\n\tstruct btrfs_disk_key disk_key;\n\tbtrfs_node_key(eb, &disk_key, nr);\n\tbtrfs_disk_key_to_cpu(key, &disk_key);\n}\n\nstatic inline void btrfs_item_key_to_cpu(struct extent_buffer *eb,\n\t\t\t\t  struct btrfs_key *key, int nr)\n{\n\tstruct btrfs_disk_key disk_key;\n\tbtrfs_item_key(eb, &disk_key, nr);\n\tbtrfs_disk_key_to_cpu(key, &disk_key);\n}\n\nstatic inline void btrfs_dir_item_key_to_cpu(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_dir_item *item,\n\t\t\t\t      struct btrfs_key *key)\n{\n\tstruct btrfs_disk_key disk_key;\n\tbtrfs_dir_item_key(eb, item, &disk_key);\n\tbtrfs_disk_key_to_cpu(key, &disk_key);\n}\n\n\nstatic inline u8 btrfs_key_type(struct btrfs_key *key)\n{\n\treturn key->type;\n}\n\nstatic inline void btrfs_set_key_type(struct btrfs_key *key, u8 val)\n{\n\tkey->type = val;\n}\n\n/* struct btrfs_header */\nBTRFS_SETGET_HEADER_FUNCS(header_bytenr, struct btrfs_header, bytenr, 64);\nBTRFS_SETGET_HEADER_FUNCS(header_generation, struct btrfs_header,\n\t\t\t  generation, 64);\nBTRFS_SETGET_HEADER_FUNCS(header_owner, struct btrfs_header, owner, 64);\nBTRFS_SETGET_HEADER_FUNCS(header_nritems, struct btrfs_header, nritems, 32);\nBTRFS_SETGET_HEADER_FUNCS(header_flags, struct btrfs_header, flags, 64);\nBTRFS_SETGET_HEADER_FUNCS(header_level, struct btrfs_header, level, 8);\n\nstatic inline int btrfs_header_flag(struct extent_buffer *eb, u64 flag)\n{\n\treturn (btrfs_header_flags(eb) & flag) == flag;\n}\n\nstatic inline int btrfs_set_header_flag(struct extent_buffer *eb, u64 flag)\n{\n\tu64 flags = btrfs_header_flags(eb);\n\tbtrfs_set_header_flags(eb, flags | flag);\n\treturn (flags & flag) == flag;\n}\n\nstatic inline int btrfs_clear_header_flag(struct extent_buffer *eb, u64 flag)\n{\n\tu64 flags = btrfs_header_flags(eb);\n\tbtrfs_set_header_flags(eb, flags & ~flag);\n\treturn (flags & flag) == flag;\n}\n\nstatic inline int btrfs_header_backref_rev(struct extent_buffer *eb)\n{\n\tu64 flags = btrfs_header_flags(eb);\n\treturn flags >> BTRFS_BACKREF_REV_SHIFT;\n}\n\nstatic inline void btrfs_set_header_backref_rev(struct extent_buffer *eb,\n\t\t\t\t\t\tint rev)\n{\n\tu64 flags = btrfs_header_flags(eb);\n\tflags &= ~BTRFS_BACKREF_REV_MASK;\n\tflags |= (u64)rev << BTRFS_BACKREF_REV_SHIFT;\n\tbtrfs_set_header_flags(eb, flags);\n}\n\nstatic inline u8 *btrfs_header_fsid(struct extent_buffer *eb)\n{\n\tunsigned long ptr = offsetof(struct btrfs_header, fsid);\n\treturn (u8 *)ptr;\n}\n\nstatic inline u8 *btrfs_header_chunk_tree_uuid(struct extent_buffer *eb)\n{\n\tunsigned long ptr = offsetof(struct btrfs_header, chunk_tree_uuid);\n\treturn (u8 *)ptr;\n}\n\nstatic inline int btrfs_is_leaf(struct extent_buffer *eb)\n{\n\treturn btrfs_header_level(eb) == 0;\n}\n\n/* struct btrfs_root_item */\nBTRFS_SETGET_FUNCS(disk_root_generation, struct btrfs_root_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(disk_root_refs, struct btrfs_root_item, refs, 32);\nBTRFS_SETGET_FUNCS(disk_root_bytenr, struct btrfs_root_item, bytenr, 64);\nBTRFS_SETGET_FUNCS(disk_root_level, struct btrfs_root_item, level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(root_generation, struct btrfs_root_item,\n\t\t\t generation, 64);\nBTRFS_SETGET_STACK_FUNCS(root_bytenr, struct btrfs_root_item, bytenr, 64);\nBTRFS_SETGET_STACK_FUNCS(root_level, struct btrfs_root_item, level, 8);\nBTRFS_SETGET_STACK_FUNCS(root_dirid, struct btrfs_root_item, root_dirid, 64);\nBTRFS_SETGET_STACK_FUNCS(root_refs, struct btrfs_root_item, refs, 32);\nBTRFS_SETGET_STACK_FUNCS(root_flags, struct btrfs_root_item, flags, 64);\nBTRFS_SETGET_STACK_FUNCS(root_used, struct btrfs_root_item, bytes_used, 64);\nBTRFS_SETGET_STACK_FUNCS(root_limit, struct btrfs_root_item, byte_limit, 64);\nBTRFS_SETGET_STACK_FUNCS(root_last_snapshot, struct btrfs_root_item,\n\t\t\t last_snapshot, 64);\nBTRFS_SETGET_STACK_FUNCS(root_generation_v2, struct btrfs_root_item,\n\t\t\t generation_v2, 64);\nBTRFS_SETGET_STACK_FUNCS(root_ctransid, struct btrfs_root_item,\n\t\t\t ctransid, 64);\nBTRFS_SETGET_STACK_FUNCS(root_otransid, struct btrfs_root_item,\n\t\t\t otransid, 64);\nBTRFS_SETGET_STACK_FUNCS(root_stransid, struct btrfs_root_item,\n\t\t\t stransid, 64);\nBTRFS_SETGET_STACK_FUNCS(root_rtransid, struct btrfs_root_item,\n\t\t\t rtransid, 64);\n\nstatic inline bool btrfs_root_readonly(struct btrfs_root *root)\n{\n\treturn (root->root_item.flags & cpu_to_le64(BTRFS_ROOT_SUBVOL_RDONLY)) != 0;\n}\n\n/* struct btrfs_root_backup */\nBTRFS_SETGET_STACK_FUNCS(backup_tree_root, struct btrfs_root_backup,\n\t\t   tree_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_tree_root_gen, struct btrfs_root_backup,\n\t\t   tree_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_tree_root_level, struct btrfs_root_backup,\n\t\t   tree_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_chunk_root, struct btrfs_root_backup,\n\t\t   chunk_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_chunk_root_gen, struct btrfs_root_backup,\n\t\t   chunk_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_chunk_root_level, struct btrfs_root_backup,\n\t\t   chunk_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_extent_root, struct btrfs_root_backup,\n\t\t   extent_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_extent_root_gen, struct btrfs_root_backup,\n\t\t   extent_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_extent_root_level, struct btrfs_root_backup,\n\t\t   extent_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_fs_root, struct btrfs_root_backup,\n\t\t   fs_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_fs_root_gen, struct btrfs_root_backup,\n\t\t   fs_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_fs_root_level, struct btrfs_root_backup,\n\t\t   fs_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_dev_root, struct btrfs_root_backup,\n\t\t   dev_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_dev_root_gen, struct btrfs_root_backup,\n\t\t   dev_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_dev_root_level, struct btrfs_root_backup,\n\t\t   dev_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_csum_root, struct btrfs_root_backup,\n\t\t   csum_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_csum_root_gen, struct btrfs_root_backup,\n\t\t   csum_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_csum_root_level, struct btrfs_root_backup,\n\t\t   csum_root_level, 8);\nBTRFS_SETGET_STACK_FUNCS(backup_total_bytes, struct btrfs_root_backup,\n\t\t   total_bytes, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_bytes_used, struct btrfs_root_backup,\n\t\t   bytes_used, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_num_devices, struct btrfs_root_backup,\n\t\t   num_devices, 64);\n\n/* struct btrfs_balance_item */\nBTRFS_SETGET_FUNCS(balance_flags, struct btrfs_balance_item, flags, 64);\n\nstatic inline void btrfs_balance_data(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_balance_item *bi,\n\t\t\t\t      struct btrfs_disk_balance_args *ba)\n{\n\tread_eb_member(eb, bi, struct btrfs_balance_item, data, ba);\n}\n\nstatic inline void btrfs_set_balance_data(struct extent_buffer *eb,\n\t\t\t\t\t  struct btrfs_balance_item *bi,\n\t\t\t\t\t  struct btrfs_disk_balance_args *ba)\n{\n\twrite_eb_member(eb, bi, struct btrfs_balance_item, data, ba);\n}\n\nstatic inline void btrfs_balance_meta(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_balance_item *bi,\n\t\t\t\t      struct btrfs_disk_balance_args *ba)\n{\n\tread_eb_member(eb, bi, struct btrfs_balance_item, meta, ba);\n}\n\nstatic inline void btrfs_set_balance_meta(struct extent_buffer *eb,\n\t\t\t\t\t  struct btrfs_balance_item *bi,\n\t\t\t\t\t  struct btrfs_disk_balance_args *ba)\n{\n\twrite_eb_member(eb, bi, struct btrfs_balance_item, meta, ba);\n}\n\nstatic inline void btrfs_balance_sys(struct extent_buffer *eb,\n\t\t\t\t     struct btrfs_balance_item *bi,\n\t\t\t\t     struct btrfs_disk_balance_args *ba)\n{\n\tread_eb_member(eb, bi, struct btrfs_balance_item, sys, ba);\n}\n\nstatic inline void btrfs_set_balance_sys(struct extent_buffer *eb,\n\t\t\t\t\t struct btrfs_balance_item *bi,\n\t\t\t\t\t struct btrfs_disk_balance_args *ba)\n{\n\twrite_eb_member(eb, bi, struct btrfs_balance_item, sys, ba);\n}\n\nstatic inline void\nbtrfs_disk_balance_args_to_cpu(struct btrfs_balance_args *cpu,\n\t\t\t       struct btrfs_disk_balance_args *disk)\n{\n\tmemset(cpu, 0, sizeof(*cpu));\n\n\tcpu->profiles = le64_to_cpu(disk->profiles);\n\tcpu->usage = le64_to_cpu(disk->usage);\n\tcpu->devid = le64_to_cpu(disk->devid);\n\tcpu->pstart = le64_to_cpu(disk->pstart);\n\tcpu->pend = le64_to_cpu(disk->pend);\n\tcpu->vstart = le64_to_cpu(disk->vstart);\n\tcpu->vend = le64_to_cpu(disk->vend);\n\tcpu->target = le64_to_cpu(disk->target);\n\tcpu->flags = le64_to_cpu(disk->flags);\n}\n\nstatic inline void\nbtrfs_cpu_balance_args_to_disk(struct btrfs_disk_balance_args *disk,\n\t\t\t       struct btrfs_balance_args *cpu)\n{\n\tmemset(disk, 0, sizeof(*disk));\n\n\tdisk->profiles = cpu_to_le64(cpu->profiles);\n\tdisk->usage = cpu_to_le64(cpu->usage);\n\tdisk->devid = cpu_to_le64(cpu->devid);\n\tdisk->pstart = cpu_to_le64(cpu->pstart);\n\tdisk->pend = cpu_to_le64(cpu->pend);\n\tdisk->vstart = cpu_to_le64(cpu->vstart);\n\tdisk->vend = cpu_to_le64(cpu->vend);\n\tdisk->target = cpu_to_le64(cpu->target);\n\tdisk->flags = cpu_to_le64(cpu->flags);\n}\n\n/* struct btrfs_super_block */\nBTRFS_SETGET_STACK_FUNCS(super_bytenr, struct btrfs_super_block, bytenr, 64);\nBTRFS_SETGET_STACK_FUNCS(super_flags, struct btrfs_super_block, flags, 64);\nBTRFS_SETGET_STACK_FUNCS(super_generation, struct btrfs_super_block,\n\t\t\t generation, 64);\nBTRFS_SETGET_STACK_FUNCS(super_root, struct btrfs_super_block, root, 64);\nBTRFS_SETGET_STACK_FUNCS(super_sys_array_size,\n\t\t\t struct btrfs_super_block, sys_chunk_array_size, 32);\nBTRFS_SETGET_STACK_FUNCS(super_chunk_root_generation,\n\t\t\t struct btrfs_super_block, chunk_root_generation, 64);\nBTRFS_SETGET_STACK_FUNCS(super_root_level, struct btrfs_super_block,\n\t\t\t root_level, 8);\nBTRFS_SETGET_STACK_FUNCS(super_chunk_root, struct btrfs_super_block,\n\t\t\t chunk_root, 64);\nBTRFS_SETGET_STACK_FUNCS(super_chunk_root_level, struct btrfs_super_block,\n\t\t\t chunk_root_level, 8);\nBTRFS_SETGET_STACK_FUNCS(super_log_root, struct btrfs_super_block,\n\t\t\t log_root, 64);\nBTRFS_SETGET_STACK_FUNCS(super_log_root_transid, struct btrfs_super_block,\n\t\t\t log_root_transid, 64);\nBTRFS_SETGET_STACK_FUNCS(super_log_root_level, struct btrfs_super_block,\n\t\t\t log_root_level, 8);\nBTRFS_SETGET_STACK_FUNCS(super_total_bytes, struct btrfs_super_block,\n\t\t\t total_bytes, 64);\nBTRFS_SETGET_STACK_FUNCS(super_bytes_used, struct btrfs_super_block,\n\t\t\t bytes_used, 64);\nBTRFS_SETGET_STACK_FUNCS(super_sectorsize, struct btrfs_super_block,\n\t\t\t sectorsize, 32);\nBTRFS_SETGET_STACK_FUNCS(super_nodesize, struct btrfs_super_block,\n\t\t\t nodesize, 32);\nBTRFS_SETGET_STACK_FUNCS(super_leafsize, struct btrfs_super_block,\n\t\t\t leafsize, 32);\nBTRFS_SETGET_STACK_FUNCS(super_stripesize, struct btrfs_super_block,\n\t\t\t stripesize, 32);\nBTRFS_SETGET_STACK_FUNCS(super_root_dir, struct btrfs_super_block,\n\t\t\t root_dir_objectid, 64);\nBTRFS_SETGET_STACK_FUNCS(super_num_devices, struct btrfs_super_block,\n\t\t\t num_devices, 64);\nBTRFS_SETGET_STACK_FUNCS(super_compat_flags, struct btrfs_super_block,\n\t\t\t compat_flags, 64);\nBTRFS_SETGET_STACK_FUNCS(super_compat_ro_flags, struct btrfs_super_block,\n\t\t\t compat_ro_flags, 64);\nBTRFS_SETGET_STACK_FUNCS(super_incompat_flags, struct btrfs_super_block,\n\t\t\t incompat_flags, 64);\nBTRFS_SETGET_STACK_FUNCS(super_csum_type, struct btrfs_super_block,\n\t\t\t csum_type, 16);\nBTRFS_SETGET_STACK_FUNCS(super_cache_generation, struct btrfs_super_block,\n\t\t\t cache_generation, 64);\n\nstatic inline int btrfs_super_csum_size(struct btrfs_super_block *s)\n{\n\tint t = btrfs_super_csum_type(s);\n\tBUG_ON(t >= ARRAY_SIZE(btrfs_csum_sizes));\n\treturn btrfs_csum_sizes[t];\n}\n\nstatic inline unsigned long btrfs_leaf_data(struct extent_buffer *l)\n{\n\treturn offsetof(struct btrfs_leaf, items);\n}\n\n/* struct btrfs_file_extent_item */\nBTRFS_SETGET_FUNCS(file_extent_type, struct btrfs_file_extent_item, type, 8);\n\nstatic inline unsigned long\nbtrfs_file_extent_inline_start(struct btrfs_file_extent_item *e)\n{\n\tunsigned long offset = (unsigned long)e;\n\toffset += offsetof(struct btrfs_file_extent_item, disk_bytenr);\n\treturn offset;\n}\n\nstatic inline u32 btrfs_file_extent_calc_inline_size(u32 datasize)\n{\n\treturn offsetof(struct btrfs_file_extent_item, disk_bytenr) + datasize;\n}\n\nBTRFS_SETGET_FUNCS(file_extent_disk_bytenr, struct btrfs_file_extent_item,\n\t\t   disk_bytenr, 64);\nBTRFS_SETGET_FUNCS(file_extent_generation, struct btrfs_file_extent_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(file_extent_disk_num_bytes, struct btrfs_file_extent_item,\n\t\t   disk_num_bytes, 64);\nBTRFS_SETGET_FUNCS(file_extent_offset, struct btrfs_file_extent_item,\n\t\t  offset, 64);\nBTRFS_SETGET_FUNCS(file_extent_num_bytes, struct btrfs_file_extent_item,\n\t\t   num_bytes, 64);\nBTRFS_SETGET_FUNCS(file_extent_ram_bytes, struct btrfs_file_extent_item,\n\t\t   ram_bytes, 64);\nBTRFS_SETGET_FUNCS(file_extent_compression, struct btrfs_file_extent_item,\n\t\t   compression, 8);\nBTRFS_SETGET_FUNCS(file_extent_encryption, struct btrfs_file_extent_item,\n\t\t   encryption, 8);\nBTRFS_SETGET_FUNCS(file_extent_other_encoding, struct btrfs_file_extent_item,\n\t\t   other_encoding, 16);\n\n/* this returns the number of file bytes represented by the inline item.\n * If an item is compressed, this is the uncompressed size\n */\nstatic inline u32 btrfs_file_extent_inline_len(struct extent_buffer *eb,\n\t\t\t\t\t       struct btrfs_file_extent_item *e)\n{\n\treturn btrfs_file_extent_ram_bytes(eb, e);\n}\n\n/*\n * this returns the number of bytes used by the item on disk, minus the\n * size of any extent headers.  If a file is compressed on disk, this is\n * the compressed size\n */\nstatic inline u32 btrfs_file_extent_inline_item_len(struct extent_buffer *eb,\n\t\t\t\t\t\t    struct btrfs_item *e)\n{\n\tunsigned long offset;\n\toffset = offsetof(struct btrfs_file_extent_item, disk_bytenr);\n\treturn btrfs_item_size(eb, e) - offset;\n}\n\n/* btrfs_dev_stats_item */\nstatic inline u64 btrfs_dev_stats_value(struct extent_buffer *eb,\n\t\t\t\t\tstruct btrfs_dev_stats_item *ptr,\n\t\t\t\t\tint index)\n{\n\tu64 val;\n\n\tread_extent_buffer(eb, &val,\n\t\t\t   offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t    ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t   sizeof(val));\n\treturn val;\n}\n\nstatic inline void btrfs_set_dev_stats_value(struct extent_buffer *eb,\n\t\t\t\t\t     struct btrfs_dev_stats_item *ptr,\n\t\t\t\t\t     int index, u64 val)\n{\n\twrite_extent_buffer(eb, &val,\n\t\t\t    offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t     ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t    sizeof(val));\n}\n\n/* btrfs_qgroup_status_item */\nBTRFS_SETGET_FUNCS(qgroup_status_generation, struct btrfs_qgroup_status_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(qgroup_status_version, struct btrfs_qgroup_status_item,\n\t\t   version, 64);\nBTRFS_SETGET_FUNCS(qgroup_status_flags, struct btrfs_qgroup_status_item,\n\t\t   flags, 64);\nBTRFS_SETGET_FUNCS(qgroup_status_scan, struct btrfs_qgroup_status_item,\n\t\t   scan, 64);\n\n/* btrfs_qgroup_info_item */\nBTRFS_SETGET_FUNCS(qgroup_info_generation, struct btrfs_qgroup_info_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(qgroup_info_rfer, struct btrfs_qgroup_info_item, rfer, 64);\nBTRFS_SETGET_FUNCS(qgroup_info_rfer_cmpr, struct btrfs_qgroup_info_item,\n\t\t   rfer_cmpr, 64);\nBTRFS_SETGET_FUNCS(qgroup_info_excl, struct btrfs_qgroup_info_item, excl, 64);\nBTRFS_SETGET_FUNCS(qgroup_info_excl_cmpr, struct btrfs_qgroup_info_item,\n\t\t   excl_cmpr, 64);\n\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_generation,\n\t\t\t struct btrfs_qgroup_info_item, generation, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_rfer, struct btrfs_qgroup_info_item,\n\t\t\t rfer, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_rfer_cmpr,\n\t\t\t struct btrfs_qgroup_info_item, rfer_cmpr, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_excl, struct btrfs_qgroup_info_item,\n\t\t\t excl, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_excl_cmpr,\n\t\t\t struct btrfs_qgroup_info_item, excl_cmpr, 64);\n\n/* btrfs_qgroup_limit_item */\nBTRFS_SETGET_FUNCS(qgroup_limit_flags, struct btrfs_qgroup_limit_item,\n\t\t   flags, 64);\nBTRFS_SETGET_FUNCS(qgroup_limit_max_rfer, struct btrfs_qgroup_limit_item,\n\t\t   max_rfer, 64);\nBTRFS_SETGET_FUNCS(qgroup_limit_max_excl, struct btrfs_qgroup_limit_item,\n\t\t   max_excl, 64);\nBTRFS_SETGET_FUNCS(qgroup_limit_rsv_rfer, struct btrfs_qgroup_limit_item,\n\t\t   rsv_rfer, 64);\nBTRFS_SETGET_FUNCS(qgroup_limit_rsv_excl, struct btrfs_qgroup_limit_item,\n\t\t   rsv_excl, 64);\n\n/* btrfs_dev_replace_item */\nBTRFS_SETGET_FUNCS(dev_replace_src_devid,\n\t\t   struct btrfs_dev_replace_item, src_devid, 64);\nBTRFS_SETGET_FUNCS(dev_replace_cont_reading_from_srcdev_mode,\n\t\t   struct btrfs_dev_replace_item, cont_reading_from_srcdev_mode,\n\t\t   64);\nBTRFS_SETGET_FUNCS(dev_replace_replace_state, struct btrfs_dev_replace_item,\n\t\t   replace_state, 64);\nBTRFS_SETGET_FUNCS(dev_replace_time_started, struct btrfs_dev_replace_item,\n\t\t   time_started, 64);\nBTRFS_SETGET_FUNCS(dev_replace_time_stopped, struct btrfs_dev_replace_item,\n\t\t   time_stopped, 64);\nBTRFS_SETGET_FUNCS(dev_replace_num_write_errors, struct btrfs_dev_replace_item,\n\t\t   num_write_errors, 64);\nBTRFS_SETGET_FUNCS(dev_replace_num_uncorrectable_read_errors,\n\t\t   struct btrfs_dev_replace_item, num_uncorrectable_read_errors,\n\t\t   64);\nBTRFS_SETGET_FUNCS(dev_replace_cursor_left, struct btrfs_dev_replace_item,\n\t\t   cursor_left, 64);\nBTRFS_SETGET_FUNCS(dev_replace_cursor_right, struct btrfs_dev_replace_item,\n\t\t   cursor_right, 64);\n\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_src_devid,\n\t\t\t struct btrfs_dev_replace_item, src_devid, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_cont_reading_from_srcdev_mode,\n\t\t\t struct btrfs_dev_replace_item,\n\t\t\t cont_reading_from_srcdev_mode, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_replace_state,\n\t\t\t struct btrfs_dev_replace_item, replace_state, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_time_started,\n\t\t\t struct btrfs_dev_replace_item, time_started, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_time_stopped,\n\t\t\t struct btrfs_dev_replace_item, time_stopped, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_num_write_errors,\n\t\t\t struct btrfs_dev_replace_item, num_write_errors, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_num_uncorrectable_read_errors,\n\t\t\t struct btrfs_dev_replace_item,\n\t\t\t num_uncorrectable_read_errors, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_cursor_left,\n\t\t\t struct btrfs_dev_replace_item, cursor_left, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_cursor_right,\n\t\t\t struct btrfs_dev_replace_item, cursor_right, 64);\n\nstatic inline struct btrfs_fs_info *btrfs_sb(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\nstatic inline u32 btrfs_level_size(struct btrfs_root *root, int level)\n{\n\tif (level == 0)\n\t\treturn root->leafsize;\n\treturn root->nodesize;\n}\n\n/* helper function to cast into the data area of the leaf. */\n#define btrfs_item_ptr(leaf, slot, type) \\\n\t((type *)(btrfs_leaf_data(leaf) + \\\n\tbtrfs_item_offset_nr(leaf, slot)))\n\n#define btrfs_item_ptr_offset(leaf, slot) \\\n\t((unsigned long)(btrfs_leaf_data(leaf) + \\\n\tbtrfs_item_offset_nr(leaf, slot)))\n\nstatic inline struct dentry *fdentry(struct file *file)\n{\n\treturn file->f_path.dentry;\n}\n\nstatic inline bool btrfs_mixed_space_info(struct btrfs_space_info *space_info)\n{\n\treturn ((space_info->flags & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t(space_info->flags & BTRFS_BLOCK_GROUP_DATA));\n}\n\nstatic inline gfp_t btrfs_alloc_write_mask(struct address_space *mapping)\n{\n\treturn mapping_gfp_mask(mapping) & ~__GFP_FS;\n}\n\n/* extent-tree.c */\nstatic inline u64 btrfs_calc_trans_metadata_size(struct btrfs_root *root,\n\t\t\t\t\t\t unsigned num_items)\n{\n\treturn (root->leafsize + root->nodesize * (BTRFS_MAX_LEVEL - 1)) *\n\t\t3 * num_items;\n}\n\n/*\n * Doing a truncate won't result in new nodes or leaves, just what we need for\n * COW.\n */\nstatic inline u64 btrfs_calc_trunc_metadata_size(struct btrfs_root *root,\n\t\t\t\t\t\t unsigned num_items)\n{\n\treturn (root->leafsize + root->nodesize * (BTRFS_MAX_LEVEL - 1)) *\n\t\tnum_items;\n}\n\nvoid btrfs_put_block_group(struct btrfs_block_group_cache *cache);\nint btrfs_run_delayed_refs(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root, unsigned long count);\nint btrfs_lookup_extent(struct btrfs_root *root, u64 start, u64 len);\nint btrfs_lookup_extent_info(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root, u64 bytenr,\n\t\t\t     u64 num_bytes, u64 *refs, u64 *flags);\nint btrfs_pin_extent(struct btrfs_root *root,\n\t\t     u64 bytenr, u64 num, int reserved);\nint btrfs_pin_extent_for_log_replay(struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_root *root,\n\t\t\t\t    u64 bytenr, u64 num_bytes);\nint btrfs_cross_ref_exist(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root,\n\t\t\t  u64 objectid, u64 offset, u64 bytenr);\nstruct btrfs_block_group_cache *btrfs_lookup_block_group(\n\t\t\t\t\t\t struct btrfs_fs_info *info,\n\t\t\t\t\t\t u64 bytenr);\nvoid btrfs_put_block_group(struct btrfs_block_group_cache *cache);\nu64 btrfs_find_block_group(struct btrfs_root *root,\n\t\t\t   u64 search_start, u64 search_hint, int owner);\nstruct extent_buffer *btrfs_alloc_free_block(struct btrfs_trans_handle *trans,\n\t\t\t\t\tstruct btrfs_root *root, u32 blocksize,\n\t\t\t\t\tu64 parent, u64 root_objectid,\n\t\t\t\t\tstruct btrfs_disk_key *key, int level,\n\t\t\t\t\tu64 hint, u64 empty_size);\nvoid btrfs_free_tree_block(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   struct extent_buffer *buf,\n\t\t\t   u64 parent, int last_ref);\nstruct extent_buffer *btrfs_init_new_buffer(struct btrfs_trans_handle *trans,\n\t\t\t\t\t    struct btrfs_root *root,\n\t\t\t\t\t    u64 bytenr, u32 blocksize,\n\t\t\t\t\t    int level);\nint btrfs_alloc_reserved_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t     u64 root_objectid, u64 owner,\n\t\t\t\t     u64 offset, struct btrfs_key *ins);\nint btrfs_alloc_logged_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t   u64 root_objectid, u64 owner, u64 offset,\n\t\t\t\t   struct btrfs_key *ins);\nint btrfs_reserve_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t  u64 num_bytes, u64 min_alloc_size,\n\t\t\t\t  u64 empty_size, u64 hint_byte,\n\t\t\t\t  struct btrfs_key *ins, u64 data);\nint btrfs_inc_ref(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t  struct extent_buffer *buf, int full_backref, int for_cow);\nint btrfs_dec_ref(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t  struct extent_buffer *buf, int full_backref, int for_cow);\nint btrfs_set_disk_extent_flags(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root,\n\t\t\t\tu64 bytenr, u64 num_bytes, u64 flags,\n\t\t\t\tint is_data);\nint btrfs_free_extent(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_root *root,\n\t\t      u64 bytenr, u64 num_bytes, u64 parent, u64 root_objectid,\n\t\t      u64 owner, u64 offset, int for_cow);\n\nint btrfs_free_reserved_extent(struct btrfs_root *root, u64 start, u64 len);\nint btrfs_free_and_pin_reserved_extent(struct btrfs_root *root,\n\t\t\t\t       u64 start, u64 len);\nvoid btrfs_prepare_extent_commit(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root);\nint btrfs_finish_extent_commit(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root);\nint btrfs_inc_extent_ref(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_root *root,\n\t\t\t u64 bytenr, u64 num_bytes, u64 parent,\n\t\t\t u64 root_objectid, u64 owner, u64 offset, int for_cow);\n\nint btrfs_write_dirty_block_groups(struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_root *root);\nint btrfs_extent_readonly(struct btrfs_root *root, u64 bytenr);\nint btrfs_free_block_groups(struct btrfs_fs_info *info);\nint btrfs_read_block_groups(struct btrfs_root *root);\nint btrfs_can_relocate(struct btrfs_root *root, u64 bytenr);\nint btrfs_make_block_group(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root, u64 bytes_used,\n\t\t\t   u64 type, u64 chunk_objectid, u64 chunk_offset,\n\t\t\t   u64 size);\nint btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root, u64 group_start);\nvoid btrfs_create_pending_block_groups(struct btrfs_trans_handle *trans,\n\t\t\t\t       struct btrfs_root *root);\nu64 btrfs_reduce_alloc_profile(struct btrfs_root *root, u64 flags);\nu64 btrfs_get_alloc_profile(struct btrfs_root *root, int data);\nvoid btrfs_clear_space_info_full(struct btrfs_fs_info *info);\n\nenum btrfs_reserve_flush_enum {\n\t/* If we are in the transaction, we can't flush anything.*/\n\tBTRFS_RESERVE_NO_FLUSH,\n\t/*\n\t * Flushing delalloc may cause deadlock somewhere, in this\n\t * case, use FLUSH LIMIT\n\t */\n\tBTRFS_RESERVE_FLUSH_LIMIT,\n\tBTRFS_RESERVE_FLUSH_ALL,\n};\n\nint btrfs_check_data_free_space(struct inode *inode, u64 bytes);\nvoid btrfs_free_reserved_data_space(struct inode *inode, u64 bytes);\nvoid btrfs_trans_release_metadata(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root);\nint btrfs_orphan_reserve_metadata(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct inode *inode);\nvoid btrfs_orphan_release_metadata(struct inode *inode);\nint btrfs_snap_reserve_metadata(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_pending_snapshot *pending);\nint btrfs_delalloc_reserve_metadata(struct inode *inode, u64 num_bytes);\nvoid btrfs_delalloc_release_metadata(struct inode *inode, u64 num_bytes);\nint btrfs_delalloc_reserve_space(struct inode *inode, u64 num_bytes);\nvoid btrfs_delalloc_release_space(struct inode *inode, u64 num_bytes);\nvoid btrfs_init_block_rsv(struct btrfs_block_rsv *rsv, unsigned short type);\nstruct btrfs_block_rsv *btrfs_alloc_block_rsv(struct btrfs_root *root,\n\t\t\t\t\t      unsigned short type);\nvoid btrfs_free_block_rsv(struct btrfs_root *root,\n\t\t\t  struct btrfs_block_rsv *rsv);\nint btrfs_block_rsv_add(struct btrfs_root *root,\n\t\t\tstruct btrfs_block_rsv *block_rsv, u64 num_bytes,\n\t\t\tenum btrfs_reserve_flush_enum flush);\nint btrfs_block_rsv_check(struct btrfs_root *root,\n\t\t\t  struct btrfs_block_rsv *block_rsv, int min_factor);\nint btrfs_block_rsv_refill(struct btrfs_root *root,\n\t\t\t   struct btrfs_block_rsv *block_rsv, u64 min_reserved,\n\t\t\t   enum btrfs_reserve_flush_enum flush);\nint btrfs_block_rsv_migrate(struct btrfs_block_rsv *src_rsv,\n\t\t\t    struct btrfs_block_rsv *dst_rsv,\n\t\t\t    u64 num_bytes);\nvoid btrfs_block_rsv_release(struct btrfs_root *root,\n\t\t\t     struct btrfs_block_rsv *block_rsv,\n\t\t\t     u64 num_bytes);\nint btrfs_set_block_group_ro(struct btrfs_root *root,\n\t\t\t     struct btrfs_block_group_cache *cache);\nvoid btrfs_set_block_group_rw(struct btrfs_root *root,\n\t\t\t      struct btrfs_block_group_cache *cache);\nvoid btrfs_put_block_group_cache(struct btrfs_fs_info *info);\nu64 btrfs_account_ro_block_groups_free_space(struct btrfs_space_info *sinfo);\nint btrfs_error_unpin_extent_range(struct btrfs_root *root,\n\t\t\t\t   u64 start, u64 end);\nint btrfs_error_discard_extent(struct btrfs_root *root, u64 bytenr,\n\t\t\t       u64 num_bytes, u64 *actual_bytes);\nint btrfs_force_chunk_alloc(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root, u64 type);\nint btrfs_trim_fs(struct btrfs_root *root, struct fstrim_range *range);\n\nint btrfs_init_space_info(struct btrfs_fs_info *fs_info);\nint btrfs_delayed_refs_qgroup_accounting(struct btrfs_trans_handle *trans,\n\t\t\t\t\t struct btrfs_fs_info *fs_info);\nint __get_raid_index(u64 flags);\n/* ctree.c */\nint btrfs_bin_search(struct extent_buffer *eb, struct btrfs_key *key,\n\t\t     int level, int *slot);\nint btrfs_comp_cpu_keys(struct btrfs_key *k1, struct btrfs_key *k2);\nint btrfs_previous_item(struct btrfs_root *root,\n\t\t\tstruct btrfs_path *path, u64 min_objectid,\n\t\t\tint type);\nvoid btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root, struct btrfs_path *path,\n\t\t\t     struct btrfs_key *new_key);\nstruct extent_buffer *btrfs_root_node(struct btrfs_root *root);\nstruct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root);\nint btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,\n\t\t\tstruct btrfs_key *key, int lowest_level,\n\t\t\tint cache_only, u64 min_trans);\nint btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,\n\t\t\t struct btrfs_key *max_key,\n\t\t\t struct btrfs_path *path, int cache_only,\n\t\t\t u64 min_trans);\nenum btrfs_compare_tree_result {\n\tBTRFS_COMPARE_TREE_NEW,\n\tBTRFS_COMPARE_TREE_DELETED,\n\tBTRFS_COMPARE_TREE_CHANGED,\n};\ntypedef int (*btrfs_changed_cb_t)(struct btrfs_root *left_root,\n\t\t\t\t  struct btrfs_root *right_root,\n\t\t\t\t  struct btrfs_path *left_path,\n\t\t\t\t  struct btrfs_path *right_path,\n\t\t\t\t  struct btrfs_key *key,\n\t\t\t\t  enum btrfs_compare_tree_result result,\n\t\t\t\t  void *ctx);\nint btrfs_compare_trees(struct btrfs_root *left_root,\n\t\t\tstruct btrfs_root *right_root,\n\t\t\tbtrfs_changed_cb_t cb, void *ctx);\nint btrfs_cow_block(struct btrfs_trans_handle *trans,\n\t\t    struct btrfs_root *root, struct extent_buffer *buf,\n\t\t    struct extent_buffer *parent, int parent_slot,\n\t\t    struct extent_buffer **cow_ret);\nint btrfs_copy_root(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_root *root,\n\t\t      struct extent_buffer *buf,\n\t\t      struct extent_buffer **cow_ret, u64 new_root_objectid);\nint btrfs_block_can_be_shared(struct btrfs_root *root,\n\t\t\t      struct extent_buffer *buf);\nvoid btrfs_extend_item(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root, struct btrfs_path *path,\n\t\t       u32 data_size);\nvoid btrfs_truncate_item(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_root *root,\n\t\t\t struct btrfs_path *path,\n\t\t\t u32 new_size, int from_end);\nint btrfs_split_item(struct btrfs_trans_handle *trans,\n\t\t     struct btrfs_root *root,\n\t\t     struct btrfs_path *path,\n\t\t     struct btrfs_key *new_key,\n\t\t     unsigned long split_offset);\nint btrfs_duplicate_item(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_root *root,\n\t\t\t struct btrfs_path *path,\n\t\t\t struct btrfs_key *new_key);\nint btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t      *root, struct btrfs_key *key, struct btrfs_path *p, int\n\t\t      ins_len, int cow);\nint btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,\n\t\t\t  struct btrfs_path *p, u64 time_seq);\nint btrfs_search_slot_for_read(struct btrfs_root *root,\n\t\t\t       struct btrfs_key *key, struct btrfs_path *p,\n\t\t\t       int find_higher, int return_any);\nint btrfs_realloc_node(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root, struct extent_buffer *parent,\n\t\t       int start_slot, int cache_only, u64 *last_ret,\n\t\t       struct btrfs_key *progress);\nvoid btrfs_release_path(struct btrfs_path *p);\nstruct btrfs_path *btrfs_alloc_path(void);\nvoid btrfs_free_path(struct btrfs_path *p);\nvoid btrfs_set_path_blocking(struct btrfs_path *p);\nvoid btrfs_clear_path_blocking(struct btrfs_path *p,\n\t\t\t       struct extent_buffer *held, int held_rw);\nvoid btrfs_unlock_up_safe(struct btrfs_path *p, int level);\n\nint btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t   struct btrfs_path *path, int slot, int nr);\nstatic inline int btrfs_del_item(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root,\n\t\t\t\t struct btrfs_path *path)\n{\n\treturn btrfs_del_items(trans, root, path, path->slots[0], 1);\n}\n\nvoid setup_items_for_insert(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root, struct btrfs_path *path,\n\t\t\t    struct btrfs_key *cpu_key, u32 *data_size,\n\t\t\t    u32 total_data, u32 total_size, int nr);\nint btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t      *root, struct btrfs_key *key, void *data, u32 data_size);\nint btrfs_insert_empty_items(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root,\n\t\t\t     struct btrfs_path *path,\n\t\t\t     struct btrfs_key *cpu_key, u32 *data_size, int nr);\n\nstatic inline int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t  struct btrfs_key *key,\n\t\t\t\t\t  u32 data_size)\n{\n\treturn btrfs_insert_empty_items(trans, root, path, key, &data_size, 1);\n}\n\nint btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path);\nint btrfs_next_leaf_write(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root, struct btrfs_path *path,\n\t\t\t  int del);\nint btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,\n\t\t\tu64 time_seq);\nstatic inline int btrfs_next_old_item(struct btrfs_root *root,\n\t\t\t\t      struct btrfs_path *p, u64 time_seq)\n{\n\t++p->slots[0];\n\tif (p->slots[0] >= btrfs_header_nritems(p->nodes[0]))\n\t\treturn btrfs_next_old_leaf(root, p, time_seq);\n\treturn 0;\n}\nstatic inline int btrfs_next_item(struct btrfs_root *root, struct btrfs_path *p)\n{\n\treturn btrfs_next_old_item(root, p, 0);\n}\nint btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path);\nint btrfs_leaf_free_space(struct btrfs_root *root, struct extent_buffer *leaf);\nint __must_check btrfs_drop_snapshot(struct btrfs_root *root,\n\t\t\t\t     struct btrfs_block_rsv *block_rsv,\n\t\t\t\t     int update_ref, int for_reloc);\nint btrfs_drop_subtree(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root,\n\t\t\tstruct extent_buffer *node,\n\t\t\tstruct extent_buffer *parent);\nstatic inline int btrfs_fs_closing(struct btrfs_fs_info *fs_info)\n{\n\t/*\n\t * Get synced with close_ctree()\n\t */\n\tsmp_mb();\n\treturn fs_info->closing;\n}\nstatic inline void free_fs_info(struct btrfs_fs_info *fs_info)\n{\n\tkfree(fs_info->balance_ctl);\n\tkfree(fs_info->delayed_root);\n\tkfree(fs_info->extent_root);\n\tkfree(fs_info->tree_root);\n\tkfree(fs_info->chunk_root);\n\tkfree(fs_info->dev_root);\n\tkfree(fs_info->csum_root);\n\tkfree(fs_info->quota_root);\n\tkfree(fs_info->super_copy);\n\tkfree(fs_info->super_for_commit);\n\tkfree(fs_info);\n}\n\n/* tree mod log functions from ctree.c */\nu64 btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,\n\t\t\t   struct seq_list *elem);\nvoid btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,\n\t\t\t    struct seq_list *elem);\nstatic inline u64 btrfs_inc_tree_mod_seq(struct btrfs_fs_info *fs_info)\n{\n\treturn atomic_inc_return(&fs_info->tree_mod_seq);\n}\nint btrfs_old_root_level(struct btrfs_root *root, u64 time_seq);\n\n/* root-item.c */\nint btrfs_find_root_ref(struct btrfs_root *tree_root,\n\t\t\tstruct btrfs_path *path,\n\t\t\tu64 root_id, u64 ref_id);\nint btrfs_add_root_ref(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *tree_root,\n\t\t       u64 root_id, u64 ref_id, u64 dirid, u64 sequence,\n\t\t       const char *name, int name_len);\nint btrfs_del_root_ref(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *tree_root,\n\t\t       u64 root_id, u64 ref_id, u64 dirid, u64 *sequence,\n\t\t       const char *name, int name_len);\nint btrfs_del_root(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t   struct btrfs_key *key);\nint btrfs_insert_root(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t      *root, struct btrfs_key *key, struct btrfs_root_item\n\t\t      *item);\nint __must_check btrfs_update_root(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t   struct btrfs_key *key,\n\t\t\t\t   struct btrfs_root_item *item);\nvoid btrfs_read_root_item(struct btrfs_root *root,\n\t\t\t struct extent_buffer *eb, int slot,\n\t\t\t struct btrfs_root_item *item);\nint btrfs_find_last_root(struct btrfs_root *root, u64 objectid, struct\n\t\t\t btrfs_root_item *item, struct btrfs_key *key);\nint btrfs_find_dead_roots(struct btrfs_root *root, u64 objectid);\nint btrfs_find_orphan_roots(struct btrfs_root *tree_root);\nvoid btrfs_set_root_node(struct btrfs_root_item *item,\n\t\t\t struct extent_buffer *node);\nvoid btrfs_check_and_init_root_item(struct btrfs_root_item *item);\nvoid btrfs_update_root_times(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root);\n\n/* dir-item.c */\nint btrfs_insert_dir_item(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root, const char *name,\n\t\t\t  int name_len, struct inode *dir,\n\t\t\t  struct btrfs_key *location, u8 type, u64 index);\nstruct btrfs_dir_item *btrfs_lookup_dir_item(struct btrfs_trans_handle *trans,\n\t\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t\t     struct btrfs_path *path, u64 dir,\n\t\t\t\t\t     const char *name, int name_len,\n\t\t\t\t\t     int mod);\nstruct btrfs_dir_item *\nbtrfs_lookup_dir_index_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 dir,\n\t\t\t    u64 objectid, const char *name, int name_len,\n\t\t\t    int mod);\nstruct btrfs_dir_item *\nbtrfs_search_dir_index_item(struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 dirid,\n\t\t\t    const char *name, int name_len);\nstruct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len);\nint btrfs_delete_one_dir_name(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      struct btrfs_dir_item *di);\nint btrfs_insert_xattr_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 objectid,\n\t\t\t    const char *name, u16 name_len,\n\t\t\t    const void *data, u16 data_len);\nstruct btrfs_dir_item *btrfs_lookup_xattr(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_path *path, u64 dir,\n\t\t\t\t\t  const char *name, u16 name_len,\n\t\t\t\t\t  int mod);\nint verify_dir_item(struct btrfs_root *root,\n\t\t    struct extent_buffer *leaf,\n\t\t    struct btrfs_dir_item *dir_item);\n\n/* orphan.c */\nint btrfs_insert_orphan_item(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root, u64 offset);\nint btrfs_del_orphan_item(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root, u64 offset);\nint btrfs_find_orphan_item(struct btrfs_root *root, u64 offset);\n\n/* inode-item.c */\nint btrfs_insert_inode_ref(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   const char *name, int name_len,\n\t\t\t   u64 inode_objectid, u64 ref_objectid, u64 index);\nint btrfs_del_inode_ref(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   const char *name, int name_len,\n\t\t\t   u64 inode_objectid, u64 ref_objectid, u64 *index);\nint btrfs_get_inode_ref_index(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len,\n\t\t\t      u64 inode_objectid, u64 ref_objectid, int mod,\n\t\t\t      u64 *ret_index);\nint btrfs_insert_empty_inode(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root,\n\t\t\t     struct btrfs_path *path, u64 objectid);\nint btrfs_lookup_inode(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t       *root, struct btrfs_path *path,\n\t\t       struct btrfs_key *location, int mod);\n\nstruct btrfs_inode_extref *\nbtrfs_lookup_inode_extref(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root,\n\t\t\t  struct btrfs_path *path,\n\t\t\t  const char *name, int name_len,\n\t\t\t  u64 inode_objectid, u64 ref_objectid, int ins_len,\n\t\t\t  int cow);\n\nint btrfs_find_name_in_ext_backref(struct btrfs_path *path,\n\t\t\t\t   u64 ref_objectid, const char *name,\n\t\t\t\t   int name_len,\n\t\t\t\t   struct btrfs_inode_extref **extref_ret);\n\n/* file-item.c */\nint btrfs_del_csums(struct btrfs_trans_handle *trans,\n\t\t    struct btrfs_root *root, u64 bytenr, u64 len);\nint btrfs_lookup_bio_sums(struct btrfs_root *root, struct inode *inode,\n\t\t\t  struct bio *bio, u32 *dst);\nint btrfs_lookup_bio_sums_dio(struct btrfs_root *root, struct inode *inode,\n\t\t\t      struct bio *bio, u64 logical_offset);\nint btrfs_insert_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root,\n\t\t\t     u64 objectid, u64 pos,\n\t\t\t     u64 disk_offset, u64 disk_num_bytes,\n\t\t\t     u64 num_bytes, u64 offset, u64 ram_bytes,\n\t\t\t     u8 compression, u8 encryption, u16 other_encoding);\nint btrfs_lookup_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root,\n\t\t\t     struct btrfs_path *path, u64 objectid,\n\t\t\t     u64 bytenr, int mod);\nu64 btrfs_file_extent_length(struct btrfs_path *path);\nint btrfs_csum_file_blocks(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   struct btrfs_ordered_sum *sums);\nint btrfs_csum_one_bio(struct btrfs_root *root, struct inode *inode,\n\t\t       struct bio *bio, u64 file_start, int contig);\nstruct btrfs_csum_item *btrfs_lookup_csum(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t  u64 bytenr, int cow);\nint btrfs_csum_truncate(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root, struct btrfs_path *path,\n\t\t\tu64 isize);\nint btrfs_lookup_csums_range(struct btrfs_root *root, u64 start, u64 end,\n\t\t\t     struct list_head *list, int search_commit);\n/* inode.c */\nstruct btrfs_delalloc_work {\n\tstruct inode *inode;\n\tint wait;\n\tint delay_iput;\n\tstruct completion completion;\n\tstruct list_head list;\n\tstruct btrfs_work work;\n};\n\nstruct btrfs_delalloc_work *btrfs_alloc_delalloc_work(struct inode *inode,\n\t\t\t\t\t\t    int wait, int delay_iput);\nvoid btrfs_wait_and_free_delalloc_work(struct btrfs_delalloc_work *work);\n\nstruct extent_map *btrfs_get_extent_fiemap(struct inode *inode, struct page *page,\n\t\t\t\t\t   size_t pg_offset, u64 start, u64 len,\n\t\t\t\t\t   int create);\n\n/* RHEL and EL kernels have a patch that renames PG_checked to FsMisc */\n#if defined(ClearPageFsMisc) && !defined(ClearPageChecked)\n#define ClearPageChecked ClearPageFsMisc\n#define SetPageChecked SetPageFsMisc\n#define PageChecked PageFsMisc\n#endif\n\n/* This forces readahead on a given range of bytes in an inode */\nstatic inline void btrfs_force_ra(struct address_space *mapping,\n\t\t\t\t  struct file_ra_state *ra, struct file *file,\n\t\t\t\t  pgoff_t offset, unsigned long req_size)\n{\n\tpage_cache_sync_readahead(mapping, ra, file, offset, req_size);\n}\n\nstruct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry);\nint btrfs_set_inode_index(struct inode *dir, u64 *index);\nint btrfs_unlink_inode(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root,\n\t\t       struct inode *dir, struct inode *inode,\n\t\t       const char *name, int name_len);\nint btrfs_add_link(struct btrfs_trans_handle *trans,\n\t\t   struct inode *parent_inode, struct inode *inode,\n\t\t   const char *name, int name_len, int add_backref, u64 index);\nint btrfs_unlink_subvol(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root,\n\t\t\tstruct inode *dir, u64 objectid,\n\t\t\tconst char *name, int name_len);\nint btrfs_truncate_page(struct inode *inode, loff_t from, loff_t len,\n\t\t\tint front);\nint btrfs_truncate_inode_items(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root,\n\t\t\t       struct inode *inode, u64 new_size,\n\t\t\t       u32 min_type);\n\nint btrfs_start_delalloc_inodes(struct btrfs_root *root, int delay_iput);\nint btrfs_set_extent_delalloc(struct inode *inode, u64 start, u64 end,\n\t\t\t      struct extent_state **cached_state);\nint btrfs_writepages(struct address_space *mapping,\n\t\t     struct writeback_control *wbc);\nint btrfs_create_subvol_root(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *new_root, u64 new_dirid);\nint btrfs_merge_bio_hook(struct page *page, unsigned long offset,\n\t\t\t size_t size, struct bio *bio, unsigned long bio_flags);\n\nint btrfs_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);\nint btrfs_readpage(struct file *file, struct page *page);\nvoid btrfs_evict_inode(struct inode *inode);\nint btrfs_write_inode(struct inode *inode, struct writeback_control *wbc);\nint btrfs_dirty_inode(struct inode *inode);\nstruct inode *btrfs_alloc_inode(struct super_block *sb);\nvoid btrfs_destroy_inode(struct inode *inode);\nint btrfs_drop_inode(struct inode *inode);\nint btrfs_init_cachep(void);\nvoid btrfs_destroy_cachep(void);\nlong btrfs_ioctl_trans_end(struct file *file);\nstruct inode *btrfs_iget(struct super_block *s, struct btrfs_key *location,\n\t\t\t struct btrfs_root *root, int *was_new);\nstruct extent_map *btrfs_get_extent(struct inode *inode, struct page *page,\n\t\t\t\t    size_t pg_offset, u64 start, u64 end,\n\t\t\t\t    int create);\nint btrfs_update_inode(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root,\n\t\t\t      struct inode *inode);\nint btrfs_update_inode_fallback(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, struct inode *inode);\nint btrfs_orphan_add(struct btrfs_trans_handle *trans, struct inode *inode);\nint btrfs_orphan_del(struct btrfs_trans_handle *trans, struct inode *inode);\nint btrfs_orphan_cleanup(struct btrfs_root *root);\nvoid btrfs_orphan_commit_root(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root);\nint btrfs_cont_expand(struct inode *inode, loff_t oldsize, loff_t size);\nvoid btrfs_invalidate_inodes(struct btrfs_root *root);\nvoid btrfs_add_delayed_iput(struct inode *inode);\nvoid btrfs_run_delayed_iputs(struct btrfs_root *root);\nint btrfs_prealloc_file_range(struct inode *inode, int mode,\n\t\t\t      u64 start, u64 num_bytes, u64 min_size,\n\t\t\t      loff_t actual_len, u64 *alloc_hint);\nint btrfs_prealloc_file_range_trans(struct inode *inode,\n\t\t\t\t    struct btrfs_trans_handle *trans, int mode,\n\t\t\t\t    u64 start, u64 num_bytes, u64 min_size,\n\t\t\t\t    loff_t actual_len, u64 *alloc_hint);\nextern const struct dentry_operations btrfs_dentry_operations;\n\n/* ioctl.c */\nlong btrfs_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\nvoid btrfs_update_iflags(struct inode *inode);\nvoid btrfs_inherit_iflags(struct inode *inode, struct inode *dir);\nint btrfs_defrag_file(struct inode *inode, struct file *file,\n\t\t      struct btrfs_ioctl_defrag_range_args *range,\n\t\t      u64 newer_than, unsigned long max_pages);\nvoid btrfs_get_block_group_info(struct list_head *groups_list,\n\t\t\t\tstruct btrfs_ioctl_space_info *space);\n\n/* file.c */\nint btrfs_auto_defrag_init(void);\nvoid btrfs_auto_defrag_exit(void);\nint btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,\n\t\t\t   struct inode *inode);\nint btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info);\nvoid btrfs_cleanup_defrag_inodes(struct btrfs_fs_info *fs_info);\nint btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync);\nvoid btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,\n\t\t\t     int skip_pinned);\nint btrfs_replace_extent_cache(struct inode *inode, struct extent_map *replace,\n\t\t\t       u64 start, u64 end, int skip_pinned,\n\t\t\t       int modified);\nextern const struct file_operations btrfs_file_operations;\nint __btrfs_drop_extents(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_root *root, struct inode *inode,\n\t\t\t struct btrfs_path *path, u64 start, u64 end,\n\t\t\t u64 *drop_end, int drop_cache);\nint btrfs_drop_extents(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root, struct inode *inode, u64 start,\n\t\t       u64 end, int drop_cache);\nint btrfs_mark_extent_written(struct btrfs_trans_handle *trans,\n\t\t\t      struct inode *inode, u64 start, u64 end);\nint btrfs_release_file(struct inode *inode, struct file *file);\nvoid btrfs_drop_pages(struct page **pages, size_t num_pages);\nint btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,\n\t\t      struct page **pages, size_t num_pages,\n\t\t      loff_t pos, size_t write_bytes,\n\t\t      struct extent_state **cached);\n\n/* tree-defrag.c */\nint btrfs_defrag_leaves(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root, int cache_only);\n\n/* sysfs.c */\nint btrfs_init_sysfs(void);\nvoid btrfs_exit_sysfs(void);\n\n/* xattr.c */\nssize_t btrfs_listxattr(struct dentry *dentry, char *buffer, size_t size);\n\n/* super.c */\nint btrfs_parse_options(struct btrfs_root *root, char *options);\nint btrfs_sync_fs(struct super_block *sb, int wait);\n\n#ifdef CONFIG_PRINTK\n__printf(2, 3)\nvoid btrfs_printk(struct btrfs_fs_info *fs_info, const char *fmt, ...);\n#else\nstatic inline __printf(2, 3)\nvoid btrfs_printk(struct btrfs_fs_info *fs_info, const char *fmt, ...)\n{\n}\n#endif\n\n__printf(5, 6)\nvoid __btrfs_std_error(struct btrfs_fs_info *fs_info, const char *function,\n\t\t     unsigned int line, int errno, const char *fmt, ...);\n\n\nvoid __btrfs_abort_transaction(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root, const char *function,\n\t\t\t       unsigned int line, int errno);\n\n#define btrfs_set_fs_incompat(__fs_info, opt) \\\n\t__btrfs_set_fs_incompat((__fs_info), BTRFS_FEATURE_INCOMPAT_##opt)\n\nstatic inline void __btrfs_set_fs_incompat(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t   u64 flag)\n{\n\tstruct btrfs_super_block *disk_super;\n\tu64 features;\n\n\tdisk_super = fs_info->super_copy;\n\tfeatures = btrfs_super_incompat_flags(disk_super);\n\tif (!(features & flag)) {\n\t\tfeatures |= flag;\n\t\tbtrfs_set_super_incompat_flags(disk_super, features);\n\t}\n}\n\n/*\n * Call btrfs_abort_transaction as early as possible when an error condition is\n * detected, that way the exact line number is reported.\n */\n\n#define btrfs_abort_transaction(trans, root, errno)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\t__btrfs_abort_transaction(trans, root, __func__,\t\\\n\t\t\t\t  __LINE__, errno);\t\t\\\n} while (0)\n\n#define btrfs_std_error(fs_info, errno)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif ((errno))\t\t\t\t\t\t\\\n\t\t__btrfs_std_error((fs_info), __func__,\t\t\\\n\t\t\t\t   __LINE__, (errno), NULL);\t\\\n} while (0)\n\n#define btrfs_error(fs_info, errno, fmt, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\t__btrfs_std_error((fs_info), __func__, __LINE__,\t\\\n\t\t\t  (errno), fmt, ##args);\t\t\\\n} while (0)\n\n__printf(5, 6)\nvoid __btrfs_panic(struct btrfs_fs_info *fs_info, const char *function,\n\t\t   unsigned int line, int errno, const char *fmt, ...);\n\n#define btrfs_panic(fs_info, errno, fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstruct btrfs_fs_info *_i = (fs_info);\t\t\t\t\\\n\t__btrfs_panic(_i, __func__, __LINE__, errno, fmt, ##args);\t\\\n\tBUG_ON(!(_i->mount_opt & BTRFS_MOUNT_PANIC_ON_FATAL_ERROR));\t\\\n} while (0)\n\n/* acl.c */\n#ifdef CONFIG_BTRFS_FS_POSIX_ACL\nstruct posix_acl *btrfs_get_acl(struct inode *inode, int type);\nint btrfs_init_acl(struct btrfs_trans_handle *trans,\n\t\t   struct inode *inode, struct inode *dir);\nint btrfs_acl_chmod(struct inode *inode);\n#else\n#define btrfs_get_acl NULL\nstatic inline int btrfs_init_acl(struct btrfs_trans_handle *trans,\n\t\t\t\t struct inode *inode, struct inode *dir)\n{\n\treturn 0;\n}\nstatic inline int btrfs_acl_chmod(struct inode *inode)\n{\n\treturn 0;\n}\n#endif\n\n/* relocation.c */\nint btrfs_relocate_block_group(struct btrfs_root *root, u64 group_start);\nint btrfs_init_reloc_root(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root);\nint btrfs_update_reloc_root(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root);\nint btrfs_recover_relocation(struct btrfs_root *root);\nint btrfs_reloc_clone_csums(struct inode *inode, u64 file_pos, u64 len);\nvoid btrfs_reloc_cow_block(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root, struct extent_buffer *buf,\n\t\t\t   struct extent_buffer *cow);\nvoid btrfs_reloc_pre_snapshot(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_pending_snapshot *pending,\n\t\t\t      u64 *bytes_to_reserve);\nint btrfs_reloc_post_snapshot(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_pending_snapshot *pending);\n\n/* scrub.c */\nint btrfs_scrub_dev(struct btrfs_fs_info *fs_info, u64 devid, u64 start,\n\t\t    u64 end, struct btrfs_scrub_progress *progress,\n\t\t    int readonly, int is_dev_replace);\nvoid btrfs_scrub_pause(struct btrfs_root *root);\nvoid btrfs_scrub_pause_super(struct btrfs_root *root);\nvoid btrfs_scrub_continue(struct btrfs_root *root);\nvoid btrfs_scrub_continue_super(struct btrfs_root *root);\nint btrfs_scrub_cancel(struct btrfs_fs_info *info);\nint btrfs_scrub_cancel_dev(struct btrfs_fs_info *info,\n\t\t\t   struct btrfs_device *dev);\nint btrfs_scrub_cancel_devid(struct btrfs_root *root, u64 devid);\nint btrfs_scrub_progress(struct btrfs_root *root, u64 devid,\n\t\t\t struct btrfs_scrub_progress *progress);\n\n/* reada.c */\nstruct reada_control {\n\tstruct btrfs_root\t*root;\t\t/* tree to prefetch */\n\tstruct btrfs_key\tkey_start;\n\tstruct btrfs_key\tkey_end;\t/* exclusive */\n\tatomic_t\t\telems;\n\tstruct kref\t\trefcnt;\n\twait_queue_head_t\twait;\n};\nstruct reada_control *btrfs_reada_add(struct btrfs_root *root,\n\t\t\t      struct btrfs_key *start, struct btrfs_key *end);\nint btrfs_reada_wait(void *handle);\nvoid btrfs_reada_detach(void *handle);\nint btree_readahead_hook(struct btrfs_root *root, struct extent_buffer *eb,\n\t\t\t u64 start, int err);\n\n/* qgroup.c */\nstruct qgroup_update {\n\tstruct list_head list;\n\tstruct btrfs_delayed_ref_node *node;\n\tstruct btrfs_delayed_extent_op *extent_op;\n};\n\nint btrfs_quota_enable(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_fs_info *fs_info);\nint btrfs_quota_disable(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_fs_info *fs_info);\nint btrfs_quota_rescan(struct btrfs_fs_info *fs_info);\nint btrfs_add_qgroup_relation(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_fs_info *fs_info, u64 src, u64 dst);\nint btrfs_del_qgroup_relation(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_fs_info *fs_info, u64 src, u64 dst);\nint btrfs_create_qgroup(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_fs_info *fs_info, u64 qgroupid,\n\t\t\tchar *name);\nint btrfs_remove_qgroup(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_fs_info *fs_info, u64 qgroupid);\nint btrfs_limit_qgroup(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_fs_info *fs_info, u64 qgroupid,\n\t\t       struct btrfs_qgroup_limit *limit);\nint btrfs_read_qgroup_config(struct btrfs_fs_info *fs_info);\nvoid btrfs_free_qgroup_config(struct btrfs_fs_info *fs_info);\nstruct btrfs_delayed_extent_op;\nint btrfs_qgroup_record_ref(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_delayed_ref_node *node,\n\t\t\t    struct btrfs_delayed_extent_op *extent_op);\nint btrfs_qgroup_account_ref(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_fs_info *fs_info,\n\t\t\t     struct btrfs_delayed_ref_node *node,\n\t\t\t     struct btrfs_delayed_extent_op *extent_op);\nint btrfs_run_qgroups(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_fs_info *fs_info);\nint btrfs_qgroup_inherit(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_fs_info *fs_info, u64 srcid, u64 objectid,\n\t\t\t struct btrfs_qgroup_inherit *inherit);\nint btrfs_qgroup_reserve(struct btrfs_root *root, u64 num_bytes);\nvoid btrfs_qgroup_free(struct btrfs_root *root, u64 num_bytes);\n\nvoid assert_qgroups_uptodate(struct btrfs_trans_handle *trans);\n\nstatic inline int is_fstree(u64 rootid)\n{\n\tif (rootid == BTRFS_FS_TREE_OBJECTID ||\n\t    (s64)rootid >= (s64)BTRFS_FIRST_FREE_OBJECTID)\n\t\treturn 1;\n\treturn 0;\n}\n#endif\n", "/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"hash.h\"\n#include \"transaction.h\"\n\n/*\n * insert a name into a directory, doing overflow properly if there is a hash\n * collision.  data_size indicates how big the item inserted should be.  On\n * success a struct btrfs_dir_item pointer is returned, otherwise it is\n * an ERR_PTR.\n *\n * The name is not copied into the dir item, you have to do that yourself.\n */\nstatic struct btrfs_dir_item *insert_with_overflow(struct btrfs_trans_handle\n\t\t\t\t\t\t   *trans,\n\t\t\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t\t\t   struct btrfs_key *cpu_key,\n\t\t\t\t\t\t   u32 data_size,\n\t\t\t\t\t\t   const char *name,\n\t\t\t\t\t\t   int name_len)\n{\n\tint ret;\n\tchar *ptr;\n\tstruct btrfs_item *item;\n\tstruct extent_buffer *leaf;\n\n\tret = btrfs_insert_empty_item(trans, root, path, cpu_key, data_size);\n\tif (ret == -EEXIST) {\n\t\tstruct btrfs_dir_item *di;\n\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n\t\tif (di)\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\tbtrfs_extend_item(trans, root, path, data_size);\n\t} else if (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tWARN_ON(ret > 0);\n\tleaf = path->nodes[0];\n\titem = btrfs_item_nr(leaf, path->slots[0]);\n\tptr = btrfs_item_ptr(leaf, path->slots[0], char);\n\tBUG_ON(data_size > btrfs_item_size(leaf, item));\n\tptr += btrfs_item_size(leaf, item) - data_size;\n\treturn (struct btrfs_dir_item *)ptr;\n}\n\n/*\n * xattrs work a lot like directories, this inserts an xattr item\n * into the tree\n */\nint btrfs_insert_xattr_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 objectid,\n\t\t\t    const char *name, u16 name_len,\n\t\t\t    const void *data, u16 data_len)\n{\n\tint ret = 0;\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr, data_ptr;\n\tstruct btrfs_key key, location;\n\tstruct btrfs_disk_key disk_key;\n\tstruct extent_buffer *leaf;\n\tu32 data_size;\n\n\tBUG_ON(name_len + data_len > BTRFS_MAX_XATTR_SIZE(root));\n\n\tkey.objectid = objectid;\n\tbtrfs_set_key_type(&key, BTRFS_XATTR_ITEM_KEY);\n\tkey.offset = btrfs_name_hash(name, name_len);\n\n\tdata_size = sizeof(*dir_item) + name_len + data_len;\n\tdir_item = insert_with_overflow(trans, root, path, &key, data_size,\n\t\t\t\t\tname, name_len);\n\tif (IS_ERR(dir_item))\n\t\treturn PTR_ERR(dir_item);\n\tmemset(&location, 0, sizeof(location));\n\n\tleaf = path->nodes[0];\n\tbtrfs_cpu_key_to_disk(&disk_key, &location);\n\tbtrfs_set_dir_item_key(leaf, dir_item, &disk_key);\n\tbtrfs_set_dir_type(leaf, dir_item, BTRFS_FT_XATTR);\n\tbtrfs_set_dir_name_len(leaf, dir_item, name_len);\n\tbtrfs_set_dir_transid(leaf, dir_item, trans->transid);\n\tbtrfs_set_dir_data_len(leaf, dir_item, data_len);\n\tname_ptr = (unsigned long)(dir_item + 1);\n\tdata_ptr = (unsigned long)((char *)name_ptr + name_len);\n\n\twrite_extent_buffer(leaf, name, name_ptr, name_len);\n\twrite_extent_buffer(leaf, data, data_ptr, data_len);\n\tbtrfs_mark_buffer_dirty(path->nodes[0]);\n\n\treturn ret;\n}\n\n/*\n * insert a directory item in the tree, doing all the magic for\n * both indexes. 'dir' indicates which objectid to insert it into,\n * 'location' is the key to stuff into the directory item, 'type' is the\n * type of the inode we're pointing to, and 'index' is the sequence number\n * to use for the second index (if one is created).\n * Will return 0 or -ENOMEM\n */\nint btrfs_insert_dir_item(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t\t  *root, const char *name, int name_len,\n\t\t\t  struct inode *dir, struct btrfs_key *location,\n\t\t\t  u8 type, u64 index)\n{\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dir_item *dir_item;\n\tstruct extent_buffer *leaf;\n\tunsigned long name_ptr;\n\tstruct btrfs_key key;\n\tstruct btrfs_disk_key disk_key;\n\tu32 data_size;\n\n\tkey.objectid = btrfs_ino(dir);\n\tbtrfs_set_key_type(&key, BTRFS_DIR_ITEM_KEY);\n\tkey.offset = btrfs_name_hash(name, name_len);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tpath->leave_spinning = 1;\n\n\tbtrfs_cpu_key_to_disk(&disk_key, location);\n\n\tdata_size = sizeof(*dir_item) + name_len;\n\tdir_item = insert_with_overflow(trans, root, path, &key, data_size,\n\t\t\t\t\tname, name_len);\n\tif (IS_ERR(dir_item)) {\n\t\tret = PTR_ERR(dir_item);\n\t\tif (ret == -EEXIST)\n\t\t\tgoto second_insert;\n\t\tgoto out_free;\n\t}\n\n\tleaf = path->nodes[0];\n\tbtrfs_set_dir_item_key(leaf, dir_item, &disk_key);\n\tbtrfs_set_dir_type(leaf, dir_item, type);\n\tbtrfs_set_dir_data_len(leaf, dir_item, 0);\n\tbtrfs_set_dir_name_len(leaf, dir_item, name_len);\n\tbtrfs_set_dir_transid(leaf, dir_item, trans->transid);\n\tname_ptr = (unsigned long)(dir_item + 1);\n\n\twrite_extent_buffer(leaf, name, name_ptr, name_len);\n\tbtrfs_mark_buffer_dirty(leaf);\n\nsecond_insert:\n\t/* FIXME, use some real flag for selecting the extra index */\n\tif (root == root->fs_info->tree_root) {\n\t\tret = 0;\n\t\tgoto out_free;\n\t}\n\tbtrfs_release_path(path);\n\n\tret2 = btrfs_insert_delayed_dir_index(trans, root, name, name_len, dir,\n\t\t\t\t\t      &disk_key, type, index);\nout_free:\n\tbtrfs_free_path(path);\n\tif (ret)\n\t\treturn ret;\n\tif (ret2)\n\t\treturn ret2;\n\treturn 0;\n}\n\n/*\n * lookup a directory item based on name.  'dir' is the objectid\n * we're searching in, and 'mod' tells us if you plan on deleting the\n * item (use mod < 0) or changing the options (use mod > 0)\n */\nstruct btrfs_dir_item *btrfs_lookup_dir_item(struct btrfs_trans_handle *trans,\n\t\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t\t     struct btrfs_path *path, u64 dir,\n\t\t\t\t\t     const char *name, int name_len,\n\t\t\t\t\t     int mod)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tint ins_len = mod < 0 ? -1 : 0;\n\tint cow = mod != 0;\n\n\tkey.objectid = dir;\n\tbtrfs_set_key_type(&key, BTRFS_DIR_ITEM_KEY);\n\n\tkey.offset = btrfs_name_hash(name, name_len);\n\n\tret = btrfs_search_slot(trans, root, &key, path, ins_len, cow);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (ret > 0)\n\t\treturn NULL;\n\n\treturn btrfs_match_dir_item_name(root, path, name, name_len);\n}\n\n/*\n * lookup a directory item based on index.  'dir' is the objectid\n * we're searching in, and 'mod' tells us if you plan on deleting the\n * item (use mod < 0) or changing the options (use mod > 0)\n *\n * The name is used to make sure the index really points to the name you were\n * looking for.\n */\nstruct btrfs_dir_item *\nbtrfs_lookup_dir_index_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 dir,\n\t\t\t    u64 objectid, const char *name, int name_len,\n\t\t\t    int mod)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tint ins_len = mod < 0 ? -1 : 0;\n\tint cow = mod != 0;\n\n\tkey.objectid = dir;\n\tbtrfs_set_key_type(&key, BTRFS_DIR_INDEX_KEY);\n\tkey.offset = objectid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, ins_len, cow);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (ret > 0)\n\t\treturn ERR_PTR(-ENOENT);\n\treturn btrfs_match_dir_item_name(root, path, name, name_len);\n}\n\nstruct btrfs_dir_item *\nbtrfs_search_dir_index_item(struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 dirid,\n\t\t\t    const char *name, int name_len)\n{\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_key key;\n\tu32 nritems;\n\tint ret;\n\n\tkey.objectid = dirid;\n\tkey.type = BTRFS_DIR_INDEX_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tleaf = path->nodes[0];\n\tnritems = btrfs_header_nritems(leaf);\n\n\twhile (1) {\n\t\tif (path->slots[0] >= nritems) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ERR_PTR(ret);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tleaf = path->nodes[0];\n\t\t\tnritems = btrfs_header_nritems(leaf);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != dirid || key.type != BTRFS_DIR_INDEX_KEY)\n\t\t\tbreak;\n\n\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n\t\tif (di)\n\t\t\treturn di;\n\n\t\tpath->slots[0]++;\n\t}\n\treturn NULL;\n}\n\nstruct btrfs_dir_item *btrfs_lookup_xattr(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_path *path, u64 dir,\n\t\t\t\t\t  const char *name, u16 name_len,\n\t\t\t\t\t  int mod)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tint ins_len = mod < 0 ? -1 : 0;\n\tint cow = mod != 0;\n\n\tkey.objectid = dir;\n\tbtrfs_set_key_type(&key, BTRFS_XATTR_ITEM_KEY);\n\tkey.offset = btrfs_name_hash(name, name_len);\n\tret = btrfs_search_slot(trans, root, &key, path, ins_len, cow);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (ret > 0)\n\t\treturn NULL;\n\n\treturn btrfs_match_dir_item_name(root, path, name, name_len);\n}\n\n/*\n * helper function to look at the directory item pointed to by 'path'\n * this walks through all the entries in a dir item and finds one\n * for a specific name.\n */\nstruct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len)\n{\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr;\n\tu32 total_len;\n\tu32 cur = 0;\n\tu32 this_len;\n\tstruct extent_buffer *leaf;\n\n\tleaf = path->nodes[0];\n\tdir_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dir_item);\n\tif (verify_dir_item(root, leaf, dir_item))\n\t\treturn NULL;\n\n\ttotal_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\twhile (cur < total_len) {\n\t\tthis_len = sizeof(*dir_item) +\n\t\t\tbtrfs_dir_name_len(leaf, dir_item) +\n\t\t\tbtrfs_dir_data_len(leaf, dir_item);\n\t\tname_ptr = (unsigned long)(dir_item + 1);\n\n\t\tif (btrfs_dir_name_len(leaf, dir_item) == name_len &&\n\t\t    memcmp_extent_buffer(leaf, name, name_ptr, name_len) == 0)\n\t\t\treturn dir_item;\n\n\t\tcur += this_len;\n\t\tdir_item = (struct btrfs_dir_item *)((char *)dir_item +\n\t\t\t\t\t\t     this_len);\n\t}\n\treturn NULL;\n}\n\n/*\n * given a pointer into a directory item, delete it.  This\n * handles items that have more than one entry in them.\n */\nint btrfs_delete_one_dir_name(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      struct btrfs_dir_item *di)\n{\n\n\tstruct extent_buffer *leaf;\n\tu32 sub_item_len;\n\tu32 item_len;\n\tint ret = 0;\n\n\tleaf = path->nodes[0];\n\tsub_item_len = sizeof(*di) + btrfs_dir_name_len(leaf, di) +\n\t\tbtrfs_dir_data_len(leaf, di);\n\titem_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\tif (sub_item_len == item_len) {\n\t\tret = btrfs_del_item(trans, root, path);\n\t} else {\n\t\t/* MARKER */\n\t\tunsigned long ptr = (unsigned long)di;\n\t\tunsigned long start;\n\n\t\tstart = btrfs_item_ptr_offset(leaf, path->slots[0]);\n\t\tmemmove_extent_buffer(leaf, ptr, ptr + sub_item_len,\n\t\t\titem_len - (ptr + sub_item_len - start));\n\t\tbtrfs_truncate_item(trans, root, path,\n\t\t\t\t    item_len - sub_item_len, 1);\n\t}\n\treturn ret;\n}\n\nint verify_dir_item(struct btrfs_root *root,\n\t\t    struct extent_buffer *leaf,\n\t\t    struct btrfs_dir_item *dir_item)\n{\n\tu16 namelen = BTRFS_NAME_LEN;\n\tu8 type = btrfs_dir_type(leaf, dir_item);\n\n\tif (type >= BTRFS_FT_MAX) {\n\t\tprintk(KERN_CRIT \"btrfs: invalid dir item type: %d\\n\",\n\t\t       (int)type);\n\t\treturn 1;\n\t}\n\n\tif (type == BTRFS_FT_XATTR)\n\t\tnamelen = XATTR_NAME_MAX;\n\n\tif (btrfs_dir_name_len(leaf, dir_item) > namelen) {\n\t\tprintk(KERN_CRIT \"btrfs: invalid dir item name len: %u\\n\",\n\t\t       (unsigned)btrfs_dir_data_len(leaf, dir_item));\n\t\treturn 1;\n\t}\n\n\t/* BTRFS_MAX_XATTR_SIZE is the same for all dir items */\n\tif (btrfs_dir_data_len(leaf, dir_item) > BTRFS_MAX_XATTR_SIZE(root)) {\n\t\tprintk(KERN_CRIT \"btrfs: invalid dir item data len: %u\\n\",\n\t\t       (unsigned)btrfs_dir_data_len(leaf, dir_item));\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n", "/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#include <linux/kernel.h>\n#include <linux/bio.h>\n#include <linux/buffer_head.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/time.h>\n#include <linux/init.h>\n#include <linux/string.h>\n#include <linux/backing-dev.h>\n#include <linux/mpage.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/statfs.h>\n#include <linux/compat.h>\n#include <linux/bit_spinlock.h>\n#include <linux/xattr.h>\n#include <linux/posix_acl.h>\n#include <linux/falloc.h>\n#include <linux/slab.h>\n#include <linux/ratelimit.h>\n#include <linux/mount.h>\n#include \"compat.h\"\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"btrfs_inode.h\"\n#include \"ioctl.h\"\n#include \"print-tree.h\"\n#include \"ordered-data.h\"\n#include \"xattr.h\"\n#include \"tree-log.h\"\n#include \"volumes.h\"\n#include \"compression.h\"\n#include \"locking.h\"\n#include \"free-space-cache.h\"\n#include \"inode-map.h\"\n\nstruct btrfs_iget_args {\n\tu64 ino;\n\tstruct btrfs_root *root;\n};\n\nstatic const struct inode_operations btrfs_dir_inode_operations;\nstatic const struct inode_operations btrfs_symlink_inode_operations;\nstatic const struct inode_operations btrfs_dir_ro_inode_operations;\nstatic const struct inode_operations btrfs_special_inode_operations;\nstatic const struct inode_operations btrfs_file_inode_operations;\nstatic const struct address_space_operations btrfs_aops;\nstatic const struct address_space_operations btrfs_symlink_aops;\nstatic const struct file_operations btrfs_dir_file_operations;\nstatic struct extent_io_ops btrfs_extent_io_ops;\n\nstatic struct kmem_cache *btrfs_inode_cachep;\nstatic struct kmem_cache *btrfs_delalloc_work_cachep;\nstruct kmem_cache *btrfs_trans_handle_cachep;\nstruct kmem_cache *btrfs_transaction_cachep;\nstruct kmem_cache *btrfs_path_cachep;\nstruct kmem_cache *btrfs_free_space_cachep;\n\n#define S_SHIFT 12\nstatic unsigned char btrfs_type_by_mode[S_IFMT >> S_SHIFT] = {\n\t[S_IFREG >> S_SHIFT]\t= BTRFS_FT_REG_FILE,\n\t[S_IFDIR >> S_SHIFT]\t= BTRFS_FT_DIR,\n\t[S_IFCHR >> S_SHIFT]\t= BTRFS_FT_CHRDEV,\n\t[S_IFBLK >> S_SHIFT]\t= BTRFS_FT_BLKDEV,\n\t[S_IFIFO >> S_SHIFT]\t= BTRFS_FT_FIFO,\n\t[S_IFSOCK >> S_SHIFT]\t= BTRFS_FT_SOCK,\n\t[S_IFLNK >> S_SHIFT]\t= BTRFS_FT_SYMLINK,\n};\n\nstatic int btrfs_setsize(struct inode *inode, loff_t newsize);\nstatic int btrfs_truncate(struct inode *inode);\nstatic int btrfs_finish_ordered_io(struct btrfs_ordered_extent *ordered_extent);\nstatic noinline int cow_file_range(struct inode *inode,\n\t\t\t\t   struct page *locked_page,\n\t\t\t\t   u64 start, u64 end, int *page_started,\n\t\t\t\t   unsigned long *nr_written, int unlock);\nstatic struct extent_map *create_pinned_em(struct inode *inode, u64 start,\n\t\t\t\t\t   u64 len, u64 orig_start,\n\t\t\t\t\t   u64 block_start, u64 block_len,\n\t\t\t\t\t   u64 orig_block_len, int type);\n\nstatic int btrfs_init_inode_security(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct inode *inode,  struct inode *dir,\n\t\t\t\t     const struct qstr *qstr)\n{\n\tint err;\n\n\terr = btrfs_init_acl(trans, inode, dir);\n\tif (!err)\n\t\terr = btrfs_xattr_security_init(trans, inode, dir, qstr);\n\treturn err;\n}\n\n/*\n * this does all the hard work for inserting an inline extent into\n * the btree.  The caller should have done a btrfs_drop_extents so that\n * no overlapping inline items exist in the btree\n */\nstatic noinline int insert_inline_extent(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, struct inode *inode,\n\t\t\t\tu64 start, size_t size, size_t compressed_size,\n\t\t\t\tint compress_type,\n\t\t\t\tstruct page **compressed_pages)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct page *page = NULL;\n\tchar *kaddr;\n\tunsigned long ptr;\n\tstruct btrfs_file_extent_item *ei;\n\tint err = 0;\n\tint ret;\n\tsize_t cur_size = size;\n\tsize_t datasize;\n\tunsigned long offset;\n\n\tif (compressed_size && compressed_pages)\n\t\tcur_size = compressed_size;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->leave_spinning = 1;\n\n\tkey.objectid = btrfs_ino(inode);\n\tkey.offset = start;\n\tbtrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);\n\tdatasize = btrfs_file_extent_calc_inline_size(cur_size);\n\n\tinode_add_bytes(inode, size);\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      datasize);\n\tif (ret) {\n\t\terr = ret;\n\t\tgoto fail;\n\t}\n\tleaf = path->nodes[0];\n\tei = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t    struct btrfs_file_extent_item);\n\tbtrfs_set_file_extent_generation(leaf, ei, trans->transid);\n\tbtrfs_set_file_extent_type(leaf, ei, BTRFS_FILE_EXTENT_INLINE);\n\tbtrfs_set_file_extent_encryption(leaf, ei, 0);\n\tbtrfs_set_file_extent_other_encoding(leaf, ei, 0);\n\tbtrfs_set_file_extent_ram_bytes(leaf, ei, size);\n\tptr = btrfs_file_extent_inline_start(ei);\n\n\tif (compress_type != BTRFS_COMPRESS_NONE) {\n\t\tstruct page *cpage;\n\t\tint i = 0;\n\t\twhile (compressed_size > 0) {\n\t\t\tcpage = compressed_pages[i];\n\t\t\tcur_size = min_t(unsigned long, compressed_size,\n\t\t\t\t       PAGE_CACHE_SIZE);\n\n\t\t\tkaddr = kmap_atomic(cpage);\n\t\t\twrite_extent_buffer(leaf, kaddr, ptr, cur_size);\n\t\t\tkunmap_atomic(kaddr);\n\n\t\t\ti++;\n\t\t\tptr += cur_size;\n\t\t\tcompressed_size -= cur_size;\n\t\t}\n\t\tbtrfs_set_file_extent_compression(leaf, ei,\n\t\t\t\t\t\t  compress_type);\n\t} else {\n\t\tpage = find_get_page(inode->i_mapping,\n\t\t\t\t     start >> PAGE_CACHE_SHIFT);\n\t\tbtrfs_set_file_extent_compression(leaf, ei, 0);\n\t\tkaddr = kmap_atomic(page);\n\t\toffset = start & (PAGE_CACHE_SIZE - 1);\n\t\twrite_extent_buffer(leaf, kaddr + offset, ptr, size);\n\t\tkunmap_atomic(kaddr);\n\t\tpage_cache_release(page);\n\t}\n\tbtrfs_mark_buffer_dirty(leaf);\n\tbtrfs_free_path(path);\n\n\t/*\n\t * we're an inline extent, so nobody can\n\t * extend the file past i_size without locking\n\t * a page we already have locked.\n\t *\n\t * We must do any isize and inode updates\n\t * before we unlock the pages.  Otherwise we\n\t * could end up racing with unlink.\n\t */\n\tBTRFS_I(inode)->disk_i_size = inode->i_size;\n\tret = btrfs_update_inode(trans, root, inode);\n\n\treturn ret;\nfail:\n\tbtrfs_free_path(path);\n\treturn err;\n}\n\n\n/*\n * conditionally insert an inline extent into the file.  This\n * does the checks required to make sure the data is small enough\n * to fit as an inline extent.\n */\nstatic noinline int cow_file_range_inline(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root,\n\t\t\t\t struct inode *inode, u64 start, u64 end,\n\t\t\t\t size_t compressed_size, int compress_type,\n\t\t\t\t struct page **compressed_pages)\n{\n\tu64 isize = i_size_read(inode);\n\tu64 actual_end = min(end + 1, isize);\n\tu64 inline_len = actual_end - start;\n\tu64 aligned_end = (end + root->sectorsize - 1) &\n\t\t\t~((u64)root->sectorsize - 1);\n\tu64 data_len = inline_len;\n\tint ret;\n\n\tif (compressed_size)\n\t\tdata_len = compressed_size;\n\n\tif (start > 0 ||\n\t    actual_end >= PAGE_CACHE_SIZE ||\n\t    data_len >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||\n\t    (!compressed_size &&\n\t    (actual_end & (root->sectorsize - 1)) == 0) ||\n\t    end + 1 < isize ||\n\t    data_len > root->fs_info->max_inline) {\n\t\treturn 1;\n\t}\n\n\tret = btrfs_drop_extents(trans, root, inode, start, aligned_end, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tif (isize > actual_end)\n\t\tinline_len = min_t(u64, isize, actual_end);\n\tret = insert_inline_extent(trans, root, inode, start,\n\t\t\t\t   inline_len, compressed_size,\n\t\t\t\t   compress_type, compressed_pages);\n\tif (ret && ret != -ENOSPC) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\treturn ret;\n\t} else if (ret == -ENOSPC) {\n\t\treturn 1;\n\t}\n\n\tbtrfs_delalloc_release_metadata(inode, end + 1 - start);\n\tbtrfs_drop_extent_cache(inode, start, aligned_end - 1, 0);\n\treturn 0;\n}\n\nstruct async_extent {\n\tu64 start;\n\tu64 ram_size;\n\tu64 compressed_size;\n\tstruct page **pages;\n\tunsigned long nr_pages;\n\tint compress_type;\n\tstruct list_head list;\n};\n\nstruct async_cow {\n\tstruct inode *inode;\n\tstruct btrfs_root *root;\n\tstruct page *locked_page;\n\tu64 start;\n\tu64 end;\n\tstruct list_head extents;\n\tstruct btrfs_work work;\n};\n\nstatic noinline int add_async_extent(struct async_cow *cow,\n\t\t\t\t     u64 start, u64 ram_size,\n\t\t\t\t     u64 compressed_size,\n\t\t\t\t     struct page **pages,\n\t\t\t\t     unsigned long nr_pages,\n\t\t\t\t     int compress_type)\n{\n\tstruct async_extent *async_extent;\n\n\tasync_extent = kmalloc(sizeof(*async_extent), GFP_NOFS);\n\tBUG_ON(!async_extent); /* -ENOMEM */\n\tasync_extent->start = start;\n\tasync_extent->ram_size = ram_size;\n\tasync_extent->compressed_size = compressed_size;\n\tasync_extent->pages = pages;\n\tasync_extent->nr_pages = nr_pages;\n\tasync_extent->compress_type = compress_type;\n\tlist_add_tail(&async_extent->list, &cow->extents);\n\treturn 0;\n}\n\n/*\n * we create compressed extents in two phases.  The first\n * phase compresses a range of pages that have already been\n * locked (both pages and state bits are locked).\n *\n * This is done inside an ordered work queue, and the compression\n * is spread across many cpus.  The actual IO submission is step\n * two, and the ordered work queue takes care of making sure that\n * happens in the same order things were put onto the queue by\n * writepages and friends.\n *\n * If this code finds it can't get good compression, it puts an\n * entry onto the work queue to write the uncompressed bytes.  This\n * makes sure that both compressed inodes and uncompressed inodes\n * are written in the same order that the flusher thread sent them\n * down.\n */\nstatic noinline int compress_file_range(struct inode *inode,\n\t\t\t\t\tstruct page *locked_page,\n\t\t\t\t\tu64 start, u64 end,\n\t\t\t\t\tstruct async_cow *async_cow,\n\t\t\t\t\tint *num_added)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tu64 num_bytes;\n\tu64 blocksize = root->sectorsize;\n\tu64 actual_end;\n\tu64 isize = i_size_read(inode);\n\tint ret = 0;\n\tstruct page **pages = NULL;\n\tunsigned long nr_pages;\n\tunsigned long nr_pages_ret = 0;\n\tunsigned long total_compressed = 0;\n\tunsigned long total_in = 0;\n\tunsigned long max_compressed = 128 * 1024;\n\tunsigned long max_uncompressed = 128 * 1024;\n\tint i;\n\tint will_compress;\n\tint compress_type = root->fs_info->compress_type;\n\n\t/* if this is a small write inside eof, kick off a defrag */\n\tif ((end - start + 1) < 16 * 1024 &&\n\t    (start > 0 || end + 1 < BTRFS_I(inode)->disk_i_size))\n\t\tbtrfs_add_inode_defrag(NULL, inode);\n\n\tactual_end = min_t(u64, isize, end + 1);\nagain:\n\twill_compress = 0;\n\tnr_pages = (end >> PAGE_CACHE_SHIFT) - (start >> PAGE_CACHE_SHIFT) + 1;\n\tnr_pages = min(nr_pages, (128 * 1024UL) / PAGE_CACHE_SIZE);\n\n\t/*\n\t * we don't want to send crud past the end of i_size through\n\t * compression, that's just a waste of CPU time.  So, if the\n\t * end of the file is before the start of our current\n\t * requested range of bytes, we bail out to the uncompressed\n\t * cleanup code that can deal with all of this.\n\t *\n\t * It isn't really the fastest way to fix things, but this is a\n\t * very uncommon corner.\n\t */\n\tif (actual_end <= start)\n\t\tgoto cleanup_and_bail_uncompressed;\n\n\ttotal_compressed = actual_end - start;\n\n\t/* we want to make sure that amount of ram required to uncompress\n\t * an extent is reasonable, so we limit the total size in ram\n\t * of a compressed extent to 128k.  This is a crucial number\n\t * because it also controls how easily we can spread reads across\n\t * cpus for decompression.\n\t *\n\t * We also want to make sure the amount of IO required to do\n\t * a random read is reasonably small, so we limit the size of\n\t * a compressed extent to 128k.\n\t */\n\ttotal_compressed = min(total_compressed, max_uncompressed);\n\tnum_bytes = (end - start + blocksize) & ~(blocksize - 1);\n\tnum_bytes = max(blocksize,  num_bytes);\n\ttotal_in = 0;\n\tret = 0;\n\n\t/*\n\t * we do compression for mount -o compress and when the\n\t * inode has not been flagged as nocompress.  This flag can\n\t * change at any time if we discover bad compression ratios.\n\t */\n\tif (!(BTRFS_I(inode)->flags & BTRFS_INODE_NOCOMPRESS) &&\n\t    (btrfs_test_opt(root, COMPRESS) ||\n\t     (BTRFS_I(inode)->force_compress) ||\n\t     (BTRFS_I(inode)->flags & BTRFS_INODE_COMPRESS))) {\n\t\tWARN_ON(pages);\n\t\tpages = kzalloc(sizeof(struct page *) * nr_pages, GFP_NOFS);\n\t\tif (!pages) {\n\t\t\t/* just bail out to the uncompressed code */\n\t\t\tgoto cont;\n\t\t}\n\n\t\tif (BTRFS_I(inode)->force_compress)\n\t\t\tcompress_type = BTRFS_I(inode)->force_compress;\n\n\t\tret = btrfs_compress_pages(compress_type,\n\t\t\t\t\t   inode->i_mapping, start,\n\t\t\t\t\t   total_compressed, pages,\n\t\t\t\t\t   nr_pages, &nr_pages_ret,\n\t\t\t\t\t   &total_in,\n\t\t\t\t\t   &total_compressed,\n\t\t\t\t\t   max_compressed);\n\n\t\tif (!ret) {\n\t\t\tunsigned long offset = total_compressed &\n\t\t\t\t(PAGE_CACHE_SIZE - 1);\n\t\t\tstruct page *page = pages[nr_pages_ret - 1];\n\t\t\tchar *kaddr;\n\n\t\t\t/* zero the tail end of the last page, we might be\n\t\t\t * sending it down to disk\n\t\t\t */\n\t\t\tif (offset) {\n\t\t\t\tkaddr = kmap_atomic(page);\n\t\t\t\tmemset(kaddr + offset, 0,\n\t\t\t\t       PAGE_CACHE_SIZE - offset);\n\t\t\t\tkunmap_atomic(kaddr);\n\t\t\t}\n\t\t\twill_compress = 1;\n\t\t}\n\t}\ncont:\n\tif (start == 0) {\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto cleanup_and_out;\n\t\t}\n\t\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\t\t/* lets try to make an inline extent */\n\t\tif (ret || total_in < (actual_end - start)) {\n\t\t\t/* we didn't compress the entire range, try\n\t\t\t * to make an uncompressed inline extent.\n\t\t\t */\n\t\t\tret = cow_file_range_inline(trans, root, inode,\n\t\t\t\t\t\t    start, end, 0, 0, NULL);\n\t\t} else {\n\t\t\t/* try making a compressed inline extent */\n\t\t\tret = cow_file_range_inline(trans, root, inode,\n\t\t\t\t\t\t    start, end,\n\t\t\t\t\t\t    total_compressed,\n\t\t\t\t\t\t    compress_type, pages);\n\t\t}\n\t\tif (ret <= 0) {\n\t\t\t/*\n\t\t\t * inline extent creation worked or returned error,\n\t\t\t * we don't need to create any more async work items.\n\t\t\t * Unlock and free up our temp pages.\n\t\t\t */\n\t\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     start, end, NULL,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE | EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_SET_WRITEBACK | EXTENT_END_WRITEBACK);\n\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tgoto free_pages_out;\n\t\t}\n\t\tbtrfs_end_transaction(trans, root);\n\t}\n\n\tif (will_compress) {\n\t\t/*\n\t\t * we aren't doing an inline extent round the compressed size\n\t\t * up to a block size boundary so the allocator does sane\n\t\t * things\n\t\t */\n\t\ttotal_compressed = (total_compressed + blocksize - 1) &\n\t\t\t~(blocksize - 1);\n\n\t\t/*\n\t\t * one last check to make sure the compression is really a\n\t\t * win, compare the page count read with the blocks on disk\n\t\t */\n\t\ttotal_in = (total_in + PAGE_CACHE_SIZE - 1) &\n\t\t\t~(PAGE_CACHE_SIZE - 1);\n\t\tif (total_compressed >= total_in) {\n\t\t\twill_compress = 0;\n\t\t} else {\n\t\t\tnum_bytes = total_in;\n\t\t}\n\t}\n\tif (!will_compress && pages) {\n\t\t/*\n\t\t * the compression code ran but failed to make things smaller,\n\t\t * free any pages it allocated and our page pointer array\n\t\t */\n\t\tfor (i = 0; i < nr_pages_ret; i++) {\n\t\t\tWARN_ON(pages[i]->mapping);\n\t\t\tpage_cache_release(pages[i]);\n\t\t}\n\t\tkfree(pages);\n\t\tpages = NULL;\n\t\ttotal_compressed = 0;\n\t\tnr_pages_ret = 0;\n\n\t\t/* flag the file so we don't compress in the future */\n\t\tif (!btrfs_test_opt(root, FORCE_COMPRESS) &&\n\t\t    !(BTRFS_I(inode)->force_compress)) {\n\t\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NOCOMPRESS;\n\t\t}\n\t}\n\tif (will_compress) {\n\t\t*num_added += 1;\n\n\t\t/* the async work queues will take care of doing actual\n\t\t * allocation on disk for these compressed pages,\n\t\t * and will submit them to the elevator.\n\t\t */\n\t\tadd_async_extent(async_cow, start, num_bytes,\n\t\t\t\t total_compressed, pages, nr_pages_ret,\n\t\t\t\t compress_type);\n\n\t\tif (start + num_bytes < end) {\n\t\t\tstart += num_bytes;\n\t\t\tpages = NULL;\n\t\t\tcond_resched();\n\t\t\tgoto again;\n\t\t}\n\t} else {\ncleanup_and_bail_uncompressed:\n\t\t/*\n\t\t * No compression, but we still need to write the pages in\n\t\t * the file we've been given so far.  redirty the locked\n\t\t * page if it corresponds to our extent and set things up\n\t\t * for the async work queue to run cow_file_range to do\n\t\t * the normal delalloc dance\n\t\t */\n\t\tif (page_offset(locked_page) >= start &&\n\t\t    page_offset(locked_page) <= end) {\n\t\t\t__set_page_dirty_nobuffers(locked_page);\n\t\t\t/* unlocked later on in the async handlers */\n\t\t}\n\t\tadd_async_extent(async_cow, start, end - start + 1,\n\t\t\t\t 0, NULL, 0, BTRFS_COMPRESS_NONE);\n\t\t*num_added += 1;\n\t}\n\nout:\n\treturn ret;\n\nfree_pages_out:\n\tfor (i = 0; i < nr_pages_ret; i++) {\n\t\tWARN_ON(pages[i]->mapping);\n\t\tpage_cache_release(pages[i]);\n\t}\n\tkfree(pages);\n\n\tgoto out;\n\ncleanup_and_out:\n\textent_clear_unlock_delalloc(inode, &BTRFS_I(inode)->io_tree,\n\t\t\t\t     start, end, NULL,\n\t\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t\t     EXTENT_END_WRITEBACK);\n\tif (!trans || IS_ERR(trans))\n\t\tbtrfs_error(root->fs_info, ret, \"Failed to join transaction\");\n\telse\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\tgoto free_pages_out;\n}\n\n/*\n * phase two of compressed writeback.  This is the ordered portion\n * of the code, which only gets called in the order the work was\n * queued.  We walk all the async extents created by compress_file_range\n * and send them down to the disk.\n */\nstatic noinline int submit_compressed_extents(struct inode *inode,\n\t\t\t\t\t      struct async_cow *async_cow)\n{\n\tstruct async_extent *async_extent;\n\tu64 alloc_hint = 0;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_key ins;\n\tstruct extent_map *em;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree;\n\tint ret = 0;\n\n\tif (list_empty(&async_cow->extents))\n\t\treturn 0;\n\n\n\twhile (!list_empty(&async_cow->extents)) {\n\t\tasync_extent = list_entry(async_cow->extents.next,\n\t\t\t\t\t  struct async_extent, list);\n\t\tlist_del(&async_extent->list);\n\n\t\tio_tree = &BTRFS_I(inode)->io_tree;\n\nretry:\n\t\t/* did the compression code fall back to uncompressed IO? */\n\t\tif (!async_extent->pages) {\n\t\t\tint page_started = 0;\n\t\t\tunsigned long nr_written = 0;\n\n\t\t\tlock_extent(io_tree, async_extent->start,\n\t\t\t\t\t async_extent->start +\n\t\t\t\t\t async_extent->ram_size - 1);\n\n\t\t\t/* allocate blocks */\n\t\t\tret = cow_file_range(inode, async_cow->locked_page,\n\t\t\t\t\t     async_extent->start,\n\t\t\t\t\t     async_extent->start +\n\t\t\t\t\t     async_extent->ram_size - 1,\n\t\t\t\t\t     &page_started, &nr_written, 0);\n\n\t\t\t/* JDM XXX */\n\n\t\t\t/*\n\t\t\t * if page_started, cow_file_range inserted an\n\t\t\t * inline extent and took care of all the unlocking\n\t\t\t * and IO for us.  Otherwise, we need to submit\n\t\t\t * all those pages down to the drive.\n\t\t\t */\n\t\t\tif (!page_started && !ret)\n\t\t\t\textent_write_locked_range(io_tree,\n\t\t\t\t\t\t  inode, async_extent->start,\n\t\t\t\t\t\t  async_extent->start +\n\t\t\t\t\t\t  async_extent->ram_size - 1,\n\t\t\t\t\t\t  btrfs_get_extent,\n\t\t\t\t\t\t  WB_SYNC_ALL);\n\t\t\tkfree(async_extent);\n\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t}\n\n\t\tlock_extent(io_tree, async_extent->start,\n\t\t\t    async_extent->start + async_extent->ram_size - 1);\n\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t} else {\n\t\t\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\t\t\tret = btrfs_reserve_extent(trans, root,\n\t\t\t\t\t   async_extent->compressed_size,\n\t\t\t\t\t   async_extent->compressed_size,\n\t\t\t\t\t   0, alloc_hint, &ins, 1);\n\t\t\tif (ret && ret != -ENOSPC)\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t}\n\n\t\tif (ret) {\n\t\t\tint i;\n\t\t\tfor (i = 0; i < async_extent->nr_pages; i++) {\n\t\t\t\tWARN_ON(async_extent->pages[i]->mapping);\n\t\t\t\tpage_cache_release(async_extent->pages[i]);\n\t\t\t}\n\t\t\tkfree(async_extent->pages);\n\t\t\tasync_extent->nr_pages = 0;\n\t\t\tasync_extent->pages = NULL;\n\t\t\tunlock_extent(io_tree, async_extent->start,\n\t\t\t\t      async_extent->start +\n\t\t\t\t      async_extent->ram_size - 1);\n\t\t\tif (ret == -ENOSPC)\n\t\t\t\tgoto retry;\n\t\t\tgoto out_free; /* JDM: Requeue? */\n\t\t}\n\n\t\t/*\n\t\t * here we're doing allocation and writeback of the\n\t\t * compressed pages\n\t\t */\n\t\tbtrfs_drop_extent_cache(inode, async_extent->start,\n\t\t\t\t\tasync_extent->start +\n\t\t\t\t\tasync_extent->ram_size - 1, 0);\n\n\t\tem = alloc_extent_map();\n\t\tBUG_ON(!em); /* -ENOMEM */\n\t\tem->start = async_extent->start;\n\t\tem->len = async_extent->ram_size;\n\t\tem->orig_start = em->start;\n\n\t\tem->block_start = ins.objectid;\n\t\tem->block_len = ins.offset;\n\t\tem->orig_block_len = ins.offset;\n\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\tem->compress_type = async_extent->compress_type;\n\t\tset_bit(EXTENT_FLAG_PINNED, &em->flags);\n\t\tset_bit(EXTENT_FLAG_COMPRESSED, &em->flags);\n\t\tem->generation = -1;\n\n\t\twhile (1) {\n\t\t\twrite_lock(&em_tree->lock);\n\t\t\tret = add_extent_mapping(em_tree, em);\n\t\t\tif (!ret)\n\t\t\t\tlist_move(&em->list,\n\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\twrite_unlock(&em_tree->lock);\n\t\t\tif (ret != -EEXIST) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbtrfs_drop_extent_cache(inode, async_extent->start,\n\t\t\t\t\t\tasync_extent->start +\n\t\t\t\t\t\tasync_extent->ram_size - 1, 0);\n\t\t}\n\n\t\tret = btrfs_add_ordered_extent_compress(inode,\n\t\t\t\t\t\tasync_extent->start,\n\t\t\t\t\t\tins.objectid,\n\t\t\t\t\t\tasync_extent->ram_size,\n\t\t\t\t\t\tins.offset,\n\t\t\t\t\t\tBTRFS_ORDERED_COMPRESSED,\n\t\t\t\t\t\tasync_extent->compress_type);\n\t\tBUG_ON(ret); /* -ENOMEM */\n\n\t\t/*\n\t\t * clear dirty, set writeback and unlock the pages.\n\t\t */\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t\t&BTRFS_I(inode)->io_tree,\n\t\t\t\tasync_extent->start,\n\t\t\t\tasync_extent->start +\n\t\t\t\tasync_extent->ram_size - 1,\n\t\t\t\tNULL, EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t\tEXTENT_CLEAR_UNLOCK |\n\t\t\t\tEXTENT_CLEAR_DELALLOC |\n\t\t\t\tEXTENT_CLEAR_DIRTY | EXTENT_SET_WRITEBACK);\n\n\t\tret = btrfs_submit_compressed_write(inode,\n\t\t\t\t    async_extent->start,\n\t\t\t\t    async_extent->ram_size,\n\t\t\t\t    ins.objectid,\n\t\t\t\t    ins.offset, async_extent->pages,\n\t\t\t\t    async_extent->nr_pages);\n\n\t\tBUG_ON(ret); /* -ENOMEM */\n\t\talloc_hint = ins.objectid + ins.offset;\n\t\tkfree(async_extent);\n\t\tcond_resched();\n\t}\n\tret = 0;\nout:\n\treturn ret;\nout_free:\n\tkfree(async_extent);\n\tgoto out;\n}\n\nstatic u64 get_extent_allocation_hint(struct inode *inode, u64 start,\n\t\t\t\t      u64 num_bytes)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_map *em;\n\tu64 alloc_hint = 0;\n\n\tread_lock(&em_tree->lock);\n\tem = search_extent_mapping(em_tree, start, num_bytes);\n\tif (em) {\n\t\t/*\n\t\t * if block start isn't an actual block number then find the\n\t\t * first block in this inode and use that as a hint.  If that\n\t\t * block is also bogus then just don't worry about it.\n\t\t */\n\t\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\t\tfree_extent_map(em);\n\t\t\tem = search_extent_mapping(em_tree, 0, 0);\n\t\t\tif (em && em->block_start < EXTENT_MAP_LAST_BYTE)\n\t\t\t\talloc_hint = em->block_start;\n\t\t\tif (em)\n\t\t\t\tfree_extent_map(em);\n\t\t} else {\n\t\t\talloc_hint = em->block_start;\n\t\t\tfree_extent_map(em);\n\t\t}\n\t}\n\tread_unlock(&em_tree->lock);\n\n\treturn alloc_hint;\n}\n\n/*\n * when extent_io.c finds a delayed allocation range in the file,\n * the call backs end up in this code.  The basic idea is to\n * allocate extents on disk for the range, and create ordered data structs\n * in ram to track those extents.\n *\n * locked_page is the page that writepage had locked already.  We use\n * it to make sure we don't do extra locks or unlocks.\n *\n * *page_started is set to one if we unlock locked_page and do everything\n * required to start IO on it.  It may be clean and already done with\n * IO when we return.\n */\nstatic noinline int __cow_file_range(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t     struct page *locked_page,\n\t\t\t\t     u64 start, u64 end, int *page_started,\n\t\t\t\t     unsigned long *nr_written,\n\t\t\t\t     int unlock)\n{\n\tu64 alloc_hint = 0;\n\tu64 num_bytes;\n\tunsigned long ram_size;\n\tu64 disk_num_bytes;\n\tu64 cur_alloc_size;\n\tu64 blocksize = root->sectorsize;\n\tstruct btrfs_key ins;\n\tstruct extent_map *em;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tint ret = 0;\n\n\tBUG_ON(btrfs_is_free_space_inode(inode));\n\n\tnum_bytes = (end - start + blocksize) & ~(blocksize - 1);\n\tnum_bytes = max(blocksize,  num_bytes);\n\tdisk_num_bytes = num_bytes;\n\n\t/* if this is a small write inside eof, kick off defrag */\n\tif (num_bytes < 64 * 1024 &&\n\t    (start > 0 || end + 1 < BTRFS_I(inode)->disk_i_size))\n\t\tbtrfs_add_inode_defrag(trans, inode);\n\n\tif (start == 0) {\n\t\t/* lets try to make an inline extent */\n\t\tret = cow_file_range_inline(trans, root, inode,\n\t\t\t\t\t    start, end, 0, 0, NULL);\n\t\tif (ret == 0) {\n\t\t\textent_clear_unlock_delalloc(inode,\n\t\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t\t     start, end, NULL,\n\t\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t\t     EXTENT_END_WRITEBACK);\n\n\t\t\t*nr_written = *nr_written +\n\t\t\t     (end - start + PAGE_CACHE_SIZE) / PAGE_CACHE_SIZE;\n\t\t\t*page_started = 1;\n\t\t\tgoto out;\n\t\t} else if (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tBUG_ON(disk_num_bytes >\n\t       btrfs_super_total_bytes(root->fs_info->super_copy));\n\n\talloc_hint = get_extent_allocation_hint(inode, start, num_bytes);\n\tbtrfs_drop_extent_cache(inode, start, start + num_bytes - 1, 0);\n\n\twhile (disk_num_bytes > 0) {\n\t\tunsigned long op;\n\n\t\tcur_alloc_size = disk_num_bytes;\n\t\tret = btrfs_reserve_extent(trans, root, cur_alloc_size,\n\t\t\t\t\t   root->sectorsize, 0, alloc_hint,\n\t\t\t\t\t   &ins, 1);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tem = alloc_extent_map();\n\t\tBUG_ON(!em); /* -ENOMEM */\n\t\tem->start = start;\n\t\tem->orig_start = em->start;\n\t\tram_size = ins.offset;\n\t\tem->len = ins.offset;\n\n\t\tem->block_start = ins.objectid;\n\t\tem->block_len = ins.offset;\n\t\tem->orig_block_len = ins.offset;\n\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\tset_bit(EXTENT_FLAG_PINNED, &em->flags);\n\t\tem->generation = -1;\n\n\t\twhile (1) {\n\t\t\twrite_lock(&em_tree->lock);\n\t\t\tret = add_extent_mapping(em_tree, em);\n\t\t\tif (!ret)\n\t\t\t\tlist_move(&em->list,\n\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\twrite_unlock(&em_tree->lock);\n\t\t\tif (ret != -EEXIST) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbtrfs_drop_extent_cache(inode, start,\n\t\t\t\t\t\tstart + ram_size - 1, 0);\n\t\t}\n\n\t\tcur_alloc_size = ins.offset;\n\t\tret = btrfs_add_ordered_extent(inode, start, ins.objectid,\n\t\t\t\t\t       ram_size, cur_alloc_size, 0);\n\t\tBUG_ON(ret); /* -ENOMEM */\n\n\t\tif (root->root_key.objectid ==\n\t\t    BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tret = btrfs_reloc_clone_csums(inode, start,\n\t\t\t\t\t\t      cur_alloc_size);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\tif (disk_num_bytes < cur_alloc_size)\n\t\t\tbreak;\n\n\t\t/* we're not doing compressed IO, don't unlock the first\n\t\t * page (which the caller expects to stay locked), don't\n\t\t * clear any dirty bits and don't set any writeback bits\n\t\t *\n\t\t * Do set the Private2 bit so we know this page was properly\n\t\t * setup for writepage\n\t\t */\n\t\top = unlock ? EXTENT_CLEAR_UNLOCK_PAGE : 0;\n\t\top |= EXTENT_CLEAR_UNLOCK | EXTENT_CLEAR_DELALLOC |\n\t\t\tEXTENT_SET_PRIVATE2;\n\n\t\textent_clear_unlock_delalloc(inode, &BTRFS_I(inode)->io_tree,\n\t\t\t\t\t     start, start + ram_size - 1,\n\t\t\t\t\t     locked_page, op);\n\t\tdisk_num_bytes -= cur_alloc_size;\n\t\tnum_bytes -= cur_alloc_size;\n\t\talloc_hint = ins.objectid + ins.offset;\n\t\tstart += cur_alloc_size;\n\t}\nout:\n\treturn ret;\n\nout_unlock:\n\textent_clear_unlock_delalloc(inode,\n\t\t     &BTRFS_I(inode)->io_tree,\n\t\t     start, end, locked_page,\n\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t     EXTENT_CLEAR_DIRTY |\n\t\t     EXTENT_SET_WRITEBACK |\n\t\t     EXTENT_END_WRITEBACK);\n\n\tgoto out;\n}\n\nstatic noinline int cow_file_range(struct inode *inode,\n\t\t\t\t   struct page *locked_page,\n\t\t\t\t   u64 start, u64 end, int *page_started,\n\t\t\t\t   unsigned long *nr_written,\n\t\t\t\t   int unlock)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     start, end, locked_page,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t     EXTENT_END_WRITEBACK);\n\t\treturn PTR_ERR(trans);\n\t}\n\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\tret = __cow_file_range(trans, inode, root, locked_page, start, end,\n\t\t\t       page_started, nr_written, unlock);\n\n\tbtrfs_end_transaction(trans, root);\n\n\treturn ret;\n}\n\n/*\n * work queue call back to started compression on a file and pages\n */\nstatic noinline void async_cow_start(struct btrfs_work *work)\n{\n\tstruct async_cow *async_cow;\n\tint num_added = 0;\n\tasync_cow = container_of(work, struct async_cow, work);\n\n\tcompress_file_range(async_cow->inode, async_cow->locked_page,\n\t\t\t    async_cow->start, async_cow->end, async_cow,\n\t\t\t    &num_added);\n\tif (num_added == 0) {\n\t\tbtrfs_add_delayed_iput(async_cow->inode);\n\t\tasync_cow->inode = NULL;\n\t}\n}\n\n/*\n * work queue call back to submit previously compressed pages\n */\nstatic noinline void async_cow_submit(struct btrfs_work *work)\n{\n\tstruct async_cow *async_cow;\n\tstruct btrfs_root *root;\n\tunsigned long nr_pages;\n\n\tasync_cow = container_of(work, struct async_cow, work);\n\n\troot = async_cow->root;\n\tnr_pages = (async_cow->end - async_cow->start + PAGE_CACHE_SIZE) >>\n\t\tPAGE_CACHE_SHIFT;\n\n\tif (atomic_sub_return(nr_pages, &root->fs_info->async_delalloc_pages) <\n\t    5 * 1024 * 1024 &&\n\t    waitqueue_active(&root->fs_info->async_submit_wait))\n\t\twake_up(&root->fs_info->async_submit_wait);\n\n\tif (async_cow->inode)\n\t\tsubmit_compressed_extents(async_cow->inode, async_cow);\n}\n\nstatic noinline void async_cow_free(struct btrfs_work *work)\n{\n\tstruct async_cow *async_cow;\n\tasync_cow = container_of(work, struct async_cow, work);\n\tif (async_cow->inode)\n\t\tbtrfs_add_delayed_iput(async_cow->inode);\n\tkfree(async_cow);\n}\n\nstatic int cow_file_range_async(struct inode *inode, struct page *locked_page,\n\t\t\t\tu64 start, u64 end, int *page_started,\n\t\t\t\tunsigned long *nr_written)\n{\n\tstruct async_cow *async_cow;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tunsigned long nr_pages;\n\tu64 cur_end;\n\tint limit = 10 * 1024 * 1024;\n\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, start, end, EXTENT_LOCKED,\n\t\t\t 1, 0, NULL, GFP_NOFS);\n\twhile (start < end) {\n\t\tasync_cow = kmalloc(sizeof(*async_cow), GFP_NOFS);\n\t\tBUG_ON(!async_cow); /* -ENOMEM */\n\t\tasync_cow->inode = igrab(inode);\n\t\tasync_cow->root = root;\n\t\tasync_cow->locked_page = locked_page;\n\t\tasync_cow->start = start;\n\n\t\tif (BTRFS_I(inode)->flags & BTRFS_INODE_NOCOMPRESS)\n\t\t\tcur_end = end;\n\t\telse\n\t\t\tcur_end = min(end, start + 512 * 1024 - 1);\n\n\t\tasync_cow->end = cur_end;\n\t\tINIT_LIST_HEAD(&async_cow->extents);\n\n\t\tasync_cow->work.func = async_cow_start;\n\t\tasync_cow->work.ordered_func = async_cow_submit;\n\t\tasync_cow->work.ordered_free = async_cow_free;\n\t\tasync_cow->work.flags = 0;\n\n\t\tnr_pages = (cur_end - start + PAGE_CACHE_SIZE) >>\n\t\t\tPAGE_CACHE_SHIFT;\n\t\tatomic_add(nr_pages, &root->fs_info->async_delalloc_pages);\n\n\t\tbtrfs_queue_worker(&root->fs_info->delalloc_workers,\n\t\t\t\t   &async_cow->work);\n\n\t\tif (atomic_read(&root->fs_info->async_delalloc_pages) > limit) {\n\t\t\twait_event(root->fs_info->async_submit_wait,\n\t\t\t   (atomic_read(&root->fs_info->async_delalloc_pages) <\n\t\t\t    limit));\n\t\t}\n\n\t\twhile (atomic_read(&root->fs_info->async_submit_draining) &&\n\t\t      atomic_read(&root->fs_info->async_delalloc_pages)) {\n\t\t\twait_event(root->fs_info->async_submit_wait,\n\t\t\t  (atomic_read(&root->fs_info->async_delalloc_pages) ==\n\t\t\t   0));\n\t\t}\n\n\t\t*nr_written += nr_pages;\n\t\tstart = cur_end + 1;\n\t}\n\t*page_started = 1;\n\treturn 0;\n}\n\nstatic noinline int csum_exist_in_range(struct btrfs_root *root,\n\t\t\t\t\tu64 bytenr, u64 num_bytes)\n{\n\tint ret;\n\tstruct btrfs_ordered_sum *sums;\n\tLIST_HEAD(list);\n\n\tret = btrfs_lookup_csums_range(root->fs_info->csum_root, bytenr,\n\t\t\t\t       bytenr + num_bytes - 1, &list, 0);\n\tif (ret == 0 && list_empty(&list))\n\t\treturn 0;\n\n\twhile (!list_empty(&list)) {\n\t\tsums = list_entry(list.next, struct btrfs_ordered_sum, list);\n\t\tlist_del(&sums->list);\n\t\tkfree(sums);\n\t}\n\treturn 1;\n}\n\n/*\n * when nowcow writeback call back.  This checks for snapshots or COW copies\n * of the extents that exist in the file, and COWs the file as required.\n *\n * If no cow copies or snapshots exist, we write directly to the existing\n * blocks on disk\n */\nstatic noinline int run_delalloc_nocow(struct inode *inode,\n\t\t\t\t       struct page *locked_page,\n\t\t\t      u64 start, u64 end, int *page_started, int force,\n\t\t\t      unsigned long *nr_written)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_path *path;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_key found_key;\n\tu64 cow_start;\n\tu64 cur_offset;\n\tu64 extent_end;\n\tu64 extent_offset;\n\tu64 disk_bytenr;\n\tu64 num_bytes;\n\tu64 disk_num_bytes;\n\tint extent_type;\n\tint ret, err;\n\tint type;\n\tint nocow;\n\tint check_prev = 1;\n\tbool nolock;\n\tu64 ino = btrfs_ino(inode);\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     start, end, locked_page,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t     EXTENT_END_WRITEBACK);\n\t\treturn -ENOMEM;\n\t}\n\n\tnolock = btrfs_is_free_space_inode(inode);\n\n\tif (nolock)\n\t\ttrans = btrfs_join_transaction_nolock(root);\n\telse\n\t\ttrans = btrfs_join_transaction(root);\n\n\tif (IS_ERR(trans)) {\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     start, end, locked_page,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t     EXTENT_END_WRITEBACK);\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\tcow_start = (u64)-1;\n\tcur_offset = start;\n\twhile (1) {\n\t\tret = btrfs_lookup_file_extent(trans, root, path, ino,\n\t\t\t\t\t       cur_offset, 0);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto error;\n\t\t}\n\t\tif (ret > 0 && path->slots[0] > 0 && check_prev) {\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &found_key,\n\t\t\t\t\t      path->slots[0] - 1);\n\t\t\tif (found_key.objectid == ino &&\n\t\t\t    found_key.type == BTRFS_EXTENT_DATA_KEY)\n\t\t\t\tpath->slots[0]--;\n\t\t}\n\t\tcheck_prev = 0;\nnext_slot:\n\t\tleaf = path->nodes[0];\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\n\t\tnocow = 0;\n\t\tdisk_bytenr = 0;\n\t\tnum_bytes = 0;\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\tif (found_key.objectid > ino ||\n\t\t    found_key.type > BTRFS_EXTENT_DATA_KEY ||\n\t\t    found_key.offset > end)\n\t\t\tbreak;\n\n\t\tif (found_key.offset > cur_offset) {\n\t\t\textent_end = found_key.offset;\n\t\t\textent_type = 0;\n\t\t\tgoto out_check;\n\t\t}\n\n\t\tfi = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\textent_type = btrfs_file_extent_type(leaf, fi);\n\n\t\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\tdisk_bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);\n\t\t\textent_offset = btrfs_file_extent_offset(leaf, fi);\n\t\t\textent_end = found_key.offset +\n\t\t\t\tbtrfs_file_extent_num_bytes(leaf, fi);\n\t\t\tdisk_num_bytes =\n\t\t\t\tbtrfs_file_extent_disk_num_bytes(leaf, fi);\n\t\t\tif (extent_end <= start) {\n\t\t\t\tpath->slots[0]++;\n\t\t\t\tgoto next_slot;\n\t\t\t}\n\t\t\tif (disk_bytenr == 0)\n\t\t\t\tgoto out_check;\n\t\t\tif (btrfs_file_extent_compression(leaf, fi) ||\n\t\t\t    btrfs_file_extent_encryption(leaf, fi) ||\n\t\t\t    btrfs_file_extent_other_encoding(leaf, fi))\n\t\t\t\tgoto out_check;\n\t\t\tif (extent_type == BTRFS_FILE_EXTENT_REG && !force)\n\t\t\t\tgoto out_check;\n\t\t\tif (btrfs_extent_readonly(root, disk_bytenr))\n\t\t\t\tgoto out_check;\n\t\t\tif (btrfs_cross_ref_exist(trans, root, ino,\n\t\t\t\t\t\t  found_key.offset -\n\t\t\t\t\t\t  extent_offset, disk_bytenr))\n\t\t\t\tgoto out_check;\n\t\t\tdisk_bytenr += extent_offset;\n\t\t\tdisk_bytenr += cur_offset - found_key.offset;\n\t\t\tnum_bytes = min(end + 1, extent_end) - cur_offset;\n\t\t\t/*\n\t\t\t * force cow if csum exists in the range.\n\t\t\t * this ensure that csum for a given extent are\n\t\t\t * either valid or do not exist.\n\t\t\t */\n\t\t\tif (csum_exist_in_range(root, disk_bytenr, num_bytes))\n\t\t\t\tgoto out_check;\n\t\t\tnocow = 1;\n\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\textent_end = found_key.offset +\n\t\t\t\tbtrfs_file_extent_inline_len(leaf, fi);\n\t\t\textent_end = ALIGN(extent_end, root->sectorsize);\n\t\t} else {\n\t\t\tBUG_ON(1);\n\t\t}\nout_check:\n\t\tif (extent_end <= start) {\n\t\t\tpath->slots[0]++;\n\t\t\tgoto next_slot;\n\t\t}\n\t\tif (!nocow) {\n\t\t\tif (cow_start == (u64)-1)\n\t\t\t\tcow_start = cur_offset;\n\t\t\tcur_offset = extent_end;\n\t\t\tif (cur_offset > end)\n\t\t\t\tbreak;\n\t\t\tpath->slots[0]++;\n\t\t\tgoto next_slot;\n\t\t}\n\n\t\tbtrfs_release_path(path);\n\t\tif (cow_start != (u64)-1) {\n\t\t\tret = __cow_file_range(trans, inode, root, locked_page,\n\t\t\t\t\t       cow_start, found_key.offset - 1,\n\t\t\t\t\t       page_started, nr_written, 1);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tcow_start = (u64)-1;\n\t\t}\n\n\t\tif (extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\tstruct extent_map *em;\n\t\t\tstruct extent_map_tree *em_tree;\n\t\t\tem_tree = &BTRFS_I(inode)->extent_tree;\n\t\t\tem = alloc_extent_map();\n\t\t\tBUG_ON(!em); /* -ENOMEM */\n\t\t\tem->start = cur_offset;\n\t\t\tem->orig_start = found_key.offset - extent_offset;\n\t\t\tem->len = num_bytes;\n\t\t\tem->block_len = num_bytes;\n\t\t\tem->block_start = disk_bytenr;\n\t\t\tem->orig_block_len = disk_num_bytes;\n\t\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\t\tset_bit(EXTENT_FLAG_PINNED, &em->flags);\n\t\t\tset_bit(EXTENT_FLAG_FILLING, &em->flags);\n\t\t\tem->generation = -1;\n\t\t\twhile (1) {\n\t\t\t\twrite_lock(&em_tree->lock);\n\t\t\t\tret = add_extent_mapping(em_tree, em);\n\t\t\t\tif (!ret)\n\t\t\t\t\tlist_move(&em->list,\n\t\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\t\twrite_unlock(&em_tree->lock);\n\t\t\t\tif (ret != -EEXIST) {\n\t\t\t\t\tfree_extent_map(em);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbtrfs_drop_extent_cache(inode, em->start,\n\t\t\t\t\t\tem->start + em->len - 1, 0);\n\t\t\t}\n\t\t\ttype = BTRFS_ORDERED_PREALLOC;\n\t\t} else {\n\t\t\ttype = BTRFS_ORDERED_NOCOW;\n\t\t}\n\n\t\tret = btrfs_add_ordered_extent(inode, cur_offset, disk_bytenr,\n\t\t\t\t\t       num_bytes, num_bytes, type);\n\t\tBUG_ON(ret); /* -ENOMEM */\n\n\t\tif (root->root_key.objectid ==\n\t\t    BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tret = btrfs_reloc_clone_csums(inode, cur_offset,\n\t\t\t\t\t\t      num_bytes);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\n\t\textent_clear_unlock_delalloc(inode, &BTRFS_I(inode)->io_tree,\n\t\t\t\tcur_offset, cur_offset + num_bytes - 1,\n\t\t\t\tlocked_page, EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t\tEXTENT_CLEAR_UNLOCK | EXTENT_CLEAR_DELALLOC |\n\t\t\t\tEXTENT_SET_PRIVATE2);\n\t\tcur_offset = extent_end;\n\t\tif (cur_offset > end)\n\t\t\tbreak;\n\t}\n\tbtrfs_release_path(path);\n\n\tif (cur_offset <= end && cow_start == (u64)-1) {\n\t\tcow_start = cur_offset;\n\t\tcur_offset = end;\n\t}\n\n\tif (cow_start != (u64)-1) {\n\t\tret = __cow_file_range(trans, inode, root, locked_page,\n\t\t\t\t       cow_start, end,\n\t\t\t\t       page_started, nr_written, 1);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto error;\n\t\t}\n\t}\n\nerror:\n\terr = btrfs_end_transaction(trans, root);\n\tif (!ret)\n\t\tret = err;\n\n\tif (ret && cur_offset < end)\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     cur_offset, end, locked_page,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t     EXTENT_END_WRITEBACK);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * extent_io.c call back to do delayed allocation processing\n */\nstatic int run_delalloc_range(struct inode *inode, struct page *locked_page,\n\t\t\t      u64 start, u64 end, int *page_started,\n\t\t\t      unsigned long *nr_written)\n{\n\tint ret;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\tif (BTRFS_I(inode)->flags & BTRFS_INODE_NODATACOW) {\n\t\tret = run_delalloc_nocow(inode, locked_page, start, end,\n\t\t\t\t\t page_started, 1, nr_written);\n\t} else if (BTRFS_I(inode)->flags & BTRFS_INODE_PREALLOC) {\n\t\tret = run_delalloc_nocow(inode, locked_page, start, end,\n\t\t\t\t\t page_started, 0, nr_written);\n\t} else if (!btrfs_test_opt(root, COMPRESS) &&\n\t\t   !(BTRFS_I(inode)->force_compress) &&\n\t\t   !(BTRFS_I(inode)->flags & BTRFS_INODE_COMPRESS)) {\n\t\tret = cow_file_range(inode, locked_page, start, end,\n\t\t\t\t      page_started, nr_written, 1);\n\t} else {\n\t\tset_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,\n\t\t\t&BTRFS_I(inode)->runtime_flags);\n\t\tret = cow_file_range_async(inode, locked_page, start, end,\n\t\t\t\t\t   page_started, nr_written);\n\t}\n\treturn ret;\n}\n\nstatic void btrfs_split_extent_hook(struct inode *inode,\n\t\t\t\t    struct extent_state *orig, u64 split)\n{\n\t/* not delalloc, ignore it */\n\tif (!(orig->state & EXTENT_DELALLOC))\n\t\treturn;\n\n\tspin_lock(&BTRFS_I(inode)->lock);\n\tBTRFS_I(inode)->outstanding_extents++;\n\tspin_unlock(&BTRFS_I(inode)->lock);\n}\n\n/*\n * extent_io.c merge_extent_hook, used to track merged delayed allocation\n * extents so we can keep track of new extents that are just merged onto old\n * extents, such as when we are doing sequential writes, so we can properly\n * account for the metadata space we'll need.\n */\nstatic void btrfs_merge_extent_hook(struct inode *inode,\n\t\t\t\t    struct extent_state *new,\n\t\t\t\t    struct extent_state *other)\n{\n\t/* not delalloc, ignore it */\n\tif (!(other->state & EXTENT_DELALLOC))\n\t\treturn;\n\n\tspin_lock(&BTRFS_I(inode)->lock);\n\tBTRFS_I(inode)->outstanding_extents--;\n\tspin_unlock(&BTRFS_I(inode)->lock);\n}\n\n/*\n * extent_io.c set_bit_hook, used to track delayed allocation\n * bytes in this file, and to maintain the list of inodes that\n * have pending delalloc work to be done.\n */\nstatic void btrfs_set_bit_hook(struct inode *inode,\n\t\t\t       struct extent_state *state, int *bits)\n{\n\n\t/*\n\t * set_bit and clear bit hooks normally require _irqsave/restore\n\t * but in this case, we are only testing for the DELALLOC\n\t * bit, which is only set or cleared with irqs on\n\t */\n\tif (!(state->state & EXTENT_DELALLOC) && (*bits & EXTENT_DELALLOC)) {\n\t\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\t\tu64 len = state->end + 1 - state->start;\n\t\tbool do_list = !btrfs_is_free_space_inode(inode);\n\n\t\tif (*bits & EXTENT_FIRST_DELALLOC) {\n\t\t\t*bits &= ~EXTENT_FIRST_DELALLOC;\n\t\t} else {\n\t\t\tspin_lock(&BTRFS_I(inode)->lock);\n\t\t\tBTRFS_I(inode)->outstanding_extents++;\n\t\t\tspin_unlock(&BTRFS_I(inode)->lock);\n\t\t}\n\n\t\tspin_lock(&root->fs_info->delalloc_lock);\n\t\tBTRFS_I(inode)->delalloc_bytes += len;\n\t\troot->fs_info->delalloc_bytes += len;\n\t\tif (do_list && list_empty(&BTRFS_I(inode)->delalloc_inodes)) {\n\t\t\tlist_add_tail(&BTRFS_I(inode)->delalloc_inodes,\n\t\t\t\t      &root->fs_info->delalloc_inodes);\n\t\t}\n\t\tspin_unlock(&root->fs_info->delalloc_lock);\n\t}\n}\n\n/*\n * extent_io.c clear_bit_hook, see set_bit_hook for why\n */\nstatic void btrfs_clear_bit_hook(struct inode *inode,\n\t\t\t\t struct extent_state *state, int *bits)\n{\n\t/*\n\t * set_bit and clear bit hooks normally require _irqsave/restore\n\t * but in this case, we are only testing for the DELALLOC\n\t * bit, which is only set or cleared with irqs on\n\t */\n\tif ((state->state & EXTENT_DELALLOC) && (*bits & EXTENT_DELALLOC)) {\n\t\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\t\tu64 len = state->end + 1 - state->start;\n\t\tbool do_list = !btrfs_is_free_space_inode(inode);\n\n\t\tif (*bits & EXTENT_FIRST_DELALLOC) {\n\t\t\t*bits &= ~EXTENT_FIRST_DELALLOC;\n\t\t} else if (!(*bits & EXTENT_DO_ACCOUNTING)) {\n\t\t\tspin_lock(&BTRFS_I(inode)->lock);\n\t\t\tBTRFS_I(inode)->outstanding_extents--;\n\t\t\tspin_unlock(&BTRFS_I(inode)->lock);\n\t\t}\n\n\t\tif (*bits & EXTENT_DO_ACCOUNTING)\n\t\t\tbtrfs_delalloc_release_metadata(inode, len);\n\n\t\tif (root->root_key.objectid != BTRFS_DATA_RELOC_TREE_OBJECTID\n\t\t    && do_list)\n\t\t\tbtrfs_free_reserved_data_space(inode, len);\n\n\t\tspin_lock(&root->fs_info->delalloc_lock);\n\t\troot->fs_info->delalloc_bytes -= len;\n\t\tBTRFS_I(inode)->delalloc_bytes -= len;\n\n\t\tif (do_list && BTRFS_I(inode)->delalloc_bytes == 0 &&\n\t\t    !list_empty(&BTRFS_I(inode)->delalloc_inodes)) {\n\t\t\tlist_del_init(&BTRFS_I(inode)->delalloc_inodes);\n\t\t}\n\t\tspin_unlock(&root->fs_info->delalloc_lock);\n\t}\n}\n\n/*\n * extent_io.c merge_bio_hook, this must check the chunk tree to make sure\n * we don't create bios that span stripes or chunks\n */\nint btrfs_merge_bio_hook(struct page *page, unsigned long offset,\n\t\t\t size_t size, struct bio *bio,\n\t\t\t unsigned long bio_flags)\n{\n\tstruct btrfs_root *root = BTRFS_I(page->mapping->host)->root;\n\tu64 logical = (u64)bio->bi_sector << 9;\n\tu64 length = 0;\n\tu64 map_length;\n\tint ret;\n\n\tif (bio_flags & EXTENT_BIO_COMPRESSED)\n\t\treturn 0;\n\n\tlength = bio->bi_size;\n\tmap_length = length;\n\tret = btrfs_map_block(root->fs_info, READ, logical,\n\t\t\t      &map_length, NULL, 0);\n\t/* Will always return 0 with map_multi == NULL */\n\tBUG_ON(ret < 0);\n\tif (map_length < length + size)\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * in order to insert checksums into the metadata in large chunks,\n * we wait until bio submission time.   All the pages in the bio are\n * checksummed and sums are attached onto the ordered extent record.\n *\n * At IO completion time the cums attached on the ordered extent record\n * are inserted into the btree\n */\nstatic int __btrfs_submit_bio_start(struct inode *inode, int rw,\n\t\t\t\t    struct bio *bio, int mirror_num,\n\t\t\t\t    unsigned long bio_flags,\n\t\t\t\t    u64 bio_offset)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret = 0;\n\n\tret = btrfs_csum_one_bio(root, inode, bio, 0, 0);\n\tBUG_ON(ret); /* -ENOMEM */\n\treturn 0;\n}\n\n/*\n * in order to insert checksums into the metadata in large chunks,\n * we wait until bio submission time.   All the pages in the bio are\n * checksummed and sums are attached onto the ordered extent record.\n *\n * At IO completion time the cums attached on the ordered extent record\n * are inserted into the btree\n */\nstatic int __btrfs_submit_bio_done(struct inode *inode, int rw, struct bio *bio,\n\t\t\t  int mirror_num, unsigned long bio_flags,\n\t\t\t  u64 bio_offset)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\tret = btrfs_map_bio(root, rw, bio, mirror_num, 1);\n\tif (ret)\n\t\tbio_endio(bio, ret);\n\treturn ret;\n}\n\n/*\n * extent_io.c submission hook. This does the right thing for csum calculation\n * on write, or reading the csums from the tree before a read\n */\nstatic int btrfs_submit_bio_hook(struct inode *inode, int rw, struct bio *bio,\n\t\t\t  int mirror_num, unsigned long bio_flags,\n\t\t\t  u64 bio_offset)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret = 0;\n\tint skip_sum;\n\tint metadata = 0;\n\tint async = !atomic_read(&BTRFS_I(inode)->sync_writers);\n\n\tskip_sum = BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM;\n\n\tif (btrfs_is_free_space_inode(inode))\n\t\tmetadata = 2;\n\n\tif (!(rw & REQ_WRITE)) {\n\t\tret = btrfs_bio_wq_end_io(root->fs_info, bio, metadata);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (bio_flags & EXTENT_BIO_COMPRESSED) {\n\t\t\tret = btrfs_submit_compressed_read(inode, bio,\n\t\t\t\t\t\t\t   mirror_num,\n\t\t\t\t\t\t\t   bio_flags);\n\t\t\tgoto out;\n\t\t} else if (!skip_sum) {\n\t\t\tret = btrfs_lookup_bio_sums(root, inode, bio, NULL);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tgoto mapit;\n\t} else if (async && !skip_sum) {\n\t\t/* csum items have already been cloned */\n\t\tif (root->root_key.objectid == BTRFS_DATA_RELOC_TREE_OBJECTID)\n\t\t\tgoto mapit;\n\t\t/* we're doing a write, do the async checksumming */\n\t\tret = btrfs_wq_submit_bio(BTRFS_I(inode)->root->fs_info,\n\t\t\t\t   inode, rw, bio, mirror_num,\n\t\t\t\t   bio_flags, bio_offset,\n\t\t\t\t   __btrfs_submit_bio_start,\n\t\t\t\t   __btrfs_submit_bio_done);\n\t\tgoto out;\n\t} else if (!skip_sum) {\n\t\tret = btrfs_csum_one_bio(root, inode, bio, 0, 0);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nmapit:\n\tret = btrfs_map_bio(root, rw, bio, mirror_num, 0);\n\nout:\n\tif (ret < 0)\n\t\tbio_endio(bio, ret);\n\treturn ret;\n}\n\n/*\n * given a list of ordered sums record them in the inode.  This happens\n * at IO completion time based on sums calculated at bio submission time.\n */\nstatic noinline int add_pending_csums(struct btrfs_trans_handle *trans,\n\t\t\t     struct inode *inode, u64 file_offset,\n\t\t\t     struct list_head *list)\n{\n\tstruct btrfs_ordered_sum *sum;\n\n\tlist_for_each_entry(sum, list, list) {\n\t\tbtrfs_csum_file_blocks(trans,\n\t\t       BTRFS_I(inode)->root->fs_info->csum_root, sum);\n\t}\n\treturn 0;\n}\n\nint btrfs_set_extent_delalloc(struct inode *inode, u64 start, u64 end,\n\t\t\t      struct extent_state **cached_state)\n{\n\tWARN_ON((end & (PAGE_CACHE_SIZE - 1)) == 0);\n\treturn set_extent_delalloc(&BTRFS_I(inode)->io_tree, start, end,\n\t\t\t\t   cached_state, GFP_NOFS);\n}\n\n/* see btrfs_writepage_start_hook for details on why this is required */\nstruct btrfs_writepage_fixup {\n\tstruct page *page;\n\tstruct btrfs_work work;\n};\n\nstatic void btrfs_writepage_fixup_worker(struct btrfs_work *work)\n{\n\tstruct btrfs_writepage_fixup *fixup;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tstruct page *page;\n\tstruct inode *inode;\n\tu64 page_start;\n\tu64 page_end;\n\tint ret;\n\n\tfixup = container_of(work, struct btrfs_writepage_fixup, work);\n\tpage = fixup->page;\nagain:\n\tlock_page(page);\n\tif (!page->mapping || !PageDirty(page) || !PageChecked(page)) {\n\t\tClearPageChecked(page);\n\t\tgoto out_page;\n\t}\n\n\tinode = page->mapping->host;\n\tpage_start = page_offset(page);\n\tpage_end = page_offset(page) + PAGE_CACHE_SIZE - 1;\n\n\tlock_extent_bits(&BTRFS_I(inode)->io_tree, page_start, page_end, 0,\n\t\t\t &cached_state);\n\n\t/* already ordered? We're done */\n\tif (PagePrivate2(page))\n\t\tgoto out;\n\n\tordered = btrfs_lookup_ordered_extent(inode, page_start);\n\tif (ordered) {\n\t\tunlock_extent_cached(&BTRFS_I(inode)->io_tree, page_start,\n\t\t\t\t     page_end, &cached_state, GFP_NOFS);\n\t\tunlock_page(page);\n\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tgoto again;\n\t}\n\n\tret = btrfs_delalloc_reserve_space(inode, PAGE_CACHE_SIZE);\n\tif (ret) {\n\t\tmapping_set_error(page->mapping, ret);\n\t\tend_extent_writepage(page, ret, page_start, page_end);\n\t\tClearPageChecked(page);\n\t\tgoto out;\n\t }\n\n\tbtrfs_set_extent_delalloc(inode, page_start, page_end, &cached_state);\n\tClearPageChecked(page);\n\tset_page_dirty(page);\nout:\n\tunlock_extent_cached(&BTRFS_I(inode)->io_tree, page_start, page_end,\n\t\t\t     &cached_state, GFP_NOFS);\nout_page:\n\tunlock_page(page);\n\tpage_cache_release(page);\n\tkfree(fixup);\n}\n\n/*\n * There are a few paths in the higher layers of the kernel that directly\n * set the page dirty bit without asking the filesystem if it is a\n * good idea.  This causes problems because we want to make sure COW\n * properly happens and the data=ordered rules are followed.\n *\n * In our case any range that doesn't have the ORDERED bit set\n * hasn't been properly setup for IO.  We kick off an async process\n * to fix it up.  The async helper will wait for ordered extents, set\n * the delalloc bit and make it safe to write the page.\n */\nstatic int btrfs_writepage_start_hook(struct page *page, u64 start, u64 end)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct btrfs_writepage_fixup *fixup;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\t/* this page is properly in the ordered list */\n\tif (TestClearPagePrivate2(page))\n\t\treturn 0;\n\n\tif (PageChecked(page))\n\t\treturn -EAGAIN;\n\n\tfixup = kzalloc(sizeof(*fixup), GFP_NOFS);\n\tif (!fixup)\n\t\treturn -EAGAIN;\n\n\tSetPageChecked(page);\n\tpage_cache_get(page);\n\tfixup->work.func = btrfs_writepage_fixup_worker;\n\tfixup->page = page;\n\tbtrfs_queue_worker(&root->fs_info->fixup_workers, &fixup->work);\n\treturn -EBUSY;\n}\n\nstatic int insert_reserved_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t       struct inode *inode, u64 file_pos,\n\t\t\t\t       u64 disk_bytenr, u64 disk_num_bytes,\n\t\t\t\t       u64 num_bytes, u64 ram_bytes,\n\t\t\t\t       u8 compression, u8 encryption,\n\t\t\t\t       u16 other_encoding, int extent_type)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key ins;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->leave_spinning = 1;\n\n\t/*\n\t * we may be replacing one extent in the tree with another.\n\t * The new extent is pinned in the extent map, and we don't want\n\t * to drop it from the cache until it is completely in the btree.\n\t *\n\t * So, tell btrfs_drop_extents to leave this extent in the cache.\n\t * the caller is expected to unpin it and allow it to be merged\n\t * with the others.\n\t */\n\tret = btrfs_drop_extents(trans, root, inode, file_pos,\n\t\t\t\t file_pos + num_bytes, 0);\n\tif (ret)\n\t\tgoto out;\n\n\tins.objectid = btrfs_ino(inode);\n\tins.offset = file_pos;\n\tins.type = BTRFS_EXTENT_DATA_KEY;\n\tret = btrfs_insert_empty_item(trans, root, path, &ins, sizeof(*fi));\n\tif (ret)\n\t\tgoto out;\n\tleaf = path->nodes[0];\n\tfi = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t    struct btrfs_file_extent_item);\n\tbtrfs_set_file_extent_generation(leaf, fi, trans->transid);\n\tbtrfs_set_file_extent_type(leaf, fi, extent_type);\n\tbtrfs_set_file_extent_disk_bytenr(leaf, fi, disk_bytenr);\n\tbtrfs_set_file_extent_disk_num_bytes(leaf, fi, disk_num_bytes);\n\tbtrfs_set_file_extent_offset(leaf, fi, 0);\n\tbtrfs_set_file_extent_num_bytes(leaf, fi, num_bytes);\n\tbtrfs_set_file_extent_ram_bytes(leaf, fi, ram_bytes);\n\tbtrfs_set_file_extent_compression(leaf, fi, compression);\n\tbtrfs_set_file_extent_encryption(leaf, fi, encryption);\n\tbtrfs_set_file_extent_other_encoding(leaf, fi, other_encoding);\n\n\tbtrfs_mark_buffer_dirty(leaf);\n\tbtrfs_release_path(path);\n\n\tinode_add_bytes(inode, num_bytes);\n\n\tins.objectid = disk_bytenr;\n\tins.offset = disk_num_bytes;\n\tins.type = BTRFS_EXTENT_ITEM_KEY;\n\tret = btrfs_alloc_reserved_file_extent(trans, root,\n\t\t\t\t\troot->root_key.objectid,\n\t\t\t\t\tbtrfs_ino(inode), file_pos, &ins);\nout:\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\n/*\n * helper function for btrfs_finish_ordered_io, this\n * just reads in some of the csum leaves to prime them into ram\n * before we start the transaction.  It limits the amount of btree\n * reads required while inside the transaction.\n */\n/* as ordered data IO finishes, this gets called so we can finish\n * an ordered extent if the range of bytes in the file it covers are\n * fully written.\n */\nstatic int btrfs_finish_ordered_io(struct btrfs_ordered_extent *ordered_extent)\n{\n\tstruct inode *inode = ordered_extent->inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_state *cached_state = NULL;\n\tint compress_type = 0;\n\tint ret;\n\tbool nolock;\n\n\tnolock = btrfs_is_free_space_inode(inode);\n\n\tif (test_bit(BTRFS_ORDERED_IOERR, &ordered_extent->flags)) {\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_ORDERED_NOCOW, &ordered_extent->flags)) {\n\t\tBUG_ON(!list_empty(&ordered_extent->list)); /* Logic error */\n\t\tbtrfs_ordered_update_i_size(inode, 0, ordered_extent);\n\t\tif (nolock)\n\t\t\ttrans = btrfs_join_transaction_nolock(root);\n\t\telse\n\t\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\t\tret = btrfs_update_inode_fallback(trans, root, inode);\n\t\tif (ret) /* -ENOMEM or corruption */\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out;\n\t}\n\n\tlock_extent_bits(io_tree, ordered_extent->file_offset,\n\t\t\t ordered_extent->file_offset + ordered_extent->len - 1,\n\t\t\t 0, &cached_state);\n\n\tif (nolock)\n\t\ttrans = btrfs_join_transaction_nolock(root);\n\telse\n\t\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\ttrans = NULL;\n\t\tgoto out_unlock;\n\t}\n\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\tif (test_bit(BTRFS_ORDERED_COMPRESSED, &ordered_extent->flags))\n\t\tcompress_type = ordered_extent->compress_type;\n\tif (test_bit(BTRFS_ORDERED_PREALLOC, &ordered_extent->flags)) {\n\t\tBUG_ON(compress_type);\n\t\tret = btrfs_mark_extent_written(trans, inode,\n\t\t\t\t\t\tordered_extent->file_offset,\n\t\t\t\t\t\tordered_extent->file_offset +\n\t\t\t\t\t\tordered_extent->len);\n\t} else {\n\t\tBUG_ON(root == root->fs_info->tree_root);\n\t\tret = insert_reserved_file_extent(trans, inode,\n\t\t\t\t\t\tordered_extent->file_offset,\n\t\t\t\t\t\tordered_extent->start,\n\t\t\t\t\t\tordered_extent->disk_len,\n\t\t\t\t\t\tordered_extent->len,\n\t\t\t\t\t\tordered_extent->len,\n\t\t\t\t\t\tcompress_type, 0, 0,\n\t\t\t\t\t\tBTRFS_FILE_EXTENT_REG);\n\t}\n\tunpin_extent_cache(&BTRFS_I(inode)->extent_tree,\n\t\t\t   ordered_extent->file_offset, ordered_extent->len,\n\t\t\t   trans->transid);\n\tif (ret < 0) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_unlock;\n\t}\n\n\tadd_pending_csums(trans, inode, ordered_extent->file_offset,\n\t\t\t  &ordered_extent->list);\n\n\tbtrfs_ordered_update_i_size(inode, 0, ordered_extent);\n\tret = btrfs_update_inode_fallback(trans, root, inode);\n\tif (ret) { /* -ENOMEM or corruption */\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_unlock;\n\t}\n\tret = 0;\nout_unlock:\n\tunlock_extent_cached(io_tree, ordered_extent->file_offset,\n\t\t\t     ordered_extent->file_offset +\n\t\t\t     ordered_extent->len - 1, &cached_state, GFP_NOFS);\nout:\n\tif (root != root->fs_info->tree_root)\n\t\tbtrfs_delalloc_release_metadata(inode, ordered_extent->len);\n\tif (trans)\n\t\tbtrfs_end_transaction(trans, root);\n\n\tif (ret)\n\t\tclear_extent_uptodate(io_tree, ordered_extent->file_offset,\n\t\t\t\t      ordered_extent->file_offset +\n\t\t\t\t      ordered_extent->len - 1, NULL, GFP_NOFS);\n\n\t/*\n\t * This needs to be done to make sure anybody waiting knows we are done\n\t * updating everything for this ordered extent.\n\t */\n\tbtrfs_remove_ordered_extent(inode, ordered_extent);\n\n\t/* once for us */\n\tbtrfs_put_ordered_extent(ordered_extent);\n\t/* once for the tree */\n\tbtrfs_put_ordered_extent(ordered_extent);\n\n\treturn ret;\n}\n\nstatic void finish_ordered_fn(struct btrfs_work *work)\n{\n\tstruct btrfs_ordered_extent *ordered_extent;\n\tordered_extent = container_of(work, struct btrfs_ordered_extent, work);\n\tbtrfs_finish_ordered_io(ordered_extent);\n}\n\nstatic int btrfs_writepage_end_io_hook(struct page *page, u64 start, u64 end,\n\t\t\t\tstruct extent_state *state, int uptodate)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ordered_extent *ordered_extent = NULL;\n\tstruct btrfs_workers *workers;\n\n\ttrace_btrfs_writepage_end_io_hook(page, start, end, uptodate);\n\n\tClearPagePrivate2(page);\n\tif (!btrfs_dec_test_ordered_pending(inode, &ordered_extent, start,\n\t\t\t\t\t    end - start + 1, uptodate))\n\t\treturn 0;\n\n\tordered_extent->work.func = finish_ordered_fn;\n\tordered_extent->work.flags = 0;\n\n\tif (btrfs_is_free_space_inode(inode))\n\t\tworkers = &root->fs_info->endio_freespace_worker;\n\telse\n\t\tworkers = &root->fs_info->endio_write_workers;\n\tbtrfs_queue_worker(workers, &ordered_extent->work);\n\n\treturn 0;\n}\n\n/*\n * when reads are done, we need to check csums to verify the data is correct\n * if there's a match, we allow the bio to finish.  If not, the code in\n * extent_io.c will try to find good copies for us.\n */\nstatic int btrfs_readpage_end_io_hook(struct page *page, u64 start, u64 end,\n\t\t\t       struct extent_state *state, int mirror)\n{\n\tsize_t offset = start - ((u64)page->index << PAGE_CACHE_SHIFT);\n\tstruct inode *inode = page->mapping->host;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tchar *kaddr;\n\tu64 private = ~(u32)0;\n\tint ret;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tu32 csum = ~(u32)0;\n\n\tif (PageChecked(page)) {\n\t\tClearPageChecked(page);\n\t\tgoto good;\n\t}\n\n\tif (BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM)\n\t\tgoto good;\n\n\tif (root->root_key.objectid == BTRFS_DATA_RELOC_TREE_OBJECTID &&\n\t    test_range_bit(io_tree, start, end, EXTENT_NODATASUM, 1, NULL)) {\n\t\tclear_extent_bits(io_tree, start, end, EXTENT_NODATASUM,\n\t\t\t\t  GFP_NOFS);\n\t\treturn 0;\n\t}\n\n\tif (state && state->start == start) {\n\t\tprivate = state->private;\n\t\tret = 0;\n\t} else {\n\t\tret = get_state_private(io_tree, start, &private);\n\t}\n\tkaddr = kmap_atomic(page);\n\tif (ret)\n\t\tgoto zeroit;\n\n\tcsum = btrfs_csum_data(root, kaddr + offset, csum,  end - start + 1);\n\tbtrfs_csum_final(csum, (char *)&csum);\n\tif (csum != private)\n\t\tgoto zeroit;\n\n\tkunmap_atomic(kaddr);\ngood:\n\treturn 0;\n\nzeroit:\n\tprintk_ratelimited(KERN_INFO \"btrfs csum failed ino %llu off %llu csum %u \"\n\t\t       \"private %llu\\n\",\n\t\t       (unsigned long long)btrfs_ino(page->mapping->host),\n\t\t       (unsigned long long)start, csum,\n\t\t       (unsigned long long)private);\n\tmemset(kaddr + offset, 1, end - start + 1);\n\tflush_dcache_page(page);\n\tkunmap_atomic(kaddr);\n\tif (private == 0)\n\t\treturn 0;\n\treturn -EIO;\n}\n\nstruct delayed_iput {\n\tstruct list_head list;\n\tstruct inode *inode;\n};\n\n/* JDM: If this is fs-wide, why can't we add a pointer to\n * btrfs_inode instead and avoid the allocation? */\nvoid btrfs_add_delayed_iput(struct inode *inode)\n{\n\tstruct btrfs_fs_info *fs_info = BTRFS_I(inode)->root->fs_info;\n\tstruct delayed_iput *delayed;\n\n\tif (atomic_add_unless(&inode->i_count, -1, 1))\n\t\treturn;\n\n\tdelayed = kmalloc(sizeof(*delayed), GFP_NOFS | __GFP_NOFAIL);\n\tdelayed->inode = inode;\n\n\tspin_lock(&fs_info->delayed_iput_lock);\n\tlist_add_tail(&delayed->list, &fs_info->delayed_iputs);\n\tspin_unlock(&fs_info->delayed_iput_lock);\n}\n\nvoid btrfs_run_delayed_iputs(struct btrfs_root *root)\n{\n\tLIST_HEAD(list);\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct delayed_iput *delayed;\n\tint empty;\n\n\tspin_lock(&fs_info->delayed_iput_lock);\n\tempty = list_empty(&fs_info->delayed_iputs);\n\tspin_unlock(&fs_info->delayed_iput_lock);\n\tif (empty)\n\t\treturn;\n\n\tspin_lock(&fs_info->delayed_iput_lock);\n\tlist_splice_init(&fs_info->delayed_iputs, &list);\n\tspin_unlock(&fs_info->delayed_iput_lock);\n\n\twhile (!list_empty(&list)) {\n\t\tdelayed = list_entry(list.next, struct delayed_iput, list);\n\t\tlist_del(&delayed->list);\n\t\tiput(delayed->inode);\n\t\tkfree(delayed);\n\t}\n}\n\nenum btrfs_orphan_cleanup_state {\n\tORPHAN_CLEANUP_STARTED\t= 1,\n\tORPHAN_CLEANUP_DONE\t= 2,\n};\n\n/*\n * This is called in transaction commit time. If there are no orphan\n * files in the subvolume, it removes orphan item and frees block_rsv\n * structure.\n */\nvoid btrfs_orphan_commit_root(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root)\n{\n\tstruct btrfs_block_rsv *block_rsv;\n\tint ret;\n\n\tif (atomic_read(&root->orphan_inodes) ||\n\t    root->orphan_cleanup_state != ORPHAN_CLEANUP_DONE)\n\t\treturn;\n\n\tspin_lock(&root->orphan_lock);\n\tif (atomic_read(&root->orphan_inodes)) {\n\t\tspin_unlock(&root->orphan_lock);\n\t\treturn;\n\t}\n\n\tif (root->orphan_cleanup_state != ORPHAN_CLEANUP_DONE) {\n\t\tspin_unlock(&root->orphan_lock);\n\t\treturn;\n\t}\n\n\tblock_rsv = root->orphan_block_rsv;\n\troot->orphan_block_rsv = NULL;\n\tspin_unlock(&root->orphan_lock);\n\n\tif (root->orphan_item_inserted &&\n\t    btrfs_root_refs(&root->root_item) > 0) {\n\t\tret = btrfs_del_orphan_item(trans, root->fs_info->tree_root,\n\t\t\t\t\t    root->root_key.objectid);\n\t\tBUG_ON(ret);\n\t\troot->orphan_item_inserted = 0;\n\t}\n\n\tif (block_rsv) {\n\t\tWARN_ON(block_rsv->size > 0);\n\t\tbtrfs_free_block_rsv(root, block_rsv);\n\t}\n}\n\n/*\n * This creates an orphan entry for the given inode in case something goes\n * wrong in the middle of an unlink/truncate.\n *\n * NOTE: caller of this function should reserve 5 units of metadata for\n *\t this function.\n */\nint btrfs_orphan_add(struct btrfs_trans_handle *trans, struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_block_rsv *block_rsv = NULL;\n\tint reserve = 0;\n\tint insert = 0;\n\tint ret;\n\n\tif (!root->orphan_block_rsv) {\n\t\tblock_rsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);\n\t\tif (!block_rsv)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&root->orphan_lock);\n\tif (!root->orphan_block_rsv) {\n\t\troot->orphan_block_rsv = block_rsv;\n\t} else if (block_rsv) {\n\t\tbtrfs_free_block_rsv(root, block_rsv);\n\t\tblock_rsv = NULL;\n\t}\n\n\tif (!test_and_set_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t      &BTRFS_I(inode)->runtime_flags)) {\n#if 0\n\t\t/*\n\t\t * For proper ENOSPC handling, we should do orphan\n\t\t * cleanup when mounting. But this introduces backward\n\t\t * compatibility issue.\n\t\t */\n\t\tif (!xchg(&root->orphan_item_inserted, 1))\n\t\t\tinsert = 2;\n\t\telse\n\t\t\tinsert = 1;\n#endif\n\t\tinsert = 1;\n\t\tatomic_inc(&root->orphan_inodes);\n\t}\n\n\tif (!test_and_set_bit(BTRFS_INODE_ORPHAN_META_RESERVED,\n\t\t\t      &BTRFS_I(inode)->runtime_flags))\n\t\treserve = 1;\n\tspin_unlock(&root->orphan_lock);\n\n\t/* grab metadata reservation from transaction handle */\n\tif (reserve) {\n\t\tret = btrfs_orphan_reserve_metadata(trans, inode);\n\t\tBUG_ON(ret); /* -ENOSPC in reservation; Logic error? JDM */\n\t}\n\n\t/* insert an orphan item to track this unlinked/truncated file */\n\tif (insert >= 1) {\n\t\tret = btrfs_insert_orphan_item(trans, root, btrfs_ino(inode));\n\t\tif (ret && ret != -EEXIST) {\n\t\t\tclear_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t\t  &BTRFS_I(inode)->runtime_flags);\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\treturn ret;\n\t\t}\n\t\tret = 0;\n\t}\n\n\t/* insert an orphan item to track subvolume contains orphan files */\n\tif (insert >= 2) {\n\t\tret = btrfs_insert_orphan_item(trans, root->fs_info->tree_root,\n\t\t\t\t\t       root->root_key.objectid);\n\t\tif (ret && ret != -EEXIST) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * We have done the truncate/delete so we can go ahead and remove the orphan\n * item for this particular inode.\n */\nint btrfs_orphan_del(struct btrfs_trans_handle *trans, struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint delete_item = 0;\n\tint release_rsv = 0;\n\tint ret = 0;\n\n\tspin_lock(&root->orphan_lock);\n\tif (test_and_clear_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t       &BTRFS_I(inode)->runtime_flags))\n\t\tdelete_item = 1;\n\n\tif (test_and_clear_bit(BTRFS_INODE_ORPHAN_META_RESERVED,\n\t\t\t       &BTRFS_I(inode)->runtime_flags))\n\t\trelease_rsv = 1;\n\tspin_unlock(&root->orphan_lock);\n\n\tif (trans && delete_item) {\n\t\tret = btrfs_del_orphan_item(trans, root, btrfs_ino(inode));\n\t\tBUG_ON(ret); /* -ENOMEM or corruption (JDM: Recheck) */\n\t}\n\n\tif (release_rsv) {\n\t\tbtrfs_orphan_release_metadata(inode);\n\t\tatomic_dec(&root->orphan_inodes);\n\t}\n\n\treturn 0;\n}\n\n/*\n * this cleans up any orphans that may be left on the list from the last use\n * of this root.\n */\nint btrfs_orphan_cleanup(struct btrfs_root *root)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key, found_key;\n\tstruct btrfs_trans_handle *trans;\n\tstruct inode *inode;\n\tu64 last_objectid = 0;\n\tint ret = 0, nr_unlink = 0, nr_truncate = 0;\n\n\tif (cmpxchg(&root->orphan_cleanup_state, 0, ORPHAN_CLEANUP_STARTED))\n\t\treturn 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpath->reada = -1;\n\n\tkey.objectid = BTRFS_ORPHAN_OBJECTID;\n\tbtrfs_set_key_type(&key, BTRFS_ORPHAN_ITEM_KEY);\n\tkey.offset = (u64)-1;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * if ret == 0 means we found what we were searching for, which\n\t\t * is weird, but possible, so only screw with path if we didn't\n\t\t * find the key and see if we have stuff that matches\n\t\t */\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tif (path->slots[0] == 0)\n\t\t\t\tbreak;\n\t\t\tpath->slots[0]--;\n\t\t}\n\n\t\t/* pull out the item */\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\t/* make sure the item matches what we want */\n\t\tif (found_key.objectid != BTRFS_ORPHAN_OBJECTID)\n\t\t\tbreak;\n\t\tif (btrfs_key_type(&found_key) != BTRFS_ORPHAN_ITEM_KEY)\n\t\t\tbreak;\n\n\t\t/* release the path since we're done with it */\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * this is where we are basically btrfs_lookup, without the\n\t\t * crossing root thing.  we store the inode number in the\n\t\t * offset of the orphan item.\n\t\t */\n\n\t\tif (found_key.offset == last_objectid) {\n\t\t\tprintk(KERN_ERR \"btrfs: Error removing orphan entry, \"\n\t\t\t       \"stopping orphan cleanup\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlast_objectid = found_key.offset;\n\n\t\tfound_key.objectid = found_key.offset;\n\t\tfound_key.type = BTRFS_INODE_ITEM_KEY;\n\t\tfound_key.offset = 0;\n\t\tinode = btrfs_iget(root->fs_info->sb, &found_key, root, NULL);\n\t\tret = PTR_RET(inode);\n\t\tif (ret && ret != -ESTALE)\n\t\t\tgoto out;\n\n\t\tif (ret == -ESTALE && root == root->fs_info->tree_root) {\n\t\t\tstruct btrfs_root *dead_root;\n\t\t\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\t\t\tint is_dead_root = 0;\n\n\t\t\t/*\n\t\t\t * this is an orphan in the tree root. Currently these\n\t\t\t * could come from 2 sources:\n\t\t\t *  a) a snapshot deletion in progress\n\t\t\t *  b) a free space cache inode\n\t\t\t * We need to distinguish those two, as the snapshot\n\t\t\t * orphan must not get deleted.\n\t\t\t * find_dead_roots already ran before us, so if this\n\t\t\t * is a snapshot deletion, we should find the root\n\t\t\t * in the dead_roots list\n\t\t\t */\n\t\t\tspin_lock(&fs_info->trans_lock);\n\t\t\tlist_for_each_entry(dead_root, &fs_info->dead_roots,\n\t\t\t\t\t    root_list) {\n\t\t\t\tif (dead_root->root_key.objectid ==\n\t\t\t\t    found_key.objectid) {\n\t\t\t\t\tis_dead_root = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\tif (is_dead_root) {\n\t\t\t\t/* prevent this orphan from being found again */\n\t\t\t\tkey.offset = found_key.objectid - 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Inode is already gone but the orphan item is still there,\n\t\t * kill the orphan item.\n\t\t */\n\t\tif (ret == -ESTALE) {\n\t\t\ttrans = btrfs_start_transaction(root, 1);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tprintk(KERN_ERR \"auto deleting %Lu\\n\",\n\t\t\t       found_key.objectid);\n\t\t\tret = btrfs_del_orphan_item(trans, root,\n\t\t\t\t\t\t    found_key.objectid);\n\t\t\tBUG_ON(ret); /* -ENOMEM or corruption (JDM: Recheck) */\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * add this inode to the orphan list so btrfs_orphan_del does\n\t\t * the proper thing when we hit it\n\t\t */\n\t\tset_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t&BTRFS_I(inode)->runtime_flags);\n\n\t\t/* if we have links, this was a truncate, lets do that */\n\t\tif (inode->i_nlink) {\n\t\t\tif (!S_ISREG(inode->i_mode)) {\n\t\t\t\tWARN_ON(1);\n\t\t\t\tiput(inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tnr_truncate++;\n\t\t\tret = btrfs_truncate(inode);\n\t\t} else {\n\t\t\tnr_unlink++;\n\t\t}\n\n\t\t/* this will do delete_inode and everything for us */\n\t\tiput(inode);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\t/* release the path since we're done with it */\n\tbtrfs_release_path(path);\n\n\troot->orphan_cleanup_state = ORPHAN_CLEANUP_DONE;\n\n\tif (root->orphan_block_rsv)\n\t\tbtrfs_block_rsv_release(root, root->orphan_block_rsv,\n\t\t\t\t\t(u64)-1);\n\n\tif (root->orphan_block_rsv || root->orphan_item_inserted) {\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (!IS_ERR(trans))\n\t\t\tbtrfs_end_transaction(trans, root);\n\t}\n\n\tif (nr_unlink)\n\t\tprintk(KERN_INFO \"btrfs: unlinked %d orphans\\n\", nr_unlink);\n\tif (nr_truncate)\n\t\tprintk(KERN_INFO \"btrfs: truncated %d orphans\\n\", nr_truncate);\n\nout:\n\tif (ret)\n\t\tprintk(KERN_CRIT \"btrfs: could not do orphan cleanup %d\\n\", ret);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * very simple check to peek ahead in the leaf looking for xattrs.  If we\n * don't find any xattrs, we know there can't be any acls.\n *\n * slot is the slot the inode is in, objectid is the objectid of the inode\n */\nstatic noinline int acls_after_inode_item(struct extent_buffer *leaf,\n\t\t\t\t\t  int slot, u64 objectid)\n{\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tstruct btrfs_key found_key;\n\tint scanned = 0;\n\n\tslot++;\n\twhile (slot < nritems) {\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\t/* we found a different objectid, there must not be acls */\n\t\tif (found_key.objectid != objectid)\n\t\t\treturn 0;\n\n\t\t/* we found an xattr, assume we've got an acl */\n\t\tif (found_key.type == BTRFS_XATTR_ITEM_KEY)\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * we found a key greater than an xattr key, there can't\n\t\t * be any acls later on\n\t\t */\n\t\tif (found_key.type > BTRFS_XATTR_ITEM_KEY)\n\t\t\treturn 0;\n\n\t\tslot++;\n\t\tscanned++;\n\n\t\t/*\n\t\t * it goes inode, inode backrefs, xattrs, extents,\n\t\t * so if there are a ton of hard links to an inode there can\n\t\t * be a lot of backrefs.  Don't waste time searching too hard,\n\t\t * this is just an optimization\n\t\t */\n\t\tif (scanned >= 8)\n\t\t\tbreak;\n\t}\n\t/* we hit the end of the leaf before we found an xattr or\n\t * something larger than an xattr.  We have to assume the inode\n\t * has acls\n\t */\n\treturn 1;\n}\n\n/*\n * read an inode from the btree into the in-memory inode\n */\nstatic void btrfs_read_locked_inode(struct inode *inode)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct btrfs_timespec *tspec;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_key location;\n\tint maybe_acls;\n\tu32 rdev;\n\tint ret;\n\tbool filled = false;\n\n\tret = btrfs_fill_inode(inode, &rdev);\n\tif (!ret)\n\t\tfilled = true;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\tgoto make_bad;\n\n\tpath->leave_spinning = 1;\n\tmemcpy(&location, &BTRFS_I(inode)->location, sizeof(location));\n\n\tret = btrfs_lookup_inode(NULL, root, path, &location, 0);\n\tif (ret)\n\t\tgoto make_bad;\n\n\tleaf = path->nodes[0];\n\n\tif (filled)\n\t\tgoto cache_acl;\n\n\tinode_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_inode_item);\n\tinode->i_mode = btrfs_inode_mode(leaf, inode_item);\n\tset_nlink(inode, btrfs_inode_nlink(leaf, inode_item));\n\ti_uid_write(inode, btrfs_inode_uid(leaf, inode_item));\n\ti_gid_write(inode, btrfs_inode_gid(leaf, inode_item));\n\tbtrfs_i_size_write(inode, btrfs_inode_size(leaf, inode_item));\n\n\ttspec = btrfs_inode_atime(inode_item);\n\tinode->i_atime.tv_sec = btrfs_timespec_sec(leaf, tspec);\n\tinode->i_atime.tv_nsec = btrfs_timespec_nsec(leaf, tspec);\n\n\ttspec = btrfs_inode_mtime(inode_item);\n\tinode->i_mtime.tv_sec = btrfs_timespec_sec(leaf, tspec);\n\tinode->i_mtime.tv_nsec = btrfs_timespec_nsec(leaf, tspec);\n\n\ttspec = btrfs_inode_ctime(inode_item);\n\tinode->i_ctime.tv_sec = btrfs_timespec_sec(leaf, tspec);\n\tinode->i_ctime.tv_nsec = btrfs_timespec_nsec(leaf, tspec);\n\n\tinode_set_bytes(inode, btrfs_inode_nbytes(leaf, inode_item));\n\tBTRFS_I(inode)->generation = btrfs_inode_generation(leaf, inode_item);\n\tBTRFS_I(inode)->last_trans = btrfs_inode_transid(leaf, inode_item);\n\n\t/*\n\t * If we were modified in the current generation and evicted from memory\n\t * and then re-read we need to do a full sync since we don't have any\n\t * idea about which extents were modified before we were evicted from\n\t * cache.\n\t */\n\tif (BTRFS_I(inode)->last_trans == root->fs_info->generation)\n\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC,\n\t\t\t&BTRFS_I(inode)->runtime_flags);\n\n\tinode->i_version = btrfs_inode_sequence(leaf, inode_item);\n\tinode->i_generation = BTRFS_I(inode)->generation;\n\tinode->i_rdev = 0;\n\trdev = btrfs_inode_rdev(leaf, inode_item);\n\n\tBTRFS_I(inode)->index_cnt = (u64)-1;\n\tBTRFS_I(inode)->flags = btrfs_inode_flags(leaf, inode_item);\ncache_acl:\n\t/*\n\t * try to precache a NULL acl entry for files that don't have\n\t * any xattrs or acls\n\t */\n\tmaybe_acls = acls_after_inode_item(leaf, path->slots[0],\n\t\t\t\t\t   btrfs_ino(inode));\n\tif (!maybe_acls)\n\t\tcache_no_acl(inode);\n\n\tbtrfs_free_path(path);\n\n\tswitch (inode->i_mode & S_IFMT) {\n\tcase S_IFREG:\n\t\tinode->i_mapping->a_ops = &btrfs_aops;\n\t\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\t\tBTRFS_I(inode)->io_tree.ops = &btrfs_extent_io_ops;\n\t\tinode->i_fop = &btrfs_file_operations;\n\t\tinode->i_op = &btrfs_file_inode_operations;\n\t\tbreak;\n\tcase S_IFDIR:\n\t\tinode->i_fop = &btrfs_dir_file_operations;\n\t\tif (root == root->fs_info->tree_root)\n\t\t\tinode->i_op = &btrfs_dir_ro_inode_operations;\n\t\telse\n\t\t\tinode->i_op = &btrfs_dir_inode_operations;\n\t\tbreak;\n\tcase S_IFLNK:\n\t\tinode->i_op = &btrfs_symlink_inode_operations;\n\t\tinode->i_mapping->a_ops = &btrfs_symlink_aops;\n\t\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\t\tbreak;\n\tdefault:\n\t\tinode->i_op = &btrfs_special_inode_operations;\n\t\tinit_special_inode(inode, inode->i_mode, rdev);\n\t\tbreak;\n\t}\n\n\tbtrfs_update_iflags(inode);\n\treturn;\n\nmake_bad:\n\tbtrfs_free_path(path);\n\tmake_bad_inode(inode);\n}\n\n/*\n * given a leaf and an inode, copy the inode fields into the leaf\n */\nstatic void fill_inode_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct extent_buffer *leaf,\n\t\t\t    struct btrfs_inode_item *item,\n\t\t\t    struct inode *inode)\n{\n\tbtrfs_set_inode_uid(leaf, item, i_uid_read(inode));\n\tbtrfs_set_inode_gid(leaf, item, i_gid_read(inode));\n\tbtrfs_set_inode_size(leaf, item, BTRFS_I(inode)->disk_i_size);\n\tbtrfs_set_inode_mode(leaf, item, inode->i_mode);\n\tbtrfs_set_inode_nlink(leaf, item, inode->i_nlink);\n\n\tbtrfs_set_timespec_sec(leaf, btrfs_inode_atime(item),\n\t\t\t       inode->i_atime.tv_sec);\n\tbtrfs_set_timespec_nsec(leaf, btrfs_inode_atime(item),\n\t\t\t\tinode->i_atime.tv_nsec);\n\n\tbtrfs_set_timespec_sec(leaf, btrfs_inode_mtime(item),\n\t\t\t       inode->i_mtime.tv_sec);\n\tbtrfs_set_timespec_nsec(leaf, btrfs_inode_mtime(item),\n\t\t\t\tinode->i_mtime.tv_nsec);\n\n\tbtrfs_set_timespec_sec(leaf, btrfs_inode_ctime(item),\n\t\t\t       inode->i_ctime.tv_sec);\n\tbtrfs_set_timespec_nsec(leaf, btrfs_inode_ctime(item),\n\t\t\t\tinode->i_ctime.tv_nsec);\n\n\tbtrfs_set_inode_nbytes(leaf, item, inode_get_bytes(inode));\n\tbtrfs_set_inode_generation(leaf, item, BTRFS_I(inode)->generation);\n\tbtrfs_set_inode_sequence(leaf, item, inode->i_version);\n\tbtrfs_set_inode_transid(leaf, item, trans->transid);\n\tbtrfs_set_inode_rdev(leaf, item, inode->i_rdev);\n\tbtrfs_set_inode_flags(leaf, item, BTRFS_I(inode)->flags);\n\tbtrfs_set_inode_block_group(leaf, item, 0);\n}\n\n/*\n * copy everything in the in-memory inode into the btree.\n */\nstatic noinline int btrfs_update_inode_item(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, struct inode *inode)\n{\n\tstruct btrfs_inode_item *inode_item;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->leave_spinning = 1;\n\tret = btrfs_lookup_inode(trans, root, path, &BTRFS_I(inode)->location,\n\t\t\t\t 1);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = -ENOENT;\n\t\tgoto failed;\n\t}\n\n\tbtrfs_unlock_up_safe(path, 1);\n\tleaf = path->nodes[0];\n\tinode_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_inode_item);\n\n\tfill_inode_item(trans, leaf, inode_item, inode);\n\tbtrfs_mark_buffer_dirty(leaf);\n\tbtrfs_set_inode_last_trans(trans, inode);\n\tret = 0;\nfailed:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * copy everything in the in-memory inode into the btree.\n */\nnoinline int btrfs_update_inode(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, struct inode *inode)\n{\n\tint ret;\n\n\t/*\n\t * If the inode is a free space inode, we can deadlock during commit\n\t * if we put it into the delayed code.\n\t *\n\t * The data relocation inode should also be directly updated\n\t * without delay\n\t */\n\tif (!btrfs_is_free_space_inode(inode)\n\t    && root->root_key.objectid != BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\tbtrfs_update_root_times(trans, root);\n\n\t\tret = btrfs_delayed_update_inode(trans, root, inode);\n\t\tif (!ret)\n\t\t\tbtrfs_set_inode_last_trans(trans, inode);\n\t\treturn ret;\n\t}\n\n\treturn btrfs_update_inode_item(trans, root, inode);\n}\n\nnoinline int btrfs_update_inode_fallback(struct btrfs_trans_handle *trans,\n\t\t\t\t\t struct btrfs_root *root,\n\t\t\t\t\t struct inode *inode)\n{\n\tint ret;\n\n\tret = btrfs_update_inode(trans, root, inode);\n\tif (ret == -ENOSPC)\n\t\treturn btrfs_update_inode_item(trans, root, inode);\n\treturn ret;\n}\n\n/*\n * unlink helper that gets used here in inode.c and in the tree logging\n * recovery code.  It remove a link in a directory with a given name, and\n * also drops the back refs in the inode to the directory\n */\nstatic int __btrfs_unlink_inode(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root,\n\t\t\t\tstruct inode *dir, struct inode *inode,\n\t\t\t\tconst char *name, int name_len)\n{\n\tstruct btrfs_path *path;\n\tint ret = 0;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_key key;\n\tu64 index;\n\tu64 ino = btrfs_ino(inode);\n\tu64 dir_ino = btrfs_ino(dir);\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpath->leave_spinning = 1;\n\tdi = btrfs_lookup_dir_item(trans, root, path, dir_ino,\n\t\t\t\t    name, name_len, -1);\n\tif (IS_ERR(di)) {\n\t\tret = PTR_ERR(di);\n\t\tgoto err;\n\t}\n\tif (!di) {\n\t\tret = -ENOENT;\n\t\tgoto err;\n\t}\n\tleaf = path->nodes[0];\n\tbtrfs_dir_item_key_to_cpu(leaf, di, &key);\n\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n\tif (ret)\n\t\tgoto err;\n\tbtrfs_release_path(path);\n\n\tret = btrfs_del_inode_ref(trans, root, name, name_len, ino,\n\t\t\t\t  dir_ino, &index);\n\tif (ret) {\n\t\tprintk(KERN_INFO \"btrfs failed to delete reference to %.*s, \"\n\t\t       \"inode %llu parent %llu\\n\", name_len, name,\n\t\t       (unsigned long long)ino, (unsigned long long)dir_ino);\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto err;\n\t}\n\n\tret = btrfs_delete_delayed_dir_index(trans, root, dir, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto err;\n\t}\n\n\tret = btrfs_del_inode_ref_in_log(trans, root, name, name_len,\n\t\t\t\t\t inode, dir_ino);\n\tif (ret != 0 && ret != -ENOENT) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto err;\n\t}\n\n\tret = btrfs_del_dir_entries_in_log(trans, root, name, name_len,\n\t\t\t\t\t   dir, index);\n\tif (ret == -ENOENT)\n\t\tret = 0;\nerr:\n\tbtrfs_free_path(path);\n\tif (ret)\n\t\tgoto out;\n\n\tbtrfs_i_size_write(dir, dir->i_size - name_len * 2);\n\tinode_inc_iversion(inode);\n\tinode_inc_iversion(dir);\n\tinode->i_ctime = dir->i_mtime = dir->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode(trans, root, dir);\nout:\n\treturn ret;\n}\n\nint btrfs_unlink_inode(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root,\n\t\t       struct inode *dir, struct inode *inode,\n\t\t       const char *name, int name_len)\n{\n\tint ret;\n\tret = __btrfs_unlink_inode(trans, root, dir, inode, name, name_len);\n\tif (!ret) {\n\t\tbtrfs_drop_nlink(inode);\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t}\n\treturn ret;\n}\n\t\t\n\n/* helper to check if there is any shared block in the path */\nstatic int check_path_shared(struct btrfs_root *root,\n\t\t\t     struct btrfs_path *path)\n{\n\tstruct extent_buffer *eb;\n\tint level;\n\tu64 refs = 1;\n\n\tfor (level = 0; level < BTRFS_MAX_LEVEL; level++) {\n\t\tint ret;\n\n\t\tif (!path->nodes[level])\n\t\t\tbreak;\n\t\teb = path->nodes[level];\n\t\tif (!btrfs_block_can_be_shared(root, eb))\n\t\t\tcontinue;\n\t\tret = btrfs_lookup_extent_info(NULL, root, eb->start, eb->len,\n\t\t\t\t\t       &refs, NULL);\n\t\tif (refs > 1)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * helper to start transaction for unlink and rmdir.\n *\n * unlink and rmdir are special in btrfs, they do not always free space.\n * so in enospc case, we should make sure they will free space before\n * allowing them to use the global metadata reservation.\n */\nstatic struct btrfs_trans_handle *__unlink_start_trans(struct inode *dir,\n\t\t\t\t\t\t       struct dentry *dentry)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dir_item *di;\n\tstruct inode *inode = dentry->d_inode;\n\tu64 index;\n\tint check_link = 1;\n\tint err = -ENOSPC;\n\tint ret;\n\tu64 ino = btrfs_ino(inode);\n\tu64 dir_ino = btrfs_ino(dir);\n\n\t/*\n\t * 1 for the possible orphan item\n\t * 1 for the dir item\n\t * 1 for the dir index\n\t * 1 for the inode ref\n\t * 1 for the inode ref in the tree log\n\t * 2 for the dir entries in the log\n\t * 1 for the inode\n\t */\n\ttrans = btrfs_start_transaction(root, 8);\n\tif (!IS_ERR(trans) || PTR_ERR(trans) != -ENOSPC)\n\t\treturn trans;\n\n\tif (ino == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\t/* check if there is someone else holds reference */\n\tif (S_ISDIR(inode->i_mode) && atomic_read(&inode->i_count) > 1)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tif (atomic_read(&inode->i_count) > 2)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tif (xchg(&root->fs_info->enospc_unlink, 1))\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\troot->fs_info->enospc_unlink = 0;\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* 1 for the orphan item */\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\troot->fs_info->enospc_unlink = 0;\n\t\treturn trans;\n\t}\n\n\tpath->skip_locking = 1;\n\tpath->search_commit_root = 1;\n\n\tret = btrfs_lookup_inode(trans, root, path,\n\t\t\t\t&BTRFS_I(dir)->location, 0);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\tif (ret == 0) {\n\t\tif (check_path_shared(root, path))\n\t\t\tgoto out;\n\t} else {\n\t\tcheck_link = 0;\n\t}\n\tbtrfs_release_path(path);\n\n\tret = btrfs_lookup_inode(trans, root, path,\n\t\t\t\t&BTRFS_I(inode)->location, 0);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\tif (ret == 0) {\n\t\tif (check_path_shared(root, path))\n\t\t\tgoto out;\n\t} else {\n\t\tcheck_link = 0;\n\t}\n\tbtrfs_release_path(path);\n\n\tif (ret == 0 && S_ISREG(inode->i_mode)) {\n\t\tret = btrfs_lookup_file_extent(trans, root, path,\n\t\t\t\t\t       ino, (u64)-1, 0);\n\t\tif (ret < 0) {\n\t\t\terr = ret;\n\t\t\tgoto out;\n\t\t}\n\t\tBUG_ON(ret == 0); /* Corruption */\n\t\tif (check_path_shared(root, path))\n\t\t\tgoto out;\n\t\tbtrfs_release_path(path);\n\t}\n\n\tif (!check_link) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tdi = btrfs_lookup_dir_item(trans, root, path, dir_ino,\n\t\t\t\tdentry->d_name.name, dentry->d_name.len, 0);\n\tif (IS_ERR(di)) {\n\t\terr = PTR_ERR(di);\n\t\tgoto out;\n\t}\n\tif (di) {\n\t\tif (check_path_shared(root, path))\n\t\t\tgoto out;\n\t} else {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\tbtrfs_release_path(path);\n\n\tret = btrfs_get_inode_ref_index(trans, root, path, dentry->d_name.name,\n\t\t\t\t\tdentry->d_name.len, ino, dir_ino, 0,\n\t\t\t\t\t&index);\n\tif (ret) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tif (check_path_shared(root, path))\n\t\tgoto out;\n\n\tbtrfs_release_path(path);\n\n\t/*\n\t * This is a commit root search, if we can lookup inode item and other\n\t * relative items in the commit root, it means the transaction of\n\t * dir/file creation has been committed, and the dir index item that we\n\t * delay to insert has also been inserted into the commit root. So\n\t * we needn't worry about the delayed insertion of the dir index item\n\t * here.\n\t */\n\tdi = btrfs_lookup_dir_index_item(trans, root, path, dir_ino, index,\n\t\t\t\tdentry->d_name.name, dentry->d_name.len, 0);\n\tif (IS_ERR(di)) {\n\t\terr = PTR_ERR(di);\n\t\tgoto out;\n\t}\n\tBUG_ON(ret == -ENOENT);\n\tif (check_path_shared(root, path))\n\t\tgoto out;\n\n\terr = 0;\nout:\n\tbtrfs_free_path(path);\n\t/* Migrate the orphan reservation over */\n\tif (!err)\n\t\terr = btrfs_block_rsv_migrate(trans->block_rsv,\n\t\t\t\t&root->fs_info->global_block_rsv,\n\t\t\t\ttrans->bytes_reserved);\n\n\tif (err) {\n\t\tbtrfs_end_transaction(trans, root);\n\t\troot->fs_info->enospc_unlink = 0;\n\t\treturn ERR_PTR(err);\n\t}\n\n\ttrans->block_rsv = &root->fs_info->global_block_rsv;\n\treturn trans;\n}\n\nstatic void __unlink_end_trans(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root)\n{\n\tif (trans->block_rsv->type == BTRFS_BLOCK_RSV_GLOBAL) {\n\t\tbtrfs_block_rsv_release(root, trans->block_rsv,\n\t\t\t\t\ttrans->bytes_reserved);\n\t\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\t\tBUG_ON(!root->fs_info->enospc_unlink);\n\t\troot->fs_info->enospc_unlink = 0;\n\t}\n\tbtrfs_end_transaction(trans, root);\n}\n\nstatic int btrfs_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct inode *inode = dentry->d_inode;\n\tint ret;\n\n\ttrans = __unlink_start_trans(dir, dentry);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tbtrfs_record_unlink_dir(trans, dir, dentry->d_inode, 0);\n\n\tret = btrfs_unlink_inode(trans, root, dir, dentry->d_inode,\n\t\t\t\t dentry->d_name.name, dentry->d_name.len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (inode->i_nlink == 0) {\n\t\tret = btrfs_orphan_add(trans, inode);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\t__unlink_end_trans(trans, root);\n\tbtrfs_btree_balance_dirty(root);\n\treturn ret;\n}\n\nint btrfs_unlink_subvol(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root,\n\t\t\tstruct inode *dir, u64 objectid,\n\t\t\tconst char *name, int name_len)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_key key;\n\tu64 index;\n\tint ret;\n\tu64 dir_ino = btrfs_ino(dir);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tdi = btrfs_lookup_dir_item(trans, root, path, dir_ino,\n\t\t\t\t   name, name_len, -1);\n\tif (IS_ERR_OR_NULL(di)) {\n\t\tif (!di)\n\t\t\tret = -ENOENT;\n\t\telse\n\t\t\tret = PTR_ERR(di);\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tbtrfs_dir_item_key_to_cpu(leaf, di, &key);\n\tWARN_ON(key.type != BTRFS_ROOT_ITEM_KEY || key.objectid != objectid);\n\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out;\n\t}\n\tbtrfs_release_path(path);\n\n\tret = btrfs_del_root_ref(trans, root->fs_info->tree_root,\n\t\t\t\t objectid, root->root_key.objectid,\n\t\t\t\t dir_ino, &index, name, name_len);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out;\n\t\t}\n\t\tdi = btrfs_search_dir_index_item(root, path, dir_ino,\n\t\t\t\t\t\t name, name_len);\n\t\tif (IS_ERR_OR_NULL(di)) {\n\t\t\tif (!di)\n\t\t\t\tret = -ENOENT;\n\t\t\telse\n\t\t\t\tret = PTR_ERR(di);\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tbtrfs_release_path(path);\n\t\tindex = key.offset;\n\t}\n\tbtrfs_release_path(path);\n\n\tret = btrfs_delete_delayed_dir_index(trans, root, dir, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out;\n\t}\n\n\tbtrfs_i_size_write(dir, dir->i_size - name_len * 2);\n\tinode_inc_iversion(dir);\n\tdir->i_mtime = dir->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode_fallback(trans, root, dir);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, root, ret);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_rmdir(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint err = 0;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_trans_handle *trans;\n\n\tif (inode->i_size > BTRFS_EMPTY_DIR_SIZE)\n\t\treturn -ENOTEMPTY;\n\tif (btrfs_ino(inode) == BTRFS_FIRST_FREE_OBJECTID)\n\t\treturn -EPERM;\n\n\ttrans = __unlink_start_trans(dir, dentry);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tif (unlikely(btrfs_ino(inode) == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)) {\n\t\terr = btrfs_unlink_subvol(trans, root, dir,\n\t\t\t\t\t  BTRFS_I(inode)->location.objectid,\n\t\t\t\t\t  dentry->d_name.name,\n\t\t\t\t\t  dentry->d_name.len);\n\t\tgoto out;\n\t}\n\n\terr = btrfs_orphan_add(trans, inode);\n\tif (err)\n\t\tgoto out;\n\n\t/* now the directory is empty */\n\terr = btrfs_unlink_inode(trans, root, dir, dentry->d_inode,\n\t\t\t\t dentry->d_name.name, dentry->d_name.len);\n\tif (!err)\n\t\tbtrfs_i_size_write(inode, 0);\nout:\n\t__unlink_end_trans(trans, root);\n\tbtrfs_btree_balance_dirty(root);\n\n\treturn err;\n}\n\n/*\n * this can truncate away extent items, csum items and directory items.\n * It starts at a high offset and removes keys until it can't find\n * any higher than new_size\n *\n * csum items that cross the new i_size are truncated to the new size\n * as well.\n *\n * min_type is the minimum key type to truncate down to.  If set to 0, this\n * will kill all the items on this inode, including the INODE_ITEM_KEY.\n */\nint btrfs_truncate_inode_items(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root,\n\t\t\t       struct inode *inode,\n\t\t\t       u64 new_size, u32 min_type)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 extent_start = 0;\n\tu64 extent_num_bytes = 0;\n\tu64 extent_offset = 0;\n\tu64 item_end = 0;\n\tu64 mask = root->sectorsize - 1;\n\tu32 found_type = (u8)-1;\n\tint found_extent;\n\tint del_item;\n\tint pending_del_nr = 0;\n\tint pending_del_slot = 0;\n\tint extent_type = -1;\n\tint ret;\n\tint err = 0;\n\tu64 ino = btrfs_ino(inode);\n\n\tBUG_ON(new_size > 0 && min_type != BTRFS_EXTENT_DATA_KEY);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tpath->reada = -1;\n\n\t/*\n\t * We want to drop from the next block forward in case this new size is\n\t * not block aligned since we will be keeping the last block of the\n\t * extent just the way it is.\n\t */\n\tif (root->ref_cows || root == root->fs_info->tree_root)\n\t\tbtrfs_drop_extent_cache(inode, (new_size + mask) & (~mask), (u64)-1, 0);\n\n\t/*\n\t * This function is also used to drop the items in the log tree before\n\t * we relog the inode, so if root != BTRFS_I(inode)->root, it means\n\t * it is used to drop the loged items. So we shouldn't kill the delayed\n\t * items.\n\t */\n\tif (min_type == 0 && root == BTRFS_I(inode)->root)\n\t\tbtrfs_kill_delayed_inode_items(inode);\n\n\tkey.objectid = ino;\n\tkey.offset = (u64)-1;\n\tkey.type = (u8)-1;\n\nsearch_again:\n\tpath->leave_spinning = 1;\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tif (ret > 0) {\n\t\t/* there are no items in the tree for us to truncate, we're\n\t\t * done\n\t\t */\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto out;\n\t\tpath->slots[0]--;\n\t}\n\n\twhile (1) {\n\t\tfi = NULL;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tfound_type = btrfs_key_type(&found_key);\n\n\t\tif (found_key.objectid != ino)\n\t\t\tbreak;\n\n\t\tif (found_type < min_type)\n\t\t\tbreak;\n\n\t\titem_end = found_key.offset;\n\t\tif (found_type == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tfi = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t    struct btrfs_file_extent_item);\n\t\t\textent_type = btrfs_file_extent_type(leaf, fi);\n\t\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\titem_end +=\n\t\t\t\t    btrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\titem_end += btrfs_file_extent_inline_len(leaf,\n\t\t\t\t\t\t\t\t\t fi);\n\t\t\t}\n\t\t\titem_end--;\n\t\t}\n\t\tif (found_type > min_type) {\n\t\t\tdel_item = 1;\n\t\t} else {\n\t\t\tif (item_end < new_size)\n\t\t\t\tbreak;\n\t\t\tif (found_key.offset >= new_size)\n\t\t\t\tdel_item = 1;\n\t\t\telse\n\t\t\t\tdel_item = 0;\n\t\t}\n\t\tfound_extent = 0;\n\t\t/* FIXME, shrink the extent if the ref count is only 1 */\n\t\tif (found_type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto delete;\n\n\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\tu64 num_dec;\n\t\t\textent_start = btrfs_file_extent_disk_bytenr(leaf, fi);\n\t\t\tif (!del_item) {\n\t\t\t\tu64 orig_num_bytes =\n\t\t\t\t\tbtrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t\textent_num_bytes = new_size -\n\t\t\t\t\tfound_key.offset + root->sectorsize - 1;\n\t\t\t\textent_num_bytes = extent_num_bytes &\n\t\t\t\t\t~((u64)root->sectorsize - 1);\n\t\t\t\tbtrfs_set_file_extent_num_bytes(leaf, fi,\n\t\t\t\t\t\t\t extent_num_bytes);\n\t\t\t\tnum_dec = (orig_num_bytes -\n\t\t\t\t\t   extent_num_bytes);\n\t\t\t\tif (root->ref_cows && extent_start != 0)\n\t\t\t\t\tinode_sub_bytes(inode, num_dec);\n\t\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t\t} else {\n\t\t\t\textent_num_bytes =\n\t\t\t\t\tbtrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t\t fi);\n\t\t\t\textent_offset = found_key.offset -\n\t\t\t\t\tbtrfs_file_extent_offset(leaf, fi);\n\n\t\t\t\t/* FIXME blocksize != 4096 */\n\t\t\t\tnum_dec = btrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t\tif (extent_start != 0) {\n\t\t\t\t\tfound_extent = 1;\n\t\t\t\t\tif (root->ref_cows)\n\t\t\t\t\t\tinode_sub_bytes(inode, num_dec);\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t/*\n\t\t\t * we can't truncate inline items that have had\n\t\t\t * special encodings\n\t\t\t */\n\t\t\tif (!del_item &&\n\t\t\t    btrfs_file_extent_compression(leaf, fi) == 0 &&\n\t\t\t    btrfs_file_extent_encryption(leaf, fi) == 0 &&\n\t\t\t    btrfs_file_extent_other_encoding(leaf, fi) == 0) {\n\t\t\t\tu32 size = new_size - found_key.offset;\n\n\t\t\t\tif (root->ref_cows) {\n\t\t\t\t\tinode_sub_bytes(inode, item_end + 1 -\n\t\t\t\t\t\t\tnew_size);\n\t\t\t\t}\n\t\t\t\tsize =\n\t\t\t\t    btrfs_file_extent_calc_inline_size(size);\n\t\t\t\tbtrfs_truncate_item(trans, root, path,\n\t\t\t\t\t\t    size, 1);\n\t\t\t} else if (root->ref_cows) {\n\t\t\t\tinode_sub_bytes(inode, item_end + 1 -\n\t\t\t\t\t\tfound_key.offset);\n\t\t\t}\n\t\t}\ndelete:\n\t\tif (del_item) {\n\t\t\tif (!pending_del_nr) {\n\t\t\t\t/* no pending yet, add ourselves */\n\t\t\t\tpending_del_slot = path->slots[0];\n\t\t\t\tpending_del_nr = 1;\n\t\t\t} else if (pending_del_nr &&\n\t\t\t\t   path->slots[0] + 1 == pending_del_slot) {\n\t\t\t\t/* hop on the pending chunk */\n\t\t\t\tpending_del_nr++;\n\t\t\t\tpending_del_slot = path->slots[0];\n\t\t\t} else {\n\t\t\t\tBUG();\n\t\t\t}\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tif (found_extent && (root->ref_cows ||\n\t\t\t\t     root == root->fs_info->tree_root)) {\n\t\t\tbtrfs_set_path_blocking(path);\n\t\t\tret = btrfs_free_extent(trans, root, extent_start,\n\t\t\t\t\t\textent_num_bytes, 0,\n\t\t\t\t\t\tbtrfs_header_owner(leaf),\n\t\t\t\t\t\tino, extent_offset, 0);\n\t\t\tBUG_ON(ret);\n\t\t}\n\n\t\tif (found_type == BTRFS_INODE_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tif (path->slots[0] == 0 ||\n\t\t    path->slots[0] != pending_del_slot) {\n\t\t\tif (pending_del_nr) {\n\t\t\t\tret = btrfs_del_items(trans, root, path,\n\t\t\t\t\t\tpending_del_slot,\n\t\t\t\t\t\tpending_del_nr);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\troot, ret);\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tpending_del_nr = 0;\n\t\t\t}\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto search_again;\n\t\t} else {\n\t\t\tpath->slots[0]--;\n\t\t}\n\t}\nout:\n\tif (pending_del_nr) {\n\t\tret = btrfs_del_items(trans, root, path, pending_del_slot,\n\t\t\t\t      pending_del_nr);\n\t\tif (ret)\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t}\nerror:\n\tbtrfs_free_path(path);\n\treturn err;\n}\n\n/*\n * btrfs_truncate_page - read, zero a chunk and write a page\n * @inode - inode that we're zeroing\n * @from - the offset to start zeroing\n * @len - the length to zero, 0 to zero the entire range respective to the\n *\toffset\n * @front - zero up to the offset instead of from the offset on\n *\n * This will find the page for the \"from\" offset and cow the page and zero the\n * part we want to zero.  This is used with truncate and hole punching.\n */\nint btrfs_truncate_page(struct inode *inode, loff_t from, loff_t len,\n\t\t\tint front)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tchar *kaddr;\n\tu32 blocksize = root->sectorsize;\n\tpgoff_t index = from >> PAGE_CACHE_SHIFT;\n\tunsigned offset = from & (PAGE_CACHE_SIZE-1);\n\tstruct page *page;\n\tgfp_t mask = btrfs_alloc_write_mask(mapping);\n\tint ret = 0;\n\tu64 page_start;\n\tu64 page_end;\n\n\tif ((offset & (blocksize - 1)) == 0 &&\n\t    (!len || ((len & (blocksize - 1)) == 0)))\n\t\tgoto out;\n\tret = btrfs_delalloc_reserve_space(inode, PAGE_CACHE_SIZE);\n\tif (ret)\n\t\tgoto out;\n\nagain:\n\tpage = find_or_create_page(mapping, index, mask);\n\tif (!page) {\n\t\tbtrfs_delalloc_release_space(inode, PAGE_CACHE_SIZE);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpage_start = page_offset(page);\n\tpage_end = page_start + PAGE_CACHE_SIZE - 1;\n\n\tif (!PageUptodate(page)) {\n\t\tret = btrfs_readpage(NULL, page);\n\t\tlock_page(page);\n\t\tif (page->mapping != mapping) {\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t\tgoto again;\n\t\t}\n\t\tif (!PageUptodate(page)) {\n\t\t\tret = -EIO;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\twait_on_page_writeback(page);\n\n\tlock_extent_bits(io_tree, page_start, page_end, 0, &cached_state);\n\tset_page_extent_mapped(page);\n\n\tordered = btrfs_lookup_ordered_extent(inode, page_start);\n\tif (ordered) {\n\t\tunlock_extent_cached(io_tree, page_start, page_end,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tgoto again;\n\t}\n\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, page_start, page_end,\n\t\t\t  EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,\n\t\t\t  0, 0, &cached_state, GFP_NOFS);\n\n\tret = btrfs_set_extent_delalloc(inode, page_start, page_end,\n\t\t\t\t\t&cached_state);\n\tif (ret) {\n\t\tunlock_extent_cached(io_tree, page_start, page_end,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tgoto out_unlock;\n\t}\n\n\tif (offset != PAGE_CACHE_SIZE) {\n\t\tif (!len)\n\t\t\tlen = PAGE_CACHE_SIZE - offset;\n\t\tkaddr = kmap(page);\n\t\tif (front)\n\t\t\tmemset(kaddr, 0, offset);\n\t\telse\n\t\t\tmemset(kaddr + offset, 0, len);\n\t\tflush_dcache_page(page);\n\t\tkunmap(page);\n\t}\n\tClearPageChecked(page);\n\tset_page_dirty(page);\n\tunlock_extent_cached(io_tree, page_start, page_end, &cached_state,\n\t\t\t     GFP_NOFS);\n\nout_unlock:\n\tif (ret)\n\t\tbtrfs_delalloc_release_space(inode, PAGE_CACHE_SIZE);\n\tunlock_page(page);\n\tpage_cache_release(page);\nout:\n\treturn ret;\n}\n\n/*\n * This function puts in dummy file extents for the area we're creating a hole\n * for.  So if we are truncating this file to a larger size we need to insert\n * these file extents so that btrfs_get_extent will return a EXTENT_MAP_HOLE for\n * the range between oldsize and size\n */\nint btrfs_cont_expand(struct inode *inode, loff_t oldsize, loff_t size)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em = NULL;\n\tstruct extent_state *cached_state = NULL;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tu64 mask = root->sectorsize - 1;\n\tu64 hole_start = (oldsize + mask) & ~mask;\n\tu64 block_end = (size + mask) & ~mask;\n\tu64 last_byte;\n\tu64 cur_offset;\n\tu64 hole_size;\n\tint err = 0;\n\n\tif (size <= hole_start)\n\t\treturn 0;\n\n\twhile (1) {\n\t\tstruct btrfs_ordered_extent *ordered;\n\t\tbtrfs_wait_ordered_range(inode, hole_start,\n\t\t\t\t\t block_end - hole_start);\n\t\tlock_extent_bits(io_tree, hole_start, block_end - 1, 0,\n\t\t\t\t &cached_state);\n\t\tordered = btrfs_lookup_ordered_extent(inode, hole_start);\n\t\tif (!ordered)\n\t\t\tbreak;\n\t\tunlock_extent_cached(io_tree, hole_start, block_end - 1,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t}\n\n\tcur_offset = hole_start;\n\twhile (1) {\n\t\tem = btrfs_get_extent(inode, NULL, 0, cur_offset,\n\t\t\t\tblock_end - cur_offset, 0);\n\t\tif (IS_ERR(em)) {\n\t\t\terr = PTR_ERR(em);\n\t\t\tbreak;\n\t\t}\n\t\tlast_byte = min(extent_map_end(em), block_end);\n\t\tlast_byte = (last_byte + mask) & ~mask;\n\t\tif (!test_bit(EXTENT_FLAG_PREALLOC, &em->flags)) {\n\t\t\tstruct extent_map *hole_em;\n\t\t\thole_size = last_byte - cur_offset;\n\n\t\t\ttrans = btrfs_start_transaction(root, 3);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\terr = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t\t cur_offset,\n\t\t\t\t\t\t cur_offset + hole_size, 1);\n\t\t\tif (err) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, err);\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = btrfs_insert_file_extent(trans, root,\n\t\t\t\t\tbtrfs_ino(inode), cur_offset, 0,\n\t\t\t\t\t0, hole_size, 0, hole_size,\n\t\t\t\t\t0, 0, 0);\n\t\t\tif (err) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, err);\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbtrfs_drop_extent_cache(inode, cur_offset,\n\t\t\t\t\t\tcur_offset + hole_size - 1, 0);\n\t\t\thole_em = alloc_extent_map();\n\t\t\tif (!hole_em) {\n\t\t\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC,\n\t\t\t\t\t&BTRFS_I(inode)->runtime_flags);\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\thole_em->start = cur_offset;\n\t\t\thole_em->len = hole_size;\n\t\t\thole_em->orig_start = cur_offset;\n\n\t\t\thole_em->block_start = EXTENT_MAP_HOLE;\n\t\t\thole_em->block_len = 0;\n\t\t\thole_em->orig_block_len = 0;\n\t\t\thole_em->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\t\thole_em->compress_type = BTRFS_COMPRESS_NONE;\n\t\t\thole_em->generation = trans->transid;\n\n\t\t\twhile (1) {\n\t\t\t\twrite_lock(&em_tree->lock);\n\t\t\t\terr = add_extent_mapping(em_tree, hole_em);\n\t\t\t\tif (!err)\n\t\t\t\t\tlist_move(&hole_em->list,\n\t\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\t\twrite_unlock(&em_tree->lock);\n\t\t\t\tif (err != -EEXIST)\n\t\t\t\t\tbreak;\n\t\t\t\tbtrfs_drop_extent_cache(inode, cur_offset,\n\t\t\t\t\t\t\tcur_offset +\n\t\t\t\t\t\t\thole_size - 1, 0);\n\t\t\t}\n\t\t\tfree_extent_map(hole_em);\nnext:\n\t\t\tbtrfs_update_inode(trans, root, inode);\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t}\n\t\tfree_extent_map(em);\n\t\tem = NULL;\n\t\tcur_offset = last_byte;\n\t\tif (cur_offset >= block_end)\n\t\t\tbreak;\n\t}\n\n\tfree_extent_map(em);\n\tunlock_extent_cached(io_tree, hole_start, block_end - 1, &cached_state,\n\t\t\t     GFP_NOFS);\n\treturn err;\n}\n\nstatic int btrfs_setsize(struct inode *inode, loff_t newsize)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tloff_t oldsize = i_size_read(inode);\n\tint ret;\n\n\tif (newsize == oldsize)\n\t\treturn 0;\n\n\tif (newsize > oldsize) {\n\t\ttruncate_pagecache(inode, oldsize, newsize);\n\t\tret = btrfs_cont_expand(inode, oldsize, newsize);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\ttrans = btrfs_start_transaction(root, 1);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\ti_size_write(inode, newsize);\n\t\tbtrfs_ordered_update_i_size(inode, i_size_read(inode), NULL);\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t\tbtrfs_end_transaction(trans, root);\n\t} else {\n\n\t\t/*\n\t\t * We're truncating a file that used to have good data down to\n\t\t * zero. Make sure it gets into the ordered flush list so that\n\t\t * any new writes get down to disk quickly.\n\t\t */\n\t\tif (newsize == 0)\n\t\t\tset_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,\n\t\t\t\t&BTRFS_I(inode)->runtime_flags);\n\n\t\t/* we don't support swapfiles, so vmtruncate shouldn't fail */\n\t\ttruncate_setsize(inode, newsize);\n\t\tret = btrfs_truncate(inode);\n\t}\n\n\treturn ret;\n}\n\nstatic int btrfs_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint err;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\terr = inode_change_ok(inode, attr);\n\tif (err)\n\t\treturn err;\n\n\tif (S_ISREG(inode->i_mode) && (attr->ia_valid & ATTR_SIZE)) {\n\t\terr = btrfs_setsize(inode, attr->ia_size);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (attr->ia_valid) {\n\t\tsetattr_copy(inode, attr);\n\t\tinode_inc_iversion(inode);\n\t\terr = btrfs_dirty_inode(inode);\n\n\t\tif (!err && attr->ia_valid & ATTR_MODE)\n\t\t\terr = btrfs_acl_chmod(inode);\n\t}\n\n\treturn err;\n}\n\nvoid btrfs_evict_inode(struct inode *inode)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_block_rsv *rsv, *global_rsv;\n\tu64 min_size = btrfs_calc_trunc_metadata_size(root, 1);\n\tint ret;\n\n\ttrace_btrfs_inode_evict(inode);\n\n\ttruncate_inode_pages(&inode->i_data, 0);\n\tif (inode->i_nlink && (btrfs_root_refs(&root->root_item) != 0 ||\n\t\t\t       btrfs_is_free_space_inode(inode)))\n\t\tgoto no_delete;\n\n\tif (is_bad_inode(inode)) {\n\t\tbtrfs_orphan_del(NULL, inode);\n\t\tgoto no_delete;\n\t}\n\t/* do we really want it for ->i_nlink > 0 and zero btrfs_root_refs? */\n\tbtrfs_wait_ordered_range(inode, 0, (u64)-1);\n\n\tif (root->fs_info->log_root_recovering) {\n\t\tBUG_ON(test_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t\t &BTRFS_I(inode)->runtime_flags));\n\t\tgoto no_delete;\n\t}\n\n\tif (inode->i_nlink > 0) {\n\t\tBUG_ON(btrfs_root_refs(&root->root_item) != 0);\n\t\tgoto no_delete;\n\t}\n\n\trsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);\n\tif (!rsv) {\n\t\tbtrfs_orphan_del(NULL, inode);\n\t\tgoto no_delete;\n\t}\n\trsv->size = min_size;\n\trsv->failfast = 1;\n\tglobal_rsv = &root->fs_info->global_block_rsv;\n\n\tbtrfs_i_size_write(inode, 0);\n\n\t/*\n\t * This is a bit simpler than btrfs_truncate since we've already\n\t * reserved our space for our orphan item in the unlink, so we just\n\t * need to reserve some slack space in case we add bytes and update\n\t * inode item when doing the truncate.\n\t */\n\twhile (1) {\n\t\tret = btrfs_block_rsv_refill(root, rsv, min_size,\n\t\t\t\t\t     BTRFS_RESERVE_FLUSH_LIMIT);\n\n\t\t/*\n\t\t * Try and steal from the global reserve since we will\n\t\t * likely not use this space anyway, we want to try as\n\t\t * hard as possible to get this to work.\n\t\t */\n\t\tif (ret)\n\t\t\tret = btrfs_block_rsv_migrate(global_rsv, rsv, min_size);\n\n\t\tif (ret) {\n\t\t\tprintk(KERN_WARNING \"Could not get space for a \"\n\t\t\t       \"delete, will truncate on mount %d\\n\", ret);\n\t\t\tbtrfs_orphan_del(NULL, inode);\n\t\t\tbtrfs_free_block_rsv(root, rsv);\n\t\t\tgoto no_delete;\n\t\t}\n\n\t\ttrans = btrfs_start_transaction_lflush(root, 1);\n\t\tif (IS_ERR(trans)) {\n\t\t\tbtrfs_orphan_del(NULL, inode);\n\t\t\tbtrfs_free_block_rsv(root, rsv);\n\t\t\tgoto no_delete;\n\t\t}\n\n\t\ttrans->block_rsv = rsv;\n\n\t\tret = btrfs_truncate_inode_items(trans, root, inode, 0, 0);\n\t\tif (ret != -ENOSPC)\n\t\t\tbreak;\n\n\t\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t\tBUG_ON(ret);\n\n\t\tbtrfs_end_transaction(trans, root);\n\t\ttrans = NULL;\n\t\tbtrfs_btree_balance_dirty(root);\n\t}\n\n\tbtrfs_free_block_rsv(root, rsv);\n\n\tif (ret == 0) {\n\t\ttrans->block_rsv = root->orphan_block_rsv;\n\t\tret = btrfs_orphan_del(trans, inode);\n\t\tBUG_ON(ret);\n\t}\n\n\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\tif (!(root == root->fs_info->tree_root ||\n\t      root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID))\n\t\tbtrfs_return_ino(root, btrfs_ino(inode));\n\n\tbtrfs_end_transaction(trans, root);\n\tbtrfs_btree_balance_dirty(root);\nno_delete:\n\tclear_inode(inode);\n\treturn;\n}\n\n/*\n * this returns the key found in the dir entry in the location pointer.\n * If no dir entries were found, location->objectid is 0.\n */\nstatic int btrfs_inode_by_name(struct inode *dir, struct dentry *dentry,\n\t\t\t       struct btrfs_key *location)\n{\n\tconst char *name = dentry->d_name.name;\n\tint namelen = dentry->d_name.len;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tint ret = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tdi = btrfs_lookup_dir_item(NULL, root, path, btrfs_ino(dir), name,\n\t\t\t\t    namelen, 0);\n\tif (IS_ERR(di))\n\t\tret = PTR_ERR(di);\n\n\tif (IS_ERR_OR_NULL(di))\n\t\tgoto out_err;\n\n\tbtrfs_dir_item_key_to_cpu(path->nodes[0], di, location);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\nout_err:\n\tlocation->objectid = 0;\n\tgoto out;\n}\n\n/*\n * when we hit a tree root in a directory, the btrfs part of the inode\n * needs to be changed to reflect the root directory of the tree root.  This\n * is kind of like crossing a mount point.\n */\nstatic int fixup_tree_root_location(struct btrfs_root *root,\n\t\t\t\t    struct inode *dir,\n\t\t\t\t    struct dentry *dentry,\n\t\t\t\t    struct btrfs_key *location,\n\t\t\t\t    struct btrfs_root **sub_root)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *new_root;\n\tstruct btrfs_root_ref *ref;\n\tstruct extent_buffer *leaf;\n\tint ret;\n\tint err = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = -ENOENT;\n\tret = btrfs_find_root_ref(root->fs_info->tree_root, path,\n\t\t\t\t  BTRFS_I(dir)->root->root_key.objectid,\n\t\t\t\t  location->objectid);\n\tif (ret) {\n\t\tif (ret < 0)\n\t\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tref = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_root_ref);\n\tif (btrfs_root_ref_dirid(leaf, ref) != btrfs_ino(dir) ||\n\t    btrfs_root_ref_name_len(leaf, ref) != dentry->d_name.len)\n\t\tgoto out;\n\n\tret = memcmp_extent_buffer(leaf, dentry->d_name.name,\n\t\t\t\t   (unsigned long)(ref + 1),\n\t\t\t\t   dentry->d_name.len);\n\tif (ret)\n\t\tgoto out;\n\n\tbtrfs_release_path(path);\n\n\tnew_root = btrfs_read_fs_root_no_name(root->fs_info, location);\n\tif (IS_ERR(new_root)) {\n\t\terr = PTR_ERR(new_root);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_root_refs(&new_root->root_item) == 0) {\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\n\t*sub_root = new_root;\n\tlocation->objectid = btrfs_root_dirid(&new_root->root_item);\n\tlocation->type = BTRFS_INODE_ITEM_KEY;\n\tlocation->offset = 0;\n\terr = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn err;\n}\n\nstatic void inode_tree_add(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_inode *entry;\n\tstruct rb_node **p;\n\tstruct rb_node *parent;\n\tu64 ino = btrfs_ino(inode);\nagain:\n\tp = &root->inode_tree.rb_node;\n\tparent = NULL;\n\n\tif (inode_unhashed(inode))\n\t\treturn;\n\n\tspin_lock(&root->inode_lock);\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct btrfs_inode, rb_node);\n\n\t\tif (ino < btrfs_ino(&entry->vfs_inode))\n\t\t\tp = &parent->rb_left;\n\t\telse if (ino > btrfs_ino(&entry->vfs_inode))\n\t\t\tp = &parent->rb_right;\n\t\telse {\n\t\t\tWARN_ON(!(entry->vfs_inode.i_state &\n\t\t\t\t  (I_WILL_FREE | I_FREEING)));\n\t\t\trb_erase(parent, &root->inode_tree);\n\t\t\tRB_CLEAR_NODE(parent);\n\t\t\tspin_unlock(&root->inode_lock);\n\t\t\tgoto again;\n\t\t}\n\t}\n\trb_link_node(&BTRFS_I(inode)->rb_node, parent, p);\n\trb_insert_color(&BTRFS_I(inode)->rb_node, &root->inode_tree);\n\tspin_unlock(&root->inode_lock);\n}\n\nstatic void inode_tree_del(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint empty = 0;\n\n\tspin_lock(&root->inode_lock);\n\tif (!RB_EMPTY_NODE(&BTRFS_I(inode)->rb_node)) {\n\t\trb_erase(&BTRFS_I(inode)->rb_node, &root->inode_tree);\n\t\tRB_CLEAR_NODE(&BTRFS_I(inode)->rb_node);\n\t\tempty = RB_EMPTY_ROOT(&root->inode_tree);\n\t}\n\tspin_unlock(&root->inode_lock);\n\n\t/*\n\t * Free space cache has inodes in the tree root, but the tree root has a\n\t * root_refs of 0, so this could end up dropping the tree root as a\n\t * snapshot, so we need the extra !root->fs_info->tree_root check to\n\t * make sure we don't drop it.\n\t */\n\tif (empty && btrfs_root_refs(&root->root_item) == 0 &&\n\t    root != root->fs_info->tree_root) {\n\t\tsynchronize_srcu(&root->fs_info->subvol_srcu);\n\t\tspin_lock(&root->inode_lock);\n\t\tempty = RB_EMPTY_ROOT(&root->inode_tree);\n\t\tspin_unlock(&root->inode_lock);\n\t\tif (empty)\n\t\t\tbtrfs_add_dead_root(root);\n\t}\n}\n\nvoid btrfs_invalidate_inodes(struct btrfs_root *root)\n{\n\tstruct rb_node *node;\n\tstruct rb_node *prev;\n\tstruct btrfs_inode *entry;\n\tstruct inode *inode;\n\tu64 objectid = 0;\n\n\tWARN_ON(btrfs_root_refs(&root->root_item) != 0);\n\n\tspin_lock(&root->inode_lock);\nagain:\n\tnode = root->inode_tree.rb_node;\n\tprev = NULL;\n\twhile (node) {\n\t\tprev = node;\n\t\tentry = rb_entry(node, struct btrfs_inode, rb_node);\n\n\t\tif (objectid < btrfs_ino(&entry->vfs_inode))\n\t\t\tnode = node->rb_left;\n\t\telse if (objectid > btrfs_ino(&entry->vfs_inode))\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\tbreak;\n\t}\n\tif (!node) {\n\t\twhile (prev) {\n\t\t\tentry = rb_entry(prev, struct btrfs_inode, rb_node);\n\t\t\tif (objectid <= btrfs_ino(&entry->vfs_inode)) {\n\t\t\t\tnode = prev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tprev = rb_next(prev);\n\t\t}\n\t}\n\twhile (node) {\n\t\tentry = rb_entry(node, struct btrfs_inode, rb_node);\n\t\tobjectid = btrfs_ino(&entry->vfs_inode) + 1;\n\t\tinode = igrab(&entry->vfs_inode);\n\t\tif (inode) {\n\t\t\tspin_unlock(&root->inode_lock);\n\t\t\tif (atomic_read(&inode->i_count) > 1)\n\t\t\t\td_prune_aliases(inode);\n\t\t\t/*\n\t\t\t * btrfs_drop_inode will have it removed from\n\t\t\t * the inode cache when its usage count\n\t\t\t * hits zero.\n\t\t\t */\n\t\t\tiput(inode);\n\t\t\tcond_resched();\n\t\t\tspin_lock(&root->inode_lock);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (cond_resched_lock(&root->inode_lock))\n\t\t\tgoto again;\n\n\t\tnode = rb_next(node);\n\t}\n\tspin_unlock(&root->inode_lock);\n}\n\nstatic int btrfs_init_locked_inode(struct inode *inode, void *p)\n{\n\tstruct btrfs_iget_args *args = p;\n\tinode->i_ino = args->ino;\n\tBTRFS_I(inode)->root = args->root;\n\treturn 0;\n}\n\nstatic int btrfs_find_actor(struct inode *inode, void *opaque)\n{\n\tstruct btrfs_iget_args *args = opaque;\n\treturn args->ino == btrfs_ino(inode) &&\n\t\targs->root == BTRFS_I(inode)->root;\n}\n\nstatic struct inode *btrfs_iget_locked(struct super_block *s,\n\t\t\t\t       u64 objectid,\n\t\t\t\t       struct btrfs_root *root)\n{\n\tstruct inode *inode;\n\tstruct btrfs_iget_args args;\n\targs.ino = objectid;\n\targs.root = root;\n\n\tinode = iget5_locked(s, objectid, btrfs_find_actor,\n\t\t\t     btrfs_init_locked_inode,\n\t\t\t     (void *)&args);\n\treturn inode;\n}\n\n/* Get an inode object given its location and corresponding root.\n * Returns in *is_new if the inode was read from disk\n */\nstruct inode *btrfs_iget(struct super_block *s, struct btrfs_key *location,\n\t\t\t struct btrfs_root *root, int *new)\n{\n\tstruct inode *inode;\n\n\tinode = btrfs_iget_locked(s, location->objectid, root);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (inode->i_state & I_NEW) {\n\t\tBTRFS_I(inode)->root = root;\n\t\tmemcpy(&BTRFS_I(inode)->location, location, sizeof(*location));\n\t\tbtrfs_read_locked_inode(inode);\n\t\tif (!is_bad_inode(inode)) {\n\t\t\tinode_tree_add(inode);\n\t\t\tunlock_new_inode(inode);\n\t\t\tif (new)\n\t\t\t\t*new = 1;\n\t\t} else {\n\t\t\tunlock_new_inode(inode);\n\t\t\tiput(inode);\n\t\t\tinode = ERR_PTR(-ESTALE);\n\t\t}\n\t}\n\n\treturn inode;\n}\n\nstatic struct inode *new_simple_dir(struct super_block *s,\n\t\t\t\t    struct btrfs_key *key,\n\t\t\t\t    struct btrfs_root *root)\n{\n\tstruct inode *inode = new_inode(s);\n\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tBTRFS_I(inode)->root = root;\n\tmemcpy(&BTRFS_I(inode)->location, key, sizeof(*key));\n\tset_bit(BTRFS_INODE_DUMMY, &BTRFS_I(inode)->runtime_flags);\n\n\tinode->i_ino = BTRFS_EMPTY_SUBVOL_DIR_OBJECTID;\n\tinode->i_op = &btrfs_dir_ro_inode_operations;\n\tinode->i_fop = &simple_dir_operations;\n\tinode->i_mode = S_IFDIR | S_IRUGO | S_IWUSR | S_IXUGO;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;\n\n\treturn inode;\n}\n\nstruct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *sub_root = root;\n\tstruct btrfs_key location;\n\tint index;\n\tint ret = 0;\n\n\tif (dentry->d_name.len > BTRFS_NAME_LEN)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tif (unlikely(d_need_lookup(dentry))) {\n\t\tmemcpy(&location, dentry->d_fsdata, sizeof(struct btrfs_key));\n\t\tkfree(dentry->d_fsdata);\n\t\tdentry->d_fsdata = NULL;\n\t\t/* This thing is hashed, drop it for now */\n\t\td_drop(dentry);\n\t} else {\n\t\tret = btrfs_inode_by_name(dir, dentry, &location);\n\t}\n\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tif (location.objectid == 0)\n\t\treturn NULL;\n\n\tif (location.type == BTRFS_INODE_ITEM_KEY) {\n\t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n\t\treturn inode;\n\t}\n\n\tBUG_ON(location.type != BTRFS_ROOT_ITEM_KEY);\n\n\tindex = srcu_read_lock(&root->fs_info->subvol_srcu);\n\tret = fixup_tree_root_location(root, dir, dentry,\n\t\t\t\t       &location, &sub_root);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT)\n\t\t\tinode = ERR_PTR(ret);\n\t\telse\n\t\t\tinode = new_simple_dir(dir->i_sb, &location, sub_root);\n\t} else {\n\t\tinode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n\t}\n\tsrcu_read_unlock(&root->fs_info->subvol_srcu, index);\n\n\tif (!IS_ERR(inode) && root != sub_root) {\n\t\tdown_read(&root->fs_info->cleanup_work_sem);\n\t\tif (!(inode->i_sb->s_flags & MS_RDONLY))\n\t\t\tret = btrfs_orphan_cleanup(sub_root);\n\t\tup_read(&root->fs_info->cleanup_work_sem);\n\t\tif (ret)\n\t\t\tinode = ERR_PTR(ret);\n\t}\n\n\treturn inode;\n}\n\nstatic int btrfs_dentry_delete(const struct dentry *dentry)\n{\n\tstruct btrfs_root *root;\n\tstruct inode *inode = dentry->d_inode;\n\n\tif (!inode && !IS_ROOT(dentry))\n\t\tinode = dentry->d_parent->d_inode;\n\n\tif (inode) {\n\t\troot = BTRFS_I(inode)->root;\n\t\tif (btrfs_root_refs(&root->root_item) == 0)\n\t\t\treturn 1;\n\n\t\tif (btrfs_ino(inode) == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void btrfs_dentry_release(struct dentry *dentry)\n{\n\tif (dentry->d_fsdata)\n\t\tkfree(dentry->d_fsdata);\n}\n\nstatic struct dentry *btrfs_lookup(struct inode *dir, struct dentry *dentry,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct dentry *ret;\n\n\tret = d_splice_alias(btrfs_lookup_dentry(dir, dentry), dentry);\n\tif (unlikely(d_need_lookup(dentry))) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_NEED_LOOKUP;\n\t\tspin_unlock(&dentry->d_lock);\n\t}\n\treturn ret;\n}\n\nunsigned char btrfs_filetype_table[] = {\n\tDT_UNKNOWN, DT_REG, DT_DIR, DT_CHR, DT_BLK, DT_FIFO, DT_SOCK, DT_LNK\n};\n\nstatic int btrfs_real_readdir(struct file *filp, void *dirent,\n\t\t\t      filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_dentry->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_item *item;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_path *path;\n\tstruct list_head ins_list;\n\tstruct list_head del_list;\n\tint ret;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tunsigned char d_type;\n\tint over = 0;\n\tu32 di_cur;\n\tu32 di_total;\n\tu32 di_len;\n\tint key_type = BTRFS_DIR_INDEX_KEY;\n\tchar tmp_name[32];\n\tchar *name_ptr;\n\tint name_len;\n\tint is_curr = 0;\t/* filp->f_pos points to the current index? */\n\n\t/* FIXME, use a real flag for deciding about the key type */\n\tif (root->fs_info->tree_root == root)\n\t\tkey_type = BTRFS_DIR_ITEM_KEY;\n\n\t/* special case for \".\" */\n\tif (filp->f_pos == 0) {\n\t\tover = filldir(dirent, \".\", 1,\n\t\t\t       filp->f_pos, btrfs_ino(inode), DT_DIR);\n\t\tif (over)\n\t\t\treturn 0;\n\t\tfilp->f_pos = 1;\n\t}\n\t/* special case for .., just use the back ref */\n\tif (filp->f_pos == 1) {\n\t\tu64 pino = parent_ino(filp->f_path.dentry);\n\t\tover = filldir(dirent, \"..\", 2,\n\t\t\t       filp->f_pos, pino, DT_DIR);\n\t\tif (over)\n\t\t\treturn 0;\n\t\tfilp->f_pos = 2;\n\t}\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = 1;\n\n\tif (key_type == BTRFS_DIR_INDEX_KEY) {\n\t\tINIT_LIST_HEAD(&ins_list);\n\t\tINIT_LIST_HEAD(&del_list);\n\t\tbtrfs_get_delayed_items(inode, &ins_list, &del_list);\n\t}\n\n\tbtrfs_set_key_type(&key, key_type);\n\tkey.offset = filp->f_pos;\n\tkey.objectid = btrfs_ino(inode);\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto err;\n\n\twhile (1) {\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto err;\n\t\t\telse if (ret > 0)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\titem = btrfs_item_nr(leaf, slot);\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid != key.objectid)\n\t\t\tbreak;\n\t\tif (btrfs_key_type(&found_key) != key_type)\n\t\t\tbreak;\n\t\tif (found_key.offset < filp->f_pos)\n\t\t\tgoto next;\n\t\tif (key_type == BTRFS_DIR_INDEX_KEY &&\n\t\t    btrfs_should_delete_dir_index(&del_list,\n\t\t\t\t\t\t  found_key.offset))\n\t\t\tgoto next;\n\n\t\tfilp->f_pos = found_key.offset;\n\t\tis_curr = 1;\n\n\t\tdi = btrfs_item_ptr(leaf, slot, struct btrfs_dir_item);\n\t\tdi_cur = 0;\n\t\tdi_total = btrfs_item_size(leaf, item);\n\n\t\twhile (di_cur < di_total) {\n\t\t\tstruct btrfs_key location;\n\n\t\t\tif (verify_dir_item(root, leaf, di))\n\t\t\t\tbreak;\n\n\t\t\tname_len = btrfs_dir_name_len(leaf, di);\n\t\t\tif (name_len <= sizeof(tmp_name)) {\n\t\t\t\tname_ptr = tmp_name;\n\t\t\t} else {\n\t\t\t\tname_ptr = kmalloc(name_len, GFP_NOFS);\n\t\t\t\tif (!name_ptr) {\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t}\n\t\t\tread_extent_buffer(leaf, name_ptr,\n\t\t\t\t\t   (unsigned long)(di + 1), name_len);\n\n\t\t\td_type = btrfs_filetype_table[btrfs_dir_type(leaf, di)];\n\t\t\tbtrfs_dir_item_key_to_cpu(leaf, di, &location);\n\n\n\t\t\t/* is this a reference to our own snapshot? If so\n\t\t\t * skip it.\n\t\t\t *\n\t\t\t * In contrast to old kernels, we insert the snapshot's\n\t\t\t * dir item and dir index after it has been created, so\n\t\t\t * we won't find a reference to our own snapshot. We\n\t\t\t * still keep the following code for backward\n\t\t\t * compatibility.\n\t\t\t */\n\t\t\tif (location.type == BTRFS_ROOT_ITEM_KEY &&\n\t\t\t    location.objectid == root->root_key.objectid) {\n\t\t\t\tover = 0;\n\t\t\t\tgoto skip;\n\t\t\t}\n\t\t\tover = filldir(dirent, name_ptr, name_len,\n\t\t\t\t       found_key.offset, location.objectid,\n\t\t\t\t       d_type);\n\nskip:\n\t\t\tif (name_ptr != tmp_name)\n\t\t\t\tkfree(name_ptr);\n\n\t\t\tif (over)\n\t\t\t\tgoto nopos;\n\t\t\tdi_len = btrfs_dir_name_len(leaf, di) +\n\t\t\t\t btrfs_dir_data_len(leaf, di) + sizeof(*di);\n\t\t\tdi_cur += di_len;\n\t\t\tdi = (struct btrfs_dir_item *)((char *)di + di_len);\n\t\t}\nnext:\n\t\tpath->slots[0]++;\n\t}\n\n\tif (key_type == BTRFS_DIR_INDEX_KEY) {\n\t\tif (is_curr)\n\t\t\tfilp->f_pos++;\n\t\tret = btrfs_readdir_delayed_dir_index(filp, dirent, filldir,\n\t\t\t\t\t\t      &ins_list);\n\t\tif (ret)\n\t\t\tgoto nopos;\n\t}\n\n\t/* Reached end of directory/root. Bump pos past the last item. */\n\tif (key_type == BTRFS_DIR_INDEX_KEY)\n\t\t/*\n\t\t * 32-bit glibc will use getdents64, but then strtol -\n\t\t * so the last number we can serve is this.\n\t\t */\n\t\tfilp->f_pos = 0x7fffffff;\n\telse\n\t\tfilp->f_pos++;\nnopos:\n\tret = 0;\nerr:\n\tif (key_type == BTRFS_DIR_INDEX_KEY)\n\t\tbtrfs_put_delayed_items(&ins_list, &del_list);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_write_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tint ret = 0;\n\tbool nolock = false;\n\n\tif (test_bit(BTRFS_INODE_DUMMY, &BTRFS_I(inode)->runtime_flags))\n\t\treturn 0;\n\n\tif (btrfs_fs_closing(root->fs_info) && btrfs_is_free_space_inode(inode))\n\t\tnolock = true;\n\n\tif (wbc->sync_mode == WB_SYNC_ALL) {\n\t\tif (nolock)\n\t\t\ttrans = btrfs_join_transaction_nolock(root);\n\t\telse\n\t\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\t\tret = btrfs_commit_transaction(trans, root);\n\t}\n\treturn ret;\n}\n\n/*\n * This is somewhat expensive, updating the tree every time the\n * inode changes.  But, it is most likely to find the inode in cache.\n * FIXME, needs more benchmarking...there are no reasons other than performance\n * to keep or drop this code.\n */\nint btrfs_dirty_inode(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\tif (test_bit(BTRFS_INODE_DUMMY, &BTRFS_I(inode)->runtime_flags))\n\t\treturn 0;\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tret = btrfs_update_inode(trans, root, inode);\n\tif (ret && ret == -ENOSPC) {\n\t\t/* whoops, lets try again with the full transaction */\n\t\tbtrfs_end_transaction(trans, root);\n\t\ttrans = btrfs_start_transaction(root, 1);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t}\n\tbtrfs_end_transaction(trans, root);\n\tif (BTRFS_I(inode)->delayed_node)\n\t\tbtrfs_balance_delayed_items(root);\n\n\treturn ret;\n}\n\n/*\n * This is a copy of file_update_time.  We need this so we can return error on\n * ENOSPC for updating the inode in the case of file write and mmap writes.\n */\nstatic int btrfs_update_time(struct inode *inode, struct timespec *now,\n\t\t\t     int flags)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tif (flags & S_VERSION)\n\t\tinode_inc_iversion(inode);\n\tif (flags & S_CTIME)\n\t\tinode->i_ctime = *now;\n\tif (flags & S_MTIME)\n\t\tinode->i_mtime = *now;\n\tif (flags & S_ATIME)\n\t\tinode->i_atime = *now;\n\treturn btrfs_dirty_inode(inode);\n}\n\n/*\n * find the highest existing sequence number in a directory\n * and then set the in-memory index_cnt variable to reflect\n * free sequence numbers\n */\nstatic int btrfs_set_inode_index_count(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_key key, found_key;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tint ret;\n\n\tkey.objectid = btrfs_ino(inode);\n\tbtrfs_set_key_type(&key, BTRFS_DIR_INDEX_KEY);\n\tkey.offset = (u64)-1;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\t/* FIXME: we should be able to handle this */\n\tif (ret == 0)\n\t\tgoto out;\n\tret = 0;\n\n\t/*\n\t * MAGIC NUMBER EXPLANATION:\n\t * since we search a directory based on f_pos we have to start at 2\n\t * since '.' and '..' have f_pos of 0 and 1 respectively, so everybody\n\t * else has to start at 2\n\t */\n\tif (path->slots[0] == 0) {\n\t\tBTRFS_I(inode)->index_cnt = 2;\n\t\tgoto out;\n\t}\n\n\tpath->slots[0]--;\n\n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\tif (found_key.objectid != btrfs_ino(inode) ||\n\t    btrfs_key_type(&found_key) != BTRFS_DIR_INDEX_KEY) {\n\t\tBTRFS_I(inode)->index_cnt = 2;\n\t\tgoto out;\n\t}\n\n\tBTRFS_I(inode)->index_cnt = found_key.offset + 1;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * helper to find a free sequence number in a given directory.  This current\n * code is very simple, later versions will do smarter things in the btree\n */\nint btrfs_set_inode_index(struct inode *dir, u64 *index)\n{\n\tint ret = 0;\n\n\tif (BTRFS_I(dir)->index_cnt == (u64)-1) {\n\t\tret = btrfs_inode_delayed_dir_index_count(dir);\n\t\tif (ret) {\n\t\t\tret = btrfs_set_inode_index_count(dir);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\t*index = BTRFS_I(dir)->index_cnt;\n\tBTRFS_I(dir)->index_cnt++;\n\n\treturn ret;\n}\n\nstatic struct inode *btrfs_new_inode(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t     struct inode *dir,\n\t\t\t\t     const char *name, int name_len,\n\t\t\t\t     u64 ref_objectid, u64 objectid,\n\t\t\t\t     umode_t mode, u64 *index)\n{\n\tstruct inode *inode;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct btrfs_key *location;\n\tstruct btrfs_path *path;\n\tstruct btrfs_inode_ref *ref;\n\tstruct btrfs_key key[2];\n\tu32 sizes[2];\n\tunsigned long ptr;\n\tint ret;\n\tint owner;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinode = new_inode(root->fs_info->sb);\n\tif (!inode) {\n\t\tbtrfs_free_path(path);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/*\n\t * we have to initialize this early, so we can reclaim the inode\n\t * number if we fail afterwards in this function.\n\t */\n\tinode->i_ino = objectid;\n\n\tif (dir) {\n\t\ttrace_btrfs_inode_request(dir);\n\n\t\tret = btrfs_set_inode_index(dir, index);\n\t\tif (ret) {\n\t\t\tbtrfs_free_path(path);\n\t\t\tiput(inode);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\t/*\n\t * index_cnt is ignored for everything but a dir,\n\t * btrfs_get_inode_index_count has an explanation for the magic\n\t * number\n\t */\n\tBTRFS_I(inode)->index_cnt = 2;\n\tBTRFS_I(inode)->root = root;\n\tBTRFS_I(inode)->generation = trans->transid;\n\tinode->i_generation = BTRFS_I(inode)->generation;\n\n\t/*\n\t * We could have gotten an inode number from somebody who was fsynced\n\t * and then removed in this same transaction, so let's just set full\n\t * sync since it will be a full sync anyway and this will blow away the\n\t * old info in the log.\n\t */\n\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &BTRFS_I(inode)->runtime_flags);\n\n\tif (S_ISDIR(mode))\n\t\towner = 0;\n\telse\n\t\towner = 1;\n\n\tkey[0].objectid = objectid;\n\tbtrfs_set_key_type(&key[0], BTRFS_INODE_ITEM_KEY);\n\tkey[0].offset = 0;\n\n\t/*\n\t * Start new inodes with an inode_ref. This is slightly more\n\t * efficient for small numbers of hard links since they will\n\t * be packed into one item. Extended refs will kick in if we\n\t * add more hard links than can fit in the ref item.\n\t */\n\tkey[1].objectid = objectid;\n\tbtrfs_set_key_type(&key[1], BTRFS_INODE_REF_KEY);\n\tkey[1].offset = ref_objectid;\n\n\tsizes[0] = sizeof(struct btrfs_inode_item);\n\tsizes[1] = name_len + sizeof(*ref);\n\n\tpath->leave_spinning = 1;\n\tret = btrfs_insert_empty_items(trans, root, path, key, sizes, 2);\n\tif (ret != 0)\n\t\tgoto fail;\n\n\tinode_init_owner(inode, dir, mode);\n\tinode_set_bytes(inode, 0);\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;\n\tinode_item = btrfs_item_ptr(path->nodes[0], path->slots[0],\n\t\t\t\t  struct btrfs_inode_item);\n\tmemset_extent_buffer(path->nodes[0], 0, (unsigned long)inode_item,\n\t\t\t     sizeof(*inode_item));\n\tfill_inode_item(trans, path->nodes[0], inode_item, inode);\n\n\tref = btrfs_item_ptr(path->nodes[0], path->slots[0] + 1,\n\t\t\t     struct btrfs_inode_ref);\n\tbtrfs_set_inode_ref_name_len(path->nodes[0], ref, name_len);\n\tbtrfs_set_inode_ref_index(path->nodes[0], ref, *index);\n\tptr = (unsigned long)(ref + 1);\n\twrite_extent_buffer(path->nodes[0], name, ptr, name_len);\n\n\tbtrfs_mark_buffer_dirty(path->nodes[0]);\n\tbtrfs_free_path(path);\n\n\tlocation = &BTRFS_I(inode)->location;\n\tlocation->objectid = objectid;\n\tlocation->offset = 0;\n\tbtrfs_set_key_type(location, BTRFS_INODE_ITEM_KEY);\n\n\tbtrfs_inherit_iflags(inode, dir);\n\n\tif (S_ISREG(mode)) {\n\t\tif (btrfs_test_opt(root, NODATASUM))\n\t\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NODATASUM;\n\t\tif (btrfs_test_opt(root, NODATACOW) ||\n\t\t    (BTRFS_I(dir)->flags & BTRFS_INODE_NODATACOW))\n\t\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NODATACOW;\n\t}\n\n\tinsert_inode_hash(inode);\n\tinode_tree_add(inode);\n\n\ttrace_btrfs_inode_new(inode);\n\tbtrfs_set_inode_last_trans(trans, inode);\n\n\tbtrfs_update_root_times(trans, root);\n\n\treturn inode;\nfail:\n\tif (dir)\n\t\tBTRFS_I(dir)->index_cnt--;\n\tbtrfs_free_path(path);\n\tiput(inode);\n\treturn ERR_PTR(ret);\n}\n\nstatic inline u8 btrfs_inode_type(struct inode *inode)\n{\n\treturn btrfs_type_by_mode[(inode->i_mode & S_IFMT) >> S_SHIFT];\n}\n\n/*\n * utility function to add 'inode' into 'parent_inode' with\n * a give name and a given sequence number.\n * if 'add_backref' is true, also insert a backref from the\n * inode to the parent directory.\n */\nint btrfs_add_link(struct btrfs_trans_handle *trans,\n\t\t   struct inode *parent_inode, struct inode *inode,\n\t\t   const char *name, int name_len, int add_backref, u64 index)\n{\n\tint ret = 0;\n\tstruct btrfs_key key;\n\tstruct btrfs_root *root = BTRFS_I(parent_inode)->root;\n\tu64 ino = btrfs_ino(inode);\n\tu64 parent_ino = btrfs_ino(parent_inode);\n\n\tif (unlikely(ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\tmemcpy(&key, &BTRFS_I(inode)->root->root_key, sizeof(key));\n\t} else {\n\t\tkey.objectid = ino;\n\t\tbtrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);\n\t\tkey.offset = 0;\n\t}\n\n\tif (unlikely(ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\tret = btrfs_add_root_ref(trans, root->fs_info->tree_root,\n\t\t\t\t\t key.objectid, root->root_key.objectid,\n\t\t\t\t\t parent_ino, index, name, name_len);\n\t} else if (add_backref) {\n\t\tret = btrfs_insert_inode_ref(trans, root, name, name_len, ino,\n\t\t\t\t\t     parent_ino, index);\n\t}\n\n\t/* Nothing to clean up yet */\n\tif (ret)\n\t\treturn ret;\n\n\tret = btrfs_insert_dir_item(trans, root, name, name_len,\n\t\t\t\t    parent_inode, &key,\n\t\t\t\t    btrfs_inode_type(inode), index);\n\tif (ret == -EEXIST)\n\t\tgoto fail_dir_item;\n\telse if (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\treturn ret;\n\t}\n\n\tbtrfs_i_size_write(parent_inode, parent_inode->i_size +\n\t\t\t   name_len * 2);\n\tinode_inc_iversion(parent_inode);\n\tparent_inode->i_mtime = parent_inode->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode(trans, root, parent_inode);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\treturn ret;\n\nfail_dir_item:\n\tif (unlikely(ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\tu64 local_index;\n\t\tint err;\n\t\terr = btrfs_del_root_ref(trans, root->fs_info->tree_root,\n\t\t\t\t key.objectid, root->root_key.objectid,\n\t\t\t\t parent_ino, &local_index, name, name_len);\n\n\t} else if (add_backref) {\n\t\tu64 local_index;\n\t\tint err;\n\n\t\terr = btrfs_del_inode_ref(trans, root, name, name_len,\n\t\t\t\t\t  ino, parent_ino, &local_index);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_add_nondir(struct btrfs_trans_handle *trans,\n\t\t\t    struct inode *dir, struct dentry *dentry,\n\t\t\t    struct inode *inode, int backref, u64 index)\n{\n\tint err = btrfs_add_link(trans, dir, inode,\n\t\t\t\t dentry->d_name.name, dentry->d_name.len,\n\t\t\t\t backref, index);\n\tif (err > 0)\n\t\terr = -EEXIST;\n\treturn err;\n}\n\nstatic int btrfs_mknod(struct inode *dir, struct dentry *dentry,\n\t\t\tumode_t mode, dev_t rdev)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct inode *inode = NULL;\n\tint err;\n\tint drop_inode = 0;\n\tu64 objectid;\n\tu64 index = 0;\n\n\tif (!new_valid_dev(rdev))\n\t\treturn -EINVAL;\n\n\t/*\n\t * 2 for inode item and ref\n\t * 2 for dir items\n\t * 1 for xattr if selinux is on\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\terr = btrfs_find_free_ino(root, &objectid);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tinode = btrfs_new_inode(trans, root, dir, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len, btrfs_ino(dir), objectid,\n\t\t\t\tmode, &index);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\tgoto out_unlock;\n\t}\n\n\terr = btrfs_init_inode_security(trans, inode, dir, &dentry->d_name);\n\tif (err) {\n\t\tdrop_inode = 1;\n\t\tgoto out_unlock;\n\t}\n\n\terr = btrfs_update_inode(trans, root, inode);\n\tif (err) {\n\t\tdrop_inode = 1;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t* If the active LSM wants to access the inode during\n\t* d_instantiate it needs these. Smack checks to see\n\t* if the filesystem supports xattrs by looking at the\n\t* ops vector.\n\t*/\n\n\tinode->i_op = &btrfs_special_inode_operations;\n\terr = btrfs_add_nondir(trans, dir, dentry, inode, 0, index);\n\tif (err)\n\t\tdrop_inode = 1;\n\telse {\n\t\tinit_special_inode(inode, inode->i_mode, rdev);\n\t\tbtrfs_update_inode(trans, root, inode);\n\t\td_instantiate(dentry, inode);\n\t}\nout_unlock:\n\tbtrfs_end_transaction(trans, root);\n\tbtrfs_btree_balance_dirty(root);\n\tif (drop_inode) {\n\t\tinode_dec_link_count(inode);\n\t\tiput(inode);\n\t}\n\treturn err;\n}\n\nstatic int btrfs_create(struct inode *dir, struct dentry *dentry,\n\t\t\tumode_t mode, bool excl)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct inode *inode = NULL;\n\tint drop_inode_on_err = 0;\n\tint err;\n\tu64 objectid;\n\tu64 index = 0;\n\n\t/*\n\t * 2 for inode item and ref\n\t * 2 for dir items\n\t * 1 for xattr if selinux is on\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\terr = btrfs_find_free_ino(root, &objectid);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tinode = btrfs_new_inode(trans, root, dir, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len, btrfs_ino(dir), objectid,\n\t\t\t\tmode, &index);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\tgoto out_unlock;\n\t}\n\tdrop_inode_on_err = 1;\n\n\terr = btrfs_init_inode_security(trans, inode, dir, &dentry->d_name);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = btrfs_update_inode(trans, root, inode);\n\tif (err)\n\t\tgoto out_unlock;\n\n\t/*\n\t* If the active LSM wants to access the inode during\n\t* d_instantiate it needs these. Smack checks to see\n\t* if the filesystem supports xattrs by looking at the\n\t* ops vector.\n\t*/\n\tinode->i_fop = &btrfs_file_operations;\n\tinode->i_op = &btrfs_file_inode_operations;\n\n\terr = btrfs_add_nondir(trans, dir, dentry, inode, 0, index);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tinode->i_mapping->a_ops = &btrfs_aops;\n\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\tBTRFS_I(inode)->io_tree.ops = &btrfs_extent_io_ops;\n\td_instantiate(dentry, inode);\n\nout_unlock:\n\tbtrfs_end_transaction(trans, root);\n\tif (err && drop_inode_on_err) {\n\t\tinode_dec_link_count(inode);\n\t\tiput(inode);\n\t}\n\tbtrfs_btree_balance_dirty(root);\n\treturn err;\n}\n\nstatic int btrfs_link(struct dentry *old_dentry, struct inode *dir,\n\t\t      struct dentry *dentry)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct inode *inode = old_dentry->d_inode;\n\tu64 index;\n\tint err;\n\tint drop_inode = 0;\n\n\t/* do not allow sys_link's with other subvols of the same device */\n\tif (root->objectid != BTRFS_I(inode)->root->objectid)\n\t\treturn -EXDEV;\n\n\tif (inode->i_nlink >= BTRFS_LINK_MAX)\n\t\treturn -EMLINK;\n\n\terr = btrfs_set_inode_index(dir, &index);\n\tif (err)\n\t\tgoto fail;\n\n\t/*\n\t * 2 items for inode and inode ref\n\t * 2 items for dir items\n\t * 1 item for parent inode\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans)) {\n\t\terr = PTR_ERR(trans);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_inc_nlink(inode);\n\tinode_inc_iversion(inode);\n\tinode->i_ctime = CURRENT_TIME;\n\tihold(inode);\n\tset_bit(BTRFS_INODE_COPY_EVERYTHING, &BTRFS_I(inode)->runtime_flags);\n\n\terr = btrfs_add_nondir(trans, dir, dentry, inode, 1, index);\n\n\tif (err) {\n\t\tdrop_inode = 1;\n\t} else {\n\t\tstruct dentry *parent = dentry->d_parent;\n\t\terr = btrfs_update_inode(trans, root, inode);\n\t\tif (err)\n\t\t\tgoto fail;\n\t\td_instantiate(dentry, inode);\n\t\tbtrfs_log_new_name(trans, inode, NULL, parent);\n\t}\n\n\tbtrfs_end_transaction(trans, root);\nfail:\n\tif (drop_inode) {\n\t\tinode_dec_link_count(inode);\n\t\tiput(inode);\n\t}\n\tbtrfs_btree_balance_dirty(root);\n\treturn err;\n}\n\nstatic int btrfs_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)\n{\n\tstruct inode *inode = NULL;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tint err = 0;\n\tint drop_on_err = 0;\n\tu64 objectid = 0;\n\tu64 index = 0;\n\n\t/*\n\t * 2 items for inode and ref\n\t * 2 items for dir items\n\t * 1 for xattr if selinux is on\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\terr = btrfs_find_free_ino(root, &objectid);\n\tif (err)\n\t\tgoto out_fail;\n\n\tinode = btrfs_new_inode(trans, root, dir, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len, btrfs_ino(dir), objectid,\n\t\t\t\tS_IFDIR | mode, &index);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\tgoto out_fail;\n\t}\n\n\tdrop_on_err = 1;\n\n\terr = btrfs_init_inode_security(trans, inode, dir, &dentry->d_name);\n\tif (err)\n\t\tgoto out_fail;\n\n\tinode->i_op = &btrfs_dir_inode_operations;\n\tinode->i_fop = &btrfs_dir_file_operations;\n\n\tbtrfs_i_size_write(inode, 0);\n\terr = btrfs_update_inode(trans, root, inode);\n\tif (err)\n\t\tgoto out_fail;\n\n\terr = btrfs_add_link(trans, dir, inode, dentry->d_name.name,\n\t\t\t     dentry->d_name.len, 0, index);\n\tif (err)\n\t\tgoto out_fail;\n\n\td_instantiate(dentry, inode);\n\tdrop_on_err = 0;\n\nout_fail:\n\tbtrfs_end_transaction(trans, root);\n\tif (drop_on_err)\n\t\tiput(inode);\n\tbtrfs_btree_balance_dirty(root);\n\treturn err;\n}\n\n/* helper for btfs_get_extent.  Given an existing extent in the tree,\n * and an extent that you want to insert, deal with overlap and insert\n * the new extent into the tree.\n */\nstatic int merge_extent_mapping(struct extent_map_tree *em_tree,\n\t\t\t\tstruct extent_map *existing,\n\t\t\t\tstruct extent_map *em,\n\t\t\t\tu64 map_start, u64 map_len)\n{\n\tu64 start_diff;\n\n\tBUG_ON(map_start < em->start || map_start >= extent_map_end(em));\n\tstart_diff = map_start - em->start;\n\tem->start = map_start;\n\tem->len = map_len;\n\tif (em->block_start < EXTENT_MAP_LAST_BYTE &&\n\t    !test_bit(EXTENT_FLAG_COMPRESSED, &em->flags)) {\n\t\tem->block_start += start_diff;\n\t\tem->block_len -= start_diff;\n\t}\n\treturn add_extent_mapping(em_tree, em);\n}\n\nstatic noinline int uncompress_inline(struct btrfs_path *path,\n\t\t\t\t      struct inode *inode, struct page *page,\n\t\t\t\t      size_t pg_offset, u64 extent_offset,\n\t\t\t\t      struct btrfs_file_extent_item *item)\n{\n\tint ret;\n\tstruct extent_buffer *leaf = path->nodes[0];\n\tchar *tmp;\n\tsize_t max_size;\n\tunsigned long inline_size;\n\tunsigned long ptr;\n\tint compress_type;\n\n\tWARN_ON(pg_offset != 0);\n\tcompress_type = btrfs_file_extent_compression(leaf, item);\n\tmax_size = btrfs_file_extent_ram_bytes(leaf, item);\n\tinline_size = btrfs_file_extent_inline_item_len(leaf,\n\t\t\t\t\tbtrfs_item_nr(leaf, path->slots[0]));\n\ttmp = kmalloc(inline_size, GFP_NOFS);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\tptr = btrfs_file_extent_inline_start(item);\n\n\tread_extent_buffer(leaf, tmp, ptr, inline_size);\n\n\tmax_size = min_t(unsigned long, PAGE_CACHE_SIZE, max_size);\n\tret = btrfs_decompress(compress_type, tmp, page,\n\t\t\t       extent_offset, inline_size, max_size);\n\tif (ret) {\n\t\tchar *kaddr = kmap_atomic(page);\n\t\tunsigned long copy_size = min_t(u64,\n\t\t\t\t  PAGE_CACHE_SIZE - pg_offset,\n\t\t\t\t  max_size - extent_offset);\n\t\tmemset(kaddr + pg_offset, 0, copy_size);\n\t\tkunmap_atomic(kaddr);\n\t}\n\tkfree(tmp);\n\treturn 0;\n}\n\n/*\n * a bit scary, this does extent mapping from logical file offset to the disk.\n * the ugly parts come from merging extents from the disk with the in-ram\n * representation.  This gets more complex because of the data=ordered code,\n * where the in-ram extents might be locked pending data=ordered completion.\n *\n * This also copies inline extents directly into the page.\n */\n\nstruct extent_map *btrfs_get_extent(struct inode *inode, struct page *page,\n\t\t\t\t    size_t pg_offset, u64 start, u64 len,\n\t\t\t\t    int create)\n{\n\tint ret;\n\tint err = 0;\n\tu64 bytenr;\n\tu64 extent_start = 0;\n\tu64 extent_end = 0;\n\tu64 objectid = btrfs_ino(inode);\n\tu32 found_type;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_file_extent_item *item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key found_key;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tint compress_type;\n\nagain:\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tif (em)\n\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tif (em->start > start || em->start + em->len <= start)\n\t\t\tfree_extent_map(em);\n\t\telse if (em->block_start == EXTENT_MAP_INLINE && page)\n\t\t\tfree_extent_map(em);\n\t\telse\n\t\t\tgoto out;\n\t}\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\tem->start = EXTENT_MAP_HOLE;\n\tem->orig_start = EXTENT_MAP_HOLE;\n\tem->len = (u64)-1;\n\tem->block_len = (u64)-1;\n\n\tif (!path) {\n\t\tpath = btrfs_alloc_path();\n\t\tif (!path) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * Chances are we'll be called again, so go ahead and do\n\t\t * readahead\n\t\t */\n\t\tpath->reada = 1;\n\t}\n\n\tret = btrfs_lookup_file_extent(trans, root, path,\n\t\t\t\t       objectid, start, trans != NULL);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tif (ret != 0) {\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto not_found;\n\t\tpath->slots[0]--;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t      struct btrfs_file_extent_item);\n\t/* are we inside the extent that was found? */\n\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\tfound_type = btrfs_key_type(&found_key);\n\tif (found_key.objectid != objectid ||\n\t    found_type != BTRFS_EXTENT_DATA_KEY) {\n\t\tgoto not_found;\n\t}\n\n\tfound_type = btrfs_file_extent_type(leaf, item);\n\textent_start = found_key.offset;\n\tcompress_type = btrfs_file_extent_compression(leaf, item);\n\tif (found_type == BTRFS_FILE_EXTENT_REG ||\n\t    found_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\textent_end = extent_start +\n\t\t       btrfs_file_extent_num_bytes(leaf, item);\n\t} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tsize_t size;\n\t\tsize = btrfs_file_extent_inline_len(leaf, item);\n\t\textent_end = (extent_start + size + root->sectorsize - 1) &\n\t\t\t~((u64)root->sectorsize - 1);\n\t}\n\n\tif (start >= extent_end) {\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\terr = ret;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (ret > 0)\n\t\t\t\tgoto not_found;\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tif (found_key.objectid != objectid ||\n\t\t    found_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto not_found;\n\t\tif (start + len <= found_key.offset)\n\t\t\tgoto not_found;\n\t\tem->start = start;\n\t\tem->orig_start = start;\n\t\tem->len = found_key.offset - start;\n\t\tgoto not_found_em;\n\t}\n\n\tif (found_type == BTRFS_FILE_EXTENT_REG ||\n\t    found_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\tem->start = extent_start;\n\t\tem->len = extent_end - extent_start;\n\t\tem->orig_start = extent_start -\n\t\t\t\t btrfs_file_extent_offset(leaf, item);\n\t\tem->orig_block_len = btrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t      item);\n\t\tbytenr = btrfs_file_extent_disk_bytenr(leaf, item);\n\t\tif (bytenr == 0) {\n\t\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\t\tgoto insert;\n\t\t}\n\t\tif (compress_type != BTRFS_COMPRESS_NONE) {\n\t\t\tset_bit(EXTENT_FLAG_COMPRESSED, &em->flags);\n\t\t\tem->compress_type = compress_type;\n\t\t\tem->block_start = bytenr;\n\t\t\tem->block_len = em->orig_block_len;\n\t\t} else {\n\t\t\tbytenr += btrfs_file_extent_offset(leaf, item);\n\t\t\tem->block_start = bytenr;\n\t\t\tem->block_len = em->len;\n\t\t\tif (found_type == BTRFS_FILE_EXTENT_PREALLOC)\n\t\t\t\tset_bit(EXTENT_FLAG_PREALLOC, &em->flags);\n\t\t}\n\t\tgoto insert;\n\t} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tunsigned long ptr;\n\t\tchar *map;\n\t\tsize_t size;\n\t\tsize_t extent_offset;\n\t\tsize_t copy_size;\n\n\t\tem->block_start = EXTENT_MAP_INLINE;\n\t\tif (!page || create) {\n\t\t\tem->start = extent_start;\n\t\t\tem->len = extent_end - extent_start;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsize = btrfs_file_extent_inline_len(leaf, item);\n\t\textent_offset = page_offset(page) + pg_offset - extent_start;\n\t\tcopy_size = min_t(u64, PAGE_CACHE_SIZE - pg_offset,\n\t\t\t\tsize - extent_offset);\n\t\tem->start = extent_start + extent_offset;\n\t\tem->len = (copy_size + root->sectorsize - 1) &\n\t\t\t~((u64)root->sectorsize - 1);\n\t\tem->orig_block_len = em->len;\n\t\tem->orig_start = em->start;\n\t\tif (compress_type) {\n\t\t\tset_bit(EXTENT_FLAG_COMPRESSED, &em->flags);\n\t\t\tem->compress_type = compress_type;\n\t\t}\n\t\tptr = btrfs_file_extent_inline_start(item) + extent_offset;\n\t\tif (create == 0 && !PageUptodate(page)) {\n\t\t\tif (btrfs_file_extent_compression(leaf, item) !=\n\t\t\t    BTRFS_COMPRESS_NONE) {\n\t\t\t\tret = uncompress_inline(path, inode, page,\n\t\t\t\t\t\t\tpg_offset,\n\t\t\t\t\t\t\textent_offset, item);\n\t\t\t\tBUG_ON(ret); /* -ENOMEM */\n\t\t\t} else {\n\t\t\t\tmap = kmap(page);\n\t\t\t\tread_extent_buffer(leaf, map + pg_offset, ptr,\n\t\t\t\t\t\t   copy_size);\n\t\t\t\tif (pg_offset + copy_size < PAGE_CACHE_SIZE) {\n\t\t\t\t\tmemset(map + pg_offset + copy_size, 0,\n\t\t\t\t\t       PAGE_CACHE_SIZE - pg_offset -\n\t\t\t\t\t       copy_size);\n\t\t\t\t}\n\t\t\t\tkunmap(page);\n\t\t\t}\n\t\t\tflush_dcache_page(page);\n\t\t} else if (create && PageUptodate(page)) {\n\t\t\tBUG();\n\t\t\tif (!trans) {\n\t\t\t\tkunmap(page);\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tem = NULL;\n\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\ttrans = btrfs_join_transaction(root);\n\n\t\t\t\tif (IS_ERR(trans))\n\t\t\t\t\treturn ERR_CAST(trans);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\tmap = kmap(page);\n\t\t\twrite_extent_buffer(leaf, map + pg_offset, ptr,\n\t\t\t\t\t    copy_size);\n\t\t\tkunmap(page);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\t\tset_extent_uptodate(io_tree, em->start,\n\t\t\t\t    extent_map_end(em) - 1, NULL, GFP_NOFS);\n\t\tgoto insert;\n\t} else {\n\t\tWARN(1, KERN_ERR \"btrfs unknown found_type %d\\n\", found_type);\n\t}\nnot_found:\n\tem->start = start;\n\tem->orig_start = start;\n\tem->len = len;\nnot_found_em:\n\tem->block_start = EXTENT_MAP_HOLE;\n\tset_bit(EXTENT_FLAG_VACANCY, &em->flags);\ninsert:\n\tbtrfs_release_path(path);\n\tif (em->start > start || extent_map_end(em) <= start) {\n\t\tprintk(KERN_ERR \"Btrfs: bad extent! em: [%llu %llu] passed \"\n\t\t       \"[%llu %llu]\\n\", (unsigned long long)em->start,\n\t\t       (unsigned long long)em->len,\n\t\t       (unsigned long long)start,\n\t\t       (unsigned long long)len);\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em);\n\t/* it is possible that someone inserted the extent into the tree\n\t * while we had the lock dropped.  It is also possible that\n\t * an overlapping map exists in the tree\n\t */\n\tif (ret == -EEXIST) {\n\t\tstruct extent_map *existing;\n\n\t\tret = 0;\n\n\t\texisting = lookup_extent_mapping(em_tree, start, len);\n\t\tif (existing && (existing->start > start ||\n\t\t    existing->start + existing->len <= start)) {\n\t\t\tfree_extent_map(existing);\n\t\t\texisting = NULL;\n\t\t}\n\t\tif (!existing) {\n\t\t\texisting = lookup_extent_mapping(em_tree, em->start,\n\t\t\t\t\t\t\t em->len);\n\t\t\tif (existing) {\n\t\t\t\terr = merge_extent_mapping(em_tree, existing,\n\t\t\t\t\t\t\t   em, start,\n\t\t\t\t\t\t\t   root->sectorsize);\n\t\t\t\tfree_extent_map(existing);\n\t\t\t\tif (err) {\n\t\t\t\t\tfree_extent_map(em);\n\t\t\t\t\tem = NULL;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = -EIO;\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tem = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tfree_extent_map(em);\n\t\t\tem = existing;\n\t\t\terr = 0;\n\t\t}\n\t}\n\twrite_unlock(&em_tree->lock);\nout:\n\n\tif (em)\n\t\ttrace_btrfs_get_extent(root, em);\n\n\tif (path)\n\t\tbtrfs_free_path(path);\n\tif (trans) {\n\t\tret = btrfs_end_transaction(trans, root);\n\t\tif (!err)\n\t\t\terr = ret;\n\t}\n\tif (err) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(err);\n\t}\n\tBUG_ON(!em); /* Error is always set */\n\treturn em;\n}\n\nstruct extent_map *btrfs_get_extent_fiemap(struct inode *inode, struct page *page,\n\t\t\t\t\t   size_t pg_offset, u64 start, u64 len,\n\t\t\t\t\t   int create)\n{\n\tstruct extent_map *em;\n\tstruct extent_map *hole_em = NULL;\n\tu64 range_start = start;\n\tu64 end;\n\tu64 found;\n\tu64 found_end;\n\tint err = 0;\n\n\tem = btrfs_get_extent(inode, page, pg_offset, start, len, create);\n\tif (IS_ERR(em))\n\t\treturn em;\n\tif (em) {\n\t\t/*\n\t\t * if our em maps to a hole, there might\n\t\t * actually be delalloc bytes behind it\n\t\t */\n\t\tif (em->block_start != EXTENT_MAP_HOLE)\n\t\t\treturn em;\n\t\telse\n\t\t\thole_em = em;\n\t}\n\n\t/* check to see if we've wrapped (len == -1 or similar) */\n\tend = start + len;\n\tif (end < start)\n\t\tend = (u64)-1;\n\telse\n\t\tend -= 1;\n\n\tem = NULL;\n\n\t/* ok, we didn't find anything, lets look for delalloc */\n\tfound = count_range_bits(&BTRFS_I(inode)->io_tree, &range_start,\n\t\t\t\t end, len, EXTENT_DELALLOC, 1);\n\tfound_end = range_start + found;\n\tif (found_end < range_start)\n\t\tfound_end = (u64)-1;\n\n\t/*\n\t * we didn't find anything useful, return\n\t * the original results from get_extent()\n\t */\n\tif (range_start > end || found_end <= start) {\n\t\tem = hole_em;\n\t\thole_em = NULL;\n\t\tgoto out;\n\t}\n\n\t/* adjust the range_start to make sure it doesn't\n\t * go backwards from the start they passed in\n\t */\n\trange_start = max(start,range_start);\n\tfound = found_end - range_start;\n\n\tif (found > 0) {\n\t\tu64 hole_start = start;\n\t\tu64 hole_len = len;\n\n\t\tem = alloc_extent_map();\n\t\tif (!em) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * when btrfs_get_extent can't find anything it\n\t\t * returns one huge hole\n\t\t *\n\t\t * make sure what it found really fits our range, and\n\t\t * adjust to make sure it is based on the start from\n\t\t * the caller\n\t\t */\n\t\tif (hole_em) {\n\t\t\tu64 calc_end = extent_map_end(hole_em);\n\n\t\t\tif (calc_end <= start || (hole_em->start > end)) {\n\t\t\t\tfree_extent_map(hole_em);\n\t\t\t\thole_em = NULL;\n\t\t\t} else {\n\t\t\t\thole_start = max(hole_em->start, start);\n\t\t\t\thole_len = calc_end - hole_start;\n\t\t\t}\n\t\t}\n\t\tem->bdev = NULL;\n\t\tif (hole_em && range_start > hole_start) {\n\t\t\t/* our hole starts before our delalloc, so we\n\t\t\t * have to return just the parts of the hole\n\t\t\t * that go until  the delalloc starts\n\t\t\t */\n\t\t\tem->len = min(hole_len,\n\t\t\t\t      range_start - hole_start);\n\t\t\tem->start = hole_start;\n\t\t\tem->orig_start = hole_start;\n\t\t\t/*\n\t\t\t * don't adjust block start at all,\n\t\t\t * it is fixed at EXTENT_MAP_HOLE\n\t\t\t */\n\t\t\tem->block_start = hole_em->block_start;\n\t\t\tem->block_len = hole_len;\n\t\t} else {\n\t\t\tem->start = range_start;\n\t\t\tem->len = found;\n\t\t\tem->orig_start = range_start;\n\t\t\tem->block_start = EXTENT_MAP_DELALLOC;\n\t\t\tem->block_len = found;\n\t\t}\n\t} else if (hole_em) {\n\t\treturn hole_em;\n\t}\nout:\n\n\tfree_extent_map(hole_em);\n\tif (err) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(err);\n\t}\n\treturn em;\n}\n\nstatic struct extent_map *btrfs_new_extent_direct(struct inode *inode,\n\t\t\t\t\t\t  u64 start, u64 len)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct extent_map *em;\n\tstruct btrfs_key ins;\n\tu64 alloc_hint;\n\tint ret;\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans))\n\t\treturn ERR_CAST(trans);\n\n\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\talloc_hint = get_extent_allocation_hint(inode, start, len);\n\tret = btrfs_reserve_extent(trans, root, len, root->sectorsize, 0,\n\t\t\t\t   alloc_hint, &ins, 1);\n\tif (ret) {\n\t\tem = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tem = create_pinned_em(inode, start, ins.offset, start, ins.objectid,\n\t\t\t      ins.offset, ins.offset, 0);\n\tif (IS_ERR(em))\n\t\tgoto out;\n\n\tret = btrfs_add_ordered_extent_dio(inode, start, ins.objectid,\n\t\t\t\t\t   ins.offset, ins.offset, 0);\n\tif (ret) {\n\t\tbtrfs_free_reserved_extent(root, ins.objectid, ins.offset);\n\t\tem = ERR_PTR(ret);\n\t}\nout:\n\tbtrfs_end_transaction(trans, root);\n\treturn em;\n}\n\n/*\n * returns 1 when the nocow is safe, < 1 on error, 0 if the\n * block must be cow'd\n */\nstatic noinline int can_nocow_odirect(struct btrfs_trans_handle *trans,\n\t\t\t\t      struct inode *inode, u64 offset, u64 len)\n{\n\tstruct btrfs_path *path;\n\tint ret;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_key key;\n\tu64 disk_bytenr;\n\tu64 backref_offset;\n\tu64 extent_end;\n\tu64 num_bytes;\n\tint slot;\n\tint found_type;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tret = btrfs_lookup_file_extent(trans, root, path, btrfs_ino(inode),\n\t\t\t\t       offset, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tslot = path->slots[0];\n\tif (ret == 1) {\n\t\tif (slot == 0) {\n\t\t\t/* can't find the item, must cow */\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tslot--;\n\t}\n\tret = 0;\n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\tif (key.objectid != btrfs_ino(inode) ||\n\t    key.type != BTRFS_EXTENT_DATA_KEY) {\n\t\t/* not our file or wrong item type, must cow */\n\t\tgoto out;\n\t}\n\n\tif (key.offset > offset) {\n\t\t/* Wrong offset, must cow */\n\t\tgoto out;\n\t}\n\n\tfi = btrfs_item_ptr(leaf, slot, struct btrfs_file_extent_item);\n\tfound_type = btrfs_file_extent_type(leaf, fi);\n\tif (found_type != BTRFS_FILE_EXTENT_REG &&\n\t    found_type != BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t/* not a regular extent, must cow */\n\t\tgoto out;\n\t}\n\tdisk_bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);\n\tbackref_offset = btrfs_file_extent_offset(leaf, fi);\n\n\textent_end = key.offset + btrfs_file_extent_num_bytes(leaf, fi);\n\tif (extent_end < offset + len) {\n\t\t/* extent doesn't include our full range, must cow */\n\t\tgoto out;\n\t}\n\n\tif (btrfs_extent_readonly(root, disk_bytenr))\n\t\tgoto out;\n\n\t/*\n\t * look for other files referencing this extent, if we\n\t * find any we must cow\n\t */\n\tif (btrfs_cross_ref_exist(trans, root, btrfs_ino(inode),\n\t\t\t\t  key.offset - backref_offset, disk_bytenr))\n\t\tgoto out;\n\n\t/*\n\t * adjust disk_bytenr and num_bytes to cover just the bytes\n\t * in this extent we are about to write.  If there\n\t * are any csums in that range we have to cow in order\n\t * to keep the csums correct\n\t */\n\tdisk_bytenr += backref_offset;\n\tdisk_bytenr += offset - key.offset;\n\tnum_bytes = min(offset + len, extent_end) - offset;\n\tif (csum_exist_in_range(root, disk_bytenr, num_bytes))\n\t\t\t\tgoto out;\n\t/*\n\t * all of the above have passed, it is safe to overwrite this extent\n\t * without cow\n\t */\n\tret = 1;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int lock_extent_direct(struct inode *inode, u64 lockstart, u64 lockend,\n\t\t\t      struct extent_state **cached_state, int writing)\n{\n\tstruct btrfs_ordered_extent *ordered;\n\tint ret = 0;\n\n\twhile (1) {\n\t\tlock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend,\n\t\t\t\t 0, cached_state);\n\t\t/*\n\t\t * We're concerned with the entire range that we're going to be\n\t\t * doing DIO to, so we need to make sure theres no ordered\n\t\t * extents in this range.\n\t\t */\n\t\tordered = btrfs_lookup_ordered_range(inode, lockstart,\n\t\t\t\t\t\t     lockend - lockstart + 1);\n\n\t\t/*\n\t\t * We need to make sure there are no buffered pages in this\n\t\t * range either, we could have raced between the invalidate in\n\t\t * generic_file_direct_write and locking the extent.  The\n\t\t * invalidate needs to happen so that reads after a write do not\n\t\t * get stale data.\n\t\t */\n\t\tif (!ordered && (!writing ||\n\t\t    !test_range_bit(&BTRFS_I(inode)->io_tree,\n\t\t\t\t    lockstart, lockend, EXTENT_UPTODATE, 0,\n\t\t\t\t    *cached_state)))\n\t\t\tbreak;\n\n\t\tunlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,\n\t\t\t\t     cached_state, GFP_NOFS);\n\n\t\tif (ordered) {\n\t\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t} else {\n\t\t\t/* Screw you mmap */\n\t\t\tret = filemap_write_and_wait_range(inode->i_mapping,\n\t\t\t\t\t\t\t   lockstart,\n\t\t\t\t\t\t\t   lockend);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * If we found a page that couldn't be invalidated just\n\t\t\t * fall back to buffered.\n\t\t\t */\n\t\t\tret = invalidate_inode_pages2_range(inode->i_mapping,\n\t\t\t\t\tlockstart >> PAGE_CACHE_SHIFT,\n\t\t\t\t\tlockend >> PAGE_CACHE_SHIFT);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcond_resched();\n\t}\n\n\treturn ret;\n}\n\nstatic struct extent_map *create_pinned_em(struct inode *inode, u64 start,\n\t\t\t\t\t   u64 len, u64 orig_start,\n\t\t\t\t\t   u64 block_start, u64 block_len,\n\t\t\t\t\t   u64 orig_block_len, int type)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\tem_tree = &BTRFS_I(inode)->extent_tree;\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tem->start = start;\n\tem->orig_start = orig_start;\n\tem->len = len;\n\tem->block_len = block_len;\n\tem->block_start = block_start;\n\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\tem->orig_block_len = orig_block_len;\n\tem->generation = -1;\n\tset_bit(EXTENT_FLAG_PINNED, &em->flags);\n\tif (type == BTRFS_ORDERED_PREALLOC)\n\t\tset_bit(EXTENT_FLAG_FILLING, &em->flags);\n\n\tdo {\n\t\tbtrfs_drop_extent_cache(inode, em->start,\n\t\t\t\tem->start + em->len - 1, 0);\n\t\twrite_lock(&em_tree->lock);\n\t\tret = add_extent_mapping(em_tree, em);\n\t\tif (!ret)\n\t\t\tlist_move(&em->list,\n\t\t\t\t  &em_tree->modified_extents);\n\t\twrite_unlock(&em_tree->lock);\n\t} while (ret == -EEXIST);\n\n\tif (ret) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn em;\n}\n\n\nstatic int btrfs_get_blocks_direct(struct inode *inode, sector_t iblock,\n\t\t\t\t   struct buffer_head *bh_result, int create)\n{\n\tstruct extent_map *em;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_state *cached_state = NULL;\n\tu64 start = iblock << inode->i_blkbits;\n\tu64 lockstart, lockend;\n\tu64 len = bh_result->b_size;\n\tstruct btrfs_trans_handle *trans;\n\tint unlock_bits = EXTENT_LOCKED;\n\tint ret;\n\n\tif (create) {\n\t\tret = btrfs_delalloc_reserve_space(inode, len);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tunlock_bits |= EXTENT_DELALLOC | EXTENT_DIRTY;\n\t} else {\n\t\tlen = min_t(u64, len, root->sectorsize);\n\t}\n\n\tlockstart = start;\n\tlockend = start + len - 1;\n\n\t/*\n\t * If this errors out it's because we couldn't invalidate pagecache for\n\t * this range and we need to fallback to buffered.\n\t */\n\tif (lock_extent_direct(inode, lockstart, lockend, &cached_state, create))\n\t\treturn -ENOTBLK;\n\n\tif (create) {\n\t\tret = set_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,\n\t\t\t\t     lockend, EXTENT_DELALLOC, NULL,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tif (ret)\n\t\t\tgoto unlock_err;\n\t}\n\n\tem = btrfs_get_extent(inode, NULL, 0, start, len, 0);\n\tif (IS_ERR(em)) {\n\t\tret = PTR_ERR(em);\n\t\tgoto unlock_err;\n\t}\n\n\t/*\n\t * Ok for INLINE and COMPRESSED extents we need to fallback on buffered\n\t * io.  INLINE is special, and we could probably kludge it in here, but\n\t * it's still buffered so for safety lets just fall back to the generic\n\t * buffered path.\n\t *\n\t * For COMPRESSED we _have_ to read the entire extent in so we can\n\t * decompress it, so there will be buffering required no matter what we\n\t * do, so go ahead and fallback to buffered.\n\t *\n\t * We return -ENOTBLK because thats what makes DIO go ahead and go back\n\t * to buffered IO.  Don't blame me, this is the price we pay for using\n\t * the generic code.\n\t */\n\tif (test_bit(EXTENT_FLAG_COMPRESSED, &em->flags) ||\n\t    em->block_start == EXTENT_MAP_INLINE) {\n\t\tfree_extent_map(em);\n\t\tret = -ENOTBLK;\n\t\tgoto unlock_err;\n\t}\n\n\t/* Just a good old fashioned hole, return */\n\tif (!create && (em->block_start == EXTENT_MAP_HOLE ||\n\t\t\ttest_bit(EXTENT_FLAG_PREALLOC, &em->flags))) {\n\t\tfree_extent_map(em);\n\t\tret = 0;\n\t\tgoto unlock_err;\n\t}\n\n\t/*\n\t * We don't allocate a new extent in the following cases\n\t *\n\t * 1) The inode is marked as NODATACOW.  In this case we'll just use the\n\t * existing extent.\n\t * 2) The extent is marked as PREALLOC.  We're good to go here and can\n\t * just use the extent.\n\t *\n\t */\n\tif (!create) {\n\t\tlen = min(len, em->len - (start - em->start));\n\t\tlockstart = start + len;\n\t\tgoto unlock;\n\t}\n\n\tif (test_bit(EXTENT_FLAG_PREALLOC, &em->flags) ||\n\t    ((BTRFS_I(inode)->flags & BTRFS_INODE_NODATACOW) &&\n\t     em->block_start != EXTENT_MAP_HOLE)) {\n\t\tint type;\n\t\tint ret;\n\t\tu64 block_start;\n\n\t\tif (test_bit(EXTENT_FLAG_PREALLOC, &em->flags))\n\t\t\ttype = BTRFS_ORDERED_PREALLOC;\n\t\telse\n\t\t\ttype = BTRFS_ORDERED_NOCOW;\n\t\tlen = min(len, em->len - (start - em->start));\n\t\tblock_start = em->block_start + (start - em->start);\n\n\t\t/*\n\t\t * we're not going to log anything, but we do need\n\t\t * to make sure the current transaction stays open\n\t\t * while we look for nocow cross refs\n\t\t */\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans))\n\t\t\tgoto must_cow;\n\n\t\tif (can_nocow_odirect(trans, inode, start, len) == 1) {\n\t\t\tu64 orig_start = em->orig_start;\n\t\t\tu64 orig_block_len = em->orig_block_len;\n\n\t\t\tif (type == BTRFS_ORDERED_PREALLOC) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tem = create_pinned_em(inode, start, len,\n\t\t\t\t\t\t       orig_start,\n\t\t\t\t\t\t       block_start, len,\n\t\t\t\t\t\t       orig_block_len, type);\n\t\t\t\tif (IS_ERR(em)) {\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto unlock_err;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = btrfs_add_ordered_extent_dio(inode, start,\n\t\t\t\t\t   block_start, len, len, type);\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tif (ret) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tgoto unlock_err;\n\t\t\t}\n\t\t\tgoto unlock;\n\t\t}\n\t\tbtrfs_end_transaction(trans, root);\n\t}\nmust_cow:\n\t/*\n\t * this will cow the extent, reset the len in case we changed\n\t * it above\n\t */\n\tlen = bh_result->b_size;\n\tfree_extent_map(em);\n\tem = btrfs_new_extent_direct(inode, start, len);\n\tif (IS_ERR(em)) {\n\t\tret = PTR_ERR(em);\n\t\tgoto unlock_err;\n\t}\n\tlen = min(len, em->len - (start - em->start));\nunlock:\n\tbh_result->b_blocknr = (em->block_start + (start - em->start)) >>\n\t\tinode->i_blkbits;\n\tbh_result->b_size = len;\n\tbh_result->b_bdev = em->bdev;\n\tset_buffer_mapped(bh_result);\n\tif (create) {\n\t\tif (!test_bit(EXTENT_FLAG_PREALLOC, &em->flags))\n\t\t\tset_buffer_new(bh_result);\n\n\t\t/*\n\t\t * Need to update the i_size under the extent lock so buffered\n\t\t * readers will get the updated i_size when we unlock.\n\t\t */\n\t\tif (start + len > i_size_read(inode))\n\t\t\ti_size_write(inode, start + len);\n\t}\n\n\t/*\n\t * In the case of write we need to clear and unlock the entire range,\n\t * in the case of read we need to unlock only the end area that we\n\t * aren't using if there is any left over space.\n\t */\n\tif (lockstart < lockend) {\n\t\tif (create && len < lockend - lockstart) {\n\t\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,\n\t\t\t\t\t lockstart + len - 1,\n\t\t\t\t\t unlock_bits | EXTENT_DEFRAG, 1, 0,\n\t\t\t\t\t &cached_state, GFP_NOFS);\n\t\t\t/*\n\t\t\t * Beside unlock, we also need to cleanup reserved space\n\t\t\t * for the left range by attaching EXTENT_DO_ACCOUNTING.\n\t\t\t */\n\t\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree,\n\t\t\t\t\t lockstart + len, lockend,\n\t\t\t\t\t unlock_bits | EXTENT_DO_ACCOUNTING |\n\t\t\t\t\t EXTENT_DEFRAG, 1, 0, NULL, GFP_NOFS);\n\t\t} else {\n\t\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,\n\t\t\t\t\t lockend, unlock_bits, 1, 0,\n\t\t\t\t\t &cached_state, GFP_NOFS);\n\t\t}\n\t} else {\n\t\tfree_extent_state(cached_state);\n\t}\n\n\tfree_extent_map(em);\n\n\treturn 0;\n\nunlock_err:\n\tif (create)\n\t\tunlock_bits |= EXTENT_DO_ACCOUNTING;\n\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, lockstart, lockend,\n\t\t\t unlock_bits, 1, 0, &cached_state, GFP_NOFS);\n\treturn ret;\n}\n\nstruct btrfs_dio_private {\n\tstruct inode *inode;\n\tu64 logical_offset;\n\tu64 disk_bytenr;\n\tu64 bytes;\n\tvoid *private;\n\n\t/* number of bios pending for this dio */\n\tatomic_t pending_bios;\n\n\t/* IO errors */\n\tint errors;\n\n\tstruct bio *orig_bio;\n};\n\nstatic void btrfs_endio_direct_read(struct bio *bio, int err)\n{\n\tstruct btrfs_dio_private *dip = bio->bi_private;\n\tstruct bio_vec *bvec_end = bio->bi_io_vec + bio->bi_vcnt - 1;\n\tstruct bio_vec *bvec = bio->bi_io_vec;\n\tstruct inode *inode = dip->inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tu64 start;\n\n\tstart = dip->logical_offset;\n\tdo {\n\t\tif (!(BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM)) {\n\t\t\tstruct page *page = bvec->bv_page;\n\t\t\tchar *kaddr;\n\t\t\tu32 csum = ~(u32)0;\n\t\t\tu64 private = ~(u32)0;\n\t\t\tunsigned long flags;\n\n\t\t\tif (get_state_private(&BTRFS_I(inode)->io_tree,\n\t\t\t\t\t      start, &private))\n\t\t\t\tgoto failed;\n\t\t\tlocal_irq_save(flags);\n\t\t\tkaddr = kmap_atomic(page);\n\t\t\tcsum = btrfs_csum_data(root, kaddr + bvec->bv_offset,\n\t\t\t\t\t       csum, bvec->bv_len);\n\t\t\tbtrfs_csum_final(csum, (char *)&csum);\n\t\t\tkunmap_atomic(kaddr);\n\t\t\tlocal_irq_restore(flags);\n\n\t\t\tflush_dcache_page(bvec->bv_page);\n\t\t\tif (csum != private) {\nfailed:\n\t\t\t\tprintk(KERN_ERR \"btrfs csum failed ino %llu off\"\n\t\t\t\t      \" %llu csum %u private %u\\n\",\n\t\t\t\t      (unsigned long long)btrfs_ino(inode),\n\t\t\t\t      (unsigned long long)start,\n\t\t\t\t      csum, (unsigned)private);\n\t\t\t\terr = -EIO;\n\t\t\t}\n\t\t}\n\n\t\tstart += bvec->bv_len;\n\t\tbvec++;\n\t} while (bvec <= bvec_end);\n\n\tunlock_extent(&BTRFS_I(inode)->io_tree, dip->logical_offset,\n\t\t      dip->logical_offset + dip->bytes - 1);\n\tbio->bi_private = dip->private;\n\n\tkfree(dip);\n\n\t/* If we had a csum failure make sure to clear the uptodate flag */\n\tif (err)\n\t\tclear_bit(BIO_UPTODATE, &bio->bi_flags);\n\tdio_end_io(bio, err);\n}\n\nstatic void btrfs_endio_direct_write(struct bio *bio, int err)\n{\n\tstruct btrfs_dio_private *dip = bio->bi_private;\n\tstruct inode *inode = dip->inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ordered_extent *ordered = NULL;\n\tu64 ordered_offset = dip->logical_offset;\n\tu64 ordered_bytes = dip->bytes;\n\tint ret;\n\n\tif (err)\n\t\tgoto out_done;\nagain:\n\tret = btrfs_dec_test_first_ordered_pending(inode, &ordered,\n\t\t\t\t\t\t   &ordered_offset,\n\t\t\t\t\t\t   ordered_bytes, !err);\n\tif (!ret)\n\t\tgoto out_test;\n\n\tordered->work.func = finish_ordered_fn;\n\tordered->work.flags = 0;\n\tbtrfs_queue_worker(&root->fs_info->endio_write_workers,\n\t\t\t   &ordered->work);\nout_test:\n\t/*\n\t * our bio might span multiple ordered extents.  If we haven't\n\t * completed the accounting for the whole dio, go back and try again\n\t */\n\tif (ordered_offset < dip->logical_offset + dip->bytes) {\n\t\tordered_bytes = dip->logical_offset + dip->bytes -\n\t\t\tordered_offset;\n\t\tordered = NULL;\n\t\tgoto again;\n\t}\nout_done:\n\tbio->bi_private = dip->private;\n\n\tkfree(dip);\n\n\t/* If we had an error make sure to clear the uptodate flag */\n\tif (err)\n\t\tclear_bit(BIO_UPTODATE, &bio->bi_flags);\n\tdio_end_io(bio, err);\n}\n\nstatic int __btrfs_submit_bio_start_direct_io(struct inode *inode, int rw,\n\t\t\t\t    struct bio *bio, int mirror_num,\n\t\t\t\t    unsigned long bio_flags, u64 offset)\n{\n\tint ret;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tret = btrfs_csum_one_bio(root, inode, bio, offset, 1);\n\tBUG_ON(ret); /* -ENOMEM */\n\treturn 0;\n}\n\nstatic void btrfs_end_dio_bio(struct bio *bio, int err)\n{\n\tstruct btrfs_dio_private *dip = bio->bi_private;\n\n\tif (err) {\n\t\tprintk(KERN_ERR \"btrfs direct IO failed ino %llu rw %lu \"\n\t\t      \"sector %#Lx len %u err no %d\\n\",\n\t\t      (unsigned long long)btrfs_ino(dip->inode), bio->bi_rw,\n\t\t      (unsigned long long)bio->bi_sector, bio->bi_size, err);\n\t\tdip->errors = 1;\n\n\t\t/*\n\t\t * before atomic variable goto zero, we must make sure\n\t\t * dip->errors is perceived to be set.\n\t\t */\n\t\tsmp_mb__before_atomic_dec();\n\t}\n\n\t/* if there are more bios still pending for this dio, just exit */\n\tif (!atomic_dec_and_test(&dip->pending_bios))\n\t\tgoto out;\n\n\tif (dip->errors)\n\t\tbio_io_error(dip->orig_bio);\n\telse {\n\t\tset_bit(BIO_UPTODATE, &dip->orig_bio->bi_flags);\n\t\tbio_endio(dip->orig_bio, 0);\n\t}\nout:\n\tbio_put(bio);\n}\n\nstatic struct bio *btrfs_dio_bio_alloc(struct block_device *bdev,\n\t\t\t\t       u64 first_sector, gfp_t gfp_flags)\n{\n\tint nr_vecs = bio_get_nr_vecs(bdev);\n\treturn btrfs_bio_alloc(bdev, first_sector, nr_vecs, gfp_flags);\n}\n\nstatic inline int __btrfs_submit_dio_bio(struct bio *bio, struct inode *inode,\n\t\t\t\t\t int rw, u64 file_offset, int skip_sum,\n\t\t\t\t\t int async_submit)\n{\n\tint write = rw & REQ_WRITE;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\tif (async_submit)\n\t\tasync_submit = !atomic_read(&BTRFS_I(inode)->sync_writers);\n\n\tbio_get(bio);\n\n\tif (!write) {\n\t\tret = btrfs_bio_wq_end_io(root->fs_info, bio, 0);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tif (skip_sum)\n\t\tgoto map;\n\n\tif (write && async_submit) {\n\t\tret = btrfs_wq_submit_bio(root->fs_info,\n\t\t\t\t   inode, rw, bio, 0, 0,\n\t\t\t\t   file_offset,\n\t\t\t\t   __btrfs_submit_bio_start_direct_io,\n\t\t\t\t   __btrfs_submit_bio_done);\n\t\tgoto err;\n\t} else if (write) {\n\t\t/*\n\t\t * If we aren't doing async submit, calculate the csum of the\n\t\t * bio now.\n\t\t */\n\t\tret = btrfs_csum_one_bio(root, inode, bio, file_offset, 1);\n\t\tif (ret)\n\t\t\tgoto err;\n\t} else if (!skip_sum) {\n\t\tret = btrfs_lookup_bio_sums_dio(root, inode, bio, file_offset);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\nmap:\n\tret = btrfs_map_bio(root, rw, bio, 0, async_submit);\nerr:\n\tbio_put(bio);\n\treturn ret;\n}\n\nstatic int btrfs_submit_direct_hook(int rw, struct btrfs_dio_private *dip,\n\t\t\t\t    int skip_sum)\n{\n\tstruct inode *inode = dip->inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct bio *bio;\n\tstruct bio *orig_bio = dip->orig_bio;\n\tstruct bio_vec *bvec = orig_bio->bi_io_vec;\n\tu64 start_sector = orig_bio->bi_sector;\n\tu64 file_offset = dip->logical_offset;\n\tu64 submit_len = 0;\n\tu64 map_length;\n\tint nr_pages = 0;\n\tint ret = 0;\n\tint async_submit = 0;\n\n\tmap_length = orig_bio->bi_size;\n\tret = btrfs_map_block(root->fs_info, READ, start_sector << 9,\n\t\t\t      &map_length, NULL, 0);\n\tif (ret) {\n\t\tbio_put(orig_bio);\n\t\treturn -EIO;\n\t}\n\n\tif (map_length >= orig_bio->bi_size) {\n\t\tbio = orig_bio;\n\t\tgoto submit;\n\t}\n\n\tasync_submit = 1;\n\tbio = btrfs_dio_bio_alloc(orig_bio->bi_bdev, start_sector, GFP_NOFS);\n\tif (!bio)\n\t\treturn -ENOMEM;\n\tbio->bi_private = dip;\n\tbio->bi_end_io = btrfs_end_dio_bio;\n\tatomic_inc(&dip->pending_bios);\n\n\twhile (bvec <= (orig_bio->bi_io_vec + orig_bio->bi_vcnt - 1)) {\n\t\tif (unlikely(map_length < submit_len + bvec->bv_len ||\n\t\t    bio_add_page(bio, bvec->bv_page, bvec->bv_len,\n\t\t\t\t bvec->bv_offset) < bvec->bv_len)) {\n\t\t\t/*\n\t\t\t * inc the count before we submit the bio so\n\t\t\t * we know the end IO handler won't happen before\n\t\t\t * we inc the count. Otherwise, the dip might get freed\n\t\t\t * before we're done setting it up\n\t\t\t */\n\t\t\tatomic_inc(&dip->pending_bios);\n\t\t\tret = __btrfs_submit_dio_bio(bio, inode, rw,\n\t\t\t\t\t\t     file_offset, skip_sum,\n\t\t\t\t\t\t     async_submit);\n\t\t\tif (ret) {\n\t\t\t\tbio_put(bio);\n\t\t\t\tatomic_dec(&dip->pending_bios);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\n\t\t\tstart_sector += submit_len >> 9;\n\t\t\tfile_offset += submit_len;\n\n\t\t\tsubmit_len = 0;\n\t\t\tnr_pages = 0;\n\n\t\t\tbio = btrfs_dio_bio_alloc(orig_bio->bi_bdev,\n\t\t\t\t\t\t  start_sector, GFP_NOFS);\n\t\t\tif (!bio)\n\t\t\t\tgoto out_err;\n\t\t\tbio->bi_private = dip;\n\t\t\tbio->bi_end_io = btrfs_end_dio_bio;\n\n\t\t\tmap_length = orig_bio->bi_size;\n\t\t\tret = btrfs_map_block(root->fs_info, READ,\n\t\t\t\t\t      start_sector << 9,\n\t\t\t\t\t      &map_length, NULL, 0);\n\t\t\tif (ret) {\n\t\t\t\tbio_put(bio);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t} else {\n\t\t\tsubmit_len += bvec->bv_len;\n\t\t\tnr_pages ++;\n\t\t\tbvec++;\n\t\t}\n\t}\n\nsubmit:\n\tret = __btrfs_submit_dio_bio(bio, inode, rw, file_offset, skip_sum,\n\t\t\t\t     async_submit);\n\tif (!ret)\n\t\treturn 0;\n\n\tbio_put(bio);\nout_err:\n\tdip->errors = 1;\n\t/*\n\t * before atomic variable goto zero, we must\n\t * make sure dip->errors is perceived to be set.\n\t */\n\tsmp_mb__before_atomic_dec();\n\tif (atomic_dec_and_test(&dip->pending_bios))\n\t\tbio_io_error(dip->orig_bio);\n\n\t/* bio_end_io() will handle error, so we needn't return it */\n\treturn 0;\n}\n\nstatic void btrfs_submit_direct(int rw, struct bio *bio, struct inode *inode,\n\t\t\t\tloff_t file_offset)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_dio_private *dip;\n\tstruct bio_vec *bvec = bio->bi_io_vec;\n\tint skip_sum;\n\tint write = rw & REQ_WRITE;\n\tint ret = 0;\n\n\tskip_sum = BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM;\n\n\tdip = kmalloc(sizeof(*dip), GFP_NOFS);\n\tif (!dip) {\n\t\tret = -ENOMEM;\n\t\tgoto free_ordered;\n\t}\n\n\tdip->private = bio->bi_private;\n\tdip->inode = inode;\n\tdip->logical_offset = file_offset;\n\n\tdip->bytes = 0;\n\tdo {\n\t\tdip->bytes += bvec->bv_len;\n\t\tbvec++;\n\t} while (bvec <= (bio->bi_io_vec + bio->bi_vcnt - 1));\n\n\tdip->disk_bytenr = (u64)bio->bi_sector << 9;\n\tbio->bi_private = dip;\n\tdip->errors = 0;\n\tdip->orig_bio = bio;\n\tatomic_set(&dip->pending_bios, 0);\n\n\tif (write)\n\t\tbio->bi_end_io = btrfs_endio_direct_write;\n\telse\n\t\tbio->bi_end_io = btrfs_endio_direct_read;\n\n\tret = btrfs_submit_direct_hook(rw, dip, skip_sum);\n\tif (!ret)\n\t\treturn;\nfree_ordered:\n\t/*\n\t * If this is a write, we need to clean up the reserved space and kill\n\t * the ordered extent.\n\t */\n\tif (write) {\n\t\tstruct btrfs_ordered_extent *ordered;\n\t\tordered = btrfs_lookup_ordered_extent(inode, file_offset);\n\t\tif (!test_bit(BTRFS_ORDERED_PREALLOC, &ordered->flags) &&\n\t\t    !test_bit(BTRFS_ORDERED_NOCOW, &ordered->flags))\n\t\t\tbtrfs_free_reserved_extent(root, ordered->start,\n\t\t\t\t\t\t   ordered->disk_len);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t}\n\tbio_endio(bio, ret);\n}\n\nstatic ssize_t check_direct_IO(struct btrfs_root *root, int rw, struct kiocb *iocb,\n\t\t\tconst struct iovec *iov, loff_t offset,\n\t\t\tunsigned long nr_segs)\n{\n\tint seg;\n\tint i;\n\tsize_t size;\n\tunsigned long addr;\n\tunsigned blocksize_mask = root->sectorsize - 1;\n\tssize_t retval = -EINVAL;\n\tloff_t end = offset;\n\n\tif (offset & blocksize_mask)\n\t\tgoto out;\n\n\t/* Check the memory alignment.  Blocks cannot straddle pages */\n\tfor (seg = 0; seg < nr_segs; seg++) {\n\t\taddr = (unsigned long)iov[seg].iov_base;\n\t\tsize = iov[seg].iov_len;\n\t\tend += size;\n\t\tif ((addr & blocksize_mask) || (size & blocksize_mask))\n\t\t\tgoto out;\n\n\t\t/* If this is a write we don't need to check anymore */\n\t\tif (rw & WRITE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Check to make sure we don't have duplicate iov_base's in this\n\t\t * iovec, if so return EINVAL, otherwise we'll get csum errors\n\t\t * when reading back.\n\t\t */\n\t\tfor (i = seg + 1; i < nr_segs; i++) {\n\t\t\tif (iov[seg].iov_base == iov[i].iov_base)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\tretval = 0;\nout:\n\treturn retval;\n}\n\nstatic ssize_t btrfs_direct_IO(int rw, struct kiocb *iocb,\n\t\t\tconst struct iovec *iov, loff_t offset,\n\t\t\tunsigned long nr_segs)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\n\tif (check_direct_IO(BTRFS_I(inode)->root, rw, iocb, iov,\n\t\t\t    offset, nr_segs))\n\t\treturn 0;\n\n\treturn __blockdev_direct_IO(rw, iocb, inode,\n\t\t   BTRFS_I(inode)->root->fs_info->fs_devices->latest_bdev,\n\t\t   iov, offset, nr_segs, btrfs_get_blocks_direct, NULL,\n\t\t   btrfs_submit_direct, 0);\n}\n\n#define BTRFS_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC)\n\nstatic int btrfs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\tint\tret;\n\n\tret = fiemap_check_flags(fieinfo, BTRFS_FIEMAP_FLAGS);\n\tif (ret)\n\t\treturn ret;\n\n\treturn extent_fiemap(inode, fieinfo, start, len, btrfs_get_extent_fiemap);\n}\n\nint btrfs_readpage(struct file *file, struct page *page)\n{\n\tstruct extent_io_tree *tree;\n\ttree = &BTRFS_I(page->mapping->host)->io_tree;\n\treturn extent_read_full_page(tree, page, btrfs_get_extent, 0);\n}\n\nstatic int btrfs_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tstruct extent_io_tree *tree;\n\n\n\tif (current->flags & PF_MEMALLOC) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tunlock_page(page);\n\t\treturn 0;\n\t}\n\ttree = &BTRFS_I(page->mapping->host)->io_tree;\n\treturn extent_write_full_page(tree, page, btrfs_get_extent, wbc);\n}\n\nint btrfs_writepages(struct address_space *mapping,\n\t\t     struct writeback_control *wbc)\n{\n\tstruct extent_io_tree *tree;\n\n\ttree = &BTRFS_I(mapping->host)->io_tree;\n\treturn extent_writepages(tree, mapping, btrfs_get_extent, wbc);\n}\n\nstatic int\nbtrfs_readpages(struct file *file, struct address_space *mapping,\n\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\tstruct extent_io_tree *tree;\n\ttree = &BTRFS_I(mapping->host)->io_tree;\n\treturn extent_readpages(tree, mapping, pages, nr_pages,\n\t\t\t\tbtrfs_get_extent);\n}\nstatic int __btrfs_releasepage(struct page *page, gfp_t gfp_flags)\n{\n\tstruct extent_io_tree *tree;\n\tstruct extent_map_tree *map;\n\tint ret;\n\n\ttree = &BTRFS_I(page->mapping->host)->io_tree;\n\tmap = &BTRFS_I(page->mapping->host)->extent_tree;\n\tret = try_release_extent_mapping(map, tree, page, gfp_flags);\n\tif (ret == 1) {\n\t\tClearPagePrivate(page);\n\t\tset_page_private(page, 0);\n\t\tpage_cache_release(page);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_releasepage(struct page *page, gfp_t gfp_flags)\n{\n\tif (PageWriteback(page) || PageDirty(page))\n\t\treturn 0;\n\treturn __btrfs_releasepage(page, gfp_flags & GFP_NOFS);\n}\n\nstatic void btrfs_invalidatepage(struct page *page, unsigned long offset)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct extent_io_tree *tree;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tu64 page_start = page_offset(page);\n\tu64 page_end = page_start + PAGE_CACHE_SIZE - 1;\n\n\t/*\n\t * we have the page locked, so new writeback can't start,\n\t * and the dirty bit won't be cleared while we are here.\n\t *\n\t * Wait for IO on this page so that we can safely clear\n\t * the PagePrivate2 bit and do ordered accounting\n\t */\n\twait_on_page_writeback(page);\n\n\ttree = &BTRFS_I(inode)->io_tree;\n\tif (offset) {\n\t\tbtrfs_releasepage(page, GFP_NOFS);\n\t\treturn;\n\t}\n\tlock_extent_bits(tree, page_start, page_end, 0, &cached_state);\n\tordered = btrfs_lookup_ordered_extent(inode,\n\t\t\t\t\t   page_offset(page));\n\tif (ordered) {\n\t\t/*\n\t\t * IO on this page will never be started, so we need\n\t\t * to account for any ordered extents now\n\t\t */\n\t\tclear_extent_bit(tree, page_start, page_end,\n\t\t\t\t EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t\t EXTENT_LOCKED | EXTENT_DO_ACCOUNTING |\n\t\t\t\t EXTENT_DEFRAG, 1, 0, &cached_state, GFP_NOFS);\n\t\t/*\n\t\t * whoever cleared the private bit is responsible\n\t\t * for the finish_ordered_io\n\t\t */\n\t\tif (TestClearPagePrivate2(page) &&\n\t\t    btrfs_dec_test_ordered_pending(inode, &ordered, page_start,\n\t\t\t\t\t\t   PAGE_CACHE_SIZE, 1)) {\n\t\t\tbtrfs_finish_ordered_io(ordered);\n\t\t}\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tcached_state = NULL;\n\t\tlock_extent_bits(tree, page_start, page_end, 0, &cached_state);\n\t}\n\tclear_extent_bit(tree, page_start, page_end,\n\t\t EXTENT_LOCKED | EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 1, 1,\n\t\t &cached_state, GFP_NOFS);\n\t__btrfs_releasepage(page, GFP_NOFS);\n\n\tClearPageChecked(page);\n\tif (PagePrivate(page)) {\n\t\tClearPagePrivate(page);\n\t\tset_page_private(page, 0);\n\t\tpage_cache_release(page);\n\t}\n}\n\n/*\n * btrfs_page_mkwrite() is not allowed to change the file size as it gets\n * called from a page fault handler when a page is first dirtied. Hence we must\n * be careful to check for EOF conditions here. We set the page up correctly\n * for a written page which means we get ENOSPC checking when writing into\n * holes and correct delalloc and unwritten extent mapping on filesystems that\n * support these features.\n *\n * We are not allowed to take the i_mutex here so we have to play games to\n * protect against truncate races as the page could now be beyond EOF.  Because\n * vmtruncate() writes the inode size before removing pages, once we have the\n * page lock we can determine safely if the page is beyond EOF. If it is not\n * beyond EOF, then the page is guaranteed safe against truncation until we\n * unlock the page.\n */\nint btrfs_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page = vmf->page;\n\tstruct inode *inode = fdentry(vma->vm_file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tchar *kaddr;\n\tunsigned long zero_start;\n\tloff_t size;\n\tint ret;\n\tint reserved = 0;\n\tu64 page_start;\n\tu64 page_end;\n\n\tsb_start_pagefault(inode->i_sb);\n\tret  = btrfs_delalloc_reserve_space(inode, PAGE_CACHE_SIZE);\n\tif (!ret) {\n\t\tret = file_update_time(vma->vm_file);\n\t\treserved = 1;\n\t}\n\tif (ret) {\n\t\tif (ret == -ENOMEM)\n\t\t\tret = VM_FAULT_OOM;\n\t\telse /* -ENOSPC, -EIO, etc */\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\tif (reserved)\n\t\t\tgoto out;\n\t\tgoto out_noreserve;\n\t}\n\n\tret = VM_FAULT_NOPAGE; /* make the VM retry the fault */\nagain:\n\tlock_page(page);\n\tsize = i_size_read(inode);\n\tpage_start = page_offset(page);\n\tpage_end = page_start + PAGE_CACHE_SIZE - 1;\n\n\tif ((page->mapping != inode->i_mapping) ||\n\t    (page_start >= size)) {\n\t\t/* page got truncated out from underneath us */\n\t\tgoto out_unlock;\n\t}\n\twait_on_page_writeback(page);\n\n\tlock_extent_bits(io_tree, page_start, page_end, 0, &cached_state);\n\tset_page_extent_mapped(page);\n\n\t/*\n\t * we can't set the delalloc bits if there are pending ordered\n\t * extents.  Drop our locks and wait for them to finish\n\t */\n\tordered = btrfs_lookup_ordered_extent(inode, page_start);\n\tif (ordered) {\n\t\tunlock_extent_cached(io_tree, page_start, page_end,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tunlock_page(page);\n\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tgoto again;\n\t}\n\n\t/*\n\t * XXX - page_mkwrite gets called every time the page is dirtied, even\n\t * if it was already dirty, so for space accounting reasons we need to\n\t * clear any delalloc bits for the range we are fixing to save.  There\n\t * is probably a better way to do this, but for now keep consistent with\n\t * prepare_pages in the normal write path.\n\t */\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, page_start, page_end,\n\t\t\t  EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,\n\t\t\t  0, 0, &cached_state, GFP_NOFS);\n\n\tret = btrfs_set_extent_delalloc(inode, page_start, page_end,\n\t\t\t\t\t&cached_state);\n\tif (ret) {\n\t\tunlock_extent_cached(io_tree, page_start, page_end,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_unlock;\n\t}\n\tret = 0;\n\n\t/* page is wholly or partially inside EOF */\n\tif (page_start + PAGE_CACHE_SIZE > size)\n\t\tzero_start = size & ~PAGE_CACHE_MASK;\n\telse\n\t\tzero_start = PAGE_CACHE_SIZE;\n\n\tif (zero_start != PAGE_CACHE_SIZE) {\n\t\tkaddr = kmap(page);\n\t\tmemset(kaddr + zero_start, 0, PAGE_CACHE_SIZE - zero_start);\n\t\tflush_dcache_page(page);\n\t\tkunmap(page);\n\t}\n\tClearPageChecked(page);\n\tset_page_dirty(page);\n\tSetPageUptodate(page);\n\n\tBTRFS_I(inode)->last_trans = root->fs_info->generation;\n\tBTRFS_I(inode)->last_sub_trans = BTRFS_I(inode)->root->log_transid;\n\tBTRFS_I(inode)->last_log_commit = BTRFS_I(inode)->root->last_log_commit;\n\n\tunlock_extent_cached(io_tree, page_start, page_end, &cached_state, GFP_NOFS);\n\nout_unlock:\n\tif (!ret) {\n\t\tsb_end_pagefault(inode->i_sb);\n\t\treturn VM_FAULT_LOCKED;\n\t}\n\tunlock_page(page);\nout:\n\tbtrfs_delalloc_release_space(inode, PAGE_CACHE_SIZE);\nout_noreserve:\n\tsb_end_pagefault(inode->i_sb);\n\treturn ret;\n}\n\nstatic int btrfs_truncate(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_block_rsv *rsv;\n\tint ret;\n\tint err = 0;\n\tstruct btrfs_trans_handle *trans;\n\tu64 mask = root->sectorsize - 1;\n\tu64 min_size = btrfs_calc_trunc_metadata_size(root, 1);\n\n\tret = btrfs_truncate_page(inode, inode->i_size, 0, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tbtrfs_wait_ordered_range(inode, inode->i_size & (~mask), (u64)-1);\n\tbtrfs_ordered_update_i_size(inode, inode->i_size, NULL);\n\n\t/*\n\t * Yes ladies and gentelment, this is indeed ugly.  The fact is we have\n\t * 3 things going on here\n\t *\n\t * 1) We need to reserve space for our orphan item and the space to\n\t * delete our orphan item.  Lord knows we don't want to have a dangling\n\t * orphan item because we didn't reserve space to remove it.\n\t *\n\t * 2) We need to reserve space to update our inode.\n\t *\n\t * 3) We need to have something to cache all the space that is going to\n\t * be free'd up by the truncate operation, but also have some slack\n\t * space reserved in case it uses space during the truncate (thank you\n\t * very much snapshotting).\n\t *\n\t * And we need these to all be seperate.  The fact is we can use alot of\n\t * space doing the truncate, and we have no earthly idea how much space\n\t * we will use, so we need the truncate reservation to be seperate so it\n\t * doesn't end up using space reserved for updating the inode or\n\t * removing the orphan item.  We also need to be able to stop the\n\t * transaction and start a new one, which means we need to be able to\n\t * update the inode several times, and we have no idea of knowing how\n\t * many times that will be, so we can't just reserve 1 item for the\n\t * entirety of the opration, so that has to be done seperately as well.\n\t * Then there is the orphan item, which does indeed need to be held on\n\t * to for the whole operation, and we need nobody to touch this reserved\n\t * space except the orphan code.\n\t *\n\t * So that leaves us with\n\t *\n\t * 1) root->orphan_block_rsv - for the orphan deletion.\n\t * 2) rsv - for the truncate reservation, which we will steal from the\n\t * transaction reservation.\n\t * 3) fs_info->trans_block_rsv - this will have 1 items worth left for\n\t * updating the inode.\n\t */\n\trsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);\n\tif (!rsv)\n\t\treturn -ENOMEM;\n\trsv->size = min_size;\n\trsv->failfast = 1;\n\n\t/*\n\t * 1 for the truncate slack space\n\t * 1 for the orphan item we're going to add\n\t * 1 for the orphan item deletion\n\t * 1 for updating the inode.\n\t */\n\ttrans = btrfs_start_transaction(root, 4);\n\tif (IS_ERR(trans)) {\n\t\terr = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\t/* Migrate the slack space for the truncate to our reserve */\n\tret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv, rsv,\n\t\t\t\t      min_size);\n\tBUG_ON(ret);\n\n\tret = btrfs_orphan_add(trans, inode);\n\tif (ret) {\n\t\tbtrfs_end_transaction(trans, root);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * setattr is responsible for setting the ordered_data_close flag,\n\t * but that is only tested during the last file release.  That\n\t * could happen well after the next commit, leaving a great big\n\t * window where new writes may get lost if someone chooses to write\n\t * to this file after truncating to zero\n\t *\n\t * The inode doesn't have any dirty data here, and so if we commit\n\t * this is a noop.  If someone immediately starts writing to the inode\n\t * it is very likely we'll catch some of their writes in this\n\t * transaction, and the commit will find this file on the ordered\n\t * data list with good things to send down.\n\t *\n\t * This is a best effort solution, there is still a window where\n\t * using truncate to replace the contents of the file will\n\t * end up with a zero length file after a crash.\n\t */\n\tif (inode->i_size == 0 && test_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,\n\t\t\t\t\t   &BTRFS_I(inode)->runtime_flags))\n\t\tbtrfs_add_ordered_operation(trans, root, inode);\n\n\t/*\n\t * So if we truncate and then write and fsync we normally would just\n\t * write the extents that changed, which is a problem if we need to\n\t * first truncate that entire inode.  So set this flag so we write out\n\t * all of the extents in the inode to the sync log so we're completely\n\t * safe.\n\t */\n\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &BTRFS_I(inode)->runtime_flags);\n\ttrans->block_rsv = rsv;\n\n\twhile (1) {\n\t\tret = btrfs_truncate_inode_items(trans, root, inode,\n\t\t\t\t\t\t inode->i_size,\n\t\t\t\t\t\t BTRFS_EXTENT_DATA_KEY);\n\t\tif (ret != -ENOSPC) {\n\t\t\terr = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t\tif (ret) {\n\t\t\terr = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\tbtrfs_end_transaction(trans, root);\n\t\tbtrfs_btree_balance_dirty(root);\n\n\t\ttrans = btrfs_start_transaction(root, 2);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = err = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv,\n\t\t\t\t\t      rsv, min_size);\n\t\tBUG_ON(ret);\t/* shouldn't happen */\n\t\ttrans->block_rsv = rsv;\n\t}\n\n\tif (ret == 0 && inode->i_nlink > 0) {\n\t\ttrans->block_rsv = root->orphan_block_rsv;\n\t\tret = btrfs_orphan_del(trans, inode);\n\t\tif (ret)\n\t\t\terr = ret;\n\t} else if (ret && inode->i_nlink > 0) {\n\t\t/*\n\t\t * Failed to do the truncate, remove us from the in memory\n\t\t * orphan list.\n\t\t */\n\t\tret = btrfs_orphan_del(NULL, inode);\n\t}\n\n\tif (trans) {\n\t\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t\tif (ret && !err)\n\t\t\terr = ret;\n\n\t\tret = btrfs_end_transaction(trans, root);\n\t\tbtrfs_btree_balance_dirty(root);\n\t}\n\nout:\n\tbtrfs_free_block_rsv(root, rsv);\n\n\tif (ret && !err)\n\t\terr = ret;\n\n\treturn err;\n}\n\n/*\n * create a new subvolume directory/inode (helper for the ioctl).\n */\nint btrfs_create_subvol_root(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *new_root, u64 new_dirid)\n{\n\tstruct inode *inode;\n\tint err;\n\tu64 index = 0;\n\n\tinode = btrfs_new_inode(trans, new_root, NULL, \"..\", 2,\n\t\t\t\tnew_dirid, new_dirid,\n\t\t\t\tS_IFDIR | (~current_umask() & S_IRWXUGO),\n\t\t\t\t&index);\n\tif (IS_ERR(inode))\n\t\treturn PTR_ERR(inode);\n\tinode->i_op = &btrfs_dir_inode_operations;\n\tinode->i_fop = &btrfs_dir_file_operations;\n\n\tset_nlink(inode, 1);\n\tbtrfs_i_size_write(inode, 0);\n\n\terr = btrfs_update_inode(trans, new_root, inode);\n\n\tiput(inode);\n\treturn err;\n}\n\nstruct inode *btrfs_alloc_inode(struct super_block *sb)\n{\n\tstruct btrfs_inode *ei;\n\tstruct inode *inode;\n\n\tei = kmem_cache_alloc(btrfs_inode_cachep, GFP_NOFS);\n\tif (!ei)\n\t\treturn NULL;\n\n\tei->root = NULL;\n\tei->generation = 0;\n\tei->last_trans = 0;\n\tei->last_sub_trans = 0;\n\tei->logged_trans = 0;\n\tei->delalloc_bytes = 0;\n\tei->disk_i_size = 0;\n\tei->flags = 0;\n\tei->csum_bytes = 0;\n\tei->index_cnt = (u64)-1;\n\tei->last_unlink_trans = 0;\n\tei->last_log_commit = 0;\n\n\tspin_lock_init(&ei->lock);\n\tei->outstanding_extents = 0;\n\tei->reserved_extents = 0;\n\n\tei->runtime_flags = 0;\n\tei->force_compress = BTRFS_COMPRESS_NONE;\n\n\tei->delayed_node = NULL;\n\n\tinode = &ei->vfs_inode;\n\textent_map_tree_init(&ei->extent_tree);\n\textent_io_tree_init(&ei->io_tree, &inode->i_data);\n\textent_io_tree_init(&ei->io_failure_tree, &inode->i_data);\n\tei->io_tree.track_uptodate = 1;\n\tei->io_failure_tree.track_uptodate = 1;\n\tatomic_set(&ei->sync_writers, 0);\n\tmutex_init(&ei->log_mutex);\n\tmutex_init(&ei->delalloc_mutex);\n\tbtrfs_ordered_inode_tree_init(&ei->ordered_tree);\n\tINIT_LIST_HEAD(&ei->delalloc_inodes);\n\tINIT_LIST_HEAD(&ei->ordered_operations);\n\tRB_CLEAR_NODE(&ei->rb_node);\n\n\treturn inode;\n}\n\nstatic void btrfs_i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(btrfs_inode_cachep, BTRFS_I(inode));\n}\n\nvoid btrfs_destroy_inode(struct inode *inode)\n{\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\tWARN_ON(!hlist_empty(&inode->i_dentry));\n\tWARN_ON(inode->i_data.nrpages);\n\tWARN_ON(BTRFS_I(inode)->outstanding_extents);\n\tWARN_ON(BTRFS_I(inode)->reserved_extents);\n\tWARN_ON(BTRFS_I(inode)->delalloc_bytes);\n\tWARN_ON(BTRFS_I(inode)->csum_bytes);\n\n\t/*\n\t * This can happen where we create an inode, but somebody else also\n\t * created the same inode and we need to destroy the one we already\n\t * created.\n\t */\n\tif (!root)\n\t\tgoto free;\n\n\t/*\n\t * Make sure we're properly removed from the ordered operation\n\t * lists.\n\t */\n\tsmp_mb();\n\tif (!list_empty(&BTRFS_I(inode)->ordered_operations)) {\n\t\tspin_lock(&root->fs_info->ordered_extent_lock);\n\t\tlist_del_init(&BTRFS_I(inode)->ordered_operations);\n\t\tspin_unlock(&root->fs_info->ordered_extent_lock);\n\t}\n\n\tif (test_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t     &BTRFS_I(inode)->runtime_flags)) {\n\t\tprintk(KERN_INFO \"BTRFS: inode %llu still on the orphan list\\n\",\n\t\t       (unsigned long long)btrfs_ino(inode));\n\t\tatomic_dec(&root->orphan_inodes);\n\t}\n\n\twhile (1) {\n\t\tordered = btrfs_lookup_first_ordered_extent(inode, (u64)-1);\n\t\tif (!ordered)\n\t\t\tbreak;\n\t\telse {\n\t\t\tprintk(KERN_ERR \"btrfs found ordered \"\n\t\t\t       \"extent %llu %llu on inode cleanup\\n\",\n\t\t\t       (unsigned long long)ordered->file_offset,\n\t\t\t       (unsigned long long)ordered->len);\n\t\t\tbtrfs_remove_ordered_extent(inode, ordered);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t}\n\t}\n\tinode_tree_del(inode);\n\tbtrfs_drop_extent_cache(inode, 0, (u64)-1, 0);\nfree:\n\tbtrfs_remove_delayed_node(inode);\n\tcall_rcu(&inode->i_rcu, btrfs_i_callback);\n}\n\nint btrfs_drop_inode(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\tif (btrfs_root_refs(&root->root_item) == 0 &&\n\t    !btrfs_is_free_space_inode(inode))\n\t\treturn 1;\n\telse\n\t\treturn generic_drop_inode(inode);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct btrfs_inode *ei = (struct btrfs_inode *) foo;\n\n\tinode_init_once(&ei->vfs_inode);\n}\n\nvoid btrfs_destroy_cachep(void)\n{\n\t/*\n\t * Make sure all delayed rcu free inodes are flushed before we\n\t * destroy cache.\n\t */\n\trcu_barrier();\n\tif (btrfs_inode_cachep)\n\t\tkmem_cache_destroy(btrfs_inode_cachep);\n\tif (btrfs_trans_handle_cachep)\n\t\tkmem_cache_destroy(btrfs_trans_handle_cachep);\n\tif (btrfs_transaction_cachep)\n\t\tkmem_cache_destroy(btrfs_transaction_cachep);\n\tif (btrfs_path_cachep)\n\t\tkmem_cache_destroy(btrfs_path_cachep);\n\tif (btrfs_free_space_cachep)\n\t\tkmem_cache_destroy(btrfs_free_space_cachep);\n\tif (btrfs_delalloc_work_cachep)\n\t\tkmem_cache_destroy(btrfs_delalloc_work_cachep);\n}\n\nint btrfs_init_cachep(void)\n{\n\tbtrfs_inode_cachep = kmem_cache_create(\"btrfs_inode\",\n\t\t\tsizeof(struct btrfs_inode), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, init_once);\n\tif (!btrfs_inode_cachep)\n\t\tgoto fail;\n\n\tbtrfs_trans_handle_cachep = kmem_cache_create(\"btrfs_trans_handle\",\n\t\t\tsizeof(struct btrfs_trans_handle), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_trans_handle_cachep)\n\t\tgoto fail;\n\n\tbtrfs_transaction_cachep = kmem_cache_create(\"btrfs_transaction\",\n\t\t\tsizeof(struct btrfs_transaction), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_transaction_cachep)\n\t\tgoto fail;\n\n\tbtrfs_path_cachep = kmem_cache_create(\"btrfs_path\",\n\t\t\tsizeof(struct btrfs_path), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_path_cachep)\n\t\tgoto fail;\n\n\tbtrfs_free_space_cachep = kmem_cache_create(\"btrfs_free_space\",\n\t\t\tsizeof(struct btrfs_free_space), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_free_space_cachep)\n\t\tgoto fail;\n\n\tbtrfs_delalloc_work_cachep = kmem_cache_create(\"btrfs_delalloc_work\",\n\t\t\tsizeof(struct btrfs_delalloc_work), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD,\n\t\t\tNULL);\n\tif (!btrfs_delalloc_work_cachep)\n\t\tgoto fail;\n\n\treturn 0;\nfail:\n\tbtrfs_destroy_cachep();\n\treturn -ENOMEM;\n}\n\nstatic int btrfs_getattr(struct vfsmount *mnt,\n\t\t\t struct dentry *dentry, struct kstat *stat)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tu32 blocksize = inode->i_sb->s_blocksize;\n\n\tgeneric_fillattr(inode, stat);\n\tstat->dev = BTRFS_I(inode)->root->anon_dev;\n\tstat->blksize = PAGE_CACHE_SIZE;\n\tstat->blocks = (ALIGN(inode_get_bytes(inode), blocksize) +\n\t\tALIGN(BTRFS_I(inode)->delalloc_bytes, blocksize)) >> 9;\n\treturn 0;\n}\n\n/*\n * If a file is moved, it will inherit the cow and compression flags of the new\n * directory.\n */\nstatic void fixup_inode_flags(struct inode *dir, struct inode *inode)\n{\n\tstruct btrfs_inode *b_dir = BTRFS_I(dir);\n\tstruct btrfs_inode *b_inode = BTRFS_I(inode);\n\n\tif (b_dir->flags & BTRFS_INODE_NODATACOW)\n\t\tb_inode->flags |= BTRFS_INODE_NODATACOW;\n\telse\n\t\tb_inode->flags &= ~BTRFS_INODE_NODATACOW;\n\n\tif (b_dir->flags & BTRFS_INODE_COMPRESS) {\n\t\tb_inode->flags |= BTRFS_INODE_COMPRESS;\n\t\tb_inode->flags &= ~BTRFS_INODE_NOCOMPRESS;\n\t} else {\n\t\tb_inode->flags &= ~(BTRFS_INODE_COMPRESS |\n\t\t\t\t    BTRFS_INODE_NOCOMPRESS);\n\t}\n}\n\nstatic int btrfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t\t\t   struct inode *new_dir, struct dentry *new_dentry)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(old_dir)->root;\n\tstruct btrfs_root *dest = BTRFS_I(new_dir)->root;\n\tstruct inode *new_inode = new_dentry->d_inode;\n\tstruct inode *old_inode = old_dentry->d_inode;\n\tstruct timespec ctime = CURRENT_TIME;\n\tu64 index = 0;\n\tu64 root_objectid;\n\tint ret;\n\tu64 old_ino = btrfs_ino(old_inode);\n\n\tif (btrfs_ino(new_dir) == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)\n\t\treturn -EPERM;\n\n\t/* we only allow rename subvolume link between subvolumes */\n\tif (old_ino != BTRFS_FIRST_FREE_OBJECTID && root != dest)\n\t\treturn -EXDEV;\n\n\tif (old_ino == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID ||\n\t    (new_inode && btrfs_ino(new_inode) == BTRFS_FIRST_FREE_OBJECTID))\n\t\treturn -ENOTEMPTY;\n\n\tif (S_ISDIR(old_inode->i_mode) && new_inode &&\n\t    new_inode->i_size > BTRFS_EMPTY_DIR_SIZE)\n\t\treturn -ENOTEMPTY;\n\t/*\n\t * we're using rename to replace one file with another.\n\t * and the replacement file is large.  Start IO on it now so\n\t * we don't add too much work to the end of the transaction\n\t */\n\tif (new_inode && S_ISREG(old_inode->i_mode) && new_inode->i_size &&\n\t    old_inode->i_size > BTRFS_ORDERED_OPERATIONS_FLUSH_LIMIT)\n\t\tfilemap_flush(old_inode->i_mapping);\n\n\t/* close the racy window with snapshot create/destroy ioctl */\n\tif (old_ino == BTRFS_FIRST_FREE_OBJECTID)\n\t\tdown_read(&root->fs_info->subvol_sem);\n\t/*\n\t * We want to reserve the absolute worst case amount of items.  So if\n\t * both inodes are subvols and we need to unlink them then that would\n\t * require 4 item modifications, but if they are both normal inodes it\n\t * would require 5 item modifications, so we'll assume their normal\n\t * inodes.  So 5 * 2 is 10, plus 1 for the new link, so 11 total items\n\t * should cover the worst case number of items we'll modify.\n\t */\n\ttrans = btrfs_start_transaction(root, 20);\n\tif (IS_ERR(trans)) {\n                ret = PTR_ERR(trans);\n                goto out_notrans;\n        }\n\n\tif (dest != root)\n\t\tbtrfs_record_root_in_trans(trans, dest);\n\n\tret = btrfs_set_inode_index(new_dir, &index);\n\tif (ret)\n\t\tgoto out_fail;\n\n\tif (unlikely(old_ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\t/* force full log commit if subvolume involved. */\n\t\troot->fs_info->last_trans_log_full_commit = trans->transid;\n\t} else {\n\t\tret = btrfs_insert_inode_ref(trans, dest,\n\t\t\t\t\t     new_dentry->d_name.name,\n\t\t\t\t\t     new_dentry->d_name.len,\n\t\t\t\t\t     old_ino,\n\t\t\t\t\t     btrfs_ino(new_dir), index);\n\t\tif (ret)\n\t\t\tgoto out_fail;\n\t\t/*\n\t\t * this is an ugly little race, but the rename is required\n\t\t * to make sure that if we crash, the inode is either at the\n\t\t * old name or the new one.  pinning the log transaction lets\n\t\t * us make sure we don't allow a log commit to come in after\n\t\t * we unlink the name but before we add the new name back in.\n\t\t */\n\t\tbtrfs_pin_log_trans(root);\n\t}\n\t/*\n\t * make sure the inode gets flushed if it is replacing\n\t * something.\n\t */\n\tif (new_inode && new_inode->i_size && S_ISREG(old_inode->i_mode))\n\t\tbtrfs_add_ordered_operation(trans, root, old_inode);\n\n\tinode_inc_iversion(old_dir);\n\tinode_inc_iversion(new_dir);\n\tinode_inc_iversion(old_inode);\n\told_dir->i_ctime = old_dir->i_mtime = ctime;\n\tnew_dir->i_ctime = new_dir->i_mtime = ctime;\n\told_inode->i_ctime = ctime;\n\n\tif (old_dentry->d_parent != new_dentry->d_parent)\n\t\tbtrfs_record_unlink_dir(trans, old_dir, old_inode, 1);\n\n\tif (unlikely(old_ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\troot_objectid = BTRFS_I(old_inode)->root->root_key.objectid;\n\t\tret = btrfs_unlink_subvol(trans, root, old_dir, root_objectid,\n\t\t\t\t\told_dentry->d_name.name,\n\t\t\t\t\told_dentry->d_name.len);\n\t} else {\n\t\tret = __btrfs_unlink_inode(trans, root, old_dir,\n\t\t\t\t\told_dentry->d_inode,\n\t\t\t\t\told_dentry->d_name.name,\n\t\t\t\t\told_dentry->d_name.len);\n\t\tif (!ret)\n\t\t\tret = btrfs_update_inode(trans, root, old_inode);\n\t}\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_fail;\n\t}\n\n\tif (new_inode) {\n\t\tinode_inc_iversion(new_inode);\n\t\tnew_inode->i_ctime = CURRENT_TIME;\n\t\tif (unlikely(btrfs_ino(new_inode) ==\n\t\t\t     BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)) {\n\t\t\troot_objectid = BTRFS_I(new_inode)->location.objectid;\n\t\t\tret = btrfs_unlink_subvol(trans, dest, new_dir,\n\t\t\t\t\t\troot_objectid,\n\t\t\t\t\t\tnew_dentry->d_name.name,\n\t\t\t\t\t\tnew_dentry->d_name.len);\n\t\t\tBUG_ON(new_inode->i_nlink == 0);\n\t\t} else {\n\t\t\tret = btrfs_unlink_inode(trans, dest, new_dir,\n\t\t\t\t\t\t new_dentry->d_inode,\n\t\t\t\t\t\t new_dentry->d_name.name,\n\t\t\t\t\t\t new_dentry->d_name.len);\n\t\t}\n\t\tif (!ret && new_inode->i_nlink == 0) {\n\t\t\tret = btrfs_orphan_add(trans, new_dentry->d_inode);\n\t\t\tBUG_ON(ret);\n\t\t}\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out_fail;\n\t\t}\n\t}\n\n\tfixup_inode_flags(new_dir, old_inode);\n\n\tret = btrfs_add_link(trans, new_dir, old_inode,\n\t\t\t     new_dentry->d_name.name,\n\t\t\t     new_dentry->d_name.len, 0, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_fail;\n\t}\n\n\tif (old_ino != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tstruct dentry *parent = new_dentry->d_parent;\n\t\tbtrfs_log_new_name(trans, old_inode, old_dir, parent);\n\t\tbtrfs_end_log_trans(root);\n\t}\nout_fail:\n\tbtrfs_end_transaction(trans, root);\nout_notrans:\n\tif (old_ino == BTRFS_FIRST_FREE_OBJECTID)\n\t\tup_read(&root->fs_info->subvol_sem);\n\n\treturn ret;\n}\n\nstatic void btrfs_run_delalloc_work(struct btrfs_work *work)\n{\n\tstruct btrfs_delalloc_work *delalloc_work;\n\n\tdelalloc_work = container_of(work, struct btrfs_delalloc_work,\n\t\t\t\t     work);\n\tif (delalloc_work->wait)\n\t\tbtrfs_wait_ordered_range(delalloc_work->inode, 0, (u64)-1);\n\telse\n\t\tfilemap_flush(delalloc_work->inode->i_mapping);\n\n\tif (delalloc_work->delay_iput)\n\t\tbtrfs_add_delayed_iput(delalloc_work->inode);\n\telse\n\t\tiput(delalloc_work->inode);\n\tcomplete(&delalloc_work->completion);\n}\n\nstruct btrfs_delalloc_work *btrfs_alloc_delalloc_work(struct inode *inode,\n\t\t\t\t\t\t    int wait, int delay_iput)\n{\n\tstruct btrfs_delalloc_work *work;\n\n\twork = kmem_cache_zalloc(btrfs_delalloc_work_cachep, GFP_NOFS);\n\tif (!work)\n\t\treturn NULL;\n\n\tinit_completion(&work->completion);\n\tINIT_LIST_HEAD(&work->list);\n\twork->inode = inode;\n\twork->wait = wait;\n\twork->delay_iput = delay_iput;\n\twork->work.func = btrfs_run_delalloc_work;\n\n\treturn work;\n}\n\nvoid btrfs_wait_and_free_delalloc_work(struct btrfs_delalloc_work *work)\n{\n\twait_for_completion(&work->completion);\n\tkmem_cache_free(btrfs_delalloc_work_cachep, work);\n}\n\n/*\n * some fairly slow code that needs optimization. This walks the list\n * of all the inodes with pending delalloc and forces them to disk.\n */\nint btrfs_start_delalloc_inodes(struct btrfs_root *root, int delay_iput)\n{\n\tstruct list_head *head = &root->fs_info->delalloc_inodes;\n\tstruct btrfs_inode *binode;\n\tstruct inode *inode;\n\tstruct btrfs_delalloc_work *work, *next;\n\tstruct list_head works;\n\tint ret = 0;\n\n\tif (root->fs_info->sb->s_flags & MS_RDONLY)\n\t\treturn -EROFS;\n\n\tINIT_LIST_HEAD(&works);\n\n\tspin_lock(&root->fs_info->delalloc_lock);\n\twhile (!list_empty(head)) {\n\t\tbinode = list_entry(head->next, struct btrfs_inode,\n\t\t\t\t    delalloc_inodes);\n\t\tinode = igrab(&binode->vfs_inode);\n\t\tif (!inode)\n\t\t\tlist_del_init(&binode->delalloc_inodes);\n\t\tspin_unlock(&root->fs_info->delalloc_lock);\n\t\tif (inode) {\n\t\t\twork = btrfs_alloc_delalloc_work(inode, 0, delay_iput);\n\t\t\tif (!work) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tlist_add_tail(&work->list, &works);\n\t\t\tbtrfs_queue_worker(&root->fs_info->flush_workers,\n\t\t\t\t\t   &work->work);\n\t\t}\n\t\tcond_resched();\n\t\tspin_lock(&root->fs_info->delalloc_lock);\n\t}\n\tspin_unlock(&root->fs_info->delalloc_lock);\n\n\t/* the filemap_flush will queue IO into the worker threads, but\n\t * we have to make sure the IO is actually started and that\n\t * ordered extents get created before we return\n\t */\n\tatomic_inc(&root->fs_info->async_submit_draining);\n\twhile (atomic_read(&root->fs_info->nr_async_submits) ||\n\t      atomic_read(&root->fs_info->async_delalloc_pages)) {\n\t\twait_event(root->fs_info->async_submit_wait,\n\t\t   (atomic_read(&root->fs_info->nr_async_submits) == 0 &&\n\t\t    atomic_read(&root->fs_info->async_delalloc_pages) == 0));\n\t}\n\tatomic_dec(&root->fs_info->async_submit_draining);\nout:\n\tlist_for_each_entry_safe(work, next, &works, list) {\n\t\tlist_del_init(&work->list);\n\t\tbtrfs_wait_and_free_delalloc_work(work);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_symlink(struct inode *dir, struct dentry *dentry,\n\t\t\t const char *symname)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct inode *inode = NULL;\n\tint err;\n\tint drop_inode = 0;\n\tu64 objectid;\n\tu64 index = 0 ;\n\tint name_len;\n\tint datasize;\n\tunsigned long ptr;\n\tstruct btrfs_file_extent_item *ei;\n\tstruct extent_buffer *leaf;\n\n\tname_len = strlen(symname) + 1;\n\tif (name_len > BTRFS_MAX_INLINE_DATA_SIZE(root))\n\t\treturn -ENAMETOOLONG;\n\n\t/*\n\t * 2 items for inode item and ref\n\t * 2 items for dir items\n\t * 1 item for xattr if selinux is on\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\terr = btrfs_find_free_ino(root, &objectid);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tinode = btrfs_new_inode(trans, root, dir, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len, btrfs_ino(dir), objectid,\n\t\t\t\tS_IFLNK|S_IRWXUGO, &index);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\tgoto out_unlock;\n\t}\n\n\terr = btrfs_init_inode_security(trans, inode, dir, &dentry->d_name);\n\tif (err) {\n\t\tdrop_inode = 1;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t* If the active LSM wants to access the inode during\n\t* d_instantiate it needs these. Smack checks to see\n\t* if the filesystem supports xattrs by looking at the\n\t* ops vector.\n\t*/\n\tinode->i_fop = &btrfs_file_operations;\n\tinode->i_op = &btrfs_file_inode_operations;\n\n\terr = btrfs_add_nondir(trans, dir, dentry, inode, 0, index);\n\tif (err)\n\t\tdrop_inode = 1;\n\telse {\n\t\tinode->i_mapping->a_ops = &btrfs_aops;\n\t\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\t\tBTRFS_I(inode)->io_tree.ops = &btrfs_extent_io_ops;\n\t}\n\tif (drop_inode)\n\t\tgoto out_unlock;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tdrop_inode = 1;\n\t\tgoto out_unlock;\n\t}\n\tkey.objectid = btrfs_ino(inode);\n\tkey.offset = 0;\n\tbtrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);\n\tdatasize = btrfs_file_extent_calc_inline_size(name_len);\n\terr = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      datasize);\n\tif (err) {\n\t\tdrop_inode = 1;\n\t\tbtrfs_free_path(path);\n\t\tgoto out_unlock;\n\t}\n\tleaf = path->nodes[0];\n\tei = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t    struct btrfs_file_extent_item);\n\tbtrfs_set_file_extent_generation(leaf, ei, trans->transid);\n\tbtrfs_set_file_extent_type(leaf, ei,\n\t\t\t\t   BTRFS_FILE_EXTENT_INLINE);\n\tbtrfs_set_file_extent_encryption(leaf, ei, 0);\n\tbtrfs_set_file_extent_compression(leaf, ei, 0);\n\tbtrfs_set_file_extent_other_encoding(leaf, ei, 0);\n\tbtrfs_set_file_extent_ram_bytes(leaf, ei, name_len);\n\n\tptr = btrfs_file_extent_inline_start(ei);\n\twrite_extent_buffer(leaf, symname, ptr, name_len);\n\tbtrfs_mark_buffer_dirty(leaf);\n\tbtrfs_free_path(path);\n\n\tinode->i_op = &btrfs_symlink_inode_operations;\n\tinode->i_mapping->a_ops = &btrfs_symlink_aops;\n\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\tinode_set_bytes(inode, name_len);\n\tbtrfs_i_size_write(inode, name_len - 1);\n\terr = btrfs_update_inode(trans, root, inode);\n\tif (err)\n\t\tdrop_inode = 1;\n\nout_unlock:\n\tif (!err)\n\t\td_instantiate(dentry, inode);\n\tbtrfs_end_transaction(trans, root);\n\tif (drop_inode) {\n\t\tinode_dec_link_count(inode);\n\t\tiput(inode);\n\t}\n\tbtrfs_btree_balance_dirty(root);\n\treturn err;\n}\n\nstatic int __btrfs_prealloc_file_range(struct inode *inode, int mode,\n\t\t\t\t       u64 start, u64 num_bytes, u64 min_size,\n\t\t\t\t       loff_t actual_len, u64 *alloc_hint,\n\t\t\t\t       struct btrfs_trans_handle *trans)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_key ins;\n\tu64 cur_offset = start;\n\tu64 i_size;\n\tint ret = 0;\n\tbool own_trans = true;\n\n\tif (trans)\n\t\town_trans = false;\n\twhile (num_bytes > 0) {\n\t\tif (own_trans) {\n\t\t\ttrans = btrfs_start_transaction(root, 3);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tret = btrfs_reserve_extent(trans, root, num_bytes, min_size,\n\t\t\t\t\t   0, *alloc_hint, &ins, 1);\n\t\tif (ret) {\n\t\t\tif (own_trans)\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tbreak;\n\t\t}\n\n\t\tret = insert_reserved_file_extent(trans, inode,\n\t\t\t\t\t\t  cur_offset, ins.objectid,\n\t\t\t\t\t\t  ins.offset, ins.offset,\n\t\t\t\t\t\t  ins.offset, 0, 0, 0,\n\t\t\t\t\t\t  BTRFS_FILE_EXTENT_PREALLOC);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tif (own_trans)\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_drop_extent_cache(inode, cur_offset,\n\t\t\t\t\tcur_offset + ins.offset -1, 0);\n\n\t\tem = alloc_extent_map();\n\t\tif (!em) {\n\t\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC,\n\t\t\t\t&BTRFS_I(inode)->runtime_flags);\n\t\t\tgoto next;\n\t\t}\n\n\t\tem->start = cur_offset;\n\t\tem->orig_start = cur_offset;\n\t\tem->len = ins.offset;\n\t\tem->block_start = ins.objectid;\n\t\tem->block_len = ins.offset;\n\t\tem->orig_block_len = ins.offset;\n\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\tset_bit(EXTENT_FLAG_PREALLOC, &em->flags);\n\t\tem->generation = trans->transid;\n\n\t\twhile (1) {\n\t\t\twrite_lock(&em_tree->lock);\n\t\t\tret = add_extent_mapping(em_tree, em);\n\t\t\tif (!ret)\n\t\t\t\tlist_move(&em->list,\n\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\twrite_unlock(&em_tree->lock);\n\t\t\tif (ret != -EEXIST)\n\t\t\t\tbreak;\n\t\t\tbtrfs_drop_extent_cache(inode, cur_offset,\n\t\t\t\t\t\tcur_offset + ins.offset - 1,\n\t\t\t\t\t\t0);\n\t\t}\n\t\tfree_extent_map(em);\nnext:\n\t\tnum_bytes -= ins.offset;\n\t\tcur_offset += ins.offset;\n\t\t*alloc_hint = ins.objectid + ins.offset;\n\n\t\tinode_inc_iversion(inode);\n\t\tinode->i_ctime = CURRENT_TIME;\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_PREALLOC;\n\t\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t\t    (actual_len > inode->i_size) &&\n\t\t    (cur_offset > inode->i_size)) {\n\t\t\tif (cur_offset > actual_len)\n\t\t\t\ti_size = actual_len;\n\t\t\telse\n\t\t\t\ti_size = cur_offset;\n\t\t\ti_size_write(inode, i_size);\n\t\t\tbtrfs_ordered_update_i_size(inode, i_size, NULL);\n\t\t}\n\n\t\tret = btrfs_update_inode(trans, root, inode);\n\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tif (own_trans)\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (own_trans)\n\t\t\tbtrfs_end_transaction(trans, root);\n\t}\n\treturn ret;\n}\n\nint btrfs_prealloc_file_range(struct inode *inode, int mode,\n\t\t\t      u64 start, u64 num_bytes, u64 min_size,\n\t\t\t      loff_t actual_len, u64 *alloc_hint)\n{\n\treturn __btrfs_prealloc_file_range(inode, mode, start, num_bytes,\n\t\t\t\t\t   min_size, actual_len, alloc_hint,\n\t\t\t\t\t   NULL);\n}\n\nint btrfs_prealloc_file_range_trans(struct inode *inode,\n\t\t\t\t    struct btrfs_trans_handle *trans, int mode,\n\t\t\t\t    u64 start, u64 num_bytes, u64 min_size,\n\t\t\t\t    loff_t actual_len, u64 *alloc_hint)\n{\n\treturn __btrfs_prealloc_file_range(inode, mode, start, num_bytes,\n\t\t\t\t\t   min_size, actual_len, alloc_hint, trans);\n}\n\nstatic int btrfs_set_page_dirty(struct page *page)\n{\n\treturn __set_page_dirty_nobuffers(page);\n}\n\nstatic int btrfs_permission(struct inode *inode, int mask)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tumode_t mode = inode->i_mode;\n\n\tif (mask & MAY_WRITE &&\n\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode))) {\n\t\tif (btrfs_root_readonly(root))\n\t\t\treturn -EROFS;\n\t\tif (BTRFS_I(inode)->flags & BTRFS_INODE_READONLY)\n\t\t\treturn -EACCES;\n\t}\n\treturn generic_permission(inode, mask);\n}\n\nstatic const struct inode_operations btrfs_dir_inode_operations = {\n\t.getattr\t= btrfs_getattr,\n\t.lookup\t\t= btrfs_lookup,\n\t.create\t\t= btrfs_create,\n\t.unlink\t\t= btrfs_unlink,\n\t.link\t\t= btrfs_link,\n\t.mkdir\t\t= btrfs_mkdir,\n\t.rmdir\t\t= btrfs_rmdir,\n\t.rename\t\t= btrfs_rename,\n\t.symlink\t= btrfs_symlink,\n\t.setattr\t= btrfs_setattr,\n\t.mknod\t\t= btrfs_mknod,\n\t.setxattr\t= btrfs_setxattr,\n\t.getxattr\t= btrfs_getxattr,\n\t.listxattr\t= btrfs_listxattr,\n\t.removexattr\t= btrfs_removexattr,\n\t.permission\t= btrfs_permission,\n\t.get_acl\t= btrfs_get_acl,\n};\nstatic const struct inode_operations btrfs_dir_ro_inode_operations = {\n\t.lookup\t\t= btrfs_lookup,\n\t.permission\t= btrfs_permission,\n\t.get_acl\t= btrfs_get_acl,\n};\n\nstatic const struct file_operations btrfs_dir_file_operations = {\n\t.llseek\t\t= generic_file_llseek,\n\t.read\t\t= generic_read_dir,\n\t.readdir\t= btrfs_real_readdir,\n\t.unlocked_ioctl\t= btrfs_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t= btrfs_ioctl,\n#endif\n\t.release        = btrfs_release_file,\n\t.fsync\t\t= btrfs_sync_file,\n};\n\nstatic struct extent_io_ops btrfs_extent_io_ops = {\n\t.fill_delalloc = run_delalloc_range,\n\t.submit_bio_hook = btrfs_submit_bio_hook,\n\t.merge_bio_hook = btrfs_merge_bio_hook,\n\t.readpage_end_io_hook = btrfs_readpage_end_io_hook,\n\t.writepage_end_io_hook = btrfs_writepage_end_io_hook,\n\t.writepage_start_hook = btrfs_writepage_start_hook,\n\t.set_bit_hook = btrfs_set_bit_hook,\n\t.clear_bit_hook = btrfs_clear_bit_hook,\n\t.merge_extent_hook = btrfs_merge_extent_hook,\n\t.split_extent_hook = btrfs_split_extent_hook,\n};\n\n/*\n * btrfs doesn't support the bmap operation because swapfiles\n * use bmap to make a mapping of extents in the file.  They assume\n * these extents won't change over the life of the file and they\n * use the bmap result to do IO directly to the drive.\n *\n * the btrfs bmap call would return logical addresses that aren't\n * suitable for IO and they also will change frequently as COW\n * operations happen.  So, swapfile + btrfs == corruption.\n *\n * For now we're avoiding this by dropping bmap.\n */\nstatic const struct address_space_operations btrfs_aops = {\n\t.readpage\t= btrfs_readpage,\n\t.writepage\t= btrfs_writepage,\n\t.writepages\t= btrfs_writepages,\n\t.readpages\t= btrfs_readpages,\n\t.direct_IO\t= btrfs_direct_IO,\n\t.invalidatepage = btrfs_invalidatepage,\n\t.releasepage\t= btrfs_releasepage,\n\t.set_page_dirty\t= btrfs_set_page_dirty,\n\t.error_remove_page = generic_error_remove_page,\n};\n\nstatic const struct address_space_operations btrfs_symlink_aops = {\n\t.readpage\t= btrfs_readpage,\n\t.writepage\t= btrfs_writepage,\n\t.invalidatepage = btrfs_invalidatepage,\n\t.releasepage\t= btrfs_releasepage,\n};\n\nstatic const struct inode_operations btrfs_file_inode_operations = {\n\t.getattr\t= btrfs_getattr,\n\t.setattr\t= btrfs_setattr,\n\t.setxattr\t= btrfs_setxattr,\n\t.getxattr\t= btrfs_getxattr,\n\t.listxattr      = btrfs_listxattr,\n\t.removexattr\t= btrfs_removexattr,\n\t.permission\t= btrfs_permission,\n\t.fiemap\t\t= btrfs_fiemap,\n\t.get_acl\t= btrfs_get_acl,\n\t.update_time\t= btrfs_update_time,\n};\nstatic const struct inode_operations btrfs_special_inode_operations = {\n\t.getattr\t= btrfs_getattr,\n\t.setattr\t= btrfs_setattr,\n\t.permission\t= btrfs_permission,\n\t.setxattr\t= btrfs_setxattr,\n\t.getxattr\t= btrfs_getxattr,\n\t.listxattr\t= btrfs_listxattr,\n\t.removexattr\t= btrfs_removexattr,\n\t.get_acl\t= btrfs_get_acl,\n\t.update_time\t= btrfs_update_time,\n};\nstatic const struct inode_operations btrfs_symlink_inode_operations = {\n\t.readlink\t= generic_readlink,\n\t.follow_link\t= page_follow_link_light,\n\t.put_link\t= page_put_link,\n\t.getattr\t= btrfs_getattr,\n\t.setattr\t= btrfs_setattr,\n\t.permission\t= btrfs_permission,\n\t.setxattr\t= btrfs_setxattr,\n\t.getxattr\t= btrfs_getxattr,\n\t.listxattr\t= btrfs_listxattr,\n\t.removexattr\t= btrfs_removexattr,\n\t.get_acl\t= btrfs_get_acl,\n\t.update_time\t= btrfs_update_time,\n};\n\nconst struct dentry_operations btrfs_dentry_operations = {\n\t.d_delete\t= btrfs_dentry_delete,\n\t.d_release\t= btrfs_dentry_release,\n};\n", "/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#include <linux/kernel.h>\n#include <linux/bio.h>\n#include <linux/buffer_head.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/fsnotify.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/time.h>\n#include <linux/init.h>\n#include <linux/string.h>\n#include <linux/backing-dev.h>\n#include <linux/mount.h>\n#include <linux/mpage.h>\n#include <linux/namei.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/statfs.h>\n#include <linux/compat.h>\n#include <linux/bit_spinlock.h>\n#include <linux/security.h>\n#include <linux/xattr.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/uuid.h>\n#include \"compat.h\"\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"btrfs_inode.h\"\n#include \"ioctl.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"locking.h\"\n#include \"inode-map.h\"\n#include \"backref.h\"\n#include \"rcu-string.h\"\n#include \"send.h\"\n#include \"dev-replace.h\"\n\n/* Mask out flags that are inappropriate for the given type of inode. */\nstatic inline __u32 btrfs_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & ~FS_DIRSYNC_FL;\n\telse\n\t\treturn flags & (FS_NODUMP_FL | FS_NOATIME_FL);\n}\n\n/*\n * Export inode flags to the format expected by the FS_IOC_GETFLAGS ioctl.\n */\nstatic unsigned int btrfs_flags_to_ioctl(unsigned int flags)\n{\n\tunsigned int iflags = 0;\n\n\tif (flags & BTRFS_INODE_SYNC)\n\t\tiflags |= FS_SYNC_FL;\n\tif (flags & BTRFS_INODE_IMMUTABLE)\n\t\tiflags |= FS_IMMUTABLE_FL;\n\tif (flags & BTRFS_INODE_APPEND)\n\t\tiflags |= FS_APPEND_FL;\n\tif (flags & BTRFS_INODE_NODUMP)\n\t\tiflags |= FS_NODUMP_FL;\n\tif (flags & BTRFS_INODE_NOATIME)\n\t\tiflags |= FS_NOATIME_FL;\n\tif (flags & BTRFS_INODE_DIRSYNC)\n\t\tiflags |= FS_DIRSYNC_FL;\n\tif (flags & BTRFS_INODE_NODATACOW)\n\t\tiflags |= FS_NOCOW_FL;\n\n\tif ((flags & BTRFS_INODE_COMPRESS) && !(flags & BTRFS_INODE_NOCOMPRESS))\n\t\tiflags |= FS_COMPR_FL;\n\telse if (flags & BTRFS_INODE_NOCOMPRESS)\n\t\tiflags |= FS_NOCOMP_FL;\n\n\treturn iflags;\n}\n\n/*\n * Update inode->i_flags based on the btrfs internal flags.\n */\nvoid btrfs_update_iflags(struct inode *inode)\n{\n\tstruct btrfs_inode *ip = BTRFS_I(inode);\n\n\tinode->i_flags &= ~(S_SYNC|S_APPEND|S_IMMUTABLE|S_NOATIME|S_DIRSYNC);\n\n\tif (ip->flags & BTRFS_INODE_SYNC)\n\t\tinode->i_flags |= S_SYNC;\n\tif (ip->flags & BTRFS_INODE_IMMUTABLE)\n\t\tinode->i_flags |= S_IMMUTABLE;\n\tif (ip->flags & BTRFS_INODE_APPEND)\n\t\tinode->i_flags |= S_APPEND;\n\tif (ip->flags & BTRFS_INODE_NOATIME)\n\t\tinode->i_flags |= S_NOATIME;\n\tif (ip->flags & BTRFS_INODE_DIRSYNC)\n\t\tinode->i_flags |= S_DIRSYNC;\n}\n\n/*\n * Inherit flags from the parent inode.\n *\n * Currently only the compression flags and the cow flags are inherited.\n */\nvoid btrfs_inherit_iflags(struct inode *inode, struct inode *dir)\n{\n\tunsigned int flags;\n\n\tif (!dir)\n\t\treturn;\n\n\tflags = BTRFS_I(dir)->flags;\n\n\tif (flags & BTRFS_INODE_NOCOMPRESS) {\n\t\tBTRFS_I(inode)->flags &= ~BTRFS_INODE_COMPRESS;\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NOCOMPRESS;\n\t} else if (flags & BTRFS_INODE_COMPRESS) {\n\t\tBTRFS_I(inode)->flags &= ~BTRFS_INODE_NOCOMPRESS;\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_COMPRESS;\n\t}\n\n\tif (flags & BTRFS_INODE_NODATACOW)\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NODATACOW;\n\n\tbtrfs_update_iflags(inode);\n}\n\nstatic int btrfs_ioctl_getflags(struct file *file, void __user *arg)\n{\n\tstruct btrfs_inode *ip = BTRFS_I(file->f_path.dentry->d_inode);\n\tunsigned int flags = btrfs_flags_to_ioctl(ip->flags);\n\n\tif (copy_to_user(arg, &flags, sizeof(flags)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int check_flags(unsigned int flags)\n{\n\tif (flags & ~(FS_IMMUTABLE_FL | FS_APPEND_FL | \\\n\t\t      FS_NOATIME_FL | FS_NODUMP_FL | \\\n\t\t      FS_SYNC_FL | FS_DIRSYNC_FL | \\\n\t\t      FS_NOCOMP_FL | FS_COMPR_FL |\n\t\t      FS_NOCOW_FL))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((flags & FS_NOCOMP_FL) && (flags & FS_COMPR_FL))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int btrfs_ioctl_setflags(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct btrfs_inode *ip = BTRFS_I(inode);\n\tstruct btrfs_root *root = ip->root;\n\tstruct btrfs_trans_handle *trans;\n\tunsigned int flags, oldflags;\n\tint ret;\n\tu64 ip_oldflags;\n\tunsigned int i_oldflags;\n\tumode_t mode;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tif (copy_from_user(&flags, arg, sizeof(flags)))\n\t\treturn -EFAULT;\n\n\tret = check_flags(flags);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EACCES;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tip_oldflags = ip->flags;\n\ti_oldflags = inode->i_flags;\n\tmode = inode->i_mode;\n\n\tflags = btrfs_mask_flags(inode->i_mode, flags);\n\toldflags = btrfs_flags_to_ioctl(ip->flags);\n\tif ((flags ^ oldflags) & (FS_APPEND_FL | FS_IMMUTABLE_FL)) {\n\t\tif (!capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (flags & FS_SYNC_FL)\n\t\tip->flags |= BTRFS_INODE_SYNC;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_SYNC;\n\tif (flags & FS_IMMUTABLE_FL)\n\t\tip->flags |= BTRFS_INODE_IMMUTABLE;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_IMMUTABLE;\n\tif (flags & FS_APPEND_FL)\n\t\tip->flags |= BTRFS_INODE_APPEND;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_APPEND;\n\tif (flags & FS_NODUMP_FL)\n\t\tip->flags |= BTRFS_INODE_NODUMP;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_NODUMP;\n\tif (flags & FS_NOATIME_FL)\n\t\tip->flags |= BTRFS_INODE_NOATIME;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_NOATIME;\n\tif (flags & FS_DIRSYNC_FL)\n\t\tip->flags |= BTRFS_INODE_DIRSYNC;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_DIRSYNC;\n\tif (flags & FS_NOCOW_FL) {\n\t\tif (S_ISREG(mode)) {\n\t\t\t/*\n\t\t\t * It's safe to turn csums off here, no extents exist.\n\t\t\t * Otherwise we want the flag to reflect the real COW\n\t\t\t * status of the file and will not set it.\n\t\t\t */\n\t\t\tif (inode->i_size == 0)\n\t\t\t\tip->flags |= BTRFS_INODE_NODATACOW\n\t\t\t\t\t   | BTRFS_INODE_NODATASUM;\n\t\t} else {\n\t\t\tip->flags |= BTRFS_INODE_NODATACOW;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * Revert back under same assuptions as above\n\t\t */\n\t\tif (S_ISREG(mode)) {\n\t\t\tif (inode->i_size == 0)\n\t\t\t\tip->flags &= ~(BTRFS_INODE_NODATACOW\n\t\t\t\t             | BTRFS_INODE_NODATASUM);\n\t\t} else {\n\t\t\tip->flags &= ~BTRFS_INODE_NODATACOW;\n\t\t}\n\t}\n\n\t/*\n\t * The COMPRESS flag can only be changed by users, while the NOCOMPRESS\n\t * flag may be changed automatically if compression code won't make\n\t * things smaller.\n\t */\n\tif (flags & FS_NOCOMP_FL) {\n\t\tip->flags &= ~BTRFS_INODE_COMPRESS;\n\t\tip->flags |= BTRFS_INODE_NOCOMPRESS;\n\t} else if (flags & FS_COMPR_FL) {\n\t\tip->flags |= BTRFS_INODE_COMPRESS;\n\t\tip->flags &= ~BTRFS_INODE_NOCOMPRESS;\n\t} else {\n\t\tip->flags &= ~(BTRFS_INODE_COMPRESS | BTRFS_INODE_NOCOMPRESS);\n\t}\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_drop;\n\t}\n\n\tbtrfs_update_iflags(inode);\n\tinode_inc_iversion(inode);\n\tinode->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode(trans, root, inode);\n\n\tbtrfs_end_transaction(trans, root);\n out_drop:\n\tif (ret) {\n\t\tip->flags = ip_oldflags;\n\t\tinode->i_flags = i_oldflags;\n\t}\n\n out_unlock:\n\tmutex_unlock(&inode->i_mutex);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic int btrfs_ioctl_getversion(struct file *file, int __user *arg)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\n\treturn put_user(inode->i_generation, arg);\n}\n\nstatic noinline int btrfs_ioctl_fitrim(struct file *file, void __user *arg)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(fdentry(file)->d_sb);\n\tstruct btrfs_device *device;\n\tstruct request_queue *q;\n\tstruct fstrim_range range;\n\tu64 minlen = ULLONG_MAX;\n\tu64 num_devices = 0;\n\tu64 total_bytes = btrfs_super_total_bytes(fs_info->super_copy);\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(device, &fs_info->fs_devices->devices,\n\t\t\t\tdev_list) {\n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\t\tq = bdev_get_queue(device->bdev);\n\t\tif (blk_queue_discard(q)) {\n\t\t\tnum_devices++;\n\t\t\tminlen = min((u64)q->limits.discard_granularity,\n\t\t\t\t     minlen);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (!num_devices)\n\t\treturn -EOPNOTSUPP;\n\tif (copy_from_user(&range, arg, sizeof(range)))\n\t\treturn -EFAULT;\n\tif (range.start > total_bytes ||\n\t    range.len < fs_info->sb->s_blocksize)\n\t\treturn -EINVAL;\n\n\trange.len = min(range.len, total_bytes - range.start);\n\trange.minlen = max(range.minlen, minlen);\n\tret = btrfs_trim_fs(fs_info->tree_root, &range);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (copy_to_user(arg, &range, sizeof(range)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic noinline int create_subvol(struct btrfs_root *root,\n\t\t\t\t  struct dentry *dentry,\n\t\t\t\t  char *name, int namelen,\n\t\t\t\t  u64 *async_transid,\n\t\t\t\t  struct btrfs_qgroup_inherit **inherit)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_key key;\n\tstruct btrfs_root_item root_item;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_root *new_root;\n\tstruct dentry *parent = dentry->d_parent;\n\tstruct inode *dir;\n\tstruct timespec cur_time = CURRENT_TIME;\n\tint ret;\n\tint err;\n\tu64 objectid;\n\tu64 new_dirid = BTRFS_FIRST_FREE_OBJECTID;\n\tu64 index = 0;\n\tuuid_le new_uuid;\n\n\tret = btrfs_find_free_objectid(root->fs_info->tree_root, &objectid);\n\tif (ret)\n\t\treturn ret;\n\n\tdir = parent->d_inode;\n\n\t/*\n\t * 1 - inode item\n\t * 2 - refs\n\t * 1 - root item\n\t * 2 - dir items\n\t */\n\ttrans = btrfs_start_transaction(root, 6);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tret = btrfs_qgroup_inherit(trans, root->fs_info, 0, objectid,\n\t\t\t\t   inherit ? *inherit : NULL);\n\tif (ret)\n\t\tgoto fail;\n\n\tleaf = btrfs_alloc_free_block(trans, root, root->leafsize,\n\t\t\t\t      0, objectid, NULL, 0, 0, 0);\n\tif (IS_ERR(leaf)) {\n\t\tret = PTR_ERR(leaf);\n\t\tgoto fail;\n\t}\n\n\tmemset_extent_buffer(leaf, 0, 0, sizeof(struct btrfs_header));\n\tbtrfs_set_header_bytenr(leaf, leaf->start);\n\tbtrfs_set_header_generation(leaf, trans->transid);\n\tbtrfs_set_header_backref_rev(leaf, BTRFS_MIXED_BACKREF_REV);\n\tbtrfs_set_header_owner(leaf, objectid);\n\n\twrite_extent_buffer(leaf, root->fs_info->fsid,\n\t\t\t    (unsigned long)btrfs_header_fsid(leaf),\n\t\t\t    BTRFS_FSID_SIZE);\n\twrite_extent_buffer(leaf, root->fs_info->chunk_tree_uuid,\n\t\t\t    (unsigned long)btrfs_header_chunk_tree_uuid(leaf),\n\t\t\t    BTRFS_UUID_SIZE);\n\tbtrfs_mark_buffer_dirty(leaf);\n\n\tmemset(&root_item, 0, sizeof(root_item));\n\n\tinode_item = &root_item.inode;\n\tinode_item->generation = cpu_to_le64(1);\n\tinode_item->size = cpu_to_le64(3);\n\tinode_item->nlink = cpu_to_le32(1);\n\tinode_item->nbytes = cpu_to_le64(root->leafsize);\n\tinode_item->mode = cpu_to_le32(S_IFDIR | 0755);\n\n\troot_item.flags = 0;\n\troot_item.byte_limit = 0;\n\tinode_item->flags = cpu_to_le64(BTRFS_INODE_ROOT_ITEM_INIT);\n\n\tbtrfs_set_root_bytenr(&root_item, leaf->start);\n\tbtrfs_set_root_generation(&root_item, trans->transid);\n\tbtrfs_set_root_level(&root_item, 0);\n\tbtrfs_set_root_refs(&root_item, 1);\n\tbtrfs_set_root_used(&root_item, leaf->len);\n\tbtrfs_set_root_last_snapshot(&root_item, 0);\n\n\tbtrfs_set_root_generation_v2(&root_item,\n\t\t\tbtrfs_root_generation(&root_item));\n\tuuid_le_gen(&new_uuid);\n\tmemcpy(root_item.uuid, new_uuid.b, BTRFS_UUID_SIZE);\n\troot_item.otime.sec = cpu_to_le64(cur_time.tv_sec);\n\troot_item.otime.nsec = cpu_to_le32(cur_time.tv_nsec);\n\troot_item.ctime = root_item.otime;\n\tbtrfs_set_root_ctransid(&root_item, trans->transid);\n\tbtrfs_set_root_otransid(&root_item, trans->transid);\n\n\tbtrfs_tree_unlock(leaf);\n\tfree_extent_buffer(leaf);\n\tleaf = NULL;\n\n\tbtrfs_set_root_dirid(&root_item, new_dirid);\n\n\tkey.objectid = objectid;\n\tkey.offset = 0;\n\tbtrfs_set_key_type(&key, BTRFS_ROOT_ITEM_KEY);\n\tret = btrfs_insert_root(trans, root->fs_info->tree_root, &key,\n\t\t\t\t&root_item);\n\tif (ret)\n\t\tgoto fail;\n\n\tkey.offset = (u64)-1;\n\tnew_root = btrfs_read_fs_root_no_name(root->fs_info, &key);\n\tif (IS_ERR(new_root)) {\n\t\tbtrfs_abort_transaction(trans, root, PTR_ERR(new_root));\n\t\tret = PTR_ERR(new_root);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_record_root_in_trans(trans, new_root);\n\n\tret = btrfs_create_subvol_root(trans, new_root, new_dirid);\n\tif (ret) {\n\t\t/* We potentially lose an unused inode item here */\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * insert the directory item\n\t */\n\tret = btrfs_set_inode_index(dir, &index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_insert_dir_item(trans, root,\n\t\t\t\t    name, namelen, dir, &key,\n\t\t\t\t    BTRFS_FT_DIR, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_i_size_write(dir, dir->i_size + namelen * 2);\n\tret = btrfs_update_inode(trans, root, dir);\n\tBUG_ON(ret);\n\n\tret = btrfs_add_root_ref(trans, root->fs_info->tree_root,\n\t\t\t\t objectid, root->root_key.objectid,\n\t\t\t\t btrfs_ino(dir), index, name, namelen);\n\n\tBUG_ON(ret);\n\n\td_instantiate(dentry, btrfs_lookup_dentry(dir, dentry));\nfail:\n\tif (async_transid) {\n\t\t*async_transid = trans->transid;\n\t\terr = btrfs_commit_transaction_async(trans, root, 1);\n\t} else {\n\t\terr = btrfs_commit_transaction(trans, root);\n\t}\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int create_snapshot(struct btrfs_root *root, struct dentry *dentry,\n\t\t\t   char *name, int namelen, u64 *async_transid,\n\t\t\t   bool readonly, struct btrfs_qgroup_inherit **inherit)\n{\n\tstruct inode *inode;\n\tstruct btrfs_pending_snapshot *pending_snapshot;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\tif (!root->ref_cows)\n\t\treturn -EINVAL;\n\n\tpending_snapshot = kzalloc(sizeof(*pending_snapshot), GFP_NOFS);\n\tif (!pending_snapshot)\n\t\treturn -ENOMEM;\n\n\tbtrfs_init_block_rsv(&pending_snapshot->block_rsv,\n\t\t\t     BTRFS_BLOCK_RSV_TEMP);\n\tpending_snapshot->dentry = dentry;\n\tpending_snapshot->root = root;\n\tpending_snapshot->readonly = readonly;\n\tif (inherit) {\n\t\tpending_snapshot->inherit = *inherit;\n\t\t*inherit = NULL;\t/* take responsibility to free it */\n\t}\n\n\ttrans = btrfs_start_transaction(root->fs_info->extent_root, 6);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_snap_reserve_metadata(trans, pending_snapshot);\n\tBUG_ON(ret);\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tlist_add(&pending_snapshot->list,\n\t\t &trans->transaction->pending_snapshots);\n\tspin_unlock(&root->fs_info->trans_lock);\n\tif (async_transid) {\n\t\t*async_transid = trans->transid;\n\t\tret = btrfs_commit_transaction_async(trans,\n\t\t\t\t     root->fs_info->extent_root, 1);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans,\n\t\t\t\t\t       root->fs_info->extent_root);\n\t}\n\tif (ret) {\n\t\t/* cleanup_transaction has freed this for us */\n\t\tif (trans->aborted)\n\t\t\tpending_snapshot = NULL;\n\t\tgoto fail;\n\t}\n\n\tret = pending_snapshot->error;\n\tif (ret)\n\t\tgoto fail;\n\n\tret = btrfs_orphan_cleanup(pending_snapshot->snap);\n\tif (ret)\n\t\tgoto fail;\n\n\tinode = btrfs_lookup_dentry(dentry->d_parent->d_inode, dentry);\n\tif (IS_ERR(inode)) {\n\t\tret = PTR_ERR(inode);\n\t\tgoto fail;\n\t}\n\tBUG_ON(!inode);\n\td_instantiate(dentry, inode);\n\tret = 0;\nfail:\n\tkfree(pending_snapshot);\n\treturn ret;\n}\n\n/*  copy of check_sticky in fs/namei.c()\n* It's inline, so penalty for filesystems that don't use sticky bit is\n* minimal.\n*/\nstatic inline int btrfs_check_sticky(struct inode *dir, struct inode *inode)\n{\n\tkuid_t fsuid = current_fsuid();\n\n\tif (!(dir->i_mode & S_ISVTX))\n\t\treturn 0;\n\tif (uid_eq(inode->i_uid, fsuid))\n\t\treturn 0;\n\tif (uid_eq(dir->i_uid, fsuid))\n\t\treturn 0;\n\treturn !capable(CAP_FOWNER);\n}\n\n/*  copy of may_delete in fs/namei.c()\n *\tCheck whether we can remove a link victim from directory dir, check\n *  whether the type of victim is right.\n *  1. We can't do it if dir is read-only (done in permission())\n *  2. We should have write and exec permissions on dir\n *  3. We can't remove anything from append-only dir\n *  4. We can't do anything with immutable dir (done in permission())\n *  5. If the sticky bit on dir is set we should either\n *\ta. be owner of dir, or\n *\tb. be owner of victim, or\n *\tc. have CAP_FOWNER capability\n *  6. If the victim is append-only or immutable we can't do antyhing with\n *     links pointing to it.\n *  7. If we were asked to remove a directory and victim isn't one - ENOTDIR.\n *  8. If we were asked to remove a non-directory and victim isn't one - EISDIR.\n *  9. We can't remove a root or mountpoint.\n * 10. We don't allow removal of NFS sillyrenamed files; it's handled by\n *     nfs_async_unlink().\n */\n\nstatic int btrfs_may_delete(struct inode *dir,struct dentry *victim,int isdir)\n{\n\tint error;\n\n\tif (!victim->d_inode)\n\t\treturn -ENOENT;\n\n\tBUG_ON(victim->d_parent->d_inode != dir);\n\taudit_inode_child(dir, victim, AUDIT_TYPE_CHILD_DELETE);\n\n\terror = inode_permission(dir, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\tif (IS_APPEND(dir))\n\t\treturn -EPERM;\n\tif (btrfs_check_sticky(dir, victim->d_inode)||\n\t\tIS_APPEND(victim->d_inode)||\n\t    IS_IMMUTABLE(victim->d_inode) || IS_SWAPFILE(victim->d_inode))\n\t\treturn -EPERM;\n\tif (isdir) {\n\t\tif (!S_ISDIR(victim->d_inode->i_mode))\n\t\t\treturn -ENOTDIR;\n\t\tif (IS_ROOT(victim))\n\t\t\treturn -EBUSY;\n\t} else if (S_ISDIR(victim->d_inode->i_mode))\n\t\treturn -EISDIR;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\tif (victim->d_flags & DCACHE_NFSFS_RENAMED)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\n/* copy of may_create in fs/namei.c() */\nstatic inline int btrfs_may_create(struct inode *dir, struct dentry *child)\n{\n\tif (child->d_inode)\n\t\treturn -EEXIST;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\treturn inode_permission(dir, MAY_WRITE | MAY_EXEC);\n}\n\n/*\n * Create a new subvolume below @parent.  This is largely modeled after\n * sys_mkdirat and vfs_mkdir, but we only do a single component lookup\n * inside this filesystem so it's quite a bit simpler.\n */\nstatic noinline int btrfs_mksubvol(struct path *parent,\n\t\t\t\t   char *name, int namelen,\n\t\t\t\t   struct btrfs_root *snap_src,\n\t\t\t\t   u64 *async_transid, bool readonly,\n\t\t\t\t   struct btrfs_qgroup_inherit **inherit)\n{\n\tstruct inode *dir  = parent->dentry->d_inode;\n\tstruct dentry *dentry;\n\tint error;\n\n\tmutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);\n\n\tdentry = lookup_one_len(name, parent->dentry, namelen);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto out_unlock;\n\n\terror = -EEXIST;\n\tif (dentry->d_inode)\n\t\tgoto out_dput;\n\n\terror = btrfs_may_create(dir, dentry);\n\tif (error)\n\t\tgoto out_dput;\n\n\tdown_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);\n\n\tif (btrfs_root_refs(&BTRFS_I(dir)->root->root_item) == 0)\n\t\tgoto out_up_read;\n\n\tif (snap_src) {\n\t\terror = create_snapshot(snap_src, dentry, name, namelen,\n\t\t\t\t\tasync_transid, readonly, inherit);\n\t} else {\n\t\terror = create_subvol(BTRFS_I(dir)->root, dentry,\n\t\t\t\t      name, namelen, async_transid, inherit);\n\t}\n\tif (!error)\n\t\tfsnotify_mkdir(dir, dentry);\nout_up_read:\n\tup_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);\nout_dput:\n\tdput(dentry);\nout_unlock:\n\tmutex_unlock(&dir->i_mutex);\n\treturn error;\n}\n\n/*\n * When we're defragging a range, we don't want to kick it off again\n * if it is really just waiting for delalloc to send it down.\n * If we find a nice big extent or delalloc range for the bytes in the\n * file you want to defrag, we return 0 to let you know to skip this\n * part of the file\n */\nstatic int check_defrag_in_cache(struct inode *inode, u64 offset, int thresh)\n{\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tu64 end;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, offset, PAGE_CACHE_SIZE);\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tend = extent_map_end(em);\n\t\tfree_extent_map(em);\n\t\tif (end - offset > thresh)\n\t\t\treturn 0;\n\t}\n\t/* if we already have a nice delalloc here, just stop */\n\tthresh /= 2;\n\tend = count_range_bits(io_tree, &offset, offset + thresh,\n\t\t\t       thresh, EXTENT_DELALLOC, 1);\n\tif (end >= thresh)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * helper function to walk through a file and find extents\n * newer than a specific transid, and smaller than thresh.\n *\n * This is used by the defragging code to find new and small\n * extents\n */\nstatic int find_new_extents(struct btrfs_root *root,\n\t\t\t    struct inode *inode, u64 newer_than,\n\t\t\t    u64 *off, int thresh)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_key min_key;\n\tstruct btrfs_key max_key;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *extent;\n\tint type;\n\tint ret;\n\tu64 ino = btrfs_ino(inode);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmin_key.objectid = ino;\n\tmin_key.type = BTRFS_EXTENT_DATA_KEY;\n\tmin_key.offset = *off;\n\n\tmax_key.objectid = ino;\n\tmax_key.type = (u8)-1;\n\tmax_key.offset = (u64)-1;\n\n\tpath->keep_locks = 1;\n\n\twhile(1) {\n\t\tret = btrfs_search_forward(root, &min_key, &max_key,\n\t\t\t\t\t   path, 0, newer_than);\n\t\tif (ret != 0)\n\t\t\tgoto none;\n\t\tif (min_key.objectid != ino)\n\t\t\tgoto none;\n\t\tif (min_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto none;\n\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_file_extent_item);\n\n\t\ttype = btrfs_file_extent_type(leaf, extent);\n\t\tif (type == BTRFS_FILE_EXTENT_REG &&\n\t\t    btrfs_file_extent_num_bytes(leaf, extent) < thresh &&\n\t\t    check_defrag_in_cache(inode, min_key.offset, thresh)) {\n\t\t\t*off = min_key.offset;\n\t\t\tbtrfs_free_path(path);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (min_key.offset == (u64)-1)\n\t\t\tgoto none;\n\n\t\tmin_key.offset++;\n\t\tbtrfs_release_path(path);\n\t}\nnone:\n\tbtrfs_free_path(path);\n\treturn -ENOENT;\n}\n\nstatic struct extent_map *defrag_lookup_extent(struct inode *inode, u64 start)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em;\n\tu64 len = PAGE_CACHE_SIZE;\n\n\t/*\n\t * hopefully we have this extent in the tree already, try without\n\t * the full extent lock\n\t */\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\t/* get the big lock and read metadata off disk */\n\t\tlock_extent(io_tree, start, start + len - 1);\n\t\tem = btrfs_get_extent(inode, NULL, 0, start, len, 0);\n\t\tunlock_extent(io_tree, start, start + len - 1);\n\n\t\tif (IS_ERR(em))\n\t\t\treturn NULL;\n\t}\n\n\treturn em;\n}\n\nstatic bool defrag_check_next_extent(struct inode *inode, struct extent_map *em)\n{\n\tstruct extent_map *next;\n\tbool ret = true;\n\n\t/* this is the last extent */\n\tif (em->start + em->len >= i_size_read(inode))\n\t\treturn false;\n\n\tnext = defrag_lookup_extent(inode, em->start + em->len);\n\tif (!next || next->block_start >= EXTENT_MAP_LAST_BYTE)\n\t\tret = false;\n\n\tfree_extent_map(next);\n\treturn ret;\n}\n\nstatic int should_defrag_range(struct inode *inode, u64 start, int thresh,\n\t\t\t       u64 *last_len, u64 *skip, u64 *defrag_end,\n\t\t\t       int compress)\n{\n\tstruct extent_map *em;\n\tint ret = 1;\n\tbool next_mergeable = true;\n\n\t/*\n\t * make sure that once we start defragging an extent, we keep on\n\t * defragging it\n\t */\n\tif (start < *defrag_end)\n\t\treturn 1;\n\n\t*skip = 0;\n\n\tem = defrag_lookup_extent(inode, start);\n\tif (!em)\n\t\treturn 0;\n\n\t/* this will cover holes, and inline extents */\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tnext_mergeable = defrag_check_next_extent(inode, em);\n\n\t/*\n\t * we hit a real extent, if it is big or the next extent is not a\n\t * real extent, don't bother defragging it\n\t */\n\tif (!compress && (*last_len == 0 || *last_len >= thresh) &&\n\t    (em->len >= thresh || !next_mergeable))\n\t\tret = 0;\nout:\n\t/*\n\t * last_len ends up being a counter of how many bytes we've defragged.\n\t * every time we choose not to defrag an extent, we reset *last_len\n\t * so that the next tiny extent will force a defrag.\n\t *\n\t * The end result of this is that tiny extents before a single big\n\t * extent will force at least part of that big extent to be defragged.\n\t */\n\tif (ret) {\n\t\t*defrag_end = extent_map_end(em);\n\t} else {\n\t\t*last_len = 0;\n\t\t*skip = extent_map_end(em);\n\t\t*defrag_end = 0;\n\t}\n\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * it doesn't do much good to defrag one or two pages\n * at a time.  This pulls in a nice chunk of pages\n * to COW and defrag.\n *\n * It also makes sure the delalloc code has enough\n * dirty data to avoid making new small extents as part\n * of the defrag\n *\n * It's a good idea to start RA on this range\n * before calling this.\n */\nstatic int cluster_pages_for_defrag(struct inode *inode,\n\t\t\t\t    struct page **pages,\n\t\t\t\t    unsigned long start_index,\n\t\t\t\t    int num_pages)\n{\n\tunsigned long file_end;\n\tu64 isize = i_size_read(inode);\n\tu64 page_start;\n\tu64 page_end;\n\tu64 page_cnt;\n\tint ret;\n\tint i;\n\tint i_done;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tstruct extent_io_tree *tree;\n\tgfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);\n\n\tfile_end = (isize - 1) >> PAGE_CACHE_SHIFT;\n\tif (!isize || start_index > file_end)\n\t\treturn 0;\n\n\tpage_cnt = min_t(u64, (u64)num_pages, (u64)file_end - start_index + 1);\n\n\tret = btrfs_delalloc_reserve_space(inode,\n\t\t\t\t\t   page_cnt << PAGE_CACHE_SHIFT);\n\tif (ret)\n\t\treturn ret;\n\ti_done = 0;\n\ttree = &BTRFS_I(inode)->io_tree;\n\n\t/* step one, lock all the pages */\n\tfor (i = 0; i < page_cnt; i++) {\n\t\tstruct page *page;\nagain:\n\t\tpage = find_or_create_page(inode->i_mapping,\n\t\t\t\t\t   start_index + i, mask);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tpage_start = page_offset(page);\n\t\tpage_end = page_start + PAGE_CACHE_SIZE - 1;\n\t\twhile (1) {\n\t\t\tlock_extent(tree, page_start, page_end);\n\t\t\tordered = btrfs_lookup_ordered_extent(inode,\n\t\t\t\t\t\t\t      page_start);\n\t\t\tunlock_extent(tree, page_start, page_end);\n\t\t\tif (!ordered)\n\t\t\t\tbreak;\n\n\t\t\tunlock_page(page);\n\t\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t\tlock_page(page);\n\t\t\t/*\n\t\t\t * we unlocked the page above, so we need check if\n\t\t\t * it was released or not.\n\t\t\t */\n\t\t\tif (page->mapping != inode->i_mapping) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tpage_cache_release(page);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\n\t\tif (!PageUptodate(page)) {\n\t\t\tbtrfs_readpage(NULL, page);\n\t\t\tlock_page(page);\n\t\t\tif (!PageUptodate(page)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tpage_cache_release(page);\n\t\t\t\tret = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (page->mapping != inode->i_mapping) {\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t\tgoto again;\n\t\t}\n\n\t\tpages[i] = page;\n\t\ti_done++;\n\t}\n\tif (!i_done || ret)\n\t\tgoto out;\n\n\tif (!(inode->i_sb->s_flags & MS_ACTIVE))\n\t\tgoto out;\n\n\t/*\n\t * so now we have a nice long stream of locked\n\t * and up to date pages, lets wait on them\n\t */\n\tfor (i = 0; i < i_done; i++)\n\t\twait_on_page_writeback(pages[i]);\n\n\tpage_start = page_offset(pages[0]);\n\tpage_end = page_offset(pages[i_done - 1]) + PAGE_CACHE_SIZE;\n\n\tlock_extent_bits(&BTRFS_I(inode)->io_tree,\n\t\t\t page_start, page_end - 1, 0, &cached_state);\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, page_start,\n\t\t\t  page_end - 1, EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0,\n\t\t\t  &cached_state, GFP_NOFS);\n\n\tif (i_done != page_cnt) {\n\t\tspin_lock(&BTRFS_I(inode)->lock);\n\t\tBTRFS_I(inode)->outstanding_extents++;\n\t\tspin_unlock(&BTRFS_I(inode)->lock);\n\t\tbtrfs_delalloc_release_space(inode,\n\t\t\t\t     (page_cnt - i_done) << PAGE_CACHE_SHIFT);\n\t}\n\n\n\tset_extent_defrag(&BTRFS_I(inode)->io_tree, page_start, page_end - 1,\n\t\t\t  &cached_state, GFP_NOFS);\n\n\tunlock_extent_cached(&BTRFS_I(inode)->io_tree,\n\t\t\t     page_start, page_end - 1, &cached_state,\n\t\t\t     GFP_NOFS);\n\n\tfor (i = 0; i < i_done; i++) {\n\t\tclear_page_dirty_for_io(pages[i]);\n\t\tClearPageChecked(pages[i]);\n\t\tset_page_extent_mapped(pages[i]);\n\t\tset_page_dirty(pages[i]);\n\t\tunlock_page(pages[i]);\n\t\tpage_cache_release(pages[i]);\n\t}\n\treturn i_done;\nout:\n\tfor (i = 0; i < i_done; i++) {\n\t\tunlock_page(pages[i]);\n\t\tpage_cache_release(pages[i]);\n\t}\n\tbtrfs_delalloc_release_space(inode, page_cnt << PAGE_CACHE_SHIFT);\n\treturn ret;\n\n}\n\nint btrfs_defrag_file(struct inode *inode, struct file *file,\n\t\t      struct btrfs_ioctl_defrag_range_args *range,\n\t\t      u64 newer_than, unsigned long max_to_defrag)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct file_ra_state *ra = NULL;\n\tunsigned long last_index;\n\tu64 isize = i_size_read(inode);\n\tu64 last_len = 0;\n\tu64 skip = 0;\n\tu64 defrag_end = 0;\n\tu64 newer_off = range->start;\n\tunsigned long i;\n\tunsigned long ra_index = 0;\n\tint ret;\n\tint defrag_count = 0;\n\tint compress_type = BTRFS_COMPRESS_ZLIB;\n\tint extent_thresh = range->extent_thresh;\n\tint max_cluster = (256 * 1024) >> PAGE_CACHE_SHIFT;\n\tint cluster = max_cluster;\n\tu64 new_align = ~((u64)128 * 1024 - 1);\n\tstruct page **pages = NULL;\n\n\tif (extent_thresh == 0)\n\t\textent_thresh = 256 * 1024;\n\n\tif (range->flags & BTRFS_DEFRAG_RANGE_COMPRESS) {\n\t\tif (range->compress_type > BTRFS_COMPRESS_TYPES)\n\t\t\treturn -EINVAL;\n\t\tif (range->compress_type)\n\t\t\tcompress_type = range->compress_type;\n\t}\n\n\tif (isize == 0)\n\t\treturn 0;\n\n\t/*\n\t * if we were not given a file, allocate a readahead\n\t * context\n\t */\n\tif (!file) {\n\t\tra = kzalloc(sizeof(*ra), GFP_NOFS);\n\t\tif (!ra)\n\t\t\treturn -ENOMEM;\n\t\tfile_ra_state_init(ra, inode->i_mapping);\n\t} else {\n\t\tra = &file->f_ra;\n\t}\n\n\tpages = kmalloc(sizeof(struct page *) * max_cluster,\n\t\t\tGFP_NOFS);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out_ra;\n\t}\n\n\t/* find the last page to defrag */\n\tif (range->start + range->len > range->start) {\n\t\tlast_index = min_t(u64, isize - 1,\n\t\t\t range->start + range->len - 1) >> PAGE_CACHE_SHIFT;\n\t} else {\n\t\tlast_index = (isize - 1) >> PAGE_CACHE_SHIFT;\n\t}\n\n\tif (newer_than) {\n\t\tret = find_new_extents(root, inode, newer_than,\n\t\t\t\t       &newer_off, 64 * 1024);\n\t\tif (!ret) {\n\t\t\trange->start = newer_off;\n\t\t\t/*\n\t\t\t * we always align our defrag to help keep\n\t\t\t * the extents in the file evenly spaced\n\t\t\t */\n\t\t\ti = (newer_off & new_align) >> PAGE_CACHE_SHIFT;\n\t\t} else\n\t\t\tgoto out_ra;\n\t} else {\n\t\ti = range->start >> PAGE_CACHE_SHIFT;\n\t}\n\tif (!max_to_defrag)\n\t\tmax_to_defrag = last_index + 1;\n\n\t/*\n\t * make writeback starts from i, so the defrag range can be\n\t * written sequentially.\n\t */\n\tif (i < inode->i_mapping->writeback_index)\n\t\tinode->i_mapping->writeback_index = i;\n\n\twhile (i <= last_index && defrag_count < max_to_defrag &&\n\t       (i < (i_size_read(inode) + PAGE_CACHE_SIZE - 1) >>\n\t\tPAGE_CACHE_SHIFT)) {\n\t\t/*\n\t\t * make sure we stop running if someone unmounts\n\t\t * the FS\n\t\t */\n\t\tif (!(inode->i_sb->s_flags & MS_ACTIVE))\n\t\t\tbreak;\n\n\t\tif (!should_defrag_range(inode, (u64)i << PAGE_CACHE_SHIFT,\n\t\t\t\t\t extent_thresh, &last_len, &skip,\n\t\t\t\t\t &defrag_end, range->flags &\n\t\t\t\t\t BTRFS_DEFRAG_RANGE_COMPRESS)) {\n\t\t\tunsigned long next;\n\t\t\t/*\n\t\t\t * the should_defrag function tells us how much to skip\n\t\t\t * bump our counter by the suggested amount\n\t\t\t */\n\t\t\tnext = (skip + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\t\t\ti = max(i + 1, next);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!newer_than) {\n\t\t\tcluster = (PAGE_CACHE_ALIGN(defrag_end) >>\n\t\t\t\t   PAGE_CACHE_SHIFT) - i;\n\t\t\tcluster = min(cluster, max_cluster);\n\t\t} else {\n\t\t\tcluster = max_cluster;\n\t\t}\n\n\t\tif (range->flags & BTRFS_DEFRAG_RANGE_COMPRESS)\n\t\t\tBTRFS_I(inode)->force_compress = compress_type;\n\n\t\tif (i + cluster > ra_index) {\n\t\t\tra_index = max(i, ra_index);\n\t\t\tbtrfs_force_ra(inode->i_mapping, ra, file, ra_index,\n\t\t\t\t       cluster);\n\t\t\tra_index += max_cluster;\n\t\t}\n\n\t\tmutex_lock(&inode->i_mutex);\n\t\tret = cluster_pages_for_defrag(inode, pages, i, cluster);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&inode->i_mutex);\n\t\t\tgoto out_ra;\n\t\t}\n\n\t\tdefrag_count += ret;\n\t\tbalance_dirty_pages_ratelimited_nr(inode->i_mapping, ret);\n\t\tmutex_unlock(&inode->i_mutex);\n\n\t\tif (newer_than) {\n\t\t\tif (newer_off == (u64)-1)\n\t\t\t\tbreak;\n\n\t\t\tif (ret > 0)\n\t\t\t\ti += ret;\n\n\t\t\tnewer_off = max(newer_off + 1,\n\t\t\t\t\t(u64)i << PAGE_CACHE_SHIFT);\n\n\t\t\tret = find_new_extents(root, inode,\n\t\t\t\t\t       newer_than, &newer_off,\n\t\t\t\t\t       64 * 1024);\n\t\t\tif (!ret) {\n\t\t\t\trange->start = newer_off;\n\t\t\t\ti = (newer_off & new_align) >> PAGE_CACHE_SHIFT;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (ret > 0) {\n\t\t\t\ti += ret;\n\t\t\t\tlast_len += ret << PAGE_CACHE_SHIFT;\n\t\t\t} else {\n\t\t\t\ti++;\n\t\t\t\tlast_len = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((range->flags & BTRFS_DEFRAG_RANGE_START_IO))\n\t\tfilemap_flush(inode->i_mapping);\n\n\tif ((range->flags & BTRFS_DEFRAG_RANGE_COMPRESS)) {\n\t\t/* the filemap_flush will queue IO into the worker threads, but\n\t\t * we have to make sure the IO is actually started and that\n\t\t * ordered extents get created before we return\n\t\t */\n\t\tatomic_inc(&root->fs_info->async_submit_draining);\n\t\twhile (atomic_read(&root->fs_info->nr_async_submits) ||\n\t\t      atomic_read(&root->fs_info->async_delalloc_pages)) {\n\t\t\twait_event(root->fs_info->async_submit_wait,\n\t\t\t   (atomic_read(&root->fs_info->nr_async_submits) == 0 &&\n\t\t\t    atomic_read(&root->fs_info->async_delalloc_pages) == 0));\n\t\t}\n\t\tatomic_dec(&root->fs_info->async_submit_draining);\n\n\t\tmutex_lock(&inode->i_mutex);\n\t\tBTRFS_I(inode)->force_compress = BTRFS_COMPRESS_NONE;\n\t\tmutex_unlock(&inode->i_mutex);\n\t}\n\n\tif (range->compress_type == BTRFS_COMPRESS_LZO) {\n\t\tbtrfs_set_fs_incompat(root->fs_info, COMPRESS_LZO);\n\t}\n\n\tret = defrag_count;\n\nout_ra:\n\tif (!file)\n\t\tkfree(ra);\n\tkfree(pages);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_resize(struct file *file,\n\t\t\t\t\tvoid __user *arg)\n{\n\tu64 new_size;\n\tu64 old_size;\n\tu64 devid = 1;\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device = NULL;\n\tchar *sizestr;\n\tchar *devstr = NULL;\n\tint ret = 0;\n\tint mod = 0;\n\n\tif (root->fs_info->sb->s_flags & MS_RDONLY)\n\t\treturn -EROFS;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\treturn -EINPROGRESS;\n\t}\n\n\tmutex_lock(&root->fs_info->volume_mutex);\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tsizestr = vol_args->name;\n\tdevstr = strchr(sizestr, ':');\n\tif (devstr) {\n\t\tchar *end;\n\t\tsizestr = devstr + 1;\n\t\t*devstr = '\\0';\n\t\tdevstr = vol_args->name;\n\t\tdevid = simple_strtoull(devstr, &end, 10);\n\t\tprintk(KERN_INFO \"btrfs: resizing devid %llu\\n\",\n\t\t       (unsigned long long)devid);\n\t}\n\tdevice = btrfs_find_device(root->fs_info, devid, NULL, NULL);\n\tif (!device) {\n\t\tprintk(KERN_INFO \"btrfs: resizer unable to find device %llu\\n\",\n\t\t       (unsigned long long)devid);\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (device->fs_devices && device->fs_devices->seeding) {\n\t\tprintk(KERN_INFO \"btrfs: resizer unable to apply on \"\n\t\t       \"seeding device %llu\\n\",\n\t\t       (unsigned long long)devid);\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tif (!strcmp(sizestr, \"max\"))\n\t\tnew_size = device->bdev->bd_inode->i_size;\n\telse {\n\t\tif (sizestr[0] == '-') {\n\t\t\tmod = -1;\n\t\t\tsizestr++;\n\t\t} else if (sizestr[0] == '+') {\n\t\t\tmod = 1;\n\t\t\tsizestr++;\n\t\t}\n\t\tnew_size = memparse(sizestr, NULL);\n\t\tif (new_size == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (device->is_tgtdev_for_dev_replace) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\told_size = device->total_bytes;\n\n\tif (mod < 0) {\n\t\tif (new_size > old_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size - new_size;\n\t} else if (mod > 0) {\n\t\tnew_size = old_size + new_size;\n\t}\n\n\tif (new_size < 256 * 1024 * 1024) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (new_size > device->bdev->bd_inode->i_size) {\n\t\tret = -EFBIG;\n\t\tgoto out_free;\n\t}\n\n\tdo_div(new_size, root->sectorsize);\n\tnew_size *= root->sectorsize;\n\n\tprintk_in_rcu(KERN_INFO \"btrfs: new size for %s is %llu\\n\",\n\t\t      rcu_str_deref(device->name),\n\t\t      (unsigned long long)new_size);\n\n\tif (new_size > old_size) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = btrfs_grow_device(trans, device, new_size);\n\t\tbtrfs_commit_transaction(trans, root);\n\t} else if (new_size < old_size) {\n\t\tret = btrfs_shrink_device(device, new_size);\n\t} /* equal, nothing need to do */\n\nout_free:\n\tkfree(vol_args);\nout:\n\tmutex_unlock(&root->fs_info->volume_mutex);\n\tmnt_drop_write_file(file);\n\tatomic_set(&root->fs_info->mutually_exclusive_operation_running, 0);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create_transid(struct file *file,\n\t\t\t\tchar *name, unsigned long fd, int subvol,\n\t\t\t\tu64 *transid, bool readonly,\n\t\t\t\tstruct btrfs_qgroup_inherit **inherit)\n{\n\tint namelen;\n\tint ret = 0;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tnamelen = strlen(name);\n\tif (strchr(name, '/')) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (name[0] == '.' &&\n\t   (namelen == 1 || (name[1] == '.' && namelen == 2))) {\n\t\tret = -EEXIST;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (subvol) {\n\t\tret = btrfs_mksubvol(&file->f_path, name, namelen,\n\t\t\t\t     NULL, transid, readonly, inherit);\n\t} else {\n\t\tstruct fd src = fdget(fd);\n\t\tstruct inode *src_inode;\n\t\tif (!src.file) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_drop_write;\n\t\t}\n\n\t\tsrc_inode = src.file->f_path.dentry->d_inode;\n\t\tif (src_inode->i_sb != file->f_path.dentry->d_inode->i_sb) {\n\t\t\tprintk(KERN_INFO \"btrfs: Snapshot src from \"\n\t\t\t       \"another FS\\n\");\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tret = btrfs_mksubvol(&file->f_path, name, namelen,\n\t\t\t\t\t     BTRFS_I(src_inode)->root,\n\t\t\t\t\t     transid, readonly, inherit);\n\t\t}\n\t\tfdput(src);\n\t}\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create(struct file *file,\n\t\t\t\t\t    void __user *arg, int subvol)\n{\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tret = btrfs_ioctl_snap_create_transid(file, vol_args->name,\n\t\t\t\t\t      vol_args->fd, subvol,\n\t\t\t\t\t      NULL, false, NULL);\n\n\tkfree(vol_args);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create_v2(struct file *file,\n\t\t\t\t\t       void __user *arg, int subvol)\n{\n\tstruct btrfs_ioctl_vol_args_v2 *vol_args;\n\tint ret;\n\tu64 transid = 0;\n\tu64 *ptr = NULL;\n\tbool readonly = false;\n\tstruct btrfs_qgroup_inherit *inherit = NULL;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\tvol_args->name[BTRFS_SUBVOL_NAME_MAX] = '\\0';\n\n\tif (vol_args->flags &\n\t    ~(BTRFS_SUBVOL_CREATE_ASYNC | BTRFS_SUBVOL_RDONLY |\n\t      BTRFS_SUBVOL_QGROUP_INHERIT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (vol_args->flags & BTRFS_SUBVOL_CREATE_ASYNC)\n\t\tptr = &transid;\n\tif (vol_args->flags & BTRFS_SUBVOL_RDONLY)\n\t\treadonly = true;\n\tif (vol_args->flags & BTRFS_SUBVOL_QGROUP_INHERIT) {\n\t\tif (vol_args->size > PAGE_CACHE_SIZE) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tinherit = memdup_user(vol_args->qgroup_inherit, vol_args->size);\n\t\tif (IS_ERR(inherit)) {\n\t\t\tret = PTR_ERR(inherit);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = btrfs_ioctl_snap_create_transid(file, vol_args->name,\n\t\t\t\t\t      vol_args->fd, subvol, ptr,\n\t\t\t\t\t      readonly, &inherit);\n\n\tif (ret == 0 && ptr &&\n\t    copy_to_user(arg +\n\t\t\t offsetof(struct btrfs_ioctl_vol_args_v2,\n\t\t\t\t  transid), ptr, sizeof(*ptr)))\n\t\tret = -EFAULT;\nout:\n\tkfree(vol_args);\n\tkfree(inherit);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_subvol_getflags(struct file *file,\n\t\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret = 0;\n\tu64 flags = 0;\n\n\tif (btrfs_ino(inode) != BTRFS_FIRST_FREE_OBJECTID)\n\t\treturn -EINVAL;\n\n\tdown_read(&root->fs_info->subvol_sem);\n\tif (btrfs_root_readonly(root))\n\t\tflags |= BTRFS_SUBVOL_RDONLY;\n\tup_read(&root->fs_info->subvol_sem);\n\n\tif (copy_to_user(arg, &flags, sizeof(flags)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_subvol_setflags(struct file *file,\n\t\t\t\t\t      void __user *arg)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tu64 root_flags;\n\tu64 flags;\n\tint ret = 0;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tif (btrfs_ino(inode) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (copy_from_user(&flags, arg, sizeof(flags))) {\n\t\tret = -EFAULT;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (flags & BTRFS_SUBVOL_CREATE_ASYNC) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (flags & ~BTRFS_SUBVOL_RDONLY) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (!inode_owner_or_capable(inode)) {\n\t\tret = -EACCES;\n\t\tgoto out_drop_write;\n\t}\n\n\tdown_write(&root->fs_info->subvol_sem);\n\n\t/* nothing to do */\n\tif (!!(flags & BTRFS_SUBVOL_RDONLY) == btrfs_root_readonly(root))\n\t\tgoto out_drop_sem;\n\n\troot_flags = btrfs_root_flags(&root->root_item);\n\tif (flags & BTRFS_SUBVOL_RDONLY)\n\t\tbtrfs_set_root_flags(&root->root_item,\n\t\t\t\t     root_flags | BTRFS_ROOT_SUBVOL_RDONLY);\n\telse\n\t\tbtrfs_set_root_flags(&root->root_item,\n\t\t\t\t     root_flags & ~BTRFS_ROOT_SUBVOL_RDONLY);\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_reset;\n\t}\n\n\tret = btrfs_update_root(trans, root->fs_info->tree_root,\n\t\t\t\t&root->root_key, &root->root_item);\n\n\tbtrfs_commit_transaction(trans, root);\nout_reset:\n\tif (ret)\n\t\tbtrfs_set_root_flags(&root->root_item, root_flags);\nout_drop_sem:\n\tup_write(&root->fs_info->subvol_sem);\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\n/*\n * helper to check if the subvolume references other subvolumes\n */\nstatic noinline int may_destroy_subvol(struct btrfs_root *root)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = root->root_key.objectid;\n\tkey.type = BTRFS_ROOT_REF_KEY;\n\tkey.offset = (u64)-1;\n\n\tret = btrfs_search_slot(NULL, root->fs_info->tree_root,\n\t\t\t\t&key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tBUG_ON(ret == 0);\n\n\tret = 0;\n\tif (path->slots[0] > 0) {\n\t\tpath->slots[0]--;\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\t\tif (key.objectid == root->root_key.objectid &&\n\t\t    key.type == BTRFS_ROOT_REF_KEY)\n\t\t\tret = -ENOTEMPTY;\n\t}\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int key_in_sk(struct btrfs_key *key,\n\t\t\t      struct btrfs_ioctl_search_key *sk)\n{\n\tstruct btrfs_key test;\n\tint ret;\n\n\ttest.objectid = sk->min_objectid;\n\ttest.type = sk->min_type;\n\ttest.offset = sk->min_offset;\n\n\tret = btrfs_comp_cpu_keys(key, &test);\n\tif (ret < 0)\n\t\treturn 0;\n\n\ttest.objectid = sk->max_objectid;\n\ttest.type = sk->max_type;\n\ttest.offset = sk->max_offset;\n\n\tret = btrfs_comp_cpu_keys(key, &test);\n\tif (ret > 0)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic noinline int copy_to_sk(struct btrfs_root *root,\n\t\t\t       struct btrfs_path *path,\n\t\t\t       struct btrfs_key *key,\n\t\t\t       struct btrfs_ioctl_search_key *sk,\n\t\t\t       char *buf,\n\t\t\t       unsigned long *sk_offset,\n\t\t\t       int *num_found)\n{\n\tu64 found_transid;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_ioctl_search_header sh;\n\tunsigned long item_off;\n\tunsigned long item_len;\n\tint nritems;\n\tint i;\n\tint slot;\n\tint ret = 0;\n\n\tleaf = path->nodes[0];\n\tslot = path->slots[0];\n\tnritems = btrfs_header_nritems(leaf);\n\n\tif (btrfs_header_generation(leaf) > sk->max_transid) {\n\t\ti = nritems;\n\t\tgoto advance_key;\n\t}\n\tfound_transid = btrfs_header_generation(leaf);\n\n\tfor (i = slot; i < nritems; i++) {\n\t\titem_off = btrfs_item_ptr_offset(leaf, i);\n\t\titem_len = btrfs_item_size_nr(leaf, i);\n\n\t\tif (item_len > BTRFS_SEARCH_ARGS_BUFSIZE)\n\t\t\titem_len = 0;\n\n\t\tif (sizeof(sh) + item_len + *sk_offset >\n\t\t    BTRFS_SEARCH_ARGS_BUFSIZE) {\n\t\t\tret = 1;\n\t\t\tgoto overflow;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, key, i);\n\t\tif (!key_in_sk(key, sk))\n\t\t\tcontinue;\n\n\t\tsh.objectid = key->objectid;\n\t\tsh.offset = key->offset;\n\t\tsh.type = key->type;\n\t\tsh.len = item_len;\n\t\tsh.transid = found_transid;\n\n\t\t/* copy search result header */\n\t\tmemcpy(buf + *sk_offset, &sh, sizeof(sh));\n\t\t*sk_offset += sizeof(sh);\n\n\t\tif (item_len) {\n\t\t\tchar *p = buf + *sk_offset;\n\t\t\t/* copy the item */\n\t\t\tread_extent_buffer(leaf, p,\n\t\t\t\t\t   item_off, item_len);\n\t\t\t*sk_offset += item_len;\n\t\t}\n\t\t(*num_found)++;\n\n\t\tif (*num_found >= sk->nr_items)\n\t\t\tbreak;\n\t}\nadvance_key:\n\tret = 0;\n\tif (key->offset < (u64)-1 && key->offset < sk->max_offset)\n\t\tkey->offset++;\n\telse if (key->type < (u8)-1 && key->type < sk->max_type) {\n\t\tkey->offset = 0;\n\t\tkey->type++;\n\t} else if (key->objectid < (u64)-1 && key->objectid < sk->max_objectid) {\n\t\tkey->offset = 0;\n\t\tkey->type = 0;\n\t\tkey->objectid++;\n\t} else\n\t\tret = 1;\noverflow:\n\treturn ret;\n}\n\nstatic noinline int search_ioctl(struct inode *inode,\n\t\t\t\t struct btrfs_ioctl_search_args *args)\n{\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tstruct btrfs_key max_key;\n\tstruct btrfs_path *path;\n\tstruct btrfs_ioctl_search_key *sk = &args->key;\n\tstruct btrfs_fs_info *info = BTRFS_I(inode)->root->fs_info;\n\tint ret;\n\tint num_found = 0;\n\tunsigned long sk_offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tif (sk->tree_id == 0) {\n\t\t/* search the root of the inode that was passed */\n\t\troot = BTRFS_I(inode)->root;\n\t} else {\n\t\tkey.objectid = sk->tree_id;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\t\troot = btrfs_read_fs_root_no_name(info, &key);\n\t\tif (IS_ERR(root)) {\n\t\t\tprintk(KERN_ERR \"could not find root %llu\\n\",\n\t\t\t       sk->tree_id);\n\t\t\tbtrfs_free_path(path);\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\tkey.objectid = sk->min_objectid;\n\tkey.type = sk->min_type;\n\tkey.offset = sk->min_offset;\n\n\tmax_key.objectid = sk->max_objectid;\n\tmax_key.type = sk->max_type;\n\tmax_key.offset = sk->max_offset;\n\n\tpath->keep_locks = 1;\n\n\twhile(1) {\n\t\tret = btrfs_search_forward(root, &key, &max_key, path, 0,\n\t\t\t\t\t   sk->min_transid);\n\t\tif (ret != 0) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tgoto err;\n\t\t}\n\t\tret = copy_to_sk(root, path, &key, sk, args->buf,\n\t\t\t\t &sk_offset, &num_found);\n\t\tbtrfs_release_path(path);\n\t\tif (ret || num_found >= sk->nr_items)\n\t\t\tbreak;\n\n\t}\n\tret = 0;\nerr:\n\tsk->nr_items = num_found;\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_tree_search(struct file *file,\n\t\t\t\t\t   void __user *argp)\n{\n\t struct btrfs_ioctl_search_args *args;\n\t struct inode *inode;\n\t int ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\targs = memdup_user(argp, sizeof(*args));\n\tif (IS_ERR(args))\n\t\treturn PTR_ERR(args);\n\n\tinode = fdentry(file)->d_inode;\n\tret = search_ioctl(inode, args);\n\tif (ret == 0 && copy_to_user(argp, args, sizeof(*args)))\n\t\tret = -EFAULT;\n\tkfree(args);\n\treturn ret;\n}\n\n/*\n * Search INODE_REFs to identify path name of 'dirid' directory\n * in a 'tree_id' tree. and sets path name to 'name'.\n */\nstatic noinline int btrfs_search_path_in_tree(struct btrfs_fs_info *info,\n\t\t\t\tu64 tree_id, u64 dirid, char *name)\n{\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tchar *ptr;\n\tint ret = -1;\n\tint slot;\n\tint len;\n\tint total_len = 0;\n\tstruct btrfs_inode_ref *iref;\n\tstruct extent_buffer *l;\n\tstruct btrfs_path *path;\n\n\tif (dirid == BTRFS_FIRST_FREE_OBJECTID) {\n\t\tname[0]='\\0';\n\t\treturn 0;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tptr = &name[BTRFS_INO_LOOKUP_PATH_MAX];\n\n\tkey.objectid = tree_id;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\troot = btrfs_read_fs_root_no_name(info, &key);\n\tif (IS_ERR(root)) {\n\t\tprintk(KERN_ERR \"could not find root %llu\\n\", tree_id);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = dirid;\n\tkey.type = BTRFS_INODE_REF_KEY;\n\tkey.offset = (u64)-1;\n\n\twhile(1) {\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (ret > 0 && slot > 0)\n\t\t\tslot--;\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tif (ret > 0 && (key.objectid != dirid ||\n\t\t\t\tkey.type != BTRFS_INODE_REF_KEY)) {\n\t\t\tret = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tiref = btrfs_item_ptr(l, slot, struct btrfs_inode_ref);\n\t\tlen = btrfs_inode_ref_name_len(l, iref);\n\t\tptr -= len + 1;\n\t\ttotal_len += len + 1;\n\t\tif (ptr < name)\n\t\t\tgoto out;\n\n\t\t*(ptr + len) = '/';\n\t\tread_extent_buffer(l, ptr,(unsigned long)(iref + 1), len);\n\n\t\tif (key.offset == BTRFS_FIRST_FREE_OBJECTID)\n\t\t\tbreak;\n\n\t\tbtrfs_release_path(path);\n\t\tkey.objectid = key.offset;\n\t\tkey.offset = (u64)-1;\n\t\tdirid = key.objectid;\n\t}\n\tif (ptr < name)\n\t\tgoto out;\n\tmemmove(name, ptr, total_len);\n\tname[total_len]='\\0';\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_ino_lookup(struct file *file,\n\t\t\t\t\t   void __user *argp)\n{\n\t struct btrfs_ioctl_ino_lookup_args *args;\n\t struct inode *inode;\n\t int ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\targs = memdup_user(argp, sizeof(*args));\n\tif (IS_ERR(args))\n\t\treturn PTR_ERR(args);\n\n\tinode = fdentry(file)->d_inode;\n\n\tif (args->treeid == 0)\n\t\targs->treeid = BTRFS_I(inode)->root->root_key.objectid;\n\n\tret = btrfs_search_path_in_tree(BTRFS_I(inode)->root->fs_info,\n\t\t\t\t\targs->treeid, args->objectid,\n\t\t\t\t\targs->name);\n\n\tif (ret == 0 && copy_to_user(argp, args, sizeof(*args)))\n\t\tret = -EFAULT;\n\n\tkfree(args);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_destroy(struct file *file,\n\t\t\t\t\t     void __user *arg)\n{\n\tstruct dentry *parent = fdentry(file);\n\tstruct dentry *dentry;\n\tstruct inode *dir = parent->d_inode;\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *dest = NULL;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tint namelen;\n\tint ret;\n\tint err = 0;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tnamelen = strlen(vol_args->name);\n\tif (strchr(vol_args->name, '/') ||\n\t    strncmp(vol_args->name, \"..\", namelen) == 0) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = mnt_want_write_file(file);\n\tif (err)\n\t\tgoto out;\n\n\tmutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);\n\tdentry = lookup_one_len(vol_args->name, parent, namelen);\n\tif (IS_ERR(dentry)) {\n\t\terr = PTR_ERR(dentry);\n\t\tgoto out_unlock_dir;\n\t}\n\n\tif (!dentry->d_inode) {\n\t\terr = -ENOENT;\n\t\tgoto out_dput;\n\t}\n\n\tinode = dentry->d_inode;\n\tdest = BTRFS_I(inode)->root;\n\tif (!capable(CAP_SYS_ADMIN)){\n\t\t/*\n\t\t * Regular user.  Only allow this with a special mount\n\t\t * option, when the user has write+exec access to the\n\t\t * subvol root, and when rmdir(2) would have been\n\t\t * allowed.\n\t\t *\n\t\t * Note that this is _not_ check that the subvol is\n\t\t * empty or doesn't contain data that we wouldn't\n\t\t * otherwise be able to delete.\n\t\t *\n\t\t * Users who want to delete empty subvols should try\n\t\t * rmdir(2).\n\t\t */\n\t\terr = -EPERM;\n\t\tif (!btrfs_test_opt(root, USER_SUBVOL_RM_ALLOWED))\n\t\t\tgoto out_dput;\n\n\t\t/*\n\t\t * Do not allow deletion if the parent dir is the same\n\t\t * as the dir to be deleted.  That means the ioctl\n\t\t * must be called on the dentry referencing the root\n\t\t * of the subvol, not a random directory contained\n\t\t * within it.\n\t\t */\n\t\terr = -EINVAL;\n\t\tif (root == dest)\n\t\t\tgoto out_dput;\n\n\t\terr = inode_permission(inode, MAY_WRITE | MAY_EXEC);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\n\t\t/* check if subvolume may be deleted by a non-root user */\n\t\terr = btrfs_may_delete(dir, dentry, 1);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\tif (btrfs_ino(inode) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\terr = -EINVAL;\n\t\tgoto out_dput;\n\t}\n\n\tmutex_lock(&inode->i_mutex);\n\terr = d_invalidate(dentry);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tdown_write(&root->fs_info->subvol_sem);\n\n\terr = may_destroy_subvol(dest);\n\tif (err)\n\t\tgoto out_up_write;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\terr = PTR_ERR(trans);\n\t\tgoto out_up_write;\n\t}\n\ttrans->block_rsv = &root->fs_info->global_block_rsv;\n\n\tret = btrfs_unlink_subvol(trans, root, dir,\n\t\t\t\tdest->root_key.objectid,\n\t\t\t\tdentry->d_name.name,\n\t\t\t\tdentry->d_name.len);\n\tif (ret) {\n\t\terr = ret;\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_end_trans;\n\t}\n\n\tbtrfs_record_root_in_trans(trans, dest);\n\n\tmemset(&dest->root_item.drop_progress, 0,\n\t\tsizeof(dest->root_item.drop_progress));\n\tdest->root_item.drop_level = 0;\n\tbtrfs_set_root_refs(&dest->root_item, 0);\n\n\tif (!xchg(&dest->orphan_item_inserted, 1)) {\n\t\tret = btrfs_insert_orphan_item(trans,\n\t\t\t\t\troot->fs_info->tree_root,\n\t\t\t\t\tdest->root_key.objectid);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\terr = ret;\n\t\t\tgoto out_end_trans;\n\t\t}\n\t}\nout_end_trans:\n\tret = btrfs_end_transaction(trans, root);\n\tif (ret && !err)\n\t\terr = ret;\n\tinode->i_flags |= S_DEAD;\nout_up_write:\n\tup_write(&root->fs_info->subvol_sem);\nout_unlock:\n\tmutex_unlock(&inode->i_mutex);\n\tif (!err) {\n\t\tshrink_dcache_sb(root->fs_info->sb);\n\t\tbtrfs_invalidate_inodes(dest);\n\t\td_delete(dentry);\n\t}\nout_dput:\n\tdput(dentry);\nout_unlock_dir:\n\tmutex_unlock(&dir->i_mutex);\n\tmnt_drop_write_file(file);\nout:\n\tkfree(vol_args);\n\treturn err;\n}\n\nstatic int btrfs_ioctl_defrag(struct file *file, void __user *argp)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_defrag_range_args *range;\n\tint ret;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\treturn -EINPROGRESS;\n\t}\n\tret = mnt_want_write_file(file);\n\tif (ret) {\n\t\tatomic_set(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t   0);\n\t\treturn ret;\n\t}\n\n\tswitch (inode->i_mode & S_IFMT) {\n\tcase S_IFDIR:\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = btrfs_defrag_root(root, 0);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tret = btrfs_defrag_root(root->fs_info->extent_root, 0);\n\t\tbreak;\n\tcase S_IFREG:\n\t\tif (!(file->f_mode & FMODE_WRITE)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\trange = kzalloc(sizeof(*range), GFP_KERNEL);\n\t\tif (!range) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (argp) {\n\t\t\tif (copy_from_user(range, argp,\n\t\t\t\t\t   sizeof(*range))) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tkfree(range);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t/* compression requires us to start the IO */\n\t\t\tif ((range->flags & BTRFS_DEFRAG_RANGE_COMPRESS)) {\n\t\t\t\trange->flags |= BTRFS_DEFRAG_RANGE_START_IO;\n\t\t\t\trange->extent_thresh = (u32)-1;\n\t\t\t}\n\t\t} else {\n\t\t\t/* the rest are all set to zero by kzalloc */\n\t\t\trange->len = (u64)-1;\n\t\t}\n\t\tret = btrfs_defrag_file(fdentry(file)->d_inode, file,\n\t\t\t\t\trange, 0, 0);\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t\tkfree(range);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\nout:\n\tmnt_drop_write_file(file);\n\tatomic_set(&root->fs_info->mutually_exclusive_operation_running, 0);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_add_dev(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\treturn -EINPROGRESS;\n\t}\n\n\tmutex_lock(&root->fs_info->volume_mutex);\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tret = btrfs_init_new_device(root, vol_args->name);\n\n\tkfree(vol_args);\nout:\n\tmutex_unlock(&root->fs_info->volume_mutex);\n\tatomic_set(&root->fs_info->mutually_exclusive_operation_running, 0);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_rm_dev(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\tmnt_drop_write_file(file);\n\t\treturn -EINPROGRESS;\n\t}\n\n\tmutex_lock(&root->fs_info->volume_mutex);\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tret = btrfs_rm_device(root, vol_args->name);\n\n\tkfree(vol_args);\nout:\n\tmutex_unlock(&root->fs_info->volume_mutex);\n\tmnt_drop_write_file(file);\n\tatomic_set(&root->fs_info->mutually_exclusive_operation_running, 0);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_fs_info(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_fs_info_args *fi_args;\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *next;\n\tstruct btrfs_fs_devices *fs_devices = root->fs_info->fs_devices;\n\tint ret = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tfi_args = kzalloc(sizeof(*fi_args), GFP_KERNEL);\n\tif (!fi_args)\n\t\treturn -ENOMEM;\n\n\tfi_args->num_devices = fs_devices->num_devices;\n\tmemcpy(&fi_args->fsid, root->fs_info->fsid, sizeof(fi_args->fsid));\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry_safe(device, next, &fs_devices->devices, dev_list) {\n\t\tif (device->devid > fi_args->max_id)\n\t\t\tfi_args->max_id = device->devid;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (copy_to_user(arg, fi_args, sizeof(*fi_args)))\n\t\tret = -EFAULT;\n\n\tkfree(fi_args);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_dev_info(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_info_args *di_args;\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = root->fs_info->fs_devices;\n\tint ret = 0;\n\tchar *s_uuid = NULL;\n\tchar empty_uuid[BTRFS_UUID_SIZE] = {0};\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tdi_args = memdup_user(arg, sizeof(*di_args));\n\tif (IS_ERR(di_args))\n\t\treturn PTR_ERR(di_args);\n\n\tif (memcmp(empty_uuid, di_args->uuid, BTRFS_UUID_SIZE) != 0)\n\t\ts_uuid = di_args->uuid;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(root->fs_info, di_args->devid, s_uuid, NULL);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tdi_args->devid = dev->devid;\n\tdi_args->bytes_used = dev->bytes_used;\n\tdi_args->total_bytes = dev->total_bytes;\n\tmemcpy(di_args->uuid, dev->uuid, sizeof(di_args->uuid));\n\tif (dev->name) {\n\t\tstruct rcu_string *name;\n\n\t\trcu_read_lock();\n\t\tname = rcu_dereference(dev->name);\n\t\tstrncpy(di_args->path, name->str, sizeof(di_args->path));\n\t\trcu_read_unlock();\n\t\tdi_args->path[sizeof(di_args->path) - 1] = 0;\n\t} else {\n\t\tdi_args->path[0] = '\\0';\n\t}\n\nout:\n\tif (ret == 0 && copy_to_user(arg, di_args, sizeof(*di_args)))\n\t\tret = -EFAULT;\n\n\tkfree(di_args);\n\treturn ret;\n}\n\nstatic noinline long btrfs_ioctl_clone(struct file *file, unsigned long srcfd,\n\t\t\t\t       u64 off, u64 olen, u64 destoff)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct fd src_file;\n\tstruct inode *src;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tchar *buf;\n\tstruct btrfs_key key;\n\tu32 nritems;\n\tint slot;\n\tint ret;\n\tu64 len = olen;\n\tu64 bs = root->fs_info->sb->s_blocksize;\n\n\t/*\n\t * TODO:\n\t * - split compressed inline extents.  annoying: we need to\n\t *   decompress into destination's address_space (the file offset\n\t *   may change, so source mapping won't do), then recompress (or\n\t *   otherwise reinsert) a subrange.\n\t * - allow ranges within the same file to be cloned (provided\n\t *   they don't overlap)?\n\t */\n\n\t/* the destination must be opened for writing */\n\tif (!(file->f_mode & FMODE_WRITE) || (file->f_flags & O_APPEND))\n\t\treturn -EINVAL;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsrc_file = fdget(srcfd);\n\tif (!src_file.file) {\n\t\tret = -EBADF;\n\t\tgoto out_drop_write;\n\t}\n\n\tret = -EXDEV;\n\tif (src_file.file->f_path.mnt != file->f_path.mnt)\n\t\tgoto out_fput;\n\n\tsrc = src_file.file->f_dentry->d_inode;\n\n\tret = -EINVAL;\n\tif (src == inode)\n\t\tgoto out_fput;\n\n\t/* the src must be open for reading */\n\tif (!(src_file.file->f_mode & FMODE_READ))\n\t\tgoto out_fput;\n\n\t/* don't make the dst file partly checksummed */\n\tif ((BTRFS_I(src)->flags & BTRFS_INODE_NODATASUM) !=\n\t    (BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM))\n\t\tgoto out_fput;\n\n\tret = -EISDIR;\n\tif (S_ISDIR(src->i_mode) || S_ISDIR(inode->i_mode))\n\t\tgoto out_fput;\n\n\tret = -EXDEV;\n\tif (src->i_sb != inode->i_sb)\n\t\tgoto out_fput;\n\n\tret = -ENOMEM;\n\tbuf = vmalloc(btrfs_level_size(root, 0));\n\tif (!buf)\n\t\tgoto out_fput;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tvfree(buf);\n\t\tgoto out_fput;\n\t}\n\tpath->reada = 2;\n\n\tif (inode < src) {\n\t\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n\t\tmutex_lock_nested(&src->i_mutex, I_MUTEX_CHILD);\n\t} else {\n\t\tmutex_lock_nested(&src->i_mutex, I_MUTEX_PARENT);\n\t\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_CHILD);\n\t}\n\n\t/* determine range to clone */\n\tret = -EINVAL;\n\tif (off + len > src->i_size || off + len < off)\n\t\tgoto out_unlock;\n\tif (len == 0)\n\t\tolen = len = src->i_size - off;\n\t/* if we extend to eof, continue to block boundary */\n\tif (off + len == src->i_size)\n\t\tlen = ALIGN(src->i_size, bs) - off;\n\n\t/* verify the end result is block aligned */\n\tif (!IS_ALIGNED(off, bs) || !IS_ALIGNED(off + len, bs) ||\n\t    !IS_ALIGNED(destoff, bs))\n\t\tgoto out_unlock;\n\n\tif (destoff > inode->i_size) {\n\t\tret = btrfs_cont_expand(inode, inode->i_size, destoff);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* truncate page cache pages from target inode range */\n\ttruncate_inode_pages_range(&inode->i_data, destoff,\n\t\t\t\t   PAGE_CACHE_ALIGN(destoff + len) - 1);\n\n\t/* do any pending delalloc/csum calc on src, one way or\n\t   another, and lock file content */\n\twhile (1) {\n\t\tstruct btrfs_ordered_extent *ordered;\n\t\tlock_extent(&BTRFS_I(src)->io_tree, off, off + len - 1);\n\t\tordered = btrfs_lookup_first_ordered_extent(src, off + len - 1);\n\t\tif (!ordered &&\n\t\t    !test_range_bit(&BTRFS_I(src)->io_tree, off, off + len - 1,\n\t\t\t\t    EXTENT_DELALLOC, 0, NULL))\n\t\t\tbreak;\n\t\tunlock_extent(&BTRFS_I(src)->io_tree, off, off + len - 1);\n\t\tif (ordered)\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\tbtrfs_wait_ordered_range(src, off, len);\n\t}\n\n\t/* clone data */\n\tkey.objectid = btrfs_ino(src);\n\tkey.type = BTRFS_EXTENT_DATA_KEY;\n\tkey.offset = 0;\n\n\twhile (1) {\n\t\t/*\n\t\t * note the key will change type as we walk through the\n\t\t * tree.\n\t\t */\n\t\tret = btrfs_search_slot(NULL, BTRFS_I(src)->root, &key, path,\n\t\t\t\t0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tnritems = btrfs_header_nritems(path->nodes[0]);\n\t\tif (path->slots[0] >= nritems) {\n\t\t\tret = btrfs_next_leaf(BTRFS_I(src)->root, path);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tnritems = btrfs_header_nritems(path->nodes[0]);\n\t\t}\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (btrfs_key_type(&key) > BTRFS_EXTENT_DATA_KEY ||\n\t\t    key.objectid != btrfs_ino(src))\n\t\t\tbreak;\n\n\t\tif (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tstruct btrfs_file_extent_item *extent;\n\t\t\tint type;\n\t\t\tu32 size;\n\t\t\tstruct btrfs_key new_key;\n\t\t\tu64 disko = 0, diskl = 0;\n\t\t\tu64 datao = 0, datal = 0;\n\t\t\tu8 comp;\n\t\t\tu64 endoff;\n\n\t\t\tsize = btrfs_item_size_nr(leaf, slot);\n\t\t\tread_extent_buffer(leaf, buf,\n\t\t\t\t\t   btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t   size);\n\n\t\t\textent = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\tstruct btrfs_file_extent_item);\n\t\t\tcomp = btrfs_file_extent_compression(leaf, extent);\n\t\t\ttype = btrfs_file_extent_type(leaf, extent);\n\t\t\tif (type == BTRFS_FILE_EXTENT_REG ||\n\t\t\t    type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\t\tdisko = btrfs_file_extent_disk_bytenr(leaf,\n\t\t\t\t\t\t\t\t      extent);\n\t\t\t\tdiskl = btrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t extent);\n\t\t\t\tdatao = btrfs_file_extent_offset(leaf, extent);\n\t\t\t\tdatal = btrfs_file_extent_num_bytes(leaf,\n\t\t\t\t\t\t\t\t    extent);\n\t\t\t} else if (type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\t/* take upper bound, may be compressed */\n\t\t\t\tdatal = btrfs_file_extent_ram_bytes(leaf,\n\t\t\t\t\t\t\t\t    extent);\n\t\t\t}\n\t\t\tbtrfs_release_path(path);\n\n\t\t\tif (key.offset + datal <= off ||\n\t\t\t    key.offset >= off + len - 1)\n\t\t\t\tgoto next;\n\n\t\t\tmemcpy(&new_key, &key, sizeof(new_key));\n\t\t\tnew_key.objectid = btrfs_ino(inode);\n\t\t\tif (off <= key.offset)\n\t\t\t\tnew_key.offset = key.offset + destoff - off;\n\t\t\telse\n\t\t\t\tnew_key.offset = destoff;\n\n\t\t\t/*\n\t\t\t * 1 - adjusting old extent (we may have to split it)\n\t\t\t * 1 - add new extent\n\t\t\t * 1 - inode update\n\t\t\t */\n\t\t\ttrans = btrfs_start_transaction(root, 3);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (type == BTRFS_FILE_EXTENT_REG ||\n\t\t\t    type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\t\t/*\n\t\t\t\t *    a  | --- range to clone ---|  b\n\t\t\t\t * | ------------- extent ------------- |\n\t\t\t\t */\n\n\t\t\t\t/* substract range b */\n\t\t\t\tif (key.offset + datal > off + len)\n\t\t\t\t\tdatal = off + len - key.offset;\n\n\t\t\t\t/* substract range a */\n\t\t\t\tif (off > key.offset) {\n\t\t\t\t\tdatao += off - key.offset;\n\t\t\t\t\tdatal -= off - key.offset;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t\t\t new_key.offset,\n\t\t\t\t\t\t\t new_key.offset + datal,\n\t\t\t\t\t\t\t 1);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, root,\n\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_insert_empty_item(trans, root, path,\n\t\t\t\t\t\t\t      &new_key, size);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, root,\n\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tleaf = path->nodes[0];\n\t\t\t\tslot = path->slots[0];\n\t\t\t\twrite_extent_buffer(leaf, buf,\n\t\t\t\t\t    btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t    size);\n\n\t\t\t\textent = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\tstruct btrfs_file_extent_item);\n\n\t\t\t\t/* disko == 0 means it's a hole */\n\t\t\t\tif (!disko)\n\t\t\t\t\tdatao = 0;\n\n\t\t\t\tbtrfs_set_file_extent_offset(leaf, extent,\n\t\t\t\t\t\t\t     datao);\n\t\t\t\tbtrfs_set_file_extent_num_bytes(leaf, extent,\n\t\t\t\t\t\t\t\tdatal);\n\t\t\t\tif (disko) {\n\t\t\t\t\tinode_add_bytes(inode, datal);\n\t\t\t\t\tret = btrfs_inc_extent_ref(trans, root,\n\t\t\t\t\t\t\tdisko, diskl, 0,\n\t\t\t\t\t\t\troot->root_key.objectid,\n\t\t\t\t\t\t\tbtrfs_ino(inode),\n\t\t\t\t\t\t\tnew_key.offset - datao,\n\t\t\t\t\t\t\t0);\n\t\t\t\t\tif (ret) {\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\troot,\n\t\t\t\t\t\t\t\t\tret);\n\t\t\t\t\t\tbtrfs_end_transaction(trans,\n\t\t\t\t\t\t\t\t      root);\n\t\t\t\t\t\tgoto out;\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\tu64 skip = 0;\n\t\t\t\tu64 trim = 0;\n\t\t\t\tif (off > key.offset) {\n\t\t\t\t\tskip = off - key.offset;\n\t\t\t\t\tnew_key.offset += skip;\n\t\t\t\t}\n\n\t\t\t\tif (key.offset + datal > off + len)\n\t\t\t\t\ttrim = key.offset + datal - (off + len);\n\n\t\t\t\tif (comp && (skip || trim)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tsize -= skip + trim;\n\t\t\t\tdatal -= skip + trim;\n\n\t\t\t\tret = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t\t\t new_key.offset,\n\t\t\t\t\t\t\t new_key.offset + datal,\n\t\t\t\t\t\t\t 1);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, root,\n\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_insert_empty_item(trans, root, path,\n\t\t\t\t\t\t\t      &new_key, size);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, root,\n\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tif (skip) {\n\t\t\t\t\tu32 start =\n\t\t\t\t\t  btrfs_file_extent_calc_inline_size(0);\n\t\t\t\t\tmemmove(buf+start, buf+start+skip,\n\t\t\t\t\t\tdatal);\n\t\t\t\t}\n\n\t\t\t\tleaf = path->nodes[0];\n\t\t\t\tslot = path->slots[0];\n\t\t\t\twrite_extent_buffer(leaf, buf,\n\t\t\t\t\t    btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t    size);\n\t\t\t\tinode_add_bytes(inode, datal);\n\t\t\t}\n\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t\tbtrfs_release_path(path);\n\n\t\t\tinode_inc_iversion(inode);\n\t\t\tinode->i_mtime = inode->i_ctime = CURRENT_TIME;\n\n\t\t\t/*\n\t\t\t * we round up to the block size at eof when\n\t\t\t * determining which extents to clone above,\n\t\t\t * but shouldn't round up the file size\n\t\t\t */\n\t\t\tendoff = new_key.offset + datal;\n\t\t\tif (endoff > destoff+olen)\n\t\t\t\tendoff = destoff+olen;\n\t\t\tif (endoff > inode->i_size)\n\t\t\t\tbtrfs_i_size_write(inode, endoff);\n\n\t\t\tret = btrfs_update_inode(trans, root, inode);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = btrfs_end_transaction(trans, root);\n\t\t}\nnext:\n\t\tbtrfs_release_path(path);\n\t\tkey.offset++;\n\t}\n\tret = 0;\nout:\n\tbtrfs_release_path(path);\n\tunlock_extent(&BTRFS_I(src)->io_tree, off, off + len - 1);\nout_unlock:\n\tmutex_unlock(&src->i_mutex);\n\tmutex_unlock(&inode->i_mutex);\n\tvfree(buf);\n\tbtrfs_free_path(path);\nout_fput:\n\tfdput(src_file);\nout_drop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_clone_range(struct file *file, void __user *argp)\n{\n\tstruct btrfs_ioctl_clone_range_args args;\n\n\tif (copy_from_user(&args, argp, sizeof(args)))\n\t\treturn -EFAULT;\n\treturn btrfs_ioctl_clone(file, args.src_fd, args.src_offset,\n\t\t\t\t args.src_length, args.dest_offset);\n}\n\n/*\n * there are many ways the trans_start and trans_end ioctls can lead\n * to deadlocks.  They should only be used by applications that\n * basically own the machine, and have a very in depth understanding\n * of all the possible deadlocks and enospc problems.\n */\nstatic long btrfs_ioctl_trans_start(struct file *file)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\tret = -EPERM;\n\tif (!capable(CAP_SYS_ADMIN))\n\t\tgoto out;\n\n\tret = -EINPROGRESS;\n\tif (file->private_data)\n\t\tgoto out;\n\n\tret = -EROFS;\n\tif (btrfs_root_readonly(root))\n\t\tgoto out;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tatomic_inc(&root->fs_info->open_ioctl_trans);\n\n\tret = -ENOMEM;\n\ttrans = btrfs_start_ioctl_transaction(root);\n\tif (IS_ERR(trans))\n\t\tgoto out_drop;\n\n\tfile->private_data = trans;\n\treturn 0;\n\nout_drop:\n\tatomic_dec(&root->fs_info->open_ioctl_trans);\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_default_subvol(struct file *file, void __user *argp)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_root *new_root;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key location;\n\tstruct btrfs_disk_key disk_key;\n\tu64 objectid = 0;\n\tu64 dir_id;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (copy_from_user(&objectid, argp, sizeof(objectid))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (!objectid)\n\t\tobjectid = root->root_key.objectid;\n\n\tlocation.objectid = objectid;\n\tlocation.type = BTRFS_ROOT_ITEM_KEY;\n\tlocation.offset = (u64)-1;\n\n\tnew_root = btrfs_read_fs_root_no_name(root->fs_info, &location);\n\tif (IS_ERR(new_root)) {\n\t\tret = PTR_ERR(new_root);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_root_refs(&new_root->root_item) == 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpath->leave_spinning = 1;\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tdir_id = btrfs_super_root_dir(root->fs_info->super_copy);\n\tdi = btrfs_lookup_dir_item(trans, root->fs_info->tree_root, path,\n\t\t\t\t   dir_id, \"default\", 7, 1);\n\tif (IS_ERR_OR_NULL(di)) {\n\t\tbtrfs_free_path(path);\n\t\tbtrfs_end_transaction(trans, root);\n\t\tprintk(KERN_ERR \"Umm, you don't have the default dir item, \"\n\t\t       \"this isn't going to work\\n\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tbtrfs_cpu_key_to_disk(&disk_key, &new_root->root_key);\n\tbtrfs_set_dir_item_key(path->nodes[0], di, &disk_key);\n\tbtrfs_mark_buffer_dirty(path->nodes[0]);\n\tbtrfs_free_path(path);\n\n\tbtrfs_set_fs_incompat(root->fs_info, DEFAULT_SUBVOL);\n\tbtrfs_end_transaction(trans, root);\nout:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nvoid btrfs_get_block_group_info(struct list_head *groups_list,\n\t\t\t\tstruct btrfs_ioctl_space_info *space)\n{\n\tstruct btrfs_block_group_cache *block_group;\n\n\tspace->total_bytes = 0;\n\tspace->used_bytes = 0;\n\tspace->flags = 0;\n\tlist_for_each_entry(block_group, groups_list, list) {\n\t\tspace->flags = block_group->flags;\n\t\tspace->total_bytes += block_group->key.offset;\n\t\tspace->used_bytes +=\n\t\t\tbtrfs_block_group_used(&block_group->item);\n\t}\n}\n\nlong btrfs_ioctl_space_info(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_space_args space_args;\n\tstruct btrfs_ioctl_space_info space;\n\tstruct btrfs_ioctl_space_info *dest;\n\tstruct btrfs_ioctl_space_info *dest_orig;\n\tstruct btrfs_ioctl_space_info __user *user_dest;\n\tstruct btrfs_space_info *info;\n\tu64 types[] = {BTRFS_BLOCK_GROUP_DATA,\n\t\t       BTRFS_BLOCK_GROUP_SYSTEM,\n\t\t       BTRFS_BLOCK_GROUP_METADATA,\n\t\t       BTRFS_BLOCK_GROUP_DATA | BTRFS_BLOCK_GROUP_METADATA};\n\tint num_types = 4;\n\tint alloc_size;\n\tint ret = 0;\n\tu64 slot_count = 0;\n\tint i, c;\n\n\tif (copy_from_user(&space_args,\n\t\t\t   (struct btrfs_ioctl_space_args __user *)arg,\n\t\t\t   sizeof(space_args)))\n\t\treturn -EFAULT;\n\n\tfor (i = 0; i < num_types; i++) {\n\t\tstruct btrfs_space_info *tmp;\n\n\t\tinfo = NULL;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(tmp, &root->fs_info->space_info,\n\t\t\t\t\tlist) {\n\t\t\tif (tmp->flags == types[i]) {\n\t\t\t\tinfo = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!info)\n\t\t\tcontinue;\n\n\t\tdown_read(&info->groups_sem);\n\t\tfor (c = 0; c < BTRFS_NR_RAID_TYPES; c++) {\n\t\t\tif (!list_empty(&info->block_groups[c]))\n\t\t\t\tslot_count++;\n\t\t}\n\t\tup_read(&info->groups_sem);\n\t}\n\n\t/* space_slots == 0 means they are asking for a count */\n\tif (space_args.space_slots == 0) {\n\t\tspace_args.total_spaces = slot_count;\n\t\tgoto out;\n\t}\n\n\tslot_count = min_t(u64, space_args.space_slots, slot_count);\n\n\talloc_size = sizeof(*dest) * slot_count;\n\n\t/* we generally have at most 6 or so space infos, one for each raid\n\t * level.  So, a whole page should be more than enough for everyone\n\t */\n\tif (alloc_size > PAGE_CACHE_SIZE)\n\t\treturn -ENOMEM;\n\n\tspace_args.total_spaces = 0;\n\tdest = kmalloc(alloc_size, GFP_NOFS);\n\tif (!dest)\n\t\treturn -ENOMEM;\n\tdest_orig = dest;\n\n\t/* now we have a buffer to copy into */\n\tfor (i = 0; i < num_types; i++) {\n\t\tstruct btrfs_space_info *tmp;\n\n\t\tif (!slot_count)\n\t\t\tbreak;\n\n\t\tinfo = NULL;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(tmp, &root->fs_info->space_info,\n\t\t\t\t\tlist) {\n\t\t\tif (tmp->flags == types[i]) {\n\t\t\t\tinfo = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!info)\n\t\t\tcontinue;\n\t\tdown_read(&info->groups_sem);\n\t\tfor (c = 0; c < BTRFS_NR_RAID_TYPES; c++) {\n\t\t\tif (!list_empty(&info->block_groups[c])) {\n\t\t\t\tbtrfs_get_block_group_info(\n\t\t\t\t\t&info->block_groups[c], &space);\n\t\t\t\tmemcpy(dest, &space, sizeof(space));\n\t\t\t\tdest++;\n\t\t\t\tspace_args.total_spaces++;\n\t\t\t\tslot_count--;\n\t\t\t}\n\t\t\tif (!slot_count)\n\t\t\t\tbreak;\n\t\t}\n\t\tup_read(&info->groups_sem);\n\t}\n\n\tuser_dest = (struct btrfs_ioctl_space_info __user *)\n\t\t(arg + sizeof(struct btrfs_ioctl_space_args));\n\n\tif (copy_to_user(user_dest, dest_orig, alloc_size))\n\t\tret = -EFAULT;\n\n\tkfree(dest_orig);\nout:\n\tif (ret == 0 && copy_to_user(arg, &space_args, sizeof(space_args)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\n/*\n * there are many ways the trans_start and trans_end ioctls can lead\n * to deadlocks.  They should only be used by applications that\n * basically own the machine, and have a very in depth understanding\n * of all the possible deadlocks and enospc problems.\n */\nlong btrfs_ioctl_trans_end(struct file *file)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\n\ttrans = file->private_data;\n\tif (!trans)\n\t\treturn -EINVAL;\n\tfile->private_data = NULL;\n\n\tbtrfs_end_transaction(trans, root);\n\n\tatomic_dec(&root->fs_info->open_ioctl_trans);\n\n\tmnt_drop_write_file(file);\n\treturn 0;\n}\n\nstatic noinline long btrfs_ioctl_start_sync(struct btrfs_root *root,\n\t\t\t\t\t    void __user *argp)\n{\n\tstruct btrfs_trans_handle *trans;\n\tu64 transid;\n\tint ret;\n\n\ttrans = btrfs_attach_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tif (PTR_ERR(trans) != -ENOENT)\n\t\t\treturn PTR_ERR(trans);\n\n\t\t/* No running transaction, don't bother */\n\t\ttransid = root->fs_info->last_trans_committed;\n\t\tgoto out;\n\t}\n\ttransid = trans->transid;\n\tret = btrfs_commit_transaction_async(trans, root, 0);\n\tif (ret) {\n\t\tbtrfs_end_transaction(trans, root);\n\t\treturn ret;\n\t}\nout:\n\tif (argp)\n\t\tif (copy_to_user(argp, &transid, sizeof(transid)))\n\t\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic noinline long btrfs_ioctl_wait_sync(struct btrfs_root *root,\n\t\t\t\t\t   void __user *argp)\n{\n\tu64 transid;\n\n\tif (argp) {\n\t\tif (copy_from_user(&transid, argp, sizeof(transid)))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\ttransid = 0;  /* current trans */\n\t}\n\treturn btrfs_wait_for_commit(root, transid);\n}\n\nstatic long btrfs_ioctl_scrub(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_scrub_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tif (!(sa->flags & BTRFS_SCRUB_READONLY)) {\n\t\tret = mnt_want_write_file(file);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = btrfs_scrub_dev(root->fs_info, sa->devid, sa->start, sa->end,\n\t\t\t      &sa->progress, sa->flags & BTRFS_SCRUB_READONLY,\n\t\t\t      0);\n\n\tif (copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tif (!(sa->flags & BTRFS_SCRUB_READONLY))\n\t\tmnt_drop_write_file(file);\nout:\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_scrub_cancel(struct btrfs_root *root, void __user *arg)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn btrfs_scrub_cancel(root->fs_info);\n}\n\nstatic long btrfs_ioctl_scrub_progress(struct btrfs_root *root,\n\t\t\t\t       void __user *arg)\n{\n\tstruct btrfs_ioctl_scrub_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tret = btrfs_scrub_progress(root, sa->devid, &sa->progress);\n\n\tif (copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_get_dev_stats(struct btrfs_root *root,\n\t\t\t\t      void __user *arg)\n{\n\tstruct btrfs_ioctl_get_dev_stats *sa;\n\tint ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tif ((sa->flags & BTRFS_DEV_STATS_RESET) && !capable(CAP_SYS_ADMIN)) {\n\t\tkfree(sa);\n\t\treturn -EPERM;\n\t}\n\n\tret = btrfs_get_dev_stats(root, sa);\n\n\tif (copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_dev_replace(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_replace_args *p;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tp = memdup_user(arg, sizeof(*p));\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\tswitch (p->cmd) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_START:\n\t\tif (atomic_xchg(\n\t\t\t&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\t\tret = -EINPROGRESS;\n\t\t} else {\n\t\t\tret = btrfs_dev_replace_start(root, p);\n\t\t\tatomic_set(\n\t\t\t &root->fs_info->mutually_exclusive_operation_running,\n\t\t\t 0);\n\t\t}\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_STATUS:\n\t\tbtrfs_dev_replace_status(root->fs_info, p);\n\t\tret = 0;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_CANCEL:\n\t\tret = btrfs_dev_replace_cancel(root->fs_info, p);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (copy_to_user(arg, p, sizeof(*p)))\n\t\tret = -EFAULT;\n\n\tkfree(p);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_ino_to_path(struct btrfs_root *root, void __user *arg)\n{\n\tint ret = 0;\n\tint i;\n\tu64 rel_ptr;\n\tint size;\n\tstruct btrfs_ioctl_ino_path_args *ipa = NULL;\n\tstruct inode_fs_paths *ipath = NULL;\n\tstruct btrfs_path *path;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tipa = memdup_user(arg, sizeof(*ipa));\n\tif (IS_ERR(ipa)) {\n\t\tret = PTR_ERR(ipa);\n\t\tipa = NULL;\n\t\tgoto out;\n\t}\n\n\tsize = min_t(u32, ipa->size, 4096);\n\tipath = init_ipath(size, root, path);\n\tif (IS_ERR(ipath)) {\n\t\tret = PTR_ERR(ipath);\n\t\tipath = NULL;\n\t\tgoto out;\n\t}\n\n\tret = paths_from_inode(ipa->inum, ipath);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tfor (i = 0; i < ipath->fspath->elem_cnt; ++i) {\n\t\trel_ptr = ipath->fspath->val[i] -\n\t\t\t  (u64)(unsigned long)ipath->fspath->val;\n\t\tipath->fspath->val[i] = rel_ptr;\n\t}\n\n\tret = copy_to_user((void *)(unsigned long)ipa->fspath,\n\t\t\t   (void *)(unsigned long)ipath->fspath, size);\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tfree_ipath(ipath);\n\tkfree(ipa);\n\n\treturn ret;\n}\n\nstatic int build_ino_list(u64 inum, u64 offset, u64 root, void *ctx)\n{\n\tstruct btrfs_data_container *inodes = ctx;\n\tconst size_t c = 3 * sizeof(u64);\n\n\tif (inodes->bytes_left >= c) {\n\t\tinodes->bytes_left -= c;\n\t\tinodes->val[inodes->elem_cnt] = inum;\n\t\tinodes->val[inodes->elem_cnt + 1] = offset;\n\t\tinodes->val[inodes->elem_cnt + 2] = root;\n\t\tinodes->elem_cnt += 3;\n\t} else {\n\t\tinodes->bytes_missing += c - inodes->bytes_left;\n\t\tinodes->bytes_left = 0;\n\t\tinodes->elem_missed += 3;\n\t}\n\n\treturn 0;\n}\n\nstatic long btrfs_ioctl_logical_to_ino(struct btrfs_root *root,\n\t\t\t\t\tvoid __user *arg)\n{\n\tint ret = 0;\n\tint size;\n\tstruct btrfs_ioctl_logical_ino_args *loi;\n\tstruct btrfs_data_container *inodes = NULL;\n\tstruct btrfs_path *path = NULL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tloi = memdup_user(arg, sizeof(*loi));\n\tif (IS_ERR(loi)) {\n\t\tret = PTR_ERR(loi);\n\t\tloi = NULL;\n\t\tgoto out;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tsize = min_t(u32, loi->size, 64 * 1024);\n\tinodes = init_data_container(size);\n\tif (IS_ERR(inodes)) {\n\t\tret = PTR_ERR(inodes);\n\t\tinodes = NULL;\n\t\tgoto out;\n\t}\n\n\tret = iterate_inodes_from_logical(loi->logical, root->fs_info, path,\n\t\t\t\t\t  build_ino_list, inodes);\n\tif (ret == -EINVAL)\n\t\tret = -ENOENT;\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = copy_to_user((void *)(unsigned long)loi->inodes,\n\t\t\t   (void *)(unsigned long)inodes, size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tbtrfs_free_path(path);\n\tvfree(inodes);\n\tkfree(loi);\n\n\treturn ret;\n}\n\nvoid update_ioctl_balance_args(struct btrfs_fs_info *fs_info, int lock,\n\t\t\t       struct btrfs_ioctl_balance_args *bargs)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbargs->flags = bctl->flags;\n\n\tif (atomic_read(&fs_info->balance_running))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_RUNNING;\n\tif (atomic_read(&fs_info->balance_pause_req))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_PAUSE_REQ;\n\tif (atomic_read(&fs_info->balance_cancel_req))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_CANCEL_REQ;\n\n\tmemcpy(&bargs->data, &bctl->data, sizeof(bargs->data));\n\tmemcpy(&bargs->meta, &bctl->meta, sizeof(bargs->meta));\n\tmemcpy(&bargs->sys, &bctl->sys, sizeof(bargs->sys));\n\n\tif (lock) {\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tmemcpy(&bargs->stat, &bctl->stat, sizeof(bargs->stat));\n\t\tspin_unlock(&fs_info->balance_lock);\n\t} else {\n\t\tmemcpy(&bargs->stat, &bctl->stat, sizeof(bargs->stat));\n\t}\n}\n\nstatic long btrfs_ioctl_balance(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_ioctl_balance_args *bargs;\n\tstruct btrfs_balance_control *bctl;\n\tint ret;\n\tint need_to_clear_lock = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&fs_info->volume_mutex);\n\tmutex_lock(&fs_info->balance_mutex);\n\n\tif (arg) {\n\t\tbargs = memdup_user(arg, sizeof(*bargs));\n\t\tif (IS_ERR(bargs)) {\n\t\t\tret = PTR_ERR(bargs);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (bargs->flags & BTRFS_BALANCE_RESUME) {\n\t\t\tif (!fs_info->balance_ctl) {\n\t\t\t\tret = -ENOTCONN;\n\t\t\t\tgoto out_bargs;\n\t\t\t}\n\n\t\t\tbctl = fs_info->balance_ctl;\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tgoto do_balance;\n\t\t}\n\t} else {\n\t\tbargs = NULL;\n\t}\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\tret = -EINPROGRESS;\n\t\tgoto out_bargs;\n\t}\n\tneed_to_clear_lock = 1;\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_NOFS);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out_bargs;\n\t}\n\n\tbctl->fs_info = fs_info;\n\tif (arg) {\n\t\tmemcpy(&bctl->data, &bargs->data, sizeof(bctl->data));\n\t\tmemcpy(&bctl->meta, &bargs->meta, sizeof(bctl->meta));\n\t\tmemcpy(&bctl->sys, &bargs->sys, sizeof(bctl->sys));\n\n\t\tbctl->flags = bargs->flags;\n\t} else {\n\t\t/* balance everything - no filters */\n\t\tbctl->flags |= BTRFS_BALANCE_TYPE_MASK;\n\t}\n\ndo_balance:\n\tret = btrfs_balance(bctl, bargs);\n\t/*\n\t * bctl is freed in __cancel_balance or in free_fs_info if\n\t * restriper was paused all the way until unmount\n\t */\n\tif (arg) {\n\t\tif (copy_to_user(arg, bargs, sizeof(*bargs)))\n\t\t\tret = -EFAULT;\n\t}\n\nout_bargs:\n\tkfree(bargs);\nout:\n\tif (need_to_clear_lock)\n\t\tatomic_set(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t   0);\n\tmutex_unlock(&fs_info->balance_mutex);\n\tmutex_unlock(&fs_info->volume_mutex);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_balance_ctl(struct btrfs_root *root, int cmd)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tswitch (cmd) {\n\tcase BTRFS_BALANCE_CTL_PAUSE:\n\t\treturn btrfs_pause_balance(root->fs_info);\n\tcase BTRFS_BALANCE_CTL_CANCEL:\n\t\treturn btrfs_cancel_balance(root->fs_info);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic long btrfs_ioctl_balance_progress(struct btrfs_root *root,\n\t\t\t\t\t void __user *arg)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_ioctl_balance_args *bargs;\n\tint ret = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tret = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\tbargs = kzalloc(sizeof(*bargs), GFP_NOFS);\n\tif (!bargs) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tupdate_ioctl_balance_args(fs_info, 1, bargs);\n\n\tif (copy_to_user(arg, bargs, sizeof(*bargs)))\n\t\tret = -EFAULT;\n\n\tkfree(bargs);\nout:\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_ctl(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_quota_ctl_args *sa;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\tif (sa->cmd != BTRFS_QUOTA_CTL_RESCAN) {\n\t\ttrans = btrfs_start_transaction(root, 2);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (sa->cmd) {\n\tcase BTRFS_QUOTA_CTL_ENABLE:\n\t\tret = btrfs_quota_enable(trans, root->fs_info);\n\t\tbreak;\n\tcase BTRFS_QUOTA_CTL_DISABLE:\n\t\tret = btrfs_quota_disable(trans, root->fs_info);\n\t\tbreak;\n\tcase BTRFS_QUOTA_CTL_RESCAN:\n\t\tret = btrfs_quota_rescan(root->fs_info);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tif (trans) {\n\t\terr = btrfs_commit_transaction(trans, root);\n\t\tif (err && !ret)\n\t\t\tret = err;\n\t}\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_assign(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_qgroup_assign_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\t/* FIXME: check if the IDs really exist */\n\tif (sa->assign) {\n\t\tret = btrfs_add_qgroup_relation(trans, root->fs_info,\n\t\t\t\t\t\tsa->src, sa->dst);\n\t} else {\n\t\tret = btrfs_del_qgroup_relation(trans, root->fs_info,\n\t\t\t\t\t\tsa->src, sa->dst);\n\t}\n\n\terr = btrfs_end_transaction(trans, root);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_create(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_qgroup_create_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\t/* FIXME: check if the IDs really exist */\n\tif (sa->create) {\n\t\tret = btrfs_create_qgroup(trans, root->fs_info, sa->qgroupid,\n\t\t\t\t\t  NULL);\n\t} else {\n\t\tret = btrfs_remove_qgroup(trans, root->fs_info, sa->qgroupid);\n\t}\n\n\terr = btrfs_end_transaction(trans, root);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_limit(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_qgroup_limit_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\tu64 qgroupid;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tqgroupid = sa->qgroupid;\n\tif (!qgroupid) {\n\t\t/* take the current subvol as qgroup */\n\t\tqgroupid = root->root_key.objectid;\n\t}\n\n\t/* FIXME: check if the IDs really exist */\n\tret = btrfs_limit_qgroup(trans, root->fs_info, qgroupid, &sa->lim);\n\n\terr = btrfs_end_transaction(trans, root);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_set_received_subvol(struct file *file,\n\t\t\t\t\t    void __user *arg)\n{\n\tstruct btrfs_ioctl_received_subvol_args *sa = NULL;\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_root_item *root_item = &root->root_item;\n\tstruct btrfs_trans_handle *trans;\n\tstruct timespec ct = CURRENT_TIME;\n\tint ret = 0;\n\n\tret = mnt_want_write_file(file);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tdown_write(&root->fs_info->subvol_sem);\n\n\tif (btrfs_ino(inode) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (btrfs_root_readonly(root)) {\n\t\tret = -EROFS;\n\t\tgoto out;\n\t}\n\n\tif (!inode_owner_or_capable(inode)) {\n\t\tret = -EACCES;\n\t\tgoto out;\n\t}\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tsa = NULL;\n\t\tgoto out;\n\t}\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\ttrans = NULL;\n\t\tgoto out;\n\t}\n\n\tsa->rtransid = trans->transid;\n\tsa->rtime.sec = ct.tv_sec;\n\tsa->rtime.nsec = ct.tv_nsec;\n\n\tmemcpy(root_item->received_uuid, sa->uuid, BTRFS_UUID_SIZE);\n\tbtrfs_set_root_stransid(root_item, sa->stransid);\n\tbtrfs_set_root_rtransid(root_item, sa->rtransid);\n\troot_item->stime.sec = cpu_to_le64(sa->stime.sec);\n\troot_item->stime.nsec = cpu_to_le32(sa->stime.nsec);\n\troot_item->rtime.sec = cpu_to_le64(sa->rtime.sec);\n\troot_item->rtime.nsec = cpu_to_le32(sa->rtime.nsec);\n\n\tret = btrfs_update_root(trans, root->fs_info->tree_root,\n\t\t\t\t&root->root_key, &root->root_item);\n\tif (ret < 0) {\n\t\tbtrfs_end_transaction(trans, root);\n\t\ttrans = NULL;\n\t\tgoto out;\n\t} else {\n\t\tret = btrfs_commit_transaction(trans, root);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = copy_to_user(arg, sa, sizeof(*sa));\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(sa);\n\tup_write(&root->fs_info->subvol_sem);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nlong btrfs_ioctl(struct file *file, unsigned int\n\t\tcmd, unsigned long arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase FS_IOC_GETFLAGS:\n\t\treturn btrfs_ioctl_getflags(file, argp);\n\tcase FS_IOC_SETFLAGS:\n\t\treturn btrfs_ioctl_setflags(file, argp);\n\tcase FS_IOC_GETVERSION:\n\t\treturn btrfs_ioctl_getversion(file, argp);\n\tcase FITRIM:\n\t\treturn btrfs_ioctl_fitrim(file, argp);\n\tcase BTRFS_IOC_SNAP_CREATE:\n\t\treturn btrfs_ioctl_snap_create(file, argp, 0);\n\tcase BTRFS_IOC_SNAP_CREATE_V2:\n\t\treturn btrfs_ioctl_snap_create_v2(file, argp, 0);\n\tcase BTRFS_IOC_SUBVOL_CREATE:\n\t\treturn btrfs_ioctl_snap_create(file, argp, 1);\n\tcase BTRFS_IOC_SUBVOL_CREATE_V2:\n\t\treturn btrfs_ioctl_snap_create_v2(file, argp, 1);\n\tcase BTRFS_IOC_SNAP_DESTROY:\n\t\treturn btrfs_ioctl_snap_destroy(file, argp);\n\tcase BTRFS_IOC_SUBVOL_GETFLAGS:\n\t\treturn btrfs_ioctl_subvol_getflags(file, argp);\n\tcase BTRFS_IOC_SUBVOL_SETFLAGS:\n\t\treturn btrfs_ioctl_subvol_setflags(file, argp);\n\tcase BTRFS_IOC_DEFAULT_SUBVOL:\n\t\treturn btrfs_ioctl_default_subvol(file, argp);\n\tcase BTRFS_IOC_DEFRAG:\n\t\treturn btrfs_ioctl_defrag(file, NULL);\n\tcase BTRFS_IOC_DEFRAG_RANGE:\n\t\treturn btrfs_ioctl_defrag(file, argp);\n\tcase BTRFS_IOC_RESIZE:\n\t\treturn btrfs_ioctl_resize(file, argp);\n\tcase BTRFS_IOC_ADD_DEV:\n\t\treturn btrfs_ioctl_add_dev(root, argp);\n\tcase BTRFS_IOC_RM_DEV:\n\t\treturn btrfs_ioctl_rm_dev(file, argp);\n\tcase BTRFS_IOC_FS_INFO:\n\t\treturn btrfs_ioctl_fs_info(root, argp);\n\tcase BTRFS_IOC_DEV_INFO:\n\t\treturn btrfs_ioctl_dev_info(root, argp);\n\tcase BTRFS_IOC_BALANCE:\n\t\treturn btrfs_ioctl_balance(file, NULL);\n\tcase BTRFS_IOC_CLONE:\n\t\treturn btrfs_ioctl_clone(file, arg, 0, 0, 0);\n\tcase BTRFS_IOC_CLONE_RANGE:\n\t\treturn btrfs_ioctl_clone_range(file, argp);\n\tcase BTRFS_IOC_TRANS_START:\n\t\treturn btrfs_ioctl_trans_start(file);\n\tcase BTRFS_IOC_TRANS_END:\n\t\treturn btrfs_ioctl_trans_end(file);\n\tcase BTRFS_IOC_TREE_SEARCH:\n\t\treturn btrfs_ioctl_tree_search(file, argp);\n\tcase BTRFS_IOC_INO_LOOKUP:\n\t\treturn btrfs_ioctl_ino_lookup(file, argp);\n\tcase BTRFS_IOC_INO_PATHS:\n\t\treturn btrfs_ioctl_ino_to_path(root, argp);\n\tcase BTRFS_IOC_LOGICAL_INO:\n\t\treturn btrfs_ioctl_logical_to_ino(root, argp);\n\tcase BTRFS_IOC_SPACE_INFO:\n\t\treturn btrfs_ioctl_space_info(root, argp);\n\tcase BTRFS_IOC_SYNC:\n\t\tbtrfs_sync_fs(file->f_dentry->d_sb, 1);\n\t\treturn 0;\n\tcase BTRFS_IOC_START_SYNC:\n\t\treturn btrfs_ioctl_start_sync(root, argp);\n\tcase BTRFS_IOC_WAIT_SYNC:\n\t\treturn btrfs_ioctl_wait_sync(root, argp);\n\tcase BTRFS_IOC_SCRUB:\n\t\treturn btrfs_ioctl_scrub(file, argp);\n\tcase BTRFS_IOC_SCRUB_CANCEL:\n\t\treturn btrfs_ioctl_scrub_cancel(root, argp);\n\tcase BTRFS_IOC_SCRUB_PROGRESS:\n\t\treturn btrfs_ioctl_scrub_progress(root, argp);\n\tcase BTRFS_IOC_BALANCE_V2:\n\t\treturn btrfs_ioctl_balance(file, argp);\n\tcase BTRFS_IOC_BALANCE_CTL:\n\t\treturn btrfs_ioctl_balance_ctl(root, arg);\n\tcase BTRFS_IOC_BALANCE_PROGRESS:\n\t\treturn btrfs_ioctl_balance_progress(root, argp);\n\tcase BTRFS_IOC_SET_RECEIVED_SUBVOL:\n\t\treturn btrfs_ioctl_set_received_subvol(file, argp);\n\tcase BTRFS_IOC_SEND:\n\t\treturn btrfs_ioctl_send(file, argp);\n\tcase BTRFS_IOC_GET_DEV_STATS:\n\t\treturn btrfs_ioctl_get_dev_stats(root, argp);\n\tcase BTRFS_IOC_QUOTA_CTL:\n\t\treturn btrfs_ioctl_quota_ctl(file, argp);\n\tcase BTRFS_IOC_QGROUP_ASSIGN:\n\t\treturn btrfs_ioctl_qgroup_assign(file, argp);\n\tcase BTRFS_IOC_QGROUP_CREATE:\n\t\treturn btrfs_ioctl_qgroup_create(file, argp);\n\tcase BTRFS_IOC_QGROUP_LIMIT:\n\t\treturn btrfs_ioctl_qgroup_limit(file, argp);\n\tcase BTRFS_IOC_DEV_REPLACE:\n\t\treturn btrfs_ioctl_dev_replace(root, argp);\n\t}\n\n\treturn -ENOTTY;\n}\n", "/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/writeback.h>\n#include <linux/pagemap.h>\n#include <linux/blkdev.h>\n#include <linux/uuid.h>\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"locking.h\"\n#include \"tree-log.h\"\n#include \"inode-map.h\"\n#include \"volumes.h\"\n#include \"dev-replace.h\"\n\n#define BTRFS_ROOT_TRANS_TAG 0\n\nvoid put_transaction(struct btrfs_transaction *transaction)\n{\n\tWARN_ON(atomic_read(&transaction->use_count) == 0);\n\tif (atomic_dec_and_test(&transaction->use_count)) {\n\t\tBUG_ON(!list_empty(&transaction->list));\n\t\tWARN_ON(transaction->delayed_refs.root.rb_node);\n\t\tmemset(transaction, 0, sizeof(*transaction));\n\t\tkmem_cache_free(btrfs_transaction_cachep, transaction);\n\t}\n}\n\nstatic noinline void switch_commit_root(struct btrfs_root *root)\n{\n\tfree_extent_buffer(root->commit_root);\n\troot->commit_root = btrfs_root_node(root);\n}\n\n/*\n * either allocate a new transaction or hop into the existing one\n */\nstatic noinline int join_transaction(struct btrfs_root *root, int type)\n{\n\tstruct btrfs_transaction *cur_trans;\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\n\tspin_lock(&fs_info->trans_lock);\nloop:\n\t/* The file system has been taken offline. No new transactions. */\n\tif (fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\treturn -EROFS;\n\t}\n\n\tif (fs_info->trans_no_join) {\n\t\t/* \n\t\t * If we are JOIN_NOLOCK we're already committing a current\n\t\t * transaction, we just need a handle to deal with something\n\t\t * when committing the transaction, such as inode cache and\n\t\t * space cache. It is a special case.\n\t\t */\n\t\tif (type != TRANS_JOIN_NOLOCK) {\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\n\tcur_trans = fs_info->running_transaction;\n\tif (cur_trans) {\n\t\tif (cur_trans->aborted) {\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\treturn cur_trans->aborted;\n\t\t}\n\t\tatomic_inc(&cur_trans->use_count);\n\t\tatomic_inc(&cur_trans->num_writers);\n\t\tcur_trans->num_joined++;\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\treturn 0;\n\t}\n\tspin_unlock(&fs_info->trans_lock);\n\n\t/*\n\t * If we are ATTACH, we just want to catch the current transaction,\n\t * and commit it. If there is no transaction, just return ENOENT.\n\t */\n\tif (type == TRANS_ATTACH)\n\t\treturn -ENOENT;\n\n\tcur_trans = kmem_cache_alloc(btrfs_transaction_cachep, GFP_NOFS);\n\tif (!cur_trans)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&fs_info->trans_lock);\n\tif (fs_info->running_transaction) {\n\t\t/*\n\t\t * someone started a transaction after we unlocked.  Make sure\n\t\t * to redo the trans_no_join checks above\n\t\t */\n\t\tkmem_cache_free(btrfs_transaction_cachep, cur_trans);\n\t\tcur_trans = fs_info->running_transaction;\n\t\tgoto loop;\n\t} else if (fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\tkmem_cache_free(btrfs_transaction_cachep, cur_trans);\n\t\treturn -EROFS;\n\t}\n\n\tatomic_set(&cur_trans->num_writers, 1);\n\tcur_trans->num_joined = 0;\n\tinit_waitqueue_head(&cur_trans->writer_wait);\n\tinit_waitqueue_head(&cur_trans->commit_wait);\n\tcur_trans->in_commit = 0;\n\tcur_trans->blocked = 0;\n\t/*\n\t * One for this trans handle, one so it will live on until we\n\t * commit the transaction.\n\t */\n\tatomic_set(&cur_trans->use_count, 2);\n\tcur_trans->commit_done = 0;\n\tcur_trans->start_time = get_seconds();\n\n\tcur_trans->delayed_refs.root = RB_ROOT;\n\tcur_trans->delayed_refs.num_entries = 0;\n\tcur_trans->delayed_refs.num_heads_ready = 0;\n\tcur_trans->delayed_refs.num_heads = 0;\n\tcur_trans->delayed_refs.flushing = 0;\n\tcur_trans->delayed_refs.run_delayed_start = 0;\n\n\t/*\n\t * although the tree mod log is per file system and not per transaction,\n\t * the log must never go across transaction boundaries.\n\t */\n\tsmp_mb();\n\tif (!list_empty(&fs_info->tree_mod_seq_list))\n\t\tWARN(1, KERN_ERR \"btrfs: tree_mod_seq_list not empty when \"\n\t\t\t\"creating a fresh transaction\\n\");\n\tif (!RB_EMPTY_ROOT(&fs_info->tree_mod_log))\n\t\tWARN(1, KERN_ERR \"btrfs: tree_mod_log rb tree not empty when \"\n\t\t\t\"creating a fresh transaction\\n\");\n\tatomic_set(&fs_info->tree_mod_seq, 0);\n\n\tspin_lock_init(&cur_trans->commit_lock);\n\tspin_lock_init(&cur_trans->delayed_refs.lock);\n\n\tINIT_LIST_HEAD(&cur_trans->pending_snapshots);\n\tlist_add_tail(&cur_trans->list, &fs_info->trans_list);\n\textent_io_tree_init(&cur_trans->dirty_pages,\n\t\t\t     fs_info->btree_inode->i_mapping);\n\tfs_info->generation++;\n\tcur_trans->transid = fs_info->generation;\n\tfs_info->running_transaction = cur_trans;\n\tcur_trans->aborted = 0;\n\tspin_unlock(&fs_info->trans_lock);\n\n\treturn 0;\n}\n\n/*\n * this does all the record keeping required to make sure that a reference\n * counted root is properly recorded in a given transaction.  This is required\n * to make sure the old root from before we joined the transaction is deleted\n * when the transaction commits\n */\nstatic int record_root_in_trans(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root)\n{\n\tif (root->ref_cows && root->last_trans < trans->transid) {\n\t\tWARN_ON(root == root->fs_info->extent_root);\n\t\tWARN_ON(root->commit_root != root->node);\n\n\t\t/*\n\t\t * see below for in_trans_setup usage rules\n\t\t * we have the reloc mutex held now, so there\n\t\t * is only one writer in this function\n\t\t */\n\t\troot->in_trans_setup = 1;\n\n\t\t/* make sure readers find in_trans_setup before\n\t\t * they find our root->last_trans update\n\t\t */\n\t\tsmp_wmb();\n\n\t\tspin_lock(&root->fs_info->fs_roots_radix_lock);\n\t\tif (root->last_trans == trans->transid) {\n\t\t\tspin_unlock(&root->fs_info->fs_roots_radix_lock);\n\t\t\treturn 0;\n\t\t}\n\t\tradix_tree_tag_set(&root->fs_info->fs_roots_radix,\n\t\t\t   (unsigned long)root->root_key.objectid,\n\t\t\t   BTRFS_ROOT_TRANS_TAG);\n\t\tspin_unlock(&root->fs_info->fs_roots_radix_lock);\n\t\troot->last_trans = trans->transid;\n\n\t\t/* this is pretty tricky.  We don't want to\n\t\t * take the relocation lock in btrfs_record_root_in_trans\n\t\t * unless we're really doing the first setup for this root in\n\t\t * this transaction.\n\t\t *\n\t\t * Normally we'd use root->last_trans as a flag to decide\n\t\t * if we want to take the expensive mutex.\n\t\t *\n\t\t * But, we have to set root->last_trans before we\n\t\t * init the relocation root, otherwise, we trip over warnings\n\t\t * in ctree.c.  The solution used here is to flag ourselves\n\t\t * with root->in_trans_setup.  When this is 1, we're still\n\t\t * fixing up the reloc trees and everyone must wait.\n\t\t *\n\t\t * When this is zero, they can trust root->last_trans and fly\n\t\t * through btrfs_record_root_in_trans without having to take the\n\t\t * lock.  smp_wmb() makes sure that all the writes above are\n\t\t * done before we pop in the zero below\n\t\t */\n\t\tbtrfs_init_reloc_root(trans, root);\n\t\tsmp_wmb();\n\t\troot->in_trans_setup = 0;\n\t}\n\treturn 0;\n}\n\n\nint btrfs_record_root_in_trans(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root)\n{\n\tif (!root->ref_cows)\n\t\treturn 0;\n\n\t/*\n\t * see record_root_in_trans for comments about in_trans_setup usage\n\t * and barriers\n\t */\n\tsmp_rmb();\n\tif (root->last_trans == trans->transid &&\n\t    !root->in_trans_setup)\n\t\treturn 0;\n\n\tmutex_lock(&root->fs_info->reloc_mutex);\n\trecord_root_in_trans(trans, root);\n\tmutex_unlock(&root->fs_info->reloc_mutex);\n\n\treturn 0;\n}\n\n/* wait for commit against the current transaction to become unblocked\n * when this is done, it is safe to start a new transaction, but the current\n * transaction might not be fully on disk.\n */\nstatic void wait_current_trans(struct btrfs_root *root)\n{\n\tstruct btrfs_transaction *cur_trans;\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tcur_trans = root->fs_info->running_transaction;\n\tif (cur_trans && cur_trans->blocked) {\n\t\tatomic_inc(&cur_trans->use_count);\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\n\t\twait_event(root->fs_info->transaction_wait,\n\t\t\t   !cur_trans->blocked);\n\t\tput_transaction(cur_trans);\n\t} else {\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t}\n}\n\nstatic int may_wait_transaction(struct btrfs_root *root, int type)\n{\n\tif (root->fs_info->log_root_recovering)\n\t\treturn 0;\n\n\tif (type == TRANS_USERSPACE)\n\t\treturn 1;\n\n\tif (type == TRANS_START &&\n\t    !atomic_read(&root->fs_info->open_ioctl_trans))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic struct btrfs_trans_handle *\nstart_transaction(struct btrfs_root *root, u64 num_items, int type,\n\t\t  enum btrfs_reserve_flush_enum flush)\n{\n\tstruct btrfs_trans_handle *h;\n\tstruct btrfs_transaction *cur_trans;\n\tu64 num_bytes = 0;\n\tint ret;\n\tu64 qgroup_reserved = 0;\n\n\tif (root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR)\n\t\treturn ERR_PTR(-EROFS);\n\n\tif (current->journal_info) {\n\t\tWARN_ON(type != TRANS_JOIN && type != TRANS_JOIN_NOLOCK);\n\t\th = current->journal_info;\n\t\th->use_count++;\n\t\tWARN_ON(h->use_count > 2);\n\t\th->orig_rsv = h->block_rsv;\n\t\th->block_rsv = NULL;\n\t\tgoto got_it;\n\t}\n\n\t/*\n\t * Do the reservation before we join the transaction so we can do all\n\t * the appropriate flushing if need be.\n\t */\n\tif (num_items > 0 && root != root->fs_info->chunk_root) {\n\t\tif (root->fs_info->quota_enabled &&\n\t\t    is_fstree(root->root_key.objectid)) {\n\t\t\tqgroup_reserved = num_items * root->leafsize;\n\t\t\tret = btrfs_qgroup_reserve(root, qgroup_reserved);\n\t\t\tif (ret)\n\t\t\t\treturn ERR_PTR(ret);\n\t\t}\n\n\t\tnum_bytes = btrfs_calc_trans_metadata_size(root, num_items);\n\t\tret = btrfs_block_rsv_add(root,\n\t\t\t\t\t  &root->fs_info->trans_block_rsv,\n\t\t\t\t\t  num_bytes, flush);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\nagain:\n\th = kmem_cache_alloc(btrfs_trans_handle_cachep, GFP_NOFS);\n\tif (!h)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * If we are JOIN_NOLOCK we're already committing a transaction and\n\t * waiting on this guy, so we don't need to do the sb_start_intwrite\n\t * because we're already holding a ref.  We need this because we could\n\t * have raced in and did an fsync() on a file which can kick a commit\n\t * and then we deadlock with somebody doing a freeze.\n\t *\n\t * If we are ATTACH, it means we just want to catch the current\n\t * transaction and commit it, so we needn't do sb_start_intwrite(). \n\t */\n\tif (type < TRANS_JOIN_NOLOCK)\n\t\tsb_start_intwrite(root->fs_info->sb);\n\n\tif (may_wait_transaction(root, type))\n\t\twait_current_trans(root);\n\n\tdo {\n\t\tret = join_transaction(root, type);\n\t\tif (ret == -EBUSY)\n\t\t\twait_current_trans(root);\n\t} while (ret == -EBUSY);\n\n\tif (ret < 0) {\n\t\t/* We must get the transaction if we are JOIN_NOLOCK. */\n\t\tBUG_ON(type == TRANS_JOIN_NOLOCK);\n\n\t\tif (type < TRANS_JOIN_NOLOCK)\n\t\t\tsb_end_intwrite(root->fs_info->sb);\n\t\tkmem_cache_free(btrfs_trans_handle_cachep, h);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tcur_trans = root->fs_info->running_transaction;\n\n\th->transid = cur_trans->transid;\n\th->transaction = cur_trans;\n\th->blocks_used = 0;\n\th->bytes_reserved = 0;\n\th->root = root;\n\th->delayed_ref_updates = 0;\n\th->use_count = 1;\n\th->adding_csums = 0;\n\th->block_rsv = NULL;\n\th->orig_rsv = NULL;\n\th->aborted = 0;\n\th->qgroup_reserved = qgroup_reserved;\n\th->delayed_ref_elem.seq = 0;\n\th->type = type;\n\tINIT_LIST_HEAD(&h->qgroup_ref_list);\n\tINIT_LIST_HEAD(&h->new_bgs);\n\n\tsmp_mb();\n\tif (cur_trans->blocked && may_wait_transaction(root, type)) {\n\t\tbtrfs_commit_transaction(h, root);\n\t\tgoto again;\n\t}\n\n\tif (num_bytes) {\n\t\ttrace_btrfs_space_reservation(root->fs_info, \"transaction\",\n\t\t\t\t\t      h->transid, num_bytes, 1);\n\t\th->block_rsv = &root->fs_info->trans_block_rsv;\n\t\th->bytes_reserved = num_bytes;\n\t}\n\ngot_it:\n\tbtrfs_record_root_in_trans(h, root);\n\n\tif (!current->journal_info && type != TRANS_USERSPACE)\n\t\tcurrent->journal_info = h;\n\treturn h;\n}\n\nstruct btrfs_trans_handle *btrfs_start_transaction(struct btrfs_root *root,\n\t\t\t\t\t\t   int num_items)\n{\n\treturn start_transaction(root, num_items, TRANS_START,\n\t\t\t\t BTRFS_RESERVE_FLUSH_ALL);\n}\n\nstruct btrfs_trans_handle *btrfs_start_transaction_lflush(\n\t\t\t\t\tstruct btrfs_root *root, int num_items)\n{\n\treturn start_transaction(root, num_items, TRANS_START,\n\t\t\t\t BTRFS_RESERVE_FLUSH_LIMIT);\n}\n\nstruct btrfs_trans_handle *btrfs_join_transaction(struct btrfs_root *root)\n{\n\treturn start_transaction(root, 0, TRANS_JOIN, 0);\n}\n\nstruct btrfs_trans_handle *btrfs_join_transaction_nolock(struct btrfs_root *root)\n{\n\treturn start_transaction(root, 0, TRANS_JOIN_NOLOCK, 0);\n}\n\nstruct btrfs_trans_handle *btrfs_start_ioctl_transaction(struct btrfs_root *root)\n{\n\treturn start_transaction(root, 0, TRANS_USERSPACE, 0);\n}\n\nstruct btrfs_trans_handle *btrfs_attach_transaction(struct btrfs_root *root)\n{\n\treturn start_transaction(root, 0, TRANS_ATTACH, 0);\n}\n\n/* wait for a transaction commit to be fully complete */\nstatic noinline void wait_for_commit(struct btrfs_root *root,\n\t\t\t\t    struct btrfs_transaction *commit)\n{\n\twait_event(commit->commit_wait, commit->commit_done);\n}\n\nint btrfs_wait_for_commit(struct btrfs_root *root, u64 transid)\n{\n\tstruct btrfs_transaction *cur_trans = NULL, *t;\n\tint ret = 0;\n\n\tif (transid) {\n\t\tif (transid <= root->fs_info->last_trans_committed)\n\t\t\tgoto out;\n\n\t\tret = -EINVAL;\n\t\t/* find specified transaction */\n\t\tspin_lock(&root->fs_info->trans_lock);\n\t\tlist_for_each_entry(t, &root->fs_info->trans_list, list) {\n\t\t\tif (t->transid == transid) {\n\t\t\t\tcur_trans = t;\n\t\t\t\tatomic_inc(&cur_trans->use_count);\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (t->transid > transid) {\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t\t/* The specified transaction doesn't exist */\n\t\tif (!cur_trans)\n\t\t\tgoto out;\n\t} else {\n\t\t/* find newest transaction that is committing | committed */\n\t\tspin_lock(&root->fs_info->trans_lock);\n\t\tlist_for_each_entry_reverse(t, &root->fs_info->trans_list,\n\t\t\t\t\t    list) {\n\t\t\tif (t->in_commit) {\n\t\t\t\tif (t->commit_done)\n\t\t\t\t\tbreak;\n\t\t\t\tcur_trans = t;\n\t\t\t\tatomic_inc(&cur_trans->use_count);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t\tif (!cur_trans)\n\t\t\tgoto out;  /* nothing committing|committed */\n\t}\n\n\twait_for_commit(root, cur_trans);\n\tput_transaction(cur_trans);\nout:\n\treturn ret;\n}\n\nvoid btrfs_throttle(struct btrfs_root *root)\n{\n\tif (!atomic_read(&root->fs_info->open_ioctl_trans))\n\t\twait_current_trans(root);\n}\n\nstatic int should_end_transaction(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct btrfs_root *root)\n{\n\tint ret;\n\n\tret = btrfs_block_rsv_check(root, &root->fs_info->global_block_rsv, 5);\n\treturn ret ? 1 : 0;\n}\n\nint btrfs_should_end_transaction(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root)\n{\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tint updates;\n\tint err;\n\n\tsmp_mb();\n\tif (cur_trans->blocked || cur_trans->delayed_refs.flushing)\n\t\treturn 1;\n\n\tupdates = trans->delayed_ref_updates;\n\ttrans->delayed_ref_updates = 0;\n\tif (updates) {\n\t\terr = btrfs_run_delayed_refs(trans, root, updates);\n\t\tif (err) /* Error code will also eval true */\n\t\t\treturn err;\n\t}\n\n\treturn should_end_transaction(trans, root);\n}\n\nstatic int __btrfs_end_transaction(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root, int throttle)\n{\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tstruct btrfs_fs_info *info = root->fs_info;\n\tint count = 0;\n\tint lock = (trans->type != TRANS_JOIN_NOLOCK);\n\tint err = 0;\n\n\tif (--trans->use_count) {\n\t\ttrans->block_rsv = trans->orig_rsv;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * do the qgroup accounting as early as possible\n\t */\n\terr = btrfs_delayed_refs_qgroup_accounting(trans, info);\n\n\tbtrfs_trans_release_metadata(trans, root);\n\ttrans->block_rsv = NULL;\n\t/*\n\t * the same root has to be passed to start_transaction and\n\t * end_transaction. Subvolume quota depends on this.\n\t */\n\tWARN_ON(trans->root != root);\n\n\tif (trans->qgroup_reserved) {\n\t\tbtrfs_qgroup_free(root, trans->qgroup_reserved);\n\t\ttrans->qgroup_reserved = 0;\n\t}\n\n\tif (!list_empty(&trans->new_bgs))\n\t\tbtrfs_create_pending_block_groups(trans, root);\n\n\twhile (count < 2) {\n\t\tunsigned long cur = trans->delayed_ref_updates;\n\t\ttrans->delayed_ref_updates = 0;\n\t\tif (cur &&\n\t\t    trans->transaction->delayed_refs.num_heads_ready > 64) {\n\t\t\ttrans->delayed_ref_updates = 0;\n\t\t\tbtrfs_run_delayed_refs(trans, root, cur);\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tcount++;\n\t}\n\tbtrfs_trans_release_metadata(trans, root);\n\ttrans->block_rsv = NULL;\n\n\tif (!list_empty(&trans->new_bgs))\n\t\tbtrfs_create_pending_block_groups(trans, root);\n\n\tif (lock && !atomic_read(&root->fs_info->open_ioctl_trans) &&\n\t    should_end_transaction(trans, root)) {\n\t\ttrans->transaction->blocked = 1;\n\t\tsmp_wmb();\n\t}\n\n\tif (lock && cur_trans->blocked && !cur_trans->in_commit) {\n\t\tif (throttle) {\n\t\t\t/*\n\t\t\t * We may race with somebody else here so end up having\n\t\t\t * to call end_transaction on ourselves again, so inc\n\t\t\t * our use_count.\n\t\t\t */\n\t\t\ttrans->use_count++;\n\t\t\treturn btrfs_commit_transaction(trans, root);\n\t\t} else {\n\t\t\twake_up_process(info->transaction_kthread);\n\t\t}\n\t}\n\n\tif (trans->type < TRANS_JOIN_NOLOCK)\n\t\tsb_end_intwrite(root->fs_info->sb);\n\n\tWARN_ON(cur_trans != info->running_transaction);\n\tWARN_ON(atomic_read(&cur_trans->num_writers) < 1);\n\tatomic_dec(&cur_trans->num_writers);\n\n\tsmp_mb();\n\tif (waitqueue_active(&cur_trans->writer_wait))\n\t\twake_up(&cur_trans->writer_wait);\n\tput_transaction(cur_trans);\n\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\n\tif (throttle)\n\t\tbtrfs_run_delayed_iputs(root);\n\n\tif (trans->aborted ||\n\t    root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {\n\t\terr = -EIO;\n\t}\n\tassert_qgroups_uptodate(trans);\n\n\tmemset(trans, 0, sizeof(*trans));\n\tkmem_cache_free(btrfs_trans_handle_cachep, trans);\n\treturn err;\n}\n\nint btrfs_end_transaction(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root)\n{\n\tint ret;\n\n\tret = __btrfs_end_transaction(trans, root, 0);\n\tif (ret)\n\t\treturn ret;\n\treturn 0;\n}\n\nint btrfs_end_transaction_throttle(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root)\n{\n\tint ret;\n\n\tret = __btrfs_end_transaction(trans, root, 1);\n\tif (ret)\n\t\treturn ret;\n\treturn 0;\n}\n\nint btrfs_end_transaction_dmeta(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root)\n{\n\treturn __btrfs_end_transaction(trans, root, 1);\n}\n\n/*\n * when btree blocks are allocated, they have some corresponding bits set for\n * them in one of two extent_io trees.  This is used to make sure all of\n * those extents are sent to disk but does not wait on them\n */\nint btrfs_write_marked_extents(struct btrfs_root *root,\n\t\t\t       struct extent_io_tree *dirty_pages, int mark)\n{\n\tint err = 0;\n\tint werr = 0;\n\tstruct address_space *mapping = root->fs_info->btree_inode->i_mapping;\n\tstruct extent_state *cached_state = NULL;\n\tu64 start = 0;\n\tu64 end;\n\n\twhile (!find_first_extent_bit(dirty_pages, start, &start, &end,\n\t\t\t\t      mark, &cached_state)) {\n\t\tconvert_extent_bit(dirty_pages, start, end, EXTENT_NEED_WAIT,\n\t\t\t\t   mark, &cached_state, GFP_NOFS);\n\t\tcached_state = NULL;\n\t\terr = filemap_fdatawrite_range(mapping, start, end);\n\t\tif (err)\n\t\t\twerr = err;\n\t\tcond_resched();\n\t\tstart = end + 1;\n\t}\n\tif (err)\n\t\twerr = err;\n\treturn werr;\n}\n\n/*\n * when btree blocks are allocated, they have some corresponding bits set for\n * them in one of two extent_io trees.  This is used to make sure all of\n * those extents are on disk for transaction or log commit.  We wait\n * on all the pages and clear them from the dirty pages state tree\n */\nint btrfs_wait_marked_extents(struct btrfs_root *root,\n\t\t\t      struct extent_io_tree *dirty_pages, int mark)\n{\n\tint err = 0;\n\tint werr = 0;\n\tstruct address_space *mapping = root->fs_info->btree_inode->i_mapping;\n\tstruct extent_state *cached_state = NULL;\n\tu64 start = 0;\n\tu64 end;\n\n\twhile (!find_first_extent_bit(dirty_pages, start, &start, &end,\n\t\t\t\t      EXTENT_NEED_WAIT, &cached_state)) {\n\t\tclear_extent_bit(dirty_pages, start, end, EXTENT_NEED_WAIT,\n\t\t\t\t 0, 0, &cached_state, GFP_NOFS);\n\t\terr = filemap_fdatawait_range(mapping, start, end);\n\t\tif (err)\n\t\t\twerr = err;\n\t\tcond_resched();\n\t\tstart = end + 1;\n\t}\n\tif (err)\n\t\twerr = err;\n\treturn werr;\n}\n\n/*\n * when btree blocks are allocated, they have some corresponding bits set for\n * them in one of two extent_io trees.  This is used to make sure all of\n * those extents are on disk for transaction or log commit\n */\nint btrfs_write_and_wait_marked_extents(struct btrfs_root *root,\n\t\t\t\tstruct extent_io_tree *dirty_pages, int mark)\n{\n\tint ret;\n\tint ret2;\n\n\tret = btrfs_write_marked_extents(root, dirty_pages, mark);\n\tret2 = btrfs_wait_marked_extents(root, dirty_pages, mark);\n\n\tif (ret)\n\t\treturn ret;\n\tif (ret2)\n\t\treturn ret2;\n\treturn 0;\n}\n\nint btrfs_write_and_wait_transaction(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_root *root)\n{\n\tif (!trans || !trans->transaction) {\n\t\tstruct inode *btree_inode;\n\t\tbtree_inode = root->fs_info->btree_inode;\n\t\treturn filemap_write_and_wait(btree_inode->i_mapping);\n\t}\n\treturn btrfs_write_and_wait_marked_extents(root,\n\t\t\t\t\t   &trans->transaction->dirty_pages,\n\t\t\t\t\t   EXTENT_DIRTY);\n}\n\n/*\n * this is used to update the root pointer in the tree of tree roots.\n *\n * But, in the case of the extent allocation tree, updating the root\n * pointer may allocate blocks which may change the root of the extent\n * allocation tree.\n *\n * So, this loops and repeats and makes sure the cowonly root didn't\n * change while the root pointer was being updated in the metadata.\n */\nstatic int update_cowonly_root(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root)\n{\n\tint ret;\n\tu64 old_root_bytenr;\n\tu64 old_root_used;\n\tstruct btrfs_root *tree_root = root->fs_info->tree_root;\n\n\told_root_used = btrfs_root_used(&root->root_item);\n\tbtrfs_write_dirty_block_groups(trans, root);\n\n\twhile (1) {\n\t\told_root_bytenr = btrfs_root_bytenr(&root->root_item);\n\t\tif (old_root_bytenr == root->node->start &&\n\t\t    old_root_used == btrfs_root_used(&root->root_item))\n\t\t\tbreak;\n\n\t\tbtrfs_set_root_node(&root->root_item, root->node);\n\t\tret = btrfs_update_root(trans, tree_root,\n\t\t\t\t\t&root->root_key,\n\t\t\t\t\t&root->root_item);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\told_root_used = btrfs_root_used(&root->root_item);\n\t\tret = btrfs_write_dirty_block_groups(trans, root);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (root != root->fs_info->extent_root)\n\t\tswitch_commit_root(root);\n\n\treturn 0;\n}\n\n/*\n * update all the cowonly tree roots on disk\n *\n * The error handling in this function may not be obvious. Any of the\n * failures will cause the file system to go offline. We still need\n * to clean up the delayed refs.\n */\nstatic noinline int commit_cowonly_roots(struct btrfs_trans_handle *trans,\n\t\t\t\t\t struct btrfs_root *root)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct list_head *next;\n\tstruct extent_buffer *eb;\n\tint ret;\n\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tif (ret)\n\t\treturn ret;\n\n\teb = btrfs_lock_root_node(fs_info->tree_root);\n\tret = btrfs_cow_block(trans, fs_info->tree_root, eb, NULL,\n\t\t\t      0, &eb);\n\tbtrfs_tree_unlock(eb);\n\tfree_extent_buffer(eb);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tif (ret)\n\t\treturn ret;\n\n\tret = btrfs_run_dev_stats(trans, root->fs_info);\n\tWARN_ON(ret);\n\tret = btrfs_run_dev_replace(trans, root->fs_info);\n\tWARN_ON(ret);\n\n\tret = btrfs_run_qgroups(trans, root->fs_info);\n\tBUG_ON(ret);\n\n\t/* run_qgroups might have added some more refs */\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tBUG_ON(ret);\n\n\twhile (!list_empty(&fs_info->dirty_cowonly_roots)) {\n\t\tnext = fs_info->dirty_cowonly_roots.next;\n\t\tlist_del_init(next);\n\t\troot = list_entry(next, struct btrfs_root, dirty_list);\n\n\t\tret = update_cowonly_root(trans, root);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tdown_write(&fs_info->extent_commit_sem);\n\tswitch_commit_root(fs_info->extent_root);\n\tup_write(&fs_info->extent_commit_sem);\n\n\tbtrfs_after_dev_replace_commit(fs_info);\n\n\treturn 0;\n}\n\n/*\n * dead roots are old snapshots that need to be deleted.  This allocates\n * a dirty root struct and adds it into the list of dead roots that need to\n * be deleted\n */\nint btrfs_add_dead_root(struct btrfs_root *root)\n{\n\tspin_lock(&root->fs_info->trans_lock);\n\tlist_add(&root->root_list, &root->fs_info->dead_roots);\n\tspin_unlock(&root->fs_info->trans_lock);\n\treturn 0;\n}\n\n/*\n * update all the cowonly tree roots on disk\n */\nstatic noinline int commit_fs_roots(struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_root *root)\n{\n\tstruct btrfs_root *gang[8];\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tint i;\n\tint ret;\n\tint err = 0;\n\n\tspin_lock(&fs_info->fs_roots_radix_lock);\n\twhile (1) {\n\t\tret = radix_tree_gang_lookup_tag(&fs_info->fs_roots_radix,\n\t\t\t\t\t\t (void **)gang, 0,\n\t\t\t\t\t\t ARRAY_SIZE(gang),\n\t\t\t\t\t\t BTRFS_ROOT_TRANS_TAG);\n\t\tif (ret == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < ret; i++) {\n\t\t\troot = gang[i];\n\t\t\tradix_tree_tag_clear(&fs_info->fs_roots_radix,\n\t\t\t\t\t(unsigned long)root->root_key.objectid,\n\t\t\t\t\tBTRFS_ROOT_TRANS_TAG);\n\t\t\tspin_unlock(&fs_info->fs_roots_radix_lock);\n\n\t\t\tbtrfs_free_log(trans, root);\n\t\t\tbtrfs_update_reloc_root(trans, root);\n\t\t\tbtrfs_orphan_commit_root(trans, root);\n\n\t\t\tbtrfs_save_ino_cache(root, trans);\n\n\t\t\t/* see comments in should_cow_block() */\n\t\t\troot->force_cow = 0;\n\t\t\tsmp_wmb();\n\n\t\t\tif (root->commit_root != root->node) {\n\t\t\t\tmutex_lock(&root->fs_commit_mutex);\n\t\t\t\tswitch_commit_root(root);\n\t\t\t\tbtrfs_unpin_free_ino(root);\n\t\t\t\tmutex_unlock(&root->fs_commit_mutex);\n\n\t\t\t\tbtrfs_set_root_node(&root->root_item,\n\t\t\t\t\t\t    root->node);\n\t\t\t}\n\n\t\t\terr = btrfs_update_root(trans, fs_info->tree_root,\n\t\t\t\t\t\t&root->root_key,\n\t\t\t\t\t\t&root->root_item);\n\t\t\tspin_lock(&fs_info->fs_roots_radix_lock);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&fs_info->fs_roots_radix_lock);\n\treturn err;\n}\n\n/*\n * defrag a given btree.  If cacheonly == 1, this won't read from the disk,\n * otherwise every leaf in the btree is read and defragged.\n */\nint btrfs_defrag_root(struct btrfs_root *root, int cacheonly)\n{\n\tstruct btrfs_fs_info *info = root->fs_info;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\tif (xchg(&root->defrag_running, 1))\n\t\treturn 0;\n\n\twhile (1) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\tret = btrfs_defrag_leaves(trans, root, cacheonly);\n\n\t\tbtrfs_end_transaction(trans, root);\n\t\tbtrfs_btree_balance_dirty(info->tree_root);\n\t\tcond_resched();\n\n\t\tif (btrfs_fs_closing(root->fs_info) || ret != -EAGAIN)\n\t\t\tbreak;\n\t}\n\troot->defrag_running = 0;\n\treturn ret;\n}\n\n/*\n * new snapshots need to be created at a very specific time in the\n * transaction commit.  This does the actual creation\n */\nstatic noinline int create_pending_snapshot(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_fs_info *fs_info,\n\t\t\t\t   struct btrfs_pending_snapshot *pending)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_root_item *new_root_item;\n\tstruct btrfs_root *tree_root = fs_info->tree_root;\n\tstruct btrfs_root *root = pending->root;\n\tstruct btrfs_root *parent_root;\n\tstruct btrfs_block_rsv *rsv;\n\tstruct inode *parent_inode;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dir_item *dir_item;\n\tstruct dentry *parent;\n\tstruct dentry *dentry;\n\tstruct extent_buffer *tmp;\n\tstruct extent_buffer *old;\n\tstruct timespec cur_time = CURRENT_TIME;\n\tint ret;\n\tu64 to_reserve = 0;\n\tu64 index = 0;\n\tu64 objectid;\n\tu64 root_flags;\n\tuuid_le new_uuid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = pending->error = -ENOMEM;\n\t\tgoto path_alloc_fail;\n\t}\n\n\tnew_root_item = kmalloc(sizeof(*new_root_item), GFP_NOFS);\n\tif (!new_root_item) {\n\t\tret = pending->error = -ENOMEM;\n\t\tgoto root_item_alloc_fail;\n\t}\n\n\tret = btrfs_find_free_objectid(tree_root, &objectid);\n\tif (ret) {\n\t\tpending->error = ret;\n\t\tgoto no_free_objectid;\n\t}\n\n\tbtrfs_reloc_pre_snapshot(trans, pending, &to_reserve);\n\n\tif (to_reserve > 0) {\n\t\tret = btrfs_block_rsv_add(root, &pending->block_rsv,\n\t\t\t\t\t  to_reserve,\n\t\t\t\t\t  BTRFS_RESERVE_NO_FLUSH);\n\t\tif (ret) {\n\t\t\tpending->error = ret;\n\t\t\tgoto no_free_objectid;\n\t\t}\n\t}\n\n\tret = btrfs_qgroup_inherit(trans, fs_info, root->root_key.objectid,\n\t\t\t\t   objectid, pending->inherit);\n\tif (ret) {\n\t\tpending->error = ret;\n\t\tgoto no_free_objectid;\n\t}\n\n\tkey.objectid = objectid;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\n\trsv = trans->block_rsv;\n\ttrans->block_rsv = &pending->block_rsv;\n\n\tdentry = pending->dentry;\n\tparent = dget_parent(dentry);\n\tparent_inode = parent->d_inode;\n\tparent_root = BTRFS_I(parent_inode)->root;\n\trecord_root_in_trans(trans, parent_root);\n\n\t/*\n\t * insert the directory item\n\t */\n\tret = btrfs_set_inode_index(parent_inode, &index);\n\tBUG_ON(ret); /* -ENOMEM */\n\n\t/* check if there is a file/dir which has the same name. */\n\tdir_item = btrfs_lookup_dir_item(NULL, parent_root, path,\n\t\t\t\t\t btrfs_ino(parent_inode),\n\t\t\t\t\t dentry->d_name.name,\n\t\t\t\t\t dentry->d_name.len, 0);\n\tif (dir_item != NULL && !IS_ERR(dir_item)) {\n\t\tpending->error = -EEXIST;\n\t\tgoto fail;\n\t} else if (IS_ERR(dir_item)) {\n\t\tret = PTR_ERR(dir_item);\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\tbtrfs_release_path(path);\n\n\t/*\n\t * pull in the delayed directory update\n\t * and the delayed inode item\n\t * otherwise we corrupt the FS during\n\t * snapshot\n\t */\n\tret = btrfs_run_delayed_items(trans, root);\n\tif (ret) {\t/* Transaction aborted */\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\trecord_root_in_trans(trans, root);\n\tbtrfs_set_root_last_snapshot(&root->root_item, trans->transid);\n\tmemcpy(new_root_item, &root->root_item, sizeof(*new_root_item));\n\tbtrfs_check_and_init_root_item(new_root_item);\n\n\troot_flags = btrfs_root_flags(new_root_item);\n\tif (pending->readonly)\n\t\troot_flags |= BTRFS_ROOT_SUBVOL_RDONLY;\n\telse\n\t\troot_flags &= ~BTRFS_ROOT_SUBVOL_RDONLY;\n\tbtrfs_set_root_flags(new_root_item, root_flags);\n\n\tbtrfs_set_root_generation_v2(new_root_item,\n\t\t\ttrans->transid);\n\tuuid_le_gen(&new_uuid);\n\tmemcpy(new_root_item->uuid, new_uuid.b, BTRFS_UUID_SIZE);\n\tmemcpy(new_root_item->parent_uuid, root->root_item.uuid,\n\t\t\tBTRFS_UUID_SIZE);\n\tnew_root_item->otime.sec = cpu_to_le64(cur_time.tv_sec);\n\tnew_root_item->otime.nsec = cpu_to_le32(cur_time.tv_nsec);\n\tbtrfs_set_root_otransid(new_root_item, trans->transid);\n\tmemset(&new_root_item->stime, 0, sizeof(new_root_item->stime));\n\tmemset(&new_root_item->rtime, 0, sizeof(new_root_item->rtime));\n\tbtrfs_set_root_stransid(new_root_item, 0);\n\tbtrfs_set_root_rtransid(new_root_item, 0);\n\n\told = btrfs_lock_root_node(root);\n\tret = btrfs_cow_block(trans, root, old, NULL, 0, &old);\n\tif (ret) {\n\t\tbtrfs_tree_unlock(old);\n\t\tfree_extent_buffer(old);\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_set_lock_blocking(old);\n\n\tret = btrfs_copy_root(trans, root, old, &tmp, objectid);\n\t/* clean up in any case */\n\tbtrfs_tree_unlock(old);\n\tfree_extent_buffer(old);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\t/* see comments in should_cow_block() */\n\troot->force_cow = 1;\n\tsmp_wmb();\n\n\tbtrfs_set_root_node(new_root_item, tmp);\n\t/* record when the snapshot was created in key.offset */\n\tkey.offset = trans->transid;\n\tret = btrfs_insert_root(trans, tree_root, &key, new_root_item);\n\tbtrfs_tree_unlock(tmp);\n\tfree_extent_buffer(tmp);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * insert root back/forward references\n\t */\n\tret = btrfs_add_root_ref(trans, tree_root, objectid,\n\t\t\t\t parent_root->root_key.objectid,\n\t\t\t\t btrfs_ino(parent_inode), index,\n\t\t\t\t dentry->d_name.name, dentry->d_name.len);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tkey.offset = (u64)-1;\n\tpending->snap = btrfs_read_fs_root_no_name(root->fs_info, &key);\n\tif (IS_ERR(pending->snap)) {\n\t\tret = PTR_ERR(pending->snap);\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_reloc_post_snapshot(trans, pending);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_insert_dir_item(trans, parent_root,\n\t\t\t\t    dentry->d_name.name, dentry->d_name.len,\n\t\t\t\t    parent_inode, &key,\n\t\t\t\t    BTRFS_FT_DIR, index);\n\t/* We have check then name at the beginning, so it is impossible. */\n\tBUG_ON(ret == -EEXIST);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_i_size_write(parent_inode, parent_inode->i_size +\n\t\t\t\t\t dentry->d_name.len * 2);\n\tparent_inode->i_mtime = parent_inode->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode_fallback(trans, parent_root, parent_inode);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, root, ret);\nfail:\n\tdput(parent);\n\ttrans->block_rsv = rsv;\nno_free_objectid:\n\tkfree(new_root_item);\nroot_item_alloc_fail:\n\tbtrfs_free_path(path);\npath_alloc_fail:\n\tbtrfs_block_rsv_release(root, &pending->block_rsv, (u64)-1);\n\treturn ret;\n}\n\n/*\n * create all the snapshots we've scheduled for creation\n */\nstatic noinline int create_pending_snapshots(struct btrfs_trans_handle *trans,\n\t\t\t\t\t     struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_pending_snapshot *pending;\n\tstruct list_head *head = &trans->transaction->pending_snapshots;\n\n\tlist_for_each_entry(pending, head, list)\n\t\tcreate_pending_snapshot(trans, fs_info, pending);\n\treturn 0;\n}\n\nstatic void update_super_roots(struct btrfs_root *root)\n{\n\tstruct btrfs_root_item *root_item;\n\tstruct btrfs_super_block *super;\n\n\tsuper = root->fs_info->super_copy;\n\n\troot_item = &root->fs_info->chunk_root->root_item;\n\tsuper->chunk_root = root_item->bytenr;\n\tsuper->chunk_root_generation = root_item->generation;\n\tsuper->chunk_root_level = root_item->level;\n\n\troot_item = &root->fs_info->tree_root->root_item;\n\tsuper->root = root_item->bytenr;\n\tsuper->generation = root_item->generation;\n\tsuper->root_level = root_item->level;\n\tif (btrfs_test_opt(root, SPACE_CACHE))\n\t\tsuper->cache_generation = root_item->generation;\n}\n\nint btrfs_transaction_in_commit(struct btrfs_fs_info *info)\n{\n\tint ret = 0;\n\tspin_lock(&info->trans_lock);\n\tif (info->running_transaction)\n\t\tret = info->running_transaction->in_commit;\n\tspin_unlock(&info->trans_lock);\n\treturn ret;\n}\n\nint btrfs_transaction_blocked(struct btrfs_fs_info *info)\n{\n\tint ret = 0;\n\tspin_lock(&info->trans_lock);\n\tif (info->running_transaction)\n\t\tret = info->running_transaction->blocked;\n\tspin_unlock(&info->trans_lock);\n\treturn ret;\n}\n\n/*\n * wait for the current transaction commit to start and block subsequent\n * transaction joins\n */\nstatic void wait_current_trans_commit_start(struct btrfs_root *root,\n\t\t\t\t\t    struct btrfs_transaction *trans)\n{\n\twait_event(root->fs_info->transaction_blocked_wait, trans->in_commit);\n}\n\n/*\n * wait for the current transaction to start and then become unblocked.\n * caller holds ref.\n */\nstatic void wait_current_trans_commit_start_and_unblock(struct btrfs_root *root,\n\t\t\t\t\t struct btrfs_transaction *trans)\n{\n\twait_event(root->fs_info->transaction_wait,\n\t\t   trans->commit_done || (trans->in_commit && !trans->blocked));\n}\n\n/*\n * commit transactions asynchronously. once btrfs_commit_transaction_async\n * returns, any subsequent transaction will not be allowed to join.\n */\nstruct btrfs_async_commit {\n\tstruct btrfs_trans_handle *newtrans;\n\tstruct btrfs_root *root;\n\tstruct delayed_work work;\n};\n\nstatic void do_async_commit(struct work_struct *work)\n{\n\tstruct btrfs_async_commit *ac =\n\t\tcontainer_of(work, struct btrfs_async_commit, work.work);\n\n\t/*\n\t * We've got freeze protection passed with the transaction.\n\t * Tell lockdep about it.\n\t */\n\tif (ac->newtrans->type < TRANS_JOIN_NOLOCK)\n\t\trwsem_acquire_read(\n\t\t     &ac->root->fs_info->sb->s_writers.lock_map[SB_FREEZE_FS-1],\n\t\t     0, 1, _THIS_IP_);\n\n\tcurrent->journal_info = ac->newtrans;\n\n\tbtrfs_commit_transaction(ac->newtrans, ac->root);\n\tkfree(ac);\n}\n\nint btrfs_commit_transaction_async(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t   int wait_for_unblock)\n{\n\tstruct btrfs_async_commit *ac;\n\tstruct btrfs_transaction *cur_trans;\n\n\tac = kmalloc(sizeof(*ac), GFP_NOFS);\n\tif (!ac)\n\t\treturn -ENOMEM;\n\n\tINIT_DELAYED_WORK(&ac->work, do_async_commit);\n\tac->root = root;\n\tac->newtrans = btrfs_join_transaction(root);\n\tif (IS_ERR(ac->newtrans)) {\n\t\tint err = PTR_ERR(ac->newtrans);\n\t\tkfree(ac);\n\t\treturn err;\n\t}\n\n\t/* take transaction reference */\n\tcur_trans = trans->transaction;\n\tatomic_inc(&cur_trans->use_count);\n\n\tbtrfs_end_transaction(trans, root);\n\n\t/*\n\t * Tell lockdep we've released the freeze rwsem, since the\n\t * async commit thread will be the one to unlock it.\n\t */\n\tif (trans->type < TRANS_JOIN_NOLOCK)\n\t\trwsem_release(\n\t\t\t&root->fs_info->sb->s_writers.lock_map[SB_FREEZE_FS-1],\n\t\t\t1, _THIS_IP_);\n\n\tschedule_delayed_work(&ac->work, 0);\n\n\t/* wait for transaction to start and unblock */\n\tif (wait_for_unblock)\n\t\twait_current_trans_commit_start_and_unblock(root, cur_trans);\n\telse\n\t\twait_current_trans_commit_start(root, cur_trans);\n\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\n\tput_transaction(cur_trans);\n\treturn 0;\n}\n\n\nstatic void cleanup_transaction(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, int err)\n{\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\n\tWARN_ON(trans->use_count > 1);\n\n\tbtrfs_abort_transaction(trans, root, err);\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tlist_del_init(&cur_trans->list);\n\tif (cur_trans == root->fs_info->running_transaction) {\n\t\troot->fs_info->running_transaction = NULL;\n\t\troot->fs_info->trans_no_join = 0;\n\t}\n\tspin_unlock(&root->fs_info->trans_lock);\n\n\tbtrfs_cleanup_one_transaction(trans->transaction, root);\n\n\tput_transaction(cur_trans);\n\tput_transaction(cur_trans);\n\n\ttrace_btrfs_transaction_commit(root);\n\n\tbtrfs_scrub_continue(root);\n\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\n\tkmem_cache_free(btrfs_trans_handle_cachep, trans);\n}\n\nstatic int btrfs_flush_all_pending_stuffs(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root)\n{\n\tint flush_on_commit = btrfs_test_opt(root, FLUSHONCOMMIT);\n\tint snap_pending = 0;\n\tint ret;\n\n\tif (!flush_on_commit) {\n\t\tspin_lock(&root->fs_info->trans_lock);\n\t\tif (!list_empty(&trans->transaction->pending_snapshots))\n\t\t\tsnap_pending = 1;\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t}\n\n\tif (flush_on_commit || snap_pending) {\n\t\tbtrfs_start_delalloc_inodes(root, 1);\n\t\tbtrfs_wait_ordered_extents(root, 1);\n\t}\n\n\tret = btrfs_run_delayed_items(trans, root);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * running the delayed items may have added new refs. account\n\t * them now so that they hinder processing of more delayed refs\n\t * as little as possible.\n\t */\n\tbtrfs_delayed_refs_qgroup_accounting(trans, root->fs_info);\n\n\t/*\n\t * rename don't use btrfs_join_transaction, so, once we\n\t * set the transaction to blocked above, we aren't going\n\t * to get any new ordered operations.  We can safely run\n\t * it here and no for sure that nothing new will be added\n\t * to the list\n\t */\n\tbtrfs_run_ordered_operations(root, 1);\n\n\treturn 0;\n}\n\n/*\n * btrfs_transaction state sequence:\n *    in_commit = 0, blocked = 0  (initial)\n *    in_commit = 1, blocked = 1\n *    blocked = 0\n *    commit_done = 1\n */\nint btrfs_commit_transaction(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root)\n{\n\tunsigned long joined = 0;\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tstruct btrfs_transaction *prev_trans = NULL;\n\tDEFINE_WAIT(wait);\n\tint ret;\n\tint should_grow = 0;\n\tunsigned long now = get_seconds();\n\n\tret = btrfs_run_ordered_operations(root, 0);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto cleanup_transaction;\n\t}\n\n\tif (cur_trans->aborted) {\n\t\tret = cur_trans->aborted;\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/* make a pass through all the delayed refs we have so far\n\t * any runnings procs may add more while we are here\n\t */\n\tret = btrfs_run_delayed_refs(trans, root, 0);\n\tif (ret)\n\t\tgoto cleanup_transaction;\n\n\tbtrfs_trans_release_metadata(trans, root);\n\ttrans->block_rsv = NULL;\n\n\tcur_trans = trans->transaction;\n\n\t/*\n\t * set the flushing flag so procs in this transaction have to\n\t * start sending their work down.\n\t */\n\tcur_trans->delayed_refs.flushing = 1;\n\n\tif (!list_empty(&trans->new_bgs))\n\t\tbtrfs_create_pending_block_groups(trans, root);\n\n\tret = btrfs_run_delayed_refs(trans, root, 0);\n\tif (ret)\n\t\tgoto cleanup_transaction;\n\n\tspin_lock(&cur_trans->commit_lock);\n\tif (cur_trans->in_commit) {\n\t\tspin_unlock(&cur_trans->commit_lock);\n\t\tatomic_inc(&cur_trans->use_count);\n\t\tret = btrfs_end_transaction(trans, root);\n\n\t\twait_for_commit(root, cur_trans);\n\n\t\tput_transaction(cur_trans);\n\n\t\treturn ret;\n\t}\n\n\ttrans->transaction->in_commit = 1;\n\ttrans->transaction->blocked = 1;\n\tspin_unlock(&cur_trans->commit_lock);\n\twake_up(&root->fs_info->transaction_blocked_wait);\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tif (cur_trans->list.prev != &root->fs_info->trans_list) {\n\t\tprev_trans = list_entry(cur_trans->list.prev,\n\t\t\t\t\tstruct btrfs_transaction, list);\n\t\tif (!prev_trans->commit_done) {\n\t\t\tatomic_inc(&prev_trans->use_count);\n\t\t\tspin_unlock(&root->fs_info->trans_lock);\n\n\t\t\twait_for_commit(root, prev_trans);\n\n\t\t\tput_transaction(prev_trans);\n\t\t} else {\n\t\t\tspin_unlock(&root->fs_info->trans_lock);\n\t\t}\n\t} else {\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t}\n\n\tif (!btrfs_test_opt(root, SSD) &&\n\t    (now < cur_trans->start_time || now - cur_trans->start_time < 1))\n\t\tshould_grow = 1;\n\n\tdo {\n\t\tjoined = cur_trans->num_joined;\n\n\t\tWARN_ON(cur_trans != trans->transaction);\n\n\t\tret = btrfs_flush_all_pending_stuffs(trans, root);\n\t\tif (ret)\n\t\t\tgoto cleanup_transaction;\n\n\t\tprepare_to_wait(&cur_trans->writer_wait, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\tif (atomic_read(&cur_trans->num_writers) > 1)\n\t\t\tschedule_timeout(MAX_SCHEDULE_TIMEOUT);\n\t\telse if (should_grow)\n\t\t\tschedule_timeout(1);\n\n\t\tfinish_wait(&cur_trans->writer_wait, &wait);\n\t} while (atomic_read(&cur_trans->num_writers) > 1 ||\n\t\t (should_grow && cur_trans->num_joined != joined));\n\n\tret = btrfs_flush_all_pending_stuffs(trans, root);\n\tif (ret)\n\t\tgoto cleanup_transaction;\n\n\t/*\n\t * Ok now we need to make sure to block out any other joins while we\n\t * commit the transaction.  We could have started a join before setting\n\t * no_join so make sure to wait for num_writers to == 1 again.\n\t */\n\tspin_lock(&root->fs_info->trans_lock);\n\troot->fs_info->trans_no_join = 1;\n\tspin_unlock(&root->fs_info->trans_lock);\n\twait_event(cur_trans->writer_wait,\n\t\t   atomic_read(&cur_trans->num_writers) == 1);\n\n\t/*\n\t * the reloc mutex makes sure that we stop\n\t * the balancing code from coming in and moving\n\t * extents around in the middle of the commit\n\t */\n\tmutex_lock(&root->fs_info->reloc_mutex);\n\n\t/*\n\t * We needn't worry about the delayed items because we will\n\t * deal with them in create_pending_snapshot(), which is the\n\t * core function of the snapshot creation.\n\t */\n\tret = create_pending_snapshots(trans, root->fs_info);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/*\n\t * We insert the dir indexes of the snapshots and update the inode\n\t * of the snapshots' parents after the snapshot creation, so there\n\t * are some delayed items which are not dealt with. Now deal with\n\t * them.\n\t *\n\t * We needn't worry that this operation will corrupt the snapshots,\n\t * because all the tree which are snapshoted will be forced to COW\n\t * the nodes and leaves.\n\t */\n\tret = btrfs_run_delayed_items(trans, root);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/*\n\t * make sure none of the code above managed to slip in a\n\t * delayed item\n\t */\n\tbtrfs_assert_delayed_root_empty(root);\n\n\tWARN_ON(cur_trans != trans->transaction);\n\n\tbtrfs_scrub_pause(root);\n\t/* btrfs_commit_tree_roots is responsible for getting the\n\t * various roots consistent with each other.  Every pointer\n\t * in the tree of tree roots has to point to the most up to date\n\t * root for every subvolume and other tree.  So, we have to keep\n\t * the tree logging code from jumping in and changing any\n\t * of the trees.\n\t *\n\t * At this point in the commit, there can't be any tree-log\n\t * writers, but a little lower down we drop the trans mutex\n\t * and let new people in.  By holding the tree_log_mutex\n\t * from now until after the super is written, we avoid races\n\t * with the tree-log code.\n\t */\n\tmutex_lock(&root->fs_info->tree_log_mutex);\n\n\tret = commit_fs_roots(trans, root);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/* commit_fs_roots gets rid of all the tree log roots, it is now\n\t * safe to free the root of tree log roots\n\t */\n\tbtrfs_free_log_root_tree(trans, root->fs_info);\n\n\tret = commit_cowonly_roots(trans, root);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\tbtrfs_prepare_extent_commit(trans, root);\n\n\tcur_trans = root->fs_info->running_transaction;\n\n\tbtrfs_set_root_node(&root->fs_info->tree_root->root_item,\n\t\t\t    root->fs_info->tree_root->node);\n\tswitch_commit_root(root->fs_info->tree_root);\n\n\tbtrfs_set_root_node(&root->fs_info->chunk_root->root_item,\n\t\t\t    root->fs_info->chunk_root->node);\n\tswitch_commit_root(root->fs_info->chunk_root);\n\n\tassert_qgroups_uptodate(trans);\n\tupdate_super_roots(root);\n\n\tif (!root->fs_info->log_root_recovering) {\n\t\tbtrfs_set_super_log_root(root->fs_info->super_copy, 0);\n\t\tbtrfs_set_super_log_root_level(root->fs_info->super_copy, 0);\n\t}\n\n\tmemcpy(root->fs_info->super_for_commit, root->fs_info->super_copy,\n\t       sizeof(*root->fs_info->super_copy));\n\n\ttrans->transaction->blocked = 0;\n\tspin_lock(&root->fs_info->trans_lock);\n\troot->fs_info->running_transaction = NULL;\n\troot->fs_info->trans_no_join = 0;\n\tspin_unlock(&root->fs_info->trans_lock);\n\tmutex_unlock(&root->fs_info->reloc_mutex);\n\n\twake_up(&root->fs_info->transaction_wait);\n\n\tret = btrfs_write_and_wait_transaction(trans, root);\n\tif (ret) {\n\t\tbtrfs_error(root->fs_info, ret,\n\t\t\t    \"Error while writing out transaction.\");\n\t\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\tret = write_ctree_super(trans, root, 0);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/*\n\t * the super is written, we can safely allow the tree-loggers\n\t * to go about their business\n\t */\n\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\n\tbtrfs_finish_extent_commit(trans, root);\n\n\tcur_trans->commit_done = 1;\n\n\troot->fs_info->last_trans_committed = cur_trans->transid;\n\n\twake_up(&cur_trans->commit_wait);\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tlist_del_init(&cur_trans->list);\n\tspin_unlock(&root->fs_info->trans_lock);\n\n\tput_transaction(cur_trans);\n\tput_transaction(cur_trans);\n\n\tif (trans->type < TRANS_JOIN_NOLOCK)\n\t\tsb_end_intwrite(root->fs_info->sb);\n\n\ttrace_btrfs_transaction_commit(root);\n\n\tbtrfs_scrub_continue(root);\n\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\n\tkmem_cache_free(btrfs_trans_handle_cachep, trans);\n\n\tif (current != root->fs_info->transaction_kthread)\n\t\tbtrfs_run_delayed_iputs(root);\n\n\treturn ret;\n\ncleanup_transaction:\n\tbtrfs_trans_release_metadata(trans, root);\n\ttrans->block_rsv = NULL;\n\tbtrfs_printk(root->fs_info, \"Skipping commit of aborted transaction.\\n\");\n//\tWARN_ON(1);\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\tcleanup_transaction(trans, root, ret);\n\n\treturn ret;\n}\n\n/*\n * interface function to delete all the snapshots we have scheduled for deletion\n */\nint btrfs_clean_old_snapshots(struct btrfs_root *root)\n{\n\tLIST_HEAD(list);\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\n\tspin_lock(&fs_info->trans_lock);\n\tlist_splice_init(&fs_info->dead_roots, &list);\n\tspin_unlock(&fs_info->trans_lock);\n\n\twhile (!list_empty(&list)) {\n\t\tint ret;\n\n\t\troot = list_entry(list.next, struct btrfs_root, root_list);\n\t\tlist_del(&root->root_list);\n\n\t\tbtrfs_kill_all_delayed_nodes(root);\n\n\t\tif (btrfs_header_backref_rev(root->node) <\n\t\t    BTRFS_MIXED_BACKREF_REV)\n\t\t\tret = btrfs_drop_snapshot(root, NULL, 0, 0);\n\t\telse\n\t\t\tret =btrfs_drop_snapshot(root, NULL, 1, 0);\n\t\tBUG_ON(ret < 0);\n\t}\n\treturn 0;\n}\n"], "fixing_code": ["/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#ifndef __BTRFS_CTREE__\n#define __BTRFS_CTREE__\n\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/fs.h>\n#include <linux/rwsem.h>\n#include <linux/completion.h>\n#include <linux/backing-dev.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n#include <linux/kobject.h>\n#include <trace/events/btrfs.h>\n#include <asm/kmap_types.h>\n#include <linux/pagemap.h>\n#include \"extent_io.h\"\n#include \"extent_map.h\"\n#include \"async-thread.h\"\n#include \"ioctl.h\"\n\nstruct btrfs_trans_handle;\nstruct btrfs_transaction;\nstruct btrfs_pending_snapshot;\nextern struct kmem_cache *btrfs_trans_handle_cachep;\nextern struct kmem_cache *btrfs_transaction_cachep;\nextern struct kmem_cache *btrfs_bit_radix_cachep;\nextern struct kmem_cache *btrfs_path_cachep;\nextern struct kmem_cache *btrfs_free_space_cachep;\nstruct btrfs_ordered_sum;\n\n#define BTRFS_MAGIC \"_BHRfS_M\"\n\n#define BTRFS_MAX_MIRRORS 3\n\n#define BTRFS_MAX_LEVEL 8\n\n#define BTRFS_COMPAT_EXTENT_TREE_V0\n\n/*\n * files bigger than this get some pre-flushing when they are added\n * to the ordered operations list.  That way we limit the total\n * work done by the commit\n */\n#define BTRFS_ORDERED_OPERATIONS_FLUSH_LIMIT (8 * 1024 * 1024)\n\n/* holds pointers to all of the tree roots */\n#define BTRFS_ROOT_TREE_OBJECTID 1ULL\n\n/* stores information about which extents are in use, and reference counts */\n#define BTRFS_EXTENT_TREE_OBJECTID 2ULL\n\n/*\n * chunk tree stores translations from logical -> physical block numbering\n * the super block points to the chunk tree\n */\n#define BTRFS_CHUNK_TREE_OBJECTID 3ULL\n\n/*\n * stores information about which areas of a given device are in use.\n * one per device.  The tree of tree roots points to the device tree\n */\n#define BTRFS_DEV_TREE_OBJECTID 4ULL\n\n/* one per subvolume, storing files and directories */\n#define BTRFS_FS_TREE_OBJECTID 5ULL\n\n/* directory objectid inside the root tree */\n#define BTRFS_ROOT_TREE_DIR_OBJECTID 6ULL\n\n/* holds checksums of all the data extents */\n#define BTRFS_CSUM_TREE_OBJECTID 7ULL\n\n/* for storing balance parameters in the root tree */\n#define BTRFS_BALANCE_OBJECTID -4ULL\n\n/* holds quota configuration and tracking */\n#define BTRFS_QUOTA_TREE_OBJECTID 8ULL\n\n/* orhpan objectid for tracking unlinked/truncated files */\n#define BTRFS_ORPHAN_OBJECTID -5ULL\n\n/* does write ahead logging to speed up fsyncs */\n#define BTRFS_TREE_LOG_OBJECTID -6ULL\n#define BTRFS_TREE_LOG_FIXUP_OBJECTID -7ULL\n\n/* for space balancing */\n#define BTRFS_TREE_RELOC_OBJECTID -8ULL\n#define BTRFS_DATA_RELOC_TREE_OBJECTID -9ULL\n\n/*\n * extent checksums all have this objectid\n * this allows them to share the logging tree\n * for fsyncs\n */\n#define BTRFS_EXTENT_CSUM_OBJECTID -10ULL\n\n/* For storing free space cache */\n#define BTRFS_FREE_SPACE_OBJECTID -11ULL\n\n/*\n * The inode number assigned to the special inode for storing\n * free ino cache\n */\n#define BTRFS_FREE_INO_OBJECTID -12ULL\n\n/* dummy objectid represents multiple objectids */\n#define BTRFS_MULTIPLE_OBJECTIDS -255ULL\n\n/*\n * All files have objectids in this range.\n */\n#define BTRFS_FIRST_FREE_OBJECTID 256ULL\n#define BTRFS_LAST_FREE_OBJECTID -256ULL\n#define BTRFS_FIRST_CHUNK_TREE_OBJECTID 256ULL\n\n\n/*\n * the device items go into the chunk tree.  The key is in the form\n * [ 1 BTRFS_DEV_ITEM_KEY device_id ]\n */\n#define BTRFS_DEV_ITEMS_OBJECTID 1ULL\n\n#define BTRFS_BTREE_INODE_OBJECTID 1\n\n#define BTRFS_EMPTY_SUBVOL_DIR_OBJECTID 2\n\n#define BTRFS_DEV_REPLACE_DEVID 0\n\n/*\n * the max metadata block size.  This limit is somewhat artificial,\n * but the memmove costs go through the roof for larger blocks.\n */\n#define BTRFS_MAX_METADATA_BLOCKSIZE 65536\n\n/*\n * we can actually store much bigger names, but lets not confuse the rest\n * of linux\n */\n#define BTRFS_NAME_LEN 255\n\n/*\n * Theoretical limit is larger, but we keep this down to a sane\n * value. That should limit greatly the possibility of collisions on\n * inode ref items.\n */\n#define BTRFS_LINK_MAX 65535U\n\n/* 32 bytes in various csum fields */\n#define BTRFS_CSUM_SIZE 32\n\n/* csum types */\n#define BTRFS_CSUM_TYPE_CRC32\t0\n\nstatic int btrfs_csum_sizes[] = { 4, 0 };\n\n/* four bytes for CRC32 */\n#define BTRFS_EMPTY_DIR_SIZE 0\n\n/* spefic to btrfs_map_block(), therefore not in include/linux/blk_types.h */\n#define REQ_GET_READ_MIRRORS\t(1 << 30)\n\n#define BTRFS_FT_UNKNOWN\t0\n#define BTRFS_FT_REG_FILE\t1\n#define BTRFS_FT_DIR\t\t2\n#define BTRFS_FT_CHRDEV\t\t3\n#define BTRFS_FT_BLKDEV\t\t4\n#define BTRFS_FT_FIFO\t\t5\n#define BTRFS_FT_SOCK\t\t6\n#define BTRFS_FT_SYMLINK\t7\n#define BTRFS_FT_XATTR\t\t8\n#define BTRFS_FT_MAX\t\t9\n\n/* ioprio of readahead is set to idle */\n#define BTRFS_IOPRIO_READA (IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0))\n\n/*\n * The key defines the order in the tree, and so it also defines (optimal)\n * block layout.\n *\n * objectid corresponds to the inode number.\n *\n * type tells us things about the object, and is a kind of stream selector.\n * so for a given inode, keys with type of 1 might refer to the inode data,\n * type of 2 may point to file data in the btree and type == 3 may point to\n * extents.\n *\n * offset is the starting byte offset for this key in the stream.\n *\n * btrfs_disk_key is in disk byte order.  struct btrfs_key is always\n * in cpu native order.  Otherwise they are identical and their sizes\n * should be the same (ie both packed)\n */\nstruct btrfs_disk_key {\n\t__le64 objectid;\n\tu8 type;\n\t__le64 offset;\n} __attribute__ ((__packed__));\n\nstruct btrfs_key {\n\tu64 objectid;\n\tu8 type;\n\tu64 offset;\n} __attribute__ ((__packed__));\n\nstruct btrfs_mapping_tree {\n\tstruct extent_map_tree map_tree;\n};\n\nstruct btrfs_dev_item {\n\t/* the internal btrfs device id */\n\t__le64 devid;\n\n\t/* size of the device */\n\t__le64 total_bytes;\n\n\t/* bytes used */\n\t__le64 bytes_used;\n\n\t/* optimal io alignment for this device */\n\t__le32 io_align;\n\n\t/* optimal io width for this device */\n\t__le32 io_width;\n\n\t/* minimal io size for this device */\n\t__le32 sector_size;\n\n\t/* type and info about this device */\n\t__le64 type;\n\n\t/* expected generation for this device */\n\t__le64 generation;\n\n\t/*\n\t * starting byte of this partition on the device,\n\t * to allow for stripe alignment in the future\n\t */\n\t__le64 start_offset;\n\n\t/* grouping information for allocation decisions */\n\t__le32 dev_group;\n\n\t/* seek speed 0-100 where 100 is fastest */\n\tu8 seek_speed;\n\n\t/* bandwidth 0-100 where 100 is fastest */\n\tu8 bandwidth;\n\n\t/* btrfs generated uuid for this device */\n\tu8 uuid[BTRFS_UUID_SIZE];\n\n\t/* uuid of FS who owns this device */\n\tu8 fsid[BTRFS_UUID_SIZE];\n} __attribute__ ((__packed__));\n\nstruct btrfs_stripe {\n\t__le64 devid;\n\t__le64 offset;\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n} __attribute__ ((__packed__));\n\nstruct btrfs_chunk {\n\t/* size of this chunk in bytes */\n\t__le64 length;\n\n\t/* objectid of the root referencing this chunk */\n\t__le64 owner;\n\n\t__le64 stripe_len;\n\t__le64 type;\n\n\t/* optimal io alignment for this chunk */\n\t__le32 io_align;\n\n\t/* optimal io width for this chunk */\n\t__le32 io_width;\n\n\t/* minimal io size for this chunk */\n\t__le32 sector_size;\n\n\t/* 2^16 stripes is quite a lot, a second limit is the size of a single\n\t * item in the btree\n\t */\n\t__le16 num_stripes;\n\n\t/* sub stripes only matter for raid10 */\n\t__le16 sub_stripes;\n\tstruct btrfs_stripe stripe;\n\t/* additional stripes go here */\n} __attribute__ ((__packed__));\n\n#define BTRFS_FREE_SPACE_EXTENT\t1\n#define BTRFS_FREE_SPACE_BITMAP\t2\n\nstruct btrfs_free_space_entry {\n\t__le64 offset;\n\t__le64 bytes;\n\tu8 type;\n} __attribute__ ((__packed__));\n\nstruct btrfs_free_space_header {\n\tstruct btrfs_disk_key location;\n\t__le64 generation;\n\t__le64 num_entries;\n\t__le64 num_bitmaps;\n} __attribute__ ((__packed__));\n\nstatic inline unsigned long btrfs_chunk_item_size(int num_stripes)\n{\n\tBUG_ON(num_stripes == 0);\n\treturn sizeof(struct btrfs_chunk) +\n\t\tsizeof(struct btrfs_stripe) * (num_stripes - 1);\n}\n\n#define BTRFS_HEADER_FLAG_WRITTEN\t(1ULL << 0)\n#define BTRFS_HEADER_FLAG_RELOC\t\t(1ULL << 1)\n\n/*\n * File system states\n */\n\n/* Errors detected */\n#define BTRFS_SUPER_FLAG_ERROR\t\t(1ULL << 2)\n\n#define BTRFS_SUPER_FLAG_SEEDING\t(1ULL << 32)\n#define BTRFS_SUPER_FLAG_METADUMP\t(1ULL << 33)\n\n#define BTRFS_BACKREF_REV_MAX\t\t256\n#define BTRFS_BACKREF_REV_SHIFT\t\t56\n#define BTRFS_BACKREF_REV_MASK\t\t(((u64)BTRFS_BACKREF_REV_MAX - 1) << \\\n\t\t\t\t\t BTRFS_BACKREF_REV_SHIFT)\n\n#define BTRFS_OLD_BACKREF_REV\t\t0\n#define BTRFS_MIXED_BACKREF_REV\t\t1\n\n/*\n * every tree block (leaf or node) starts with this header.\n */\nstruct btrfs_header {\n\t/* these first four must match the super block */\n\tu8 csum[BTRFS_CSUM_SIZE];\n\tu8 fsid[BTRFS_FSID_SIZE]; /* FS specific uuid */\n\t__le64 bytenr; /* which block this node is supposed to live in */\n\t__le64 flags;\n\n\t/* allowed to be different from the super from here on down */\n\tu8 chunk_tree_uuid[BTRFS_UUID_SIZE];\n\t__le64 generation;\n\t__le64 owner;\n\t__le32 nritems;\n\tu8 level;\n} __attribute__ ((__packed__));\n\n#define BTRFS_NODEPTRS_PER_BLOCK(r) (((r)->nodesize - \\\n\t\t\t\t      sizeof(struct btrfs_header)) / \\\n\t\t\t\t     sizeof(struct btrfs_key_ptr))\n#define __BTRFS_LEAF_DATA_SIZE(bs) ((bs) - sizeof(struct btrfs_header))\n#define BTRFS_LEAF_DATA_SIZE(r) (__BTRFS_LEAF_DATA_SIZE(r->leafsize))\n#define BTRFS_MAX_INLINE_DATA_SIZE(r) (BTRFS_LEAF_DATA_SIZE(r) - \\\n\t\t\t\t\tsizeof(struct btrfs_item) - \\\n\t\t\t\t\tsizeof(struct btrfs_file_extent_item))\n#define BTRFS_MAX_XATTR_SIZE(r)\t(BTRFS_LEAF_DATA_SIZE(r) - \\\n\t\t\t\t sizeof(struct btrfs_item) -\\\n\t\t\t\t sizeof(struct btrfs_dir_item))\n\n\n/*\n * this is a very generous portion of the super block, giving us\n * room to translate 14 chunks with 3 stripes each.\n */\n#define BTRFS_SYSTEM_CHUNK_ARRAY_SIZE 2048\n#define BTRFS_LABEL_SIZE 256\n\n/*\n * just in case we somehow lose the roots and are not able to mount,\n * we store an array of the roots from previous transactions\n * in the super.\n */\n#define BTRFS_NUM_BACKUP_ROOTS 4\nstruct btrfs_root_backup {\n\t__le64 tree_root;\n\t__le64 tree_root_gen;\n\n\t__le64 chunk_root;\n\t__le64 chunk_root_gen;\n\n\t__le64 extent_root;\n\t__le64 extent_root_gen;\n\n\t__le64 fs_root;\n\t__le64 fs_root_gen;\n\n\t__le64 dev_root;\n\t__le64 dev_root_gen;\n\n\t__le64 csum_root;\n\t__le64 csum_root_gen;\n\n\t__le64 total_bytes;\n\t__le64 bytes_used;\n\t__le64 num_devices;\n\t/* future */\n\t__le64 unused_64[4];\n\n\tu8 tree_root_level;\n\tu8 chunk_root_level;\n\tu8 extent_root_level;\n\tu8 fs_root_level;\n\tu8 dev_root_level;\n\tu8 csum_root_level;\n\t/* future and to align */\n\tu8 unused_8[10];\n} __attribute__ ((__packed__));\n\n/*\n * the super block basically lists the main trees of the FS\n * it currently lacks any block count etc etc\n */\nstruct btrfs_super_block {\n\tu8 csum[BTRFS_CSUM_SIZE];\n\t/* the first 4 fields must match struct btrfs_header */\n\tu8 fsid[BTRFS_FSID_SIZE];    /* FS specific uuid */\n\t__le64 bytenr; /* this block number */\n\t__le64 flags;\n\n\t/* allowed to be different from the btrfs_header from here own down */\n\t__le64 magic;\n\t__le64 generation;\n\t__le64 root;\n\t__le64 chunk_root;\n\t__le64 log_root;\n\n\t/* this will help find the new super based on the log root */\n\t__le64 log_root_transid;\n\t__le64 total_bytes;\n\t__le64 bytes_used;\n\t__le64 root_dir_objectid;\n\t__le64 num_devices;\n\t__le32 sectorsize;\n\t__le32 nodesize;\n\t__le32 leafsize;\n\t__le32 stripesize;\n\t__le32 sys_chunk_array_size;\n\t__le64 chunk_root_generation;\n\t__le64 compat_flags;\n\t__le64 compat_ro_flags;\n\t__le64 incompat_flags;\n\t__le16 csum_type;\n\tu8 root_level;\n\tu8 chunk_root_level;\n\tu8 log_root_level;\n\tstruct btrfs_dev_item dev_item;\n\n\tchar label[BTRFS_LABEL_SIZE];\n\n\t__le64 cache_generation;\n\n\t/* future expansion */\n\t__le64 reserved[31];\n\tu8 sys_chunk_array[BTRFS_SYSTEM_CHUNK_ARRAY_SIZE];\n\tstruct btrfs_root_backup super_roots[BTRFS_NUM_BACKUP_ROOTS];\n} __attribute__ ((__packed__));\n\n/*\n * Compat flags that we support.  If any incompat flags are set other than the\n * ones specified below then we will fail to mount\n */\n#define BTRFS_FEATURE_INCOMPAT_MIXED_BACKREF\t(1ULL << 0)\n#define BTRFS_FEATURE_INCOMPAT_DEFAULT_SUBVOL\t(1ULL << 1)\n#define BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS\t(1ULL << 2)\n#define BTRFS_FEATURE_INCOMPAT_COMPRESS_LZO\t(1ULL << 3)\n/*\n * some patches floated around with a second compression method\n * lets save that incompat here for when they do get in\n * Note we don't actually support it, we're just reserving the\n * number\n */\n#define BTRFS_FEATURE_INCOMPAT_COMPRESS_LZOv2\t(1ULL << 4)\n\n/*\n * older kernels tried to do bigger metadata blocks, but the\n * code was pretty buggy.  Lets not let them try anymore.\n */\n#define BTRFS_FEATURE_INCOMPAT_BIG_METADATA\t(1ULL << 5)\n\n#define BTRFS_FEATURE_INCOMPAT_EXTENDED_IREF\t(1ULL << 6)\n\n#define BTRFS_FEATURE_COMPAT_SUPP\t\t0ULL\n#define BTRFS_FEATURE_COMPAT_RO_SUPP\t\t0ULL\n#define BTRFS_FEATURE_INCOMPAT_SUPP\t\t\t\\\n\t(BTRFS_FEATURE_INCOMPAT_MIXED_BACKREF |\t\t\\\n\t BTRFS_FEATURE_INCOMPAT_DEFAULT_SUBVOL |\t\\\n\t BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS |\t\t\\\n\t BTRFS_FEATURE_INCOMPAT_BIG_METADATA |\t\t\\\n\t BTRFS_FEATURE_INCOMPAT_COMPRESS_LZO |\t\t\\\n\t BTRFS_FEATURE_INCOMPAT_EXTENDED_IREF)\n\n/*\n * A leaf is full of items. offset and size tell us where to find\n * the item in the leaf (relative to the start of the data area)\n */\nstruct btrfs_item {\n\tstruct btrfs_disk_key key;\n\t__le32 offset;\n\t__le32 size;\n} __attribute__ ((__packed__));\n\n/*\n * leaves have an item area and a data area:\n * [item0, item1....itemN] [free space] [dataN...data1, data0]\n *\n * The data is separate from the items to get the keys closer together\n * during searches.\n */\nstruct btrfs_leaf {\n\tstruct btrfs_header header;\n\tstruct btrfs_item items[];\n} __attribute__ ((__packed__));\n\n/*\n * all non-leaf blocks are nodes, they hold only keys and pointers to\n * other blocks\n */\nstruct btrfs_key_ptr {\n\tstruct btrfs_disk_key key;\n\t__le64 blockptr;\n\t__le64 generation;\n} __attribute__ ((__packed__));\n\nstruct btrfs_node {\n\tstruct btrfs_header header;\n\tstruct btrfs_key_ptr ptrs[];\n} __attribute__ ((__packed__));\n\n/*\n * btrfs_paths remember the path taken from the root down to the leaf.\n * level 0 is always the leaf, and nodes[1...BTRFS_MAX_LEVEL] will point\n * to any other levels that are present.\n *\n * The slots array records the index of the item or block pointer\n * used while walking the tree.\n */\nstruct btrfs_path {\n\tstruct extent_buffer *nodes[BTRFS_MAX_LEVEL];\n\tint slots[BTRFS_MAX_LEVEL];\n\t/* if there is real range locking, this locks field will change */\n\tint locks[BTRFS_MAX_LEVEL];\n\tint reada;\n\t/* keep some upper locks as we walk down */\n\tint lowest_level;\n\n\t/*\n\t * set by btrfs_split_item, tells search_slot to keep all locks\n\t * and to force calls to keep space in the nodes\n\t */\n\tunsigned int search_for_split:1;\n\tunsigned int keep_locks:1;\n\tunsigned int skip_locking:1;\n\tunsigned int leave_spinning:1;\n\tunsigned int search_commit_root:1;\n\tunsigned int really_keep_locks:1;\n};\n\n/*\n * items in the extent btree are used to record the objectid of the\n * owner of the block and the number of references\n */\n\nstruct btrfs_extent_item {\n\t__le64 refs;\n\t__le64 generation;\n\t__le64 flags;\n} __attribute__ ((__packed__));\n\nstruct btrfs_extent_item_v0 {\n\t__le32 refs;\n} __attribute__ ((__packed__));\n\n#define BTRFS_MAX_EXTENT_ITEM_SIZE(r) ((BTRFS_LEAF_DATA_SIZE(r) >> 4) - \\\n\t\t\t\t\tsizeof(struct btrfs_item))\n\n#define BTRFS_EXTENT_FLAG_DATA\t\t(1ULL << 0)\n#define BTRFS_EXTENT_FLAG_TREE_BLOCK\t(1ULL << 1)\n\n/* following flags only apply to tree blocks */\n\n/* use full backrefs for extent pointers in the block */\n#define BTRFS_BLOCK_FLAG_FULL_BACKREF\t(1ULL << 8)\n\n/*\n * this flag is only used internally by scrub and may be changed at any time\n * it is only declared here to avoid collisions\n */\n#define BTRFS_EXTENT_FLAG_SUPER\t\t(1ULL << 48)\n\nstruct btrfs_tree_block_info {\n\tstruct btrfs_disk_key key;\n\tu8 level;\n} __attribute__ ((__packed__));\n\nstruct btrfs_extent_data_ref {\n\t__le64 root;\n\t__le64 objectid;\n\t__le64 offset;\n\t__le32 count;\n} __attribute__ ((__packed__));\n\nstruct btrfs_shared_data_ref {\n\t__le32 count;\n} __attribute__ ((__packed__));\n\nstruct btrfs_extent_inline_ref {\n\tu8 type;\n\t__le64 offset;\n} __attribute__ ((__packed__));\n\n/* old style backrefs item */\nstruct btrfs_extent_ref_v0 {\n\t__le64 root;\n\t__le64 generation;\n\t__le64 objectid;\n\t__le32 count;\n} __attribute__ ((__packed__));\n\n\n/* dev extents record free space on individual devices.  The owner\n * field points back to the chunk allocation mapping tree that allocated\n * the extent.  The chunk tree uuid field is a way to double check the owner\n */\nstruct btrfs_dev_extent {\n\t__le64 chunk_tree;\n\t__le64 chunk_objectid;\n\t__le64 chunk_offset;\n\t__le64 length;\n\tu8 chunk_tree_uuid[BTRFS_UUID_SIZE];\n} __attribute__ ((__packed__));\n\nstruct btrfs_inode_ref {\n\t__le64 index;\n\t__le16 name_len;\n\t/* name goes here */\n} __attribute__ ((__packed__));\n\nstruct btrfs_inode_extref {\n\t__le64 parent_objectid;\n\t__le64 index;\n\t__le16 name_len;\n\t__u8   name[0];\n\t/* name goes here */\n} __attribute__ ((__packed__));\n\nstruct btrfs_timespec {\n\t__le64 sec;\n\t__le32 nsec;\n} __attribute__ ((__packed__));\n\nenum btrfs_compression_type {\n\tBTRFS_COMPRESS_NONE  = 0,\n\tBTRFS_COMPRESS_ZLIB  = 1,\n\tBTRFS_COMPRESS_LZO   = 2,\n\tBTRFS_COMPRESS_TYPES = 2,\n\tBTRFS_COMPRESS_LAST  = 3,\n};\n\nstruct btrfs_inode_item {\n\t/* nfs style generation number */\n\t__le64 generation;\n\t/* transid that last touched this inode */\n\t__le64 transid;\n\t__le64 size;\n\t__le64 nbytes;\n\t__le64 block_group;\n\t__le32 nlink;\n\t__le32 uid;\n\t__le32 gid;\n\t__le32 mode;\n\t__le64 rdev;\n\t__le64 flags;\n\n\t/* modification sequence number for NFS */\n\t__le64 sequence;\n\n\t/*\n\t * a little future expansion, for more than this we can\n\t * just grow the inode item and version it\n\t */\n\t__le64 reserved[4];\n\tstruct btrfs_timespec atime;\n\tstruct btrfs_timespec ctime;\n\tstruct btrfs_timespec mtime;\n\tstruct btrfs_timespec otime;\n} __attribute__ ((__packed__));\n\nstruct btrfs_dir_log_item {\n\t__le64 end;\n} __attribute__ ((__packed__));\n\nstruct btrfs_dir_item {\n\tstruct btrfs_disk_key location;\n\t__le64 transid;\n\t__le16 data_len;\n\t__le16 name_len;\n\tu8 type;\n} __attribute__ ((__packed__));\n\n#define BTRFS_ROOT_SUBVOL_RDONLY\t(1ULL << 0)\n\nstruct btrfs_root_item {\n\tstruct btrfs_inode_item inode;\n\t__le64 generation;\n\t__le64 root_dirid;\n\t__le64 bytenr;\n\t__le64 byte_limit;\n\t__le64 bytes_used;\n\t__le64 last_snapshot;\n\t__le64 flags;\n\t__le32 refs;\n\tstruct btrfs_disk_key drop_progress;\n\tu8 drop_level;\n\tu8 level;\n\n\t/*\n\t * The following fields appear after subvol_uuids+subvol_times\n\t * were introduced.\n\t */\n\n\t/*\n\t * This generation number is used to test if the new fields are valid\n\t * and up to date while reading the root item. Everytime the root item\n\t * is written out, the \"generation\" field is copied into this field. If\n\t * anyone ever mounted the fs with an older kernel, we will have\n\t * mismatching generation values here and thus must invalidate the\n\t * new fields. See btrfs_update_root and btrfs_find_last_root for\n\t * details.\n\t * the offset of generation_v2 is also used as the start for the memset\n\t * when invalidating the fields.\n\t */\n\t__le64 generation_v2;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tu8 parent_uuid[BTRFS_UUID_SIZE];\n\tu8 received_uuid[BTRFS_UUID_SIZE];\n\t__le64 ctransid; /* updated when an inode changes */\n\t__le64 otransid; /* trans when created */\n\t__le64 stransid; /* trans when sent. non-zero for received subvol */\n\t__le64 rtransid; /* trans when received. non-zero for received subvol */\n\tstruct btrfs_timespec ctime;\n\tstruct btrfs_timespec otime;\n\tstruct btrfs_timespec stime;\n\tstruct btrfs_timespec rtime;\n\t__le64 reserved[8]; /* for future */\n} __attribute__ ((__packed__));\n\n/*\n * this is used for both forward and backward root refs\n */\nstruct btrfs_root_ref {\n\t__le64 dirid;\n\t__le64 sequence;\n\t__le16 name_len;\n} __attribute__ ((__packed__));\n\nstruct btrfs_disk_balance_args {\n\t/*\n\t * profiles to operate on, single is denoted by\n\t * BTRFS_AVAIL_ALLOC_BIT_SINGLE\n\t */\n\t__le64 profiles;\n\n\t/* usage filter */\n\t__le64 usage;\n\n\t/* devid filter */\n\t__le64 devid;\n\n\t/* devid subset filter [pstart..pend) */\n\t__le64 pstart;\n\t__le64 pend;\n\n\t/* btrfs virtual address space subset filter [vstart..vend) */\n\t__le64 vstart;\n\t__le64 vend;\n\n\t/*\n\t * profile to convert to, single is denoted by\n\t * BTRFS_AVAIL_ALLOC_BIT_SINGLE\n\t */\n\t__le64 target;\n\n\t/* BTRFS_BALANCE_ARGS_* */\n\t__le64 flags;\n\n\t__le64 unused[8];\n} __attribute__ ((__packed__));\n\n/*\n * store balance parameters to disk so that balance can be properly\n * resumed after crash or unmount\n */\nstruct btrfs_balance_item {\n\t/* BTRFS_BALANCE_* */\n\t__le64 flags;\n\n\tstruct btrfs_disk_balance_args data;\n\tstruct btrfs_disk_balance_args meta;\n\tstruct btrfs_disk_balance_args sys;\n\n\t__le64 unused[4];\n} __attribute__ ((__packed__));\n\n#define BTRFS_FILE_EXTENT_INLINE 0\n#define BTRFS_FILE_EXTENT_REG 1\n#define BTRFS_FILE_EXTENT_PREALLOC 2\n\nstruct btrfs_file_extent_item {\n\t/*\n\t * transaction id that created this extent\n\t */\n\t__le64 generation;\n\t/*\n\t * max number of bytes to hold this extent in ram\n\t * when we split a compressed extent we can't know how big\n\t * each of the resulting pieces will be.  So, this is\n\t * an upper limit on the size of the extent in ram instead of\n\t * an exact limit.\n\t */\n\t__le64 ram_bytes;\n\n\t/*\n\t * 32 bits for the various ways we might encode the data,\n\t * including compression and encryption.  If any of these\n\t * are set to something a given disk format doesn't understand\n\t * it is treated like an incompat flag for reading and writing,\n\t * but not for stat.\n\t */\n\tu8 compression;\n\tu8 encryption;\n\t__le16 other_encoding; /* spare for later use */\n\n\t/* are we inline data or a real extent? */\n\tu8 type;\n\n\t/*\n\t * disk space consumed by the extent, checksum blocks are included\n\t * in these numbers\n\t */\n\t__le64 disk_bytenr;\n\t__le64 disk_num_bytes;\n\t/*\n\t * the logical offset in file blocks (no csums)\n\t * this extent record is for.  This allows a file extent to point\n\t * into the middle of an existing extent on disk, sharing it\n\t * between two snapshots (useful if some bytes in the middle of the\n\t * extent have changed\n\t */\n\t__le64 offset;\n\t/*\n\t * the logical number of file blocks (no csums included).  This\n\t * always reflects the size uncompressed and without encoding.\n\t */\n\t__le64 num_bytes;\n\n} __attribute__ ((__packed__));\n\nstruct btrfs_csum_item {\n\tu8 csum;\n} __attribute__ ((__packed__));\n\nstruct btrfs_dev_stats_item {\n\t/*\n\t * grow this item struct at the end for future enhancements and keep\n\t * the existing values unchanged\n\t */\n\t__le64 values[BTRFS_DEV_STAT_VALUES_MAX];\n} __attribute__ ((__packed__));\n\n#define BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_ALWAYS\t0\n#define BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_AVOID\t1\n#define BTRFS_DEV_REPLACE_ITEM_STATE_NEVER_STARTED\t0\n#define BTRFS_DEV_REPLACE_ITEM_STATE_STARTED\t\t1\n#define BTRFS_DEV_REPLACE_ITEM_STATE_SUSPENDED\t\t2\n#define BTRFS_DEV_REPLACE_ITEM_STATE_FINISHED\t\t3\n#define BTRFS_DEV_REPLACE_ITEM_STATE_CANCELED\t\t4\n\nstruct btrfs_dev_replace {\n\tu64 replace_state;\t/* see #define above */\n\tu64 time_started;\t/* seconds since 1-Jan-1970 */\n\tu64 time_stopped;\t/* seconds since 1-Jan-1970 */\n\tatomic64_t num_write_errors;\n\tatomic64_t num_uncorrectable_read_errors;\n\n\tu64 cursor_left;\n\tu64 committed_cursor_left;\n\tu64 cursor_left_last_write_of_item;\n\tu64 cursor_right;\n\n\tu64 cont_reading_from_srcdev_mode;\t/* see #define above */\n\n\tint is_valid;\n\tint item_needs_writeback;\n\tstruct btrfs_device *srcdev;\n\tstruct btrfs_device *tgtdev;\n\n\tpid_t lock_owner;\n\tatomic_t nesting_level;\n\tstruct mutex lock_finishing_cancel_unmount;\n\tstruct mutex lock_management_lock;\n\tstruct mutex lock;\n\n\tstruct btrfs_scrub_progress scrub_progress;\n};\n\nstruct btrfs_dev_replace_item {\n\t/*\n\t * grow this item struct at the end for future enhancements and keep\n\t * the existing values unchanged\n\t */\n\t__le64 src_devid;\n\t__le64 cursor_left;\n\t__le64 cursor_right;\n\t__le64 cont_reading_from_srcdev_mode;\n\n\t__le64 replace_state;\n\t__le64 time_started;\n\t__le64 time_stopped;\n\t__le64 num_write_errors;\n\t__le64 num_uncorrectable_read_errors;\n} __attribute__ ((__packed__));\n\n/* different types of block groups (and chunks) */\n#define BTRFS_BLOCK_GROUP_DATA\t\t(1ULL << 0)\n#define BTRFS_BLOCK_GROUP_SYSTEM\t(1ULL << 1)\n#define BTRFS_BLOCK_GROUP_METADATA\t(1ULL << 2)\n#define BTRFS_BLOCK_GROUP_RAID0\t\t(1ULL << 3)\n#define BTRFS_BLOCK_GROUP_RAID1\t\t(1ULL << 4)\n#define BTRFS_BLOCK_GROUP_DUP\t\t(1ULL << 5)\n#define BTRFS_BLOCK_GROUP_RAID10\t(1ULL << 6)\n#define BTRFS_BLOCK_GROUP_RESERVED\tBTRFS_AVAIL_ALLOC_BIT_SINGLE\n#define BTRFS_NR_RAID_TYPES\t\t5\n\n#define BTRFS_BLOCK_GROUP_TYPE_MASK\t(BTRFS_BLOCK_GROUP_DATA |    \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_SYSTEM |  \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_METADATA)\n\n#define BTRFS_BLOCK_GROUP_PROFILE_MASK\t(BTRFS_BLOCK_GROUP_RAID0 |   \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_RAID1 |   \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_DUP |     \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_RAID10)\n/*\n * We need a bit for restriper to be able to tell when chunks of type\n * SINGLE are available.  This \"extended\" profile format is used in\n * fs_info->avail_*_alloc_bits (in-memory) and balance item fields\n * (on-disk).  The corresponding on-disk bit in chunk.type is reserved\n * to avoid remappings between two formats in future.\n */\n#define BTRFS_AVAIL_ALLOC_BIT_SINGLE\t(1ULL << 48)\n\n#define BTRFS_EXTENDED_PROFILE_MASK\t(BTRFS_BLOCK_GROUP_PROFILE_MASK | \\\n\t\t\t\t\t BTRFS_AVAIL_ALLOC_BIT_SINGLE)\n\nstatic inline u64 chunk_to_extended(u64 flags)\n{\n\tif ((flags & BTRFS_BLOCK_GROUP_PROFILE_MASK) == 0)\n\t\tflags |= BTRFS_AVAIL_ALLOC_BIT_SINGLE;\n\n\treturn flags;\n}\nstatic inline u64 extended_to_chunk(u64 flags)\n{\n\treturn flags & ~BTRFS_AVAIL_ALLOC_BIT_SINGLE;\n}\n\nstruct btrfs_block_group_item {\n\t__le64 used;\n\t__le64 chunk_objectid;\n\t__le64 flags;\n} __attribute__ ((__packed__));\n\n/*\n * is subvolume quota turned on?\n */\n#define BTRFS_QGROUP_STATUS_FLAG_ON\t\t(1ULL << 0)\n/*\n * SCANNING is set during the initialization phase\n */\n#define BTRFS_QGROUP_STATUS_FLAG_SCANNING\t(1ULL << 1)\n/*\n * Some qgroup entries are known to be out of date,\n * either because the configuration has changed in a way that\n * makes a rescan necessary, or because the fs has been mounted\n * with a non-qgroup-aware version.\n * Turning qouta off and on again makes it inconsistent, too.\n */\n#define BTRFS_QGROUP_STATUS_FLAG_INCONSISTENT\t(1ULL << 2)\n\n#define BTRFS_QGROUP_STATUS_VERSION        1\n\nstruct btrfs_qgroup_status_item {\n\t__le64 version;\n\t/*\n\t * the generation is updated during every commit. As older\n\t * versions of btrfs are not aware of qgroups, it will be\n\t * possible to detect inconsistencies by checking the\n\t * generation on mount time\n\t */\n\t__le64 generation;\n\n\t/* flag definitions see above */\n\t__le64 flags;\n\n\t/*\n\t * only used during scanning to record the progress\n\t * of the scan. It contains a logical address\n\t */\n\t__le64 scan;\n} __attribute__ ((__packed__));\n\nstruct btrfs_qgroup_info_item {\n\t__le64 generation;\n\t__le64 rfer;\n\t__le64 rfer_cmpr;\n\t__le64 excl;\n\t__le64 excl_cmpr;\n} __attribute__ ((__packed__));\n\n/* flags definition for qgroup limits */\n#define BTRFS_QGROUP_LIMIT_MAX_RFER\t(1ULL << 0)\n#define BTRFS_QGROUP_LIMIT_MAX_EXCL\t(1ULL << 1)\n#define BTRFS_QGROUP_LIMIT_RSV_RFER\t(1ULL << 2)\n#define BTRFS_QGROUP_LIMIT_RSV_EXCL\t(1ULL << 3)\n#define BTRFS_QGROUP_LIMIT_RFER_CMPR\t(1ULL << 4)\n#define BTRFS_QGROUP_LIMIT_EXCL_CMPR\t(1ULL << 5)\n\nstruct btrfs_qgroup_limit_item {\n\t/*\n\t * only updated when any of the other values change\n\t */\n\t__le64 flags;\n\t__le64 max_rfer;\n\t__le64 max_excl;\n\t__le64 rsv_rfer;\n\t__le64 rsv_excl;\n} __attribute__ ((__packed__));\n\nstruct btrfs_space_info {\n\tu64 flags;\n\n\tu64 total_bytes;\t/* total bytes in the space,\n\t\t\t\t   this doesn't take mirrors into account */\n\tu64 bytes_used;\t\t/* total bytes used,\n\t\t\t\t   this doesn't take mirrors into account */\n\tu64 bytes_pinned;\t/* total bytes pinned, will be freed when the\n\t\t\t\t   transaction finishes */\n\tu64 bytes_reserved;\t/* total bytes the allocator has reserved for\n\t\t\t\t   current allocations */\n\tu64 bytes_readonly;\t/* total bytes that are read only */\n\n\tu64 bytes_may_use;\t/* number of bytes that may be used for\n\t\t\t\t   delalloc/allocations */\n\tu64 disk_used;\t\t/* total bytes used on disk */\n\tu64 disk_total;\t\t/* total bytes on disk, takes mirrors into\n\t\t\t\t   account */\n\n\t/*\n\t * we bump reservation progress every time we decrement\n\t * bytes_reserved.  This way people waiting for reservations\n\t * know something good has happened and they can check\n\t * for progress.  The number here isn't to be trusted, it\n\t * just shows reclaim activity\n\t */\n\tunsigned long reservation_progress;\n\n\tunsigned int full:1;\t/* indicates that we cannot allocate any more\n\t\t\t\t   chunks for this space */\n\tunsigned int chunk_alloc:1;\t/* set if we are allocating a chunk */\n\n\tunsigned int flush:1;\t\t/* set if we are trying to make space */\n\n\tunsigned int force_alloc;\t/* set if we need to force a chunk\n\t\t\t\t\t   alloc for this space */\n\n\tstruct list_head list;\n\n\t/* for block groups in our same type */\n\tstruct list_head block_groups[BTRFS_NR_RAID_TYPES];\n\tspinlock_t lock;\n\tstruct rw_semaphore groups_sem;\n\twait_queue_head_t wait;\n};\n\n#define\tBTRFS_BLOCK_RSV_GLOBAL\t\t1\n#define\tBTRFS_BLOCK_RSV_DELALLOC\t2\n#define\tBTRFS_BLOCK_RSV_TRANS\t\t3\n#define\tBTRFS_BLOCK_RSV_CHUNK\t\t4\n#define\tBTRFS_BLOCK_RSV_DELOPS\t\t5\n#define\tBTRFS_BLOCK_RSV_EMPTY\t\t6\n#define\tBTRFS_BLOCK_RSV_TEMP\t\t7\n\nstruct btrfs_block_rsv {\n\tu64 size;\n\tu64 reserved;\n\tstruct btrfs_space_info *space_info;\n\tspinlock_t lock;\n\tunsigned short full;\n\tunsigned short type;\n\tunsigned short failfast;\n};\n\n/*\n * free clusters are used to claim free space in relatively large chunks,\n * allowing us to do less seeky writes.  They are used for all metadata\n * allocations and data allocations in ssd mode.\n */\nstruct btrfs_free_cluster {\n\tspinlock_t lock;\n\tspinlock_t refill_lock;\n\tstruct rb_root root;\n\n\t/* largest extent in this cluster */\n\tu64 max_size;\n\n\t/* first extent starting offset */\n\tu64 window_start;\n\n\tstruct btrfs_block_group_cache *block_group;\n\t/*\n\t * when a cluster is allocated from a block group, we put the\n\t * cluster onto a list in the block group so that it can\n\t * be freed before the block group is freed.\n\t */\n\tstruct list_head block_group_list;\n};\n\nenum btrfs_caching_type {\n\tBTRFS_CACHE_NO\t\t= 0,\n\tBTRFS_CACHE_STARTED\t= 1,\n\tBTRFS_CACHE_FAST\t= 2,\n\tBTRFS_CACHE_FINISHED\t= 3,\n};\n\nenum btrfs_disk_cache_state {\n\tBTRFS_DC_WRITTEN\t= 0,\n\tBTRFS_DC_ERROR\t\t= 1,\n\tBTRFS_DC_CLEAR\t\t= 2,\n\tBTRFS_DC_SETUP\t\t= 3,\n\tBTRFS_DC_NEED_WRITE\t= 4,\n};\n\nstruct btrfs_caching_control {\n\tstruct list_head list;\n\tstruct mutex mutex;\n\twait_queue_head_t wait;\n\tstruct btrfs_work work;\n\tstruct btrfs_block_group_cache *block_group;\n\tu64 progress;\n\tatomic_t count;\n};\n\nstruct btrfs_block_group_cache {\n\tstruct btrfs_key key;\n\tstruct btrfs_block_group_item item;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct inode *inode;\n\tspinlock_t lock;\n\tu64 pinned;\n\tu64 reserved;\n\tu64 bytes_super;\n\tu64 flags;\n\tu64 sectorsize;\n\tu64 cache_generation;\n\tunsigned int ro:1;\n\tunsigned int dirty:1;\n\tunsigned int iref:1;\n\n\tint disk_cache_state;\n\n\t/* cache tracking stuff */\n\tint cached;\n\tstruct btrfs_caching_control *caching_ctl;\n\tu64 last_byte_to_unpin;\n\n\tstruct btrfs_space_info *space_info;\n\n\t/* free space cache stuff */\n\tstruct btrfs_free_space_ctl *free_space_ctl;\n\n\t/* block group cache stuff */\n\tstruct rb_node cache_node;\n\n\t/* for block groups in the same raid type */\n\tstruct list_head list;\n\n\t/* usage count */\n\tatomic_t count;\n\n\t/* List of struct btrfs_free_clusters for this block group.\n\t * Today it will only have one thing on it, but that may change\n\t */\n\tstruct list_head cluster_list;\n\n\t/* For delayed block group creation */\n\tstruct list_head new_bg_list;\n};\n\n/* delayed seq elem */\nstruct seq_list {\n\tstruct list_head list;\n\tu64 seq;\n};\n\n/* fs_info */\nstruct reloc_control;\nstruct btrfs_device;\nstruct btrfs_fs_devices;\nstruct btrfs_balance_control;\nstruct btrfs_delayed_root;\nstruct btrfs_fs_info {\n\tu8 fsid[BTRFS_FSID_SIZE];\n\tu8 chunk_tree_uuid[BTRFS_UUID_SIZE];\n\tstruct btrfs_root *extent_root;\n\tstruct btrfs_root *tree_root;\n\tstruct btrfs_root *chunk_root;\n\tstruct btrfs_root *dev_root;\n\tstruct btrfs_root *fs_root;\n\tstruct btrfs_root *csum_root;\n\tstruct btrfs_root *quota_root;\n\n\t/* the log root tree is a directory of all the other log roots */\n\tstruct btrfs_root *log_root_tree;\n\n\tspinlock_t fs_roots_radix_lock;\n\tstruct radix_tree_root fs_roots_radix;\n\n\t/* block group cache stuff */\n\tspinlock_t block_group_cache_lock;\n\tstruct rb_root block_group_cache_tree;\n\n\t/* keep track of unallocated space */\n\tspinlock_t free_chunk_lock;\n\tu64 free_chunk_space;\n\n\tstruct extent_io_tree freed_extents[2];\n\tstruct extent_io_tree *pinned_extents;\n\n\t/* logical->physical extent mapping */\n\tstruct btrfs_mapping_tree mapping_tree;\n\n\t/*\n\t * block reservation for extent, checksum, root tree and\n\t * delayed dir index item\n\t */\n\tstruct btrfs_block_rsv global_block_rsv;\n\t/* block reservation for delay allocation */\n\tstruct btrfs_block_rsv delalloc_block_rsv;\n\t/* block reservation for metadata operations */\n\tstruct btrfs_block_rsv trans_block_rsv;\n\t/* block reservation for chunk tree */\n\tstruct btrfs_block_rsv chunk_block_rsv;\n\t/* block reservation for delayed operations */\n\tstruct btrfs_block_rsv delayed_block_rsv;\n\n\tstruct btrfs_block_rsv empty_block_rsv;\n\n\tu64 generation;\n\tu64 last_trans_committed;\n\n\t/*\n\t * this is updated to the current trans every time a full commit\n\t * is required instead of the faster short fsync log commits\n\t */\n\tu64 last_trans_log_full_commit;\n\tunsigned long mount_opt;\n\tunsigned long compress_type:4;\n\tu64 max_inline;\n\tu64 alloc_start;\n\tstruct btrfs_transaction *running_transaction;\n\twait_queue_head_t transaction_throttle;\n\twait_queue_head_t transaction_wait;\n\twait_queue_head_t transaction_blocked_wait;\n\twait_queue_head_t async_submit_wait;\n\n\tstruct btrfs_super_block *super_copy;\n\tstruct btrfs_super_block *super_for_commit;\n\tstruct block_device *__bdev;\n\tstruct super_block *sb;\n\tstruct inode *btree_inode;\n\tstruct backing_dev_info bdi;\n\tstruct mutex tree_log_mutex;\n\tstruct mutex transaction_kthread_mutex;\n\tstruct mutex cleaner_mutex;\n\tstruct mutex chunk_mutex;\n\tstruct mutex volume_mutex;\n\t/*\n\t * this protects the ordered operations list only while we are\n\t * processing all of the entries on it.  This way we make\n\t * sure the commit code doesn't find the list temporarily empty\n\t * because another function happens to be doing non-waiting preflush\n\t * before jumping into the main commit.\n\t */\n\tstruct mutex ordered_operations_mutex;\n\tstruct rw_semaphore extent_commit_sem;\n\n\tstruct rw_semaphore cleanup_work_sem;\n\n\tstruct rw_semaphore subvol_sem;\n\tstruct srcu_struct subvol_srcu;\n\n\tspinlock_t trans_lock;\n\t/*\n\t * the reloc mutex goes with the trans lock, it is taken\n\t * during commit to protect us from the relocation code\n\t */\n\tstruct mutex reloc_mutex;\n\n\tstruct list_head trans_list;\n\tstruct list_head dead_roots;\n\tstruct list_head caching_block_groups;\n\n\tspinlock_t delayed_iput_lock;\n\tstruct list_head delayed_iputs;\n\n\t/* this protects tree_mod_seq_list */\n\tspinlock_t tree_mod_seq_lock;\n\tatomic_t tree_mod_seq;\n\tstruct list_head tree_mod_seq_list;\n\tstruct seq_list tree_mod_seq_elem;\n\n\t/* this protects tree_mod_log */\n\trwlock_t tree_mod_log_lock;\n\tstruct rb_root tree_mod_log;\n\n\tatomic_t nr_async_submits;\n\tatomic_t async_submit_draining;\n\tatomic_t nr_async_bios;\n\tatomic_t async_delalloc_pages;\n\tatomic_t open_ioctl_trans;\n\n\t/*\n\t * this is used by the balancing code to wait for all the pending\n\t * ordered extents\n\t */\n\tspinlock_t ordered_extent_lock;\n\n\t/*\n\t * all of the data=ordered extents pending writeback\n\t * these can span multiple transactions and basically include\n\t * every dirty data page that isn't from nodatacow\n\t */\n\tstruct list_head ordered_extents;\n\n\t/*\n\t * all of the inodes that have delalloc bytes.  It is possible for\n\t * this list to be empty even when there is still dirty data=ordered\n\t * extents waiting to finish IO.\n\t */\n\tstruct list_head delalloc_inodes;\n\n\t/*\n\t * special rename and truncate targets that must be on disk before\n\t * we're allowed to commit.  This is basically the ext3 style\n\t * data=ordered list.\n\t */\n\tstruct list_head ordered_operations;\n\n\t/*\n\t * there is a pool of worker threads for checksumming during writes\n\t * and a pool for checksumming after reads.  This is because readers\n\t * can run with FS locks held, and the writers may be waiting for\n\t * those locks.  We don't want ordering in the pending list to cause\n\t * deadlocks, and so the two are serviced separately.\n\t *\n\t * A third pool does submit_bio to avoid deadlocking with the other\n\t * two\n\t */\n\tstruct btrfs_workers generic_worker;\n\tstruct btrfs_workers workers;\n\tstruct btrfs_workers delalloc_workers;\n\tstruct btrfs_workers flush_workers;\n\tstruct btrfs_workers endio_workers;\n\tstruct btrfs_workers endio_meta_workers;\n\tstruct btrfs_workers endio_meta_write_workers;\n\tstruct btrfs_workers endio_write_workers;\n\tstruct btrfs_workers endio_freespace_worker;\n\tstruct btrfs_workers submit_workers;\n\tstruct btrfs_workers caching_workers;\n\tstruct btrfs_workers readahead_workers;\n\n\t/*\n\t * fixup workers take dirty pages that didn't properly go through\n\t * the cow mechanism and make them safe to write.  It happens\n\t * for the sys_munmap function call path\n\t */\n\tstruct btrfs_workers fixup_workers;\n\tstruct btrfs_workers delayed_workers;\n\tstruct task_struct *transaction_kthread;\n\tstruct task_struct *cleaner_kthread;\n\tint thread_pool_size;\n\n\tstruct kobject super_kobj;\n\tstruct completion kobj_unregister;\n\tint do_barriers;\n\tint closing;\n\tint log_root_recovering;\n\tint enospc_unlink;\n\tint trans_no_join;\n\n\tu64 total_pinned;\n\n\t/* protected by the delalloc lock, used to keep from writing\n\t * metadata until there is a nice batch\n\t */\n\tu64 dirty_metadata_bytes;\n\tstruct list_head dirty_cowonly_roots;\n\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * the space_info list is almost entirely read only.  It only changes\n\t * when we add a new raid type to the FS, and that happens\n\t * very rarely.  RCU is used to protect it.\n\t */\n\tstruct list_head space_info;\n\n\tstruct btrfs_space_info *data_sinfo;\n\n\tstruct reloc_control *reloc_ctl;\n\n\tspinlock_t delalloc_lock;\n\tu64 delalloc_bytes;\n\n\t/* data_alloc_cluster is only used in ssd mode */\n\tstruct btrfs_free_cluster data_alloc_cluster;\n\n\t/* all metadata allocations go through this cluster */\n\tstruct btrfs_free_cluster meta_alloc_cluster;\n\n\t/* auto defrag inodes go here */\n\tspinlock_t defrag_inodes_lock;\n\tstruct rb_root defrag_inodes;\n\tatomic_t defrag_running;\n\n\t/*\n\t * these three are in extended format (availability of single\n\t * chunks is denoted by BTRFS_AVAIL_ALLOC_BIT_SINGLE bit, other\n\t * types are denoted by corresponding BTRFS_BLOCK_GROUP_* bits)\n\t */\n\tu64 avail_data_alloc_bits;\n\tu64 avail_metadata_alloc_bits;\n\tu64 avail_system_alloc_bits;\n\n\t/* restriper state */\n\tspinlock_t balance_lock;\n\tstruct mutex balance_mutex;\n\tatomic_t balance_running;\n\tatomic_t balance_pause_req;\n\tatomic_t balance_cancel_req;\n\tstruct btrfs_balance_control *balance_ctl;\n\twait_queue_head_t balance_wait_q;\n\n\tunsigned data_chunk_allocations;\n\tunsigned metadata_ratio;\n\n\tvoid *bdev_holder;\n\n\t/* private scrub information */\n\tstruct mutex scrub_lock;\n\tatomic_t scrubs_running;\n\tatomic_t scrub_pause_req;\n\tatomic_t scrubs_paused;\n\tatomic_t scrub_cancel_req;\n\twait_queue_head_t scrub_pause_wait;\n\tstruct rw_semaphore scrub_super_lock;\n\tint scrub_workers_refcnt;\n\tstruct btrfs_workers scrub_workers;\n\tstruct btrfs_workers scrub_wr_completion_workers;\n\tstruct btrfs_workers scrub_nocow_workers;\n\n#ifdef CONFIG_BTRFS_FS_CHECK_INTEGRITY\n\tu32 check_integrity_print_mask;\n#endif\n\t/*\n\t * quota information\n\t */\n\tunsigned int quota_enabled:1;\n\n\t/*\n\t * quota_enabled only changes state after a commit. This holds the\n\t * next state.\n\t */\n\tunsigned int pending_quota_state:1;\n\n\t/* is qgroup tracking in a consistent state? */\n\tu64 qgroup_flags;\n\n\t/* holds configuration and tracking. Protected by qgroup_lock */\n\tstruct rb_root qgroup_tree;\n\tspinlock_t qgroup_lock;\n\n\t/* list of dirty qgroups to be written at next commit */\n\tstruct list_head dirty_qgroups;\n\n\t/* used by btrfs_qgroup_record_ref for an efficient tree traversal */\n\tu64 qgroup_seq;\n\n\t/* filesystem state */\n\tu64 fs_state;\n\n\tstruct btrfs_delayed_root *delayed_root;\n\n\t/* readahead tree */\n\tspinlock_t reada_lock;\n\tstruct radix_tree_root reada_tree;\n\n\t/* next backup root to be overwritten */\n\tint backup_root_index;\n\n\tint num_tolerated_disk_barrier_failures;\n\n\t/* device replace state */\n\tstruct btrfs_dev_replace dev_replace;\n\n\tatomic_t mutually_exclusive_operation_running;\n};\n\n/*\n * in ram representation of the tree.  extent_root is used for all allocations\n * and for the extent tree extent_root root.\n */\nstruct btrfs_root {\n\tstruct extent_buffer *node;\n\n\tstruct extent_buffer *commit_root;\n\tstruct btrfs_root *log_root;\n\tstruct btrfs_root *reloc_root;\n\n\tstruct btrfs_root_item root_item;\n\tstruct btrfs_key root_key;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct extent_io_tree dirty_log_pages;\n\n\tstruct kobject root_kobj;\n\tstruct completion kobj_unregister;\n\tstruct mutex objectid_mutex;\n\n\tspinlock_t accounting_lock;\n\tstruct btrfs_block_rsv *block_rsv;\n\n\t/* free ino cache stuff */\n\tstruct mutex fs_commit_mutex;\n\tstruct btrfs_free_space_ctl *free_ino_ctl;\n\tenum btrfs_caching_type cached;\n\tspinlock_t cache_lock;\n\twait_queue_head_t cache_wait;\n\tstruct btrfs_free_space_ctl *free_ino_pinned;\n\tu64 cache_progress;\n\tstruct inode *cache_inode;\n\n\tstruct mutex log_mutex;\n\twait_queue_head_t log_writer_wait;\n\twait_queue_head_t log_commit_wait[2];\n\tatomic_t log_writers;\n\tatomic_t log_commit[2];\n\tatomic_t log_batch;\n\tunsigned long log_transid;\n\tunsigned long last_log_commit;\n\tpid_t log_start_pid;\n\tbool log_multiple_pids;\n\n\tu64 objectid;\n\tu64 last_trans;\n\n\t/* data allocations are done in sectorsize units */\n\tu32 sectorsize;\n\n\t/* node allocations are done in nodesize units */\n\tu32 nodesize;\n\n\t/* leaf allocations are done in leafsize units */\n\tu32 leafsize;\n\n\tu32 stripesize;\n\n\tu32 type;\n\n\tu64 highest_objectid;\n\n\t/* btrfs_record_root_in_trans is a multi-step process,\n\t * and it can race with the balancing code.   But the\n\t * race is very small, and only the first time the root\n\t * is added to each transaction.  So in_trans_setup\n\t * is used to tell us when more checks are required\n\t */\n\tunsigned long in_trans_setup;\n\tint ref_cows;\n\tint track_dirty;\n\tint in_radix;\n\n\tu64 defrag_trans_start;\n\tstruct btrfs_key defrag_progress;\n\tstruct btrfs_key defrag_max;\n\tint defrag_running;\n\tchar *name;\n\n\t/* the dirty list is only used by non-reference counted roots */\n\tstruct list_head dirty_list;\n\n\tstruct list_head root_list;\n\n\tspinlock_t orphan_lock;\n\tatomic_t orphan_inodes;\n\tstruct btrfs_block_rsv *orphan_block_rsv;\n\tint orphan_item_inserted;\n\tint orphan_cleanup_state;\n\n\tspinlock_t inode_lock;\n\t/* red-black tree that keeps track of in-memory inodes */\n\tstruct rb_root inode_tree;\n\n\t/*\n\t * radix tree that keeps track of delayed nodes of every inode,\n\t * protected by inode_lock\n\t */\n\tstruct radix_tree_root delayed_nodes_tree;\n\t/*\n\t * right now this just gets used so that a root has its own devid\n\t * for stat.  It may be used for more later\n\t */\n\tdev_t anon_dev;\n\n\tint force_cow;\n\n\tspinlock_t root_item_lock;\n};\n\nstruct btrfs_ioctl_defrag_range_args {\n\t/* start of the defrag operation */\n\t__u64 start;\n\n\t/* number of bytes to defrag, use (u64)-1 to say all */\n\t__u64 len;\n\n\t/*\n\t * flags for the operation, which can include turning\n\t * on compression for this one defrag\n\t */\n\t__u64 flags;\n\n\t/*\n\t * any extent bigger than this will be considered\n\t * already defragged.  Use 0 to take the kernel default\n\t * Use 1 to say every single extent must be rewritten\n\t */\n\t__u32 extent_thresh;\n\n\t/*\n\t * which compression method to use if turning on compression\n\t * for this defrag operation.  If unspecified, zlib will\n\t * be used\n\t */\n\t__u32 compress_type;\n\n\t/* spare for later */\n\t__u32 unused[4];\n};\n\n\n/*\n * inode items have the data typically returned from stat and store other\n * info about object characteristics.  There is one for every file and dir in\n * the FS\n */\n#define BTRFS_INODE_ITEM_KEY\t\t1\n#define BTRFS_INODE_REF_KEY\t\t12\n#define BTRFS_INODE_EXTREF_KEY\t\t13\n#define BTRFS_XATTR_ITEM_KEY\t\t24\n#define BTRFS_ORPHAN_ITEM_KEY\t\t48\n/* reserve 2-15 close to the inode for later flexibility */\n\n/*\n * dir items are the name -> inode pointers in a directory.  There is one\n * for every name in a directory.\n */\n#define BTRFS_DIR_LOG_ITEM_KEY  60\n#define BTRFS_DIR_LOG_INDEX_KEY 72\n#define BTRFS_DIR_ITEM_KEY\t84\n#define BTRFS_DIR_INDEX_KEY\t96\n/*\n * extent data is for file data\n */\n#define BTRFS_EXTENT_DATA_KEY\t108\n\n/*\n * extent csums are stored in a separate tree and hold csums for\n * an entire extent on disk.\n */\n#define BTRFS_EXTENT_CSUM_KEY\t128\n\n/*\n * root items point to tree roots.  They are typically in the root\n * tree used by the super block to find all the other trees\n */\n#define BTRFS_ROOT_ITEM_KEY\t132\n\n/*\n * root backrefs tie subvols and snapshots to the directory entries that\n * reference them\n */\n#define BTRFS_ROOT_BACKREF_KEY\t144\n\n/*\n * root refs make a fast index for listing all of the snapshots and\n * subvolumes referenced by a given root.  They point directly to the\n * directory item in the root that references the subvol\n */\n#define BTRFS_ROOT_REF_KEY\t156\n\n/*\n * extent items are in the extent map tree.  These record which blocks\n * are used, and how many references there are to each block\n */\n#define BTRFS_EXTENT_ITEM_KEY\t168\n\n#define BTRFS_TREE_BLOCK_REF_KEY\t176\n\n#define BTRFS_EXTENT_DATA_REF_KEY\t178\n\n#define BTRFS_EXTENT_REF_V0_KEY\t\t180\n\n#define BTRFS_SHARED_BLOCK_REF_KEY\t182\n\n#define BTRFS_SHARED_DATA_REF_KEY\t184\n\n/*\n * block groups give us hints into the extent allocation trees.  Which\n * blocks are free etc etc\n */\n#define BTRFS_BLOCK_GROUP_ITEM_KEY 192\n\n#define BTRFS_DEV_EXTENT_KEY\t204\n#define BTRFS_DEV_ITEM_KEY\t216\n#define BTRFS_CHUNK_ITEM_KEY\t228\n\n/*\n * Records the overall state of the qgroups.\n * There's only one instance of this key present,\n * (0, BTRFS_QGROUP_STATUS_KEY, 0)\n */\n#define BTRFS_QGROUP_STATUS_KEY         240\n/*\n * Records the currently used space of the qgroup.\n * One key per qgroup, (0, BTRFS_QGROUP_INFO_KEY, qgroupid).\n */\n#define BTRFS_QGROUP_INFO_KEY           242\n/*\n * Contains the user configured limits for the qgroup.\n * One key per qgroup, (0, BTRFS_QGROUP_LIMIT_KEY, qgroupid).\n */\n#define BTRFS_QGROUP_LIMIT_KEY          244\n/*\n * Records the child-parent relationship of qgroups. For\n * each relation, 2 keys are present:\n * (childid, BTRFS_QGROUP_RELATION_KEY, parentid)\n * (parentid, BTRFS_QGROUP_RELATION_KEY, childid)\n */\n#define BTRFS_QGROUP_RELATION_KEY       246\n\n#define BTRFS_BALANCE_ITEM_KEY\t248\n\n/*\n * Persistantly stores the io stats in the device tree.\n * One key for all stats, (0, BTRFS_DEV_STATS_KEY, devid).\n */\n#define BTRFS_DEV_STATS_KEY\t249\n\n/*\n * Persistantly stores the device replace state in the device tree.\n * The key is built like this: (0, BTRFS_DEV_REPLACE_KEY, 0).\n */\n#define BTRFS_DEV_REPLACE_KEY\t250\n\n/*\n * string items are for debugging.  They just store a short string of\n * data in the FS\n */\n#define BTRFS_STRING_ITEM_KEY\t253\n\n/*\n * Flags for mount options.\n *\n * Note: don't forget to add new options to btrfs_show_options()\n */\n#define BTRFS_MOUNT_NODATASUM\t\t(1 << 0)\n#define BTRFS_MOUNT_NODATACOW\t\t(1 << 1)\n#define BTRFS_MOUNT_NOBARRIER\t\t(1 << 2)\n#define BTRFS_MOUNT_SSD\t\t\t(1 << 3)\n#define BTRFS_MOUNT_DEGRADED\t\t(1 << 4)\n#define BTRFS_MOUNT_COMPRESS\t\t(1 << 5)\n#define BTRFS_MOUNT_NOTREELOG           (1 << 6)\n#define BTRFS_MOUNT_FLUSHONCOMMIT       (1 << 7)\n#define BTRFS_MOUNT_SSD_SPREAD\t\t(1 << 8)\n#define BTRFS_MOUNT_NOSSD\t\t(1 << 9)\n#define BTRFS_MOUNT_DISCARD\t\t(1 << 10)\n#define BTRFS_MOUNT_FORCE_COMPRESS      (1 << 11)\n#define BTRFS_MOUNT_SPACE_CACHE\t\t(1 << 12)\n#define BTRFS_MOUNT_CLEAR_CACHE\t\t(1 << 13)\n#define BTRFS_MOUNT_USER_SUBVOL_RM_ALLOWED (1 << 14)\n#define BTRFS_MOUNT_ENOSPC_DEBUG\t (1 << 15)\n#define BTRFS_MOUNT_AUTO_DEFRAG\t\t(1 << 16)\n#define BTRFS_MOUNT_INODE_MAP_CACHE\t(1 << 17)\n#define BTRFS_MOUNT_RECOVERY\t\t(1 << 18)\n#define BTRFS_MOUNT_SKIP_BALANCE\t(1 << 19)\n#define BTRFS_MOUNT_CHECK_INTEGRITY\t(1 << 20)\n#define BTRFS_MOUNT_CHECK_INTEGRITY_INCLUDING_EXTENT_DATA (1 << 21)\n#define BTRFS_MOUNT_PANIC_ON_FATAL_ERROR\t(1 << 22)\n\n#define btrfs_clear_opt(o, opt)\t\t((o) &= ~BTRFS_MOUNT_##opt)\n#define btrfs_set_opt(o, opt)\t\t((o) |= BTRFS_MOUNT_##opt)\n#define btrfs_test_opt(root, opt)\t((root)->fs_info->mount_opt & \\\n\t\t\t\t\t BTRFS_MOUNT_##opt)\n/*\n * Inode flags\n */\n#define BTRFS_INODE_NODATASUM\t\t(1 << 0)\n#define BTRFS_INODE_NODATACOW\t\t(1 << 1)\n#define BTRFS_INODE_READONLY\t\t(1 << 2)\n#define BTRFS_INODE_NOCOMPRESS\t\t(1 << 3)\n#define BTRFS_INODE_PREALLOC\t\t(1 << 4)\n#define BTRFS_INODE_SYNC\t\t(1 << 5)\n#define BTRFS_INODE_IMMUTABLE\t\t(1 << 6)\n#define BTRFS_INODE_APPEND\t\t(1 << 7)\n#define BTRFS_INODE_NODUMP\t\t(1 << 8)\n#define BTRFS_INODE_NOATIME\t\t(1 << 9)\n#define BTRFS_INODE_DIRSYNC\t\t(1 << 10)\n#define BTRFS_INODE_COMPRESS\t\t(1 << 11)\n\n#define BTRFS_INODE_ROOT_ITEM_INIT\t(1 << 31)\n\nstruct btrfs_map_token {\n\tstruct extent_buffer *eb;\n\tchar *kaddr;\n\tunsigned long offset;\n};\n\nstatic inline void btrfs_init_map_token (struct btrfs_map_token *token)\n{\n\ttoken->kaddr = NULL;\n}\n\n/* some macros to generate set/get funcs for the struct fields.  This\n * assumes there is a lefoo_to_cpu for every type, so lets make a simple\n * one for u8:\n */\n#define le8_to_cpu(v) (v)\n#define cpu_to_le8(v) (v)\n#define __le8 u8\n\n#define read_eb_member(eb, ptr, type, member, result) (\t\t\t\\\n\tread_extent_buffer(eb, (char *)(result),\t\t\t\\\n\t\t\t   ((unsigned long)(ptr)) +\t\t\t\\\n\t\t\t    offsetof(type, member),\t\t\t\\\n\t\t\t   sizeof(((type *)0)->member)))\n\n#define write_eb_member(eb, ptr, type, member, result) (\t\t\\\n\twrite_extent_buffer(eb, (char *)(result),\t\t\t\\\n\t\t\t   ((unsigned long)(ptr)) +\t\t\t\\\n\t\t\t    offsetof(type, member),\t\t\t\\\n\t\t\t   sizeof(((type *)0)->member)))\n\n#define DECLARE_BTRFS_SETGET_BITS(bits)\t\t\t\t\t\\\nu##bits btrfs_get_token_##bits(struct extent_buffer *eb, void *ptr,\t\\\n\t\t\t       unsigned long off,\t\t\t\\\n                              struct btrfs_map_token *token);\t\t\\\nvoid btrfs_set_token_##bits(struct extent_buffer *eb, void *ptr,\t\\\n\t\t\t    unsigned long off, u##bits val,\t\t\\\n\t\t\t    struct btrfs_map_token *token);\t\t\\\nstatic inline u##bits btrfs_get_##bits(struct extent_buffer *eb, void *ptr, \\\n\t\t\t\t       unsigned long off)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn btrfs_get_token_##bits(eb, ptr, off, NULL);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_##bits(struct extent_buffer *eb, void *ptr, \\\n\t\t\t\t    unsigned long off, u##bits val)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n       btrfs_set_token_##bits(eb, ptr, off, val, NULL);\t\t\t\\\n}\n\nDECLARE_BTRFS_SETGET_BITS(8)\nDECLARE_BTRFS_SETGET_BITS(16)\nDECLARE_BTRFS_SETGET_BITS(32)\nDECLARE_BTRFS_SETGET_BITS(64)\n\n#define BTRFS_SETGET_FUNCS(name, type, member, bits)\t\t\t\\\nstatic inline u##bits btrfs_##name(struct extent_buffer *eb, type *s)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);\t\\\n\treturn btrfs_get_##bits(eb, s, offsetof(type, member));\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_##name(struct extent_buffer *eb, type *s,\t\\\n\t\t\t\t    u##bits val)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);\t\\\n\tbtrfs_set_##bits(eb, s, offsetof(type, member), val);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline u##bits btrfs_token_##name(struct extent_buffer *eb, type *s, \\\n\t\t\t\t\t struct btrfs_map_token *token)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);\t\\\n\treturn btrfs_get_token_##bits(eb, s, offsetof(type, member), token); \\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_token_##name(struct extent_buffer *eb,\t\\\n\t\t\t\t\t  type *s, u##bits val,\t\t\\\n                                         struct btrfs_map_token *token)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);\t\\\n\tbtrfs_set_token_##bits(eb, s, offsetof(type, member), val, token); \\\n}\n\n#define BTRFS_SETGET_HEADER_FUNCS(name, type, member, bits)\t\t\\\nstatic inline u##bits btrfs_##name(struct extent_buffer *eb)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *p = page_address(eb->pages[0]);\t\t\t\t\\\n\tu##bits res = le##bits##_to_cpu(p->member);\t\t\t\\\n\treturn res;\t\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_##name(struct extent_buffer *eb,\t\t\\\n\t\t\t\t    u##bits val)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ttype *p = page_address(eb->pages[0]);\t\t\t\t\\\n\tp->member = cpu_to_le##bits(val);\t\t\t\t\\\n}\n\n#define BTRFS_SETGET_STACK_FUNCS(name, type, member, bits)\t\t\\\nstatic inline u##bits btrfs_##name(type *s)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn le##bits##_to_cpu(s->member);\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void btrfs_set_##name(type *s, u##bits val)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\ts->member = cpu_to_le##bits(val);\t\t\t\t\\\n}\n\nBTRFS_SETGET_FUNCS(device_type, struct btrfs_dev_item, type, 64);\nBTRFS_SETGET_FUNCS(device_total_bytes, struct btrfs_dev_item, total_bytes, 64);\nBTRFS_SETGET_FUNCS(device_bytes_used, struct btrfs_dev_item, bytes_used, 64);\nBTRFS_SETGET_FUNCS(device_io_align, struct btrfs_dev_item, io_align, 32);\nBTRFS_SETGET_FUNCS(device_io_width, struct btrfs_dev_item, io_width, 32);\nBTRFS_SETGET_FUNCS(device_start_offset, struct btrfs_dev_item,\n\t\t   start_offset, 64);\nBTRFS_SETGET_FUNCS(device_sector_size, struct btrfs_dev_item, sector_size, 32);\nBTRFS_SETGET_FUNCS(device_id, struct btrfs_dev_item, devid, 64);\nBTRFS_SETGET_FUNCS(device_group, struct btrfs_dev_item, dev_group, 32);\nBTRFS_SETGET_FUNCS(device_seek_speed, struct btrfs_dev_item, seek_speed, 8);\nBTRFS_SETGET_FUNCS(device_bandwidth, struct btrfs_dev_item, bandwidth, 8);\nBTRFS_SETGET_FUNCS(device_generation, struct btrfs_dev_item, generation, 64);\n\nBTRFS_SETGET_STACK_FUNCS(stack_device_type, struct btrfs_dev_item, type, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_device_total_bytes, struct btrfs_dev_item,\n\t\t\t total_bytes, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_device_bytes_used, struct btrfs_dev_item,\n\t\t\t bytes_used, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_device_io_align, struct btrfs_dev_item,\n\t\t\t io_align, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_device_io_width, struct btrfs_dev_item,\n\t\t\t io_width, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_device_sector_size, struct btrfs_dev_item,\n\t\t\t sector_size, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_device_id, struct btrfs_dev_item, devid, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_device_group, struct btrfs_dev_item,\n\t\t\t dev_group, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_device_seek_speed, struct btrfs_dev_item,\n\t\t\t seek_speed, 8);\nBTRFS_SETGET_STACK_FUNCS(stack_device_bandwidth, struct btrfs_dev_item,\n\t\t\t bandwidth, 8);\nBTRFS_SETGET_STACK_FUNCS(stack_device_generation, struct btrfs_dev_item,\n\t\t\t generation, 64);\n\nstatic inline char *btrfs_device_uuid(struct btrfs_dev_item *d)\n{\n\treturn (char *)d + offsetof(struct btrfs_dev_item, uuid);\n}\n\nstatic inline char *btrfs_device_fsid(struct btrfs_dev_item *d)\n{\n\treturn (char *)d + offsetof(struct btrfs_dev_item, fsid);\n}\n\nBTRFS_SETGET_FUNCS(chunk_length, struct btrfs_chunk, length, 64);\nBTRFS_SETGET_FUNCS(chunk_owner, struct btrfs_chunk, owner, 64);\nBTRFS_SETGET_FUNCS(chunk_stripe_len, struct btrfs_chunk, stripe_len, 64);\nBTRFS_SETGET_FUNCS(chunk_io_align, struct btrfs_chunk, io_align, 32);\nBTRFS_SETGET_FUNCS(chunk_io_width, struct btrfs_chunk, io_width, 32);\nBTRFS_SETGET_FUNCS(chunk_sector_size, struct btrfs_chunk, sector_size, 32);\nBTRFS_SETGET_FUNCS(chunk_type, struct btrfs_chunk, type, 64);\nBTRFS_SETGET_FUNCS(chunk_num_stripes, struct btrfs_chunk, num_stripes, 16);\nBTRFS_SETGET_FUNCS(chunk_sub_stripes, struct btrfs_chunk, sub_stripes, 16);\nBTRFS_SETGET_FUNCS(stripe_devid, struct btrfs_stripe, devid, 64);\nBTRFS_SETGET_FUNCS(stripe_offset, struct btrfs_stripe, offset, 64);\n\nstatic inline char *btrfs_stripe_dev_uuid(struct btrfs_stripe *s)\n{\n\treturn (char *)s + offsetof(struct btrfs_stripe, dev_uuid);\n}\n\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_length, struct btrfs_chunk, length, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_owner, struct btrfs_chunk, owner, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_stripe_len, struct btrfs_chunk,\n\t\t\t stripe_len, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_io_align, struct btrfs_chunk,\n\t\t\t io_align, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_io_width, struct btrfs_chunk,\n\t\t\t io_width, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_sector_size, struct btrfs_chunk,\n\t\t\t sector_size, 32);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_type, struct btrfs_chunk, type, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_num_stripes, struct btrfs_chunk,\n\t\t\t num_stripes, 16);\nBTRFS_SETGET_STACK_FUNCS(stack_chunk_sub_stripes, struct btrfs_chunk,\n\t\t\t sub_stripes, 16);\nBTRFS_SETGET_STACK_FUNCS(stack_stripe_devid, struct btrfs_stripe, devid, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_stripe_offset, struct btrfs_stripe, offset, 64);\n\nstatic inline struct btrfs_stripe *btrfs_stripe_nr(struct btrfs_chunk *c,\n\t\t\t\t\t\t   int nr)\n{\n\tunsigned long offset = (unsigned long)c;\n\toffset += offsetof(struct btrfs_chunk, stripe);\n\toffset += nr * sizeof(struct btrfs_stripe);\n\treturn (struct btrfs_stripe *)offset;\n}\n\nstatic inline char *btrfs_stripe_dev_uuid_nr(struct btrfs_chunk *c, int nr)\n{\n\treturn btrfs_stripe_dev_uuid(btrfs_stripe_nr(c, nr));\n}\n\nstatic inline u64 btrfs_stripe_offset_nr(struct extent_buffer *eb,\n\t\t\t\t\t struct btrfs_chunk *c, int nr)\n{\n\treturn btrfs_stripe_offset(eb, btrfs_stripe_nr(c, nr));\n}\n\nstatic inline u64 btrfs_stripe_devid_nr(struct extent_buffer *eb,\n\t\t\t\t\t struct btrfs_chunk *c, int nr)\n{\n\treturn btrfs_stripe_devid(eb, btrfs_stripe_nr(c, nr));\n}\n\n/* struct btrfs_block_group_item */\nBTRFS_SETGET_STACK_FUNCS(block_group_used, struct btrfs_block_group_item,\n\t\t\t used, 64);\nBTRFS_SETGET_FUNCS(disk_block_group_used, struct btrfs_block_group_item,\n\t\t\t used, 64);\nBTRFS_SETGET_STACK_FUNCS(block_group_chunk_objectid,\n\t\t\tstruct btrfs_block_group_item, chunk_objectid, 64);\n\nBTRFS_SETGET_FUNCS(disk_block_group_chunk_objectid,\n\t\t   struct btrfs_block_group_item, chunk_objectid, 64);\nBTRFS_SETGET_FUNCS(disk_block_group_flags,\n\t\t   struct btrfs_block_group_item, flags, 64);\nBTRFS_SETGET_STACK_FUNCS(block_group_flags,\n\t\t\tstruct btrfs_block_group_item, flags, 64);\n\n/* struct btrfs_inode_ref */\nBTRFS_SETGET_FUNCS(inode_ref_name_len, struct btrfs_inode_ref, name_len, 16);\nBTRFS_SETGET_FUNCS(inode_ref_index, struct btrfs_inode_ref, index, 64);\n\n/* struct btrfs_inode_extref */\nBTRFS_SETGET_FUNCS(inode_extref_parent, struct btrfs_inode_extref,\n\t\t   parent_objectid, 64);\nBTRFS_SETGET_FUNCS(inode_extref_name_len, struct btrfs_inode_extref,\n\t\t   name_len, 16);\nBTRFS_SETGET_FUNCS(inode_extref_index, struct btrfs_inode_extref, index, 64);\n\n/* struct btrfs_inode_item */\nBTRFS_SETGET_FUNCS(inode_generation, struct btrfs_inode_item, generation, 64);\nBTRFS_SETGET_FUNCS(inode_sequence, struct btrfs_inode_item, sequence, 64);\nBTRFS_SETGET_FUNCS(inode_transid, struct btrfs_inode_item, transid, 64);\nBTRFS_SETGET_FUNCS(inode_size, struct btrfs_inode_item, size, 64);\nBTRFS_SETGET_FUNCS(inode_nbytes, struct btrfs_inode_item, nbytes, 64);\nBTRFS_SETGET_FUNCS(inode_block_group, struct btrfs_inode_item, block_group, 64);\nBTRFS_SETGET_FUNCS(inode_nlink, struct btrfs_inode_item, nlink, 32);\nBTRFS_SETGET_FUNCS(inode_uid, struct btrfs_inode_item, uid, 32);\nBTRFS_SETGET_FUNCS(inode_gid, struct btrfs_inode_item, gid, 32);\nBTRFS_SETGET_FUNCS(inode_mode, struct btrfs_inode_item, mode, 32);\nBTRFS_SETGET_FUNCS(inode_rdev, struct btrfs_inode_item, rdev, 64);\nBTRFS_SETGET_FUNCS(inode_flags, struct btrfs_inode_item, flags, 64);\n\nstatic inline struct btrfs_timespec *\nbtrfs_inode_atime(struct btrfs_inode_item *inode_item)\n{\n\tunsigned long ptr = (unsigned long)inode_item;\n\tptr += offsetof(struct btrfs_inode_item, atime);\n\treturn (struct btrfs_timespec *)ptr;\n}\n\nstatic inline struct btrfs_timespec *\nbtrfs_inode_mtime(struct btrfs_inode_item *inode_item)\n{\n\tunsigned long ptr = (unsigned long)inode_item;\n\tptr += offsetof(struct btrfs_inode_item, mtime);\n\treturn (struct btrfs_timespec *)ptr;\n}\n\nstatic inline struct btrfs_timespec *\nbtrfs_inode_ctime(struct btrfs_inode_item *inode_item)\n{\n\tunsigned long ptr = (unsigned long)inode_item;\n\tptr += offsetof(struct btrfs_inode_item, ctime);\n\treturn (struct btrfs_timespec *)ptr;\n}\n\nBTRFS_SETGET_FUNCS(timespec_sec, struct btrfs_timespec, sec, 64);\nBTRFS_SETGET_FUNCS(timespec_nsec, struct btrfs_timespec, nsec, 32);\n\n/* struct btrfs_dev_extent */\nBTRFS_SETGET_FUNCS(dev_extent_chunk_tree, struct btrfs_dev_extent,\n\t\t   chunk_tree, 64);\nBTRFS_SETGET_FUNCS(dev_extent_chunk_objectid, struct btrfs_dev_extent,\n\t\t   chunk_objectid, 64);\nBTRFS_SETGET_FUNCS(dev_extent_chunk_offset, struct btrfs_dev_extent,\n\t\t   chunk_offset, 64);\nBTRFS_SETGET_FUNCS(dev_extent_length, struct btrfs_dev_extent, length, 64);\n\nstatic inline u8 *btrfs_dev_extent_chunk_tree_uuid(struct btrfs_dev_extent *dev)\n{\n\tunsigned long ptr = offsetof(struct btrfs_dev_extent, chunk_tree_uuid);\n\treturn (u8 *)((unsigned long)dev + ptr);\n}\n\nBTRFS_SETGET_FUNCS(extent_refs, struct btrfs_extent_item, refs, 64);\nBTRFS_SETGET_FUNCS(extent_generation, struct btrfs_extent_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(extent_flags, struct btrfs_extent_item, flags, 64);\n\nBTRFS_SETGET_FUNCS(extent_refs_v0, struct btrfs_extent_item_v0, refs, 32);\n\n\nBTRFS_SETGET_FUNCS(tree_block_level, struct btrfs_tree_block_info, level, 8);\n\nstatic inline void btrfs_tree_block_key(struct extent_buffer *eb,\n\t\t\t\t\tstruct btrfs_tree_block_info *item,\n\t\t\t\t\tstruct btrfs_disk_key *key)\n{\n\tread_eb_member(eb, item, struct btrfs_tree_block_info, key, key);\n}\n\nstatic inline void btrfs_set_tree_block_key(struct extent_buffer *eb,\n\t\t\t\t\t    struct btrfs_tree_block_info *item,\n\t\t\t\t\t    struct btrfs_disk_key *key)\n{\n\twrite_eb_member(eb, item, struct btrfs_tree_block_info, key, key);\n}\n\nBTRFS_SETGET_FUNCS(extent_data_ref_root, struct btrfs_extent_data_ref,\n\t\t   root, 64);\nBTRFS_SETGET_FUNCS(extent_data_ref_objectid, struct btrfs_extent_data_ref,\n\t\t   objectid, 64);\nBTRFS_SETGET_FUNCS(extent_data_ref_offset, struct btrfs_extent_data_ref,\n\t\t   offset, 64);\nBTRFS_SETGET_FUNCS(extent_data_ref_count, struct btrfs_extent_data_ref,\n\t\t   count, 32);\n\nBTRFS_SETGET_FUNCS(shared_data_ref_count, struct btrfs_shared_data_ref,\n\t\t   count, 32);\n\nBTRFS_SETGET_FUNCS(extent_inline_ref_type, struct btrfs_extent_inline_ref,\n\t\t   type, 8);\nBTRFS_SETGET_FUNCS(extent_inline_ref_offset, struct btrfs_extent_inline_ref,\n\t\t   offset, 64);\n\nstatic inline u32 btrfs_extent_inline_ref_size(int type)\n{\n\tif (type == BTRFS_TREE_BLOCK_REF_KEY ||\n\t    type == BTRFS_SHARED_BLOCK_REF_KEY)\n\t\treturn sizeof(struct btrfs_extent_inline_ref);\n\tif (type == BTRFS_SHARED_DATA_REF_KEY)\n\t\treturn sizeof(struct btrfs_shared_data_ref) +\n\t\t       sizeof(struct btrfs_extent_inline_ref);\n\tif (type == BTRFS_EXTENT_DATA_REF_KEY)\n\t\treturn sizeof(struct btrfs_extent_data_ref) +\n\t\t       offsetof(struct btrfs_extent_inline_ref, offset);\n\tBUG();\n\treturn 0;\n}\n\nBTRFS_SETGET_FUNCS(ref_root_v0, struct btrfs_extent_ref_v0, root, 64);\nBTRFS_SETGET_FUNCS(ref_generation_v0, struct btrfs_extent_ref_v0,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(ref_objectid_v0, struct btrfs_extent_ref_v0, objectid, 64);\nBTRFS_SETGET_FUNCS(ref_count_v0, struct btrfs_extent_ref_v0, count, 32);\n\n/* struct btrfs_node */\nBTRFS_SETGET_FUNCS(key_blockptr, struct btrfs_key_ptr, blockptr, 64);\nBTRFS_SETGET_FUNCS(key_generation, struct btrfs_key_ptr, generation, 64);\n\nstatic inline u64 btrfs_node_blockptr(struct extent_buffer *eb, int nr)\n{\n\tunsigned long ptr;\n\tptr = offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n\treturn btrfs_key_blockptr(eb, (struct btrfs_key_ptr *)ptr);\n}\n\nstatic inline void btrfs_set_node_blockptr(struct extent_buffer *eb,\n\t\t\t\t\t   int nr, u64 val)\n{\n\tunsigned long ptr;\n\tptr = offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n\tbtrfs_set_key_blockptr(eb, (struct btrfs_key_ptr *)ptr, val);\n}\n\nstatic inline u64 btrfs_node_ptr_generation(struct extent_buffer *eb, int nr)\n{\n\tunsigned long ptr;\n\tptr = offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n\treturn btrfs_key_generation(eb, (struct btrfs_key_ptr *)ptr);\n}\n\nstatic inline void btrfs_set_node_ptr_generation(struct extent_buffer *eb,\n\t\t\t\t\t\t int nr, u64 val)\n{\n\tunsigned long ptr;\n\tptr = offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n\tbtrfs_set_key_generation(eb, (struct btrfs_key_ptr *)ptr, val);\n}\n\nstatic inline unsigned long btrfs_node_key_ptr_offset(int nr)\n{\n\treturn offsetof(struct btrfs_node, ptrs) +\n\t\tsizeof(struct btrfs_key_ptr) * nr;\n}\n\nvoid btrfs_node_key(struct extent_buffer *eb,\n\t\t    struct btrfs_disk_key *disk_key, int nr);\n\nstatic inline void btrfs_set_node_key(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_disk_key *disk_key, int nr)\n{\n\tunsigned long ptr;\n\tptr = btrfs_node_key_ptr_offset(nr);\n\twrite_eb_member(eb, (struct btrfs_key_ptr *)ptr,\n\t\t       struct btrfs_key_ptr, key, disk_key);\n}\n\n/* struct btrfs_item */\nBTRFS_SETGET_FUNCS(item_offset, struct btrfs_item, offset, 32);\nBTRFS_SETGET_FUNCS(item_size, struct btrfs_item, size, 32);\n\nstatic inline unsigned long btrfs_item_nr_offset(int nr)\n{\n\treturn offsetof(struct btrfs_leaf, items) +\n\t\tsizeof(struct btrfs_item) * nr;\n}\n\nstatic inline struct btrfs_item *btrfs_item_nr(struct extent_buffer *eb,\n\t\t\t\t\t       int nr)\n{\n\treturn (struct btrfs_item *)btrfs_item_nr_offset(nr);\n}\n\nstatic inline u32 btrfs_item_end(struct extent_buffer *eb,\n\t\t\t\t struct btrfs_item *item)\n{\n\treturn btrfs_item_offset(eb, item) + btrfs_item_size(eb, item);\n}\n\nstatic inline u32 btrfs_item_end_nr(struct extent_buffer *eb, int nr)\n{\n\treturn btrfs_item_end(eb, btrfs_item_nr(eb, nr));\n}\n\nstatic inline u32 btrfs_item_offset_nr(struct extent_buffer *eb, int nr)\n{\n\treturn btrfs_item_offset(eb, btrfs_item_nr(eb, nr));\n}\n\nstatic inline u32 btrfs_item_size_nr(struct extent_buffer *eb, int nr)\n{\n\treturn btrfs_item_size(eb, btrfs_item_nr(eb, nr));\n}\n\nstatic inline void btrfs_item_key(struct extent_buffer *eb,\n\t\t\t   struct btrfs_disk_key *disk_key, int nr)\n{\n\tstruct btrfs_item *item = btrfs_item_nr(eb, nr);\n\tread_eb_member(eb, item, struct btrfs_item, key, disk_key);\n}\n\nstatic inline void btrfs_set_item_key(struct extent_buffer *eb,\n\t\t\t       struct btrfs_disk_key *disk_key, int nr)\n{\n\tstruct btrfs_item *item = btrfs_item_nr(eb, nr);\n\twrite_eb_member(eb, item, struct btrfs_item, key, disk_key);\n}\n\nBTRFS_SETGET_FUNCS(dir_log_end, struct btrfs_dir_log_item, end, 64);\n\n/*\n * struct btrfs_root_ref\n */\nBTRFS_SETGET_FUNCS(root_ref_dirid, struct btrfs_root_ref, dirid, 64);\nBTRFS_SETGET_FUNCS(root_ref_sequence, struct btrfs_root_ref, sequence, 64);\nBTRFS_SETGET_FUNCS(root_ref_name_len, struct btrfs_root_ref, name_len, 16);\n\n/* struct btrfs_dir_item */\nBTRFS_SETGET_FUNCS(dir_data_len, struct btrfs_dir_item, data_len, 16);\nBTRFS_SETGET_FUNCS(dir_type, struct btrfs_dir_item, type, 8);\nBTRFS_SETGET_FUNCS(dir_name_len, struct btrfs_dir_item, name_len, 16);\nBTRFS_SETGET_FUNCS(dir_transid, struct btrfs_dir_item, transid, 64);\n\nstatic inline void btrfs_dir_item_key(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_dir_item *item,\n\t\t\t\t      struct btrfs_disk_key *key)\n{\n\tread_eb_member(eb, item, struct btrfs_dir_item, location, key);\n}\n\nstatic inline void btrfs_set_dir_item_key(struct extent_buffer *eb,\n\t\t\t\t\t  struct btrfs_dir_item *item,\n\t\t\t\t\t  struct btrfs_disk_key *key)\n{\n\twrite_eb_member(eb, item, struct btrfs_dir_item, location, key);\n}\n\nBTRFS_SETGET_FUNCS(free_space_entries, struct btrfs_free_space_header,\n\t\t   num_entries, 64);\nBTRFS_SETGET_FUNCS(free_space_bitmaps, struct btrfs_free_space_header,\n\t\t   num_bitmaps, 64);\nBTRFS_SETGET_FUNCS(free_space_generation, struct btrfs_free_space_header,\n\t\t   generation, 64);\n\nstatic inline void btrfs_free_space_key(struct extent_buffer *eb,\n\t\t\t\t\tstruct btrfs_free_space_header *h,\n\t\t\t\t\tstruct btrfs_disk_key *key)\n{\n\tread_eb_member(eb, h, struct btrfs_free_space_header, location, key);\n}\n\nstatic inline void btrfs_set_free_space_key(struct extent_buffer *eb,\n\t\t\t\t\t    struct btrfs_free_space_header *h,\n\t\t\t\t\t    struct btrfs_disk_key *key)\n{\n\twrite_eb_member(eb, h, struct btrfs_free_space_header, location, key);\n}\n\n/* struct btrfs_disk_key */\nBTRFS_SETGET_STACK_FUNCS(disk_key_objectid, struct btrfs_disk_key,\n\t\t\t objectid, 64);\nBTRFS_SETGET_STACK_FUNCS(disk_key_offset, struct btrfs_disk_key, offset, 64);\nBTRFS_SETGET_STACK_FUNCS(disk_key_type, struct btrfs_disk_key, type, 8);\n\nstatic inline void btrfs_disk_key_to_cpu(struct btrfs_key *cpu,\n\t\t\t\t\t struct btrfs_disk_key *disk)\n{\n\tcpu->offset = le64_to_cpu(disk->offset);\n\tcpu->type = disk->type;\n\tcpu->objectid = le64_to_cpu(disk->objectid);\n}\n\nstatic inline void btrfs_cpu_key_to_disk(struct btrfs_disk_key *disk,\n\t\t\t\t\t struct btrfs_key *cpu)\n{\n\tdisk->offset = cpu_to_le64(cpu->offset);\n\tdisk->type = cpu->type;\n\tdisk->objectid = cpu_to_le64(cpu->objectid);\n}\n\nstatic inline void btrfs_node_key_to_cpu(struct extent_buffer *eb,\n\t\t\t\t  struct btrfs_key *key, int nr)\n{\n\tstruct btrfs_disk_key disk_key;\n\tbtrfs_node_key(eb, &disk_key, nr);\n\tbtrfs_disk_key_to_cpu(key, &disk_key);\n}\n\nstatic inline void btrfs_item_key_to_cpu(struct extent_buffer *eb,\n\t\t\t\t  struct btrfs_key *key, int nr)\n{\n\tstruct btrfs_disk_key disk_key;\n\tbtrfs_item_key(eb, &disk_key, nr);\n\tbtrfs_disk_key_to_cpu(key, &disk_key);\n}\n\nstatic inline void btrfs_dir_item_key_to_cpu(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_dir_item *item,\n\t\t\t\t      struct btrfs_key *key)\n{\n\tstruct btrfs_disk_key disk_key;\n\tbtrfs_dir_item_key(eb, item, &disk_key);\n\tbtrfs_disk_key_to_cpu(key, &disk_key);\n}\n\n\nstatic inline u8 btrfs_key_type(struct btrfs_key *key)\n{\n\treturn key->type;\n}\n\nstatic inline void btrfs_set_key_type(struct btrfs_key *key, u8 val)\n{\n\tkey->type = val;\n}\n\n/* struct btrfs_header */\nBTRFS_SETGET_HEADER_FUNCS(header_bytenr, struct btrfs_header, bytenr, 64);\nBTRFS_SETGET_HEADER_FUNCS(header_generation, struct btrfs_header,\n\t\t\t  generation, 64);\nBTRFS_SETGET_HEADER_FUNCS(header_owner, struct btrfs_header, owner, 64);\nBTRFS_SETGET_HEADER_FUNCS(header_nritems, struct btrfs_header, nritems, 32);\nBTRFS_SETGET_HEADER_FUNCS(header_flags, struct btrfs_header, flags, 64);\nBTRFS_SETGET_HEADER_FUNCS(header_level, struct btrfs_header, level, 8);\n\nstatic inline int btrfs_header_flag(struct extent_buffer *eb, u64 flag)\n{\n\treturn (btrfs_header_flags(eb) & flag) == flag;\n}\n\nstatic inline int btrfs_set_header_flag(struct extent_buffer *eb, u64 flag)\n{\n\tu64 flags = btrfs_header_flags(eb);\n\tbtrfs_set_header_flags(eb, flags | flag);\n\treturn (flags & flag) == flag;\n}\n\nstatic inline int btrfs_clear_header_flag(struct extent_buffer *eb, u64 flag)\n{\n\tu64 flags = btrfs_header_flags(eb);\n\tbtrfs_set_header_flags(eb, flags & ~flag);\n\treturn (flags & flag) == flag;\n}\n\nstatic inline int btrfs_header_backref_rev(struct extent_buffer *eb)\n{\n\tu64 flags = btrfs_header_flags(eb);\n\treturn flags >> BTRFS_BACKREF_REV_SHIFT;\n}\n\nstatic inline void btrfs_set_header_backref_rev(struct extent_buffer *eb,\n\t\t\t\t\t\tint rev)\n{\n\tu64 flags = btrfs_header_flags(eb);\n\tflags &= ~BTRFS_BACKREF_REV_MASK;\n\tflags |= (u64)rev << BTRFS_BACKREF_REV_SHIFT;\n\tbtrfs_set_header_flags(eb, flags);\n}\n\nstatic inline u8 *btrfs_header_fsid(struct extent_buffer *eb)\n{\n\tunsigned long ptr = offsetof(struct btrfs_header, fsid);\n\treturn (u8 *)ptr;\n}\n\nstatic inline u8 *btrfs_header_chunk_tree_uuid(struct extent_buffer *eb)\n{\n\tunsigned long ptr = offsetof(struct btrfs_header, chunk_tree_uuid);\n\treturn (u8 *)ptr;\n}\n\nstatic inline int btrfs_is_leaf(struct extent_buffer *eb)\n{\n\treturn btrfs_header_level(eb) == 0;\n}\n\n/* struct btrfs_root_item */\nBTRFS_SETGET_FUNCS(disk_root_generation, struct btrfs_root_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(disk_root_refs, struct btrfs_root_item, refs, 32);\nBTRFS_SETGET_FUNCS(disk_root_bytenr, struct btrfs_root_item, bytenr, 64);\nBTRFS_SETGET_FUNCS(disk_root_level, struct btrfs_root_item, level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(root_generation, struct btrfs_root_item,\n\t\t\t generation, 64);\nBTRFS_SETGET_STACK_FUNCS(root_bytenr, struct btrfs_root_item, bytenr, 64);\nBTRFS_SETGET_STACK_FUNCS(root_level, struct btrfs_root_item, level, 8);\nBTRFS_SETGET_STACK_FUNCS(root_dirid, struct btrfs_root_item, root_dirid, 64);\nBTRFS_SETGET_STACK_FUNCS(root_refs, struct btrfs_root_item, refs, 32);\nBTRFS_SETGET_STACK_FUNCS(root_flags, struct btrfs_root_item, flags, 64);\nBTRFS_SETGET_STACK_FUNCS(root_used, struct btrfs_root_item, bytes_used, 64);\nBTRFS_SETGET_STACK_FUNCS(root_limit, struct btrfs_root_item, byte_limit, 64);\nBTRFS_SETGET_STACK_FUNCS(root_last_snapshot, struct btrfs_root_item,\n\t\t\t last_snapshot, 64);\nBTRFS_SETGET_STACK_FUNCS(root_generation_v2, struct btrfs_root_item,\n\t\t\t generation_v2, 64);\nBTRFS_SETGET_STACK_FUNCS(root_ctransid, struct btrfs_root_item,\n\t\t\t ctransid, 64);\nBTRFS_SETGET_STACK_FUNCS(root_otransid, struct btrfs_root_item,\n\t\t\t otransid, 64);\nBTRFS_SETGET_STACK_FUNCS(root_stransid, struct btrfs_root_item,\n\t\t\t stransid, 64);\nBTRFS_SETGET_STACK_FUNCS(root_rtransid, struct btrfs_root_item,\n\t\t\t rtransid, 64);\n\nstatic inline bool btrfs_root_readonly(struct btrfs_root *root)\n{\n\treturn (root->root_item.flags & cpu_to_le64(BTRFS_ROOT_SUBVOL_RDONLY)) != 0;\n}\n\n/* struct btrfs_root_backup */\nBTRFS_SETGET_STACK_FUNCS(backup_tree_root, struct btrfs_root_backup,\n\t\t   tree_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_tree_root_gen, struct btrfs_root_backup,\n\t\t   tree_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_tree_root_level, struct btrfs_root_backup,\n\t\t   tree_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_chunk_root, struct btrfs_root_backup,\n\t\t   chunk_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_chunk_root_gen, struct btrfs_root_backup,\n\t\t   chunk_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_chunk_root_level, struct btrfs_root_backup,\n\t\t   chunk_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_extent_root, struct btrfs_root_backup,\n\t\t   extent_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_extent_root_gen, struct btrfs_root_backup,\n\t\t   extent_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_extent_root_level, struct btrfs_root_backup,\n\t\t   extent_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_fs_root, struct btrfs_root_backup,\n\t\t   fs_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_fs_root_gen, struct btrfs_root_backup,\n\t\t   fs_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_fs_root_level, struct btrfs_root_backup,\n\t\t   fs_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_dev_root, struct btrfs_root_backup,\n\t\t   dev_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_dev_root_gen, struct btrfs_root_backup,\n\t\t   dev_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_dev_root_level, struct btrfs_root_backup,\n\t\t   dev_root_level, 8);\n\nBTRFS_SETGET_STACK_FUNCS(backup_csum_root, struct btrfs_root_backup,\n\t\t   csum_root, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_csum_root_gen, struct btrfs_root_backup,\n\t\t   csum_root_gen, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_csum_root_level, struct btrfs_root_backup,\n\t\t   csum_root_level, 8);\nBTRFS_SETGET_STACK_FUNCS(backup_total_bytes, struct btrfs_root_backup,\n\t\t   total_bytes, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_bytes_used, struct btrfs_root_backup,\n\t\t   bytes_used, 64);\nBTRFS_SETGET_STACK_FUNCS(backup_num_devices, struct btrfs_root_backup,\n\t\t   num_devices, 64);\n\n/* struct btrfs_balance_item */\nBTRFS_SETGET_FUNCS(balance_flags, struct btrfs_balance_item, flags, 64);\n\nstatic inline void btrfs_balance_data(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_balance_item *bi,\n\t\t\t\t      struct btrfs_disk_balance_args *ba)\n{\n\tread_eb_member(eb, bi, struct btrfs_balance_item, data, ba);\n}\n\nstatic inline void btrfs_set_balance_data(struct extent_buffer *eb,\n\t\t\t\t\t  struct btrfs_balance_item *bi,\n\t\t\t\t\t  struct btrfs_disk_balance_args *ba)\n{\n\twrite_eb_member(eb, bi, struct btrfs_balance_item, data, ba);\n}\n\nstatic inline void btrfs_balance_meta(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_balance_item *bi,\n\t\t\t\t      struct btrfs_disk_balance_args *ba)\n{\n\tread_eb_member(eb, bi, struct btrfs_balance_item, meta, ba);\n}\n\nstatic inline void btrfs_set_balance_meta(struct extent_buffer *eb,\n\t\t\t\t\t  struct btrfs_balance_item *bi,\n\t\t\t\t\t  struct btrfs_disk_balance_args *ba)\n{\n\twrite_eb_member(eb, bi, struct btrfs_balance_item, meta, ba);\n}\n\nstatic inline void btrfs_balance_sys(struct extent_buffer *eb,\n\t\t\t\t     struct btrfs_balance_item *bi,\n\t\t\t\t     struct btrfs_disk_balance_args *ba)\n{\n\tread_eb_member(eb, bi, struct btrfs_balance_item, sys, ba);\n}\n\nstatic inline void btrfs_set_balance_sys(struct extent_buffer *eb,\n\t\t\t\t\t struct btrfs_balance_item *bi,\n\t\t\t\t\t struct btrfs_disk_balance_args *ba)\n{\n\twrite_eb_member(eb, bi, struct btrfs_balance_item, sys, ba);\n}\n\nstatic inline void\nbtrfs_disk_balance_args_to_cpu(struct btrfs_balance_args *cpu,\n\t\t\t       struct btrfs_disk_balance_args *disk)\n{\n\tmemset(cpu, 0, sizeof(*cpu));\n\n\tcpu->profiles = le64_to_cpu(disk->profiles);\n\tcpu->usage = le64_to_cpu(disk->usage);\n\tcpu->devid = le64_to_cpu(disk->devid);\n\tcpu->pstart = le64_to_cpu(disk->pstart);\n\tcpu->pend = le64_to_cpu(disk->pend);\n\tcpu->vstart = le64_to_cpu(disk->vstart);\n\tcpu->vend = le64_to_cpu(disk->vend);\n\tcpu->target = le64_to_cpu(disk->target);\n\tcpu->flags = le64_to_cpu(disk->flags);\n}\n\nstatic inline void\nbtrfs_cpu_balance_args_to_disk(struct btrfs_disk_balance_args *disk,\n\t\t\t       struct btrfs_balance_args *cpu)\n{\n\tmemset(disk, 0, sizeof(*disk));\n\n\tdisk->profiles = cpu_to_le64(cpu->profiles);\n\tdisk->usage = cpu_to_le64(cpu->usage);\n\tdisk->devid = cpu_to_le64(cpu->devid);\n\tdisk->pstart = cpu_to_le64(cpu->pstart);\n\tdisk->pend = cpu_to_le64(cpu->pend);\n\tdisk->vstart = cpu_to_le64(cpu->vstart);\n\tdisk->vend = cpu_to_le64(cpu->vend);\n\tdisk->target = cpu_to_le64(cpu->target);\n\tdisk->flags = cpu_to_le64(cpu->flags);\n}\n\n/* struct btrfs_super_block */\nBTRFS_SETGET_STACK_FUNCS(super_bytenr, struct btrfs_super_block, bytenr, 64);\nBTRFS_SETGET_STACK_FUNCS(super_flags, struct btrfs_super_block, flags, 64);\nBTRFS_SETGET_STACK_FUNCS(super_generation, struct btrfs_super_block,\n\t\t\t generation, 64);\nBTRFS_SETGET_STACK_FUNCS(super_root, struct btrfs_super_block, root, 64);\nBTRFS_SETGET_STACK_FUNCS(super_sys_array_size,\n\t\t\t struct btrfs_super_block, sys_chunk_array_size, 32);\nBTRFS_SETGET_STACK_FUNCS(super_chunk_root_generation,\n\t\t\t struct btrfs_super_block, chunk_root_generation, 64);\nBTRFS_SETGET_STACK_FUNCS(super_root_level, struct btrfs_super_block,\n\t\t\t root_level, 8);\nBTRFS_SETGET_STACK_FUNCS(super_chunk_root, struct btrfs_super_block,\n\t\t\t chunk_root, 64);\nBTRFS_SETGET_STACK_FUNCS(super_chunk_root_level, struct btrfs_super_block,\n\t\t\t chunk_root_level, 8);\nBTRFS_SETGET_STACK_FUNCS(super_log_root, struct btrfs_super_block,\n\t\t\t log_root, 64);\nBTRFS_SETGET_STACK_FUNCS(super_log_root_transid, struct btrfs_super_block,\n\t\t\t log_root_transid, 64);\nBTRFS_SETGET_STACK_FUNCS(super_log_root_level, struct btrfs_super_block,\n\t\t\t log_root_level, 8);\nBTRFS_SETGET_STACK_FUNCS(super_total_bytes, struct btrfs_super_block,\n\t\t\t total_bytes, 64);\nBTRFS_SETGET_STACK_FUNCS(super_bytes_used, struct btrfs_super_block,\n\t\t\t bytes_used, 64);\nBTRFS_SETGET_STACK_FUNCS(super_sectorsize, struct btrfs_super_block,\n\t\t\t sectorsize, 32);\nBTRFS_SETGET_STACK_FUNCS(super_nodesize, struct btrfs_super_block,\n\t\t\t nodesize, 32);\nBTRFS_SETGET_STACK_FUNCS(super_leafsize, struct btrfs_super_block,\n\t\t\t leafsize, 32);\nBTRFS_SETGET_STACK_FUNCS(super_stripesize, struct btrfs_super_block,\n\t\t\t stripesize, 32);\nBTRFS_SETGET_STACK_FUNCS(super_root_dir, struct btrfs_super_block,\n\t\t\t root_dir_objectid, 64);\nBTRFS_SETGET_STACK_FUNCS(super_num_devices, struct btrfs_super_block,\n\t\t\t num_devices, 64);\nBTRFS_SETGET_STACK_FUNCS(super_compat_flags, struct btrfs_super_block,\n\t\t\t compat_flags, 64);\nBTRFS_SETGET_STACK_FUNCS(super_compat_ro_flags, struct btrfs_super_block,\n\t\t\t compat_ro_flags, 64);\nBTRFS_SETGET_STACK_FUNCS(super_incompat_flags, struct btrfs_super_block,\n\t\t\t incompat_flags, 64);\nBTRFS_SETGET_STACK_FUNCS(super_csum_type, struct btrfs_super_block,\n\t\t\t csum_type, 16);\nBTRFS_SETGET_STACK_FUNCS(super_cache_generation, struct btrfs_super_block,\n\t\t\t cache_generation, 64);\n\nstatic inline int btrfs_super_csum_size(struct btrfs_super_block *s)\n{\n\tint t = btrfs_super_csum_type(s);\n\tBUG_ON(t >= ARRAY_SIZE(btrfs_csum_sizes));\n\treturn btrfs_csum_sizes[t];\n}\n\nstatic inline unsigned long btrfs_leaf_data(struct extent_buffer *l)\n{\n\treturn offsetof(struct btrfs_leaf, items);\n}\n\n/* struct btrfs_file_extent_item */\nBTRFS_SETGET_FUNCS(file_extent_type, struct btrfs_file_extent_item, type, 8);\n\nstatic inline unsigned long\nbtrfs_file_extent_inline_start(struct btrfs_file_extent_item *e)\n{\n\tunsigned long offset = (unsigned long)e;\n\toffset += offsetof(struct btrfs_file_extent_item, disk_bytenr);\n\treturn offset;\n}\n\nstatic inline u32 btrfs_file_extent_calc_inline_size(u32 datasize)\n{\n\treturn offsetof(struct btrfs_file_extent_item, disk_bytenr) + datasize;\n}\n\nBTRFS_SETGET_FUNCS(file_extent_disk_bytenr, struct btrfs_file_extent_item,\n\t\t   disk_bytenr, 64);\nBTRFS_SETGET_FUNCS(file_extent_generation, struct btrfs_file_extent_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(file_extent_disk_num_bytes, struct btrfs_file_extent_item,\n\t\t   disk_num_bytes, 64);\nBTRFS_SETGET_FUNCS(file_extent_offset, struct btrfs_file_extent_item,\n\t\t  offset, 64);\nBTRFS_SETGET_FUNCS(file_extent_num_bytes, struct btrfs_file_extent_item,\n\t\t   num_bytes, 64);\nBTRFS_SETGET_FUNCS(file_extent_ram_bytes, struct btrfs_file_extent_item,\n\t\t   ram_bytes, 64);\nBTRFS_SETGET_FUNCS(file_extent_compression, struct btrfs_file_extent_item,\n\t\t   compression, 8);\nBTRFS_SETGET_FUNCS(file_extent_encryption, struct btrfs_file_extent_item,\n\t\t   encryption, 8);\nBTRFS_SETGET_FUNCS(file_extent_other_encoding, struct btrfs_file_extent_item,\n\t\t   other_encoding, 16);\n\n/* this returns the number of file bytes represented by the inline item.\n * If an item is compressed, this is the uncompressed size\n */\nstatic inline u32 btrfs_file_extent_inline_len(struct extent_buffer *eb,\n\t\t\t\t\t       struct btrfs_file_extent_item *e)\n{\n\treturn btrfs_file_extent_ram_bytes(eb, e);\n}\n\n/*\n * this returns the number of bytes used by the item on disk, minus the\n * size of any extent headers.  If a file is compressed on disk, this is\n * the compressed size\n */\nstatic inline u32 btrfs_file_extent_inline_item_len(struct extent_buffer *eb,\n\t\t\t\t\t\t    struct btrfs_item *e)\n{\n\tunsigned long offset;\n\toffset = offsetof(struct btrfs_file_extent_item, disk_bytenr);\n\treturn btrfs_item_size(eb, e) - offset;\n}\n\n/* btrfs_dev_stats_item */\nstatic inline u64 btrfs_dev_stats_value(struct extent_buffer *eb,\n\t\t\t\t\tstruct btrfs_dev_stats_item *ptr,\n\t\t\t\t\tint index)\n{\n\tu64 val;\n\n\tread_extent_buffer(eb, &val,\n\t\t\t   offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t    ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t   sizeof(val));\n\treturn val;\n}\n\nstatic inline void btrfs_set_dev_stats_value(struct extent_buffer *eb,\n\t\t\t\t\t     struct btrfs_dev_stats_item *ptr,\n\t\t\t\t\t     int index, u64 val)\n{\n\twrite_extent_buffer(eb, &val,\n\t\t\t    offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t     ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t    sizeof(val));\n}\n\n/* btrfs_qgroup_status_item */\nBTRFS_SETGET_FUNCS(qgroup_status_generation, struct btrfs_qgroup_status_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(qgroup_status_version, struct btrfs_qgroup_status_item,\n\t\t   version, 64);\nBTRFS_SETGET_FUNCS(qgroup_status_flags, struct btrfs_qgroup_status_item,\n\t\t   flags, 64);\nBTRFS_SETGET_FUNCS(qgroup_status_scan, struct btrfs_qgroup_status_item,\n\t\t   scan, 64);\n\n/* btrfs_qgroup_info_item */\nBTRFS_SETGET_FUNCS(qgroup_info_generation, struct btrfs_qgroup_info_item,\n\t\t   generation, 64);\nBTRFS_SETGET_FUNCS(qgroup_info_rfer, struct btrfs_qgroup_info_item, rfer, 64);\nBTRFS_SETGET_FUNCS(qgroup_info_rfer_cmpr, struct btrfs_qgroup_info_item,\n\t\t   rfer_cmpr, 64);\nBTRFS_SETGET_FUNCS(qgroup_info_excl, struct btrfs_qgroup_info_item, excl, 64);\nBTRFS_SETGET_FUNCS(qgroup_info_excl_cmpr, struct btrfs_qgroup_info_item,\n\t\t   excl_cmpr, 64);\n\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_generation,\n\t\t\t struct btrfs_qgroup_info_item, generation, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_rfer, struct btrfs_qgroup_info_item,\n\t\t\t rfer, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_rfer_cmpr,\n\t\t\t struct btrfs_qgroup_info_item, rfer_cmpr, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_excl, struct btrfs_qgroup_info_item,\n\t\t\t excl, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_qgroup_info_excl_cmpr,\n\t\t\t struct btrfs_qgroup_info_item, excl_cmpr, 64);\n\n/* btrfs_qgroup_limit_item */\nBTRFS_SETGET_FUNCS(qgroup_limit_flags, struct btrfs_qgroup_limit_item,\n\t\t   flags, 64);\nBTRFS_SETGET_FUNCS(qgroup_limit_max_rfer, struct btrfs_qgroup_limit_item,\n\t\t   max_rfer, 64);\nBTRFS_SETGET_FUNCS(qgroup_limit_max_excl, struct btrfs_qgroup_limit_item,\n\t\t   max_excl, 64);\nBTRFS_SETGET_FUNCS(qgroup_limit_rsv_rfer, struct btrfs_qgroup_limit_item,\n\t\t   rsv_rfer, 64);\nBTRFS_SETGET_FUNCS(qgroup_limit_rsv_excl, struct btrfs_qgroup_limit_item,\n\t\t   rsv_excl, 64);\n\n/* btrfs_dev_replace_item */\nBTRFS_SETGET_FUNCS(dev_replace_src_devid,\n\t\t   struct btrfs_dev_replace_item, src_devid, 64);\nBTRFS_SETGET_FUNCS(dev_replace_cont_reading_from_srcdev_mode,\n\t\t   struct btrfs_dev_replace_item, cont_reading_from_srcdev_mode,\n\t\t   64);\nBTRFS_SETGET_FUNCS(dev_replace_replace_state, struct btrfs_dev_replace_item,\n\t\t   replace_state, 64);\nBTRFS_SETGET_FUNCS(dev_replace_time_started, struct btrfs_dev_replace_item,\n\t\t   time_started, 64);\nBTRFS_SETGET_FUNCS(dev_replace_time_stopped, struct btrfs_dev_replace_item,\n\t\t   time_stopped, 64);\nBTRFS_SETGET_FUNCS(dev_replace_num_write_errors, struct btrfs_dev_replace_item,\n\t\t   num_write_errors, 64);\nBTRFS_SETGET_FUNCS(dev_replace_num_uncorrectable_read_errors,\n\t\t   struct btrfs_dev_replace_item, num_uncorrectable_read_errors,\n\t\t   64);\nBTRFS_SETGET_FUNCS(dev_replace_cursor_left, struct btrfs_dev_replace_item,\n\t\t   cursor_left, 64);\nBTRFS_SETGET_FUNCS(dev_replace_cursor_right, struct btrfs_dev_replace_item,\n\t\t   cursor_right, 64);\n\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_src_devid,\n\t\t\t struct btrfs_dev_replace_item, src_devid, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_cont_reading_from_srcdev_mode,\n\t\t\t struct btrfs_dev_replace_item,\n\t\t\t cont_reading_from_srcdev_mode, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_replace_state,\n\t\t\t struct btrfs_dev_replace_item, replace_state, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_time_started,\n\t\t\t struct btrfs_dev_replace_item, time_started, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_time_stopped,\n\t\t\t struct btrfs_dev_replace_item, time_stopped, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_num_write_errors,\n\t\t\t struct btrfs_dev_replace_item, num_write_errors, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_num_uncorrectable_read_errors,\n\t\t\t struct btrfs_dev_replace_item,\n\t\t\t num_uncorrectable_read_errors, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_cursor_left,\n\t\t\t struct btrfs_dev_replace_item, cursor_left, 64);\nBTRFS_SETGET_STACK_FUNCS(stack_dev_replace_cursor_right,\n\t\t\t struct btrfs_dev_replace_item, cursor_right, 64);\n\nstatic inline struct btrfs_fs_info *btrfs_sb(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\nstatic inline u32 btrfs_level_size(struct btrfs_root *root, int level)\n{\n\tif (level == 0)\n\t\treturn root->leafsize;\n\treturn root->nodesize;\n}\n\n/* helper function to cast into the data area of the leaf. */\n#define btrfs_item_ptr(leaf, slot, type) \\\n\t((type *)(btrfs_leaf_data(leaf) + \\\n\tbtrfs_item_offset_nr(leaf, slot)))\n\n#define btrfs_item_ptr_offset(leaf, slot) \\\n\t((unsigned long)(btrfs_leaf_data(leaf) + \\\n\tbtrfs_item_offset_nr(leaf, slot)))\n\nstatic inline struct dentry *fdentry(struct file *file)\n{\n\treturn file->f_path.dentry;\n}\n\nstatic inline bool btrfs_mixed_space_info(struct btrfs_space_info *space_info)\n{\n\treturn ((space_info->flags & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t(space_info->flags & BTRFS_BLOCK_GROUP_DATA));\n}\n\nstatic inline gfp_t btrfs_alloc_write_mask(struct address_space *mapping)\n{\n\treturn mapping_gfp_mask(mapping) & ~__GFP_FS;\n}\n\n/* extent-tree.c */\nstatic inline u64 btrfs_calc_trans_metadata_size(struct btrfs_root *root,\n\t\t\t\t\t\t unsigned num_items)\n{\n\treturn (root->leafsize + root->nodesize * (BTRFS_MAX_LEVEL - 1)) *\n\t\t3 * num_items;\n}\n\n/*\n * Doing a truncate won't result in new nodes or leaves, just what we need for\n * COW.\n */\nstatic inline u64 btrfs_calc_trunc_metadata_size(struct btrfs_root *root,\n\t\t\t\t\t\t unsigned num_items)\n{\n\treturn (root->leafsize + root->nodesize * (BTRFS_MAX_LEVEL - 1)) *\n\t\tnum_items;\n}\n\nvoid btrfs_put_block_group(struct btrfs_block_group_cache *cache);\nint btrfs_run_delayed_refs(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root, unsigned long count);\nint btrfs_lookup_extent(struct btrfs_root *root, u64 start, u64 len);\nint btrfs_lookup_extent_info(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root, u64 bytenr,\n\t\t\t     u64 num_bytes, u64 *refs, u64 *flags);\nint btrfs_pin_extent(struct btrfs_root *root,\n\t\t     u64 bytenr, u64 num, int reserved);\nint btrfs_pin_extent_for_log_replay(struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_root *root,\n\t\t\t\t    u64 bytenr, u64 num_bytes);\nint btrfs_cross_ref_exist(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root,\n\t\t\t  u64 objectid, u64 offset, u64 bytenr);\nstruct btrfs_block_group_cache *btrfs_lookup_block_group(\n\t\t\t\t\t\t struct btrfs_fs_info *info,\n\t\t\t\t\t\t u64 bytenr);\nvoid btrfs_put_block_group(struct btrfs_block_group_cache *cache);\nu64 btrfs_find_block_group(struct btrfs_root *root,\n\t\t\t   u64 search_start, u64 search_hint, int owner);\nstruct extent_buffer *btrfs_alloc_free_block(struct btrfs_trans_handle *trans,\n\t\t\t\t\tstruct btrfs_root *root, u32 blocksize,\n\t\t\t\t\tu64 parent, u64 root_objectid,\n\t\t\t\t\tstruct btrfs_disk_key *key, int level,\n\t\t\t\t\tu64 hint, u64 empty_size);\nvoid btrfs_free_tree_block(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   struct extent_buffer *buf,\n\t\t\t   u64 parent, int last_ref);\nstruct extent_buffer *btrfs_init_new_buffer(struct btrfs_trans_handle *trans,\n\t\t\t\t\t    struct btrfs_root *root,\n\t\t\t\t\t    u64 bytenr, u32 blocksize,\n\t\t\t\t\t    int level);\nint btrfs_alloc_reserved_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t     u64 root_objectid, u64 owner,\n\t\t\t\t     u64 offset, struct btrfs_key *ins);\nint btrfs_alloc_logged_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t   u64 root_objectid, u64 owner, u64 offset,\n\t\t\t\t   struct btrfs_key *ins);\nint btrfs_reserve_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t  u64 num_bytes, u64 min_alloc_size,\n\t\t\t\t  u64 empty_size, u64 hint_byte,\n\t\t\t\t  struct btrfs_key *ins, u64 data);\nint btrfs_inc_ref(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t  struct extent_buffer *buf, int full_backref, int for_cow);\nint btrfs_dec_ref(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t  struct extent_buffer *buf, int full_backref, int for_cow);\nint btrfs_set_disk_extent_flags(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root,\n\t\t\t\tu64 bytenr, u64 num_bytes, u64 flags,\n\t\t\t\tint is_data);\nint btrfs_free_extent(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_root *root,\n\t\t      u64 bytenr, u64 num_bytes, u64 parent, u64 root_objectid,\n\t\t      u64 owner, u64 offset, int for_cow);\n\nint btrfs_free_reserved_extent(struct btrfs_root *root, u64 start, u64 len);\nint btrfs_free_and_pin_reserved_extent(struct btrfs_root *root,\n\t\t\t\t       u64 start, u64 len);\nvoid btrfs_prepare_extent_commit(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root);\nint btrfs_finish_extent_commit(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root);\nint btrfs_inc_extent_ref(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_root *root,\n\t\t\t u64 bytenr, u64 num_bytes, u64 parent,\n\t\t\t u64 root_objectid, u64 owner, u64 offset, int for_cow);\n\nint btrfs_write_dirty_block_groups(struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_root *root);\nint btrfs_extent_readonly(struct btrfs_root *root, u64 bytenr);\nint btrfs_free_block_groups(struct btrfs_fs_info *info);\nint btrfs_read_block_groups(struct btrfs_root *root);\nint btrfs_can_relocate(struct btrfs_root *root, u64 bytenr);\nint btrfs_make_block_group(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root, u64 bytes_used,\n\t\t\t   u64 type, u64 chunk_objectid, u64 chunk_offset,\n\t\t\t   u64 size);\nint btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root, u64 group_start);\nvoid btrfs_create_pending_block_groups(struct btrfs_trans_handle *trans,\n\t\t\t\t       struct btrfs_root *root);\nu64 btrfs_reduce_alloc_profile(struct btrfs_root *root, u64 flags);\nu64 btrfs_get_alloc_profile(struct btrfs_root *root, int data);\nvoid btrfs_clear_space_info_full(struct btrfs_fs_info *info);\n\nenum btrfs_reserve_flush_enum {\n\t/* If we are in the transaction, we can't flush anything.*/\n\tBTRFS_RESERVE_NO_FLUSH,\n\t/*\n\t * Flushing delalloc may cause deadlock somewhere, in this\n\t * case, use FLUSH LIMIT\n\t */\n\tBTRFS_RESERVE_FLUSH_LIMIT,\n\tBTRFS_RESERVE_FLUSH_ALL,\n};\n\nint btrfs_check_data_free_space(struct inode *inode, u64 bytes);\nvoid btrfs_free_reserved_data_space(struct inode *inode, u64 bytes);\nvoid btrfs_trans_release_metadata(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root);\nint btrfs_orphan_reserve_metadata(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct inode *inode);\nvoid btrfs_orphan_release_metadata(struct inode *inode);\nint btrfs_snap_reserve_metadata(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_pending_snapshot *pending);\nint btrfs_delalloc_reserve_metadata(struct inode *inode, u64 num_bytes);\nvoid btrfs_delalloc_release_metadata(struct inode *inode, u64 num_bytes);\nint btrfs_delalloc_reserve_space(struct inode *inode, u64 num_bytes);\nvoid btrfs_delalloc_release_space(struct inode *inode, u64 num_bytes);\nvoid btrfs_init_block_rsv(struct btrfs_block_rsv *rsv, unsigned short type);\nstruct btrfs_block_rsv *btrfs_alloc_block_rsv(struct btrfs_root *root,\n\t\t\t\t\t      unsigned short type);\nvoid btrfs_free_block_rsv(struct btrfs_root *root,\n\t\t\t  struct btrfs_block_rsv *rsv);\nint btrfs_block_rsv_add(struct btrfs_root *root,\n\t\t\tstruct btrfs_block_rsv *block_rsv, u64 num_bytes,\n\t\t\tenum btrfs_reserve_flush_enum flush);\nint btrfs_block_rsv_check(struct btrfs_root *root,\n\t\t\t  struct btrfs_block_rsv *block_rsv, int min_factor);\nint btrfs_block_rsv_refill(struct btrfs_root *root,\n\t\t\t   struct btrfs_block_rsv *block_rsv, u64 min_reserved,\n\t\t\t   enum btrfs_reserve_flush_enum flush);\nint btrfs_block_rsv_migrate(struct btrfs_block_rsv *src_rsv,\n\t\t\t    struct btrfs_block_rsv *dst_rsv,\n\t\t\t    u64 num_bytes);\nvoid btrfs_block_rsv_release(struct btrfs_root *root,\n\t\t\t     struct btrfs_block_rsv *block_rsv,\n\t\t\t     u64 num_bytes);\nint btrfs_set_block_group_ro(struct btrfs_root *root,\n\t\t\t     struct btrfs_block_group_cache *cache);\nvoid btrfs_set_block_group_rw(struct btrfs_root *root,\n\t\t\t      struct btrfs_block_group_cache *cache);\nvoid btrfs_put_block_group_cache(struct btrfs_fs_info *info);\nu64 btrfs_account_ro_block_groups_free_space(struct btrfs_space_info *sinfo);\nint btrfs_error_unpin_extent_range(struct btrfs_root *root,\n\t\t\t\t   u64 start, u64 end);\nint btrfs_error_discard_extent(struct btrfs_root *root, u64 bytenr,\n\t\t\t       u64 num_bytes, u64 *actual_bytes);\nint btrfs_force_chunk_alloc(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root, u64 type);\nint btrfs_trim_fs(struct btrfs_root *root, struct fstrim_range *range);\n\nint btrfs_init_space_info(struct btrfs_fs_info *fs_info);\nint btrfs_delayed_refs_qgroup_accounting(struct btrfs_trans_handle *trans,\n\t\t\t\t\t struct btrfs_fs_info *fs_info);\nint __get_raid_index(u64 flags);\n/* ctree.c */\nint btrfs_bin_search(struct extent_buffer *eb, struct btrfs_key *key,\n\t\t     int level, int *slot);\nint btrfs_comp_cpu_keys(struct btrfs_key *k1, struct btrfs_key *k2);\nint btrfs_previous_item(struct btrfs_root *root,\n\t\t\tstruct btrfs_path *path, u64 min_objectid,\n\t\t\tint type);\nvoid btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root, struct btrfs_path *path,\n\t\t\t     struct btrfs_key *new_key);\nstruct extent_buffer *btrfs_root_node(struct btrfs_root *root);\nstruct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root);\nint btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,\n\t\t\tstruct btrfs_key *key, int lowest_level,\n\t\t\tint cache_only, u64 min_trans);\nint btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,\n\t\t\t struct btrfs_key *max_key,\n\t\t\t struct btrfs_path *path, int cache_only,\n\t\t\t u64 min_trans);\nenum btrfs_compare_tree_result {\n\tBTRFS_COMPARE_TREE_NEW,\n\tBTRFS_COMPARE_TREE_DELETED,\n\tBTRFS_COMPARE_TREE_CHANGED,\n};\ntypedef int (*btrfs_changed_cb_t)(struct btrfs_root *left_root,\n\t\t\t\t  struct btrfs_root *right_root,\n\t\t\t\t  struct btrfs_path *left_path,\n\t\t\t\t  struct btrfs_path *right_path,\n\t\t\t\t  struct btrfs_key *key,\n\t\t\t\t  enum btrfs_compare_tree_result result,\n\t\t\t\t  void *ctx);\nint btrfs_compare_trees(struct btrfs_root *left_root,\n\t\t\tstruct btrfs_root *right_root,\n\t\t\tbtrfs_changed_cb_t cb, void *ctx);\nint btrfs_cow_block(struct btrfs_trans_handle *trans,\n\t\t    struct btrfs_root *root, struct extent_buffer *buf,\n\t\t    struct extent_buffer *parent, int parent_slot,\n\t\t    struct extent_buffer **cow_ret);\nint btrfs_copy_root(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_root *root,\n\t\t      struct extent_buffer *buf,\n\t\t      struct extent_buffer **cow_ret, u64 new_root_objectid);\nint btrfs_block_can_be_shared(struct btrfs_root *root,\n\t\t\t      struct extent_buffer *buf);\nvoid btrfs_extend_item(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root, struct btrfs_path *path,\n\t\t       u32 data_size);\nvoid btrfs_truncate_item(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_root *root,\n\t\t\t struct btrfs_path *path,\n\t\t\t u32 new_size, int from_end);\nint btrfs_split_item(struct btrfs_trans_handle *trans,\n\t\t     struct btrfs_root *root,\n\t\t     struct btrfs_path *path,\n\t\t     struct btrfs_key *new_key,\n\t\t     unsigned long split_offset);\nint btrfs_duplicate_item(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_root *root,\n\t\t\t struct btrfs_path *path,\n\t\t\t struct btrfs_key *new_key);\nint btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t      *root, struct btrfs_key *key, struct btrfs_path *p, int\n\t\t      ins_len, int cow);\nint btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,\n\t\t\t  struct btrfs_path *p, u64 time_seq);\nint btrfs_search_slot_for_read(struct btrfs_root *root,\n\t\t\t       struct btrfs_key *key, struct btrfs_path *p,\n\t\t\t       int find_higher, int return_any);\nint btrfs_realloc_node(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root, struct extent_buffer *parent,\n\t\t       int start_slot, int cache_only, u64 *last_ret,\n\t\t       struct btrfs_key *progress);\nvoid btrfs_release_path(struct btrfs_path *p);\nstruct btrfs_path *btrfs_alloc_path(void);\nvoid btrfs_free_path(struct btrfs_path *p);\nvoid btrfs_set_path_blocking(struct btrfs_path *p);\nvoid btrfs_clear_path_blocking(struct btrfs_path *p,\n\t\t\t       struct extent_buffer *held, int held_rw);\nvoid btrfs_unlock_up_safe(struct btrfs_path *p, int level);\n\nint btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t   struct btrfs_path *path, int slot, int nr);\nstatic inline int btrfs_del_item(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root,\n\t\t\t\t struct btrfs_path *path)\n{\n\treturn btrfs_del_items(trans, root, path, path->slots[0], 1);\n}\n\nvoid setup_items_for_insert(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root, struct btrfs_path *path,\n\t\t\t    struct btrfs_key *cpu_key, u32 *data_size,\n\t\t\t    u32 total_data, u32 total_size, int nr);\nint btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t      *root, struct btrfs_key *key, void *data, u32 data_size);\nint btrfs_insert_empty_items(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root,\n\t\t\t     struct btrfs_path *path,\n\t\t\t     struct btrfs_key *cpu_key, u32 *data_size, int nr);\n\nstatic inline int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t  struct btrfs_key *key,\n\t\t\t\t\t  u32 data_size)\n{\n\treturn btrfs_insert_empty_items(trans, root, path, key, &data_size, 1);\n}\n\nint btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path);\nint btrfs_next_leaf_write(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root, struct btrfs_path *path,\n\t\t\t  int del);\nint btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,\n\t\t\tu64 time_seq);\nstatic inline int btrfs_next_old_item(struct btrfs_root *root,\n\t\t\t\t      struct btrfs_path *p, u64 time_seq)\n{\n\t++p->slots[0];\n\tif (p->slots[0] >= btrfs_header_nritems(p->nodes[0]))\n\t\treturn btrfs_next_old_leaf(root, p, time_seq);\n\treturn 0;\n}\nstatic inline int btrfs_next_item(struct btrfs_root *root, struct btrfs_path *p)\n{\n\treturn btrfs_next_old_item(root, p, 0);\n}\nint btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path);\nint btrfs_leaf_free_space(struct btrfs_root *root, struct extent_buffer *leaf);\nint __must_check btrfs_drop_snapshot(struct btrfs_root *root,\n\t\t\t\t     struct btrfs_block_rsv *block_rsv,\n\t\t\t\t     int update_ref, int for_reloc);\nint btrfs_drop_subtree(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root,\n\t\t\tstruct extent_buffer *node,\n\t\t\tstruct extent_buffer *parent);\nstatic inline int btrfs_fs_closing(struct btrfs_fs_info *fs_info)\n{\n\t/*\n\t * Get synced with close_ctree()\n\t */\n\tsmp_mb();\n\treturn fs_info->closing;\n}\nstatic inline void free_fs_info(struct btrfs_fs_info *fs_info)\n{\n\tkfree(fs_info->balance_ctl);\n\tkfree(fs_info->delayed_root);\n\tkfree(fs_info->extent_root);\n\tkfree(fs_info->tree_root);\n\tkfree(fs_info->chunk_root);\n\tkfree(fs_info->dev_root);\n\tkfree(fs_info->csum_root);\n\tkfree(fs_info->quota_root);\n\tkfree(fs_info->super_copy);\n\tkfree(fs_info->super_for_commit);\n\tkfree(fs_info);\n}\n\n/* tree mod log functions from ctree.c */\nu64 btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,\n\t\t\t   struct seq_list *elem);\nvoid btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,\n\t\t\t    struct seq_list *elem);\nstatic inline u64 btrfs_inc_tree_mod_seq(struct btrfs_fs_info *fs_info)\n{\n\treturn atomic_inc_return(&fs_info->tree_mod_seq);\n}\nint btrfs_old_root_level(struct btrfs_root *root, u64 time_seq);\n\n/* root-item.c */\nint btrfs_find_root_ref(struct btrfs_root *tree_root,\n\t\t\tstruct btrfs_path *path,\n\t\t\tu64 root_id, u64 ref_id);\nint btrfs_add_root_ref(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *tree_root,\n\t\t       u64 root_id, u64 ref_id, u64 dirid, u64 sequence,\n\t\t       const char *name, int name_len);\nint btrfs_del_root_ref(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *tree_root,\n\t\t       u64 root_id, u64 ref_id, u64 dirid, u64 *sequence,\n\t\t       const char *name, int name_len);\nint btrfs_del_root(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t   struct btrfs_key *key);\nint btrfs_insert_root(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t      *root, struct btrfs_key *key, struct btrfs_root_item\n\t\t      *item);\nint __must_check btrfs_update_root(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t   struct btrfs_key *key,\n\t\t\t\t   struct btrfs_root_item *item);\nvoid btrfs_read_root_item(struct btrfs_root *root,\n\t\t\t struct extent_buffer *eb, int slot,\n\t\t\t struct btrfs_root_item *item);\nint btrfs_find_last_root(struct btrfs_root *root, u64 objectid, struct\n\t\t\t btrfs_root_item *item, struct btrfs_key *key);\nint btrfs_find_dead_roots(struct btrfs_root *root, u64 objectid);\nint btrfs_find_orphan_roots(struct btrfs_root *tree_root);\nvoid btrfs_set_root_node(struct btrfs_root_item *item,\n\t\t\t struct extent_buffer *node);\nvoid btrfs_check_and_init_root_item(struct btrfs_root_item *item);\nvoid btrfs_update_root_times(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root);\n\n/* dir-item.c */\nint btrfs_check_dir_item_collision(struct btrfs_root *root, u64 dir,\n\t\t\t  const char *name, int name_len);\nint btrfs_insert_dir_item(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root, const char *name,\n\t\t\t  int name_len, struct inode *dir,\n\t\t\t  struct btrfs_key *location, u8 type, u64 index);\nstruct btrfs_dir_item *btrfs_lookup_dir_item(struct btrfs_trans_handle *trans,\n\t\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t\t     struct btrfs_path *path, u64 dir,\n\t\t\t\t\t     const char *name, int name_len,\n\t\t\t\t\t     int mod);\nstruct btrfs_dir_item *\nbtrfs_lookup_dir_index_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 dir,\n\t\t\t    u64 objectid, const char *name, int name_len,\n\t\t\t    int mod);\nstruct btrfs_dir_item *\nbtrfs_search_dir_index_item(struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 dirid,\n\t\t\t    const char *name, int name_len);\nstruct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len);\nint btrfs_delete_one_dir_name(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      struct btrfs_dir_item *di);\nint btrfs_insert_xattr_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 objectid,\n\t\t\t    const char *name, u16 name_len,\n\t\t\t    const void *data, u16 data_len);\nstruct btrfs_dir_item *btrfs_lookup_xattr(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_path *path, u64 dir,\n\t\t\t\t\t  const char *name, u16 name_len,\n\t\t\t\t\t  int mod);\nint verify_dir_item(struct btrfs_root *root,\n\t\t    struct extent_buffer *leaf,\n\t\t    struct btrfs_dir_item *dir_item);\n\n/* orphan.c */\nint btrfs_insert_orphan_item(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root, u64 offset);\nint btrfs_del_orphan_item(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root, u64 offset);\nint btrfs_find_orphan_item(struct btrfs_root *root, u64 offset);\n\n/* inode-item.c */\nint btrfs_insert_inode_ref(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   const char *name, int name_len,\n\t\t\t   u64 inode_objectid, u64 ref_objectid, u64 index);\nint btrfs_del_inode_ref(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   const char *name, int name_len,\n\t\t\t   u64 inode_objectid, u64 ref_objectid, u64 *index);\nint btrfs_get_inode_ref_index(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len,\n\t\t\t      u64 inode_objectid, u64 ref_objectid, int mod,\n\t\t\t      u64 *ret_index);\nint btrfs_insert_empty_inode(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root,\n\t\t\t     struct btrfs_path *path, u64 objectid);\nint btrfs_lookup_inode(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t       *root, struct btrfs_path *path,\n\t\t       struct btrfs_key *location, int mod);\n\nstruct btrfs_inode_extref *\nbtrfs_lookup_inode_extref(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root,\n\t\t\t  struct btrfs_path *path,\n\t\t\t  const char *name, int name_len,\n\t\t\t  u64 inode_objectid, u64 ref_objectid, int ins_len,\n\t\t\t  int cow);\n\nint btrfs_find_name_in_ext_backref(struct btrfs_path *path,\n\t\t\t\t   u64 ref_objectid, const char *name,\n\t\t\t\t   int name_len,\n\t\t\t\t   struct btrfs_inode_extref **extref_ret);\n\n/* file-item.c */\nint btrfs_del_csums(struct btrfs_trans_handle *trans,\n\t\t    struct btrfs_root *root, u64 bytenr, u64 len);\nint btrfs_lookup_bio_sums(struct btrfs_root *root, struct inode *inode,\n\t\t\t  struct bio *bio, u32 *dst);\nint btrfs_lookup_bio_sums_dio(struct btrfs_root *root, struct inode *inode,\n\t\t\t      struct bio *bio, u64 logical_offset);\nint btrfs_insert_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root,\n\t\t\t     u64 objectid, u64 pos,\n\t\t\t     u64 disk_offset, u64 disk_num_bytes,\n\t\t\t     u64 num_bytes, u64 offset, u64 ram_bytes,\n\t\t\t     u8 compression, u8 encryption, u16 other_encoding);\nint btrfs_lookup_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root,\n\t\t\t     struct btrfs_path *path, u64 objectid,\n\t\t\t     u64 bytenr, int mod);\nu64 btrfs_file_extent_length(struct btrfs_path *path);\nint btrfs_csum_file_blocks(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   struct btrfs_ordered_sum *sums);\nint btrfs_csum_one_bio(struct btrfs_root *root, struct inode *inode,\n\t\t       struct bio *bio, u64 file_start, int contig);\nstruct btrfs_csum_item *btrfs_lookup_csum(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t  u64 bytenr, int cow);\nint btrfs_csum_truncate(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root, struct btrfs_path *path,\n\t\t\tu64 isize);\nint btrfs_lookup_csums_range(struct btrfs_root *root, u64 start, u64 end,\n\t\t\t     struct list_head *list, int search_commit);\n/* inode.c */\nstruct btrfs_delalloc_work {\n\tstruct inode *inode;\n\tint wait;\n\tint delay_iput;\n\tstruct completion completion;\n\tstruct list_head list;\n\tstruct btrfs_work work;\n};\n\nstruct btrfs_delalloc_work *btrfs_alloc_delalloc_work(struct inode *inode,\n\t\t\t\t\t\t    int wait, int delay_iput);\nvoid btrfs_wait_and_free_delalloc_work(struct btrfs_delalloc_work *work);\n\nstruct extent_map *btrfs_get_extent_fiemap(struct inode *inode, struct page *page,\n\t\t\t\t\t   size_t pg_offset, u64 start, u64 len,\n\t\t\t\t\t   int create);\n\n/* RHEL and EL kernels have a patch that renames PG_checked to FsMisc */\n#if defined(ClearPageFsMisc) && !defined(ClearPageChecked)\n#define ClearPageChecked ClearPageFsMisc\n#define SetPageChecked SetPageFsMisc\n#define PageChecked PageFsMisc\n#endif\n\n/* This forces readahead on a given range of bytes in an inode */\nstatic inline void btrfs_force_ra(struct address_space *mapping,\n\t\t\t\t  struct file_ra_state *ra, struct file *file,\n\t\t\t\t  pgoff_t offset, unsigned long req_size)\n{\n\tpage_cache_sync_readahead(mapping, ra, file, offset, req_size);\n}\n\nstruct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry);\nint btrfs_set_inode_index(struct inode *dir, u64 *index);\nint btrfs_unlink_inode(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root,\n\t\t       struct inode *dir, struct inode *inode,\n\t\t       const char *name, int name_len);\nint btrfs_add_link(struct btrfs_trans_handle *trans,\n\t\t   struct inode *parent_inode, struct inode *inode,\n\t\t   const char *name, int name_len, int add_backref, u64 index);\nint btrfs_unlink_subvol(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root,\n\t\t\tstruct inode *dir, u64 objectid,\n\t\t\tconst char *name, int name_len);\nint btrfs_truncate_page(struct inode *inode, loff_t from, loff_t len,\n\t\t\tint front);\nint btrfs_truncate_inode_items(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root,\n\t\t\t       struct inode *inode, u64 new_size,\n\t\t\t       u32 min_type);\n\nint btrfs_start_delalloc_inodes(struct btrfs_root *root, int delay_iput);\nint btrfs_set_extent_delalloc(struct inode *inode, u64 start, u64 end,\n\t\t\t      struct extent_state **cached_state);\nint btrfs_writepages(struct address_space *mapping,\n\t\t     struct writeback_control *wbc);\nint btrfs_create_subvol_root(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *new_root, u64 new_dirid);\nint btrfs_merge_bio_hook(struct page *page, unsigned long offset,\n\t\t\t size_t size, struct bio *bio, unsigned long bio_flags);\n\nint btrfs_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);\nint btrfs_readpage(struct file *file, struct page *page);\nvoid btrfs_evict_inode(struct inode *inode);\nint btrfs_write_inode(struct inode *inode, struct writeback_control *wbc);\nint btrfs_dirty_inode(struct inode *inode);\nstruct inode *btrfs_alloc_inode(struct super_block *sb);\nvoid btrfs_destroy_inode(struct inode *inode);\nint btrfs_drop_inode(struct inode *inode);\nint btrfs_init_cachep(void);\nvoid btrfs_destroy_cachep(void);\nlong btrfs_ioctl_trans_end(struct file *file);\nstruct inode *btrfs_iget(struct super_block *s, struct btrfs_key *location,\n\t\t\t struct btrfs_root *root, int *was_new);\nstruct extent_map *btrfs_get_extent(struct inode *inode, struct page *page,\n\t\t\t\t    size_t pg_offset, u64 start, u64 end,\n\t\t\t\t    int create);\nint btrfs_update_inode(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root,\n\t\t\t      struct inode *inode);\nint btrfs_update_inode_fallback(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, struct inode *inode);\nint btrfs_orphan_add(struct btrfs_trans_handle *trans, struct inode *inode);\nint btrfs_orphan_del(struct btrfs_trans_handle *trans, struct inode *inode);\nint btrfs_orphan_cleanup(struct btrfs_root *root);\nvoid btrfs_orphan_commit_root(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root);\nint btrfs_cont_expand(struct inode *inode, loff_t oldsize, loff_t size);\nvoid btrfs_invalidate_inodes(struct btrfs_root *root);\nvoid btrfs_add_delayed_iput(struct inode *inode);\nvoid btrfs_run_delayed_iputs(struct btrfs_root *root);\nint btrfs_prealloc_file_range(struct inode *inode, int mode,\n\t\t\t      u64 start, u64 num_bytes, u64 min_size,\n\t\t\t      loff_t actual_len, u64 *alloc_hint);\nint btrfs_prealloc_file_range_trans(struct inode *inode,\n\t\t\t\t    struct btrfs_trans_handle *trans, int mode,\n\t\t\t\t    u64 start, u64 num_bytes, u64 min_size,\n\t\t\t\t    loff_t actual_len, u64 *alloc_hint);\nextern const struct dentry_operations btrfs_dentry_operations;\n\n/* ioctl.c */\nlong btrfs_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\nvoid btrfs_update_iflags(struct inode *inode);\nvoid btrfs_inherit_iflags(struct inode *inode, struct inode *dir);\nint btrfs_defrag_file(struct inode *inode, struct file *file,\n\t\t      struct btrfs_ioctl_defrag_range_args *range,\n\t\t      u64 newer_than, unsigned long max_pages);\nvoid btrfs_get_block_group_info(struct list_head *groups_list,\n\t\t\t\tstruct btrfs_ioctl_space_info *space);\n\n/* file.c */\nint btrfs_auto_defrag_init(void);\nvoid btrfs_auto_defrag_exit(void);\nint btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,\n\t\t\t   struct inode *inode);\nint btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info);\nvoid btrfs_cleanup_defrag_inodes(struct btrfs_fs_info *fs_info);\nint btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync);\nvoid btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,\n\t\t\t     int skip_pinned);\nint btrfs_replace_extent_cache(struct inode *inode, struct extent_map *replace,\n\t\t\t       u64 start, u64 end, int skip_pinned,\n\t\t\t       int modified);\nextern const struct file_operations btrfs_file_operations;\nint __btrfs_drop_extents(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_root *root, struct inode *inode,\n\t\t\t struct btrfs_path *path, u64 start, u64 end,\n\t\t\t u64 *drop_end, int drop_cache);\nint btrfs_drop_extents(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root, struct inode *inode, u64 start,\n\t\t       u64 end, int drop_cache);\nint btrfs_mark_extent_written(struct btrfs_trans_handle *trans,\n\t\t\t      struct inode *inode, u64 start, u64 end);\nint btrfs_release_file(struct inode *inode, struct file *file);\nvoid btrfs_drop_pages(struct page **pages, size_t num_pages);\nint btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,\n\t\t      struct page **pages, size_t num_pages,\n\t\t      loff_t pos, size_t write_bytes,\n\t\t      struct extent_state **cached);\n\n/* tree-defrag.c */\nint btrfs_defrag_leaves(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root, int cache_only);\n\n/* sysfs.c */\nint btrfs_init_sysfs(void);\nvoid btrfs_exit_sysfs(void);\n\n/* xattr.c */\nssize_t btrfs_listxattr(struct dentry *dentry, char *buffer, size_t size);\n\n/* super.c */\nint btrfs_parse_options(struct btrfs_root *root, char *options);\nint btrfs_sync_fs(struct super_block *sb, int wait);\n\n#ifdef CONFIG_PRINTK\n__printf(2, 3)\nvoid btrfs_printk(struct btrfs_fs_info *fs_info, const char *fmt, ...);\n#else\nstatic inline __printf(2, 3)\nvoid btrfs_printk(struct btrfs_fs_info *fs_info, const char *fmt, ...)\n{\n}\n#endif\n\n__printf(5, 6)\nvoid __btrfs_std_error(struct btrfs_fs_info *fs_info, const char *function,\n\t\t     unsigned int line, int errno, const char *fmt, ...);\n\n\nvoid __btrfs_abort_transaction(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root, const char *function,\n\t\t\t       unsigned int line, int errno);\n\n#define btrfs_set_fs_incompat(__fs_info, opt) \\\n\t__btrfs_set_fs_incompat((__fs_info), BTRFS_FEATURE_INCOMPAT_##opt)\n\nstatic inline void __btrfs_set_fs_incompat(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t   u64 flag)\n{\n\tstruct btrfs_super_block *disk_super;\n\tu64 features;\n\n\tdisk_super = fs_info->super_copy;\n\tfeatures = btrfs_super_incompat_flags(disk_super);\n\tif (!(features & flag)) {\n\t\tfeatures |= flag;\n\t\tbtrfs_set_super_incompat_flags(disk_super, features);\n\t}\n}\n\n/*\n * Call btrfs_abort_transaction as early as possible when an error condition is\n * detected, that way the exact line number is reported.\n */\n\n#define btrfs_abort_transaction(trans, root, errno)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\t__btrfs_abort_transaction(trans, root, __func__,\t\\\n\t\t\t\t  __LINE__, errno);\t\t\\\n} while (0)\n\n#define btrfs_std_error(fs_info, errno)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif ((errno))\t\t\t\t\t\t\\\n\t\t__btrfs_std_error((fs_info), __func__,\t\t\\\n\t\t\t\t   __LINE__, (errno), NULL);\t\\\n} while (0)\n\n#define btrfs_error(fs_info, errno, fmt, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\t__btrfs_std_error((fs_info), __func__, __LINE__,\t\\\n\t\t\t  (errno), fmt, ##args);\t\t\\\n} while (0)\n\n__printf(5, 6)\nvoid __btrfs_panic(struct btrfs_fs_info *fs_info, const char *function,\n\t\t   unsigned int line, int errno, const char *fmt, ...);\n\n#define btrfs_panic(fs_info, errno, fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstruct btrfs_fs_info *_i = (fs_info);\t\t\t\t\\\n\t__btrfs_panic(_i, __func__, __LINE__, errno, fmt, ##args);\t\\\n\tBUG_ON(!(_i->mount_opt & BTRFS_MOUNT_PANIC_ON_FATAL_ERROR));\t\\\n} while (0)\n\n/* acl.c */\n#ifdef CONFIG_BTRFS_FS_POSIX_ACL\nstruct posix_acl *btrfs_get_acl(struct inode *inode, int type);\nint btrfs_init_acl(struct btrfs_trans_handle *trans,\n\t\t   struct inode *inode, struct inode *dir);\nint btrfs_acl_chmod(struct inode *inode);\n#else\n#define btrfs_get_acl NULL\nstatic inline int btrfs_init_acl(struct btrfs_trans_handle *trans,\n\t\t\t\t struct inode *inode, struct inode *dir)\n{\n\treturn 0;\n}\nstatic inline int btrfs_acl_chmod(struct inode *inode)\n{\n\treturn 0;\n}\n#endif\n\n/* relocation.c */\nint btrfs_relocate_block_group(struct btrfs_root *root, u64 group_start);\nint btrfs_init_reloc_root(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root);\nint btrfs_update_reloc_root(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root);\nint btrfs_recover_relocation(struct btrfs_root *root);\nint btrfs_reloc_clone_csums(struct inode *inode, u64 file_pos, u64 len);\nvoid btrfs_reloc_cow_block(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root, struct extent_buffer *buf,\n\t\t\t   struct extent_buffer *cow);\nvoid btrfs_reloc_pre_snapshot(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_pending_snapshot *pending,\n\t\t\t      u64 *bytes_to_reserve);\nint btrfs_reloc_post_snapshot(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_pending_snapshot *pending);\n\n/* scrub.c */\nint btrfs_scrub_dev(struct btrfs_fs_info *fs_info, u64 devid, u64 start,\n\t\t    u64 end, struct btrfs_scrub_progress *progress,\n\t\t    int readonly, int is_dev_replace);\nvoid btrfs_scrub_pause(struct btrfs_root *root);\nvoid btrfs_scrub_pause_super(struct btrfs_root *root);\nvoid btrfs_scrub_continue(struct btrfs_root *root);\nvoid btrfs_scrub_continue_super(struct btrfs_root *root);\nint btrfs_scrub_cancel(struct btrfs_fs_info *info);\nint btrfs_scrub_cancel_dev(struct btrfs_fs_info *info,\n\t\t\t   struct btrfs_device *dev);\nint btrfs_scrub_cancel_devid(struct btrfs_root *root, u64 devid);\nint btrfs_scrub_progress(struct btrfs_root *root, u64 devid,\n\t\t\t struct btrfs_scrub_progress *progress);\n\n/* reada.c */\nstruct reada_control {\n\tstruct btrfs_root\t*root;\t\t/* tree to prefetch */\n\tstruct btrfs_key\tkey_start;\n\tstruct btrfs_key\tkey_end;\t/* exclusive */\n\tatomic_t\t\telems;\n\tstruct kref\t\trefcnt;\n\twait_queue_head_t\twait;\n};\nstruct reada_control *btrfs_reada_add(struct btrfs_root *root,\n\t\t\t      struct btrfs_key *start, struct btrfs_key *end);\nint btrfs_reada_wait(void *handle);\nvoid btrfs_reada_detach(void *handle);\nint btree_readahead_hook(struct btrfs_root *root, struct extent_buffer *eb,\n\t\t\t u64 start, int err);\n\n/* qgroup.c */\nstruct qgroup_update {\n\tstruct list_head list;\n\tstruct btrfs_delayed_ref_node *node;\n\tstruct btrfs_delayed_extent_op *extent_op;\n};\n\nint btrfs_quota_enable(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_fs_info *fs_info);\nint btrfs_quota_disable(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_fs_info *fs_info);\nint btrfs_quota_rescan(struct btrfs_fs_info *fs_info);\nint btrfs_add_qgroup_relation(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_fs_info *fs_info, u64 src, u64 dst);\nint btrfs_del_qgroup_relation(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_fs_info *fs_info, u64 src, u64 dst);\nint btrfs_create_qgroup(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_fs_info *fs_info, u64 qgroupid,\n\t\t\tchar *name);\nint btrfs_remove_qgroup(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_fs_info *fs_info, u64 qgroupid);\nint btrfs_limit_qgroup(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_fs_info *fs_info, u64 qgroupid,\n\t\t       struct btrfs_qgroup_limit *limit);\nint btrfs_read_qgroup_config(struct btrfs_fs_info *fs_info);\nvoid btrfs_free_qgroup_config(struct btrfs_fs_info *fs_info);\nstruct btrfs_delayed_extent_op;\nint btrfs_qgroup_record_ref(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_delayed_ref_node *node,\n\t\t\t    struct btrfs_delayed_extent_op *extent_op);\nint btrfs_qgroup_account_ref(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_fs_info *fs_info,\n\t\t\t     struct btrfs_delayed_ref_node *node,\n\t\t\t     struct btrfs_delayed_extent_op *extent_op);\nint btrfs_run_qgroups(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_fs_info *fs_info);\nint btrfs_qgroup_inherit(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_fs_info *fs_info, u64 srcid, u64 objectid,\n\t\t\t struct btrfs_qgroup_inherit *inherit);\nint btrfs_qgroup_reserve(struct btrfs_root *root, u64 num_bytes);\nvoid btrfs_qgroup_free(struct btrfs_root *root, u64 num_bytes);\n\nvoid assert_qgroups_uptodate(struct btrfs_trans_handle *trans);\n\nstatic inline int is_fstree(u64 rootid)\n{\n\tif (rootid == BTRFS_FS_TREE_OBJECTID ||\n\t    (s64)rootid >= (s64)BTRFS_FIRST_FREE_OBJECTID)\n\t\treturn 1;\n\treturn 0;\n}\n#endif\n", "/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"hash.h\"\n#include \"transaction.h\"\n\n/*\n * insert a name into a directory, doing overflow properly if there is a hash\n * collision.  data_size indicates how big the item inserted should be.  On\n * success a struct btrfs_dir_item pointer is returned, otherwise it is\n * an ERR_PTR.\n *\n * The name is not copied into the dir item, you have to do that yourself.\n */\nstatic struct btrfs_dir_item *insert_with_overflow(struct btrfs_trans_handle\n\t\t\t\t\t\t   *trans,\n\t\t\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t\t\t   struct btrfs_key *cpu_key,\n\t\t\t\t\t\t   u32 data_size,\n\t\t\t\t\t\t   const char *name,\n\t\t\t\t\t\t   int name_len)\n{\n\tint ret;\n\tchar *ptr;\n\tstruct btrfs_item *item;\n\tstruct extent_buffer *leaf;\n\n\tret = btrfs_insert_empty_item(trans, root, path, cpu_key, data_size);\n\tif (ret == -EEXIST) {\n\t\tstruct btrfs_dir_item *di;\n\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n\t\tif (di)\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\tbtrfs_extend_item(trans, root, path, data_size);\n\t} else if (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tWARN_ON(ret > 0);\n\tleaf = path->nodes[0];\n\titem = btrfs_item_nr(leaf, path->slots[0]);\n\tptr = btrfs_item_ptr(leaf, path->slots[0], char);\n\tBUG_ON(data_size > btrfs_item_size(leaf, item));\n\tptr += btrfs_item_size(leaf, item) - data_size;\n\treturn (struct btrfs_dir_item *)ptr;\n}\n\n/*\n * xattrs work a lot like directories, this inserts an xattr item\n * into the tree\n */\nint btrfs_insert_xattr_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 objectid,\n\t\t\t    const char *name, u16 name_len,\n\t\t\t    const void *data, u16 data_len)\n{\n\tint ret = 0;\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr, data_ptr;\n\tstruct btrfs_key key, location;\n\tstruct btrfs_disk_key disk_key;\n\tstruct extent_buffer *leaf;\n\tu32 data_size;\n\n\tBUG_ON(name_len + data_len > BTRFS_MAX_XATTR_SIZE(root));\n\n\tkey.objectid = objectid;\n\tbtrfs_set_key_type(&key, BTRFS_XATTR_ITEM_KEY);\n\tkey.offset = btrfs_name_hash(name, name_len);\n\n\tdata_size = sizeof(*dir_item) + name_len + data_len;\n\tdir_item = insert_with_overflow(trans, root, path, &key, data_size,\n\t\t\t\t\tname, name_len);\n\tif (IS_ERR(dir_item))\n\t\treturn PTR_ERR(dir_item);\n\tmemset(&location, 0, sizeof(location));\n\n\tleaf = path->nodes[0];\n\tbtrfs_cpu_key_to_disk(&disk_key, &location);\n\tbtrfs_set_dir_item_key(leaf, dir_item, &disk_key);\n\tbtrfs_set_dir_type(leaf, dir_item, BTRFS_FT_XATTR);\n\tbtrfs_set_dir_name_len(leaf, dir_item, name_len);\n\tbtrfs_set_dir_transid(leaf, dir_item, trans->transid);\n\tbtrfs_set_dir_data_len(leaf, dir_item, data_len);\n\tname_ptr = (unsigned long)(dir_item + 1);\n\tdata_ptr = (unsigned long)((char *)name_ptr + name_len);\n\n\twrite_extent_buffer(leaf, name, name_ptr, name_len);\n\twrite_extent_buffer(leaf, data, data_ptr, data_len);\n\tbtrfs_mark_buffer_dirty(path->nodes[0]);\n\n\treturn ret;\n}\n\n/*\n * insert a directory item in the tree, doing all the magic for\n * both indexes. 'dir' indicates which objectid to insert it into,\n * 'location' is the key to stuff into the directory item, 'type' is the\n * type of the inode we're pointing to, and 'index' is the sequence number\n * to use for the second index (if one is created).\n * Will return 0 or -ENOMEM\n */\nint btrfs_insert_dir_item(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t\t  *root, const char *name, int name_len,\n\t\t\t  struct inode *dir, struct btrfs_key *location,\n\t\t\t  u8 type, u64 index)\n{\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dir_item *dir_item;\n\tstruct extent_buffer *leaf;\n\tunsigned long name_ptr;\n\tstruct btrfs_key key;\n\tstruct btrfs_disk_key disk_key;\n\tu32 data_size;\n\n\tkey.objectid = btrfs_ino(dir);\n\tbtrfs_set_key_type(&key, BTRFS_DIR_ITEM_KEY);\n\tkey.offset = btrfs_name_hash(name, name_len);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tpath->leave_spinning = 1;\n\n\tbtrfs_cpu_key_to_disk(&disk_key, location);\n\n\tdata_size = sizeof(*dir_item) + name_len;\n\tdir_item = insert_with_overflow(trans, root, path, &key, data_size,\n\t\t\t\t\tname, name_len);\n\tif (IS_ERR(dir_item)) {\n\t\tret = PTR_ERR(dir_item);\n\t\tif (ret == -EEXIST)\n\t\t\tgoto second_insert;\n\t\tgoto out_free;\n\t}\n\n\tleaf = path->nodes[0];\n\tbtrfs_set_dir_item_key(leaf, dir_item, &disk_key);\n\tbtrfs_set_dir_type(leaf, dir_item, type);\n\tbtrfs_set_dir_data_len(leaf, dir_item, 0);\n\tbtrfs_set_dir_name_len(leaf, dir_item, name_len);\n\tbtrfs_set_dir_transid(leaf, dir_item, trans->transid);\n\tname_ptr = (unsigned long)(dir_item + 1);\n\n\twrite_extent_buffer(leaf, name, name_ptr, name_len);\n\tbtrfs_mark_buffer_dirty(leaf);\n\nsecond_insert:\n\t/* FIXME, use some real flag for selecting the extra index */\n\tif (root == root->fs_info->tree_root) {\n\t\tret = 0;\n\t\tgoto out_free;\n\t}\n\tbtrfs_release_path(path);\n\n\tret2 = btrfs_insert_delayed_dir_index(trans, root, name, name_len, dir,\n\t\t\t\t\t      &disk_key, type, index);\nout_free:\n\tbtrfs_free_path(path);\n\tif (ret)\n\t\treturn ret;\n\tif (ret2)\n\t\treturn ret2;\n\treturn 0;\n}\n\n/*\n * lookup a directory item based on name.  'dir' is the objectid\n * we're searching in, and 'mod' tells us if you plan on deleting the\n * item (use mod < 0) or changing the options (use mod > 0)\n */\nstruct btrfs_dir_item *btrfs_lookup_dir_item(struct btrfs_trans_handle *trans,\n\t\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t\t     struct btrfs_path *path, u64 dir,\n\t\t\t\t\t     const char *name, int name_len,\n\t\t\t\t\t     int mod)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tint ins_len = mod < 0 ? -1 : 0;\n\tint cow = mod != 0;\n\n\tkey.objectid = dir;\n\tbtrfs_set_key_type(&key, BTRFS_DIR_ITEM_KEY);\n\n\tkey.offset = btrfs_name_hash(name, name_len);\n\n\tret = btrfs_search_slot(trans, root, &key, path, ins_len, cow);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (ret > 0)\n\t\treturn NULL;\n\n\treturn btrfs_match_dir_item_name(root, path, name, name_len);\n}\n\nint btrfs_check_dir_item_collision(struct btrfs_root *root, u64 dir,\n\t\t\t\t   const char *name, int name_len)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tstruct btrfs_dir_item *di;\n\tint data_size;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tstruct btrfs_path *path;\n\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = dir;\n\tbtrfs_set_key_type(&key, BTRFS_DIR_ITEM_KEY);\n\tkey.offset = btrfs_name_hash(name, name_len);\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\n\t/* return back any errors */\n\tif (ret < 0)\n\t\tgoto out;\n\n\t/* nothing found, we're safe */\n\tif (ret > 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\t/* we found an item, look for our name in the item */\n\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n\tif (di) {\n\t\t/* our exact name was found */\n\t\tret = -EEXIST;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * see if there is room in the item to insert this\n\t * name\n\t */\n\tdata_size = sizeof(*di) + name_len + sizeof(struct btrfs_item);\n\tleaf = path->nodes[0];\n\tslot = path->slots[0];\n\tif (data_size + btrfs_item_size_nr(leaf, slot) +\n\t    sizeof(struct btrfs_item) > BTRFS_LEAF_DATA_SIZE(root)) {\n\t\tret = -EOVERFLOW;\n\t} else {\n\t\t/* plenty of insertion room */\n\t\tret = 0;\n\t}\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * lookup a directory item based on index.  'dir' is the objectid\n * we're searching in, and 'mod' tells us if you plan on deleting the\n * item (use mod < 0) or changing the options (use mod > 0)\n *\n * The name is used to make sure the index really points to the name you were\n * looking for.\n */\nstruct btrfs_dir_item *\nbtrfs_lookup_dir_index_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 dir,\n\t\t\t    u64 objectid, const char *name, int name_len,\n\t\t\t    int mod)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tint ins_len = mod < 0 ? -1 : 0;\n\tint cow = mod != 0;\n\n\tkey.objectid = dir;\n\tbtrfs_set_key_type(&key, BTRFS_DIR_INDEX_KEY);\n\tkey.offset = objectid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, ins_len, cow);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (ret > 0)\n\t\treturn ERR_PTR(-ENOENT);\n\treturn btrfs_match_dir_item_name(root, path, name, name_len);\n}\n\nstruct btrfs_dir_item *\nbtrfs_search_dir_index_item(struct btrfs_root *root,\n\t\t\t    struct btrfs_path *path, u64 dirid,\n\t\t\t    const char *name, int name_len)\n{\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_key key;\n\tu32 nritems;\n\tint ret;\n\n\tkey.objectid = dirid;\n\tkey.type = BTRFS_DIR_INDEX_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tleaf = path->nodes[0];\n\tnritems = btrfs_header_nritems(leaf);\n\n\twhile (1) {\n\t\tif (path->slots[0] >= nritems) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ERR_PTR(ret);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tleaf = path->nodes[0];\n\t\t\tnritems = btrfs_header_nritems(leaf);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != dirid || key.type != BTRFS_DIR_INDEX_KEY)\n\t\t\tbreak;\n\n\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n\t\tif (di)\n\t\t\treturn di;\n\n\t\tpath->slots[0]++;\n\t}\n\treturn NULL;\n}\n\nstruct btrfs_dir_item *btrfs_lookup_xattr(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_path *path, u64 dir,\n\t\t\t\t\t  const char *name, u16 name_len,\n\t\t\t\t\t  int mod)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tint ins_len = mod < 0 ? -1 : 0;\n\tint cow = mod != 0;\n\n\tkey.objectid = dir;\n\tbtrfs_set_key_type(&key, BTRFS_XATTR_ITEM_KEY);\n\tkey.offset = btrfs_name_hash(name, name_len);\n\tret = btrfs_search_slot(trans, root, &key, path, ins_len, cow);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (ret > 0)\n\t\treturn NULL;\n\n\treturn btrfs_match_dir_item_name(root, path, name, name_len);\n}\n\n/*\n * helper function to look at the directory item pointed to by 'path'\n * this walks through all the entries in a dir item and finds one\n * for a specific name.\n */\nstruct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len)\n{\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr;\n\tu32 total_len;\n\tu32 cur = 0;\n\tu32 this_len;\n\tstruct extent_buffer *leaf;\n\n\tleaf = path->nodes[0];\n\tdir_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dir_item);\n\tif (verify_dir_item(root, leaf, dir_item))\n\t\treturn NULL;\n\n\ttotal_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\twhile (cur < total_len) {\n\t\tthis_len = sizeof(*dir_item) +\n\t\t\tbtrfs_dir_name_len(leaf, dir_item) +\n\t\t\tbtrfs_dir_data_len(leaf, dir_item);\n\t\tname_ptr = (unsigned long)(dir_item + 1);\n\n\t\tif (btrfs_dir_name_len(leaf, dir_item) == name_len &&\n\t\t    memcmp_extent_buffer(leaf, name, name_ptr, name_len) == 0)\n\t\t\treturn dir_item;\n\n\t\tcur += this_len;\n\t\tdir_item = (struct btrfs_dir_item *)((char *)dir_item +\n\t\t\t\t\t\t     this_len);\n\t}\n\treturn NULL;\n}\n\n/*\n * given a pointer into a directory item, delete it.  This\n * handles items that have more than one entry in them.\n */\nint btrfs_delete_one_dir_name(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      struct btrfs_dir_item *di)\n{\n\n\tstruct extent_buffer *leaf;\n\tu32 sub_item_len;\n\tu32 item_len;\n\tint ret = 0;\n\n\tleaf = path->nodes[0];\n\tsub_item_len = sizeof(*di) + btrfs_dir_name_len(leaf, di) +\n\t\tbtrfs_dir_data_len(leaf, di);\n\titem_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\tif (sub_item_len == item_len) {\n\t\tret = btrfs_del_item(trans, root, path);\n\t} else {\n\t\t/* MARKER */\n\t\tunsigned long ptr = (unsigned long)di;\n\t\tunsigned long start;\n\n\t\tstart = btrfs_item_ptr_offset(leaf, path->slots[0]);\n\t\tmemmove_extent_buffer(leaf, ptr, ptr + sub_item_len,\n\t\t\titem_len - (ptr + sub_item_len - start));\n\t\tbtrfs_truncate_item(trans, root, path,\n\t\t\t\t    item_len - sub_item_len, 1);\n\t}\n\treturn ret;\n}\n\nint verify_dir_item(struct btrfs_root *root,\n\t\t    struct extent_buffer *leaf,\n\t\t    struct btrfs_dir_item *dir_item)\n{\n\tu16 namelen = BTRFS_NAME_LEN;\n\tu8 type = btrfs_dir_type(leaf, dir_item);\n\n\tif (type >= BTRFS_FT_MAX) {\n\t\tprintk(KERN_CRIT \"btrfs: invalid dir item type: %d\\n\",\n\t\t       (int)type);\n\t\treturn 1;\n\t}\n\n\tif (type == BTRFS_FT_XATTR)\n\t\tnamelen = XATTR_NAME_MAX;\n\n\tif (btrfs_dir_name_len(leaf, dir_item) > namelen) {\n\t\tprintk(KERN_CRIT \"btrfs: invalid dir item name len: %u\\n\",\n\t\t       (unsigned)btrfs_dir_data_len(leaf, dir_item));\n\t\treturn 1;\n\t}\n\n\t/* BTRFS_MAX_XATTR_SIZE is the same for all dir items */\n\tif (btrfs_dir_data_len(leaf, dir_item) > BTRFS_MAX_XATTR_SIZE(root)) {\n\t\tprintk(KERN_CRIT \"btrfs: invalid dir item data len: %u\\n\",\n\t\t       (unsigned)btrfs_dir_data_len(leaf, dir_item));\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n", "/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#include <linux/kernel.h>\n#include <linux/bio.h>\n#include <linux/buffer_head.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/time.h>\n#include <linux/init.h>\n#include <linux/string.h>\n#include <linux/backing-dev.h>\n#include <linux/mpage.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/statfs.h>\n#include <linux/compat.h>\n#include <linux/bit_spinlock.h>\n#include <linux/xattr.h>\n#include <linux/posix_acl.h>\n#include <linux/falloc.h>\n#include <linux/slab.h>\n#include <linux/ratelimit.h>\n#include <linux/mount.h>\n#include \"compat.h\"\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"btrfs_inode.h\"\n#include \"ioctl.h\"\n#include \"print-tree.h\"\n#include \"ordered-data.h\"\n#include \"xattr.h\"\n#include \"tree-log.h\"\n#include \"volumes.h\"\n#include \"compression.h\"\n#include \"locking.h\"\n#include \"free-space-cache.h\"\n#include \"inode-map.h\"\n\nstruct btrfs_iget_args {\n\tu64 ino;\n\tstruct btrfs_root *root;\n};\n\nstatic const struct inode_operations btrfs_dir_inode_operations;\nstatic const struct inode_operations btrfs_symlink_inode_operations;\nstatic const struct inode_operations btrfs_dir_ro_inode_operations;\nstatic const struct inode_operations btrfs_special_inode_operations;\nstatic const struct inode_operations btrfs_file_inode_operations;\nstatic const struct address_space_operations btrfs_aops;\nstatic const struct address_space_operations btrfs_symlink_aops;\nstatic const struct file_operations btrfs_dir_file_operations;\nstatic struct extent_io_ops btrfs_extent_io_ops;\n\nstatic struct kmem_cache *btrfs_inode_cachep;\nstatic struct kmem_cache *btrfs_delalloc_work_cachep;\nstruct kmem_cache *btrfs_trans_handle_cachep;\nstruct kmem_cache *btrfs_transaction_cachep;\nstruct kmem_cache *btrfs_path_cachep;\nstruct kmem_cache *btrfs_free_space_cachep;\n\n#define S_SHIFT 12\nstatic unsigned char btrfs_type_by_mode[S_IFMT >> S_SHIFT] = {\n\t[S_IFREG >> S_SHIFT]\t= BTRFS_FT_REG_FILE,\n\t[S_IFDIR >> S_SHIFT]\t= BTRFS_FT_DIR,\n\t[S_IFCHR >> S_SHIFT]\t= BTRFS_FT_CHRDEV,\n\t[S_IFBLK >> S_SHIFT]\t= BTRFS_FT_BLKDEV,\n\t[S_IFIFO >> S_SHIFT]\t= BTRFS_FT_FIFO,\n\t[S_IFSOCK >> S_SHIFT]\t= BTRFS_FT_SOCK,\n\t[S_IFLNK >> S_SHIFT]\t= BTRFS_FT_SYMLINK,\n};\n\nstatic int btrfs_setsize(struct inode *inode, loff_t newsize);\nstatic int btrfs_truncate(struct inode *inode);\nstatic int btrfs_finish_ordered_io(struct btrfs_ordered_extent *ordered_extent);\nstatic noinline int cow_file_range(struct inode *inode,\n\t\t\t\t   struct page *locked_page,\n\t\t\t\t   u64 start, u64 end, int *page_started,\n\t\t\t\t   unsigned long *nr_written, int unlock);\nstatic struct extent_map *create_pinned_em(struct inode *inode, u64 start,\n\t\t\t\t\t   u64 len, u64 orig_start,\n\t\t\t\t\t   u64 block_start, u64 block_len,\n\t\t\t\t\t   u64 orig_block_len, int type);\n\nstatic int btrfs_init_inode_security(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct inode *inode,  struct inode *dir,\n\t\t\t\t     const struct qstr *qstr)\n{\n\tint err;\n\n\terr = btrfs_init_acl(trans, inode, dir);\n\tif (!err)\n\t\terr = btrfs_xattr_security_init(trans, inode, dir, qstr);\n\treturn err;\n}\n\n/*\n * this does all the hard work for inserting an inline extent into\n * the btree.  The caller should have done a btrfs_drop_extents so that\n * no overlapping inline items exist in the btree\n */\nstatic noinline int insert_inline_extent(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, struct inode *inode,\n\t\t\t\tu64 start, size_t size, size_t compressed_size,\n\t\t\t\tint compress_type,\n\t\t\t\tstruct page **compressed_pages)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct page *page = NULL;\n\tchar *kaddr;\n\tunsigned long ptr;\n\tstruct btrfs_file_extent_item *ei;\n\tint err = 0;\n\tint ret;\n\tsize_t cur_size = size;\n\tsize_t datasize;\n\tunsigned long offset;\n\n\tif (compressed_size && compressed_pages)\n\t\tcur_size = compressed_size;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->leave_spinning = 1;\n\n\tkey.objectid = btrfs_ino(inode);\n\tkey.offset = start;\n\tbtrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);\n\tdatasize = btrfs_file_extent_calc_inline_size(cur_size);\n\n\tinode_add_bytes(inode, size);\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      datasize);\n\tif (ret) {\n\t\terr = ret;\n\t\tgoto fail;\n\t}\n\tleaf = path->nodes[0];\n\tei = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t    struct btrfs_file_extent_item);\n\tbtrfs_set_file_extent_generation(leaf, ei, trans->transid);\n\tbtrfs_set_file_extent_type(leaf, ei, BTRFS_FILE_EXTENT_INLINE);\n\tbtrfs_set_file_extent_encryption(leaf, ei, 0);\n\tbtrfs_set_file_extent_other_encoding(leaf, ei, 0);\n\tbtrfs_set_file_extent_ram_bytes(leaf, ei, size);\n\tptr = btrfs_file_extent_inline_start(ei);\n\n\tif (compress_type != BTRFS_COMPRESS_NONE) {\n\t\tstruct page *cpage;\n\t\tint i = 0;\n\t\twhile (compressed_size > 0) {\n\t\t\tcpage = compressed_pages[i];\n\t\t\tcur_size = min_t(unsigned long, compressed_size,\n\t\t\t\t       PAGE_CACHE_SIZE);\n\n\t\t\tkaddr = kmap_atomic(cpage);\n\t\t\twrite_extent_buffer(leaf, kaddr, ptr, cur_size);\n\t\t\tkunmap_atomic(kaddr);\n\n\t\t\ti++;\n\t\t\tptr += cur_size;\n\t\t\tcompressed_size -= cur_size;\n\t\t}\n\t\tbtrfs_set_file_extent_compression(leaf, ei,\n\t\t\t\t\t\t  compress_type);\n\t} else {\n\t\tpage = find_get_page(inode->i_mapping,\n\t\t\t\t     start >> PAGE_CACHE_SHIFT);\n\t\tbtrfs_set_file_extent_compression(leaf, ei, 0);\n\t\tkaddr = kmap_atomic(page);\n\t\toffset = start & (PAGE_CACHE_SIZE - 1);\n\t\twrite_extent_buffer(leaf, kaddr + offset, ptr, size);\n\t\tkunmap_atomic(kaddr);\n\t\tpage_cache_release(page);\n\t}\n\tbtrfs_mark_buffer_dirty(leaf);\n\tbtrfs_free_path(path);\n\n\t/*\n\t * we're an inline extent, so nobody can\n\t * extend the file past i_size without locking\n\t * a page we already have locked.\n\t *\n\t * We must do any isize and inode updates\n\t * before we unlock the pages.  Otherwise we\n\t * could end up racing with unlink.\n\t */\n\tBTRFS_I(inode)->disk_i_size = inode->i_size;\n\tret = btrfs_update_inode(trans, root, inode);\n\n\treturn ret;\nfail:\n\tbtrfs_free_path(path);\n\treturn err;\n}\n\n\n/*\n * conditionally insert an inline extent into the file.  This\n * does the checks required to make sure the data is small enough\n * to fit as an inline extent.\n */\nstatic noinline int cow_file_range_inline(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root,\n\t\t\t\t struct inode *inode, u64 start, u64 end,\n\t\t\t\t size_t compressed_size, int compress_type,\n\t\t\t\t struct page **compressed_pages)\n{\n\tu64 isize = i_size_read(inode);\n\tu64 actual_end = min(end + 1, isize);\n\tu64 inline_len = actual_end - start;\n\tu64 aligned_end = (end + root->sectorsize - 1) &\n\t\t\t~((u64)root->sectorsize - 1);\n\tu64 data_len = inline_len;\n\tint ret;\n\n\tif (compressed_size)\n\t\tdata_len = compressed_size;\n\n\tif (start > 0 ||\n\t    actual_end >= PAGE_CACHE_SIZE ||\n\t    data_len >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||\n\t    (!compressed_size &&\n\t    (actual_end & (root->sectorsize - 1)) == 0) ||\n\t    end + 1 < isize ||\n\t    data_len > root->fs_info->max_inline) {\n\t\treturn 1;\n\t}\n\n\tret = btrfs_drop_extents(trans, root, inode, start, aligned_end, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tif (isize > actual_end)\n\t\tinline_len = min_t(u64, isize, actual_end);\n\tret = insert_inline_extent(trans, root, inode, start,\n\t\t\t\t   inline_len, compressed_size,\n\t\t\t\t   compress_type, compressed_pages);\n\tif (ret && ret != -ENOSPC) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\treturn ret;\n\t} else if (ret == -ENOSPC) {\n\t\treturn 1;\n\t}\n\n\tbtrfs_delalloc_release_metadata(inode, end + 1 - start);\n\tbtrfs_drop_extent_cache(inode, start, aligned_end - 1, 0);\n\treturn 0;\n}\n\nstruct async_extent {\n\tu64 start;\n\tu64 ram_size;\n\tu64 compressed_size;\n\tstruct page **pages;\n\tunsigned long nr_pages;\n\tint compress_type;\n\tstruct list_head list;\n};\n\nstruct async_cow {\n\tstruct inode *inode;\n\tstruct btrfs_root *root;\n\tstruct page *locked_page;\n\tu64 start;\n\tu64 end;\n\tstruct list_head extents;\n\tstruct btrfs_work work;\n};\n\nstatic noinline int add_async_extent(struct async_cow *cow,\n\t\t\t\t     u64 start, u64 ram_size,\n\t\t\t\t     u64 compressed_size,\n\t\t\t\t     struct page **pages,\n\t\t\t\t     unsigned long nr_pages,\n\t\t\t\t     int compress_type)\n{\n\tstruct async_extent *async_extent;\n\n\tasync_extent = kmalloc(sizeof(*async_extent), GFP_NOFS);\n\tBUG_ON(!async_extent); /* -ENOMEM */\n\tasync_extent->start = start;\n\tasync_extent->ram_size = ram_size;\n\tasync_extent->compressed_size = compressed_size;\n\tasync_extent->pages = pages;\n\tasync_extent->nr_pages = nr_pages;\n\tasync_extent->compress_type = compress_type;\n\tlist_add_tail(&async_extent->list, &cow->extents);\n\treturn 0;\n}\n\n/*\n * we create compressed extents in two phases.  The first\n * phase compresses a range of pages that have already been\n * locked (both pages and state bits are locked).\n *\n * This is done inside an ordered work queue, and the compression\n * is spread across many cpus.  The actual IO submission is step\n * two, and the ordered work queue takes care of making sure that\n * happens in the same order things were put onto the queue by\n * writepages and friends.\n *\n * If this code finds it can't get good compression, it puts an\n * entry onto the work queue to write the uncompressed bytes.  This\n * makes sure that both compressed inodes and uncompressed inodes\n * are written in the same order that the flusher thread sent them\n * down.\n */\nstatic noinline int compress_file_range(struct inode *inode,\n\t\t\t\t\tstruct page *locked_page,\n\t\t\t\t\tu64 start, u64 end,\n\t\t\t\t\tstruct async_cow *async_cow,\n\t\t\t\t\tint *num_added)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tu64 num_bytes;\n\tu64 blocksize = root->sectorsize;\n\tu64 actual_end;\n\tu64 isize = i_size_read(inode);\n\tint ret = 0;\n\tstruct page **pages = NULL;\n\tunsigned long nr_pages;\n\tunsigned long nr_pages_ret = 0;\n\tunsigned long total_compressed = 0;\n\tunsigned long total_in = 0;\n\tunsigned long max_compressed = 128 * 1024;\n\tunsigned long max_uncompressed = 128 * 1024;\n\tint i;\n\tint will_compress;\n\tint compress_type = root->fs_info->compress_type;\n\n\t/* if this is a small write inside eof, kick off a defrag */\n\tif ((end - start + 1) < 16 * 1024 &&\n\t    (start > 0 || end + 1 < BTRFS_I(inode)->disk_i_size))\n\t\tbtrfs_add_inode_defrag(NULL, inode);\n\n\tactual_end = min_t(u64, isize, end + 1);\nagain:\n\twill_compress = 0;\n\tnr_pages = (end >> PAGE_CACHE_SHIFT) - (start >> PAGE_CACHE_SHIFT) + 1;\n\tnr_pages = min(nr_pages, (128 * 1024UL) / PAGE_CACHE_SIZE);\n\n\t/*\n\t * we don't want to send crud past the end of i_size through\n\t * compression, that's just a waste of CPU time.  So, if the\n\t * end of the file is before the start of our current\n\t * requested range of bytes, we bail out to the uncompressed\n\t * cleanup code that can deal with all of this.\n\t *\n\t * It isn't really the fastest way to fix things, but this is a\n\t * very uncommon corner.\n\t */\n\tif (actual_end <= start)\n\t\tgoto cleanup_and_bail_uncompressed;\n\n\ttotal_compressed = actual_end - start;\n\n\t/* we want to make sure that amount of ram required to uncompress\n\t * an extent is reasonable, so we limit the total size in ram\n\t * of a compressed extent to 128k.  This is a crucial number\n\t * because it also controls how easily we can spread reads across\n\t * cpus for decompression.\n\t *\n\t * We also want to make sure the amount of IO required to do\n\t * a random read is reasonably small, so we limit the size of\n\t * a compressed extent to 128k.\n\t */\n\ttotal_compressed = min(total_compressed, max_uncompressed);\n\tnum_bytes = (end - start + blocksize) & ~(blocksize - 1);\n\tnum_bytes = max(blocksize,  num_bytes);\n\ttotal_in = 0;\n\tret = 0;\n\n\t/*\n\t * we do compression for mount -o compress and when the\n\t * inode has not been flagged as nocompress.  This flag can\n\t * change at any time if we discover bad compression ratios.\n\t */\n\tif (!(BTRFS_I(inode)->flags & BTRFS_INODE_NOCOMPRESS) &&\n\t    (btrfs_test_opt(root, COMPRESS) ||\n\t     (BTRFS_I(inode)->force_compress) ||\n\t     (BTRFS_I(inode)->flags & BTRFS_INODE_COMPRESS))) {\n\t\tWARN_ON(pages);\n\t\tpages = kzalloc(sizeof(struct page *) * nr_pages, GFP_NOFS);\n\t\tif (!pages) {\n\t\t\t/* just bail out to the uncompressed code */\n\t\t\tgoto cont;\n\t\t}\n\n\t\tif (BTRFS_I(inode)->force_compress)\n\t\t\tcompress_type = BTRFS_I(inode)->force_compress;\n\n\t\tret = btrfs_compress_pages(compress_type,\n\t\t\t\t\t   inode->i_mapping, start,\n\t\t\t\t\t   total_compressed, pages,\n\t\t\t\t\t   nr_pages, &nr_pages_ret,\n\t\t\t\t\t   &total_in,\n\t\t\t\t\t   &total_compressed,\n\t\t\t\t\t   max_compressed);\n\n\t\tif (!ret) {\n\t\t\tunsigned long offset = total_compressed &\n\t\t\t\t(PAGE_CACHE_SIZE - 1);\n\t\t\tstruct page *page = pages[nr_pages_ret - 1];\n\t\t\tchar *kaddr;\n\n\t\t\t/* zero the tail end of the last page, we might be\n\t\t\t * sending it down to disk\n\t\t\t */\n\t\t\tif (offset) {\n\t\t\t\tkaddr = kmap_atomic(page);\n\t\t\t\tmemset(kaddr + offset, 0,\n\t\t\t\t       PAGE_CACHE_SIZE - offset);\n\t\t\t\tkunmap_atomic(kaddr);\n\t\t\t}\n\t\t\twill_compress = 1;\n\t\t}\n\t}\ncont:\n\tif (start == 0) {\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto cleanup_and_out;\n\t\t}\n\t\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\t\t/* lets try to make an inline extent */\n\t\tif (ret || total_in < (actual_end - start)) {\n\t\t\t/* we didn't compress the entire range, try\n\t\t\t * to make an uncompressed inline extent.\n\t\t\t */\n\t\t\tret = cow_file_range_inline(trans, root, inode,\n\t\t\t\t\t\t    start, end, 0, 0, NULL);\n\t\t} else {\n\t\t\t/* try making a compressed inline extent */\n\t\t\tret = cow_file_range_inline(trans, root, inode,\n\t\t\t\t\t\t    start, end,\n\t\t\t\t\t\t    total_compressed,\n\t\t\t\t\t\t    compress_type, pages);\n\t\t}\n\t\tif (ret <= 0) {\n\t\t\t/*\n\t\t\t * inline extent creation worked or returned error,\n\t\t\t * we don't need to create any more async work items.\n\t\t\t * Unlock and free up our temp pages.\n\t\t\t */\n\t\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     start, end, NULL,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE | EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_SET_WRITEBACK | EXTENT_END_WRITEBACK);\n\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tgoto free_pages_out;\n\t\t}\n\t\tbtrfs_end_transaction(trans, root);\n\t}\n\n\tif (will_compress) {\n\t\t/*\n\t\t * we aren't doing an inline extent round the compressed size\n\t\t * up to a block size boundary so the allocator does sane\n\t\t * things\n\t\t */\n\t\ttotal_compressed = (total_compressed + blocksize - 1) &\n\t\t\t~(blocksize - 1);\n\n\t\t/*\n\t\t * one last check to make sure the compression is really a\n\t\t * win, compare the page count read with the blocks on disk\n\t\t */\n\t\ttotal_in = (total_in + PAGE_CACHE_SIZE - 1) &\n\t\t\t~(PAGE_CACHE_SIZE - 1);\n\t\tif (total_compressed >= total_in) {\n\t\t\twill_compress = 0;\n\t\t} else {\n\t\t\tnum_bytes = total_in;\n\t\t}\n\t}\n\tif (!will_compress && pages) {\n\t\t/*\n\t\t * the compression code ran but failed to make things smaller,\n\t\t * free any pages it allocated and our page pointer array\n\t\t */\n\t\tfor (i = 0; i < nr_pages_ret; i++) {\n\t\t\tWARN_ON(pages[i]->mapping);\n\t\t\tpage_cache_release(pages[i]);\n\t\t}\n\t\tkfree(pages);\n\t\tpages = NULL;\n\t\ttotal_compressed = 0;\n\t\tnr_pages_ret = 0;\n\n\t\t/* flag the file so we don't compress in the future */\n\t\tif (!btrfs_test_opt(root, FORCE_COMPRESS) &&\n\t\t    !(BTRFS_I(inode)->force_compress)) {\n\t\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NOCOMPRESS;\n\t\t}\n\t}\n\tif (will_compress) {\n\t\t*num_added += 1;\n\n\t\t/* the async work queues will take care of doing actual\n\t\t * allocation on disk for these compressed pages,\n\t\t * and will submit them to the elevator.\n\t\t */\n\t\tadd_async_extent(async_cow, start, num_bytes,\n\t\t\t\t total_compressed, pages, nr_pages_ret,\n\t\t\t\t compress_type);\n\n\t\tif (start + num_bytes < end) {\n\t\t\tstart += num_bytes;\n\t\t\tpages = NULL;\n\t\t\tcond_resched();\n\t\t\tgoto again;\n\t\t}\n\t} else {\ncleanup_and_bail_uncompressed:\n\t\t/*\n\t\t * No compression, but we still need to write the pages in\n\t\t * the file we've been given so far.  redirty the locked\n\t\t * page if it corresponds to our extent and set things up\n\t\t * for the async work queue to run cow_file_range to do\n\t\t * the normal delalloc dance\n\t\t */\n\t\tif (page_offset(locked_page) >= start &&\n\t\t    page_offset(locked_page) <= end) {\n\t\t\t__set_page_dirty_nobuffers(locked_page);\n\t\t\t/* unlocked later on in the async handlers */\n\t\t}\n\t\tadd_async_extent(async_cow, start, end - start + 1,\n\t\t\t\t 0, NULL, 0, BTRFS_COMPRESS_NONE);\n\t\t*num_added += 1;\n\t}\n\nout:\n\treturn ret;\n\nfree_pages_out:\n\tfor (i = 0; i < nr_pages_ret; i++) {\n\t\tWARN_ON(pages[i]->mapping);\n\t\tpage_cache_release(pages[i]);\n\t}\n\tkfree(pages);\n\n\tgoto out;\n\ncleanup_and_out:\n\textent_clear_unlock_delalloc(inode, &BTRFS_I(inode)->io_tree,\n\t\t\t\t     start, end, NULL,\n\t\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t\t     EXTENT_END_WRITEBACK);\n\tif (!trans || IS_ERR(trans))\n\t\tbtrfs_error(root->fs_info, ret, \"Failed to join transaction\");\n\telse\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\tgoto free_pages_out;\n}\n\n/*\n * phase two of compressed writeback.  This is the ordered portion\n * of the code, which only gets called in the order the work was\n * queued.  We walk all the async extents created by compress_file_range\n * and send them down to the disk.\n */\nstatic noinline int submit_compressed_extents(struct inode *inode,\n\t\t\t\t\t      struct async_cow *async_cow)\n{\n\tstruct async_extent *async_extent;\n\tu64 alloc_hint = 0;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_key ins;\n\tstruct extent_map *em;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree;\n\tint ret = 0;\n\n\tif (list_empty(&async_cow->extents))\n\t\treturn 0;\n\n\n\twhile (!list_empty(&async_cow->extents)) {\n\t\tasync_extent = list_entry(async_cow->extents.next,\n\t\t\t\t\t  struct async_extent, list);\n\t\tlist_del(&async_extent->list);\n\n\t\tio_tree = &BTRFS_I(inode)->io_tree;\n\nretry:\n\t\t/* did the compression code fall back to uncompressed IO? */\n\t\tif (!async_extent->pages) {\n\t\t\tint page_started = 0;\n\t\t\tunsigned long nr_written = 0;\n\n\t\t\tlock_extent(io_tree, async_extent->start,\n\t\t\t\t\t async_extent->start +\n\t\t\t\t\t async_extent->ram_size - 1);\n\n\t\t\t/* allocate blocks */\n\t\t\tret = cow_file_range(inode, async_cow->locked_page,\n\t\t\t\t\t     async_extent->start,\n\t\t\t\t\t     async_extent->start +\n\t\t\t\t\t     async_extent->ram_size - 1,\n\t\t\t\t\t     &page_started, &nr_written, 0);\n\n\t\t\t/* JDM XXX */\n\n\t\t\t/*\n\t\t\t * if page_started, cow_file_range inserted an\n\t\t\t * inline extent and took care of all the unlocking\n\t\t\t * and IO for us.  Otherwise, we need to submit\n\t\t\t * all those pages down to the drive.\n\t\t\t */\n\t\t\tif (!page_started && !ret)\n\t\t\t\textent_write_locked_range(io_tree,\n\t\t\t\t\t\t  inode, async_extent->start,\n\t\t\t\t\t\t  async_extent->start +\n\t\t\t\t\t\t  async_extent->ram_size - 1,\n\t\t\t\t\t\t  btrfs_get_extent,\n\t\t\t\t\t\t  WB_SYNC_ALL);\n\t\t\tkfree(async_extent);\n\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t}\n\n\t\tlock_extent(io_tree, async_extent->start,\n\t\t\t    async_extent->start + async_extent->ram_size - 1);\n\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t} else {\n\t\t\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\t\t\tret = btrfs_reserve_extent(trans, root,\n\t\t\t\t\t   async_extent->compressed_size,\n\t\t\t\t\t   async_extent->compressed_size,\n\t\t\t\t\t   0, alloc_hint, &ins, 1);\n\t\t\tif (ret && ret != -ENOSPC)\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t}\n\n\t\tif (ret) {\n\t\t\tint i;\n\t\t\tfor (i = 0; i < async_extent->nr_pages; i++) {\n\t\t\t\tWARN_ON(async_extent->pages[i]->mapping);\n\t\t\t\tpage_cache_release(async_extent->pages[i]);\n\t\t\t}\n\t\t\tkfree(async_extent->pages);\n\t\t\tasync_extent->nr_pages = 0;\n\t\t\tasync_extent->pages = NULL;\n\t\t\tunlock_extent(io_tree, async_extent->start,\n\t\t\t\t      async_extent->start +\n\t\t\t\t      async_extent->ram_size - 1);\n\t\t\tif (ret == -ENOSPC)\n\t\t\t\tgoto retry;\n\t\t\tgoto out_free; /* JDM: Requeue? */\n\t\t}\n\n\t\t/*\n\t\t * here we're doing allocation and writeback of the\n\t\t * compressed pages\n\t\t */\n\t\tbtrfs_drop_extent_cache(inode, async_extent->start,\n\t\t\t\t\tasync_extent->start +\n\t\t\t\t\tasync_extent->ram_size - 1, 0);\n\n\t\tem = alloc_extent_map();\n\t\tBUG_ON(!em); /* -ENOMEM */\n\t\tem->start = async_extent->start;\n\t\tem->len = async_extent->ram_size;\n\t\tem->orig_start = em->start;\n\n\t\tem->block_start = ins.objectid;\n\t\tem->block_len = ins.offset;\n\t\tem->orig_block_len = ins.offset;\n\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\tem->compress_type = async_extent->compress_type;\n\t\tset_bit(EXTENT_FLAG_PINNED, &em->flags);\n\t\tset_bit(EXTENT_FLAG_COMPRESSED, &em->flags);\n\t\tem->generation = -1;\n\n\t\twhile (1) {\n\t\t\twrite_lock(&em_tree->lock);\n\t\t\tret = add_extent_mapping(em_tree, em);\n\t\t\tif (!ret)\n\t\t\t\tlist_move(&em->list,\n\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\twrite_unlock(&em_tree->lock);\n\t\t\tif (ret != -EEXIST) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbtrfs_drop_extent_cache(inode, async_extent->start,\n\t\t\t\t\t\tasync_extent->start +\n\t\t\t\t\t\tasync_extent->ram_size - 1, 0);\n\t\t}\n\n\t\tret = btrfs_add_ordered_extent_compress(inode,\n\t\t\t\t\t\tasync_extent->start,\n\t\t\t\t\t\tins.objectid,\n\t\t\t\t\t\tasync_extent->ram_size,\n\t\t\t\t\t\tins.offset,\n\t\t\t\t\t\tBTRFS_ORDERED_COMPRESSED,\n\t\t\t\t\t\tasync_extent->compress_type);\n\t\tBUG_ON(ret); /* -ENOMEM */\n\n\t\t/*\n\t\t * clear dirty, set writeback and unlock the pages.\n\t\t */\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t\t&BTRFS_I(inode)->io_tree,\n\t\t\t\tasync_extent->start,\n\t\t\t\tasync_extent->start +\n\t\t\t\tasync_extent->ram_size - 1,\n\t\t\t\tNULL, EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t\tEXTENT_CLEAR_UNLOCK |\n\t\t\t\tEXTENT_CLEAR_DELALLOC |\n\t\t\t\tEXTENT_CLEAR_DIRTY | EXTENT_SET_WRITEBACK);\n\n\t\tret = btrfs_submit_compressed_write(inode,\n\t\t\t\t    async_extent->start,\n\t\t\t\t    async_extent->ram_size,\n\t\t\t\t    ins.objectid,\n\t\t\t\t    ins.offset, async_extent->pages,\n\t\t\t\t    async_extent->nr_pages);\n\n\t\tBUG_ON(ret); /* -ENOMEM */\n\t\talloc_hint = ins.objectid + ins.offset;\n\t\tkfree(async_extent);\n\t\tcond_resched();\n\t}\n\tret = 0;\nout:\n\treturn ret;\nout_free:\n\tkfree(async_extent);\n\tgoto out;\n}\n\nstatic u64 get_extent_allocation_hint(struct inode *inode, u64 start,\n\t\t\t\t      u64 num_bytes)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_map *em;\n\tu64 alloc_hint = 0;\n\n\tread_lock(&em_tree->lock);\n\tem = search_extent_mapping(em_tree, start, num_bytes);\n\tif (em) {\n\t\t/*\n\t\t * if block start isn't an actual block number then find the\n\t\t * first block in this inode and use that as a hint.  If that\n\t\t * block is also bogus then just don't worry about it.\n\t\t */\n\t\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\t\tfree_extent_map(em);\n\t\t\tem = search_extent_mapping(em_tree, 0, 0);\n\t\t\tif (em && em->block_start < EXTENT_MAP_LAST_BYTE)\n\t\t\t\talloc_hint = em->block_start;\n\t\t\tif (em)\n\t\t\t\tfree_extent_map(em);\n\t\t} else {\n\t\t\talloc_hint = em->block_start;\n\t\t\tfree_extent_map(em);\n\t\t}\n\t}\n\tread_unlock(&em_tree->lock);\n\n\treturn alloc_hint;\n}\n\n/*\n * when extent_io.c finds a delayed allocation range in the file,\n * the call backs end up in this code.  The basic idea is to\n * allocate extents on disk for the range, and create ordered data structs\n * in ram to track those extents.\n *\n * locked_page is the page that writepage had locked already.  We use\n * it to make sure we don't do extra locks or unlocks.\n *\n * *page_started is set to one if we unlock locked_page and do everything\n * required to start IO on it.  It may be clean and already done with\n * IO when we return.\n */\nstatic noinline int __cow_file_range(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t     struct page *locked_page,\n\t\t\t\t     u64 start, u64 end, int *page_started,\n\t\t\t\t     unsigned long *nr_written,\n\t\t\t\t     int unlock)\n{\n\tu64 alloc_hint = 0;\n\tu64 num_bytes;\n\tunsigned long ram_size;\n\tu64 disk_num_bytes;\n\tu64 cur_alloc_size;\n\tu64 blocksize = root->sectorsize;\n\tstruct btrfs_key ins;\n\tstruct extent_map *em;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tint ret = 0;\n\n\tBUG_ON(btrfs_is_free_space_inode(inode));\n\n\tnum_bytes = (end - start + blocksize) & ~(blocksize - 1);\n\tnum_bytes = max(blocksize,  num_bytes);\n\tdisk_num_bytes = num_bytes;\n\n\t/* if this is a small write inside eof, kick off defrag */\n\tif (num_bytes < 64 * 1024 &&\n\t    (start > 0 || end + 1 < BTRFS_I(inode)->disk_i_size))\n\t\tbtrfs_add_inode_defrag(trans, inode);\n\n\tif (start == 0) {\n\t\t/* lets try to make an inline extent */\n\t\tret = cow_file_range_inline(trans, root, inode,\n\t\t\t\t\t    start, end, 0, 0, NULL);\n\t\tif (ret == 0) {\n\t\t\textent_clear_unlock_delalloc(inode,\n\t\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t\t     start, end, NULL,\n\t\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t\t     EXTENT_END_WRITEBACK);\n\n\t\t\t*nr_written = *nr_written +\n\t\t\t     (end - start + PAGE_CACHE_SIZE) / PAGE_CACHE_SIZE;\n\t\t\t*page_started = 1;\n\t\t\tgoto out;\n\t\t} else if (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tBUG_ON(disk_num_bytes >\n\t       btrfs_super_total_bytes(root->fs_info->super_copy));\n\n\talloc_hint = get_extent_allocation_hint(inode, start, num_bytes);\n\tbtrfs_drop_extent_cache(inode, start, start + num_bytes - 1, 0);\n\n\twhile (disk_num_bytes > 0) {\n\t\tunsigned long op;\n\n\t\tcur_alloc_size = disk_num_bytes;\n\t\tret = btrfs_reserve_extent(trans, root, cur_alloc_size,\n\t\t\t\t\t   root->sectorsize, 0, alloc_hint,\n\t\t\t\t\t   &ins, 1);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tem = alloc_extent_map();\n\t\tBUG_ON(!em); /* -ENOMEM */\n\t\tem->start = start;\n\t\tem->orig_start = em->start;\n\t\tram_size = ins.offset;\n\t\tem->len = ins.offset;\n\n\t\tem->block_start = ins.objectid;\n\t\tem->block_len = ins.offset;\n\t\tem->orig_block_len = ins.offset;\n\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\tset_bit(EXTENT_FLAG_PINNED, &em->flags);\n\t\tem->generation = -1;\n\n\t\twhile (1) {\n\t\t\twrite_lock(&em_tree->lock);\n\t\t\tret = add_extent_mapping(em_tree, em);\n\t\t\tif (!ret)\n\t\t\t\tlist_move(&em->list,\n\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\twrite_unlock(&em_tree->lock);\n\t\t\tif (ret != -EEXIST) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbtrfs_drop_extent_cache(inode, start,\n\t\t\t\t\t\tstart + ram_size - 1, 0);\n\t\t}\n\n\t\tcur_alloc_size = ins.offset;\n\t\tret = btrfs_add_ordered_extent(inode, start, ins.objectid,\n\t\t\t\t\t       ram_size, cur_alloc_size, 0);\n\t\tBUG_ON(ret); /* -ENOMEM */\n\n\t\tif (root->root_key.objectid ==\n\t\t    BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tret = btrfs_reloc_clone_csums(inode, start,\n\t\t\t\t\t\t      cur_alloc_size);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\tif (disk_num_bytes < cur_alloc_size)\n\t\t\tbreak;\n\n\t\t/* we're not doing compressed IO, don't unlock the first\n\t\t * page (which the caller expects to stay locked), don't\n\t\t * clear any dirty bits and don't set any writeback bits\n\t\t *\n\t\t * Do set the Private2 bit so we know this page was properly\n\t\t * setup for writepage\n\t\t */\n\t\top = unlock ? EXTENT_CLEAR_UNLOCK_PAGE : 0;\n\t\top |= EXTENT_CLEAR_UNLOCK | EXTENT_CLEAR_DELALLOC |\n\t\t\tEXTENT_SET_PRIVATE2;\n\n\t\textent_clear_unlock_delalloc(inode, &BTRFS_I(inode)->io_tree,\n\t\t\t\t\t     start, start + ram_size - 1,\n\t\t\t\t\t     locked_page, op);\n\t\tdisk_num_bytes -= cur_alloc_size;\n\t\tnum_bytes -= cur_alloc_size;\n\t\talloc_hint = ins.objectid + ins.offset;\n\t\tstart += cur_alloc_size;\n\t}\nout:\n\treturn ret;\n\nout_unlock:\n\textent_clear_unlock_delalloc(inode,\n\t\t     &BTRFS_I(inode)->io_tree,\n\t\t     start, end, locked_page,\n\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t     EXTENT_CLEAR_DIRTY |\n\t\t     EXTENT_SET_WRITEBACK |\n\t\t     EXTENT_END_WRITEBACK);\n\n\tgoto out;\n}\n\nstatic noinline int cow_file_range(struct inode *inode,\n\t\t\t\t   struct page *locked_page,\n\t\t\t\t   u64 start, u64 end, int *page_started,\n\t\t\t\t   unsigned long *nr_written,\n\t\t\t\t   int unlock)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     start, end, locked_page,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t     EXTENT_END_WRITEBACK);\n\t\treturn PTR_ERR(trans);\n\t}\n\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\tret = __cow_file_range(trans, inode, root, locked_page, start, end,\n\t\t\t       page_started, nr_written, unlock);\n\n\tbtrfs_end_transaction(trans, root);\n\n\treturn ret;\n}\n\n/*\n * work queue call back to started compression on a file and pages\n */\nstatic noinline void async_cow_start(struct btrfs_work *work)\n{\n\tstruct async_cow *async_cow;\n\tint num_added = 0;\n\tasync_cow = container_of(work, struct async_cow, work);\n\n\tcompress_file_range(async_cow->inode, async_cow->locked_page,\n\t\t\t    async_cow->start, async_cow->end, async_cow,\n\t\t\t    &num_added);\n\tif (num_added == 0) {\n\t\tbtrfs_add_delayed_iput(async_cow->inode);\n\t\tasync_cow->inode = NULL;\n\t}\n}\n\n/*\n * work queue call back to submit previously compressed pages\n */\nstatic noinline void async_cow_submit(struct btrfs_work *work)\n{\n\tstruct async_cow *async_cow;\n\tstruct btrfs_root *root;\n\tunsigned long nr_pages;\n\n\tasync_cow = container_of(work, struct async_cow, work);\n\n\troot = async_cow->root;\n\tnr_pages = (async_cow->end - async_cow->start + PAGE_CACHE_SIZE) >>\n\t\tPAGE_CACHE_SHIFT;\n\n\tif (atomic_sub_return(nr_pages, &root->fs_info->async_delalloc_pages) <\n\t    5 * 1024 * 1024 &&\n\t    waitqueue_active(&root->fs_info->async_submit_wait))\n\t\twake_up(&root->fs_info->async_submit_wait);\n\n\tif (async_cow->inode)\n\t\tsubmit_compressed_extents(async_cow->inode, async_cow);\n}\n\nstatic noinline void async_cow_free(struct btrfs_work *work)\n{\n\tstruct async_cow *async_cow;\n\tasync_cow = container_of(work, struct async_cow, work);\n\tif (async_cow->inode)\n\t\tbtrfs_add_delayed_iput(async_cow->inode);\n\tkfree(async_cow);\n}\n\nstatic int cow_file_range_async(struct inode *inode, struct page *locked_page,\n\t\t\t\tu64 start, u64 end, int *page_started,\n\t\t\t\tunsigned long *nr_written)\n{\n\tstruct async_cow *async_cow;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tunsigned long nr_pages;\n\tu64 cur_end;\n\tint limit = 10 * 1024 * 1024;\n\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, start, end, EXTENT_LOCKED,\n\t\t\t 1, 0, NULL, GFP_NOFS);\n\twhile (start < end) {\n\t\tasync_cow = kmalloc(sizeof(*async_cow), GFP_NOFS);\n\t\tBUG_ON(!async_cow); /* -ENOMEM */\n\t\tasync_cow->inode = igrab(inode);\n\t\tasync_cow->root = root;\n\t\tasync_cow->locked_page = locked_page;\n\t\tasync_cow->start = start;\n\n\t\tif (BTRFS_I(inode)->flags & BTRFS_INODE_NOCOMPRESS)\n\t\t\tcur_end = end;\n\t\telse\n\t\t\tcur_end = min(end, start + 512 * 1024 - 1);\n\n\t\tasync_cow->end = cur_end;\n\t\tINIT_LIST_HEAD(&async_cow->extents);\n\n\t\tasync_cow->work.func = async_cow_start;\n\t\tasync_cow->work.ordered_func = async_cow_submit;\n\t\tasync_cow->work.ordered_free = async_cow_free;\n\t\tasync_cow->work.flags = 0;\n\n\t\tnr_pages = (cur_end - start + PAGE_CACHE_SIZE) >>\n\t\t\tPAGE_CACHE_SHIFT;\n\t\tatomic_add(nr_pages, &root->fs_info->async_delalloc_pages);\n\n\t\tbtrfs_queue_worker(&root->fs_info->delalloc_workers,\n\t\t\t\t   &async_cow->work);\n\n\t\tif (atomic_read(&root->fs_info->async_delalloc_pages) > limit) {\n\t\t\twait_event(root->fs_info->async_submit_wait,\n\t\t\t   (atomic_read(&root->fs_info->async_delalloc_pages) <\n\t\t\t    limit));\n\t\t}\n\n\t\twhile (atomic_read(&root->fs_info->async_submit_draining) &&\n\t\t      atomic_read(&root->fs_info->async_delalloc_pages)) {\n\t\t\twait_event(root->fs_info->async_submit_wait,\n\t\t\t  (atomic_read(&root->fs_info->async_delalloc_pages) ==\n\t\t\t   0));\n\t\t}\n\n\t\t*nr_written += nr_pages;\n\t\tstart = cur_end + 1;\n\t}\n\t*page_started = 1;\n\treturn 0;\n}\n\nstatic noinline int csum_exist_in_range(struct btrfs_root *root,\n\t\t\t\t\tu64 bytenr, u64 num_bytes)\n{\n\tint ret;\n\tstruct btrfs_ordered_sum *sums;\n\tLIST_HEAD(list);\n\n\tret = btrfs_lookup_csums_range(root->fs_info->csum_root, bytenr,\n\t\t\t\t       bytenr + num_bytes - 1, &list, 0);\n\tif (ret == 0 && list_empty(&list))\n\t\treturn 0;\n\n\twhile (!list_empty(&list)) {\n\t\tsums = list_entry(list.next, struct btrfs_ordered_sum, list);\n\t\tlist_del(&sums->list);\n\t\tkfree(sums);\n\t}\n\treturn 1;\n}\n\n/*\n * when nowcow writeback call back.  This checks for snapshots or COW copies\n * of the extents that exist in the file, and COWs the file as required.\n *\n * If no cow copies or snapshots exist, we write directly to the existing\n * blocks on disk\n */\nstatic noinline int run_delalloc_nocow(struct inode *inode,\n\t\t\t\t       struct page *locked_page,\n\t\t\t      u64 start, u64 end, int *page_started, int force,\n\t\t\t      unsigned long *nr_written)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_path *path;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_key found_key;\n\tu64 cow_start;\n\tu64 cur_offset;\n\tu64 extent_end;\n\tu64 extent_offset;\n\tu64 disk_bytenr;\n\tu64 num_bytes;\n\tu64 disk_num_bytes;\n\tint extent_type;\n\tint ret, err;\n\tint type;\n\tint nocow;\n\tint check_prev = 1;\n\tbool nolock;\n\tu64 ino = btrfs_ino(inode);\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     start, end, locked_page,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t     EXTENT_END_WRITEBACK);\n\t\treturn -ENOMEM;\n\t}\n\n\tnolock = btrfs_is_free_space_inode(inode);\n\n\tif (nolock)\n\t\ttrans = btrfs_join_transaction_nolock(root);\n\telse\n\t\ttrans = btrfs_join_transaction(root);\n\n\tif (IS_ERR(trans)) {\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     start, end, locked_page,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t     EXTENT_END_WRITEBACK);\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\tcow_start = (u64)-1;\n\tcur_offset = start;\n\twhile (1) {\n\t\tret = btrfs_lookup_file_extent(trans, root, path, ino,\n\t\t\t\t\t       cur_offset, 0);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto error;\n\t\t}\n\t\tif (ret > 0 && path->slots[0] > 0 && check_prev) {\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &found_key,\n\t\t\t\t\t      path->slots[0] - 1);\n\t\t\tif (found_key.objectid == ino &&\n\t\t\t    found_key.type == BTRFS_EXTENT_DATA_KEY)\n\t\t\t\tpath->slots[0]--;\n\t\t}\n\t\tcheck_prev = 0;\nnext_slot:\n\t\tleaf = path->nodes[0];\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\n\t\tnocow = 0;\n\t\tdisk_bytenr = 0;\n\t\tnum_bytes = 0;\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\tif (found_key.objectid > ino ||\n\t\t    found_key.type > BTRFS_EXTENT_DATA_KEY ||\n\t\t    found_key.offset > end)\n\t\t\tbreak;\n\n\t\tif (found_key.offset > cur_offset) {\n\t\t\textent_end = found_key.offset;\n\t\t\textent_type = 0;\n\t\t\tgoto out_check;\n\t\t}\n\n\t\tfi = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\textent_type = btrfs_file_extent_type(leaf, fi);\n\n\t\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\tdisk_bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);\n\t\t\textent_offset = btrfs_file_extent_offset(leaf, fi);\n\t\t\textent_end = found_key.offset +\n\t\t\t\tbtrfs_file_extent_num_bytes(leaf, fi);\n\t\t\tdisk_num_bytes =\n\t\t\t\tbtrfs_file_extent_disk_num_bytes(leaf, fi);\n\t\t\tif (extent_end <= start) {\n\t\t\t\tpath->slots[0]++;\n\t\t\t\tgoto next_slot;\n\t\t\t}\n\t\t\tif (disk_bytenr == 0)\n\t\t\t\tgoto out_check;\n\t\t\tif (btrfs_file_extent_compression(leaf, fi) ||\n\t\t\t    btrfs_file_extent_encryption(leaf, fi) ||\n\t\t\t    btrfs_file_extent_other_encoding(leaf, fi))\n\t\t\t\tgoto out_check;\n\t\t\tif (extent_type == BTRFS_FILE_EXTENT_REG && !force)\n\t\t\t\tgoto out_check;\n\t\t\tif (btrfs_extent_readonly(root, disk_bytenr))\n\t\t\t\tgoto out_check;\n\t\t\tif (btrfs_cross_ref_exist(trans, root, ino,\n\t\t\t\t\t\t  found_key.offset -\n\t\t\t\t\t\t  extent_offset, disk_bytenr))\n\t\t\t\tgoto out_check;\n\t\t\tdisk_bytenr += extent_offset;\n\t\t\tdisk_bytenr += cur_offset - found_key.offset;\n\t\t\tnum_bytes = min(end + 1, extent_end) - cur_offset;\n\t\t\t/*\n\t\t\t * force cow if csum exists in the range.\n\t\t\t * this ensure that csum for a given extent are\n\t\t\t * either valid or do not exist.\n\t\t\t */\n\t\t\tif (csum_exist_in_range(root, disk_bytenr, num_bytes))\n\t\t\t\tgoto out_check;\n\t\t\tnocow = 1;\n\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\textent_end = found_key.offset +\n\t\t\t\tbtrfs_file_extent_inline_len(leaf, fi);\n\t\t\textent_end = ALIGN(extent_end, root->sectorsize);\n\t\t} else {\n\t\t\tBUG_ON(1);\n\t\t}\nout_check:\n\t\tif (extent_end <= start) {\n\t\t\tpath->slots[0]++;\n\t\t\tgoto next_slot;\n\t\t}\n\t\tif (!nocow) {\n\t\t\tif (cow_start == (u64)-1)\n\t\t\t\tcow_start = cur_offset;\n\t\t\tcur_offset = extent_end;\n\t\t\tif (cur_offset > end)\n\t\t\t\tbreak;\n\t\t\tpath->slots[0]++;\n\t\t\tgoto next_slot;\n\t\t}\n\n\t\tbtrfs_release_path(path);\n\t\tif (cow_start != (u64)-1) {\n\t\t\tret = __cow_file_range(trans, inode, root, locked_page,\n\t\t\t\t\t       cow_start, found_key.offset - 1,\n\t\t\t\t\t       page_started, nr_written, 1);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tcow_start = (u64)-1;\n\t\t}\n\n\t\tif (extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\tstruct extent_map *em;\n\t\t\tstruct extent_map_tree *em_tree;\n\t\t\tem_tree = &BTRFS_I(inode)->extent_tree;\n\t\t\tem = alloc_extent_map();\n\t\t\tBUG_ON(!em); /* -ENOMEM */\n\t\t\tem->start = cur_offset;\n\t\t\tem->orig_start = found_key.offset - extent_offset;\n\t\t\tem->len = num_bytes;\n\t\t\tem->block_len = num_bytes;\n\t\t\tem->block_start = disk_bytenr;\n\t\t\tem->orig_block_len = disk_num_bytes;\n\t\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\t\tset_bit(EXTENT_FLAG_PINNED, &em->flags);\n\t\t\tset_bit(EXTENT_FLAG_FILLING, &em->flags);\n\t\t\tem->generation = -1;\n\t\t\twhile (1) {\n\t\t\t\twrite_lock(&em_tree->lock);\n\t\t\t\tret = add_extent_mapping(em_tree, em);\n\t\t\t\tif (!ret)\n\t\t\t\t\tlist_move(&em->list,\n\t\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\t\twrite_unlock(&em_tree->lock);\n\t\t\t\tif (ret != -EEXIST) {\n\t\t\t\t\tfree_extent_map(em);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbtrfs_drop_extent_cache(inode, em->start,\n\t\t\t\t\t\tem->start + em->len - 1, 0);\n\t\t\t}\n\t\t\ttype = BTRFS_ORDERED_PREALLOC;\n\t\t} else {\n\t\t\ttype = BTRFS_ORDERED_NOCOW;\n\t\t}\n\n\t\tret = btrfs_add_ordered_extent(inode, cur_offset, disk_bytenr,\n\t\t\t\t\t       num_bytes, num_bytes, type);\n\t\tBUG_ON(ret); /* -ENOMEM */\n\n\t\tif (root->root_key.objectid ==\n\t\t    BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tret = btrfs_reloc_clone_csums(inode, cur_offset,\n\t\t\t\t\t\t      num_bytes);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\n\t\textent_clear_unlock_delalloc(inode, &BTRFS_I(inode)->io_tree,\n\t\t\t\tcur_offset, cur_offset + num_bytes - 1,\n\t\t\t\tlocked_page, EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t\tEXTENT_CLEAR_UNLOCK | EXTENT_CLEAR_DELALLOC |\n\t\t\t\tEXTENT_SET_PRIVATE2);\n\t\tcur_offset = extent_end;\n\t\tif (cur_offset > end)\n\t\t\tbreak;\n\t}\n\tbtrfs_release_path(path);\n\n\tif (cur_offset <= end && cow_start == (u64)-1) {\n\t\tcow_start = cur_offset;\n\t\tcur_offset = end;\n\t}\n\n\tif (cow_start != (u64)-1) {\n\t\tret = __cow_file_range(trans, inode, root, locked_page,\n\t\t\t\t       cow_start, end,\n\t\t\t\t       page_started, nr_written, 1);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto error;\n\t\t}\n\t}\n\nerror:\n\terr = btrfs_end_transaction(trans, root);\n\tif (!ret)\n\t\tret = err;\n\n\tif (ret && cur_offset < end)\n\t\textent_clear_unlock_delalloc(inode,\n\t\t\t     &BTRFS_I(inode)->io_tree,\n\t\t\t     cur_offset, end, locked_page,\n\t\t\t     EXTENT_CLEAR_UNLOCK_PAGE |\n\t\t\t     EXTENT_CLEAR_UNLOCK |\n\t\t\t     EXTENT_CLEAR_DELALLOC |\n\t\t\t     EXTENT_CLEAR_DIRTY |\n\t\t\t     EXTENT_SET_WRITEBACK |\n\t\t\t     EXTENT_END_WRITEBACK);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * extent_io.c call back to do delayed allocation processing\n */\nstatic int run_delalloc_range(struct inode *inode, struct page *locked_page,\n\t\t\t      u64 start, u64 end, int *page_started,\n\t\t\t      unsigned long *nr_written)\n{\n\tint ret;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\tif (BTRFS_I(inode)->flags & BTRFS_INODE_NODATACOW) {\n\t\tret = run_delalloc_nocow(inode, locked_page, start, end,\n\t\t\t\t\t page_started, 1, nr_written);\n\t} else if (BTRFS_I(inode)->flags & BTRFS_INODE_PREALLOC) {\n\t\tret = run_delalloc_nocow(inode, locked_page, start, end,\n\t\t\t\t\t page_started, 0, nr_written);\n\t} else if (!btrfs_test_opt(root, COMPRESS) &&\n\t\t   !(BTRFS_I(inode)->force_compress) &&\n\t\t   !(BTRFS_I(inode)->flags & BTRFS_INODE_COMPRESS)) {\n\t\tret = cow_file_range(inode, locked_page, start, end,\n\t\t\t\t      page_started, nr_written, 1);\n\t} else {\n\t\tset_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,\n\t\t\t&BTRFS_I(inode)->runtime_flags);\n\t\tret = cow_file_range_async(inode, locked_page, start, end,\n\t\t\t\t\t   page_started, nr_written);\n\t}\n\treturn ret;\n}\n\nstatic void btrfs_split_extent_hook(struct inode *inode,\n\t\t\t\t    struct extent_state *orig, u64 split)\n{\n\t/* not delalloc, ignore it */\n\tif (!(orig->state & EXTENT_DELALLOC))\n\t\treturn;\n\n\tspin_lock(&BTRFS_I(inode)->lock);\n\tBTRFS_I(inode)->outstanding_extents++;\n\tspin_unlock(&BTRFS_I(inode)->lock);\n}\n\n/*\n * extent_io.c merge_extent_hook, used to track merged delayed allocation\n * extents so we can keep track of new extents that are just merged onto old\n * extents, such as when we are doing sequential writes, so we can properly\n * account for the metadata space we'll need.\n */\nstatic void btrfs_merge_extent_hook(struct inode *inode,\n\t\t\t\t    struct extent_state *new,\n\t\t\t\t    struct extent_state *other)\n{\n\t/* not delalloc, ignore it */\n\tif (!(other->state & EXTENT_DELALLOC))\n\t\treturn;\n\n\tspin_lock(&BTRFS_I(inode)->lock);\n\tBTRFS_I(inode)->outstanding_extents--;\n\tspin_unlock(&BTRFS_I(inode)->lock);\n}\n\n/*\n * extent_io.c set_bit_hook, used to track delayed allocation\n * bytes in this file, and to maintain the list of inodes that\n * have pending delalloc work to be done.\n */\nstatic void btrfs_set_bit_hook(struct inode *inode,\n\t\t\t       struct extent_state *state, int *bits)\n{\n\n\t/*\n\t * set_bit and clear bit hooks normally require _irqsave/restore\n\t * but in this case, we are only testing for the DELALLOC\n\t * bit, which is only set or cleared with irqs on\n\t */\n\tif (!(state->state & EXTENT_DELALLOC) && (*bits & EXTENT_DELALLOC)) {\n\t\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\t\tu64 len = state->end + 1 - state->start;\n\t\tbool do_list = !btrfs_is_free_space_inode(inode);\n\n\t\tif (*bits & EXTENT_FIRST_DELALLOC) {\n\t\t\t*bits &= ~EXTENT_FIRST_DELALLOC;\n\t\t} else {\n\t\t\tspin_lock(&BTRFS_I(inode)->lock);\n\t\t\tBTRFS_I(inode)->outstanding_extents++;\n\t\t\tspin_unlock(&BTRFS_I(inode)->lock);\n\t\t}\n\n\t\tspin_lock(&root->fs_info->delalloc_lock);\n\t\tBTRFS_I(inode)->delalloc_bytes += len;\n\t\troot->fs_info->delalloc_bytes += len;\n\t\tif (do_list && list_empty(&BTRFS_I(inode)->delalloc_inodes)) {\n\t\t\tlist_add_tail(&BTRFS_I(inode)->delalloc_inodes,\n\t\t\t\t      &root->fs_info->delalloc_inodes);\n\t\t}\n\t\tspin_unlock(&root->fs_info->delalloc_lock);\n\t}\n}\n\n/*\n * extent_io.c clear_bit_hook, see set_bit_hook for why\n */\nstatic void btrfs_clear_bit_hook(struct inode *inode,\n\t\t\t\t struct extent_state *state, int *bits)\n{\n\t/*\n\t * set_bit and clear bit hooks normally require _irqsave/restore\n\t * but in this case, we are only testing for the DELALLOC\n\t * bit, which is only set or cleared with irqs on\n\t */\n\tif ((state->state & EXTENT_DELALLOC) && (*bits & EXTENT_DELALLOC)) {\n\t\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\t\tu64 len = state->end + 1 - state->start;\n\t\tbool do_list = !btrfs_is_free_space_inode(inode);\n\n\t\tif (*bits & EXTENT_FIRST_DELALLOC) {\n\t\t\t*bits &= ~EXTENT_FIRST_DELALLOC;\n\t\t} else if (!(*bits & EXTENT_DO_ACCOUNTING)) {\n\t\t\tspin_lock(&BTRFS_I(inode)->lock);\n\t\t\tBTRFS_I(inode)->outstanding_extents--;\n\t\t\tspin_unlock(&BTRFS_I(inode)->lock);\n\t\t}\n\n\t\tif (*bits & EXTENT_DO_ACCOUNTING)\n\t\t\tbtrfs_delalloc_release_metadata(inode, len);\n\n\t\tif (root->root_key.objectid != BTRFS_DATA_RELOC_TREE_OBJECTID\n\t\t    && do_list)\n\t\t\tbtrfs_free_reserved_data_space(inode, len);\n\n\t\tspin_lock(&root->fs_info->delalloc_lock);\n\t\troot->fs_info->delalloc_bytes -= len;\n\t\tBTRFS_I(inode)->delalloc_bytes -= len;\n\n\t\tif (do_list && BTRFS_I(inode)->delalloc_bytes == 0 &&\n\t\t    !list_empty(&BTRFS_I(inode)->delalloc_inodes)) {\n\t\t\tlist_del_init(&BTRFS_I(inode)->delalloc_inodes);\n\t\t}\n\t\tspin_unlock(&root->fs_info->delalloc_lock);\n\t}\n}\n\n/*\n * extent_io.c merge_bio_hook, this must check the chunk tree to make sure\n * we don't create bios that span stripes or chunks\n */\nint btrfs_merge_bio_hook(struct page *page, unsigned long offset,\n\t\t\t size_t size, struct bio *bio,\n\t\t\t unsigned long bio_flags)\n{\n\tstruct btrfs_root *root = BTRFS_I(page->mapping->host)->root;\n\tu64 logical = (u64)bio->bi_sector << 9;\n\tu64 length = 0;\n\tu64 map_length;\n\tint ret;\n\n\tif (bio_flags & EXTENT_BIO_COMPRESSED)\n\t\treturn 0;\n\n\tlength = bio->bi_size;\n\tmap_length = length;\n\tret = btrfs_map_block(root->fs_info, READ, logical,\n\t\t\t      &map_length, NULL, 0);\n\t/* Will always return 0 with map_multi == NULL */\n\tBUG_ON(ret < 0);\n\tif (map_length < length + size)\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * in order to insert checksums into the metadata in large chunks,\n * we wait until bio submission time.   All the pages in the bio are\n * checksummed and sums are attached onto the ordered extent record.\n *\n * At IO completion time the cums attached on the ordered extent record\n * are inserted into the btree\n */\nstatic int __btrfs_submit_bio_start(struct inode *inode, int rw,\n\t\t\t\t    struct bio *bio, int mirror_num,\n\t\t\t\t    unsigned long bio_flags,\n\t\t\t\t    u64 bio_offset)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret = 0;\n\n\tret = btrfs_csum_one_bio(root, inode, bio, 0, 0);\n\tBUG_ON(ret); /* -ENOMEM */\n\treturn 0;\n}\n\n/*\n * in order to insert checksums into the metadata in large chunks,\n * we wait until bio submission time.   All the pages in the bio are\n * checksummed and sums are attached onto the ordered extent record.\n *\n * At IO completion time the cums attached on the ordered extent record\n * are inserted into the btree\n */\nstatic int __btrfs_submit_bio_done(struct inode *inode, int rw, struct bio *bio,\n\t\t\t  int mirror_num, unsigned long bio_flags,\n\t\t\t  u64 bio_offset)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\tret = btrfs_map_bio(root, rw, bio, mirror_num, 1);\n\tif (ret)\n\t\tbio_endio(bio, ret);\n\treturn ret;\n}\n\n/*\n * extent_io.c submission hook. This does the right thing for csum calculation\n * on write, or reading the csums from the tree before a read\n */\nstatic int btrfs_submit_bio_hook(struct inode *inode, int rw, struct bio *bio,\n\t\t\t  int mirror_num, unsigned long bio_flags,\n\t\t\t  u64 bio_offset)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret = 0;\n\tint skip_sum;\n\tint metadata = 0;\n\tint async = !atomic_read(&BTRFS_I(inode)->sync_writers);\n\n\tskip_sum = BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM;\n\n\tif (btrfs_is_free_space_inode(inode))\n\t\tmetadata = 2;\n\n\tif (!(rw & REQ_WRITE)) {\n\t\tret = btrfs_bio_wq_end_io(root->fs_info, bio, metadata);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (bio_flags & EXTENT_BIO_COMPRESSED) {\n\t\t\tret = btrfs_submit_compressed_read(inode, bio,\n\t\t\t\t\t\t\t   mirror_num,\n\t\t\t\t\t\t\t   bio_flags);\n\t\t\tgoto out;\n\t\t} else if (!skip_sum) {\n\t\t\tret = btrfs_lookup_bio_sums(root, inode, bio, NULL);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tgoto mapit;\n\t} else if (async && !skip_sum) {\n\t\t/* csum items have already been cloned */\n\t\tif (root->root_key.objectid == BTRFS_DATA_RELOC_TREE_OBJECTID)\n\t\t\tgoto mapit;\n\t\t/* we're doing a write, do the async checksumming */\n\t\tret = btrfs_wq_submit_bio(BTRFS_I(inode)->root->fs_info,\n\t\t\t\t   inode, rw, bio, mirror_num,\n\t\t\t\t   bio_flags, bio_offset,\n\t\t\t\t   __btrfs_submit_bio_start,\n\t\t\t\t   __btrfs_submit_bio_done);\n\t\tgoto out;\n\t} else if (!skip_sum) {\n\t\tret = btrfs_csum_one_bio(root, inode, bio, 0, 0);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nmapit:\n\tret = btrfs_map_bio(root, rw, bio, mirror_num, 0);\n\nout:\n\tif (ret < 0)\n\t\tbio_endio(bio, ret);\n\treturn ret;\n}\n\n/*\n * given a list of ordered sums record them in the inode.  This happens\n * at IO completion time based on sums calculated at bio submission time.\n */\nstatic noinline int add_pending_csums(struct btrfs_trans_handle *trans,\n\t\t\t     struct inode *inode, u64 file_offset,\n\t\t\t     struct list_head *list)\n{\n\tstruct btrfs_ordered_sum *sum;\n\n\tlist_for_each_entry(sum, list, list) {\n\t\tbtrfs_csum_file_blocks(trans,\n\t\t       BTRFS_I(inode)->root->fs_info->csum_root, sum);\n\t}\n\treturn 0;\n}\n\nint btrfs_set_extent_delalloc(struct inode *inode, u64 start, u64 end,\n\t\t\t      struct extent_state **cached_state)\n{\n\tWARN_ON((end & (PAGE_CACHE_SIZE - 1)) == 0);\n\treturn set_extent_delalloc(&BTRFS_I(inode)->io_tree, start, end,\n\t\t\t\t   cached_state, GFP_NOFS);\n}\n\n/* see btrfs_writepage_start_hook for details on why this is required */\nstruct btrfs_writepage_fixup {\n\tstruct page *page;\n\tstruct btrfs_work work;\n};\n\nstatic void btrfs_writepage_fixup_worker(struct btrfs_work *work)\n{\n\tstruct btrfs_writepage_fixup *fixup;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tstruct page *page;\n\tstruct inode *inode;\n\tu64 page_start;\n\tu64 page_end;\n\tint ret;\n\n\tfixup = container_of(work, struct btrfs_writepage_fixup, work);\n\tpage = fixup->page;\nagain:\n\tlock_page(page);\n\tif (!page->mapping || !PageDirty(page) || !PageChecked(page)) {\n\t\tClearPageChecked(page);\n\t\tgoto out_page;\n\t}\n\n\tinode = page->mapping->host;\n\tpage_start = page_offset(page);\n\tpage_end = page_offset(page) + PAGE_CACHE_SIZE - 1;\n\n\tlock_extent_bits(&BTRFS_I(inode)->io_tree, page_start, page_end, 0,\n\t\t\t &cached_state);\n\n\t/* already ordered? We're done */\n\tif (PagePrivate2(page))\n\t\tgoto out;\n\n\tordered = btrfs_lookup_ordered_extent(inode, page_start);\n\tif (ordered) {\n\t\tunlock_extent_cached(&BTRFS_I(inode)->io_tree, page_start,\n\t\t\t\t     page_end, &cached_state, GFP_NOFS);\n\t\tunlock_page(page);\n\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tgoto again;\n\t}\n\n\tret = btrfs_delalloc_reserve_space(inode, PAGE_CACHE_SIZE);\n\tif (ret) {\n\t\tmapping_set_error(page->mapping, ret);\n\t\tend_extent_writepage(page, ret, page_start, page_end);\n\t\tClearPageChecked(page);\n\t\tgoto out;\n\t }\n\n\tbtrfs_set_extent_delalloc(inode, page_start, page_end, &cached_state);\n\tClearPageChecked(page);\n\tset_page_dirty(page);\nout:\n\tunlock_extent_cached(&BTRFS_I(inode)->io_tree, page_start, page_end,\n\t\t\t     &cached_state, GFP_NOFS);\nout_page:\n\tunlock_page(page);\n\tpage_cache_release(page);\n\tkfree(fixup);\n}\n\n/*\n * There are a few paths in the higher layers of the kernel that directly\n * set the page dirty bit without asking the filesystem if it is a\n * good idea.  This causes problems because we want to make sure COW\n * properly happens and the data=ordered rules are followed.\n *\n * In our case any range that doesn't have the ORDERED bit set\n * hasn't been properly setup for IO.  We kick off an async process\n * to fix it up.  The async helper will wait for ordered extents, set\n * the delalloc bit and make it safe to write the page.\n */\nstatic int btrfs_writepage_start_hook(struct page *page, u64 start, u64 end)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct btrfs_writepage_fixup *fixup;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\t/* this page is properly in the ordered list */\n\tif (TestClearPagePrivate2(page))\n\t\treturn 0;\n\n\tif (PageChecked(page))\n\t\treturn -EAGAIN;\n\n\tfixup = kzalloc(sizeof(*fixup), GFP_NOFS);\n\tif (!fixup)\n\t\treturn -EAGAIN;\n\n\tSetPageChecked(page);\n\tpage_cache_get(page);\n\tfixup->work.func = btrfs_writepage_fixup_worker;\n\tfixup->page = page;\n\tbtrfs_queue_worker(&root->fs_info->fixup_workers, &fixup->work);\n\treturn -EBUSY;\n}\n\nstatic int insert_reserved_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t       struct inode *inode, u64 file_pos,\n\t\t\t\t       u64 disk_bytenr, u64 disk_num_bytes,\n\t\t\t\t       u64 num_bytes, u64 ram_bytes,\n\t\t\t\t       u8 compression, u8 encryption,\n\t\t\t\t       u16 other_encoding, int extent_type)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key ins;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->leave_spinning = 1;\n\n\t/*\n\t * we may be replacing one extent in the tree with another.\n\t * The new extent is pinned in the extent map, and we don't want\n\t * to drop it from the cache until it is completely in the btree.\n\t *\n\t * So, tell btrfs_drop_extents to leave this extent in the cache.\n\t * the caller is expected to unpin it and allow it to be merged\n\t * with the others.\n\t */\n\tret = btrfs_drop_extents(trans, root, inode, file_pos,\n\t\t\t\t file_pos + num_bytes, 0);\n\tif (ret)\n\t\tgoto out;\n\n\tins.objectid = btrfs_ino(inode);\n\tins.offset = file_pos;\n\tins.type = BTRFS_EXTENT_DATA_KEY;\n\tret = btrfs_insert_empty_item(trans, root, path, &ins, sizeof(*fi));\n\tif (ret)\n\t\tgoto out;\n\tleaf = path->nodes[0];\n\tfi = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t    struct btrfs_file_extent_item);\n\tbtrfs_set_file_extent_generation(leaf, fi, trans->transid);\n\tbtrfs_set_file_extent_type(leaf, fi, extent_type);\n\tbtrfs_set_file_extent_disk_bytenr(leaf, fi, disk_bytenr);\n\tbtrfs_set_file_extent_disk_num_bytes(leaf, fi, disk_num_bytes);\n\tbtrfs_set_file_extent_offset(leaf, fi, 0);\n\tbtrfs_set_file_extent_num_bytes(leaf, fi, num_bytes);\n\tbtrfs_set_file_extent_ram_bytes(leaf, fi, ram_bytes);\n\tbtrfs_set_file_extent_compression(leaf, fi, compression);\n\tbtrfs_set_file_extent_encryption(leaf, fi, encryption);\n\tbtrfs_set_file_extent_other_encoding(leaf, fi, other_encoding);\n\n\tbtrfs_mark_buffer_dirty(leaf);\n\tbtrfs_release_path(path);\n\n\tinode_add_bytes(inode, num_bytes);\n\n\tins.objectid = disk_bytenr;\n\tins.offset = disk_num_bytes;\n\tins.type = BTRFS_EXTENT_ITEM_KEY;\n\tret = btrfs_alloc_reserved_file_extent(trans, root,\n\t\t\t\t\troot->root_key.objectid,\n\t\t\t\t\tbtrfs_ino(inode), file_pos, &ins);\nout:\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\n/*\n * helper function for btrfs_finish_ordered_io, this\n * just reads in some of the csum leaves to prime them into ram\n * before we start the transaction.  It limits the amount of btree\n * reads required while inside the transaction.\n */\n/* as ordered data IO finishes, this gets called so we can finish\n * an ordered extent if the range of bytes in the file it covers are\n * fully written.\n */\nstatic int btrfs_finish_ordered_io(struct btrfs_ordered_extent *ordered_extent)\n{\n\tstruct inode *inode = ordered_extent->inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_state *cached_state = NULL;\n\tint compress_type = 0;\n\tint ret;\n\tbool nolock;\n\n\tnolock = btrfs_is_free_space_inode(inode);\n\n\tif (test_bit(BTRFS_ORDERED_IOERR, &ordered_extent->flags)) {\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_ORDERED_NOCOW, &ordered_extent->flags)) {\n\t\tBUG_ON(!list_empty(&ordered_extent->list)); /* Logic error */\n\t\tbtrfs_ordered_update_i_size(inode, 0, ordered_extent);\n\t\tif (nolock)\n\t\t\ttrans = btrfs_join_transaction_nolock(root);\n\t\telse\n\t\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\t\tret = btrfs_update_inode_fallback(trans, root, inode);\n\t\tif (ret) /* -ENOMEM or corruption */\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out;\n\t}\n\n\tlock_extent_bits(io_tree, ordered_extent->file_offset,\n\t\t\t ordered_extent->file_offset + ordered_extent->len - 1,\n\t\t\t 0, &cached_state);\n\n\tif (nolock)\n\t\ttrans = btrfs_join_transaction_nolock(root);\n\telse\n\t\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\ttrans = NULL;\n\t\tgoto out_unlock;\n\t}\n\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\tif (test_bit(BTRFS_ORDERED_COMPRESSED, &ordered_extent->flags))\n\t\tcompress_type = ordered_extent->compress_type;\n\tif (test_bit(BTRFS_ORDERED_PREALLOC, &ordered_extent->flags)) {\n\t\tBUG_ON(compress_type);\n\t\tret = btrfs_mark_extent_written(trans, inode,\n\t\t\t\t\t\tordered_extent->file_offset,\n\t\t\t\t\t\tordered_extent->file_offset +\n\t\t\t\t\t\tordered_extent->len);\n\t} else {\n\t\tBUG_ON(root == root->fs_info->tree_root);\n\t\tret = insert_reserved_file_extent(trans, inode,\n\t\t\t\t\t\tordered_extent->file_offset,\n\t\t\t\t\t\tordered_extent->start,\n\t\t\t\t\t\tordered_extent->disk_len,\n\t\t\t\t\t\tordered_extent->len,\n\t\t\t\t\t\tordered_extent->len,\n\t\t\t\t\t\tcompress_type, 0, 0,\n\t\t\t\t\t\tBTRFS_FILE_EXTENT_REG);\n\t}\n\tunpin_extent_cache(&BTRFS_I(inode)->extent_tree,\n\t\t\t   ordered_extent->file_offset, ordered_extent->len,\n\t\t\t   trans->transid);\n\tif (ret < 0) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_unlock;\n\t}\n\n\tadd_pending_csums(trans, inode, ordered_extent->file_offset,\n\t\t\t  &ordered_extent->list);\n\n\tbtrfs_ordered_update_i_size(inode, 0, ordered_extent);\n\tret = btrfs_update_inode_fallback(trans, root, inode);\n\tif (ret) { /* -ENOMEM or corruption */\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_unlock;\n\t}\n\tret = 0;\nout_unlock:\n\tunlock_extent_cached(io_tree, ordered_extent->file_offset,\n\t\t\t     ordered_extent->file_offset +\n\t\t\t     ordered_extent->len - 1, &cached_state, GFP_NOFS);\nout:\n\tif (root != root->fs_info->tree_root)\n\t\tbtrfs_delalloc_release_metadata(inode, ordered_extent->len);\n\tif (trans)\n\t\tbtrfs_end_transaction(trans, root);\n\n\tif (ret)\n\t\tclear_extent_uptodate(io_tree, ordered_extent->file_offset,\n\t\t\t\t      ordered_extent->file_offset +\n\t\t\t\t      ordered_extent->len - 1, NULL, GFP_NOFS);\n\n\t/*\n\t * This needs to be done to make sure anybody waiting knows we are done\n\t * updating everything for this ordered extent.\n\t */\n\tbtrfs_remove_ordered_extent(inode, ordered_extent);\n\n\t/* once for us */\n\tbtrfs_put_ordered_extent(ordered_extent);\n\t/* once for the tree */\n\tbtrfs_put_ordered_extent(ordered_extent);\n\n\treturn ret;\n}\n\nstatic void finish_ordered_fn(struct btrfs_work *work)\n{\n\tstruct btrfs_ordered_extent *ordered_extent;\n\tordered_extent = container_of(work, struct btrfs_ordered_extent, work);\n\tbtrfs_finish_ordered_io(ordered_extent);\n}\n\nstatic int btrfs_writepage_end_io_hook(struct page *page, u64 start, u64 end,\n\t\t\t\tstruct extent_state *state, int uptodate)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ordered_extent *ordered_extent = NULL;\n\tstruct btrfs_workers *workers;\n\n\ttrace_btrfs_writepage_end_io_hook(page, start, end, uptodate);\n\n\tClearPagePrivate2(page);\n\tif (!btrfs_dec_test_ordered_pending(inode, &ordered_extent, start,\n\t\t\t\t\t    end - start + 1, uptodate))\n\t\treturn 0;\n\n\tordered_extent->work.func = finish_ordered_fn;\n\tordered_extent->work.flags = 0;\n\n\tif (btrfs_is_free_space_inode(inode))\n\t\tworkers = &root->fs_info->endio_freespace_worker;\n\telse\n\t\tworkers = &root->fs_info->endio_write_workers;\n\tbtrfs_queue_worker(workers, &ordered_extent->work);\n\n\treturn 0;\n}\n\n/*\n * when reads are done, we need to check csums to verify the data is correct\n * if there's a match, we allow the bio to finish.  If not, the code in\n * extent_io.c will try to find good copies for us.\n */\nstatic int btrfs_readpage_end_io_hook(struct page *page, u64 start, u64 end,\n\t\t\t       struct extent_state *state, int mirror)\n{\n\tsize_t offset = start - ((u64)page->index << PAGE_CACHE_SHIFT);\n\tstruct inode *inode = page->mapping->host;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tchar *kaddr;\n\tu64 private = ~(u32)0;\n\tint ret;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tu32 csum = ~(u32)0;\n\n\tif (PageChecked(page)) {\n\t\tClearPageChecked(page);\n\t\tgoto good;\n\t}\n\n\tif (BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM)\n\t\tgoto good;\n\n\tif (root->root_key.objectid == BTRFS_DATA_RELOC_TREE_OBJECTID &&\n\t    test_range_bit(io_tree, start, end, EXTENT_NODATASUM, 1, NULL)) {\n\t\tclear_extent_bits(io_tree, start, end, EXTENT_NODATASUM,\n\t\t\t\t  GFP_NOFS);\n\t\treturn 0;\n\t}\n\n\tif (state && state->start == start) {\n\t\tprivate = state->private;\n\t\tret = 0;\n\t} else {\n\t\tret = get_state_private(io_tree, start, &private);\n\t}\n\tkaddr = kmap_atomic(page);\n\tif (ret)\n\t\tgoto zeroit;\n\n\tcsum = btrfs_csum_data(root, kaddr + offset, csum,  end - start + 1);\n\tbtrfs_csum_final(csum, (char *)&csum);\n\tif (csum != private)\n\t\tgoto zeroit;\n\n\tkunmap_atomic(kaddr);\ngood:\n\treturn 0;\n\nzeroit:\n\tprintk_ratelimited(KERN_INFO \"btrfs csum failed ino %llu off %llu csum %u \"\n\t\t       \"private %llu\\n\",\n\t\t       (unsigned long long)btrfs_ino(page->mapping->host),\n\t\t       (unsigned long long)start, csum,\n\t\t       (unsigned long long)private);\n\tmemset(kaddr + offset, 1, end - start + 1);\n\tflush_dcache_page(page);\n\tkunmap_atomic(kaddr);\n\tif (private == 0)\n\t\treturn 0;\n\treturn -EIO;\n}\n\nstruct delayed_iput {\n\tstruct list_head list;\n\tstruct inode *inode;\n};\n\n/* JDM: If this is fs-wide, why can't we add a pointer to\n * btrfs_inode instead and avoid the allocation? */\nvoid btrfs_add_delayed_iput(struct inode *inode)\n{\n\tstruct btrfs_fs_info *fs_info = BTRFS_I(inode)->root->fs_info;\n\tstruct delayed_iput *delayed;\n\n\tif (atomic_add_unless(&inode->i_count, -1, 1))\n\t\treturn;\n\n\tdelayed = kmalloc(sizeof(*delayed), GFP_NOFS | __GFP_NOFAIL);\n\tdelayed->inode = inode;\n\n\tspin_lock(&fs_info->delayed_iput_lock);\n\tlist_add_tail(&delayed->list, &fs_info->delayed_iputs);\n\tspin_unlock(&fs_info->delayed_iput_lock);\n}\n\nvoid btrfs_run_delayed_iputs(struct btrfs_root *root)\n{\n\tLIST_HEAD(list);\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct delayed_iput *delayed;\n\tint empty;\n\n\tspin_lock(&fs_info->delayed_iput_lock);\n\tempty = list_empty(&fs_info->delayed_iputs);\n\tspin_unlock(&fs_info->delayed_iput_lock);\n\tif (empty)\n\t\treturn;\n\n\tspin_lock(&fs_info->delayed_iput_lock);\n\tlist_splice_init(&fs_info->delayed_iputs, &list);\n\tspin_unlock(&fs_info->delayed_iput_lock);\n\n\twhile (!list_empty(&list)) {\n\t\tdelayed = list_entry(list.next, struct delayed_iput, list);\n\t\tlist_del(&delayed->list);\n\t\tiput(delayed->inode);\n\t\tkfree(delayed);\n\t}\n}\n\nenum btrfs_orphan_cleanup_state {\n\tORPHAN_CLEANUP_STARTED\t= 1,\n\tORPHAN_CLEANUP_DONE\t= 2,\n};\n\n/*\n * This is called in transaction commit time. If there are no orphan\n * files in the subvolume, it removes orphan item and frees block_rsv\n * structure.\n */\nvoid btrfs_orphan_commit_root(struct btrfs_trans_handle *trans,\n\t\t\t      struct btrfs_root *root)\n{\n\tstruct btrfs_block_rsv *block_rsv;\n\tint ret;\n\n\tif (atomic_read(&root->orphan_inodes) ||\n\t    root->orphan_cleanup_state != ORPHAN_CLEANUP_DONE)\n\t\treturn;\n\n\tspin_lock(&root->orphan_lock);\n\tif (atomic_read(&root->orphan_inodes)) {\n\t\tspin_unlock(&root->orphan_lock);\n\t\treturn;\n\t}\n\n\tif (root->orphan_cleanup_state != ORPHAN_CLEANUP_DONE) {\n\t\tspin_unlock(&root->orphan_lock);\n\t\treturn;\n\t}\n\n\tblock_rsv = root->orphan_block_rsv;\n\troot->orphan_block_rsv = NULL;\n\tspin_unlock(&root->orphan_lock);\n\n\tif (root->orphan_item_inserted &&\n\t    btrfs_root_refs(&root->root_item) > 0) {\n\t\tret = btrfs_del_orphan_item(trans, root->fs_info->tree_root,\n\t\t\t\t\t    root->root_key.objectid);\n\t\tBUG_ON(ret);\n\t\troot->orphan_item_inserted = 0;\n\t}\n\n\tif (block_rsv) {\n\t\tWARN_ON(block_rsv->size > 0);\n\t\tbtrfs_free_block_rsv(root, block_rsv);\n\t}\n}\n\n/*\n * This creates an orphan entry for the given inode in case something goes\n * wrong in the middle of an unlink/truncate.\n *\n * NOTE: caller of this function should reserve 5 units of metadata for\n *\t this function.\n */\nint btrfs_orphan_add(struct btrfs_trans_handle *trans, struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_block_rsv *block_rsv = NULL;\n\tint reserve = 0;\n\tint insert = 0;\n\tint ret;\n\n\tif (!root->orphan_block_rsv) {\n\t\tblock_rsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);\n\t\tif (!block_rsv)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&root->orphan_lock);\n\tif (!root->orphan_block_rsv) {\n\t\troot->orphan_block_rsv = block_rsv;\n\t} else if (block_rsv) {\n\t\tbtrfs_free_block_rsv(root, block_rsv);\n\t\tblock_rsv = NULL;\n\t}\n\n\tif (!test_and_set_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t      &BTRFS_I(inode)->runtime_flags)) {\n#if 0\n\t\t/*\n\t\t * For proper ENOSPC handling, we should do orphan\n\t\t * cleanup when mounting. But this introduces backward\n\t\t * compatibility issue.\n\t\t */\n\t\tif (!xchg(&root->orphan_item_inserted, 1))\n\t\t\tinsert = 2;\n\t\telse\n\t\t\tinsert = 1;\n#endif\n\t\tinsert = 1;\n\t\tatomic_inc(&root->orphan_inodes);\n\t}\n\n\tif (!test_and_set_bit(BTRFS_INODE_ORPHAN_META_RESERVED,\n\t\t\t      &BTRFS_I(inode)->runtime_flags))\n\t\treserve = 1;\n\tspin_unlock(&root->orphan_lock);\n\n\t/* grab metadata reservation from transaction handle */\n\tif (reserve) {\n\t\tret = btrfs_orphan_reserve_metadata(trans, inode);\n\t\tBUG_ON(ret); /* -ENOSPC in reservation; Logic error? JDM */\n\t}\n\n\t/* insert an orphan item to track this unlinked/truncated file */\n\tif (insert >= 1) {\n\t\tret = btrfs_insert_orphan_item(trans, root, btrfs_ino(inode));\n\t\tif (ret && ret != -EEXIST) {\n\t\t\tclear_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t\t  &BTRFS_I(inode)->runtime_flags);\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\treturn ret;\n\t\t}\n\t\tret = 0;\n\t}\n\n\t/* insert an orphan item to track subvolume contains orphan files */\n\tif (insert >= 2) {\n\t\tret = btrfs_insert_orphan_item(trans, root->fs_info->tree_root,\n\t\t\t\t\t       root->root_key.objectid);\n\t\tif (ret && ret != -EEXIST) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * We have done the truncate/delete so we can go ahead and remove the orphan\n * item for this particular inode.\n */\nint btrfs_orphan_del(struct btrfs_trans_handle *trans, struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint delete_item = 0;\n\tint release_rsv = 0;\n\tint ret = 0;\n\n\tspin_lock(&root->orphan_lock);\n\tif (test_and_clear_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t       &BTRFS_I(inode)->runtime_flags))\n\t\tdelete_item = 1;\n\n\tif (test_and_clear_bit(BTRFS_INODE_ORPHAN_META_RESERVED,\n\t\t\t       &BTRFS_I(inode)->runtime_flags))\n\t\trelease_rsv = 1;\n\tspin_unlock(&root->orphan_lock);\n\n\tif (trans && delete_item) {\n\t\tret = btrfs_del_orphan_item(trans, root, btrfs_ino(inode));\n\t\tBUG_ON(ret); /* -ENOMEM or corruption (JDM: Recheck) */\n\t}\n\n\tif (release_rsv) {\n\t\tbtrfs_orphan_release_metadata(inode);\n\t\tatomic_dec(&root->orphan_inodes);\n\t}\n\n\treturn 0;\n}\n\n/*\n * this cleans up any orphans that may be left on the list from the last use\n * of this root.\n */\nint btrfs_orphan_cleanup(struct btrfs_root *root)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key, found_key;\n\tstruct btrfs_trans_handle *trans;\n\tstruct inode *inode;\n\tu64 last_objectid = 0;\n\tint ret = 0, nr_unlink = 0, nr_truncate = 0;\n\n\tif (cmpxchg(&root->orphan_cleanup_state, 0, ORPHAN_CLEANUP_STARTED))\n\t\treturn 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpath->reada = -1;\n\n\tkey.objectid = BTRFS_ORPHAN_OBJECTID;\n\tbtrfs_set_key_type(&key, BTRFS_ORPHAN_ITEM_KEY);\n\tkey.offset = (u64)-1;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * if ret == 0 means we found what we were searching for, which\n\t\t * is weird, but possible, so only screw with path if we didn't\n\t\t * find the key and see if we have stuff that matches\n\t\t */\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tif (path->slots[0] == 0)\n\t\t\t\tbreak;\n\t\t\tpath->slots[0]--;\n\t\t}\n\n\t\t/* pull out the item */\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\t/* make sure the item matches what we want */\n\t\tif (found_key.objectid != BTRFS_ORPHAN_OBJECTID)\n\t\t\tbreak;\n\t\tif (btrfs_key_type(&found_key) != BTRFS_ORPHAN_ITEM_KEY)\n\t\t\tbreak;\n\n\t\t/* release the path since we're done with it */\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * this is where we are basically btrfs_lookup, without the\n\t\t * crossing root thing.  we store the inode number in the\n\t\t * offset of the orphan item.\n\t\t */\n\n\t\tif (found_key.offset == last_objectid) {\n\t\t\tprintk(KERN_ERR \"btrfs: Error removing orphan entry, \"\n\t\t\t       \"stopping orphan cleanup\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlast_objectid = found_key.offset;\n\n\t\tfound_key.objectid = found_key.offset;\n\t\tfound_key.type = BTRFS_INODE_ITEM_KEY;\n\t\tfound_key.offset = 0;\n\t\tinode = btrfs_iget(root->fs_info->sb, &found_key, root, NULL);\n\t\tret = PTR_RET(inode);\n\t\tif (ret && ret != -ESTALE)\n\t\t\tgoto out;\n\n\t\tif (ret == -ESTALE && root == root->fs_info->tree_root) {\n\t\t\tstruct btrfs_root *dead_root;\n\t\t\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\t\t\tint is_dead_root = 0;\n\n\t\t\t/*\n\t\t\t * this is an orphan in the tree root. Currently these\n\t\t\t * could come from 2 sources:\n\t\t\t *  a) a snapshot deletion in progress\n\t\t\t *  b) a free space cache inode\n\t\t\t * We need to distinguish those two, as the snapshot\n\t\t\t * orphan must not get deleted.\n\t\t\t * find_dead_roots already ran before us, so if this\n\t\t\t * is a snapshot deletion, we should find the root\n\t\t\t * in the dead_roots list\n\t\t\t */\n\t\t\tspin_lock(&fs_info->trans_lock);\n\t\t\tlist_for_each_entry(dead_root, &fs_info->dead_roots,\n\t\t\t\t\t    root_list) {\n\t\t\t\tif (dead_root->root_key.objectid ==\n\t\t\t\t    found_key.objectid) {\n\t\t\t\t\tis_dead_root = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\tif (is_dead_root) {\n\t\t\t\t/* prevent this orphan from being found again */\n\t\t\t\tkey.offset = found_key.objectid - 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Inode is already gone but the orphan item is still there,\n\t\t * kill the orphan item.\n\t\t */\n\t\tif (ret == -ESTALE) {\n\t\t\ttrans = btrfs_start_transaction(root, 1);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tprintk(KERN_ERR \"auto deleting %Lu\\n\",\n\t\t\t       found_key.objectid);\n\t\t\tret = btrfs_del_orphan_item(trans, root,\n\t\t\t\t\t\t    found_key.objectid);\n\t\t\tBUG_ON(ret); /* -ENOMEM or corruption (JDM: Recheck) */\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * add this inode to the orphan list so btrfs_orphan_del does\n\t\t * the proper thing when we hit it\n\t\t */\n\t\tset_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t&BTRFS_I(inode)->runtime_flags);\n\n\t\t/* if we have links, this was a truncate, lets do that */\n\t\tif (inode->i_nlink) {\n\t\t\tif (!S_ISREG(inode->i_mode)) {\n\t\t\t\tWARN_ON(1);\n\t\t\t\tiput(inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tnr_truncate++;\n\t\t\tret = btrfs_truncate(inode);\n\t\t} else {\n\t\t\tnr_unlink++;\n\t\t}\n\n\t\t/* this will do delete_inode and everything for us */\n\t\tiput(inode);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\t/* release the path since we're done with it */\n\tbtrfs_release_path(path);\n\n\troot->orphan_cleanup_state = ORPHAN_CLEANUP_DONE;\n\n\tif (root->orphan_block_rsv)\n\t\tbtrfs_block_rsv_release(root, root->orphan_block_rsv,\n\t\t\t\t\t(u64)-1);\n\n\tif (root->orphan_block_rsv || root->orphan_item_inserted) {\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (!IS_ERR(trans))\n\t\t\tbtrfs_end_transaction(trans, root);\n\t}\n\n\tif (nr_unlink)\n\t\tprintk(KERN_INFO \"btrfs: unlinked %d orphans\\n\", nr_unlink);\n\tif (nr_truncate)\n\t\tprintk(KERN_INFO \"btrfs: truncated %d orphans\\n\", nr_truncate);\n\nout:\n\tif (ret)\n\t\tprintk(KERN_CRIT \"btrfs: could not do orphan cleanup %d\\n\", ret);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * very simple check to peek ahead in the leaf looking for xattrs.  If we\n * don't find any xattrs, we know there can't be any acls.\n *\n * slot is the slot the inode is in, objectid is the objectid of the inode\n */\nstatic noinline int acls_after_inode_item(struct extent_buffer *leaf,\n\t\t\t\t\t  int slot, u64 objectid)\n{\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tstruct btrfs_key found_key;\n\tint scanned = 0;\n\n\tslot++;\n\twhile (slot < nritems) {\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\t/* we found a different objectid, there must not be acls */\n\t\tif (found_key.objectid != objectid)\n\t\t\treturn 0;\n\n\t\t/* we found an xattr, assume we've got an acl */\n\t\tif (found_key.type == BTRFS_XATTR_ITEM_KEY)\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * we found a key greater than an xattr key, there can't\n\t\t * be any acls later on\n\t\t */\n\t\tif (found_key.type > BTRFS_XATTR_ITEM_KEY)\n\t\t\treturn 0;\n\n\t\tslot++;\n\t\tscanned++;\n\n\t\t/*\n\t\t * it goes inode, inode backrefs, xattrs, extents,\n\t\t * so if there are a ton of hard links to an inode there can\n\t\t * be a lot of backrefs.  Don't waste time searching too hard,\n\t\t * this is just an optimization\n\t\t */\n\t\tif (scanned >= 8)\n\t\t\tbreak;\n\t}\n\t/* we hit the end of the leaf before we found an xattr or\n\t * something larger than an xattr.  We have to assume the inode\n\t * has acls\n\t */\n\treturn 1;\n}\n\n/*\n * read an inode from the btree into the in-memory inode\n */\nstatic void btrfs_read_locked_inode(struct inode *inode)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct btrfs_timespec *tspec;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_key location;\n\tint maybe_acls;\n\tu32 rdev;\n\tint ret;\n\tbool filled = false;\n\n\tret = btrfs_fill_inode(inode, &rdev);\n\tif (!ret)\n\t\tfilled = true;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\tgoto make_bad;\n\n\tpath->leave_spinning = 1;\n\tmemcpy(&location, &BTRFS_I(inode)->location, sizeof(location));\n\n\tret = btrfs_lookup_inode(NULL, root, path, &location, 0);\n\tif (ret)\n\t\tgoto make_bad;\n\n\tleaf = path->nodes[0];\n\n\tif (filled)\n\t\tgoto cache_acl;\n\n\tinode_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_inode_item);\n\tinode->i_mode = btrfs_inode_mode(leaf, inode_item);\n\tset_nlink(inode, btrfs_inode_nlink(leaf, inode_item));\n\ti_uid_write(inode, btrfs_inode_uid(leaf, inode_item));\n\ti_gid_write(inode, btrfs_inode_gid(leaf, inode_item));\n\tbtrfs_i_size_write(inode, btrfs_inode_size(leaf, inode_item));\n\n\ttspec = btrfs_inode_atime(inode_item);\n\tinode->i_atime.tv_sec = btrfs_timespec_sec(leaf, tspec);\n\tinode->i_atime.tv_nsec = btrfs_timespec_nsec(leaf, tspec);\n\n\ttspec = btrfs_inode_mtime(inode_item);\n\tinode->i_mtime.tv_sec = btrfs_timespec_sec(leaf, tspec);\n\tinode->i_mtime.tv_nsec = btrfs_timespec_nsec(leaf, tspec);\n\n\ttspec = btrfs_inode_ctime(inode_item);\n\tinode->i_ctime.tv_sec = btrfs_timespec_sec(leaf, tspec);\n\tinode->i_ctime.tv_nsec = btrfs_timespec_nsec(leaf, tspec);\n\n\tinode_set_bytes(inode, btrfs_inode_nbytes(leaf, inode_item));\n\tBTRFS_I(inode)->generation = btrfs_inode_generation(leaf, inode_item);\n\tBTRFS_I(inode)->last_trans = btrfs_inode_transid(leaf, inode_item);\n\n\t/*\n\t * If we were modified in the current generation and evicted from memory\n\t * and then re-read we need to do a full sync since we don't have any\n\t * idea about which extents were modified before we were evicted from\n\t * cache.\n\t */\n\tif (BTRFS_I(inode)->last_trans == root->fs_info->generation)\n\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC,\n\t\t\t&BTRFS_I(inode)->runtime_flags);\n\n\tinode->i_version = btrfs_inode_sequence(leaf, inode_item);\n\tinode->i_generation = BTRFS_I(inode)->generation;\n\tinode->i_rdev = 0;\n\trdev = btrfs_inode_rdev(leaf, inode_item);\n\n\tBTRFS_I(inode)->index_cnt = (u64)-1;\n\tBTRFS_I(inode)->flags = btrfs_inode_flags(leaf, inode_item);\ncache_acl:\n\t/*\n\t * try to precache a NULL acl entry for files that don't have\n\t * any xattrs or acls\n\t */\n\tmaybe_acls = acls_after_inode_item(leaf, path->slots[0],\n\t\t\t\t\t   btrfs_ino(inode));\n\tif (!maybe_acls)\n\t\tcache_no_acl(inode);\n\n\tbtrfs_free_path(path);\n\n\tswitch (inode->i_mode & S_IFMT) {\n\tcase S_IFREG:\n\t\tinode->i_mapping->a_ops = &btrfs_aops;\n\t\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\t\tBTRFS_I(inode)->io_tree.ops = &btrfs_extent_io_ops;\n\t\tinode->i_fop = &btrfs_file_operations;\n\t\tinode->i_op = &btrfs_file_inode_operations;\n\t\tbreak;\n\tcase S_IFDIR:\n\t\tinode->i_fop = &btrfs_dir_file_operations;\n\t\tif (root == root->fs_info->tree_root)\n\t\t\tinode->i_op = &btrfs_dir_ro_inode_operations;\n\t\telse\n\t\t\tinode->i_op = &btrfs_dir_inode_operations;\n\t\tbreak;\n\tcase S_IFLNK:\n\t\tinode->i_op = &btrfs_symlink_inode_operations;\n\t\tinode->i_mapping->a_ops = &btrfs_symlink_aops;\n\t\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\t\tbreak;\n\tdefault:\n\t\tinode->i_op = &btrfs_special_inode_operations;\n\t\tinit_special_inode(inode, inode->i_mode, rdev);\n\t\tbreak;\n\t}\n\n\tbtrfs_update_iflags(inode);\n\treturn;\n\nmake_bad:\n\tbtrfs_free_path(path);\n\tmake_bad_inode(inode);\n}\n\n/*\n * given a leaf and an inode, copy the inode fields into the leaf\n */\nstatic void fill_inode_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct extent_buffer *leaf,\n\t\t\t    struct btrfs_inode_item *item,\n\t\t\t    struct inode *inode)\n{\n\tbtrfs_set_inode_uid(leaf, item, i_uid_read(inode));\n\tbtrfs_set_inode_gid(leaf, item, i_gid_read(inode));\n\tbtrfs_set_inode_size(leaf, item, BTRFS_I(inode)->disk_i_size);\n\tbtrfs_set_inode_mode(leaf, item, inode->i_mode);\n\tbtrfs_set_inode_nlink(leaf, item, inode->i_nlink);\n\n\tbtrfs_set_timespec_sec(leaf, btrfs_inode_atime(item),\n\t\t\t       inode->i_atime.tv_sec);\n\tbtrfs_set_timespec_nsec(leaf, btrfs_inode_atime(item),\n\t\t\t\tinode->i_atime.tv_nsec);\n\n\tbtrfs_set_timespec_sec(leaf, btrfs_inode_mtime(item),\n\t\t\t       inode->i_mtime.tv_sec);\n\tbtrfs_set_timespec_nsec(leaf, btrfs_inode_mtime(item),\n\t\t\t\tinode->i_mtime.tv_nsec);\n\n\tbtrfs_set_timespec_sec(leaf, btrfs_inode_ctime(item),\n\t\t\t       inode->i_ctime.tv_sec);\n\tbtrfs_set_timespec_nsec(leaf, btrfs_inode_ctime(item),\n\t\t\t\tinode->i_ctime.tv_nsec);\n\n\tbtrfs_set_inode_nbytes(leaf, item, inode_get_bytes(inode));\n\tbtrfs_set_inode_generation(leaf, item, BTRFS_I(inode)->generation);\n\tbtrfs_set_inode_sequence(leaf, item, inode->i_version);\n\tbtrfs_set_inode_transid(leaf, item, trans->transid);\n\tbtrfs_set_inode_rdev(leaf, item, inode->i_rdev);\n\tbtrfs_set_inode_flags(leaf, item, BTRFS_I(inode)->flags);\n\tbtrfs_set_inode_block_group(leaf, item, 0);\n}\n\n/*\n * copy everything in the in-memory inode into the btree.\n */\nstatic noinline int btrfs_update_inode_item(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, struct inode *inode)\n{\n\tstruct btrfs_inode_item *inode_item;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->leave_spinning = 1;\n\tret = btrfs_lookup_inode(trans, root, path, &BTRFS_I(inode)->location,\n\t\t\t\t 1);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = -ENOENT;\n\t\tgoto failed;\n\t}\n\n\tbtrfs_unlock_up_safe(path, 1);\n\tleaf = path->nodes[0];\n\tinode_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_inode_item);\n\n\tfill_inode_item(trans, leaf, inode_item, inode);\n\tbtrfs_mark_buffer_dirty(leaf);\n\tbtrfs_set_inode_last_trans(trans, inode);\n\tret = 0;\nfailed:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * copy everything in the in-memory inode into the btree.\n */\nnoinline int btrfs_update_inode(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, struct inode *inode)\n{\n\tint ret;\n\n\t/*\n\t * If the inode is a free space inode, we can deadlock during commit\n\t * if we put it into the delayed code.\n\t *\n\t * The data relocation inode should also be directly updated\n\t * without delay\n\t */\n\tif (!btrfs_is_free_space_inode(inode)\n\t    && root->root_key.objectid != BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\tbtrfs_update_root_times(trans, root);\n\n\t\tret = btrfs_delayed_update_inode(trans, root, inode);\n\t\tif (!ret)\n\t\t\tbtrfs_set_inode_last_trans(trans, inode);\n\t\treturn ret;\n\t}\n\n\treturn btrfs_update_inode_item(trans, root, inode);\n}\n\nnoinline int btrfs_update_inode_fallback(struct btrfs_trans_handle *trans,\n\t\t\t\t\t struct btrfs_root *root,\n\t\t\t\t\t struct inode *inode)\n{\n\tint ret;\n\n\tret = btrfs_update_inode(trans, root, inode);\n\tif (ret == -ENOSPC)\n\t\treturn btrfs_update_inode_item(trans, root, inode);\n\treturn ret;\n}\n\n/*\n * unlink helper that gets used here in inode.c and in the tree logging\n * recovery code.  It remove a link in a directory with a given name, and\n * also drops the back refs in the inode to the directory\n */\nstatic int __btrfs_unlink_inode(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root,\n\t\t\t\tstruct inode *dir, struct inode *inode,\n\t\t\t\tconst char *name, int name_len)\n{\n\tstruct btrfs_path *path;\n\tint ret = 0;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_key key;\n\tu64 index;\n\tu64 ino = btrfs_ino(inode);\n\tu64 dir_ino = btrfs_ino(dir);\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpath->leave_spinning = 1;\n\tdi = btrfs_lookup_dir_item(trans, root, path, dir_ino,\n\t\t\t\t    name, name_len, -1);\n\tif (IS_ERR(di)) {\n\t\tret = PTR_ERR(di);\n\t\tgoto err;\n\t}\n\tif (!di) {\n\t\tret = -ENOENT;\n\t\tgoto err;\n\t}\n\tleaf = path->nodes[0];\n\tbtrfs_dir_item_key_to_cpu(leaf, di, &key);\n\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n\tif (ret)\n\t\tgoto err;\n\tbtrfs_release_path(path);\n\n\tret = btrfs_del_inode_ref(trans, root, name, name_len, ino,\n\t\t\t\t  dir_ino, &index);\n\tif (ret) {\n\t\tprintk(KERN_INFO \"btrfs failed to delete reference to %.*s, \"\n\t\t       \"inode %llu parent %llu\\n\", name_len, name,\n\t\t       (unsigned long long)ino, (unsigned long long)dir_ino);\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto err;\n\t}\n\n\tret = btrfs_delete_delayed_dir_index(trans, root, dir, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto err;\n\t}\n\n\tret = btrfs_del_inode_ref_in_log(trans, root, name, name_len,\n\t\t\t\t\t inode, dir_ino);\n\tif (ret != 0 && ret != -ENOENT) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto err;\n\t}\n\n\tret = btrfs_del_dir_entries_in_log(trans, root, name, name_len,\n\t\t\t\t\t   dir, index);\n\tif (ret == -ENOENT)\n\t\tret = 0;\nerr:\n\tbtrfs_free_path(path);\n\tif (ret)\n\t\tgoto out;\n\n\tbtrfs_i_size_write(dir, dir->i_size - name_len * 2);\n\tinode_inc_iversion(inode);\n\tinode_inc_iversion(dir);\n\tinode->i_ctime = dir->i_mtime = dir->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode(trans, root, dir);\nout:\n\treturn ret;\n}\n\nint btrfs_unlink_inode(struct btrfs_trans_handle *trans,\n\t\t       struct btrfs_root *root,\n\t\t       struct inode *dir, struct inode *inode,\n\t\t       const char *name, int name_len)\n{\n\tint ret;\n\tret = __btrfs_unlink_inode(trans, root, dir, inode, name, name_len);\n\tif (!ret) {\n\t\tbtrfs_drop_nlink(inode);\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t}\n\treturn ret;\n}\n\t\t\n\n/* helper to check if there is any shared block in the path */\nstatic int check_path_shared(struct btrfs_root *root,\n\t\t\t     struct btrfs_path *path)\n{\n\tstruct extent_buffer *eb;\n\tint level;\n\tu64 refs = 1;\n\n\tfor (level = 0; level < BTRFS_MAX_LEVEL; level++) {\n\t\tint ret;\n\n\t\tif (!path->nodes[level])\n\t\t\tbreak;\n\t\teb = path->nodes[level];\n\t\tif (!btrfs_block_can_be_shared(root, eb))\n\t\t\tcontinue;\n\t\tret = btrfs_lookup_extent_info(NULL, root, eb->start, eb->len,\n\t\t\t\t\t       &refs, NULL);\n\t\tif (refs > 1)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * helper to start transaction for unlink and rmdir.\n *\n * unlink and rmdir are special in btrfs, they do not always free space.\n * so in enospc case, we should make sure they will free space before\n * allowing them to use the global metadata reservation.\n */\nstatic struct btrfs_trans_handle *__unlink_start_trans(struct inode *dir,\n\t\t\t\t\t\t       struct dentry *dentry)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dir_item *di;\n\tstruct inode *inode = dentry->d_inode;\n\tu64 index;\n\tint check_link = 1;\n\tint err = -ENOSPC;\n\tint ret;\n\tu64 ino = btrfs_ino(inode);\n\tu64 dir_ino = btrfs_ino(dir);\n\n\t/*\n\t * 1 for the possible orphan item\n\t * 1 for the dir item\n\t * 1 for the dir index\n\t * 1 for the inode ref\n\t * 1 for the inode ref in the tree log\n\t * 2 for the dir entries in the log\n\t * 1 for the inode\n\t */\n\ttrans = btrfs_start_transaction(root, 8);\n\tif (!IS_ERR(trans) || PTR_ERR(trans) != -ENOSPC)\n\t\treturn trans;\n\n\tif (ino == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\t/* check if there is someone else holds reference */\n\tif (S_ISDIR(inode->i_mode) && atomic_read(&inode->i_count) > 1)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tif (atomic_read(&inode->i_count) > 2)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tif (xchg(&root->fs_info->enospc_unlink, 1))\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\troot->fs_info->enospc_unlink = 0;\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* 1 for the orphan item */\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\troot->fs_info->enospc_unlink = 0;\n\t\treturn trans;\n\t}\n\n\tpath->skip_locking = 1;\n\tpath->search_commit_root = 1;\n\n\tret = btrfs_lookup_inode(trans, root, path,\n\t\t\t\t&BTRFS_I(dir)->location, 0);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\tif (ret == 0) {\n\t\tif (check_path_shared(root, path))\n\t\t\tgoto out;\n\t} else {\n\t\tcheck_link = 0;\n\t}\n\tbtrfs_release_path(path);\n\n\tret = btrfs_lookup_inode(trans, root, path,\n\t\t\t\t&BTRFS_I(inode)->location, 0);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\tif (ret == 0) {\n\t\tif (check_path_shared(root, path))\n\t\t\tgoto out;\n\t} else {\n\t\tcheck_link = 0;\n\t}\n\tbtrfs_release_path(path);\n\n\tif (ret == 0 && S_ISREG(inode->i_mode)) {\n\t\tret = btrfs_lookup_file_extent(trans, root, path,\n\t\t\t\t\t       ino, (u64)-1, 0);\n\t\tif (ret < 0) {\n\t\t\terr = ret;\n\t\t\tgoto out;\n\t\t}\n\t\tBUG_ON(ret == 0); /* Corruption */\n\t\tif (check_path_shared(root, path))\n\t\t\tgoto out;\n\t\tbtrfs_release_path(path);\n\t}\n\n\tif (!check_link) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tdi = btrfs_lookup_dir_item(trans, root, path, dir_ino,\n\t\t\t\tdentry->d_name.name, dentry->d_name.len, 0);\n\tif (IS_ERR(di)) {\n\t\terr = PTR_ERR(di);\n\t\tgoto out;\n\t}\n\tif (di) {\n\t\tif (check_path_shared(root, path))\n\t\t\tgoto out;\n\t} else {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\tbtrfs_release_path(path);\n\n\tret = btrfs_get_inode_ref_index(trans, root, path, dentry->d_name.name,\n\t\t\t\t\tdentry->d_name.len, ino, dir_ino, 0,\n\t\t\t\t\t&index);\n\tif (ret) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tif (check_path_shared(root, path))\n\t\tgoto out;\n\n\tbtrfs_release_path(path);\n\n\t/*\n\t * This is a commit root search, if we can lookup inode item and other\n\t * relative items in the commit root, it means the transaction of\n\t * dir/file creation has been committed, and the dir index item that we\n\t * delay to insert has also been inserted into the commit root. So\n\t * we needn't worry about the delayed insertion of the dir index item\n\t * here.\n\t */\n\tdi = btrfs_lookup_dir_index_item(trans, root, path, dir_ino, index,\n\t\t\t\tdentry->d_name.name, dentry->d_name.len, 0);\n\tif (IS_ERR(di)) {\n\t\terr = PTR_ERR(di);\n\t\tgoto out;\n\t}\n\tBUG_ON(ret == -ENOENT);\n\tif (check_path_shared(root, path))\n\t\tgoto out;\n\n\terr = 0;\nout:\n\tbtrfs_free_path(path);\n\t/* Migrate the orphan reservation over */\n\tif (!err)\n\t\terr = btrfs_block_rsv_migrate(trans->block_rsv,\n\t\t\t\t&root->fs_info->global_block_rsv,\n\t\t\t\ttrans->bytes_reserved);\n\n\tif (err) {\n\t\tbtrfs_end_transaction(trans, root);\n\t\troot->fs_info->enospc_unlink = 0;\n\t\treturn ERR_PTR(err);\n\t}\n\n\ttrans->block_rsv = &root->fs_info->global_block_rsv;\n\treturn trans;\n}\n\nstatic void __unlink_end_trans(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root)\n{\n\tif (trans->block_rsv->type == BTRFS_BLOCK_RSV_GLOBAL) {\n\t\tbtrfs_block_rsv_release(root, trans->block_rsv,\n\t\t\t\t\ttrans->bytes_reserved);\n\t\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\t\tBUG_ON(!root->fs_info->enospc_unlink);\n\t\troot->fs_info->enospc_unlink = 0;\n\t}\n\tbtrfs_end_transaction(trans, root);\n}\n\nstatic int btrfs_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct inode *inode = dentry->d_inode;\n\tint ret;\n\n\ttrans = __unlink_start_trans(dir, dentry);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tbtrfs_record_unlink_dir(trans, dir, dentry->d_inode, 0);\n\n\tret = btrfs_unlink_inode(trans, root, dir, dentry->d_inode,\n\t\t\t\t dentry->d_name.name, dentry->d_name.len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (inode->i_nlink == 0) {\n\t\tret = btrfs_orphan_add(trans, inode);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\t__unlink_end_trans(trans, root);\n\tbtrfs_btree_balance_dirty(root);\n\treturn ret;\n}\n\nint btrfs_unlink_subvol(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root,\n\t\t\tstruct inode *dir, u64 objectid,\n\t\t\tconst char *name, int name_len)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_key key;\n\tu64 index;\n\tint ret;\n\tu64 dir_ino = btrfs_ino(dir);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tdi = btrfs_lookup_dir_item(trans, root, path, dir_ino,\n\t\t\t\t   name, name_len, -1);\n\tif (IS_ERR_OR_NULL(di)) {\n\t\tif (!di)\n\t\t\tret = -ENOENT;\n\t\telse\n\t\t\tret = PTR_ERR(di);\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tbtrfs_dir_item_key_to_cpu(leaf, di, &key);\n\tWARN_ON(key.type != BTRFS_ROOT_ITEM_KEY || key.objectid != objectid);\n\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out;\n\t}\n\tbtrfs_release_path(path);\n\n\tret = btrfs_del_root_ref(trans, root->fs_info->tree_root,\n\t\t\t\t objectid, root->root_key.objectid,\n\t\t\t\t dir_ino, &index, name, name_len);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out;\n\t\t}\n\t\tdi = btrfs_search_dir_index_item(root, path, dir_ino,\n\t\t\t\t\t\t name, name_len);\n\t\tif (IS_ERR_OR_NULL(di)) {\n\t\t\tif (!di)\n\t\t\t\tret = -ENOENT;\n\t\t\telse\n\t\t\t\tret = PTR_ERR(di);\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tbtrfs_release_path(path);\n\t\tindex = key.offset;\n\t}\n\tbtrfs_release_path(path);\n\n\tret = btrfs_delete_delayed_dir_index(trans, root, dir, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out;\n\t}\n\n\tbtrfs_i_size_write(dir, dir->i_size - name_len * 2);\n\tinode_inc_iversion(dir);\n\tdir->i_mtime = dir->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode_fallback(trans, root, dir);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, root, ret);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_rmdir(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint err = 0;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_trans_handle *trans;\n\n\tif (inode->i_size > BTRFS_EMPTY_DIR_SIZE)\n\t\treturn -ENOTEMPTY;\n\tif (btrfs_ino(inode) == BTRFS_FIRST_FREE_OBJECTID)\n\t\treturn -EPERM;\n\n\ttrans = __unlink_start_trans(dir, dentry);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tif (unlikely(btrfs_ino(inode) == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)) {\n\t\terr = btrfs_unlink_subvol(trans, root, dir,\n\t\t\t\t\t  BTRFS_I(inode)->location.objectid,\n\t\t\t\t\t  dentry->d_name.name,\n\t\t\t\t\t  dentry->d_name.len);\n\t\tgoto out;\n\t}\n\n\terr = btrfs_orphan_add(trans, inode);\n\tif (err)\n\t\tgoto out;\n\n\t/* now the directory is empty */\n\terr = btrfs_unlink_inode(trans, root, dir, dentry->d_inode,\n\t\t\t\t dentry->d_name.name, dentry->d_name.len);\n\tif (!err)\n\t\tbtrfs_i_size_write(inode, 0);\nout:\n\t__unlink_end_trans(trans, root);\n\tbtrfs_btree_balance_dirty(root);\n\n\treturn err;\n}\n\n/*\n * this can truncate away extent items, csum items and directory items.\n * It starts at a high offset and removes keys until it can't find\n * any higher than new_size\n *\n * csum items that cross the new i_size are truncated to the new size\n * as well.\n *\n * min_type is the minimum key type to truncate down to.  If set to 0, this\n * will kill all the items on this inode, including the INODE_ITEM_KEY.\n */\nint btrfs_truncate_inode_items(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root,\n\t\t\t       struct inode *inode,\n\t\t\t       u64 new_size, u32 min_type)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 extent_start = 0;\n\tu64 extent_num_bytes = 0;\n\tu64 extent_offset = 0;\n\tu64 item_end = 0;\n\tu64 mask = root->sectorsize - 1;\n\tu32 found_type = (u8)-1;\n\tint found_extent;\n\tint del_item;\n\tint pending_del_nr = 0;\n\tint pending_del_slot = 0;\n\tint extent_type = -1;\n\tint ret;\n\tint err = 0;\n\tu64 ino = btrfs_ino(inode);\n\n\tBUG_ON(new_size > 0 && min_type != BTRFS_EXTENT_DATA_KEY);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tpath->reada = -1;\n\n\t/*\n\t * We want to drop from the next block forward in case this new size is\n\t * not block aligned since we will be keeping the last block of the\n\t * extent just the way it is.\n\t */\n\tif (root->ref_cows || root == root->fs_info->tree_root)\n\t\tbtrfs_drop_extent_cache(inode, (new_size + mask) & (~mask), (u64)-1, 0);\n\n\t/*\n\t * This function is also used to drop the items in the log tree before\n\t * we relog the inode, so if root != BTRFS_I(inode)->root, it means\n\t * it is used to drop the loged items. So we shouldn't kill the delayed\n\t * items.\n\t */\n\tif (min_type == 0 && root == BTRFS_I(inode)->root)\n\t\tbtrfs_kill_delayed_inode_items(inode);\n\n\tkey.objectid = ino;\n\tkey.offset = (u64)-1;\n\tkey.type = (u8)-1;\n\nsearch_again:\n\tpath->leave_spinning = 1;\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tif (ret > 0) {\n\t\t/* there are no items in the tree for us to truncate, we're\n\t\t * done\n\t\t */\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto out;\n\t\tpath->slots[0]--;\n\t}\n\n\twhile (1) {\n\t\tfi = NULL;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tfound_type = btrfs_key_type(&found_key);\n\n\t\tif (found_key.objectid != ino)\n\t\t\tbreak;\n\n\t\tif (found_type < min_type)\n\t\t\tbreak;\n\n\t\titem_end = found_key.offset;\n\t\tif (found_type == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tfi = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t    struct btrfs_file_extent_item);\n\t\t\textent_type = btrfs_file_extent_type(leaf, fi);\n\t\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\titem_end +=\n\t\t\t\t    btrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\titem_end += btrfs_file_extent_inline_len(leaf,\n\t\t\t\t\t\t\t\t\t fi);\n\t\t\t}\n\t\t\titem_end--;\n\t\t}\n\t\tif (found_type > min_type) {\n\t\t\tdel_item = 1;\n\t\t} else {\n\t\t\tif (item_end < new_size)\n\t\t\t\tbreak;\n\t\t\tif (found_key.offset >= new_size)\n\t\t\t\tdel_item = 1;\n\t\t\telse\n\t\t\t\tdel_item = 0;\n\t\t}\n\t\tfound_extent = 0;\n\t\t/* FIXME, shrink the extent if the ref count is only 1 */\n\t\tif (found_type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto delete;\n\n\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\tu64 num_dec;\n\t\t\textent_start = btrfs_file_extent_disk_bytenr(leaf, fi);\n\t\t\tif (!del_item) {\n\t\t\t\tu64 orig_num_bytes =\n\t\t\t\t\tbtrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t\textent_num_bytes = new_size -\n\t\t\t\t\tfound_key.offset + root->sectorsize - 1;\n\t\t\t\textent_num_bytes = extent_num_bytes &\n\t\t\t\t\t~((u64)root->sectorsize - 1);\n\t\t\t\tbtrfs_set_file_extent_num_bytes(leaf, fi,\n\t\t\t\t\t\t\t extent_num_bytes);\n\t\t\t\tnum_dec = (orig_num_bytes -\n\t\t\t\t\t   extent_num_bytes);\n\t\t\t\tif (root->ref_cows && extent_start != 0)\n\t\t\t\t\tinode_sub_bytes(inode, num_dec);\n\t\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t\t} else {\n\t\t\t\textent_num_bytes =\n\t\t\t\t\tbtrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t\t fi);\n\t\t\t\textent_offset = found_key.offset -\n\t\t\t\t\tbtrfs_file_extent_offset(leaf, fi);\n\n\t\t\t\t/* FIXME blocksize != 4096 */\n\t\t\t\tnum_dec = btrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t\tif (extent_start != 0) {\n\t\t\t\t\tfound_extent = 1;\n\t\t\t\t\tif (root->ref_cows)\n\t\t\t\t\t\tinode_sub_bytes(inode, num_dec);\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t/*\n\t\t\t * we can't truncate inline items that have had\n\t\t\t * special encodings\n\t\t\t */\n\t\t\tif (!del_item &&\n\t\t\t    btrfs_file_extent_compression(leaf, fi) == 0 &&\n\t\t\t    btrfs_file_extent_encryption(leaf, fi) == 0 &&\n\t\t\t    btrfs_file_extent_other_encoding(leaf, fi) == 0) {\n\t\t\t\tu32 size = new_size - found_key.offset;\n\n\t\t\t\tif (root->ref_cows) {\n\t\t\t\t\tinode_sub_bytes(inode, item_end + 1 -\n\t\t\t\t\t\t\tnew_size);\n\t\t\t\t}\n\t\t\t\tsize =\n\t\t\t\t    btrfs_file_extent_calc_inline_size(size);\n\t\t\t\tbtrfs_truncate_item(trans, root, path,\n\t\t\t\t\t\t    size, 1);\n\t\t\t} else if (root->ref_cows) {\n\t\t\t\tinode_sub_bytes(inode, item_end + 1 -\n\t\t\t\t\t\tfound_key.offset);\n\t\t\t}\n\t\t}\ndelete:\n\t\tif (del_item) {\n\t\t\tif (!pending_del_nr) {\n\t\t\t\t/* no pending yet, add ourselves */\n\t\t\t\tpending_del_slot = path->slots[0];\n\t\t\t\tpending_del_nr = 1;\n\t\t\t} else if (pending_del_nr &&\n\t\t\t\t   path->slots[0] + 1 == pending_del_slot) {\n\t\t\t\t/* hop on the pending chunk */\n\t\t\t\tpending_del_nr++;\n\t\t\t\tpending_del_slot = path->slots[0];\n\t\t\t} else {\n\t\t\t\tBUG();\n\t\t\t}\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tif (found_extent && (root->ref_cows ||\n\t\t\t\t     root == root->fs_info->tree_root)) {\n\t\t\tbtrfs_set_path_blocking(path);\n\t\t\tret = btrfs_free_extent(trans, root, extent_start,\n\t\t\t\t\t\textent_num_bytes, 0,\n\t\t\t\t\t\tbtrfs_header_owner(leaf),\n\t\t\t\t\t\tino, extent_offset, 0);\n\t\t\tBUG_ON(ret);\n\t\t}\n\n\t\tif (found_type == BTRFS_INODE_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tif (path->slots[0] == 0 ||\n\t\t    path->slots[0] != pending_del_slot) {\n\t\t\tif (pending_del_nr) {\n\t\t\t\tret = btrfs_del_items(trans, root, path,\n\t\t\t\t\t\tpending_del_slot,\n\t\t\t\t\t\tpending_del_nr);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\troot, ret);\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tpending_del_nr = 0;\n\t\t\t}\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto search_again;\n\t\t} else {\n\t\t\tpath->slots[0]--;\n\t\t}\n\t}\nout:\n\tif (pending_del_nr) {\n\t\tret = btrfs_del_items(trans, root, path, pending_del_slot,\n\t\t\t\t      pending_del_nr);\n\t\tif (ret)\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t}\nerror:\n\tbtrfs_free_path(path);\n\treturn err;\n}\n\n/*\n * btrfs_truncate_page - read, zero a chunk and write a page\n * @inode - inode that we're zeroing\n * @from - the offset to start zeroing\n * @len - the length to zero, 0 to zero the entire range respective to the\n *\toffset\n * @front - zero up to the offset instead of from the offset on\n *\n * This will find the page for the \"from\" offset and cow the page and zero the\n * part we want to zero.  This is used with truncate and hole punching.\n */\nint btrfs_truncate_page(struct inode *inode, loff_t from, loff_t len,\n\t\t\tint front)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tchar *kaddr;\n\tu32 blocksize = root->sectorsize;\n\tpgoff_t index = from >> PAGE_CACHE_SHIFT;\n\tunsigned offset = from & (PAGE_CACHE_SIZE-1);\n\tstruct page *page;\n\tgfp_t mask = btrfs_alloc_write_mask(mapping);\n\tint ret = 0;\n\tu64 page_start;\n\tu64 page_end;\n\n\tif ((offset & (blocksize - 1)) == 0 &&\n\t    (!len || ((len & (blocksize - 1)) == 0)))\n\t\tgoto out;\n\tret = btrfs_delalloc_reserve_space(inode, PAGE_CACHE_SIZE);\n\tif (ret)\n\t\tgoto out;\n\nagain:\n\tpage = find_or_create_page(mapping, index, mask);\n\tif (!page) {\n\t\tbtrfs_delalloc_release_space(inode, PAGE_CACHE_SIZE);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpage_start = page_offset(page);\n\tpage_end = page_start + PAGE_CACHE_SIZE - 1;\n\n\tif (!PageUptodate(page)) {\n\t\tret = btrfs_readpage(NULL, page);\n\t\tlock_page(page);\n\t\tif (page->mapping != mapping) {\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t\tgoto again;\n\t\t}\n\t\tif (!PageUptodate(page)) {\n\t\t\tret = -EIO;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\twait_on_page_writeback(page);\n\n\tlock_extent_bits(io_tree, page_start, page_end, 0, &cached_state);\n\tset_page_extent_mapped(page);\n\n\tordered = btrfs_lookup_ordered_extent(inode, page_start);\n\tif (ordered) {\n\t\tunlock_extent_cached(io_tree, page_start, page_end,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tgoto again;\n\t}\n\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, page_start, page_end,\n\t\t\t  EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,\n\t\t\t  0, 0, &cached_state, GFP_NOFS);\n\n\tret = btrfs_set_extent_delalloc(inode, page_start, page_end,\n\t\t\t\t\t&cached_state);\n\tif (ret) {\n\t\tunlock_extent_cached(io_tree, page_start, page_end,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tgoto out_unlock;\n\t}\n\n\tif (offset != PAGE_CACHE_SIZE) {\n\t\tif (!len)\n\t\t\tlen = PAGE_CACHE_SIZE - offset;\n\t\tkaddr = kmap(page);\n\t\tif (front)\n\t\t\tmemset(kaddr, 0, offset);\n\t\telse\n\t\t\tmemset(kaddr + offset, 0, len);\n\t\tflush_dcache_page(page);\n\t\tkunmap(page);\n\t}\n\tClearPageChecked(page);\n\tset_page_dirty(page);\n\tunlock_extent_cached(io_tree, page_start, page_end, &cached_state,\n\t\t\t     GFP_NOFS);\n\nout_unlock:\n\tif (ret)\n\t\tbtrfs_delalloc_release_space(inode, PAGE_CACHE_SIZE);\n\tunlock_page(page);\n\tpage_cache_release(page);\nout:\n\treturn ret;\n}\n\n/*\n * This function puts in dummy file extents for the area we're creating a hole\n * for.  So if we are truncating this file to a larger size we need to insert\n * these file extents so that btrfs_get_extent will return a EXTENT_MAP_HOLE for\n * the range between oldsize and size\n */\nint btrfs_cont_expand(struct inode *inode, loff_t oldsize, loff_t size)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em = NULL;\n\tstruct extent_state *cached_state = NULL;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tu64 mask = root->sectorsize - 1;\n\tu64 hole_start = (oldsize + mask) & ~mask;\n\tu64 block_end = (size + mask) & ~mask;\n\tu64 last_byte;\n\tu64 cur_offset;\n\tu64 hole_size;\n\tint err = 0;\n\n\tif (size <= hole_start)\n\t\treturn 0;\n\n\twhile (1) {\n\t\tstruct btrfs_ordered_extent *ordered;\n\t\tbtrfs_wait_ordered_range(inode, hole_start,\n\t\t\t\t\t block_end - hole_start);\n\t\tlock_extent_bits(io_tree, hole_start, block_end - 1, 0,\n\t\t\t\t &cached_state);\n\t\tordered = btrfs_lookup_ordered_extent(inode, hole_start);\n\t\tif (!ordered)\n\t\t\tbreak;\n\t\tunlock_extent_cached(io_tree, hole_start, block_end - 1,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t}\n\n\tcur_offset = hole_start;\n\twhile (1) {\n\t\tem = btrfs_get_extent(inode, NULL, 0, cur_offset,\n\t\t\t\tblock_end - cur_offset, 0);\n\t\tif (IS_ERR(em)) {\n\t\t\terr = PTR_ERR(em);\n\t\t\tbreak;\n\t\t}\n\t\tlast_byte = min(extent_map_end(em), block_end);\n\t\tlast_byte = (last_byte + mask) & ~mask;\n\t\tif (!test_bit(EXTENT_FLAG_PREALLOC, &em->flags)) {\n\t\t\tstruct extent_map *hole_em;\n\t\t\thole_size = last_byte - cur_offset;\n\n\t\t\ttrans = btrfs_start_transaction(root, 3);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\terr = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t\t cur_offset,\n\t\t\t\t\t\t cur_offset + hole_size, 1);\n\t\t\tif (err) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, err);\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = btrfs_insert_file_extent(trans, root,\n\t\t\t\t\tbtrfs_ino(inode), cur_offset, 0,\n\t\t\t\t\t0, hole_size, 0, hole_size,\n\t\t\t\t\t0, 0, 0);\n\t\t\tif (err) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, err);\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbtrfs_drop_extent_cache(inode, cur_offset,\n\t\t\t\t\t\tcur_offset + hole_size - 1, 0);\n\t\t\thole_em = alloc_extent_map();\n\t\t\tif (!hole_em) {\n\t\t\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC,\n\t\t\t\t\t&BTRFS_I(inode)->runtime_flags);\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\thole_em->start = cur_offset;\n\t\t\thole_em->len = hole_size;\n\t\t\thole_em->orig_start = cur_offset;\n\n\t\t\thole_em->block_start = EXTENT_MAP_HOLE;\n\t\t\thole_em->block_len = 0;\n\t\t\thole_em->orig_block_len = 0;\n\t\t\thole_em->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\t\thole_em->compress_type = BTRFS_COMPRESS_NONE;\n\t\t\thole_em->generation = trans->transid;\n\n\t\t\twhile (1) {\n\t\t\t\twrite_lock(&em_tree->lock);\n\t\t\t\terr = add_extent_mapping(em_tree, hole_em);\n\t\t\t\tif (!err)\n\t\t\t\t\tlist_move(&hole_em->list,\n\t\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\t\twrite_unlock(&em_tree->lock);\n\t\t\t\tif (err != -EEXIST)\n\t\t\t\t\tbreak;\n\t\t\t\tbtrfs_drop_extent_cache(inode, cur_offset,\n\t\t\t\t\t\t\tcur_offset +\n\t\t\t\t\t\t\thole_size - 1, 0);\n\t\t\t}\n\t\t\tfree_extent_map(hole_em);\nnext:\n\t\t\tbtrfs_update_inode(trans, root, inode);\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t}\n\t\tfree_extent_map(em);\n\t\tem = NULL;\n\t\tcur_offset = last_byte;\n\t\tif (cur_offset >= block_end)\n\t\t\tbreak;\n\t}\n\n\tfree_extent_map(em);\n\tunlock_extent_cached(io_tree, hole_start, block_end - 1, &cached_state,\n\t\t\t     GFP_NOFS);\n\treturn err;\n}\n\nstatic int btrfs_setsize(struct inode *inode, loff_t newsize)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tloff_t oldsize = i_size_read(inode);\n\tint ret;\n\n\tif (newsize == oldsize)\n\t\treturn 0;\n\n\tif (newsize > oldsize) {\n\t\ttruncate_pagecache(inode, oldsize, newsize);\n\t\tret = btrfs_cont_expand(inode, oldsize, newsize);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\ttrans = btrfs_start_transaction(root, 1);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\ti_size_write(inode, newsize);\n\t\tbtrfs_ordered_update_i_size(inode, i_size_read(inode), NULL);\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t\tbtrfs_end_transaction(trans, root);\n\t} else {\n\n\t\t/*\n\t\t * We're truncating a file that used to have good data down to\n\t\t * zero. Make sure it gets into the ordered flush list so that\n\t\t * any new writes get down to disk quickly.\n\t\t */\n\t\tif (newsize == 0)\n\t\t\tset_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,\n\t\t\t\t&BTRFS_I(inode)->runtime_flags);\n\n\t\t/* we don't support swapfiles, so vmtruncate shouldn't fail */\n\t\ttruncate_setsize(inode, newsize);\n\t\tret = btrfs_truncate(inode);\n\t}\n\n\treturn ret;\n}\n\nstatic int btrfs_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint err;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\terr = inode_change_ok(inode, attr);\n\tif (err)\n\t\treturn err;\n\n\tif (S_ISREG(inode->i_mode) && (attr->ia_valid & ATTR_SIZE)) {\n\t\terr = btrfs_setsize(inode, attr->ia_size);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (attr->ia_valid) {\n\t\tsetattr_copy(inode, attr);\n\t\tinode_inc_iversion(inode);\n\t\terr = btrfs_dirty_inode(inode);\n\n\t\tif (!err && attr->ia_valid & ATTR_MODE)\n\t\t\terr = btrfs_acl_chmod(inode);\n\t}\n\n\treturn err;\n}\n\nvoid btrfs_evict_inode(struct inode *inode)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_block_rsv *rsv, *global_rsv;\n\tu64 min_size = btrfs_calc_trunc_metadata_size(root, 1);\n\tint ret;\n\n\ttrace_btrfs_inode_evict(inode);\n\n\ttruncate_inode_pages(&inode->i_data, 0);\n\tif (inode->i_nlink && (btrfs_root_refs(&root->root_item) != 0 ||\n\t\t\t       btrfs_is_free_space_inode(inode)))\n\t\tgoto no_delete;\n\n\tif (is_bad_inode(inode)) {\n\t\tbtrfs_orphan_del(NULL, inode);\n\t\tgoto no_delete;\n\t}\n\t/* do we really want it for ->i_nlink > 0 and zero btrfs_root_refs? */\n\tbtrfs_wait_ordered_range(inode, 0, (u64)-1);\n\n\tif (root->fs_info->log_root_recovering) {\n\t\tBUG_ON(test_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t\t\t &BTRFS_I(inode)->runtime_flags));\n\t\tgoto no_delete;\n\t}\n\n\tif (inode->i_nlink > 0) {\n\t\tBUG_ON(btrfs_root_refs(&root->root_item) != 0);\n\t\tgoto no_delete;\n\t}\n\n\trsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);\n\tif (!rsv) {\n\t\tbtrfs_orphan_del(NULL, inode);\n\t\tgoto no_delete;\n\t}\n\trsv->size = min_size;\n\trsv->failfast = 1;\n\tglobal_rsv = &root->fs_info->global_block_rsv;\n\n\tbtrfs_i_size_write(inode, 0);\n\n\t/*\n\t * This is a bit simpler than btrfs_truncate since we've already\n\t * reserved our space for our orphan item in the unlink, so we just\n\t * need to reserve some slack space in case we add bytes and update\n\t * inode item when doing the truncate.\n\t */\n\twhile (1) {\n\t\tret = btrfs_block_rsv_refill(root, rsv, min_size,\n\t\t\t\t\t     BTRFS_RESERVE_FLUSH_LIMIT);\n\n\t\t/*\n\t\t * Try and steal from the global reserve since we will\n\t\t * likely not use this space anyway, we want to try as\n\t\t * hard as possible to get this to work.\n\t\t */\n\t\tif (ret)\n\t\t\tret = btrfs_block_rsv_migrate(global_rsv, rsv, min_size);\n\n\t\tif (ret) {\n\t\t\tprintk(KERN_WARNING \"Could not get space for a \"\n\t\t\t       \"delete, will truncate on mount %d\\n\", ret);\n\t\t\tbtrfs_orphan_del(NULL, inode);\n\t\t\tbtrfs_free_block_rsv(root, rsv);\n\t\t\tgoto no_delete;\n\t\t}\n\n\t\ttrans = btrfs_start_transaction_lflush(root, 1);\n\t\tif (IS_ERR(trans)) {\n\t\t\tbtrfs_orphan_del(NULL, inode);\n\t\t\tbtrfs_free_block_rsv(root, rsv);\n\t\t\tgoto no_delete;\n\t\t}\n\n\t\ttrans->block_rsv = rsv;\n\n\t\tret = btrfs_truncate_inode_items(trans, root, inode, 0, 0);\n\t\tif (ret != -ENOSPC)\n\t\t\tbreak;\n\n\t\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t\tBUG_ON(ret);\n\n\t\tbtrfs_end_transaction(trans, root);\n\t\ttrans = NULL;\n\t\tbtrfs_btree_balance_dirty(root);\n\t}\n\n\tbtrfs_free_block_rsv(root, rsv);\n\n\tif (ret == 0) {\n\t\ttrans->block_rsv = root->orphan_block_rsv;\n\t\tret = btrfs_orphan_del(trans, inode);\n\t\tBUG_ON(ret);\n\t}\n\n\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\tif (!(root == root->fs_info->tree_root ||\n\t      root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID))\n\t\tbtrfs_return_ino(root, btrfs_ino(inode));\n\n\tbtrfs_end_transaction(trans, root);\n\tbtrfs_btree_balance_dirty(root);\nno_delete:\n\tclear_inode(inode);\n\treturn;\n}\n\n/*\n * this returns the key found in the dir entry in the location pointer.\n * If no dir entries were found, location->objectid is 0.\n */\nstatic int btrfs_inode_by_name(struct inode *dir, struct dentry *dentry,\n\t\t\t       struct btrfs_key *location)\n{\n\tconst char *name = dentry->d_name.name;\n\tint namelen = dentry->d_name.len;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tint ret = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tdi = btrfs_lookup_dir_item(NULL, root, path, btrfs_ino(dir), name,\n\t\t\t\t    namelen, 0);\n\tif (IS_ERR(di))\n\t\tret = PTR_ERR(di);\n\n\tif (IS_ERR_OR_NULL(di))\n\t\tgoto out_err;\n\n\tbtrfs_dir_item_key_to_cpu(path->nodes[0], di, location);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\nout_err:\n\tlocation->objectid = 0;\n\tgoto out;\n}\n\n/*\n * when we hit a tree root in a directory, the btrfs part of the inode\n * needs to be changed to reflect the root directory of the tree root.  This\n * is kind of like crossing a mount point.\n */\nstatic int fixup_tree_root_location(struct btrfs_root *root,\n\t\t\t\t    struct inode *dir,\n\t\t\t\t    struct dentry *dentry,\n\t\t\t\t    struct btrfs_key *location,\n\t\t\t\t    struct btrfs_root **sub_root)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *new_root;\n\tstruct btrfs_root_ref *ref;\n\tstruct extent_buffer *leaf;\n\tint ret;\n\tint err = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = -ENOENT;\n\tret = btrfs_find_root_ref(root->fs_info->tree_root, path,\n\t\t\t\t  BTRFS_I(dir)->root->root_key.objectid,\n\t\t\t\t  location->objectid);\n\tif (ret) {\n\t\tif (ret < 0)\n\t\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tref = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_root_ref);\n\tif (btrfs_root_ref_dirid(leaf, ref) != btrfs_ino(dir) ||\n\t    btrfs_root_ref_name_len(leaf, ref) != dentry->d_name.len)\n\t\tgoto out;\n\n\tret = memcmp_extent_buffer(leaf, dentry->d_name.name,\n\t\t\t\t   (unsigned long)(ref + 1),\n\t\t\t\t   dentry->d_name.len);\n\tif (ret)\n\t\tgoto out;\n\n\tbtrfs_release_path(path);\n\n\tnew_root = btrfs_read_fs_root_no_name(root->fs_info, location);\n\tif (IS_ERR(new_root)) {\n\t\terr = PTR_ERR(new_root);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_root_refs(&new_root->root_item) == 0) {\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\n\t*sub_root = new_root;\n\tlocation->objectid = btrfs_root_dirid(&new_root->root_item);\n\tlocation->type = BTRFS_INODE_ITEM_KEY;\n\tlocation->offset = 0;\n\terr = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn err;\n}\n\nstatic void inode_tree_add(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_inode *entry;\n\tstruct rb_node **p;\n\tstruct rb_node *parent;\n\tu64 ino = btrfs_ino(inode);\nagain:\n\tp = &root->inode_tree.rb_node;\n\tparent = NULL;\n\n\tif (inode_unhashed(inode))\n\t\treturn;\n\n\tspin_lock(&root->inode_lock);\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct btrfs_inode, rb_node);\n\n\t\tif (ino < btrfs_ino(&entry->vfs_inode))\n\t\t\tp = &parent->rb_left;\n\t\telse if (ino > btrfs_ino(&entry->vfs_inode))\n\t\t\tp = &parent->rb_right;\n\t\telse {\n\t\t\tWARN_ON(!(entry->vfs_inode.i_state &\n\t\t\t\t  (I_WILL_FREE | I_FREEING)));\n\t\t\trb_erase(parent, &root->inode_tree);\n\t\t\tRB_CLEAR_NODE(parent);\n\t\t\tspin_unlock(&root->inode_lock);\n\t\t\tgoto again;\n\t\t}\n\t}\n\trb_link_node(&BTRFS_I(inode)->rb_node, parent, p);\n\trb_insert_color(&BTRFS_I(inode)->rb_node, &root->inode_tree);\n\tspin_unlock(&root->inode_lock);\n}\n\nstatic void inode_tree_del(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint empty = 0;\n\n\tspin_lock(&root->inode_lock);\n\tif (!RB_EMPTY_NODE(&BTRFS_I(inode)->rb_node)) {\n\t\trb_erase(&BTRFS_I(inode)->rb_node, &root->inode_tree);\n\t\tRB_CLEAR_NODE(&BTRFS_I(inode)->rb_node);\n\t\tempty = RB_EMPTY_ROOT(&root->inode_tree);\n\t}\n\tspin_unlock(&root->inode_lock);\n\n\t/*\n\t * Free space cache has inodes in the tree root, but the tree root has a\n\t * root_refs of 0, so this could end up dropping the tree root as a\n\t * snapshot, so we need the extra !root->fs_info->tree_root check to\n\t * make sure we don't drop it.\n\t */\n\tif (empty && btrfs_root_refs(&root->root_item) == 0 &&\n\t    root != root->fs_info->tree_root) {\n\t\tsynchronize_srcu(&root->fs_info->subvol_srcu);\n\t\tspin_lock(&root->inode_lock);\n\t\tempty = RB_EMPTY_ROOT(&root->inode_tree);\n\t\tspin_unlock(&root->inode_lock);\n\t\tif (empty)\n\t\t\tbtrfs_add_dead_root(root);\n\t}\n}\n\nvoid btrfs_invalidate_inodes(struct btrfs_root *root)\n{\n\tstruct rb_node *node;\n\tstruct rb_node *prev;\n\tstruct btrfs_inode *entry;\n\tstruct inode *inode;\n\tu64 objectid = 0;\n\n\tWARN_ON(btrfs_root_refs(&root->root_item) != 0);\n\n\tspin_lock(&root->inode_lock);\nagain:\n\tnode = root->inode_tree.rb_node;\n\tprev = NULL;\n\twhile (node) {\n\t\tprev = node;\n\t\tentry = rb_entry(node, struct btrfs_inode, rb_node);\n\n\t\tif (objectid < btrfs_ino(&entry->vfs_inode))\n\t\t\tnode = node->rb_left;\n\t\telse if (objectid > btrfs_ino(&entry->vfs_inode))\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\tbreak;\n\t}\n\tif (!node) {\n\t\twhile (prev) {\n\t\t\tentry = rb_entry(prev, struct btrfs_inode, rb_node);\n\t\t\tif (objectid <= btrfs_ino(&entry->vfs_inode)) {\n\t\t\t\tnode = prev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tprev = rb_next(prev);\n\t\t}\n\t}\n\twhile (node) {\n\t\tentry = rb_entry(node, struct btrfs_inode, rb_node);\n\t\tobjectid = btrfs_ino(&entry->vfs_inode) + 1;\n\t\tinode = igrab(&entry->vfs_inode);\n\t\tif (inode) {\n\t\t\tspin_unlock(&root->inode_lock);\n\t\t\tif (atomic_read(&inode->i_count) > 1)\n\t\t\t\td_prune_aliases(inode);\n\t\t\t/*\n\t\t\t * btrfs_drop_inode will have it removed from\n\t\t\t * the inode cache when its usage count\n\t\t\t * hits zero.\n\t\t\t */\n\t\t\tiput(inode);\n\t\t\tcond_resched();\n\t\t\tspin_lock(&root->inode_lock);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (cond_resched_lock(&root->inode_lock))\n\t\t\tgoto again;\n\n\t\tnode = rb_next(node);\n\t}\n\tspin_unlock(&root->inode_lock);\n}\n\nstatic int btrfs_init_locked_inode(struct inode *inode, void *p)\n{\n\tstruct btrfs_iget_args *args = p;\n\tinode->i_ino = args->ino;\n\tBTRFS_I(inode)->root = args->root;\n\treturn 0;\n}\n\nstatic int btrfs_find_actor(struct inode *inode, void *opaque)\n{\n\tstruct btrfs_iget_args *args = opaque;\n\treturn args->ino == btrfs_ino(inode) &&\n\t\targs->root == BTRFS_I(inode)->root;\n}\n\nstatic struct inode *btrfs_iget_locked(struct super_block *s,\n\t\t\t\t       u64 objectid,\n\t\t\t\t       struct btrfs_root *root)\n{\n\tstruct inode *inode;\n\tstruct btrfs_iget_args args;\n\targs.ino = objectid;\n\targs.root = root;\n\n\tinode = iget5_locked(s, objectid, btrfs_find_actor,\n\t\t\t     btrfs_init_locked_inode,\n\t\t\t     (void *)&args);\n\treturn inode;\n}\n\n/* Get an inode object given its location and corresponding root.\n * Returns in *is_new if the inode was read from disk\n */\nstruct inode *btrfs_iget(struct super_block *s, struct btrfs_key *location,\n\t\t\t struct btrfs_root *root, int *new)\n{\n\tstruct inode *inode;\n\n\tinode = btrfs_iget_locked(s, location->objectid, root);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (inode->i_state & I_NEW) {\n\t\tBTRFS_I(inode)->root = root;\n\t\tmemcpy(&BTRFS_I(inode)->location, location, sizeof(*location));\n\t\tbtrfs_read_locked_inode(inode);\n\t\tif (!is_bad_inode(inode)) {\n\t\t\tinode_tree_add(inode);\n\t\t\tunlock_new_inode(inode);\n\t\t\tif (new)\n\t\t\t\t*new = 1;\n\t\t} else {\n\t\t\tunlock_new_inode(inode);\n\t\t\tiput(inode);\n\t\t\tinode = ERR_PTR(-ESTALE);\n\t\t}\n\t}\n\n\treturn inode;\n}\n\nstatic struct inode *new_simple_dir(struct super_block *s,\n\t\t\t\t    struct btrfs_key *key,\n\t\t\t\t    struct btrfs_root *root)\n{\n\tstruct inode *inode = new_inode(s);\n\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tBTRFS_I(inode)->root = root;\n\tmemcpy(&BTRFS_I(inode)->location, key, sizeof(*key));\n\tset_bit(BTRFS_INODE_DUMMY, &BTRFS_I(inode)->runtime_flags);\n\n\tinode->i_ino = BTRFS_EMPTY_SUBVOL_DIR_OBJECTID;\n\tinode->i_op = &btrfs_dir_ro_inode_operations;\n\tinode->i_fop = &simple_dir_operations;\n\tinode->i_mode = S_IFDIR | S_IRUGO | S_IWUSR | S_IXUGO;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;\n\n\treturn inode;\n}\n\nstruct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *sub_root = root;\n\tstruct btrfs_key location;\n\tint index;\n\tint ret = 0;\n\n\tif (dentry->d_name.len > BTRFS_NAME_LEN)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tif (unlikely(d_need_lookup(dentry))) {\n\t\tmemcpy(&location, dentry->d_fsdata, sizeof(struct btrfs_key));\n\t\tkfree(dentry->d_fsdata);\n\t\tdentry->d_fsdata = NULL;\n\t\t/* This thing is hashed, drop it for now */\n\t\td_drop(dentry);\n\t} else {\n\t\tret = btrfs_inode_by_name(dir, dentry, &location);\n\t}\n\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tif (location.objectid == 0)\n\t\treturn NULL;\n\n\tif (location.type == BTRFS_INODE_ITEM_KEY) {\n\t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n\t\treturn inode;\n\t}\n\n\tBUG_ON(location.type != BTRFS_ROOT_ITEM_KEY);\n\n\tindex = srcu_read_lock(&root->fs_info->subvol_srcu);\n\tret = fixup_tree_root_location(root, dir, dentry,\n\t\t\t\t       &location, &sub_root);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT)\n\t\t\tinode = ERR_PTR(ret);\n\t\telse\n\t\t\tinode = new_simple_dir(dir->i_sb, &location, sub_root);\n\t} else {\n\t\tinode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n\t}\n\tsrcu_read_unlock(&root->fs_info->subvol_srcu, index);\n\n\tif (!IS_ERR(inode) && root != sub_root) {\n\t\tdown_read(&root->fs_info->cleanup_work_sem);\n\t\tif (!(inode->i_sb->s_flags & MS_RDONLY))\n\t\t\tret = btrfs_orphan_cleanup(sub_root);\n\t\tup_read(&root->fs_info->cleanup_work_sem);\n\t\tif (ret)\n\t\t\tinode = ERR_PTR(ret);\n\t}\n\n\treturn inode;\n}\n\nstatic int btrfs_dentry_delete(const struct dentry *dentry)\n{\n\tstruct btrfs_root *root;\n\tstruct inode *inode = dentry->d_inode;\n\n\tif (!inode && !IS_ROOT(dentry))\n\t\tinode = dentry->d_parent->d_inode;\n\n\tif (inode) {\n\t\troot = BTRFS_I(inode)->root;\n\t\tif (btrfs_root_refs(&root->root_item) == 0)\n\t\t\treturn 1;\n\n\t\tif (btrfs_ino(inode) == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void btrfs_dentry_release(struct dentry *dentry)\n{\n\tif (dentry->d_fsdata)\n\t\tkfree(dentry->d_fsdata);\n}\n\nstatic struct dentry *btrfs_lookup(struct inode *dir, struct dentry *dentry,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct dentry *ret;\n\n\tret = d_splice_alias(btrfs_lookup_dentry(dir, dentry), dentry);\n\tif (unlikely(d_need_lookup(dentry))) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_NEED_LOOKUP;\n\t\tspin_unlock(&dentry->d_lock);\n\t}\n\treturn ret;\n}\n\nunsigned char btrfs_filetype_table[] = {\n\tDT_UNKNOWN, DT_REG, DT_DIR, DT_CHR, DT_BLK, DT_FIFO, DT_SOCK, DT_LNK\n};\n\nstatic int btrfs_real_readdir(struct file *filp, void *dirent,\n\t\t\t      filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_dentry->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_item *item;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_path *path;\n\tstruct list_head ins_list;\n\tstruct list_head del_list;\n\tint ret;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tunsigned char d_type;\n\tint over = 0;\n\tu32 di_cur;\n\tu32 di_total;\n\tu32 di_len;\n\tint key_type = BTRFS_DIR_INDEX_KEY;\n\tchar tmp_name[32];\n\tchar *name_ptr;\n\tint name_len;\n\tint is_curr = 0;\t/* filp->f_pos points to the current index? */\n\n\t/* FIXME, use a real flag for deciding about the key type */\n\tif (root->fs_info->tree_root == root)\n\t\tkey_type = BTRFS_DIR_ITEM_KEY;\n\n\t/* special case for \".\" */\n\tif (filp->f_pos == 0) {\n\t\tover = filldir(dirent, \".\", 1,\n\t\t\t       filp->f_pos, btrfs_ino(inode), DT_DIR);\n\t\tif (over)\n\t\t\treturn 0;\n\t\tfilp->f_pos = 1;\n\t}\n\t/* special case for .., just use the back ref */\n\tif (filp->f_pos == 1) {\n\t\tu64 pino = parent_ino(filp->f_path.dentry);\n\t\tover = filldir(dirent, \"..\", 2,\n\t\t\t       filp->f_pos, pino, DT_DIR);\n\t\tif (over)\n\t\t\treturn 0;\n\t\tfilp->f_pos = 2;\n\t}\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = 1;\n\n\tif (key_type == BTRFS_DIR_INDEX_KEY) {\n\t\tINIT_LIST_HEAD(&ins_list);\n\t\tINIT_LIST_HEAD(&del_list);\n\t\tbtrfs_get_delayed_items(inode, &ins_list, &del_list);\n\t}\n\n\tbtrfs_set_key_type(&key, key_type);\n\tkey.offset = filp->f_pos;\n\tkey.objectid = btrfs_ino(inode);\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto err;\n\n\twhile (1) {\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto err;\n\t\t\telse if (ret > 0)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\titem = btrfs_item_nr(leaf, slot);\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid != key.objectid)\n\t\t\tbreak;\n\t\tif (btrfs_key_type(&found_key) != key_type)\n\t\t\tbreak;\n\t\tif (found_key.offset < filp->f_pos)\n\t\t\tgoto next;\n\t\tif (key_type == BTRFS_DIR_INDEX_KEY &&\n\t\t    btrfs_should_delete_dir_index(&del_list,\n\t\t\t\t\t\t  found_key.offset))\n\t\t\tgoto next;\n\n\t\tfilp->f_pos = found_key.offset;\n\t\tis_curr = 1;\n\n\t\tdi = btrfs_item_ptr(leaf, slot, struct btrfs_dir_item);\n\t\tdi_cur = 0;\n\t\tdi_total = btrfs_item_size(leaf, item);\n\n\t\twhile (di_cur < di_total) {\n\t\t\tstruct btrfs_key location;\n\n\t\t\tif (verify_dir_item(root, leaf, di))\n\t\t\t\tbreak;\n\n\t\t\tname_len = btrfs_dir_name_len(leaf, di);\n\t\t\tif (name_len <= sizeof(tmp_name)) {\n\t\t\t\tname_ptr = tmp_name;\n\t\t\t} else {\n\t\t\t\tname_ptr = kmalloc(name_len, GFP_NOFS);\n\t\t\t\tif (!name_ptr) {\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t}\n\t\t\tread_extent_buffer(leaf, name_ptr,\n\t\t\t\t\t   (unsigned long)(di + 1), name_len);\n\n\t\t\td_type = btrfs_filetype_table[btrfs_dir_type(leaf, di)];\n\t\t\tbtrfs_dir_item_key_to_cpu(leaf, di, &location);\n\n\n\t\t\t/* is this a reference to our own snapshot? If so\n\t\t\t * skip it.\n\t\t\t *\n\t\t\t * In contrast to old kernels, we insert the snapshot's\n\t\t\t * dir item and dir index after it has been created, so\n\t\t\t * we won't find a reference to our own snapshot. We\n\t\t\t * still keep the following code for backward\n\t\t\t * compatibility.\n\t\t\t */\n\t\t\tif (location.type == BTRFS_ROOT_ITEM_KEY &&\n\t\t\t    location.objectid == root->root_key.objectid) {\n\t\t\t\tover = 0;\n\t\t\t\tgoto skip;\n\t\t\t}\n\t\t\tover = filldir(dirent, name_ptr, name_len,\n\t\t\t\t       found_key.offset, location.objectid,\n\t\t\t\t       d_type);\n\nskip:\n\t\t\tif (name_ptr != tmp_name)\n\t\t\t\tkfree(name_ptr);\n\n\t\t\tif (over)\n\t\t\t\tgoto nopos;\n\t\t\tdi_len = btrfs_dir_name_len(leaf, di) +\n\t\t\t\t btrfs_dir_data_len(leaf, di) + sizeof(*di);\n\t\t\tdi_cur += di_len;\n\t\t\tdi = (struct btrfs_dir_item *)((char *)di + di_len);\n\t\t}\nnext:\n\t\tpath->slots[0]++;\n\t}\n\n\tif (key_type == BTRFS_DIR_INDEX_KEY) {\n\t\tif (is_curr)\n\t\t\tfilp->f_pos++;\n\t\tret = btrfs_readdir_delayed_dir_index(filp, dirent, filldir,\n\t\t\t\t\t\t      &ins_list);\n\t\tif (ret)\n\t\t\tgoto nopos;\n\t}\n\n\t/* Reached end of directory/root. Bump pos past the last item. */\n\tif (key_type == BTRFS_DIR_INDEX_KEY)\n\t\t/*\n\t\t * 32-bit glibc will use getdents64, but then strtol -\n\t\t * so the last number we can serve is this.\n\t\t */\n\t\tfilp->f_pos = 0x7fffffff;\n\telse\n\t\tfilp->f_pos++;\nnopos:\n\tret = 0;\nerr:\n\tif (key_type == BTRFS_DIR_INDEX_KEY)\n\t\tbtrfs_put_delayed_items(&ins_list, &del_list);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_write_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tint ret = 0;\n\tbool nolock = false;\n\n\tif (test_bit(BTRFS_INODE_DUMMY, &BTRFS_I(inode)->runtime_flags))\n\t\treturn 0;\n\n\tif (btrfs_fs_closing(root->fs_info) && btrfs_is_free_space_inode(inode))\n\t\tnolock = true;\n\n\tif (wbc->sync_mode == WB_SYNC_ALL) {\n\t\tif (nolock)\n\t\t\ttrans = btrfs_join_transaction_nolock(root);\n\t\telse\n\t\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\t\tret = btrfs_commit_transaction(trans, root);\n\t}\n\treturn ret;\n}\n\n/*\n * This is somewhat expensive, updating the tree every time the\n * inode changes.  But, it is most likely to find the inode in cache.\n * FIXME, needs more benchmarking...there are no reasons other than performance\n * to keep or drop this code.\n */\nint btrfs_dirty_inode(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\tif (test_bit(BTRFS_INODE_DUMMY, &BTRFS_I(inode)->runtime_flags))\n\t\treturn 0;\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tret = btrfs_update_inode(trans, root, inode);\n\tif (ret && ret == -ENOSPC) {\n\t\t/* whoops, lets try again with the full transaction */\n\t\tbtrfs_end_transaction(trans, root);\n\t\ttrans = btrfs_start_transaction(root, 1);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t}\n\tbtrfs_end_transaction(trans, root);\n\tif (BTRFS_I(inode)->delayed_node)\n\t\tbtrfs_balance_delayed_items(root);\n\n\treturn ret;\n}\n\n/*\n * This is a copy of file_update_time.  We need this so we can return error on\n * ENOSPC for updating the inode in the case of file write and mmap writes.\n */\nstatic int btrfs_update_time(struct inode *inode, struct timespec *now,\n\t\t\t     int flags)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tif (flags & S_VERSION)\n\t\tinode_inc_iversion(inode);\n\tif (flags & S_CTIME)\n\t\tinode->i_ctime = *now;\n\tif (flags & S_MTIME)\n\t\tinode->i_mtime = *now;\n\tif (flags & S_ATIME)\n\t\tinode->i_atime = *now;\n\treturn btrfs_dirty_inode(inode);\n}\n\n/*\n * find the highest existing sequence number in a directory\n * and then set the in-memory index_cnt variable to reflect\n * free sequence numbers\n */\nstatic int btrfs_set_inode_index_count(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_key key, found_key;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tint ret;\n\n\tkey.objectid = btrfs_ino(inode);\n\tbtrfs_set_key_type(&key, BTRFS_DIR_INDEX_KEY);\n\tkey.offset = (u64)-1;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\t/* FIXME: we should be able to handle this */\n\tif (ret == 0)\n\t\tgoto out;\n\tret = 0;\n\n\t/*\n\t * MAGIC NUMBER EXPLANATION:\n\t * since we search a directory based on f_pos we have to start at 2\n\t * since '.' and '..' have f_pos of 0 and 1 respectively, so everybody\n\t * else has to start at 2\n\t */\n\tif (path->slots[0] == 0) {\n\t\tBTRFS_I(inode)->index_cnt = 2;\n\t\tgoto out;\n\t}\n\n\tpath->slots[0]--;\n\n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\tif (found_key.objectid != btrfs_ino(inode) ||\n\t    btrfs_key_type(&found_key) != BTRFS_DIR_INDEX_KEY) {\n\t\tBTRFS_I(inode)->index_cnt = 2;\n\t\tgoto out;\n\t}\n\n\tBTRFS_I(inode)->index_cnt = found_key.offset + 1;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * helper to find a free sequence number in a given directory.  This current\n * code is very simple, later versions will do smarter things in the btree\n */\nint btrfs_set_inode_index(struct inode *dir, u64 *index)\n{\n\tint ret = 0;\n\n\tif (BTRFS_I(dir)->index_cnt == (u64)-1) {\n\t\tret = btrfs_inode_delayed_dir_index_count(dir);\n\t\tif (ret) {\n\t\t\tret = btrfs_set_inode_index_count(dir);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\t*index = BTRFS_I(dir)->index_cnt;\n\tBTRFS_I(dir)->index_cnt++;\n\n\treturn ret;\n}\n\nstatic struct inode *btrfs_new_inode(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t     struct inode *dir,\n\t\t\t\t     const char *name, int name_len,\n\t\t\t\t     u64 ref_objectid, u64 objectid,\n\t\t\t\t     umode_t mode, u64 *index)\n{\n\tstruct inode *inode;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct btrfs_key *location;\n\tstruct btrfs_path *path;\n\tstruct btrfs_inode_ref *ref;\n\tstruct btrfs_key key[2];\n\tu32 sizes[2];\n\tunsigned long ptr;\n\tint ret;\n\tint owner;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinode = new_inode(root->fs_info->sb);\n\tif (!inode) {\n\t\tbtrfs_free_path(path);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/*\n\t * we have to initialize this early, so we can reclaim the inode\n\t * number if we fail afterwards in this function.\n\t */\n\tinode->i_ino = objectid;\n\n\tif (dir) {\n\t\ttrace_btrfs_inode_request(dir);\n\n\t\tret = btrfs_set_inode_index(dir, index);\n\t\tif (ret) {\n\t\t\tbtrfs_free_path(path);\n\t\t\tiput(inode);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\t/*\n\t * index_cnt is ignored for everything but a dir,\n\t * btrfs_get_inode_index_count has an explanation for the magic\n\t * number\n\t */\n\tBTRFS_I(inode)->index_cnt = 2;\n\tBTRFS_I(inode)->root = root;\n\tBTRFS_I(inode)->generation = trans->transid;\n\tinode->i_generation = BTRFS_I(inode)->generation;\n\n\t/*\n\t * We could have gotten an inode number from somebody who was fsynced\n\t * and then removed in this same transaction, so let's just set full\n\t * sync since it will be a full sync anyway and this will blow away the\n\t * old info in the log.\n\t */\n\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &BTRFS_I(inode)->runtime_flags);\n\n\tif (S_ISDIR(mode))\n\t\towner = 0;\n\telse\n\t\towner = 1;\n\n\tkey[0].objectid = objectid;\n\tbtrfs_set_key_type(&key[0], BTRFS_INODE_ITEM_KEY);\n\tkey[0].offset = 0;\n\n\t/*\n\t * Start new inodes with an inode_ref. This is slightly more\n\t * efficient for small numbers of hard links since they will\n\t * be packed into one item. Extended refs will kick in if we\n\t * add more hard links than can fit in the ref item.\n\t */\n\tkey[1].objectid = objectid;\n\tbtrfs_set_key_type(&key[1], BTRFS_INODE_REF_KEY);\n\tkey[1].offset = ref_objectid;\n\n\tsizes[0] = sizeof(struct btrfs_inode_item);\n\tsizes[1] = name_len + sizeof(*ref);\n\n\tpath->leave_spinning = 1;\n\tret = btrfs_insert_empty_items(trans, root, path, key, sizes, 2);\n\tif (ret != 0)\n\t\tgoto fail;\n\n\tinode_init_owner(inode, dir, mode);\n\tinode_set_bytes(inode, 0);\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;\n\tinode_item = btrfs_item_ptr(path->nodes[0], path->slots[0],\n\t\t\t\t  struct btrfs_inode_item);\n\tmemset_extent_buffer(path->nodes[0], 0, (unsigned long)inode_item,\n\t\t\t     sizeof(*inode_item));\n\tfill_inode_item(trans, path->nodes[0], inode_item, inode);\n\n\tref = btrfs_item_ptr(path->nodes[0], path->slots[0] + 1,\n\t\t\t     struct btrfs_inode_ref);\n\tbtrfs_set_inode_ref_name_len(path->nodes[0], ref, name_len);\n\tbtrfs_set_inode_ref_index(path->nodes[0], ref, *index);\n\tptr = (unsigned long)(ref + 1);\n\twrite_extent_buffer(path->nodes[0], name, ptr, name_len);\n\n\tbtrfs_mark_buffer_dirty(path->nodes[0]);\n\tbtrfs_free_path(path);\n\n\tlocation = &BTRFS_I(inode)->location;\n\tlocation->objectid = objectid;\n\tlocation->offset = 0;\n\tbtrfs_set_key_type(location, BTRFS_INODE_ITEM_KEY);\n\n\tbtrfs_inherit_iflags(inode, dir);\n\n\tif (S_ISREG(mode)) {\n\t\tif (btrfs_test_opt(root, NODATASUM))\n\t\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NODATASUM;\n\t\tif (btrfs_test_opt(root, NODATACOW) ||\n\t\t    (BTRFS_I(dir)->flags & BTRFS_INODE_NODATACOW))\n\t\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NODATACOW;\n\t}\n\n\tinsert_inode_hash(inode);\n\tinode_tree_add(inode);\n\n\ttrace_btrfs_inode_new(inode);\n\tbtrfs_set_inode_last_trans(trans, inode);\n\n\tbtrfs_update_root_times(trans, root);\n\n\treturn inode;\nfail:\n\tif (dir)\n\t\tBTRFS_I(dir)->index_cnt--;\n\tbtrfs_free_path(path);\n\tiput(inode);\n\treturn ERR_PTR(ret);\n}\n\nstatic inline u8 btrfs_inode_type(struct inode *inode)\n{\n\treturn btrfs_type_by_mode[(inode->i_mode & S_IFMT) >> S_SHIFT];\n}\n\n/*\n * utility function to add 'inode' into 'parent_inode' with\n * a give name and a given sequence number.\n * if 'add_backref' is true, also insert a backref from the\n * inode to the parent directory.\n */\nint btrfs_add_link(struct btrfs_trans_handle *trans,\n\t\t   struct inode *parent_inode, struct inode *inode,\n\t\t   const char *name, int name_len, int add_backref, u64 index)\n{\n\tint ret = 0;\n\tstruct btrfs_key key;\n\tstruct btrfs_root *root = BTRFS_I(parent_inode)->root;\n\tu64 ino = btrfs_ino(inode);\n\tu64 parent_ino = btrfs_ino(parent_inode);\n\n\tif (unlikely(ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\tmemcpy(&key, &BTRFS_I(inode)->root->root_key, sizeof(key));\n\t} else {\n\t\tkey.objectid = ino;\n\t\tbtrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);\n\t\tkey.offset = 0;\n\t}\n\n\tif (unlikely(ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\tret = btrfs_add_root_ref(trans, root->fs_info->tree_root,\n\t\t\t\t\t key.objectid, root->root_key.objectid,\n\t\t\t\t\t parent_ino, index, name, name_len);\n\t} else if (add_backref) {\n\t\tret = btrfs_insert_inode_ref(trans, root, name, name_len, ino,\n\t\t\t\t\t     parent_ino, index);\n\t}\n\n\t/* Nothing to clean up yet */\n\tif (ret)\n\t\treturn ret;\n\n\tret = btrfs_insert_dir_item(trans, root, name, name_len,\n\t\t\t\t    parent_inode, &key,\n\t\t\t\t    btrfs_inode_type(inode), index);\n\tif (ret == -EEXIST || ret == -EOVERFLOW)\n\t\tgoto fail_dir_item;\n\telse if (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\treturn ret;\n\t}\n\n\tbtrfs_i_size_write(parent_inode, parent_inode->i_size +\n\t\t\t   name_len * 2);\n\tinode_inc_iversion(parent_inode);\n\tparent_inode->i_mtime = parent_inode->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode(trans, root, parent_inode);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\treturn ret;\n\nfail_dir_item:\n\tif (unlikely(ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\tu64 local_index;\n\t\tint err;\n\t\terr = btrfs_del_root_ref(trans, root->fs_info->tree_root,\n\t\t\t\t key.objectid, root->root_key.objectid,\n\t\t\t\t parent_ino, &local_index, name, name_len);\n\n\t} else if (add_backref) {\n\t\tu64 local_index;\n\t\tint err;\n\n\t\terr = btrfs_del_inode_ref(trans, root, name, name_len,\n\t\t\t\t\t  ino, parent_ino, &local_index);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_add_nondir(struct btrfs_trans_handle *trans,\n\t\t\t    struct inode *dir, struct dentry *dentry,\n\t\t\t    struct inode *inode, int backref, u64 index)\n{\n\tint err = btrfs_add_link(trans, dir, inode,\n\t\t\t\t dentry->d_name.name, dentry->d_name.len,\n\t\t\t\t backref, index);\n\tif (err > 0)\n\t\terr = -EEXIST;\n\treturn err;\n}\n\nstatic int btrfs_mknod(struct inode *dir, struct dentry *dentry,\n\t\t\tumode_t mode, dev_t rdev)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct inode *inode = NULL;\n\tint err;\n\tint drop_inode = 0;\n\tu64 objectid;\n\tu64 index = 0;\n\n\tif (!new_valid_dev(rdev))\n\t\treturn -EINVAL;\n\n\t/*\n\t * 2 for inode item and ref\n\t * 2 for dir items\n\t * 1 for xattr if selinux is on\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\terr = btrfs_find_free_ino(root, &objectid);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tinode = btrfs_new_inode(trans, root, dir, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len, btrfs_ino(dir), objectid,\n\t\t\t\tmode, &index);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\tgoto out_unlock;\n\t}\n\n\terr = btrfs_init_inode_security(trans, inode, dir, &dentry->d_name);\n\tif (err) {\n\t\tdrop_inode = 1;\n\t\tgoto out_unlock;\n\t}\n\n\terr = btrfs_update_inode(trans, root, inode);\n\tif (err) {\n\t\tdrop_inode = 1;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t* If the active LSM wants to access the inode during\n\t* d_instantiate it needs these. Smack checks to see\n\t* if the filesystem supports xattrs by looking at the\n\t* ops vector.\n\t*/\n\n\tinode->i_op = &btrfs_special_inode_operations;\n\terr = btrfs_add_nondir(trans, dir, dentry, inode, 0, index);\n\tif (err)\n\t\tdrop_inode = 1;\n\telse {\n\t\tinit_special_inode(inode, inode->i_mode, rdev);\n\t\tbtrfs_update_inode(trans, root, inode);\n\t\td_instantiate(dentry, inode);\n\t}\nout_unlock:\n\tbtrfs_end_transaction(trans, root);\n\tbtrfs_btree_balance_dirty(root);\n\tif (drop_inode) {\n\t\tinode_dec_link_count(inode);\n\t\tiput(inode);\n\t}\n\treturn err;\n}\n\nstatic int btrfs_create(struct inode *dir, struct dentry *dentry,\n\t\t\tumode_t mode, bool excl)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct inode *inode = NULL;\n\tint drop_inode_on_err = 0;\n\tint err;\n\tu64 objectid;\n\tu64 index = 0;\n\n\t/*\n\t * 2 for inode item and ref\n\t * 2 for dir items\n\t * 1 for xattr if selinux is on\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\terr = btrfs_find_free_ino(root, &objectid);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tinode = btrfs_new_inode(trans, root, dir, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len, btrfs_ino(dir), objectid,\n\t\t\t\tmode, &index);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\tgoto out_unlock;\n\t}\n\tdrop_inode_on_err = 1;\n\n\terr = btrfs_init_inode_security(trans, inode, dir, &dentry->d_name);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = btrfs_update_inode(trans, root, inode);\n\tif (err)\n\t\tgoto out_unlock;\n\n\t/*\n\t* If the active LSM wants to access the inode during\n\t* d_instantiate it needs these. Smack checks to see\n\t* if the filesystem supports xattrs by looking at the\n\t* ops vector.\n\t*/\n\tinode->i_fop = &btrfs_file_operations;\n\tinode->i_op = &btrfs_file_inode_operations;\n\n\terr = btrfs_add_nondir(trans, dir, dentry, inode, 0, index);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tinode->i_mapping->a_ops = &btrfs_aops;\n\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\tBTRFS_I(inode)->io_tree.ops = &btrfs_extent_io_ops;\n\td_instantiate(dentry, inode);\n\nout_unlock:\n\tbtrfs_end_transaction(trans, root);\n\tif (err && drop_inode_on_err) {\n\t\tinode_dec_link_count(inode);\n\t\tiput(inode);\n\t}\n\tbtrfs_btree_balance_dirty(root);\n\treturn err;\n}\n\nstatic int btrfs_link(struct dentry *old_dentry, struct inode *dir,\n\t\t      struct dentry *dentry)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct inode *inode = old_dentry->d_inode;\n\tu64 index;\n\tint err;\n\tint drop_inode = 0;\n\n\t/* do not allow sys_link's with other subvols of the same device */\n\tif (root->objectid != BTRFS_I(inode)->root->objectid)\n\t\treturn -EXDEV;\n\n\tif (inode->i_nlink >= BTRFS_LINK_MAX)\n\t\treturn -EMLINK;\n\n\terr = btrfs_set_inode_index(dir, &index);\n\tif (err)\n\t\tgoto fail;\n\n\t/*\n\t * 2 items for inode and inode ref\n\t * 2 items for dir items\n\t * 1 item for parent inode\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans)) {\n\t\terr = PTR_ERR(trans);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_inc_nlink(inode);\n\tinode_inc_iversion(inode);\n\tinode->i_ctime = CURRENT_TIME;\n\tihold(inode);\n\tset_bit(BTRFS_INODE_COPY_EVERYTHING, &BTRFS_I(inode)->runtime_flags);\n\n\terr = btrfs_add_nondir(trans, dir, dentry, inode, 1, index);\n\n\tif (err) {\n\t\tdrop_inode = 1;\n\t} else {\n\t\tstruct dentry *parent = dentry->d_parent;\n\t\terr = btrfs_update_inode(trans, root, inode);\n\t\tif (err)\n\t\t\tgoto fail;\n\t\td_instantiate(dentry, inode);\n\t\tbtrfs_log_new_name(trans, inode, NULL, parent);\n\t}\n\n\tbtrfs_end_transaction(trans, root);\nfail:\n\tif (drop_inode) {\n\t\tinode_dec_link_count(inode);\n\t\tiput(inode);\n\t}\n\tbtrfs_btree_balance_dirty(root);\n\treturn err;\n}\n\nstatic int btrfs_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)\n{\n\tstruct inode *inode = NULL;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tint err = 0;\n\tint drop_on_err = 0;\n\tu64 objectid = 0;\n\tu64 index = 0;\n\n\t/*\n\t * 2 items for inode and ref\n\t * 2 items for dir items\n\t * 1 for xattr if selinux is on\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\terr = btrfs_find_free_ino(root, &objectid);\n\tif (err)\n\t\tgoto out_fail;\n\n\tinode = btrfs_new_inode(trans, root, dir, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len, btrfs_ino(dir), objectid,\n\t\t\t\tS_IFDIR | mode, &index);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\tgoto out_fail;\n\t}\n\n\tdrop_on_err = 1;\n\n\terr = btrfs_init_inode_security(trans, inode, dir, &dentry->d_name);\n\tif (err)\n\t\tgoto out_fail;\n\n\tinode->i_op = &btrfs_dir_inode_operations;\n\tinode->i_fop = &btrfs_dir_file_operations;\n\n\tbtrfs_i_size_write(inode, 0);\n\terr = btrfs_update_inode(trans, root, inode);\n\tif (err)\n\t\tgoto out_fail;\n\n\terr = btrfs_add_link(trans, dir, inode, dentry->d_name.name,\n\t\t\t     dentry->d_name.len, 0, index);\n\tif (err)\n\t\tgoto out_fail;\n\n\td_instantiate(dentry, inode);\n\tdrop_on_err = 0;\n\nout_fail:\n\tbtrfs_end_transaction(trans, root);\n\tif (drop_on_err)\n\t\tiput(inode);\n\tbtrfs_btree_balance_dirty(root);\n\treturn err;\n}\n\n/* helper for btfs_get_extent.  Given an existing extent in the tree,\n * and an extent that you want to insert, deal with overlap and insert\n * the new extent into the tree.\n */\nstatic int merge_extent_mapping(struct extent_map_tree *em_tree,\n\t\t\t\tstruct extent_map *existing,\n\t\t\t\tstruct extent_map *em,\n\t\t\t\tu64 map_start, u64 map_len)\n{\n\tu64 start_diff;\n\n\tBUG_ON(map_start < em->start || map_start >= extent_map_end(em));\n\tstart_diff = map_start - em->start;\n\tem->start = map_start;\n\tem->len = map_len;\n\tif (em->block_start < EXTENT_MAP_LAST_BYTE &&\n\t    !test_bit(EXTENT_FLAG_COMPRESSED, &em->flags)) {\n\t\tem->block_start += start_diff;\n\t\tem->block_len -= start_diff;\n\t}\n\treturn add_extent_mapping(em_tree, em);\n}\n\nstatic noinline int uncompress_inline(struct btrfs_path *path,\n\t\t\t\t      struct inode *inode, struct page *page,\n\t\t\t\t      size_t pg_offset, u64 extent_offset,\n\t\t\t\t      struct btrfs_file_extent_item *item)\n{\n\tint ret;\n\tstruct extent_buffer *leaf = path->nodes[0];\n\tchar *tmp;\n\tsize_t max_size;\n\tunsigned long inline_size;\n\tunsigned long ptr;\n\tint compress_type;\n\n\tWARN_ON(pg_offset != 0);\n\tcompress_type = btrfs_file_extent_compression(leaf, item);\n\tmax_size = btrfs_file_extent_ram_bytes(leaf, item);\n\tinline_size = btrfs_file_extent_inline_item_len(leaf,\n\t\t\t\t\tbtrfs_item_nr(leaf, path->slots[0]));\n\ttmp = kmalloc(inline_size, GFP_NOFS);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\tptr = btrfs_file_extent_inline_start(item);\n\n\tread_extent_buffer(leaf, tmp, ptr, inline_size);\n\n\tmax_size = min_t(unsigned long, PAGE_CACHE_SIZE, max_size);\n\tret = btrfs_decompress(compress_type, tmp, page,\n\t\t\t       extent_offset, inline_size, max_size);\n\tif (ret) {\n\t\tchar *kaddr = kmap_atomic(page);\n\t\tunsigned long copy_size = min_t(u64,\n\t\t\t\t  PAGE_CACHE_SIZE - pg_offset,\n\t\t\t\t  max_size - extent_offset);\n\t\tmemset(kaddr + pg_offset, 0, copy_size);\n\t\tkunmap_atomic(kaddr);\n\t}\n\tkfree(tmp);\n\treturn 0;\n}\n\n/*\n * a bit scary, this does extent mapping from logical file offset to the disk.\n * the ugly parts come from merging extents from the disk with the in-ram\n * representation.  This gets more complex because of the data=ordered code,\n * where the in-ram extents might be locked pending data=ordered completion.\n *\n * This also copies inline extents directly into the page.\n */\n\nstruct extent_map *btrfs_get_extent(struct inode *inode, struct page *page,\n\t\t\t\t    size_t pg_offset, u64 start, u64 len,\n\t\t\t\t    int create)\n{\n\tint ret;\n\tint err = 0;\n\tu64 bytenr;\n\tu64 extent_start = 0;\n\tu64 extent_end = 0;\n\tu64 objectid = btrfs_ino(inode);\n\tu32 found_type;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_file_extent_item *item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key found_key;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tint compress_type;\n\nagain:\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tif (em)\n\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tif (em->start > start || em->start + em->len <= start)\n\t\t\tfree_extent_map(em);\n\t\telse if (em->block_start == EXTENT_MAP_INLINE && page)\n\t\t\tfree_extent_map(em);\n\t\telse\n\t\t\tgoto out;\n\t}\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\tem->start = EXTENT_MAP_HOLE;\n\tem->orig_start = EXTENT_MAP_HOLE;\n\tem->len = (u64)-1;\n\tem->block_len = (u64)-1;\n\n\tif (!path) {\n\t\tpath = btrfs_alloc_path();\n\t\tif (!path) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * Chances are we'll be called again, so go ahead and do\n\t\t * readahead\n\t\t */\n\t\tpath->reada = 1;\n\t}\n\n\tret = btrfs_lookup_file_extent(trans, root, path,\n\t\t\t\t       objectid, start, trans != NULL);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tif (ret != 0) {\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto not_found;\n\t\tpath->slots[0]--;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t      struct btrfs_file_extent_item);\n\t/* are we inside the extent that was found? */\n\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\tfound_type = btrfs_key_type(&found_key);\n\tif (found_key.objectid != objectid ||\n\t    found_type != BTRFS_EXTENT_DATA_KEY) {\n\t\tgoto not_found;\n\t}\n\n\tfound_type = btrfs_file_extent_type(leaf, item);\n\textent_start = found_key.offset;\n\tcompress_type = btrfs_file_extent_compression(leaf, item);\n\tif (found_type == BTRFS_FILE_EXTENT_REG ||\n\t    found_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\textent_end = extent_start +\n\t\t       btrfs_file_extent_num_bytes(leaf, item);\n\t} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tsize_t size;\n\t\tsize = btrfs_file_extent_inline_len(leaf, item);\n\t\textent_end = (extent_start + size + root->sectorsize - 1) &\n\t\t\t~((u64)root->sectorsize - 1);\n\t}\n\n\tif (start >= extent_end) {\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\terr = ret;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (ret > 0)\n\t\t\t\tgoto not_found;\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tif (found_key.objectid != objectid ||\n\t\t    found_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto not_found;\n\t\tif (start + len <= found_key.offset)\n\t\t\tgoto not_found;\n\t\tem->start = start;\n\t\tem->orig_start = start;\n\t\tem->len = found_key.offset - start;\n\t\tgoto not_found_em;\n\t}\n\n\tif (found_type == BTRFS_FILE_EXTENT_REG ||\n\t    found_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\tem->start = extent_start;\n\t\tem->len = extent_end - extent_start;\n\t\tem->orig_start = extent_start -\n\t\t\t\t btrfs_file_extent_offset(leaf, item);\n\t\tem->orig_block_len = btrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t      item);\n\t\tbytenr = btrfs_file_extent_disk_bytenr(leaf, item);\n\t\tif (bytenr == 0) {\n\t\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\t\tgoto insert;\n\t\t}\n\t\tif (compress_type != BTRFS_COMPRESS_NONE) {\n\t\t\tset_bit(EXTENT_FLAG_COMPRESSED, &em->flags);\n\t\t\tem->compress_type = compress_type;\n\t\t\tem->block_start = bytenr;\n\t\t\tem->block_len = em->orig_block_len;\n\t\t} else {\n\t\t\tbytenr += btrfs_file_extent_offset(leaf, item);\n\t\t\tem->block_start = bytenr;\n\t\t\tem->block_len = em->len;\n\t\t\tif (found_type == BTRFS_FILE_EXTENT_PREALLOC)\n\t\t\t\tset_bit(EXTENT_FLAG_PREALLOC, &em->flags);\n\t\t}\n\t\tgoto insert;\n\t} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tunsigned long ptr;\n\t\tchar *map;\n\t\tsize_t size;\n\t\tsize_t extent_offset;\n\t\tsize_t copy_size;\n\n\t\tem->block_start = EXTENT_MAP_INLINE;\n\t\tif (!page || create) {\n\t\t\tem->start = extent_start;\n\t\t\tem->len = extent_end - extent_start;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsize = btrfs_file_extent_inline_len(leaf, item);\n\t\textent_offset = page_offset(page) + pg_offset - extent_start;\n\t\tcopy_size = min_t(u64, PAGE_CACHE_SIZE - pg_offset,\n\t\t\t\tsize - extent_offset);\n\t\tem->start = extent_start + extent_offset;\n\t\tem->len = (copy_size + root->sectorsize - 1) &\n\t\t\t~((u64)root->sectorsize - 1);\n\t\tem->orig_block_len = em->len;\n\t\tem->orig_start = em->start;\n\t\tif (compress_type) {\n\t\t\tset_bit(EXTENT_FLAG_COMPRESSED, &em->flags);\n\t\t\tem->compress_type = compress_type;\n\t\t}\n\t\tptr = btrfs_file_extent_inline_start(item) + extent_offset;\n\t\tif (create == 0 && !PageUptodate(page)) {\n\t\t\tif (btrfs_file_extent_compression(leaf, item) !=\n\t\t\t    BTRFS_COMPRESS_NONE) {\n\t\t\t\tret = uncompress_inline(path, inode, page,\n\t\t\t\t\t\t\tpg_offset,\n\t\t\t\t\t\t\textent_offset, item);\n\t\t\t\tBUG_ON(ret); /* -ENOMEM */\n\t\t\t} else {\n\t\t\t\tmap = kmap(page);\n\t\t\t\tread_extent_buffer(leaf, map + pg_offset, ptr,\n\t\t\t\t\t\t   copy_size);\n\t\t\t\tif (pg_offset + copy_size < PAGE_CACHE_SIZE) {\n\t\t\t\t\tmemset(map + pg_offset + copy_size, 0,\n\t\t\t\t\t       PAGE_CACHE_SIZE - pg_offset -\n\t\t\t\t\t       copy_size);\n\t\t\t\t}\n\t\t\t\tkunmap(page);\n\t\t\t}\n\t\t\tflush_dcache_page(page);\n\t\t} else if (create && PageUptodate(page)) {\n\t\t\tBUG();\n\t\t\tif (!trans) {\n\t\t\t\tkunmap(page);\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tem = NULL;\n\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\ttrans = btrfs_join_transaction(root);\n\n\t\t\t\tif (IS_ERR(trans))\n\t\t\t\t\treturn ERR_CAST(trans);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\tmap = kmap(page);\n\t\t\twrite_extent_buffer(leaf, map + pg_offset, ptr,\n\t\t\t\t\t    copy_size);\n\t\t\tkunmap(page);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\t\tset_extent_uptodate(io_tree, em->start,\n\t\t\t\t    extent_map_end(em) - 1, NULL, GFP_NOFS);\n\t\tgoto insert;\n\t} else {\n\t\tWARN(1, KERN_ERR \"btrfs unknown found_type %d\\n\", found_type);\n\t}\nnot_found:\n\tem->start = start;\n\tem->orig_start = start;\n\tem->len = len;\nnot_found_em:\n\tem->block_start = EXTENT_MAP_HOLE;\n\tset_bit(EXTENT_FLAG_VACANCY, &em->flags);\ninsert:\n\tbtrfs_release_path(path);\n\tif (em->start > start || extent_map_end(em) <= start) {\n\t\tprintk(KERN_ERR \"Btrfs: bad extent! em: [%llu %llu] passed \"\n\t\t       \"[%llu %llu]\\n\", (unsigned long long)em->start,\n\t\t       (unsigned long long)em->len,\n\t\t       (unsigned long long)start,\n\t\t       (unsigned long long)len);\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em);\n\t/* it is possible that someone inserted the extent into the tree\n\t * while we had the lock dropped.  It is also possible that\n\t * an overlapping map exists in the tree\n\t */\n\tif (ret == -EEXIST) {\n\t\tstruct extent_map *existing;\n\n\t\tret = 0;\n\n\t\texisting = lookup_extent_mapping(em_tree, start, len);\n\t\tif (existing && (existing->start > start ||\n\t\t    existing->start + existing->len <= start)) {\n\t\t\tfree_extent_map(existing);\n\t\t\texisting = NULL;\n\t\t}\n\t\tif (!existing) {\n\t\t\texisting = lookup_extent_mapping(em_tree, em->start,\n\t\t\t\t\t\t\t em->len);\n\t\t\tif (existing) {\n\t\t\t\terr = merge_extent_mapping(em_tree, existing,\n\t\t\t\t\t\t\t   em, start,\n\t\t\t\t\t\t\t   root->sectorsize);\n\t\t\t\tfree_extent_map(existing);\n\t\t\t\tif (err) {\n\t\t\t\t\tfree_extent_map(em);\n\t\t\t\t\tem = NULL;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = -EIO;\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tem = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tfree_extent_map(em);\n\t\t\tem = existing;\n\t\t\terr = 0;\n\t\t}\n\t}\n\twrite_unlock(&em_tree->lock);\nout:\n\n\tif (em)\n\t\ttrace_btrfs_get_extent(root, em);\n\n\tif (path)\n\t\tbtrfs_free_path(path);\n\tif (trans) {\n\t\tret = btrfs_end_transaction(trans, root);\n\t\tif (!err)\n\t\t\terr = ret;\n\t}\n\tif (err) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(err);\n\t}\n\tBUG_ON(!em); /* Error is always set */\n\treturn em;\n}\n\nstruct extent_map *btrfs_get_extent_fiemap(struct inode *inode, struct page *page,\n\t\t\t\t\t   size_t pg_offset, u64 start, u64 len,\n\t\t\t\t\t   int create)\n{\n\tstruct extent_map *em;\n\tstruct extent_map *hole_em = NULL;\n\tu64 range_start = start;\n\tu64 end;\n\tu64 found;\n\tu64 found_end;\n\tint err = 0;\n\n\tem = btrfs_get_extent(inode, page, pg_offset, start, len, create);\n\tif (IS_ERR(em))\n\t\treturn em;\n\tif (em) {\n\t\t/*\n\t\t * if our em maps to a hole, there might\n\t\t * actually be delalloc bytes behind it\n\t\t */\n\t\tif (em->block_start != EXTENT_MAP_HOLE)\n\t\t\treturn em;\n\t\telse\n\t\t\thole_em = em;\n\t}\n\n\t/* check to see if we've wrapped (len == -1 or similar) */\n\tend = start + len;\n\tif (end < start)\n\t\tend = (u64)-1;\n\telse\n\t\tend -= 1;\n\n\tem = NULL;\n\n\t/* ok, we didn't find anything, lets look for delalloc */\n\tfound = count_range_bits(&BTRFS_I(inode)->io_tree, &range_start,\n\t\t\t\t end, len, EXTENT_DELALLOC, 1);\n\tfound_end = range_start + found;\n\tif (found_end < range_start)\n\t\tfound_end = (u64)-1;\n\n\t/*\n\t * we didn't find anything useful, return\n\t * the original results from get_extent()\n\t */\n\tif (range_start > end || found_end <= start) {\n\t\tem = hole_em;\n\t\thole_em = NULL;\n\t\tgoto out;\n\t}\n\n\t/* adjust the range_start to make sure it doesn't\n\t * go backwards from the start they passed in\n\t */\n\trange_start = max(start,range_start);\n\tfound = found_end - range_start;\n\n\tif (found > 0) {\n\t\tu64 hole_start = start;\n\t\tu64 hole_len = len;\n\n\t\tem = alloc_extent_map();\n\t\tif (!em) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * when btrfs_get_extent can't find anything it\n\t\t * returns one huge hole\n\t\t *\n\t\t * make sure what it found really fits our range, and\n\t\t * adjust to make sure it is based on the start from\n\t\t * the caller\n\t\t */\n\t\tif (hole_em) {\n\t\t\tu64 calc_end = extent_map_end(hole_em);\n\n\t\t\tif (calc_end <= start || (hole_em->start > end)) {\n\t\t\t\tfree_extent_map(hole_em);\n\t\t\t\thole_em = NULL;\n\t\t\t} else {\n\t\t\t\thole_start = max(hole_em->start, start);\n\t\t\t\thole_len = calc_end - hole_start;\n\t\t\t}\n\t\t}\n\t\tem->bdev = NULL;\n\t\tif (hole_em && range_start > hole_start) {\n\t\t\t/* our hole starts before our delalloc, so we\n\t\t\t * have to return just the parts of the hole\n\t\t\t * that go until  the delalloc starts\n\t\t\t */\n\t\t\tem->len = min(hole_len,\n\t\t\t\t      range_start - hole_start);\n\t\t\tem->start = hole_start;\n\t\t\tem->orig_start = hole_start;\n\t\t\t/*\n\t\t\t * don't adjust block start at all,\n\t\t\t * it is fixed at EXTENT_MAP_HOLE\n\t\t\t */\n\t\t\tem->block_start = hole_em->block_start;\n\t\t\tem->block_len = hole_len;\n\t\t} else {\n\t\t\tem->start = range_start;\n\t\t\tem->len = found;\n\t\t\tem->orig_start = range_start;\n\t\t\tem->block_start = EXTENT_MAP_DELALLOC;\n\t\t\tem->block_len = found;\n\t\t}\n\t} else if (hole_em) {\n\t\treturn hole_em;\n\t}\nout:\n\n\tfree_extent_map(hole_em);\n\tif (err) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(err);\n\t}\n\treturn em;\n}\n\nstatic struct extent_map *btrfs_new_extent_direct(struct inode *inode,\n\t\t\t\t\t\t  u64 start, u64 len)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct extent_map *em;\n\tstruct btrfs_key ins;\n\tu64 alloc_hint;\n\tint ret;\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans))\n\t\treturn ERR_CAST(trans);\n\n\ttrans->block_rsv = &root->fs_info->delalloc_block_rsv;\n\n\talloc_hint = get_extent_allocation_hint(inode, start, len);\n\tret = btrfs_reserve_extent(trans, root, len, root->sectorsize, 0,\n\t\t\t\t   alloc_hint, &ins, 1);\n\tif (ret) {\n\t\tem = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tem = create_pinned_em(inode, start, ins.offset, start, ins.objectid,\n\t\t\t      ins.offset, ins.offset, 0);\n\tif (IS_ERR(em))\n\t\tgoto out;\n\n\tret = btrfs_add_ordered_extent_dio(inode, start, ins.objectid,\n\t\t\t\t\t   ins.offset, ins.offset, 0);\n\tif (ret) {\n\t\tbtrfs_free_reserved_extent(root, ins.objectid, ins.offset);\n\t\tem = ERR_PTR(ret);\n\t}\nout:\n\tbtrfs_end_transaction(trans, root);\n\treturn em;\n}\n\n/*\n * returns 1 when the nocow is safe, < 1 on error, 0 if the\n * block must be cow'd\n */\nstatic noinline int can_nocow_odirect(struct btrfs_trans_handle *trans,\n\t\t\t\t      struct inode *inode, u64 offset, u64 len)\n{\n\tstruct btrfs_path *path;\n\tint ret;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_key key;\n\tu64 disk_bytenr;\n\tu64 backref_offset;\n\tu64 extent_end;\n\tu64 num_bytes;\n\tint slot;\n\tint found_type;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tret = btrfs_lookup_file_extent(trans, root, path, btrfs_ino(inode),\n\t\t\t\t       offset, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tslot = path->slots[0];\n\tif (ret == 1) {\n\t\tif (slot == 0) {\n\t\t\t/* can't find the item, must cow */\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tslot--;\n\t}\n\tret = 0;\n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\tif (key.objectid != btrfs_ino(inode) ||\n\t    key.type != BTRFS_EXTENT_DATA_KEY) {\n\t\t/* not our file or wrong item type, must cow */\n\t\tgoto out;\n\t}\n\n\tif (key.offset > offset) {\n\t\t/* Wrong offset, must cow */\n\t\tgoto out;\n\t}\n\n\tfi = btrfs_item_ptr(leaf, slot, struct btrfs_file_extent_item);\n\tfound_type = btrfs_file_extent_type(leaf, fi);\n\tif (found_type != BTRFS_FILE_EXTENT_REG &&\n\t    found_type != BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t/* not a regular extent, must cow */\n\t\tgoto out;\n\t}\n\tdisk_bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);\n\tbackref_offset = btrfs_file_extent_offset(leaf, fi);\n\n\textent_end = key.offset + btrfs_file_extent_num_bytes(leaf, fi);\n\tif (extent_end < offset + len) {\n\t\t/* extent doesn't include our full range, must cow */\n\t\tgoto out;\n\t}\n\n\tif (btrfs_extent_readonly(root, disk_bytenr))\n\t\tgoto out;\n\n\t/*\n\t * look for other files referencing this extent, if we\n\t * find any we must cow\n\t */\n\tif (btrfs_cross_ref_exist(trans, root, btrfs_ino(inode),\n\t\t\t\t  key.offset - backref_offset, disk_bytenr))\n\t\tgoto out;\n\n\t/*\n\t * adjust disk_bytenr and num_bytes to cover just the bytes\n\t * in this extent we are about to write.  If there\n\t * are any csums in that range we have to cow in order\n\t * to keep the csums correct\n\t */\n\tdisk_bytenr += backref_offset;\n\tdisk_bytenr += offset - key.offset;\n\tnum_bytes = min(offset + len, extent_end) - offset;\n\tif (csum_exist_in_range(root, disk_bytenr, num_bytes))\n\t\t\t\tgoto out;\n\t/*\n\t * all of the above have passed, it is safe to overwrite this extent\n\t * without cow\n\t */\n\tret = 1;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int lock_extent_direct(struct inode *inode, u64 lockstart, u64 lockend,\n\t\t\t      struct extent_state **cached_state, int writing)\n{\n\tstruct btrfs_ordered_extent *ordered;\n\tint ret = 0;\n\n\twhile (1) {\n\t\tlock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend,\n\t\t\t\t 0, cached_state);\n\t\t/*\n\t\t * We're concerned with the entire range that we're going to be\n\t\t * doing DIO to, so we need to make sure theres no ordered\n\t\t * extents in this range.\n\t\t */\n\t\tordered = btrfs_lookup_ordered_range(inode, lockstart,\n\t\t\t\t\t\t     lockend - lockstart + 1);\n\n\t\t/*\n\t\t * We need to make sure there are no buffered pages in this\n\t\t * range either, we could have raced between the invalidate in\n\t\t * generic_file_direct_write and locking the extent.  The\n\t\t * invalidate needs to happen so that reads after a write do not\n\t\t * get stale data.\n\t\t */\n\t\tif (!ordered && (!writing ||\n\t\t    !test_range_bit(&BTRFS_I(inode)->io_tree,\n\t\t\t\t    lockstart, lockend, EXTENT_UPTODATE, 0,\n\t\t\t\t    *cached_state)))\n\t\t\tbreak;\n\n\t\tunlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,\n\t\t\t\t     cached_state, GFP_NOFS);\n\n\t\tif (ordered) {\n\t\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t} else {\n\t\t\t/* Screw you mmap */\n\t\t\tret = filemap_write_and_wait_range(inode->i_mapping,\n\t\t\t\t\t\t\t   lockstart,\n\t\t\t\t\t\t\t   lockend);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * If we found a page that couldn't be invalidated just\n\t\t\t * fall back to buffered.\n\t\t\t */\n\t\t\tret = invalidate_inode_pages2_range(inode->i_mapping,\n\t\t\t\t\tlockstart >> PAGE_CACHE_SHIFT,\n\t\t\t\t\tlockend >> PAGE_CACHE_SHIFT);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcond_resched();\n\t}\n\n\treturn ret;\n}\n\nstatic struct extent_map *create_pinned_em(struct inode *inode, u64 start,\n\t\t\t\t\t   u64 len, u64 orig_start,\n\t\t\t\t\t   u64 block_start, u64 block_len,\n\t\t\t\t\t   u64 orig_block_len, int type)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\tem_tree = &BTRFS_I(inode)->extent_tree;\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tem->start = start;\n\tem->orig_start = orig_start;\n\tem->len = len;\n\tem->block_len = block_len;\n\tem->block_start = block_start;\n\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\tem->orig_block_len = orig_block_len;\n\tem->generation = -1;\n\tset_bit(EXTENT_FLAG_PINNED, &em->flags);\n\tif (type == BTRFS_ORDERED_PREALLOC)\n\t\tset_bit(EXTENT_FLAG_FILLING, &em->flags);\n\n\tdo {\n\t\tbtrfs_drop_extent_cache(inode, em->start,\n\t\t\t\tem->start + em->len - 1, 0);\n\t\twrite_lock(&em_tree->lock);\n\t\tret = add_extent_mapping(em_tree, em);\n\t\tif (!ret)\n\t\t\tlist_move(&em->list,\n\t\t\t\t  &em_tree->modified_extents);\n\t\twrite_unlock(&em_tree->lock);\n\t} while (ret == -EEXIST);\n\n\tif (ret) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn em;\n}\n\n\nstatic int btrfs_get_blocks_direct(struct inode *inode, sector_t iblock,\n\t\t\t\t   struct buffer_head *bh_result, int create)\n{\n\tstruct extent_map *em;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_state *cached_state = NULL;\n\tu64 start = iblock << inode->i_blkbits;\n\tu64 lockstart, lockend;\n\tu64 len = bh_result->b_size;\n\tstruct btrfs_trans_handle *trans;\n\tint unlock_bits = EXTENT_LOCKED;\n\tint ret;\n\n\tif (create) {\n\t\tret = btrfs_delalloc_reserve_space(inode, len);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tunlock_bits |= EXTENT_DELALLOC | EXTENT_DIRTY;\n\t} else {\n\t\tlen = min_t(u64, len, root->sectorsize);\n\t}\n\n\tlockstart = start;\n\tlockend = start + len - 1;\n\n\t/*\n\t * If this errors out it's because we couldn't invalidate pagecache for\n\t * this range and we need to fallback to buffered.\n\t */\n\tif (lock_extent_direct(inode, lockstart, lockend, &cached_state, create))\n\t\treturn -ENOTBLK;\n\n\tif (create) {\n\t\tret = set_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,\n\t\t\t\t     lockend, EXTENT_DELALLOC, NULL,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tif (ret)\n\t\t\tgoto unlock_err;\n\t}\n\n\tem = btrfs_get_extent(inode, NULL, 0, start, len, 0);\n\tif (IS_ERR(em)) {\n\t\tret = PTR_ERR(em);\n\t\tgoto unlock_err;\n\t}\n\n\t/*\n\t * Ok for INLINE and COMPRESSED extents we need to fallback on buffered\n\t * io.  INLINE is special, and we could probably kludge it in here, but\n\t * it's still buffered so for safety lets just fall back to the generic\n\t * buffered path.\n\t *\n\t * For COMPRESSED we _have_ to read the entire extent in so we can\n\t * decompress it, so there will be buffering required no matter what we\n\t * do, so go ahead and fallback to buffered.\n\t *\n\t * We return -ENOTBLK because thats what makes DIO go ahead and go back\n\t * to buffered IO.  Don't blame me, this is the price we pay for using\n\t * the generic code.\n\t */\n\tif (test_bit(EXTENT_FLAG_COMPRESSED, &em->flags) ||\n\t    em->block_start == EXTENT_MAP_INLINE) {\n\t\tfree_extent_map(em);\n\t\tret = -ENOTBLK;\n\t\tgoto unlock_err;\n\t}\n\n\t/* Just a good old fashioned hole, return */\n\tif (!create && (em->block_start == EXTENT_MAP_HOLE ||\n\t\t\ttest_bit(EXTENT_FLAG_PREALLOC, &em->flags))) {\n\t\tfree_extent_map(em);\n\t\tret = 0;\n\t\tgoto unlock_err;\n\t}\n\n\t/*\n\t * We don't allocate a new extent in the following cases\n\t *\n\t * 1) The inode is marked as NODATACOW.  In this case we'll just use the\n\t * existing extent.\n\t * 2) The extent is marked as PREALLOC.  We're good to go here and can\n\t * just use the extent.\n\t *\n\t */\n\tif (!create) {\n\t\tlen = min(len, em->len - (start - em->start));\n\t\tlockstart = start + len;\n\t\tgoto unlock;\n\t}\n\n\tif (test_bit(EXTENT_FLAG_PREALLOC, &em->flags) ||\n\t    ((BTRFS_I(inode)->flags & BTRFS_INODE_NODATACOW) &&\n\t     em->block_start != EXTENT_MAP_HOLE)) {\n\t\tint type;\n\t\tint ret;\n\t\tu64 block_start;\n\n\t\tif (test_bit(EXTENT_FLAG_PREALLOC, &em->flags))\n\t\t\ttype = BTRFS_ORDERED_PREALLOC;\n\t\telse\n\t\t\ttype = BTRFS_ORDERED_NOCOW;\n\t\tlen = min(len, em->len - (start - em->start));\n\t\tblock_start = em->block_start + (start - em->start);\n\n\t\t/*\n\t\t * we're not going to log anything, but we do need\n\t\t * to make sure the current transaction stays open\n\t\t * while we look for nocow cross refs\n\t\t */\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans))\n\t\t\tgoto must_cow;\n\n\t\tif (can_nocow_odirect(trans, inode, start, len) == 1) {\n\t\t\tu64 orig_start = em->orig_start;\n\t\t\tu64 orig_block_len = em->orig_block_len;\n\n\t\t\tif (type == BTRFS_ORDERED_PREALLOC) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tem = create_pinned_em(inode, start, len,\n\t\t\t\t\t\t       orig_start,\n\t\t\t\t\t\t       block_start, len,\n\t\t\t\t\t\t       orig_block_len, type);\n\t\t\t\tif (IS_ERR(em)) {\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto unlock_err;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = btrfs_add_ordered_extent_dio(inode, start,\n\t\t\t\t\t   block_start, len, len, type);\n\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tif (ret) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tgoto unlock_err;\n\t\t\t}\n\t\t\tgoto unlock;\n\t\t}\n\t\tbtrfs_end_transaction(trans, root);\n\t}\nmust_cow:\n\t/*\n\t * this will cow the extent, reset the len in case we changed\n\t * it above\n\t */\n\tlen = bh_result->b_size;\n\tfree_extent_map(em);\n\tem = btrfs_new_extent_direct(inode, start, len);\n\tif (IS_ERR(em)) {\n\t\tret = PTR_ERR(em);\n\t\tgoto unlock_err;\n\t}\n\tlen = min(len, em->len - (start - em->start));\nunlock:\n\tbh_result->b_blocknr = (em->block_start + (start - em->start)) >>\n\t\tinode->i_blkbits;\n\tbh_result->b_size = len;\n\tbh_result->b_bdev = em->bdev;\n\tset_buffer_mapped(bh_result);\n\tif (create) {\n\t\tif (!test_bit(EXTENT_FLAG_PREALLOC, &em->flags))\n\t\t\tset_buffer_new(bh_result);\n\n\t\t/*\n\t\t * Need to update the i_size under the extent lock so buffered\n\t\t * readers will get the updated i_size when we unlock.\n\t\t */\n\t\tif (start + len > i_size_read(inode))\n\t\t\ti_size_write(inode, start + len);\n\t}\n\n\t/*\n\t * In the case of write we need to clear and unlock the entire range,\n\t * in the case of read we need to unlock only the end area that we\n\t * aren't using if there is any left over space.\n\t */\n\tif (lockstart < lockend) {\n\t\tif (create && len < lockend - lockstart) {\n\t\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,\n\t\t\t\t\t lockstart + len - 1,\n\t\t\t\t\t unlock_bits | EXTENT_DEFRAG, 1, 0,\n\t\t\t\t\t &cached_state, GFP_NOFS);\n\t\t\t/*\n\t\t\t * Beside unlock, we also need to cleanup reserved space\n\t\t\t * for the left range by attaching EXTENT_DO_ACCOUNTING.\n\t\t\t */\n\t\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree,\n\t\t\t\t\t lockstart + len, lockend,\n\t\t\t\t\t unlock_bits | EXTENT_DO_ACCOUNTING |\n\t\t\t\t\t EXTENT_DEFRAG, 1, 0, NULL, GFP_NOFS);\n\t\t} else {\n\t\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,\n\t\t\t\t\t lockend, unlock_bits, 1, 0,\n\t\t\t\t\t &cached_state, GFP_NOFS);\n\t\t}\n\t} else {\n\t\tfree_extent_state(cached_state);\n\t}\n\n\tfree_extent_map(em);\n\n\treturn 0;\n\nunlock_err:\n\tif (create)\n\t\tunlock_bits |= EXTENT_DO_ACCOUNTING;\n\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, lockstart, lockend,\n\t\t\t unlock_bits, 1, 0, &cached_state, GFP_NOFS);\n\treturn ret;\n}\n\nstruct btrfs_dio_private {\n\tstruct inode *inode;\n\tu64 logical_offset;\n\tu64 disk_bytenr;\n\tu64 bytes;\n\tvoid *private;\n\n\t/* number of bios pending for this dio */\n\tatomic_t pending_bios;\n\n\t/* IO errors */\n\tint errors;\n\n\tstruct bio *orig_bio;\n};\n\nstatic void btrfs_endio_direct_read(struct bio *bio, int err)\n{\n\tstruct btrfs_dio_private *dip = bio->bi_private;\n\tstruct bio_vec *bvec_end = bio->bi_io_vec + bio->bi_vcnt - 1;\n\tstruct bio_vec *bvec = bio->bi_io_vec;\n\tstruct inode *inode = dip->inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tu64 start;\n\n\tstart = dip->logical_offset;\n\tdo {\n\t\tif (!(BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM)) {\n\t\t\tstruct page *page = bvec->bv_page;\n\t\t\tchar *kaddr;\n\t\t\tu32 csum = ~(u32)0;\n\t\t\tu64 private = ~(u32)0;\n\t\t\tunsigned long flags;\n\n\t\t\tif (get_state_private(&BTRFS_I(inode)->io_tree,\n\t\t\t\t\t      start, &private))\n\t\t\t\tgoto failed;\n\t\t\tlocal_irq_save(flags);\n\t\t\tkaddr = kmap_atomic(page);\n\t\t\tcsum = btrfs_csum_data(root, kaddr + bvec->bv_offset,\n\t\t\t\t\t       csum, bvec->bv_len);\n\t\t\tbtrfs_csum_final(csum, (char *)&csum);\n\t\t\tkunmap_atomic(kaddr);\n\t\t\tlocal_irq_restore(flags);\n\n\t\t\tflush_dcache_page(bvec->bv_page);\n\t\t\tif (csum != private) {\nfailed:\n\t\t\t\tprintk(KERN_ERR \"btrfs csum failed ino %llu off\"\n\t\t\t\t      \" %llu csum %u private %u\\n\",\n\t\t\t\t      (unsigned long long)btrfs_ino(inode),\n\t\t\t\t      (unsigned long long)start,\n\t\t\t\t      csum, (unsigned)private);\n\t\t\t\terr = -EIO;\n\t\t\t}\n\t\t}\n\n\t\tstart += bvec->bv_len;\n\t\tbvec++;\n\t} while (bvec <= bvec_end);\n\n\tunlock_extent(&BTRFS_I(inode)->io_tree, dip->logical_offset,\n\t\t      dip->logical_offset + dip->bytes - 1);\n\tbio->bi_private = dip->private;\n\n\tkfree(dip);\n\n\t/* If we had a csum failure make sure to clear the uptodate flag */\n\tif (err)\n\t\tclear_bit(BIO_UPTODATE, &bio->bi_flags);\n\tdio_end_io(bio, err);\n}\n\nstatic void btrfs_endio_direct_write(struct bio *bio, int err)\n{\n\tstruct btrfs_dio_private *dip = bio->bi_private;\n\tstruct inode *inode = dip->inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ordered_extent *ordered = NULL;\n\tu64 ordered_offset = dip->logical_offset;\n\tu64 ordered_bytes = dip->bytes;\n\tint ret;\n\n\tif (err)\n\t\tgoto out_done;\nagain:\n\tret = btrfs_dec_test_first_ordered_pending(inode, &ordered,\n\t\t\t\t\t\t   &ordered_offset,\n\t\t\t\t\t\t   ordered_bytes, !err);\n\tif (!ret)\n\t\tgoto out_test;\n\n\tordered->work.func = finish_ordered_fn;\n\tordered->work.flags = 0;\n\tbtrfs_queue_worker(&root->fs_info->endio_write_workers,\n\t\t\t   &ordered->work);\nout_test:\n\t/*\n\t * our bio might span multiple ordered extents.  If we haven't\n\t * completed the accounting for the whole dio, go back and try again\n\t */\n\tif (ordered_offset < dip->logical_offset + dip->bytes) {\n\t\tordered_bytes = dip->logical_offset + dip->bytes -\n\t\t\tordered_offset;\n\t\tordered = NULL;\n\t\tgoto again;\n\t}\nout_done:\n\tbio->bi_private = dip->private;\n\n\tkfree(dip);\n\n\t/* If we had an error make sure to clear the uptodate flag */\n\tif (err)\n\t\tclear_bit(BIO_UPTODATE, &bio->bi_flags);\n\tdio_end_io(bio, err);\n}\n\nstatic int __btrfs_submit_bio_start_direct_io(struct inode *inode, int rw,\n\t\t\t\t    struct bio *bio, int mirror_num,\n\t\t\t\t    unsigned long bio_flags, u64 offset)\n{\n\tint ret;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tret = btrfs_csum_one_bio(root, inode, bio, offset, 1);\n\tBUG_ON(ret); /* -ENOMEM */\n\treturn 0;\n}\n\nstatic void btrfs_end_dio_bio(struct bio *bio, int err)\n{\n\tstruct btrfs_dio_private *dip = bio->bi_private;\n\n\tif (err) {\n\t\tprintk(KERN_ERR \"btrfs direct IO failed ino %llu rw %lu \"\n\t\t      \"sector %#Lx len %u err no %d\\n\",\n\t\t      (unsigned long long)btrfs_ino(dip->inode), bio->bi_rw,\n\t\t      (unsigned long long)bio->bi_sector, bio->bi_size, err);\n\t\tdip->errors = 1;\n\n\t\t/*\n\t\t * before atomic variable goto zero, we must make sure\n\t\t * dip->errors is perceived to be set.\n\t\t */\n\t\tsmp_mb__before_atomic_dec();\n\t}\n\n\t/* if there are more bios still pending for this dio, just exit */\n\tif (!atomic_dec_and_test(&dip->pending_bios))\n\t\tgoto out;\n\n\tif (dip->errors)\n\t\tbio_io_error(dip->orig_bio);\n\telse {\n\t\tset_bit(BIO_UPTODATE, &dip->orig_bio->bi_flags);\n\t\tbio_endio(dip->orig_bio, 0);\n\t}\nout:\n\tbio_put(bio);\n}\n\nstatic struct bio *btrfs_dio_bio_alloc(struct block_device *bdev,\n\t\t\t\t       u64 first_sector, gfp_t gfp_flags)\n{\n\tint nr_vecs = bio_get_nr_vecs(bdev);\n\treturn btrfs_bio_alloc(bdev, first_sector, nr_vecs, gfp_flags);\n}\n\nstatic inline int __btrfs_submit_dio_bio(struct bio *bio, struct inode *inode,\n\t\t\t\t\t int rw, u64 file_offset, int skip_sum,\n\t\t\t\t\t int async_submit)\n{\n\tint write = rw & REQ_WRITE;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret;\n\n\tif (async_submit)\n\t\tasync_submit = !atomic_read(&BTRFS_I(inode)->sync_writers);\n\n\tbio_get(bio);\n\n\tif (!write) {\n\t\tret = btrfs_bio_wq_end_io(root->fs_info, bio, 0);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tif (skip_sum)\n\t\tgoto map;\n\n\tif (write && async_submit) {\n\t\tret = btrfs_wq_submit_bio(root->fs_info,\n\t\t\t\t   inode, rw, bio, 0, 0,\n\t\t\t\t   file_offset,\n\t\t\t\t   __btrfs_submit_bio_start_direct_io,\n\t\t\t\t   __btrfs_submit_bio_done);\n\t\tgoto err;\n\t} else if (write) {\n\t\t/*\n\t\t * If we aren't doing async submit, calculate the csum of the\n\t\t * bio now.\n\t\t */\n\t\tret = btrfs_csum_one_bio(root, inode, bio, file_offset, 1);\n\t\tif (ret)\n\t\t\tgoto err;\n\t} else if (!skip_sum) {\n\t\tret = btrfs_lookup_bio_sums_dio(root, inode, bio, file_offset);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\nmap:\n\tret = btrfs_map_bio(root, rw, bio, 0, async_submit);\nerr:\n\tbio_put(bio);\n\treturn ret;\n}\n\nstatic int btrfs_submit_direct_hook(int rw, struct btrfs_dio_private *dip,\n\t\t\t\t    int skip_sum)\n{\n\tstruct inode *inode = dip->inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct bio *bio;\n\tstruct bio *orig_bio = dip->orig_bio;\n\tstruct bio_vec *bvec = orig_bio->bi_io_vec;\n\tu64 start_sector = orig_bio->bi_sector;\n\tu64 file_offset = dip->logical_offset;\n\tu64 submit_len = 0;\n\tu64 map_length;\n\tint nr_pages = 0;\n\tint ret = 0;\n\tint async_submit = 0;\n\n\tmap_length = orig_bio->bi_size;\n\tret = btrfs_map_block(root->fs_info, READ, start_sector << 9,\n\t\t\t      &map_length, NULL, 0);\n\tif (ret) {\n\t\tbio_put(orig_bio);\n\t\treturn -EIO;\n\t}\n\n\tif (map_length >= orig_bio->bi_size) {\n\t\tbio = orig_bio;\n\t\tgoto submit;\n\t}\n\n\tasync_submit = 1;\n\tbio = btrfs_dio_bio_alloc(orig_bio->bi_bdev, start_sector, GFP_NOFS);\n\tif (!bio)\n\t\treturn -ENOMEM;\n\tbio->bi_private = dip;\n\tbio->bi_end_io = btrfs_end_dio_bio;\n\tatomic_inc(&dip->pending_bios);\n\n\twhile (bvec <= (orig_bio->bi_io_vec + orig_bio->bi_vcnt - 1)) {\n\t\tif (unlikely(map_length < submit_len + bvec->bv_len ||\n\t\t    bio_add_page(bio, bvec->bv_page, bvec->bv_len,\n\t\t\t\t bvec->bv_offset) < bvec->bv_len)) {\n\t\t\t/*\n\t\t\t * inc the count before we submit the bio so\n\t\t\t * we know the end IO handler won't happen before\n\t\t\t * we inc the count. Otherwise, the dip might get freed\n\t\t\t * before we're done setting it up\n\t\t\t */\n\t\t\tatomic_inc(&dip->pending_bios);\n\t\t\tret = __btrfs_submit_dio_bio(bio, inode, rw,\n\t\t\t\t\t\t     file_offset, skip_sum,\n\t\t\t\t\t\t     async_submit);\n\t\t\tif (ret) {\n\t\t\t\tbio_put(bio);\n\t\t\t\tatomic_dec(&dip->pending_bios);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\n\t\t\tstart_sector += submit_len >> 9;\n\t\t\tfile_offset += submit_len;\n\n\t\t\tsubmit_len = 0;\n\t\t\tnr_pages = 0;\n\n\t\t\tbio = btrfs_dio_bio_alloc(orig_bio->bi_bdev,\n\t\t\t\t\t\t  start_sector, GFP_NOFS);\n\t\t\tif (!bio)\n\t\t\t\tgoto out_err;\n\t\t\tbio->bi_private = dip;\n\t\t\tbio->bi_end_io = btrfs_end_dio_bio;\n\n\t\t\tmap_length = orig_bio->bi_size;\n\t\t\tret = btrfs_map_block(root->fs_info, READ,\n\t\t\t\t\t      start_sector << 9,\n\t\t\t\t\t      &map_length, NULL, 0);\n\t\t\tif (ret) {\n\t\t\t\tbio_put(bio);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t} else {\n\t\t\tsubmit_len += bvec->bv_len;\n\t\t\tnr_pages ++;\n\t\t\tbvec++;\n\t\t}\n\t}\n\nsubmit:\n\tret = __btrfs_submit_dio_bio(bio, inode, rw, file_offset, skip_sum,\n\t\t\t\t     async_submit);\n\tif (!ret)\n\t\treturn 0;\n\n\tbio_put(bio);\nout_err:\n\tdip->errors = 1;\n\t/*\n\t * before atomic variable goto zero, we must\n\t * make sure dip->errors is perceived to be set.\n\t */\n\tsmp_mb__before_atomic_dec();\n\tif (atomic_dec_and_test(&dip->pending_bios))\n\t\tbio_io_error(dip->orig_bio);\n\n\t/* bio_end_io() will handle error, so we needn't return it */\n\treturn 0;\n}\n\nstatic void btrfs_submit_direct(int rw, struct bio *bio, struct inode *inode,\n\t\t\t\tloff_t file_offset)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_dio_private *dip;\n\tstruct bio_vec *bvec = bio->bi_io_vec;\n\tint skip_sum;\n\tint write = rw & REQ_WRITE;\n\tint ret = 0;\n\n\tskip_sum = BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM;\n\n\tdip = kmalloc(sizeof(*dip), GFP_NOFS);\n\tif (!dip) {\n\t\tret = -ENOMEM;\n\t\tgoto free_ordered;\n\t}\n\n\tdip->private = bio->bi_private;\n\tdip->inode = inode;\n\tdip->logical_offset = file_offset;\n\n\tdip->bytes = 0;\n\tdo {\n\t\tdip->bytes += bvec->bv_len;\n\t\tbvec++;\n\t} while (bvec <= (bio->bi_io_vec + bio->bi_vcnt - 1));\n\n\tdip->disk_bytenr = (u64)bio->bi_sector << 9;\n\tbio->bi_private = dip;\n\tdip->errors = 0;\n\tdip->orig_bio = bio;\n\tatomic_set(&dip->pending_bios, 0);\n\n\tif (write)\n\t\tbio->bi_end_io = btrfs_endio_direct_write;\n\telse\n\t\tbio->bi_end_io = btrfs_endio_direct_read;\n\n\tret = btrfs_submit_direct_hook(rw, dip, skip_sum);\n\tif (!ret)\n\t\treturn;\nfree_ordered:\n\t/*\n\t * If this is a write, we need to clean up the reserved space and kill\n\t * the ordered extent.\n\t */\n\tif (write) {\n\t\tstruct btrfs_ordered_extent *ordered;\n\t\tordered = btrfs_lookup_ordered_extent(inode, file_offset);\n\t\tif (!test_bit(BTRFS_ORDERED_PREALLOC, &ordered->flags) &&\n\t\t    !test_bit(BTRFS_ORDERED_NOCOW, &ordered->flags))\n\t\t\tbtrfs_free_reserved_extent(root, ordered->start,\n\t\t\t\t\t\t   ordered->disk_len);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t}\n\tbio_endio(bio, ret);\n}\n\nstatic ssize_t check_direct_IO(struct btrfs_root *root, int rw, struct kiocb *iocb,\n\t\t\tconst struct iovec *iov, loff_t offset,\n\t\t\tunsigned long nr_segs)\n{\n\tint seg;\n\tint i;\n\tsize_t size;\n\tunsigned long addr;\n\tunsigned blocksize_mask = root->sectorsize - 1;\n\tssize_t retval = -EINVAL;\n\tloff_t end = offset;\n\n\tif (offset & blocksize_mask)\n\t\tgoto out;\n\n\t/* Check the memory alignment.  Blocks cannot straddle pages */\n\tfor (seg = 0; seg < nr_segs; seg++) {\n\t\taddr = (unsigned long)iov[seg].iov_base;\n\t\tsize = iov[seg].iov_len;\n\t\tend += size;\n\t\tif ((addr & blocksize_mask) || (size & blocksize_mask))\n\t\t\tgoto out;\n\n\t\t/* If this is a write we don't need to check anymore */\n\t\tif (rw & WRITE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Check to make sure we don't have duplicate iov_base's in this\n\t\t * iovec, if so return EINVAL, otherwise we'll get csum errors\n\t\t * when reading back.\n\t\t */\n\t\tfor (i = seg + 1; i < nr_segs; i++) {\n\t\t\tif (iov[seg].iov_base == iov[i].iov_base)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\tretval = 0;\nout:\n\treturn retval;\n}\n\nstatic ssize_t btrfs_direct_IO(int rw, struct kiocb *iocb,\n\t\t\tconst struct iovec *iov, loff_t offset,\n\t\t\tunsigned long nr_segs)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\n\tif (check_direct_IO(BTRFS_I(inode)->root, rw, iocb, iov,\n\t\t\t    offset, nr_segs))\n\t\treturn 0;\n\n\treturn __blockdev_direct_IO(rw, iocb, inode,\n\t\t   BTRFS_I(inode)->root->fs_info->fs_devices->latest_bdev,\n\t\t   iov, offset, nr_segs, btrfs_get_blocks_direct, NULL,\n\t\t   btrfs_submit_direct, 0);\n}\n\n#define BTRFS_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC)\n\nstatic int btrfs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\tint\tret;\n\n\tret = fiemap_check_flags(fieinfo, BTRFS_FIEMAP_FLAGS);\n\tif (ret)\n\t\treturn ret;\n\n\treturn extent_fiemap(inode, fieinfo, start, len, btrfs_get_extent_fiemap);\n}\n\nint btrfs_readpage(struct file *file, struct page *page)\n{\n\tstruct extent_io_tree *tree;\n\ttree = &BTRFS_I(page->mapping->host)->io_tree;\n\treturn extent_read_full_page(tree, page, btrfs_get_extent, 0);\n}\n\nstatic int btrfs_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tstruct extent_io_tree *tree;\n\n\n\tif (current->flags & PF_MEMALLOC) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tunlock_page(page);\n\t\treturn 0;\n\t}\n\ttree = &BTRFS_I(page->mapping->host)->io_tree;\n\treturn extent_write_full_page(tree, page, btrfs_get_extent, wbc);\n}\n\nint btrfs_writepages(struct address_space *mapping,\n\t\t     struct writeback_control *wbc)\n{\n\tstruct extent_io_tree *tree;\n\n\ttree = &BTRFS_I(mapping->host)->io_tree;\n\treturn extent_writepages(tree, mapping, btrfs_get_extent, wbc);\n}\n\nstatic int\nbtrfs_readpages(struct file *file, struct address_space *mapping,\n\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\tstruct extent_io_tree *tree;\n\ttree = &BTRFS_I(mapping->host)->io_tree;\n\treturn extent_readpages(tree, mapping, pages, nr_pages,\n\t\t\t\tbtrfs_get_extent);\n}\nstatic int __btrfs_releasepage(struct page *page, gfp_t gfp_flags)\n{\n\tstruct extent_io_tree *tree;\n\tstruct extent_map_tree *map;\n\tint ret;\n\n\ttree = &BTRFS_I(page->mapping->host)->io_tree;\n\tmap = &BTRFS_I(page->mapping->host)->extent_tree;\n\tret = try_release_extent_mapping(map, tree, page, gfp_flags);\n\tif (ret == 1) {\n\t\tClearPagePrivate(page);\n\t\tset_page_private(page, 0);\n\t\tpage_cache_release(page);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_releasepage(struct page *page, gfp_t gfp_flags)\n{\n\tif (PageWriteback(page) || PageDirty(page))\n\t\treturn 0;\n\treturn __btrfs_releasepage(page, gfp_flags & GFP_NOFS);\n}\n\nstatic void btrfs_invalidatepage(struct page *page, unsigned long offset)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct extent_io_tree *tree;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tu64 page_start = page_offset(page);\n\tu64 page_end = page_start + PAGE_CACHE_SIZE - 1;\n\n\t/*\n\t * we have the page locked, so new writeback can't start,\n\t * and the dirty bit won't be cleared while we are here.\n\t *\n\t * Wait for IO on this page so that we can safely clear\n\t * the PagePrivate2 bit and do ordered accounting\n\t */\n\twait_on_page_writeback(page);\n\n\ttree = &BTRFS_I(inode)->io_tree;\n\tif (offset) {\n\t\tbtrfs_releasepage(page, GFP_NOFS);\n\t\treturn;\n\t}\n\tlock_extent_bits(tree, page_start, page_end, 0, &cached_state);\n\tordered = btrfs_lookup_ordered_extent(inode,\n\t\t\t\t\t   page_offset(page));\n\tif (ordered) {\n\t\t/*\n\t\t * IO on this page will never be started, so we need\n\t\t * to account for any ordered extents now\n\t\t */\n\t\tclear_extent_bit(tree, page_start, page_end,\n\t\t\t\t EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t\t EXTENT_LOCKED | EXTENT_DO_ACCOUNTING |\n\t\t\t\t EXTENT_DEFRAG, 1, 0, &cached_state, GFP_NOFS);\n\t\t/*\n\t\t * whoever cleared the private bit is responsible\n\t\t * for the finish_ordered_io\n\t\t */\n\t\tif (TestClearPagePrivate2(page) &&\n\t\t    btrfs_dec_test_ordered_pending(inode, &ordered, page_start,\n\t\t\t\t\t\t   PAGE_CACHE_SIZE, 1)) {\n\t\t\tbtrfs_finish_ordered_io(ordered);\n\t\t}\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tcached_state = NULL;\n\t\tlock_extent_bits(tree, page_start, page_end, 0, &cached_state);\n\t}\n\tclear_extent_bit(tree, page_start, page_end,\n\t\t EXTENT_LOCKED | EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 1, 1,\n\t\t &cached_state, GFP_NOFS);\n\t__btrfs_releasepage(page, GFP_NOFS);\n\n\tClearPageChecked(page);\n\tif (PagePrivate(page)) {\n\t\tClearPagePrivate(page);\n\t\tset_page_private(page, 0);\n\t\tpage_cache_release(page);\n\t}\n}\n\n/*\n * btrfs_page_mkwrite() is not allowed to change the file size as it gets\n * called from a page fault handler when a page is first dirtied. Hence we must\n * be careful to check for EOF conditions here. We set the page up correctly\n * for a written page which means we get ENOSPC checking when writing into\n * holes and correct delalloc and unwritten extent mapping on filesystems that\n * support these features.\n *\n * We are not allowed to take the i_mutex here so we have to play games to\n * protect against truncate races as the page could now be beyond EOF.  Because\n * vmtruncate() writes the inode size before removing pages, once we have the\n * page lock we can determine safely if the page is beyond EOF. If it is not\n * beyond EOF, then the page is guaranteed safe against truncation until we\n * unlock the page.\n */\nint btrfs_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page = vmf->page;\n\tstruct inode *inode = fdentry(vma->vm_file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tchar *kaddr;\n\tunsigned long zero_start;\n\tloff_t size;\n\tint ret;\n\tint reserved = 0;\n\tu64 page_start;\n\tu64 page_end;\n\n\tsb_start_pagefault(inode->i_sb);\n\tret  = btrfs_delalloc_reserve_space(inode, PAGE_CACHE_SIZE);\n\tif (!ret) {\n\t\tret = file_update_time(vma->vm_file);\n\t\treserved = 1;\n\t}\n\tif (ret) {\n\t\tif (ret == -ENOMEM)\n\t\t\tret = VM_FAULT_OOM;\n\t\telse /* -ENOSPC, -EIO, etc */\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\tif (reserved)\n\t\t\tgoto out;\n\t\tgoto out_noreserve;\n\t}\n\n\tret = VM_FAULT_NOPAGE; /* make the VM retry the fault */\nagain:\n\tlock_page(page);\n\tsize = i_size_read(inode);\n\tpage_start = page_offset(page);\n\tpage_end = page_start + PAGE_CACHE_SIZE - 1;\n\n\tif ((page->mapping != inode->i_mapping) ||\n\t    (page_start >= size)) {\n\t\t/* page got truncated out from underneath us */\n\t\tgoto out_unlock;\n\t}\n\twait_on_page_writeback(page);\n\n\tlock_extent_bits(io_tree, page_start, page_end, 0, &cached_state);\n\tset_page_extent_mapped(page);\n\n\t/*\n\t * we can't set the delalloc bits if there are pending ordered\n\t * extents.  Drop our locks and wait for them to finish\n\t */\n\tordered = btrfs_lookup_ordered_extent(inode, page_start);\n\tif (ordered) {\n\t\tunlock_extent_cached(io_tree, page_start, page_end,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tunlock_page(page);\n\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tgoto again;\n\t}\n\n\t/*\n\t * XXX - page_mkwrite gets called every time the page is dirtied, even\n\t * if it was already dirty, so for space accounting reasons we need to\n\t * clear any delalloc bits for the range we are fixing to save.  There\n\t * is probably a better way to do this, but for now keep consistent with\n\t * prepare_pages in the normal write path.\n\t */\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, page_start, page_end,\n\t\t\t  EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,\n\t\t\t  0, 0, &cached_state, GFP_NOFS);\n\n\tret = btrfs_set_extent_delalloc(inode, page_start, page_end,\n\t\t\t\t\t&cached_state);\n\tif (ret) {\n\t\tunlock_extent_cached(io_tree, page_start, page_end,\n\t\t\t\t     &cached_state, GFP_NOFS);\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_unlock;\n\t}\n\tret = 0;\n\n\t/* page is wholly or partially inside EOF */\n\tif (page_start + PAGE_CACHE_SIZE > size)\n\t\tzero_start = size & ~PAGE_CACHE_MASK;\n\telse\n\t\tzero_start = PAGE_CACHE_SIZE;\n\n\tif (zero_start != PAGE_CACHE_SIZE) {\n\t\tkaddr = kmap(page);\n\t\tmemset(kaddr + zero_start, 0, PAGE_CACHE_SIZE - zero_start);\n\t\tflush_dcache_page(page);\n\t\tkunmap(page);\n\t}\n\tClearPageChecked(page);\n\tset_page_dirty(page);\n\tSetPageUptodate(page);\n\n\tBTRFS_I(inode)->last_trans = root->fs_info->generation;\n\tBTRFS_I(inode)->last_sub_trans = BTRFS_I(inode)->root->log_transid;\n\tBTRFS_I(inode)->last_log_commit = BTRFS_I(inode)->root->last_log_commit;\n\n\tunlock_extent_cached(io_tree, page_start, page_end, &cached_state, GFP_NOFS);\n\nout_unlock:\n\tif (!ret) {\n\t\tsb_end_pagefault(inode->i_sb);\n\t\treturn VM_FAULT_LOCKED;\n\t}\n\tunlock_page(page);\nout:\n\tbtrfs_delalloc_release_space(inode, PAGE_CACHE_SIZE);\nout_noreserve:\n\tsb_end_pagefault(inode->i_sb);\n\treturn ret;\n}\n\nstatic int btrfs_truncate(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_block_rsv *rsv;\n\tint ret;\n\tint err = 0;\n\tstruct btrfs_trans_handle *trans;\n\tu64 mask = root->sectorsize - 1;\n\tu64 min_size = btrfs_calc_trunc_metadata_size(root, 1);\n\n\tret = btrfs_truncate_page(inode, inode->i_size, 0, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tbtrfs_wait_ordered_range(inode, inode->i_size & (~mask), (u64)-1);\n\tbtrfs_ordered_update_i_size(inode, inode->i_size, NULL);\n\n\t/*\n\t * Yes ladies and gentelment, this is indeed ugly.  The fact is we have\n\t * 3 things going on here\n\t *\n\t * 1) We need to reserve space for our orphan item and the space to\n\t * delete our orphan item.  Lord knows we don't want to have a dangling\n\t * orphan item because we didn't reserve space to remove it.\n\t *\n\t * 2) We need to reserve space to update our inode.\n\t *\n\t * 3) We need to have something to cache all the space that is going to\n\t * be free'd up by the truncate operation, but also have some slack\n\t * space reserved in case it uses space during the truncate (thank you\n\t * very much snapshotting).\n\t *\n\t * And we need these to all be seperate.  The fact is we can use alot of\n\t * space doing the truncate, and we have no earthly idea how much space\n\t * we will use, so we need the truncate reservation to be seperate so it\n\t * doesn't end up using space reserved for updating the inode or\n\t * removing the orphan item.  We also need to be able to stop the\n\t * transaction and start a new one, which means we need to be able to\n\t * update the inode several times, and we have no idea of knowing how\n\t * many times that will be, so we can't just reserve 1 item for the\n\t * entirety of the opration, so that has to be done seperately as well.\n\t * Then there is the orphan item, which does indeed need to be held on\n\t * to for the whole operation, and we need nobody to touch this reserved\n\t * space except the orphan code.\n\t *\n\t * So that leaves us with\n\t *\n\t * 1) root->orphan_block_rsv - for the orphan deletion.\n\t * 2) rsv - for the truncate reservation, which we will steal from the\n\t * transaction reservation.\n\t * 3) fs_info->trans_block_rsv - this will have 1 items worth left for\n\t * updating the inode.\n\t */\n\trsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);\n\tif (!rsv)\n\t\treturn -ENOMEM;\n\trsv->size = min_size;\n\trsv->failfast = 1;\n\n\t/*\n\t * 1 for the truncate slack space\n\t * 1 for the orphan item we're going to add\n\t * 1 for the orphan item deletion\n\t * 1 for updating the inode.\n\t */\n\ttrans = btrfs_start_transaction(root, 4);\n\tif (IS_ERR(trans)) {\n\t\terr = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\t/* Migrate the slack space for the truncate to our reserve */\n\tret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv, rsv,\n\t\t\t\t      min_size);\n\tBUG_ON(ret);\n\n\tret = btrfs_orphan_add(trans, inode);\n\tif (ret) {\n\t\tbtrfs_end_transaction(trans, root);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * setattr is responsible for setting the ordered_data_close flag,\n\t * but that is only tested during the last file release.  That\n\t * could happen well after the next commit, leaving a great big\n\t * window where new writes may get lost if someone chooses to write\n\t * to this file after truncating to zero\n\t *\n\t * The inode doesn't have any dirty data here, and so if we commit\n\t * this is a noop.  If someone immediately starts writing to the inode\n\t * it is very likely we'll catch some of their writes in this\n\t * transaction, and the commit will find this file on the ordered\n\t * data list with good things to send down.\n\t *\n\t * This is a best effort solution, there is still a window where\n\t * using truncate to replace the contents of the file will\n\t * end up with a zero length file after a crash.\n\t */\n\tif (inode->i_size == 0 && test_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,\n\t\t\t\t\t   &BTRFS_I(inode)->runtime_flags))\n\t\tbtrfs_add_ordered_operation(trans, root, inode);\n\n\t/*\n\t * So if we truncate and then write and fsync we normally would just\n\t * write the extents that changed, which is a problem if we need to\n\t * first truncate that entire inode.  So set this flag so we write out\n\t * all of the extents in the inode to the sync log so we're completely\n\t * safe.\n\t */\n\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &BTRFS_I(inode)->runtime_flags);\n\ttrans->block_rsv = rsv;\n\n\twhile (1) {\n\t\tret = btrfs_truncate_inode_items(trans, root, inode,\n\t\t\t\t\t\t inode->i_size,\n\t\t\t\t\t\t BTRFS_EXTENT_DATA_KEY);\n\t\tif (ret != -ENOSPC) {\n\t\t\terr = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t\tif (ret) {\n\t\t\terr = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\tbtrfs_end_transaction(trans, root);\n\t\tbtrfs_btree_balance_dirty(root);\n\n\t\ttrans = btrfs_start_transaction(root, 2);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = err = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv,\n\t\t\t\t\t      rsv, min_size);\n\t\tBUG_ON(ret);\t/* shouldn't happen */\n\t\ttrans->block_rsv = rsv;\n\t}\n\n\tif (ret == 0 && inode->i_nlink > 0) {\n\t\ttrans->block_rsv = root->orphan_block_rsv;\n\t\tret = btrfs_orphan_del(trans, inode);\n\t\tif (ret)\n\t\t\terr = ret;\n\t} else if (ret && inode->i_nlink > 0) {\n\t\t/*\n\t\t * Failed to do the truncate, remove us from the in memory\n\t\t * orphan list.\n\t\t */\n\t\tret = btrfs_orphan_del(NULL, inode);\n\t}\n\n\tif (trans) {\n\t\ttrans->block_rsv = &root->fs_info->trans_block_rsv;\n\t\tret = btrfs_update_inode(trans, root, inode);\n\t\tif (ret && !err)\n\t\t\terr = ret;\n\n\t\tret = btrfs_end_transaction(trans, root);\n\t\tbtrfs_btree_balance_dirty(root);\n\t}\n\nout:\n\tbtrfs_free_block_rsv(root, rsv);\n\n\tif (ret && !err)\n\t\terr = ret;\n\n\treturn err;\n}\n\n/*\n * create a new subvolume directory/inode (helper for the ioctl).\n */\nint btrfs_create_subvol_root(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *new_root, u64 new_dirid)\n{\n\tstruct inode *inode;\n\tint err;\n\tu64 index = 0;\n\n\tinode = btrfs_new_inode(trans, new_root, NULL, \"..\", 2,\n\t\t\t\tnew_dirid, new_dirid,\n\t\t\t\tS_IFDIR | (~current_umask() & S_IRWXUGO),\n\t\t\t\t&index);\n\tif (IS_ERR(inode))\n\t\treturn PTR_ERR(inode);\n\tinode->i_op = &btrfs_dir_inode_operations;\n\tinode->i_fop = &btrfs_dir_file_operations;\n\n\tset_nlink(inode, 1);\n\tbtrfs_i_size_write(inode, 0);\n\n\terr = btrfs_update_inode(trans, new_root, inode);\n\n\tiput(inode);\n\treturn err;\n}\n\nstruct inode *btrfs_alloc_inode(struct super_block *sb)\n{\n\tstruct btrfs_inode *ei;\n\tstruct inode *inode;\n\n\tei = kmem_cache_alloc(btrfs_inode_cachep, GFP_NOFS);\n\tif (!ei)\n\t\treturn NULL;\n\n\tei->root = NULL;\n\tei->generation = 0;\n\tei->last_trans = 0;\n\tei->last_sub_trans = 0;\n\tei->logged_trans = 0;\n\tei->delalloc_bytes = 0;\n\tei->disk_i_size = 0;\n\tei->flags = 0;\n\tei->csum_bytes = 0;\n\tei->index_cnt = (u64)-1;\n\tei->last_unlink_trans = 0;\n\tei->last_log_commit = 0;\n\n\tspin_lock_init(&ei->lock);\n\tei->outstanding_extents = 0;\n\tei->reserved_extents = 0;\n\n\tei->runtime_flags = 0;\n\tei->force_compress = BTRFS_COMPRESS_NONE;\n\n\tei->delayed_node = NULL;\n\n\tinode = &ei->vfs_inode;\n\textent_map_tree_init(&ei->extent_tree);\n\textent_io_tree_init(&ei->io_tree, &inode->i_data);\n\textent_io_tree_init(&ei->io_failure_tree, &inode->i_data);\n\tei->io_tree.track_uptodate = 1;\n\tei->io_failure_tree.track_uptodate = 1;\n\tatomic_set(&ei->sync_writers, 0);\n\tmutex_init(&ei->log_mutex);\n\tmutex_init(&ei->delalloc_mutex);\n\tbtrfs_ordered_inode_tree_init(&ei->ordered_tree);\n\tINIT_LIST_HEAD(&ei->delalloc_inodes);\n\tINIT_LIST_HEAD(&ei->ordered_operations);\n\tRB_CLEAR_NODE(&ei->rb_node);\n\n\treturn inode;\n}\n\nstatic void btrfs_i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(btrfs_inode_cachep, BTRFS_I(inode));\n}\n\nvoid btrfs_destroy_inode(struct inode *inode)\n{\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\tWARN_ON(!hlist_empty(&inode->i_dentry));\n\tWARN_ON(inode->i_data.nrpages);\n\tWARN_ON(BTRFS_I(inode)->outstanding_extents);\n\tWARN_ON(BTRFS_I(inode)->reserved_extents);\n\tWARN_ON(BTRFS_I(inode)->delalloc_bytes);\n\tWARN_ON(BTRFS_I(inode)->csum_bytes);\n\n\t/*\n\t * This can happen where we create an inode, but somebody else also\n\t * created the same inode and we need to destroy the one we already\n\t * created.\n\t */\n\tif (!root)\n\t\tgoto free;\n\n\t/*\n\t * Make sure we're properly removed from the ordered operation\n\t * lists.\n\t */\n\tsmp_mb();\n\tif (!list_empty(&BTRFS_I(inode)->ordered_operations)) {\n\t\tspin_lock(&root->fs_info->ordered_extent_lock);\n\t\tlist_del_init(&BTRFS_I(inode)->ordered_operations);\n\t\tspin_unlock(&root->fs_info->ordered_extent_lock);\n\t}\n\n\tif (test_bit(BTRFS_INODE_HAS_ORPHAN_ITEM,\n\t\t     &BTRFS_I(inode)->runtime_flags)) {\n\t\tprintk(KERN_INFO \"BTRFS: inode %llu still on the orphan list\\n\",\n\t\t       (unsigned long long)btrfs_ino(inode));\n\t\tatomic_dec(&root->orphan_inodes);\n\t}\n\n\twhile (1) {\n\t\tordered = btrfs_lookup_first_ordered_extent(inode, (u64)-1);\n\t\tif (!ordered)\n\t\t\tbreak;\n\t\telse {\n\t\t\tprintk(KERN_ERR \"btrfs found ordered \"\n\t\t\t       \"extent %llu %llu on inode cleanup\\n\",\n\t\t\t       (unsigned long long)ordered->file_offset,\n\t\t\t       (unsigned long long)ordered->len);\n\t\t\tbtrfs_remove_ordered_extent(inode, ordered);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t}\n\t}\n\tinode_tree_del(inode);\n\tbtrfs_drop_extent_cache(inode, 0, (u64)-1, 0);\nfree:\n\tbtrfs_remove_delayed_node(inode);\n\tcall_rcu(&inode->i_rcu, btrfs_i_callback);\n}\n\nint btrfs_drop_inode(struct inode *inode)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\n\tif (btrfs_root_refs(&root->root_item) == 0 &&\n\t    !btrfs_is_free_space_inode(inode))\n\t\treturn 1;\n\telse\n\t\treturn generic_drop_inode(inode);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct btrfs_inode *ei = (struct btrfs_inode *) foo;\n\n\tinode_init_once(&ei->vfs_inode);\n}\n\nvoid btrfs_destroy_cachep(void)\n{\n\t/*\n\t * Make sure all delayed rcu free inodes are flushed before we\n\t * destroy cache.\n\t */\n\trcu_barrier();\n\tif (btrfs_inode_cachep)\n\t\tkmem_cache_destroy(btrfs_inode_cachep);\n\tif (btrfs_trans_handle_cachep)\n\t\tkmem_cache_destroy(btrfs_trans_handle_cachep);\n\tif (btrfs_transaction_cachep)\n\t\tkmem_cache_destroy(btrfs_transaction_cachep);\n\tif (btrfs_path_cachep)\n\t\tkmem_cache_destroy(btrfs_path_cachep);\n\tif (btrfs_free_space_cachep)\n\t\tkmem_cache_destroy(btrfs_free_space_cachep);\n\tif (btrfs_delalloc_work_cachep)\n\t\tkmem_cache_destroy(btrfs_delalloc_work_cachep);\n}\n\nint btrfs_init_cachep(void)\n{\n\tbtrfs_inode_cachep = kmem_cache_create(\"btrfs_inode\",\n\t\t\tsizeof(struct btrfs_inode), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, init_once);\n\tif (!btrfs_inode_cachep)\n\t\tgoto fail;\n\n\tbtrfs_trans_handle_cachep = kmem_cache_create(\"btrfs_trans_handle\",\n\t\t\tsizeof(struct btrfs_trans_handle), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_trans_handle_cachep)\n\t\tgoto fail;\n\n\tbtrfs_transaction_cachep = kmem_cache_create(\"btrfs_transaction\",\n\t\t\tsizeof(struct btrfs_transaction), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_transaction_cachep)\n\t\tgoto fail;\n\n\tbtrfs_path_cachep = kmem_cache_create(\"btrfs_path\",\n\t\t\tsizeof(struct btrfs_path), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_path_cachep)\n\t\tgoto fail;\n\n\tbtrfs_free_space_cachep = kmem_cache_create(\"btrfs_free_space\",\n\t\t\tsizeof(struct btrfs_free_space), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_free_space_cachep)\n\t\tgoto fail;\n\n\tbtrfs_delalloc_work_cachep = kmem_cache_create(\"btrfs_delalloc_work\",\n\t\t\tsizeof(struct btrfs_delalloc_work), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD,\n\t\t\tNULL);\n\tif (!btrfs_delalloc_work_cachep)\n\t\tgoto fail;\n\n\treturn 0;\nfail:\n\tbtrfs_destroy_cachep();\n\treturn -ENOMEM;\n}\n\nstatic int btrfs_getattr(struct vfsmount *mnt,\n\t\t\t struct dentry *dentry, struct kstat *stat)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tu32 blocksize = inode->i_sb->s_blocksize;\n\n\tgeneric_fillattr(inode, stat);\n\tstat->dev = BTRFS_I(inode)->root->anon_dev;\n\tstat->blksize = PAGE_CACHE_SIZE;\n\tstat->blocks = (ALIGN(inode_get_bytes(inode), blocksize) +\n\t\tALIGN(BTRFS_I(inode)->delalloc_bytes, blocksize)) >> 9;\n\treturn 0;\n}\n\n/*\n * If a file is moved, it will inherit the cow and compression flags of the new\n * directory.\n */\nstatic void fixup_inode_flags(struct inode *dir, struct inode *inode)\n{\n\tstruct btrfs_inode *b_dir = BTRFS_I(dir);\n\tstruct btrfs_inode *b_inode = BTRFS_I(inode);\n\n\tif (b_dir->flags & BTRFS_INODE_NODATACOW)\n\t\tb_inode->flags |= BTRFS_INODE_NODATACOW;\n\telse\n\t\tb_inode->flags &= ~BTRFS_INODE_NODATACOW;\n\n\tif (b_dir->flags & BTRFS_INODE_COMPRESS) {\n\t\tb_inode->flags |= BTRFS_INODE_COMPRESS;\n\t\tb_inode->flags &= ~BTRFS_INODE_NOCOMPRESS;\n\t} else {\n\t\tb_inode->flags &= ~(BTRFS_INODE_COMPRESS |\n\t\t\t\t    BTRFS_INODE_NOCOMPRESS);\n\t}\n}\n\nstatic int btrfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t\t\t   struct inode *new_dir, struct dentry *new_dentry)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(old_dir)->root;\n\tstruct btrfs_root *dest = BTRFS_I(new_dir)->root;\n\tstruct inode *new_inode = new_dentry->d_inode;\n\tstruct inode *old_inode = old_dentry->d_inode;\n\tstruct timespec ctime = CURRENT_TIME;\n\tu64 index = 0;\n\tu64 root_objectid;\n\tint ret;\n\tu64 old_ino = btrfs_ino(old_inode);\n\n\tif (btrfs_ino(new_dir) == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)\n\t\treturn -EPERM;\n\n\t/* we only allow rename subvolume link between subvolumes */\n\tif (old_ino != BTRFS_FIRST_FREE_OBJECTID && root != dest)\n\t\treturn -EXDEV;\n\n\tif (old_ino == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID ||\n\t    (new_inode && btrfs_ino(new_inode) == BTRFS_FIRST_FREE_OBJECTID))\n\t\treturn -ENOTEMPTY;\n\n\tif (S_ISDIR(old_inode->i_mode) && new_inode &&\n\t    new_inode->i_size > BTRFS_EMPTY_DIR_SIZE)\n\t\treturn -ENOTEMPTY;\n\n\n\t/* check for collisions, even if the  name isn't there */\n\tret = btrfs_check_dir_item_collision(root, new_dir->i_ino,\n\t\t\t     new_dentry->d_name.name,\n\t\t\t     new_dentry->d_name.len);\n\n\tif (ret) {\n\t\tif (ret == -EEXIST) {\n\t\t\t/* we shouldn't get\n\t\t\t * eexist without a new_inode */\n\t\t\tif (!new_inode) {\n\t\t\t\tWARN_ON(1);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t} else {\n\t\t\t/* maybe -EOVERFLOW */\n\t\t\treturn ret;\n\t\t}\n\t}\n\tret = 0;\n\n\t/*\n\t * we're using rename to replace one file with another.\n\t * and the replacement file is large.  Start IO on it now so\n\t * we don't add too much work to the end of the transaction\n\t */\n\tif (new_inode && S_ISREG(old_inode->i_mode) && new_inode->i_size &&\n\t    old_inode->i_size > BTRFS_ORDERED_OPERATIONS_FLUSH_LIMIT)\n\t\tfilemap_flush(old_inode->i_mapping);\n\n\t/* close the racy window with snapshot create/destroy ioctl */\n\tif (old_ino == BTRFS_FIRST_FREE_OBJECTID)\n\t\tdown_read(&root->fs_info->subvol_sem);\n\t/*\n\t * We want to reserve the absolute worst case amount of items.  So if\n\t * both inodes are subvols and we need to unlink them then that would\n\t * require 4 item modifications, but if they are both normal inodes it\n\t * would require 5 item modifications, so we'll assume their normal\n\t * inodes.  So 5 * 2 is 10, plus 1 for the new link, so 11 total items\n\t * should cover the worst case number of items we'll modify.\n\t */\n\ttrans = btrfs_start_transaction(root, 20);\n\tif (IS_ERR(trans)) {\n                ret = PTR_ERR(trans);\n                goto out_notrans;\n        }\n\n\tif (dest != root)\n\t\tbtrfs_record_root_in_trans(trans, dest);\n\n\tret = btrfs_set_inode_index(new_dir, &index);\n\tif (ret)\n\t\tgoto out_fail;\n\n\tif (unlikely(old_ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\t/* force full log commit if subvolume involved. */\n\t\troot->fs_info->last_trans_log_full_commit = trans->transid;\n\t} else {\n\t\tret = btrfs_insert_inode_ref(trans, dest,\n\t\t\t\t\t     new_dentry->d_name.name,\n\t\t\t\t\t     new_dentry->d_name.len,\n\t\t\t\t\t     old_ino,\n\t\t\t\t\t     btrfs_ino(new_dir), index);\n\t\tif (ret)\n\t\t\tgoto out_fail;\n\t\t/*\n\t\t * this is an ugly little race, but the rename is required\n\t\t * to make sure that if we crash, the inode is either at the\n\t\t * old name or the new one.  pinning the log transaction lets\n\t\t * us make sure we don't allow a log commit to come in after\n\t\t * we unlink the name but before we add the new name back in.\n\t\t */\n\t\tbtrfs_pin_log_trans(root);\n\t}\n\t/*\n\t * make sure the inode gets flushed if it is replacing\n\t * something.\n\t */\n\tif (new_inode && new_inode->i_size && S_ISREG(old_inode->i_mode))\n\t\tbtrfs_add_ordered_operation(trans, root, old_inode);\n\n\tinode_inc_iversion(old_dir);\n\tinode_inc_iversion(new_dir);\n\tinode_inc_iversion(old_inode);\n\told_dir->i_ctime = old_dir->i_mtime = ctime;\n\tnew_dir->i_ctime = new_dir->i_mtime = ctime;\n\told_inode->i_ctime = ctime;\n\n\tif (old_dentry->d_parent != new_dentry->d_parent)\n\t\tbtrfs_record_unlink_dir(trans, old_dir, old_inode, 1);\n\n\tif (unlikely(old_ino == BTRFS_FIRST_FREE_OBJECTID)) {\n\t\troot_objectid = BTRFS_I(old_inode)->root->root_key.objectid;\n\t\tret = btrfs_unlink_subvol(trans, root, old_dir, root_objectid,\n\t\t\t\t\told_dentry->d_name.name,\n\t\t\t\t\told_dentry->d_name.len);\n\t} else {\n\t\tret = __btrfs_unlink_inode(trans, root, old_dir,\n\t\t\t\t\told_dentry->d_inode,\n\t\t\t\t\told_dentry->d_name.name,\n\t\t\t\t\told_dentry->d_name.len);\n\t\tif (!ret)\n\t\t\tret = btrfs_update_inode(trans, root, old_inode);\n\t}\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_fail;\n\t}\n\n\tif (new_inode) {\n\t\tinode_inc_iversion(new_inode);\n\t\tnew_inode->i_ctime = CURRENT_TIME;\n\t\tif (unlikely(btrfs_ino(new_inode) ==\n\t\t\t     BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)) {\n\t\t\troot_objectid = BTRFS_I(new_inode)->location.objectid;\n\t\t\tret = btrfs_unlink_subvol(trans, dest, new_dir,\n\t\t\t\t\t\troot_objectid,\n\t\t\t\t\t\tnew_dentry->d_name.name,\n\t\t\t\t\t\tnew_dentry->d_name.len);\n\t\t\tBUG_ON(new_inode->i_nlink == 0);\n\t\t} else {\n\t\t\tret = btrfs_unlink_inode(trans, dest, new_dir,\n\t\t\t\t\t\t new_dentry->d_inode,\n\t\t\t\t\t\t new_dentry->d_name.name,\n\t\t\t\t\t\t new_dentry->d_name.len);\n\t\t}\n\t\tif (!ret && new_inode->i_nlink == 0) {\n\t\t\tret = btrfs_orphan_add(trans, new_dentry->d_inode);\n\t\t\tBUG_ON(ret);\n\t\t}\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tgoto out_fail;\n\t\t}\n\t}\n\n\tfixup_inode_flags(new_dir, old_inode);\n\n\tret = btrfs_add_link(trans, new_dir, old_inode,\n\t\t\t     new_dentry->d_name.name,\n\t\t\t     new_dentry->d_name.len, 0, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_fail;\n\t}\n\n\tif (old_ino != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tstruct dentry *parent = new_dentry->d_parent;\n\t\tbtrfs_log_new_name(trans, old_inode, old_dir, parent);\n\t\tbtrfs_end_log_trans(root);\n\t}\nout_fail:\n\tbtrfs_end_transaction(trans, root);\nout_notrans:\n\tif (old_ino == BTRFS_FIRST_FREE_OBJECTID)\n\t\tup_read(&root->fs_info->subvol_sem);\n\n\treturn ret;\n}\n\nstatic void btrfs_run_delalloc_work(struct btrfs_work *work)\n{\n\tstruct btrfs_delalloc_work *delalloc_work;\n\n\tdelalloc_work = container_of(work, struct btrfs_delalloc_work,\n\t\t\t\t     work);\n\tif (delalloc_work->wait)\n\t\tbtrfs_wait_ordered_range(delalloc_work->inode, 0, (u64)-1);\n\telse\n\t\tfilemap_flush(delalloc_work->inode->i_mapping);\n\n\tif (delalloc_work->delay_iput)\n\t\tbtrfs_add_delayed_iput(delalloc_work->inode);\n\telse\n\t\tiput(delalloc_work->inode);\n\tcomplete(&delalloc_work->completion);\n}\n\nstruct btrfs_delalloc_work *btrfs_alloc_delalloc_work(struct inode *inode,\n\t\t\t\t\t\t    int wait, int delay_iput)\n{\n\tstruct btrfs_delalloc_work *work;\n\n\twork = kmem_cache_zalloc(btrfs_delalloc_work_cachep, GFP_NOFS);\n\tif (!work)\n\t\treturn NULL;\n\n\tinit_completion(&work->completion);\n\tINIT_LIST_HEAD(&work->list);\n\twork->inode = inode;\n\twork->wait = wait;\n\twork->delay_iput = delay_iput;\n\twork->work.func = btrfs_run_delalloc_work;\n\n\treturn work;\n}\n\nvoid btrfs_wait_and_free_delalloc_work(struct btrfs_delalloc_work *work)\n{\n\twait_for_completion(&work->completion);\n\tkmem_cache_free(btrfs_delalloc_work_cachep, work);\n}\n\n/*\n * some fairly slow code that needs optimization. This walks the list\n * of all the inodes with pending delalloc and forces them to disk.\n */\nint btrfs_start_delalloc_inodes(struct btrfs_root *root, int delay_iput)\n{\n\tstruct list_head *head = &root->fs_info->delalloc_inodes;\n\tstruct btrfs_inode *binode;\n\tstruct inode *inode;\n\tstruct btrfs_delalloc_work *work, *next;\n\tstruct list_head works;\n\tint ret = 0;\n\n\tif (root->fs_info->sb->s_flags & MS_RDONLY)\n\t\treturn -EROFS;\n\n\tINIT_LIST_HEAD(&works);\n\n\tspin_lock(&root->fs_info->delalloc_lock);\n\twhile (!list_empty(head)) {\n\t\tbinode = list_entry(head->next, struct btrfs_inode,\n\t\t\t\t    delalloc_inodes);\n\t\tinode = igrab(&binode->vfs_inode);\n\t\tif (!inode)\n\t\t\tlist_del_init(&binode->delalloc_inodes);\n\t\tspin_unlock(&root->fs_info->delalloc_lock);\n\t\tif (inode) {\n\t\t\twork = btrfs_alloc_delalloc_work(inode, 0, delay_iput);\n\t\t\tif (!work) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tlist_add_tail(&work->list, &works);\n\t\t\tbtrfs_queue_worker(&root->fs_info->flush_workers,\n\t\t\t\t\t   &work->work);\n\t\t}\n\t\tcond_resched();\n\t\tspin_lock(&root->fs_info->delalloc_lock);\n\t}\n\tspin_unlock(&root->fs_info->delalloc_lock);\n\n\t/* the filemap_flush will queue IO into the worker threads, but\n\t * we have to make sure the IO is actually started and that\n\t * ordered extents get created before we return\n\t */\n\tatomic_inc(&root->fs_info->async_submit_draining);\n\twhile (atomic_read(&root->fs_info->nr_async_submits) ||\n\t      atomic_read(&root->fs_info->async_delalloc_pages)) {\n\t\twait_event(root->fs_info->async_submit_wait,\n\t\t   (atomic_read(&root->fs_info->nr_async_submits) == 0 &&\n\t\t    atomic_read(&root->fs_info->async_delalloc_pages) == 0));\n\t}\n\tatomic_dec(&root->fs_info->async_submit_draining);\nout:\n\tlist_for_each_entry_safe(work, next, &works, list) {\n\t\tlist_del_init(&work->list);\n\t\tbtrfs_wait_and_free_delalloc_work(work);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_symlink(struct inode *dir, struct dentry *dentry,\n\t\t\t const char *symname)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct inode *inode = NULL;\n\tint err;\n\tint drop_inode = 0;\n\tu64 objectid;\n\tu64 index = 0 ;\n\tint name_len;\n\tint datasize;\n\tunsigned long ptr;\n\tstruct btrfs_file_extent_item *ei;\n\tstruct extent_buffer *leaf;\n\n\tname_len = strlen(symname) + 1;\n\tif (name_len > BTRFS_MAX_INLINE_DATA_SIZE(root))\n\t\treturn -ENAMETOOLONG;\n\n\t/*\n\t * 2 items for inode item and ref\n\t * 2 items for dir items\n\t * 1 item for xattr if selinux is on\n\t */\n\ttrans = btrfs_start_transaction(root, 5);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\terr = btrfs_find_free_ino(root, &objectid);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tinode = btrfs_new_inode(trans, root, dir, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len, btrfs_ino(dir), objectid,\n\t\t\t\tS_IFLNK|S_IRWXUGO, &index);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\tgoto out_unlock;\n\t}\n\n\terr = btrfs_init_inode_security(trans, inode, dir, &dentry->d_name);\n\tif (err) {\n\t\tdrop_inode = 1;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t* If the active LSM wants to access the inode during\n\t* d_instantiate it needs these. Smack checks to see\n\t* if the filesystem supports xattrs by looking at the\n\t* ops vector.\n\t*/\n\tinode->i_fop = &btrfs_file_operations;\n\tinode->i_op = &btrfs_file_inode_operations;\n\n\terr = btrfs_add_nondir(trans, dir, dentry, inode, 0, index);\n\tif (err)\n\t\tdrop_inode = 1;\n\telse {\n\t\tinode->i_mapping->a_ops = &btrfs_aops;\n\t\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\t\tBTRFS_I(inode)->io_tree.ops = &btrfs_extent_io_ops;\n\t}\n\tif (drop_inode)\n\t\tgoto out_unlock;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tdrop_inode = 1;\n\t\tgoto out_unlock;\n\t}\n\tkey.objectid = btrfs_ino(inode);\n\tkey.offset = 0;\n\tbtrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);\n\tdatasize = btrfs_file_extent_calc_inline_size(name_len);\n\terr = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      datasize);\n\tif (err) {\n\t\tdrop_inode = 1;\n\t\tbtrfs_free_path(path);\n\t\tgoto out_unlock;\n\t}\n\tleaf = path->nodes[0];\n\tei = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t    struct btrfs_file_extent_item);\n\tbtrfs_set_file_extent_generation(leaf, ei, trans->transid);\n\tbtrfs_set_file_extent_type(leaf, ei,\n\t\t\t\t   BTRFS_FILE_EXTENT_INLINE);\n\tbtrfs_set_file_extent_encryption(leaf, ei, 0);\n\tbtrfs_set_file_extent_compression(leaf, ei, 0);\n\tbtrfs_set_file_extent_other_encoding(leaf, ei, 0);\n\tbtrfs_set_file_extent_ram_bytes(leaf, ei, name_len);\n\n\tptr = btrfs_file_extent_inline_start(ei);\n\twrite_extent_buffer(leaf, symname, ptr, name_len);\n\tbtrfs_mark_buffer_dirty(leaf);\n\tbtrfs_free_path(path);\n\n\tinode->i_op = &btrfs_symlink_inode_operations;\n\tinode->i_mapping->a_ops = &btrfs_symlink_aops;\n\tinode->i_mapping->backing_dev_info = &root->fs_info->bdi;\n\tinode_set_bytes(inode, name_len);\n\tbtrfs_i_size_write(inode, name_len - 1);\n\terr = btrfs_update_inode(trans, root, inode);\n\tif (err)\n\t\tdrop_inode = 1;\n\nout_unlock:\n\tif (!err)\n\t\td_instantiate(dentry, inode);\n\tbtrfs_end_transaction(trans, root);\n\tif (drop_inode) {\n\t\tinode_dec_link_count(inode);\n\t\tiput(inode);\n\t}\n\tbtrfs_btree_balance_dirty(root);\n\treturn err;\n}\n\nstatic int __btrfs_prealloc_file_range(struct inode *inode, int mode,\n\t\t\t\t       u64 start, u64 num_bytes, u64 min_size,\n\t\t\t\t       loff_t actual_len, u64 *alloc_hint,\n\t\t\t\t       struct btrfs_trans_handle *trans)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_key ins;\n\tu64 cur_offset = start;\n\tu64 i_size;\n\tint ret = 0;\n\tbool own_trans = true;\n\n\tif (trans)\n\t\town_trans = false;\n\twhile (num_bytes > 0) {\n\t\tif (own_trans) {\n\t\t\ttrans = btrfs_start_transaction(root, 3);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tret = btrfs_reserve_extent(trans, root, num_bytes, min_size,\n\t\t\t\t\t   0, *alloc_hint, &ins, 1);\n\t\tif (ret) {\n\t\t\tif (own_trans)\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tbreak;\n\t\t}\n\n\t\tret = insert_reserved_file_extent(trans, inode,\n\t\t\t\t\t\t  cur_offset, ins.objectid,\n\t\t\t\t\t\t  ins.offset, ins.offset,\n\t\t\t\t\t\t  ins.offset, 0, 0, 0,\n\t\t\t\t\t\t  BTRFS_FILE_EXTENT_PREALLOC);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tif (own_trans)\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_drop_extent_cache(inode, cur_offset,\n\t\t\t\t\tcur_offset + ins.offset -1, 0);\n\n\t\tem = alloc_extent_map();\n\t\tif (!em) {\n\t\t\tset_bit(BTRFS_INODE_NEEDS_FULL_SYNC,\n\t\t\t\t&BTRFS_I(inode)->runtime_flags);\n\t\t\tgoto next;\n\t\t}\n\n\t\tem->start = cur_offset;\n\t\tem->orig_start = cur_offset;\n\t\tem->len = ins.offset;\n\t\tem->block_start = ins.objectid;\n\t\tem->block_len = ins.offset;\n\t\tem->orig_block_len = ins.offset;\n\t\tem->bdev = root->fs_info->fs_devices->latest_bdev;\n\t\tset_bit(EXTENT_FLAG_PREALLOC, &em->flags);\n\t\tem->generation = trans->transid;\n\n\t\twhile (1) {\n\t\t\twrite_lock(&em_tree->lock);\n\t\t\tret = add_extent_mapping(em_tree, em);\n\t\t\tif (!ret)\n\t\t\t\tlist_move(&em->list,\n\t\t\t\t\t  &em_tree->modified_extents);\n\t\t\twrite_unlock(&em_tree->lock);\n\t\t\tif (ret != -EEXIST)\n\t\t\t\tbreak;\n\t\t\tbtrfs_drop_extent_cache(inode, cur_offset,\n\t\t\t\t\t\tcur_offset + ins.offset - 1,\n\t\t\t\t\t\t0);\n\t\t}\n\t\tfree_extent_map(em);\nnext:\n\t\tnum_bytes -= ins.offset;\n\t\tcur_offset += ins.offset;\n\t\t*alloc_hint = ins.objectid + ins.offset;\n\n\t\tinode_inc_iversion(inode);\n\t\tinode->i_ctime = CURRENT_TIME;\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_PREALLOC;\n\t\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t\t    (actual_len > inode->i_size) &&\n\t\t    (cur_offset > inode->i_size)) {\n\t\t\tif (cur_offset > actual_len)\n\t\t\t\ti_size = actual_len;\n\t\t\telse\n\t\t\t\ti_size = cur_offset;\n\t\t\ti_size_write(inode, i_size);\n\t\t\tbtrfs_ordered_update_i_size(inode, i_size, NULL);\n\t\t}\n\n\t\tret = btrfs_update_inode(trans, root, inode);\n\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\tif (own_trans)\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (own_trans)\n\t\t\tbtrfs_end_transaction(trans, root);\n\t}\n\treturn ret;\n}\n\nint btrfs_prealloc_file_range(struct inode *inode, int mode,\n\t\t\t      u64 start, u64 num_bytes, u64 min_size,\n\t\t\t      loff_t actual_len, u64 *alloc_hint)\n{\n\treturn __btrfs_prealloc_file_range(inode, mode, start, num_bytes,\n\t\t\t\t\t   min_size, actual_len, alloc_hint,\n\t\t\t\t\t   NULL);\n}\n\nint btrfs_prealloc_file_range_trans(struct inode *inode,\n\t\t\t\t    struct btrfs_trans_handle *trans, int mode,\n\t\t\t\t    u64 start, u64 num_bytes, u64 min_size,\n\t\t\t\t    loff_t actual_len, u64 *alloc_hint)\n{\n\treturn __btrfs_prealloc_file_range(inode, mode, start, num_bytes,\n\t\t\t\t\t   min_size, actual_len, alloc_hint, trans);\n}\n\nstatic int btrfs_set_page_dirty(struct page *page)\n{\n\treturn __set_page_dirty_nobuffers(page);\n}\n\nstatic int btrfs_permission(struct inode *inode, int mask)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tumode_t mode = inode->i_mode;\n\n\tif (mask & MAY_WRITE &&\n\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode))) {\n\t\tif (btrfs_root_readonly(root))\n\t\t\treturn -EROFS;\n\t\tif (BTRFS_I(inode)->flags & BTRFS_INODE_READONLY)\n\t\t\treturn -EACCES;\n\t}\n\treturn generic_permission(inode, mask);\n}\n\nstatic const struct inode_operations btrfs_dir_inode_operations = {\n\t.getattr\t= btrfs_getattr,\n\t.lookup\t\t= btrfs_lookup,\n\t.create\t\t= btrfs_create,\n\t.unlink\t\t= btrfs_unlink,\n\t.link\t\t= btrfs_link,\n\t.mkdir\t\t= btrfs_mkdir,\n\t.rmdir\t\t= btrfs_rmdir,\n\t.rename\t\t= btrfs_rename,\n\t.symlink\t= btrfs_symlink,\n\t.setattr\t= btrfs_setattr,\n\t.mknod\t\t= btrfs_mknod,\n\t.setxattr\t= btrfs_setxattr,\n\t.getxattr\t= btrfs_getxattr,\n\t.listxattr\t= btrfs_listxattr,\n\t.removexattr\t= btrfs_removexattr,\n\t.permission\t= btrfs_permission,\n\t.get_acl\t= btrfs_get_acl,\n};\nstatic const struct inode_operations btrfs_dir_ro_inode_operations = {\n\t.lookup\t\t= btrfs_lookup,\n\t.permission\t= btrfs_permission,\n\t.get_acl\t= btrfs_get_acl,\n};\n\nstatic const struct file_operations btrfs_dir_file_operations = {\n\t.llseek\t\t= generic_file_llseek,\n\t.read\t\t= generic_read_dir,\n\t.readdir\t= btrfs_real_readdir,\n\t.unlocked_ioctl\t= btrfs_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t= btrfs_ioctl,\n#endif\n\t.release        = btrfs_release_file,\n\t.fsync\t\t= btrfs_sync_file,\n};\n\nstatic struct extent_io_ops btrfs_extent_io_ops = {\n\t.fill_delalloc = run_delalloc_range,\n\t.submit_bio_hook = btrfs_submit_bio_hook,\n\t.merge_bio_hook = btrfs_merge_bio_hook,\n\t.readpage_end_io_hook = btrfs_readpage_end_io_hook,\n\t.writepage_end_io_hook = btrfs_writepage_end_io_hook,\n\t.writepage_start_hook = btrfs_writepage_start_hook,\n\t.set_bit_hook = btrfs_set_bit_hook,\n\t.clear_bit_hook = btrfs_clear_bit_hook,\n\t.merge_extent_hook = btrfs_merge_extent_hook,\n\t.split_extent_hook = btrfs_split_extent_hook,\n};\n\n/*\n * btrfs doesn't support the bmap operation because swapfiles\n * use bmap to make a mapping of extents in the file.  They assume\n * these extents won't change over the life of the file and they\n * use the bmap result to do IO directly to the drive.\n *\n * the btrfs bmap call would return logical addresses that aren't\n * suitable for IO and they also will change frequently as COW\n * operations happen.  So, swapfile + btrfs == corruption.\n *\n * For now we're avoiding this by dropping bmap.\n */\nstatic const struct address_space_operations btrfs_aops = {\n\t.readpage\t= btrfs_readpage,\n\t.writepage\t= btrfs_writepage,\n\t.writepages\t= btrfs_writepages,\n\t.readpages\t= btrfs_readpages,\n\t.direct_IO\t= btrfs_direct_IO,\n\t.invalidatepage = btrfs_invalidatepage,\n\t.releasepage\t= btrfs_releasepage,\n\t.set_page_dirty\t= btrfs_set_page_dirty,\n\t.error_remove_page = generic_error_remove_page,\n};\n\nstatic const struct address_space_operations btrfs_symlink_aops = {\n\t.readpage\t= btrfs_readpage,\n\t.writepage\t= btrfs_writepage,\n\t.invalidatepage = btrfs_invalidatepage,\n\t.releasepage\t= btrfs_releasepage,\n};\n\nstatic const struct inode_operations btrfs_file_inode_operations = {\n\t.getattr\t= btrfs_getattr,\n\t.setattr\t= btrfs_setattr,\n\t.setxattr\t= btrfs_setxattr,\n\t.getxattr\t= btrfs_getxattr,\n\t.listxattr      = btrfs_listxattr,\n\t.removexattr\t= btrfs_removexattr,\n\t.permission\t= btrfs_permission,\n\t.fiemap\t\t= btrfs_fiemap,\n\t.get_acl\t= btrfs_get_acl,\n\t.update_time\t= btrfs_update_time,\n};\nstatic const struct inode_operations btrfs_special_inode_operations = {\n\t.getattr\t= btrfs_getattr,\n\t.setattr\t= btrfs_setattr,\n\t.permission\t= btrfs_permission,\n\t.setxattr\t= btrfs_setxattr,\n\t.getxattr\t= btrfs_getxattr,\n\t.listxattr\t= btrfs_listxattr,\n\t.removexattr\t= btrfs_removexattr,\n\t.get_acl\t= btrfs_get_acl,\n\t.update_time\t= btrfs_update_time,\n};\nstatic const struct inode_operations btrfs_symlink_inode_operations = {\n\t.readlink\t= generic_readlink,\n\t.follow_link\t= page_follow_link_light,\n\t.put_link\t= page_put_link,\n\t.getattr\t= btrfs_getattr,\n\t.setattr\t= btrfs_setattr,\n\t.permission\t= btrfs_permission,\n\t.setxattr\t= btrfs_setxattr,\n\t.getxattr\t= btrfs_getxattr,\n\t.listxattr\t= btrfs_listxattr,\n\t.removexattr\t= btrfs_removexattr,\n\t.get_acl\t= btrfs_get_acl,\n\t.update_time\t= btrfs_update_time,\n};\n\nconst struct dentry_operations btrfs_dentry_operations = {\n\t.d_delete\t= btrfs_dentry_delete,\n\t.d_release\t= btrfs_dentry_release,\n};\n", "/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#include <linux/kernel.h>\n#include <linux/bio.h>\n#include <linux/buffer_head.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/fsnotify.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/time.h>\n#include <linux/init.h>\n#include <linux/string.h>\n#include <linux/backing-dev.h>\n#include <linux/mount.h>\n#include <linux/mpage.h>\n#include <linux/namei.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/statfs.h>\n#include <linux/compat.h>\n#include <linux/bit_spinlock.h>\n#include <linux/security.h>\n#include <linux/xattr.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/uuid.h>\n#include \"compat.h\"\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"btrfs_inode.h\"\n#include \"ioctl.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"locking.h\"\n#include \"inode-map.h\"\n#include \"backref.h\"\n#include \"rcu-string.h\"\n#include \"send.h\"\n#include \"dev-replace.h\"\n\n/* Mask out flags that are inappropriate for the given type of inode. */\nstatic inline __u32 btrfs_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & ~FS_DIRSYNC_FL;\n\telse\n\t\treturn flags & (FS_NODUMP_FL | FS_NOATIME_FL);\n}\n\n/*\n * Export inode flags to the format expected by the FS_IOC_GETFLAGS ioctl.\n */\nstatic unsigned int btrfs_flags_to_ioctl(unsigned int flags)\n{\n\tunsigned int iflags = 0;\n\n\tif (flags & BTRFS_INODE_SYNC)\n\t\tiflags |= FS_SYNC_FL;\n\tif (flags & BTRFS_INODE_IMMUTABLE)\n\t\tiflags |= FS_IMMUTABLE_FL;\n\tif (flags & BTRFS_INODE_APPEND)\n\t\tiflags |= FS_APPEND_FL;\n\tif (flags & BTRFS_INODE_NODUMP)\n\t\tiflags |= FS_NODUMP_FL;\n\tif (flags & BTRFS_INODE_NOATIME)\n\t\tiflags |= FS_NOATIME_FL;\n\tif (flags & BTRFS_INODE_DIRSYNC)\n\t\tiflags |= FS_DIRSYNC_FL;\n\tif (flags & BTRFS_INODE_NODATACOW)\n\t\tiflags |= FS_NOCOW_FL;\n\n\tif ((flags & BTRFS_INODE_COMPRESS) && !(flags & BTRFS_INODE_NOCOMPRESS))\n\t\tiflags |= FS_COMPR_FL;\n\telse if (flags & BTRFS_INODE_NOCOMPRESS)\n\t\tiflags |= FS_NOCOMP_FL;\n\n\treturn iflags;\n}\n\n/*\n * Update inode->i_flags based on the btrfs internal flags.\n */\nvoid btrfs_update_iflags(struct inode *inode)\n{\n\tstruct btrfs_inode *ip = BTRFS_I(inode);\n\n\tinode->i_flags &= ~(S_SYNC|S_APPEND|S_IMMUTABLE|S_NOATIME|S_DIRSYNC);\n\n\tif (ip->flags & BTRFS_INODE_SYNC)\n\t\tinode->i_flags |= S_SYNC;\n\tif (ip->flags & BTRFS_INODE_IMMUTABLE)\n\t\tinode->i_flags |= S_IMMUTABLE;\n\tif (ip->flags & BTRFS_INODE_APPEND)\n\t\tinode->i_flags |= S_APPEND;\n\tif (ip->flags & BTRFS_INODE_NOATIME)\n\t\tinode->i_flags |= S_NOATIME;\n\tif (ip->flags & BTRFS_INODE_DIRSYNC)\n\t\tinode->i_flags |= S_DIRSYNC;\n}\n\n/*\n * Inherit flags from the parent inode.\n *\n * Currently only the compression flags and the cow flags are inherited.\n */\nvoid btrfs_inherit_iflags(struct inode *inode, struct inode *dir)\n{\n\tunsigned int flags;\n\n\tif (!dir)\n\t\treturn;\n\n\tflags = BTRFS_I(dir)->flags;\n\n\tif (flags & BTRFS_INODE_NOCOMPRESS) {\n\t\tBTRFS_I(inode)->flags &= ~BTRFS_INODE_COMPRESS;\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NOCOMPRESS;\n\t} else if (flags & BTRFS_INODE_COMPRESS) {\n\t\tBTRFS_I(inode)->flags &= ~BTRFS_INODE_NOCOMPRESS;\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_COMPRESS;\n\t}\n\n\tif (flags & BTRFS_INODE_NODATACOW)\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NODATACOW;\n\n\tbtrfs_update_iflags(inode);\n}\n\nstatic int btrfs_ioctl_getflags(struct file *file, void __user *arg)\n{\n\tstruct btrfs_inode *ip = BTRFS_I(file->f_path.dentry->d_inode);\n\tunsigned int flags = btrfs_flags_to_ioctl(ip->flags);\n\n\tif (copy_to_user(arg, &flags, sizeof(flags)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int check_flags(unsigned int flags)\n{\n\tif (flags & ~(FS_IMMUTABLE_FL | FS_APPEND_FL | \\\n\t\t      FS_NOATIME_FL | FS_NODUMP_FL | \\\n\t\t      FS_SYNC_FL | FS_DIRSYNC_FL | \\\n\t\t      FS_NOCOMP_FL | FS_COMPR_FL |\n\t\t      FS_NOCOW_FL))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((flags & FS_NOCOMP_FL) && (flags & FS_COMPR_FL))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int btrfs_ioctl_setflags(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct btrfs_inode *ip = BTRFS_I(inode);\n\tstruct btrfs_root *root = ip->root;\n\tstruct btrfs_trans_handle *trans;\n\tunsigned int flags, oldflags;\n\tint ret;\n\tu64 ip_oldflags;\n\tunsigned int i_oldflags;\n\tumode_t mode;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tif (copy_from_user(&flags, arg, sizeof(flags)))\n\t\treturn -EFAULT;\n\n\tret = check_flags(flags);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EACCES;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tip_oldflags = ip->flags;\n\ti_oldflags = inode->i_flags;\n\tmode = inode->i_mode;\n\n\tflags = btrfs_mask_flags(inode->i_mode, flags);\n\toldflags = btrfs_flags_to_ioctl(ip->flags);\n\tif ((flags ^ oldflags) & (FS_APPEND_FL | FS_IMMUTABLE_FL)) {\n\t\tif (!capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (flags & FS_SYNC_FL)\n\t\tip->flags |= BTRFS_INODE_SYNC;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_SYNC;\n\tif (flags & FS_IMMUTABLE_FL)\n\t\tip->flags |= BTRFS_INODE_IMMUTABLE;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_IMMUTABLE;\n\tif (flags & FS_APPEND_FL)\n\t\tip->flags |= BTRFS_INODE_APPEND;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_APPEND;\n\tif (flags & FS_NODUMP_FL)\n\t\tip->flags |= BTRFS_INODE_NODUMP;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_NODUMP;\n\tif (flags & FS_NOATIME_FL)\n\t\tip->flags |= BTRFS_INODE_NOATIME;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_NOATIME;\n\tif (flags & FS_DIRSYNC_FL)\n\t\tip->flags |= BTRFS_INODE_DIRSYNC;\n\telse\n\t\tip->flags &= ~BTRFS_INODE_DIRSYNC;\n\tif (flags & FS_NOCOW_FL) {\n\t\tif (S_ISREG(mode)) {\n\t\t\t/*\n\t\t\t * It's safe to turn csums off here, no extents exist.\n\t\t\t * Otherwise we want the flag to reflect the real COW\n\t\t\t * status of the file and will not set it.\n\t\t\t */\n\t\t\tif (inode->i_size == 0)\n\t\t\t\tip->flags |= BTRFS_INODE_NODATACOW\n\t\t\t\t\t   | BTRFS_INODE_NODATASUM;\n\t\t} else {\n\t\t\tip->flags |= BTRFS_INODE_NODATACOW;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * Revert back under same assuptions as above\n\t\t */\n\t\tif (S_ISREG(mode)) {\n\t\t\tif (inode->i_size == 0)\n\t\t\t\tip->flags &= ~(BTRFS_INODE_NODATACOW\n\t\t\t\t             | BTRFS_INODE_NODATASUM);\n\t\t} else {\n\t\t\tip->flags &= ~BTRFS_INODE_NODATACOW;\n\t\t}\n\t}\n\n\t/*\n\t * The COMPRESS flag can only be changed by users, while the NOCOMPRESS\n\t * flag may be changed automatically if compression code won't make\n\t * things smaller.\n\t */\n\tif (flags & FS_NOCOMP_FL) {\n\t\tip->flags &= ~BTRFS_INODE_COMPRESS;\n\t\tip->flags |= BTRFS_INODE_NOCOMPRESS;\n\t} else if (flags & FS_COMPR_FL) {\n\t\tip->flags |= BTRFS_INODE_COMPRESS;\n\t\tip->flags &= ~BTRFS_INODE_NOCOMPRESS;\n\t} else {\n\t\tip->flags &= ~(BTRFS_INODE_COMPRESS | BTRFS_INODE_NOCOMPRESS);\n\t}\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_drop;\n\t}\n\n\tbtrfs_update_iflags(inode);\n\tinode_inc_iversion(inode);\n\tinode->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode(trans, root, inode);\n\n\tbtrfs_end_transaction(trans, root);\n out_drop:\n\tif (ret) {\n\t\tip->flags = ip_oldflags;\n\t\tinode->i_flags = i_oldflags;\n\t}\n\n out_unlock:\n\tmutex_unlock(&inode->i_mutex);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic int btrfs_ioctl_getversion(struct file *file, int __user *arg)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\n\treturn put_user(inode->i_generation, arg);\n}\n\nstatic noinline int btrfs_ioctl_fitrim(struct file *file, void __user *arg)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(fdentry(file)->d_sb);\n\tstruct btrfs_device *device;\n\tstruct request_queue *q;\n\tstruct fstrim_range range;\n\tu64 minlen = ULLONG_MAX;\n\tu64 num_devices = 0;\n\tu64 total_bytes = btrfs_super_total_bytes(fs_info->super_copy);\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(device, &fs_info->fs_devices->devices,\n\t\t\t\tdev_list) {\n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\t\tq = bdev_get_queue(device->bdev);\n\t\tif (blk_queue_discard(q)) {\n\t\t\tnum_devices++;\n\t\t\tminlen = min((u64)q->limits.discard_granularity,\n\t\t\t\t     minlen);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (!num_devices)\n\t\treturn -EOPNOTSUPP;\n\tif (copy_from_user(&range, arg, sizeof(range)))\n\t\treturn -EFAULT;\n\tif (range.start > total_bytes ||\n\t    range.len < fs_info->sb->s_blocksize)\n\t\treturn -EINVAL;\n\n\trange.len = min(range.len, total_bytes - range.start);\n\trange.minlen = max(range.minlen, minlen);\n\tret = btrfs_trim_fs(fs_info->tree_root, &range);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (copy_to_user(arg, &range, sizeof(range)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic noinline int create_subvol(struct btrfs_root *root,\n\t\t\t\t  struct dentry *dentry,\n\t\t\t\t  char *name, int namelen,\n\t\t\t\t  u64 *async_transid,\n\t\t\t\t  struct btrfs_qgroup_inherit **inherit)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_key key;\n\tstruct btrfs_root_item root_item;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_root *new_root;\n\tstruct dentry *parent = dentry->d_parent;\n\tstruct inode *dir;\n\tstruct timespec cur_time = CURRENT_TIME;\n\tint ret;\n\tint err;\n\tu64 objectid;\n\tu64 new_dirid = BTRFS_FIRST_FREE_OBJECTID;\n\tu64 index = 0;\n\tuuid_le new_uuid;\n\n\tret = btrfs_find_free_objectid(root->fs_info->tree_root, &objectid);\n\tif (ret)\n\t\treturn ret;\n\n\tdir = parent->d_inode;\n\n\t/*\n\t * 1 - inode item\n\t * 2 - refs\n\t * 1 - root item\n\t * 2 - dir items\n\t */\n\ttrans = btrfs_start_transaction(root, 6);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tret = btrfs_qgroup_inherit(trans, root->fs_info, 0, objectid,\n\t\t\t\t   inherit ? *inherit : NULL);\n\tif (ret)\n\t\tgoto fail;\n\n\tleaf = btrfs_alloc_free_block(trans, root, root->leafsize,\n\t\t\t\t      0, objectid, NULL, 0, 0, 0);\n\tif (IS_ERR(leaf)) {\n\t\tret = PTR_ERR(leaf);\n\t\tgoto fail;\n\t}\n\n\tmemset_extent_buffer(leaf, 0, 0, sizeof(struct btrfs_header));\n\tbtrfs_set_header_bytenr(leaf, leaf->start);\n\tbtrfs_set_header_generation(leaf, trans->transid);\n\tbtrfs_set_header_backref_rev(leaf, BTRFS_MIXED_BACKREF_REV);\n\tbtrfs_set_header_owner(leaf, objectid);\n\n\twrite_extent_buffer(leaf, root->fs_info->fsid,\n\t\t\t    (unsigned long)btrfs_header_fsid(leaf),\n\t\t\t    BTRFS_FSID_SIZE);\n\twrite_extent_buffer(leaf, root->fs_info->chunk_tree_uuid,\n\t\t\t    (unsigned long)btrfs_header_chunk_tree_uuid(leaf),\n\t\t\t    BTRFS_UUID_SIZE);\n\tbtrfs_mark_buffer_dirty(leaf);\n\n\tmemset(&root_item, 0, sizeof(root_item));\n\n\tinode_item = &root_item.inode;\n\tinode_item->generation = cpu_to_le64(1);\n\tinode_item->size = cpu_to_le64(3);\n\tinode_item->nlink = cpu_to_le32(1);\n\tinode_item->nbytes = cpu_to_le64(root->leafsize);\n\tinode_item->mode = cpu_to_le32(S_IFDIR | 0755);\n\n\troot_item.flags = 0;\n\troot_item.byte_limit = 0;\n\tinode_item->flags = cpu_to_le64(BTRFS_INODE_ROOT_ITEM_INIT);\n\n\tbtrfs_set_root_bytenr(&root_item, leaf->start);\n\tbtrfs_set_root_generation(&root_item, trans->transid);\n\tbtrfs_set_root_level(&root_item, 0);\n\tbtrfs_set_root_refs(&root_item, 1);\n\tbtrfs_set_root_used(&root_item, leaf->len);\n\tbtrfs_set_root_last_snapshot(&root_item, 0);\n\n\tbtrfs_set_root_generation_v2(&root_item,\n\t\t\tbtrfs_root_generation(&root_item));\n\tuuid_le_gen(&new_uuid);\n\tmemcpy(root_item.uuid, new_uuid.b, BTRFS_UUID_SIZE);\n\troot_item.otime.sec = cpu_to_le64(cur_time.tv_sec);\n\troot_item.otime.nsec = cpu_to_le32(cur_time.tv_nsec);\n\troot_item.ctime = root_item.otime;\n\tbtrfs_set_root_ctransid(&root_item, trans->transid);\n\tbtrfs_set_root_otransid(&root_item, trans->transid);\n\n\tbtrfs_tree_unlock(leaf);\n\tfree_extent_buffer(leaf);\n\tleaf = NULL;\n\n\tbtrfs_set_root_dirid(&root_item, new_dirid);\n\n\tkey.objectid = objectid;\n\tkey.offset = 0;\n\tbtrfs_set_key_type(&key, BTRFS_ROOT_ITEM_KEY);\n\tret = btrfs_insert_root(trans, root->fs_info->tree_root, &key,\n\t\t\t\t&root_item);\n\tif (ret)\n\t\tgoto fail;\n\n\tkey.offset = (u64)-1;\n\tnew_root = btrfs_read_fs_root_no_name(root->fs_info, &key);\n\tif (IS_ERR(new_root)) {\n\t\tbtrfs_abort_transaction(trans, root, PTR_ERR(new_root));\n\t\tret = PTR_ERR(new_root);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_record_root_in_trans(trans, new_root);\n\n\tret = btrfs_create_subvol_root(trans, new_root, new_dirid);\n\tif (ret) {\n\t\t/* We potentially lose an unused inode item here */\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * insert the directory item\n\t */\n\tret = btrfs_set_inode_index(dir, &index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_insert_dir_item(trans, root,\n\t\t\t\t    name, namelen, dir, &key,\n\t\t\t\t    BTRFS_FT_DIR, index);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_i_size_write(dir, dir->i_size + namelen * 2);\n\tret = btrfs_update_inode(trans, root, dir);\n\tBUG_ON(ret);\n\n\tret = btrfs_add_root_ref(trans, root->fs_info->tree_root,\n\t\t\t\t objectid, root->root_key.objectid,\n\t\t\t\t btrfs_ino(dir), index, name, namelen);\n\n\tBUG_ON(ret);\n\n\td_instantiate(dentry, btrfs_lookup_dentry(dir, dentry));\nfail:\n\tif (async_transid) {\n\t\t*async_transid = trans->transid;\n\t\terr = btrfs_commit_transaction_async(trans, root, 1);\n\t} else {\n\t\terr = btrfs_commit_transaction(trans, root);\n\t}\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int create_snapshot(struct btrfs_root *root, struct dentry *dentry,\n\t\t\t   char *name, int namelen, u64 *async_transid,\n\t\t\t   bool readonly, struct btrfs_qgroup_inherit **inherit)\n{\n\tstruct inode *inode;\n\tstruct btrfs_pending_snapshot *pending_snapshot;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\tif (!root->ref_cows)\n\t\treturn -EINVAL;\n\n\tpending_snapshot = kzalloc(sizeof(*pending_snapshot), GFP_NOFS);\n\tif (!pending_snapshot)\n\t\treturn -ENOMEM;\n\n\tbtrfs_init_block_rsv(&pending_snapshot->block_rsv,\n\t\t\t     BTRFS_BLOCK_RSV_TEMP);\n\tpending_snapshot->dentry = dentry;\n\tpending_snapshot->root = root;\n\tpending_snapshot->readonly = readonly;\n\tif (inherit) {\n\t\tpending_snapshot->inherit = *inherit;\n\t\t*inherit = NULL;\t/* take responsibility to free it */\n\t}\n\n\ttrans = btrfs_start_transaction(root->fs_info->extent_root, 6);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_snap_reserve_metadata(trans, pending_snapshot);\n\tBUG_ON(ret);\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tlist_add(&pending_snapshot->list,\n\t\t &trans->transaction->pending_snapshots);\n\tspin_unlock(&root->fs_info->trans_lock);\n\tif (async_transid) {\n\t\t*async_transid = trans->transid;\n\t\tret = btrfs_commit_transaction_async(trans,\n\t\t\t\t     root->fs_info->extent_root, 1);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans,\n\t\t\t\t\t       root->fs_info->extent_root);\n\t}\n\tif (ret) {\n\t\t/* cleanup_transaction has freed this for us */\n\t\tif (trans->aborted)\n\t\t\tpending_snapshot = NULL;\n\t\tgoto fail;\n\t}\n\n\tret = pending_snapshot->error;\n\tif (ret)\n\t\tgoto fail;\n\n\tret = btrfs_orphan_cleanup(pending_snapshot->snap);\n\tif (ret)\n\t\tgoto fail;\n\n\tinode = btrfs_lookup_dentry(dentry->d_parent->d_inode, dentry);\n\tif (IS_ERR(inode)) {\n\t\tret = PTR_ERR(inode);\n\t\tgoto fail;\n\t}\n\tBUG_ON(!inode);\n\td_instantiate(dentry, inode);\n\tret = 0;\nfail:\n\tkfree(pending_snapshot);\n\treturn ret;\n}\n\n/*  copy of check_sticky in fs/namei.c()\n* It's inline, so penalty for filesystems that don't use sticky bit is\n* minimal.\n*/\nstatic inline int btrfs_check_sticky(struct inode *dir, struct inode *inode)\n{\n\tkuid_t fsuid = current_fsuid();\n\n\tif (!(dir->i_mode & S_ISVTX))\n\t\treturn 0;\n\tif (uid_eq(inode->i_uid, fsuid))\n\t\treturn 0;\n\tif (uid_eq(dir->i_uid, fsuid))\n\t\treturn 0;\n\treturn !capable(CAP_FOWNER);\n}\n\n/*  copy of may_delete in fs/namei.c()\n *\tCheck whether we can remove a link victim from directory dir, check\n *  whether the type of victim is right.\n *  1. We can't do it if dir is read-only (done in permission())\n *  2. We should have write and exec permissions on dir\n *  3. We can't remove anything from append-only dir\n *  4. We can't do anything with immutable dir (done in permission())\n *  5. If the sticky bit on dir is set we should either\n *\ta. be owner of dir, or\n *\tb. be owner of victim, or\n *\tc. have CAP_FOWNER capability\n *  6. If the victim is append-only or immutable we can't do antyhing with\n *     links pointing to it.\n *  7. If we were asked to remove a directory and victim isn't one - ENOTDIR.\n *  8. If we were asked to remove a non-directory and victim isn't one - EISDIR.\n *  9. We can't remove a root or mountpoint.\n * 10. We don't allow removal of NFS sillyrenamed files; it's handled by\n *     nfs_async_unlink().\n */\n\nstatic int btrfs_may_delete(struct inode *dir,struct dentry *victim,int isdir)\n{\n\tint error;\n\n\tif (!victim->d_inode)\n\t\treturn -ENOENT;\n\n\tBUG_ON(victim->d_parent->d_inode != dir);\n\taudit_inode_child(dir, victim, AUDIT_TYPE_CHILD_DELETE);\n\n\terror = inode_permission(dir, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\tif (IS_APPEND(dir))\n\t\treturn -EPERM;\n\tif (btrfs_check_sticky(dir, victim->d_inode)||\n\t\tIS_APPEND(victim->d_inode)||\n\t    IS_IMMUTABLE(victim->d_inode) || IS_SWAPFILE(victim->d_inode))\n\t\treturn -EPERM;\n\tif (isdir) {\n\t\tif (!S_ISDIR(victim->d_inode->i_mode))\n\t\t\treturn -ENOTDIR;\n\t\tif (IS_ROOT(victim))\n\t\t\treturn -EBUSY;\n\t} else if (S_ISDIR(victim->d_inode->i_mode))\n\t\treturn -EISDIR;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\tif (victim->d_flags & DCACHE_NFSFS_RENAMED)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\n/* copy of may_create in fs/namei.c() */\nstatic inline int btrfs_may_create(struct inode *dir, struct dentry *child)\n{\n\tif (child->d_inode)\n\t\treturn -EEXIST;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\treturn inode_permission(dir, MAY_WRITE | MAY_EXEC);\n}\n\n/*\n * Create a new subvolume below @parent.  This is largely modeled after\n * sys_mkdirat and vfs_mkdir, but we only do a single component lookup\n * inside this filesystem so it's quite a bit simpler.\n */\nstatic noinline int btrfs_mksubvol(struct path *parent,\n\t\t\t\t   char *name, int namelen,\n\t\t\t\t   struct btrfs_root *snap_src,\n\t\t\t\t   u64 *async_transid, bool readonly,\n\t\t\t\t   struct btrfs_qgroup_inherit **inherit)\n{\n\tstruct inode *dir  = parent->dentry->d_inode;\n\tstruct dentry *dentry;\n\tint error;\n\n\tmutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);\n\n\tdentry = lookup_one_len(name, parent->dentry, namelen);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto out_unlock;\n\n\terror = -EEXIST;\n\tif (dentry->d_inode)\n\t\tgoto out_dput;\n\n\terror = btrfs_may_create(dir, dentry);\n\tif (error)\n\t\tgoto out_dput;\n\n\t/*\n\t * even if this name doesn't exist, we may get hash collisions.\n\t * check for them now when we can safely fail\n\t */\n\terror = btrfs_check_dir_item_collision(BTRFS_I(dir)->root,\n\t\t\t\t\t       dir->i_ino, name,\n\t\t\t\t\t       namelen);\n\tif (error)\n\t\tgoto out_dput;\n\n\tdown_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);\n\n\tif (btrfs_root_refs(&BTRFS_I(dir)->root->root_item) == 0)\n\t\tgoto out_up_read;\n\n\tif (snap_src) {\n\t\terror = create_snapshot(snap_src, dentry, name, namelen,\n\t\t\t\t\tasync_transid, readonly, inherit);\n\t} else {\n\t\terror = create_subvol(BTRFS_I(dir)->root, dentry,\n\t\t\t\t      name, namelen, async_transid, inherit);\n\t}\n\tif (!error)\n\t\tfsnotify_mkdir(dir, dentry);\nout_up_read:\n\tup_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);\nout_dput:\n\tdput(dentry);\nout_unlock:\n\tmutex_unlock(&dir->i_mutex);\n\treturn error;\n}\n\n/*\n * When we're defragging a range, we don't want to kick it off again\n * if it is really just waiting for delalloc to send it down.\n * If we find a nice big extent or delalloc range for the bytes in the\n * file you want to defrag, we return 0 to let you know to skip this\n * part of the file\n */\nstatic int check_defrag_in_cache(struct inode *inode, u64 offset, int thresh)\n{\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tu64 end;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, offset, PAGE_CACHE_SIZE);\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tend = extent_map_end(em);\n\t\tfree_extent_map(em);\n\t\tif (end - offset > thresh)\n\t\t\treturn 0;\n\t}\n\t/* if we already have a nice delalloc here, just stop */\n\tthresh /= 2;\n\tend = count_range_bits(io_tree, &offset, offset + thresh,\n\t\t\t       thresh, EXTENT_DELALLOC, 1);\n\tif (end >= thresh)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * helper function to walk through a file and find extents\n * newer than a specific transid, and smaller than thresh.\n *\n * This is used by the defragging code to find new and small\n * extents\n */\nstatic int find_new_extents(struct btrfs_root *root,\n\t\t\t    struct inode *inode, u64 newer_than,\n\t\t\t    u64 *off, int thresh)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_key min_key;\n\tstruct btrfs_key max_key;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *extent;\n\tint type;\n\tint ret;\n\tu64 ino = btrfs_ino(inode);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmin_key.objectid = ino;\n\tmin_key.type = BTRFS_EXTENT_DATA_KEY;\n\tmin_key.offset = *off;\n\n\tmax_key.objectid = ino;\n\tmax_key.type = (u8)-1;\n\tmax_key.offset = (u64)-1;\n\n\tpath->keep_locks = 1;\n\n\twhile(1) {\n\t\tret = btrfs_search_forward(root, &min_key, &max_key,\n\t\t\t\t\t   path, 0, newer_than);\n\t\tif (ret != 0)\n\t\t\tgoto none;\n\t\tif (min_key.objectid != ino)\n\t\t\tgoto none;\n\t\tif (min_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto none;\n\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_file_extent_item);\n\n\t\ttype = btrfs_file_extent_type(leaf, extent);\n\t\tif (type == BTRFS_FILE_EXTENT_REG &&\n\t\t    btrfs_file_extent_num_bytes(leaf, extent) < thresh &&\n\t\t    check_defrag_in_cache(inode, min_key.offset, thresh)) {\n\t\t\t*off = min_key.offset;\n\t\t\tbtrfs_free_path(path);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (min_key.offset == (u64)-1)\n\t\t\tgoto none;\n\n\t\tmin_key.offset++;\n\t\tbtrfs_release_path(path);\n\t}\nnone:\n\tbtrfs_free_path(path);\n\treturn -ENOENT;\n}\n\nstatic struct extent_map *defrag_lookup_extent(struct inode *inode, u64 start)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em;\n\tu64 len = PAGE_CACHE_SIZE;\n\n\t/*\n\t * hopefully we have this extent in the tree already, try without\n\t * the full extent lock\n\t */\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\t/* get the big lock and read metadata off disk */\n\t\tlock_extent(io_tree, start, start + len - 1);\n\t\tem = btrfs_get_extent(inode, NULL, 0, start, len, 0);\n\t\tunlock_extent(io_tree, start, start + len - 1);\n\n\t\tif (IS_ERR(em))\n\t\t\treturn NULL;\n\t}\n\n\treturn em;\n}\n\nstatic bool defrag_check_next_extent(struct inode *inode, struct extent_map *em)\n{\n\tstruct extent_map *next;\n\tbool ret = true;\n\n\t/* this is the last extent */\n\tif (em->start + em->len >= i_size_read(inode))\n\t\treturn false;\n\n\tnext = defrag_lookup_extent(inode, em->start + em->len);\n\tif (!next || next->block_start >= EXTENT_MAP_LAST_BYTE)\n\t\tret = false;\n\n\tfree_extent_map(next);\n\treturn ret;\n}\n\nstatic int should_defrag_range(struct inode *inode, u64 start, int thresh,\n\t\t\t       u64 *last_len, u64 *skip, u64 *defrag_end,\n\t\t\t       int compress)\n{\n\tstruct extent_map *em;\n\tint ret = 1;\n\tbool next_mergeable = true;\n\n\t/*\n\t * make sure that once we start defragging an extent, we keep on\n\t * defragging it\n\t */\n\tif (start < *defrag_end)\n\t\treturn 1;\n\n\t*skip = 0;\n\n\tem = defrag_lookup_extent(inode, start);\n\tif (!em)\n\t\treturn 0;\n\n\t/* this will cover holes, and inline extents */\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tnext_mergeable = defrag_check_next_extent(inode, em);\n\n\t/*\n\t * we hit a real extent, if it is big or the next extent is not a\n\t * real extent, don't bother defragging it\n\t */\n\tif (!compress && (*last_len == 0 || *last_len >= thresh) &&\n\t    (em->len >= thresh || !next_mergeable))\n\t\tret = 0;\nout:\n\t/*\n\t * last_len ends up being a counter of how many bytes we've defragged.\n\t * every time we choose not to defrag an extent, we reset *last_len\n\t * so that the next tiny extent will force a defrag.\n\t *\n\t * The end result of this is that tiny extents before a single big\n\t * extent will force at least part of that big extent to be defragged.\n\t */\n\tif (ret) {\n\t\t*defrag_end = extent_map_end(em);\n\t} else {\n\t\t*last_len = 0;\n\t\t*skip = extent_map_end(em);\n\t\t*defrag_end = 0;\n\t}\n\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * it doesn't do much good to defrag one or two pages\n * at a time.  This pulls in a nice chunk of pages\n * to COW and defrag.\n *\n * It also makes sure the delalloc code has enough\n * dirty data to avoid making new small extents as part\n * of the defrag\n *\n * It's a good idea to start RA on this range\n * before calling this.\n */\nstatic int cluster_pages_for_defrag(struct inode *inode,\n\t\t\t\t    struct page **pages,\n\t\t\t\t    unsigned long start_index,\n\t\t\t\t    int num_pages)\n{\n\tunsigned long file_end;\n\tu64 isize = i_size_read(inode);\n\tu64 page_start;\n\tu64 page_end;\n\tu64 page_cnt;\n\tint ret;\n\tint i;\n\tint i_done;\n\tstruct btrfs_ordered_extent *ordered;\n\tstruct extent_state *cached_state = NULL;\n\tstruct extent_io_tree *tree;\n\tgfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);\n\n\tfile_end = (isize - 1) >> PAGE_CACHE_SHIFT;\n\tif (!isize || start_index > file_end)\n\t\treturn 0;\n\n\tpage_cnt = min_t(u64, (u64)num_pages, (u64)file_end - start_index + 1);\n\n\tret = btrfs_delalloc_reserve_space(inode,\n\t\t\t\t\t   page_cnt << PAGE_CACHE_SHIFT);\n\tif (ret)\n\t\treturn ret;\n\ti_done = 0;\n\ttree = &BTRFS_I(inode)->io_tree;\n\n\t/* step one, lock all the pages */\n\tfor (i = 0; i < page_cnt; i++) {\n\t\tstruct page *page;\nagain:\n\t\tpage = find_or_create_page(inode->i_mapping,\n\t\t\t\t\t   start_index + i, mask);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tpage_start = page_offset(page);\n\t\tpage_end = page_start + PAGE_CACHE_SIZE - 1;\n\t\twhile (1) {\n\t\t\tlock_extent(tree, page_start, page_end);\n\t\t\tordered = btrfs_lookup_ordered_extent(inode,\n\t\t\t\t\t\t\t      page_start);\n\t\t\tunlock_extent(tree, page_start, page_end);\n\t\t\tif (!ordered)\n\t\t\t\tbreak;\n\n\t\t\tunlock_page(page);\n\t\t\tbtrfs_start_ordered_extent(inode, ordered, 1);\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\t\tlock_page(page);\n\t\t\t/*\n\t\t\t * we unlocked the page above, so we need check if\n\t\t\t * it was released or not.\n\t\t\t */\n\t\t\tif (page->mapping != inode->i_mapping) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tpage_cache_release(page);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\n\t\tif (!PageUptodate(page)) {\n\t\t\tbtrfs_readpage(NULL, page);\n\t\t\tlock_page(page);\n\t\t\tif (!PageUptodate(page)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tpage_cache_release(page);\n\t\t\t\tret = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (page->mapping != inode->i_mapping) {\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t\tgoto again;\n\t\t}\n\n\t\tpages[i] = page;\n\t\ti_done++;\n\t}\n\tif (!i_done || ret)\n\t\tgoto out;\n\n\tif (!(inode->i_sb->s_flags & MS_ACTIVE))\n\t\tgoto out;\n\n\t/*\n\t * so now we have a nice long stream of locked\n\t * and up to date pages, lets wait on them\n\t */\n\tfor (i = 0; i < i_done; i++)\n\t\twait_on_page_writeback(pages[i]);\n\n\tpage_start = page_offset(pages[0]);\n\tpage_end = page_offset(pages[i_done - 1]) + PAGE_CACHE_SIZE;\n\n\tlock_extent_bits(&BTRFS_I(inode)->io_tree,\n\t\t\t page_start, page_end - 1, 0, &cached_state);\n\tclear_extent_bit(&BTRFS_I(inode)->io_tree, page_start,\n\t\t\t  page_end - 1, EXTENT_DIRTY | EXTENT_DELALLOC |\n\t\t\t  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0,\n\t\t\t  &cached_state, GFP_NOFS);\n\n\tif (i_done != page_cnt) {\n\t\tspin_lock(&BTRFS_I(inode)->lock);\n\t\tBTRFS_I(inode)->outstanding_extents++;\n\t\tspin_unlock(&BTRFS_I(inode)->lock);\n\t\tbtrfs_delalloc_release_space(inode,\n\t\t\t\t     (page_cnt - i_done) << PAGE_CACHE_SHIFT);\n\t}\n\n\n\tset_extent_defrag(&BTRFS_I(inode)->io_tree, page_start, page_end - 1,\n\t\t\t  &cached_state, GFP_NOFS);\n\n\tunlock_extent_cached(&BTRFS_I(inode)->io_tree,\n\t\t\t     page_start, page_end - 1, &cached_state,\n\t\t\t     GFP_NOFS);\n\n\tfor (i = 0; i < i_done; i++) {\n\t\tclear_page_dirty_for_io(pages[i]);\n\t\tClearPageChecked(pages[i]);\n\t\tset_page_extent_mapped(pages[i]);\n\t\tset_page_dirty(pages[i]);\n\t\tunlock_page(pages[i]);\n\t\tpage_cache_release(pages[i]);\n\t}\n\treturn i_done;\nout:\n\tfor (i = 0; i < i_done; i++) {\n\t\tunlock_page(pages[i]);\n\t\tpage_cache_release(pages[i]);\n\t}\n\tbtrfs_delalloc_release_space(inode, page_cnt << PAGE_CACHE_SHIFT);\n\treturn ret;\n\n}\n\nint btrfs_defrag_file(struct inode *inode, struct file *file,\n\t\t      struct btrfs_ioctl_defrag_range_args *range,\n\t\t      u64 newer_than, unsigned long max_to_defrag)\n{\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct file_ra_state *ra = NULL;\n\tunsigned long last_index;\n\tu64 isize = i_size_read(inode);\n\tu64 last_len = 0;\n\tu64 skip = 0;\n\tu64 defrag_end = 0;\n\tu64 newer_off = range->start;\n\tunsigned long i;\n\tunsigned long ra_index = 0;\n\tint ret;\n\tint defrag_count = 0;\n\tint compress_type = BTRFS_COMPRESS_ZLIB;\n\tint extent_thresh = range->extent_thresh;\n\tint max_cluster = (256 * 1024) >> PAGE_CACHE_SHIFT;\n\tint cluster = max_cluster;\n\tu64 new_align = ~((u64)128 * 1024 - 1);\n\tstruct page **pages = NULL;\n\n\tif (extent_thresh == 0)\n\t\textent_thresh = 256 * 1024;\n\n\tif (range->flags & BTRFS_DEFRAG_RANGE_COMPRESS) {\n\t\tif (range->compress_type > BTRFS_COMPRESS_TYPES)\n\t\t\treturn -EINVAL;\n\t\tif (range->compress_type)\n\t\t\tcompress_type = range->compress_type;\n\t}\n\n\tif (isize == 0)\n\t\treturn 0;\n\n\t/*\n\t * if we were not given a file, allocate a readahead\n\t * context\n\t */\n\tif (!file) {\n\t\tra = kzalloc(sizeof(*ra), GFP_NOFS);\n\t\tif (!ra)\n\t\t\treturn -ENOMEM;\n\t\tfile_ra_state_init(ra, inode->i_mapping);\n\t} else {\n\t\tra = &file->f_ra;\n\t}\n\n\tpages = kmalloc(sizeof(struct page *) * max_cluster,\n\t\t\tGFP_NOFS);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out_ra;\n\t}\n\n\t/* find the last page to defrag */\n\tif (range->start + range->len > range->start) {\n\t\tlast_index = min_t(u64, isize - 1,\n\t\t\t range->start + range->len - 1) >> PAGE_CACHE_SHIFT;\n\t} else {\n\t\tlast_index = (isize - 1) >> PAGE_CACHE_SHIFT;\n\t}\n\n\tif (newer_than) {\n\t\tret = find_new_extents(root, inode, newer_than,\n\t\t\t\t       &newer_off, 64 * 1024);\n\t\tif (!ret) {\n\t\t\trange->start = newer_off;\n\t\t\t/*\n\t\t\t * we always align our defrag to help keep\n\t\t\t * the extents in the file evenly spaced\n\t\t\t */\n\t\t\ti = (newer_off & new_align) >> PAGE_CACHE_SHIFT;\n\t\t} else\n\t\t\tgoto out_ra;\n\t} else {\n\t\ti = range->start >> PAGE_CACHE_SHIFT;\n\t}\n\tif (!max_to_defrag)\n\t\tmax_to_defrag = last_index + 1;\n\n\t/*\n\t * make writeback starts from i, so the defrag range can be\n\t * written sequentially.\n\t */\n\tif (i < inode->i_mapping->writeback_index)\n\t\tinode->i_mapping->writeback_index = i;\n\n\twhile (i <= last_index && defrag_count < max_to_defrag &&\n\t       (i < (i_size_read(inode) + PAGE_CACHE_SIZE - 1) >>\n\t\tPAGE_CACHE_SHIFT)) {\n\t\t/*\n\t\t * make sure we stop running if someone unmounts\n\t\t * the FS\n\t\t */\n\t\tif (!(inode->i_sb->s_flags & MS_ACTIVE))\n\t\t\tbreak;\n\n\t\tif (!should_defrag_range(inode, (u64)i << PAGE_CACHE_SHIFT,\n\t\t\t\t\t extent_thresh, &last_len, &skip,\n\t\t\t\t\t &defrag_end, range->flags &\n\t\t\t\t\t BTRFS_DEFRAG_RANGE_COMPRESS)) {\n\t\t\tunsigned long next;\n\t\t\t/*\n\t\t\t * the should_defrag function tells us how much to skip\n\t\t\t * bump our counter by the suggested amount\n\t\t\t */\n\t\t\tnext = (skip + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\t\t\ti = max(i + 1, next);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!newer_than) {\n\t\t\tcluster = (PAGE_CACHE_ALIGN(defrag_end) >>\n\t\t\t\t   PAGE_CACHE_SHIFT) - i;\n\t\t\tcluster = min(cluster, max_cluster);\n\t\t} else {\n\t\t\tcluster = max_cluster;\n\t\t}\n\n\t\tif (range->flags & BTRFS_DEFRAG_RANGE_COMPRESS)\n\t\t\tBTRFS_I(inode)->force_compress = compress_type;\n\n\t\tif (i + cluster > ra_index) {\n\t\t\tra_index = max(i, ra_index);\n\t\t\tbtrfs_force_ra(inode->i_mapping, ra, file, ra_index,\n\t\t\t\t       cluster);\n\t\t\tra_index += max_cluster;\n\t\t}\n\n\t\tmutex_lock(&inode->i_mutex);\n\t\tret = cluster_pages_for_defrag(inode, pages, i, cluster);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&inode->i_mutex);\n\t\t\tgoto out_ra;\n\t\t}\n\n\t\tdefrag_count += ret;\n\t\tbalance_dirty_pages_ratelimited_nr(inode->i_mapping, ret);\n\t\tmutex_unlock(&inode->i_mutex);\n\n\t\tif (newer_than) {\n\t\t\tif (newer_off == (u64)-1)\n\t\t\t\tbreak;\n\n\t\t\tif (ret > 0)\n\t\t\t\ti += ret;\n\n\t\t\tnewer_off = max(newer_off + 1,\n\t\t\t\t\t(u64)i << PAGE_CACHE_SHIFT);\n\n\t\t\tret = find_new_extents(root, inode,\n\t\t\t\t\t       newer_than, &newer_off,\n\t\t\t\t\t       64 * 1024);\n\t\t\tif (!ret) {\n\t\t\t\trange->start = newer_off;\n\t\t\t\ti = (newer_off & new_align) >> PAGE_CACHE_SHIFT;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (ret > 0) {\n\t\t\t\ti += ret;\n\t\t\t\tlast_len += ret << PAGE_CACHE_SHIFT;\n\t\t\t} else {\n\t\t\t\ti++;\n\t\t\t\tlast_len = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((range->flags & BTRFS_DEFRAG_RANGE_START_IO))\n\t\tfilemap_flush(inode->i_mapping);\n\n\tif ((range->flags & BTRFS_DEFRAG_RANGE_COMPRESS)) {\n\t\t/* the filemap_flush will queue IO into the worker threads, but\n\t\t * we have to make sure the IO is actually started and that\n\t\t * ordered extents get created before we return\n\t\t */\n\t\tatomic_inc(&root->fs_info->async_submit_draining);\n\t\twhile (atomic_read(&root->fs_info->nr_async_submits) ||\n\t\t      atomic_read(&root->fs_info->async_delalloc_pages)) {\n\t\t\twait_event(root->fs_info->async_submit_wait,\n\t\t\t   (atomic_read(&root->fs_info->nr_async_submits) == 0 &&\n\t\t\t    atomic_read(&root->fs_info->async_delalloc_pages) == 0));\n\t\t}\n\t\tatomic_dec(&root->fs_info->async_submit_draining);\n\n\t\tmutex_lock(&inode->i_mutex);\n\t\tBTRFS_I(inode)->force_compress = BTRFS_COMPRESS_NONE;\n\t\tmutex_unlock(&inode->i_mutex);\n\t}\n\n\tif (range->compress_type == BTRFS_COMPRESS_LZO) {\n\t\tbtrfs_set_fs_incompat(root->fs_info, COMPRESS_LZO);\n\t}\n\n\tret = defrag_count;\n\nout_ra:\n\tif (!file)\n\t\tkfree(ra);\n\tkfree(pages);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_resize(struct file *file,\n\t\t\t\t\tvoid __user *arg)\n{\n\tu64 new_size;\n\tu64 old_size;\n\tu64 devid = 1;\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device = NULL;\n\tchar *sizestr;\n\tchar *devstr = NULL;\n\tint ret = 0;\n\tint mod = 0;\n\n\tif (root->fs_info->sb->s_flags & MS_RDONLY)\n\t\treturn -EROFS;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\treturn -EINPROGRESS;\n\t}\n\n\tmutex_lock(&root->fs_info->volume_mutex);\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tsizestr = vol_args->name;\n\tdevstr = strchr(sizestr, ':');\n\tif (devstr) {\n\t\tchar *end;\n\t\tsizestr = devstr + 1;\n\t\t*devstr = '\\0';\n\t\tdevstr = vol_args->name;\n\t\tdevid = simple_strtoull(devstr, &end, 10);\n\t\tprintk(KERN_INFO \"btrfs: resizing devid %llu\\n\",\n\t\t       (unsigned long long)devid);\n\t}\n\tdevice = btrfs_find_device(root->fs_info, devid, NULL, NULL);\n\tif (!device) {\n\t\tprintk(KERN_INFO \"btrfs: resizer unable to find device %llu\\n\",\n\t\t       (unsigned long long)devid);\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (device->fs_devices && device->fs_devices->seeding) {\n\t\tprintk(KERN_INFO \"btrfs: resizer unable to apply on \"\n\t\t       \"seeding device %llu\\n\",\n\t\t       (unsigned long long)devid);\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tif (!strcmp(sizestr, \"max\"))\n\t\tnew_size = device->bdev->bd_inode->i_size;\n\telse {\n\t\tif (sizestr[0] == '-') {\n\t\t\tmod = -1;\n\t\t\tsizestr++;\n\t\t} else if (sizestr[0] == '+') {\n\t\t\tmod = 1;\n\t\t\tsizestr++;\n\t\t}\n\t\tnew_size = memparse(sizestr, NULL);\n\t\tif (new_size == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (device->is_tgtdev_for_dev_replace) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\told_size = device->total_bytes;\n\n\tif (mod < 0) {\n\t\tif (new_size > old_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size - new_size;\n\t} else if (mod > 0) {\n\t\tnew_size = old_size + new_size;\n\t}\n\n\tif (new_size < 256 * 1024 * 1024) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (new_size > device->bdev->bd_inode->i_size) {\n\t\tret = -EFBIG;\n\t\tgoto out_free;\n\t}\n\n\tdo_div(new_size, root->sectorsize);\n\tnew_size *= root->sectorsize;\n\n\tprintk_in_rcu(KERN_INFO \"btrfs: new size for %s is %llu\\n\",\n\t\t      rcu_str_deref(device->name),\n\t\t      (unsigned long long)new_size);\n\n\tif (new_size > old_size) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = btrfs_grow_device(trans, device, new_size);\n\t\tbtrfs_commit_transaction(trans, root);\n\t} else if (new_size < old_size) {\n\t\tret = btrfs_shrink_device(device, new_size);\n\t} /* equal, nothing need to do */\n\nout_free:\n\tkfree(vol_args);\nout:\n\tmutex_unlock(&root->fs_info->volume_mutex);\n\tmnt_drop_write_file(file);\n\tatomic_set(&root->fs_info->mutually_exclusive_operation_running, 0);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create_transid(struct file *file,\n\t\t\t\tchar *name, unsigned long fd, int subvol,\n\t\t\t\tu64 *transid, bool readonly,\n\t\t\t\tstruct btrfs_qgroup_inherit **inherit)\n{\n\tint namelen;\n\tint ret = 0;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tnamelen = strlen(name);\n\tif (strchr(name, '/')) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (name[0] == '.' &&\n\t   (namelen == 1 || (name[1] == '.' && namelen == 2))) {\n\t\tret = -EEXIST;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (subvol) {\n\t\tret = btrfs_mksubvol(&file->f_path, name, namelen,\n\t\t\t\t     NULL, transid, readonly, inherit);\n\t} else {\n\t\tstruct fd src = fdget(fd);\n\t\tstruct inode *src_inode;\n\t\tif (!src.file) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_drop_write;\n\t\t}\n\n\t\tsrc_inode = src.file->f_path.dentry->d_inode;\n\t\tif (src_inode->i_sb != file->f_path.dentry->d_inode->i_sb) {\n\t\t\tprintk(KERN_INFO \"btrfs: Snapshot src from \"\n\t\t\t       \"another FS\\n\");\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tret = btrfs_mksubvol(&file->f_path, name, namelen,\n\t\t\t\t\t     BTRFS_I(src_inode)->root,\n\t\t\t\t\t     transid, readonly, inherit);\n\t\t}\n\t\tfdput(src);\n\t}\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create(struct file *file,\n\t\t\t\t\t    void __user *arg, int subvol)\n{\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tret = btrfs_ioctl_snap_create_transid(file, vol_args->name,\n\t\t\t\t\t      vol_args->fd, subvol,\n\t\t\t\t\t      NULL, false, NULL);\n\n\tkfree(vol_args);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_create_v2(struct file *file,\n\t\t\t\t\t       void __user *arg, int subvol)\n{\n\tstruct btrfs_ioctl_vol_args_v2 *vol_args;\n\tint ret;\n\tu64 transid = 0;\n\tu64 *ptr = NULL;\n\tbool readonly = false;\n\tstruct btrfs_qgroup_inherit *inherit = NULL;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\tvol_args->name[BTRFS_SUBVOL_NAME_MAX] = '\\0';\n\n\tif (vol_args->flags &\n\t    ~(BTRFS_SUBVOL_CREATE_ASYNC | BTRFS_SUBVOL_RDONLY |\n\t      BTRFS_SUBVOL_QGROUP_INHERIT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (vol_args->flags & BTRFS_SUBVOL_CREATE_ASYNC)\n\t\tptr = &transid;\n\tif (vol_args->flags & BTRFS_SUBVOL_RDONLY)\n\t\treadonly = true;\n\tif (vol_args->flags & BTRFS_SUBVOL_QGROUP_INHERIT) {\n\t\tif (vol_args->size > PAGE_CACHE_SIZE) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tinherit = memdup_user(vol_args->qgroup_inherit, vol_args->size);\n\t\tif (IS_ERR(inherit)) {\n\t\t\tret = PTR_ERR(inherit);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = btrfs_ioctl_snap_create_transid(file, vol_args->name,\n\t\t\t\t\t      vol_args->fd, subvol, ptr,\n\t\t\t\t\t      readonly, &inherit);\n\n\tif (ret == 0 && ptr &&\n\t    copy_to_user(arg +\n\t\t\t offsetof(struct btrfs_ioctl_vol_args_v2,\n\t\t\t\t  transid), ptr, sizeof(*ptr)))\n\t\tret = -EFAULT;\nout:\n\tkfree(vol_args);\n\tkfree(inherit);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_subvol_getflags(struct file *file,\n\t\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tint ret = 0;\n\tu64 flags = 0;\n\n\tif (btrfs_ino(inode) != BTRFS_FIRST_FREE_OBJECTID)\n\t\treturn -EINVAL;\n\n\tdown_read(&root->fs_info->subvol_sem);\n\tif (btrfs_root_readonly(root))\n\t\tflags |= BTRFS_SUBVOL_RDONLY;\n\tup_read(&root->fs_info->subvol_sem);\n\n\tif (copy_to_user(arg, &flags, sizeof(flags)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_subvol_setflags(struct file *file,\n\t\t\t\t\t      void __user *arg)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tu64 root_flags;\n\tu64 flags;\n\tint ret = 0;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tif (btrfs_ino(inode) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (copy_from_user(&flags, arg, sizeof(flags))) {\n\t\tret = -EFAULT;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (flags & BTRFS_SUBVOL_CREATE_ASYNC) {\n\t\tret = -EINVAL;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (flags & ~BTRFS_SUBVOL_RDONLY) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_drop_write;\n\t}\n\n\tif (!inode_owner_or_capable(inode)) {\n\t\tret = -EACCES;\n\t\tgoto out_drop_write;\n\t}\n\n\tdown_write(&root->fs_info->subvol_sem);\n\n\t/* nothing to do */\n\tif (!!(flags & BTRFS_SUBVOL_RDONLY) == btrfs_root_readonly(root))\n\t\tgoto out_drop_sem;\n\n\troot_flags = btrfs_root_flags(&root->root_item);\n\tif (flags & BTRFS_SUBVOL_RDONLY)\n\t\tbtrfs_set_root_flags(&root->root_item,\n\t\t\t\t     root_flags | BTRFS_ROOT_SUBVOL_RDONLY);\n\telse\n\t\tbtrfs_set_root_flags(&root->root_item,\n\t\t\t\t     root_flags & ~BTRFS_ROOT_SUBVOL_RDONLY);\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out_reset;\n\t}\n\n\tret = btrfs_update_root(trans, root->fs_info->tree_root,\n\t\t\t\t&root->root_key, &root->root_item);\n\n\tbtrfs_commit_transaction(trans, root);\nout_reset:\n\tif (ret)\n\t\tbtrfs_set_root_flags(&root->root_item, root_flags);\nout_drop_sem:\n\tup_write(&root->fs_info->subvol_sem);\nout_drop_write:\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\n/*\n * helper to check if the subvolume references other subvolumes\n */\nstatic noinline int may_destroy_subvol(struct btrfs_root *root)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = root->root_key.objectid;\n\tkey.type = BTRFS_ROOT_REF_KEY;\n\tkey.offset = (u64)-1;\n\n\tret = btrfs_search_slot(NULL, root->fs_info->tree_root,\n\t\t\t\t&key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tBUG_ON(ret == 0);\n\n\tret = 0;\n\tif (path->slots[0] > 0) {\n\t\tpath->slots[0]--;\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\t\tif (key.objectid == root->root_key.objectid &&\n\t\t    key.type == BTRFS_ROOT_REF_KEY)\n\t\t\tret = -ENOTEMPTY;\n\t}\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int key_in_sk(struct btrfs_key *key,\n\t\t\t      struct btrfs_ioctl_search_key *sk)\n{\n\tstruct btrfs_key test;\n\tint ret;\n\n\ttest.objectid = sk->min_objectid;\n\ttest.type = sk->min_type;\n\ttest.offset = sk->min_offset;\n\n\tret = btrfs_comp_cpu_keys(key, &test);\n\tif (ret < 0)\n\t\treturn 0;\n\n\ttest.objectid = sk->max_objectid;\n\ttest.type = sk->max_type;\n\ttest.offset = sk->max_offset;\n\n\tret = btrfs_comp_cpu_keys(key, &test);\n\tif (ret > 0)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic noinline int copy_to_sk(struct btrfs_root *root,\n\t\t\t       struct btrfs_path *path,\n\t\t\t       struct btrfs_key *key,\n\t\t\t       struct btrfs_ioctl_search_key *sk,\n\t\t\t       char *buf,\n\t\t\t       unsigned long *sk_offset,\n\t\t\t       int *num_found)\n{\n\tu64 found_transid;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_ioctl_search_header sh;\n\tunsigned long item_off;\n\tunsigned long item_len;\n\tint nritems;\n\tint i;\n\tint slot;\n\tint ret = 0;\n\n\tleaf = path->nodes[0];\n\tslot = path->slots[0];\n\tnritems = btrfs_header_nritems(leaf);\n\n\tif (btrfs_header_generation(leaf) > sk->max_transid) {\n\t\ti = nritems;\n\t\tgoto advance_key;\n\t}\n\tfound_transid = btrfs_header_generation(leaf);\n\n\tfor (i = slot; i < nritems; i++) {\n\t\titem_off = btrfs_item_ptr_offset(leaf, i);\n\t\titem_len = btrfs_item_size_nr(leaf, i);\n\n\t\tif (item_len > BTRFS_SEARCH_ARGS_BUFSIZE)\n\t\t\titem_len = 0;\n\n\t\tif (sizeof(sh) + item_len + *sk_offset >\n\t\t    BTRFS_SEARCH_ARGS_BUFSIZE) {\n\t\t\tret = 1;\n\t\t\tgoto overflow;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, key, i);\n\t\tif (!key_in_sk(key, sk))\n\t\t\tcontinue;\n\n\t\tsh.objectid = key->objectid;\n\t\tsh.offset = key->offset;\n\t\tsh.type = key->type;\n\t\tsh.len = item_len;\n\t\tsh.transid = found_transid;\n\n\t\t/* copy search result header */\n\t\tmemcpy(buf + *sk_offset, &sh, sizeof(sh));\n\t\t*sk_offset += sizeof(sh);\n\n\t\tif (item_len) {\n\t\t\tchar *p = buf + *sk_offset;\n\t\t\t/* copy the item */\n\t\t\tread_extent_buffer(leaf, p,\n\t\t\t\t\t   item_off, item_len);\n\t\t\t*sk_offset += item_len;\n\t\t}\n\t\t(*num_found)++;\n\n\t\tif (*num_found >= sk->nr_items)\n\t\t\tbreak;\n\t}\nadvance_key:\n\tret = 0;\n\tif (key->offset < (u64)-1 && key->offset < sk->max_offset)\n\t\tkey->offset++;\n\telse if (key->type < (u8)-1 && key->type < sk->max_type) {\n\t\tkey->offset = 0;\n\t\tkey->type++;\n\t} else if (key->objectid < (u64)-1 && key->objectid < sk->max_objectid) {\n\t\tkey->offset = 0;\n\t\tkey->type = 0;\n\t\tkey->objectid++;\n\t} else\n\t\tret = 1;\noverflow:\n\treturn ret;\n}\n\nstatic noinline int search_ioctl(struct inode *inode,\n\t\t\t\t struct btrfs_ioctl_search_args *args)\n{\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tstruct btrfs_key max_key;\n\tstruct btrfs_path *path;\n\tstruct btrfs_ioctl_search_key *sk = &args->key;\n\tstruct btrfs_fs_info *info = BTRFS_I(inode)->root->fs_info;\n\tint ret;\n\tint num_found = 0;\n\tunsigned long sk_offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tif (sk->tree_id == 0) {\n\t\t/* search the root of the inode that was passed */\n\t\troot = BTRFS_I(inode)->root;\n\t} else {\n\t\tkey.objectid = sk->tree_id;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\t\troot = btrfs_read_fs_root_no_name(info, &key);\n\t\tif (IS_ERR(root)) {\n\t\t\tprintk(KERN_ERR \"could not find root %llu\\n\",\n\t\t\t       sk->tree_id);\n\t\t\tbtrfs_free_path(path);\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\tkey.objectid = sk->min_objectid;\n\tkey.type = sk->min_type;\n\tkey.offset = sk->min_offset;\n\n\tmax_key.objectid = sk->max_objectid;\n\tmax_key.type = sk->max_type;\n\tmax_key.offset = sk->max_offset;\n\n\tpath->keep_locks = 1;\n\n\twhile(1) {\n\t\tret = btrfs_search_forward(root, &key, &max_key, path, 0,\n\t\t\t\t\t   sk->min_transid);\n\t\tif (ret != 0) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tgoto err;\n\t\t}\n\t\tret = copy_to_sk(root, path, &key, sk, args->buf,\n\t\t\t\t &sk_offset, &num_found);\n\t\tbtrfs_release_path(path);\n\t\tif (ret || num_found >= sk->nr_items)\n\t\t\tbreak;\n\n\t}\n\tret = 0;\nerr:\n\tsk->nr_items = num_found;\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_tree_search(struct file *file,\n\t\t\t\t\t   void __user *argp)\n{\n\t struct btrfs_ioctl_search_args *args;\n\t struct inode *inode;\n\t int ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\targs = memdup_user(argp, sizeof(*args));\n\tif (IS_ERR(args))\n\t\treturn PTR_ERR(args);\n\n\tinode = fdentry(file)->d_inode;\n\tret = search_ioctl(inode, args);\n\tif (ret == 0 && copy_to_user(argp, args, sizeof(*args)))\n\t\tret = -EFAULT;\n\tkfree(args);\n\treturn ret;\n}\n\n/*\n * Search INODE_REFs to identify path name of 'dirid' directory\n * in a 'tree_id' tree. and sets path name to 'name'.\n */\nstatic noinline int btrfs_search_path_in_tree(struct btrfs_fs_info *info,\n\t\t\t\tu64 tree_id, u64 dirid, char *name)\n{\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tchar *ptr;\n\tint ret = -1;\n\tint slot;\n\tint len;\n\tint total_len = 0;\n\tstruct btrfs_inode_ref *iref;\n\tstruct extent_buffer *l;\n\tstruct btrfs_path *path;\n\n\tif (dirid == BTRFS_FIRST_FREE_OBJECTID) {\n\t\tname[0]='\\0';\n\t\treturn 0;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tptr = &name[BTRFS_INO_LOOKUP_PATH_MAX];\n\n\tkey.objectid = tree_id;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\troot = btrfs_read_fs_root_no_name(info, &key);\n\tif (IS_ERR(root)) {\n\t\tprintk(KERN_ERR \"could not find root %llu\\n\", tree_id);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = dirid;\n\tkey.type = BTRFS_INODE_REF_KEY;\n\tkey.offset = (u64)-1;\n\n\twhile(1) {\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (ret > 0 && slot > 0)\n\t\t\tslot--;\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tif (ret > 0 && (key.objectid != dirid ||\n\t\t\t\tkey.type != BTRFS_INODE_REF_KEY)) {\n\t\t\tret = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tiref = btrfs_item_ptr(l, slot, struct btrfs_inode_ref);\n\t\tlen = btrfs_inode_ref_name_len(l, iref);\n\t\tptr -= len + 1;\n\t\ttotal_len += len + 1;\n\t\tif (ptr < name)\n\t\t\tgoto out;\n\n\t\t*(ptr + len) = '/';\n\t\tread_extent_buffer(l, ptr,(unsigned long)(iref + 1), len);\n\n\t\tif (key.offset == BTRFS_FIRST_FREE_OBJECTID)\n\t\t\tbreak;\n\n\t\tbtrfs_release_path(path);\n\t\tkey.objectid = key.offset;\n\t\tkey.offset = (u64)-1;\n\t\tdirid = key.objectid;\n\t}\n\tif (ptr < name)\n\t\tgoto out;\n\tmemmove(name, ptr, total_len);\n\tname[total_len]='\\0';\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_ino_lookup(struct file *file,\n\t\t\t\t\t   void __user *argp)\n{\n\t struct btrfs_ioctl_ino_lookup_args *args;\n\t struct inode *inode;\n\t int ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\targs = memdup_user(argp, sizeof(*args));\n\tif (IS_ERR(args))\n\t\treturn PTR_ERR(args);\n\n\tinode = fdentry(file)->d_inode;\n\n\tif (args->treeid == 0)\n\t\targs->treeid = BTRFS_I(inode)->root->root_key.objectid;\n\n\tret = btrfs_search_path_in_tree(BTRFS_I(inode)->root->fs_info,\n\t\t\t\t\targs->treeid, args->objectid,\n\t\t\t\t\targs->name);\n\n\tif (ret == 0 && copy_to_user(argp, args, sizeof(*args)))\n\t\tret = -EFAULT;\n\n\tkfree(args);\n\treturn ret;\n}\n\nstatic noinline int btrfs_ioctl_snap_destroy(struct file *file,\n\t\t\t\t\t     void __user *arg)\n{\n\tstruct dentry *parent = fdentry(file);\n\tstruct dentry *dentry;\n\tstruct inode *dir = parent->d_inode;\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *dest = NULL;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tint namelen;\n\tint ret;\n\tint err = 0;\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args))\n\t\treturn PTR_ERR(vol_args);\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tnamelen = strlen(vol_args->name);\n\tif (strchr(vol_args->name, '/') ||\n\t    strncmp(vol_args->name, \"..\", namelen) == 0) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = mnt_want_write_file(file);\n\tif (err)\n\t\tgoto out;\n\n\tmutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);\n\tdentry = lookup_one_len(vol_args->name, parent, namelen);\n\tif (IS_ERR(dentry)) {\n\t\terr = PTR_ERR(dentry);\n\t\tgoto out_unlock_dir;\n\t}\n\n\tif (!dentry->d_inode) {\n\t\terr = -ENOENT;\n\t\tgoto out_dput;\n\t}\n\n\tinode = dentry->d_inode;\n\tdest = BTRFS_I(inode)->root;\n\tif (!capable(CAP_SYS_ADMIN)){\n\t\t/*\n\t\t * Regular user.  Only allow this with a special mount\n\t\t * option, when the user has write+exec access to the\n\t\t * subvol root, and when rmdir(2) would have been\n\t\t * allowed.\n\t\t *\n\t\t * Note that this is _not_ check that the subvol is\n\t\t * empty or doesn't contain data that we wouldn't\n\t\t * otherwise be able to delete.\n\t\t *\n\t\t * Users who want to delete empty subvols should try\n\t\t * rmdir(2).\n\t\t */\n\t\terr = -EPERM;\n\t\tif (!btrfs_test_opt(root, USER_SUBVOL_RM_ALLOWED))\n\t\t\tgoto out_dput;\n\n\t\t/*\n\t\t * Do not allow deletion if the parent dir is the same\n\t\t * as the dir to be deleted.  That means the ioctl\n\t\t * must be called on the dentry referencing the root\n\t\t * of the subvol, not a random directory contained\n\t\t * within it.\n\t\t */\n\t\terr = -EINVAL;\n\t\tif (root == dest)\n\t\t\tgoto out_dput;\n\n\t\terr = inode_permission(inode, MAY_WRITE | MAY_EXEC);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\n\t\t/* check if subvolume may be deleted by a non-root user */\n\t\terr = btrfs_may_delete(dir, dentry, 1);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\tif (btrfs_ino(inode) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\terr = -EINVAL;\n\t\tgoto out_dput;\n\t}\n\n\tmutex_lock(&inode->i_mutex);\n\terr = d_invalidate(dentry);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tdown_write(&root->fs_info->subvol_sem);\n\n\terr = may_destroy_subvol(dest);\n\tif (err)\n\t\tgoto out_up_write;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\terr = PTR_ERR(trans);\n\t\tgoto out_up_write;\n\t}\n\ttrans->block_rsv = &root->fs_info->global_block_rsv;\n\n\tret = btrfs_unlink_subvol(trans, root, dir,\n\t\t\t\tdest->root_key.objectid,\n\t\t\t\tdentry->d_name.name,\n\t\t\t\tdentry->d_name.len);\n\tif (ret) {\n\t\terr = ret;\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto out_end_trans;\n\t}\n\n\tbtrfs_record_root_in_trans(trans, dest);\n\n\tmemset(&dest->root_item.drop_progress, 0,\n\t\tsizeof(dest->root_item.drop_progress));\n\tdest->root_item.drop_level = 0;\n\tbtrfs_set_root_refs(&dest->root_item, 0);\n\n\tif (!xchg(&dest->orphan_item_inserted, 1)) {\n\t\tret = btrfs_insert_orphan_item(trans,\n\t\t\t\t\troot->fs_info->tree_root,\n\t\t\t\t\tdest->root_key.objectid);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\terr = ret;\n\t\t\tgoto out_end_trans;\n\t\t}\n\t}\nout_end_trans:\n\tret = btrfs_end_transaction(trans, root);\n\tif (ret && !err)\n\t\terr = ret;\n\tinode->i_flags |= S_DEAD;\nout_up_write:\n\tup_write(&root->fs_info->subvol_sem);\nout_unlock:\n\tmutex_unlock(&inode->i_mutex);\n\tif (!err) {\n\t\tshrink_dcache_sb(root->fs_info->sb);\n\t\tbtrfs_invalidate_inodes(dest);\n\t\td_delete(dentry);\n\t}\nout_dput:\n\tdput(dentry);\nout_unlock_dir:\n\tmutex_unlock(&dir->i_mutex);\n\tmnt_drop_write_file(file);\nout:\n\tkfree(vol_args);\n\treturn err;\n}\n\nstatic int btrfs_ioctl_defrag(struct file *file, void __user *argp)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_defrag_range_args *range;\n\tint ret;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\treturn -EINPROGRESS;\n\t}\n\tret = mnt_want_write_file(file);\n\tif (ret) {\n\t\tatomic_set(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t   0);\n\t\treturn ret;\n\t}\n\n\tswitch (inode->i_mode & S_IFMT) {\n\tcase S_IFDIR:\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = btrfs_defrag_root(root, 0);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tret = btrfs_defrag_root(root->fs_info->extent_root, 0);\n\t\tbreak;\n\tcase S_IFREG:\n\t\tif (!(file->f_mode & FMODE_WRITE)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\trange = kzalloc(sizeof(*range), GFP_KERNEL);\n\t\tif (!range) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (argp) {\n\t\t\tif (copy_from_user(range, argp,\n\t\t\t\t\t   sizeof(*range))) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tkfree(range);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t/* compression requires us to start the IO */\n\t\t\tif ((range->flags & BTRFS_DEFRAG_RANGE_COMPRESS)) {\n\t\t\t\trange->flags |= BTRFS_DEFRAG_RANGE_START_IO;\n\t\t\t\trange->extent_thresh = (u32)-1;\n\t\t\t}\n\t\t} else {\n\t\t\t/* the rest are all set to zero by kzalloc */\n\t\t\trange->len = (u64)-1;\n\t\t}\n\t\tret = btrfs_defrag_file(fdentry(file)->d_inode, file,\n\t\t\t\t\trange, 0, 0);\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t\tkfree(range);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\nout:\n\tmnt_drop_write_file(file);\n\tatomic_set(&root->fs_info->mutually_exclusive_operation_running, 0);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_add_dev(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\treturn -EINPROGRESS;\n\t}\n\n\tmutex_lock(&root->fs_info->volume_mutex);\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tret = btrfs_init_new_device(root, vol_args->name);\n\n\tkfree(vol_args);\nout:\n\tmutex_unlock(&root->fs_info->volume_mutex);\n\tatomic_set(&root->fs_info->mutually_exclusive_operation_running, 0);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_rm_dev(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\tmnt_drop_write_file(file);\n\t\treturn -EINPROGRESS;\n\t}\n\n\tmutex_lock(&root->fs_info->volume_mutex);\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\tret = btrfs_rm_device(root, vol_args->name);\n\n\tkfree(vol_args);\nout:\n\tmutex_unlock(&root->fs_info->volume_mutex);\n\tmnt_drop_write_file(file);\n\tatomic_set(&root->fs_info->mutually_exclusive_operation_running, 0);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_fs_info(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_fs_info_args *fi_args;\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *next;\n\tstruct btrfs_fs_devices *fs_devices = root->fs_info->fs_devices;\n\tint ret = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tfi_args = kzalloc(sizeof(*fi_args), GFP_KERNEL);\n\tif (!fi_args)\n\t\treturn -ENOMEM;\n\n\tfi_args->num_devices = fs_devices->num_devices;\n\tmemcpy(&fi_args->fsid, root->fs_info->fsid, sizeof(fi_args->fsid));\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry_safe(device, next, &fs_devices->devices, dev_list) {\n\t\tif (device->devid > fi_args->max_id)\n\t\t\tfi_args->max_id = device->devid;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (copy_to_user(arg, fi_args, sizeof(*fi_args)))\n\t\tret = -EFAULT;\n\n\tkfree(fi_args);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_dev_info(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_info_args *di_args;\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = root->fs_info->fs_devices;\n\tint ret = 0;\n\tchar *s_uuid = NULL;\n\tchar empty_uuid[BTRFS_UUID_SIZE] = {0};\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tdi_args = memdup_user(arg, sizeof(*di_args));\n\tif (IS_ERR(di_args))\n\t\treturn PTR_ERR(di_args);\n\n\tif (memcmp(empty_uuid, di_args->uuid, BTRFS_UUID_SIZE) != 0)\n\t\ts_uuid = di_args->uuid;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(root->fs_info, di_args->devid, s_uuid, NULL);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tdi_args->devid = dev->devid;\n\tdi_args->bytes_used = dev->bytes_used;\n\tdi_args->total_bytes = dev->total_bytes;\n\tmemcpy(di_args->uuid, dev->uuid, sizeof(di_args->uuid));\n\tif (dev->name) {\n\t\tstruct rcu_string *name;\n\n\t\trcu_read_lock();\n\t\tname = rcu_dereference(dev->name);\n\t\tstrncpy(di_args->path, name->str, sizeof(di_args->path));\n\t\trcu_read_unlock();\n\t\tdi_args->path[sizeof(di_args->path) - 1] = 0;\n\t} else {\n\t\tdi_args->path[0] = '\\0';\n\t}\n\nout:\n\tif (ret == 0 && copy_to_user(arg, di_args, sizeof(*di_args)))\n\t\tret = -EFAULT;\n\n\tkfree(di_args);\n\treturn ret;\n}\n\nstatic noinline long btrfs_ioctl_clone(struct file *file, unsigned long srcfd,\n\t\t\t\t       u64 off, u64 olen, u64 destoff)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct fd src_file;\n\tstruct inode *src;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tchar *buf;\n\tstruct btrfs_key key;\n\tu32 nritems;\n\tint slot;\n\tint ret;\n\tu64 len = olen;\n\tu64 bs = root->fs_info->sb->s_blocksize;\n\n\t/*\n\t * TODO:\n\t * - split compressed inline extents.  annoying: we need to\n\t *   decompress into destination's address_space (the file offset\n\t *   may change, so source mapping won't do), then recompress (or\n\t *   otherwise reinsert) a subrange.\n\t * - allow ranges within the same file to be cloned (provided\n\t *   they don't overlap)?\n\t */\n\n\t/* the destination must be opened for writing */\n\tif (!(file->f_mode & FMODE_WRITE) || (file->f_flags & O_APPEND))\n\t\treturn -EINVAL;\n\n\tif (btrfs_root_readonly(root))\n\t\treturn -EROFS;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsrc_file = fdget(srcfd);\n\tif (!src_file.file) {\n\t\tret = -EBADF;\n\t\tgoto out_drop_write;\n\t}\n\n\tret = -EXDEV;\n\tif (src_file.file->f_path.mnt != file->f_path.mnt)\n\t\tgoto out_fput;\n\n\tsrc = src_file.file->f_dentry->d_inode;\n\n\tret = -EINVAL;\n\tif (src == inode)\n\t\tgoto out_fput;\n\n\t/* the src must be open for reading */\n\tif (!(src_file.file->f_mode & FMODE_READ))\n\t\tgoto out_fput;\n\n\t/* don't make the dst file partly checksummed */\n\tif ((BTRFS_I(src)->flags & BTRFS_INODE_NODATASUM) !=\n\t    (BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM))\n\t\tgoto out_fput;\n\n\tret = -EISDIR;\n\tif (S_ISDIR(src->i_mode) || S_ISDIR(inode->i_mode))\n\t\tgoto out_fput;\n\n\tret = -EXDEV;\n\tif (src->i_sb != inode->i_sb)\n\t\tgoto out_fput;\n\n\tret = -ENOMEM;\n\tbuf = vmalloc(btrfs_level_size(root, 0));\n\tif (!buf)\n\t\tgoto out_fput;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tvfree(buf);\n\t\tgoto out_fput;\n\t}\n\tpath->reada = 2;\n\n\tif (inode < src) {\n\t\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n\t\tmutex_lock_nested(&src->i_mutex, I_MUTEX_CHILD);\n\t} else {\n\t\tmutex_lock_nested(&src->i_mutex, I_MUTEX_PARENT);\n\t\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_CHILD);\n\t}\n\n\t/* determine range to clone */\n\tret = -EINVAL;\n\tif (off + len > src->i_size || off + len < off)\n\t\tgoto out_unlock;\n\tif (len == 0)\n\t\tolen = len = src->i_size - off;\n\t/* if we extend to eof, continue to block boundary */\n\tif (off + len == src->i_size)\n\t\tlen = ALIGN(src->i_size, bs) - off;\n\n\t/* verify the end result is block aligned */\n\tif (!IS_ALIGNED(off, bs) || !IS_ALIGNED(off + len, bs) ||\n\t    !IS_ALIGNED(destoff, bs))\n\t\tgoto out_unlock;\n\n\tif (destoff > inode->i_size) {\n\t\tret = btrfs_cont_expand(inode, inode->i_size, destoff);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* truncate page cache pages from target inode range */\n\ttruncate_inode_pages_range(&inode->i_data, destoff,\n\t\t\t\t   PAGE_CACHE_ALIGN(destoff + len) - 1);\n\n\t/* do any pending delalloc/csum calc on src, one way or\n\t   another, and lock file content */\n\twhile (1) {\n\t\tstruct btrfs_ordered_extent *ordered;\n\t\tlock_extent(&BTRFS_I(src)->io_tree, off, off + len - 1);\n\t\tordered = btrfs_lookup_first_ordered_extent(src, off + len - 1);\n\t\tif (!ordered &&\n\t\t    !test_range_bit(&BTRFS_I(src)->io_tree, off, off + len - 1,\n\t\t\t\t    EXTENT_DELALLOC, 0, NULL))\n\t\t\tbreak;\n\t\tunlock_extent(&BTRFS_I(src)->io_tree, off, off + len - 1);\n\t\tif (ordered)\n\t\t\tbtrfs_put_ordered_extent(ordered);\n\t\tbtrfs_wait_ordered_range(src, off, len);\n\t}\n\n\t/* clone data */\n\tkey.objectid = btrfs_ino(src);\n\tkey.type = BTRFS_EXTENT_DATA_KEY;\n\tkey.offset = 0;\n\n\twhile (1) {\n\t\t/*\n\t\t * note the key will change type as we walk through the\n\t\t * tree.\n\t\t */\n\t\tret = btrfs_search_slot(NULL, BTRFS_I(src)->root, &key, path,\n\t\t\t\t0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tnritems = btrfs_header_nritems(path->nodes[0]);\n\t\tif (path->slots[0] >= nritems) {\n\t\t\tret = btrfs_next_leaf(BTRFS_I(src)->root, path);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tnritems = btrfs_header_nritems(path->nodes[0]);\n\t\t}\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (btrfs_key_type(&key) > BTRFS_EXTENT_DATA_KEY ||\n\t\t    key.objectid != btrfs_ino(src))\n\t\t\tbreak;\n\n\t\tif (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tstruct btrfs_file_extent_item *extent;\n\t\t\tint type;\n\t\t\tu32 size;\n\t\t\tstruct btrfs_key new_key;\n\t\t\tu64 disko = 0, diskl = 0;\n\t\t\tu64 datao = 0, datal = 0;\n\t\t\tu8 comp;\n\t\t\tu64 endoff;\n\n\t\t\tsize = btrfs_item_size_nr(leaf, slot);\n\t\t\tread_extent_buffer(leaf, buf,\n\t\t\t\t\t   btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t   size);\n\n\t\t\textent = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\tstruct btrfs_file_extent_item);\n\t\t\tcomp = btrfs_file_extent_compression(leaf, extent);\n\t\t\ttype = btrfs_file_extent_type(leaf, extent);\n\t\t\tif (type == BTRFS_FILE_EXTENT_REG ||\n\t\t\t    type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\t\tdisko = btrfs_file_extent_disk_bytenr(leaf,\n\t\t\t\t\t\t\t\t      extent);\n\t\t\t\tdiskl = btrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t extent);\n\t\t\t\tdatao = btrfs_file_extent_offset(leaf, extent);\n\t\t\t\tdatal = btrfs_file_extent_num_bytes(leaf,\n\t\t\t\t\t\t\t\t    extent);\n\t\t\t} else if (type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\t/* take upper bound, may be compressed */\n\t\t\t\tdatal = btrfs_file_extent_ram_bytes(leaf,\n\t\t\t\t\t\t\t\t    extent);\n\t\t\t}\n\t\t\tbtrfs_release_path(path);\n\n\t\t\tif (key.offset + datal <= off ||\n\t\t\t    key.offset >= off + len - 1)\n\t\t\t\tgoto next;\n\n\t\t\tmemcpy(&new_key, &key, sizeof(new_key));\n\t\t\tnew_key.objectid = btrfs_ino(inode);\n\t\t\tif (off <= key.offset)\n\t\t\t\tnew_key.offset = key.offset + destoff - off;\n\t\t\telse\n\t\t\t\tnew_key.offset = destoff;\n\n\t\t\t/*\n\t\t\t * 1 - adjusting old extent (we may have to split it)\n\t\t\t * 1 - add new extent\n\t\t\t * 1 - inode update\n\t\t\t */\n\t\t\ttrans = btrfs_start_transaction(root, 3);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (type == BTRFS_FILE_EXTENT_REG ||\n\t\t\t    type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\t\t/*\n\t\t\t\t *    a  | --- range to clone ---|  b\n\t\t\t\t * | ------------- extent ------------- |\n\t\t\t\t */\n\n\t\t\t\t/* substract range b */\n\t\t\t\tif (key.offset + datal > off + len)\n\t\t\t\t\tdatal = off + len - key.offset;\n\n\t\t\t\t/* substract range a */\n\t\t\t\tif (off > key.offset) {\n\t\t\t\t\tdatao += off - key.offset;\n\t\t\t\t\tdatal -= off - key.offset;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t\t\t new_key.offset,\n\t\t\t\t\t\t\t new_key.offset + datal,\n\t\t\t\t\t\t\t 1);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, root,\n\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_insert_empty_item(trans, root, path,\n\t\t\t\t\t\t\t      &new_key, size);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, root,\n\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tleaf = path->nodes[0];\n\t\t\t\tslot = path->slots[0];\n\t\t\t\twrite_extent_buffer(leaf, buf,\n\t\t\t\t\t    btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t    size);\n\n\t\t\t\textent = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\tstruct btrfs_file_extent_item);\n\n\t\t\t\t/* disko == 0 means it's a hole */\n\t\t\t\tif (!disko)\n\t\t\t\t\tdatao = 0;\n\n\t\t\t\tbtrfs_set_file_extent_offset(leaf, extent,\n\t\t\t\t\t\t\t     datao);\n\t\t\t\tbtrfs_set_file_extent_num_bytes(leaf, extent,\n\t\t\t\t\t\t\t\tdatal);\n\t\t\t\tif (disko) {\n\t\t\t\t\tinode_add_bytes(inode, datal);\n\t\t\t\t\tret = btrfs_inc_extent_ref(trans, root,\n\t\t\t\t\t\t\tdisko, diskl, 0,\n\t\t\t\t\t\t\troot->root_key.objectid,\n\t\t\t\t\t\t\tbtrfs_ino(inode),\n\t\t\t\t\t\t\tnew_key.offset - datao,\n\t\t\t\t\t\t\t0);\n\t\t\t\t\tif (ret) {\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\troot,\n\t\t\t\t\t\t\t\t\tret);\n\t\t\t\t\t\tbtrfs_end_transaction(trans,\n\t\t\t\t\t\t\t\t      root);\n\t\t\t\t\t\tgoto out;\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\tu64 skip = 0;\n\t\t\t\tu64 trim = 0;\n\t\t\t\tif (off > key.offset) {\n\t\t\t\t\tskip = off - key.offset;\n\t\t\t\t\tnew_key.offset += skip;\n\t\t\t\t}\n\n\t\t\t\tif (key.offset + datal > off + len)\n\t\t\t\t\ttrim = key.offset + datal - (off + len);\n\n\t\t\t\tif (comp && (skip || trim)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tsize -= skip + trim;\n\t\t\t\tdatal -= skip + trim;\n\n\t\t\t\tret = btrfs_drop_extents(trans, root, inode,\n\t\t\t\t\t\t\t new_key.offset,\n\t\t\t\t\t\t\t new_key.offset + datal,\n\t\t\t\t\t\t\t 1);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, root,\n\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tret = btrfs_insert_empty_item(trans, root, path,\n\t\t\t\t\t\t\t      &new_key, size);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans, root,\n\t\t\t\t\t\t\t\tret);\n\t\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tif (skip) {\n\t\t\t\t\tu32 start =\n\t\t\t\t\t  btrfs_file_extent_calc_inline_size(0);\n\t\t\t\t\tmemmove(buf+start, buf+start+skip,\n\t\t\t\t\t\tdatal);\n\t\t\t\t}\n\n\t\t\t\tleaf = path->nodes[0];\n\t\t\t\tslot = path->slots[0];\n\t\t\t\twrite_extent_buffer(leaf, buf,\n\t\t\t\t\t    btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\t    size);\n\t\t\t\tinode_add_bytes(inode, datal);\n\t\t\t}\n\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t\tbtrfs_release_path(path);\n\n\t\t\tinode_inc_iversion(inode);\n\t\t\tinode->i_mtime = inode->i_ctime = CURRENT_TIME;\n\n\t\t\t/*\n\t\t\t * we round up to the block size at eof when\n\t\t\t * determining which extents to clone above,\n\t\t\t * but shouldn't round up the file size\n\t\t\t */\n\t\t\tendoff = new_key.offset + datal;\n\t\t\tif (endoff > destoff+olen)\n\t\t\t\tendoff = destoff+olen;\n\t\t\tif (endoff > inode->i_size)\n\t\t\t\tbtrfs_i_size_write(inode, endoff);\n\n\t\t\tret = btrfs_update_inode(trans, root, inode);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\t\t\tbtrfs_end_transaction(trans, root);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = btrfs_end_transaction(trans, root);\n\t\t}\nnext:\n\t\tbtrfs_release_path(path);\n\t\tkey.offset++;\n\t}\n\tret = 0;\nout:\n\tbtrfs_release_path(path);\n\tunlock_extent(&BTRFS_I(src)->io_tree, off, off + len - 1);\nout_unlock:\n\tmutex_unlock(&src->i_mutex);\n\tmutex_unlock(&inode->i_mutex);\n\tvfree(buf);\n\tbtrfs_free_path(path);\nout_fput:\n\tfdput(src_file);\nout_drop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_clone_range(struct file *file, void __user *argp)\n{\n\tstruct btrfs_ioctl_clone_range_args args;\n\n\tif (copy_from_user(&args, argp, sizeof(args)))\n\t\treturn -EFAULT;\n\treturn btrfs_ioctl_clone(file, args.src_fd, args.src_offset,\n\t\t\t\t args.src_length, args.dest_offset);\n}\n\n/*\n * there are many ways the trans_start and trans_end ioctls can lead\n * to deadlocks.  They should only be used by applications that\n * basically own the machine, and have a very in depth understanding\n * of all the possible deadlocks and enospc problems.\n */\nstatic long btrfs_ioctl_trans_start(struct file *file)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\tret = -EPERM;\n\tif (!capable(CAP_SYS_ADMIN))\n\t\tgoto out;\n\n\tret = -EINPROGRESS;\n\tif (file->private_data)\n\t\tgoto out;\n\n\tret = -EROFS;\n\tif (btrfs_root_readonly(root))\n\t\tgoto out;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\tgoto out;\n\n\tatomic_inc(&root->fs_info->open_ioctl_trans);\n\n\tret = -ENOMEM;\n\ttrans = btrfs_start_ioctl_transaction(root);\n\tif (IS_ERR(trans))\n\t\tgoto out_drop;\n\n\tfile->private_data = trans;\n\treturn 0;\n\nout_drop:\n\tatomic_dec(&root->fs_info->open_ioctl_trans);\n\tmnt_drop_write_file(file);\nout:\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_default_subvol(struct file *file, void __user *argp)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_root *new_root;\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key location;\n\tstruct btrfs_disk_key disk_key;\n\tu64 objectid = 0;\n\tu64 dir_id;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (copy_from_user(&objectid, argp, sizeof(objectid))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (!objectid)\n\t\tobjectid = root->root_key.objectid;\n\n\tlocation.objectid = objectid;\n\tlocation.type = BTRFS_ROOT_ITEM_KEY;\n\tlocation.offset = (u64)-1;\n\n\tnew_root = btrfs_read_fs_root_no_name(root->fs_info, &location);\n\tif (IS_ERR(new_root)) {\n\t\tret = PTR_ERR(new_root);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_root_refs(&new_root->root_item) == 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpath->leave_spinning = 1;\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tdir_id = btrfs_super_root_dir(root->fs_info->super_copy);\n\tdi = btrfs_lookup_dir_item(trans, root->fs_info->tree_root, path,\n\t\t\t\t   dir_id, \"default\", 7, 1);\n\tif (IS_ERR_OR_NULL(di)) {\n\t\tbtrfs_free_path(path);\n\t\tbtrfs_end_transaction(trans, root);\n\t\tprintk(KERN_ERR \"Umm, you don't have the default dir item, \"\n\t\t       \"this isn't going to work\\n\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tbtrfs_cpu_key_to_disk(&disk_key, &new_root->root_key);\n\tbtrfs_set_dir_item_key(path->nodes[0], di, &disk_key);\n\tbtrfs_mark_buffer_dirty(path->nodes[0]);\n\tbtrfs_free_path(path);\n\n\tbtrfs_set_fs_incompat(root->fs_info, DEFAULT_SUBVOL);\n\tbtrfs_end_transaction(trans, root);\nout:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nvoid btrfs_get_block_group_info(struct list_head *groups_list,\n\t\t\t\tstruct btrfs_ioctl_space_info *space)\n{\n\tstruct btrfs_block_group_cache *block_group;\n\n\tspace->total_bytes = 0;\n\tspace->used_bytes = 0;\n\tspace->flags = 0;\n\tlist_for_each_entry(block_group, groups_list, list) {\n\t\tspace->flags = block_group->flags;\n\t\tspace->total_bytes += block_group->key.offset;\n\t\tspace->used_bytes +=\n\t\t\tbtrfs_block_group_used(&block_group->item);\n\t}\n}\n\nlong btrfs_ioctl_space_info(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_space_args space_args;\n\tstruct btrfs_ioctl_space_info space;\n\tstruct btrfs_ioctl_space_info *dest;\n\tstruct btrfs_ioctl_space_info *dest_orig;\n\tstruct btrfs_ioctl_space_info __user *user_dest;\n\tstruct btrfs_space_info *info;\n\tu64 types[] = {BTRFS_BLOCK_GROUP_DATA,\n\t\t       BTRFS_BLOCK_GROUP_SYSTEM,\n\t\t       BTRFS_BLOCK_GROUP_METADATA,\n\t\t       BTRFS_BLOCK_GROUP_DATA | BTRFS_BLOCK_GROUP_METADATA};\n\tint num_types = 4;\n\tint alloc_size;\n\tint ret = 0;\n\tu64 slot_count = 0;\n\tint i, c;\n\n\tif (copy_from_user(&space_args,\n\t\t\t   (struct btrfs_ioctl_space_args __user *)arg,\n\t\t\t   sizeof(space_args)))\n\t\treturn -EFAULT;\n\n\tfor (i = 0; i < num_types; i++) {\n\t\tstruct btrfs_space_info *tmp;\n\n\t\tinfo = NULL;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(tmp, &root->fs_info->space_info,\n\t\t\t\t\tlist) {\n\t\t\tif (tmp->flags == types[i]) {\n\t\t\t\tinfo = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!info)\n\t\t\tcontinue;\n\n\t\tdown_read(&info->groups_sem);\n\t\tfor (c = 0; c < BTRFS_NR_RAID_TYPES; c++) {\n\t\t\tif (!list_empty(&info->block_groups[c]))\n\t\t\t\tslot_count++;\n\t\t}\n\t\tup_read(&info->groups_sem);\n\t}\n\n\t/* space_slots == 0 means they are asking for a count */\n\tif (space_args.space_slots == 0) {\n\t\tspace_args.total_spaces = slot_count;\n\t\tgoto out;\n\t}\n\n\tslot_count = min_t(u64, space_args.space_slots, slot_count);\n\n\talloc_size = sizeof(*dest) * slot_count;\n\n\t/* we generally have at most 6 or so space infos, one for each raid\n\t * level.  So, a whole page should be more than enough for everyone\n\t */\n\tif (alloc_size > PAGE_CACHE_SIZE)\n\t\treturn -ENOMEM;\n\n\tspace_args.total_spaces = 0;\n\tdest = kmalloc(alloc_size, GFP_NOFS);\n\tif (!dest)\n\t\treturn -ENOMEM;\n\tdest_orig = dest;\n\n\t/* now we have a buffer to copy into */\n\tfor (i = 0; i < num_types; i++) {\n\t\tstruct btrfs_space_info *tmp;\n\n\t\tif (!slot_count)\n\t\t\tbreak;\n\n\t\tinfo = NULL;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(tmp, &root->fs_info->space_info,\n\t\t\t\t\tlist) {\n\t\t\tif (tmp->flags == types[i]) {\n\t\t\t\tinfo = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!info)\n\t\t\tcontinue;\n\t\tdown_read(&info->groups_sem);\n\t\tfor (c = 0; c < BTRFS_NR_RAID_TYPES; c++) {\n\t\t\tif (!list_empty(&info->block_groups[c])) {\n\t\t\t\tbtrfs_get_block_group_info(\n\t\t\t\t\t&info->block_groups[c], &space);\n\t\t\t\tmemcpy(dest, &space, sizeof(space));\n\t\t\t\tdest++;\n\t\t\t\tspace_args.total_spaces++;\n\t\t\t\tslot_count--;\n\t\t\t}\n\t\t\tif (!slot_count)\n\t\t\t\tbreak;\n\t\t}\n\t\tup_read(&info->groups_sem);\n\t}\n\n\tuser_dest = (struct btrfs_ioctl_space_info __user *)\n\t\t(arg + sizeof(struct btrfs_ioctl_space_args));\n\n\tif (copy_to_user(user_dest, dest_orig, alloc_size))\n\t\tret = -EFAULT;\n\n\tkfree(dest_orig);\nout:\n\tif (ret == 0 && copy_to_user(arg, &space_args, sizeof(space_args)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\n/*\n * there are many ways the trans_start and trans_end ioctls can lead\n * to deadlocks.  They should only be used by applications that\n * basically own the machine, and have a very in depth understanding\n * of all the possible deadlocks and enospc problems.\n */\nlong btrfs_ioctl_trans_end(struct file *file)\n{\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_trans_handle *trans;\n\n\ttrans = file->private_data;\n\tif (!trans)\n\t\treturn -EINVAL;\n\tfile->private_data = NULL;\n\n\tbtrfs_end_transaction(trans, root);\n\n\tatomic_dec(&root->fs_info->open_ioctl_trans);\n\n\tmnt_drop_write_file(file);\n\treturn 0;\n}\n\nstatic noinline long btrfs_ioctl_start_sync(struct btrfs_root *root,\n\t\t\t\t\t    void __user *argp)\n{\n\tstruct btrfs_trans_handle *trans;\n\tu64 transid;\n\tint ret;\n\n\ttrans = btrfs_attach_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tif (PTR_ERR(trans) != -ENOENT)\n\t\t\treturn PTR_ERR(trans);\n\n\t\t/* No running transaction, don't bother */\n\t\ttransid = root->fs_info->last_trans_committed;\n\t\tgoto out;\n\t}\n\ttransid = trans->transid;\n\tret = btrfs_commit_transaction_async(trans, root, 0);\n\tif (ret) {\n\t\tbtrfs_end_transaction(trans, root);\n\t\treturn ret;\n\t}\nout:\n\tif (argp)\n\t\tif (copy_to_user(argp, &transid, sizeof(transid)))\n\t\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic noinline long btrfs_ioctl_wait_sync(struct btrfs_root *root,\n\t\t\t\t\t   void __user *argp)\n{\n\tu64 transid;\n\n\tif (argp) {\n\t\tif (copy_from_user(&transid, argp, sizeof(transid)))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\ttransid = 0;  /* current trans */\n\t}\n\treturn btrfs_wait_for_commit(root, transid);\n}\n\nstatic long btrfs_ioctl_scrub(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_scrub_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tif (!(sa->flags & BTRFS_SCRUB_READONLY)) {\n\t\tret = mnt_want_write_file(file);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = btrfs_scrub_dev(root->fs_info, sa->devid, sa->start, sa->end,\n\t\t\t      &sa->progress, sa->flags & BTRFS_SCRUB_READONLY,\n\t\t\t      0);\n\n\tif (copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tif (!(sa->flags & BTRFS_SCRUB_READONLY))\n\t\tmnt_drop_write_file(file);\nout:\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_scrub_cancel(struct btrfs_root *root, void __user *arg)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn btrfs_scrub_cancel(root->fs_info);\n}\n\nstatic long btrfs_ioctl_scrub_progress(struct btrfs_root *root,\n\t\t\t\t       void __user *arg)\n{\n\tstruct btrfs_ioctl_scrub_args *sa;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tret = btrfs_scrub_progress(root, sa->devid, &sa->progress);\n\n\tif (copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_get_dev_stats(struct btrfs_root *root,\n\t\t\t\t      void __user *arg)\n{\n\tstruct btrfs_ioctl_get_dev_stats *sa;\n\tint ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa))\n\t\treturn PTR_ERR(sa);\n\n\tif ((sa->flags & BTRFS_DEV_STATS_RESET) && !capable(CAP_SYS_ADMIN)) {\n\t\tkfree(sa);\n\t\treturn -EPERM;\n\t}\n\n\tret = btrfs_get_dev_stats(root, sa);\n\n\tif (copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tkfree(sa);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_dev_replace(struct btrfs_root *root, void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_replace_args *p;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tp = memdup_user(arg, sizeof(*p));\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\tswitch (p->cmd) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_START:\n\t\tif (atomic_xchg(\n\t\t\t&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\t\tret = -EINPROGRESS;\n\t\t} else {\n\t\t\tret = btrfs_dev_replace_start(root, p);\n\t\t\tatomic_set(\n\t\t\t &root->fs_info->mutually_exclusive_operation_running,\n\t\t\t 0);\n\t\t}\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_STATUS:\n\t\tbtrfs_dev_replace_status(root->fs_info, p);\n\t\tret = 0;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_CMD_CANCEL:\n\t\tret = btrfs_dev_replace_cancel(root->fs_info, p);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (copy_to_user(arg, p, sizeof(*p)))\n\t\tret = -EFAULT;\n\n\tkfree(p);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_ino_to_path(struct btrfs_root *root, void __user *arg)\n{\n\tint ret = 0;\n\tint i;\n\tu64 rel_ptr;\n\tint size;\n\tstruct btrfs_ioctl_ino_path_args *ipa = NULL;\n\tstruct inode_fs_paths *ipath = NULL;\n\tstruct btrfs_path *path;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tipa = memdup_user(arg, sizeof(*ipa));\n\tif (IS_ERR(ipa)) {\n\t\tret = PTR_ERR(ipa);\n\t\tipa = NULL;\n\t\tgoto out;\n\t}\n\n\tsize = min_t(u32, ipa->size, 4096);\n\tipath = init_ipath(size, root, path);\n\tif (IS_ERR(ipath)) {\n\t\tret = PTR_ERR(ipath);\n\t\tipath = NULL;\n\t\tgoto out;\n\t}\n\n\tret = paths_from_inode(ipa->inum, ipath);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tfor (i = 0; i < ipath->fspath->elem_cnt; ++i) {\n\t\trel_ptr = ipath->fspath->val[i] -\n\t\t\t  (u64)(unsigned long)ipath->fspath->val;\n\t\tipath->fspath->val[i] = rel_ptr;\n\t}\n\n\tret = copy_to_user((void *)(unsigned long)ipa->fspath,\n\t\t\t   (void *)(unsigned long)ipath->fspath, size);\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tfree_ipath(ipath);\n\tkfree(ipa);\n\n\treturn ret;\n}\n\nstatic int build_ino_list(u64 inum, u64 offset, u64 root, void *ctx)\n{\n\tstruct btrfs_data_container *inodes = ctx;\n\tconst size_t c = 3 * sizeof(u64);\n\n\tif (inodes->bytes_left >= c) {\n\t\tinodes->bytes_left -= c;\n\t\tinodes->val[inodes->elem_cnt] = inum;\n\t\tinodes->val[inodes->elem_cnt + 1] = offset;\n\t\tinodes->val[inodes->elem_cnt + 2] = root;\n\t\tinodes->elem_cnt += 3;\n\t} else {\n\t\tinodes->bytes_missing += c - inodes->bytes_left;\n\t\tinodes->bytes_left = 0;\n\t\tinodes->elem_missed += 3;\n\t}\n\n\treturn 0;\n}\n\nstatic long btrfs_ioctl_logical_to_ino(struct btrfs_root *root,\n\t\t\t\t\tvoid __user *arg)\n{\n\tint ret = 0;\n\tint size;\n\tstruct btrfs_ioctl_logical_ino_args *loi;\n\tstruct btrfs_data_container *inodes = NULL;\n\tstruct btrfs_path *path = NULL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tloi = memdup_user(arg, sizeof(*loi));\n\tif (IS_ERR(loi)) {\n\t\tret = PTR_ERR(loi);\n\t\tloi = NULL;\n\t\tgoto out;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tsize = min_t(u32, loi->size, 64 * 1024);\n\tinodes = init_data_container(size);\n\tif (IS_ERR(inodes)) {\n\t\tret = PTR_ERR(inodes);\n\t\tinodes = NULL;\n\t\tgoto out;\n\t}\n\n\tret = iterate_inodes_from_logical(loi->logical, root->fs_info, path,\n\t\t\t\t\t  build_ino_list, inodes);\n\tif (ret == -EINVAL)\n\t\tret = -ENOENT;\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = copy_to_user((void *)(unsigned long)loi->inodes,\n\t\t\t   (void *)(unsigned long)inodes, size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tbtrfs_free_path(path);\n\tvfree(inodes);\n\tkfree(loi);\n\n\treturn ret;\n}\n\nvoid update_ioctl_balance_args(struct btrfs_fs_info *fs_info, int lock,\n\t\t\t       struct btrfs_ioctl_balance_args *bargs)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbargs->flags = bctl->flags;\n\n\tif (atomic_read(&fs_info->balance_running))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_RUNNING;\n\tif (atomic_read(&fs_info->balance_pause_req))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_PAUSE_REQ;\n\tif (atomic_read(&fs_info->balance_cancel_req))\n\t\tbargs->state |= BTRFS_BALANCE_STATE_CANCEL_REQ;\n\n\tmemcpy(&bargs->data, &bctl->data, sizeof(bargs->data));\n\tmemcpy(&bargs->meta, &bctl->meta, sizeof(bargs->meta));\n\tmemcpy(&bargs->sys, &bctl->sys, sizeof(bargs->sys));\n\n\tif (lock) {\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tmemcpy(&bargs->stat, &bctl->stat, sizeof(bargs->stat));\n\t\tspin_unlock(&fs_info->balance_lock);\n\t} else {\n\t\tmemcpy(&bargs->stat, &bctl->stat, sizeof(bargs->stat));\n\t}\n}\n\nstatic long btrfs_ioctl_balance(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_ioctl_balance_args *bargs;\n\tstruct btrfs_balance_control *bctl;\n\tint ret;\n\tint need_to_clear_lock = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&fs_info->volume_mutex);\n\tmutex_lock(&fs_info->balance_mutex);\n\n\tif (arg) {\n\t\tbargs = memdup_user(arg, sizeof(*bargs));\n\t\tif (IS_ERR(bargs)) {\n\t\t\tret = PTR_ERR(bargs);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (bargs->flags & BTRFS_BALANCE_RESUME) {\n\t\t\tif (!fs_info->balance_ctl) {\n\t\t\t\tret = -ENOTCONN;\n\t\t\t\tgoto out_bargs;\n\t\t\t}\n\n\t\t\tbctl = fs_info->balance_ctl;\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tgoto do_balance;\n\t\t}\n\t} else {\n\t\tbargs = NULL;\n\t}\n\n\tif (atomic_xchg(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t1)) {\n\t\tpr_info(\"btrfs: dev add/delete/balance/replace/resize operation in progress\\n\");\n\t\tret = -EINPROGRESS;\n\t\tgoto out_bargs;\n\t}\n\tneed_to_clear_lock = 1;\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_NOFS);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out_bargs;\n\t}\n\n\tbctl->fs_info = fs_info;\n\tif (arg) {\n\t\tmemcpy(&bctl->data, &bargs->data, sizeof(bctl->data));\n\t\tmemcpy(&bctl->meta, &bargs->meta, sizeof(bctl->meta));\n\t\tmemcpy(&bctl->sys, &bargs->sys, sizeof(bctl->sys));\n\n\t\tbctl->flags = bargs->flags;\n\t} else {\n\t\t/* balance everything - no filters */\n\t\tbctl->flags |= BTRFS_BALANCE_TYPE_MASK;\n\t}\n\ndo_balance:\n\tret = btrfs_balance(bctl, bargs);\n\t/*\n\t * bctl is freed in __cancel_balance or in free_fs_info if\n\t * restriper was paused all the way until unmount\n\t */\n\tif (arg) {\n\t\tif (copy_to_user(arg, bargs, sizeof(*bargs)))\n\t\t\tret = -EFAULT;\n\t}\n\nout_bargs:\n\tkfree(bargs);\nout:\n\tif (need_to_clear_lock)\n\t\tatomic_set(&root->fs_info->mutually_exclusive_operation_running,\n\t\t\t   0);\n\tmutex_unlock(&fs_info->balance_mutex);\n\tmutex_unlock(&fs_info->volume_mutex);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_balance_ctl(struct btrfs_root *root, int cmd)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tswitch (cmd) {\n\tcase BTRFS_BALANCE_CTL_PAUSE:\n\t\treturn btrfs_pause_balance(root->fs_info);\n\tcase BTRFS_BALANCE_CTL_CANCEL:\n\t\treturn btrfs_cancel_balance(root->fs_info);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic long btrfs_ioctl_balance_progress(struct btrfs_root *root,\n\t\t\t\t\t void __user *arg)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_ioctl_balance_args *bargs;\n\tint ret = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tret = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\tbargs = kzalloc(sizeof(*bargs), GFP_NOFS);\n\tif (!bargs) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tupdate_ioctl_balance_args(fs_info, 1, bargs);\n\n\tif (copy_to_user(arg, bargs, sizeof(*bargs)))\n\t\tret = -EFAULT;\n\n\tkfree(bargs);\nout:\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_quota_ctl(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_quota_ctl_args *sa;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\tif (sa->cmd != BTRFS_QUOTA_CTL_RESCAN) {\n\t\ttrans = btrfs_start_transaction(root, 2);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (sa->cmd) {\n\tcase BTRFS_QUOTA_CTL_ENABLE:\n\t\tret = btrfs_quota_enable(trans, root->fs_info);\n\t\tbreak;\n\tcase BTRFS_QUOTA_CTL_DISABLE:\n\t\tret = btrfs_quota_disable(trans, root->fs_info);\n\t\tbreak;\n\tcase BTRFS_QUOTA_CTL_RESCAN:\n\t\tret = btrfs_quota_rescan(root->fs_info);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (copy_to_user(arg, sa, sizeof(*sa)))\n\t\tret = -EFAULT;\n\n\tif (trans) {\n\t\terr = btrfs_commit_transaction(trans, root);\n\t\tif (err && !ret)\n\t\t\tret = err;\n\t}\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_assign(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_qgroup_assign_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\t/* FIXME: check if the IDs really exist */\n\tif (sa->assign) {\n\t\tret = btrfs_add_qgroup_relation(trans, root->fs_info,\n\t\t\t\t\t\tsa->src, sa->dst);\n\t} else {\n\t\tret = btrfs_del_qgroup_relation(trans, root->fs_info,\n\t\t\t\t\t\tsa->src, sa->dst);\n\t}\n\n\terr = btrfs_end_transaction(trans, root);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_create(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_qgroup_create_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\t/* FIXME: check if the IDs really exist */\n\tif (sa->create) {\n\t\tret = btrfs_create_qgroup(trans, root->fs_info, sa->qgroupid,\n\t\t\t\t\t  NULL);\n\t} else {\n\t\tret = btrfs_remove_qgroup(trans, root->fs_info, sa->qgroupid);\n\t}\n\n\terr = btrfs_end_transaction(trans, root);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_qgroup_limit(struct file *file, void __user *arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tstruct btrfs_ioctl_qgroup_limit_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\tu64 qgroupid;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\n\tqgroupid = sa->qgroupid;\n\tif (!qgroupid) {\n\t\t/* take the current subvol as qgroup */\n\t\tqgroupid = root->root_key.objectid;\n\t}\n\n\t/* FIXME: check if the IDs really exist */\n\tret = btrfs_limit_qgroup(trans, root->fs_info, qgroupid, &sa->lim);\n\n\terr = btrfs_end_transaction(trans, root);\n\tif (err && !ret)\n\t\tret = err;\n\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nstatic long btrfs_ioctl_set_received_subvol(struct file *file,\n\t\t\t\t\t    void __user *arg)\n{\n\tstruct btrfs_ioctl_received_subvol_args *sa = NULL;\n\tstruct inode *inode = fdentry(file)->d_inode;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_root_item *root_item = &root->root_item;\n\tstruct btrfs_trans_handle *trans;\n\tstruct timespec ct = CURRENT_TIME;\n\tint ret = 0;\n\n\tret = mnt_want_write_file(file);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tdown_write(&root->fs_info->subvol_sem);\n\n\tif (btrfs_ino(inode) != BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (btrfs_root_readonly(root)) {\n\t\tret = -EROFS;\n\t\tgoto out;\n\t}\n\n\tif (!inode_owner_or_capable(inode)) {\n\t\tret = -EACCES;\n\t\tgoto out;\n\t}\n\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tsa = NULL;\n\t\tgoto out;\n\t}\n\n\ttrans = btrfs_start_transaction(root, 1);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\ttrans = NULL;\n\t\tgoto out;\n\t}\n\n\tsa->rtransid = trans->transid;\n\tsa->rtime.sec = ct.tv_sec;\n\tsa->rtime.nsec = ct.tv_nsec;\n\n\tmemcpy(root_item->received_uuid, sa->uuid, BTRFS_UUID_SIZE);\n\tbtrfs_set_root_stransid(root_item, sa->stransid);\n\tbtrfs_set_root_rtransid(root_item, sa->rtransid);\n\troot_item->stime.sec = cpu_to_le64(sa->stime.sec);\n\troot_item->stime.nsec = cpu_to_le32(sa->stime.nsec);\n\troot_item->rtime.sec = cpu_to_le64(sa->rtime.sec);\n\troot_item->rtime.nsec = cpu_to_le32(sa->rtime.nsec);\n\n\tret = btrfs_update_root(trans, root->fs_info->tree_root,\n\t\t\t\t&root->root_key, &root->root_item);\n\tif (ret < 0) {\n\t\tbtrfs_end_transaction(trans, root);\n\t\ttrans = NULL;\n\t\tgoto out;\n\t} else {\n\t\tret = btrfs_commit_transaction(trans, root);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = copy_to_user(arg, sa, sizeof(*sa));\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(sa);\n\tup_write(&root->fs_info->subvol_sem);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}\n\nlong btrfs_ioctl(struct file *file, unsigned int\n\t\tcmd, unsigned long arg)\n{\n\tstruct btrfs_root *root = BTRFS_I(fdentry(file)->d_inode)->root;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase FS_IOC_GETFLAGS:\n\t\treturn btrfs_ioctl_getflags(file, argp);\n\tcase FS_IOC_SETFLAGS:\n\t\treturn btrfs_ioctl_setflags(file, argp);\n\tcase FS_IOC_GETVERSION:\n\t\treturn btrfs_ioctl_getversion(file, argp);\n\tcase FITRIM:\n\t\treturn btrfs_ioctl_fitrim(file, argp);\n\tcase BTRFS_IOC_SNAP_CREATE:\n\t\treturn btrfs_ioctl_snap_create(file, argp, 0);\n\tcase BTRFS_IOC_SNAP_CREATE_V2:\n\t\treturn btrfs_ioctl_snap_create_v2(file, argp, 0);\n\tcase BTRFS_IOC_SUBVOL_CREATE:\n\t\treturn btrfs_ioctl_snap_create(file, argp, 1);\n\tcase BTRFS_IOC_SUBVOL_CREATE_V2:\n\t\treturn btrfs_ioctl_snap_create_v2(file, argp, 1);\n\tcase BTRFS_IOC_SNAP_DESTROY:\n\t\treturn btrfs_ioctl_snap_destroy(file, argp);\n\tcase BTRFS_IOC_SUBVOL_GETFLAGS:\n\t\treturn btrfs_ioctl_subvol_getflags(file, argp);\n\tcase BTRFS_IOC_SUBVOL_SETFLAGS:\n\t\treturn btrfs_ioctl_subvol_setflags(file, argp);\n\tcase BTRFS_IOC_DEFAULT_SUBVOL:\n\t\treturn btrfs_ioctl_default_subvol(file, argp);\n\tcase BTRFS_IOC_DEFRAG:\n\t\treturn btrfs_ioctl_defrag(file, NULL);\n\tcase BTRFS_IOC_DEFRAG_RANGE:\n\t\treturn btrfs_ioctl_defrag(file, argp);\n\tcase BTRFS_IOC_RESIZE:\n\t\treturn btrfs_ioctl_resize(file, argp);\n\tcase BTRFS_IOC_ADD_DEV:\n\t\treturn btrfs_ioctl_add_dev(root, argp);\n\tcase BTRFS_IOC_RM_DEV:\n\t\treturn btrfs_ioctl_rm_dev(file, argp);\n\tcase BTRFS_IOC_FS_INFO:\n\t\treturn btrfs_ioctl_fs_info(root, argp);\n\tcase BTRFS_IOC_DEV_INFO:\n\t\treturn btrfs_ioctl_dev_info(root, argp);\n\tcase BTRFS_IOC_BALANCE:\n\t\treturn btrfs_ioctl_balance(file, NULL);\n\tcase BTRFS_IOC_CLONE:\n\t\treturn btrfs_ioctl_clone(file, arg, 0, 0, 0);\n\tcase BTRFS_IOC_CLONE_RANGE:\n\t\treturn btrfs_ioctl_clone_range(file, argp);\n\tcase BTRFS_IOC_TRANS_START:\n\t\treturn btrfs_ioctl_trans_start(file);\n\tcase BTRFS_IOC_TRANS_END:\n\t\treturn btrfs_ioctl_trans_end(file);\n\tcase BTRFS_IOC_TREE_SEARCH:\n\t\treturn btrfs_ioctl_tree_search(file, argp);\n\tcase BTRFS_IOC_INO_LOOKUP:\n\t\treturn btrfs_ioctl_ino_lookup(file, argp);\n\tcase BTRFS_IOC_INO_PATHS:\n\t\treturn btrfs_ioctl_ino_to_path(root, argp);\n\tcase BTRFS_IOC_LOGICAL_INO:\n\t\treturn btrfs_ioctl_logical_to_ino(root, argp);\n\tcase BTRFS_IOC_SPACE_INFO:\n\t\treturn btrfs_ioctl_space_info(root, argp);\n\tcase BTRFS_IOC_SYNC:\n\t\tbtrfs_sync_fs(file->f_dentry->d_sb, 1);\n\t\treturn 0;\n\tcase BTRFS_IOC_START_SYNC:\n\t\treturn btrfs_ioctl_start_sync(root, argp);\n\tcase BTRFS_IOC_WAIT_SYNC:\n\t\treturn btrfs_ioctl_wait_sync(root, argp);\n\tcase BTRFS_IOC_SCRUB:\n\t\treturn btrfs_ioctl_scrub(file, argp);\n\tcase BTRFS_IOC_SCRUB_CANCEL:\n\t\treturn btrfs_ioctl_scrub_cancel(root, argp);\n\tcase BTRFS_IOC_SCRUB_PROGRESS:\n\t\treturn btrfs_ioctl_scrub_progress(root, argp);\n\tcase BTRFS_IOC_BALANCE_V2:\n\t\treturn btrfs_ioctl_balance(file, argp);\n\tcase BTRFS_IOC_BALANCE_CTL:\n\t\treturn btrfs_ioctl_balance_ctl(root, arg);\n\tcase BTRFS_IOC_BALANCE_PROGRESS:\n\t\treturn btrfs_ioctl_balance_progress(root, argp);\n\tcase BTRFS_IOC_SET_RECEIVED_SUBVOL:\n\t\treturn btrfs_ioctl_set_received_subvol(file, argp);\n\tcase BTRFS_IOC_SEND:\n\t\treturn btrfs_ioctl_send(file, argp);\n\tcase BTRFS_IOC_GET_DEV_STATS:\n\t\treturn btrfs_ioctl_get_dev_stats(root, argp);\n\tcase BTRFS_IOC_QUOTA_CTL:\n\t\treturn btrfs_ioctl_quota_ctl(file, argp);\n\tcase BTRFS_IOC_QGROUP_ASSIGN:\n\t\treturn btrfs_ioctl_qgroup_assign(file, argp);\n\tcase BTRFS_IOC_QGROUP_CREATE:\n\t\treturn btrfs_ioctl_qgroup_create(file, argp);\n\tcase BTRFS_IOC_QGROUP_LIMIT:\n\t\treturn btrfs_ioctl_qgroup_limit(file, argp);\n\tcase BTRFS_IOC_DEV_REPLACE:\n\t\treturn btrfs_ioctl_dev_replace(root, argp);\n\t}\n\n\treturn -ENOTTY;\n}\n", "/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public\n * License v2 as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n * You should have received a copy of the GNU General Public\n * License along with this program; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 021110-1307, USA.\n */\n\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/writeback.h>\n#include <linux/pagemap.h>\n#include <linux/blkdev.h>\n#include <linux/uuid.h>\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"locking.h\"\n#include \"tree-log.h\"\n#include \"inode-map.h\"\n#include \"volumes.h\"\n#include \"dev-replace.h\"\n\n#define BTRFS_ROOT_TRANS_TAG 0\n\nvoid put_transaction(struct btrfs_transaction *transaction)\n{\n\tWARN_ON(atomic_read(&transaction->use_count) == 0);\n\tif (atomic_dec_and_test(&transaction->use_count)) {\n\t\tBUG_ON(!list_empty(&transaction->list));\n\t\tWARN_ON(transaction->delayed_refs.root.rb_node);\n\t\tmemset(transaction, 0, sizeof(*transaction));\n\t\tkmem_cache_free(btrfs_transaction_cachep, transaction);\n\t}\n}\n\nstatic noinline void switch_commit_root(struct btrfs_root *root)\n{\n\tfree_extent_buffer(root->commit_root);\n\troot->commit_root = btrfs_root_node(root);\n}\n\n/*\n * either allocate a new transaction or hop into the existing one\n */\nstatic noinline int join_transaction(struct btrfs_root *root, int type)\n{\n\tstruct btrfs_transaction *cur_trans;\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\n\tspin_lock(&fs_info->trans_lock);\nloop:\n\t/* The file system has been taken offline. No new transactions. */\n\tif (fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\treturn -EROFS;\n\t}\n\n\tif (fs_info->trans_no_join) {\n\t\t/* \n\t\t * If we are JOIN_NOLOCK we're already committing a current\n\t\t * transaction, we just need a handle to deal with something\n\t\t * when committing the transaction, such as inode cache and\n\t\t * space cache. It is a special case.\n\t\t */\n\t\tif (type != TRANS_JOIN_NOLOCK) {\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\n\tcur_trans = fs_info->running_transaction;\n\tif (cur_trans) {\n\t\tif (cur_trans->aborted) {\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\treturn cur_trans->aborted;\n\t\t}\n\t\tatomic_inc(&cur_trans->use_count);\n\t\tatomic_inc(&cur_trans->num_writers);\n\t\tcur_trans->num_joined++;\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\treturn 0;\n\t}\n\tspin_unlock(&fs_info->trans_lock);\n\n\t/*\n\t * If we are ATTACH, we just want to catch the current transaction,\n\t * and commit it. If there is no transaction, just return ENOENT.\n\t */\n\tif (type == TRANS_ATTACH)\n\t\treturn -ENOENT;\n\n\tcur_trans = kmem_cache_alloc(btrfs_transaction_cachep, GFP_NOFS);\n\tif (!cur_trans)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&fs_info->trans_lock);\n\tif (fs_info->running_transaction) {\n\t\t/*\n\t\t * someone started a transaction after we unlocked.  Make sure\n\t\t * to redo the trans_no_join checks above\n\t\t */\n\t\tkmem_cache_free(btrfs_transaction_cachep, cur_trans);\n\t\tcur_trans = fs_info->running_transaction;\n\t\tgoto loop;\n\t} else if (fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\tkmem_cache_free(btrfs_transaction_cachep, cur_trans);\n\t\treturn -EROFS;\n\t}\n\n\tatomic_set(&cur_trans->num_writers, 1);\n\tcur_trans->num_joined = 0;\n\tinit_waitqueue_head(&cur_trans->writer_wait);\n\tinit_waitqueue_head(&cur_trans->commit_wait);\n\tcur_trans->in_commit = 0;\n\tcur_trans->blocked = 0;\n\t/*\n\t * One for this trans handle, one so it will live on until we\n\t * commit the transaction.\n\t */\n\tatomic_set(&cur_trans->use_count, 2);\n\tcur_trans->commit_done = 0;\n\tcur_trans->start_time = get_seconds();\n\n\tcur_trans->delayed_refs.root = RB_ROOT;\n\tcur_trans->delayed_refs.num_entries = 0;\n\tcur_trans->delayed_refs.num_heads_ready = 0;\n\tcur_trans->delayed_refs.num_heads = 0;\n\tcur_trans->delayed_refs.flushing = 0;\n\tcur_trans->delayed_refs.run_delayed_start = 0;\n\n\t/*\n\t * although the tree mod log is per file system and not per transaction,\n\t * the log must never go across transaction boundaries.\n\t */\n\tsmp_mb();\n\tif (!list_empty(&fs_info->tree_mod_seq_list))\n\t\tWARN(1, KERN_ERR \"btrfs: tree_mod_seq_list not empty when \"\n\t\t\t\"creating a fresh transaction\\n\");\n\tif (!RB_EMPTY_ROOT(&fs_info->tree_mod_log))\n\t\tWARN(1, KERN_ERR \"btrfs: tree_mod_log rb tree not empty when \"\n\t\t\t\"creating a fresh transaction\\n\");\n\tatomic_set(&fs_info->tree_mod_seq, 0);\n\n\tspin_lock_init(&cur_trans->commit_lock);\n\tspin_lock_init(&cur_trans->delayed_refs.lock);\n\n\tINIT_LIST_HEAD(&cur_trans->pending_snapshots);\n\tlist_add_tail(&cur_trans->list, &fs_info->trans_list);\n\textent_io_tree_init(&cur_trans->dirty_pages,\n\t\t\t     fs_info->btree_inode->i_mapping);\n\tfs_info->generation++;\n\tcur_trans->transid = fs_info->generation;\n\tfs_info->running_transaction = cur_trans;\n\tcur_trans->aborted = 0;\n\tspin_unlock(&fs_info->trans_lock);\n\n\treturn 0;\n}\n\n/*\n * this does all the record keeping required to make sure that a reference\n * counted root is properly recorded in a given transaction.  This is required\n * to make sure the old root from before we joined the transaction is deleted\n * when the transaction commits\n */\nstatic int record_root_in_trans(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root)\n{\n\tif (root->ref_cows && root->last_trans < trans->transid) {\n\t\tWARN_ON(root == root->fs_info->extent_root);\n\t\tWARN_ON(root->commit_root != root->node);\n\n\t\t/*\n\t\t * see below for in_trans_setup usage rules\n\t\t * we have the reloc mutex held now, so there\n\t\t * is only one writer in this function\n\t\t */\n\t\troot->in_trans_setup = 1;\n\n\t\t/* make sure readers find in_trans_setup before\n\t\t * they find our root->last_trans update\n\t\t */\n\t\tsmp_wmb();\n\n\t\tspin_lock(&root->fs_info->fs_roots_radix_lock);\n\t\tif (root->last_trans == trans->transid) {\n\t\t\tspin_unlock(&root->fs_info->fs_roots_radix_lock);\n\t\t\treturn 0;\n\t\t}\n\t\tradix_tree_tag_set(&root->fs_info->fs_roots_radix,\n\t\t\t   (unsigned long)root->root_key.objectid,\n\t\t\t   BTRFS_ROOT_TRANS_TAG);\n\t\tspin_unlock(&root->fs_info->fs_roots_radix_lock);\n\t\troot->last_trans = trans->transid;\n\n\t\t/* this is pretty tricky.  We don't want to\n\t\t * take the relocation lock in btrfs_record_root_in_trans\n\t\t * unless we're really doing the first setup for this root in\n\t\t * this transaction.\n\t\t *\n\t\t * Normally we'd use root->last_trans as a flag to decide\n\t\t * if we want to take the expensive mutex.\n\t\t *\n\t\t * But, we have to set root->last_trans before we\n\t\t * init the relocation root, otherwise, we trip over warnings\n\t\t * in ctree.c.  The solution used here is to flag ourselves\n\t\t * with root->in_trans_setup.  When this is 1, we're still\n\t\t * fixing up the reloc trees and everyone must wait.\n\t\t *\n\t\t * When this is zero, they can trust root->last_trans and fly\n\t\t * through btrfs_record_root_in_trans without having to take the\n\t\t * lock.  smp_wmb() makes sure that all the writes above are\n\t\t * done before we pop in the zero below\n\t\t */\n\t\tbtrfs_init_reloc_root(trans, root);\n\t\tsmp_wmb();\n\t\troot->in_trans_setup = 0;\n\t}\n\treturn 0;\n}\n\n\nint btrfs_record_root_in_trans(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root)\n{\n\tif (!root->ref_cows)\n\t\treturn 0;\n\n\t/*\n\t * see record_root_in_trans for comments about in_trans_setup usage\n\t * and barriers\n\t */\n\tsmp_rmb();\n\tif (root->last_trans == trans->transid &&\n\t    !root->in_trans_setup)\n\t\treturn 0;\n\n\tmutex_lock(&root->fs_info->reloc_mutex);\n\trecord_root_in_trans(trans, root);\n\tmutex_unlock(&root->fs_info->reloc_mutex);\n\n\treturn 0;\n}\n\n/* wait for commit against the current transaction to become unblocked\n * when this is done, it is safe to start a new transaction, but the current\n * transaction might not be fully on disk.\n */\nstatic void wait_current_trans(struct btrfs_root *root)\n{\n\tstruct btrfs_transaction *cur_trans;\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tcur_trans = root->fs_info->running_transaction;\n\tif (cur_trans && cur_trans->blocked) {\n\t\tatomic_inc(&cur_trans->use_count);\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\n\t\twait_event(root->fs_info->transaction_wait,\n\t\t\t   !cur_trans->blocked);\n\t\tput_transaction(cur_trans);\n\t} else {\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t}\n}\n\nstatic int may_wait_transaction(struct btrfs_root *root, int type)\n{\n\tif (root->fs_info->log_root_recovering)\n\t\treturn 0;\n\n\tif (type == TRANS_USERSPACE)\n\t\treturn 1;\n\n\tif (type == TRANS_START &&\n\t    !atomic_read(&root->fs_info->open_ioctl_trans))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic struct btrfs_trans_handle *\nstart_transaction(struct btrfs_root *root, u64 num_items, int type,\n\t\t  enum btrfs_reserve_flush_enum flush)\n{\n\tstruct btrfs_trans_handle *h;\n\tstruct btrfs_transaction *cur_trans;\n\tu64 num_bytes = 0;\n\tint ret;\n\tu64 qgroup_reserved = 0;\n\n\tif (root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR)\n\t\treturn ERR_PTR(-EROFS);\n\n\tif (current->journal_info) {\n\t\tWARN_ON(type != TRANS_JOIN && type != TRANS_JOIN_NOLOCK);\n\t\th = current->journal_info;\n\t\th->use_count++;\n\t\tWARN_ON(h->use_count > 2);\n\t\th->orig_rsv = h->block_rsv;\n\t\th->block_rsv = NULL;\n\t\tgoto got_it;\n\t}\n\n\t/*\n\t * Do the reservation before we join the transaction so we can do all\n\t * the appropriate flushing if need be.\n\t */\n\tif (num_items > 0 && root != root->fs_info->chunk_root) {\n\t\tif (root->fs_info->quota_enabled &&\n\t\t    is_fstree(root->root_key.objectid)) {\n\t\t\tqgroup_reserved = num_items * root->leafsize;\n\t\t\tret = btrfs_qgroup_reserve(root, qgroup_reserved);\n\t\t\tif (ret)\n\t\t\t\treturn ERR_PTR(ret);\n\t\t}\n\n\t\tnum_bytes = btrfs_calc_trans_metadata_size(root, num_items);\n\t\tret = btrfs_block_rsv_add(root,\n\t\t\t\t\t  &root->fs_info->trans_block_rsv,\n\t\t\t\t\t  num_bytes, flush);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\nagain:\n\th = kmem_cache_alloc(btrfs_trans_handle_cachep, GFP_NOFS);\n\tif (!h)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * If we are JOIN_NOLOCK we're already committing a transaction and\n\t * waiting on this guy, so we don't need to do the sb_start_intwrite\n\t * because we're already holding a ref.  We need this because we could\n\t * have raced in and did an fsync() on a file which can kick a commit\n\t * and then we deadlock with somebody doing a freeze.\n\t *\n\t * If we are ATTACH, it means we just want to catch the current\n\t * transaction and commit it, so we needn't do sb_start_intwrite(). \n\t */\n\tif (type < TRANS_JOIN_NOLOCK)\n\t\tsb_start_intwrite(root->fs_info->sb);\n\n\tif (may_wait_transaction(root, type))\n\t\twait_current_trans(root);\n\n\tdo {\n\t\tret = join_transaction(root, type);\n\t\tif (ret == -EBUSY)\n\t\t\twait_current_trans(root);\n\t} while (ret == -EBUSY);\n\n\tif (ret < 0) {\n\t\t/* We must get the transaction if we are JOIN_NOLOCK. */\n\t\tBUG_ON(type == TRANS_JOIN_NOLOCK);\n\n\t\tif (type < TRANS_JOIN_NOLOCK)\n\t\t\tsb_end_intwrite(root->fs_info->sb);\n\t\tkmem_cache_free(btrfs_trans_handle_cachep, h);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tcur_trans = root->fs_info->running_transaction;\n\n\th->transid = cur_trans->transid;\n\th->transaction = cur_trans;\n\th->blocks_used = 0;\n\th->bytes_reserved = 0;\n\th->root = root;\n\th->delayed_ref_updates = 0;\n\th->use_count = 1;\n\th->adding_csums = 0;\n\th->block_rsv = NULL;\n\th->orig_rsv = NULL;\n\th->aborted = 0;\n\th->qgroup_reserved = qgroup_reserved;\n\th->delayed_ref_elem.seq = 0;\n\th->type = type;\n\tINIT_LIST_HEAD(&h->qgroup_ref_list);\n\tINIT_LIST_HEAD(&h->new_bgs);\n\n\tsmp_mb();\n\tif (cur_trans->blocked && may_wait_transaction(root, type)) {\n\t\tbtrfs_commit_transaction(h, root);\n\t\tgoto again;\n\t}\n\n\tif (num_bytes) {\n\t\ttrace_btrfs_space_reservation(root->fs_info, \"transaction\",\n\t\t\t\t\t      h->transid, num_bytes, 1);\n\t\th->block_rsv = &root->fs_info->trans_block_rsv;\n\t\th->bytes_reserved = num_bytes;\n\t}\n\ngot_it:\n\tbtrfs_record_root_in_trans(h, root);\n\n\tif (!current->journal_info && type != TRANS_USERSPACE)\n\t\tcurrent->journal_info = h;\n\treturn h;\n}\n\nstruct btrfs_trans_handle *btrfs_start_transaction(struct btrfs_root *root,\n\t\t\t\t\t\t   int num_items)\n{\n\treturn start_transaction(root, num_items, TRANS_START,\n\t\t\t\t BTRFS_RESERVE_FLUSH_ALL);\n}\n\nstruct btrfs_trans_handle *btrfs_start_transaction_lflush(\n\t\t\t\t\tstruct btrfs_root *root, int num_items)\n{\n\treturn start_transaction(root, num_items, TRANS_START,\n\t\t\t\t BTRFS_RESERVE_FLUSH_LIMIT);\n}\n\nstruct btrfs_trans_handle *btrfs_join_transaction(struct btrfs_root *root)\n{\n\treturn start_transaction(root, 0, TRANS_JOIN, 0);\n}\n\nstruct btrfs_trans_handle *btrfs_join_transaction_nolock(struct btrfs_root *root)\n{\n\treturn start_transaction(root, 0, TRANS_JOIN_NOLOCK, 0);\n}\n\nstruct btrfs_trans_handle *btrfs_start_ioctl_transaction(struct btrfs_root *root)\n{\n\treturn start_transaction(root, 0, TRANS_USERSPACE, 0);\n}\n\nstruct btrfs_trans_handle *btrfs_attach_transaction(struct btrfs_root *root)\n{\n\treturn start_transaction(root, 0, TRANS_ATTACH, 0);\n}\n\n/* wait for a transaction commit to be fully complete */\nstatic noinline void wait_for_commit(struct btrfs_root *root,\n\t\t\t\t    struct btrfs_transaction *commit)\n{\n\twait_event(commit->commit_wait, commit->commit_done);\n}\n\nint btrfs_wait_for_commit(struct btrfs_root *root, u64 transid)\n{\n\tstruct btrfs_transaction *cur_trans = NULL, *t;\n\tint ret = 0;\n\n\tif (transid) {\n\t\tif (transid <= root->fs_info->last_trans_committed)\n\t\t\tgoto out;\n\n\t\tret = -EINVAL;\n\t\t/* find specified transaction */\n\t\tspin_lock(&root->fs_info->trans_lock);\n\t\tlist_for_each_entry(t, &root->fs_info->trans_list, list) {\n\t\t\tif (t->transid == transid) {\n\t\t\t\tcur_trans = t;\n\t\t\t\tatomic_inc(&cur_trans->use_count);\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (t->transid > transid) {\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t\t/* The specified transaction doesn't exist */\n\t\tif (!cur_trans)\n\t\t\tgoto out;\n\t} else {\n\t\t/* find newest transaction that is committing | committed */\n\t\tspin_lock(&root->fs_info->trans_lock);\n\t\tlist_for_each_entry_reverse(t, &root->fs_info->trans_list,\n\t\t\t\t\t    list) {\n\t\t\tif (t->in_commit) {\n\t\t\t\tif (t->commit_done)\n\t\t\t\t\tbreak;\n\t\t\t\tcur_trans = t;\n\t\t\t\tatomic_inc(&cur_trans->use_count);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t\tif (!cur_trans)\n\t\t\tgoto out;  /* nothing committing|committed */\n\t}\n\n\twait_for_commit(root, cur_trans);\n\tput_transaction(cur_trans);\nout:\n\treturn ret;\n}\n\nvoid btrfs_throttle(struct btrfs_root *root)\n{\n\tif (!atomic_read(&root->fs_info->open_ioctl_trans))\n\t\twait_current_trans(root);\n}\n\nstatic int should_end_transaction(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct btrfs_root *root)\n{\n\tint ret;\n\n\tret = btrfs_block_rsv_check(root, &root->fs_info->global_block_rsv, 5);\n\treturn ret ? 1 : 0;\n}\n\nint btrfs_should_end_transaction(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root)\n{\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tint updates;\n\tint err;\n\n\tsmp_mb();\n\tif (cur_trans->blocked || cur_trans->delayed_refs.flushing)\n\t\treturn 1;\n\n\tupdates = trans->delayed_ref_updates;\n\ttrans->delayed_ref_updates = 0;\n\tif (updates) {\n\t\terr = btrfs_run_delayed_refs(trans, root, updates);\n\t\tif (err) /* Error code will also eval true */\n\t\t\treturn err;\n\t}\n\n\treturn should_end_transaction(trans, root);\n}\n\nstatic int __btrfs_end_transaction(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root, int throttle)\n{\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tstruct btrfs_fs_info *info = root->fs_info;\n\tint count = 0;\n\tint lock = (trans->type != TRANS_JOIN_NOLOCK);\n\tint err = 0;\n\n\tif (--trans->use_count) {\n\t\ttrans->block_rsv = trans->orig_rsv;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * do the qgroup accounting as early as possible\n\t */\n\terr = btrfs_delayed_refs_qgroup_accounting(trans, info);\n\n\tbtrfs_trans_release_metadata(trans, root);\n\ttrans->block_rsv = NULL;\n\t/*\n\t * the same root has to be passed to start_transaction and\n\t * end_transaction. Subvolume quota depends on this.\n\t */\n\tWARN_ON(trans->root != root);\n\n\tif (trans->qgroup_reserved) {\n\t\tbtrfs_qgroup_free(root, trans->qgroup_reserved);\n\t\ttrans->qgroup_reserved = 0;\n\t}\n\n\tif (!list_empty(&trans->new_bgs))\n\t\tbtrfs_create_pending_block_groups(trans, root);\n\n\twhile (count < 2) {\n\t\tunsigned long cur = trans->delayed_ref_updates;\n\t\ttrans->delayed_ref_updates = 0;\n\t\tif (cur &&\n\t\t    trans->transaction->delayed_refs.num_heads_ready > 64) {\n\t\t\ttrans->delayed_ref_updates = 0;\n\t\t\tbtrfs_run_delayed_refs(trans, root, cur);\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tcount++;\n\t}\n\tbtrfs_trans_release_metadata(trans, root);\n\ttrans->block_rsv = NULL;\n\n\tif (!list_empty(&trans->new_bgs))\n\t\tbtrfs_create_pending_block_groups(trans, root);\n\n\tif (lock && !atomic_read(&root->fs_info->open_ioctl_trans) &&\n\t    should_end_transaction(trans, root)) {\n\t\ttrans->transaction->blocked = 1;\n\t\tsmp_wmb();\n\t}\n\n\tif (lock && cur_trans->blocked && !cur_trans->in_commit) {\n\t\tif (throttle) {\n\t\t\t/*\n\t\t\t * We may race with somebody else here so end up having\n\t\t\t * to call end_transaction on ourselves again, so inc\n\t\t\t * our use_count.\n\t\t\t */\n\t\t\ttrans->use_count++;\n\t\t\treturn btrfs_commit_transaction(trans, root);\n\t\t} else {\n\t\t\twake_up_process(info->transaction_kthread);\n\t\t}\n\t}\n\n\tif (trans->type < TRANS_JOIN_NOLOCK)\n\t\tsb_end_intwrite(root->fs_info->sb);\n\n\tWARN_ON(cur_trans != info->running_transaction);\n\tWARN_ON(atomic_read(&cur_trans->num_writers) < 1);\n\tatomic_dec(&cur_trans->num_writers);\n\n\tsmp_mb();\n\tif (waitqueue_active(&cur_trans->writer_wait))\n\t\twake_up(&cur_trans->writer_wait);\n\tput_transaction(cur_trans);\n\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\n\tif (throttle)\n\t\tbtrfs_run_delayed_iputs(root);\n\n\tif (trans->aborted ||\n\t    root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {\n\t\terr = -EIO;\n\t}\n\tassert_qgroups_uptodate(trans);\n\n\tmemset(trans, 0, sizeof(*trans));\n\tkmem_cache_free(btrfs_trans_handle_cachep, trans);\n\treturn err;\n}\n\nint btrfs_end_transaction(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_root *root)\n{\n\tint ret;\n\n\tret = __btrfs_end_transaction(trans, root, 0);\n\tif (ret)\n\t\treturn ret;\n\treturn 0;\n}\n\nint btrfs_end_transaction_throttle(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root)\n{\n\tint ret;\n\n\tret = __btrfs_end_transaction(trans, root, 1);\n\tif (ret)\n\t\treturn ret;\n\treturn 0;\n}\n\nint btrfs_end_transaction_dmeta(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root)\n{\n\treturn __btrfs_end_transaction(trans, root, 1);\n}\n\n/*\n * when btree blocks are allocated, they have some corresponding bits set for\n * them in one of two extent_io trees.  This is used to make sure all of\n * those extents are sent to disk but does not wait on them\n */\nint btrfs_write_marked_extents(struct btrfs_root *root,\n\t\t\t       struct extent_io_tree *dirty_pages, int mark)\n{\n\tint err = 0;\n\tint werr = 0;\n\tstruct address_space *mapping = root->fs_info->btree_inode->i_mapping;\n\tstruct extent_state *cached_state = NULL;\n\tu64 start = 0;\n\tu64 end;\n\n\twhile (!find_first_extent_bit(dirty_pages, start, &start, &end,\n\t\t\t\t      mark, &cached_state)) {\n\t\tconvert_extent_bit(dirty_pages, start, end, EXTENT_NEED_WAIT,\n\t\t\t\t   mark, &cached_state, GFP_NOFS);\n\t\tcached_state = NULL;\n\t\terr = filemap_fdatawrite_range(mapping, start, end);\n\t\tif (err)\n\t\t\twerr = err;\n\t\tcond_resched();\n\t\tstart = end + 1;\n\t}\n\tif (err)\n\t\twerr = err;\n\treturn werr;\n}\n\n/*\n * when btree blocks are allocated, they have some corresponding bits set for\n * them in one of two extent_io trees.  This is used to make sure all of\n * those extents are on disk for transaction or log commit.  We wait\n * on all the pages and clear them from the dirty pages state tree\n */\nint btrfs_wait_marked_extents(struct btrfs_root *root,\n\t\t\t      struct extent_io_tree *dirty_pages, int mark)\n{\n\tint err = 0;\n\tint werr = 0;\n\tstruct address_space *mapping = root->fs_info->btree_inode->i_mapping;\n\tstruct extent_state *cached_state = NULL;\n\tu64 start = 0;\n\tu64 end;\n\n\twhile (!find_first_extent_bit(dirty_pages, start, &start, &end,\n\t\t\t\t      EXTENT_NEED_WAIT, &cached_state)) {\n\t\tclear_extent_bit(dirty_pages, start, end, EXTENT_NEED_WAIT,\n\t\t\t\t 0, 0, &cached_state, GFP_NOFS);\n\t\terr = filemap_fdatawait_range(mapping, start, end);\n\t\tif (err)\n\t\t\twerr = err;\n\t\tcond_resched();\n\t\tstart = end + 1;\n\t}\n\tif (err)\n\t\twerr = err;\n\treturn werr;\n}\n\n/*\n * when btree blocks are allocated, they have some corresponding bits set for\n * them in one of two extent_io trees.  This is used to make sure all of\n * those extents are on disk for transaction or log commit\n */\nint btrfs_write_and_wait_marked_extents(struct btrfs_root *root,\n\t\t\t\tstruct extent_io_tree *dirty_pages, int mark)\n{\n\tint ret;\n\tint ret2;\n\n\tret = btrfs_write_marked_extents(root, dirty_pages, mark);\n\tret2 = btrfs_wait_marked_extents(root, dirty_pages, mark);\n\n\tif (ret)\n\t\treturn ret;\n\tif (ret2)\n\t\treturn ret2;\n\treturn 0;\n}\n\nint btrfs_write_and_wait_transaction(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_root *root)\n{\n\tif (!trans || !trans->transaction) {\n\t\tstruct inode *btree_inode;\n\t\tbtree_inode = root->fs_info->btree_inode;\n\t\treturn filemap_write_and_wait(btree_inode->i_mapping);\n\t}\n\treturn btrfs_write_and_wait_marked_extents(root,\n\t\t\t\t\t   &trans->transaction->dirty_pages,\n\t\t\t\t\t   EXTENT_DIRTY);\n}\n\n/*\n * this is used to update the root pointer in the tree of tree roots.\n *\n * But, in the case of the extent allocation tree, updating the root\n * pointer may allocate blocks which may change the root of the extent\n * allocation tree.\n *\n * So, this loops and repeats and makes sure the cowonly root didn't\n * change while the root pointer was being updated in the metadata.\n */\nstatic int update_cowonly_root(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root)\n{\n\tint ret;\n\tu64 old_root_bytenr;\n\tu64 old_root_used;\n\tstruct btrfs_root *tree_root = root->fs_info->tree_root;\n\n\told_root_used = btrfs_root_used(&root->root_item);\n\tbtrfs_write_dirty_block_groups(trans, root);\n\n\twhile (1) {\n\t\told_root_bytenr = btrfs_root_bytenr(&root->root_item);\n\t\tif (old_root_bytenr == root->node->start &&\n\t\t    old_root_used == btrfs_root_used(&root->root_item))\n\t\t\tbreak;\n\n\t\tbtrfs_set_root_node(&root->root_item, root->node);\n\t\tret = btrfs_update_root(trans, tree_root,\n\t\t\t\t\t&root->root_key,\n\t\t\t\t\t&root->root_item);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\told_root_used = btrfs_root_used(&root->root_item);\n\t\tret = btrfs_write_dirty_block_groups(trans, root);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (root != root->fs_info->extent_root)\n\t\tswitch_commit_root(root);\n\n\treturn 0;\n}\n\n/*\n * update all the cowonly tree roots on disk\n *\n * The error handling in this function may not be obvious. Any of the\n * failures will cause the file system to go offline. We still need\n * to clean up the delayed refs.\n */\nstatic noinline int commit_cowonly_roots(struct btrfs_trans_handle *trans,\n\t\t\t\t\t struct btrfs_root *root)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct list_head *next;\n\tstruct extent_buffer *eb;\n\tint ret;\n\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tif (ret)\n\t\treturn ret;\n\n\teb = btrfs_lock_root_node(fs_info->tree_root);\n\tret = btrfs_cow_block(trans, fs_info->tree_root, eb, NULL,\n\t\t\t      0, &eb);\n\tbtrfs_tree_unlock(eb);\n\tfree_extent_buffer(eb);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tif (ret)\n\t\treturn ret;\n\n\tret = btrfs_run_dev_stats(trans, root->fs_info);\n\tWARN_ON(ret);\n\tret = btrfs_run_dev_replace(trans, root->fs_info);\n\tWARN_ON(ret);\n\n\tret = btrfs_run_qgroups(trans, root->fs_info);\n\tBUG_ON(ret);\n\n\t/* run_qgroups might have added some more refs */\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tBUG_ON(ret);\n\n\twhile (!list_empty(&fs_info->dirty_cowonly_roots)) {\n\t\tnext = fs_info->dirty_cowonly_roots.next;\n\t\tlist_del_init(next);\n\t\troot = list_entry(next, struct btrfs_root, dirty_list);\n\n\t\tret = update_cowonly_root(trans, root);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tdown_write(&fs_info->extent_commit_sem);\n\tswitch_commit_root(fs_info->extent_root);\n\tup_write(&fs_info->extent_commit_sem);\n\n\tbtrfs_after_dev_replace_commit(fs_info);\n\n\treturn 0;\n}\n\n/*\n * dead roots are old snapshots that need to be deleted.  This allocates\n * a dirty root struct and adds it into the list of dead roots that need to\n * be deleted\n */\nint btrfs_add_dead_root(struct btrfs_root *root)\n{\n\tspin_lock(&root->fs_info->trans_lock);\n\tlist_add(&root->root_list, &root->fs_info->dead_roots);\n\tspin_unlock(&root->fs_info->trans_lock);\n\treturn 0;\n}\n\n/*\n * update all the cowonly tree roots on disk\n */\nstatic noinline int commit_fs_roots(struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_root *root)\n{\n\tstruct btrfs_root *gang[8];\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tint i;\n\tint ret;\n\tint err = 0;\n\n\tspin_lock(&fs_info->fs_roots_radix_lock);\n\twhile (1) {\n\t\tret = radix_tree_gang_lookup_tag(&fs_info->fs_roots_radix,\n\t\t\t\t\t\t (void **)gang, 0,\n\t\t\t\t\t\t ARRAY_SIZE(gang),\n\t\t\t\t\t\t BTRFS_ROOT_TRANS_TAG);\n\t\tif (ret == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < ret; i++) {\n\t\t\troot = gang[i];\n\t\t\tradix_tree_tag_clear(&fs_info->fs_roots_radix,\n\t\t\t\t\t(unsigned long)root->root_key.objectid,\n\t\t\t\t\tBTRFS_ROOT_TRANS_TAG);\n\t\t\tspin_unlock(&fs_info->fs_roots_radix_lock);\n\n\t\t\tbtrfs_free_log(trans, root);\n\t\t\tbtrfs_update_reloc_root(trans, root);\n\t\t\tbtrfs_orphan_commit_root(trans, root);\n\n\t\t\tbtrfs_save_ino_cache(root, trans);\n\n\t\t\t/* see comments in should_cow_block() */\n\t\t\troot->force_cow = 0;\n\t\t\tsmp_wmb();\n\n\t\t\tif (root->commit_root != root->node) {\n\t\t\t\tmutex_lock(&root->fs_commit_mutex);\n\t\t\t\tswitch_commit_root(root);\n\t\t\t\tbtrfs_unpin_free_ino(root);\n\t\t\t\tmutex_unlock(&root->fs_commit_mutex);\n\n\t\t\t\tbtrfs_set_root_node(&root->root_item,\n\t\t\t\t\t\t    root->node);\n\t\t\t}\n\n\t\t\terr = btrfs_update_root(trans, fs_info->tree_root,\n\t\t\t\t\t\t&root->root_key,\n\t\t\t\t\t\t&root->root_item);\n\t\t\tspin_lock(&fs_info->fs_roots_radix_lock);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&fs_info->fs_roots_radix_lock);\n\treturn err;\n}\n\n/*\n * defrag a given btree.  If cacheonly == 1, this won't read from the disk,\n * otherwise every leaf in the btree is read and defragged.\n */\nint btrfs_defrag_root(struct btrfs_root *root, int cacheonly)\n{\n\tstruct btrfs_fs_info *info = root->fs_info;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\tif (xchg(&root->defrag_running, 1))\n\t\treturn 0;\n\n\twhile (1) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\tret = btrfs_defrag_leaves(trans, root, cacheonly);\n\n\t\tbtrfs_end_transaction(trans, root);\n\t\tbtrfs_btree_balance_dirty(info->tree_root);\n\t\tcond_resched();\n\n\t\tif (btrfs_fs_closing(root->fs_info) || ret != -EAGAIN)\n\t\t\tbreak;\n\t}\n\troot->defrag_running = 0;\n\treturn ret;\n}\n\n/*\n * new snapshots need to be created at a very specific time in the\n * transaction commit.  This does the actual creation\n */\nstatic noinline int create_pending_snapshot(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_fs_info *fs_info,\n\t\t\t\t   struct btrfs_pending_snapshot *pending)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_root_item *new_root_item;\n\tstruct btrfs_root *tree_root = fs_info->tree_root;\n\tstruct btrfs_root *root = pending->root;\n\tstruct btrfs_root *parent_root;\n\tstruct btrfs_block_rsv *rsv;\n\tstruct inode *parent_inode;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dir_item *dir_item;\n\tstruct dentry *parent;\n\tstruct dentry *dentry;\n\tstruct extent_buffer *tmp;\n\tstruct extent_buffer *old;\n\tstruct timespec cur_time = CURRENT_TIME;\n\tint ret;\n\tu64 to_reserve = 0;\n\tu64 index = 0;\n\tu64 objectid;\n\tu64 root_flags;\n\tuuid_le new_uuid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = pending->error = -ENOMEM;\n\t\tgoto path_alloc_fail;\n\t}\n\n\tnew_root_item = kmalloc(sizeof(*new_root_item), GFP_NOFS);\n\tif (!new_root_item) {\n\t\tret = pending->error = -ENOMEM;\n\t\tgoto root_item_alloc_fail;\n\t}\n\n\tret = btrfs_find_free_objectid(tree_root, &objectid);\n\tif (ret) {\n\t\tpending->error = ret;\n\t\tgoto no_free_objectid;\n\t}\n\n\tbtrfs_reloc_pre_snapshot(trans, pending, &to_reserve);\n\n\tif (to_reserve > 0) {\n\t\tret = btrfs_block_rsv_add(root, &pending->block_rsv,\n\t\t\t\t\t  to_reserve,\n\t\t\t\t\t  BTRFS_RESERVE_NO_FLUSH);\n\t\tif (ret) {\n\t\t\tpending->error = ret;\n\t\t\tgoto no_free_objectid;\n\t\t}\n\t}\n\n\tret = btrfs_qgroup_inherit(trans, fs_info, root->root_key.objectid,\n\t\t\t\t   objectid, pending->inherit);\n\tif (ret) {\n\t\tpending->error = ret;\n\t\tgoto no_free_objectid;\n\t}\n\n\tkey.objectid = objectid;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\n\trsv = trans->block_rsv;\n\ttrans->block_rsv = &pending->block_rsv;\n\n\tdentry = pending->dentry;\n\tparent = dget_parent(dentry);\n\tparent_inode = parent->d_inode;\n\tparent_root = BTRFS_I(parent_inode)->root;\n\trecord_root_in_trans(trans, parent_root);\n\n\t/*\n\t * insert the directory item\n\t */\n\tret = btrfs_set_inode_index(parent_inode, &index);\n\tBUG_ON(ret); /* -ENOMEM */\n\n\t/* check if there is a file/dir which has the same name. */\n\tdir_item = btrfs_lookup_dir_item(NULL, parent_root, path,\n\t\t\t\t\t btrfs_ino(parent_inode),\n\t\t\t\t\t dentry->d_name.name,\n\t\t\t\t\t dentry->d_name.len, 0);\n\tif (dir_item != NULL && !IS_ERR(dir_item)) {\n\t\tpending->error = -EEXIST;\n\t\tgoto fail;\n\t} else if (IS_ERR(dir_item)) {\n\t\tret = PTR_ERR(dir_item);\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\tbtrfs_release_path(path);\n\n\t/*\n\t * pull in the delayed directory update\n\t * and the delayed inode item\n\t * otherwise we corrupt the FS during\n\t * snapshot\n\t */\n\tret = btrfs_run_delayed_items(trans, root);\n\tif (ret) {\t/* Transaction aborted */\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\trecord_root_in_trans(trans, root);\n\tbtrfs_set_root_last_snapshot(&root->root_item, trans->transid);\n\tmemcpy(new_root_item, &root->root_item, sizeof(*new_root_item));\n\tbtrfs_check_and_init_root_item(new_root_item);\n\n\troot_flags = btrfs_root_flags(new_root_item);\n\tif (pending->readonly)\n\t\troot_flags |= BTRFS_ROOT_SUBVOL_RDONLY;\n\telse\n\t\troot_flags &= ~BTRFS_ROOT_SUBVOL_RDONLY;\n\tbtrfs_set_root_flags(new_root_item, root_flags);\n\n\tbtrfs_set_root_generation_v2(new_root_item,\n\t\t\ttrans->transid);\n\tuuid_le_gen(&new_uuid);\n\tmemcpy(new_root_item->uuid, new_uuid.b, BTRFS_UUID_SIZE);\n\tmemcpy(new_root_item->parent_uuid, root->root_item.uuid,\n\t\t\tBTRFS_UUID_SIZE);\n\tnew_root_item->otime.sec = cpu_to_le64(cur_time.tv_sec);\n\tnew_root_item->otime.nsec = cpu_to_le32(cur_time.tv_nsec);\n\tbtrfs_set_root_otransid(new_root_item, trans->transid);\n\tmemset(&new_root_item->stime, 0, sizeof(new_root_item->stime));\n\tmemset(&new_root_item->rtime, 0, sizeof(new_root_item->rtime));\n\tbtrfs_set_root_stransid(new_root_item, 0);\n\tbtrfs_set_root_rtransid(new_root_item, 0);\n\n\told = btrfs_lock_root_node(root);\n\tret = btrfs_cow_block(trans, root, old, NULL, 0, &old);\n\tif (ret) {\n\t\tbtrfs_tree_unlock(old);\n\t\tfree_extent_buffer(old);\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_set_lock_blocking(old);\n\n\tret = btrfs_copy_root(trans, root, old, &tmp, objectid);\n\t/* clean up in any case */\n\tbtrfs_tree_unlock(old);\n\tfree_extent_buffer(old);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\t/* see comments in should_cow_block() */\n\troot->force_cow = 1;\n\tsmp_wmb();\n\n\tbtrfs_set_root_node(new_root_item, tmp);\n\t/* record when the snapshot was created in key.offset */\n\tkey.offset = trans->transid;\n\tret = btrfs_insert_root(trans, tree_root, &key, new_root_item);\n\tbtrfs_tree_unlock(tmp);\n\tfree_extent_buffer(tmp);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * insert root back/forward references\n\t */\n\tret = btrfs_add_root_ref(trans, tree_root, objectid,\n\t\t\t\t parent_root->root_key.objectid,\n\t\t\t\t btrfs_ino(parent_inode), index,\n\t\t\t\t dentry->d_name.name, dentry->d_name.len);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tkey.offset = (u64)-1;\n\tpending->snap = btrfs_read_fs_root_no_name(root->fs_info, &key);\n\tif (IS_ERR(pending->snap)) {\n\t\tret = PTR_ERR(pending->snap);\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_reloc_post_snapshot(trans, pending);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tret = btrfs_insert_dir_item(trans, parent_root,\n\t\t\t\t    dentry->d_name.name, dentry->d_name.len,\n\t\t\t\t    parent_inode, &key,\n\t\t\t\t    BTRFS_FT_DIR, index);\n\t/* We have check then name at the beginning, so it is impossible. */\n\tBUG_ON(ret == -EEXIST || ret == -EOVERFLOW);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto fail;\n\t}\n\n\tbtrfs_i_size_write(parent_inode, parent_inode->i_size +\n\t\t\t\t\t dentry->d_name.len * 2);\n\tparent_inode->i_mtime = parent_inode->i_ctime = CURRENT_TIME;\n\tret = btrfs_update_inode_fallback(trans, parent_root, parent_inode);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, root, ret);\nfail:\n\tdput(parent);\n\ttrans->block_rsv = rsv;\nno_free_objectid:\n\tkfree(new_root_item);\nroot_item_alloc_fail:\n\tbtrfs_free_path(path);\npath_alloc_fail:\n\tbtrfs_block_rsv_release(root, &pending->block_rsv, (u64)-1);\n\treturn ret;\n}\n\n/*\n * create all the snapshots we've scheduled for creation\n */\nstatic noinline int create_pending_snapshots(struct btrfs_trans_handle *trans,\n\t\t\t\t\t     struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_pending_snapshot *pending;\n\tstruct list_head *head = &trans->transaction->pending_snapshots;\n\n\tlist_for_each_entry(pending, head, list)\n\t\tcreate_pending_snapshot(trans, fs_info, pending);\n\treturn 0;\n}\n\nstatic void update_super_roots(struct btrfs_root *root)\n{\n\tstruct btrfs_root_item *root_item;\n\tstruct btrfs_super_block *super;\n\n\tsuper = root->fs_info->super_copy;\n\n\troot_item = &root->fs_info->chunk_root->root_item;\n\tsuper->chunk_root = root_item->bytenr;\n\tsuper->chunk_root_generation = root_item->generation;\n\tsuper->chunk_root_level = root_item->level;\n\n\troot_item = &root->fs_info->tree_root->root_item;\n\tsuper->root = root_item->bytenr;\n\tsuper->generation = root_item->generation;\n\tsuper->root_level = root_item->level;\n\tif (btrfs_test_opt(root, SPACE_CACHE))\n\t\tsuper->cache_generation = root_item->generation;\n}\n\nint btrfs_transaction_in_commit(struct btrfs_fs_info *info)\n{\n\tint ret = 0;\n\tspin_lock(&info->trans_lock);\n\tif (info->running_transaction)\n\t\tret = info->running_transaction->in_commit;\n\tspin_unlock(&info->trans_lock);\n\treturn ret;\n}\n\nint btrfs_transaction_blocked(struct btrfs_fs_info *info)\n{\n\tint ret = 0;\n\tspin_lock(&info->trans_lock);\n\tif (info->running_transaction)\n\t\tret = info->running_transaction->blocked;\n\tspin_unlock(&info->trans_lock);\n\treturn ret;\n}\n\n/*\n * wait for the current transaction commit to start and block subsequent\n * transaction joins\n */\nstatic void wait_current_trans_commit_start(struct btrfs_root *root,\n\t\t\t\t\t    struct btrfs_transaction *trans)\n{\n\twait_event(root->fs_info->transaction_blocked_wait, trans->in_commit);\n}\n\n/*\n * wait for the current transaction to start and then become unblocked.\n * caller holds ref.\n */\nstatic void wait_current_trans_commit_start_and_unblock(struct btrfs_root *root,\n\t\t\t\t\t struct btrfs_transaction *trans)\n{\n\twait_event(root->fs_info->transaction_wait,\n\t\t   trans->commit_done || (trans->in_commit && !trans->blocked));\n}\n\n/*\n * commit transactions asynchronously. once btrfs_commit_transaction_async\n * returns, any subsequent transaction will not be allowed to join.\n */\nstruct btrfs_async_commit {\n\tstruct btrfs_trans_handle *newtrans;\n\tstruct btrfs_root *root;\n\tstruct delayed_work work;\n};\n\nstatic void do_async_commit(struct work_struct *work)\n{\n\tstruct btrfs_async_commit *ac =\n\t\tcontainer_of(work, struct btrfs_async_commit, work.work);\n\n\t/*\n\t * We've got freeze protection passed with the transaction.\n\t * Tell lockdep about it.\n\t */\n\tif (ac->newtrans->type < TRANS_JOIN_NOLOCK)\n\t\trwsem_acquire_read(\n\t\t     &ac->root->fs_info->sb->s_writers.lock_map[SB_FREEZE_FS-1],\n\t\t     0, 1, _THIS_IP_);\n\n\tcurrent->journal_info = ac->newtrans;\n\n\tbtrfs_commit_transaction(ac->newtrans, ac->root);\n\tkfree(ac);\n}\n\nint btrfs_commit_transaction_async(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t   int wait_for_unblock)\n{\n\tstruct btrfs_async_commit *ac;\n\tstruct btrfs_transaction *cur_trans;\n\n\tac = kmalloc(sizeof(*ac), GFP_NOFS);\n\tif (!ac)\n\t\treturn -ENOMEM;\n\n\tINIT_DELAYED_WORK(&ac->work, do_async_commit);\n\tac->root = root;\n\tac->newtrans = btrfs_join_transaction(root);\n\tif (IS_ERR(ac->newtrans)) {\n\t\tint err = PTR_ERR(ac->newtrans);\n\t\tkfree(ac);\n\t\treturn err;\n\t}\n\n\t/* take transaction reference */\n\tcur_trans = trans->transaction;\n\tatomic_inc(&cur_trans->use_count);\n\n\tbtrfs_end_transaction(trans, root);\n\n\t/*\n\t * Tell lockdep we've released the freeze rwsem, since the\n\t * async commit thread will be the one to unlock it.\n\t */\n\tif (trans->type < TRANS_JOIN_NOLOCK)\n\t\trwsem_release(\n\t\t\t&root->fs_info->sb->s_writers.lock_map[SB_FREEZE_FS-1],\n\t\t\t1, _THIS_IP_);\n\n\tschedule_delayed_work(&ac->work, 0);\n\n\t/* wait for transaction to start and unblock */\n\tif (wait_for_unblock)\n\t\twait_current_trans_commit_start_and_unblock(root, cur_trans);\n\telse\n\t\twait_current_trans_commit_start(root, cur_trans);\n\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\n\tput_transaction(cur_trans);\n\treturn 0;\n}\n\n\nstatic void cleanup_transaction(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_root *root, int err)\n{\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\n\tWARN_ON(trans->use_count > 1);\n\n\tbtrfs_abort_transaction(trans, root, err);\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tlist_del_init(&cur_trans->list);\n\tif (cur_trans == root->fs_info->running_transaction) {\n\t\troot->fs_info->running_transaction = NULL;\n\t\troot->fs_info->trans_no_join = 0;\n\t}\n\tspin_unlock(&root->fs_info->trans_lock);\n\n\tbtrfs_cleanup_one_transaction(trans->transaction, root);\n\n\tput_transaction(cur_trans);\n\tput_transaction(cur_trans);\n\n\ttrace_btrfs_transaction_commit(root);\n\n\tbtrfs_scrub_continue(root);\n\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\n\tkmem_cache_free(btrfs_trans_handle_cachep, trans);\n}\n\nstatic int btrfs_flush_all_pending_stuffs(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_root *root)\n{\n\tint flush_on_commit = btrfs_test_opt(root, FLUSHONCOMMIT);\n\tint snap_pending = 0;\n\tint ret;\n\n\tif (!flush_on_commit) {\n\t\tspin_lock(&root->fs_info->trans_lock);\n\t\tif (!list_empty(&trans->transaction->pending_snapshots))\n\t\t\tsnap_pending = 1;\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t}\n\n\tif (flush_on_commit || snap_pending) {\n\t\tbtrfs_start_delalloc_inodes(root, 1);\n\t\tbtrfs_wait_ordered_extents(root, 1);\n\t}\n\n\tret = btrfs_run_delayed_items(trans, root);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * running the delayed items may have added new refs. account\n\t * them now so that they hinder processing of more delayed refs\n\t * as little as possible.\n\t */\n\tbtrfs_delayed_refs_qgroup_accounting(trans, root->fs_info);\n\n\t/*\n\t * rename don't use btrfs_join_transaction, so, once we\n\t * set the transaction to blocked above, we aren't going\n\t * to get any new ordered operations.  We can safely run\n\t * it here and no for sure that nothing new will be added\n\t * to the list\n\t */\n\tbtrfs_run_ordered_operations(root, 1);\n\n\treturn 0;\n}\n\n/*\n * btrfs_transaction state sequence:\n *    in_commit = 0, blocked = 0  (initial)\n *    in_commit = 1, blocked = 1\n *    blocked = 0\n *    commit_done = 1\n */\nint btrfs_commit_transaction(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_root *root)\n{\n\tunsigned long joined = 0;\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tstruct btrfs_transaction *prev_trans = NULL;\n\tDEFINE_WAIT(wait);\n\tint ret;\n\tint should_grow = 0;\n\tunsigned long now = get_seconds();\n\n\tret = btrfs_run_ordered_operations(root, 0);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, root, ret);\n\t\tgoto cleanup_transaction;\n\t}\n\n\tif (cur_trans->aborted) {\n\t\tret = cur_trans->aborted;\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/* make a pass through all the delayed refs we have so far\n\t * any runnings procs may add more while we are here\n\t */\n\tret = btrfs_run_delayed_refs(trans, root, 0);\n\tif (ret)\n\t\tgoto cleanup_transaction;\n\n\tbtrfs_trans_release_metadata(trans, root);\n\ttrans->block_rsv = NULL;\n\n\tcur_trans = trans->transaction;\n\n\t/*\n\t * set the flushing flag so procs in this transaction have to\n\t * start sending their work down.\n\t */\n\tcur_trans->delayed_refs.flushing = 1;\n\n\tif (!list_empty(&trans->new_bgs))\n\t\tbtrfs_create_pending_block_groups(trans, root);\n\n\tret = btrfs_run_delayed_refs(trans, root, 0);\n\tif (ret)\n\t\tgoto cleanup_transaction;\n\n\tspin_lock(&cur_trans->commit_lock);\n\tif (cur_trans->in_commit) {\n\t\tspin_unlock(&cur_trans->commit_lock);\n\t\tatomic_inc(&cur_trans->use_count);\n\t\tret = btrfs_end_transaction(trans, root);\n\n\t\twait_for_commit(root, cur_trans);\n\n\t\tput_transaction(cur_trans);\n\n\t\treturn ret;\n\t}\n\n\ttrans->transaction->in_commit = 1;\n\ttrans->transaction->blocked = 1;\n\tspin_unlock(&cur_trans->commit_lock);\n\twake_up(&root->fs_info->transaction_blocked_wait);\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tif (cur_trans->list.prev != &root->fs_info->trans_list) {\n\t\tprev_trans = list_entry(cur_trans->list.prev,\n\t\t\t\t\tstruct btrfs_transaction, list);\n\t\tif (!prev_trans->commit_done) {\n\t\t\tatomic_inc(&prev_trans->use_count);\n\t\t\tspin_unlock(&root->fs_info->trans_lock);\n\n\t\t\twait_for_commit(root, prev_trans);\n\n\t\t\tput_transaction(prev_trans);\n\t\t} else {\n\t\t\tspin_unlock(&root->fs_info->trans_lock);\n\t\t}\n\t} else {\n\t\tspin_unlock(&root->fs_info->trans_lock);\n\t}\n\n\tif (!btrfs_test_opt(root, SSD) &&\n\t    (now < cur_trans->start_time || now - cur_trans->start_time < 1))\n\t\tshould_grow = 1;\n\n\tdo {\n\t\tjoined = cur_trans->num_joined;\n\n\t\tWARN_ON(cur_trans != trans->transaction);\n\n\t\tret = btrfs_flush_all_pending_stuffs(trans, root);\n\t\tif (ret)\n\t\t\tgoto cleanup_transaction;\n\n\t\tprepare_to_wait(&cur_trans->writer_wait, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\tif (atomic_read(&cur_trans->num_writers) > 1)\n\t\t\tschedule_timeout(MAX_SCHEDULE_TIMEOUT);\n\t\telse if (should_grow)\n\t\t\tschedule_timeout(1);\n\n\t\tfinish_wait(&cur_trans->writer_wait, &wait);\n\t} while (atomic_read(&cur_trans->num_writers) > 1 ||\n\t\t (should_grow && cur_trans->num_joined != joined));\n\n\tret = btrfs_flush_all_pending_stuffs(trans, root);\n\tif (ret)\n\t\tgoto cleanup_transaction;\n\n\t/*\n\t * Ok now we need to make sure to block out any other joins while we\n\t * commit the transaction.  We could have started a join before setting\n\t * no_join so make sure to wait for num_writers to == 1 again.\n\t */\n\tspin_lock(&root->fs_info->trans_lock);\n\troot->fs_info->trans_no_join = 1;\n\tspin_unlock(&root->fs_info->trans_lock);\n\twait_event(cur_trans->writer_wait,\n\t\t   atomic_read(&cur_trans->num_writers) == 1);\n\n\t/*\n\t * the reloc mutex makes sure that we stop\n\t * the balancing code from coming in and moving\n\t * extents around in the middle of the commit\n\t */\n\tmutex_lock(&root->fs_info->reloc_mutex);\n\n\t/*\n\t * We needn't worry about the delayed items because we will\n\t * deal with them in create_pending_snapshot(), which is the\n\t * core function of the snapshot creation.\n\t */\n\tret = create_pending_snapshots(trans, root->fs_info);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/*\n\t * We insert the dir indexes of the snapshots and update the inode\n\t * of the snapshots' parents after the snapshot creation, so there\n\t * are some delayed items which are not dealt with. Now deal with\n\t * them.\n\t *\n\t * We needn't worry that this operation will corrupt the snapshots,\n\t * because all the tree which are snapshoted will be forced to COW\n\t * the nodes and leaves.\n\t */\n\tret = btrfs_run_delayed_items(trans, root);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\tret = btrfs_run_delayed_refs(trans, root, (unsigned long)-1);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/*\n\t * make sure none of the code above managed to slip in a\n\t * delayed item\n\t */\n\tbtrfs_assert_delayed_root_empty(root);\n\n\tWARN_ON(cur_trans != trans->transaction);\n\n\tbtrfs_scrub_pause(root);\n\t/* btrfs_commit_tree_roots is responsible for getting the\n\t * various roots consistent with each other.  Every pointer\n\t * in the tree of tree roots has to point to the most up to date\n\t * root for every subvolume and other tree.  So, we have to keep\n\t * the tree logging code from jumping in and changing any\n\t * of the trees.\n\t *\n\t * At this point in the commit, there can't be any tree-log\n\t * writers, but a little lower down we drop the trans mutex\n\t * and let new people in.  By holding the tree_log_mutex\n\t * from now until after the super is written, we avoid races\n\t * with the tree-log code.\n\t */\n\tmutex_lock(&root->fs_info->tree_log_mutex);\n\n\tret = commit_fs_roots(trans, root);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/* commit_fs_roots gets rid of all the tree log roots, it is now\n\t * safe to free the root of tree log roots\n\t */\n\tbtrfs_free_log_root_tree(trans, root->fs_info);\n\n\tret = commit_cowonly_roots(trans, root);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\t\tmutex_unlock(&root->fs_info->reloc_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\tbtrfs_prepare_extent_commit(trans, root);\n\n\tcur_trans = root->fs_info->running_transaction;\n\n\tbtrfs_set_root_node(&root->fs_info->tree_root->root_item,\n\t\t\t    root->fs_info->tree_root->node);\n\tswitch_commit_root(root->fs_info->tree_root);\n\n\tbtrfs_set_root_node(&root->fs_info->chunk_root->root_item,\n\t\t\t    root->fs_info->chunk_root->node);\n\tswitch_commit_root(root->fs_info->chunk_root);\n\n\tassert_qgroups_uptodate(trans);\n\tupdate_super_roots(root);\n\n\tif (!root->fs_info->log_root_recovering) {\n\t\tbtrfs_set_super_log_root(root->fs_info->super_copy, 0);\n\t\tbtrfs_set_super_log_root_level(root->fs_info->super_copy, 0);\n\t}\n\n\tmemcpy(root->fs_info->super_for_commit, root->fs_info->super_copy,\n\t       sizeof(*root->fs_info->super_copy));\n\n\ttrans->transaction->blocked = 0;\n\tspin_lock(&root->fs_info->trans_lock);\n\troot->fs_info->running_transaction = NULL;\n\troot->fs_info->trans_no_join = 0;\n\tspin_unlock(&root->fs_info->trans_lock);\n\tmutex_unlock(&root->fs_info->reloc_mutex);\n\n\twake_up(&root->fs_info->transaction_wait);\n\n\tret = btrfs_write_and_wait_transaction(trans, root);\n\tif (ret) {\n\t\tbtrfs_error(root->fs_info, ret,\n\t\t\t    \"Error while writing out transaction.\");\n\t\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\tret = write_ctree_super(trans, root, 0);\n\tif (ret) {\n\t\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\t\tgoto cleanup_transaction;\n\t}\n\n\t/*\n\t * the super is written, we can safely allow the tree-loggers\n\t * to go about their business\n\t */\n\tmutex_unlock(&root->fs_info->tree_log_mutex);\n\n\tbtrfs_finish_extent_commit(trans, root);\n\n\tcur_trans->commit_done = 1;\n\n\troot->fs_info->last_trans_committed = cur_trans->transid;\n\n\twake_up(&cur_trans->commit_wait);\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tlist_del_init(&cur_trans->list);\n\tspin_unlock(&root->fs_info->trans_lock);\n\n\tput_transaction(cur_trans);\n\tput_transaction(cur_trans);\n\n\tif (trans->type < TRANS_JOIN_NOLOCK)\n\t\tsb_end_intwrite(root->fs_info->sb);\n\n\ttrace_btrfs_transaction_commit(root);\n\n\tbtrfs_scrub_continue(root);\n\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\n\tkmem_cache_free(btrfs_trans_handle_cachep, trans);\n\n\tif (current != root->fs_info->transaction_kthread)\n\t\tbtrfs_run_delayed_iputs(root);\n\n\treturn ret;\n\ncleanup_transaction:\n\tbtrfs_trans_release_metadata(trans, root);\n\ttrans->block_rsv = NULL;\n\tbtrfs_printk(root->fs_info, \"Skipping commit of aborted transaction.\\n\");\n//\tWARN_ON(1);\n\tif (current->journal_info == trans)\n\t\tcurrent->journal_info = NULL;\n\tcleanup_transaction(trans, root, ret);\n\n\treturn ret;\n}\n\n/*\n * interface function to delete all the snapshots we have scheduled for deletion\n */\nint btrfs_clean_old_snapshots(struct btrfs_root *root)\n{\n\tLIST_HEAD(list);\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\n\tspin_lock(&fs_info->trans_lock);\n\tlist_splice_init(&fs_info->dead_roots, &list);\n\tspin_unlock(&fs_info->trans_lock);\n\n\twhile (!list_empty(&list)) {\n\t\tint ret;\n\n\t\troot = list_entry(list.next, struct btrfs_root, root_list);\n\t\tlist_del(&root->root_list);\n\n\t\tbtrfs_kill_all_delayed_nodes(root);\n\n\t\tif (btrfs_header_backref_rev(root->node) <\n\t\t    BTRFS_MIXED_BACKREF_REV)\n\t\t\tret = btrfs_drop_snapshot(root, NULL, 0, 0);\n\t\telse\n\t\t\tret =btrfs_drop_snapshot(root, NULL, 1, 0);\n\t\tBUG_ON(ret < 0);\n\t}\n\treturn 0;\n}\n"], "filenames": ["fs/btrfs/ctree.h", "fs/btrfs/dir-item.c", "fs/btrfs/inode.c", "fs/btrfs/ioctl.c", "fs/btrfs/transaction.c"], "buggy_code_start_loc": [3285, 215, 4888, 709, 1193], "buggy_code_end_loc": [3285, 215, 7338, 709, 1194], "fixing_code_start_loc": [3286, 216, 4888, 710, 1193], "fixing_code_end_loc": [3288, 275, 7361, 720, 1194], "type": "CWE-310", "message": "The CRC32C feature in the Btrfs implementation in the Linux kernel before 3.8-rc1 allows local users to cause a denial of service (prevention of file creation) by leveraging the ability to write to a directory important to the victim, and creating a file with a crafted name that is associated with a specific CRC32C hash value.", "other": {"cve": {"id": "CVE-2012-5375", "sourceIdentifier": "cve@mitre.org", "published": "2013-02-18T11:56:38.760", "lastModified": "2014-01-04T04:42:35.680", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The CRC32C feature in the Btrfs implementation in the Linux kernel before 3.8-rc1 allows local users to cause a denial of service (prevention of file creation) by leveraging the ability to write to a directory important to the victim, and creating a file with a crafted name that is associated with a specific CRC32C hash value."}, {"lang": "es", "value": "La caracter\u00edstica CRC32C en la aplicaci\u00f3n Btrfs en el kernel Linux 3.8-rc1 antes permite a usuarios locales provocar una denegaci\u00f3n de servicio (prevenci\u00f3n de la creaci\u00f3n de archivos) mediante el aprovechamiento de la capacidad de escribir en un directorio importante para la v\u00edctima, y la creaci\u00f3n de un archivo con una nombre dise\u00f1ado que se asocia con un determinado valor de hash CRC32C."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:H/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "HIGH", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 1.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-310"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.8", "matchCriteriaId": "F7FA84F2-3FEE-4CCF-8F3C-D30454AED0DE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.35:*:*:*:*:*:*:*", "matchCriteriaId": "F72412E3-8DA9-4CC9-A426-B534202ADBA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.36:*:*:*:*:*:*:*", "matchCriteriaId": "FCAA9D7A-3C3E-4C0B-9D38-EA80E68C2E46"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.37:*:*:*:*:*:*:*", "matchCriteriaId": "4A9E3AE5-3FCF-4CBB-A30B-082BCFBFB0CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.38:*:*:*:*:*:*:*", "matchCriteriaId": "CF715657-4C3A-4392-B85D-1BBF4DE45D89"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.39:*:*:*:*:*:*:*", "matchCriteriaId": "4B63C618-AC3D-4EF7-AFDF-27B9BF482B78"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.40:*:*:*:*:*:*:*", "matchCriteriaId": "C33DA5A9-5E40-4365-9602-82FB4DCD15B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.41:*:*:*:*:*:*:*", "matchCriteriaId": "EFAFDB74-40BD-46FA-89AC-617EB2C7160B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.42:*:*:*:*:*:*:*", "matchCriteriaId": "CF5F17DA-30A7-40CF-BD7C-CEDF06D64617"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.43:*:*:*:*:*:*:*", "matchCriteriaId": "71A276F5-BD9D-4C1B-90DF-9B0C15B6F7DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.44:*:*:*:*:*:*:*", "matchCriteriaId": "F8F6EBEC-3C29-444B-BB85-6EF239B59EC1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.21:*:*:*:*:*:*:*", "matchCriteriaId": "1FF841F3-48A7-41D7-9C45-A8170435A5EB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.22:*:*:*:*:*:*:*", "matchCriteriaId": "EF506916-A6DC-4B1E-90E5-959492AF55F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.23:*:*:*:*:*:*:*", "matchCriteriaId": "B3CDAD1F-2C6A-48C0-8FAB-C2659373FA25"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.24:*:*:*:*:*:*:*", "matchCriteriaId": "4FFE4B22-C96A-43D0-B993-F51EDD9C5E0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.25:*:*:*:*:*:*:*", "matchCriteriaId": "F571CC8B-B212-4553-B463-1DB01D616E8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.26:*:*:*:*:*:*:*", "matchCriteriaId": "84E3E151-D437-48ED-A529-731EEFF88567"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.27:*:*:*:*:*:*:*", "matchCriteriaId": "E9E3EA3C-CCA5-4433-86E0-3D02C4757A0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.28:*:*:*:*:*:*:*", "matchCriteriaId": "F7AC4F7D-9FA6-4CF1-B2E9-70BF7D4D177C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.29:*:*:*:*:*:*:*", "matchCriteriaId": "3CE3A80D-9648-43CC-8F99-D741ED6552BF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.30:*:*:*:*:*:*:*", "matchCriteriaId": "C8A98C03-A465-41B4-A551-A26FEC7FFD94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "916E97D4-1FAB-42F5-826B-653B1C0909A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.5:*:*:*:*:*:*:*", "matchCriteriaId": "33FD2217-C5D0-48C1-AD74-3527127FEF9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.6:*:*:*:*:*:*:*", "matchCriteriaId": "2E92971F-B629-4E0A-9A50-8B235F9704B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.7:*:*:*:*:*:*:*", "matchCriteriaId": "EDD3A069-3829-4EE2-9D5A-29459F29D4C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.8:*:*:*:*:*:*:*", "matchCriteriaId": "A4A0964C-CEB2-41D7-A69C-1599B05B6171"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "A0F36FAC-141D-476D-84C5-A558C199F904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "51D64824-25F6-4761-BD6A-29038A143744"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "E284C8A1-740F-454D-A774-99CD3A21B594"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "C70D72AE-0CBF-4324-9935-57E28EC6279C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F674B06B-7E86-4E41-9126-8152D0DDABAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.6:*:*:*:*:*:*:*", "matchCriteriaId": "7039B3EC-8B22-413E-B582-B4BEC6181241"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.7:*:*:*:*:*:*:*", "matchCriteriaId": "35CF1DD2-80B9-4476-8963-5C3EF52B33F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.8:*:*:*:*:*:*:*", "matchCriteriaId": "BFB0B05B-A5CE-4B9C-AE7F-83062868D35B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.9:*:*:*:*:*:*:*", "matchCriteriaId": "D166A66E-7454-47EC-BB56-861A9AFEAFE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7DA94F50-2A62-4300-BF4D-A342AAE35629"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.11:*:*:*:*:*:*:*", "matchCriteriaId": "252D937B-50DC-444F-AE73-5FCF6203DF27"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.12:*:*:*:*:*:*:*", "matchCriteriaId": "F6D8EE51-02C1-47BC-A92C-0A8ABEFD28FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.13:*:*:*:*:*:*:*", "matchCriteriaId": "7F20A5D7-3B38-4911-861A-04C8310D5916"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.14:*:*:*:*:*:*:*", "matchCriteriaId": "D472DE3A-71D8-4F40-9DDE-85929A2B047D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.15:*:*:*:*:*:*:*", "matchCriteriaId": "B2AED943-65A8-4FDB-BBD0-CCEF8682A48C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.16:*:*:*:*:*:*:*", "matchCriteriaId": "D4640185-F3D8-4575-A71D-4C889A93DE2C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.17:*:*:*:*:*:*:*", "matchCriteriaId": "144CCF7C-025E-4879-B2E7-ABB8E4390BE5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.18:*:*:*:*:*:*:*", "matchCriteriaId": "B6FAA052-0B2B-40CE-8C98-919B8D08A5ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.19:*:*:*:*:*:*:*", "matchCriteriaId": "4B5A53DE-9C83-4A6B-96F3-23C03BF445D9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.20:*:*:*:*:*:*:*", "matchCriteriaId": "063EB879-CB05-4E33-AA90-9E43516839B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.21:*:*:*:*:*:*:*", "matchCriteriaId": "2D25764F-4B02-4C65-954E-8C7D6632DE00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.22:*:*:*:*:*:*:*", "matchCriteriaId": "F31F5BF3-CD0A-465C-857F-273841BCD28A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.23:*:*:*:*:*:*:*", "matchCriteriaId": "FF302C8A-079B-42B9-B455-CD9083BFA067"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.24:*:*:*:*:*:*:*", "matchCriteriaId": "744999C0-33D3-4363-B3DB-E0D02CDD3918"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.1:*:*:*:*:*:*:*", "matchCriteriaId": "962B0C45-AB29-4383-AC16-C6E8245D0FF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.2:*:*:*:*:*:*:*", "matchCriteriaId": "A0EE126B-74B2-4F79-BFE1-3DC169F3F9B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.3:*:*:*:*:*:*:*", "matchCriteriaId": "392075E0-A9C7-4B4A-90F9-7F1ADFF5EFA7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.4:*:*:*:*:*:*:*", "matchCriteriaId": "ECC66968-06F0-4874-A95A-A292C36E45C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.5:*:*:*:*:*:*:*", "matchCriteriaId": "5FE986E6-1068-4E1B-8EAB-DF1EAF32B4E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.6:*:*:*:*:*:*:*", "matchCriteriaId": "543E8536-1A8E-4E76-B89F-1B1F9F26FAB8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.5.7:*:*:*:*:*:*:*", "matchCriteriaId": "EC2B45E3-31E1-4B46-85FA-3A84E75B8F84"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.9:*:*:*:*:*:*:*", "matchCriteriaId": "5AC4A13E-F560-4D01-98A3-E2A2B82EB25B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.10:*:*:*:*:*:*:*", "matchCriteriaId": "942C462A-5398-4BB9-A792-598682E1FEF2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.6.11:*:*:*:*:*:*:*", "matchCriteriaId": "B852F7E0-0282-483D-BB4D-18CB7A4F1392"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7:*:*:*:*:*:*:*", "matchCriteriaId": "53ED9A31-99CC-41C8-8B72-5B2A9B49AA6C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.1:*:*:*:*:*:*:*", "matchCriteriaId": "EFD646BC-62F7-47CF-B0BE-768F701F7D9A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.2:*:*:*:*:*:*:*", "matchCriteriaId": "F43D418E-87C1-4C83-9FF1-4F45B4F452DD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.3:*:*:*:*:*:*:*", "matchCriteriaId": "680D0E00-F29A-487C-8770-8E7EAC672B7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.4:*:*:*:*:*:*:*", "matchCriteriaId": "2DCA96A4-A836-4E94-A39C-3AD3EA1D9611"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.5:*:*:*:*:*:*:*", "matchCriteriaId": "753C05E3-B603-4E36-B9BA-FAEDCBF62A7D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.6:*:*:*:*:*:*:*", "matchCriteriaId": "E385C2E0-B9F1-4564-8E6D-56FD9E762405"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.7:*:*:*:*:*:*:*", "matchCriteriaId": "041335D4-05E1-4004-9381-28AAD5994B47"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.8:*:*:*:*:*:*:*", "matchCriteriaId": "370F2AE5-3DBC-46B9-AC70-F052C9229C00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.7.9:*:*:*:*:*:*:*", "matchCriteriaId": "7A971BE3-259D-4494-BBC5-12793D92DB57"}]}]}], "references": [{"url": "http://crypto.junod.info/2012/12/13/hash-dos-and-btrfs/", "source": "cve@mitre.org"}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=9c52057c698fb96f8f07e7a4bcf4801a092bda89", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "http://openwall.com/lists/oss-security/2012/12/13/20", "source": "cve@mitre.org"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/testing/patch-3.8-rc1.bz2", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "http://www.ubuntu.com/usn/USN-1944-1", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-1945-1", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-1946-1", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-1947-1", "source": "cve@mitre.org"}, {"url": "http://www.ubuntu.com/usn/USN-2017-1", "source": "cve@mitre.org"}, {"url": "https://github.com/torvalds/linux/commit/9c52057c698fb96f8f07e7a4bcf4801a092bda89", "source": "cve@mitre.org", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/9c52057c698fb96f8f07e7a4bcf4801a092bda89"}}