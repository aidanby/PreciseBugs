{"buggy_code": ["/*\n * fs/f2fs/f2fs.h\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#ifndef _LINUX_F2FS_H\n#define _LINUX_F2FS_H\n\n#include <linux/types.h>\n#include <linux/page-flags.h>\n#include <linux/buffer_head.h>\n#include <linux/slab.h>\n#include <linux/crc32.h>\n#include <linux/magic.h>\n#include <linux/kobject.h>\n#include <linux/sched.h>\n#include <linux/vmalloc.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/quotaops.h>\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\n#include <linux/fscrypt_supp.h>\n#else\n#include <linux/fscrypt_notsupp.h>\n#endif\n#include <crypto/hash.h>\n\n#ifdef CONFIG_F2FS_CHECK_FS\n#define f2fs_bug_on(sbi, condition)\tBUG_ON(condition)\n#else\n#define f2fs_bug_on(sbi, condition)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (unlikely(condition)) {\t\t\t\t\\\n\t\t\tWARN_ON(1);\t\t\t\t\t\\\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n#endif\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\nenum {\n\tFAULT_KMALLOC,\n\tFAULT_PAGE_ALLOC,\n\tFAULT_ALLOC_NID,\n\tFAULT_ORPHAN,\n\tFAULT_BLOCK,\n\tFAULT_DIR_DEPTH,\n\tFAULT_EVICT_INODE,\n\tFAULT_TRUNCATE,\n\tFAULT_IO,\n\tFAULT_CHECKPOINT,\n\tFAULT_MAX,\n};\n\nstruct f2fs_fault_info {\n\tatomic_t inject_ops;\n\tunsigned int inject_rate;\n\tunsigned int inject_type;\n};\n\nextern char *fault_name[FAULT_MAX];\n#define IS_FAULT_SET(fi, type) ((fi)->inject_type & (1 << (type)))\n#endif\n\n/*\n * For mount options\n */\n#define F2FS_MOUNT_BG_GC\t\t0x00000001\n#define F2FS_MOUNT_DISABLE_ROLL_FORWARD\t0x00000002\n#define F2FS_MOUNT_DISCARD\t\t0x00000004\n#define F2FS_MOUNT_NOHEAP\t\t0x00000008\n#define F2FS_MOUNT_XATTR_USER\t\t0x00000010\n#define F2FS_MOUNT_POSIX_ACL\t\t0x00000020\n#define F2FS_MOUNT_DISABLE_EXT_IDENTIFY\t0x00000040\n#define F2FS_MOUNT_INLINE_XATTR\t\t0x00000080\n#define F2FS_MOUNT_INLINE_DATA\t\t0x00000100\n#define F2FS_MOUNT_INLINE_DENTRY\t0x00000200\n#define F2FS_MOUNT_FLUSH_MERGE\t\t0x00000400\n#define F2FS_MOUNT_NOBARRIER\t\t0x00000800\n#define F2FS_MOUNT_FASTBOOT\t\t0x00001000\n#define F2FS_MOUNT_EXTENT_CACHE\t\t0x00002000\n#define F2FS_MOUNT_FORCE_FG_GC\t\t0x00004000\n#define F2FS_MOUNT_DATA_FLUSH\t\t0x00008000\n#define F2FS_MOUNT_FAULT_INJECTION\t0x00010000\n#define F2FS_MOUNT_ADAPTIVE\t\t0x00020000\n#define F2FS_MOUNT_LFS\t\t\t0x00040000\n#define F2FS_MOUNT_USRQUOTA\t\t0x00080000\n#define F2FS_MOUNT_GRPQUOTA\t\t0x00100000\n#define F2FS_MOUNT_PRJQUOTA\t\t0x00200000\n#define F2FS_MOUNT_QUOTA\t\t0x00400000\n\n#define clear_opt(sbi, option)\t((sbi)->mount_opt.opt &= ~F2FS_MOUNT_##option)\n#define set_opt(sbi, option)\t((sbi)->mount_opt.opt |= F2FS_MOUNT_##option)\n#define test_opt(sbi, option)\t((sbi)->mount_opt.opt & F2FS_MOUNT_##option)\n\n#define ver_after(a, b)\t(typecheck(unsigned long long, a) &&\t\t\\\n\t\ttypecheck(unsigned long long, b) &&\t\t\t\\\n\t\t((long long)((a) - (b)) > 0))\n\ntypedef u32 block_t;\t/*\n\t\t\t * should not change u32, since it is the on-disk block\n\t\t\t * address format, __le32.\n\t\t\t */\ntypedef u32 nid_t;\n\nstruct f2fs_mount_info {\n\tunsigned int\topt;\n};\n\n#define F2FS_FEATURE_ENCRYPT\t\t0x0001\n#define F2FS_FEATURE_BLKZONED\t\t0x0002\n#define F2FS_FEATURE_ATOMIC_WRITE\t0x0004\n#define F2FS_FEATURE_EXTRA_ATTR\t\t0x0008\n#define F2FS_FEATURE_PRJQUOTA\t\t0x0010\n#define F2FS_FEATURE_INODE_CHKSUM\t0x0020\n\n#define F2FS_HAS_FEATURE(sb, mask)\t\t\t\t\t\\\n\t((F2FS_SB(sb)->raw_super->feature & cpu_to_le32(mask)) != 0)\n#define F2FS_SET_FEATURE(sb, mask)\t\t\t\t\t\\\n\t(F2FS_SB(sb)->raw_super->feature |= cpu_to_le32(mask))\n#define F2FS_CLEAR_FEATURE(sb, mask)\t\t\t\t\t\\\n\t(F2FS_SB(sb)->raw_super->feature &= ~cpu_to_le32(mask))\n\n/*\n * For checkpoint manager\n */\nenum {\n\tNAT_BITMAP,\n\tSIT_BITMAP\n};\n\n#define\tCP_UMOUNT\t0x00000001\n#define\tCP_FASTBOOT\t0x00000002\n#define\tCP_SYNC\t\t0x00000004\n#define\tCP_RECOVERY\t0x00000008\n#define\tCP_DISCARD\t0x00000010\n#define CP_TRIMMED\t0x00000020\n\n#define DEF_BATCHED_TRIM_SECTIONS\t2048\n#define BATCHED_TRIM_SEGMENTS(sbi)\t\\\n\t\t(GET_SEG_FROM_SEC(sbi, SM_I(sbi)->trim_sections))\n#define BATCHED_TRIM_BLOCKS(sbi)\t\\\n\t\t(BATCHED_TRIM_SEGMENTS(sbi) << (sbi)->log_blocks_per_seg)\n#define MAX_DISCARD_BLOCKS(sbi)\t\tBLKS_PER_SEC(sbi)\n#define DISCARD_ISSUE_RATE\t\t8\n#define DEF_MIN_DISCARD_ISSUE_TIME\t50\t/* 50 ms, if exists */\n#define DEF_MAX_DISCARD_ISSUE_TIME\t60000\t/* 60 s, if no candidates */\n#define DEF_CP_INTERVAL\t\t\t60\t/* 60 secs */\n#define DEF_IDLE_INTERVAL\t\t5\t/* 5 secs */\n\nstruct cp_control {\n\tint reason;\n\t__u64 trim_start;\n\t__u64 trim_end;\n\t__u64 trim_minlen;\n\t__u64 trimmed;\n};\n\n/*\n * For CP/NAT/SIT/SSA readahead\n */\nenum {\n\tMETA_CP,\n\tMETA_NAT,\n\tMETA_SIT,\n\tMETA_SSA,\n\tMETA_POR,\n};\n\n/* for the list of ino */\nenum {\n\tORPHAN_INO,\t\t/* for orphan ino list */\n\tAPPEND_INO,\t\t/* for append ino list */\n\tUPDATE_INO,\t\t/* for update ino list */\n\tMAX_INO_ENTRY,\t\t/* max. list */\n};\n\nstruct ino_entry {\n\tstruct list_head list;\t/* list head */\n\tnid_t ino;\t\t/* inode number */\n};\n\n/* for the list of inodes to be GCed */\nstruct inode_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct inode *inode;\t/* vfs inode pointer */\n};\n\n/* for the bitmap indicate blocks to be discarded */\nstruct discard_entry {\n\tstruct list_head list;\t/* list head */\n\tblock_t start_blkaddr;\t/* start blockaddr of current segment */\n\tunsigned char discard_map[SIT_VBLOCK_MAP_SIZE];\t/* segment discard bitmap */\n};\n\n/* default discard granularity of inner discard thread, unit: block count */\n#define DEFAULT_DISCARD_GRANULARITY\t\t16\n\n/* max discard pend list number */\n#define MAX_PLIST_NUM\t\t512\n#define plist_idx(blk_num)\t((blk_num) >= MAX_PLIST_NUM ?\t\t\\\n\t\t\t\t\t(MAX_PLIST_NUM - 1) : (blk_num - 1))\n\n#define P_ACTIVE\t0x01\n#define P_TRIM\t\t0x02\n#define plist_issue(tag)\t(((tag) & P_ACTIVE) || ((tag) & P_TRIM))\n\nenum {\n\tD_PREP,\n\tD_SUBMIT,\n\tD_DONE,\n};\n\nstruct discard_info {\n\tblock_t lstart;\t\t\t/* logical start address */\n\tblock_t len;\t\t\t/* length */\n\tblock_t start;\t\t\t/* actual start address in dev */\n};\n\nstruct discard_cmd {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tunion {\n\t\tstruct {\n\t\t\tblock_t lstart;\t/* logical start address */\n\t\t\tblock_t len;\t/* length */\n\t\t\tblock_t start;\t/* actual start address in dev */\n\t\t};\n\t\tstruct discard_info di;\t/* discard info */\n\n\t};\n\tstruct list_head list;\t\t/* command list */\n\tstruct completion wait;\t\t/* compleation */\n\tstruct block_device *bdev;\t/* bdev */\n\tunsigned short ref;\t\t/* reference count */\n\tunsigned char state;\t\t/* state */\n\tint error;\t\t\t/* bio error */\n};\n\nstruct discard_cmd_control {\n\tstruct task_struct *f2fs_issue_discard;\t/* discard thread */\n\tstruct list_head entry_list;\t\t/* 4KB discard entry list */\n\tstruct list_head pend_list[MAX_PLIST_NUM];/* store pending entries */\n\tunsigned char pend_list_tag[MAX_PLIST_NUM];/* tag for pending entries */\n\tstruct list_head wait_list;\t\t/* store on-flushing entries */\n\twait_queue_head_t discard_wait_queue;\t/* waiting queue for wake-up */\n\tunsigned int discard_wake;\t\t/* to wake up discard thread */\n\tstruct mutex cmd_lock;\n\tunsigned int nr_discards;\t\t/* # of discards in the list */\n\tunsigned int max_discards;\t\t/* max. discards to be issued */\n\tunsigned int discard_granularity;\t/* discard granularity */\n\tunsigned int undiscard_blks;\t\t/* # of undiscard blocks */\n\tatomic_t issued_discard;\t\t/* # of issued discard */\n\tatomic_t issing_discard;\t\t/* # of issing discard */\n\tatomic_t discard_cmd_cnt;\t\t/* # of cached cmd count */\n\tstruct rb_root root;\t\t\t/* root of discard rb-tree */\n};\n\n/* for the list of fsync inodes, used only during recovery */\nstruct fsync_inode_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct inode *inode;\t/* vfs inode pointer */\n\tblock_t blkaddr;\t/* block address locating the last fsync */\n\tblock_t last_dentry;\t/* block address locating the last dentry */\n};\n\n#define nats_in_cursum(jnl)\t\t(le16_to_cpu((jnl)->n_nats))\n#define sits_in_cursum(jnl)\t\t(le16_to_cpu((jnl)->n_sits))\n\n#define nat_in_journal(jnl, i)\t\t((jnl)->nat_j.entries[i].ne)\n#define nid_in_journal(jnl, i)\t\t((jnl)->nat_j.entries[i].nid)\n#define sit_in_journal(jnl, i)\t\t((jnl)->sit_j.entries[i].se)\n#define segno_in_journal(jnl, i)\t((jnl)->sit_j.entries[i].segno)\n\n#define MAX_NAT_JENTRIES(jnl)\t(NAT_JOURNAL_ENTRIES - nats_in_cursum(jnl))\n#define MAX_SIT_JENTRIES(jnl)\t(SIT_JOURNAL_ENTRIES - sits_in_cursum(jnl))\n\nstatic inline int update_nats_in_cursum(struct f2fs_journal *journal, int i)\n{\n\tint before = nats_in_cursum(journal);\n\n\tjournal->n_nats = cpu_to_le16(before + i);\n\treturn before;\n}\n\nstatic inline int update_sits_in_cursum(struct f2fs_journal *journal, int i)\n{\n\tint before = sits_in_cursum(journal);\n\n\tjournal->n_sits = cpu_to_le16(before + i);\n\treturn before;\n}\n\nstatic inline bool __has_cursum_space(struct f2fs_journal *journal,\n\t\t\t\t\t\t\tint size, int type)\n{\n\tif (type == NAT_JOURNAL)\n\t\treturn size <= MAX_NAT_JENTRIES(journal);\n\treturn size <= MAX_SIT_JENTRIES(journal);\n}\n\n/*\n * ioctl commands\n */\n#define F2FS_IOC_GETFLAGS\t\tFS_IOC_GETFLAGS\n#define F2FS_IOC_SETFLAGS\t\tFS_IOC_SETFLAGS\n#define F2FS_IOC_GETVERSION\t\tFS_IOC_GETVERSION\n\n#define F2FS_IOCTL_MAGIC\t\t0xf5\n#define F2FS_IOC_START_ATOMIC_WRITE\t_IO(F2FS_IOCTL_MAGIC, 1)\n#define F2FS_IOC_COMMIT_ATOMIC_WRITE\t_IO(F2FS_IOCTL_MAGIC, 2)\n#define F2FS_IOC_START_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 3)\n#define F2FS_IOC_RELEASE_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 4)\n#define F2FS_IOC_ABORT_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 5)\n#define F2FS_IOC_GARBAGE_COLLECT\t_IOW(F2FS_IOCTL_MAGIC, 6, __u32)\n#define F2FS_IOC_WRITE_CHECKPOINT\t_IO(F2FS_IOCTL_MAGIC, 7)\n#define F2FS_IOC_DEFRAGMENT\t\t_IOWR(F2FS_IOCTL_MAGIC, 8,\t\\\n\t\t\t\t\t\tstruct f2fs_defragment)\n#define F2FS_IOC_MOVE_RANGE\t\t_IOWR(F2FS_IOCTL_MAGIC, 9,\t\\\n\t\t\t\t\t\tstruct f2fs_move_range)\n#define F2FS_IOC_FLUSH_DEVICE\t\t_IOW(F2FS_IOCTL_MAGIC, 10,\t\\\n\t\t\t\t\t\tstruct f2fs_flush_device)\n#define F2FS_IOC_GARBAGE_COLLECT_RANGE\t_IOW(F2FS_IOCTL_MAGIC, 11,\t\\\n\t\t\t\t\t\tstruct f2fs_gc_range)\n#define F2FS_IOC_GET_FEATURES\t\t_IOR(F2FS_IOCTL_MAGIC, 12, __u32)\n\n#define F2FS_IOC_SET_ENCRYPTION_POLICY\tFS_IOC_SET_ENCRYPTION_POLICY\n#define F2FS_IOC_GET_ENCRYPTION_POLICY\tFS_IOC_GET_ENCRYPTION_POLICY\n#define F2FS_IOC_GET_ENCRYPTION_PWSALT\tFS_IOC_GET_ENCRYPTION_PWSALT\n\n/*\n * should be same as XFS_IOC_GOINGDOWN.\n * Flags for going down operation used by FS_IOC_GOINGDOWN\n */\n#define F2FS_IOC_SHUTDOWN\t_IOR('X', 125, __u32)\t/* Shutdown */\n#define F2FS_GOING_DOWN_FULLSYNC\t0x0\t/* going down with full sync */\n#define F2FS_GOING_DOWN_METASYNC\t0x1\t/* going down with metadata */\n#define F2FS_GOING_DOWN_NOSYNC\t\t0x2\t/* going down */\n#define F2FS_GOING_DOWN_METAFLUSH\t0x3\t/* going down with meta flush */\n\n#if defined(__KERNEL__) && defined(CONFIG_COMPAT)\n/*\n * ioctl commands in 32 bit emulation\n */\n#define F2FS_IOC32_GETFLAGS\t\tFS_IOC32_GETFLAGS\n#define F2FS_IOC32_SETFLAGS\t\tFS_IOC32_SETFLAGS\n#define F2FS_IOC32_GETVERSION\t\tFS_IOC32_GETVERSION\n#endif\n\n#define F2FS_IOC_FSGETXATTR\t\tFS_IOC_FSGETXATTR\n#define F2FS_IOC_FSSETXATTR\t\tFS_IOC_FSSETXATTR\n\nstruct f2fs_gc_range {\n\tu32 sync;\n\tu64 start;\n\tu64 len;\n};\n\nstruct f2fs_defragment {\n\tu64 start;\n\tu64 len;\n};\n\nstruct f2fs_move_range {\n\tu32 dst_fd;\t\t/* destination fd */\n\tu64 pos_in;\t\t/* start position in src_fd */\n\tu64 pos_out;\t\t/* start position in dst_fd */\n\tu64 len;\t\t/* size to move */\n};\n\nstruct f2fs_flush_device {\n\tu32 dev_num;\t\t/* device number to flush */\n\tu32 segments;\t\t/* # of segments to flush */\n};\n\n/* for inline stuff */\n#define DEF_INLINE_RESERVED_SIZE\t1\nstatic inline int get_extra_isize(struct inode *inode);\n#define MAX_INLINE_DATA(inode)\t(sizeof(__le32) * \\\n\t\t\t\t(CUR_ADDRS_PER_INODE(inode) - \\\n\t\t\t\tDEF_INLINE_RESERVED_SIZE - \\\n\t\t\t\tF2FS_INLINE_XATTR_ADDRS))\n\n/* for inline dir */\n#define NR_INLINE_DENTRY(inode)\t(MAX_INLINE_DATA(inode) * BITS_PER_BYTE / \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tBITS_PER_BYTE + 1))\n#define INLINE_DENTRY_BITMAP_SIZE(inode)\t((NR_INLINE_DENTRY(inode) + \\\n\t\t\t\t\tBITS_PER_BYTE - 1) / BITS_PER_BYTE)\n#define INLINE_RESERVED_SIZE(inode)\t(MAX_INLINE_DATA(inode) - \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tNR_INLINE_DENTRY(inode) + \\\n\t\t\t\tINLINE_DENTRY_BITMAP_SIZE(inode)))\n\n/*\n * For INODE and NODE manager\n */\n/* for directory operations */\nstruct f2fs_dentry_ptr {\n\tstruct inode *inode;\n\tvoid *bitmap;\n\tstruct f2fs_dir_entry *dentry;\n\t__u8 (*filename)[F2FS_SLOT_LEN];\n\tint max;\n\tint nr_bitmap;\n};\n\nstatic inline void make_dentry_ptr_block(struct inode *inode,\n\t\tstruct f2fs_dentry_ptr *d, struct f2fs_dentry_block *t)\n{\n\td->inode = inode;\n\td->max = NR_DENTRY_IN_BLOCK;\n\td->nr_bitmap = SIZE_OF_DENTRY_BITMAP;\n\td->bitmap = &t->dentry_bitmap;\n\td->dentry = t->dentry;\n\td->filename = t->filename;\n}\n\nstatic inline void make_dentry_ptr_inline(struct inode *inode,\n\t\t\t\t\tstruct f2fs_dentry_ptr *d, void *t)\n{\n\tint entry_cnt = NR_INLINE_DENTRY(inode);\n\tint bitmap_size = INLINE_DENTRY_BITMAP_SIZE(inode);\n\tint reserved_size = INLINE_RESERVED_SIZE(inode);\n\n\td->inode = inode;\n\td->max = entry_cnt;\n\td->nr_bitmap = bitmap_size;\n\td->bitmap = t;\n\td->dentry = t + bitmap_size + reserved_size;\n\td->filename = t + bitmap_size + reserved_size +\n\t\t\t\t\tSIZE_OF_DIR_ENTRY * entry_cnt;\n}\n\n/*\n * XATTR_NODE_OFFSET stores xattrs to one node block per file keeping -1\n * as its node offset to distinguish from index node blocks.\n * But some bits are used to mark the node block.\n */\n#define XATTR_NODE_OFFSET\t((((unsigned int)-1) << OFFSET_BIT_SHIFT) \\\n\t\t\t\t>> OFFSET_BIT_SHIFT)\nenum {\n\tALLOC_NODE,\t\t\t/* allocate a new node page if needed */\n\tLOOKUP_NODE,\t\t\t/* look up a node without readahead */\n\tLOOKUP_NODE_RA,\t\t\t/*\n\t\t\t\t\t * look up a node with readahead called\n\t\t\t\t\t * by get_data_block.\n\t\t\t\t\t */\n};\n\n#define F2FS_LINK_MAX\t0xffffffff\t/* maximum link count per file */\n\n#define MAX_DIR_RA_PAGES\t4\t/* maximum ra pages of dir */\n\n/* vector size for gang look-up from extent cache that consists of radix tree */\n#define EXT_TREE_VEC_SIZE\t64\n\n/* for in-memory extent cache entry */\n#define F2FS_MIN_EXTENT_LEN\t64\t/* minimum extent length */\n\n/* number of extent info in extent cache we try to shrink */\n#define EXTENT_CACHE_SHRINK_NUMBER\t128\n\nstruct rb_entry {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tunsigned int ofs;\t\t/* start offset of the entry */\n\tunsigned int len;\t\t/* length of the entry */\n};\n\nstruct extent_info {\n\tunsigned int fofs;\t\t/* start offset in a file */\n\tunsigned int len;\t\t/* length of the extent */\n\tu32 blk;\t\t\t/* start block address of the extent */\n};\n\nstruct extent_node {\n\tstruct rb_node rb_node;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned int fofs;\n\t\t\tunsigned int len;\n\t\t\tu32 blk;\n\t\t};\n\t\tstruct extent_info ei;\t/* extent info */\n\n\t};\n\tstruct list_head list;\t\t/* node in global extent list of sbi */\n\tstruct extent_tree *et;\t\t/* extent tree pointer */\n};\n\nstruct extent_tree {\n\tnid_t ino;\t\t\t/* inode number */\n\tstruct rb_root root;\t\t/* root of extent info rb-tree */\n\tstruct extent_node *cached_en;\t/* recently accessed extent node */\n\tstruct extent_info largest;\t/* largested extent info */\n\tstruct list_head list;\t\t/* to be used by sbi->zombie_list */\n\trwlock_t lock;\t\t\t/* protect extent info rb-tree */\n\tatomic_t node_cnt;\t\t/* # of extent node in rb-tree*/\n};\n\n/*\n * This structure is taken from ext4_map_blocks.\n *\n * Note that, however, f2fs uses NEW and MAPPED flags for f2fs_map_blocks().\n */\n#define F2FS_MAP_NEW\t\t(1 << BH_New)\n#define F2FS_MAP_MAPPED\t\t(1 << BH_Mapped)\n#define F2FS_MAP_UNWRITTEN\t(1 << BH_Unwritten)\n#define F2FS_MAP_FLAGS\t\t(F2FS_MAP_NEW | F2FS_MAP_MAPPED |\\\n\t\t\t\tF2FS_MAP_UNWRITTEN)\n\nstruct f2fs_map_blocks {\n\tblock_t m_pblk;\n\tblock_t m_lblk;\n\tunsigned int m_len;\n\tunsigned int m_flags;\n\tpgoff_t *m_next_pgofs;\t\t/* point next possible non-hole pgofs */\n};\n\n/* for flag in get_data_block */\nenum {\n\tF2FS_GET_BLOCK_DEFAULT,\n\tF2FS_GET_BLOCK_FIEMAP,\n\tF2FS_GET_BLOCK_BMAP,\n\tF2FS_GET_BLOCK_PRE_DIO,\n\tF2FS_GET_BLOCK_PRE_AIO,\n};\n\n/*\n * i_advise uses FADVISE_XXX_BIT. We can add additional hints later.\n */\n#define FADVISE_COLD_BIT\t0x01\n#define FADVISE_LOST_PINO_BIT\t0x02\n#define FADVISE_ENCRYPT_BIT\t0x04\n#define FADVISE_ENC_NAME_BIT\t0x08\n#define FADVISE_KEEP_SIZE_BIT\t0x10\n\n#define file_is_cold(inode)\tis_file(inode, FADVISE_COLD_BIT)\n#define file_wrong_pino(inode)\tis_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_set_cold(inode)\tset_file(inode, FADVISE_COLD_BIT)\n#define file_lost_pino(inode)\tset_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_clear_cold(inode)\tclear_file(inode, FADVISE_COLD_BIT)\n#define file_got_pino(inode)\tclear_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_is_encrypt(inode)\tis_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_set_encrypt(inode)\tset_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_clear_encrypt(inode) clear_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_enc_name(inode)\tis_file(inode, FADVISE_ENC_NAME_BIT)\n#define file_set_enc_name(inode) set_file(inode, FADVISE_ENC_NAME_BIT)\n#define file_keep_isize(inode)\tis_file(inode, FADVISE_KEEP_SIZE_BIT)\n#define file_set_keep_isize(inode) set_file(inode, FADVISE_KEEP_SIZE_BIT)\n\n#define DEF_DIR_LEVEL\t\t0\n\nstruct f2fs_inode_info {\n\tstruct inode vfs_inode;\t\t/* serve a vfs inode */\n\tunsigned long i_flags;\t\t/* keep an inode flags for ioctl */\n\tunsigned char i_advise;\t\t/* use to give file attribute hints */\n\tunsigned char i_dir_level;\t/* use for dentry level for large dir */\n\tunsigned int i_current_depth;\t/* use only in directory structure */\n\tunsigned int i_pino;\t\t/* parent inode number */\n\tumode_t i_acl_mode;\t\t/* keep file acl mode temporarily */\n\n\t/* Use below internally in f2fs*/\n\tunsigned long flags;\t\t/* use to pass per-file flags */\n\tstruct rw_semaphore i_sem;\t/* protect fi info */\n\tatomic_t dirty_pages;\t\t/* # of dirty pages */\n\tf2fs_hash_t chash;\t\t/* hash value of given file name */\n\tunsigned int clevel;\t\t/* maximum level of given file name */\n\tstruct task_struct *task;\t/* lookup and create consistency */\n\tstruct task_struct *cp_task;\t/* separate cp/wb IO stats*/\n\tnid_t i_xattr_nid;\t\t/* node id that contains xattrs */\n\tloff_t\tlast_disk_size;\t\t/* lastly written file size */\n\n#ifdef CONFIG_QUOTA\n\tstruct dquot *i_dquot[MAXQUOTAS];\n\n\t/* quota space reservation, managed internally by quota code */\n\tqsize_t i_reserved_quota;\n#endif\n\tstruct list_head dirty_list;\t/* dirty list for dirs and files */\n\tstruct list_head gdirty_list;\t/* linked in global dirty list */\n\tstruct list_head inmem_pages;\t/* inmemory pages managed by f2fs */\n\tstruct task_struct *inmem_task;\t/* store inmemory task */\n\tstruct mutex inmem_lock;\t/* lock for inmemory pages */\n\tstruct extent_tree *extent_tree;\t/* cached extent_tree entry */\n\tstruct rw_semaphore dio_rwsem[2];/* avoid racing between dio and gc */\n\tstruct rw_semaphore i_mmap_sem;\n\tstruct rw_semaphore i_xattr_sem; /* avoid racing between reading and changing EAs */\n\n\tint i_extra_isize;\t\t/* size of extra space located in i_addr */\n\tkprojid_t i_projid;\t\t/* id for project quota */\n};\n\nstatic inline void get_extent_info(struct extent_info *ext,\n\t\t\t\t\tstruct f2fs_extent *i_ext)\n{\n\text->fofs = le32_to_cpu(i_ext->fofs);\n\text->blk = le32_to_cpu(i_ext->blk);\n\text->len = le32_to_cpu(i_ext->len);\n}\n\nstatic inline void set_raw_extent(struct extent_info *ext,\n\t\t\t\t\tstruct f2fs_extent *i_ext)\n{\n\ti_ext->fofs = cpu_to_le32(ext->fofs);\n\ti_ext->blk = cpu_to_le32(ext->blk);\n\ti_ext->len = cpu_to_le32(ext->len);\n}\n\nstatic inline void set_extent_info(struct extent_info *ei, unsigned int fofs,\n\t\t\t\t\t\tu32 blk, unsigned int len)\n{\n\tei->fofs = fofs;\n\tei->blk = blk;\n\tei->len = len;\n}\n\nstatic inline bool __is_discard_mergeable(struct discard_info *back,\n\t\t\t\t\t\tstruct discard_info *front)\n{\n\treturn back->lstart + back->len == front->lstart;\n}\n\nstatic inline bool __is_discard_back_mergeable(struct discard_info *cur,\n\t\t\t\t\t\tstruct discard_info *back)\n{\n\treturn __is_discard_mergeable(back, cur);\n}\n\nstatic inline bool __is_discard_front_mergeable(struct discard_info *cur,\n\t\t\t\t\t\tstruct discard_info *front)\n{\n\treturn __is_discard_mergeable(cur, front);\n}\n\nstatic inline bool __is_extent_mergeable(struct extent_info *back,\n\t\t\t\t\t\tstruct extent_info *front)\n{\n\treturn (back->fofs + back->len == front->fofs &&\n\t\t\tback->blk + back->len == front->blk);\n}\n\nstatic inline bool __is_back_mergeable(struct extent_info *cur,\n\t\t\t\t\t\tstruct extent_info *back)\n{\n\treturn __is_extent_mergeable(back, cur);\n}\n\nstatic inline bool __is_front_mergeable(struct extent_info *cur,\n\t\t\t\t\t\tstruct extent_info *front)\n{\n\treturn __is_extent_mergeable(cur, front);\n}\n\nextern void f2fs_mark_inode_dirty_sync(struct inode *inode, bool sync);\nstatic inline void __try_update_largest_extent(struct inode *inode,\n\t\t\tstruct extent_tree *et, struct extent_node *en)\n{\n\tif (en->ei.len > et->largest.len) {\n\t\tet->largest = en->ei;\n\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t}\n}\n\nenum nid_list {\n\tFREE_NID_LIST,\n\tALLOC_NID_LIST,\n\tMAX_NID_LIST,\n};\n\nstruct f2fs_nm_info {\n\tblock_t nat_blkaddr;\t\t/* base disk address of NAT */\n\tnid_t max_nid;\t\t\t/* maximum possible node ids */\n\tnid_t available_nids;\t\t/* # of available node ids */\n\tnid_t next_scan_nid;\t\t/* the next nid to be scanned */\n\tunsigned int ram_thresh;\t/* control the memory footprint */\n\tunsigned int ra_nid_pages;\t/* # of nid pages to be readaheaded */\n\tunsigned int dirty_nats_ratio;\t/* control dirty nats ratio threshold */\n\n\t/* NAT cache management */\n\tstruct radix_tree_root nat_root;/* root of the nat entry cache */\n\tstruct radix_tree_root nat_set_root;/* root of the nat set cache */\n\tstruct rw_semaphore nat_tree_lock;\t/* protect nat_tree_lock */\n\tstruct list_head nat_entries;\t/* cached nat entry list (clean) */\n\tunsigned int nat_cnt;\t\t/* the # of cached nat entries */\n\tunsigned int dirty_nat_cnt;\t/* total num of nat entries in set */\n\tunsigned int nat_blocks;\t/* # of nat blocks */\n\n\t/* free node ids management */\n\tstruct radix_tree_root free_nid_root;/* root of the free_nid cache */\n\tstruct list_head nid_list[MAX_NID_LIST];/* lists for free nids */\n\tunsigned int nid_cnt[MAX_NID_LIST];\t/* the number of free node id */\n\tspinlock_t nid_list_lock;\t/* protect nid lists ops */\n\tstruct mutex build_lock;\t/* lock for build free nids */\n\tunsigned char (*free_nid_bitmap)[NAT_ENTRY_BITMAP_SIZE];\n\tunsigned char *nat_block_bitmap;\n\tunsigned short *free_nid_count;\t/* free nid count of NAT block */\n\n\t/* for checkpoint */\n\tchar *nat_bitmap;\t\t/* NAT bitmap pointer */\n\n\tunsigned int nat_bits_blocks;\t/* # of nat bits blocks */\n\tunsigned char *nat_bits;\t/* NAT bits blocks */\n\tunsigned char *full_nat_bits;\t/* full NAT pages */\n\tunsigned char *empty_nat_bits;\t/* empty NAT pages */\n#ifdef CONFIG_F2FS_CHECK_FS\n\tchar *nat_bitmap_mir;\t\t/* NAT bitmap mirror */\n#endif\n\tint bitmap_size;\t\t/* bitmap size */\n};\n\n/*\n * this structure is used as one of function parameters.\n * all the information are dedicated to a given direct node block determined\n * by the data offset in a file.\n */\nstruct dnode_of_data {\n\tstruct inode *inode;\t\t/* vfs inode pointer */\n\tstruct page *inode_page;\t/* its inode page, NULL is possible */\n\tstruct page *node_page;\t\t/* cached direct node page */\n\tnid_t nid;\t\t\t/* node id of the direct node block */\n\tunsigned int ofs_in_node;\t/* data offset in the node page */\n\tbool inode_page_locked;\t\t/* inode page is locked or not */\n\tbool node_changed;\t\t/* is node block changed */\n\tchar cur_level;\t\t\t/* level of hole node page */\n\tchar max_level;\t\t\t/* level of current page located */\n\tblock_t\tdata_blkaddr;\t\t/* block address of the node block */\n};\n\nstatic inline void set_new_dnode(struct dnode_of_data *dn, struct inode *inode,\n\t\tstruct page *ipage, struct page *npage, nid_t nid)\n{\n\tmemset(dn, 0, sizeof(*dn));\n\tdn->inode = inode;\n\tdn->inode_page = ipage;\n\tdn->node_page = npage;\n\tdn->nid = nid;\n}\n\n/*\n * For SIT manager\n *\n * By default, there are 6 active log areas across the whole main area.\n * When considering hot and cold data separation to reduce cleaning overhead,\n * we split 3 for data logs and 3 for node logs as hot, warm, and cold types,\n * respectively.\n * In the current design, you should not change the numbers intentionally.\n * Instead, as a mount option such as active_logs=x, you can use 2, 4, and 6\n * logs individually according to the underlying devices. (default: 6)\n * Just in case, on-disk layout covers maximum 16 logs that consist of 8 for\n * data and 8 for node logs.\n */\n#define\tNR_CURSEG_DATA_TYPE\t(3)\n#define NR_CURSEG_NODE_TYPE\t(3)\n#define NR_CURSEG_TYPE\t(NR_CURSEG_DATA_TYPE + NR_CURSEG_NODE_TYPE)\n\nenum {\n\tCURSEG_HOT_DATA\t= 0,\t/* directory entry blocks */\n\tCURSEG_WARM_DATA,\t/* data blocks */\n\tCURSEG_COLD_DATA,\t/* multimedia or GCed data blocks */\n\tCURSEG_HOT_NODE,\t/* direct node blocks of directory files */\n\tCURSEG_WARM_NODE,\t/* direct node blocks of normal files */\n\tCURSEG_COLD_NODE,\t/* indirect node blocks */\n\tNO_CHECK_TYPE,\n};\n\nstruct flush_cmd {\n\tstruct completion wait;\n\tstruct llist_node llnode;\n\tint ret;\n};\n\nstruct flush_cmd_control {\n\tstruct task_struct *f2fs_issue_flush;\t/* flush thread */\n\twait_queue_head_t flush_wait_queue;\t/* waiting queue for wake-up */\n\tatomic_t issued_flush;\t\t\t/* # of issued flushes */\n\tatomic_t issing_flush;\t\t\t/* # of issing flushes */\n\tstruct llist_head issue_list;\t\t/* list for command issue */\n\tstruct llist_node *dispatch_list;\t/* list for command dispatch */\n};\n\nstruct f2fs_sm_info {\n\tstruct sit_info *sit_info;\t\t/* whole segment information */\n\tstruct free_segmap_info *free_info;\t/* free segment information */\n\tstruct dirty_seglist_info *dirty_info;\t/* dirty segment information */\n\tstruct curseg_info *curseg_array;\t/* active segment information */\n\n\tblock_t seg0_blkaddr;\t\t/* block address of 0'th segment */\n\tblock_t main_blkaddr;\t\t/* start block address of main area */\n\tblock_t ssa_blkaddr;\t\t/* start block address of SSA area */\n\n\tunsigned int segment_count;\t/* total # of segments */\n\tunsigned int main_segments;\t/* # of segments in main area */\n\tunsigned int reserved_segments;\t/* # of reserved segments */\n\tunsigned int ovp_segments;\t/* # of overprovision segments */\n\n\t/* a threshold to reclaim prefree segments */\n\tunsigned int rec_prefree_segments;\n\n\t/* for batched trimming */\n\tunsigned int trim_sections;\t\t/* # of sections to trim */\n\n\tstruct list_head sit_entry_set;\t/* sit entry set list */\n\n\tunsigned int ipu_policy;\t/* in-place-update policy */\n\tunsigned int min_ipu_util;\t/* in-place-update threshold */\n\tunsigned int min_fsync_blocks;\t/* threshold for fsync */\n\tunsigned int min_hot_blocks;\t/* threshold for hot block allocation */\n\n\t/* for flush command control */\n\tstruct flush_cmd_control *fcc_info;\n\n\t/* for discard command control */\n\tstruct discard_cmd_control *dcc_info;\n};\n\n/*\n * For superblock\n */\n/*\n * COUNT_TYPE for monitoring\n *\n * f2fs monitors the number of several block types such as on-writeback,\n * dirty dentry blocks, dirty node blocks, and dirty meta blocks.\n */\n#define WB_DATA_TYPE(p)\t(__is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)\nenum count_type {\n\tF2FS_DIRTY_DENTS,\n\tF2FS_DIRTY_DATA,\n\tF2FS_DIRTY_NODES,\n\tF2FS_DIRTY_META,\n\tF2FS_INMEM_PAGES,\n\tF2FS_DIRTY_IMETA,\n\tF2FS_WB_CP_DATA,\n\tF2FS_WB_DATA,\n\tNR_COUNT_TYPE,\n};\n\n/*\n * The below are the page types of bios used in submit_bio().\n * The available types are:\n * DATA\t\t\tUser data pages. It operates as async mode.\n * NODE\t\t\tNode pages. It operates as async mode.\n * META\t\t\tFS metadata pages such as SIT, NAT, CP.\n * NR_PAGE_TYPE\t\tThe number of page types.\n * META_FLUSH\t\tMake sure the previous pages are written\n *\t\t\twith waiting the bio's completion\n * ...\t\t\tOnly can be used with META.\n */\n#define PAGE_TYPE_OF_BIO(type)\t((type) > META ? META : (type))\nenum page_type {\n\tDATA,\n\tNODE,\n\tMETA,\n\tNR_PAGE_TYPE,\n\tMETA_FLUSH,\n\tINMEM,\t\t/* the below types are used by tracepoints only. */\n\tINMEM_DROP,\n\tINMEM_INVALIDATE,\n\tINMEM_REVOKE,\n\tIPU,\n\tOPU,\n};\n\nenum temp_type {\n\tHOT = 0,\t/* must be zero for meta bio */\n\tWARM,\n\tCOLD,\n\tNR_TEMP_TYPE,\n};\n\nenum need_lock_type {\n\tLOCK_REQ = 0,\n\tLOCK_DONE,\n\tLOCK_RETRY,\n};\n\nenum iostat_type {\n\tAPP_DIRECT_IO,\t\t\t/* app direct IOs */\n\tAPP_BUFFERED_IO,\t\t/* app buffered IOs */\n\tAPP_WRITE_IO,\t\t\t/* app write IOs */\n\tAPP_MAPPED_IO,\t\t\t/* app mapped IOs */\n\tFS_DATA_IO,\t\t\t/* data IOs from kworker/fsync/reclaimer */\n\tFS_NODE_IO,\t\t\t/* node IOs from kworker/fsync/reclaimer */\n\tFS_META_IO,\t\t\t/* meta IOs from kworker/reclaimer */\n\tFS_GC_DATA_IO,\t\t\t/* data IOs from forground gc */\n\tFS_GC_NODE_IO,\t\t\t/* node IOs from forground gc */\n\tFS_CP_DATA_IO,\t\t\t/* data IOs from checkpoint */\n\tFS_CP_NODE_IO,\t\t\t/* node IOs from checkpoint */\n\tFS_CP_META_IO,\t\t\t/* meta IOs from checkpoint */\n\tFS_DISCARD,\t\t\t/* discard */\n\tNR_IO_TYPE,\n};\n\nstruct f2fs_io_info {\n\tstruct f2fs_sb_info *sbi;\t/* f2fs_sb_info pointer */\n\tenum page_type type;\t/* contains DATA/NODE/META/META_FLUSH */\n\tenum temp_type temp;\t/* contains HOT/WARM/COLD */\n\tint op;\t\t\t/* contains REQ_OP_ */\n\tint op_flags;\t\t/* req_flag_bits */\n\tblock_t new_blkaddr;\t/* new block address to be written */\n\tblock_t old_blkaddr;\t/* old block address before Cow */\n\tstruct page *page;\t/* page to be written */\n\tstruct page *encrypted_page;\t/* encrypted page */\n\tstruct list_head list;\t\t/* serialize IOs */\n\tbool submitted;\t\t/* indicate IO submission */\n\tint need_lock;\t\t/* indicate we need to lock cp_rwsem */\n\tbool in_list;\t\t/* indicate fio is in io_list */\n\tenum iostat_type io_type;\t/* io type */\n};\n\n#define is_read_io(rw) ((rw) == READ)\nstruct f2fs_bio_info {\n\tstruct f2fs_sb_info *sbi;\t/* f2fs superblock */\n\tstruct bio *bio;\t\t/* bios to merge */\n\tsector_t last_block_in_bio;\t/* last block number */\n\tstruct f2fs_io_info fio;\t/* store buffered io info. */\n\tstruct rw_semaphore io_rwsem;\t/* blocking op for bio */\n\tspinlock_t io_lock;\t\t/* serialize DATA/NODE IOs */\n\tstruct list_head io_list;\t/* track fios */\n};\n\n#define FDEV(i)\t\t\t\t(sbi->devs[i])\n#define RDEV(i)\t\t\t\t(raw_super->devs[i])\nstruct f2fs_dev_info {\n\tstruct block_device *bdev;\n\tchar path[MAX_PATH_LEN];\n\tunsigned int total_segments;\n\tblock_t start_blk;\n\tblock_t end_blk;\n#ifdef CONFIG_BLK_DEV_ZONED\n\tunsigned int nr_blkz;\t\t\t/* Total number of zones */\n\tu8 *blkz_type;\t\t\t\t/* Array of zones type */\n#endif\n};\n\nenum inode_type {\n\tDIR_INODE,\t\t\t/* for dirty dir inode */\n\tFILE_INODE,\t\t\t/* for dirty regular/symlink inode */\n\tDIRTY_META,\t\t\t/* for all dirtied inode metadata */\n\tNR_INODE_TYPE,\n};\n\n/* for inner inode cache management */\nstruct inode_management {\n\tstruct radix_tree_root ino_root;\t/* ino entry array */\n\tspinlock_t ino_lock;\t\t\t/* for ino entry lock */\n\tstruct list_head ino_list;\t\t/* inode list head */\n\tunsigned long ino_num;\t\t\t/* number of entries */\n};\n\n/* For s_flag in struct f2fs_sb_info */\nenum {\n\tSBI_IS_DIRTY,\t\t\t\t/* dirty flag for checkpoint */\n\tSBI_IS_CLOSE,\t\t\t\t/* specify unmounting */\n\tSBI_NEED_FSCK,\t\t\t\t/* need fsck.f2fs to fix */\n\tSBI_POR_DOING,\t\t\t\t/* recovery is doing or not */\n\tSBI_NEED_SB_WRITE,\t\t\t/* need to recover superblock */\n\tSBI_NEED_CP,\t\t\t\t/* need to checkpoint */\n};\n\nenum {\n\tCP_TIME,\n\tREQ_TIME,\n\tMAX_TIME,\n};\n\nstruct f2fs_sb_info {\n\tstruct super_block *sb;\t\t\t/* pointer to VFS super block */\n\tstruct proc_dir_entry *s_proc;\t\t/* proc entry */\n\tstruct f2fs_super_block *raw_super;\t/* raw super block pointer */\n\tint valid_super_block;\t\t\t/* valid super block no */\n\tunsigned long s_flag;\t\t\t\t/* flags for sbi */\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\tunsigned int blocks_per_blkz;\t\t/* F2FS blocks per zone */\n\tunsigned int log_blocks_per_blkz;\t/* log2 F2FS blocks per zone */\n#endif\n\n\t/* for node-related operations */\n\tstruct f2fs_nm_info *nm_info;\t\t/* node manager */\n\tstruct inode *node_inode;\t\t/* cache node blocks */\n\n\t/* for segment-related operations */\n\tstruct f2fs_sm_info *sm_info;\t\t/* segment manager */\n\n\t/* for bio operations */\n\tstruct f2fs_bio_info *write_io[NR_PAGE_TYPE];\t/* for write bios */\n\tstruct mutex wio_mutex[NR_PAGE_TYPE - 1][NR_TEMP_TYPE];\n\t\t\t\t\t\t/* bio ordering for NODE/DATA */\n\tint write_io_size_bits;\t\t\t/* Write IO size bits */\n\tmempool_t *write_io_dummy;\t\t/* Dummy pages */\n\n\t/* for checkpoint */\n\tstruct f2fs_checkpoint *ckpt;\t\t/* raw checkpoint pointer */\n\tint cur_cp_pack;\t\t\t/* remain current cp pack */\n\tspinlock_t cp_lock;\t\t\t/* for flag in ckpt */\n\tstruct inode *meta_inode;\t\t/* cache meta blocks */\n\tstruct mutex cp_mutex;\t\t\t/* checkpoint procedure lock */\n\tstruct rw_semaphore cp_rwsem;\t\t/* blocking FS operations */\n\tstruct rw_semaphore node_write;\t\t/* locking node writes */\n\tstruct rw_semaphore node_change;\t/* locking node change */\n\twait_queue_head_t cp_wait;\n\tunsigned long last_time[MAX_TIME];\t/* to store time in jiffies */\n\tlong interval_time[MAX_TIME];\t\t/* to store thresholds */\n\n\tstruct inode_management im[MAX_INO_ENTRY];      /* manage inode cache */\n\n\t/* for orphan inode, use 0'th array */\n\tunsigned int max_orphans;\t\t/* max orphan inodes */\n\n\t/* for inode management */\n\tstruct list_head inode_list[NR_INODE_TYPE];\t/* dirty inode list */\n\tspinlock_t inode_lock[NR_INODE_TYPE];\t/* for dirty inode list lock */\n\n\t/* for extent tree cache */\n\tstruct radix_tree_root extent_tree_root;/* cache extent cache entries */\n\tstruct mutex extent_tree_lock;\t/* locking extent radix tree */\n\tstruct list_head extent_list;\t\t/* lru list for shrinker */\n\tspinlock_t extent_lock;\t\t\t/* locking extent lru list */\n\tatomic_t total_ext_tree;\t\t/* extent tree count */\n\tstruct list_head zombie_list;\t\t/* extent zombie tree list */\n\tatomic_t total_zombie_tree;\t\t/* extent zombie tree count */\n\tatomic_t total_ext_node;\t\t/* extent info count */\n\n\t/* basic filesystem units */\n\tunsigned int log_sectors_per_block;\t/* log2 sectors per block */\n\tunsigned int log_blocksize;\t\t/* log2 block size */\n\tunsigned int blocksize;\t\t\t/* block size */\n\tunsigned int root_ino_num;\t\t/* root inode number*/\n\tunsigned int node_ino_num;\t\t/* node inode number*/\n\tunsigned int meta_ino_num;\t\t/* meta inode number*/\n\tunsigned int log_blocks_per_seg;\t/* log2 blocks per segment */\n\tunsigned int blocks_per_seg;\t\t/* blocks per segment */\n\tunsigned int segs_per_sec;\t\t/* segments per section */\n\tunsigned int secs_per_zone;\t\t/* sections per zone */\n\tunsigned int total_sections;\t\t/* total section count */\n\tunsigned int total_node_count;\t\t/* total node block count */\n\tunsigned int total_valid_node_count;\t/* valid node block count */\n\tloff_t max_file_blocks;\t\t\t/* max block index of file */\n\tint active_logs;\t\t\t/* # of active logs */\n\tint dir_level;\t\t\t\t/* directory level */\n\n\tblock_t user_block_count;\t\t/* # of user blocks */\n\tblock_t total_valid_block_count;\t/* # of valid blocks */\n\tblock_t discard_blks;\t\t\t/* discard command candidats */\n\tblock_t last_valid_block_count;\t\t/* for recovery */\n\tblock_t reserved_blocks;\t\t/* configurable reserved blocks */\n\n\tu32 s_next_generation;\t\t\t/* for NFS support */\n\n\t/* # of pages, see count_type */\n\tatomic_t nr_pages[NR_COUNT_TYPE];\n\t/* # of allocated blocks */\n\tstruct percpu_counter alloc_valid_block_count;\n\n\t/* writeback control */\n\tatomic_t wb_sync_req;\t\t\t/* count # of WB_SYNC threads */\n\n\t/* valid inode count */\n\tstruct percpu_counter total_valid_inode_count;\n\n\tstruct f2fs_mount_info mount_opt;\t/* mount options */\n\n\t/* for cleaning operations */\n\tstruct mutex gc_mutex;\t\t\t/* mutex for GC */\n\tstruct f2fs_gc_kthread\t*gc_thread;\t/* GC thread */\n\tunsigned int cur_victim_sec;\t\t/* current victim section num */\n\n\t/* threshold for converting bg victims for fg */\n\tu64 fggc_threshold;\n\n\t/* maximum # of trials to find a victim segment for SSR and GC */\n\tunsigned int max_victim_search;\n\n\t/*\n\t * for stat information.\n\t * one is for the LFS mode, and the other is for the SSR mode.\n\t */\n#ifdef CONFIG_F2FS_STAT_FS\n\tstruct f2fs_stat_info *stat_info;\t/* FS status information */\n\tunsigned int segment_count[2];\t\t/* # of allocated segments */\n\tunsigned int block_count[2];\t\t/* # of allocated blocks */\n\tatomic_t inplace_count;\t\t/* # of inplace update */\n\tatomic64_t total_hit_ext;\t\t/* # of lookup extent cache */\n\tatomic64_t read_hit_rbtree;\t\t/* # of hit rbtree extent node */\n\tatomic64_t read_hit_largest;\t\t/* # of hit largest extent node */\n\tatomic64_t read_hit_cached;\t\t/* # of hit cached extent node */\n\tatomic_t inline_xattr;\t\t\t/* # of inline_xattr inodes */\n\tatomic_t inline_inode;\t\t\t/* # of inline_data inodes */\n\tatomic_t inline_dir;\t\t\t/* # of inline_dentry inodes */\n\tatomic_t aw_cnt;\t\t\t/* # of atomic writes */\n\tatomic_t vw_cnt;\t\t\t/* # of volatile writes */\n\tatomic_t max_aw_cnt;\t\t\t/* max # of atomic writes */\n\tatomic_t max_vw_cnt;\t\t\t/* max # of volatile writes */\n\tint bg_gc;\t\t\t\t/* background gc calls */\n\tunsigned int ndirty_inode[NR_INODE_TYPE];\t/* # of dirty inodes */\n#endif\n\tspinlock_t stat_lock;\t\t\t/* lock for stat operations */\n\n\t/* For app/fs IO statistics */\n\tspinlock_t iostat_lock;\n\tunsigned long long write_iostat[NR_IO_TYPE];\n\tbool iostat_enable;\n\n\t/* For sysfs suppport */\n\tstruct kobject s_kobj;\n\tstruct completion s_kobj_unregister;\n\n\t/* For shrinker support */\n\tstruct list_head s_list;\n\tint s_ndevs;\t\t\t\t/* number of devices */\n\tstruct f2fs_dev_info *devs;\t\t/* for device list */\n\tstruct mutex umount_mutex;\n\tunsigned int shrinker_run_no;\n\n\t/* For write statistics */\n\tu64 sectors_written_start;\n\tu64 kbytes_written;\n\n\t/* Reference to checksum algorithm driver via cryptoapi */\n\tstruct crypto_shash *s_chksum_driver;\n\n\t/* Precomputed FS UUID checksum for seeding other checksums */\n\t__u32 s_chksum_seed;\n\n\t/* For fault injection */\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct f2fs_fault_info fault_info;\n#endif\n\n#ifdef CONFIG_QUOTA\n\t/* Names of quota files with journalled quota */\n\tchar *s_qf_names[MAXQUOTAS];\n\tint s_jquota_fmt;\t\t\t/* Format of quota to use */\n#endif\n};\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n#define f2fs_show_injection_info(type)\t\t\t\t\\\n\tprintk(\"%sF2FS-fs : inject %s in %s of %pF\\n\",\t\t\\\n\t\tKERN_INFO, fault_name[type],\t\t\t\\\n\t\t__func__, __builtin_return_address(0))\nstatic inline bool time_to_inject(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_fault_info *ffi = &sbi->fault_info;\n\n\tif (!ffi->inject_rate)\n\t\treturn false;\n\n\tif (!IS_FAULT_SET(ffi, type))\n\t\treturn false;\n\n\tatomic_inc(&ffi->inject_ops);\n\tif (atomic_read(&ffi->inject_ops) >= ffi->inject_rate) {\n\t\tatomic_set(&ffi->inject_ops, 0);\n\t\treturn true;\n\t}\n\treturn false;\n}\n#endif\n\n/* For write statistics. Suppose sector size is 512 bytes,\n * and the return value is in kbytes. s is of struct f2fs_sb_info.\n */\n#define BD_PART_WRITTEN(s)\t\t\t\t\t\t \\\n(((u64)part_stat_read((s)->sb->s_bdev->bd_part, sectors[1]) -\t\t \\\n\t\t(s)->sectors_written_start) >> 1)\n\nstatic inline void f2fs_update_time(struct f2fs_sb_info *sbi, int type)\n{\n\tsbi->last_time[type] = jiffies;\n}\n\nstatic inline bool f2fs_time_over(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct timespec ts = {sbi->interval_time[type], 0};\n\tunsigned long interval = timespec_to_jiffies(&ts);\n\n\treturn time_after(jiffies, sbi->last_time[type] + interval);\n}\n\nstatic inline bool is_idle(struct f2fs_sb_info *sbi)\n{\n\tstruct block_device *bdev = sbi->sb->s_bdev;\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\tstruct request_list *rl = &q->root_rl;\n\n\tif (rl->count[BLK_RW_SYNC] || rl->count[BLK_RW_ASYNC])\n\t\treturn 0;\n\n\treturn f2fs_time_over(sbi, REQ_TIME);\n}\n\n/*\n * Inline functions\n */\nstatic inline u32 f2fs_crc32(struct f2fs_sb_info *sbi, const void *address,\n\t\t\t   unsigned int length)\n{\n\tSHASH_DESC_ON_STACK(shash, sbi->s_chksum_driver);\n\tu32 *ctx = (u32 *)shash_desc_ctx(shash);\n\tu32 retval;\n\tint err;\n\n\tshash->tfm = sbi->s_chksum_driver;\n\tshash->flags = 0;\n\t*ctx = F2FS_SUPER_MAGIC;\n\n\terr = crypto_shash_update(shash, address, length);\n\tBUG_ON(err);\n\n\tretval = *ctx;\n\tbarrier_data(ctx);\n\treturn retval;\n}\n\nstatic inline bool f2fs_crc_valid(struct f2fs_sb_info *sbi, __u32 blk_crc,\n\t\t\t\t  void *buf, size_t buf_size)\n{\n\treturn f2fs_crc32(sbi, buf, buf_size) == blk_crc;\n}\n\nstatic inline u32 f2fs_chksum(struct f2fs_sb_info *sbi, u32 crc,\n\t\t\t      const void *address, unsigned int length)\n{\n\tstruct {\n\t\tstruct shash_desc shash;\n\t\tchar ctx[4];\n\t} desc;\n\tint err;\n\n\tBUG_ON(crypto_shash_descsize(sbi->s_chksum_driver) != sizeof(desc.ctx));\n\n\tdesc.shash.tfm = sbi->s_chksum_driver;\n\tdesc.shash.flags = 0;\n\t*(u32 *)desc.ctx = crc;\n\n\terr = crypto_shash_update(&desc.shash, address, length);\n\tBUG_ON(err);\n\n\treturn *(u32 *)desc.ctx;\n}\n\nstatic inline struct f2fs_inode_info *F2FS_I(struct inode *inode)\n{\n\treturn container_of(inode, struct f2fs_inode_info, vfs_inode);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\nstatic inline struct f2fs_sb_info *F2FS_I_SB(struct inode *inode)\n{\n\treturn F2FS_SB(inode->i_sb);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_M_SB(struct address_space *mapping)\n{\n\treturn F2FS_I_SB(mapping->host);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_P_SB(struct page *page)\n{\n\treturn F2FS_M_SB(page->mapping);\n}\n\nstatic inline struct f2fs_super_block *F2FS_RAW_SUPER(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_super_block *)(sbi->raw_super);\n}\n\nstatic inline struct f2fs_checkpoint *F2FS_CKPT(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_checkpoint *)(sbi->ckpt);\n}\n\nstatic inline struct f2fs_node *F2FS_NODE(struct page *page)\n{\n\treturn (struct f2fs_node *)page_address(page);\n}\n\nstatic inline struct f2fs_inode *F2FS_INODE(struct page *page)\n{\n\treturn &((struct f2fs_node *)page_address(page))->i;\n}\n\nstatic inline struct f2fs_nm_info *NM_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_nm_info *)(sbi->nm_info);\n}\n\nstatic inline struct f2fs_sm_info *SM_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_sm_info *)(sbi->sm_info);\n}\n\nstatic inline struct sit_info *SIT_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct sit_info *)(SM_I(sbi)->sit_info);\n}\n\nstatic inline struct free_segmap_info *FREE_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct free_segmap_info *)(SM_I(sbi)->free_info);\n}\n\nstatic inline struct dirty_seglist_info *DIRTY_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct dirty_seglist_info *)(SM_I(sbi)->dirty_info);\n}\n\nstatic inline struct address_space *META_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->meta_inode->i_mapping;\n}\n\nstatic inline struct address_space *NODE_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->node_inode->i_mapping;\n}\n\nstatic inline bool is_sbi_flag_set(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\treturn test_bit(type, &sbi->s_flag);\n}\n\nstatic inline void set_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\tset_bit(type, &sbi->s_flag);\n}\n\nstatic inline void clear_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\tclear_bit(type, &sbi->s_flag);\n}\n\nstatic inline unsigned long long cur_cp_version(struct f2fs_checkpoint *cp)\n{\n\treturn le64_to_cpu(cp->checkpoint_ver);\n}\n\nstatic inline __u64 cur_cp_crc(struct f2fs_checkpoint *cp)\n{\n\tsize_t crc_offset = le32_to_cpu(cp->checksum_offset);\n\treturn le32_to_cpu(*((__le32 *)((unsigned char *)cp + crc_offset)));\n}\n\nstatic inline bool __is_set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\n\treturn ckpt_flags & f;\n}\n\nstatic inline bool is_set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\treturn __is_set_ckpt_flags(F2FS_CKPT(sbi), f);\n}\n\nstatic inline void __set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags;\n\n\tckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\tckpt_flags |= f;\n\tcp->ckpt_flags = cpu_to_le32(ckpt_flags);\n}\n\nstatic inline void set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__set_ckpt_flags(F2FS_CKPT(sbi), f);\n\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline void __clear_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags;\n\n\tckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\tckpt_flags &= (~f);\n\tcp->ckpt_flags = cpu_to_le32(ckpt_flags);\n}\n\nstatic inline void clear_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__clear_ckpt_flags(F2FS_CKPT(sbi), f);\n\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline void disable_nat_bits(struct f2fs_sb_info *sbi, bool lock)\n{\n\tunsigned long flags;\n\n\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\tif (lock)\n\t\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__clear_ckpt_flags(F2FS_CKPT(sbi), CP_NAT_BITS_FLAG);\n\tkfree(NM_I(sbi)->nat_bits);\n\tNM_I(sbi)->nat_bits = NULL;\n\tif (lock)\n\t\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline bool enabled_nat_bits(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct cp_control *cpc)\n{\n\tbool set = is_set_ckpt_flags(sbi, CP_NAT_BITS_FLAG);\n\n\treturn (cpc) ? (cpc->reason & CP_UMOUNT) && set : set;\n}\n\nstatic inline void f2fs_lock_op(struct f2fs_sb_info *sbi)\n{\n\tdown_read(&sbi->cp_rwsem);\n}\n\nstatic inline int f2fs_trylock_op(struct f2fs_sb_info *sbi)\n{\n\treturn down_read_trylock(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_unlock_op(struct f2fs_sb_info *sbi)\n{\n\tup_read(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_lock_all(struct f2fs_sb_info *sbi)\n{\n\tdown_write(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_unlock_all(struct f2fs_sb_info *sbi)\n{\n\tup_write(&sbi->cp_rwsem);\n}\n\nstatic inline int __get_cp_reason(struct f2fs_sb_info *sbi)\n{\n\tint reason = CP_SYNC;\n\n\tif (test_opt(sbi, FASTBOOT))\n\t\treason = CP_FASTBOOT;\n\tif (is_sbi_flag_set(sbi, SBI_IS_CLOSE))\n\t\treason = CP_UMOUNT;\n\treturn reason;\n}\n\nstatic inline bool __remain_node_summaries(int reason)\n{\n\treturn (reason & (CP_UMOUNT | CP_FASTBOOT));\n}\n\nstatic inline bool __exist_node_summaries(struct f2fs_sb_info *sbi)\n{\n\treturn (is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG) ||\n\t\t\tis_set_ckpt_flags(sbi, CP_FASTBOOT_FLAG));\n}\n\n/*\n * Check whether the given nid is within node id range.\n */\nstatic inline int check_nid_range(struct f2fs_sb_info *sbi, nid_t nid)\n{\n\tif (unlikely(nid < F2FS_ROOT_INO(sbi)))\n\t\treturn -EINVAL;\n\tif (unlikely(nid >= NM_I(sbi)->max_nid))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\n/*\n * Check whether the inode has blocks or not\n */\nstatic inline int F2FS_HAS_BLOCKS(struct inode *inode)\n{\n\tblock_t xattr_block = F2FS_I(inode)->i_xattr_nid ? 1 : 0;\n\n\treturn (inode->i_blocks >> F2FS_LOG_SECTORS_PER_BLOCK) > xattr_block;\n}\n\nstatic inline bool f2fs_has_xattr_block(unsigned int ofs)\n{\n\treturn ofs == XATTR_NODE_OFFSET;\n}\n\nstatic inline void f2fs_i_blocks_write(struct inode *, block_t, bool, bool);\nstatic inline int inc_valid_block_count(struct f2fs_sb_info *sbi,\n\t\t\t\t struct inode *inode, blkcnt_t *count)\n{\n\tblkcnt_t diff = 0, release = 0;\n\tblock_t avail_user_block_count;\n\tint ret;\n\n\tret = dquot_reserve_block(inode, *count);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(sbi, FAULT_BLOCK)) {\n\t\tf2fs_show_injection_info(FAULT_BLOCK);\n\t\trelease = *count;\n\t\tgoto enospc;\n\t}\n#endif\n\t/*\n\t * let's increase this in prior to actual block count change in order\n\t * for f2fs_sync_file to avoid data races when deciding checkpoint.\n\t */\n\tpercpu_counter_add(&sbi->alloc_valid_block_count, (*count));\n\n\tspin_lock(&sbi->stat_lock);\n\tsbi->total_valid_block_count += (block_t)(*count);\n\tavail_user_block_count = sbi->user_block_count - sbi->reserved_blocks;\n\tif (unlikely(sbi->total_valid_block_count > avail_user_block_count)) {\n\t\tdiff = sbi->total_valid_block_count - avail_user_block_count;\n\t\t*count -= diff;\n\t\trelease = diff;\n\t\tsbi->total_valid_block_count = avail_user_block_count;\n\t\tif (!*count) {\n\t\t\tspin_unlock(&sbi->stat_lock);\n\t\t\tpercpu_counter_sub(&sbi->alloc_valid_block_count, diff);\n\t\t\tgoto enospc;\n\t\t}\n\t}\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (release)\n\t\tdquot_release_reservation_block(inode, release);\n\tf2fs_i_blocks_write(inode, *count, true, true);\n\treturn 0;\n\nenospc:\n\tdquot_release_reservation_block(inode, release);\n\treturn -ENOSPC;\n}\n\nstatic inline void dec_valid_block_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tblock_t count)\n{\n\tblkcnt_t sectors = count << F2FS_LOG_SECTORS_PER_BLOCK;\n\n\tspin_lock(&sbi->stat_lock);\n\tf2fs_bug_on(sbi, sbi->total_valid_block_count < (block_t) count);\n\tf2fs_bug_on(sbi, inode->i_blocks < sectors);\n\tsbi->total_valid_block_count -= (block_t)count;\n\tspin_unlock(&sbi->stat_lock);\n\tf2fs_i_blocks_write(inode, count, false, true);\n}\n\nstatic inline void inc_page_count(struct f2fs_sb_info *sbi, int count_type)\n{\n\tatomic_inc(&sbi->nr_pages[count_type]);\n\n\tif (count_type == F2FS_DIRTY_DATA || count_type == F2FS_INMEM_PAGES ||\n\t\tcount_type == F2FS_WB_CP_DATA || count_type == F2FS_WB_DATA)\n\t\treturn;\n\n\tset_sbi_flag(sbi, SBI_IS_DIRTY);\n}\n\nstatic inline void inode_inc_dirty_pages(struct inode *inode)\n{\n\tatomic_inc(&F2FS_I(inode)->dirty_pages);\n\tinc_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?\n\t\t\t\tF2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);\n}\n\nstatic inline void dec_page_count(struct f2fs_sb_info *sbi, int count_type)\n{\n\tatomic_dec(&sbi->nr_pages[count_type]);\n}\n\nstatic inline void inode_dec_dirty_pages(struct inode *inode)\n{\n\tif (!S_ISDIR(inode->i_mode) && !S_ISREG(inode->i_mode) &&\n\t\t\t!S_ISLNK(inode->i_mode))\n\t\treturn;\n\n\tatomic_dec(&F2FS_I(inode)->dirty_pages);\n\tdec_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?\n\t\t\t\tF2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);\n}\n\nstatic inline s64 get_pages(struct f2fs_sb_info *sbi, int count_type)\n{\n\treturn atomic_read(&sbi->nr_pages[count_type]);\n}\n\nstatic inline int get_dirty_pages(struct inode *inode)\n{\n\treturn atomic_read(&F2FS_I(inode)->dirty_pages);\n}\n\nstatic inline int get_blocktype_secs(struct f2fs_sb_info *sbi, int block_type)\n{\n\tunsigned int pages_per_sec = sbi->segs_per_sec * sbi->blocks_per_seg;\n\tunsigned int segs = (get_pages(sbi, block_type) + pages_per_sec - 1) >>\n\t\t\t\t\t\tsbi->log_blocks_per_seg;\n\n\treturn segs / sbi->segs_per_sec;\n}\n\nstatic inline block_t valid_user_blocks(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->total_valid_block_count;\n}\n\nstatic inline block_t discard_blocks(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->discard_blks;\n}\n\nstatic inline unsigned long __bitmap_size(struct f2fs_sb_info *sbi, int flag)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\n\t/* return NAT or SIT bitmap */\n\tif (flag == NAT_BITMAP)\n\t\treturn le32_to_cpu(ckpt->nat_ver_bitmap_bytesize);\n\telse if (flag == SIT_BITMAP)\n\t\treturn le32_to_cpu(ckpt->sit_ver_bitmap_bytesize);\n\n\treturn 0;\n}\n\nstatic inline block_t __cp_payload(struct f2fs_sb_info *sbi)\n{\n\treturn le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_payload);\n}\n\nstatic inline void *__bitmap_ptr(struct f2fs_sb_info *sbi, int flag)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tint offset;\n\n\tif (__cp_payload(sbi) > 0) {\n\t\tif (flag == NAT_BITMAP)\n\t\t\treturn &ckpt->sit_nat_version_bitmap;\n\t\telse\n\t\t\treturn (unsigned char *)ckpt + F2FS_BLKSIZE;\n\t} else {\n\t\toffset = (flag == NAT_BITMAP) ?\n\t\t\tle32_to_cpu(ckpt->sit_ver_bitmap_bytesize) : 0;\n\t\treturn &ckpt->sit_nat_version_bitmap + offset;\n\t}\n}\n\nstatic inline block_t __start_cp_addr(struct f2fs_sb_info *sbi)\n{\n\tblock_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);\n\n\tif (sbi->cur_cp_pack == 2)\n\t\tstart_addr += sbi->blocks_per_seg;\n\treturn start_addr;\n}\n\nstatic inline block_t __start_cp_next_addr(struct f2fs_sb_info *sbi)\n{\n\tblock_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);\n\n\tif (sbi->cur_cp_pack == 1)\n\t\tstart_addr += sbi->blocks_per_seg;\n\treturn start_addr;\n}\n\nstatic inline void __set_cp_next_pack(struct f2fs_sb_info *sbi)\n{\n\tsbi->cur_cp_pack = (sbi->cur_cp_pack == 1) ? 2 : 1;\n}\n\nstatic inline block_t __start_sum_addr(struct f2fs_sb_info *sbi)\n{\n\treturn le32_to_cpu(F2FS_CKPT(sbi)->cp_pack_start_sum);\n}\n\nstatic inline int inc_valid_node_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool is_inode)\n{\n\tblock_t\tvalid_block_count;\n\tunsigned int valid_node_count;\n\tbool quota = inode && !is_inode;\n\n\tif (quota) {\n\t\tint ret = dquot_reserve_block(inode, 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tspin_lock(&sbi->stat_lock);\n\n\tvalid_block_count = sbi->total_valid_block_count + 1;\n\tif (unlikely(valid_block_count + sbi->reserved_blocks >\n\t\t\t\t\t\tsbi->user_block_count)) {\n\t\tspin_unlock(&sbi->stat_lock);\n\t\tgoto enospc;\n\t}\n\n\tvalid_node_count = sbi->total_valid_node_count + 1;\n\tif (unlikely(valid_node_count > sbi->total_node_count)) {\n\t\tspin_unlock(&sbi->stat_lock);\n\t\tgoto enospc;\n\t}\n\n\tsbi->total_valid_node_count++;\n\tsbi->total_valid_block_count++;\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (inode) {\n\t\tif (is_inode)\n\t\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t\telse\n\t\t\tf2fs_i_blocks_write(inode, 1, true, true);\n\t}\n\n\tpercpu_counter_inc(&sbi->alloc_valid_block_count);\n\treturn 0;\n\nenospc:\n\tif (quota)\n\t\tdquot_release_reservation_block(inode, 1);\n\treturn -ENOSPC;\n}\n\nstatic inline void dec_valid_node_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool is_inode)\n{\n\tspin_lock(&sbi->stat_lock);\n\n\tf2fs_bug_on(sbi, !sbi->total_valid_block_count);\n\tf2fs_bug_on(sbi, !sbi->total_valid_node_count);\n\tf2fs_bug_on(sbi, !is_inode && !inode->i_blocks);\n\n\tsbi->total_valid_node_count--;\n\tsbi->total_valid_block_count--;\n\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (!is_inode)\n\t\tf2fs_i_blocks_write(inode, 1, false, true);\n}\n\nstatic inline unsigned int valid_node_count(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->total_valid_node_count;\n}\n\nstatic inline void inc_valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_inc(&sbi->total_valid_inode_count);\n}\n\nstatic inline void dec_valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_dec(&sbi->total_valid_inode_count);\n}\n\nstatic inline s64 valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\treturn percpu_counter_sum_positive(&sbi->total_valid_inode_count);\n}\n\nstatic inline struct page *f2fs_grab_cache_page(struct address_space *mapping,\n\t\t\t\t\t\tpgoff_t index, bool for_write)\n{\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct page *page = find_lock_page(mapping, index);\n\n\tif (page)\n\t\treturn page;\n\n\tif (time_to_inject(F2FS_M_SB(mapping), FAULT_PAGE_ALLOC)) {\n\t\tf2fs_show_injection_info(FAULT_PAGE_ALLOC);\n\t\treturn NULL;\n\t}\n#endif\n\tif (!for_write)\n\t\treturn grab_cache_page(mapping, index);\n\treturn grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);\n}\n\nstatic inline void f2fs_copy_page(struct page *src, struct page *dst)\n{\n\tchar *src_kaddr = kmap(src);\n\tchar *dst_kaddr = kmap(dst);\n\n\tmemcpy(dst_kaddr, src_kaddr, PAGE_SIZE);\n\tkunmap(dst);\n\tkunmap(src);\n}\n\nstatic inline void f2fs_put_page(struct page *page, int unlock)\n{\n\tif (!page)\n\t\treturn;\n\n\tif (unlock) {\n\t\tf2fs_bug_on(F2FS_P_SB(page), !PageLocked(page));\n\t\tunlock_page(page);\n\t}\n\tput_page(page);\n}\n\nstatic inline void f2fs_put_dnode(struct dnode_of_data *dn)\n{\n\tif (dn->node_page)\n\t\tf2fs_put_page(dn->node_page, 1);\n\tif (dn->inode_page && dn->node_page != dn->inode_page)\n\t\tf2fs_put_page(dn->inode_page, 0);\n\tdn->node_page = NULL;\n\tdn->inode_page = NULL;\n}\n\nstatic inline struct kmem_cache *f2fs_kmem_cache_create(const char *name,\n\t\t\t\t\tsize_t size)\n{\n\treturn kmem_cache_create(name, size, 0, SLAB_RECLAIM_ACCOUNT, NULL);\n}\n\nstatic inline void *f2fs_kmem_cache_alloc(struct kmem_cache *cachep,\n\t\t\t\t\t\tgfp_t flags)\n{\n\tvoid *entry;\n\n\tentry = kmem_cache_alloc(cachep, flags);\n\tif (!entry)\n\t\tentry = kmem_cache_alloc(cachep, flags | __GFP_NOFAIL);\n\treturn entry;\n}\n\nstatic inline struct bio *f2fs_bio_alloc(int npages)\n{\n\tstruct bio *bio;\n\n\t/* No failure on bio allocation */\n\tbio = bio_alloc(GFP_NOIO, npages);\n\tif (!bio)\n\t\tbio = bio_alloc(GFP_NOIO | __GFP_NOFAIL, npages);\n\treturn bio;\n}\n\nstatic inline void f2fs_radix_tree_insert(struct radix_tree_root *root,\n\t\t\t\tunsigned long index, void *item)\n{\n\twhile (radix_tree_insert(root, index, item))\n\t\tcond_resched();\n}\n\n#define RAW_IS_INODE(p)\t((p)->footer.nid == (p)->footer.ino)\n\nstatic inline bool IS_INODE(struct page *page)\n{\n\tstruct f2fs_node *p = F2FS_NODE(page);\n\n\treturn RAW_IS_INODE(p);\n}\n\nstatic inline int offset_in_addr(struct f2fs_inode *i)\n{\n\treturn (i->i_inline & F2FS_EXTRA_ATTR) ?\n\t\t\t(le16_to_cpu(i->i_extra_isize) / sizeof(__le32)) : 0;\n}\n\nstatic inline __le32 *blkaddr_in_node(struct f2fs_node *node)\n{\n\treturn RAW_IS_INODE(node) ? node->i.i_addr : node->dn.addr;\n}\n\nstatic inline int f2fs_has_extra_attr(struct inode *inode);\nstatic inline block_t datablock_addr(struct inode *inode,\n\t\t\tstruct page *node_page, unsigned int offset)\n{\n\tstruct f2fs_node *raw_node;\n\t__le32 *addr_array;\n\tint base = 0;\n\tbool is_inode = IS_INODE(node_page);\n\n\traw_node = F2FS_NODE(node_page);\n\n\t/* from GC path only */\n\tif (!inode) {\n\t\tif (is_inode)\n\t\t\tbase = offset_in_addr(&raw_node->i);\n\t} else if (f2fs_has_extra_attr(inode) && is_inode) {\n\t\tbase = get_extra_isize(inode);\n\t}\n\n\taddr_array = blkaddr_in_node(raw_node);\n\treturn le32_to_cpu(addr_array[base + offset]);\n}\n\nstatic inline int f2fs_test_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\treturn mask & *addr;\n}\n\nstatic inline void f2fs_set_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr |= mask;\n}\n\nstatic inline void f2fs_clear_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr &= ~mask;\n}\n\nstatic inline int f2fs_test_and_set_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\tint ret;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\tret = mask & *addr;\n\t*addr |= mask;\n\treturn ret;\n}\n\nstatic inline int f2fs_test_and_clear_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\tint ret;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\tret = mask & *addr;\n\t*addr &= ~mask;\n\treturn ret;\n}\n\nstatic inline void f2fs_change_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr ^= mask;\n}\n\n#define F2FS_REG_FLMASK\t\t(~(FS_DIRSYNC_FL | FS_TOPDIR_FL))\n#define F2FS_OTHER_FLMASK\t(FS_NODUMP_FL | FS_NOATIME_FL)\n#define F2FS_FL_INHERITED\t(FS_PROJINHERIT_FL)\n\nstatic inline __u32 f2fs_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & F2FS_REG_FLMASK;\n\telse\n\t\treturn flags & F2FS_OTHER_FLMASK;\n}\n\n/* used for f2fs_inode_info->flags */\nenum {\n\tFI_NEW_INODE,\t\t/* indicate newly allocated inode */\n\tFI_DIRTY_INODE,\t\t/* indicate inode is dirty or not */\n\tFI_AUTO_RECOVER,\t/* indicate inode is recoverable */\n\tFI_DIRTY_DIR,\t\t/* indicate directory has dirty pages */\n\tFI_INC_LINK,\t\t/* need to increment i_nlink */\n\tFI_ACL_MODE,\t\t/* indicate acl mode */\n\tFI_NO_ALLOC,\t\t/* should not allocate any blocks */\n\tFI_FREE_NID,\t\t/* free allocated nide */\n\tFI_NO_EXTENT,\t\t/* not to use the extent cache */\n\tFI_INLINE_XATTR,\t/* used for inline xattr */\n\tFI_INLINE_DATA,\t\t/* used for inline data*/\n\tFI_INLINE_DENTRY,\t/* used for inline dentry */\n\tFI_APPEND_WRITE,\t/* inode has appended data */\n\tFI_UPDATE_WRITE,\t/* inode has in-place-update data */\n\tFI_NEED_IPU,\t\t/* used for ipu per file */\n\tFI_ATOMIC_FILE,\t\t/* indicate atomic file */\n\tFI_ATOMIC_COMMIT,\t/* indicate the state of atomical committing */\n\tFI_VOLATILE_FILE,\t/* indicate volatile file */\n\tFI_FIRST_BLOCK_WRITTEN,\t/* indicate #0 data block was written */\n\tFI_DROP_CACHE,\t\t/* drop dirty page cache */\n\tFI_DATA_EXIST,\t\t/* indicate data exists */\n\tFI_INLINE_DOTS,\t\t/* indicate inline dot dentries */\n\tFI_DO_DEFRAG,\t\t/* indicate defragment is running */\n\tFI_DIRTY_FILE,\t\t/* indicate regular/symlink has dirty pages */\n\tFI_NO_PREALLOC,\t\t/* indicate skipped preallocated blocks */\n\tFI_HOT_DATA,\t\t/* indicate file is hot */\n\tFI_EXTRA_ATTR,\t\t/* indicate file has extra attribute */\n\tFI_PROJ_INHERIT,\t/* indicate file inherits projectid */\n};\n\nstatic inline void __mark_inode_dirty_flag(struct inode *inode,\n\t\t\t\t\t\tint flag, bool set)\n{\n\tswitch (flag) {\n\tcase FI_INLINE_XATTR:\n\tcase FI_INLINE_DATA:\n\tcase FI_INLINE_DENTRY:\n\t\tif (set)\n\t\t\treturn;\n\tcase FI_DATA_EXIST:\n\tcase FI_INLINE_DOTS:\n\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t}\n}\n\nstatic inline void set_inode_flag(struct inode *inode, int flag)\n{\n\tif (!test_bit(flag, &F2FS_I(inode)->flags))\n\t\tset_bit(flag, &F2FS_I(inode)->flags);\n\t__mark_inode_dirty_flag(inode, flag, true);\n}\n\nstatic inline int is_inode_flag_set(struct inode *inode, int flag)\n{\n\treturn test_bit(flag, &F2FS_I(inode)->flags);\n}\n\nstatic inline void clear_inode_flag(struct inode *inode, int flag)\n{\n\tif (test_bit(flag, &F2FS_I(inode)->flags))\n\t\tclear_bit(flag, &F2FS_I(inode)->flags);\n\t__mark_inode_dirty_flag(inode, flag, false);\n}\n\nstatic inline void set_acl_inode(struct inode *inode, umode_t mode)\n{\n\tF2FS_I(inode)->i_acl_mode = mode;\n\tset_inode_flag(inode, FI_ACL_MODE);\n\tf2fs_mark_inode_dirty_sync(inode, false);\n}\n\nstatic inline void f2fs_i_links_write(struct inode *inode, bool inc)\n{\n\tif (inc)\n\t\tinc_nlink(inode);\n\telse\n\t\tdrop_nlink(inode);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_blocks_write(struct inode *inode,\n\t\t\t\t\tblock_t diff, bool add, bool claim)\n{\n\tbool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);\n\tbool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);\n\n\t/* add = 1, claim = 1 should be dquot_reserve_block in pair */\n\tif (add) {\n\t\tif (claim)\n\t\t\tdquot_claim_block(inode, diff);\n\t\telse\n\t\t\tdquot_alloc_block_nofail(inode, diff);\n\t} else {\n\t\tdquot_free_block(inode, diff);\n\t}\n\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (clean || recover)\n\t\tset_inode_flag(inode, FI_AUTO_RECOVER);\n}\n\nstatic inline void f2fs_i_size_write(struct inode *inode, loff_t i_size)\n{\n\tbool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);\n\tbool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);\n\n\tif (i_size_read(inode) == i_size)\n\t\treturn;\n\n\ti_size_write(inode, i_size);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (clean || recover)\n\t\tset_inode_flag(inode, FI_AUTO_RECOVER);\n}\n\nstatic inline void f2fs_i_depth_write(struct inode *inode, unsigned int depth)\n{\n\tF2FS_I(inode)->i_current_depth = depth;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_xnid_write(struct inode *inode, nid_t xnid)\n{\n\tF2FS_I(inode)->i_xattr_nid = xnid;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_pino_write(struct inode *inode, nid_t pino)\n{\n\tF2FS_I(inode)->i_pino = pino;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void get_inline_info(struct inode *inode, struct f2fs_inode *ri)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tif (ri->i_inline & F2FS_INLINE_XATTR)\n\t\tset_bit(FI_INLINE_XATTR, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DATA)\n\t\tset_bit(FI_INLINE_DATA, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DENTRY)\n\t\tset_bit(FI_INLINE_DENTRY, &fi->flags);\n\tif (ri->i_inline & F2FS_DATA_EXIST)\n\t\tset_bit(FI_DATA_EXIST, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DOTS)\n\t\tset_bit(FI_INLINE_DOTS, &fi->flags);\n\tif (ri->i_inline & F2FS_EXTRA_ATTR)\n\t\tset_bit(FI_EXTRA_ATTR, &fi->flags);\n}\n\nstatic inline void set_raw_inline(struct inode *inode, struct f2fs_inode *ri)\n{\n\tri->i_inline = 0;\n\n\tif (is_inode_flag_set(inode, FI_INLINE_XATTR))\n\t\tri->i_inline |= F2FS_INLINE_XATTR;\n\tif (is_inode_flag_set(inode, FI_INLINE_DATA))\n\t\tri->i_inline |= F2FS_INLINE_DATA;\n\tif (is_inode_flag_set(inode, FI_INLINE_DENTRY))\n\t\tri->i_inline |= F2FS_INLINE_DENTRY;\n\tif (is_inode_flag_set(inode, FI_DATA_EXIST))\n\t\tri->i_inline |= F2FS_DATA_EXIST;\n\tif (is_inode_flag_set(inode, FI_INLINE_DOTS))\n\t\tri->i_inline |= F2FS_INLINE_DOTS;\n\tif (is_inode_flag_set(inode, FI_EXTRA_ATTR))\n\t\tri->i_inline |= F2FS_EXTRA_ATTR;\n}\n\nstatic inline int f2fs_has_extra_attr(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_EXTRA_ATTR);\n}\n\nstatic inline int f2fs_has_inline_xattr(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_XATTR);\n}\n\nstatic inline unsigned int addrs_per_inode(struct inode *inode)\n{\n\tif (f2fs_has_inline_xattr(inode))\n\t\treturn CUR_ADDRS_PER_INODE(inode) - F2FS_INLINE_XATTR_ADDRS;\n\treturn CUR_ADDRS_PER_INODE(inode);\n}\n\nstatic inline void *inline_xattr_addr(struct page *page)\n{\n\tstruct f2fs_inode *ri = F2FS_INODE(page);\n\n\treturn (void *)&(ri->i_addr[DEF_ADDRS_PER_INODE -\n\t\t\t\t\tF2FS_INLINE_XATTR_ADDRS]);\n}\n\nstatic inline int inline_xattr_size(struct inode *inode)\n{\n\tif (f2fs_has_inline_xattr(inode))\n\t\treturn F2FS_INLINE_XATTR_ADDRS << 2;\n\telse\n\t\treturn 0;\n}\n\nstatic inline int f2fs_has_inline_data(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DATA);\n}\n\nstatic inline int f2fs_exist_data(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_DATA_EXIST);\n}\n\nstatic inline int f2fs_has_inline_dots(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DOTS);\n}\n\nstatic inline bool f2fs_is_atomic_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_ATOMIC_FILE);\n}\n\nstatic inline bool f2fs_is_commit_atomic_write(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_ATOMIC_COMMIT);\n}\n\nstatic inline bool f2fs_is_volatile_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_VOLATILE_FILE);\n}\n\nstatic inline bool f2fs_is_first_block_written(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_FIRST_BLOCK_WRITTEN);\n}\n\nstatic inline bool f2fs_is_drop_cache(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_DROP_CACHE);\n}\n\nstatic inline void *inline_data_addr(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode *ri = F2FS_INODE(page);\n\tint extra_size = get_extra_isize(inode);\n\n\treturn (void *)&(ri->i_addr[extra_size + DEF_INLINE_RESERVED_SIZE]);\n}\n\nstatic inline int f2fs_has_inline_dentry(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DENTRY);\n}\n\nstatic inline void f2fs_dentry_kunmap(struct inode *dir, struct page *page)\n{\n\tif (!f2fs_has_inline_dentry(dir))\n\t\tkunmap(page);\n}\n\nstatic inline int is_file(struct inode *inode, int type)\n{\n\treturn F2FS_I(inode)->i_advise & type;\n}\n\nstatic inline void set_file(struct inode *inode, int type)\n{\n\tF2FS_I(inode)->i_advise |= type;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void clear_file(struct inode *inode, int type)\n{\n\tF2FS_I(inode)->i_advise &= ~type;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline bool f2fs_skip_inode_update(struct inode *inode, int dsync)\n{\n\tif (dsync) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\t\tbool ret;\n\n\t\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\t\tret = list_empty(&F2FS_I(inode)->gdirty_list);\n\t\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\t\treturn ret;\n\t}\n\tif (!is_inode_flag_set(inode, FI_AUTO_RECOVER) ||\n\t\t\tfile_keep_isize(inode) ||\n\t\t\ti_size_read(inode) & PAGE_MASK)\n\t\treturn false;\n\treturn F2FS_I(inode)->last_disk_size == i_size_read(inode);\n}\n\nstatic inline int f2fs_readonly(struct super_block *sb)\n{\n\treturn sb->s_flags & MS_RDONLY;\n}\n\nstatic inline bool f2fs_cp_error(struct f2fs_sb_info *sbi)\n{\n\treturn is_set_ckpt_flags(sbi, CP_ERROR_FLAG);\n}\n\nstatic inline bool is_dot_dotdot(const struct qstr *str)\n{\n\tif (str->len == 1 && str->name[0] == '.')\n\t\treturn true;\n\n\tif (str->len == 2 && str->name[0] == '.' && str->name[1] == '.')\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool f2fs_may_extent_tree(struct inode *inode)\n{\n\tif (!test_opt(F2FS_I_SB(inode), EXTENT_CACHE) ||\n\t\t\tis_inode_flag_set(inode, FI_NO_EXTENT))\n\t\treturn false;\n\n\treturn S_ISREG(inode->i_mode);\n}\n\nstatic inline void *f2fs_kmalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(sbi, FAULT_KMALLOC)) {\n\t\tf2fs_show_injection_info(FAULT_KMALLOC);\n\t\treturn NULL;\n\t}\n#endif\n\treturn kmalloc(size, flags);\n}\n\nstatic inline int get_extra_isize(struct inode *inode)\n{\n\treturn F2FS_I(inode)->i_extra_isize / sizeof(__le32);\n}\n\n#define get_inode_mode(i) \\\n\t((is_inode_flag_set(i, FI_ACL_MODE)) ? \\\n\t (F2FS_I(i)->i_acl_mode) : ((i)->i_mode))\n\n#define F2FS_TOTAL_EXTRA_ATTR_SIZE\t\t\t\\\n\t(offsetof(struct f2fs_inode, i_extra_end) -\t\\\n\toffsetof(struct f2fs_inode, i_extra_isize))\t\\\n\n#define F2FS_OLD_ATTRIBUTE_SIZE\t(offsetof(struct f2fs_inode, i_addr))\n#define F2FS_FITS_IN_INODE(f2fs_inode, extra_isize, field)\t\t\\\n\t\t((offsetof(typeof(*f2fs_inode), field) +\t\\\n\t\tsizeof((f2fs_inode)->field))\t\t\t\\\n\t\t<= (F2FS_OLD_ATTRIBUTE_SIZE + extra_isize))\t\\\n\nstatic inline void f2fs_reset_iostat(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tspin_lock(&sbi->iostat_lock);\n\tfor (i = 0; i < NR_IO_TYPE; i++)\n\t\tsbi->write_iostat[i] = 0;\n\tspin_unlock(&sbi->iostat_lock);\n}\n\nstatic inline void f2fs_update_iostat(struct f2fs_sb_info *sbi,\n\t\t\tenum iostat_type type, unsigned long long io_bytes)\n{\n\tif (!sbi->iostat_enable)\n\t\treturn;\n\tspin_lock(&sbi->iostat_lock);\n\tsbi->write_iostat[type] += io_bytes;\n\n\tif (type == APP_WRITE_IO || type == APP_DIRECT_IO)\n\t\tsbi->write_iostat[APP_BUFFERED_IO] =\n\t\t\tsbi->write_iostat[APP_WRITE_IO] -\n\t\t\tsbi->write_iostat[APP_DIRECT_IO];\n\tspin_unlock(&sbi->iostat_lock);\n}\n\n/*\n * file.c\n */\nint f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync);\nvoid truncate_data_blocks(struct dnode_of_data *dn);\nint truncate_blocks(struct inode *inode, u64 from, bool lock);\nint f2fs_truncate(struct inode *inode);\nint f2fs_getattr(const struct path *path, struct kstat *stat,\n\t\t\tu32 request_mask, unsigned int flags);\nint f2fs_setattr(struct dentry *dentry, struct iattr *attr);\nint truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end);\nint truncate_data_blocks_range(struct dnode_of_data *dn, int count);\nlong f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);\nlong f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\n\n/*\n * inode.c\n */\nvoid f2fs_set_inode_flags(struct inode *inode);\nbool f2fs_inode_chksum_verify(struct f2fs_sb_info *sbi, struct page *page);\nvoid f2fs_inode_chksum_set(struct f2fs_sb_info *sbi, struct page *page);\nstruct inode *f2fs_iget(struct super_block *sb, unsigned long ino);\nstruct inode *f2fs_iget_retry(struct super_block *sb, unsigned long ino);\nint try_to_free_nats(struct f2fs_sb_info *sbi, int nr_shrink);\nint update_inode(struct inode *inode, struct page *node_page);\nint update_inode_page(struct inode *inode);\nint f2fs_write_inode(struct inode *inode, struct writeback_control *wbc);\nvoid f2fs_evict_inode(struct inode *inode);\nvoid handle_failed_inode(struct inode *inode);\n\n/*\n * namei.c\n */\nstruct dentry *f2fs_get_parent(struct dentry *child);\n\n/*\n * dir.c\n */\nvoid set_de_type(struct f2fs_dir_entry *de, umode_t mode);\nunsigned char get_de_type(struct f2fs_dir_entry *de);\nstruct f2fs_dir_entry *find_target_dentry(struct fscrypt_name *fname,\n\t\t\tf2fs_hash_t namehash, int *max_slots,\n\t\t\tstruct f2fs_dentry_ptr *d);\nint f2fs_fill_dentries(struct dir_context *ctx, struct f2fs_dentry_ptr *d,\n\t\t\tunsigned int start_pos, struct fscrypt_str *fstr);\nvoid do_make_empty_dir(struct inode *inode, struct inode *parent,\n\t\t\tstruct f2fs_dentry_ptr *d);\nstruct page *init_inode_metadata(struct inode *inode, struct inode *dir,\n\t\t\tconst struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name, struct page *dpage);\nvoid update_parent_metadata(struct inode *dir, struct inode *inode,\n\t\t\tunsigned int current_depth);\nint room_for_filename(const void *bitmap, int slots, int max_slots);\nvoid f2fs_drop_nlink(struct inode *dir, struct inode *inode);\nstruct f2fs_dir_entry *__f2fs_find_entry(struct inode *dir,\n\t\t\tstruct fscrypt_name *fname, struct page **res_page);\nstruct f2fs_dir_entry *f2fs_find_entry(struct inode *dir,\n\t\t\tconst struct qstr *child, struct page **res_page);\nstruct f2fs_dir_entry *f2fs_parent_dir(struct inode *dir, struct page **p);\nino_t f2fs_inode_by_name(struct inode *dir, const struct qstr *qstr,\n\t\t\tstruct page **page);\nvoid f2fs_set_link(struct inode *dir, struct f2fs_dir_entry *de,\n\t\t\tstruct page *page, struct inode *inode);\nvoid f2fs_update_dentry(nid_t ino, umode_t mode, struct f2fs_dentry_ptr *d,\n\t\t\tconst struct qstr *name, f2fs_hash_t name_hash,\n\t\t\tunsigned int bit_pos);\nint f2fs_add_regular_entry(struct inode *dir, const struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nint __f2fs_do_add_link(struct inode *dir, struct fscrypt_name *fname,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nint __f2fs_add_link(struct inode *dir, const struct qstr *name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nvoid f2fs_delete_entry(struct f2fs_dir_entry *dentry, struct page *page,\n\t\t\tstruct inode *dir, struct inode *inode);\nint f2fs_do_tmpfile(struct inode *inode, struct inode *dir);\nbool f2fs_empty_dir(struct inode *dir);\n\nstatic inline int f2fs_add_link(struct dentry *dentry, struct inode *inode)\n{\n\treturn __f2fs_add_link(d_inode(dentry->d_parent), &dentry->d_name,\n\t\t\t\tinode, inode->i_ino, inode->i_mode);\n}\n\n/*\n * super.c\n */\nint f2fs_inode_dirtied(struct inode *inode, bool sync);\nvoid f2fs_inode_synced(struct inode *inode);\nvoid f2fs_enable_quota_files(struct f2fs_sb_info *sbi);\nvoid f2fs_quota_off_umount(struct super_block *sb);\nint f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover);\nint f2fs_sync_fs(struct super_block *sb, int sync);\nextern __printf(3, 4)\nvoid f2fs_msg(struct super_block *sb, const char *level, const char *fmt, ...);\nint sanity_check_ckpt(struct f2fs_sb_info *sbi);\n\n/*\n * hash.c\n */\nf2fs_hash_t f2fs_dentry_hash(const struct qstr *name_info,\n\t\t\t\tstruct fscrypt_name *fname);\n\n/*\n * node.c\n */\nstruct dnode_of_data;\nstruct node_info;\n\nbool available_free_memory(struct f2fs_sb_info *sbi, int type);\nint need_dentry_mark(struct f2fs_sb_info *sbi, nid_t nid);\nbool is_checkpointed_node(struct f2fs_sb_info *sbi, nid_t nid);\nbool need_inode_block_update(struct f2fs_sb_info *sbi, nid_t ino);\nvoid get_node_info(struct f2fs_sb_info *sbi, nid_t nid, struct node_info *ni);\npgoff_t get_next_page_offset(struct dnode_of_data *dn, pgoff_t pgofs);\nint get_dnode_of_data(struct dnode_of_data *dn, pgoff_t index, int mode);\nint truncate_inode_blocks(struct inode *inode, pgoff_t from);\nint truncate_xattr_node(struct inode *inode, struct page *page);\nint wait_on_node_pages_writeback(struct f2fs_sb_info *sbi, nid_t ino);\nint remove_inode_page(struct inode *inode);\nstruct page *new_inode_page(struct inode *inode);\nstruct page *new_node_page(struct dnode_of_data *dn, unsigned int ofs);\nvoid ra_node_page(struct f2fs_sb_info *sbi, nid_t nid);\nstruct page *get_node_page(struct f2fs_sb_info *sbi, pgoff_t nid);\nstruct page *get_node_page_ra(struct page *parent, int start);\nvoid move_node_page(struct page *node_page, int gc_type);\nint fsync_node_pages(struct f2fs_sb_info *sbi, struct inode *inode,\n\t\t\tstruct writeback_control *wbc, bool atomic);\nint sync_node_pages(struct f2fs_sb_info *sbi, struct writeback_control *wbc,\n\t\t\tbool do_balance, enum iostat_type io_type);\nvoid build_free_nids(struct f2fs_sb_info *sbi, bool sync, bool mount);\nbool alloc_nid(struct f2fs_sb_info *sbi, nid_t *nid);\nvoid alloc_nid_done(struct f2fs_sb_info *sbi, nid_t nid);\nvoid alloc_nid_failed(struct f2fs_sb_info *sbi, nid_t nid);\nint try_to_free_nids(struct f2fs_sb_info *sbi, int nr_shrink);\nvoid recover_inline_xattr(struct inode *inode, struct page *page);\nint recover_xattr_data(struct inode *inode, struct page *page,\n\t\t\tblock_t blkaddr);\nint recover_inode_page(struct f2fs_sb_info *sbi, struct page *page);\nint restore_node_summary(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int segno, struct f2fs_summary_block *sum);\nvoid flush_nat_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nint build_node_manager(struct f2fs_sb_info *sbi);\nvoid destroy_node_manager(struct f2fs_sb_info *sbi);\nint __init create_node_manager_caches(void);\nvoid destroy_node_manager_caches(void);\n\n/*\n * segment.c\n */\nbool need_SSR(struct f2fs_sb_info *sbi);\nvoid register_inmem_page(struct inode *inode, struct page *page);\nvoid drop_inmem_pages(struct inode *inode);\nvoid drop_inmem_page(struct inode *inode, struct page *page);\nint commit_inmem_pages(struct inode *inode);\nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need);\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi);\nint f2fs_issue_flush(struct f2fs_sb_info *sbi);\nint create_flush_cmd_control(struct f2fs_sb_info *sbi);\nvoid destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free);\nvoid invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr);\nbool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new);\nvoid stop_discard_thread(struct f2fs_sb_info *sbi);\nvoid f2fs_wait_discard_bios(struct f2fs_sb_info *sbi);\nvoid clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nvoid release_discard_addrs(struct f2fs_sb_info *sbi);\nint npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra);\nvoid allocate_new_segments(struct f2fs_sb_info *sbi);\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range);\nbool exist_trim_candidates(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nstruct page *get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno);\nvoid update_meta_page(struct f2fs_sb_info *sbi, void *src, block_t blk_addr);\nvoid write_meta_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\t\tenum iostat_type io_type);\nvoid write_node_page(unsigned int nid, struct f2fs_io_info *fio);\nvoid write_data_page(struct dnode_of_data *dn, struct f2fs_io_info *fio);\nint rewrite_data_page(struct f2fs_io_info *fio);\nvoid __f2fs_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\tbool recover_curseg, bool recover_newaddr);\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\tunsigned char version, bool recover_curseg,\n\t\t\tbool recover_newaddr);\nvoid allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\t\tstruct f2fs_summary *sum, int type,\n\t\t\tstruct f2fs_io_info *fio, bool add_list);\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\tenum page_type type, bool ordered);\nvoid f2fs_wait_on_block_writeback(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk);\nvoid write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk);\nint lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\tunsigned int val, int alloc);\nvoid flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nint build_segment_manager(struct f2fs_sb_info *sbi);\nvoid destroy_segment_manager(struct f2fs_sb_info *sbi);\nint __init create_segment_manager_caches(void);\nvoid destroy_segment_manager_caches(void);\n\n/*\n * checkpoint.c\n */\nvoid f2fs_stop_checkpoint(struct f2fs_sb_info *sbi, bool end_io);\nstruct page *grab_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *get_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *get_tmp_page(struct f2fs_sb_info *sbi, pgoff_t index);\nbool is_valid_blkaddr(struct f2fs_sb_info *sbi, block_t blkaddr, int type);\nint ra_meta_pages(struct f2fs_sb_info *sbi, block_t start, int nrpages,\n\t\t\tint type, bool sync);\nvoid ra_meta_pages_cond(struct f2fs_sb_info *sbi, pgoff_t index);\nlong sync_meta_pages(struct f2fs_sb_info *sbi, enum page_type type,\n\t\t\tlong nr_to_write, enum iostat_type io_type);\nvoid add_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);\nvoid remove_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);\nvoid release_ino_entry(struct f2fs_sb_info *sbi, bool all);\nbool exist_written_data(struct f2fs_sb_info *sbi, nid_t ino, int mode);\nint f2fs_sync_inode_meta(struct f2fs_sb_info *sbi);\nint acquire_orphan_inode(struct f2fs_sb_info *sbi);\nvoid release_orphan_inode(struct f2fs_sb_info *sbi);\nvoid add_orphan_inode(struct inode *inode);\nvoid remove_orphan_inode(struct f2fs_sb_info *sbi, nid_t ino);\nint recover_orphan_inodes(struct f2fs_sb_info *sbi);\nint get_valid_checkpoint(struct f2fs_sb_info *sbi);\nvoid update_dirty_page(struct inode *inode, struct page *page);\nvoid remove_dirty_inode(struct inode *inode);\nint sync_dirty_inodes(struct f2fs_sb_info *sbi, enum inode_type type);\nint write_checkpoint(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nvoid init_ino_entry_info(struct f2fs_sb_info *sbi);\nint __init create_checkpoint_caches(void);\nvoid destroy_checkpoint_caches(void);\n\n/*\n * data.c\n */\nvoid f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type);\nvoid f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, nid_t ino, pgoff_t idx,\n\t\t\t\tenum page_type type);\nvoid f2fs_flush_merged_writes(struct f2fs_sb_info *sbi);\nint f2fs_submit_page_bio(struct f2fs_io_info *fio);\nint f2fs_submit_page_write(struct f2fs_io_info *fio);\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\t\tblock_t blk_addr, struct bio *bio);\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid set_data_blkaddr(struct dnode_of_data *dn);\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr);\nint reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count);\nint reserve_new_block(struct dnode_of_data *dn);\nint f2fs_get_block(struct dnode_of_data *dn, pgoff_t index);\nint f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from);\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index);\nstruct page *get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\tint op_flags, bool for_write);\nstruct page *find_data_page(struct inode *inode, pgoff_t index);\nstruct page *get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\tbool for_write);\nstruct page *get_new_data_page(struct inode *inode,\n\t\t\tstruct page *ipage, pgoff_t index, bool new_i_size);\nint do_write_data_page(struct f2fs_io_info *fio);\nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,\n\t\t\tint create, int flag);\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t\tu64 start, u64 len);\nvoid f2fs_set_page_dirty_nobuffers(struct page *page);\nint __f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\t\tenum iostat_type io_type);\nvoid f2fs_invalidate_page(struct page *page, unsigned int offset,\n\t\t\tunsigned int length);\nint f2fs_release_page(struct page *page, gfp_t wait);\n#ifdef CONFIG_MIGRATION\nint f2fs_migrate_page(struct address_space *mapping, struct page *newpage,\n\t\t\tstruct page *page, enum migrate_mode mode);\n#endif\n\n/*\n * gc.c\n */\nint start_gc_thread(struct f2fs_sb_info *sbi);\nvoid stop_gc_thread(struct f2fs_sb_info *sbi);\nblock_t start_bidx_of_node(unsigned int node_ofs, struct inode *inode);\nint f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background,\n\t\t\tunsigned int segno);\nvoid build_gc_manager(struct f2fs_sb_info *sbi);\n\n/*\n * recovery.c\n */\nint recover_fsync_data(struct f2fs_sb_info *sbi, bool check_only);\nbool space_for_roll_forward(struct f2fs_sb_info *sbi);\n\n/*\n * debug.c\n */\n#ifdef CONFIG_F2FS_STAT_FS\nstruct f2fs_stat_info {\n\tstruct list_head stat_list;\n\tstruct f2fs_sb_info *sbi;\n\tint all_area_segs, sit_area_segs, nat_area_segs, ssa_area_segs;\n\tint main_area_segs, main_area_sections, main_area_zones;\n\tunsigned long long hit_largest, hit_cached, hit_rbtree;\n\tunsigned long long hit_total, total_ext;\n\tint ext_tree, zombie_tree, ext_node;\n\tint ndirty_node, ndirty_dent, ndirty_meta, ndirty_data, ndirty_imeta;\n\tint inmem_pages;\n\tunsigned int ndirty_dirs, ndirty_files, ndirty_all;\n\tint nats, dirty_nats, sits, dirty_sits;\n\tint free_nids, avail_nids, alloc_nids;\n\tint total_count, utilization;\n\tint bg_gc, nr_wb_cp_data, nr_wb_data;\n\tint nr_flushing, nr_flushed, nr_discarding, nr_discarded;\n\tint nr_discard_cmd;\n\tunsigned int undiscard_blks;\n\tint inline_xattr, inline_inode, inline_dir, append, update, orphans;\n\tint aw_cnt, max_aw_cnt, vw_cnt, max_vw_cnt;\n\tunsigned int valid_count, valid_node_count, valid_inode_count, discard_blks;\n\tunsigned int bimodal, avg_vblocks;\n\tint util_free, util_valid, util_invalid;\n\tint rsvd_segs, overp_segs;\n\tint dirty_count, node_pages, meta_pages;\n\tint prefree_count, call_count, cp_count, bg_cp_count;\n\tint tot_segs, node_segs, data_segs, free_segs, free_secs;\n\tint bg_node_segs, bg_data_segs;\n\tint tot_blks, data_blks, node_blks;\n\tint bg_data_blks, bg_node_blks;\n\tint curseg[NR_CURSEG_TYPE];\n\tint cursec[NR_CURSEG_TYPE];\n\tint curzone[NR_CURSEG_TYPE];\n\n\tunsigned int segment_count[2];\n\tunsigned int block_count[2];\n\tunsigned int inplace_count;\n\tunsigned long long base_mem, cache_mem, page_mem;\n};\n\nstatic inline struct f2fs_stat_info *F2FS_STAT(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_stat_info *)sbi->stat_info;\n}\n\n#define stat_inc_cp_count(si)\t\t((si)->cp_count++)\n#define stat_inc_bg_cp_count(si)\t((si)->bg_cp_count++)\n#define stat_inc_call_count(si)\t\t((si)->call_count++)\n#define stat_inc_bggc_count(sbi)\t((sbi)->bg_gc++)\n#define stat_inc_dirty_inode(sbi, type)\t((sbi)->ndirty_inode[type]++)\n#define stat_dec_dirty_inode(sbi, type)\t((sbi)->ndirty_inode[type]--)\n#define stat_inc_total_hit(sbi)\t\t(atomic64_inc(&(sbi)->total_hit_ext))\n#define stat_inc_rbtree_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_rbtree))\n#define stat_inc_largest_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_largest))\n#define stat_inc_cached_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_cached))\n#define stat_inc_inline_xattr(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_xattr(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_xattr));\t\\\n\t} while (0)\n#define stat_dec_inline_xattr(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_xattr(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_xattr));\t\\\n\t} while (0)\n#define stat_inc_inline_inode(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_data(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_inode));\t\\\n\t} while (0)\n#define stat_dec_inline_inode(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_data(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_inode));\t\\\n\t} while (0)\n#define stat_inc_inline_dir(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_dentry(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_dir));\t\\\n\t} while (0)\n#define stat_dec_inline_dir(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_dentry(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_dir));\t\\\n\t} while (0)\n#define stat_inc_seg_type(sbi, curseg)\t\t\t\t\t\\\n\t\t((sbi)->segment_count[(curseg)->alloc_type]++)\n#define stat_inc_block_count(sbi, curseg)\t\t\t\t\\\n\t\t((sbi)->block_count[(curseg)->alloc_type]++)\n#define stat_inc_inplace_blocks(sbi)\t\t\t\t\t\\\n\t\t(atomic_inc(&(sbi)->inplace_count))\n#define stat_inc_atomic_write(inode)\t\t\t\t\t\\\n\t\t(atomic_inc(&F2FS_I_SB(inode)->aw_cnt))\n#define stat_dec_atomic_write(inode)\t\t\t\t\t\\\n\t\t(atomic_dec(&F2FS_I_SB(inode)->aw_cnt))\n#define stat_update_max_atomic_write(inode)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint cur = atomic_read(&F2FS_I_SB(inode)->aw_cnt);\t\\\n\t\tint max = atomic_read(&F2FS_I_SB(inode)->max_aw_cnt);\t\\\n\t\tif (cur > max)\t\t\t\t\t\t\\\n\t\t\tatomic_set(&F2FS_I_SB(inode)->max_aw_cnt, cur);\t\\\n\t} while (0)\n#define stat_inc_volatile_write(inode)\t\t\t\t\t\\\n\t\t(atomic_inc(&F2FS_I_SB(inode)->vw_cnt))\n#define stat_dec_volatile_write(inode)\t\t\t\t\t\\\n\t\t(atomic_dec(&F2FS_I_SB(inode)->vw_cnt))\n#define stat_update_max_volatile_write(inode)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint cur = atomic_read(&F2FS_I_SB(inode)->vw_cnt);\t\\\n\t\tint max = atomic_read(&F2FS_I_SB(inode)->max_vw_cnt);\t\\\n\t\tif (cur > max)\t\t\t\t\t\t\\\n\t\t\tatomic_set(&F2FS_I_SB(inode)->max_vw_cnt, cur);\t\\\n\t} while (0)\n#define stat_inc_seg_count(sbi, type, gc_type)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tsi->tot_segs++;\t\t\t\t\t\t\\\n\t\tif ((type) == SUM_TYPE_DATA) {\t\t\t\t\\\n\t\t\tsi->data_segs++;\t\t\t\t\\\n\t\t\tsi->bg_data_segs += (gc_type == BG_GC) ? 1 : 0;\t\\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\tsi->node_segs++;\t\t\t\t\\\n\t\t\tsi->bg_node_segs += (gc_type == BG_GC) ? 1 : 0;\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define stat_inc_tot_blk_count(si, blks)\t\t\t\t\\\n\t((si)->tot_blks += (blks))\n\n#define stat_inc_data_blk_count(sbi, blks, gc_type)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tstat_inc_tot_blk_count(si, blks);\t\t\t\\\n\t\tsi->data_blks += (blks);\t\t\t\t\\\n\t\tsi->bg_data_blks += ((gc_type) == BG_GC) ? (blks) : 0;\t\\\n\t} while (0)\n\n#define stat_inc_node_blk_count(sbi, blks, gc_type)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tstat_inc_tot_blk_count(si, blks);\t\t\t\\\n\t\tsi->node_blks += (blks);\t\t\t\t\\\n\t\tsi->bg_node_blks += ((gc_type) == BG_GC) ? (blks) : 0;\t\\\n\t} while (0)\n\nint f2fs_build_stats(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_stats(struct f2fs_sb_info *sbi);\nint __init f2fs_create_root_stats(void);\nvoid f2fs_destroy_root_stats(void);\n#else\n#define stat_inc_cp_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_bg_cp_count(si)\t\t\tdo { } while (0)\n#define stat_inc_call_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_bggc_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_dirty_inode(sbi, type)\t\t\tdo { } while (0)\n#define stat_dec_dirty_inode(sbi, type)\t\t\tdo { } while (0)\n#define stat_inc_total_hit(sb)\t\t\t\tdo { } while (0)\n#define stat_inc_rbtree_node_hit(sb)\t\t\tdo { } while (0)\n#define stat_inc_largest_node_hit(sbi)\t\t\tdo { } while (0)\n#define stat_inc_cached_node_hit(sbi)\t\t\tdo { } while (0)\n#define stat_inc_inline_xattr(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_xattr(inode)\t\t\tdo { } while (0)\n#define stat_inc_inline_inode(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_inode(inode)\t\t\tdo { } while (0)\n#define stat_inc_inline_dir(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_dir(inode)\t\t\tdo { } while (0)\n#define stat_inc_atomic_write(inode)\t\t\tdo { } while (0)\n#define stat_dec_atomic_write(inode)\t\t\tdo { } while (0)\n#define stat_update_max_atomic_write(inode)\t\tdo { } while (0)\n#define stat_inc_volatile_write(inode)\t\t\tdo { } while (0)\n#define stat_dec_volatile_write(inode)\t\t\tdo { } while (0)\n#define stat_update_max_volatile_write(inode)\t\tdo { } while (0)\n#define stat_inc_seg_type(sbi, curseg)\t\t\tdo { } while (0)\n#define stat_inc_block_count(sbi, curseg)\t\tdo { } while (0)\n#define stat_inc_inplace_blocks(sbi)\t\t\tdo { } while (0)\n#define stat_inc_seg_count(sbi, type, gc_type)\t\tdo { } while (0)\n#define stat_inc_tot_blk_count(si, blks)\t\tdo { } while (0)\n#define stat_inc_data_blk_count(sbi, blks, gc_type)\tdo { } while (0)\n#define stat_inc_node_blk_count(sbi, blks, gc_type)\tdo { } while (0)\n\nstatic inline int f2fs_build_stats(struct f2fs_sb_info *sbi) { return 0; }\nstatic inline void f2fs_destroy_stats(struct f2fs_sb_info *sbi) { }\nstatic inline int __init f2fs_create_root_stats(void) { return 0; }\nstatic inline void f2fs_destroy_root_stats(void) { }\n#endif\n\nextern const struct file_operations f2fs_dir_operations;\nextern const struct file_operations f2fs_file_operations;\nextern const struct inode_operations f2fs_file_inode_operations;\nextern const struct address_space_operations f2fs_dblock_aops;\nextern const struct address_space_operations f2fs_node_aops;\nextern const struct address_space_operations f2fs_meta_aops;\nextern const struct inode_operations f2fs_dir_inode_operations;\nextern const struct inode_operations f2fs_symlink_inode_operations;\nextern const struct inode_operations f2fs_encrypted_symlink_inode_operations;\nextern const struct inode_operations f2fs_special_inode_operations;\nextern struct kmem_cache *inode_entry_slab;\n\n/*\n * inline.c\n */\nbool f2fs_may_inline_data(struct inode *inode);\nbool f2fs_may_inline_dentry(struct inode *inode);\nvoid read_inline_data(struct page *page, struct page *ipage);\nvoid truncate_inline_inode(struct inode *inode, struct page *ipage, u64 from);\nint f2fs_read_inline_data(struct inode *inode, struct page *page);\nint f2fs_convert_inline_page(struct dnode_of_data *dn, struct page *page);\nint f2fs_convert_inline_inode(struct inode *inode);\nint f2fs_write_inline_data(struct inode *inode, struct page *page);\nbool recover_inline_data(struct inode *inode, struct page *npage);\nstruct f2fs_dir_entry *find_in_inline_dir(struct inode *dir,\n\t\t\tstruct fscrypt_name *fname, struct page **res_page);\nint make_empty_inline_dir(struct inode *inode, struct inode *parent,\n\t\t\tstruct page *ipage);\nint f2fs_add_inline_entry(struct inode *dir, const struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nvoid f2fs_delete_inline_entry(struct f2fs_dir_entry *dentry, struct page *page,\n\t\t\tstruct inode *dir, struct inode *inode);\nbool f2fs_empty_inline_dir(struct inode *dir);\nint f2fs_read_inline_dir(struct file *file, struct dir_context *ctx,\n\t\t\tstruct fscrypt_str *fstr);\nint f2fs_inline_data_fiemap(struct inode *inode,\n\t\t\tstruct fiemap_extent_info *fieinfo,\n\t\t\t__u64 start, __u64 len);\n\n/*\n * shrinker.c\n */\nunsigned long f2fs_shrink_count(struct shrinker *shrink,\n\t\t\tstruct shrink_control *sc);\nunsigned long f2fs_shrink_scan(struct shrinker *shrink,\n\t\t\tstruct shrink_control *sc);\nvoid f2fs_join_shrinker(struct f2fs_sb_info *sbi);\nvoid f2fs_leave_shrinker(struct f2fs_sb_info *sbi);\n\n/*\n * extent_cache.c\n */\nstruct rb_entry *__lookup_rb_tree(struct rb_root *root,\n\t\t\t\tstruct rb_entry *cached_re, unsigned int ofs);\nstruct rb_node **__lookup_rb_tree_for_insert(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct rb_root *root, struct rb_node **parent,\n\t\t\t\tunsigned int ofs);\nstruct rb_entry *__lookup_rb_tree_ret(struct rb_root *root,\n\t\tstruct rb_entry *cached_re, unsigned int ofs,\n\t\tstruct rb_entry **prev_entry, struct rb_entry **next_entry,\n\t\tstruct rb_node ***insert_p, struct rb_node **insert_parent,\n\t\tbool force);\nbool __check_rb_tree_consistence(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct rb_root *root);\nunsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink);\nbool f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext);\nvoid f2fs_drop_extent_tree(struct inode *inode);\nunsigned int f2fs_destroy_extent_node(struct inode *inode);\nvoid f2fs_destroy_extent_tree(struct inode *inode);\nbool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,\n\t\t\tstruct extent_info *ei);\nvoid f2fs_update_extent_cache(struct dnode_of_data *dn);\nvoid f2fs_update_extent_cache_range(struct dnode_of_data *dn,\n\t\t\tpgoff_t fofs, block_t blkaddr, unsigned int len);\nvoid init_extent_cache_info(struct f2fs_sb_info *sbi);\nint __init create_extent_cache(void);\nvoid destroy_extent_cache(void);\n\n/*\n * sysfs.c\n */\nint __init f2fs_init_sysfs(void);\nvoid f2fs_exit_sysfs(void);\nint f2fs_register_sysfs(struct f2fs_sb_info *sbi);\nvoid f2fs_unregister_sysfs(struct f2fs_sb_info *sbi);\n\n/*\n * crypto support\n */\nstatic inline bool f2fs_encrypted_inode(struct inode *inode)\n{\n\treturn file_is_encrypt(inode);\n}\n\nstatic inline bool f2fs_encrypted_file(struct inode *inode)\n{\n\treturn f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode);\n}\n\nstatic inline void f2fs_set_encrypted_inode(struct inode *inode)\n{\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\n\tfile_set_encrypt(inode);\n#endif\n}\n\nstatic inline bool f2fs_bio_encrypted(struct bio *bio)\n{\n\treturn bio->bi_private != NULL;\n}\n\nstatic inline int f2fs_sb_has_crypto(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_ENCRYPT);\n}\n\nstatic inline int f2fs_sb_mounted_blkzoned(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_BLKZONED);\n}\n\nstatic inline int f2fs_sb_has_extra_attr(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_EXTRA_ATTR);\n}\n\nstatic inline int f2fs_sb_has_project_quota(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_PRJQUOTA);\n}\n\nstatic inline int f2fs_sb_has_inode_chksum(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_INODE_CHKSUM);\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic inline int get_blkz_type(struct f2fs_sb_info *sbi,\n\t\t\tstruct block_device *bdev, block_t blkaddr)\n{\n\tunsigned int zno = blkaddr >> sbi->log_blocks_per_blkz;\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (FDEV(i).bdev == bdev)\n\t\t\treturn FDEV(i).blkz_type[zno];\n\treturn -EINVAL;\n}\n#endif\n\nstatic inline bool f2fs_discard_en(struct f2fs_sb_info *sbi)\n{\n\tstruct request_queue *q = bdev_get_queue(sbi->sb->s_bdev);\n\n\treturn blk_queue_discard(q) || f2fs_sb_mounted_blkzoned(sbi->sb);\n}\n\nstatic inline void set_opt_mode(struct f2fs_sb_info *sbi, unsigned int mt)\n{\n\tclear_opt(sbi, ADAPTIVE);\n\tclear_opt(sbi, LFS);\n\n\tswitch (mt) {\n\tcase F2FS_MOUNT_ADAPTIVE:\n\t\tset_opt(sbi, ADAPTIVE);\n\t\tbreak;\n\tcase F2FS_MOUNT_LFS:\n\t\tset_opt(sbi, LFS);\n\t\tbreak;\n\t}\n}\n\nstatic inline bool f2fs_may_encrypt(struct inode *inode)\n{\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\n\tumode_t mode = inode->i_mode;\n\n\treturn (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode));\n#else\n\treturn 0;\n#endif\n}\n\n#endif\n", "/*\n * fs/f2fs/segment.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/prefetch.h>\n#include <linux/kthread.h>\n#include <linux/swap.h>\n#include <linux/timer.h>\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n\n#include \"f2fs.h\"\n#include \"segment.h\"\n#include \"node.h\"\n#include \"gc.h\"\n#include \"trace.h\"\n#include <trace/events/f2fs.h>\n\n#define __reverse_ffz(x) __reverse_ffs(~(x))\n\nstatic struct kmem_cache *discard_entry_slab;\nstatic struct kmem_cache *discard_cmd_slab;\nstatic struct kmem_cache *sit_entry_set_slab;\nstatic struct kmem_cache *inmem_entry_slab;\n\nstatic unsigned long __reverse_ulong(unsigned char *str)\n{\n\tunsigned long tmp = 0;\n\tint shift = 24, idx = 0;\n\n#if BITS_PER_LONG == 64\n\tshift = 56;\n#endif\n\twhile (shift >= 0) {\n\t\ttmp |= (unsigned long)str[idx++] << shift;\n\t\tshift -= BITS_PER_BYTE;\n\t}\n\treturn tmp;\n}\n\n/*\n * __reverse_ffs is copied from include/asm-generic/bitops/__ffs.h since\n * MSB and LSB are reversed in a byte by f2fs_set_bit.\n */\nstatic inline unsigned long __reverse_ffs(unsigned long word)\n{\n\tint num = 0;\n\n#if BITS_PER_LONG == 64\n\tif ((word & 0xffffffff00000000UL) == 0)\n\t\tnum += 32;\n\telse\n\t\tword >>= 32;\n#endif\n\tif ((word & 0xffff0000) == 0)\n\t\tnum += 16;\n\telse\n\t\tword >>= 16;\n\n\tif ((word & 0xff00) == 0)\n\t\tnum += 8;\n\telse\n\t\tword >>= 8;\n\n\tif ((word & 0xf0) == 0)\n\t\tnum += 4;\n\telse\n\t\tword >>= 4;\n\n\tif ((word & 0xc) == 0)\n\t\tnum += 2;\n\telse\n\t\tword >>= 2;\n\n\tif ((word & 0x2) == 0)\n\t\tnum += 1;\n\treturn num;\n}\n\n/*\n * __find_rev_next(_zero)_bit is copied from lib/find_next_bit.c because\n * f2fs_set_bit makes MSB and LSB reversed in a byte.\n * @size must be integral times of unsigned long.\n * Example:\n *                             MSB <--> LSB\n *   f2fs_set_bit(0, bitmap) => 1000 0000\n *   f2fs_set_bit(7, bitmap) => 0000 0001\n */\nstatic unsigned long __find_rev_next_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == 0)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\ttmp &= ~0UL >> offset;\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp &= (~0UL << (BITS_PER_LONG - size));\n\t\tif (tmp)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffs(tmp);\n}\n\nstatic unsigned long __find_rev_next_zero_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == ~0UL)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\tif (offset)\n\t\t\ttmp |= ~0UL << (BITS_PER_LONG - offset);\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp |= ~0UL >> size;\n\t\tif (tmp != ~0UL)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffz(tmp);\n}\n\nbool need_SSR(struct f2fs_sb_info *sbi)\n{\n\tint node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);\n\tint dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);\n\tint imeta_secs = get_blocktype_secs(sbi, F2FS_DIRTY_IMETA);\n\n\tif (test_opt(sbi, LFS))\n\t\treturn false;\n\tif (sbi->gc_thread && sbi->gc_thread->gc_urgent)\n\t\treturn true;\n\n\treturn free_sections(sbi) <= (node_secs + 2 * dent_secs + imeta_secs +\n\t\t\t\t\t\t2 * reserved_sections(sbi));\n}\n\nvoid register_inmem_page(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inmem_pages *new;\n\n\tf2fs_trace_pid(page);\n\n\tset_page_private(page, (unsigned long)ATOMIC_WRITTEN_PAGE);\n\tSetPagePrivate(page);\n\n\tnew = f2fs_kmem_cache_alloc(inmem_entry_slab, GFP_NOFS);\n\n\t/* add atomic page indices to the list */\n\tnew->page = page;\n\tINIT_LIST_HEAD(&new->list);\n\n\t/* increase reference count with clean state */\n\tmutex_lock(&fi->inmem_lock);\n\tget_page(page);\n\tlist_add_tail(&new->list, &fi->inmem_pages);\n\tinc_page_count(F2FS_I_SB(inode), F2FS_INMEM_PAGES);\n\tmutex_unlock(&fi->inmem_lock);\n\n\ttrace_f2fs_register_inmem_page(page, INMEM);\n}\n\nstatic int __revoke_inmem_pages(struct inode *inode,\n\t\t\t\tstruct list_head *head, bool drop, bool recover)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct inmem_pages *cur, *tmp;\n\tint err = 0;\n\n\tlist_for_each_entry_safe(cur, tmp, head, list) {\n\t\tstruct page *page = cur->page;\n\n\t\tif (drop)\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM_DROP);\n\n\t\tlock_page(page);\n\n\t\tif (recover) {\n\t\t\tstruct dnode_of_data dn;\n\t\t\tstruct node_info ni;\n\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM_REVOKE);\nretry:\n\t\t\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\t\t\terr = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);\n\t\t\tif (err) {\n\t\t\t\tif (err == -ENOMEM) {\n\t\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\t\t\tcond_resched();\n\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tget_node_info(sbi, dn.nid, &ni);\n\t\t\tf2fs_replace_block(sbi, &dn, dn.data_blkaddr,\n\t\t\t\t\tcur->old_addr, ni.version, true, true);\n\t\t\tf2fs_put_dnode(&dn);\n\t\t}\nnext:\n\t\t/* we don't need to invalidate this in the sccessful status */\n\t\tif (drop || recover)\n\t\t\tClearPageUptodate(page);\n\t\tset_page_private(page, 0);\n\t\tClearPagePrivate(page);\n\t\tf2fs_put_page(page, 1);\n\n\t\tlist_del(&cur->list);\n\t\tkmem_cache_free(inmem_entry_slab, cur);\n\t\tdec_page_count(F2FS_I_SB(inode), F2FS_INMEM_PAGES);\n\t}\n\treturn err;\n}\n\nvoid drop_inmem_pages(struct inode *inode)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tmutex_lock(&fi->inmem_lock);\n\t__revoke_inmem_pages(inode, &fi->inmem_pages, true, false);\n\tmutex_unlock(&fi->inmem_lock);\n\n\tclear_inode_flag(inode, FI_ATOMIC_FILE);\n\tclear_inode_flag(inode, FI_HOT_DATA);\n\tstat_dec_atomic_write(inode);\n}\n\nvoid drop_inmem_page(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct list_head *head = &fi->inmem_pages;\n\tstruct inmem_pages *cur = NULL;\n\n\tf2fs_bug_on(sbi, !IS_ATOMIC_WRITTEN_PAGE(page));\n\n\tmutex_lock(&fi->inmem_lock);\n\tlist_for_each_entry(cur, head, list) {\n\t\tif (cur->page == page)\n\t\t\tbreak;\n\t}\n\n\tf2fs_bug_on(sbi, !cur || cur->page != page);\n\tlist_del(&cur->list);\n\tmutex_unlock(&fi->inmem_lock);\n\n\tdec_page_count(sbi, F2FS_INMEM_PAGES);\n\tkmem_cache_free(inmem_entry_slab, cur);\n\n\tClearPageUptodate(page);\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n\tf2fs_put_page(page, 0);\n\n\ttrace_f2fs_commit_inmem_page(page, INMEM_INVALIDATE);\n}\n\nstatic int __commit_inmem_pages(struct inode *inode,\n\t\t\t\t\tstruct list_head *revoke_list)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inmem_pages *cur, *tmp;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_PRIO,\n\t\t.io_type = FS_DATA_IO,\n\t};\n\tpgoff_t last_idx = ULONG_MAX;\n\tint err = 0;\n\n\tlist_for_each_entry_safe(cur, tmp, &fi->inmem_pages, list) {\n\t\tstruct page *page = cur->page;\n\n\t\tlock_page(page);\n\t\tif (page->mapping == inode->i_mapping) {\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM);\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_wait_on_page_writeback(page, DATA, true);\n\t\t\tif (clear_page_dirty_for_io(page)) {\n\t\t\t\tinode_dec_dirty_pages(inode);\n\t\t\t\tremove_dirty_inode(inode);\n\t\t\t}\nretry:\n\t\t\tfio.page = page;\n\t\t\tfio.old_blkaddr = NULL_ADDR;\n\t\t\tfio.encrypted_page = NULL;\n\t\t\tfio.need_lock = LOCK_DONE;\n\t\t\terr = do_write_data_page(&fio);\n\t\t\tif (err) {\n\t\t\t\tif (err == -ENOMEM) {\n\t\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\t\t\tcond_resched();\n\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t\tunlock_page(page);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* record old blkaddr for revoking */\n\t\t\tcur->old_addr = fio.old_blkaddr;\n\t\t\tlast_idx = page->index;\n\t\t}\n\t\tunlock_page(page);\n\t\tlist_move_tail(&cur->list, revoke_list);\n\t}\n\n\tif (last_idx != ULONG_MAX)\n\t\tf2fs_submit_merged_write_cond(sbi, inode, 0, last_idx, DATA);\n\n\tif (!err)\n\t\t__revoke_inmem_pages(inode, revoke_list, false, false);\n\n\treturn err;\n}\n\nint commit_inmem_pages(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct list_head revoke_list;\n\tint err;\n\n\tINIT_LIST_HEAD(&revoke_list);\n\tf2fs_balance_fs(sbi, true);\n\tf2fs_lock_op(sbi);\n\n\tset_inode_flag(inode, FI_ATOMIC_COMMIT);\n\n\tmutex_lock(&fi->inmem_lock);\n\terr = __commit_inmem_pages(inode, &revoke_list);\n\tif (err) {\n\t\tint ret;\n\t\t/*\n\t\t * try to revoke all committed pages, but still we could fail\n\t\t * due to no memory or other reason, if that happened, EAGAIN\n\t\t * will be returned, which means in such case, transaction is\n\t\t * already not integrity, caller should use journal to do the\n\t\t * recovery or rewrite & commit last transaction. For other\n\t\t * error number, revoking was done by filesystem itself.\n\t\t */\n\t\tret = __revoke_inmem_pages(inode, &revoke_list, false, true);\n\t\tif (ret)\n\t\t\terr = ret;\n\n\t\t/* drop all uncommitted pages */\n\t\t__revoke_inmem_pages(inode, &fi->inmem_pages, true, false);\n\t}\n\tmutex_unlock(&fi->inmem_lock);\n\n\tclear_inode_flag(inode, FI_ATOMIC_COMMIT);\n\n\tf2fs_unlock_op(sbi);\n\treturn err;\n}\n\n/*\n * This function balances dirty node and dentry pages.\n * In addition, it controls garbage collection.\n */\nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need)\n{\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(sbi, FAULT_CHECKPOINT)) {\n\t\tf2fs_show_injection_info(FAULT_CHECKPOINT);\n\t\tf2fs_stop_checkpoint(sbi, false);\n\t}\n#endif\n\n\t/* balance_fs_bg is able to be pending */\n\tif (need && excess_cached_nats(sbi))\n\t\tf2fs_balance_fs_bg(sbi);\n\n\t/*\n\t * We should do GC or end up with checkpoint, if there are so many dirty\n\t * dir/node pages without enough free segments.\n\t */\n\tif (has_not_enough_free_secs(sbi, 0, 0)) {\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\tf2fs_gc(sbi, false, false, NULL_SEGNO);\n\t}\n}\n\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi)\n{\n\t/* try to shrink extent cache when there is no enough memory */\n\tif (!available_free_memory(sbi, EXTENT_CACHE))\n\t\tf2fs_shrink_extent_tree(sbi, EXTENT_CACHE_SHRINK_NUMBER);\n\n\t/* check the # of cached NAT entries */\n\tif (!available_free_memory(sbi, NAT_ENTRIES))\n\t\ttry_to_free_nats(sbi, NAT_ENTRY_PER_BLOCK);\n\n\tif (!available_free_memory(sbi, FREE_NIDS))\n\t\ttry_to_free_nids(sbi, MAX_FREE_NIDS);\n\telse\n\t\tbuild_free_nids(sbi, false, false);\n\n\tif (!is_idle(sbi) && !excess_dirty_nats(sbi))\n\t\treturn;\n\n\t/* checkpoint is the only way to shrink partial cached entries */\n\tif (!available_free_memory(sbi, NAT_ENTRIES) ||\n\t\t\t!available_free_memory(sbi, INO_ENTRIES) ||\n\t\t\texcess_prefree_segs(sbi) ||\n\t\t\texcess_dirty_nats(sbi) ||\n\t\t\tf2fs_time_over(sbi, CP_TIME)) {\n\t\tif (test_opt(sbi, DATA_FLUSH)) {\n\t\t\tstruct blk_plug plug;\n\n\t\t\tblk_start_plug(&plug);\n\t\t\tsync_dirty_inodes(sbi, FILE_INODE);\n\t\t\tblk_finish_plug(&plug);\n\t\t}\n\t\tf2fs_sync_fs(sbi->sb, true);\n\t\tstat_inc_bg_cp_count(sbi->stat_info);\n\t}\n}\n\nstatic int __submit_flush_wait(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev)\n{\n\tstruct bio *bio = f2fs_bio_alloc(0);\n\tint ret;\n\n\tbio->bi_opf = REQ_OP_WRITE | REQ_SYNC | REQ_PREFLUSH;\n\tbio_set_dev(bio, bdev);\n\tret = submit_bio_wait(bio);\n\tbio_put(bio);\n\n\ttrace_f2fs_issue_flush(bdev, test_opt(sbi, NOBARRIER),\n\t\t\t\ttest_opt(sbi, FLUSH_MERGE), ret);\n\treturn ret;\n}\n\nstatic int submit_flush_wait(struct f2fs_sb_info *sbi)\n{\n\tint ret = __submit_flush_wait(sbi, sbi->sb->s_bdev);\n\tint i;\n\n\tif (!sbi->s_ndevs || ret)\n\t\treturn ret;\n\n\tfor (i = 1; i < sbi->s_ndevs; i++) {\n\t\tret = __submit_flush_wait(sbi, FDEV(i).bdev);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int issue_flush_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\twait_queue_head_t *q = &fcc->flush_wait_queue;\nrepeat:\n\tif (kthread_should_stop())\n\t\treturn 0;\n\n\tsb_start_intwrite(sbi->sb);\n\n\tif (!llist_empty(&fcc->issue_list)) {\n\t\tstruct flush_cmd *cmd, *next;\n\t\tint ret;\n\n\t\tfcc->dispatch_list = llist_del_all(&fcc->issue_list);\n\t\tfcc->dispatch_list = llist_reverse_order(fcc->dispatch_list);\n\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_inc(&fcc->issued_flush);\n\n\t\tllist_for_each_entry_safe(cmd, next,\n\t\t\t\t\t  fcc->dispatch_list, llnode) {\n\t\t\tcmd->ret = ret;\n\t\t\tcomplete(&cmd->wait);\n\t\t}\n\t\tfcc->dispatch_list = NULL;\n\t}\n\n\tsb_end_intwrite(sbi->sb);\n\n\twait_event_interruptible(*q,\n\t\tkthread_should_stop() || !llist_empty(&fcc->issue_list));\n\tgoto repeat;\n}\n\nint f2fs_issue_flush(struct f2fs_sb_info *sbi)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\tstruct flush_cmd cmd;\n\tint ret;\n\n\tif (test_opt(sbi, NOBARRIER))\n\t\treturn 0;\n\n\tif (!test_opt(sbi, FLUSH_MERGE)) {\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tif (atomic_inc_return(&fcc->issing_flush) == 1) {\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_dec(&fcc->issing_flush);\n\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tinit_completion(&cmd.wait);\n\n\tllist_add(&cmd.llnode, &fcc->issue_list);\n\n\t/* update issue_list before we wake up issue_flush thread */\n\tsmp_mb();\n\n\tif (waitqueue_active(&fcc->flush_wait_queue))\n\t\twake_up(&fcc->flush_wait_queue);\n\n\tif (fcc->f2fs_issue_flush) {\n\t\twait_for_completion(&cmd.wait);\n\t\tatomic_dec(&fcc->issing_flush);\n\t} else {\n\t\tstruct llist_node *list;\n\n\t\tlist = llist_del_all(&fcc->issue_list);\n\t\tif (!list) {\n\t\t\twait_for_completion(&cmd.wait);\n\t\t\tatomic_dec(&fcc->issing_flush);\n\t\t} else {\n\t\t\tstruct flush_cmd *tmp, *next;\n\n\t\t\tret = submit_flush_wait(sbi);\n\n\t\t\tllist_for_each_entry_safe(tmp, next, list, llnode) {\n\t\t\t\tif (tmp == &cmd) {\n\t\t\t\t\tcmd.ret = ret;\n\t\t\t\t\tatomic_dec(&fcc->issing_flush);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\ttmp->ret = ret;\n\t\t\t\tcomplete(&tmp->wait);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn cmd.ret;\n}\n\nint create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\tint err = 0;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tif (fcc->f2fs_issue_flush)\n\t\t\treturn err;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = kzalloc(sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->issing_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\n\tif (!test_opt(sbi, FLUSH_MERGE))\n\t\treturn err;\n\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\terr = PTR_ERR(fcc->f2fs_issue_flush);\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nvoid destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\n\tif (fcc && fcc->f2fs_issue_flush) {\n\t\tstruct task_struct *flush_thread = fcc->f2fs_issue_flush;\n\n\t\tfcc->f2fs_issue_flush = NULL;\n\t\tkthread_stop(flush_thread);\n\t}\n\tif (free) {\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t}\n}\n\nstatic void __locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\t/* need not be added */\n\tif (IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]++;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (unlikely(t >= DIRTY)) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\treturn;\n\t\t}\n\t\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]++;\n\t}\n}\n\nstatic void __remove_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]--;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]--;\n\n\t\tif (get_valid_blocks(sbi, segno, true) == 0)\n\t\t\tclear_bit(GET_SEC_FROM_SEG(sbi, segno),\n\t\t\t\t\t\tdirty_i->victim_secmap);\n\t}\n}\n\n/*\n * Should not occur error such as -ENOMEM.\n * Adding dirty entry into seglist is not critical operation.\n * If a given segment is one of current working segments, it won't be added.\n */\nstatic void locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned short valid_blocks;\n\n\tif (segno == NULL_SEGNO || IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\n\tif (valid_blocks == 0) {\n\t\t__locate_dirty_segment(sbi, segno, PRE);\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t} else if (valid_blocks < sbi->blocks_per_seg) {\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t} else {\n\t\t/* Recovery routine with SSR needs this */\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t}\n\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic struct discard_cmd *__create_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t lstart,\n\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc;\n\n\tf2fs_bug_on(sbi, !len);\n\n\tpend_list = &dcc->pend_list[plist_idx(len)];\n\n\tdc = f2fs_kmem_cache_alloc(discard_cmd_slab, GFP_NOFS);\n\tINIT_LIST_HEAD(&dc->list);\n\tdc->bdev = bdev;\n\tdc->lstart = lstart;\n\tdc->start = start;\n\tdc->len = len;\n\tdc->ref = 0;\n\tdc->state = D_PREP;\n\tdc->error = 0;\n\tinit_completion(&dc->wait);\n\tlist_add_tail(&dc->list, pend_list);\n\tatomic_inc(&dcc->discard_cmd_cnt);\n\tdcc->undiscard_blks += len;\n\n\treturn dc;\n}\n\nstatic struct discard_cmd *__attach_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len,\n\t\t\t\tstruct rb_node *parent, struct rb_node **p)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\n\tdc = __create_discard_cmd(sbi, bdev, lstart, start, len);\n\n\trb_link_node(&dc->rb_node, parent, p);\n\trb_insert_color(&dc->rb_node, &dcc->root);\n\n\treturn dc;\n}\n\nstatic void __detach_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tif (dc->state == D_DONE)\n\t\tatomic_dec(&dcc->issing_discard);\n\n\tlist_del(&dc->list);\n\trb_erase(&dc->rb_node, &dcc->root);\n\tdcc->undiscard_blks -= dc->len;\n\n\tkmem_cache_free(discard_cmd_slab, dc);\n\n\tatomic_dec(&dcc->discard_cmd_cnt);\n}\n\nstatic void __remove_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tf2fs_bug_on(sbi, dc->ref);\n\n\tif (dc->error == -EOPNOTSUPP)\n\t\tdc->error = 0;\n\n\tif (dc->error)\n\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"Issue discard(%u, %u, %u) failed, ret: %d\",\n\t\t\tdc->lstart, dc->start, dc->len, dc->error);\n\t__detach_discard_cmd(dcc, dc);\n}\n\nstatic void f2fs_submit_discard_endio(struct bio *bio)\n{\n\tstruct discard_cmd *dc = (struct discard_cmd *)bio->bi_private;\n\n\tdc->error = blk_status_to_errno(bio->bi_status);\n\tdc->state = D_DONE;\n\tcomplete_all(&dc->wait);\n\tbio_put(bio);\n}\n\nvoid __check_sit_bitmap(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t start, block_t end)\n{\n#ifdef CONFIG_F2FS_CHECK_FS\n\tstruct seg_entry *sentry;\n\tunsigned int segno;\n\tblock_t blk = start;\n\tunsigned long offset, size, max_blocks = sbi->blocks_per_seg;\n\tunsigned long *map;\n\n\twhile (blk < end) {\n\t\tsegno = GET_SEGNO(sbi, blk);\n\t\tsentry = get_seg_entry(sbi, segno);\n\t\toffset = GET_BLKOFF_FROM_SEG0(sbi, blk);\n\n\t\tif (end < START_BLOCK(sbi, segno + 1))\n\t\t\tsize = GET_BLKOFF_FROM_SEG0(sbi, end);\n\t\telse\n\t\t\tsize = max_blocks;\n\t\tmap = (unsigned long *)(sentry->cur_valid_map);\n\t\toffset = __find_rev_next_bit(map, size, offset);\n\t\tf2fs_bug_on(sbi, offset != size);\n\t\tblk = START_BLOCK(sbi, segno + 1);\n\t}\n#endif\n}\n\n/* this function is copied from blkdev_issue_discard from block/blk-lib.c */\nstatic void __submit_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct bio *bio = NULL;\n\n\tif (dc->state != D_PREP)\n\t\treturn;\n\n\ttrace_f2fs_issue_discard(dc->bdev, dc->start, dc->len);\n\n\tdc->error = __blkdev_issue_discard(dc->bdev,\n\t\t\t\tSECTOR_FROM_BLOCK(dc->start),\n\t\t\t\tSECTOR_FROM_BLOCK(dc->len),\n\t\t\t\tGFP_NOFS, 0, &bio);\n\tif (!dc->error) {\n\t\t/* should keep before submission to avoid D_DONE right away */\n\t\tdc->state = D_SUBMIT;\n\t\tatomic_inc(&dcc->issued_discard);\n\t\tatomic_inc(&dcc->issing_discard);\n\t\tif (bio) {\n\t\t\tbio->bi_private = dc;\n\t\t\tbio->bi_end_io = f2fs_submit_discard_endio;\n\t\t\tbio->bi_opf |= REQ_SYNC;\n\t\t\tsubmit_bio(bio);\n\t\t\tlist_move_tail(&dc->list, &dcc->wait_list);\n\t\t\t__check_sit_bitmap(sbi, dc->start, dc->start + dc->len);\n\n\t\t\tf2fs_update_iostat(sbi, FS_DISCARD, 1);\n\t\t}\n\t} else {\n\t\t__remove_discard_cmd(sbi, dc);\n\t}\n}\n\nstatic struct discard_cmd *__insert_discard_tree(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len,\n\t\t\t\tstruct rb_node **insert_p,\n\t\t\t\tstruct rb_node *insert_parent)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct rb_node **p = &dcc->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct discard_cmd *dc = NULL;\n\n\tif (insert_p && insert_parent) {\n\t\tparent = insert_parent;\n\t\tp = insert_p;\n\t\tgoto do_insert;\n\t}\n\n\tp = __lookup_rb_tree_for_insert(sbi, &dcc->root, &parent, lstart);\ndo_insert:\n\tdc = __attach_discard_cmd(sbi, bdev, lstart, start, len, parent, p);\n\tif (!dc)\n\t\treturn NULL;\n\n\treturn dc;\n}\n\nstatic void __relocate_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tlist_move_tail(&dc->list, &dcc->pend_list[plist_idx(dc->len)]);\n}\n\nstatic void __punch_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_info di = dc->di;\n\tbool modified = false;\n\n\tif (dc->state == D_DONE || dc->len == 1) {\n\t\t__remove_discard_cmd(sbi, dc);\n\t\treturn;\n\t}\n\n\tdcc->undiscard_blks -= di.len;\n\n\tif (blkaddr > di.lstart) {\n\t\tdc->len = blkaddr - dc->lstart;\n\t\tdcc->undiscard_blks += dc->len;\n\t\t__relocate_discard_cmd(dcc, dc);\n\t\tmodified = true;\n\t}\n\n\tif (blkaddr < di.lstart + di.len - 1) {\n\t\tif (modified) {\n\t\t\t__insert_discard_tree(sbi, dc->bdev, blkaddr + 1,\n\t\t\t\t\tdi.start + blkaddr + 1 - di.lstart,\n\t\t\t\t\tdi.lstart + di.len - 1 - blkaddr,\n\t\t\t\t\tNULL, NULL);\n\t\t} else {\n\t\t\tdc->lstart++;\n\t\t\tdc->len--;\n\t\t\tdc->start++;\n\t\t\tdcc->undiscard_blks += dc->len;\n\t\t\t__relocate_discard_cmd(dcc, dc);\n\t\t}\n\t}\n}\n\nstatic void __update_discard_tree_range(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *prev_dc = NULL, *next_dc = NULL;\n\tstruct discard_cmd *dc;\n\tstruct discard_info di = {0};\n\tstruct rb_node **insert_p = NULL, *insert_parent = NULL;\n\tblock_t end = lstart + len;\n\n\tmutex_lock(&dcc->cmd_lock);\n\n\tdc = (struct discard_cmd *)__lookup_rb_tree_ret(&dcc->root,\n\t\t\t\t\tNULL, lstart,\n\t\t\t\t\t(struct rb_entry **)&prev_dc,\n\t\t\t\t\t(struct rb_entry **)&next_dc,\n\t\t\t\t\t&insert_p, &insert_parent, true);\n\tif (dc)\n\t\tprev_dc = dc;\n\n\tif (!prev_dc) {\n\t\tdi.lstart = lstart;\n\t\tdi.len = next_dc ? next_dc->lstart - lstart : len;\n\t\tdi.len = min(di.len, len);\n\t\tdi.start = start;\n\t}\n\n\twhile (1) {\n\t\tstruct rb_node *node;\n\t\tbool merged = false;\n\t\tstruct discard_cmd *tdc = NULL;\n\n\t\tif (prev_dc) {\n\t\t\tdi.lstart = prev_dc->lstart + prev_dc->len;\n\t\t\tif (di.lstart < lstart)\n\t\t\t\tdi.lstart = lstart;\n\t\t\tif (di.lstart >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (!next_dc || next_dc->lstart > end)\n\t\t\t\tdi.len = end - di.lstart;\n\t\t\telse\n\t\t\t\tdi.len = next_dc->lstart - di.lstart;\n\t\t\tdi.start = start + di.lstart - lstart;\n\t\t}\n\n\t\tif (!di.len)\n\t\t\tgoto next;\n\n\t\tif (prev_dc && prev_dc->state == D_PREP &&\n\t\t\tprev_dc->bdev == bdev &&\n\t\t\t__is_discard_back_mergeable(&di, &prev_dc->di)) {\n\t\t\tprev_dc->di.len += di.len;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, prev_dc);\n\t\t\tdi = prev_dc->di;\n\t\t\ttdc = prev_dc;\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (next_dc && next_dc->state == D_PREP &&\n\t\t\tnext_dc->bdev == bdev &&\n\t\t\t__is_discard_front_mergeable(&di, &next_dc->di)) {\n\t\t\tnext_dc->di.lstart = di.lstart;\n\t\t\tnext_dc->di.len += di.len;\n\t\t\tnext_dc->di.start = di.start;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, next_dc);\n\t\t\tif (tdc)\n\t\t\t\t__remove_discard_cmd(sbi, tdc);\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (!merged) {\n\t\t\t__insert_discard_tree(sbi, bdev, di.lstart, di.start,\n\t\t\t\t\t\t\tdi.len, NULL, NULL);\n\t\t}\n next:\n\t\tprev_dc = next_dc;\n\t\tif (!prev_dc)\n\t\t\tbreak;\n\n\t\tnode = rb_next(&prev_dc->rb_node);\n\t\tnext_dc = rb_entry_safe(node, struct discard_cmd, rb_node);\n\t}\n\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic int __queue_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tblock_t lblkstart = blkstart;\n\n\ttrace_f2fs_queue_discard(bdev, blkstart, blklen);\n\n\tif (sbi->s_ndevs) {\n\t\tint devi = f2fs_target_device_index(sbi, blkstart);\n\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\t__update_discard_tree_range(sbi, bdev, lblkstart, blkstart, blklen);\n\treturn 0;\n}\n\nstatic int __issue_discard_cmd(struct f2fs_sb_info *sbi, bool issue_cond)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc, *tmp;\n\tstruct blk_plug plug;\n\tint iter = 0, issued = 0;\n\tint i;\n\tbool io_interrupted = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tf2fs_bug_on(sbi,\n\t\t!__check_rb_tree_consistence(sbi, &dcc->root));\n\tblk_start_plug(&plug);\n\tfor (i = MAX_PLIST_NUM - 1;\n\t\t\ti >= 0 && plist_issue(dcc->pend_list_tag[i]); i--) {\n\t\tpend_list = &dcc->pend_list[i];\n\t\tlist_for_each_entry_safe(dc, tmp, pend_list, list) {\n\t\t\tf2fs_bug_on(sbi, dc->state != D_PREP);\n\n\t\t\t/* Hurry up to finish fstrim */\n\t\t\tif (dcc->pend_list_tag[i] & P_TRIM) {\n\t\t\t\t__submit_discard_cmd(sbi, dc);\n\t\t\t\tissued++;\n\n\t\t\t\tif (fatal_signal_pending(current))\n\t\t\t\t\tbreak;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!issue_cond) {\n\t\t\t\t__submit_discard_cmd(sbi, dc);\n\t\t\t\tissued++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (is_idle(sbi)) {\n\t\t\t\t__submit_discard_cmd(sbi, dc);\n\t\t\t\tissued++;\n\t\t\t} else {\n\t\t\t\tio_interrupted = true;\n\t\t\t}\n\n\t\t\tif (++iter >= DISCARD_ISSUE_RATE)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (list_empty(pend_list) && dcc->pend_list_tag[i] & P_TRIM)\n\t\t\tdcc->pend_list_tag[i] &= (~P_TRIM);\n\t}\nout:\n\tblk_finish_plug(&plug);\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (!issued && io_interrupted)\n\t\tissued = -1;\n\n\treturn issued;\n}\n\nstatic void __drop_discard_cmd(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc, *tmp;\n\tint i;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tfor (i = MAX_PLIST_NUM - 1; i >= 0; i--) {\n\t\tpend_list = &dcc->pend_list[i];\n\t\tlist_for_each_entry_safe(dc, tmp, pend_list, list) {\n\t\t\tf2fs_bug_on(sbi, dc->state != D_PREP);\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic void __wait_one_discard_bio(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\twait_for_completion_io(&dc->wait);\n\tmutex_lock(&dcc->cmd_lock);\n\tf2fs_bug_on(sbi, dc->state != D_DONE);\n\tdc->ref--;\n\tif (!dc->ref)\n\t\t__remove_discard_cmd(sbi, dc);\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic void __wait_discard_cmd(struct f2fs_sb_info *sbi, bool wait_cond)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *wait_list = &(dcc->wait_list);\n\tstruct discard_cmd *dc, *tmp;\n\tbool need_wait;\n\nnext:\n\tneed_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tlist_for_each_entry_safe(dc, tmp, wait_list, list) {\n\t\tif (!wait_cond || (dc->state == D_DONE && !dc->ref)) {\n\t\t\twait_for_completion_io(&dc->wait);\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait) {\n\t\t__wait_one_discard_bio(sbi, dc);\n\t\tgoto next;\n\t}\n}\n\n/* This should be covered by global mutex, &sit_i->sentry_lock */\nvoid f2fs_wait_discard_bio(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\tbool need_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tdc = (struct discard_cmd *)__lookup_rb_tree(&dcc->root, NULL, blkaddr);\n\tif (dc) {\n\t\tif (dc->state == D_PREP) {\n\t\t\t__punch_discard_cmd(sbi, dc, blkaddr);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait)\n\t\t__wait_one_discard_bio(sbi, dc);\n}\n\nvoid stop_discard_thread(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (dcc && dcc->f2fs_issue_discard) {\n\t\tstruct task_struct *discard_thread = dcc->f2fs_issue_discard;\n\n\t\tdcc->f2fs_issue_discard = NULL;\n\t\tkthread_stop(discard_thread);\n\t}\n}\n\n/* This comes from f2fs_put_super and f2fs_trim_fs */\nvoid f2fs_wait_discard_bios(struct f2fs_sb_info *sbi)\n{\n\t__issue_discard_cmd(sbi, false);\n\t__drop_discard_cmd(sbi);\n\t__wait_discard_cmd(sbi, false);\n}\n\nstatic void mark_discard_range_all(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tint i;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tfor (i = 0; i < MAX_PLIST_NUM; i++)\n\t\tdcc->pend_list_tag[i] |= P_TRIM;\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic int issue_discard_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\twait_queue_head_t *q = &dcc->discard_wait_queue;\n\tunsigned int wait_ms = DEF_MIN_DISCARD_ISSUE_TIME;\n\tint issued;\n\n\tset_freezable();\n\n\tdo {\n\t\twait_event_interruptible_timeout(*q,\n\t\t\t\tkthread_should_stop() || freezing(current) ||\n\t\t\t\tdcc->discard_wake,\n\t\t\t\tmsecs_to_jiffies(wait_ms));\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\t\tif (kthread_should_stop())\n\t\t\treturn 0;\n\n\t\tif (dcc->discard_wake) {\n\t\t\tdcc->discard_wake = 0;\n\t\t\tif (sbi->gc_thread && sbi->gc_thread->gc_urgent)\n\t\t\t\tmark_discard_range_all(sbi);\n\t\t}\n\n\t\tsb_start_intwrite(sbi->sb);\n\n\t\tissued = __issue_discard_cmd(sbi, true);\n\t\tif (issued) {\n\t\t\t__wait_discard_cmd(sbi, true);\n\t\t\twait_ms = DEF_MIN_DISCARD_ISSUE_TIME;\n\t\t} else {\n\t\t\twait_ms = DEF_MAX_DISCARD_ISSUE_TIME;\n\t\t}\n\n\t\tsb_end_intwrite(sbi->sb);\n\n\t} while (!kthread_should_stop());\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int __f2fs_issue_discard_zone(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tsector_t sector, nr_sects;\n\tblock_t lblkstart = blkstart;\n\tint devi = 0;\n\n\tif (sbi->s_ndevs) {\n\t\tdevi = f2fs_target_device_index(sbi, blkstart);\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\n\t/*\n\t * We need to know the type of the zone: for conventional zones,\n\t * use regular discard if the drive supports it. For sequential\n\t * zones, reset the zone write pointer.\n\t */\n\tswitch (get_blkz_type(sbi, bdev, blkstart)) {\n\n\tcase BLK_ZONE_TYPE_CONVENTIONAL:\n\t\tif (!blk_queue_discard(bdev_get_queue(bdev)))\n\t\t\treturn 0;\n\t\treturn __queue_discard_cmd(sbi, bdev, lblkstart, blklen);\n\tcase BLK_ZONE_TYPE_SEQWRITE_REQ:\n\tcase BLK_ZONE_TYPE_SEQWRITE_PREF:\n\t\tsector = SECTOR_FROM_BLOCK(blkstart);\n\t\tnr_sects = SECTOR_FROM_BLOCK(blklen);\n\n\t\tif (sector & (bdev_zone_sectors(bdev) - 1) ||\n\t\t\t\tnr_sects != bdev_zone_sectors(bdev)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\t\"(%d) %s: Unaligned discard attempted (block %x + %x)\",\n\t\t\t\tdevi, sbi->s_ndevs ? FDEV(devi).path: \"\",\n\t\t\t\tblkstart, blklen);\n\t\t\treturn -EIO;\n\t\t}\n\t\ttrace_f2fs_issue_reset_zone(bdev, blkstart);\n\t\treturn blkdev_reset_zones(bdev, sector,\n\t\t\t\t\t  nr_sects, GFP_NOFS);\n\tdefault:\n\t\t/* Unknown zone type: broken device ? */\n\t\treturn -EIO;\n\t}\n}\n#endif\n\nstatic int __issue_discard_async(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_mounted_blkzoned(sbi->sb) &&\n\t\t\t\tbdev_zoned_model(bdev) != BLK_ZONED_NONE)\n\t\treturn __f2fs_issue_discard_zone(sbi, bdev, blkstart, blklen);\n#endif\n\treturn __queue_discard_cmd(sbi, bdev, blkstart, blklen);\n}\n\nstatic int f2fs_issue_discard(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blkstart, block_t blklen)\n{\n\tsector_t start = blkstart, len = 0;\n\tstruct block_device *bdev;\n\tstruct seg_entry *se;\n\tunsigned int offset;\n\tblock_t i;\n\tint err = 0;\n\n\tbdev = f2fs_target_device(sbi, blkstart, NULL);\n\n\tfor (i = blkstart; i < blkstart + blklen; i++, len++) {\n\t\tif (i != start) {\n\t\t\tstruct block_device *bdev2 =\n\t\t\t\tf2fs_target_device(sbi, i, NULL);\n\n\t\t\tif (bdev2 != bdev) {\n\t\t\t\terr = __issue_discard_async(sbi, bdev,\n\t\t\t\t\t\tstart, len);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tbdev = bdev2;\n\t\t\t\tstart = i;\n\t\t\t\tlen = 0;\n\t\t\t}\n\t\t}\n\n\t\tse = get_seg_entry(sbi, GET_SEGNO(sbi, i));\n\t\toffset = GET_BLKOFF_FROM_SEG0(sbi, i);\n\n\t\tif (!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\t}\n\n\tif (len)\n\t\terr = __issue_discard_async(sbi, bdev, start, len);\n\treturn err;\n}\n\nstatic bool add_discard_addrs(struct f2fs_sb_info *sbi, struct cp_control *cpc,\n\t\t\t\t\t\t\tbool check_only)\n{\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tint max_blocks = sbi->blocks_per_seg;\n\tstruct seg_entry *se = get_seg_entry(sbi, cpc->trim_start);\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *discard_map = (unsigned long *)se->discard_map;\n\tunsigned long *dmap = SIT_I(sbi)->tmp_map;\n\tunsigned int start = 0, end = -1;\n\tbool force = (cpc->reason & CP_DISCARD);\n\tstruct discard_entry *de = NULL;\n\tstruct list_head *head = &SM_I(sbi)->dcc_info->entry_list;\n\tint i;\n\n\tif (se->valid_blocks == max_blocks || !f2fs_discard_en(sbi))\n\t\treturn false;\n\n\tif (!force) {\n\t\tif (!test_opt(sbi, DISCARD) || !se->valid_blocks ||\n\t\t\tSM_I(sbi)->dcc_info->nr_discards >=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards)\n\t\t\treturn false;\n\t}\n\n\t/* SIT_VBLOCK_MAP_SIZE should be multiple of sizeof(unsigned long) */\n\tfor (i = 0; i < entries; i++)\n\t\tdmap[i] = force ? ~ckpt_map[i] & ~discard_map[i] :\n\t\t\t\t(cur_map[i] ^ ckpt_map[i]) & ckpt_map[i];\n\n\twhile (force || SM_I(sbi)->dcc_info->nr_discards <=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards) {\n\t\tstart = __find_rev_next_bit(dmap, max_blocks, end + 1);\n\t\tif (start >= max_blocks)\n\t\t\tbreak;\n\n\t\tend = __find_rev_next_zero_bit(dmap, max_blocks, start + 1);\n\t\tif (force && start && end != max_blocks\n\t\t\t\t\t&& (end - start) < cpc->trim_minlen)\n\t\t\tcontinue;\n\n\t\tif (check_only)\n\t\t\treturn true;\n\n\t\tif (!de) {\n\t\t\tde = f2fs_kmem_cache_alloc(discard_entry_slab,\n\t\t\t\t\t\t\t\tGFP_F2FS_ZERO);\n\t\t\tde->start_blkaddr = START_BLOCK(sbi, cpc->trim_start);\n\t\t\tlist_add_tail(&de->list, head);\n\t\t}\n\n\t\tfor (i = start; i < end; i++)\n\t\t\t__set_bit_le(i, (void *)de->discard_map);\n\n\t\tSM_I(sbi)->dcc_info->nr_discards += end - start;\n\t}\n\treturn false;\n}\n\nvoid release_discard_addrs(struct f2fs_sb_info *sbi)\n{\n\tstruct list_head *head = &(SM_I(sbi)->dcc_info->entry_list);\n\tstruct discard_entry *entry, *this;\n\n\t/* drop caches */\n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tlist_del(&entry->list);\n\t\tkmem_cache_free(discard_entry_slab, entry);\n\t}\n}\n\n/*\n * Should call clear_prefree_segments after checkpoint is done.\n */\nstatic void set_prefree_as_free_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor_each_set_bit(segno, dirty_i->dirty_segmap[PRE], MAIN_SEGS(sbi))\n\t\t__set_test_and_free(sbi, segno);\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nvoid clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *head = &dcc->entry_list;\n\tstruct discard_entry *entry, *this;\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned long *prefree_map = dirty_i->dirty_segmap[PRE];\n\tunsigned int start = 0, end = -1;\n\tunsigned int secno, start_segno;\n\tbool force = (cpc->reason & CP_DISCARD);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\twhile (1) {\n\t\tint i;\n\t\tstart = find_next_bit(prefree_map, MAIN_SEGS(sbi), end + 1);\n\t\tif (start >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\tend = find_next_zero_bit(prefree_map, MAIN_SEGS(sbi),\n\t\t\t\t\t\t\t\tstart + 1);\n\n\t\tfor (i = start; i < end; i++)\n\t\t\tclear_bit(i, prefree_map);\n\n\t\tdirty_i->nr_dirty[PRE] -= end - start;\n\n\t\tif (!test_opt(sbi, DISCARD))\n\t\t\tcontinue;\n\n\t\tif (force && start >= cpc->trim_start &&\n\t\t\t\t\t(end - 1) <= cpc->trim_end)\n\t\t\t\tcontinue;\n\n\t\tif (!test_opt(sbi, LFS) || sbi->segs_per_sec == 1) {\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start),\n\t\t\t\t(end - start) << sbi->log_blocks_per_seg);\n\t\t\tcontinue;\n\t\t}\nnext:\n\t\tsecno = GET_SEC_FROM_SEG(sbi, start);\n\t\tstart_segno = GET_SEG_FROM_SEC(sbi, secno);\n\t\tif (!IS_CURSEC(sbi, secno) &&\n\t\t\t!get_valid_blocks(sbi, start, true))\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start_segno),\n\t\t\t\tsbi->segs_per_sec << sbi->log_blocks_per_seg);\n\n\t\tstart = start_segno + sbi->segs_per_sec;\n\t\tif (start < end)\n\t\t\tgoto next;\n\t\telse\n\t\t\tend = start - 1;\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\t/* send small discards */\n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tunsigned int cur_pos = 0, next_pos, len, total_len = 0;\n\t\tbool is_valid = test_bit_le(0, entry->discard_map);\n\nfind_next:\n\t\tif (is_valid) {\n\t\t\tnext_pos = find_next_zero_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t\tlen = next_pos - cur_pos;\n\n\t\t\tif (f2fs_sb_mounted_blkzoned(sbi->sb) ||\n\t\t\t    (force && len < cpc->trim_minlen))\n\t\t\t\tgoto skip;\n\n\t\t\tf2fs_issue_discard(sbi, entry->start_blkaddr + cur_pos,\n\t\t\t\t\t\t\t\t\tlen);\n\t\t\tcpc->trimmed += len;\n\t\t\ttotal_len += len;\n\t\t} else {\n\t\t\tnext_pos = find_next_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t}\nskip:\n\t\tcur_pos = next_pos;\n\t\tis_valid = !is_valid;\n\n\t\tif (cur_pos < sbi->blocks_per_seg)\n\t\t\tgoto find_next;\n\n\t\tlist_del(&entry->list);\n\t\tdcc->nr_discards -= total_len;\n\t\tkmem_cache_free(discard_entry_slab, entry);\n\t}\n\n\twake_up_discard_thread(sbi, false);\n}\n\nstatic int create_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct discard_cmd_control *dcc;\n\tint err = 0, i;\n\n\tif (SM_I(sbi)->dcc_info) {\n\t\tdcc = SM_I(sbi)->dcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tdcc = kzalloc(sizeof(struct discard_cmd_control), GFP_KERNEL);\n\tif (!dcc)\n\t\treturn -ENOMEM;\n\n\tdcc->discard_granularity = DEFAULT_DISCARD_GRANULARITY;\n\tINIT_LIST_HEAD(&dcc->entry_list);\n\tfor (i = 0; i < MAX_PLIST_NUM; i++) {\n\t\tINIT_LIST_HEAD(&dcc->pend_list[i]);\n\t\tif (i >= dcc->discard_granularity - 1)\n\t\t\tdcc->pend_list_tag[i] |= P_ACTIVE;\n\t}\n\tINIT_LIST_HEAD(&dcc->wait_list);\n\tmutex_init(&dcc->cmd_lock);\n\tatomic_set(&dcc->issued_discard, 0);\n\tatomic_set(&dcc->issing_discard, 0);\n\tatomic_set(&dcc->discard_cmd_cnt, 0);\n\tdcc->nr_discards = 0;\n\tdcc->max_discards = MAIN_SEGS(sbi) << sbi->log_blocks_per_seg;\n\tdcc->undiscard_blks = 0;\n\tdcc->root = RB_ROOT;\n\n\tinit_waitqueue_head(&dcc->discard_wait_queue);\n\tSM_I(sbi)->dcc_info = dcc;\ninit_thread:\n\tdcc->f2fs_issue_discard = kthread_run(issue_discard_thread, sbi,\n\t\t\t\t\"f2fs_discard-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(dcc->f2fs_issue_discard)) {\n\t\terr = PTR_ERR(dcc->f2fs_issue_discard);\n\t\tkfree(dcc);\n\t\tSM_I(sbi)->dcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nstatic void destroy_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (!dcc)\n\t\treturn;\n\n\tstop_discard_thread(sbi);\n\n\tkfree(dcc);\n\tSM_I(sbi)->dcc_info = NULL;\n}\n\nstatic bool __mark_sit_entry_dirty(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tif (!__test_and_set_bit(segno, sit_i->dirty_sentries_bitmap)) {\n\t\tsit_i->dirty_sentries++;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void __set_sit_entry_type(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tunsigned int segno, int modified)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, segno);\n\tse->type = type;\n\tif (modified)\n\t\t__mark_sit_entry_dirty(sbi, segno);\n}\n\nstatic void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)\n{\n\tstruct seg_entry *se;\n\tunsigned int segno, offset;\n\tlong int new_vblocks;\n\tbool exist;\n#ifdef CONFIG_F2FS_CHECK_FS\n\tbool mir_exist;\n#endif\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\n\tse = get_seg_entry(sbi, segno);\n\tnew_vblocks = se->valid_blocks + del;\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tf2fs_bug_on(sbi, (new_vblocks >> (sizeof(unsigned short) << 3) ||\n\t\t\t\t(new_vblocks > sbi->blocks_per_seg)));\n\n\tse->valid_blocks = new_vblocks;\n\tse->mtime = get_mtime(sbi);\n\tSIT_I(sbi)->max_mtime = se->mtime;\n\n\t/* Update valid block bitmap */\n\tif (del > 0) {\n\t\texist = f2fs_test_and_set_bit(offset, se->cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tmir_exist = f2fs_test_and_set_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir);\n\t\tif (unlikely(exist != mir_exist)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR, \"Inconsistent error \"\n\t\t\t\t\"when setting bitmap, blk:%u, old bit:%d\",\n\t\t\t\tblkaddr, exist);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n#endif\n\t\tif (unlikely(exist)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\"Bitmap was wrongly set, blk:%u\", blkaddr);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tse->valid_blocks--;\n\t\t\tdel = 0;\n\t\t}\n\n\t\tif (f2fs_discard_en(sbi) &&\n\t\t\t!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\n\t\t/* don't overwrite by SSR to keep node chain */\n\t\tif (se->type == CURSEG_WARM_NODE) {\n\t\t\tif (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))\n\t\t\t\tse->ckpt_valid_blocks++;\n\t\t}\n\t} else {\n\t\texist = f2fs_test_and_clear_bit(offset, se->cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tmir_exist = f2fs_test_and_clear_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir);\n\t\tif (unlikely(exist != mir_exist)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR, \"Inconsistent error \"\n\t\t\t\t\"when clearing bitmap, blk:%u, old bit:%d\",\n\t\t\t\tblkaddr, exist);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n#endif\n\t\tif (unlikely(!exist)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\"Bitmap was wrongly cleared, blk:%u\", blkaddr);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tse->valid_blocks++;\n\t\t\tdel = 0;\n\t\t}\n\n\t\tif (f2fs_discard_en(sbi) &&\n\t\t\tf2fs_test_and_clear_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks++;\n\t}\n\tif (!f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tse->ckpt_valid_blocks += del;\n\n\t__mark_sit_entry_dirty(sbi, segno);\n\n\t/* update total number of valid blocks to be written in ckpt area */\n\tSIT_I(sbi)->written_valid_blocks += del;\n\n\tif (sbi->segs_per_sec > 1)\n\t\tget_sec_entry(sbi, segno)->valid_blocks += del;\n}\n\nvoid refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new)\n{\n\tupdate_sit_entry(sbi, new, 1);\n\tif (GET_SEGNO(sbi, old) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old, -1);\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new));\n}\n\nvoid invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr)\n{\n\tunsigned int segno = GET_SEGNO(sbi, addr);\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tf2fs_bug_on(sbi, addr == NULL_ADDR);\n\tif (addr == NEW_ADDR)\n\t\treturn;\n\n\t/* add it into sit main buffer */\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tupdate_sit_entry(sbi, addr, -1);\n\n\t/* add it into dirty seglist */\n\tlocate_dirty_segment(sbi, segno);\n\n\tmutex_unlock(&sit_i->sentry_lock);\n}\n\nbool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno, offset;\n\tstruct seg_entry *se;\n\tbool is_cp = false;\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn true;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tif (f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tis_cp = true;\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\treturn is_cp;\n}\n\n/*\n * This function should be resided under the curseg_mutex lock\n */\nstatic void __add_sum_entry(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tstruct f2fs_summary *sum)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tvoid *addr = curseg->sum_blk;\n\taddr += curseg->next_blkoff * sizeof(struct f2fs_summary);\n\tmemcpy(addr, sum, sizeof(struct f2fs_summary));\n}\n\n/*\n * Calculate the number of current summary pages for writing\n */\nint npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra)\n{\n\tint valid_sum_count = 0;\n\tint i, sum_in_page;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tif (sbi->ckpt->alloc_type[i] == SSR)\n\t\t\tvalid_sum_count += sbi->blocks_per_seg;\n\t\telse {\n\t\t\tif (for_ra)\n\t\t\t\tvalid_sum_count += le16_to_cpu(\n\t\t\t\t\tF2FS_CKPT(sbi)->cur_data_blkoff[i]);\n\t\t\telse\n\t\t\t\tvalid_sum_count += curseg_blkoff(sbi, i);\n\t\t}\n\t}\n\n\tsum_in_page = (PAGE_SIZE - 2 * SUM_JOURNAL_SIZE -\n\t\t\tSUM_FOOTER_SIZE) / SUMMARY_SIZE;\n\tif (valid_sum_count <= sum_in_page)\n\t\treturn 1;\n\telse if ((valid_sum_count - sum_in_page) <=\n\t\t(PAGE_SIZE - SUM_FOOTER_SIZE) / SUMMARY_SIZE)\n\t\treturn 2;\n\treturn 3;\n}\n\n/*\n * Caller should put this summary page\n */\nstruct page *get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\treturn get_meta_page(sbi, GET_SUM_BLOCK(sbi, segno));\n}\n\nvoid update_meta_page(struct f2fs_sb_info *sbi, void *src, block_t blk_addr)\n{\n\tstruct page *page = grab_meta_page(sbi, blk_addr);\n\tvoid *dst = page_address(page);\n\n\tif (src)\n\t\tmemcpy(dst, src, PAGE_SIZE);\n\telse\n\t\tmemset(dst, 0, PAGE_SIZE);\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic void write_sum_page(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_summary_block *sum_blk, block_t blk_addr)\n{\n\tupdate_meta_page(sbi, (void *)sum_blk, blk_addr);\n}\n\nstatic void write_current_sum_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, block_t blk_addr)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct page *page = grab_meta_page(sbi, blk_addr);\n\tstruct f2fs_summary_block *src = curseg->sum_blk;\n\tstruct f2fs_summary_block *dst;\n\n\tdst = (struct f2fs_summary_block *)page_address(page);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\n\tdown_read(&curseg->journal_rwsem);\n\tmemcpy(&dst->journal, curseg->journal, SUM_JOURNAL_SIZE);\n\tup_read(&curseg->journal_rwsem);\n\n\tmemcpy(dst->entries, src->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&dst->footer, &src->footer, SUM_FOOTER_SIZE);\n\n\tmutex_unlock(&curseg->curseg_mutex);\n\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic int is_next_segment_free(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno = curseg->segno + 1;\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\n\tif (segno < MAIN_SEGS(sbi) && segno % sbi->segs_per_sec)\n\t\treturn !test_bit(segno, free_i->free_segmap);\n\treturn 0;\n}\n\n/*\n * Find a new segment from the free segments bitmap to right order\n * This function should be returned with success, otherwise BUG\n */\nstatic void get_new_segment(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int *newseg, bool new_sec, int dir)\n{\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno, secno, zoneno;\n\tunsigned int total_zones = MAIN_SECS(sbi) / sbi->secs_per_zone;\n\tunsigned int hint = GET_SEC_FROM_SEG(sbi, *newseg);\n\tunsigned int old_zoneno = GET_ZONE_FROM_SEG(sbi, *newseg);\n\tunsigned int left_start = hint;\n\tbool init = true;\n\tint go_left = 0;\n\tint i;\n\n\tspin_lock(&free_i->segmap_lock);\n\n\tif (!new_sec && ((*newseg + 1) % sbi->segs_per_sec)) {\n\t\tsegno = find_next_zero_bit(free_i->free_segmap,\n\t\t\tGET_SEG_FROM_SEC(sbi, hint + 1), *newseg + 1);\n\t\tif (segno < GET_SEG_FROM_SEC(sbi, hint + 1))\n\t\t\tgoto got_it;\n\t}\nfind_other_zone:\n\tsecno = find_next_zero_bit(free_i->free_secmap, MAIN_SECS(sbi), hint);\n\tif (secno >= MAIN_SECS(sbi)) {\n\t\tif (dir == ALLOC_RIGHT) {\n\t\t\tsecno = find_next_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi), 0);\n\t\t\tf2fs_bug_on(sbi, secno >= MAIN_SECS(sbi));\n\t\t} else {\n\t\t\tgo_left = 1;\n\t\t\tleft_start = hint - 1;\n\t\t}\n\t}\n\tif (go_left == 0)\n\t\tgoto skip_left;\n\n\twhile (test_bit(left_start, free_i->free_secmap)) {\n\t\tif (left_start > 0) {\n\t\t\tleft_start--;\n\t\t\tcontinue;\n\t\t}\n\t\tleft_start = find_next_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi), 0);\n\t\tf2fs_bug_on(sbi, left_start >= MAIN_SECS(sbi));\n\t\tbreak;\n\t}\n\tsecno = left_start;\nskip_left:\n\thint = secno;\n\tsegno = GET_SEG_FROM_SEC(sbi, secno);\n\tzoneno = GET_ZONE_FROM_SEC(sbi, secno);\n\n\t/* give up on finding another zone */\n\tif (!init)\n\t\tgoto got_it;\n\tif (sbi->secs_per_zone == 1)\n\t\tgoto got_it;\n\tif (zoneno == old_zoneno)\n\t\tgoto got_it;\n\tif (dir == ALLOC_LEFT) {\n\t\tif (!go_left && zoneno + 1 >= total_zones)\n\t\t\tgoto got_it;\n\t\tif (go_left && zoneno == 0)\n\t\t\tgoto got_it;\n\t}\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++)\n\t\tif (CURSEG_I(sbi, i)->zone == zoneno)\n\t\t\tbreak;\n\n\tif (i < NR_CURSEG_TYPE) {\n\t\t/* zone is in user, try another */\n\t\tif (go_left)\n\t\t\thint = zoneno * sbi->secs_per_zone - 1;\n\t\telse if (zoneno + 1 >= total_zones)\n\t\t\thint = 0;\n\t\telse\n\t\t\thint = (zoneno + 1) * sbi->secs_per_zone;\n\t\tinit = false;\n\t\tgoto find_other_zone;\n\t}\ngot_it:\n\t/* set it as dirty segment in free segmap */\n\tf2fs_bug_on(sbi, test_bit(segno, free_i->free_segmap));\n\t__set_inuse(sbi, segno);\n\t*newseg = segno;\n\tspin_unlock(&free_i->segmap_lock);\n}\n\nstatic void reset_curseg(struct f2fs_sb_info *sbi, int type, int modified)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct summary_footer *sum_footer;\n\n\tcurseg->segno = curseg->next_segno;\n\tcurseg->zone = GET_ZONE_FROM_SEG(sbi, curseg->segno);\n\tcurseg->next_blkoff = 0;\n\tcurseg->next_segno = NULL_SEGNO;\n\n\tsum_footer = &(curseg->sum_blk->footer);\n\tmemset(sum_footer, 0, sizeof(struct summary_footer));\n\tif (IS_DATASEG(type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_DATA);\n\tif (IS_NODESEG(type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_NODE);\n\t__set_sit_entry_type(sbi, type, curseg->segno, modified);\n}\n\nstatic unsigned int __get_next_segno(struct f2fs_sb_info *sbi, int type)\n{\n\t/* if segs_per_sec is large than 1, we need to keep original policy. */\n\tif (sbi->segs_per_sec != 1)\n\t\treturn CURSEG_I(sbi, type)->segno;\n\n\tif (type == CURSEG_HOT_DATA || IS_NODESEG(type))\n\t\treturn 0;\n\n\tif (SIT_I(sbi)->last_victim[ALLOC_NEXT])\n\t\treturn SIT_I(sbi)->last_victim[ALLOC_NEXT];\n\treturn CURSEG_I(sbi, type)->segno;\n}\n\n/*\n * Allocate a current working segment.\n * This function always allocates a free segment in LFS manner.\n */\nstatic void new_curseg(struct f2fs_sb_info *sbi, int type, bool new_sec)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno = curseg->segno;\n\tint dir = ALLOC_LEFT;\n\n\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, segno));\n\tif (type == CURSEG_WARM_DATA || type == CURSEG_COLD_DATA)\n\t\tdir = ALLOC_RIGHT;\n\n\tif (test_opt(sbi, NOHEAP))\n\t\tdir = ALLOC_RIGHT;\n\n\tsegno = __get_next_segno(sbi, type);\n\tget_new_segment(sbi, &segno, new_sec, dir);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = LFS;\n}\n\nstatic void __next_free_blkoff(struct f2fs_sb_info *sbi,\n\t\t\tstruct curseg_info *seg, block_t start)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, seg->segno);\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tunsigned long *target_map = SIT_I(sbi)->tmp_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tint i, pos;\n\n\tfor (i = 0; i < entries; i++)\n\t\ttarget_map[i] = ckpt_map[i] | cur_map[i];\n\n\tpos = __find_rev_next_zero_bit(target_map, sbi->blocks_per_seg, start);\n\n\tseg->next_blkoff = pos;\n}\n\n/*\n * If a segment is written by LFS manner, next block offset is just obtained\n * by increasing the current block offset. However, if a segment is written by\n * SSR manner, next block offset obtained by calling __next_free_blkoff\n */\nstatic void __refresh_next_blkoff(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct curseg_info *seg)\n{\n\tif (seg->alloc_type == SSR)\n\t\t__next_free_blkoff(sbi, seg, seg->next_blkoff + 1);\n\telse\n\t\tseg->next_blkoff++;\n}\n\n/*\n * This function always allocates a used segment(from dirty seglist) by SSR\n * manner, so it should recover the existing segment information of valid blocks\n */\nstatic void change_curseg(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int new_segno = curseg->next_segno;\n\tstruct f2fs_summary_block *sum_node;\n\tstruct page *sum_page;\n\n\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, curseg->segno));\n\t__set_test_and_inuse(sbi, new_segno);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\t__remove_dirty_segment(sbi, new_segno, PRE);\n\t__remove_dirty_segment(sbi, new_segno, DIRTY);\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = SSR;\n\t__next_free_blkoff(sbi, curseg, 0);\n\n\tsum_page = get_sum_page(sbi, new_segno);\n\tsum_node = (struct f2fs_summary_block *)page_address(sum_page);\n\tmemcpy(curseg->sum_blk, sum_node, SUM_ENTRY_SIZE);\n\tf2fs_put_page(sum_page, 1);\n}\n\nstatic int get_ssr_segment(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tconst struct victim_selection *v_ops = DIRTY_I(sbi)->v_ops;\n\tunsigned segno = NULL_SEGNO;\n\tint i, cnt;\n\tbool reversed = false;\n\n\t/* need_SSR() already forces to do this */\n\tif (v_ops->get_victim(sbi, &segno, BG_GC, type, SSR)) {\n\t\tcurseg->next_segno = segno;\n\t\treturn 1;\n\t}\n\n\t/* For node segments, let's do SSR more intensively */\n\tif (IS_NODESEG(type)) {\n\t\tif (type >= CURSEG_WARM_NODE) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_NODE;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_NODE;\n\t\t}\n\t\tcnt = NR_CURSEG_NODE_TYPE;\n\t} else {\n\t\tif (type >= CURSEG_WARM_DATA) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_DATA;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_DATA;\n\t\t}\n\t\tcnt = NR_CURSEG_DATA_TYPE;\n\t}\n\n\tfor (; cnt-- > 0; reversed ? i-- : i++) {\n\t\tif (i == type)\n\t\t\tcontinue;\n\t\tif (v_ops->get_victim(sbi, &segno, BG_GC, i, SSR)) {\n\t\t\tcurseg->next_segno = segno;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * flush out current segment and replace it with new segment\n * This function should be returned with success, otherwise BUG\n */\nstatic void allocate_segment_by_default(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, bool force)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tif (force)\n\t\tnew_curseg(sbi, type, true);\n\telse if (!is_set_ckpt_flags(sbi, CP_CRC_RECOVERY_FLAG) &&\n\t\t\t\t\ttype == CURSEG_WARM_NODE)\n\t\tnew_curseg(sbi, type, false);\n\telse if (curseg->alloc_type == LFS && is_next_segment_free(sbi, type))\n\t\tnew_curseg(sbi, type, false);\n\telse if (need_SSR(sbi) && get_ssr_segment(sbi, type))\n\t\tchange_curseg(sbi, type);\n\telse\n\t\tnew_curseg(sbi, type, false);\n\n\tstat_inc_seg_type(sbi, curseg);\n}\n\nvoid allocate_new_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg;\n\tunsigned int old_segno;\n\tint i;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tcurseg = CURSEG_I(sbi, i);\n\t\told_segno = curseg->segno;\n\t\tSIT_I(sbi)->s_ops->allocate_segment(sbi, i, true);\n\t\tlocate_dirty_segment(sbi, old_segno);\n\t}\n}\n\nstatic const struct segment_allocation default_salloc_ops = {\n\t.allocate_segment = allocate_segment_by_default,\n};\n\nbool exist_trim_candidates(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\t__u64 trim_start = cpc->trim_start;\n\tbool has_candidate = false;\n\n\tmutex_lock(&SIT_I(sbi)->sentry_lock);\n\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++) {\n\t\tif (add_discard_addrs(sbi, cpc, true)) {\n\t\t\thas_candidate = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&SIT_I(sbi)->sentry_lock);\n\n\tcpc->trim_start = trim_start;\n\treturn has_candidate;\n}\n\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range)\n{\n\t__u64 start = F2FS_BYTES_TO_BLK(range->start);\n\t__u64 end = start + F2FS_BYTES_TO_BLK(range->len) - 1;\n\tunsigned int start_segno, end_segno;\n\tstruct cp_control cpc;\n\tint err = 0;\n\n\tif (start >= MAX_BLKADDR(sbi) || range->len < sbi->blocksize)\n\t\treturn -EINVAL;\n\n\tcpc.trimmed = 0;\n\tif (end <= MAIN_BLKADDR(sbi))\n\t\tgoto out;\n\n\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\"Found FS corruption, run fsck to fix.\");\n\t\tgoto out;\n\t}\n\n\t/* start/end segment number in main_area */\n\tstart_segno = (start <= MAIN_BLKADDR(sbi)) ? 0 : GET_SEGNO(sbi, start);\n\tend_segno = (end >= MAX_BLKADDR(sbi)) ? MAIN_SEGS(sbi) - 1 :\n\t\t\t\t\t\tGET_SEGNO(sbi, end);\n\tcpc.reason = CP_DISCARD;\n\tcpc.trim_minlen = max_t(__u64, 1, F2FS_BYTES_TO_BLK(range->minlen));\n\n\t/* do checkpoint to issue discard commands safely */\n\tfor (; start_segno <= end_segno; start_segno = cpc.trim_end + 1) {\n\t\tcpc.trim_start = start_segno;\n\n\t\tif (sbi->discard_blks == 0)\n\t\t\tbreak;\n\t\telse if (sbi->discard_blks < BATCHED_TRIM_BLOCKS(sbi))\n\t\t\tcpc.trim_end = end_segno;\n\t\telse\n\t\t\tcpc.trim_end = min_t(unsigned int,\n\t\t\t\trounddown(start_segno +\n\t\t\t\tBATCHED_TRIM_SEGMENTS(sbi),\n\t\t\t\tsbi->segs_per_sec) - 1, end_segno);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\t/* It's time to issue all the filed discards */\n\tmark_discard_range_all(sbi);\n\tf2fs_wait_discard_bios(sbi);\nout:\n\trange->len = F2FS_BLK_TO_BYTES(cpc.trimmed);\n\treturn err;\n}\n\nstatic bool __has_curseg_space(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tif (curseg->next_blkoff < sbi->blocks_per_seg)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int __get_segment_type_2(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA)\n\t\treturn CURSEG_HOT_DATA;\n\telse\n\t\treturn CURSEG_HOT_NODE;\n}\n\nstatic int __get_segment_type_4(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (S_ISDIR(inode->i_mode))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\telse\n\t\t\treturn CURSEG_COLD_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page) && is_cold_node(fio->page))\n\t\t\treturn CURSEG_WARM_NODE;\n\t\telse\n\t\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type_6(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (is_cold_data(fio->page) || file_is_cold(inode))\n\t\t\treturn CURSEG_COLD_DATA;\n\t\tif (is_inode_flag_set(inode, FI_HOT_DATA))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\treturn CURSEG_WARM_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page))\n\t\t\treturn is_cold_node(fio->page) ? CURSEG_WARM_NODE :\n\t\t\t\t\t\tCURSEG_HOT_NODE;\n\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type(struct f2fs_io_info *fio)\n{\n\tint type = 0;\n\n\tswitch (fio->sbi->active_logs) {\n\tcase 2:\n\t\ttype = __get_segment_type_2(fio);\n\t\tbreak;\n\tcase 4:\n\t\ttype = __get_segment_type_4(fio);\n\t\tbreak;\n\tcase 6:\n\t\ttype = __get_segment_type_6(fio);\n\t\tbreak;\n\tdefault:\n\t\tf2fs_bug_on(fio->sbi, true);\n\t}\n\n\tif (IS_HOT(type))\n\t\tfio->temp = HOT;\n\telse if (IS_WARM(type))\n\t\tfio->temp = WARM;\n\telse\n\t\tfio->temp = COLD;\n\treturn type;\n}\n\nvoid allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\tstruct f2fs_summary *sum, int type,\n\t\tstruct f2fs_io_info *fio, bool add_list)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tmutex_lock(&sit_i->sentry_lock);\n\n\t*new_blkaddr = NEXT_FREE_BLKADDR(sbi, curseg);\n\n\tf2fs_wait_discard_bio(sbi, *new_blkaddr);\n\n\t/*\n\t * __add_sum_entry should be resided under the curseg_mutex\n\t * because, this function updates a summary entry in the\n\t * current summary block.\n\t */\n\t__add_sum_entry(sbi, type, sum);\n\n\t__refresh_next_blkoff(sbi, curseg);\n\n\tstat_inc_block_count(sbi, curseg);\n\n\tif (!__has_curseg_space(sbi, type))\n\t\tsit_i->s_ops->allocate_segment(sbi, type, false);\n\t/*\n\t * SIT information should be updated after segment allocation,\n\t * since we need to keep dirty segments precisely under SSR.\n\t */\n\trefresh_sit_entry(sbi, old_blkaddr, *new_blkaddr);\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\tif (page && IS_NODESEG(type)) {\n\t\tfill_node_footer_blkaddr(page, NEXT_FREE_BLKADDR(sbi, curseg));\n\n\t\tf2fs_inode_chksum_set(sbi, page);\n\t}\n\n\tif (add_list) {\n\t\tstruct f2fs_bio_info *io;\n\n\t\tINIT_LIST_HEAD(&fio->list);\n\t\tfio->in_list = true;\n\t\tio = sbi->write_io[fio->type] + fio->temp;\n\t\tspin_lock(&io->io_lock);\n\t\tlist_add_tail(&fio->list, &io->io_list);\n\t\tspin_unlock(&io->io_lock);\n\t}\n\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nstatic void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)\n{\n\tint type = __get_segment_type(fio);\n\tint err;\n\nreallocate:\n\tallocate_data_block(fio->sbi, fio->page, fio->old_blkaddr,\n\t\t\t&fio->new_blkaddr, sum, type, fio, true);\n\n\t/* writeout dirty page into bdev */\n\terr = f2fs_submit_page_write(fio);\n\tif (err == -EAGAIN) {\n\t\tfio->old_blkaddr = fio->new_blkaddr;\n\t\tgoto reallocate;\n\t}\n}\n\nvoid write_meta_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = META,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_META | REQ_PRIO,\n\t\t.old_blkaddr = page->index,\n\t\t.new_blkaddr = page->index,\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t\t.in_list = false,\n\t};\n\n\tif (unlikely(page->index >= MAIN_BLKADDR(sbi)))\n\t\tfio.op_flags &= ~REQ_META;\n\n\tset_page_writeback(page);\n\tf2fs_submit_page_write(&fio);\n\n\tf2fs_update_iostat(sbi, io_type, F2FS_BLKSIZE);\n}\n\nvoid write_node_page(unsigned int nid, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, nid, 0, 0);\n\tdo_write_page(&sum, fio);\n\n\tf2fs_update_iostat(fio->sbi, fio->io_type, F2FS_BLKSIZE);\n}\n\nvoid write_data_page(struct dnode_of_data *dn, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\n\tf2fs_bug_on(sbi, dn->data_blkaddr == NULL_ADDR);\n\tget_node_info(sbi, dn->nid, &ni);\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\tdo_write_page(&sum, fio);\n\tf2fs_update_data_blkaddr(dn, fio->new_blkaddr);\n\n\tf2fs_update_iostat(sbi, fio->io_type, F2FS_BLKSIZE);\n}\n\nint rewrite_data_page(struct f2fs_io_info *fio)\n{\n\tint err;\n\n\tfio->new_blkaddr = fio->old_blkaddr;\n\tstat_inc_inplace_blocks(fio->sbi);\n\n\terr = f2fs_submit_page_bio(fio);\n\n\tf2fs_update_iostat(fio->sbi, fio->io_type, F2FS_BLKSIZE);\n\n\treturn err;\n}\n\nvoid __f2fs_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\t\tbool recover_curseg, bool recover_newaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg;\n\tunsigned int segno, old_cursegno;\n\tstruct seg_entry *se;\n\tint type;\n\tunsigned short old_blkoff;\n\n\tsegno = GET_SEGNO(sbi, new_blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\ttype = se->type;\n\n\tif (!recover_curseg) {\n\t\t/* for recovery flow */\n\t\tif (se->valid_blocks == 0 && !IS_CURSEG(sbi, segno)) {\n\t\t\tif (old_blkaddr == NULL_ADDR)\n\t\t\t\ttype = CURSEG_COLD_DATA;\n\t\t\telse\n\t\t\t\ttype = CURSEG_WARM_DATA;\n\t\t}\n\t} else {\n\t\tif (!IS_CURSEG(sbi, segno))\n\t\t\ttype = CURSEG_WARM_DATA;\n\t}\n\n\tcurseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tmutex_lock(&sit_i->sentry_lock);\n\n\told_cursegno = curseg->segno;\n\told_blkoff = curseg->next_blkoff;\n\n\t/* change the current segment */\n\tif (segno != curseg->segno) {\n\t\tcurseg->next_segno = segno;\n\t\tchange_curseg(sbi, type);\n\t}\n\n\tcurseg->next_blkoff = GET_BLKOFF_FROM_SEG0(sbi, new_blkaddr);\n\t__add_sum_entry(sbi, type, sum);\n\n\tif (!recover_curseg || recover_newaddr)\n\t\tupdate_sit_entry(sbi, new_blkaddr, 1);\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old_blkaddr, -1);\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old_blkaddr));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new_blkaddr));\n\n\tlocate_dirty_segment(sbi, old_cursegno);\n\n\tif (recover_curseg) {\n\t\tif (old_cursegno != curseg->segno) {\n\t\t\tcurseg->next_segno = old_cursegno;\n\t\t\tchange_curseg(sbi, type);\n\t\t}\n\t\tcurseg->next_blkoff = old_blkoff;\n\t}\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\t\tunsigned char version, bool recover_curseg,\n\t\t\t\tbool recover_newaddr)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, version);\n\n\t__f2fs_replace_block(sbi, &sum, old_addr, new_addr,\n\t\t\t\t\trecover_curseg, recover_newaddr);\n\n\tf2fs_update_data_blkaddr(dn, new_addr);\n}\n\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\t\tenum page_type type, bool ordered)\n{\n\tif (PageWriteback(page)) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_P_SB(page);\n\n\t\tf2fs_submit_merged_write_cond(sbi, page->mapping->host,\n\t\t\t\t\t\t0, page->index, type);\n\t\tif (ordered)\n\t\t\twait_on_page_writeback(page);\n\t\telse\n\t\t\twait_for_stable_page(page);\n\t}\n}\n\nvoid f2fs_wait_on_block_writeback(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct page *cpage;\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn;\n\n\tcpage = find_lock_page(META_MAPPING(sbi), blkaddr);\n\tif (cpage) {\n\t\tf2fs_wait_on_page_writeback(cpage, DATA, true);\n\t\tf2fs_put_page(cpage, 1);\n\t}\n}\n\nstatic int read_compacted_summaries(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct curseg_info *seg_i;\n\tunsigned char *kaddr;\n\tstruct page *page;\n\tblock_t start;\n\tint i, j, offset;\n\n\tstart = start_sum_block(sbi);\n\n\tpage = get_meta_page(sbi, start++);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t/* Step 1: restore nat cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(seg_i->journal, kaddr, SUM_JOURNAL_SIZE);\n\n\t/* Step 2: restore sit cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(seg_i->journal, kaddr + SUM_JOURNAL_SIZE, SUM_JOURNAL_SIZE);\n\toffset = 2 * SUM_JOURNAL_SIZE;\n\n\t/* Step 3: restore summary entries */\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blk_off;\n\t\tunsigned int segno;\n\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[i]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[i]);\n\t\tseg_i->next_segno = segno;\n\t\treset_curseg(sbi, i, 0);\n\t\tseg_i->alloc_type = ckpt->alloc_type[i];\n\t\tseg_i->next_blkoff = blk_off;\n\n\t\tif (seg_i->alloc_type == SSR)\n\t\t\tblk_off = sbi->blocks_per_seg;\n\n\t\tfor (j = 0; j < blk_off; j++) {\n\t\t\tstruct f2fs_summary *s;\n\t\t\ts = (struct f2fs_summary *)(kaddr + offset);\n\t\t\tseg_i->sum_blk->entries[j] = *s;\n\t\t\toffset += SUMMARY_SIZE;\n\t\t\tif (offset + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\n\t\t\tpage = get_meta_page(sbi, start++);\n\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\toffset = 0;\n\t\t}\n\t}\n\tf2fs_put_page(page, 1);\n\treturn 0;\n}\n\nstatic int read_normal_summaries(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_summary_block *sum;\n\tstruct curseg_info *curseg;\n\tstruct page *new;\n\tunsigned short blk_off;\n\tunsigned int segno = 0;\n\tblock_t blk_addr = 0;\n\n\t/* get segment number and block addr */\n\tif (IS_DATASEG(type)) {\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[type]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_DATA]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_TYPE, type);\n\t\telse\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_DATA_TYPE, type);\n\t} else {\n\t\tsegno = le32_to_cpu(ckpt->cur_node_segno[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_node_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_NODE_TYPE,\n\t\t\t\t\t\t\ttype - CURSEG_HOT_NODE);\n\t\telse\n\t\t\tblk_addr = GET_SUM_BLOCK(sbi, segno);\n\t}\n\n\tnew = get_meta_page(sbi, blk_addr);\n\tsum = (struct f2fs_summary_block *)page_address(new);\n\n\tif (IS_NODESEG(type)) {\n\t\tif (__exist_node_summaries(sbi)) {\n\t\t\tstruct f2fs_summary *ns = &sum->entries[0];\n\t\t\tint i;\n\t\t\tfor (i = 0; i < sbi->blocks_per_seg; i++, ns++) {\n\t\t\t\tns->version = 0;\n\t\t\t\tns->ofs_in_node = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tint err;\n\n\t\t\terr = restore_node_summary(sbi, segno, sum);\n\t\t\tif (err) {\n\t\t\t\tf2fs_put_page(new, 1);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* set uncompleted segment to curseg */\n\tcurseg = CURSEG_I(sbi, type);\n\tmutex_lock(&curseg->curseg_mutex);\n\n\t/* update journal info */\n\tdown_write(&curseg->journal_rwsem);\n\tmemcpy(curseg->journal, &sum->journal, SUM_JOURNAL_SIZE);\n\tup_write(&curseg->journal_rwsem);\n\n\tmemcpy(curseg->sum_blk->entries, sum->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&curseg->sum_blk->footer, &sum->footer, SUM_FOOTER_SIZE);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 0);\n\tcurseg->alloc_type = ckpt->alloc_type[type];\n\tcurseg->next_blkoff = blk_off;\n\tmutex_unlock(&curseg->curseg_mutex);\n\tf2fs_put_page(new, 1);\n\treturn 0;\n}\n\nstatic int restore_curseg_summaries(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_journal *sit_j = CURSEG_I(sbi, CURSEG_COLD_DATA)->journal;\n\tstruct f2fs_journal *nat_j = CURSEG_I(sbi, CURSEG_HOT_DATA)->journal;\n\tint type = CURSEG_HOT_DATA;\n\tint err;\n\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG)) {\n\t\tint npages = npages_for_summary_flush(sbi, true);\n\n\t\tif (npages >= 2)\n\t\t\tra_meta_pages(sbi, start_sum_block(sbi), npages,\n\t\t\t\t\t\t\tMETA_CP, true);\n\n\t\t/* restore for compacted data summary */\n\t\tif (read_compacted_summaries(sbi))\n\t\t\treturn -EINVAL;\n\t\ttype = CURSEG_HOT_NODE;\n\t}\n\n\tif (__exist_node_summaries(sbi))\n\t\tra_meta_pages(sbi, sum_blk_addr(sbi, NR_CURSEG_TYPE, type),\n\t\t\t\t\tNR_CURSEG_TYPE - type, META_CP, true);\n\n\tfor (; type <= CURSEG_COLD_NODE; type++) {\n\t\terr = read_normal_summaries(sbi, type);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* sanity check for summary blocks */\n\tif (nats_in_cursum(nat_j) > NAT_JOURNAL_ENTRIES ||\n\t\t\tsits_in_cursum(sit_j) > SIT_JOURNAL_ENTRIES)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void write_compacted_summaries(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct page *page;\n\tunsigned char *kaddr;\n\tstruct f2fs_summary *summary;\n\tstruct curseg_info *seg_i;\n\tint written_size = 0;\n\tint i, j;\n\n\tpage = grab_meta_page(sbi, blkaddr++);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t/* Step 1: write nat cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(kaddr, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t/* Step 2: write sit cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(kaddr + written_size, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t/* Step 3: write summary entries */\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blkoff;\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tif (sbi->ckpt->alloc_type[i] == SSR)\n\t\t\tblkoff = sbi->blocks_per_seg;\n\t\telse\n\t\t\tblkoff = curseg_blkoff(sbi, i);\n\n\t\tfor (j = 0; j < blkoff; j++) {\n\t\t\tif (!page) {\n\t\t\t\tpage = grab_meta_page(sbi, blkaddr++);\n\t\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\t\twritten_size = 0;\n\t\t\t}\n\t\t\tsummary = (struct f2fs_summary *)(kaddr + written_size);\n\t\t\t*summary = seg_i->sum_blk->entries[j];\n\t\t\twritten_size += SUMMARY_SIZE;\n\n\t\t\tif (written_size + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\t\t}\n\t}\n\tif (page) {\n\t\tset_page_dirty(page);\n\t\tf2fs_put_page(page, 1);\n\t}\n}\n\nstatic void write_normal_summaries(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type)\n{\n\tint i, end;\n\tif (IS_DATASEG(type))\n\t\tend = type + NR_CURSEG_DATA_TYPE;\n\telse\n\t\tend = type + NR_CURSEG_NODE_TYPE;\n\n\tfor (i = type; i < end; i++)\n\t\twrite_current_sum_page(sbi, i, blkaddr + (i - type));\n}\n\nvoid write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG))\n\t\twrite_compacted_summaries(sbi, start_blk);\n\telse\n\t\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_DATA);\n}\n\nvoid write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_NODE);\n}\n\nint lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\t\t\tunsigned int val, int alloc)\n{\n\tint i;\n\n\tif (type == NAT_JOURNAL) {\n\t\tfor (i = 0; i < nats_in_cursum(journal); i++) {\n\t\t\tif (le32_to_cpu(nid_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\t}\n\t\tif (alloc && __has_cursum_space(journal, 1, NAT_JOURNAL))\n\t\t\treturn update_nats_in_cursum(journal, 1);\n\t} else if (type == SIT_JOURNAL) {\n\t\tfor (i = 0; i < sits_in_cursum(journal); i++)\n\t\t\tif (le32_to_cpu(segno_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\tif (alloc && __has_cursum_space(journal, 1, SIT_JOURNAL))\n\t\t\treturn update_sits_in_cursum(journal, 1);\n\t}\n\treturn -1;\n}\n\nstatic struct page *get_current_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int segno)\n{\n\treturn get_meta_page(sbi, current_sit_addr(sbi, segno));\n}\n\nstatic struct page *get_next_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int start)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct page *src_page, *dst_page;\n\tpgoff_t src_off, dst_off;\n\tvoid *src_addr, *dst_addr;\n\n\tsrc_off = current_sit_addr(sbi, start);\n\tdst_off = next_sit_addr(sbi, src_off);\n\n\t/* get current sit block page without lock */\n\tsrc_page = get_meta_page(sbi, src_off);\n\tdst_page = grab_meta_page(sbi, dst_off);\n\tf2fs_bug_on(sbi, PageDirty(src_page));\n\n\tsrc_addr = page_address(src_page);\n\tdst_addr = page_address(dst_page);\n\tmemcpy(dst_addr, src_addr, PAGE_SIZE);\n\n\tset_page_dirty(dst_page);\n\tf2fs_put_page(src_page, 1);\n\n\tset_to_next_sit(sit_i, start);\n\n\treturn dst_page;\n}\n\nstatic struct sit_entry_set *grab_sit_entry_set(void)\n{\n\tstruct sit_entry_set *ses =\n\t\t\tf2fs_kmem_cache_alloc(sit_entry_set_slab, GFP_NOFS);\n\n\tses->entry_cnt = 0;\n\tINIT_LIST_HEAD(&ses->set_list);\n\treturn ses;\n}\n\nstatic void release_sit_entry_set(struct sit_entry_set *ses)\n{\n\tlist_del(&ses->set_list);\n\tkmem_cache_free(sit_entry_set_slab, ses);\n}\n\nstatic void adjust_sit_entry_set(struct sit_entry_set *ses,\n\t\t\t\t\t\tstruct list_head *head)\n{\n\tstruct sit_entry_set *next = ses;\n\n\tif (list_is_last(&ses->set_list, head))\n\t\treturn;\n\n\tlist_for_each_entry_continue(next, head, set_list)\n\t\tif (ses->entry_cnt <= next->entry_cnt)\n\t\t\tbreak;\n\n\tlist_move_tail(&ses->set_list, &next->set_list);\n}\n\nstatic void add_sit_entry(unsigned int segno, struct list_head *head)\n{\n\tstruct sit_entry_set *ses;\n\tunsigned int start_segno = START_SEGNO(segno);\n\n\tlist_for_each_entry(ses, head, set_list) {\n\t\tif (ses->start_segno == start_segno) {\n\t\t\tses->entry_cnt++;\n\t\t\tadjust_sit_entry_set(ses, head);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tses = grab_sit_entry_set();\n\n\tses->start_segno = start_segno;\n\tses->entry_cnt++;\n\tlist_add(&ses->set_list, head);\n}\n\nstatic void add_sits_in_set(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\tstruct list_head *set_list = &sm_info->sit_entry_set;\n\tunsigned long *bitmap = SIT_I(sbi)->dirty_sentries_bitmap;\n\tunsigned int segno;\n\n\tfor_each_set_bit(segno, bitmap, MAIN_SEGS(sbi))\n\t\tadd_sit_entry(segno, set_list);\n}\n\nstatic void remove_sits_in_journal(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tint i;\n\n\tdown_write(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int segno;\n\t\tbool dirtied;\n\n\t\tsegno = le32_to_cpu(segno_in_journal(journal, i));\n\t\tdirtied = __mark_sit_entry_dirty(sbi, segno);\n\n\t\tif (!dirtied)\n\t\t\tadd_sit_entry(segno, &SM_I(sbi)->sit_entry_set);\n\t}\n\tupdate_sits_in_cursum(journal, -i);\n\tup_write(&curseg->journal_rwsem);\n}\n\n/*\n * CP calls this function, which flushes SIT entries including sit_journal,\n * and moves prefree segs to free segs.\n */\nvoid flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned long *bitmap = sit_i->dirty_sentries_bitmap;\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct sit_entry_set *ses, *tmp;\n\tstruct list_head *head = &SM_I(sbi)->sit_entry_set;\n\tbool to_journal = true;\n\tstruct seg_entry *se;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tif (!sit_i->dirty_sentries)\n\t\tgoto out;\n\n\t/*\n\t * add and account sit entries of dirty bitmap in sit entry\n\t * set temporarily\n\t */\n\tadd_sits_in_set(sbi);\n\n\t/*\n\t * if there are no enough space in journal to store dirty sit\n\t * entries, remove all entries from journal and add and account\n\t * them in sit entry set.\n\t */\n\tif (!__has_cursum_space(journal, sit_i->dirty_sentries, SIT_JOURNAL))\n\t\tremove_sits_in_journal(sbi);\n\n\t/*\n\t * there are two steps to flush sit entries:\n\t * #1, flush sit entries to journal in current cold data summary block.\n\t * #2, flush sit entries to sit page.\n\t */\n\tlist_for_each_entry_safe(ses, tmp, head, set_list) {\n\t\tstruct page *page = NULL;\n\t\tstruct f2fs_sit_block *raw_sit = NULL;\n\t\tunsigned int start_segno = ses->start_segno;\n\t\tunsigned int end = min(start_segno + SIT_ENTRY_PER_BLOCK,\n\t\t\t\t\t\t(unsigned long)MAIN_SEGS(sbi));\n\t\tunsigned int segno = start_segno;\n\n\t\tif (to_journal &&\n\t\t\t!__has_cursum_space(journal, ses->entry_cnt, SIT_JOURNAL))\n\t\t\tto_journal = false;\n\n\t\tif (to_journal) {\n\t\t\tdown_write(&curseg->journal_rwsem);\n\t\t} else {\n\t\t\tpage = get_next_sit_page(sbi, start_segno);\n\t\t\traw_sit = page_address(page);\n\t\t}\n\n\t\t/* flush dirty sit entries in region of current sit set */\n\t\tfor_each_set_bit_from(segno, bitmap, end) {\n\t\t\tint offset, sit_offset;\n\n\t\t\tse = get_seg_entry(sbi, segno);\n\n\t\t\t/* add discard candidates */\n\t\t\tif (!(cpc->reason & CP_DISCARD)) {\n\t\t\t\tcpc->trim_start = segno;\n\t\t\t\tadd_discard_addrs(sbi, cpc, false);\n\t\t\t}\n\n\t\t\tif (to_journal) {\n\t\t\t\toffset = lookup_journal_in_cursum(journal,\n\t\t\t\t\t\t\tSIT_JOURNAL, segno, 1);\n\t\t\t\tf2fs_bug_on(sbi, offset < 0);\n\t\t\t\tsegno_in_journal(journal, offset) =\n\t\t\t\t\t\t\tcpu_to_le32(segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t&sit_in_journal(journal, offset));\n\t\t\t} else {\n\t\t\t\tsit_offset = SIT_ENTRY_OFFSET(sit_i, segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t\t&raw_sit->entries[sit_offset]);\n\t\t\t}\n\n\t\t\t__clear_bit(segno, bitmap);\n\t\t\tsit_i->dirty_sentries--;\n\t\t\tses->entry_cnt--;\n\t\t}\n\n\t\tif (to_journal)\n\t\t\tup_write(&curseg->journal_rwsem);\n\t\telse\n\t\t\tf2fs_put_page(page, 1);\n\n\t\tf2fs_bug_on(sbi, ses->entry_cnt);\n\t\trelease_sit_entry_set(ses);\n\t}\n\n\tf2fs_bug_on(sbi, !list_empty(head));\n\tf2fs_bug_on(sbi, sit_i->dirty_sentries);\nout:\n\tif (cpc->reason & CP_DISCARD) {\n\t\t__u64 trim_start = cpc->trim_start;\n\n\t\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++)\n\t\t\tadd_discard_addrs(sbi, cpc, false);\n\n\t\tcpc->trim_start = trim_start;\n\t}\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\tset_prefree_as_free_segments(sbi);\n}\n\nstatic int build_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct sit_info *sit_i;\n\tunsigned int sit_segs, start;\n\tchar *src_bitmap;\n\tunsigned int bitmap_size;\n\n\t/* allocate memory for SIT information */\n\tsit_i = kzalloc(sizeof(struct sit_info), GFP_KERNEL);\n\tif (!sit_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->sit_info = sit_i;\n\n\tsit_i->sentries = kvzalloc(MAIN_SEGS(sbi) *\n\t\t\t\t\tsizeof(struct seg_entry), GFP_KERNEL);\n\tif (!sit_i->sentries)\n\t\treturn -ENOMEM;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tsit_i->dirty_sentries_bitmap = kvzalloc(bitmap_size, GFP_KERNEL);\n\tif (!sit_i->dirty_sentries_bitmap)\n\t\treturn -ENOMEM;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tsit_i->sentries[start].cur_valid_map\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tsit_i->sentries[start].ckpt_valid_map\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tif (!sit_i->sentries[start].cur_valid_map ||\n\t\t\t\t!sit_i->sentries[start].ckpt_valid_map)\n\t\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tsit_i->sentries[start].cur_valid_map_mir\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tif (!sit_i->sentries[start].cur_valid_map_mir)\n\t\t\treturn -ENOMEM;\n#endif\n\n\t\tif (f2fs_discard_en(sbi)) {\n\t\t\tsit_i->sentries[start].discard_map\n\t\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\t\tif (!sit_i->sentries[start].discard_map)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tsit_i->tmp_map = kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\tif (!sit_i->tmp_map)\n\t\treturn -ENOMEM;\n\n\tif (sbi->segs_per_sec > 1) {\n\t\tsit_i->sec_entries = kvzalloc(MAIN_SECS(sbi) *\n\t\t\t\t\tsizeof(struct sec_entry), GFP_KERNEL);\n\t\tif (!sit_i->sec_entries)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* get information related with SIT */\n\tsit_segs = le32_to_cpu(raw_super->segment_count_sit) >> 1;\n\n\t/* setup SIT bitmap from ckeckpoint pack */\n\tbitmap_size = __bitmap_size(sbi, SIT_BITMAP);\n\tsrc_bitmap = __bitmap_ptr(sbi, SIT_BITMAP);\n\n\tsit_i->sit_bitmap = kmemdup(src_bitmap, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap)\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\tsit_i->sit_bitmap_mir = kmemdup(src_bitmap, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap_mir)\n\t\treturn -ENOMEM;\n#endif\n\n\t/* init SIT information */\n\tsit_i->s_ops = &default_salloc_ops;\n\n\tsit_i->sit_base_addr = le32_to_cpu(raw_super->sit_blkaddr);\n\tsit_i->sit_blocks = sit_segs << sbi->log_blocks_per_seg;\n\tsit_i->written_valid_blocks = 0;\n\tsit_i->bitmap_size = bitmap_size;\n\tsit_i->dirty_sentries = 0;\n\tsit_i->sents_per_block = SIT_ENTRY_PER_BLOCK;\n\tsit_i->elapsed_time = le64_to_cpu(sbi->ckpt->elapsed_time);\n\tsit_i->mounted_time = ktime_get_real_seconds();\n\tmutex_init(&sit_i->sentry_lock);\n\treturn 0;\n}\n\nstatic int build_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i;\n\tunsigned int bitmap_size, sec_bitmap_size;\n\n\t/* allocate memory for free segmap information */\n\tfree_i = kzalloc(sizeof(struct free_segmap_info), GFP_KERNEL);\n\tif (!free_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->free_info = free_i;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tfree_i->free_segmap = kvmalloc(bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_segmap)\n\t\treturn -ENOMEM;\n\n\tsec_bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\tfree_i->free_secmap = kvmalloc(sec_bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_secmap)\n\t\treturn -ENOMEM;\n\n\t/* set all segments as dirty temporarily */\n\tmemset(free_i->free_segmap, 0xff, bitmap_size);\n\tmemset(free_i->free_secmap, 0xff, sec_bitmap_size);\n\n\t/* init free segmap information */\n\tfree_i->start_segno = GET_SEGNO_FROM_SEG0(sbi, MAIN_BLKADDR(sbi));\n\tfree_i->free_segments = 0;\n\tfree_i->free_sections = 0;\n\tspin_lock_init(&free_i->segmap_lock);\n\treturn 0;\n}\n\nstatic int build_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array;\n\tint i;\n\n\tarray = kcalloc(NR_CURSEG_TYPE, sizeof(*array), GFP_KERNEL);\n\tif (!array)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->curseg_array = array;\n\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tmutex_init(&array[i].curseg_mutex);\n\t\tarray[i].sum_blk = kzalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (!array[i].sum_blk)\n\t\t\treturn -ENOMEM;\n\t\tinit_rwsem(&array[i].journal_rwsem);\n\t\tarray[i].journal = kzalloc(sizeof(struct f2fs_journal),\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!array[i].journal)\n\t\t\treturn -ENOMEM;\n\t\tarray[i].segno = NULL_SEGNO;\n\t\tarray[i].next_blkoff = 0;\n\t}\n\treturn restore_curseg_summaries(sbi);\n}\n\nstatic void build_sit_entries(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct seg_entry *se;\n\tstruct f2fs_sit_entry sit;\n\tint sit_blk_cnt = SIT_BLK_CNT(sbi);\n\tunsigned int i, start, end;\n\tunsigned int readed, start_blk = 0;\n\n\tdo {\n\t\treaded = ra_meta_pages(sbi, start_blk, BIO_MAX_PAGES,\n\t\t\t\t\t\t\tMETA_SIT, true);\n\n\t\tstart = start_blk * sit_i->sents_per_block;\n\t\tend = (start_blk + readed) * sit_i->sents_per_block;\n\n\t\tfor (; start < end && start < MAIN_SEGS(sbi); start++) {\n\t\t\tstruct f2fs_sit_block *sit_blk;\n\t\t\tstruct page *page;\n\n\t\t\tse = &sit_i->sentries[start];\n\t\t\tpage = get_current_sit_page(sbi, start);\n\t\t\tsit_blk = (struct f2fs_sit_block *)page_address(page);\n\t\t\tsit = sit_blk->entries[SIT_ENTRY_OFFSET(sit_i, start)];\n\t\t\tf2fs_put_page(page, 1);\n\n\t\t\tcheck_block_count(sbi, start, &sit);\n\t\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\t\t/* build discard map only one time */\n\t\t\tif (f2fs_discard_en(sbi)) {\n\t\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t} else {\n\t\t\t\t\tmemcpy(se->discard_map,\n\t\t\t\t\t\tse->cur_valid_map,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t\tsbi->discard_blks +=\n\t\t\t\t\t\tsbi->blocks_per_seg -\n\t\t\t\t\t\tse->valid_blocks;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sbi->segs_per_sec > 1)\n\t\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t}\n\t\tstart_blk += readed;\n\t} while (start_blk < sit_blk_cnt);\n\n\tdown_read(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int old_valid_blocks;\n\n\t\tstart = le32_to_cpu(segno_in_journal(journal, i));\n\t\tse = &sit_i->sentries[start];\n\t\tsit = sit_in_journal(journal, i);\n\n\t\told_valid_blocks = se->valid_blocks;\n\n\t\tcheck_block_count(sbi, start, &sit);\n\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\tif (f2fs_discard_en(sbi)) {\n\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t} else {\n\t\t\t\tmemcpy(se->discard_map, se->cur_valid_map,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\tsbi->discard_blks += old_valid_blocks -\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t\t}\n\t\t}\n\n\t\tif (sbi->segs_per_sec > 1)\n\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\tse->valid_blocks - old_valid_blocks;\n\t}\n\tup_read(&curseg->journal_rwsem);\n}\n\nstatic void init_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tunsigned int start;\n\tint type;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, start);\n\t\tif (!sentry->valid_blocks)\n\t\t\t__set_free(sbi, start);\n\t\telse\n\t\t\tSIT_I(sbi)->written_valid_blocks +=\n\t\t\t\t\t\tsentry->valid_blocks;\n\t}\n\n\t/* set use the current segments */\n\tfor (type = CURSEG_HOT_DATA; type <= CURSEG_COLD_NODE; type++) {\n\t\tstruct curseg_info *curseg_t = CURSEG_I(sbi, type);\n\t\t__set_test_and_inuse(sbi, curseg_t->segno);\n\t}\n}\n\nstatic void init_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno = 0, offset = 0;\n\tunsigned short valid_blocks;\n\n\twhile (1) {\n\t\t/* find dirty segment based on free segmap */\n\t\tsegno = find_next_inuse(free_i, MAIN_SEGS(sbi), offset);\n\t\tif (segno >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\toffset = segno + 1;\n\t\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\t\tif (valid_blocks == sbi->blocks_per_seg || !valid_blocks)\n\t\t\tcontinue;\n\t\tif (valid_blocks > sbi->blocks_per_seg) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tcontinue;\n\t\t}\n\t\tmutex_lock(&dirty_i->seglist_lock);\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t\tmutex_unlock(&dirty_i->seglist_lock);\n\t}\n}\n\nstatic int init_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\n\tdirty_i->victim_secmap = kvzalloc(bitmap_size, GFP_KERNEL);\n\tif (!dirty_i->victim_secmap)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic int build_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i;\n\tunsigned int bitmap_size, i;\n\n\t/* allocate memory for dirty segments list information */\n\tdirty_i = kzalloc(sizeof(struct dirty_seglist_info), GFP_KERNEL);\n\tif (!dirty_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->dirty_info = dirty_i;\n\tmutex_init(&dirty_i->seglist_lock);\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++) {\n\t\tdirty_i->dirty_segmap[i] = kvzalloc(bitmap_size, GFP_KERNEL);\n\t\tif (!dirty_i->dirty_segmap[i])\n\t\t\treturn -ENOMEM;\n\t}\n\n\tinit_dirty_segmap(sbi);\n\treturn init_victim_secmap(sbi);\n}\n\n/*\n * Update min, max modified time for cost-benefit GC algorithm\n */\nstatic void init_min_max_mtime(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tsit_i->min_mtime = LLONG_MAX;\n\n\tfor (segno = 0; segno < MAIN_SEGS(sbi); segno += sbi->segs_per_sec) {\n\t\tunsigned int i;\n\t\tunsigned long long mtime = 0;\n\n\t\tfor (i = 0; i < sbi->segs_per_sec; i++)\n\t\t\tmtime += get_seg_entry(sbi, segno + i)->mtime;\n\n\t\tmtime = div_u64(mtime, sbi->segs_per_sec);\n\n\t\tif (sit_i->min_mtime > mtime)\n\t\t\tsit_i->min_mtime = mtime;\n\t}\n\tsit_i->max_mtime = get_mtime(sbi);\n\tmutex_unlock(&sit_i->sentry_lock);\n}\n\nint build_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_sm_info *sm_info;\n\tint err;\n\n\tsm_info = kzalloc(sizeof(struct f2fs_sm_info), GFP_KERNEL);\n\tif (!sm_info)\n\t\treturn -ENOMEM;\n\n\t/* init sm info */\n\tsbi->sm_info = sm_info;\n\tsm_info->seg0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tsm_info->main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tsm_info->segment_count = le32_to_cpu(raw_super->segment_count);\n\tsm_info->reserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\tsm_info->ovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\tsm_info->main_segments = le32_to_cpu(raw_super->segment_count_main);\n\tsm_info->ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tsm_info->rec_prefree_segments = sm_info->main_segments *\n\t\t\t\t\tDEF_RECLAIM_PREFREE_SEGMENTS / 100;\n\tif (sm_info->rec_prefree_segments > DEF_MAX_RECLAIM_PREFREE_SEGMENTS)\n\t\tsm_info->rec_prefree_segments = DEF_MAX_RECLAIM_PREFREE_SEGMENTS;\n\n\tif (!test_opt(sbi, LFS))\n\t\tsm_info->ipu_policy = 1 << F2FS_IPU_FSYNC;\n\tsm_info->min_ipu_util = DEF_MIN_IPU_UTIL;\n\tsm_info->min_fsync_blocks = DEF_MIN_FSYNC_BLOCKS;\n\tsm_info->min_hot_blocks = DEF_MIN_HOT_BLOCKS;\n\n\tsm_info->trim_sections = DEF_BATCHED_TRIM_SECTIONS;\n\n\tINIT_LIST_HEAD(&sm_info->sit_entry_set);\n\n\tif (!f2fs_readonly(sbi->sb)) {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = create_discard_cmd_control(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = build_sit_info(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_free_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_curseg(sbi);\n\tif (err)\n\t\treturn err;\n\n\t/* reinit free segmap based on SIT */\n\tbuild_sit_entries(sbi);\n\n\tinit_free_segmap(sbi);\n\terr = build_dirty_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\n\tinit_min_max_mtime(sbi);\n\treturn 0;\n}\n\nstatic void discard_dirty_segmap(struct f2fs_sb_info *sbi,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tkvfree(dirty_i->dirty_segmap[dirty_type]);\n\tdirty_i->nr_dirty[dirty_type] = 0;\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic void destroy_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tkvfree(dirty_i->victim_secmap);\n}\n\nstatic void destroy_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tint i;\n\n\tif (!dirty_i)\n\t\treturn;\n\n\t/* discard pre-free/dirty segments list */\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++)\n\t\tdiscard_dirty_segmap(sbi, i);\n\n\tdestroy_victim_secmap(sbi);\n\tSM_I(sbi)->dirty_info = NULL;\n\tkfree(dirty_i);\n}\n\nstatic void destroy_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array = SM_I(sbi)->curseg_array;\n\tint i;\n\n\tif (!array)\n\t\treturn;\n\tSM_I(sbi)->curseg_array = NULL;\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tkfree(array[i].sum_blk);\n\t\tkfree(array[i].journal);\n\t}\n\tkfree(array);\n}\n\nstatic void destroy_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i = SM_I(sbi)->free_info;\n\tif (!free_i)\n\t\treturn;\n\tSM_I(sbi)->free_info = NULL;\n\tkvfree(free_i->free_segmap);\n\tkvfree(free_i->free_secmap);\n\tkfree(free_i);\n}\n\nstatic void destroy_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int start;\n\n\tif (!sit_i)\n\t\treturn;\n\n\tif (sit_i->sentries) {\n\t\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\t\tkfree(sit_i->sentries[start].cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tkfree(sit_i->sentries[start].cur_valid_map_mir);\n#endif\n\t\t\tkfree(sit_i->sentries[start].ckpt_valid_map);\n\t\t\tkfree(sit_i->sentries[start].discard_map);\n\t\t}\n\t}\n\tkfree(sit_i->tmp_map);\n\n\tkvfree(sit_i->sentries);\n\tkvfree(sit_i->sec_entries);\n\tkvfree(sit_i->dirty_sentries_bitmap);\n\n\tSM_I(sbi)->sit_info = NULL;\n\tkfree(sit_i->sit_bitmap);\n#ifdef CONFIG_F2FS_CHECK_FS\n\tkfree(sit_i->sit_bitmap_mir);\n#endif\n\tkfree(sit_i);\n}\n\nvoid destroy_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\n\tif (!sm_info)\n\t\treturn;\n\tdestroy_flush_cmd_control(sbi, true);\n\tdestroy_discard_cmd_control(sbi);\n\tdestroy_dirty_segmap(sbi);\n\tdestroy_curseg(sbi);\n\tdestroy_free_segmap(sbi);\n\tdestroy_sit_info(sbi);\n\tsbi->sm_info = NULL;\n\tkfree(sm_info);\n}\n\nint __init create_segment_manager_caches(void)\n{\n\tdiscard_entry_slab = f2fs_kmem_cache_create(\"discard_entry\",\n\t\t\tsizeof(struct discard_entry));\n\tif (!discard_entry_slab)\n\t\tgoto fail;\n\n\tdiscard_cmd_slab = f2fs_kmem_cache_create(\"discard_cmd\",\n\t\t\tsizeof(struct discard_cmd));\n\tif (!discard_cmd_slab)\n\t\tgoto destroy_discard_entry;\n\n\tsit_entry_set_slab = f2fs_kmem_cache_create(\"sit_entry_set\",\n\t\t\tsizeof(struct sit_entry_set));\n\tif (!sit_entry_set_slab)\n\t\tgoto destroy_discard_cmd;\n\n\tinmem_entry_slab = f2fs_kmem_cache_create(\"inmem_page_entry\",\n\t\t\tsizeof(struct inmem_pages));\n\tif (!inmem_entry_slab)\n\t\tgoto destroy_sit_entry_set;\n\treturn 0;\n\ndestroy_sit_entry_set:\n\tkmem_cache_destroy(sit_entry_set_slab);\ndestroy_discard_cmd:\n\tkmem_cache_destroy(discard_cmd_slab);\ndestroy_discard_entry:\n\tkmem_cache_destroy(discard_entry_slab);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid destroy_segment_manager_caches(void)\n{\n\tkmem_cache_destroy(sit_entry_set_slab);\n\tkmem_cache_destroy(discard_cmd_slab);\n\tkmem_cache_destroy(discard_entry_slab);\n\tkmem_cache_destroy(inmem_entry_slab);\n}\n", "/*\n * fs/f2fs/super.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/statfs.h>\n#include <linux/buffer_head.h>\n#include <linux/backing-dev.h>\n#include <linux/kthread.h>\n#include <linux/parser.h>\n#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/random.h>\n#include <linux/exportfs.h>\n#include <linux/blkdev.h>\n#include <linux/quotaops.h>\n#include <linux/f2fs_fs.h>\n#include <linux/sysfs.h>\n#include <linux/quota.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"xattr.h\"\n#include \"gc.h\"\n#include \"trace.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/f2fs.h>\n\nstatic struct kmem_cache *f2fs_inode_cachep;\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\nchar *fault_name[FAULT_MAX] = {\n\t[FAULT_KMALLOC]\t\t= \"kmalloc\",\n\t[FAULT_PAGE_ALLOC]\t= \"page alloc\",\n\t[FAULT_ALLOC_NID]\t= \"alloc nid\",\n\t[FAULT_ORPHAN]\t\t= \"orphan\",\n\t[FAULT_BLOCK]\t\t= \"no more block\",\n\t[FAULT_DIR_DEPTH]\t= \"too big dir depth\",\n\t[FAULT_EVICT_INODE]\t= \"evict_inode fail\",\n\t[FAULT_TRUNCATE]\t= \"truncate fail\",\n\t[FAULT_IO]\t\t= \"IO error\",\n\t[FAULT_CHECKPOINT]\t= \"checkpoint error\",\n};\n\nstatic void f2fs_build_fault_attr(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tunsigned int rate)\n{\n\tstruct f2fs_fault_info *ffi = &sbi->fault_info;\n\n\tif (rate) {\n\t\tatomic_set(&ffi->inject_ops, 0);\n\t\tffi->inject_rate = rate;\n\t\tffi->inject_type = (1 << FAULT_MAX) - 1;\n\t} else {\n\t\tmemset(ffi, 0, sizeof(struct f2fs_fault_info));\n\t}\n}\n#endif\n\n/* f2fs-wide shrinker description */\nstatic struct shrinker f2fs_shrinker_info = {\n\t.scan_objects = f2fs_shrink_scan,\n\t.count_objects = f2fs_shrink_count,\n\t.seeks = DEFAULT_SEEKS,\n};\n\nenum {\n\tOpt_gc_background,\n\tOpt_disable_roll_forward,\n\tOpt_norecovery,\n\tOpt_discard,\n\tOpt_nodiscard,\n\tOpt_noheap,\n\tOpt_heap,\n\tOpt_user_xattr,\n\tOpt_nouser_xattr,\n\tOpt_acl,\n\tOpt_noacl,\n\tOpt_active_logs,\n\tOpt_disable_ext_identify,\n\tOpt_inline_xattr,\n\tOpt_noinline_xattr,\n\tOpt_inline_data,\n\tOpt_inline_dentry,\n\tOpt_noinline_dentry,\n\tOpt_flush_merge,\n\tOpt_noflush_merge,\n\tOpt_nobarrier,\n\tOpt_fastboot,\n\tOpt_extent_cache,\n\tOpt_noextent_cache,\n\tOpt_noinline_data,\n\tOpt_data_flush,\n\tOpt_mode,\n\tOpt_io_size_bits,\n\tOpt_fault_injection,\n\tOpt_lazytime,\n\tOpt_nolazytime,\n\tOpt_quota,\n\tOpt_noquota,\n\tOpt_usrquota,\n\tOpt_grpquota,\n\tOpt_prjquota,\n\tOpt_usrjquota,\n\tOpt_grpjquota,\n\tOpt_prjjquota,\n\tOpt_offusrjquota,\n\tOpt_offgrpjquota,\n\tOpt_offprjjquota,\n\tOpt_jqfmt_vfsold,\n\tOpt_jqfmt_vfsv0,\n\tOpt_jqfmt_vfsv1,\n\tOpt_err,\n};\n\nstatic match_table_t f2fs_tokens = {\n\t{Opt_gc_background, \"background_gc=%s\"},\n\t{Opt_disable_roll_forward, \"disable_roll_forward\"},\n\t{Opt_norecovery, \"norecovery\"},\n\t{Opt_discard, \"discard\"},\n\t{Opt_nodiscard, \"nodiscard\"},\n\t{Opt_noheap, \"no_heap\"},\n\t{Opt_heap, \"heap\"},\n\t{Opt_user_xattr, \"user_xattr\"},\n\t{Opt_nouser_xattr, \"nouser_xattr\"},\n\t{Opt_acl, \"acl\"},\n\t{Opt_noacl, \"noacl\"},\n\t{Opt_active_logs, \"active_logs=%u\"},\n\t{Opt_disable_ext_identify, \"disable_ext_identify\"},\n\t{Opt_inline_xattr, \"inline_xattr\"},\n\t{Opt_noinline_xattr, \"noinline_xattr\"},\n\t{Opt_inline_data, \"inline_data\"},\n\t{Opt_inline_dentry, \"inline_dentry\"},\n\t{Opt_noinline_dentry, \"noinline_dentry\"},\n\t{Opt_flush_merge, \"flush_merge\"},\n\t{Opt_noflush_merge, \"noflush_merge\"},\n\t{Opt_nobarrier, \"nobarrier\"},\n\t{Opt_fastboot, \"fastboot\"},\n\t{Opt_extent_cache, \"extent_cache\"},\n\t{Opt_noextent_cache, \"noextent_cache\"},\n\t{Opt_noinline_data, \"noinline_data\"},\n\t{Opt_data_flush, \"data_flush\"},\n\t{Opt_mode, \"mode=%s\"},\n\t{Opt_io_size_bits, \"io_bits=%u\"},\n\t{Opt_fault_injection, \"fault_injection=%u\"},\n\t{Opt_lazytime, \"lazytime\"},\n\t{Opt_nolazytime, \"nolazytime\"},\n\t{Opt_quota, \"quota\"},\n\t{Opt_noquota, \"noquota\"},\n\t{Opt_usrquota, \"usrquota\"},\n\t{Opt_grpquota, \"grpquota\"},\n\t{Opt_prjquota, \"prjquota\"},\n\t{Opt_usrjquota, \"usrjquota=%s\"},\n\t{Opt_grpjquota, \"grpjquota=%s\"},\n\t{Opt_prjjquota, \"prjjquota=%s\"},\n\t{Opt_offusrjquota, \"usrjquota=\"},\n\t{Opt_offgrpjquota, \"grpjquota=\"},\n\t{Opt_offprjjquota, \"prjjquota=\"},\n\t{Opt_jqfmt_vfsold, \"jqfmt=vfsold\"},\n\t{Opt_jqfmt_vfsv0, \"jqfmt=vfsv0\"},\n\t{Opt_jqfmt_vfsv1, \"jqfmt=vfsv1\"},\n\t{Opt_err, NULL},\n};\n\nvoid f2fs_msg(struct super_block *sb, const char *level, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk_ratelimited(\"%sF2FS-fs (%s): %pV\\n\", level, sb->s_id, &vaf);\n\tva_end(args);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct f2fs_inode_info *fi = (struct f2fs_inode_info *) foo;\n\n\tinode_init_once(&fi->vfs_inode);\n}\n\n#ifdef CONFIG_QUOTA\nstatic const char * const quotatypes[] = INITQFNAMES;\n#define QTYPE2NAME(t) (quotatypes[t])\nstatic int f2fs_set_qf_name(struct super_block *sb, int qtype,\n\t\t\t\t\t\t\tsubstring_t *args)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tchar *qname;\n\tint ret = -EINVAL;\n\n\tif (sb_any_quota_loaded(sb) && !sbi->s_qf_names[qtype]) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Cannot change journaled \"\n\t\t\t\"quota options when quota turned on\");\n\t\treturn -EINVAL;\n\t}\n\tqname = match_strdup(args);\n\tif (!qname) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Not enough memory for storing quotafile name\");\n\t\treturn -EINVAL;\n\t}\n\tif (sbi->s_qf_names[qtype]) {\n\t\tif (strcmp(sbi->s_qf_names[qtype], qname) == 0)\n\t\t\tret = 0;\n\t\telse\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t \"%s quota file already specified\",\n\t\t\t\t QTYPE2NAME(qtype));\n\t\tgoto errout;\n\t}\n\tif (strchr(qname, '/')) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"quotafile must be on filesystem root\");\n\t\tgoto errout;\n\t}\n\tsbi->s_qf_names[qtype] = qname;\n\tset_opt(sbi, QUOTA);\n\treturn 0;\nerrout:\n\tkfree(qname);\n\treturn ret;\n}\n\nstatic int f2fs_clear_qf_name(struct super_block *sb, int qtype)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\n\tif (sb_any_quota_loaded(sb) && sbi->s_qf_names[qtype]) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Cannot change journaled quota options\"\n\t\t\t\" when quota turned on\");\n\t\treturn -EINVAL;\n\t}\n\tkfree(sbi->s_qf_names[qtype]);\n\tsbi->s_qf_names[qtype] = NULL;\n\treturn 0;\n}\n\nstatic int f2fs_check_quota_options(struct f2fs_sb_info *sbi)\n{\n\t/*\n\t * We do the test below only for project quotas. 'usrquota' and\n\t * 'grpquota' mount options are allowed even without quota feature\n\t * to support legacy quotas in quota files.\n\t */\n\tif (test_opt(sbi, PRJQUOTA) && !f2fs_sb_has_project_quota(sbi->sb)) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR, \"Project quota feature not enabled. \"\n\t\t\t \"Cannot enable project quota enforcement.\");\n\t\treturn -1;\n\t}\n\tif (sbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA] ||\n\t\t\tsbi->s_qf_names[PRJQUOTA]) {\n\t\tif (test_opt(sbi, USRQUOTA) && sbi->s_qf_names[USRQUOTA])\n\t\t\tclear_opt(sbi, USRQUOTA);\n\n\t\tif (test_opt(sbi, GRPQUOTA) && sbi->s_qf_names[GRPQUOTA])\n\t\t\tclear_opt(sbi, GRPQUOTA);\n\n\t\tif (test_opt(sbi, PRJQUOTA) && sbi->s_qf_names[PRJQUOTA])\n\t\t\tclear_opt(sbi, PRJQUOTA);\n\n\t\tif (test_opt(sbi, GRPQUOTA) || test_opt(sbi, USRQUOTA) ||\n\t\t\t\ttest_opt(sbi, PRJQUOTA)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR, \"old and new quota \"\n\t\t\t\t\t\"format mixing\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!sbi->s_jquota_fmt) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR, \"journaled quota format \"\n\t\t\t\t\t\"not specified\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n#endif\n\nstatic int parse_options(struct super_block *sb, char *options)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct request_queue *q;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tchar *p, *name;\n\tint arg = 0;\n#ifdef CONFIG_QUOTA\n\tint ret;\n#endif\n\n\tif (!options)\n\t\treturn 0;\n\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tint token;\n\t\tif (!*p)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Initialize args struct so we know whether arg was\n\t\t * found; some options take optional arguments.\n\t\t */\n\t\targs[0].to = args[0].from = NULL;\n\t\ttoken = match_token(p, f2fs_tokens, args);\n\n\t\tswitch (token) {\n\t\tcase Opt_gc_background:\n\t\t\tname = match_strdup(&args[0]);\n\n\t\t\tif (!name)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (strlen(name) == 2 && !strncmp(name, \"on\", 2)) {\n\t\t\t\tset_opt(sbi, BG_GC);\n\t\t\t\tclear_opt(sbi, FORCE_FG_GC);\n\t\t\t} else if (strlen(name) == 3 && !strncmp(name, \"off\", 3)) {\n\t\t\t\tclear_opt(sbi, BG_GC);\n\t\t\t\tclear_opt(sbi, FORCE_FG_GC);\n\t\t\t} else if (strlen(name) == 4 && !strncmp(name, \"sync\", 4)) {\n\t\t\t\tset_opt(sbi, BG_GC);\n\t\t\t\tset_opt(sbi, FORCE_FG_GC);\n\t\t\t} else {\n\t\t\t\tkfree(name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tkfree(name);\n\t\t\tbreak;\n\t\tcase Opt_disable_roll_forward:\n\t\t\tset_opt(sbi, DISABLE_ROLL_FORWARD);\n\t\t\tbreak;\n\t\tcase Opt_norecovery:\n\t\t\t/* this option mounts f2fs with ro */\n\t\t\tset_opt(sbi, DISABLE_ROLL_FORWARD);\n\t\t\tif (!f2fs_readonly(sb))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase Opt_discard:\n\t\t\tq = bdev_get_queue(sb->s_bdev);\n\t\t\tif (blk_queue_discard(q)) {\n\t\t\t\tset_opt(sbi, DISCARD);\n\t\t\t} else if (!f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"mounting with \\\"discard\\\" option, but \"\n\t\t\t\t\t\"the device does not support discard\");\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Opt_nodiscard:\n\t\t\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"discard is required for zoned block devices\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tclear_opt(sbi, DISCARD);\n\t\t\tbreak;\n\t\tcase Opt_noheap:\n\t\t\tset_opt(sbi, NOHEAP);\n\t\t\tbreak;\n\t\tcase Opt_heap:\n\t\t\tclear_opt(sbi, NOHEAP);\n\t\t\tbreak;\n#ifdef CONFIG_F2FS_FS_XATTR\n\t\tcase Opt_user_xattr:\n\t\t\tset_opt(sbi, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tclear_opt(sbi, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_inline_xattr:\n\t\t\tset_opt(sbi, INLINE_XATTR);\n\t\t\tbreak;\n\t\tcase Opt_noinline_xattr:\n\t\t\tclear_opt(sbi, INLINE_XATTR);\n\t\t\tbreak;\n#else\n\t\tcase Opt_user_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"user_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"nouser_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_inline_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"inline_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noinline_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"noinline_xattr options not supported\");\n\t\t\tbreak;\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\t\tcase Opt_acl:\n\t\t\tset_opt(sbi, POSIX_ACL);\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tclear_opt(sbi, POSIX_ACL);\n\t\t\tbreak;\n#else\n\t\tcase Opt_acl:\n\t\t\tf2fs_msg(sb, KERN_INFO, \"acl options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tf2fs_msg(sb, KERN_INFO, \"noacl options not supported\");\n\t\t\tbreak;\n#endif\n\t\tcase Opt_active_logs:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (arg != 2 && arg != 4 && arg != NR_CURSEG_TYPE)\n\t\t\t\treturn -EINVAL;\n\t\t\tsbi->active_logs = arg;\n\t\t\tbreak;\n\t\tcase Opt_disable_ext_identify:\n\t\t\tset_opt(sbi, DISABLE_EXT_IDENTIFY);\n\t\t\tbreak;\n\t\tcase Opt_inline_data:\n\t\t\tset_opt(sbi, INLINE_DATA);\n\t\t\tbreak;\n\t\tcase Opt_inline_dentry:\n\t\t\tset_opt(sbi, INLINE_DENTRY);\n\t\t\tbreak;\n\t\tcase Opt_noinline_dentry:\n\t\t\tclear_opt(sbi, INLINE_DENTRY);\n\t\t\tbreak;\n\t\tcase Opt_flush_merge:\n\t\t\tset_opt(sbi, FLUSH_MERGE);\n\t\t\tbreak;\n\t\tcase Opt_noflush_merge:\n\t\t\tclear_opt(sbi, FLUSH_MERGE);\n\t\t\tbreak;\n\t\tcase Opt_nobarrier:\n\t\t\tset_opt(sbi, NOBARRIER);\n\t\t\tbreak;\n\t\tcase Opt_fastboot:\n\t\t\tset_opt(sbi, FASTBOOT);\n\t\t\tbreak;\n\t\tcase Opt_extent_cache:\n\t\t\tset_opt(sbi, EXTENT_CACHE);\n\t\t\tbreak;\n\t\tcase Opt_noextent_cache:\n\t\t\tclear_opt(sbi, EXTENT_CACHE);\n\t\t\tbreak;\n\t\tcase Opt_noinline_data:\n\t\t\tclear_opt(sbi, INLINE_DATA);\n\t\t\tbreak;\n\t\tcase Opt_data_flush:\n\t\t\tset_opt(sbi, DATA_FLUSH);\n\t\t\tbreak;\n\t\tcase Opt_mode:\n\t\t\tname = match_strdup(&args[0]);\n\n\t\t\tif (!name)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (strlen(name) == 8 &&\n\t\t\t\t\t!strncmp(name, \"adaptive\", 8)) {\n\t\t\t\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\t \"adaptive mode is not allowed with \"\n\t\t\t\t\t\t \"zoned block device feature\");\n\t\t\t\t\tkfree(name);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tset_opt_mode(sbi, F2FS_MOUNT_ADAPTIVE);\n\t\t\t} else if (strlen(name) == 3 &&\n\t\t\t\t\t!strncmp(name, \"lfs\", 3)) {\n\t\t\t\tset_opt_mode(sbi, F2FS_MOUNT_LFS);\n\t\t\t} else {\n\t\t\t\tkfree(name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tkfree(name);\n\t\t\tbreak;\n\t\tcase Opt_io_size_bits:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (arg > __ilog2_u32(BIO_MAX_PAGES)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"Not support %d, larger than %d\",\n\t\t\t\t\t1 << arg, BIO_MAX_PAGES);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsbi->write_io_size_bits = arg;\n\t\t\tbreak;\n\t\tcase Opt_fault_injection:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\t\t\tf2fs_build_fault_attr(sbi, arg);\n\t\t\tset_opt(sbi, FAULT_INJECTION);\n#else\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"FAULT_INJECTION was not selected\");\n#endif\n\t\t\tbreak;\n\t\tcase Opt_lazytime:\n\t\t\tsb->s_flags |= MS_LAZYTIME;\n\t\t\tbreak;\n\t\tcase Opt_nolazytime:\n\t\t\tsb->s_flags &= ~MS_LAZYTIME;\n\t\t\tbreak;\n#ifdef CONFIG_QUOTA\n\t\tcase Opt_quota:\n\t\tcase Opt_usrquota:\n\t\t\tset_opt(sbi, USRQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_grpquota:\n\t\t\tset_opt(sbi, GRPQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_prjquota:\n\t\t\tset_opt(sbi, PRJQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_usrjquota:\n\t\t\tret = f2fs_set_qf_name(sb, USRQUOTA, &args[0]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_grpjquota:\n\t\t\tret = f2fs_set_qf_name(sb, GRPQUOTA, &args[0]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_prjjquota:\n\t\t\tret = f2fs_set_qf_name(sb, PRJQUOTA, &args[0]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_offusrjquota:\n\t\t\tret = f2fs_clear_qf_name(sb, USRQUOTA);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_offgrpjquota:\n\t\t\tret = f2fs_clear_qf_name(sb, GRPQUOTA);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_offprjjquota:\n\t\t\tret = f2fs_clear_qf_name(sb, PRJQUOTA);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_jqfmt_vfsold:\n\t\t\tsbi->s_jquota_fmt = QFMT_VFS_OLD;\n\t\t\tbreak;\n\t\tcase Opt_jqfmt_vfsv0:\n\t\t\tsbi->s_jquota_fmt = QFMT_VFS_V0;\n\t\t\tbreak;\n\t\tcase Opt_jqfmt_vfsv1:\n\t\t\tsbi->s_jquota_fmt = QFMT_VFS_V1;\n\t\t\tbreak;\n\t\tcase Opt_noquota:\n\t\t\tclear_opt(sbi, QUOTA);\n\t\t\tclear_opt(sbi, USRQUOTA);\n\t\t\tclear_opt(sbi, GRPQUOTA);\n\t\t\tclear_opt(sbi, PRJQUOTA);\n\t\t\tbreak;\n#else\n\t\tcase Opt_quota:\n\t\tcase Opt_usrquota:\n\t\tcase Opt_grpquota:\n\t\tcase Opt_prjquota:\n\t\tcase Opt_usrjquota:\n\t\tcase Opt_grpjquota:\n\t\tcase Opt_prjjquota:\n\t\tcase Opt_offusrjquota:\n\t\tcase Opt_offgrpjquota:\n\t\tcase Opt_offprjjquota:\n\t\tcase Opt_jqfmt_vfsold:\n\t\tcase Opt_jqfmt_vfsv0:\n\t\tcase Opt_jqfmt_vfsv1:\n\t\tcase Opt_noquota:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\t\"quota operations not supported\");\n\t\t\tbreak;\n#endif\n\t\tdefault:\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Unrecognized mount option \\\"%s\\\" or missing value\",\n\t\t\t\tp);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n#ifdef CONFIG_QUOTA\n\tif (f2fs_check_quota_options(sbi))\n\t\treturn -EINVAL;\n#endif\n\n\tif (F2FS_IO_SIZE_BITS(sbi) && !test_opt(sbi, LFS)) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Should set mode=lfs with %uKB-sized IO\",\n\t\t\t\tF2FS_IO_SIZE_KB(sbi));\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic struct inode *f2fs_alloc_inode(struct super_block *sb)\n{\n\tstruct f2fs_inode_info *fi;\n\n\tfi = kmem_cache_alloc(f2fs_inode_cachep, GFP_F2FS_ZERO);\n\tif (!fi)\n\t\treturn NULL;\n\n\tinit_once((void *) fi);\n\n\t/* Initialize f2fs-specific inode info */\n\tfi->vfs_inode.i_version = 1;\n\tatomic_set(&fi->dirty_pages, 0);\n\tfi->i_current_depth = 1;\n\tfi->i_advise = 0;\n\tinit_rwsem(&fi->i_sem);\n\tINIT_LIST_HEAD(&fi->dirty_list);\n\tINIT_LIST_HEAD(&fi->gdirty_list);\n\tINIT_LIST_HEAD(&fi->inmem_pages);\n\tmutex_init(&fi->inmem_lock);\n\tinit_rwsem(&fi->dio_rwsem[READ]);\n\tinit_rwsem(&fi->dio_rwsem[WRITE]);\n\tinit_rwsem(&fi->i_mmap_sem);\n\tinit_rwsem(&fi->i_xattr_sem);\n\n#ifdef CONFIG_QUOTA\n\tmemset(&fi->i_dquot, 0, sizeof(fi->i_dquot));\n\tfi->i_reserved_quota = 0;\n#endif\n\t/* Will be used by directory only */\n\tfi->i_dir_level = F2FS_SB(sb)->dir_level;\n\n\treturn &fi->vfs_inode;\n}\n\nstatic int f2fs_drop_inode(struct inode *inode)\n{\n\tint ret;\n\t/*\n\t * This is to avoid a deadlock condition like below.\n\t * writeback_single_inode(inode)\n\t *  - f2fs_write_data_page\n\t *    - f2fs_gc -> iput -> evict\n\t *       - inode_wait_for_writeback(inode)\n\t */\n\tif ((!inode_unhashed(inode) && inode->i_state & I_SYNC)) {\n\t\tif (!inode->i_nlink && !is_bad_inode(inode)) {\n\t\t\t/* to avoid evict_inode call simultaneously */\n\t\t\tatomic_inc(&inode->i_count);\n\t\t\tspin_unlock(&inode->i_lock);\n\n\t\t\t/* some remained atomic pages should discarded */\n\t\t\tif (f2fs_is_atomic_file(inode))\n\t\t\t\tdrop_inmem_pages(inode);\n\n\t\t\t/* should remain fi->extent_tree for writepage */\n\t\t\tf2fs_destroy_extent_node(inode);\n\n\t\t\tsb_start_intwrite(inode->i_sb);\n\t\t\tf2fs_i_size_write(inode, 0);\n\n\t\t\tif (F2FS_HAS_BLOCKS(inode))\n\t\t\t\tf2fs_truncate(inode);\n\n\t\t\tsb_end_intwrite(inode->i_sb);\n\n\t\t\tfscrypt_put_encryption_info(inode, NULL);\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tatomic_dec(&inode->i_count);\n\t\t}\n\t\ttrace_f2fs_drop_inode(inode, 0);\n\t\treturn 0;\n\t}\n\tret = generic_drop_inode(inode);\n\ttrace_f2fs_drop_inode(inode, ret);\n\treturn ret;\n}\n\nint f2fs_inode_dirtied(struct inode *inode, bool sync)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint ret = 0;\n\n\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\tif (is_inode_flag_set(inode, FI_DIRTY_INODE)) {\n\t\tret = 1;\n\t} else {\n\t\tset_inode_flag(inode, FI_DIRTY_INODE);\n\t\tstat_inc_dirty_inode(sbi, DIRTY_META);\n\t}\n\tif (sync && list_empty(&F2FS_I(inode)->gdirty_list)) {\n\t\tlist_add_tail(&F2FS_I(inode)->gdirty_list,\n\t\t\t\t&sbi->inode_list[DIRTY_META]);\n\t\tinc_page_count(sbi, F2FS_DIRTY_IMETA);\n\t}\n\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\treturn ret;\n}\n\nvoid f2fs_inode_synced(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\tif (!is_inode_flag_set(inode, FI_DIRTY_INODE)) {\n\t\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\t\treturn;\n\t}\n\tif (!list_empty(&F2FS_I(inode)->gdirty_list)) {\n\t\tlist_del_init(&F2FS_I(inode)->gdirty_list);\n\t\tdec_page_count(sbi, F2FS_DIRTY_IMETA);\n\t}\n\tclear_inode_flag(inode, FI_DIRTY_INODE);\n\tclear_inode_flag(inode, FI_AUTO_RECOVER);\n\tstat_dec_dirty_inode(F2FS_I_SB(inode), DIRTY_META);\n\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n}\n\n/*\n * f2fs_dirty_inode() is called from __mark_inode_dirty()\n *\n * We should call set_dirty_inode to write the dirty inode through write_inode.\n */\nstatic void f2fs_dirty_inode(struct inode *inode, int flags)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_NODE_INO(sbi) ||\n\t\t\tinode->i_ino == F2FS_META_INO(sbi))\n\t\treturn;\n\n\tif (flags == I_DIRTY_TIME)\n\t\treturn;\n\n\tif (is_inode_flag_set(inode, FI_AUTO_RECOVER))\n\t\tclear_inode_flag(inode, FI_AUTO_RECOVER);\n\n\tf2fs_inode_dirtied(inode, false);\n}\n\nstatic void f2fs_i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(f2fs_inode_cachep, F2FS_I(inode));\n}\n\nstatic void f2fs_destroy_inode(struct inode *inode)\n{\n\tcall_rcu(&inode->i_rcu, f2fs_i_callback);\n}\n\nstatic void destroy_percpu_info(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_destroy(&sbi->alloc_valid_block_count);\n\tpercpu_counter_destroy(&sbi->total_valid_inode_count);\n}\n\nstatic void destroy_device_list(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tblkdev_put(FDEV(i).bdev, FMODE_EXCL);\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tkfree(FDEV(i).blkz_type);\n#endif\n\t}\n\tkfree(sbi->devs);\n}\n\nstatic void f2fs_put_super(struct super_block *sb)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tint i;\n\n\tf2fs_quota_off_umount(sb);\n\n\t/* prevent remaining shrinker jobs */\n\tmutex_lock(&sbi->umount_mutex);\n\n\t/*\n\t * We don't need to do checkpoint when superblock is clean.\n\t * But, the previous checkpoint was not done by umount, it needs to do\n\t * clean checkpoint again.\n\t */\n\tif (is_sbi_flag_set(sbi, SBI_IS_DIRTY) ||\n\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* be sure to wait for any on-going discard commands */\n\tf2fs_wait_discard_bios(sbi);\n\n\tif (f2fs_discard_en(sbi) && !sbi->discard_blks) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT | CP_TRIMMED,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* write_checkpoint can update stat informaion */\n\tf2fs_destroy_stats(sbi);\n\n\t/*\n\t * normally superblock is clean, so we need to release this.\n\t * In addition, EIO will skip do checkpoint, we need this as well.\n\t */\n\trelease_ino_entry(sbi, true);\n\n\tf2fs_leave_shrinker(sbi);\n\tmutex_unlock(&sbi->umount_mutex);\n\n\t/* our cp_error case, we can wait for any writeback page */\n\tf2fs_flush_merged_writes(sbi);\n\n\tiput(sbi->node_inode);\n\tiput(sbi->meta_inode);\n\n\t/* destroy f2fs internal modules */\n\tdestroy_node_manager(sbi);\n\tdestroy_segment_manager(sbi);\n\n\tkfree(sbi->ckpt);\n\n\tf2fs_unregister_sysfs(sbi);\n\n\tsb->s_fs_info = NULL;\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi->raw_super);\n\n\tdestroy_device_list(sbi);\n\tmempool_destroy(sbi->write_io_dummy);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\tdestroy_percpu_info(sbi);\n\tfor (i = 0; i < NR_PAGE_TYPE; i++)\n\t\tkfree(sbi->write_io[i]);\n\tkfree(sbi);\n}\n\nint f2fs_sync_fs(struct super_block *sb, int sync)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tint err = 0;\n\n\ttrace_f2fs_sync_fs(sb, sync);\n\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\treturn -EAGAIN;\n\n\tif (sync) {\n\t\tstruct cp_control cpc;\n\n\t\tcpc.reason = __get_cp_reason(sbi);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t}\n\tf2fs_trace_ios(NULL, 1);\n\n\treturn err;\n}\n\nstatic int f2fs_freeze(struct super_block *sb)\n{\n\tif (f2fs_readonly(sb))\n\t\treturn 0;\n\n\t/* IO error happened before */\n\tif (unlikely(f2fs_cp_error(F2FS_SB(sb))))\n\t\treturn -EIO;\n\n\t/* must be clean, since sync_filesystem() was already called */\n\tif (is_sbi_flag_set(F2FS_SB(sb), SBI_IS_DIRTY))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int f2fs_unfreeze(struct super_block *sb)\n{\n\treturn 0;\n}\n\n#ifdef CONFIG_QUOTA\nstatic int f2fs_statfs_project(struct super_block *sb,\n\t\t\t\tkprojid_t projid, struct kstatfs *buf)\n{\n\tstruct kqid qid;\n\tstruct dquot *dquot;\n\tu64 limit;\n\tu64 curblock;\n\n\tqid = make_kqid_projid(projid);\n\tdquot = dqget(sb, qid);\n\tif (IS_ERR(dquot))\n\t\treturn PTR_ERR(dquot);\n\tspin_lock(&dq_data_lock);\n\n\tlimit = (dquot->dq_dqb.dqb_bsoftlimit ?\n\t\t dquot->dq_dqb.dqb_bsoftlimit :\n\t\t dquot->dq_dqb.dqb_bhardlimit) >> sb->s_blocksize_bits;\n\tif (limit && buf->f_blocks > limit) {\n\t\tcurblock = dquot->dq_dqb.dqb_curspace >> sb->s_blocksize_bits;\n\t\tbuf->f_blocks = limit;\n\t\tbuf->f_bfree = buf->f_bavail =\n\t\t\t(buf->f_blocks > curblock) ?\n\t\t\t (buf->f_blocks - curblock) : 0;\n\t}\n\n\tlimit = dquot->dq_dqb.dqb_isoftlimit ?\n\t\tdquot->dq_dqb.dqb_isoftlimit :\n\t\tdquot->dq_dqb.dqb_ihardlimit;\n\tif (limit && buf->f_files > limit) {\n\t\tbuf->f_files = limit;\n\t\tbuf->f_ffree =\n\t\t\t(buf->f_files > dquot->dq_dqb.dqb_curinodes) ?\n\t\t\t (buf->f_files - dquot->dq_dqb.dqb_curinodes) : 0;\n\t}\n\n\tspin_unlock(&dq_data_lock);\n\tdqput(dquot);\n\treturn 0;\n}\n#endif\n\nstatic int f2fs_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct super_block *sb = dentry->d_sb;\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tu64 id = huge_encode_dev(sb->s_bdev->bd_dev);\n\tblock_t total_count, user_block_count, start_count, ovp_count;\n\tu64 avail_node_count;\n\n\ttotal_count = le64_to_cpu(sbi->raw_super->block_count);\n\tuser_block_count = sbi->user_block_count;\n\tstart_count = le32_to_cpu(sbi->raw_super->segment0_blkaddr);\n\tovp_count = SM_I(sbi)->ovp_segments << sbi->log_blocks_per_seg;\n\tbuf->f_type = F2FS_SUPER_MAGIC;\n\tbuf->f_bsize = sbi->blocksize;\n\n\tbuf->f_blocks = total_count - start_count;\n\tbuf->f_bfree = user_block_count - valid_user_blocks(sbi) + ovp_count;\n\tbuf->f_bavail = user_block_count - valid_user_blocks(sbi) -\n\t\t\t\t\t\tsbi->reserved_blocks;\n\n\tavail_node_count = sbi->total_node_count - F2FS_RESERVED_NODE_NUM;\n\n\tif (avail_node_count > user_block_count) {\n\t\tbuf->f_files = user_block_count;\n\t\tbuf->f_ffree = buf->f_bavail;\n\t} else {\n\t\tbuf->f_files = avail_node_count;\n\t\tbuf->f_ffree = min(avail_node_count - valid_node_count(sbi),\n\t\t\t\t\tbuf->f_bavail);\n\t}\n\n\tbuf->f_namelen = F2FS_NAME_LEN;\n\tbuf->f_fsid.val[0] = (u32)id;\n\tbuf->f_fsid.val[1] = (u32)(id >> 32);\n\n#ifdef CONFIG_QUOTA\n\tif (is_inode_flag_set(dentry->d_inode, FI_PROJ_INHERIT) &&\n\t\t\tsb_has_quota_limits_enabled(sb, PRJQUOTA)) {\n\t\tf2fs_statfs_project(sb, F2FS_I(dentry->d_inode)->i_projid, buf);\n\t}\n#endif\n\treturn 0;\n}\n\nstatic inline void f2fs_show_quota_options(struct seq_file *seq,\n\t\t\t\t\t   struct super_block *sb)\n{\n#ifdef CONFIG_QUOTA\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\n\tif (sbi->s_jquota_fmt) {\n\t\tchar *fmtname = \"\";\n\n\t\tswitch (sbi->s_jquota_fmt) {\n\t\tcase QFMT_VFS_OLD:\n\t\t\tfmtname = \"vfsold\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V0:\n\t\t\tfmtname = \"vfsv0\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V1:\n\t\t\tfmtname = \"vfsv1\";\n\t\t\tbreak;\n\t\t}\n\t\tseq_printf(seq, \",jqfmt=%s\", fmtname);\n\t}\n\n\tif (sbi->s_qf_names[USRQUOTA])\n\t\tseq_show_option(seq, \"usrjquota\", sbi->s_qf_names[USRQUOTA]);\n\n\tif (sbi->s_qf_names[GRPQUOTA])\n\t\tseq_show_option(seq, \"grpjquota\", sbi->s_qf_names[GRPQUOTA]);\n\n\tif (sbi->s_qf_names[PRJQUOTA])\n\t\tseq_show_option(seq, \"prjjquota\", sbi->s_qf_names[PRJQUOTA]);\n#endif\n}\n\nstatic int f2fs_show_options(struct seq_file *seq, struct dentry *root)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(root->d_sb);\n\n\tif (!f2fs_readonly(sbi->sb) && test_opt(sbi, BG_GC)) {\n\t\tif (test_opt(sbi, FORCE_FG_GC))\n\t\t\tseq_printf(seq, \",background_gc=%s\", \"sync\");\n\t\telse\n\t\t\tseq_printf(seq, \",background_gc=%s\", \"on\");\n\t} else {\n\t\tseq_printf(seq, \",background_gc=%s\", \"off\");\n\t}\n\tif (test_opt(sbi, DISABLE_ROLL_FORWARD))\n\t\tseq_puts(seq, \",disable_roll_forward\");\n\tif (test_opt(sbi, DISCARD))\n\t\tseq_puts(seq, \",discard\");\n\tif (test_opt(sbi, NOHEAP))\n\t\tseq_puts(seq, \",no_heap\");\n\telse\n\t\tseq_puts(seq, \",heap\");\n#ifdef CONFIG_F2FS_FS_XATTR\n\tif (test_opt(sbi, XATTR_USER))\n\t\tseq_puts(seq, \",user_xattr\");\n\telse\n\t\tseq_puts(seq, \",nouser_xattr\");\n\tif (test_opt(sbi, INLINE_XATTR))\n\t\tseq_puts(seq, \",inline_xattr\");\n\telse\n\t\tseq_puts(seq, \",noinline_xattr\");\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\tif (test_opt(sbi, POSIX_ACL))\n\t\tseq_puts(seq, \",acl\");\n\telse\n\t\tseq_puts(seq, \",noacl\");\n#endif\n\tif (test_opt(sbi, DISABLE_EXT_IDENTIFY))\n\t\tseq_puts(seq, \",disable_ext_identify\");\n\tif (test_opt(sbi, INLINE_DATA))\n\t\tseq_puts(seq, \",inline_data\");\n\telse\n\t\tseq_puts(seq, \",noinline_data\");\n\tif (test_opt(sbi, INLINE_DENTRY))\n\t\tseq_puts(seq, \",inline_dentry\");\n\telse\n\t\tseq_puts(seq, \",noinline_dentry\");\n\tif (!f2fs_readonly(sbi->sb) && test_opt(sbi, FLUSH_MERGE))\n\t\tseq_puts(seq, \",flush_merge\");\n\tif (test_opt(sbi, NOBARRIER))\n\t\tseq_puts(seq, \",nobarrier\");\n\tif (test_opt(sbi, FASTBOOT))\n\t\tseq_puts(seq, \",fastboot\");\n\tif (test_opt(sbi, EXTENT_CACHE))\n\t\tseq_puts(seq, \",extent_cache\");\n\telse\n\t\tseq_puts(seq, \",noextent_cache\");\n\tif (test_opt(sbi, DATA_FLUSH))\n\t\tseq_puts(seq, \",data_flush\");\n\n\tseq_puts(seq, \",mode=\");\n\tif (test_opt(sbi, ADAPTIVE))\n\t\tseq_puts(seq, \"adaptive\");\n\telse if (test_opt(sbi, LFS))\n\t\tseq_puts(seq, \"lfs\");\n\tseq_printf(seq, \",active_logs=%u\", sbi->active_logs);\n\tif (F2FS_IO_SIZE_BITS(sbi))\n\t\tseq_printf(seq, \",io_size=%uKB\", F2FS_IO_SIZE_KB(sbi));\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (test_opt(sbi, FAULT_INJECTION))\n\t\tseq_printf(seq, \",fault_injection=%u\",\n\t\t\t\tsbi->fault_info.inject_rate);\n#endif\n#ifdef CONFIG_QUOTA\n\tif (test_opt(sbi, QUOTA))\n\t\tseq_puts(seq, \",quota\");\n\tif (test_opt(sbi, USRQUOTA))\n\t\tseq_puts(seq, \",usrquota\");\n\tif (test_opt(sbi, GRPQUOTA))\n\t\tseq_puts(seq, \",grpquota\");\n\tif (test_opt(sbi, PRJQUOTA))\n\t\tseq_puts(seq, \",prjquota\");\n#endif\n\tf2fs_show_quota_options(seq, sbi->sb);\n\n\treturn 0;\n}\n\nstatic void default_options(struct f2fs_sb_info *sbi)\n{\n\t/* init some FS parameters */\n\tsbi->active_logs = NR_CURSEG_TYPE;\n\n\tset_opt(sbi, BG_GC);\n\tset_opt(sbi, INLINE_XATTR);\n\tset_opt(sbi, INLINE_DATA);\n\tset_opt(sbi, INLINE_DENTRY);\n\tset_opt(sbi, EXTENT_CACHE);\n\tset_opt(sbi, NOHEAP);\n\tsbi->sb->s_flags |= MS_LAZYTIME;\n\tset_opt(sbi, FLUSH_MERGE);\n\tif (f2fs_sb_mounted_blkzoned(sbi->sb)) {\n\t\tset_opt_mode(sbi, F2FS_MOUNT_LFS);\n\t\tset_opt(sbi, DISCARD);\n\t} else {\n\t\tset_opt_mode(sbi, F2FS_MOUNT_ADAPTIVE);\n\t}\n\n#ifdef CONFIG_F2FS_FS_XATTR\n\tset_opt(sbi, XATTR_USER);\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\tset_opt(sbi, POSIX_ACL);\n#endif\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tf2fs_build_fault_attr(sbi, 0);\n#endif\n}\n\nstatic int f2fs_remount(struct super_block *sb, int *flags, char *data)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct f2fs_mount_info org_mount_opt;\n\tunsigned long old_sb_flags;\n\tint err, active_logs;\n\tbool need_restart_gc = false;\n\tbool need_stop_gc = false;\n\tbool no_extent_cache = !test_opt(sbi, EXTENT_CACHE);\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct f2fs_fault_info ffi = sbi->fault_info;\n#endif\n#ifdef CONFIG_QUOTA\n\tint s_jquota_fmt;\n\tchar *s_qf_names[MAXQUOTAS];\n\tint i, j;\n#endif\n\n\t/*\n\t * Save the old mount options in case we\n\t * need to restore them.\n\t */\n\torg_mount_opt = sbi->mount_opt;\n\told_sb_flags = sb->s_flags;\n\tactive_logs = sbi->active_logs;\n\n#ifdef CONFIG_QUOTA\n\ts_jquota_fmt = sbi->s_jquota_fmt;\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (sbi->s_qf_names[i]) {\n\t\t\ts_qf_names[i] = kstrdup(sbi->s_qf_names[i],\n\t\t\t\t\t\t\t GFP_KERNEL);\n\t\t\tif (!s_qf_names[i]) {\n\t\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\t\tkfree(s_qf_names[j]);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\ts_qf_names[i] = NULL;\n\t\t}\n\t}\n#endif\n\n\t/* recover superblocks we couldn't write due to previous RO mount */\n\tif (!(*flags & MS_RDONLY) && is_sbi_flag_set(sbi, SBI_NEED_SB_WRITE)) {\n\t\terr = f2fs_commit_super(sbi, false);\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Try to recover all the superblocks, ret: %d\", err);\n\t\tif (!err)\n\t\t\tclear_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t}\n\n\tdefault_options(sbi);\n\n\t/* parse mount options */\n\terr = parse_options(sb, data);\n\tif (err)\n\t\tgoto restore_opts;\n\n\t/*\n\t * Previous and new state of filesystem is RO,\n\t * so skip checking GC and FLUSH_MERGE conditions.\n\t */\n\tif (f2fs_readonly(sb) && (*flags & MS_RDONLY))\n\t\tgoto skip;\n\n\tif (!f2fs_readonly(sb) && (*flags & MS_RDONLY)) {\n\t\terr = dquot_suspend(sb, -1);\n\t\tif (err < 0)\n\t\t\tgoto restore_opts;\n\t} else {\n\t\t/* dquot_resume needs RW */\n\t\tsb->s_flags &= ~MS_RDONLY;\n\t\tdquot_resume(sb, -1);\n\t}\n\n\t/* disallow enable/disable extent_cache dynamically */\n\tif (no_extent_cache == !!test_opt(sbi, EXTENT_CACHE)) {\n\t\terr = -EINVAL;\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\t\"switch extent_cache option is not allowed\");\n\t\tgoto restore_opts;\n\t}\n\n\t/*\n\t * We stop the GC thread if FS is mounted as RO\n\t * or if background_gc = off is passed in mount\n\t * option. Also sync the filesystem.\n\t */\n\tif ((*flags & MS_RDONLY) || !test_opt(sbi, BG_GC)) {\n\t\tif (sbi->gc_thread) {\n\t\t\tstop_gc_thread(sbi);\n\t\t\tneed_restart_gc = true;\n\t\t}\n\t} else if (!sbi->gc_thread) {\n\t\terr = start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto restore_opts;\n\t\tneed_stop_gc = true;\n\t}\n\n\tif (*flags & MS_RDONLY) {\n\t\twriteback_inodes_sb(sb, WB_REASON_SYNC);\n\t\tsync_inodes_sb(sb);\n\n\t\tset_sbi_flag(sbi, SBI_IS_DIRTY);\n\t\tset_sbi_flag(sbi, SBI_IS_CLOSE);\n\t\tf2fs_sync_fs(sb, 1);\n\t\tclear_sbi_flag(sbi, SBI_IS_CLOSE);\n\t}\n\n\t/*\n\t * We stop issue flush thread if FS is mounted as RO\n\t * or if flush_merge is not passed in mount option.\n\t */\n\tif ((*flags & MS_RDONLY) || !test_opt(sbi, FLUSH_MERGE)) {\n\t\tclear_opt(sbi, FLUSH_MERGE);\n\t\tdestroy_flush_cmd_control(sbi, false);\n\t} else {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\tgoto restore_gc;\n\t}\nskip:\n#ifdef CONFIG_QUOTA\n\t/* Release old quota file names */\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(s_qf_names[i]);\n#endif\n\t/* Update the POSIXACL Flag */\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? MS_POSIXACL : 0);\n\n\treturn 0;\nrestore_gc:\n\tif (need_restart_gc) {\n\t\tif (start_gc_thread(sbi))\n\t\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\t\"background gc thread has stopped\");\n\t} else if (need_stop_gc) {\n\t\tstop_gc_thread(sbi);\n\t}\nrestore_opts:\n#ifdef CONFIG_QUOTA\n\tsbi->s_jquota_fmt = s_jquota_fmt;\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tkfree(sbi->s_qf_names[i]);\n\t\tsbi->s_qf_names[i] = s_qf_names[i];\n\t}\n#endif\n\tsbi->mount_opt = org_mount_opt;\n\tsbi->active_logs = active_logs;\n\tsb->s_flags = old_sb_flags;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tsbi->fault_info = ffi;\n#endif\n\treturn err;\n}\n\n#ifdef CONFIG_QUOTA\n/* Read data from quotafile */\nstatic ssize_t f2fs_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\tstruct address_space *mapping = inode->i_mapping;\n\tblock_t blkidx = F2FS_BYTES_TO_BLK(off);\n\tint offset = off & (sb->s_blocksize - 1);\n\tint tocopy;\n\tsize_t toread;\n\tloff_t i_size = i_size_read(inode);\n\tstruct page *page;\n\tchar *kaddr;\n\n\tif (off > i_size)\n\t\treturn 0;\n\n\tif (off + len > i_size)\n\t\tlen = i_size - off;\n\ttoread = len;\n\twhile (toread > 0) {\n\t\ttocopy = min_t(unsigned long, sb->s_blocksize - offset, toread);\nrepeat:\n\t\tpage = read_mapping_page(mapping, blkidx, NULL);\n\t\tif (IS_ERR(page))\n\t\t\treturn PTR_ERR(page);\n\n\t\tlock_page(page);\n\n\t\tif (unlikely(page->mapping != mapping)) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(!PageUptodate(page))) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(data, kaddr + offset, tocopy);\n\t\tkunmap_atomic(kaddr);\n\t\tf2fs_put_page(page, 1);\n\n\t\toffset = 0;\n\t\ttoread -= tocopy;\n\t\tdata += tocopy;\n\t\tblkidx++;\n\t}\n\treturn len;\n}\n\n/* Write to quotafile */\nstatic ssize_t f2fs_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\tstruct address_space *mapping = inode->i_mapping;\n\tconst struct address_space_operations *a_ops = mapping->a_ops;\n\tint offset = off & (sb->s_blocksize - 1);\n\tsize_t towrite = len;\n\tstruct page *page;\n\tchar *kaddr;\n\tint err = 0;\n\tint tocopy;\n\n\twhile (towrite > 0) {\n\t\ttocopy = min_t(unsigned long, sb->s_blocksize - offset,\n\t\t\t\t\t\t\t\ttowrite);\n\n\t\terr = a_ops->write_begin(NULL, mapping, off, tocopy, 0,\n\t\t\t\t\t\t\t&page, NULL);\n\t\tif (unlikely(err))\n\t\t\tbreak;\n\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(kaddr + offset, data, tocopy);\n\t\tkunmap_atomic(kaddr);\n\t\tflush_dcache_page(page);\n\n\t\ta_ops->write_end(NULL, mapping, off, tocopy, tocopy,\n\t\t\t\t\t\tpage, NULL);\n\t\toffset = 0;\n\t\ttowrite -= tocopy;\n\t\toff += tocopy;\n\t\tdata += tocopy;\n\t\tcond_resched();\n\t}\n\n\tif (len == towrite)\n\t\treturn 0;\n\tinode->i_version++;\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\tf2fs_mark_inode_dirty_sync(inode, false);\n\treturn len - towrite;\n}\n\nstatic struct dquot **f2fs_get_dquots(struct inode *inode)\n{\n\treturn F2FS_I(inode)->i_dquot;\n}\n\nstatic qsize_t *f2fs_get_reserved_space(struct inode *inode)\n{\n\treturn &F2FS_I(inode)->i_reserved_quota;\n}\n\nstatic int f2fs_quota_on_mount(struct f2fs_sb_info *sbi, int type)\n{\n\treturn dquot_quota_on_mount(sbi->sb, sbi->s_qf_names[type],\n\t\t\t\t\t\tsbi->s_jquota_fmt, type);\n}\n\nvoid f2fs_enable_quota_files(struct f2fs_sb_info *sbi)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (sbi->s_qf_names[i]) {\n\t\t\tret = f2fs_quota_on_mount(sbi, i);\n\t\t\tif (ret < 0)\n\t\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\t\"Cannot turn on journaled \"\n\t\t\t\t\t\"quota: error %d\", ret);\n\t\t}\n\t}\n}\n\nstatic int f2fs_quota_sync(struct super_block *sb, int type)\n{\n\tstruct quota_info *dqopt = sb_dqopt(sb);\n\tint cnt;\n\tint ret;\n\n\tret = dquot_writeback_dquots(sb, type);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Now when everything is written we can discard the pagecache so\n\t * that userspace sees the changes.\n\t */\n\tfor (cnt = 0; cnt < MAXQUOTAS; cnt++) {\n\t\tif (type != -1 && cnt != type)\n\t\t\tcontinue;\n\t\tif (!sb_has_quota_active(sb, cnt))\n\t\t\tcontinue;\n\n\t\tret = filemap_write_and_wait(dqopt->files[cnt]->i_mapping);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tinode_lock(dqopt->files[cnt]);\n\t\ttruncate_inode_pages(&dqopt->files[cnt]->i_data, 0);\n\t\tinode_unlock(dqopt->files[cnt]);\n\t}\n\treturn 0;\n}\n\nstatic int f2fs_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t\t\t\t\tconst struct path *path)\n{\n\tstruct inode *inode;\n\tint err;\n\n\terr = f2fs_quota_sync(sb, type);\n\tif (err)\n\t\treturn err;\n\n\terr = dquot_quota_on(sb, type, format_id, path);\n\tif (err)\n\t\treturn err;\n\n\tinode = d_inode(path->dentry);\n\n\tinode_lock(inode);\n\tF2FS_I(inode)->i_flags |= FS_NOATIME_FL | FS_IMMUTABLE_FL;\n\tinode_set_flags(inode, S_NOATIME | S_IMMUTABLE,\n\t\t\t\t\tS_NOATIME | S_IMMUTABLE);\n\tinode_unlock(inode);\n\tf2fs_mark_inode_dirty_sync(inode, false);\n\n\treturn 0;\n}\n\nstatic int f2fs_quota_off(struct super_block *sb, int type)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\tint err;\n\n\tif (!inode || !igrab(inode))\n\t\treturn dquot_quota_off(sb, type);\n\n\tf2fs_quota_sync(sb, type);\n\n\terr = dquot_quota_off(sb, type);\n\tif (err)\n\t\tgoto out_put;\n\n\tinode_lock(inode);\n\tF2FS_I(inode)->i_flags &= ~(FS_NOATIME_FL | FS_IMMUTABLE_FL);\n\tinode_set_flags(inode, 0, S_NOATIME | S_IMMUTABLE);\n\tinode_unlock(inode);\n\tf2fs_mark_inode_dirty_sync(inode, false);\nout_put:\n\tiput(inode);\n\treturn err;\n}\n\nvoid f2fs_quota_off_umount(struct super_block *sb)\n{\n\tint type;\n\n\tfor (type = 0; type < MAXQUOTAS; type++)\n\t\tf2fs_quota_off(sb, type);\n}\n\nint f2fs_get_projid(struct inode *inode, kprojid_t *projid)\n{\n\t*projid = F2FS_I(inode)->i_projid;\n\treturn 0;\n}\n\nstatic const struct dquot_operations f2fs_quota_operations = {\n\t.get_reserved_space = f2fs_get_reserved_space,\n\t.write_dquot\t= dquot_commit,\n\t.acquire_dquot\t= dquot_acquire,\n\t.release_dquot\t= dquot_release,\n\t.mark_dirty\t= dquot_mark_dquot_dirty,\n\t.write_info\t= dquot_commit_info,\n\t.alloc_dquot\t= dquot_alloc,\n\t.destroy_dquot\t= dquot_destroy,\n\t.get_projid\t= f2fs_get_projid,\n\t.get_next_id\t= dquot_get_next_id,\n};\n\nstatic const struct quotactl_ops f2fs_quotactl_ops = {\n\t.quota_on\t= f2fs_quota_on,\n\t.quota_off\t= f2fs_quota_off,\n\t.quota_sync\t= f2fs_quota_sync,\n\t.get_state\t= dquot_get_state,\n\t.set_info\t= dquot_set_dqinfo,\n\t.get_dqblk\t= dquot_get_dqblk,\n\t.set_dqblk\t= dquot_set_dqblk,\n\t.get_nextdqblk\t= dquot_get_next_dqblk,\n};\n#else\nvoid f2fs_quota_off_umount(struct super_block *sb)\n{\n}\n#endif\n\nstatic const struct super_operations f2fs_sops = {\n\t.alloc_inode\t= f2fs_alloc_inode,\n\t.drop_inode\t= f2fs_drop_inode,\n\t.destroy_inode\t= f2fs_destroy_inode,\n\t.write_inode\t= f2fs_write_inode,\n\t.dirty_inode\t= f2fs_dirty_inode,\n\t.show_options\t= f2fs_show_options,\n#ifdef CONFIG_QUOTA\n\t.quota_read\t= f2fs_quota_read,\n\t.quota_write\t= f2fs_quota_write,\n\t.get_dquots\t= f2fs_get_dquots,\n#endif\n\t.evict_inode\t= f2fs_evict_inode,\n\t.put_super\t= f2fs_put_super,\n\t.sync_fs\t= f2fs_sync_fs,\n\t.freeze_fs\t= f2fs_freeze,\n\t.unfreeze_fs\t= f2fs_unfreeze,\n\t.statfs\t\t= f2fs_statfs,\n\t.remount_fs\t= f2fs_remount,\n};\n\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\nstatic int f2fs_get_context(struct inode *inode, void *ctx, size_t len)\n{\n\treturn f2fs_getxattr(inode, F2FS_XATTR_INDEX_ENCRYPTION,\n\t\t\t\tF2FS_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\tctx, len, NULL);\n}\n\nstatic int f2fs_set_context(struct inode *inode, const void *ctx, size_t len,\n\t\t\t\t\t\t\tvoid *fs_data)\n{\n\treturn f2fs_setxattr(inode, F2FS_XATTR_INDEX_ENCRYPTION,\n\t\t\t\tF2FS_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\tctx, len, fs_data, XATTR_CREATE);\n}\n\nstatic unsigned f2fs_max_namelen(struct inode *inode)\n{\n\treturn S_ISLNK(inode->i_mode) ?\n\t\t\tinode->i_sb->s_blocksize : F2FS_NAME_LEN;\n}\n\nstatic const struct fscrypt_operations f2fs_cryptops = {\n\t.key_prefix\t= \"f2fs:\",\n\t.get_context\t= f2fs_get_context,\n\t.set_context\t= f2fs_set_context,\n\t.is_encrypted\t= f2fs_encrypted_inode,\n\t.empty_dir\t= f2fs_empty_dir,\n\t.max_namelen\t= f2fs_max_namelen,\n};\n#else\nstatic const struct fscrypt_operations f2fs_cryptops = {\n\t.is_encrypted\t= f2fs_encrypted_inode,\n};\n#endif\n\nstatic struct inode *f2fs_nfs_get_inode(struct super_block *sb,\n\t\tu64 ino, u32 generation)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct inode *inode;\n\n\tif (check_nid_range(sbi, ino))\n\t\treturn ERR_PTR(-ESTALE);\n\n\t/*\n\t * f2fs_iget isn't quite right if the inode is currently unallocated!\n\t * However f2fs_iget currently does appropriate checks to handle stale\n\t * inodes so everything is OK.\n\t */\n\tinode = f2fs_iget(sb, ino);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (unlikely(generation && inode->i_generation != generation)) {\n\t\t/* we didn't find the right inode.. */\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\treturn inode;\n}\n\nstatic struct dentry *f2fs_fh_to_dentry(struct super_block *sb, struct fid *fid,\n\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_dentry(sb, fid, fh_len, fh_type,\n\t\t\t\t    f2fs_nfs_get_inode);\n}\n\nstatic struct dentry *f2fs_fh_to_parent(struct super_block *sb, struct fid *fid,\n\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_parent(sb, fid, fh_len, fh_type,\n\t\t\t\t    f2fs_nfs_get_inode);\n}\n\nstatic const struct export_operations f2fs_export_ops = {\n\t.fh_to_dentry = f2fs_fh_to_dentry,\n\t.fh_to_parent = f2fs_fh_to_parent,\n\t.get_parent = f2fs_get_parent,\n};\n\nstatic loff_t max_file_blocks(void)\n{\n\tloff_t result = 0;\n\tloff_t leaf_count = ADDRS_PER_BLOCK;\n\n\t/*\n\t * note: previously, result is equal to (DEF_ADDRS_PER_INODE -\n\t * F2FS_INLINE_XATTR_ADDRS), but now f2fs try to reserve more\n\t * space in inode.i_addr, it will be more safe to reassign\n\t * result as zero.\n\t */\n\n\t/* two direct node blocks */\n\tresult += (leaf_count * 2);\n\n\t/* two indirect node blocks */\n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += (leaf_count * 2);\n\n\t/* one double indirect node block */\n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += leaf_count;\n\n\treturn result;\n}\n\nstatic int __f2fs_commit_super(struct buffer_head *bh,\n\t\t\tstruct f2fs_super_block *super)\n{\n\tlock_buffer(bh);\n\tif (super)\n\t\tmemcpy(bh->b_data + F2FS_SUPER_OFFSET, super, sizeof(*super));\n\tset_buffer_uptodate(bh);\n\tset_buffer_dirty(bh);\n\tunlock_buffer(bh);\n\n\t/* it's rare case, we can do fua all the time */\n\treturn __sync_dirty_buffer(bh, REQ_SYNC | REQ_PREFLUSH | REQ_FUA);\n}\n\nstatic inline bool sanity_check_area_boundary(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct buffer_head *bh)\n{\n\tstruct f2fs_super_block *raw_super = (struct f2fs_super_block *)\n\t\t\t\t\t(bh->b_data + F2FS_SUPER_OFFSET);\n\tstruct super_block *sb = sbi->sb;\n\tu32 segment0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tu32 cp_blkaddr = le32_to_cpu(raw_super->cp_blkaddr);\n\tu32 sit_blkaddr = le32_to_cpu(raw_super->sit_blkaddr);\n\tu32 nat_blkaddr = le32_to_cpu(raw_super->nat_blkaddr);\n\tu32 ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tu32 main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tu32 segment_count_ckpt = le32_to_cpu(raw_super->segment_count_ckpt);\n\tu32 segment_count_sit = le32_to_cpu(raw_super->segment_count_sit);\n\tu32 segment_count_nat = le32_to_cpu(raw_super->segment_count_nat);\n\tu32 segment_count_ssa = le32_to_cpu(raw_super->segment_count_ssa);\n\tu32 segment_count_main = le32_to_cpu(raw_super->segment_count_main);\n\tu32 segment_count = le32_to_cpu(raw_super->segment_count);\n\tu32 log_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tu64 main_end_blkaddr = main_blkaddr +\n\t\t\t\t(segment_count_main << log_blocks_per_seg);\n\tu64 seg_end_blkaddr = segment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg);\n\n\tif (segment0_blkaddr != cp_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Mismatch start address, segment0(%u) cp_blkaddr(%u)\",\n\t\t\tsegment0_blkaddr, cp_blkaddr);\n\t\treturn true;\n\t}\n\n\tif (cp_blkaddr + (segment_count_ckpt << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tsit_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong CP boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tcp_blkaddr, sit_blkaddr,\n\t\t\tsegment_count_ckpt << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (sit_blkaddr + (segment_count_sit << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tnat_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong SIT boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tsit_blkaddr, nat_blkaddr,\n\t\t\tsegment_count_sit << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (nat_blkaddr + (segment_count_nat << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tssa_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong NAT boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tnat_blkaddr, ssa_blkaddr,\n\t\t\tsegment_count_nat << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (ssa_blkaddr + (segment_count_ssa << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tmain_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong SSA boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tssa_blkaddr, main_blkaddr,\n\t\t\tsegment_count_ssa << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (main_end_blkaddr > seg_end_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong MAIN_AREA boundary, start(%u) end(%u) block(%u)\",\n\t\t\tmain_blkaddr,\n\t\t\tsegment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg),\n\t\t\tsegment_count_main << log_blocks_per_seg);\n\t\treturn true;\n\t} else if (main_end_blkaddr < seg_end_blkaddr) {\n\t\tint err = 0;\n\t\tchar *res;\n\n\t\t/* fix in-memory information all the time */\n\t\traw_super->segment_count = cpu_to_le32((main_end_blkaddr -\n\t\t\t\tsegment0_blkaddr) >> log_blocks_per_seg);\n\n\t\tif (f2fs_readonly(sb) || bdev_read_only(sb->s_bdev)) {\n\t\t\tset_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t\t\tres = \"internally\";\n\t\t} else {\n\t\t\terr = __f2fs_commit_super(bh, NULL);\n\t\t\tres = err ? \"failed\" : \"done\";\n\t\t}\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Fix alignment : %s, start(%u) end(%u) block(%u)\",\n\t\t\tres, main_blkaddr,\n\t\t\tsegment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg),\n\t\t\tsegment_count_main << log_blocks_per_seg);\n\t\tif (err)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int sanity_check_raw_super(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct buffer_head *bh)\n{\n\tstruct f2fs_super_block *raw_super = (struct f2fs_super_block *)\n\t\t\t\t\t(bh->b_data + F2FS_SUPER_OFFSET);\n\tstruct super_block *sb = sbi->sb;\n\tunsigned int blocksize;\n\n\tif (F2FS_SUPER_MAGIC != le32_to_cpu(raw_super->magic)) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Magic Mismatch, valid(0x%x) - read(0x%x)\",\n\t\t\tF2FS_SUPER_MAGIC, le32_to_cpu(raw_super->magic));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB page cache size */\n\tif (F2FS_BLKSIZE != PAGE_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid page_cache_size (%lu), supports only 4KB\\n\",\n\t\t\tPAGE_SIZE);\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB block size */\n\tblocksize = 1 << le32_to_cpu(raw_super->log_blocksize);\n\tif (blocksize != F2FS_BLKSIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid blocksize (%u), supports only 4KB\\n\",\n\t\t\tblocksize);\n\t\treturn 1;\n\t}\n\n\t/* check log blocks per segment */\n\tif (le32_to_cpu(raw_super->log_blocks_per_seg) != 9) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log blocks per segment (%u)\\n\",\n\t\t\tle32_to_cpu(raw_super->log_blocks_per_seg));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support 512/1024/2048/4096 bytes sector size */\n\tif (le32_to_cpu(raw_super->log_sectorsize) >\n\t\t\t\tF2FS_MAX_LOG_SECTOR_SIZE ||\n\t\tle32_to_cpu(raw_super->log_sectorsize) <\n\t\t\t\tF2FS_MIN_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO, \"Invalid log sectorsize (%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\tif (le32_to_cpu(raw_super->log_sectors_per_block) +\n\t\tle32_to_cpu(raw_super->log_sectorsize) !=\n\t\t\tF2FS_MAX_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log sectors per block(%u) log sectorsize(%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectors_per_block),\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\n\t/* check reserved ino info */\n\tif (le32_to_cpu(raw_super->node_ino) != 1 ||\n\t\tle32_to_cpu(raw_super->meta_ino) != 2 ||\n\t\tle32_to_cpu(raw_super->root_ino) != 3) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid Fs Meta Ino: node(%u) meta(%u) root(%u)\",\n\t\t\tle32_to_cpu(raw_super->node_ino),\n\t\t\tle32_to_cpu(raw_super->meta_ino),\n\t\t\tle32_to_cpu(raw_super->root_ino));\n\t\treturn 1;\n\t}\n\n\tif (le32_to_cpu(raw_super->segment_count) > F2FS_MAX_SEGMENT) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid segment count (%u)\",\n\t\t\tle32_to_cpu(raw_super->segment_count));\n\t\treturn 1;\n\t}\n\n\t/* check CP/SIT/NAT/SSA/MAIN_AREA area boundary */\n\tif (sanity_check_area_boundary(sbi, bh))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nint sanity_check_ckpt(struct f2fs_sb_info *sbi)\n{\n\tunsigned int total, fsmeta;\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tunsigned int ovp_segments, reserved_segments;\n\tunsigned int main_segs, blocks_per_seg;\n\tint i;\n\n\ttotal = le32_to_cpu(raw_super->segment_count);\n\tfsmeta = le32_to_cpu(raw_super->segment_count_ckpt);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_sit);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_nat);\n\tfsmeta += le32_to_cpu(ckpt->rsvd_segment_count);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_ssa);\n\n\tif (unlikely(fsmeta >= total))\n\t\treturn 1;\n\n\tovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\treserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\n\tif (unlikely(fsmeta < F2FS_MIN_SEGMENTS ||\n\t\t\tovp_segments == 0 || reserved_segments == 0)) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong layout: check mkfs.f2fs version\");\n\t\treturn 1;\n\t}\n\n\tmain_segs = le32_to_cpu(raw_super->segment_count_main);\n\tblocks_per_seg = sbi->blocks_per_seg;\n\n\tfor (i = 0; i < NR_CURSEG_NODE_TYPE; i++) {\n\t\tif (le32_to_cpu(ckpt->cur_node_segno[i]) >= main_segs ||\n\t\t\tle16_to_cpu(ckpt->cur_node_blkoff[i]) >= blocks_per_seg)\n\t\t\treturn 1;\n\t}\n\tfor (i = 0; i < NR_CURSEG_DATA_TYPE; i++) {\n\t\tif (le32_to_cpu(ckpt->cur_data_segno[i]) >= main_segs ||\n\t\t\tle16_to_cpu(ckpt->cur_data_blkoff[i]) >= blocks_per_seg)\n\t\t\treturn 1;\n\t}\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR, \"A bug case: need to run fsck\");\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void init_sb_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = sbi->raw_super;\n\tint i, j;\n\n\tsbi->log_sectors_per_block =\n\t\tle32_to_cpu(raw_super->log_sectors_per_block);\n\tsbi->log_blocksize = le32_to_cpu(raw_super->log_blocksize);\n\tsbi->blocksize = 1 << sbi->log_blocksize;\n\tsbi->log_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tsbi->blocks_per_seg = 1 << sbi->log_blocks_per_seg;\n\tsbi->segs_per_sec = le32_to_cpu(raw_super->segs_per_sec);\n\tsbi->secs_per_zone = le32_to_cpu(raw_super->secs_per_zone);\n\tsbi->total_sections = le32_to_cpu(raw_super->section_count);\n\tsbi->total_node_count =\n\t\t(le32_to_cpu(raw_super->segment_count_nat) / 2)\n\t\t\t* sbi->blocks_per_seg * NAT_ENTRY_PER_BLOCK;\n\tsbi->root_ino_num = le32_to_cpu(raw_super->root_ino);\n\tsbi->node_ino_num = le32_to_cpu(raw_super->node_ino);\n\tsbi->meta_ino_num = le32_to_cpu(raw_super->meta_ino);\n\tsbi->cur_victim_sec = NULL_SECNO;\n\tsbi->max_victim_search = DEF_MAX_VICTIM_SEARCH;\n\n\tsbi->dir_level = DEF_DIR_LEVEL;\n\tsbi->interval_time[CP_TIME] = DEF_CP_INTERVAL;\n\tsbi->interval_time[REQ_TIME] = DEF_IDLE_INTERVAL;\n\tclear_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\tfor (i = 0; i < NR_COUNT_TYPE; i++)\n\t\tatomic_set(&sbi->nr_pages[i], 0);\n\n\tatomic_set(&sbi->wb_sync_req, 0);\n\n\tINIT_LIST_HEAD(&sbi->s_list);\n\tmutex_init(&sbi->umount_mutex);\n\tfor (i = 0; i < NR_PAGE_TYPE - 1; i++)\n\t\tfor (j = HOT; j < NR_TEMP_TYPE; j++)\n\t\t\tmutex_init(&sbi->wio_mutex[i][j]);\n\tspin_lock_init(&sbi->cp_lock);\n}\n\nstatic int init_percpu_info(struct f2fs_sb_info *sbi)\n{\n\tint err;\n\n\terr = percpu_counter_init(&sbi->alloc_valid_block_count, 0, GFP_KERNEL);\n\tif (err)\n\t\treturn err;\n\n\treturn percpu_counter_init(&sbi->total_valid_inode_count, 0,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int init_blkz_info(struct f2fs_sb_info *sbi, int devi)\n{\n\tstruct block_device *bdev = FDEV(devi).bdev;\n\tsector_t nr_sectors = bdev->bd_part->nr_sects;\n\tsector_t sector = 0;\n\tstruct blk_zone *zones;\n\tunsigned int i, nr_zones;\n\tunsigned int n = 0;\n\tint err = -EIO;\n\n\tif (!f2fs_sb_mounted_blkzoned(sbi->sb))\n\t\treturn 0;\n\n\tif (sbi->blocks_per_blkz && sbi->blocks_per_blkz !=\n\t\t\t\tSECTOR_TO_BLOCK(bdev_zone_sectors(bdev)))\n\t\treturn -EINVAL;\n\tsbi->blocks_per_blkz = SECTOR_TO_BLOCK(bdev_zone_sectors(bdev));\n\tif (sbi->log_blocks_per_blkz && sbi->log_blocks_per_blkz !=\n\t\t\t\t__ilog2_u32(sbi->blocks_per_blkz))\n\t\treturn -EINVAL;\n\tsbi->log_blocks_per_blkz = __ilog2_u32(sbi->blocks_per_blkz);\n\tFDEV(devi).nr_blkz = SECTOR_TO_BLOCK(nr_sectors) >>\n\t\t\t\t\tsbi->log_blocks_per_blkz;\n\tif (nr_sectors & (bdev_zone_sectors(bdev) - 1))\n\t\tFDEV(devi).nr_blkz++;\n\n\tFDEV(devi).blkz_type = kmalloc(FDEV(devi).nr_blkz, GFP_KERNEL);\n\tif (!FDEV(devi).blkz_type)\n\t\treturn -ENOMEM;\n\n#define F2FS_REPORT_NR_ZONES   4096\n\n\tzones = kcalloc(F2FS_REPORT_NR_ZONES, sizeof(struct blk_zone),\n\t\t\tGFP_KERNEL);\n\tif (!zones)\n\t\treturn -ENOMEM;\n\n\t/* Get block zones type */\n\twhile (zones && sector < nr_sectors) {\n\n\t\tnr_zones = F2FS_REPORT_NR_ZONES;\n\t\terr = blkdev_report_zones(bdev, sector,\n\t\t\t\t\t  zones, &nr_zones,\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!nr_zones) {\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < nr_zones; i++) {\n\t\t\tFDEV(devi).blkz_type[n] = zones[i].type;\n\t\t\tsector += zones[i].len;\n\t\t\tn++;\n\t\t}\n\t}\n\n\tkfree(zones);\n\n\treturn err;\n}\n#endif\n\n/*\n * Read f2fs raw super block.\n * Because we have two copies of super block, so read both of them\n * to get the first valid one. If any one of them is broken, we pass\n * them recovery flag back to the caller.\n */\nstatic int read_raw_super_block(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_super_block **raw_super,\n\t\t\tint *valid_super_block, int *recovery)\n{\n\tstruct super_block *sb = sbi->sb;\n\tint block;\n\tstruct buffer_head *bh;\n\tstruct f2fs_super_block *super;\n\tint err = 0;\n\n\tsuper = kzalloc(sizeof(struct f2fs_super_block), GFP_KERNEL);\n\tif (!super)\n\t\treturn -ENOMEM;\n\n\tfor (block = 0; block < 2; block++) {\n\t\tbh = sb_bread(sb, block);\n\t\tif (!bh) {\n\t\t\tf2fs_msg(sb, KERN_ERR, \"Unable to read %dth superblock\",\n\t\t\t\tblock + 1);\n\t\t\terr = -EIO;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* sanity checking of raw super */\n\t\tif (sanity_check_raw_super(sbi, bh)) {\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Can't find valid F2FS filesystem in %dth superblock\",\n\t\t\t\tblock + 1);\n\t\t\terr = -EINVAL;\n\t\t\tbrelse(bh);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!*raw_super) {\n\t\t\tmemcpy(super, bh->b_data + F2FS_SUPER_OFFSET,\n\t\t\t\t\t\t\tsizeof(*super));\n\t\t\t*valid_super_block = block;\n\t\t\t*raw_super = super;\n\t\t}\n\t\tbrelse(bh);\n\t}\n\n\t/* Fail to read any one of the superblocks*/\n\tif (err < 0)\n\t\t*recovery = 1;\n\n\t/* No valid superblock */\n\tif (!*raw_super)\n\t\tkfree(super);\n\telse\n\t\terr = 0;\n\n\treturn err;\n}\n\nint f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover)\n{\n\tstruct buffer_head *bh;\n\tint err;\n\n\tif ((recover && f2fs_readonly(sbi->sb)) ||\n\t\t\t\tbdev_read_only(sbi->sb->s_bdev)) {\n\t\tset_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t\treturn -EROFS;\n\t}\n\n\t/* write back-up superblock first */\n\tbh = sb_getblk(sbi->sb, sbi->valid_super_block ? 0: 1);\n\tif (!bh)\n\t\treturn -EIO;\n\terr = __f2fs_commit_super(bh, F2FS_RAW_SUPER(sbi));\n\tbrelse(bh);\n\n\t/* if we are in recovery path, skip writing valid superblock */\n\tif (recover || err)\n\t\treturn err;\n\n\t/* write current valid superblock */\n\tbh = sb_getblk(sbi->sb, sbi->valid_super_block);\n\tif (!bh)\n\t\treturn -EIO;\n\terr = __f2fs_commit_super(bh, F2FS_RAW_SUPER(sbi));\n\tbrelse(bh);\n\treturn err;\n}\n\nstatic int f2fs_scan_devices(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tunsigned int max_devices = MAX_DEVICES;\n\tint i;\n\n\t/* Initialize single device information */\n\tif (!RDEV(0).path[0]) {\n\t\tif (!bdev_is_zoned(sbi->sb->s_bdev))\n\t\t\treturn 0;\n\t\tmax_devices = 1;\n\t}\n\n\t/*\n\t * Initialize multiple devices information, or single\n\t * zoned block device information.\n\t */\n\tsbi->devs = kcalloc(max_devices, sizeof(struct f2fs_dev_info),\n\t\t\t\tGFP_KERNEL);\n\tif (!sbi->devs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < max_devices; i++) {\n\n\t\tif (i > 0 && !RDEV(i).path[0])\n\t\t\tbreak;\n\n\t\tif (max_devices == 1) {\n\t\t\t/* Single zoned block device mount */\n\t\t\tFDEV(0).bdev =\n\t\t\t\tblkdev_get_by_dev(sbi->sb->s_bdev->bd_dev,\n\t\t\t\t\tsbi->sb->s_mode, sbi->sb->s_type);\n\t\t} else {\n\t\t\t/* Multi-device mount */\n\t\t\tmemcpy(FDEV(i).path, RDEV(i).path, MAX_PATH_LEN);\n\t\t\tFDEV(i).total_segments =\n\t\t\t\tle32_to_cpu(RDEV(i).total_segments);\n\t\t\tif (i == 0) {\n\t\t\t\tFDEV(i).start_blk = 0;\n\t\t\t\tFDEV(i).end_blk = FDEV(i).start_blk +\n\t\t\t\t    (FDEV(i).total_segments <<\n\t\t\t\t    sbi->log_blocks_per_seg) - 1 +\n\t\t\t\t    le32_to_cpu(raw_super->segment0_blkaddr);\n\t\t\t} else {\n\t\t\t\tFDEV(i).start_blk = FDEV(i - 1).end_blk + 1;\n\t\t\t\tFDEV(i).end_blk = FDEV(i).start_blk +\n\t\t\t\t\t(FDEV(i).total_segments <<\n\t\t\t\t\tsbi->log_blocks_per_seg) - 1;\n\t\t\t}\n\t\t\tFDEV(i).bdev = blkdev_get_by_path(FDEV(i).path,\n\t\t\t\t\tsbi->sb->s_mode, sbi->sb->s_type);\n\t\t}\n\t\tif (IS_ERR(FDEV(i).bdev))\n\t\t\treturn PTR_ERR(FDEV(i).bdev);\n\n\t\t/* to release errored devices */\n\t\tsbi->s_ndevs = i + 1;\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tif (bdev_zoned_model(FDEV(i).bdev) == BLK_ZONED_HM &&\n\t\t\t\t!f2fs_sb_mounted_blkzoned(sbi->sb)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\"Zoned block device feature not enabled\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (bdev_zoned_model(FDEV(i).bdev) != BLK_ZONED_NONE) {\n\t\t\tif (init_blkz_info(sbi, i)) {\n\t\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\t\"Failed to initialize F2FS blkzone information\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (max_devices == 1)\n\t\t\t\tbreak;\n\t\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\t\"Mount Device [%2d]: %20s, %8u, %8x - %8x (zone: %s)\",\n\t\t\t\ti, FDEV(i).path,\n\t\t\t\tFDEV(i).total_segments,\n\t\t\t\tFDEV(i).start_blk, FDEV(i).end_blk,\n\t\t\t\tbdev_zoned_model(FDEV(i).bdev) == BLK_ZONED_HA ?\n\t\t\t\t\"Host-aware\" : \"Host-managed\");\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"Mount Device [%2d]: %20s, %8u, %8x - %8x\",\n\t\t\t\ti, FDEV(i).path,\n\t\t\t\tFDEV(i).total_segments,\n\t\t\t\tFDEV(i).start_blk, FDEV(i).end_blk);\n\t}\n\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"IO Block Size: %8d KB\", F2FS_IO_SIZE_KB(sbi));\n\treturn 0;\n}\n\nstatic int f2fs_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct f2fs_sb_info *sbi;\n\tstruct f2fs_super_block *raw_super;\n\tstruct inode *root;\n\tint err;\n\tbool retry = true, need_fsck = false;\n\tchar *options = NULL;\n\tint recovery, i, valid_super_block;\n\tstruct curseg_info *seg_i;\n\ntry_onemore:\n\terr = -EINVAL;\n\traw_super = NULL;\n\tvalid_super_block = -1;\n\trecovery = 0;\n\n\t/* allocate memory for f2fs-specific super block info */\n\tsbi = kzalloc(sizeof(struct f2fs_sb_info), GFP_KERNEL);\n\tif (!sbi)\n\t\treturn -ENOMEM;\n\n\tsbi->sb = sb;\n\n\t/* Load the checksum driver */\n\tsbi->s_chksum_driver = crypto_alloc_shash(\"crc32\", 0, 0);\n\tif (IS_ERR(sbi->s_chksum_driver)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Cannot load crc32 driver.\");\n\t\terr = PTR_ERR(sbi->s_chksum_driver);\n\t\tsbi->s_chksum_driver = NULL;\n\t\tgoto free_sbi;\n\t}\n\n\t/* set a block size */\n\tif (unlikely(!sb_set_blocksize(sb, F2FS_BLKSIZE))) {\n\t\tf2fs_msg(sb, KERN_ERR, \"unable to set blocksize\");\n\t\tgoto free_sbi;\n\t}\n\n\terr = read_raw_super_block(sbi, &raw_super, &valid_super_block,\n\t\t\t\t\t\t\t\t&recovery);\n\tif (err)\n\t\tgoto free_sbi;\n\n\tsb->s_fs_info = sbi;\n\tsbi->raw_super = raw_super;\n\n\t/* precompute checksum seed for metadata */\n\tif (f2fs_sb_has_inode_chksum(sb))\n\t\tsbi->s_chksum_seed = f2fs_chksum(sbi, ~0, raw_super->uuid,\n\t\t\t\t\t\tsizeof(raw_super->uuid));\n\n\t/*\n\t * The BLKZONED feature indicates that the drive was formatted with\n\t * zone alignment optimization. This is optional for host-aware\n\t * devices, but mandatory for host-managed zoned block devices.\n\t */\n#ifndef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t \"Zoned block device support is not enabled\\n\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto free_sb_buf;\n\t}\n#endif\n\tdefault_options(sbi);\n\t/* parse mount options */\n\toptions = kstrdup((const char *)data, GFP_KERNEL);\n\tif (data && !options) {\n\t\terr = -ENOMEM;\n\t\tgoto free_sb_buf;\n\t}\n\n\terr = parse_options(sb, options);\n\tif (err)\n\t\tgoto free_options;\n\n\tsbi->max_file_blocks = max_file_blocks();\n\tsb->s_maxbytes = sbi->max_file_blocks <<\n\t\t\t\tle32_to_cpu(raw_super->log_blocksize);\n\tsb->s_max_links = F2FS_LINK_MAX;\n\tget_random_bytes(&sbi->s_next_generation, sizeof(u32));\n\n#ifdef CONFIG_QUOTA\n\tsb->dq_op = &f2fs_quota_operations;\n\tsb->s_qcop = &f2fs_quotactl_ops;\n\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ;\n#endif\n\n\tsb->s_op = &f2fs_sops;\n\tsb->s_cop = &f2fs_cryptops;\n\tsb->s_xattr = f2fs_xattr_handlers;\n\tsb->s_export_op = &f2fs_export_ops;\n\tsb->s_magic = F2FS_SUPER_MAGIC;\n\tsb->s_time_gran = 1;\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? MS_POSIXACL : 0);\n\tmemcpy(&sb->s_uuid, raw_super->uuid, sizeof(raw_super->uuid));\n\n\t/* init f2fs-specific super block info */\n\tsbi->valid_super_block = valid_super_block;\n\tmutex_init(&sbi->gc_mutex);\n\tmutex_init(&sbi->cp_mutex);\n\tinit_rwsem(&sbi->node_write);\n\tinit_rwsem(&sbi->node_change);\n\n\t/* disallow all the data/node/meta page writes */\n\tset_sbi_flag(sbi, SBI_POR_DOING);\n\tspin_lock_init(&sbi->stat_lock);\n\n\t/* init iostat info */\n\tspin_lock_init(&sbi->iostat_lock);\n\tsbi->iostat_enable = false;\n\n\tfor (i = 0; i < NR_PAGE_TYPE; i++) {\n\t\tint n = (i == META) ? 1: NR_TEMP_TYPE;\n\t\tint j;\n\n\t\tsbi->write_io[i] = kmalloc(n * sizeof(struct f2fs_bio_info),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!sbi->write_io[i]) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_options;\n\t\t}\n\n\t\tfor (j = HOT; j < n; j++) {\n\t\t\tinit_rwsem(&sbi->write_io[i][j].io_rwsem);\n\t\t\tsbi->write_io[i][j].sbi = sbi;\n\t\t\tsbi->write_io[i][j].bio = NULL;\n\t\t\tspin_lock_init(&sbi->write_io[i][j].io_lock);\n\t\t\tINIT_LIST_HEAD(&sbi->write_io[i][j].io_list);\n\t\t}\n\t}\n\n\tinit_rwsem(&sbi->cp_rwsem);\n\tinit_waitqueue_head(&sbi->cp_wait);\n\tinit_sb_info(sbi);\n\n\terr = init_percpu_info(sbi);\n\tif (err)\n\t\tgoto free_options;\n\n\tif (F2FS_IO_SIZE(sbi) > 1) {\n\t\tsbi->write_io_dummy =\n\t\t\tmempool_create_page_pool(2 * (F2FS_IO_SIZE(sbi) - 1), 0);\n\t\tif (!sbi->write_io_dummy) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_options;\n\t\t}\n\t}\n\n\t/* get an inode for meta space */\n\tsbi->meta_inode = f2fs_iget(sb, F2FS_META_INO(sbi));\n\tif (IS_ERR(sbi->meta_inode)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read F2FS meta data inode\");\n\t\terr = PTR_ERR(sbi->meta_inode);\n\t\tgoto free_io_dummy;\n\t}\n\n\terr = get_valid_checkpoint(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to get valid F2FS checkpoint\");\n\t\tgoto free_meta_inode;\n\t}\n\n\t/* Initialize device list */\n\terr = f2fs_scan_devices(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to find devices\");\n\t\tgoto free_devices;\n\t}\n\n\tsbi->total_valid_node_count =\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_node_count);\n\tpercpu_counter_set(&sbi->total_valid_inode_count,\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_inode_count));\n\tsbi->user_block_count = le64_to_cpu(sbi->ckpt->user_block_count);\n\tsbi->total_valid_block_count =\n\t\t\t\tle64_to_cpu(sbi->ckpt->valid_block_count);\n\tsbi->last_valid_block_count = sbi->total_valid_block_count;\n\tsbi->reserved_blocks = 0;\n\n\tfor (i = 0; i < NR_INODE_TYPE; i++) {\n\t\tINIT_LIST_HEAD(&sbi->inode_list[i]);\n\t\tspin_lock_init(&sbi->inode_lock[i]);\n\t}\n\n\tinit_extent_cache_info(sbi);\n\n\tinit_ino_entry_info(sbi);\n\n\t/* setup f2fs internal modules */\n\terr = build_segment_manager(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Failed to initialize F2FS segment manager\");\n\t\tgoto free_sm;\n\t}\n\terr = build_node_manager(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Failed to initialize F2FS node manager\");\n\t\tgoto free_nm;\n\t}\n\n\t/* For write statistics */\n\tif (sb->s_bdev->bd_part)\n\t\tsbi->sectors_written_start =\n\t\t\t(u64)part_stat_read(sb->s_bdev->bd_part, sectors[1]);\n\n\t/* Read accumulated write IO statistics if exists */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_NODE);\n\tif (__exist_node_summaries(sbi))\n\t\tsbi->kbytes_written =\n\t\t\tle64_to_cpu(seg_i->journal->info.kbytes_written);\n\n\tbuild_gc_manager(sbi);\n\n\t/* get an inode for node space */\n\tsbi->node_inode = f2fs_iget(sb, F2FS_NODE_INO(sbi));\n\tif (IS_ERR(sbi->node_inode)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read node inode\");\n\t\terr = PTR_ERR(sbi->node_inode);\n\t\tgoto free_nm;\n\t}\n\n\tf2fs_join_shrinker(sbi);\n\n\terr = f2fs_build_stats(sbi);\n\tif (err)\n\t\tgoto free_nm;\n\n\t/* read root inode and dentry */\n\troot = f2fs_iget(sb, F2FS_ROOT_INO(sbi));\n\tif (IS_ERR(root)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read root inode\");\n\t\terr = PTR_ERR(root);\n\t\tgoto free_node_inode;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) {\n\t\tiput(root);\n\t\terr = -EINVAL;\n\t\tgoto free_node_inode;\n\t}\n\n\tsb->s_root = d_make_root(root); /* allocate root dentry */\n\tif (!sb->s_root) {\n\t\terr = -ENOMEM;\n\t\tgoto free_root_inode;\n\t}\n\n\terr = f2fs_register_sysfs(sbi);\n\tif (err)\n\t\tgoto free_root_inode;\n\n\t/* if there are nt orphan nodes free them */\n\terr = recover_orphan_inodes(sbi);\n\tif (err)\n\t\tgoto free_sysfs;\n\n\t/* recover fsynced data */\n\tif (!test_opt(sbi, DISABLE_ROLL_FORWARD)) {\n\t\t/*\n\t\t * mount should be failed, when device has readonly mode, and\n\t\t * previous checkpoint was not done by clean system shutdown.\n\t\t */\n\t\tif (bdev_read_only(sb->s_bdev) &&\n\t\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\t\terr = -EROFS;\n\t\t\tgoto free_meta;\n\t\t}\n\n\t\tif (need_fsck)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\t\tif (!retry)\n\t\t\tgoto skip_recovery;\n\n\t\terr = recover_fsync_data(sbi, false);\n\t\tif (err < 0) {\n\t\t\tneed_fsck = true;\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Cannot recover all fsync data errno=%d\", err);\n\t\t\tgoto free_meta;\n\t\t}\n\t} else {\n\t\terr = recover_fsync_data(sbi, true);\n\n\t\tif (!f2fs_readonly(sb) && err > 0) {\n\t\t\terr = -EINVAL;\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Need to recover fsync data\");\n\t\t\tgoto free_sysfs;\n\t\t}\n\t}\nskip_recovery:\n\t/* recover_fsync_data() cleared this already */\n\tclear_sbi_flag(sbi, SBI_POR_DOING);\n\n\t/*\n\t * If filesystem is not mounted as read-only then\n\t * do start the gc_thread.\n\t */\n\tif (test_opt(sbi, BG_GC) && !f2fs_readonly(sb)) {\n\t\t/* After POR, we can run background GC thread.*/\n\t\terr = start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto free_meta;\n\t}\n\tkfree(options);\n\n\t/* recover broken superblock */\n\tif (recovery) {\n\t\terr = f2fs_commit_super(sbi, true);\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Try to recover %dth superblock, ret: %d\",\n\t\t\tsbi->valid_super_block ? 1 : 2, err);\n\t}\n\n\tf2fs_msg(sbi->sb, KERN_NOTICE, \"Mounted with checkpoint version = %llx\",\n\t\t\t\tcur_cp_version(F2FS_CKPT(sbi)));\n\tf2fs_update_time(sbi, CP_TIME);\n\tf2fs_update_time(sbi, REQ_TIME);\n\treturn 0;\n\nfree_meta:\n\tf2fs_sync_inode_meta(sbi);\n\t/*\n\t * Some dirty meta pages can be produced by recover_orphan_inodes()\n\t * failed by EIO. Then, iput(node_inode) can trigger balance_fs_bg()\n\t * followed by write_checkpoint() through f2fs_write_node_pages(), which\n\t * falls into an infinite loop in sync_meta_pages().\n\t */\n\ttruncate_inode_pages_final(META_MAPPING(sbi));\nfree_sysfs:\n\tf2fs_unregister_sysfs(sbi);\nfree_root_inode:\n\tdput(sb->s_root);\n\tsb->s_root = NULL;\nfree_node_inode:\n\ttruncate_inode_pages_final(NODE_MAPPING(sbi));\n\tmutex_lock(&sbi->umount_mutex);\n\trelease_ino_entry(sbi, true);\n\tf2fs_leave_shrinker(sbi);\n\tiput(sbi->node_inode);\n\tmutex_unlock(&sbi->umount_mutex);\n\tf2fs_destroy_stats(sbi);\nfree_nm:\n\tdestroy_node_manager(sbi);\nfree_sm:\n\tdestroy_segment_manager(sbi);\nfree_devices:\n\tdestroy_device_list(sbi);\n\tkfree(sbi->ckpt);\nfree_meta_inode:\n\tmake_bad_inode(sbi->meta_inode);\n\tiput(sbi->meta_inode);\nfree_io_dummy:\n\tmempool_destroy(sbi->write_io_dummy);\nfree_options:\n\tfor (i = 0; i < NR_PAGE_TYPE; i++)\n\t\tkfree(sbi->write_io[i]);\n\tdestroy_percpu_info(sbi);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\tkfree(options);\nfree_sb_buf:\n\tkfree(raw_super);\nfree_sbi:\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi);\n\n\t/* give only one another chance */\n\tif (retry) {\n\t\tretry = false;\n\t\tshrink_dcache_sb(sb);\n\t\tgoto try_onemore;\n\t}\n\treturn err;\n}\n\nstatic struct dentry *f2fs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *data)\n{\n\treturn mount_bdev(fs_type, flags, dev_name, data, f2fs_fill_super);\n}\n\nstatic void kill_f2fs_super(struct super_block *sb)\n{\n\tif (sb->s_root) {\n\t\tset_sbi_flag(F2FS_SB(sb), SBI_IS_CLOSE);\n\t\tstop_gc_thread(F2FS_SB(sb));\n\t\tstop_discard_thread(F2FS_SB(sb));\n\t}\n\tkill_block_super(sb);\n}\n\nstatic struct file_system_type f2fs_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"f2fs\",\n\t.mount\t\t= f2fs_mount,\n\t.kill_sb\t= kill_f2fs_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"f2fs\");\n\nstatic int __init init_inodecache(void)\n{\n\tf2fs_inode_cachep = kmem_cache_create(\"f2fs_inode_cache\",\n\t\t\tsizeof(struct f2fs_inode_info), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT|SLAB_ACCOUNT, NULL);\n\tif (!f2fs_inode_cachep)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void destroy_inodecache(void)\n{\n\t/*\n\t * Make sure all delayed rcu free inodes are flushed before we\n\t * destroy cache.\n\t */\n\trcu_barrier();\n\tkmem_cache_destroy(f2fs_inode_cachep);\n}\n\nstatic int __init init_f2fs_fs(void)\n{\n\tint err;\n\n\tf2fs_build_trace_ios();\n\n\terr = init_inodecache();\n\tif (err)\n\t\tgoto fail;\n\terr = create_node_manager_caches();\n\tif (err)\n\t\tgoto free_inodecache;\n\terr = create_segment_manager_caches();\n\tif (err)\n\t\tgoto free_node_manager_caches;\n\terr = create_checkpoint_caches();\n\tif (err)\n\t\tgoto free_segment_manager_caches;\n\terr = create_extent_cache();\n\tif (err)\n\t\tgoto free_checkpoint_caches;\n\terr = f2fs_init_sysfs();\n\tif (err)\n\t\tgoto free_extent_cache;\n\terr = register_shrinker(&f2fs_shrinker_info);\n\tif (err)\n\t\tgoto free_sysfs;\n\terr = register_filesystem(&f2fs_fs_type);\n\tif (err)\n\t\tgoto free_shrinker;\n\terr = f2fs_create_root_stats();\n\tif (err)\n\t\tgoto free_filesystem;\n\treturn 0;\n\nfree_filesystem:\n\tunregister_filesystem(&f2fs_fs_type);\nfree_shrinker:\n\tunregister_shrinker(&f2fs_shrinker_info);\nfree_sysfs:\n\tf2fs_exit_sysfs();\nfree_extent_cache:\n\tdestroy_extent_cache();\nfree_checkpoint_caches:\n\tdestroy_checkpoint_caches();\nfree_segment_manager_caches:\n\tdestroy_segment_manager_caches();\nfree_node_manager_caches:\n\tdestroy_node_manager_caches();\nfree_inodecache:\n\tdestroy_inodecache();\nfail:\n\treturn err;\n}\n\nstatic void __exit exit_f2fs_fs(void)\n{\n\tf2fs_destroy_root_stats();\n\tunregister_filesystem(&f2fs_fs_type);\n\tunregister_shrinker(&f2fs_shrinker_info);\n\tf2fs_exit_sysfs();\n\tdestroy_extent_cache();\n\tdestroy_checkpoint_caches();\n\tdestroy_segment_manager_caches();\n\tdestroy_node_manager_caches();\n\tdestroy_inodecache();\n\tf2fs_destroy_trace_ios();\n}\n\nmodule_init(init_f2fs_fs)\nmodule_exit(exit_f2fs_fs)\n\nMODULE_AUTHOR(\"Samsung Electronics's Praesto Team\");\nMODULE_DESCRIPTION(\"Flash Friendly File System\");\nMODULE_LICENSE(\"GPL\");\n\n"], "fixing_code": ["/*\n * fs/f2fs/f2fs.h\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#ifndef _LINUX_F2FS_H\n#define _LINUX_F2FS_H\n\n#include <linux/types.h>\n#include <linux/page-flags.h>\n#include <linux/buffer_head.h>\n#include <linux/slab.h>\n#include <linux/crc32.h>\n#include <linux/magic.h>\n#include <linux/kobject.h>\n#include <linux/sched.h>\n#include <linux/vmalloc.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/quotaops.h>\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\n#include <linux/fscrypt_supp.h>\n#else\n#include <linux/fscrypt_notsupp.h>\n#endif\n#include <crypto/hash.h>\n\n#ifdef CONFIG_F2FS_CHECK_FS\n#define f2fs_bug_on(sbi, condition)\tBUG_ON(condition)\n#else\n#define f2fs_bug_on(sbi, condition)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (unlikely(condition)) {\t\t\t\t\\\n\t\t\tWARN_ON(1);\t\t\t\t\t\\\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n#endif\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\nenum {\n\tFAULT_KMALLOC,\n\tFAULT_PAGE_ALLOC,\n\tFAULT_ALLOC_NID,\n\tFAULT_ORPHAN,\n\tFAULT_BLOCK,\n\tFAULT_DIR_DEPTH,\n\tFAULT_EVICT_INODE,\n\tFAULT_TRUNCATE,\n\tFAULT_IO,\n\tFAULT_CHECKPOINT,\n\tFAULT_MAX,\n};\n\nstruct f2fs_fault_info {\n\tatomic_t inject_ops;\n\tunsigned int inject_rate;\n\tunsigned int inject_type;\n};\n\nextern char *fault_name[FAULT_MAX];\n#define IS_FAULT_SET(fi, type) ((fi)->inject_type & (1 << (type)))\n#endif\n\n/*\n * For mount options\n */\n#define F2FS_MOUNT_BG_GC\t\t0x00000001\n#define F2FS_MOUNT_DISABLE_ROLL_FORWARD\t0x00000002\n#define F2FS_MOUNT_DISCARD\t\t0x00000004\n#define F2FS_MOUNT_NOHEAP\t\t0x00000008\n#define F2FS_MOUNT_XATTR_USER\t\t0x00000010\n#define F2FS_MOUNT_POSIX_ACL\t\t0x00000020\n#define F2FS_MOUNT_DISABLE_EXT_IDENTIFY\t0x00000040\n#define F2FS_MOUNT_INLINE_XATTR\t\t0x00000080\n#define F2FS_MOUNT_INLINE_DATA\t\t0x00000100\n#define F2FS_MOUNT_INLINE_DENTRY\t0x00000200\n#define F2FS_MOUNT_FLUSH_MERGE\t\t0x00000400\n#define F2FS_MOUNT_NOBARRIER\t\t0x00000800\n#define F2FS_MOUNT_FASTBOOT\t\t0x00001000\n#define F2FS_MOUNT_EXTENT_CACHE\t\t0x00002000\n#define F2FS_MOUNT_FORCE_FG_GC\t\t0x00004000\n#define F2FS_MOUNT_DATA_FLUSH\t\t0x00008000\n#define F2FS_MOUNT_FAULT_INJECTION\t0x00010000\n#define F2FS_MOUNT_ADAPTIVE\t\t0x00020000\n#define F2FS_MOUNT_LFS\t\t\t0x00040000\n#define F2FS_MOUNT_USRQUOTA\t\t0x00080000\n#define F2FS_MOUNT_GRPQUOTA\t\t0x00100000\n#define F2FS_MOUNT_PRJQUOTA\t\t0x00200000\n#define F2FS_MOUNT_QUOTA\t\t0x00400000\n\n#define clear_opt(sbi, option)\t((sbi)->mount_opt.opt &= ~F2FS_MOUNT_##option)\n#define set_opt(sbi, option)\t((sbi)->mount_opt.opt |= F2FS_MOUNT_##option)\n#define test_opt(sbi, option)\t((sbi)->mount_opt.opt & F2FS_MOUNT_##option)\n\n#define ver_after(a, b)\t(typecheck(unsigned long long, a) &&\t\t\\\n\t\ttypecheck(unsigned long long, b) &&\t\t\t\\\n\t\t((long long)((a) - (b)) > 0))\n\ntypedef u32 block_t;\t/*\n\t\t\t * should not change u32, since it is the on-disk block\n\t\t\t * address format, __le32.\n\t\t\t */\ntypedef u32 nid_t;\n\nstruct f2fs_mount_info {\n\tunsigned int\topt;\n};\n\n#define F2FS_FEATURE_ENCRYPT\t\t0x0001\n#define F2FS_FEATURE_BLKZONED\t\t0x0002\n#define F2FS_FEATURE_ATOMIC_WRITE\t0x0004\n#define F2FS_FEATURE_EXTRA_ATTR\t\t0x0008\n#define F2FS_FEATURE_PRJQUOTA\t\t0x0010\n#define F2FS_FEATURE_INODE_CHKSUM\t0x0020\n\n#define F2FS_HAS_FEATURE(sb, mask)\t\t\t\t\t\\\n\t((F2FS_SB(sb)->raw_super->feature & cpu_to_le32(mask)) != 0)\n#define F2FS_SET_FEATURE(sb, mask)\t\t\t\t\t\\\n\t(F2FS_SB(sb)->raw_super->feature |= cpu_to_le32(mask))\n#define F2FS_CLEAR_FEATURE(sb, mask)\t\t\t\t\t\\\n\t(F2FS_SB(sb)->raw_super->feature &= ~cpu_to_le32(mask))\n\n/*\n * For checkpoint manager\n */\nenum {\n\tNAT_BITMAP,\n\tSIT_BITMAP\n};\n\n#define\tCP_UMOUNT\t0x00000001\n#define\tCP_FASTBOOT\t0x00000002\n#define\tCP_SYNC\t\t0x00000004\n#define\tCP_RECOVERY\t0x00000008\n#define\tCP_DISCARD\t0x00000010\n#define CP_TRIMMED\t0x00000020\n\n#define DEF_BATCHED_TRIM_SECTIONS\t2048\n#define BATCHED_TRIM_SEGMENTS(sbi)\t\\\n\t\t(GET_SEG_FROM_SEC(sbi, SM_I(sbi)->trim_sections))\n#define BATCHED_TRIM_BLOCKS(sbi)\t\\\n\t\t(BATCHED_TRIM_SEGMENTS(sbi) << (sbi)->log_blocks_per_seg)\n#define MAX_DISCARD_BLOCKS(sbi)\t\tBLKS_PER_SEC(sbi)\n#define DISCARD_ISSUE_RATE\t\t8\n#define DEF_MIN_DISCARD_ISSUE_TIME\t50\t/* 50 ms, if exists */\n#define DEF_MAX_DISCARD_ISSUE_TIME\t60000\t/* 60 s, if no candidates */\n#define DEF_CP_INTERVAL\t\t\t60\t/* 60 secs */\n#define DEF_IDLE_INTERVAL\t\t5\t/* 5 secs */\n\nstruct cp_control {\n\tint reason;\n\t__u64 trim_start;\n\t__u64 trim_end;\n\t__u64 trim_minlen;\n\t__u64 trimmed;\n};\n\n/*\n * For CP/NAT/SIT/SSA readahead\n */\nenum {\n\tMETA_CP,\n\tMETA_NAT,\n\tMETA_SIT,\n\tMETA_SSA,\n\tMETA_POR,\n};\n\n/* for the list of ino */\nenum {\n\tORPHAN_INO,\t\t/* for orphan ino list */\n\tAPPEND_INO,\t\t/* for append ino list */\n\tUPDATE_INO,\t\t/* for update ino list */\n\tMAX_INO_ENTRY,\t\t/* max. list */\n};\n\nstruct ino_entry {\n\tstruct list_head list;\t/* list head */\n\tnid_t ino;\t\t/* inode number */\n};\n\n/* for the list of inodes to be GCed */\nstruct inode_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct inode *inode;\t/* vfs inode pointer */\n};\n\n/* for the bitmap indicate blocks to be discarded */\nstruct discard_entry {\n\tstruct list_head list;\t/* list head */\n\tblock_t start_blkaddr;\t/* start blockaddr of current segment */\n\tunsigned char discard_map[SIT_VBLOCK_MAP_SIZE];\t/* segment discard bitmap */\n};\n\n/* default discard granularity of inner discard thread, unit: block count */\n#define DEFAULT_DISCARD_GRANULARITY\t\t16\n\n/* max discard pend list number */\n#define MAX_PLIST_NUM\t\t512\n#define plist_idx(blk_num)\t((blk_num) >= MAX_PLIST_NUM ?\t\t\\\n\t\t\t\t\t(MAX_PLIST_NUM - 1) : (blk_num - 1))\n\n#define P_ACTIVE\t0x01\n#define P_TRIM\t\t0x02\n#define plist_issue(tag)\t(((tag) & P_ACTIVE) || ((tag) & P_TRIM))\n\nenum {\n\tD_PREP,\n\tD_SUBMIT,\n\tD_DONE,\n};\n\nstruct discard_info {\n\tblock_t lstart;\t\t\t/* logical start address */\n\tblock_t len;\t\t\t/* length */\n\tblock_t start;\t\t\t/* actual start address in dev */\n};\n\nstruct discard_cmd {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tunion {\n\t\tstruct {\n\t\t\tblock_t lstart;\t/* logical start address */\n\t\t\tblock_t len;\t/* length */\n\t\t\tblock_t start;\t/* actual start address in dev */\n\t\t};\n\t\tstruct discard_info di;\t/* discard info */\n\n\t};\n\tstruct list_head list;\t\t/* command list */\n\tstruct completion wait;\t\t/* compleation */\n\tstruct block_device *bdev;\t/* bdev */\n\tunsigned short ref;\t\t/* reference count */\n\tunsigned char state;\t\t/* state */\n\tint error;\t\t\t/* bio error */\n};\n\nstruct discard_cmd_control {\n\tstruct task_struct *f2fs_issue_discard;\t/* discard thread */\n\tstruct list_head entry_list;\t\t/* 4KB discard entry list */\n\tstruct list_head pend_list[MAX_PLIST_NUM];/* store pending entries */\n\tunsigned char pend_list_tag[MAX_PLIST_NUM];/* tag for pending entries */\n\tstruct list_head wait_list;\t\t/* store on-flushing entries */\n\twait_queue_head_t discard_wait_queue;\t/* waiting queue for wake-up */\n\tunsigned int discard_wake;\t\t/* to wake up discard thread */\n\tstruct mutex cmd_lock;\n\tunsigned int nr_discards;\t\t/* # of discards in the list */\n\tunsigned int max_discards;\t\t/* max. discards to be issued */\n\tunsigned int discard_granularity;\t/* discard granularity */\n\tunsigned int undiscard_blks;\t\t/* # of undiscard blocks */\n\tatomic_t issued_discard;\t\t/* # of issued discard */\n\tatomic_t issing_discard;\t\t/* # of issing discard */\n\tatomic_t discard_cmd_cnt;\t\t/* # of cached cmd count */\n\tstruct rb_root root;\t\t\t/* root of discard rb-tree */\n};\n\n/* for the list of fsync inodes, used only during recovery */\nstruct fsync_inode_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct inode *inode;\t/* vfs inode pointer */\n\tblock_t blkaddr;\t/* block address locating the last fsync */\n\tblock_t last_dentry;\t/* block address locating the last dentry */\n};\n\n#define nats_in_cursum(jnl)\t\t(le16_to_cpu((jnl)->n_nats))\n#define sits_in_cursum(jnl)\t\t(le16_to_cpu((jnl)->n_sits))\n\n#define nat_in_journal(jnl, i)\t\t((jnl)->nat_j.entries[i].ne)\n#define nid_in_journal(jnl, i)\t\t((jnl)->nat_j.entries[i].nid)\n#define sit_in_journal(jnl, i)\t\t((jnl)->sit_j.entries[i].se)\n#define segno_in_journal(jnl, i)\t((jnl)->sit_j.entries[i].segno)\n\n#define MAX_NAT_JENTRIES(jnl)\t(NAT_JOURNAL_ENTRIES - nats_in_cursum(jnl))\n#define MAX_SIT_JENTRIES(jnl)\t(SIT_JOURNAL_ENTRIES - sits_in_cursum(jnl))\n\nstatic inline int update_nats_in_cursum(struct f2fs_journal *journal, int i)\n{\n\tint before = nats_in_cursum(journal);\n\n\tjournal->n_nats = cpu_to_le16(before + i);\n\treturn before;\n}\n\nstatic inline int update_sits_in_cursum(struct f2fs_journal *journal, int i)\n{\n\tint before = sits_in_cursum(journal);\n\n\tjournal->n_sits = cpu_to_le16(before + i);\n\treturn before;\n}\n\nstatic inline bool __has_cursum_space(struct f2fs_journal *journal,\n\t\t\t\t\t\t\tint size, int type)\n{\n\tif (type == NAT_JOURNAL)\n\t\treturn size <= MAX_NAT_JENTRIES(journal);\n\treturn size <= MAX_SIT_JENTRIES(journal);\n}\n\n/*\n * ioctl commands\n */\n#define F2FS_IOC_GETFLAGS\t\tFS_IOC_GETFLAGS\n#define F2FS_IOC_SETFLAGS\t\tFS_IOC_SETFLAGS\n#define F2FS_IOC_GETVERSION\t\tFS_IOC_GETVERSION\n\n#define F2FS_IOCTL_MAGIC\t\t0xf5\n#define F2FS_IOC_START_ATOMIC_WRITE\t_IO(F2FS_IOCTL_MAGIC, 1)\n#define F2FS_IOC_COMMIT_ATOMIC_WRITE\t_IO(F2FS_IOCTL_MAGIC, 2)\n#define F2FS_IOC_START_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 3)\n#define F2FS_IOC_RELEASE_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 4)\n#define F2FS_IOC_ABORT_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 5)\n#define F2FS_IOC_GARBAGE_COLLECT\t_IOW(F2FS_IOCTL_MAGIC, 6, __u32)\n#define F2FS_IOC_WRITE_CHECKPOINT\t_IO(F2FS_IOCTL_MAGIC, 7)\n#define F2FS_IOC_DEFRAGMENT\t\t_IOWR(F2FS_IOCTL_MAGIC, 8,\t\\\n\t\t\t\t\t\tstruct f2fs_defragment)\n#define F2FS_IOC_MOVE_RANGE\t\t_IOWR(F2FS_IOCTL_MAGIC, 9,\t\\\n\t\t\t\t\t\tstruct f2fs_move_range)\n#define F2FS_IOC_FLUSH_DEVICE\t\t_IOW(F2FS_IOCTL_MAGIC, 10,\t\\\n\t\t\t\t\t\tstruct f2fs_flush_device)\n#define F2FS_IOC_GARBAGE_COLLECT_RANGE\t_IOW(F2FS_IOCTL_MAGIC, 11,\t\\\n\t\t\t\t\t\tstruct f2fs_gc_range)\n#define F2FS_IOC_GET_FEATURES\t\t_IOR(F2FS_IOCTL_MAGIC, 12, __u32)\n\n#define F2FS_IOC_SET_ENCRYPTION_POLICY\tFS_IOC_SET_ENCRYPTION_POLICY\n#define F2FS_IOC_GET_ENCRYPTION_POLICY\tFS_IOC_GET_ENCRYPTION_POLICY\n#define F2FS_IOC_GET_ENCRYPTION_PWSALT\tFS_IOC_GET_ENCRYPTION_PWSALT\n\n/*\n * should be same as XFS_IOC_GOINGDOWN.\n * Flags for going down operation used by FS_IOC_GOINGDOWN\n */\n#define F2FS_IOC_SHUTDOWN\t_IOR('X', 125, __u32)\t/* Shutdown */\n#define F2FS_GOING_DOWN_FULLSYNC\t0x0\t/* going down with full sync */\n#define F2FS_GOING_DOWN_METASYNC\t0x1\t/* going down with metadata */\n#define F2FS_GOING_DOWN_NOSYNC\t\t0x2\t/* going down */\n#define F2FS_GOING_DOWN_METAFLUSH\t0x3\t/* going down with meta flush */\n\n#if defined(__KERNEL__) && defined(CONFIG_COMPAT)\n/*\n * ioctl commands in 32 bit emulation\n */\n#define F2FS_IOC32_GETFLAGS\t\tFS_IOC32_GETFLAGS\n#define F2FS_IOC32_SETFLAGS\t\tFS_IOC32_SETFLAGS\n#define F2FS_IOC32_GETVERSION\t\tFS_IOC32_GETVERSION\n#endif\n\n#define F2FS_IOC_FSGETXATTR\t\tFS_IOC_FSGETXATTR\n#define F2FS_IOC_FSSETXATTR\t\tFS_IOC_FSSETXATTR\n\nstruct f2fs_gc_range {\n\tu32 sync;\n\tu64 start;\n\tu64 len;\n};\n\nstruct f2fs_defragment {\n\tu64 start;\n\tu64 len;\n};\n\nstruct f2fs_move_range {\n\tu32 dst_fd;\t\t/* destination fd */\n\tu64 pos_in;\t\t/* start position in src_fd */\n\tu64 pos_out;\t\t/* start position in dst_fd */\n\tu64 len;\t\t/* size to move */\n};\n\nstruct f2fs_flush_device {\n\tu32 dev_num;\t\t/* device number to flush */\n\tu32 segments;\t\t/* # of segments to flush */\n};\n\n/* for inline stuff */\n#define DEF_INLINE_RESERVED_SIZE\t1\nstatic inline int get_extra_isize(struct inode *inode);\n#define MAX_INLINE_DATA(inode)\t(sizeof(__le32) * \\\n\t\t\t\t(CUR_ADDRS_PER_INODE(inode) - \\\n\t\t\t\tDEF_INLINE_RESERVED_SIZE - \\\n\t\t\t\tF2FS_INLINE_XATTR_ADDRS))\n\n/* for inline dir */\n#define NR_INLINE_DENTRY(inode)\t(MAX_INLINE_DATA(inode) * BITS_PER_BYTE / \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tBITS_PER_BYTE + 1))\n#define INLINE_DENTRY_BITMAP_SIZE(inode)\t((NR_INLINE_DENTRY(inode) + \\\n\t\t\t\t\tBITS_PER_BYTE - 1) / BITS_PER_BYTE)\n#define INLINE_RESERVED_SIZE(inode)\t(MAX_INLINE_DATA(inode) - \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tNR_INLINE_DENTRY(inode) + \\\n\t\t\t\tINLINE_DENTRY_BITMAP_SIZE(inode)))\n\n/*\n * For INODE and NODE manager\n */\n/* for directory operations */\nstruct f2fs_dentry_ptr {\n\tstruct inode *inode;\n\tvoid *bitmap;\n\tstruct f2fs_dir_entry *dentry;\n\t__u8 (*filename)[F2FS_SLOT_LEN];\n\tint max;\n\tint nr_bitmap;\n};\n\nstatic inline void make_dentry_ptr_block(struct inode *inode,\n\t\tstruct f2fs_dentry_ptr *d, struct f2fs_dentry_block *t)\n{\n\td->inode = inode;\n\td->max = NR_DENTRY_IN_BLOCK;\n\td->nr_bitmap = SIZE_OF_DENTRY_BITMAP;\n\td->bitmap = &t->dentry_bitmap;\n\td->dentry = t->dentry;\n\td->filename = t->filename;\n}\n\nstatic inline void make_dentry_ptr_inline(struct inode *inode,\n\t\t\t\t\tstruct f2fs_dentry_ptr *d, void *t)\n{\n\tint entry_cnt = NR_INLINE_DENTRY(inode);\n\tint bitmap_size = INLINE_DENTRY_BITMAP_SIZE(inode);\n\tint reserved_size = INLINE_RESERVED_SIZE(inode);\n\n\td->inode = inode;\n\td->max = entry_cnt;\n\td->nr_bitmap = bitmap_size;\n\td->bitmap = t;\n\td->dentry = t + bitmap_size + reserved_size;\n\td->filename = t + bitmap_size + reserved_size +\n\t\t\t\t\tSIZE_OF_DIR_ENTRY * entry_cnt;\n}\n\n/*\n * XATTR_NODE_OFFSET stores xattrs to one node block per file keeping -1\n * as its node offset to distinguish from index node blocks.\n * But some bits are used to mark the node block.\n */\n#define XATTR_NODE_OFFSET\t((((unsigned int)-1) << OFFSET_BIT_SHIFT) \\\n\t\t\t\t>> OFFSET_BIT_SHIFT)\nenum {\n\tALLOC_NODE,\t\t\t/* allocate a new node page if needed */\n\tLOOKUP_NODE,\t\t\t/* look up a node without readahead */\n\tLOOKUP_NODE_RA,\t\t\t/*\n\t\t\t\t\t * look up a node with readahead called\n\t\t\t\t\t * by get_data_block.\n\t\t\t\t\t */\n};\n\n#define F2FS_LINK_MAX\t0xffffffff\t/* maximum link count per file */\n\n#define MAX_DIR_RA_PAGES\t4\t/* maximum ra pages of dir */\n\n/* vector size for gang look-up from extent cache that consists of radix tree */\n#define EXT_TREE_VEC_SIZE\t64\n\n/* for in-memory extent cache entry */\n#define F2FS_MIN_EXTENT_LEN\t64\t/* minimum extent length */\n\n/* number of extent info in extent cache we try to shrink */\n#define EXTENT_CACHE_SHRINK_NUMBER\t128\n\nstruct rb_entry {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tunsigned int ofs;\t\t/* start offset of the entry */\n\tunsigned int len;\t\t/* length of the entry */\n};\n\nstruct extent_info {\n\tunsigned int fofs;\t\t/* start offset in a file */\n\tunsigned int len;\t\t/* length of the extent */\n\tu32 blk;\t\t\t/* start block address of the extent */\n};\n\nstruct extent_node {\n\tstruct rb_node rb_node;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned int fofs;\n\t\t\tunsigned int len;\n\t\t\tu32 blk;\n\t\t};\n\t\tstruct extent_info ei;\t/* extent info */\n\n\t};\n\tstruct list_head list;\t\t/* node in global extent list of sbi */\n\tstruct extent_tree *et;\t\t/* extent tree pointer */\n};\n\nstruct extent_tree {\n\tnid_t ino;\t\t\t/* inode number */\n\tstruct rb_root root;\t\t/* root of extent info rb-tree */\n\tstruct extent_node *cached_en;\t/* recently accessed extent node */\n\tstruct extent_info largest;\t/* largested extent info */\n\tstruct list_head list;\t\t/* to be used by sbi->zombie_list */\n\trwlock_t lock;\t\t\t/* protect extent info rb-tree */\n\tatomic_t node_cnt;\t\t/* # of extent node in rb-tree*/\n};\n\n/*\n * This structure is taken from ext4_map_blocks.\n *\n * Note that, however, f2fs uses NEW and MAPPED flags for f2fs_map_blocks().\n */\n#define F2FS_MAP_NEW\t\t(1 << BH_New)\n#define F2FS_MAP_MAPPED\t\t(1 << BH_Mapped)\n#define F2FS_MAP_UNWRITTEN\t(1 << BH_Unwritten)\n#define F2FS_MAP_FLAGS\t\t(F2FS_MAP_NEW | F2FS_MAP_MAPPED |\\\n\t\t\t\tF2FS_MAP_UNWRITTEN)\n\nstruct f2fs_map_blocks {\n\tblock_t m_pblk;\n\tblock_t m_lblk;\n\tunsigned int m_len;\n\tunsigned int m_flags;\n\tpgoff_t *m_next_pgofs;\t\t/* point next possible non-hole pgofs */\n};\n\n/* for flag in get_data_block */\nenum {\n\tF2FS_GET_BLOCK_DEFAULT,\n\tF2FS_GET_BLOCK_FIEMAP,\n\tF2FS_GET_BLOCK_BMAP,\n\tF2FS_GET_BLOCK_PRE_DIO,\n\tF2FS_GET_BLOCK_PRE_AIO,\n};\n\n/*\n * i_advise uses FADVISE_XXX_BIT. We can add additional hints later.\n */\n#define FADVISE_COLD_BIT\t0x01\n#define FADVISE_LOST_PINO_BIT\t0x02\n#define FADVISE_ENCRYPT_BIT\t0x04\n#define FADVISE_ENC_NAME_BIT\t0x08\n#define FADVISE_KEEP_SIZE_BIT\t0x10\n\n#define file_is_cold(inode)\tis_file(inode, FADVISE_COLD_BIT)\n#define file_wrong_pino(inode)\tis_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_set_cold(inode)\tset_file(inode, FADVISE_COLD_BIT)\n#define file_lost_pino(inode)\tset_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_clear_cold(inode)\tclear_file(inode, FADVISE_COLD_BIT)\n#define file_got_pino(inode)\tclear_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_is_encrypt(inode)\tis_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_set_encrypt(inode)\tset_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_clear_encrypt(inode) clear_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_enc_name(inode)\tis_file(inode, FADVISE_ENC_NAME_BIT)\n#define file_set_enc_name(inode) set_file(inode, FADVISE_ENC_NAME_BIT)\n#define file_keep_isize(inode)\tis_file(inode, FADVISE_KEEP_SIZE_BIT)\n#define file_set_keep_isize(inode) set_file(inode, FADVISE_KEEP_SIZE_BIT)\n\n#define DEF_DIR_LEVEL\t\t0\n\nstruct f2fs_inode_info {\n\tstruct inode vfs_inode;\t\t/* serve a vfs inode */\n\tunsigned long i_flags;\t\t/* keep an inode flags for ioctl */\n\tunsigned char i_advise;\t\t/* use to give file attribute hints */\n\tunsigned char i_dir_level;\t/* use for dentry level for large dir */\n\tunsigned int i_current_depth;\t/* use only in directory structure */\n\tunsigned int i_pino;\t\t/* parent inode number */\n\tumode_t i_acl_mode;\t\t/* keep file acl mode temporarily */\n\n\t/* Use below internally in f2fs*/\n\tunsigned long flags;\t\t/* use to pass per-file flags */\n\tstruct rw_semaphore i_sem;\t/* protect fi info */\n\tatomic_t dirty_pages;\t\t/* # of dirty pages */\n\tf2fs_hash_t chash;\t\t/* hash value of given file name */\n\tunsigned int clevel;\t\t/* maximum level of given file name */\n\tstruct task_struct *task;\t/* lookup and create consistency */\n\tstruct task_struct *cp_task;\t/* separate cp/wb IO stats*/\n\tnid_t i_xattr_nid;\t\t/* node id that contains xattrs */\n\tloff_t\tlast_disk_size;\t\t/* lastly written file size */\n\n#ifdef CONFIG_QUOTA\n\tstruct dquot *i_dquot[MAXQUOTAS];\n\n\t/* quota space reservation, managed internally by quota code */\n\tqsize_t i_reserved_quota;\n#endif\n\tstruct list_head dirty_list;\t/* dirty list for dirs and files */\n\tstruct list_head gdirty_list;\t/* linked in global dirty list */\n\tstruct list_head inmem_pages;\t/* inmemory pages managed by f2fs */\n\tstruct task_struct *inmem_task;\t/* store inmemory task */\n\tstruct mutex inmem_lock;\t/* lock for inmemory pages */\n\tstruct extent_tree *extent_tree;\t/* cached extent_tree entry */\n\tstruct rw_semaphore dio_rwsem[2];/* avoid racing between dio and gc */\n\tstruct rw_semaphore i_mmap_sem;\n\tstruct rw_semaphore i_xattr_sem; /* avoid racing between reading and changing EAs */\n\n\tint i_extra_isize;\t\t/* size of extra space located in i_addr */\n\tkprojid_t i_projid;\t\t/* id for project quota */\n};\n\nstatic inline void get_extent_info(struct extent_info *ext,\n\t\t\t\t\tstruct f2fs_extent *i_ext)\n{\n\text->fofs = le32_to_cpu(i_ext->fofs);\n\text->blk = le32_to_cpu(i_ext->blk);\n\text->len = le32_to_cpu(i_ext->len);\n}\n\nstatic inline void set_raw_extent(struct extent_info *ext,\n\t\t\t\t\tstruct f2fs_extent *i_ext)\n{\n\ti_ext->fofs = cpu_to_le32(ext->fofs);\n\ti_ext->blk = cpu_to_le32(ext->blk);\n\ti_ext->len = cpu_to_le32(ext->len);\n}\n\nstatic inline void set_extent_info(struct extent_info *ei, unsigned int fofs,\n\t\t\t\t\t\tu32 blk, unsigned int len)\n{\n\tei->fofs = fofs;\n\tei->blk = blk;\n\tei->len = len;\n}\n\nstatic inline bool __is_discard_mergeable(struct discard_info *back,\n\t\t\t\t\t\tstruct discard_info *front)\n{\n\treturn back->lstart + back->len == front->lstart;\n}\n\nstatic inline bool __is_discard_back_mergeable(struct discard_info *cur,\n\t\t\t\t\t\tstruct discard_info *back)\n{\n\treturn __is_discard_mergeable(back, cur);\n}\n\nstatic inline bool __is_discard_front_mergeable(struct discard_info *cur,\n\t\t\t\t\t\tstruct discard_info *front)\n{\n\treturn __is_discard_mergeable(cur, front);\n}\n\nstatic inline bool __is_extent_mergeable(struct extent_info *back,\n\t\t\t\t\t\tstruct extent_info *front)\n{\n\treturn (back->fofs + back->len == front->fofs &&\n\t\t\tback->blk + back->len == front->blk);\n}\n\nstatic inline bool __is_back_mergeable(struct extent_info *cur,\n\t\t\t\t\t\tstruct extent_info *back)\n{\n\treturn __is_extent_mergeable(back, cur);\n}\n\nstatic inline bool __is_front_mergeable(struct extent_info *cur,\n\t\t\t\t\t\tstruct extent_info *front)\n{\n\treturn __is_extent_mergeable(cur, front);\n}\n\nextern void f2fs_mark_inode_dirty_sync(struct inode *inode, bool sync);\nstatic inline void __try_update_largest_extent(struct inode *inode,\n\t\t\tstruct extent_tree *et, struct extent_node *en)\n{\n\tif (en->ei.len > et->largest.len) {\n\t\tet->largest = en->ei;\n\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t}\n}\n\nenum nid_list {\n\tFREE_NID_LIST,\n\tALLOC_NID_LIST,\n\tMAX_NID_LIST,\n};\n\nstruct f2fs_nm_info {\n\tblock_t nat_blkaddr;\t\t/* base disk address of NAT */\n\tnid_t max_nid;\t\t\t/* maximum possible node ids */\n\tnid_t available_nids;\t\t/* # of available node ids */\n\tnid_t next_scan_nid;\t\t/* the next nid to be scanned */\n\tunsigned int ram_thresh;\t/* control the memory footprint */\n\tunsigned int ra_nid_pages;\t/* # of nid pages to be readaheaded */\n\tunsigned int dirty_nats_ratio;\t/* control dirty nats ratio threshold */\n\n\t/* NAT cache management */\n\tstruct radix_tree_root nat_root;/* root of the nat entry cache */\n\tstruct radix_tree_root nat_set_root;/* root of the nat set cache */\n\tstruct rw_semaphore nat_tree_lock;\t/* protect nat_tree_lock */\n\tstruct list_head nat_entries;\t/* cached nat entry list (clean) */\n\tunsigned int nat_cnt;\t\t/* the # of cached nat entries */\n\tunsigned int dirty_nat_cnt;\t/* total num of nat entries in set */\n\tunsigned int nat_blocks;\t/* # of nat blocks */\n\n\t/* free node ids management */\n\tstruct radix_tree_root free_nid_root;/* root of the free_nid cache */\n\tstruct list_head nid_list[MAX_NID_LIST];/* lists for free nids */\n\tunsigned int nid_cnt[MAX_NID_LIST];\t/* the number of free node id */\n\tspinlock_t nid_list_lock;\t/* protect nid lists ops */\n\tstruct mutex build_lock;\t/* lock for build free nids */\n\tunsigned char (*free_nid_bitmap)[NAT_ENTRY_BITMAP_SIZE];\n\tunsigned char *nat_block_bitmap;\n\tunsigned short *free_nid_count;\t/* free nid count of NAT block */\n\n\t/* for checkpoint */\n\tchar *nat_bitmap;\t\t/* NAT bitmap pointer */\n\n\tunsigned int nat_bits_blocks;\t/* # of nat bits blocks */\n\tunsigned char *nat_bits;\t/* NAT bits blocks */\n\tunsigned char *full_nat_bits;\t/* full NAT pages */\n\tunsigned char *empty_nat_bits;\t/* empty NAT pages */\n#ifdef CONFIG_F2FS_CHECK_FS\n\tchar *nat_bitmap_mir;\t\t/* NAT bitmap mirror */\n#endif\n\tint bitmap_size;\t\t/* bitmap size */\n};\n\n/*\n * this structure is used as one of function parameters.\n * all the information are dedicated to a given direct node block determined\n * by the data offset in a file.\n */\nstruct dnode_of_data {\n\tstruct inode *inode;\t\t/* vfs inode pointer */\n\tstruct page *inode_page;\t/* its inode page, NULL is possible */\n\tstruct page *node_page;\t\t/* cached direct node page */\n\tnid_t nid;\t\t\t/* node id of the direct node block */\n\tunsigned int ofs_in_node;\t/* data offset in the node page */\n\tbool inode_page_locked;\t\t/* inode page is locked or not */\n\tbool node_changed;\t\t/* is node block changed */\n\tchar cur_level;\t\t\t/* level of hole node page */\n\tchar max_level;\t\t\t/* level of current page located */\n\tblock_t\tdata_blkaddr;\t\t/* block address of the node block */\n};\n\nstatic inline void set_new_dnode(struct dnode_of_data *dn, struct inode *inode,\n\t\tstruct page *ipage, struct page *npage, nid_t nid)\n{\n\tmemset(dn, 0, sizeof(*dn));\n\tdn->inode = inode;\n\tdn->inode_page = ipage;\n\tdn->node_page = npage;\n\tdn->nid = nid;\n}\n\n/*\n * For SIT manager\n *\n * By default, there are 6 active log areas across the whole main area.\n * When considering hot and cold data separation to reduce cleaning overhead,\n * we split 3 for data logs and 3 for node logs as hot, warm, and cold types,\n * respectively.\n * In the current design, you should not change the numbers intentionally.\n * Instead, as a mount option such as active_logs=x, you can use 2, 4, and 6\n * logs individually according to the underlying devices. (default: 6)\n * Just in case, on-disk layout covers maximum 16 logs that consist of 8 for\n * data and 8 for node logs.\n */\n#define\tNR_CURSEG_DATA_TYPE\t(3)\n#define NR_CURSEG_NODE_TYPE\t(3)\n#define NR_CURSEG_TYPE\t(NR_CURSEG_DATA_TYPE + NR_CURSEG_NODE_TYPE)\n\nenum {\n\tCURSEG_HOT_DATA\t= 0,\t/* directory entry blocks */\n\tCURSEG_WARM_DATA,\t/* data blocks */\n\tCURSEG_COLD_DATA,\t/* multimedia or GCed data blocks */\n\tCURSEG_HOT_NODE,\t/* direct node blocks of directory files */\n\tCURSEG_WARM_NODE,\t/* direct node blocks of normal files */\n\tCURSEG_COLD_NODE,\t/* indirect node blocks */\n\tNO_CHECK_TYPE,\n};\n\nstruct flush_cmd {\n\tstruct completion wait;\n\tstruct llist_node llnode;\n\tint ret;\n};\n\nstruct flush_cmd_control {\n\tstruct task_struct *f2fs_issue_flush;\t/* flush thread */\n\twait_queue_head_t flush_wait_queue;\t/* waiting queue for wake-up */\n\tatomic_t issued_flush;\t\t\t/* # of issued flushes */\n\tatomic_t issing_flush;\t\t\t/* # of issing flushes */\n\tstruct llist_head issue_list;\t\t/* list for command issue */\n\tstruct llist_node *dispatch_list;\t/* list for command dispatch */\n};\n\nstruct f2fs_sm_info {\n\tstruct sit_info *sit_info;\t\t/* whole segment information */\n\tstruct free_segmap_info *free_info;\t/* free segment information */\n\tstruct dirty_seglist_info *dirty_info;\t/* dirty segment information */\n\tstruct curseg_info *curseg_array;\t/* active segment information */\n\n\tblock_t seg0_blkaddr;\t\t/* block address of 0'th segment */\n\tblock_t main_blkaddr;\t\t/* start block address of main area */\n\tblock_t ssa_blkaddr;\t\t/* start block address of SSA area */\n\n\tunsigned int segment_count;\t/* total # of segments */\n\tunsigned int main_segments;\t/* # of segments in main area */\n\tunsigned int reserved_segments;\t/* # of reserved segments */\n\tunsigned int ovp_segments;\t/* # of overprovision segments */\n\n\t/* a threshold to reclaim prefree segments */\n\tunsigned int rec_prefree_segments;\n\n\t/* for batched trimming */\n\tunsigned int trim_sections;\t\t/* # of sections to trim */\n\n\tstruct list_head sit_entry_set;\t/* sit entry set list */\n\n\tunsigned int ipu_policy;\t/* in-place-update policy */\n\tunsigned int min_ipu_util;\t/* in-place-update threshold */\n\tunsigned int min_fsync_blocks;\t/* threshold for fsync */\n\tunsigned int min_hot_blocks;\t/* threshold for hot block allocation */\n\n\t/* for flush command control */\n\tstruct flush_cmd_control *fcc_info;\n\n\t/* for discard command control */\n\tstruct discard_cmd_control *dcc_info;\n};\n\n/*\n * For superblock\n */\n/*\n * COUNT_TYPE for monitoring\n *\n * f2fs monitors the number of several block types such as on-writeback,\n * dirty dentry blocks, dirty node blocks, and dirty meta blocks.\n */\n#define WB_DATA_TYPE(p)\t(__is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)\nenum count_type {\n\tF2FS_DIRTY_DENTS,\n\tF2FS_DIRTY_DATA,\n\tF2FS_DIRTY_NODES,\n\tF2FS_DIRTY_META,\n\tF2FS_INMEM_PAGES,\n\tF2FS_DIRTY_IMETA,\n\tF2FS_WB_CP_DATA,\n\tF2FS_WB_DATA,\n\tNR_COUNT_TYPE,\n};\n\n/*\n * The below are the page types of bios used in submit_bio().\n * The available types are:\n * DATA\t\t\tUser data pages. It operates as async mode.\n * NODE\t\t\tNode pages. It operates as async mode.\n * META\t\t\tFS metadata pages such as SIT, NAT, CP.\n * NR_PAGE_TYPE\t\tThe number of page types.\n * META_FLUSH\t\tMake sure the previous pages are written\n *\t\t\twith waiting the bio's completion\n * ...\t\t\tOnly can be used with META.\n */\n#define PAGE_TYPE_OF_BIO(type)\t((type) > META ? META : (type))\nenum page_type {\n\tDATA,\n\tNODE,\n\tMETA,\n\tNR_PAGE_TYPE,\n\tMETA_FLUSH,\n\tINMEM,\t\t/* the below types are used by tracepoints only. */\n\tINMEM_DROP,\n\tINMEM_INVALIDATE,\n\tINMEM_REVOKE,\n\tIPU,\n\tOPU,\n};\n\nenum temp_type {\n\tHOT = 0,\t/* must be zero for meta bio */\n\tWARM,\n\tCOLD,\n\tNR_TEMP_TYPE,\n};\n\nenum need_lock_type {\n\tLOCK_REQ = 0,\n\tLOCK_DONE,\n\tLOCK_RETRY,\n};\n\nenum iostat_type {\n\tAPP_DIRECT_IO,\t\t\t/* app direct IOs */\n\tAPP_BUFFERED_IO,\t\t/* app buffered IOs */\n\tAPP_WRITE_IO,\t\t\t/* app write IOs */\n\tAPP_MAPPED_IO,\t\t\t/* app mapped IOs */\n\tFS_DATA_IO,\t\t\t/* data IOs from kworker/fsync/reclaimer */\n\tFS_NODE_IO,\t\t\t/* node IOs from kworker/fsync/reclaimer */\n\tFS_META_IO,\t\t\t/* meta IOs from kworker/reclaimer */\n\tFS_GC_DATA_IO,\t\t\t/* data IOs from forground gc */\n\tFS_GC_NODE_IO,\t\t\t/* node IOs from forground gc */\n\tFS_CP_DATA_IO,\t\t\t/* data IOs from checkpoint */\n\tFS_CP_NODE_IO,\t\t\t/* node IOs from checkpoint */\n\tFS_CP_META_IO,\t\t\t/* meta IOs from checkpoint */\n\tFS_DISCARD,\t\t\t/* discard */\n\tNR_IO_TYPE,\n};\n\nstruct f2fs_io_info {\n\tstruct f2fs_sb_info *sbi;\t/* f2fs_sb_info pointer */\n\tenum page_type type;\t/* contains DATA/NODE/META/META_FLUSH */\n\tenum temp_type temp;\t/* contains HOT/WARM/COLD */\n\tint op;\t\t\t/* contains REQ_OP_ */\n\tint op_flags;\t\t/* req_flag_bits */\n\tblock_t new_blkaddr;\t/* new block address to be written */\n\tblock_t old_blkaddr;\t/* old block address before Cow */\n\tstruct page *page;\t/* page to be written */\n\tstruct page *encrypted_page;\t/* encrypted page */\n\tstruct list_head list;\t\t/* serialize IOs */\n\tbool submitted;\t\t/* indicate IO submission */\n\tint need_lock;\t\t/* indicate we need to lock cp_rwsem */\n\tbool in_list;\t\t/* indicate fio is in io_list */\n\tenum iostat_type io_type;\t/* io type */\n};\n\n#define is_read_io(rw) ((rw) == READ)\nstruct f2fs_bio_info {\n\tstruct f2fs_sb_info *sbi;\t/* f2fs superblock */\n\tstruct bio *bio;\t\t/* bios to merge */\n\tsector_t last_block_in_bio;\t/* last block number */\n\tstruct f2fs_io_info fio;\t/* store buffered io info. */\n\tstruct rw_semaphore io_rwsem;\t/* blocking op for bio */\n\tspinlock_t io_lock;\t\t/* serialize DATA/NODE IOs */\n\tstruct list_head io_list;\t/* track fios */\n};\n\n#define FDEV(i)\t\t\t\t(sbi->devs[i])\n#define RDEV(i)\t\t\t\t(raw_super->devs[i])\nstruct f2fs_dev_info {\n\tstruct block_device *bdev;\n\tchar path[MAX_PATH_LEN];\n\tunsigned int total_segments;\n\tblock_t start_blk;\n\tblock_t end_blk;\n#ifdef CONFIG_BLK_DEV_ZONED\n\tunsigned int nr_blkz;\t\t\t/* Total number of zones */\n\tu8 *blkz_type;\t\t\t\t/* Array of zones type */\n#endif\n};\n\nenum inode_type {\n\tDIR_INODE,\t\t\t/* for dirty dir inode */\n\tFILE_INODE,\t\t\t/* for dirty regular/symlink inode */\n\tDIRTY_META,\t\t\t/* for all dirtied inode metadata */\n\tNR_INODE_TYPE,\n};\n\n/* for inner inode cache management */\nstruct inode_management {\n\tstruct radix_tree_root ino_root;\t/* ino entry array */\n\tspinlock_t ino_lock;\t\t\t/* for ino entry lock */\n\tstruct list_head ino_list;\t\t/* inode list head */\n\tunsigned long ino_num;\t\t\t/* number of entries */\n};\n\n/* For s_flag in struct f2fs_sb_info */\nenum {\n\tSBI_IS_DIRTY,\t\t\t\t/* dirty flag for checkpoint */\n\tSBI_IS_CLOSE,\t\t\t\t/* specify unmounting */\n\tSBI_NEED_FSCK,\t\t\t\t/* need fsck.f2fs to fix */\n\tSBI_POR_DOING,\t\t\t\t/* recovery is doing or not */\n\tSBI_NEED_SB_WRITE,\t\t\t/* need to recover superblock */\n\tSBI_NEED_CP,\t\t\t\t/* need to checkpoint */\n};\n\nenum {\n\tCP_TIME,\n\tREQ_TIME,\n\tMAX_TIME,\n};\n\nstruct f2fs_sb_info {\n\tstruct super_block *sb;\t\t\t/* pointer to VFS super block */\n\tstruct proc_dir_entry *s_proc;\t\t/* proc entry */\n\tstruct f2fs_super_block *raw_super;\t/* raw super block pointer */\n\tint valid_super_block;\t\t\t/* valid super block no */\n\tunsigned long s_flag;\t\t\t\t/* flags for sbi */\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\tunsigned int blocks_per_blkz;\t\t/* F2FS blocks per zone */\n\tunsigned int log_blocks_per_blkz;\t/* log2 F2FS blocks per zone */\n#endif\n\n\t/* for node-related operations */\n\tstruct f2fs_nm_info *nm_info;\t\t/* node manager */\n\tstruct inode *node_inode;\t\t/* cache node blocks */\n\n\t/* for segment-related operations */\n\tstruct f2fs_sm_info *sm_info;\t\t/* segment manager */\n\n\t/* for bio operations */\n\tstruct f2fs_bio_info *write_io[NR_PAGE_TYPE];\t/* for write bios */\n\tstruct mutex wio_mutex[NR_PAGE_TYPE - 1][NR_TEMP_TYPE];\n\t\t\t\t\t\t/* bio ordering for NODE/DATA */\n\tint write_io_size_bits;\t\t\t/* Write IO size bits */\n\tmempool_t *write_io_dummy;\t\t/* Dummy pages */\n\n\t/* for checkpoint */\n\tstruct f2fs_checkpoint *ckpt;\t\t/* raw checkpoint pointer */\n\tint cur_cp_pack;\t\t\t/* remain current cp pack */\n\tspinlock_t cp_lock;\t\t\t/* for flag in ckpt */\n\tstruct inode *meta_inode;\t\t/* cache meta blocks */\n\tstruct mutex cp_mutex;\t\t\t/* checkpoint procedure lock */\n\tstruct rw_semaphore cp_rwsem;\t\t/* blocking FS operations */\n\tstruct rw_semaphore node_write;\t\t/* locking node writes */\n\tstruct rw_semaphore node_change;\t/* locking node change */\n\twait_queue_head_t cp_wait;\n\tunsigned long last_time[MAX_TIME];\t/* to store time in jiffies */\n\tlong interval_time[MAX_TIME];\t\t/* to store thresholds */\n\n\tstruct inode_management im[MAX_INO_ENTRY];      /* manage inode cache */\n\n\t/* for orphan inode, use 0'th array */\n\tunsigned int max_orphans;\t\t/* max orphan inodes */\n\n\t/* for inode management */\n\tstruct list_head inode_list[NR_INODE_TYPE];\t/* dirty inode list */\n\tspinlock_t inode_lock[NR_INODE_TYPE];\t/* for dirty inode list lock */\n\n\t/* for extent tree cache */\n\tstruct radix_tree_root extent_tree_root;/* cache extent cache entries */\n\tstruct mutex extent_tree_lock;\t/* locking extent radix tree */\n\tstruct list_head extent_list;\t\t/* lru list for shrinker */\n\tspinlock_t extent_lock;\t\t\t/* locking extent lru list */\n\tatomic_t total_ext_tree;\t\t/* extent tree count */\n\tstruct list_head zombie_list;\t\t/* extent zombie tree list */\n\tatomic_t total_zombie_tree;\t\t/* extent zombie tree count */\n\tatomic_t total_ext_node;\t\t/* extent info count */\n\n\t/* basic filesystem units */\n\tunsigned int log_sectors_per_block;\t/* log2 sectors per block */\n\tunsigned int log_blocksize;\t\t/* log2 block size */\n\tunsigned int blocksize;\t\t\t/* block size */\n\tunsigned int root_ino_num;\t\t/* root inode number*/\n\tunsigned int node_ino_num;\t\t/* node inode number*/\n\tunsigned int meta_ino_num;\t\t/* meta inode number*/\n\tunsigned int log_blocks_per_seg;\t/* log2 blocks per segment */\n\tunsigned int blocks_per_seg;\t\t/* blocks per segment */\n\tunsigned int segs_per_sec;\t\t/* segments per section */\n\tunsigned int secs_per_zone;\t\t/* sections per zone */\n\tunsigned int total_sections;\t\t/* total section count */\n\tunsigned int total_node_count;\t\t/* total node block count */\n\tunsigned int total_valid_node_count;\t/* valid node block count */\n\tloff_t max_file_blocks;\t\t\t/* max block index of file */\n\tint active_logs;\t\t\t/* # of active logs */\n\tint dir_level;\t\t\t\t/* directory level */\n\n\tblock_t user_block_count;\t\t/* # of user blocks */\n\tblock_t total_valid_block_count;\t/* # of valid blocks */\n\tblock_t discard_blks;\t\t\t/* discard command candidats */\n\tblock_t last_valid_block_count;\t\t/* for recovery */\n\tblock_t reserved_blocks;\t\t/* configurable reserved blocks */\n\n\tu32 s_next_generation;\t\t\t/* for NFS support */\n\n\t/* # of pages, see count_type */\n\tatomic_t nr_pages[NR_COUNT_TYPE];\n\t/* # of allocated blocks */\n\tstruct percpu_counter alloc_valid_block_count;\n\n\t/* writeback control */\n\tatomic_t wb_sync_req;\t\t\t/* count # of WB_SYNC threads */\n\n\t/* valid inode count */\n\tstruct percpu_counter total_valid_inode_count;\n\n\tstruct f2fs_mount_info mount_opt;\t/* mount options */\n\n\t/* for cleaning operations */\n\tstruct mutex gc_mutex;\t\t\t/* mutex for GC */\n\tstruct f2fs_gc_kthread\t*gc_thread;\t/* GC thread */\n\tunsigned int cur_victim_sec;\t\t/* current victim section num */\n\n\t/* threshold for converting bg victims for fg */\n\tu64 fggc_threshold;\n\n\t/* maximum # of trials to find a victim segment for SSR and GC */\n\tunsigned int max_victim_search;\n\n\t/*\n\t * for stat information.\n\t * one is for the LFS mode, and the other is for the SSR mode.\n\t */\n#ifdef CONFIG_F2FS_STAT_FS\n\tstruct f2fs_stat_info *stat_info;\t/* FS status information */\n\tunsigned int segment_count[2];\t\t/* # of allocated segments */\n\tunsigned int block_count[2];\t\t/* # of allocated blocks */\n\tatomic_t inplace_count;\t\t/* # of inplace update */\n\tatomic64_t total_hit_ext;\t\t/* # of lookup extent cache */\n\tatomic64_t read_hit_rbtree;\t\t/* # of hit rbtree extent node */\n\tatomic64_t read_hit_largest;\t\t/* # of hit largest extent node */\n\tatomic64_t read_hit_cached;\t\t/* # of hit cached extent node */\n\tatomic_t inline_xattr;\t\t\t/* # of inline_xattr inodes */\n\tatomic_t inline_inode;\t\t\t/* # of inline_data inodes */\n\tatomic_t inline_dir;\t\t\t/* # of inline_dentry inodes */\n\tatomic_t aw_cnt;\t\t\t/* # of atomic writes */\n\tatomic_t vw_cnt;\t\t\t/* # of volatile writes */\n\tatomic_t max_aw_cnt;\t\t\t/* max # of atomic writes */\n\tatomic_t max_vw_cnt;\t\t\t/* max # of volatile writes */\n\tint bg_gc;\t\t\t\t/* background gc calls */\n\tunsigned int ndirty_inode[NR_INODE_TYPE];\t/* # of dirty inodes */\n#endif\n\tspinlock_t stat_lock;\t\t\t/* lock for stat operations */\n\n\t/* For app/fs IO statistics */\n\tspinlock_t iostat_lock;\n\tunsigned long long write_iostat[NR_IO_TYPE];\n\tbool iostat_enable;\n\n\t/* For sysfs suppport */\n\tstruct kobject s_kobj;\n\tstruct completion s_kobj_unregister;\n\n\t/* For shrinker support */\n\tstruct list_head s_list;\n\tint s_ndevs;\t\t\t\t/* number of devices */\n\tstruct f2fs_dev_info *devs;\t\t/* for device list */\n\tstruct mutex umount_mutex;\n\tunsigned int shrinker_run_no;\n\n\t/* For write statistics */\n\tu64 sectors_written_start;\n\tu64 kbytes_written;\n\n\t/* Reference to checksum algorithm driver via cryptoapi */\n\tstruct crypto_shash *s_chksum_driver;\n\n\t/* Precomputed FS UUID checksum for seeding other checksums */\n\t__u32 s_chksum_seed;\n\n\t/* For fault injection */\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct f2fs_fault_info fault_info;\n#endif\n\n#ifdef CONFIG_QUOTA\n\t/* Names of quota files with journalled quota */\n\tchar *s_qf_names[MAXQUOTAS];\n\tint s_jquota_fmt;\t\t\t/* Format of quota to use */\n#endif\n};\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n#define f2fs_show_injection_info(type)\t\t\t\t\\\n\tprintk(\"%sF2FS-fs : inject %s in %s of %pF\\n\",\t\t\\\n\t\tKERN_INFO, fault_name[type],\t\t\t\\\n\t\t__func__, __builtin_return_address(0))\nstatic inline bool time_to_inject(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_fault_info *ffi = &sbi->fault_info;\n\n\tif (!ffi->inject_rate)\n\t\treturn false;\n\n\tif (!IS_FAULT_SET(ffi, type))\n\t\treturn false;\n\n\tatomic_inc(&ffi->inject_ops);\n\tif (atomic_read(&ffi->inject_ops) >= ffi->inject_rate) {\n\t\tatomic_set(&ffi->inject_ops, 0);\n\t\treturn true;\n\t}\n\treturn false;\n}\n#endif\n\n/* For write statistics. Suppose sector size is 512 bytes,\n * and the return value is in kbytes. s is of struct f2fs_sb_info.\n */\n#define BD_PART_WRITTEN(s)\t\t\t\t\t\t \\\n(((u64)part_stat_read((s)->sb->s_bdev->bd_part, sectors[1]) -\t\t \\\n\t\t(s)->sectors_written_start) >> 1)\n\nstatic inline void f2fs_update_time(struct f2fs_sb_info *sbi, int type)\n{\n\tsbi->last_time[type] = jiffies;\n}\n\nstatic inline bool f2fs_time_over(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct timespec ts = {sbi->interval_time[type], 0};\n\tunsigned long interval = timespec_to_jiffies(&ts);\n\n\treturn time_after(jiffies, sbi->last_time[type] + interval);\n}\n\nstatic inline bool is_idle(struct f2fs_sb_info *sbi)\n{\n\tstruct block_device *bdev = sbi->sb->s_bdev;\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\tstruct request_list *rl = &q->root_rl;\n\n\tif (rl->count[BLK_RW_SYNC] || rl->count[BLK_RW_ASYNC])\n\t\treturn 0;\n\n\treturn f2fs_time_over(sbi, REQ_TIME);\n}\n\n/*\n * Inline functions\n */\nstatic inline u32 f2fs_crc32(struct f2fs_sb_info *sbi, const void *address,\n\t\t\t   unsigned int length)\n{\n\tSHASH_DESC_ON_STACK(shash, sbi->s_chksum_driver);\n\tu32 *ctx = (u32 *)shash_desc_ctx(shash);\n\tu32 retval;\n\tint err;\n\n\tshash->tfm = sbi->s_chksum_driver;\n\tshash->flags = 0;\n\t*ctx = F2FS_SUPER_MAGIC;\n\n\terr = crypto_shash_update(shash, address, length);\n\tBUG_ON(err);\n\n\tretval = *ctx;\n\tbarrier_data(ctx);\n\treturn retval;\n}\n\nstatic inline bool f2fs_crc_valid(struct f2fs_sb_info *sbi, __u32 blk_crc,\n\t\t\t\t  void *buf, size_t buf_size)\n{\n\treturn f2fs_crc32(sbi, buf, buf_size) == blk_crc;\n}\n\nstatic inline u32 f2fs_chksum(struct f2fs_sb_info *sbi, u32 crc,\n\t\t\t      const void *address, unsigned int length)\n{\n\tstruct {\n\t\tstruct shash_desc shash;\n\t\tchar ctx[4];\n\t} desc;\n\tint err;\n\n\tBUG_ON(crypto_shash_descsize(sbi->s_chksum_driver) != sizeof(desc.ctx));\n\n\tdesc.shash.tfm = sbi->s_chksum_driver;\n\tdesc.shash.flags = 0;\n\t*(u32 *)desc.ctx = crc;\n\n\terr = crypto_shash_update(&desc.shash, address, length);\n\tBUG_ON(err);\n\n\treturn *(u32 *)desc.ctx;\n}\n\nstatic inline struct f2fs_inode_info *F2FS_I(struct inode *inode)\n{\n\treturn container_of(inode, struct f2fs_inode_info, vfs_inode);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\nstatic inline struct f2fs_sb_info *F2FS_I_SB(struct inode *inode)\n{\n\treturn F2FS_SB(inode->i_sb);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_M_SB(struct address_space *mapping)\n{\n\treturn F2FS_I_SB(mapping->host);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_P_SB(struct page *page)\n{\n\treturn F2FS_M_SB(page->mapping);\n}\n\nstatic inline struct f2fs_super_block *F2FS_RAW_SUPER(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_super_block *)(sbi->raw_super);\n}\n\nstatic inline struct f2fs_checkpoint *F2FS_CKPT(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_checkpoint *)(sbi->ckpt);\n}\n\nstatic inline struct f2fs_node *F2FS_NODE(struct page *page)\n{\n\treturn (struct f2fs_node *)page_address(page);\n}\n\nstatic inline struct f2fs_inode *F2FS_INODE(struct page *page)\n{\n\treturn &((struct f2fs_node *)page_address(page))->i;\n}\n\nstatic inline struct f2fs_nm_info *NM_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_nm_info *)(sbi->nm_info);\n}\n\nstatic inline struct f2fs_sm_info *SM_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_sm_info *)(sbi->sm_info);\n}\n\nstatic inline struct sit_info *SIT_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct sit_info *)(SM_I(sbi)->sit_info);\n}\n\nstatic inline struct free_segmap_info *FREE_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct free_segmap_info *)(SM_I(sbi)->free_info);\n}\n\nstatic inline struct dirty_seglist_info *DIRTY_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct dirty_seglist_info *)(SM_I(sbi)->dirty_info);\n}\n\nstatic inline struct address_space *META_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->meta_inode->i_mapping;\n}\n\nstatic inline struct address_space *NODE_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->node_inode->i_mapping;\n}\n\nstatic inline bool is_sbi_flag_set(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\treturn test_bit(type, &sbi->s_flag);\n}\n\nstatic inline void set_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\tset_bit(type, &sbi->s_flag);\n}\n\nstatic inline void clear_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\tclear_bit(type, &sbi->s_flag);\n}\n\nstatic inline unsigned long long cur_cp_version(struct f2fs_checkpoint *cp)\n{\n\treturn le64_to_cpu(cp->checkpoint_ver);\n}\n\nstatic inline __u64 cur_cp_crc(struct f2fs_checkpoint *cp)\n{\n\tsize_t crc_offset = le32_to_cpu(cp->checksum_offset);\n\treturn le32_to_cpu(*((__le32 *)((unsigned char *)cp + crc_offset)));\n}\n\nstatic inline bool __is_set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\n\treturn ckpt_flags & f;\n}\n\nstatic inline bool is_set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\treturn __is_set_ckpt_flags(F2FS_CKPT(sbi), f);\n}\n\nstatic inline void __set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags;\n\n\tckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\tckpt_flags |= f;\n\tcp->ckpt_flags = cpu_to_le32(ckpt_flags);\n}\n\nstatic inline void set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__set_ckpt_flags(F2FS_CKPT(sbi), f);\n\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline void __clear_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags;\n\n\tckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\tckpt_flags &= (~f);\n\tcp->ckpt_flags = cpu_to_le32(ckpt_flags);\n}\n\nstatic inline void clear_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__clear_ckpt_flags(F2FS_CKPT(sbi), f);\n\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline void disable_nat_bits(struct f2fs_sb_info *sbi, bool lock)\n{\n\tunsigned long flags;\n\n\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\tif (lock)\n\t\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__clear_ckpt_flags(F2FS_CKPT(sbi), CP_NAT_BITS_FLAG);\n\tkfree(NM_I(sbi)->nat_bits);\n\tNM_I(sbi)->nat_bits = NULL;\n\tif (lock)\n\t\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline bool enabled_nat_bits(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct cp_control *cpc)\n{\n\tbool set = is_set_ckpt_flags(sbi, CP_NAT_BITS_FLAG);\n\n\treturn (cpc) ? (cpc->reason & CP_UMOUNT) && set : set;\n}\n\nstatic inline void f2fs_lock_op(struct f2fs_sb_info *sbi)\n{\n\tdown_read(&sbi->cp_rwsem);\n}\n\nstatic inline int f2fs_trylock_op(struct f2fs_sb_info *sbi)\n{\n\treturn down_read_trylock(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_unlock_op(struct f2fs_sb_info *sbi)\n{\n\tup_read(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_lock_all(struct f2fs_sb_info *sbi)\n{\n\tdown_write(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_unlock_all(struct f2fs_sb_info *sbi)\n{\n\tup_write(&sbi->cp_rwsem);\n}\n\nstatic inline int __get_cp_reason(struct f2fs_sb_info *sbi)\n{\n\tint reason = CP_SYNC;\n\n\tif (test_opt(sbi, FASTBOOT))\n\t\treason = CP_FASTBOOT;\n\tif (is_sbi_flag_set(sbi, SBI_IS_CLOSE))\n\t\treason = CP_UMOUNT;\n\treturn reason;\n}\n\nstatic inline bool __remain_node_summaries(int reason)\n{\n\treturn (reason & (CP_UMOUNT | CP_FASTBOOT));\n}\n\nstatic inline bool __exist_node_summaries(struct f2fs_sb_info *sbi)\n{\n\treturn (is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG) ||\n\t\t\tis_set_ckpt_flags(sbi, CP_FASTBOOT_FLAG));\n}\n\n/*\n * Check whether the given nid is within node id range.\n */\nstatic inline int check_nid_range(struct f2fs_sb_info *sbi, nid_t nid)\n{\n\tif (unlikely(nid < F2FS_ROOT_INO(sbi)))\n\t\treturn -EINVAL;\n\tif (unlikely(nid >= NM_I(sbi)->max_nid))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\n/*\n * Check whether the inode has blocks or not\n */\nstatic inline int F2FS_HAS_BLOCKS(struct inode *inode)\n{\n\tblock_t xattr_block = F2FS_I(inode)->i_xattr_nid ? 1 : 0;\n\n\treturn (inode->i_blocks >> F2FS_LOG_SECTORS_PER_BLOCK) > xattr_block;\n}\n\nstatic inline bool f2fs_has_xattr_block(unsigned int ofs)\n{\n\treturn ofs == XATTR_NODE_OFFSET;\n}\n\nstatic inline void f2fs_i_blocks_write(struct inode *, block_t, bool, bool);\nstatic inline int inc_valid_block_count(struct f2fs_sb_info *sbi,\n\t\t\t\t struct inode *inode, blkcnt_t *count)\n{\n\tblkcnt_t diff = 0, release = 0;\n\tblock_t avail_user_block_count;\n\tint ret;\n\n\tret = dquot_reserve_block(inode, *count);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(sbi, FAULT_BLOCK)) {\n\t\tf2fs_show_injection_info(FAULT_BLOCK);\n\t\trelease = *count;\n\t\tgoto enospc;\n\t}\n#endif\n\t/*\n\t * let's increase this in prior to actual block count change in order\n\t * for f2fs_sync_file to avoid data races when deciding checkpoint.\n\t */\n\tpercpu_counter_add(&sbi->alloc_valid_block_count, (*count));\n\n\tspin_lock(&sbi->stat_lock);\n\tsbi->total_valid_block_count += (block_t)(*count);\n\tavail_user_block_count = sbi->user_block_count - sbi->reserved_blocks;\n\tif (unlikely(sbi->total_valid_block_count > avail_user_block_count)) {\n\t\tdiff = sbi->total_valid_block_count - avail_user_block_count;\n\t\t*count -= diff;\n\t\trelease = diff;\n\t\tsbi->total_valid_block_count = avail_user_block_count;\n\t\tif (!*count) {\n\t\t\tspin_unlock(&sbi->stat_lock);\n\t\t\tpercpu_counter_sub(&sbi->alloc_valid_block_count, diff);\n\t\t\tgoto enospc;\n\t\t}\n\t}\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (release)\n\t\tdquot_release_reservation_block(inode, release);\n\tf2fs_i_blocks_write(inode, *count, true, true);\n\treturn 0;\n\nenospc:\n\tdquot_release_reservation_block(inode, release);\n\treturn -ENOSPC;\n}\n\nstatic inline void dec_valid_block_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tblock_t count)\n{\n\tblkcnt_t sectors = count << F2FS_LOG_SECTORS_PER_BLOCK;\n\n\tspin_lock(&sbi->stat_lock);\n\tf2fs_bug_on(sbi, sbi->total_valid_block_count < (block_t) count);\n\tf2fs_bug_on(sbi, inode->i_blocks < sectors);\n\tsbi->total_valid_block_count -= (block_t)count;\n\tspin_unlock(&sbi->stat_lock);\n\tf2fs_i_blocks_write(inode, count, false, true);\n}\n\nstatic inline void inc_page_count(struct f2fs_sb_info *sbi, int count_type)\n{\n\tatomic_inc(&sbi->nr_pages[count_type]);\n\n\tif (count_type == F2FS_DIRTY_DATA || count_type == F2FS_INMEM_PAGES ||\n\t\tcount_type == F2FS_WB_CP_DATA || count_type == F2FS_WB_DATA)\n\t\treturn;\n\n\tset_sbi_flag(sbi, SBI_IS_DIRTY);\n}\n\nstatic inline void inode_inc_dirty_pages(struct inode *inode)\n{\n\tatomic_inc(&F2FS_I(inode)->dirty_pages);\n\tinc_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?\n\t\t\t\tF2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);\n}\n\nstatic inline void dec_page_count(struct f2fs_sb_info *sbi, int count_type)\n{\n\tatomic_dec(&sbi->nr_pages[count_type]);\n}\n\nstatic inline void inode_dec_dirty_pages(struct inode *inode)\n{\n\tif (!S_ISDIR(inode->i_mode) && !S_ISREG(inode->i_mode) &&\n\t\t\t!S_ISLNK(inode->i_mode))\n\t\treturn;\n\n\tatomic_dec(&F2FS_I(inode)->dirty_pages);\n\tdec_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?\n\t\t\t\tF2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);\n}\n\nstatic inline s64 get_pages(struct f2fs_sb_info *sbi, int count_type)\n{\n\treturn atomic_read(&sbi->nr_pages[count_type]);\n}\n\nstatic inline int get_dirty_pages(struct inode *inode)\n{\n\treturn atomic_read(&F2FS_I(inode)->dirty_pages);\n}\n\nstatic inline int get_blocktype_secs(struct f2fs_sb_info *sbi, int block_type)\n{\n\tunsigned int pages_per_sec = sbi->segs_per_sec * sbi->blocks_per_seg;\n\tunsigned int segs = (get_pages(sbi, block_type) + pages_per_sec - 1) >>\n\t\t\t\t\t\tsbi->log_blocks_per_seg;\n\n\treturn segs / sbi->segs_per_sec;\n}\n\nstatic inline block_t valid_user_blocks(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->total_valid_block_count;\n}\n\nstatic inline block_t discard_blocks(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->discard_blks;\n}\n\nstatic inline unsigned long __bitmap_size(struct f2fs_sb_info *sbi, int flag)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\n\t/* return NAT or SIT bitmap */\n\tif (flag == NAT_BITMAP)\n\t\treturn le32_to_cpu(ckpt->nat_ver_bitmap_bytesize);\n\telse if (flag == SIT_BITMAP)\n\t\treturn le32_to_cpu(ckpt->sit_ver_bitmap_bytesize);\n\n\treturn 0;\n}\n\nstatic inline block_t __cp_payload(struct f2fs_sb_info *sbi)\n{\n\treturn le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_payload);\n}\n\nstatic inline void *__bitmap_ptr(struct f2fs_sb_info *sbi, int flag)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tint offset;\n\n\tif (__cp_payload(sbi) > 0) {\n\t\tif (flag == NAT_BITMAP)\n\t\t\treturn &ckpt->sit_nat_version_bitmap;\n\t\telse\n\t\t\treturn (unsigned char *)ckpt + F2FS_BLKSIZE;\n\t} else {\n\t\toffset = (flag == NAT_BITMAP) ?\n\t\t\tle32_to_cpu(ckpt->sit_ver_bitmap_bytesize) : 0;\n\t\treturn &ckpt->sit_nat_version_bitmap + offset;\n\t}\n}\n\nstatic inline block_t __start_cp_addr(struct f2fs_sb_info *sbi)\n{\n\tblock_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);\n\n\tif (sbi->cur_cp_pack == 2)\n\t\tstart_addr += sbi->blocks_per_seg;\n\treturn start_addr;\n}\n\nstatic inline block_t __start_cp_next_addr(struct f2fs_sb_info *sbi)\n{\n\tblock_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);\n\n\tif (sbi->cur_cp_pack == 1)\n\t\tstart_addr += sbi->blocks_per_seg;\n\treturn start_addr;\n}\n\nstatic inline void __set_cp_next_pack(struct f2fs_sb_info *sbi)\n{\n\tsbi->cur_cp_pack = (sbi->cur_cp_pack == 1) ? 2 : 1;\n}\n\nstatic inline block_t __start_sum_addr(struct f2fs_sb_info *sbi)\n{\n\treturn le32_to_cpu(F2FS_CKPT(sbi)->cp_pack_start_sum);\n}\n\nstatic inline int inc_valid_node_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool is_inode)\n{\n\tblock_t\tvalid_block_count;\n\tunsigned int valid_node_count;\n\tbool quota = inode && !is_inode;\n\n\tif (quota) {\n\t\tint ret = dquot_reserve_block(inode, 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tspin_lock(&sbi->stat_lock);\n\n\tvalid_block_count = sbi->total_valid_block_count + 1;\n\tif (unlikely(valid_block_count + sbi->reserved_blocks >\n\t\t\t\t\t\tsbi->user_block_count)) {\n\t\tspin_unlock(&sbi->stat_lock);\n\t\tgoto enospc;\n\t}\n\n\tvalid_node_count = sbi->total_valid_node_count + 1;\n\tif (unlikely(valid_node_count > sbi->total_node_count)) {\n\t\tspin_unlock(&sbi->stat_lock);\n\t\tgoto enospc;\n\t}\n\n\tsbi->total_valid_node_count++;\n\tsbi->total_valid_block_count++;\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (inode) {\n\t\tif (is_inode)\n\t\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t\telse\n\t\t\tf2fs_i_blocks_write(inode, 1, true, true);\n\t}\n\n\tpercpu_counter_inc(&sbi->alloc_valid_block_count);\n\treturn 0;\n\nenospc:\n\tif (quota)\n\t\tdquot_release_reservation_block(inode, 1);\n\treturn -ENOSPC;\n}\n\nstatic inline void dec_valid_node_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool is_inode)\n{\n\tspin_lock(&sbi->stat_lock);\n\n\tf2fs_bug_on(sbi, !sbi->total_valid_block_count);\n\tf2fs_bug_on(sbi, !sbi->total_valid_node_count);\n\tf2fs_bug_on(sbi, !is_inode && !inode->i_blocks);\n\n\tsbi->total_valid_node_count--;\n\tsbi->total_valid_block_count--;\n\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (!is_inode)\n\t\tf2fs_i_blocks_write(inode, 1, false, true);\n}\n\nstatic inline unsigned int valid_node_count(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->total_valid_node_count;\n}\n\nstatic inline void inc_valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_inc(&sbi->total_valid_inode_count);\n}\n\nstatic inline void dec_valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_dec(&sbi->total_valid_inode_count);\n}\n\nstatic inline s64 valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\treturn percpu_counter_sum_positive(&sbi->total_valid_inode_count);\n}\n\nstatic inline struct page *f2fs_grab_cache_page(struct address_space *mapping,\n\t\t\t\t\t\tpgoff_t index, bool for_write)\n{\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct page *page = find_lock_page(mapping, index);\n\n\tif (page)\n\t\treturn page;\n\n\tif (time_to_inject(F2FS_M_SB(mapping), FAULT_PAGE_ALLOC)) {\n\t\tf2fs_show_injection_info(FAULT_PAGE_ALLOC);\n\t\treturn NULL;\n\t}\n#endif\n\tif (!for_write)\n\t\treturn grab_cache_page(mapping, index);\n\treturn grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);\n}\n\nstatic inline void f2fs_copy_page(struct page *src, struct page *dst)\n{\n\tchar *src_kaddr = kmap(src);\n\tchar *dst_kaddr = kmap(dst);\n\n\tmemcpy(dst_kaddr, src_kaddr, PAGE_SIZE);\n\tkunmap(dst);\n\tkunmap(src);\n}\n\nstatic inline void f2fs_put_page(struct page *page, int unlock)\n{\n\tif (!page)\n\t\treturn;\n\n\tif (unlock) {\n\t\tf2fs_bug_on(F2FS_P_SB(page), !PageLocked(page));\n\t\tunlock_page(page);\n\t}\n\tput_page(page);\n}\n\nstatic inline void f2fs_put_dnode(struct dnode_of_data *dn)\n{\n\tif (dn->node_page)\n\t\tf2fs_put_page(dn->node_page, 1);\n\tif (dn->inode_page && dn->node_page != dn->inode_page)\n\t\tf2fs_put_page(dn->inode_page, 0);\n\tdn->node_page = NULL;\n\tdn->inode_page = NULL;\n}\n\nstatic inline struct kmem_cache *f2fs_kmem_cache_create(const char *name,\n\t\t\t\t\tsize_t size)\n{\n\treturn kmem_cache_create(name, size, 0, SLAB_RECLAIM_ACCOUNT, NULL);\n}\n\nstatic inline void *f2fs_kmem_cache_alloc(struct kmem_cache *cachep,\n\t\t\t\t\t\tgfp_t flags)\n{\n\tvoid *entry;\n\n\tentry = kmem_cache_alloc(cachep, flags);\n\tif (!entry)\n\t\tentry = kmem_cache_alloc(cachep, flags | __GFP_NOFAIL);\n\treturn entry;\n}\n\nstatic inline struct bio *f2fs_bio_alloc(int npages)\n{\n\tstruct bio *bio;\n\n\t/* No failure on bio allocation */\n\tbio = bio_alloc(GFP_NOIO, npages);\n\tif (!bio)\n\t\tbio = bio_alloc(GFP_NOIO | __GFP_NOFAIL, npages);\n\treturn bio;\n}\n\nstatic inline void f2fs_radix_tree_insert(struct radix_tree_root *root,\n\t\t\t\tunsigned long index, void *item)\n{\n\twhile (radix_tree_insert(root, index, item))\n\t\tcond_resched();\n}\n\n#define RAW_IS_INODE(p)\t((p)->footer.nid == (p)->footer.ino)\n\nstatic inline bool IS_INODE(struct page *page)\n{\n\tstruct f2fs_node *p = F2FS_NODE(page);\n\n\treturn RAW_IS_INODE(p);\n}\n\nstatic inline int offset_in_addr(struct f2fs_inode *i)\n{\n\treturn (i->i_inline & F2FS_EXTRA_ATTR) ?\n\t\t\t(le16_to_cpu(i->i_extra_isize) / sizeof(__le32)) : 0;\n}\n\nstatic inline __le32 *blkaddr_in_node(struct f2fs_node *node)\n{\n\treturn RAW_IS_INODE(node) ? node->i.i_addr : node->dn.addr;\n}\n\nstatic inline int f2fs_has_extra_attr(struct inode *inode);\nstatic inline block_t datablock_addr(struct inode *inode,\n\t\t\tstruct page *node_page, unsigned int offset)\n{\n\tstruct f2fs_node *raw_node;\n\t__le32 *addr_array;\n\tint base = 0;\n\tbool is_inode = IS_INODE(node_page);\n\n\traw_node = F2FS_NODE(node_page);\n\n\t/* from GC path only */\n\tif (!inode) {\n\t\tif (is_inode)\n\t\t\tbase = offset_in_addr(&raw_node->i);\n\t} else if (f2fs_has_extra_attr(inode) && is_inode) {\n\t\tbase = get_extra_isize(inode);\n\t}\n\n\taddr_array = blkaddr_in_node(raw_node);\n\treturn le32_to_cpu(addr_array[base + offset]);\n}\n\nstatic inline int f2fs_test_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\treturn mask & *addr;\n}\n\nstatic inline void f2fs_set_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr |= mask;\n}\n\nstatic inline void f2fs_clear_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr &= ~mask;\n}\n\nstatic inline int f2fs_test_and_set_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\tint ret;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\tret = mask & *addr;\n\t*addr |= mask;\n\treturn ret;\n}\n\nstatic inline int f2fs_test_and_clear_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\tint ret;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\tret = mask & *addr;\n\t*addr &= ~mask;\n\treturn ret;\n}\n\nstatic inline void f2fs_change_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr ^= mask;\n}\n\n#define F2FS_REG_FLMASK\t\t(~(FS_DIRSYNC_FL | FS_TOPDIR_FL))\n#define F2FS_OTHER_FLMASK\t(FS_NODUMP_FL | FS_NOATIME_FL)\n#define F2FS_FL_INHERITED\t(FS_PROJINHERIT_FL)\n\nstatic inline __u32 f2fs_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & F2FS_REG_FLMASK;\n\telse\n\t\treturn flags & F2FS_OTHER_FLMASK;\n}\n\n/* used for f2fs_inode_info->flags */\nenum {\n\tFI_NEW_INODE,\t\t/* indicate newly allocated inode */\n\tFI_DIRTY_INODE,\t\t/* indicate inode is dirty or not */\n\tFI_AUTO_RECOVER,\t/* indicate inode is recoverable */\n\tFI_DIRTY_DIR,\t\t/* indicate directory has dirty pages */\n\tFI_INC_LINK,\t\t/* need to increment i_nlink */\n\tFI_ACL_MODE,\t\t/* indicate acl mode */\n\tFI_NO_ALLOC,\t\t/* should not allocate any blocks */\n\tFI_FREE_NID,\t\t/* free allocated nide */\n\tFI_NO_EXTENT,\t\t/* not to use the extent cache */\n\tFI_INLINE_XATTR,\t/* used for inline xattr */\n\tFI_INLINE_DATA,\t\t/* used for inline data*/\n\tFI_INLINE_DENTRY,\t/* used for inline dentry */\n\tFI_APPEND_WRITE,\t/* inode has appended data */\n\tFI_UPDATE_WRITE,\t/* inode has in-place-update data */\n\tFI_NEED_IPU,\t\t/* used for ipu per file */\n\tFI_ATOMIC_FILE,\t\t/* indicate atomic file */\n\tFI_ATOMIC_COMMIT,\t/* indicate the state of atomical committing */\n\tFI_VOLATILE_FILE,\t/* indicate volatile file */\n\tFI_FIRST_BLOCK_WRITTEN,\t/* indicate #0 data block was written */\n\tFI_DROP_CACHE,\t\t/* drop dirty page cache */\n\tFI_DATA_EXIST,\t\t/* indicate data exists */\n\tFI_INLINE_DOTS,\t\t/* indicate inline dot dentries */\n\tFI_DO_DEFRAG,\t\t/* indicate defragment is running */\n\tFI_DIRTY_FILE,\t\t/* indicate regular/symlink has dirty pages */\n\tFI_NO_PREALLOC,\t\t/* indicate skipped preallocated blocks */\n\tFI_HOT_DATA,\t\t/* indicate file is hot */\n\tFI_EXTRA_ATTR,\t\t/* indicate file has extra attribute */\n\tFI_PROJ_INHERIT,\t/* indicate file inherits projectid */\n};\n\nstatic inline void __mark_inode_dirty_flag(struct inode *inode,\n\t\t\t\t\t\tint flag, bool set)\n{\n\tswitch (flag) {\n\tcase FI_INLINE_XATTR:\n\tcase FI_INLINE_DATA:\n\tcase FI_INLINE_DENTRY:\n\t\tif (set)\n\t\t\treturn;\n\tcase FI_DATA_EXIST:\n\tcase FI_INLINE_DOTS:\n\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t}\n}\n\nstatic inline void set_inode_flag(struct inode *inode, int flag)\n{\n\tif (!test_bit(flag, &F2FS_I(inode)->flags))\n\t\tset_bit(flag, &F2FS_I(inode)->flags);\n\t__mark_inode_dirty_flag(inode, flag, true);\n}\n\nstatic inline int is_inode_flag_set(struct inode *inode, int flag)\n{\n\treturn test_bit(flag, &F2FS_I(inode)->flags);\n}\n\nstatic inline void clear_inode_flag(struct inode *inode, int flag)\n{\n\tif (test_bit(flag, &F2FS_I(inode)->flags))\n\t\tclear_bit(flag, &F2FS_I(inode)->flags);\n\t__mark_inode_dirty_flag(inode, flag, false);\n}\n\nstatic inline void set_acl_inode(struct inode *inode, umode_t mode)\n{\n\tF2FS_I(inode)->i_acl_mode = mode;\n\tset_inode_flag(inode, FI_ACL_MODE);\n\tf2fs_mark_inode_dirty_sync(inode, false);\n}\n\nstatic inline void f2fs_i_links_write(struct inode *inode, bool inc)\n{\n\tif (inc)\n\t\tinc_nlink(inode);\n\telse\n\t\tdrop_nlink(inode);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_blocks_write(struct inode *inode,\n\t\t\t\t\tblock_t diff, bool add, bool claim)\n{\n\tbool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);\n\tbool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);\n\n\t/* add = 1, claim = 1 should be dquot_reserve_block in pair */\n\tif (add) {\n\t\tif (claim)\n\t\t\tdquot_claim_block(inode, diff);\n\t\telse\n\t\t\tdquot_alloc_block_nofail(inode, diff);\n\t} else {\n\t\tdquot_free_block(inode, diff);\n\t}\n\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (clean || recover)\n\t\tset_inode_flag(inode, FI_AUTO_RECOVER);\n}\n\nstatic inline void f2fs_i_size_write(struct inode *inode, loff_t i_size)\n{\n\tbool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);\n\tbool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);\n\n\tif (i_size_read(inode) == i_size)\n\t\treturn;\n\n\ti_size_write(inode, i_size);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (clean || recover)\n\t\tset_inode_flag(inode, FI_AUTO_RECOVER);\n}\n\nstatic inline void f2fs_i_depth_write(struct inode *inode, unsigned int depth)\n{\n\tF2FS_I(inode)->i_current_depth = depth;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_xnid_write(struct inode *inode, nid_t xnid)\n{\n\tF2FS_I(inode)->i_xattr_nid = xnid;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_pino_write(struct inode *inode, nid_t pino)\n{\n\tF2FS_I(inode)->i_pino = pino;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void get_inline_info(struct inode *inode, struct f2fs_inode *ri)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tif (ri->i_inline & F2FS_INLINE_XATTR)\n\t\tset_bit(FI_INLINE_XATTR, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DATA)\n\t\tset_bit(FI_INLINE_DATA, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DENTRY)\n\t\tset_bit(FI_INLINE_DENTRY, &fi->flags);\n\tif (ri->i_inline & F2FS_DATA_EXIST)\n\t\tset_bit(FI_DATA_EXIST, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DOTS)\n\t\tset_bit(FI_INLINE_DOTS, &fi->flags);\n\tif (ri->i_inline & F2FS_EXTRA_ATTR)\n\t\tset_bit(FI_EXTRA_ATTR, &fi->flags);\n}\n\nstatic inline void set_raw_inline(struct inode *inode, struct f2fs_inode *ri)\n{\n\tri->i_inline = 0;\n\n\tif (is_inode_flag_set(inode, FI_INLINE_XATTR))\n\t\tri->i_inline |= F2FS_INLINE_XATTR;\n\tif (is_inode_flag_set(inode, FI_INLINE_DATA))\n\t\tri->i_inline |= F2FS_INLINE_DATA;\n\tif (is_inode_flag_set(inode, FI_INLINE_DENTRY))\n\t\tri->i_inline |= F2FS_INLINE_DENTRY;\n\tif (is_inode_flag_set(inode, FI_DATA_EXIST))\n\t\tri->i_inline |= F2FS_DATA_EXIST;\n\tif (is_inode_flag_set(inode, FI_INLINE_DOTS))\n\t\tri->i_inline |= F2FS_INLINE_DOTS;\n\tif (is_inode_flag_set(inode, FI_EXTRA_ATTR))\n\t\tri->i_inline |= F2FS_EXTRA_ATTR;\n}\n\nstatic inline int f2fs_has_extra_attr(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_EXTRA_ATTR);\n}\n\nstatic inline int f2fs_has_inline_xattr(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_XATTR);\n}\n\nstatic inline unsigned int addrs_per_inode(struct inode *inode)\n{\n\tif (f2fs_has_inline_xattr(inode))\n\t\treturn CUR_ADDRS_PER_INODE(inode) - F2FS_INLINE_XATTR_ADDRS;\n\treturn CUR_ADDRS_PER_INODE(inode);\n}\n\nstatic inline void *inline_xattr_addr(struct page *page)\n{\n\tstruct f2fs_inode *ri = F2FS_INODE(page);\n\n\treturn (void *)&(ri->i_addr[DEF_ADDRS_PER_INODE -\n\t\t\t\t\tF2FS_INLINE_XATTR_ADDRS]);\n}\n\nstatic inline int inline_xattr_size(struct inode *inode)\n{\n\tif (f2fs_has_inline_xattr(inode))\n\t\treturn F2FS_INLINE_XATTR_ADDRS << 2;\n\telse\n\t\treturn 0;\n}\n\nstatic inline int f2fs_has_inline_data(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DATA);\n}\n\nstatic inline int f2fs_exist_data(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_DATA_EXIST);\n}\n\nstatic inline int f2fs_has_inline_dots(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DOTS);\n}\n\nstatic inline bool f2fs_is_atomic_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_ATOMIC_FILE);\n}\n\nstatic inline bool f2fs_is_commit_atomic_write(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_ATOMIC_COMMIT);\n}\n\nstatic inline bool f2fs_is_volatile_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_VOLATILE_FILE);\n}\n\nstatic inline bool f2fs_is_first_block_written(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_FIRST_BLOCK_WRITTEN);\n}\n\nstatic inline bool f2fs_is_drop_cache(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_DROP_CACHE);\n}\n\nstatic inline void *inline_data_addr(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode *ri = F2FS_INODE(page);\n\tint extra_size = get_extra_isize(inode);\n\n\treturn (void *)&(ri->i_addr[extra_size + DEF_INLINE_RESERVED_SIZE]);\n}\n\nstatic inline int f2fs_has_inline_dentry(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DENTRY);\n}\n\nstatic inline void f2fs_dentry_kunmap(struct inode *dir, struct page *page)\n{\n\tif (!f2fs_has_inline_dentry(dir))\n\t\tkunmap(page);\n}\n\nstatic inline int is_file(struct inode *inode, int type)\n{\n\treturn F2FS_I(inode)->i_advise & type;\n}\n\nstatic inline void set_file(struct inode *inode, int type)\n{\n\tF2FS_I(inode)->i_advise |= type;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void clear_file(struct inode *inode, int type)\n{\n\tF2FS_I(inode)->i_advise &= ~type;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline bool f2fs_skip_inode_update(struct inode *inode, int dsync)\n{\n\tif (dsync) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\t\tbool ret;\n\n\t\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\t\tret = list_empty(&F2FS_I(inode)->gdirty_list);\n\t\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\t\treturn ret;\n\t}\n\tif (!is_inode_flag_set(inode, FI_AUTO_RECOVER) ||\n\t\t\tfile_keep_isize(inode) ||\n\t\t\ti_size_read(inode) & PAGE_MASK)\n\t\treturn false;\n\treturn F2FS_I(inode)->last_disk_size == i_size_read(inode);\n}\n\nstatic inline int f2fs_readonly(struct super_block *sb)\n{\n\treturn sb->s_flags & MS_RDONLY;\n}\n\nstatic inline bool f2fs_cp_error(struct f2fs_sb_info *sbi)\n{\n\treturn is_set_ckpt_flags(sbi, CP_ERROR_FLAG);\n}\n\nstatic inline bool is_dot_dotdot(const struct qstr *str)\n{\n\tif (str->len == 1 && str->name[0] == '.')\n\t\treturn true;\n\n\tif (str->len == 2 && str->name[0] == '.' && str->name[1] == '.')\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool f2fs_may_extent_tree(struct inode *inode)\n{\n\tif (!test_opt(F2FS_I_SB(inode), EXTENT_CACHE) ||\n\t\t\tis_inode_flag_set(inode, FI_NO_EXTENT))\n\t\treturn false;\n\n\treturn S_ISREG(inode->i_mode);\n}\n\nstatic inline void *f2fs_kmalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(sbi, FAULT_KMALLOC)) {\n\t\tf2fs_show_injection_info(FAULT_KMALLOC);\n\t\treturn NULL;\n\t}\n#endif\n\treturn kmalloc(size, flags);\n}\n\nstatic inline int get_extra_isize(struct inode *inode)\n{\n\treturn F2FS_I(inode)->i_extra_isize / sizeof(__le32);\n}\n\n#define get_inode_mode(i) \\\n\t((is_inode_flag_set(i, FI_ACL_MODE)) ? \\\n\t (F2FS_I(i)->i_acl_mode) : ((i)->i_mode))\n\n#define F2FS_TOTAL_EXTRA_ATTR_SIZE\t\t\t\\\n\t(offsetof(struct f2fs_inode, i_extra_end) -\t\\\n\toffsetof(struct f2fs_inode, i_extra_isize))\t\\\n\n#define F2FS_OLD_ATTRIBUTE_SIZE\t(offsetof(struct f2fs_inode, i_addr))\n#define F2FS_FITS_IN_INODE(f2fs_inode, extra_isize, field)\t\t\\\n\t\t((offsetof(typeof(*f2fs_inode), field) +\t\\\n\t\tsizeof((f2fs_inode)->field))\t\t\t\\\n\t\t<= (F2FS_OLD_ATTRIBUTE_SIZE + extra_isize))\t\\\n\nstatic inline void f2fs_reset_iostat(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tspin_lock(&sbi->iostat_lock);\n\tfor (i = 0; i < NR_IO_TYPE; i++)\n\t\tsbi->write_iostat[i] = 0;\n\tspin_unlock(&sbi->iostat_lock);\n}\n\nstatic inline void f2fs_update_iostat(struct f2fs_sb_info *sbi,\n\t\t\tenum iostat_type type, unsigned long long io_bytes)\n{\n\tif (!sbi->iostat_enable)\n\t\treturn;\n\tspin_lock(&sbi->iostat_lock);\n\tsbi->write_iostat[type] += io_bytes;\n\n\tif (type == APP_WRITE_IO || type == APP_DIRECT_IO)\n\t\tsbi->write_iostat[APP_BUFFERED_IO] =\n\t\t\tsbi->write_iostat[APP_WRITE_IO] -\n\t\t\tsbi->write_iostat[APP_DIRECT_IO];\n\tspin_unlock(&sbi->iostat_lock);\n}\n\n/*\n * file.c\n */\nint f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync);\nvoid truncate_data_blocks(struct dnode_of_data *dn);\nint truncate_blocks(struct inode *inode, u64 from, bool lock);\nint f2fs_truncate(struct inode *inode);\nint f2fs_getattr(const struct path *path, struct kstat *stat,\n\t\t\tu32 request_mask, unsigned int flags);\nint f2fs_setattr(struct dentry *dentry, struct iattr *attr);\nint truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end);\nint truncate_data_blocks_range(struct dnode_of_data *dn, int count);\nlong f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);\nlong f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\n\n/*\n * inode.c\n */\nvoid f2fs_set_inode_flags(struct inode *inode);\nbool f2fs_inode_chksum_verify(struct f2fs_sb_info *sbi, struct page *page);\nvoid f2fs_inode_chksum_set(struct f2fs_sb_info *sbi, struct page *page);\nstruct inode *f2fs_iget(struct super_block *sb, unsigned long ino);\nstruct inode *f2fs_iget_retry(struct super_block *sb, unsigned long ino);\nint try_to_free_nats(struct f2fs_sb_info *sbi, int nr_shrink);\nint update_inode(struct inode *inode, struct page *node_page);\nint update_inode_page(struct inode *inode);\nint f2fs_write_inode(struct inode *inode, struct writeback_control *wbc);\nvoid f2fs_evict_inode(struct inode *inode);\nvoid handle_failed_inode(struct inode *inode);\n\n/*\n * namei.c\n */\nstruct dentry *f2fs_get_parent(struct dentry *child);\n\n/*\n * dir.c\n */\nvoid set_de_type(struct f2fs_dir_entry *de, umode_t mode);\nunsigned char get_de_type(struct f2fs_dir_entry *de);\nstruct f2fs_dir_entry *find_target_dentry(struct fscrypt_name *fname,\n\t\t\tf2fs_hash_t namehash, int *max_slots,\n\t\t\tstruct f2fs_dentry_ptr *d);\nint f2fs_fill_dentries(struct dir_context *ctx, struct f2fs_dentry_ptr *d,\n\t\t\tunsigned int start_pos, struct fscrypt_str *fstr);\nvoid do_make_empty_dir(struct inode *inode, struct inode *parent,\n\t\t\tstruct f2fs_dentry_ptr *d);\nstruct page *init_inode_metadata(struct inode *inode, struct inode *dir,\n\t\t\tconst struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name, struct page *dpage);\nvoid update_parent_metadata(struct inode *dir, struct inode *inode,\n\t\t\tunsigned int current_depth);\nint room_for_filename(const void *bitmap, int slots, int max_slots);\nvoid f2fs_drop_nlink(struct inode *dir, struct inode *inode);\nstruct f2fs_dir_entry *__f2fs_find_entry(struct inode *dir,\n\t\t\tstruct fscrypt_name *fname, struct page **res_page);\nstruct f2fs_dir_entry *f2fs_find_entry(struct inode *dir,\n\t\t\tconst struct qstr *child, struct page **res_page);\nstruct f2fs_dir_entry *f2fs_parent_dir(struct inode *dir, struct page **p);\nino_t f2fs_inode_by_name(struct inode *dir, const struct qstr *qstr,\n\t\t\tstruct page **page);\nvoid f2fs_set_link(struct inode *dir, struct f2fs_dir_entry *de,\n\t\t\tstruct page *page, struct inode *inode);\nvoid f2fs_update_dentry(nid_t ino, umode_t mode, struct f2fs_dentry_ptr *d,\n\t\t\tconst struct qstr *name, f2fs_hash_t name_hash,\n\t\t\tunsigned int bit_pos);\nint f2fs_add_regular_entry(struct inode *dir, const struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nint __f2fs_do_add_link(struct inode *dir, struct fscrypt_name *fname,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nint __f2fs_add_link(struct inode *dir, const struct qstr *name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nvoid f2fs_delete_entry(struct f2fs_dir_entry *dentry, struct page *page,\n\t\t\tstruct inode *dir, struct inode *inode);\nint f2fs_do_tmpfile(struct inode *inode, struct inode *dir);\nbool f2fs_empty_dir(struct inode *dir);\n\nstatic inline int f2fs_add_link(struct dentry *dentry, struct inode *inode)\n{\n\treturn __f2fs_add_link(d_inode(dentry->d_parent), &dentry->d_name,\n\t\t\t\tinode, inode->i_ino, inode->i_mode);\n}\n\n/*\n * super.c\n */\nint f2fs_inode_dirtied(struct inode *inode, bool sync);\nvoid f2fs_inode_synced(struct inode *inode);\nvoid f2fs_enable_quota_files(struct f2fs_sb_info *sbi);\nvoid f2fs_quota_off_umount(struct super_block *sb);\nint f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover);\nint f2fs_sync_fs(struct super_block *sb, int sync);\nextern __printf(3, 4)\nvoid f2fs_msg(struct super_block *sb, const char *level, const char *fmt, ...);\nint sanity_check_ckpt(struct f2fs_sb_info *sbi);\n\n/*\n * hash.c\n */\nf2fs_hash_t f2fs_dentry_hash(const struct qstr *name_info,\n\t\t\t\tstruct fscrypt_name *fname);\n\n/*\n * node.c\n */\nstruct dnode_of_data;\nstruct node_info;\n\nbool available_free_memory(struct f2fs_sb_info *sbi, int type);\nint need_dentry_mark(struct f2fs_sb_info *sbi, nid_t nid);\nbool is_checkpointed_node(struct f2fs_sb_info *sbi, nid_t nid);\nbool need_inode_block_update(struct f2fs_sb_info *sbi, nid_t ino);\nvoid get_node_info(struct f2fs_sb_info *sbi, nid_t nid, struct node_info *ni);\npgoff_t get_next_page_offset(struct dnode_of_data *dn, pgoff_t pgofs);\nint get_dnode_of_data(struct dnode_of_data *dn, pgoff_t index, int mode);\nint truncate_inode_blocks(struct inode *inode, pgoff_t from);\nint truncate_xattr_node(struct inode *inode, struct page *page);\nint wait_on_node_pages_writeback(struct f2fs_sb_info *sbi, nid_t ino);\nint remove_inode_page(struct inode *inode);\nstruct page *new_inode_page(struct inode *inode);\nstruct page *new_node_page(struct dnode_of_data *dn, unsigned int ofs);\nvoid ra_node_page(struct f2fs_sb_info *sbi, nid_t nid);\nstruct page *get_node_page(struct f2fs_sb_info *sbi, pgoff_t nid);\nstruct page *get_node_page_ra(struct page *parent, int start);\nvoid move_node_page(struct page *node_page, int gc_type);\nint fsync_node_pages(struct f2fs_sb_info *sbi, struct inode *inode,\n\t\t\tstruct writeback_control *wbc, bool atomic);\nint sync_node_pages(struct f2fs_sb_info *sbi, struct writeback_control *wbc,\n\t\t\tbool do_balance, enum iostat_type io_type);\nvoid build_free_nids(struct f2fs_sb_info *sbi, bool sync, bool mount);\nbool alloc_nid(struct f2fs_sb_info *sbi, nid_t *nid);\nvoid alloc_nid_done(struct f2fs_sb_info *sbi, nid_t nid);\nvoid alloc_nid_failed(struct f2fs_sb_info *sbi, nid_t nid);\nint try_to_free_nids(struct f2fs_sb_info *sbi, int nr_shrink);\nvoid recover_inline_xattr(struct inode *inode, struct page *page);\nint recover_xattr_data(struct inode *inode, struct page *page,\n\t\t\tblock_t blkaddr);\nint recover_inode_page(struct f2fs_sb_info *sbi, struct page *page);\nint restore_node_summary(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int segno, struct f2fs_summary_block *sum);\nvoid flush_nat_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nint build_node_manager(struct f2fs_sb_info *sbi);\nvoid destroy_node_manager(struct f2fs_sb_info *sbi);\nint __init create_node_manager_caches(void);\nvoid destroy_node_manager_caches(void);\n\n/*\n * segment.c\n */\nbool need_SSR(struct f2fs_sb_info *sbi);\nvoid register_inmem_page(struct inode *inode, struct page *page);\nvoid drop_inmem_pages(struct inode *inode);\nvoid drop_inmem_page(struct inode *inode, struct page *page);\nint commit_inmem_pages(struct inode *inode);\nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need);\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi);\nint f2fs_issue_flush(struct f2fs_sb_info *sbi);\nint create_flush_cmd_control(struct f2fs_sb_info *sbi);\nvoid destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free);\nvoid invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr);\nbool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new);\nvoid stop_discard_thread(struct f2fs_sb_info *sbi);\nvoid f2fs_wait_discard_bios(struct f2fs_sb_info *sbi, bool umount);\nvoid clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nvoid release_discard_addrs(struct f2fs_sb_info *sbi);\nint npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra);\nvoid allocate_new_segments(struct f2fs_sb_info *sbi);\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range);\nbool exist_trim_candidates(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nstruct page *get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno);\nvoid update_meta_page(struct f2fs_sb_info *sbi, void *src, block_t blk_addr);\nvoid write_meta_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\t\tenum iostat_type io_type);\nvoid write_node_page(unsigned int nid, struct f2fs_io_info *fio);\nvoid write_data_page(struct dnode_of_data *dn, struct f2fs_io_info *fio);\nint rewrite_data_page(struct f2fs_io_info *fio);\nvoid __f2fs_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\tbool recover_curseg, bool recover_newaddr);\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\tunsigned char version, bool recover_curseg,\n\t\t\tbool recover_newaddr);\nvoid allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\t\tstruct f2fs_summary *sum, int type,\n\t\t\tstruct f2fs_io_info *fio, bool add_list);\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\tenum page_type type, bool ordered);\nvoid f2fs_wait_on_block_writeback(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk);\nvoid write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk);\nint lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\tunsigned int val, int alloc);\nvoid flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nint build_segment_manager(struct f2fs_sb_info *sbi);\nvoid destroy_segment_manager(struct f2fs_sb_info *sbi);\nint __init create_segment_manager_caches(void);\nvoid destroy_segment_manager_caches(void);\n\n/*\n * checkpoint.c\n */\nvoid f2fs_stop_checkpoint(struct f2fs_sb_info *sbi, bool end_io);\nstruct page *grab_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *get_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *get_tmp_page(struct f2fs_sb_info *sbi, pgoff_t index);\nbool is_valid_blkaddr(struct f2fs_sb_info *sbi, block_t blkaddr, int type);\nint ra_meta_pages(struct f2fs_sb_info *sbi, block_t start, int nrpages,\n\t\t\tint type, bool sync);\nvoid ra_meta_pages_cond(struct f2fs_sb_info *sbi, pgoff_t index);\nlong sync_meta_pages(struct f2fs_sb_info *sbi, enum page_type type,\n\t\t\tlong nr_to_write, enum iostat_type io_type);\nvoid add_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);\nvoid remove_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);\nvoid release_ino_entry(struct f2fs_sb_info *sbi, bool all);\nbool exist_written_data(struct f2fs_sb_info *sbi, nid_t ino, int mode);\nint f2fs_sync_inode_meta(struct f2fs_sb_info *sbi);\nint acquire_orphan_inode(struct f2fs_sb_info *sbi);\nvoid release_orphan_inode(struct f2fs_sb_info *sbi);\nvoid add_orphan_inode(struct inode *inode);\nvoid remove_orphan_inode(struct f2fs_sb_info *sbi, nid_t ino);\nint recover_orphan_inodes(struct f2fs_sb_info *sbi);\nint get_valid_checkpoint(struct f2fs_sb_info *sbi);\nvoid update_dirty_page(struct inode *inode, struct page *page);\nvoid remove_dirty_inode(struct inode *inode);\nint sync_dirty_inodes(struct f2fs_sb_info *sbi, enum inode_type type);\nint write_checkpoint(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nvoid init_ino_entry_info(struct f2fs_sb_info *sbi);\nint __init create_checkpoint_caches(void);\nvoid destroy_checkpoint_caches(void);\n\n/*\n * data.c\n */\nvoid f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type);\nvoid f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, nid_t ino, pgoff_t idx,\n\t\t\t\tenum page_type type);\nvoid f2fs_flush_merged_writes(struct f2fs_sb_info *sbi);\nint f2fs_submit_page_bio(struct f2fs_io_info *fio);\nint f2fs_submit_page_write(struct f2fs_io_info *fio);\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\t\tblock_t blk_addr, struct bio *bio);\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid set_data_blkaddr(struct dnode_of_data *dn);\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr);\nint reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count);\nint reserve_new_block(struct dnode_of_data *dn);\nint f2fs_get_block(struct dnode_of_data *dn, pgoff_t index);\nint f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from);\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index);\nstruct page *get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\tint op_flags, bool for_write);\nstruct page *find_data_page(struct inode *inode, pgoff_t index);\nstruct page *get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\tbool for_write);\nstruct page *get_new_data_page(struct inode *inode,\n\t\t\tstruct page *ipage, pgoff_t index, bool new_i_size);\nint do_write_data_page(struct f2fs_io_info *fio);\nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,\n\t\t\tint create, int flag);\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t\tu64 start, u64 len);\nvoid f2fs_set_page_dirty_nobuffers(struct page *page);\nint __f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\t\tenum iostat_type io_type);\nvoid f2fs_invalidate_page(struct page *page, unsigned int offset,\n\t\t\tunsigned int length);\nint f2fs_release_page(struct page *page, gfp_t wait);\n#ifdef CONFIG_MIGRATION\nint f2fs_migrate_page(struct address_space *mapping, struct page *newpage,\n\t\t\tstruct page *page, enum migrate_mode mode);\n#endif\n\n/*\n * gc.c\n */\nint start_gc_thread(struct f2fs_sb_info *sbi);\nvoid stop_gc_thread(struct f2fs_sb_info *sbi);\nblock_t start_bidx_of_node(unsigned int node_ofs, struct inode *inode);\nint f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background,\n\t\t\tunsigned int segno);\nvoid build_gc_manager(struct f2fs_sb_info *sbi);\n\n/*\n * recovery.c\n */\nint recover_fsync_data(struct f2fs_sb_info *sbi, bool check_only);\nbool space_for_roll_forward(struct f2fs_sb_info *sbi);\n\n/*\n * debug.c\n */\n#ifdef CONFIG_F2FS_STAT_FS\nstruct f2fs_stat_info {\n\tstruct list_head stat_list;\n\tstruct f2fs_sb_info *sbi;\n\tint all_area_segs, sit_area_segs, nat_area_segs, ssa_area_segs;\n\tint main_area_segs, main_area_sections, main_area_zones;\n\tunsigned long long hit_largest, hit_cached, hit_rbtree;\n\tunsigned long long hit_total, total_ext;\n\tint ext_tree, zombie_tree, ext_node;\n\tint ndirty_node, ndirty_dent, ndirty_meta, ndirty_data, ndirty_imeta;\n\tint inmem_pages;\n\tunsigned int ndirty_dirs, ndirty_files, ndirty_all;\n\tint nats, dirty_nats, sits, dirty_sits;\n\tint free_nids, avail_nids, alloc_nids;\n\tint total_count, utilization;\n\tint bg_gc, nr_wb_cp_data, nr_wb_data;\n\tint nr_flushing, nr_flushed, nr_discarding, nr_discarded;\n\tint nr_discard_cmd;\n\tunsigned int undiscard_blks;\n\tint inline_xattr, inline_inode, inline_dir, append, update, orphans;\n\tint aw_cnt, max_aw_cnt, vw_cnt, max_vw_cnt;\n\tunsigned int valid_count, valid_node_count, valid_inode_count, discard_blks;\n\tunsigned int bimodal, avg_vblocks;\n\tint util_free, util_valid, util_invalid;\n\tint rsvd_segs, overp_segs;\n\tint dirty_count, node_pages, meta_pages;\n\tint prefree_count, call_count, cp_count, bg_cp_count;\n\tint tot_segs, node_segs, data_segs, free_segs, free_secs;\n\tint bg_node_segs, bg_data_segs;\n\tint tot_blks, data_blks, node_blks;\n\tint bg_data_blks, bg_node_blks;\n\tint curseg[NR_CURSEG_TYPE];\n\tint cursec[NR_CURSEG_TYPE];\n\tint curzone[NR_CURSEG_TYPE];\n\n\tunsigned int segment_count[2];\n\tunsigned int block_count[2];\n\tunsigned int inplace_count;\n\tunsigned long long base_mem, cache_mem, page_mem;\n};\n\nstatic inline struct f2fs_stat_info *F2FS_STAT(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_stat_info *)sbi->stat_info;\n}\n\n#define stat_inc_cp_count(si)\t\t((si)->cp_count++)\n#define stat_inc_bg_cp_count(si)\t((si)->bg_cp_count++)\n#define stat_inc_call_count(si)\t\t((si)->call_count++)\n#define stat_inc_bggc_count(sbi)\t((sbi)->bg_gc++)\n#define stat_inc_dirty_inode(sbi, type)\t((sbi)->ndirty_inode[type]++)\n#define stat_dec_dirty_inode(sbi, type)\t((sbi)->ndirty_inode[type]--)\n#define stat_inc_total_hit(sbi)\t\t(atomic64_inc(&(sbi)->total_hit_ext))\n#define stat_inc_rbtree_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_rbtree))\n#define stat_inc_largest_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_largest))\n#define stat_inc_cached_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_cached))\n#define stat_inc_inline_xattr(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_xattr(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_xattr));\t\\\n\t} while (0)\n#define stat_dec_inline_xattr(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_xattr(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_xattr));\t\\\n\t} while (0)\n#define stat_inc_inline_inode(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_data(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_inode));\t\\\n\t} while (0)\n#define stat_dec_inline_inode(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_data(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_inode));\t\\\n\t} while (0)\n#define stat_inc_inline_dir(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_dentry(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_dir));\t\\\n\t} while (0)\n#define stat_dec_inline_dir(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_dentry(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_dir));\t\\\n\t} while (0)\n#define stat_inc_seg_type(sbi, curseg)\t\t\t\t\t\\\n\t\t((sbi)->segment_count[(curseg)->alloc_type]++)\n#define stat_inc_block_count(sbi, curseg)\t\t\t\t\\\n\t\t((sbi)->block_count[(curseg)->alloc_type]++)\n#define stat_inc_inplace_blocks(sbi)\t\t\t\t\t\\\n\t\t(atomic_inc(&(sbi)->inplace_count))\n#define stat_inc_atomic_write(inode)\t\t\t\t\t\\\n\t\t(atomic_inc(&F2FS_I_SB(inode)->aw_cnt))\n#define stat_dec_atomic_write(inode)\t\t\t\t\t\\\n\t\t(atomic_dec(&F2FS_I_SB(inode)->aw_cnt))\n#define stat_update_max_atomic_write(inode)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint cur = atomic_read(&F2FS_I_SB(inode)->aw_cnt);\t\\\n\t\tint max = atomic_read(&F2FS_I_SB(inode)->max_aw_cnt);\t\\\n\t\tif (cur > max)\t\t\t\t\t\t\\\n\t\t\tatomic_set(&F2FS_I_SB(inode)->max_aw_cnt, cur);\t\\\n\t} while (0)\n#define stat_inc_volatile_write(inode)\t\t\t\t\t\\\n\t\t(atomic_inc(&F2FS_I_SB(inode)->vw_cnt))\n#define stat_dec_volatile_write(inode)\t\t\t\t\t\\\n\t\t(atomic_dec(&F2FS_I_SB(inode)->vw_cnt))\n#define stat_update_max_volatile_write(inode)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint cur = atomic_read(&F2FS_I_SB(inode)->vw_cnt);\t\\\n\t\tint max = atomic_read(&F2FS_I_SB(inode)->max_vw_cnt);\t\\\n\t\tif (cur > max)\t\t\t\t\t\t\\\n\t\t\tatomic_set(&F2FS_I_SB(inode)->max_vw_cnt, cur);\t\\\n\t} while (0)\n#define stat_inc_seg_count(sbi, type, gc_type)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tsi->tot_segs++;\t\t\t\t\t\t\\\n\t\tif ((type) == SUM_TYPE_DATA) {\t\t\t\t\\\n\t\t\tsi->data_segs++;\t\t\t\t\\\n\t\t\tsi->bg_data_segs += (gc_type == BG_GC) ? 1 : 0;\t\\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\tsi->node_segs++;\t\t\t\t\\\n\t\t\tsi->bg_node_segs += (gc_type == BG_GC) ? 1 : 0;\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define stat_inc_tot_blk_count(si, blks)\t\t\t\t\\\n\t((si)->tot_blks += (blks))\n\n#define stat_inc_data_blk_count(sbi, blks, gc_type)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tstat_inc_tot_blk_count(si, blks);\t\t\t\\\n\t\tsi->data_blks += (blks);\t\t\t\t\\\n\t\tsi->bg_data_blks += ((gc_type) == BG_GC) ? (blks) : 0;\t\\\n\t} while (0)\n\n#define stat_inc_node_blk_count(sbi, blks, gc_type)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tstat_inc_tot_blk_count(si, blks);\t\t\t\\\n\t\tsi->node_blks += (blks);\t\t\t\t\\\n\t\tsi->bg_node_blks += ((gc_type) == BG_GC) ? (blks) : 0;\t\\\n\t} while (0)\n\nint f2fs_build_stats(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_stats(struct f2fs_sb_info *sbi);\nint __init f2fs_create_root_stats(void);\nvoid f2fs_destroy_root_stats(void);\n#else\n#define stat_inc_cp_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_bg_cp_count(si)\t\t\tdo { } while (0)\n#define stat_inc_call_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_bggc_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_dirty_inode(sbi, type)\t\t\tdo { } while (0)\n#define stat_dec_dirty_inode(sbi, type)\t\t\tdo { } while (0)\n#define stat_inc_total_hit(sb)\t\t\t\tdo { } while (0)\n#define stat_inc_rbtree_node_hit(sb)\t\t\tdo { } while (0)\n#define stat_inc_largest_node_hit(sbi)\t\t\tdo { } while (0)\n#define stat_inc_cached_node_hit(sbi)\t\t\tdo { } while (0)\n#define stat_inc_inline_xattr(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_xattr(inode)\t\t\tdo { } while (0)\n#define stat_inc_inline_inode(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_inode(inode)\t\t\tdo { } while (0)\n#define stat_inc_inline_dir(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_dir(inode)\t\t\tdo { } while (0)\n#define stat_inc_atomic_write(inode)\t\t\tdo { } while (0)\n#define stat_dec_atomic_write(inode)\t\t\tdo { } while (0)\n#define stat_update_max_atomic_write(inode)\t\tdo { } while (0)\n#define stat_inc_volatile_write(inode)\t\t\tdo { } while (0)\n#define stat_dec_volatile_write(inode)\t\t\tdo { } while (0)\n#define stat_update_max_volatile_write(inode)\t\tdo { } while (0)\n#define stat_inc_seg_type(sbi, curseg)\t\t\tdo { } while (0)\n#define stat_inc_block_count(sbi, curseg)\t\tdo { } while (0)\n#define stat_inc_inplace_blocks(sbi)\t\t\tdo { } while (0)\n#define stat_inc_seg_count(sbi, type, gc_type)\t\tdo { } while (0)\n#define stat_inc_tot_blk_count(si, blks)\t\tdo { } while (0)\n#define stat_inc_data_blk_count(sbi, blks, gc_type)\tdo { } while (0)\n#define stat_inc_node_blk_count(sbi, blks, gc_type)\tdo { } while (0)\n\nstatic inline int f2fs_build_stats(struct f2fs_sb_info *sbi) { return 0; }\nstatic inline void f2fs_destroy_stats(struct f2fs_sb_info *sbi) { }\nstatic inline int __init f2fs_create_root_stats(void) { return 0; }\nstatic inline void f2fs_destroy_root_stats(void) { }\n#endif\n\nextern const struct file_operations f2fs_dir_operations;\nextern const struct file_operations f2fs_file_operations;\nextern const struct inode_operations f2fs_file_inode_operations;\nextern const struct address_space_operations f2fs_dblock_aops;\nextern const struct address_space_operations f2fs_node_aops;\nextern const struct address_space_operations f2fs_meta_aops;\nextern const struct inode_operations f2fs_dir_inode_operations;\nextern const struct inode_operations f2fs_symlink_inode_operations;\nextern const struct inode_operations f2fs_encrypted_symlink_inode_operations;\nextern const struct inode_operations f2fs_special_inode_operations;\nextern struct kmem_cache *inode_entry_slab;\n\n/*\n * inline.c\n */\nbool f2fs_may_inline_data(struct inode *inode);\nbool f2fs_may_inline_dentry(struct inode *inode);\nvoid read_inline_data(struct page *page, struct page *ipage);\nvoid truncate_inline_inode(struct inode *inode, struct page *ipage, u64 from);\nint f2fs_read_inline_data(struct inode *inode, struct page *page);\nint f2fs_convert_inline_page(struct dnode_of_data *dn, struct page *page);\nint f2fs_convert_inline_inode(struct inode *inode);\nint f2fs_write_inline_data(struct inode *inode, struct page *page);\nbool recover_inline_data(struct inode *inode, struct page *npage);\nstruct f2fs_dir_entry *find_in_inline_dir(struct inode *dir,\n\t\t\tstruct fscrypt_name *fname, struct page **res_page);\nint make_empty_inline_dir(struct inode *inode, struct inode *parent,\n\t\t\tstruct page *ipage);\nint f2fs_add_inline_entry(struct inode *dir, const struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nvoid f2fs_delete_inline_entry(struct f2fs_dir_entry *dentry, struct page *page,\n\t\t\tstruct inode *dir, struct inode *inode);\nbool f2fs_empty_inline_dir(struct inode *dir);\nint f2fs_read_inline_dir(struct file *file, struct dir_context *ctx,\n\t\t\tstruct fscrypt_str *fstr);\nint f2fs_inline_data_fiemap(struct inode *inode,\n\t\t\tstruct fiemap_extent_info *fieinfo,\n\t\t\t__u64 start, __u64 len);\n\n/*\n * shrinker.c\n */\nunsigned long f2fs_shrink_count(struct shrinker *shrink,\n\t\t\tstruct shrink_control *sc);\nunsigned long f2fs_shrink_scan(struct shrinker *shrink,\n\t\t\tstruct shrink_control *sc);\nvoid f2fs_join_shrinker(struct f2fs_sb_info *sbi);\nvoid f2fs_leave_shrinker(struct f2fs_sb_info *sbi);\n\n/*\n * extent_cache.c\n */\nstruct rb_entry *__lookup_rb_tree(struct rb_root *root,\n\t\t\t\tstruct rb_entry *cached_re, unsigned int ofs);\nstruct rb_node **__lookup_rb_tree_for_insert(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct rb_root *root, struct rb_node **parent,\n\t\t\t\tunsigned int ofs);\nstruct rb_entry *__lookup_rb_tree_ret(struct rb_root *root,\n\t\tstruct rb_entry *cached_re, unsigned int ofs,\n\t\tstruct rb_entry **prev_entry, struct rb_entry **next_entry,\n\t\tstruct rb_node ***insert_p, struct rb_node **insert_parent,\n\t\tbool force);\nbool __check_rb_tree_consistence(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct rb_root *root);\nunsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink);\nbool f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext);\nvoid f2fs_drop_extent_tree(struct inode *inode);\nunsigned int f2fs_destroy_extent_node(struct inode *inode);\nvoid f2fs_destroy_extent_tree(struct inode *inode);\nbool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,\n\t\t\tstruct extent_info *ei);\nvoid f2fs_update_extent_cache(struct dnode_of_data *dn);\nvoid f2fs_update_extent_cache_range(struct dnode_of_data *dn,\n\t\t\tpgoff_t fofs, block_t blkaddr, unsigned int len);\nvoid init_extent_cache_info(struct f2fs_sb_info *sbi);\nint __init create_extent_cache(void);\nvoid destroy_extent_cache(void);\n\n/*\n * sysfs.c\n */\nint __init f2fs_init_sysfs(void);\nvoid f2fs_exit_sysfs(void);\nint f2fs_register_sysfs(struct f2fs_sb_info *sbi);\nvoid f2fs_unregister_sysfs(struct f2fs_sb_info *sbi);\n\n/*\n * crypto support\n */\nstatic inline bool f2fs_encrypted_inode(struct inode *inode)\n{\n\treturn file_is_encrypt(inode);\n}\n\nstatic inline bool f2fs_encrypted_file(struct inode *inode)\n{\n\treturn f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode);\n}\n\nstatic inline void f2fs_set_encrypted_inode(struct inode *inode)\n{\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\n\tfile_set_encrypt(inode);\n#endif\n}\n\nstatic inline bool f2fs_bio_encrypted(struct bio *bio)\n{\n\treturn bio->bi_private != NULL;\n}\n\nstatic inline int f2fs_sb_has_crypto(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_ENCRYPT);\n}\n\nstatic inline int f2fs_sb_mounted_blkzoned(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_BLKZONED);\n}\n\nstatic inline int f2fs_sb_has_extra_attr(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_EXTRA_ATTR);\n}\n\nstatic inline int f2fs_sb_has_project_quota(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_PRJQUOTA);\n}\n\nstatic inline int f2fs_sb_has_inode_chksum(struct super_block *sb)\n{\n\treturn F2FS_HAS_FEATURE(sb, F2FS_FEATURE_INODE_CHKSUM);\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic inline int get_blkz_type(struct f2fs_sb_info *sbi,\n\t\t\tstruct block_device *bdev, block_t blkaddr)\n{\n\tunsigned int zno = blkaddr >> sbi->log_blocks_per_blkz;\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (FDEV(i).bdev == bdev)\n\t\t\treturn FDEV(i).blkz_type[zno];\n\treturn -EINVAL;\n}\n#endif\n\nstatic inline bool f2fs_discard_en(struct f2fs_sb_info *sbi)\n{\n\tstruct request_queue *q = bdev_get_queue(sbi->sb->s_bdev);\n\n\treturn blk_queue_discard(q) || f2fs_sb_mounted_blkzoned(sbi->sb);\n}\n\nstatic inline void set_opt_mode(struct f2fs_sb_info *sbi, unsigned int mt)\n{\n\tclear_opt(sbi, ADAPTIVE);\n\tclear_opt(sbi, LFS);\n\n\tswitch (mt) {\n\tcase F2FS_MOUNT_ADAPTIVE:\n\t\tset_opt(sbi, ADAPTIVE);\n\t\tbreak;\n\tcase F2FS_MOUNT_LFS:\n\t\tset_opt(sbi, LFS);\n\t\tbreak;\n\t}\n}\n\nstatic inline bool f2fs_may_encrypt(struct inode *inode)\n{\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\n\tumode_t mode = inode->i_mode;\n\n\treturn (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode));\n#else\n\treturn 0;\n#endif\n}\n\n#endif\n", "/*\n * fs/f2fs/segment.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/prefetch.h>\n#include <linux/kthread.h>\n#include <linux/swap.h>\n#include <linux/timer.h>\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n\n#include \"f2fs.h\"\n#include \"segment.h\"\n#include \"node.h\"\n#include \"gc.h\"\n#include \"trace.h\"\n#include <trace/events/f2fs.h>\n\n#define __reverse_ffz(x) __reverse_ffs(~(x))\n\nstatic struct kmem_cache *discard_entry_slab;\nstatic struct kmem_cache *discard_cmd_slab;\nstatic struct kmem_cache *sit_entry_set_slab;\nstatic struct kmem_cache *inmem_entry_slab;\n\nstatic unsigned long __reverse_ulong(unsigned char *str)\n{\n\tunsigned long tmp = 0;\n\tint shift = 24, idx = 0;\n\n#if BITS_PER_LONG == 64\n\tshift = 56;\n#endif\n\twhile (shift >= 0) {\n\t\ttmp |= (unsigned long)str[idx++] << shift;\n\t\tshift -= BITS_PER_BYTE;\n\t}\n\treturn tmp;\n}\n\n/*\n * __reverse_ffs is copied from include/asm-generic/bitops/__ffs.h since\n * MSB and LSB are reversed in a byte by f2fs_set_bit.\n */\nstatic inline unsigned long __reverse_ffs(unsigned long word)\n{\n\tint num = 0;\n\n#if BITS_PER_LONG == 64\n\tif ((word & 0xffffffff00000000UL) == 0)\n\t\tnum += 32;\n\telse\n\t\tword >>= 32;\n#endif\n\tif ((word & 0xffff0000) == 0)\n\t\tnum += 16;\n\telse\n\t\tword >>= 16;\n\n\tif ((word & 0xff00) == 0)\n\t\tnum += 8;\n\telse\n\t\tword >>= 8;\n\n\tif ((word & 0xf0) == 0)\n\t\tnum += 4;\n\telse\n\t\tword >>= 4;\n\n\tif ((word & 0xc) == 0)\n\t\tnum += 2;\n\telse\n\t\tword >>= 2;\n\n\tif ((word & 0x2) == 0)\n\t\tnum += 1;\n\treturn num;\n}\n\n/*\n * __find_rev_next(_zero)_bit is copied from lib/find_next_bit.c because\n * f2fs_set_bit makes MSB and LSB reversed in a byte.\n * @size must be integral times of unsigned long.\n * Example:\n *                             MSB <--> LSB\n *   f2fs_set_bit(0, bitmap) => 1000 0000\n *   f2fs_set_bit(7, bitmap) => 0000 0001\n */\nstatic unsigned long __find_rev_next_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == 0)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\ttmp &= ~0UL >> offset;\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp &= (~0UL << (BITS_PER_LONG - size));\n\t\tif (tmp)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffs(tmp);\n}\n\nstatic unsigned long __find_rev_next_zero_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == ~0UL)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\tif (offset)\n\t\t\ttmp |= ~0UL << (BITS_PER_LONG - offset);\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp |= ~0UL >> size;\n\t\tif (tmp != ~0UL)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffz(tmp);\n}\n\nbool need_SSR(struct f2fs_sb_info *sbi)\n{\n\tint node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);\n\tint dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);\n\tint imeta_secs = get_blocktype_secs(sbi, F2FS_DIRTY_IMETA);\n\n\tif (test_opt(sbi, LFS))\n\t\treturn false;\n\tif (sbi->gc_thread && sbi->gc_thread->gc_urgent)\n\t\treturn true;\n\n\treturn free_sections(sbi) <= (node_secs + 2 * dent_secs + imeta_secs +\n\t\t\t\t\t\t2 * reserved_sections(sbi));\n}\n\nvoid register_inmem_page(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inmem_pages *new;\n\n\tf2fs_trace_pid(page);\n\n\tset_page_private(page, (unsigned long)ATOMIC_WRITTEN_PAGE);\n\tSetPagePrivate(page);\n\n\tnew = f2fs_kmem_cache_alloc(inmem_entry_slab, GFP_NOFS);\n\n\t/* add atomic page indices to the list */\n\tnew->page = page;\n\tINIT_LIST_HEAD(&new->list);\n\n\t/* increase reference count with clean state */\n\tmutex_lock(&fi->inmem_lock);\n\tget_page(page);\n\tlist_add_tail(&new->list, &fi->inmem_pages);\n\tinc_page_count(F2FS_I_SB(inode), F2FS_INMEM_PAGES);\n\tmutex_unlock(&fi->inmem_lock);\n\n\ttrace_f2fs_register_inmem_page(page, INMEM);\n}\n\nstatic int __revoke_inmem_pages(struct inode *inode,\n\t\t\t\tstruct list_head *head, bool drop, bool recover)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct inmem_pages *cur, *tmp;\n\tint err = 0;\n\n\tlist_for_each_entry_safe(cur, tmp, head, list) {\n\t\tstruct page *page = cur->page;\n\n\t\tif (drop)\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM_DROP);\n\n\t\tlock_page(page);\n\n\t\tif (recover) {\n\t\t\tstruct dnode_of_data dn;\n\t\t\tstruct node_info ni;\n\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM_REVOKE);\nretry:\n\t\t\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\t\t\terr = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);\n\t\t\tif (err) {\n\t\t\t\tif (err == -ENOMEM) {\n\t\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\t\t\tcond_resched();\n\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tget_node_info(sbi, dn.nid, &ni);\n\t\t\tf2fs_replace_block(sbi, &dn, dn.data_blkaddr,\n\t\t\t\t\tcur->old_addr, ni.version, true, true);\n\t\t\tf2fs_put_dnode(&dn);\n\t\t}\nnext:\n\t\t/* we don't need to invalidate this in the sccessful status */\n\t\tif (drop || recover)\n\t\t\tClearPageUptodate(page);\n\t\tset_page_private(page, 0);\n\t\tClearPagePrivate(page);\n\t\tf2fs_put_page(page, 1);\n\n\t\tlist_del(&cur->list);\n\t\tkmem_cache_free(inmem_entry_slab, cur);\n\t\tdec_page_count(F2FS_I_SB(inode), F2FS_INMEM_PAGES);\n\t}\n\treturn err;\n}\n\nvoid drop_inmem_pages(struct inode *inode)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tmutex_lock(&fi->inmem_lock);\n\t__revoke_inmem_pages(inode, &fi->inmem_pages, true, false);\n\tmutex_unlock(&fi->inmem_lock);\n\n\tclear_inode_flag(inode, FI_ATOMIC_FILE);\n\tclear_inode_flag(inode, FI_HOT_DATA);\n\tstat_dec_atomic_write(inode);\n}\n\nvoid drop_inmem_page(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct list_head *head = &fi->inmem_pages;\n\tstruct inmem_pages *cur = NULL;\n\n\tf2fs_bug_on(sbi, !IS_ATOMIC_WRITTEN_PAGE(page));\n\n\tmutex_lock(&fi->inmem_lock);\n\tlist_for_each_entry(cur, head, list) {\n\t\tif (cur->page == page)\n\t\t\tbreak;\n\t}\n\n\tf2fs_bug_on(sbi, !cur || cur->page != page);\n\tlist_del(&cur->list);\n\tmutex_unlock(&fi->inmem_lock);\n\n\tdec_page_count(sbi, F2FS_INMEM_PAGES);\n\tkmem_cache_free(inmem_entry_slab, cur);\n\n\tClearPageUptodate(page);\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n\tf2fs_put_page(page, 0);\n\n\ttrace_f2fs_commit_inmem_page(page, INMEM_INVALIDATE);\n}\n\nstatic int __commit_inmem_pages(struct inode *inode,\n\t\t\t\t\tstruct list_head *revoke_list)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inmem_pages *cur, *tmp;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_PRIO,\n\t\t.io_type = FS_DATA_IO,\n\t};\n\tpgoff_t last_idx = ULONG_MAX;\n\tint err = 0;\n\n\tlist_for_each_entry_safe(cur, tmp, &fi->inmem_pages, list) {\n\t\tstruct page *page = cur->page;\n\n\t\tlock_page(page);\n\t\tif (page->mapping == inode->i_mapping) {\n\t\t\ttrace_f2fs_commit_inmem_page(page, INMEM);\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_wait_on_page_writeback(page, DATA, true);\n\t\t\tif (clear_page_dirty_for_io(page)) {\n\t\t\t\tinode_dec_dirty_pages(inode);\n\t\t\t\tremove_dirty_inode(inode);\n\t\t\t}\nretry:\n\t\t\tfio.page = page;\n\t\t\tfio.old_blkaddr = NULL_ADDR;\n\t\t\tfio.encrypted_page = NULL;\n\t\t\tfio.need_lock = LOCK_DONE;\n\t\t\terr = do_write_data_page(&fio);\n\t\t\tif (err) {\n\t\t\t\tif (err == -ENOMEM) {\n\t\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\t\t\tcond_resched();\n\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t\tunlock_page(page);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* record old blkaddr for revoking */\n\t\t\tcur->old_addr = fio.old_blkaddr;\n\t\t\tlast_idx = page->index;\n\t\t}\n\t\tunlock_page(page);\n\t\tlist_move_tail(&cur->list, revoke_list);\n\t}\n\n\tif (last_idx != ULONG_MAX)\n\t\tf2fs_submit_merged_write_cond(sbi, inode, 0, last_idx, DATA);\n\n\tif (!err)\n\t\t__revoke_inmem_pages(inode, revoke_list, false, false);\n\n\treturn err;\n}\n\nint commit_inmem_pages(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct list_head revoke_list;\n\tint err;\n\n\tINIT_LIST_HEAD(&revoke_list);\n\tf2fs_balance_fs(sbi, true);\n\tf2fs_lock_op(sbi);\n\n\tset_inode_flag(inode, FI_ATOMIC_COMMIT);\n\n\tmutex_lock(&fi->inmem_lock);\n\terr = __commit_inmem_pages(inode, &revoke_list);\n\tif (err) {\n\t\tint ret;\n\t\t/*\n\t\t * try to revoke all committed pages, but still we could fail\n\t\t * due to no memory or other reason, if that happened, EAGAIN\n\t\t * will be returned, which means in such case, transaction is\n\t\t * already not integrity, caller should use journal to do the\n\t\t * recovery or rewrite & commit last transaction. For other\n\t\t * error number, revoking was done by filesystem itself.\n\t\t */\n\t\tret = __revoke_inmem_pages(inode, &revoke_list, false, true);\n\t\tif (ret)\n\t\t\terr = ret;\n\n\t\t/* drop all uncommitted pages */\n\t\t__revoke_inmem_pages(inode, &fi->inmem_pages, true, false);\n\t}\n\tmutex_unlock(&fi->inmem_lock);\n\n\tclear_inode_flag(inode, FI_ATOMIC_COMMIT);\n\n\tf2fs_unlock_op(sbi);\n\treturn err;\n}\n\n/*\n * This function balances dirty node and dentry pages.\n * In addition, it controls garbage collection.\n */\nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need)\n{\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (time_to_inject(sbi, FAULT_CHECKPOINT)) {\n\t\tf2fs_show_injection_info(FAULT_CHECKPOINT);\n\t\tf2fs_stop_checkpoint(sbi, false);\n\t}\n#endif\n\n\t/* balance_fs_bg is able to be pending */\n\tif (need && excess_cached_nats(sbi))\n\t\tf2fs_balance_fs_bg(sbi);\n\n\t/*\n\t * We should do GC or end up with checkpoint, if there are so many dirty\n\t * dir/node pages without enough free segments.\n\t */\n\tif (has_not_enough_free_secs(sbi, 0, 0)) {\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\tf2fs_gc(sbi, false, false, NULL_SEGNO);\n\t}\n}\n\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi)\n{\n\t/* try to shrink extent cache when there is no enough memory */\n\tif (!available_free_memory(sbi, EXTENT_CACHE))\n\t\tf2fs_shrink_extent_tree(sbi, EXTENT_CACHE_SHRINK_NUMBER);\n\n\t/* check the # of cached NAT entries */\n\tif (!available_free_memory(sbi, NAT_ENTRIES))\n\t\ttry_to_free_nats(sbi, NAT_ENTRY_PER_BLOCK);\n\n\tif (!available_free_memory(sbi, FREE_NIDS))\n\t\ttry_to_free_nids(sbi, MAX_FREE_NIDS);\n\telse\n\t\tbuild_free_nids(sbi, false, false);\n\n\tif (!is_idle(sbi) && !excess_dirty_nats(sbi))\n\t\treturn;\n\n\t/* checkpoint is the only way to shrink partial cached entries */\n\tif (!available_free_memory(sbi, NAT_ENTRIES) ||\n\t\t\t!available_free_memory(sbi, INO_ENTRIES) ||\n\t\t\texcess_prefree_segs(sbi) ||\n\t\t\texcess_dirty_nats(sbi) ||\n\t\t\tf2fs_time_over(sbi, CP_TIME)) {\n\t\tif (test_opt(sbi, DATA_FLUSH)) {\n\t\t\tstruct blk_plug plug;\n\n\t\t\tblk_start_plug(&plug);\n\t\t\tsync_dirty_inodes(sbi, FILE_INODE);\n\t\t\tblk_finish_plug(&plug);\n\t\t}\n\t\tf2fs_sync_fs(sbi->sb, true);\n\t\tstat_inc_bg_cp_count(sbi->stat_info);\n\t}\n}\n\nstatic int __submit_flush_wait(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev)\n{\n\tstruct bio *bio = f2fs_bio_alloc(0);\n\tint ret;\n\n\tbio->bi_opf = REQ_OP_WRITE | REQ_SYNC | REQ_PREFLUSH;\n\tbio_set_dev(bio, bdev);\n\tret = submit_bio_wait(bio);\n\tbio_put(bio);\n\n\ttrace_f2fs_issue_flush(bdev, test_opt(sbi, NOBARRIER),\n\t\t\t\ttest_opt(sbi, FLUSH_MERGE), ret);\n\treturn ret;\n}\n\nstatic int submit_flush_wait(struct f2fs_sb_info *sbi)\n{\n\tint ret = __submit_flush_wait(sbi, sbi->sb->s_bdev);\n\tint i;\n\n\tif (!sbi->s_ndevs || ret)\n\t\treturn ret;\n\n\tfor (i = 1; i < sbi->s_ndevs; i++) {\n\t\tret = __submit_flush_wait(sbi, FDEV(i).bdev);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int issue_flush_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\twait_queue_head_t *q = &fcc->flush_wait_queue;\nrepeat:\n\tif (kthread_should_stop())\n\t\treturn 0;\n\n\tsb_start_intwrite(sbi->sb);\n\n\tif (!llist_empty(&fcc->issue_list)) {\n\t\tstruct flush_cmd *cmd, *next;\n\t\tint ret;\n\n\t\tfcc->dispatch_list = llist_del_all(&fcc->issue_list);\n\t\tfcc->dispatch_list = llist_reverse_order(fcc->dispatch_list);\n\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_inc(&fcc->issued_flush);\n\n\t\tllist_for_each_entry_safe(cmd, next,\n\t\t\t\t\t  fcc->dispatch_list, llnode) {\n\t\t\tcmd->ret = ret;\n\t\t\tcomplete(&cmd->wait);\n\t\t}\n\t\tfcc->dispatch_list = NULL;\n\t}\n\n\tsb_end_intwrite(sbi->sb);\n\n\twait_event_interruptible(*q,\n\t\tkthread_should_stop() || !llist_empty(&fcc->issue_list));\n\tgoto repeat;\n}\n\nint f2fs_issue_flush(struct f2fs_sb_info *sbi)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\tstruct flush_cmd cmd;\n\tint ret;\n\n\tif (test_opt(sbi, NOBARRIER))\n\t\treturn 0;\n\n\tif (!test_opt(sbi, FLUSH_MERGE)) {\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tif (atomic_inc_return(&fcc->issing_flush) == 1) {\n\t\tret = submit_flush_wait(sbi);\n\t\tatomic_dec(&fcc->issing_flush);\n\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tinit_completion(&cmd.wait);\n\n\tllist_add(&cmd.llnode, &fcc->issue_list);\n\n\t/* update issue_list before we wake up issue_flush thread */\n\tsmp_mb();\n\n\tif (waitqueue_active(&fcc->flush_wait_queue))\n\t\twake_up(&fcc->flush_wait_queue);\n\n\tif (fcc->f2fs_issue_flush) {\n\t\twait_for_completion(&cmd.wait);\n\t\tatomic_dec(&fcc->issing_flush);\n\t} else {\n\t\tstruct llist_node *list;\n\n\t\tlist = llist_del_all(&fcc->issue_list);\n\t\tif (!list) {\n\t\t\twait_for_completion(&cmd.wait);\n\t\t\tatomic_dec(&fcc->issing_flush);\n\t\t} else {\n\t\t\tstruct flush_cmd *tmp, *next;\n\n\t\t\tret = submit_flush_wait(sbi);\n\n\t\t\tllist_for_each_entry_safe(tmp, next, list, llnode) {\n\t\t\t\tif (tmp == &cmd) {\n\t\t\t\t\tcmd.ret = ret;\n\t\t\t\t\tatomic_dec(&fcc->issing_flush);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\ttmp->ret = ret;\n\t\t\t\tcomplete(&tmp->wait);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn cmd.ret;\n}\n\nint create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\tint err = 0;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tif (fcc->f2fs_issue_flush)\n\t\t\treturn err;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = kzalloc(sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->issing_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\n\tif (!test_opt(sbi, FLUSH_MERGE))\n\t\treturn err;\n\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\terr = PTR_ERR(fcc->f2fs_issue_flush);\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nvoid destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\n\tif (fcc && fcc->f2fs_issue_flush) {\n\t\tstruct task_struct *flush_thread = fcc->f2fs_issue_flush;\n\n\t\tfcc->f2fs_issue_flush = NULL;\n\t\tkthread_stop(flush_thread);\n\t}\n\tif (free) {\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t}\n}\n\nstatic void __locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\t/* need not be added */\n\tif (IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]++;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (unlikely(t >= DIRTY)) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\treturn;\n\t\t}\n\t\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]++;\n\t}\n}\n\nstatic void __remove_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]--;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]--;\n\n\t\tif (get_valid_blocks(sbi, segno, true) == 0)\n\t\t\tclear_bit(GET_SEC_FROM_SEG(sbi, segno),\n\t\t\t\t\t\tdirty_i->victim_secmap);\n\t}\n}\n\n/*\n * Should not occur error such as -ENOMEM.\n * Adding dirty entry into seglist is not critical operation.\n * If a given segment is one of current working segments, it won't be added.\n */\nstatic void locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned short valid_blocks;\n\n\tif (segno == NULL_SEGNO || IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\n\tif (valid_blocks == 0) {\n\t\t__locate_dirty_segment(sbi, segno, PRE);\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t} else if (valid_blocks < sbi->blocks_per_seg) {\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t} else {\n\t\t/* Recovery routine with SSR needs this */\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t}\n\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic struct discard_cmd *__create_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t lstart,\n\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc;\n\n\tf2fs_bug_on(sbi, !len);\n\n\tpend_list = &dcc->pend_list[plist_idx(len)];\n\n\tdc = f2fs_kmem_cache_alloc(discard_cmd_slab, GFP_NOFS);\n\tINIT_LIST_HEAD(&dc->list);\n\tdc->bdev = bdev;\n\tdc->lstart = lstart;\n\tdc->start = start;\n\tdc->len = len;\n\tdc->ref = 0;\n\tdc->state = D_PREP;\n\tdc->error = 0;\n\tinit_completion(&dc->wait);\n\tlist_add_tail(&dc->list, pend_list);\n\tatomic_inc(&dcc->discard_cmd_cnt);\n\tdcc->undiscard_blks += len;\n\n\treturn dc;\n}\n\nstatic struct discard_cmd *__attach_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len,\n\t\t\t\tstruct rb_node *parent, struct rb_node **p)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\n\tdc = __create_discard_cmd(sbi, bdev, lstart, start, len);\n\n\trb_link_node(&dc->rb_node, parent, p);\n\trb_insert_color(&dc->rb_node, &dcc->root);\n\n\treturn dc;\n}\n\nstatic void __detach_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tif (dc->state == D_DONE)\n\t\tatomic_dec(&dcc->issing_discard);\n\n\tlist_del(&dc->list);\n\trb_erase(&dc->rb_node, &dcc->root);\n\tdcc->undiscard_blks -= dc->len;\n\n\tkmem_cache_free(discard_cmd_slab, dc);\n\n\tatomic_dec(&dcc->discard_cmd_cnt);\n}\n\nstatic void __remove_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tf2fs_bug_on(sbi, dc->ref);\n\n\tif (dc->error == -EOPNOTSUPP)\n\t\tdc->error = 0;\n\n\tif (dc->error)\n\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"Issue discard(%u, %u, %u) failed, ret: %d\",\n\t\t\tdc->lstart, dc->start, dc->len, dc->error);\n\t__detach_discard_cmd(dcc, dc);\n}\n\nstatic void f2fs_submit_discard_endio(struct bio *bio)\n{\n\tstruct discard_cmd *dc = (struct discard_cmd *)bio->bi_private;\n\n\tdc->error = blk_status_to_errno(bio->bi_status);\n\tdc->state = D_DONE;\n\tcomplete_all(&dc->wait);\n\tbio_put(bio);\n}\n\nvoid __check_sit_bitmap(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t start, block_t end)\n{\n#ifdef CONFIG_F2FS_CHECK_FS\n\tstruct seg_entry *sentry;\n\tunsigned int segno;\n\tblock_t blk = start;\n\tunsigned long offset, size, max_blocks = sbi->blocks_per_seg;\n\tunsigned long *map;\n\n\twhile (blk < end) {\n\t\tsegno = GET_SEGNO(sbi, blk);\n\t\tsentry = get_seg_entry(sbi, segno);\n\t\toffset = GET_BLKOFF_FROM_SEG0(sbi, blk);\n\n\t\tif (end < START_BLOCK(sbi, segno + 1))\n\t\t\tsize = GET_BLKOFF_FROM_SEG0(sbi, end);\n\t\telse\n\t\t\tsize = max_blocks;\n\t\tmap = (unsigned long *)(sentry->cur_valid_map);\n\t\toffset = __find_rev_next_bit(map, size, offset);\n\t\tf2fs_bug_on(sbi, offset != size);\n\t\tblk = START_BLOCK(sbi, segno + 1);\n\t}\n#endif\n}\n\n/* this function is copied from blkdev_issue_discard from block/blk-lib.c */\nstatic void __submit_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct bio *bio = NULL;\n\n\tif (dc->state != D_PREP)\n\t\treturn;\n\n\ttrace_f2fs_issue_discard(dc->bdev, dc->start, dc->len);\n\n\tdc->error = __blkdev_issue_discard(dc->bdev,\n\t\t\t\tSECTOR_FROM_BLOCK(dc->start),\n\t\t\t\tSECTOR_FROM_BLOCK(dc->len),\n\t\t\t\tGFP_NOFS, 0, &bio);\n\tif (!dc->error) {\n\t\t/* should keep before submission to avoid D_DONE right away */\n\t\tdc->state = D_SUBMIT;\n\t\tatomic_inc(&dcc->issued_discard);\n\t\tatomic_inc(&dcc->issing_discard);\n\t\tif (bio) {\n\t\t\tbio->bi_private = dc;\n\t\t\tbio->bi_end_io = f2fs_submit_discard_endio;\n\t\t\tbio->bi_opf |= REQ_SYNC;\n\t\t\tsubmit_bio(bio);\n\t\t\tlist_move_tail(&dc->list, &dcc->wait_list);\n\t\t\t__check_sit_bitmap(sbi, dc->start, dc->start + dc->len);\n\n\t\t\tf2fs_update_iostat(sbi, FS_DISCARD, 1);\n\t\t}\n\t} else {\n\t\t__remove_discard_cmd(sbi, dc);\n\t}\n}\n\nstatic struct discard_cmd *__insert_discard_tree(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len,\n\t\t\t\tstruct rb_node **insert_p,\n\t\t\t\tstruct rb_node *insert_parent)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct rb_node **p = &dcc->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct discard_cmd *dc = NULL;\n\n\tif (insert_p && insert_parent) {\n\t\tparent = insert_parent;\n\t\tp = insert_p;\n\t\tgoto do_insert;\n\t}\n\n\tp = __lookup_rb_tree_for_insert(sbi, &dcc->root, &parent, lstart);\ndo_insert:\n\tdc = __attach_discard_cmd(sbi, bdev, lstart, start, len, parent, p);\n\tif (!dc)\n\t\treturn NULL;\n\n\treturn dc;\n}\n\nstatic void __relocate_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tlist_move_tail(&dc->list, &dcc->pend_list[plist_idx(dc->len)]);\n}\n\nstatic void __punch_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_info di = dc->di;\n\tbool modified = false;\n\n\tif (dc->state == D_DONE || dc->len == 1) {\n\t\t__remove_discard_cmd(sbi, dc);\n\t\treturn;\n\t}\n\n\tdcc->undiscard_blks -= di.len;\n\n\tif (blkaddr > di.lstart) {\n\t\tdc->len = blkaddr - dc->lstart;\n\t\tdcc->undiscard_blks += dc->len;\n\t\t__relocate_discard_cmd(dcc, dc);\n\t\tmodified = true;\n\t}\n\n\tif (blkaddr < di.lstart + di.len - 1) {\n\t\tif (modified) {\n\t\t\t__insert_discard_tree(sbi, dc->bdev, blkaddr + 1,\n\t\t\t\t\tdi.start + blkaddr + 1 - di.lstart,\n\t\t\t\t\tdi.lstart + di.len - 1 - blkaddr,\n\t\t\t\t\tNULL, NULL);\n\t\t} else {\n\t\t\tdc->lstart++;\n\t\t\tdc->len--;\n\t\t\tdc->start++;\n\t\t\tdcc->undiscard_blks += dc->len;\n\t\t\t__relocate_discard_cmd(dcc, dc);\n\t\t}\n\t}\n}\n\nstatic void __update_discard_tree_range(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *prev_dc = NULL, *next_dc = NULL;\n\tstruct discard_cmd *dc;\n\tstruct discard_info di = {0};\n\tstruct rb_node **insert_p = NULL, *insert_parent = NULL;\n\tblock_t end = lstart + len;\n\n\tmutex_lock(&dcc->cmd_lock);\n\n\tdc = (struct discard_cmd *)__lookup_rb_tree_ret(&dcc->root,\n\t\t\t\t\tNULL, lstart,\n\t\t\t\t\t(struct rb_entry **)&prev_dc,\n\t\t\t\t\t(struct rb_entry **)&next_dc,\n\t\t\t\t\t&insert_p, &insert_parent, true);\n\tif (dc)\n\t\tprev_dc = dc;\n\n\tif (!prev_dc) {\n\t\tdi.lstart = lstart;\n\t\tdi.len = next_dc ? next_dc->lstart - lstart : len;\n\t\tdi.len = min(di.len, len);\n\t\tdi.start = start;\n\t}\n\n\twhile (1) {\n\t\tstruct rb_node *node;\n\t\tbool merged = false;\n\t\tstruct discard_cmd *tdc = NULL;\n\n\t\tif (prev_dc) {\n\t\t\tdi.lstart = prev_dc->lstart + prev_dc->len;\n\t\t\tif (di.lstart < lstart)\n\t\t\t\tdi.lstart = lstart;\n\t\t\tif (di.lstart >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (!next_dc || next_dc->lstart > end)\n\t\t\t\tdi.len = end - di.lstart;\n\t\t\telse\n\t\t\t\tdi.len = next_dc->lstart - di.lstart;\n\t\t\tdi.start = start + di.lstart - lstart;\n\t\t}\n\n\t\tif (!di.len)\n\t\t\tgoto next;\n\n\t\tif (prev_dc && prev_dc->state == D_PREP &&\n\t\t\tprev_dc->bdev == bdev &&\n\t\t\t__is_discard_back_mergeable(&di, &prev_dc->di)) {\n\t\t\tprev_dc->di.len += di.len;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, prev_dc);\n\t\t\tdi = prev_dc->di;\n\t\t\ttdc = prev_dc;\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (next_dc && next_dc->state == D_PREP &&\n\t\t\tnext_dc->bdev == bdev &&\n\t\t\t__is_discard_front_mergeable(&di, &next_dc->di)) {\n\t\t\tnext_dc->di.lstart = di.lstart;\n\t\t\tnext_dc->di.len += di.len;\n\t\t\tnext_dc->di.start = di.start;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, next_dc);\n\t\t\tif (tdc)\n\t\t\t\t__remove_discard_cmd(sbi, tdc);\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (!merged) {\n\t\t\t__insert_discard_tree(sbi, bdev, di.lstart, di.start,\n\t\t\t\t\t\t\tdi.len, NULL, NULL);\n\t\t}\n next:\n\t\tprev_dc = next_dc;\n\t\tif (!prev_dc)\n\t\t\tbreak;\n\n\t\tnode = rb_next(&prev_dc->rb_node);\n\t\tnext_dc = rb_entry_safe(node, struct discard_cmd, rb_node);\n\t}\n\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic int __queue_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tblock_t lblkstart = blkstart;\n\n\ttrace_f2fs_queue_discard(bdev, blkstart, blklen);\n\n\tif (sbi->s_ndevs) {\n\t\tint devi = f2fs_target_device_index(sbi, blkstart);\n\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\t__update_discard_tree_range(sbi, bdev, lblkstart, blkstart, blklen);\n\treturn 0;\n}\n\nstatic int __issue_discard_cmd(struct f2fs_sb_info *sbi, bool issue_cond)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc, *tmp;\n\tstruct blk_plug plug;\n\tint iter = 0, issued = 0;\n\tint i;\n\tbool io_interrupted = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tf2fs_bug_on(sbi,\n\t\t!__check_rb_tree_consistence(sbi, &dcc->root));\n\tblk_start_plug(&plug);\n\tfor (i = MAX_PLIST_NUM - 1;\n\t\t\ti >= 0 && plist_issue(dcc->pend_list_tag[i]); i--) {\n\t\tpend_list = &dcc->pend_list[i];\n\t\tlist_for_each_entry_safe(dc, tmp, pend_list, list) {\n\t\t\tf2fs_bug_on(sbi, dc->state != D_PREP);\n\n\t\t\t/* Hurry up to finish fstrim */\n\t\t\tif (dcc->pend_list_tag[i] & P_TRIM) {\n\t\t\t\t__submit_discard_cmd(sbi, dc);\n\t\t\t\tissued++;\n\n\t\t\t\tif (fatal_signal_pending(current))\n\t\t\t\t\tbreak;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!issue_cond) {\n\t\t\t\t__submit_discard_cmd(sbi, dc);\n\t\t\t\tissued++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (is_idle(sbi)) {\n\t\t\t\t__submit_discard_cmd(sbi, dc);\n\t\t\t\tissued++;\n\t\t\t} else {\n\t\t\t\tio_interrupted = true;\n\t\t\t}\n\n\t\t\tif (++iter >= DISCARD_ISSUE_RATE)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (list_empty(pend_list) && dcc->pend_list_tag[i] & P_TRIM)\n\t\t\tdcc->pend_list_tag[i] &= (~P_TRIM);\n\t}\nout:\n\tblk_finish_plug(&plug);\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (!issued && io_interrupted)\n\t\tissued = -1;\n\n\treturn issued;\n}\n\nstatic void __drop_discard_cmd(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc, *tmp;\n\tint i;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tfor (i = MAX_PLIST_NUM - 1; i >= 0; i--) {\n\t\tpend_list = &dcc->pend_list[i];\n\t\tlist_for_each_entry_safe(dc, tmp, pend_list, list) {\n\t\t\tf2fs_bug_on(sbi, dc->state != D_PREP);\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic void __wait_one_discard_bio(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\twait_for_completion_io(&dc->wait);\n\tmutex_lock(&dcc->cmd_lock);\n\tf2fs_bug_on(sbi, dc->state != D_DONE);\n\tdc->ref--;\n\tif (!dc->ref)\n\t\t__remove_discard_cmd(sbi, dc);\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic void __wait_discard_cmd(struct f2fs_sb_info *sbi, bool wait_cond)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *wait_list = &(dcc->wait_list);\n\tstruct discard_cmd *dc, *tmp;\n\tbool need_wait;\n\nnext:\n\tneed_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tlist_for_each_entry_safe(dc, tmp, wait_list, list) {\n\t\tif (!wait_cond || (dc->state == D_DONE && !dc->ref)) {\n\t\t\twait_for_completion_io(&dc->wait);\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait) {\n\t\t__wait_one_discard_bio(sbi, dc);\n\t\tgoto next;\n\t}\n}\n\n/* This should be covered by global mutex, &sit_i->sentry_lock */\nvoid f2fs_wait_discard_bio(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\tbool need_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tdc = (struct discard_cmd *)__lookup_rb_tree(&dcc->root, NULL, blkaddr);\n\tif (dc) {\n\t\tif (dc->state == D_PREP) {\n\t\t\t__punch_discard_cmd(sbi, dc, blkaddr);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait)\n\t\t__wait_one_discard_bio(sbi, dc);\n}\n\nvoid stop_discard_thread(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (dcc && dcc->f2fs_issue_discard) {\n\t\tstruct task_struct *discard_thread = dcc->f2fs_issue_discard;\n\n\t\tdcc->f2fs_issue_discard = NULL;\n\t\tkthread_stop(discard_thread);\n\t}\n}\n\n/* This comes from f2fs_put_super and f2fs_trim_fs */\nvoid f2fs_wait_discard_bios(struct f2fs_sb_info *sbi, bool umount)\n{\n\t__issue_discard_cmd(sbi, false);\n\t__drop_discard_cmd(sbi);\n\t__wait_discard_cmd(sbi, !umount);\n}\n\nstatic void mark_discard_range_all(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tint i;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tfor (i = 0; i < MAX_PLIST_NUM; i++)\n\t\tdcc->pend_list_tag[i] |= P_TRIM;\n\tmutex_unlock(&dcc->cmd_lock);\n}\n\nstatic int issue_discard_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\twait_queue_head_t *q = &dcc->discard_wait_queue;\n\tunsigned int wait_ms = DEF_MIN_DISCARD_ISSUE_TIME;\n\tint issued;\n\n\tset_freezable();\n\n\tdo {\n\t\twait_event_interruptible_timeout(*q,\n\t\t\t\tkthread_should_stop() || freezing(current) ||\n\t\t\t\tdcc->discard_wake,\n\t\t\t\tmsecs_to_jiffies(wait_ms));\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\t\tif (kthread_should_stop())\n\t\t\treturn 0;\n\n\t\tif (dcc->discard_wake) {\n\t\t\tdcc->discard_wake = 0;\n\t\t\tif (sbi->gc_thread && sbi->gc_thread->gc_urgent)\n\t\t\t\tmark_discard_range_all(sbi);\n\t\t}\n\n\t\tsb_start_intwrite(sbi->sb);\n\n\t\tissued = __issue_discard_cmd(sbi, true);\n\t\tif (issued) {\n\t\t\t__wait_discard_cmd(sbi, true);\n\t\t\twait_ms = DEF_MIN_DISCARD_ISSUE_TIME;\n\t\t} else {\n\t\t\twait_ms = DEF_MAX_DISCARD_ISSUE_TIME;\n\t\t}\n\n\t\tsb_end_intwrite(sbi->sb);\n\n\t} while (!kthread_should_stop());\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int __f2fs_issue_discard_zone(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tsector_t sector, nr_sects;\n\tblock_t lblkstart = blkstart;\n\tint devi = 0;\n\n\tif (sbi->s_ndevs) {\n\t\tdevi = f2fs_target_device_index(sbi, blkstart);\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\n\t/*\n\t * We need to know the type of the zone: for conventional zones,\n\t * use regular discard if the drive supports it. For sequential\n\t * zones, reset the zone write pointer.\n\t */\n\tswitch (get_blkz_type(sbi, bdev, blkstart)) {\n\n\tcase BLK_ZONE_TYPE_CONVENTIONAL:\n\t\tif (!blk_queue_discard(bdev_get_queue(bdev)))\n\t\t\treturn 0;\n\t\treturn __queue_discard_cmd(sbi, bdev, lblkstart, blklen);\n\tcase BLK_ZONE_TYPE_SEQWRITE_REQ:\n\tcase BLK_ZONE_TYPE_SEQWRITE_PREF:\n\t\tsector = SECTOR_FROM_BLOCK(blkstart);\n\t\tnr_sects = SECTOR_FROM_BLOCK(blklen);\n\n\t\tif (sector & (bdev_zone_sectors(bdev) - 1) ||\n\t\t\t\tnr_sects != bdev_zone_sectors(bdev)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\t\"(%d) %s: Unaligned discard attempted (block %x + %x)\",\n\t\t\t\tdevi, sbi->s_ndevs ? FDEV(devi).path: \"\",\n\t\t\t\tblkstart, blklen);\n\t\t\treturn -EIO;\n\t\t}\n\t\ttrace_f2fs_issue_reset_zone(bdev, blkstart);\n\t\treturn blkdev_reset_zones(bdev, sector,\n\t\t\t\t\t  nr_sects, GFP_NOFS);\n\tdefault:\n\t\t/* Unknown zone type: broken device ? */\n\t\treturn -EIO;\n\t}\n}\n#endif\n\nstatic int __issue_discard_async(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_mounted_blkzoned(sbi->sb) &&\n\t\t\t\tbdev_zoned_model(bdev) != BLK_ZONED_NONE)\n\t\treturn __f2fs_issue_discard_zone(sbi, bdev, blkstart, blklen);\n#endif\n\treturn __queue_discard_cmd(sbi, bdev, blkstart, blklen);\n}\n\nstatic int f2fs_issue_discard(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blkstart, block_t blklen)\n{\n\tsector_t start = blkstart, len = 0;\n\tstruct block_device *bdev;\n\tstruct seg_entry *se;\n\tunsigned int offset;\n\tblock_t i;\n\tint err = 0;\n\n\tbdev = f2fs_target_device(sbi, blkstart, NULL);\n\n\tfor (i = blkstart; i < blkstart + blklen; i++, len++) {\n\t\tif (i != start) {\n\t\t\tstruct block_device *bdev2 =\n\t\t\t\tf2fs_target_device(sbi, i, NULL);\n\n\t\t\tif (bdev2 != bdev) {\n\t\t\t\terr = __issue_discard_async(sbi, bdev,\n\t\t\t\t\t\tstart, len);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tbdev = bdev2;\n\t\t\t\tstart = i;\n\t\t\t\tlen = 0;\n\t\t\t}\n\t\t}\n\n\t\tse = get_seg_entry(sbi, GET_SEGNO(sbi, i));\n\t\toffset = GET_BLKOFF_FROM_SEG0(sbi, i);\n\n\t\tif (!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\t}\n\n\tif (len)\n\t\terr = __issue_discard_async(sbi, bdev, start, len);\n\treturn err;\n}\n\nstatic bool add_discard_addrs(struct f2fs_sb_info *sbi, struct cp_control *cpc,\n\t\t\t\t\t\t\tbool check_only)\n{\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tint max_blocks = sbi->blocks_per_seg;\n\tstruct seg_entry *se = get_seg_entry(sbi, cpc->trim_start);\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *discard_map = (unsigned long *)se->discard_map;\n\tunsigned long *dmap = SIT_I(sbi)->tmp_map;\n\tunsigned int start = 0, end = -1;\n\tbool force = (cpc->reason & CP_DISCARD);\n\tstruct discard_entry *de = NULL;\n\tstruct list_head *head = &SM_I(sbi)->dcc_info->entry_list;\n\tint i;\n\n\tif (se->valid_blocks == max_blocks || !f2fs_discard_en(sbi))\n\t\treturn false;\n\n\tif (!force) {\n\t\tif (!test_opt(sbi, DISCARD) || !se->valid_blocks ||\n\t\t\tSM_I(sbi)->dcc_info->nr_discards >=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards)\n\t\t\treturn false;\n\t}\n\n\t/* SIT_VBLOCK_MAP_SIZE should be multiple of sizeof(unsigned long) */\n\tfor (i = 0; i < entries; i++)\n\t\tdmap[i] = force ? ~ckpt_map[i] & ~discard_map[i] :\n\t\t\t\t(cur_map[i] ^ ckpt_map[i]) & ckpt_map[i];\n\n\twhile (force || SM_I(sbi)->dcc_info->nr_discards <=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards) {\n\t\tstart = __find_rev_next_bit(dmap, max_blocks, end + 1);\n\t\tif (start >= max_blocks)\n\t\t\tbreak;\n\n\t\tend = __find_rev_next_zero_bit(dmap, max_blocks, start + 1);\n\t\tif (force && start && end != max_blocks\n\t\t\t\t\t&& (end - start) < cpc->trim_minlen)\n\t\t\tcontinue;\n\n\t\tif (check_only)\n\t\t\treturn true;\n\n\t\tif (!de) {\n\t\t\tde = f2fs_kmem_cache_alloc(discard_entry_slab,\n\t\t\t\t\t\t\t\tGFP_F2FS_ZERO);\n\t\t\tde->start_blkaddr = START_BLOCK(sbi, cpc->trim_start);\n\t\t\tlist_add_tail(&de->list, head);\n\t\t}\n\n\t\tfor (i = start; i < end; i++)\n\t\t\t__set_bit_le(i, (void *)de->discard_map);\n\n\t\tSM_I(sbi)->dcc_info->nr_discards += end - start;\n\t}\n\treturn false;\n}\n\nvoid release_discard_addrs(struct f2fs_sb_info *sbi)\n{\n\tstruct list_head *head = &(SM_I(sbi)->dcc_info->entry_list);\n\tstruct discard_entry *entry, *this;\n\n\t/* drop caches */\n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tlist_del(&entry->list);\n\t\tkmem_cache_free(discard_entry_slab, entry);\n\t}\n}\n\n/*\n * Should call clear_prefree_segments after checkpoint is done.\n */\nstatic void set_prefree_as_free_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor_each_set_bit(segno, dirty_i->dirty_segmap[PRE], MAIN_SEGS(sbi))\n\t\t__set_test_and_free(sbi, segno);\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nvoid clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *head = &dcc->entry_list;\n\tstruct discard_entry *entry, *this;\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned long *prefree_map = dirty_i->dirty_segmap[PRE];\n\tunsigned int start = 0, end = -1;\n\tunsigned int secno, start_segno;\n\tbool force = (cpc->reason & CP_DISCARD);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\twhile (1) {\n\t\tint i;\n\t\tstart = find_next_bit(prefree_map, MAIN_SEGS(sbi), end + 1);\n\t\tif (start >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\tend = find_next_zero_bit(prefree_map, MAIN_SEGS(sbi),\n\t\t\t\t\t\t\t\tstart + 1);\n\n\t\tfor (i = start; i < end; i++)\n\t\t\tclear_bit(i, prefree_map);\n\n\t\tdirty_i->nr_dirty[PRE] -= end - start;\n\n\t\tif (!test_opt(sbi, DISCARD))\n\t\t\tcontinue;\n\n\t\tif (force && start >= cpc->trim_start &&\n\t\t\t\t\t(end - 1) <= cpc->trim_end)\n\t\t\t\tcontinue;\n\n\t\tif (!test_opt(sbi, LFS) || sbi->segs_per_sec == 1) {\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start),\n\t\t\t\t(end - start) << sbi->log_blocks_per_seg);\n\t\t\tcontinue;\n\t\t}\nnext:\n\t\tsecno = GET_SEC_FROM_SEG(sbi, start);\n\t\tstart_segno = GET_SEG_FROM_SEC(sbi, secno);\n\t\tif (!IS_CURSEC(sbi, secno) &&\n\t\t\t!get_valid_blocks(sbi, start, true))\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start_segno),\n\t\t\t\tsbi->segs_per_sec << sbi->log_blocks_per_seg);\n\n\t\tstart = start_segno + sbi->segs_per_sec;\n\t\tif (start < end)\n\t\t\tgoto next;\n\t\telse\n\t\t\tend = start - 1;\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\t/* send small discards */\n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tunsigned int cur_pos = 0, next_pos, len, total_len = 0;\n\t\tbool is_valid = test_bit_le(0, entry->discard_map);\n\nfind_next:\n\t\tif (is_valid) {\n\t\t\tnext_pos = find_next_zero_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t\tlen = next_pos - cur_pos;\n\n\t\t\tif (f2fs_sb_mounted_blkzoned(sbi->sb) ||\n\t\t\t    (force && len < cpc->trim_minlen))\n\t\t\t\tgoto skip;\n\n\t\t\tf2fs_issue_discard(sbi, entry->start_blkaddr + cur_pos,\n\t\t\t\t\t\t\t\t\tlen);\n\t\t\tcpc->trimmed += len;\n\t\t\ttotal_len += len;\n\t\t} else {\n\t\t\tnext_pos = find_next_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t}\nskip:\n\t\tcur_pos = next_pos;\n\t\tis_valid = !is_valid;\n\n\t\tif (cur_pos < sbi->blocks_per_seg)\n\t\t\tgoto find_next;\n\n\t\tlist_del(&entry->list);\n\t\tdcc->nr_discards -= total_len;\n\t\tkmem_cache_free(discard_entry_slab, entry);\n\t}\n\n\twake_up_discard_thread(sbi, false);\n}\n\nstatic int create_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct discard_cmd_control *dcc;\n\tint err = 0, i;\n\n\tif (SM_I(sbi)->dcc_info) {\n\t\tdcc = SM_I(sbi)->dcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tdcc = kzalloc(sizeof(struct discard_cmd_control), GFP_KERNEL);\n\tif (!dcc)\n\t\treturn -ENOMEM;\n\n\tdcc->discard_granularity = DEFAULT_DISCARD_GRANULARITY;\n\tINIT_LIST_HEAD(&dcc->entry_list);\n\tfor (i = 0; i < MAX_PLIST_NUM; i++) {\n\t\tINIT_LIST_HEAD(&dcc->pend_list[i]);\n\t\tif (i >= dcc->discard_granularity - 1)\n\t\t\tdcc->pend_list_tag[i] |= P_ACTIVE;\n\t}\n\tINIT_LIST_HEAD(&dcc->wait_list);\n\tmutex_init(&dcc->cmd_lock);\n\tatomic_set(&dcc->issued_discard, 0);\n\tatomic_set(&dcc->issing_discard, 0);\n\tatomic_set(&dcc->discard_cmd_cnt, 0);\n\tdcc->nr_discards = 0;\n\tdcc->max_discards = MAIN_SEGS(sbi) << sbi->log_blocks_per_seg;\n\tdcc->undiscard_blks = 0;\n\tdcc->root = RB_ROOT;\n\n\tinit_waitqueue_head(&dcc->discard_wait_queue);\n\tSM_I(sbi)->dcc_info = dcc;\ninit_thread:\n\tdcc->f2fs_issue_discard = kthread_run(issue_discard_thread, sbi,\n\t\t\t\t\"f2fs_discard-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(dcc->f2fs_issue_discard)) {\n\t\terr = PTR_ERR(dcc->f2fs_issue_discard);\n\t\tkfree(dcc);\n\t\tSM_I(sbi)->dcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nstatic void destroy_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (!dcc)\n\t\treturn;\n\n\tstop_discard_thread(sbi);\n\n\tkfree(dcc);\n\tSM_I(sbi)->dcc_info = NULL;\n}\n\nstatic bool __mark_sit_entry_dirty(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tif (!__test_and_set_bit(segno, sit_i->dirty_sentries_bitmap)) {\n\t\tsit_i->dirty_sentries++;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void __set_sit_entry_type(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tunsigned int segno, int modified)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, segno);\n\tse->type = type;\n\tif (modified)\n\t\t__mark_sit_entry_dirty(sbi, segno);\n}\n\nstatic void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)\n{\n\tstruct seg_entry *se;\n\tunsigned int segno, offset;\n\tlong int new_vblocks;\n\tbool exist;\n#ifdef CONFIG_F2FS_CHECK_FS\n\tbool mir_exist;\n#endif\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\n\tse = get_seg_entry(sbi, segno);\n\tnew_vblocks = se->valid_blocks + del;\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tf2fs_bug_on(sbi, (new_vblocks >> (sizeof(unsigned short) << 3) ||\n\t\t\t\t(new_vblocks > sbi->blocks_per_seg)));\n\n\tse->valid_blocks = new_vblocks;\n\tse->mtime = get_mtime(sbi);\n\tSIT_I(sbi)->max_mtime = se->mtime;\n\n\t/* Update valid block bitmap */\n\tif (del > 0) {\n\t\texist = f2fs_test_and_set_bit(offset, se->cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tmir_exist = f2fs_test_and_set_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir);\n\t\tif (unlikely(exist != mir_exist)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR, \"Inconsistent error \"\n\t\t\t\t\"when setting bitmap, blk:%u, old bit:%d\",\n\t\t\t\tblkaddr, exist);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n#endif\n\t\tif (unlikely(exist)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\"Bitmap was wrongly set, blk:%u\", blkaddr);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tse->valid_blocks--;\n\t\t\tdel = 0;\n\t\t}\n\n\t\tif (f2fs_discard_en(sbi) &&\n\t\t\t!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\n\t\t/* don't overwrite by SSR to keep node chain */\n\t\tif (se->type == CURSEG_WARM_NODE) {\n\t\t\tif (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))\n\t\t\t\tse->ckpt_valid_blocks++;\n\t\t}\n\t} else {\n\t\texist = f2fs_test_and_clear_bit(offset, se->cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tmir_exist = f2fs_test_and_clear_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir);\n\t\tif (unlikely(exist != mir_exist)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR, \"Inconsistent error \"\n\t\t\t\t\"when clearing bitmap, blk:%u, old bit:%d\",\n\t\t\t\tblkaddr, exist);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n#endif\n\t\tif (unlikely(!exist)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\"Bitmap was wrongly cleared, blk:%u\", blkaddr);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tse->valid_blocks++;\n\t\t\tdel = 0;\n\t\t}\n\n\t\tif (f2fs_discard_en(sbi) &&\n\t\t\tf2fs_test_and_clear_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks++;\n\t}\n\tif (!f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tse->ckpt_valid_blocks += del;\n\n\t__mark_sit_entry_dirty(sbi, segno);\n\n\t/* update total number of valid blocks to be written in ckpt area */\n\tSIT_I(sbi)->written_valid_blocks += del;\n\n\tif (sbi->segs_per_sec > 1)\n\t\tget_sec_entry(sbi, segno)->valid_blocks += del;\n}\n\nvoid refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new)\n{\n\tupdate_sit_entry(sbi, new, 1);\n\tif (GET_SEGNO(sbi, old) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old, -1);\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new));\n}\n\nvoid invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr)\n{\n\tunsigned int segno = GET_SEGNO(sbi, addr);\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tf2fs_bug_on(sbi, addr == NULL_ADDR);\n\tif (addr == NEW_ADDR)\n\t\treturn;\n\n\t/* add it into sit main buffer */\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tupdate_sit_entry(sbi, addr, -1);\n\n\t/* add it into dirty seglist */\n\tlocate_dirty_segment(sbi, segno);\n\n\tmutex_unlock(&sit_i->sentry_lock);\n}\n\nbool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno, offset;\n\tstruct seg_entry *se;\n\tbool is_cp = false;\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn true;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tif (f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tis_cp = true;\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\treturn is_cp;\n}\n\n/*\n * This function should be resided under the curseg_mutex lock\n */\nstatic void __add_sum_entry(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tstruct f2fs_summary *sum)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tvoid *addr = curseg->sum_blk;\n\taddr += curseg->next_blkoff * sizeof(struct f2fs_summary);\n\tmemcpy(addr, sum, sizeof(struct f2fs_summary));\n}\n\n/*\n * Calculate the number of current summary pages for writing\n */\nint npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra)\n{\n\tint valid_sum_count = 0;\n\tint i, sum_in_page;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tif (sbi->ckpt->alloc_type[i] == SSR)\n\t\t\tvalid_sum_count += sbi->blocks_per_seg;\n\t\telse {\n\t\t\tif (for_ra)\n\t\t\t\tvalid_sum_count += le16_to_cpu(\n\t\t\t\t\tF2FS_CKPT(sbi)->cur_data_blkoff[i]);\n\t\t\telse\n\t\t\t\tvalid_sum_count += curseg_blkoff(sbi, i);\n\t\t}\n\t}\n\n\tsum_in_page = (PAGE_SIZE - 2 * SUM_JOURNAL_SIZE -\n\t\t\tSUM_FOOTER_SIZE) / SUMMARY_SIZE;\n\tif (valid_sum_count <= sum_in_page)\n\t\treturn 1;\n\telse if ((valid_sum_count - sum_in_page) <=\n\t\t(PAGE_SIZE - SUM_FOOTER_SIZE) / SUMMARY_SIZE)\n\t\treturn 2;\n\treturn 3;\n}\n\n/*\n * Caller should put this summary page\n */\nstruct page *get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\treturn get_meta_page(sbi, GET_SUM_BLOCK(sbi, segno));\n}\n\nvoid update_meta_page(struct f2fs_sb_info *sbi, void *src, block_t blk_addr)\n{\n\tstruct page *page = grab_meta_page(sbi, blk_addr);\n\tvoid *dst = page_address(page);\n\n\tif (src)\n\t\tmemcpy(dst, src, PAGE_SIZE);\n\telse\n\t\tmemset(dst, 0, PAGE_SIZE);\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic void write_sum_page(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_summary_block *sum_blk, block_t blk_addr)\n{\n\tupdate_meta_page(sbi, (void *)sum_blk, blk_addr);\n}\n\nstatic void write_current_sum_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, block_t blk_addr)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct page *page = grab_meta_page(sbi, blk_addr);\n\tstruct f2fs_summary_block *src = curseg->sum_blk;\n\tstruct f2fs_summary_block *dst;\n\n\tdst = (struct f2fs_summary_block *)page_address(page);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\n\tdown_read(&curseg->journal_rwsem);\n\tmemcpy(&dst->journal, curseg->journal, SUM_JOURNAL_SIZE);\n\tup_read(&curseg->journal_rwsem);\n\n\tmemcpy(dst->entries, src->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&dst->footer, &src->footer, SUM_FOOTER_SIZE);\n\n\tmutex_unlock(&curseg->curseg_mutex);\n\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic int is_next_segment_free(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno = curseg->segno + 1;\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\n\tif (segno < MAIN_SEGS(sbi) && segno % sbi->segs_per_sec)\n\t\treturn !test_bit(segno, free_i->free_segmap);\n\treturn 0;\n}\n\n/*\n * Find a new segment from the free segments bitmap to right order\n * This function should be returned with success, otherwise BUG\n */\nstatic void get_new_segment(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int *newseg, bool new_sec, int dir)\n{\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno, secno, zoneno;\n\tunsigned int total_zones = MAIN_SECS(sbi) / sbi->secs_per_zone;\n\tunsigned int hint = GET_SEC_FROM_SEG(sbi, *newseg);\n\tunsigned int old_zoneno = GET_ZONE_FROM_SEG(sbi, *newseg);\n\tunsigned int left_start = hint;\n\tbool init = true;\n\tint go_left = 0;\n\tint i;\n\n\tspin_lock(&free_i->segmap_lock);\n\n\tif (!new_sec && ((*newseg + 1) % sbi->segs_per_sec)) {\n\t\tsegno = find_next_zero_bit(free_i->free_segmap,\n\t\t\tGET_SEG_FROM_SEC(sbi, hint + 1), *newseg + 1);\n\t\tif (segno < GET_SEG_FROM_SEC(sbi, hint + 1))\n\t\t\tgoto got_it;\n\t}\nfind_other_zone:\n\tsecno = find_next_zero_bit(free_i->free_secmap, MAIN_SECS(sbi), hint);\n\tif (secno >= MAIN_SECS(sbi)) {\n\t\tif (dir == ALLOC_RIGHT) {\n\t\t\tsecno = find_next_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi), 0);\n\t\t\tf2fs_bug_on(sbi, secno >= MAIN_SECS(sbi));\n\t\t} else {\n\t\t\tgo_left = 1;\n\t\t\tleft_start = hint - 1;\n\t\t}\n\t}\n\tif (go_left == 0)\n\t\tgoto skip_left;\n\n\twhile (test_bit(left_start, free_i->free_secmap)) {\n\t\tif (left_start > 0) {\n\t\t\tleft_start--;\n\t\t\tcontinue;\n\t\t}\n\t\tleft_start = find_next_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi), 0);\n\t\tf2fs_bug_on(sbi, left_start >= MAIN_SECS(sbi));\n\t\tbreak;\n\t}\n\tsecno = left_start;\nskip_left:\n\thint = secno;\n\tsegno = GET_SEG_FROM_SEC(sbi, secno);\n\tzoneno = GET_ZONE_FROM_SEC(sbi, secno);\n\n\t/* give up on finding another zone */\n\tif (!init)\n\t\tgoto got_it;\n\tif (sbi->secs_per_zone == 1)\n\t\tgoto got_it;\n\tif (zoneno == old_zoneno)\n\t\tgoto got_it;\n\tif (dir == ALLOC_LEFT) {\n\t\tif (!go_left && zoneno + 1 >= total_zones)\n\t\t\tgoto got_it;\n\t\tif (go_left && zoneno == 0)\n\t\t\tgoto got_it;\n\t}\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++)\n\t\tif (CURSEG_I(sbi, i)->zone == zoneno)\n\t\t\tbreak;\n\n\tif (i < NR_CURSEG_TYPE) {\n\t\t/* zone is in user, try another */\n\t\tif (go_left)\n\t\t\thint = zoneno * sbi->secs_per_zone - 1;\n\t\telse if (zoneno + 1 >= total_zones)\n\t\t\thint = 0;\n\t\telse\n\t\t\thint = (zoneno + 1) * sbi->secs_per_zone;\n\t\tinit = false;\n\t\tgoto find_other_zone;\n\t}\ngot_it:\n\t/* set it as dirty segment in free segmap */\n\tf2fs_bug_on(sbi, test_bit(segno, free_i->free_segmap));\n\t__set_inuse(sbi, segno);\n\t*newseg = segno;\n\tspin_unlock(&free_i->segmap_lock);\n}\n\nstatic void reset_curseg(struct f2fs_sb_info *sbi, int type, int modified)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct summary_footer *sum_footer;\n\n\tcurseg->segno = curseg->next_segno;\n\tcurseg->zone = GET_ZONE_FROM_SEG(sbi, curseg->segno);\n\tcurseg->next_blkoff = 0;\n\tcurseg->next_segno = NULL_SEGNO;\n\n\tsum_footer = &(curseg->sum_blk->footer);\n\tmemset(sum_footer, 0, sizeof(struct summary_footer));\n\tif (IS_DATASEG(type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_DATA);\n\tif (IS_NODESEG(type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_NODE);\n\t__set_sit_entry_type(sbi, type, curseg->segno, modified);\n}\n\nstatic unsigned int __get_next_segno(struct f2fs_sb_info *sbi, int type)\n{\n\t/* if segs_per_sec is large than 1, we need to keep original policy. */\n\tif (sbi->segs_per_sec != 1)\n\t\treturn CURSEG_I(sbi, type)->segno;\n\n\tif (type == CURSEG_HOT_DATA || IS_NODESEG(type))\n\t\treturn 0;\n\n\tif (SIT_I(sbi)->last_victim[ALLOC_NEXT])\n\t\treturn SIT_I(sbi)->last_victim[ALLOC_NEXT];\n\treturn CURSEG_I(sbi, type)->segno;\n}\n\n/*\n * Allocate a current working segment.\n * This function always allocates a free segment in LFS manner.\n */\nstatic void new_curseg(struct f2fs_sb_info *sbi, int type, bool new_sec)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno = curseg->segno;\n\tint dir = ALLOC_LEFT;\n\n\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, segno));\n\tif (type == CURSEG_WARM_DATA || type == CURSEG_COLD_DATA)\n\t\tdir = ALLOC_RIGHT;\n\n\tif (test_opt(sbi, NOHEAP))\n\t\tdir = ALLOC_RIGHT;\n\n\tsegno = __get_next_segno(sbi, type);\n\tget_new_segment(sbi, &segno, new_sec, dir);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = LFS;\n}\n\nstatic void __next_free_blkoff(struct f2fs_sb_info *sbi,\n\t\t\tstruct curseg_info *seg, block_t start)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, seg->segno);\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tunsigned long *target_map = SIT_I(sbi)->tmp_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tint i, pos;\n\n\tfor (i = 0; i < entries; i++)\n\t\ttarget_map[i] = ckpt_map[i] | cur_map[i];\n\n\tpos = __find_rev_next_zero_bit(target_map, sbi->blocks_per_seg, start);\n\n\tseg->next_blkoff = pos;\n}\n\n/*\n * If a segment is written by LFS manner, next block offset is just obtained\n * by increasing the current block offset. However, if a segment is written by\n * SSR manner, next block offset obtained by calling __next_free_blkoff\n */\nstatic void __refresh_next_blkoff(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct curseg_info *seg)\n{\n\tif (seg->alloc_type == SSR)\n\t\t__next_free_blkoff(sbi, seg, seg->next_blkoff + 1);\n\telse\n\t\tseg->next_blkoff++;\n}\n\n/*\n * This function always allocates a used segment(from dirty seglist) by SSR\n * manner, so it should recover the existing segment information of valid blocks\n */\nstatic void change_curseg(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int new_segno = curseg->next_segno;\n\tstruct f2fs_summary_block *sum_node;\n\tstruct page *sum_page;\n\n\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, curseg->segno));\n\t__set_test_and_inuse(sbi, new_segno);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\t__remove_dirty_segment(sbi, new_segno, PRE);\n\t__remove_dirty_segment(sbi, new_segno, DIRTY);\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = SSR;\n\t__next_free_blkoff(sbi, curseg, 0);\n\n\tsum_page = get_sum_page(sbi, new_segno);\n\tsum_node = (struct f2fs_summary_block *)page_address(sum_page);\n\tmemcpy(curseg->sum_blk, sum_node, SUM_ENTRY_SIZE);\n\tf2fs_put_page(sum_page, 1);\n}\n\nstatic int get_ssr_segment(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tconst struct victim_selection *v_ops = DIRTY_I(sbi)->v_ops;\n\tunsigned segno = NULL_SEGNO;\n\tint i, cnt;\n\tbool reversed = false;\n\n\t/* need_SSR() already forces to do this */\n\tif (v_ops->get_victim(sbi, &segno, BG_GC, type, SSR)) {\n\t\tcurseg->next_segno = segno;\n\t\treturn 1;\n\t}\n\n\t/* For node segments, let's do SSR more intensively */\n\tif (IS_NODESEG(type)) {\n\t\tif (type >= CURSEG_WARM_NODE) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_NODE;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_NODE;\n\t\t}\n\t\tcnt = NR_CURSEG_NODE_TYPE;\n\t} else {\n\t\tif (type >= CURSEG_WARM_DATA) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_DATA;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_DATA;\n\t\t}\n\t\tcnt = NR_CURSEG_DATA_TYPE;\n\t}\n\n\tfor (; cnt-- > 0; reversed ? i-- : i++) {\n\t\tif (i == type)\n\t\t\tcontinue;\n\t\tif (v_ops->get_victim(sbi, &segno, BG_GC, i, SSR)) {\n\t\t\tcurseg->next_segno = segno;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * flush out current segment and replace it with new segment\n * This function should be returned with success, otherwise BUG\n */\nstatic void allocate_segment_by_default(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, bool force)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tif (force)\n\t\tnew_curseg(sbi, type, true);\n\telse if (!is_set_ckpt_flags(sbi, CP_CRC_RECOVERY_FLAG) &&\n\t\t\t\t\ttype == CURSEG_WARM_NODE)\n\t\tnew_curseg(sbi, type, false);\n\telse if (curseg->alloc_type == LFS && is_next_segment_free(sbi, type))\n\t\tnew_curseg(sbi, type, false);\n\telse if (need_SSR(sbi) && get_ssr_segment(sbi, type))\n\t\tchange_curseg(sbi, type);\n\telse\n\t\tnew_curseg(sbi, type, false);\n\n\tstat_inc_seg_type(sbi, curseg);\n}\n\nvoid allocate_new_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg;\n\tunsigned int old_segno;\n\tint i;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tcurseg = CURSEG_I(sbi, i);\n\t\told_segno = curseg->segno;\n\t\tSIT_I(sbi)->s_ops->allocate_segment(sbi, i, true);\n\t\tlocate_dirty_segment(sbi, old_segno);\n\t}\n}\n\nstatic const struct segment_allocation default_salloc_ops = {\n\t.allocate_segment = allocate_segment_by_default,\n};\n\nbool exist_trim_candidates(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\t__u64 trim_start = cpc->trim_start;\n\tbool has_candidate = false;\n\n\tmutex_lock(&SIT_I(sbi)->sentry_lock);\n\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++) {\n\t\tif (add_discard_addrs(sbi, cpc, true)) {\n\t\t\thas_candidate = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&SIT_I(sbi)->sentry_lock);\n\n\tcpc->trim_start = trim_start;\n\treturn has_candidate;\n}\n\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range)\n{\n\t__u64 start = F2FS_BYTES_TO_BLK(range->start);\n\t__u64 end = start + F2FS_BYTES_TO_BLK(range->len) - 1;\n\tunsigned int start_segno, end_segno;\n\tstruct cp_control cpc;\n\tint err = 0;\n\n\tif (start >= MAX_BLKADDR(sbi) || range->len < sbi->blocksize)\n\t\treturn -EINVAL;\n\n\tcpc.trimmed = 0;\n\tif (end <= MAIN_BLKADDR(sbi))\n\t\tgoto out;\n\n\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\"Found FS corruption, run fsck to fix.\");\n\t\tgoto out;\n\t}\n\n\t/* start/end segment number in main_area */\n\tstart_segno = (start <= MAIN_BLKADDR(sbi)) ? 0 : GET_SEGNO(sbi, start);\n\tend_segno = (end >= MAX_BLKADDR(sbi)) ? MAIN_SEGS(sbi) - 1 :\n\t\t\t\t\t\tGET_SEGNO(sbi, end);\n\tcpc.reason = CP_DISCARD;\n\tcpc.trim_minlen = max_t(__u64, 1, F2FS_BYTES_TO_BLK(range->minlen));\n\n\t/* do checkpoint to issue discard commands safely */\n\tfor (; start_segno <= end_segno; start_segno = cpc.trim_end + 1) {\n\t\tcpc.trim_start = start_segno;\n\n\t\tif (sbi->discard_blks == 0)\n\t\t\tbreak;\n\t\telse if (sbi->discard_blks < BATCHED_TRIM_BLOCKS(sbi))\n\t\t\tcpc.trim_end = end_segno;\n\t\telse\n\t\t\tcpc.trim_end = min_t(unsigned int,\n\t\t\t\trounddown(start_segno +\n\t\t\t\tBATCHED_TRIM_SEGMENTS(sbi),\n\t\t\t\tsbi->segs_per_sec) - 1, end_segno);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\t/* It's time to issue all the filed discards */\n\tmark_discard_range_all(sbi);\n\tf2fs_wait_discard_bios(sbi, false);\nout:\n\trange->len = F2FS_BLK_TO_BYTES(cpc.trimmed);\n\treturn err;\n}\n\nstatic bool __has_curseg_space(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tif (curseg->next_blkoff < sbi->blocks_per_seg)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int __get_segment_type_2(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA)\n\t\treturn CURSEG_HOT_DATA;\n\telse\n\t\treturn CURSEG_HOT_NODE;\n}\n\nstatic int __get_segment_type_4(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (S_ISDIR(inode->i_mode))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\telse\n\t\t\treturn CURSEG_COLD_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page) && is_cold_node(fio->page))\n\t\t\treturn CURSEG_WARM_NODE;\n\t\telse\n\t\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type_6(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (is_cold_data(fio->page) || file_is_cold(inode))\n\t\t\treturn CURSEG_COLD_DATA;\n\t\tif (is_inode_flag_set(inode, FI_HOT_DATA))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\treturn CURSEG_WARM_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page))\n\t\t\treturn is_cold_node(fio->page) ? CURSEG_WARM_NODE :\n\t\t\t\t\t\tCURSEG_HOT_NODE;\n\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type(struct f2fs_io_info *fio)\n{\n\tint type = 0;\n\n\tswitch (fio->sbi->active_logs) {\n\tcase 2:\n\t\ttype = __get_segment_type_2(fio);\n\t\tbreak;\n\tcase 4:\n\t\ttype = __get_segment_type_4(fio);\n\t\tbreak;\n\tcase 6:\n\t\ttype = __get_segment_type_6(fio);\n\t\tbreak;\n\tdefault:\n\t\tf2fs_bug_on(fio->sbi, true);\n\t}\n\n\tif (IS_HOT(type))\n\t\tfio->temp = HOT;\n\telse if (IS_WARM(type))\n\t\tfio->temp = WARM;\n\telse\n\t\tfio->temp = COLD;\n\treturn type;\n}\n\nvoid allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\tstruct f2fs_summary *sum, int type,\n\t\tstruct f2fs_io_info *fio, bool add_list)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tmutex_lock(&sit_i->sentry_lock);\n\n\t*new_blkaddr = NEXT_FREE_BLKADDR(sbi, curseg);\n\n\tf2fs_wait_discard_bio(sbi, *new_blkaddr);\n\n\t/*\n\t * __add_sum_entry should be resided under the curseg_mutex\n\t * because, this function updates a summary entry in the\n\t * current summary block.\n\t */\n\t__add_sum_entry(sbi, type, sum);\n\n\t__refresh_next_blkoff(sbi, curseg);\n\n\tstat_inc_block_count(sbi, curseg);\n\n\tif (!__has_curseg_space(sbi, type))\n\t\tsit_i->s_ops->allocate_segment(sbi, type, false);\n\t/*\n\t * SIT information should be updated after segment allocation,\n\t * since we need to keep dirty segments precisely under SSR.\n\t */\n\trefresh_sit_entry(sbi, old_blkaddr, *new_blkaddr);\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\tif (page && IS_NODESEG(type)) {\n\t\tfill_node_footer_blkaddr(page, NEXT_FREE_BLKADDR(sbi, curseg));\n\n\t\tf2fs_inode_chksum_set(sbi, page);\n\t}\n\n\tif (add_list) {\n\t\tstruct f2fs_bio_info *io;\n\n\t\tINIT_LIST_HEAD(&fio->list);\n\t\tfio->in_list = true;\n\t\tio = sbi->write_io[fio->type] + fio->temp;\n\t\tspin_lock(&io->io_lock);\n\t\tlist_add_tail(&fio->list, &io->io_list);\n\t\tspin_unlock(&io->io_lock);\n\t}\n\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nstatic void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)\n{\n\tint type = __get_segment_type(fio);\n\tint err;\n\nreallocate:\n\tallocate_data_block(fio->sbi, fio->page, fio->old_blkaddr,\n\t\t\t&fio->new_blkaddr, sum, type, fio, true);\n\n\t/* writeout dirty page into bdev */\n\terr = f2fs_submit_page_write(fio);\n\tif (err == -EAGAIN) {\n\t\tfio->old_blkaddr = fio->new_blkaddr;\n\t\tgoto reallocate;\n\t}\n}\n\nvoid write_meta_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = META,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_META | REQ_PRIO,\n\t\t.old_blkaddr = page->index,\n\t\t.new_blkaddr = page->index,\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t\t.in_list = false,\n\t};\n\n\tif (unlikely(page->index >= MAIN_BLKADDR(sbi)))\n\t\tfio.op_flags &= ~REQ_META;\n\n\tset_page_writeback(page);\n\tf2fs_submit_page_write(&fio);\n\n\tf2fs_update_iostat(sbi, io_type, F2FS_BLKSIZE);\n}\n\nvoid write_node_page(unsigned int nid, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, nid, 0, 0);\n\tdo_write_page(&sum, fio);\n\n\tf2fs_update_iostat(fio->sbi, fio->io_type, F2FS_BLKSIZE);\n}\n\nvoid write_data_page(struct dnode_of_data *dn, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\n\tf2fs_bug_on(sbi, dn->data_blkaddr == NULL_ADDR);\n\tget_node_info(sbi, dn->nid, &ni);\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\tdo_write_page(&sum, fio);\n\tf2fs_update_data_blkaddr(dn, fio->new_blkaddr);\n\n\tf2fs_update_iostat(sbi, fio->io_type, F2FS_BLKSIZE);\n}\n\nint rewrite_data_page(struct f2fs_io_info *fio)\n{\n\tint err;\n\n\tfio->new_blkaddr = fio->old_blkaddr;\n\tstat_inc_inplace_blocks(fio->sbi);\n\n\terr = f2fs_submit_page_bio(fio);\n\n\tf2fs_update_iostat(fio->sbi, fio->io_type, F2FS_BLKSIZE);\n\n\treturn err;\n}\n\nvoid __f2fs_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\t\tbool recover_curseg, bool recover_newaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg;\n\tunsigned int segno, old_cursegno;\n\tstruct seg_entry *se;\n\tint type;\n\tunsigned short old_blkoff;\n\n\tsegno = GET_SEGNO(sbi, new_blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\ttype = se->type;\n\n\tif (!recover_curseg) {\n\t\t/* for recovery flow */\n\t\tif (se->valid_blocks == 0 && !IS_CURSEG(sbi, segno)) {\n\t\t\tif (old_blkaddr == NULL_ADDR)\n\t\t\t\ttype = CURSEG_COLD_DATA;\n\t\t\telse\n\t\t\t\ttype = CURSEG_WARM_DATA;\n\t\t}\n\t} else {\n\t\tif (!IS_CURSEG(sbi, segno))\n\t\t\ttype = CURSEG_WARM_DATA;\n\t}\n\n\tcurseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tmutex_lock(&sit_i->sentry_lock);\n\n\told_cursegno = curseg->segno;\n\told_blkoff = curseg->next_blkoff;\n\n\t/* change the current segment */\n\tif (segno != curseg->segno) {\n\t\tcurseg->next_segno = segno;\n\t\tchange_curseg(sbi, type);\n\t}\n\n\tcurseg->next_blkoff = GET_BLKOFF_FROM_SEG0(sbi, new_blkaddr);\n\t__add_sum_entry(sbi, type, sum);\n\n\tif (!recover_curseg || recover_newaddr)\n\t\tupdate_sit_entry(sbi, new_blkaddr, 1);\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old_blkaddr, -1);\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old_blkaddr));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new_blkaddr));\n\n\tlocate_dirty_segment(sbi, old_cursegno);\n\n\tif (recover_curseg) {\n\t\tif (old_cursegno != curseg->segno) {\n\t\t\tcurseg->next_segno = old_cursegno;\n\t\t\tchange_curseg(sbi, type);\n\t\t}\n\t\tcurseg->next_blkoff = old_blkoff;\n\t}\n\n\tmutex_unlock(&sit_i->sentry_lock);\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\t\tunsigned char version, bool recover_curseg,\n\t\t\t\tbool recover_newaddr)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, version);\n\n\t__f2fs_replace_block(sbi, &sum, old_addr, new_addr,\n\t\t\t\t\trecover_curseg, recover_newaddr);\n\n\tf2fs_update_data_blkaddr(dn, new_addr);\n}\n\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\t\tenum page_type type, bool ordered)\n{\n\tif (PageWriteback(page)) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_P_SB(page);\n\n\t\tf2fs_submit_merged_write_cond(sbi, page->mapping->host,\n\t\t\t\t\t\t0, page->index, type);\n\t\tif (ordered)\n\t\t\twait_on_page_writeback(page);\n\t\telse\n\t\t\twait_for_stable_page(page);\n\t}\n}\n\nvoid f2fs_wait_on_block_writeback(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct page *cpage;\n\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn;\n\n\tcpage = find_lock_page(META_MAPPING(sbi), blkaddr);\n\tif (cpage) {\n\t\tf2fs_wait_on_page_writeback(cpage, DATA, true);\n\t\tf2fs_put_page(cpage, 1);\n\t}\n}\n\nstatic int read_compacted_summaries(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct curseg_info *seg_i;\n\tunsigned char *kaddr;\n\tstruct page *page;\n\tblock_t start;\n\tint i, j, offset;\n\n\tstart = start_sum_block(sbi);\n\n\tpage = get_meta_page(sbi, start++);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t/* Step 1: restore nat cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(seg_i->journal, kaddr, SUM_JOURNAL_SIZE);\n\n\t/* Step 2: restore sit cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(seg_i->journal, kaddr + SUM_JOURNAL_SIZE, SUM_JOURNAL_SIZE);\n\toffset = 2 * SUM_JOURNAL_SIZE;\n\n\t/* Step 3: restore summary entries */\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blk_off;\n\t\tunsigned int segno;\n\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[i]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[i]);\n\t\tseg_i->next_segno = segno;\n\t\treset_curseg(sbi, i, 0);\n\t\tseg_i->alloc_type = ckpt->alloc_type[i];\n\t\tseg_i->next_blkoff = blk_off;\n\n\t\tif (seg_i->alloc_type == SSR)\n\t\t\tblk_off = sbi->blocks_per_seg;\n\n\t\tfor (j = 0; j < blk_off; j++) {\n\t\t\tstruct f2fs_summary *s;\n\t\t\ts = (struct f2fs_summary *)(kaddr + offset);\n\t\t\tseg_i->sum_blk->entries[j] = *s;\n\t\t\toffset += SUMMARY_SIZE;\n\t\t\tif (offset + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\n\t\t\tpage = get_meta_page(sbi, start++);\n\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\toffset = 0;\n\t\t}\n\t}\n\tf2fs_put_page(page, 1);\n\treturn 0;\n}\n\nstatic int read_normal_summaries(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_summary_block *sum;\n\tstruct curseg_info *curseg;\n\tstruct page *new;\n\tunsigned short blk_off;\n\tunsigned int segno = 0;\n\tblock_t blk_addr = 0;\n\n\t/* get segment number and block addr */\n\tif (IS_DATASEG(type)) {\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[type]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_DATA]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_TYPE, type);\n\t\telse\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_DATA_TYPE, type);\n\t} else {\n\t\tsegno = le32_to_cpu(ckpt->cur_node_segno[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_node_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_NODE_TYPE,\n\t\t\t\t\t\t\ttype - CURSEG_HOT_NODE);\n\t\telse\n\t\t\tblk_addr = GET_SUM_BLOCK(sbi, segno);\n\t}\n\n\tnew = get_meta_page(sbi, blk_addr);\n\tsum = (struct f2fs_summary_block *)page_address(new);\n\n\tif (IS_NODESEG(type)) {\n\t\tif (__exist_node_summaries(sbi)) {\n\t\t\tstruct f2fs_summary *ns = &sum->entries[0];\n\t\t\tint i;\n\t\t\tfor (i = 0; i < sbi->blocks_per_seg; i++, ns++) {\n\t\t\t\tns->version = 0;\n\t\t\t\tns->ofs_in_node = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tint err;\n\n\t\t\terr = restore_node_summary(sbi, segno, sum);\n\t\t\tif (err) {\n\t\t\t\tf2fs_put_page(new, 1);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* set uncompleted segment to curseg */\n\tcurseg = CURSEG_I(sbi, type);\n\tmutex_lock(&curseg->curseg_mutex);\n\n\t/* update journal info */\n\tdown_write(&curseg->journal_rwsem);\n\tmemcpy(curseg->journal, &sum->journal, SUM_JOURNAL_SIZE);\n\tup_write(&curseg->journal_rwsem);\n\n\tmemcpy(curseg->sum_blk->entries, sum->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&curseg->sum_blk->footer, &sum->footer, SUM_FOOTER_SIZE);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 0);\n\tcurseg->alloc_type = ckpt->alloc_type[type];\n\tcurseg->next_blkoff = blk_off;\n\tmutex_unlock(&curseg->curseg_mutex);\n\tf2fs_put_page(new, 1);\n\treturn 0;\n}\n\nstatic int restore_curseg_summaries(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_journal *sit_j = CURSEG_I(sbi, CURSEG_COLD_DATA)->journal;\n\tstruct f2fs_journal *nat_j = CURSEG_I(sbi, CURSEG_HOT_DATA)->journal;\n\tint type = CURSEG_HOT_DATA;\n\tint err;\n\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG)) {\n\t\tint npages = npages_for_summary_flush(sbi, true);\n\n\t\tif (npages >= 2)\n\t\t\tra_meta_pages(sbi, start_sum_block(sbi), npages,\n\t\t\t\t\t\t\tMETA_CP, true);\n\n\t\t/* restore for compacted data summary */\n\t\tif (read_compacted_summaries(sbi))\n\t\t\treturn -EINVAL;\n\t\ttype = CURSEG_HOT_NODE;\n\t}\n\n\tif (__exist_node_summaries(sbi))\n\t\tra_meta_pages(sbi, sum_blk_addr(sbi, NR_CURSEG_TYPE, type),\n\t\t\t\t\tNR_CURSEG_TYPE - type, META_CP, true);\n\n\tfor (; type <= CURSEG_COLD_NODE; type++) {\n\t\terr = read_normal_summaries(sbi, type);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* sanity check for summary blocks */\n\tif (nats_in_cursum(nat_j) > NAT_JOURNAL_ENTRIES ||\n\t\t\tsits_in_cursum(sit_j) > SIT_JOURNAL_ENTRIES)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void write_compacted_summaries(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct page *page;\n\tunsigned char *kaddr;\n\tstruct f2fs_summary *summary;\n\tstruct curseg_info *seg_i;\n\tint written_size = 0;\n\tint i, j;\n\n\tpage = grab_meta_page(sbi, blkaddr++);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t/* Step 1: write nat cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(kaddr, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t/* Step 2: write sit cache */\n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(kaddr + written_size, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t/* Step 3: write summary entries */\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blkoff;\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tif (sbi->ckpt->alloc_type[i] == SSR)\n\t\t\tblkoff = sbi->blocks_per_seg;\n\t\telse\n\t\t\tblkoff = curseg_blkoff(sbi, i);\n\n\t\tfor (j = 0; j < blkoff; j++) {\n\t\t\tif (!page) {\n\t\t\t\tpage = grab_meta_page(sbi, blkaddr++);\n\t\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\t\twritten_size = 0;\n\t\t\t}\n\t\t\tsummary = (struct f2fs_summary *)(kaddr + written_size);\n\t\t\t*summary = seg_i->sum_blk->entries[j];\n\t\t\twritten_size += SUMMARY_SIZE;\n\n\t\t\tif (written_size + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\t\t}\n\t}\n\tif (page) {\n\t\tset_page_dirty(page);\n\t\tf2fs_put_page(page, 1);\n\t}\n}\n\nstatic void write_normal_summaries(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type)\n{\n\tint i, end;\n\tif (IS_DATASEG(type))\n\t\tend = type + NR_CURSEG_DATA_TYPE;\n\telse\n\t\tend = type + NR_CURSEG_NODE_TYPE;\n\n\tfor (i = type; i < end; i++)\n\t\twrite_current_sum_page(sbi, i, blkaddr + (i - type));\n}\n\nvoid write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG))\n\t\twrite_compacted_summaries(sbi, start_blk);\n\telse\n\t\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_DATA);\n}\n\nvoid write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_NODE);\n}\n\nint lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\t\t\tunsigned int val, int alloc)\n{\n\tint i;\n\n\tif (type == NAT_JOURNAL) {\n\t\tfor (i = 0; i < nats_in_cursum(journal); i++) {\n\t\t\tif (le32_to_cpu(nid_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\t}\n\t\tif (alloc && __has_cursum_space(journal, 1, NAT_JOURNAL))\n\t\t\treturn update_nats_in_cursum(journal, 1);\n\t} else if (type == SIT_JOURNAL) {\n\t\tfor (i = 0; i < sits_in_cursum(journal); i++)\n\t\t\tif (le32_to_cpu(segno_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\tif (alloc && __has_cursum_space(journal, 1, SIT_JOURNAL))\n\t\t\treturn update_sits_in_cursum(journal, 1);\n\t}\n\treturn -1;\n}\n\nstatic struct page *get_current_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int segno)\n{\n\treturn get_meta_page(sbi, current_sit_addr(sbi, segno));\n}\n\nstatic struct page *get_next_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int start)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct page *src_page, *dst_page;\n\tpgoff_t src_off, dst_off;\n\tvoid *src_addr, *dst_addr;\n\n\tsrc_off = current_sit_addr(sbi, start);\n\tdst_off = next_sit_addr(sbi, src_off);\n\n\t/* get current sit block page without lock */\n\tsrc_page = get_meta_page(sbi, src_off);\n\tdst_page = grab_meta_page(sbi, dst_off);\n\tf2fs_bug_on(sbi, PageDirty(src_page));\n\n\tsrc_addr = page_address(src_page);\n\tdst_addr = page_address(dst_page);\n\tmemcpy(dst_addr, src_addr, PAGE_SIZE);\n\n\tset_page_dirty(dst_page);\n\tf2fs_put_page(src_page, 1);\n\n\tset_to_next_sit(sit_i, start);\n\n\treturn dst_page;\n}\n\nstatic struct sit_entry_set *grab_sit_entry_set(void)\n{\n\tstruct sit_entry_set *ses =\n\t\t\tf2fs_kmem_cache_alloc(sit_entry_set_slab, GFP_NOFS);\n\n\tses->entry_cnt = 0;\n\tINIT_LIST_HEAD(&ses->set_list);\n\treturn ses;\n}\n\nstatic void release_sit_entry_set(struct sit_entry_set *ses)\n{\n\tlist_del(&ses->set_list);\n\tkmem_cache_free(sit_entry_set_slab, ses);\n}\n\nstatic void adjust_sit_entry_set(struct sit_entry_set *ses,\n\t\t\t\t\t\tstruct list_head *head)\n{\n\tstruct sit_entry_set *next = ses;\n\n\tif (list_is_last(&ses->set_list, head))\n\t\treturn;\n\n\tlist_for_each_entry_continue(next, head, set_list)\n\t\tif (ses->entry_cnt <= next->entry_cnt)\n\t\t\tbreak;\n\n\tlist_move_tail(&ses->set_list, &next->set_list);\n}\n\nstatic void add_sit_entry(unsigned int segno, struct list_head *head)\n{\n\tstruct sit_entry_set *ses;\n\tunsigned int start_segno = START_SEGNO(segno);\n\n\tlist_for_each_entry(ses, head, set_list) {\n\t\tif (ses->start_segno == start_segno) {\n\t\t\tses->entry_cnt++;\n\t\t\tadjust_sit_entry_set(ses, head);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tses = grab_sit_entry_set();\n\n\tses->start_segno = start_segno;\n\tses->entry_cnt++;\n\tlist_add(&ses->set_list, head);\n}\n\nstatic void add_sits_in_set(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\tstruct list_head *set_list = &sm_info->sit_entry_set;\n\tunsigned long *bitmap = SIT_I(sbi)->dirty_sentries_bitmap;\n\tunsigned int segno;\n\n\tfor_each_set_bit(segno, bitmap, MAIN_SEGS(sbi))\n\t\tadd_sit_entry(segno, set_list);\n}\n\nstatic void remove_sits_in_journal(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tint i;\n\n\tdown_write(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int segno;\n\t\tbool dirtied;\n\n\t\tsegno = le32_to_cpu(segno_in_journal(journal, i));\n\t\tdirtied = __mark_sit_entry_dirty(sbi, segno);\n\n\t\tif (!dirtied)\n\t\t\tadd_sit_entry(segno, &SM_I(sbi)->sit_entry_set);\n\t}\n\tupdate_sits_in_cursum(journal, -i);\n\tup_write(&curseg->journal_rwsem);\n}\n\n/*\n * CP calls this function, which flushes SIT entries including sit_journal,\n * and moves prefree segs to free segs.\n */\nvoid flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned long *bitmap = sit_i->dirty_sentries_bitmap;\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct sit_entry_set *ses, *tmp;\n\tstruct list_head *head = &SM_I(sbi)->sit_entry_set;\n\tbool to_journal = true;\n\tstruct seg_entry *se;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tif (!sit_i->dirty_sentries)\n\t\tgoto out;\n\n\t/*\n\t * add and account sit entries of dirty bitmap in sit entry\n\t * set temporarily\n\t */\n\tadd_sits_in_set(sbi);\n\n\t/*\n\t * if there are no enough space in journal to store dirty sit\n\t * entries, remove all entries from journal and add and account\n\t * them in sit entry set.\n\t */\n\tif (!__has_cursum_space(journal, sit_i->dirty_sentries, SIT_JOURNAL))\n\t\tremove_sits_in_journal(sbi);\n\n\t/*\n\t * there are two steps to flush sit entries:\n\t * #1, flush sit entries to journal in current cold data summary block.\n\t * #2, flush sit entries to sit page.\n\t */\n\tlist_for_each_entry_safe(ses, tmp, head, set_list) {\n\t\tstruct page *page = NULL;\n\t\tstruct f2fs_sit_block *raw_sit = NULL;\n\t\tunsigned int start_segno = ses->start_segno;\n\t\tunsigned int end = min(start_segno + SIT_ENTRY_PER_BLOCK,\n\t\t\t\t\t\t(unsigned long)MAIN_SEGS(sbi));\n\t\tunsigned int segno = start_segno;\n\n\t\tif (to_journal &&\n\t\t\t!__has_cursum_space(journal, ses->entry_cnt, SIT_JOURNAL))\n\t\t\tto_journal = false;\n\n\t\tif (to_journal) {\n\t\t\tdown_write(&curseg->journal_rwsem);\n\t\t} else {\n\t\t\tpage = get_next_sit_page(sbi, start_segno);\n\t\t\traw_sit = page_address(page);\n\t\t}\n\n\t\t/* flush dirty sit entries in region of current sit set */\n\t\tfor_each_set_bit_from(segno, bitmap, end) {\n\t\t\tint offset, sit_offset;\n\n\t\t\tse = get_seg_entry(sbi, segno);\n\n\t\t\t/* add discard candidates */\n\t\t\tif (!(cpc->reason & CP_DISCARD)) {\n\t\t\t\tcpc->trim_start = segno;\n\t\t\t\tadd_discard_addrs(sbi, cpc, false);\n\t\t\t}\n\n\t\t\tif (to_journal) {\n\t\t\t\toffset = lookup_journal_in_cursum(journal,\n\t\t\t\t\t\t\tSIT_JOURNAL, segno, 1);\n\t\t\t\tf2fs_bug_on(sbi, offset < 0);\n\t\t\t\tsegno_in_journal(journal, offset) =\n\t\t\t\t\t\t\tcpu_to_le32(segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t&sit_in_journal(journal, offset));\n\t\t\t} else {\n\t\t\t\tsit_offset = SIT_ENTRY_OFFSET(sit_i, segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t\t&raw_sit->entries[sit_offset]);\n\t\t\t}\n\n\t\t\t__clear_bit(segno, bitmap);\n\t\t\tsit_i->dirty_sentries--;\n\t\t\tses->entry_cnt--;\n\t\t}\n\n\t\tif (to_journal)\n\t\t\tup_write(&curseg->journal_rwsem);\n\t\telse\n\t\t\tf2fs_put_page(page, 1);\n\n\t\tf2fs_bug_on(sbi, ses->entry_cnt);\n\t\trelease_sit_entry_set(ses);\n\t}\n\n\tf2fs_bug_on(sbi, !list_empty(head));\n\tf2fs_bug_on(sbi, sit_i->dirty_sentries);\nout:\n\tif (cpc->reason & CP_DISCARD) {\n\t\t__u64 trim_start = cpc->trim_start;\n\n\t\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++)\n\t\t\tadd_discard_addrs(sbi, cpc, false);\n\n\t\tcpc->trim_start = trim_start;\n\t}\n\tmutex_unlock(&sit_i->sentry_lock);\n\n\tset_prefree_as_free_segments(sbi);\n}\n\nstatic int build_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct sit_info *sit_i;\n\tunsigned int sit_segs, start;\n\tchar *src_bitmap;\n\tunsigned int bitmap_size;\n\n\t/* allocate memory for SIT information */\n\tsit_i = kzalloc(sizeof(struct sit_info), GFP_KERNEL);\n\tif (!sit_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->sit_info = sit_i;\n\n\tsit_i->sentries = kvzalloc(MAIN_SEGS(sbi) *\n\t\t\t\t\tsizeof(struct seg_entry), GFP_KERNEL);\n\tif (!sit_i->sentries)\n\t\treturn -ENOMEM;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tsit_i->dirty_sentries_bitmap = kvzalloc(bitmap_size, GFP_KERNEL);\n\tif (!sit_i->dirty_sentries_bitmap)\n\t\treturn -ENOMEM;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tsit_i->sentries[start].cur_valid_map\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tsit_i->sentries[start].ckpt_valid_map\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tif (!sit_i->sentries[start].cur_valid_map ||\n\t\t\t\t!sit_i->sentries[start].ckpt_valid_map)\n\t\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tsit_i->sentries[start].cur_valid_map_mir\n\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\tif (!sit_i->sentries[start].cur_valid_map_mir)\n\t\t\treturn -ENOMEM;\n#endif\n\n\t\tif (f2fs_discard_en(sbi)) {\n\t\t\tsit_i->sentries[start].discard_map\n\t\t\t\t= kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\t\t\tif (!sit_i->sentries[start].discard_map)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tsit_i->tmp_map = kzalloc(SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\tif (!sit_i->tmp_map)\n\t\treturn -ENOMEM;\n\n\tif (sbi->segs_per_sec > 1) {\n\t\tsit_i->sec_entries = kvzalloc(MAIN_SECS(sbi) *\n\t\t\t\t\tsizeof(struct sec_entry), GFP_KERNEL);\n\t\tif (!sit_i->sec_entries)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* get information related with SIT */\n\tsit_segs = le32_to_cpu(raw_super->segment_count_sit) >> 1;\n\n\t/* setup SIT bitmap from ckeckpoint pack */\n\tbitmap_size = __bitmap_size(sbi, SIT_BITMAP);\n\tsrc_bitmap = __bitmap_ptr(sbi, SIT_BITMAP);\n\n\tsit_i->sit_bitmap = kmemdup(src_bitmap, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap)\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\tsit_i->sit_bitmap_mir = kmemdup(src_bitmap, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap_mir)\n\t\treturn -ENOMEM;\n#endif\n\n\t/* init SIT information */\n\tsit_i->s_ops = &default_salloc_ops;\n\n\tsit_i->sit_base_addr = le32_to_cpu(raw_super->sit_blkaddr);\n\tsit_i->sit_blocks = sit_segs << sbi->log_blocks_per_seg;\n\tsit_i->written_valid_blocks = 0;\n\tsit_i->bitmap_size = bitmap_size;\n\tsit_i->dirty_sentries = 0;\n\tsit_i->sents_per_block = SIT_ENTRY_PER_BLOCK;\n\tsit_i->elapsed_time = le64_to_cpu(sbi->ckpt->elapsed_time);\n\tsit_i->mounted_time = ktime_get_real_seconds();\n\tmutex_init(&sit_i->sentry_lock);\n\treturn 0;\n}\n\nstatic int build_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i;\n\tunsigned int bitmap_size, sec_bitmap_size;\n\n\t/* allocate memory for free segmap information */\n\tfree_i = kzalloc(sizeof(struct free_segmap_info), GFP_KERNEL);\n\tif (!free_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->free_info = free_i;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tfree_i->free_segmap = kvmalloc(bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_segmap)\n\t\treturn -ENOMEM;\n\n\tsec_bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\tfree_i->free_secmap = kvmalloc(sec_bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_secmap)\n\t\treturn -ENOMEM;\n\n\t/* set all segments as dirty temporarily */\n\tmemset(free_i->free_segmap, 0xff, bitmap_size);\n\tmemset(free_i->free_secmap, 0xff, sec_bitmap_size);\n\n\t/* init free segmap information */\n\tfree_i->start_segno = GET_SEGNO_FROM_SEG0(sbi, MAIN_BLKADDR(sbi));\n\tfree_i->free_segments = 0;\n\tfree_i->free_sections = 0;\n\tspin_lock_init(&free_i->segmap_lock);\n\treturn 0;\n}\n\nstatic int build_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array;\n\tint i;\n\n\tarray = kcalloc(NR_CURSEG_TYPE, sizeof(*array), GFP_KERNEL);\n\tif (!array)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->curseg_array = array;\n\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tmutex_init(&array[i].curseg_mutex);\n\t\tarray[i].sum_blk = kzalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (!array[i].sum_blk)\n\t\t\treturn -ENOMEM;\n\t\tinit_rwsem(&array[i].journal_rwsem);\n\t\tarray[i].journal = kzalloc(sizeof(struct f2fs_journal),\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!array[i].journal)\n\t\t\treturn -ENOMEM;\n\t\tarray[i].segno = NULL_SEGNO;\n\t\tarray[i].next_blkoff = 0;\n\t}\n\treturn restore_curseg_summaries(sbi);\n}\n\nstatic void build_sit_entries(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct seg_entry *se;\n\tstruct f2fs_sit_entry sit;\n\tint sit_blk_cnt = SIT_BLK_CNT(sbi);\n\tunsigned int i, start, end;\n\tunsigned int readed, start_blk = 0;\n\n\tdo {\n\t\treaded = ra_meta_pages(sbi, start_blk, BIO_MAX_PAGES,\n\t\t\t\t\t\t\tMETA_SIT, true);\n\n\t\tstart = start_blk * sit_i->sents_per_block;\n\t\tend = (start_blk + readed) * sit_i->sents_per_block;\n\n\t\tfor (; start < end && start < MAIN_SEGS(sbi); start++) {\n\t\t\tstruct f2fs_sit_block *sit_blk;\n\t\t\tstruct page *page;\n\n\t\t\tse = &sit_i->sentries[start];\n\t\t\tpage = get_current_sit_page(sbi, start);\n\t\t\tsit_blk = (struct f2fs_sit_block *)page_address(page);\n\t\t\tsit = sit_blk->entries[SIT_ENTRY_OFFSET(sit_i, start)];\n\t\t\tf2fs_put_page(page, 1);\n\n\t\t\tcheck_block_count(sbi, start, &sit);\n\t\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\t\t/* build discard map only one time */\n\t\t\tif (f2fs_discard_en(sbi)) {\n\t\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t} else {\n\t\t\t\t\tmemcpy(se->discard_map,\n\t\t\t\t\t\tse->cur_valid_map,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t\tsbi->discard_blks +=\n\t\t\t\t\t\tsbi->blocks_per_seg -\n\t\t\t\t\t\tse->valid_blocks;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sbi->segs_per_sec > 1)\n\t\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t}\n\t\tstart_blk += readed;\n\t} while (start_blk < sit_blk_cnt);\n\n\tdown_read(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int old_valid_blocks;\n\n\t\tstart = le32_to_cpu(segno_in_journal(journal, i));\n\t\tse = &sit_i->sentries[start];\n\t\tsit = sit_in_journal(journal, i);\n\n\t\told_valid_blocks = se->valid_blocks;\n\n\t\tcheck_block_count(sbi, start, &sit);\n\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\tif (f2fs_discard_en(sbi)) {\n\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t} else {\n\t\t\t\tmemcpy(se->discard_map, se->cur_valid_map,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\tsbi->discard_blks += old_valid_blocks -\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t\t}\n\t\t}\n\n\t\tif (sbi->segs_per_sec > 1)\n\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\tse->valid_blocks - old_valid_blocks;\n\t}\n\tup_read(&curseg->journal_rwsem);\n}\n\nstatic void init_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tunsigned int start;\n\tint type;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, start);\n\t\tif (!sentry->valid_blocks)\n\t\t\t__set_free(sbi, start);\n\t\telse\n\t\t\tSIT_I(sbi)->written_valid_blocks +=\n\t\t\t\t\t\tsentry->valid_blocks;\n\t}\n\n\t/* set use the current segments */\n\tfor (type = CURSEG_HOT_DATA; type <= CURSEG_COLD_NODE; type++) {\n\t\tstruct curseg_info *curseg_t = CURSEG_I(sbi, type);\n\t\t__set_test_and_inuse(sbi, curseg_t->segno);\n\t}\n}\n\nstatic void init_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno = 0, offset = 0;\n\tunsigned short valid_blocks;\n\n\twhile (1) {\n\t\t/* find dirty segment based on free segmap */\n\t\tsegno = find_next_inuse(free_i, MAIN_SEGS(sbi), offset);\n\t\tif (segno >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\toffset = segno + 1;\n\t\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\t\tif (valid_blocks == sbi->blocks_per_seg || !valid_blocks)\n\t\t\tcontinue;\n\t\tif (valid_blocks > sbi->blocks_per_seg) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tcontinue;\n\t\t}\n\t\tmutex_lock(&dirty_i->seglist_lock);\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t\tmutex_unlock(&dirty_i->seglist_lock);\n\t}\n}\n\nstatic int init_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\n\tdirty_i->victim_secmap = kvzalloc(bitmap_size, GFP_KERNEL);\n\tif (!dirty_i->victim_secmap)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic int build_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i;\n\tunsigned int bitmap_size, i;\n\n\t/* allocate memory for dirty segments list information */\n\tdirty_i = kzalloc(sizeof(struct dirty_seglist_info), GFP_KERNEL);\n\tif (!dirty_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->dirty_info = dirty_i;\n\tmutex_init(&dirty_i->seglist_lock);\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++) {\n\t\tdirty_i->dirty_segmap[i] = kvzalloc(bitmap_size, GFP_KERNEL);\n\t\tif (!dirty_i->dirty_segmap[i])\n\t\t\treturn -ENOMEM;\n\t}\n\n\tinit_dirty_segmap(sbi);\n\treturn init_victim_secmap(sbi);\n}\n\n/*\n * Update min, max modified time for cost-benefit GC algorithm\n */\nstatic void init_min_max_mtime(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&sit_i->sentry_lock);\n\n\tsit_i->min_mtime = LLONG_MAX;\n\n\tfor (segno = 0; segno < MAIN_SEGS(sbi); segno += sbi->segs_per_sec) {\n\t\tunsigned int i;\n\t\tunsigned long long mtime = 0;\n\n\t\tfor (i = 0; i < sbi->segs_per_sec; i++)\n\t\t\tmtime += get_seg_entry(sbi, segno + i)->mtime;\n\n\t\tmtime = div_u64(mtime, sbi->segs_per_sec);\n\n\t\tif (sit_i->min_mtime > mtime)\n\t\t\tsit_i->min_mtime = mtime;\n\t}\n\tsit_i->max_mtime = get_mtime(sbi);\n\tmutex_unlock(&sit_i->sentry_lock);\n}\n\nint build_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_sm_info *sm_info;\n\tint err;\n\n\tsm_info = kzalloc(sizeof(struct f2fs_sm_info), GFP_KERNEL);\n\tif (!sm_info)\n\t\treturn -ENOMEM;\n\n\t/* init sm info */\n\tsbi->sm_info = sm_info;\n\tsm_info->seg0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tsm_info->main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tsm_info->segment_count = le32_to_cpu(raw_super->segment_count);\n\tsm_info->reserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\tsm_info->ovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\tsm_info->main_segments = le32_to_cpu(raw_super->segment_count_main);\n\tsm_info->ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tsm_info->rec_prefree_segments = sm_info->main_segments *\n\t\t\t\t\tDEF_RECLAIM_PREFREE_SEGMENTS / 100;\n\tif (sm_info->rec_prefree_segments > DEF_MAX_RECLAIM_PREFREE_SEGMENTS)\n\t\tsm_info->rec_prefree_segments = DEF_MAX_RECLAIM_PREFREE_SEGMENTS;\n\n\tif (!test_opt(sbi, LFS))\n\t\tsm_info->ipu_policy = 1 << F2FS_IPU_FSYNC;\n\tsm_info->min_ipu_util = DEF_MIN_IPU_UTIL;\n\tsm_info->min_fsync_blocks = DEF_MIN_FSYNC_BLOCKS;\n\tsm_info->min_hot_blocks = DEF_MIN_HOT_BLOCKS;\n\n\tsm_info->trim_sections = DEF_BATCHED_TRIM_SECTIONS;\n\n\tINIT_LIST_HEAD(&sm_info->sit_entry_set);\n\n\tif (!f2fs_readonly(sbi->sb)) {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = create_discard_cmd_control(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = build_sit_info(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_free_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_curseg(sbi);\n\tif (err)\n\t\treturn err;\n\n\t/* reinit free segmap based on SIT */\n\tbuild_sit_entries(sbi);\n\n\tinit_free_segmap(sbi);\n\terr = build_dirty_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\n\tinit_min_max_mtime(sbi);\n\treturn 0;\n}\n\nstatic void discard_dirty_segmap(struct f2fs_sb_info *sbi,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tkvfree(dirty_i->dirty_segmap[dirty_type]);\n\tdirty_i->nr_dirty[dirty_type] = 0;\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic void destroy_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tkvfree(dirty_i->victim_secmap);\n}\n\nstatic void destroy_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tint i;\n\n\tif (!dirty_i)\n\t\treturn;\n\n\t/* discard pre-free/dirty segments list */\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++)\n\t\tdiscard_dirty_segmap(sbi, i);\n\n\tdestroy_victim_secmap(sbi);\n\tSM_I(sbi)->dirty_info = NULL;\n\tkfree(dirty_i);\n}\n\nstatic void destroy_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array = SM_I(sbi)->curseg_array;\n\tint i;\n\n\tif (!array)\n\t\treturn;\n\tSM_I(sbi)->curseg_array = NULL;\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tkfree(array[i].sum_blk);\n\t\tkfree(array[i].journal);\n\t}\n\tkfree(array);\n}\n\nstatic void destroy_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i = SM_I(sbi)->free_info;\n\tif (!free_i)\n\t\treturn;\n\tSM_I(sbi)->free_info = NULL;\n\tkvfree(free_i->free_segmap);\n\tkvfree(free_i->free_secmap);\n\tkfree(free_i);\n}\n\nstatic void destroy_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int start;\n\n\tif (!sit_i)\n\t\treturn;\n\n\tif (sit_i->sentries) {\n\t\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\t\tkfree(sit_i->sentries[start].cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tkfree(sit_i->sentries[start].cur_valid_map_mir);\n#endif\n\t\t\tkfree(sit_i->sentries[start].ckpt_valid_map);\n\t\t\tkfree(sit_i->sentries[start].discard_map);\n\t\t}\n\t}\n\tkfree(sit_i->tmp_map);\n\n\tkvfree(sit_i->sentries);\n\tkvfree(sit_i->sec_entries);\n\tkvfree(sit_i->dirty_sentries_bitmap);\n\n\tSM_I(sbi)->sit_info = NULL;\n\tkfree(sit_i->sit_bitmap);\n#ifdef CONFIG_F2FS_CHECK_FS\n\tkfree(sit_i->sit_bitmap_mir);\n#endif\n\tkfree(sit_i);\n}\n\nvoid destroy_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\n\tif (!sm_info)\n\t\treturn;\n\tdestroy_flush_cmd_control(sbi, true);\n\tdestroy_discard_cmd_control(sbi);\n\tdestroy_dirty_segmap(sbi);\n\tdestroy_curseg(sbi);\n\tdestroy_free_segmap(sbi);\n\tdestroy_sit_info(sbi);\n\tsbi->sm_info = NULL;\n\tkfree(sm_info);\n}\n\nint __init create_segment_manager_caches(void)\n{\n\tdiscard_entry_slab = f2fs_kmem_cache_create(\"discard_entry\",\n\t\t\tsizeof(struct discard_entry));\n\tif (!discard_entry_slab)\n\t\tgoto fail;\n\n\tdiscard_cmd_slab = f2fs_kmem_cache_create(\"discard_cmd\",\n\t\t\tsizeof(struct discard_cmd));\n\tif (!discard_cmd_slab)\n\t\tgoto destroy_discard_entry;\n\n\tsit_entry_set_slab = f2fs_kmem_cache_create(\"sit_entry_set\",\n\t\t\tsizeof(struct sit_entry_set));\n\tif (!sit_entry_set_slab)\n\t\tgoto destroy_discard_cmd;\n\n\tinmem_entry_slab = f2fs_kmem_cache_create(\"inmem_page_entry\",\n\t\t\tsizeof(struct inmem_pages));\n\tif (!inmem_entry_slab)\n\t\tgoto destroy_sit_entry_set;\n\treturn 0;\n\ndestroy_sit_entry_set:\n\tkmem_cache_destroy(sit_entry_set_slab);\ndestroy_discard_cmd:\n\tkmem_cache_destroy(discard_cmd_slab);\ndestroy_discard_entry:\n\tkmem_cache_destroy(discard_entry_slab);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid destroy_segment_manager_caches(void)\n{\n\tkmem_cache_destroy(sit_entry_set_slab);\n\tkmem_cache_destroy(discard_cmd_slab);\n\tkmem_cache_destroy(discard_entry_slab);\n\tkmem_cache_destroy(inmem_entry_slab);\n}\n", "/*\n * fs/f2fs/super.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/statfs.h>\n#include <linux/buffer_head.h>\n#include <linux/backing-dev.h>\n#include <linux/kthread.h>\n#include <linux/parser.h>\n#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/random.h>\n#include <linux/exportfs.h>\n#include <linux/blkdev.h>\n#include <linux/quotaops.h>\n#include <linux/f2fs_fs.h>\n#include <linux/sysfs.h>\n#include <linux/quota.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"xattr.h\"\n#include \"gc.h\"\n#include \"trace.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/f2fs.h>\n\nstatic struct kmem_cache *f2fs_inode_cachep;\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\nchar *fault_name[FAULT_MAX] = {\n\t[FAULT_KMALLOC]\t\t= \"kmalloc\",\n\t[FAULT_PAGE_ALLOC]\t= \"page alloc\",\n\t[FAULT_ALLOC_NID]\t= \"alloc nid\",\n\t[FAULT_ORPHAN]\t\t= \"orphan\",\n\t[FAULT_BLOCK]\t\t= \"no more block\",\n\t[FAULT_DIR_DEPTH]\t= \"too big dir depth\",\n\t[FAULT_EVICT_INODE]\t= \"evict_inode fail\",\n\t[FAULT_TRUNCATE]\t= \"truncate fail\",\n\t[FAULT_IO]\t\t= \"IO error\",\n\t[FAULT_CHECKPOINT]\t= \"checkpoint error\",\n};\n\nstatic void f2fs_build_fault_attr(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tunsigned int rate)\n{\n\tstruct f2fs_fault_info *ffi = &sbi->fault_info;\n\n\tif (rate) {\n\t\tatomic_set(&ffi->inject_ops, 0);\n\t\tffi->inject_rate = rate;\n\t\tffi->inject_type = (1 << FAULT_MAX) - 1;\n\t} else {\n\t\tmemset(ffi, 0, sizeof(struct f2fs_fault_info));\n\t}\n}\n#endif\n\n/* f2fs-wide shrinker description */\nstatic struct shrinker f2fs_shrinker_info = {\n\t.scan_objects = f2fs_shrink_scan,\n\t.count_objects = f2fs_shrink_count,\n\t.seeks = DEFAULT_SEEKS,\n};\n\nenum {\n\tOpt_gc_background,\n\tOpt_disable_roll_forward,\n\tOpt_norecovery,\n\tOpt_discard,\n\tOpt_nodiscard,\n\tOpt_noheap,\n\tOpt_heap,\n\tOpt_user_xattr,\n\tOpt_nouser_xattr,\n\tOpt_acl,\n\tOpt_noacl,\n\tOpt_active_logs,\n\tOpt_disable_ext_identify,\n\tOpt_inline_xattr,\n\tOpt_noinline_xattr,\n\tOpt_inline_data,\n\tOpt_inline_dentry,\n\tOpt_noinline_dentry,\n\tOpt_flush_merge,\n\tOpt_noflush_merge,\n\tOpt_nobarrier,\n\tOpt_fastboot,\n\tOpt_extent_cache,\n\tOpt_noextent_cache,\n\tOpt_noinline_data,\n\tOpt_data_flush,\n\tOpt_mode,\n\tOpt_io_size_bits,\n\tOpt_fault_injection,\n\tOpt_lazytime,\n\tOpt_nolazytime,\n\tOpt_quota,\n\tOpt_noquota,\n\tOpt_usrquota,\n\tOpt_grpquota,\n\tOpt_prjquota,\n\tOpt_usrjquota,\n\tOpt_grpjquota,\n\tOpt_prjjquota,\n\tOpt_offusrjquota,\n\tOpt_offgrpjquota,\n\tOpt_offprjjquota,\n\tOpt_jqfmt_vfsold,\n\tOpt_jqfmt_vfsv0,\n\tOpt_jqfmt_vfsv1,\n\tOpt_err,\n};\n\nstatic match_table_t f2fs_tokens = {\n\t{Opt_gc_background, \"background_gc=%s\"},\n\t{Opt_disable_roll_forward, \"disable_roll_forward\"},\n\t{Opt_norecovery, \"norecovery\"},\n\t{Opt_discard, \"discard\"},\n\t{Opt_nodiscard, \"nodiscard\"},\n\t{Opt_noheap, \"no_heap\"},\n\t{Opt_heap, \"heap\"},\n\t{Opt_user_xattr, \"user_xattr\"},\n\t{Opt_nouser_xattr, \"nouser_xattr\"},\n\t{Opt_acl, \"acl\"},\n\t{Opt_noacl, \"noacl\"},\n\t{Opt_active_logs, \"active_logs=%u\"},\n\t{Opt_disable_ext_identify, \"disable_ext_identify\"},\n\t{Opt_inline_xattr, \"inline_xattr\"},\n\t{Opt_noinline_xattr, \"noinline_xattr\"},\n\t{Opt_inline_data, \"inline_data\"},\n\t{Opt_inline_dentry, \"inline_dentry\"},\n\t{Opt_noinline_dentry, \"noinline_dentry\"},\n\t{Opt_flush_merge, \"flush_merge\"},\n\t{Opt_noflush_merge, \"noflush_merge\"},\n\t{Opt_nobarrier, \"nobarrier\"},\n\t{Opt_fastboot, \"fastboot\"},\n\t{Opt_extent_cache, \"extent_cache\"},\n\t{Opt_noextent_cache, \"noextent_cache\"},\n\t{Opt_noinline_data, \"noinline_data\"},\n\t{Opt_data_flush, \"data_flush\"},\n\t{Opt_mode, \"mode=%s\"},\n\t{Opt_io_size_bits, \"io_bits=%u\"},\n\t{Opt_fault_injection, \"fault_injection=%u\"},\n\t{Opt_lazytime, \"lazytime\"},\n\t{Opt_nolazytime, \"nolazytime\"},\n\t{Opt_quota, \"quota\"},\n\t{Opt_noquota, \"noquota\"},\n\t{Opt_usrquota, \"usrquota\"},\n\t{Opt_grpquota, \"grpquota\"},\n\t{Opt_prjquota, \"prjquota\"},\n\t{Opt_usrjquota, \"usrjquota=%s\"},\n\t{Opt_grpjquota, \"grpjquota=%s\"},\n\t{Opt_prjjquota, \"prjjquota=%s\"},\n\t{Opt_offusrjquota, \"usrjquota=\"},\n\t{Opt_offgrpjquota, \"grpjquota=\"},\n\t{Opt_offprjjquota, \"prjjquota=\"},\n\t{Opt_jqfmt_vfsold, \"jqfmt=vfsold\"},\n\t{Opt_jqfmt_vfsv0, \"jqfmt=vfsv0\"},\n\t{Opt_jqfmt_vfsv1, \"jqfmt=vfsv1\"},\n\t{Opt_err, NULL},\n};\n\nvoid f2fs_msg(struct super_block *sb, const char *level, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tprintk_ratelimited(\"%sF2FS-fs (%s): %pV\\n\", level, sb->s_id, &vaf);\n\tva_end(args);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct f2fs_inode_info *fi = (struct f2fs_inode_info *) foo;\n\n\tinode_init_once(&fi->vfs_inode);\n}\n\n#ifdef CONFIG_QUOTA\nstatic const char * const quotatypes[] = INITQFNAMES;\n#define QTYPE2NAME(t) (quotatypes[t])\nstatic int f2fs_set_qf_name(struct super_block *sb, int qtype,\n\t\t\t\t\t\t\tsubstring_t *args)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tchar *qname;\n\tint ret = -EINVAL;\n\n\tif (sb_any_quota_loaded(sb) && !sbi->s_qf_names[qtype]) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Cannot change journaled \"\n\t\t\t\"quota options when quota turned on\");\n\t\treturn -EINVAL;\n\t}\n\tqname = match_strdup(args);\n\tif (!qname) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Not enough memory for storing quotafile name\");\n\t\treturn -EINVAL;\n\t}\n\tif (sbi->s_qf_names[qtype]) {\n\t\tif (strcmp(sbi->s_qf_names[qtype], qname) == 0)\n\t\t\tret = 0;\n\t\telse\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t \"%s quota file already specified\",\n\t\t\t\t QTYPE2NAME(qtype));\n\t\tgoto errout;\n\t}\n\tif (strchr(qname, '/')) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"quotafile must be on filesystem root\");\n\t\tgoto errout;\n\t}\n\tsbi->s_qf_names[qtype] = qname;\n\tset_opt(sbi, QUOTA);\n\treturn 0;\nerrout:\n\tkfree(qname);\n\treturn ret;\n}\n\nstatic int f2fs_clear_qf_name(struct super_block *sb, int qtype)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\n\tif (sb_any_quota_loaded(sb) && sbi->s_qf_names[qtype]) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Cannot change journaled quota options\"\n\t\t\t\" when quota turned on\");\n\t\treturn -EINVAL;\n\t}\n\tkfree(sbi->s_qf_names[qtype]);\n\tsbi->s_qf_names[qtype] = NULL;\n\treturn 0;\n}\n\nstatic int f2fs_check_quota_options(struct f2fs_sb_info *sbi)\n{\n\t/*\n\t * We do the test below only for project quotas. 'usrquota' and\n\t * 'grpquota' mount options are allowed even without quota feature\n\t * to support legacy quotas in quota files.\n\t */\n\tif (test_opt(sbi, PRJQUOTA) && !f2fs_sb_has_project_quota(sbi->sb)) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR, \"Project quota feature not enabled. \"\n\t\t\t \"Cannot enable project quota enforcement.\");\n\t\treturn -1;\n\t}\n\tif (sbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA] ||\n\t\t\tsbi->s_qf_names[PRJQUOTA]) {\n\t\tif (test_opt(sbi, USRQUOTA) && sbi->s_qf_names[USRQUOTA])\n\t\t\tclear_opt(sbi, USRQUOTA);\n\n\t\tif (test_opt(sbi, GRPQUOTA) && sbi->s_qf_names[GRPQUOTA])\n\t\t\tclear_opt(sbi, GRPQUOTA);\n\n\t\tif (test_opt(sbi, PRJQUOTA) && sbi->s_qf_names[PRJQUOTA])\n\t\t\tclear_opt(sbi, PRJQUOTA);\n\n\t\tif (test_opt(sbi, GRPQUOTA) || test_opt(sbi, USRQUOTA) ||\n\t\t\t\ttest_opt(sbi, PRJQUOTA)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR, \"old and new quota \"\n\t\t\t\t\t\"format mixing\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!sbi->s_jquota_fmt) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR, \"journaled quota format \"\n\t\t\t\t\t\"not specified\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n#endif\n\nstatic int parse_options(struct super_block *sb, char *options)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct request_queue *q;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tchar *p, *name;\n\tint arg = 0;\n#ifdef CONFIG_QUOTA\n\tint ret;\n#endif\n\n\tif (!options)\n\t\treturn 0;\n\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tint token;\n\t\tif (!*p)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Initialize args struct so we know whether arg was\n\t\t * found; some options take optional arguments.\n\t\t */\n\t\targs[0].to = args[0].from = NULL;\n\t\ttoken = match_token(p, f2fs_tokens, args);\n\n\t\tswitch (token) {\n\t\tcase Opt_gc_background:\n\t\t\tname = match_strdup(&args[0]);\n\n\t\t\tif (!name)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (strlen(name) == 2 && !strncmp(name, \"on\", 2)) {\n\t\t\t\tset_opt(sbi, BG_GC);\n\t\t\t\tclear_opt(sbi, FORCE_FG_GC);\n\t\t\t} else if (strlen(name) == 3 && !strncmp(name, \"off\", 3)) {\n\t\t\t\tclear_opt(sbi, BG_GC);\n\t\t\t\tclear_opt(sbi, FORCE_FG_GC);\n\t\t\t} else if (strlen(name) == 4 && !strncmp(name, \"sync\", 4)) {\n\t\t\t\tset_opt(sbi, BG_GC);\n\t\t\t\tset_opt(sbi, FORCE_FG_GC);\n\t\t\t} else {\n\t\t\t\tkfree(name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tkfree(name);\n\t\t\tbreak;\n\t\tcase Opt_disable_roll_forward:\n\t\t\tset_opt(sbi, DISABLE_ROLL_FORWARD);\n\t\t\tbreak;\n\t\tcase Opt_norecovery:\n\t\t\t/* this option mounts f2fs with ro */\n\t\t\tset_opt(sbi, DISABLE_ROLL_FORWARD);\n\t\t\tif (!f2fs_readonly(sb))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase Opt_discard:\n\t\t\tq = bdev_get_queue(sb->s_bdev);\n\t\t\tif (blk_queue_discard(q)) {\n\t\t\t\tset_opt(sbi, DISCARD);\n\t\t\t} else if (!f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"mounting with \\\"discard\\\" option, but \"\n\t\t\t\t\t\"the device does not support discard\");\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Opt_nodiscard:\n\t\t\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"discard is required for zoned block devices\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tclear_opt(sbi, DISCARD);\n\t\t\tbreak;\n\t\tcase Opt_noheap:\n\t\t\tset_opt(sbi, NOHEAP);\n\t\t\tbreak;\n\t\tcase Opt_heap:\n\t\t\tclear_opt(sbi, NOHEAP);\n\t\t\tbreak;\n#ifdef CONFIG_F2FS_FS_XATTR\n\t\tcase Opt_user_xattr:\n\t\t\tset_opt(sbi, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tclear_opt(sbi, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_inline_xattr:\n\t\t\tset_opt(sbi, INLINE_XATTR);\n\t\t\tbreak;\n\t\tcase Opt_noinline_xattr:\n\t\t\tclear_opt(sbi, INLINE_XATTR);\n\t\t\tbreak;\n#else\n\t\tcase Opt_user_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"user_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"nouser_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_inline_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"inline_xattr options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noinline_xattr:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"noinline_xattr options not supported\");\n\t\t\tbreak;\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\t\tcase Opt_acl:\n\t\t\tset_opt(sbi, POSIX_ACL);\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tclear_opt(sbi, POSIX_ACL);\n\t\t\tbreak;\n#else\n\t\tcase Opt_acl:\n\t\t\tf2fs_msg(sb, KERN_INFO, \"acl options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tf2fs_msg(sb, KERN_INFO, \"noacl options not supported\");\n\t\t\tbreak;\n#endif\n\t\tcase Opt_active_logs:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (arg != 2 && arg != 4 && arg != NR_CURSEG_TYPE)\n\t\t\t\treturn -EINVAL;\n\t\t\tsbi->active_logs = arg;\n\t\t\tbreak;\n\t\tcase Opt_disable_ext_identify:\n\t\t\tset_opt(sbi, DISABLE_EXT_IDENTIFY);\n\t\t\tbreak;\n\t\tcase Opt_inline_data:\n\t\t\tset_opt(sbi, INLINE_DATA);\n\t\t\tbreak;\n\t\tcase Opt_inline_dentry:\n\t\t\tset_opt(sbi, INLINE_DENTRY);\n\t\t\tbreak;\n\t\tcase Opt_noinline_dentry:\n\t\t\tclear_opt(sbi, INLINE_DENTRY);\n\t\t\tbreak;\n\t\tcase Opt_flush_merge:\n\t\t\tset_opt(sbi, FLUSH_MERGE);\n\t\t\tbreak;\n\t\tcase Opt_noflush_merge:\n\t\t\tclear_opt(sbi, FLUSH_MERGE);\n\t\t\tbreak;\n\t\tcase Opt_nobarrier:\n\t\t\tset_opt(sbi, NOBARRIER);\n\t\t\tbreak;\n\t\tcase Opt_fastboot:\n\t\t\tset_opt(sbi, FASTBOOT);\n\t\t\tbreak;\n\t\tcase Opt_extent_cache:\n\t\t\tset_opt(sbi, EXTENT_CACHE);\n\t\t\tbreak;\n\t\tcase Opt_noextent_cache:\n\t\t\tclear_opt(sbi, EXTENT_CACHE);\n\t\t\tbreak;\n\t\tcase Opt_noinline_data:\n\t\t\tclear_opt(sbi, INLINE_DATA);\n\t\t\tbreak;\n\t\tcase Opt_data_flush:\n\t\t\tset_opt(sbi, DATA_FLUSH);\n\t\t\tbreak;\n\t\tcase Opt_mode:\n\t\t\tname = match_strdup(&args[0]);\n\n\t\t\tif (!name)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (strlen(name) == 8 &&\n\t\t\t\t\t!strncmp(name, \"adaptive\", 8)) {\n\t\t\t\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\t \"adaptive mode is not allowed with \"\n\t\t\t\t\t\t \"zoned block device feature\");\n\t\t\t\t\tkfree(name);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tset_opt_mode(sbi, F2FS_MOUNT_ADAPTIVE);\n\t\t\t} else if (strlen(name) == 3 &&\n\t\t\t\t\t!strncmp(name, \"lfs\", 3)) {\n\t\t\t\tset_opt_mode(sbi, F2FS_MOUNT_LFS);\n\t\t\t} else {\n\t\t\t\tkfree(name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tkfree(name);\n\t\t\tbreak;\n\t\tcase Opt_io_size_bits:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (arg > __ilog2_u32(BIO_MAX_PAGES)) {\n\t\t\t\tf2fs_msg(sb, KERN_WARNING,\n\t\t\t\t\t\"Not support %d, larger than %d\",\n\t\t\t\t\t1 << arg, BIO_MAX_PAGES);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsbi->write_io_size_bits = arg;\n\t\t\tbreak;\n\t\tcase Opt_fault_injection:\n\t\t\tif (args->from && match_int(args, &arg))\n\t\t\t\treturn -EINVAL;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\t\t\tf2fs_build_fault_attr(sbi, arg);\n\t\t\tset_opt(sbi, FAULT_INJECTION);\n#else\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\"FAULT_INJECTION was not selected\");\n#endif\n\t\t\tbreak;\n\t\tcase Opt_lazytime:\n\t\t\tsb->s_flags |= MS_LAZYTIME;\n\t\t\tbreak;\n\t\tcase Opt_nolazytime:\n\t\t\tsb->s_flags &= ~MS_LAZYTIME;\n\t\t\tbreak;\n#ifdef CONFIG_QUOTA\n\t\tcase Opt_quota:\n\t\tcase Opt_usrquota:\n\t\t\tset_opt(sbi, USRQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_grpquota:\n\t\t\tset_opt(sbi, GRPQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_prjquota:\n\t\t\tset_opt(sbi, PRJQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_usrjquota:\n\t\t\tret = f2fs_set_qf_name(sb, USRQUOTA, &args[0]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_grpjquota:\n\t\t\tret = f2fs_set_qf_name(sb, GRPQUOTA, &args[0]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_prjjquota:\n\t\t\tret = f2fs_set_qf_name(sb, PRJQUOTA, &args[0]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_offusrjquota:\n\t\t\tret = f2fs_clear_qf_name(sb, USRQUOTA);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_offgrpjquota:\n\t\t\tret = f2fs_clear_qf_name(sb, GRPQUOTA);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_offprjjquota:\n\t\t\tret = f2fs_clear_qf_name(sb, PRJQUOTA);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase Opt_jqfmt_vfsold:\n\t\t\tsbi->s_jquota_fmt = QFMT_VFS_OLD;\n\t\t\tbreak;\n\t\tcase Opt_jqfmt_vfsv0:\n\t\t\tsbi->s_jquota_fmt = QFMT_VFS_V0;\n\t\t\tbreak;\n\t\tcase Opt_jqfmt_vfsv1:\n\t\t\tsbi->s_jquota_fmt = QFMT_VFS_V1;\n\t\t\tbreak;\n\t\tcase Opt_noquota:\n\t\t\tclear_opt(sbi, QUOTA);\n\t\t\tclear_opt(sbi, USRQUOTA);\n\t\t\tclear_opt(sbi, GRPQUOTA);\n\t\t\tclear_opt(sbi, PRJQUOTA);\n\t\t\tbreak;\n#else\n\t\tcase Opt_quota:\n\t\tcase Opt_usrquota:\n\t\tcase Opt_grpquota:\n\t\tcase Opt_prjquota:\n\t\tcase Opt_usrjquota:\n\t\tcase Opt_grpjquota:\n\t\tcase Opt_prjjquota:\n\t\tcase Opt_offusrjquota:\n\t\tcase Opt_offgrpjquota:\n\t\tcase Opt_offprjjquota:\n\t\tcase Opt_jqfmt_vfsold:\n\t\tcase Opt_jqfmt_vfsv0:\n\t\tcase Opt_jqfmt_vfsv1:\n\t\tcase Opt_noquota:\n\t\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\t\t\"quota operations not supported\");\n\t\t\tbreak;\n#endif\n\t\tdefault:\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Unrecognized mount option \\\"%s\\\" or missing value\",\n\t\t\t\tp);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n#ifdef CONFIG_QUOTA\n\tif (f2fs_check_quota_options(sbi))\n\t\treturn -EINVAL;\n#endif\n\n\tif (F2FS_IO_SIZE_BITS(sbi) && !test_opt(sbi, LFS)) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Should set mode=lfs with %uKB-sized IO\",\n\t\t\t\tF2FS_IO_SIZE_KB(sbi));\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic struct inode *f2fs_alloc_inode(struct super_block *sb)\n{\n\tstruct f2fs_inode_info *fi;\n\n\tfi = kmem_cache_alloc(f2fs_inode_cachep, GFP_F2FS_ZERO);\n\tif (!fi)\n\t\treturn NULL;\n\n\tinit_once((void *) fi);\n\n\t/* Initialize f2fs-specific inode info */\n\tfi->vfs_inode.i_version = 1;\n\tatomic_set(&fi->dirty_pages, 0);\n\tfi->i_current_depth = 1;\n\tfi->i_advise = 0;\n\tinit_rwsem(&fi->i_sem);\n\tINIT_LIST_HEAD(&fi->dirty_list);\n\tINIT_LIST_HEAD(&fi->gdirty_list);\n\tINIT_LIST_HEAD(&fi->inmem_pages);\n\tmutex_init(&fi->inmem_lock);\n\tinit_rwsem(&fi->dio_rwsem[READ]);\n\tinit_rwsem(&fi->dio_rwsem[WRITE]);\n\tinit_rwsem(&fi->i_mmap_sem);\n\tinit_rwsem(&fi->i_xattr_sem);\n\n#ifdef CONFIG_QUOTA\n\tmemset(&fi->i_dquot, 0, sizeof(fi->i_dquot));\n\tfi->i_reserved_quota = 0;\n#endif\n\t/* Will be used by directory only */\n\tfi->i_dir_level = F2FS_SB(sb)->dir_level;\n\n\treturn &fi->vfs_inode;\n}\n\nstatic int f2fs_drop_inode(struct inode *inode)\n{\n\tint ret;\n\t/*\n\t * This is to avoid a deadlock condition like below.\n\t * writeback_single_inode(inode)\n\t *  - f2fs_write_data_page\n\t *    - f2fs_gc -> iput -> evict\n\t *       - inode_wait_for_writeback(inode)\n\t */\n\tif ((!inode_unhashed(inode) && inode->i_state & I_SYNC)) {\n\t\tif (!inode->i_nlink && !is_bad_inode(inode)) {\n\t\t\t/* to avoid evict_inode call simultaneously */\n\t\t\tatomic_inc(&inode->i_count);\n\t\t\tspin_unlock(&inode->i_lock);\n\n\t\t\t/* some remained atomic pages should discarded */\n\t\t\tif (f2fs_is_atomic_file(inode))\n\t\t\t\tdrop_inmem_pages(inode);\n\n\t\t\t/* should remain fi->extent_tree for writepage */\n\t\t\tf2fs_destroy_extent_node(inode);\n\n\t\t\tsb_start_intwrite(inode->i_sb);\n\t\t\tf2fs_i_size_write(inode, 0);\n\n\t\t\tif (F2FS_HAS_BLOCKS(inode))\n\t\t\t\tf2fs_truncate(inode);\n\n\t\t\tsb_end_intwrite(inode->i_sb);\n\n\t\t\tfscrypt_put_encryption_info(inode, NULL);\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tatomic_dec(&inode->i_count);\n\t\t}\n\t\ttrace_f2fs_drop_inode(inode, 0);\n\t\treturn 0;\n\t}\n\tret = generic_drop_inode(inode);\n\ttrace_f2fs_drop_inode(inode, ret);\n\treturn ret;\n}\n\nint f2fs_inode_dirtied(struct inode *inode, bool sync)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint ret = 0;\n\n\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\tif (is_inode_flag_set(inode, FI_DIRTY_INODE)) {\n\t\tret = 1;\n\t} else {\n\t\tset_inode_flag(inode, FI_DIRTY_INODE);\n\t\tstat_inc_dirty_inode(sbi, DIRTY_META);\n\t}\n\tif (sync && list_empty(&F2FS_I(inode)->gdirty_list)) {\n\t\tlist_add_tail(&F2FS_I(inode)->gdirty_list,\n\t\t\t\t&sbi->inode_list[DIRTY_META]);\n\t\tinc_page_count(sbi, F2FS_DIRTY_IMETA);\n\t}\n\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\treturn ret;\n}\n\nvoid f2fs_inode_synced(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\tif (!is_inode_flag_set(inode, FI_DIRTY_INODE)) {\n\t\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\t\treturn;\n\t}\n\tif (!list_empty(&F2FS_I(inode)->gdirty_list)) {\n\t\tlist_del_init(&F2FS_I(inode)->gdirty_list);\n\t\tdec_page_count(sbi, F2FS_DIRTY_IMETA);\n\t}\n\tclear_inode_flag(inode, FI_DIRTY_INODE);\n\tclear_inode_flag(inode, FI_AUTO_RECOVER);\n\tstat_dec_dirty_inode(F2FS_I_SB(inode), DIRTY_META);\n\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n}\n\n/*\n * f2fs_dirty_inode() is called from __mark_inode_dirty()\n *\n * We should call set_dirty_inode to write the dirty inode through write_inode.\n */\nstatic void f2fs_dirty_inode(struct inode *inode, int flags)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_NODE_INO(sbi) ||\n\t\t\tinode->i_ino == F2FS_META_INO(sbi))\n\t\treturn;\n\n\tif (flags == I_DIRTY_TIME)\n\t\treturn;\n\n\tif (is_inode_flag_set(inode, FI_AUTO_RECOVER))\n\t\tclear_inode_flag(inode, FI_AUTO_RECOVER);\n\n\tf2fs_inode_dirtied(inode, false);\n}\n\nstatic void f2fs_i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(f2fs_inode_cachep, F2FS_I(inode));\n}\n\nstatic void f2fs_destroy_inode(struct inode *inode)\n{\n\tcall_rcu(&inode->i_rcu, f2fs_i_callback);\n}\n\nstatic void destroy_percpu_info(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_destroy(&sbi->alloc_valid_block_count);\n\tpercpu_counter_destroy(&sbi->total_valid_inode_count);\n}\n\nstatic void destroy_device_list(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tblkdev_put(FDEV(i).bdev, FMODE_EXCL);\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tkfree(FDEV(i).blkz_type);\n#endif\n\t}\n\tkfree(sbi->devs);\n}\n\nstatic void f2fs_put_super(struct super_block *sb)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tint i;\n\n\tf2fs_quota_off_umount(sb);\n\n\t/* prevent remaining shrinker jobs */\n\tmutex_lock(&sbi->umount_mutex);\n\n\t/*\n\t * We don't need to do checkpoint when superblock is clean.\n\t * But, the previous checkpoint was not done by umount, it needs to do\n\t * clean checkpoint again.\n\t */\n\tif (is_sbi_flag_set(sbi, SBI_IS_DIRTY) ||\n\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* be sure to wait for any on-going discard commands */\n\tf2fs_wait_discard_bios(sbi, true);\n\n\tif (f2fs_discard_en(sbi) && !sbi->discard_blks) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT | CP_TRIMMED,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* write_checkpoint can update stat informaion */\n\tf2fs_destroy_stats(sbi);\n\n\t/*\n\t * normally superblock is clean, so we need to release this.\n\t * In addition, EIO will skip do checkpoint, we need this as well.\n\t */\n\trelease_ino_entry(sbi, true);\n\n\tf2fs_leave_shrinker(sbi);\n\tmutex_unlock(&sbi->umount_mutex);\n\n\t/* our cp_error case, we can wait for any writeback page */\n\tf2fs_flush_merged_writes(sbi);\n\n\tiput(sbi->node_inode);\n\tiput(sbi->meta_inode);\n\n\t/* destroy f2fs internal modules */\n\tdestroy_node_manager(sbi);\n\tdestroy_segment_manager(sbi);\n\n\tkfree(sbi->ckpt);\n\n\tf2fs_unregister_sysfs(sbi);\n\n\tsb->s_fs_info = NULL;\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi->raw_super);\n\n\tdestroy_device_list(sbi);\n\tmempool_destroy(sbi->write_io_dummy);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\tdestroy_percpu_info(sbi);\n\tfor (i = 0; i < NR_PAGE_TYPE; i++)\n\t\tkfree(sbi->write_io[i]);\n\tkfree(sbi);\n}\n\nint f2fs_sync_fs(struct super_block *sb, int sync)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tint err = 0;\n\n\ttrace_f2fs_sync_fs(sb, sync);\n\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\treturn -EAGAIN;\n\n\tif (sync) {\n\t\tstruct cp_control cpc;\n\n\t\tcpc.reason = __get_cp_reason(sbi);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t}\n\tf2fs_trace_ios(NULL, 1);\n\n\treturn err;\n}\n\nstatic int f2fs_freeze(struct super_block *sb)\n{\n\tif (f2fs_readonly(sb))\n\t\treturn 0;\n\n\t/* IO error happened before */\n\tif (unlikely(f2fs_cp_error(F2FS_SB(sb))))\n\t\treturn -EIO;\n\n\t/* must be clean, since sync_filesystem() was already called */\n\tif (is_sbi_flag_set(F2FS_SB(sb), SBI_IS_DIRTY))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int f2fs_unfreeze(struct super_block *sb)\n{\n\treturn 0;\n}\n\n#ifdef CONFIG_QUOTA\nstatic int f2fs_statfs_project(struct super_block *sb,\n\t\t\t\tkprojid_t projid, struct kstatfs *buf)\n{\n\tstruct kqid qid;\n\tstruct dquot *dquot;\n\tu64 limit;\n\tu64 curblock;\n\n\tqid = make_kqid_projid(projid);\n\tdquot = dqget(sb, qid);\n\tif (IS_ERR(dquot))\n\t\treturn PTR_ERR(dquot);\n\tspin_lock(&dq_data_lock);\n\n\tlimit = (dquot->dq_dqb.dqb_bsoftlimit ?\n\t\t dquot->dq_dqb.dqb_bsoftlimit :\n\t\t dquot->dq_dqb.dqb_bhardlimit) >> sb->s_blocksize_bits;\n\tif (limit && buf->f_blocks > limit) {\n\t\tcurblock = dquot->dq_dqb.dqb_curspace >> sb->s_blocksize_bits;\n\t\tbuf->f_blocks = limit;\n\t\tbuf->f_bfree = buf->f_bavail =\n\t\t\t(buf->f_blocks > curblock) ?\n\t\t\t (buf->f_blocks - curblock) : 0;\n\t}\n\n\tlimit = dquot->dq_dqb.dqb_isoftlimit ?\n\t\tdquot->dq_dqb.dqb_isoftlimit :\n\t\tdquot->dq_dqb.dqb_ihardlimit;\n\tif (limit && buf->f_files > limit) {\n\t\tbuf->f_files = limit;\n\t\tbuf->f_ffree =\n\t\t\t(buf->f_files > dquot->dq_dqb.dqb_curinodes) ?\n\t\t\t (buf->f_files - dquot->dq_dqb.dqb_curinodes) : 0;\n\t}\n\n\tspin_unlock(&dq_data_lock);\n\tdqput(dquot);\n\treturn 0;\n}\n#endif\n\nstatic int f2fs_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct super_block *sb = dentry->d_sb;\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tu64 id = huge_encode_dev(sb->s_bdev->bd_dev);\n\tblock_t total_count, user_block_count, start_count, ovp_count;\n\tu64 avail_node_count;\n\n\ttotal_count = le64_to_cpu(sbi->raw_super->block_count);\n\tuser_block_count = sbi->user_block_count;\n\tstart_count = le32_to_cpu(sbi->raw_super->segment0_blkaddr);\n\tovp_count = SM_I(sbi)->ovp_segments << sbi->log_blocks_per_seg;\n\tbuf->f_type = F2FS_SUPER_MAGIC;\n\tbuf->f_bsize = sbi->blocksize;\n\n\tbuf->f_blocks = total_count - start_count;\n\tbuf->f_bfree = user_block_count - valid_user_blocks(sbi) + ovp_count;\n\tbuf->f_bavail = user_block_count - valid_user_blocks(sbi) -\n\t\t\t\t\t\tsbi->reserved_blocks;\n\n\tavail_node_count = sbi->total_node_count - F2FS_RESERVED_NODE_NUM;\n\n\tif (avail_node_count > user_block_count) {\n\t\tbuf->f_files = user_block_count;\n\t\tbuf->f_ffree = buf->f_bavail;\n\t} else {\n\t\tbuf->f_files = avail_node_count;\n\t\tbuf->f_ffree = min(avail_node_count - valid_node_count(sbi),\n\t\t\t\t\tbuf->f_bavail);\n\t}\n\n\tbuf->f_namelen = F2FS_NAME_LEN;\n\tbuf->f_fsid.val[0] = (u32)id;\n\tbuf->f_fsid.val[1] = (u32)(id >> 32);\n\n#ifdef CONFIG_QUOTA\n\tif (is_inode_flag_set(dentry->d_inode, FI_PROJ_INHERIT) &&\n\t\t\tsb_has_quota_limits_enabled(sb, PRJQUOTA)) {\n\t\tf2fs_statfs_project(sb, F2FS_I(dentry->d_inode)->i_projid, buf);\n\t}\n#endif\n\treturn 0;\n}\n\nstatic inline void f2fs_show_quota_options(struct seq_file *seq,\n\t\t\t\t\t   struct super_block *sb)\n{\n#ifdef CONFIG_QUOTA\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\n\tif (sbi->s_jquota_fmt) {\n\t\tchar *fmtname = \"\";\n\n\t\tswitch (sbi->s_jquota_fmt) {\n\t\tcase QFMT_VFS_OLD:\n\t\t\tfmtname = \"vfsold\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V0:\n\t\t\tfmtname = \"vfsv0\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V1:\n\t\t\tfmtname = \"vfsv1\";\n\t\t\tbreak;\n\t\t}\n\t\tseq_printf(seq, \",jqfmt=%s\", fmtname);\n\t}\n\n\tif (sbi->s_qf_names[USRQUOTA])\n\t\tseq_show_option(seq, \"usrjquota\", sbi->s_qf_names[USRQUOTA]);\n\n\tif (sbi->s_qf_names[GRPQUOTA])\n\t\tseq_show_option(seq, \"grpjquota\", sbi->s_qf_names[GRPQUOTA]);\n\n\tif (sbi->s_qf_names[PRJQUOTA])\n\t\tseq_show_option(seq, \"prjjquota\", sbi->s_qf_names[PRJQUOTA]);\n#endif\n}\n\nstatic int f2fs_show_options(struct seq_file *seq, struct dentry *root)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(root->d_sb);\n\n\tif (!f2fs_readonly(sbi->sb) && test_opt(sbi, BG_GC)) {\n\t\tif (test_opt(sbi, FORCE_FG_GC))\n\t\t\tseq_printf(seq, \",background_gc=%s\", \"sync\");\n\t\telse\n\t\t\tseq_printf(seq, \",background_gc=%s\", \"on\");\n\t} else {\n\t\tseq_printf(seq, \",background_gc=%s\", \"off\");\n\t}\n\tif (test_opt(sbi, DISABLE_ROLL_FORWARD))\n\t\tseq_puts(seq, \",disable_roll_forward\");\n\tif (test_opt(sbi, DISCARD))\n\t\tseq_puts(seq, \",discard\");\n\tif (test_opt(sbi, NOHEAP))\n\t\tseq_puts(seq, \",no_heap\");\n\telse\n\t\tseq_puts(seq, \",heap\");\n#ifdef CONFIG_F2FS_FS_XATTR\n\tif (test_opt(sbi, XATTR_USER))\n\t\tseq_puts(seq, \",user_xattr\");\n\telse\n\t\tseq_puts(seq, \",nouser_xattr\");\n\tif (test_opt(sbi, INLINE_XATTR))\n\t\tseq_puts(seq, \",inline_xattr\");\n\telse\n\t\tseq_puts(seq, \",noinline_xattr\");\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\tif (test_opt(sbi, POSIX_ACL))\n\t\tseq_puts(seq, \",acl\");\n\telse\n\t\tseq_puts(seq, \",noacl\");\n#endif\n\tif (test_opt(sbi, DISABLE_EXT_IDENTIFY))\n\t\tseq_puts(seq, \",disable_ext_identify\");\n\tif (test_opt(sbi, INLINE_DATA))\n\t\tseq_puts(seq, \",inline_data\");\n\telse\n\t\tseq_puts(seq, \",noinline_data\");\n\tif (test_opt(sbi, INLINE_DENTRY))\n\t\tseq_puts(seq, \",inline_dentry\");\n\telse\n\t\tseq_puts(seq, \",noinline_dentry\");\n\tif (!f2fs_readonly(sbi->sb) && test_opt(sbi, FLUSH_MERGE))\n\t\tseq_puts(seq, \",flush_merge\");\n\tif (test_opt(sbi, NOBARRIER))\n\t\tseq_puts(seq, \",nobarrier\");\n\tif (test_opt(sbi, FASTBOOT))\n\t\tseq_puts(seq, \",fastboot\");\n\tif (test_opt(sbi, EXTENT_CACHE))\n\t\tseq_puts(seq, \",extent_cache\");\n\telse\n\t\tseq_puts(seq, \",noextent_cache\");\n\tif (test_opt(sbi, DATA_FLUSH))\n\t\tseq_puts(seq, \",data_flush\");\n\n\tseq_puts(seq, \",mode=\");\n\tif (test_opt(sbi, ADAPTIVE))\n\t\tseq_puts(seq, \"adaptive\");\n\telse if (test_opt(sbi, LFS))\n\t\tseq_puts(seq, \"lfs\");\n\tseq_printf(seq, \",active_logs=%u\", sbi->active_logs);\n\tif (F2FS_IO_SIZE_BITS(sbi))\n\t\tseq_printf(seq, \",io_size=%uKB\", F2FS_IO_SIZE_KB(sbi));\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tif (test_opt(sbi, FAULT_INJECTION))\n\t\tseq_printf(seq, \",fault_injection=%u\",\n\t\t\t\tsbi->fault_info.inject_rate);\n#endif\n#ifdef CONFIG_QUOTA\n\tif (test_opt(sbi, QUOTA))\n\t\tseq_puts(seq, \",quota\");\n\tif (test_opt(sbi, USRQUOTA))\n\t\tseq_puts(seq, \",usrquota\");\n\tif (test_opt(sbi, GRPQUOTA))\n\t\tseq_puts(seq, \",grpquota\");\n\tif (test_opt(sbi, PRJQUOTA))\n\t\tseq_puts(seq, \",prjquota\");\n#endif\n\tf2fs_show_quota_options(seq, sbi->sb);\n\n\treturn 0;\n}\n\nstatic void default_options(struct f2fs_sb_info *sbi)\n{\n\t/* init some FS parameters */\n\tsbi->active_logs = NR_CURSEG_TYPE;\n\n\tset_opt(sbi, BG_GC);\n\tset_opt(sbi, INLINE_XATTR);\n\tset_opt(sbi, INLINE_DATA);\n\tset_opt(sbi, INLINE_DENTRY);\n\tset_opt(sbi, EXTENT_CACHE);\n\tset_opt(sbi, NOHEAP);\n\tsbi->sb->s_flags |= MS_LAZYTIME;\n\tset_opt(sbi, FLUSH_MERGE);\n\tif (f2fs_sb_mounted_blkzoned(sbi->sb)) {\n\t\tset_opt_mode(sbi, F2FS_MOUNT_LFS);\n\t\tset_opt(sbi, DISCARD);\n\t} else {\n\t\tset_opt_mode(sbi, F2FS_MOUNT_ADAPTIVE);\n\t}\n\n#ifdef CONFIG_F2FS_FS_XATTR\n\tset_opt(sbi, XATTR_USER);\n#endif\n#ifdef CONFIG_F2FS_FS_POSIX_ACL\n\tset_opt(sbi, POSIX_ACL);\n#endif\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tf2fs_build_fault_attr(sbi, 0);\n#endif\n}\n\nstatic int f2fs_remount(struct super_block *sb, int *flags, char *data)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct f2fs_mount_info org_mount_opt;\n\tunsigned long old_sb_flags;\n\tint err, active_logs;\n\tbool need_restart_gc = false;\n\tbool need_stop_gc = false;\n\tbool no_extent_cache = !test_opt(sbi, EXTENT_CACHE);\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct f2fs_fault_info ffi = sbi->fault_info;\n#endif\n#ifdef CONFIG_QUOTA\n\tint s_jquota_fmt;\n\tchar *s_qf_names[MAXQUOTAS];\n\tint i, j;\n#endif\n\n\t/*\n\t * Save the old mount options in case we\n\t * need to restore them.\n\t */\n\torg_mount_opt = sbi->mount_opt;\n\told_sb_flags = sb->s_flags;\n\tactive_logs = sbi->active_logs;\n\n#ifdef CONFIG_QUOTA\n\ts_jquota_fmt = sbi->s_jquota_fmt;\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (sbi->s_qf_names[i]) {\n\t\t\ts_qf_names[i] = kstrdup(sbi->s_qf_names[i],\n\t\t\t\t\t\t\t GFP_KERNEL);\n\t\t\tif (!s_qf_names[i]) {\n\t\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\t\tkfree(s_qf_names[j]);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\ts_qf_names[i] = NULL;\n\t\t}\n\t}\n#endif\n\n\t/* recover superblocks we couldn't write due to previous RO mount */\n\tif (!(*flags & MS_RDONLY) && is_sbi_flag_set(sbi, SBI_NEED_SB_WRITE)) {\n\t\terr = f2fs_commit_super(sbi, false);\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Try to recover all the superblocks, ret: %d\", err);\n\t\tif (!err)\n\t\t\tclear_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t}\n\n\tdefault_options(sbi);\n\n\t/* parse mount options */\n\terr = parse_options(sb, data);\n\tif (err)\n\t\tgoto restore_opts;\n\n\t/*\n\t * Previous and new state of filesystem is RO,\n\t * so skip checking GC and FLUSH_MERGE conditions.\n\t */\n\tif (f2fs_readonly(sb) && (*flags & MS_RDONLY))\n\t\tgoto skip;\n\n\tif (!f2fs_readonly(sb) && (*flags & MS_RDONLY)) {\n\t\terr = dquot_suspend(sb, -1);\n\t\tif (err < 0)\n\t\t\tgoto restore_opts;\n\t} else {\n\t\t/* dquot_resume needs RW */\n\t\tsb->s_flags &= ~MS_RDONLY;\n\t\tdquot_resume(sb, -1);\n\t}\n\n\t/* disallow enable/disable extent_cache dynamically */\n\tif (no_extent_cache == !!test_opt(sbi, EXTENT_CACHE)) {\n\t\terr = -EINVAL;\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\t\"switch extent_cache option is not allowed\");\n\t\tgoto restore_opts;\n\t}\n\n\t/*\n\t * We stop the GC thread if FS is mounted as RO\n\t * or if background_gc = off is passed in mount\n\t * option. Also sync the filesystem.\n\t */\n\tif ((*flags & MS_RDONLY) || !test_opt(sbi, BG_GC)) {\n\t\tif (sbi->gc_thread) {\n\t\t\tstop_gc_thread(sbi);\n\t\t\tneed_restart_gc = true;\n\t\t}\n\t} else if (!sbi->gc_thread) {\n\t\terr = start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto restore_opts;\n\t\tneed_stop_gc = true;\n\t}\n\n\tif (*flags & MS_RDONLY) {\n\t\twriteback_inodes_sb(sb, WB_REASON_SYNC);\n\t\tsync_inodes_sb(sb);\n\n\t\tset_sbi_flag(sbi, SBI_IS_DIRTY);\n\t\tset_sbi_flag(sbi, SBI_IS_CLOSE);\n\t\tf2fs_sync_fs(sb, 1);\n\t\tclear_sbi_flag(sbi, SBI_IS_CLOSE);\n\t}\n\n\t/*\n\t * We stop issue flush thread if FS is mounted as RO\n\t * or if flush_merge is not passed in mount option.\n\t */\n\tif ((*flags & MS_RDONLY) || !test_opt(sbi, FLUSH_MERGE)) {\n\t\tclear_opt(sbi, FLUSH_MERGE);\n\t\tdestroy_flush_cmd_control(sbi, false);\n\t} else {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\tgoto restore_gc;\n\t}\nskip:\n#ifdef CONFIG_QUOTA\n\t/* Release old quota file names */\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(s_qf_names[i]);\n#endif\n\t/* Update the POSIXACL Flag */\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? MS_POSIXACL : 0);\n\n\treturn 0;\nrestore_gc:\n\tif (need_restart_gc) {\n\t\tif (start_gc_thread(sbi))\n\t\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\t\"background gc thread has stopped\");\n\t} else if (need_stop_gc) {\n\t\tstop_gc_thread(sbi);\n\t}\nrestore_opts:\n#ifdef CONFIG_QUOTA\n\tsbi->s_jquota_fmt = s_jquota_fmt;\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tkfree(sbi->s_qf_names[i]);\n\t\tsbi->s_qf_names[i] = s_qf_names[i];\n\t}\n#endif\n\tsbi->mount_opt = org_mount_opt;\n\tsbi->active_logs = active_logs;\n\tsb->s_flags = old_sb_flags;\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tsbi->fault_info = ffi;\n#endif\n\treturn err;\n}\n\n#ifdef CONFIG_QUOTA\n/* Read data from quotafile */\nstatic ssize_t f2fs_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\tstruct address_space *mapping = inode->i_mapping;\n\tblock_t blkidx = F2FS_BYTES_TO_BLK(off);\n\tint offset = off & (sb->s_blocksize - 1);\n\tint tocopy;\n\tsize_t toread;\n\tloff_t i_size = i_size_read(inode);\n\tstruct page *page;\n\tchar *kaddr;\n\n\tif (off > i_size)\n\t\treturn 0;\n\n\tif (off + len > i_size)\n\t\tlen = i_size - off;\n\ttoread = len;\n\twhile (toread > 0) {\n\t\ttocopy = min_t(unsigned long, sb->s_blocksize - offset, toread);\nrepeat:\n\t\tpage = read_mapping_page(mapping, blkidx, NULL);\n\t\tif (IS_ERR(page))\n\t\t\treturn PTR_ERR(page);\n\n\t\tlock_page(page);\n\n\t\tif (unlikely(page->mapping != mapping)) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(!PageUptodate(page))) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(data, kaddr + offset, tocopy);\n\t\tkunmap_atomic(kaddr);\n\t\tf2fs_put_page(page, 1);\n\n\t\toffset = 0;\n\t\ttoread -= tocopy;\n\t\tdata += tocopy;\n\t\tblkidx++;\n\t}\n\treturn len;\n}\n\n/* Write to quotafile */\nstatic ssize_t f2fs_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\tstruct address_space *mapping = inode->i_mapping;\n\tconst struct address_space_operations *a_ops = mapping->a_ops;\n\tint offset = off & (sb->s_blocksize - 1);\n\tsize_t towrite = len;\n\tstruct page *page;\n\tchar *kaddr;\n\tint err = 0;\n\tint tocopy;\n\n\twhile (towrite > 0) {\n\t\ttocopy = min_t(unsigned long, sb->s_blocksize - offset,\n\t\t\t\t\t\t\t\ttowrite);\n\n\t\terr = a_ops->write_begin(NULL, mapping, off, tocopy, 0,\n\t\t\t\t\t\t\t&page, NULL);\n\t\tif (unlikely(err))\n\t\t\tbreak;\n\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(kaddr + offset, data, tocopy);\n\t\tkunmap_atomic(kaddr);\n\t\tflush_dcache_page(page);\n\n\t\ta_ops->write_end(NULL, mapping, off, tocopy, tocopy,\n\t\t\t\t\t\tpage, NULL);\n\t\toffset = 0;\n\t\ttowrite -= tocopy;\n\t\toff += tocopy;\n\t\tdata += tocopy;\n\t\tcond_resched();\n\t}\n\n\tif (len == towrite)\n\t\treturn 0;\n\tinode->i_version++;\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\tf2fs_mark_inode_dirty_sync(inode, false);\n\treturn len - towrite;\n}\n\nstatic struct dquot **f2fs_get_dquots(struct inode *inode)\n{\n\treturn F2FS_I(inode)->i_dquot;\n}\n\nstatic qsize_t *f2fs_get_reserved_space(struct inode *inode)\n{\n\treturn &F2FS_I(inode)->i_reserved_quota;\n}\n\nstatic int f2fs_quota_on_mount(struct f2fs_sb_info *sbi, int type)\n{\n\treturn dquot_quota_on_mount(sbi->sb, sbi->s_qf_names[type],\n\t\t\t\t\t\tsbi->s_jquota_fmt, type);\n}\n\nvoid f2fs_enable_quota_files(struct f2fs_sb_info *sbi)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (sbi->s_qf_names[i]) {\n\t\t\tret = f2fs_quota_on_mount(sbi, i);\n\t\t\tif (ret < 0)\n\t\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\t\"Cannot turn on journaled \"\n\t\t\t\t\t\"quota: error %d\", ret);\n\t\t}\n\t}\n}\n\nstatic int f2fs_quota_sync(struct super_block *sb, int type)\n{\n\tstruct quota_info *dqopt = sb_dqopt(sb);\n\tint cnt;\n\tint ret;\n\n\tret = dquot_writeback_dquots(sb, type);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Now when everything is written we can discard the pagecache so\n\t * that userspace sees the changes.\n\t */\n\tfor (cnt = 0; cnt < MAXQUOTAS; cnt++) {\n\t\tif (type != -1 && cnt != type)\n\t\t\tcontinue;\n\t\tif (!sb_has_quota_active(sb, cnt))\n\t\t\tcontinue;\n\n\t\tret = filemap_write_and_wait(dqopt->files[cnt]->i_mapping);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tinode_lock(dqopt->files[cnt]);\n\t\ttruncate_inode_pages(&dqopt->files[cnt]->i_data, 0);\n\t\tinode_unlock(dqopt->files[cnt]);\n\t}\n\treturn 0;\n}\n\nstatic int f2fs_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t\t\t\t\tconst struct path *path)\n{\n\tstruct inode *inode;\n\tint err;\n\n\terr = f2fs_quota_sync(sb, type);\n\tif (err)\n\t\treturn err;\n\n\terr = dquot_quota_on(sb, type, format_id, path);\n\tif (err)\n\t\treturn err;\n\n\tinode = d_inode(path->dentry);\n\n\tinode_lock(inode);\n\tF2FS_I(inode)->i_flags |= FS_NOATIME_FL | FS_IMMUTABLE_FL;\n\tinode_set_flags(inode, S_NOATIME | S_IMMUTABLE,\n\t\t\t\t\tS_NOATIME | S_IMMUTABLE);\n\tinode_unlock(inode);\n\tf2fs_mark_inode_dirty_sync(inode, false);\n\n\treturn 0;\n}\n\nstatic int f2fs_quota_off(struct super_block *sb, int type)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\tint err;\n\n\tif (!inode || !igrab(inode))\n\t\treturn dquot_quota_off(sb, type);\n\n\tf2fs_quota_sync(sb, type);\n\n\terr = dquot_quota_off(sb, type);\n\tif (err)\n\t\tgoto out_put;\n\n\tinode_lock(inode);\n\tF2FS_I(inode)->i_flags &= ~(FS_NOATIME_FL | FS_IMMUTABLE_FL);\n\tinode_set_flags(inode, 0, S_NOATIME | S_IMMUTABLE);\n\tinode_unlock(inode);\n\tf2fs_mark_inode_dirty_sync(inode, false);\nout_put:\n\tiput(inode);\n\treturn err;\n}\n\nvoid f2fs_quota_off_umount(struct super_block *sb)\n{\n\tint type;\n\n\tfor (type = 0; type < MAXQUOTAS; type++)\n\t\tf2fs_quota_off(sb, type);\n}\n\nint f2fs_get_projid(struct inode *inode, kprojid_t *projid)\n{\n\t*projid = F2FS_I(inode)->i_projid;\n\treturn 0;\n}\n\nstatic const struct dquot_operations f2fs_quota_operations = {\n\t.get_reserved_space = f2fs_get_reserved_space,\n\t.write_dquot\t= dquot_commit,\n\t.acquire_dquot\t= dquot_acquire,\n\t.release_dquot\t= dquot_release,\n\t.mark_dirty\t= dquot_mark_dquot_dirty,\n\t.write_info\t= dquot_commit_info,\n\t.alloc_dquot\t= dquot_alloc,\n\t.destroy_dquot\t= dquot_destroy,\n\t.get_projid\t= f2fs_get_projid,\n\t.get_next_id\t= dquot_get_next_id,\n};\n\nstatic const struct quotactl_ops f2fs_quotactl_ops = {\n\t.quota_on\t= f2fs_quota_on,\n\t.quota_off\t= f2fs_quota_off,\n\t.quota_sync\t= f2fs_quota_sync,\n\t.get_state\t= dquot_get_state,\n\t.set_info\t= dquot_set_dqinfo,\n\t.get_dqblk\t= dquot_get_dqblk,\n\t.set_dqblk\t= dquot_set_dqblk,\n\t.get_nextdqblk\t= dquot_get_next_dqblk,\n};\n#else\nvoid f2fs_quota_off_umount(struct super_block *sb)\n{\n}\n#endif\n\nstatic const struct super_operations f2fs_sops = {\n\t.alloc_inode\t= f2fs_alloc_inode,\n\t.drop_inode\t= f2fs_drop_inode,\n\t.destroy_inode\t= f2fs_destroy_inode,\n\t.write_inode\t= f2fs_write_inode,\n\t.dirty_inode\t= f2fs_dirty_inode,\n\t.show_options\t= f2fs_show_options,\n#ifdef CONFIG_QUOTA\n\t.quota_read\t= f2fs_quota_read,\n\t.quota_write\t= f2fs_quota_write,\n\t.get_dquots\t= f2fs_get_dquots,\n#endif\n\t.evict_inode\t= f2fs_evict_inode,\n\t.put_super\t= f2fs_put_super,\n\t.sync_fs\t= f2fs_sync_fs,\n\t.freeze_fs\t= f2fs_freeze,\n\t.unfreeze_fs\t= f2fs_unfreeze,\n\t.statfs\t\t= f2fs_statfs,\n\t.remount_fs\t= f2fs_remount,\n};\n\n#ifdef CONFIG_F2FS_FS_ENCRYPTION\nstatic int f2fs_get_context(struct inode *inode, void *ctx, size_t len)\n{\n\treturn f2fs_getxattr(inode, F2FS_XATTR_INDEX_ENCRYPTION,\n\t\t\t\tF2FS_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\tctx, len, NULL);\n}\n\nstatic int f2fs_set_context(struct inode *inode, const void *ctx, size_t len,\n\t\t\t\t\t\t\tvoid *fs_data)\n{\n\treturn f2fs_setxattr(inode, F2FS_XATTR_INDEX_ENCRYPTION,\n\t\t\t\tF2FS_XATTR_NAME_ENCRYPTION_CONTEXT,\n\t\t\t\tctx, len, fs_data, XATTR_CREATE);\n}\n\nstatic unsigned f2fs_max_namelen(struct inode *inode)\n{\n\treturn S_ISLNK(inode->i_mode) ?\n\t\t\tinode->i_sb->s_blocksize : F2FS_NAME_LEN;\n}\n\nstatic const struct fscrypt_operations f2fs_cryptops = {\n\t.key_prefix\t= \"f2fs:\",\n\t.get_context\t= f2fs_get_context,\n\t.set_context\t= f2fs_set_context,\n\t.is_encrypted\t= f2fs_encrypted_inode,\n\t.empty_dir\t= f2fs_empty_dir,\n\t.max_namelen\t= f2fs_max_namelen,\n};\n#else\nstatic const struct fscrypt_operations f2fs_cryptops = {\n\t.is_encrypted\t= f2fs_encrypted_inode,\n};\n#endif\n\nstatic struct inode *f2fs_nfs_get_inode(struct super_block *sb,\n\t\tu64 ino, u32 generation)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tstruct inode *inode;\n\n\tif (check_nid_range(sbi, ino))\n\t\treturn ERR_PTR(-ESTALE);\n\n\t/*\n\t * f2fs_iget isn't quite right if the inode is currently unallocated!\n\t * However f2fs_iget currently does appropriate checks to handle stale\n\t * inodes so everything is OK.\n\t */\n\tinode = f2fs_iget(sb, ino);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (unlikely(generation && inode->i_generation != generation)) {\n\t\t/* we didn't find the right inode.. */\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\treturn inode;\n}\n\nstatic struct dentry *f2fs_fh_to_dentry(struct super_block *sb, struct fid *fid,\n\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_dentry(sb, fid, fh_len, fh_type,\n\t\t\t\t    f2fs_nfs_get_inode);\n}\n\nstatic struct dentry *f2fs_fh_to_parent(struct super_block *sb, struct fid *fid,\n\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_parent(sb, fid, fh_len, fh_type,\n\t\t\t\t    f2fs_nfs_get_inode);\n}\n\nstatic const struct export_operations f2fs_export_ops = {\n\t.fh_to_dentry = f2fs_fh_to_dentry,\n\t.fh_to_parent = f2fs_fh_to_parent,\n\t.get_parent = f2fs_get_parent,\n};\n\nstatic loff_t max_file_blocks(void)\n{\n\tloff_t result = 0;\n\tloff_t leaf_count = ADDRS_PER_BLOCK;\n\n\t/*\n\t * note: previously, result is equal to (DEF_ADDRS_PER_INODE -\n\t * F2FS_INLINE_XATTR_ADDRS), but now f2fs try to reserve more\n\t * space in inode.i_addr, it will be more safe to reassign\n\t * result as zero.\n\t */\n\n\t/* two direct node blocks */\n\tresult += (leaf_count * 2);\n\n\t/* two indirect node blocks */\n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += (leaf_count * 2);\n\n\t/* one double indirect node block */\n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += leaf_count;\n\n\treturn result;\n}\n\nstatic int __f2fs_commit_super(struct buffer_head *bh,\n\t\t\tstruct f2fs_super_block *super)\n{\n\tlock_buffer(bh);\n\tif (super)\n\t\tmemcpy(bh->b_data + F2FS_SUPER_OFFSET, super, sizeof(*super));\n\tset_buffer_uptodate(bh);\n\tset_buffer_dirty(bh);\n\tunlock_buffer(bh);\n\n\t/* it's rare case, we can do fua all the time */\n\treturn __sync_dirty_buffer(bh, REQ_SYNC | REQ_PREFLUSH | REQ_FUA);\n}\n\nstatic inline bool sanity_check_area_boundary(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct buffer_head *bh)\n{\n\tstruct f2fs_super_block *raw_super = (struct f2fs_super_block *)\n\t\t\t\t\t(bh->b_data + F2FS_SUPER_OFFSET);\n\tstruct super_block *sb = sbi->sb;\n\tu32 segment0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tu32 cp_blkaddr = le32_to_cpu(raw_super->cp_blkaddr);\n\tu32 sit_blkaddr = le32_to_cpu(raw_super->sit_blkaddr);\n\tu32 nat_blkaddr = le32_to_cpu(raw_super->nat_blkaddr);\n\tu32 ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tu32 main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tu32 segment_count_ckpt = le32_to_cpu(raw_super->segment_count_ckpt);\n\tu32 segment_count_sit = le32_to_cpu(raw_super->segment_count_sit);\n\tu32 segment_count_nat = le32_to_cpu(raw_super->segment_count_nat);\n\tu32 segment_count_ssa = le32_to_cpu(raw_super->segment_count_ssa);\n\tu32 segment_count_main = le32_to_cpu(raw_super->segment_count_main);\n\tu32 segment_count = le32_to_cpu(raw_super->segment_count);\n\tu32 log_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tu64 main_end_blkaddr = main_blkaddr +\n\t\t\t\t(segment_count_main << log_blocks_per_seg);\n\tu64 seg_end_blkaddr = segment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg);\n\n\tif (segment0_blkaddr != cp_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Mismatch start address, segment0(%u) cp_blkaddr(%u)\",\n\t\t\tsegment0_blkaddr, cp_blkaddr);\n\t\treturn true;\n\t}\n\n\tif (cp_blkaddr + (segment_count_ckpt << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tsit_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong CP boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tcp_blkaddr, sit_blkaddr,\n\t\t\tsegment_count_ckpt << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (sit_blkaddr + (segment_count_sit << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tnat_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong SIT boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tsit_blkaddr, nat_blkaddr,\n\t\t\tsegment_count_sit << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (nat_blkaddr + (segment_count_nat << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tssa_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong NAT boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tnat_blkaddr, ssa_blkaddr,\n\t\t\tsegment_count_nat << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (ssa_blkaddr + (segment_count_ssa << log_blocks_per_seg) !=\n\t\t\t\t\t\t\tmain_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong SSA boundary, start(%u) end(%u) blocks(%u)\",\n\t\t\tssa_blkaddr, main_blkaddr,\n\t\t\tsegment_count_ssa << log_blocks_per_seg);\n\t\treturn true;\n\t}\n\n\tif (main_end_blkaddr > seg_end_blkaddr) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Wrong MAIN_AREA boundary, start(%u) end(%u) block(%u)\",\n\t\t\tmain_blkaddr,\n\t\t\tsegment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg),\n\t\t\tsegment_count_main << log_blocks_per_seg);\n\t\treturn true;\n\t} else if (main_end_blkaddr < seg_end_blkaddr) {\n\t\tint err = 0;\n\t\tchar *res;\n\n\t\t/* fix in-memory information all the time */\n\t\traw_super->segment_count = cpu_to_le32((main_end_blkaddr -\n\t\t\t\tsegment0_blkaddr) >> log_blocks_per_seg);\n\n\t\tif (f2fs_readonly(sb) || bdev_read_only(sb->s_bdev)) {\n\t\t\tset_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t\t\tres = \"internally\";\n\t\t} else {\n\t\t\terr = __f2fs_commit_super(bh, NULL);\n\t\t\tres = err ? \"failed\" : \"done\";\n\t\t}\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Fix alignment : %s, start(%u) end(%u) block(%u)\",\n\t\t\tres, main_blkaddr,\n\t\t\tsegment0_blkaddr +\n\t\t\t\t(segment_count << log_blocks_per_seg),\n\t\t\tsegment_count_main << log_blocks_per_seg);\n\t\tif (err)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int sanity_check_raw_super(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct buffer_head *bh)\n{\n\tstruct f2fs_super_block *raw_super = (struct f2fs_super_block *)\n\t\t\t\t\t(bh->b_data + F2FS_SUPER_OFFSET);\n\tstruct super_block *sb = sbi->sb;\n\tunsigned int blocksize;\n\n\tif (F2FS_SUPER_MAGIC != le32_to_cpu(raw_super->magic)) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Magic Mismatch, valid(0x%x) - read(0x%x)\",\n\t\t\tF2FS_SUPER_MAGIC, le32_to_cpu(raw_super->magic));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB page cache size */\n\tif (F2FS_BLKSIZE != PAGE_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid page_cache_size (%lu), supports only 4KB\\n\",\n\t\t\tPAGE_SIZE);\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB block size */\n\tblocksize = 1 << le32_to_cpu(raw_super->log_blocksize);\n\tif (blocksize != F2FS_BLKSIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid blocksize (%u), supports only 4KB\\n\",\n\t\t\tblocksize);\n\t\treturn 1;\n\t}\n\n\t/* check log blocks per segment */\n\tif (le32_to_cpu(raw_super->log_blocks_per_seg) != 9) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log blocks per segment (%u)\\n\",\n\t\t\tle32_to_cpu(raw_super->log_blocks_per_seg));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support 512/1024/2048/4096 bytes sector size */\n\tif (le32_to_cpu(raw_super->log_sectorsize) >\n\t\t\t\tF2FS_MAX_LOG_SECTOR_SIZE ||\n\t\tle32_to_cpu(raw_super->log_sectorsize) <\n\t\t\t\tF2FS_MIN_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO, \"Invalid log sectorsize (%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\tif (le32_to_cpu(raw_super->log_sectors_per_block) +\n\t\tle32_to_cpu(raw_super->log_sectorsize) !=\n\t\t\tF2FS_MAX_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log sectors per block(%u) log sectorsize(%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectors_per_block),\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\n\t/* check reserved ino info */\n\tif (le32_to_cpu(raw_super->node_ino) != 1 ||\n\t\tle32_to_cpu(raw_super->meta_ino) != 2 ||\n\t\tle32_to_cpu(raw_super->root_ino) != 3) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid Fs Meta Ino: node(%u) meta(%u) root(%u)\",\n\t\t\tle32_to_cpu(raw_super->node_ino),\n\t\t\tle32_to_cpu(raw_super->meta_ino),\n\t\t\tle32_to_cpu(raw_super->root_ino));\n\t\treturn 1;\n\t}\n\n\tif (le32_to_cpu(raw_super->segment_count) > F2FS_MAX_SEGMENT) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid segment count (%u)\",\n\t\t\tle32_to_cpu(raw_super->segment_count));\n\t\treturn 1;\n\t}\n\n\t/* check CP/SIT/NAT/SSA/MAIN_AREA area boundary */\n\tif (sanity_check_area_boundary(sbi, bh))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nint sanity_check_ckpt(struct f2fs_sb_info *sbi)\n{\n\tunsigned int total, fsmeta;\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tunsigned int ovp_segments, reserved_segments;\n\tunsigned int main_segs, blocks_per_seg;\n\tint i;\n\n\ttotal = le32_to_cpu(raw_super->segment_count);\n\tfsmeta = le32_to_cpu(raw_super->segment_count_ckpt);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_sit);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_nat);\n\tfsmeta += le32_to_cpu(ckpt->rsvd_segment_count);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_ssa);\n\n\tif (unlikely(fsmeta >= total))\n\t\treturn 1;\n\n\tovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\treserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\n\tif (unlikely(fsmeta < F2FS_MIN_SEGMENTS ||\n\t\t\tovp_segments == 0 || reserved_segments == 0)) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong layout: check mkfs.f2fs version\");\n\t\treturn 1;\n\t}\n\n\tmain_segs = le32_to_cpu(raw_super->segment_count_main);\n\tblocks_per_seg = sbi->blocks_per_seg;\n\n\tfor (i = 0; i < NR_CURSEG_NODE_TYPE; i++) {\n\t\tif (le32_to_cpu(ckpt->cur_node_segno[i]) >= main_segs ||\n\t\t\tle16_to_cpu(ckpt->cur_node_blkoff[i]) >= blocks_per_seg)\n\t\t\treturn 1;\n\t}\n\tfor (i = 0; i < NR_CURSEG_DATA_TYPE; i++) {\n\t\tif (le32_to_cpu(ckpt->cur_data_segno[i]) >= main_segs ||\n\t\t\tle16_to_cpu(ckpt->cur_data_blkoff[i]) >= blocks_per_seg)\n\t\t\treturn 1;\n\t}\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR, \"A bug case: need to run fsck\");\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void init_sb_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = sbi->raw_super;\n\tint i, j;\n\n\tsbi->log_sectors_per_block =\n\t\tle32_to_cpu(raw_super->log_sectors_per_block);\n\tsbi->log_blocksize = le32_to_cpu(raw_super->log_blocksize);\n\tsbi->blocksize = 1 << sbi->log_blocksize;\n\tsbi->log_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tsbi->blocks_per_seg = 1 << sbi->log_blocks_per_seg;\n\tsbi->segs_per_sec = le32_to_cpu(raw_super->segs_per_sec);\n\tsbi->secs_per_zone = le32_to_cpu(raw_super->secs_per_zone);\n\tsbi->total_sections = le32_to_cpu(raw_super->section_count);\n\tsbi->total_node_count =\n\t\t(le32_to_cpu(raw_super->segment_count_nat) / 2)\n\t\t\t* sbi->blocks_per_seg * NAT_ENTRY_PER_BLOCK;\n\tsbi->root_ino_num = le32_to_cpu(raw_super->root_ino);\n\tsbi->node_ino_num = le32_to_cpu(raw_super->node_ino);\n\tsbi->meta_ino_num = le32_to_cpu(raw_super->meta_ino);\n\tsbi->cur_victim_sec = NULL_SECNO;\n\tsbi->max_victim_search = DEF_MAX_VICTIM_SEARCH;\n\n\tsbi->dir_level = DEF_DIR_LEVEL;\n\tsbi->interval_time[CP_TIME] = DEF_CP_INTERVAL;\n\tsbi->interval_time[REQ_TIME] = DEF_IDLE_INTERVAL;\n\tclear_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\tfor (i = 0; i < NR_COUNT_TYPE; i++)\n\t\tatomic_set(&sbi->nr_pages[i], 0);\n\n\tatomic_set(&sbi->wb_sync_req, 0);\n\n\tINIT_LIST_HEAD(&sbi->s_list);\n\tmutex_init(&sbi->umount_mutex);\n\tfor (i = 0; i < NR_PAGE_TYPE - 1; i++)\n\t\tfor (j = HOT; j < NR_TEMP_TYPE; j++)\n\t\t\tmutex_init(&sbi->wio_mutex[i][j]);\n\tspin_lock_init(&sbi->cp_lock);\n}\n\nstatic int init_percpu_info(struct f2fs_sb_info *sbi)\n{\n\tint err;\n\n\terr = percpu_counter_init(&sbi->alloc_valid_block_count, 0, GFP_KERNEL);\n\tif (err)\n\t\treturn err;\n\n\treturn percpu_counter_init(&sbi->total_valid_inode_count, 0,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int init_blkz_info(struct f2fs_sb_info *sbi, int devi)\n{\n\tstruct block_device *bdev = FDEV(devi).bdev;\n\tsector_t nr_sectors = bdev->bd_part->nr_sects;\n\tsector_t sector = 0;\n\tstruct blk_zone *zones;\n\tunsigned int i, nr_zones;\n\tunsigned int n = 0;\n\tint err = -EIO;\n\n\tif (!f2fs_sb_mounted_blkzoned(sbi->sb))\n\t\treturn 0;\n\n\tif (sbi->blocks_per_blkz && sbi->blocks_per_blkz !=\n\t\t\t\tSECTOR_TO_BLOCK(bdev_zone_sectors(bdev)))\n\t\treturn -EINVAL;\n\tsbi->blocks_per_blkz = SECTOR_TO_BLOCK(bdev_zone_sectors(bdev));\n\tif (sbi->log_blocks_per_blkz && sbi->log_blocks_per_blkz !=\n\t\t\t\t__ilog2_u32(sbi->blocks_per_blkz))\n\t\treturn -EINVAL;\n\tsbi->log_blocks_per_blkz = __ilog2_u32(sbi->blocks_per_blkz);\n\tFDEV(devi).nr_blkz = SECTOR_TO_BLOCK(nr_sectors) >>\n\t\t\t\t\tsbi->log_blocks_per_blkz;\n\tif (nr_sectors & (bdev_zone_sectors(bdev) - 1))\n\t\tFDEV(devi).nr_blkz++;\n\n\tFDEV(devi).blkz_type = kmalloc(FDEV(devi).nr_blkz, GFP_KERNEL);\n\tif (!FDEV(devi).blkz_type)\n\t\treturn -ENOMEM;\n\n#define F2FS_REPORT_NR_ZONES   4096\n\n\tzones = kcalloc(F2FS_REPORT_NR_ZONES, sizeof(struct blk_zone),\n\t\t\tGFP_KERNEL);\n\tif (!zones)\n\t\treturn -ENOMEM;\n\n\t/* Get block zones type */\n\twhile (zones && sector < nr_sectors) {\n\n\t\tnr_zones = F2FS_REPORT_NR_ZONES;\n\t\terr = blkdev_report_zones(bdev, sector,\n\t\t\t\t\t  zones, &nr_zones,\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!nr_zones) {\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < nr_zones; i++) {\n\t\t\tFDEV(devi).blkz_type[n] = zones[i].type;\n\t\t\tsector += zones[i].len;\n\t\t\tn++;\n\t\t}\n\t}\n\n\tkfree(zones);\n\n\treturn err;\n}\n#endif\n\n/*\n * Read f2fs raw super block.\n * Because we have two copies of super block, so read both of them\n * to get the first valid one. If any one of them is broken, we pass\n * them recovery flag back to the caller.\n */\nstatic int read_raw_super_block(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_super_block **raw_super,\n\t\t\tint *valid_super_block, int *recovery)\n{\n\tstruct super_block *sb = sbi->sb;\n\tint block;\n\tstruct buffer_head *bh;\n\tstruct f2fs_super_block *super;\n\tint err = 0;\n\n\tsuper = kzalloc(sizeof(struct f2fs_super_block), GFP_KERNEL);\n\tif (!super)\n\t\treturn -ENOMEM;\n\n\tfor (block = 0; block < 2; block++) {\n\t\tbh = sb_bread(sb, block);\n\t\tif (!bh) {\n\t\t\tf2fs_msg(sb, KERN_ERR, \"Unable to read %dth superblock\",\n\t\t\t\tblock + 1);\n\t\t\terr = -EIO;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* sanity checking of raw super */\n\t\tif (sanity_check_raw_super(sbi, bh)) {\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Can't find valid F2FS filesystem in %dth superblock\",\n\t\t\t\tblock + 1);\n\t\t\terr = -EINVAL;\n\t\t\tbrelse(bh);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!*raw_super) {\n\t\t\tmemcpy(super, bh->b_data + F2FS_SUPER_OFFSET,\n\t\t\t\t\t\t\tsizeof(*super));\n\t\t\t*valid_super_block = block;\n\t\t\t*raw_super = super;\n\t\t}\n\t\tbrelse(bh);\n\t}\n\n\t/* Fail to read any one of the superblocks*/\n\tif (err < 0)\n\t\t*recovery = 1;\n\n\t/* No valid superblock */\n\tif (!*raw_super)\n\t\tkfree(super);\n\telse\n\t\terr = 0;\n\n\treturn err;\n}\n\nint f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover)\n{\n\tstruct buffer_head *bh;\n\tint err;\n\n\tif ((recover && f2fs_readonly(sbi->sb)) ||\n\t\t\t\tbdev_read_only(sbi->sb->s_bdev)) {\n\t\tset_sbi_flag(sbi, SBI_NEED_SB_WRITE);\n\t\treturn -EROFS;\n\t}\n\n\t/* write back-up superblock first */\n\tbh = sb_getblk(sbi->sb, sbi->valid_super_block ? 0: 1);\n\tif (!bh)\n\t\treturn -EIO;\n\terr = __f2fs_commit_super(bh, F2FS_RAW_SUPER(sbi));\n\tbrelse(bh);\n\n\t/* if we are in recovery path, skip writing valid superblock */\n\tif (recover || err)\n\t\treturn err;\n\n\t/* write current valid superblock */\n\tbh = sb_getblk(sbi->sb, sbi->valid_super_block);\n\tif (!bh)\n\t\treturn -EIO;\n\terr = __f2fs_commit_super(bh, F2FS_RAW_SUPER(sbi));\n\tbrelse(bh);\n\treturn err;\n}\n\nstatic int f2fs_scan_devices(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tunsigned int max_devices = MAX_DEVICES;\n\tint i;\n\n\t/* Initialize single device information */\n\tif (!RDEV(0).path[0]) {\n\t\tif (!bdev_is_zoned(sbi->sb->s_bdev))\n\t\t\treturn 0;\n\t\tmax_devices = 1;\n\t}\n\n\t/*\n\t * Initialize multiple devices information, or single\n\t * zoned block device information.\n\t */\n\tsbi->devs = kcalloc(max_devices, sizeof(struct f2fs_dev_info),\n\t\t\t\tGFP_KERNEL);\n\tif (!sbi->devs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < max_devices; i++) {\n\n\t\tif (i > 0 && !RDEV(i).path[0])\n\t\t\tbreak;\n\n\t\tif (max_devices == 1) {\n\t\t\t/* Single zoned block device mount */\n\t\t\tFDEV(0).bdev =\n\t\t\t\tblkdev_get_by_dev(sbi->sb->s_bdev->bd_dev,\n\t\t\t\t\tsbi->sb->s_mode, sbi->sb->s_type);\n\t\t} else {\n\t\t\t/* Multi-device mount */\n\t\t\tmemcpy(FDEV(i).path, RDEV(i).path, MAX_PATH_LEN);\n\t\t\tFDEV(i).total_segments =\n\t\t\t\tle32_to_cpu(RDEV(i).total_segments);\n\t\t\tif (i == 0) {\n\t\t\t\tFDEV(i).start_blk = 0;\n\t\t\t\tFDEV(i).end_blk = FDEV(i).start_blk +\n\t\t\t\t    (FDEV(i).total_segments <<\n\t\t\t\t    sbi->log_blocks_per_seg) - 1 +\n\t\t\t\t    le32_to_cpu(raw_super->segment0_blkaddr);\n\t\t\t} else {\n\t\t\t\tFDEV(i).start_blk = FDEV(i - 1).end_blk + 1;\n\t\t\t\tFDEV(i).end_blk = FDEV(i).start_blk +\n\t\t\t\t\t(FDEV(i).total_segments <<\n\t\t\t\t\tsbi->log_blocks_per_seg) - 1;\n\t\t\t}\n\t\t\tFDEV(i).bdev = blkdev_get_by_path(FDEV(i).path,\n\t\t\t\t\tsbi->sb->s_mode, sbi->sb->s_type);\n\t\t}\n\t\tif (IS_ERR(FDEV(i).bdev))\n\t\t\treturn PTR_ERR(FDEV(i).bdev);\n\n\t\t/* to release errored devices */\n\t\tsbi->s_ndevs = i + 1;\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tif (bdev_zoned_model(FDEV(i).bdev) == BLK_ZONED_HM &&\n\t\t\t\t!f2fs_sb_mounted_blkzoned(sbi->sb)) {\n\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\"Zoned block device feature not enabled\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (bdev_zoned_model(FDEV(i).bdev) != BLK_ZONED_NONE) {\n\t\t\tif (init_blkz_info(sbi, i)) {\n\t\t\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\t\t\"Failed to initialize F2FS blkzone information\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (max_devices == 1)\n\t\t\t\tbreak;\n\t\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\t\"Mount Device [%2d]: %20s, %8u, %8x - %8x (zone: %s)\",\n\t\t\t\ti, FDEV(i).path,\n\t\t\t\tFDEV(i).total_segments,\n\t\t\t\tFDEV(i).start_blk, FDEV(i).end_blk,\n\t\t\t\tbdev_zoned_model(FDEV(i).bdev) == BLK_ZONED_HA ?\n\t\t\t\t\"Host-aware\" : \"Host-managed\");\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"Mount Device [%2d]: %20s, %8u, %8x - %8x\",\n\t\t\t\ti, FDEV(i).path,\n\t\t\t\tFDEV(i).total_segments,\n\t\t\t\tFDEV(i).start_blk, FDEV(i).end_blk);\n\t}\n\tf2fs_msg(sbi->sb, KERN_INFO,\n\t\t\t\"IO Block Size: %8d KB\", F2FS_IO_SIZE_KB(sbi));\n\treturn 0;\n}\n\nstatic int f2fs_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct f2fs_sb_info *sbi;\n\tstruct f2fs_super_block *raw_super;\n\tstruct inode *root;\n\tint err;\n\tbool retry = true, need_fsck = false;\n\tchar *options = NULL;\n\tint recovery, i, valid_super_block;\n\tstruct curseg_info *seg_i;\n\ntry_onemore:\n\terr = -EINVAL;\n\traw_super = NULL;\n\tvalid_super_block = -1;\n\trecovery = 0;\n\n\t/* allocate memory for f2fs-specific super block info */\n\tsbi = kzalloc(sizeof(struct f2fs_sb_info), GFP_KERNEL);\n\tif (!sbi)\n\t\treturn -ENOMEM;\n\n\tsbi->sb = sb;\n\n\t/* Load the checksum driver */\n\tsbi->s_chksum_driver = crypto_alloc_shash(\"crc32\", 0, 0);\n\tif (IS_ERR(sbi->s_chksum_driver)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Cannot load crc32 driver.\");\n\t\terr = PTR_ERR(sbi->s_chksum_driver);\n\t\tsbi->s_chksum_driver = NULL;\n\t\tgoto free_sbi;\n\t}\n\n\t/* set a block size */\n\tif (unlikely(!sb_set_blocksize(sb, F2FS_BLKSIZE))) {\n\t\tf2fs_msg(sb, KERN_ERR, \"unable to set blocksize\");\n\t\tgoto free_sbi;\n\t}\n\n\terr = read_raw_super_block(sbi, &raw_super, &valid_super_block,\n\t\t\t\t\t\t\t\t&recovery);\n\tif (err)\n\t\tgoto free_sbi;\n\n\tsb->s_fs_info = sbi;\n\tsbi->raw_super = raw_super;\n\n\t/* precompute checksum seed for metadata */\n\tif (f2fs_sb_has_inode_chksum(sb))\n\t\tsbi->s_chksum_seed = f2fs_chksum(sbi, ~0, raw_super->uuid,\n\t\t\t\t\t\tsizeof(raw_super->uuid));\n\n\t/*\n\t * The BLKZONED feature indicates that the drive was formatted with\n\t * zone alignment optimization. This is optional for host-aware\n\t * devices, but mandatory for host-managed zoned block devices.\n\t */\n#ifndef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_mounted_blkzoned(sb)) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t \"Zoned block device support is not enabled\\n\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto free_sb_buf;\n\t}\n#endif\n\tdefault_options(sbi);\n\t/* parse mount options */\n\toptions = kstrdup((const char *)data, GFP_KERNEL);\n\tif (data && !options) {\n\t\terr = -ENOMEM;\n\t\tgoto free_sb_buf;\n\t}\n\n\terr = parse_options(sb, options);\n\tif (err)\n\t\tgoto free_options;\n\n\tsbi->max_file_blocks = max_file_blocks();\n\tsb->s_maxbytes = sbi->max_file_blocks <<\n\t\t\t\tle32_to_cpu(raw_super->log_blocksize);\n\tsb->s_max_links = F2FS_LINK_MAX;\n\tget_random_bytes(&sbi->s_next_generation, sizeof(u32));\n\n#ifdef CONFIG_QUOTA\n\tsb->dq_op = &f2fs_quota_operations;\n\tsb->s_qcop = &f2fs_quotactl_ops;\n\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ;\n#endif\n\n\tsb->s_op = &f2fs_sops;\n\tsb->s_cop = &f2fs_cryptops;\n\tsb->s_xattr = f2fs_xattr_handlers;\n\tsb->s_export_op = &f2fs_export_ops;\n\tsb->s_magic = F2FS_SUPER_MAGIC;\n\tsb->s_time_gran = 1;\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? MS_POSIXACL : 0);\n\tmemcpy(&sb->s_uuid, raw_super->uuid, sizeof(raw_super->uuid));\n\n\t/* init f2fs-specific super block info */\n\tsbi->valid_super_block = valid_super_block;\n\tmutex_init(&sbi->gc_mutex);\n\tmutex_init(&sbi->cp_mutex);\n\tinit_rwsem(&sbi->node_write);\n\tinit_rwsem(&sbi->node_change);\n\n\t/* disallow all the data/node/meta page writes */\n\tset_sbi_flag(sbi, SBI_POR_DOING);\n\tspin_lock_init(&sbi->stat_lock);\n\n\t/* init iostat info */\n\tspin_lock_init(&sbi->iostat_lock);\n\tsbi->iostat_enable = false;\n\n\tfor (i = 0; i < NR_PAGE_TYPE; i++) {\n\t\tint n = (i == META) ? 1: NR_TEMP_TYPE;\n\t\tint j;\n\n\t\tsbi->write_io[i] = kmalloc(n * sizeof(struct f2fs_bio_info),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!sbi->write_io[i]) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_options;\n\t\t}\n\n\t\tfor (j = HOT; j < n; j++) {\n\t\t\tinit_rwsem(&sbi->write_io[i][j].io_rwsem);\n\t\t\tsbi->write_io[i][j].sbi = sbi;\n\t\t\tsbi->write_io[i][j].bio = NULL;\n\t\t\tspin_lock_init(&sbi->write_io[i][j].io_lock);\n\t\t\tINIT_LIST_HEAD(&sbi->write_io[i][j].io_list);\n\t\t}\n\t}\n\n\tinit_rwsem(&sbi->cp_rwsem);\n\tinit_waitqueue_head(&sbi->cp_wait);\n\tinit_sb_info(sbi);\n\n\terr = init_percpu_info(sbi);\n\tif (err)\n\t\tgoto free_options;\n\n\tif (F2FS_IO_SIZE(sbi) > 1) {\n\t\tsbi->write_io_dummy =\n\t\t\tmempool_create_page_pool(2 * (F2FS_IO_SIZE(sbi) - 1), 0);\n\t\tif (!sbi->write_io_dummy) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_options;\n\t\t}\n\t}\n\n\t/* get an inode for meta space */\n\tsbi->meta_inode = f2fs_iget(sb, F2FS_META_INO(sbi));\n\tif (IS_ERR(sbi->meta_inode)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read F2FS meta data inode\");\n\t\terr = PTR_ERR(sbi->meta_inode);\n\t\tgoto free_io_dummy;\n\t}\n\n\terr = get_valid_checkpoint(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to get valid F2FS checkpoint\");\n\t\tgoto free_meta_inode;\n\t}\n\n\t/* Initialize device list */\n\terr = f2fs_scan_devices(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to find devices\");\n\t\tgoto free_devices;\n\t}\n\n\tsbi->total_valid_node_count =\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_node_count);\n\tpercpu_counter_set(&sbi->total_valid_inode_count,\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_inode_count));\n\tsbi->user_block_count = le64_to_cpu(sbi->ckpt->user_block_count);\n\tsbi->total_valid_block_count =\n\t\t\t\tle64_to_cpu(sbi->ckpt->valid_block_count);\n\tsbi->last_valid_block_count = sbi->total_valid_block_count;\n\tsbi->reserved_blocks = 0;\n\n\tfor (i = 0; i < NR_INODE_TYPE; i++) {\n\t\tINIT_LIST_HEAD(&sbi->inode_list[i]);\n\t\tspin_lock_init(&sbi->inode_lock[i]);\n\t}\n\n\tinit_extent_cache_info(sbi);\n\n\tinit_ino_entry_info(sbi);\n\n\t/* setup f2fs internal modules */\n\terr = build_segment_manager(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Failed to initialize F2FS segment manager\");\n\t\tgoto free_sm;\n\t}\n\terr = build_node_manager(sbi);\n\tif (err) {\n\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\"Failed to initialize F2FS node manager\");\n\t\tgoto free_nm;\n\t}\n\n\t/* For write statistics */\n\tif (sb->s_bdev->bd_part)\n\t\tsbi->sectors_written_start =\n\t\t\t(u64)part_stat_read(sb->s_bdev->bd_part, sectors[1]);\n\n\t/* Read accumulated write IO statistics if exists */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_NODE);\n\tif (__exist_node_summaries(sbi))\n\t\tsbi->kbytes_written =\n\t\t\tle64_to_cpu(seg_i->journal->info.kbytes_written);\n\n\tbuild_gc_manager(sbi);\n\n\t/* get an inode for node space */\n\tsbi->node_inode = f2fs_iget(sb, F2FS_NODE_INO(sbi));\n\tif (IS_ERR(sbi->node_inode)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read node inode\");\n\t\terr = PTR_ERR(sbi->node_inode);\n\t\tgoto free_nm;\n\t}\n\n\tf2fs_join_shrinker(sbi);\n\n\terr = f2fs_build_stats(sbi);\n\tif (err)\n\t\tgoto free_nm;\n\n\t/* read root inode and dentry */\n\troot = f2fs_iget(sb, F2FS_ROOT_INO(sbi));\n\tif (IS_ERR(root)) {\n\t\tf2fs_msg(sb, KERN_ERR, \"Failed to read root inode\");\n\t\terr = PTR_ERR(root);\n\t\tgoto free_node_inode;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) {\n\t\tiput(root);\n\t\terr = -EINVAL;\n\t\tgoto free_node_inode;\n\t}\n\n\tsb->s_root = d_make_root(root); /* allocate root dentry */\n\tif (!sb->s_root) {\n\t\terr = -ENOMEM;\n\t\tgoto free_root_inode;\n\t}\n\n\terr = f2fs_register_sysfs(sbi);\n\tif (err)\n\t\tgoto free_root_inode;\n\n\t/* if there are nt orphan nodes free them */\n\terr = recover_orphan_inodes(sbi);\n\tif (err)\n\t\tgoto free_sysfs;\n\n\t/* recover fsynced data */\n\tif (!test_opt(sbi, DISABLE_ROLL_FORWARD)) {\n\t\t/*\n\t\t * mount should be failed, when device has readonly mode, and\n\t\t * previous checkpoint was not done by clean system shutdown.\n\t\t */\n\t\tif (bdev_read_only(sb->s_bdev) &&\n\t\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\t\terr = -EROFS;\n\t\t\tgoto free_meta;\n\t\t}\n\n\t\tif (need_fsck)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\t\tif (!retry)\n\t\t\tgoto skip_recovery;\n\n\t\terr = recover_fsync_data(sbi, false);\n\t\tif (err < 0) {\n\t\t\tneed_fsck = true;\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Cannot recover all fsync data errno=%d\", err);\n\t\t\tgoto free_meta;\n\t\t}\n\t} else {\n\t\terr = recover_fsync_data(sbi, true);\n\n\t\tif (!f2fs_readonly(sb) && err > 0) {\n\t\t\terr = -EINVAL;\n\t\t\tf2fs_msg(sb, KERN_ERR,\n\t\t\t\t\"Need to recover fsync data\");\n\t\t\tgoto free_sysfs;\n\t\t}\n\t}\nskip_recovery:\n\t/* recover_fsync_data() cleared this already */\n\tclear_sbi_flag(sbi, SBI_POR_DOING);\n\n\t/*\n\t * If filesystem is not mounted as read-only then\n\t * do start the gc_thread.\n\t */\n\tif (test_opt(sbi, BG_GC) && !f2fs_readonly(sb)) {\n\t\t/* After POR, we can run background GC thread.*/\n\t\terr = start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto free_meta;\n\t}\n\tkfree(options);\n\n\t/* recover broken superblock */\n\tif (recovery) {\n\t\terr = f2fs_commit_super(sbi, true);\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Try to recover %dth superblock, ret: %d\",\n\t\t\tsbi->valid_super_block ? 1 : 2, err);\n\t}\n\n\tf2fs_msg(sbi->sb, KERN_NOTICE, \"Mounted with checkpoint version = %llx\",\n\t\t\t\tcur_cp_version(F2FS_CKPT(sbi)));\n\tf2fs_update_time(sbi, CP_TIME);\n\tf2fs_update_time(sbi, REQ_TIME);\n\treturn 0;\n\nfree_meta:\n\tf2fs_sync_inode_meta(sbi);\n\t/*\n\t * Some dirty meta pages can be produced by recover_orphan_inodes()\n\t * failed by EIO. Then, iput(node_inode) can trigger balance_fs_bg()\n\t * followed by write_checkpoint() through f2fs_write_node_pages(), which\n\t * falls into an infinite loop in sync_meta_pages().\n\t */\n\ttruncate_inode_pages_final(META_MAPPING(sbi));\nfree_sysfs:\n\tf2fs_unregister_sysfs(sbi);\nfree_root_inode:\n\tdput(sb->s_root);\n\tsb->s_root = NULL;\nfree_node_inode:\n\ttruncate_inode_pages_final(NODE_MAPPING(sbi));\n\tmutex_lock(&sbi->umount_mutex);\n\trelease_ino_entry(sbi, true);\n\tf2fs_leave_shrinker(sbi);\n\tiput(sbi->node_inode);\n\tmutex_unlock(&sbi->umount_mutex);\n\tf2fs_destroy_stats(sbi);\nfree_nm:\n\tdestroy_node_manager(sbi);\nfree_sm:\n\tdestroy_segment_manager(sbi);\nfree_devices:\n\tdestroy_device_list(sbi);\n\tkfree(sbi->ckpt);\nfree_meta_inode:\n\tmake_bad_inode(sbi->meta_inode);\n\tiput(sbi->meta_inode);\nfree_io_dummy:\n\tmempool_destroy(sbi->write_io_dummy);\nfree_options:\n\tfor (i = 0; i < NR_PAGE_TYPE; i++)\n\t\tkfree(sbi->write_io[i]);\n\tdestroy_percpu_info(sbi);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\tkfree(options);\nfree_sb_buf:\n\tkfree(raw_super);\nfree_sbi:\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi);\n\n\t/* give only one another chance */\n\tif (retry) {\n\t\tretry = false;\n\t\tshrink_dcache_sb(sb);\n\t\tgoto try_onemore;\n\t}\n\treturn err;\n}\n\nstatic struct dentry *f2fs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *data)\n{\n\treturn mount_bdev(fs_type, flags, dev_name, data, f2fs_fill_super);\n}\n\nstatic void kill_f2fs_super(struct super_block *sb)\n{\n\tif (sb->s_root) {\n\t\tset_sbi_flag(F2FS_SB(sb), SBI_IS_CLOSE);\n\t\tstop_gc_thread(F2FS_SB(sb));\n\t\tstop_discard_thread(F2FS_SB(sb));\n\t}\n\tkill_block_super(sb);\n}\n\nstatic struct file_system_type f2fs_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"f2fs\",\n\t.mount\t\t= f2fs_mount,\n\t.kill_sb\t= kill_f2fs_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\nMODULE_ALIAS_FS(\"f2fs\");\n\nstatic int __init init_inodecache(void)\n{\n\tf2fs_inode_cachep = kmem_cache_create(\"f2fs_inode_cache\",\n\t\t\tsizeof(struct f2fs_inode_info), 0,\n\t\t\tSLAB_RECLAIM_ACCOUNT|SLAB_ACCOUNT, NULL);\n\tif (!f2fs_inode_cachep)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void destroy_inodecache(void)\n{\n\t/*\n\t * Make sure all delayed rcu free inodes are flushed before we\n\t * destroy cache.\n\t */\n\trcu_barrier();\n\tkmem_cache_destroy(f2fs_inode_cachep);\n}\n\nstatic int __init init_f2fs_fs(void)\n{\n\tint err;\n\n\tf2fs_build_trace_ios();\n\n\terr = init_inodecache();\n\tif (err)\n\t\tgoto fail;\n\terr = create_node_manager_caches();\n\tif (err)\n\t\tgoto free_inodecache;\n\terr = create_segment_manager_caches();\n\tif (err)\n\t\tgoto free_node_manager_caches;\n\terr = create_checkpoint_caches();\n\tif (err)\n\t\tgoto free_segment_manager_caches;\n\terr = create_extent_cache();\n\tif (err)\n\t\tgoto free_checkpoint_caches;\n\terr = f2fs_init_sysfs();\n\tif (err)\n\t\tgoto free_extent_cache;\n\terr = register_shrinker(&f2fs_shrinker_info);\n\tif (err)\n\t\tgoto free_sysfs;\n\terr = register_filesystem(&f2fs_fs_type);\n\tif (err)\n\t\tgoto free_shrinker;\n\terr = f2fs_create_root_stats();\n\tif (err)\n\t\tgoto free_filesystem;\n\treturn 0;\n\nfree_filesystem:\n\tunregister_filesystem(&f2fs_fs_type);\nfree_shrinker:\n\tunregister_shrinker(&f2fs_shrinker_info);\nfree_sysfs:\n\tf2fs_exit_sysfs();\nfree_extent_cache:\n\tdestroy_extent_cache();\nfree_checkpoint_caches:\n\tdestroy_checkpoint_caches();\nfree_segment_manager_caches:\n\tdestroy_segment_manager_caches();\nfree_node_manager_caches:\n\tdestroy_node_manager_caches();\nfree_inodecache:\n\tdestroy_inodecache();\nfail:\n\treturn err;\n}\n\nstatic void __exit exit_f2fs_fs(void)\n{\n\tf2fs_destroy_root_stats();\n\tunregister_filesystem(&f2fs_fs_type);\n\tunregister_shrinker(&f2fs_shrinker_info);\n\tf2fs_exit_sysfs();\n\tdestroy_extent_cache();\n\tdestroy_checkpoint_caches();\n\tdestroy_segment_manager_caches();\n\tdestroy_node_manager_caches();\n\tdestroy_inodecache();\n\tf2fs_destroy_trace_ios();\n}\n\nmodule_init(init_f2fs_fs)\nmodule_exit(exit_f2fs_fs)\n\nMODULE_AUTHOR(\"Samsung Electronics's Praesto Team\");\nMODULE_DESCRIPTION(\"Flash Friendly File System\");\nMODULE_LICENSE(\"GPL\");\n\n"], "filenames": ["fs/f2fs/f2fs.h", "fs/f2fs/segment.c", "fs/f2fs/super.c"], "buggy_code_start_loc": [2528, 1213, 804], "buggy_code_end_loc": [2529, 2248, 805], "fixing_code_start_loc": [2528, 1213, 804], "fixing_code_end_loc": [2529, 2248, 805], "type": "CWE-20", "message": "The f2fs implementation in the Linux kernel before 4.14 mishandles reference counts associated with f2fs_wait_discard_bios calls, which allows local users to cause a denial of service (BUG), as demonstrated by fstrim.", "other": {"cve": {"id": "CVE-2017-18200", "sourceIdentifier": "cve@mitre.org", "published": "2018-02-26T03:29:00.227", "lastModified": "2018-03-16T17:24:15.580", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The f2fs implementation in the Linux kernel before 4.14 mishandles reference counts associated with f2fs_wait_discard_bios calls, which allows local users to cause a denial of service (BUG), as demonstrated by fstrim."}, {"lang": "es", "value": "La implementaci\u00f3n f2fs en el kernel de Linux, en versiones anteriores a la 4.14, gestiona err\u00f3neamente las cuentas asociadas a las llamadas f2fs_wait_discard_bios. Esto permite que usuarios locales provoquen una denegaci\u00f3n de servicio (bug), tal y como demuestra fstrim."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.13", "matchCriteriaId": "F74D9BEC-6597-40F8-9E56-253FAA432D85"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=638164a2718f337ea224b747cf5977ef143166a4", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/638164a2718f337ea224b747cf5977ef143166a4", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/638164a2718f337ea224b747cf5977ef143166a4"}}