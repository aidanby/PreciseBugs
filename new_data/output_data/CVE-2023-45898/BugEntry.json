{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n *  fs/ext4/extents_status.c\n *\n * Written by Yongqiang Yang <xiaoqiangnk@gmail.com>\n * Modified by\n *\tAllison Henderson <achender@linux.vnet.ibm.com>\n *\tHugh Dickins <hughd@google.com>\n *\tZheng Liu <wenqing.lz@taobao.com>\n *\n * Ext4 extents status tree core functions.\n */\n#include <linux/list_sort.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include \"ext4.h\"\n\n#include <trace/events/ext4.h>\n\n/*\n * According to previous discussion in Ext4 Developer Workshop, we\n * will introduce a new structure called io tree to track all extent\n * status in order to solve some problems that we have met\n * (e.g. Reservation space warning), and provide extent-level locking.\n * Delay extent tree is the first step to achieve this goal.  It is\n * original built by Yongqiang Yang.  At that time it is called delay\n * extent tree, whose goal is only track delayed extents in memory to\n * simplify the implementation of fiemap and bigalloc, and introduce\n * lseek SEEK_DATA/SEEK_HOLE support.  That is why it is still called\n * delay extent tree at the first commit.  But for better understand\n * what it does, it has been rename to extent status tree.\n *\n * Step1:\n * Currently the first step has been done.  All delayed extents are\n * tracked in the tree.  It maintains the delayed extent when a delayed\n * allocation is issued, and the delayed extent is written out or\n * invalidated.  Therefore the implementation of fiemap and bigalloc\n * are simplified, and SEEK_DATA/SEEK_HOLE are introduced.\n *\n * The following comment describes the implemenmtation of extent\n * status tree and future works.\n *\n * Step2:\n * In this step all extent status are tracked by extent status tree.\n * Thus, we can first try to lookup a block mapping in this tree before\n * finding it in extent tree.  Hence, single extent cache can be removed\n * because extent status tree can do a better job.  Extents in status\n * tree are loaded on-demand.  Therefore, the extent status tree may not\n * contain all of the extents in a file.  Meanwhile we define a shrinker\n * to reclaim memory from extent status tree because fragmented extent\n * tree will make status tree cost too much memory.  written/unwritten/-\n * hole extents in the tree will be reclaimed by this shrinker when we\n * are under high memory pressure.  Delayed extents will not be\n * reclimed because fiemap, bigalloc, and seek_data/hole need it.\n */\n\n/*\n * Extent status tree implementation for ext4.\n *\n *\n * ==========================================================================\n * Extent status tree tracks all extent status.\n *\n * 1. Why we need to implement extent status tree?\n *\n * Without extent status tree, ext4 identifies a delayed extent by looking\n * up page cache, this has several deficiencies - complicated, buggy,\n * and inefficient code.\n *\n * FIEMAP, SEEK_HOLE/DATA, bigalloc, and writeout all need to know if a\n * block or a range of blocks are belonged to a delayed extent.\n *\n * Let us have a look at how they do without extent status tree.\n *   --\tFIEMAP\n *\tFIEMAP looks up page cache to identify delayed allocations from holes.\n *\n *   --\tSEEK_HOLE/DATA\n *\tSEEK_HOLE/DATA has the same problem as FIEMAP.\n *\n *   --\tbigalloc\n *\tbigalloc looks up page cache to figure out if a block is\n *\talready under delayed allocation or not to determine whether\n *\tquota reserving is needed for the cluster.\n *\n *   --\twriteout\n *\tWriteout looks up whole page cache to see if a buffer is\n *\tmapped, If there are not very many delayed buffers, then it is\n *\ttime consuming.\n *\n * With extent status tree implementation, FIEMAP, SEEK_HOLE/DATA,\n * bigalloc and writeout can figure out if a block or a range of\n * blocks is under delayed allocation(belonged to a delayed extent) or\n * not by searching the extent tree.\n *\n *\n * ==========================================================================\n * 2. Ext4 extent status tree impelmentation\n *\n *   --\textent\n *\tA extent is a range of blocks which are contiguous logically and\n *\tphysically.  Unlike extent in extent tree, this extent in ext4 is\n *\ta in-memory struct, there is no corresponding on-disk data.  There\n *\tis no limit on length of extent, so an extent can contain as many\n *\tblocks as they are contiguous logically and physically.\n *\n *   --\textent status tree\n *\tEvery inode has an extent status tree and all allocation blocks\n *\tare added to the tree with different status.  The extent in the\n *\ttree are ordered by logical block no.\n *\n *   --\toperations on a extent status tree\n *\tThere are three important operations on a delayed extent tree: find\n *\tnext extent, adding a extent(a range of blocks) and removing a extent.\n *\n *   --\trace on a extent status tree\n *\tExtent status tree is protected by inode->i_es_lock.\n *\n *   --\tmemory consumption\n *      Fragmented extent tree will make extent status tree cost too much\n *      memory.  Hence, we will reclaim written/unwritten/hole extents from\n *      the tree under a heavy memory pressure.\n *\n *\n * ==========================================================================\n * 3. Performance analysis\n *\n *   --\toverhead\n *\t1. There is a cache extent for write access, so if writes are\n *\tnot very random, adding space operaions are in O(1) time.\n *\n *   --\tgain\n *\t2. Code is much simpler, more readable, more maintainable and\n *\tmore efficient.\n *\n *\n * ==========================================================================\n * 4. TODO list\n *\n *   -- Refactor delayed space reservation\n *\n *   -- Extent-level locking\n */\n\nstatic struct kmem_cache *ext4_es_cachep;\nstatic struct kmem_cache *ext4_pending_cachep;\n\nstatic int __es_insert_extent(struct inode *inode, struct extent_status *newes,\n\t\t\t      struct extent_status *prealloc);\nstatic int __es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t      ext4_lblk_t end, int *reserved,\n\t\t\t      struct extent_status *prealloc);\nstatic int es_reclaim_extents(struct ext4_inode_info *ei, int *nr_to_scan);\nstatic int __es_shrink(struct ext4_sb_info *sbi, int nr_to_scan,\n\t\t       struct ext4_inode_info *locked_ei);\nstatic void __revise_pending(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t     ext4_lblk_t len);\n\nint __init ext4_init_es(void)\n{\n\text4_es_cachep = KMEM_CACHE(extent_status, SLAB_RECLAIM_ACCOUNT);\n\tif (ext4_es_cachep == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid ext4_exit_es(void)\n{\n\tkmem_cache_destroy(ext4_es_cachep);\n}\n\nvoid ext4_es_init_tree(struct ext4_es_tree *tree)\n{\n\ttree->root = RB_ROOT;\n\ttree->cache_es = NULL;\n}\n\n#ifdef ES_DEBUG__\nstatic void ext4_es_print_tree(struct inode *inode)\n{\n\tstruct ext4_es_tree *tree;\n\tstruct rb_node *node;\n\n\tprintk(KERN_DEBUG \"status extents for inode %lu:\", inode->i_ino);\n\ttree = &EXT4_I(inode)->i_es_tree;\n\tnode = rb_first(&tree->root);\n\twhile (node) {\n\t\tstruct extent_status *es;\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\tprintk(KERN_DEBUG \" [%u/%u) %llu %x\",\n\t\t       es->es_lblk, es->es_len,\n\t\t       ext4_es_pblock(es), ext4_es_status(es));\n\t\tnode = rb_next(node);\n\t}\n\tprintk(KERN_DEBUG \"\\n\");\n}\n#else\n#define ext4_es_print_tree(inode)\n#endif\n\nstatic inline ext4_lblk_t ext4_es_end(struct extent_status *es)\n{\n\tBUG_ON(es->es_lblk + es->es_len < es->es_lblk);\n\treturn es->es_lblk + es->es_len - 1;\n}\n\n/*\n * search through the tree for an delayed extent with a given offset.  If\n * it can't be found, try to find next extent.\n */\nstatic struct extent_status *__es_tree_search(struct rb_root *root,\n\t\t\t\t\t      ext4_lblk_t lblk)\n{\n\tstruct rb_node *node = root->rb_node;\n\tstruct extent_status *es = NULL;\n\n\twhile (node) {\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\tif (lblk < es->es_lblk)\n\t\t\tnode = node->rb_left;\n\t\telse if (lblk > ext4_es_end(es))\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn es;\n\t}\n\n\tif (es && lblk < es->es_lblk)\n\t\treturn es;\n\n\tif (es && lblk > ext4_es_end(es)) {\n\t\tnode = rb_next(&es->rb_node);\n\t\treturn node ? rb_entry(node, struct extent_status, rb_node) :\n\t\t\t      NULL;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * ext4_es_find_extent_range - find extent with specified status within block\n *                             range or next extent following block range in\n *                             extents status tree\n *\n * @inode - file containing the range\n * @matching_fn - pointer to function that matches extents with desired status\n * @lblk - logical block defining start of range\n * @end - logical block defining end of range\n * @es - extent found, if any\n *\n * Find the first extent within the block range specified by @lblk and @end\n * in the extents status tree that satisfies @matching_fn.  If a match\n * is found, it's returned in @es.  If not, and a matching extent is found\n * beyond the block range, it's returned in @es.  If no match is found, an\n * extent is returned in @es whose es_lblk, es_len, and es_pblk components\n * are 0.\n */\nstatic void __es_find_extent_range(struct inode *inode,\n\t\t\t\t   int (*matching_fn)(struct extent_status *es),\n\t\t\t\t   ext4_lblk_t lblk, ext4_lblk_t end,\n\t\t\t\t   struct extent_status *es)\n{\n\tstruct ext4_es_tree *tree = NULL;\n\tstruct extent_status *es1 = NULL;\n\tstruct rb_node *node;\n\n\tWARN_ON(es == NULL);\n\tWARN_ON(end < lblk);\n\n\ttree = &EXT4_I(inode)->i_es_tree;\n\n\t/* see if the extent has been cached */\n\tes->es_lblk = es->es_len = es->es_pblk = 0;\n\tes1 = READ_ONCE(tree->cache_es);\n\tif (es1 && in_range(lblk, es1->es_lblk, es1->es_len)) {\n\t\tes_debug(\"%u cached by [%u/%u) %llu %x\\n\",\n\t\t\t lblk, es1->es_lblk, es1->es_len,\n\t\t\t ext4_es_pblock(es1), ext4_es_status(es1));\n\t\tgoto out;\n\t}\n\n\tes1 = __es_tree_search(&tree->root, lblk);\n\nout:\n\tif (es1 && !matching_fn(es1)) {\n\t\twhile ((node = rb_next(&es1->rb_node)) != NULL) {\n\t\t\tes1 = rb_entry(node, struct extent_status, rb_node);\n\t\t\tif (es1->es_lblk > end) {\n\t\t\t\tes1 = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (matching_fn(es1))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (es1 && matching_fn(es1)) {\n\t\tWRITE_ONCE(tree->cache_es, es1);\n\t\tes->es_lblk = es1->es_lblk;\n\t\tes->es_len = es1->es_len;\n\t\tes->es_pblk = es1->es_pblk;\n\t}\n\n}\n\n/*\n * Locking for __es_find_extent_range() for external use\n */\nvoid ext4_es_find_extent_range(struct inode *inode,\n\t\t\t       int (*matching_fn)(struct extent_status *es),\n\t\t\t       ext4_lblk_t lblk, ext4_lblk_t end,\n\t\t\t       struct extent_status *es)\n{\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_find_extent_range_enter(inode, lblk);\n\n\tread_lock(&EXT4_I(inode)->i_es_lock);\n\t__es_find_extent_range(inode, matching_fn, lblk, end, es);\n\tread_unlock(&EXT4_I(inode)->i_es_lock);\n\n\ttrace_ext4_es_find_extent_range_exit(inode, es);\n}\n\n/*\n * __es_scan_range - search block range for block with specified status\n *                   in extents status tree\n *\n * @inode - file containing the range\n * @matching_fn - pointer to function that matches extents with desired status\n * @lblk - logical block defining start of range\n * @end - logical block defining end of range\n *\n * Returns true if at least one block in the specified block range satisfies\n * the criterion specified by @matching_fn, and false if not.  If at least\n * one extent has the specified status, then there is at least one block\n * in the cluster with that status.  Should only be called by code that has\n * taken i_es_lock.\n */\nstatic bool __es_scan_range(struct inode *inode,\n\t\t\t    int (*matching_fn)(struct extent_status *es),\n\t\t\t    ext4_lblk_t start, ext4_lblk_t end)\n{\n\tstruct extent_status es;\n\n\t__es_find_extent_range(inode, matching_fn, start, end, &es);\n\tif (es.es_len == 0)\n\t\treturn false;   /* no matching extent in the tree */\n\telse if (es.es_lblk <= start &&\n\t\t start < es.es_lblk + es.es_len)\n\t\treturn true;\n\telse if (start <= es.es_lblk && es.es_lblk <= end)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n/*\n * Locking for __es_scan_range() for external use\n */\nbool ext4_es_scan_range(struct inode *inode,\n\t\t\tint (*matching_fn)(struct extent_status *es),\n\t\t\text4_lblk_t lblk, ext4_lblk_t end)\n{\n\tbool ret;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn false;\n\n\tread_lock(&EXT4_I(inode)->i_es_lock);\n\tret = __es_scan_range(inode, matching_fn, lblk, end);\n\tread_unlock(&EXT4_I(inode)->i_es_lock);\n\n\treturn ret;\n}\n\n/*\n * __es_scan_clu - search cluster for block with specified status in\n *                 extents status tree\n *\n * @inode - file containing the cluster\n * @matching_fn - pointer to function that matches extents with desired status\n * @lblk - logical block in cluster to be searched\n *\n * Returns true if at least one extent in the cluster containing @lblk\n * satisfies the criterion specified by @matching_fn, and false if not.  If at\n * least one extent has the specified status, then there is at least one block\n * in the cluster with that status.  Should only be called by code that has\n * taken i_es_lock.\n */\nstatic bool __es_scan_clu(struct inode *inode,\n\t\t\t  int (*matching_fn)(struct extent_status *es),\n\t\t\t  ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t lblk_start, lblk_end;\n\n\tlblk_start = EXT4_LBLK_CMASK(sbi, lblk);\n\tlblk_end = lblk_start + sbi->s_cluster_ratio - 1;\n\n\treturn __es_scan_range(inode, matching_fn, lblk_start, lblk_end);\n}\n\n/*\n * Locking for __es_scan_clu() for external use\n */\nbool ext4_es_scan_clu(struct inode *inode,\n\t\t      int (*matching_fn)(struct extent_status *es),\n\t\t      ext4_lblk_t lblk)\n{\n\tbool ret;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn false;\n\n\tread_lock(&EXT4_I(inode)->i_es_lock);\n\tret = __es_scan_clu(inode, matching_fn, lblk);\n\tread_unlock(&EXT4_I(inode)->i_es_lock);\n\n\treturn ret;\n}\n\nstatic void ext4_es_list_add(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\tif (!list_empty(&ei->i_es_list))\n\t\treturn;\n\n\tspin_lock(&sbi->s_es_lock);\n\tif (list_empty(&ei->i_es_list)) {\n\t\tlist_add_tail(&ei->i_es_list, &sbi->s_es_list);\n\t\tsbi->s_es_nr_inode++;\n\t}\n\tspin_unlock(&sbi->s_es_lock);\n}\n\nstatic void ext4_es_list_del(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\tspin_lock(&sbi->s_es_lock);\n\tif (!list_empty(&ei->i_es_list)) {\n\t\tlist_del_init(&ei->i_es_list);\n\t\tsbi->s_es_nr_inode--;\n\t\tWARN_ON_ONCE(sbi->s_es_nr_inode < 0);\n\t}\n\tspin_unlock(&sbi->s_es_lock);\n}\n\n/*\n * Returns true if we cannot fail to allocate memory for this extent_status\n * entry and cannot reclaim it until its status changes.\n */\nstatic inline bool ext4_es_must_keep(struct extent_status *es)\n{\n\t/* fiemap, bigalloc, and seek_data/hole need to use it. */\n\tif (ext4_es_is_delayed(es))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline struct extent_status *__es_alloc_extent(bool nofail)\n{\n\tif (!nofail)\n\t\treturn kmem_cache_alloc(ext4_es_cachep, GFP_ATOMIC);\n\n\treturn kmem_cache_zalloc(ext4_es_cachep, GFP_KERNEL | __GFP_NOFAIL);\n}\n\nstatic void ext4_es_init_extent(struct inode *inode, struct extent_status *es,\n\t\text4_lblk_t lblk, ext4_lblk_t len, ext4_fsblk_t pblk)\n{\n\tes->es_lblk = lblk;\n\tes->es_len = len;\n\tes->es_pblk = pblk;\n\n\t/* We never try to reclaim a must kept extent, so we don't count it. */\n\tif (!ext4_es_must_keep(es)) {\n\t\tif (!EXT4_I(inode)->i_es_shk_nr++)\n\t\t\text4_es_list_add(inode);\n\t\tpercpu_counter_inc(&EXT4_SB(inode->i_sb)->\n\t\t\t\t\ts_es_stats.es_stats_shk_cnt);\n\t}\n\n\tEXT4_I(inode)->i_es_all_nr++;\n\tpercpu_counter_inc(&EXT4_SB(inode->i_sb)->s_es_stats.es_stats_all_cnt);\n}\n\nstatic inline void __es_free_extent(struct extent_status *es)\n{\n\tkmem_cache_free(ext4_es_cachep, es);\n}\n\nstatic void ext4_es_free_extent(struct inode *inode, struct extent_status *es)\n{\n\tEXT4_I(inode)->i_es_all_nr--;\n\tpercpu_counter_dec(&EXT4_SB(inode->i_sb)->s_es_stats.es_stats_all_cnt);\n\n\t/* Decrease the shrink counter when we can reclaim the extent. */\n\tif (!ext4_es_must_keep(es)) {\n\t\tBUG_ON(EXT4_I(inode)->i_es_shk_nr == 0);\n\t\tif (!--EXT4_I(inode)->i_es_shk_nr)\n\t\t\text4_es_list_del(inode);\n\t\tpercpu_counter_dec(&EXT4_SB(inode->i_sb)->\n\t\t\t\t\ts_es_stats.es_stats_shk_cnt);\n\t}\n\n\t__es_free_extent(es);\n}\n\n/*\n * Check whether or not two extents can be merged\n * Condition:\n *  - logical block number is contiguous\n *  - physical block number is contiguous\n *  - status is equal\n */\nstatic int ext4_es_can_be_merged(struct extent_status *es1,\n\t\t\t\t struct extent_status *es2)\n{\n\tif (ext4_es_type(es1) != ext4_es_type(es2))\n\t\treturn 0;\n\n\tif (((__u64) es1->es_len) + es2->es_len > EXT_MAX_BLOCKS) {\n\t\tpr_warn(\"ES assertion failed when merging extents. \"\n\t\t\t\"The sum of lengths of es1 (%d) and es2 (%d) \"\n\t\t\t\"is bigger than allowed file size (%d)\\n\",\n\t\t\tes1->es_len, es2->es_len, EXT_MAX_BLOCKS);\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\n\tif (((__u64) es1->es_lblk) + es1->es_len != es2->es_lblk)\n\t\treturn 0;\n\n\tif ((ext4_es_is_written(es1) || ext4_es_is_unwritten(es1)) &&\n\t    (ext4_es_pblock(es1) + es1->es_len == ext4_es_pblock(es2)))\n\t\treturn 1;\n\n\tif (ext4_es_is_hole(es1))\n\t\treturn 1;\n\n\t/* we need to check delayed extent is without unwritten status */\n\tif (ext4_es_is_delayed(es1) && !ext4_es_is_unwritten(es1))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic struct extent_status *\next4_es_try_to_merge_left(struct inode *inode, struct extent_status *es)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct extent_status *es1;\n\tstruct rb_node *node;\n\n\tnode = rb_prev(&es->rb_node);\n\tif (!node)\n\t\treturn es;\n\n\tes1 = rb_entry(node, struct extent_status, rb_node);\n\tif (ext4_es_can_be_merged(es1, es)) {\n\t\tes1->es_len += es->es_len;\n\t\tif (ext4_es_is_referenced(es))\n\t\t\text4_es_set_referenced(es1);\n\t\trb_erase(&es->rb_node, &tree->root);\n\t\text4_es_free_extent(inode, es);\n\t\tes = es1;\n\t}\n\n\treturn es;\n}\n\nstatic struct extent_status *\next4_es_try_to_merge_right(struct inode *inode, struct extent_status *es)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct extent_status *es1;\n\tstruct rb_node *node;\n\n\tnode = rb_next(&es->rb_node);\n\tif (!node)\n\t\treturn es;\n\n\tes1 = rb_entry(node, struct extent_status, rb_node);\n\tif (ext4_es_can_be_merged(es, es1)) {\n\t\tes->es_len += es1->es_len;\n\t\tif (ext4_es_is_referenced(es1))\n\t\t\text4_es_set_referenced(es);\n\t\trb_erase(node, &tree->root);\n\t\text4_es_free_extent(inode, es1);\n\t}\n\n\treturn es;\n}\n\n#ifdef ES_AGGRESSIVE_TEST\n#include \"ext4_extents.h\"\t/* Needed when ES_AGGRESSIVE_TEST is defined */\n\nstatic void ext4_es_insert_extent_ext_check(struct inode *inode,\n\t\t\t\t\t    struct extent_status *es)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\text4_fsblk_t ee_start;\n\tunsigned short ee_len;\n\tint depth, ee_status, es_status;\n\n\tpath = ext4_find_extent(inode, es->es_lblk, NULL, EXT4_EX_NOCACHE);\n\tif (IS_ERR(path))\n\t\treturn;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\n\tif (ex) {\n\n\t\tee_block = le32_to_cpu(ex->ee_block);\n\t\tee_start = ext4_ext_pblock(ex);\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\n\t\tee_status = ext4_ext_is_unwritten(ex) ? 1 : 0;\n\t\tes_status = ext4_es_is_unwritten(es) ? 1 : 0;\n\n\t\t/*\n\t\t * Make sure ex and es are not overlap when we try to insert\n\t\t * a delayed/hole extent.\n\t\t */\n\t\tif (!ext4_es_is_written(es) && !ext4_es_is_unwritten(es)) {\n\t\t\tif (in_range(es->es_lblk, ee_block, ee_len)) {\n\t\t\t\tpr_warn(\"ES insert assertion failed for \"\n\t\t\t\t\t\"inode: %lu we can find an extent \"\n\t\t\t\t\t\"at block [%d/%d/%llu/%c], but we \"\n\t\t\t\t\t\"want to add a delayed/hole extent \"\n\t\t\t\t\t\"[%d/%d/%llu/%x]\\n\",\n\t\t\t\t\tinode->i_ino, ee_block, ee_len,\n\t\t\t\t\tee_start, ee_status ? 'u' : 'w',\n\t\t\t\t\tes->es_lblk, es->es_len,\n\t\t\t\t\text4_es_pblock(es), ext4_es_status(es));\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * We don't check ee_block == es->es_lblk, etc. because es\n\t\t * might be a part of whole extent, vice versa.\n\t\t */\n\t\tif (es->es_lblk < ee_block ||\n\t\t    ext4_es_pblock(es) != ee_start + es->es_lblk - ee_block) {\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"ex_status [%d/%d/%llu/%c] != \"\n\t\t\t\t\"es_status [%d/%d/%llu/%c]\\n\", inode->i_ino,\n\t\t\t\tee_block, ee_len, ee_start,\n\t\t\t\tee_status ? 'u' : 'w', es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), es_status ? 'u' : 'w');\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (ee_status ^ es_status) {\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"ex_status [%d/%d/%llu/%c] != \"\n\t\t\t\t\"es_status [%d/%d/%llu/%c]\\n\", inode->i_ino,\n\t\t\t\tee_block, ee_len, ee_start,\n\t\t\t\tee_status ? 'u' : 'w', es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), es_status ? 'u' : 'w');\n\t\t}\n\t} else {\n\t\t/*\n\t\t * We can't find an extent on disk.  So we need to make sure\n\t\t * that we don't want to add an written/unwritten extent.\n\t\t */\n\t\tif (!ext4_es_is_delayed(es) && !ext4_es_is_hole(es)) {\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"can't find an extent at block %d but we want \"\n\t\t\t\t\"to add a written/unwritten extent \"\n\t\t\t\t\"[%d/%d/%llu/%x]\\n\", inode->i_ino,\n\t\t\t\tes->es_lblk, es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), ext4_es_status(es));\n\t\t}\n\t}\nout:\n\text4_free_ext_path(path);\n}\n\nstatic void ext4_es_insert_extent_ind_check(struct inode *inode,\n\t\t\t\t\t    struct extent_status *es)\n{\n\tstruct ext4_map_blocks map;\n\tint retval;\n\n\t/*\n\t * Here we call ext4_ind_map_blocks to lookup a block mapping because\n\t * 'Indirect' structure is defined in indirect.c.  So we couldn't\n\t * access direct/indirect tree from outside.  It is too dirty to define\n\t * this function in indirect.c file.\n\t */\n\n\tmap.m_lblk = es->es_lblk;\n\tmap.m_len = es->es_len;\n\n\tretval = ext4_ind_map_blocks(NULL, inode, &map, 0);\n\tif (retval > 0) {\n\t\tif (ext4_es_is_delayed(es) || ext4_es_is_hole(es)) {\n\t\t\t/*\n\t\t\t * We want to add a delayed/hole extent but this\n\t\t\t * block has been allocated.\n\t\t\t */\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"We can find blocks but we want to add a \"\n\t\t\t\t\"delayed/hole extent [%d/%d/%llu/%x]\\n\",\n\t\t\t\tinode->i_ino, es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), ext4_es_status(es));\n\t\t\treturn;\n\t\t} else if (ext4_es_is_written(es)) {\n\t\t\tif (retval != es->es_len) {\n\t\t\t\tpr_warn(\"ES insert assertion failed for \"\n\t\t\t\t\t\"inode: %lu retval %d != es_len %d\\n\",\n\t\t\t\t\tinode->i_ino, retval, es->es_len);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tif (map.m_pblk != ext4_es_pblock(es)) {\n\t\t\t\tpr_warn(\"ES insert assertion failed for \"\n\t\t\t\t\t\"inode: %lu m_pblk %llu != \"\n\t\t\t\t\t\"es_pblk %llu\\n\",\n\t\t\t\t\tinode->i_ino, map.m_pblk,\n\t\t\t\t\text4_es_pblock(es));\n\t\t\t\treturn;\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * We don't need to check unwritten extent because\n\t\t\t * indirect-based file doesn't have it.\n\t\t\t */\n\t\t\tBUG();\n\t\t}\n\t} else if (retval == 0) {\n\t\tif (ext4_es_is_written(es)) {\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"We can't find the block but we want to add \"\n\t\t\t\t\"a written extent [%d/%d/%llu/%x]\\n\",\n\t\t\t\tinode->i_ino, es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), ext4_es_status(es));\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic inline void ext4_es_insert_extent_check(struct inode *inode,\n\t\t\t\t\t       struct extent_status *es)\n{\n\t/*\n\t * We don't need to worry about the race condition because\n\t * caller takes i_data_sem locking.\n\t */\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode)->i_data_sem));\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\text4_es_insert_extent_ext_check(inode, es);\n\telse\n\t\text4_es_insert_extent_ind_check(inode, es);\n}\n#else\nstatic inline void ext4_es_insert_extent_check(struct inode *inode,\n\t\t\t\t\t       struct extent_status *es)\n{\n}\n#endif\n\nstatic int __es_insert_extent(struct inode *inode, struct extent_status *newes,\n\t\t\t      struct extent_status *prealloc)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct rb_node **p = &tree->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct extent_status *es;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tes = rb_entry(parent, struct extent_status, rb_node);\n\n\t\tif (newes->es_lblk < es->es_lblk) {\n\t\t\tif (ext4_es_can_be_merged(newes, es)) {\n\t\t\t\t/*\n\t\t\t\t * Here we can modify es_lblk directly\n\t\t\t\t * because it isn't overlapped.\n\t\t\t\t */\n\t\t\t\tes->es_lblk = newes->es_lblk;\n\t\t\t\tes->es_len += newes->es_len;\n\t\t\t\tif (ext4_es_is_written(es) ||\n\t\t\t\t    ext4_es_is_unwritten(es))\n\t\t\t\t\text4_es_store_pblock(es,\n\t\t\t\t\t\t\t     newes->es_pblk);\n\t\t\t\tes = ext4_es_try_to_merge_left(inode, es);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (newes->es_lblk > ext4_es_end(es)) {\n\t\t\tif (ext4_es_can_be_merged(es, newes)) {\n\t\t\t\tes->es_len += newes->es_len;\n\t\t\t\tes = ext4_es_try_to_merge_right(inode, es);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\tBUG();\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (prealloc)\n\t\tes = prealloc;\n\telse\n\t\tes = __es_alloc_extent(false);\n\tif (!es)\n\t\treturn -ENOMEM;\n\text4_es_init_extent(inode, es, newes->es_lblk, newes->es_len,\n\t\t\t    newes->es_pblk);\n\n\trb_link_node(&es->rb_node, parent, p);\n\trb_insert_color(&es->rb_node, &tree->root);\n\nout:\n\ttree->cache_es = es;\n\treturn 0;\n}\n\n/*\n * ext4_es_insert_extent() adds information to an inode's extent\n * status tree.\n */\nvoid ext4_es_insert_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len, ext4_fsblk_t pblk,\n\t\t\t   unsigned int status)\n{\n\tstruct extent_status newes;\n\text4_lblk_t end = lblk + len - 1;\n\tint err1 = 0;\n\tint err2 = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct extent_status *es1 = NULL;\n\tstruct extent_status *es2 = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\tes_debug(\"add [%u/%u) %llu %x to extent status tree of inode %lu\\n\",\n\t\t lblk, len, pblk, status, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tBUG_ON(end < lblk);\n\n\tif ((status & EXTENT_STATUS_DELAYED) &&\n\t    (status & EXTENT_STATUS_WRITTEN)) {\n\t\text4_warning(inode->i_sb, \"Inserting extent [%u/%u] as \"\n\t\t\t\t\" delayed and written which can potentially \"\n\t\t\t\t\" cause data loss.\", lblk, len);\n\t\tWARN_ON(1);\n\t}\n\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = len;\n\text4_es_store_pblock_status(&newes, pblk, status);\n\ttrace_ext4_es_insert_extent(inode, &newes);\n\n\text4_es_insert_extent_check(inode, &newes);\n\nretry:\n\tif (err1 && !es1)\n\t\tes1 = __es_alloc_extent(true);\n\tif ((err1 || err2) && !es2)\n\t\tes2 = __es_alloc_extent(true);\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\n\terr1 = __es_remove_extent(inode, lblk, end, NULL, es1);\n\tif (err1 != 0)\n\t\tgoto error;\n\n\terr2 = __es_insert_extent(inode, &newes, es2);\n\tif (err2 == -ENOMEM && !ext4_es_must_keep(&newes))\n\t\terr2 = 0;\n\tif (err2 != 0)\n\t\tgoto error;\n\n\tif (sbi->s_cluster_ratio > 1 && test_opt(inode->i_sb, DELALLOC) &&\n\t    (status & EXTENT_STATUS_WRITTEN ||\n\t     status & EXTENT_STATUS_UNWRITTEN))\n\t\t__revise_pending(inode, lblk, len);\n\n\t/* es is pre-allocated but not used, free it. */\n\tif (es1 && !es1->es_len)\n\t\t__es_free_extent(es1);\n\tif (es2 && !es2->es_len)\n\t\t__es_free_extent(es2);\nerror:\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err1 || err2)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\treturn;\n}\n\n/*\n * ext4_es_cache_extent() inserts information into the extent status\n * tree if and only if there isn't information about the range in\n * question already.\n */\nvoid ext4_es_cache_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t  ext4_lblk_t len, ext4_fsblk_t pblk,\n\t\t\t  unsigned int status)\n{\n\tstruct extent_status *es;\n\tstruct extent_status newes;\n\text4_lblk_t end = lblk + len - 1;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = len;\n\text4_es_store_pblock_status(&newes, pblk, status);\n\ttrace_ext4_es_cache_extent(inode, &newes);\n\n\tif (!len)\n\t\treturn;\n\n\tBUG_ON(end < lblk);\n\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\n\tes = __es_tree_search(&EXT4_I(inode)->i_es_tree.root, lblk);\n\tif (!es || es->es_lblk > end)\n\t\t__es_insert_extent(inode, &newes, NULL);\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n}\n\n/*\n * ext4_es_lookup_extent() looks up an extent in extent status tree.\n *\n * ext4_es_lookup_extent is called by ext4_map_blocks/ext4_da_map_blocks.\n *\n * Return: 1 on found, 0 on not\n */\nint ext4_es_lookup_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t  ext4_lblk_t *next_lblk,\n\t\t\t  struct extent_status *es)\n{\n\tstruct ext4_es_tree *tree;\n\tstruct ext4_es_stats *stats;\n\tstruct extent_status *es1 = NULL;\n\tstruct rb_node *node;\n\tint found = 0;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn 0;\n\n\ttrace_ext4_es_lookup_extent_enter(inode, lblk);\n\tes_debug(\"lookup extent in block %u\\n\", lblk);\n\n\ttree = &EXT4_I(inode)->i_es_tree;\n\tread_lock(&EXT4_I(inode)->i_es_lock);\n\n\t/* find extent in cache firstly */\n\tes->es_lblk = es->es_len = es->es_pblk = 0;\n\tes1 = READ_ONCE(tree->cache_es);\n\tif (es1 && in_range(lblk, es1->es_lblk, es1->es_len)) {\n\t\tes_debug(\"%u cached by [%u/%u)\\n\",\n\t\t\t lblk, es1->es_lblk, es1->es_len);\n\t\tfound = 1;\n\t\tgoto out;\n\t}\n\n\tnode = tree->root.rb_node;\n\twhile (node) {\n\t\tes1 = rb_entry(node, struct extent_status, rb_node);\n\t\tif (lblk < es1->es_lblk)\n\t\t\tnode = node->rb_left;\n\t\telse if (lblk > ext4_es_end(es1))\n\t\t\tnode = node->rb_right;\n\t\telse {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tstats = &EXT4_SB(inode->i_sb)->s_es_stats;\n\tif (found) {\n\t\tBUG_ON(!es1);\n\t\tes->es_lblk = es1->es_lblk;\n\t\tes->es_len = es1->es_len;\n\t\tes->es_pblk = es1->es_pblk;\n\t\tif (!ext4_es_is_referenced(es1))\n\t\t\text4_es_set_referenced(es1);\n\t\tpercpu_counter_inc(&stats->es_stats_cache_hits);\n\t\tif (next_lblk) {\n\t\t\tnode = rb_next(&es1->rb_node);\n\t\t\tif (node) {\n\t\t\t\tes1 = rb_entry(node, struct extent_status,\n\t\t\t\t\t       rb_node);\n\t\t\t\t*next_lblk = es1->es_lblk;\n\t\t\t} else\n\t\t\t\t*next_lblk = 0;\n\t\t}\n\t} else {\n\t\tpercpu_counter_inc(&stats->es_stats_cache_misses);\n\t}\n\n\tread_unlock(&EXT4_I(inode)->i_es_lock);\n\n\ttrace_ext4_es_lookup_extent_exit(inode, es, found);\n\treturn found;\n}\n\nstruct rsvd_count {\n\tint ndelonly;\n\tbool first_do_lblk_found;\n\text4_lblk_t first_do_lblk;\n\text4_lblk_t last_do_lblk;\n\tstruct extent_status *left_es;\n\tbool partial;\n\text4_lblk_t lclu;\n};\n\n/*\n * init_rsvd - initialize reserved count data before removing block range\n *\t       in file from extent status tree\n *\n * @inode - file containing range\n * @lblk - first block in range\n * @es - pointer to first extent in range\n * @rc - pointer to reserved count data\n *\n * Assumes es is not NULL\n */\nstatic void init_rsvd(struct inode *inode, ext4_lblk_t lblk,\n\t\t      struct extent_status *es, struct rsvd_count *rc)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct rb_node *node;\n\n\trc->ndelonly = 0;\n\n\t/*\n\t * for bigalloc, note the first delonly block in the range has not\n\t * been found, record the extent containing the block to the left of\n\t * the region to be removed, if any, and note that there's no partial\n\t * cluster to track\n\t */\n\tif (sbi->s_cluster_ratio > 1) {\n\t\trc->first_do_lblk_found = false;\n\t\tif (lblk > es->es_lblk) {\n\t\t\trc->left_es = es;\n\t\t} else {\n\t\t\tnode = rb_prev(&es->rb_node);\n\t\t\trc->left_es = node ? rb_entry(node,\n\t\t\t\t\t\t      struct extent_status,\n\t\t\t\t\t\t      rb_node) : NULL;\n\t\t}\n\t\trc->partial = false;\n\t}\n}\n\n/*\n * count_rsvd - count the clusters containing delayed and not unwritten\n *\t\t(delonly) blocks in a range within an extent and add to\n *\t        the running tally in rsvd_count\n *\n * @inode - file containing extent\n * @lblk - first block in range\n * @len - length of range in blocks\n * @es - pointer to extent containing clusters to be counted\n * @rc - pointer to reserved count data\n *\n * Tracks partial clusters found at the beginning and end of extents so\n * they aren't overcounted when they span adjacent extents\n */\nstatic void count_rsvd(struct inode *inode, ext4_lblk_t lblk, long len,\n\t\t       struct extent_status *es, struct rsvd_count *rc)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t i, end, nclu;\n\n\tif (!ext4_es_is_delonly(es))\n\t\treturn;\n\n\tWARN_ON(len <= 0);\n\n\tif (sbi->s_cluster_ratio == 1) {\n\t\trc->ndelonly += (int) len;\n\t\treturn;\n\t}\n\n\t/* bigalloc */\n\n\ti = (lblk < es->es_lblk) ? es->es_lblk : lblk;\n\tend = lblk + (ext4_lblk_t) len - 1;\n\tend = (end > ext4_es_end(es)) ? ext4_es_end(es) : end;\n\n\t/* record the first block of the first delonly extent seen */\n\tif (!rc->first_do_lblk_found) {\n\t\trc->first_do_lblk = i;\n\t\trc->first_do_lblk_found = true;\n\t}\n\n\t/* update the last lblk in the region seen so far */\n\trc->last_do_lblk = end;\n\n\t/*\n\t * if we're tracking a partial cluster and the current extent\n\t * doesn't start with it, count it and stop tracking\n\t */\n\tif (rc->partial && (rc->lclu != EXT4_B2C(sbi, i))) {\n\t\trc->ndelonly++;\n\t\trc->partial = false;\n\t}\n\n\t/*\n\t * if the first cluster doesn't start on a cluster boundary but\n\t * ends on one, count it\n\t */\n\tif (EXT4_LBLK_COFF(sbi, i) != 0) {\n\t\tif (end >= EXT4_LBLK_CFILL(sbi, i)) {\n\t\t\trc->ndelonly++;\n\t\t\trc->partial = false;\n\t\t\ti = EXT4_LBLK_CFILL(sbi, i) + 1;\n\t\t}\n\t}\n\n\t/*\n\t * if the current cluster starts on a cluster boundary, count the\n\t * number of whole delonly clusters in the extent\n\t */\n\tif ((i + sbi->s_cluster_ratio - 1) <= end) {\n\t\tnclu = (end - i + 1) >> sbi->s_cluster_bits;\n\t\trc->ndelonly += nclu;\n\t\ti += nclu << sbi->s_cluster_bits;\n\t}\n\n\t/*\n\t * start tracking a partial cluster if there's a partial at the end\n\t * of the current extent and we're not already tracking one\n\t */\n\tif (!rc->partial && i <= end) {\n\t\trc->partial = true;\n\t\trc->lclu = EXT4_B2C(sbi, i);\n\t}\n}\n\n/*\n * __pr_tree_search - search for a pending cluster reservation\n *\n * @root - root of pending reservation tree\n * @lclu - logical cluster to search for\n *\n * Returns the pending reservation for the cluster identified by @lclu\n * if found.  If not, returns a reservation for the next cluster if any,\n * and if not, returns NULL.\n */\nstatic struct pending_reservation *__pr_tree_search(struct rb_root *root,\n\t\t\t\t\t\t    ext4_lblk_t lclu)\n{\n\tstruct rb_node *node = root->rb_node;\n\tstruct pending_reservation *pr = NULL;\n\n\twhile (node) {\n\t\tpr = rb_entry(node, struct pending_reservation, rb_node);\n\t\tif (lclu < pr->lclu)\n\t\t\tnode = node->rb_left;\n\t\telse if (lclu > pr->lclu)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn pr;\n\t}\n\tif (pr && lclu < pr->lclu)\n\t\treturn pr;\n\tif (pr && lclu > pr->lclu) {\n\t\tnode = rb_next(&pr->rb_node);\n\t\treturn node ? rb_entry(node, struct pending_reservation,\n\t\t\t\t       rb_node) : NULL;\n\t}\n\treturn NULL;\n}\n\n/*\n * get_rsvd - calculates and returns the number of cluster reservations to be\n *\t      released when removing a block range from the extent status tree\n *\t      and releases any pending reservations within the range\n *\n * @inode - file containing block range\n * @end - last block in range\n * @right_es - pointer to extent containing next block beyond end or NULL\n * @rc - pointer to reserved count data\n *\n * The number of reservations to be released is equal to the number of\n * clusters containing delayed and not unwritten (delonly) blocks within\n * the range, minus the number of clusters still containing delonly blocks\n * at the ends of the range, and minus the number of pending reservations\n * within the range.\n */\nstatic unsigned int get_rsvd(struct inode *inode, ext4_lblk_t end,\n\t\t\t     struct extent_status *right_es,\n\t\t\t     struct rsvd_count *rc)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct pending_reservation *pr;\n\tstruct ext4_pending_tree *tree = &EXT4_I(inode)->i_pending_tree;\n\tstruct rb_node *node;\n\text4_lblk_t first_lclu, last_lclu;\n\tbool left_delonly, right_delonly, count_pending;\n\tstruct extent_status *es;\n\n\tif (sbi->s_cluster_ratio > 1) {\n\t\t/* count any remaining partial cluster */\n\t\tif (rc->partial)\n\t\t\trc->ndelonly++;\n\n\t\tif (rc->ndelonly == 0)\n\t\t\treturn 0;\n\n\t\tfirst_lclu = EXT4_B2C(sbi, rc->first_do_lblk);\n\t\tlast_lclu = EXT4_B2C(sbi, rc->last_do_lblk);\n\n\t\t/*\n\t\t * decrease the delonly count by the number of clusters at the\n\t\t * ends of the range that still contain delonly blocks -\n\t\t * these clusters still need to be reserved\n\t\t */\n\t\tleft_delonly = right_delonly = false;\n\n\t\tes = rc->left_es;\n\t\twhile (es && ext4_es_end(es) >=\n\t\t       EXT4_LBLK_CMASK(sbi, rc->first_do_lblk)) {\n\t\t\tif (ext4_es_is_delonly(es)) {\n\t\t\t\trc->ndelonly--;\n\t\t\t\tleft_delonly = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tnode = rb_prev(&es->rb_node);\n\t\t\tif (!node)\n\t\t\t\tbreak;\n\t\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\t}\n\t\tif (right_es && (!left_delonly || first_lclu != last_lclu)) {\n\t\t\tif (end < ext4_es_end(right_es)) {\n\t\t\t\tes = right_es;\n\t\t\t} else {\n\t\t\t\tnode = rb_next(&right_es->rb_node);\n\t\t\t\tes = node ? rb_entry(node, struct extent_status,\n\t\t\t\t\t\t     rb_node) : NULL;\n\t\t\t}\n\t\t\twhile (es && es->es_lblk <=\n\t\t\t       EXT4_LBLK_CFILL(sbi, rc->last_do_lblk)) {\n\t\t\t\tif (ext4_es_is_delonly(es)) {\n\t\t\t\t\trc->ndelonly--;\n\t\t\t\t\tright_delonly = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode = rb_next(&es->rb_node);\n\t\t\t\tif (!node)\n\t\t\t\t\tbreak;\n\t\t\t\tes = rb_entry(node, struct extent_status,\n\t\t\t\t\t      rb_node);\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Determine the block range that should be searched for\n\t\t * pending reservations, if any.  Clusters on the ends of the\n\t\t * original removed range containing delonly blocks are\n\t\t * excluded.  They've already been accounted for and it's not\n\t\t * possible to determine if an associated pending reservation\n\t\t * should be released with the information available in the\n\t\t * extents status tree.\n\t\t */\n\t\tif (first_lclu == last_lclu) {\n\t\t\tif (left_delonly | right_delonly)\n\t\t\t\tcount_pending = false;\n\t\t\telse\n\t\t\t\tcount_pending = true;\n\t\t} else {\n\t\t\tif (left_delonly)\n\t\t\t\tfirst_lclu++;\n\t\t\tif (right_delonly)\n\t\t\t\tlast_lclu--;\n\t\t\tif (first_lclu <= last_lclu)\n\t\t\t\tcount_pending = true;\n\t\t\telse\n\t\t\t\tcount_pending = false;\n\t\t}\n\n\t\t/*\n\t\t * a pending reservation found between first_lclu and last_lclu\n\t\t * represents an allocated cluster that contained at least one\n\t\t * delonly block, so the delonly total must be reduced by one\n\t\t * for each pending reservation found and released\n\t\t */\n\t\tif (count_pending) {\n\t\t\tpr = __pr_tree_search(&tree->root, first_lclu);\n\t\t\twhile (pr && pr->lclu <= last_lclu) {\n\t\t\t\trc->ndelonly--;\n\t\t\t\tnode = rb_next(&pr->rb_node);\n\t\t\t\trb_erase(&pr->rb_node, &tree->root);\n\t\t\t\tkmem_cache_free(ext4_pending_cachep, pr);\n\t\t\t\tif (!node)\n\t\t\t\t\tbreak;\n\t\t\t\tpr = rb_entry(node, struct pending_reservation,\n\t\t\t\t\t      rb_node);\n\t\t\t}\n\t\t}\n\t}\n\treturn rc->ndelonly;\n}\n\n\n/*\n * __es_remove_extent - removes block range from extent status tree\n *\n * @inode - file containing range\n * @lblk - first block in range\n * @end - last block in range\n * @reserved - number of cluster reservations released\n * @prealloc - pre-allocated es to avoid memory allocation failures\n *\n * If @reserved is not NULL and delayed allocation is enabled, counts\n * block/cluster reservations freed by removing range and if bigalloc\n * enabled cancels pending reservations as needed. Returns 0 on success,\n * error code on failure.\n */\nstatic int __es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t      ext4_lblk_t end, int *reserved,\n\t\t\t      struct extent_status *prealloc)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct rb_node *node;\n\tstruct extent_status *es;\n\tstruct extent_status orig_es;\n\text4_lblk_t len1, len2;\n\text4_fsblk_t block;\n\tint err = 0;\n\tbool count_reserved = true;\n\tstruct rsvd_count rc;\n\n\tif (reserved == NULL || !test_opt(inode->i_sb, DELALLOC))\n\t\tcount_reserved = false;\n\n\tes = __es_tree_search(&tree->root, lblk);\n\tif (!es)\n\t\tgoto out;\n\tif (es->es_lblk > end)\n\t\tgoto out;\n\n\t/* Simply invalidate cache_es. */\n\ttree->cache_es = NULL;\n\tif (count_reserved)\n\t\tinit_rsvd(inode, lblk, es, &rc);\n\n\torig_es.es_lblk = es->es_lblk;\n\torig_es.es_len = es->es_len;\n\torig_es.es_pblk = es->es_pblk;\n\n\tlen1 = lblk > es->es_lblk ? lblk - es->es_lblk : 0;\n\tlen2 = ext4_es_end(es) > end ? ext4_es_end(es) - end : 0;\n\tif (len1 > 0)\n\t\tes->es_len = len1;\n\tif (len2 > 0) {\n\t\tif (len1 > 0) {\n\t\t\tstruct extent_status newes;\n\n\t\t\tnewes.es_lblk = end + 1;\n\t\t\tnewes.es_len = len2;\n\t\t\tblock = 0x7FDEADBEEFULL;\n\t\t\tif (ext4_es_is_written(&orig_es) ||\n\t\t\t    ext4_es_is_unwritten(&orig_es))\n\t\t\t\tblock = ext4_es_pblock(&orig_es) +\n\t\t\t\t\torig_es.es_len - len2;\n\t\t\text4_es_store_pblock_status(&newes, block,\n\t\t\t\t\t\t    ext4_es_status(&orig_es));\n\t\t\terr = __es_insert_extent(inode, &newes, prealloc);\n\t\t\tif (err) {\n\t\t\t\tif (!ext4_es_must_keep(&newes))\n\t\t\t\t\treturn 0;\n\n\t\t\t\tes->es_lblk = orig_es.es_lblk;\n\t\t\t\tes->es_len = orig_es.es_len;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tes->es_lblk = end + 1;\n\t\t\tes->es_len = len2;\n\t\t\tif (ext4_es_is_written(es) ||\n\t\t\t    ext4_es_is_unwritten(es)) {\n\t\t\t\tblock = orig_es.es_pblk + orig_es.es_len - len2;\n\t\t\t\text4_es_store_pblock(es, block);\n\t\t\t}\n\t\t}\n\t\tif (count_reserved)\n\t\t\tcount_rsvd(inode, lblk, orig_es.es_len - len1 - len2,\n\t\t\t\t   &orig_es, &rc);\n\t\tgoto out_get_reserved;\n\t}\n\n\tif (len1 > 0) {\n\t\tif (count_reserved)\n\t\t\tcount_rsvd(inode, lblk, orig_es.es_len - len1,\n\t\t\t\t   &orig_es, &rc);\n\t\tnode = rb_next(&es->rb_node);\n\t\tif (node)\n\t\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\telse\n\t\t\tes = NULL;\n\t}\n\n\twhile (es && ext4_es_end(es) <= end) {\n\t\tif (count_reserved)\n\t\t\tcount_rsvd(inode, es->es_lblk, es->es_len, es, &rc);\n\t\tnode = rb_next(&es->rb_node);\n\t\trb_erase(&es->rb_node, &tree->root);\n\t\text4_es_free_extent(inode, es);\n\t\tif (!node) {\n\t\t\tes = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t}\n\n\tif (es && es->es_lblk < end + 1) {\n\t\text4_lblk_t orig_len = es->es_len;\n\n\t\tlen1 = ext4_es_end(es) - end;\n\t\tif (count_reserved)\n\t\t\tcount_rsvd(inode, es->es_lblk, orig_len - len1,\n\t\t\t\t   es, &rc);\n\t\tes->es_lblk = end + 1;\n\t\tes->es_len = len1;\n\t\tif (ext4_es_is_written(es) || ext4_es_is_unwritten(es)) {\n\t\t\tblock = es->es_pblk + orig_len - len1;\n\t\t\text4_es_store_pblock(es, block);\n\t\t}\n\t}\n\nout_get_reserved:\n\tif (count_reserved)\n\t\t*reserved = get_rsvd(inode, end, es, &rc);\nout:\n\treturn err;\n}\n\n/*\n * ext4_es_remove_extent - removes block range from extent status tree\n *\n * @inode - file containing range\n * @lblk - first block in range\n * @len - number of blocks to remove\n *\n * Reduces block/cluster reservation count and for bigalloc cancels pending\n * reservations as needed.\n */\nvoid ext4_es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len)\n{\n\text4_lblk_t end;\n\tint err = 0;\n\tint reserved = 0;\n\tstruct extent_status *es = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_remove_extent(inode, lblk, len);\n\tes_debug(\"remove [%u/%u) from extent status tree of inode %lu\\n\",\n\t\t lblk, len, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tend = lblk + len - 1;\n\tBUG_ON(end < lblk);\n\nretry:\n\tif (err && !es)\n\t\tes = __es_alloc_extent(true);\n\t/*\n\t * ext4_clear_inode() depends on us taking i_es_lock unconditionally\n\t * so that we are sure __es_shrink() is done with the inode before it\n\t * is reclaimed.\n\t */\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n\tif (es && !es->es_len)\n\t\t__es_free_extent(es);\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_da_release_space(inode, reserved);\n\treturn;\n}\n\nstatic int __es_shrink(struct ext4_sb_info *sbi, int nr_to_scan,\n\t\t       struct ext4_inode_info *locked_ei)\n{\n\tstruct ext4_inode_info *ei;\n\tstruct ext4_es_stats *es_stats;\n\tktime_t start_time;\n\tu64 scan_time;\n\tint nr_to_walk;\n\tint nr_shrunk = 0;\n\tint retried = 0, nr_skipped = 0;\n\n\tes_stats = &sbi->s_es_stats;\n\tstart_time = ktime_get();\n\nretry:\n\tspin_lock(&sbi->s_es_lock);\n\tnr_to_walk = sbi->s_es_nr_inode;\n\twhile (nr_to_walk-- > 0) {\n\t\tif (list_empty(&sbi->s_es_list)) {\n\t\t\tspin_unlock(&sbi->s_es_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tei = list_first_entry(&sbi->s_es_list, struct ext4_inode_info,\n\t\t\t\t      i_es_list);\n\t\t/* Move the inode to the tail */\n\t\tlist_move_tail(&ei->i_es_list, &sbi->s_es_list);\n\n\t\t/*\n\t\t * Normally we try hard to avoid shrinking precached inodes,\n\t\t * but we will as a last resort.\n\t\t */\n\t\tif (!retried && ext4_test_inode_state(&ei->vfs_inode,\n\t\t\t\t\t\tEXT4_STATE_EXT_PRECACHED)) {\n\t\t\tnr_skipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ei == locked_ei || !write_trylock(&ei->i_es_lock)) {\n\t\t\tnr_skipped++;\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * Now we hold i_es_lock which protects us from inode reclaim\n\t\t * freeing inode under us\n\t\t */\n\t\tspin_unlock(&sbi->s_es_lock);\n\n\t\tnr_shrunk += es_reclaim_extents(ei, &nr_to_scan);\n\t\twrite_unlock(&ei->i_es_lock);\n\n\t\tif (nr_to_scan <= 0)\n\t\t\tgoto out;\n\t\tspin_lock(&sbi->s_es_lock);\n\t}\n\tspin_unlock(&sbi->s_es_lock);\n\n\t/*\n\t * If we skipped any inodes, and we weren't able to make any\n\t * forward progress, try again to scan precached inodes.\n\t */\n\tif ((nr_shrunk == 0) && nr_skipped && !retried) {\n\t\tretried++;\n\t\tgoto retry;\n\t}\n\n\tif (locked_ei && nr_shrunk == 0)\n\t\tnr_shrunk = es_reclaim_extents(locked_ei, &nr_to_scan);\n\nout:\n\tscan_time = ktime_to_ns(ktime_sub(ktime_get(), start_time));\n\tif (likely(es_stats->es_stats_scan_time))\n\t\tes_stats->es_stats_scan_time = (scan_time +\n\t\t\t\tes_stats->es_stats_scan_time*3) / 4;\n\telse\n\t\tes_stats->es_stats_scan_time = scan_time;\n\tif (scan_time > es_stats->es_stats_max_scan_time)\n\t\tes_stats->es_stats_max_scan_time = scan_time;\n\tif (likely(es_stats->es_stats_shrunk))\n\t\tes_stats->es_stats_shrunk = (nr_shrunk +\n\t\t\t\tes_stats->es_stats_shrunk*3) / 4;\n\telse\n\t\tes_stats->es_stats_shrunk = nr_shrunk;\n\n\ttrace_ext4_es_shrink(sbi->s_sb, nr_shrunk, scan_time,\n\t\t\t     nr_skipped, retried);\n\treturn nr_shrunk;\n}\n\nstatic unsigned long ext4_es_count(struct shrinker *shrink,\n\t\t\t\t   struct shrink_control *sc)\n{\n\tunsigned long nr;\n\tstruct ext4_sb_info *sbi;\n\n\tsbi = container_of(shrink, struct ext4_sb_info, s_es_shrinker);\n\tnr = percpu_counter_read_positive(&sbi->s_es_stats.es_stats_shk_cnt);\n\ttrace_ext4_es_shrink_count(sbi->s_sb, sc->nr_to_scan, nr);\n\treturn nr;\n}\n\nstatic unsigned long ext4_es_scan(struct shrinker *shrink,\n\t\t\t\t  struct shrink_control *sc)\n{\n\tstruct ext4_sb_info *sbi = container_of(shrink,\n\t\t\t\t\tstruct ext4_sb_info, s_es_shrinker);\n\tint nr_to_scan = sc->nr_to_scan;\n\tint ret, nr_shrunk;\n\n\tret = percpu_counter_read_positive(&sbi->s_es_stats.es_stats_shk_cnt);\n\ttrace_ext4_es_shrink_scan_enter(sbi->s_sb, nr_to_scan, ret);\n\n\tnr_shrunk = __es_shrink(sbi, nr_to_scan, NULL);\n\n\tret = percpu_counter_read_positive(&sbi->s_es_stats.es_stats_shk_cnt);\n\ttrace_ext4_es_shrink_scan_exit(sbi->s_sb, nr_shrunk, ret);\n\treturn nr_shrunk;\n}\n\nint ext4_seq_es_shrinker_info_show(struct seq_file *seq, void *v)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB((struct super_block *) seq->private);\n\tstruct ext4_es_stats *es_stats = &sbi->s_es_stats;\n\tstruct ext4_inode_info *ei, *max = NULL;\n\tunsigned int inode_cnt = 0;\n\n\tif (v != SEQ_START_TOKEN)\n\t\treturn 0;\n\n\t/* here we just find an inode that has the max nr. of objects */\n\tspin_lock(&sbi->s_es_lock);\n\tlist_for_each_entry(ei, &sbi->s_es_list, i_es_list) {\n\t\tinode_cnt++;\n\t\tif (max && max->i_es_all_nr < ei->i_es_all_nr)\n\t\t\tmax = ei;\n\t\telse if (!max)\n\t\t\tmax = ei;\n\t}\n\tspin_unlock(&sbi->s_es_lock);\n\n\tseq_printf(seq, \"stats:\\n  %lld objects\\n  %lld reclaimable objects\\n\",\n\t\t   percpu_counter_sum_positive(&es_stats->es_stats_all_cnt),\n\t\t   percpu_counter_sum_positive(&es_stats->es_stats_shk_cnt));\n\tseq_printf(seq, \"  %lld/%lld cache hits/misses\\n\",\n\t\t   percpu_counter_sum_positive(&es_stats->es_stats_cache_hits),\n\t\t   percpu_counter_sum_positive(&es_stats->es_stats_cache_misses));\n\tif (inode_cnt)\n\t\tseq_printf(seq, \"  %d inodes on list\\n\", inode_cnt);\n\n\tseq_printf(seq, \"average:\\n  %llu us scan time\\n\",\n\t    div_u64(es_stats->es_stats_scan_time, 1000));\n\tseq_printf(seq, \"  %lu shrunk objects\\n\", es_stats->es_stats_shrunk);\n\tif (inode_cnt)\n\t\tseq_printf(seq,\n\t\t    \"maximum:\\n  %lu inode (%u objects, %u reclaimable)\\n\"\n\t\t    \"  %llu us max scan time\\n\",\n\t\t    max->vfs_inode.i_ino, max->i_es_all_nr, max->i_es_shk_nr,\n\t\t    div_u64(es_stats->es_stats_max_scan_time, 1000));\n\n\treturn 0;\n}\n\nint ext4_es_register_shrinker(struct ext4_sb_info *sbi)\n{\n\tint err;\n\n\t/* Make sure we have enough bits for physical block number */\n\tBUILD_BUG_ON(ES_SHIFT < 48);\n\tINIT_LIST_HEAD(&sbi->s_es_list);\n\tsbi->s_es_nr_inode = 0;\n\tspin_lock_init(&sbi->s_es_lock);\n\tsbi->s_es_stats.es_stats_shrunk = 0;\n\terr = percpu_counter_init(&sbi->s_es_stats.es_stats_cache_hits, 0,\n\t\t\t\t  GFP_KERNEL);\n\tif (err)\n\t\treturn err;\n\terr = percpu_counter_init(&sbi->s_es_stats.es_stats_cache_misses, 0,\n\t\t\t\t  GFP_KERNEL);\n\tif (err)\n\t\tgoto err1;\n\tsbi->s_es_stats.es_stats_scan_time = 0;\n\tsbi->s_es_stats.es_stats_max_scan_time = 0;\n\terr = percpu_counter_init(&sbi->s_es_stats.es_stats_all_cnt, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err2;\n\terr = percpu_counter_init(&sbi->s_es_stats.es_stats_shk_cnt, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err3;\n\n\tsbi->s_es_shrinker.scan_objects = ext4_es_scan;\n\tsbi->s_es_shrinker.count_objects = ext4_es_count;\n\tsbi->s_es_shrinker.seeks = DEFAULT_SEEKS;\n\terr = register_shrinker(&sbi->s_es_shrinker, \"ext4-es:%s\",\n\t\t\t\tsbi->s_sb->s_id);\n\tif (err)\n\t\tgoto err4;\n\n\treturn 0;\nerr4:\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_shk_cnt);\nerr3:\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_all_cnt);\nerr2:\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_cache_misses);\nerr1:\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_cache_hits);\n\treturn err;\n}\n\nvoid ext4_es_unregister_shrinker(struct ext4_sb_info *sbi)\n{\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_cache_hits);\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_cache_misses);\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_all_cnt);\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_shk_cnt);\n\tunregister_shrinker(&sbi->s_es_shrinker);\n}\n\n/*\n * Shrink extents in given inode from ei->i_es_shrink_lblk till end. Scan at\n * most *nr_to_scan extents, update *nr_to_scan accordingly.\n *\n * Return 0 if we hit end of tree / interval, 1 if we exhausted nr_to_scan.\n * Increment *nr_shrunk by the number of reclaimed extents. Also update\n * ei->i_es_shrink_lblk to where we should continue scanning.\n */\nstatic int es_do_reclaim_extents(struct ext4_inode_info *ei, ext4_lblk_t end,\n\t\t\t\t int *nr_to_scan, int *nr_shrunk)\n{\n\tstruct inode *inode = &ei->vfs_inode;\n\tstruct ext4_es_tree *tree = &ei->i_es_tree;\n\tstruct extent_status *es;\n\tstruct rb_node *node;\n\n\tes = __es_tree_search(&tree->root, ei->i_es_shrink_lblk);\n\tif (!es)\n\t\tgoto out_wrap;\n\n\twhile (*nr_to_scan > 0) {\n\t\tif (es->es_lblk > end) {\n\t\t\tei->i_es_shrink_lblk = end + 1;\n\t\t\treturn 0;\n\t\t}\n\n\t\t(*nr_to_scan)--;\n\t\tnode = rb_next(&es->rb_node);\n\n\t\tif (ext4_es_must_keep(es))\n\t\t\tgoto next;\n\t\tif (ext4_es_is_referenced(es)) {\n\t\t\text4_es_clear_referenced(es);\n\t\t\tgoto next;\n\t\t}\n\n\t\trb_erase(&es->rb_node, &tree->root);\n\t\text4_es_free_extent(inode, es);\n\t\t(*nr_shrunk)++;\nnext:\n\t\tif (!node)\n\t\t\tgoto out_wrap;\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t}\n\tei->i_es_shrink_lblk = es->es_lblk;\n\treturn 1;\nout_wrap:\n\tei->i_es_shrink_lblk = 0;\n\treturn 0;\n}\n\nstatic int es_reclaim_extents(struct ext4_inode_info *ei, int *nr_to_scan)\n{\n\tstruct inode *inode = &ei->vfs_inode;\n\tint nr_shrunk = 0;\n\text4_lblk_t start = ei->i_es_shrink_lblk;\n\tstatic DEFINE_RATELIMIT_STATE(_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tif (ei->i_es_shk_nr == 0)\n\t\treturn 0;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_EXT_PRECACHED) &&\n\t    __ratelimit(&_rs))\n\t\text4_warning(inode->i_sb, \"forced shrink of precached extents\");\n\n\tif (!es_do_reclaim_extents(ei, EXT_MAX_BLOCKS, nr_to_scan, &nr_shrunk) &&\n\t    start != 0)\n\t\tes_do_reclaim_extents(ei, start - 1, nr_to_scan, &nr_shrunk);\n\n\tei->i_es_tree.cache_es = NULL;\n\treturn nr_shrunk;\n}\n\n/*\n * Called to support EXT4_IOC_CLEAR_ES_CACHE.  We can only remove\n * discretionary entries from the extent status cache.  (Some entries\n * must be present for proper operations.)\n */\nvoid ext4_clear_inode_es(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct extent_status *es;\n\tstruct ext4_es_tree *tree;\n\tstruct rb_node *node;\n\n\twrite_lock(&ei->i_es_lock);\n\ttree = &EXT4_I(inode)->i_es_tree;\n\ttree->cache_es = NULL;\n\tnode = rb_first(&tree->root);\n\twhile (node) {\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\tnode = rb_next(node);\n\t\tif (!ext4_es_must_keep(es)) {\n\t\t\trb_erase(&es->rb_node, &tree->root);\n\t\t\text4_es_free_extent(inode, es);\n\t\t}\n\t}\n\text4_clear_inode_state(inode, EXT4_STATE_EXT_PRECACHED);\n\twrite_unlock(&ei->i_es_lock);\n}\n\n#ifdef ES_DEBUG__\nstatic void ext4_print_pending_tree(struct inode *inode)\n{\n\tstruct ext4_pending_tree *tree;\n\tstruct rb_node *node;\n\tstruct pending_reservation *pr;\n\n\tprintk(KERN_DEBUG \"pending reservations for inode %lu:\", inode->i_ino);\n\ttree = &EXT4_I(inode)->i_pending_tree;\n\tnode = rb_first(&tree->root);\n\twhile (node) {\n\t\tpr = rb_entry(node, struct pending_reservation, rb_node);\n\t\tprintk(KERN_DEBUG \" %u\", pr->lclu);\n\t\tnode = rb_next(node);\n\t}\n\tprintk(KERN_DEBUG \"\\n\");\n}\n#else\n#define ext4_print_pending_tree(inode)\n#endif\n\nint __init ext4_init_pending(void)\n{\n\text4_pending_cachep = KMEM_CACHE(pending_reservation, SLAB_RECLAIM_ACCOUNT);\n\tif (ext4_pending_cachep == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid ext4_exit_pending(void)\n{\n\tkmem_cache_destroy(ext4_pending_cachep);\n}\n\nvoid ext4_init_pending_tree(struct ext4_pending_tree *tree)\n{\n\ttree->root = RB_ROOT;\n}\n\n/*\n * __get_pending - retrieve a pointer to a pending reservation\n *\n * @inode - file containing the pending cluster reservation\n * @lclu - logical cluster of interest\n *\n * Returns a pointer to a pending reservation if it's a member of\n * the set, and NULL if not.  Must be called holding i_es_lock.\n */\nstatic struct pending_reservation *__get_pending(struct inode *inode,\n\t\t\t\t\t\t ext4_lblk_t lclu)\n{\n\tstruct ext4_pending_tree *tree;\n\tstruct rb_node *node;\n\tstruct pending_reservation *pr = NULL;\n\n\ttree = &EXT4_I(inode)->i_pending_tree;\n\tnode = (&tree->root)->rb_node;\n\n\twhile (node) {\n\t\tpr = rb_entry(node, struct pending_reservation, rb_node);\n\t\tif (lclu < pr->lclu)\n\t\t\tnode = node->rb_left;\n\t\telse if (lclu > pr->lclu)\n\t\t\tnode = node->rb_right;\n\t\telse if (lclu == pr->lclu)\n\t\t\treturn pr;\n\t}\n\treturn NULL;\n}\n\n/*\n * __insert_pending - adds a pending cluster reservation to the set of\n *                    pending reservations\n *\n * @inode - file containing the cluster\n * @lblk - logical block in the cluster to be added\n *\n * Returns 0 on successful insertion and -ENOMEM on failure.  If the\n * pending reservation is already in the set, returns successfully.\n */\nstatic int __insert_pending(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_pending_tree *tree = &EXT4_I(inode)->i_pending_tree;\n\tstruct rb_node **p = &tree->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct pending_reservation *pr;\n\text4_lblk_t lclu;\n\tint ret = 0;\n\n\tlclu = EXT4_B2C(sbi, lblk);\n\t/* search to find parent for insertion */\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpr = rb_entry(parent, struct pending_reservation, rb_node);\n\n\t\tif (lclu < pr->lclu) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (lclu > pr->lclu) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\t/* pending reservation already inserted */\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr = kmem_cache_alloc(ext4_pending_cachep, GFP_ATOMIC);\n\tif (pr == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpr->lclu = lclu;\n\n\trb_link_node(&pr->rb_node, parent, p);\n\trb_insert_color(&pr->rb_node, &tree->root);\n\nout:\n\treturn ret;\n}\n\n/*\n * __remove_pending - removes a pending cluster reservation from the set\n *                    of pending reservations\n *\n * @inode - file containing the cluster\n * @lblk - logical block in the pending cluster reservation to be removed\n *\n * Returns successfully if pending reservation is not a member of the set.\n */\nstatic void __remove_pending(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct pending_reservation *pr;\n\tstruct ext4_pending_tree *tree;\n\n\tpr = __get_pending(inode, EXT4_B2C(sbi, lblk));\n\tif (pr != NULL) {\n\t\ttree = &EXT4_I(inode)->i_pending_tree;\n\t\trb_erase(&pr->rb_node, &tree->root);\n\t\tkmem_cache_free(ext4_pending_cachep, pr);\n\t}\n}\n\n/*\n * ext4_remove_pending - removes a pending cluster reservation from the set\n *                       of pending reservations\n *\n * @inode - file containing the cluster\n * @lblk - logical block in the pending cluster reservation to be removed\n *\n * Locking for external use of __remove_pending.\n */\nvoid ext4_remove_pending(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\twrite_lock(&ei->i_es_lock);\n\t__remove_pending(inode, lblk);\n\twrite_unlock(&ei->i_es_lock);\n}\n\n/*\n * ext4_is_pending - determine whether a cluster has a pending reservation\n *                   on it\n *\n * @inode - file containing the cluster\n * @lblk - logical block in the cluster\n *\n * Returns true if there's a pending reservation for the cluster in the\n * set of pending reservations, and false if not.\n */\nbool ext4_is_pending(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tbool ret;\n\n\tread_lock(&ei->i_es_lock);\n\tret = (bool)(__get_pending(inode, EXT4_B2C(sbi, lblk)) != NULL);\n\tread_unlock(&ei->i_es_lock);\n\n\treturn ret;\n}\n\n/*\n * ext4_es_insert_delayed_block - adds a delayed block to the extents status\n *                                tree, adding a pending reservation where\n *                                needed\n *\n * @inode - file containing the newly added block\n * @lblk - logical block to be added\n * @allocated - indicates whether a physical cluster has been allocated for\n *              the logical cluster that contains the block\n */\nvoid ext4_es_insert_delayed_block(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t\t  bool allocated)\n{\n\tstruct extent_status newes;\n\tint err1 = 0;\n\tint err2 = 0;\n\tstruct extent_status *es1 = NULL;\n\tstruct extent_status *es2 = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\tes_debug(\"add [%u/1) delayed to extent status tree of inode %lu\\n\",\n\t\t lblk, inode->i_ino);\n\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = 1;\n\text4_es_store_pblock_status(&newes, ~0, EXTENT_STATUS_DELAYED);\n\ttrace_ext4_es_insert_delayed_block(inode, &newes, allocated);\n\n\text4_es_insert_extent_check(inode, &newes);\n\nretry:\n\tif (err1 && !es1)\n\t\tes1 = __es_alloc_extent(true);\n\tif ((err1 || err2) && !es2)\n\t\tes2 = __es_alloc_extent(true);\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\n\terr1 = __es_remove_extent(inode, lblk, lblk, NULL, es1);\n\tif (err1 != 0)\n\t\tgoto error;\n\n\terr2 = __es_insert_extent(inode, &newes, es2);\n\tif (err2 != 0)\n\t\tgoto error;\n\n\tif (allocated)\n\t\t__insert_pending(inode, lblk);\n\n\t/* es is pre-allocated but not used, free it. */\n\tif (es1 && !es1->es_len)\n\t\t__es_free_extent(es1);\n\tif (es2 && !es2->es_len)\n\t\t__es_free_extent(es2);\nerror:\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err1 || err2)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_print_pending_tree(inode);\n\treturn;\n}\n\n/*\n * __es_delayed_clu - count number of clusters containing blocks that\n *                    are delayed only\n *\n * @inode - file containing block range\n * @start - logical block defining start of range\n * @end - logical block defining end of range\n *\n * Returns the number of clusters containing only delayed (not delayed\n * and unwritten) blocks in the range specified by @start and @end.  Any\n * cluster or part of a cluster within the range and containing a delayed\n * and not unwritten block within the range is counted as a whole cluster.\n */\nstatic unsigned int __es_delayed_clu(struct inode *inode, ext4_lblk_t start,\n\t\t\t\t     ext4_lblk_t end)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct extent_status *es;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct rb_node *node;\n\text4_lblk_t first_lclu, last_lclu;\n\tunsigned long long last_counted_lclu;\n\tunsigned int n = 0;\n\n\t/* guaranteed to be unequal to any ext4_lblk_t value */\n\tlast_counted_lclu = ~0ULL;\n\n\tes = __es_tree_search(&tree->root, start);\n\n\twhile (es && (es->es_lblk <= end)) {\n\t\tif (ext4_es_is_delonly(es)) {\n\t\t\tif (es->es_lblk <= start)\n\t\t\t\tfirst_lclu = EXT4_B2C(sbi, start);\n\t\t\telse\n\t\t\t\tfirst_lclu = EXT4_B2C(sbi, es->es_lblk);\n\n\t\t\tif (ext4_es_end(es) >= end)\n\t\t\t\tlast_lclu = EXT4_B2C(sbi, end);\n\t\t\telse\n\t\t\t\tlast_lclu = EXT4_B2C(sbi, ext4_es_end(es));\n\n\t\t\tif (first_lclu == last_counted_lclu)\n\t\t\t\tn += last_lclu - first_lclu;\n\t\t\telse\n\t\t\t\tn += last_lclu - first_lclu + 1;\n\t\t\tlast_counted_lclu = last_lclu;\n\t\t}\n\t\tnode = rb_next(&es->rb_node);\n\t\tif (!node)\n\t\t\tbreak;\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t}\n\n\treturn n;\n}\n\n/*\n * ext4_es_delayed_clu - count number of clusters containing blocks that\n *                       are both delayed and unwritten\n *\n * @inode - file containing block range\n * @lblk - logical block defining start of range\n * @len - number of blocks in range\n *\n * Locking for external use of __es_delayed_clu().\n */\nunsigned int ext4_es_delayed_clu(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t\t ext4_lblk_t len)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\text4_lblk_t end;\n\tunsigned int n;\n\n\tif (len == 0)\n\t\treturn 0;\n\n\tend = lblk + len - 1;\n\tWARN_ON(end < lblk);\n\n\tread_lock(&ei->i_es_lock);\n\n\tn = __es_delayed_clu(inode, lblk, end);\n\n\tread_unlock(&ei->i_es_lock);\n\n\treturn n;\n}\n\n/*\n * __revise_pending - makes, cancels, or leaves unchanged pending cluster\n *                    reservations for a specified block range depending\n *                    upon the presence or absence of delayed blocks\n *                    outside the range within clusters at the ends of the\n *                    range\n *\n * @inode - file containing the range\n * @lblk - logical block defining the start of range\n * @len  - length of range in blocks\n *\n * Used after a newly allocated extent is added to the extents status tree.\n * Requires that the extents in the range have either written or unwritten\n * status.  Must be called while holding i_es_lock.\n */\nstatic void __revise_pending(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t     ext4_lblk_t len)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t end = lblk + len - 1;\n\text4_lblk_t first, last;\n\tbool f_del = false, l_del = false;\n\n\tif (len == 0)\n\t\treturn;\n\n\t/*\n\t * Two cases - block range within single cluster and block range\n\t * spanning two or more clusters.  Note that a cluster belonging\n\t * to a range starting and/or ending on a cluster boundary is treated\n\t * as if it does not contain a delayed extent.  The new range may\n\t * have allocated space for previously delayed blocks out to the\n\t * cluster boundary, requiring that any pre-existing pending\n\t * reservation be canceled.  Because this code only looks at blocks\n\t * outside the range, it should revise pending reservations\n\t * correctly even if the extent represented by the range can't be\n\t * inserted in the extents status tree due to ENOSPC.\n\t */\n\n\tif (EXT4_B2C(sbi, lblk) == EXT4_B2C(sbi, end)) {\n\t\tfirst = EXT4_LBLK_CMASK(sbi, lblk);\n\t\tif (first != lblk)\n\t\t\tf_del = __es_scan_range(inode, &ext4_es_is_delonly,\n\t\t\t\t\t\tfirst, lblk - 1);\n\t\tif (f_del) {\n\t\t\t__insert_pending(inode, first);\n\t\t} else {\n\t\t\tlast = EXT4_LBLK_CMASK(sbi, end) +\n\t\t\t       sbi->s_cluster_ratio - 1;\n\t\t\tif (last != end)\n\t\t\t\tl_del = __es_scan_range(inode,\n\t\t\t\t\t\t\t&ext4_es_is_delonly,\n\t\t\t\t\t\t\tend + 1, last);\n\t\t\tif (l_del)\n\t\t\t\t__insert_pending(inode, last);\n\t\t\telse\n\t\t\t\t__remove_pending(inode, last);\n\t\t}\n\t} else {\n\t\tfirst = EXT4_LBLK_CMASK(sbi, lblk);\n\t\tif (first != lblk)\n\t\t\tf_del = __es_scan_range(inode, &ext4_es_is_delonly,\n\t\t\t\t\t\tfirst, lblk - 1);\n\t\tif (f_del)\n\t\t\t__insert_pending(inode, first);\n\t\telse\n\t\t\t__remove_pending(inode, first);\n\n\t\tlast = EXT4_LBLK_CMASK(sbi, end) + sbi->s_cluster_ratio - 1;\n\t\tif (last != end)\n\t\t\tl_del = __es_scan_range(inode, &ext4_es_is_delonly,\n\t\t\t\t\t\tend + 1, last);\n\t\tif (l_del)\n\t\t\t__insert_pending(inode, last);\n\t\telse\n\t\t\t__remove_pending(inode, last);\n\t}\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n *  fs/ext4/extents_status.c\n *\n * Written by Yongqiang Yang <xiaoqiangnk@gmail.com>\n * Modified by\n *\tAllison Henderson <achender@linux.vnet.ibm.com>\n *\tHugh Dickins <hughd@google.com>\n *\tZheng Liu <wenqing.lz@taobao.com>\n *\n * Ext4 extents status tree core functions.\n */\n#include <linux/list_sort.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include \"ext4.h\"\n\n#include <trace/events/ext4.h>\n\n/*\n * According to previous discussion in Ext4 Developer Workshop, we\n * will introduce a new structure called io tree to track all extent\n * status in order to solve some problems that we have met\n * (e.g. Reservation space warning), and provide extent-level locking.\n * Delay extent tree is the first step to achieve this goal.  It is\n * original built by Yongqiang Yang.  At that time it is called delay\n * extent tree, whose goal is only track delayed extents in memory to\n * simplify the implementation of fiemap and bigalloc, and introduce\n * lseek SEEK_DATA/SEEK_HOLE support.  That is why it is still called\n * delay extent tree at the first commit.  But for better understand\n * what it does, it has been rename to extent status tree.\n *\n * Step1:\n * Currently the first step has been done.  All delayed extents are\n * tracked in the tree.  It maintains the delayed extent when a delayed\n * allocation is issued, and the delayed extent is written out or\n * invalidated.  Therefore the implementation of fiemap and bigalloc\n * are simplified, and SEEK_DATA/SEEK_HOLE are introduced.\n *\n * The following comment describes the implemenmtation of extent\n * status tree and future works.\n *\n * Step2:\n * In this step all extent status are tracked by extent status tree.\n * Thus, we can first try to lookup a block mapping in this tree before\n * finding it in extent tree.  Hence, single extent cache can be removed\n * because extent status tree can do a better job.  Extents in status\n * tree are loaded on-demand.  Therefore, the extent status tree may not\n * contain all of the extents in a file.  Meanwhile we define a shrinker\n * to reclaim memory from extent status tree because fragmented extent\n * tree will make status tree cost too much memory.  written/unwritten/-\n * hole extents in the tree will be reclaimed by this shrinker when we\n * are under high memory pressure.  Delayed extents will not be\n * reclimed because fiemap, bigalloc, and seek_data/hole need it.\n */\n\n/*\n * Extent status tree implementation for ext4.\n *\n *\n * ==========================================================================\n * Extent status tree tracks all extent status.\n *\n * 1. Why we need to implement extent status tree?\n *\n * Without extent status tree, ext4 identifies a delayed extent by looking\n * up page cache, this has several deficiencies - complicated, buggy,\n * and inefficient code.\n *\n * FIEMAP, SEEK_HOLE/DATA, bigalloc, and writeout all need to know if a\n * block or a range of blocks are belonged to a delayed extent.\n *\n * Let us have a look at how they do without extent status tree.\n *   --\tFIEMAP\n *\tFIEMAP looks up page cache to identify delayed allocations from holes.\n *\n *   --\tSEEK_HOLE/DATA\n *\tSEEK_HOLE/DATA has the same problem as FIEMAP.\n *\n *   --\tbigalloc\n *\tbigalloc looks up page cache to figure out if a block is\n *\talready under delayed allocation or not to determine whether\n *\tquota reserving is needed for the cluster.\n *\n *   --\twriteout\n *\tWriteout looks up whole page cache to see if a buffer is\n *\tmapped, If there are not very many delayed buffers, then it is\n *\ttime consuming.\n *\n * With extent status tree implementation, FIEMAP, SEEK_HOLE/DATA,\n * bigalloc and writeout can figure out if a block or a range of\n * blocks is under delayed allocation(belonged to a delayed extent) or\n * not by searching the extent tree.\n *\n *\n * ==========================================================================\n * 2. Ext4 extent status tree impelmentation\n *\n *   --\textent\n *\tA extent is a range of blocks which are contiguous logically and\n *\tphysically.  Unlike extent in extent tree, this extent in ext4 is\n *\ta in-memory struct, there is no corresponding on-disk data.  There\n *\tis no limit on length of extent, so an extent can contain as many\n *\tblocks as they are contiguous logically and physically.\n *\n *   --\textent status tree\n *\tEvery inode has an extent status tree and all allocation blocks\n *\tare added to the tree with different status.  The extent in the\n *\ttree are ordered by logical block no.\n *\n *   --\toperations on a extent status tree\n *\tThere are three important operations on a delayed extent tree: find\n *\tnext extent, adding a extent(a range of blocks) and removing a extent.\n *\n *   --\trace on a extent status tree\n *\tExtent status tree is protected by inode->i_es_lock.\n *\n *   --\tmemory consumption\n *      Fragmented extent tree will make extent status tree cost too much\n *      memory.  Hence, we will reclaim written/unwritten/hole extents from\n *      the tree under a heavy memory pressure.\n *\n *\n * ==========================================================================\n * 3. Performance analysis\n *\n *   --\toverhead\n *\t1. There is a cache extent for write access, so if writes are\n *\tnot very random, adding space operaions are in O(1) time.\n *\n *   --\tgain\n *\t2. Code is much simpler, more readable, more maintainable and\n *\tmore efficient.\n *\n *\n * ==========================================================================\n * 4. TODO list\n *\n *   -- Refactor delayed space reservation\n *\n *   -- Extent-level locking\n */\n\nstatic struct kmem_cache *ext4_es_cachep;\nstatic struct kmem_cache *ext4_pending_cachep;\n\nstatic int __es_insert_extent(struct inode *inode, struct extent_status *newes,\n\t\t\t      struct extent_status *prealloc);\nstatic int __es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t      ext4_lblk_t end, int *reserved,\n\t\t\t      struct extent_status *prealloc);\nstatic int es_reclaim_extents(struct ext4_inode_info *ei, int *nr_to_scan);\nstatic int __es_shrink(struct ext4_sb_info *sbi, int nr_to_scan,\n\t\t       struct ext4_inode_info *locked_ei);\nstatic void __revise_pending(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t     ext4_lblk_t len);\n\nint __init ext4_init_es(void)\n{\n\text4_es_cachep = KMEM_CACHE(extent_status, SLAB_RECLAIM_ACCOUNT);\n\tif (ext4_es_cachep == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid ext4_exit_es(void)\n{\n\tkmem_cache_destroy(ext4_es_cachep);\n}\n\nvoid ext4_es_init_tree(struct ext4_es_tree *tree)\n{\n\ttree->root = RB_ROOT;\n\ttree->cache_es = NULL;\n}\n\n#ifdef ES_DEBUG__\nstatic void ext4_es_print_tree(struct inode *inode)\n{\n\tstruct ext4_es_tree *tree;\n\tstruct rb_node *node;\n\n\tprintk(KERN_DEBUG \"status extents for inode %lu:\", inode->i_ino);\n\ttree = &EXT4_I(inode)->i_es_tree;\n\tnode = rb_first(&tree->root);\n\twhile (node) {\n\t\tstruct extent_status *es;\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\tprintk(KERN_DEBUG \" [%u/%u) %llu %x\",\n\t\t       es->es_lblk, es->es_len,\n\t\t       ext4_es_pblock(es), ext4_es_status(es));\n\t\tnode = rb_next(node);\n\t}\n\tprintk(KERN_DEBUG \"\\n\");\n}\n#else\n#define ext4_es_print_tree(inode)\n#endif\n\nstatic inline ext4_lblk_t ext4_es_end(struct extent_status *es)\n{\n\tBUG_ON(es->es_lblk + es->es_len < es->es_lblk);\n\treturn es->es_lblk + es->es_len - 1;\n}\n\n/*\n * search through the tree for an delayed extent with a given offset.  If\n * it can't be found, try to find next extent.\n */\nstatic struct extent_status *__es_tree_search(struct rb_root *root,\n\t\t\t\t\t      ext4_lblk_t lblk)\n{\n\tstruct rb_node *node = root->rb_node;\n\tstruct extent_status *es = NULL;\n\n\twhile (node) {\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\tif (lblk < es->es_lblk)\n\t\t\tnode = node->rb_left;\n\t\telse if (lblk > ext4_es_end(es))\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn es;\n\t}\n\n\tif (es && lblk < es->es_lblk)\n\t\treturn es;\n\n\tif (es && lblk > ext4_es_end(es)) {\n\t\tnode = rb_next(&es->rb_node);\n\t\treturn node ? rb_entry(node, struct extent_status, rb_node) :\n\t\t\t      NULL;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * ext4_es_find_extent_range - find extent with specified status within block\n *                             range or next extent following block range in\n *                             extents status tree\n *\n * @inode - file containing the range\n * @matching_fn - pointer to function that matches extents with desired status\n * @lblk - logical block defining start of range\n * @end - logical block defining end of range\n * @es - extent found, if any\n *\n * Find the first extent within the block range specified by @lblk and @end\n * in the extents status tree that satisfies @matching_fn.  If a match\n * is found, it's returned in @es.  If not, and a matching extent is found\n * beyond the block range, it's returned in @es.  If no match is found, an\n * extent is returned in @es whose es_lblk, es_len, and es_pblk components\n * are 0.\n */\nstatic void __es_find_extent_range(struct inode *inode,\n\t\t\t\t   int (*matching_fn)(struct extent_status *es),\n\t\t\t\t   ext4_lblk_t lblk, ext4_lblk_t end,\n\t\t\t\t   struct extent_status *es)\n{\n\tstruct ext4_es_tree *tree = NULL;\n\tstruct extent_status *es1 = NULL;\n\tstruct rb_node *node;\n\n\tWARN_ON(es == NULL);\n\tWARN_ON(end < lblk);\n\n\ttree = &EXT4_I(inode)->i_es_tree;\n\n\t/* see if the extent has been cached */\n\tes->es_lblk = es->es_len = es->es_pblk = 0;\n\tes1 = READ_ONCE(tree->cache_es);\n\tif (es1 && in_range(lblk, es1->es_lblk, es1->es_len)) {\n\t\tes_debug(\"%u cached by [%u/%u) %llu %x\\n\",\n\t\t\t lblk, es1->es_lblk, es1->es_len,\n\t\t\t ext4_es_pblock(es1), ext4_es_status(es1));\n\t\tgoto out;\n\t}\n\n\tes1 = __es_tree_search(&tree->root, lblk);\n\nout:\n\tif (es1 && !matching_fn(es1)) {\n\t\twhile ((node = rb_next(&es1->rb_node)) != NULL) {\n\t\t\tes1 = rb_entry(node, struct extent_status, rb_node);\n\t\t\tif (es1->es_lblk > end) {\n\t\t\t\tes1 = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (matching_fn(es1))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (es1 && matching_fn(es1)) {\n\t\tWRITE_ONCE(tree->cache_es, es1);\n\t\tes->es_lblk = es1->es_lblk;\n\t\tes->es_len = es1->es_len;\n\t\tes->es_pblk = es1->es_pblk;\n\t}\n\n}\n\n/*\n * Locking for __es_find_extent_range() for external use\n */\nvoid ext4_es_find_extent_range(struct inode *inode,\n\t\t\t       int (*matching_fn)(struct extent_status *es),\n\t\t\t       ext4_lblk_t lblk, ext4_lblk_t end,\n\t\t\t       struct extent_status *es)\n{\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_find_extent_range_enter(inode, lblk);\n\n\tread_lock(&EXT4_I(inode)->i_es_lock);\n\t__es_find_extent_range(inode, matching_fn, lblk, end, es);\n\tread_unlock(&EXT4_I(inode)->i_es_lock);\n\n\ttrace_ext4_es_find_extent_range_exit(inode, es);\n}\n\n/*\n * __es_scan_range - search block range for block with specified status\n *                   in extents status tree\n *\n * @inode - file containing the range\n * @matching_fn - pointer to function that matches extents with desired status\n * @lblk - logical block defining start of range\n * @end - logical block defining end of range\n *\n * Returns true if at least one block in the specified block range satisfies\n * the criterion specified by @matching_fn, and false if not.  If at least\n * one extent has the specified status, then there is at least one block\n * in the cluster with that status.  Should only be called by code that has\n * taken i_es_lock.\n */\nstatic bool __es_scan_range(struct inode *inode,\n\t\t\t    int (*matching_fn)(struct extent_status *es),\n\t\t\t    ext4_lblk_t start, ext4_lblk_t end)\n{\n\tstruct extent_status es;\n\n\t__es_find_extent_range(inode, matching_fn, start, end, &es);\n\tif (es.es_len == 0)\n\t\treturn false;   /* no matching extent in the tree */\n\telse if (es.es_lblk <= start &&\n\t\t start < es.es_lblk + es.es_len)\n\t\treturn true;\n\telse if (start <= es.es_lblk && es.es_lblk <= end)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n/*\n * Locking for __es_scan_range() for external use\n */\nbool ext4_es_scan_range(struct inode *inode,\n\t\t\tint (*matching_fn)(struct extent_status *es),\n\t\t\text4_lblk_t lblk, ext4_lblk_t end)\n{\n\tbool ret;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn false;\n\n\tread_lock(&EXT4_I(inode)->i_es_lock);\n\tret = __es_scan_range(inode, matching_fn, lblk, end);\n\tread_unlock(&EXT4_I(inode)->i_es_lock);\n\n\treturn ret;\n}\n\n/*\n * __es_scan_clu - search cluster for block with specified status in\n *                 extents status tree\n *\n * @inode - file containing the cluster\n * @matching_fn - pointer to function that matches extents with desired status\n * @lblk - logical block in cluster to be searched\n *\n * Returns true if at least one extent in the cluster containing @lblk\n * satisfies the criterion specified by @matching_fn, and false if not.  If at\n * least one extent has the specified status, then there is at least one block\n * in the cluster with that status.  Should only be called by code that has\n * taken i_es_lock.\n */\nstatic bool __es_scan_clu(struct inode *inode,\n\t\t\t  int (*matching_fn)(struct extent_status *es),\n\t\t\t  ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t lblk_start, lblk_end;\n\n\tlblk_start = EXT4_LBLK_CMASK(sbi, lblk);\n\tlblk_end = lblk_start + sbi->s_cluster_ratio - 1;\n\n\treturn __es_scan_range(inode, matching_fn, lblk_start, lblk_end);\n}\n\n/*\n * Locking for __es_scan_clu() for external use\n */\nbool ext4_es_scan_clu(struct inode *inode,\n\t\t      int (*matching_fn)(struct extent_status *es),\n\t\t      ext4_lblk_t lblk)\n{\n\tbool ret;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn false;\n\n\tread_lock(&EXT4_I(inode)->i_es_lock);\n\tret = __es_scan_clu(inode, matching_fn, lblk);\n\tread_unlock(&EXT4_I(inode)->i_es_lock);\n\n\treturn ret;\n}\n\nstatic void ext4_es_list_add(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\tif (!list_empty(&ei->i_es_list))\n\t\treturn;\n\n\tspin_lock(&sbi->s_es_lock);\n\tif (list_empty(&ei->i_es_list)) {\n\t\tlist_add_tail(&ei->i_es_list, &sbi->s_es_list);\n\t\tsbi->s_es_nr_inode++;\n\t}\n\tspin_unlock(&sbi->s_es_lock);\n}\n\nstatic void ext4_es_list_del(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\tspin_lock(&sbi->s_es_lock);\n\tif (!list_empty(&ei->i_es_list)) {\n\t\tlist_del_init(&ei->i_es_list);\n\t\tsbi->s_es_nr_inode--;\n\t\tWARN_ON_ONCE(sbi->s_es_nr_inode < 0);\n\t}\n\tspin_unlock(&sbi->s_es_lock);\n}\n\n/*\n * Returns true if we cannot fail to allocate memory for this extent_status\n * entry and cannot reclaim it until its status changes.\n */\nstatic inline bool ext4_es_must_keep(struct extent_status *es)\n{\n\t/* fiemap, bigalloc, and seek_data/hole need to use it. */\n\tif (ext4_es_is_delayed(es))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline struct extent_status *__es_alloc_extent(bool nofail)\n{\n\tif (!nofail)\n\t\treturn kmem_cache_alloc(ext4_es_cachep, GFP_ATOMIC);\n\n\treturn kmem_cache_zalloc(ext4_es_cachep, GFP_KERNEL | __GFP_NOFAIL);\n}\n\nstatic void ext4_es_init_extent(struct inode *inode, struct extent_status *es,\n\t\text4_lblk_t lblk, ext4_lblk_t len, ext4_fsblk_t pblk)\n{\n\tes->es_lblk = lblk;\n\tes->es_len = len;\n\tes->es_pblk = pblk;\n\n\t/* We never try to reclaim a must kept extent, so we don't count it. */\n\tif (!ext4_es_must_keep(es)) {\n\t\tif (!EXT4_I(inode)->i_es_shk_nr++)\n\t\t\text4_es_list_add(inode);\n\t\tpercpu_counter_inc(&EXT4_SB(inode->i_sb)->\n\t\t\t\t\ts_es_stats.es_stats_shk_cnt);\n\t}\n\n\tEXT4_I(inode)->i_es_all_nr++;\n\tpercpu_counter_inc(&EXT4_SB(inode->i_sb)->s_es_stats.es_stats_all_cnt);\n}\n\nstatic inline void __es_free_extent(struct extent_status *es)\n{\n\tkmem_cache_free(ext4_es_cachep, es);\n}\n\nstatic void ext4_es_free_extent(struct inode *inode, struct extent_status *es)\n{\n\tEXT4_I(inode)->i_es_all_nr--;\n\tpercpu_counter_dec(&EXT4_SB(inode->i_sb)->s_es_stats.es_stats_all_cnt);\n\n\t/* Decrease the shrink counter when we can reclaim the extent. */\n\tif (!ext4_es_must_keep(es)) {\n\t\tBUG_ON(EXT4_I(inode)->i_es_shk_nr == 0);\n\t\tif (!--EXT4_I(inode)->i_es_shk_nr)\n\t\t\text4_es_list_del(inode);\n\t\tpercpu_counter_dec(&EXT4_SB(inode->i_sb)->\n\t\t\t\t\ts_es_stats.es_stats_shk_cnt);\n\t}\n\n\t__es_free_extent(es);\n}\n\n/*\n * Check whether or not two extents can be merged\n * Condition:\n *  - logical block number is contiguous\n *  - physical block number is contiguous\n *  - status is equal\n */\nstatic int ext4_es_can_be_merged(struct extent_status *es1,\n\t\t\t\t struct extent_status *es2)\n{\n\tif (ext4_es_type(es1) != ext4_es_type(es2))\n\t\treturn 0;\n\n\tif (((__u64) es1->es_len) + es2->es_len > EXT_MAX_BLOCKS) {\n\t\tpr_warn(\"ES assertion failed when merging extents. \"\n\t\t\t\"The sum of lengths of es1 (%d) and es2 (%d) \"\n\t\t\t\"is bigger than allowed file size (%d)\\n\",\n\t\t\tes1->es_len, es2->es_len, EXT_MAX_BLOCKS);\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n\n\tif (((__u64) es1->es_lblk) + es1->es_len != es2->es_lblk)\n\t\treturn 0;\n\n\tif ((ext4_es_is_written(es1) || ext4_es_is_unwritten(es1)) &&\n\t    (ext4_es_pblock(es1) + es1->es_len == ext4_es_pblock(es2)))\n\t\treturn 1;\n\n\tif (ext4_es_is_hole(es1))\n\t\treturn 1;\n\n\t/* we need to check delayed extent is without unwritten status */\n\tif (ext4_es_is_delayed(es1) && !ext4_es_is_unwritten(es1))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic struct extent_status *\next4_es_try_to_merge_left(struct inode *inode, struct extent_status *es)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct extent_status *es1;\n\tstruct rb_node *node;\n\n\tnode = rb_prev(&es->rb_node);\n\tif (!node)\n\t\treturn es;\n\n\tes1 = rb_entry(node, struct extent_status, rb_node);\n\tif (ext4_es_can_be_merged(es1, es)) {\n\t\tes1->es_len += es->es_len;\n\t\tif (ext4_es_is_referenced(es))\n\t\t\text4_es_set_referenced(es1);\n\t\trb_erase(&es->rb_node, &tree->root);\n\t\text4_es_free_extent(inode, es);\n\t\tes = es1;\n\t}\n\n\treturn es;\n}\n\nstatic struct extent_status *\next4_es_try_to_merge_right(struct inode *inode, struct extent_status *es)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct extent_status *es1;\n\tstruct rb_node *node;\n\n\tnode = rb_next(&es->rb_node);\n\tif (!node)\n\t\treturn es;\n\n\tes1 = rb_entry(node, struct extent_status, rb_node);\n\tif (ext4_es_can_be_merged(es, es1)) {\n\t\tes->es_len += es1->es_len;\n\t\tif (ext4_es_is_referenced(es1))\n\t\t\text4_es_set_referenced(es);\n\t\trb_erase(node, &tree->root);\n\t\text4_es_free_extent(inode, es1);\n\t}\n\n\treturn es;\n}\n\n#ifdef ES_AGGRESSIVE_TEST\n#include \"ext4_extents.h\"\t/* Needed when ES_AGGRESSIVE_TEST is defined */\n\nstatic void ext4_es_insert_extent_ext_check(struct inode *inode,\n\t\t\t\t\t    struct extent_status *es)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\text4_fsblk_t ee_start;\n\tunsigned short ee_len;\n\tint depth, ee_status, es_status;\n\n\tpath = ext4_find_extent(inode, es->es_lblk, NULL, EXT4_EX_NOCACHE);\n\tif (IS_ERR(path))\n\t\treturn;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\n\tif (ex) {\n\n\t\tee_block = le32_to_cpu(ex->ee_block);\n\t\tee_start = ext4_ext_pblock(ex);\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\n\t\tee_status = ext4_ext_is_unwritten(ex) ? 1 : 0;\n\t\tes_status = ext4_es_is_unwritten(es) ? 1 : 0;\n\n\t\t/*\n\t\t * Make sure ex and es are not overlap when we try to insert\n\t\t * a delayed/hole extent.\n\t\t */\n\t\tif (!ext4_es_is_written(es) && !ext4_es_is_unwritten(es)) {\n\t\t\tif (in_range(es->es_lblk, ee_block, ee_len)) {\n\t\t\t\tpr_warn(\"ES insert assertion failed for \"\n\t\t\t\t\t\"inode: %lu we can find an extent \"\n\t\t\t\t\t\"at block [%d/%d/%llu/%c], but we \"\n\t\t\t\t\t\"want to add a delayed/hole extent \"\n\t\t\t\t\t\"[%d/%d/%llu/%x]\\n\",\n\t\t\t\t\tinode->i_ino, ee_block, ee_len,\n\t\t\t\t\tee_start, ee_status ? 'u' : 'w',\n\t\t\t\t\tes->es_lblk, es->es_len,\n\t\t\t\t\text4_es_pblock(es), ext4_es_status(es));\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * We don't check ee_block == es->es_lblk, etc. because es\n\t\t * might be a part of whole extent, vice versa.\n\t\t */\n\t\tif (es->es_lblk < ee_block ||\n\t\t    ext4_es_pblock(es) != ee_start + es->es_lblk - ee_block) {\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"ex_status [%d/%d/%llu/%c] != \"\n\t\t\t\t\"es_status [%d/%d/%llu/%c]\\n\", inode->i_ino,\n\t\t\t\tee_block, ee_len, ee_start,\n\t\t\t\tee_status ? 'u' : 'w', es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), es_status ? 'u' : 'w');\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (ee_status ^ es_status) {\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"ex_status [%d/%d/%llu/%c] != \"\n\t\t\t\t\"es_status [%d/%d/%llu/%c]\\n\", inode->i_ino,\n\t\t\t\tee_block, ee_len, ee_start,\n\t\t\t\tee_status ? 'u' : 'w', es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), es_status ? 'u' : 'w');\n\t\t}\n\t} else {\n\t\t/*\n\t\t * We can't find an extent on disk.  So we need to make sure\n\t\t * that we don't want to add an written/unwritten extent.\n\t\t */\n\t\tif (!ext4_es_is_delayed(es) && !ext4_es_is_hole(es)) {\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"can't find an extent at block %d but we want \"\n\t\t\t\t\"to add a written/unwritten extent \"\n\t\t\t\t\"[%d/%d/%llu/%x]\\n\", inode->i_ino,\n\t\t\t\tes->es_lblk, es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), ext4_es_status(es));\n\t\t}\n\t}\nout:\n\text4_free_ext_path(path);\n}\n\nstatic void ext4_es_insert_extent_ind_check(struct inode *inode,\n\t\t\t\t\t    struct extent_status *es)\n{\n\tstruct ext4_map_blocks map;\n\tint retval;\n\n\t/*\n\t * Here we call ext4_ind_map_blocks to lookup a block mapping because\n\t * 'Indirect' structure is defined in indirect.c.  So we couldn't\n\t * access direct/indirect tree from outside.  It is too dirty to define\n\t * this function in indirect.c file.\n\t */\n\n\tmap.m_lblk = es->es_lblk;\n\tmap.m_len = es->es_len;\n\n\tretval = ext4_ind_map_blocks(NULL, inode, &map, 0);\n\tif (retval > 0) {\n\t\tif (ext4_es_is_delayed(es) || ext4_es_is_hole(es)) {\n\t\t\t/*\n\t\t\t * We want to add a delayed/hole extent but this\n\t\t\t * block has been allocated.\n\t\t\t */\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"We can find blocks but we want to add a \"\n\t\t\t\t\"delayed/hole extent [%d/%d/%llu/%x]\\n\",\n\t\t\t\tinode->i_ino, es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), ext4_es_status(es));\n\t\t\treturn;\n\t\t} else if (ext4_es_is_written(es)) {\n\t\t\tif (retval != es->es_len) {\n\t\t\t\tpr_warn(\"ES insert assertion failed for \"\n\t\t\t\t\t\"inode: %lu retval %d != es_len %d\\n\",\n\t\t\t\t\tinode->i_ino, retval, es->es_len);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tif (map.m_pblk != ext4_es_pblock(es)) {\n\t\t\t\tpr_warn(\"ES insert assertion failed for \"\n\t\t\t\t\t\"inode: %lu m_pblk %llu != \"\n\t\t\t\t\t\"es_pblk %llu\\n\",\n\t\t\t\t\tinode->i_ino, map.m_pblk,\n\t\t\t\t\text4_es_pblock(es));\n\t\t\t\treturn;\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * We don't need to check unwritten extent because\n\t\t\t * indirect-based file doesn't have it.\n\t\t\t */\n\t\t\tBUG();\n\t\t}\n\t} else if (retval == 0) {\n\t\tif (ext4_es_is_written(es)) {\n\t\t\tpr_warn(\"ES insert assertion failed for inode: %lu \"\n\t\t\t\t\"We can't find the block but we want to add \"\n\t\t\t\t\"a written extent [%d/%d/%llu/%x]\\n\",\n\t\t\t\tinode->i_ino, es->es_lblk, es->es_len,\n\t\t\t\text4_es_pblock(es), ext4_es_status(es));\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic inline void ext4_es_insert_extent_check(struct inode *inode,\n\t\t\t\t\t       struct extent_status *es)\n{\n\t/*\n\t * We don't need to worry about the race condition because\n\t * caller takes i_data_sem locking.\n\t */\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode)->i_data_sem));\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\text4_es_insert_extent_ext_check(inode, es);\n\telse\n\t\text4_es_insert_extent_ind_check(inode, es);\n}\n#else\nstatic inline void ext4_es_insert_extent_check(struct inode *inode,\n\t\t\t\t\t       struct extent_status *es)\n{\n}\n#endif\n\nstatic int __es_insert_extent(struct inode *inode, struct extent_status *newes,\n\t\t\t      struct extent_status *prealloc)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct rb_node **p = &tree->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct extent_status *es;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tes = rb_entry(parent, struct extent_status, rb_node);\n\n\t\tif (newes->es_lblk < es->es_lblk) {\n\t\t\tif (ext4_es_can_be_merged(newes, es)) {\n\t\t\t\t/*\n\t\t\t\t * Here we can modify es_lblk directly\n\t\t\t\t * because it isn't overlapped.\n\t\t\t\t */\n\t\t\t\tes->es_lblk = newes->es_lblk;\n\t\t\t\tes->es_len += newes->es_len;\n\t\t\t\tif (ext4_es_is_written(es) ||\n\t\t\t\t    ext4_es_is_unwritten(es))\n\t\t\t\t\text4_es_store_pblock(es,\n\t\t\t\t\t\t\t     newes->es_pblk);\n\t\t\t\tes = ext4_es_try_to_merge_left(inode, es);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (newes->es_lblk > ext4_es_end(es)) {\n\t\t\tif (ext4_es_can_be_merged(es, newes)) {\n\t\t\t\tes->es_len += newes->es_len;\n\t\t\t\tes = ext4_es_try_to_merge_right(inode, es);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\tBUG();\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (prealloc)\n\t\tes = prealloc;\n\telse\n\t\tes = __es_alloc_extent(false);\n\tif (!es)\n\t\treturn -ENOMEM;\n\text4_es_init_extent(inode, es, newes->es_lblk, newes->es_len,\n\t\t\t    newes->es_pblk);\n\n\trb_link_node(&es->rb_node, parent, p);\n\trb_insert_color(&es->rb_node, &tree->root);\n\nout:\n\ttree->cache_es = es;\n\treturn 0;\n}\n\n/*\n * ext4_es_insert_extent() adds information to an inode's extent\n * status tree.\n */\nvoid ext4_es_insert_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len, ext4_fsblk_t pblk,\n\t\t\t   unsigned int status)\n{\n\tstruct extent_status newes;\n\text4_lblk_t end = lblk + len - 1;\n\tint err1 = 0;\n\tint err2 = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct extent_status *es1 = NULL;\n\tstruct extent_status *es2 = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\tes_debug(\"add [%u/%u) %llu %x to extent status tree of inode %lu\\n\",\n\t\t lblk, len, pblk, status, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tBUG_ON(end < lblk);\n\n\tif ((status & EXTENT_STATUS_DELAYED) &&\n\t    (status & EXTENT_STATUS_WRITTEN)) {\n\t\text4_warning(inode->i_sb, \"Inserting extent [%u/%u] as \"\n\t\t\t\t\" delayed and written which can potentially \"\n\t\t\t\t\" cause data loss.\", lblk, len);\n\t\tWARN_ON(1);\n\t}\n\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = len;\n\text4_es_store_pblock_status(&newes, pblk, status);\n\ttrace_ext4_es_insert_extent(inode, &newes);\n\n\text4_es_insert_extent_check(inode, &newes);\n\nretry:\n\tif (err1 && !es1)\n\t\tes1 = __es_alloc_extent(true);\n\tif ((err1 || err2) && !es2)\n\t\tes2 = __es_alloc_extent(true);\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\n\terr1 = __es_remove_extent(inode, lblk, end, NULL, es1);\n\tif (err1 != 0)\n\t\tgoto error;\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es1) {\n\t\tif (!es1->es_len)\n\t\t\t__es_free_extent(es1);\n\t\tes1 = NULL;\n\t}\n\n\terr2 = __es_insert_extent(inode, &newes, es2);\n\tif (err2 == -ENOMEM && !ext4_es_must_keep(&newes))\n\t\terr2 = 0;\n\tif (err2 != 0)\n\t\tgoto error;\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es2) {\n\t\tif (!es2->es_len)\n\t\t\t__es_free_extent(es2);\n\t\tes2 = NULL;\n\t}\n\n\tif (sbi->s_cluster_ratio > 1 && test_opt(inode->i_sb, DELALLOC) &&\n\t    (status & EXTENT_STATUS_WRITTEN ||\n\t     status & EXTENT_STATUS_UNWRITTEN))\n\t\t__revise_pending(inode, lblk, len);\nerror:\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err1 || err2)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\treturn;\n}\n\n/*\n * ext4_es_cache_extent() inserts information into the extent status\n * tree if and only if there isn't information about the range in\n * question already.\n */\nvoid ext4_es_cache_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t  ext4_lblk_t len, ext4_fsblk_t pblk,\n\t\t\t  unsigned int status)\n{\n\tstruct extent_status *es;\n\tstruct extent_status newes;\n\text4_lblk_t end = lblk + len - 1;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = len;\n\text4_es_store_pblock_status(&newes, pblk, status);\n\ttrace_ext4_es_cache_extent(inode, &newes);\n\n\tif (!len)\n\t\treturn;\n\n\tBUG_ON(end < lblk);\n\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\n\tes = __es_tree_search(&EXT4_I(inode)->i_es_tree.root, lblk);\n\tif (!es || es->es_lblk > end)\n\t\t__es_insert_extent(inode, &newes, NULL);\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n}\n\n/*\n * ext4_es_lookup_extent() looks up an extent in extent status tree.\n *\n * ext4_es_lookup_extent is called by ext4_map_blocks/ext4_da_map_blocks.\n *\n * Return: 1 on found, 0 on not\n */\nint ext4_es_lookup_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t  ext4_lblk_t *next_lblk,\n\t\t\t  struct extent_status *es)\n{\n\tstruct ext4_es_tree *tree;\n\tstruct ext4_es_stats *stats;\n\tstruct extent_status *es1 = NULL;\n\tstruct rb_node *node;\n\tint found = 0;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn 0;\n\n\ttrace_ext4_es_lookup_extent_enter(inode, lblk);\n\tes_debug(\"lookup extent in block %u\\n\", lblk);\n\n\ttree = &EXT4_I(inode)->i_es_tree;\n\tread_lock(&EXT4_I(inode)->i_es_lock);\n\n\t/* find extent in cache firstly */\n\tes->es_lblk = es->es_len = es->es_pblk = 0;\n\tes1 = READ_ONCE(tree->cache_es);\n\tif (es1 && in_range(lblk, es1->es_lblk, es1->es_len)) {\n\t\tes_debug(\"%u cached by [%u/%u)\\n\",\n\t\t\t lblk, es1->es_lblk, es1->es_len);\n\t\tfound = 1;\n\t\tgoto out;\n\t}\n\n\tnode = tree->root.rb_node;\n\twhile (node) {\n\t\tes1 = rb_entry(node, struct extent_status, rb_node);\n\t\tif (lblk < es1->es_lblk)\n\t\t\tnode = node->rb_left;\n\t\telse if (lblk > ext4_es_end(es1))\n\t\t\tnode = node->rb_right;\n\t\telse {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tstats = &EXT4_SB(inode->i_sb)->s_es_stats;\n\tif (found) {\n\t\tBUG_ON(!es1);\n\t\tes->es_lblk = es1->es_lblk;\n\t\tes->es_len = es1->es_len;\n\t\tes->es_pblk = es1->es_pblk;\n\t\tif (!ext4_es_is_referenced(es1))\n\t\t\text4_es_set_referenced(es1);\n\t\tpercpu_counter_inc(&stats->es_stats_cache_hits);\n\t\tif (next_lblk) {\n\t\t\tnode = rb_next(&es1->rb_node);\n\t\t\tif (node) {\n\t\t\t\tes1 = rb_entry(node, struct extent_status,\n\t\t\t\t\t       rb_node);\n\t\t\t\t*next_lblk = es1->es_lblk;\n\t\t\t} else\n\t\t\t\t*next_lblk = 0;\n\t\t}\n\t} else {\n\t\tpercpu_counter_inc(&stats->es_stats_cache_misses);\n\t}\n\n\tread_unlock(&EXT4_I(inode)->i_es_lock);\n\n\ttrace_ext4_es_lookup_extent_exit(inode, es, found);\n\treturn found;\n}\n\nstruct rsvd_count {\n\tint ndelonly;\n\tbool first_do_lblk_found;\n\text4_lblk_t first_do_lblk;\n\text4_lblk_t last_do_lblk;\n\tstruct extent_status *left_es;\n\tbool partial;\n\text4_lblk_t lclu;\n};\n\n/*\n * init_rsvd - initialize reserved count data before removing block range\n *\t       in file from extent status tree\n *\n * @inode - file containing range\n * @lblk - first block in range\n * @es - pointer to first extent in range\n * @rc - pointer to reserved count data\n *\n * Assumes es is not NULL\n */\nstatic void init_rsvd(struct inode *inode, ext4_lblk_t lblk,\n\t\t      struct extent_status *es, struct rsvd_count *rc)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct rb_node *node;\n\n\trc->ndelonly = 0;\n\n\t/*\n\t * for bigalloc, note the first delonly block in the range has not\n\t * been found, record the extent containing the block to the left of\n\t * the region to be removed, if any, and note that there's no partial\n\t * cluster to track\n\t */\n\tif (sbi->s_cluster_ratio > 1) {\n\t\trc->first_do_lblk_found = false;\n\t\tif (lblk > es->es_lblk) {\n\t\t\trc->left_es = es;\n\t\t} else {\n\t\t\tnode = rb_prev(&es->rb_node);\n\t\t\trc->left_es = node ? rb_entry(node,\n\t\t\t\t\t\t      struct extent_status,\n\t\t\t\t\t\t      rb_node) : NULL;\n\t\t}\n\t\trc->partial = false;\n\t}\n}\n\n/*\n * count_rsvd - count the clusters containing delayed and not unwritten\n *\t\t(delonly) blocks in a range within an extent and add to\n *\t        the running tally in rsvd_count\n *\n * @inode - file containing extent\n * @lblk - first block in range\n * @len - length of range in blocks\n * @es - pointer to extent containing clusters to be counted\n * @rc - pointer to reserved count data\n *\n * Tracks partial clusters found at the beginning and end of extents so\n * they aren't overcounted when they span adjacent extents\n */\nstatic void count_rsvd(struct inode *inode, ext4_lblk_t lblk, long len,\n\t\t       struct extent_status *es, struct rsvd_count *rc)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t i, end, nclu;\n\n\tif (!ext4_es_is_delonly(es))\n\t\treturn;\n\n\tWARN_ON(len <= 0);\n\n\tif (sbi->s_cluster_ratio == 1) {\n\t\trc->ndelonly += (int) len;\n\t\treturn;\n\t}\n\n\t/* bigalloc */\n\n\ti = (lblk < es->es_lblk) ? es->es_lblk : lblk;\n\tend = lblk + (ext4_lblk_t) len - 1;\n\tend = (end > ext4_es_end(es)) ? ext4_es_end(es) : end;\n\n\t/* record the first block of the first delonly extent seen */\n\tif (!rc->first_do_lblk_found) {\n\t\trc->first_do_lblk = i;\n\t\trc->first_do_lblk_found = true;\n\t}\n\n\t/* update the last lblk in the region seen so far */\n\trc->last_do_lblk = end;\n\n\t/*\n\t * if we're tracking a partial cluster and the current extent\n\t * doesn't start with it, count it and stop tracking\n\t */\n\tif (rc->partial && (rc->lclu != EXT4_B2C(sbi, i))) {\n\t\trc->ndelonly++;\n\t\trc->partial = false;\n\t}\n\n\t/*\n\t * if the first cluster doesn't start on a cluster boundary but\n\t * ends on one, count it\n\t */\n\tif (EXT4_LBLK_COFF(sbi, i) != 0) {\n\t\tif (end >= EXT4_LBLK_CFILL(sbi, i)) {\n\t\t\trc->ndelonly++;\n\t\t\trc->partial = false;\n\t\t\ti = EXT4_LBLK_CFILL(sbi, i) + 1;\n\t\t}\n\t}\n\n\t/*\n\t * if the current cluster starts on a cluster boundary, count the\n\t * number of whole delonly clusters in the extent\n\t */\n\tif ((i + sbi->s_cluster_ratio - 1) <= end) {\n\t\tnclu = (end - i + 1) >> sbi->s_cluster_bits;\n\t\trc->ndelonly += nclu;\n\t\ti += nclu << sbi->s_cluster_bits;\n\t}\n\n\t/*\n\t * start tracking a partial cluster if there's a partial at the end\n\t * of the current extent and we're not already tracking one\n\t */\n\tif (!rc->partial && i <= end) {\n\t\trc->partial = true;\n\t\trc->lclu = EXT4_B2C(sbi, i);\n\t}\n}\n\n/*\n * __pr_tree_search - search for a pending cluster reservation\n *\n * @root - root of pending reservation tree\n * @lclu - logical cluster to search for\n *\n * Returns the pending reservation for the cluster identified by @lclu\n * if found.  If not, returns a reservation for the next cluster if any,\n * and if not, returns NULL.\n */\nstatic struct pending_reservation *__pr_tree_search(struct rb_root *root,\n\t\t\t\t\t\t    ext4_lblk_t lclu)\n{\n\tstruct rb_node *node = root->rb_node;\n\tstruct pending_reservation *pr = NULL;\n\n\twhile (node) {\n\t\tpr = rb_entry(node, struct pending_reservation, rb_node);\n\t\tif (lclu < pr->lclu)\n\t\t\tnode = node->rb_left;\n\t\telse if (lclu > pr->lclu)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn pr;\n\t}\n\tif (pr && lclu < pr->lclu)\n\t\treturn pr;\n\tif (pr && lclu > pr->lclu) {\n\t\tnode = rb_next(&pr->rb_node);\n\t\treturn node ? rb_entry(node, struct pending_reservation,\n\t\t\t\t       rb_node) : NULL;\n\t}\n\treturn NULL;\n}\n\n/*\n * get_rsvd - calculates and returns the number of cluster reservations to be\n *\t      released when removing a block range from the extent status tree\n *\t      and releases any pending reservations within the range\n *\n * @inode - file containing block range\n * @end - last block in range\n * @right_es - pointer to extent containing next block beyond end or NULL\n * @rc - pointer to reserved count data\n *\n * The number of reservations to be released is equal to the number of\n * clusters containing delayed and not unwritten (delonly) blocks within\n * the range, minus the number of clusters still containing delonly blocks\n * at the ends of the range, and minus the number of pending reservations\n * within the range.\n */\nstatic unsigned int get_rsvd(struct inode *inode, ext4_lblk_t end,\n\t\t\t     struct extent_status *right_es,\n\t\t\t     struct rsvd_count *rc)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct pending_reservation *pr;\n\tstruct ext4_pending_tree *tree = &EXT4_I(inode)->i_pending_tree;\n\tstruct rb_node *node;\n\text4_lblk_t first_lclu, last_lclu;\n\tbool left_delonly, right_delonly, count_pending;\n\tstruct extent_status *es;\n\n\tif (sbi->s_cluster_ratio > 1) {\n\t\t/* count any remaining partial cluster */\n\t\tif (rc->partial)\n\t\t\trc->ndelonly++;\n\n\t\tif (rc->ndelonly == 0)\n\t\t\treturn 0;\n\n\t\tfirst_lclu = EXT4_B2C(sbi, rc->first_do_lblk);\n\t\tlast_lclu = EXT4_B2C(sbi, rc->last_do_lblk);\n\n\t\t/*\n\t\t * decrease the delonly count by the number of clusters at the\n\t\t * ends of the range that still contain delonly blocks -\n\t\t * these clusters still need to be reserved\n\t\t */\n\t\tleft_delonly = right_delonly = false;\n\n\t\tes = rc->left_es;\n\t\twhile (es && ext4_es_end(es) >=\n\t\t       EXT4_LBLK_CMASK(sbi, rc->first_do_lblk)) {\n\t\t\tif (ext4_es_is_delonly(es)) {\n\t\t\t\trc->ndelonly--;\n\t\t\t\tleft_delonly = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tnode = rb_prev(&es->rb_node);\n\t\t\tif (!node)\n\t\t\t\tbreak;\n\t\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\t}\n\t\tif (right_es && (!left_delonly || first_lclu != last_lclu)) {\n\t\t\tif (end < ext4_es_end(right_es)) {\n\t\t\t\tes = right_es;\n\t\t\t} else {\n\t\t\t\tnode = rb_next(&right_es->rb_node);\n\t\t\t\tes = node ? rb_entry(node, struct extent_status,\n\t\t\t\t\t\t     rb_node) : NULL;\n\t\t\t}\n\t\t\twhile (es && es->es_lblk <=\n\t\t\t       EXT4_LBLK_CFILL(sbi, rc->last_do_lblk)) {\n\t\t\t\tif (ext4_es_is_delonly(es)) {\n\t\t\t\t\trc->ndelonly--;\n\t\t\t\t\tright_delonly = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode = rb_next(&es->rb_node);\n\t\t\t\tif (!node)\n\t\t\t\t\tbreak;\n\t\t\t\tes = rb_entry(node, struct extent_status,\n\t\t\t\t\t      rb_node);\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Determine the block range that should be searched for\n\t\t * pending reservations, if any.  Clusters on the ends of the\n\t\t * original removed range containing delonly blocks are\n\t\t * excluded.  They've already been accounted for and it's not\n\t\t * possible to determine if an associated pending reservation\n\t\t * should be released with the information available in the\n\t\t * extents status tree.\n\t\t */\n\t\tif (first_lclu == last_lclu) {\n\t\t\tif (left_delonly | right_delonly)\n\t\t\t\tcount_pending = false;\n\t\t\telse\n\t\t\t\tcount_pending = true;\n\t\t} else {\n\t\t\tif (left_delonly)\n\t\t\t\tfirst_lclu++;\n\t\t\tif (right_delonly)\n\t\t\t\tlast_lclu--;\n\t\t\tif (first_lclu <= last_lclu)\n\t\t\t\tcount_pending = true;\n\t\t\telse\n\t\t\t\tcount_pending = false;\n\t\t}\n\n\t\t/*\n\t\t * a pending reservation found between first_lclu and last_lclu\n\t\t * represents an allocated cluster that contained at least one\n\t\t * delonly block, so the delonly total must be reduced by one\n\t\t * for each pending reservation found and released\n\t\t */\n\t\tif (count_pending) {\n\t\t\tpr = __pr_tree_search(&tree->root, first_lclu);\n\t\t\twhile (pr && pr->lclu <= last_lclu) {\n\t\t\t\trc->ndelonly--;\n\t\t\t\tnode = rb_next(&pr->rb_node);\n\t\t\t\trb_erase(&pr->rb_node, &tree->root);\n\t\t\t\tkmem_cache_free(ext4_pending_cachep, pr);\n\t\t\t\tif (!node)\n\t\t\t\t\tbreak;\n\t\t\t\tpr = rb_entry(node, struct pending_reservation,\n\t\t\t\t\t      rb_node);\n\t\t\t}\n\t\t}\n\t}\n\treturn rc->ndelonly;\n}\n\n\n/*\n * __es_remove_extent - removes block range from extent status tree\n *\n * @inode - file containing range\n * @lblk - first block in range\n * @end - last block in range\n * @reserved - number of cluster reservations released\n * @prealloc - pre-allocated es to avoid memory allocation failures\n *\n * If @reserved is not NULL and delayed allocation is enabled, counts\n * block/cluster reservations freed by removing range and if bigalloc\n * enabled cancels pending reservations as needed. Returns 0 on success,\n * error code on failure.\n */\nstatic int __es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t      ext4_lblk_t end, int *reserved,\n\t\t\t      struct extent_status *prealloc)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct rb_node *node;\n\tstruct extent_status *es;\n\tstruct extent_status orig_es;\n\text4_lblk_t len1, len2;\n\text4_fsblk_t block;\n\tint err = 0;\n\tbool count_reserved = true;\n\tstruct rsvd_count rc;\n\n\tif (reserved == NULL || !test_opt(inode->i_sb, DELALLOC))\n\t\tcount_reserved = false;\n\n\tes = __es_tree_search(&tree->root, lblk);\n\tif (!es)\n\t\tgoto out;\n\tif (es->es_lblk > end)\n\t\tgoto out;\n\n\t/* Simply invalidate cache_es. */\n\ttree->cache_es = NULL;\n\tif (count_reserved)\n\t\tinit_rsvd(inode, lblk, es, &rc);\n\n\torig_es.es_lblk = es->es_lblk;\n\torig_es.es_len = es->es_len;\n\torig_es.es_pblk = es->es_pblk;\n\n\tlen1 = lblk > es->es_lblk ? lblk - es->es_lblk : 0;\n\tlen2 = ext4_es_end(es) > end ? ext4_es_end(es) - end : 0;\n\tif (len1 > 0)\n\t\tes->es_len = len1;\n\tif (len2 > 0) {\n\t\tif (len1 > 0) {\n\t\t\tstruct extent_status newes;\n\n\t\t\tnewes.es_lblk = end + 1;\n\t\t\tnewes.es_len = len2;\n\t\t\tblock = 0x7FDEADBEEFULL;\n\t\t\tif (ext4_es_is_written(&orig_es) ||\n\t\t\t    ext4_es_is_unwritten(&orig_es))\n\t\t\t\tblock = ext4_es_pblock(&orig_es) +\n\t\t\t\t\torig_es.es_len - len2;\n\t\t\text4_es_store_pblock_status(&newes, block,\n\t\t\t\t\t\t    ext4_es_status(&orig_es));\n\t\t\terr = __es_insert_extent(inode, &newes, prealloc);\n\t\t\tif (err) {\n\t\t\t\tif (!ext4_es_must_keep(&newes))\n\t\t\t\t\treturn 0;\n\n\t\t\t\tes->es_lblk = orig_es.es_lblk;\n\t\t\t\tes->es_len = orig_es.es_len;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tes->es_lblk = end + 1;\n\t\t\tes->es_len = len2;\n\t\t\tif (ext4_es_is_written(es) ||\n\t\t\t    ext4_es_is_unwritten(es)) {\n\t\t\t\tblock = orig_es.es_pblk + orig_es.es_len - len2;\n\t\t\t\text4_es_store_pblock(es, block);\n\t\t\t}\n\t\t}\n\t\tif (count_reserved)\n\t\t\tcount_rsvd(inode, lblk, orig_es.es_len - len1 - len2,\n\t\t\t\t   &orig_es, &rc);\n\t\tgoto out_get_reserved;\n\t}\n\n\tif (len1 > 0) {\n\t\tif (count_reserved)\n\t\t\tcount_rsvd(inode, lblk, orig_es.es_len - len1,\n\t\t\t\t   &orig_es, &rc);\n\t\tnode = rb_next(&es->rb_node);\n\t\tif (node)\n\t\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\telse\n\t\t\tes = NULL;\n\t}\n\n\twhile (es && ext4_es_end(es) <= end) {\n\t\tif (count_reserved)\n\t\t\tcount_rsvd(inode, es->es_lblk, es->es_len, es, &rc);\n\t\tnode = rb_next(&es->rb_node);\n\t\trb_erase(&es->rb_node, &tree->root);\n\t\text4_es_free_extent(inode, es);\n\t\tif (!node) {\n\t\t\tes = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t}\n\n\tif (es && es->es_lblk < end + 1) {\n\t\text4_lblk_t orig_len = es->es_len;\n\n\t\tlen1 = ext4_es_end(es) - end;\n\t\tif (count_reserved)\n\t\t\tcount_rsvd(inode, es->es_lblk, orig_len - len1,\n\t\t\t\t   es, &rc);\n\t\tes->es_lblk = end + 1;\n\t\tes->es_len = len1;\n\t\tif (ext4_es_is_written(es) || ext4_es_is_unwritten(es)) {\n\t\t\tblock = es->es_pblk + orig_len - len1;\n\t\t\text4_es_store_pblock(es, block);\n\t\t}\n\t}\n\nout_get_reserved:\n\tif (count_reserved)\n\t\t*reserved = get_rsvd(inode, end, es, &rc);\nout:\n\treturn err;\n}\n\n/*\n * ext4_es_remove_extent - removes block range from extent status tree\n *\n * @inode - file containing range\n * @lblk - first block in range\n * @len - number of blocks to remove\n *\n * Reduces block/cluster reservation count and for bigalloc cancels pending\n * reservations as needed.\n */\nvoid ext4_es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len)\n{\n\text4_lblk_t end;\n\tint err = 0;\n\tint reserved = 0;\n\tstruct extent_status *es = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_remove_extent(inode, lblk, len);\n\tes_debug(\"remove [%u/%u) from extent status tree of inode %lu\\n\",\n\t\t lblk, len, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tend = lblk + len - 1;\n\tBUG_ON(end < lblk);\n\nretry:\n\tif (err && !es)\n\t\tes = __es_alloc_extent(true);\n\t/*\n\t * ext4_clear_inode() depends on us taking i_es_lock unconditionally\n\t * so that we are sure __es_shrink() is done with the inode before it\n\t * is reclaimed.\n\t */\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es) {\n\t\tif (!es->es_len)\n\t\t\t__es_free_extent(es);\n\t\tes = NULL;\n\t}\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_da_release_space(inode, reserved);\n\treturn;\n}\n\nstatic int __es_shrink(struct ext4_sb_info *sbi, int nr_to_scan,\n\t\t       struct ext4_inode_info *locked_ei)\n{\n\tstruct ext4_inode_info *ei;\n\tstruct ext4_es_stats *es_stats;\n\tktime_t start_time;\n\tu64 scan_time;\n\tint nr_to_walk;\n\tint nr_shrunk = 0;\n\tint retried = 0, nr_skipped = 0;\n\n\tes_stats = &sbi->s_es_stats;\n\tstart_time = ktime_get();\n\nretry:\n\tspin_lock(&sbi->s_es_lock);\n\tnr_to_walk = sbi->s_es_nr_inode;\n\twhile (nr_to_walk-- > 0) {\n\t\tif (list_empty(&sbi->s_es_list)) {\n\t\t\tspin_unlock(&sbi->s_es_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tei = list_first_entry(&sbi->s_es_list, struct ext4_inode_info,\n\t\t\t\t      i_es_list);\n\t\t/* Move the inode to the tail */\n\t\tlist_move_tail(&ei->i_es_list, &sbi->s_es_list);\n\n\t\t/*\n\t\t * Normally we try hard to avoid shrinking precached inodes,\n\t\t * but we will as a last resort.\n\t\t */\n\t\tif (!retried && ext4_test_inode_state(&ei->vfs_inode,\n\t\t\t\t\t\tEXT4_STATE_EXT_PRECACHED)) {\n\t\t\tnr_skipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ei == locked_ei || !write_trylock(&ei->i_es_lock)) {\n\t\t\tnr_skipped++;\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * Now we hold i_es_lock which protects us from inode reclaim\n\t\t * freeing inode under us\n\t\t */\n\t\tspin_unlock(&sbi->s_es_lock);\n\n\t\tnr_shrunk += es_reclaim_extents(ei, &nr_to_scan);\n\t\twrite_unlock(&ei->i_es_lock);\n\n\t\tif (nr_to_scan <= 0)\n\t\t\tgoto out;\n\t\tspin_lock(&sbi->s_es_lock);\n\t}\n\tspin_unlock(&sbi->s_es_lock);\n\n\t/*\n\t * If we skipped any inodes, and we weren't able to make any\n\t * forward progress, try again to scan precached inodes.\n\t */\n\tif ((nr_shrunk == 0) && nr_skipped && !retried) {\n\t\tretried++;\n\t\tgoto retry;\n\t}\n\n\tif (locked_ei && nr_shrunk == 0)\n\t\tnr_shrunk = es_reclaim_extents(locked_ei, &nr_to_scan);\n\nout:\n\tscan_time = ktime_to_ns(ktime_sub(ktime_get(), start_time));\n\tif (likely(es_stats->es_stats_scan_time))\n\t\tes_stats->es_stats_scan_time = (scan_time +\n\t\t\t\tes_stats->es_stats_scan_time*3) / 4;\n\telse\n\t\tes_stats->es_stats_scan_time = scan_time;\n\tif (scan_time > es_stats->es_stats_max_scan_time)\n\t\tes_stats->es_stats_max_scan_time = scan_time;\n\tif (likely(es_stats->es_stats_shrunk))\n\t\tes_stats->es_stats_shrunk = (nr_shrunk +\n\t\t\t\tes_stats->es_stats_shrunk*3) / 4;\n\telse\n\t\tes_stats->es_stats_shrunk = nr_shrunk;\n\n\ttrace_ext4_es_shrink(sbi->s_sb, nr_shrunk, scan_time,\n\t\t\t     nr_skipped, retried);\n\treturn nr_shrunk;\n}\n\nstatic unsigned long ext4_es_count(struct shrinker *shrink,\n\t\t\t\t   struct shrink_control *sc)\n{\n\tunsigned long nr;\n\tstruct ext4_sb_info *sbi;\n\n\tsbi = container_of(shrink, struct ext4_sb_info, s_es_shrinker);\n\tnr = percpu_counter_read_positive(&sbi->s_es_stats.es_stats_shk_cnt);\n\ttrace_ext4_es_shrink_count(sbi->s_sb, sc->nr_to_scan, nr);\n\treturn nr;\n}\n\nstatic unsigned long ext4_es_scan(struct shrinker *shrink,\n\t\t\t\t  struct shrink_control *sc)\n{\n\tstruct ext4_sb_info *sbi = container_of(shrink,\n\t\t\t\t\tstruct ext4_sb_info, s_es_shrinker);\n\tint nr_to_scan = sc->nr_to_scan;\n\tint ret, nr_shrunk;\n\n\tret = percpu_counter_read_positive(&sbi->s_es_stats.es_stats_shk_cnt);\n\ttrace_ext4_es_shrink_scan_enter(sbi->s_sb, nr_to_scan, ret);\n\n\tnr_shrunk = __es_shrink(sbi, nr_to_scan, NULL);\n\n\tret = percpu_counter_read_positive(&sbi->s_es_stats.es_stats_shk_cnt);\n\ttrace_ext4_es_shrink_scan_exit(sbi->s_sb, nr_shrunk, ret);\n\treturn nr_shrunk;\n}\n\nint ext4_seq_es_shrinker_info_show(struct seq_file *seq, void *v)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB((struct super_block *) seq->private);\n\tstruct ext4_es_stats *es_stats = &sbi->s_es_stats;\n\tstruct ext4_inode_info *ei, *max = NULL;\n\tunsigned int inode_cnt = 0;\n\n\tif (v != SEQ_START_TOKEN)\n\t\treturn 0;\n\n\t/* here we just find an inode that has the max nr. of objects */\n\tspin_lock(&sbi->s_es_lock);\n\tlist_for_each_entry(ei, &sbi->s_es_list, i_es_list) {\n\t\tinode_cnt++;\n\t\tif (max && max->i_es_all_nr < ei->i_es_all_nr)\n\t\t\tmax = ei;\n\t\telse if (!max)\n\t\t\tmax = ei;\n\t}\n\tspin_unlock(&sbi->s_es_lock);\n\n\tseq_printf(seq, \"stats:\\n  %lld objects\\n  %lld reclaimable objects\\n\",\n\t\t   percpu_counter_sum_positive(&es_stats->es_stats_all_cnt),\n\t\t   percpu_counter_sum_positive(&es_stats->es_stats_shk_cnt));\n\tseq_printf(seq, \"  %lld/%lld cache hits/misses\\n\",\n\t\t   percpu_counter_sum_positive(&es_stats->es_stats_cache_hits),\n\t\t   percpu_counter_sum_positive(&es_stats->es_stats_cache_misses));\n\tif (inode_cnt)\n\t\tseq_printf(seq, \"  %d inodes on list\\n\", inode_cnt);\n\n\tseq_printf(seq, \"average:\\n  %llu us scan time\\n\",\n\t    div_u64(es_stats->es_stats_scan_time, 1000));\n\tseq_printf(seq, \"  %lu shrunk objects\\n\", es_stats->es_stats_shrunk);\n\tif (inode_cnt)\n\t\tseq_printf(seq,\n\t\t    \"maximum:\\n  %lu inode (%u objects, %u reclaimable)\\n\"\n\t\t    \"  %llu us max scan time\\n\",\n\t\t    max->vfs_inode.i_ino, max->i_es_all_nr, max->i_es_shk_nr,\n\t\t    div_u64(es_stats->es_stats_max_scan_time, 1000));\n\n\treturn 0;\n}\n\nint ext4_es_register_shrinker(struct ext4_sb_info *sbi)\n{\n\tint err;\n\n\t/* Make sure we have enough bits for physical block number */\n\tBUILD_BUG_ON(ES_SHIFT < 48);\n\tINIT_LIST_HEAD(&sbi->s_es_list);\n\tsbi->s_es_nr_inode = 0;\n\tspin_lock_init(&sbi->s_es_lock);\n\tsbi->s_es_stats.es_stats_shrunk = 0;\n\terr = percpu_counter_init(&sbi->s_es_stats.es_stats_cache_hits, 0,\n\t\t\t\t  GFP_KERNEL);\n\tif (err)\n\t\treturn err;\n\terr = percpu_counter_init(&sbi->s_es_stats.es_stats_cache_misses, 0,\n\t\t\t\t  GFP_KERNEL);\n\tif (err)\n\t\tgoto err1;\n\tsbi->s_es_stats.es_stats_scan_time = 0;\n\tsbi->s_es_stats.es_stats_max_scan_time = 0;\n\terr = percpu_counter_init(&sbi->s_es_stats.es_stats_all_cnt, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err2;\n\terr = percpu_counter_init(&sbi->s_es_stats.es_stats_shk_cnt, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err3;\n\n\tsbi->s_es_shrinker.scan_objects = ext4_es_scan;\n\tsbi->s_es_shrinker.count_objects = ext4_es_count;\n\tsbi->s_es_shrinker.seeks = DEFAULT_SEEKS;\n\terr = register_shrinker(&sbi->s_es_shrinker, \"ext4-es:%s\",\n\t\t\t\tsbi->s_sb->s_id);\n\tif (err)\n\t\tgoto err4;\n\n\treturn 0;\nerr4:\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_shk_cnt);\nerr3:\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_all_cnt);\nerr2:\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_cache_misses);\nerr1:\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_cache_hits);\n\treturn err;\n}\n\nvoid ext4_es_unregister_shrinker(struct ext4_sb_info *sbi)\n{\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_cache_hits);\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_cache_misses);\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_all_cnt);\n\tpercpu_counter_destroy(&sbi->s_es_stats.es_stats_shk_cnt);\n\tunregister_shrinker(&sbi->s_es_shrinker);\n}\n\n/*\n * Shrink extents in given inode from ei->i_es_shrink_lblk till end. Scan at\n * most *nr_to_scan extents, update *nr_to_scan accordingly.\n *\n * Return 0 if we hit end of tree / interval, 1 if we exhausted nr_to_scan.\n * Increment *nr_shrunk by the number of reclaimed extents. Also update\n * ei->i_es_shrink_lblk to where we should continue scanning.\n */\nstatic int es_do_reclaim_extents(struct ext4_inode_info *ei, ext4_lblk_t end,\n\t\t\t\t int *nr_to_scan, int *nr_shrunk)\n{\n\tstruct inode *inode = &ei->vfs_inode;\n\tstruct ext4_es_tree *tree = &ei->i_es_tree;\n\tstruct extent_status *es;\n\tstruct rb_node *node;\n\n\tes = __es_tree_search(&tree->root, ei->i_es_shrink_lblk);\n\tif (!es)\n\t\tgoto out_wrap;\n\n\twhile (*nr_to_scan > 0) {\n\t\tif (es->es_lblk > end) {\n\t\t\tei->i_es_shrink_lblk = end + 1;\n\t\t\treturn 0;\n\t\t}\n\n\t\t(*nr_to_scan)--;\n\t\tnode = rb_next(&es->rb_node);\n\n\t\tif (ext4_es_must_keep(es))\n\t\t\tgoto next;\n\t\tif (ext4_es_is_referenced(es)) {\n\t\t\text4_es_clear_referenced(es);\n\t\t\tgoto next;\n\t\t}\n\n\t\trb_erase(&es->rb_node, &tree->root);\n\t\text4_es_free_extent(inode, es);\n\t\t(*nr_shrunk)++;\nnext:\n\t\tif (!node)\n\t\t\tgoto out_wrap;\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t}\n\tei->i_es_shrink_lblk = es->es_lblk;\n\treturn 1;\nout_wrap:\n\tei->i_es_shrink_lblk = 0;\n\treturn 0;\n}\n\nstatic int es_reclaim_extents(struct ext4_inode_info *ei, int *nr_to_scan)\n{\n\tstruct inode *inode = &ei->vfs_inode;\n\tint nr_shrunk = 0;\n\text4_lblk_t start = ei->i_es_shrink_lblk;\n\tstatic DEFINE_RATELIMIT_STATE(_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tif (ei->i_es_shk_nr == 0)\n\t\treturn 0;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_EXT_PRECACHED) &&\n\t    __ratelimit(&_rs))\n\t\text4_warning(inode->i_sb, \"forced shrink of precached extents\");\n\n\tif (!es_do_reclaim_extents(ei, EXT_MAX_BLOCKS, nr_to_scan, &nr_shrunk) &&\n\t    start != 0)\n\t\tes_do_reclaim_extents(ei, start - 1, nr_to_scan, &nr_shrunk);\n\n\tei->i_es_tree.cache_es = NULL;\n\treturn nr_shrunk;\n}\n\n/*\n * Called to support EXT4_IOC_CLEAR_ES_CACHE.  We can only remove\n * discretionary entries from the extent status cache.  (Some entries\n * must be present for proper operations.)\n */\nvoid ext4_clear_inode_es(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct extent_status *es;\n\tstruct ext4_es_tree *tree;\n\tstruct rb_node *node;\n\n\twrite_lock(&ei->i_es_lock);\n\ttree = &EXT4_I(inode)->i_es_tree;\n\ttree->cache_es = NULL;\n\tnode = rb_first(&tree->root);\n\twhile (node) {\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t\tnode = rb_next(node);\n\t\tif (!ext4_es_must_keep(es)) {\n\t\t\trb_erase(&es->rb_node, &tree->root);\n\t\t\text4_es_free_extent(inode, es);\n\t\t}\n\t}\n\text4_clear_inode_state(inode, EXT4_STATE_EXT_PRECACHED);\n\twrite_unlock(&ei->i_es_lock);\n}\n\n#ifdef ES_DEBUG__\nstatic void ext4_print_pending_tree(struct inode *inode)\n{\n\tstruct ext4_pending_tree *tree;\n\tstruct rb_node *node;\n\tstruct pending_reservation *pr;\n\n\tprintk(KERN_DEBUG \"pending reservations for inode %lu:\", inode->i_ino);\n\ttree = &EXT4_I(inode)->i_pending_tree;\n\tnode = rb_first(&tree->root);\n\twhile (node) {\n\t\tpr = rb_entry(node, struct pending_reservation, rb_node);\n\t\tprintk(KERN_DEBUG \" %u\", pr->lclu);\n\t\tnode = rb_next(node);\n\t}\n\tprintk(KERN_DEBUG \"\\n\");\n}\n#else\n#define ext4_print_pending_tree(inode)\n#endif\n\nint __init ext4_init_pending(void)\n{\n\text4_pending_cachep = KMEM_CACHE(pending_reservation, SLAB_RECLAIM_ACCOUNT);\n\tif (ext4_pending_cachep == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid ext4_exit_pending(void)\n{\n\tkmem_cache_destroy(ext4_pending_cachep);\n}\n\nvoid ext4_init_pending_tree(struct ext4_pending_tree *tree)\n{\n\ttree->root = RB_ROOT;\n}\n\n/*\n * __get_pending - retrieve a pointer to a pending reservation\n *\n * @inode - file containing the pending cluster reservation\n * @lclu - logical cluster of interest\n *\n * Returns a pointer to a pending reservation if it's a member of\n * the set, and NULL if not.  Must be called holding i_es_lock.\n */\nstatic struct pending_reservation *__get_pending(struct inode *inode,\n\t\t\t\t\t\t ext4_lblk_t lclu)\n{\n\tstruct ext4_pending_tree *tree;\n\tstruct rb_node *node;\n\tstruct pending_reservation *pr = NULL;\n\n\ttree = &EXT4_I(inode)->i_pending_tree;\n\tnode = (&tree->root)->rb_node;\n\n\twhile (node) {\n\t\tpr = rb_entry(node, struct pending_reservation, rb_node);\n\t\tif (lclu < pr->lclu)\n\t\t\tnode = node->rb_left;\n\t\telse if (lclu > pr->lclu)\n\t\t\tnode = node->rb_right;\n\t\telse if (lclu == pr->lclu)\n\t\t\treturn pr;\n\t}\n\treturn NULL;\n}\n\n/*\n * __insert_pending - adds a pending cluster reservation to the set of\n *                    pending reservations\n *\n * @inode - file containing the cluster\n * @lblk - logical block in the cluster to be added\n *\n * Returns 0 on successful insertion and -ENOMEM on failure.  If the\n * pending reservation is already in the set, returns successfully.\n */\nstatic int __insert_pending(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_pending_tree *tree = &EXT4_I(inode)->i_pending_tree;\n\tstruct rb_node **p = &tree->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct pending_reservation *pr;\n\text4_lblk_t lclu;\n\tint ret = 0;\n\n\tlclu = EXT4_B2C(sbi, lblk);\n\t/* search to find parent for insertion */\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpr = rb_entry(parent, struct pending_reservation, rb_node);\n\n\t\tif (lclu < pr->lclu) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (lclu > pr->lclu) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\t/* pending reservation already inserted */\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr = kmem_cache_alloc(ext4_pending_cachep, GFP_ATOMIC);\n\tif (pr == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpr->lclu = lclu;\n\n\trb_link_node(&pr->rb_node, parent, p);\n\trb_insert_color(&pr->rb_node, &tree->root);\n\nout:\n\treturn ret;\n}\n\n/*\n * __remove_pending - removes a pending cluster reservation from the set\n *                    of pending reservations\n *\n * @inode - file containing the cluster\n * @lblk - logical block in the pending cluster reservation to be removed\n *\n * Returns successfully if pending reservation is not a member of the set.\n */\nstatic void __remove_pending(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct pending_reservation *pr;\n\tstruct ext4_pending_tree *tree;\n\n\tpr = __get_pending(inode, EXT4_B2C(sbi, lblk));\n\tif (pr != NULL) {\n\t\ttree = &EXT4_I(inode)->i_pending_tree;\n\t\trb_erase(&pr->rb_node, &tree->root);\n\t\tkmem_cache_free(ext4_pending_cachep, pr);\n\t}\n}\n\n/*\n * ext4_remove_pending - removes a pending cluster reservation from the set\n *                       of pending reservations\n *\n * @inode - file containing the cluster\n * @lblk - logical block in the pending cluster reservation to be removed\n *\n * Locking for external use of __remove_pending.\n */\nvoid ext4_remove_pending(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\twrite_lock(&ei->i_es_lock);\n\t__remove_pending(inode, lblk);\n\twrite_unlock(&ei->i_es_lock);\n}\n\n/*\n * ext4_is_pending - determine whether a cluster has a pending reservation\n *                   on it\n *\n * @inode - file containing the cluster\n * @lblk - logical block in the cluster\n *\n * Returns true if there's a pending reservation for the cluster in the\n * set of pending reservations, and false if not.\n */\nbool ext4_is_pending(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tbool ret;\n\n\tread_lock(&ei->i_es_lock);\n\tret = (bool)(__get_pending(inode, EXT4_B2C(sbi, lblk)) != NULL);\n\tread_unlock(&ei->i_es_lock);\n\n\treturn ret;\n}\n\n/*\n * ext4_es_insert_delayed_block - adds a delayed block to the extents status\n *                                tree, adding a pending reservation where\n *                                needed\n *\n * @inode - file containing the newly added block\n * @lblk - logical block to be added\n * @allocated - indicates whether a physical cluster has been allocated for\n *              the logical cluster that contains the block\n */\nvoid ext4_es_insert_delayed_block(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t\t  bool allocated)\n{\n\tstruct extent_status newes;\n\tint err1 = 0;\n\tint err2 = 0;\n\tstruct extent_status *es1 = NULL;\n\tstruct extent_status *es2 = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\tes_debug(\"add [%u/1) delayed to extent status tree of inode %lu\\n\",\n\t\t lblk, inode->i_ino);\n\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = 1;\n\text4_es_store_pblock_status(&newes, ~0, EXTENT_STATUS_DELAYED);\n\ttrace_ext4_es_insert_delayed_block(inode, &newes, allocated);\n\n\text4_es_insert_extent_check(inode, &newes);\n\nretry:\n\tif (err1 && !es1)\n\t\tes1 = __es_alloc_extent(true);\n\tif ((err1 || err2) && !es2)\n\t\tes2 = __es_alloc_extent(true);\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\n\terr1 = __es_remove_extent(inode, lblk, lblk, NULL, es1);\n\tif (err1 != 0)\n\t\tgoto error;\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es1) {\n\t\tif (!es1->es_len)\n\t\t\t__es_free_extent(es1);\n\t\tes1 = NULL;\n\t}\n\n\terr2 = __es_insert_extent(inode, &newes, es2);\n\tif (err2 != 0)\n\t\tgoto error;\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es2) {\n\t\tif (!es2->es_len)\n\t\t\t__es_free_extent(es2);\n\t\tes2 = NULL;\n\t}\n\n\tif (allocated)\n\t\t__insert_pending(inode, lblk);\nerror:\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err1 || err2)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_print_pending_tree(inode);\n\treturn;\n}\n\n/*\n * __es_delayed_clu - count number of clusters containing blocks that\n *                    are delayed only\n *\n * @inode - file containing block range\n * @start - logical block defining start of range\n * @end - logical block defining end of range\n *\n * Returns the number of clusters containing only delayed (not delayed\n * and unwritten) blocks in the range specified by @start and @end.  Any\n * cluster or part of a cluster within the range and containing a delayed\n * and not unwritten block within the range is counted as a whole cluster.\n */\nstatic unsigned int __es_delayed_clu(struct inode *inode, ext4_lblk_t start,\n\t\t\t\t     ext4_lblk_t end)\n{\n\tstruct ext4_es_tree *tree = &EXT4_I(inode)->i_es_tree;\n\tstruct extent_status *es;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct rb_node *node;\n\text4_lblk_t first_lclu, last_lclu;\n\tunsigned long long last_counted_lclu;\n\tunsigned int n = 0;\n\n\t/* guaranteed to be unequal to any ext4_lblk_t value */\n\tlast_counted_lclu = ~0ULL;\n\n\tes = __es_tree_search(&tree->root, start);\n\n\twhile (es && (es->es_lblk <= end)) {\n\t\tif (ext4_es_is_delonly(es)) {\n\t\t\tif (es->es_lblk <= start)\n\t\t\t\tfirst_lclu = EXT4_B2C(sbi, start);\n\t\t\telse\n\t\t\t\tfirst_lclu = EXT4_B2C(sbi, es->es_lblk);\n\n\t\t\tif (ext4_es_end(es) >= end)\n\t\t\t\tlast_lclu = EXT4_B2C(sbi, end);\n\t\t\telse\n\t\t\t\tlast_lclu = EXT4_B2C(sbi, ext4_es_end(es));\n\n\t\t\tif (first_lclu == last_counted_lclu)\n\t\t\t\tn += last_lclu - first_lclu;\n\t\t\telse\n\t\t\t\tn += last_lclu - first_lclu + 1;\n\t\t\tlast_counted_lclu = last_lclu;\n\t\t}\n\t\tnode = rb_next(&es->rb_node);\n\t\tif (!node)\n\t\t\tbreak;\n\t\tes = rb_entry(node, struct extent_status, rb_node);\n\t}\n\n\treturn n;\n}\n\n/*\n * ext4_es_delayed_clu - count number of clusters containing blocks that\n *                       are both delayed and unwritten\n *\n * @inode - file containing block range\n * @lblk - logical block defining start of range\n * @len - number of blocks in range\n *\n * Locking for external use of __es_delayed_clu().\n */\nunsigned int ext4_es_delayed_clu(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t\t ext4_lblk_t len)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\text4_lblk_t end;\n\tunsigned int n;\n\n\tif (len == 0)\n\t\treturn 0;\n\n\tend = lblk + len - 1;\n\tWARN_ON(end < lblk);\n\n\tread_lock(&ei->i_es_lock);\n\n\tn = __es_delayed_clu(inode, lblk, end);\n\n\tread_unlock(&ei->i_es_lock);\n\n\treturn n;\n}\n\n/*\n * __revise_pending - makes, cancels, or leaves unchanged pending cluster\n *                    reservations for a specified block range depending\n *                    upon the presence or absence of delayed blocks\n *                    outside the range within clusters at the ends of the\n *                    range\n *\n * @inode - file containing the range\n * @lblk - logical block defining the start of range\n * @len  - length of range in blocks\n *\n * Used after a newly allocated extent is added to the extents status tree.\n * Requires that the extents in the range have either written or unwritten\n * status.  Must be called while holding i_es_lock.\n */\nstatic void __revise_pending(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t     ext4_lblk_t len)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t end = lblk + len - 1;\n\text4_lblk_t first, last;\n\tbool f_del = false, l_del = false;\n\n\tif (len == 0)\n\t\treturn;\n\n\t/*\n\t * Two cases - block range within single cluster and block range\n\t * spanning two or more clusters.  Note that a cluster belonging\n\t * to a range starting and/or ending on a cluster boundary is treated\n\t * as if it does not contain a delayed extent.  The new range may\n\t * have allocated space for previously delayed blocks out to the\n\t * cluster boundary, requiring that any pre-existing pending\n\t * reservation be canceled.  Because this code only looks at blocks\n\t * outside the range, it should revise pending reservations\n\t * correctly even if the extent represented by the range can't be\n\t * inserted in the extents status tree due to ENOSPC.\n\t */\n\n\tif (EXT4_B2C(sbi, lblk) == EXT4_B2C(sbi, end)) {\n\t\tfirst = EXT4_LBLK_CMASK(sbi, lblk);\n\t\tif (first != lblk)\n\t\t\tf_del = __es_scan_range(inode, &ext4_es_is_delonly,\n\t\t\t\t\t\tfirst, lblk - 1);\n\t\tif (f_del) {\n\t\t\t__insert_pending(inode, first);\n\t\t} else {\n\t\t\tlast = EXT4_LBLK_CMASK(sbi, end) +\n\t\t\t       sbi->s_cluster_ratio - 1;\n\t\t\tif (last != end)\n\t\t\t\tl_del = __es_scan_range(inode,\n\t\t\t\t\t\t\t&ext4_es_is_delonly,\n\t\t\t\t\t\t\tend + 1, last);\n\t\t\tif (l_del)\n\t\t\t\t__insert_pending(inode, last);\n\t\t\telse\n\t\t\t\t__remove_pending(inode, last);\n\t\t}\n\t} else {\n\t\tfirst = EXT4_LBLK_CMASK(sbi, lblk);\n\t\tif (first != lblk)\n\t\t\tf_del = __es_scan_range(inode, &ext4_es_is_delonly,\n\t\t\t\t\t\tfirst, lblk - 1);\n\t\tif (f_del)\n\t\t\t__insert_pending(inode, first);\n\t\telse\n\t\t\t__remove_pending(inode, first);\n\n\t\tlast = EXT4_LBLK_CMASK(sbi, end) + sbi->s_cluster_ratio - 1;\n\t\tif (last != end)\n\t\t\tl_del = __es_scan_range(inode, &ext4_es_is_delonly,\n\t\t\t\t\t\tend + 1, last);\n\t\tif (l_del)\n\t\t\t__insert_pending(inode, last);\n\t\telse\n\t\t\t__remove_pending(inode, last);\n\t}\n}\n"], "filenames": ["fs/ext4/extents_status.c"], "buggy_code_start_loc": [880], "buggy_code_end_loc": [2063], "fixing_code_start_loc": [881], "fixing_code_end_loc": [2078], "type": "CWE-416", "message": "The Linux kernel before 6.5.4 has an es1 use-after-free in fs/ext4/extents_status.c, related to ext4_es_insert_extent.", "other": {"cve": {"id": "CVE-2023-45898", "sourceIdentifier": "cve@mitre.org", "published": "2023-10-16T03:15:09.320", "lastModified": "2023-11-07T04:21:49.253", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel before 6.5.4 has an es1 use-after-free in fs/ext4/extents_status.c, related to ext4_es_insert_extent."}, {"lang": "es", "value": "El kernel de Linux anterior a 6.5.4 tiene un es1 use-after-free en fs/ext4/extents_status.c, relacionado con ext4_es_insert_extent."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "6.5.4", "matchCriteriaId": "F125497B-0184-43A2-AB25-BCC4F3E2E97E"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v6.x/ChangeLog-6.5.4", "source": "cve@mitre.org", "tags": ["Release Notes"]}, {"url": "https://github.com/torvalds/linux/commit/768d612f79822d30a1e7d132a4d4b05337ce42ec", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://lkml.org/lkml/2023/8/13/477", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch"]}, {"url": "https://lore.kernel.org/lkml/aa03f191-445c-0d2e-d6d7-0a3208d7df7a%40huawei.com/T/", "source": "cve@mitre.org"}, {"url": "https://www.spinics.net/lists/stable-commits/msg317086.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/768d612f79822d30a1e7d132a4d4b05337ce42ec"}}