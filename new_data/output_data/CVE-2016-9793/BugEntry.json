{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tGeneric socket support routines. Memory allocators, socket lock/release\n *\t\thandler for protocols to use and generic option handler.\n *\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tAlan Cox, <A.Cox@swansea.ac.uk>\n *\n * Fixes:\n *\t\tAlan Cox\t: \tNumerous verify_area() problems\n *\t\tAlan Cox\t:\tConnecting on a connecting socket\n *\t\t\t\t\tnow returns an error for tcp.\n *\t\tAlan Cox\t:\tsock->protocol is set correctly.\n *\t\t\t\t\tand is not sometimes left as 0.\n *\t\tAlan Cox\t:\tconnect handles icmp errors on a\n *\t\t\t\t\tconnect properly. Unfortunately there\n *\t\t\t\t\tis a restart syscall nasty there. I\n *\t\t\t\t\tcan't match BSD without hacking the C\n *\t\t\t\t\tlibrary. Ideas urgently sought!\n *\t\tAlan Cox\t:\tDisallow bind() to addresses that are\n *\t\t\t\t\tnot ours - especially broadcast ones!!\n *\t\tAlan Cox\t:\tSocket 1024 _IS_ ok for users. (fencepost)\n *\t\tAlan Cox\t:\tsock_wfree/sock_rfree don't destroy sockets,\n *\t\t\t\t\tinstead they leave that for the DESTROY timer.\n *\t\tAlan Cox\t:\tClean up error flag in accept\n *\t\tAlan Cox\t:\tTCP ack handling is buggy, the DESTROY timer\n *\t\t\t\t\twas buggy. Put a remove_sock() in the handler\n *\t\t\t\t\tfor memory when we hit 0. Also altered the timer\n *\t\t\t\t\tcode. The ACK stuff can wait and needs major\n *\t\t\t\t\tTCP layer surgery.\n *\t\tAlan Cox\t:\tFixed TCP ack bug, removed remove sock\n *\t\t\t\t\tand fixed timer/inet_bh race.\n *\t\tAlan Cox\t:\tAdded zapped flag for TCP\n *\t\tAlan Cox\t:\tMove kfree_skb into skbuff.c and tidied up surplus code\n *\t\tAlan Cox\t:\tfor new sk_buff allocations wmalloc/rmalloc now call alloc_skb\n *\t\tAlan Cox\t:\tkfree_s calls now are kfree_skbmem so we can track skb resources\n *\t\tAlan Cox\t:\tSupports socket option broadcast now as does udp. Packet and raw need fixing.\n *\t\tAlan Cox\t:\tAdded RCVBUF,SNDBUF size setting. It suddenly occurred to me how easy it was so...\n *\t\tRick Sladkey\t:\tRelaxed UDP rules for matching packets.\n *\t\tC.E.Hawkins\t:\tIFF_PROMISC/SIOCGHWADDR support\n *\tPauline Middelink\t:\tidentd support\n *\t\tAlan Cox\t:\tFixed connect() taking signals I think.\n *\t\tAlan Cox\t:\tSO_LINGER supported\n *\t\tAlan Cox\t:\tError reporting fixes\n *\t\tAnonymous\t:\tinet_create tidied up (sk->reuse setting)\n *\t\tAlan Cox\t:\tinet sockets don't set sk->type!\n *\t\tAlan Cox\t:\tSplit socket option code\n *\t\tAlan Cox\t:\tCallbacks\n *\t\tAlan Cox\t:\tNagle flag for Charles & Johannes stuff\n *\t\tAlex\t\t:\tRemoved restriction on inet fioctl\n *\t\tAlan Cox\t:\tSplitting INET from NET core\n *\t\tAlan Cox\t:\tFixed bogus SO_TYPE handling in getsockopt()\n *\t\tAdam Caldwell\t:\tMissing return in SO_DONTROUTE/SO_DEBUG code\n *\t\tAlan Cox\t:\tSplit IP from generic code\n *\t\tAlan Cox\t:\tNew kfree_skbmem()\n *\t\tAlan Cox\t:\tMake SO_DEBUG superuser only.\n *\t\tAlan Cox\t:\tAllow anyone to clear SO_DEBUG\n *\t\t\t\t\t(compatibility fix)\n *\t\tAlan Cox\t:\tAdded optimistic memory grabbing for AF_UNIX throughput.\n *\t\tAlan Cox\t:\tAllocator for a socket is settable.\n *\t\tAlan Cox\t:\tSO_ERROR includes soft errors.\n *\t\tAlan Cox\t:\tAllow NULL arguments on some SO_ opts\n *\t\tAlan Cox\t: \tGeneric socket allocation to make hooks\n *\t\t\t\t\teasier (suggested by Craig Metz).\n *\t\tMichael Pall\t:\tSO_ERROR returns positive errno again\n *              Steve Whitehouse:       Added default destructor to free\n *                                      protocol private data.\n *              Steve Whitehouse:       Added various other default routines\n *                                      common to several socket families.\n *              Chris Evans     :       Call suser() check last on F_SETOWN\n *\t\tJay Schulist\t:\tAdded SO_ATTACH_FILTER and SO_DETACH_FILTER.\n *\t\tAndi Kleen\t:\tAdd sock_kmalloc()/sock_kfree_s()\n *\t\tAndi Kleen\t:\tFix write_space callback\n *\t\tChris Evans\t:\tSecurity fixes - signedness again\n *\t\tArnaldo C. Melo :       cleanups, use skb_queue_purge\n *\n * To Fix:\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/errqueue.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/poll.h>\n#include <linux/tcp.h>\n#include <linux/init.h>\n#include <linux/highmem.h>\n#include <linux/user_namespace.h>\n#include <linux/static_key.h>\n#include <linux/memcontrol.h>\n#include <linux/prefetch.h>\n\n#include <asm/uaccess.h>\n\n#include <linux/netdevice.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/request_sock.h>\n#include <net/sock.h>\n#include <linux/net_tstamp.h>\n#include <net/xfrm.h>\n#include <linux/ipsec.h>\n#include <net/cls_cgroup.h>\n#include <net/netprio_cgroup.h>\n#include <linux/sock_diag.h>\n\n#include <linux/filter.h>\n#include <net/sock_reuseport.h>\n\n#include <trace/events/sock.h>\n\n#ifdef CONFIG_INET\n#include <net/tcp.h>\n#endif\n\n#include <net/busy_poll.h>\n\nstatic DEFINE_MUTEX(proto_list_mutex);\nstatic LIST_HEAD(proto_list);\n\n/**\n * sk_ns_capable - General socket capability test\n * @sk: Socket to use a capability on or through\n * @user_ns: The user namespace of the capability to use\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket had when the socket was\n * created and the current process has the capability @cap in the user\n * namespace @user_ns.\n */\nbool sk_ns_capable(const struct sock *sk,\n\t\t   struct user_namespace *user_ns, int cap)\n{\n\treturn file_ns_capable(sk->sk_socket->file, user_ns, cap) &&\n\t\tns_capable(user_ns, cap);\n}\nEXPORT_SYMBOL(sk_ns_capable);\n\n/**\n * sk_capable - Socket global capability test\n * @sk: Socket to use a capability on or through\n * @cap: The global capability to use\n *\n * Test to see if the opener of the socket had when the socket was\n * created and the current process has the capability @cap in all user\n * namespaces.\n */\nbool sk_capable(const struct sock *sk, int cap)\n{\n\treturn sk_ns_capable(sk, &init_user_ns, cap);\n}\nEXPORT_SYMBOL(sk_capable);\n\n/**\n * sk_net_capable - Network namespace socket capability test\n * @sk: Socket to use a capability on or through\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket had when the socket was created\n * and the current process has the capability @cap over the network namespace\n * the socket is a member of.\n */\nbool sk_net_capable(const struct sock *sk, int cap)\n{\n\treturn sk_ns_capable(sk, sock_net(sk)->user_ns, cap);\n}\nEXPORT_SYMBOL(sk_net_capable);\n\n/*\n * Each address family might have different locking rules, so we have\n * one slock key per address family:\n */\nstatic struct lock_class_key af_family_keys[AF_MAX];\nstatic struct lock_class_key af_family_slock_keys[AF_MAX];\n\n/*\n * Make lock validator output more readable. (we pre-construct these\n * strings build-time, so that runtime initialization of socket\n * locks is fast):\n */\nstatic const char *const af_family_key_strings[AF_MAX+1] = {\n  \"sk_lock-AF_UNSPEC\", \"sk_lock-AF_UNIX\"     , \"sk_lock-AF_INET\"     ,\n  \"sk_lock-AF_AX25\"  , \"sk_lock-AF_IPX\"      , \"sk_lock-AF_APPLETALK\",\n  \"sk_lock-AF_NETROM\", \"sk_lock-AF_BRIDGE\"   , \"sk_lock-AF_ATMPVC\"   ,\n  \"sk_lock-AF_X25\"   , \"sk_lock-AF_INET6\"    , \"sk_lock-AF_ROSE\"     ,\n  \"sk_lock-AF_DECnet\", \"sk_lock-AF_NETBEUI\"  , \"sk_lock-AF_SECURITY\" ,\n  \"sk_lock-AF_KEY\"   , \"sk_lock-AF_NETLINK\"  , \"sk_lock-AF_PACKET\"   ,\n  \"sk_lock-AF_ASH\"   , \"sk_lock-AF_ECONET\"   , \"sk_lock-AF_ATMSVC\"   ,\n  \"sk_lock-AF_RDS\"   , \"sk_lock-AF_SNA\"      , \"sk_lock-AF_IRDA\"     ,\n  \"sk_lock-AF_PPPOX\" , \"sk_lock-AF_WANPIPE\"  , \"sk_lock-AF_LLC\"      ,\n  \"sk_lock-27\"       , \"sk_lock-28\"          , \"sk_lock-AF_CAN\"      ,\n  \"sk_lock-AF_TIPC\"  , \"sk_lock-AF_BLUETOOTH\", \"sk_lock-IUCV\"        ,\n  \"sk_lock-AF_RXRPC\" , \"sk_lock-AF_ISDN\"     , \"sk_lock-AF_PHONET\"   ,\n  \"sk_lock-AF_IEEE802154\", \"sk_lock-AF_CAIF\" , \"sk_lock-AF_ALG\"      ,\n  \"sk_lock-AF_NFC\"   , \"sk_lock-AF_VSOCK\"    , \"sk_lock-AF_KCM\"      ,\n  \"sk_lock-AF_MAX\"\n};\nstatic const char *const af_family_slock_key_strings[AF_MAX+1] = {\n  \"slock-AF_UNSPEC\", \"slock-AF_UNIX\"     , \"slock-AF_INET\"     ,\n  \"slock-AF_AX25\"  , \"slock-AF_IPX\"      , \"slock-AF_APPLETALK\",\n  \"slock-AF_NETROM\", \"slock-AF_BRIDGE\"   , \"slock-AF_ATMPVC\"   ,\n  \"slock-AF_X25\"   , \"slock-AF_INET6\"    , \"slock-AF_ROSE\"     ,\n  \"slock-AF_DECnet\", \"slock-AF_NETBEUI\"  , \"slock-AF_SECURITY\" ,\n  \"slock-AF_KEY\"   , \"slock-AF_NETLINK\"  , \"slock-AF_PACKET\"   ,\n  \"slock-AF_ASH\"   , \"slock-AF_ECONET\"   , \"slock-AF_ATMSVC\"   ,\n  \"slock-AF_RDS\"   , \"slock-AF_SNA\"      , \"slock-AF_IRDA\"     ,\n  \"slock-AF_PPPOX\" , \"slock-AF_WANPIPE\"  , \"slock-AF_LLC\"      ,\n  \"slock-27\"       , \"slock-28\"          , \"slock-AF_CAN\"      ,\n  \"slock-AF_TIPC\"  , \"slock-AF_BLUETOOTH\", \"slock-AF_IUCV\"     ,\n  \"slock-AF_RXRPC\" , \"slock-AF_ISDN\"     , \"slock-AF_PHONET\"   ,\n  \"slock-AF_IEEE802154\", \"slock-AF_CAIF\" , \"slock-AF_ALG\"      ,\n  \"slock-AF_NFC\"   , \"slock-AF_VSOCK\"    ,\"slock-AF_KCM\"       ,\n  \"slock-AF_MAX\"\n};\nstatic const char *const af_family_clock_key_strings[AF_MAX+1] = {\n  \"clock-AF_UNSPEC\", \"clock-AF_UNIX\"     , \"clock-AF_INET\"     ,\n  \"clock-AF_AX25\"  , \"clock-AF_IPX\"      , \"clock-AF_APPLETALK\",\n  \"clock-AF_NETROM\", \"clock-AF_BRIDGE\"   , \"clock-AF_ATMPVC\"   ,\n  \"clock-AF_X25\"   , \"clock-AF_INET6\"    , \"clock-AF_ROSE\"     ,\n  \"clock-AF_DECnet\", \"clock-AF_NETBEUI\"  , \"clock-AF_SECURITY\" ,\n  \"clock-AF_KEY\"   , \"clock-AF_NETLINK\"  , \"clock-AF_PACKET\"   ,\n  \"clock-AF_ASH\"   , \"clock-AF_ECONET\"   , \"clock-AF_ATMSVC\"   ,\n  \"clock-AF_RDS\"   , \"clock-AF_SNA\"      , \"clock-AF_IRDA\"     ,\n  \"clock-AF_PPPOX\" , \"clock-AF_WANPIPE\"  , \"clock-AF_LLC\"      ,\n  \"clock-27\"       , \"clock-28\"          , \"clock-AF_CAN\"      ,\n  \"clock-AF_TIPC\"  , \"clock-AF_BLUETOOTH\", \"clock-AF_IUCV\"     ,\n  \"clock-AF_RXRPC\" , \"clock-AF_ISDN\"     , \"clock-AF_PHONET\"   ,\n  \"clock-AF_IEEE802154\", \"clock-AF_CAIF\" , \"clock-AF_ALG\"      ,\n  \"clock-AF_NFC\"   , \"clock-AF_VSOCK\"    , \"clock-AF_KCM\"      ,\n  \"clock-AF_MAX\"\n};\n\n/*\n * sk_callback_lock locking rules are per-address-family,\n * so split the lock classes by using a per-AF key:\n */\nstatic struct lock_class_key af_callback_keys[AF_MAX];\n\n/* Take into consideration the size of the struct sk_buff overhead in the\n * determination of these values, since that is non-constant across\n * platforms.  This makes socket queueing behavior and performance\n * not depend upon such differences.\n */\n#define _SK_MEM_PACKETS\t\t256\n#define _SK_MEM_OVERHEAD\tSKB_TRUESIZE(256)\n#define SK_WMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n#define SK_RMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n\n/* Run time adjustable parameters. */\n__u32 sysctl_wmem_max __read_mostly = SK_WMEM_MAX;\nEXPORT_SYMBOL(sysctl_wmem_max);\n__u32 sysctl_rmem_max __read_mostly = SK_RMEM_MAX;\nEXPORT_SYMBOL(sysctl_rmem_max);\n__u32 sysctl_wmem_default __read_mostly = SK_WMEM_MAX;\n__u32 sysctl_rmem_default __read_mostly = SK_RMEM_MAX;\n\n/* Maximal space eaten by iovec or ancillary data plus some space */\nint sysctl_optmem_max __read_mostly = sizeof(unsigned long)*(2*UIO_MAXIOV+512);\nEXPORT_SYMBOL(sysctl_optmem_max);\n\nint sysctl_tstamp_allow_data __read_mostly = 1;\n\nstruct static_key memalloc_socks = STATIC_KEY_INIT_FALSE;\nEXPORT_SYMBOL_GPL(memalloc_socks);\n\n/**\n * sk_set_memalloc - sets %SOCK_MEMALLOC\n * @sk: socket to set it on\n *\n * Set %SOCK_MEMALLOC on a socket for access to emergency reserves.\n * It's the responsibility of the admin to adjust min_free_kbytes\n * to meet the requirements\n */\nvoid sk_set_memalloc(struct sock *sk)\n{\n\tsock_set_flag(sk, SOCK_MEMALLOC);\n\tsk->sk_allocation |= __GFP_MEMALLOC;\n\tstatic_key_slow_inc(&memalloc_socks);\n}\nEXPORT_SYMBOL_GPL(sk_set_memalloc);\n\nvoid sk_clear_memalloc(struct sock *sk)\n{\n\tsock_reset_flag(sk, SOCK_MEMALLOC);\n\tsk->sk_allocation &= ~__GFP_MEMALLOC;\n\tstatic_key_slow_dec(&memalloc_socks);\n\n\t/*\n\t * SOCK_MEMALLOC is allowed to ignore rmem limits to ensure forward\n\t * progress of swapping. SOCK_MEMALLOC may be cleared while\n\t * it has rmem allocations due to the last swapfile being deactivated\n\t * but there is a risk that the socket is unusable due to exceeding\n\t * the rmem limits. Reclaim the reserves and obey rmem limits again.\n\t */\n\tsk_mem_reclaim(sk);\n}\nEXPORT_SYMBOL_GPL(sk_clear_memalloc);\n\nint __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tint ret;\n\tunsigned long pflags = current->flags;\n\n\t/* these should have been dropped before queueing */\n\tBUG_ON(!sock_flag(sk, SOCK_MEMALLOC));\n\n\tcurrent->flags |= PF_MEMALLOC;\n\tret = sk->sk_backlog_rcv(sk, skb);\n\ttsk_restore_flags(current, pflags, PF_MEMALLOC);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__sk_backlog_rcv);\n\nstatic int sock_set_timeout(long *timeo_p, char __user *optval, int optlen)\n{\n\tstruct timeval tv;\n\n\tif (optlen < sizeof(tv))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&tv, optval, sizeof(tv)))\n\t\treturn -EFAULT;\n\tif (tv.tv_usec < 0 || tv.tv_usec >= USEC_PER_SEC)\n\t\treturn -EDOM;\n\n\tif (tv.tv_sec < 0) {\n\t\tstatic int warned __read_mostly;\n\n\t\t*timeo_p = 0;\n\t\tif (warned < 10 && net_ratelimit()) {\n\t\t\twarned++;\n\t\t\tpr_info(\"%s: `%s' (pid %d) tries to set negative timeout\\n\",\n\t\t\t\t__func__, current->comm, task_pid_nr(current));\n\t\t}\n\t\treturn 0;\n\t}\n\t*timeo_p = MAX_SCHEDULE_TIMEOUT;\n\tif (tv.tv_sec == 0 && tv.tv_usec == 0)\n\t\treturn 0;\n\tif (tv.tv_sec < (MAX_SCHEDULE_TIMEOUT/HZ - 1))\n\t\t*timeo_p = tv.tv_sec*HZ + (tv.tv_usec+(1000000/HZ-1))/(1000000/HZ);\n\treturn 0;\n}\n\nstatic void sock_warn_obsolete_bsdism(const char *name)\n{\n\tstatic int warned;\n\tstatic char warncomm[TASK_COMM_LEN];\n\tif (strcmp(warncomm, current->comm) && warned < 5) {\n\t\tstrcpy(warncomm,  current->comm);\n\t\tpr_warn(\"process `%s' is using obsolete %s SO_BSDCOMPAT\\n\",\n\t\t\twarncomm, name);\n\t\twarned++;\n\t}\n}\n\nstatic bool sock_needs_netstamp(const struct sock *sk)\n{\n\tswitch (sk->sk_family) {\n\tcase AF_UNSPEC:\n\tcase AF_UNIX:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic void sock_disable_timestamp(struct sock *sk, unsigned long flags)\n{\n\tif (sk->sk_flags & flags) {\n\t\tsk->sk_flags &= ~flags;\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    !(sk->sk_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_disable_timestamp();\n\t}\n}\n\n\nint __sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tunsigned long flags;\n\tstruct sk_buff_head *list = &sk->sk_receive_queue;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\ttrace_sock_rcvqueue_full(sk, skb);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (!sk_rmem_schedule(sk, skb, skb->truesize)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\treturn -ENOBUFS;\n\t}\n\n\tskb->dev = NULL;\n\tskb_set_owner_r(skb, sk);\n\n\t/* we escape from rcu protected region, make sure we dont leak\n\t * a norefcounted dst\n\t */\n\tskb_dst_force(skb);\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tsock_skb_set_dropcount(sk, skb);\n\t__skb_queue_tail(list, skb);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(__sock_queue_rcv_skb);\n\nint sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint err;\n\n\terr = sk_filter(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\treturn __sock_queue_rcv_skb(sk, skb);\n}\nEXPORT_SYMBOL(sock_queue_rcv_skb);\n\nint __sk_receive_skb(struct sock *sk, struct sk_buff *skb,\n\t\t     const int nested, unsigned int trim_cap, bool refcounted)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter_trim_cap(sk, skb, trim_cap))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t/*\n\t\t * trylock + unlock semantics:\n\t\t */\n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\t} else if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\n\tbh_unlock_sock(sk);\nout:\n\tif (refcounted)\n\t\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}\nEXPORT_SYMBOL(__sk_receive_skb);\n\nstruct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = __sk_dst_get(sk);\n\n\tif (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {\n\t\tsk_tx_queue_clear(sk);\n\t\tRCU_INIT_POINTER(sk->sk_dst_cache, NULL);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(__sk_dst_check);\n\nstruct dst_entry *sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = sk_dst_get(sk);\n\n\tif (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {\n\t\tsk_dst_reset(sk);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(sk_dst_check);\n\nstatic int sock_setbindtodevice(struct sock *sk, char __user *optval,\n\t\t\t\tint optlen)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tstruct net *net = sock_net(sk);\n\tchar devname[IFNAMSIZ];\n\tint index;\n\n\t/* Sorry... */\n\tret = -EPERM;\n\tif (!ns_capable(net->user_ns, CAP_NET_RAW))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (optlen < 0)\n\t\tgoto out;\n\n\t/* Bind this socket to a particular device like \"eth0\",\n\t * as specified in the passed interface name. If the\n\t * name is \"\" or the option length is zero the socket\n\t * is not bound.\n\t */\n\tif (optlen > IFNAMSIZ - 1)\n\t\toptlen = IFNAMSIZ - 1;\n\tmemset(devname, 0, sizeof(devname));\n\n\tret = -EFAULT;\n\tif (copy_from_user(devname, optval, optlen))\n\t\tgoto out;\n\n\tindex = 0;\n\tif (devname[0] != '\\0') {\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_name_rcu(net, devname);\n\t\tif (dev)\n\t\t\tindex = dev->ifindex;\n\t\trcu_read_unlock();\n\t\tret = -ENODEV;\n\t\tif (!dev)\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tsk->sk_bound_dev_if = index;\n\tsk_dst_reset(sk);\n\trelease_sock(sk);\n\n\tret = 0;\n\nout:\n#endif\n\n\treturn ret;\n}\n\nstatic int sock_getbindtodevice(struct sock *sk, char __user *optval,\n\t\t\t\tint __user *optlen, int len)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tstruct net *net = sock_net(sk);\n\tchar devname[IFNAMSIZ];\n\n\tif (sk->sk_bound_dev_if == 0) {\n\t\tlen = 0;\n\t\tgoto zero;\n\t}\n\n\tret = -EINVAL;\n\tif (len < IFNAMSIZ)\n\t\tgoto out;\n\n\tret = netdev_get_name(net, devname, sk->sk_bound_dev_if);\n\tif (ret)\n\t\tgoto out;\n\n\tlen = strlen(devname) + 1;\n\n\tret = -EFAULT;\n\tif (copy_to_user(optval, devname, len))\n\t\tgoto out;\n\nzero:\n\tret = -EFAULT;\n\tif (put_user(len, optlen))\n\t\tgoto out;\n\n\tret = 0;\n\nout:\n#endif\n\n\treturn ret;\n}\n\nstatic inline void sock_valbool_flag(struct sock *sk, int bit, int valbool)\n{\n\tif (valbool)\n\t\tsock_set_flag(sk, bit);\n\telse\n\t\tsock_reset_flag(sk, bit);\n}\n\nbool sk_mc_loop(struct sock *sk)\n{\n\tif (dev_recursion_level())\n\t\treturn false;\n\tif (!sk)\n\t\treturn true;\n\tswitch (sk->sk_family) {\n\tcase AF_INET:\n\t\treturn inet_sk(sk)->mc_loop;\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase AF_INET6:\n\t\treturn inet6_sk(sk)->mc_loop;\n#endif\n\t}\n\tWARN_ON(1);\n\treturn true;\n}\nEXPORT_SYMBOL(sk_mc_loop);\n\n/*\n *\tThis is meant for all protocols to use and covers goings on\n *\tat the socket level. Everything here is generic.\n */\n\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_setbindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_REUSEPORT:\n\t\tsk->sk_reuseport = valbool;\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check_tx = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) ||\n\t\t    ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (val & SOF_TIMESTAMPING_OPT_ID &&\n\t\t    !(sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)) {\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t\t    sk->sk_type == SOCK_STREAM) {\n\t\t\t\tif ((1 << sk->sk_state) &\n\t\t\t\t    (TCPF_CLOSE | TCPF_LISTEN)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tsk->sk_tskey = tcp_sk(sk)->snd_una;\n\t\t\t} else {\n\t\t\t\tsk->sk_tskey = 0;\n\t\t\t}\n\t\t}\n\t\tsk->sk_tsflags = val;\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_ATTACH_BPF:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(u32)) {\n\t\t\tu32 ufd;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&ufd, optval, sizeof(ufd)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_bpf(ufd, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_ATTACH_REUSEPORT_CBPF:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_reuseport_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_ATTACH_REUSEPORT_EBPF:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(u32)) {\n\t\t\tu32 ufd;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&ufd, optval, sizeof(ufd)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_reuseport_attach_bpf(ufd, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_LOCK_FILTER:\n\t\tif (sock_flag(sk, SOCK_FILTER_LOCKED) && !valbool)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_FILTER_LOCKED, valbool);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tret = sock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tsock_valbool_flag(sk, SOCK_SELECT_ERR_QUEUE, valbool);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\t/* allow unprivileged users to decrease the value */\n\t\tif ((val > sk->sk_ll_usec) && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse {\n\t\t\tif (val < 0)\n\t\t\t\tret = -EINVAL;\n\t\t\telse\n\t\t\t\tsk->sk_ll_usec = val;\n\t\t}\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tsk->sk_max_pacing_rate = val;\n\t\tsk->sk_pacing_rate = min(sk->sk_pacing_rate,\n\t\t\t\t\t sk->sk_max_pacing_rate);\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tsk->sk_incoming_cpu = val;\n\t\tbreak;\n\n\tcase SO_CNX_ADVICE:\n\t\tif (val == 1)\n\t\t\tdst_negative_advice(sk);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\nEXPORT_SYMBOL(sock_setsockopt);\n\n\nstatic void cred_to_ucred(struct pid *pid, const struct cred *cred,\n\t\t\t  struct ucred *ucred)\n{\n\tucred->pid = pid_vnr(pid);\n\tucred->uid = ucred->gid = -1;\n\tif (cred) {\n\t\tstruct user_namespace *current_ns = current_user_ns();\n\n\t\tucred->uid = from_kuid_munged(current_ns, cred->euid);\n\t\tucred->gid = from_kgid_munged(current_ns, cred->egid);\n\t}\n}\n\nint sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tstruct linger ling;\n\t\tstruct timeval tm;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"getsockopt\");\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tv.val = sk->sk_tsflags;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tlv = sizeof(struct timeval);\n\t\tif (sk->sk_rcvtimeo == MAX_SCHEDULE_TIMEOUT) {\n\t\t\tv.tm.tv_sec = 0;\n\t\t\tv.tm.tv_usec = 0;\n\t\t} else {\n\t\t\tv.tm.tv_sec = sk->sk_rcvtimeo / HZ;\n\t\t\tv.tm.tv_usec = ((sk->sk_rcvtimeo % HZ) * 1000000) / HZ;\n\t\t}\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tlv = sizeof(struct timeval);\n\t\tif (sk->sk_sndtimeo == MAX_SCHEDULE_TIMEOUT) {\n\t\t\tv.tm.tv_sec = 0;\n\t\t\tv.tm.tv_usec = 0;\n\t\t} else {\n\t\t\tv.tm.tv_sec = sk->sk_sndtimeo / HZ;\n\t\t\tv.tm.tv_usec = ((sk->sk_sndtimeo % HZ) * 1000000) / HZ;\n\t\t}\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tif (sock->ops->getname(sock, (struct sockaddr *)address, &lv, 2))\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, (struct sock_filter __user *)optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = sk->sk_ll_usec;\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tv.val = sk->sk_max_pacing_rate;\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = sk->sk_incoming_cpu;\n\t\tbreak;\n\n\tdefault:\n\t\t/* We implement the SO_SNDLOWAT etc to not be settable\n\t\t * (1003.1g 7).\n\t\t */\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n * Initialize an sk_lock.\n *\n * (We also register the sk_lock with the lock validator.)\n */\nstatic inline void sock_lock_init(struct sock *sk)\n{\n\tsock_lock_init_class_and_name(sk,\n\t\t\taf_family_slock_key_strings[sk->sk_family],\n\t\t\taf_family_slock_keys + sk->sk_family,\n\t\t\taf_family_key_strings[sk->sk_family],\n\t\t\taf_family_keys + sk->sk_family);\n}\n\n/*\n * Copy all fields from osk to nsk but nsk->sk_refcnt must not change yet,\n * even temporarly, because of RCU lookups. sk_node should also be left as is.\n * We must not copy fields between sk_dontcopy_begin and sk_dontcopy_end\n */\nstatic void sock_copy(struct sock *nsk, const struct sock *osk)\n{\n#ifdef CONFIG_SECURITY_NETWORK\n\tvoid *sptr = nsk->sk_security;\n#endif\n\tmemcpy(nsk, osk, offsetof(struct sock, sk_dontcopy_begin));\n\n\tmemcpy(&nsk->sk_dontcopy_end, &osk->sk_dontcopy_end,\n\t       osk->sk_prot->obj_size - offsetof(struct sock, sk_dontcopy_end));\n\n#ifdef CONFIG_SECURITY_NETWORK\n\tnsk->sk_security = sptr;\n\tsecurity_sk_clone(osk, nsk);\n#endif\n}\n\nstatic struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,\n\t\tint family)\n{\n\tstruct sock *sk;\n\tstruct kmem_cache *slab;\n\n\tslab = prot->slab;\n\tif (slab != NULL) {\n\t\tsk = kmem_cache_alloc(slab, priority & ~__GFP_ZERO);\n\t\tif (!sk)\n\t\t\treturn sk;\n\t\tif (priority & __GFP_ZERO)\n\t\t\tsk_prot_clear_nulls(sk, prot->obj_size);\n\t} else\n\t\tsk = kmalloc(prot->obj_size, priority);\n\n\tif (sk != NULL) {\n\t\tkmemcheck_annotate_bitfield(sk, flags);\n\n\t\tif (security_sk_alloc(sk, family, priority))\n\t\t\tgoto out_free;\n\n\t\tif (!try_module_get(prot->owner))\n\t\t\tgoto out_free_sec;\n\t\tsk_tx_queue_clear(sk);\n\t}\n\n\treturn sk;\n\nout_free_sec:\n\tsecurity_sk_free(sk);\nout_free:\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\treturn NULL;\n}\n\nstatic void sk_prot_free(struct proto *prot, struct sock *sk)\n{\n\tstruct kmem_cache *slab;\n\tstruct module *owner;\n\n\towner = prot->owner;\n\tslab = prot->slab;\n\n\tcgroup_sk_free(&sk->sk_cgrp_data);\n\tmem_cgroup_sk_free(sk);\n\tsecurity_sk_free(sk);\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\tmodule_put(owner);\n}\n\n/**\n *\tsk_alloc - All socket objects are allocated here\n *\t@net: the applicable net namespace\n *\t@family: protocol family\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\t@prot: struct proto associated with this new sock instance\n *\t@kern: is this to be a kernel socket?\n */\nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\n\t\t      struct proto *prot, int kern)\n{\n\tstruct sock *sk;\n\n\tsk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);\n\tif (sk) {\n\t\tsk->sk_family = family;\n\t\t/*\n\t\t * See comment in struct sock definition to understand\n\t\t * why we need sk_prot_creator -acme\n\t\t */\n\t\tsk->sk_prot = sk->sk_prot_creator = prot;\n\t\tsock_lock_init(sk);\n\t\tsk->sk_net_refcnt = kern ? 0 : 1;\n\t\tif (likely(sk->sk_net_refcnt))\n\t\t\tget_net(net);\n\t\tsock_net_set(sk, net);\n\t\tatomic_set(&sk->sk_wmem_alloc, 1);\n\n\t\tmem_cgroup_sk_alloc(sk);\n\t\tcgroup_sk_alloc(&sk->sk_cgrp_data);\n\t\tsock_update_classid(&sk->sk_cgrp_data);\n\t\tsock_update_netprioidx(&sk->sk_cgrp_data);\n\t}\n\n\treturn sk;\n}\nEXPORT_SYMBOL(sk_alloc);\n\n/* Sockets having SOCK_RCU_FREE will call this function after one RCU\n * grace period. This is the case for UDP sockets and TCP listeners.\n */\nstatic void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       atomic_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\tif (rcu_access_pointer(sk->sk_reuseport_cb))\n\t\treuseport_detach_sock(sk);\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}\n\nvoid sk_destruct(struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_RCU_FREE))\n\t\tcall_rcu(&sk->sk_rcu, __sk_destruct);\n\telse\n\t\t__sk_destruct(&sk->sk_rcu);\n}\n\nstatic void __sk_free(struct sock *sk)\n{\n\tif (unlikely(sock_diag_has_destroy_listeners(sk) && sk->sk_net_refcnt))\n\t\tsock_diag_broadcast_destroy(sk);\n\telse\n\t\tsk_destruct(sk);\n}\n\nvoid sk_free(struct sock *sk)\n{\n\t/*\n\t * We subtract one from sk_wmem_alloc and can know if\n\t * some packets are still in some tx queue.\n\t * If not null, sock_wfree() will call __sk_free(sk) later\n\t */\n\tif (atomic_dec_and_test(&sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sk_free);\n\n/**\n *\tsk_clone_lock - clone a socket, and lock its clone\n *\t@sk: the socket to clone\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\n *\tCaller must unlock socket even in error path (bh_unlock_sock(newsk))\n */\nstruct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct sock *newsk;\n\tbool is_charged = true;\n\n\tnewsk = sk_prot_alloc(sk->sk_prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\t/* SANITY */\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\tatomic_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tskb_queue_head_init(&newsk->sk_receive_queue);\n\t\tskb_queue_head_init(&newsk->sk_write_queue);\n\n\t\trwlock_init(&newsk->sk_callback_lock);\n\t\tlockdep_set_class_and_name(&newsk->sk_callback_lock,\n\t\t\t\taf_callback_keys + newsk->sk_family,\n\t\t\t\taf_family_clock_key_strings[newsk->sk_family]);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tatomic_set(&newsk->sk_drops, 0);\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\t\tskb_queue_head_init(&newsk->sk_error_queue);\n\n\t\tfilter = rcu_dereference_protected(newsk->sk_filter, 1);\n\t\tif (filter != NULL)\n\t\t\t/* though it's an empty new sock, the charging may fail\n\t\t\t * if sysctl_optmem_max was changed between creation of\n\t\t\t * original socket and cloning\n\t\t\t */\n\t\t\tis_charged = sk_filter_charge(newsk, filter);\n\n\t\tif (unlikely(!is_charged || xfrm_sk_clone_policy(newsk, sk))) {\n\t\t\t/* It is still raw copy of parent, so invalidate\n\t\t\t * destructor and make plain sk_free() */\n\t\t\tnewsk->sk_destruct = NULL;\n\t\t\tbh_unlock_sock(newsk);\n\t\t\tsk_free(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tRCU_INIT_POINTER(newsk->sk_reuseport_cb, NULL);\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_err_soft = 0;\n\t\tnewsk->sk_priority = 0;\n\t\tnewsk->sk_incoming_cpu = raw_smp_processor_id();\n\t\tatomic64_set(&newsk->sk_cookie, 0);\n\n\t\tmem_cgroup_sk_alloc(newsk);\n\t\tcgroup_sk_alloc(&newsk->sk_cgrp_data);\n\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\tatomic_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tnewsk->sk_wq = NULL;\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tsk_sockets_allocated_inc(newsk);\n\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(sk_clone_lock);\n\nvoid sk_setup_caps(struct sock *sk, struct dst_entry *dst)\n{\n\tu32 max_segs = 1;\n\n\tsk_dst_set(sk, dst);\n\tsk->sk_route_caps = dst->dev->features;\n\tif (sk->sk_route_caps & NETIF_F_GSO)\n\t\tsk->sk_route_caps |= NETIF_F_GSO_SOFTWARE;\n\tsk->sk_route_caps &= ~sk->sk_route_nocaps;\n\tif (sk_can_gso(sk)) {\n\t\tif (dst->header_len) {\n\t\t\tsk->sk_route_caps &= ~NETIF_F_GSO_MASK;\n\t\t} else {\n\t\t\tsk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;\n\t\t\tsk->sk_gso_max_size = dst->dev->gso_max_size;\n\t\t\tmax_segs = max_t(u32, dst->dev->gso_max_segs, 1);\n\t\t}\n\t}\n\tsk->sk_gso_max_segs = max_segs;\n}\nEXPORT_SYMBOL_GPL(sk_setup_caps);\n\n/*\n *\tSimple resource managers for sockets.\n */\n\n\n/*\n * Write buffer destructor automatically called from kfree_skb.\n */\nvoid sock_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tif (!sock_flag(sk, SOCK_USE_WRITE_QUEUE)) {\n\t\t/*\n\t\t * Keep a reference on sk_wmem_alloc, this will be released\n\t\t * after sk_write_space() call\n\t\t */\n\t\tatomic_sub(len - 1, &sk->sk_wmem_alloc);\n\t\tsk->sk_write_space(sk);\n\t\tlen = 1;\n\t}\n\t/*\n\t * if sk_wmem_alloc reaches 0, we must finish what sk_free()\n\t * could not do because of in-flight packets\n\t */\n\tif (atomic_sub_and_test(len, &sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sock_wfree);\n\n/* This variant of sock_wfree() is used by TCP,\n * since it sets SOCK_USE_WRITE_QUEUE.\n */\nvoid __sock_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (atomic_sub_and_test(skb->truesize, &sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\n\nvoid skb_set_owner_w(struct sk_buff *skb, struct sock *sk)\n{\n\tskb_orphan(skb);\n\tskb->sk = sk;\n#ifdef CONFIG_INET\n\tif (unlikely(!sk_fullsock(sk))) {\n\t\tskb->destructor = sock_edemux;\n\t\tsock_hold(sk);\n\t\treturn;\n\t}\n#endif\n\tskb->destructor = sock_wfree;\n\tskb_set_hash_from_sk(skb, sk);\n\t/*\n\t * We used to take a refcount on sk, but following operation\n\t * is enough to guarantee sk_free() wont free this sock until\n\t * all in-flight packets are completed\n\t */\n\tatomic_add(skb->truesize, &sk->sk_wmem_alloc);\n}\nEXPORT_SYMBOL(skb_set_owner_w);\n\n/* This helper is used by netem, as it can hold packets in its\n * delay queue. We want to allow the owner socket to send more\n * packets, as if they were already TX completed by a typical driver.\n * But we also want to keep skb->sk set because some packet schedulers\n * rely on it (sch_fq for example). So we set skb->truesize to a small\n * amount (1) and decrease sk_wmem_alloc accordingly.\n */\nvoid skb_orphan_partial(struct sk_buff *skb)\n{\n\t/* If this skb is a TCP pure ACK or already went here,\n\t * we have nothing to do. 2 is already a very small truesize.\n\t */\n\tif (skb->truesize <= 2)\n\t\treturn;\n\n\t/* TCP stack sets skb->ooo_okay based on sk_wmem_alloc,\n\t * so we do not completely orphan skb, but transfert all\n\t * accounted bytes but one, to avoid unexpected reorders.\n\t */\n\tif (skb->destructor == sock_wfree\n#ifdef CONFIG_INET\n\t    || skb->destructor == tcp_wfree\n#endif\n\t\t) {\n\t\tatomic_sub(skb->truesize - 1, &skb->sk->sk_wmem_alloc);\n\t\tskb->truesize = 1;\n\t} else {\n\t\tskb_orphan(skb);\n\t}\n}\nEXPORT_SYMBOL(skb_orphan_partial);\n\n/*\n * Read buffer destructor automatically called from kfree_skb.\n */\nvoid sock_rfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tatomic_sub(len, &sk->sk_rmem_alloc);\n\tsk_mem_uncharge(sk, len);\n}\nEXPORT_SYMBOL(sock_rfree);\n\n/*\n * Buffer destructor for skbs that are not used directly in read or write\n * path, e.g. for error handler skbs. Automatically called from kfree_skb.\n */\nvoid sock_efree(struct sk_buff *skb)\n{\n\tsock_put(skb->sk);\n}\nEXPORT_SYMBOL(sock_efree);\n\nkuid_t sock_i_uid(struct sock *sk)\n{\n\tkuid_t uid;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tuid = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_uid : GLOBAL_ROOT_UID;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn uid;\n}\nEXPORT_SYMBOL(sock_i_uid);\n\nunsigned long sock_i_ino(struct sock *sk)\n{\n\tunsigned long ino;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tino = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_ino : 0;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn ino;\n}\nEXPORT_SYMBOL(sock_i_ino);\n\n/*\n * Allocate a skb from the socket's send buffer.\n */\nstruct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,\n\t\t\t     gfp_t priority)\n{\n\tif (force || atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n\t\tstruct sk_buff *skb = alloc_skb(size, priority);\n\t\tif (skb) {\n\t\t\tskb_set_owner_w(skb, sk);\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_wmalloc);\n\n/*\n * Allocate a memory block from the socket's option memory buffer.\n */\nvoid *sock_kmalloc(struct sock *sk, int size, gfp_t priority)\n{\n\tif ((unsigned int)size <= sysctl_optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + size < sysctl_optmem_max) {\n\t\tvoid *mem;\n\t\t/* First do the add, to avoid the race if kmalloc\n\t\t * might sleep.\n\t\t */\n\t\tatomic_add(size, &sk->sk_omem_alloc);\n\t\tmem = kmalloc(size, priority);\n\t\tif (mem)\n\t\t\treturn mem;\n\t\tatomic_sub(size, &sk->sk_omem_alloc);\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_kmalloc);\n\n/* Free an option memory block. Note, we actually want the inline\n * here as this allows gcc to detect the nullify and fold away the\n * condition entirely.\n */\nstatic inline void __sock_kfree_s(struct sock *sk, void *mem, int size,\n\t\t\t\t  const bool nullify)\n{\n\tif (WARN_ON_ONCE(!mem))\n\t\treturn;\n\tif (nullify)\n\t\tkzfree(mem);\n\telse\n\t\tkfree(mem);\n\tatomic_sub(size, &sk->sk_omem_alloc);\n}\n\nvoid sock_kfree_s(struct sock *sk, void *mem, int size)\n{\n\t__sock_kfree_s(sk, mem, size, false);\n}\nEXPORT_SYMBOL(sock_kfree_s);\n\nvoid sock_kzfree_s(struct sock *sk, void *mem, int size)\n{\n\t__sock_kfree_s(sk, mem, size, true);\n}\nEXPORT_SYMBOL(sock_kzfree_s);\n\n/* It is almost wait_for_tcp_memory minus release_sock/lock_sock.\n   I think, these locks should be removed for datagram sockets.\n */\nstatic long sock_wait_for_wmem(struct sock *sk, long timeo)\n{\n\tDEFINE_WAIT(wait);\n\n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\tfor (;;) {\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf)\n\t\t\tbreak;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tbreak;\n\t\tif (sk->sk_err)\n\t\t\tbreak;\n\t\ttimeo = schedule_timeout(timeo);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn timeo;\n}\n\n\n/*\n *\tGeneric send/receive buffer handlers\n */\n\nstruct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,\n\t\t\t\t     unsigned long data_len, int noblock,\n\t\t\t\t     int *errcode, int max_page_order)\n{\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tint err;\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\tfor (;;) {\n\t\terr = sock_error(sk);\n\t\tif (err != 0)\n\t\t\tgoto failure;\n\n\t\terr = -EPIPE;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tgoto failure;\n\n\t\tif (sk_wmem_alloc_get(sk) < sk->sk_sndbuf)\n\t\t\tbreak;\n\n\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto failure;\n\t\tif (signal_pending(current))\n\t\t\tgoto interrupted;\n\t\ttimeo = sock_wait_for_wmem(sk, timeo);\n\t}\n\tskb = alloc_skb_with_frags(header_len, data_len, max_page_order,\n\t\t\t\t   errcode, sk->sk_allocation);\n\tif (skb)\n\t\tskb_set_owner_w(skb, sk);\n\treturn skb;\n\ninterrupted:\n\terr = sock_intr_errno(timeo);\nfailure:\n\t*errcode = err;\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_alloc_send_pskb);\n\nstruct sk_buff *sock_alloc_send_skb(struct sock *sk, unsigned long size,\n\t\t\t\t    int noblock, int *errcode)\n{\n\treturn sock_alloc_send_pskb(sk, size, 0, noblock, errcode, 0);\n}\nEXPORT_SYMBOL(sock_alloc_send_skb);\n\nint __sock_cmsg_send(struct sock *sk, struct msghdr *msg, struct cmsghdr *cmsg,\n\t\t     struct sockcm_cookie *sockc)\n{\n\tu32 tsflags;\n\n\tswitch (cmsg->cmsg_type) {\n\tcase SO_MARK:\n\t\tif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))\n\t\t\treturn -EINVAL;\n\t\tsockc->mark = *(u32 *)CMSG_DATA(cmsg);\n\t\tbreak;\n\tcase SO_TIMESTAMPING:\n\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))\n\t\t\treturn -EINVAL;\n\n\t\ttsflags = *(u32 *)CMSG_DATA(cmsg);\n\t\tif (tsflags & ~SOF_TIMESTAMPING_TX_RECORD_MASK)\n\t\t\treturn -EINVAL;\n\n\t\tsockc->tsflags &= ~SOF_TIMESTAMPING_TX_RECORD_MASK;\n\t\tsockc->tsflags |= tsflags;\n\t\tbreak;\n\t/* SCM_RIGHTS and SCM_CREDENTIALS are semantically in SOL_UNIX. */\n\tcase SCM_RIGHTS:\n\tcase SCM_CREDENTIALS:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__sock_cmsg_send);\n\nint sock_cmsg_send(struct sock *sk, struct msghdr *msg,\n\t\t   struct sockcm_cookie *sockc)\n{\n\tstruct cmsghdr *cmsg;\n\tint ret;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\t\tif (cmsg->cmsg_level != SOL_SOCKET)\n\t\t\tcontinue;\n\t\tret = __sock_cmsg_send(sk, msg, cmsg, sockc);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_cmsg_send);\n\n/* On 32bit arches, an skb frag is limited to 2^15 */\n#define SKB_FRAG_PAGE_ORDER\tget_order(32768)\n\n/**\n * skb_page_frag_refill - check that a page_frag contains enough room\n * @sz: minimum size of the fragment we want to get\n * @pfrag: pointer to page_frag\n * @gfp: priority for memory allocation\n *\n * Note: While this allocator tries to use high order pages, there is\n * no guarantee that allocations succeed. Therefore, @sz MUST be\n * less or equal than PAGE_SIZE.\n */\nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)\n{\n\tif (pfrag->page) {\n\t\tif (page_ref_count(pfrag->page) == 1) {\n\t\t\tpfrag->offset = 0;\n\t\t\treturn true;\n\t\t}\n\t\tif (pfrag->offset + sz <= pfrag->size)\n\t\t\treturn true;\n\t\tput_page(pfrag->page);\n\t}\n\n\tpfrag->offset = 0;\n\tif (SKB_FRAG_PAGE_ORDER) {\n\t\t/* Avoid direct reclaim but allow kswapd to wake */\n\t\tpfrag->page = alloc_pages((gfp & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t\t  __GFP_COMP | __GFP_NOWARN |\n\t\t\t\t\t  __GFP_NORETRY,\n\t\t\t\t\t  SKB_FRAG_PAGE_ORDER);\n\t\tif (likely(pfrag->page)) {\n\t\t\tpfrag->size = PAGE_SIZE << SKB_FRAG_PAGE_ORDER;\n\t\t\treturn true;\n\t\t}\n\t}\n\tpfrag->page = alloc_page(gfp);\n\tif (likely(pfrag->page)) {\n\t\tpfrag->size = PAGE_SIZE;\n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL(skb_page_frag_refill);\n\nbool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag)\n{\n\tif (likely(skb_page_frag_refill(32U, pfrag, sk->sk_allocation)))\n\t\treturn true;\n\n\tsk_enter_memory_pressure(sk);\n\tsk_stream_moderate_sndbuf(sk);\n\treturn false;\n}\nEXPORT_SYMBOL(sk_page_frag_refill);\n\nstatic void __lock_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(&sk->sk_lock.wq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\t\tschedule();\n\t\tspin_lock_bh(&sk->sk_lock.slock);\n\t\tif (!sock_owned_by_user(sk))\n\t\t\tbreak;\n\t}\n\tfinish_wait(&sk->sk_lock.wq, &wait);\n}\n\nstatic void __release_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tstruct sk_buff *skb, *next;\n\n\twhile ((skb = sk->sk_backlog.head) != NULL) {\n\t\tsk->sk_backlog.head = sk->sk_backlog.tail = NULL;\n\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\n\t\tdo {\n\t\t\tnext = skb->next;\n\t\t\tprefetch(next);\n\t\t\tWARN_ON_ONCE(skb_dst_is_noref(skb));\n\t\t\tskb->next = NULL;\n\t\t\tsk_backlog_rcv(sk, skb);\n\n\t\t\tcond_resched();\n\n\t\t\tskb = next;\n\t\t} while (skb != NULL);\n\n\t\tspin_lock_bh(&sk->sk_lock.slock);\n\t}\n\n\t/*\n\t * Doing the zeroing here guarantee we can not loop forever\n\t * while a wild producer attempts to flood us.\n\t */\n\tsk->sk_backlog.len = 0;\n}\n\nvoid __sk_flush_backlog(struct sock *sk)\n{\n\tspin_lock_bh(&sk->sk_lock.slock);\n\t__release_sock(sk);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\n\n/**\n * sk_wait_data - wait for data to arrive at sk_receive_queue\n * @sk:    sock to wait on\n * @timeo: for how long\n * @skb:   last skb seen on sk_receive_queue\n *\n * Now socket state including sk->sk_err is changed only under lock,\n * hence we may omit checks after joining wait queue.\n * We check receive queue before schedule() only as optimization;\n * it is very likely that release_sock() added new data.\n */\nint sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb)\n{\n\tint rc;\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\tsk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\trc = sk_wait_event(sk, timeo, skb_peek_tail(&sk->sk_receive_queue) != skb);\n\tsk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn rc;\n}\nEXPORT_SYMBOL(sk_wait_data);\n\n/**\n *\t__sk_mem_schedule - increase sk_forward_alloc and memory_allocated\n *\t@sk: socket\n *\t@size: memory size to allocate\n *\t@kind: allocation type\n *\n *\tIf kind is SK_MEM_SEND, it means wmem allocation. Otherwise it means\n *\trmem allocation. This function assumes that protocols which have\n *\tmemory_pressure use sk_wmem_queued as write buffer accounting.\n */\nint __sk_mem_schedule(struct sock *sk, int size, int kind)\n{\n\tstruct proto *prot = sk->sk_prot;\n\tint amt = sk_mem_pages(size);\n\tlong allocated;\n\n\tsk->sk_forward_alloc += amt * SK_MEM_QUANTUM;\n\n\tallocated = sk_memory_allocated_add(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg &&\n\t    !mem_cgroup_charge_skmem(sk->sk_memcg, amt))\n\t\tgoto suppress_allocation;\n\n\t/* Under limit. */\n\tif (allocated <= sk_prot_mem_limits(sk, 0)) {\n\t\tsk_leave_memory_pressure(sk);\n\t\treturn 1;\n\t}\n\n\t/* Under pressure. */\n\tif (allocated > sk_prot_mem_limits(sk, 1))\n\t\tsk_enter_memory_pressure(sk);\n\n\t/* Over hard limit. */\n\tif (allocated > sk_prot_mem_limits(sk, 2))\n\t\tgoto suppress_allocation;\n\n\t/* guarantee minimum buffer size under pressure */\n\tif (kind == SK_MEM_RECV) {\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < prot->sysctl_rmem[0])\n\t\t\treturn 1;\n\n\t} else { /* SK_MEM_SEND */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tif (sk->sk_wmem_queued < prot->sysctl_wmem[0])\n\t\t\t\treturn 1;\n\t\t} else if (atomic_read(&sk->sk_wmem_alloc) <\n\t\t\t   prot->sysctl_wmem[0])\n\t\t\t\treturn 1;\n\t}\n\n\tif (sk_has_memory_pressure(sk)) {\n\t\tint alloc;\n\n\t\tif (!sk_under_memory_pressure(sk))\n\t\t\treturn 1;\n\t\talloc = sk_sockets_allocated_read_positive(sk);\n\t\tif (sk_prot_mem_limits(sk, 2) > alloc *\n\t\t    sk_mem_pages(sk->sk_wmem_queued +\n\t\t\t\t atomic_read(&sk->sk_rmem_alloc) +\n\t\t\t\t sk->sk_forward_alloc))\n\t\t\treturn 1;\n\t}\n\nsuppress_allocation:\n\n\tif (kind == SK_MEM_SEND && sk->sk_type == SOCK_STREAM) {\n\t\tsk_stream_moderate_sndbuf(sk);\n\n\t\t/* Fail only if socket is _under_ its sndbuf.\n\t\t * In this case we cannot block, so that we have to fail.\n\t\t */\n\t\tif (sk->sk_wmem_queued + size >= sk->sk_sndbuf)\n\t\t\treturn 1;\n\t}\n\n\ttrace_sock_exceed_buf_limit(sk, prot, allocated);\n\n\t/* Alas. Undo changes. */\n\tsk->sk_forward_alloc -= amt * SK_MEM_QUANTUM;\n\n\tsk_memory_allocated_sub(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_uncharge_skmem(sk->sk_memcg, amt);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(__sk_mem_schedule);\n\n/**\n *\t__sk_mem_reclaim - reclaim memory_allocated\n *\t@sk: socket\n *\t@amount: number of bytes (rounded down to a SK_MEM_QUANTUM multiple)\n */\nvoid __sk_mem_reclaim(struct sock *sk, int amount)\n{\n\tamount >>= SK_MEM_QUANTUM_SHIFT;\n\tsk_memory_allocated_sub(sk, amount);\n\tsk->sk_forward_alloc -= amount << SK_MEM_QUANTUM_SHIFT;\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_uncharge_skmem(sk->sk_memcg, amount);\n\n\tif (sk_under_memory_pressure(sk) &&\n\t    (sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)))\n\t\tsk_leave_memory_pressure(sk);\n}\nEXPORT_SYMBOL(__sk_mem_reclaim);\n\nint sk_set_peek_off(struct sock *sk, int val)\n{\n\tif (val < 0)\n\t\treturn -EINVAL;\n\n\tsk->sk_peek_off = val;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sk_set_peek_off);\n\n/*\n * Set of default routines for initialising struct proto_ops when\n * the protocol does not support a particular function. In certain\n * cases where it makes no sense for a protocol to have a \"do nothing\"\n * function, some default processing is provided.\n */\n\nint sock_no_bind(struct socket *sock, struct sockaddr *saddr, int len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_bind);\n\nint sock_no_connect(struct socket *sock, struct sockaddr *saddr,\n\t\t    int len, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_connect);\n\nint sock_no_socketpair(struct socket *sock1, struct socket *sock2)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_socketpair);\n\nint sock_no_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_accept);\n\nint sock_no_getname(struct socket *sock, struct sockaddr *saddr,\n\t\t    int *len, int peer)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getname);\n\nunsigned int sock_no_poll(struct file *file, struct socket *sock, poll_table *pt)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_no_poll);\n\nint sock_no_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_ioctl);\n\nint sock_no_listen(struct socket *sock, int backlog)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_listen);\n\nint sock_no_shutdown(struct socket *sock, int how)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_shutdown);\n\nint sock_no_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_setsockopt);\n\nint sock_no_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getsockopt);\n\nint sock_no_sendmsg(struct socket *sock, struct msghdr *m, size_t len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_sendmsg);\n\nint sock_no_recvmsg(struct socket *sock, struct msghdr *m, size_t len,\n\t\t    int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_recvmsg);\n\nint sock_no_mmap(struct file *file, struct socket *sock, struct vm_area_struct *vma)\n{\n\t/* Mirror missing mmap method error code */\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL(sock_no_mmap);\n\nssize_t sock_no_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags)\n{\n\tssize_t res;\n\tstruct msghdr msg = {.msg_flags = flags};\n\tstruct kvec iov;\n\tchar *kaddr = kmap(page);\n\tiov.iov_base = kaddr + offset;\n\tiov.iov_len = size;\n\tres = kernel_sendmsg(sock, &msg, &iov, 1, size);\n\tkunmap(page);\n\treturn res;\n}\nEXPORT_SYMBOL(sock_no_sendpage);\n\n/*\n *\tDefault Socket Callbacks\n */\n\nstatic void sock_def_wakeup(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_all(&wq->wait);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_error_report(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_poll(&wq->wait, POLLERR);\n\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_ERR);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_readable(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_sync_poll(&wq->wait, POLLIN | POLLPRI |\n\t\t\t\t\t\tPOLLRDNORM | POLLRDBAND);\n\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_write_space(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\n\t/* Do not wake up a writer until he can make \"significant\"\n\t * progress.  --DaveM\n\t */\n\tif ((atomic_read(&sk->sk_wmem_alloc) << 1) <= sk->sk_sndbuf) {\n\t\twq = rcu_dereference(sk->sk_wq);\n\t\tif (skwq_has_sleeper(wq))\n\t\t\twake_up_interruptible_sync_poll(&wq->wait, POLLOUT |\n\t\t\t\t\t\tPOLLWRNORM | POLLWRBAND);\n\n\t\t/* Should agree with poll, otherwise some programs break */\n\t\tif (sock_writeable(sk))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\t}\n\n\trcu_read_unlock();\n}\n\nstatic void sock_def_destruct(struct sock *sk)\n{\n}\n\nvoid sk_send_sigurg(struct sock *sk)\n{\n\tif (sk->sk_socket && sk->sk_socket->file)\n\t\tif (send_sigurg(&sk->sk_socket->file->f_owner))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_URG, POLL_PRI);\n}\nEXPORT_SYMBOL(sk_send_sigurg);\n\nvoid sk_reset_timer(struct sock *sk, struct timer_list* timer,\n\t\t    unsigned long expires)\n{\n\tif (!mod_timer(timer, expires))\n\t\tsock_hold(sk);\n}\nEXPORT_SYMBOL(sk_reset_timer);\n\nvoid sk_stop_timer(struct sock *sk, struct timer_list* timer)\n{\n\tif (del_timer(timer))\n\t\t__sock_put(sk);\n}\nEXPORT_SYMBOL(sk_stop_timer);\n\nvoid sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n\n\tsk->sk_send_head\t=\tNULL;\n\n\tinit_timer(&sk->sk_timer);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tsk->sk_wq\t=\tsock->wq;\n\t\tsock->sk\t=\tsk;\n\t} else\n\t\tsk->sk_wq\t=\tNULL;\n\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = ktime_set(-1L, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tsysctl_net_busy_read;\n#endif\n\n\tsk->sk_max_pacing_rate = ~0U;\n\tsk->sk_pacing_rate = ~0U;\n\tsk->sk_incoming_cpu = -1;\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t */\n\tsmp_wmb();\n\tatomic_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}\nEXPORT_SYMBOL(sock_init_data);\n\nvoid lock_sock_nested(struct sock *sk, int subclass)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_lock.owned)\n\t\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock(&sk->sk_lock.slock);\n\t/*\n\t * The sk_lock has mutex_lock() semantics here:\n\t */\n\tmutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);\n\tlocal_bh_enable();\n}\nEXPORT_SYMBOL(lock_sock_nested);\n\nvoid release_sock(struct sock *sk)\n{\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_backlog.tail)\n\t\t__release_sock(sk);\n\n\t/* Warning : release_cb() might need to release sk ownership,\n\t * ie call sock_release_ownership(sk) before us.\n\t */\n\tif (sk->sk_prot->release_cb)\n\t\tsk->sk_prot->release_cb(sk);\n\n\tsock_release_ownership(sk);\n\tif (waitqueue_active(&sk->sk_lock.wq))\n\t\twake_up(&sk->sk_lock.wq);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\nEXPORT_SYMBOL(release_sock);\n\n/**\n * lock_sock_fast - fast version of lock_sock\n * @sk: socket\n *\n * This version should be used for very small section, where process wont block\n * return false if fast path is taken\n *   sk_lock.slock locked, owned = 0, BH disabled\n * return true if slow path is taken\n *   sk_lock.slock unlocked, owned = 1, BH enabled\n */\nbool lock_sock_fast(struct sock *sk)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\n\tif (!sk->sk_lock.owned)\n\t\t/*\n\t\t * Note : We must disable BH\n\t\t */\n\t\treturn false;\n\n\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock(&sk->sk_lock.slock);\n\t/*\n\t * The sk_lock has mutex_lock() semantics here:\n\t */\n\tmutex_acquire(&sk->sk_lock.dep_map, 0, 0, _RET_IP_);\n\tlocal_bh_enable();\n\treturn true;\n}\nEXPORT_SYMBOL(lock_sock_fast);\n\nint sock_get_timestamp(struct sock *sk, struct timeval __user *userstamp)\n{\n\tstruct timeval tv;\n\tif (!sock_flag(sk, SOCK_TIMESTAMP))\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\ttv = ktime_to_timeval(sk->sk_stamp);\n\tif (tv.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (tv.tv_sec == 0) {\n\t\tsk->sk_stamp = ktime_get_real();\n\t\ttv = ktime_to_timeval(sk->sk_stamp);\n\t}\n\treturn copy_to_user(userstamp, &tv, sizeof(tv)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(sock_get_timestamp);\n\nint sock_get_timestampns(struct sock *sk, struct timespec __user *userstamp)\n{\n\tstruct timespec ts;\n\tif (!sock_flag(sk, SOCK_TIMESTAMP))\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\tts = ktime_to_timespec(sk->sk_stamp);\n\tif (ts.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (ts.tv_sec == 0) {\n\t\tsk->sk_stamp = ktime_get_real();\n\t\tts = ktime_to_timespec(sk->sk_stamp);\n\t}\n\treturn copy_to_user(userstamp, &ts, sizeof(ts)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(sock_get_timestampns);\n\nvoid sock_enable_timestamp(struct sock *sk, int flag)\n{\n\tif (!sock_flag(sk, flag)) {\n\t\tunsigned long previous_flags = sk->sk_flags;\n\n\t\tsock_set_flag(sk, flag);\n\t\t/*\n\t\t * we just set one of the two flags which require net\n\t\t * time stamping, but time stamping might have been on\n\t\t * already because of the other one\n\t\t */\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    !(previous_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_enable_timestamp();\n\t}\n}\n\nint sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len,\n\t\t       int level, int type)\n{\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\n\terr = -EAGAIN;\n\tskb = sock_dequeue_err_skb(sk);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\tput_cmsg(msg, level, type, sizeof(serr->ee), &serr->ee);\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(sock_recv_errqueue);\n\n/*\n *\tGet a socket option on an socket.\n *\n *\tFIX: POSIX 1003.1g is very ambiguous here. It states that\n *\tasynchronous errors should be reported by getsockopt. We assume\n *\tthis means if you specify SO_ERROR (otherwise whats the point of it).\n */\nint sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_prot->compat_getsockopt != NULL)\n\t\treturn sk->sk_prot->compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t      optval, optlen);\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_sock_common_getsockopt);\n#endif\n\nint sock_common_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\t\tint flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\terr = sk->sk_prot->recvmsg(sk, msg, size, flags & MSG_DONTWAIT,\n\t\t\t\t   flags & ~MSG_DONTWAIT, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(sock_common_recvmsg);\n\n/*\n *\tSet socket options on an inet socket.\n */\nint sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_prot->compat_setsockopt != NULL)\n\t\treturn sk->sk_prot->compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t      optval, optlen);\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_sock_common_setsockopt);\n#endif\n\nvoid sk_common_release(struct sock *sk)\n{\n\tif (sk->sk_prot->destroy)\n\t\tsk->sk_prot->destroy(sk);\n\n\t/*\n\t * Observation: when sock_common_release is called, processes have\n\t * no access to socket. But net still has.\n\t * Step one, detach it from networking:\n\t *\n\t * A. Remove from hash tables.\n\t */\n\n\tsk->sk_prot->unhash(sk);\n\n\t/*\n\t * In this point socket cannot receive new packets, but it is possible\n\t * that some packets are in flight because some CPU runs receiver and\n\t * did hash table lookup before we unhashed socket. They will achieve\n\t * receive queue and will be purged by socket destructor.\n\t *\n\t * Also we still have packets pending on receive queue and probably,\n\t * our own packets waiting in device queues. sock_destroy will drain\n\t * receive queue, but transmitted packets will delay socket destruction\n\t * until the last reference will be released.\n\t */\n\n\tsock_orphan(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsk_refcnt_debug_release(sk);\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(sk_common_release);\n\n#ifdef CONFIG_PROC_FS\n#define PROTO_INUSE_NR\t64\t/* should be enough for the first time */\nstruct prot_inuse {\n\tint val[PROTO_INUSE_NR];\n};\n\nstatic DECLARE_BITMAP(proto_inuse_idx, PROTO_INUSE_NR);\n\n#ifdef CONFIG_NET_NS\nvoid sock_prot_inuse_add(struct net *net, struct proto *prot, int val)\n{\n\t__this_cpu_add(net->core.inuse->val[prot->inuse_idx], val);\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_add);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu_ptr(net->core.inuse, cpu)->val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n\nstatic int __net_init sock_inuse_init_net(struct net *net)\n{\n\tnet->core.inuse = alloc_percpu(struct prot_inuse);\n\treturn net->core.inuse ? 0 : -ENOMEM;\n}\n\nstatic void __net_exit sock_inuse_exit_net(struct net *net)\n{\n\tfree_percpu(net->core.inuse);\n}\n\nstatic struct pernet_operations net_inuse_ops = {\n\t.init = sock_inuse_init_net,\n\t.exit = sock_inuse_exit_net,\n};\n\nstatic __init int net_inuse_init(void)\n{\n\tif (register_pernet_subsys(&net_inuse_ops))\n\t\tpanic(\"Cannot initialize net inuse counters\");\n\n\treturn 0;\n}\n\ncore_initcall(net_inuse_init);\n#else\nstatic DEFINE_PER_CPU(struct prot_inuse, prot_inuse);\n\nvoid sock_prot_inuse_add(struct net *net, struct proto *prot, int val)\n{\n\t__this_cpu_add(prot_inuse.val[prot->inuse_idx], val);\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_add);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu(prot_inuse, cpu).val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n#endif\n\nstatic void assign_proto_idx(struct proto *prot)\n{\n\tprot->inuse_idx = find_first_zero_bit(proto_inuse_idx, PROTO_INUSE_NR);\n\n\tif (unlikely(prot->inuse_idx == PROTO_INUSE_NR - 1)) {\n\t\tpr_err(\"PROTO_INUSE_NR exhausted\\n\");\n\t\treturn;\n\t}\n\n\tset_bit(prot->inuse_idx, proto_inuse_idx);\n}\n\nstatic void release_proto_idx(struct proto *prot)\n{\n\tif (prot->inuse_idx != PROTO_INUSE_NR - 1)\n\t\tclear_bit(prot->inuse_idx, proto_inuse_idx);\n}\n#else\nstatic inline void assign_proto_idx(struct proto *prot)\n{\n}\n\nstatic inline void release_proto_idx(struct proto *prot)\n{\n}\n#endif\n\nstatic void req_prot_cleanup(struct request_sock_ops *rsk_prot)\n{\n\tif (!rsk_prot)\n\t\treturn;\n\tkfree(rsk_prot->slab_name);\n\trsk_prot->slab_name = NULL;\n\tkmem_cache_destroy(rsk_prot->slab);\n\trsk_prot->slab = NULL;\n}\n\nstatic int req_prot_init(const struct proto *prot)\n{\n\tstruct request_sock_ops *rsk_prot = prot->rsk_prot;\n\n\tif (!rsk_prot)\n\t\treturn 0;\n\n\trsk_prot->slab_name = kasprintf(GFP_KERNEL, \"request_sock_%s\",\n\t\t\t\t\tprot->name);\n\tif (!rsk_prot->slab_name)\n\t\treturn -ENOMEM;\n\n\trsk_prot->slab = kmem_cache_create(rsk_prot->slab_name,\n\t\t\t\t\t   rsk_prot->obj_size, 0,\n\t\t\t\t\t   prot->slab_flags, NULL);\n\n\tif (!rsk_prot->slab) {\n\t\tpr_crit(\"%s: Can't create request sock SLAB cache!\\n\",\n\t\t\tprot->name);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nint proto_register(struct proto *prot, int alloc_slab)\n{\n\tif (alloc_slab) {\n\t\tprot->slab = kmem_cache_create(prot->name, prot->obj_size, 0,\n\t\t\t\t\tSLAB_HWCACHE_ALIGN | prot->slab_flags,\n\t\t\t\t\tNULL);\n\n\t\tif (prot->slab == NULL) {\n\t\t\tpr_crit(\"%s: Can't create sock SLAB cache!\\n\",\n\t\t\t\tprot->name);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (req_prot_init(prot))\n\t\t\tgoto out_free_request_sock_slab;\n\n\t\tif (prot->twsk_prot != NULL) {\n\t\t\tprot->twsk_prot->twsk_slab_name = kasprintf(GFP_KERNEL, \"tw_sock_%s\", prot->name);\n\n\t\t\tif (prot->twsk_prot->twsk_slab_name == NULL)\n\t\t\t\tgoto out_free_request_sock_slab;\n\n\t\t\tprot->twsk_prot->twsk_slab =\n\t\t\t\tkmem_cache_create(prot->twsk_prot->twsk_slab_name,\n\t\t\t\t\t\t  prot->twsk_prot->twsk_obj_size,\n\t\t\t\t\t\t  0,\n\t\t\t\t\t\t  prot->slab_flags,\n\t\t\t\t\t\t  NULL);\n\t\t\tif (prot->twsk_prot->twsk_slab == NULL)\n\t\t\t\tgoto out_free_timewait_sock_slab_name;\n\t\t}\n\t}\n\n\tmutex_lock(&proto_list_mutex);\n\tlist_add(&prot->node, &proto_list);\n\tassign_proto_idx(prot);\n\tmutex_unlock(&proto_list_mutex);\n\treturn 0;\n\nout_free_timewait_sock_slab_name:\n\tkfree(prot->twsk_prot->twsk_slab_name);\nout_free_request_sock_slab:\n\treq_prot_cleanup(prot->rsk_prot);\n\n\tkmem_cache_destroy(prot->slab);\n\tprot->slab = NULL;\nout:\n\treturn -ENOBUFS;\n}\nEXPORT_SYMBOL(proto_register);\n\nvoid proto_unregister(struct proto *prot)\n{\n\tmutex_lock(&proto_list_mutex);\n\trelease_proto_idx(prot);\n\tlist_del(&prot->node);\n\tmutex_unlock(&proto_list_mutex);\n\n\tkmem_cache_destroy(prot->slab);\n\tprot->slab = NULL;\n\n\treq_prot_cleanup(prot->rsk_prot);\n\n\tif (prot->twsk_prot != NULL && prot->twsk_prot->twsk_slab != NULL) {\n\t\tkmem_cache_destroy(prot->twsk_prot->twsk_slab);\n\t\tkfree(prot->twsk_prot->twsk_slab_name);\n\t\tprot->twsk_prot->twsk_slab = NULL;\n\t}\n}\nEXPORT_SYMBOL(proto_unregister);\n\n#ifdef CONFIG_PROC_FS\nstatic void *proto_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(proto_list_mutex)\n{\n\tmutex_lock(&proto_list_mutex);\n\treturn seq_list_start_head(&proto_list, *pos);\n}\n\nstatic void *proto_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\treturn seq_list_next(v, &proto_list, pos);\n}\n\nstatic void proto_seq_stop(struct seq_file *seq, void *v)\n\t__releases(proto_list_mutex)\n{\n\tmutex_unlock(&proto_list_mutex);\n}\n\nstatic char proto_method_implemented(const void *method)\n{\n\treturn method == NULL ? 'n' : 'y';\n}\nstatic long sock_prot_memory_allocated(struct proto *proto)\n{\n\treturn proto->memory_allocated != NULL ? proto_memory_allocated(proto) : -1L;\n}\n\nstatic char *sock_prot_memory_pressure(struct proto *proto)\n{\n\treturn proto->memory_pressure != NULL ?\n\tproto_memory_pressure(proto) ? \"yes\" : \"no\" : \"NI\";\n}\n\nstatic void proto_seq_printf(struct seq_file *seq, struct proto *proto)\n{\n\n\tseq_printf(seq, \"%-9s %4u %6d  %6ld   %-3s %6u   %-3s  %-10s \"\n\t\t\t\"%2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c\\n\",\n\t\t   proto->name,\n\t\t   proto->obj_size,\n\t\t   sock_prot_inuse_get(seq_file_net(seq), proto),\n\t\t   sock_prot_memory_allocated(proto),\n\t\t   sock_prot_memory_pressure(proto),\n\t\t   proto->max_header,\n\t\t   proto->slab == NULL ? \"no\" : \"yes\",\n\t\t   module_name(proto->owner),\n\t\t   proto_method_implemented(proto->close),\n\t\t   proto_method_implemented(proto->connect),\n\t\t   proto_method_implemented(proto->disconnect),\n\t\t   proto_method_implemented(proto->accept),\n\t\t   proto_method_implemented(proto->ioctl),\n\t\t   proto_method_implemented(proto->init),\n\t\t   proto_method_implemented(proto->destroy),\n\t\t   proto_method_implemented(proto->shutdown),\n\t\t   proto_method_implemented(proto->setsockopt),\n\t\t   proto_method_implemented(proto->getsockopt),\n\t\t   proto_method_implemented(proto->sendmsg),\n\t\t   proto_method_implemented(proto->recvmsg),\n\t\t   proto_method_implemented(proto->sendpage),\n\t\t   proto_method_implemented(proto->bind),\n\t\t   proto_method_implemented(proto->backlog_rcv),\n\t\t   proto_method_implemented(proto->hash),\n\t\t   proto_method_implemented(proto->unhash),\n\t\t   proto_method_implemented(proto->get_port),\n\t\t   proto_method_implemented(proto->enter_memory_pressure));\n}\n\nstatic int proto_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == &proto_list)\n\t\tseq_printf(seq, \"%-9s %-4s %-8s %-6s %-5s %-7s %-4s %-10s %s\",\n\t\t\t   \"protocol\",\n\t\t\t   \"size\",\n\t\t\t   \"sockets\",\n\t\t\t   \"memory\",\n\t\t\t   \"press\",\n\t\t\t   \"maxhdr\",\n\t\t\t   \"slab\",\n\t\t\t   \"module\",\n\t\t\t   \"cl co di ac io in de sh ss gs se re sp bi br ha uh gp em\\n\");\n\telse\n\t\tproto_seq_printf(seq, list_entry(v, struct proto, node));\n\treturn 0;\n}\n\nstatic const struct seq_operations proto_seq_ops = {\n\t.start  = proto_seq_start,\n\t.next   = proto_seq_next,\n\t.stop   = proto_seq_stop,\n\t.show   = proto_seq_show,\n};\n\nstatic int proto_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &proto_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations proto_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= proto_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\nstatic __net_init int proto_init_net(struct net *net)\n{\n\tif (!proc_create(\"protocols\", S_IRUGO, net->proc_net, &proto_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic __net_exit void proto_exit_net(struct net *net)\n{\n\tremove_proc_entry(\"protocols\", net->proc_net);\n}\n\n\nstatic __net_initdata struct pernet_operations proto_net_ops = {\n\t.init = proto_init_net,\n\t.exit = proto_exit_net,\n};\n\nstatic int __init proto_init(void)\n{\n\treturn register_pernet_subsys(&proto_net_ops);\n}\n\nsubsys_initcall(proto_init);\n\n#endif /* PROC_FS */\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tGeneric socket support routines. Memory allocators, socket lock/release\n *\t\thandler for protocols to use and generic option handler.\n *\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tAlan Cox, <A.Cox@swansea.ac.uk>\n *\n * Fixes:\n *\t\tAlan Cox\t: \tNumerous verify_area() problems\n *\t\tAlan Cox\t:\tConnecting on a connecting socket\n *\t\t\t\t\tnow returns an error for tcp.\n *\t\tAlan Cox\t:\tsock->protocol is set correctly.\n *\t\t\t\t\tand is not sometimes left as 0.\n *\t\tAlan Cox\t:\tconnect handles icmp errors on a\n *\t\t\t\t\tconnect properly. Unfortunately there\n *\t\t\t\t\tis a restart syscall nasty there. I\n *\t\t\t\t\tcan't match BSD without hacking the C\n *\t\t\t\t\tlibrary. Ideas urgently sought!\n *\t\tAlan Cox\t:\tDisallow bind() to addresses that are\n *\t\t\t\t\tnot ours - especially broadcast ones!!\n *\t\tAlan Cox\t:\tSocket 1024 _IS_ ok for users. (fencepost)\n *\t\tAlan Cox\t:\tsock_wfree/sock_rfree don't destroy sockets,\n *\t\t\t\t\tinstead they leave that for the DESTROY timer.\n *\t\tAlan Cox\t:\tClean up error flag in accept\n *\t\tAlan Cox\t:\tTCP ack handling is buggy, the DESTROY timer\n *\t\t\t\t\twas buggy. Put a remove_sock() in the handler\n *\t\t\t\t\tfor memory when we hit 0. Also altered the timer\n *\t\t\t\t\tcode. The ACK stuff can wait and needs major\n *\t\t\t\t\tTCP layer surgery.\n *\t\tAlan Cox\t:\tFixed TCP ack bug, removed remove sock\n *\t\t\t\t\tand fixed timer/inet_bh race.\n *\t\tAlan Cox\t:\tAdded zapped flag for TCP\n *\t\tAlan Cox\t:\tMove kfree_skb into skbuff.c and tidied up surplus code\n *\t\tAlan Cox\t:\tfor new sk_buff allocations wmalloc/rmalloc now call alloc_skb\n *\t\tAlan Cox\t:\tkfree_s calls now are kfree_skbmem so we can track skb resources\n *\t\tAlan Cox\t:\tSupports socket option broadcast now as does udp. Packet and raw need fixing.\n *\t\tAlan Cox\t:\tAdded RCVBUF,SNDBUF size setting. It suddenly occurred to me how easy it was so...\n *\t\tRick Sladkey\t:\tRelaxed UDP rules for matching packets.\n *\t\tC.E.Hawkins\t:\tIFF_PROMISC/SIOCGHWADDR support\n *\tPauline Middelink\t:\tidentd support\n *\t\tAlan Cox\t:\tFixed connect() taking signals I think.\n *\t\tAlan Cox\t:\tSO_LINGER supported\n *\t\tAlan Cox\t:\tError reporting fixes\n *\t\tAnonymous\t:\tinet_create tidied up (sk->reuse setting)\n *\t\tAlan Cox\t:\tinet sockets don't set sk->type!\n *\t\tAlan Cox\t:\tSplit socket option code\n *\t\tAlan Cox\t:\tCallbacks\n *\t\tAlan Cox\t:\tNagle flag for Charles & Johannes stuff\n *\t\tAlex\t\t:\tRemoved restriction on inet fioctl\n *\t\tAlan Cox\t:\tSplitting INET from NET core\n *\t\tAlan Cox\t:\tFixed bogus SO_TYPE handling in getsockopt()\n *\t\tAdam Caldwell\t:\tMissing return in SO_DONTROUTE/SO_DEBUG code\n *\t\tAlan Cox\t:\tSplit IP from generic code\n *\t\tAlan Cox\t:\tNew kfree_skbmem()\n *\t\tAlan Cox\t:\tMake SO_DEBUG superuser only.\n *\t\tAlan Cox\t:\tAllow anyone to clear SO_DEBUG\n *\t\t\t\t\t(compatibility fix)\n *\t\tAlan Cox\t:\tAdded optimistic memory grabbing for AF_UNIX throughput.\n *\t\tAlan Cox\t:\tAllocator for a socket is settable.\n *\t\tAlan Cox\t:\tSO_ERROR includes soft errors.\n *\t\tAlan Cox\t:\tAllow NULL arguments on some SO_ opts\n *\t\tAlan Cox\t: \tGeneric socket allocation to make hooks\n *\t\t\t\t\teasier (suggested by Craig Metz).\n *\t\tMichael Pall\t:\tSO_ERROR returns positive errno again\n *              Steve Whitehouse:       Added default destructor to free\n *                                      protocol private data.\n *              Steve Whitehouse:       Added various other default routines\n *                                      common to several socket families.\n *              Chris Evans     :       Call suser() check last on F_SETOWN\n *\t\tJay Schulist\t:\tAdded SO_ATTACH_FILTER and SO_DETACH_FILTER.\n *\t\tAndi Kleen\t:\tAdd sock_kmalloc()/sock_kfree_s()\n *\t\tAndi Kleen\t:\tFix write_space callback\n *\t\tChris Evans\t:\tSecurity fixes - signedness again\n *\t\tArnaldo C. Melo :       cleanups, use skb_queue_purge\n *\n * To Fix:\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/errqueue.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/poll.h>\n#include <linux/tcp.h>\n#include <linux/init.h>\n#include <linux/highmem.h>\n#include <linux/user_namespace.h>\n#include <linux/static_key.h>\n#include <linux/memcontrol.h>\n#include <linux/prefetch.h>\n\n#include <asm/uaccess.h>\n\n#include <linux/netdevice.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/request_sock.h>\n#include <net/sock.h>\n#include <linux/net_tstamp.h>\n#include <net/xfrm.h>\n#include <linux/ipsec.h>\n#include <net/cls_cgroup.h>\n#include <net/netprio_cgroup.h>\n#include <linux/sock_diag.h>\n\n#include <linux/filter.h>\n#include <net/sock_reuseport.h>\n\n#include <trace/events/sock.h>\n\n#ifdef CONFIG_INET\n#include <net/tcp.h>\n#endif\n\n#include <net/busy_poll.h>\n\nstatic DEFINE_MUTEX(proto_list_mutex);\nstatic LIST_HEAD(proto_list);\n\n/**\n * sk_ns_capable - General socket capability test\n * @sk: Socket to use a capability on or through\n * @user_ns: The user namespace of the capability to use\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket had when the socket was\n * created and the current process has the capability @cap in the user\n * namespace @user_ns.\n */\nbool sk_ns_capable(const struct sock *sk,\n\t\t   struct user_namespace *user_ns, int cap)\n{\n\treturn file_ns_capable(sk->sk_socket->file, user_ns, cap) &&\n\t\tns_capable(user_ns, cap);\n}\nEXPORT_SYMBOL(sk_ns_capable);\n\n/**\n * sk_capable - Socket global capability test\n * @sk: Socket to use a capability on or through\n * @cap: The global capability to use\n *\n * Test to see if the opener of the socket had when the socket was\n * created and the current process has the capability @cap in all user\n * namespaces.\n */\nbool sk_capable(const struct sock *sk, int cap)\n{\n\treturn sk_ns_capable(sk, &init_user_ns, cap);\n}\nEXPORT_SYMBOL(sk_capable);\n\n/**\n * sk_net_capable - Network namespace socket capability test\n * @sk: Socket to use a capability on or through\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket had when the socket was created\n * and the current process has the capability @cap over the network namespace\n * the socket is a member of.\n */\nbool sk_net_capable(const struct sock *sk, int cap)\n{\n\treturn sk_ns_capable(sk, sock_net(sk)->user_ns, cap);\n}\nEXPORT_SYMBOL(sk_net_capable);\n\n/*\n * Each address family might have different locking rules, so we have\n * one slock key per address family:\n */\nstatic struct lock_class_key af_family_keys[AF_MAX];\nstatic struct lock_class_key af_family_slock_keys[AF_MAX];\n\n/*\n * Make lock validator output more readable. (we pre-construct these\n * strings build-time, so that runtime initialization of socket\n * locks is fast):\n */\nstatic const char *const af_family_key_strings[AF_MAX+1] = {\n  \"sk_lock-AF_UNSPEC\", \"sk_lock-AF_UNIX\"     , \"sk_lock-AF_INET\"     ,\n  \"sk_lock-AF_AX25\"  , \"sk_lock-AF_IPX\"      , \"sk_lock-AF_APPLETALK\",\n  \"sk_lock-AF_NETROM\", \"sk_lock-AF_BRIDGE\"   , \"sk_lock-AF_ATMPVC\"   ,\n  \"sk_lock-AF_X25\"   , \"sk_lock-AF_INET6\"    , \"sk_lock-AF_ROSE\"     ,\n  \"sk_lock-AF_DECnet\", \"sk_lock-AF_NETBEUI\"  , \"sk_lock-AF_SECURITY\" ,\n  \"sk_lock-AF_KEY\"   , \"sk_lock-AF_NETLINK\"  , \"sk_lock-AF_PACKET\"   ,\n  \"sk_lock-AF_ASH\"   , \"sk_lock-AF_ECONET\"   , \"sk_lock-AF_ATMSVC\"   ,\n  \"sk_lock-AF_RDS\"   , \"sk_lock-AF_SNA\"      , \"sk_lock-AF_IRDA\"     ,\n  \"sk_lock-AF_PPPOX\" , \"sk_lock-AF_WANPIPE\"  , \"sk_lock-AF_LLC\"      ,\n  \"sk_lock-27\"       , \"sk_lock-28\"          , \"sk_lock-AF_CAN\"      ,\n  \"sk_lock-AF_TIPC\"  , \"sk_lock-AF_BLUETOOTH\", \"sk_lock-IUCV\"        ,\n  \"sk_lock-AF_RXRPC\" , \"sk_lock-AF_ISDN\"     , \"sk_lock-AF_PHONET\"   ,\n  \"sk_lock-AF_IEEE802154\", \"sk_lock-AF_CAIF\" , \"sk_lock-AF_ALG\"      ,\n  \"sk_lock-AF_NFC\"   , \"sk_lock-AF_VSOCK\"    , \"sk_lock-AF_KCM\"      ,\n  \"sk_lock-AF_MAX\"\n};\nstatic const char *const af_family_slock_key_strings[AF_MAX+1] = {\n  \"slock-AF_UNSPEC\", \"slock-AF_UNIX\"     , \"slock-AF_INET\"     ,\n  \"slock-AF_AX25\"  , \"slock-AF_IPX\"      , \"slock-AF_APPLETALK\",\n  \"slock-AF_NETROM\", \"slock-AF_BRIDGE\"   , \"slock-AF_ATMPVC\"   ,\n  \"slock-AF_X25\"   , \"slock-AF_INET6\"    , \"slock-AF_ROSE\"     ,\n  \"slock-AF_DECnet\", \"slock-AF_NETBEUI\"  , \"slock-AF_SECURITY\" ,\n  \"slock-AF_KEY\"   , \"slock-AF_NETLINK\"  , \"slock-AF_PACKET\"   ,\n  \"slock-AF_ASH\"   , \"slock-AF_ECONET\"   , \"slock-AF_ATMSVC\"   ,\n  \"slock-AF_RDS\"   , \"slock-AF_SNA\"      , \"slock-AF_IRDA\"     ,\n  \"slock-AF_PPPOX\" , \"slock-AF_WANPIPE\"  , \"slock-AF_LLC\"      ,\n  \"slock-27\"       , \"slock-28\"          , \"slock-AF_CAN\"      ,\n  \"slock-AF_TIPC\"  , \"slock-AF_BLUETOOTH\", \"slock-AF_IUCV\"     ,\n  \"slock-AF_RXRPC\" , \"slock-AF_ISDN\"     , \"slock-AF_PHONET\"   ,\n  \"slock-AF_IEEE802154\", \"slock-AF_CAIF\" , \"slock-AF_ALG\"      ,\n  \"slock-AF_NFC\"   , \"slock-AF_VSOCK\"    ,\"slock-AF_KCM\"       ,\n  \"slock-AF_MAX\"\n};\nstatic const char *const af_family_clock_key_strings[AF_MAX+1] = {\n  \"clock-AF_UNSPEC\", \"clock-AF_UNIX\"     , \"clock-AF_INET\"     ,\n  \"clock-AF_AX25\"  , \"clock-AF_IPX\"      , \"clock-AF_APPLETALK\",\n  \"clock-AF_NETROM\", \"clock-AF_BRIDGE\"   , \"clock-AF_ATMPVC\"   ,\n  \"clock-AF_X25\"   , \"clock-AF_INET6\"    , \"clock-AF_ROSE\"     ,\n  \"clock-AF_DECnet\", \"clock-AF_NETBEUI\"  , \"clock-AF_SECURITY\" ,\n  \"clock-AF_KEY\"   , \"clock-AF_NETLINK\"  , \"clock-AF_PACKET\"   ,\n  \"clock-AF_ASH\"   , \"clock-AF_ECONET\"   , \"clock-AF_ATMSVC\"   ,\n  \"clock-AF_RDS\"   , \"clock-AF_SNA\"      , \"clock-AF_IRDA\"     ,\n  \"clock-AF_PPPOX\" , \"clock-AF_WANPIPE\"  , \"clock-AF_LLC\"      ,\n  \"clock-27\"       , \"clock-28\"          , \"clock-AF_CAN\"      ,\n  \"clock-AF_TIPC\"  , \"clock-AF_BLUETOOTH\", \"clock-AF_IUCV\"     ,\n  \"clock-AF_RXRPC\" , \"clock-AF_ISDN\"     , \"clock-AF_PHONET\"   ,\n  \"clock-AF_IEEE802154\", \"clock-AF_CAIF\" , \"clock-AF_ALG\"      ,\n  \"clock-AF_NFC\"   , \"clock-AF_VSOCK\"    , \"clock-AF_KCM\"      ,\n  \"clock-AF_MAX\"\n};\n\n/*\n * sk_callback_lock locking rules are per-address-family,\n * so split the lock classes by using a per-AF key:\n */\nstatic struct lock_class_key af_callback_keys[AF_MAX];\n\n/* Take into consideration the size of the struct sk_buff overhead in the\n * determination of these values, since that is non-constant across\n * platforms.  This makes socket queueing behavior and performance\n * not depend upon such differences.\n */\n#define _SK_MEM_PACKETS\t\t256\n#define _SK_MEM_OVERHEAD\tSKB_TRUESIZE(256)\n#define SK_WMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n#define SK_RMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n\n/* Run time adjustable parameters. */\n__u32 sysctl_wmem_max __read_mostly = SK_WMEM_MAX;\nEXPORT_SYMBOL(sysctl_wmem_max);\n__u32 sysctl_rmem_max __read_mostly = SK_RMEM_MAX;\nEXPORT_SYMBOL(sysctl_rmem_max);\n__u32 sysctl_wmem_default __read_mostly = SK_WMEM_MAX;\n__u32 sysctl_rmem_default __read_mostly = SK_RMEM_MAX;\n\n/* Maximal space eaten by iovec or ancillary data plus some space */\nint sysctl_optmem_max __read_mostly = sizeof(unsigned long)*(2*UIO_MAXIOV+512);\nEXPORT_SYMBOL(sysctl_optmem_max);\n\nint sysctl_tstamp_allow_data __read_mostly = 1;\n\nstruct static_key memalloc_socks = STATIC_KEY_INIT_FALSE;\nEXPORT_SYMBOL_GPL(memalloc_socks);\n\n/**\n * sk_set_memalloc - sets %SOCK_MEMALLOC\n * @sk: socket to set it on\n *\n * Set %SOCK_MEMALLOC on a socket for access to emergency reserves.\n * It's the responsibility of the admin to adjust min_free_kbytes\n * to meet the requirements\n */\nvoid sk_set_memalloc(struct sock *sk)\n{\n\tsock_set_flag(sk, SOCK_MEMALLOC);\n\tsk->sk_allocation |= __GFP_MEMALLOC;\n\tstatic_key_slow_inc(&memalloc_socks);\n}\nEXPORT_SYMBOL_GPL(sk_set_memalloc);\n\nvoid sk_clear_memalloc(struct sock *sk)\n{\n\tsock_reset_flag(sk, SOCK_MEMALLOC);\n\tsk->sk_allocation &= ~__GFP_MEMALLOC;\n\tstatic_key_slow_dec(&memalloc_socks);\n\n\t/*\n\t * SOCK_MEMALLOC is allowed to ignore rmem limits to ensure forward\n\t * progress of swapping. SOCK_MEMALLOC may be cleared while\n\t * it has rmem allocations due to the last swapfile being deactivated\n\t * but there is a risk that the socket is unusable due to exceeding\n\t * the rmem limits. Reclaim the reserves and obey rmem limits again.\n\t */\n\tsk_mem_reclaim(sk);\n}\nEXPORT_SYMBOL_GPL(sk_clear_memalloc);\n\nint __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tint ret;\n\tunsigned long pflags = current->flags;\n\n\t/* these should have been dropped before queueing */\n\tBUG_ON(!sock_flag(sk, SOCK_MEMALLOC));\n\n\tcurrent->flags |= PF_MEMALLOC;\n\tret = sk->sk_backlog_rcv(sk, skb);\n\ttsk_restore_flags(current, pflags, PF_MEMALLOC);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__sk_backlog_rcv);\n\nstatic int sock_set_timeout(long *timeo_p, char __user *optval, int optlen)\n{\n\tstruct timeval tv;\n\n\tif (optlen < sizeof(tv))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&tv, optval, sizeof(tv)))\n\t\treturn -EFAULT;\n\tif (tv.tv_usec < 0 || tv.tv_usec >= USEC_PER_SEC)\n\t\treturn -EDOM;\n\n\tif (tv.tv_sec < 0) {\n\t\tstatic int warned __read_mostly;\n\n\t\t*timeo_p = 0;\n\t\tif (warned < 10 && net_ratelimit()) {\n\t\t\twarned++;\n\t\t\tpr_info(\"%s: `%s' (pid %d) tries to set negative timeout\\n\",\n\t\t\t\t__func__, current->comm, task_pid_nr(current));\n\t\t}\n\t\treturn 0;\n\t}\n\t*timeo_p = MAX_SCHEDULE_TIMEOUT;\n\tif (tv.tv_sec == 0 && tv.tv_usec == 0)\n\t\treturn 0;\n\tif (tv.tv_sec < (MAX_SCHEDULE_TIMEOUT/HZ - 1))\n\t\t*timeo_p = tv.tv_sec*HZ + (tv.tv_usec+(1000000/HZ-1))/(1000000/HZ);\n\treturn 0;\n}\n\nstatic void sock_warn_obsolete_bsdism(const char *name)\n{\n\tstatic int warned;\n\tstatic char warncomm[TASK_COMM_LEN];\n\tif (strcmp(warncomm, current->comm) && warned < 5) {\n\t\tstrcpy(warncomm,  current->comm);\n\t\tpr_warn(\"process `%s' is using obsolete %s SO_BSDCOMPAT\\n\",\n\t\t\twarncomm, name);\n\t\twarned++;\n\t}\n}\n\nstatic bool sock_needs_netstamp(const struct sock *sk)\n{\n\tswitch (sk->sk_family) {\n\tcase AF_UNSPEC:\n\tcase AF_UNIX:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic void sock_disable_timestamp(struct sock *sk, unsigned long flags)\n{\n\tif (sk->sk_flags & flags) {\n\t\tsk->sk_flags &= ~flags;\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    !(sk->sk_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_disable_timestamp();\n\t}\n}\n\n\nint __sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tunsigned long flags;\n\tstruct sk_buff_head *list = &sk->sk_receive_queue;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\ttrace_sock_rcvqueue_full(sk, skb);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (!sk_rmem_schedule(sk, skb, skb->truesize)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\treturn -ENOBUFS;\n\t}\n\n\tskb->dev = NULL;\n\tskb_set_owner_r(skb, sk);\n\n\t/* we escape from rcu protected region, make sure we dont leak\n\t * a norefcounted dst\n\t */\n\tskb_dst_force(skb);\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tsock_skb_set_dropcount(sk, skb);\n\t__skb_queue_tail(list, skb);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(__sock_queue_rcv_skb);\n\nint sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint err;\n\n\terr = sk_filter(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\treturn __sock_queue_rcv_skb(sk, skb);\n}\nEXPORT_SYMBOL(sock_queue_rcv_skb);\n\nint __sk_receive_skb(struct sock *sk, struct sk_buff *skb,\n\t\t     const int nested, unsigned int trim_cap, bool refcounted)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter_trim_cap(sk, skb, trim_cap))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t/*\n\t\t * trylock + unlock semantics:\n\t\t */\n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\t} else if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\n\tbh_unlock_sock(sk);\nout:\n\tif (refcounted)\n\t\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}\nEXPORT_SYMBOL(__sk_receive_skb);\n\nstruct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = __sk_dst_get(sk);\n\n\tif (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {\n\t\tsk_tx_queue_clear(sk);\n\t\tRCU_INIT_POINTER(sk->sk_dst_cache, NULL);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(__sk_dst_check);\n\nstruct dst_entry *sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = sk_dst_get(sk);\n\n\tif (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {\n\t\tsk_dst_reset(sk);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(sk_dst_check);\n\nstatic int sock_setbindtodevice(struct sock *sk, char __user *optval,\n\t\t\t\tint optlen)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tstruct net *net = sock_net(sk);\n\tchar devname[IFNAMSIZ];\n\tint index;\n\n\t/* Sorry... */\n\tret = -EPERM;\n\tif (!ns_capable(net->user_ns, CAP_NET_RAW))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (optlen < 0)\n\t\tgoto out;\n\n\t/* Bind this socket to a particular device like \"eth0\",\n\t * as specified in the passed interface name. If the\n\t * name is \"\" or the option length is zero the socket\n\t * is not bound.\n\t */\n\tif (optlen > IFNAMSIZ - 1)\n\t\toptlen = IFNAMSIZ - 1;\n\tmemset(devname, 0, sizeof(devname));\n\n\tret = -EFAULT;\n\tif (copy_from_user(devname, optval, optlen))\n\t\tgoto out;\n\n\tindex = 0;\n\tif (devname[0] != '\\0') {\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_name_rcu(net, devname);\n\t\tif (dev)\n\t\t\tindex = dev->ifindex;\n\t\trcu_read_unlock();\n\t\tret = -ENODEV;\n\t\tif (!dev)\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tsk->sk_bound_dev_if = index;\n\tsk_dst_reset(sk);\n\trelease_sock(sk);\n\n\tret = 0;\n\nout:\n#endif\n\n\treturn ret;\n}\n\nstatic int sock_getbindtodevice(struct sock *sk, char __user *optval,\n\t\t\t\tint __user *optlen, int len)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tstruct net *net = sock_net(sk);\n\tchar devname[IFNAMSIZ];\n\n\tif (sk->sk_bound_dev_if == 0) {\n\t\tlen = 0;\n\t\tgoto zero;\n\t}\n\n\tret = -EINVAL;\n\tif (len < IFNAMSIZ)\n\t\tgoto out;\n\n\tret = netdev_get_name(net, devname, sk->sk_bound_dev_if);\n\tif (ret)\n\t\tgoto out;\n\n\tlen = strlen(devname) + 1;\n\n\tret = -EFAULT;\n\tif (copy_to_user(optval, devname, len))\n\t\tgoto out;\n\nzero:\n\tret = -EFAULT;\n\tif (put_user(len, optlen))\n\t\tgoto out;\n\n\tret = 0;\n\nout:\n#endif\n\n\treturn ret;\n}\n\nstatic inline void sock_valbool_flag(struct sock *sk, int bit, int valbool)\n{\n\tif (valbool)\n\t\tsock_set_flag(sk, bit);\n\telse\n\t\tsock_reset_flag(sk, bit);\n}\n\nbool sk_mc_loop(struct sock *sk)\n{\n\tif (dev_recursion_level())\n\t\treturn false;\n\tif (!sk)\n\t\treturn true;\n\tswitch (sk->sk_family) {\n\tcase AF_INET:\n\t\treturn inet_sk(sk)->mc_loop;\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase AF_INET6:\n\t\treturn inet6_sk(sk)->mc_loop;\n#endif\n\t}\n\tWARN_ON(1);\n\treturn true;\n}\nEXPORT_SYMBOL(sk_mc_loop);\n\n/*\n *\tThis is meant for all protocols to use and covers goings on\n *\tat the socket level. Everything here is generic.\n */\n\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_setbindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_REUSEPORT:\n\t\tsk->sk_reuseport = valbool;\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(int, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(int, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check_tx = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) ||\n\t\t    ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (val & SOF_TIMESTAMPING_OPT_ID &&\n\t\t    !(sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)) {\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t\t    sk->sk_type == SOCK_STREAM) {\n\t\t\t\tif ((1 << sk->sk_state) &\n\t\t\t\t    (TCPF_CLOSE | TCPF_LISTEN)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tsk->sk_tskey = tcp_sk(sk)->snd_una;\n\t\t\t} else {\n\t\t\t\tsk->sk_tskey = 0;\n\t\t\t}\n\t\t}\n\t\tsk->sk_tsflags = val;\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_ATTACH_BPF:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(u32)) {\n\t\t\tu32 ufd;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&ufd, optval, sizeof(ufd)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_bpf(ufd, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_ATTACH_REUSEPORT_CBPF:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_reuseport_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_ATTACH_REUSEPORT_EBPF:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(u32)) {\n\t\t\tu32 ufd;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&ufd, optval, sizeof(ufd)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_reuseport_attach_bpf(ufd, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_LOCK_FILTER:\n\t\tif (sock_flag(sk, SOCK_FILTER_LOCKED) && !valbool)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_FILTER_LOCKED, valbool);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tret = sock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tsock_valbool_flag(sk, SOCK_SELECT_ERR_QUEUE, valbool);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\t/* allow unprivileged users to decrease the value */\n\t\tif ((val > sk->sk_ll_usec) && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse {\n\t\t\tif (val < 0)\n\t\t\t\tret = -EINVAL;\n\t\t\telse\n\t\t\t\tsk->sk_ll_usec = val;\n\t\t}\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tsk->sk_max_pacing_rate = val;\n\t\tsk->sk_pacing_rate = min(sk->sk_pacing_rate,\n\t\t\t\t\t sk->sk_max_pacing_rate);\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tsk->sk_incoming_cpu = val;\n\t\tbreak;\n\n\tcase SO_CNX_ADVICE:\n\t\tif (val == 1)\n\t\t\tdst_negative_advice(sk);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\nEXPORT_SYMBOL(sock_setsockopt);\n\n\nstatic void cred_to_ucred(struct pid *pid, const struct cred *cred,\n\t\t\t  struct ucred *ucred)\n{\n\tucred->pid = pid_vnr(pid);\n\tucred->uid = ucred->gid = -1;\n\tif (cred) {\n\t\tstruct user_namespace *current_ns = current_user_ns();\n\n\t\tucred->uid = from_kuid_munged(current_ns, cred->euid);\n\t\tucred->gid = from_kgid_munged(current_ns, cred->egid);\n\t}\n}\n\nint sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tstruct linger ling;\n\t\tstruct timeval tm;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"getsockopt\");\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tv.val = sk->sk_tsflags;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tlv = sizeof(struct timeval);\n\t\tif (sk->sk_rcvtimeo == MAX_SCHEDULE_TIMEOUT) {\n\t\t\tv.tm.tv_sec = 0;\n\t\t\tv.tm.tv_usec = 0;\n\t\t} else {\n\t\t\tv.tm.tv_sec = sk->sk_rcvtimeo / HZ;\n\t\t\tv.tm.tv_usec = ((sk->sk_rcvtimeo % HZ) * 1000000) / HZ;\n\t\t}\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tlv = sizeof(struct timeval);\n\t\tif (sk->sk_sndtimeo == MAX_SCHEDULE_TIMEOUT) {\n\t\t\tv.tm.tv_sec = 0;\n\t\t\tv.tm.tv_usec = 0;\n\t\t} else {\n\t\t\tv.tm.tv_sec = sk->sk_sndtimeo / HZ;\n\t\t\tv.tm.tv_usec = ((sk->sk_sndtimeo % HZ) * 1000000) / HZ;\n\t\t}\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tif (sock->ops->getname(sock, (struct sockaddr *)address, &lv, 2))\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, (struct sock_filter __user *)optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = sk->sk_ll_usec;\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tv.val = sk->sk_max_pacing_rate;\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = sk->sk_incoming_cpu;\n\t\tbreak;\n\n\tdefault:\n\t\t/* We implement the SO_SNDLOWAT etc to not be settable\n\t\t * (1003.1g 7).\n\t\t */\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n * Initialize an sk_lock.\n *\n * (We also register the sk_lock with the lock validator.)\n */\nstatic inline void sock_lock_init(struct sock *sk)\n{\n\tsock_lock_init_class_and_name(sk,\n\t\t\taf_family_slock_key_strings[sk->sk_family],\n\t\t\taf_family_slock_keys + sk->sk_family,\n\t\t\taf_family_key_strings[sk->sk_family],\n\t\t\taf_family_keys + sk->sk_family);\n}\n\n/*\n * Copy all fields from osk to nsk but nsk->sk_refcnt must not change yet,\n * even temporarly, because of RCU lookups. sk_node should also be left as is.\n * We must not copy fields between sk_dontcopy_begin and sk_dontcopy_end\n */\nstatic void sock_copy(struct sock *nsk, const struct sock *osk)\n{\n#ifdef CONFIG_SECURITY_NETWORK\n\tvoid *sptr = nsk->sk_security;\n#endif\n\tmemcpy(nsk, osk, offsetof(struct sock, sk_dontcopy_begin));\n\n\tmemcpy(&nsk->sk_dontcopy_end, &osk->sk_dontcopy_end,\n\t       osk->sk_prot->obj_size - offsetof(struct sock, sk_dontcopy_end));\n\n#ifdef CONFIG_SECURITY_NETWORK\n\tnsk->sk_security = sptr;\n\tsecurity_sk_clone(osk, nsk);\n#endif\n}\n\nstatic struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,\n\t\tint family)\n{\n\tstruct sock *sk;\n\tstruct kmem_cache *slab;\n\n\tslab = prot->slab;\n\tif (slab != NULL) {\n\t\tsk = kmem_cache_alloc(slab, priority & ~__GFP_ZERO);\n\t\tif (!sk)\n\t\t\treturn sk;\n\t\tif (priority & __GFP_ZERO)\n\t\t\tsk_prot_clear_nulls(sk, prot->obj_size);\n\t} else\n\t\tsk = kmalloc(prot->obj_size, priority);\n\n\tif (sk != NULL) {\n\t\tkmemcheck_annotate_bitfield(sk, flags);\n\n\t\tif (security_sk_alloc(sk, family, priority))\n\t\t\tgoto out_free;\n\n\t\tif (!try_module_get(prot->owner))\n\t\t\tgoto out_free_sec;\n\t\tsk_tx_queue_clear(sk);\n\t}\n\n\treturn sk;\n\nout_free_sec:\n\tsecurity_sk_free(sk);\nout_free:\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\treturn NULL;\n}\n\nstatic void sk_prot_free(struct proto *prot, struct sock *sk)\n{\n\tstruct kmem_cache *slab;\n\tstruct module *owner;\n\n\towner = prot->owner;\n\tslab = prot->slab;\n\n\tcgroup_sk_free(&sk->sk_cgrp_data);\n\tmem_cgroup_sk_free(sk);\n\tsecurity_sk_free(sk);\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\tmodule_put(owner);\n}\n\n/**\n *\tsk_alloc - All socket objects are allocated here\n *\t@net: the applicable net namespace\n *\t@family: protocol family\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\t@prot: struct proto associated with this new sock instance\n *\t@kern: is this to be a kernel socket?\n */\nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\n\t\t      struct proto *prot, int kern)\n{\n\tstruct sock *sk;\n\n\tsk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);\n\tif (sk) {\n\t\tsk->sk_family = family;\n\t\t/*\n\t\t * See comment in struct sock definition to understand\n\t\t * why we need sk_prot_creator -acme\n\t\t */\n\t\tsk->sk_prot = sk->sk_prot_creator = prot;\n\t\tsock_lock_init(sk);\n\t\tsk->sk_net_refcnt = kern ? 0 : 1;\n\t\tif (likely(sk->sk_net_refcnt))\n\t\t\tget_net(net);\n\t\tsock_net_set(sk, net);\n\t\tatomic_set(&sk->sk_wmem_alloc, 1);\n\n\t\tmem_cgroup_sk_alloc(sk);\n\t\tcgroup_sk_alloc(&sk->sk_cgrp_data);\n\t\tsock_update_classid(&sk->sk_cgrp_data);\n\t\tsock_update_netprioidx(&sk->sk_cgrp_data);\n\t}\n\n\treturn sk;\n}\nEXPORT_SYMBOL(sk_alloc);\n\n/* Sockets having SOCK_RCU_FREE will call this function after one RCU\n * grace period. This is the case for UDP sockets and TCP listeners.\n */\nstatic void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       atomic_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\tif (rcu_access_pointer(sk->sk_reuseport_cb))\n\t\treuseport_detach_sock(sk);\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}\n\nvoid sk_destruct(struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_RCU_FREE))\n\t\tcall_rcu(&sk->sk_rcu, __sk_destruct);\n\telse\n\t\t__sk_destruct(&sk->sk_rcu);\n}\n\nstatic void __sk_free(struct sock *sk)\n{\n\tif (unlikely(sock_diag_has_destroy_listeners(sk) && sk->sk_net_refcnt))\n\t\tsock_diag_broadcast_destroy(sk);\n\telse\n\t\tsk_destruct(sk);\n}\n\nvoid sk_free(struct sock *sk)\n{\n\t/*\n\t * We subtract one from sk_wmem_alloc and can know if\n\t * some packets are still in some tx queue.\n\t * If not null, sock_wfree() will call __sk_free(sk) later\n\t */\n\tif (atomic_dec_and_test(&sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sk_free);\n\n/**\n *\tsk_clone_lock - clone a socket, and lock its clone\n *\t@sk: the socket to clone\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\n *\tCaller must unlock socket even in error path (bh_unlock_sock(newsk))\n */\nstruct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct sock *newsk;\n\tbool is_charged = true;\n\n\tnewsk = sk_prot_alloc(sk->sk_prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\t/* SANITY */\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\tatomic_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tskb_queue_head_init(&newsk->sk_receive_queue);\n\t\tskb_queue_head_init(&newsk->sk_write_queue);\n\n\t\trwlock_init(&newsk->sk_callback_lock);\n\t\tlockdep_set_class_and_name(&newsk->sk_callback_lock,\n\t\t\t\taf_callback_keys + newsk->sk_family,\n\t\t\t\taf_family_clock_key_strings[newsk->sk_family]);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tatomic_set(&newsk->sk_drops, 0);\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\t\tskb_queue_head_init(&newsk->sk_error_queue);\n\n\t\tfilter = rcu_dereference_protected(newsk->sk_filter, 1);\n\t\tif (filter != NULL)\n\t\t\t/* though it's an empty new sock, the charging may fail\n\t\t\t * if sysctl_optmem_max was changed between creation of\n\t\t\t * original socket and cloning\n\t\t\t */\n\t\t\tis_charged = sk_filter_charge(newsk, filter);\n\n\t\tif (unlikely(!is_charged || xfrm_sk_clone_policy(newsk, sk))) {\n\t\t\t/* It is still raw copy of parent, so invalidate\n\t\t\t * destructor and make plain sk_free() */\n\t\t\tnewsk->sk_destruct = NULL;\n\t\t\tbh_unlock_sock(newsk);\n\t\t\tsk_free(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tRCU_INIT_POINTER(newsk->sk_reuseport_cb, NULL);\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_err_soft = 0;\n\t\tnewsk->sk_priority = 0;\n\t\tnewsk->sk_incoming_cpu = raw_smp_processor_id();\n\t\tatomic64_set(&newsk->sk_cookie, 0);\n\n\t\tmem_cgroup_sk_alloc(newsk);\n\t\tcgroup_sk_alloc(&newsk->sk_cgrp_data);\n\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\tatomic_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tnewsk->sk_wq = NULL;\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tsk_sockets_allocated_inc(newsk);\n\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(sk_clone_lock);\n\nvoid sk_setup_caps(struct sock *sk, struct dst_entry *dst)\n{\n\tu32 max_segs = 1;\n\n\tsk_dst_set(sk, dst);\n\tsk->sk_route_caps = dst->dev->features;\n\tif (sk->sk_route_caps & NETIF_F_GSO)\n\t\tsk->sk_route_caps |= NETIF_F_GSO_SOFTWARE;\n\tsk->sk_route_caps &= ~sk->sk_route_nocaps;\n\tif (sk_can_gso(sk)) {\n\t\tif (dst->header_len) {\n\t\t\tsk->sk_route_caps &= ~NETIF_F_GSO_MASK;\n\t\t} else {\n\t\t\tsk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;\n\t\t\tsk->sk_gso_max_size = dst->dev->gso_max_size;\n\t\t\tmax_segs = max_t(u32, dst->dev->gso_max_segs, 1);\n\t\t}\n\t}\n\tsk->sk_gso_max_segs = max_segs;\n}\nEXPORT_SYMBOL_GPL(sk_setup_caps);\n\n/*\n *\tSimple resource managers for sockets.\n */\n\n\n/*\n * Write buffer destructor automatically called from kfree_skb.\n */\nvoid sock_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tif (!sock_flag(sk, SOCK_USE_WRITE_QUEUE)) {\n\t\t/*\n\t\t * Keep a reference on sk_wmem_alloc, this will be released\n\t\t * after sk_write_space() call\n\t\t */\n\t\tatomic_sub(len - 1, &sk->sk_wmem_alloc);\n\t\tsk->sk_write_space(sk);\n\t\tlen = 1;\n\t}\n\t/*\n\t * if sk_wmem_alloc reaches 0, we must finish what sk_free()\n\t * could not do because of in-flight packets\n\t */\n\tif (atomic_sub_and_test(len, &sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sock_wfree);\n\n/* This variant of sock_wfree() is used by TCP,\n * since it sets SOCK_USE_WRITE_QUEUE.\n */\nvoid __sock_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (atomic_sub_and_test(skb->truesize, &sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\n\nvoid skb_set_owner_w(struct sk_buff *skb, struct sock *sk)\n{\n\tskb_orphan(skb);\n\tskb->sk = sk;\n#ifdef CONFIG_INET\n\tif (unlikely(!sk_fullsock(sk))) {\n\t\tskb->destructor = sock_edemux;\n\t\tsock_hold(sk);\n\t\treturn;\n\t}\n#endif\n\tskb->destructor = sock_wfree;\n\tskb_set_hash_from_sk(skb, sk);\n\t/*\n\t * We used to take a refcount on sk, but following operation\n\t * is enough to guarantee sk_free() wont free this sock until\n\t * all in-flight packets are completed\n\t */\n\tatomic_add(skb->truesize, &sk->sk_wmem_alloc);\n}\nEXPORT_SYMBOL(skb_set_owner_w);\n\n/* This helper is used by netem, as it can hold packets in its\n * delay queue. We want to allow the owner socket to send more\n * packets, as if they were already TX completed by a typical driver.\n * But we also want to keep skb->sk set because some packet schedulers\n * rely on it (sch_fq for example). So we set skb->truesize to a small\n * amount (1) and decrease sk_wmem_alloc accordingly.\n */\nvoid skb_orphan_partial(struct sk_buff *skb)\n{\n\t/* If this skb is a TCP pure ACK or already went here,\n\t * we have nothing to do. 2 is already a very small truesize.\n\t */\n\tif (skb->truesize <= 2)\n\t\treturn;\n\n\t/* TCP stack sets skb->ooo_okay based on sk_wmem_alloc,\n\t * so we do not completely orphan skb, but transfert all\n\t * accounted bytes but one, to avoid unexpected reorders.\n\t */\n\tif (skb->destructor == sock_wfree\n#ifdef CONFIG_INET\n\t    || skb->destructor == tcp_wfree\n#endif\n\t\t) {\n\t\tatomic_sub(skb->truesize - 1, &skb->sk->sk_wmem_alloc);\n\t\tskb->truesize = 1;\n\t} else {\n\t\tskb_orphan(skb);\n\t}\n}\nEXPORT_SYMBOL(skb_orphan_partial);\n\n/*\n * Read buffer destructor automatically called from kfree_skb.\n */\nvoid sock_rfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tatomic_sub(len, &sk->sk_rmem_alloc);\n\tsk_mem_uncharge(sk, len);\n}\nEXPORT_SYMBOL(sock_rfree);\n\n/*\n * Buffer destructor for skbs that are not used directly in read or write\n * path, e.g. for error handler skbs. Automatically called from kfree_skb.\n */\nvoid sock_efree(struct sk_buff *skb)\n{\n\tsock_put(skb->sk);\n}\nEXPORT_SYMBOL(sock_efree);\n\nkuid_t sock_i_uid(struct sock *sk)\n{\n\tkuid_t uid;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tuid = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_uid : GLOBAL_ROOT_UID;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn uid;\n}\nEXPORT_SYMBOL(sock_i_uid);\n\nunsigned long sock_i_ino(struct sock *sk)\n{\n\tunsigned long ino;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tino = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_ino : 0;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn ino;\n}\nEXPORT_SYMBOL(sock_i_ino);\n\n/*\n * Allocate a skb from the socket's send buffer.\n */\nstruct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,\n\t\t\t     gfp_t priority)\n{\n\tif (force || atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n\t\tstruct sk_buff *skb = alloc_skb(size, priority);\n\t\tif (skb) {\n\t\t\tskb_set_owner_w(skb, sk);\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_wmalloc);\n\n/*\n * Allocate a memory block from the socket's option memory buffer.\n */\nvoid *sock_kmalloc(struct sock *sk, int size, gfp_t priority)\n{\n\tif ((unsigned int)size <= sysctl_optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + size < sysctl_optmem_max) {\n\t\tvoid *mem;\n\t\t/* First do the add, to avoid the race if kmalloc\n\t\t * might sleep.\n\t\t */\n\t\tatomic_add(size, &sk->sk_omem_alloc);\n\t\tmem = kmalloc(size, priority);\n\t\tif (mem)\n\t\t\treturn mem;\n\t\tatomic_sub(size, &sk->sk_omem_alloc);\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_kmalloc);\n\n/* Free an option memory block. Note, we actually want the inline\n * here as this allows gcc to detect the nullify and fold away the\n * condition entirely.\n */\nstatic inline void __sock_kfree_s(struct sock *sk, void *mem, int size,\n\t\t\t\t  const bool nullify)\n{\n\tif (WARN_ON_ONCE(!mem))\n\t\treturn;\n\tif (nullify)\n\t\tkzfree(mem);\n\telse\n\t\tkfree(mem);\n\tatomic_sub(size, &sk->sk_omem_alloc);\n}\n\nvoid sock_kfree_s(struct sock *sk, void *mem, int size)\n{\n\t__sock_kfree_s(sk, mem, size, false);\n}\nEXPORT_SYMBOL(sock_kfree_s);\n\nvoid sock_kzfree_s(struct sock *sk, void *mem, int size)\n{\n\t__sock_kfree_s(sk, mem, size, true);\n}\nEXPORT_SYMBOL(sock_kzfree_s);\n\n/* It is almost wait_for_tcp_memory minus release_sock/lock_sock.\n   I think, these locks should be removed for datagram sockets.\n */\nstatic long sock_wait_for_wmem(struct sock *sk, long timeo)\n{\n\tDEFINE_WAIT(wait);\n\n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\tfor (;;) {\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf)\n\t\t\tbreak;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tbreak;\n\t\tif (sk->sk_err)\n\t\t\tbreak;\n\t\ttimeo = schedule_timeout(timeo);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn timeo;\n}\n\n\n/*\n *\tGeneric send/receive buffer handlers\n */\n\nstruct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,\n\t\t\t\t     unsigned long data_len, int noblock,\n\t\t\t\t     int *errcode, int max_page_order)\n{\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tint err;\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\tfor (;;) {\n\t\terr = sock_error(sk);\n\t\tif (err != 0)\n\t\t\tgoto failure;\n\n\t\terr = -EPIPE;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tgoto failure;\n\n\t\tif (sk_wmem_alloc_get(sk) < sk->sk_sndbuf)\n\t\t\tbreak;\n\n\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto failure;\n\t\tif (signal_pending(current))\n\t\t\tgoto interrupted;\n\t\ttimeo = sock_wait_for_wmem(sk, timeo);\n\t}\n\tskb = alloc_skb_with_frags(header_len, data_len, max_page_order,\n\t\t\t\t   errcode, sk->sk_allocation);\n\tif (skb)\n\t\tskb_set_owner_w(skb, sk);\n\treturn skb;\n\ninterrupted:\n\terr = sock_intr_errno(timeo);\nfailure:\n\t*errcode = err;\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_alloc_send_pskb);\n\nstruct sk_buff *sock_alloc_send_skb(struct sock *sk, unsigned long size,\n\t\t\t\t    int noblock, int *errcode)\n{\n\treturn sock_alloc_send_pskb(sk, size, 0, noblock, errcode, 0);\n}\nEXPORT_SYMBOL(sock_alloc_send_skb);\n\nint __sock_cmsg_send(struct sock *sk, struct msghdr *msg, struct cmsghdr *cmsg,\n\t\t     struct sockcm_cookie *sockc)\n{\n\tu32 tsflags;\n\n\tswitch (cmsg->cmsg_type) {\n\tcase SO_MARK:\n\t\tif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))\n\t\t\treturn -EINVAL;\n\t\tsockc->mark = *(u32 *)CMSG_DATA(cmsg);\n\t\tbreak;\n\tcase SO_TIMESTAMPING:\n\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))\n\t\t\treturn -EINVAL;\n\n\t\ttsflags = *(u32 *)CMSG_DATA(cmsg);\n\t\tif (tsflags & ~SOF_TIMESTAMPING_TX_RECORD_MASK)\n\t\t\treturn -EINVAL;\n\n\t\tsockc->tsflags &= ~SOF_TIMESTAMPING_TX_RECORD_MASK;\n\t\tsockc->tsflags |= tsflags;\n\t\tbreak;\n\t/* SCM_RIGHTS and SCM_CREDENTIALS are semantically in SOL_UNIX. */\n\tcase SCM_RIGHTS:\n\tcase SCM_CREDENTIALS:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__sock_cmsg_send);\n\nint sock_cmsg_send(struct sock *sk, struct msghdr *msg,\n\t\t   struct sockcm_cookie *sockc)\n{\n\tstruct cmsghdr *cmsg;\n\tint ret;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\t\tif (cmsg->cmsg_level != SOL_SOCKET)\n\t\t\tcontinue;\n\t\tret = __sock_cmsg_send(sk, msg, cmsg, sockc);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_cmsg_send);\n\n/* On 32bit arches, an skb frag is limited to 2^15 */\n#define SKB_FRAG_PAGE_ORDER\tget_order(32768)\n\n/**\n * skb_page_frag_refill - check that a page_frag contains enough room\n * @sz: minimum size of the fragment we want to get\n * @pfrag: pointer to page_frag\n * @gfp: priority for memory allocation\n *\n * Note: While this allocator tries to use high order pages, there is\n * no guarantee that allocations succeed. Therefore, @sz MUST be\n * less or equal than PAGE_SIZE.\n */\nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)\n{\n\tif (pfrag->page) {\n\t\tif (page_ref_count(pfrag->page) == 1) {\n\t\t\tpfrag->offset = 0;\n\t\t\treturn true;\n\t\t}\n\t\tif (pfrag->offset + sz <= pfrag->size)\n\t\t\treturn true;\n\t\tput_page(pfrag->page);\n\t}\n\n\tpfrag->offset = 0;\n\tif (SKB_FRAG_PAGE_ORDER) {\n\t\t/* Avoid direct reclaim but allow kswapd to wake */\n\t\tpfrag->page = alloc_pages((gfp & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t\t  __GFP_COMP | __GFP_NOWARN |\n\t\t\t\t\t  __GFP_NORETRY,\n\t\t\t\t\t  SKB_FRAG_PAGE_ORDER);\n\t\tif (likely(pfrag->page)) {\n\t\t\tpfrag->size = PAGE_SIZE << SKB_FRAG_PAGE_ORDER;\n\t\t\treturn true;\n\t\t}\n\t}\n\tpfrag->page = alloc_page(gfp);\n\tif (likely(pfrag->page)) {\n\t\tpfrag->size = PAGE_SIZE;\n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL(skb_page_frag_refill);\n\nbool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag)\n{\n\tif (likely(skb_page_frag_refill(32U, pfrag, sk->sk_allocation)))\n\t\treturn true;\n\n\tsk_enter_memory_pressure(sk);\n\tsk_stream_moderate_sndbuf(sk);\n\treturn false;\n}\nEXPORT_SYMBOL(sk_page_frag_refill);\n\nstatic void __lock_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(&sk->sk_lock.wq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\t\tschedule();\n\t\tspin_lock_bh(&sk->sk_lock.slock);\n\t\tif (!sock_owned_by_user(sk))\n\t\t\tbreak;\n\t}\n\tfinish_wait(&sk->sk_lock.wq, &wait);\n}\n\nstatic void __release_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tstruct sk_buff *skb, *next;\n\n\twhile ((skb = sk->sk_backlog.head) != NULL) {\n\t\tsk->sk_backlog.head = sk->sk_backlog.tail = NULL;\n\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\n\t\tdo {\n\t\t\tnext = skb->next;\n\t\t\tprefetch(next);\n\t\t\tWARN_ON_ONCE(skb_dst_is_noref(skb));\n\t\t\tskb->next = NULL;\n\t\t\tsk_backlog_rcv(sk, skb);\n\n\t\t\tcond_resched();\n\n\t\t\tskb = next;\n\t\t} while (skb != NULL);\n\n\t\tspin_lock_bh(&sk->sk_lock.slock);\n\t}\n\n\t/*\n\t * Doing the zeroing here guarantee we can not loop forever\n\t * while a wild producer attempts to flood us.\n\t */\n\tsk->sk_backlog.len = 0;\n}\n\nvoid __sk_flush_backlog(struct sock *sk)\n{\n\tspin_lock_bh(&sk->sk_lock.slock);\n\t__release_sock(sk);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\n\n/**\n * sk_wait_data - wait for data to arrive at sk_receive_queue\n * @sk:    sock to wait on\n * @timeo: for how long\n * @skb:   last skb seen on sk_receive_queue\n *\n * Now socket state including sk->sk_err is changed only under lock,\n * hence we may omit checks after joining wait queue.\n * We check receive queue before schedule() only as optimization;\n * it is very likely that release_sock() added new data.\n */\nint sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb)\n{\n\tint rc;\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\tsk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\trc = sk_wait_event(sk, timeo, skb_peek_tail(&sk->sk_receive_queue) != skb);\n\tsk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn rc;\n}\nEXPORT_SYMBOL(sk_wait_data);\n\n/**\n *\t__sk_mem_schedule - increase sk_forward_alloc and memory_allocated\n *\t@sk: socket\n *\t@size: memory size to allocate\n *\t@kind: allocation type\n *\n *\tIf kind is SK_MEM_SEND, it means wmem allocation. Otherwise it means\n *\trmem allocation. This function assumes that protocols which have\n *\tmemory_pressure use sk_wmem_queued as write buffer accounting.\n */\nint __sk_mem_schedule(struct sock *sk, int size, int kind)\n{\n\tstruct proto *prot = sk->sk_prot;\n\tint amt = sk_mem_pages(size);\n\tlong allocated;\n\n\tsk->sk_forward_alloc += amt * SK_MEM_QUANTUM;\n\n\tallocated = sk_memory_allocated_add(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg &&\n\t    !mem_cgroup_charge_skmem(sk->sk_memcg, amt))\n\t\tgoto suppress_allocation;\n\n\t/* Under limit. */\n\tif (allocated <= sk_prot_mem_limits(sk, 0)) {\n\t\tsk_leave_memory_pressure(sk);\n\t\treturn 1;\n\t}\n\n\t/* Under pressure. */\n\tif (allocated > sk_prot_mem_limits(sk, 1))\n\t\tsk_enter_memory_pressure(sk);\n\n\t/* Over hard limit. */\n\tif (allocated > sk_prot_mem_limits(sk, 2))\n\t\tgoto suppress_allocation;\n\n\t/* guarantee minimum buffer size under pressure */\n\tif (kind == SK_MEM_RECV) {\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < prot->sysctl_rmem[0])\n\t\t\treturn 1;\n\n\t} else { /* SK_MEM_SEND */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tif (sk->sk_wmem_queued < prot->sysctl_wmem[0])\n\t\t\t\treturn 1;\n\t\t} else if (atomic_read(&sk->sk_wmem_alloc) <\n\t\t\t   prot->sysctl_wmem[0])\n\t\t\t\treturn 1;\n\t}\n\n\tif (sk_has_memory_pressure(sk)) {\n\t\tint alloc;\n\n\t\tif (!sk_under_memory_pressure(sk))\n\t\t\treturn 1;\n\t\talloc = sk_sockets_allocated_read_positive(sk);\n\t\tif (sk_prot_mem_limits(sk, 2) > alloc *\n\t\t    sk_mem_pages(sk->sk_wmem_queued +\n\t\t\t\t atomic_read(&sk->sk_rmem_alloc) +\n\t\t\t\t sk->sk_forward_alloc))\n\t\t\treturn 1;\n\t}\n\nsuppress_allocation:\n\n\tif (kind == SK_MEM_SEND && sk->sk_type == SOCK_STREAM) {\n\t\tsk_stream_moderate_sndbuf(sk);\n\n\t\t/* Fail only if socket is _under_ its sndbuf.\n\t\t * In this case we cannot block, so that we have to fail.\n\t\t */\n\t\tif (sk->sk_wmem_queued + size >= sk->sk_sndbuf)\n\t\t\treturn 1;\n\t}\n\n\ttrace_sock_exceed_buf_limit(sk, prot, allocated);\n\n\t/* Alas. Undo changes. */\n\tsk->sk_forward_alloc -= amt * SK_MEM_QUANTUM;\n\n\tsk_memory_allocated_sub(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_uncharge_skmem(sk->sk_memcg, amt);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(__sk_mem_schedule);\n\n/**\n *\t__sk_mem_reclaim - reclaim memory_allocated\n *\t@sk: socket\n *\t@amount: number of bytes (rounded down to a SK_MEM_QUANTUM multiple)\n */\nvoid __sk_mem_reclaim(struct sock *sk, int amount)\n{\n\tamount >>= SK_MEM_QUANTUM_SHIFT;\n\tsk_memory_allocated_sub(sk, amount);\n\tsk->sk_forward_alloc -= amount << SK_MEM_QUANTUM_SHIFT;\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_uncharge_skmem(sk->sk_memcg, amount);\n\n\tif (sk_under_memory_pressure(sk) &&\n\t    (sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)))\n\t\tsk_leave_memory_pressure(sk);\n}\nEXPORT_SYMBOL(__sk_mem_reclaim);\n\nint sk_set_peek_off(struct sock *sk, int val)\n{\n\tif (val < 0)\n\t\treturn -EINVAL;\n\n\tsk->sk_peek_off = val;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sk_set_peek_off);\n\n/*\n * Set of default routines for initialising struct proto_ops when\n * the protocol does not support a particular function. In certain\n * cases where it makes no sense for a protocol to have a \"do nothing\"\n * function, some default processing is provided.\n */\n\nint sock_no_bind(struct socket *sock, struct sockaddr *saddr, int len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_bind);\n\nint sock_no_connect(struct socket *sock, struct sockaddr *saddr,\n\t\t    int len, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_connect);\n\nint sock_no_socketpair(struct socket *sock1, struct socket *sock2)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_socketpair);\n\nint sock_no_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_accept);\n\nint sock_no_getname(struct socket *sock, struct sockaddr *saddr,\n\t\t    int *len, int peer)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getname);\n\nunsigned int sock_no_poll(struct file *file, struct socket *sock, poll_table *pt)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_no_poll);\n\nint sock_no_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_ioctl);\n\nint sock_no_listen(struct socket *sock, int backlog)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_listen);\n\nint sock_no_shutdown(struct socket *sock, int how)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_shutdown);\n\nint sock_no_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_setsockopt);\n\nint sock_no_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getsockopt);\n\nint sock_no_sendmsg(struct socket *sock, struct msghdr *m, size_t len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_sendmsg);\n\nint sock_no_recvmsg(struct socket *sock, struct msghdr *m, size_t len,\n\t\t    int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_recvmsg);\n\nint sock_no_mmap(struct file *file, struct socket *sock, struct vm_area_struct *vma)\n{\n\t/* Mirror missing mmap method error code */\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL(sock_no_mmap);\n\nssize_t sock_no_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags)\n{\n\tssize_t res;\n\tstruct msghdr msg = {.msg_flags = flags};\n\tstruct kvec iov;\n\tchar *kaddr = kmap(page);\n\tiov.iov_base = kaddr + offset;\n\tiov.iov_len = size;\n\tres = kernel_sendmsg(sock, &msg, &iov, 1, size);\n\tkunmap(page);\n\treturn res;\n}\nEXPORT_SYMBOL(sock_no_sendpage);\n\n/*\n *\tDefault Socket Callbacks\n */\n\nstatic void sock_def_wakeup(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_all(&wq->wait);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_error_report(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_poll(&wq->wait, POLLERR);\n\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_ERR);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_readable(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_sync_poll(&wq->wait, POLLIN | POLLPRI |\n\t\t\t\t\t\tPOLLRDNORM | POLLRDBAND);\n\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_write_space(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\n\t/* Do not wake up a writer until he can make \"significant\"\n\t * progress.  --DaveM\n\t */\n\tif ((atomic_read(&sk->sk_wmem_alloc) << 1) <= sk->sk_sndbuf) {\n\t\twq = rcu_dereference(sk->sk_wq);\n\t\tif (skwq_has_sleeper(wq))\n\t\t\twake_up_interruptible_sync_poll(&wq->wait, POLLOUT |\n\t\t\t\t\t\tPOLLWRNORM | POLLWRBAND);\n\n\t\t/* Should agree with poll, otherwise some programs break */\n\t\tif (sock_writeable(sk))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\t}\n\n\trcu_read_unlock();\n}\n\nstatic void sock_def_destruct(struct sock *sk)\n{\n}\n\nvoid sk_send_sigurg(struct sock *sk)\n{\n\tif (sk->sk_socket && sk->sk_socket->file)\n\t\tif (send_sigurg(&sk->sk_socket->file->f_owner))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_URG, POLL_PRI);\n}\nEXPORT_SYMBOL(sk_send_sigurg);\n\nvoid sk_reset_timer(struct sock *sk, struct timer_list* timer,\n\t\t    unsigned long expires)\n{\n\tif (!mod_timer(timer, expires))\n\t\tsock_hold(sk);\n}\nEXPORT_SYMBOL(sk_reset_timer);\n\nvoid sk_stop_timer(struct sock *sk, struct timer_list* timer)\n{\n\tif (del_timer(timer))\n\t\t__sock_put(sk);\n}\nEXPORT_SYMBOL(sk_stop_timer);\n\nvoid sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n\n\tsk->sk_send_head\t=\tNULL;\n\n\tinit_timer(&sk->sk_timer);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tsk->sk_wq\t=\tsock->wq;\n\t\tsock->sk\t=\tsk;\n\t} else\n\t\tsk->sk_wq\t=\tNULL;\n\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = ktime_set(-1L, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tsysctl_net_busy_read;\n#endif\n\n\tsk->sk_max_pacing_rate = ~0U;\n\tsk->sk_pacing_rate = ~0U;\n\tsk->sk_incoming_cpu = -1;\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t */\n\tsmp_wmb();\n\tatomic_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}\nEXPORT_SYMBOL(sock_init_data);\n\nvoid lock_sock_nested(struct sock *sk, int subclass)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_lock.owned)\n\t\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock(&sk->sk_lock.slock);\n\t/*\n\t * The sk_lock has mutex_lock() semantics here:\n\t */\n\tmutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);\n\tlocal_bh_enable();\n}\nEXPORT_SYMBOL(lock_sock_nested);\n\nvoid release_sock(struct sock *sk)\n{\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_backlog.tail)\n\t\t__release_sock(sk);\n\n\t/* Warning : release_cb() might need to release sk ownership,\n\t * ie call sock_release_ownership(sk) before us.\n\t */\n\tif (sk->sk_prot->release_cb)\n\t\tsk->sk_prot->release_cb(sk);\n\n\tsock_release_ownership(sk);\n\tif (waitqueue_active(&sk->sk_lock.wq))\n\t\twake_up(&sk->sk_lock.wq);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\nEXPORT_SYMBOL(release_sock);\n\n/**\n * lock_sock_fast - fast version of lock_sock\n * @sk: socket\n *\n * This version should be used for very small section, where process wont block\n * return false if fast path is taken\n *   sk_lock.slock locked, owned = 0, BH disabled\n * return true if slow path is taken\n *   sk_lock.slock unlocked, owned = 1, BH enabled\n */\nbool lock_sock_fast(struct sock *sk)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\n\tif (!sk->sk_lock.owned)\n\t\t/*\n\t\t * Note : We must disable BH\n\t\t */\n\t\treturn false;\n\n\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock(&sk->sk_lock.slock);\n\t/*\n\t * The sk_lock has mutex_lock() semantics here:\n\t */\n\tmutex_acquire(&sk->sk_lock.dep_map, 0, 0, _RET_IP_);\n\tlocal_bh_enable();\n\treturn true;\n}\nEXPORT_SYMBOL(lock_sock_fast);\n\nint sock_get_timestamp(struct sock *sk, struct timeval __user *userstamp)\n{\n\tstruct timeval tv;\n\tif (!sock_flag(sk, SOCK_TIMESTAMP))\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\ttv = ktime_to_timeval(sk->sk_stamp);\n\tif (tv.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (tv.tv_sec == 0) {\n\t\tsk->sk_stamp = ktime_get_real();\n\t\ttv = ktime_to_timeval(sk->sk_stamp);\n\t}\n\treturn copy_to_user(userstamp, &tv, sizeof(tv)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(sock_get_timestamp);\n\nint sock_get_timestampns(struct sock *sk, struct timespec __user *userstamp)\n{\n\tstruct timespec ts;\n\tif (!sock_flag(sk, SOCK_TIMESTAMP))\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\tts = ktime_to_timespec(sk->sk_stamp);\n\tif (ts.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (ts.tv_sec == 0) {\n\t\tsk->sk_stamp = ktime_get_real();\n\t\tts = ktime_to_timespec(sk->sk_stamp);\n\t}\n\treturn copy_to_user(userstamp, &ts, sizeof(ts)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(sock_get_timestampns);\n\nvoid sock_enable_timestamp(struct sock *sk, int flag)\n{\n\tif (!sock_flag(sk, flag)) {\n\t\tunsigned long previous_flags = sk->sk_flags;\n\n\t\tsock_set_flag(sk, flag);\n\t\t/*\n\t\t * we just set one of the two flags which require net\n\t\t * time stamping, but time stamping might have been on\n\t\t * already because of the other one\n\t\t */\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    !(previous_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_enable_timestamp();\n\t}\n}\n\nint sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len,\n\t\t       int level, int type)\n{\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\n\terr = -EAGAIN;\n\tskb = sock_dequeue_err_skb(sk);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\tput_cmsg(msg, level, type, sizeof(serr->ee), &serr->ee);\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(sock_recv_errqueue);\n\n/*\n *\tGet a socket option on an socket.\n *\n *\tFIX: POSIX 1003.1g is very ambiguous here. It states that\n *\tasynchronous errors should be reported by getsockopt. We assume\n *\tthis means if you specify SO_ERROR (otherwise whats the point of it).\n */\nint sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_prot->compat_getsockopt != NULL)\n\t\treturn sk->sk_prot->compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t      optval, optlen);\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_sock_common_getsockopt);\n#endif\n\nint sock_common_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\t\tint flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\terr = sk->sk_prot->recvmsg(sk, msg, size, flags & MSG_DONTWAIT,\n\t\t\t\t   flags & ~MSG_DONTWAIT, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(sock_common_recvmsg);\n\n/*\n *\tSet socket options on an inet socket.\n */\nint sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_prot->compat_setsockopt != NULL)\n\t\treturn sk->sk_prot->compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t      optval, optlen);\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_sock_common_setsockopt);\n#endif\n\nvoid sk_common_release(struct sock *sk)\n{\n\tif (sk->sk_prot->destroy)\n\t\tsk->sk_prot->destroy(sk);\n\n\t/*\n\t * Observation: when sock_common_release is called, processes have\n\t * no access to socket. But net still has.\n\t * Step one, detach it from networking:\n\t *\n\t * A. Remove from hash tables.\n\t */\n\n\tsk->sk_prot->unhash(sk);\n\n\t/*\n\t * In this point socket cannot receive new packets, but it is possible\n\t * that some packets are in flight because some CPU runs receiver and\n\t * did hash table lookup before we unhashed socket. They will achieve\n\t * receive queue and will be purged by socket destructor.\n\t *\n\t * Also we still have packets pending on receive queue and probably,\n\t * our own packets waiting in device queues. sock_destroy will drain\n\t * receive queue, but transmitted packets will delay socket destruction\n\t * until the last reference will be released.\n\t */\n\n\tsock_orphan(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsk_refcnt_debug_release(sk);\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(sk_common_release);\n\n#ifdef CONFIG_PROC_FS\n#define PROTO_INUSE_NR\t64\t/* should be enough for the first time */\nstruct prot_inuse {\n\tint val[PROTO_INUSE_NR];\n};\n\nstatic DECLARE_BITMAP(proto_inuse_idx, PROTO_INUSE_NR);\n\n#ifdef CONFIG_NET_NS\nvoid sock_prot_inuse_add(struct net *net, struct proto *prot, int val)\n{\n\t__this_cpu_add(net->core.inuse->val[prot->inuse_idx], val);\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_add);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu_ptr(net->core.inuse, cpu)->val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n\nstatic int __net_init sock_inuse_init_net(struct net *net)\n{\n\tnet->core.inuse = alloc_percpu(struct prot_inuse);\n\treturn net->core.inuse ? 0 : -ENOMEM;\n}\n\nstatic void __net_exit sock_inuse_exit_net(struct net *net)\n{\n\tfree_percpu(net->core.inuse);\n}\n\nstatic struct pernet_operations net_inuse_ops = {\n\t.init = sock_inuse_init_net,\n\t.exit = sock_inuse_exit_net,\n};\n\nstatic __init int net_inuse_init(void)\n{\n\tif (register_pernet_subsys(&net_inuse_ops))\n\t\tpanic(\"Cannot initialize net inuse counters\");\n\n\treturn 0;\n}\n\ncore_initcall(net_inuse_init);\n#else\nstatic DEFINE_PER_CPU(struct prot_inuse, prot_inuse);\n\nvoid sock_prot_inuse_add(struct net *net, struct proto *prot, int val)\n{\n\t__this_cpu_add(prot_inuse.val[prot->inuse_idx], val);\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_add);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu(prot_inuse, cpu).val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n#endif\n\nstatic void assign_proto_idx(struct proto *prot)\n{\n\tprot->inuse_idx = find_first_zero_bit(proto_inuse_idx, PROTO_INUSE_NR);\n\n\tif (unlikely(prot->inuse_idx == PROTO_INUSE_NR - 1)) {\n\t\tpr_err(\"PROTO_INUSE_NR exhausted\\n\");\n\t\treturn;\n\t}\n\n\tset_bit(prot->inuse_idx, proto_inuse_idx);\n}\n\nstatic void release_proto_idx(struct proto *prot)\n{\n\tif (prot->inuse_idx != PROTO_INUSE_NR - 1)\n\t\tclear_bit(prot->inuse_idx, proto_inuse_idx);\n}\n#else\nstatic inline void assign_proto_idx(struct proto *prot)\n{\n}\n\nstatic inline void release_proto_idx(struct proto *prot)\n{\n}\n#endif\n\nstatic void req_prot_cleanup(struct request_sock_ops *rsk_prot)\n{\n\tif (!rsk_prot)\n\t\treturn;\n\tkfree(rsk_prot->slab_name);\n\trsk_prot->slab_name = NULL;\n\tkmem_cache_destroy(rsk_prot->slab);\n\trsk_prot->slab = NULL;\n}\n\nstatic int req_prot_init(const struct proto *prot)\n{\n\tstruct request_sock_ops *rsk_prot = prot->rsk_prot;\n\n\tif (!rsk_prot)\n\t\treturn 0;\n\n\trsk_prot->slab_name = kasprintf(GFP_KERNEL, \"request_sock_%s\",\n\t\t\t\t\tprot->name);\n\tif (!rsk_prot->slab_name)\n\t\treturn -ENOMEM;\n\n\trsk_prot->slab = kmem_cache_create(rsk_prot->slab_name,\n\t\t\t\t\t   rsk_prot->obj_size, 0,\n\t\t\t\t\t   prot->slab_flags, NULL);\n\n\tif (!rsk_prot->slab) {\n\t\tpr_crit(\"%s: Can't create request sock SLAB cache!\\n\",\n\t\t\tprot->name);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nint proto_register(struct proto *prot, int alloc_slab)\n{\n\tif (alloc_slab) {\n\t\tprot->slab = kmem_cache_create(prot->name, prot->obj_size, 0,\n\t\t\t\t\tSLAB_HWCACHE_ALIGN | prot->slab_flags,\n\t\t\t\t\tNULL);\n\n\t\tif (prot->slab == NULL) {\n\t\t\tpr_crit(\"%s: Can't create sock SLAB cache!\\n\",\n\t\t\t\tprot->name);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (req_prot_init(prot))\n\t\t\tgoto out_free_request_sock_slab;\n\n\t\tif (prot->twsk_prot != NULL) {\n\t\t\tprot->twsk_prot->twsk_slab_name = kasprintf(GFP_KERNEL, \"tw_sock_%s\", prot->name);\n\n\t\t\tif (prot->twsk_prot->twsk_slab_name == NULL)\n\t\t\t\tgoto out_free_request_sock_slab;\n\n\t\t\tprot->twsk_prot->twsk_slab =\n\t\t\t\tkmem_cache_create(prot->twsk_prot->twsk_slab_name,\n\t\t\t\t\t\t  prot->twsk_prot->twsk_obj_size,\n\t\t\t\t\t\t  0,\n\t\t\t\t\t\t  prot->slab_flags,\n\t\t\t\t\t\t  NULL);\n\t\t\tif (prot->twsk_prot->twsk_slab == NULL)\n\t\t\t\tgoto out_free_timewait_sock_slab_name;\n\t\t}\n\t}\n\n\tmutex_lock(&proto_list_mutex);\n\tlist_add(&prot->node, &proto_list);\n\tassign_proto_idx(prot);\n\tmutex_unlock(&proto_list_mutex);\n\treturn 0;\n\nout_free_timewait_sock_slab_name:\n\tkfree(prot->twsk_prot->twsk_slab_name);\nout_free_request_sock_slab:\n\treq_prot_cleanup(prot->rsk_prot);\n\n\tkmem_cache_destroy(prot->slab);\n\tprot->slab = NULL;\nout:\n\treturn -ENOBUFS;\n}\nEXPORT_SYMBOL(proto_register);\n\nvoid proto_unregister(struct proto *prot)\n{\n\tmutex_lock(&proto_list_mutex);\n\trelease_proto_idx(prot);\n\tlist_del(&prot->node);\n\tmutex_unlock(&proto_list_mutex);\n\n\tkmem_cache_destroy(prot->slab);\n\tprot->slab = NULL;\n\n\treq_prot_cleanup(prot->rsk_prot);\n\n\tif (prot->twsk_prot != NULL && prot->twsk_prot->twsk_slab != NULL) {\n\t\tkmem_cache_destroy(prot->twsk_prot->twsk_slab);\n\t\tkfree(prot->twsk_prot->twsk_slab_name);\n\t\tprot->twsk_prot->twsk_slab = NULL;\n\t}\n}\nEXPORT_SYMBOL(proto_unregister);\n\n#ifdef CONFIG_PROC_FS\nstatic void *proto_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(proto_list_mutex)\n{\n\tmutex_lock(&proto_list_mutex);\n\treturn seq_list_start_head(&proto_list, *pos);\n}\n\nstatic void *proto_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\treturn seq_list_next(v, &proto_list, pos);\n}\n\nstatic void proto_seq_stop(struct seq_file *seq, void *v)\n\t__releases(proto_list_mutex)\n{\n\tmutex_unlock(&proto_list_mutex);\n}\n\nstatic char proto_method_implemented(const void *method)\n{\n\treturn method == NULL ? 'n' : 'y';\n}\nstatic long sock_prot_memory_allocated(struct proto *proto)\n{\n\treturn proto->memory_allocated != NULL ? proto_memory_allocated(proto) : -1L;\n}\n\nstatic char *sock_prot_memory_pressure(struct proto *proto)\n{\n\treturn proto->memory_pressure != NULL ?\n\tproto_memory_pressure(proto) ? \"yes\" : \"no\" : \"NI\";\n}\n\nstatic void proto_seq_printf(struct seq_file *seq, struct proto *proto)\n{\n\n\tseq_printf(seq, \"%-9s %4u %6d  %6ld   %-3s %6u   %-3s  %-10s \"\n\t\t\t\"%2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c\\n\",\n\t\t   proto->name,\n\t\t   proto->obj_size,\n\t\t   sock_prot_inuse_get(seq_file_net(seq), proto),\n\t\t   sock_prot_memory_allocated(proto),\n\t\t   sock_prot_memory_pressure(proto),\n\t\t   proto->max_header,\n\t\t   proto->slab == NULL ? \"no\" : \"yes\",\n\t\t   module_name(proto->owner),\n\t\t   proto_method_implemented(proto->close),\n\t\t   proto_method_implemented(proto->connect),\n\t\t   proto_method_implemented(proto->disconnect),\n\t\t   proto_method_implemented(proto->accept),\n\t\t   proto_method_implemented(proto->ioctl),\n\t\t   proto_method_implemented(proto->init),\n\t\t   proto_method_implemented(proto->destroy),\n\t\t   proto_method_implemented(proto->shutdown),\n\t\t   proto_method_implemented(proto->setsockopt),\n\t\t   proto_method_implemented(proto->getsockopt),\n\t\t   proto_method_implemented(proto->sendmsg),\n\t\t   proto_method_implemented(proto->recvmsg),\n\t\t   proto_method_implemented(proto->sendpage),\n\t\t   proto_method_implemented(proto->bind),\n\t\t   proto_method_implemented(proto->backlog_rcv),\n\t\t   proto_method_implemented(proto->hash),\n\t\t   proto_method_implemented(proto->unhash),\n\t\t   proto_method_implemented(proto->get_port),\n\t\t   proto_method_implemented(proto->enter_memory_pressure));\n}\n\nstatic int proto_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == &proto_list)\n\t\tseq_printf(seq, \"%-9s %-4s %-8s %-6s %-5s %-7s %-4s %-10s %s\",\n\t\t\t   \"protocol\",\n\t\t\t   \"size\",\n\t\t\t   \"sockets\",\n\t\t\t   \"memory\",\n\t\t\t   \"press\",\n\t\t\t   \"maxhdr\",\n\t\t\t   \"slab\",\n\t\t\t   \"module\",\n\t\t\t   \"cl co di ac io in de sh ss gs se re sp bi br ha uh gp em\\n\");\n\telse\n\t\tproto_seq_printf(seq, list_entry(v, struct proto, node));\n\treturn 0;\n}\n\nstatic const struct seq_operations proto_seq_ops = {\n\t.start  = proto_seq_start,\n\t.next   = proto_seq_next,\n\t.stop   = proto_seq_stop,\n\t.show   = proto_seq_show,\n};\n\nstatic int proto_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &proto_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations proto_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= proto_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\nstatic __net_init int proto_init_net(struct net *net)\n{\n\tif (!proc_create(\"protocols\", S_IRUGO, net->proc_net, &proto_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic __net_exit void proto_exit_net(struct net *net)\n{\n\tremove_proc_entry(\"protocols\", net->proc_net);\n}\n\n\nstatic __net_initdata struct pernet_operations proto_net_ops = {\n\t.init = proto_init_net,\n\t.exit = proto_exit_net,\n};\n\nstatic int __init proto_init(void)\n{\n\treturn register_pernet_subsys(&proto_net_ops);\n}\n\nsubsys_initcall(proto_init);\n\n#endif /* PROC_FS */\n"], "filenames": ["net/core/sock.c"], "buggy_code_start_loc": [718], "buggy_code_end_loc": [755], "fixing_code_start_loc": [718], "fixing_code_end_loc": [755], "type": "CWE-119", "message": "The sock_setsockopt function in net/core/sock.c in the Linux kernel before 4.8.14 mishandles negative values of sk_sndbuf and sk_rcvbuf, which allows local users to cause a denial of service (memory corruption and system crash) or possibly have unspecified other impact by leveraging the CAP_NET_ADMIN capability for a crafted setsockopt system call with the (1) SO_SNDBUFFORCE or (2) SO_RCVBUFFORCE option.", "other": {"cve": {"id": "CVE-2016-9793", "sourceIdentifier": "cve@mitre.org", "published": "2016-12-28T07:59:00.557", "lastModified": "2023-01-17T21:05:19.187", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The sock_setsockopt function in net/core/sock.c in the Linux kernel before 4.8.14 mishandles negative values of sk_sndbuf and sk_rcvbuf, which allows local users to cause a denial of service (memory corruption and system crash) or possibly have unspecified other impact by leveraging the CAP_NET_ADMIN capability for a crafted setsockopt system call with the (1) SO_SNDBUFFORCE or (2) SO_RCVBUFFORCE option."}, {"lang": "es", "value": "La funci\u00f3n sock_setsockopt en net/core/sock.c en el kernel de Linux en versiones anteriores a 4.8.14 no maneja adecuadamente valores negativos de sk_sndbuf y sk_rcvbuf, lo que permite a usuarios locales provocar una denegaci\u00f3n de servicio (corrupci\u00f3n de memoria y ca\u00edda del sistema) o posiblemente tener otros impactos no especificados aprovechando la capacidad CAP_NET_ADMIN para una llamada al sistema setsockopt manipulada con la opci\u00f3n (1) SO_SNDBUFFORCE o (2) SO_RCVBUFFORCE."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.5", "versionEndExcluding": "3.12.69", "matchCriteriaId": "4E1C5AC9-663B-4BE0-A4A5-46EFD7A95ABC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.16.40", "matchCriteriaId": "1331ABAB-8C2B-4379-BA77-B655A5B9A83F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.52", "matchCriteriaId": "8104AAC1-9700-4372-8E11-37B09309A76F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.50", "matchCriteriaId": "F71F6650-13B4-486F-80AC-20D871806D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.38", "matchCriteriaId": "ECA0BA21-3E8F-49EF-A94F-D1DFD18343FC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.8.14", "matchCriteriaId": "2454EAB6-FC42-4FA4-BE76-CBAA81D4ADC4"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b98b0bc8c431e3ceb4b26b0dfc8db509518fb290", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.8.14", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/12/03/1", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/94655", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1037968", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:0931", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:0932", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:0933", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1402013", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/b98b0bc8c431e3ceb4b26b0dfc8db509518fb290", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/xairy/kernel-exploits/tree/master/CVE-2016-9793", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://source.android.com/security/bulletin/2017-03-01.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b98b0bc8c431e3ceb4b26b0dfc8db509518fb290"}}