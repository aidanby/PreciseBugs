{"buggy_code": ["#ifndef _ASM_X86_MMU_CONTEXT_H\n#define _ASM_X86_MMU_CONTEXT_H\n\n#include <asm/desc.h>\n#include <linux/atomic.h>\n#include <linux/mm_types.h>\n#include <linux/pkeys.h>\n\n#include <trace/events/tlb.h>\n\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include <asm/paravirt.h>\n#include <asm/mpx.h>\n#ifndef CONFIG_PARAVIRT\nstatic inline void paravirt_activate_mm(struct mm_struct *prev,\n\t\t\t\t\tstruct mm_struct *next)\n{\n}\n#endif\t/* !CONFIG_PARAVIRT */\n\n#ifdef CONFIG_PERF_EVENTS\nextern struct static_key rdpmc_always_available;\n\nstatic inline void load_mm_cr4(struct mm_struct *mm)\n{\n\tif (static_key_false(&rdpmc_always_available) ||\n\t    atomic_read(&mm->context.perf_rdpmc_allowed))\n\t\tcr4_set_bits(X86_CR4_PCE);\n\telse\n\t\tcr4_clear_bits(X86_CR4_PCE);\n}\n#else\nstatic inline void load_mm_cr4(struct mm_struct *mm) {}\n#endif\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n/*\n * ldt_structs can be allocated, used, and freed, but they are never\n * modified while live.\n */\nstruct ldt_struct {\n\t/*\n\t * Xen requires page-aligned LDTs with special permissions.  This is\n\t * needed to prevent us from installing evil descriptors such as\n\t * call gates.  On native, we could merge the ldt_struct and LDT\n\t * allocations, but it's not worth trying to optimize.\n\t */\n\tstruct desc_struct *entries;\n\tunsigned int nr_entries;\n};\n\n/*\n * Used for LDT copy/destruction.\n */\nint init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);\nvoid destroy_context_ldt(struct mm_struct *mm);\n#else\t/* CONFIG_MODIFY_LDT_SYSCALL */\nstatic inline int init_new_context_ldt(struct task_struct *tsk,\n\t\t\t\t       struct mm_struct *mm)\n{\n\treturn 0;\n}\nstatic inline void destroy_context_ldt(struct mm_struct *mm) {}\n#endif\n\nstatic inline void load_mm_ldt(struct mm_struct *mm)\n{\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\tstruct ldt_struct *ldt;\n\n\t/* lockless_dereference synchronizes with smp_store_release */\n\tldt = lockless_dereference(mm->context.ldt);\n\n\t/*\n\t * Any change to mm->context.ldt is followed by an IPI to all\n\t * CPUs with the mm active.  The LDT will not be freed until\n\t * after the IPI is handled by all such CPUs.  This means that,\n\t * if the ldt_struct changes before we return, the values we see\n\t * will be safe, and the new values will be loaded before we run\n\t * any user code.\n\t *\n\t * NB: don't try to convert this to use RCU without extreme care.\n\t * We would still need IRQs off, because we don't want to change\n\t * the local LDT after an IPI loaded a newer value than the one\n\t * that we can see.\n\t */\n\n\tif (unlikely(ldt))\n\t\tset_ldt(ldt->entries, ldt->nr_entries);\n\telse\n\t\tclear_LDT();\n#else\n\tclear_LDT();\n#endif\n}\n\nstatic inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)\n{\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t/*\n\t * Load the LDT if either the old or new mm had an LDT.\n\t *\n\t * An mm will never go from having an LDT to not having an LDT.  Two\n\t * mms never share an LDT, so we don't gain anything by checking to\n\t * see whether the LDT changed.  There's also no guarantee that\n\t * prev->context.ldt actually matches LDTR, but, if LDTR is non-NULL,\n\t * then prev->context.ldt will also be non-NULL.\n\t *\n\t * If we really cared, we could optimize the case where prev == next\n\t * and we're exiting lazy mode.  Most of the time, if this happens,\n\t * we don't actually need to reload LDTR, but modify_ldt() is mostly\n\t * used by legacy code and emulators where we don't need this level of\n\t * performance.\n\t *\n\t * This uses | instead of || because it generates better code.\n\t */\n\tif (unlikely((unsigned long)prev->context.ldt |\n\t\t     (unsigned long)next->context.ldt))\n\t\tload_mm_ldt(next);\n#endif\n\n\tDEBUG_LOCKS_WARN_ON(preemptible());\n}\n\nstatic inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)\n{\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);\n}\n\nstatic inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\t#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (cpu_feature_enabled(X86_FEATURE_OSPKE)) {\n\t\t/* pkey 0 is the default and always allocated */\n\t\tmm->context.pkey_allocation_map = 0x1;\n\t\t/* -1 means unallocated or invalid */\n\t\tmm->context.execute_only_pkey = -1;\n\t}\n\t#endif\n\tinit_new_context_ldt(tsk, mm);\n\n\treturn 0;\n}\nstatic inline void destroy_context(struct mm_struct *mm)\n{\n\tdestroy_context_ldt(mm);\n}\n\nextern void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t      struct task_struct *tsk);\n\nextern void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t       struct task_struct *tsk);\n#define switch_mm_irqs_off switch_mm_irqs_off\n\n#define activate_mm(prev, next)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tparavirt_activate_mm((prev), (next));\t\\\n\tswitch_mm((prev), (next), NULL);\t\\\n} while (0);\n\n#ifdef CONFIG_X86_32\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tlazy_load_gs(0);\t\t\t\\\n} while (0)\n#else\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tload_gs_index(0);\t\t\t\\\n\tloadsegment(fs, 0);\t\t\t\\\n} while (0)\n#endif\n\nstatic inline void arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t struct mm_struct *mm)\n{\n\tparavirt_arch_dup_mmap(oldmm, mm);\n}\n\nstatic inline void arch_exit_mmap(struct mm_struct *mm)\n{\n\tparavirt_arch_exit_mmap(mm);\n}\n\n#ifdef CONFIG_X86_64\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn\t!IS_ENABLED(CONFIG_IA32_EMULATION) ||\n\t\t!(mm->context.ia32_compat == TIF_IA32);\n}\n#else\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn false;\n}\n#endif\n\nstatic inline void arch_bprm_mm_init(struct mm_struct *mm,\n\t\tstruct vm_area_struct *vma)\n{\n\tmpx_mm_init(mm);\n}\n\nstatic inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t      unsigned long start, unsigned long end)\n{\n\t/*\n\t * mpx_notify_unmap() goes and reads a rarely-hot\n\t * cacheline in the mm_struct.  That can be expensive\n\t * enough to be seen in profiles.\n\t *\n\t * The mpx_notify_unmap() call and its contents have been\n\t * observed to affect munmap() performance on hardware\n\t * where MPX is not present.\n\t *\n\t * The unlikely() optimizes for the fast case: no MPX\n\t * in the CPU, or no MPX use in the process.  Even if\n\t * we get this wrong (in the unlikely event that MPX\n\t * is widely enabled on some system) the overhead of\n\t * MPX itself (reading bounds tables) is expected to\n\t * overwhelm the overhead of getting this unlikely()\n\t * consistently wrong.\n\t */\n\tif (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))\n\t\tmpx_notify_unmap(mm, vma, start, end);\n}\n\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\nstatic inline int vma_pkey(struct vm_area_struct *vma)\n{\n\tunsigned long vma_pkey_mask = VM_PKEY_BIT0 | VM_PKEY_BIT1 |\n\t\t\t\t      VM_PKEY_BIT2 | VM_PKEY_BIT3;\n\n\treturn (vma->vm_flags & vma_pkey_mask) >> VM_PKEY_SHIFT;\n}\n#else\nstatic inline int vma_pkey(struct vm_area_struct *vma)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * We only want to enforce protection keys on the current process\n * because we effectively have no access to PKRU for other\n * processes or any way to tell *which * PKRU in a threaded\n * process we could use.\n *\n * So do not enforce things if the VMA is not from the current\n * mm, or if we are in a kernel thread.\n */\nstatic inline bool vma_is_foreign(struct vm_area_struct *vma)\n{\n\tif (!current->mm)\n\t\treturn true;\n\t/*\n\t * Should PKRU be enforced on the access to this VMA?  If\n\t * the VMA is from another process, then PKRU has no\n\t * relevance and should not be enforced.\n\t */\n\tif (current->mm != vma->vm_mm)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool arch_vma_access_permitted(struct vm_area_struct *vma,\n\t\tbool write, bool execute, bool foreign)\n{\n\t/* pkeys never affect instruction fetches */\n\tif (execute)\n\t\treturn true;\n\t/* allow access if the VMA is not one from this process */\n\tif (foreign || vma_is_foreign(vma))\n\t\treturn true;\n\treturn __pkru_allows_pkey(vma_pkey(vma), write);\n}\n\n\n/*\n * This can be used from process context to figure out what the value of\n * CR3 is without needing to do a (slow) __read_cr3().\n *\n * It's intended to be used for code like KVM that sneakily changes CR3\n * and needs to restore it.  It needs to be used very carefully.\n */\nstatic inline unsigned long __get_current_cr3_fast(void)\n{\n\tunsigned long cr3 = __pa(this_cpu_read(cpu_tlbstate.loaded_mm)->pgd);\n\n\t/* For now, be very restrictive about when this can be called. */\n\tVM_WARN_ON(in_nmi() || preemptible());\n\n\tVM_BUG_ON(cr3 != __read_cr3());\n\treturn cr3;\n}\n\n#endif /* _ASM_X86_MMU_CONTEXT_H */\n"], "fixing_code": ["#ifndef _ASM_X86_MMU_CONTEXT_H\n#define _ASM_X86_MMU_CONTEXT_H\n\n#include <asm/desc.h>\n#include <linux/atomic.h>\n#include <linux/mm_types.h>\n#include <linux/pkeys.h>\n\n#include <trace/events/tlb.h>\n\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include <asm/paravirt.h>\n#include <asm/mpx.h>\n#ifndef CONFIG_PARAVIRT\nstatic inline void paravirt_activate_mm(struct mm_struct *prev,\n\t\t\t\t\tstruct mm_struct *next)\n{\n}\n#endif\t/* !CONFIG_PARAVIRT */\n\n#ifdef CONFIG_PERF_EVENTS\nextern struct static_key rdpmc_always_available;\n\nstatic inline void load_mm_cr4(struct mm_struct *mm)\n{\n\tif (static_key_false(&rdpmc_always_available) ||\n\t    atomic_read(&mm->context.perf_rdpmc_allowed))\n\t\tcr4_set_bits(X86_CR4_PCE);\n\telse\n\t\tcr4_clear_bits(X86_CR4_PCE);\n}\n#else\nstatic inline void load_mm_cr4(struct mm_struct *mm) {}\n#endif\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n/*\n * ldt_structs can be allocated, used, and freed, but they are never\n * modified while live.\n */\nstruct ldt_struct {\n\t/*\n\t * Xen requires page-aligned LDTs with special permissions.  This is\n\t * needed to prevent us from installing evil descriptors such as\n\t * call gates.  On native, we could merge the ldt_struct and LDT\n\t * allocations, but it's not worth trying to optimize.\n\t */\n\tstruct desc_struct *entries;\n\tunsigned int nr_entries;\n};\n\n/*\n * Used for LDT copy/destruction.\n */\nint init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);\nvoid destroy_context_ldt(struct mm_struct *mm);\n#else\t/* CONFIG_MODIFY_LDT_SYSCALL */\nstatic inline int init_new_context_ldt(struct task_struct *tsk,\n\t\t\t\t       struct mm_struct *mm)\n{\n\treturn 0;\n}\nstatic inline void destroy_context_ldt(struct mm_struct *mm) {}\n#endif\n\nstatic inline void load_mm_ldt(struct mm_struct *mm)\n{\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\tstruct ldt_struct *ldt;\n\n\t/* lockless_dereference synchronizes with smp_store_release */\n\tldt = lockless_dereference(mm->context.ldt);\n\n\t/*\n\t * Any change to mm->context.ldt is followed by an IPI to all\n\t * CPUs with the mm active.  The LDT will not be freed until\n\t * after the IPI is handled by all such CPUs.  This means that,\n\t * if the ldt_struct changes before we return, the values we see\n\t * will be safe, and the new values will be loaded before we run\n\t * any user code.\n\t *\n\t * NB: don't try to convert this to use RCU without extreme care.\n\t * We would still need IRQs off, because we don't want to change\n\t * the local LDT after an IPI loaded a newer value than the one\n\t * that we can see.\n\t */\n\n\tif (unlikely(ldt))\n\t\tset_ldt(ldt->entries, ldt->nr_entries);\n\telse\n\t\tclear_LDT();\n#else\n\tclear_LDT();\n#endif\n}\n\nstatic inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)\n{\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t/*\n\t * Load the LDT if either the old or new mm had an LDT.\n\t *\n\t * An mm will never go from having an LDT to not having an LDT.  Two\n\t * mms never share an LDT, so we don't gain anything by checking to\n\t * see whether the LDT changed.  There's also no guarantee that\n\t * prev->context.ldt actually matches LDTR, but, if LDTR is non-NULL,\n\t * then prev->context.ldt will also be non-NULL.\n\t *\n\t * If we really cared, we could optimize the case where prev == next\n\t * and we're exiting lazy mode.  Most of the time, if this happens,\n\t * we don't actually need to reload LDTR, but modify_ldt() is mostly\n\t * used by legacy code and emulators where we don't need this level of\n\t * performance.\n\t *\n\t * This uses | instead of || because it generates better code.\n\t */\n\tif (unlikely((unsigned long)prev->context.ldt |\n\t\t     (unsigned long)next->context.ldt))\n\t\tload_mm_ldt(next);\n#endif\n\n\tDEBUG_LOCKS_WARN_ON(preemptible());\n}\n\nstatic inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)\n{\n\tif (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);\n}\n\nstatic inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\t#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (cpu_feature_enabled(X86_FEATURE_OSPKE)) {\n\t\t/* pkey 0 is the default and always allocated */\n\t\tmm->context.pkey_allocation_map = 0x1;\n\t\t/* -1 means unallocated or invalid */\n\t\tmm->context.execute_only_pkey = -1;\n\t}\n\t#endif\n\treturn init_new_context_ldt(tsk, mm);\n}\nstatic inline void destroy_context(struct mm_struct *mm)\n{\n\tdestroy_context_ldt(mm);\n}\n\nextern void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t      struct task_struct *tsk);\n\nextern void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t       struct task_struct *tsk);\n#define switch_mm_irqs_off switch_mm_irqs_off\n\n#define activate_mm(prev, next)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tparavirt_activate_mm((prev), (next));\t\\\n\tswitch_mm((prev), (next), NULL);\t\\\n} while (0);\n\n#ifdef CONFIG_X86_32\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tlazy_load_gs(0);\t\t\t\\\n} while (0)\n#else\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tload_gs_index(0);\t\t\t\\\n\tloadsegment(fs, 0);\t\t\t\\\n} while (0)\n#endif\n\nstatic inline void arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t struct mm_struct *mm)\n{\n\tparavirt_arch_dup_mmap(oldmm, mm);\n}\n\nstatic inline void arch_exit_mmap(struct mm_struct *mm)\n{\n\tparavirt_arch_exit_mmap(mm);\n}\n\n#ifdef CONFIG_X86_64\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn\t!IS_ENABLED(CONFIG_IA32_EMULATION) ||\n\t\t!(mm->context.ia32_compat == TIF_IA32);\n}\n#else\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn false;\n}\n#endif\n\nstatic inline void arch_bprm_mm_init(struct mm_struct *mm,\n\t\tstruct vm_area_struct *vma)\n{\n\tmpx_mm_init(mm);\n}\n\nstatic inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t      unsigned long start, unsigned long end)\n{\n\t/*\n\t * mpx_notify_unmap() goes and reads a rarely-hot\n\t * cacheline in the mm_struct.  That can be expensive\n\t * enough to be seen in profiles.\n\t *\n\t * The mpx_notify_unmap() call and its contents have been\n\t * observed to affect munmap() performance on hardware\n\t * where MPX is not present.\n\t *\n\t * The unlikely() optimizes for the fast case: no MPX\n\t * in the CPU, or no MPX use in the process.  Even if\n\t * we get this wrong (in the unlikely event that MPX\n\t * is widely enabled on some system) the overhead of\n\t * MPX itself (reading bounds tables) is expected to\n\t * overwhelm the overhead of getting this unlikely()\n\t * consistently wrong.\n\t */\n\tif (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))\n\t\tmpx_notify_unmap(mm, vma, start, end);\n}\n\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\nstatic inline int vma_pkey(struct vm_area_struct *vma)\n{\n\tunsigned long vma_pkey_mask = VM_PKEY_BIT0 | VM_PKEY_BIT1 |\n\t\t\t\t      VM_PKEY_BIT2 | VM_PKEY_BIT3;\n\n\treturn (vma->vm_flags & vma_pkey_mask) >> VM_PKEY_SHIFT;\n}\n#else\nstatic inline int vma_pkey(struct vm_area_struct *vma)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * We only want to enforce protection keys on the current process\n * because we effectively have no access to PKRU for other\n * processes or any way to tell *which * PKRU in a threaded\n * process we could use.\n *\n * So do not enforce things if the VMA is not from the current\n * mm, or if we are in a kernel thread.\n */\nstatic inline bool vma_is_foreign(struct vm_area_struct *vma)\n{\n\tif (!current->mm)\n\t\treturn true;\n\t/*\n\t * Should PKRU be enforced on the access to this VMA?  If\n\t * the VMA is from another process, then PKRU has no\n\t * relevance and should not be enforced.\n\t */\n\tif (current->mm != vma->vm_mm)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool arch_vma_access_permitted(struct vm_area_struct *vma,\n\t\tbool write, bool execute, bool foreign)\n{\n\t/* pkeys never affect instruction fetches */\n\tif (execute)\n\t\treturn true;\n\t/* allow access if the VMA is not one from this process */\n\tif (foreign || vma_is_foreign(vma))\n\t\treturn true;\n\treturn __pkru_allows_pkey(vma_pkey(vma), write);\n}\n\n\n/*\n * This can be used from process context to figure out what the value of\n * CR3 is without needing to do a (slow) __read_cr3().\n *\n * It's intended to be used for code like KVM that sneakily changes CR3\n * and needs to restore it.  It needs to be used very carefully.\n */\nstatic inline unsigned long __get_current_cr3_fast(void)\n{\n\tunsigned long cr3 = __pa(this_cpu_read(cpu_tlbstate.loaded_mm)->pgd);\n\n\t/* For now, be very restrictive about when this can be called. */\n\tVM_WARN_ON(in_nmi() || preemptible());\n\n\tVM_BUG_ON(cr3 != __read_cr3());\n\treturn cr3;\n}\n\n#endif /* _ASM_X86_MMU_CONTEXT_H */\n"], "filenames": ["arch/x86/include/asm/mmu_context.h"], "buggy_code_start_loc": [143], "buggy_code_end_loc": [146], "fixing_code_start_loc": [143], "fixing_code_end_loc": [144], "type": "CWE-416", "message": "The init_new_context function in arch/x86/include/asm/mmu_context.h in the Linux kernel before 4.12.10 does not correctly handle errors from LDT table allocation when forking a new process, allowing a local attacker to achieve a use-after-free or possibly have unspecified other impact by running a specially crafted program. This vulnerability only affected kernels built with CONFIG_MODIFY_LDT_SYSCALL=y.", "other": {"cve": {"id": "CVE-2017-17053", "sourceIdentifier": "cve@mitre.org", "published": "2017-11-29T03:29:00.300", "lastModified": "2018-12-19T14:04:51.890", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The init_new_context function in arch/x86/include/asm/mmu_context.h in the Linux kernel before 4.12.10 does not correctly handle errors from LDT table allocation when forking a new process, allowing a local attacker to achieve a use-after-free or possibly have unspecified other impact by running a specially crafted program. This vulnerability only affected kernels built with CONFIG_MODIFY_LDT_SYSCALL=y."}, {"lang": "es", "value": "La funci\u00f3n init_new_context en arch/x86/include/asm/mmu_context.h en el kernel de Linux en versiones anteriores a la 4.12.10 no gestiona errores de asignaci\u00f3n de tablas LDT al bifurcar un nuevo proceso. Esto permite que un atacante local logre un uso de memoria previamente liberada o que, posiblemente, tenga otro impacto sin especificar ejecutando un programa especialmente manipulado. Esta vulnerabilidad solo afecta a los kernels construidos con CONFIG_MODIFY_LDT_SYSCALL=y."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.12.10", "matchCriteriaId": "EB6F4341-55E4-4E6C-95A9-89265E0566CB"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=ccd5b3235180eef3cfec337df1c8554ab151b5cc", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch"]}, {"url": "http://www.securityfocus.com/bid/102010", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:0676", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/ccd5b3235180eef3cfec337df1c8554ab151b5cc", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.12.10", "source": "cve@mitre.org", "tags": ["Issue Tracking"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/ccd5b3235180eef3cfec337df1c8554ab151b5cc"}}