{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tSupport for INET connection oriented protocols.\n *\n * Authors:\tSee the TCP sources\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or(at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/jhash.h>\n\n#include <net/inet_connection_sock.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_timewait_sock.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/tcp_states.h>\n#include <net/xfrm.h>\n#include <net/tcp.h>\n#include <net/sock_reuseport.h>\n\n#ifdef INET_CSK_DEBUG\nconst char inet_csk_timer_bug_msg[] = \"inet_csk BUG: unknown timer value\\n\";\nEXPORT_SYMBOL(inet_csk_timer_bug_msg);\n#endif\n\n#if IS_ENABLED(CONFIG_IPV6)\n/* match_wildcard == true:  IPV6_ADDR_ANY equals to any IPv6 addresses if IPv6\n *                          only, and any IPv4 addresses if not IPv6 only\n * match_wildcard == false: addresses must be exactly the same, i.e.\n *                          IPV6_ADDR_ANY only equals to IPV6_ADDR_ANY,\n *                          and 0.0.0.0 equals to 0.0.0.0 only\n */\nstatic int ipv6_rcv_saddr_equal(const struct in6_addr *sk1_rcv_saddr6,\n\t\t\t\tconst struct in6_addr *sk2_rcv_saddr6,\n\t\t\t\t__be32 sk1_rcv_saddr, __be32 sk2_rcv_saddr,\n\t\t\t\tbool sk1_ipv6only, bool sk2_ipv6only,\n\t\t\t\tbool match_wildcard)\n{\n\tint addr_type = ipv6_addr_type(sk1_rcv_saddr6);\n\tint addr_type2 = sk2_rcv_saddr6 ? ipv6_addr_type(sk2_rcv_saddr6) : IPV6_ADDR_MAPPED;\n\n\t/* if both are mapped, treat as IPv4 */\n\tif (addr_type == IPV6_ADDR_MAPPED && addr_type2 == IPV6_ADDR_MAPPED) {\n\t\tif (!sk2_ipv6only) {\n\t\t\tif (sk1_rcv_saddr == sk2_rcv_saddr)\n\t\t\t\treturn 1;\n\t\t\tif (!sk1_rcv_saddr || !sk2_rcv_saddr)\n\t\t\t\treturn match_wildcard;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (addr_type == IPV6_ADDR_ANY && addr_type2 == IPV6_ADDR_ANY)\n\t\treturn 1;\n\n\tif (addr_type2 == IPV6_ADDR_ANY && match_wildcard &&\n\t    !(sk2_ipv6only && addr_type == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (addr_type == IPV6_ADDR_ANY && match_wildcard &&\n\t    !(sk1_ipv6only && addr_type2 == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (sk2_rcv_saddr6 &&\n\t    ipv6_addr_equal(sk1_rcv_saddr6, sk2_rcv_saddr6))\n\t\treturn 1;\n\n\treturn 0;\n}\n#endif\n\n/* match_wildcard == true:  0.0.0.0 equals to any IPv4 addresses\n * match_wildcard == false: addresses must be exactly the same, i.e.\n *                          0.0.0.0 only equals to 0.0.0.0\n */\nstatic int ipv4_rcv_saddr_equal(__be32 sk1_rcv_saddr, __be32 sk2_rcv_saddr,\n\t\t\t\tbool sk2_ipv6only, bool match_wildcard)\n{\n\tif (!sk2_ipv6only) {\n\t\tif (sk1_rcv_saddr == sk2_rcv_saddr)\n\t\t\treturn 1;\n\t\tif (!sk1_rcv_saddr || !sk2_rcv_saddr)\n\t\t\treturn match_wildcard;\n\t}\n\treturn 0;\n}\n\nint inet_rcv_saddr_equal(const struct sock *sk, const struct sock *sk2,\n\t\t\t bool match_wildcard)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == AF_INET6)\n\t\treturn ipv6_rcv_saddr_equal(&sk->sk_v6_rcv_saddr,\n\t\t\t\t\t    inet6_rcv_saddr(sk2),\n\t\t\t\t\t    sk->sk_rcv_saddr,\n\t\t\t\t\t    sk2->sk_rcv_saddr,\n\t\t\t\t\t    ipv6_only_sock(sk),\n\t\t\t\t\t    ipv6_only_sock(sk2),\n\t\t\t\t\t    match_wildcard);\n#endif\n\treturn ipv4_rcv_saddr_equal(sk->sk_rcv_saddr, sk2->sk_rcv_saddr,\n\t\t\t\t    ipv6_only_sock(sk2), match_wildcard);\n}\nEXPORT_SYMBOL(inet_rcv_saddr_equal);\n\nvoid inet_get_local_port_range(struct net *net, int *low, int *high)\n{\n\tunsigned int seq;\n\n\tdo {\n\t\tseq = read_seqbegin(&net->ipv4.ip_local_ports.lock);\n\n\t\t*low = net->ipv4.ip_local_ports.range[0];\n\t\t*high = net->ipv4.ip_local_ports.range[1];\n\t} while (read_seqretry(&net->ipv4.ip_local_ports.lock, seq));\n}\nEXPORT_SYMBOL(inet_get_local_port_range);\n\nstatic int inet_csk_bind_conflict(const struct sock *sk,\n\t\t\t\t  const struct inet_bind_bucket *tb,\n\t\t\t\t  bool relax, bool reuseport_ok)\n{\n\tstruct sock *sk2;\n\tbool reuse = sk->sk_reuse;\n\tbool reuseport = !!sk->sk_reuseport && reuseport_ok;\n\tkuid_t uid = sock_i_uid((struct sock *)sk);\n\n\t/*\n\t * Unlike other sk lookup places we do not check\n\t * for sk_net here, since _all_ the socks listed\n\t * in tb->owners list belong to the same net - the\n\t * one this bucket belongs to.\n\t */\n\n\tsk_for_each_bound(sk2, &tb->owners) {\n\t\tif (sk != sk2 &&\n\t\t    (!sk->sk_bound_dev_if ||\n\t\t     !sk2->sk_bound_dev_if ||\n\t\t     sk->sk_bound_dev_if == sk2->sk_bound_dev_if)) {\n\t\t\tif ((!reuse || !sk2->sk_reuse ||\n\t\t\t    sk2->sk_state == TCP_LISTEN) &&\n\t\t\t    (!reuseport || !sk2->sk_reuseport ||\n\t\t\t     rcu_access_pointer(sk->sk_reuseport_cb) ||\n\t\t\t     (sk2->sk_state != TCP_TIME_WAIT &&\n\t\t\t     !uid_eq(uid, sock_i_uid(sk2))))) {\n\t\t\t\tif (inet_rcv_saddr_equal(sk, sk2, true))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!relax && reuse && sk2->sk_reuse &&\n\t\t\t    sk2->sk_state != TCP_LISTEN) {\n\t\t\t\tif (inet_rcv_saddr_equal(sk, sk2, true))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn sk2 != NULL;\n}\n\n/*\n * Find an open port number for the socket.  Returns with the\n * inet_bind_hashbucket lock held.\n */\nstatic struct inet_bind_hashbucket *\ninet_csk_find_open_port(struct sock *sk, struct inet_bind_bucket **tb_ret, int *port_ret)\n{\n\tstruct inet_hashinfo *hinfo = sk->sk_prot->h.hashinfo;\n\tint port = 0;\n\tstruct inet_bind_hashbucket *head;\n\tstruct net *net = sock_net(sk);\n\tint i, low, high, attempt_half;\n\tstruct inet_bind_bucket *tb;\n\tu32 remaining, offset;\n\n\tattempt_half = (sk->sk_reuse == SK_CAN_REUSE) ? 1 : 0;\nother_half_scan:\n\tinet_get_local_port_range(net, &low, &high);\n\thigh++; /* [32768, 60999] -> [32768, 61000[ */\n\tif (high - low < 4)\n\t\tattempt_half = 0;\n\tif (attempt_half) {\n\t\tint half = low + (((high - low) >> 2) << 1);\n\n\t\tif (attempt_half == 1)\n\t\t\thigh = half;\n\t\telse\n\t\t\tlow = half;\n\t}\n\tremaining = high - low;\n\tif (likely(remaining > 1))\n\t\tremaining &= ~1U;\n\n\toffset = prandom_u32() % remaining;\n\t/* __inet_hash_connect() favors ports having @low parity\n\t * We do the opposite to not pollute connect() users.\n\t */\n\toffset |= 1U;\n\nother_parity_scan:\n\tport = low + offset;\n\tfor (i = 0; i < remaining; i += 2, port += 2) {\n\t\tif (unlikely(port >= high))\n\t\t\tport -= remaining;\n\t\tif (inet_is_local_reserved_port(net, port))\n\t\t\tcontinue;\n\t\thead = &hinfo->bhash[inet_bhashfn(net, port,\n\t\t\t\t\t\t  hinfo->bhash_size)];\n\t\tspin_lock_bh(&head->lock);\n\t\tinet_bind_bucket_for_each(tb, &head->chain)\n\t\t\tif (net_eq(ib_net(tb), net) && tb->port == port) {\n\t\t\t\tif (!inet_csk_bind_conflict(sk, tb, false, false))\n\t\t\t\t\tgoto success;\n\t\t\t\tgoto next_port;\n\t\t\t}\n\t\ttb = NULL;\n\t\tgoto success;\nnext_port:\n\t\tspin_unlock_bh(&head->lock);\n\t\tcond_resched();\n\t}\n\n\toffset--;\n\tif (!(offset & 1))\n\t\tgoto other_parity_scan;\n\n\tif (attempt_half == 1) {\n\t\t/* OK we now try the upper half of the range */\n\t\tattempt_half = 2;\n\t\tgoto other_half_scan;\n\t}\n\treturn NULL;\nsuccess:\n\t*port_ret = port;\n\t*tb_ret = tb;\n\treturn head;\n}\n\nstatic inline int sk_reuseport_match(struct inet_bind_bucket *tb,\n\t\t\t\t     struct sock *sk)\n{\n\tkuid_t uid = sock_i_uid(sk);\n\n\tif (tb->fastreuseport <= 0)\n\t\treturn 0;\n\tif (!sk->sk_reuseport)\n\t\treturn 0;\n\tif (rcu_access_pointer(sk->sk_reuseport_cb))\n\t\treturn 0;\n\tif (!uid_eq(tb->fastuid, uid))\n\t\treturn 0;\n\t/* We only need to check the rcv_saddr if this tb was once marked\n\t * without fastreuseport and then was reset, as we can only know that\n\t * the fast_*rcv_saddr doesn't have any conflicts with the socks on the\n\t * owners list.\n\t */\n\tif (tb->fastreuseport == FASTREUSEPORT_ANY)\n\t\treturn 1;\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (tb->fast_sk_family == AF_INET6)\n\t\treturn ipv6_rcv_saddr_equal(&tb->fast_v6_rcv_saddr,\n\t\t\t\t\t    &sk->sk_v6_rcv_saddr,\n\t\t\t\t\t    tb->fast_rcv_saddr,\n\t\t\t\t\t    sk->sk_rcv_saddr,\n\t\t\t\t\t    tb->fast_ipv6_only,\n\t\t\t\t\t    ipv6_only_sock(sk), true);\n#endif\n\treturn ipv4_rcv_saddr_equal(tb->fast_rcv_saddr, sk->sk_rcv_saddr,\n\t\t\t\t    ipv6_only_sock(sk), true);\n}\n\n/* Obtain a reference to a local port for the given sock,\n * if snum is zero it means select any available local port.\n * We try to allocate an odd port (and leave even ports for connect())\n */\nint inet_csk_get_port(struct sock *sk, unsigned short snum)\n{\n\tbool reuse = sk->sk_reuse && sk->sk_state != TCP_LISTEN;\n\tstruct inet_hashinfo *hinfo = sk->sk_prot->h.hashinfo;\n\tint ret = 1, port = snum;\n\tstruct inet_bind_hashbucket *head;\n\tstruct net *net = sock_net(sk);\n\tstruct inet_bind_bucket *tb = NULL;\n\tkuid_t uid = sock_i_uid(sk);\n\n\tif (!port) {\n\t\thead = inet_csk_find_open_port(sk, &tb, &port);\n\t\tif (!head)\n\t\t\treturn ret;\n\t\tif (!tb)\n\t\t\tgoto tb_not_found;\n\t\tgoto success;\n\t}\n\thead = &hinfo->bhash[inet_bhashfn(net, port,\n\t\t\t\t\t  hinfo->bhash_size)];\n\tspin_lock_bh(&head->lock);\n\tinet_bind_bucket_for_each(tb, &head->chain)\n\t\tif (net_eq(ib_net(tb), net) && tb->port == port)\n\t\t\tgoto tb_found;\ntb_not_found:\n\ttb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,\n\t\t\t\t     net, head, port);\n\tif (!tb)\n\t\tgoto fail_unlock;\ntb_found:\n\tif (!hlist_empty(&tb->owners)) {\n\t\tif (sk->sk_reuse == SK_FORCE_REUSE)\n\t\t\tgoto success;\n\n\t\tif ((tb->fastreuse > 0 && reuse) ||\n\t\t    sk_reuseport_match(tb, sk))\n\t\t\tgoto success;\n\t\tif (inet_csk_bind_conflict(sk, tb, true, true))\n\t\t\tgoto fail_unlock;\n\t}\nsuccess:\n\tif (!hlist_empty(&tb->owners)) {\n\t\ttb->fastreuse = reuse;\n\t\tif (sk->sk_reuseport) {\n\t\t\ttb->fastreuseport = FASTREUSEPORT_ANY;\n\t\t\ttb->fastuid = uid;\n\t\t\ttb->fast_rcv_saddr = sk->sk_rcv_saddr;\n\t\t\ttb->fast_ipv6_only = ipv6_only_sock(sk);\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\ttb->fast_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n#endif\n\t\t} else {\n\t\t\ttb->fastreuseport = 0;\n\t\t}\n\t} else {\n\t\tif (!reuse)\n\t\t\ttb->fastreuse = 0;\n\t\tif (sk->sk_reuseport) {\n\t\t\t/* We didn't match or we don't have fastreuseport set on\n\t\t\t * the tb, but we have sk_reuseport set on this socket\n\t\t\t * and we know that there are no bind conflicts with\n\t\t\t * this socket in this tb, so reset our tb's reuseport\n\t\t\t * settings so that any subsequent sockets that match\n\t\t\t * our current socket will be put on the fast path.\n\t\t\t *\n\t\t\t * If we reset we need to set FASTREUSEPORT_STRICT so we\n\t\t\t * do extra checking for all subsequent sk_reuseport\n\t\t\t * socks.\n\t\t\t */\n\t\t\tif (!sk_reuseport_match(tb, sk)) {\n\t\t\t\ttb->fastreuseport = FASTREUSEPORT_STRICT;\n\t\t\t\ttb->fastuid = uid;\n\t\t\t\ttb->fast_rcv_saddr = sk->sk_rcv_saddr;\n\t\t\t\ttb->fast_ipv6_only = ipv6_only_sock(sk);\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\t\ttb->fast_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n#endif\n\t\t\t}\n\t\t} else {\n\t\t\ttb->fastreuseport = 0;\n\t\t}\n\t}\n\tif (!inet_csk(sk)->icsk_bind_hash)\n\t\tinet_bind_hash(sk, tb, port);\n\tWARN_ON(inet_csk(sk)->icsk_bind_hash != tb);\n\tret = 0;\n\nfail_unlock:\n\tspin_unlock_bh(&head->lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(inet_csk_get_port);\n\n/*\n * Wait for an incoming connection, avoid race conditions. This must be called\n * with the socket locked.\n */\nstatic int inet_csk_wait_for_connect(struct sock *sk, long timeo)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tDEFINE_WAIT(wait);\n\tint err;\n\n\t/*\n\t * True wake-one mechanism for incoming connections: only\n\t * one process gets woken up, not the 'whole herd'.\n\t * Since we do not 'race & poll' for established sockets\n\t * anymore, the common case will execute the loop only once.\n\t *\n\t * Subtle issue: \"add_wait_queue_exclusive()\" will be added\n\t * after any current non-exclusive waiters, and we know that\n\t * it will always _stay_ after any new non-exclusive waiters\n\t * because all non-exclusive waiters are added at the\n\t * beginning of the wait-queue. As such, it's ok to \"drop\"\n\t * our exclusiveness temporarily when we get woken up without\n\t * having to remove and re-insert us on the wait queue.\n\t */\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(sk_sleep(sk), &wait,\n\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\trelease_sock(sk);\n\t\tif (reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\ttimeo = schedule_timeout(timeo);\n\t\tsched_annotate_sleep();\n\t\tlock_sock(sk);\n\t\terr = 0;\n\t\tif (!reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\tbreak;\n\t\terr = -EINVAL;\n\t\tif (sk->sk_state != TCP_LISTEN)\n\t\t\tbreak;\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tbreak;\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn err;\n}\n\n/*\n * This will accept the next outstanding connection.\n */\nstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err, bool kern)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tstruct request_sock *req;\n\tstruct sock *newsk;\n\tint error;\n\n\tlock_sock(sk);\n\n\t/* We need to make sure that this socket is listening,\n\t * and that it has something pending.\n\t */\n\terror = -EINVAL;\n\tif (sk->sk_state != TCP_LISTEN)\n\t\tgoto out_err;\n\n\t/* Find already established connection */\n\tif (reqsk_queue_empty(queue)) {\n\t\tlong timeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\n\n\t\t/* If this is a non blocking socket don't sleep */\n\t\terror = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_err;\n\n\t\terror = inet_csk_wait_for_connect(sk, timeo);\n\t\tif (error)\n\t\t\tgoto out_err;\n\t}\n\treq = reqsk_queue_remove(queue, sk);\n\tnewsk = req->sk;\n\n\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t    tcp_rsk(req)->tfo_listener) {\n\t\tspin_lock_bh(&queue->fastopenq.lock);\n\t\tif (tcp_rsk(req)->tfo_listener) {\n\t\t\t/* We are still waiting for the final ACK from 3WHS\n\t\t\t * so can't free req now. Instead, we set req->sk to\n\t\t\t * NULL to signify that the child socket is taken\n\t\t\t * so reqsk_fastopen_remove() will free the req\n\t\t\t * when 3WHS finishes (or is aborted).\n\t\t\t */\n\t\t\treq->sk = NULL;\n\t\t\treq = NULL;\n\t\t}\n\t\tspin_unlock_bh(&queue->fastopenq.lock);\n\t}\nout:\n\trelease_sock(sk);\n\tif (req)\n\t\treqsk_put(req);\n\treturn newsk;\nout_err:\n\tnewsk = NULL;\n\treq = NULL;\n\t*err = error;\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_csk_accept);\n\n/*\n * Using different timers for retransmit, delayed acks and probes\n * We may wish use just one timer maintaining a list of expire jiffies\n * to optimize.\n */\nvoid inet_csk_init_xmit_timers(struct sock *sk,\n\t\t\t       void (*retransmit_handler)(unsigned long),\n\t\t\t       void (*delack_handler)(unsigned long),\n\t\t\t       void (*keepalive_handler)(unsigned long))\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tsetup_timer(&icsk->icsk_retransmit_timer, retransmit_handler,\n\t\t\t(unsigned long)sk);\n\tsetup_timer(&icsk->icsk_delack_timer, delack_handler,\n\t\t\t(unsigned long)sk);\n\tsetup_timer(&sk->sk_timer, keepalive_handler, (unsigned long)sk);\n\ticsk->icsk_pending = icsk->icsk_ack.pending = 0;\n}\nEXPORT_SYMBOL(inet_csk_init_xmit_timers);\n\nvoid inet_csk_clear_xmit_timers(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_pending = icsk->icsk_ack.pending = icsk->icsk_ack.blocked = 0;\n\n\tsk_stop_timer(sk, &icsk->icsk_retransmit_timer);\n\tsk_stop_timer(sk, &icsk->icsk_delack_timer);\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_clear_xmit_timers);\n\nvoid inet_csk_delete_keepalive_timer(struct sock *sk)\n{\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_delete_keepalive_timer);\n\nvoid inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long len)\n{\n\tsk_reset_timer(sk, &sk->sk_timer, jiffies + len);\n}\nEXPORT_SYMBOL(inet_csk_reset_keepalive_timer);\n\nstruct dst_entry *inet_csk_route_req(const struct sock *sk,\n\t\t\t\t     struct flowi4 *fl4,\n\t\t\t\t     const struct request_sock *req)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct net *net = read_pnet(&ireq->ireq_net);\n\tstruct ip_options_rcu *opt = ireq->opt;\n\tstruct rtable *rt;\n\n\tflowi4_init_output(fl4, ireq->ir_iif, ireq->ir_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\n\t\t\t   ireq->ir_loc_addr, ireq->ir_rmt_port,\n\t\t\t   htons(ireq->ir_num), sk->sk_uid);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(fl4));\n\trt = ip_route_output_flow(net, fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\t__IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_route_req);\n\nstruct dst_entry *inet_csk_route_child_sock(const struct sock *sk,\n\t\t\t\t\t    struct sock *newsk,\n\t\t\t\t\t    const struct request_sock *req)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct net *net = read_pnet(&ireq->ireq_net);\n\tstruct inet_sock *newinet = inet_sk(newsk);\n\tstruct ip_options_rcu *opt;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\n\tfl4 = &newinet->cork.fl.u.ip4;\n\n\trcu_read_lock();\n\topt = rcu_dereference(newinet->inet_opt);\n\tflowi4_init_output(fl4, ireq->ir_iif, ireq->ir_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\n\t\t\t   ireq->ir_loc_addr, ireq->ir_rmt_port,\n\t\t\t   htons(ireq->ir_num), sk->sk_uid);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(fl4));\n\trt = ip_route_output_flow(net, fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\n\t\tgoto route_err;\n\trcu_read_unlock();\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\trcu_read_unlock();\n\t__IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_route_child_sock);\n\n#if IS_ENABLED(CONFIG_IPV6)\n#define AF_INET_FAMILY(fam) ((fam) == AF_INET)\n#else\n#define AF_INET_FAMILY(fam) true\n#endif\n\n/* Decide when to expire the request and when to resend SYN-ACK */\nstatic inline void syn_ack_recalc(struct request_sock *req, const int thresh,\n\t\t\t\t  const int max_retries,\n\t\t\t\t  const u8 rskq_defer_accept,\n\t\t\t\t  int *expire, int *resend)\n{\n\tif (!rskq_defer_accept) {\n\t\t*expire = req->num_timeout >= thresh;\n\t\t*resend = 1;\n\t\treturn;\n\t}\n\t*expire = req->num_timeout >= thresh &&\n\t\t  (!inet_rsk(req)->acked || req->num_timeout >= max_retries);\n\t/*\n\t * Do not resend while waiting for data after ACK,\n\t * start to resend on end of deferring period to give\n\t * last chance for data or ACK to create established socket.\n\t */\n\t*resend = !inet_rsk(req)->acked ||\n\t\t  req->num_timeout >= rskq_defer_accept - 1;\n}\n\nint inet_rtx_syn_ack(const struct sock *parent, struct request_sock *req)\n{\n\tint err = req->rsk_ops->rtx_syn_ack(parent, req);\n\n\tif (!err)\n\t\treq->num_retrans++;\n\treturn err;\n}\nEXPORT_SYMBOL(inet_rtx_syn_ack);\n\n/* return true if req was found in the ehash table */\nstatic bool reqsk_queue_unlink(struct request_sock_queue *queue,\n\t\t\t       struct request_sock *req)\n{\n\tstruct inet_hashinfo *hashinfo = req_to_sk(req)->sk_prot->h.hashinfo;\n\tbool found = false;\n\n\tif (sk_hashed(req_to_sk(req))) {\n\t\tspinlock_t *lock = inet_ehash_lockp(hashinfo, req->rsk_hash);\n\n\t\tspin_lock(lock);\n\t\tfound = __sk_nulls_del_node_init_rcu(req_to_sk(req));\n\t\tspin_unlock(lock);\n\t}\n\tif (timer_pending(&req->rsk_timer) && del_timer_sync(&req->rsk_timer))\n\t\treqsk_put(req);\n\treturn found;\n}\n\nvoid inet_csk_reqsk_queue_drop(struct sock *sk, struct request_sock *req)\n{\n\tif (reqsk_queue_unlink(&inet_csk(sk)->icsk_accept_queue, req)) {\n\t\treqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);\n\t\treqsk_put(req);\n\t}\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_drop);\n\nvoid inet_csk_reqsk_queue_drop_and_put(struct sock *sk, struct request_sock *req)\n{\n\tinet_csk_reqsk_queue_drop(sk, req);\n\treqsk_put(req);\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_drop_and_put);\n\nstatic void reqsk_timer_handler(unsigned long data)\n{\n\tstruct request_sock *req = (struct request_sock *)data;\n\tstruct sock *sk_listener = req->rsk_listener;\n\tstruct net *net = sock_net(sk_listener);\n\tstruct inet_connection_sock *icsk = inet_csk(sk_listener);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tint qlen, expire = 0, resend = 0;\n\tint max_retries, thresh;\n\tu8 defer_accept;\n\n\tif (sk_state_load(sk_listener) != TCP_LISTEN)\n\t\tgoto drop;\n\n\tmax_retries = icsk->icsk_syn_retries ? : net->ipv4.sysctl_tcp_synack_retries;\n\tthresh = max_retries;\n\t/* Normally all the openreqs are young and become mature\n\t * (i.e. converted to established socket) for first timeout.\n\t * If synack was not acknowledged for 1 second, it means\n\t * one of the following things: synack was lost, ack was lost,\n\t * rtt is high or nobody planned to ack (i.e. synflood).\n\t * When server is a bit loaded, queue is populated with old\n\t * open requests, reducing effective size of queue.\n\t * When server is well loaded, queue size reduces to zero\n\t * after several minutes of work. It is not synflood,\n\t * it is normal operation. The solution is pruning\n\t * too old entries overriding normal timeout, when\n\t * situation becomes dangerous.\n\t *\n\t * Essentially, we reserve half of room for young\n\t * embrions; and abort old ones without pity, if old\n\t * ones are about to clog our table.\n\t */\n\tqlen = reqsk_queue_len(queue);\n\tif ((qlen << 1) > max(8U, sk_listener->sk_max_ack_backlog)) {\n\t\tint young = reqsk_queue_len_young(queue) << 1;\n\n\t\twhile (thresh > 2) {\n\t\t\tif (qlen < young)\n\t\t\t\tbreak;\n\t\t\tthresh--;\n\t\t\tyoung <<= 1;\n\t\t}\n\t}\n\tdefer_accept = READ_ONCE(queue->rskq_defer_accept);\n\tif (defer_accept)\n\t\tmax_retries = defer_accept;\n\tsyn_ack_recalc(req, thresh, max_retries, defer_accept,\n\t\t       &expire, &resend);\n\treq->rsk_ops->syn_ack_timeout(req);\n\tif (!expire &&\n\t    (!resend ||\n\t     !inet_rtx_syn_ack(sk_listener, req) ||\n\t     inet_rsk(req)->acked)) {\n\t\tunsigned long timeo;\n\n\t\tif (req->num_timeout++ == 0)\n\t\t\tatomic_dec(&queue->young);\n\t\ttimeo = min(TCP_TIMEOUT_INIT << req->num_timeout, TCP_RTO_MAX);\n\t\tmod_timer(&req->rsk_timer, jiffies + timeo);\n\t\treturn;\n\t}\ndrop:\n\tinet_csk_reqsk_queue_drop_and_put(sk_listener, req);\n}\n\nstatic void reqsk_queue_hash_req(struct request_sock *req,\n\t\t\t\t unsigned long timeout)\n{\n\treq->num_retrans = 0;\n\treq->num_timeout = 0;\n\treq->sk = NULL;\n\n\tsetup_pinned_timer(&req->rsk_timer, reqsk_timer_handler,\n\t\t\t    (unsigned long)req);\n\tmod_timer(&req->rsk_timer, jiffies + timeout);\n\n\tinet_ehash_insert(req_to_sk(req), NULL);\n\t/* before letting lookups find us, make sure all req fields\n\t * are committed to memory and refcnt initialized.\n\t */\n\tsmp_wmb();\n\tatomic_set(&req->rsk_refcnt, 2 + 1);\n}\n\nvoid inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,\n\t\t\t\t   unsigned long timeout)\n{\n\treqsk_queue_hash_req(req, timeout);\n\tinet_csk_reqsk_queue_added(sk);\n}\nEXPORT_SYMBOL_GPL(inet_csk_reqsk_queue_hash_add);\n\n/**\n *\tinet_csk_clone_lock - clone an inet socket, and lock its clone\n *\t@sk: the socket to clone\n *\t@req: request_sock\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\n *\tCaller must unlock socket even in error path (bh_unlock_sock(newsk))\n */\nstruct sock *inet_csk_clone_lock(const struct sock *sk,\n\t\t\t\t const struct request_sock *req,\n\t\t\t\t const gfp_t priority)\n{\n\tstruct sock *newsk = sk_clone_lock(sk, priority);\n\n\tif (newsk) {\n\t\tstruct inet_connection_sock *newicsk = inet_csk(newsk);\n\n\t\tnewsk->sk_state = TCP_SYN_RECV;\n\t\tnewicsk->icsk_bind_hash = NULL;\n\n\t\tinet_sk(newsk)->inet_dport = inet_rsk(req)->ir_rmt_port;\n\t\tinet_sk(newsk)->inet_num = inet_rsk(req)->ir_num;\n\t\tinet_sk(newsk)->inet_sport = htons(inet_rsk(req)->ir_num);\n\t\tnewsk->sk_write_space = sk_stream_write_space;\n\n\t\t/* listeners have SOCK_RCU_FREE, not the children */\n\t\tsock_reset_flag(newsk, SOCK_RCU_FREE);\n\n\t\tnewsk->sk_mark = inet_rsk(req)->ir_mark;\n\t\tatomic64_set(&newsk->sk_cookie,\n\t\t\t     atomic64_read(&inet_rsk(req)->ir_cookie));\n\n\t\tnewicsk->icsk_retransmits = 0;\n\t\tnewicsk->icsk_backoff\t  = 0;\n\t\tnewicsk->icsk_probes_out  = 0;\n\n\t\t/* Deinitialize accept_queue to trap illegal accesses. */\n\t\tmemset(&newicsk->icsk_accept_queue, 0, sizeof(newicsk->icsk_accept_queue));\n\n\t\tsecurity_inet_csk_clone(newsk, req);\n\t}\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(inet_csk_clone_lock);\n\n/*\n * At this point, there should be no process reference to this\n * socket, and thus no user references at all.  Therefore we\n * can assume the socket waitqueue is inactive and nobody will\n * try to jump onto it.\n */\nvoid inet_csk_destroy_sock(struct sock *sk)\n{\n\tWARN_ON(sk->sk_state != TCP_CLOSE);\n\tWARN_ON(!sock_flag(sk, SOCK_DEAD));\n\n\t/* It cannot be in hash table! */\n\tWARN_ON(!sk_unhashed(sk));\n\n\t/* If it has not 0 inet_sk(sk)->inet_num, it must be bound */\n\tWARN_ON(inet_sk(sk)->inet_num && !inet_csk(sk)->icsk_bind_hash);\n\n\tsk->sk_prot->destroy(sk);\n\n\tsk_stream_kill_queues(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsk_refcnt_debug_release(sk);\n\n\tpercpu_counter_dec(sk->sk_prot->orphan_count);\n\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(inet_csk_destroy_sock);\n\n/* This function allows to force a closure of a socket after the call to\n * tcp/dccp_create_openreq_child().\n */\nvoid inet_csk_prepare_forced_close(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n{\n\t/* sk_clone_lock locked the socket and set refcnt to 2 */\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\n\t/* The below has to be done to allow calling inet_csk_destroy_sock */\n\tsock_set_flag(sk, SOCK_DEAD);\n\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\tinet_sk(sk)->inet_num = 0;\n}\nEXPORT_SYMBOL(inet_csk_prepare_forced_close);\n\nint inet_csk_listen_start(struct sock *sk, int backlog)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err = -EADDRINUSE;\n\n\treqsk_queue_alloc(&icsk->icsk_accept_queue);\n\n\tsk->sk_max_ack_backlog = backlog;\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\t/* There is race window here: we announce ourselves listening,\n\t * but this transition is still not validated by get_port().\n\t * It is OK, because this socket enters to hash table only\n\t * after validation is complete.\n\t */\n\tsk_state_store(sk, TCP_LISTEN);\n\tif (!sk->sk_prot->get_port(sk, inet->inet_num)) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\terr = sk->sk_prot->hash(sk);\n\n\t\tif (likely(!err))\n\t\t\treturn 0;\n\t}\n\n\tsk->sk_state = TCP_CLOSE;\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_start);\n\nstatic void inet_child_forget(struct sock *sk, struct request_sock *req,\n\t\t\t      struct sock *child)\n{\n\tsk->sk_prot->disconnect(child, O_NONBLOCK);\n\n\tsock_orphan(child);\n\n\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\n\tif (sk->sk_protocol == IPPROTO_TCP && tcp_rsk(req)->tfo_listener) {\n\t\tBUG_ON(tcp_sk(child)->fastopen_rsk != req);\n\t\tBUG_ON(sk != req->rsk_listener);\n\n\t\t/* Paranoid, to prevent race condition if\n\t\t * an inbound pkt destined for child is\n\t\t * blocked by sock lock in tcp_v4_rcv().\n\t\t * Also to satisfy an assertion in\n\t\t * tcp_v4_destroy_sock().\n\t\t */\n\t\ttcp_sk(child)->fastopen_rsk = NULL;\n\t}\n\tinet_csk_destroy_sock(child);\n\treqsk_put(req);\n}\n\nstruct sock *inet_csk_reqsk_queue_add(struct sock *sk,\n\t\t\t\t      struct request_sock *req,\n\t\t\t\t      struct sock *child)\n{\n\tstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\n\n\tspin_lock(&queue->rskq_lock);\n\tif (unlikely(sk->sk_state != TCP_LISTEN)) {\n\t\tinet_child_forget(sk, req, child);\n\t\tchild = NULL;\n\t} else {\n\t\treq->sk = child;\n\t\treq->dl_next = NULL;\n\t\tif (queue->rskq_accept_head == NULL)\n\t\t\tqueue->rskq_accept_head = req;\n\t\telse\n\t\t\tqueue->rskq_accept_tail->dl_next = req;\n\t\tqueue->rskq_accept_tail = req;\n\t\tsk_acceptq_added(sk);\n\t}\n\tspin_unlock(&queue->rskq_lock);\n\treturn child;\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_add);\n\nstruct sock *inet_csk_complete_hashdance(struct sock *sk, struct sock *child,\n\t\t\t\t\t struct request_sock *req, bool own_req)\n{\n\tif (own_req) {\n\t\tinet_csk_reqsk_queue_drop(sk, req);\n\t\treqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);\n\t\tif (inet_csk_reqsk_queue_add(sk, req, child))\n\t\t\treturn child;\n\t}\n\t/* Too bad, another child took ownership of the request, undo. */\n\tbh_unlock_sock(child);\n\tsock_put(child);\n\treturn NULL;\n}\nEXPORT_SYMBOL(inet_csk_complete_hashdance);\n\n/*\n *\tThis routine closes sockets which have been at least partially\n *\topened, but not yet accepted.\n */\nvoid inet_csk_listen_stop(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tstruct request_sock *next, *req;\n\n\t/* Following specs, it would be better either to send FIN\n\t * (and enter FIN-WAIT-1, it is normal close)\n\t * or to send active reset (abort).\n\t * Certainly, it is pretty dangerous while synflood, but it is\n\t * bad justification for our negligence 8)\n\t * To be honest, we are not able to make either\n\t * of the variants now.\t\t\t--ANK\n\t */\n\twhile ((req = reqsk_queue_remove(queue, sk)) != NULL) {\n\t\tstruct sock *child = req->sk;\n\n\t\tlocal_bh_disable();\n\t\tbh_lock_sock(child);\n\t\tWARN_ON(sock_owned_by_user(child));\n\t\tsock_hold(child);\n\n\t\tinet_child_forget(sk, req, child);\n\t\tbh_unlock_sock(child);\n\t\tlocal_bh_enable();\n\t\tsock_put(child);\n\n\t\tcond_resched();\n\t}\n\tif (queue->fastopenq.rskq_rst_head) {\n\t\t/* Free all the reqs queued in rskq_rst_head. */\n\t\tspin_lock_bh(&queue->fastopenq.lock);\n\t\treq = queue->fastopenq.rskq_rst_head;\n\t\tqueue->fastopenq.rskq_rst_head = NULL;\n\t\tspin_unlock_bh(&queue->fastopenq.lock);\n\t\twhile (req != NULL) {\n\t\t\tnext = req->dl_next;\n\t\t\treqsk_put(req);\n\t\t\treq = next;\n\t\t}\n\t}\n\tWARN_ON_ONCE(sk->sk_ack_backlog);\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_stop);\n\nvoid inet_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\n{\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)uaddr;\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\tsin->sin_family\t\t= AF_INET;\n\tsin->sin_addr.s_addr\t= inet->inet_daddr;\n\tsin->sin_port\t\t= inet->inet_dport;\n}\nEXPORT_SYMBOL_GPL(inet_csk_addr2sockaddr);\n\n#ifdef CONFIG_COMPAT\nint inet_csk_compat_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_af_ops->compat_getsockopt)\n\t\treturn icsk->icsk_af_ops->compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t\t    optval, optlen);\n\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL_GPL(inet_csk_compat_getsockopt);\n\nint inet_csk_compat_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_af_ops->compat_setsockopt)\n\t\treturn icsk->icsk_af_ops->compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t    optval, optlen);\n\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL_GPL(inet_csk_compat_setsockopt);\n#endif\n\nstatic struct dst_entry *inet_csk_rebuild_route(struct sock *sk, struct flowi *fl)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\tfl4 = &fl->u.ip4;\n\trt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\tif (IS_ERR(rt))\n\t\trt = NULL;\n\tif (rt)\n\t\tsk_setup_caps(sk, &rt->dst);\n\trcu_read_unlock();\n\n\treturn &rt->dst;\n}\n\nstruct dst_entry *inet_csk_update_pmtu(struct sock *sk, u32 mtu)\n{\n\tstruct dst_entry *dst = __sk_dst_check(sk, 0);\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (!dst) {\n\t\tdst = inet_csk_rebuild_route(sk, &inet->cork.fl);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\tdst->ops->update_pmtu(dst, sk, NULL, mtu);\n\n\tdst = __sk_dst_check(sk, 0);\n\tif (!dst)\n\t\tdst = inet_csk_rebuild_route(sk, &inet->cork.fl);\nout:\n\treturn dst;\n}\nEXPORT_SYMBOL_GPL(inet_csk_update_pmtu);\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tSupport for INET connection oriented protocols.\n *\n * Authors:\tSee the TCP sources\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or(at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/jhash.h>\n\n#include <net/inet_connection_sock.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_timewait_sock.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/tcp_states.h>\n#include <net/xfrm.h>\n#include <net/tcp.h>\n#include <net/sock_reuseport.h>\n\n#ifdef INET_CSK_DEBUG\nconst char inet_csk_timer_bug_msg[] = \"inet_csk BUG: unknown timer value\\n\";\nEXPORT_SYMBOL(inet_csk_timer_bug_msg);\n#endif\n\n#if IS_ENABLED(CONFIG_IPV6)\n/* match_wildcard == true:  IPV6_ADDR_ANY equals to any IPv6 addresses if IPv6\n *                          only, and any IPv4 addresses if not IPv6 only\n * match_wildcard == false: addresses must be exactly the same, i.e.\n *                          IPV6_ADDR_ANY only equals to IPV6_ADDR_ANY,\n *                          and 0.0.0.0 equals to 0.0.0.0 only\n */\nstatic int ipv6_rcv_saddr_equal(const struct in6_addr *sk1_rcv_saddr6,\n\t\t\t\tconst struct in6_addr *sk2_rcv_saddr6,\n\t\t\t\t__be32 sk1_rcv_saddr, __be32 sk2_rcv_saddr,\n\t\t\t\tbool sk1_ipv6only, bool sk2_ipv6only,\n\t\t\t\tbool match_wildcard)\n{\n\tint addr_type = ipv6_addr_type(sk1_rcv_saddr6);\n\tint addr_type2 = sk2_rcv_saddr6 ? ipv6_addr_type(sk2_rcv_saddr6) : IPV6_ADDR_MAPPED;\n\n\t/* if both are mapped, treat as IPv4 */\n\tif (addr_type == IPV6_ADDR_MAPPED && addr_type2 == IPV6_ADDR_MAPPED) {\n\t\tif (!sk2_ipv6only) {\n\t\t\tif (sk1_rcv_saddr == sk2_rcv_saddr)\n\t\t\t\treturn 1;\n\t\t\tif (!sk1_rcv_saddr || !sk2_rcv_saddr)\n\t\t\t\treturn match_wildcard;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (addr_type == IPV6_ADDR_ANY && addr_type2 == IPV6_ADDR_ANY)\n\t\treturn 1;\n\n\tif (addr_type2 == IPV6_ADDR_ANY && match_wildcard &&\n\t    !(sk2_ipv6only && addr_type == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (addr_type == IPV6_ADDR_ANY && match_wildcard &&\n\t    !(sk1_ipv6only && addr_type2 == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (sk2_rcv_saddr6 &&\n\t    ipv6_addr_equal(sk1_rcv_saddr6, sk2_rcv_saddr6))\n\t\treturn 1;\n\n\treturn 0;\n}\n#endif\n\n/* match_wildcard == true:  0.0.0.0 equals to any IPv4 addresses\n * match_wildcard == false: addresses must be exactly the same, i.e.\n *                          0.0.0.0 only equals to 0.0.0.0\n */\nstatic int ipv4_rcv_saddr_equal(__be32 sk1_rcv_saddr, __be32 sk2_rcv_saddr,\n\t\t\t\tbool sk2_ipv6only, bool match_wildcard)\n{\n\tif (!sk2_ipv6only) {\n\t\tif (sk1_rcv_saddr == sk2_rcv_saddr)\n\t\t\treturn 1;\n\t\tif (!sk1_rcv_saddr || !sk2_rcv_saddr)\n\t\t\treturn match_wildcard;\n\t}\n\treturn 0;\n}\n\nint inet_rcv_saddr_equal(const struct sock *sk, const struct sock *sk2,\n\t\t\t bool match_wildcard)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == AF_INET6)\n\t\treturn ipv6_rcv_saddr_equal(&sk->sk_v6_rcv_saddr,\n\t\t\t\t\t    inet6_rcv_saddr(sk2),\n\t\t\t\t\t    sk->sk_rcv_saddr,\n\t\t\t\t\t    sk2->sk_rcv_saddr,\n\t\t\t\t\t    ipv6_only_sock(sk),\n\t\t\t\t\t    ipv6_only_sock(sk2),\n\t\t\t\t\t    match_wildcard);\n#endif\n\treturn ipv4_rcv_saddr_equal(sk->sk_rcv_saddr, sk2->sk_rcv_saddr,\n\t\t\t\t    ipv6_only_sock(sk2), match_wildcard);\n}\nEXPORT_SYMBOL(inet_rcv_saddr_equal);\n\nvoid inet_get_local_port_range(struct net *net, int *low, int *high)\n{\n\tunsigned int seq;\n\n\tdo {\n\t\tseq = read_seqbegin(&net->ipv4.ip_local_ports.lock);\n\n\t\t*low = net->ipv4.ip_local_ports.range[0];\n\t\t*high = net->ipv4.ip_local_ports.range[1];\n\t} while (read_seqretry(&net->ipv4.ip_local_ports.lock, seq));\n}\nEXPORT_SYMBOL(inet_get_local_port_range);\n\nstatic int inet_csk_bind_conflict(const struct sock *sk,\n\t\t\t\t  const struct inet_bind_bucket *tb,\n\t\t\t\t  bool relax, bool reuseport_ok)\n{\n\tstruct sock *sk2;\n\tbool reuse = sk->sk_reuse;\n\tbool reuseport = !!sk->sk_reuseport && reuseport_ok;\n\tkuid_t uid = sock_i_uid((struct sock *)sk);\n\n\t/*\n\t * Unlike other sk lookup places we do not check\n\t * for sk_net here, since _all_ the socks listed\n\t * in tb->owners list belong to the same net - the\n\t * one this bucket belongs to.\n\t */\n\n\tsk_for_each_bound(sk2, &tb->owners) {\n\t\tif (sk != sk2 &&\n\t\t    (!sk->sk_bound_dev_if ||\n\t\t     !sk2->sk_bound_dev_if ||\n\t\t     sk->sk_bound_dev_if == sk2->sk_bound_dev_if)) {\n\t\t\tif ((!reuse || !sk2->sk_reuse ||\n\t\t\t    sk2->sk_state == TCP_LISTEN) &&\n\t\t\t    (!reuseport || !sk2->sk_reuseport ||\n\t\t\t     rcu_access_pointer(sk->sk_reuseport_cb) ||\n\t\t\t     (sk2->sk_state != TCP_TIME_WAIT &&\n\t\t\t     !uid_eq(uid, sock_i_uid(sk2))))) {\n\t\t\t\tif (inet_rcv_saddr_equal(sk, sk2, true))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!relax && reuse && sk2->sk_reuse &&\n\t\t\t    sk2->sk_state != TCP_LISTEN) {\n\t\t\t\tif (inet_rcv_saddr_equal(sk, sk2, true))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn sk2 != NULL;\n}\n\n/*\n * Find an open port number for the socket.  Returns with the\n * inet_bind_hashbucket lock held.\n */\nstatic struct inet_bind_hashbucket *\ninet_csk_find_open_port(struct sock *sk, struct inet_bind_bucket **tb_ret, int *port_ret)\n{\n\tstruct inet_hashinfo *hinfo = sk->sk_prot->h.hashinfo;\n\tint port = 0;\n\tstruct inet_bind_hashbucket *head;\n\tstruct net *net = sock_net(sk);\n\tint i, low, high, attempt_half;\n\tstruct inet_bind_bucket *tb;\n\tu32 remaining, offset;\n\n\tattempt_half = (sk->sk_reuse == SK_CAN_REUSE) ? 1 : 0;\nother_half_scan:\n\tinet_get_local_port_range(net, &low, &high);\n\thigh++; /* [32768, 60999] -> [32768, 61000[ */\n\tif (high - low < 4)\n\t\tattempt_half = 0;\n\tif (attempt_half) {\n\t\tint half = low + (((high - low) >> 2) << 1);\n\n\t\tif (attempt_half == 1)\n\t\t\thigh = half;\n\t\telse\n\t\t\tlow = half;\n\t}\n\tremaining = high - low;\n\tif (likely(remaining > 1))\n\t\tremaining &= ~1U;\n\n\toffset = prandom_u32() % remaining;\n\t/* __inet_hash_connect() favors ports having @low parity\n\t * We do the opposite to not pollute connect() users.\n\t */\n\toffset |= 1U;\n\nother_parity_scan:\n\tport = low + offset;\n\tfor (i = 0; i < remaining; i += 2, port += 2) {\n\t\tif (unlikely(port >= high))\n\t\t\tport -= remaining;\n\t\tif (inet_is_local_reserved_port(net, port))\n\t\t\tcontinue;\n\t\thead = &hinfo->bhash[inet_bhashfn(net, port,\n\t\t\t\t\t\t  hinfo->bhash_size)];\n\t\tspin_lock_bh(&head->lock);\n\t\tinet_bind_bucket_for_each(tb, &head->chain)\n\t\t\tif (net_eq(ib_net(tb), net) && tb->port == port) {\n\t\t\t\tif (!inet_csk_bind_conflict(sk, tb, false, false))\n\t\t\t\t\tgoto success;\n\t\t\t\tgoto next_port;\n\t\t\t}\n\t\ttb = NULL;\n\t\tgoto success;\nnext_port:\n\t\tspin_unlock_bh(&head->lock);\n\t\tcond_resched();\n\t}\n\n\toffset--;\n\tif (!(offset & 1))\n\t\tgoto other_parity_scan;\n\n\tif (attempt_half == 1) {\n\t\t/* OK we now try the upper half of the range */\n\t\tattempt_half = 2;\n\t\tgoto other_half_scan;\n\t}\n\treturn NULL;\nsuccess:\n\t*port_ret = port;\n\t*tb_ret = tb;\n\treturn head;\n}\n\nstatic inline int sk_reuseport_match(struct inet_bind_bucket *tb,\n\t\t\t\t     struct sock *sk)\n{\n\tkuid_t uid = sock_i_uid(sk);\n\n\tif (tb->fastreuseport <= 0)\n\t\treturn 0;\n\tif (!sk->sk_reuseport)\n\t\treturn 0;\n\tif (rcu_access_pointer(sk->sk_reuseport_cb))\n\t\treturn 0;\n\tif (!uid_eq(tb->fastuid, uid))\n\t\treturn 0;\n\t/* We only need to check the rcv_saddr if this tb was once marked\n\t * without fastreuseport and then was reset, as we can only know that\n\t * the fast_*rcv_saddr doesn't have any conflicts with the socks on the\n\t * owners list.\n\t */\n\tif (tb->fastreuseport == FASTREUSEPORT_ANY)\n\t\treturn 1;\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (tb->fast_sk_family == AF_INET6)\n\t\treturn ipv6_rcv_saddr_equal(&tb->fast_v6_rcv_saddr,\n\t\t\t\t\t    &sk->sk_v6_rcv_saddr,\n\t\t\t\t\t    tb->fast_rcv_saddr,\n\t\t\t\t\t    sk->sk_rcv_saddr,\n\t\t\t\t\t    tb->fast_ipv6_only,\n\t\t\t\t\t    ipv6_only_sock(sk), true);\n#endif\n\treturn ipv4_rcv_saddr_equal(tb->fast_rcv_saddr, sk->sk_rcv_saddr,\n\t\t\t\t    ipv6_only_sock(sk), true);\n}\n\n/* Obtain a reference to a local port for the given sock,\n * if snum is zero it means select any available local port.\n * We try to allocate an odd port (and leave even ports for connect())\n */\nint inet_csk_get_port(struct sock *sk, unsigned short snum)\n{\n\tbool reuse = sk->sk_reuse && sk->sk_state != TCP_LISTEN;\n\tstruct inet_hashinfo *hinfo = sk->sk_prot->h.hashinfo;\n\tint ret = 1, port = snum;\n\tstruct inet_bind_hashbucket *head;\n\tstruct net *net = sock_net(sk);\n\tstruct inet_bind_bucket *tb = NULL;\n\tkuid_t uid = sock_i_uid(sk);\n\n\tif (!port) {\n\t\thead = inet_csk_find_open_port(sk, &tb, &port);\n\t\tif (!head)\n\t\t\treturn ret;\n\t\tif (!tb)\n\t\t\tgoto tb_not_found;\n\t\tgoto success;\n\t}\n\thead = &hinfo->bhash[inet_bhashfn(net, port,\n\t\t\t\t\t  hinfo->bhash_size)];\n\tspin_lock_bh(&head->lock);\n\tinet_bind_bucket_for_each(tb, &head->chain)\n\t\tif (net_eq(ib_net(tb), net) && tb->port == port)\n\t\t\tgoto tb_found;\ntb_not_found:\n\ttb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,\n\t\t\t\t     net, head, port);\n\tif (!tb)\n\t\tgoto fail_unlock;\ntb_found:\n\tif (!hlist_empty(&tb->owners)) {\n\t\tif (sk->sk_reuse == SK_FORCE_REUSE)\n\t\t\tgoto success;\n\n\t\tif ((tb->fastreuse > 0 && reuse) ||\n\t\t    sk_reuseport_match(tb, sk))\n\t\t\tgoto success;\n\t\tif (inet_csk_bind_conflict(sk, tb, true, true))\n\t\t\tgoto fail_unlock;\n\t}\nsuccess:\n\tif (!hlist_empty(&tb->owners)) {\n\t\ttb->fastreuse = reuse;\n\t\tif (sk->sk_reuseport) {\n\t\t\ttb->fastreuseport = FASTREUSEPORT_ANY;\n\t\t\ttb->fastuid = uid;\n\t\t\ttb->fast_rcv_saddr = sk->sk_rcv_saddr;\n\t\t\ttb->fast_ipv6_only = ipv6_only_sock(sk);\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\ttb->fast_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n#endif\n\t\t} else {\n\t\t\ttb->fastreuseport = 0;\n\t\t}\n\t} else {\n\t\tif (!reuse)\n\t\t\ttb->fastreuse = 0;\n\t\tif (sk->sk_reuseport) {\n\t\t\t/* We didn't match or we don't have fastreuseport set on\n\t\t\t * the tb, but we have sk_reuseport set on this socket\n\t\t\t * and we know that there are no bind conflicts with\n\t\t\t * this socket in this tb, so reset our tb's reuseport\n\t\t\t * settings so that any subsequent sockets that match\n\t\t\t * our current socket will be put on the fast path.\n\t\t\t *\n\t\t\t * If we reset we need to set FASTREUSEPORT_STRICT so we\n\t\t\t * do extra checking for all subsequent sk_reuseport\n\t\t\t * socks.\n\t\t\t */\n\t\t\tif (!sk_reuseport_match(tb, sk)) {\n\t\t\t\ttb->fastreuseport = FASTREUSEPORT_STRICT;\n\t\t\t\ttb->fastuid = uid;\n\t\t\t\ttb->fast_rcv_saddr = sk->sk_rcv_saddr;\n\t\t\t\ttb->fast_ipv6_only = ipv6_only_sock(sk);\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\t\ttb->fast_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n#endif\n\t\t\t}\n\t\t} else {\n\t\t\ttb->fastreuseport = 0;\n\t\t}\n\t}\n\tif (!inet_csk(sk)->icsk_bind_hash)\n\t\tinet_bind_hash(sk, tb, port);\n\tWARN_ON(inet_csk(sk)->icsk_bind_hash != tb);\n\tret = 0;\n\nfail_unlock:\n\tspin_unlock_bh(&head->lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(inet_csk_get_port);\n\n/*\n * Wait for an incoming connection, avoid race conditions. This must be called\n * with the socket locked.\n */\nstatic int inet_csk_wait_for_connect(struct sock *sk, long timeo)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tDEFINE_WAIT(wait);\n\tint err;\n\n\t/*\n\t * True wake-one mechanism for incoming connections: only\n\t * one process gets woken up, not the 'whole herd'.\n\t * Since we do not 'race & poll' for established sockets\n\t * anymore, the common case will execute the loop only once.\n\t *\n\t * Subtle issue: \"add_wait_queue_exclusive()\" will be added\n\t * after any current non-exclusive waiters, and we know that\n\t * it will always _stay_ after any new non-exclusive waiters\n\t * because all non-exclusive waiters are added at the\n\t * beginning of the wait-queue. As such, it's ok to \"drop\"\n\t * our exclusiveness temporarily when we get woken up without\n\t * having to remove and re-insert us on the wait queue.\n\t */\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(sk_sleep(sk), &wait,\n\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\trelease_sock(sk);\n\t\tif (reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\ttimeo = schedule_timeout(timeo);\n\t\tsched_annotate_sleep();\n\t\tlock_sock(sk);\n\t\terr = 0;\n\t\tif (!reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\tbreak;\n\t\terr = -EINVAL;\n\t\tif (sk->sk_state != TCP_LISTEN)\n\t\t\tbreak;\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tbreak;\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn err;\n}\n\n/*\n * This will accept the next outstanding connection.\n */\nstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err, bool kern)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tstruct request_sock *req;\n\tstruct sock *newsk;\n\tint error;\n\n\tlock_sock(sk);\n\n\t/* We need to make sure that this socket is listening,\n\t * and that it has something pending.\n\t */\n\terror = -EINVAL;\n\tif (sk->sk_state != TCP_LISTEN)\n\t\tgoto out_err;\n\n\t/* Find already established connection */\n\tif (reqsk_queue_empty(queue)) {\n\t\tlong timeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\n\n\t\t/* If this is a non blocking socket don't sleep */\n\t\terror = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_err;\n\n\t\terror = inet_csk_wait_for_connect(sk, timeo);\n\t\tif (error)\n\t\t\tgoto out_err;\n\t}\n\treq = reqsk_queue_remove(queue, sk);\n\tnewsk = req->sk;\n\n\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t    tcp_rsk(req)->tfo_listener) {\n\t\tspin_lock_bh(&queue->fastopenq.lock);\n\t\tif (tcp_rsk(req)->tfo_listener) {\n\t\t\t/* We are still waiting for the final ACK from 3WHS\n\t\t\t * so can't free req now. Instead, we set req->sk to\n\t\t\t * NULL to signify that the child socket is taken\n\t\t\t * so reqsk_fastopen_remove() will free the req\n\t\t\t * when 3WHS finishes (or is aborted).\n\t\t\t */\n\t\t\treq->sk = NULL;\n\t\t\treq = NULL;\n\t\t}\n\t\tspin_unlock_bh(&queue->fastopenq.lock);\n\t}\nout:\n\trelease_sock(sk);\n\tif (req)\n\t\treqsk_put(req);\n\treturn newsk;\nout_err:\n\tnewsk = NULL;\n\treq = NULL;\n\t*err = error;\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_csk_accept);\n\n/*\n * Using different timers for retransmit, delayed acks and probes\n * We may wish use just one timer maintaining a list of expire jiffies\n * to optimize.\n */\nvoid inet_csk_init_xmit_timers(struct sock *sk,\n\t\t\t       void (*retransmit_handler)(unsigned long),\n\t\t\t       void (*delack_handler)(unsigned long),\n\t\t\t       void (*keepalive_handler)(unsigned long))\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tsetup_timer(&icsk->icsk_retransmit_timer, retransmit_handler,\n\t\t\t(unsigned long)sk);\n\tsetup_timer(&icsk->icsk_delack_timer, delack_handler,\n\t\t\t(unsigned long)sk);\n\tsetup_timer(&sk->sk_timer, keepalive_handler, (unsigned long)sk);\n\ticsk->icsk_pending = icsk->icsk_ack.pending = 0;\n}\nEXPORT_SYMBOL(inet_csk_init_xmit_timers);\n\nvoid inet_csk_clear_xmit_timers(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_pending = icsk->icsk_ack.pending = icsk->icsk_ack.blocked = 0;\n\n\tsk_stop_timer(sk, &icsk->icsk_retransmit_timer);\n\tsk_stop_timer(sk, &icsk->icsk_delack_timer);\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_clear_xmit_timers);\n\nvoid inet_csk_delete_keepalive_timer(struct sock *sk)\n{\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_delete_keepalive_timer);\n\nvoid inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long len)\n{\n\tsk_reset_timer(sk, &sk->sk_timer, jiffies + len);\n}\nEXPORT_SYMBOL(inet_csk_reset_keepalive_timer);\n\nstruct dst_entry *inet_csk_route_req(const struct sock *sk,\n\t\t\t\t     struct flowi4 *fl4,\n\t\t\t\t     const struct request_sock *req)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct net *net = read_pnet(&ireq->ireq_net);\n\tstruct ip_options_rcu *opt = ireq->opt;\n\tstruct rtable *rt;\n\n\tflowi4_init_output(fl4, ireq->ir_iif, ireq->ir_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\n\t\t\t   ireq->ir_loc_addr, ireq->ir_rmt_port,\n\t\t\t   htons(ireq->ir_num), sk->sk_uid);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(fl4));\n\trt = ip_route_output_flow(net, fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\t__IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_route_req);\n\nstruct dst_entry *inet_csk_route_child_sock(const struct sock *sk,\n\t\t\t\t\t    struct sock *newsk,\n\t\t\t\t\t    const struct request_sock *req)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct net *net = read_pnet(&ireq->ireq_net);\n\tstruct inet_sock *newinet = inet_sk(newsk);\n\tstruct ip_options_rcu *opt;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\n\tfl4 = &newinet->cork.fl.u.ip4;\n\n\trcu_read_lock();\n\topt = rcu_dereference(newinet->inet_opt);\n\tflowi4_init_output(fl4, ireq->ir_iif, ireq->ir_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\n\t\t\t   ireq->ir_loc_addr, ireq->ir_rmt_port,\n\t\t\t   htons(ireq->ir_num), sk->sk_uid);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(fl4));\n\trt = ip_route_output_flow(net, fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\n\t\tgoto route_err;\n\trcu_read_unlock();\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\trcu_read_unlock();\n\t__IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_route_child_sock);\n\n#if IS_ENABLED(CONFIG_IPV6)\n#define AF_INET_FAMILY(fam) ((fam) == AF_INET)\n#else\n#define AF_INET_FAMILY(fam) true\n#endif\n\n/* Decide when to expire the request and when to resend SYN-ACK */\nstatic inline void syn_ack_recalc(struct request_sock *req, const int thresh,\n\t\t\t\t  const int max_retries,\n\t\t\t\t  const u8 rskq_defer_accept,\n\t\t\t\t  int *expire, int *resend)\n{\n\tif (!rskq_defer_accept) {\n\t\t*expire = req->num_timeout >= thresh;\n\t\t*resend = 1;\n\t\treturn;\n\t}\n\t*expire = req->num_timeout >= thresh &&\n\t\t  (!inet_rsk(req)->acked || req->num_timeout >= max_retries);\n\t/*\n\t * Do not resend while waiting for data after ACK,\n\t * start to resend on end of deferring period to give\n\t * last chance for data or ACK to create established socket.\n\t */\n\t*resend = !inet_rsk(req)->acked ||\n\t\t  req->num_timeout >= rskq_defer_accept - 1;\n}\n\nint inet_rtx_syn_ack(const struct sock *parent, struct request_sock *req)\n{\n\tint err = req->rsk_ops->rtx_syn_ack(parent, req);\n\n\tif (!err)\n\t\treq->num_retrans++;\n\treturn err;\n}\nEXPORT_SYMBOL(inet_rtx_syn_ack);\n\n/* return true if req was found in the ehash table */\nstatic bool reqsk_queue_unlink(struct request_sock_queue *queue,\n\t\t\t       struct request_sock *req)\n{\n\tstruct inet_hashinfo *hashinfo = req_to_sk(req)->sk_prot->h.hashinfo;\n\tbool found = false;\n\n\tif (sk_hashed(req_to_sk(req))) {\n\t\tspinlock_t *lock = inet_ehash_lockp(hashinfo, req->rsk_hash);\n\n\t\tspin_lock(lock);\n\t\tfound = __sk_nulls_del_node_init_rcu(req_to_sk(req));\n\t\tspin_unlock(lock);\n\t}\n\tif (timer_pending(&req->rsk_timer) && del_timer_sync(&req->rsk_timer))\n\t\treqsk_put(req);\n\treturn found;\n}\n\nvoid inet_csk_reqsk_queue_drop(struct sock *sk, struct request_sock *req)\n{\n\tif (reqsk_queue_unlink(&inet_csk(sk)->icsk_accept_queue, req)) {\n\t\treqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);\n\t\treqsk_put(req);\n\t}\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_drop);\n\nvoid inet_csk_reqsk_queue_drop_and_put(struct sock *sk, struct request_sock *req)\n{\n\tinet_csk_reqsk_queue_drop(sk, req);\n\treqsk_put(req);\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_drop_and_put);\n\nstatic void reqsk_timer_handler(unsigned long data)\n{\n\tstruct request_sock *req = (struct request_sock *)data;\n\tstruct sock *sk_listener = req->rsk_listener;\n\tstruct net *net = sock_net(sk_listener);\n\tstruct inet_connection_sock *icsk = inet_csk(sk_listener);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tint qlen, expire = 0, resend = 0;\n\tint max_retries, thresh;\n\tu8 defer_accept;\n\n\tif (sk_state_load(sk_listener) != TCP_LISTEN)\n\t\tgoto drop;\n\n\tmax_retries = icsk->icsk_syn_retries ? : net->ipv4.sysctl_tcp_synack_retries;\n\tthresh = max_retries;\n\t/* Normally all the openreqs are young and become mature\n\t * (i.e. converted to established socket) for first timeout.\n\t * If synack was not acknowledged for 1 second, it means\n\t * one of the following things: synack was lost, ack was lost,\n\t * rtt is high or nobody planned to ack (i.e. synflood).\n\t * When server is a bit loaded, queue is populated with old\n\t * open requests, reducing effective size of queue.\n\t * When server is well loaded, queue size reduces to zero\n\t * after several minutes of work. It is not synflood,\n\t * it is normal operation. The solution is pruning\n\t * too old entries overriding normal timeout, when\n\t * situation becomes dangerous.\n\t *\n\t * Essentially, we reserve half of room for young\n\t * embrions; and abort old ones without pity, if old\n\t * ones are about to clog our table.\n\t */\n\tqlen = reqsk_queue_len(queue);\n\tif ((qlen << 1) > max(8U, sk_listener->sk_max_ack_backlog)) {\n\t\tint young = reqsk_queue_len_young(queue) << 1;\n\n\t\twhile (thresh > 2) {\n\t\t\tif (qlen < young)\n\t\t\t\tbreak;\n\t\t\tthresh--;\n\t\t\tyoung <<= 1;\n\t\t}\n\t}\n\tdefer_accept = READ_ONCE(queue->rskq_defer_accept);\n\tif (defer_accept)\n\t\tmax_retries = defer_accept;\n\tsyn_ack_recalc(req, thresh, max_retries, defer_accept,\n\t\t       &expire, &resend);\n\treq->rsk_ops->syn_ack_timeout(req);\n\tif (!expire &&\n\t    (!resend ||\n\t     !inet_rtx_syn_ack(sk_listener, req) ||\n\t     inet_rsk(req)->acked)) {\n\t\tunsigned long timeo;\n\n\t\tif (req->num_timeout++ == 0)\n\t\t\tatomic_dec(&queue->young);\n\t\ttimeo = min(TCP_TIMEOUT_INIT << req->num_timeout, TCP_RTO_MAX);\n\t\tmod_timer(&req->rsk_timer, jiffies + timeo);\n\t\treturn;\n\t}\ndrop:\n\tinet_csk_reqsk_queue_drop_and_put(sk_listener, req);\n}\n\nstatic void reqsk_queue_hash_req(struct request_sock *req,\n\t\t\t\t unsigned long timeout)\n{\n\treq->num_retrans = 0;\n\treq->num_timeout = 0;\n\treq->sk = NULL;\n\n\tsetup_pinned_timer(&req->rsk_timer, reqsk_timer_handler,\n\t\t\t    (unsigned long)req);\n\tmod_timer(&req->rsk_timer, jiffies + timeout);\n\n\tinet_ehash_insert(req_to_sk(req), NULL);\n\t/* before letting lookups find us, make sure all req fields\n\t * are committed to memory and refcnt initialized.\n\t */\n\tsmp_wmb();\n\tatomic_set(&req->rsk_refcnt, 2 + 1);\n}\n\nvoid inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,\n\t\t\t\t   unsigned long timeout)\n{\n\treqsk_queue_hash_req(req, timeout);\n\tinet_csk_reqsk_queue_added(sk);\n}\nEXPORT_SYMBOL_GPL(inet_csk_reqsk_queue_hash_add);\n\n/**\n *\tinet_csk_clone_lock - clone an inet socket, and lock its clone\n *\t@sk: the socket to clone\n *\t@req: request_sock\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\n *\tCaller must unlock socket even in error path (bh_unlock_sock(newsk))\n */\nstruct sock *inet_csk_clone_lock(const struct sock *sk,\n\t\t\t\t const struct request_sock *req,\n\t\t\t\t const gfp_t priority)\n{\n\tstruct sock *newsk = sk_clone_lock(sk, priority);\n\n\tif (newsk) {\n\t\tstruct inet_connection_sock *newicsk = inet_csk(newsk);\n\n\t\tnewsk->sk_state = TCP_SYN_RECV;\n\t\tnewicsk->icsk_bind_hash = NULL;\n\n\t\tinet_sk(newsk)->inet_dport = inet_rsk(req)->ir_rmt_port;\n\t\tinet_sk(newsk)->inet_num = inet_rsk(req)->ir_num;\n\t\tinet_sk(newsk)->inet_sport = htons(inet_rsk(req)->ir_num);\n\t\tnewsk->sk_write_space = sk_stream_write_space;\n\n\t\t/* listeners have SOCK_RCU_FREE, not the children */\n\t\tsock_reset_flag(newsk, SOCK_RCU_FREE);\n\n\t\tinet_sk(newsk)->mc_list = NULL;\n\n\t\tnewsk->sk_mark = inet_rsk(req)->ir_mark;\n\t\tatomic64_set(&newsk->sk_cookie,\n\t\t\t     atomic64_read(&inet_rsk(req)->ir_cookie));\n\n\t\tnewicsk->icsk_retransmits = 0;\n\t\tnewicsk->icsk_backoff\t  = 0;\n\t\tnewicsk->icsk_probes_out  = 0;\n\n\t\t/* Deinitialize accept_queue to trap illegal accesses. */\n\t\tmemset(&newicsk->icsk_accept_queue, 0, sizeof(newicsk->icsk_accept_queue));\n\n\t\tsecurity_inet_csk_clone(newsk, req);\n\t}\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(inet_csk_clone_lock);\n\n/*\n * At this point, there should be no process reference to this\n * socket, and thus no user references at all.  Therefore we\n * can assume the socket waitqueue is inactive and nobody will\n * try to jump onto it.\n */\nvoid inet_csk_destroy_sock(struct sock *sk)\n{\n\tWARN_ON(sk->sk_state != TCP_CLOSE);\n\tWARN_ON(!sock_flag(sk, SOCK_DEAD));\n\n\t/* It cannot be in hash table! */\n\tWARN_ON(!sk_unhashed(sk));\n\n\t/* If it has not 0 inet_sk(sk)->inet_num, it must be bound */\n\tWARN_ON(inet_sk(sk)->inet_num && !inet_csk(sk)->icsk_bind_hash);\n\n\tsk->sk_prot->destroy(sk);\n\n\tsk_stream_kill_queues(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsk_refcnt_debug_release(sk);\n\n\tpercpu_counter_dec(sk->sk_prot->orphan_count);\n\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(inet_csk_destroy_sock);\n\n/* This function allows to force a closure of a socket after the call to\n * tcp/dccp_create_openreq_child().\n */\nvoid inet_csk_prepare_forced_close(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n{\n\t/* sk_clone_lock locked the socket and set refcnt to 2 */\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\n\t/* The below has to be done to allow calling inet_csk_destroy_sock */\n\tsock_set_flag(sk, SOCK_DEAD);\n\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\tinet_sk(sk)->inet_num = 0;\n}\nEXPORT_SYMBOL(inet_csk_prepare_forced_close);\n\nint inet_csk_listen_start(struct sock *sk, int backlog)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err = -EADDRINUSE;\n\n\treqsk_queue_alloc(&icsk->icsk_accept_queue);\n\n\tsk->sk_max_ack_backlog = backlog;\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\t/* There is race window here: we announce ourselves listening,\n\t * but this transition is still not validated by get_port().\n\t * It is OK, because this socket enters to hash table only\n\t * after validation is complete.\n\t */\n\tsk_state_store(sk, TCP_LISTEN);\n\tif (!sk->sk_prot->get_port(sk, inet->inet_num)) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\terr = sk->sk_prot->hash(sk);\n\n\t\tif (likely(!err))\n\t\t\treturn 0;\n\t}\n\n\tsk->sk_state = TCP_CLOSE;\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_start);\n\nstatic void inet_child_forget(struct sock *sk, struct request_sock *req,\n\t\t\t      struct sock *child)\n{\n\tsk->sk_prot->disconnect(child, O_NONBLOCK);\n\n\tsock_orphan(child);\n\n\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\n\tif (sk->sk_protocol == IPPROTO_TCP && tcp_rsk(req)->tfo_listener) {\n\t\tBUG_ON(tcp_sk(child)->fastopen_rsk != req);\n\t\tBUG_ON(sk != req->rsk_listener);\n\n\t\t/* Paranoid, to prevent race condition if\n\t\t * an inbound pkt destined for child is\n\t\t * blocked by sock lock in tcp_v4_rcv().\n\t\t * Also to satisfy an assertion in\n\t\t * tcp_v4_destroy_sock().\n\t\t */\n\t\ttcp_sk(child)->fastopen_rsk = NULL;\n\t}\n\tinet_csk_destroy_sock(child);\n\treqsk_put(req);\n}\n\nstruct sock *inet_csk_reqsk_queue_add(struct sock *sk,\n\t\t\t\t      struct request_sock *req,\n\t\t\t\t      struct sock *child)\n{\n\tstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\n\n\tspin_lock(&queue->rskq_lock);\n\tif (unlikely(sk->sk_state != TCP_LISTEN)) {\n\t\tinet_child_forget(sk, req, child);\n\t\tchild = NULL;\n\t} else {\n\t\treq->sk = child;\n\t\treq->dl_next = NULL;\n\t\tif (queue->rskq_accept_head == NULL)\n\t\t\tqueue->rskq_accept_head = req;\n\t\telse\n\t\t\tqueue->rskq_accept_tail->dl_next = req;\n\t\tqueue->rskq_accept_tail = req;\n\t\tsk_acceptq_added(sk);\n\t}\n\tspin_unlock(&queue->rskq_lock);\n\treturn child;\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_add);\n\nstruct sock *inet_csk_complete_hashdance(struct sock *sk, struct sock *child,\n\t\t\t\t\t struct request_sock *req, bool own_req)\n{\n\tif (own_req) {\n\t\tinet_csk_reqsk_queue_drop(sk, req);\n\t\treqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);\n\t\tif (inet_csk_reqsk_queue_add(sk, req, child))\n\t\t\treturn child;\n\t}\n\t/* Too bad, another child took ownership of the request, undo. */\n\tbh_unlock_sock(child);\n\tsock_put(child);\n\treturn NULL;\n}\nEXPORT_SYMBOL(inet_csk_complete_hashdance);\n\n/*\n *\tThis routine closes sockets which have been at least partially\n *\topened, but not yet accepted.\n */\nvoid inet_csk_listen_stop(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tstruct request_sock *next, *req;\n\n\t/* Following specs, it would be better either to send FIN\n\t * (and enter FIN-WAIT-1, it is normal close)\n\t * or to send active reset (abort).\n\t * Certainly, it is pretty dangerous while synflood, but it is\n\t * bad justification for our negligence 8)\n\t * To be honest, we are not able to make either\n\t * of the variants now.\t\t\t--ANK\n\t */\n\twhile ((req = reqsk_queue_remove(queue, sk)) != NULL) {\n\t\tstruct sock *child = req->sk;\n\n\t\tlocal_bh_disable();\n\t\tbh_lock_sock(child);\n\t\tWARN_ON(sock_owned_by_user(child));\n\t\tsock_hold(child);\n\n\t\tinet_child_forget(sk, req, child);\n\t\tbh_unlock_sock(child);\n\t\tlocal_bh_enable();\n\t\tsock_put(child);\n\n\t\tcond_resched();\n\t}\n\tif (queue->fastopenq.rskq_rst_head) {\n\t\t/* Free all the reqs queued in rskq_rst_head. */\n\t\tspin_lock_bh(&queue->fastopenq.lock);\n\t\treq = queue->fastopenq.rskq_rst_head;\n\t\tqueue->fastopenq.rskq_rst_head = NULL;\n\t\tspin_unlock_bh(&queue->fastopenq.lock);\n\t\twhile (req != NULL) {\n\t\t\tnext = req->dl_next;\n\t\t\treqsk_put(req);\n\t\t\treq = next;\n\t\t}\n\t}\n\tWARN_ON_ONCE(sk->sk_ack_backlog);\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_stop);\n\nvoid inet_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\n{\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)uaddr;\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\tsin->sin_family\t\t= AF_INET;\n\tsin->sin_addr.s_addr\t= inet->inet_daddr;\n\tsin->sin_port\t\t= inet->inet_dport;\n}\nEXPORT_SYMBOL_GPL(inet_csk_addr2sockaddr);\n\n#ifdef CONFIG_COMPAT\nint inet_csk_compat_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_af_ops->compat_getsockopt)\n\t\treturn icsk->icsk_af_ops->compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t\t    optval, optlen);\n\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL_GPL(inet_csk_compat_getsockopt);\n\nint inet_csk_compat_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_af_ops->compat_setsockopt)\n\t\treturn icsk->icsk_af_ops->compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t    optval, optlen);\n\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL_GPL(inet_csk_compat_setsockopt);\n#endif\n\nstatic struct dst_entry *inet_csk_rebuild_route(struct sock *sk, struct flowi *fl)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\tfl4 = &fl->u.ip4;\n\trt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\tif (IS_ERR(rt))\n\t\trt = NULL;\n\tif (rt)\n\t\tsk_setup_caps(sk, &rt->dst);\n\trcu_read_unlock();\n\n\treturn &rt->dst;\n}\n\nstruct dst_entry *inet_csk_update_pmtu(struct sock *sk, u32 mtu)\n{\n\tstruct dst_entry *dst = __sk_dst_check(sk, 0);\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (!dst) {\n\t\tdst = inet_csk_rebuild_route(sk, &inet->cork.fl);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\tdst->ops->update_pmtu(dst, sk, NULL, mtu);\n\n\tdst = __sk_dst_check(sk, 0);\n\tif (!dst)\n\t\tdst = inet_csk_rebuild_route(sk, &inet->cork.fl);\nout:\n\treturn dst;\n}\nEXPORT_SYMBOL_GPL(inet_csk_update_pmtu);\n"], "filenames": ["net/ipv4/inet_connection_sock.c"], "buggy_code_start_loc": [796], "buggy_code_end_loc": [796], "fixing_code_start_loc": [797], "fixing_code_end_loc": [799], "type": "CWE-415", "message": "The inet_csk_clone_lock function in net/ipv4/inet_connection_sock.c in the Linux kernel through 4.10.15 allows attackers to cause a denial of service (double free) or possibly have unspecified other impact by leveraging use of the accept system call.", "other": {"cve": {"id": "CVE-2017-8890", "sourceIdentifier": "cve@mitre.org", "published": "2017-05-10T16:29:00.197", "lastModified": "2023-02-24T18:32:39.967", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The inet_csk_clone_lock function in net/ipv4/inet_connection_sock.c in the Linux kernel through 4.10.15 allows attackers to cause a denial of service (double free) or possibly have unspecified other impact by leveraging use of the accept system call."}, {"lang": "es", "value": "La funci\u00f3n inet_csk_clone_lock en net / ipv4 / inet_connection_sock.c en el kernel de Linux hasta la versi\u00f3n 4.10.15 permite a los atacantes causar una denegaci\u00f3n de servicio (double free) u otro impacto no especificado al aprovechar el uso de la llamada al sistema accept."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-415"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.2.89", "matchCriteriaId": "9A5C1F01-214B-4477-A3A1-F6DF10181D3C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.10.106", "matchCriteriaId": "3116EF11-56E7-4D40-9FD0-6109280D0247"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.16.44", "matchCriteriaId": "38DDEB69-B6EE-41DF-9DE5-F91F21C8592F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.56", "matchCriteriaId": "5837A5CA-266C-4BC5-B95F-1344AE42D24F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.42", "matchCriteriaId": "5EDD29F4-10F1-415A-877F-5586A004E320"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.71", "matchCriteriaId": "CF0C6096-CC39-4352-AFAF-5D7A7C5C2838"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.31", "matchCriteriaId": "001F55C3-810A-444F-AE18-F067A84F6B31"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.11.4", "matchCriteriaId": "1A25FD29-5617-4236-AC9A-6D68DC220925"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=657831ffc38e30092a2d5f03d385d710eb88b09a", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.debian.org/security/2017/dsa-3886", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/98562", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:1842", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2077", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2669", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:1854", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/657831ffc38e30092a2d5f03d385d710eb88b09a", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://source.android.com/security/bulletin/2017-09-01", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/657831ffc38e30092a2d5f03d385d710eb88b09a"}}