{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.\n * All Rights Reserved.\n */\n#include \"xfs.h\"\n#include \"xfs_fs.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_shared.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_bit.h\"\n#include \"xfs_sb.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_defer.h\"\n#include \"xfs_btree.h\"\n#include \"xfs_rmap.h\"\n#include \"xfs_alloc_btree.h\"\n#include \"xfs_alloc.h\"\n#include \"xfs_extent_busy.h\"\n#include \"xfs_errortag.h\"\n#include \"xfs_error.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_buf_item.h\"\n#include \"xfs_log.h\"\n#include \"xfs_ag_resv.h\"\n#include \"xfs_bmap.h\"\n\nextern kmem_zone_t\t*xfs_bmap_free_item_zone;\n\nstruct workqueue_struct *xfs_alloc_wq;\n\n#define XFS_ABSDIFF(a,b)\t(((a) <= (b)) ? ((b) - (a)) : ((a) - (b)))\n\n#define\tXFSA_FIXUP_BNO_OK\t1\n#define\tXFSA_FIXUP_CNT_OK\t2\n\nSTATIC int xfs_alloc_ag_vextent_exact(xfs_alloc_arg_t *);\nSTATIC int xfs_alloc_ag_vextent_near(xfs_alloc_arg_t *);\nSTATIC int xfs_alloc_ag_vextent_size(xfs_alloc_arg_t *);\n\n/*\n * Size of the AGFL.  For CRC-enabled filesystes we steal a couple of slots in\n * the beginning of the block for a proper header with the location information\n * and CRC.\n */\nunsigned int\nxfs_agfl_size(\n\tstruct xfs_mount\t*mp)\n{\n\tunsigned int\t\tsize = mp->m_sb.sb_sectsize;\n\n\tif (xfs_sb_version_hascrc(&mp->m_sb))\n\t\tsize -= sizeof(struct xfs_agfl);\n\n\treturn size / sizeof(xfs_agblock_t);\n}\n\nunsigned int\nxfs_refc_block(\n\tstruct xfs_mount\t*mp)\n{\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb))\n\t\treturn XFS_RMAP_BLOCK(mp) + 1;\n\tif (xfs_sb_version_hasfinobt(&mp->m_sb))\n\t\treturn XFS_FIBT_BLOCK(mp) + 1;\n\treturn XFS_IBT_BLOCK(mp) + 1;\n}\n\nxfs_extlen_t\nxfs_prealloc_blocks(\n\tstruct xfs_mount\t*mp)\n{\n\tif (xfs_sb_version_hasreflink(&mp->m_sb))\n\t\treturn xfs_refc_block(mp) + 1;\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb))\n\t\treturn XFS_RMAP_BLOCK(mp) + 1;\n\tif (xfs_sb_version_hasfinobt(&mp->m_sb))\n\t\treturn XFS_FIBT_BLOCK(mp) + 1;\n\treturn XFS_IBT_BLOCK(mp) + 1;\n}\n\n/*\n * In order to avoid ENOSPC-related deadlock caused by out-of-order locking of\n * AGF buffer (PV 947395), we place constraints on the relationship among\n * actual allocations for data blocks, freelist blocks, and potential file data\n * bmap btree blocks. However, these restrictions may result in no actual space\n * allocated for a delayed extent, for example, a data block in a certain AG is\n * allocated but there is no additional block for the additional bmap btree\n * block due to a split of the bmap btree of the file. The result of this may\n * lead to an infinite loop when the file gets flushed to disk and all delayed\n * extents need to be actually allocated. To get around this, we explicitly set\n * aside a few blocks which will not be reserved in delayed allocation.\n *\n * We need to reserve 4 fsbs _per AG_ for the freelist and 4 more to handle a\n * potential split of the file's bmap btree.\n */\nunsigned int\nxfs_alloc_set_aside(\n\tstruct xfs_mount\t*mp)\n{\n\treturn mp->m_sb.sb_agcount * (XFS_ALLOC_AGFL_RESERVE + 4);\n}\n\n/*\n * When deciding how much space to allocate out of an AG, we limit the\n * allocation maximum size to the size the AG. However, we cannot use all the\n * blocks in the AG - some are permanently used by metadata. These\n * blocks are generally:\n *\t- the AG superblock, AGF, AGI and AGFL\n *\t- the AGF (bno and cnt) and AGI btree root blocks, and optionally\n *\t  the AGI free inode and rmap btree root blocks.\n *\t- blocks on the AGFL according to xfs_alloc_set_aside() limits\n *\t- the rmapbt root block\n *\n * The AG headers are sector sized, so the amount of space they take up is\n * dependent on filesystem geometry. The others are all single blocks.\n */\nunsigned int\nxfs_alloc_ag_max_usable(\n\tstruct xfs_mount\t*mp)\n{\n\tunsigned int\t\tblocks;\n\n\tblocks = XFS_BB_TO_FSB(mp, XFS_FSS_TO_BB(mp, 4)); /* ag headers */\n\tblocks += XFS_ALLOC_AGFL_RESERVE;\n\tblocks += 3;\t\t\t/* AGF, AGI btree root blocks */\n\tif (xfs_sb_version_hasfinobt(&mp->m_sb))\n\t\tblocks++;\t\t/* finobt root block */\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb))\n\t\tblocks++; \t\t/* rmap root block */\n\tif (xfs_sb_version_hasreflink(&mp->m_sb))\n\t\tblocks++;\t\t/* refcount root block */\n\n\treturn mp->m_sb.sb_agblocks - blocks;\n}\n\n/*\n * Lookup the record equal to [bno, len] in the btree given by cur.\n */\nSTATIC int\t\t\t\t/* error */\nxfs_alloc_lookup_eq(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\tbno,\t/* starting block of extent */\n\txfs_extlen_t\t\tlen,\t/* length of extent */\n\tint\t\t\t*stat)\t/* success/failure */\n{\n\tint\t\t\terror;\n\n\tcur->bc_rec.a.ar_startblock = bno;\n\tcur->bc_rec.a.ar_blockcount = len;\n\terror = xfs_btree_lookup(cur, XFS_LOOKUP_EQ, stat);\n\tcur->bc_private.a.priv.abt.active = (*stat == 1);\n\treturn error;\n}\n\n/*\n * Lookup the first record greater than or equal to [bno, len]\n * in the btree given by cur.\n */\nint\t\t\t\t/* error */\nxfs_alloc_lookup_ge(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\tbno,\t/* starting block of extent */\n\txfs_extlen_t\t\tlen,\t/* length of extent */\n\tint\t\t\t*stat)\t/* success/failure */\n{\n\tint\t\t\terror;\n\n\tcur->bc_rec.a.ar_startblock = bno;\n\tcur->bc_rec.a.ar_blockcount = len;\n\terror = xfs_btree_lookup(cur, XFS_LOOKUP_GE, stat);\n\tcur->bc_private.a.priv.abt.active = (*stat == 1);\n\treturn error;\n}\n\n/*\n * Lookup the first record less than or equal to [bno, len]\n * in the btree given by cur.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_lookup_le(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\tbno,\t/* starting block of extent */\n\txfs_extlen_t\t\tlen,\t/* length of extent */\n\tint\t\t\t*stat)\t/* success/failure */\n{\n\tint\t\t\terror;\n\tcur->bc_rec.a.ar_startblock = bno;\n\tcur->bc_rec.a.ar_blockcount = len;\n\terror = xfs_btree_lookup(cur, XFS_LOOKUP_LE, stat);\n\tcur->bc_private.a.priv.abt.active = (*stat == 1);\n\treturn error;\n}\n\nstatic inline bool\nxfs_alloc_cur_active(\n\tstruct xfs_btree_cur\t*cur)\n{\n\treturn cur && cur->bc_private.a.priv.abt.active;\n}\n\n/*\n * Update the record referred to by cur to the value given\n * by [bno, len].\n * This either works (return 0) or gets an EFSCORRUPTED error.\n */\nSTATIC int\t\t\t\t/* error */\nxfs_alloc_update(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\tbno,\t/* starting block of extent */\n\txfs_extlen_t\t\tlen)\t/* length of extent */\n{\n\tunion xfs_btree_rec\trec;\n\n\trec.alloc.ar_startblock = cpu_to_be32(bno);\n\trec.alloc.ar_blockcount = cpu_to_be32(len);\n\treturn xfs_btree_update(cur, &rec);\n}\n\n/*\n * Get the data from the pointed-to record.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_get_rec(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\t*bno,\t/* output: starting block of extent */\n\txfs_extlen_t\t\t*len,\t/* output: length of extent */\n\tint\t\t\t*stat)\t/* output: success/failure */\n{\n\tstruct xfs_mount\t*mp = cur->bc_mp;\n\txfs_agnumber_t\t\tagno = cur->bc_private.a.agno;\n\tunion xfs_btree_rec\t*rec;\n\tint\t\t\terror;\n\n\terror = xfs_btree_get_rec(cur, &rec, stat);\n\tif (error || !(*stat))\n\t\treturn error;\n\n\t*bno = be32_to_cpu(rec->alloc.ar_startblock);\n\t*len = be32_to_cpu(rec->alloc.ar_blockcount);\n\n\tif (*len == 0)\n\t\tgoto out_bad_rec;\n\n\t/* check for valid extent range, including overflow */\n\tif (!xfs_verify_agbno(mp, agno, *bno))\n\t\tgoto out_bad_rec;\n\tif (*bno > *bno + *len)\n\t\tgoto out_bad_rec;\n\tif (!xfs_verify_agbno(mp, agno, *bno + *len - 1))\n\t\tgoto out_bad_rec;\n\n\treturn 0;\n\nout_bad_rec:\n\txfs_warn(mp,\n\t\t\"%s Freespace BTree record corruption in AG %d detected!\",\n\t\tcur->bc_btnum == XFS_BTNUM_BNO ? \"Block\" : \"Size\", agno);\n\txfs_warn(mp,\n\t\t\"start block 0x%x block count 0x%x\", *bno, *len);\n\treturn -EFSCORRUPTED;\n}\n\n/*\n * Compute aligned version of the found extent.\n * Takes alignment and min length into account.\n */\nSTATIC bool\nxfs_alloc_compute_aligned(\n\txfs_alloc_arg_t\t*args,\t\t/* allocation argument structure */\n\txfs_agblock_t\tfoundbno,\t/* starting block in found extent */\n\txfs_extlen_t\tfoundlen,\t/* length in found extent */\n\txfs_agblock_t\t*resbno,\t/* result block number */\n\txfs_extlen_t\t*reslen,\t/* result length */\n\tunsigned\t*busy_gen)\n{\n\txfs_agblock_t\tbno = foundbno;\n\txfs_extlen_t\tlen = foundlen;\n\txfs_extlen_t\tdiff;\n\tbool\t\tbusy;\n\n\t/* Trim busy sections out of found extent */\n\tbusy = xfs_extent_busy_trim(args, &bno, &len, busy_gen);\n\n\t/*\n\t * If we have a largish extent that happens to start before min_agbno,\n\t * see if we can shift it into range...\n\t */\n\tif (bno < args->min_agbno && bno + len > args->min_agbno) {\n\t\tdiff = args->min_agbno - bno;\n\t\tif (len > diff) {\n\t\t\tbno += diff;\n\t\t\tlen -= diff;\n\t\t}\n\t}\n\n\tif (args->alignment > 1 && len >= args->minlen) {\n\t\txfs_agblock_t\taligned_bno = roundup(bno, args->alignment);\n\n\t\tdiff = aligned_bno - bno;\n\n\t\t*resbno = aligned_bno;\n\t\t*reslen = diff >= len ? 0 : len - diff;\n\t} else {\n\t\t*resbno = bno;\n\t\t*reslen = len;\n\t}\n\n\treturn busy;\n}\n\n/*\n * Compute best start block and diff for \"near\" allocations.\n * freelen >= wantlen already checked by caller.\n */\nSTATIC xfs_extlen_t\t\t\t/* difference value (absolute) */\nxfs_alloc_compute_diff(\n\txfs_agblock_t\twantbno,\t/* target starting block */\n\txfs_extlen_t\twantlen,\t/* target length */\n\txfs_extlen_t\talignment,\t/* target alignment */\n\tint\t\tdatatype,\t/* are we allocating data? */\n\txfs_agblock_t\tfreebno,\t/* freespace's starting block */\n\txfs_extlen_t\tfreelen,\t/* freespace's length */\n\txfs_agblock_t\t*newbnop)\t/* result: best start block from free */\n{\n\txfs_agblock_t\tfreeend;\t/* end of freespace extent */\n\txfs_agblock_t\tnewbno1;\t/* return block number */\n\txfs_agblock_t\tnewbno2;\t/* other new block number */\n\txfs_extlen_t\tnewlen1=0;\t/* length with newbno1 */\n\txfs_extlen_t\tnewlen2=0;\t/* length with newbno2 */\n\txfs_agblock_t\twantend;\t/* end of target extent */\n\tbool\t\tuserdata = datatype & XFS_ALLOC_USERDATA;\n\n\tASSERT(freelen >= wantlen);\n\tfreeend = freebno + freelen;\n\twantend = wantbno + wantlen;\n\t/*\n\t * We want to allocate from the start of a free extent if it is past\n\t * the desired block or if we are allocating user data and the free\n\t * extent is before desired block. The second case is there to allow\n\t * for contiguous allocation from the remaining free space if the file\n\t * grows in the short term.\n\t */\n\tif (freebno >= wantbno || (userdata && freeend < wantend)) {\n\t\tif ((newbno1 = roundup(freebno, alignment)) >= freeend)\n\t\t\tnewbno1 = NULLAGBLOCK;\n\t} else if (freeend >= wantend && alignment > 1) {\n\t\tnewbno1 = roundup(wantbno, alignment);\n\t\tnewbno2 = newbno1 - alignment;\n\t\tif (newbno1 >= freeend)\n\t\t\tnewbno1 = NULLAGBLOCK;\n\t\telse\n\t\t\tnewlen1 = XFS_EXTLEN_MIN(wantlen, freeend - newbno1);\n\t\tif (newbno2 < freebno)\n\t\t\tnewbno2 = NULLAGBLOCK;\n\t\telse\n\t\t\tnewlen2 = XFS_EXTLEN_MIN(wantlen, freeend - newbno2);\n\t\tif (newbno1 != NULLAGBLOCK && newbno2 != NULLAGBLOCK) {\n\t\t\tif (newlen1 < newlen2 ||\n\t\t\t    (newlen1 == newlen2 &&\n\t\t\t     XFS_ABSDIFF(newbno1, wantbno) >\n\t\t\t     XFS_ABSDIFF(newbno2, wantbno)))\n\t\t\t\tnewbno1 = newbno2;\n\t\t} else if (newbno2 != NULLAGBLOCK)\n\t\t\tnewbno1 = newbno2;\n\t} else if (freeend >= wantend) {\n\t\tnewbno1 = wantbno;\n\t} else if (alignment > 1) {\n\t\tnewbno1 = roundup(freeend - wantlen, alignment);\n\t\tif (newbno1 > freeend - wantlen &&\n\t\t    newbno1 - alignment >= freebno)\n\t\t\tnewbno1 -= alignment;\n\t\telse if (newbno1 >= freeend)\n\t\t\tnewbno1 = NULLAGBLOCK;\n\t} else\n\t\tnewbno1 = freeend - wantlen;\n\t*newbnop = newbno1;\n\treturn newbno1 == NULLAGBLOCK ? 0 : XFS_ABSDIFF(newbno1, wantbno);\n}\n\n/*\n * Fix up the length, based on mod and prod.\n * len should be k * prod + mod for some k.\n * If len is too small it is returned unchanged.\n * If len hits maxlen it is left alone.\n */\nSTATIC void\nxfs_alloc_fix_len(\n\txfs_alloc_arg_t\t*args)\t\t/* allocation argument structure */\n{\n\txfs_extlen_t\tk;\n\txfs_extlen_t\trlen;\n\n\tASSERT(args->mod < args->prod);\n\trlen = args->len;\n\tASSERT(rlen >= args->minlen);\n\tASSERT(rlen <= args->maxlen);\n\tif (args->prod <= 1 || rlen < args->mod || rlen == args->maxlen ||\n\t    (args->mod == 0 && rlen < args->prod))\n\t\treturn;\n\tk = rlen % args->prod;\n\tif (k == args->mod)\n\t\treturn;\n\tif (k > args->mod)\n\t\trlen = rlen - (k - args->mod);\n\telse\n\t\trlen = rlen - args->prod + (args->mod - k);\n\t/* casts to (int) catch length underflows */\n\tif ((int)rlen < (int)args->minlen)\n\t\treturn;\n\tASSERT(rlen >= args->minlen && rlen <= args->maxlen);\n\tASSERT(rlen % args->prod == args->mod);\n\tASSERT(args->pag->pagf_freeblks + args->pag->pagf_flcount >=\n\t\trlen + args->minleft);\n\targs->len = rlen;\n}\n\n/*\n * Update the two btrees, logically removing from freespace the extent\n * starting at rbno, rlen blocks.  The extent is contained within the\n * actual (current) free extent fbno for flen blocks.\n * Flags are passed in indicating whether the cursors are set to the\n * relevant records.\n */\nSTATIC int\t\t\t\t/* error code */\nxfs_alloc_fixup_trees(\n\txfs_btree_cur_t\t*cnt_cur,\t/* cursor for by-size btree */\n\txfs_btree_cur_t\t*bno_cur,\t/* cursor for by-block btree */\n\txfs_agblock_t\tfbno,\t\t/* starting block of free extent */\n\txfs_extlen_t\tflen,\t\t/* length of free extent */\n\txfs_agblock_t\trbno,\t\t/* starting block of returned extent */\n\txfs_extlen_t\trlen,\t\t/* length of returned extent */\n\tint\t\tflags)\t\t/* flags, XFSA_FIXUP_... */\n{\n\tint\t\terror;\t\t/* error code */\n\tint\t\ti;\t\t/* operation results */\n\txfs_agblock_t\tnfbno1;\t\t/* first new free startblock */\n\txfs_agblock_t\tnfbno2;\t\t/* second new free startblock */\n\txfs_extlen_t\tnflen1=0;\t/* first new free length */\n\txfs_extlen_t\tnflen2=0;\t/* second new free length */\n\tstruct xfs_mount *mp;\n\n\tmp = cnt_cur->bc_mp;\n\n\t/*\n\t * Look up the record in the by-size tree if necessary.\n\t */\n\tif (flags & XFSA_FIXUP_CNT_OK) {\n#ifdef DEBUG\n\t\tif ((error = xfs_alloc_get_rec(cnt_cur, &nfbno1, &nflen1, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp,\n\t\t\t\t   i != 1 ||\n\t\t\t\t   nfbno1 != fbno ||\n\t\t\t\t   nflen1 != flen))\n\t\t\treturn -EFSCORRUPTED;\n#endif\n\t} else {\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, fbno, flen, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\t/*\n\t * Look up the record in the by-block tree if necessary.\n\t */\n\tif (flags & XFSA_FIXUP_BNO_OK) {\n#ifdef DEBUG\n\t\tif ((error = xfs_alloc_get_rec(bno_cur, &nfbno1, &nflen1, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp,\n\t\t\t\t   i != 1 ||\n\t\t\t\t   nfbno1 != fbno ||\n\t\t\t\t   nflen1 != flen))\n\t\t\treturn -EFSCORRUPTED;\n#endif\n\t} else {\n\t\tif ((error = xfs_alloc_lookup_eq(bno_cur, fbno, flen, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\n#ifdef DEBUG\n\tif (bno_cur->bc_nlevels == 1 && cnt_cur->bc_nlevels == 1) {\n\t\tstruct xfs_btree_block\t*bnoblock;\n\t\tstruct xfs_btree_block\t*cntblock;\n\n\t\tbnoblock = XFS_BUF_TO_BLOCK(bno_cur->bc_bufs[0]);\n\t\tcntblock = XFS_BUF_TO_BLOCK(cnt_cur->bc_bufs[0]);\n\n\t\tif (XFS_IS_CORRUPT(mp,\n\t\t\t\t   bnoblock->bb_numrecs !=\n\t\t\t\t   cntblock->bb_numrecs))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n#endif\n\n\t/*\n\t * Deal with all four cases: the allocated record is contained\n\t * within the freespace record, so we can have new freespace\n\t * at either (or both) end, or no freespace remaining.\n\t */\n\tif (rbno == fbno && rlen == flen)\n\t\tnfbno1 = nfbno2 = NULLAGBLOCK;\n\telse if (rbno == fbno) {\n\t\tnfbno1 = rbno + rlen;\n\t\tnflen1 = flen - rlen;\n\t\tnfbno2 = NULLAGBLOCK;\n\t} else if (rbno + rlen == fbno + flen) {\n\t\tnfbno1 = fbno;\n\t\tnflen1 = flen - rlen;\n\t\tnfbno2 = NULLAGBLOCK;\n\t} else {\n\t\tnfbno1 = fbno;\n\t\tnflen1 = rbno - fbno;\n\t\tnfbno2 = rbno + rlen;\n\t\tnflen2 = (fbno + flen) - nfbno2;\n\t}\n\t/*\n\t * Delete the entry from the by-size btree.\n\t */\n\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\treturn error;\n\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\treturn -EFSCORRUPTED;\n\t/*\n\t * Add new by-size btree entry(s).\n\t */\n\tif (nfbno1 != NULLAGBLOCK) {\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, nfbno1, nflen1, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 0))\n\t\t\treturn -EFSCORRUPTED;\n\t\tif ((error = xfs_btree_insert(cnt_cur, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\tif (nfbno2 != NULLAGBLOCK) {\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, nfbno2, nflen2, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 0))\n\t\t\treturn -EFSCORRUPTED;\n\t\tif ((error = xfs_btree_insert(cnt_cur, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\t/*\n\t * Fix up the by-block btree entry(s).\n\t */\n\tif (nfbno1 == NULLAGBLOCK) {\n\t\t/*\n\t\t * No remaining freespace, just delete the by-block tree entry.\n\t\t */\n\t\tif ((error = xfs_btree_delete(bno_cur, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t} else {\n\t\t/*\n\t\t * Update the by-block entry to start later|be shorter.\n\t\t */\n\t\tif ((error = xfs_alloc_update(bno_cur, nfbno1, nflen1)))\n\t\t\treturn error;\n\t}\n\tif (nfbno2 != NULLAGBLOCK) {\n\t\t/*\n\t\t * 2 resulting free entries, need to add one.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(bno_cur, nfbno2, nflen2, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 0))\n\t\t\treturn -EFSCORRUPTED;\n\t\tif ((error = xfs_btree_insert(bno_cur, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}\n\nstatic xfs_failaddr_t\nxfs_agfl_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount *mp = bp->b_mount;\n\tstruct xfs_agfl\t*agfl = XFS_BUF_TO_AGFL(bp);\n\tint\t\ti;\n\n\t/*\n\t * There is no verification of non-crc AGFLs because mkfs does not\n\t * initialise the AGFL to zero or NULL. Hence the only valid part of the\n\t * AGFL is what the AGF says is active. We can't get to the AGF, so we\n\t * can't verify just those entries are valid.\n\t */\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn NULL;\n\n\tif (!xfs_verify_magic(bp, agfl->agfl_magicnum))\n\t\treturn __this_address;\n\tif (!uuid_equal(&agfl->agfl_uuid, &mp->m_sb.sb_meta_uuid))\n\t\treturn __this_address;\n\t/*\n\t * during growfs operations, the perag is not fully initialised,\n\t * so we can't use it for any useful checking. growfs ensures we can't\n\t * use it by using uncached buffers that don't have the perag attached\n\t * so we can detect and avoid this problem.\n\t */\n\tif (bp->b_pag && be32_to_cpu(agfl->agfl_seqno) != bp->b_pag->pag_agno)\n\t\treturn __this_address;\n\n\tfor (i = 0; i < xfs_agfl_size(mp); i++) {\n\t\tif (be32_to_cpu(agfl->agfl_bno[i]) != NULLAGBLOCK &&\n\t\t    be32_to_cpu(agfl->agfl_bno[i]) >= mp->m_sb.sb_agblocks)\n\t\t\treturn __this_address;\n\t}\n\n\tif (!xfs_log_check_lsn(mp, be64_to_cpu(XFS_BUF_TO_AGFL(bp)->agfl_lsn)))\n\t\treturn __this_address;\n\treturn NULL;\n}\n\nstatic void\nxfs_agfl_read_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount *mp = bp->b_mount;\n\txfs_failaddr_t\tfa;\n\n\t/*\n\t * There is no verification of non-crc AGFLs because mkfs does not\n\t * initialise the AGFL to zero or NULL. Hence the only valid part of the\n\t * AGFL is what the AGF says is active. We can't get to the AGF, so we\n\t * can't verify just those entries are valid.\n\t */\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn;\n\n\tif (!xfs_buf_verify_cksum(bp, XFS_AGFL_CRC_OFF))\n\t\txfs_verifier_error(bp, -EFSBADCRC, __this_address);\n\telse {\n\t\tfa = xfs_agfl_verify(bp);\n\t\tif (fa)\n\t\t\txfs_verifier_error(bp, -EFSCORRUPTED, fa);\n\t}\n}\n\nstatic void\nxfs_agfl_write_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tstruct xfs_buf_log_item\t*bip = bp->b_log_item;\n\txfs_failaddr_t\t\tfa;\n\n\t/* no verification of non-crc AGFLs */\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn;\n\n\tfa = xfs_agfl_verify(bp);\n\tif (fa) {\n\t\txfs_verifier_error(bp, -EFSCORRUPTED, fa);\n\t\treturn;\n\t}\n\n\tif (bip)\n\t\tXFS_BUF_TO_AGFL(bp)->agfl_lsn = cpu_to_be64(bip->bli_item.li_lsn);\n\n\txfs_buf_update_cksum(bp, XFS_AGFL_CRC_OFF);\n}\n\nconst struct xfs_buf_ops xfs_agfl_buf_ops = {\n\t.name = \"xfs_agfl\",\n\t.magic = { cpu_to_be32(XFS_AGFL_MAGIC), cpu_to_be32(XFS_AGFL_MAGIC) },\n\t.verify_read = xfs_agfl_read_verify,\n\t.verify_write = xfs_agfl_write_verify,\n\t.verify_struct = xfs_agfl_verify,\n};\n\n/*\n * Read in the allocation group free block array.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_read_agfl(\n\txfs_mount_t\t*mp,\t\t/* mount point structure */\n\txfs_trans_t\t*tp,\t\t/* transaction pointer */\n\txfs_agnumber_t\tagno,\t\t/* allocation group number */\n\txfs_buf_t\t**bpp)\t\t/* buffer for the ag free block array */\n{\n\txfs_buf_t\t*bp;\t\t/* return value */\n\tint\t\terror;\n\n\tASSERT(agno != NULLAGNUMBER);\n\terror = xfs_trans_read_buf(\n\t\t\tmp, tp, mp->m_ddev_targp,\n\t\t\tXFS_AG_DADDR(mp, agno, XFS_AGFL_DADDR(mp)),\n\t\t\tXFS_FSS_TO_BB(mp, 1), 0, &bp, &xfs_agfl_buf_ops);\n\tif (error)\n\t\treturn error;\n\txfs_buf_set_ref(bp, XFS_AGFL_REF);\n\t*bpp = bp;\n\treturn 0;\n}\n\nSTATIC int\nxfs_alloc_update_counters(\n\tstruct xfs_trans\t*tp,\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_buf\t\t*agbp,\n\tlong\t\t\tlen)\n{\n\tstruct xfs_agf\t\t*agf = XFS_BUF_TO_AGF(agbp);\n\n\tpag->pagf_freeblks += len;\n\tbe32_add_cpu(&agf->agf_freeblks, len);\n\n\txfs_trans_agblocks_delta(tp, len);\n\tif (unlikely(be32_to_cpu(agf->agf_freeblks) >\n\t\t     be32_to_cpu(agf->agf_length))) {\n\t\txfs_buf_corruption_error(agbp);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\txfs_alloc_log_agf(tp, agbp, XFS_AGF_FREEBLKS);\n\treturn 0;\n}\n\n/*\n * Block allocation algorithm and data structures.\n */\nstruct xfs_alloc_cur {\n\tstruct xfs_btree_cur\t\t*cnt;\t/* btree cursors */\n\tstruct xfs_btree_cur\t\t*bnolt;\n\tstruct xfs_btree_cur\t\t*bnogt;\n\txfs_extlen_t\t\t\tcur_len;/* current search length */\n\txfs_agblock_t\t\t\trec_bno;/* extent startblock */\n\txfs_extlen_t\t\t\trec_len;/* extent length */\n\txfs_agblock_t\t\t\tbno;\t/* alloc bno */\n\txfs_extlen_t\t\t\tlen;\t/* alloc len */\n\txfs_extlen_t\t\t\tdiff;\t/* diff from search bno */\n\tunsigned int\t\t\tbusy_gen;/* busy state */\n\tbool\t\t\t\tbusy;\n};\n\n/*\n * Set up cursors, etc. in the extent allocation cursor. This function can be\n * called multiple times to reset an initialized structure without having to\n * reallocate cursors.\n */\nstatic int\nxfs_alloc_cur_setup(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur)\n{\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n\tASSERT(args->alignment == 1 || args->type != XFS_ALLOCTYPE_THIS_BNO);\n\n\tacur->cur_len = args->maxlen;\n\tacur->rec_bno = 0;\n\tacur->rec_len = 0;\n\tacur->bno = 0;\n\tacur->len = 0;\n\tacur->diff = -1;\n\tacur->busy = false;\n\tacur->busy_gen = 0;\n\n\t/*\n\t * Perform an initial cntbt lookup to check for availability of maxlen\n\t * extents. If this fails, we'll return -ENOSPC to signal the caller to\n\t * attempt a small allocation.\n\t */\n\tif (!acur->cnt)\n\t\tacur->cnt = xfs_allocbt_init_cursor(args->mp, args->tp,\n\t\t\t\t\targs->agbp, args->agno, XFS_BTNUM_CNT);\n\terror = xfs_alloc_lookup_ge(acur->cnt, 0, args->maxlen, &i);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * Allocate the bnobt left and right search cursors.\n\t */\n\tif (!acur->bnolt)\n\t\tacur->bnolt = xfs_allocbt_init_cursor(args->mp, args->tp,\n\t\t\t\t\targs->agbp, args->agno, XFS_BTNUM_BNO);\n\tif (!acur->bnogt)\n\t\tacur->bnogt = xfs_allocbt_init_cursor(args->mp, args->tp,\n\t\t\t\t\targs->agbp, args->agno, XFS_BTNUM_BNO);\n\treturn i == 1 ? 0 : -ENOSPC;\n}\n\nstatic void\nxfs_alloc_cur_close(\n\tstruct xfs_alloc_cur\t*acur,\n\tbool\t\t\terror)\n{\n\tint\t\t\tcur_error = XFS_BTREE_NOERROR;\n\n\tif (error)\n\t\tcur_error = XFS_BTREE_ERROR;\n\n\tif (acur->cnt)\n\t\txfs_btree_del_cursor(acur->cnt, cur_error);\n\tif (acur->bnolt)\n\t\txfs_btree_del_cursor(acur->bnolt, cur_error);\n\tif (acur->bnogt)\n\t\txfs_btree_del_cursor(acur->bnogt, cur_error);\n\tacur->cnt = acur->bnolt = acur->bnogt = NULL;\n}\n\n/*\n * Check an extent for allocation and track the best available candidate in the\n * allocation structure. The cursor is deactivated if it has entered an out of\n * range state based on allocation arguments. Optionally return the extent\n * extent geometry and allocation status if requested by the caller.\n */\nstatic int\nxfs_alloc_cur_check(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur,\n\tstruct xfs_btree_cur\t*cur,\n\tint\t\t\t*new)\n{\n\tint\t\t\terror, i;\n\txfs_agblock_t\t\tbno, bnoa, bnew;\n\txfs_extlen_t\t\tlen, lena, diff = -1;\n\tbool\t\t\tbusy;\n\tunsigned\t\tbusy_gen = 0;\n\tbool\t\t\tdeactivate = false;\n\tbool\t\t\tisbnobt = cur->bc_btnum == XFS_BTNUM_BNO;\n\n\t*new = 0;\n\n\terror = xfs_alloc_get_rec(cur, &bno, &len, &i);\n\tif (error)\n\t\treturn error;\n\tif (XFS_IS_CORRUPT(args->mp, i != 1))\n\t\treturn -EFSCORRUPTED;\n\n\t/*\n\t * Check minlen and deactivate a cntbt cursor if out of acceptable size\n\t * range (i.e., walking backwards looking for a minlen extent).\n\t */\n\tif (len < args->minlen) {\n\t\tdeactivate = !isbnobt;\n\t\tgoto out;\n\t}\n\n\tbusy = xfs_alloc_compute_aligned(args, bno, len, &bnoa, &lena,\n\t\t\t\t\t &busy_gen);\n\tacur->busy |= busy;\n\tif (busy)\n\t\tacur->busy_gen = busy_gen;\n\t/* deactivate a bnobt cursor outside of locality range */\n\tif (bnoa < args->min_agbno || bnoa > args->max_agbno) {\n\t\tdeactivate = isbnobt;\n\t\tgoto out;\n\t}\n\tif (lena < args->minlen)\n\t\tgoto out;\n\n\targs->len = XFS_EXTLEN_MIN(lena, args->maxlen);\n\txfs_alloc_fix_len(args);\n\tASSERT(args->len >= args->minlen);\n\tif (args->len < acur->len)\n\t\tgoto out;\n\n\t/*\n\t * We have an aligned record that satisfies minlen and beats or matches\n\t * the candidate extent size. Compare locality for near allocation mode.\n\t */\n\tASSERT(args->type == XFS_ALLOCTYPE_NEAR_BNO);\n\tdiff = xfs_alloc_compute_diff(args->agbno, args->len,\n\t\t\t\t      args->alignment, args->datatype,\n\t\t\t\t      bnoa, lena, &bnew);\n\tif (bnew == NULLAGBLOCK)\n\t\tgoto out;\n\n\t/*\n\t * Deactivate a bnobt cursor with worse locality than the current best.\n\t */\n\tif (diff > acur->diff) {\n\t\tdeactivate = isbnobt;\n\t\tgoto out;\n\t}\n\n\tASSERT(args->len > acur->len ||\n\t       (args->len == acur->len && diff <= acur->diff));\n\tacur->rec_bno = bno;\n\tacur->rec_len = len;\n\tacur->bno = bnew;\n\tacur->len = args->len;\n\tacur->diff = diff;\n\t*new = 1;\n\n\t/*\n\t * We're done if we found a perfect allocation. This only deactivates\n\t * the current cursor, but this is just an optimization to terminate a\n\t * cntbt search that otherwise runs to the edge of the tree.\n\t */\n\tif (acur->diff == 0 && acur->len == args->maxlen)\n\t\tdeactivate = true;\nout:\n\tif (deactivate)\n\t\tcur->bc_private.a.priv.abt.active = false;\n\ttrace_xfs_alloc_cur_check(args->mp, cur->bc_btnum, bno, len, diff,\n\t\t\t\t  *new);\n\treturn 0;\n}\n\n/*\n * Complete an allocation of a candidate extent. Remove the extent from both\n * trees and update the args structure.\n */\nSTATIC int\nxfs_alloc_cur_finish(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur)\n{\n\tint\t\t\terror;\n\n\tASSERT(acur->cnt && acur->bnolt);\n\tASSERT(acur->bno >= acur->rec_bno);\n\tASSERT(acur->bno + acur->len <= acur->rec_bno + acur->rec_len);\n\tASSERT(acur->rec_bno + acur->rec_len <=\n\t       be32_to_cpu(XFS_BUF_TO_AGF(args->agbp)->agf_length));\n\n\terror = xfs_alloc_fixup_trees(acur->cnt, acur->bnolt, acur->rec_bno,\n\t\t\t\t      acur->rec_len, acur->bno, acur->len, 0);\n\tif (error)\n\t\treturn error;\n\n\targs->agbno = acur->bno;\n\targs->len = acur->len;\n\targs->wasfromfl = 0;\n\n\ttrace_xfs_alloc_cur(args);\n\treturn 0;\n}\n\n/*\n * Locality allocation lookup algorithm. This expects a cntbt cursor and uses\n * bno optimized lookup to search for extents with ideal size and locality.\n */\nSTATIC int\nxfs_alloc_cntbt_iter(\n\tstruct xfs_alloc_arg\t\t*args,\n\tstruct xfs_alloc_cur\t\t*acur)\n{\n\tstruct xfs_btree_cur\t*cur = acur->cnt;\n\txfs_agblock_t\t\tbno;\n\txfs_extlen_t\t\tlen, cur_len;\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n\tif (!xfs_alloc_cur_active(cur))\n\t\treturn 0;\n\n\t/* locality optimized lookup */\n\tcur_len = acur->cur_len;\n\terror = xfs_alloc_lookup_ge(cur, args->agbno, cur_len, &i);\n\tif (error)\n\t\treturn error;\n\tif (i == 0)\n\t\treturn 0;\n\terror = xfs_alloc_get_rec(cur, &bno, &len, &i);\n\tif (error)\n\t\treturn error;\n\n\t/* check the current record and update search length from it */\n\terror = xfs_alloc_cur_check(args, acur, cur, &i);\n\tif (error)\n\t\treturn error;\n\tASSERT(len >= acur->cur_len);\n\tacur->cur_len = len;\n\n\t/*\n\t * We looked up the first record >= [agbno, len] above. The agbno is a\n\t * secondary key and so the current record may lie just before or after\n\t * agbno. If it is past agbno, check the previous record too so long as\n\t * the length matches as it may be closer. Don't check a smaller record\n\t * because that could deactivate our cursor.\n\t */\n\tif (bno > args->agbno) {\n\t\terror = xfs_btree_decrement(cur, 0, &i);\n\t\tif (!error && i) {\n\t\t\terror = xfs_alloc_get_rec(cur, &bno, &len, &i);\n\t\t\tif (!error && i && len == acur->cur_len)\n\t\t\t\terror = xfs_alloc_cur_check(args, acur, cur,\n\t\t\t\t\t\t\t    &i);\n\t\t}\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * Increment the search key until we find at least one allocation\n\t * candidate or if the extent we found was larger. Otherwise, double the\n\t * search key to optimize the search. Efficiency is more important here\n\t * than absolute best locality.\n\t */\n\tcur_len <<= 1;\n\tif (!acur->len || acur->cur_len >= cur_len)\n\t\tacur->cur_len++;\n\telse\n\t\tacur->cur_len = cur_len;\n\n\treturn error;\n}\n\n/*\n * Deal with the case where only small freespaces remain. Either return the\n * contents of the last freespace record, or allocate space from the freelist if\n * there is nothing in the tree.\n */\nSTATIC int\t\t\t/* error */\nxfs_alloc_ag_vextent_small(\n\tstruct xfs_alloc_arg\t*args,\t/* allocation argument structure */\n\tstruct xfs_btree_cur\t*ccur,\t/* optional by-size cursor */\n\txfs_agblock_t\t\t*fbnop,\t/* result block number */\n\txfs_extlen_t\t\t*flenp,\t/* result length */\n\tint\t\t\t*stat)\t/* status: 0-freelist, 1-normal/none */\n{\n\tint\t\t\terror = 0;\n\txfs_agblock_t\t\tfbno = NULLAGBLOCK;\n\txfs_extlen_t\t\tflen = 0;\n\tint\t\t\ti = 0;\n\n\t/*\n\t * If a cntbt cursor is provided, try to allocate the largest record in\n\t * the tree. Try the AGFL if the cntbt is empty, otherwise fail the\n\t * allocation. Make sure to respect minleft even when pulling from the\n\t * freelist.\n\t */\n\tif (ccur)\n\t\terror = xfs_btree_decrement(ccur, 0, &i);\n\tif (error)\n\t\tgoto error;\n\tif (i) {\n\t\terror = xfs_alloc_get_rec(ccur, &fbno, &flen, &i);\n\t\tif (error)\n\t\t\tgoto error;\n\t\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (args->minlen != 1 || args->alignment != 1 ||\n\t    args->resv == XFS_AG_RESV_AGFL ||\n\t    (be32_to_cpu(XFS_BUF_TO_AGF(args->agbp)->agf_flcount) <=\n\t     args->minleft))\n\t\tgoto out;\n\n\terror = xfs_alloc_get_freelist(args->tp, args->agbp, &fbno, 0);\n\tif (error)\n\t\tgoto error;\n\tif (fbno == NULLAGBLOCK)\n\t\tgoto out;\n\n\txfs_extent_busy_reuse(args->mp, args->agno, fbno, 1,\n\t\t\t      (args->datatype & XFS_ALLOC_NOBUSY));\n\n\tif (args->datatype & XFS_ALLOC_USERDATA) {\n\t\tstruct xfs_buf\t*bp;\n\n\t\terror = xfs_trans_get_buf(args->tp, args->mp->m_ddev_targp,\n\t\t\t\tXFS_AGB_TO_DADDR(args->mp, args->agno, fbno),\n\t\t\t\targs->mp->m_bsize, 0, &bp);\n\t\tif (error)\n\t\t\tgoto error;\n\t\txfs_trans_binval(args->tp, bp);\n\t}\n\t*fbnop = args->agbno = fbno;\n\t*flenp = args->len = 1;\n\tif (XFS_IS_CORRUPT(args->mp,\n\t\t\t   fbno >= be32_to_cpu(\n\t\t\t\t   XFS_BUF_TO_AGF(args->agbp)->agf_length))) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error;\n\t}\n\targs->wasfromfl = 1;\n\ttrace_xfs_alloc_small_freelist(args);\n\n\t/*\n\t * If we're feeding an AGFL block to something that doesn't live in the\n\t * free space, we need to clear out the OWN_AG rmap.\n\t */\n\terror = xfs_rmap_free(args->tp, args->agbp, args->agno, fbno, 1,\n\t\t\t      &XFS_RMAP_OINFO_AG);\n\tif (error)\n\t\tgoto error;\n\n\t*stat = 0;\n\treturn 0;\n\nout:\n\t/*\n\t * Can't do the allocation, give up.\n\t */\n\tif (flen < args->minlen) {\n\t\targs->agbno = NULLAGBLOCK;\n\t\ttrace_xfs_alloc_small_notenough(args);\n\t\tflen = 0;\n\t}\n\t*fbnop = fbno;\n\t*flenp = flen;\n\t*stat = 1;\n\ttrace_xfs_alloc_small_done(args);\n\treturn 0;\n\nerror:\n\ttrace_xfs_alloc_small_error(args);\n\treturn error;\n}\n\n/*\n * Allocate a variable extent in the allocation group agno.\n * Type and bno are used to determine where in the allocation group the\n * extent will start.\n * Extent's length (returned in *len) will be between minlen and maxlen,\n * and of the form k * prod + mod unless there's nothing that large.\n * Return the starting a.g. block, or NULLAGBLOCK if we can't do it.\n */\nSTATIC int\t\t\t/* error */\nxfs_alloc_ag_vextent(\n\txfs_alloc_arg_t\t*args)\t/* argument structure for allocation */\n{\n\tint\t\terror=0;\n\n\tASSERT(args->minlen > 0);\n\tASSERT(args->maxlen > 0);\n\tASSERT(args->minlen <= args->maxlen);\n\tASSERT(args->mod < args->prod);\n\tASSERT(args->alignment > 0);\n\n\t/*\n\t * Branch to correct routine based on the type.\n\t */\n\targs->wasfromfl = 0;\n\tswitch (args->type) {\n\tcase XFS_ALLOCTYPE_THIS_AG:\n\t\terror = xfs_alloc_ag_vextent_size(args);\n\t\tbreak;\n\tcase XFS_ALLOCTYPE_NEAR_BNO:\n\t\terror = xfs_alloc_ag_vextent_near(args);\n\t\tbreak;\n\tcase XFS_ALLOCTYPE_THIS_BNO:\n\t\terror = xfs_alloc_ag_vextent_exact(args);\n\t\tbreak;\n\tdefault:\n\t\tASSERT(0);\n\t\t/* NOTREACHED */\n\t}\n\n\tif (error || args->agbno == NULLAGBLOCK)\n\t\treturn error;\n\n\tASSERT(args->len >= args->minlen);\n\tASSERT(args->len <= args->maxlen);\n\tASSERT(!args->wasfromfl || args->resv != XFS_AG_RESV_AGFL);\n\tASSERT(args->agbno % args->alignment == 0);\n\n\t/* if not file data, insert new block into the reverse map btree */\n\tif (!xfs_rmap_should_skip_owner_update(&args->oinfo)) {\n\t\terror = xfs_rmap_alloc(args->tp, args->agbp, args->agno,\n\t\t\t\t       args->agbno, args->len, &args->oinfo);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (!args->wasfromfl) {\n\t\terror = xfs_alloc_update_counters(args->tp, args->pag,\n\t\t\t\t\t\t  args->agbp,\n\t\t\t\t\t\t  -((long)(args->len)));\n\t\tif (error)\n\t\t\treturn error;\n\n\t\tASSERT(!xfs_extent_busy_search(args->mp, args->agno,\n\t\t\t\t\t      args->agbno, args->len));\n\t}\n\n\txfs_ag_resv_alloc_extent(args->pag, args->resv, args);\n\n\tXFS_STATS_INC(args->mp, xs_allocx);\n\tXFS_STATS_ADD(args->mp, xs_allocb, args->len);\n\treturn error;\n}\n\n/*\n * Allocate a variable extent at exactly agno/bno.\n * Extent's length (returned in *len) will be between minlen and maxlen,\n * and of the form k * prod + mod unless there's nothing that large.\n * Return the starting a.g. block (bno), or NULLAGBLOCK if we can't do it.\n */\nSTATIC int\t\t\t/* error */\nxfs_alloc_ag_vextent_exact(\n\txfs_alloc_arg_t\t*args)\t/* allocation argument structure */\n{\n\txfs_btree_cur_t\t*bno_cur;/* by block-number btree cursor */\n\txfs_btree_cur_t\t*cnt_cur;/* by count btree cursor */\n\tint\t\terror;\n\txfs_agblock_t\tfbno;\t/* start block of found extent */\n\txfs_extlen_t\tflen;\t/* length of found extent */\n\txfs_agblock_t\ttbno;\t/* start block of busy extent */\n\txfs_extlen_t\ttlen;\t/* length of busy extent */\n\txfs_agblock_t\ttend;\t/* end block of busy extent */\n\tint\t\ti;\t/* success/failure of operation */\n\tunsigned\tbusy_gen;\n\n\tASSERT(args->alignment == 1);\n\n\t/*\n\t * Allocate/initialize a cursor for the by-number freespace btree.\n\t */\n\tbno_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,\n\t\t\t\t\t  args->agno, XFS_BTNUM_BNO);\n\n\t/*\n\t * Lookup bno and minlen in the btree (minlen is irrelevant, really).\n\t * Look for the closest free block <= bno, it must contain bno\n\t * if any free block does.\n\t */\n\terror = xfs_alloc_lookup_le(bno_cur, args->agbno, args->minlen, &i);\n\tif (error)\n\t\tgoto error0;\n\tif (!i)\n\t\tgoto not_found;\n\n\t/*\n\t * Grab the freespace record.\n\t */\n\terror = xfs_alloc_get_rec(bno_cur, &fbno, &flen, &i);\n\tif (error)\n\t\tgoto error0;\n\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\tASSERT(fbno <= args->agbno);\n\n\t/*\n\t * Check for overlapping busy extents.\n\t */\n\ttbno = fbno;\n\ttlen = flen;\n\txfs_extent_busy_trim(args, &tbno, &tlen, &busy_gen);\n\n\t/*\n\t * Give up if the start of the extent is busy, or the freespace isn't\n\t * long enough for the minimum request.\n\t */\n\tif (tbno > args->agbno)\n\t\tgoto not_found;\n\tif (tlen < args->minlen)\n\t\tgoto not_found;\n\ttend = tbno + tlen;\n\tif (tend < args->agbno + args->minlen)\n\t\tgoto not_found;\n\n\t/*\n\t * End of extent will be smaller of the freespace end and the\n\t * maximal requested end.\n\t *\n\t * Fix the length according to mod and prod if given.\n\t */\n\targs->len = XFS_AGBLOCK_MIN(tend, args->agbno + args->maxlen)\n\t\t\t\t\t\t- args->agbno;\n\txfs_alloc_fix_len(args);\n\tASSERT(args->agbno + args->len <= tend);\n\n\t/*\n\t * We are allocating agbno for args->len\n\t * Allocate/initialize a cursor for the by-size btree.\n\t */\n\tcnt_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,\n\t\targs->agno, XFS_BTNUM_CNT);\n\tASSERT(args->agbno + args->len <=\n\t\tbe32_to_cpu(XFS_BUF_TO_AGF(args->agbp)->agf_length));\n\terror = xfs_alloc_fixup_trees(cnt_cur, bno_cur, fbno, flen, args->agbno,\n\t\t\t\t      args->len, XFSA_FIXUP_BNO_OK);\n\tif (error) {\n\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_ERROR);\n\t\tgoto error0;\n\t}\n\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);\n\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\n\targs->wasfromfl = 0;\n\ttrace_xfs_alloc_exact_done(args);\n\treturn 0;\n\nnot_found:\n\t/* Didn't find it, return null. */\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);\n\targs->agbno = NULLAGBLOCK;\n\ttrace_xfs_alloc_exact_notfound(args);\n\treturn 0;\n\nerror0:\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_ERROR);\n\ttrace_xfs_alloc_exact_error(args);\n\treturn error;\n}\n\n/*\n * Search a given number of btree records in a given direction. Check each\n * record against the good extent we've already found.\n */\nSTATIC int\nxfs_alloc_walk_iter(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur,\n\tstruct xfs_btree_cur\t*cur,\n\tbool\t\t\tincrement,\n\tbool\t\t\tfind_one, /* quit on first candidate */\n\tint\t\t\tcount,    /* rec count (-1 for infinite) */\n\tint\t\t\t*stat)\n{\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n\t*stat = 0;\n\n\t/*\n\t * Search so long as the cursor is active or we find a better extent.\n\t * The cursor is deactivated if it extends beyond the range of the\n\t * current allocation candidate.\n\t */\n\twhile (xfs_alloc_cur_active(cur) && count) {\n\t\terror = xfs_alloc_cur_check(args, acur, cur, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == 1) {\n\t\t\t*stat = 1;\n\t\t\tif (find_one)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!xfs_alloc_cur_active(cur))\n\t\t\tbreak;\n\n\t\tif (increment)\n\t\t\terror = xfs_btree_increment(cur, 0, &i);\n\t\telse\n\t\t\terror = xfs_btree_decrement(cur, 0, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == 0)\n\t\t\tcur->bc_private.a.priv.abt.active = false;\n\n\t\tif (count > 0)\n\t\t\tcount--;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Search the by-bno and by-size btrees in parallel in search of an extent with\n * ideal locality based on the NEAR mode ->agbno locality hint.\n */\nSTATIC int\nxfs_alloc_ag_vextent_locality(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur,\n\tint\t\t\t*stat)\n{\n\tstruct xfs_btree_cur\t*fbcur = NULL;\n\tint\t\t\terror;\n\tint\t\t\ti;\n\tbool\t\t\tfbinc;\n\n\tASSERT(acur->len == 0);\n\tASSERT(args->type == XFS_ALLOCTYPE_NEAR_BNO);\n\n\t*stat = 0;\n\n\terror = xfs_alloc_lookup_ge(acur->cnt, args->agbno, acur->cur_len, &i);\n\tif (error)\n\t\treturn error;\n\terror = xfs_alloc_lookup_le(acur->bnolt, args->agbno, 0, &i);\n\tif (error)\n\t\treturn error;\n\terror = xfs_alloc_lookup_ge(acur->bnogt, args->agbno, 0, &i);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * Search the bnobt and cntbt in parallel. Search the bnobt left and\n\t * right and lookup the closest extent to the locality hint for each\n\t * extent size key in the cntbt. The entire search terminates\n\t * immediately on a bnobt hit because that means we've found best case\n\t * locality. Otherwise the search continues until the cntbt cursor runs\n\t * off the end of the tree. If no allocation candidate is found at this\n\t * point, give up on locality, walk backwards from the end of the cntbt\n\t * and take the first available extent.\n\t *\n\t * The parallel tree searches balance each other out to provide fairly\n\t * consistent performance for various situations. The bnobt search can\n\t * have pathological behavior in the worst case scenario of larger\n\t * allocation requests and fragmented free space. On the other hand, the\n\t * bnobt is able to satisfy most smaller allocation requests much more\n\t * quickly than the cntbt. The cntbt search can sift through fragmented\n\t * free space and sets of free extents for larger allocation requests\n\t * more quickly than the bnobt. Since the locality hint is just a hint\n\t * and we don't want to scan the entire bnobt for perfect locality, the\n\t * cntbt search essentially bounds the bnobt search such that we can\n\t * find good enough locality at reasonable performance in most cases.\n\t */\n\twhile (xfs_alloc_cur_active(acur->bnolt) ||\n\t       xfs_alloc_cur_active(acur->bnogt) ||\n\t       xfs_alloc_cur_active(acur->cnt)) {\n\n\t\ttrace_xfs_alloc_cur_lookup(args);\n\n\t\t/*\n\t\t * Search the bnobt left and right. In the case of a hit, finish\n\t\t * the search in the opposite direction and we're done.\n\t\t */\n\t\terror = xfs_alloc_walk_iter(args, acur, acur->bnolt, false,\n\t\t\t\t\t    true, 1, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == 1) {\n\t\t\ttrace_xfs_alloc_cur_left(args);\n\t\t\tfbcur = acur->bnogt;\n\t\t\tfbinc = true;\n\t\t\tbreak;\n\t\t}\n\t\terror = xfs_alloc_walk_iter(args, acur, acur->bnogt, true, true,\n\t\t\t\t\t    1, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == 1) {\n\t\t\ttrace_xfs_alloc_cur_right(args);\n\t\t\tfbcur = acur->bnolt;\n\t\t\tfbinc = false;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Check the extent with best locality based on the current\n\t\t * extent size search key and keep track of the best candidate.\n\t\t */\n\t\terror = xfs_alloc_cntbt_iter(args, acur);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (!xfs_alloc_cur_active(acur->cnt)) {\n\t\t\ttrace_xfs_alloc_cur_lookup_done(args);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * If we failed to find anything due to busy extents, return empty\n\t * handed so the caller can flush and retry. If no busy extents were\n\t * found, walk backwards from the end of the cntbt as a last resort.\n\t */\n\tif (!xfs_alloc_cur_active(acur->cnt) && !acur->len && !acur->busy) {\n\t\terror = xfs_btree_decrement(acur->cnt, 0, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i) {\n\t\t\tacur->cnt->bc_private.a.priv.abt.active = true;\n\t\t\tfbcur = acur->cnt;\n\t\t\tfbinc = false;\n\t\t}\n\t}\n\n\t/*\n\t * Search in the opposite direction for a better entry in the case of\n\t * a bnobt hit or walk backwards from the end of the cntbt.\n\t */\n\tif (fbcur) {\n\t\terror = xfs_alloc_walk_iter(args, acur, fbcur, fbinc, true, -1,\n\t\t\t\t\t    &i);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (acur->len)\n\t\t*stat = 1;\n\n\treturn 0;\n}\n\n/* Check the last block of the cnt btree for allocations. */\nstatic int\nxfs_alloc_ag_vextent_lastblock(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur,\n\txfs_agblock_t\t\t*bno,\n\txfs_extlen_t\t\t*len,\n\tbool\t\t\t*allocated)\n{\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n#ifdef DEBUG\n\t/* Randomly don't execute the first algorithm. */\n\tif (prandom_u32() & 1)\n\t\treturn 0;\n#endif\n\n\t/*\n\t * Start from the entry that lookup found, sequence through all larger\n\t * free blocks.  If we're actually pointing at a record smaller than\n\t * maxlen, go to the start of this block, and skip all those smaller\n\t * than minlen.\n\t */\n\tif (len || args->alignment > 1) {\n\t\tacur->cnt->bc_ptrs[0] = 1;\n\t\tdo {\n\t\t\terror = xfs_alloc_get_rec(acur->cnt, bno, len, &i);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\tif (XFS_IS_CORRUPT(args->mp, i != 1))\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tif (*len >= args->minlen)\n\t\t\t\tbreak;\n\t\t\terror = xfs_btree_increment(acur->cnt, 0, &i);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t} while (i);\n\t\tASSERT(*len >= args->minlen);\n\t\tif (!i)\n\t\t\treturn 0;\n\t}\n\n\terror = xfs_alloc_walk_iter(args, acur, acur->cnt, true, false, -1, &i);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * It didn't work.  We COULD be in a case where there's a good record\n\t * somewhere, so try again.\n\t */\n\tif (acur->len == 0)\n\t\treturn 0;\n\n\ttrace_xfs_alloc_near_first(args);\n\t*allocated = true;\n\treturn 0;\n}\n\n/*\n * Allocate a variable extent near bno in the allocation group agno.\n * Extent's length (returned in len) will be between minlen and maxlen,\n * and of the form k * prod + mod unless there's nothing that large.\n * Return the starting a.g. block, or NULLAGBLOCK if we can't do it.\n */\nSTATIC int\nxfs_alloc_ag_vextent_near(\n\tstruct xfs_alloc_arg\t*args)\n{\n\tstruct xfs_alloc_cur\tacur = {};\n\tint\t\t\terror;\t\t/* error code */\n\tint\t\t\ti;\t\t/* result code, temporary */\n\txfs_agblock_t\t\tbno;\n\txfs_extlen_t\t\tlen;\n\n\t/* handle uninitialized agbno range so caller doesn't have to */\n\tif (!args->min_agbno && !args->max_agbno)\n\t\targs->max_agbno = args->mp->m_sb.sb_agblocks - 1;\n\tASSERT(args->min_agbno <= args->max_agbno);\n\n\t/* clamp agbno to the range if it's outside */\n\tif (args->agbno < args->min_agbno)\n\t\targs->agbno = args->min_agbno;\n\tif (args->agbno > args->max_agbno)\n\t\targs->agbno = args->max_agbno;\n\nrestart:\n\tlen = 0;\n\n\t/*\n\t * Set up cursors and see if there are any free extents as big as\n\t * maxlen. If not, pick the last entry in the tree unless the tree is\n\t * empty.\n\t */\n\terror = xfs_alloc_cur_setup(args, &acur);\n\tif (error == -ENOSPC) {\n\t\terror = xfs_alloc_ag_vextent_small(args, acur.cnt, &bno,\n\t\t\t\t&len, &i);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tif (i == 0 || len == 0) {\n\t\t\ttrace_xfs_alloc_near_noentry(args);\n\t\t\tgoto out;\n\t\t}\n\t\tASSERT(i == 1);\n\t} else if (error) {\n\t\tgoto out;\n\t}\n\n\t/*\n\t * First algorithm.\n\t * If the requested extent is large wrt the freespaces available\n\t * in this a.g., then the cursor will be pointing to a btree entry\n\t * near the right edge of the tree.  If it's in the last btree leaf\n\t * block, then we just examine all the entries in that block\n\t * that are big enough, and pick the best one.\n\t */\n\tif (xfs_btree_islastblock(acur.cnt, 0)) {\n\t\tbool\t\tallocated = false;\n\n\t\terror = xfs_alloc_ag_vextent_lastblock(args, &acur, &bno, &len,\n\t\t\t\t&allocated);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tif (allocated)\n\t\t\tgoto alloc_finish;\n\t}\n\n\t/*\n\t * Second algorithm. Combined cntbt and bnobt search to find ideal\n\t * locality.\n\t */\n\terror = xfs_alloc_ag_vextent_locality(args, &acur, &i);\n\tif (error)\n\t\tgoto out;\n\n\t/*\n\t * If we couldn't get anything, give up.\n\t */\n\tif (!acur.len) {\n\t\tif (acur.busy) {\n\t\t\ttrace_xfs_alloc_near_busy(args);\n\t\t\txfs_extent_busy_flush(args->mp, args->pag,\n\t\t\t\t\t      acur.busy_gen);\n\t\t\tgoto restart;\n\t\t}\n\t\ttrace_xfs_alloc_size_neither(args);\n\t\targs->agbno = NULLAGBLOCK;\n\t\tgoto out;\n\t}\n\nalloc_finish:\n\t/* fix up btrees on a successful allocation */\n\terror = xfs_alloc_cur_finish(args, &acur);\n\nout:\n\txfs_alloc_cur_close(&acur, error);\n\treturn error;\n}\n\n/*\n * Allocate a variable extent anywhere in the allocation group agno.\n * Extent's length (returned in len) will be between minlen and maxlen,\n * and of the form k * prod + mod unless there's nothing that large.\n * Return the starting a.g. block, or NULLAGBLOCK if we can't do it.\n */\nSTATIC int\t\t\t\t/* error */\nxfs_alloc_ag_vextent_size(\n\txfs_alloc_arg_t\t*args)\t\t/* allocation argument structure */\n{\n\txfs_btree_cur_t\t*bno_cur;\t/* cursor for bno btree */\n\txfs_btree_cur_t\t*cnt_cur;\t/* cursor for cnt btree */\n\tint\t\terror;\t\t/* error result */\n\txfs_agblock_t\tfbno;\t\t/* start of found freespace */\n\txfs_extlen_t\tflen;\t\t/* length of found freespace */\n\tint\t\ti;\t\t/* temp status variable */\n\txfs_agblock_t\trbno;\t\t/* returned block number */\n\txfs_extlen_t\trlen;\t\t/* length of returned extent */\n\tbool\t\tbusy;\n\tunsigned\tbusy_gen;\n\nrestart:\n\t/*\n\t * Allocate and initialize a cursor for the by-size btree.\n\t */\n\tcnt_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,\n\t\targs->agno, XFS_BTNUM_CNT);\n\tbno_cur = NULL;\n\tbusy = false;\n\n\t/*\n\t * Look for an entry >= maxlen+alignment-1 blocks.\n\t */\n\tif ((error = xfs_alloc_lookup_ge(cnt_cur, 0,\n\t\t\targs->maxlen + args->alignment - 1, &i)))\n\t\tgoto error0;\n\n\t/*\n\t * If none then we have to settle for a smaller extent. In the case that\n\t * there are no large extents, this will return the last entry in the\n\t * tree unless the tree is empty. In the case that there are only busy\n\t * large extents, this will return the largest small extent unless there\n\t * are no smaller extents available.\n\t */\n\tif (!i) {\n\t\terror = xfs_alloc_ag_vextent_small(args, cnt_cur,\n\t\t\t\t\t\t   &fbno, &flen, &i);\n\t\tif (error)\n\t\t\tgoto error0;\n\t\tif (i == 0 || flen == 0) {\n\t\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\t\t\ttrace_xfs_alloc_size_noentry(args);\n\t\t\treturn 0;\n\t\t}\n\t\tASSERT(i == 1);\n\t\tbusy = xfs_alloc_compute_aligned(args, fbno, flen, &rbno,\n\t\t\t\t&rlen, &busy_gen);\n\t} else {\n\t\t/*\n\t\t * Search for a non-busy extent that is large enough.\n\t\t */\n\t\tfor (;;) {\n\t\t\terror = xfs_alloc_get_rec(cnt_cur, &fbno, &flen, &i);\n\t\t\tif (error)\n\t\t\t\tgoto error0;\n\t\t\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\n\t\t\tbusy = xfs_alloc_compute_aligned(args, fbno, flen,\n\t\t\t\t\t&rbno, &rlen, &busy_gen);\n\n\t\t\tif (rlen >= args->maxlen)\n\t\t\t\tbreak;\n\n\t\t\terror = xfs_btree_increment(cnt_cur, 0, &i);\n\t\t\tif (error)\n\t\t\t\tgoto error0;\n\t\t\tif (i == 0) {\n\t\t\t\t/*\n\t\t\t\t * Our only valid extents must have been busy.\n\t\t\t\t * Make it unbusy by forcing the log out and\n\t\t\t\t * retrying.\n\t\t\t\t */\n\t\t\t\txfs_btree_del_cursor(cnt_cur,\n\t\t\t\t\t\t     XFS_BTREE_NOERROR);\n\t\t\t\ttrace_xfs_alloc_size_busy(args);\n\t\t\t\txfs_extent_busy_flush(args->mp,\n\t\t\t\t\t\t\targs->pag, busy_gen);\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * In the first case above, we got the last entry in the\n\t * by-size btree.  Now we check to see if the space hits maxlen\n\t * once aligned; if not, we search left for something better.\n\t * This can't happen in the second case above.\n\t */\n\trlen = XFS_EXTLEN_MIN(args->maxlen, rlen);\n\tif (XFS_IS_CORRUPT(args->mp,\n\t\t\t   rlen != 0 &&\n\t\t\t   (rlen > flen ||\n\t\t\t    rbno + rlen > fbno + flen))) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\tif (rlen < args->maxlen) {\n\t\txfs_agblock_t\tbestfbno;\n\t\txfs_extlen_t\tbestflen;\n\t\txfs_agblock_t\tbestrbno;\n\t\txfs_extlen_t\tbestrlen;\n\n\t\tbestrlen = rlen;\n\t\tbestrbno = rbno;\n\t\tbestflen = flen;\n\t\tbestfbno = fbno;\n\t\tfor (;;) {\n\t\t\tif ((error = xfs_btree_decrement(cnt_cur, 0, &i)))\n\t\t\t\tgoto error0;\n\t\t\tif (i == 0)\n\t\t\t\tbreak;\n\t\t\tif ((error = xfs_alloc_get_rec(cnt_cur, &fbno, &flen,\n\t\t\t\t\t&i)))\n\t\t\t\tgoto error0;\n\t\t\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t\tif (flen < bestrlen)\n\t\t\t\tbreak;\n\t\t\tbusy = xfs_alloc_compute_aligned(args, fbno, flen,\n\t\t\t\t\t&rbno, &rlen, &busy_gen);\n\t\t\trlen = XFS_EXTLEN_MIN(args->maxlen, rlen);\n\t\t\tif (XFS_IS_CORRUPT(args->mp,\n\t\t\t\t\t   rlen != 0 &&\n\t\t\t\t\t   (rlen > flen ||\n\t\t\t\t\t    rbno + rlen > fbno + flen))) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t\tif (rlen > bestrlen) {\n\t\t\t\tbestrlen = rlen;\n\t\t\t\tbestrbno = rbno;\n\t\t\t\tbestflen = flen;\n\t\t\t\tbestfbno = fbno;\n\t\t\t\tif (rlen == args->maxlen)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, bestfbno, bestflen,\n\t\t\t\t&i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\trlen = bestrlen;\n\t\trbno = bestrbno;\n\t\tflen = bestflen;\n\t\tfbno = bestfbno;\n\t}\n\targs->wasfromfl = 0;\n\t/*\n\t * Fix up the length.\n\t */\n\targs->len = rlen;\n\tif (rlen < args->minlen) {\n\t\tif (busy) {\n\t\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\t\t\ttrace_xfs_alloc_size_busy(args);\n\t\t\txfs_extent_busy_flush(args->mp, args->pag, busy_gen);\n\t\t\tgoto restart;\n\t\t}\n\t\tgoto out_nominleft;\n\t}\n\txfs_alloc_fix_len(args);\n\n\trlen = args->len;\n\tif (XFS_IS_CORRUPT(args->mp, rlen > flen)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\t/*\n\t * Allocate and initialize a cursor for the by-block tree.\n\t */\n\tbno_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,\n\t\targs->agno, XFS_BTNUM_BNO);\n\tif ((error = xfs_alloc_fixup_trees(cnt_cur, bno_cur, fbno, flen,\n\t\t\trbno, rlen, XFSA_FIXUP_CNT_OK)))\n\t\tgoto error0;\n\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);\n\tcnt_cur = bno_cur = NULL;\n\targs->len = rlen;\n\targs->agbno = rbno;\n\tif (XFS_IS_CORRUPT(args->mp,\n\t\t\t   args->agbno + args->len >\n\t\t\t   be32_to_cpu(\n\t\t\t\t   XFS_BUF_TO_AGF(args->agbp)->agf_length))) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\ttrace_xfs_alloc_size_done(args);\n\treturn 0;\n\nerror0:\n\ttrace_xfs_alloc_size_error(args);\n\tif (cnt_cur)\n\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_ERROR);\n\tif (bno_cur)\n\t\txfs_btree_del_cursor(bno_cur, XFS_BTREE_ERROR);\n\treturn error;\n\nout_nominleft:\n\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\ttrace_xfs_alloc_size_nominleft(args);\n\targs->agbno = NULLAGBLOCK;\n\treturn 0;\n}\n\n/*\n * Free the extent starting at agno/bno for length.\n */\nSTATIC int\nxfs_free_ag_extent(\n\tstruct xfs_trans\t\t*tp,\n\tstruct xfs_buf\t\t\t*agbp,\n\txfs_agnumber_t\t\t\tagno,\n\txfs_agblock_t\t\t\tbno,\n\txfs_extlen_t\t\t\tlen,\n\tconst struct xfs_owner_info\t*oinfo,\n\tenum xfs_ag_resv_type\t\ttype)\n{\n\tstruct xfs_mount\t\t*mp;\n\tstruct xfs_perag\t\t*pag;\n\tstruct xfs_btree_cur\t\t*bno_cur;\n\tstruct xfs_btree_cur\t\t*cnt_cur;\n\txfs_agblock_t\t\t\tgtbno; /* start of right neighbor */\n\txfs_extlen_t\t\t\tgtlen; /* length of right neighbor */\n\txfs_agblock_t\t\t\tltbno; /* start of left neighbor */\n\txfs_extlen_t\t\t\tltlen; /* length of left neighbor */\n\txfs_agblock_t\t\t\tnbno; /* new starting block of freesp */\n\txfs_extlen_t\t\t\tnlen; /* new length of freespace */\n\tint\t\t\t\thaveleft; /* have a left neighbor */\n\tint\t\t\t\thaveright; /* have a right neighbor */\n\tint\t\t\t\ti;\n\tint\t\t\t\terror;\n\n\tbno_cur = cnt_cur = NULL;\n\tmp = tp->t_mountp;\n\n\tif (!xfs_rmap_should_skip_owner_update(oinfo)) {\n\t\terror = xfs_rmap_free(tp, agbp, agno, bno, len, oinfo);\n\t\tif (error)\n\t\t\tgoto error0;\n\t}\n\n\t/*\n\t * Allocate and initialize a cursor for the by-block btree.\n\t */\n\tbno_cur = xfs_allocbt_init_cursor(mp, tp, agbp, agno, XFS_BTNUM_BNO);\n\t/*\n\t * Look for a neighboring block on the left (lower block numbers)\n\t * that is contiguous with this space.\n\t */\n\tif ((error = xfs_alloc_lookup_le(bno_cur, bno, len, &haveleft)))\n\t\tgoto error0;\n\tif (haveleft) {\n\t\t/*\n\t\t * There is a block to our left.\n\t\t */\n\t\tif ((error = xfs_alloc_get_rec(bno_cur, &ltbno, &ltlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * It's not contiguous, though.\n\t\t */\n\t\tif (ltbno + ltlen < bno)\n\t\t\thaveleft = 0;\n\t\telse {\n\t\t\t/*\n\t\t\t * If this failure happens the request to free this\n\t\t\t * space was invalid, it's (partly) already free.\n\t\t\t * Very bad.\n\t\t\t */\n\t\t\tif (XFS_IS_CORRUPT(mp, ltbno + ltlen > bno)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t}\n\t}\n\t/*\n\t * Look for a neighboring block on the right (higher block numbers)\n\t * that is contiguous with this space.\n\t */\n\tif ((error = xfs_btree_increment(bno_cur, 0, &haveright)))\n\t\tgoto error0;\n\tif (haveright) {\n\t\t/*\n\t\t * There is a block to our right.\n\t\t */\n\t\tif ((error = xfs_alloc_get_rec(bno_cur, &gtbno, &gtlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * It's not contiguous, though.\n\t\t */\n\t\tif (bno + len < gtbno)\n\t\t\thaveright = 0;\n\t\telse {\n\t\t\t/*\n\t\t\t * If this failure happens the request to free this\n\t\t\t * space was invalid, it's (partly) already free.\n\t\t\t * Very bad.\n\t\t\t */\n\t\t\tif (XFS_IS_CORRUPT(mp, bno + len > gtbno)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t}\n\t}\n\t/*\n\t * Now allocate and initialize a cursor for the by-size tree.\n\t */\n\tcnt_cur = xfs_allocbt_init_cursor(mp, tp, agbp, agno, XFS_BTNUM_CNT);\n\t/*\n\t * Have both left and right contiguous neighbors.\n\t * Merge all three into a single free block.\n\t */\n\tif (haveleft && haveright) {\n\t\t/*\n\t\t * Delete the old by-size entry on the left.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, ltbno, ltlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Delete the old by-size entry on the right.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, gtbno, gtlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Delete the old by-block entry for the right block.\n\t\t */\n\t\tif ((error = xfs_btree_delete(bno_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Move the by-block cursor back to the left neighbor.\n\t\t */\n\t\tif ((error = xfs_btree_decrement(bno_cur, 0, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n#ifdef DEBUG\n\t\t/*\n\t\t * Check that this is the right record: delete didn't\n\t\t * mangle the cursor.\n\t\t */\n\t\t{\n\t\t\txfs_agblock_t\txxbno;\n\t\t\txfs_extlen_t\txxlen;\n\n\t\t\tif ((error = xfs_alloc_get_rec(bno_cur, &xxbno, &xxlen,\n\t\t\t\t\t&i)))\n\t\t\t\tgoto error0;\n\t\t\tif (XFS_IS_CORRUPT(mp,\n\t\t\t\t\t   i != 1 ||\n\t\t\t\t\t   xxbno != ltbno ||\n\t\t\t\t\t   xxlen != ltlen)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t}\n#endif\n\t\t/*\n\t\t * Update remaining by-block entry to the new, joined block.\n\t\t */\n\t\tnbno = ltbno;\n\t\tnlen = len + ltlen + gtlen;\n\t\tif ((error = xfs_alloc_update(bno_cur, nbno, nlen)))\n\t\t\tgoto error0;\n\t}\n\t/*\n\t * Have only a left contiguous neighbor.\n\t * Merge it together with the new freespace.\n\t */\n\telse if (haveleft) {\n\t\t/*\n\t\t * Delete the old by-size entry on the left.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, ltbno, ltlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Back up the by-block cursor to the left neighbor, and\n\t\t * update its length.\n\t\t */\n\t\tif ((error = xfs_btree_decrement(bno_cur, 0, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tnbno = ltbno;\n\t\tnlen = len + ltlen;\n\t\tif ((error = xfs_alloc_update(bno_cur, nbno, nlen)))\n\t\t\tgoto error0;\n\t}\n\t/*\n\t * Have only a right contiguous neighbor.\n\t * Merge it together with the new freespace.\n\t */\n\telse if (haveright) {\n\t\t/*\n\t\t * Delete the old by-size entry on the right.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, gtbno, gtlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Update the starting block and length of the right\n\t\t * neighbor in the by-block tree.\n\t\t */\n\t\tnbno = bno;\n\t\tnlen = len + gtlen;\n\t\tif ((error = xfs_alloc_update(bno_cur, nbno, nlen)))\n\t\t\tgoto error0;\n\t}\n\t/*\n\t * No contiguous neighbors.\n\t * Insert the new freespace into the by-block tree.\n\t */\n\telse {\n\t\tnbno = bno;\n\t\tnlen = len;\n\t\tif ((error = xfs_btree_insert(bno_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t}\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);\n\tbno_cur = NULL;\n\t/*\n\t * In all cases we need to insert the new freespace in the by-size tree.\n\t */\n\tif ((error = xfs_alloc_lookup_eq(cnt_cur, nbno, nlen, &i)))\n\t\tgoto error0;\n\tif (XFS_IS_CORRUPT(mp, i != 0)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\tif ((error = xfs_btree_insert(cnt_cur, &i)))\n\t\tgoto error0;\n\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\tcnt_cur = NULL;\n\n\t/*\n\t * Update the freespace totals in the ag and superblock.\n\t */\n\tpag = xfs_perag_get(mp, agno);\n\terror = xfs_alloc_update_counters(tp, pag, agbp, len);\n\txfs_ag_resv_free_extent(pag, type, tp, len);\n\txfs_perag_put(pag);\n\tif (error)\n\t\tgoto error0;\n\n\tXFS_STATS_INC(mp, xs_freex);\n\tXFS_STATS_ADD(mp, xs_freeb, len);\n\n\ttrace_xfs_free_extent(mp, agno, bno, len, type, haveleft, haveright);\n\n\treturn 0;\n\n error0:\n\ttrace_xfs_free_extent(mp, agno, bno, len, type, -1, -1);\n\tif (bno_cur)\n\t\txfs_btree_del_cursor(bno_cur, XFS_BTREE_ERROR);\n\tif (cnt_cur)\n\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_ERROR);\n\treturn error;\n}\n\n/*\n * Visible (exported) allocation/free functions.\n * Some of these are used just by xfs_alloc_btree.c and this file.\n */\n\n/*\n * Compute and fill in value of m_ag_maxlevels.\n */\nvoid\nxfs_alloc_compute_maxlevels(\n\txfs_mount_t\t*mp)\t/* file system mount structure */\n{\n\tmp->m_ag_maxlevels = xfs_btree_compute_maxlevels(mp->m_alloc_mnr,\n\t\t\t(mp->m_sb.sb_agblocks + 1) / 2);\n}\n\n/*\n * Find the length of the longest extent in an AG.  The 'need' parameter\n * specifies how much space we're going to need for the AGFL and the\n * 'reserved' parameter tells us how many blocks in this AG are reserved for\n * other callers.\n */\nxfs_extlen_t\nxfs_alloc_longest_free_extent(\n\tstruct xfs_perag\t*pag,\n\txfs_extlen_t\t\tneed,\n\txfs_extlen_t\t\treserved)\n{\n\txfs_extlen_t\t\tdelta = 0;\n\n\t/*\n\t * If the AGFL needs a recharge, we'll have to subtract that from the\n\t * longest extent.\n\t */\n\tif (need > pag->pagf_flcount)\n\t\tdelta = need - pag->pagf_flcount;\n\n\t/*\n\t * If we cannot maintain others' reservations with space from the\n\t * not-longest freesp extents, we'll have to subtract /that/ from\n\t * the longest extent too.\n\t */\n\tif (pag->pagf_freeblks - pag->pagf_longest < reserved)\n\t\tdelta += reserved - (pag->pagf_freeblks - pag->pagf_longest);\n\n\t/*\n\t * If the longest extent is long enough to satisfy all the\n\t * reservations and AGFL rules in place, we can return this extent.\n\t */\n\tif (pag->pagf_longest > delta)\n\t\treturn min_t(xfs_extlen_t, pag->pag_mount->m_ag_max_usable,\n\t\t\t\tpag->pagf_longest - delta);\n\n\t/* Otherwise, let the caller try for 1 block if there's space. */\n\treturn pag->pagf_flcount > 0 || pag->pagf_longest > 0;\n}\n\n/*\n * Compute the minimum length of the AGFL in the given AG.  If @pag is NULL,\n * return the largest possible minimum length.\n */\nunsigned int\nxfs_alloc_min_freelist(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag)\n{\n\t/* AG btrees have at least 1 level. */\n\tstatic const uint8_t\tfake_levels[XFS_BTNUM_AGF] = {1, 1, 1};\n\tconst uint8_t\t\t*levels = pag ? pag->pagf_levels : fake_levels;\n\tunsigned int\t\tmin_free;\n\n\tASSERT(mp->m_ag_maxlevels > 0);\n\n\t/* space needed by-bno freespace btree */\n\tmin_free = min_t(unsigned int, levels[XFS_BTNUM_BNOi] + 1,\n\t\t\t\t       mp->m_ag_maxlevels);\n\t/* space needed by-size freespace btree */\n\tmin_free += min_t(unsigned int, levels[XFS_BTNUM_CNTi] + 1,\n\t\t\t\t       mp->m_ag_maxlevels);\n\t/* space needed reverse mapping used space btree */\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb))\n\t\tmin_free += min_t(unsigned int, levels[XFS_BTNUM_RMAPi] + 1,\n\t\t\t\t\t\tmp->m_rmap_maxlevels);\n\n\treturn min_free;\n}\n\n/*\n * Check if the operation we are fixing up the freelist for should go ahead or\n * not. If we are freeing blocks, we always allow it, otherwise the allocation\n * is dependent on whether the size and shape of free space available will\n * permit the requested allocation to take place.\n */\nstatic bool\nxfs_alloc_space_available(\n\tstruct xfs_alloc_arg\t*args,\n\txfs_extlen_t\t\tmin_free,\n\tint\t\t\tflags)\n{\n\tstruct xfs_perag\t*pag = args->pag;\n\txfs_extlen_t\t\talloc_len, longest;\n\txfs_extlen_t\t\treservation; /* blocks that are still reserved */\n\tint\t\t\tavailable;\n\txfs_extlen_t\t\tagflcount;\n\n\tif (flags & XFS_ALLOC_FLAG_FREEING)\n\t\treturn true;\n\n\treservation = xfs_ag_resv_needed(pag, args->resv);\n\n\t/* do we have enough contiguous free space for the allocation? */\n\talloc_len = args->minlen + (args->alignment - 1) + args->minalignslop;\n\tlongest = xfs_alloc_longest_free_extent(pag, min_free, reservation);\n\tif (longest < alloc_len)\n\t\treturn false;\n\n\t/*\n\t * Do we have enough free space remaining for the allocation? Don't\n\t * account extra agfl blocks because we are about to defer free them,\n\t * making them unavailable until the current transaction commits.\n\t */\n\tagflcount = min_t(xfs_extlen_t, pag->pagf_flcount, min_free);\n\tavailable = (int)(pag->pagf_freeblks + agflcount -\n\t\t\t  reservation - min_free - args->minleft);\n\tif (available < (int)max(args->total, alloc_len))\n\t\treturn false;\n\n\t/*\n\t * Clamp maxlen to the amount of free space available for the actual\n\t * extent allocation.\n\t */\n\tif (available < (int)args->maxlen && !(flags & XFS_ALLOC_FLAG_CHECK)) {\n\t\targs->maxlen = available;\n\t\tASSERT(args->maxlen > 0);\n\t\tASSERT(args->maxlen >= args->minlen);\n\t}\n\n\treturn true;\n}\n\nint\nxfs_free_agfl_block(\n\tstruct xfs_trans\t*tp,\n\txfs_agnumber_t\t\tagno,\n\txfs_agblock_t\t\tagbno,\n\tstruct xfs_buf\t\t*agbp,\n\tstruct xfs_owner_info\t*oinfo)\n{\n\tint\t\t\terror;\n\tstruct xfs_buf\t\t*bp;\n\n\terror = xfs_free_ag_extent(tp, agbp, agno, agbno, 1, oinfo,\n\t\t\t\t   XFS_AG_RESV_AGFL);\n\tif (error)\n\t\treturn error;\n\n\terror = xfs_trans_get_buf(tp, tp->t_mountp->m_ddev_targp,\n\t\t\tXFS_AGB_TO_DADDR(tp->t_mountp, agno, agbno),\n\t\t\ttp->t_mountp->m_bsize, 0, &bp);\n\tif (error)\n\t\treturn error;\n\txfs_trans_binval(tp, bp);\n\n\treturn 0;\n}\n\n/*\n * Check the agfl fields of the agf for inconsistency or corruption. The purpose\n * is to detect an agfl header padding mismatch between current and early v5\n * kernels. This problem manifests as a 1-slot size difference between the\n * on-disk flcount and the active [first, last] range of a wrapped agfl. This\n * may also catch variants of agfl count corruption unrelated to padding. Either\n * way, we'll reset the agfl and warn the user.\n *\n * Return true if a reset is required before the agfl can be used, false\n * otherwise.\n */\nstatic bool\nxfs_agfl_needs_reset(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_agf\t\t*agf)\n{\n\tuint32_t\t\tf = be32_to_cpu(agf->agf_flfirst);\n\tuint32_t\t\tl = be32_to_cpu(agf->agf_fllast);\n\tuint32_t\t\tc = be32_to_cpu(agf->agf_flcount);\n\tint\t\t\tagfl_size = xfs_agfl_size(mp);\n\tint\t\t\tactive;\n\n\t/* no agfl header on v4 supers */\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn false;\n\n\t/*\n\t * The agf read verifier catches severe corruption of these fields.\n\t * Repeat some sanity checks to cover a packed -> unpacked mismatch if\n\t * the verifier allows it.\n\t */\n\tif (f >= agfl_size || l >= agfl_size)\n\t\treturn true;\n\tif (c > agfl_size)\n\t\treturn true;\n\n\t/*\n\t * Check consistency between the on-disk count and the active range. An\n\t * agfl padding mismatch manifests as an inconsistent flcount.\n\t */\n\tif (c && l >= f)\n\t\tactive = l - f + 1;\n\telse if (c)\n\t\tactive = agfl_size - f + l + 1;\n\telse\n\t\tactive = 0;\n\n\treturn active != c;\n}\n\n/*\n * Reset the agfl to an empty state. Ignore/drop any existing blocks since the\n * agfl content cannot be trusted. Warn the user that a repair is required to\n * recover leaked blocks.\n *\n * The purpose of this mechanism is to handle filesystems affected by the agfl\n * header padding mismatch problem. A reset keeps the filesystem online with a\n * relatively minor free space accounting inconsistency rather than suffer the\n * inevitable crash from use of an invalid agfl block.\n */\nstatic void\nxfs_agfl_reset(\n\tstruct xfs_trans\t*tp,\n\tstruct xfs_buf\t\t*agbp,\n\tstruct xfs_perag\t*pag)\n{\n\tstruct xfs_mount\t*mp = tp->t_mountp;\n\tstruct xfs_agf\t\t*agf = XFS_BUF_TO_AGF(agbp);\n\n\tASSERT(pag->pagf_agflreset);\n\ttrace_xfs_agfl_reset(mp, agf, 0, _RET_IP_);\n\n\txfs_warn(mp,\n\t       \"WARNING: Reset corrupted AGFL on AG %u. %d blocks leaked. \"\n\t       \"Please unmount and run xfs_repair.\",\n\t         pag->pag_agno, pag->pagf_flcount);\n\n\tagf->agf_flfirst = 0;\n\tagf->agf_fllast = cpu_to_be32(xfs_agfl_size(mp) - 1);\n\tagf->agf_flcount = 0;\n\txfs_alloc_log_agf(tp, agbp, XFS_AGF_FLFIRST | XFS_AGF_FLLAST |\n\t\t\t\t    XFS_AGF_FLCOUNT);\n\n\tpag->pagf_flcount = 0;\n\tpag->pagf_agflreset = false;\n}\n\n/*\n * Defer an AGFL block free. This is effectively equivalent to\n * xfs_bmap_add_free() with some special handling particular to AGFL blocks.\n *\n * Deferring AGFL frees helps prevent log reservation overruns due to too many\n * allocation operations in a transaction. AGFL frees are prone to this problem\n * because for one they are always freed one at a time. Further, an immediate\n * AGFL block free can cause a btree join and require another block free before\n * the real allocation can proceed. Deferring the free disconnects freeing up\n * the AGFL slot from freeing the block.\n */\nSTATIC void\nxfs_defer_agfl_block(\n\tstruct xfs_trans\t\t*tp,\n\txfs_agnumber_t\t\t\tagno,\n\txfs_fsblock_t\t\t\tagbno,\n\tstruct xfs_owner_info\t\t*oinfo)\n{\n\tstruct xfs_mount\t\t*mp = tp->t_mountp;\n\tstruct xfs_extent_free_item\t*new;\t\t/* new element */\n\n\tASSERT(xfs_bmap_free_item_zone != NULL);\n\tASSERT(oinfo != NULL);\n\n\tnew = kmem_zone_alloc(xfs_bmap_free_item_zone, 0);\n\tnew->xefi_startblock = XFS_AGB_TO_FSB(mp, agno, agbno);\n\tnew->xefi_blockcount = 1;\n\tnew->xefi_oinfo = *oinfo;\n\n\ttrace_xfs_agfl_free_defer(mp, agno, 0, agbno, 1);\n\n\txfs_defer_add(tp, XFS_DEFER_OPS_TYPE_AGFL_FREE, &new->xefi_list);\n}\n\n/*\n * Decide whether to use this allocation group for this allocation.\n * If so, fix up the btree freelist's size.\n */\nint\t\t\t/* error */\nxfs_alloc_fix_freelist(\n\tstruct xfs_alloc_arg\t*args,\t/* allocation argument structure */\n\tint\t\t\tflags)\t/* XFS_ALLOC_FLAG_... */\n{\n\tstruct xfs_mount\t*mp = args->mp;\n\tstruct xfs_perag\t*pag = args->pag;\n\tstruct xfs_trans\t*tp = args->tp;\n\tstruct xfs_buf\t\t*agbp = NULL;\n\tstruct xfs_buf\t\t*agflbp = NULL;\n\tstruct xfs_alloc_arg\ttargs;\t/* local allocation arguments */\n\txfs_agblock_t\t\tbno;\t/* freelist block */\n\txfs_extlen_t\t\tneed;\t/* total blocks needed in freelist */\n\tint\t\t\terror = 0;\n\n\t/* deferred ops (AGFL block frees) require permanent transactions */\n\tASSERT(tp->t_flags & XFS_TRANS_PERM_LOG_RES);\n\n\tif (!pag->pagf_init) {\n\t\terror = xfs_alloc_read_agf(mp, tp, args->agno, flags, &agbp);\n\t\tif (error) {\n\t\t\t/* Couldn't lock the AGF so skip this AG. */\n\t\t\tif (error == -EAGAIN)\n\t\t\t\terror = 0;\n\t\t\tgoto out_no_agbp;\n\t\t}\n\t}\n\n\t/*\n\t * If this is a metadata preferred pag and we are user data then try\n\t * somewhere else if we are not being asked to try harder at this\n\t * point\n\t */\n\tif (pag->pagf_metadata && (args->datatype & XFS_ALLOC_USERDATA) &&\n\t    (flags & XFS_ALLOC_FLAG_TRYLOCK)) {\n\t\tASSERT(!(flags & XFS_ALLOC_FLAG_FREEING));\n\t\tgoto out_agbp_relse;\n\t}\n\n\tneed = xfs_alloc_min_freelist(mp, pag);\n\tif (!xfs_alloc_space_available(args, need, flags |\n\t\t\tXFS_ALLOC_FLAG_CHECK))\n\t\tgoto out_agbp_relse;\n\n\t/*\n\t * Get the a.g. freespace buffer.\n\t * Can fail if we're not blocking on locks, and it's held.\n\t */\n\tif (!agbp) {\n\t\terror = xfs_alloc_read_agf(mp, tp, args->agno, flags, &agbp);\n\t\tif (error) {\n\t\t\t/* Couldn't lock the AGF so skip this AG. */\n\t\t\tif (error == -EAGAIN)\n\t\t\t\terror = 0;\n\t\t\tgoto out_no_agbp;\n\t\t}\n\t}\n\n\t/* reset a padding mismatched agfl before final free space check */\n\tif (pag->pagf_agflreset)\n\t\txfs_agfl_reset(tp, agbp, pag);\n\n\t/* If there isn't enough total space or single-extent, reject it. */\n\tneed = xfs_alloc_min_freelist(mp, pag);\n\tif (!xfs_alloc_space_available(args, need, flags))\n\t\tgoto out_agbp_relse;\n\n\t/*\n\t * Make the freelist shorter if it's too long.\n\t *\n\t * Note that from this point onwards, we will always release the agf and\n\t * agfl buffers on error. This handles the case where we error out and\n\t * the buffers are clean or may not have been joined to the transaction\n\t * and hence need to be released manually. If they have been joined to\n\t * the transaction, then xfs_trans_brelse() will handle them\n\t * appropriately based on the recursion count and dirty state of the\n\t * buffer.\n\t *\n\t * XXX (dgc): When we have lots of free space, does this buy us\n\t * anything other than extra overhead when we need to put more blocks\n\t * back on the free list? Maybe we should only do this when space is\n\t * getting low or the AGFL is more than half full?\n\t *\n\t * The NOSHRINK flag prevents the AGFL from being shrunk if it's too\n\t * big; the NORMAP flag prevents AGFL expand/shrink operations from\n\t * updating the rmapbt.  Both flags are used in xfs_repair while we're\n\t * rebuilding the rmapbt, and neither are used by the kernel.  They're\n\t * both required to ensure that rmaps are correctly recorded for the\n\t * regenerated AGFL, bnobt, and cntbt.  See repair/phase5.c and\n\t * repair/rmap.c in xfsprogs for details.\n\t */\n\tmemset(&targs, 0, sizeof(targs));\n\t/* struct copy below */\n\tif (flags & XFS_ALLOC_FLAG_NORMAP)\n\t\ttargs.oinfo = XFS_RMAP_OINFO_SKIP_UPDATE;\n\telse\n\t\ttargs.oinfo = XFS_RMAP_OINFO_AG;\n\twhile (!(flags & XFS_ALLOC_FLAG_NOSHRINK) && pag->pagf_flcount > need) {\n\t\terror = xfs_alloc_get_freelist(tp, agbp, &bno, 0);\n\t\tif (error)\n\t\t\tgoto out_agbp_relse;\n\n\t\t/* defer agfl frees */\n\t\txfs_defer_agfl_block(tp, args->agno, bno, &targs.oinfo);\n\t}\n\n\ttargs.tp = tp;\n\ttargs.mp = mp;\n\ttargs.agbp = agbp;\n\ttargs.agno = args->agno;\n\ttargs.alignment = targs.minlen = targs.prod = 1;\n\ttargs.type = XFS_ALLOCTYPE_THIS_AG;\n\ttargs.pag = pag;\n\terror = xfs_alloc_read_agfl(mp, tp, targs.agno, &agflbp);\n\tif (error)\n\t\tgoto out_agbp_relse;\n\n\t/* Make the freelist longer if it's too short. */\n\twhile (pag->pagf_flcount < need) {\n\t\ttargs.agbno = 0;\n\t\ttargs.maxlen = need - pag->pagf_flcount;\n\t\ttargs.resv = XFS_AG_RESV_AGFL;\n\n\t\t/* Allocate as many blocks as possible at once. */\n\t\terror = xfs_alloc_ag_vextent(&targs);\n\t\tif (error)\n\t\t\tgoto out_agflbp_relse;\n\n\t\t/*\n\t\t * Stop if we run out.  Won't happen if callers are obeying\n\t\t * the restrictions correctly.  Can happen for free calls\n\t\t * on a completely full ag.\n\t\t */\n\t\tif (targs.agbno == NULLAGBLOCK) {\n\t\t\tif (flags & XFS_ALLOC_FLAG_FREEING)\n\t\t\t\tbreak;\n\t\t\tgoto out_agflbp_relse;\n\t\t}\n\t\t/*\n\t\t * Put each allocated block on the list.\n\t\t */\n\t\tfor (bno = targs.agbno; bno < targs.agbno + targs.len; bno++) {\n\t\t\terror = xfs_alloc_put_freelist(tp, agbp,\n\t\t\t\t\t\t\tagflbp, bno, 0);\n\t\t\tif (error)\n\t\t\t\tgoto out_agflbp_relse;\n\t\t}\n\t}\n\txfs_trans_brelse(tp, agflbp);\n\targs->agbp = agbp;\n\treturn 0;\n\nout_agflbp_relse:\n\txfs_trans_brelse(tp, agflbp);\nout_agbp_relse:\n\tif (agbp)\n\t\txfs_trans_brelse(tp, agbp);\nout_no_agbp:\n\targs->agbp = NULL;\n\treturn error;\n}\n\n/*\n * Get a block from the freelist.\n * Returns with the buffer for the block gotten.\n */\nint\t\t\t\t/* error */\nxfs_alloc_get_freelist(\n\txfs_trans_t\t*tp,\t/* transaction pointer */\n\txfs_buf_t\t*agbp,\t/* buffer containing the agf structure */\n\txfs_agblock_t\t*bnop,\t/* block address retrieved from freelist */\n\tint\t\tbtreeblk) /* destination is a AGF btree */\n{\n\txfs_agf_t\t*agf;\t/* a.g. freespace structure */\n\txfs_buf_t\t*agflbp;/* buffer for a.g. freelist structure */\n\txfs_agblock_t\tbno;\t/* block number returned */\n\t__be32\t\t*agfl_bno;\n\tint\t\terror;\n\tint\t\tlogflags;\n\txfs_mount_t\t*mp = tp->t_mountp;\n\txfs_perag_t\t*pag;\t/* per allocation group data */\n\n\t/*\n\t * Freelist is empty, give up.\n\t */\n\tagf = XFS_BUF_TO_AGF(agbp);\n\tif (!agf->agf_flcount) {\n\t\t*bnop = NULLAGBLOCK;\n\t\treturn 0;\n\t}\n\t/*\n\t * Read the array of free blocks.\n\t */\n\terror = xfs_alloc_read_agfl(mp, tp, be32_to_cpu(agf->agf_seqno),\n\t\t\t\t    &agflbp);\n\tif (error)\n\t\treturn error;\n\n\n\t/*\n\t * Get the block number and update the data structures.\n\t */\n\tagfl_bno = XFS_BUF_TO_AGFL_BNO(mp, agflbp);\n\tbno = be32_to_cpu(agfl_bno[be32_to_cpu(agf->agf_flfirst)]);\n\tbe32_add_cpu(&agf->agf_flfirst, 1);\n\txfs_trans_brelse(tp, agflbp);\n\tif (be32_to_cpu(agf->agf_flfirst) == xfs_agfl_size(mp))\n\t\tagf->agf_flfirst = 0;\n\n\tpag = xfs_perag_get(mp, be32_to_cpu(agf->agf_seqno));\n\tASSERT(!pag->pagf_agflreset);\n\tbe32_add_cpu(&agf->agf_flcount, -1);\n\txfs_trans_agflist_delta(tp, -1);\n\tpag->pagf_flcount--;\n\n\tlogflags = XFS_AGF_FLFIRST | XFS_AGF_FLCOUNT;\n\tif (btreeblk) {\n\t\tbe32_add_cpu(&agf->agf_btreeblks, 1);\n\t\tpag->pagf_btreeblks++;\n\t\tlogflags |= XFS_AGF_BTREEBLKS;\n\t}\n\txfs_perag_put(pag);\n\n\txfs_alloc_log_agf(tp, agbp, logflags);\n\t*bnop = bno;\n\n\treturn 0;\n}\n\n/*\n * Log the given fields from the agf structure.\n */\nvoid\nxfs_alloc_log_agf(\n\txfs_trans_t\t*tp,\t/* transaction pointer */\n\txfs_buf_t\t*bp,\t/* buffer for a.g. freelist header */\n\tint\t\tfields)\t/* mask of fields to be logged (XFS_AGF_...) */\n{\n\tint\tfirst;\t\t/* first byte offset */\n\tint\tlast;\t\t/* last byte offset */\n\tstatic const short\toffsets[] = {\n\t\toffsetof(xfs_agf_t, agf_magicnum),\n\t\toffsetof(xfs_agf_t, agf_versionnum),\n\t\toffsetof(xfs_agf_t, agf_seqno),\n\t\toffsetof(xfs_agf_t, agf_length),\n\t\toffsetof(xfs_agf_t, agf_roots[0]),\n\t\toffsetof(xfs_agf_t, agf_levels[0]),\n\t\toffsetof(xfs_agf_t, agf_flfirst),\n\t\toffsetof(xfs_agf_t, agf_fllast),\n\t\toffsetof(xfs_agf_t, agf_flcount),\n\t\toffsetof(xfs_agf_t, agf_freeblks),\n\t\toffsetof(xfs_agf_t, agf_longest),\n\t\toffsetof(xfs_agf_t, agf_btreeblks),\n\t\toffsetof(xfs_agf_t, agf_uuid),\n\t\toffsetof(xfs_agf_t, agf_rmap_blocks),\n\t\toffsetof(xfs_agf_t, agf_refcount_blocks),\n\t\toffsetof(xfs_agf_t, agf_refcount_root),\n\t\toffsetof(xfs_agf_t, agf_refcount_level),\n\t\t/* needed so that we don't log the whole rest of the structure: */\n\t\toffsetof(xfs_agf_t, agf_spare64),\n\t\tsizeof(xfs_agf_t)\n\t};\n\n\ttrace_xfs_agf(tp->t_mountp, XFS_BUF_TO_AGF(bp), fields, _RET_IP_);\n\n\txfs_trans_buf_set_type(tp, bp, XFS_BLFT_AGF_BUF);\n\n\txfs_btree_offsets(fields, offsets, XFS_AGF_NUM_BITS, &first, &last);\n\txfs_trans_log_buf(tp, bp, (uint)first, (uint)last);\n}\n\n/*\n * Interface for inode allocation to force the pag data to be initialized.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_pagf_init(\n\txfs_mount_t\t\t*mp,\t/* file system mount structure */\n\txfs_trans_t\t\t*tp,\t/* transaction pointer */\n\txfs_agnumber_t\t\tagno,\t/* allocation group number */\n\tint\t\t\tflags)\t/* XFS_ALLOC_FLAGS_... */\n{\n\txfs_buf_t\t\t*bp;\n\tint\t\t\terror;\n\n\terror = xfs_alloc_read_agf(mp, tp, agno, flags, &bp);\n\tif (!error)\n\t\txfs_trans_brelse(tp, bp);\n\treturn error;\n}\n\n/*\n * Put the block on the freelist for the allocation group.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_put_freelist(\n\txfs_trans_t\t\t*tp,\t/* transaction pointer */\n\txfs_buf_t\t\t*agbp,\t/* buffer for a.g. freelist header */\n\txfs_buf_t\t\t*agflbp,/* buffer for a.g. free block array */\n\txfs_agblock_t\t\tbno,\t/* block being freed */\n\tint\t\t\tbtreeblk) /* block came from a AGF btree */\n{\n\txfs_agf_t\t\t*agf;\t/* a.g. freespace structure */\n\t__be32\t\t\t*blockp;/* pointer to array entry */\n\tint\t\t\terror;\n\tint\t\t\tlogflags;\n\txfs_mount_t\t\t*mp;\t/* mount structure */\n\txfs_perag_t\t\t*pag;\t/* per allocation group data */\n\t__be32\t\t\t*agfl_bno;\n\tint\t\t\tstartoff;\n\n\tagf = XFS_BUF_TO_AGF(agbp);\n\tmp = tp->t_mountp;\n\n\tif (!agflbp && (error = xfs_alloc_read_agfl(mp, tp,\n\t\t\tbe32_to_cpu(agf->agf_seqno), &agflbp)))\n\t\treturn error;\n\tbe32_add_cpu(&agf->agf_fllast, 1);\n\tif (be32_to_cpu(agf->agf_fllast) == xfs_agfl_size(mp))\n\t\tagf->agf_fllast = 0;\n\n\tpag = xfs_perag_get(mp, be32_to_cpu(agf->agf_seqno));\n\tASSERT(!pag->pagf_agflreset);\n\tbe32_add_cpu(&agf->agf_flcount, 1);\n\txfs_trans_agflist_delta(tp, 1);\n\tpag->pagf_flcount++;\n\n\tlogflags = XFS_AGF_FLLAST | XFS_AGF_FLCOUNT;\n\tif (btreeblk) {\n\t\tbe32_add_cpu(&agf->agf_btreeblks, -1);\n\t\tpag->pagf_btreeblks--;\n\t\tlogflags |= XFS_AGF_BTREEBLKS;\n\t}\n\txfs_perag_put(pag);\n\n\txfs_alloc_log_agf(tp, agbp, logflags);\n\n\tASSERT(be32_to_cpu(agf->agf_flcount) <= xfs_agfl_size(mp));\n\n\tagfl_bno = XFS_BUF_TO_AGFL_BNO(mp, agflbp);\n\tblockp = &agfl_bno[be32_to_cpu(agf->agf_fllast)];\n\t*blockp = cpu_to_be32(bno);\n\tstartoff = (char *)blockp - (char *)agflbp->b_addr;\n\n\txfs_alloc_log_agf(tp, agbp, logflags);\n\n\txfs_trans_buf_set_type(tp, agflbp, XFS_BLFT_AGFL_BUF);\n\txfs_trans_log_buf(tp, agflbp, startoff,\n\t\t\t  startoff + sizeof(xfs_agblock_t) - 1);\n\treturn 0;\n}\n\nstatic xfs_failaddr_t\nxfs_agf_verify(\n\tstruct xfs_buf\t\t*bp)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tstruct xfs_agf\t\t*agf = XFS_BUF_TO_AGF(bp);\n\n\tif (xfs_sb_version_hascrc(&mp->m_sb)) {\n\t\tif (!uuid_equal(&agf->agf_uuid, &mp->m_sb.sb_meta_uuid))\n\t\t\treturn __this_address;\n\t\tif (!xfs_log_check_lsn(mp,\n\t\t\t\tbe64_to_cpu(XFS_BUF_TO_AGF(bp)->agf_lsn)))\n\t\t\treturn __this_address;\n\t}\n\n\tif (!xfs_verify_magic(bp, agf->agf_magicnum))\n\t\treturn __this_address;\n\n\tif (!(XFS_AGF_GOOD_VERSION(be32_to_cpu(agf->agf_versionnum)) &&\n\t      be32_to_cpu(agf->agf_freeblks) <= be32_to_cpu(agf->agf_length) &&\n\t      be32_to_cpu(agf->agf_flfirst) < xfs_agfl_size(mp) &&\n\t      be32_to_cpu(agf->agf_fllast) < xfs_agfl_size(mp) &&\n\t      be32_to_cpu(agf->agf_flcount) <= xfs_agfl_size(mp)))\n\t\treturn __this_address;\n\n\tif (be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNO]) < 1 ||\n\t    be32_to_cpu(agf->agf_levels[XFS_BTNUM_CNT]) < 1 ||\n\t    be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNO]) > XFS_BTREE_MAXLEVELS ||\n\t    be32_to_cpu(agf->agf_levels[XFS_BTNUM_CNT]) > XFS_BTREE_MAXLEVELS)\n\t\treturn __this_address;\n\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb) &&\n\t    (be32_to_cpu(agf->agf_levels[XFS_BTNUM_RMAP]) < 1 ||\n\t     be32_to_cpu(agf->agf_levels[XFS_BTNUM_RMAP]) > XFS_BTREE_MAXLEVELS))\n\t\treturn __this_address;\n\n\t/*\n\t * during growfs operations, the perag is not fully initialised,\n\t * so we can't use it for any useful checking. growfs ensures we can't\n\t * use it by using uncached buffers that don't have the perag attached\n\t * so we can detect and avoid this problem.\n\t */\n\tif (bp->b_pag && be32_to_cpu(agf->agf_seqno) != bp->b_pag->pag_agno)\n\t\treturn __this_address;\n\n\tif (xfs_sb_version_haslazysbcount(&mp->m_sb) &&\n\t    be32_to_cpu(agf->agf_btreeblks) > be32_to_cpu(agf->agf_length))\n\t\treturn __this_address;\n\n\tif (xfs_sb_version_hasreflink(&mp->m_sb) &&\n\t    (be32_to_cpu(agf->agf_refcount_level) < 1 ||\n\t     be32_to_cpu(agf->agf_refcount_level) > XFS_BTREE_MAXLEVELS))\n\t\treturn __this_address;\n\n\treturn NULL;\n\n}\n\nstatic void\nxfs_agf_read_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount *mp = bp->b_mount;\n\txfs_failaddr_t\tfa;\n\n\tif (xfs_sb_version_hascrc(&mp->m_sb) &&\n\t    !xfs_buf_verify_cksum(bp, XFS_AGF_CRC_OFF))\n\t\txfs_verifier_error(bp, -EFSBADCRC, __this_address);\n\telse {\n\t\tfa = xfs_agf_verify(bp);\n\t\tif (XFS_TEST_ERROR(fa, mp, XFS_ERRTAG_ALLOC_READ_AGF))\n\t\t\txfs_verifier_error(bp, -EFSCORRUPTED, fa);\n\t}\n}\n\nstatic void\nxfs_agf_write_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tstruct xfs_buf_log_item\t*bip = bp->b_log_item;\n\txfs_failaddr_t\t\tfa;\n\n\tfa = xfs_agf_verify(bp);\n\tif (fa) {\n\t\txfs_verifier_error(bp, -EFSCORRUPTED, fa);\n\t\treturn;\n\t}\n\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn;\n\n\tif (bip)\n\t\tXFS_BUF_TO_AGF(bp)->agf_lsn = cpu_to_be64(bip->bli_item.li_lsn);\n\n\txfs_buf_update_cksum(bp, XFS_AGF_CRC_OFF);\n}\n\nconst struct xfs_buf_ops xfs_agf_buf_ops = {\n\t.name = \"xfs_agf\",\n\t.magic = { cpu_to_be32(XFS_AGF_MAGIC), cpu_to_be32(XFS_AGF_MAGIC) },\n\t.verify_read = xfs_agf_read_verify,\n\t.verify_write = xfs_agf_write_verify,\n\t.verify_struct = xfs_agf_verify,\n};\n\n/*\n * Read in the allocation group header (free/alloc section).\n */\nint\t\t\t\t\t/* error */\nxfs_read_agf(\n\tstruct xfs_mount\t*mp,\t/* mount point structure */\n\tstruct xfs_trans\t*tp,\t/* transaction pointer */\n\txfs_agnumber_t\t\tagno,\t/* allocation group number */\n\tint\t\t\tflags,\t/* XFS_BUF_ */\n\tstruct xfs_buf\t\t**bpp)\t/* buffer for the ag freelist header */\n{\n\tint\t\terror;\n\n\ttrace_xfs_read_agf(mp, agno);\n\n\tASSERT(agno != NULLAGNUMBER);\n\terror = xfs_trans_read_buf(mp, tp, mp->m_ddev_targp,\n\t\t\tXFS_AG_DADDR(mp, agno, XFS_AGF_DADDR(mp)),\n\t\t\tXFS_FSS_TO_BB(mp, 1), flags, bpp, &xfs_agf_buf_ops);\n\tif (error)\n\t\treturn error;\n\n\tASSERT(!(*bpp)->b_error);\n\txfs_buf_set_ref(*bpp, XFS_AGF_REF);\n\treturn 0;\n}\n\n/*\n * Read in the allocation group header (free/alloc section).\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_read_agf(\n\tstruct xfs_mount\t*mp,\t/* mount point structure */\n\tstruct xfs_trans\t*tp,\t/* transaction pointer */\n\txfs_agnumber_t\t\tagno,\t/* allocation group number */\n\tint\t\t\tflags,\t/* XFS_ALLOC_FLAG_... */\n\tstruct xfs_buf\t\t**bpp)\t/* buffer for the ag freelist header */\n{\n\tstruct xfs_agf\t\t*agf;\t\t/* ag freelist header */\n\tstruct xfs_perag\t*pag;\t\t/* per allocation group data */\n\tint\t\t\terror;\n\n\ttrace_xfs_alloc_read_agf(mp, agno);\n\n\t/* We don't support trylock when freeing. */\n\tASSERT((flags & (XFS_ALLOC_FLAG_FREEING | XFS_ALLOC_FLAG_TRYLOCK)) !=\n\t\t\t(XFS_ALLOC_FLAG_FREEING | XFS_ALLOC_FLAG_TRYLOCK));\n\tASSERT(agno != NULLAGNUMBER);\n\terror = xfs_read_agf(mp, tp, agno,\n\t\t\t(flags & XFS_ALLOC_FLAG_TRYLOCK) ? XBF_TRYLOCK : 0,\n\t\t\tbpp);\n\tif (error)\n\t\treturn error;\n\tASSERT(!(*bpp)->b_error);\n\n\tagf = XFS_BUF_TO_AGF(*bpp);\n\tpag = xfs_perag_get(mp, agno);\n\tif (!pag->pagf_init) {\n\t\tpag->pagf_freeblks = be32_to_cpu(agf->agf_freeblks);\n\t\tpag->pagf_btreeblks = be32_to_cpu(agf->agf_btreeblks);\n\t\tpag->pagf_flcount = be32_to_cpu(agf->agf_flcount);\n\t\tpag->pagf_longest = be32_to_cpu(agf->agf_longest);\n\t\tpag->pagf_levels[XFS_BTNUM_BNOi] =\n\t\t\tbe32_to_cpu(agf->agf_levels[XFS_BTNUM_BNOi]);\n\t\tpag->pagf_levels[XFS_BTNUM_CNTi] =\n\t\t\tbe32_to_cpu(agf->agf_levels[XFS_BTNUM_CNTi]);\n\t\tpag->pagf_levels[XFS_BTNUM_RMAPi] =\n\t\t\tbe32_to_cpu(agf->agf_levels[XFS_BTNUM_RMAPi]);\n\t\tpag->pagf_refcount_level = be32_to_cpu(agf->agf_refcount_level);\n\t\tpag->pagf_init = 1;\n\t\tpag->pagf_agflreset = xfs_agfl_needs_reset(mp, agf);\n\t}\n#ifdef DEBUG\n\telse if (!XFS_FORCED_SHUTDOWN(mp)) {\n\t\tASSERT(pag->pagf_freeblks == be32_to_cpu(agf->agf_freeblks));\n\t\tASSERT(pag->pagf_btreeblks == be32_to_cpu(agf->agf_btreeblks));\n\t\tASSERT(pag->pagf_flcount == be32_to_cpu(agf->agf_flcount));\n\t\tASSERT(pag->pagf_longest == be32_to_cpu(agf->agf_longest));\n\t\tASSERT(pag->pagf_levels[XFS_BTNUM_BNOi] ==\n\t\t       be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNOi]));\n\t\tASSERT(pag->pagf_levels[XFS_BTNUM_CNTi] ==\n\t\t       be32_to_cpu(agf->agf_levels[XFS_BTNUM_CNTi]));\n\t}\n#endif\n\txfs_perag_put(pag);\n\treturn 0;\n}\n\n/*\n * Allocate an extent (variable-size).\n * Depending on the allocation type, we either look in a single allocation\n * group or loop over the allocation groups to find the result.\n */\nint\t\t\t\t/* error */\nxfs_alloc_vextent(\n\tstruct xfs_alloc_arg\t*args)\t/* allocation argument structure */\n{\n\txfs_agblock_t\t\tagsize;\t/* allocation group size */\n\tint\t\t\terror;\n\tint\t\t\tflags;\t/* XFS_ALLOC_FLAG_... locking flags */\n\tstruct xfs_mount\t*mp;\t/* mount structure pointer */\n\txfs_agnumber_t\t\tsagno;\t/* starting allocation group number */\n\txfs_alloctype_t\t\ttype;\t/* input allocation type */\n\tint\t\t\tbump_rotor = 0;\n\txfs_agnumber_t\t\trotorstep = xfs_rotorstep; /* inode32 agf stepper */\n\n\tmp = args->mp;\n\ttype = args->otype = args->type;\n\targs->agbno = NULLAGBLOCK;\n\t/*\n\t * Just fix this up, for the case where the last a.g. is shorter\n\t * (or there's only one a.g.) and the caller couldn't easily figure\n\t * that out (xfs_bmap_alloc).\n\t */\n\tagsize = mp->m_sb.sb_agblocks;\n\tif (args->maxlen > agsize)\n\t\targs->maxlen = agsize;\n\tif (args->alignment == 0)\n\t\targs->alignment = 1;\n\tASSERT(XFS_FSB_TO_AGNO(mp, args->fsbno) < mp->m_sb.sb_agcount);\n\tASSERT(XFS_FSB_TO_AGBNO(mp, args->fsbno) < agsize);\n\tASSERT(args->minlen <= args->maxlen);\n\tASSERT(args->minlen <= agsize);\n\tASSERT(args->mod < args->prod);\n\tif (XFS_FSB_TO_AGNO(mp, args->fsbno) >= mp->m_sb.sb_agcount ||\n\t    XFS_FSB_TO_AGBNO(mp, args->fsbno) >= agsize ||\n\t    args->minlen > args->maxlen || args->minlen > agsize ||\n\t    args->mod >= args->prod) {\n\t\targs->fsbno = NULLFSBLOCK;\n\t\ttrace_xfs_alloc_vextent_badargs(args);\n\t\treturn 0;\n\t}\n\n\tswitch (type) {\n\tcase XFS_ALLOCTYPE_THIS_AG:\n\tcase XFS_ALLOCTYPE_NEAR_BNO:\n\tcase XFS_ALLOCTYPE_THIS_BNO:\n\t\t/*\n\t\t * These three force us into a single a.g.\n\t\t */\n\t\targs->agno = XFS_FSB_TO_AGNO(mp, args->fsbno);\n\t\targs->pag = xfs_perag_get(mp, args->agno);\n\t\terror = xfs_alloc_fix_freelist(args, 0);\n\t\tif (error) {\n\t\t\ttrace_xfs_alloc_vextent_nofix(args);\n\t\t\tgoto error0;\n\t\t}\n\t\tif (!args->agbp) {\n\t\t\ttrace_xfs_alloc_vextent_noagbp(args);\n\t\t\tbreak;\n\t\t}\n\t\targs->agbno = XFS_FSB_TO_AGBNO(mp, args->fsbno);\n\t\tif ((error = xfs_alloc_ag_vextent(args)))\n\t\t\tgoto error0;\n\t\tbreak;\n\tcase XFS_ALLOCTYPE_START_BNO:\n\t\t/*\n\t\t * Try near allocation first, then anywhere-in-ag after\n\t\t * the first a.g. fails.\n\t\t */\n\t\tif ((args->datatype & XFS_ALLOC_INITIAL_USER_DATA) &&\n\t\t    (mp->m_flags & XFS_MOUNT_32BITINODES)) {\n\t\t\targs->fsbno = XFS_AGB_TO_FSB(mp,\n\t\t\t\t\t((mp->m_agfrotor / rotorstep) %\n\t\t\t\t\tmp->m_sb.sb_agcount), 0);\n\t\t\tbump_rotor = 1;\n\t\t}\n\t\targs->agbno = XFS_FSB_TO_AGBNO(mp, args->fsbno);\n\t\targs->type = XFS_ALLOCTYPE_NEAR_BNO;\n\t\t/* FALLTHROUGH */\n\tcase XFS_ALLOCTYPE_FIRST_AG:\n\t\t/*\n\t\t * Rotate through the allocation groups looking for a winner.\n\t\t */\n\t\tif (type == XFS_ALLOCTYPE_FIRST_AG) {\n\t\t\t/*\n\t\t\t * Start with allocation group given by bno.\n\t\t\t */\n\t\t\targs->agno = XFS_FSB_TO_AGNO(mp, args->fsbno);\n\t\t\targs->type = XFS_ALLOCTYPE_THIS_AG;\n\t\t\tsagno = 0;\n\t\t\tflags = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Start with the given allocation group.\n\t\t\t */\n\t\t\targs->agno = sagno = XFS_FSB_TO_AGNO(mp, args->fsbno);\n\t\t\tflags = XFS_ALLOC_FLAG_TRYLOCK;\n\t\t}\n\t\t/*\n\t\t * Loop over allocation groups twice; first time with\n\t\t * trylock set, second time without.\n\t\t */\n\t\tfor (;;) {\n\t\t\targs->pag = xfs_perag_get(mp, args->agno);\n\t\t\terror = xfs_alloc_fix_freelist(args, flags);\n\t\t\tif (error) {\n\t\t\t\ttrace_xfs_alloc_vextent_nofix(args);\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t\t/*\n\t\t\t * If we get a buffer back then the allocation will fly.\n\t\t\t */\n\t\t\tif (args->agbp) {\n\t\t\t\tif ((error = xfs_alloc_ag_vextent(args)))\n\t\t\t\t\tgoto error0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\ttrace_xfs_alloc_vextent_loopfailed(args);\n\n\t\t\t/*\n\t\t\t * Didn't work, figure out the next iteration.\n\t\t\t */\n\t\t\tif (args->agno == sagno &&\n\t\t\t    type == XFS_ALLOCTYPE_START_BNO)\n\t\t\t\targs->type = XFS_ALLOCTYPE_THIS_AG;\n\t\t\t/*\n\t\t\t* For the first allocation, we can try any AG to get\n\t\t\t* space.  However, if we already have allocated a\n\t\t\t* block, we don't want to try AGs whose number is below\n\t\t\t* sagno. Otherwise, we may end up with out-of-order\n\t\t\t* locking of AGF, which might cause deadlock.\n\t\t\t*/\n\t\t\tif (++(args->agno) == mp->m_sb.sb_agcount) {\n\t\t\t\tif (args->tp->t_firstblock != NULLFSBLOCK)\n\t\t\t\t\targs->agno = sagno;\n\t\t\t\telse\n\t\t\t\t\targs->agno = 0;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Reached the starting a.g., must either be done\n\t\t\t * or switch to non-trylock mode.\n\t\t\t */\n\t\t\tif (args->agno == sagno) {\n\t\t\t\tif (flags == 0) {\n\t\t\t\t\targs->agbno = NULLAGBLOCK;\n\t\t\t\t\ttrace_xfs_alloc_vextent_allfailed(args);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tflags = 0;\n\t\t\t\tif (type == XFS_ALLOCTYPE_START_BNO) {\n\t\t\t\t\targs->agbno = XFS_FSB_TO_AGBNO(mp,\n\t\t\t\t\t\targs->fsbno);\n\t\t\t\t\targs->type = XFS_ALLOCTYPE_NEAR_BNO;\n\t\t\t\t}\n\t\t\t}\n\t\t\txfs_perag_put(args->pag);\n\t\t}\n\t\tif (bump_rotor) {\n\t\t\tif (args->agno == sagno)\n\t\t\t\tmp->m_agfrotor = (mp->m_agfrotor + 1) %\n\t\t\t\t\t(mp->m_sb.sb_agcount * rotorstep);\n\t\t\telse\n\t\t\t\tmp->m_agfrotor = (args->agno * rotorstep + 1) %\n\t\t\t\t\t(mp->m_sb.sb_agcount * rotorstep);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tASSERT(0);\n\t\t/* NOTREACHED */\n\t}\n\tif (args->agbno == NULLAGBLOCK)\n\t\targs->fsbno = NULLFSBLOCK;\n\telse {\n\t\targs->fsbno = XFS_AGB_TO_FSB(mp, args->agno, args->agbno);\n#ifdef DEBUG\n\t\tASSERT(args->len >= args->minlen);\n\t\tASSERT(args->len <= args->maxlen);\n\t\tASSERT(args->agbno % args->alignment == 0);\n\t\tXFS_AG_CHECK_DADDR(mp, XFS_FSB_TO_DADDR(mp, args->fsbno),\n\t\t\targs->len);\n#endif\n\n\t}\n\txfs_perag_put(args->pag);\n\treturn 0;\nerror0:\n\txfs_perag_put(args->pag);\n\treturn error;\n}\n\n/* Ensure that the freelist is at full capacity. */\nint\nxfs_free_extent_fix_freelist(\n\tstruct xfs_trans\t*tp,\n\txfs_agnumber_t\t\tagno,\n\tstruct xfs_buf\t\t**agbp)\n{\n\tstruct xfs_alloc_arg\targs;\n\tint\t\t\terror;\n\n\tmemset(&args, 0, sizeof(struct xfs_alloc_arg));\n\targs.tp = tp;\n\targs.mp = tp->t_mountp;\n\targs.agno = agno;\n\n\t/*\n\t * validate that the block number is legal - the enables us to detect\n\t * and handle a silent filesystem corruption rather than crashing.\n\t */\n\tif (args.agno >= args.mp->m_sb.sb_agcount)\n\t\treturn -EFSCORRUPTED;\n\n\targs.pag = xfs_perag_get(args.mp, args.agno);\n\tASSERT(args.pag);\n\n\terror = xfs_alloc_fix_freelist(&args, XFS_ALLOC_FLAG_FREEING);\n\tif (error)\n\t\tgoto out;\n\n\t*agbp = args.agbp;\nout:\n\txfs_perag_put(args.pag);\n\treturn error;\n}\n\n/*\n * Free an extent.\n * Just break up the extent address and hand off to xfs_free_ag_extent\n * after fixing up the freelist.\n */\nint\n__xfs_free_extent(\n\tstruct xfs_trans\t\t*tp,\n\txfs_fsblock_t\t\t\tbno,\n\txfs_extlen_t\t\t\tlen,\n\tconst struct xfs_owner_info\t*oinfo,\n\tenum xfs_ag_resv_type\t\ttype,\n\tbool\t\t\t\tskip_discard)\n{\n\tstruct xfs_mount\t\t*mp = tp->t_mountp;\n\tstruct xfs_buf\t\t\t*agbp;\n\txfs_agnumber_t\t\t\tagno = XFS_FSB_TO_AGNO(mp, bno);\n\txfs_agblock_t\t\t\tagbno = XFS_FSB_TO_AGBNO(mp, bno);\n\tint\t\t\t\terror;\n\tunsigned int\t\t\tbusy_flags = 0;\n\n\tASSERT(len != 0);\n\tASSERT(type != XFS_AG_RESV_AGFL);\n\n\tif (XFS_TEST_ERROR(false, mp,\n\t\t\tXFS_ERRTAG_FREE_EXTENT))\n\t\treturn -EIO;\n\n\terror = xfs_free_extent_fix_freelist(tp, agno, &agbp);\n\tif (error)\n\t\treturn error;\n\n\tif (XFS_IS_CORRUPT(mp, agbno >= mp->m_sb.sb_agblocks)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto err;\n\t}\n\n\t/* validate the extent size is legal now we have the agf locked */\n\tif (XFS_IS_CORRUPT(mp,\n\t\t\t   agbno + len >\n\t\t\t   be32_to_cpu(XFS_BUF_TO_AGF(agbp)->agf_length))) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto err;\n\t}\n\n\terror = xfs_free_ag_extent(tp, agbp, agno, agbno, len, oinfo, type);\n\tif (error)\n\t\tgoto err;\n\n\tif (skip_discard)\n\t\tbusy_flags |= XFS_EXTENT_BUSY_SKIP_DISCARD;\n\txfs_extent_busy_insert(tp, agno, agbno, len, busy_flags);\n\treturn 0;\n\nerr:\n\txfs_trans_brelse(tp, agbp);\n\treturn error;\n}\n\nstruct xfs_alloc_query_range_info {\n\txfs_alloc_query_range_fn\tfn;\n\tvoid\t\t\t\t*priv;\n};\n\n/* Format btree record and pass to our callback. */\nSTATIC int\nxfs_alloc_query_range_helper(\n\tstruct xfs_btree_cur\t\t*cur,\n\tunion xfs_btree_rec\t\t*rec,\n\tvoid\t\t\t\t*priv)\n{\n\tstruct xfs_alloc_query_range_info\t*query = priv;\n\tstruct xfs_alloc_rec_incore\t\tirec;\n\n\tirec.ar_startblock = be32_to_cpu(rec->alloc.ar_startblock);\n\tirec.ar_blockcount = be32_to_cpu(rec->alloc.ar_blockcount);\n\treturn query->fn(cur, &irec, query->priv);\n}\n\n/* Find all free space within a given range of blocks. */\nint\nxfs_alloc_query_range(\n\tstruct xfs_btree_cur\t\t\t*cur,\n\tstruct xfs_alloc_rec_incore\t\t*low_rec,\n\tstruct xfs_alloc_rec_incore\t\t*high_rec,\n\txfs_alloc_query_range_fn\t\tfn,\n\tvoid\t\t\t\t\t*priv)\n{\n\tunion xfs_btree_irec\t\t\tlow_brec;\n\tunion xfs_btree_irec\t\t\thigh_brec;\n\tstruct xfs_alloc_query_range_info\tquery;\n\n\tASSERT(cur->bc_btnum == XFS_BTNUM_BNO);\n\tlow_brec.a = *low_rec;\n\thigh_brec.a = *high_rec;\n\tquery.priv = priv;\n\tquery.fn = fn;\n\treturn xfs_btree_query_range(cur, &low_brec, &high_brec,\n\t\t\txfs_alloc_query_range_helper, &query);\n}\n\n/* Find all free space records. */\nint\nxfs_alloc_query_all(\n\tstruct xfs_btree_cur\t\t\t*cur,\n\txfs_alloc_query_range_fn\t\tfn,\n\tvoid\t\t\t\t\t*priv)\n{\n\tstruct xfs_alloc_query_range_info\tquery;\n\n\tASSERT(cur->bc_btnum == XFS_BTNUM_BNO);\n\tquery.priv = priv;\n\tquery.fn = fn;\n\treturn xfs_btree_query_all(cur, xfs_alloc_query_range_helper, &query);\n}\n\n/* Is there a record covering a given extent? */\nint\nxfs_alloc_has_record(\n\tstruct xfs_btree_cur\t*cur,\n\txfs_agblock_t\t\tbno,\n\txfs_extlen_t\t\tlen,\n\tbool\t\t\t*exists)\n{\n\tunion xfs_btree_irec\tlow;\n\tunion xfs_btree_irec\thigh;\n\n\tmemset(&low, 0, sizeof(low));\n\tlow.a.ar_startblock = bno;\n\tmemset(&high, 0xFF, sizeof(high));\n\thigh.a.ar_startblock = bno + len - 1;\n\n\treturn xfs_btree_has_record(cur, &low, &high, exists);\n}\n\n/*\n * Walk all the blocks in the AGFL.  The @walk_fn can return any negative\n * error code or XFS_ITER_*.\n */\nint\nxfs_agfl_walk(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_agf\t\t*agf,\n\tstruct xfs_buf\t\t*agflbp,\n\txfs_agfl_walk_fn\twalk_fn,\n\tvoid\t\t\t*priv)\n{\n\t__be32\t\t\t*agfl_bno;\n\tunsigned int\t\ti;\n\tint\t\t\terror;\n\n\tagfl_bno = XFS_BUF_TO_AGFL_BNO(mp, agflbp);\n\ti = be32_to_cpu(agf->agf_flfirst);\n\n\t/* Nothing to walk in an empty AGFL. */\n\tif (agf->agf_flcount == cpu_to_be32(0))\n\t\treturn 0;\n\n\t/* Otherwise, walk from first to last, wrapping as needed. */\n\tfor (;;) {\n\t\terror = walk_fn(mp, be32_to_cpu(agfl_bno[i]), priv);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == be32_to_cpu(agf->agf_fllast))\n\t\t\tbreak;\n\t\tif (++i == xfs_agfl_size(mp))\n\t\t\ti = 0;\n\t}\n\n\treturn 0;\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.\n * All Rights Reserved.\n */\n#include \"xfs.h\"\n#include \"xfs_fs.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_shared.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_bit.h\"\n#include \"xfs_sb.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_defer.h\"\n#include \"xfs_btree.h\"\n#include \"xfs_rmap.h\"\n#include \"xfs_alloc_btree.h\"\n#include \"xfs_alloc.h\"\n#include \"xfs_extent_busy.h\"\n#include \"xfs_errortag.h\"\n#include \"xfs_error.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_buf_item.h\"\n#include \"xfs_log.h\"\n#include \"xfs_ag_resv.h\"\n#include \"xfs_bmap.h\"\n\nextern kmem_zone_t\t*xfs_bmap_free_item_zone;\n\nstruct workqueue_struct *xfs_alloc_wq;\n\n#define XFS_ABSDIFF(a,b)\t(((a) <= (b)) ? ((b) - (a)) : ((a) - (b)))\n\n#define\tXFSA_FIXUP_BNO_OK\t1\n#define\tXFSA_FIXUP_CNT_OK\t2\n\nSTATIC int xfs_alloc_ag_vextent_exact(xfs_alloc_arg_t *);\nSTATIC int xfs_alloc_ag_vextent_near(xfs_alloc_arg_t *);\nSTATIC int xfs_alloc_ag_vextent_size(xfs_alloc_arg_t *);\n\n/*\n * Size of the AGFL.  For CRC-enabled filesystes we steal a couple of slots in\n * the beginning of the block for a proper header with the location information\n * and CRC.\n */\nunsigned int\nxfs_agfl_size(\n\tstruct xfs_mount\t*mp)\n{\n\tunsigned int\t\tsize = mp->m_sb.sb_sectsize;\n\n\tif (xfs_sb_version_hascrc(&mp->m_sb))\n\t\tsize -= sizeof(struct xfs_agfl);\n\n\treturn size / sizeof(xfs_agblock_t);\n}\n\nunsigned int\nxfs_refc_block(\n\tstruct xfs_mount\t*mp)\n{\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb))\n\t\treturn XFS_RMAP_BLOCK(mp) + 1;\n\tif (xfs_sb_version_hasfinobt(&mp->m_sb))\n\t\treturn XFS_FIBT_BLOCK(mp) + 1;\n\treturn XFS_IBT_BLOCK(mp) + 1;\n}\n\nxfs_extlen_t\nxfs_prealloc_blocks(\n\tstruct xfs_mount\t*mp)\n{\n\tif (xfs_sb_version_hasreflink(&mp->m_sb))\n\t\treturn xfs_refc_block(mp) + 1;\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb))\n\t\treturn XFS_RMAP_BLOCK(mp) + 1;\n\tif (xfs_sb_version_hasfinobt(&mp->m_sb))\n\t\treturn XFS_FIBT_BLOCK(mp) + 1;\n\treturn XFS_IBT_BLOCK(mp) + 1;\n}\n\n/*\n * In order to avoid ENOSPC-related deadlock caused by out-of-order locking of\n * AGF buffer (PV 947395), we place constraints on the relationship among\n * actual allocations for data blocks, freelist blocks, and potential file data\n * bmap btree blocks. However, these restrictions may result in no actual space\n * allocated for a delayed extent, for example, a data block in a certain AG is\n * allocated but there is no additional block for the additional bmap btree\n * block due to a split of the bmap btree of the file. The result of this may\n * lead to an infinite loop when the file gets flushed to disk and all delayed\n * extents need to be actually allocated. To get around this, we explicitly set\n * aside a few blocks which will not be reserved in delayed allocation.\n *\n * We need to reserve 4 fsbs _per AG_ for the freelist and 4 more to handle a\n * potential split of the file's bmap btree.\n */\nunsigned int\nxfs_alloc_set_aside(\n\tstruct xfs_mount\t*mp)\n{\n\treturn mp->m_sb.sb_agcount * (XFS_ALLOC_AGFL_RESERVE + 4);\n}\n\n/*\n * When deciding how much space to allocate out of an AG, we limit the\n * allocation maximum size to the size the AG. However, we cannot use all the\n * blocks in the AG - some are permanently used by metadata. These\n * blocks are generally:\n *\t- the AG superblock, AGF, AGI and AGFL\n *\t- the AGF (bno and cnt) and AGI btree root blocks, and optionally\n *\t  the AGI free inode and rmap btree root blocks.\n *\t- blocks on the AGFL according to xfs_alloc_set_aside() limits\n *\t- the rmapbt root block\n *\n * The AG headers are sector sized, so the amount of space they take up is\n * dependent on filesystem geometry. The others are all single blocks.\n */\nunsigned int\nxfs_alloc_ag_max_usable(\n\tstruct xfs_mount\t*mp)\n{\n\tunsigned int\t\tblocks;\n\n\tblocks = XFS_BB_TO_FSB(mp, XFS_FSS_TO_BB(mp, 4)); /* ag headers */\n\tblocks += XFS_ALLOC_AGFL_RESERVE;\n\tblocks += 3;\t\t\t/* AGF, AGI btree root blocks */\n\tif (xfs_sb_version_hasfinobt(&mp->m_sb))\n\t\tblocks++;\t\t/* finobt root block */\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb))\n\t\tblocks++; \t\t/* rmap root block */\n\tif (xfs_sb_version_hasreflink(&mp->m_sb))\n\t\tblocks++;\t\t/* refcount root block */\n\n\treturn mp->m_sb.sb_agblocks - blocks;\n}\n\n/*\n * Lookup the record equal to [bno, len] in the btree given by cur.\n */\nSTATIC int\t\t\t\t/* error */\nxfs_alloc_lookup_eq(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\tbno,\t/* starting block of extent */\n\txfs_extlen_t\t\tlen,\t/* length of extent */\n\tint\t\t\t*stat)\t/* success/failure */\n{\n\tint\t\t\terror;\n\n\tcur->bc_rec.a.ar_startblock = bno;\n\tcur->bc_rec.a.ar_blockcount = len;\n\terror = xfs_btree_lookup(cur, XFS_LOOKUP_EQ, stat);\n\tcur->bc_private.a.priv.abt.active = (*stat == 1);\n\treturn error;\n}\n\n/*\n * Lookup the first record greater than or equal to [bno, len]\n * in the btree given by cur.\n */\nint\t\t\t\t/* error */\nxfs_alloc_lookup_ge(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\tbno,\t/* starting block of extent */\n\txfs_extlen_t\t\tlen,\t/* length of extent */\n\tint\t\t\t*stat)\t/* success/failure */\n{\n\tint\t\t\terror;\n\n\tcur->bc_rec.a.ar_startblock = bno;\n\tcur->bc_rec.a.ar_blockcount = len;\n\terror = xfs_btree_lookup(cur, XFS_LOOKUP_GE, stat);\n\tcur->bc_private.a.priv.abt.active = (*stat == 1);\n\treturn error;\n}\n\n/*\n * Lookup the first record less than or equal to [bno, len]\n * in the btree given by cur.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_lookup_le(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\tbno,\t/* starting block of extent */\n\txfs_extlen_t\t\tlen,\t/* length of extent */\n\tint\t\t\t*stat)\t/* success/failure */\n{\n\tint\t\t\terror;\n\tcur->bc_rec.a.ar_startblock = bno;\n\tcur->bc_rec.a.ar_blockcount = len;\n\terror = xfs_btree_lookup(cur, XFS_LOOKUP_LE, stat);\n\tcur->bc_private.a.priv.abt.active = (*stat == 1);\n\treturn error;\n}\n\nstatic inline bool\nxfs_alloc_cur_active(\n\tstruct xfs_btree_cur\t*cur)\n{\n\treturn cur && cur->bc_private.a.priv.abt.active;\n}\n\n/*\n * Update the record referred to by cur to the value given\n * by [bno, len].\n * This either works (return 0) or gets an EFSCORRUPTED error.\n */\nSTATIC int\t\t\t\t/* error */\nxfs_alloc_update(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\tbno,\t/* starting block of extent */\n\txfs_extlen_t\t\tlen)\t/* length of extent */\n{\n\tunion xfs_btree_rec\trec;\n\n\trec.alloc.ar_startblock = cpu_to_be32(bno);\n\trec.alloc.ar_blockcount = cpu_to_be32(len);\n\treturn xfs_btree_update(cur, &rec);\n}\n\n/*\n * Get the data from the pointed-to record.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_get_rec(\n\tstruct xfs_btree_cur\t*cur,\t/* btree cursor */\n\txfs_agblock_t\t\t*bno,\t/* output: starting block of extent */\n\txfs_extlen_t\t\t*len,\t/* output: length of extent */\n\tint\t\t\t*stat)\t/* output: success/failure */\n{\n\tstruct xfs_mount\t*mp = cur->bc_mp;\n\txfs_agnumber_t\t\tagno = cur->bc_private.a.agno;\n\tunion xfs_btree_rec\t*rec;\n\tint\t\t\terror;\n\n\terror = xfs_btree_get_rec(cur, &rec, stat);\n\tif (error || !(*stat))\n\t\treturn error;\n\n\t*bno = be32_to_cpu(rec->alloc.ar_startblock);\n\t*len = be32_to_cpu(rec->alloc.ar_blockcount);\n\n\tif (*len == 0)\n\t\tgoto out_bad_rec;\n\n\t/* check for valid extent range, including overflow */\n\tif (!xfs_verify_agbno(mp, agno, *bno))\n\t\tgoto out_bad_rec;\n\tif (*bno > *bno + *len)\n\t\tgoto out_bad_rec;\n\tif (!xfs_verify_agbno(mp, agno, *bno + *len - 1))\n\t\tgoto out_bad_rec;\n\n\treturn 0;\n\nout_bad_rec:\n\txfs_warn(mp,\n\t\t\"%s Freespace BTree record corruption in AG %d detected!\",\n\t\tcur->bc_btnum == XFS_BTNUM_BNO ? \"Block\" : \"Size\", agno);\n\txfs_warn(mp,\n\t\t\"start block 0x%x block count 0x%x\", *bno, *len);\n\treturn -EFSCORRUPTED;\n}\n\n/*\n * Compute aligned version of the found extent.\n * Takes alignment and min length into account.\n */\nSTATIC bool\nxfs_alloc_compute_aligned(\n\txfs_alloc_arg_t\t*args,\t\t/* allocation argument structure */\n\txfs_agblock_t\tfoundbno,\t/* starting block in found extent */\n\txfs_extlen_t\tfoundlen,\t/* length in found extent */\n\txfs_agblock_t\t*resbno,\t/* result block number */\n\txfs_extlen_t\t*reslen,\t/* result length */\n\tunsigned\t*busy_gen)\n{\n\txfs_agblock_t\tbno = foundbno;\n\txfs_extlen_t\tlen = foundlen;\n\txfs_extlen_t\tdiff;\n\tbool\t\tbusy;\n\n\t/* Trim busy sections out of found extent */\n\tbusy = xfs_extent_busy_trim(args, &bno, &len, busy_gen);\n\n\t/*\n\t * If we have a largish extent that happens to start before min_agbno,\n\t * see if we can shift it into range...\n\t */\n\tif (bno < args->min_agbno && bno + len > args->min_agbno) {\n\t\tdiff = args->min_agbno - bno;\n\t\tif (len > diff) {\n\t\t\tbno += diff;\n\t\t\tlen -= diff;\n\t\t}\n\t}\n\n\tif (args->alignment > 1 && len >= args->minlen) {\n\t\txfs_agblock_t\taligned_bno = roundup(bno, args->alignment);\n\n\t\tdiff = aligned_bno - bno;\n\n\t\t*resbno = aligned_bno;\n\t\t*reslen = diff >= len ? 0 : len - diff;\n\t} else {\n\t\t*resbno = bno;\n\t\t*reslen = len;\n\t}\n\n\treturn busy;\n}\n\n/*\n * Compute best start block and diff for \"near\" allocations.\n * freelen >= wantlen already checked by caller.\n */\nSTATIC xfs_extlen_t\t\t\t/* difference value (absolute) */\nxfs_alloc_compute_diff(\n\txfs_agblock_t\twantbno,\t/* target starting block */\n\txfs_extlen_t\twantlen,\t/* target length */\n\txfs_extlen_t\talignment,\t/* target alignment */\n\tint\t\tdatatype,\t/* are we allocating data? */\n\txfs_agblock_t\tfreebno,\t/* freespace's starting block */\n\txfs_extlen_t\tfreelen,\t/* freespace's length */\n\txfs_agblock_t\t*newbnop)\t/* result: best start block from free */\n{\n\txfs_agblock_t\tfreeend;\t/* end of freespace extent */\n\txfs_agblock_t\tnewbno1;\t/* return block number */\n\txfs_agblock_t\tnewbno2;\t/* other new block number */\n\txfs_extlen_t\tnewlen1=0;\t/* length with newbno1 */\n\txfs_extlen_t\tnewlen2=0;\t/* length with newbno2 */\n\txfs_agblock_t\twantend;\t/* end of target extent */\n\tbool\t\tuserdata = datatype & XFS_ALLOC_USERDATA;\n\n\tASSERT(freelen >= wantlen);\n\tfreeend = freebno + freelen;\n\twantend = wantbno + wantlen;\n\t/*\n\t * We want to allocate from the start of a free extent if it is past\n\t * the desired block or if we are allocating user data and the free\n\t * extent is before desired block. The second case is there to allow\n\t * for contiguous allocation from the remaining free space if the file\n\t * grows in the short term.\n\t */\n\tif (freebno >= wantbno || (userdata && freeend < wantend)) {\n\t\tif ((newbno1 = roundup(freebno, alignment)) >= freeend)\n\t\t\tnewbno1 = NULLAGBLOCK;\n\t} else if (freeend >= wantend && alignment > 1) {\n\t\tnewbno1 = roundup(wantbno, alignment);\n\t\tnewbno2 = newbno1 - alignment;\n\t\tif (newbno1 >= freeend)\n\t\t\tnewbno1 = NULLAGBLOCK;\n\t\telse\n\t\t\tnewlen1 = XFS_EXTLEN_MIN(wantlen, freeend - newbno1);\n\t\tif (newbno2 < freebno)\n\t\t\tnewbno2 = NULLAGBLOCK;\n\t\telse\n\t\t\tnewlen2 = XFS_EXTLEN_MIN(wantlen, freeend - newbno2);\n\t\tif (newbno1 != NULLAGBLOCK && newbno2 != NULLAGBLOCK) {\n\t\t\tif (newlen1 < newlen2 ||\n\t\t\t    (newlen1 == newlen2 &&\n\t\t\t     XFS_ABSDIFF(newbno1, wantbno) >\n\t\t\t     XFS_ABSDIFF(newbno2, wantbno)))\n\t\t\t\tnewbno1 = newbno2;\n\t\t} else if (newbno2 != NULLAGBLOCK)\n\t\t\tnewbno1 = newbno2;\n\t} else if (freeend >= wantend) {\n\t\tnewbno1 = wantbno;\n\t} else if (alignment > 1) {\n\t\tnewbno1 = roundup(freeend - wantlen, alignment);\n\t\tif (newbno1 > freeend - wantlen &&\n\t\t    newbno1 - alignment >= freebno)\n\t\t\tnewbno1 -= alignment;\n\t\telse if (newbno1 >= freeend)\n\t\t\tnewbno1 = NULLAGBLOCK;\n\t} else\n\t\tnewbno1 = freeend - wantlen;\n\t*newbnop = newbno1;\n\treturn newbno1 == NULLAGBLOCK ? 0 : XFS_ABSDIFF(newbno1, wantbno);\n}\n\n/*\n * Fix up the length, based on mod and prod.\n * len should be k * prod + mod for some k.\n * If len is too small it is returned unchanged.\n * If len hits maxlen it is left alone.\n */\nSTATIC void\nxfs_alloc_fix_len(\n\txfs_alloc_arg_t\t*args)\t\t/* allocation argument structure */\n{\n\txfs_extlen_t\tk;\n\txfs_extlen_t\trlen;\n\n\tASSERT(args->mod < args->prod);\n\trlen = args->len;\n\tASSERT(rlen >= args->minlen);\n\tASSERT(rlen <= args->maxlen);\n\tif (args->prod <= 1 || rlen < args->mod || rlen == args->maxlen ||\n\t    (args->mod == 0 && rlen < args->prod))\n\t\treturn;\n\tk = rlen % args->prod;\n\tif (k == args->mod)\n\t\treturn;\n\tif (k > args->mod)\n\t\trlen = rlen - (k - args->mod);\n\telse\n\t\trlen = rlen - args->prod + (args->mod - k);\n\t/* casts to (int) catch length underflows */\n\tif ((int)rlen < (int)args->minlen)\n\t\treturn;\n\tASSERT(rlen >= args->minlen && rlen <= args->maxlen);\n\tASSERT(rlen % args->prod == args->mod);\n\tASSERT(args->pag->pagf_freeblks + args->pag->pagf_flcount >=\n\t\trlen + args->minleft);\n\targs->len = rlen;\n}\n\n/*\n * Update the two btrees, logically removing from freespace the extent\n * starting at rbno, rlen blocks.  The extent is contained within the\n * actual (current) free extent fbno for flen blocks.\n * Flags are passed in indicating whether the cursors are set to the\n * relevant records.\n */\nSTATIC int\t\t\t\t/* error code */\nxfs_alloc_fixup_trees(\n\txfs_btree_cur_t\t*cnt_cur,\t/* cursor for by-size btree */\n\txfs_btree_cur_t\t*bno_cur,\t/* cursor for by-block btree */\n\txfs_agblock_t\tfbno,\t\t/* starting block of free extent */\n\txfs_extlen_t\tflen,\t\t/* length of free extent */\n\txfs_agblock_t\trbno,\t\t/* starting block of returned extent */\n\txfs_extlen_t\trlen,\t\t/* length of returned extent */\n\tint\t\tflags)\t\t/* flags, XFSA_FIXUP_... */\n{\n\tint\t\terror;\t\t/* error code */\n\tint\t\ti;\t\t/* operation results */\n\txfs_agblock_t\tnfbno1;\t\t/* first new free startblock */\n\txfs_agblock_t\tnfbno2;\t\t/* second new free startblock */\n\txfs_extlen_t\tnflen1=0;\t/* first new free length */\n\txfs_extlen_t\tnflen2=0;\t/* second new free length */\n\tstruct xfs_mount *mp;\n\n\tmp = cnt_cur->bc_mp;\n\n\t/*\n\t * Look up the record in the by-size tree if necessary.\n\t */\n\tif (flags & XFSA_FIXUP_CNT_OK) {\n#ifdef DEBUG\n\t\tif ((error = xfs_alloc_get_rec(cnt_cur, &nfbno1, &nflen1, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp,\n\t\t\t\t   i != 1 ||\n\t\t\t\t   nfbno1 != fbno ||\n\t\t\t\t   nflen1 != flen))\n\t\t\treturn -EFSCORRUPTED;\n#endif\n\t} else {\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, fbno, flen, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\t/*\n\t * Look up the record in the by-block tree if necessary.\n\t */\n\tif (flags & XFSA_FIXUP_BNO_OK) {\n#ifdef DEBUG\n\t\tif ((error = xfs_alloc_get_rec(bno_cur, &nfbno1, &nflen1, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp,\n\t\t\t\t   i != 1 ||\n\t\t\t\t   nfbno1 != fbno ||\n\t\t\t\t   nflen1 != flen))\n\t\t\treturn -EFSCORRUPTED;\n#endif\n\t} else {\n\t\tif ((error = xfs_alloc_lookup_eq(bno_cur, fbno, flen, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\n#ifdef DEBUG\n\tif (bno_cur->bc_nlevels == 1 && cnt_cur->bc_nlevels == 1) {\n\t\tstruct xfs_btree_block\t*bnoblock;\n\t\tstruct xfs_btree_block\t*cntblock;\n\n\t\tbnoblock = XFS_BUF_TO_BLOCK(bno_cur->bc_bufs[0]);\n\t\tcntblock = XFS_BUF_TO_BLOCK(cnt_cur->bc_bufs[0]);\n\n\t\tif (XFS_IS_CORRUPT(mp,\n\t\t\t\t   bnoblock->bb_numrecs !=\n\t\t\t\t   cntblock->bb_numrecs))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n#endif\n\n\t/*\n\t * Deal with all four cases: the allocated record is contained\n\t * within the freespace record, so we can have new freespace\n\t * at either (or both) end, or no freespace remaining.\n\t */\n\tif (rbno == fbno && rlen == flen)\n\t\tnfbno1 = nfbno2 = NULLAGBLOCK;\n\telse if (rbno == fbno) {\n\t\tnfbno1 = rbno + rlen;\n\t\tnflen1 = flen - rlen;\n\t\tnfbno2 = NULLAGBLOCK;\n\t} else if (rbno + rlen == fbno + flen) {\n\t\tnfbno1 = fbno;\n\t\tnflen1 = flen - rlen;\n\t\tnfbno2 = NULLAGBLOCK;\n\t} else {\n\t\tnfbno1 = fbno;\n\t\tnflen1 = rbno - fbno;\n\t\tnfbno2 = rbno + rlen;\n\t\tnflen2 = (fbno + flen) - nfbno2;\n\t}\n\t/*\n\t * Delete the entry from the by-size btree.\n\t */\n\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\treturn error;\n\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\treturn -EFSCORRUPTED;\n\t/*\n\t * Add new by-size btree entry(s).\n\t */\n\tif (nfbno1 != NULLAGBLOCK) {\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, nfbno1, nflen1, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 0))\n\t\t\treturn -EFSCORRUPTED;\n\t\tif ((error = xfs_btree_insert(cnt_cur, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\tif (nfbno2 != NULLAGBLOCK) {\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, nfbno2, nflen2, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 0))\n\t\t\treturn -EFSCORRUPTED;\n\t\tif ((error = xfs_btree_insert(cnt_cur, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\t/*\n\t * Fix up the by-block btree entry(s).\n\t */\n\tif (nfbno1 == NULLAGBLOCK) {\n\t\t/*\n\t\t * No remaining freespace, just delete the by-block tree entry.\n\t\t */\n\t\tif ((error = xfs_btree_delete(bno_cur, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t} else {\n\t\t/*\n\t\t * Update the by-block entry to start later|be shorter.\n\t\t */\n\t\tif ((error = xfs_alloc_update(bno_cur, nfbno1, nflen1)))\n\t\t\treturn error;\n\t}\n\tif (nfbno2 != NULLAGBLOCK) {\n\t\t/*\n\t\t * 2 resulting free entries, need to add one.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(bno_cur, nfbno2, nflen2, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 0))\n\t\t\treturn -EFSCORRUPTED;\n\t\tif ((error = xfs_btree_insert(bno_cur, &i)))\n\t\t\treturn error;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1))\n\t\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}\n\nstatic xfs_failaddr_t\nxfs_agfl_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount *mp = bp->b_mount;\n\tstruct xfs_agfl\t*agfl = XFS_BUF_TO_AGFL(bp);\n\tint\t\ti;\n\n\t/*\n\t * There is no verification of non-crc AGFLs because mkfs does not\n\t * initialise the AGFL to zero or NULL. Hence the only valid part of the\n\t * AGFL is what the AGF says is active. We can't get to the AGF, so we\n\t * can't verify just those entries are valid.\n\t */\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn NULL;\n\n\tif (!xfs_verify_magic(bp, agfl->agfl_magicnum))\n\t\treturn __this_address;\n\tif (!uuid_equal(&agfl->agfl_uuid, &mp->m_sb.sb_meta_uuid))\n\t\treturn __this_address;\n\t/*\n\t * during growfs operations, the perag is not fully initialised,\n\t * so we can't use it for any useful checking. growfs ensures we can't\n\t * use it by using uncached buffers that don't have the perag attached\n\t * so we can detect and avoid this problem.\n\t */\n\tif (bp->b_pag && be32_to_cpu(agfl->agfl_seqno) != bp->b_pag->pag_agno)\n\t\treturn __this_address;\n\n\tfor (i = 0; i < xfs_agfl_size(mp); i++) {\n\t\tif (be32_to_cpu(agfl->agfl_bno[i]) != NULLAGBLOCK &&\n\t\t    be32_to_cpu(agfl->agfl_bno[i]) >= mp->m_sb.sb_agblocks)\n\t\t\treturn __this_address;\n\t}\n\n\tif (!xfs_log_check_lsn(mp, be64_to_cpu(XFS_BUF_TO_AGFL(bp)->agfl_lsn)))\n\t\treturn __this_address;\n\treturn NULL;\n}\n\nstatic void\nxfs_agfl_read_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount *mp = bp->b_mount;\n\txfs_failaddr_t\tfa;\n\n\t/*\n\t * There is no verification of non-crc AGFLs because mkfs does not\n\t * initialise the AGFL to zero or NULL. Hence the only valid part of the\n\t * AGFL is what the AGF says is active. We can't get to the AGF, so we\n\t * can't verify just those entries are valid.\n\t */\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn;\n\n\tif (!xfs_buf_verify_cksum(bp, XFS_AGFL_CRC_OFF))\n\t\txfs_verifier_error(bp, -EFSBADCRC, __this_address);\n\telse {\n\t\tfa = xfs_agfl_verify(bp);\n\t\tif (fa)\n\t\t\txfs_verifier_error(bp, -EFSCORRUPTED, fa);\n\t}\n}\n\nstatic void\nxfs_agfl_write_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tstruct xfs_buf_log_item\t*bip = bp->b_log_item;\n\txfs_failaddr_t\t\tfa;\n\n\t/* no verification of non-crc AGFLs */\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn;\n\n\tfa = xfs_agfl_verify(bp);\n\tif (fa) {\n\t\txfs_verifier_error(bp, -EFSCORRUPTED, fa);\n\t\treturn;\n\t}\n\n\tif (bip)\n\t\tXFS_BUF_TO_AGFL(bp)->agfl_lsn = cpu_to_be64(bip->bli_item.li_lsn);\n\n\txfs_buf_update_cksum(bp, XFS_AGFL_CRC_OFF);\n}\n\nconst struct xfs_buf_ops xfs_agfl_buf_ops = {\n\t.name = \"xfs_agfl\",\n\t.magic = { cpu_to_be32(XFS_AGFL_MAGIC), cpu_to_be32(XFS_AGFL_MAGIC) },\n\t.verify_read = xfs_agfl_read_verify,\n\t.verify_write = xfs_agfl_write_verify,\n\t.verify_struct = xfs_agfl_verify,\n};\n\n/*\n * Read in the allocation group free block array.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_read_agfl(\n\txfs_mount_t\t*mp,\t\t/* mount point structure */\n\txfs_trans_t\t*tp,\t\t/* transaction pointer */\n\txfs_agnumber_t\tagno,\t\t/* allocation group number */\n\txfs_buf_t\t**bpp)\t\t/* buffer for the ag free block array */\n{\n\txfs_buf_t\t*bp;\t\t/* return value */\n\tint\t\terror;\n\n\tASSERT(agno != NULLAGNUMBER);\n\terror = xfs_trans_read_buf(\n\t\t\tmp, tp, mp->m_ddev_targp,\n\t\t\tXFS_AG_DADDR(mp, agno, XFS_AGFL_DADDR(mp)),\n\t\t\tXFS_FSS_TO_BB(mp, 1), 0, &bp, &xfs_agfl_buf_ops);\n\tif (error)\n\t\treturn error;\n\txfs_buf_set_ref(bp, XFS_AGFL_REF);\n\t*bpp = bp;\n\treturn 0;\n}\n\nSTATIC int\nxfs_alloc_update_counters(\n\tstruct xfs_trans\t*tp,\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_buf\t\t*agbp,\n\tlong\t\t\tlen)\n{\n\tstruct xfs_agf\t\t*agf = XFS_BUF_TO_AGF(agbp);\n\n\tpag->pagf_freeblks += len;\n\tbe32_add_cpu(&agf->agf_freeblks, len);\n\n\txfs_trans_agblocks_delta(tp, len);\n\tif (unlikely(be32_to_cpu(agf->agf_freeblks) >\n\t\t     be32_to_cpu(agf->agf_length))) {\n\t\txfs_buf_corruption_error(agbp);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\txfs_alloc_log_agf(tp, agbp, XFS_AGF_FREEBLKS);\n\treturn 0;\n}\n\n/*\n * Block allocation algorithm and data structures.\n */\nstruct xfs_alloc_cur {\n\tstruct xfs_btree_cur\t\t*cnt;\t/* btree cursors */\n\tstruct xfs_btree_cur\t\t*bnolt;\n\tstruct xfs_btree_cur\t\t*bnogt;\n\txfs_extlen_t\t\t\tcur_len;/* current search length */\n\txfs_agblock_t\t\t\trec_bno;/* extent startblock */\n\txfs_extlen_t\t\t\trec_len;/* extent length */\n\txfs_agblock_t\t\t\tbno;\t/* alloc bno */\n\txfs_extlen_t\t\t\tlen;\t/* alloc len */\n\txfs_extlen_t\t\t\tdiff;\t/* diff from search bno */\n\tunsigned int\t\t\tbusy_gen;/* busy state */\n\tbool\t\t\t\tbusy;\n};\n\n/*\n * Set up cursors, etc. in the extent allocation cursor. This function can be\n * called multiple times to reset an initialized structure without having to\n * reallocate cursors.\n */\nstatic int\nxfs_alloc_cur_setup(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur)\n{\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n\tASSERT(args->alignment == 1 || args->type != XFS_ALLOCTYPE_THIS_BNO);\n\n\tacur->cur_len = args->maxlen;\n\tacur->rec_bno = 0;\n\tacur->rec_len = 0;\n\tacur->bno = 0;\n\tacur->len = 0;\n\tacur->diff = -1;\n\tacur->busy = false;\n\tacur->busy_gen = 0;\n\n\t/*\n\t * Perform an initial cntbt lookup to check for availability of maxlen\n\t * extents. If this fails, we'll return -ENOSPC to signal the caller to\n\t * attempt a small allocation.\n\t */\n\tif (!acur->cnt)\n\t\tacur->cnt = xfs_allocbt_init_cursor(args->mp, args->tp,\n\t\t\t\t\targs->agbp, args->agno, XFS_BTNUM_CNT);\n\terror = xfs_alloc_lookup_ge(acur->cnt, 0, args->maxlen, &i);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * Allocate the bnobt left and right search cursors.\n\t */\n\tif (!acur->bnolt)\n\t\tacur->bnolt = xfs_allocbt_init_cursor(args->mp, args->tp,\n\t\t\t\t\targs->agbp, args->agno, XFS_BTNUM_BNO);\n\tif (!acur->bnogt)\n\t\tacur->bnogt = xfs_allocbt_init_cursor(args->mp, args->tp,\n\t\t\t\t\targs->agbp, args->agno, XFS_BTNUM_BNO);\n\treturn i == 1 ? 0 : -ENOSPC;\n}\n\nstatic void\nxfs_alloc_cur_close(\n\tstruct xfs_alloc_cur\t*acur,\n\tbool\t\t\terror)\n{\n\tint\t\t\tcur_error = XFS_BTREE_NOERROR;\n\n\tif (error)\n\t\tcur_error = XFS_BTREE_ERROR;\n\n\tif (acur->cnt)\n\t\txfs_btree_del_cursor(acur->cnt, cur_error);\n\tif (acur->bnolt)\n\t\txfs_btree_del_cursor(acur->bnolt, cur_error);\n\tif (acur->bnogt)\n\t\txfs_btree_del_cursor(acur->bnogt, cur_error);\n\tacur->cnt = acur->bnolt = acur->bnogt = NULL;\n}\n\n/*\n * Check an extent for allocation and track the best available candidate in the\n * allocation structure. The cursor is deactivated if it has entered an out of\n * range state based on allocation arguments. Optionally return the extent\n * extent geometry and allocation status if requested by the caller.\n */\nstatic int\nxfs_alloc_cur_check(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur,\n\tstruct xfs_btree_cur\t*cur,\n\tint\t\t\t*new)\n{\n\tint\t\t\terror, i;\n\txfs_agblock_t\t\tbno, bnoa, bnew;\n\txfs_extlen_t\t\tlen, lena, diff = -1;\n\tbool\t\t\tbusy;\n\tunsigned\t\tbusy_gen = 0;\n\tbool\t\t\tdeactivate = false;\n\tbool\t\t\tisbnobt = cur->bc_btnum == XFS_BTNUM_BNO;\n\n\t*new = 0;\n\n\terror = xfs_alloc_get_rec(cur, &bno, &len, &i);\n\tif (error)\n\t\treturn error;\n\tif (XFS_IS_CORRUPT(args->mp, i != 1))\n\t\treturn -EFSCORRUPTED;\n\n\t/*\n\t * Check minlen and deactivate a cntbt cursor if out of acceptable size\n\t * range (i.e., walking backwards looking for a minlen extent).\n\t */\n\tif (len < args->minlen) {\n\t\tdeactivate = !isbnobt;\n\t\tgoto out;\n\t}\n\n\tbusy = xfs_alloc_compute_aligned(args, bno, len, &bnoa, &lena,\n\t\t\t\t\t &busy_gen);\n\tacur->busy |= busy;\n\tif (busy)\n\t\tacur->busy_gen = busy_gen;\n\t/* deactivate a bnobt cursor outside of locality range */\n\tif (bnoa < args->min_agbno || bnoa > args->max_agbno) {\n\t\tdeactivate = isbnobt;\n\t\tgoto out;\n\t}\n\tif (lena < args->minlen)\n\t\tgoto out;\n\n\targs->len = XFS_EXTLEN_MIN(lena, args->maxlen);\n\txfs_alloc_fix_len(args);\n\tASSERT(args->len >= args->minlen);\n\tif (args->len < acur->len)\n\t\tgoto out;\n\n\t/*\n\t * We have an aligned record that satisfies minlen and beats or matches\n\t * the candidate extent size. Compare locality for near allocation mode.\n\t */\n\tASSERT(args->type == XFS_ALLOCTYPE_NEAR_BNO);\n\tdiff = xfs_alloc_compute_diff(args->agbno, args->len,\n\t\t\t\t      args->alignment, args->datatype,\n\t\t\t\t      bnoa, lena, &bnew);\n\tif (bnew == NULLAGBLOCK)\n\t\tgoto out;\n\n\t/*\n\t * Deactivate a bnobt cursor with worse locality than the current best.\n\t */\n\tif (diff > acur->diff) {\n\t\tdeactivate = isbnobt;\n\t\tgoto out;\n\t}\n\n\tASSERT(args->len > acur->len ||\n\t       (args->len == acur->len && diff <= acur->diff));\n\tacur->rec_bno = bno;\n\tacur->rec_len = len;\n\tacur->bno = bnew;\n\tacur->len = args->len;\n\tacur->diff = diff;\n\t*new = 1;\n\n\t/*\n\t * We're done if we found a perfect allocation. This only deactivates\n\t * the current cursor, but this is just an optimization to terminate a\n\t * cntbt search that otherwise runs to the edge of the tree.\n\t */\n\tif (acur->diff == 0 && acur->len == args->maxlen)\n\t\tdeactivate = true;\nout:\n\tif (deactivate)\n\t\tcur->bc_private.a.priv.abt.active = false;\n\ttrace_xfs_alloc_cur_check(args->mp, cur->bc_btnum, bno, len, diff,\n\t\t\t\t  *new);\n\treturn 0;\n}\n\n/*\n * Complete an allocation of a candidate extent. Remove the extent from both\n * trees and update the args structure.\n */\nSTATIC int\nxfs_alloc_cur_finish(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur)\n{\n\tint\t\t\terror;\n\n\tASSERT(acur->cnt && acur->bnolt);\n\tASSERT(acur->bno >= acur->rec_bno);\n\tASSERT(acur->bno + acur->len <= acur->rec_bno + acur->rec_len);\n\tASSERT(acur->rec_bno + acur->rec_len <=\n\t       be32_to_cpu(XFS_BUF_TO_AGF(args->agbp)->agf_length));\n\n\terror = xfs_alloc_fixup_trees(acur->cnt, acur->bnolt, acur->rec_bno,\n\t\t\t\t      acur->rec_len, acur->bno, acur->len, 0);\n\tif (error)\n\t\treturn error;\n\n\targs->agbno = acur->bno;\n\targs->len = acur->len;\n\targs->wasfromfl = 0;\n\n\ttrace_xfs_alloc_cur(args);\n\treturn 0;\n}\n\n/*\n * Locality allocation lookup algorithm. This expects a cntbt cursor and uses\n * bno optimized lookup to search for extents with ideal size and locality.\n */\nSTATIC int\nxfs_alloc_cntbt_iter(\n\tstruct xfs_alloc_arg\t\t*args,\n\tstruct xfs_alloc_cur\t\t*acur)\n{\n\tstruct xfs_btree_cur\t*cur = acur->cnt;\n\txfs_agblock_t\t\tbno;\n\txfs_extlen_t\t\tlen, cur_len;\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n\tif (!xfs_alloc_cur_active(cur))\n\t\treturn 0;\n\n\t/* locality optimized lookup */\n\tcur_len = acur->cur_len;\n\terror = xfs_alloc_lookup_ge(cur, args->agbno, cur_len, &i);\n\tif (error)\n\t\treturn error;\n\tif (i == 0)\n\t\treturn 0;\n\terror = xfs_alloc_get_rec(cur, &bno, &len, &i);\n\tif (error)\n\t\treturn error;\n\n\t/* check the current record and update search length from it */\n\terror = xfs_alloc_cur_check(args, acur, cur, &i);\n\tif (error)\n\t\treturn error;\n\tASSERT(len >= acur->cur_len);\n\tacur->cur_len = len;\n\n\t/*\n\t * We looked up the first record >= [agbno, len] above. The agbno is a\n\t * secondary key and so the current record may lie just before or after\n\t * agbno. If it is past agbno, check the previous record too so long as\n\t * the length matches as it may be closer. Don't check a smaller record\n\t * because that could deactivate our cursor.\n\t */\n\tif (bno > args->agbno) {\n\t\terror = xfs_btree_decrement(cur, 0, &i);\n\t\tif (!error && i) {\n\t\t\terror = xfs_alloc_get_rec(cur, &bno, &len, &i);\n\t\t\tif (!error && i && len == acur->cur_len)\n\t\t\t\terror = xfs_alloc_cur_check(args, acur, cur,\n\t\t\t\t\t\t\t    &i);\n\t\t}\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * Increment the search key until we find at least one allocation\n\t * candidate or if the extent we found was larger. Otherwise, double the\n\t * search key to optimize the search. Efficiency is more important here\n\t * than absolute best locality.\n\t */\n\tcur_len <<= 1;\n\tif (!acur->len || acur->cur_len >= cur_len)\n\t\tacur->cur_len++;\n\telse\n\t\tacur->cur_len = cur_len;\n\n\treturn error;\n}\n\n/*\n * Deal with the case where only small freespaces remain. Either return the\n * contents of the last freespace record, or allocate space from the freelist if\n * there is nothing in the tree.\n */\nSTATIC int\t\t\t/* error */\nxfs_alloc_ag_vextent_small(\n\tstruct xfs_alloc_arg\t*args,\t/* allocation argument structure */\n\tstruct xfs_btree_cur\t*ccur,\t/* optional by-size cursor */\n\txfs_agblock_t\t\t*fbnop,\t/* result block number */\n\txfs_extlen_t\t\t*flenp,\t/* result length */\n\tint\t\t\t*stat)\t/* status: 0-freelist, 1-normal/none */\n{\n\tint\t\t\terror = 0;\n\txfs_agblock_t\t\tfbno = NULLAGBLOCK;\n\txfs_extlen_t\t\tflen = 0;\n\tint\t\t\ti = 0;\n\n\t/*\n\t * If a cntbt cursor is provided, try to allocate the largest record in\n\t * the tree. Try the AGFL if the cntbt is empty, otherwise fail the\n\t * allocation. Make sure to respect minleft even when pulling from the\n\t * freelist.\n\t */\n\tif (ccur)\n\t\terror = xfs_btree_decrement(ccur, 0, &i);\n\tif (error)\n\t\tgoto error;\n\tif (i) {\n\t\terror = xfs_alloc_get_rec(ccur, &fbno, &flen, &i);\n\t\tif (error)\n\t\t\tgoto error;\n\t\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (args->minlen != 1 || args->alignment != 1 ||\n\t    args->resv == XFS_AG_RESV_AGFL ||\n\t    (be32_to_cpu(XFS_BUF_TO_AGF(args->agbp)->agf_flcount) <=\n\t     args->minleft))\n\t\tgoto out;\n\n\terror = xfs_alloc_get_freelist(args->tp, args->agbp, &fbno, 0);\n\tif (error)\n\t\tgoto error;\n\tif (fbno == NULLAGBLOCK)\n\t\tgoto out;\n\n\txfs_extent_busy_reuse(args->mp, args->agno, fbno, 1,\n\t\t\t      (args->datatype & XFS_ALLOC_NOBUSY));\n\n\tif (args->datatype & XFS_ALLOC_USERDATA) {\n\t\tstruct xfs_buf\t*bp;\n\n\t\terror = xfs_trans_get_buf(args->tp, args->mp->m_ddev_targp,\n\t\t\t\tXFS_AGB_TO_DADDR(args->mp, args->agno, fbno),\n\t\t\t\targs->mp->m_bsize, 0, &bp);\n\t\tif (error)\n\t\t\tgoto error;\n\t\txfs_trans_binval(args->tp, bp);\n\t}\n\t*fbnop = args->agbno = fbno;\n\t*flenp = args->len = 1;\n\tif (XFS_IS_CORRUPT(args->mp,\n\t\t\t   fbno >= be32_to_cpu(\n\t\t\t\t   XFS_BUF_TO_AGF(args->agbp)->agf_length))) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error;\n\t}\n\targs->wasfromfl = 1;\n\ttrace_xfs_alloc_small_freelist(args);\n\n\t/*\n\t * If we're feeding an AGFL block to something that doesn't live in the\n\t * free space, we need to clear out the OWN_AG rmap.\n\t */\n\terror = xfs_rmap_free(args->tp, args->agbp, args->agno, fbno, 1,\n\t\t\t      &XFS_RMAP_OINFO_AG);\n\tif (error)\n\t\tgoto error;\n\n\t*stat = 0;\n\treturn 0;\n\nout:\n\t/*\n\t * Can't do the allocation, give up.\n\t */\n\tif (flen < args->minlen) {\n\t\targs->agbno = NULLAGBLOCK;\n\t\ttrace_xfs_alloc_small_notenough(args);\n\t\tflen = 0;\n\t}\n\t*fbnop = fbno;\n\t*flenp = flen;\n\t*stat = 1;\n\ttrace_xfs_alloc_small_done(args);\n\treturn 0;\n\nerror:\n\ttrace_xfs_alloc_small_error(args);\n\treturn error;\n}\n\n/*\n * Allocate a variable extent in the allocation group agno.\n * Type and bno are used to determine where in the allocation group the\n * extent will start.\n * Extent's length (returned in *len) will be between minlen and maxlen,\n * and of the form k * prod + mod unless there's nothing that large.\n * Return the starting a.g. block, or NULLAGBLOCK if we can't do it.\n */\nSTATIC int\t\t\t/* error */\nxfs_alloc_ag_vextent(\n\txfs_alloc_arg_t\t*args)\t/* argument structure for allocation */\n{\n\tint\t\terror=0;\n\n\tASSERT(args->minlen > 0);\n\tASSERT(args->maxlen > 0);\n\tASSERT(args->minlen <= args->maxlen);\n\tASSERT(args->mod < args->prod);\n\tASSERT(args->alignment > 0);\n\n\t/*\n\t * Branch to correct routine based on the type.\n\t */\n\targs->wasfromfl = 0;\n\tswitch (args->type) {\n\tcase XFS_ALLOCTYPE_THIS_AG:\n\t\terror = xfs_alloc_ag_vextent_size(args);\n\t\tbreak;\n\tcase XFS_ALLOCTYPE_NEAR_BNO:\n\t\terror = xfs_alloc_ag_vextent_near(args);\n\t\tbreak;\n\tcase XFS_ALLOCTYPE_THIS_BNO:\n\t\terror = xfs_alloc_ag_vextent_exact(args);\n\t\tbreak;\n\tdefault:\n\t\tASSERT(0);\n\t\t/* NOTREACHED */\n\t}\n\n\tif (error || args->agbno == NULLAGBLOCK)\n\t\treturn error;\n\n\tASSERT(args->len >= args->minlen);\n\tASSERT(args->len <= args->maxlen);\n\tASSERT(!args->wasfromfl || args->resv != XFS_AG_RESV_AGFL);\n\tASSERT(args->agbno % args->alignment == 0);\n\n\t/* if not file data, insert new block into the reverse map btree */\n\tif (!xfs_rmap_should_skip_owner_update(&args->oinfo)) {\n\t\terror = xfs_rmap_alloc(args->tp, args->agbp, args->agno,\n\t\t\t\t       args->agbno, args->len, &args->oinfo);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (!args->wasfromfl) {\n\t\terror = xfs_alloc_update_counters(args->tp, args->pag,\n\t\t\t\t\t\t  args->agbp,\n\t\t\t\t\t\t  -((long)(args->len)));\n\t\tif (error)\n\t\t\treturn error;\n\n\t\tASSERT(!xfs_extent_busy_search(args->mp, args->agno,\n\t\t\t\t\t      args->agbno, args->len));\n\t}\n\n\txfs_ag_resv_alloc_extent(args->pag, args->resv, args);\n\n\tXFS_STATS_INC(args->mp, xs_allocx);\n\tXFS_STATS_ADD(args->mp, xs_allocb, args->len);\n\treturn error;\n}\n\n/*\n * Allocate a variable extent at exactly agno/bno.\n * Extent's length (returned in *len) will be between minlen and maxlen,\n * and of the form k * prod + mod unless there's nothing that large.\n * Return the starting a.g. block (bno), or NULLAGBLOCK if we can't do it.\n */\nSTATIC int\t\t\t/* error */\nxfs_alloc_ag_vextent_exact(\n\txfs_alloc_arg_t\t*args)\t/* allocation argument structure */\n{\n\txfs_btree_cur_t\t*bno_cur;/* by block-number btree cursor */\n\txfs_btree_cur_t\t*cnt_cur;/* by count btree cursor */\n\tint\t\terror;\n\txfs_agblock_t\tfbno;\t/* start block of found extent */\n\txfs_extlen_t\tflen;\t/* length of found extent */\n\txfs_agblock_t\ttbno;\t/* start block of busy extent */\n\txfs_extlen_t\ttlen;\t/* length of busy extent */\n\txfs_agblock_t\ttend;\t/* end block of busy extent */\n\tint\t\ti;\t/* success/failure of operation */\n\tunsigned\tbusy_gen;\n\n\tASSERT(args->alignment == 1);\n\n\t/*\n\t * Allocate/initialize a cursor for the by-number freespace btree.\n\t */\n\tbno_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,\n\t\t\t\t\t  args->agno, XFS_BTNUM_BNO);\n\n\t/*\n\t * Lookup bno and minlen in the btree (minlen is irrelevant, really).\n\t * Look for the closest free block <= bno, it must contain bno\n\t * if any free block does.\n\t */\n\terror = xfs_alloc_lookup_le(bno_cur, args->agbno, args->minlen, &i);\n\tif (error)\n\t\tgoto error0;\n\tif (!i)\n\t\tgoto not_found;\n\n\t/*\n\t * Grab the freespace record.\n\t */\n\terror = xfs_alloc_get_rec(bno_cur, &fbno, &flen, &i);\n\tif (error)\n\t\tgoto error0;\n\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\tASSERT(fbno <= args->agbno);\n\n\t/*\n\t * Check for overlapping busy extents.\n\t */\n\ttbno = fbno;\n\ttlen = flen;\n\txfs_extent_busy_trim(args, &tbno, &tlen, &busy_gen);\n\n\t/*\n\t * Give up if the start of the extent is busy, or the freespace isn't\n\t * long enough for the minimum request.\n\t */\n\tif (tbno > args->agbno)\n\t\tgoto not_found;\n\tif (tlen < args->minlen)\n\t\tgoto not_found;\n\ttend = tbno + tlen;\n\tif (tend < args->agbno + args->minlen)\n\t\tgoto not_found;\n\n\t/*\n\t * End of extent will be smaller of the freespace end and the\n\t * maximal requested end.\n\t *\n\t * Fix the length according to mod and prod if given.\n\t */\n\targs->len = XFS_AGBLOCK_MIN(tend, args->agbno + args->maxlen)\n\t\t\t\t\t\t- args->agbno;\n\txfs_alloc_fix_len(args);\n\tASSERT(args->agbno + args->len <= tend);\n\n\t/*\n\t * We are allocating agbno for args->len\n\t * Allocate/initialize a cursor for the by-size btree.\n\t */\n\tcnt_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,\n\t\targs->agno, XFS_BTNUM_CNT);\n\tASSERT(args->agbno + args->len <=\n\t\tbe32_to_cpu(XFS_BUF_TO_AGF(args->agbp)->agf_length));\n\terror = xfs_alloc_fixup_trees(cnt_cur, bno_cur, fbno, flen, args->agbno,\n\t\t\t\t      args->len, XFSA_FIXUP_BNO_OK);\n\tif (error) {\n\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_ERROR);\n\t\tgoto error0;\n\t}\n\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);\n\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\n\targs->wasfromfl = 0;\n\ttrace_xfs_alloc_exact_done(args);\n\treturn 0;\n\nnot_found:\n\t/* Didn't find it, return null. */\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);\n\targs->agbno = NULLAGBLOCK;\n\ttrace_xfs_alloc_exact_notfound(args);\n\treturn 0;\n\nerror0:\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_ERROR);\n\ttrace_xfs_alloc_exact_error(args);\n\treturn error;\n}\n\n/*\n * Search a given number of btree records in a given direction. Check each\n * record against the good extent we've already found.\n */\nSTATIC int\nxfs_alloc_walk_iter(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur,\n\tstruct xfs_btree_cur\t*cur,\n\tbool\t\t\tincrement,\n\tbool\t\t\tfind_one, /* quit on first candidate */\n\tint\t\t\tcount,    /* rec count (-1 for infinite) */\n\tint\t\t\t*stat)\n{\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n\t*stat = 0;\n\n\t/*\n\t * Search so long as the cursor is active or we find a better extent.\n\t * The cursor is deactivated if it extends beyond the range of the\n\t * current allocation candidate.\n\t */\n\twhile (xfs_alloc_cur_active(cur) && count) {\n\t\terror = xfs_alloc_cur_check(args, acur, cur, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == 1) {\n\t\t\t*stat = 1;\n\t\t\tif (find_one)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!xfs_alloc_cur_active(cur))\n\t\t\tbreak;\n\n\t\tif (increment)\n\t\t\terror = xfs_btree_increment(cur, 0, &i);\n\t\telse\n\t\t\terror = xfs_btree_decrement(cur, 0, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == 0)\n\t\t\tcur->bc_private.a.priv.abt.active = false;\n\n\t\tif (count > 0)\n\t\t\tcount--;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Search the by-bno and by-size btrees in parallel in search of an extent with\n * ideal locality based on the NEAR mode ->agbno locality hint.\n */\nSTATIC int\nxfs_alloc_ag_vextent_locality(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur,\n\tint\t\t\t*stat)\n{\n\tstruct xfs_btree_cur\t*fbcur = NULL;\n\tint\t\t\terror;\n\tint\t\t\ti;\n\tbool\t\t\tfbinc;\n\n\tASSERT(acur->len == 0);\n\tASSERT(args->type == XFS_ALLOCTYPE_NEAR_BNO);\n\n\t*stat = 0;\n\n\terror = xfs_alloc_lookup_ge(acur->cnt, args->agbno, acur->cur_len, &i);\n\tif (error)\n\t\treturn error;\n\terror = xfs_alloc_lookup_le(acur->bnolt, args->agbno, 0, &i);\n\tif (error)\n\t\treturn error;\n\terror = xfs_alloc_lookup_ge(acur->bnogt, args->agbno, 0, &i);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * Search the bnobt and cntbt in parallel. Search the bnobt left and\n\t * right and lookup the closest extent to the locality hint for each\n\t * extent size key in the cntbt. The entire search terminates\n\t * immediately on a bnobt hit because that means we've found best case\n\t * locality. Otherwise the search continues until the cntbt cursor runs\n\t * off the end of the tree. If no allocation candidate is found at this\n\t * point, give up on locality, walk backwards from the end of the cntbt\n\t * and take the first available extent.\n\t *\n\t * The parallel tree searches balance each other out to provide fairly\n\t * consistent performance for various situations. The bnobt search can\n\t * have pathological behavior in the worst case scenario of larger\n\t * allocation requests and fragmented free space. On the other hand, the\n\t * bnobt is able to satisfy most smaller allocation requests much more\n\t * quickly than the cntbt. The cntbt search can sift through fragmented\n\t * free space and sets of free extents for larger allocation requests\n\t * more quickly than the bnobt. Since the locality hint is just a hint\n\t * and we don't want to scan the entire bnobt for perfect locality, the\n\t * cntbt search essentially bounds the bnobt search such that we can\n\t * find good enough locality at reasonable performance in most cases.\n\t */\n\twhile (xfs_alloc_cur_active(acur->bnolt) ||\n\t       xfs_alloc_cur_active(acur->bnogt) ||\n\t       xfs_alloc_cur_active(acur->cnt)) {\n\n\t\ttrace_xfs_alloc_cur_lookup(args);\n\n\t\t/*\n\t\t * Search the bnobt left and right. In the case of a hit, finish\n\t\t * the search in the opposite direction and we're done.\n\t\t */\n\t\terror = xfs_alloc_walk_iter(args, acur, acur->bnolt, false,\n\t\t\t\t\t    true, 1, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == 1) {\n\t\t\ttrace_xfs_alloc_cur_left(args);\n\t\t\tfbcur = acur->bnogt;\n\t\t\tfbinc = true;\n\t\t\tbreak;\n\t\t}\n\t\terror = xfs_alloc_walk_iter(args, acur, acur->bnogt, true, true,\n\t\t\t\t\t    1, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == 1) {\n\t\t\ttrace_xfs_alloc_cur_right(args);\n\t\t\tfbcur = acur->bnolt;\n\t\t\tfbinc = false;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Check the extent with best locality based on the current\n\t\t * extent size search key and keep track of the best candidate.\n\t\t */\n\t\terror = xfs_alloc_cntbt_iter(args, acur);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (!xfs_alloc_cur_active(acur->cnt)) {\n\t\t\ttrace_xfs_alloc_cur_lookup_done(args);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * If we failed to find anything due to busy extents, return empty\n\t * handed so the caller can flush and retry. If no busy extents were\n\t * found, walk backwards from the end of the cntbt as a last resort.\n\t */\n\tif (!xfs_alloc_cur_active(acur->cnt) && !acur->len && !acur->busy) {\n\t\terror = xfs_btree_decrement(acur->cnt, 0, &i);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i) {\n\t\t\tacur->cnt->bc_private.a.priv.abt.active = true;\n\t\t\tfbcur = acur->cnt;\n\t\t\tfbinc = false;\n\t\t}\n\t}\n\n\t/*\n\t * Search in the opposite direction for a better entry in the case of\n\t * a bnobt hit or walk backwards from the end of the cntbt.\n\t */\n\tif (fbcur) {\n\t\terror = xfs_alloc_walk_iter(args, acur, fbcur, fbinc, true, -1,\n\t\t\t\t\t    &i);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (acur->len)\n\t\t*stat = 1;\n\n\treturn 0;\n}\n\n/* Check the last block of the cnt btree for allocations. */\nstatic int\nxfs_alloc_ag_vextent_lastblock(\n\tstruct xfs_alloc_arg\t*args,\n\tstruct xfs_alloc_cur\t*acur,\n\txfs_agblock_t\t\t*bno,\n\txfs_extlen_t\t\t*len,\n\tbool\t\t\t*allocated)\n{\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n#ifdef DEBUG\n\t/* Randomly don't execute the first algorithm. */\n\tif (prandom_u32() & 1)\n\t\treturn 0;\n#endif\n\n\t/*\n\t * Start from the entry that lookup found, sequence through all larger\n\t * free blocks.  If we're actually pointing at a record smaller than\n\t * maxlen, go to the start of this block, and skip all those smaller\n\t * than minlen.\n\t */\n\tif (len || args->alignment > 1) {\n\t\tacur->cnt->bc_ptrs[0] = 1;\n\t\tdo {\n\t\t\terror = xfs_alloc_get_rec(acur->cnt, bno, len, &i);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\tif (XFS_IS_CORRUPT(args->mp, i != 1))\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tif (*len >= args->minlen)\n\t\t\t\tbreak;\n\t\t\terror = xfs_btree_increment(acur->cnt, 0, &i);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t} while (i);\n\t\tASSERT(*len >= args->minlen);\n\t\tif (!i)\n\t\t\treturn 0;\n\t}\n\n\terror = xfs_alloc_walk_iter(args, acur, acur->cnt, true, false, -1, &i);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * It didn't work.  We COULD be in a case where there's a good record\n\t * somewhere, so try again.\n\t */\n\tif (acur->len == 0)\n\t\treturn 0;\n\n\ttrace_xfs_alloc_near_first(args);\n\t*allocated = true;\n\treturn 0;\n}\n\n/*\n * Allocate a variable extent near bno in the allocation group agno.\n * Extent's length (returned in len) will be between minlen and maxlen,\n * and of the form k * prod + mod unless there's nothing that large.\n * Return the starting a.g. block, or NULLAGBLOCK if we can't do it.\n */\nSTATIC int\nxfs_alloc_ag_vextent_near(\n\tstruct xfs_alloc_arg\t*args)\n{\n\tstruct xfs_alloc_cur\tacur = {};\n\tint\t\t\terror;\t\t/* error code */\n\tint\t\t\ti;\t\t/* result code, temporary */\n\txfs_agblock_t\t\tbno;\n\txfs_extlen_t\t\tlen;\n\n\t/* handle uninitialized agbno range so caller doesn't have to */\n\tif (!args->min_agbno && !args->max_agbno)\n\t\targs->max_agbno = args->mp->m_sb.sb_agblocks - 1;\n\tASSERT(args->min_agbno <= args->max_agbno);\n\n\t/* clamp agbno to the range if it's outside */\n\tif (args->agbno < args->min_agbno)\n\t\targs->agbno = args->min_agbno;\n\tif (args->agbno > args->max_agbno)\n\t\targs->agbno = args->max_agbno;\n\nrestart:\n\tlen = 0;\n\n\t/*\n\t * Set up cursors and see if there are any free extents as big as\n\t * maxlen. If not, pick the last entry in the tree unless the tree is\n\t * empty.\n\t */\n\terror = xfs_alloc_cur_setup(args, &acur);\n\tif (error == -ENOSPC) {\n\t\terror = xfs_alloc_ag_vextent_small(args, acur.cnt, &bno,\n\t\t\t\t&len, &i);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tif (i == 0 || len == 0) {\n\t\t\ttrace_xfs_alloc_near_noentry(args);\n\t\t\tgoto out;\n\t\t}\n\t\tASSERT(i == 1);\n\t} else if (error) {\n\t\tgoto out;\n\t}\n\n\t/*\n\t * First algorithm.\n\t * If the requested extent is large wrt the freespaces available\n\t * in this a.g., then the cursor will be pointing to a btree entry\n\t * near the right edge of the tree.  If it's in the last btree leaf\n\t * block, then we just examine all the entries in that block\n\t * that are big enough, and pick the best one.\n\t */\n\tif (xfs_btree_islastblock(acur.cnt, 0)) {\n\t\tbool\t\tallocated = false;\n\n\t\terror = xfs_alloc_ag_vextent_lastblock(args, &acur, &bno, &len,\n\t\t\t\t&allocated);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tif (allocated)\n\t\t\tgoto alloc_finish;\n\t}\n\n\t/*\n\t * Second algorithm. Combined cntbt and bnobt search to find ideal\n\t * locality.\n\t */\n\terror = xfs_alloc_ag_vextent_locality(args, &acur, &i);\n\tif (error)\n\t\tgoto out;\n\n\t/*\n\t * If we couldn't get anything, give up.\n\t */\n\tif (!acur.len) {\n\t\tif (acur.busy) {\n\t\t\ttrace_xfs_alloc_near_busy(args);\n\t\t\txfs_extent_busy_flush(args->mp, args->pag,\n\t\t\t\t\t      acur.busy_gen);\n\t\t\tgoto restart;\n\t\t}\n\t\ttrace_xfs_alloc_size_neither(args);\n\t\targs->agbno = NULLAGBLOCK;\n\t\tgoto out;\n\t}\n\nalloc_finish:\n\t/* fix up btrees on a successful allocation */\n\terror = xfs_alloc_cur_finish(args, &acur);\n\nout:\n\txfs_alloc_cur_close(&acur, error);\n\treturn error;\n}\n\n/*\n * Allocate a variable extent anywhere in the allocation group agno.\n * Extent's length (returned in len) will be between minlen and maxlen,\n * and of the form k * prod + mod unless there's nothing that large.\n * Return the starting a.g. block, or NULLAGBLOCK if we can't do it.\n */\nSTATIC int\t\t\t\t/* error */\nxfs_alloc_ag_vextent_size(\n\txfs_alloc_arg_t\t*args)\t\t/* allocation argument structure */\n{\n\txfs_btree_cur_t\t*bno_cur;\t/* cursor for bno btree */\n\txfs_btree_cur_t\t*cnt_cur;\t/* cursor for cnt btree */\n\tint\t\terror;\t\t/* error result */\n\txfs_agblock_t\tfbno;\t\t/* start of found freespace */\n\txfs_extlen_t\tflen;\t\t/* length of found freespace */\n\tint\t\ti;\t\t/* temp status variable */\n\txfs_agblock_t\trbno;\t\t/* returned block number */\n\txfs_extlen_t\trlen;\t\t/* length of returned extent */\n\tbool\t\tbusy;\n\tunsigned\tbusy_gen;\n\nrestart:\n\t/*\n\t * Allocate and initialize a cursor for the by-size btree.\n\t */\n\tcnt_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,\n\t\targs->agno, XFS_BTNUM_CNT);\n\tbno_cur = NULL;\n\tbusy = false;\n\n\t/*\n\t * Look for an entry >= maxlen+alignment-1 blocks.\n\t */\n\tif ((error = xfs_alloc_lookup_ge(cnt_cur, 0,\n\t\t\targs->maxlen + args->alignment - 1, &i)))\n\t\tgoto error0;\n\n\t/*\n\t * If none then we have to settle for a smaller extent. In the case that\n\t * there are no large extents, this will return the last entry in the\n\t * tree unless the tree is empty. In the case that there are only busy\n\t * large extents, this will return the largest small extent unless there\n\t * are no smaller extents available.\n\t */\n\tif (!i) {\n\t\terror = xfs_alloc_ag_vextent_small(args, cnt_cur,\n\t\t\t\t\t\t   &fbno, &flen, &i);\n\t\tif (error)\n\t\t\tgoto error0;\n\t\tif (i == 0 || flen == 0) {\n\t\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\t\t\ttrace_xfs_alloc_size_noentry(args);\n\t\t\treturn 0;\n\t\t}\n\t\tASSERT(i == 1);\n\t\tbusy = xfs_alloc_compute_aligned(args, fbno, flen, &rbno,\n\t\t\t\t&rlen, &busy_gen);\n\t} else {\n\t\t/*\n\t\t * Search for a non-busy extent that is large enough.\n\t\t */\n\t\tfor (;;) {\n\t\t\terror = xfs_alloc_get_rec(cnt_cur, &fbno, &flen, &i);\n\t\t\tif (error)\n\t\t\t\tgoto error0;\n\t\t\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\n\t\t\tbusy = xfs_alloc_compute_aligned(args, fbno, flen,\n\t\t\t\t\t&rbno, &rlen, &busy_gen);\n\n\t\t\tif (rlen >= args->maxlen)\n\t\t\t\tbreak;\n\n\t\t\terror = xfs_btree_increment(cnt_cur, 0, &i);\n\t\t\tif (error)\n\t\t\t\tgoto error0;\n\t\t\tif (i == 0) {\n\t\t\t\t/*\n\t\t\t\t * Our only valid extents must have been busy.\n\t\t\t\t * Make it unbusy by forcing the log out and\n\t\t\t\t * retrying.\n\t\t\t\t */\n\t\t\t\txfs_btree_del_cursor(cnt_cur,\n\t\t\t\t\t\t     XFS_BTREE_NOERROR);\n\t\t\t\ttrace_xfs_alloc_size_busy(args);\n\t\t\t\txfs_extent_busy_flush(args->mp,\n\t\t\t\t\t\t\targs->pag, busy_gen);\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * In the first case above, we got the last entry in the\n\t * by-size btree.  Now we check to see if the space hits maxlen\n\t * once aligned; if not, we search left for something better.\n\t * This can't happen in the second case above.\n\t */\n\trlen = XFS_EXTLEN_MIN(args->maxlen, rlen);\n\tif (XFS_IS_CORRUPT(args->mp,\n\t\t\t   rlen != 0 &&\n\t\t\t   (rlen > flen ||\n\t\t\t    rbno + rlen > fbno + flen))) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\tif (rlen < args->maxlen) {\n\t\txfs_agblock_t\tbestfbno;\n\t\txfs_extlen_t\tbestflen;\n\t\txfs_agblock_t\tbestrbno;\n\t\txfs_extlen_t\tbestrlen;\n\n\t\tbestrlen = rlen;\n\t\tbestrbno = rbno;\n\t\tbestflen = flen;\n\t\tbestfbno = fbno;\n\t\tfor (;;) {\n\t\t\tif ((error = xfs_btree_decrement(cnt_cur, 0, &i)))\n\t\t\t\tgoto error0;\n\t\t\tif (i == 0)\n\t\t\t\tbreak;\n\t\t\tif ((error = xfs_alloc_get_rec(cnt_cur, &fbno, &flen,\n\t\t\t\t\t&i)))\n\t\t\t\tgoto error0;\n\t\t\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t\tif (flen < bestrlen)\n\t\t\t\tbreak;\n\t\t\tbusy = xfs_alloc_compute_aligned(args, fbno, flen,\n\t\t\t\t\t&rbno, &rlen, &busy_gen);\n\t\t\trlen = XFS_EXTLEN_MIN(args->maxlen, rlen);\n\t\t\tif (XFS_IS_CORRUPT(args->mp,\n\t\t\t\t\t   rlen != 0 &&\n\t\t\t\t\t   (rlen > flen ||\n\t\t\t\t\t    rbno + rlen > fbno + flen))) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t\tif (rlen > bestrlen) {\n\t\t\t\tbestrlen = rlen;\n\t\t\t\tbestrbno = rbno;\n\t\t\t\tbestflen = flen;\n\t\t\t\tbestfbno = fbno;\n\t\t\t\tif (rlen == args->maxlen)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, bestfbno, bestflen,\n\t\t\t\t&i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(args->mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\trlen = bestrlen;\n\t\trbno = bestrbno;\n\t\tflen = bestflen;\n\t\tfbno = bestfbno;\n\t}\n\targs->wasfromfl = 0;\n\t/*\n\t * Fix up the length.\n\t */\n\targs->len = rlen;\n\tif (rlen < args->minlen) {\n\t\tif (busy) {\n\t\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\t\t\ttrace_xfs_alloc_size_busy(args);\n\t\t\txfs_extent_busy_flush(args->mp, args->pag, busy_gen);\n\t\t\tgoto restart;\n\t\t}\n\t\tgoto out_nominleft;\n\t}\n\txfs_alloc_fix_len(args);\n\n\trlen = args->len;\n\tif (XFS_IS_CORRUPT(args->mp, rlen > flen)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\t/*\n\t * Allocate and initialize a cursor for the by-block tree.\n\t */\n\tbno_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,\n\t\targs->agno, XFS_BTNUM_BNO);\n\tif ((error = xfs_alloc_fixup_trees(cnt_cur, bno_cur, fbno, flen,\n\t\t\trbno, rlen, XFSA_FIXUP_CNT_OK)))\n\t\tgoto error0;\n\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);\n\tcnt_cur = bno_cur = NULL;\n\targs->len = rlen;\n\targs->agbno = rbno;\n\tif (XFS_IS_CORRUPT(args->mp,\n\t\t\t   args->agbno + args->len >\n\t\t\t   be32_to_cpu(\n\t\t\t\t   XFS_BUF_TO_AGF(args->agbp)->agf_length))) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\ttrace_xfs_alloc_size_done(args);\n\treturn 0;\n\nerror0:\n\ttrace_xfs_alloc_size_error(args);\n\tif (cnt_cur)\n\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_ERROR);\n\tif (bno_cur)\n\t\txfs_btree_del_cursor(bno_cur, XFS_BTREE_ERROR);\n\treturn error;\n\nout_nominleft:\n\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\ttrace_xfs_alloc_size_nominleft(args);\n\targs->agbno = NULLAGBLOCK;\n\treturn 0;\n}\n\n/*\n * Free the extent starting at agno/bno for length.\n */\nSTATIC int\nxfs_free_ag_extent(\n\tstruct xfs_trans\t\t*tp,\n\tstruct xfs_buf\t\t\t*agbp,\n\txfs_agnumber_t\t\t\tagno,\n\txfs_agblock_t\t\t\tbno,\n\txfs_extlen_t\t\t\tlen,\n\tconst struct xfs_owner_info\t*oinfo,\n\tenum xfs_ag_resv_type\t\ttype)\n{\n\tstruct xfs_mount\t\t*mp;\n\tstruct xfs_perag\t\t*pag;\n\tstruct xfs_btree_cur\t\t*bno_cur;\n\tstruct xfs_btree_cur\t\t*cnt_cur;\n\txfs_agblock_t\t\t\tgtbno; /* start of right neighbor */\n\txfs_extlen_t\t\t\tgtlen; /* length of right neighbor */\n\txfs_agblock_t\t\t\tltbno; /* start of left neighbor */\n\txfs_extlen_t\t\t\tltlen; /* length of left neighbor */\n\txfs_agblock_t\t\t\tnbno; /* new starting block of freesp */\n\txfs_extlen_t\t\t\tnlen; /* new length of freespace */\n\tint\t\t\t\thaveleft; /* have a left neighbor */\n\tint\t\t\t\thaveright; /* have a right neighbor */\n\tint\t\t\t\ti;\n\tint\t\t\t\terror;\n\n\tbno_cur = cnt_cur = NULL;\n\tmp = tp->t_mountp;\n\n\tif (!xfs_rmap_should_skip_owner_update(oinfo)) {\n\t\terror = xfs_rmap_free(tp, agbp, agno, bno, len, oinfo);\n\t\tif (error)\n\t\t\tgoto error0;\n\t}\n\n\t/*\n\t * Allocate and initialize a cursor for the by-block btree.\n\t */\n\tbno_cur = xfs_allocbt_init_cursor(mp, tp, agbp, agno, XFS_BTNUM_BNO);\n\t/*\n\t * Look for a neighboring block on the left (lower block numbers)\n\t * that is contiguous with this space.\n\t */\n\tif ((error = xfs_alloc_lookup_le(bno_cur, bno, len, &haveleft)))\n\t\tgoto error0;\n\tif (haveleft) {\n\t\t/*\n\t\t * There is a block to our left.\n\t\t */\n\t\tif ((error = xfs_alloc_get_rec(bno_cur, &ltbno, &ltlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * It's not contiguous, though.\n\t\t */\n\t\tif (ltbno + ltlen < bno)\n\t\t\thaveleft = 0;\n\t\telse {\n\t\t\t/*\n\t\t\t * If this failure happens the request to free this\n\t\t\t * space was invalid, it's (partly) already free.\n\t\t\t * Very bad.\n\t\t\t */\n\t\t\tif (XFS_IS_CORRUPT(mp, ltbno + ltlen > bno)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t}\n\t}\n\t/*\n\t * Look for a neighboring block on the right (higher block numbers)\n\t * that is contiguous with this space.\n\t */\n\tif ((error = xfs_btree_increment(bno_cur, 0, &haveright)))\n\t\tgoto error0;\n\tif (haveright) {\n\t\t/*\n\t\t * There is a block to our right.\n\t\t */\n\t\tif ((error = xfs_alloc_get_rec(bno_cur, &gtbno, &gtlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * It's not contiguous, though.\n\t\t */\n\t\tif (bno + len < gtbno)\n\t\t\thaveright = 0;\n\t\telse {\n\t\t\t/*\n\t\t\t * If this failure happens the request to free this\n\t\t\t * space was invalid, it's (partly) already free.\n\t\t\t * Very bad.\n\t\t\t */\n\t\t\tif (XFS_IS_CORRUPT(mp, bno + len > gtbno)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t}\n\t}\n\t/*\n\t * Now allocate and initialize a cursor for the by-size tree.\n\t */\n\tcnt_cur = xfs_allocbt_init_cursor(mp, tp, agbp, agno, XFS_BTNUM_CNT);\n\t/*\n\t * Have both left and right contiguous neighbors.\n\t * Merge all three into a single free block.\n\t */\n\tif (haveleft && haveright) {\n\t\t/*\n\t\t * Delete the old by-size entry on the left.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, ltbno, ltlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Delete the old by-size entry on the right.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, gtbno, gtlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Delete the old by-block entry for the right block.\n\t\t */\n\t\tif ((error = xfs_btree_delete(bno_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Move the by-block cursor back to the left neighbor.\n\t\t */\n\t\tif ((error = xfs_btree_decrement(bno_cur, 0, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n#ifdef DEBUG\n\t\t/*\n\t\t * Check that this is the right record: delete didn't\n\t\t * mangle the cursor.\n\t\t */\n\t\t{\n\t\t\txfs_agblock_t\txxbno;\n\t\t\txfs_extlen_t\txxlen;\n\n\t\t\tif ((error = xfs_alloc_get_rec(bno_cur, &xxbno, &xxlen,\n\t\t\t\t\t&i)))\n\t\t\t\tgoto error0;\n\t\t\tif (XFS_IS_CORRUPT(mp,\n\t\t\t\t\t   i != 1 ||\n\t\t\t\t\t   xxbno != ltbno ||\n\t\t\t\t\t   xxlen != ltlen)) {\n\t\t\t\terror = -EFSCORRUPTED;\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t}\n#endif\n\t\t/*\n\t\t * Update remaining by-block entry to the new, joined block.\n\t\t */\n\t\tnbno = ltbno;\n\t\tnlen = len + ltlen + gtlen;\n\t\tif ((error = xfs_alloc_update(bno_cur, nbno, nlen)))\n\t\t\tgoto error0;\n\t}\n\t/*\n\t * Have only a left contiguous neighbor.\n\t * Merge it together with the new freespace.\n\t */\n\telse if (haveleft) {\n\t\t/*\n\t\t * Delete the old by-size entry on the left.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, ltbno, ltlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Back up the by-block cursor to the left neighbor, and\n\t\t * update its length.\n\t\t */\n\t\tif ((error = xfs_btree_decrement(bno_cur, 0, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tnbno = ltbno;\n\t\tnlen = len + ltlen;\n\t\tif ((error = xfs_alloc_update(bno_cur, nbno, nlen)))\n\t\t\tgoto error0;\n\t}\n\t/*\n\t * Have only a right contiguous neighbor.\n\t * Merge it together with the new freespace.\n\t */\n\telse if (haveright) {\n\t\t/*\n\t\t * Delete the old by-size entry on the right.\n\t\t */\n\t\tif ((error = xfs_alloc_lookup_eq(cnt_cur, gtbno, gtlen, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\tif ((error = xfs_btree_delete(cnt_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t\t/*\n\t\t * Update the starting block and length of the right\n\t\t * neighbor in the by-block tree.\n\t\t */\n\t\tnbno = bno;\n\t\tnlen = len + gtlen;\n\t\tif ((error = xfs_alloc_update(bno_cur, nbno, nlen)))\n\t\t\tgoto error0;\n\t}\n\t/*\n\t * No contiguous neighbors.\n\t * Insert the new freespace into the by-block tree.\n\t */\n\telse {\n\t\tnbno = bno;\n\t\tnlen = len;\n\t\tif ((error = xfs_btree_insert(bno_cur, &i)))\n\t\t\tgoto error0;\n\t\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto error0;\n\t\t}\n\t}\n\txfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);\n\tbno_cur = NULL;\n\t/*\n\t * In all cases we need to insert the new freespace in the by-size tree.\n\t */\n\tif ((error = xfs_alloc_lookup_eq(cnt_cur, nbno, nlen, &i)))\n\t\tgoto error0;\n\tif (XFS_IS_CORRUPT(mp, i != 0)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\tif ((error = xfs_btree_insert(cnt_cur, &i)))\n\t\tgoto error0;\n\tif (XFS_IS_CORRUPT(mp, i != 1)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto error0;\n\t}\n\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);\n\tcnt_cur = NULL;\n\n\t/*\n\t * Update the freespace totals in the ag and superblock.\n\t */\n\tpag = xfs_perag_get(mp, agno);\n\terror = xfs_alloc_update_counters(tp, pag, agbp, len);\n\txfs_ag_resv_free_extent(pag, type, tp, len);\n\txfs_perag_put(pag);\n\tif (error)\n\t\tgoto error0;\n\n\tXFS_STATS_INC(mp, xs_freex);\n\tXFS_STATS_ADD(mp, xs_freeb, len);\n\n\ttrace_xfs_free_extent(mp, agno, bno, len, type, haveleft, haveright);\n\n\treturn 0;\n\n error0:\n\ttrace_xfs_free_extent(mp, agno, bno, len, type, -1, -1);\n\tif (bno_cur)\n\t\txfs_btree_del_cursor(bno_cur, XFS_BTREE_ERROR);\n\tif (cnt_cur)\n\t\txfs_btree_del_cursor(cnt_cur, XFS_BTREE_ERROR);\n\treturn error;\n}\n\n/*\n * Visible (exported) allocation/free functions.\n * Some of these are used just by xfs_alloc_btree.c and this file.\n */\n\n/*\n * Compute and fill in value of m_ag_maxlevels.\n */\nvoid\nxfs_alloc_compute_maxlevels(\n\txfs_mount_t\t*mp)\t/* file system mount structure */\n{\n\tmp->m_ag_maxlevels = xfs_btree_compute_maxlevels(mp->m_alloc_mnr,\n\t\t\t(mp->m_sb.sb_agblocks + 1) / 2);\n}\n\n/*\n * Find the length of the longest extent in an AG.  The 'need' parameter\n * specifies how much space we're going to need for the AGFL and the\n * 'reserved' parameter tells us how many blocks in this AG are reserved for\n * other callers.\n */\nxfs_extlen_t\nxfs_alloc_longest_free_extent(\n\tstruct xfs_perag\t*pag,\n\txfs_extlen_t\t\tneed,\n\txfs_extlen_t\t\treserved)\n{\n\txfs_extlen_t\t\tdelta = 0;\n\n\t/*\n\t * If the AGFL needs a recharge, we'll have to subtract that from the\n\t * longest extent.\n\t */\n\tif (need > pag->pagf_flcount)\n\t\tdelta = need - pag->pagf_flcount;\n\n\t/*\n\t * If we cannot maintain others' reservations with space from the\n\t * not-longest freesp extents, we'll have to subtract /that/ from\n\t * the longest extent too.\n\t */\n\tif (pag->pagf_freeblks - pag->pagf_longest < reserved)\n\t\tdelta += reserved - (pag->pagf_freeblks - pag->pagf_longest);\n\n\t/*\n\t * If the longest extent is long enough to satisfy all the\n\t * reservations and AGFL rules in place, we can return this extent.\n\t */\n\tif (pag->pagf_longest > delta)\n\t\treturn min_t(xfs_extlen_t, pag->pag_mount->m_ag_max_usable,\n\t\t\t\tpag->pagf_longest - delta);\n\n\t/* Otherwise, let the caller try for 1 block if there's space. */\n\treturn pag->pagf_flcount > 0 || pag->pagf_longest > 0;\n}\n\n/*\n * Compute the minimum length of the AGFL in the given AG.  If @pag is NULL,\n * return the largest possible minimum length.\n */\nunsigned int\nxfs_alloc_min_freelist(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag)\n{\n\t/* AG btrees have at least 1 level. */\n\tstatic const uint8_t\tfake_levels[XFS_BTNUM_AGF] = {1, 1, 1};\n\tconst uint8_t\t\t*levels = pag ? pag->pagf_levels : fake_levels;\n\tunsigned int\t\tmin_free;\n\n\tASSERT(mp->m_ag_maxlevels > 0);\n\n\t/* space needed by-bno freespace btree */\n\tmin_free = min_t(unsigned int, levels[XFS_BTNUM_BNOi] + 1,\n\t\t\t\t       mp->m_ag_maxlevels);\n\t/* space needed by-size freespace btree */\n\tmin_free += min_t(unsigned int, levels[XFS_BTNUM_CNTi] + 1,\n\t\t\t\t       mp->m_ag_maxlevels);\n\t/* space needed reverse mapping used space btree */\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb))\n\t\tmin_free += min_t(unsigned int, levels[XFS_BTNUM_RMAPi] + 1,\n\t\t\t\t\t\tmp->m_rmap_maxlevels);\n\n\treturn min_free;\n}\n\n/*\n * Check if the operation we are fixing up the freelist for should go ahead or\n * not. If we are freeing blocks, we always allow it, otherwise the allocation\n * is dependent on whether the size and shape of free space available will\n * permit the requested allocation to take place.\n */\nstatic bool\nxfs_alloc_space_available(\n\tstruct xfs_alloc_arg\t*args,\n\txfs_extlen_t\t\tmin_free,\n\tint\t\t\tflags)\n{\n\tstruct xfs_perag\t*pag = args->pag;\n\txfs_extlen_t\t\talloc_len, longest;\n\txfs_extlen_t\t\treservation; /* blocks that are still reserved */\n\tint\t\t\tavailable;\n\txfs_extlen_t\t\tagflcount;\n\n\tif (flags & XFS_ALLOC_FLAG_FREEING)\n\t\treturn true;\n\n\treservation = xfs_ag_resv_needed(pag, args->resv);\n\n\t/* do we have enough contiguous free space for the allocation? */\n\talloc_len = args->minlen + (args->alignment - 1) + args->minalignslop;\n\tlongest = xfs_alloc_longest_free_extent(pag, min_free, reservation);\n\tif (longest < alloc_len)\n\t\treturn false;\n\n\t/*\n\t * Do we have enough free space remaining for the allocation? Don't\n\t * account extra agfl blocks because we are about to defer free them,\n\t * making them unavailable until the current transaction commits.\n\t */\n\tagflcount = min_t(xfs_extlen_t, pag->pagf_flcount, min_free);\n\tavailable = (int)(pag->pagf_freeblks + agflcount -\n\t\t\t  reservation - min_free - args->minleft);\n\tif (available < (int)max(args->total, alloc_len))\n\t\treturn false;\n\n\t/*\n\t * Clamp maxlen to the amount of free space available for the actual\n\t * extent allocation.\n\t */\n\tif (available < (int)args->maxlen && !(flags & XFS_ALLOC_FLAG_CHECK)) {\n\t\targs->maxlen = available;\n\t\tASSERT(args->maxlen > 0);\n\t\tASSERT(args->maxlen >= args->minlen);\n\t}\n\n\treturn true;\n}\n\nint\nxfs_free_agfl_block(\n\tstruct xfs_trans\t*tp,\n\txfs_agnumber_t\t\tagno,\n\txfs_agblock_t\t\tagbno,\n\tstruct xfs_buf\t\t*agbp,\n\tstruct xfs_owner_info\t*oinfo)\n{\n\tint\t\t\terror;\n\tstruct xfs_buf\t\t*bp;\n\n\terror = xfs_free_ag_extent(tp, agbp, agno, agbno, 1, oinfo,\n\t\t\t\t   XFS_AG_RESV_AGFL);\n\tif (error)\n\t\treturn error;\n\n\terror = xfs_trans_get_buf(tp, tp->t_mountp->m_ddev_targp,\n\t\t\tXFS_AGB_TO_DADDR(tp->t_mountp, agno, agbno),\n\t\t\ttp->t_mountp->m_bsize, 0, &bp);\n\tif (error)\n\t\treturn error;\n\txfs_trans_binval(tp, bp);\n\n\treturn 0;\n}\n\n/*\n * Check the agfl fields of the agf for inconsistency or corruption. The purpose\n * is to detect an agfl header padding mismatch between current and early v5\n * kernels. This problem manifests as a 1-slot size difference between the\n * on-disk flcount and the active [first, last] range of a wrapped agfl. This\n * may also catch variants of agfl count corruption unrelated to padding. Either\n * way, we'll reset the agfl and warn the user.\n *\n * Return true if a reset is required before the agfl can be used, false\n * otherwise.\n */\nstatic bool\nxfs_agfl_needs_reset(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_agf\t\t*agf)\n{\n\tuint32_t\t\tf = be32_to_cpu(agf->agf_flfirst);\n\tuint32_t\t\tl = be32_to_cpu(agf->agf_fllast);\n\tuint32_t\t\tc = be32_to_cpu(agf->agf_flcount);\n\tint\t\t\tagfl_size = xfs_agfl_size(mp);\n\tint\t\t\tactive;\n\n\t/* no agfl header on v4 supers */\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn false;\n\n\t/*\n\t * The agf read verifier catches severe corruption of these fields.\n\t * Repeat some sanity checks to cover a packed -> unpacked mismatch if\n\t * the verifier allows it.\n\t */\n\tif (f >= agfl_size || l >= agfl_size)\n\t\treturn true;\n\tif (c > agfl_size)\n\t\treturn true;\n\n\t/*\n\t * Check consistency between the on-disk count and the active range. An\n\t * agfl padding mismatch manifests as an inconsistent flcount.\n\t */\n\tif (c && l >= f)\n\t\tactive = l - f + 1;\n\telse if (c)\n\t\tactive = agfl_size - f + l + 1;\n\telse\n\t\tactive = 0;\n\n\treturn active != c;\n}\n\n/*\n * Reset the agfl to an empty state. Ignore/drop any existing blocks since the\n * agfl content cannot be trusted. Warn the user that a repair is required to\n * recover leaked blocks.\n *\n * The purpose of this mechanism is to handle filesystems affected by the agfl\n * header padding mismatch problem. A reset keeps the filesystem online with a\n * relatively minor free space accounting inconsistency rather than suffer the\n * inevitable crash from use of an invalid agfl block.\n */\nstatic void\nxfs_agfl_reset(\n\tstruct xfs_trans\t*tp,\n\tstruct xfs_buf\t\t*agbp,\n\tstruct xfs_perag\t*pag)\n{\n\tstruct xfs_mount\t*mp = tp->t_mountp;\n\tstruct xfs_agf\t\t*agf = XFS_BUF_TO_AGF(agbp);\n\n\tASSERT(pag->pagf_agflreset);\n\ttrace_xfs_agfl_reset(mp, agf, 0, _RET_IP_);\n\n\txfs_warn(mp,\n\t       \"WARNING: Reset corrupted AGFL on AG %u. %d blocks leaked. \"\n\t       \"Please unmount and run xfs_repair.\",\n\t         pag->pag_agno, pag->pagf_flcount);\n\n\tagf->agf_flfirst = 0;\n\tagf->agf_fllast = cpu_to_be32(xfs_agfl_size(mp) - 1);\n\tagf->agf_flcount = 0;\n\txfs_alloc_log_agf(tp, agbp, XFS_AGF_FLFIRST | XFS_AGF_FLLAST |\n\t\t\t\t    XFS_AGF_FLCOUNT);\n\n\tpag->pagf_flcount = 0;\n\tpag->pagf_agflreset = false;\n}\n\n/*\n * Defer an AGFL block free. This is effectively equivalent to\n * xfs_bmap_add_free() with some special handling particular to AGFL blocks.\n *\n * Deferring AGFL frees helps prevent log reservation overruns due to too many\n * allocation operations in a transaction. AGFL frees are prone to this problem\n * because for one they are always freed one at a time. Further, an immediate\n * AGFL block free can cause a btree join and require another block free before\n * the real allocation can proceed. Deferring the free disconnects freeing up\n * the AGFL slot from freeing the block.\n */\nSTATIC void\nxfs_defer_agfl_block(\n\tstruct xfs_trans\t\t*tp,\n\txfs_agnumber_t\t\t\tagno,\n\txfs_fsblock_t\t\t\tagbno,\n\tstruct xfs_owner_info\t\t*oinfo)\n{\n\tstruct xfs_mount\t\t*mp = tp->t_mountp;\n\tstruct xfs_extent_free_item\t*new;\t\t/* new element */\n\n\tASSERT(xfs_bmap_free_item_zone != NULL);\n\tASSERT(oinfo != NULL);\n\n\tnew = kmem_zone_alloc(xfs_bmap_free_item_zone, 0);\n\tnew->xefi_startblock = XFS_AGB_TO_FSB(mp, agno, agbno);\n\tnew->xefi_blockcount = 1;\n\tnew->xefi_oinfo = *oinfo;\n\n\ttrace_xfs_agfl_free_defer(mp, agno, 0, agbno, 1);\n\n\txfs_defer_add(tp, XFS_DEFER_OPS_TYPE_AGFL_FREE, &new->xefi_list);\n}\n\n/*\n * Decide whether to use this allocation group for this allocation.\n * If so, fix up the btree freelist's size.\n */\nint\t\t\t/* error */\nxfs_alloc_fix_freelist(\n\tstruct xfs_alloc_arg\t*args,\t/* allocation argument structure */\n\tint\t\t\tflags)\t/* XFS_ALLOC_FLAG_... */\n{\n\tstruct xfs_mount\t*mp = args->mp;\n\tstruct xfs_perag\t*pag = args->pag;\n\tstruct xfs_trans\t*tp = args->tp;\n\tstruct xfs_buf\t\t*agbp = NULL;\n\tstruct xfs_buf\t\t*agflbp = NULL;\n\tstruct xfs_alloc_arg\ttargs;\t/* local allocation arguments */\n\txfs_agblock_t\t\tbno;\t/* freelist block */\n\txfs_extlen_t\t\tneed;\t/* total blocks needed in freelist */\n\tint\t\t\terror = 0;\n\n\t/* deferred ops (AGFL block frees) require permanent transactions */\n\tASSERT(tp->t_flags & XFS_TRANS_PERM_LOG_RES);\n\n\tif (!pag->pagf_init) {\n\t\terror = xfs_alloc_read_agf(mp, tp, args->agno, flags, &agbp);\n\t\tif (error) {\n\t\t\t/* Couldn't lock the AGF so skip this AG. */\n\t\t\tif (error == -EAGAIN)\n\t\t\t\terror = 0;\n\t\t\tgoto out_no_agbp;\n\t\t}\n\t}\n\n\t/*\n\t * If this is a metadata preferred pag and we are user data then try\n\t * somewhere else if we are not being asked to try harder at this\n\t * point\n\t */\n\tif (pag->pagf_metadata && (args->datatype & XFS_ALLOC_USERDATA) &&\n\t    (flags & XFS_ALLOC_FLAG_TRYLOCK)) {\n\t\tASSERT(!(flags & XFS_ALLOC_FLAG_FREEING));\n\t\tgoto out_agbp_relse;\n\t}\n\n\tneed = xfs_alloc_min_freelist(mp, pag);\n\tif (!xfs_alloc_space_available(args, need, flags |\n\t\t\tXFS_ALLOC_FLAG_CHECK))\n\t\tgoto out_agbp_relse;\n\n\t/*\n\t * Get the a.g. freespace buffer.\n\t * Can fail if we're not blocking on locks, and it's held.\n\t */\n\tif (!agbp) {\n\t\terror = xfs_alloc_read_agf(mp, tp, args->agno, flags, &agbp);\n\t\tif (error) {\n\t\t\t/* Couldn't lock the AGF so skip this AG. */\n\t\t\tif (error == -EAGAIN)\n\t\t\t\terror = 0;\n\t\t\tgoto out_no_agbp;\n\t\t}\n\t}\n\n\t/* reset a padding mismatched agfl before final free space check */\n\tif (pag->pagf_agflreset)\n\t\txfs_agfl_reset(tp, agbp, pag);\n\n\t/* If there isn't enough total space or single-extent, reject it. */\n\tneed = xfs_alloc_min_freelist(mp, pag);\n\tif (!xfs_alloc_space_available(args, need, flags))\n\t\tgoto out_agbp_relse;\n\n\t/*\n\t * Make the freelist shorter if it's too long.\n\t *\n\t * Note that from this point onwards, we will always release the agf and\n\t * agfl buffers on error. This handles the case where we error out and\n\t * the buffers are clean or may not have been joined to the transaction\n\t * and hence need to be released manually. If they have been joined to\n\t * the transaction, then xfs_trans_brelse() will handle them\n\t * appropriately based on the recursion count and dirty state of the\n\t * buffer.\n\t *\n\t * XXX (dgc): When we have lots of free space, does this buy us\n\t * anything other than extra overhead when we need to put more blocks\n\t * back on the free list? Maybe we should only do this when space is\n\t * getting low or the AGFL is more than half full?\n\t *\n\t * The NOSHRINK flag prevents the AGFL from being shrunk if it's too\n\t * big; the NORMAP flag prevents AGFL expand/shrink operations from\n\t * updating the rmapbt.  Both flags are used in xfs_repair while we're\n\t * rebuilding the rmapbt, and neither are used by the kernel.  They're\n\t * both required to ensure that rmaps are correctly recorded for the\n\t * regenerated AGFL, bnobt, and cntbt.  See repair/phase5.c and\n\t * repair/rmap.c in xfsprogs for details.\n\t */\n\tmemset(&targs, 0, sizeof(targs));\n\t/* struct copy below */\n\tif (flags & XFS_ALLOC_FLAG_NORMAP)\n\t\ttargs.oinfo = XFS_RMAP_OINFO_SKIP_UPDATE;\n\telse\n\t\ttargs.oinfo = XFS_RMAP_OINFO_AG;\n\twhile (!(flags & XFS_ALLOC_FLAG_NOSHRINK) && pag->pagf_flcount > need) {\n\t\terror = xfs_alloc_get_freelist(tp, agbp, &bno, 0);\n\t\tif (error)\n\t\t\tgoto out_agbp_relse;\n\n\t\t/* defer agfl frees */\n\t\txfs_defer_agfl_block(tp, args->agno, bno, &targs.oinfo);\n\t}\n\n\ttargs.tp = tp;\n\ttargs.mp = mp;\n\ttargs.agbp = agbp;\n\ttargs.agno = args->agno;\n\ttargs.alignment = targs.minlen = targs.prod = 1;\n\ttargs.type = XFS_ALLOCTYPE_THIS_AG;\n\ttargs.pag = pag;\n\terror = xfs_alloc_read_agfl(mp, tp, targs.agno, &agflbp);\n\tif (error)\n\t\tgoto out_agbp_relse;\n\n\t/* Make the freelist longer if it's too short. */\n\twhile (pag->pagf_flcount < need) {\n\t\ttargs.agbno = 0;\n\t\ttargs.maxlen = need - pag->pagf_flcount;\n\t\ttargs.resv = XFS_AG_RESV_AGFL;\n\n\t\t/* Allocate as many blocks as possible at once. */\n\t\terror = xfs_alloc_ag_vextent(&targs);\n\t\tif (error)\n\t\t\tgoto out_agflbp_relse;\n\n\t\t/*\n\t\t * Stop if we run out.  Won't happen if callers are obeying\n\t\t * the restrictions correctly.  Can happen for free calls\n\t\t * on a completely full ag.\n\t\t */\n\t\tif (targs.agbno == NULLAGBLOCK) {\n\t\t\tif (flags & XFS_ALLOC_FLAG_FREEING)\n\t\t\t\tbreak;\n\t\t\tgoto out_agflbp_relse;\n\t\t}\n\t\t/*\n\t\t * Put each allocated block on the list.\n\t\t */\n\t\tfor (bno = targs.agbno; bno < targs.agbno + targs.len; bno++) {\n\t\t\terror = xfs_alloc_put_freelist(tp, agbp,\n\t\t\t\t\t\t\tagflbp, bno, 0);\n\t\t\tif (error)\n\t\t\t\tgoto out_agflbp_relse;\n\t\t}\n\t}\n\txfs_trans_brelse(tp, agflbp);\n\targs->agbp = agbp;\n\treturn 0;\n\nout_agflbp_relse:\n\txfs_trans_brelse(tp, agflbp);\nout_agbp_relse:\n\tif (agbp)\n\t\txfs_trans_brelse(tp, agbp);\nout_no_agbp:\n\targs->agbp = NULL;\n\treturn error;\n}\n\n/*\n * Get a block from the freelist.\n * Returns with the buffer for the block gotten.\n */\nint\t\t\t\t/* error */\nxfs_alloc_get_freelist(\n\txfs_trans_t\t*tp,\t/* transaction pointer */\n\txfs_buf_t\t*agbp,\t/* buffer containing the agf structure */\n\txfs_agblock_t\t*bnop,\t/* block address retrieved from freelist */\n\tint\t\tbtreeblk) /* destination is a AGF btree */\n{\n\txfs_agf_t\t*agf;\t/* a.g. freespace structure */\n\txfs_buf_t\t*agflbp;/* buffer for a.g. freelist structure */\n\txfs_agblock_t\tbno;\t/* block number returned */\n\t__be32\t\t*agfl_bno;\n\tint\t\terror;\n\tint\t\tlogflags;\n\txfs_mount_t\t*mp = tp->t_mountp;\n\txfs_perag_t\t*pag;\t/* per allocation group data */\n\n\t/*\n\t * Freelist is empty, give up.\n\t */\n\tagf = XFS_BUF_TO_AGF(agbp);\n\tif (!agf->agf_flcount) {\n\t\t*bnop = NULLAGBLOCK;\n\t\treturn 0;\n\t}\n\t/*\n\t * Read the array of free blocks.\n\t */\n\terror = xfs_alloc_read_agfl(mp, tp, be32_to_cpu(agf->agf_seqno),\n\t\t\t\t    &agflbp);\n\tif (error)\n\t\treturn error;\n\n\n\t/*\n\t * Get the block number and update the data structures.\n\t */\n\tagfl_bno = XFS_BUF_TO_AGFL_BNO(mp, agflbp);\n\tbno = be32_to_cpu(agfl_bno[be32_to_cpu(agf->agf_flfirst)]);\n\tbe32_add_cpu(&agf->agf_flfirst, 1);\n\txfs_trans_brelse(tp, agflbp);\n\tif (be32_to_cpu(agf->agf_flfirst) == xfs_agfl_size(mp))\n\t\tagf->agf_flfirst = 0;\n\n\tpag = xfs_perag_get(mp, be32_to_cpu(agf->agf_seqno));\n\tASSERT(!pag->pagf_agflreset);\n\tbe32_add_cpu(&agf->agf_flcount, -1);\n\txfs_trans_agflist_delta(tp, -1);\n\tpag->pagf_flcount--;\n\n\tlogflags = XFS_AGF_FLFIRST | XFS_AGF_FLCOUNT;\n\tif (btreeblk) {\n\t\tbe32_add_cpu(&agf->agf_btreeblks, 1);\n\t\tpag->pagf_btreeblks++;\n\t\tlogflags |= XFS_AGF_BTREEBLKS;\n\t}\n\txfs_perag_put(pag);\n\n\txfs_alloc_log_agf(tp, agbp, logflags);\n\t*bnop = bno;\n\n\treturn 0;\n}\n\n/*\n * Log the given fields from the agf structure.\n */\nvoid\nxfs_alloc_log_agf(\n\txfs_trans_t\t*tp,\t/* transaction pointer */\n\txfs_buf_t\t*bp,\t/* buffer for a.g. freelist header */\n\tint\t\tfields)\t/* mask of fields to be logged (XFS_AGF_...) */\n{\n\tint\tfirst;\t\t/* first byte offset */\n\tint\tlast;\t\t/* last byte offset */\n\tstatic const short\toffsets[] = {\n\t\toffsetof(xfs_agf_t, agf_magicnum),\n\t\toffsetof(xfs_agf_t, agf_versionnum),\n\t\toffsetof(xfs_agf_t, agf_seqno),\n\t\toffsetof(xfs_agf_t, agf_length),\n\t\toffsetof(xfs_agf_t, agf_roots[0]),\n\t\toffsetof(xfs_agf_t, agf_levels[0]),\n\t\toffsetof(xfs_agf_t, agf_flfirst),\n\t\toffsetof(xfs_agf_t, agf_fllast),\n\t\toffsetof(xfs_agf_t, agf_flcount),\n\t\toffsetof(xfs_agf_t, agf_freeblks),\n\t\toffsetof(xfs_agf_t, agf_longest),\n\t\toffsetof(xfs_agf_t, agf_btreeblks),\n\t\toffsetof(xfs_agf_t, agf_uuid),\n\t\toffsetof(xfs_agf_t, agf_rmap_blocks),\n\t\toffsetof(xfs_agf_t, agf_refcount_blocks),\n\t\toffsetof(xfs_agf_t, agf_refcount_root),\n\t\toffsetof(xfs_agf_t, agf_refcount_level),\n\t\t/* needed so that we don't log the whole rest of the structure: */\n\t\toffsetof(xfs_agf_t, agf_spare64),\n\t\tsizeof(xfs_agf_t)\n\t};\n\n\ttrace_xfs_agf(tp->t_mountp, XFS_BUF_TO_AGF(bp), fields, _RET_IP_);\n\n\txfs_trans_buf_set_type(tp, bp, XFS_BLFT_AGF_BUF);\n\n\txfs_btree_offsets(fields, offsets, XFS_AGF_NUM_BITS, &first, &last);\n\txfs_trans_log_buf(tp, bp, (uint)first, (uint)last);\n}\n\n/*\n * Interface for inode allocation to force the pag data to be initialized.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_pagf_init(\n\txfs_mount_t\t\t*mp,\t/* file system mount structure */\n\txfs_trans_t\t\t*tp,\t/* transaction pointer */\n\txfs_agnumber_t\t\tagno,\t/* allocation group number */\n\tint\t\t\tflags)\t/* XFS_ALLOC_FLAGS_... */\n{\n\txfs_buf_t\t\t*bp;\n\tint\t\t\terror;\n\n\terror = xfs_alloc_read_agf(mp, tp, agno, flags, &bp);\n\tif (!error)\n\t\txfs_trans_brelse(tp, bp);\n\treturn error;\n}\n\n/*\n * Put the block on the freelist for the allocation group.\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_put_freelist(\n\txfs_trans_t\t\t*tp,\t/* transaction pointer */\n\txfs_buf_t\t\t*agbp,\t/* buffer for a.g. freelist header */\n\txfs_buf_t\t\t*agflbp,/* buffer for a.g. free block array */\n\txfs_agblock_t\t\tbno,\t/* block being freed */\n\tint\t\t\tbtreeblk) /* block came from a AGF btree */\n{\n\txfs_agf_t\t\t*agf;\t/* a.g. freespace structure */\n\t__be32\t\t\t*blockp;/* pointer to array entry */\n\tint\t\t\terror;\n\tint\t\t\tlogflags;\n\txfs_mount_t\t\t*mp;\t/* mount structure */\n\txfs_perag_t\t\t*pag;\t/* per allocation group data */\n\t__be32\t\t\t*agfl_bno;\n\tint\t\t\tstartoff;\n\n\tagf = XFS_BUF_TO_AGF(agbp);\n\tmp = tp->t_mountp;\n\n\tif (!agflbp && (error = xfs_alloc_read_agfl(mp, tp,\n\t\t\tbe32_to_cpu(agf->agf_seqno), &agflbp)))\n\t\treturn error;\n\tbe32_add_cpu(&agf->agf_fllast, 1);\n\tif (be32_to_cpu(agf->agf_fllast) == xfs_agfl_size(mp))\n\t\tagf->agf_fllast = 0;\n\n\tpag = xfs_perag_get(mp, be32_to_cpu(agf->agf_seqno));\n\tASSERT(!pag->pagf_agflreset);\n\tbe32_add_cpu(&agf->agf_flcount, 1);\n\txfs_trans_agflist_delta(tp, 1);\n\tpag->pagf_flcount++;\n\n\tlogflags = XFS_AGF_FLLAST | XFS_AGF_FLCOUNT;\n\tif (btreeblk) {\n\t\tbe32_add_cpu(&agf->agf_btreeblks, -1);\n\t\tpag->pagf_btreeblks--;\n\t\tlogflags |= XFS_AGF_BTREEBLKS;\n\t}\n\txfs_perag_put(pag);\n\n\txfs_alloc_log_agf(tp, agbp, logflags);\n\n\tASSERT(be32_to_cpu(agf->agf_flcount) <= xfs_agfl_size(mp));\n\n\tagfl_bno = XFS_BUF_TO_AGFL_BNO(mp, agflbp);\n\tblockp = &agfl_bno[be32_to_cpu(agf->agf_fllast)];\n\t*blockp = cpu_to_be32(bno);\n\tstartoff = (char *)blockp - (char *)agflbp->b_addr;\n\n\txfs_alloc_log_agf(tp, agbp, logflags);\n\n\txfs_trans_buf_set_type(tp, agflbp, XFS_BLFT_AGFL_BUF);\n\txfs_trans_log_buf(tp, agflbp, startoff,\n\t\t\t  startoff + sizeof(xfs_agblock_t) - 1);\n\treturn 0;\n}\n\nstatic xfs_failaddr_t\nxfs_agf_verify(\n\tstruct xfs_buf\t\t*bp)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tstruct xfs_agf\t\t*agf = XFS_BUF_TO_AGF(bp);\n\n\tif (xfs_sb_version_hascrc(&mp->m_sb)) {\n\t\tif (!uuid_equal(&agf->agf_uuid, &mp->m_sb.sb_meta_uuid))\n\t\t\treturn __this_address;\n\t\tif (!xfs_log_check_lsn(mp,\n\t\t\t\tbe64_to_cpu(XFS_BUF_TO_AGF(bp)->agf_lsn)))\n\t\t\treturn __this_address;\n\t}\n\n\tif (!xfs_verify_magic(bp, agf->agf_magicnum))\n\t\treturn __this_address;\n\n\tif (!(XFS_AGF_GOOD_VERSION(be32_to_cpu(agf->agf_versionnum)) &&\n\t      be32_to_cpu(agf->agf_freeblks) <= be32_to_cpu(agf->agf_length) &&\n\t      be32_to_cpu(agf->agf_flfirst) < xfs_agfl_size(mp) &&\n\t      be32_to_cpu(agf->agf_fllast) < xfs_agfl_size(mp) &&\n\t      be32_to_cpu(agf->agf_flcount) <= xfs_agfl_size(mp)))\n\t\treturn __this_address;\n\n\tif (be32_to_cpu(agf->agf_length) > mp->m_sb.sb_dblocks)\n\t\treturn __this_address;\n\n\tif (be32_to_cpu(agf->agf_freeblks) < be32_to_cpu(agf->agf_longest) ||\n\t    be32_to_cpu(agf->agf_freeblks) > be32_to_cpu(agf->agf_length))\n\t\treturn __this_address;\n\n\tif (be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNO]) < 1 ||\n\t    be32_to_cpu(agf->agf_levels[XFS_BTNUM_CNT]) < 1 ||\n\t    be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNO]) > XFS_BTREE_MAXLEVELS ||\n\t    be32_to_cpu(agf->agf_levels[XFS_BTNUM_CNT]) > XFS_BTREE_MAXLEVELS)\n\t\treturn __this_address;\n\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb) &&\n\t    (be32_to_cpu(agf->agf_levels[XFS_BTNUM_RMAP]) < 1 ||\n\t     be32_to_cpu(agf->agf_levels[XFS_BTNUM_RMAP]) > XFS_BTREE_MAXLEVELS))\n\t\treturn __this_address;\n\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb) &&\n\t    be32_to_cpu(agf->agf_rmap_blocks) > be32_to_cpu(agf->agf_length))\n\t\treturn __this_address;\n\n\t/*\n\t * during growfs operations, the perag is not fully initialised,\n\t * so we can't use it for any useful checking. growfs ensures we can't\n\t * use it by using uncached buffers that don't have the perag attached\n\t * so we can detect and avoid this problem.\n\t */\n\tif (bp->b_pag && be32_to_cpu(agf->agf_seqno) != bp->b_pag->pag_agno)\n\t\treturn __this_address;\n\n\tif (xfs_sb_version_haslazysbcount(&mp->m_sb) &&\n\t    be32_to_cpu(agf->agf_btreeblks) > be32_to_cpu(agf->agf_length))\n\t\treturn __this_address;\n\n\tif (xfs_sb_version_hasreflink(&mp->m_sb) &&\n\t    be32_to_cpu(agf->agf_refcount_blocks) >\n\t    be32_to_cpu(agf->agf_length))\n\t\treturn __this_address;\n\n\tif (xfs_sb_version_hasreflink(&mp->m_sb) &&\n\t    (be32_to_cpu(agf->agf_refcount_level) < 1 ||\n\t     be32_to_cpu(agf->agf_refcount_level) > XFS_BTREE_MAXLEVELS))\n\t\treturn __this_address;\n\n\treturn NULL;\n\n}\n\nstatic void\nxfs_agf_read_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount *mp = bp->b_mount;\n\txfs_failaddr_t\tfa;\n\n\tif (xfs_sb_version_hascrc(&mp->m_sb) &&\n\t    !xfs_buf_verify_cksum(bp, XFS_AGF_CRC_OFF))\n\t\txfs_verifier_error(bp, -EFSBADCRC, __this_address);\n\telse {\n\t\tfa = xfs_agf_verify(bp);\n\t\tif (XFS_TEST_ERROR(fa, mp, XFS_ERRTAG_ALLOC_READ_AGF))\n\t\t\txfs_verifier_error(bp, -EFSCORRUPTED, fa);\n\t}\n}\n\nstatic void\nxfs_agf_write_verify(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tstruct xfs_buf_log_item\t*bip = bp->b_log_item;\n\txfs_failaddr_t\t\tfa;\n\n\tfa = xfs_agf_verify(bp);\n\tif (fa) {\n\t\txfs_verifier_error(bp, -EFSCORRUPTED, fa);\n\t\treturn;\n\t}\n\n\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\treturn;\n\n\tif (bip)\n\t\tXFS_BUF_TO_AGF(bp)->agf_lsn = cpu_to_be64(bip->bli_item.li_lsn);\n\n\txfs_buf_update_cksum(bp, XFS_AGF_CRC_OFF);\n}\n\nconst struct xfs_buf_ops xfs_agf_buf_ops = {\n\t.name = \"xfs_agf\",\n\t.magic = { cpu_to_be32(XFS_AGF_MAGIC), cpu_to_be32(XFS_AGF_MAGIC) },\n\t.verify_read = xfs_agf_read_verify,\n\t.verify_write = xfs_agf_write_verify,\n\t.verify_struct = xfs_agf_verify,\n};\n\n/*\n * Read in the allocation group header (free/alloc section).\n */\nint\t\t\t\t\t/* error */\nxfs_read_agf(\n\tstruct xfs_mount\t*mp,\t/* mount point structure */\n\tstruct xfs_trans\t*tp,\t/* transaction pointer */\n\txfs_agnumber_t\t\tagno,\t/* allocation group number */\n\tint\t\t\tflags,\t/* XFS_BUF_ */\n\tstruct xfs_buf\t\t**bpp)\t/* buffer for the ag freelist header */\n{\n\tint\t\terror;\n\n\ttrace_xfs_read_agf(mp, agno);\n\n\tASSERT(agno != NULLAGNUMBER);\n\terror = xfs_trans_read_buf(mp, tp, mp->m_ddev_targp,\n\t\t\tXFS_AG_DADDR(mp, agno, XFS_AGF_DADDR(mp)),\n\t\t\tXFS_FSS_TO_BB(mp, 1), flags, bpp, &xfs_agf_buf_ops);\n\tif (error)\n\t\treturn error;\n\n\tASSERT(!(*bpp)->b_error);\n\txfs_buf_set_ref(*bpp, XFS_AGF_REF);\n\treturn 0;\n}\n\n/*\n * Read in the allocation group header (free/alloc section).\n */\nint\t\t\t\t\t/* error */\nxfs_alloc_read_agf(\n\tstruct xfs_mount\t*mp,\t/* mount point structure */\n\tstruct xfs_trans\t*tp,\t/* transaction pointer */\n\txfs_agnumber_t\t\tagno,\t/* allocation group number */\n\tint\t\t\tflags,\t/* XFS_ALLOC_FLAG_... */\n\tstruct xfs_buf\t\t**bpp)\t/* buffer for the ag freelist header */\n{\n\tstruct xfs_agf\t\t*agf;\t\t/* ag freelist header */\n\tstruct xfs_perag\t*pag;\t\t/* per allocation group data */\n\tint\t\t\terror;\n\n\ttrace_xfs_alloc_read_agf(mp, agno);\n\n\t/* We don't support trylock when freeing. */\n\tASSERT((flags & (XFS_ALLOC_FLAG_FREEING | XFS_ALLOC_FLAG_TRYLOCK)) !=\n\t\t\t(XFS_ALLOC_FLAG_FREEING | XFS_ALLOC_FLAG_TRYLOCK));\n\tASSERT(agno != NULLAGNUMBER);\n\terror = xfs_read_agf(mp, tp, agno,\n\t\t\t(flags & XFS_ALLOC_FLAG_TRYLOCK) ? XBF_TRYLOCK : 0,\n\t\t\tbpp);\n\tif (error)\n\t\treturn error;\n\tASSERT(!(*bpp)->b_error);\n\n\tagf = XFS_BUF_TO_AGF(*bpp);\n\tpag = xfs_perag_get(mp, agno);\n\tif (!pag->pagf_init) {\n\t\tpag->pagf_freeblks = be32_to_cpu(agf->agf_freeblks);\n\t\tpag->pagf_btreeblks = be32_to_cpu(agf->agf_btreeblks);\n\t\tpag->pagf_flcount = be32_to_cpu(agf->agf_flcount);\n\t\tpag->pagf_longest = be32_to_cpu(agf->agf_longest);\n\t\tpag->pagf_levels[XFS_BTNUM_BNOi] =\n\t\t\tbe32_to_cpu(agf->agf_levels[XFS_BTNUM_BNOi]);\n\t\tpag->pagf_levels[XFS_BTNUM_CNTi] =\n\t\t\tbe32_to_cpu(agf->agf_levels[XFS_BTNUM_CNTi]);\n\t\tpag->pagf_levels[XFS_BTNUM_RMAPi] =\n\t\t\tbe32_to_cpu(agf->agf_levels[XFS_BTNUM_RMAPi]);\n\t\tpag->pagf_refcount_level = be32_to_cpu(agf->agf_refcount_level);\n\t\tpag->pagf_init = 1;\n\t\tpag->pagf_agflreset = xfs_agfl_needs_reset(mp, agf);\n\t}\n#ifdef DEBUG\n\telse if (!XFS_FORCED_SHUTDOWN(mp)) {\n\t\tASSERT(pag->pagf_freeblks == be32_to_cpu(agf->agf_freeblks));\n\t\tASSERT(pag->pagf_btreeblks == be32_to_cpu(agf->agf_btreeblks));\n\t\tASSERT(pag->pagf_flcount == be32_to_cpu(agf->agf_flcount));\n\t\tASSERT(pag->pagf_longest == be32_to_cpu(agf->agf_longest));\n\t\tASSERT(pag->pagf_levels[XFS_BTNUM_BNOi] ==\n\t\t       be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNOi]));\n\t\tASSERT(pag->pagf_levels[XFS_BTNUM_CNTi] ==\n\t\t       be32_to_cpu(agf->agf_levels[XFS_BTNUM_CNTi]));\n\t}\n#endif\n\txfs_perag_put(pag);\n\treturn 0;\n}\n\n/*\n * Allocate an extent (variable-size).\n * Depending on the allocation type, we either look in a single allocation\n * group or loop over the allocation groups to find the result.\n */\nint\t\t\t\t/* error */\nxfs_alloc_vextent(\n\tstruct xfs_alloc_arg\t*args)\t/* allocation argument structure */\n{\n\txfs_agblock_t\t\tagsize;\t/* allocation group size */\n\tint\t\t\terror;\n\tint\t\t\tflags;\t/* XFS_ALLOC_FLAG_... locking flags */\n\tstruct xfs_mount\t*mp;\t/* mount structure pointer */\n\txfs_agnumber_t\t\tsagno;\t/* starting allocation group number */\n\txfs_alloctype_t\t\ttype;\t/* input allocation type */\n\tint\t\t\tbump_rotor = 0;\n\txfs_agnumber_t\t\trotorstep = xfs_rotorstep; /* inode32 agf stepper */\n\n\tmp = args->mp;\n\ttype = args->otype = args->type;\n\targs->agbno = NULLAGBLOCK;\n\t/*\n\t * Just fix this up, for the case where the last a.g. is shorter\n\t * (or there's only one a.g.) and the caller couldn't easily figure\n\t * that out (xfs_bmap_alloc).\n\t */\n\tagsize = mp->m_sb.sb_agblocks;\n\tif (args->maxlen > agsize)\n\t\targs->maxlen = agsize;\n\tif (args->alignment == 0)\n\t\targs->alignment = 1;\n\tASSERT(XFS_FSB_TO_AGNO(mp, args->fsbno) < mp->m_sb.sb_agcount);\n\tASSERT(XFS_FSB_TO_AGBNO(mp, args->fsbno) < agsize);\n\tASSERT(args->minlen <= args->maxlen);\n\tASSERT(args->minlen <= agsize);\n\tASSERT(args->mod < args->prod);\n\tif (XFS_FSB_TO_AGNO(mp, args->fsbno) >= mp->m_sb.sb_agcount ||\n\t    XFS_FSB_TO_AGBNO(mp, args->fsbno) >= agsize ||\n\t    args->minlen > args->maxlen || args->minlen > agsize ||\n\t    args->mod >= args->prod) {\n\t\targs->fsbno = NULLFSBLOCK;\n\t\ttrace_xfs_alloc_vextent_badargs(args);\n\t\treturn 0;\n\t}\n\n\tswitch (type) {\n\tcase XFS_ALLOCTYPE_THIS_AG:\n\tcase XFS_ALLOCTYPE_NEAR_BNO:\n\tcase XFS_ALLOCTYPE_THIS_BNO:\n\t\t/*\n\t\t * These three force us into a single a.g.\n\t\t */\n\t\targs->agno = XFS_FSB_TO_AGNO(mp, args->fsbno);\n\t\targs->pag = xfs_perag_get(mp, args->agno);\n\t\terror = xfs_alloc_fix_freelist(args, 0);\n\t\tif (error) {\n\t\t\ttrace_xfs_alloc_vextent_nofix(args);\n\t\t\tgoto error0;\n\t\t}\n\t\tif (!args->agbp) {\n\t\t\ttrace_xfs_alloc_vextent_noagbp(args);\n\t\t\tbreak;\n\t\t}\n\t\targs->agbno = XFS_FSB_TO_AGBNO(mp, args->fsbno);\n\t\tif ((error = xfs_alloc_ag_vextent(args)))\n\t\t\tgoto error0;\n\t\tbreak;\n\tcase XFS_ALLOCTYPE_START_BNO:\n\t\t/*\n\t\t * Try near allocation first, then anywhere-in-ag after\n\t\t * the first a.g. fails.\n\t\t */\n\t\tif ((args->datatype & XFS_ALLOC_INITIAL_USER_DATA) &&\n\t\t    (mp->m_flags & XFS_MOUNT_32BITINODES)) {\n\t\t\targs->fsbno = XFS_AGB_TO_FSB(mp,\n\t\t\t\t\t((mp->m_agfrotor / rotorstep) %\n\t\t\t\t\tmp->m_sb.sb_agcount), 0);\n\t\t\tbump_rotor = 1;\n\t\t}\n\t\targs->agbno = XFS_FSB_TO_AGBNO(mp, args->fsbno);\n\t\targs->type = XFS_ALLOCTYPE_NEAR_BNO;\n\t\t/* FALLTHROUGH */\n\tcase XFS_ALLOCTYPE_FIRST_AG:\n\t\t/*\n\t\t * Rotate through the allocation groups looking for a winner.\n\t\t */\n\t\tif (type == XFS_ALLOCTYPE_FIRST_AG) {\n\t\t\t/*\n\t\t\t * Start with allocation group given by bno.\n\t\t\t */\n\t\t\targs->agno = XFS_FSB_TO_AGNO(mp, args->fsbno);\n\t\t\targs->type = XFS_ALLOCTYPE_THIS_AG;\n\t\t\tsagno = 0;\n\t\t\tflags = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Start with the given allocation group.\n\t\t\t */\n\t\t\targs->agno = sagno = XFS_FSB_TO_AGNO(mp, args->fsbno);\n\t\t\tflags = XFS_ALLOC_FLAG_TRYLOCK;\n\t\t}\n\t\t/*\n\t\t * Loop over allocation groups twice; first time with\n\t\t * trylock set, second time without.\n\t\t */\n\t\tfor (;;) {\n\t\t\targs->pag = xfs_perag_get(mp, args->agno);\n\t\t\terror = xfs_alloc_fix_freelist(args, flags);\n\t\t\tif (error) {\n\t\t\t\ttrace_xfs_alloc_vextent_nofix(args);\n\t\t\t\tgoto error0;\n\t\t\t}\n\t\t\t/*\n\t\t\t * If we get a buffer back then the allocation will fly.\n\t\t\t */\n\t\t\tif (args->agbp) {\n\t\t\t\tif ((error = xfs_alloc_ag_vextent(args)))\n\t\t\t\t\tgoto error0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\ttrace_xfs_alloc_vextent_loopfailed(args);\n\n\t\t\t/*\n\t\t\t * Didn't work, figure out the next iteration.\n\t\t\t */\n\t\t\tif (args->agno == sagno &&\n\t\t\t    type == XFS_ALLOCTYPE_START_BNO)\n\t\t\t\targs->type = XFS_ALLOCTYPE_THIS_AG;\n\t\t\t/*\n\t\t\t* For the first allocation, we can try any AG to get\n\t\t\t* space.  However, if we already have allocated a\n\t\t\t* block, we don't want to try AGs whose number is below\n\t\t\t* sagno. Otherwise, we may end up with out-of-order\n\t\t\t* locking of AGF, which might cause deadlock.\n\t\t\t*/\n\t\t\tif (++(args->agno) == mp->m_sb.sb_agcount) {\n\t\t\t\tif (args->tp->t_firstblock != NULLFSBLOCK)\n\t\t\t\t\targs->agno = sagno;\n\t\t\t\telse\n\t\t\t\t\targs->agno = 0;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Reached the starting a.g., must either be done\n\t\t\t * or switch to non-trylock mode.\n\t\t\t */\n\t\t\tif (args->agno == sagno) {\n\t\t\t\tif (flags == 0) {\n\t\t\t\t\targs->agbno = NULLAGBLOCK;\n\t\t\t\t\ttrace_xfs_alloc_vextent_allfailed(args);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tflags = 0;\n\t\t\t\tif (type == XFS_ALLOCTYPE_START_BNO) {\n\t\t\t\t\targs->agbno = XFS_FSB_TO_AGBNO(mp,\n\t\t\t\t\t\targs->fsbno);\n\t\t\t\t\targs->type = XFS_ALLOCTYPE_NEAR_BNO;\n\t\t\t\t}\n\t\t\t}\n\t\t\txfs_perag_put(args->pag);\n\t\t}\n\t\tif (bump_rotor) {\n\t\t\tif (args->agno == sagno)\n\t\t\t\tmp->m_agfrotor = (mp->m_agfrotor + 1) %\n\t\t\t\t\t(mp->m_sb.sb_agcount * rotorstep);\n\t\t\telse\n\t\t\t\tmp->m_agfrotor = (args->agno * rotorstep + 1) %\n\t\t\t\t\t(mp->m_sb.sb_agcount * rotorstep);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tASSERT(0);\n\t\t/* NOTREACHED */\n\t}\n\tif (args->agbno == NULLAGBLOCK)\n\t\targs->fsbno = NULLFSBLOCK;\n\telse {\n\t\targs->fsbno = XFS_AGB_TO_FSB(mp, args->agno, args->agbno);\n#ifdef DEBUG\n\t\tASSERT(args->len >= args->minlen);\n\t\tASSERT(args->len <= args->maxlen);\n\t\tASSERT(args->agbno % args->alignment == 0);\n\t\tXFS_AG_CHECK_DADDR(mp, XFS_FSB_TO_DADDR(mp, args->fsbno),\n\t\t\targs->len);\n#endif\n\n\t}\n\txfs_perag_put(args->pag);\n\treturn 0;\nerror0:\n\txfs_perag_put(args->pag);\n\treturn error;\n}\n\n/* Ensure that the freelist is at full capacity. */\nint\nxfs_free_extent_fix_freelist(\n\tstruct xfs_trans\t*tp,\n\txfs_agnumber_t\t\tagno,\n\tstruct xfs_buf\t\t**agbp)\n{\n\tstruct xfs_alloc_arg\targs;\n\tint\t\t\terror;\n\n\tmemset(&args, 0, sizeof(struct xfs_alloc_arg));\n\targs.tp = tp;\n\targs.mp = tp->t_mountp;\n\targs.agno = agno;\n\n\t/*\n\t * validate that the block number is legal - the enables us to detect\n\t * and handle a silent filesystem corruption rather than crashing.\n\t */\n\tif (args.agno >= args.mp->m_sb.sb_agcount)\n\t\treturn -EFSCORRUPTED;\n\n\targs.pag = xfs_perag_get(args.mp, args.agno);\n\tASSERT(args.pag);\n\n\terror = xfs_alloc_fix_freelist(&args, XFS_ALLOC_FLAG_FREEING);\n\tif (error)\n\t\tgoto out;\n\n\t*agbp = args.agbp;\nout:\n\txfs_perag_put(args.pag);\n\treturn error;\n}\n\n/*\n * Free an extent.\n * Just break up the extent address and hand off to xfs_free_ag_extent\n * after fixing up the freelist.\n */\nint\n__xfs_free_extent(\n\tstruct xfs_trans\t\t*tp,\n\txfs_fsblock_t\t\t\tbno,\n\txfs_extlen_t\t\t\tlen,\n\tconst struct xfs_owner_info\t*oinfo,\n\tenum xfs_ag_resv_type\t\ttype,\n\tbool\t\t\t\tskip_discard)\n{\n\tstruct xfs_mount\t\t*mp = tp->t_mountp;\n\tstruct xfs_buf\t\t\t*agbp;\n\txfs_agnumber_t\t\t\tagno = XFS_FSB_TO_AGNO(mp, bno);\n\txfs_agblock_t\t\t\tagbno = XFS_FSB_TO_AGBNO(mp, bno);\n\tint\t\t\t\terror;\n\tunsigned int\t\t\tbusy_flags = 0;\n\n\tASSERT(len != 0);\n\tASSERT(type != XFS_AG_RESV_AGFL);\n\n\tif (XFS_TEST_ERROR(false, mp,\n\t\t\tXFS_ERRTAG_FREE_EXTENT))\n\t\treturn -EIO;\n\n\terror = xfs_free_extent_fix_freelist(tp, agno, &agbp);\n\tif (error)\n\t\treturn error;\n\n\tif (XFS_IS_CORRUPT(mp, agbno >= mp->m_sb.sb_agblocks)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto err;\n\t}\n\n\t/* validate the extent size is legal now we have the agf locked */\n\tif (XFS_IS_CORRUPT(mp,\n\t\t\t   agbno + len >\n\t\t\t   be32_to_cpu(XFS_BUF_TO_AGF(agbp)->agf_length))) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto err;\n\t}\n\n\terror = xfs_free_ag_extent(tp, agbp, agno, agbno, len, oinfo, type);\n\tif (error)\n\t\tgoto err;\n\n\tif (skip_discard)\n\t\tbusy_flags |= XFS_EXTENT_BUSY_SKIP_DISCARD;\n\txfs_extent_busy_insert(tp, agno, agbno, len, busy_flags);\n\treturn 0;\n\nerr:\n\txfs_trans_brelse(tp, agbp);\n\treturn error;\n}\n\nstruct xfs_alloc_query_range_info {\n\txfs_alloc_query_range_fn\tfn;\n\tvoid\t\t\t\t*priv;\n};\n\n/* Format btree record and pass to our callback. */\nSTATIC int\nxfs_alloc_query_range_helper(\n\tstruct xfs_btree_cur\t\t*cur,\n\tunion xfs_btree_rec\t\t*rec,\n\tvoid\t\t\t\t*priv)\n{\n\tstruct xfs_alloc_query_range_info\t*query = priv;\n\tstruct xfs_alloc_rec_incore\t\tirec;\n\n\tirec.ar_startblock = be32_to_cpu(rec->alloc.ar_startblock);\n\tirec.ar_blockcount = be32_to_cpu(rec->alloc.ar_blockcount);\n\treturn query->fn(cur, &irec, query->priv);\n}\n\n/* Find all free space within a given range of blocks. */\nint\nxfs_alloc_query_range(\n\tstruct xfs_btree_cur\t\t\t*cur,\n\tstruct xfs_alloc_rec_incore\t\t*low_rec,\n\tstruct xfs_alloc_rec_incore\t\t*high_rec,\n\txfs_alloc_query_range_fn\t\tfn,\n\tvoid\t\t\t\t\t*priv)\n{\n\tunion xfs_btree_irec\t\t\tlow_brec;\n\tunion xfs_btree_irec\t\t\thigh_brec;\n\tstruct xfs_alloc_query_range_info\tquery;\n\n\tASSERT(cur->bc_btnum == XFS_BTNUM_BNO);\n\tlow_brec.a = *low_rec;\n\thigh_brec.a = *high_rec;\n\tquery.priv = priv;\n\tquery.fn = fn;\n\treturn xfs_btree_query_range(cur, &low_brec, &high_brec,\n\t\t\txfs_alloc_query_range_helper, &query);\n}\n\n/* Find all free space records. */\nint\nxfs_alloc_query_all(\n\tstruct xfs_btree_cur\t\t\t*cur,\n\txfs_alloc_query_range_fn\t\tfn,\n\tvoid\t\t\t\t\t*priv)\n{\n\tstruct xfs_alloc_query_range_info\tquery;\n\n\tASSERT(cur->bc_btnum == XFS_BTNUM_BNO);\n\tquery.priv = priv;\n\tquery.fn = fn;\n\treturn xfs_btree_query_all(cur, xfs_alloc_query_range_helper, &query);\n}\n\n/* Is there a record covering a given extent? */\nint\nxfs_alloc_has_record(\n\tstruct xfs_btree_cur\t*cur,\n\txfs_agblock_t\t\tbno,\n\txfs_extlen_t\t\tlen,\n\tbool\t\t\t*exists)\n{\n\tunion xfs_btree_irec\tlow;\n\tunion xfs_btree_irec\thigh;\n\n\tmemset(&low, 0, sizeof(low));\n\tlow.a.ar_startblock = bno;\n\tmemset(&high, 0xFF, sizeof(high));\n\thigh.a.ar_startblock = bno + len - 1;\n\n\treturn xfs_btree_has_record(cur, &low, &high, exists);\n}\n\n/*\n * Walk all the blocks in the AGFL.  The @walk_fn can return any negative\n * error code or XFS_ITER_*.\n */\nint\nxfs_agfl_walk(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_agf\t\t*agf,\n\tstruct xfs_buf\t\t*agflbp,\n\txfs_agfl_walk_fn\twalk_fn,\n\tvoid\t\t\t*priv)\n{\n\t__be32\t\t\t*agfl_bno;\n\tunsigned int\t\ti;\n\tint\t\t\terror;\n\n\tagfl_bno = XFS_BUF_TO_AGFL_BNO(mp, agflbp);\n\ti = be32_to_cpu(agf->agf_flfirst);\n\n\t/* Nothing to walk in an empty AGFL. */\n\tif (agf->agf_flcount == cpu_to_be32(0))\n\t\treturn 0;\n\n\t/* Otherwise, walk from first to last, wrapping as needed. */\n\tfor (;;) {\n\t\terror = walk_fn(mp, be32_to_cpu(agfl_bno[i]), priv);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (i == be32_to_cpu(agf->agf_fllast))\n\t\t\tbreak;\n\t\tif (++i == xfs_agfl_size(mp))\n\t\t\ti = 0;\n\t}\n\n\treturn 0;\n}\n"], "filenames": ["fs/xfs/libxfs/xfs_alloc.c"], "buggy_code_start_loc": [2860], "buggy_code_end_loc": [2882], "fixing_code_start_loc": [2861], "fixing_code_end_loc": [2899], "type": "CWE-835", "message": "An issue was discovered in xfs_agf_verify in fs/xfs/libxfs/xfs_alloc.c in the Linux kernel through 5.6.10. Attackers may trigger a sync of excessive duration via an XFS v5 image with crafted metadata, aka CID-d0c7feaf8767.", "other": {"cve": {"id": "CVE-2020-12655", "sourceIdentifier": "cve@mitre.org", "published": "2020-05-05T06:15:11.090", "lastModified": "2022-07-12T17:42:04.277", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "An issue was discovered in xfs_agf_verify in fs/xfs/libxfs/xfs_alloc.c in the Linux kernel through 5.6.10. Attackers may trigger a sync of excessive duration via an XFS v5 image with crafted metadata, aka CID-d0c7feaf8767."}, {"lang": "es", "value": "Se detect\u00f3 un problema en la funci\u00f3n xfs_agf_verify en el archivo fs/xfs/libxfs/xfs_alloc.c en el kernel de Linux versiones hasta 5.6.10. Los atacantes pueden desencadenar una sincronizaci\u00f3n de duraci\u00f3n excesiva por medio de una imagen XFS v5 con metadatos dise\u00f1ados, tambi\u00e9n se conoce como CID-d0c7feaf8767."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-835"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "5.6.10", "matchCriteriaId": "10F6CDF5-290A-46E8-B9FF-0FE6D94A946B"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-06/msg00022.html", "source": "cve@mitre.org"}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=d0c7feaf87678371c2c09b3709400be416b2dc62", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/d0c7feaf87678371c2c09b3709400be416b2dc62", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2020/08/msg00019.html", "source": "cve@mitre.org"}, {"url": "https://lists.debian.org/debian-lts-announce/2020/10/msg00032.html", "source": "cve@mitre.org"}, {"url": "https://lists.debian.org/debian-lts-announce/2020/10/msg00034.html", "source": "cve@mitre.org"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/ES5C6ZCMALBEBMKNNCTBSLLSYGFZG3FF/", "source": "cve@mitre.org"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/IO5XIQSRI747P4RVVTNX7TUPEOCF4OPU/", "source": "cve@mitre.org"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/IZ2X3TM6RGRUS3KZAS26IJO5XGU7TBBR/", "source": "cve@mitre.org"}, {"url": "https://lore.kernel.org/linux-xfs/20200221153803.GP9506@magnolia/", "source": "cve@mitre.org", "tags": ["Vendor Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20200608-0001/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4465-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4483-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4485-1/", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/d0c7feaf87678371c2c09b3709400be416b2dc62"}}