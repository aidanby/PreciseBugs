{"buggy_code": ["/*\n * SWIOTLB-based DMA API implementation\n *\n * Copyright (C) 2012 ARM Ltd.\n * Author: Catalin Marinas <catalin.marinas@arm.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\n#include <linux/gfp.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/genalloc.h>\n#include <linux/dma-mapping.h>\n#include <linux/dma-contiguous.h>\n#include <linux/vmalloc.h>\n#include <linux/swiotlb.h>\n\n#include <asm/cacheflush.h>\n\nstruct dma_map_ops *dma_ops;\nEXPORT_SYMBOL(dma_ops);\n\nstatic pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot,\n\t\t\t\t bool coherent)\n{\n\tif (!coherent || dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs))\n\t\treturn pgprot_writecombine(prot);\n\treturn prot;\n}\n\nstatic struct gen_pool *atomic_pool;\n\n#define DEFAULT_DMA_COHERENT_POOL_SIZE  SZ_256K\nstatic size_t atomic_pool_size = DEFAULT_DMA_COHERENT_POOL_SIZE;\n\nstatic int __init early_coherent_pool(char *p)\n{\n\tatomic_pool_size = memparse(p, &p);\n\treturn 0;\n}\nearly_param(\"coherent_pool\", early_coherent_pool);\n\nstatic void *__alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)\n{\n\tunsigned long val;\n\tvoid *ptr = NULL;\n\n\tif (!atomic_pool) {\n\t\tWARN(1, \"coherent pool not initialised!\\n\");\n\t\treturn NULL;\n\t}\n\n\tval = gen_pool_alloc(atomic_pool, size);\n\tif (val) {\n\t\tphys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);\n\n\t\t*ret_page = phys_to_page(phys);\n\t\tptr = (void *)val;\n\t\tif (flags & __GFP_ZERO)\n\t\t\tmemset(ptr, 0, size);\n\t}\n\n\treturn ptr;\n}\n\nstatic bool __in_atomic_pool(void *start, size_t size)\n{\n\treturn addr_in_gen_pool(atomic_pool, (unsigned long)start, size);\n}\n\nstatic int __free_from_pool(void *start, size_t size)\n{\n\tif (!__in_atomic_pool(start, size))\n\t\treturn 0;\n\n\tgen_pool_free(atomic_pool, (unsigned long)start, size);\n\n\treturn 1;\n}\n\nstatic void *__dma_alloc_coherent(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *dma_handle, gfp_t flags,\n\t\t\t\t  struct dma_attrs *attrs)\n{\n\tif (dev == NULL) {\n\t\tWARN_ONCE(1, \"Use an actual device structure for DMA allocation\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (IS_ENABLED(CONFIG_ZONE_DMA) &&\n\t    dev->coherent_dma_mask <= DMA_BIT_MASK(32))\n\t\tflags |= GFP_DMA;\n\tif (IS_ENABLED(CONFIG_DMA_CMA) && (flags & __GFP_WAIT)) {\n\t\tstruct page *page;\n\t\tvoid *addr;\n\n\t\tsize = PAGE_ALIGN(size);\n\t\tpage = dma_alloc_from_contiguous(dev, size >> PAGE_SHIFT,\n\t\t\t\t\t\t\tget_order(size));\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n\t\taddr = page_address(page);\n\t\tif (flags & __GFP_ZERO)\n\t\t\tmemset(addr, 0, size);\n\t\treturn addr;\n\t} else {\n\t\treturn swiotlb_alloc_coherent(dev, size, dma_handle, flags);\n\t}\n}\n\nstatic void __dma_free_coherent(struct device *dev, size_t size,\n\t\t\t\tvoid *vaddr, dma_addr_t dma_handle,\n\t\t\t\tstruct dma_attrs *attrs)\n{\n\tbool freed;\n\tphys_addr_t paddr = dma_to_phys(dev, dma_handle);\n\n\tif (dev == NULL) {\n\t\tWARN_ONCE(1, \"Use an actual device structure for DMA allocation\\n\");\n\t\treturn;\n\t}\n\n\tfreed = dma_release_from_contiguous(dev,\n\t\t\t\t\tphys_to_page(paddr),\n\t\t\t\t\tsize >> PAGE_SHIFT);\n\tif (!freed)\n\t\tswiotlb_free_coherent(dev, size, vaddr, dma_handle);\n}\n\nstatic void *__dma_alloc(struct device *dev, size_t size,\n\t\t\t dma_addr_t *dma_handle, gfp_t flags,\n\t\t\t struct dma_attrs *attrs)\n{\n\tstruct page *page;\n\tvoid *ptr, *coherent_ptr;\n\tbool coherent = is_device_dma_coherent(dev);\n\n\tsize = PAGE_ALIGN(size);\n\n\tif (!coherent && !(flags & __GFP_WAIT)) {\n\t\tstruct page *page = NULL;\n\t\tvoid *addr = __alloc_from_pool(size, &page, flags);\n\n\t\tif (addr)\n\t\t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n\n\t\treturn addr;\n\t}\n\n\tptr = __dma_alloc_coherent(dev, size, dma_handle, flags, attrs);\n\tif (!ptr)\n\t\tgoto no_mem;\n\n\t/* no need for non-cacheable mapping if coherent */\n\tif (coherent)\n\t\treturn ptr;\n\n\t/* remove any dirty cache lines on the kernel alias */\n\t__dma_flush_range(ptr, ptr + size);\n\n\t/* create a coherent mapping */\n\tpage = virt_to_page(ptr);\n\tcoherent_ptr = dma_common_contiguous_remap(page, size, VM_USERMAP,\n\t\t\t\t__get_dma_pgprot(attrs,\n\t\t\t\t\t__pgprot(PROT_NORMAL_NC), false),\n\t\t\t\t\tNULL);\n\tif (!coherent_ptr)\n\t\tgoto no_map;\n\n\treturn coherent_ptr;\n\nno_map:\n\t__dma_free_coherent(dev, size, ptr, *dma_handle, attrs);\nno_mem:\n\t*dma_handle = DMA_ERROR_CODE;\n\treturn NULL;\n}\n\nstatic void __dma_free(struct device *dev, size_t size,\n\t\t       void *vaddr, dma_addr_t dma_handle,\n\t\t       struct dma_attrs *attrs)\n{\n\tvoid *swiotlb_addr = phys_to_virt(dma_to_phys(dev, dma_handle));\n\n\tif (!is_device_dma_coherent(dev)) {\n\t\tif (__free_from_pool(vaddr, size))\n\t\t\treturn;\n\t\tvunmap(vaddr);\n\t}\n\t__dma_free_coherent(dev, size, swiotlb_addr, dma_handle, attrs);\n}\n\nstatic dma_addr_t __swiotlb_map_page(struct device *dev, struct page *page,\n\t\t\t\t     unsigned long offset, size_t size,\n\t\t\t\t     enum dma_data_direction dir,\n\t\t\t\t     struct dma_attrs *attrs)\n{\n\tdma_addr_t dev_addr;\n\n\tdev_addr = swiotlb_map_page(dev, page, offset, size, dir, attrs);\n\tif (!is_device_dma_coherent(dev))\n\t\t__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);\n\n\treturn dev_addr;\n}\n\n\nstatic void __swiotlb_unmap_page(struct device *dev, dma_addr_t dev_addr,\n\t\t\t\t size_t size, enum dma_data_direction dir,\n\t\t\t\t struct dma_attrs *attrs)\n{\n\tif (!is_device_dma_coherent(dev))\n\t\t__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);\n\tswiotlb_unmap_page(dev, dev_addr, size, dir, attrs);\n}\n\nstatic int __swiotlb_map_sg_attrs(struct device *dev, struct scatterlist *sgl,\n\t\t\t\t  int nelems, enum dma_data_direction dir,\n\t\t\t\t  struct dma_attrs *attrs)\n{\n\tstruct scatterlist *sg;\n\tint i, ret;\n\n\tret = swiotlb_map_sg_attrs(dev, sgl, nelems, dir, attrs);\n\tif (!is_device_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, ret, i)\n\t\t\t__dma_map_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),\n\t\t\t\t       sg->length, dir);\n\n\treturn ret;\n}\n\nstatic void __swiotlb_unmap_sg_attrs(struct device *dev,\n\t\t\t\t     struct scatterlist *sgl, int nelems,\n\t\t\t\t     enum dma_data_direction dir,\n\t\t\t\t     struct dma_attrs *attrs)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tif (!is_device_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\t__dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),\n\t\t\t\t\t sg->length, dir);\n\tswiotlb_unmap_sg_attrs(dev, sgl, nelems, dir, attrs);\n}\n\nstatic void __swiotlb_sync_single_for_cpu(struct device *dev,\n\t\t\t\t\t  dma_addr_t dev_addr, size_t size,\n\t\t\t\t\t  enum dma_data_direction dir)\n{\n\tif (!is_device_dma_coherent(dev))\n\t\t__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);\n\tswiotlb_sync_single_for_cpu(dev, dev_addr, size, dir);\n}\n\nstatic void __swiotlb_sync_single_for_device(struct device *dev,\n\t\t\t\t\t     dma_addr_t dev_addr, size_t size,\n\t\t\t\t\t     enum dma_data_direction dir)\n{\n\tswiotlb_sync_single_for_device(dev, dev_addr, size, dir);\n\tif (!is_device_dma_coherent(dev))\n\t\t__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);\n}\n\nstatic void __swiotlb_sync_sg_for_cpu(struct device *dev,\n\t\t\t\t      struct scatterlist *sgl, int nelems,\n\t\t\t\t      enum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tif (!is_device_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\t__dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),\n\t\t\t\t\t sg->length, dir);\n\tswiotlb_sync_sg_for_cpu(dev, sgl, nelems, dir);\n}\n\nstatic void __swiotlb_sync_sg_for_device(struct device *dev,\n\t\t\t\t\t struct scatterlist *sgl, int nelems,\n\t\t\t\t\t enum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tswiotlb_sync_sg_for_device(dev, sgl, nelems, dir);\n\tif (!is_device_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\t__dma_map_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),\n\t\t\t\t       sg->length, dir);\n}\n\n/* vma->vm_page_prot must be set appropriately before calling this function */\nstatic int __dma_common_mmap(struct device *dev, struct vm_area_struct *vma,\n\t\t\t     void *cpu_addr, dma_addr_t dma_addr, size_t size)\n{\n\tint ret = -ENXIO;\n\tunsigned long nr_vma_pages = (vma->vm_end - vma->vm_start) >>\n\t\t\t\t\tPAGE_SHIFT;\n\tunsigned long nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned long pfn = dma_to_phys(dev, dma_addr) >> PAGE_SHIFT;\n\tunsigned long off = vma->vm_pgoff;\n\n\tif (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &ret))\n\t\treturn ret;\n\n\tif (off < nr_pages && nr_vma_pages <= (nr_pages - off)) {\n\t\tret = remap_pfn_range(vma, vma->vm_start,\n\t\t\t\t      pfn + off,\n\t\t\t\t      vma->vm_end - vma->vm_start,\n\t\t\t\t      vma->vm_page_prot);\n\t}\n\n\treturn ret;\n}\n\nstatic int __swiotlb_mmap(struct device *dev,\n\t\t\t  struct vm_area_struct *vma,\n\t\t\t  void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t\t  struct dma_attrs *attrs)\n{\n\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot,\n\t\t\t\t\t     is_device_dma_coherent(dev));\n\treturn __dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);\n}\n\nstatic struct dma_map_ops swiotlb_dma_ops = {\n\t.alloc = __dma_alloc,\n\t.free = __dma_free,\n\t.mmap = __swiotlb_mmap,\n\t.map_page = __swiotlb_map_page,\n\t.unmap_page = __swiotlb_unmap_page,\n\t.map_sg = __swiotlb_map_sg_attrs,\n\t.unmap_sg = __swiotlb_unmap_sg_attrs,\n\t.sync_single_for_cpu = __swiotlb_sync_single_for_cpu,\n\t.sync_single_for_device = __swiotlb_sync_single_for_device,\n\t.sync_sg_for_cpu = __swiotlb_sync_sg_for_cpu,\n\t.sync_sg_for_device = __swiotlb_sync_sg_for_device,\n\t.dma_supported = swiotlb_dma_supported,\n\t.mapping_error = swiotlb_dma_mapping_error,\n};\n\nstatic int __init atomic_pool_init(void)\n{\n\tpgprot_t prot = __pgprot(PROT_NORMAL_NC);\n\tunsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *addr;\n\tunsigned int pool_size_order = get_order(atomic_pool_size);\n\n\tif (dev_get_cma_area(NULL))\n\t\tpage = dma_alloc_from_contiguous(NULL, nr_pages,\n\t\t\t\t\t\t\tpool_size_order);\n\telse\n\t\tpage = alloc_pages(GFP_DMA, pool_size_order);\n\n\tif (page) {\n\t\tint ret;\n\t\tvoid *page_addr = page_address(page);\n\n\t\tmemset(page_addr, 0, atomic_pool_size);\n\t\t__dma_flush_range(page_addr, page_addr + atomic_pool_size);\n\n\t\tatomic_pool = gen_pool_create(PAGE_SHIFT, -1);\n\t\tif (!atomic_pool)\n\t\t\tgoto free_page;\n\n\t\taddr = dma_common_contiguous_remap(page, atomic_pool_size,\n\t\t\t\t\tVM_USERMAP, prot, atomic_pool_init);\n\n\t\tif (!addr)\n\t\t\tgoto destroy_genpool;\n\n\t\tret = gen_pool_add_virt(atomic_pool, (unsigned long)addr,\n\t\t\t\t\tpage_to_phys(page),\n\t\t\t\t\tatomic_pool_size, -1);\n\t\tif (ret)\n\t\t\tgoto remove_mapping;\n\n\t\tgen_pool_set_algo(atomic_pool,\n\t\t\t\t  gen_pool_first_fit_order_align,\n\t\t\t\t  (void *)PAGE_SHIFT);\n\n\t\tpr_info(\"DMA: preallocated %zu KiB pool for atomic allocations\\n\",\n\t\t\tatomic_pool_size / 1024);\n\t\treturn 0;\n\t}\n\tgoto out;\n\nremove_mapping:\n\tdma_common_free_remap(addr, atomic_pool_size, VM_USERMAP);\ndestroy_genpool:\n\tgen_pool_destroy(atomic_pool);\n\tatomic_pool = NULL;\nfree_page:\n\tif (!dma_release_from_contiguous(NULL, page, nr_pages))\n\t\t__free_pages(page, pool_size_order);\nout:\n\tpr_err(\"DMA: failed to allocate %zu KiB pool for atomic coherent allocation\\n\",\n\t\tatomic_pool_size / 1024);\n\treturn -ENOMEM;\n}\n\nstatic int __init arm64_dma_init(void)\n{\n\tint ret;\n\n\tdma_ops = &swiotlb_dma_ops;\n\n\tret = atomic_pool_init();\n\n\treturn ret;\n}\narch_initcall(arm64_dma_init);\n\n#define PREALLOC_DMA_DEBUG_ENTRIES\t4096\n\nstatic int __init dma_debug_do_init(void)\n{\n\tdma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);\n\treturn 0;\n}\nfs_initcall(dma_debug_do_init);\n"], "fixing_code": ["/*\n * SWIOTLB-based DMA API implementation\n *\n * Copyright (C) 2012 ARM Ltd.\n * Author: Catalin Marinas <catalin.marinas@arm.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\n#include <linux/gfp.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/genalloc.h>\n#include <linux/dma-mapping.h>\n#include <linux/dma-contiguous.h>\n#include <linux/vmalloc.h>\n#include <linux/swiotlb.h>\n\n#include <asm/cacheflush.h>\n\nstruct dma_map_ops *dma_ops;\nEXPORT_SYMBOL(dma_ops);\n\nstatic pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot,\n\t\t\t\t bool coherent)\n{\n\tif (!coherent || dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs))\n\t\treturn pgprot_writecombine(prot);\n\treturn prot;\n}\n\nstatic struct gen_pool *atomic_pool;\n\n#define DEFAULT_DMA_COHERENT_POOL_SIZE  SZ_256K\nstatic size_t atomic_pool_size = DEFAULT_DMA_COHERENT_POOL_SIZE;\n\nstatic int __init early_coherent_pool(char *p)\n{\n\tatomic_pool_size = memparse(p, &p);\n\treturn 0;\n}\nearly_param(\"coherent_pool\", early_coherent_pool);\n\nstatic void *__alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)\n{\n\tunsigned long val;\n\tvoid *ptr = NULL;\n\n\tif (!atomic_pool) {\n\t\tWARN(1, \"coherent pool not initialised!\\n\");\n\t\treturn NULL;\n\t}\n\n\tval = gen_pool_alloc(atomic_pool, size);\n\tif (val) {\n\t\tphys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);\n\n\t\t*ret_page = phys_to_page(phys);\n\t\tptr = (void *)val;\n\t\tmemset(ptr, 0, size);\n\t}\n\n\treturn ptr;\n}\n\nstatic bool __in_atomic_pool(void *start, size_t size)\n{\n\treturn addr_in_gen_pool(atomic_pool, (unsigned long)start, size);\n}\n\nstatic int __free_from_pool(void *start, size_t size)\n{\n\tif (!__in_atomic_pool(start, size))\n\t\treturn 0;\n\n\tgen_pool_free(atomic_pool, (unsigned long)start, size);\n\n\treturn 1;\n}\n\nstatic void *__dma_alloc_coherent(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *dma_handle, gfp_t flags,\n\t\t\t\t  struct dma_attrs *attrs)\n{\n\tif (dev == NULL) {\n\t\tWARN_ONCE(1, \"Use an actual device structure for DMA allocation\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (IS_ENABLED(CONFIG_ZONE_DMA) &&\n\t    dev->coherent_dma_mask <= DMA_BIT_MASK(32))\n\t\tflags |= GFP_DMA;\n\tif (IS_ENABLED(CONFIG_DMA_CMA) && (flags & __GFP_WAIT)) {\n\t\tstruct page *page;\n\t\tvoid *addr;\n\n\t\tsize = PAGE_ALIGN(size);\n\t\tpage = dma_alloc_from_contiguous(dev, size >> PAGE_SHIFT,\n\t\t\t\t\t\t\tget_order(size));\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n\t\taddr = page_address(page);\n\t\tmemset(addr, 0, size);\n\t\treturn addr;\n\t} else {\n\t\treturn swiotlb_alloc_coherent(dev, size, dma_handle, flags);\n\t}\n}\n\nstatic void __dma_free_coherent(struct device *dev, size_t size,\n\t\t\t\tvoid *vaddr, dma_addr_t dma_handle,\n\t\t\t\tstruct dma_attrs *attrs)\n{\n\tbool freed;\n\tphys_addr_t paddr = dma_to_phys(dev, dma_handle);\n\n\tif (dev == NULL) {\n\t\tWARN_ONCE(1, \"Use an actual device structure for DMA allocation\\n\");\n\t\treturn;\n\t}\n\n\tfreed = dma_release_from_contiguous(dev,\n\t\t\t\t\tphys_to_page(paddr),\n\t\t\t\t\tsize >> PAGE_SHIFT);\n\tif (!freed)\n\t\tswiotlb_free_coherent(dev, size, vaddr, dma_handle);\n}\n\nstatic void *__dma_alloc(struct device *dev, size_t size,\n\t\t\t dma_addr_t *dma_handle, gfp_t flags,\n\t\t\t struct dma_attrs *attrs)\n{\n\tstruct page *page;\n\tvoid *ptr, *coherent_ptr;\n\tbool coherent = is_device_dma_coherent(dev);\n\n\tsize = PAGE_ALIGN(size);\n\n\tif (!coherent && !(flags & __GFP_WAIT)) {\n\t\tstruct page *page = NULL;\n\t\tvoid *addr = __alloc_from_pool(size, &page, flags);\n\n\t\tif (addr)\n\t\t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n\n\t\treturn addr;\n\t}\n\n\tptr = __dma_alloc_coherent(dev, size, dma_handle, flags, attrs);\n\tif (!ptr)\n\t\tgoto no_mem;\n\n\t/* no need for non-cacheable mapping if coherent */\n\tif (coherent)\n\t\treturn ptr;\n\n\t/* remove any dirty cache lines on the kernel alias */\n\t__dma_flush_range(ptr, ptr + size);\n\n\t/* create a coherent mapping */\n\tpage = virt_to_page(ptr);\n\tcoherent_ptr = dma_common_contiguous_remap(page, size, VM_USERMAP,\n\t\t\t\t__get_dma_pgprot(attrs,\n\t\t\t\t\t__pgprot(PROT_NORMAL_NC), false),\n\t\t\t\t\tNULL);\n\tif (!coherent_ptr)\n\t\tgoto no_map;\n\n\treturn coherent_ptr;\n\nno_map:\n\t__dma_free_coherent(dev, size, ptr, *dma_handle, attrs);\nno_mem:\n\t*dma_handle = DMA_ERROR_CODE;\n\treturn NULL;\n}\n\nstatic void __dma_free(struct device *dev, size_t size,\n\t\t       void *vaddr, dma_addr_t dma_handle,\n\t\t       struct dma_attrs *attrs)\n{\n\tvoid *swiotlb_addr = phys_to_virt(dma_to_phys(dev, dma_handle));\n\n\tif (!is_device_dma_coherent(dev)) {\n\t\tif (__free_from_pool(vaddr, size))\n\t\t\treturn;\n\t\tvunmap(vaddr);\n\t}\n\t__dma_free_coherent(dev, size, swiotlb_addr, dma_handle, attrs);\n}\n\nstatic dma_addr_t __swiotlb_map_page(struct device *dev, struct page *page,\n\t\t\t\t     unsigned long offset, size_t size,\n\t\t\t\t     enum dma_data_direction dir,\n\t\t\t\t     struct dma_attrs *attrs)\n{\n\tdma_addr_t dev_addr;\n\n\tdev_addr = swiotlb_map_page(dev, page, offset, size, dir, attrs);\n\tif (!is_device_dma_coherent(dev))\n\t\t__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);\n\n\treturn dev_addr;\n}\n\n\nstatic void __swiotlb_unmap_page(struct device *dev, dma_addr_t dev_addr,\n\t\t\t\t size_t size, enum dma_data_direction dir,\n\t\t\t\t struct dma_attrs *attrs)\n{\n\tif (!is_device_dma_coherent(dev))\n\t\t__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);\n\tswiotlb_unmap_page(dev, dev_addr, size, dir, attrs);\n}\n\nstatic int __swiotlb_map_sg_attrs(struct device *dev, struct scatterlist *sgl,\n\t\t\t\t  int nelems, enum dma_data_direction dir,\n\t\t\t\t  struct dma_attrs *attrs)\n{\n\tstruct scatterlist *sg;\n\tint i, ret;\n\n\tret = swiotlb_map_sg_attrs(dev, sgl, nelems, dir, attrs);\n\tif (!is_device_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, ret, i)\n\t\t\t__dma_map_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),\n\t\t\t\t       sg->length, dir);\n\n\treturn ret;\n}\n\nstatic void __swiotlb_unmap_sg_attrs(struct device *dev,\n\t\t\t\t     struct scatterlist *sgl, int nelems,\n\t\t\t\t     enum dma_data_direction dir,\n\t\t\t\t     struct dma_attrs *attrs)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tif (!is_device_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\t__dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),\n\t\t\t\t\t sg->length, dir);\n\tswiotlb_unmap_sg_attrs(dev, sgl, nelems, dir, attrs);\n}\n\nstatic void __swiotlb_sync_single_for_cpu(struct device *dev,\n\t\t\t\t\t  dma_addr_t dev_addr, size_t size,\n\t\t\t\t\t  enum dma_data_direction dir)\n{\n\tif (!is_device_dma_coherent(dev))\n\t\t__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);\n\tswiotlb_sync_single_for_cpu(dev, dev_addr, size, dir);\n}\n\nstatic void __swiotlb_sync_single_for_device(struct device *dev,\n\t\t\t\t\t     dma_addr_t dev_addr, size_t size,\n\t\t\t\t\t     enum dma_data_direction dir)\n{\n\tswiotlb_sync_single_for_device(dev, dev_addr, size, dir);\n\tif (!is_device_dma_coherent(dev))\n\t\t__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);\n}\n\nstatic void __swiotlb_sync_sg_for_cpu(struct device *dev,\n\t\t\t\t      struct scatterlist *sgl, int nelems,\n\t\t\t\t      enum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tif (!is_device_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\t__dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),\n\t\t\t\t\t sg->length, dir);\n\tswiotlb_sync_sg_for_cpu(dev, sgl, nelems, dir);\n}\n\nstatic void __swiotlb_sync_sg_for_device(struct device *dev,\n\t\t\t\t\t struct scatterlist *sgl, int nelems,\n\t\t\t\t\t enum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tswiotlb_sync_sg_for_device(dev, sgl, nelems, dir);\n\tif (!is_device_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\t__dma_map_area(phys_to_virt(dma_to_phys(dev, sg->dma_address)),\n\t\t\t\t       sg->length, dir);\n}\n\n/* vma->vm_page_prot must be set appropriately before calling this function */\nstatic int __dma_common_mmap(struct device *dev, struct vm_area_struct *vma,\n\t\t\t     void *cpu_addr, dma_addr_t dma_addr, size_t size)\n{\n\tint ret = -ENXIO;\n\tunsigned long nr_vma_pages = (vma->vm_end - vma->vm_start) >>\n\t\t\t\t\tPAGE_SHIFT;\n\tunsigned long nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned long pfn = dma_to_phys(dev, dma_addr) >> PAGE_SHIFT;\n\tunsigned long off = vma->vm_pgoff;\n\n\tif (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &ret))\n\t\treturn ret;\n\n\tif (off < nr_pages && nr_vma_pages <= (nr_pages - off)) {\n\t\tret = remap_pfn_range(vma, vma->vm_start,\n\t\t\t\t      pfn + off,\n\t\t\t\t      vma->vm_end - vma->vm_start,\n\t\t\t\t      vma->vm_page_prot);\n\t}\n\n\treturn ret;\n}\n\nstatic int __swiotlb_mmap(struct device *dev,\n\t\t\t  struct vm_area_struct *vma,\n\t\t\t  void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t\t  struct dma_attrs *attrs)\n{\n\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot,\n\t\t\t\t\t     is_device_dma_coherent(dev));\n\treturn __dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);\n}\n\nstatic struct dma_map_ops swiotlb_dma_ops = {\n\t.alloc = __dma_alloc,\n\t.free = __dma_free,\n\t.mmap = __swiotlb_mmap,\n\t.map_page = __swiotlb_map_page,\n\t.unmap_page = __swiotlb_unmap_page,\n\t.map_sg = __swiotlb_map_sg_attrs,\n\t.unmap_sg = __swiotlb_unmap_sg_attrs,\n\t.sync_single_for_cpu = __swiotlb_sync_single_for_cpu,\n\t.sync_single_for_device = __swiotlb_sync_single_for_device,\n\t.sync_sg_for_cpu = __swiotlb_sync_sg_for_cpu,\n\t.sync_sg_for_device = __swiotlb_sync_sg_for_device,\n\t.dma_supported = swiotlb_dma_supported,\n\t.mapping_error = swiotlb_dma_mapping_error,\n};\n\nstatic int __init atomic_pool_init(void)\n{\n\tpgprot_t prot = __pgprot(PROT_NORMAL_NC);\n\tunsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *addr;\n\tunsigned int pool_size_order = get_order(atomic_pool_size);\n\n\tif (dev_get_cma_area(NULL))\n\t\tpage = dma_alloc_from_contiguous(NULL, nr_pages,\n\t\t\t\t\t\t\tpool_size_order);\n\telse\n\t\tpage = alloc_pages(GFP_DMA, pool_size_order);\n\n\tif (page) {\n\t\tint ret;\n\t\tvoid *page_addr = page_address(page);\n\n\t\tmemset(page_addr, 0, atomic_pool_size);\n\t\t__dma_flush_range(page_addr, page_addr + atomic_pool_size);\n\n\t\tatomic_pool = gen_pool_create(PAGE_SHIFT, -1);\n\t\tif (!atomic_pool)\n\t\t\tgoto free_page;\n\n\t\taddr = dma_common_contiguous_remap(page, atomic_pool_size,\n\t\t\t\t\tVM_USERMAP, prot, atomic_pool_init);\n\n\t\tif (!addr)\n\t\t\tgoto destroy_genpool;\n\n\t\tret = gen_pool_add_virt(atomic_pool, (unsigned long)addr,\n\t\t\t\t\tpage_to_phys(page),\n\t\t\t\t\tatomic_pool_size, -1);\n\t\tif (ret)\n\t\t\tgoto remove_mapping;\n\n\t\tgen_pool_set_algo(atomic_pool,\n\t\t\t\t  gen_pool_first_fit_order_align,\n\t\t\t\t  (void *)PAGE_SHIFT);\n\n\t\tpr_info(\"DMA: preallocated %zu KiB pool for atomic allocations\\n\",\n\t\t\tatomic_pool_size / 1024);\n\t\treturn 0;\n\t}\n\tgoto out;\n\nremove_mapping:\n\tdma_common_free_remap(addr, atomic_pool_size, VM_USERMAP);\ndestroy_genpool:\n\tgen_pool_destroy(atomic_pool);\n\tatomic_pool = NULL;\nfree_page:\n\tif (!dma_release_from_contiguous(NULL, page, nr_pages))\n\t\t__free_pages(page, pool_size_order);\nout:\n\tpr_err(\"DMA: failed to allocate %zu KiB pool for atomic coherent allocation\\n\",\n\t\tatomic_pool_size / 1024);\n\treturn -ENOMEM;\n}\n\nstatic int __init arm64_dma_init(void)\n{\n\tint ret;\n\n\tdma_ops = &swiotlb_dma_ops;\n\n\tret = atomic_pool_init();\n\n\treturn ret;\n}\narch_initcall(arm64_dma_init);\n\n#define PREALLOC_DMA_DEBUG_ENTRIES\t4096\n\nstatic int __init dma_debug_do_init(void)\n{\n\tdma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);\n\treturn 0;\n}\nfs_initcall(dma_debug_do_init);\n"], "filenames": ["arch/arm64/mm/dma-mapping.c"], "buggy_code_start_loc": [70], "buggy_code_end_loc": [118], "fixing_code_start_loc": [70], "fixing_code_end_loc": [116], "type": "CWE-200", "message": "arch/arm64/mm/dma-mapping.c in the Linux kernel before 4.0.3, as used in the ION subsystem in Android and other products, does not initialize certain data structures, which allows local users to obtain sensitive information from kernel memory by triggering a dma_mmap call.", "other": {"cve": {"id": "CVE-2015-8950", "sourceIdentifier": "security@android.com", "published": "2016-10-10T10:59:01.260", "lastModified": "2016-11-28T19:50:45.630", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "arch/arm64/mm/dma-mapping.c in the Linux kernel before 4.0.3, as used in the ION subsystem in Android and other products, does not initialize certain data structures, which allows local users to obtain sensitive information from kernel memory by triggering a dma_mmap call."}, {"lang": "es", "value": "arch/arm64/mm/dma-mapping.c en el kernel de Linux en versiones anteriores a 4.0.3, como es usado en el subsistema ION en Android y otros productos, no inicializa ciertas estructuras de datos, lo que permite a usuarios locales obtener informaci\u00f3n sensible de la memoria del kernel desencadenando una llamada dma_mmap."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.0.2", "matchCriteriaId": "3918BCE8-1066-4D3E-A44A-126FCFA0C97B"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=6829e274a623187c24f7cfc0e3d35f25d087fcc5", "source": "security@android.com", "tags": ["Issue Tracking", "Patch"]}, {"url": "http://source.android.com/security/bulletin/2016-10-01.html", "source": "security@android.com", "tags": ["Vendor Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.0.3", "source": "security@android.com", "tags": ["Release Notes"]}, {"url": "http://www.securityfocus.com/bid/93318", "source": "security@android.com"}, {"url": "https://github.com/torvalds/linux/commit/6829e274a623187c24f7cfc0e3d35f25d087fcc5", "source": "security@android.com", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://source.codeaurora.org/quic/la/kernel/msm-3.10/commit/?id=6e2c437a2d0a85d90d3db85a7471f99764f7bbf8", "source": "security@android.com", "tags": ["Issue Tracking", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/6829e274a623187c24f7cfc0e3d35f25d087fcc5"}}