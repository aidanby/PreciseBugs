{"buggy_code": ["/*\n *  linux/mm/oom_kill.c\n * \n *  Copyright (C)  1998,2000  Rik van Riel\n *\tThanks go out to Claus Fischer for some serious inspiration and\n *\tfor goading me into coding this file...\n *  Copyright (C)  2010  Google, Inc.\n *\tRewritten by David Rientjes\n *\n *  The routines in this file are used to kill a process when\n *  we're seriously out of memory. This gets called from __alloc_pages()\n *  in mm/page_alloc.c when we really run out of memory.\n *\n *  Since we won't call these routines often (on a well-configured\n *  machine) this file will double as a 'coding guide' and a signpost\n *  for newbie kernel hackers. It features several pointers to major\n *  kernel subsystems and hints as to where to find out what things do.\n */\n\n#include <linux/oom.h>\n#include <linux/mm.h>\n#include <linux/err.h>\n#include <linux/gfp.h>\n#include <linux/sched.h>\n#include <linux/swap.h>\n#include <linux/timex.h>\n#include <linux/jiffies.h>\n#include <linux/cpuset.h>\n#include <linux/module.h>\n#include <linux/notifier.h>\n#include <linux/memcontrol.h>\n#include <linux/mempolicy.h>\n#include <linux/security.h>\n#include <linux/ptrace.h>\n\nint sysctl_panic_on_oom;\nint sysctl_oom_kill_allocating_task;\nint sysctl_oom_dump_tasks = 1;\nstatic DEFINE_SPINLOCK(zone_scan_lock);\n\n/**\n * test_set_oom_score_adj() - set current's oom_score_adj and return old value\n * @new_val: new oom_score_adj value\n *\n * Sets the oom_score_adj value for current to @new_val with proper\n * synchronization and returns the old value.  Usually used to temporarily\n * set a value, save the old value in the caller, and then reinstate it later.\n */\nint test_set_oom_score_adj(int new_val)\n{\n\tstruct sighand_struct *sighand = current->sighand;\n\tint old_val;\n\n\tspin_lock_irq(&sighand->siglock);\n\told_val = current->signal->oom_score_adj;\n\tif (new_val != old_val) {\n\t\tif (new_val == OOM_SCORE_ADJ_MIN)\n\t\t\tatomic_inc(&current->mm->oom_disable_count);\n\t\telse if (old_val == OOM_SCORE_ADJ_MIN)\n\t\t\tatomic_dec(&current->mm->oom_disable_count);\n\t\tcurrent->signal->oom_score_adj = new_val;\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n\n\treturn old_val;\n}\n\n#ifdef CONFIG_NUMA\n/**\n * has_intersects_mems_allowed() - check task eligiblity for kill\n * @tsk: task struct of which task to consider\n * @mask: nodemask passed to page allocator for mempolicy ooms\n *\n * Task eligibility is determined by whether or not a candidate task, @tsk,\n * shares the same mempolicy nodes as current if it is bound by such a policy\n * and whether or not it has the same set of allowed cpuset nodes.\n */\nstatic bool has_intersects_mems_allowed(struct task_struct *tsk,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\tstruct task_struct *start = tsk;\n\n\tdo {\n\t\tif (mask) {\n\t\t\t/*\n\t\t\t * If this is a mempolicy constrained oom, tsk's\n\t\t\t * cpuset is irrelevant.  Only return true if its\n\t\t\t * mempolicy intersects current, otherwise it may be\n\t\t\t * needlessly killed.\n\t\t\t */\n\t\t\tif (mempolicy_nodemask_intersects(tsk, mask))\n\t\t\t\treturn true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is not a mempolicy constrained oom, so only\n\t\t\t * check the mems of tsk's cpuset.\n\t\t\t */\n\t\t\tif (cpuset_mems_allowed_intersects(current, tsk))\n\t\t\t\treturn true;\n\t\t}\n\t} while_each_thread(start, tsk);\n\n\treturn false;\n}\n#else\nstatic bool has_intersects_mems_allowed(struct task_struct *tsk,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\treturn true;\n}\n#endif /* CONFIG_NUMA */\n\n/*\n * The process p may have detached its own ->mm while exiting or through\n * use_mm(), but one or more of its subthreads may still have a valid\n * pointer.  Return p, or any of its subthreads with a valid ->mm, with\n * task_lock() held.\n */\nstruct task_struct *find_lock_task_mm(struct task_struct *p)\n{\n\tstruct task_struct *t = p;\n\n\tdo {\n\t\ttask_lock(t);\n\t\tif (likely(t->mm))\n\t\t\treturn t;\n\t\ttask_unlock(t);\n\t} while_each_thread(p, t);\n\n\treturn NULL;\n}\n\n/* return true if the task is not adequate as candidate victim task. */\nstatic bool oom_unkillable_task(struct task_struct *p,\n\t\tconst struct mem_cgroup *mem, const nodemask_t *nodemask)\n{\n\tif (is_global_init(p))\n\t\treturn true;\n\tif (p->flags & PF_KTHREAD)\n\t\treturn true;\n\n\t/* When mem_cgroup_out_of_memory() and p is not member of the group */\n\tif (mem && !task_in_mem_cgroup(p, mem))\n\t\treturn true;\n\n\t/* p may not have freeable memory in nodemask */\n\tif (!has_intersects_mems_allowed(p, nodemask))\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * oom_badness - heuristic function to determine which candidate task to kill\n * @p: task struct of which task we should calculate\n * @totalpages: total present RAM allowed for page allocation\n *\n * The heuristic for determining which task to kill is made to be as simple and\n * predictable as possible.  The goal is to return the highest value for the\n * task consuming the most memory to avoid subsequent oom failures.\n */\nunsigned int oom_badness(struct task_struct *p, struct mem_cgroup *mem,\n\t\t      const nodemask_t *nodemask, unsigned long totalpages)\n{\n\tint points;\n\n\tif (oom_unkillable_task(p, mem, nodemask))\n\t\treturn 0;\n\n\tp = find_lock_task_mm(p);\n\tif (!p)\n\t\treturn 0;\n\n\t/*\n\t * Shortcut check for a thread sharing p->mm that is OOM_SCORE_ADJ_MIN\n\t * so the entire heuristic doesn't need to be executed for something\n\t * that cannot be killed.\n\t */\n\tif (atomic_read(&p->mm->oom_disable_count)) {\n\t\ttask_unlock(p);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * The memory controller may have a limit of 0 bytes, so avoid a divide\n\t * by zero, if necessary.\n\t */\n\tif (!totalpages)\n\t\ttotalpages = 1;\n\n\t/*\n\t * The baseline for the badness score is the proportion of RAM that each\n\t * task's rss, pagetable and swap space use.\n\t */\n\tpoints = get_mm_rss(p->mm) + p->mm->nr_ptes;\n\tpoints += get_mm_counter(p->mm, MM_SWAPENTS);\n\n\tpoints *= 1000;\n\tpoints /= totalpages;\n\ttask_unlock(p);\n\n\t/*\n\t * Root processes get 3% bonus, just like the __vm_enough_memory()\n\t * implementation used by LSMs.\n\t */\n\tif (has_capability_noaudit(p, CAP_SYS_ADMIN))\n\t\tpoints -= 30;\n\n\t/*\n\t * /proc/pid/oom_score_adj ranges from -1000 to +1000 such that it may\n\t * either completely disable oom killing or always prefer a certain\n\t * task.\n\t */\n\tpoints += p->signal->oom_score_adj;\n\n\t/*\n\t * Never return 0 for an eligible task that may be killed since it's\n\t * possible that no single user task uses more than 0.1% of memory and\n\t * no single admin tasks uses more than 3.0%.\n\t */\n\tif (points <= 0)\n\t\treturn 1;\n\treturn (points < 1000) ? points : 1000;\n}\n\n/*\n * Determine the type of allocation constraint.\n */\n#ifdef CONFIG_NUMA\nstatic enum oom_constraint constrained_alloc(struct zonelist *zonelist,\n\t\t\t\tgfp_t gfp_mask, nodemask_t *nodemask,\n\t\t\t\tunsigned long *totalpages)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tenum zone_type high_zoneidx = gfp_zone(gfp_mask);\n\tbool cpuset_limited = false;\n\tint nid;\n\n\t/* Default to all available memory */\n\t*totalpages = totalram_pages + total_swap_pages;\n\n\tif (!zonelist)\n\t\treturn CONSTRAINT_NONE;\n\t/*\n\t * Reach here only when __GFP_NOFAIL is used. So, we should avoid\n\t * to kill current.We have to random task kill in this case.\n\t * Hopefully, CONSTRAINT_THISNODE...but no way to handle it, now.\n\t */\n\tif (gfp_mask & __GFP_THISNODE)\n\t\treturn CONSTRAINT_NONE;\n\n\t/*\n\t * This is not a __GFP_THISNODE allocation, so a truncated nodemask in\n\t * the page allocator means a mempolicy is in effect.  Cpuset policy\n\t * is enforced in get_page_from_freelist().\n\t */\n\tif (nodemask && !nodes_subset(node_states[N_HIGH_MEMORY], *nodemask)) {\n\t\t*totalpages = total_swap_pages;\n\t\tfor_each_node_mask(nid, *nodemask)\n\t\t\t*totalpages += node_spanned_pages(nid);\n\t\treturn CONSTRAINT_MEMORY_POLICY;\n\t}\n\n\t/* Check this allocation failure is caused by cpuset's wall function */\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n\t\t\thigh_zoneidx, nodemask)\n\t\tif (!cpuset_zone_allowed_softwall(zone, gfp_mask))\n\t\t\tcpuset_limited = true;\n\n\tif (cpuset_limited) {\n\t\t*totalpages = total_swap_pages;\n\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n\t\t\t*totalpages += node_spanned_pages(nid);\n\t\treturn CONSTRAINT_CPUSET;\n\t}\n\treturn CONSTRAINT_NONE;\n}\n#else\nstatic enum oom_constraint constrained_alloc(struct zonelist *zonelist,\n\t\t\t\tgfp_t gfp_mask, nodemask_t *nodemask,\n\t\t\t\tunsigned long *totalpages)\n{\n\t*totalpages = totalram_pages + total_swap_pages;\n\treturn CONSTRAINT_NONE;\n}\n#endif\n\n/*\n * Simple selection loop. We chose the process with the highest\n * number of 'points'. We expect the caller will lock the tasklist.\n *\n * (not docbooked, we don't want this one cluttering up the manual)\n */\nstatic struct task_struct *select_bad_process(unsigned int *ppoints,\n\t\tunsigned long totalpages, struct mem_cgroup *mem,\n\t\tconst nodemask_t *nodemask)\n{\n\tstruct task_struct *g, *p;\n\tstruct task_struct *chosen = NULL;\n\t*ppoints = 0;\n\n\tdo_each_thread(g, p) {\n\t\tunsigned int points;\n\n\t\tif (p->exit_state)\n\t\t\tcontinue;\n\t\tif (oom_unkillable_task(p, mem, nodemask))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * This task already has access to memory reserves and is\n\t\t * being killed. Don't allow any other task access to the\n\t\t * memory reserve.\n\t\t *\n\t\t * Note: this may have a chance of deadlock if it gets\n\t\t * blocked waiting for another task which itself is waiting\n\t\t * for memory. Is there a better alternative?\n\t\t */\n\t\tif (test_tsk_thread_flag(p, TIF_MEMDIE))\n\t\t\treturn ERR_PTR(-1UL);\n\t\tif (!p->mm)\n\t\t\tcontinue;\n\n\t\tif (p->flags & PF_EXITING) {\n\t\t\t/*\n\t\t\t * If p is the current task and is in the process of\n\t\t\t * releasing memory, we allow the \"kill\" to set\n\t\t\t * TIF_MEMDIE, which will allow it to gain access to\n\t\t\t * memory reserves.  Otherwise, it may stall forever.\n\t\t\t *\n\t\t\t * The loop isn't broken here, however, in case other\n\t\t\t * threads are found to have already been oom killed.\n\t\t\t */\n\t\t\tif (p == current) {\n\t\t\t\tchosen = p;\n\t\t\t\t*ppoints = 1000;\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * If this task is not being ptraced on exit,\n\t\t\t\t * then wait for it to finish before killing\n\t\t\t\t * some other task unnecessarily.\n\t\t\t\t */\n\t\t\t\tif (!(p->group_leader->ptrace & PT_TRACE_EXIT))\n\t\t\t\t\treturn ERR_PTR(-1UL);\n\t\t\t}\n\t\t}\n\n\t\tpoints = oom_badness(p, mem, nodemask, totalpages);\n\t\tif (points > *ppoints) {\n\t\t\tchosen = p;\n\t\t\t*ppoints = points;\n\t\t}\n\t} while_each_thread(g, p);\n\n\treturn chosen;\n}\n\n/**\n * dump_tasks - dump current memory state of all system tasks\n * @mem: current's memory controller, if constrained\n * @nodemask: nodemask passed to page allocator for mempolicy ooms\n *\n * Dumps the current memory state of all eligible tasks.  Tasks not in the same\n * memcg, not in the same cpuset, or bound to a disjoint set of mempolicy nodes\n * are not shown.\n * State information includes task's pid, uid, tgid, vm size, rss, cpu, oom_adj\n * value, oom_score_adj value, and name.\n *\n * Call with tasklist_lock read-locked.\n */\nstatic void dump_tasks(const struct mem_cgroup *mem, const nodemask_t *nodemask)\n{\n\tstruct task_struct *p;\n\tstruct task_struct *task;\n\n\tpr_info(\"[ pid ]   uid  tgid total_vm      rss cpu oom_adj oom_score_adj name\\n\");\n\tfor_each_process(p) {\n\t\tif (oom_unkillable_task(p, mem, nodemask))\n\t\t\tcontinue;\n\n\t\ttask = find_lock_task_mm(p);\n\t\tif (!task) {\n\t\t\t/*\n\t\t\t * This is a kthread or all of p's threads have already\n\t\t\t * detached their mm's.  There's no need to report\n\t\t\t * them; they can't be oom killed anyway.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tpr_info(\"[%5d] %5d %5d %8lu %8lu %3u     %3d         %5d %s\\n\",\n\t\t\ttask->pid, task_uid(task), task->tgid,\n\t\t\ttask->mm->total_vm, get_mm_rss(task->mm),\n\t\t\ttask_cpu(task), task->signal->oom_adj,\n\t\t\ttask->signal->oom_score_adj, task->comm);\n\t\ttask_unlock(task);\n\t}\n}\n\nstatic void dump_header(struct task_struct *p, gfp_t gfp_mask, int order,\n\t\t\tstruct mem_cgroup *mem, const nodemask_t *nodemask)\n{\n\ttask_lock(current);\n\tpr_warning(\"%s invoked oom-killer: gfp_mask=0x%x, order=%d, \"\n\t\t\"oom_adj=%d, oom_score_adj=%d\\n\",\n\t\tcurrent->comm, gfp_mask, order, current->signal->oom_adj,\n\t\tcurrent->signal->oom_score_adj);\n\tcpuset_print_task_mems_allowed(current);\n\ttask_unlock(current);\n\tdump_stack();\n\tmem_cgroup_print_oom_info(mem, p);\n\tshow_mem(SHOW_MEM_FILTER_NODES);\n\tif (sysctl_oom_dump_tasks)\n\t\tdump_tasks(mem, nodemask);\n}\n\n#define K(x) ((x) << (PAGE_SHIFT-10))\nstatic int oom_kill_task(struct task_struct *p, struct mem_cgroup *mem)\n{\n\tstruct task_struct *q;\n\tstruct mm_struct *mm;\n\n\tp = find_lock_task_mm(p);\n\tif (!p)\n\t\treturn 1;\n\n\t/* mm cannot be safely dereferenced after task_unlock(p) */\n\tmm = p->mm;\n\n\tpr_err(\"Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB\\n\",\n\t\ttask_pid_nr(p), p->comm, K(p->mm->total_vm),\n\t\tK(get_mm_counter(p->mm, MM_ANONPAGES)),\n\t\tK(get_mm_counter(p->mm, MM_FILEPAGES)));\n\ttask_unlock(p);\n\n\t/*\n\t * Kill all processes sharing p->mm in other thread groups, if any.\n\t * They don't get access to memory reserves or a higher scheduler\n\t * priority, though, to avoid depletion of all memory or task\n\t * starvation.  This prevents mm->mmap_sem livelock when an oom killed\n\t * task cannot exit because it requires the semaphore and its contended\n\t * by another thread trying to allocate memory itself.  That thread will\n\t * now get access to memory reserves since it has a pending fatal\n\t * signal.\n\t */\n\tfor_each_process(q)\n\t\tif (q->mm == mm && !same_thread_group(q, p)) {\n\t\t\ttask_lock(q);\t/* Protect ->comm from prctl() */\n\t\t\tpr_err(\"Kill process %d (%s) sharing same memory\\n\",\n\t\t\t\ttask_pid_nr(q), q->comm);\n\t\t\ttask_unlock(q);\n\t\t\tforce_sig(SIGKILL, q);\n\t\t}\n\n\tset_tsk_thread_flag(p, TIF_MEMDIE);\n\tforce_sig(SIGKILL, p);\n\n\treturn 0;\n}\n#undef K\n\nstatic int oom_kill_process(struct task_struct *p, gfp_t gfp_mask, int order,\n\t\t\t    unsigned int points, unsigned long totalpages,\n\t\t\t    struct mem_cgroup *mem, nodemask_t *nodemask,\n\t\t\t    const char *message)\n{\n\tstruct task_struct *victim = p;\n\tstruct task_struct *child;\n\tstruct task_struct *t = p;\n\tunsigned int victim_points = 0;\n\n\tif (printk_ratelimit())\n\t\tdump_header(p, gfp_mask, order, mem, nodemask);\n\n\t/*\n\t * If the task is already exiting, don't alarm the sysadmin or kill\n\t * its children or threads, just set TIF_MEMDIE so it can die quickly\n\t */\n\tif (p->flags & PF_EXITING) {\n\t\tset_tsk_thread_flag(p, TIF_MEMDIE);\n\t\treturn 0;\n\t}\n\n\ttask_lock(p);\n\tpr_err(\"%s: Kill process %d (%s) score %d or sacrifice child\\n\",\n\t\tmessage, task_pid_nr(p), p->comm, points);\n\ttask_unlock(p);\n\n\t/*\n\t * If any of p's children has a different mm and is eligible for kill,\n\t * the one with the highest oom_badness() score is sacrificed for its\n\t * parent.  This attempts to lose the minimal amount of work done while\n\t * still freeing memory.\n\t */\n\tdo {\n\t\tlist_for_each_entry(child, &t->children, sibling) {\n\t\t\tunsigned int child_points;\n\n\t\t\tif (child->mm == p->mm)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * oom_badness() returns 0 if the thread is unkillable\n\t\t\t */\n\t\t\tchild_points = oom_badness(child, mem, nodemask,\n\t\t\t\t\t\t\t\ttotalpages);\n\t\t\tif (child_points > victim_points) {\n\t\t\t\tvictim = child;\n\t\t\t\tvictim_points = child_points;\n\t\t\t}\n\t\t}\n\t} while_each_thread(p, t);\n\n\treturn oom_kill_task(victim, mem);\n}\n\n/*\n * Determines whether the kernel must panic because of the panic_on_oom sysctl.\n */\nstatic void check_panic_on_oom(enum oom_constraint constraint, gfp_t gfp_mask,\n\t\t\t\tint order, const nodemask_t *nodemask)\n{\n\tif (likely(!sysctl_panic_on_oom))\n\t\treturn;\n\tif (sysctl_panic_on_oom != 2) {\n\t\t/*\n\t\t * panic_on_oom == 1 only affects CONSTRAINT_NONE, the kernel\n\t\t * does not panic for cpuset, mempolicy, or memcg allocation\n\t\t * failures.\n\t\t */\n\t\tif (constraint != CONSTRAINT_NONE)\n\t\t\treturn;\n\t}\n\tread_lock(&tasklist_lock);\n\tdump_header(NULL, gfp_mask, order, NULL, nodemask);\n\tread_unlock(&tasklist_lock);\n\tpanic(\"Out of memory: %s panic_on_oom is enabled\\n\",\n\t\tsysctl_panic_on_oom == 2 ? \"compulsory\" : \"system-wide\");\n}\n\n#ifdef CONFIG_CGROUP_MEM_RES_CTLR\nvoid mem_cgroup_out_of_memory(struct mem_cgroup *mem, gfp_t gfp_mask)\n{\n\tunsigned long limit;\n\tunsigned int points = 0;\n\tstruct task_struct *p;\n\n\t/*\n\t * If current has a pending SIGKILL, then automatically select it.  The\n\t * goal is to allow it to allocate so that it may quickly exit and free\n\t * its memory.\n\t */\n\tif (fatal_signal_pending(current)) {\n\t\tset_thread_flag(TIF_MEMDIE);\n\t\treturn;\n\t}\n\n\tcheck_panic_on_oom(CONSTRAINT_MEMCG, gfp_mask, 0, NULL);\n\tlimit = mem_cgroup_get_limit(mem) >> PAGE_SHIFT;\n\tread_lock(&tasklist_lock);\nretry:\n\tp = select_bad_process(&points, limit, mem, NULL);\n\tif (!p || PTR_ERR(p) == -1UL)\n\t\tgoto out;\n\n\tif (oom_kill_process(p, gfp_mask, 0, points, limit, mem, NULL,\n\t\t\t\t\"Memory cgroup out of memory\"))\n\t\tgoto retry;\nout:\n\tread_unlock(&tasklist_lock);\n}\n#endif\n\nstatic BLOCKING_NOTIFIER_HEAD(oom_notify_list);\n\nint register_oom_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&oom_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(register_oom_notifier);\n\nint unregister_oom_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&oom_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(unregister_oom_notifier);\n\n/*\n * Try to acquire the OOM killer lock for the zones in zonelist.  Returns zero\n * if a parallel OOM killing is already taking place that includes a zone in\n * the zonelist.  Otherwise, locks all zones in the zonelist and returns 1.\n */\nint try_set_zonelist_oom(struct zonelist *zonelist, gfp_t gfp_mask)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tint ret = 1;\n\n\tspin_lock(&zone_scan_lock);\n\tfor_each_zone_zonelist(zone, z, zonelist, gfp_zone(gfp_mask)) {\n\t\tif (zone_is_oom_locked(zone)) {\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor_each_zone_zonelist(zone, z, zonelist, gfp_zone(gfp_mask)) {\n\t\t/*\n\t\t * Lock each zone in the zonelist under zone_scan_lock so a\n\t\t * parallel invocation of try_set_zonelist_oom() doesn't succeed\n\t\t * when it shouldn't.\n\t\t */\n\t\tzone_set_flag(zone, ZONE_OOM_LOCKED);\n\t}\n\nout:\n\tspin_unlock(&zone_scan_lock);\n\treturn ret;\n}\n\n/*\n * Clears the ZONE_OOM_LOCKED flag for all zones in the zonelist so that failed\n * allocation attempts with zonelists containing them may now recall the OOM\n * killer, if necessary.\n */\nvoid clear_zonelist_oom(struct zonelist *zonelist, gfp_t gfp_mask)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\n\tspin_lock(&zone_scan_lock);\n\tfor_each_zone_zonelist(zone, z, zonelist, gfp_zone(gfp_mask)) {\n\t\tzone_clear_flag(zone, ZONE_OOM_LOCKED);\n\t}\n\tspin_unlock(&zone_scan_lock);\n}\n\n/*\n * Try to acquire the oom killer lock for all system zones.  Returns zero if a\n * parallel oom killing is taking place, otherwise locks all zones and returns\n * non-zero.\n */\nstatic int try_set_system_oom(void)\n{\n\tstruct zone *zone;\n\tint ret = 1;\n\n\tspin_lock(&zone_scan_lock);\n\tfor_each_populated_zone(zone)\n\t\tif (zone_is_oom_locked(zone)) {\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\tfor_each_populated_zone(zone)\n\t\tzone_set_flag(zone, ZONE_OOM_LOCKED);\nout:\n\tspin_unlock(&zone_scan_lock);\n\treturn ret;\n}\n\n/*\n * Clears ZONE_OOM_LOCKED for all system zones so that failed allocation\n * attempts or page faults may now recall the oom killer, if necessary.\n */\nstatic void clear_system_oom(void)\n{\n\tstruct zone *zone;\n\n\tspin_lock(&zone_scan_lock);\n\tfor_each_populated_zone(zone)\n\t\tzone_clear_flag(zone, ZONE_OOM_LOCKED);\n\tspin_unlock(&zone_scan_lock);\n}\n\n/**\n * out_of_memory - kill the \"best\" process when we run out of memory\n * @zonelist: zonelist pointer\n * @gfp_mask: memory allocation flags\n * @order: amount of memory being requested as a power of 2\n * @nodemask: nodemask passed to page allocator\n *\n * If we run out of memory, we have the choice between either\n * killing a random task (bad), letting the system crash (worse)\n * OR try to be smart about which process to kill. Note that we\n * don't have to be perfect here, we just have to be good.\n */\nvoid out_of_memory(struct zonelist *zonelist, gfp_t gfp_mask,\n\t\tint order, nodemask_t *nodemask)\n{\n\tconst nodemask_t *mpol_mask;\n\tstruct task_struct *p;\n\tunsigned long totalpages;\n\tunsigned long freed = 0;\n\tunsigned int points;\n\tenum oom_constraint constraint = CONSTRAINT_NONE;\n\tint killed = 0;\n\n\tblocking_notifier_call_chain(&oom_notify_list, 0, &freed);\n\tif (freed > 0)\n\t\t/* Got some memory back in the last second. */\n\t\treturn;\n\n\t/*\n\t * If current has a pending SIGKILL, then automatically select it.  The\n\t * goal is to allow it to allocate so that it may quickly exit and free\n\t * its memory.\n\t */\n\tif (fatal_signal_pending(current)) {\n\t\tset_thread_flag(TIF_MEMDIE);\n\t\treturn;\n\t}\n\n\t/*\n\t * Check if there were limitations on the allocation (only relevant for\n\t * NUMA) that may require different handling.\n\t */\n\tconstraint = constrained_alloc(zonelist, gfp_mask, nodemask,\n\t\t\t\t\t\t&totalpages);\n\tmpol_mask = (constraint == CONSTRAINT_MEMORY_POLICY) ? nodemask : NULL;\n\tcheck_panic_on_oom(constraint, gfp_mask, order, mpol_mask);\n\n\tread_lock(&tasklist_lock);\n\tif (sysctl_oom_kill_allocating_task &&\n\t    !oom_unkillable_task(current, NULL, nodemask) &&\n\t    current->mm && !atomic_read(&current->mm->oom_disable_count)) {\n\t\t/*\n\t\t * oom_kill_process() needs tasklist_lock held.  If it returns\n\t\t * non-zero, current could not be killed so we must fallback to\n\t\t * the tasklist scan.\n\t\t */\n\t\tif (!oom_kill_process(current, gfp_mask, order, 0, totalpages,\n\t\t\t\tNULL, nodemask,\n\t\t\t\t\"Out of memory (oom_kill_allocating_task)\"))\n\t\t\tgoto out;\n\t}\n\nretry:\n\tp = select_bad_process(&points, totalpages, NULL, mpol_mask);\n\tif (PTR_ERR(p) == -1UL)\n\t\tgoto out;\n\n\t/* Found nothing?!?! Either we hang forever, or we panic. */\n\tif (!p) {\n\t\tdump_header(NULL, gfp_mask, order, NULL, mpol_mask);\n\t\tread_unlock(&tasklist_lock);\n\t\tpanic(\"Out of memory and no killable processes...\\n\");\n\t}\n\n\tif (oom_kill_process(p, gfp_mask, order, points, totalpages, NULL,\n\t\t\t\tnodemask, \"Out of memory\"))\n\t\tgoto retry;\n\tkilled = 1;\nout:\n\tread_unlock(&tasklist_lock);\n\n\t/*\n\t * Give \"p\" a good chance of killing itself before we\n\t * retry to allocate memory unless \"p\" is current\n\t */\n\tif (killed && !test_thread_flag(TIF_MEMDIE))\n\t\tschedule_timeout_uninterruptible(1);\n}\n\n/*\n * The pagefault handler calls here because it is out of memory, so kill a\n * memory-hogging task.  If a populated zone has ZONE_OOM_LOCKED set, a parallel\n * oom killing is already in progress so do nothing.  If a task is found with\n * TIF_MEMDIE set, it has been killed so do nothing and allow it to exit.\n */\nvoid pagefault_out_of_memory(void)\n{\n\tif (try_set_system_oom()) {\n\t\tout_of_memory(NULL, 0, 0, NULL);\n\t\tclear_system_oom();\n\t}\n\tif (!test_thread_flag(TIF_MEMDIE))\n\t\tschedule_timeout_uninterruptible(1);\n}\n"], "fixing_code": ["/*\n *  linux/mm/oom_kill.c\n * \n *  Copyright (C)  1998,2000  Rik van Riel\n *\tThanks go out to Claus Fischer for some serious inspiration and\n *\tfor goading me into coding this file...\n *  Copyright (C)  2010  Google, Inc.\n *\tRewritten by David Rientjes\n *\n *  The routines in this file are used to kill a process when\n *  we're seriously out of memory. This gets called from __alloc_pages()\n *  in mm/page_alloc.c when we really run out of memory.\n *\n *  Since we won't call these routines often (on a well-configured\n *  machine) this file will double as a 'coding guide' and a signpost\n *  for newbie kernel hackers. It features several pointers to major\n *  kernel subsystems and hints as to where to find out what things do.\n */\n\n#include <linux/oom.h>\n#include <linux/mm.h>\n#include <linux/err.h>\n#include <linux/gfp.h>\n#include <linux/sched.h>\n#include <linux/swap.h>\n#include <linux/timex.h>\n#include <linux/jiffies.h>\n#include <linux/cpuset.h>\n#include <linux/module.h>\n#include <linux/notifier.h>\n#include <linux/memcontrol.h>\n#include <linux/mempolicy.h>\n#include <linux/security.h>\n#include <linux/ptrace.h>\n\nint sysctl_panic_on_oom;\nint sysctl_oom_kill_allocating_task;\nint sysctl_oom_dump_tasks = 1;\nstatic DEFINE_SPINLOCK(zone_scan_lock);\n\n/**\n * test_set_oom_score_adj() - set current's oom_score_adj and return old value\n * @new_val: new oom_score_adj value\n *\n * Sets the oom_score_adj value for current to @new_val with proper\n * synchronization and returns the old value.  Usually used to temporarily\n * set a value, save the old value in the caller, and then reinstate it later.\n */\nint test_set_oom_score_adj(int new_val)\n{\n\tstruct sighand_struct *sighand = current->sighand;\n\tint old_val;\n\n\tspin_lock_irq(&sighand->siglock);\n\told_val = current->signal->oom_score_adj;\n\tif (new_val != old_val) {\n\t\tif (new_val == OOM_SCORE_ADJ_MIN)\n\t\t\tatomic_inc(&current->mm->oom_disable_count);\n\t\telse if (old_val == OOM_SCORE_ADJ_MIN)\n\t\t\tatomic_dec(&current->mm->oom_disable_count);\n\t\tcurrent->signal->oom_score_adj = new_val;\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n\n\treturn old_val;\n}\n\n#ifdef CONFIG_NUMA\n/**\n * has_intersects_mems_allowed() - check task eligiblity for kill\n * @tsk: task struct of which task to consider\n * @mask: nodemask passed to page allocator for mempolicy ooms\n *\n * Task eligibility is determined by whether or not a candidate task, @tsk,\n * shares the same mempolicy nodes as current if it is bound by such a policy\n * and whether or not it has the same set of allowed cpuset nodes.\n */\nstatic bool has_intersects_mems_allowed(struct task_struct *tsk,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\tstruct task_struct *start = tsk;\n\n\tdo {\n\t\tif (mask) {\n\t\t\t/*\n\t\t\t * If this is a mempolicy constrained oom, tsk's\n\t\t\t * cpuset is irrelevant.  Only return true if its\n\t\t\t * mempolicy intersects current, otherwise it may be\n\t\t\t * needlessly killed.\n\t\t\t */\n\t\t\tif (mempolicy_nodemask_intersects(tsk, mask))\n\t\t\t\treturn true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is not a mempolicy constrained oom, so only\n\t\t\t * check the mems of tsk's cpuset.\n\t\t\t */\n\t\t\tif (cpuset_mems_allowed_intersects(current, tsk))\n\t\t\t\treturn true;\n\t\t}\n\t} while_each_thread(start, tsk);\n\n\treturn false;\n}\n#else\nstatic bool has_intersects_mems_allowed(struct task_struct *tsk,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\treturn true;\n}\n#endif /* CONFIG_NUMA */\n\n/*\n * The process p may have detached its own ->mm while exiting or through\n * use_mm(), but one or more of its subthreads may still have a valid\n * pointer.  Return p, or any of its subthreads with a valid ->mm, with\n * task_lock() held.\n */\nstruct task_struct *find_lock_task_mm(struct task_struct *p)\n{\n\tstruct task_struct *t = p;\n\n\tdo {\n\t\ttask_lock(t);\n\t\tif (likely(t->mm))\n\t\t\treturn t;\n\t\ttask_unlock(t);\n\t} while_each_thread(p, t);\n\n\treturn NULL;\n}\n\n/* return true if the task is not adequate as candidate victim task. */\nstatic bool oom_unkillable_task(struct task_struct *p,\n\t\tconst struct mem_cgroup *mem, const nodemask_t *nodemask)\n{\n\tif (is_global_init(p))\n\t\treturn true;\n\tif (p->flags & PF_KTHREAD)\n\t\treturn true;\n\n\t/* When mem_cgroup_out_of_memory() and p is not member of the group */\n\tif (mem && !task_in_mem_cgroup(p, mem))\n\t\treturn true;\n\n\t/* p may not have freeable memory in nodemask */\n\tif (!has_intersects_mems_allowed(p, nodemask))\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * oom_badness - heuristic function to determine which candidate task to kill\n * @p: task struct of which task we should calculate\n * @totalpages: total present RAM allowed for page allocation\n *\n * The heuristic for determining which task to kill is made to be as simple and\n * predictable as possible.  The goal is to return the highest value for the\n * task consuming the most memory to avoid subsequent oom failures.\n */\nunsigned int oom_badness(struct task_struct *p, struct mem_cgroup *mem,\n\t\t      const nodemask_t *nodemask, unsigned long totalpages)\n{\n\tlong points;\n\n\tif (oom_unkillable_task(p, mem, nodemask))\n\t\treturn 0;\n\n\tp = find_lock_task_mm(p);\n\tif (!p)\n\t\treturn 0;\n\n\t/*\n\t * Shortcut check for a thread sharing p->mm that is OOM_SCORE_ADJ_MIN\n\t * so the entire heuristic doesn't need to be executed for something\n\t * that cannot be killed.\n\t */\n\tif (atomic_read(&p->mm->oom_disable_count)) {\n\t\ttask_unlock(p);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * The memory controller may have a limit of 0 bytes, so avoid a divide\n\t * by zero, if necessary.\n\t */\n\tif (!totalpages)\n\t\ttotalpages = 1;\n\n\t/*\n\t * The baseline for the badness score is the proportion of RAM that each\n\t * task's rss, pagetable and swap space use.\n\t */\n\tpoints = get_mm_rss(p->mm) + p->mm->nr_ptes;\n\tpoints += get_mm_counter(p->mm, MM_SWAPENTS);\n\n\tpoints *= 1000;\n\tpoints /= totalpages;\n\ttask_unlock(p);\n\n\t/*\n\t * Root processes get 3% bonus, just like the __vm_enough_memory()\n\t * implementation used by LSMs.\n\t */\n\tif (has_capability_noaudit(p, CAP_SYS_ADMIN))\n\t\tpoints -= 30;\n\n\t/*\n\t * /proc/pid/oom_score_adj ranges from -1000 to +1000 such that it may\n\t * either completely disable oom killing or always prefer a certain\n\t * task.\n\t */\n\tpoints += p->signal->oom_score_adj;\n\n\t/*\n\t * Never return 0 for an eligible task that may be killed since it's\n\t * possible that no single user task uses more than 0.1% of memory and\n\t * no single admin tasks uses more than 3.0%.\n\t */\n\tif (points <= 0)\n\t\treturn 1;\n\treturn (points < 1000) ? points : 1000;\n}\n\n/*\n * Determine the type of allocation constraint.\n */\n#ifdef CONFIG_NUMA\nstatic enum oom_constraint constrained_alloc(struct zonelist *zonelist,\n\t\t\t\tgfp_t gfp_mask, nodemask_t *nodemask,\n\t\t\t\tunsigned long *totalpages)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tenum zone_type high_zoneidx = gfp_zone(gfp_mask);\n\tbool cpuset_limited = false;\n\tint nid;\n\n\t/* Default to all available memory */\n\t*totalpages = totalram_pages + total_swap_pages;\n\n\tif (!zonelist)\n\t\treturn CONSTRAINT_NONE;\n\t/*\n\t * Reach here only when __GFP_NOFAIL is used. So, we should avoid\n\t * to kill current.We have to random task kill in this case.\n\t * Hopefully, CONSTRAINT_THISNODE...but no way to handle it, now.\n\t */\n\tif (gfp_mask & __GFP_THISNODE)\n\t\treturn CONSTRAINT_NONE;\n\n\t/*\n\t * This is not a __GFP_THISNODE allocation, so a truncated nodemask in\n\t * the page allocator means a mempolicy is in effect.  Cpuset policy\n\t * is enforced in get_page_from_freelist().\n\t */\n\tif (nodemask && !nodes_subset(node_states[N_HIGH_MEMORY], *nodemask)) {\n\t\t*totalpages = total_swap_pages;\n\t\tfor_each_node_mask(nid, *nodemask)\n\t\t\t*totalpages += node_spanned_pages(nid);\n\t\treturn CONSTRAINT_MEMORY_POLICY;\n\t}\n\n\t/* Check this allocation failure is caused by cpuset's wall function */\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n\t\t\thigh_zoneidx, nodemask)\n\t\tif (!cpuset_zone_allowed_softwall(zone, gfp_mask))\n\t\t\tcpuset_limited = true;\n\n\tif (cpuset_limited) {\n\t\t*totalpages = total_swap_pages;\n\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n\t\t\t*totalpages += node_spanned_pages(nid);\n\t\treturn CONSTRAINT_CPUSET;\n\t}\n\treturn CONSTRAINT_NONE;\n}\n#else\nstatic enum oom_constraint constrained_alloc(struct zonelist *zonelist,\n\t\t\t\tgfp_t gfp_mask, nodemask_t *nodemask,\n\t\t\t\tunsigned long *totalpages)\n{\n\t*totalpages = totalram_pages + total_swap_pages;\n\treturn CONSTRAINT_NONE;\n}\n#endif\n\n/*\n * Simple selection loop. We chose the process with the highest\n * number of 'points'. We expect the caller will lock the tasklist.\n *\n * (not docbooked, we don't want this one cluttering up the manual)\n */\nstatic struct task_struct *select_bad_process(unsigned int *ppoints,\n\t\tunsigned long totalpages, struct mem_cgroup *mem,\n\t\tconst nodemask_t *nodemask)\n{\n\tstruct task_struct *g, *p;\n\tstruct task_struct *chosen = NULL;\n\t*ppoints = 0;\n\n\tdo_each_thread(g, p) {\n\t\tunsigned int points;\n\n\t\tif (p->exit_state)\n\t\t\tcontinue;\n\t\tif (oom_unkillable_task(p, mem, nodemask))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * This task already has access to memory reserves and is\n\t\t * being killed. Don't allow any other task access to the\n\t\t * memory reserve.\n\t\t *\n\t\t * Note: this may have a chance of deadlock if it gets\n\t\t * blocked waiting for another task which itself is waiting\n\t\t * for memory. Is there a better alternative?\n\t\t */\n\t\tif (test_tsk_thread_flag(p, TIF_MEMDIE))\n\t\t\treturn ERR_PTR(-1UL);\n\t\tif (!p->mm)\n\t\t\tcontinue;\n\n\t\tif (p->flags & PF_EXITING) {\n\t\t\t/*\n\t\t\t * If p is the current task and is in the process of\n\t\t\t * releasing memory, we allow the \"kill\" to set\n\t\t\t * TIF_MEMDIE, which will allow it to gain access to\n\t\t\t * memory reserves.  Otherwise, it may stall forever.\n\t\t\t *\n\t\t\t * The loop isn't broken here, however, in case other\n\t\t\t * threads are found to have already been oom killed.\n\t\t\t */\n\t\t\tif (p == current) {\n\t\t\t\tchosen = p;\n\t\t\t\t*ppoints = 1000;\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * If this task is not being ptraced on exit,\n\t\t\t\t * then wait for it to finish before killing\n\t\t\t\t * some other task unnecessarily.\n\t\t\t\t */\n\t\t\t\tif (!(p->group_leader->ptrace & PT_TRACE_EXIT))\n\t\t\t\t\treturn ERR_PTR(-1UL);\n\t\t\t}\n\t\t}\n\n\t\tpoints = oom_badness(p, mem, nodemask, totalpages);\n\t\tif (points > *ppoints) {\n\t\t\tchosen = p;\n\t\t\t*ppoints = points;\n\t\t}\n\t} while_each_thread(g, p);\n\n\treturn chosen;\n}\n\n/**\n * dump_tasks - dump current memory state of all system tasks\n * @mem: current's memory controller, if constrained\n * @nodemask: nodemask passed to page allocator for mempolicy ooms\n *\n * Dumps the current memory state of all eligible tasks.  Tasks not in the same\n * memcg, not in the same cpuset, or bound to a disjoint set of mempolicy nodes\n * are not shown.\n * State information includes task's pid, uid, tgid, vm size, rss, cpu, oom_adj\n * value, oom_score_adj value, and name.\n *\n * Call with tasklist_lock read-locked.\n */\nstatic void dump_tasks(const struct mem_cgroup *mem, const nodemask_t *nodemask)\n{\n\tstruct task_struct *p;\n\tstruct task_struct *task;\n\n\tpr_info(\"[ pid ]   uid  tgid total_vm      rss cpu oom_adj oom_score_adj name\\n\");\n\tfor_each_process(p) {\n\t\tif (oom_unkillable_task(p, mem, nodemask))\n\t\t\tcontinue;\n\n\t\ttask = find_lock_task_mm(p);\n\t\tif (!task) {\n\t\t\t/*\n\t\t\t * This is a kthread or all of p's threads have already\n\t\t\t * detached their mm's.  There's no need to report\n\t\t\t * them; they can't be oom killed anyway.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tpr_info(\"[%5d] %5d %5d %8lu %8lu %3u     %3d         %5d %s\\n\",\n\t\t\ttask->pid, task_uid(task), task->tgid,\n\t\t\ttask->mm->total_vm, get_mm_rss(task->mm),\n\t\t\ttask_cpu(task), task->signal->oom_adj,\n\t\t\ttask->signal->oom_score_adj, task->comm);\n\t\ttask_unlock(task);\n\t}\n}\n\nstatic void dump_header(struct task_struct *p, gfp_t gfp_mask, int order,\n\t\t\tstruct mem_cgroup *mem, const nodemask_t *nodemask)\n{\n\ttask_lock(current);\n\tpr_warning(\"%s invoked oom-killer: gfp_mask=0x%x, order=%d, \"\n\t\t\"oom_adj=%d, oom_score_adj=%d\\n\",\n\t\tcurrent->comm, gfp_mask, order, current->signal->oom_adj,\n\t\tcurrent->signal->oom_score_adj);\n\tcpuset_print_task_mems_allowed(current);\n\ttask_unlock(current);\n\tdump_stack();\n\tmem_cgroup_print_oom_info(mem, p);\n\tshow_mem(SHOW_MEM_FILTER_NODES);\n\tif (sysctl_oom_dump_tasks)\n\t\tdump_tasks(mem, nodemask);\n}\n\n#define K(x) ((x) << (PAGE_SHIFT-10))\nstatic int oom_kill_task(struct task_struct *p, struct mem_cgroup *mem)\n{\n\tstruct task_struct *q;\n\tstruct mm_struct *mm;\n\n\tp = find_lock_task_mm(p);\n\tif (!p)\n\t\treturn 1;\n\n\t/* mm cannot be safely dereferenced after task_unlock(p) */\n\tmm = p->mm;\n\n\tpr_err(\"Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB\\n\",\n\t\ttask_pid_nr(p), p->comm, K(p->mm->total_vm),\n\t\tK(get_mm_counter(p->mm, MM_ANONPAGES)),\n\t\tK(get_mm_counter(p->mm, MM_FILEPAGES)));\n\ttask_unlock(p);\n\n\t/*\n\t * Kill all processes sharing p->mm in other thread groups, if any.\n\t * They don't get access to memory reserves or a higher scheduler\n\t * priority, though, to avoid depletion of all memory or task\n\t * starvation.  This prevents mm->mmap_sem livelock when an oom killed\n\t * task cannot exit because it requires the semaphore and its contended\n\t * by another thread trying to allocate memory itself.  That thread will\n\t * now get access to memory reserves since it has a pending fatal\n\t * signal.\n\t */\n\tfor_each_process(q)\n\t\tif (q->mm == mm && !same_thread_group(q, p)) {\n\t\t\ttask_lock(q);\t/* Protect ->comm from prctl() */\n\t\t\tpr_err(\"Kill process %d (%s) sharing same memory\\n\",\n\t\t\t\ttask_pid_nr(q), q->comm);\n\t\t\ttask_unlock(q);\n\t\t\tforce_sig(SIGKILL, q);\n\t\t}\n\n\tset_tsk_thread_flag(p, TIF_MEMDIE);\n\tforce_sig(SIGKILL, p);\n\n\treturn 0;\n}\n#undef K\n\nstatic int oom_kill_process(struct task_struct *p, gfp_t gfp_mask, int order,\n\t\t\t    unsigned int points, unsigned long totalpages,\n\t\t\t    struct mem_cgroup *mem, nodemask_t *nodemask,\n\t\t\t    const char *message)\n{\n\tstruct task_struct *victim = p;\n\tstruct task_struct *child;\n\tstruct task_struct *t = p;\n\tunsigned int victim_points = 0;\n\n\tif (printk_ratelimit())\n\t\tdump_header(p, gfp_mask, order, mem, nodemask);\n\n\t/*\n\t * If the task is already exiting, don't alarm the sysadmin or kill\n\t * its children or threads, just set TIF_MEMDIE so it can die quickly\n\t */\n\tif (p->flags & PF_EXITING) {\n\t\tset_tsk_thread_flag(p, TIF_MEMDIE);\n\t\treturn 0;\n\t}\n\n\ttask_lock(p);\n\tpr_err(\"%s: Kill process %d (%s) score %d or sacrifice child\\n\",\n\t\tmessage, task_pid_nr(p), p->comm, points);\n\ttask_unlock(p);\n\n\t/*\n\t * If any of p's children has a different mm and is eligible for kill,\n\t * the one with the highest oom_badness() score is sacrificed for its\n\t * parent.  This attempts to lose the minimal amount of work done while\n\t * still freeing memory.\n\t */\n\tdo {\n\t\tlist_for_each_entry(child, &t->children, sibling) {\n\t\t\tunsigned int child_points;\n\n\t\t\tif (child->mm == p->mm)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * oom_badness() returns 0 if the thread is unkillable\n\t\t\t */\n\t\t\tchild_points = oom_badness(child, mem, nodemask,\n\t\t\t\t\t\t\t\ttotalpages);\n\t\t\tif (child_points > victim_points) {\n\t\t\t\tvictim = child;\n\t\t\t\tvictim_points = child_points;\n\t\t\t}\n\t\t}\n\t} while_each_thread(p, t);\n\n\treturn oom_kill_task(victim, mem);\n}\n\n/*\n * Determines whether the kernel must panic because of the panic_on_oom sysctl.\n */\nstatic void check_panic_on_oom(enum oom_constraint constraint, gfp_t gfp_mask,\n\t\t\t\tint order, const nodemask_t *nodemask)\n{\n\tif (likely(!sysctl_panic_on_oom))\n\t\treturn;\n\tif (sysctl_panic_on_oom != 2) {\n\t\t/*\n\t\t * panic_on_oom == 1 only affects CONSTRAINT_NONE, the kernel\n\t\t * does not panic for cpuset, mempolicy, or memcg allocation\n\t\t * failures.\n\t\t */\n\t\tif (constraint != CONSTRAINT_NONE)\n\t\t\treturn;\n\t}\n\tread_lock(&tasklist_lock);\n\tdump_header(NULL, gfp_mask, order, NULL, nodemask);\n\tread_unlock(&tasklist_lock);\n\tpanic(\"Out of memory: %s panic_on_oom is enabled\\n\",\n\t\tsysctl_panic_on_oom == 2 ? \"compulsory\" : \"system-wide\");\n}\n\n#ifdef CONFIG_CGROUP_MEM_RES_CTLR\nvoid mem_cgroup_out_of_memory(struct mem_cgroup *mem, gfp_t gfp_mask)\n{\n\tunsigned long limit;\n\tunsigned int points = 0;\n\tstruct task_struct *p;\n\n\t/*\n\t * If current has a pending SIGKILL, then automatically select it.  The\n\t * goal is to allow it to allocate so that it may quickly exit and free\n\t * its memory.\n\t */\n\tif (fatal_signal_pending(current)) {\n\t\tset_thread_flag(TIF_MEMDIE);\n\t\treturn;\n\t}\n\n\tcheck_panic_on_oom(CONSTRAINT_MEMCG, gfp_mask, 0, NULL);\n\tlimit = mem_cgroup_get_limit(mem) >> PAGE_SHIFT;\n\tread_lock(&tasklist_lock);\nretry:\n\tp = select_bad_process(&points, limit, mem, NULL);\n\tif (!p || PTR_ERR(p) == -1UL)\n\t\tgoto out;\n\n\tif (oom_kill_process(p, gfp_mask, 0, points, limit, mem, NULL,\n\t\t\t\t\"Memory cgroup out of memory\"))\n\t\tgoto retry;\nout:\n\tread_unlock(&tasklist_lock);\n}\n#endif\n\nstatic BLOCKING_NOTIFIER_HEAD(oom_notify_list);\n\nint register_oom_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&oom_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(register_oom_notifier);\n\nint unregister_oom_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&oom_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(unregister_oom_notifier);\n\n/*\n * Try to acquire the OOM killer lock for the zones in zonelist.  Returns zero\n * if a parallel OOM killing is already taking place that includes a zone in\n * the zonelist.  Otherwise, locks all zones in the zonelist and returns 1.\n */\nint try_set_zonelist_oom(struct zonelist *zonelist, gfp_t gfp_mask)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tint ret = 1;\n\n\tspin_lock(&zone_scan_lock);\n\tfor_each_zone_zonelist(zone, z, zonelist, gfp_zone(gfp_mask)) {\n\t\tif (zone_is_oom_locked(zone)) {\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor_each_zone_zonelist(zone, z, zonelist, gfp_zone(gfp_mask)) {\n\t\t/*\n\t\t * Lock each zone in the zonelist under zone_scan_lock so a\n\t\t * parallel invocation of try_set_zonelist_oom() doesn't succeed\n\t\t * when it shouldn't.\n\t\t */\n\t\tzone_set_flag(zone, ZONE_OOM_LOCKED);\n\t}\n\nout:\n\tspin_unlock(&zone_scan_lock);\n\treturn ret;\n}\n\n/*\n * Clears the ZONE_OOM_LOCKED flag for all zones in the zonelist so that failed\n * allocation attempts with zonelists containing them may now recall the OOM\n * killer, if necessary.\n */\nvoid clear_zonelist_oom(struct zonelist *zonelist, gfp_t gfp_mask)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\n\tspin_lock(&zone_scan_lock);\n\tfor_each_zone_zonelist(zone, z, zonelist, gfp_zone(gfp_mask)) {\n\t\tzone_clear_flag(zone, ZONE_OOM_LOCKED);\n\t}\n\tspin_unlock(&zone_scan_lock);\n}\n\n/*\n * Try to acquire the oom killer lock for all system zones.  Returns zero if a\n * parallel oom killing is taking place, otherwise locks all zones and returns\n * non-zero.\n */\nstatic int try_set_system_oom(void)\n{\n\tstruct zone *zone;\n\tint ret = 1;\n\n\tspin_lock(&zone_scan_lock);\n\tfor_each_populated_zone(zone)\n\t\tif (zone_is_oom_locked(zone)) {\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\tfor_each_populated_zone(zone)\n\t\tzone_set_flag(zone, ZONE_OOM_LOCKED);\nout:\n\tspin_unlock(&zone_scan_lock);\n\treturn ret;\n}\n\n/*\n * Clears ZONE_OOM_LOCKED for all system zones so that failed allocation\n * attempts or page faults may now recall the oom killer, if necessary.\n */\nstatic void clear_system_oom(void)\n{\n\tstruct zone *zone;\n\n\tspin_lock(&zone_scan_lock);\n\tfor_each_populated_zone(zone)\n\t\tzone_clear_flag(zone, ZONE_OOM_LOCKED);\n\tspin_unlock(&zone_scan_lock);\n}\n\n/**\n * out_of_memory - kill the \"best\" process when we run out of memory\n * @zonelist: zonelist pointer\n * @gfp_mask: memory allocation flags\n * @order: amount of memory being requested as a power of 2\n * @nodemask: nodemask passed to page allocator\n *\n * If we run out of memory, we have the choice between either\n * killing a random task (bad), letting the system crash (worse)\n * OR try to be smart about which process to kill. Note that we\n * don't have to be perfect here, we just have to be good.\n */\nvoid out_of_memory(struct zonelist *zonelist, gfp_t gfp_mask,\n\t\tint order, nodemask_t *nodemask)\n{\n\tconst nodemask_t *mpol_mask;\n\tstruct task_struct *p;\n\tunsigned long totalpages;\n\tunsigned long freed = 0;\n\tunsigned int points;\n\tenum oom_constraint constraint = CONSTRAINT_NONE;\n\tint killed = 0;\n\n\tblocking_notifier_call_chain(&oom_notify_list, 0, &freed);\n\tif (freed > 0)\n\t\t/* Got some memory back in the last second. */\n\t\treturn;\n\n\t/*\n\t * If current has a pending SIGKILL, then automatically select it.  The\n\t * goal is to allow it to allocate so that it may quickly exit and free\n\t * its memory.\n\t */\n\tif (fatal_signal_pending(current)) {\n\t\tset_thread_flag(TIF_MEMDIE);\n\t\treturn;\n\t}\n\n\t/*\n\t * Check if there were limitations on the allocation (only relevant for\n\t * NUMA) that may require different handling.\n\t */\n\tconstraint = constrained_alloc(zonelist, gfp_mask, nodemask,\n\t\t\t\t\t\t&totalpages);\n\tmpol_mask = (constraint == CONSTRAINT_MEMORY_POLICY) ? nodemask : NULL;\n\tcheck_panic_on_oom(constraint, gfp_mask, order, mpol_mask);\n\n\tread_lock(&tasklist_lock);\n\tif (sysctl_oom_kill_allocating_task &&\n\t    !oom_unkillable_task(current, NULL, nodemask) &&\n\t    current->mm && !atomic_read(&current->mm->oom_disable_count)) {\n\t\t/*\n\t\t * oom_kill_process() needs tasklist_lock held.  If it returns\n\t\t * non-zero, current could not be killed so we must fallback to\n\t\t * the tasklist scan.\n\t\t */\n\t\tif (!oom_kill_process(current, gfp_mask, order, 0, totalpages,\n\t\t\t\tNULL, nodemask,\n\t\t\t\t\"Out of memory (oom_kill_allocating_task)\"))\n\t\t\tgoto out;\n\t}\n\nretry:\n\tp = select_bad_process(&points, totalpages, NULL, mpol_mask);\n\tif (PTR_ERR(p) == -1UL)\n\t\tgoto out;\n\n\t/* Found nothing?!?! Either we hang forever, or we panic. */\n\tif (!p) {\n\t\tdump_header(NULL, gfp_mask, order, NULL, mpol_mask);\n\t\tread_unlock(&tasklist_lock);\n\t\tpanic(\"Out of memory and no killable processes...\\n\");\n\t}\n\n\tif (oom_kill_process(p, gfp_mask, order, points, totalpages, NULL,\n\t\t\t\tnodemask, \"Out of memory\"))\n\t\tgoto retry;\n\tkilled = 1;\nout:\n\tread_unlock(&tasklist_lock);\n\n\t/*\n\t * Give \"p\" a good chance of killing itself before we\n\t * retry to allocate memory unless \"p\" is current\n\t */\n\tif (killed && !test_thread_flag(TIF_MEMDIE))\n\t\tschedule_timeout_uninterruptible(1);\n}\n\n/*\n * The pagefault handler calls here because it is out of memory, so kill a\n * memory-hogging task.  If a populated zone has ZONE_OOM_LOCKED set, a parallel\n * oom killing is already in progress so do nothing.  If a task is found with\n * TIF_MEMDIE set, it has been killed so do nothing and allow it to exit.\n */\nvoid pagefault_out_of_memory(void)\n{\n\tif (try_set_system_oom()) {\n\t\tout_of_memory(NULL, 0, 0, NULL);\n\t\tclear_system_oom();\n\t}\n\tif (!test_thread_flag(TIF_MEMDIE))\n\t\tschedule_timeout_uninterruptible(1);\n}\n"], "filenames": ["mm/oom_kill.c"], "buggy_code_start_loc": [165], "buggy_code_end_loc": [166], "fixing_code_start_loc": [165], "fixing_code_end_loc": [166], "type": "CWE-190", "message": "Integer overflow in the oom_badness function in mm/oom_kill.c in the Linux kernel before 3.1.8 on 64-bit platforms allows local users to cause a denial of service (memory consumption or process termination) by using a certain large amount of memory.", "other": {"cve": {"id": "CVE-2011-4097", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-17T11:00:32.303", "lastModified": "2023-02-13T01:21:27.080", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Integer overflow in the oom_badness function in mm/oom_kill.c in the Linux kernel before 3.1.8 on 64-bit platforms allows local users to cause a denial of service (memory consumption or process termination) by using a certain large amount of memory."}, {"lang": "es", "value": "Desbordamiento de entero en la funci\u00f3n oom_badness en mm/oom_kill.c en el n\u00facleo de Linux anteriores a v3.1.8 en plataformas de 64 bits, que permite a usuarios locales causar una denegaci\u00f3n de servicio (consumo de memoria y terminaci\u00f3n del proceso) mediante el uso de una cierta cantidad grande de memoria."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:x64:*", "versionEndExcluding": "3.1.8", "matchCriteriaId": "7AC86F0A-F1BE-4A46-B027-0C982228BD25"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:6.0:*:*:*:*:*:*:*", "matchCriteriaId": "2F6AB192-9D7D-4A9A-8995-E53A9DE9EAFC"}]}]}], "references": [{"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.1.8", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2011/11/01/2", "source": "secalert@redhat.com", "tags": ["Exploit", "Mailing List", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=750399", "source": "secalert@redhat.com", "tags": ["Exploit", "Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/56c6a8a4aadca809e04276eabe5552935c51387f", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/56c6a8a4aadca809e04276eabe5552935c51387f"}}