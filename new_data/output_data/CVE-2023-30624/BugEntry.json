{"buggy_code": ["--------------------------------------------------------------------------------\n\n## 9.0.0\n\nUnreleased.\n\n### Added\n\n### Changed\n\n--------------------------------------------------------------------------------\n\n## 8.0.0\n\nReleased 2023-04-20\n\n### Added\n\n* Allow the MPL-2.0 and OpenSSL licenses in dependencies of wasmtime.\n  [#6136](https://github.com/bytecodealliance/wasmtime/pull/6136)\n\n* Add a bounds-checking optimization for dynamic memories and guard pages.\n  [#6031](https://github.com/bytecodealliance/wasmtime/pull/6031)\n\n* Add support for generating perf maps for simple perf profiling. Additionally,\n  the `--jitdump` and `--vtune` flags have been replaced with a single\n  `--profile` flags that accepts `perfmap`, `jitdump`, and `vtune` arguments.\n  [#6030](https://github.com/bytecodealliance/wasmtime/pull/6030)\n\n* Validate faulting addresses are valid to fault on. As a mitigation to CVEs\n  like `GHSA-ff4p-7xrq-q5r8`, check that the address involved in a fault is one\n  that could be contained in a `Store`, or print a scary message and abort\n  immediately.\n  [#6028](https://github.com/bytecodealliance/wasmtime/pull/6028)\n\n* Add the `--default-values-unknown-imports` option to define unknown function\n  imports as functions that return the default value for their result type.\n  [#6010](https://github.com/bytecodealliance/wasmtime/pull/6010)\n\n* Add `Clone` for `component::InstancePre`.\n  [#5996](https://github.com/bytecodealliance/wasmtime/issues/5996)\n\n* Add `--dynamic-memory-reserved-for-growth` cli flag.\n  [#5980](https://github.com/bytecodealliance/wasmtime/issues/5980)\n\n* Introduce the `wasmtime-explorer` crate for investigating the compilation of\n  wasm modules. This functionality is also exposed via the `wasmtime explore`\n  command.\n  [#5975](https://github.com/bytecodealliance/wasmtime/pull/5975)\n\n* Added support for the Relaxed SIMD proposal.\n  [#5892](https://github.com/bytecodealliance/wasmtime/pull/5892)\n\n* Cranelift gained many new machine-independent optimizations.\n  [#5909](https://github.com/bytecodealliance/wasmtime/pull/5909)\n  [#6032](https://github.com/bytecodealliance/wasmtime/pull/6032)\n  [#6033](https://github.com/bytecodealliance/wasmtime/pull/6033)\n  [#6034](https://github.com/bytecodealliance/wasmtime/pull/6034)\n  [#6037](https://github.com/bytecodealliance/wasmtime/pull/6037)\n  [#6052](https://github.com/bytecodealliance/wasmtime/pull/6052)\n  [#6053](https://github.com/bytecodealliance/wasmtime/pull/6053)\n  [#6072](https://github.com/bytecodealliance/wasmtime/pull/6072)\n  [#6095](https://github.com/bytecodealliance/wasmtime/pull/6095)\n  [#6130](https://github.com/bytecodealliance/wasmtime/pull/6130)\n\n### Changed\n\n* Derive `Copy` on `wasmtime::ValType`.\n  [#6138](https://github.com/bytecodealliance/wasmtime/pull/6138)\n\n* Make `StoreContextMut` accessible in the epoch deadline callback.\n  [#6075](https://github.com/bytecodealliance/wasmtime/pull/6075)\n\n* Take SIGFPE signals for divide traps on `x86_64`.\n  [#6026](https://github.com/bytecodealliance/wasmtime/pull/6026)\n\n* Use more specialized AVX instructions in the `x86_64` backend.\n  [#5924](https://github.com/bytecodealliance/wasmtime/pull/5924)\n  [#5930](https://github.com/bytecodealliance/wasmtime/pull/5930)\n  [#5931](https://github.com/bytecodealliance/wasmtime/pull/5931)\n  [#5982](https://github.com/bytecodealliance/wasmtime/pull/5982)\n  [#5986](https://github.com/bytecodealliance/wasmtime/pull/5986)\n  [#5999](https://github.com/bytecodealliance/wasmtime/pull/5999)\n  [#6023](https://github.com/bytecodealliance/wasmtime/pull/6023)\n  [#6025](https://github.com/bytecodealliance/wasmtime/pull/6025)\n  [#6060](https://github.com/bytecodealliance/wasmtime/pull/6060)\n  [#6086](https://github.com/bytecodealliance/wasmtime/pull/6086)\n  [#6092](https://github.com/bytecodealliance/wasmtime/pull/6092)\n\n* Generate more cache-friendly code for traps.\n  [#6011](https://github.com/bytecodealliance/wasmtime/pull/6011)\n\n### Fixed\n\n* Fixed suboptimal code generation in the `aarch64` backend.\n  [#5976](https://github.com/bytecodealliance/wasmtime/pull/5976)\n  [#5977](https://github.com/bytecodealliance/wasmtime/pull/5977)\n  [#5987](https://github.com/bytecodealliance/wasmtime/pull/5987)\n  [#5997](https://github.com/bytecodealliance/wasmtime/pull/5997)\n  [#6078](https://github.com/bytecodealliance/wasmtime/pull/6078)\n\n* Fixed suboptimal code generation in the `riscv64` backend.\n  [#5854](https://github.com/bytecodealliance/wasmtime/pull/5854)\n  [#5857](https://github.com/bytecodealliance/wasmtime/pull/5857)\n  [#5919](https://github.com/bytecodealliance/wasmtime/pull/5919)\n  [#5951](https://github.com/bytecodealliance/wasmtime/pull/5951)\n  [#5964](https://github.com/bytecodealliance/wasmtime/pull/5964)\n  [#6087](https://github.com/bytecodealliance/wasmtime/pull/6087)\n\n\n--------------------------------------------------------------------------------\n\n## 7.0.0\n\nReleased 2023-03-20\n\n### Added\n\n* An initial implementation of the wasi-threads proposal has been implemented\n  and landed in the Wasmtime CLI. This is available behind a\n  `--wasi-modules experimental-wasi-threads` flag.\n  [#5484](https://github.com/bytecodealliance/wasmtime/pull/5484)\n\n* Support for WASI sockets has been added to the C API.\n  [#5624](https://github.com/bytecodealliance/wasmtime/pull/5624)\n\n* Support for limiting `Store`-based resource usage, such as memory, tables,\n  etc, has been added to the C API.\n  [#5761](https://github.com/bytecodealliance/wasmtime/pull/5761)\n\n* A top level alias of `anyhow::Result` as `wasmtime::Result` has been added to\n  avoid the need to explicitly depend on `anyhow`.\n  [#5853](https://github.com/bytecodealliance/wasmtime/pull/5853)\n\n* Initial support for the WebAssembly core dump format has been added to the CLI\n  with a `--coredump-on-trap` flag.\n  [#5868](https://github.com/bytecodealliance/wasmtime/pull/5868)\n\n### Changed\n\n* The `S` type parameter on component-related methods has been removed.\n  [#5722](https://github.com/bytecodealliance/wasmtime/pull/5722)\n\n* Selection of a `world` to bindgen has been updated to select any `default\n  world` in a WIT package if there is only one.\n  [#5779](https://github.com/bytecodealliance/wasmtime/pull/5779)\n\n* WASI preopened file descriptors can now be closed.\n  [#5828](https://github.com/bytecodealliance/wasmtime/pull/5828)\n\n* The host traits generated by the `bindgen!` macro are now always named `Host`,\n  but are still scoped to each individual module.\n  [#5890](https://github.com/bytecodealliance/wasmtime/pull/5890)\n\n### Fixed\n\n* Components which have `type` imports are now supported better and error/panic\n  in fewer cases.\n  [#5777](https://github.com/bytecodealliance/wasmtime/pull/5777)\n\n* Types referred to by `wasmtime::component::Val` are now reexported under\n  `wasmtime::component`.\n  [#5790](https://github.com/bytecodealliance/wasmtime/pull/5790)\n\n* A panic due to a race between `memory.atomic.{wait32,wait64,notify}`\n  instructions has been fixed.\n  [#5871](https://github.com/bytecodealliance/wasmtime/pull/5871)\n\n* Guest-controlled out-of-bounds read/write on x86\\_64\n  [GHSA-ff4p-7xrq-q5r8](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8)\n\n*  Miscompilation of `i8x16.select` with the same inputs on x86\\_64\n  [GHSA-xm67-587q-r2vw](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw)\n\n--------------------------------------------------------------------------------\n\n## 6.0.1\n\nReleased 2023-03-08.\n\n### Fixed\n\n* Guest-controlled out-of-bounds read/write on x86\\_64\n  [GHSA-ff4p-7xrq-q5r8](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8)\n\n*  Miscompilation of `i8x16.select` with the same inputs on x86\\_64\n  [GHSA-xm67-587q-r2vw](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw)\n\n--------------------------------------------------------------------------------\n\n## 6.0.0\n\nReleased 2023-02-20\n\n### Added\n\n* Wasmtime's built-in cache can now be disabled after being enabled previously.\n  [#5542](https://github.com/bytecodealliance/wasmtime/pull/5542)\n\n* Older x86\\_64 CPUs, without SSE4.1 for example, are now supported when the\n  wasm SIMD proposal is disabled.\n  [#5567](https://github.com/bytecodealliance/wasmtime/pull/5567)\n\n* The Wasmtime C API now has `WASMTIME_VERSION_*` macros defined in its header\n  files.\n  [#5651](https://github.com/bytecodealliance/wasmtime/pull/5651)\n\n* The `wasmtime` CLI executable as part of Wasmtime's precompiled release\n  artifacts now has the `all-arch` feature enabled.\n  [#5657](https://github.com/bytecodealliance/wasmtime/pull/5657)\n\n### Changed\n\n* Equality of `wasmtime::component::Val::Float{32,64}` now considers NaNs as\n  equal for assistance when fuzzing.\n  [#5535](https://github.com/bytecodealliance/wasmtime/pull/5535)\n\n* WIT syntax supported by `wasmtime::component::bindgen!` has been updated in\n  addition to the generated code being updated.\n  [#5565](https://github.com/bytecodealliance/wasmtime/pull/5565)\n  [#5692](https://github.com/bytecodealliance/wasmtime/pull/5692)\n  [#5694](https://github.com/bytecodealliance/wasmtime/pull/5694)\n\n* Cranelift's egraph-based optimization framework is now enabled by default.\n  [#5587](https://github.com/bytecodealliance/wasmtime/pull/5587)\n\n* The old `PoolingAllocationStrategy` type has been removed in favor of a more\n  flexible configuration via a new option\n  `PoolingAllocationConfig::max_unused_warm_slots` which is more flexible and\n  subsumes the previous use cases for each strategy.\n  [#5661](https://github.com/bytecodealliance/wasmtime/pull/5661)\n\n* Creation of `InstancePre` through `Linker::instantiate_pre` no longer requires\n  a `Store` to be provided. Instead a `Store`-related argument is now required\n  on `Linker::define`-style APIs instead.\n  [#5683](https://github.com/bytecodealliance/wasmtime/pull/5683)\n\n### Fixed\n\n* Compilation for FreeBSD on x86\\_64 and AArch64 has been fixed.\n  [#5606](https://github.com/bytecodealliance/wasmtime/pull/5606)\n\n--------------------------------------------------------------------------------\n\n## 5.0.1\n\nReleased 2023-03-08.\n\n### Fixed\n\n* Guest-controlled out-of-bounds read/write on x86\\_64\n  [GHSA-ff4p-7xrq-q5r8](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8)\n\n*  Miscompilation of `i8x16.select` with the same inputs on x86\\_64\n  [GHSA-xm67-587q-r2vw](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw)\n\n--------------------------------------------------------------------------------\n\n## 5.0.0\n\nReleased 2023-01-20\n\n### Added\n\n* A `wasmtime::component::bingen!` macro has been added for generating bindings\n  from `*.wit` files. Note that WIT is still heavily in development so this is\n  more of a preview of what will be as opposed to a finished feature.\n  [#5317](https://github.com/bytecodealliance/wasmtime/pull/5317)\n  [#5397](https://github.com/bytecodealliance/wasmtime/pull/5397)\n\n* The `wasmtime settings` CLI command now has a `--json` option for\n  machine-readable output.\n  [#5411](https://github.com/bytecodealliance/wasmtime/pull/5411)\n\n* Wiggle-generated bindings can now generate the trait for either `&mut self` or\n  `&self`.\n  [#5428](https://github.com/bytecodealliance/wasmtime/pull/5428)\n\n* The `wiggle` crate has more convenience APIs for working with guest data\n  that resides in shared memory.\n  [#5471](https://github.com/bytecodealliance/wasmtime/pull/5471)\n  [#5475](https://github.com/bytecodealliance/wasmtime/pull/5475)\n\n### Changed\n\n* Cranelift's egraph support has been rewritten and updated. This functionality\n  is still gated behind a flag and may become the default in the next release.\n  [#5382](https://github.com/bytecodealliance/wasmtime/pull/5382)\n\n* The implementation of codegen for WebAssembly linear memory has changed\n  significantly internally in Cranelift, moving more responsibility to the\n  Wasmtime embedding rather than Cranelift itself. This should have no\n  user-visible change, however.\n  [#5386](https://github.com/bytecodealliance/wasmtime/pull/5386)\n\n* The `Val::Float32` and `Val::Float64` variants for components now store `f32`\n  and `f64` instead of the bit representation.\n  [#5510](https://github.com/bytecodealliance/wasmtime/pull/5510)\n\n### Fixed\n\n* Handling of DWARF debugging information in components with multiple modules\n  has been fixed to ensure the right info is used for each module.\n  [#5358](https://github.com/bytecodealliance/wasmtime/pull/5358)\n\n--------------------------------------------------------------------------------\n\n## 4.0.1\n\nReleased 2023-03-08.\n\n### Fixed\n\n* Guest-controlled out-of-bounds read/write on x86\\_64\n  [GHSA-ff4p-7xrq-q5r8](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8)\n\n*  Miscompilation of `i8x16.select` with the same inputs on x86\\_64\n  [GHSA-xm67-587q-r2vw](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw)\n\n--------------------------------------------------------------------------------\n\n## 4.0.0\n\nReleased 2022-12-20\n\n### Added\n\n* Dynamic memories are now supported with the pooling instance allocator which\n  can possibly reduce the number of page faults throughout execution at the cost\n  of slower to run code. Page faults are primarily reduced by avoiding\n  releasing memory back to the system, relying on bounds checks to keep the\n  memory inaccessible.\n  [#5208](https://github.com/bytecodealliance/wasmtime/pull/5208)\n\n* The `wiggle` generator now supports function-level control over `tracing`\n  calls.\n  [#5194](https://github.com/bytecodealliance/wasmtime/pull/5194)\n\n* Support has been added to `wiggle` to be compatible with shared memories.\n  [#5225](https://github.com/bytecodealliance/wasmtime/pull/5225)\n  [#5229](https://github.com/bytecodealliance/wasmtime/pull/5229)\n  [#5264](https://github.com/bytecodealliance/wasmtime/pull/5264)\n  [#5268](https://github.com/bytecodealliance/wasmtime/pull/5268)\n  [#5054](https://github.com/bytecodealliance/wasmtime/pull/5054)\n\n* The `wiggle` generator now supports a \"trappable error\" configuration to\n  improve error conversions to guest errors and ensure that no host errors are\n  forgotten or accidentally become traps. The `wasi-common` crate has been\n  updated to use this.\n  [#5276](https://github.com/bytecodealliance/wasmtime/pull/5276)\n  [#5279](https://github.com/bytecodealliance/wasmtime/pull/5279)\n\n* The `memory.atomic.{notify,wait32,wait64}` instructions are now all\n  implemented in Wasmtime.\n  [#5255](https://github.com/bytecodealliance/wasmtime/pull/5255)\n  [#5311](https://github.com/bytecodealliance/wasmtime/pull/5311)\n\n* A `wasm_config_parallel_compilation_set` configuration function has been added\n  to the C API.\n  [#5298](https://github.com/bytecodealliance/wasmtime/pull/5298)\n\n* The `wasmtime` CLI can have its input module piped into it from stdin now.\n  [#5342](https://github.com/bytecodealliance/wasmtime/pull/5342)\n\n* `WasmBacktrace::{capture,force_capture}` methods have been added to\n  programmatically capture a backtrace outside of a trapping context.\n  [#5341](https://github.com/bytecodealliance/wasmtime/pull/5341)\n\n### Changed\n\n* The `S` type parameter on `Func::typed` and `Instance::get_typed_func` has\n  been removed and no longer needs to be specified.\n  [#5275](https://github.com/bytecodealliance/wasmtime/pull/5275)\n\n* The `SharedMemory::data` method now returns `&[UnsafeCell<u8>]` instead of the\n  prior raw slice return.\n  [#5240](https://github.com/bytecodealliance/wasmtime/pull/5240)\n\n* Creation of a `WasiCtx` will no longer unconditionally acquire randomness from\n  the OS, instead using the `rand::thread_rng()` function in Rust which is only\n  periodically reseeded with randomness from the OS.\n  [#5244](https://github.com/bytecodealliance/wasmtime/pull/5244)\n\n* Codegen of dynamically-bounds-checked wasm memory accesses has been improved.\n  [#5190](https://github.com/bytecodealliance/wasmtime/pull/5190)\n\n* Wasmtime will now emit inline stack probes in generated functions for x86\\_64,\n  aarch64, and riscv64 architectures. This guarantees a process abort if an\n  engine was misconfigured to give wasm too much stack instead of optionally\n  allowing wasm to skip the guard page.\n  [#5350](https://github.com/bytecodealliance/wasmtime/pull/5350)\n  [#5353](https://github.com/bytecodealliance/wasmtime/pull/5353)\n\n### Fixed\n\n* Dropping a `Module` will now release kernel resources in-use by the pooling\n  allocator when enabled instead of waiting for a new instance to be\n  re-instantiated into prior slots.\n  [#5321](https://github.com/bytecodealliance/wasmtime/pull/5321)\n\n--------------------------------------------------------------------------------\n\n## 3.0.1\n\nReleased 2022-12-01.\n\n### Fixed\n\n* The instruction cache is now flushed for AArch64 Android.\n  [#5331](https://github.com/bytecodealliance/wasmtime/pull/5331)\n\n* Building for FreeBSD and Android has been fixed.\n  [#5323](https://github.com/bytecodealliance/wasmtime/pull/5323)\n\n--------------------------------------------------------------------------------\n\n## 3.0.0\n\nReleased 2022-11-21\n\n### Added\n\n* New `WasiCtx::{push_file, push_dir}` methods exist for embedders to add their\n  own objects.\n  [#5027](https://github.com/bytecodealliance/wasmtime/pull/5027)\n\n* Wasmtime's `component-model` support now supports `async` host functions and\n  embedding in the same manner as core wasm.\n  [#5055](https://github.com/bytecodealliance/wasmtime/pull/5055)\n\n* The `wasmtime` CLI executable now supports a `--max-wasm-stack` flag.\n  [#5156](https://github.com/bytecodealliance/wasmtime/pull/5156)\n\n* AOT compilation support has been implemented for components (aka the\n  `component-model` feature of the Wasmtime crate).\n  [#5160](https://github.com/bytecodealliance/wasmtime/pull/5160)\n\n* A new `wasi_config_set_stdin_bytes` function is available in the C API to set\n  the stdin of a WASI-using module from an in-memory slice.\n  [#5179](https://github.com/bytecodealliance/wasmtime/pull/5179)\n\n* When using the pooling allocator there are now options to reset memory with\n  `memset` instead of `madvisev` on Linux to keep pages resident in memory to\n  reduce page faults when reusing linear memory slots.\n  [#5207](https://github.com/bytecodealliance/wasmtime/pull/5207)\n\n### Changed\n\n* Consuming 0 fuel with 0 fuel left is now considered to succeed. Additionally a\n  store may not consume its last unit of fuel.\n  [#5013](https://github.com/bytecodealliance/wasmtime/pull/5013)\n\n* A number of variants in the `wasi_common::ErrorKind` enum have been removed.\n  [#5015](https://github.com/bytecodealliance/wasmtime/pull/5015)\n\n* Methods on `WasiDir` now error-by-default instead of requiring a definition by\n  default.\n  [#5019](https://github.com/bytecodealliance/wasmtime/pull/5019)\n\n* Bindings generated by the `wiggle` crate now always depend on the `wasmtime`\n  crate meaning crates like `wasi-common` no longer compile for platforms such\n  as `wasm32-unknown-emscripten`.\n  [#5137](https://github.com/bytecodealliance/wasmtime/pull/5137)\n\n* Error handling in the `wasmtime` crate's API has been changed to primarily\n  work with `anyhow::Error` for custom errors. The `Trap` type has been replaced\n  with a simple `enum Trap { ... }` and backtrace information is now stored as a\n  `WasmBacktrace` type inserted as context into an `anyhow::Error`.\n  Host-functions are expected to return `anyhow::Result<T>` instead of the prior\n  `Trap` error return from before. Additionally the old `Trap::i32_exit`\n  constructor is now a concrete `wasi_commont::I32Exit` type which can be tested\n  for with a `downcast_ref` on the error returned from Wasmtime.\n  [#5149](https://github.com/bytecodealliance/wasmtime/pull/5149)\n\n* Configuration of the pooling allocator is now done through a builder-style\n  `PoolingAllocationConfig` API instead of the prior enum-variant API.\n  [#5205](https://github.com/bytecodealliance/wasmtime/pull/5205)\n\n### Fixed\n\n* The instruction cache is now properly flushed for AArch64 on Windows.\n  [#4997](https://github.com/bytecodealliance/wasmtime/pull/4997)\n\n* Backtrace capturing with many sequences of wasm->host calls on the stack no\n  longer exhibit quadratic capturing behavior.\n  [#5049](https://github.com/bytecodealliance/wasmtime/pull/5049)\n\n--------------------------------------------------------------------------------\n\n## 2.0.2\n\nReleased 2022-11-10.\n\n### Fixed\n\n* [CVE-2022-39392] - modules may perform out-of-bounds reads/writes when the\n  pooling allocator was configured with `memory_pages: 0`.\n\n* [CVE-2022-39393] - data can be leaked between instances when using the pooling\n  allocator.\n\n* [CVE-2022-39394] - An incorrect Rust signature for the C API\n  `wasmtime_trap_code` function could lead to an out-of-bounds write of three\n  zero bytes.\n\n[CVE-2022-39392]: https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-44mr-8vmm-wjhg\n[CVE-2022-39393]: https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-wh6w-3828-g9qf\n[CVE-2022-39394]: https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-h84q-m8rr-3v9q\n\n--------------------------------------------------------------------------------\n\n## 2.0.1\n\nReleased 2022-10-27.\n\n### Fixed\n\n* A compilation error when building only the `wasmtime` crate on Windows with\n  only the default features enabled has been fixed.\n  [#5134](https://github.com/bytecodealliance/wasmtime/pull/5134)\n\n### Changed\n\n* The `rayon` dependency added to `cranelift-isle` in 2.0.0 has been removed to\n  improve the compile time of the `cranelift-codegen` crate.\n  [#5101](https://github.com/bytecodealliance/wasmtime/pull/5101)\n\n--------------------------------------------------------------------------------\n\n## 2.0.0\n\nReleased 2022-10-20\n\n### Added\n\n* Cranelift has gained support for forward-edge CFI on the AArch64 backend.\n  [#3693](https://github.com/bytecodealliance/wasmtime/pull/3693)\n\n* A `--disable-parallel-compilation` CLI flag is now implemented for `wasmtime`.\n  [#4911](https://github.com/bytecodealliance/wasmtime/pull/4911)\n\n* [Tier 3] support has been added for for RISC-V 64 with a new backend in\n  Cranelift for this architecture.\n  [#4271](https://github.com/bytecodealliance/wasmtime/pull/4271)\n\n* Basic [tier 3] support for Windows ARM64 has been added but features such as\n  traps don't work at this time.\n  [#4990](https://github.com/bytecodealliance/wasmtime/pull/4990)\n\n### Changed\n\n* The implementation of the `random_get` function in `wasi-common` is now faster\n  by using a userspace CSPRNG rather than the OS for randomness.\n  [#4917](https://github.com/bytecodealliance/wasmtime/pull/4917)\n\n* The AArch64 backend has completed its transition to ISLE.\n  [#4851](https://github.com/bytecodealliance/wasmtime/pull/4851)\n  [#4866](https://github.com/bytecodealliance/wasmtime/pull/4866)\n  [#4898](https://github.com/bytecodealliance/wasmtime/pull/4898)\n  [#4884](https://github.com/bytecodealliance/wasmtime/pull/4884)\n  [#4820](https://github.com/bytecodealliance/wasmtime/pull/4820)\n  [#4913](https://github.com/bytecodealliance/wasmtime/pull/4913)\n  [#4942](https://github.com/bytecodealliance/wasmtime/pull/4942)\n  [#4943](https://github.com/bytecodealliance/wasmtime/pull/4943)\n\n* The size of the `sigaltstack` allocated per-thread for signal handling has\n  been increased from 16k to 64k.\n  [#4964](https://github.com/bytecodealliance/wasmtime/pull/4964)\n\n\n[Tier 3]: https://docs.wasmtime.dev/stability-tiers.html\n\n--------------------------------------------------------------------------------\n\n## 1.0.2\n\nReleased 2022-11-10.\n\n### Fixed\n\n* [CVE-2022-39392] - modules may perform out-of-bounds reads/writes when the\n  pooling allocator was configured with `memory_pages: 0`.\n\n* [CVE-2022-39393] - data can be leaked between instances when using the pooling\n  allocator.\n\n* [CVE-2022-39394] - An incorrect Rust signature for the C API\n  `wasmtime_trap_code` function could lead to an out-of-bounds write of three\n  zero bytes.\n\n--------------------------------------------------------------------------------\n\n## 1.0.1\n\nReleased 2022-09-26\n\nThis is a patch release that incorporates a fix for a miscompilation of an\natomic-CAS operator on aarch64. The instruction is not usable from Wasmtime\nwith default settings, but may be used if the Wasm atomics extension is\nenabled. The bug may also be reachable via other uses of Cranelift. Thanks to\n@bjorn3 for reporting and debugging this issue!\n\n### Fixed\n\n* Fixed a miscompilation of `atomic_cas` on aarch64. The output register was\n  swapped with a temporary register in the register-allocator constraints.\n  [#4959](https://github.com/bytecodealliance/wasmtime/pull/4959)\n  [#4960](https://github.com/bytecodealliance/wasmtime/pull/4960)\n\n--------------------------------------------------------------------------------\n\n## 1.0.0\n\nReleased 2022-09-20\n\nThis release marks the official 1.0 release of Wasmtime and represents the\nculmination of the work amongst over 300 contributors. Wasmtime has been\nbattle-tested in production through multiple embeddings for quite some time now\nand we're confident in releasing a 1.0 version to signify the stability and\nquality of the Wasmtime engine.\n\nMore information about Wasmtime's 1.0 release is on the [Bytecode Alliance's\nblog][ba-blog] with separate posts on [Wasmtime's performance\nfeatures][ba-perf], [Wasmtime's security story][ba-security], and [the 1.0\nrelease announcement][ba-1.0].\n\nAs a reminder the 2.0 release of Wasmtime is scheduled for one month from now on\nOctober 20th. For more information see the [RFC on Wasmtime's 1.0\nrelease][rfc-1.0].\n\n[ba-blog]: https://bytecodealliance.org/articles/\n[ba-perf]: https://bytecodealliance.org/articles/wasmtime-10-performance\n[ba-security]: https://bytecodealliance.org/articles/security-and-correctness-in-wasmtime\n[ba-1.0]: https://bytecodealliance.org/articles/wasmtime-1-0-fast-safe-and-now-production-ready.md\n[rfc-1.0]: https://github.com/bytecodealliance/rfcs/blob/main/accepted/wasmtime-one-dot-oh.md\n\n### Added\n\n* An incremental compilation cache for Cranelift has been added which can be\n  enabled with `Config::enable_incremental_compilation`, and this option is\n  disabled by default for now. The incremental compilation cache has been\n  measured to improve compile times for cold uncached modules as well due to\n  some wasm modules having similar-enough functions internally.\n  [#4551](https://github.com/bytecodealliance/wasmtime/pull/4551)\n\n* Source tarballs are now available as part of Wasmtime's release artifacts.\n  [#4294](https://github.com/bytecodealliance/wasmtime/pull/4294)\n\n* WASI APIs that specify the REALTIME clock are now supported.\n  [#4777](https://github.com/bytecodealliance/wasmtime/pull/4777)\n\n* WASI's socket functions are now fully implemented.\n  [#4776](https://github.com/bytecodealliance/wasmtime/pull/4776)\n\n* The native call stack for async-executed wasm functions are no longer\n  automatically reset to zero after the stack is returned to the pool when using\n  the pooling allocator. A `Config::async_stack_zeroing` option has been added\n  to restore the old behavior of zero-on-return-to-pool.\n  [#4813](https://github.com/bytecodealliance/wasmtime/pull/4813)\n\n* Inline stack probing has been implemented for the Cranelift x64 backend.\n  [#4747](https://github.com/bytecodealliance/wasmtime/pull/4747)\n\n### Changed\n\n* Generating of native unwind information has moved from a\n  `Config::wasm_backtrace` option to a new `Config::native_unwind_info` option\n  and is enabled by default.\n  [#4643](https://github.com/bytecodealliance/wasmtime/pull/4643)\n\n* The `memory-init-cow` feature is now enabled by default in the C API.\n  [#4690](https://github.com/bytecodealliance/wasmtime/pull/4690)\n\n* Back-edge CFI is now enabled by default on AArch64 macOS.\n  [#4720](https://github.com/bytecodealliance/wasmtime/pull/4720)\n\n* WASI calls will no longer return NOTCAPABLE in preparation for the removal of\n  the rights system from WASI.\n  [#4666](https://github.com/bytecodealliance/wasmtime/pull/4666)\n\n### Internal\n\nThis section of the release notes shouldn't affect external users since no\npublic-facing APIs are affected, but serves as a place to document larger\nchanges internally within Wasmtime.\n\n* Differential fuzzing has been refactored and improved into one fuzzing target\n  which can execute against any of Wasmtime itself (configured differently),\n  wasmi, V8, or the spec interpreter. Fuzzing now executes each exported\n  function with fuzz-generated inputs and the contents of all of memory and each\n  exported global is compared after each execution. Additionally more\n  interesting shapes of modules are also possible to generate.\n  [#4515](https://github.com/bytecodealliance/wasmtime/pull/4515)\n  [#4735](https://github.com/bytecodealliance/wasmtime/pull/4735)\n  [#4737](https://github.com/bytecodealliance/wasmtime/pull/4737)\n  [#4739](https://github.com/bytecodealliance/wasmtime/pull/4739)\n  [#4774](https://github.com/bytecodealliance/wasmtime/pull/4774)\n  [#4773](https://github.com/bytecodealliance/wasmtime/pull/4773)\n  [#4845](https://github.com/bytecodealliance/wasmtime/pull/4845)\n  [#4672](https://github.com/bytecodealliance/wasmtime/pull/4672)\n  [#4674](https://github.com/bytecodealliance/wasmtime/pull/4674)\n\n* The x64 backend for Cranelift has been fully migrated to ISLE.\n  [#4619](https://github.com/bytecodealliance/wasmtime/pull/4619)\n  [#4625](https://github.com/bytecodealliance/wasmtime/pull/4625)\n  [#4645](https://github.com/bytecodealliance/wasmtime/pull/4645)\n  [#4650](https://github.com/bytecodealliance/wasmtime/pull/4650)\n  [#4684](https://github.com/bytecodealliance/wasmtime/pull/4684)\n  [#4704](https://github.com/bytecodealliance/wasmtime/pull/4704)\n  [#4718](https://github.com/bytecodealliance/wasmtime/pull/4718)\n  [#4726](https://github.com/bytecodealliance/wasmtime/pull/4726)\n  [#4722](https://github.com/bytecodealliance/wasmtime/pull/4722)\n  [#4729](https://github.com/bytecodealliance/wasmtime/pull/4729)\n  [#4730](https://github.com/bytecodealliance/wasmtime/pull/4730)\n  [#4741](https://github.com/bytecodealliance/wasmtime/pull/4741)\n  [#4763](https://github.com/bytecodealliance/wasmtime/pull/4763)\n  [#4772](https://github.com/bytecodealliance/wasmtime/pull/4772)\n  [#4780](https://github.com/bytecodealliance/wasmtime/pull/4780)\n  [#4787](https://github.com/bytecodealliance/wasmtime/pull/4787)\n  [#4793](https://github.com/bytecodealliance/wasmtime/pull/4793)\n  [#4809](https://github.com/bytecodealliance/wasmtime/pull/4809)\n\n* The AArch64 backend for Cranelift has seen significant progress in being\n  ported to ISLE.\n  [#4608](https://github.com/bytecodealliance/wasmtime/pull/4608)\n  [#4639](https://github.com/bytecodealliance/wasmtime/pull/4639)\n  [#4634](https://github.com/bytecodealliance/wasmtime/pull/4634)\n  [#4748](https://github.com/bytecodealliance/wasmtime/pull/4748)\n  [#4750](https://github.com/bytecodealliance/wasmtime/pull/4750)\n  [#4751](https://github.com/bytecodealliance/wasmtime/pull/4751)\n  [#4753](https://github.com/bytecodealliance/wasmtime/pull/4753)\n  [#4788](https://github.com/bytecodealliance/wasmtime/pull/4788)\n  [#4796](https://github.com/bytecodealliance/wasmtime/pull/4796)\n  [#4785](https://github.com/bytecodealliance/wasmtime/pull/4785)\n  [#4819](https://github.com/bytecodealliance/wasmtime/pull/4819)\n  [#4821](https://github.com/bytecodealliance/wasmtime/pull/4821)\n  [#4832](https://github.com/bytecodealliance/wasmtime/pull/4832)\n\n* The s390x backend has seen improvements and additions to fully support the\n  Cranelift backend for rustc.\n  [#4682](https://github.com/bytecodealliance/wasmtime/pull/4682)\n  [#4702](https://github.com/bytecodealliance/wasmtime/pull/4702)\n  [#4616](https://github.com/bytecodealliance/wasmtime/pull/4616)\n  [#4680](https://github.com/bytecodealliance/wasmtime/pull/4680)\n\n* Significant improvements have been made to Cranelift-based fuzzing with more\n  supported features and more instructions being fuzzed.\n  [#4589](https://github.com/bytecodealliance/wasmtime/pull/4589)\n  [#4591](https://github.com/bytecodealliance/wasmtime/pull/4591)\n  [#4665](https://github.com/bytecodealliance/wasmtime/pull/4665)\n  [#4670](https://github.com/bytecodealliance/wasmtime/pull/4670)\n  [#4590](https://github.com/bytecodealliance/wasmtime/pull/4590)\n  [#4375](https://github.com/bytecodealliance/wasmtime/pull/4375)\n  [#4519](https://github.com/bytecodealliance/wasmtime/pull/4519)\n  [#4696](https://github.com/bytecodealliance/wasmtime/pull/4696)\n  [#4700](https://github.com/bytecodealliance/wasmtime/pull/4700)\n  [#4703](https://github.com/bytecodealliance/wasmtime/pull/4703)\n  [#4602](https://github.com/bytecodealliance/wasmtime/pull/4602)\n  [#4713](https://github.com/bytecodealliance/wasmtime/pull/4713)\n  [#4738](https://github.com/bytecodealliance/wasmtime/pull/4738)\n  [#4667](https://github.com/bytecodealliance/wasmtime/pull/4667)\n  [#4782](https://github.com/bytecodealliance/wasmtime/pull/4782)\n  [#4783](https://github.com/bytecodealliance/wasmtime/pull/4783)\n  [#4800](https://github.com/bytecodealliance/wasmtime/pull/4800)\n\n* Optimization work on cranelift has continued across various dimensions for\n  some modest compile-time improvements.\n  [#4621](https://github.com/bytecodealliance/wasmtime/pull/4621)\n  [#4701](https://github.com/bytecodealliance/wasmtime/pull/4701)\n  [#4697](https://github.com/bytecodealliance/wasmtime/pull/4697)\n  [#4711](https://github.com/bytecodealliance/wasmtime/pull/4711)\n  [#4710](https://github.com/bytecodealliance/wasmtime/pull/4710)\n  [#4829](https://github.com/bytecodealliance/wasmtime/pull/4829)\n\n--------------------------------------------------------------------------------\n\n## 0.40.0\n\nReleased 2022-08-20\n\nThis was a relatively quiet release in terms of user-facing features where most\nof the work was around the internals of Wasmtime and Cranelift. Improvements\ninternally have been made along the lines of:\n\n* Many more instructions are now implemented with ISLE instead of handwritten\n  lowerings.\n* Many improvements to the cranelift-based fuzzing.\n* Many platform improvements for s390x including full SIMD support, running\n  `rustc_codegen_cranelift` with features like `i128`, supporting more\n  ABIs, etc.\n* Much more of the component model has been implemented and is now fuzzed.\n\nFinally this release is currently scheduled to be the last `0.*` release of\nWasmtime. The upcoming release of Wasmtime on September 20 is planned to be\nWasmtime's 1.0 release. More information about what 1.0 means for Wasmtime is\navailable in the [1.0 RFC]\n\n[1.0 RFC]: https://github.com/bytecodealliance/rfcs/blob/main/accepted/wasmtime-one-dot-oh.md\n\n### Added\n\n* Stack walking has been reimplemented with frame pointers rather than with\n  native unwind information. This means that backtraces are feasible to capture\n  in performance-critical environments and in general stack walking is much\n  faster than before.\n  [#4431](https://github.com/bytecodealliance/wasmtime/pull/4431)\n\n* The WebAssembly `simd` proposal is now fully implemented for the s390x\n  backend.\n  [#4427](https://github.com/bytecodealliance/wasmtime/pull/4427)\n\n* Support for AArch64 has been added in the experimental native debuginfo\n  support that Wasmtime has.\n  [#4468](https://github.com/bytecodealliance/wasmtime/pull/4468)\n\n* Support building the C API of Wasmtime with CMake has been added.\n  [#4369](https://github.com/bytecodealliance/wasmtime/pull/4369)\n\n* Clarification was added to Wasmtime's documentation about \"tiers of support\"\n  for various features.\n  [#4479](https://github.com/bytecodealliance/wasmtime/pull/4479)\n\n### Fixed\n\n* Support for `filestat_get` has been improved for stdio streams in WASI.\n  [#4531](https://github.com/bytecodealliance/wasmtime/pull/4531)\n\n* Enabling the `vtune` feature no longer breaks builds on AArch64.\n  [#4533](https://github.com/bytecodealliance/wasmtime/pull/4533)\n\n--------------------------------------------------------------------------------\n\n## 0.39.1\n\nReleased 2022-07-20.\n\n### Fixed\n\n* An s390x-specific codegen bug in addition to a mistake introduced in the fix\n  of CVE-2022-31146 were fixed.\n  [#4490](https://github.com/bytecodealliance/wasmtime/pull/4490)\n\n--------------------------------------------------------------------------------\n\n## 0.39.0\n\nReleased 2022-07-20\n\n### Added\n\n* Initial support for shared memories and the `threads` WebAssembly proposal\n  has been added. Note that this feature is still experimental and not ready\n  for production use yet.\n  [#4187](https://github.com/bytecodealliance/wasmtime/pull/4187)\n\n* A new `Linker::define_unknown_imports_as_traps` method and\n  `--trap-unknown-imports` CLI flag have been added to conveniently support\n  running modules with imports that aren't dynamically called at runtime.\n  [#4312](https://github.com/bytecodealliance/wasmtime/pull/4312)\n\n* The VTune profiling strategy can now be selected through the C API.\n  [#4316](https://github.com/bytecodealliance/wasmtime/pull/4316)\n\n### Changed\n\n* Some methods on the `Config` structure now return `&mut Self` instead of\n  `Result<&mut Self>` since the validation is deferred until `Engine::new`:\n  `profiler`, `cranelift_flag_enable`, `cranelift_flag_set`, `max_wasm_stack`,\n  `async_stack_size`, and `strategy`.\n  [#4252](https://github.com/bytecodealliance/wasmtime/pull/4252)\n  [#4262](https://github.com/bytecodealliance/wasmtime/pull/4262)\n\n* Parallel compilation of WebAssembly modules is now enabled in the C API by\n  default.\n  [#4270](https://github.com/bytecodealliance/wasmtime/pull/4270)\n\n* Implicit Cargo features of the `wasmtime` introduced through `optional`\n  dependencies may have been removed since namespaced features are now used.\n  It's recommended to only used the set of named `[features]` for Wasmtime.\n  [#4293](https://github.com/bytecodealliance/wasmtime/pull/4293)\n\n* Register allocation has fixed a few issues related to excessive memory usage\n  at compile time.\n  [#4324](https://github.com/bytecodealliance/wasmtime/pull/4324)\n\n### Fixed\n\n* A refactor of `Config` was made to fix an issue that the order of calls to `Config`\n  matters now, which may lead to unexpected behavior.\n  [#4252](https://github.com/bytecodealliance/wasmtime/pull/4252)\n  [#4262](https://github.com/bytecodealliance/wasmtime/pull/4262)\n\n* Wasmtime has been fixed to work on SSE2-only x86\\_64 platforms when the\n  `simd` feature is disabled in `Config`.\n  [#4231](https://github.com/bytecodealliance/wasmtime/pull/4231)\n\n* Generation of platform-specific unwinding information is disabled if\n  `wasm_backtrace` and `wasm_reference_types` are both disabled.\n  [#4351](https://github.com/bytecodealliance/wasmtime/pull/4351)\n\n--------------------------------------------------------------------------------\n\n## 0.38.3\n\nReleased 2022-07-20.\n\n### Fixed.\n\n* An s390x-specific codegen bug in addition to a mistake introduced in the fix\n  of CVE-2022-31146 were fixed.\n  [#4491](https://github.com/bytecodealliance/wasmtime/pull/4491)\n\n--------------------------------------------------------------------------------\n\n## 0.38.2\n\nReleased 2022-07-20.\n\n### Fixed.\n\n* A miscompilation when handling constant divisors on AArch64 has been fixed.\n  [CVE-2022-31169](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-7f6x-jwh5-m9r4)\n\n* A use-after-free possible with accidentally missing stack maps has been fixed.\n  [CVE-2022-31146](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-5fhj-g3p3-pq9g)\n\n--------------------------------------------------------------------------------\n\n## 0.38.1\n\nReleased 2022-06-27.\n\n### Fixed.\n\n* A register allocator bug was fixed that could affect direct users of\n  Cranelift who use struct-return (`sret`) arguments. The bug had to do with\n  the handling of physical register constraints in the function prologue. No\n  impact should be possible for users of Cranelift via the Wasm frontend,\n  including Wasmtime.\n  [regalloc2#60](https://github.com/bytecodealliance/regalloc2/pull/60)\n  [#4333](https://github.com/bytecodealliance/wasmtime/pull/4333)\n\n* Lowering bugs for the `i8x16.swizzle` and `select`-with-`v128`-inputs\n  instructions were fixed for the x86\\_64 code generator. Note that aarch64 and\n  s390x are unaffected.\n  [#4334](https://github.com/bytecodealliance/wasmtime/pull/4334)\n\n* A bug in the 8-bit lowering of integer division on x86-64 was fixed in\n  Cranelift that could cause a register allocator panic due to an undefined\n  value in a register. (The divide instruction does not take a register `rdx`\n  as a source when 8 bits but the metadata incorrectly claimed it did.) No\n  impact on Wasm/Wasmtime users, and impact on direct Cranelift embedders\n  limited to compilation panics.\n  [#4332](https://github.com/bytecodealliance/wasmtime/pull/4332)\n\n--------------------------------------------------------------------------------\n\n## 0.38.0\n\nReleased 2022-06-21\n\n### Added\n\n* Enabling or disabling NaN canonicalization in generated code is now exposed\n  through the C API.\n  [#4154](https://github.com/bytecodealliance/wasmtime/pull/4154)\n\n* A user-defined callback can now be invoked when an epoch interruption happens\n  via the `Store::epoch_deadline_callback` API.\n  [#4152](https://github.com/bytecodealliance/wasmtime/pull/4152)\n\n* Basic alias analysis with redundant-load elimintation and store-to-load\n  forwarding optimizations has been added to Cranelift.\n  [#4163](https://github.com/bytecodealliance/wasmtime/pull/4163)\n\n### Changed\n\n* Traps originating from epoch-based interruption are now exposed as\n  `TrapCode::Interrupt`.\n  [#4105](https://github.com/bytecodealliance/wasmtime/pull/4105)\n\n* Binary builds for AArch64 now require glibc 2.17 and for s390x require glibc\n  2.16. Previously glibc 2.28 was required.\n  [#4171](https://github.com/bytecodealliance/wasmtime/pull/4171)\n\n* The `wasmtime::ValRaw` now has all of its fields listed as private and instead\n  constructors/accessors are provided for getting at the internal data.\n  [#4186](https://github.com/bytecodealliance/wasmtime/pull/4186)\n\n* The `wasm-backtrace` Cargo feature has been removed in favor of a\n  `Config::wasm_backtrace` runtime configuration option. Additionally backtraces\n  are now only captured when an embedder-generated trap actually reaches a\n  WebAssembly call stack.\n  [#4183](https://github.com/bytecodealliance/wasmtime/pull/4183)\n\n* Usage of `*_unchecked` APIs for `Func` in the `wasmtime` crate and C API now\n  take a `usize` parameter indicating the number of `ValRaw` values behind\n  the associated pointer.\n  [#4192](https://github.com/bytecodealliance/wasmtime/pull/4192)\n\n### Fixed\n\n* An improvement was made to the spill-slot allocation in code generation to fix\n  an issue where some stack slots accidentally weren't reused. This issue was\n  introduced with the landing of regalloc2 in 0.37.0 and may have resulted in\n  larger-than-intended increases in stack frame sizes.\n  [#4222](https://github.com/bytecodealliance/wasmtime/pull/4222)\n\n--------------------------------------------------------------------------------\n\n## 0.37.0\n\nReleased 2022-05-20\n\n### Added\n\n* Updated Cranelift to use regalloc2, a new register allocator. This should\n  result in ~20% faster compile times, and for programs that suffered from\n  register-allocation pressure before, up to ~20% faster generated code.\n  [#3989](https://github.com/bytecodealliance/wasmtime/pull/3989)\n\n* Pre-built binaries for macOS M1 machines are now available as release\n  artifacts.\n  [#3983](https://github.com/bytecodealliance/wasmtime/pull/3983)\n\n* Copy-on-write images of memory can now be manually initialized for a `Module`\n  with an explicit method call, but it is still not required to call this method\n  and will automatically otherwise happen on the first instantiation.\n  [#3964](https://github.com/bytecodealliance/wasmtime/pull/3964)\n\n### Fixed\n\n* Using `InstancePre::instantiate` or `Linker::instantiate` will now panic as\n  intended when used with an async-configured `Store`.\n  [#3972](https://github.com/bytecodealliance/wasmtime/pull/3972)\n\n### Changed\n\n* The unsafe `ValRaw` type in the `wasmtime` crate now always stores its values\n  in little-endian format instead of the prior native-endian format. Users of\n  `ValRaw` are recommended to audit their existing code for usage to continue\n  working on big-endian platforms.\n  [#4035](https://github.com/bytecodealliance/wasmtime/pull/4035)\n\n### Removed\n\n* Support for `Config::paged_memory_initialization` and the `uffd` crate feature\n  have been removed from the `wasmtime` crate. Users should migrate to using\n  `Config::memory_init_cow` which is more portable and faster at this point.\n  [#4040](https://github.com/bytecodealliance/wasmtime/pull/4040)\n\n--------------------------------------------------------------------------------\n\n## 0.36.0\n\nReleased 2022-04-20\n\n### Added\n\n* Support for epoch-based interruption has been added to the C API.\n  [#3925](https://github.com/bytecodealliance/wasmtime/pull/3925)\n\n* Support for disabling libunwind-based backtraces of WebAssembly code at\n  compile time has been added.\n  [#3932](https://github.com/bytecodealliance/wasmtime/pull/3932)\n\n* Async support for call hooks has been added to optionally execute \"blocking\"\n  work whenever a wasm module is entered or exited relative to the host.\n  [#3876](https://github.com/bytecodealliance/wasmtime/pull/3876)\n\n### Fixed\n\n* Loading a `Module` will now check, at runtime, that the compilation settings\n  enabled in a `Config` are compatible with the native host. For example this\n  ensures that if avx2 is enabled that the host actually has avx2 support.\n  [#3899](https://github.com/bytecodealliance/wasmtime/pull/3899)\n\n### Removed\n\n* Support for `Config::interruptable` and `InterruptHandle` has been removed\n  from the `wasmtime` crate. Users should migrate to using epoch-based\n  interruption instead.\n  [#3925](https://github.com/bytecodealliance/wasmtime/pull/3925)\n\n* The module linking implementation of Wasmtime has been removed to make room\n  for the upcoming support for the component model.\n  [#3958](https://github.com/bytecodealliance/wasmtime/pull/3958)\n\n--------------------------------------------------------------------------------\n\n## 0.35.3\n\nReleased 2022-04-11.\n\n### Fixed\n\n* Backported a bugfix for an instruction lowering issue that could cause a\n  regalloc panic due to an undefined register in some cases. No miscompilation\n  was ever possible, but panics would result in a compilation failure.\n  [#4012](https://github.com/bytecodealliance/wasmtime/pull/4012)\n\n--------------------------------------------------------------------------------\n\n## 0.35.2\n\nReleased 2022-03-31.\n\n### Security Fixes\n\n* [CVE-2022-24791](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-gwc9-348x-qwv2):\n  Fixed a use after free with `externref`s and epoch interruption.\n\n## 0.35.1\n\nReleased 2022-03-09.\n\n### Fixed\n\n* Fixed a bug in the x86-64 lowering of the `uextend` opcode for narrow (`i8`,\n  `i16`) integer sources when the value is produced by one of several\n  arithmetic instructions.\n  [#3906](https://github.com/bytecodealliance/wasmtime/pull/3906)\n\n## 0.35.0\n\nReleased 2022-03-07.\n\n### Added\n\n* The `wasmtime_wasi::add_to_linker` function now allows providing\n  a context object of a custom type instead of `wasmtime_wasi::WasiCtx`,\n  as long as that type implements the required WASI snapshot traits.\n  This allows, for example, wrapping `WasiCtx` into a struct and providing\n  custom implementations for those traits to override the default behaviour.\n\n### Changed\n\n* WebAssembly tables of `funcref` values are now lazily initialized which can,\n  in some cases, greatly speed up instantiation of a module.\n  [#3733](https://github.com/bytecodealliance/wasmtime/pull/3733)\n\n* The `memfd` feature in 0.34.0, now renamed to `memory-init-cow`, has been\n  enabled by default. This means that, where applicable, WebAssembly linear\n  memories are now initialized with copy-on-write mappings. Support from this\n  has been expanded from Linux-only to include macOS and other Unix systems when\n  modules are loaded from precompiled `*.cwasm` files on disk.\n  [#3777](https://github.com/bytecodealliance/wasmtime/pull/3777)\n  [#3778](https://github.com/bytecodealliance/wasmtime/pull/3778)\n  [#3787](https://github.com/bytecodealliance/wasmtime/pull/3787)\n  [#3819](https://github.com/bytecodealliance/wasmtime/pull/3819)\n  [#3831](https://github.com/bytecodealliance/wasmtime/pull/3831)\n\n* Clarify that SSE 4.2 (and prior) is required for running WebAssembly code with\n  simd support enabled on x86\\_64.\n  [#3816](https://github.com/bytecodealliance/wasmtime/pull/3816)\n  [#3817](https://github.com/bytecodealliance/wasmtime/pull/3817)\n  [#3833](https://github.com/bytecodealliance/wasmtime/pull/3833)\n  [#3825](https://github.com/bytecodealliance/wasmtime/pull/3825)\n\n* Support for profiling with VTune is now enabled at compile time by default,\n  but it remains disabled at runtime by default.\n  [#3821](https://github.com/bytecodealliance/wasmtime/pull/3821)\n\n* The `ModuleLimits` type has been removed from the configuration of the pooling\n  allocator in favor of configuring the total size of an instance allocation\n  rather than each individual field.\n  [#3837](https://github.com/bytecodealliance/wasmtime/pull/3837)\n\n* The native stack size allowed for WebAssembly has been decreased from 1 MiB to\n  512 KiB on all platforms to better accomodate running wasm on the main thread\n  on Windows.\n  [#3861](https://github.com/bytecodealliance/wasmtime/pull/3861)\n\n* The `wasi-common` crate now supports doing polls for both read and write\n  interest on a file descriptor at the same time.\n  [#3866](https://github.com/bytecodealliance/wasmtime/pull/3866)\n\n### Fixed\n\n* The `Store::call_hook` callback is now invoked when entering host functions\n  defined with `*_unchecked` variants.\n  [#3881](https://github.com/bytecodealliance/wasmtime/pull/3881)\n\n### Removed\n\n* The incomplete and unmaintained ARM32 backend has been removed from Cranelift.\n  [#3799](https://github.com/bytecodealliance/wasmtime/pull/3799)\n\n--------------------------------------------------------------------------------\n\n## 0.34.2\n\nReleased 2022-03-31.\n\n### Security Fixes\n\n* [CVE-2022-24791](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-gwc9-348x-qwv2):\n  Fixed a use after free with `externref`s and epoch interruption.\n\n## 0.34.1\n\nReleased 2022-02-16.\n\n### Security Fixes\n\n* [CVE-2022-23636](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-88xq-w8cq-xfg7):\n  Fixed an invalid drop of a partially-initialized instance in the pooling instance\n  allocator.\n\n## 0.33.1\n\nReleased 2022-02-16.\n\n### Security Fixes\n\n* [CVE-2022-23636](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-88xq-w8cq-xfg7):\n  Fixed an invalid drop of a partially-initialized instance in the pooling instance\n  allocator.\n\n## 0.34.0\n\nReleased 2022-02-07.\n\n### Fixed\n\n* The `wasi-common` default implementation of some attributes of files has been\n  updated to ensure that `wasi-libc`'s `isatty` function works as intended.\n  [#3696](https://github.com/bytecodealliance/wasmtime/pull/3696)\n\n* A benign debug assertion related to `externref` and garbage-collection has\n  been fixed.\n  [#3734](https://github.com/bytecodealliance/wasmtime/pull/3734)\n\n### Added\n\n* Function names are now automatically demangled when informing profilers of\n  regions of JIT code to apply Rust-specific demangling rules if applicable.\n  [#3683](https://github.com/bytecodealliance/wasmtime/pull/3683)\n\n* Support for profiling JIT-generated trampolines with VTune has been added.\n  [#3687](https://github.com/bytecodealliance/wasmtime/pull/3687)\n\n* Wasmtime now supports a new method of async preemption dubbed \"epoch-based\n  interruption\" which is intended to be much more efficient than the current\n  fuel-based method of preemption.\n  [#3699](https://github.com/bytecodealliance/wasmtime/pull/3699)\n\n* On Linux Wasmtime will now by default use copy-on-write mappings to initialize\n  memories of wasm modules where possible, accelerating instantiation by\n  avoiding costly memory copies. When combined with the pooling allocator this\n  can also be used to speed up instance-reuse cases due to fewer syscalls to\n  change memory mappings being necessary.\n  [#3697](https://github.com/bytecodealliance/wasmtime/pull/3697)\n  [#3738](https://github.com/bytecodealliance/wasmtime/pull/3738)\n  [#3760](https://github.com/bytecodealliance/wasmtime/pull/3760)\n\n* Wasmtime now supports the recently-added `sock_accept` WASI function.\n  [#3711](https://github.com/bytecodealliance/wasmtime/pull/3711)\n\n* Cranelift now has support for specifying blocks as cold.\n  [#3698](https://github.com/bytecodealliance/wasmtime/pull/3698)\n\n### Changed\n\n* Many more instructions for the x64 backend have been migrated to ISLE,\n  additionally with refactorings to make incorrect lowerings harder to\n  accidentally write.\n  [#3653](https://github.com/bytecodealliance/wasmtime/pull/3653)\n  [#3659](https://github.com/bytecodealliance/wasmtime/pull/3659)\n  [#3681](https://github.com/bytecodealliance/wasmtime/pull/3681)\n  [#3686](https://github.com/bytecodealliance/wasmtime/pull/3686)\n  [#3688](https://github.com/bytecodealliance/wasmtime/pull/3688)\n  [#3690](https://github.com/bytecodealliance/wasmtime/pull/3690)\n  [#3752](https://github.com/bytecodealliance/wasmtime/pull/3752)\n\n* More instructions in the aarch64 backend are now lowered with ISLE.\n  [#3658](https://github.com/bytecodealliance/wasmtime/pull/3658)\n  [#3662](https://github.com/bytecodealliance/wasmtime/pull/3662)\n\n* The s390x backend's lowering rules are now almost entirely defined with ISLE.\n  [#3702](https://github.com/bytecodealliance/wasmtime/pull/3702)\n  [#3703](https://github.com/bytecodealliance/wasmtime/pull/3703)\n  [#3706](https://github.com/bytecodealliance/wasmtime/pull/3706)\n  [#3717](https://github.com/bytecodealliance/wasmtime/pull/3717)\n  [#3723](https://github.com/bytecodealliance/wasmtime/pull/3723)\n  [#3724](https://github.com/bytecodealliance/wasmtime/pull/3724)\n\n* Instantiation of modules in Wasmtime has been further optimized now that the\n  copy-on-write memory initialization removed the previously most-expensive part\n  of instantiating a module.\n  [#3727](https://github.com/bytecodealliance/wasmtime/pull/3727)\n  [#3739](https://github.com/bytecodealliance/wasmtime/pull/3739)\n  [#3741](https://github.com/bytecodealliance/wasmtime/pull/3741)\n  [#3742](https://github.com/bytecodealliance/wasmtime/pull/3742)\n\n--------------------------------------------------------------------------------\n\n## 0.33.0\n\nReleased 2022-01-05.\n\n### Added\n\n* Compiled wasm modules may now optionally omit debugging information about\n  mapping addresses to source locations, resulting in smaller binaries.\n  [#3598](https://github.com/bytecodealliance/wasmtime/pull/3598)\n\n* The WebAssembly SIMD proposal is now enabled by default.\n  [#3601](https://github.com/bytecodealliance/wasmtime/pull/3601)\n\n--------------------------------------------------------------------------------\n\n## 0.32.1\n\nReleased 2022-01-04.\n\n### Fixed\n\n* Cranelift: remove recently-added build dependency on `sha2` to allow usage in\n  some dependency-sensitive environments, by computing ISLE manifest hashes\n  with a different hash function.\n  [#3619](https://github.com/bytecodealliance/wasmtime/pull/3619)\n\n* Cranelift: fixed 8- and 16-bit behavior of popcount (bit population count)\n  instruction. Does not affect Wasm frontend.\n  [#3617](https://github.com/bytecodealliance/wasmtime/pull/3617)\n\n* Cranelift: fixed miscompilation of 8- and 16-bit bit-rotate instructions.\n  Does not affect Wasm frontend.\n  [#3610](https://github.com/bytecodealliance/wasmtime/pull/3610)\n\n--------------------------------------------------------------------------------\n\n## 0.32.0\n\nReleased 2021-12-13.\n\n### Added\n\n* A new configuration option has been added to force using a \"static\" memory\n  style to automatically limit growth of memories in some configurations.\n  [#3503](https://github.com/bytecodealliance/wasmtime/pull/3503)\n\n* The `InstancePre<T>` type now implements `Clone`.\n  [#3510](https://github.com/bytecodealliance/wasmtime/pull/3510)\n\n* Cranelift's instruction selection process has begun to be migrated towards the\n  ISLE compiler and definition language.\n  [#3506](https://github.com/bytecodealliance/wasmtime/pull/3506)\n\n* A `pooling-allocator` feature has been added, which is on-by-default, to\n  disable the pooling allocator at compile time.\n  [#3514](https://github.com/bytecodealliance/wasmtime/pull/3514)\n\n### Fixed\n\n* A possible panic when parsing a WebAssembly `name` section has been fixed.\n  [#3509](https://github.com/bytecodealliance/wasmtime/pull/3509)\n\n* Generating native DWARF information for some C-produced modules has been\n  fixed, notably those where there may be DWARF about dead code.\n  [#3498](https://github.com/bytecodealliance/wasmtime/pull/3498)\n\n* A number of SIMD code generation bugs have been fixed in the x64 backend\n  by migrating their lowerings to ISLE.\n\n--------------------------------------------------------------------------------\n\n## 0.31.0\n\nReleased 2021-10-29.\n\n### Added\n\n* New `Func::new_unchecked` and `Func::call_unchecked` APIs have been added with\n  accompanying functions in the C API to improve the performance of calls into\n  wasm and the host in the C API.\n  [#3350](https://github.com/bytecodealliance/wasmtime/pull/3350)\n\n* Release binaries are now available for the s390x-unknown-linux-gnu\n  architecture.\n  [#3372](https://github.com/bytecodealliance/wasmtime/pull/3372)\n\n* A new `ResourceLimiterAsync` trait is added which allows asynchronous blocking\n  of WebAssembly on instructions such as `memory.grow`.\n  [#3393](https://github.com/bytecodealliance/wasmtime/pull/3393)\n\n### Changed\n\n* The `Func::call` method now takes a slice to write the results into rather\n  than returning a boxed slice.\n  [#3319](https://github.com/bytecodealliance/wasmtime/pull/3319)\n\n* Trampolines are now covered when jitdump profiling is enabled.\n  [#3344](https://github.com/bytecodealliance/wasmtime/pull/3344)\n\n### Fixed\n\n* Debugging with GDB has been fixed on Windows.\n  [#3373](https://github.com/bytecodealliance/wasmtime/pull/3373)\n\n* Some quadradic behavior in Wasmtime's compilation of modules has been fixed.\n  [#3469](https://github.com/bytecodealliance/wasmtime/pull/3469)\n  [#3466](https://github.com/bytecodealliance/wasmtime/pull/3466)\n\n* Bounds-checks for wasm memory accesses in certain non-default configurations\n  have been fixed to correctly allow loads at the end of the address space.\n  [#3462](https://github.com/bytecodealliance/wasmtime/pull/3462)\n\n* When type-checking memories and tables for satisfying instance imports the\n  runtime size of the table/memory is now consulted instead of the object's\n  original type.\n  [#3450](https://github.com/bytecodealliance/wasmtime/pull/3450)\n\n### Removed\n\n* The Lightbeam backend has been removed, as per [RFC 14].\n  [#3390](https://github.com/bytecodealliance/wasmtime/pull/3390)\n\n[RFC 14]: https://github.com/bytecodealliance/rfcs/pull/14\n\n* Cranelift's old x86 backend has been removed, as per [RFC 12].\n  [#3309](https://github.com/bytecodealliance/wasmtime/pull/3009)\n\n[RFC 12]: https://github.com/bytecodealliance/rfcs/pull/12\n\n## 0.30.0\n\nReleased 2021-09-17.\n\n### Security Fixes\n\n* [CVE-2021-39216](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-v4cp-h94r-m7xf):\n  Fixed a use after free passing `externref`s to Wasm in Wasmtime.\n\n* [CVE-2021-39218](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-4873-36h9-wv49):\n  Fixed an out-of-bounds read/write and invalid free with `externref`s and GC\n  safepoints in Wasmtime.\n\n* [CVE-2021-39219](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-q879-9g95-56mx):\n  Fixed a bug where using two different `Engine`s with the same `Linker`-define\n  functions caused unsafety without `unsafe` blocks.\n\n### Added\n\n* Added experimental support for the in-progress 64-bit memories Wasm proposal.\n\n* Added support to build Wasmtime without the compiler. This lets you run\n  pre-compiled Wasm modules, without the ability (or potential attack surface)\n  of compiling new Wasm modules. The compilation functionality is gated by the\n  on-by-default `cranelift` cargo feature.\n\n* Added support for NaN canonicalization with SIMD vectors.\n\n* Added support for differential fuzzing against V8's Wasm engine.\n\n* Added support for fuzzing against the Wasm spec interpreter.\n\n* Enabled SIMD fuzzing on oss-fuzz.\n\n### Changed\n\n* A variety of performance improvements to loading pre-compiled modules.\n\n* A variety of performance improvements to function calls, both through Rust and\n  the C API.\n\n* Leaf functions that do not use the stack no longer bump the frame pointer on\n  aarch64 and s390x.\n\n* Many updates and expanded instruction support to the in-progress CLIF\n  interpreter.\n\n* Expanded fuzzing of reference types and GC.\n\n### Fixed\n\n* A number of fixes to both aarch64 and x86_64 support for the Wasm SIMD\n  proposal and the underlying CLIF vector instructions.\n\n* Fixed a potential infinite loop in the SSA computation for\n  `cranelift-frontend`. This was not reachable from `cranelift-wasm` or\n  Wasmtime, but might have affected general Cranelift users.\n\n### Removed\n\n* The `wasmtime wasm2obj` subcommand has been removed. Generating raw object\n  files for linking natively is no longer supported. Use the `wasmtime compile`\n  subcommand to pre-compile a Wasm module and `wasmtime run` to run pre-compiled\n  Wasm modules.\n\n## 0.29.0\n\nReleased 2021-08-02.\n\n### Changed\n\n* Instance exports are now loaded lazily from instances instead of eagerly as\n  they were before. This is an internal-only change and is not a breaking\n  change.\n  [#2984](https://github.com/bytecodealliance/wasmtime/pull/2984)\n\n* All linear memories created by Wasmtime will now, by default, have guard pages\n  in front of them in addition to after them. This is intended to help mitigate\n  future bugs in Cranelift, should they arise.\n  [#2977](https://github.com/bytecodealliance/wasmtime/pull/2977)\n\n* Linear memories now correctly support a maximum size of 4GB. Previously, the\n  limit field was 32 bits, which did not properly support a full 4GB memory.\n  This update is also a necessary change in preparation for future memory64\n  support.\n  [#3013](https://github.com/bytecodealliance/wasmtime/pull/3013)\n  [#3134](https://github.com/bytecodealliance/wasmtime/pull/3134)\n\n* Injection counts of fuel into a `wasmtime::Store` now uses a u64 instead of a\n  u32.\n  [#3048](https://github.com/bytecodealliance/wasmtime/pull/3048)\n\n### Added\n\n* Support for `i128` has improved in the AArch64 backend.\n  [#2959](https://github.com/bytecodealliance/wasmtime/pull/2959)\n  [#2975](https://github.com/bytecodealliance/wasmtime/pull/2975)\n  [#2985](https://github.com/bytecodealliance/wasmtime/pull/2985)\n  [#2990](https://github.com/bytecodealliance/wasmtime/pull/2990)\n  [#3002](https://github.com/bytecodealliance/wasmtime/pull/3002)\n  [#3004](https://github.com/bytecodealliance/wasmtime/pull/3004)\n  [#3005](https://github.com/bytecodealliance/wasmtime/pull/3005)\n  [#3008](https://github.com/bytecodealliance/wasmtime/pull/3008)\n  [#3027](https://github.com/bytecodealliance/wasmtime/pull/3027)\n\n* The s390x backend now supports z14 and atomics.\n  [#2988](https://github.com/bytecodealliance/wasmtime/pull/2988)\n  [#2991](https://github.com/bytecodealliance/wasmtime/pull/2991)\n\n* The `wasmtime::Linker` type now implements `Clone`.\n  [#2993](https://github.com/bytecodealliance/wasmtime/pull/2993)\n\n* Support for the SIMD proposal on both x86\\_64 and AArch64 has improved. On\n  x86\\_64, all SIMD opcodes are now supported.\n  [#2997](https://github.com/bytecodealliance/wasmtime/pull/2997)\n  [#3035](https://github.com/bytecodealliance/wasmtime/pull/3035)\n  [#2982](https://github.com/bytecodealliance/wasmtime/pull/2982)\n  [#3084](https://github.com/bytecodealliance/wasmtime/pull/3084)\n  [#3082](https://github.com/bytecodealliance/wasmtime/pull/3082)\n  [#3107](https://github.com/bytecodealliance/wasmtime/pull/3107)\n  [#3105](https://github.com/bytecodealliance/wasmtime/pull/3105)\n  [#3114](https://github.com/bytecodealliance/wasmtime/pull/3114)\n  [#3070](https://github.com/bytecodealliance/wasmtime/pull/3070)\n  [#3126](https://github.com/bytecodealliance/wasmtime/pull/3126)\n\n* A `Trap` can now display its reason without also displaying the backtrace.\n  [#3033](https://github.com/bytecodealliance/wasmtime/pull/3033)\n\n* An initiall fuzzer for CLIF has been added.\n  [#3038](https://github.com/bytecodealliance/wasmtime/pull/3038)\n\n* High-level architecture documentation has been added for Wasmtime.\n  [#3019](https://github.com/bytecodealliance/wasmtime/pull/3019)\n\n* Support for multi-memory can now be configured in Wasmtime's C API.\n  [#3071](https://github.com/bytecodealliance/wasmtime/pull/3071)\n\n* The `wasmtime` crate now supports a `posix-signals-on-macos` feature to force\n  the usage of signals instead of mach ports to handle traps on macOS.\n  [#3063](https://github.com/bytecodealliance/wasmtime/pull/3063)\n\n* Wasmtime's C API now has a `wasmtime_trap_code` function to get the raw trap\n  code, if present, for a trap.\n  [#3086](https://github.com/bytecodealliance/wasmtime/pull/3086)\n\n* Wasmtime's C API now has a `wasmtime_linker_define_func` function to define a\n  store-independent function within a linker.\n  [#3122](https://github.com/bytecodealliance/wasmtime/pull/3122)\n\n* A `wasmtime::Linker::module_async` function was added as the asynchronous\n  counterpart to `wasmtime::Linker::module`.\n  [#3121](https://github.com/bytecodealliance/wasmtime/pull/3121)\n\n### Fixed\n\n* Compiling the `wasmtime` crate into a `dylib` crate type has been fixed.\n  [#3010](https://github.com/bytecodealliance/wasmtime/pull/3010)\n\n* The enter/exit hooks for WebAssembly are now executed for an instance's\n  `start` function, if present.\n  [#3001](https://github.com/bytecodealliance/wasmtime/pull/3001)\n\n* Some WASI functions in `wasi-common` have been fixed for big-endian platforms.\n  [#3016](https://github.com/bytecodealliance/wasmtime/pull/3016)\n\n* Wasmtime no longer erroneously assumes that all custom sections may contain\n  DWARF information, reducing instances of `Trap`'s `Display` implementation\n  providing misleading information to set an env var to get more information.\n  [#3083](https://github.com/bytecodealliance/wasmtime/pull/3083)\n\n* Some issues with parsing DWARF debug information have been fixed.\n  [#3116](https://github.com/bytecodealliance/wasmtime/pull/3116)\n\n## 0.28.0\n\nReleased 2021-06-09.\n\n### Changed\n\n* Breaking: Wasmtime's embedding API has been redesigned, as specified in [RFC\n  11]. Rust users can now enjoy easier times with `Send` and `Sync`, and all\n  users can now more clearly manage memory, especially in the C API. Language\n  embeddings have been updated to the new API as well.\n  [#2897](https://github.com/bytecodealliance/wasmtime/pull/2897)\n\n[RFC 11]: https://github.com/bytecodealliance/rfcs/pull/11\n\n### Added\n\n* A new `InstancePre` type, created with `Linker::instantiate_pre`, has been\n  added to perform type-checking of an instance once and reduce the work done\n  for each instantiation of a module:\n  [#2962](https://github.com/bytecodealliance/wasmtime/pull/2962)\n\n* Deserialization of a module can now optionally skip checking the wasmtime\n  version string:\n  [#2945](https://github.com/bytecodealliance/wasmtime/pull/2945)\n\n* A method has been exposed to frontload per-thread initialization costs if the\n  latency of every last wasm call is important:\n  [#2946](https://github.com/bytecodealliance/wasmtime/pull/2946)\n\n* Hooks have been added for entry/exit into wasm code to allow embeddings to\n  track time and other properties about execution in a wasm environment:\n  [#2952](https://github.com/bytecodealliance/wasmtime/pull/2952)\n\n* A [C++ embedding of Wasmtime has been written][cpp].\n\n[RFC 11]: https://github.com/bytecodealliance/rfcs/pull/11\n[cpp]: https://github.com/bytecodealliance/wasmtime-cpp\n\n### Fixed\n\n* Multiple returns on macOS AArch64 have been fixed:\n  [#2956](https://github.com/bytecodealliance/wasmtime/pull/2956)\n\n## 0.27.0\n\nReleased 2021-05-21.\n\n### Security Fixes\n\n* Fixed a security issue in Cranelift's x64 backend that could result in a heap\n  sandbox escape due to an incorrect sign-extension:\n  [#2913](https://github.com/bytecodealliance/wasmtime/issues/2913).\n\n### Added\n\n* Support for IBM z/Archiecture (`s390x`) machines in Cranelift and Wasmtime:\n  [#2836](https://github.com/bytecodealliance/wasmtime/pull/2836),\n  [#2837](https://github.com/bytecodealliance/wasmtime/pull/2837),\n  [#2838](https://github.com/bytecodealliance/wasmtime/pull/2838),\n  [#2843](https://github.com/bytecodealliance/wasmtime/pull/2843),\n  [#2854](https://github.com/bytecodealliance/wasmtime/pull/2854),\n  [#2870](https://github.com/bytecodealliance/wasmtime/pull/2870),\n  [#2871](https://github.com/bytecodealliance/wasmtime/pull/2871),\n  [#2872](https://github.com/bytecodealliance/wasmtime/pull/2872),\n  [#2874](https://github.com/bytecodealliance/wasmtime/pull/2874).\n\n* Improved async support in wasi-common runtime:\n  [#2832](https://github.com/bytecodealliance/wasmtime/pull/2832).\n\n* Added `Store::with_limits`, `StoreLimits`, and `ResourceLimiter` to the\n  Wasmtime API to help with enforcing resource limits at runtime. The\n  `ResourceLimiter` trait can be implemented by custom resource limiters to\n  decide if linear memories or tables can be grown.\n\n* Added `allow-unknown-exports` option for the run command:\n  [#2879](https://github.com/bytecodealliance/wasmtime/pull/2879).\n\n* Added API to notify that a `Store` has moved to a new thread:\n  [#2822](https://github.com/bytecodealliance/wasmtime/pull/2822).\n\n* Documented guidance around using Wasmtime in multithreaded contexts:\n  [#2812](https://github.com/bytecodealliance/wasmtime/pull/2812).\n  In the future, the Wasmtime API will change to allow some of its core types\n  to be Send/Sync; see the in-progress\n  [#2897](https://github.com/bytecodealliance/wasmtime/pull/2897) for details.\n\n* Support calls from native code to multiple-return-value functions:\n  [#2806](https://github.com/bytecodealliance/wasmtime/pull/2806).\n\n### Changed\n\n* Breaking: `Memory::new` has been changed to return `Result` as creating a\n  host memory object is now a fallible operation when the initial size of\n  the memory exceeds the store limits.\n\n### Fixed\n\n* Many instruction selection improvements on x64 and aarch64:\n  [#2819](https://github.com/bytecodealliance/wasmtime/pull/2819),\n  [#2828](https://github.com/bytecodealliance/wasmtime/pull/2828),\n  [#2823](https://github.com/bytecodealliance/wasmtime/pull/2823),\n  [#2862](https://github.com/bytecodealliance/wasmtime/pull/2862),\n  [#2886](https://github.com/bytecodealliance/wasmtime/pull/2886),\n  [#2889](https://github.com/bytecodealliance/wasmtime/pull/2889),\n  [#2905](https://github.com/bytecodealliance/wasmtime/pull/2905).\n\n* Improved performance of Wasmtime runtime substantially:\n  [#2811](https://github.com/bytecodealliance/wasmtime/pull/2811),\n  [#2818](https://github.com/bytecodealliance/wasmtime/pull/2818),\n  [#2821](https://github.com/bytecodealliance/wasmtime/pull/2821),\n  [#2847](https://github.com/bytecodealliance/wasmtime/pull/2847),\n  [#2900](https://github.com/bytecodealliance/wasmtime/pull/2900).\n\n* Fixed WASI issue with file metadata on Windows:\n  [#2884](https://github.com/bytecodealliance/wasmtime/pull/2884).\n\n* Fixed an issue with debug info and an underflowing (trapping) offset:\n  [#2866](https://github.com/bytecodealliance/wasmtime/pull/2866).\n\n* Fixed an issue with unwind information in the old x86 backend:\n  [#2845](https://github.com/bytecodealliance/wasmtime/pull/2845).\n\n* Fixed i32 spilling in x64 backend:\n  [#2840](https://github.com/bytecodealliance/wasmtime/pull/2840).\n\n## 0.26.0\n\nReleased 2021-04-05.\n\n### Added\n\n* Added the `wasmtime compile` command to support AOT compilation of Wasm\n  modules. This adds the `Engine::precompile_module` method. Also added the\n  `Config::target` method to change the compilation target of the\n  configuration. This can be used in conjunction with\n  `Engine::precompile_module` to target a different host triple than the\n  current one.\n  [#2791](https://github.com/bytecodealliance/wasmtime/pull/2791)\n\n* Support for macOS on aarch64 (Apple M1 Silicon), including Apple-specific\n  calling convention details and unwinding/exception handling using Mach ports.\n  [#2742](https://github.com/bytecodealliance/wasmtime/pull/2742),\n  [#2723](https://github.com/bytecodealliance/wasmtime/pull/2723)\n\n* A number of SIMD instruction implementations in the new x86-64 backend.\n  [#2771](https://github.com/bytecodealliance/wasmtime/pull/2771)\n\n* Added the `Config::cranelift_flag_enable` method to enable setting Cranelift\n  boolean flags or presets in a config.\n\n* Added CLI option `--cranelift-enable` to enable boolean settings and ISA presets.\n\n* Deduplicate function signatures in Wasm modules.\n  [#2772](https://github.com/bytecodealliance/wasmtime/pull/2772)\n\n* Optimize overheads of calling into Wasm functions.\n  [#2757](https://github.com/bytecodealliance/wasmtime/pull/2757),\n  [#2759](https://github.com/bytecodealliance/wasmtime/pull/2759)\n\n* Improvements related to Module Linking: compile fewer trampolines;\n\n  [#2774](https://github.com/bytecodealliance/wasmtime/pull/2774)\n\n* Re-export sibling crates from `wasmtime-wasi` to make embedding easier\n  without needing to match crate versions.\n  [#2776](https://github.com/bytecodealliance/wasmtime/pull/2776)\n\n### Changed\n\n* Switched the default compiler backend on x86-64 to Cranelift's new backend.\n  This should not have any user-visible effects other than possibly runtime\n  performance improvements. The old backend is still available with the\n  `old-x86-backend` feature flag to the `cranelift-codegen` or `wasmtime`\n  crates, or programmatically with `BackendVariant::Legacy`. We plan to\n  maintain the old backend for at least one more release and ensure it works on\n  CI.\n  [#2718](https://github.com/bytecodealliance/wasmtime/pull/2718)\n\n* Breaking: `Module::deserialize` has been removed in favor of `Module::new`.\n\n* Breaking: `Config::cranelift_clear_cpu_flags` was removed. Use `Config::target`\n  to clear the CPU flags for the host's target.\n\n* Breaking: `Config::cranelift_other_flag` was renamed to `Config::cranelift_flag_set`.\n\n* CLI changes:\n  * Wasmtime CLI options to enable WebAssembly features have been replaced with\n    a singular `--wasm-features` option. The previous options are still\n    supported, but are not displayed in help text.\n  * Breaking: the CLI option `--cranelift-flags` was changed to\n    `--cranelift-set`.\n  * Breaking: the CLI option `--enable-reference-types=false` has been changed\n    to `--wasm-features=-reference-types`.\n  * Breaking: the CLI option `--enable-multi-value=false` has been changed to\n    `--wasm-features=-multi-value`.\n  * Breaking: the CLI option `--enable-bulk-memory=false` has been changed to\n    `--wasm-features=-bulk-memory`.\n\n* Improved error-reporting in wiggle.\n  [#2760](https://github.com/bytecodealliance/wasmtime/pull/2760)\n\n* Make WASI sleeping fallible (some systems do not support sleep).\n  [#2756](https://github.com/bytecodealliance/wasmtime/pull/2756)\n\n* WASI: Support `poll_oneoff` with a sleep.\n  [#2753](https://github.com/bytecodealliance/wasmtime/pull/2753)\n\n* Allow a `StackMapSink` to be passed when defining functions with\n  `cranelift-module`.\n  [#2739](https://github.com/bytecodealliance/wasmtime/pull/2739)\n\n* Some refactoring in new x86-64 backend to prepare for VEX/EVEX (e.g.,\n  AVX-512) instruction encodings to be supported.\n  [#2799](https://github.com/bytecodealliance/wasmtime/pull/2799)\n\n### Fixed\n\n* Fixed a corner case in `srem` (signed remainder) in the new x86-64 backend:\n  `INT_MIN % -1` should return `0`, rather than trapping. This only occurred\n  when `avoid_div_traps == false` was set by the embedding.\n  [#2763](https://github.com/bytecodealliance/wasmtime/pull/2763)\n\n* Fixed a memory leak of the `Store` when an instance traps.\n  [#2803](https://github.com/bytecodealliance/wasmtime/pull/2803)\n\n* Some fuzzing-related fixes.\n  [#2788](https://github.com/bytecodealliance/wasmtime/pull/2788),\n  [#2770](https://github.com/bytecodealliance/wasmtime/pull/2770)\n\n* Fixed memory-initialization bug in uffd allocator that could copy into the\n  wrong destination under certain conditions. Does not affect the default\n  wasmtime instance allocator.\n  [#2801](https://github.com/bytecodealliance/wasmtime/pull/2801)\n\n* Fix printing of float values from the Wasmtime CLI.\n  [#2797](https://github.com/bytecodealliance/wasmtime/pull/2797)\n\n* Remove the ability for the `Linker` to instantiate modules with duplicate\n  import strings of different types.\n  [#2789](https://github.com/bytecodealliance/wasmtime/pull/2789)\n\n## 0.25.0\n\nReleased 2021-03-16.\n\n### Added\n\n* An implementation of a pooling instance allocator, optionally backed by\n  `userfaultfd` on Linux, was added to improve the performance of embeddings\n  that instantiate a large number of instances continuously.\n  [#2518](https://github.com/bytecodealliance/wasmtime/pull/2518)\n\n* Host functions can now be defined on `Config` to share the function across all\n  `Store` objects connected to an `Engine`. This can improve the time it takes\n  to instantiate instances in a short-lived `Store`.\n  [#2625](https://github.com/bytecodealliance/wasmtime/pull/2625)\n\n* The `Store` object now supports having typed values attached to it which can\n  be retrieved from host functions.\n  [#2625](https://github.com/bytecodealliance/wasmtime/pull/2625)\n\n* The `wiggle` code generator now supports `async` host functions.\n  [#2701](https://github.com/bytecodealliance/wasmtime/pull/2701)\n\n### Changed\n\n* The `Func::getN{,_async}` APIs have all been removed in favor of a new\n  `Func::typed` API which should be more compact in terms of API surface area as\n  well as more flexible in how it can be used.\n  [#2719](https://github.com/bytecodealliance/wasmtime/pull/2719)\n\n* `Engine::new` has been changed from returning `Engine` to returning\n  `anyhow::Result<Engine>`. Callers of `Engine::new` will need to be updated to\n  use the `?` operator on the return value or otherwise unwrap the result to get\n  the `Engine`.\n\n### Fixed\n\n* Interpretation of timestamps in `poll_oneoff` for WASI have been fixed to\n  correctly use nanoseconds instead of microseconds.\n  [#2717](https://github.com/bytecodealliance/wasmtime/pull/2717)\n\n## 0.24.0\n\nReleased 2021-03-04.\n\n### Added\n\n* Implement support for `async` functions in Wasmtime\n  [#2434](https://github.com/bytecodealliance/wasmtime/pull/2434)\n\n### Fixed\n\n* Fix preservation of the sigaltstack on macOS\n  [#2676](https://github.com/bytecodealliance/wasmtime/pull/2676)\n* Fix incorrect semver dependencies involving fs-set-times.\n  [#2705](https://github.com/bytecodealliance/wasmtime/pull/2705)\n* Fix some `i128` shift-related bugs in x64 backend.\n  [#2682](https://github.com/bytecodealliance/wasmtime/pull/2682)\n* Fix incomplete trap metadata due to multiple traps at one address\n  [#2685](https://github.com/bytecodealliance/wasmtime/pull/2685)\n\n## 0.23.0\n\nReleased 2021-02-16.\n\n### Added\n\n* Support for limiting WebAssembly execution with fuel was added, including\n  support in the C API.\n  [#2611](https://github.com/bytecodealliance/wasmtime/pull/2611)\n  [#2643](https://github.com/bytecodealliance/wasmtime/pull/2643)\n* Wasmtime now has more knobs for limiting memory and table allocations\n  [#2617](https://github.com/bytecodealliance/wasmtime/pull/2617)\n* Added a method to share `Config` across machines\n  [#2608](https://github.com/bytecodealliance/wasmtime/pull/2608)\n* Added a safe memory read/write API\n  [#2528](https://github.com/bytecodealliance/wasmtime/pull/2528)\n* Added support for the experimental wasi-crypto APIs\n  [#2597](https://github.com/bytecodealliance/wasmtime/pull/2597)\n* Added an instance limit to `Config`\n  [#2593](https://github.com/bytecodealliance/wasmtime/pull/2593)\n* Implemented module-linking's outer module aliases\n  [#2590](https://github.com/bytecodealliance/wasmtime/pull/2590)\n* Cranelift now supports 128-bit operations for the new x64 backend.\n  [#2539](https://github.com/bytecodealliance/wasmtime/pull/2539)\n* Cranelift now has detailed debug-info (DWARF) support in new backends (initially x64).\n  [#2565](https://github.com/bytecodealliance/wasmtime/pull/2565)\n* Cranelift now uses the `POPCNT`, `TZCNT`, and `LZCNT`, as well as SSE 4.1\n  rounding instructions on x64 when available.\n* Cranelift now uses the `CNT`, instruction on aarch64 when available.\n\n### Changed\n\n* A new WASI implementation built on the new\n  [`cap-std`](https://github.com/bytecodealliance/cap-std) crate was added,\n  replacing the previous implementation. This brings improved robustness,\n  portability, and performance.\n\n* `wasmtime_wasi::WasiCtxBuilder` moved to\n  `wasi_cap_std_sync::WasiCtxBuilder`.\n\n* The WebAssembly C API is updated, with a few minor API changes\n  [#2579](https://github.com/bytecodealliance/wasmtime/pull/2579)\n\n### Fixed\n\n* Fixed a panic in WASI `fd_readdir` on large directories\n  [#2620](https://github.com/bytecodealliance/wasmtime/pull/2620)\n* Fixed a memory leak with command modules\n  [#2017](https://github.com/bytecodealliance/wasmtime/pull/2017)\n\n--------------------------------------------------------------------------------\n\n## 0.22.0\n\nReleased 2021-01-07.\n\n### Added\n\n* Experimental support for [the module-linking\n  proposal](https://github.com/WebAssembly/module-linking) was\n  added. [#2094](https://github.com/bytecodealliance/wasmtime/pull/2094)\n\n* Added support for [the reference types\n  proposal](https://webassembly.github.io/reference-types) on the aarch64\n  architecture. [#2410](https://github.com/bytecodealliance/wasmtime/pull/2410)\n\n* Experimental support for [wasi-nn](https://github.com/WebAssembly/wasi-nn) was\n  added. [#2208](https://github.com/bytecodealliance/wasmtime/pull/2208)\n\n### Changed\n\n### Fixed\n\n* Fixed an issue where the `select` instruction didn't accept `v128` SIMD\n  operands. [#2391](https://github.com/bytecodealliance/wasmtime/pull/2391)\n\n* Fixed an issue where Wasmtime could potentially use the wrong stack map during\n  GCs, leading to a\n  panic. [#2396](https://github.com/bytecodealliance/wasmtime/pull/2396)\n\n* Fixed an issue where if a host-defined function erroneously returned a value\n  from a different store, that value would be\n  leaked. [#2424](https://github.com/bytecodealliance/wasmtime/pull/2424)\n\n* Fixed a bug where in certain cases if a module's instantiation failed, it\n  could leave trampolines in the store that referenced the no-longer-valid\n  instance. These trampolines could be reused in future instantiations, leading\n  to use after free bugs.\n  [#2408](https://github.com/bytecodealliance/wasmtime/pull/2408)\n\n* Fixed a miscompilation on aarch64 where certain instructions would read `SP`\n  instead of the zero register. This could only affect you if you explicitly\n  enabled the Wasm SIMD\n  proposal. [#2548](https://github.com/bytecodealliance/wasmtime/pull/2548)\n\n--------------------------------------------------------------------------------\n\n## 0.21.0\n\nReleased 2020-11-05.\n\n### Added\n\n* Experimental support for the multi-memory proposal was added.\n  [#2263](https://github.com/bytecodealliance/wasmtime/pull/2263)\n\n* The `Trap::trap_code` API enables learning what kind of trap was raised.\n  [#2309](https://github.com/bytecodealliance/wasmtime/pull/2309)\n\n### Changed\n\n* WebAssembly module validation is now parallelized.\n  [#2059](https://github.com/bytecodealliance/wasmtime/pull/2059)\n\n* Documentation is now available at docs.wasmtime.dev.\n  [#2317](https://github.com/bytecodealliance/wasmtime/pull/2317)\n\n* Windows now compiles like other platforms with a huge guard page instead of\n  having its own custom limit which made modules compile and run more slowly.\n  [#2326](https://github.com/bytecodealliance/wasmtime/pull/2326)\n\n* The size of the cache entry for serialized modules has been greatly reduced.\n  [#2321](https://github.com/bytecodealliance/wasmtime/pull/2321)\n  [#2322](https://github.com/bytecodealliance/wasmtime/pull/2322)\n  [#2324](https://github.com/bytecodealliance/wasmtime/pull/2324)\n  [#2325](https://github.com/bytecodealliance/wasmtime/pull/2325)\n\n* The `FuncType` API constructor and accessors are now iterator-based.\n  [#2365](https://github.com/bytecodealliance/wasmtime/pull/2365)\n\n### Fixed\n\n* A panic in compiling reference-types-using modules has been fixed.\n  [#2350](https://github.com/bytecodealliance/wasmtime/pull/2350)\n\n--------------------------------------------------------------------------------\n\n## 0.20.0\n\nReleased 2020-09-23.\n\n### Added\n\n* Support for explicitly serializing and deserializing compiled wasm modules has\n  been added.\n  [#2020](https://github.com/bytecodealliance/wasmtime/pull/2020)\n\n* A `wasmtime_store_gc` C API was added to run GC for `externref`.\n  [#2052](https://github.com/bytecodealliance/wasmtime/pull/2052)\n\n* Support for atomics in Cranelift has been added. Support is not fully\n  implemented in Wasmtime at this time, however.\n  [#2077](https://github.com/bytecodealliance/wasmtime/pull/2077)\n\n* The `Caller::get_export` function is now implemented for `Func` references as\n  well.\n  [#2108](https://github.com/bytecodealliance/wasmtime/pull/2108)\n\n### Fixed\n\n* Leaks in the C API have been fixed.\n  [#2040](https://github.com/bytecodealliance/wasmtime/pull/2040)\n\n* The `wasm_val_copy` C API has been fixed for reference types.\n  [#2041](https://github.com/bytecodealliance/wasmtime/pull/2041)\n\n* Fix a panic with `Func::new` and reference types when the store doesn't have\n  reference types enabled.\n  [#2039](https://github.com/bytecodealliance/wasmtime/pull/2039)\n\n--------------------------------------------------------------------------------\n\n## 0.19.0\n\nReleased 2020-07-14.\n\n### Added\n\n* The [WebAssembly reference-types proposal][reftypes] is now supported in\n  Wasmtime and the C API.\n  [#1832](https://github.com/bytecodealliance/wasmtime/pull/1832),\n  [#1882](https://github.com/bytecodealliance/wasmtime/pull/1882),\n  [#1894](https://github.com/bytecodealliance/wasmtime/pull/1894),\n  [#1901](https://github.com/bytecodealliance/wasmtime/pull/1901),\n  [#1923](https://github.com/bytecodealliance/wasmtime/pull/1923),\n  [#1969](https://github.com/bytecodealliance/wasmtime/pull/1969),\n  [#1973](https://github.com/bytecodealliance/wasmtime/pull/1973),\n  [#1982](https://github.com/bytecodealliance/wasmtime/pull/1982),\n  [#1984](https://github.com/bytecodealliance/wasmtime/pull/1984),\n  [#1991](https://github.com/bytecodealliance/wasmtime/pull/1991),\n  [#1996](https://github.com/bytecodealliance/wasmtime/pull/1996)\n\n* The [WebAssembly simd proposal's][simd] spec tests now pass in Wasmtime.\n  [#1765](https://github.com/bytecodealliance/wasmtime/pull/1765),\n  [#1876](https://github.com/bytecodealliance/wasmtime/pull/1876),\n  [#1941](https://github.com/bytecodealliance/wasmtime/pull/1941),\n  [#1957](https://github.com/bytecodealliance/wasmtime/pull/1957),\n  [#1990](https://github.com/bytecodealliance/wasmtime/pull/1990),\n  [#1994](https://github.com/bytecodealliance/wasmtime/pull/1994)\n\n* Wasmtime can now be compiled without the usage of threads for parallel\n  compilation, although this is still enabled by default.\n  [#1903](https://github.com/bytecodealliance/wasmtime/pull/1903)\n\n* The C API is [now\n  documented](https://bytecodealliance.github.io/wasmtime/c-api/).\n  [#1928](https://github.com/bytecodealliance/wasmtime/pull/1928),\n  [#1959](https://github.com/bytecodealliance/wasmtime/pull/1959),\n  [#1968](https://github.com/bytecodealliance/wasmtime/pull/1968)\n\n* A `wasmtime_linker_get_one_by_name` function was added to the C API.\n  [#1897](https://github.com/bytecodealliance/wasmtime/pull/1897)\n\n* A `wasmtime_trap_exit_status` function was added to the C API.\n  [#1912](https://github.com/bytecodealliance/wasmtime/pull/1912)\n\n* Compilation for the `aarch64-linux-android` target should now work, although\n  keep in mind this platform is not fully tested still.\n  [#2002](https://github.com/bytecodealliance/wasmtime/pull/2002)\n\n[reftypes]: https://github.com/WebAssembly/reference-types\n\n### Fixed\n\n* Runtime warnings when using Wasmtime on musl have been fixed.\n  [#1914](https://github.com/bytecodealliance/wasmtime/pull/1914)\n\n* A bug affecting Windows unwind information with functions that have spilled\n  floating point registers has been fixed.\n  [#1983](https://github.com/bytecodealliance/wasmtime/pull/1983)\n\n### Changed\n\n* Wasmtime's default branch and development now happens on the `main` branch\n  instead of `master`.\n  [#1924](https://github.com/bytecodealliance/wasmtime/pull/1924)\n\n### Removed\n\n* The \"host info\" support in the C API has been removed since it was never fully\n  or correctly implemented.\n  [#1922](https://github.com/bytecodealliance/wasmtime/pull/1922)\n\n* Support for the `*_same` functions in the C API has been removed in the same\n  vein as the host info APIs.\n  [#1926](https://github.com/bytecodealliance/wasmtime/pull/1926)\n\n--------------------------------------------------------------------------------\n\n## 0.18.0\n\nRelease 2020-06-09.\n\n### Added\n\nThe `WasmTy` trait is now implemented for `u32` and `u64`.\n\n  [#1808](https://github.com/bytecodealliance/wasmtime/pull/1808)\n\n--------------------------------------------------------------------------------\n\n## 0.17.0\n\nReleased 2020-06-01.\n\n### Added\n\n* The [Commands and Reactors ABI] is now supported in the Rust API. `Linker::module`\n  loads a module and automatically handles Commands and Reactors semantics.\n\n  [#1565](https://github.com/bytecodealliance/wasmtime/pull/1565)\n\n[Commands and Reactors ABI]: https://github.com/WebAssembly/WASI/blob/master/design/application-abi.md#current-unstable-abi\n\nThe `Table::grow` function now returns the previous table size, making it consistent\nwith the `table.grow` instruction.\n\n  [#1653](https://github.com/bytecodealliance/wasmtime/pull/1653)\n\nNew Wasmtime-specific C APIs for working with tables were added which provide more\ndetailed error information and which make growing a table more consistent with the\n`table.grow` instruction as well.\n\n  [#1654](https://github.com/bytecodealliance/wasmtime/pull/1654)\n\nThe C API now includes support for enabling logging in Wasmtime.\n\n  [#1737](https://github.com/bytecodealliance/wasmtime/pull/1737)\n\n### Changed\n\nThe WASI `proc_exit` function no longer exits the host process. It now unwinds the\ncallstack back to the wasm entrypoint, and the exit value is available from the\n`Trap::i32_exit_status` method.\n\n  [#1646](https://github.com/bytecodealliance/wasmtime/pull/1646)\n\nThe WebAssembly [multi-value](https://github.com/WebAssembly/multi-value/) proposal\nis now enabled by default.\n\n  [#1667](https://github.com/bytecodealliance/wasmtime/pull/1667)\n\nThe Rust API does not require a store provided during `Module::new` operation. The `Module` can be send accross threads and instantiate for a specific store. The `Instance::new` now requires the store.\n\n  [#1761](https://github.com/bytecodealliance/wasmtime/pull/1761)\n\n--------------------------------------------------------------------------------\n\n## 0.16.0\n\nReleased 2020-04-29.\n\n### Added\n\n* The `Instance` struct has new accessors, `get_func`, `get_table`,\n  `get_memory`, and `get_global` for quickly looking up exported\n  functions, tables, memories, and globals by name.\n  [#1524](https://github.com/bytecodealliance/wasmtime/pull/1524)\n\n* The C API has a number of new `wasmtime_*` functions which return error\n  objects to get detailed error information when an API fails.\n  [#1467](https://github.com/bytecodealliance/wasmtime/pull/1467)\n\n* Users now have fine-grained control over creation of instances of `Memory`\n  with a new `MemoryCreator` trait.\n  [#1400](https://github.com/bytecodealliance/wasmtime/pull/1400)\n\n* Go bindings for Wasmtime are [now available][go-bindings].\n  [#1481](https://github.com/bytecodealliance/wasmtime/pull/1481)\n\n* APIs for looking up values in a `Linker` have been added.\n  [#1480](https://github.com/bytecodealliance/wasmtime/pull/1480)\n\n* Preliminary support for AArch64, also known as ARM64.\n  [#1581](https://github.com/bytecodealliance/wasmtime/pull/1581)\n\n[go-bindings]: https://github.com/bytecodealliance/wasmtime-go\n\n### Changed\n\n* `Instance::exports` now returns `Export` objects which contain\n  the `name`s of the exports in addition to their `Extern` definitions,\n  so it's no longer necessary to use `Module::exports` to obtain the\n  export names.\n  [#1524](https://github.com/bytecodealliance/wasmtime/pull/1524)\n\n* The `Func::call` API has changed its error type from `Trap` to `anyhow::Error`\n  to distinguish between wasm traps and runtime violations (like the wrong\n  number of parameters).\n  [#1467](https://github.com/bytecodealliance/wasmtime/pull/1467)\n\n* A number of `wasmtime_linker_*` and `wasmtime_config_*` C APIs have new type\n  signatures which reflect returning errors.\n  [#1467](https://github.com/bytecodealliance/wasmtime/pull/1467)\n\n* Bindings for .NET have moved to\n  https://github.com/bytecodealliance/wasmtime-dotnet.\n  [#1477](https://github.com/bytecodealliance/wasmtime/pull/1477)\n\n* Passing too many imports to `Instance::new` is now considered an error.\n  [#1478](https://github.com/bytecodealliance/wasmtime/pull/1478)\n\n### Fixed\n\n* Spurious segfaults due to out-of-stack conditions when handling signals have\n  been fixed.\n  [#1315](https://github.com/bytecodealliance/wasmtime/pull/1315)\n\n--------------------------------------------------------------------------------\n\n## 0.15.0\n\nReleased 2020-03-31.\n\n### Fixed\n\nFull release produced for all artifacts to account for hiccups in 0.13.0 and\n0.14.0.\n\n--------------------------------------------------------------------------------\n\n## 0.14.0\n\n*This version ended up not getting a full release*\n\n### Fixed\n\nFix build errors in wasi-common on Windows.\n\n--------------------------------------------------------------------------------\n\n## 0.13.0\n\nReleased 2020-03-24.\n\n### Added\n\n* Lots of documentation of `wasmtime` has been updated. Be sure to check out the\n  [book](https://bytecodealliance.github.io/wasmtime/) and [API\n  documentation](https://bytecodealliance.github.io/wasmtime/api/wasmtime/)!\n\n* All wasmtime example programs are now in a top-level `examples` directory and\n  are available in both C and Rust.\n  [#1286](https://github.com/bytecodealliance/wasmtime/pull/1286)\n\n* A `wasmtime::Linker` type was added to conveniently link link wasm modules\n  together and create instances that reference one another.\n  [#1384](https://github.com/bytecodealliance/wasmtime/pull/1384)\n\n* Wasmtime now has \"jitdump\" support enabled by default which allows [profiling\n  wasm code on linux][jitdump].\n  [#1310](https://github.com/bytecodealliance/wasmtime/pull/1310)\n\n* The `wasmtime::Caller` type now exists as a first-class way to access the\n  caller's exports, namely memory, when implementing host APIs. This can be the\n  first argument of functions defined with `Func::new` or `Func::wrap` which\n  allows easily implementing methods which take a pointer into wasm memory. Note\n  that this only works for accessing the caller's `Memory` for now and it must\n  be exported. This will eventually be replaced with a more general-purpose\n  mechanism like interface types.\n  [#1290](https://github.com/bytecodealliance/wasmtime/pull/1290)\n\n* The bulk memory proposal has been fully implemented.\n  [#1264](https://github.com/bytecodealliance/wasmtime/pull/1264)\n  [#976](https://github.com/bytecodealliance/wasmtime/pull/976)\n\n* Virtual file support has been added to `wasi-common`.\n  [#701](https://github.com/bytecodealliance/wasmtime/pull/701)\n\n* The C API has been enhanced with a Wasmtime-specific `wasmtime_wat2wasm` to\n  parse `*.wat` files via the C API.\n  [#1206](https://github.com/bytecodealliance/wasmtime/pull/1206)\n\n[jitdump]: https://bytecodealliance.github.io/wasmtime/examples-profiling.html\n\n### Changed\n\n* The `wast` and `wasm2obj` standalone binaries have been removed. They're\n  available via the `wasmtime wast` and `wasmtime wasm2obj` subcommands.\n  [#1372](https://github.com/bytecodealliance/wasmtime/pull/1372)\n\n* The `wasi-common` crate now uses the new `wiggle` crate to auto-generate a\n  trait which is implemented for the current wasi snapshot.\n  [#1202](https://github.com/bytecodealliance/wasmtime/pull/1202)\n\n* Wasmtime no longer has a dependency on a C++ compiler.\n  [#1365](https://github.com/bytecodealliance/wasmtime/pull/1365)\n\n* The `Func::wrapN` APIs have been consolidated into one `Func::wrap` API.\n  [#1363](https://github.com/bytecodealliance/wasmtime/pull/1363)\n\n* The `Callable` trait has been removed and now `Func::new` takes a closure\n  directly.\n  [#1363](https://github.com/bytecodealliance/wasmtime/pull/1363)\n\n* The Cranelift repository has been merged into the Wasmtime repository.\n\n* Support for interface types has been temporarily removed.\n  [#1292](https://github.com/bytecodealliance/wasmtime/pull/1292)\n\n* The exit code of the `wasmtime` CLI has changed if the program traps.\n  [#1274](https://github.com/bytecodealliance/wasmtime/pull/1274)\n\n* The `wasmtime` CLI now logs to stderr by default and the `-d` flag has been\n  renamed to `--log-to-file`.\n  [#1266](https://github.com/bytecodealliance/wasmtime/pull/1266)\n\n* Values cannot cross `Store` objects, meaning you can't instantiate a module\n  with values from different stores nor pass values from different stores into\n  methods.\n  [#1016](https://github.com/bytecodealliance/wasmtime/pull/1016)\n\n--------------------------------------------------------------------------------\n\n## 0.12.0\n\nReleased 2020-02-26.\n\n### Added\n\n* Support for the [WebAssembly text annotations proposal][annotations-proposal]\n  has been added.\n  [#998](https://github.com/bytecodealliance/wasmtime/pull/998)\n\n* An initial C API for instantiating WASI modules has been added.\n  [#977](https://github.com/bytecodealliance/wasmtime/pull/977)\n\n* A new suite of `Func::getN` functions have been added to the `wasmtime` API to\n  call statically-known function signatures in a highly optimized fashion.\n  [#955](https://github.com/bytecodealliance/wasmtime/pull/955)\n\n* Initial support for profiling JIT code through perf jitdump has been added.\n  [#360](https://github.com/bytecodealliance/wasmtime/pull/360)\n\n* More CLI flags corresponding to proposed WebAssembly features have been added.\n  [#917](https://github.com/bytecodealliance/wasmtime/pull/917)\n\n[annotations-proposal]: https://github.com/webassembly/annotations\n\n### Changed\n\n* The `wasmtime` CLI as well as embedding API will optimize WebAssembly code by\n  default now.\n  [#973](https://github.com/bytecodealliance/wasmtime/pull/973)\n  [#988](https://github.com/bytecodealliance/wasmtime/pull/988)\n\n* The `verifier` pass in Cranelift is now no longer run by default when using\n  the embedding API.\n  [#882](https://github.com/bytecodealliance/wasmtime/pull/882)\n\n### Fixed\n\n* Code caching now accurately accounts for optimization levels, ensuring that if\n  you ask for optimized code you're not accidentally handed unoptimized code\n  from the cache.\n  [#974](https://github.com/bytecodealliance/wasmtime/pull/974)\n\n* Automated releases for tags should be up and running again, along with\n  automatic publication of the `wasmtime` Python package.\n  [#971](https://github.com/bytecodealliance/wasmtime/pull/971)\n", "//! Data structures for representing decoded wasm modules.\n\nuse crate::{ModuleTranslation, PrimaryMap, Tunables, WASM_PAGE_SIZE};\nuse cranelift_entity::{packed_option::ReservedValue, EntityRef};\nuse indexmap::IndexMap;\nuse serde::{Deserialize, Serialize};\nuse std::collections::BTreeMap;\nuse std::convert::TryFrom;\nuse std::mem;\nuse std::ops::Range;\nuse wasmtime_types::*;\n\n/// Implementation styles for WebAssembly linear memory.\n#[derive(Debug, Clone, Hash, Serialize, Deserialize)]\npub enum MemoryStyle {\n    /// The actual memory can be resized and moved.\n    Dynamic {\n        /// Extra space to reserve when a memory must be moved due to growth.\n        reserve: u64,\n    },\n    /// Address space is allocated up front.\n    Static {\n        /// The number of mapped and unmapped pages.\n        bound: u64,\n    },\n}\n\nimpl MemoryStyle {\n    /// Decide on an implementation style for the given `Memory`.\n    pub fn for_memory(memory: Memory, tunables: &Tunables) -> (Self, u64) {\n        // A heap with a maximum that doesn't exceed the static memory bound specified by the\n        // tunables make it static.\n        //\n        // If the module doesn't declare an explicit maximum treat it as 4GiB when not\n        // requested to use the static memory bound itself as the maximum.\n        let absolute_max_pages = if memory.memory64 {\n            crate::WASM64_MAX_PAGES\n        } else {\n            crate::WASM32_MAX_PAGES\n        };\n        let maximum = std::cmp::min(\n            memory.maximum.unwrap_or(absolute_max_pages),\n            if tunables.static_memory_bound_is_maximum {\n                std::cmp::min(tunables.static_memory_bound, absolute_max_pages)\n            } else {\n                absolute_max_pages\n            },\n        );\n\n        // Ensure the minimum is less than the maximum; the minimum might exceed the maximum\n        // when the memory is artificially bounded via `static_memory_bound_is_maximum` above\n        if memory.minimum <= maximum && maximum <= tunables.static_memory_bound {\n            return (\n                Self::Static {\n                    bound: tunables.static_memory_bound,\n                },\n                tunables.static_memory_offset_guard_size,\n            );\n        }\n\n        // Otherwise, make it dynamic.\n        (\n            Self::Dynamic {\n                reserve: tunables.dynamic_memory_growth_reserve,\n            },\n            tunables.dynamic_memory_offset_guard_size,\n        )\n    }\n}\n\n/// A WebAssembly linear memory description along with our chosen style for\n/// implementing it.\n#[derive(Debug, Clone, Hash, Serialize, Deserialize)]\npub struct MemoryPlan {\n    /// The WebAssembly linear memory description.\n    pub memory: Memory,\n    /// Our chosen implementation style.\n    pub style: MemoryStyle,\n    /// Chosen size of a guard page before the linear memory allocation.\n    pub pre_guard_size: u64,\n    /// Our chosen offset-guard size.\n    pub offset_guard_size: u64,\n}\n\nimpl MemoryPlan {\n    /// Draw up a plan for implementing a `Memory`.\n    pub fn for_memory(memory: Memory, tunables: &Tunables) -> Self {\n        let (style, offset_guard_size) = MemoryStyle::for_memory(memory, tunables);\n        Self {\n            memory,\n            style,\n            offset_guard_size,\n            pre_guard_size: if tunables.guard_before_linear_memory {\n                offset_guard_size\n            } else {\n                0\n            },\n        }\n    }\n}\n\n/// A WebAssembly linear memory initializer.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct MemoryInitializer {\n    /// The index of a linear memory to initialize.\n    pub memory_index: MemoryIndex,\n    /// Optionally, a global variable giving a base index.\n    pub base: Option<GlobalIndex>,\n    /// The offset to add to the base.\n    pub offset: u64,\n    /// The range of the data to write within the linear memory.\n    ///\n    /// This range indexes into a separately stored data section which will be\n    /// provided with the compiled module's code as well.\n    pub data: Range<u32>,\n}\n\n/// Similar to the above `MemoryInitializer` but only used when memory\n/// initializers are statically known to be valid.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct StaticMemoryInitializer {\n    /// The 64-bit offset, in bytes, of where this initializer starts.\n    pub offset: u64,\n\n    /// The range of data to write at `offset`, where these indices are indexes\n    /// into the compiled wasm module's data section.\n    pub data: Range<u32>,\n}\n\n/// The type of WebAssembly linear memory initialization to use for a module.\n#[derive(Debug, Serialize, Deserialize)]\npub enum MemoryInitialization {\n    /// Memory initialization is segmented.\n    ///\n    /// Segmented initialization can be used for any module, but it is required\n    /// if:\n    ///\n    /// * A data segment referenced an imported memory.\n    /// * A data segment uses a global base.\n    ///\n    /// Segmented initialization is performed by processing the complete set of\n    /// data segments when the module is instantiated.\n    ///\n    /// This is the default memory initialization type.\n    Segmented(Vec<MemoryInitializer>),\n\n    /// Memory initialization is statically known and involves a single `memcpy`\n    /// or otherwise simply making the defined data visible.\n    ///\n    /// To be statically initialized everything must reference a defined memory\n    /// and all data segments have a statically known in-bounds base (no\n    /// globals).\n    ///\n    /// This form of memory initialization is a more optimized version of\n    /// `Segmented` where memory can be initialized with one of a few methods:\n    ///\n    /// * First it could be initialized with a single `memcpy` of data from the\n    ///   module to the linear memory.\n    /// * Otherwise techniques like `mmap` are also possible to make this data,\n    ///   which might reside in a compiled module on disk, available immediately\n    ///   in a linear memory's address space.\n    ///\n    /// To facilitate the latter of these techniques the `try_static_init`\n    /// function below, which creates this variant, takes a host page size\n    /// argument which can page-align everything to make mmap-ing possible.\n    Static {\n        /// The initialization contents for each linear memory.\n        ///\n        /// This array has, for each module's own linear memory, the contents\n        /// necessary to initialize it. If the memory has a `None` value then no\n        /// initialization is necessary (it's zero-filled). Otherwise with\n        /// `Some` the first element of the tuple is the offset in memory to\n        /// start the initialization and the `Range` is the range within the\n        /// final data section of the compiled module of bytes to copy into the\n        /// memory.\n        ///\n        /// The offset, range base, and range end are all guaranteed to be page\n        /// aligned to the page size passed in to `try_static_init`.\n        map: PrimaryMap<MemoryIndex, Option<StaticMemoryInitializer>>,\n    },\n}\n\nimpl ModuleTranslation<'_> {\n    /// Attempts to convert segmented memory initialization into static\n    /// initialization for the module that this translation represents.\n    ///\n    /// If this module's memory initialization is not compatible with paged\n    /// initialization then this won't change anything. Otherwise if it is\n    /// compatible then the `memory_initialization` field will be updated.\n    ///\n    /// Takes a `page_size` argument in order to ensure that all\n    /// initialization is page-aligned for mmap-ability, and\n    /// `max_image_size_always_allowed` to control how we decide\n    /// whether to use static init.\n    ///\n    /// We will try to avoid generating very sparse images, which are\n    /// possible if e.g. a module has an initializer at offset 0 and a\n    /// very high offset (say, 1 GiB). To avoid this, we use a dual\n    /// condition: we always allow images less than\n    /// `max_image_size_always_allowed`, and the embedder of Wasmtime\n    /// can set this if desired to ensure that static init should\n    /// always be done if the size of the module or its heaps is\n    /// otherwise bounded by the system. We also allow images with\n    /// static init data bigger than that, but only if it is \"dense\",\n    /// defined as having at least half (50%) of its pages with some\n    /// data.\n    ///\n    /// We could do something slightly better by building a dense part\n    /// and keeping a sparse list of outlier/leftover segments (see\n    /// issue #3820). This would also allow mostly-static init of\n    /// modules that have some dynamically-placed data segments. But,\n    /// for now, this is sufficient to allow a system that \"knows what\n    /// it's doing\" to always get static init.\n    pub fn try_static_init(&mut self, page_size: u64, max_image_size_always_allowed: u64) {\n        // This method only attempts to transform a `Segmented` memory init\n        // into a `Static` one, no other state.\n        if !self.module.memory_initialization.is_segmented() {\n            return;\n        }\n\n        // First a dry run of memory initialization is performed. This\n        // collects information about the extent of memory initialized for each\n        // memory as well as the size of all data segments being copied in.\n        struct Memory {\n            data_size: u64,\n            min_addr: u64,\n            max_addr: u64,\n            // The `usize` here is a pointer into `self.data` which is the list\n            // of data segments corresponding to what was found in the original\n            // wasm module.\n            segments: Vec<(usize, StaticMemoryInitializer)>,\n        }\n        let mut info = PrimaryMap::with_capacity(self.module.memory_plans.len());\n        for _ in 0..self.module.memory_plans.len() {\n            info.push(Memory {\n                data_size: 0,\n                min_addr: u64::MAX,\n                max_addr: 0,\n                segments: Vec::new(),\n            });\n        }\n        let mut idx = 0;\n        let ok = self.module.memory_initialization.init_memory(\n            InitMemory::CompileTime(&self.module),\n            &mut |memory, init| {\n                // Currently `Static` only applies to locally-defined memories,\n                // so if a data segment references an imported memory then\n                // transitioning to a `Static` memory initializer is not\n                // possible.\n                if self.module.defined_memory_index(memory).is_none() {\n                    return false;\n                };\n                let info = &mut info[memory];\n                let data_len = u64::from(init.data.end - init.data.start);\n                if data_len > 0 {\n                    info.data_size += data_len;\n                    info.min_addr = info.min_addr.min(init.offset);\n                    info.max_addr = info.max_addr.max(init.offset + data_len);\n                    info.segments.push((idx, init.clone()));\n                }\n                idx += 1;\n                true\n            },\n        );\n        if !ok {\n            return;\n        }\n\n        // Validate that the memory information collected is indeed valid for\n        // static memory initialization.\n        for info in info.values().filter(|i| i.data_size > 0) {\n            let image_size = info.max_addr - info.min_addr;\n\n            // If the range of memory being initialized is less than twice the\n            // total size of the data itself then it's assumed that static\n            // initialization is ok. This means we'll at most double memory\n            // consumption during the memory image creation process, which is\n            // currently assumed to \"probably be ok\" but this will likely need\n            // tweaks over time.\n            if image_size < info.data_size.saturating_mul(2) {\n                continue;\n            }\n\n            // If the memory initialization image is larger than the size of all\n            // data, then we still allow memory initialization if the image will\n            // be of a relatively modest size, such as 1MB here.\n            if image_size < max_image_size_always_allowed {\n                continue;\n            }\n\n            // At this point memory initialization is concluded to be too\n            // expensive to do at compile time so it's entirely deferred to\n            // happen at runtime.\n            return;\n        }\n\n        // Here's where we've now committed to changing to static memory. The\n        // memory initialization image is built here from the page data and then\n        // it's converted to a single initializer.\n        let data = mem::replace(&mut self.data, Vec::new());\n        let mut map = PrimaryMap::with_capacity(info.len());\n        let mut module_data_size = 0u32;\n        for (memory, info) in info.iter() {\n            // Create the in-memory `image` which is the initialized contents of\n            // this linear memory.\n            let extent = if info.segments.len() > 0 {\n                (info.max_addr - info.min_addr) as usize\n            } else {\n                0\n            };\n            let mut image = Vec::with_capacity(extent);\n            for (idx, init) in info.segments.iter() {\n                let data = &data[*idx];\n                assert_eq!(data.len(), init.data.len());\n                let offset = usize::try_from(init.offset - info.min_addr).unwrap();\n                if image.len() < offset {\n                    image.resize(offset, 0u8);\n                    image.extend_from_slice(data);\n                } else {\n                    image.splice(\n                        offset..(offset + data.len()).min(image.len()),\n                        data.iter().copied(),\n                    );\n                }\n            }\n            assert_eq!(image.len(), extent);\n            assert_eq!(image.capacity(), extent);\n            let mut offset = if info.segments.len() > 0 {\n                info.min_addr\n            } else {\n                0\n            };\n\n            // Chop off trailing zeros from the image as memory is already\n            // zero-initialized. Note that `i` is the position of a nonzero\n            // entry here, so to not lose it we truncate to `i + 1`.\n            if let Some(i) = image.iter().rposition(|i| *i != 0) {\n                image.truncate(i + 1);\n            }\n\n            // Also chop off leading zeros, if any.\n            if let Some(i) = image.iter().position(|i| *i != 0) {\n                offset += i as u64;\n                image.drain(..i);\n            }\n            let mut len = u64::try_from(image.len()).unwrap();\n\n            // The goal is to enable mapping this image directly into memory, so\n            // the offset into linear memory must be a multiple of the page\n            // size. If that's not already the case then the image is padded at\n            // the front and back with extra zeros as necessary\n            if offset % page_size != 0 {\n                let zero_padding = offset % page_size;\n                self.data.push(vec![0; zero_padding as usize].into());\n                offset -= zero_padding;\n                len += zero_padding;\n            }\n            self.data.push(image.into());\n            if len % page_size != 0 {\n                let zero_padding = page_size - (len % page_size);\n                self.data.push(vec![0; zero_padding as usize].into());\n                len += zero_padding;\n            }\n\n            // Offset/length should now always be page-aligned.\n            assert!(offset % page_size == 0);\n            assert!(len % page_size == 0);\n\n            // Create the `StaticMemoryInitializer` which describes this image,\n            // only needed if the image is actually present and has a nonzero\n            // length. The `offset` has been calculates above, originally\n            // sourced from `info.min_addr`. The `data` field is the extent\n            // within the final data segment we'll emit to an ELF image, which\n            // is the concatenation of `self.data`, so here it's the size of\n            // the section-so-far plus the current segment we're appending.\n            let len = u32::try_from(len).unwrap();\n            let init = if len > 0 {\n                Some(StaticMemoryInitializer {\n                    offset,\n                    data: module_data_size..module_data_size + len,\n                })\n            } else {\n                None\n            };\n            let idx = map.push(init);\n            assert_eq!(idx, memory);\n            module_data_size += len;\n        }\n        self.data_align = Some(page_size);\n        self.module.memory_initialization = MemoryInitialization::Static { map };\n    }\n\n    /// Attempts to convert the module's table initializers to\n    /// FuncTable form where possible. This enables lazy table\n    /// initialization later by providing a one-to-one map of initial\n    /// table values, without having to parse all segments.\n    pub fn try_func_table_init(&mut self) {\n        // This should be large enough to support very large Wasm\n        // modules with huge funcref tables, but small enough to avoid\n        // OOMs or DoS on truly sparse tables.\n        const MAX_FUNC_TABLE_SIZE: u32 = 1024 * 1024;\n\n        let segments = match &self.module.table_initialization {\n            TableInitialization::Segments { segments } => segments,\n            TableInitialization::FuncTable { .. } => {\n                // Already done!\n                return;\n            }\n        };\n\n        // Build the table arrays per-table.\n        let mut tables = PrimaryMap::with_capacity(self.module.table_plans.len());\n        // Keep the \"leftovers\" for eager init.\n        let mut leftovers = vec![];\n\n        for segment in segments {\n            // Skip imported tables: we can't provide a preconstructed\n            // table for them, because their values depend on the\n            // imported table overlaid with whatever segments we have.\n            if self\n                .module\n                .defined_table_index(segment.table_index)\n                .is_none()\n            {\n                leftovers.push(segment.clone());\n                continue;\n            }\n\n            // If this is not a funcref table, then we can't support a\n            // pre-computed table of function indices.\n            if self.module.table_plans[segment.table_index].table.wasm_ty != WasmType::FuncRef {\n                leftovers.push(segment.clone());\n                continue;\n            }\n\n            // If the base of this segment is dynamic, then we can't\n            // include it in the statically-built array of initial\n            // contents.\n            if segment.base.is_some() {\n                leftovers.push(segment.clone());\n                continue;\n            }\n\n            // Get the end of this segment. If out-of-bounds, or too\n            // large for our dense table representation, then skip the\n            // segment.\n            let top = match segment.offset.checked_add(segment.elements.len() as u32) {\n                Some(top) => top,\n                None => {\n                    leftovers.push(segment.clone());\n                    continue;\n                }\n            };\n            let table_size = self.module.table_plans[segment.table_index].table.minimum;\n            if top > table_size || top > MAX_FUNC_TABLE_SIZE {\n                leftovers.push(segment.clone());\n                continue;\n            }\n\n            // We can now incorporate this segment into the initializers array.\n            while tables.len() <= segment.table_index.index() {\n                tables.push(vec![]);\n            }\n            let elements = &mut tables[segment.table_index];\n            if elements.is_empty() {\n                elements.resize(table_size as usize, FuncIndex::reserved_value());\n            }\n\n            let dst = &mut elements[(segment.offset as usize)..(top as usize)];\n            dst.copy_from_slice(&segment.elements[..]);\n        }\n\n        self.module.table_initialization = TableInitialization::FuncTable {\n            tables,\n            segments: leftovers,\n        };\n    }\n}\n\nimpl Default for MemoryInitialization {\n    fn default() -> Self {\n        Self::Segmented(Vec::new())\n    }\n}\n\nimpl MemoryInitialization {\n    /// Returns whether this initialization is of the form\n    /// `MemoryInitialization::Segmented`.\n    pub fn is_segmented(&self) -> bool {\n        match self {\n            MemoryInitialization::Segmented(_) => true,\n            _ => false,\n        }\n    }\n\n    /// Performs the memory initialization steps for this set of initializers.\n    ///\n    /// This will perform wasm initialization in compliance with the wasm spec\n    /// and how data segments are processed. This doesn't need to necessarily\n    /// only be called as part of initialization, however, as it's structured to\n    /// allow learning about memory ahead-of-time at compile time possibly.\n    ///\n    /// The various callbacks provided here are used to drive the smaller bits\n    /// of initialization, such as:\n    ///\n    /// * `get_cur_size_in_pages` - gets the current size, in wasm pages, of the\n    ///   memory specified. For compile-time purposes this would be the memory\n    ///   type's minimum size.\n    ///\n    /// * `get_global` - gets the value of the global specified. This is\n    ///   statically, via validation, a pointer to the global of the correct\n    ///   type (either u32 or u64 depending on the memory), but the value\n    ///   returned here is `u64`. A `None` value can be returned to indicate\n    ///   that the global's value isn't known yet.\n    ///\n    /// * `write` - a callback used to actually write data. This indicates that\n    ///   the specified memory must receive the specified range of data at the\n    ///   specified offset. This can internally return an false error if it\n    ///   wants to fail.\n    ///\n    /// This function will return true if all memory initializers are processed\n    /// successfully. If any initializer hits an error or, for example, a\n    /// global value is needed but `None` is returned, then false will be\n    /// returned. At compile-time this typically means that the \"error\" in\n    /// question needs to be deferred to runtime, and at runtime this means\n    /// that an invalid initializer has been found and a trap should be\n    /// generated.\n    pub fn init_memory(\n        &self,\n        state: InitMemory<'_>,\n        write: &mut dyn FnMut(MemoryIndex, &StaticMemoryInitializer) -> bool,\n    ) -> bool {\n        let initializers = match self {\n            // Fall through below to the segmented memory one-by-one\n            // initialization.\n            MemoryInitialization::Segmented(list) => list,\n\n            // If previously switched to static initialization then pass through\n            // all those parameters here to the `write` callback.\n            //\n            // Note that existence of `Static` already guarantees that all\n            // indices are in-bounds.\n            MemoryInitialization::Static { map } => {\n                for (index, init) in map {\n                    if let Some(init) = init {\n                        let result = write(index, init);\n                        if !result {\n                            return result;\n                        }\n                    }\n                }\n                return true;\n            }\n        };\n\n        for initializer in initializers {\n            let MemoryInitializer {\n                memory_index,\n                base,\n                offset,\n                ref data,\n            } = *initializer;\n\n            // First up determine the start/end range and verify that they're\n            // in-bounds for the initial size of the memory at `memory_index`.\n            // Note that this can bail if we don't have access to globals yet\n            // (e.g. this is a task happening before instantiation at\n            // compile-time).\n            let base = match base {\n                Some(index) => match &state {\n                    InitMemory::Runtime {\n                        get_global_as_u64, ..\n                    } => get_global_as_u64(index),\n                    InitMemory::CompileTime(_) => return false,\n                },\n                None => 0,\n            };\n            let start = match base.checked_add(offset) {\n                Some(start) => start,\n                None => return false,\n            };\n            let len = u64::try_from(data.len()).unwrap();\n            let end = match start.checked_add(len) {\n                Some(end) => end,\n                None => return false,\n            };\n\n            let cur_size_in_pages = match &state {\n                InitMemory::CompileTime(module) => module.memory_plans[memory_index].memory.minimum,\n                InitMemory::Runtime {\n                    memory_size_in_pages,\n                    ..\n                } => memory_size_in_pages(memory_index),\n            };\n\n            // Note that this `minimum` can overflow if `minimum` is\n            // `1 << 48`, the maximum number of minimum pages for 64-bit\n            // memories. If this overflow happens, though, then there's no need\n            // to check the `end` value since `end` fits in a `u64` and it is\n            // naturally less than the overflowed value.\n            //\n            // This is a bit esoteric though because it's impossible to actually\n            // create a memory of `u64::MAX + 1` bytes, so this is largely just\n            // here to avoid having the multiplication here overflow in debug\n            // mode.\n            if let Some(max) = cur_size_in_pages.checked_mul(u64::from(WASM_PAGE_SIZE)) {\n                if end > max {\n                    return false;\n                }\n            }\n\n            // The limits of the data segment have been validated at this point\n            // so the `write` callback is called with the range of data being\n            // written. Any erroneous result is propagated upwards.\n            let init = StaticMemoryInitializer {\n                offset: start,\n                data: data.clone(),\n            };\n            let result = write(memory_index, &init);\n            if !result {\n                return result;\n            }\n        }\n\n        return true;\n    }\n}\n\n/// Argument to [`MemoryInitialization::init_memory`] indicating the current\n/// status of the instance.\npub enum InitMemory<'a> {\n    /// This evaluation of memory initializers is happening at compile time.\n    /// This means that the current state of memories is whatever their initial\n    /// state is, and additionally globals are not available if data segments\n    /// have global offsets.\n    CompileTime(&'a Module),\n\n    /// Evaluation of memory initializers is happening at runtime when the\n    /// instance is available, and callbacks are provided to learn about the\n    /// instance's state.\n    Runtime {\n        /// Returns the size, in wasm pages, of the the memory specified.\n        memory_size_in_pages: &'a dyn Fn(MemoryIndex) -> u64,\n        /// Returns the value of the global, as a `u64`. Note that this may\n        /// involve zero-extending a 32-bit global to a 64-bit number.\n        get_global_as_u64: &'a dyn Fn(GlobalIndex) -> u64,\n    },\n}\n\n/// Implementation styles for WebAssembly tables.\n#[derive(Debug, Clone, Hash, Serialize, Deserialize)]\npub enum TableStyle {\n    /// Signatures are stored in the table and checked in the caller.\n    CallerChecksSignature,\n}\n\nimpl TableStyle {\n    /// Decide on an implementation style for the given `Table`.\n    pub fn for_table(_table: Table, _tunables: &Tunables) -> Self {\n        Self::CallerChecksSignature\n    }\n}\n\n/// A WebAssembly table description along with our chosen style for\n/// implementing it.\n#[derive(Debug, Clone, Hash, Serialize, Deserialize)]\npub struct TablePlan {\n    /// The WebAssembly table description.\n    pub table: Table,\n    /// Our chosen implementation style.\n    pub style: TableStyle,\n}\n\nimpl TablePlan {\n    /// Draw up a plan for implementing a `Table`.\n    pub fn for_table(table: Table, tunables: &Tunables) -> Self {\n        let style = TableStyle::for_table(table, tunables);\n        Self { table, style }\n    }\n}\n\n/// A WebAssembly table initializer segment.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct TableInitializer {\n    /// The index of a table to initialize.\n    pub table_index: TableIndex,\n    /// Optionally, a global variable giving a base index.\n    pub base: Option<GlobalIndex>,\n    /// The offset to add to the base.\n    pub offset: u32,\n    /// The values to write into the table elements.\n    pub elements: Box<[FuncIndex]>,\n}\n\n/// Table initialization data for all tables in the module.\n#[derive(Debug, Serialize, Deserialize)]\npub enum TableInitialization {\n    /// \"Segment\" mode: table initializer segments, possibly with\n    /// dynamic bases, possibly applying to an imported memory.\n    ///\n    /// Every kind of table initialization is supported by the\n    /// Segments mode.\n    Segments {\n        /// The segment initializers. All apply to the table for which\n        /// this TableInitialization is specified.\n        segments: Vec<TableInitializer>,\n    },\n\n    /// \"FuncTable\" mode: a single array per table, with a function\n    /// index or null per slot. This is only possible to provide for a\n    /// given table when it is defined by the module itself, and can\n    /// only include data from initializer segments that have\n    /// statically-knowable bases (i.e., not dependent on global\n    /// values).\n    ///\n    /// Any segments that are not compatible with this mode are held\n    /// in the `segments` array of \"leftover segments\", which are\n    /// still processed eagerly.\n    ///\n    /// This mode facilitates lazy initialization of the tables. It is\n    /// thus \"nice to have\", but not necessary for correctness.\n    FuncTable {\n        /// For each table, an array of function indices (or\n        /// FuncIndex::reserved_value(), meaning no initialized value,\n        /// hence null by default). Array elements correspond\n        /// one-to-one to table elements; i.e., `elements[i]` is the\n        /// initial value for `table[i]`.\n        tables: PrimaryMap<TableIndex, Vec<FuncIndex>>,\n\n        /// Leftover segments that need to be processed eagerly on\n        /// instantiation. These either apply to an imported table (so\n        /// we can't pre-build a full image of the table from this\n        /// overlay) or have dynamically (at instantiation time)\n        /// determined bases.\n        segments: Vec<TableInitializer>,\n    },\n}\n\nimpl Default for TableInitialization {\n    fn default() -> Self {\n        TableInitialization::Segments { segments: vec![] }\n    }\n}\n\n/// Different types that can appear in a module.\n///\n/// Note that each of these variants are intended to index further into a\n/// separate table.\n#[derive(Debug, Copy, Clone, Serialize, Deserialize)]\n#[allow(missing_docs)]\npub enum ModuleType {\n    Function(SignatureIndex),\n}\n\nimpl ModuleType {\n    /// Asserts this is a `ModuleType::Function`, returning the underlying\n    /// `SignatureIndex`.\n    pub fn unwrap_function(&self) -> SignatureIndex {\n        match self {\n            ModuleType::Function(f) => *f,\n        }\n    }\n}\n\n/// A translated WebAssembly module, excluding the function bodies and\n/// memory initializers.\n#[derive(Default, Debug, Serialize, Deserialize)]\npub struct Module {\n    /// The name of this wasm module, often found in the wasm file.\n    pub name: Option<String>,\n\n    /// All import records, in the order they are declared in the module.\n    pub initializers: Vec<Initializer>,\n\n    /// Exported entities.\n    pub exports: IndexMap<String, EntityIndex>,\n\n    /// The module \"start\" function, if present.\n    pub start_func: Option<FuncIndex>,\n\n    /// WebAssembly table initialization data, per table.\n    pub table_initialization: TableInitialization,\n\n    /// WebAssembly linear memory initializer.\n    pub memory_initialization: MemoryInitialization,\n\n    /// WebAssembly passive elements.\n    pub passive_elements: Vec<Box<[FuncIndex]>>,\n\n    /// The map from passive element index (element segment index space) to index in `passive_elements`.\n    pub passive_elements_map: BTreeMap<ElemIndex, usize>,\n\n    /// The map from passive data index (data segment index space) to index in `passive_data`.\n    pub passive_data_map: BTreeMap<DataIndex, Range<u32>>,\n\n    /// Types declared in the wasm module.\n    pub types: PrimaryMap<TypeIndex, ModuleType>,\n\n    /// Number of imported or aliased functions in the module.\n    pub num_imported_funcs: usize,\n\n    /// Number of imported or aliased tables in the module.\n    pub num_imported_tables: usize,\n\n    /// Number of imported or aliased memories in the module.\n    pub num_imported_memories: usize,\n\n    /// Number of imported or aliased globals in the module.\n    pub num_imported_globals: usize,\n\n    /// Number of functions that \"escape\" from this module may need to have a\n    /// `VMCallerCheckedFuncRef` constructed for them.\n    ///\n    /// This is also the number of functions in the `functions` array below with\n    /// an `anyfunc` index (and is the maximum anyfunc index).\n    pub num_escaped_funcs: usize,\n\n    /// Types of functions, imported and local.\n    pub functions: PrimaryMap<FuncIndex, FunctionType>,\n\n    /// WebAssembly tables.\n    pub table_plans: PrimaryMap<TableIndex, TablePlan>,\n\n    /// WebAssembly linear memory plans.\n    pub memory_plans: PrimaryMap<MemoryIndex, MemoryPlan>,\n\n    /// WebAssembly global variables.\n    pub globals: PrimaryMap<GlobalIndex, Global>,\n}\n\n/// Initialization routines for creating an instance, encompassing imports,\n/// modules, instances, aliases, etc.\n#[derive(Debug, Serialize, Deserialize)]\npub enum Initializer {\n    /// An imported item is required to be provided.\n    Import {\n        /// Name of this import\n        name: String,\n        /// The field name projection of this import\n        field: String,\n        /// Where this import will be placed, which also has type information\n        /// about the import.\n        index: EntityIndex,\n    },\n}\n\nimpl Module {\n    /// Allocates the module data structures.\n    pub fn new() -> Self {\n        Module::default()\n    }\n\n    /// Convert a `DefinedFuncIndex` into a `FuncIndex`.\n    #[inline]\n    pub fn func_index(&self, defined_func: DefinedFuncIndex) -> FuncIndex {\n        FuncIndex::new(self.num_imported_funcs + defined_func.index())\n    }\n\n    /// Convert a `FuncIndex` into a `DefinedFuncIndex`. Returns None if the\n    /// index is an imported function.\n    #[inline]\n    pub fn defined_func_index(&self, func: FuncIndex) -> Option<DefinedFuncIndex> {\n        if func.index() < self.num_imported_funcs {\n            None\n        } else {\n            Some(DefinedFuncIndex::new(\n                func.index() - self.num_imported_funcs,\n            ))\n        }\n    }\n\n    /// Test whether the given function index is for an imported function.\n    #[inline]\n    pub fn is_imported_function(&self, index: FuncIndex) -> bool {\n        index.index() < self.num_imported_funcs\n    }\n\n    /// Convert a `DefinedTableIndex` into a `TableIndex`.\n    #[inline]\n    pub fn table_index(&self, defined_table: DefinedTableIndex) -> TableIndex {\n        TableIndex::new(self.num_imported_tables + defined_table.index())\n    }\n\n    /// Convert a `TableIndex` into a `DefinedTableIndex`. Returns None if the\n    /// index is an imported table.\n    #[inline]\n    pub fn defined_table_index(&self, table: TableIndex) -> Option<DefinedTableIndex> {\n        if table.index() < self.num_imported_tables {\n            None\n        } else {\n            Some(DefinedTableIndex::new(\n                table.index() - self.num_imported_tables,\n            ))\n        }\n    }\n\n    /// Test whether the given table index is for an imported table.\n    #[inline]\n    pub fn is_imported_table(&self, index: TableIndex) -> bool {\n        index.index() < self.num_imported_tables\n    }\n\n    /// Convert a `DefinedMemoryIndex` into a `MemoryIndex`.\n    #[inline]\n    pub fn memory_index(&self, defined_memory: DefinedMemoryIndex) -> MemoryIndex {\n        MemoryIndex::new(self.num_imported_memories + defined_memory.index())\n    }\n\n    /// Convert a `MemoryIndex` into a `DefinedMemoryIndex`. Returns None if the\n    /// index is an imported memory.\n    #[inline]\n    pub fn defined_memory_index(&self, memory: MemoryIndex) -> Option<DefinedMemoryIndex> {\n        if memory.index() < self.num_imported_memories {\n            None\n        } else {\n            Some(DefinedMemoryIndex::new(\n                memory.index() - self.num_imported_memories,\n            ))\n        }\n    }\n\n    /// Convert a `DefinedMemoryIndex` into an `OwnedMemoryIndex`. Returns None\n    /// if the index is an imported memory.\n    #[inline]\n    pub fn owned_memory_index(&self, memory: DefinedMemoryIndex) -> OwnedMemoryIndex {\n        assert!(\n            memory.index() < self.memory_plans.len(),\n            \"non-shared memory must have an owned index\"\n        );\n\n        // Once we know that the memory index is not greater than the number of\n        // plans, we can iterate through the plans up to the memory index and\n        // count how many are not shared (i.e., owned).\n        let owned_memory_index = self\n            .memory_plans\n            .iter()\n            .skip(self.num_imported_memories)\n            .take(memory.index())\n            .filter(|(_, mp)| !mp.memory.shared)\n            .count();\n        OwnedMemoryIndex::new(owned_memory_index)\n    }\n\n    /// Test whether the given memory index is for an imported memory.\n    #[inline]\n    pub fn is_imported_memory(&self, index: MemoryIndex) -> bool {\n        index.index() < self.num_imported_memories\n    }\n\n    /// Convert a `DefinedGlobalIndex` into a `GlobalIndex`.\n    #[inline]\n    pub fn global_index(&self, defined_global: DefinedGlobalIndex) -> GlobalIndex {\n        GlobalIndex::new(self.num_imported_globals + defined_global.index())\n    }\n\n    /// Convert a `GlobalIndex` into a `DefinedGlobalIndex`. Returns None if the\n    /// index is an imported global.\n    #[inline]\n    pub fn defined_global_index(&self, global: GlobalIndex) -> Option<DefinedGlobalIndex> {\n        if global.index() < self.num_imported_globals {\n            None\n        } else {\n            Some(DefinedGlobalIndex::new(\n                global.index() - self.num_imported_globals,\n            ))\n        }\n    }\n\n    /// Test whether the given global index is for an imported global.\n    #[inline]\n    pub fn is_imported_global(&self, index: GlobalIndex) -> bool {\n        index.index() < self.num_imported_globals\n    }\n\n    /// Returns an iterator of all the imports in this module, along with their\n    /// module name, field name, and type that's being imported.\n    pub fn imports(&self) -> impl ExactSizeIterator<Item = (&str, &str, EntityType)> {\n        self.initializers.iter().map(move |i| match i {\n            Initializer::Import { name, field, index } => {\n                (name.as_str(), field.as_str(), self.type_of(*index))\n            }\n        })\n    }\n\n    /// Returns the type of an item based on its index\n    pub fn type_of(&self, index: EntityIndex) -> EntityType {\n        match index {\n            EntityIndex::Global(i) => EntityType::Global(self.globals[i]),\n            EntityIndex::Table(i) => EntityType::Table(self.table_plans[i].table),\n            EntityIndex::Memory(i) => EntityType::Memory(self.memory_plans[i].memory),\n            EntityIndex::Function(i) => EntityType::Function(self.functions[i].signature),\n        }\n    }\n\n    /// Appends a new function to this module with the given type information,\n    /// used for functions that either don't escape or aren't certain whether\n    /// they escape yet.\n    pub fn push_function(&mut self, signature: SignatureIndex) -> FuncIndex {\n        self.functions.push(FunctionType {\n            signature,\n            anyfunc: AnyfuncIndex::reserved_value(),\n        })\n    }\n\n    /// Appends a new function to this module with the given type information.\n    pub fn push_escaped_function(\n        &mut self,\n        signature: SignatureIndex,\n        anyfunc: AnyfuncIndex,\n    ) -> FuncIndex {\n        self.functions.push(FunctionType { signature, anyfunc })\n    }\n}\n\n/// Type information about functions in a wasm module.\n#[derive(Debug, Serialize, Deserialize)]\npub struct FunctionType {\n    /// The type of this function, indexed into the module-wide type tables for\n    /// a module compilation.\n    pub signature: SignatureIndex,\n    /// The index into the anyfunc table, if present. Note that this is\n    /// `reserved_value()` if the function does not escape from a module.\n    pub anyfunc: AnyfuncIndex,\n}\n\nimpl FunctionType {\n    /// Returns whether this function's type is one that \"escapes\" the current\n    /// module, meaning that the function is exported, used in `ref.func`, used\n    /// in a table, etc.\n    pub fn is_escaping(&self) -> bool {\n        !self.anyfunc.is_reserved_value()\n    }\n}\n\n/// Index into the anyfunc table within a VMContext for a function.\n#[derive(Copy, Clone, PartialEq, Eq, Hash, PartialOrd, Ord, Debug, Serialize, Deserialize)]\npub struct AnyfuncIndex(u32);\ncranelift_entity::entity_impl!(AnyfuncIndex);\n", "//! An `Instance` contains all the runtime state used by execution of a\n//! wasm module (except its callstack and register state). An\n//! `InstanceHandle` is a reference-counting handle for an `Instance`.\n\nuse crate::export::Export;\nuse crate::externref::VMExternRefActivationsTable;\nuse crate::memory::{Memory, RuntimeMemoryCreator};\nuse crate::table::{Table, TableElement, TableElementType};\nuse crate::vmcontext::{\n    VMBuiltinFunctionsArray, VMCallerCheckedFuncRef, VMContext, VMFunctionImport,\n    VMGlobalDefinition, VMGlobalImport, VMMemoryDefinition, VMMemoryImport, VMOpaqueContext,\n    VMRuntimeLimits, VMTableDefinition, VMTableImport, VMCONTEXT_MAGIC,\n};\nuse crate::{\n    ExportFunction, ExportGlobal, ExportMemory, ExportTable, Imports, ModuleRuntimeInfo, Store,\n    VMFunctionBody, VMSharedSignatureIndex, WasmFault,\n};\nuse anyhow::Error;\nuse anyhow::Result;\nuse memoffset::offset_of;\nuse std::alloc::{self, Layout};\nuse std::any::Any;\nuse std::convert::TryFrom;\nuse std::hash::Hash;\nuse std::ops::Range;\nuse std::ptr::NonNull;\nuse std::sync::atomic::AtomicU64;\nuse std::sync::Arc;\nuse std::{mem, ptr};\nuse wasmtime_environ::{\n    packed_option::ReservedValue, DataIndex, DefinedGlobalIndex, DefinedMemoryIndex,\n    DefinedTableIndex, ElemIndex, EntityIndex, EntityRef, EntitySet, FuncIndex, GlobalIndex,\n    GlobalInit, HostPtr, MemoryIndex, Module, PrimaryMap, SignatureIndex, TableIndex,\n    TableInitialization, Trap, VMOffsets, WasmType,\n};\n\nmod allocator;\n\npub use allocator::*;\n\n/// A type that roughly corresponds to a WebAssembly instance, but is also used\n/// for host-defined objects.\n///\n/// This structure is is never allocated directly but is instead managed through\n/// an `InstanceHandle`. This structure ends with a `VMContext` which has a\n/// dynamic size corresponding to the `module` configured within. Memory\n/// management of this structure is always externalized.\n///\n/// Instances here can correspond to actual instantiated modules, but it's also\n/// used ubiquitously for host-defined objects. For example creating a\n/// host-defined memory will have a `module` that looks like it exports a single\n/// memory (and similar for other constructs).\n///\n/// This `Instance` type is used as a ubiquitous representation for WebAssembly\n/// values, whether or not they were created on the host or through a module.\n#[repr(C)] // ensure that the vmctx field is last.\npub(crate) struct Instance {\n    /// The runtime info (corresponding to the \"compiled module\"\n    /// abstraction in higher layers) that is retained and needed for\n    /// lazy initialization. This provides access to the underlying\n    /// Wasm module entities, the compiled JIT code, metadata about\n    /// functions, lazy initialization state, etc.\n    runtime_info: Arc<dyn ModuleRuntimeInfo>,\n\n    /// WebAssembly linear memory data.\n    ///\n    /// This is where all runtime information about defined linear memories in\n    /// this module lives.\n    memories: PrimaryMap<DefinedMemoryIndex, Memory>,\n\n    /// WebAssembly table data.\n    ///\n    /// Like memories, this is only for defined tables in the module and\n    /// contains all of their runtime state.\n    tables: PrimaryMap<DefinedTableIndex, Table>,\n\n    /// Stores the dropped passive element segments in this instantiation by index.\n    /// If the index is present in the set, the segment has been dropped.\n    dropped_elements: EntitySet<ElemIndex>,\n\n    /// Stores the dropped passive data segments in this instantiation by index.\n    /// If the index is present in the set, the segment has been dropped.\n    dropped_data: EntitySet<DataIndex>,\n\n    /// Hosts can store arbitrary per-instance information here.\n    ///\n    /// Most of the time from Wasmtime this is `Box::new(())`, a noop\n    /// allocation, but some host-defined objects will store their state here.\n    host_state: Box<dyn Any + Send + Sync>,\n\n    /// Instance of this instance within its `InstanceAllocator` trait\n    /// implementation.\n    ///\n    /// This is always 0 for the on-demand instance allocator and it's the\n    /// index of the slot in the pooling allocator.\n    index: usize,\n\n    /// Additional context used by compiled wasm code. This field is last, and\n    /// represents a dynamically-sized array that extends beyond the nominal\n    /// end of the struct (similar to a flexible array member).\n    vmctx: VMContext,\n}\n\n#[allow(clippy::cast_ptr_alignment)]\nimpl Instance {\n    /// Create an instance at the given memory address.\n    ///\n    /// It is assumed the memory was properly aligned and the\n    /// allocation was `alloc_size` in bytes.\n    unsafe fn new(\n        req: InstanceAllocationRequest,\n        index: usize,\n        memories: PrimaryMap<DefinedMemoryIndex, Memory>,\n        tables: PrimaryMap<DefinedTableIndex, Table>,\n    ) -> InstanceHandle {\n        // The allocation must be *at least* the size required of `Instance`.\n        let layout = Self::alloc_layout(req.runtime_info.offsets());\n        let ptr = alloc::alloc(layout);\n        if ptr.is_null() {\n            alloc::handle_alloc_error(layout);\n        }\n        let ptr = ptr.cast::<Instance>();\n\n        let module = req.runtime_info.module();\n        let dropped_elements = EntitySet::with_capacity(module.passive_elements.len());\n        let dropped_data = EntitySet::with_capacity(module.passive_data_map.len());\n\n        ptr::write(\n            ptr,\n            Instance {\n                runtime_info: req.runtime_info.clone(),\n                index,\n                memories,\n                tables,\n                dropped_elements,\n                dropped_data,\n                host_state: req.host_state,\n                vmctx: VMContext {\n                    _marker: std::marker::PhantomPinned,\n                },\n            },\n        );\n\n        (*ptr).initialize_vmctx(module, req.runtime_info.offsets(), req.store, req.imports);\n        InstanceHandle { instance: ptr }\n    }\n\n    /// Helper function to access various locations offset from our `*mut\n    /// VMContext` object.\n    unsafe fn vmctx_plus_offset<T>(&self, offset: u32) -> *mut T {\n        (self.vmctx_ptr().cast::<u8>())\n            .add(usize::try_from(offset).unwrap())\n            .cast()\n    }\n\n    pub(crate) fn module(&self) -> &Arc<Module> {\n        self.runtime_info.module()\n    }\n\n    fn offsets(&self) -> &VMOffsets<HostPtr> {\n        self.runtime_info.offsets()\n    }\n\n    /// Return the indexed `VMFunctionImport`.\n    fn imported_function(&self, index: FuncIndex) -> &VMFunctionImport {\n        unsafe { &*self.vmctx_plus_offset(self.offsets().vmctx_vmfunction_import(index)) }\n    }\n\n    /// Return the index `VMTableImport`.\n    fn imported_table(&self, index: TableIndex) -> &VMTableImport {\n        unsafe { &*self.vmctx_plus_offset(self.offsets().vmctx_vmtable_import(index)) }\n    }\n\n    /// Return the indexed `VMMemoryImport`.\n    fn imported_memory(&self, index: MemoryIndex) -> &VMMemoryImport {\n        unsafe { &*self.vmctx_plus_offset(self.offsets().vmctx_vmmemory_import(index)) }\n    }\n\n    /// Return the indexed `VMGlobalImport`.\n    fn imported_global(&self, index: GlobalIndex) -> &VMGlobalImport {\n        unsafe { &*self.vmctx_plus_offset(self.offsets().vmctx_vmglobal_import(index)) }\n    }\n\n    /// Return the indexed `VMTableDefinition`.\n    #[allow(dead_code)]\n    fn table(&self, index: DefinedTableIndex) -> VMTableDefinition {\n        unsafe { *self.table_ptr(index) }\n    }\n\n    /// Updates the value for a defined table to `VMTableDefinition`.\n    fn set_table(&self, index: DefinedTableIndex, table: VMTableDefinition) {\n        unsafe {\n            *self.table_ptr(index) = table;\n        }\n    }\n\n    /// Return the indexed `VMTableDefinition`.\n    fn table_ptr(&self, index: DefinedTableIndex) -> *mut VMTableDefinition {\n        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_vmtable_definition(index)) }\n    }\n\n    /// Get a locally defined or imported memory.\n    pub(crate) fn get_memory(&self, index: MemoryIndex) -> VMMemoryDefinition {\n        if let Some(defined_index) = self.module().defined_memory_index(index) {\n            self.memory(defined_index)\n        } else {\n            let import = self.imported_memory(index);\n            unsafe { VMMemoryDefinition::load(import.from) }\n        }\n    }\n\n    /// Get a locally defined or imported memory.\n    pub(crate) fn get_runtime_memory(&mut self, index: MemoryIndex) -> &mut Memory {\n        if let Some(defined_index) = self.module().defined_memory_index(index) {\n            unsafe { &mut *self.get_defined_memory(defined_index) }\n        } else {\n            let import = self.imported_memory(index);\n            let ctx = unsafe { &mut *import.vmctx };\n            unsafe { &mut *ctx.instance_mut().get_defined_memory(import.index) }\n        }\n    }\n\n    /// Return the indexed `VMMemoryDefinition`.\n    fn memory(&self, index: DefinedMemoryIndex) -> VMMemoryDefinition {\n        unsafe { VMMemoryDefinition::load(self.memory_ptr(index)) }\n    }\n\n    /// Set the indexed memory to `VMMemoryDefinition`.\n    fn set_memory(&self, index: DefinedMemoryIndex, mem: VMMemoryDefinition) {\n        unsafe {\n            *self.memory_ptr(index) = mem;\n        }\n    }\n\n    /// Return the indexed `VMMemoryDefinition`.\n    fn memory_ptr(&self, index: DefinedMemoryIndex) -> *mut VMMemoryDefinition {\n        unsafe { *self.vmctx_plus_offset(self.offsets().vmctx_vmmemory_pointer(index)) }\n    }\n\n    /// Return the indexed `VMGlobalDefinition`.\n    fn global(&self, index: DefinedGlobalIndex) -> &VMGlobalDefinition {\n        unsafe { &*self.global_ptr(index) }\n    }\n\n    /// Return the indexed `VMGlobalDefinition`.\n    fn global_ptr(&self, index: DefinedGlobalIndex) -> *mut VMGlobalDefinition {\n        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_vmglobal_definition(index)) }\n    }\n\n    /// Get a raw pointer to the global at the given index regardless whether it\n    /// is defined locally or imported from another module.\n    ///\n    /// Panics if the index is out of bound or is the reserved value.\n    pub(crate) fn defined_or_imported_global_ptr(\n        &self,\n        index: GlobalIndex,\n    ) -> *mut VMGlobalDefinition {\n        if let Some(index) = self.module().defined_global_index(index) {\n            self.global_ptr(index)\n        } else {\n            self.imported_global(index).from\n        }\n    }\n\n    /// Return a pointer to the interrupts structure\n    pub fn runtime_limits(&self) -> *mut *const VMRuntimeLimits {\n        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_runtime_limits()) }\n    }\n\n    /// Return a pointer to the global epoch counter used by this instance.\n    pub fn epoch_ptr(&self) -> *mut *const AtomicU64 {\n        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_epoch_ptr()) }\n    }\n\n    /// Return a pointer to the `VMExternRefActivationsTable`.\n    pub fn externref_activations_table(&self) -> *mut *mut VMExternRefActivationsTable {\n        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_externref_activations_table()) }\n    }\n\n    /// Gets a pointer to this instance's `Store` which was originally\n    /// configured on creation.\n    ///\n    /// # Panics\n    ///\n    /// This will panic if the originally configured store was `None`. That can\n    /// happen for host functions so host functions can't be queried what their\n    /// original `Store` was since it's just retained as null (since host\n    /// functions are shared amongst threads and don't all share the same\n    /// store).\n    #[inline]\n    pub fn store(&self) -> *mut dyn Store {\n        let ptr =\n            unsafe { *self.vmctx_plus_offset::<*mut dyn Store>(self.offsets().vmctx_store()) };\n        assert!(!ptr.is_null());\n        ptr\n    }\n\n    pub unsafe fn set_store(&mut self, store: Option<*mut dyn Store>) {\n        if let Some(store) = store {\n            *self.vmctx_plus_offset(self.offsets().vmctx_store()) = store;\n            *self.runtime_limits() = (*store).vmruntime_limits();\n            *self.epoch_ptr() = (*store).epoch_ptr();\n            *self.externref_activations_table() = (*store).externref_activations_table().0;\n        } else {\n            assert_eq!(\n                mem::size_of::<*mut dyn Store>(),\n                mem::size_of::<[*mut (); 2]>()\n            );\n            *self.vmctx_plus_offset::<[*mut (); 2]>(self.offsets().vmctx_store()) =\n                [ptr::null_mut(), ptr::null_mut()];\n\n            *self.runtime_limits() = ptr::null_mut();\n            *self.epoch_ptr() = ptr::null_mut();\n            *self.externref_activations_table() = ptr::null_mut();\n        }\n    }\n\n    pub(crate) unsafe fn set_callee(&mut self, callee: Option<NonNull<VMFunctionBody>>) {\n        *self.vmctx_plus_offset(self.offsets().vmctx_callee()) =\n            callee.map_or(ptr::null_mut(), |c| c.as_ptr());\n    }\n\n    /// Return a reference to the vmctx used by compiled wasm code.\n    #[inline]\n    pub fn vmctx(&self) -> &VMContext {\n        &self.vmctx\n    }\n\n    /// Return a raw pointer to the vmctx used by compiled wasm code.\n    #[inline]\n    pub fn vmctx_ptr(&self) -> *mut VMContext {\n        self.vmctx() as *const VMContext as *mut VMContext\n    }\n\n    fn get_exported_func(&mut self, index: FuncIndex) -> ExportFunction {\n        let anyfunc = self.get_caller_checked_anyfunc(index).unwrap();\n        let anyfunc = NonNull::new(anyfunc as *const VMCallerCheckedFuncRef as *mut _).unwrap();\n        ExportFunction { anyfunc }\n    }\n\n    fn get_exported_table(&mut self, index: TableIndex) -> ExportTable {\n        let (definition, vmctx) = if let Some(def_index) = self.module().defined_table_index(index)\n        {\n            (self.table_ptr(def_index), self.vmctx_ptr())\n        } else {\n            let import = self.imported_table(index);\n            (import.from, import.vmctx)\n        };\n        ExportTable {\n            definition,\n            vmctx,\n            table: self.module().table_plans[index].clone(),\n        }\n    }\n\n    fn get_exported_memory(&mut self, index: MemoryIndex) -> ExportMemory {\n        let (definition, vmctx, def_index) =\n            if let Some(def_index) = self.module().defined_memory_index(index) {\n                (self.memory_ptr(def_index), self.vmctx_ptr(), def_index)\n            } else {\n                let import = self.imported_memory(index);\n                (import.from, import.vmctx, import.index)\n            };\n        ExportMemory {\n            definition,\n            vmctx,\n            memory: self.module().memory_plans[index].clone(),\n            index: def_index,\n        }\n    }\n\n    fn get_exported_global(&mut self, index: GlobalIndex) -> ExportGlobal {\n        ExportGlobal {\n            definition: if let Some(def_index) = self.module().defined_global_index(index) {\n                self.global_ptr(def_index)\n            } else {\n                self.imported_global(index).from\n            },\n            global: self.module().globals[index],\n        }\n    }\n\n    /// Return an iterator over the exports of this instance.\n    ///\n    /// Specifically, it provides access to the key-value pairs, where the keys\n    /// are export names, and the values are export declarations which can be\n    /// resolved `lookup_by_declaration`.\n    pub fn exports(&self) -> indexmap::map::Iter<String, EntityIndex> {\n        self.module().exports.iter()\n    }\n\n    /// Return a reference to the custom state attached to this instance.\n    #[inline]\n    pub fn host_state(&self) -> &dyn Any {\n        &*self.host_state\n    }\n\n    /// Return the offset from the vmctx pointer to its containing Instance.\n    #[inline]\n    pub(crate) fn vmctx_offset() -> isize {\n        offset_of!(Self, vmctx) as isize\n    }\n\n    /// Return the table index for the given `VMTableDefinition`.\n    unsafe fn table_index(&self, table: &VMTableDefinition) -> DefinedTableIndex {\n        let index = DefinedTableIndex::new(\n            usize::try_from(\n                (table as *const VMTableDefinition)\n                    .offset_from(self.table_ptr(DefinedTableIndex::new(0))),\n            )\n            .unwrap(),\n        );\n        assert!(index.index() < self.tables.len());\n        index\n    }\n\n    /// Grow memory by the specified amount of pages.\n    ///\n    /// Returns `None` if memory can't be grown by the specified amount\n    /// of pages. Returns `Some` with the old size in bytes if growth was\n    /// successful.\n    pub(crate) fn memory_grow(\n        &mut self,\n        index: MemoryIndex,\n        delta: u64,\n    ) -> Result<Option<usize>, Error> {\n        let (idx, instance) = if let Some(idx) = self.module().defined_memory_index(index) {\n            (idx, self)\n        } else {\n            let import = self.imported_memory(index);\n            unsafe {\n                let foreign_instance = (*import.vmctx).instance_mut();\n                (import.index, foreign_instance)\n            }\n        };\n        let store = unsafe { &mut *instance.store() };\n        let memory = &mut instance.memories[idx];\n\n        let result = unsafe { memory.grow(delta, Some(store)) };\n\n        // Update the state used by a non-shared Wasm memory in case the base\n        // pointer and/or the length changed.\n        if memory.as_shared_memory().is_none() {\n            let vmmemory = memory.vmmemory();\n            instance.set_memory(idx, vmmemory);\n        }\n\n        result\n    }\n\n    pub(crate) fn table_element_type(&mut self, table_index: TableIndex) -> TableElementType {\n        unsafe { (*self.get_table(table_index)).element_type() }\n    }\n\n    /// Grow table by the specified amount of elements, filling them with\n    /// `init_value`.\n    ///\n    /// Returns `None` if table can't be grown by the specified amount of\n    /// elements, or if `init_value` is the wrong type of table element.\n    pub(crate) fn table_grow(\n        &mut self,\n        table_index: TableIndex,\n        delta: u32,\n        init_value: TableElement,\n    ) -> Result<Option<u32>, Error> {\n        let (defined_table_index, instance) =\n            self.get_defined_table_index_and_instance(table_index);\n        instance.defined_table_grow(defined_table_index, delta, init_value)\n    }\n\n    fn defined_table_grow(\n        &mut self,\n        table_index: DefinedTableIndex,\n        delta: u32,\n        init_value: TableElement,\n    ) -> Result<Option<u32>, Error> {\n        let store = unsafe { &mut *self.store() };\n        let table = self\n            .tables\n            .get_mut(table_index)\n            .unwrap_or_else(|| panic!(\"no table for index {}\", table_index.index()));\n\n        let result = unsafe { table.grow(delta, init_value, store) };\n\n        // Keep the `VMContext` pointers used by compiled Wasm code up to\n        // date.\n        let element = self.tables[table_index].vmtable();\n        self.set_table(table_index, element);\n\n        result\n    }\n\n    fn alloc_layout(offsets: &VMOffsets<HostPtr>) -> Layout {\n        let size = mem::size_of::<Self>()\n            .checked_add(usize::try_from(offsets.size_of_vmctx()).unwrap())\n            .unwrap();\n        let align = mem::align_of::<Self>();\n        Layout::from_size_align(size, align).unwrap()\n    }\n\n    /// Construct a new VMCallerCheckedFuncRef for the given function\n    /// (imported or defined in this module) and store into the given\n    /// location. Used during lazy initialization.\n    ///\n    /// Note that our current lazy-init scheme actually calls this every\n    /// time the anyfunc pointer is fetched; this turns out to be better\n    /// than tracking state related to whether it's been initialized\n    /// before, because resetting that state on (re)instantiation is\n    /// very expensive if there are many anyfuncs.\n    fn construct_anyfunc(\n        &mut self,\n        index: FuncIndex,\n        sig: SignatureIndex,\n        into: *mut VMCallerCheckedFuncRef,\n    ) {\n        let type_index = unsafe {\n            let base: *const VMSharedSignatureIndex =\n                *self.vmctx_plus_offset(self.offsets().vmctx_signature_ids_array());\n            *base.add(sig.index())\n        };\n\n        let (func_ptr, vmctx) = if let Some(def_index) = self.module().defined_func_index(index) {\n            (\n                self.runtime_info.function(def_index),\n                VMOpaqueContext::from_vmcontext(self.vmctx_ptr()),\n            )\n        } else {\n            let import = self.imported_function(index);\n            (import.body.as_ptr(), import.vmctx)\n        };\n\n        // Safety: we have a `&mut self`, so we have exclusive access\n        // to this Instance.\n        unsafe {\n            *into = VMCallerCheckedFuncRef {\n                vmctx,\n                type_index,\n                func_ptr: NonNull::new(func_ptr).expect(\"Non-null function pointer\"),\n            };\n        }\n    }\n\n    /// Get a `&VMCallerCheckedFuncRef` for the given `FuncIndex`.\n    ///\n    /// Returns `None` if the index is the reserved index value.\n    ///\n    /// The returned reference is a stable reference that won't be moved and can\n    /// be passed into JIT code.\n    pub(crate) fn get_caller_checked_anyfunc(\n        &mut self,\n        index: FuncIndex,\n    ) -> Option<*mut VMCallerCheckedFuncRef> {\n        if index == FuncIndex::reserved_value() {\n            return None;\n        }\n\n        // Safety: we have a `&mut self`, so we have exclusive access\n        // to this Instance.\n        unsafe {\n            // For now, we eagerly initialize an anyfunc struct in-place\n            // whenever asked for a reference to it. This is mostly\n            // fine, because in practice each anyfunc is unlikely to be\n            // requested more than a few times: once-ish for funcref\n            // tables used for call_indirect (the usual compilation\n            // strategy places each function in the table at most once),\n            // and once or a few times when fetching exports via API.\n            // Note that for any case driven by table accesses, the lazy\n            // table init behaves like a higher-level cache layer that\n            // protects this initialization from happening multiple\n            // times, via that particular table at least.\n            //\n            // When `ref.func` becomes more commonly used or if we\n            // otherwise see a use-case where this becomes a hotpath,\n            // we can reconsider by using some state to track\n            // \"uninitialized\" explicitly, for example by zeroing the\n            // anyfuncs (perhaps together with other\n            // zeroed-at-instantiate-time state) or using a separate\n            // is-initialized bitmap.\n            //\n            // We arrived at this design because zeroing memory is\n            // expensive, so it's better for instantiation performance\n            // if we don't have to track \"is-initialized\" state at\n            // all!\n            let func = &self.module().functions[index];\n            let sig = func.signature;\n            let anyfunc: *mut VMCallerCheckedFuncRef = self\n                .vmctx_plus_offset::<VMCallerCheckedFuncRef>(\n                    self.offsets().vmctx_anyfunc(func.anyfunc),\n                );\n            self.construct_anyfunc(index, sig, anyfunc);\n\n            Some(anyfunc)\n        }\n    }\n\n    /// The `table.init` operation: initializes a portion of a table with a\n    /// passive element.\n    ///\n    /// # Errors\n    ///\n    /// Returns a `Trap` error when the range within the table is out of bounds\n    /// or the range within the passive element is out of bounds.\n    pub(crate) fn table_init(\n        &mut self,\n        table_index: TableIndex,\n        elem_index: ElemIndex,\n        dst: u32,\n        src: u32,\n        len: u32,\n    ) -> Result<(), Trap> {\n        // TODO: this `clone()` shouldn't be necessary but is used for now to\n        // inform `rustc` that the lifetime of the elements here are\n        // disconnected from the lifetime of `self`.\n        let module = self.module().clone();\n\n        let elements = match module.passive_elements_map.get(&elem_index) {\n            Some(index) if !self.dropped_elements.contains(elem_index) => {\n                module.passive_elements[*index].as_ref()\n            }\n            _ => &[],\n        };\n        self.table_init_segment(table_index, elements, dst, src, len)\n    }\n\n    pub(crate) fn table_init_segment(\n        &mut self,\n        table_index: TableIndex,\n        elements: &[FuncIndex],\n        dst: u32,\n        src: u32,\n        len: u32,\n    ) -> Result<(), Trap> {\n        // https://webassembly.github.io/bulk-memory-operations/core/exec/instructions.html#exec-table-init\n\n        let table = unsafe { &mut *self.get_table(table_index) };\n\n        let elements = match elements\n            .get(usize::try_from(src).unwrap()..)\n            .and_then(|s| s.get(..usize::try_from(len).unwrap()))\n        {\n            Some(elements) => elements,\n            None => return Err(Trap::TableOutOfBounds),\n        };\n\n        match table.element_type() {\n            TableElementType::Func => {\n                table.init_funcs(\n                    dst,\n                    elements.iter().map(|idx| {\n                        self.get_caller_checked_anyfunc(*idx)\n                            .unwrap_or(std::ptr::null_mut())\n                    }),\n                )?;\n            }\n\n            TableElementType::Extern => {\n                debug_assert!(elements.iter().all(|e| *e == FuncIndex::reserved_value()));\n                table.fill(dst, TableElement::ExternRef(None), len)?;\n            }\n        }\n        Ok(())\n    }\n\n    /// Drop an element.\n    pub(crate) fn elem_drop(&mut self, elem_index: ElemIndex) {\n        // https://webassembly.github.io/reference-types/core/exec/instructions.html#exec-elem-drop\n\n        self.dropped_elements.insert(elem_index);\n\n        // Note that we don't check that we actually removed a segment because\n        // dropping a non-passive segment is a no-op (not a trap).\n    }\n\n    /// Get a locally-defined memory.\n    pub(crate) fn get_defined_memory(&mut self, index: DefinedMemoryIndex) -> *mut Memory {\n        ptr::addr_of_mut!(self.memories[index])\n    }\n\n    /// Do a `memory.copy`\n    ///\n    /// # Errors\n    ///\n    /// Returns a `Trap` error when the source or destination ranges are out of\n    /// bounds.\n    pub(crate) fn memory_copy(\n        &mut self,\n        dst_index: MemoryIndex,\n        dst: u64,\n        src_index: MemoryIndex,\n        src: u64,\n        len: u64,\n    ) -> Result<(), Trap> {\n        // https://webassembly.github.io/reference-types/core/exec/instructions.html#exec-memory-copy\n\n        let src_mem = self.get_memory(src_index);\n        let dst_mem = self.get_memory(dst_index);\n\n        let src = self.validate_inbounds(src_mem.current_length(), src, len)?;\n        let dst = self.validate_inbounds(dst_mem.current_length(), dst, len)?;\n\n        // Bounds and casts are checked above, by this point we know that\n        // everything is safe.\n        unsafe {\n            let dst = dst_mem.base.add(dst);\n            let src = src_mem.base.add(src);\n            // FIXME audit whether this is safe in the presence of shared memory\n            // (https://github.com/bytecodealliance/wasmtime/issues/4203).\n            ptr::copy(src, dst, len as usize);\n        }\n\n        Ok(())\n    }\n\n    fn validate_inbounds(&self, max: usize, ptr: u64, len: u64) -> Result<usize, Trap> {\n        let oob = || Trap::MemoryOutOfBounds;\n        let end = ptr\n            .checked_add(len)\n            .and_then(|i| usize::try_from(i).ok())\n            .ok_or_else(oob)?;\n        if end > max {\n            Err(oob())\n        } else {\n            Ok(ptr as usize)\n        }\n    }\n\n    /// Perform the `memory.fill` operation on a locally defined memory.\n    ///\n    /// # Errors\n    ///\n    /// Returns a `Trap` error if the memory range is out of bounds.\n    pub(crate) fn memory_fill(\n        &mut self,\n        memory_index: MemoryIndex,\n        dst: u64,\n        val: u8,\n        len: u64,\n    ) -> Result<(), Trap> {\n        let memory = self.get_memory(memory_index);\n        let dst = self.validate_inbounds(memory.current_length(), dst, len)?;\n\n        // Bounds and casts are checked above, by this point we know that\n        // everything is safe.\n        unsafe {\n            let dst = memory.base.add(dst);\n            // FIXME audit whether this is safe in the presence of shared memory\n            // (https://github.com/bytecodealliance/wasmtime/issues/4203).\n            ptr::write_bytes(dst, val, len as usize);\n        }\n\n        Ok(())\n    }\n\n    /// Performs the `memory.init` operation.\n    ///\n    /// # Errors\n    ///\n    /// Returns a `Trap` error if the destination range is out of this module's\n    /// memory's bounds or if the source range is outside the data segment's\n    /// bounds.\n    pub(crate) fn memory_init(\n        &mut self,\n        memory_index: MemoryIndex,\n        data_index: DataIndex,\n        dst: u64,\n        src: u32,\n        len: u32,\n    ) -> Result<(), Trap> {\n        let range = match self.module().passive_data_map.get(&data_index).cloned() {\n            Some(range) if !self.dropped_data.contains(data_index) => range,\n            _ => 0..0,\n        };\n        self.memory_init_segment(memory_index, range, dst, src, len)\n    }\n\n    pub(crate) fn wasm_data(&self, range: Range<u32>) -> &[u8] {\n        &self.runtime_info.wasm_data()[range.start as usize..range.end as usize]\n    }\n\n    pub(crate) fn memory_init_segment(\n        &mut self,\n        memory_index: MemoryIndex,\n        range: Range<u32>,\n        dst: u64,\n        src: u32,\n        len: u32,\n    ) -> Result<(), Trap> {\n        // https://webassembly.github.io/bulk-memory-operations/core/exec/instructions.html#exec-memory-init\n\n        let memory = self.get_memory(memory_index);\n        let data = self.wasm_data(range);\n        let dst = self.validate_inbounds(memory.current_length(), dst, len.into())?;\n        let src = self.validate_inbounds(data.len(), src.into(), len.into())?;\n        let len = len as usize;\n\n        unsafe {\n            let src_start = data.as_ptr().add(src);\n            let dst_start = memory.base.add(dst);\n            // FIXME audit whether this is safe in the presence of shared memory\n            // (https://github.com/bytecodealliance/wasmtime/issues/4203).\n            ptr::copy_nonoverlapping(src_start, dst_start, len);\n        }\n\n        Ok(())\n    }\n\n    /// Drop the given data segment, truncating its length to zero.\n    pub(crate) fn data_drop(&mut self, data_index: DataIndex) {\n        self.dropped_data.insert(data_index);\n\n        // Note that we don't check that we actually removed a segment because\n        // dropping a non-passive segment is a no-op (not a trap).\n    }\n\n    /// Get a table by index regardless of whether it is locally-defined\n    /// or an imported, foreign table. Ensure that the given range of\n    /// elements in the table is lazily initialized.  We define this\n    /// operation all-in-one for safety, to ensure the lazy-init\n    /// happens.\n    ///\n    /// Takes an `Iterator` for the index-range to lazy-initialize,\n    /// for flexibility. This can be a range, single item, or empty\n    /// sequence, for example. The iterator should return indices in\n    /// increasing order, so that the break-at-out-of-bounds behavior\n    /// works correctly.\n    pub(crate) fn get_table_with_lazy_init(\n        &mut self,\n        table_index: TableIndex,\n        range: impl Iterator<Item = u32>,\n    ) -> *mut Table {\n        let (idx, instance) = self.get_defined_table_index_and_instance(table_index);\n        let elt_ty = instance.tables[idx].element_type();\n\n        if elt_ty == TableElementType::Func {\n            for i in range {\n                let value = match instance.tables[idx].get(i) {\n                    Some(value) => value,\n                    None => {\n                        // Out-of-bounds; caller will handle by likely\n                        // throwing a trap. No work to do to lazy-init\n                        // beyond the end.\n                        break;\n                    }\n                };\n                if value.is_uninit() {\n                    let table_init = match &instance.module().table_initialization {\n                        // We unfortunately can't borrow `tables`\n                        // outside the loop because we need to call\n                        // `get_caller_checked_anyfunc` (a `&mut`\n                        // method) below; so unwrap it dynamically\n                        // here.\n                        TableInitialization::FuncTable { tables, .. } => tables,\n                        _ => break,\n                    }\n                    .get(table_index);\n\n                    // The TableInitialization::FuncTable elements table may\n                    // be smaller than the current size of the table: it\n                    // always matches the initial table size, if present. We\n                    // want to iterate up through the end of the accessed\n                    // index range so that we set an \"initialized null\" even\n                    // if there is no initializer. We do a checked `get()` on\n                    // the initializer table below and unwrap to a null if\n                    // we're past its end.\n                    let func_index =\n                        table_init.and_then(|indices| indices.get(i as usize).cloned());\n                    let anyfunc = func_index\n                        .and_then(|func_index| instance.get_caller_checked_anyfunc(func_index))\n                        .unwrap_or(std::ptr::null_mut());\n\n                    let value = TableElement::FuncRef(anyfunc);\n\n                    instance.tables[idx]\n                        .set(i, value)\n                        .expect(\"Table type should match and index should be in-bounds\");\n                }\n            }\n        }\n\n        ptr::addr_of_mut!(instance.tables[idx])\n    }\n\n    /// Get a table by index regardless of whether it is locally-defined or an\n    /// imported, foreign table.\n    pub(crate) fn get_table(&mut self, table_index: TableIndex) -> *mut Table {\n        let (idx, instance) = self.get_defined_table_index_and_instance(table_index);\n        ptr::addr_of_mut!(instance.tables[idx])\n    }\n\n    /// Get a locally-defined table.\n    pub(crate) fn get_defined_table(&mut self, index: DefinedTableIndex) -> *mut Table {\n        ptr::addr_of_mut!(self.tables[index])\n    }\n\n    pub(crate) fn get_defined_table_index_and_instance(\n        &mut self,\n        index: TableIndex,\n    ) -> (DefinedTableIndex, &mut Instance) {\n        if let Some(defined_table_index) = self.module().defined_table_index(index) {\n            (defined_table_index, self)\n        } else {\n            let import = self.imported_table(index);\n            unsafe {\n                let foreign_instance = (*import.vmctx).instance_mut();\n                let foreign_table_def = &*import.from;\n                let foreign_table_index = foreign_instance.table_index(foreign_table_def);\n                (foreign_table_index, foreign_instance)\n            }\n        }\n    }\n\n    /// Initialize the VMContext data associated with this Instance.\n    ///\n    /// The `VMContext` memory is assumed to be uninitialized; any field\n    /// that we need in a certain state will be explicitly written by this\n    /// function.\n    unsafe fn initialize_vmctx(\n        &mut self,\n        module: &Module,\n        offsets: &VMOffsets<HostPtr>,\n        store: StorePtr,\n        imports: Imports,\n    ) {\n        assert!(std::ptr::eq(module, self.module().as_ref()));\n\n        *self.vmctx_plus_offset(offsets.vmctx_magic()) = VMCONTEXT_MAGIC;\n        self.set_callee(None);\n        self.set_store(store.as_raw());\n\n        // Initialize shared signatures\n        let signatures = self.runtime_info.signature_ids();\n        *self.vmctx_plus_offset(offsets.vmctx_signature_ids_array()) = signatures.as_ptr();\n\n        // Initialize the built-in functions\n        *self.vmctx_plus_offset(offsets.vmctx_builtin_functions()) = &VMBuiltinFunctionsArray::INIT;\n\n        // Initialize the imports\n        debug_assert_eq!(imports.functions.len(), module.num_imported_funcs);\n        ptr::copy_nonoverlapping(\n            imports.functions.as_ptr(),\n            self.vmctx_plus_offset(offsets.vmctx_imported_functions_begin()),\n            imports.functions.len(),\n        );\n        debug_assert_eq!(imports.tables.len(), module.num_imported_tables);\n        ptr::copy_nonoverlapping(\n            imports.tables.as_ptr(),\n            self.vmctx_plus_offset(offsets.vmctx_imported_tables_begin()),\n            imports.tables.len(),\n        );\n        debug_assert_eq!(imports.memories.len(), module.num_imported_memories);\n        ptr::copy_nonoverlapping(\n            imports.memories.as_ptr(),\n            self.vmctx_plus_offset(offsets.vmctx_imported_memories_begin()),\n            imports.memories.len(),\n        );\n        debug_assert_eq!(imports.globals.len(), module.num_imported_globals);\n        ptr::copy_nonoverlapping(\n            imports.globals.as_ptr(),\n            self.vmctx_plus_offset(offsets.vmctx_imported_globals_begin()),\n            imports.globals.len(),\n        );\n\n        // N.B.: there is no need to initialize the anyfuncs array because\n        // we eagerly construct each element in it whenever asked for a\n        // reference to that element. In other words, there is no state\n        // needed to track the lazy-init, so we don't need to initialize\n        // any state now.\n\n        // Initialize the defined tables\n        let mut ptr = self.vmctx_plus_offset(offsets.vmctx_tables_begin());\n        for i in 0..module.table_plans.len() - module.num_imported_tables {\n            ptr::write(ptr, self.tables[DefinedTableIndex::new(i)].vmtable());\n            ptr = ptr.add(1);\n        }\n\n        // Initialize the defined memories. This fills in both the\n        // `defined_memories` table and the `owned_memories` table at the same\n        // time. Entries in `defined_memories` hold a pointer to a definition\n        // (all memories) whereas the `owned_memories` hold the actual\n        // definitions of memories owned (not shared) in the module.\n        let mut ptr = self.vmctx_plus_offset(offsets.vmctx_memories_begin());\n        let mut owned_ptr = self.vmctx_plus_offset(offsets.vmctx_owned_memories_begin());\n        for i in 0..module.memory_plans.len() - module.num_imported_memories {\n            let defined_memory_index = DefinedMemoryIndex::new(i);\n            let memory_index = module.memory_index(defined_memory_index);\n            if module.memory_plans[memory_index].memory.shared {\n                let def_ptr = self.memories[defined_memory_index]\n                    .as_shared_memory()\n                    .unwrap()\n                    .vmmemory_ptr();\n                ptr::write(ptr, def_ptr.cast_mut());\n            } else {\n                ptr::write(owned_ptr, self.memories[defined_memory_index].vmmemory());\n                ptr::write(ptr, owned_ptr);\n                owned_ptr = owned_ptr.add(1);\n            }\n            ptr = ptr.add(1);\n        }\n\n        // Initialize the defined globals\n        self.initialize_vmctx_globals(module);\n    }\n\n    unsafe fn initialize_vmctx_globals(&mut self, module: &Module) {\n        let num_imports = module.num_imported_globals;\n        for (index, global) in module.globals.iter().skip(num_imports) {\n            let def_index = module.defined_global_index(index).unwrap();\n            let to = self.global_ptr(def_index);\n\n            // Initialize the global before writing to it\n            ptr::write(to, VMGlobalDefinition::new());\n\n            match global.initializer {\n                GlobalInit::I32Const(x) => *(*to).as_i32_mut() = x,\n                GlobalInit::I64Const(x) => *(*to).as_i64_mut() = x,\n                GlobalInit::F32Const(x) => *(*to).as_f32_bits_mut() = x,\n                GlobalInit::F64Const(x) => *(*to).as_f64_bits_mut() = x,\n                GlobalInit::V128Const(x) => *(*to).as_u128_mut() = x,\n                GlobalInit::GetGlobal(x) => {\n                    let from = if let Some(def_x) = module.defined_global_index(x) {\n                        self.global(def_x)\n                    } else {\n                        &*self.imported_global(x).from\n                    };\n                    // Globals of type `externref` need to manage the reference\n                    // count as values move between globals, everything else is just\n                    // copy-able bits.\n                    match global.wasm_ty {\n                        WasmType::ExternRef => {\n                            *(*to).as_externref_mut() = from.as_externref().clone()\n                        }\n                        _ => ptr::copy_nonoverlapping(from, to, 1),\n                    }\n                }\n                GlobalInit::RefFunc(f) => {\n                    *(*to).as_anyfunc_mut() = self.get_caller_checked_anyfunc(f).unwrap()\n                        as *const VMCallerCheckedFuncRef;\n                }\n                GlobalInit::RefNullConst => match global.wasm_ty {\n                    // `VMGlobalDefinition::new()` already zeroed out the bits\n                    WasmType::FuncRef => {}\n                    WasmType::ExternRef => {}\n                    ty => panic!(\"unsupported reference type for global: {:?}\", ty),\n                },\n                GlobalInit::Import => panic!(\"locally-defined global initialized as import\"),\n            }\n        }\n    }\n\n    fn wasm_fault(&self, addr: usize) -> Option<WasmFault> {\n        let mut fault = None;\n        for (_, memory) in self.memories.iter() {\n            let accessible = memory.wasm_accessible();\n            if accessible.start <= addr && addr < accessible.end {\n                // All linear memories should be disjoint so assert that no\n                // prior fault has been found.\n                assert!(fault.is_none());\n                fault = Some(WasmFault {\n                    memory_size: memory.byte_size(),\n                    wasm_address: u64::try_from(addr - accessible.start).unwrap(),\n                });\n            }\n        }\n        fault\n    }\n}\n\nimpl Drop for Instance {\n    fn drop(&mut self) {\n        // Drop any defined globals\n        for (idx, global) in self.module().globals.iter() {\n            let idx = match self.module().defined_global_index(idx) {\n                Some(idx) => idx,\n                None => continue,\n            };\n            match global.wasm_ty {\n                // For now only externref globals need to get destroyed\n                WasmType::ExternRef => {}\n                _ => continue,\n            }\n            unsafe {\n                drop((*self.global_ptr(idx)).as_externref_mut().take());\n            }\n        }\n    }\n}\n\n/// A handle holding an `Instance` of a WebAssembly module.\n#[derive(Hash, PartialEq, Eq)]\npub struct InstanceHandle {\n    instance: *mut Instance,\n}\n\n// These are only valid if the `Instance` type is send/sync, hence the\n// assertion below.\nunsafe impl Send for InstanceHandle {}\nunsafe impl Sync for InstanceHandle {}\n\nfn _assert_send_sync() {\n    fn _assert<T: Send + Sync>() {}\n    _assert::<Instance>();\n}\n\nimpl InstanceHandle {\n    /// Create a new `InstanceHandle` pointing at the instance\n    /// pointed to by the given `VMContext` pointer.\n    ///\n    /// # Safety\n    /// This is unsafe because it doesn't work on just any `VMContext`, it must\n    /// be a `VMContext` allocated as part of an `Instance`.\n    #[inline]\n    pub unsafe fn from_vmctx(vmctx: *mut VMContext) -> Self {\n        let instance = (&mut *vmctx).instance();\n        Self {\n            instance: instance as *const Instance as *mut Instance,\n        }\n    }\n\n    /// Return a reference to the vmctx used by compiled wasm code.\n    pub fn vmctx(&self) -> &VMContext {\n        self.instance().vmctx()\n    }\n\n    /// Return a raw pointer to the vmctx used by compiled wasm code.\n    #[inline]\n    pub fn vmctx_ptr(&self) -> *mut VMContext {\n        self.instance().vmctx_ptr()\n    }\n\n    /// Return a reference to a module.\n    pub fn module(&self) -> &Arc<Module> {\n        self.instance().module()\n    }\n\n    /// Lookup a function by index.\n    pub fn get_exported_func(&mut self, export: FuncIndex) -> ExportFunction {\n        self.instance_mut().get_exported_func(export)\n    }\n\n    /// Lookup a global by index.\n    pub fn get_exported_global(&mut self, export: GlobalIndex) -> ExportGlobal {\n        self.instance_mut().get_exported_global(export)\n    }\n\n    /// Lookup a memory by index.\n    pub fn get_exported_memory(&mut self, export: MemoryIndex) -> ExportMemory {\n        self.instance_mut().get_exported_memory(export)\n    }\n\n    /// Lookup a table by index.\n    pub fn get_exported_table(&mut self, export: TableIndex) -> ExportTable {\n        self.instance_mut().get_exported_table(export)\n    }\n\n    /// Lookup an item with the given index.\n    pub fn get_export_by_index(&mut self, export: EntityIndex) -> Export {\n        match export {\n            EntityIndex::Function(i) => Export::Function(self.get_exported_func(i)),\n            EntityIndex::Global(i) => Export::Global(self.get_exported_global(i)),\n            EntityIndex::Table(i) => Export::Table(self.get_exported_table(i)),\n            EntityIndex::Memory(i) => Export::Memory(self.get_exported_memory(i)),\n        }\n    }\n\n    /// Return an iterator over the exports of this instance.\n    ///\n    /// Specifically, it provides access to the key-value pairs, where the keys\n    /// are export names, and the values are export declarations which can be\n    /// resolved `lookup_by_declaration`.\n    pub fn exports(&self) -> indexmap::map::Iter<String, EntityIndex> {\n        self.instance().exports()\n    }\n\n    /// Return a reference to the custom state attached to this instance.\n    pub fn host_state(&self) -> &dyn Any {\n        self.instance().host_state()\n    }\n\n    /// Get a memory defined locally within this module.\n    pub fn get_defined_memory(&mut self, index: DefinedMemoryIndex) -> *mut Memory {\n        self.instance_mut().get_defined_memory(index)\n    }\n\n    /// Return the table index for the given `VMTableDefinition` in this instance.\n    pub unsafe fn table_index(&self, table: &VMTableDefinition) -> DefinedTableIndex {\n        self.instance().table_index(table)\n    }\n\n    /// Get a table defined locally within this module.\n    pub fn get_defined_table(&mut self, index: DefinedTableIndex) -> *mut Table {\n        self.instance_mut().get_defined_table(index)\n    }\n\n    /// Get a table defined locally within this module, lazily\n    /// initializing the given range first.\n    pub fn get_defined_table_with_lazy_init(\n        &mut self,\n        index: DefinedTableIndex,\n        range: impl Iterator<Item = u32>,\n    ) -> *mut Table {\n        let index = self.instance().module().table_index(index);\n        self.instance_mut().get_table_with_lazy_init(index, range)\n    }\n\n    /// Return a reference to the contained `Instance`.\n    #[inline]\n    pub(crate) fn instance(&self) -> &Instance {\n        unsafe { &*(self.instance as *const Instance) }\n    }\n\n    pub(crate) fn instance_mut(&mut self) -> &mut Instance {\n        unsafe { &mut *self.instance }\n    }\n\n    /// Returns the `Store` pointer that was stored on creation\n    #[inline]\n    pub fn store(&self) -> *mut dyn Store {\n        self.instance().store()\n    }\n\n    /// Configure the `*mut dyn Store` internal pointer after-the-fact.\n    ///\n    /// This is provided for the original `Store` itself to configure the first\n    /// self-pointer after the original `Box` has been initialized.\n    pub unsafe fn set_store(&mut self, store: *mut dyn Store) {\n        self.instance_mut().set_store(Some(store));\n    }\n\n    /// Returns a clone of this instance.\n    ///\n    /// This is unsafe because the returned handle here is just a cheap clone\n    /// of the internals, there's no lifetime tracking around its validity.\n    /// You'll need to ensure that the returned handles all go out of scope at\n    /// the same time.\n    #[inline]\n    pub unsafe fn clone(&self) -> InstanceHandle {\n        InstanceHandle {\n            instance: self.instance,\n        }\n    }\n\n    /// Performs post-initialization of an instance after its handle has been\n    /// creqtaed and registered with a store.\n    ///\n    /// Failure of this function means that the instance still must persist\n    /// within the store since failure may indicate partial failure, or some\n    /// state could be referenced by other instances.\n    pub fn initialize(&mut self, module: &Module, is_bulk_memory: bool) -> Result<()> {\n        allocator::initialize_instance(self.instance_mut(), module, is_bulk_memory)\n    }\n\n    /// Attempts to convert from the host `addr` specified to a WebAssembly\n    /// based address recorded in `WasmFault`.\n    ///\n    /// This method will check all linear memories that this instance contains\n    /// to see if any of them contain `addr`. If one does then `Some` is\n    /// returned with metadata about the wasm fault. Otherwise `None` is\n    /// returned and `addr` doesn't belong to this instance.\n    pub fn wasm_fault(&self, addr: usize) -> Option<WasmFault> {\n        self.instance().wasm_fault(addr)\n    }\n}\n", "use crate::imports::Imports;\nuse crate::instance::{Instance, InstanceHandle, RuntimeMemoryCreator};\nuse crate::memory::{DefaultMemoryCreator, Memory};\nuse crate::table::Table;\nuse crate::{CompiledModuleId, ModuleRuntimeInfo, Store};\nuse anyhow::{anyhow, bail, Result};\nuse std::alloc;\nuse std::any::Any;\nuse std::convert::TryFrom;\nuse std::ptr;\nuse std::sync::Arc;\nuse wasmtime_environ::{\n    DefinedMemoryIndex, DefinedTableIndex, HostPtr, InitMemory, MemoryInitialization,\n    MemoryInitializer, Module, PrimaryMap, TableInitialization, TableInitializer, Trap, VMOffsets,\n    WasmType, WASM_PAGE_SIZE,\n};\n\n#[cfg(feature = \"pooling-allocator\")]\nmod pooling;\n\n#[cfg(feature = \"pooling-allocator\")]\npub use self::pooling::{InstanceLimits, PoolingInstanceAllocator, PoolingInstanceAllocatorConfig};\n\n/// Represents a request for a new runtime instance.\npub struct InstanceAllocationRequest<'a> {\n    /// The info related to the compiled version of this module,\n    /// needed for instantiation: function metadata, JIT code\n    /// addresses, precomputed images for lazy memory and table\n    /// initialization, and the like. This Arc is cloned and held for\n    /// the lifetime of the instance.\n    pub runtime_info: &'a Arc<dyn ModuleRuntimeInfo>,\n\n    /// The imports to use for the instantiation.\n    pub imports: Imports<'a>,\n\n    /// The host state to associate with the instance.\n    pub host_state: Box<dyn Any + Send + Sync>,\n\n    /// A pointer to the \"store\" for this instance to be allocated. The store\n    /// correlates with the `Store` in wasmtime itself, and lots of contextual\n    /// information about the execution of wasm can be learned through the store.\n    ///\n    /// Note that this is a raw pointer and has a static lifetime, both of which\n    /// are a bit of a lie. This is done purely so a store can learn about\n    /// itself when it gets called as a host function, and additionally so this\n    /// runtime can access internals as necessary (such as the\n    /// VMExternRefActivationsTable or the resource limiter methods).\n    ///\n    /// Note that this ends up being a self-pointer to the instance when stored.\n    /// The reason is that the instance itself is then stored within the store.\n    /// We use a number of `PhantomPinned` declarations to indicate this to the\n    /// compiler. More info on this in `wasmtime/src/store.rs`\n    pub store: StorePtr,\n}\n\n/// A pointer to a Store. This Option<*mut dyn Store> is wrapped in a struct\n/// so that the function to create a &mut dyn Store is a method on a member of\n/// InstanceAllocationRequest, rather than on a &mut InstanceAllocationRequest\n/// itself, because several use-sites require a split mut borrow on the\n/// InstanceAllocationRequest.\npub struct StorePtr(Option<*mut dyn Store>);\nimpl StorePtr {\n    /// A pointer to no Store.\n    pub fn empty() -> Self {\n        Self(None)\n    }\n    /// A pointer to a Store.\n    pub fn new(ptr: *mut dyn Store) -> Self {\n        Self(Some(ptr))\n    }\n    /// The raw contents of this struct\n    pub fn as_raw(&self) -> Option<*mut dyn Store> {\n        self.0.clone()\n    }\n    /// Use the StorePtr as a mut ref to the Store.\n    /// Safety: must not be used outside the original lifetime of the borrow.\n    pub(crate) unsafe fn get(&mut self) -> Option<&mut dyn Store> {\n        match self.0 {\n            Some(ptr) => Some(&mut *ptr),\n            None => None,\n        }\n    }\n}\n\n/// Represents a runtime instance allocator.\n///\n/// # Safety\n///\n/// This trait is unsafe as it requires knowledge of Wasmtime's runtime internals to implement correctly.\npub unsafe trait InstanceAllocator {\n    /// Validates that a module is supported by the allocator.\n    fn validate(&self, module: &Module, offsets: &VMOffsets<HostPtr>) -> Result<()> {\n        drop((module, offsets));\n        Ok(())\n    }\n\n    /// Allocates a fresh `InstanceHandle` for the `req` given.\n    ///\n    /// This will allocate memories and tables internally from this allocator\n    /// and weave that altogether into a final and complete `InstanceHandle`\n    /// ready to be registered with a store.\n    ///\n    /// Note that the returned instance must still have `.initialize(..)` called\n    /// on it to complete the instantiation process.\n    fn allocate(&self, mut req: InstanceAllocationRequest) -> Result<InstanceHandle> {\n        let index = self.allocate_index(&req)?;\n        let module = req.runtime_info.module();\n        let mut memories =\n            PrimaryMap::with_capacity(module.memory_plans.len() - module.num_imported_memories);\n        let mut tables =\n            PrimaryMap::with_capacity(module.table_plans.len() - module.num_imported_tables);\n\n        let result = self\n            .allocate_memories(index, &mut req, &mut memories)\n            .and_then(|()| self.allocate_tables(index, &mut req, &mut tables));\n        if let Err(e) = result {\n            self.deallocate_memories(index, &mut memories);\n            self.deallocate_tables(index, &mut tables);\n            self.deallocate_index(index);\n            return Err(e);\n        }\n\n        unsafe { Ok(Instance::new(req, index, memories, tables)) }\n    }\n\n    /// Deallocates the provided instance.\n    ///\n    /// This will null-out the pointer within `handle` and otherwise reclaim\n    /// resources such as tables, memories, and the instance memory itself.\n    fn deallocate(&self, handle: &mut InstanceHandle) {\n        let index = handle.instance().index;\n        self.deallocate_memories(index, &mut handle.instance_mut().memories);\n        self.deallocate_tables(index, &mut handle.instance_mut().tables);\n        unsafe {\n            let layout = Instance::alloc_layout(handle.instance().offsets());\n            ptr::drop_in_place(handle.instance);\n            alloc::dealloc(handle.instance.cast(), layout);\n            handle.instance = std::ptr::null_mut();\n        }\n        self.deallocate_index(index);\n    }\n\n    /// Optionally allocates an allocator-defined index for the `req` provided.\n    ///\n    /// The return value here, if successful, is passed to the various methods\n    /// below for memory/table allocation/deallocation.\n    fn allocate_index(&self, req: &InstanceAllocationRequest) -> Result<usize>;\n\n    /// Deallocates indices allocated by `allocate_index`.\n    fn deallocate_index(&self, index: usize);\n\n    /// Attempts to allocate all defined linear memories for a module.\n    ///\n    /// Pushes all memories for `req` onto the `mems` storage provided which is\n    /// already appropriately allocated to contain all memories.\n    ///\n    /// Note that this is allowed to fail. Failure can additionally happen after\n    /// some memories have already been successfully allocated. All memories\n    /// pushed onto `mem` are guaranteed to one day make their way to\n    /// `deallocate_memories`.\n    fn allocate_memories(\n        &self,\n        index: usize,\n        req: &mut InstanceAllocationRequest,\n        mems: &mut PrimaryMap<DefinedMemoryIndex, Memory>,\n    ) -> Result<()>;\n\n    /// Deallocates all memories provided, optionally reclaiming resources for\n    /// the pooling allocator for example.\n    fn deallocate_memories(&self, index: usize, mems: &mut PrimaryMap<DefinedMemoryIndex, Memory>);\n\n    /// Same as `allocate_memories`, but for tables.\n    fn allocate_tables(\n        &self,\n        index: usize,\n        req: &mut InstanceAllocationRequest,\n        tables: &mut PrimaryMap<DefinedTableIndex, Table>,\n    ) -> Result<()>;\n\n    /// Same as `deallocate_memories`, but for tables.\n    fn deallocate_tables(&self, index: usize, tables: &mut PrimaryMap<DefinedTableIndex, Table>);\n\n    /// Allocates a fiber stack for calling async functions on.\n    #[cfg(feature = \"async\")]\n    fn allocate_fiber_stack(&self) -> Result<wasmtime_fiber::FiberStack>;\n\n    /// Deallocates a fiber stack that was previously allocated with `allocate_fiber_stack`.\n    ///\n    /// # Safety\n    ///\n    /// The provided stack is required to have been allocated with `allocate_fiber_stack`.\n    #[cfg(feature = \"async\")]\n    unsafe fn deallocate_fiber_stack(&self, stack: &wasmtime_fiber::FiberStack);\n\n    /// Purges all lingering resources related to `module` from within this\n    /// allocator.\n    ///\n    /// Primarily present for the pooling allocator to remove mappings of\n    /// this module from slots in linear memory.\n    fn purge_module(&self, module: CompiledModuleId);\n}\n\nfn get_table_init_start(init: &TableInitializer, instance: &Instance) -> Result<u32> {\n    match init.base {\n        Some(base) => {\n            let val = unsafe {\n                if let Some(def_index) = instance.module().defined_global_index(base) {\n                    *instance.global(def_index).as_u32()\n                } else {\n                    *(*instance.imported_global(base).from).as_u32()\n                }\n            };\n\n            init.offset\n                .checked_add(val)\n                .ok_or_else(|| anyhow!(\"element segment global base overflows\"))\n        }\n        None => Ok(init.offset),\n    }\n}\n\nfn check_table_init_bounds(instance: &mut Instance, module: &Module) -> Result<()> {\n    match &module.table_initialization {\n        TableInitialization::FuncTable { segments, .. }\n        | TableInitialization::Segments { segments } => {\n            for segment in segments {\n                let table = unsafe { &*instance.get_table(segment.table_index) };\n                let start = get_table_init_start(segment, instance)?;\n                let start = usize::try_from(start).unwrap();\n                let end = start.checked_add(segment.elements.len());\n\n                match end {\n                    Some(end) if end <= table.size() as usize => {\n                        // Initializer is in bounds\n                    }\n                    _ => {\n                        bail!(\"table out of bounds: elements segment does not fit\")\n                    }\n                }\n            }\n        }\n    }\n\n    Ok(())\n}\n\nfn initialize_tables(instance: &mut Instance, module: &Module) -> Result<()> {\n    // Note: if the module's table initializer state is in\n    // FuncTable mode, we will lazily initialize tables based on\n    // any statically-precomputed image of FuncIndexes, but there\n    // may still be \"leftover segments\" that could not be\n    // incorporated. So we have a unified handler here that\n    // iterates over all segments (Segments mode) or leftover\n    // segments (FuncTable mode) to initialize.\n    match &module.table_initialization {\n        TableInitialization::FuncTable { segments, .. }\n        | TableInitialization::Segments { segments } => {\n            for segment in segments {\n                instance.table_init_segment(\n                    segment.table_index,\n                    &segment.elements,\n                    get_table_init_start(segment, instance)?,\n                    0,\n                    segment.elements.len() as u32,\n                )?;\n            }\n        }\n    }\n\n    Ok(())\n}\n\nfn get_memory_init_start(init: &MemoryInitializer, instance: &Instance) -> Result<u64> {\n    match init.base {\n        Some(base) => {\n            let mem64 = instance.module().memory_plans[init.memory_index]\n                .memory\n                .memory64;\n            let val = unsafe {\n                let global = if let Some(def_index) = instance.module().defined_global_index(base) {\n                    instance.global(def_index)\n                } else {\n                    &*instance.imported_global(base).from\n                };\n                if mem64 {\n                    *global.as_u64()\n                } else {\n                    u64::from(*global.as_u32())\n                }\n            };\n\n            init.offset\n                .checked_add(val)\n                .ok_or_else(|| anyhow!(\"data segment global base overflows\"))\n        }\n        None => Ok(init.offset),\n    }\n}\n\nfn check_memory_init_bounds(instance: &Instance, initializers: &[MemoryInitializer]) -> Result<()> {\n    for init in initializers {\n        let memory = instance.get_memory(init.memory_index);\n        let start = get_memory_init_start(init, instance)?;\n        let end = usize::try_from(start)\n            .ok()\n            .and_then(|start| start.checked_add(init.data.len()));\n\n        match end {\n            Some(end) if end <= memory.current_length() => {\n                // Initializer is in bounds\n            }\n            _ => {\n                bail!(\"memory out of bounds: data segment does not fit\")\n            }\n        }\n    }\n\n    Ok(())\n}\n\nfn initialize_memories(instance: &mut Instance, module: &Module) -> Result<()> {\n    let memory_size_in_pages =\n        &|memory| (instance.get_memory(memory).current_length() as u64) / u64::from(WASM_PAGE_SIZE);\n\n    // Loads the `global` value and returns it as a `u64`, but sign-extends\n    // 32-bit globals which can be used as the base for 32-bit memories.\n    let get_global_as_u64 = &|global| unsafe {\n        let def = if let Some(def_index) = instance.module().defined_global_index(global) {\n            instance.global(def_index)\n        } else {\n            &*instance.imported_global(global).from\n        };\n        if module.globals[global].wasm_ty == WasmType::I64 {\n            *def.as_u64()\n        } else {\n            u64::from(*def.as_u32())\n        }\n    };\n\n    // Delegates to the `init_memory` method which is sort of a duplicate of\n    // `instance.memory_init_segment` but is used at compile-time in other\n    // contexts so is shared here to have only one method of memory\n    // initialization.\n    //\n    // This call to `init_memory` notably implements all the bells and whistles\n    // so errors only happen if an out-of-bounds segment is found, in which case\n    // a trap is returned.\n    let ok = module.memory_initialization.init_memory(\n        InitMemory::Runtime {\n            memory_size_in_pages,\n            get_global_as_u64,\n        },\n        &mut |memory_index, init| {\n            // If this initializer applies to a defined memory but that memory\n            // doesn't need initialization, due to something like copy-on-write\n            // pre-initializing it via mmap magic, then this initializer can be\n            // skipped entirely.\n            if let Some(memory_index) = module.defined_memory_index(memory_index) {\n                if !instance.memories[memory_index].needs_init() {\n                    return true;\n                }\n            }\n            let memory = instance.get_memory(memory_index);\n\n            unsafe {\n                let src = instance.wasm_data(init.data.clone());\n                let dst = memory.base.add(usize::try_from(init.offset).unwrap());\n                // FIXME audit whether this is safe in the presence of shared\n                // memory\n                // (https://github.com/bytecodealliance/wasmtime/issues/4203).\n                ptr::copy_nonoverlapping(src.as_ptr(), dst, src.len())\n            }\n            true\n        },\n    );\n    if !ok {\n        return Err(Trap::MemoryOutOfBounds.into());\n    }\n\n    Ok(())\n}\n\nfn check_init_bounds(instance: &mut Instance, module: &Module) -> Result<()> {\n    check_table_init_bounds(instance, module)?;\n\n    match &instance.module().memory_initialization {\n        MemoryInitialization::Segmented(initializers) => {\n            check_memory_init_bounds(instance, initializers)?;\n        }\n        // Statically validated already to have everything in-bounds.\n        MemoryInitialization::Static { .. } => {}\n    }\n\n    Ok(())\n}\n\npub(super) fn initialize_instance(\n    instance: &mut Instance,\n    module: &Module,\n    is_bulk_memory: bool,\n) -> Result<()> {\n    // If bulk memory is not enabled, bounds check the data and element segments before\n    // making any changes. With bulk memory enabled, initializers are processed\n    // in-order and side effects are observed up to the point of an out-of-bounds\n    // initializer, so the early checking is not desired.\n    if !is_bulk_memory {\n        check_init_bounds(instance, module)?;\n    }\n\n    // Initialize the tables\n    initialize_tables(instance, module)?;\n\n    // Initialize the memories\n    initialize_memories(instance, &module)?;\n\n    Ok(())\n}\n\n/// Represents the on-demand instance allocator.\n#[derive(Clone)]\npub struct OnDemandInstanceAllocator {\n    mem_creator: Option<Arc<dyn RuntimeMemoryCreator>>,\n    #[cfg(feature = \"async\")]\n    stack_size: usize,\n}\n\nimpl OnDemandInstanceAllocator {\n    /// Creates a new on-demand instance allocator.\n    pub fn new(mem_creator: Option<Arc<dyn RuntimeMemoryCreator>>, stack_size: usize) -> Self {\n        drop(stack_size); // suppress unused warnings w/o async feature\n        Self {\n            mem_creator,\n            #[cfg(feature = \"async\")]\n            stack_size,\n        }\n    }\n}\n\nimpl Default for OnDemandInstanceAllocator {\n    fn default() -> Self {\n        Self {\n            mem_creator: None,\n            #[cfg(feature = \"async\")]\n            stack_size: 0,\n        }\n    }\n}\n\nunsafe impl InstanceAllocator for OnDemandInstanceAllocator {\n    fn allocate_index(&self, _req: &InstanceAllocationRequest) -> Result<usize> {\n        Ok(0)\n    }\n\n    fn deallocate_index(&self, index: usize) {\n        assert_eq!(index, 0);\n    }\n\n    fn allocate_memories(\n        &self,\n        _index: usize,\n        req: &mut InstanceAllocationRequest,\n        memories: &mut PrimaryMap<DefinedMemoryIndex, Memory>,\n    ) -> Result<()> {\n        let module = req.runtime_info.module();\n        let creator = self\n            .mem_creator\n            .as_deref()\n            .unwrap_or_else(|| &DefaultMemoryCreator);\n        let num_imports = module.num_imported_memories;\n        for (memory_idx, plan) in module.memory_plans.iter().skip(num_imports) {\n            let defined_memory_idx = module\n                .defined_memory_index(memory_idx)\n                .expect(\"Skipped imports, should never be None\");\n            let image = req.runtime_info.memory_image(defined_memory_idx)?;\n\n            memories.push(Memory::new_dynamic(\n                plan,\n                creator,\n                unsafe {\n                    req.store\n                        .get()\n                        .expect(\"if module has memory plans, store is not empty\")\n                },\n                image,\n            )?);\n        }\n        Ok(())\n    }\n\n    fn deallocate_memories(\n        &self,\n        _index: usize,\n        _mems: &mut PrimaryMap<DefinedMemoryIndex, Memory>,\n    ) {\n        // normal destructors do cleanup here\n    }\n\n    fn allocate_tables(\n        &self,\n        _index: usize,\n        req: &mut InstanceAllocationRequest,\n        tables: &mut PrimaryMap<DefinedTableIndex, Table>,\n    ) -> Result<()> {\n        let module = req.runtime_info.module();\n        let num_imports = module.num_imported_tables;\n        for (_, table) in module.table_plans.iter().skip(num_imports) {\n            tables.push(Table::new_dynamic(table, unsafe {\n                req.store\n                    .get()\n                    .expect(\"if module has table plans, store is not empty\")\n            })?);\n        }\n        Ok(())\n    }\n\n    fn deallocate_tables(&self, _index: usize, _tables: &mut PrimaryMap<DefinedTableIndex, Table>) {\n        // normal destructors do cleanup here\n    }\n\n    #[cfg(feature = \"async\")]\n    fn allocate_fiber_stack(&self) -> Result<wasmtime_fiber::FiberStack> {\n        if self.stack_size == 0 {\n            bail!(\"fiber stacks are not supported by the allocator\")\n        }\n\n        let stack = wasmtime_fiber::FiberStack::new(self.stack_size)?;\n        Ok(stack)\n    }\n\n    #[cfg(feature = \"async\")]\n    unsafe fn deallocate_fiber_stack(&self, _stack: &wasmtime_fiber::FiberStack) {\n        // The on-demand allocator has no further bookkeeping for fiber stacks\n    }\n\n    fn purge_module(&self, _: CompiledModuleId) {}\n}\n", "//! Runtime library calls.\n//!\n//! Note that Wasm compilers may sometimes perform these inline rather than\n//! calling them, particularly when CPUs have special instructions which compute\n//! them directly.\n//!\n//! These functions are called by compiled Wasm code, and therefore must take\n//! certain care about some things:\n//!\n//! * They must only contain basic, raw i32/i64/f32/f64/pointer parameters that\n//!   are safe to pass across the system ABI.\n//!\n//! * If any nested function propagates an `Err(trap)` out to the library\n//!   function frame, we need to raise it. This involves some nasty and quite\n//!   unsafe code under the covers! Notably, after raising the trap, drops\n//!   **will not** be run for local variables! This can lead to things like\n//!   leaking `InstanceHandle`s which leads to never deallocating JIT code,\n//!   instances, and modules if we are not careful!\n//!\n//! * The libcall must be entered via a Wasm-to-libcall trampoline that saves\n//!   the last Wasm FP and PC for stack walking purposes. (For more details, see\n//!   `crates/runtime/src/backtrace.rs`.)\n//!\n//! To make it easier to correctly handle all these things, **all** libcalls\n//! must be defined via the `libcall!` helper macro! See its doc comments below\n//! for an example, or just look at the rest of the file.\n//!\n//! ## Dealing with `externref`s\n//!\n//! When receiving a raw `*mut u8` that is actually a `VMExternRef` reference,\n//! convert it into a proper `VMExternRef` with `VMExternRef::clone_from_raw` as\n//! soon as apossible. Any GC before raw pointer is converted into a reference\n//! can potentially collect the referenced object, which could lead to use after\n//! free.\n//!\n//! Avoid this by eagerly converting into a proper `VMExternRef`! (Unfortunately\n//! there is no macro to help us automatically get this correct, so stay\n//! vigilant!)\n//!\n//! ```ignore\n//! pub unsafe extern \"C\" my_libcall_takes_ref(raw_extern_ref: *mut u8) {\n//!     // Before `clone_from_raw`, `raw_extern_ref` is potentially unrooted,\n//!     // and doing GC here could lead to use after free!\n//!\n//!     let my_extern_ref = if raw_extern_ref.is_null() {\n//!         None\n//!     } else {\n//!         Some(VMExternRef::clone_from_raw(raw_extern_ref))\n//!     };\n//!\n//!     // Now that we did `clone_from_raw`, it is safe to do a GC (or do\n//!     // anything else that might transitively GC, like call back into\n//!     // Wasm!)\n//! }\n//! ```\n\nuse crate::externref::VMExternRef;\nuse crate::table::{Table, TableElementType};\nuse crate::vmcontext::{VMCallerCheckedFuncRef, VMContext};\nuse crate::TrapReason;\nuse anyhow::Result;\nuse std::mem;\nuse std::ptr::{self, NonNull};\nuse std::time::{Duration, Instant};\nuse wasmtime_environ::{\n    DataIndex, ElemIndex, FuncIndex, GlobalIndex, MemoryIndex, TableIndex, Trap,\n};\n\n/// Actually public trampolines which are used by the runtime as the entrypoint\n/// for libcalls.\n///\n/// Note that the trampolines here are actually defined in inline assembly right\n/// now to ensure that the fp/sp on exit are recorded for backtraces to work\n/// properly.\npub mod trampolines {\n    use crate::{TrapReason, VMContext};\n\n    macro_rules! libcall {\n        (\n            $(\n                $( #[$attr:meta] )*\n                $name:ident( vmctx: vmctx $(, $pname:ident: $param:ident )* ) $( -> $result:ident )?;\n            )*\n        ) => {paste::paste! {\n            $(\n                // The actual libcall itself, which has the `pub` name here, is\n                // defined via the `wasm_to_libcall_trampoline!` macro on\n                // supported platforms or otherwise in inline assembly for\n                // platforms like s390x which don't have stable `global_asm!`\n                // yet.\n                extern \"C\" {\n                    #[allow(missing_docs)]\n                    #[allow(improper_ctypes)]\n                    pub fn $name(\n                        vmctx: *mut VMContext,\n                        $( $pname: libcall!(@ty $param), )*\n                    ) $(-> libcall!(@ty $result))?;\n                }\n\n                wasm_to_libcall_trampoline!($name ; [<impl_ $name>]);\n\n                // This is the direct entrypoint from the inline assembly which\n                // still has the same raw signature as the trampoline itself.\n                // This will delegate to the outer module to the actual\n                // implementation and automatically perform `catch_unwind` along\n                // with conversion of the return value in the face of traps.\n                //\n                // Note that rust targets which support `global_asm!` can use\n                // the `sym` operator to get the symbol here, but other targets\n                // like s390x need to use outlined assembly files which requires\n                // `no_mangle`.\n                #[cfg_attr(target_arch = \"s390x\", no_mangle)]\n                unsafe extern \"C\" fn [<impl_ $name>](\n                    vmctx : *mut VMContext,\n                    $( $pname : libcall!(@ty $param), )*\n                ) $( -> libcall!(@ty $result))? {\n                    let result = std::panic::catch_unwind(|| {\n                        super::$name(vmctx, $($pname),*)\n                    });\n                    match result {\n                        Ok(ret) => LibcallResult::convert(ret),\n                        Err(panic) => crate::traphandlers::resume_panic(panic),\n                    }\n                }\n\n                // This works around a `rustc` bug where compiling with LTO\n                // will sometimes strip out some of these symbols resulting\n                // in a linking failure.\n                #[allow(non_upper_case_globals)]\n                #[used]\n                static [<impl_ $name _ref>]: unsafe extern \"C\" fn(\n                    *mut VMContext,\n                    $( $pname : libcall!(@ty $param), )*\n                ) $( -> libcall!(@ty $result))? = [<impl_ $name>];\n\n            )*\n        }};\n\n        (@ty i32) => (u32);\n        (@ty i64) => (u64);\n        (@ty reference) => (*mut u8);\n        (@ty pointer) => (*mut u8);\n        (@ty vmctx) => (*mut VMContext);\n    }\n\n    wasmtime_environ::foreach_builtin_function!(libcall);\n\n    // Helper trait to convert results of libcalls below into the ABI of what\n    // the libcall expects.\n    //\n    // This basically entirely exists for the `Result` implementation which\n    // \"unwraps\" via a throwing of a trap.\n    trait LibcallResult {\n        type Abi;\n        unsafe fn convert(self) -> Self::Abi;\n    }\n\n    impl LibcallResult for () {\n        type Abi = ();\n        unsafe fn convert(self) {}\n    }\n\n    impl<T, E> LibcallResult for Result<T, E>\n    where\n        E: Into<TrapReason>,\n    {\n        type Abi = T;\n        unsafe fn convert(self) -> T {\n            match self {\n                Ok(t) => t,\n                Err(e) => crate::traphandlers::raise_trap(e.into()),\n            }\n        }\n    }\n\n    impl LibcallResult for *mut u8 {\n        type Abi = *mut u8;\n        unsafe fn convert(self) -> *mut u8 {\n            self\n        }\n    }\n}\n\nunsafe fn memory32_grow(\n    vmctx: *mut VMContext,\n    delta: u64,\n    memory_index: u32,\n) -> Result<*mut u8, TrapReason> {\n    let instance = (*vmctx).instance_mut();\n    let memory_index = MemoryIndex::from_u32(memory_index);\n    let result =\n        match instance\n            .memory_grow(memory_index, delta)\n            .map_err(|error| TrapReason::User {\n                error,\n                needs_backtrace: true,\n            })? {\n            Some(size_in_bytes) => size_in_bytes / (wasmtime_environ::WASM_PAGE_SIZE as usize),\n            None => usize::max_value(),\n        };\n    Ok(result as *mut _)\n}\n\n// Implementation of `table.grow`.\n//\n// Table grow can invoke user code provided in a ResourceLimiter{,Async}, so we\n// need to catch a possible panic.\nunsafe fn table_grow(\n    vmctx: *mut VMContext,\n    table_index: u32,\n    delta: u32,\n    // NB: we don't know whether this is a pointer to a `VMCallerCheckedFuncRef`\n    // or is a `VMExternRef` until we look at the table type.\n    init_value: *mut u8,\n) -> Result<u32> {\n    let instance = (*vmctx).instance_mut();\n    let table_index = TableIndex::from_u32(table_index);\n    let element = match instance.table_element_type(table_index) {\n        TableElementType::Func => (init_value as *mut VMCallerCheckedFuncRef).into(),\n        TableElementType::Extern => {\n            let init_value = if init_value.is_null() {\n                None\n            } else {\n                Some(VMExternRef::clone_from_raw(init_value))\n            };\n            init_value.into()\n        }\n    };\n    Ok(match instance.table_grow(table_index, delta, element)? {\n        Some(r) => r,\n        None => -1_i32 as u32,\n    })\n}\n\nuse table_grow as table_grow_funcref;\nuse table_grow as table_grow_externref;\n\n// Implementation of `table.fill`.\nunsafe fn table_fill(\n    vmctx: *mut VMContext,\n    table_index: u32,\n    dst: u32,\n    // NB: we don't know whether this is a `VMExternRef` or a pointer to a\n    // `VMCallerCheckedFuncRef` until we look at the table's element type.\n    val: *mut u8,\n    len: u32,\n) -> Result<(), Trap> {\n    let instance = (*vmctx).instance_mut();\n    let table_index = TableIndex::from_u32(table_index);\n    let table = &mut *instance.get_table(table_index);\n    match table.element_type() {\n        TableElementType::Func => {\n            let val = val as *mut VMCallerCheckedFuncRef;\n            table.fill(dst, val.into(), len)\n        }\n        TableElementType::Extern => {\n            let val = if val.is_null() {\n                None\n            } else {\n                Some(VMExternRef::clone_from_raw(val))\n            };\n            table.fill(dst, val.into(), len)\n        }\n    }\n}\n\nuse table_fill as table_fill_funcref;\nuse table_fill as table_fill_externref;\n\n// Implementation of `table.copy`.\nunsafe fn table_copy(\n    vmctx: *mut VMContext,\n    dst_table_index: u32,\n    src_table_index: u32,\n    dst: u32,\n    src: u32,\n    len: u32,\n) -> Result<(), Trap> {\n    let dst_table_index = TableIndex::from_u32(dst_table_index);\n    let src_table_index = TableIndex::from_u32(src_table_index);\n    let instance = (*vmctx).instance_mut();\n    let dst_table = instance.get_table(dst_table_index);\n    // Lazy-initialize the whole range in the source table first.\n    let src_range = src..(src.checked_add(len).unwrap_or(u32::MAX));\n    let src_table = instance.get_table_with_lazy_init(src_table_index, src_range);\n    Table::copy(dst_table, src_table, dst, src, len)\n}\n\n// Implementation of `table.init`.\nunsafe fn table_init(\n    vmctx: *mut VMContext,\n    table_index: u32,\n    elem_index: u32,\n    dst: u32,\n    src: u32,\n    len: u32,\n) -> Result<(), Trap> {\n    let table_index = TableIndex::from_u32(table_index);\n    let elem_index = ElemIndex::from_u32(elem_index);\n    let instance = (*vmctx).instance_mut();\n    instance.table_init(table_index, elem_index, dst, src, len)\n}\n\n// Implementation of `elem.drop`.\nunsafe fn elem_drop(vmctx: *mut VMContext, elem_index: u32) {\n    let elem_index = ElemIndex::from_u32(elem_index);\n    let instance = (*vmctx).instance_mut();\n    instance.elem_drop(elem_index);\n}\n\n// Implementation of `memory.copy` for locally defined memories.\nunsafe fn memory_copy(\n    vmctx: *mut VMContext,\n    dst_index: u32,\n    dst: u64,\n    src_index: u32,\n    src: u64,\n    len: u64,\n) -> Result<(), Trap> {\n    let src_index = MemoryIndex::from_u32(src_index);\n    let dst_index = MemoryIndex::from_u32(dst_index);\n    let instance = (*vmctx).instance_mut();\n    instance.memory_copy(dst_index, dst, src_index, src, len)\n}\n\n// Implementation of `memory.fill` for locally defined memories.\nunsafe fn memory_fill(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    dst: u64,\n    val: u32,\n    len: u64,\n) -> Result<(), Trap> {\n    let memory_index = MemoryIndex::from_u32(memory_index);\n    let instance = (*vmctx).instance_mut();\n    instance.memory_fill(memory_index, dst, val as u8, len)\n}\n\n// Implementation of `memory.init`.\nunsafe fn memory_init(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    data_index: u32,\n    dst: u64,\n    src: u32,\n    len: u32,\n) -> Result<(), Trap> {\n    let memory_index = MemoryIndex::from_u32(memory_index);\n    let data_index = DataIndex::from_u32(data_index);\n    let instance = (*vmctx).instance_mut();\n    instance.memory_init(memory_index, data_index, dst, src, len)\n}\n\n// Implementation of `ref.func`.\nunsafe fn ref_func(vmctx: *mut VMContext, func_index: u32) -> *mut u8 {\n    let instance = (*vmctx).instance_mut();\n    let anyfunc = instance\n        .get_caller_checked_anyfunc(FuncIndex::from_u32(func_index))\n        .expect(\"ref_func: caller_checked_anyfunc should always be available for given func index\");\n    anyfunc as *mut _\n}\n\n// Implementation of `data.drop`.\nunsafe fn data_drop(vmctx: *mut VMContext, data_index: u32) {\n    let data_index = DataIndex::from_u32(data_index);\n    let instance = (*vmctx).instance_mut();\n    instance.data_drop(data_index)\n}\n\n// Returns a table entry after lazily initializing it.\nunsafe fn table_get_lazy_init_funcref(\n    vmctx: *mut VMContext,\n    table_index: u32,\n    index: u32,\n) -> *mut u8 {\n    let instance = (*vmctx).instance_mut();\n    let table_index = TableIndex::from_u32(table_index);\n    let table = instance.get_table_with_lazy_init(table_index, std::iter::once(index));\n    let elem = (*table)\n        .get(index)\n        .expect(\"table access already bounds-checked\");\n\n    elem.into_ref_asserting_initialized() as *mut _\n}\n\n// Drop a `VMExternRef`.\nunsafe fn drop_externref(_vmctx: *mut VMContext, externref: *mut u8) {\n    let externref = externref as *mut crate::externref::VMExternData;\n    let externref = NonNull::new(externref).unwrap();\n    crate::externref::VMExternData::drop_and_dealloc(externref);\n}\n\n// Do a GC and insert the given `externref` into the\n// `VMExternRefActivationsTable`.\nunsafe fn activations_table_insert_with_gc(vmctx: *mut VMContext, externref: *mut u8) {\n    let externref = VMExternRef::clone_from_raw(externref);\n    let instance = (*vmctx).instance();\n    let (activations_table, module_info_lookup) = (*instance.store()).externref_activations_table();\n\n    // Invariant: all `externref`s on the stack have an entry in the activations\n    // table. So we need to ensure that this `externref` is in the table\n    // *before* we GC, even though `insert_with_gc` will ensure that it is in\n    // the table *after* the GC. This technically results in one more hash table\n    // look up than is strictly necessary -- which we could avoid by having an\n    // additional GC method that is aware of these GC-triggering references --\n    // but it isn't really a concern because this is already a slow path.\n    activations_table.insert_without_gc(externref.clone());\n\n    activations_table.insert_with_gc(externref, module_info_lookup);\n}\n\n// Perform a Wasm `global.get` for `externref` globals.\nunsafe fn externref_global_get(vmctx: *mut VMContext, index: u32) -> *mut u8 {\n    let index = GlobalIndex::from_u32(index);\n    let instance = (*vmctx).instance();\n    let global = instance.defined_or_imported_global_ptr(index);\n    match (*global).as_externref().clone() {\n        None => ptr::null_mut(),\n        Some(externref) => {\n            let raw = externref.as_raw();\n            let (activations_table, module_info_lookup) =\n                (*instance.store()).externref_activations_table();\n            activations_table.insert_with_gc(externref, module_info_lookup);\n            raw\n        }\n    }\n}\n\n// Perform a Wasm `global.set` for `externref` globals.\nunsafe fn externref_global_set(vmctx: *mut VMContext, index: u32, externref: *mut u8) {\n    let externref = if externref.is_null() {\n        None\n    } else {\n        Some(VMExternRef::clone_from_raw(externref))\n    };\n\n    let index = GlobalIndex::from_u32(index);\n    let instance = (*vmctx).instance();\n    let global = instance.defined_or_imported_global_ptr(index);\n\n    // Swap the new `externref` value into the global before we drop the old\n    // value. This protects against an `externref` with a `Drop` implementation\n    // that calls back into Wasm and touches this global again (we want to avoid\n    // it observing a halfway-deinitialized value).\n    let old = mem::replace((*global).as_externref_mut(), externref);\n    drop(old);\n}\n\n// Implementation of `memory.atomic.notify` for locally defined memories.\nunsafe fn memory_atomic_notify(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    addr_index: u64,\n    count: u32,\n) -> Result<u32, Trap> {\n    let memory = MemoryIndex::from_u32(memory_index);\n    let instance = (*vmctx).instance_mut();\n    instance\n        .get_runtime_memory(memory)\n        .atomic_notify(addr_index, count)\n}\n\n// Implementation of `memory.atomic.wait32` for locally defined memories.\nunsafe fn memory_atomic_wait32(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    addr_index: u64,\n    expected: u32,\n    timeout: u64,\n) -> Result<u32, Trap> {\n    // convert timeout to Instant, before any wait happens on locking\n    let timeout = (timeout as i64 >= 0).then(|| Instant::now() + Duration::from_nanos(timeout));\n    let memory = MemoryIndex::from_u32(memory_index);\n    let instance = (*vmctx).instance_mut();\n    Ok(instance\n        .get_runtime_memory(memory)\n        .atomic_wait32(addr_index, expected, timeout)? as u32)\n}\n\n// Implementation of `memory.atomic.wait64` for locally defined memories.\nunsafe fn memory_atomic_wait64(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    addr_index: u64,\n    expected: u64,\n    timeout: u64,\n) -> Result<u32, Trap> {\n    // convert timeout to Instant, before any wait happens on locking\n    let timeout = (timeout as i64 >= 0).then(|| Instant::now() + Duration::from_nanos(timeout));\n    let memory = MemoryIndex::from_u32(memory_index);\n    let instance = (*vmctx).instance_mut();\n    Ok(instance\n        .get_runtime_memory(memory)\n        .atomic_wait64(addr_index, expected, timeout)? as u32)\n}\n\n// Hook for when an instance runs out of fuel.\nunsafe fn out_of_gas(vmctx: *mut VMContext) -> Result<()> {\n    (*(*vmctx).instance().store()).out_of_gas()\n}\n\n// Hook for when an instance observes that the epoch has changed.\nunsafe fn new_epoch(vmctx: *mut VMContext) -> Result<u64> {\n    (*(*vmctx).instance().store()).new_epoch()\n}\n\n/// This module contains functions which are used for resolving relocations at\n/// runtime if necessary.\n///\n/// These functions are not used by default and currently the only platform\n/// they're used for is on x86_64 when SIMD is disabled and then SSE features\n/// are further disabled. In these configurations Cranelift isn't allowed to use\n/// native CPU instructions so it falls back to libcalls and we rely on the Rust\n/// standard library generally for implementing these.\n#[allow(missing_docs)]\npub mod relocs {\n    pub extern \"C\" fn floorf32(f: f32) -> f32 {\n        f.floor()\n    }\n\n    pub extern \"C\" fn floorf64(f: f64) -> f64 {\n        f.floor()\n    }\n\n    pub extern \"C\" fn ceilf32(f: f32) -> f32 {\n        f.ceil()\n    }\n\n    pub extern \"C\" fn ceilf64(f: f64) -> f64 {\n        f.ceil()\n    }\n\n    pub extern \"C\" fn truncf32(f: f32) -> f32 {\n        f.trunc()\n    }\n\n    pub extern \"C\" fn truncf64(f: f64) -> f64 {\n        f.trunc()\n    }\n\n    const TOINT_32: f32 = 1.0 / f32::EPSILON;\n    const TOINT_64: f64 = 1.0 / f64::EPSILON;\n\n    // NB: replace with `round_ties_even` from libstd when it's stable as\n    // tracked by rust-lang/rust#96710\n    pub extern \"C\" fn nearestf32(x: f32) -> f32 {\n        // Rust doesn't have a nearest function; there's nearbyint, but it's not\n        // stabilized, so do it manually.\n        // Nearest is either ceil or floor depending on which is nearest or even.\n        // This approach exploited round half to even default mode.\n        let i = x.to_bits();\n        let e = i >> 23 & 0xff;\n        if e >= 0x7f_u32 + 23 {\n            // Check for NaNs.\n            if e == 0xff {\n                // Read the 23-bits significand.\n                if i & 0x7fffff != 0 {\n                    // Ensure it's arithmetic by setting the significand's most\n                    // significant bit to 1; it also works for canonical NaNs.\n                    return f32::from_bits(i | (1 << 22));\n                }\n            }\n            x\n        } else {\n            (x.abs() + TOINT_32 - TOINT_32).copysign(x)\n        }\n    }\n\n    pub extern \"C\" fn nearestf64(x: f64) -> f64 {\n        let i = x.to_bits();\n        let e = i >> 52 & 0x7ff;\n        if e >= 0x3ff_u64 + 52 {\n            // Check for NaNs.\n            if e == 0x7ff {\n                // Read the 52-bits significand.\n                if i & 0xfffffffffffff != 0 {\n                    // Ensure it's arithmetic by setting the significand's most\n                    // significant bit to 1; it also works for canonical NaNs.\n                    return f64::from_bits(i | (1 << 51));\n                }\n            }\n            x\n        } else {\n            (x.abs() + TOINT_64 - TOINT_64).copysign(x)\n        }\n    }\n\n    pub extern \"C\" fn fmaf32(a: f32, b: f32, c: f32) -> f32 {\n        a.mul_add(b, c)\n    }\n\n    pub extern \"C\" fn fmaf64(a: f64, b: f64, c: f64) -> f64 {\n        a.mul_add(b, c)\n    }\n}\n", "//! WebAssembly trap handling, which is built on top of the lower-level\n//! signalhandling mechanisms.\n\nmod backtrace;\n\nuse crate::{VMContext, VMRuntimeLimits};\nuse anyhow::Error;\nuse std::any::Any;\nuse std::cell::{Cell, UnsafeCell};\nuse std::mem::MaybeUninit;\nuse std::ptr;\nuse std::sync::Once;\n\npub use self::backtrace::{Backtrace, Frame};\npub use self::tls::{tls_eager_initialize, TlsRestore};\n\n#[link(name = \"wasmtime-helpers\")]\nextern \"C\" {\n    #[allow(improper_ctypes)]\n    fn wasmtime_setjmp(\n        jmp_buf: *mut *const u8,\n        callback: extern \"C\" fn(*mut u8, *mut VMContext),\n        payload: *mut u8,\n        callee: *mut VMContext,\n    ) -> i32;\n    fn wasmtime_longjmp(jmp_buf: *const u8) -> !;\n}\n\ncfg_if::cfg_if! {\n    if #[cfg(all(target_os = \"macos\", not(feature = \"posix-signals-on-macos\")))] {\n        mod macos;\n        use macos as sys;\n    } else if #[cfg(unix)] {\n        mod unix;\n        use unix as sys;\n    } else if #[cfg(target_os = \"windows\")] {\n        mod windows;\n        use windows as sys;\n    }\n}\n\npub use sys::SignalHandler;\n\n/// Globally-set callback to determine whether a program counter is actually a\n/// wasm trap.\n///\n/// This is initialized during `init_traps` below. The definition lives within\n/// `wasmtime` currently.\nstatic mut IS_WASM_PC: fn(usize) -> bool = |_| false;\n\n/// This function is required to be called before any WebAssembly is entered.\n/// This will configure global state such as signal handlers to prepare the\n/// process to receive wasm traps.\n///\n/// This function must not only be called globally once before entering\n/// WebAssembly but it must also be called once-per-thread that enters\n/// WebAssembly. Currently in wasmtime's integration this function is called on\n/// creation of a `Engine`.\n///\n/// The `is_wasm_pc` argument is used when a trap happens to determine if a\n/// program counter is the pc of an actual wasm trap or not. This is then used\n/// to disambiguate faults that happen due to wasm and faults that happen due to\n/// bugs in Rust or elsewhere.\npub fn init_traps(is_wasm_pc: fn(usize) -> bool) {\n    static INIT: Once = Once::new();\n    INIT.call_once(|| unsafe {\n        IS_WASM_PC = is_wasm_pc;\n        sys::platform_init();\n    });\n}\n\n/// Raises a trap immediately.\n///\n/// This function performs as-if a wasm trap was just executed. This trap\n/// payload is then returned from `catch_traps` below.\n///\n/// # Safety\n///\n/// Only safe to call when wasm code is on the stack, aka `catch_traps` must\n/// have been previously called. Additionally no Rust destructors can be on the\n/// stack. They will be skipped and not executed.\npub unsafe fn raise_trap(reason: TrapReason) -> ! {\n    tls::with(|info| info.unwrap().unwind_with(UnwindReason::Trap(reason)))\n}\n\n/// Raises a user-defined trap immediately.\n///\n/// This function performs as-if a wasm trap was just executed, only the trap\n/// has a dynamic payload associated with it which is user-provided. This trap\n/// payload is then returned from `catch_traps` below.\n///\n/// # Safety\n///\n/// Only safe to call when wasm code is on the stack, aka `catch_traps` must\n/// have been previously called. Additionally no Rust destructors can be on the\n/// stack. They will be skipped and not executed.\npub unsafe fn raise_user_trap(error: Error, needs_backtrace: bool) -> ! {\n    raise_trap(TrapReason::User {\n        error,\n        needs_backtrace,\n    })\n}\n\n/// Raises a trap from inside library code immediately.\n///\n/// This function performs as-if a wasm trap was just executed. This trap\n/// payload is then returned from `catch_traps` below.\n///\n/// # Safety\n///\n/// Only safe to call when wasm code is on the stack, aka `catch_traps` must\n/// have been previously called. Additionally no Rust destructors can be on the\n/// stack. They will be skipped and not executed.\npub unsafe fn raise_lib_trap(trap: wasmtime_environ::Trap) -> ! {\n    raise_trap(TrapReason::Wasm(trap))\n}\n\n/// Carries a Rust panic across wasm code and resumes the panic on the other\n/// side.\n///\n/// # Safety\n///\n/// Only safe to call when wasm code is on the stack, aka `catch_traps` must\n/// have been previously called. Additionally no Rust destructors can be on the\n/// stack. They will be skipped and not executed.\npub unsafe fn resume_panic(payload: Box<dyn Any + Send>) -> ! {\n    tls::with(|info| info.unwrap().unwind_with(UnwindReason::Panic(payload)))\n}\n\n/// Stores trace message with backtrace.\n#[derive(Debug)]\npub struct Trap {\n    /// Original reason from where this trap originated.\n    pub reason: TrapReason,\n    /// Wasm backtrace of the trap, if any.\n    pub backtrace: Option<Backtrace>,\n}\n\n/// Enumeration of different methods of raising a trap.\n#[derive(Debug)]\npub enum TrapReason {\n    /// A user-raised trap through `raise_user_trap`.\n    User {\n        /// The actual user trap error.\n        error: Error,\n        /// Whether we need to capture a backtrace for this error or not.\n        needs_backtrace: bool,\n    },\n\n    /// A trap raised from Cranelift-generated code.\n    Jit {\n        /// The program counter where this trap originated.\n        ///\n        /// This is later used with side tables from compilation to translate\n        /// the trapping address to a trap code.\n        pc: usize,\n\n        /// If the trap was a memory-related trap such as SIGSEGV then this\n        /// field will contain the address of the inaccessible data.\n        ///\n        /// Note that wasm loads/stores are not guaranteed to fill in this\n        /// information. Dynamically-bounds-checked memories, for example, will\n        /// not access an invalid address but may instead load from NULL or may\n        /// explicitly jump to a `ud2` instruction. This is only available for\n        /// fault-based traps which are one of the main ways, but not the only\n        /// way, to run wasm.\n        faulting_addr: Option<usize>,\n    },\n\n    /// A trap raised from a wasm libcall\n    Wasm(wasmtime_environ::Trap),\n}\n\nimpl TrapReason {\n    /// Create a new `TrapReason::User` that does not have a backtrace yet.\n    pub fn user_without_backtrace(error: Error) -> Self {\n        TrapReason::User {\n            error,\n            needs_backtrace: true,\n        }\n    }\n\n    /// Create a new `TrapReason::User` that already has a backtrace.\n    pub fn user_with_backtrace(error: Error) -> Self {\n        TrapReason::User {\n            error,\n            needs_backtrace: false,\n        }\n    }\n\n    /// Is this a JIT trap?\n    pub fn is_jit(&self) -> bool {\n        matches!(self, TrapReason::Jit { .. })\n    }\n}\n\nimpl From<Error> for TrapReason {\n    fn from(err: Error) -> Self {\n        TrapReason::user_without_backtrace(err)\n    }\n}\n\nimpl From<wasmtime_environ::Trap> for TrapReason {\n    fn from(code: wasmtime_environ::Trap) -> Self {\n        TrapReason::Wasm(code)\n    }\n}\n\n/// Catches any wasm traps that happen within the execution of `closure`,\n/// returning them as a `Result`.\n///\n/// Highly unsafe since `closure` won't have any dtors run.\npub unsafe fn catch_traps<'a, F>(\n    signal_handler: Option<*const SignalHandler<'static>>,\n    capture_backtrace: bool,\n    caller: *mut VMContext,\n    mut closure: F,\n) -> Result<(), Box<Trap>>\nwhere\n    F: FnMut(*mut VMContext),\n{\n    let limits = (*caller).instance().runtime_limits();\n\n    let result = CallThreadState::new(signal_handler, capture_backtrace, *limits).with(|cx| {\n        wasmtime_setjmp(\n            cx.jmp_buf.as_ptr(),\n            call_closure::<F>,\n            &mut closure as *mut F as *mut u8,\n            caller,\n        )\n    });\n\n    return match result {\n        Ok(x) => Ok(x),\n        Err((UnwindReason::Trap(reason), backtrace)) => Err(Box::new(Trap { reason, backtrace })),\n        Err((UnwindReason::Panic(panic), _)) => std::panic::resume_unwind(panic),\n    };\n\n    extern \"C\" fn call_closure<F>(payload: *mut u8, caller: *mut VMContext)\n    where\n        F: FnMut(*mut VMContext),\n    {\n        unsafe { (*(payload as *mut F))(caller) }\n    }\n}\n\n// Module to hide visibility of the `CallThreadState::prev` field and force\n// usage of its accessor methods.\nmod call_thread_state {\n    use super::*;\n    use std::mem;\n\n    /// Temporary state stored on the stack which is registered in the `tls` module\n    /// below for calls into wasm.\n    pub struct CallThreadState {\n        pub(super) unwind: UnsafeCell<MaybeUninit<(UnwindReason, Option<Backtrace>)>>,\n        pub(super) jmp_buf: Cell<*const u8>,\n        pub(super) signal_handler: Option<*const SignalHandler<'static>>,\n        pub(super) capture_backtrace: bool,\n\n        pub(crate) limits: *const VMRuntimeLimits,\n\n        prev: Cell<tls::Ptr>,\n\n        // The values of `VMRuntimeLimits::last_wasm_{exit_{pc,fp},entry_sp}` for\n        // the *previous* `CallThreadState`. Our *current* last wasm PC/FP/SP are\n        // saved in `self.limits`. We save a copy of the old registers here because\n        // the `VMRuntimeLimits` typically doesn't change across nested calls into\n        // Wasm (i.e. they are typically calls back into the same store and\n        // `self.limits == self.prev.limits`) and we must to maintain the list of\n        // contiguous-Wasm-frames stack regions for backtracing purposes.\n        old_last_wasm_exit_fp: Cell<usize>,\n        old_last_wasm_exit_pc: Cell<usize>,\n        old_last_wasm_entry_sp: Cell<usize>,\n    }\n\n    impl CallThreadState {\n        #[inline]\n        pub(super) fn new(\n            signal_handler: Option<*const SignalHandler<'static>>,\n            capture_backtrace: bool,\n            limits: *const VMRuntimeLimits,\n        ) -> CallThreadState {\n            CallThreadState {\n                unwind: UnsafeCell::new(MaybeUninit::uninit()),\n                jmp_buf: Cell::new(ptr::null()),\n                signal_handler,\n                capture_backtrace,\n                limits,\n                prev: Cell::new(ptr::null()),\n                old_last_wasm_exit_fp: Cell::new(0),\n                old_last_wasm_exit_pc: Cell::new(0),\n                old_last_wasm_entry_sp: Cell::new(0),\n            }\n        }\n\n        /// Get the saved FP upon exit from Wasm for the previous `CallThreadState`.\n        pub fn old_last_wasm_exit_fp(&self) -> usize {\n            self.old_last_wasm_exit_fp.get()\n        }\n\n        /// Get the saved PC upon exit from Wasm for the previous `CallThreadState`.\n        pub fn old_last_wasm_exit_pc(&self) -> usize {\n            self.old_last_wasm_exit_pc.get()\n        }\n\n        /// Get the saved SP upon entry into Wasm for the previous `CallThreadState`.\n        pub fn old_last_wasm_entry_sp(&self) -> usize {\n            self.old_last_wasm_entry_sp.get()\n        }\n\n        /// Get the previous `CallThreadState`.\n        pub fn prev(&self) -> tls::Ptr {\n            self.prev.get()\n        }\n\n        /// Connect the link to the previous `CallThreadState`.\n        ///\n        /// Synchronizes the last wasm FP, PC, and SP on `self` and the old\n        /// `self.prev` for the given new `prev`, and returns the old\n        /// `self.prev`.\n        pub unsafe fn set_prev(&self, prev: tls::Ptr) -> tls::Ptr {\n            let old_prev = self.prev.get();\n\n            // Restore the old `prev`'s saved registers in its\n            // `VMRuntimeLimits`. This is necessary for when we are async\n            // suspending the top `CallThreadState` and doing `set_prev(null)`\n            // on it, and so any stack walking we do subsequently will start at\n            // the old `prev` and look at its `VMRuntimeLimits` to get the\n            // initial saved registers.\n            if let Some(old_prev) = old_prev.as_ref() {\n                *(*old_prev.limits).last_wasm_exit_fp.get() = self.old_last_wasm_exit_fp();\n                *(*old_prev.limits).last_wasm_exit_pc.get() = self.old_last_wasm_exit_pc();\n                *(*old_prev.limits).last_wasm_entry_sp.get() = self.old_last_wasm_entry_sp();\n            }\n\n            self.prev.set(prev);\n\n            let mut old_last_wasm_exit_fp = 0;\n            let mut old_last_wasm_exit_pc = 0;\n            let mut old_last_wasm_entry_sp = 0;\n            if let Some(prev) = prev.as_ref() {\n                // We are entering a new `CallThreadState` or resuming a\n                // previously suspended one. This means we will push new Wasm\n                // frames that save the new Wasm FP/SP/PC registers into\n                // `VMRuntimeLimits`, we need to first save the old Wasm\n                // FP/SP/PC registers into this new `CallThreadState` to\n                // maintain our list of contiguous Wasm frame regions that we\n                // use when capturing stack traces.\n                //\n                // NB: the Wasm<--->host trampolines saved the Wasm FP/SP/PC\n                // registers in the active-at-that-time store's\n                // `VMRuntimeLimits`. For the most recent FP/PC/SP that is the\n                // `state.prev.limits` (since we haven't entered this\n                // `CallThreadState` yet). And that can be a different\n                // `VMRuntimeLimits` instance from the currently active\n                // `state.limits`, which will be used by the upcoming call into\n                // Wasm! Consider the case where we have multiple, nested calls\n                // across stores (with host code in between, by necessity, since\n                // only things in the same store can be linked directly\n                // together):\n                //\n                //     | ...             |\n                //     | Host            |  |\n                //     +-----------------+  | stack\n                //     | Wasm in store A |  | grows\n                //     +-----------------+  | down\n                //     | Host            |  |\n                //     +-----------------+  |\n                //     | Wasm in store B |  V\n                //     +-----------------+\n                //\n                // In this scenario `state.limits != state.prev.limits`,\n                // i.e. `B.limits != A.limits`! Therefore we must take care to\n                // read the old FP/SP/PC from `state.prev.limits`, rather than\n                // `state.limits`, and store those saved registers into the\n                // current `state`.\n                //\n                // See also the comment above the\n                // `CallThreadState::old_last_wasm_*` fields.\n                old_last_wasm_exit_fp =\n                    mem::replace(&mut *(*prev.limits).last_wasm_exit_fp.get(), 0);\n                old_last_wasm_exit_pc =\n                    mem::replace(&mut *(*prev.limits).last_wasm_exit_pc.get(), 0);\n                old_last_wasm_entry_sp =\n                    mem::replace(&mut *(*prev.limits).last_wasm_entry_sp.get(), 0);\n            }\n\n            self.old_last_wasm_exit_fp.set(old_last_wasm_exit_fp);\n            self.old_last_wasm_exit_pc.set(old_last_wasm_exit_pc);\n            self.old_last_wasm_entry_sp.set(old_last_wasm_entry_sp);\n\n            old_prev\n        }\n    }\n}\npub use call_thread_state::*;\n\nenum UnwindReason {\n    Panic(Box<dyn Any + Send>),\n    Trap(TrapReason),\n}\n\nimpl CallThreadState {\n    fn with(\n        mut self,\n        closure: impl FnOnce(&CallThreadState) -> i32,\n    ) -> Result<(), (UnwindReason, Option<Backtrace>)> {\n        let ret = tls::set(&mut self, |me| closure(me));\n        if ret != 0 {\n            Ok(())\n        } else {\n            Err(unsafe { self.read_unwind() })\n        }\n    }\n\n    #[cold]\n    unsafe fn read_unwind(&self) -> (UnwindReason, Option<Backtrace>) {\n        (*self.unwind.get()).as_ptr().read()\n    }\n\n    fn unwind_with(&self, reason: UnwindReason) -> ! {\n        let backtrace = match reason {\n            // Panics don't need backtraces. There is nowhere to attach the\n            // hypothetical backtrace to and it doesn't really make sense to try\n            // in the first place since this is a Rust problem rather than a\n            // Wasm problem.\n            UnwindReason::Panic(_)\n            // And if we are just propagating an existing trap that already has\n            // a backtrace attached to it, then there is no need to capture a\n            // new backtrace either.\n            | UnwindReason::Trap(TrapReason::User {\n                needs_backtrace: false,\n                ..\n            }) => None,\n            UnwindReason::Trap(_) => self.capture_backtrace(None),\n        };\n        unsafe {\n            (*self.unwind.get()).as_mut_ptr().write((reason, backtrace));\n            wasmtime_longjmp(self.jmp_buf.get());\n        }\n    }\n\n    /// Trap handler using our thread-local state.\n    ///\n    /// * `pc` - the program counter the trap happened at\n    /// * `call_handler` - a closure used to invoke the platform-specific\n    ///   signal handler for each instance, if available.\n    ///\n    /// Attempts to handle the trap if it's a wasm trap. Returns a few\n    /// different things:\n    ///\n    /// * null - the trap didn't look like a wasm trap and should continue as a\n    ///   trap\n    /// * 1 as a pointer - the trap was handled by a custom trap handler on an\n    ///   instance, and the trap handler should quickly return.\n    /// * a different pointer - a jmp_buf buffer to longjmp to, meaning that\n    ///   the wasm trap was succesfully handled.\n    #[cfg_attr(target_os = \"macos\", allow(dead_code))] // macOS is more raw and doesn't use this\n    fn take_jmp_buf_if_trap(\n        &self,\n        pc: *const u8,\n        call_handler: impl Fn(&SignalHandler) -> bool,\n    ) -> *const u8 {\n        // If we haven't even started to handle traps yet, bail out.\n        if self.jmp_buf.get().is_null() {\n            return ptr::null();\n        }\n\n        // First up see if any instance registered has a custom trap handler,\n        // in which case run them all. If anything handles the trap then we\n        // return that the trap was handled.\n        if let Some(handler) = self.signal_handler {\n            if unsafe { call_handler(&*handler) } {\n                return 1 as *const _;\n            }\n        }\n\n        // If this fault wasn't in wasm code, then it's not our problem\n        if unsafe { !IS_WASM_PC(pc as usize) } {\n            return ptr::null();\n        }\n\n        // If all that passed then this is indeed a wasm trap, so return the\n        // `jmp_buf` passed to `wasmtime_longjmp` to resume.\n        self.jmp_buf.replace(ptr::null())\n    }\n\n    fn set_jit_trap(&self, pc: *const u8, fp: usize, faulting_addr: Option<usize>) {\n        let backtrace = self.capture_backtrace(Some((pc as usize, fp)));\n        unsafe {\n            (*self.unwind.get()).as_mut_ptr().write((\n                UnwindReason::Trap(TrapReason::Jit {\n                    pc: pc as usize,\n                    faulting_addr,\n                }),\n                backtrace,\n            ));\n        }\n    }\n\n    fn capture_backtrace(&self, pc_and_fp: Option<(usize, usize)>) -> Option<Backtrace> {\n        if !self.capture_backtrace {\n            return None;\n        }\n\n        Some(unsafe { Backtrace::new_with_trap_state(self, pc_and_fp) })\n    }\n\n    pub(crate) fn iter<'a>(&'a self) -> impl Iterator<Item = &Self> + 'a {\n        let mut state = Some(self);\n        std::iter::from_fn(move || {\n            let this = state?;\n            state = unsafe { this.prev().as_ref() };\n            Some(this)\n        })\n    }\n}\n\nstruct ResetCell<'a, T: Copy>(&'a Cell<T>, T);\n\nimpl<T: Copy> Drop for ResetCell<'_, T> {\n    #[inline]\n    fn drop(&mut self) {\n        self.0.set(self.1);\n    }\n}\n\n// A private inner module for managing the TLS state that we require across\n// calls in wasm. The WebAssembly code is called from C++ and then a trap may\n// happen which requires us to read some contextual state to figure out what to\n// do with the trap. This `tls` module is used to persist that information from\n// the caller to the trap site.\nmod tls {\n    use super::CallThreadState;\n    use std::ptr;\n\n    pub use raw::Ptr;\n\n    // An even *more* inner module for dealing with TLS. This actually has the\n    // thread local variable and has functions to access the variable.\n    //\n    // Note that this is specially done to fully encapsulate that the accessors\n    // for tls may or may not be inlined. Wasmtime's async support employs stack\n    // switching which can resume execution on different OS threads. This means\n    // that borrows of our TLS pointer must never live across accesses because\n    // otherwise the access may be split across two threads and cause unsafety.\n    //\n    // This also means that extra care is taken by the runtime to save/restore\n    // these TLS values when the runtime may have crossed threads.\n    //\n    // Note, though, that if async support is disabled at compile time then\n    // these functions are free to be inlined.\n    mod raw {\n        use super::CallThreadState;\n        use std::cell::Cell;\n        use std::ptr;\n\n        pub type Ptr = *const CallThreadState;\n\n        // The first entry here is the `Ptr` which is what's used as part of the\n        // public interface of this module. The second entry is a boolean which\n        // allows the runtime to perform per-thread initialization if necessary\n        // for handling traps (e.g. setting up ports on macOS and sigaltstack on\n        // Unix).\n        thread_local!(static PTR: Cell<(Ptr, bool)> = const { Cell::new((ptr::null(), false)) });\n\n        #[cfg_attr(feature = \"async\", inline(never))] // see module docs\n        #[cfg_attr(not(feature = \"async\"), inline)]\n        pub fn replace(val: Ptr) -> Ptr {\n            PTR.with(|p| {\n                // When a new value is configured that means that we may be\n                // entering WebAssembly so check to see if this thread has\n                // performed per-thread initialization for traps.\n                let (prev, initialized) = p.get();\n                if !initialized {\n                    super::super::sys::lazy_per_thread_init();\n                }\n                p.set((val, true));\n                prev\n            })\n        }\n\n        /// Eagerly initialize thread-local runtime functionality. This will be performed\n        /// lazily by the runtime if users do not perform it eagerly.\n        #[cfg_attr(feature = \"async\", inline(never))] // see module docs\n        #[cfg_attr(not(feature = \"async\"), inline)]\n        pub fn initialize() {\n            PTR.with(|p| {\n                let (state, initialized) = p.get();\n                if initialized {\n                    return;\n                }\n                super::super::sys::lazy_per_thread_init();\n                p.set((state, true));\n            })\n        }\n\n        #[cfg_attr(feature = \"async\", inline(never))] // see module docs\n        #[cfg_attr(not(feature = \"async\"), inline)]\n        pub fn get() -> Ptr {\n            PTR.with(|p| p.get().0)\n        }\n    }\n\n    pub use raw::initialize as tls_eager_initialize;\n\n    /// Opaque state used to help control TLS state across stack switches for\n    /// async support.\n    pub struct TlsRestore {\n        state: raw::Ptr,\n    }\n\n    impl TlsRestore {\n        /// Takes the TLS state that is currently configured and returns a\n        /// token that is used to replace it later.\n        ///\n        /// This is not a safe operation since it's intended to only be used\n        /// with stack switching found with fibers and async wasmtime.\n        pub unsafe fn take() -> TlsRestore {\n            // Our tls pointer must be set at this time, and it must not be\n            // null. We need to restore the previous pointer since we're\n            // removing ourselves from the call-stack, and in the process we\n            // null out our own previous field for safety in case it's\n            // accidentally used later.\n            let state = raw::get();\n            if let Some(state) = state.as_ref() {\n                let prev_state = state.set_prev(ptr::null());\n                raw::replace(prev_state);\n            } else {\n                // Null case: we aren't in a wasm context, so theres no tls to\n                // save for restoration.\n            }\n\n            TlsRestore { state }\n        }\n\n        /// Restores a previous tls state back into this thread's TLS.\n        ///\n        /// This is unsafe because it's intended to only be used within the\n        /// context of stack switching within wasmtime.\n        pub unsafe fn replace(self) {\n            // Null case: we aren't in a wasm context, so theres no tls\n            // to restore.\n            if self.state.is_null() {\n                return;\n            }\n\n            // We need to configure our previous TLS pointer to whatever is in\n            // TLS at this time, and then we set the current state to ourselves.\n            let prev = raw::get();\n            assert!((*self.state).prev().is_null());\n            (*self.state).set_prev(prev);\n            raw::replace(self.state);\n        }\n    }\n\n    /// Configures thread local state such that for the duration of the\n    /// execution of `closure` any call to `with` will yield `state`, unless\n    /// this is recursively called again.\n    #[inline]\n    pub fn set<R>(state: &mut CallThreadState, closure: impl FnOnce(&CallThreadState) -> R) -> R {\n        struct Reset<'a> {\n            state: &'a CallThreadState,\n        }\n\n        impl Drop for Reset<'_> {\n            #[inline]\n            fn drop(&mut self) {\n                unsafe {\n                    let prev = self.state.set_prev(ptr::null());\n                    let old_state = raw::replace(prev);\n                    debug_assert!(std::ptr::eq(old_state, self.state));\n                }\n            }\n        }\n\n        let prev = raw::replace(state);\n\n        unsafe {\n            state.set_prev(prev);\n\n            let reset = Reset { state };\n            closure(reset.state)\n        }\n    }\n\n    /// Returns the last pointer configured with `set` above, if any.\n    pub fn with<R>(closure: impl FnOnce(Option<&CallThreadState>) -> R) -> R {\n        let p = raw::get();\n        unsafe { closure(if p.is_null() { None } else { Some(&*p) }) }\n    }\n}\n"], "fixing_code": ["--------------------------------------------------------------------------------\n\n## 9.0.0\n\nUnreleased.\n\n### Added\n\n### Changed\n\n--------------------------------------------------------------------------------\n\n## 8.0.1\n\nReleased 2023-04-27.\n\n### Changed\n\n* Breaking: Files opened using Wasmtime's implementation of WASI on Windows now\n  cannot be deleted until the file handle is closed. This was already true for\n  open directories. The change was necessary for the bug fix in\n  [#6163](https://github.com/bytecodealliance/wasmtime/pull/6163).\n\n### Fixed\n\n* Fixed wasi-common's implementation of the `O_DIRECTORY` flag to match POSIX.\n  [#6163](https://github.com/bytecodealliance/wasmtime/pull/6163)\n\n* Undefined Behavior in Rust runtime functions\n  [GHSA-ch89-5g45-qwc7](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ch89-5g45-qwc7)\n\n--------------------------------------------------------------------------------\n\n## 8.0.0\n\nReleased 2023-04-20\n\n### Added\n\n* Allow the MPL-2.0 and OpenSSL licenses in dependencies of wasmtime.\n  [#6136](https://github.com/bytecodealliance/wasmtime/pull/6136)\n\n* Add a bounds-checking optimization for dynamic memories and guard pages.\n  [#6031](https://github.com/bytecodealliance/wasmtime/pull/6031)\n\n* Add support for generating perf maps for simple perf profiling. Additionally,\n  the `--jitdump` and `--vtune` flags have been replaced with a single\n  `--profile` flags that accepts `perfmap`, `jitdump`, and `vtune` arguments.\n  [#6030](https://github.com/bytecodealliance/wasmtime/pull/6030)\n\n* Validate faulting addresses are valid to fault on. As a mitigation to CVEs\n  like `GHSA-ff4p-7xrq-q5r8`, check that the address involved in a fault is one\n  that could be contained in a `Store`, or print a scary message and abort\n  immediately.\n  [#6028](https://github.com/bytecodealliance/wasmtime/pull/6028)\n\n* Add the `--default-values-unknown-imports` option to define unknown function\n  imports as functions that return the default value for their result type.\n  [#6010](https://github.com/bytecodealliance/wasmtime/pull/6010)\n\n* Add `Clone` for `component::InstancePre`.\n  [#5996](https://github.com/bytecodealliance/wasmtime/issues/5996)\n\n* Add `--dynamic-memory-reserved-for-growth` cli flag.\n  [#5980](https://github.com/bytecodealliance/wasmtime/issues/5980)\n\n* Introduce the `wasmtime-explorer` crate for investigating the compilation of\n  wasm modules. This functionality is also exposed via the `wasmtime explore`\n  command.\n  [#5975](https://github.com/bytecodealliance/wasmtime/pull/5975)\n\n* Added support for the Relaxed SIMD proposal.\n  [#5892](https://github.com/bytecodealliance/wasmtime/pull/5892)\n\n* Cranelift gained many new machine-independent optimizations.\n  [#5909](https://github.com/bytecodealliance/wasmtime/pull/5909)\n  [#6032](https://github.com/bytecodealliance/wasmtime/pull/6032)\n  [#6033](https://github.com/bytecodealliance/wasmtime/pull/6033)\n  [#6034](https://github.com/bytecodealliance/wasmtime/pull/6034)\n  [#6037](https://github.com/bytecodealliance/wasmtime/pull/6037)\n  [#6052](https://github.com/bytecodealliance/wasmtime/pull/6052)\n  [#6053](https://github.com/bytecodealliance/wasmtime/pull/6053)\n  [#6072](https://github.com/bytecodealliance/wasmtime/pull/6072)\n  [#6095](https://github.com/bytecodealliance/wasmtime/pull/6095)\n  [#6130](https://github.com/bytecodealliance/wasmtime/pull/6130)\n\n### Changed\n\n* Derive `Copy` on `wasmtime::ValType`.\n  [#6138](https://github.com/bytecodealliance/wasmtime/pull/6138)\n\n* Make `StoreContextMut` accessible in the epoch deadline callback.\n  [#6075](https://github.com/bytecodealliance/wasmtime/pull/6075)\n\n* Take SIGFPE signals for divide traps on `x86_64`.\n  [#6026](https://github.com/bytecodealliance/wasmtime/pull/6026)\n\n* Use more specialized AVX instructions in the `x86_64` backend.\n  [#5924](https://github.com/bytecodealliance/wasmtime/pull/5924)\n  [#5930](https://github.com/bytecodealliance/wasmtime/pull/5930)\n  [#5931](https://github.com/bytecodealliance/wasmtime/pull/5931)\n  [#5982](https://github.com/bytecodealliance/wasmtime/pull/5982)\n  [#5986](https://github.com/bytecodealliance/wasmtime/pull/5986)\n  [#5999](https://github.com/bytecodealliance/wasmtime/pull/5999)\n  [#6023](https://github.com/bytecodealliance/wasmtime/pull/6023)\n  [#6025](https://github.com/bytecodealliance/wasmtime/pull/6025)\n  [#6060](https://github.com/bytecodealliance/wasmtime/pull/6060)\n  [#6086](https://github.com/bytecodealliance/wasmtime/pull/6086)\n  [#6092](https://github.com/bytecodealliance/wasmtime/pull/6092)\n\n* Generate more cache-friendly code for traps.\n  [#6011](https://github.com/bytecodealliance/wasmtime/pull/6011)\n\n### Fixed\n\n* Fixed suboptimal code generation in the `aarch64` backend.\n  [#5976](https://github.com/bytecodealliance/wasmtime/pull/5976)\n  [#5977](https://github.com/bytecodealliance/wasmtime/pull/5977)\n  [#5987](https://github.com/bytecodealliance/wasmtime/pull/5987)\n  [#5997](https://github.com/bytecodealliance/wasmtime/pull/5997)\n  [#6078](https://github.com/bytecodealliance/wasmtime/pull/6078)\n\n* Fixed suboptimal code generation in the `riscv64` backend.\n  [#5854](https://github.com/bytecodealliance/wasmtime/pull/5854)\n  [#5857](https://github.com/bytecodealliance/wasmtime/pull/5857)\n  [#5919](https://github.com/bytecodealliance/wasmtime/pull/5919)\n  [#5951](https://github.com/bytecodealliance/wasmtime/pull/5951)\n  [#5964](https://github.com/bytecodealliance/wasmtime/pull/5964)\n  [#6087](https://github.com/bytecodealliance/wasmtime/pull/6087)\n\n\n--------------------------------------------------------------------------------\n\n## 7.0.1\n\nReleased 2023-04-27.\n\n### Fixed\n\n* Undefined Behavior in Rust runtime functions\n  [GHSA-ch89-5g45-qwc7](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ch89-5g45-qwc7)\n\n--------------------------------------------------------------------------------\n\n## 7.0.0\n\nReleased 2023-03-20\n\n### Added\n\n* An initial implementation of the wasi-threads proposal has been implemented\n  and landed in the Wasmtime CLI. This is available behind a\n  `--wasi-modules experimental-wasi-threads` flag.\n  [#5484](https://github.com/bytecodealliance/wasmtime/pull/5484)\n\n* Support for WASI sockets has been added to the C API.\n  [#5624](https://github.com/bytecodealliance/wasmtime/pull/5624)\n\n* Support for limiting `Store`-based resource usage, such as memory, tables,\n  etc, has been added to the C API.\n  [#5761](https://github.com/bytecodealliance/wasmtime/pull/5761)\n\n* A top level alias of `anyhow::Result` as `wasmtime::Result` has been added to\n  avoid the need to explicitly depend on `anyhow`.\n  [#5853](https://github.com/bytecodealliance/wasmtime/pull/5853)\n\n* Initial support for the WebAssembly core dump format has been added to the CLI\n  with a `--coredump-on-trap` flag.\n  [#5868](https://github.com/bytecodealliance/wasmtime/pull/5868)\n\n### Changed\n\n* The `S` type parameter on component-related methods has been removed.\n  [#5722](https://github.com/bytecodealliance/wasmtime/pull/5722)\n\n* Selection of a `world` to bindgen has been updated to select any `default\n  world` in a WIT package if there is only one.\n  [#5779](https://github.com/bytecodealliance/wasmtime/pull/5779)\n\n* WASI preopened file descriptors can now be closed.\n  [#5828](https://github.com/bytecodealliance/wasmtime/pull/5828)\n\n* The host traits generated by the `bindgen!` macro are now always named `Host`,\n  but are still scoped to each individual module.\n  [#5890](https://github.com/bytecodealliance/wasmtime/pull/5890)\n\n### Fixed\n\n* Components which have `type` imports are now supported better and error/panic\n  in fewer cases.\n  [#5777](https://github.com/bytecodealliance/wasmtime/pull/5777)\n\n* Types referred to by `wasmtime::component::Val` are now reexported under\n  `wasmtime::component`.\n  [#5790](https://github.com/bytecodealliance/wasmtime/pull/5790)\n\n* A panic due to a race between `memory.atomic.{wait32,wait64,notify}`\n  instructions has been fixed.\n  [#5871](https://github.com/bytecodealliance/wasmtime/pull/5871)\n\n* Guest-controlled out-of-bounds read/write on x86\\_64\n  [GHSA-ff4p-7xrq-q5r8](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8)\n\n*  Miscompilation of `i8x16.select` with the same inputs on x86\\_64\n  [GHSA-xm67-587q-r2vw](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw)\n\n--------------------------------------------------------------------------------\n\n## 6.0.2\n\nReleased 2023-04-27.\n\n### Fixed\n\n* Undefined Behavior in Rust runtime functions\n  [GHSA-ch89-5g45-qwc7](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ch89-5g45-qwc7)\n\n--------------------------------------------------------------------------------\n\n## 6.0.1\n\nReleased 2023-03-08.\n\n### Fixed\n\n* Guest-controlled out-of-bounds read/write on x86\\_64\n  [GHSA-ff4p-7xrq-q5r8](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8)\n\n*  Miscompilation of `i8x16.select` with the same inputs on x86\\_64\n  [GHSA-xm67-587q-r2vw](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw)\n\n--------------------------------------------------------------------------------\n\n## 6.0.0\n\nReleased 2023-02-20\n\n### Added\n\n* Wasmtime's built-in cache can now be disabled after being enabled previously.\n  [#5542](https://github.com/bytecodealliance/wasmtime/pull/5542)\n\n* Older x86\\_64 CPUs, without SSE4.1 for example, are now supported when the\n  wasm SIMD proposal is disabled.\n  [#5567](https://github.com/bytecodealliance/wasmtime/pull/5567)\n\n* The Wasmtime C API now has `WASMTIME_VERSION_*` macros defined in its header\n  files.\n  [#5651](https://github.com/bytecodealliance/wasmtime/pull/5651)\n\n* The `wasmtime` CLI executable as part of Wasmtime's precompiled release\n  artifacts now has the `all-arch` feature enabled.\n  [#5657](https://github.com/bytecodealliance/wasmtime/pull/5657)\n\n### Changed\n\n* Equality of `wasmtime::component::Val::Float{32,64}` now considers NaNs as\n  equal for assistance when fuzzing.\n  [#5535](https://github.com/bytecodealliance/wasmtime/pull/5535)\n\n* WIT syntax supported by `wasmtime::component::bindgen!` has been updated in\n  addition to the generated code being updated.\n  [#5565](https://github.com/bytecodealliance/wasmtime/pull/5565)\n  [#5692](https://github.com/bytecodealliance/wasmtime/pull/5692)\n  [#5694](https://github.com/bytecodealliance/wasmtime/pull/5694)\n\n* Cranelift's egraph-based optimization framework is now enabled by default.\n  [#5587](https://github.com/bytecodealliance/wasmtime/pull/5587)\n\n* The old `PoolingAllocationStrategy` type has been removed in favor of a more\n  flexible configuration via a new option\n  `PoolingAllocationConfig::max_unused_warm_slots` which is more flexible and\n  subsumes the previous use cases for each strategy.\n  [#5661](https://github.com/bytecodealliance/wasmtime/pull/5661)\n\n* Creation of `InstancePre` through `Linker::instantiate_pre` no longer requires\n  a `Store` to be provided. Instead a `Store`-related argument is now required\n  on `Linker::define`-style APIs instead.\n  [#5683](https://github.com/bytecodealliance/wasmtime/pull/5683)\n\n### Fixed\n\n* Compilation for FreeBSD on x86\\_64 and AArch64 has been fixed.\n  [#5606](https://github.com/bytecodealliance/wasmtime/pull/5606)\n\n--------------------------------------------------------------------------------\n\n## 5.0.1\n\nReleased 2023-03-08.\n\n### Fixed\n\n* Guest-controlled out-of-bounds read/write on x86\\_64\n  [GHSA-ff4p-7xrq-q5r8](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8)\n\n*  Miscompilation of `i8x16.select` with the same inputs on x86\\_64\n  [GHSA-xm67-587q-r2vw](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw)\n\n--------------------------------------------------------------------------------\n\n## 5.0.0\n\nReleased 2023-01-20\n\n### Added\n\n* A `wasmtime::component::bingen!` macro has been added for generating bindings\n  from `*.wit` files. Note that WIT is still heavily in development so this is\n  more of a preview of what will be as opposed to a finished feature.\n  [#5317](https://github.com/bytecodealliance/wasmtime/pull/5317)\n  [#5397](https://github.com/bytecodealliance/wasmtime/pull/5397)\n\n* The `wasmtime settings` CLI command now has a `--json` option for\n  machine-readable output.\n  [#5411](https://github.com/bytecodealliance/wasmtime/pull/5411)\n\n* Wiggle-generated bindings can now generate the trait for either `&mut self` or\n  `&self`.\n  [#5428](https://github.com/bytecodealliance/wasmtime/pull/5428)\n\n* The `wiggle` crate has more convenience APIs for working with guest data\n  that resides in shared memory.\n  [#5471](https://github.com/bytecodealliance/wasmtime/pull/5471)\n  [#5475](https://github.com/bytecodealliance/wasmtime/pull/5475)\n\n### Changed\n\n* Cranelift's egraph support has been rewritten and updated. This functionality\n  is still gated behind a flag and may become the default in the next release.\n  [#5382](https://github.com/bytecodealliance/wasmtime/pull/5382)\n\n* The implementation of codegen for WebAssembly linear memory has changed\n  significantly internally in Cranelift, moving more responsibility to the\n  Wasmtime embedding rather than Cranelift itself. This should have no\n  user-visible change, however.\n  [#5386](https://github.com/bytecodealliance/wasmtime/pull/5386)\n\n* The `Val::Float32` and `Val::Float64` variants for components now store `f32`\n  and `f64` instead of the bit representation.\n  [#5510](https://github.com/bytecodealliance/wasmtime/pull/5510)\n\n### Fixed\n\n* Handling of DWARF debugging information in components with multiple modules\n  has been fixed to ensure the right info is used for each module.\n  [#5358](https://github.com/bytecodealliance/wasmtime/pull/5358)\n\n--------------------------------------------------------------------------------\n\n## 4.0.1\n\nReleased 2023-03-08.\n\n### Fixed\n\n* Guest-controlled out-of-bounds read/write on x86\\_64\n  [GHSA-ff4p-7xrq-q5r8](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8)\n\n*  Miscompilation of `i8x16.select` with the same inputs on x86\\_64\n  [GHSA-xm67-587q-r2vw](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw)\n\n--------------------------------------------------------------------------------\n\n## 4.0.0\n\nReleased 2022-12-20\n\n### Added\n\n* Dynamic memories are now supported with the pooling instance allocator which\n  can possibly reduce the number of page faults throughout execution at the cost\n  of slower to run code. Page faults are primarily reduced by avoiding\n  releasing memory back to the system, relying on bounds checks to keep the\n  memory inaccessible.\n  [#5208](https://github.com/bytecodealliance/wasmtime/pull/5208)\n\n* The `wiggle` generator now supports function-level control over `tracing`\n  calls.\n  [#5194](https://github.com/bytecodealliance/wasmtime/pull/5194)\n\n* Support has been added to `wiggle` to be compatible with shared memories.\n  [#5225](https://github.com/bytecodealliance/wasmtime/pull/5225)\n  [#5229](https://github.com/bytecodealliance/wasmtime/pull/5229)\n  [#5264](https://github.com/bytecodealliance/wasmtime/pull/5264)\n  [#5268](https://github.com/bytecodealliance/wasmtime/pull/5268)\n  [#5054](https://github.com/bytecodealliance/wasmtime/pull/5054)\n\n* The `wiggle` generator now supports a \"trappable error\" configuration to\n  improve error conversions to guest errors and ensure that no host errors are\n  forgotten or accidentally become traps. The `wasi-common` crate has been\n  updated to use this.\n  [#5276](https://github.com/bytecodealliance/wasmtime/pull/5276)\n  [#5279](https://github.com/bytecodealliance/wasmtime/pull/5279)\n\n* The `memory.atomic.{notify,wait32,wait64}` instructions are now all\n  implemented in Wasmtime.\n  [#5255](https://github.com/bytecodealliance/wasmtime/pull/5255)\n  [#5311](https://github.com/bytecodealliance/wasmtime/pull/5311)\n\n* A `wasm_config_parallel_compilation_set` configuration function has been added\n  to the C API.\n  [#5298](https://github.com/bytecodealliance/wasmtime/pull/5298)\n\n* The `wasmtime` CLI can have its input module piped into it from stdin now.\n  [#5342](https://github.com/bytecodealliance/wasmtime/pull/5342)\n\n* `WasmBacktrace::{capture,force_capture}` methods have been added to\n  programmatically capture a backtrace outside of a trapping context.\n  [#5341](https://github.com/bytecodealliance/wasmtime/pull/5341)\n\n### Changed\n\n* The `S` type parameter on `Func::typed` and `Instance::get_typed_func` has\n  been removed and no longer needs to be specified.\n  [#5275](https://github.com/bytecodealliance/wasmtime/pull/5275)\n\n* The `SharedMemory::data` method now returns `&[UnsafeCell<u8>]` instead of the\n  prior raw slice return.\n  [#5240](https://github.com/bytecodealliance/wasmtime/pull/5240)\n\n* Creation of a `WasiCtx` will no longer unconditionally acquire randomness from\n  the OS, instead using the `rand::thread_rng()` function in Rust which is only\n  periodically reseeded with randomness from the OS.\n  [#5244](https://github.com/bytecodealliance/wasmtime/pull/5244)\n\n* Codegen of dynamically-bounds-checked wasm memory accesses has been improved.\n  [#5190](https://github.com/bytecodealliance/wasmtime/pull/5190)\n\n* Wasmtime will now emit inline stack probes in generated functions for x86\\_64,\n  aarch64, and riscv64 architectures. This guarantees a process abort if an\n  engine was misconfigured to give wasm too much stack instead of optionally\n  allowing wasm to skip the guard page.\n  [#5350](https://github.com/bytecodealliance/wasmtime/pull/5350)\n  [#5353](https://github.com/bytecodealliance/wasmtime/pull/5353)\n\n### Fixed\n\n* Dropping a `Module` will now release kernel resources in-use by the pooling\n  allocator when enabled instead of waiting for a new instance to be\n  re-instantiated into prior slots.\n  [#5321](https://github.com/bytecodealliance/wasmtime/pull/5321)\n\n--------------------------------------------------------------------------------\n\n## 3.0.1\n\nReleased 2022-12-01.\n\n### Fixed\n\n* The instruction cache is now flushed for AArch64 Android.\n  [#5331](https://github.com/bytecodealliance/wasmtime/pull/5331)\n\n* Building for FreeBSD and Android has been fixed.\n  [#5323](https://github.com/bytecodealliance/wasmtime/pull/5323)\n\n--------------------------------------------------------------------------------\n\n## 3.0.0\n\nReleased 2022-11-21\n\n### Added\n\n* New `WasiCtx::{push_file, push_dir}` methods exist for embedders to add their\n  own objects.\n  [#5027](https://github.com/bytecodealliance/wasmtime/pull/5027)\n\n* Wasmtime's `component-model` support now supports `async` host functions and\n  embedding in the same manner as core wasm.\n  [#5055](https://github.com/bytecodealliance/wasmtime/pull/5055)\n\n* The `wasmtime` CLI executable now supports a `--max-wasm-stack` flag.\n  [#5156](https://github.com/bytecodealliance/wasmtime/pull/5156)\n\n* AOT compilation support has been implemented for components (aka the\n  `component-model` feature of the Wasmtime crate).\n  [#5160](https://github.com/bytecodealliance/wasmtime/pull/5160)\n\n* A new `wasi_config_set_stdin_bytes` function is available in the C API to set\n  the stdin of a WASI-using module from an in-memory slice.\n  [#5179](https://github.com/bytecodealliance/wasmtime/pull/5179)\n\n* When using the pooling allocator there are now options to reset memory with\n  `memset` instead of `madvisev` on Linux to keep pages resident in memory to\n  reduce page faults when reusing linear memory slots.\n  [#5207](https://github.com/bytecodealliance/wasmtime/pull/5207)\n\n### Changed\n\n* Consuming 0 fuel with 0 fuel left is now considered to succeed. Additionally a\n  store may not consume its last unit of fuel.\n  [#5013](https://github.com/bytecodealliance/wasmtime/pull/5013)\n\n* A number of variants in the `wasi_common::ErrorKind` enum have been removed.\n  [#5015](https://github.com/bytecodealliance/wasmtime/pull/5015)\n\n* Methods on `WasiDir` now error-by-default instead of requiring a definition by\n  default.\n  [#5019](https://github.com/bytecodealliance/wasmtime/pull/5019)\n\n* Bindings generated by the `wiggle` crate now always depend on the `wasmtime`\n  crate meaning crates like `wasi-common` no longer compile for platforms such\n  as `wasm32-unknown-emscripten`.\n  [#5137](https://github.com/bytecodealliance/wasmtime/pull/5137)\n\n* Error handling in the `wasmtime` crate's API has been changed to primarily\n  work with `anyhow::Error` for custom errors. The `Trap` type has been replaced\n  with a simple `enum Trap { ... }` and backtrace information is now stored as a\n  `WasmBacktrace` type inserted as context into an `anyhow::Error`.\n  Host-functions are expected to return `anyhow::Result<T>` instead of the prior\n  `Trap` error return from before. Additionally the old `Trap::i32_exit`\n  constructor is now a concrete `wasi_commont::I32Exit` type which can be tested\n  for with a `downcast_ref` on the error returned from Wasmtime.\n  [#5149](https://github.com/bytecodealliance/wasmtime/pull/5149)\n\n* Configuration of the pooling allocator is now done through a builder-style\n  `PoolingAllocationConfig` API instead of the prior enum-variant API.\n  [#5205](https://github.com/bytecodealliance/wasmtime/pull/5205)\n\n### Fixed\n\n* The instruction cache is now properly flushed for AArch64 on Windows.\n  [#4997](https://github.com/bytecodealliance/wasmtime/pull/4997)\n\n* Backtrace capturing with many sequences of wasm->host calls on the stack no\n  longer exhibit quadratic capturing behavior.\n  [#5049](https://github.com/bytecodealliance/wasmtime/pull/5049)\n\n--------------------------------------------------------------------------------\n\n## 2.0.2\n\nReleased 2022-11-10.\n\n### Fixed\n\n* [CVE-2022-39392] - modules may perform out-of-bounds reads/writes when the\n  pooling allocator was configured with `memory_pages: 0`.\n\n* [CVE-2022-39393] - data can be leaked between instances when using the pooling\n  allocator.\n\n* [CVE-2022-39394] - An incorrect Rust signature for the C API\n  `wasmtime_trap_code` function could lead to an out-of-bounds write of three\n  zero bytes.\n\n[CVE-2022-39392]: https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-44mr-8vmm-wjhg\n[CVE-2022-39393]: https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-wh6w-3828-g9qf\n[CVE-2022-39394]: https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-h84q-m8rr-3v9q\n\n--------------------------------------------------------------------------------\n\n## 2.0.1\n\nReleased 2022-10-27.\n\n### Fixed\n\n* A compilation error when building only the `wasmtime` crate on Windows with\n  only the default features enabled has been fixed.\n  [#5134](https://github.com/bytecodealliance/wasmtime/pull/5134)\n\n### Changed\n\n* The `rayon` dependency added to `cranelift-isle` in 2.0.0 has been removed to\n  improve the compile time of the `cranelift-codegen` crate.\n  [#5101](https://github.com/bytecodealliance/wasmtime/pull/5101)\n\n--------------------------------------------------------------------------------\n\n## 2.0.0\n\nReleased 2022-10-20\n\n### Added\n\n* Cranelift has gained support for forward-edge CFI on the AArch64 backend.\n  [#3693](https://github.com/bytecodealliance/wasmtime/pull/3693)\n\n* A `--disable-parallel-compilation` CLI flag is now implemented for `wasmtime`.\n  [#4911](https://github.com/bytecodealliance/wasmtime/pull/4911)\n\n* [Tier 3] support has been added for for RISC-V 64 with a new backend in\n  Cranelift for this architecture.\n  [#4271](https://github.com/bytecodealliance/wasmtime/pull/4271)\n\n* Basic [tier 3] support for Windows ARM64 has been added but features such as\n  traps don't work at this time.\n  [#4990](https://github.com/bytecodealliance/wasmtime/pull/4990)\n\n### Changed\n\n* The implementation of the `random_get` function in `wasi-common` is now faster\n  by using a userspace CSPRNG rather than the OS for randomness.\n  [#4917](https://github.com/bytecodealliance/wasmtime/pull/4917)\n\n* The AArch64 backend has completed its transition to ISLE.\n  [#4851](https://github.com/bytecodealliance/wasmtime/pull/4851)\n  [#4866](https://github.com/bytecodealliance/wasmtime/pull/4866)\n  [#4898](https://github.com/bytecodealliance/wasmtime/pull/4898)\n  [#4884](https://github.com/bytecodealliance/wasmtime/pull/4884)\n  [#4820](https://github.com/bytecodealliance/wasmtime/pull/4820)\n  [#4913](https://github.com/bytecodealliance/wasmtime/pull/4913)\n  [#4942](https://github.com/bytecodealliance/wasmtime/pull/4942)\n  [#4943](https://github.com/bytecodealliance/wasmtime/pull/4943)\n\n* The size of the `sigaltstack` allocated per-thread for signal handling has\n  been increased from 16k to 64k.\n  [#4964](https://github.com/bytecodealliance/wasmtime/pull/4964)\n\n\n[Tier 3]: https://docs.wasmtime.dev/stability-tiers.html\n\n--------------------------------------------------------------------------------\n\n## 1.0.2\n\nReleased 2022-11-10.\n\n### Fixed\n\n* [CVE-2022-39392] - modules may perform out-of-bounds reads/writes when the\n  pooling allocator was configured with `memory_pages: 0`.\n\n* [CVE-2022-39393] - data can be leaked between instances when using the pooling\n  allocator.\n\n* [CVE-2022-39394] - An incorrect Rust signature for the C API\n  `wasmtime_trap_code` function could lead to an out-of-bounds write of three\n  zero bytes.\n\n--------------------------------------------------------------------------------\n\n## 1.0.1\n\nReleased 2022-09-26\n\nThis is a patch release that incorporates a fix for a miscompilation of an\natomic-CAS operator on aarch64. The instruction is not usable from Wasmtime\nwith default settings, but may be used if the Wasm atomics extension is\nenabled. The bug may also be reachable via other uses of Cranelift. Thanks to\n@bjorn3 for reporting and debugging this issue!\n\n### Fixed\n\n* Fixed a miscompilation of `atomic_cas` on aarch64. The output register was\n  swapped with a temporary register in the register-allocator constraints.\n  [#4959](https://github.com/bytecodealliance/wasmtime/pull/4959)\n  [#4960](https://github.com/bytecodealliance/wasmtime/pull/4960)\n\n--------------------------------------------------------------------------------\n\n## 1.0.0\n\nReleased 2022-09-20\n\nThis release marks the official 1.0 release of Wasmtime and represents the\nculmination of the work amongst over 300 contributors. Wasmtime has been\nbattle-tested in production through multiple embeddings for quite some time now\nand we're confident in releasing a 1.0 version to signify the stability and\nquality of the Wasmtime engine.\n\nMore information about Wasmtime's 1.0 release is on the [Bytecode Alliance's\nblog][ba-blog] with separate posts on [Wasmtime's performance\nfeatures][ba-perf], [Wasmtime's security story][ba-security], and [the 1.0\nrelease announcement][ba-1.0].\n\nAs a reminder the 2.0 release of Wasmtime is scheduled for one month from now on\nOctober 20th. For more information see the [RFC on Wasmtime's 1.0\nrelease][rfc-1.0].\n\n[ba-blog]: https://bytecodealliance.org/articles/\n[ba-perf]: https://bytecodealliance.org/articles/wasmtime-10-performance\n[ba-security]: https://bytecodealliance.org/articles/security-and-correctness-in-wasmtime\n[ba-1.0]: https://bytecodealliance.org/articles/wasmtime-1-0-fast-safe-and-now-production-ready.md\n[rfc-1.0]: https://github.com/bytecodealliance/rfcs/blob/main/accepted/wasmtime-one-dot-oh.md\n\n### Added\n\n* An incremental compilation cache for Cranelift has been added which can be\n  enabled with `Config::enable_incremental_compilation`, and this option is\n  disabled by default for now. The incremental compilation cache has been\n  measured to improve compile times for cold uncached modules as well due to\n  some wasm modules having similar-enough functions internally.\n  [#4551](https://github.com/bytecodealliance/wasmtime/pull/4551)\n\n* Source tarballs are now available as part of Wasmtime's release artifacts.\n  [#4294](https://github.com/bytecodealliance/wasmtime/pull/4294)\n\n* WASI APIs that specify the REALTIME clock are now supported.\n  [#4777](https://github.com/bytecodealliance/wasmtime/pull/4777)\n\n* WASI's socket functions are now fully implemented.\n  [#4776](https://github.com/bytecodealliance/wasmtime/pull/4776)\n\n* The native call stack for async-executed wasm functions are no longer\n  automatically reset to zero after the stack is returned to the pool when using\n  the pooling allocator. A `Config::async_stack_zeroing` option has been added\n  to restore the old behavior of zero-on-return-to-pool.\n  [#4813](https://github.com/bytecodealliance/wasmtime/pull/4813)\n\n* Inline stack probing has been implemented for the Cranelift x64 backend.\n  [#4747](https://github.com/bytecodealliance/wasmtime/pull/4747)\n\n### Changed\n\n* Generating of native unwind information has moved from a\n  `Config::wasm_backtrace` option to a new `Config::native_unwind_info` option\n  and is enabled by default.\n  [#4643](https://github.com/bytecodealliance/wasmtime/pull/4643)\n\n* The `memory-init-cow` feature is now enabled by default in the C API.\n  [#4690](https://github.com/bytecodealliance/wasmtime/pull/4690)\n\n* Back-edge CFI is now enabled by default on AArch64 macOS.\n  [#4720](https://github.com/bytecodealliance/wasmtime/pull/4720)\n\n* WASI calls will no longer return NOTCAPABLE in preparation for the removal of\n  the rights system from WASI.\n  [#4666](https://github.com/bytecodealliance/wasmtime/pull/4666)\n\n### Internal\n\nThis section of the release notes shouldn't affect external users since no\npublic-facing APIs are affected, but serves as a place to document larger\nchanges internally within Wasmtime.\n\n* Differential fuzzing has been refactored and improved into one fuzzing target\n  which can execute against any of Wasmtime itself (configured differently),\n  wasmi, V8, or the spec interpreter. Fuzzing now executes each exported\n  function with fuzz-generated inputs and the contents of all of memory and each\n  exported global is compared after each execution. Additionally more\n  interesting shapes of modules are also possible to generate.\n  [#4515](https://github.com/bytecodealliance/wasmtime/pull/4515)\n  [#4735](https://github.com/bytecodealliance/wasmtime/pull/4735)\n  [#4737](https://github.com/bytecodealliance/wasmtime/pull/4737)\n  [#4739](https://github.com/bytecodealliance/wasmtime/pull/4739)\n  [#4774](https://github.com/bytecodealliance/wasmtime/pull/4774)\n  [#4773](https://github.com/bytecodealliance/wasmtime/pull/4773)\n  [#4845](https://github.com/bytecodealliance/wasmtime/pull/4845)\n  [#4672](https://github.com/bytecodealliance/wasmtime/pull/4672)\n  [#4674](https://github.com/bytecodealliance/wasmtime/pull/4674)\n\n* The x64 backend for Cranelift has been fully migrated to ISLE.\n  [#4619](https://github.com/bytecodealliance/wasmtime/pull/4619)\n  [#4625](https://github.com/bytecodealliance/wasmtime/pull/4625)\n  [#4645](https://github.com/bytecodealliance/wasmtime/pull/4645)\n  [#4650](https://github.com/bytecodealliance/wasmtime/pull/4650)\n  [#4684](https://github.com/bytecodealliance/wasmtime/pull/4684)\n  [#4704](https://github.com/bytecodealliance/wasmtime/pull/4704)\n  [#4718](https://github.com/bytecodealliance/wasmtime/pull/4718)\n  [#4726](https://github.com/bytecodealliance/wasmtime/pull/4726)\n  [#4722](https://github.com/bytecodealliance/wasmtime/pull/4722)\n  [#4729](https://github.com/bytecodealliance/wasmtime/pull/4729)\n  [#4730](https://github.com/bytecodealliance/wasmtime/pull/4730)\n  [#4741](https://github.com/bytecodealliance/wasmtime/pull/4741)\n  [#4763](https://github.com/bytecodealliance/wasmtime/pull/4763)\n  [#4772](https://github.com/bytecodealliance/wasmtime/pull/4772)\n  [#4780](https://github.com/bytecodealliance/wasmtime/pull/4780)\n  [#4787](https://github.com/bytecodealliance/wasmtime/pull/4787)\n  [#4793](https://github.com/bytecodealliance/wasmtime/pull/4793)\n  [#4809](https://github.com/bytecodealliance/wasmtime/pull/4809)\n\n* The AArch64 backend for Cranelift has seen significant progress in being\n  ported to ISLE.\n  [#4608](https://github.com/bytecodealliance/wasmtime/pull/4608)\n  [#4639](https://github.com/bytecodealliance/wasmtime/pull/4639)\n  [#4634](https://github.com/bytecodealliance/wasmtime/pull/4634)\n  [#4748](https://github.com/bytecodealliance/wasmtime/pull/4748)\n  [#4750](https://github.com/bytecodealliance/wasmtime/pull/4750)\n  [#4751](https://github.com/bytecodealliance/wasmtime/pull/4751)\n  [#4753](https://github.com/bytecodealliance/wasmtime/pull/4753)\n  [#4788](https://github.com/bytecodealliance/wasmtime/pull/4788)\n  [#4796](https://github.com/bytecodealliance/wasmtime/pull/4796)\n  [#4785](https://github.com/bytecodealliance/wasmtime/pull/4785)\n  [#4819](https://github.com/bytecodealliance/wasmtime/pull/4819)\n  [#4821](https://github.com/bytecodealliance/wasmtime/pull/4821)\n  [#4832](https://github.com/bytecodealliance/wasmtime/pull/4832)\n\n* The s390x backend has seen improvements and additions to fully support the\n  Cranelift backend for rustc.\n  [#4682](https://github.com/bytecodealliance/wasmtime/pull/4682)\n  [#4702](https://github.com/bytecodealliance/wasmtime/pull/4702)\n  [#4616](https://github.com/bytecodealliance/wasmtime/pull/4616)\n  [#4680](https://github.com/bytecodealliance/wasmtime/pull/4680)\n\n* Significant improvements have been made to Cranelift-based fuzzing with more\n  supported features and more instructions being fuzzed.\n  [#4589](https://github.com/bytecodealliance/wasmtime/pull/4589)\n  [#4591](https://github.com/bytecodealliance/wasmtime/pull/4591)\n  [#4665](https://github.com/bytecodealliance/wasmtime/pull/4665)\n  [#4670](https://github.com/bytecodealliance/wasmtime/pull/4670)\n  [#4590](https://github.com/bytecodealliance/wasmtime/pull/4590)\n  [#4375](https://github.com/bytecodealliance/wasmtime/pull/4375)\n  [#4519](https://github.com/bytecodealliance/wasmtime/pull/4519)\n  [#4696](https://github.com/bytecodealliance/wasmtime/pull/4696)\n  [#4700](https://github.com/bytecodealliance/wasmtime/pull/4700)\n  [#4703](https://github.com/bytecodealliance/wasmtime/pull/4703)\n  [#4602](https://github.com/bytecodealliance/wasmtime/pull/4602)\n  [#4713](https://github.com/bytecodealliance/wasmtime/pull/4713)\n  [#4738](https://github.com/bytecodealliance/wasmtime/pull/4738)\n  [#4667](https://github.com/bytecodealliance/wasmtime/pull/4667)\n  [#4782](https://github.com/bytecodealliance/wasmtime/pull/4782)\n  [#4783](https://github.com/bytecodealliance/wasmtime/pull/4783)\n  [#4800](https://github.com/bytecodealliance/wasmtime/pull/4800)\n\n* Optimization work on cranelift has continued across various dimensions for\n  some modest compile-time improvements.\n  [#4621](https://github.com/bytecodealliance/wasmtime/pull/4621)\n  [#4701](https://github.com/bytecodealliance/wasmtime/pull/4701)\n  [#4697](https://github.com/bytecodealliance/wasmtime/pull/4697)\n  [#4711](https://github.com/bytecodealliance/wasmtime/pull/4711)\n  [#4710](https://github.com/bytecodealliance/wasmtime/pull/4710)\n  [#4829](https://github.com/bytecodealliance/wasmtime/pull/4829)\n\n--------------------------------------------------------------------------------\n\n## 0.40.0\n\nReleased 2022-08-20\n\nThis was a relatively quiet release in terms of user-facing features where most\nof the work was around the internals of Wasmtime and Cranelift. Improvements\ninternally have been made along the lines of:\n\n* Many more instructions are now implemented with ISLE instead of handwritten\n  lowerings.\n* Many improvements to the cranelift-based fuzzing.\n* Many platform improvements for s390x including full SIMD support, running\n  `rustc_codegen_cranelift` with features like `i128`, supporting more\n  ABIs, etc.\n* Much more of the component model has been implemented and is now fuzzed.\n\nFinally this release is currently scheduled to be the last `0.*` release of\nWasmtime. The upcoming release of Wasmtime on September 20 is planned to be\nWasmtime's 1.0 release. More information about what 1.0 means for Wasmtime is\navailable in the [1.0 RFC]\n\n[1.0 RFC]: https://github.com/bytecodealliance/rfcs/blob/main/accepted/wasmtime-one-dot-oh.md\n\n### Added\n\n* Stack walking has been reimplemented with frame pointers rather than with\n  native unwind information. This means that backtraces are feasible to capture\n  in performance-critical environments and in general stack walking is much\n  faster than before.\n  [#4431](https://github.com/bytecodealliance/wasmtime/pull/4431)\n\n* The WebAssembly `simd` proposal is now fully implemented for the s390x\n  backend.\n  [#4427](https://github.com/bytecodealliance/wasmtime/pull/4427)\n\n* Support for AArch64 has been added in the experimental native debuginfo\n  support that Wasmtime has.\n  [#4468](https://github.com/bytecodealliance/wasmtime/pull/4468)\n\n* Support building the C API of Wasmtime with CMake has been added.\n  [#4369](https://github.com/bytecodealliance/wasmtime/pull/4369)\n\n* Clarification was added to Wasmtime's documentation about \"tiers of support\"\n  for various features.\n  [#4479](https://github.com/bytecodealliance/wasmtime/pull/4479)\n\n### Fixed\n\n* Support for `filestat_get` has been improved for stdio streams in WASI.\n  [#4531](https://github.com/bytecodealliance/wasmtime/pull/4531)\n\n* Enabling the `vtune` feature no longer breaks builds on AArch64.\n  [#4533](https://github.com/bytecodealliance/wasmtime/pull/4533)\n\n--------------------------------------------------------------------------------\n\n## 0.39.1\n\nReleased 2022-07-20.\n\n### Fixed\n\n* An s390x-specific codegen bug in addition to a mistake introduced in the fix\n  of CVE-2022-31146 were fixed.\n  [#4490](https://github.com/bytecodealliance/wasmtime/pull/4490)\n\n--------------------------------------------------------------------------------\n\n## 0.39.0\n\nReleased 2022-07-20\n\n### Added\n\n* Initial support for shared memories and the `threads` WebAssembly proposal\n  has been added. Note that this feature is still experimental and not ready\n  for production use yet.\n  [#4187](https://github.com/bytecodealliance/wasmtime/pull/4187)\n\n* A new `Linker::define_unknown_imports_as_traps` method and\n  `--trap-unknown-imports` CLI flag have been added to conveniently support\n  running modules with imports that aren't dynamically called at runtime.\n  [#4312](https://github.com/bytecodealliance/wasmtime/pull/4312)\n\n* The VTune profiling strategy can now be selected through the C API.\n  [#4316](https://github.com/bytecodealliance/wasmtime/pull/4316)\n\n### Changed\n\n* Some methods on the `Config` structure now return `&mut Self` instead of\n  `Result<&mut Self>` since the validation is deferred until `Engine::new`:\n  `profiler`, `cranelift_flag_enable`, `cranelift_flag_set`, `max_wasm_stack`,\n  `async_stack_size`, and `strategy`.\n  [#4252](https://github.com/bytecodealliance/wasmtime/pull/4252)\n  [#4262](https://github.com/bytecodealliance/wasmtime/pull/4262)\n\n* Parallel compilation of WebAssembly modules is now enabled in the C API by\n  default.\n  [#4270](https://github.com/bytecodealliance/wasmtime/pull/4270)\n\n* Implicit Cargo features of the `wasmtime` introduced through `optional`\n  dependencies may have been removed since namespaced features are now used.\n  It's recommended to only used the set of named `[features]` for Wasmtime.\n  [#4293](https://github.com/bytecodealliance/wasmtime/pull/4293)\n\n* Register allocation has fixed a few issues related to excessive memory usage\n  at compile time.\n  [#4324](https://github.com/bytecodealliance/wasmtime/pull/4324)\n\n### Fixed\n\n* A refactor of `Config` was made to fix an issue that the order of calls to `Config`\n  matters now, which may lead to unexpected behavior.\n  [#4252](https://github.com/bytecodealliance/wasmtime/pull/4252)\n  [#4262](https://github.com/bytecodealliance/wasmtime/pull/4262)\n\n* Wasmtime has been fixed to work on SSE2-only x86\\_64 platforms when the\n  `simd` feature is disabled in `Config`.\n  [#4231](https://github.com/bytecodealliance/wasmtime/pull/4231)\n\n* Generation of platform-specific unwinding information is disabled if\n  `wasm_backtrace` and `wasm_reference_types` are both disabled.\n  [#4351](https://github.com/bytecodealliance/wasmtime/pull/4351)\n\n--------------------------------------------------------------------------------\n\n## 0.38.3\n\nReleased 2022-07-20.\n\n### Fixed.\n\n* An s390x-specific codegen bug in addition to a mistake introduced in the fix\n  of CVE-2022-31146 were fixed.\n  [#4491](https://github.com/bytecodealliance/wasmtime/pull/4491)\n\n--------------------------------------------------------------------------------\n\n## 0.38.2\n\nReleased 2022-07-20.\n\n### Fixed.\n\n* A miscompilation when handling constant divisors on AArch64 has been fixed.\n  [CVE-2022-31169](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-7f6x-jwh5-m9r4)\n\n* A use-after-free possible with accidentally missing stack maps has been fixed.\n  [CVE-2022-31146](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-5fhj-g3p3-pq9g)\n\n--------------------------------------------------------------------------------\n\n## 0.38.1\n\nReleased 2022-06-27.\n\n### Fixed.\n\n* A register allocator bug was fixed that could affect direct users of\n  Cranelift who use struct-return (`sret`) arguments. The bug had to do with\n  the handling of physical register constraints in the function prologue. No\n  impact should be possible for users of Cranelift via the Wasm frontend,\n  including Wasmtime.\n  [regalloc2#60](https://github.com/bytecodealliance/regalloc2/pull/60)\n  [#4333](https://github.com/bytecodealliance/wasmtime/pull/4333)\n\n* Lowering bugs for the `i8x16.swizzle` and `select`-with-`v128`-inputs\n  instructions were fixed for the x86\\_64 code generator. Note that aarch64 and\n  s390x are unaffected.\n  [#4334](https://github.com/bytecodealliance/wasmtime/pull/4334)\n\n* A bug in the 8-bit lowering of integer division on x86-64 was fixed in\n  Cranelift that could cause a register allocator panic due to an undefined\n  value in a register. (The divide instruction does not take a register `rdx`\n  as a source when 8 bits but the metadata incorrectly claimed it did.) No\n  impact on Wasm/Wasmtime users, and impact on direct Cranelift embedders\n  limited to compilation panics.\n  [#4332](https://github.com/bytecodealliance/wasmtime/pull/4332)\n\n--------------------------------------------------------------------------------\n\n## 0.38.0\n\nReleased 2022-06-21\n\n### Added\n\n* Enabling or disabling NaN canonicalization in generated code is now exposed\n  through the C API.\n  [#4154](https://github.com/bytecodealliance/wasmtime/pull/4154)\n\n* A user-defined callback can now be invoked when an epoch interruption happens\n  via the `Store::epoch_deadline_callback` API.\n  [#4152](https://github.com/bytecodealliance/wasmtime/pull/4152)\n\n* Basic alias analysis with redundant-load elimintation and store-to-load\n  forwarding optimizations has been added to Cranelift.\n  [#4163](https://github.com/bytecodealliance/wasmtime/pull/4163)\n\n### Changed\n\n* Traps originating from epoch-based interruption are now exposed as\n  `TrapCode::Interrupt`.\n  [#4105](https://github.com/bytecodealliance/wasmtime/pull/4105)\n\n* Binary builds for AArch64 now require glibc 2.17 and for s390x require glibc\n  2.16. Previously glibc 2.28 was required.\n  [#4171](https://github.com/bytecodealliance/wasmtime/pull/4171)\n\n* The `wasmtime::ValRaw` now has all of its fields listed as private and instead\n  constructors/accessors are provided for getting at the internal data.\n  [#4186](https://github.com/bytecodealliance/wasmtime/pull/4186)\n\n* The `wasm-backtrace` Cargo feature has been removed in favor of a\n  `Config::wasm_backtrace` runtime configuration option. Additionally backtraces\n  are now only captured when an embedder-generated trap actually reaches a\n  WebAssembly call stack.\n  [#4183](https://github.com/bytecodealliance/wasmtime/pull/4183)\n\n* Usage of `*_unchecked` APIs for `Func` in the `wasmtime` crate and C API now\n  take a `usize` parameter indicating the number of `ValRaw` values behind\n  the associated pointer.\n  [#4192](https://github.com/bytecodealliance/wasmtime/pull/4192)\n\n### Fixed\n\n* An improvement was made to the spill-slot allocation in code generation to fix\n  an issue where some stack slots accidentally weren't reused. This issue was\n  introduced with the landing of regalloc2 in 0.37.0 and may have resulted in\n  larger-than-intended increases in stack frame sizes.\n  [#4222](https://github.com/bytecodealliance/wasmtime/pull/4222)\n\n--------------------------------------------------------------------------------\n\n## 0.37.0\n\nReleased 2022-05-20\n\n### Added\n\n* Updated Cranelift to use regalloc2, a new register allocator. This should\n  result in ~20% faster compile times, and for programs that suffered from\n  register-allocation pressure before, up to ~20% faster generated code.\n  [#3989](https://github.com/bytecodealliance/wasmtime/pull/3989)\n\n* Pre-built binaries for macOS M1 machines are now available as release\n  artifacts.\n  [#3983](https://github.com/bytecodealliance/wasmtime/pull/3983)\n\n* Copy-on-write images of memory can now be manually initialized for a `Module`\n  with an explicit method call, but it is still not required to call this method\n  and will automatically otherwise happen on the first instantiation.\n  [#3964](https://github.com/bytecodealliance/wasmtime/pull/3964)\n\n### Fixed\n\n* Using `InstancePre::instantiate` or `Linker::instantiate` will now panic as\n  intended when used with an async-configured `Store`.\n  [#3972](https://github.com/bytecodealliance/wasmtime/pull/3972)\n\n### Changed\n\n* The unsafe `ValRaw` type in the `wasmtime` crate now always stores its values\n  in little-endian format instead of the prior native-endian format. Users of\n  `ValRaw` are recommended to audit their existing code for usage to continue\n  working on big-endian platforms.\n  [#4035](https://github.com/bytecodealliance/wasmtime/pull/4035)\n\n### Removed\n\n* Support for `Config::paged_memory_initialization` and the `uffd` crate feature\n  have been removed from the `wasmtime` crate. Users should migrate to using\n  `Config::memory_init_cow` which is more portable and faster at this point.\n  [#4040](https://github.com/bytecodealliance/wasmtime/pull/4040)\n\n--------------------------------------------------------------------------------\n\n## 0.36.0\n\nReleased 2022-04-20\n\n### Added\n\n* Support for epoch-based interruption has been added to the C API.\n  [#3925](https://github.com/bytecodealliance/wasmtime/pull/3925)\n\n* Support for disabling libunwind-based backtraces of WebAssembly code at\n  compile time has been added.\n  [#3932](https://github.com/bytecodealliance/wasmtime/pull/3932)\n\n* Async support for call hooks has been added to optionally execute \"blocking\"\n  work whenever a wasm module is entered or exited relative to the host.\n  [#3876](https://github.com/bytecodealliance/wasmtime/pull/3876)\n\n### Fixed\n\n* Loading a `Module` will now check, at runtime, that the compilation settings\n  enabled in a `Config` are compatible with the native host. For example this\n  ensures that if avx2 is enabled that the host actually has avx2 support.\n  [#3899](https://github.com/bytecodealliance/wasmtime/pull/3899)\n\n### Removed\n\n* Support for `Config::interruptable` and `InterruptHandle` has been removed\n  from the `wasmtime` crate. Users should migrate to using epoch-based\n  interruption instead.\n  [#3925](https://github.com/bytecodealliance/wasmtime/pull/3925)\n\n* The module linking implementation of Wasmtime has been removed to make room\n  for the upcoming support for the component model.\n  [#3958](https://github.com/bytecodealliance/wasmtime/pull/3958)\n\n--------------------------------------------------------------------------------\n\n## 0.35.3\n\nReleased 2022-04-11.\n\n### Fixed\n\n* Backported a bugfix for an instruction lowering issue that could cause a\n  regalloc panic due to an undefined register in some cases. No miscompilation\n  was ever possible, but panics would result in a compilation failure.\n  [#4012](https://github.com/bytecodealliance/wasmtime/pull/4012)\n\n--------------------------------------------------------------------------------\n\n## 0.35.2\n\nReleased 2022-03-31.\n\n### Security Fixes\n\n* [CVE-2022-24791](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-gwc9-348x-qwv2):\n  Fixed a use after free with `externref`s and epoch interruption.\n\n## 0.35.1\n\nReleased 2022-03-09.\n\n### Fixed\n\n* Fixed a bug in the x86-64 lowering of the `uextend` opcode for narrow (`i8`,\n  `i16`) integer sources when the value is produced by one of several\n  arithmetic instructions.\n  [#3906](https://github.com/bytecodealliance/wasmtime/pull/3906)\n\n## 0.35.0\n\nReleased 2022-03-07.\n\n### Added\n\n* The `wasmtime_wasi::add_to_linker` function now allows providing\n  a context object of a custom type instead of `wasmtime_wasi::WasiCtx`,\n  as long as that type implements the required WASI snapshot traits.\n  This allows, for example, wrapping `WasiCtx` into a struct and providing\n  custom implementations for those traits to override the default behaviour.\n\n### Changed\n\n* WebAssembly tables of `funcref` values are now lazily initialized which can,\n  in some cases, greatly speed up instantiation of a module.\n  [#3733](https://github.com/bytecodealliance/wasmtime/pull/3733)\n\n* The `memfd` feature in 0.34.0, now renamed to `memory-init-cow`, has been\n  enabled by default. This means that, where applicable, WebAssembly linear\n  memories are now initialized with copy-on-write mappings. Support from this\n  has been expanded from Linux-only to include macOS and other Unix systems when\n  modules are loaded from precompiled `*.cwasm` files on disk.\n  [#3777](https://github.com/bytecodealliance/wasmtime/pull/3777)\n  [#3778](https://github.com/bytecodealliance/wasmtime/pull/3778)\n  [#3787](https://github.com/bytecodealliance/wasmtime/pull/3787)\n  [#3819](https://github.com/bytecodealliance/wasmtime/pull/3819)\n  [#3831](https://github.com/bytecodealliance/wasmtime/pull/3831)\n\n* Clarify that SSE 4.2 (and prior) is required for running WebAssembly code with\n  simd support enabled on x86\\_64.\n  [#3816](https://github.com/bytecodealliance/wasmtime/pull/3816)\n  [#3817](https://github.com/bytecodealliance/wasmtime/pull/3817)\n  [#3833](https://github.com/bytecodealliance/wasmtime/pull/3833)\n  [#3825](https://github.com/bytecodealliance/wasmtime/pull/3825)\n\n* Support for profiling with VTune is now enabled at compile time by default,\n  but it remains disabled at runtime by default.\n  [#3821](https://github.com/bytecodealliance/wasmtime/pull/3821)\n\n* The `ModuleLimits` type has been removed from the configuration of the pooling\n  allocator in favor of configuring the total size of an instance allocation\n  rather than each individual field.\n  [#3837](https://github.com/bytecodealliance/wasmtime/pull/3837)\n\n* The native stack size allowed for WebAssembly has been decreased from 1 MiB to\n  512 KiB on all platforms to better accomodate running wasm on the main thread\n  on Windows.\n  [#3861](https://github.com/bytecodealliance/wasmtime/pull/3861)\n\n* The `wasi-common` crate now supports doing polls for both read and write\n  interest on a file descriptor at the same time.\n  [#3866](https://github.com/bytecodealliance/wasmtime/pull/3866)\n\n### Fixed\n\n* The `Store::call_hook` callback is now invoked when entering host functions\n  defined with `*_unchecked` variants.\n  [#3881](https://github.com/bytecodealliance/wasmtime/pull/3881)\n\n### Removed\n\n* The incomplete and unmaintained ARM32 backend has been removed from Cranelift.\n  [#3799](https://github.com/bytecodealliance/wasmtime/pull/3799)\n\n--------------------------------------------------------------------------------\n\n## 0.34.2\n\nReleased 2022-03-31.\n\n### Security Fixes\n\n* [CVE-2022-24791](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-gwc9-348x-qwv2):\n  Fixed a use after free with `externref`s and epoch interruption.\n\n## 0.34.1\n\nReleased 2022-02-16.\n\n### Security Fixes\n\n* [CVE-2022-23636](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-88xq-w8cq-xfg7):\n  Fixed an invalid drop of a partially-initialized instance in the pooling instance\n  allocator.\n\n## 0.33.1\n\nReleased 2022-02-16.\n\n### Security Fixes\n\n* [CVE-2022-23636](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-88xq-w8cq-xfg7):\n  Fixed an invalid drop of a partially-initialized instance in the pooling instance\n  allocator.\n\n## 0.34.0\n\nReleased 2022-02-07.\n\n### Fixed\n\n* The `wasi-common` default implementation of some attributes of files has been\n  updated to ensure that `wasi-libc`'s `isatty` function works as intended.\n  [#3696](https://github.com/bytecodealliance/wasmtime/pull/3696)\n\n* A benign debug assertion related to `externref` and garbage-collection has\n  been fixed.\n  [#3734](https://github.com/bytecodealliance/wasmtime/pull/3734)\n\n### Added\n\n* Function names are now automatically demangled when informing profilers of\n  regions of JIT code to apply Rust-specific demangling rules if applicable.\n  [#3683](https://github.com/bytecodealliance/wasmtime/pull/3683)\n\n* Support for profiling JIT-generated trampolines with VTune has been added.\n  [#3687](https://github.com/bytecodealliance/wasmtime/pull/3687)\n\n* Wasmtime now supports a new method of async preemption dubbed \"epoch-based\n  interruption\" which is intended to be much more efficient than the current\n  fuel-based method of preemption.\n  [#3699](https://github.com/bytecodealliance/wasmtime/pull/3699)\n\n* On Linux Wasmtime will now by default use copy-on-write mappings to initialize\n  memories of wasm modules where possible, accelerating instantiation by\n  avoiding costly memory copies. When combined with the pooling allocator this\n  can also be used to speed up instance-reuse cases due to fewer syscalls to\n  change memory mappings being necessary.\n  [#3697](https://github.com/bytecodealliance/wasmtime/pull/3697)\n  [#3738](https://github.com/bytecodealliance/wasmtime/pull/3738)\n  [#3760](https://github.com/bytecodealliance/wasmtime/pull/3760)\n\n* Wasmtime now supports the recently-added `sock_accept` WASI function.\n  [#3711](https://github.com/bytecodealliance/wasmtime/pull/3711)\n\n* Cranelift now has support for specifying blocks as cold.\n  [#3698](https://github.com/bytecodealliance/wasmtime/pull/3698)\n\n### Changed\n\n* Many more instructions for the x64 backend have been migrated to ISLE,\n  additionally with refactorings to make incorrect lowerings harder to\n  accidentally write.\n  [#3653](https://github.com/bytecodealliance/wasmtime/pull/3653)\n  [#3659](https://github.com/bytecodealliance/wasmtime/pull/3659)\n  [#3681](https://github.com/bytecodealliance/wasmtime/pull/3681)\n  [#3686](https://github.com/bytecodealliance/wasmtime/pull/3686)\n  [#3688](https://github.com/bytecodealliance/wasmtime/pull/3688)\n  [#3690](https://github.com/bytecodealliance/wasmtime/pull/3690)\n  [#3752](https://github.com/bytecodealliance/wasmtime/pull/3752)\n\n* More instructions in the aarch64 backend are now lowered with ISLE.\n  [#3658](https://github.com/bytecodealliance/wasmtime/pull/3658)\n  [#3662](https://github.com/bytecodealliance/wasmtime/pull/3662)\n\n* The s390x backend's lowering rules are now almost entirely defined with ISLE.\n  [#3702](https://github.com/bytecodealliance/wasmtime/pull/3702)\n  [#3703](https://github.com/bytecodealliance/wasmtime/pull/3703)\n  [#3706](https://github.com/bytecodealliance/wasmtime/pull/3706)\n  [#3717](https://github.com/bytecodealliance/wasmtime/pull/3717)\n  [#3723](https://github.com/bytecodealliance/wasmtime/pull/3723)\n  [#3724](https://github.com/bytecodealliance/wasmtime/pull/3724)\n\n* Instantiation of modules in Wasmtime has been further optimized now that the\n  copy-on-write memory initialization removed the previously most-expensive part\n  of instantiating a module.\n  [#3727](https://github.com/bytecodealliance/wasmtime/pull/3727)\n  [#3739](https://github.com/bytecodealliance/wasmtime/pull/3739)\n  [#3741](https://github.com/bytecodealliance/wasmtime/pull/3741)\n  [#3742](https://github.com/bytecodealliance/wasmtime/pull/3742)\n\n--------------------------------------------------------------------------------\n\n## 0.33.0\n\nReleased 2022-01-05.\n\n### Added\n\n* Compiled wasm modules may now optionally omit debugging information about\n  mapping addresses to source locations, resulting in smaller binaries.\n  [#3598](https://github.com/bytecodealliance/wasmtime/pull/3598)\n\n* The WebAssembly SIMD proposal is now enabled by default.\n  [#3601](https://github.com/bytecodealliance/wasmtime/pull/3601)\n\n--------------------------------------------------------------------------------\n\n## 0.32.1\n\nReleased 2022-01-04.\n\n### Fixed\n\n* Cranelift: remove recently-added build dependency on `sha2` to allow usage in\n  some dependency-sensitive environments, by computing ISLE manifest hashes\n  with a different hash function.\n  [#3619](https://github.com/bytecodealliance/wasmtime/pull/3619)\n\n* Cranelift: fixed 8- and 16-bit behavior of popcount (bit population count)\n  instruction. Does not affect Wasm frontend.\n  [#3617](https://github.com/bytecodealliance/wasmtime/pull/3617)\n\n* Cranelift: fixed miscompilation of 8- and 16-bit bit-rotate instructions.\n  Does not affect Wasm frontend.\n  [#3610](https://github.com/bytecodealliance/wasmtime/pull/3610)\n\n--------------------------------------------------------------------------------\n\n## 0.32.0\n\nReleased 2021-12-13.\n\n### Added\n\n* A new configuration option has been added to force using a \"static\" memory\n  style to automatically limit growth of memories in some configurations.\n  [#3503](https://github.com/bytecodealliance/wasmtime/pull/3503)\n\n* The `InstancePre<T>` type now implements `Clone`.\n  [#3510](https://github.com/bytecodealliance/wasmtime/pull/3510)\n\n* Cranelift's instruction selection process has begun to be migrated towards the\n  ISLE compiler and definition language.\n  [#3506](https://github.com/bytecodealliance/wasmtime/pull/3506)\n\n* A `pooling-allocator` feature has been added, which is on-by-default, to\n  disable the pooling allocator at compile time.\n  [#3514](https://github.com/bytecodealliance/wasmtime/pull/3514)\n\n### Fixed\n\n* A possible panic when parsing a WebAssembly `name` section has been fixed.\n  [#3509](https://github.com/bytecodealliance/wasmtime/pull/3509)\n\n* Generating native DWARF information for some C-produced modules has been\n  fixed, notably those where there may be DWARF about dead code.\n  [#3498](https://github.com/bytecodealliance/wasmtime/pull/3498)\n\n* A number of SIMD code generation bugs have been fixed in the x64 backend\n  by migrating their lowerings to ISLE.\n\n--------------------------------------------------------------------------------\n\n## 0.31.0\n\nReleased 2021-10-29.\n\n### Added\n\n* New `Func::new_unchecked` and `Func::call_unchecked` APIs have been added with\n  accompanying functions in the C API to improve the performance of calls into\n  wasm and the host in the C API.\n  [#3350](https://github.com/bytecodealliance/wasmtime/pull/3350)\n\n* Release binaries are now available for the s390x-unknown-linux-gnu\n  architecture.\n  [#3372](https://github.com/bytecodealliance/wasmtime/pull/3372)\n\n* A new `ResourceLimiterAsync` trait is added which allows asynchronous blocking\n  of WebAssembly on instructions such as `memory.grow`.\n  [#3393](https://github.com/bytecodealliance/wasmtime/pull/3393)\n\n### Changed\n\n* The `Func::call` method now takes a slice to write the results into rather\n  than returning a boxed slice.\n  [#3319](https://github.com/bytecodealliance/wasmtime/pull/3319)\n\n* Trampolines are now covered when jitdump profiling is enabled.\n  [#3344](https://github.com/bytecodealliance/wasmtime/pull/3344)\n\n### Fixed\n\n* Debugging with GDB has been fixed on Windows.\n  [#3373](https://github.com/bytecodealliance/wasmtime/pull/3373)\n\n* Some quadradic behavior in Wasmtime's compilation of modules has been fixed.\n  [#3469](https://github.com/bytecodealliance/wasmtime/pull/3469)\n  [#3466](https://github.com/bytecodealliance/wasmtime/pull/3466)\n\n* Bounds-checks for wasm memory accesses in certain non-default configurations\n  have been fixed to correctly allow loads at the end of the address space.\n  [#3462](https://github.com/bytecodealliance/wasmtime/pull/3462)\n\n* When type-checking memories and tables for satisfying instance imports the\n  runtime size of the table/memory is now consulted instead of the object's\n  original type.\n  [#3450](https://github.com/bytecodealliance/wasmtime/pull/3450)\n\n### Removed\n\n* The Lightbeam backend has been removed, as per [RFC 14].\n  [#3390](https://github.com/bytecodealliance/wasmtime/pull/3390)\n\n[RFC 14]: https://github.com/bytecodealliance/rfcs/pull/14\n\n* Cranelift's old x86 backend has been removed, as per [RFC 12].\n  [#3309](https://github.com/bytecodealliance/wasmtime/pull/3009)\n\n[RFC 12]: https://github.com/bytecodealliance/rfcs/pull/12\n\n## 0.30.0\n\nReleased 2021-09-17.\n\n### Security Fixes\n\n* [CVE-2021-39216](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-v4cp-h94r-m7xf):\n  Fixed a use after free passing `externref`s to Wasm in Wasmtime.\n\n* [CVE-2021-39218](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-4873-36h9-wv49):\n  Fixed an out-of-bounds read/write and invalid free with `externref`s and GC\n  safepoints in Wasmtime.\n\n* [CVE-2021-39219](https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-q879-9g95-56mx):\n  Fixed a bug where using two different `Engine`s with the same `Linker`-define\n  functions caused unsafety without `unsafe` blocks.\n\n### Added\n\n* Added experimental support for the in-progress 64-bit memories Wasm proposal.\n\n* Added support to build Wasmtime without the compiler. This lets you run\n  pre-compiled Wasm modules, without the ability (or potential attack surface)\n  of compiling new Wasm modules. The compilation functionality is gated by the\n  on-by-default `cranelift` cargo feature.\n\n* Added support for NaN canonicalization with SIMD vectors.\n\n* Added support for differential fuzzing against V8's Wasm engine.\n\n* Added support for fuzzing against the Wasm spec interpreter.\n\n* Enabled SIMD fuzzing on oss-fuzz.\n\n### Changed\n\n* A variety of performance improvements to loading pre-compiled modules.\n\n* A variety of performance improvements to function calls, both through Rust and\n  the C API.\n\n* Leaf functions that do not use the stack no longer bump the frame pointer on\n  aarch64 and s390x.\n\n* Many updates and expanded instruction support to the in-progress CLIF\n  interpreter.\n\n* Expanded fuzzing of reference types and GC.\n\n### Fixed\n\n* A number of fixes to both aarch64 and x86_64 support for the Wasm SIMD\n  proposal and the underlying CLIF vector instructions.\n\n* Fixed a potential infinite loop in the SSA computation for\n  `cranelift-frontend`. This was not reachable from `cranelift-wasm` or\n  Wasmtime, but might have affected general Cranelift users.\n\n### Removed\n\n* The `wasmtime wasm2obj` subcommand has been removed. Generating raw object\n  files for linking natively is no longer supported. Use the `wasmtime compile`\n  subcommand to pre-compile a Wasm module and `wasmtime run` to run pre-compiled\n  Wasm modules.\n\n## 0.29.0\n\nReleased 2021-08-02.\n\n### Changed\n\n* Instance exports are now loaded lazily from instances instead of eagerly as\n  they were before. This is an internal-only change and is not a breaking\n  change.\n  [#2984](https://github.com/bytecodealliance/wasmtime/pull/2984)\n\n* All linear memories created by Wasmtime will now, by default, have guard pages\n  in front of them in addition to after them. This is intended to help mitigate\n  future bugs in Cranelift, should they arise.\n  [#2977](https://github.com/bytecodealliance/wasmtime/pull/2977)\n\n* Linear memories now correctly support a maximum size of 4GB. Previously, the\n  limit field was 32 bits, which did not properly support a full 4GB memory.\n  This update is also a necessary change in preparation for future memory64\n  support.\n  [#3013](https://github.com/bytecodealliance/wasmtime/pull/3013)\n  [#3134](https://github.com/bytecodealliance/wasmtime/pull/3134)\n\n* Injection counts of fuel into a `wasmtime::Store` now uses a u64 instead of a\n  u32.\n  [#3048](https://github.com/bytecodealliance/wasmtime/pull/3048)\n\n### Added\n\n* Support for `i128` has improved in the AArch64 backend.\n  [#2959](https://github.com/bytecodealliance/wasmtime/pull/2959)\n  [#2975](https://github.com/bytecodealliance/wasmtime/pull/2975)\n  [#2985](https://github.com/bytecodealliance/wasmtime/pull/2985)\n  [#2990](https://github.com/bytecodealliance/wasmtime/pull/2990)\n  [#3002](https://github.com/bytecodealliance/wasmtime/pull/3002)\n  [#3004](https://github.com/bytecodealliance/wasmtime/pull/3004)\n  [#3005](https://github.com/bytecodealliance/wasmtime/pull/3005)\n  [#3008](https://github.com/bytecodealliance/wasmtime/pull/3008)\n  [#3027](https://github.com/bytecodealliance/wasmtime/pull/3027)\n\n* The s390x backend now supports z14 and atomics.\n  [#2988](https://github.com/bytecodealliance/wasmtime/pull/2988)\n  [#2991](https://github.com/bytecodealliance/wasmtime/pull/2991)\n\n* The `wasmtime::Linker` type now implements `Clone`.\n  [#2993](https://github.com/bytecodealliance/wasmtime/pull/2993)\n\n* Support for the SIMD proposal on both x86\\_64 and AArch64 has improved. On\n  x86\\_64, all SIMD opcodes are now supported.\n  [#2997](https://github.com/bytecodealliance/wasmtime/pull/2997)\n  [#3035](https://github.com/bytecodealliance/wasmtime/pull/3035)\n  [#2982](https://github.com/bytecodealliance/wasmtime/pull/2982)\n  [#3084](https://github.com/bytecodealliance/wasmtime/pull/3084)\n  [#3082](https://github.com/bytecodealliance/wasmtime/pull/3082)\n  [#3107](https://github.com/bytecodealliance/wasmtime/pull/3107)\n  [#3105](https://github.com/bytecodealliance/wasmtime/pull/3105)\n  [#3114](https://github.com/bytecodealliance/wasmtime/pull/3114)\n  [#3070](https://github.com/bytecodealliance/wasmtime/pull/3070)\n  [#3126](https://github.com/bytecodealliance/wasmtime/pull/3126)\n\n* A `Trap` can now display its reason without also displaying the backtrace.\n  [#3033](https://github.com/bytecodealliance/wasmtime/pull/3033)\n\n* An initiall fuzzer for CLIF has been added.\n  [#3038](https://github.com/bytecodealliance/wasmtime/pull/3038)\n\n* High-level architecture documentation has been added for Wasmtime.\n  [#3019](https://github.com/bytecodealliance/wasmtime/pull/3019)\n\n* Support for multi-memory can now be configured in Wasmtime's C API.\n  [#3071](https://github.com/bytecodealliance/wasmtime/pull/3071)\n\n* The `wasmtime` crate now supports a `posix-signals-on-macos` feature to force\n  the usage of signals instead of mach ports to handle traps on macOS.\n  [#3063](https://github.com/bytecodealliance/wasmtime/pull/3063)\n\n* Wasmtime's C API now has a `wasmtime_trap_code` function to get the raw trap\n  code, if present, for a trap.\n  [#3086](https://github.com/bytecodealliance/wasmtime/pull/3086)\n\n* Wasmtime's C API now has a `wasmtime_linker_define_func` function to define a\n  store-independent function within a linker.\n  [#3122](https://github.com/bytecodealliance/wasmtime/pull/3122)\n\n* A `wasmtime::Linker::module_async` function was added as the asynchronous\n  counterpart to `wasmtime::Linker::module`.\n  [#3121](https://github.com/bytecodealliance/wasmtime/pull/3121)\n\n### Fixed\n\n* Compiling the `wasmtime` crate into a `dylib` crate type has been fixed.\n  [#3010](https://github.com/bytecodealliance/wasmtime/pull/3010)\n\n* The enter/exit hooks for WebAssembly are now executed for an instance's\n  `start` function, if present.\n  [#3001](https://github.com/bytecodealliance/wasmtime/pull/3001)\n\n* Some WASI functions in `wasi-common` have been fixed for big-endian platforms.\n  [#3016](https://github.com/bytecodealliance/wasmtime/pull/3016)\n\n* Wasmtime no longer erroneously assumes that all custom sections may contain\n  DWARF information, reducing instances of `Trap`'s `Display` implementation\n  providing misleading information to set an env var to get more information.\n  [#3083](https://github.com/bytecodealliance/wasmtime/pull/3083)\n\n* Some issues with parsing DWARF debug information have been fixed.\n  [#3116](https://github.com/bytecodealliance/wasmtime/pull/3116)\n\n## 0.28.0\n\nReleased 2021-06-09.\n\n### Changed\n\n* Breaking: Wasmtime's embedding API has been redesigned, as specified in [RFC\n  11]. Rust users can now enjoy easier times with `Send` and `Sync`, and all\n  users can now more clearly manage memory, especially in the C API. Language\n  embeddings have been updated to the new API as well.\n  [#2897](https://github.com/bytecodealliance/wasmtime/pull/2897)\n\n[RFC 11]: https://github.com/bytecodealliance/rfcs/pull/11\n\n### Added\n\n* A new `InstancePre` type, created with `Linker::instantiate_pre`, has been\n  added to perform type-checking of an instance once and reduce the work done\n  for each instantiation of a module:\n  [#2962](https://github.com/bytecodealliance/wasmtime/pull/2962)\n\n* Deserialization of a module can now optionally skip checking the wasmtime\n  version string:\n  [#2945](https://github.com/bytecodealliance/wasmtime/pull/2945)\n\n* A method has been exposed to frontload per-thread initialization costs if the\n  latency of every last wasm call is important:\n  [#2946](https://github.com/bytecodealliance/wasmtime/pull/2946)\n\n* Hooks have been added for entry/exit into wasm code to allow embeddings to\n  track time and other properties about execution in a wasm environment:\n  [#2952](https://github.com/bytecodealliance/wasmtime/pull/2952)\n\n* A [C++ embedding of Wasmtime has been written][cpp].\n\n[RFC 11]: https://github.com/bytecodealliance/rfcs/pull/11\n[cpp]: https://github.com/bytecodealliance/wasmtime-cpp\n\n### Fixed\n\n* Multiple returns on macOS AArch64 have been fixed:\n  [#2956](https://github.com/bytecodealliance/wasmtime/pull/2956)\n\n## 0.27.0\n\nReleased 2021-05-21.\n\n### Security Fixes\n\n* Fixed a security issue in Cranelift's x64 backend that could result in a heap\n  sandbox escape due to an incorrect sign-extension:\n  [#2913](https://github.com/bytecodealliance/wasmtime/issues/2913).\n\n### Added\n\n* Support for IBM z/Archiecture (`s390x`) machines in Cranelift and Wasmtime:\n  [#2836](https://github.com/bytecodealliance/wasmtime/pull/2836),\n  [#2837](https://github.com/bytecodealliance/wasmtime/pull/2837),\n  [#2838](https://github.com/bytecodealliance/wasmtime/pull/2838),\n  [#2843](https://github.com/bytecodealliance/wasmtime/pull/2843),\n  [#2854](https://github.com/bytecodealliance/wasmtime/pull/2854),\n  [#2870](https://github.com/bytecodealliance/wasmtime/pull/2870),\n  [#2871](https://github.com/bytecodealliance/wasmtime/pull/2871),\n  [#2872](https://github.com/bytecodealliance/wasmtime/pull/2872),\n  [#2874](https://github.com/bytecodealliance/wasmtime/pull/2874).\n\n* Improved async support in wasi-common runtime:\n  [#2832](https://github.com/bytecodealliance/wasmtime/pull/2832).\n\n* Added `Store::with_limits`, `StoreLimits`, and `ResourceLimiter` to the\n  Wasmtime API to help with enforcing resource limits at runtime. The\n  `ResourceLimiter` trait can be implemented by custom resource limiters to\n  decide if linear memories or tables can be grown.\n\n* Added `allow-unknown-exports` option for the run command:\n  [#2879](https://github.com/bytecodealliance/wasmtime/pull/2879).\n\n* Added API to notify that a `Store` has moved to a new thread:\n  [#2822](https://github.com/bytecodealliance/wasmtime/pull/2822).\n\n* Documented guidance around using Wasmtime in multithreaded contexts:\n  [#2812](https://github.com/bytecodealliance/wasmtime/pull/2812).\n  In the future, the Wasmtime API will change to allow some of its core types\n  to be Send/Sync; see the in-progress\n  [#2897](https://github.com/bytecodealliance/wasmtime/pull/2897) for details.\n\n* Support calls from native code to multiple-return-value functions:\n  [#2806](https://github.com/bytecodealliance/wasmtime/pull/2806).\n\n### Changed\n\n* Breaking: `Memory::new` has been changed to return `Result` as creating a\n  host memory object is now a fallible operation when the initial size of\n  the memory exceeds the store limits.\n\n### Fixed\n\n* Many instruction selection improvements on x64 and aarch64:\n  [#2819](https://github.com/bytecodealliance/wasmtime/pull/2819),\n  [#2828](https://github.com/bytecodealliance/wasmtime/pull/2828),\n  [#2823](https://github.com/bytecodealliance/wasmtime/pull/2823),\n  [#2862](https://github.com/bytecodealliance/wasmtime/pull/2862),\n  [#2886](https://github.com/bytecodealliance/wasmtime/pull/2886),\n  [#2889](https://github.com/bytecodealliance/wasmtime/pull/2889),\n  [#2905](https://github.com/bytecodealliance/wasmtime/pull/2905).\n\n* Improved performance of Wasmtime runtime substantially:\n  [#2811](https://github.com/bytecodealliance/wasmtime/pull/2811),\n  [#2818](https://github.com/bytecodealliance/wasmtime/pull/2818),\n  [#2821](https://github.com/bytecodealliance/wasmtime/pull/2821),\n  [#2847](https://github.com/bytecodealliance/wasmtime/pull/2847),\n  [#2900](https://github.com/bytecodealliance/wasmtime/pull/2900).\n\n* Fixed WASI issue with file metadata on Windows:\n  [#2884](https://github.com/bytecodealliance/wasmtime/pull/2884).\n\n* Fixed an issue with debug info and an underflowing (trapping) offset:\n  [#2866](https://github.com/bytecodealliance/wasmtime/pull/2866).\n\n* Fixed an issue with unwind information in the old x86 backend:\n  [#2845](https://github.com/bytecodealliance/wasmtime/pull/2845).\n\n* Fixed i32 spilling in x64 backend:\n  [#2840](https://github.com/bytecodealliance/wasmtime/pull/2840).\n\n## 0.26.0\n\nReleased 2021-04-05.\n\n### Added\n\n* Added the `wasmtime compile` command to support AOT compilation of Wasm\n  modules. This adds the `Engine::precompile_module` method. Also added the\n  `Config::target` method to change the compilation target of the\n  configuration. This can be used in conjunction with\n  `Engine::precompile_module` to target a different host triple than the\n  current one.\n  [#2791](https://github.com/bytecodealliance/wasmtime/pull/2791)\n\n* Support for macOS on aarch64 (Apple M1 Silicon), including Apple-specific\n  calling convention details and unwinding/exception handling using Mach ports.\n  [#2742](https://github.com/bytecodealliance/wasmtime/pull/2742),\n  [#2723](https://github.com/bytecodealliance/wasmtime/pull/2723)\n\n* A number of SIMD instruction implementations in the new x86-64 backend.\n  [#2771](https://github.com/bytecodealliance/wasmtime/pull/2771)\n\n* Added the `Config::cranelift_flag_enable` method to enable setting Cranelift\n  boolean flags or presets in a config.\n\n* Added CLI option `--cranelift-enable` to enable boolean settings and ISA presets.\n\n* Deduplicate function signatures in Wasm modules.\n  [#2772](https://github.com/bytecodealliance/wasmtime/pull/2772)\n\n* Optimize overheads of calling into Wasm functions.\n  [#2757](https://github.com/bytecodealliance/wasmtime/pull/2757),\n  [#2759](https://github.com/bytecodealliance/wasmtime/pull/2759)\n\n* Improvements related to Module Linking: compile fewer trampolines;\n\n  [#2774](https://github.com/bytecodealliance/wasmtime/pull/2774)\n\n* Re-export sibling crates from `wasmtime-wasi` to make embedding easier\n  without needing to match crate versions.\n  [#2776](https://github.com/bytecodealliance/wasmtime/pull/2776)\n\n### Changed\n\n* Switched the default compiler backend on x86-64 to Cranelift's new backend.\n  This should not have any user-visible effects other than possibly runtime\n  performance improvements. The old backend is still available with the\n  `old-x86-backend` feature flag to the `cranelift-codegen` or `wasmtime`\n  crates, or programmatically with `BackendVariant::Legacy`. We plan to\n  maintain the old backend for at least one more release and ensure it works on\n  CI.\n  [#2718](https://github.com/bytecodealliance/wasmtime/pull/2718)\n\n* Breaking: `Module::deserialize` has been removed in favor of `Module::new`.\n\n* Breaking: `Config::cranelift_clear_cpu_flags` was removed. Use `Config::target`\n  to clear the CPU flags for the host's target.\n\n* Breaking: `Config::cranelift_other_flag` was renamed to `Config::cranelift_flag_set`.\n\n* CLI changes:\n  * Wasmtime CLI options to enable WebAssembly features have been replaced with\n    a singular `--wasm-features` option. The previous options are still\n    supported, but are not displayed in help text.\n  * Breaking: the CLI option `--cranelift-flags` was changed to\n    `--cranelift-set`.\n  * Breaking: the CLI option `--enable-reference-types=false` has been changed\n    to `--wasm-features=-reference-types`.\n  * Breaking: the CLI option `--enable-multi-value=false` has been changed to\n    `--wasm-features=-multi-value`.\n  * Breaking: the CLI option `--enable-bulk-memory=false` has been changed to\n    `--wasm-features=-bulk-memory`.\n\n* Improved error-reporting in wiggle.\n  [#2760](https://github.com/bytecodealliance/wasmtime/pull/2760)\n\n* Make WASI sleeping fallible (some systems do not support sleep).\n  [#2756](https://github.com/bytecodealliance/wasmtime/pull/2756)\n\n* WASI: Support `poll_oneoff` with a sleep.\n  [#2753](https://github.com/bytecodealliance/wasmtime/pull/2753)\n\n* Allow a `StackMapSink` to be passed when defining functions with\n  `cranelift-module`.\n  [#2739](https://github.com/bytecodealliance/wasmtime/pull/2739)\n\n* Some refactoring in new x86-64 backend to prepare for VEX/EVEX (e.g.,\n  AVX-512) instruction encodings to be supported.\n  [#2799](https://github.com/bytecodealliance/wasmtime/pull/2799)\n\n### Fixed\n\n* Fixed a corner case in `srem` (signed remainder) in the new x86-64 backend:\n  `INT_MIN % -1` should return `0`, rather than trapping. This only occurred\n  when `avoid_div_traps == false` was set by the embedding.\n  [#2763](https://github.com/bytecodealliance/wasmtime/pull/2763)\n\n* Fixed a memory leak of the `Store` when an instance traps.\n  [#2803](https://github.com/bytecodealliance/wasmtime/pull/2803)\n\n* Some fuzzing-related fixes.\n  [#2788](https://github.com/bytecodealliance/wasmtime/pull/2788),\n  [#2770](https://github.com/bytecodealliance/wasmtime/pull/2770)\n\n* Fixed memory-initialization bug in uffd allocator that could copy into the\n  wrong destination under certain conditions. Does not affect the default\n  wasmtime instance allocator.\n  [#2801](https://github.com/bytecodealliance/wasmtime/pull/2801)\n\n* Fix printing of float values from the Wasmtime CLI.\n  [#2797](https://github.com/bytecodealliance/wasmtime/pull/2797)\n\n* Remove the ability for the `Linker` to instantiate modules with duplicate\n  import strings of different types.\n  [#2789](https://github.com/bytecodealliance/wasmtime/pull/2789)\n\n## 0.25.0\n\nReleased 2021-03-16.\n\n### Added\n\n* An implementation of a pooling instance allocator, optionally backed by\n  `userfaultfd` on Linux, was added to improve the performance of embeddings\n  that instantiate a large number of instances continuously.\n  [#2518](https://github.com/bytecodealliance/wasmtime/pull/2518)\n\n* Host functions can now be defined on `Config` to share the function across all\n  `Store` objects connected to an `Engine`. This can improve the time it takes\n  to instantiate instances in a short-lived `Store`.\n  [#2625](https://github.com/bytecodealliance/wasmtime/pull/2625)\n\n* The `Store` object now supports having typed values attached to it which can\n  be retrieved from host functions.\n  [#2625](https://github.com/bytecodealliance/wasmtime/pull/2625)\n\n* The `wiggle` code generator now supports `async` host functions.\n  [#2701](https://github.com/bytecodealliance/wasmtime/pull/2701)\n\n### Changed\n\n* The `Func::getN{,_async}` APIs have all been removed in favor of a new\n  `Func::typed` API which should be more compact in terms of API surface area as\n  well as more flexible in how it can be used.\n  [#2719](https://github.com/bytecodealliance/wasmtime/pull/2719)\n\n* `Engine::new` has been changed from returning `Engine` to returning\n  `anyhow::Result<Engine>`. Callers of `Engine::new` will need to be updated to\n  use the `?` operator on the return value or otherwise unwrap the result to get\n  the `Engine`.\n\n### Fixed\n\n* Interpretation of timestamps in `poll_oneoff` for WASI have been fixed to\n  correctly use nanoseconds instead of microseconds.\n  [#2717](https://github.com/bytecodealliance/wasmtime/pull/2717)\n\n## 0.24.0\n\nReleased 2021-03-04.\n\n### Added\n\n* Implement support for `async` functions in Wasmtime\n  [#2434](https://github.com/bytecodealliance/wasmtime/pull/2434)\n\n### Fixed\n\n* Fix preservation of the sigaltstack on macOS\n  [#2676](https://github.com/bytecodealliance/wasmtime/pull/2676)\n* Fix incorrect semver dependencies involving fs-set-times.\n  [#2705](https://github.com/bytecodealliance/wasmtime/pull/2705)\n* Fix some `i128` shift-related bugs in x64 backend.\n  [#2682](https://github.com/bytecodealliance/wasmtime/pull/2682)\n* Fix incomplete trap metadata due to multiple traps at one address\n  [#2685](https://github.com/bytecodealliance/wasmtime/pull/2685)\n\n## 0.23.0\n\nReleased 2021-02-16.\n\n### Added\n\n* Support for limiting WebAssembly execution with fuel was added, including\n  support in the C API.\n  [#2611](https://github.com/bytecodealliance/wasmtime/pull/2611)\n  [#2643](https://github.com/bytecodealliance/wasmtime/pull/2643)\n* Wasmtime now has more knobs for limiting memory and table allocations\n  [#2617](https://github.com/bytecodealliance/wasmtime/pull/2617)\n* Added a method to share `Config` across machines\n  [#2608](https://github.com/bytecodealliance/wasmtime/pull/2608)\n* Added a safe memory read/write API\n  [#2528](https://github.com/bytecodealliance/wasmtime/pull/2528)\n* Added support for the experimental wasi-crypto APIs\n  [#2597](https://github.com/bytecodealliance/wasmtime/pull/2597)\n* Added an instance limit to `Config`\n  [#2593](https://github.com/bytecodealliance/wasmtime/pull/2593)\n* Implemented module-linking's outer module aliases\n  [#2590](https://github.com/bytecodealliance/wasmtime/pull/2590)\n* Cranelift now supports 128-bit operations for the new x64 backend.\n  [#2539](https://github.com/bytecodealliance/wasmtime/pull/2539)\n* Cranelift now has detailed debug-info (DWARF) support in new backends (initially x64).\n  [#2565](https://github.com/bytecodealliance/wasmtime/pull/2565)\n* Cranelift now uses the `POPCNT`, `TZCNT`, and `LZCNT`, as well as SSE 4.1\n  rounding instructions on x64 when available.\n* Cranelift now uses the `CNT`, instruction on aarch64 when available.\n\n### Changed\n\n* A new WASI implementation built on the new\n  [`cap-std`](https://github.com/bytecodealliance/cap-std) crate was added,\n  replacing the previous implementation. This brings improved robustness,\n  portability, and performance.\n\n* `wasmtime_wasi::WasiCtxBuilder` moved to\n  `wasi_cap_std_sync::WasiCtxBuilder`.\n\n* The WebAssembly C API is updated, with a few minor API changes\n  [#2579](https://github.com/bytecodealliance/wasmtime/pull/2579)\n\n### Fixed\n\n* Fixed a panic in WASI `fd_readdir` on large directories\n  [#2620](https://github.com/bytecodealliance/wasmtime/pull/2620)\n* Fixed a memory leak with command modules\n  [#2017](https://github.com/bytecodealliance/wasmtime/pull/2017)\n\n--------------------------------------------------------------------------------\n\n## 0.22.0\n\nReleased 2021-01-07.\n\n### Added\n\n* Experimental support for [the module-linking\n  proposal](https://github.com/WebAssembly/module-linking) was\n  added. [#2094](https://github.com/bytecodealliance/wasmtime/pull/2094)\n\n* Added support for [the reference types\n  proposal](https://webassembly.github.io/reference-types) on the aarch64\n  architecture. [#2410](https://github.com/bytecodealliance/wasmtime/pull/2410)\n\n* Experimental support for [wasi-nn](https://github.com/WebAssembly/wasi-nn) was\n  added. [#2208](https://github.com/bytecodealliance/wasmtime/pull/2208)\n\n### Changed\n\n### Fixed\n\n* Fixed an issue where the `select` instruction didn't accept `v128` SIMD\n  operands. [#2391](https://github.com/bytecodealliance/wasmtime/pull/2391)\n\n* Fixed an issue where Wasmtime could potentially use the wrong stack map during\n  GCs, leading to a\n  panic. [#2396](https://github.com/bytecodealliance/wasmtime/pull/2396)\n\n* Fixed an issue where if a host-defined function erroneously returned a value\n  from a different store, that value would be\n  leaked. [#2424](https://github.com/bytecodealliance/wasmtime/pull/2424)\n\n* Fixed a bug where in certain cases if a module's instantiation failed, it\n  could leave trampolines in the store that referenced the no-longer-valid\n  instance. These trampolines could be reused in future instantiations, leading\n  to use after free bugs.\n  [#2408](https://github.com/bytecodealliance/wasmtime/pull/2408)\n\n* Fixed a miscompilation on aarch64 where certain instructions would read `SP`\n  instead of the zero register. This could only affect you if you explicitly\n  enabled the Wasm SIMD\n  proposal. [#2548](https://github.com/bytecodealliance/wasmtime/pull/2548)\n\n--------------------------------------------------------------------------------\n\n## 0.21.0\n\nReleased 2020-11-05.\n\n### Added\n\n* Experimental support for the multi-memory proposal was added.\n  [#2263](https://github.com/bytecodealliance/wasmtime/pull/2263)\n\n* The `Trap::trap_code` API enables learning what kind of trap was raised.\n  [#2309](https://github.com/bytecodealliance/wasmtime/pull/2309)\n\n### Changed\n\n* WebAssembly module validation is now parallelized.\n  [#2059](https://github.com/bytecodealliance/wasmtime/pull/2059)\n\n* Documentation is now available at docs.wasmtime.dev.\n  [#2317](https://github.com/bytecodealliance/wasmtime/pull/2317)\n\n* Windows now compiles like other platforms with a huge guard page instead of\n  having its own custom limit which made modules compile and run more slowly.\n  [#2326](https://github.com/bytecodealliance/wasmtime/pull/2326)\n\n* The size of the cache entry for serialized modules has been greatly reduced.\n  [#2321](https://github.com/bytecodealliance/wasmtime/pull/2321)\n  [#2322](https://github.com/bytecodealliance/wasmtime/pull/2322)\n  [#2324](https://github.com/bytecodealliance/wasmtime/pull/2324)\n  [#2325](https://github.com/bytecodealliance/wasmtime/pull/2325)\n\n* The `FuncType` API constructor and accessors are now iterator-based.\n  [#2365](https://github.com/bytecodealliance/wasmtime/pull/2365)\n\n### Fixed\n\n* A panic in compiling reference-types-using modules has been fixed.\n  [#2350](https://github.com/bytecodealliance/wasmtime/pull/2350)\n\n--------------------------------------------------------------------------------\n\n## 0.20.0\n\nReleased 2020-09-23.\n\n### Added\n\n* Support for explicitly serializing and deserializing compiled wasm modules has\n  been added.\n  [#2020](https://github.com/bytecodealliance/wasmtime/pull/2020)\n\n* A `wasmtime_store_gc` C API was added to run GC for `externref`.\n  [#2052](https://github.com/bytecodealliance/wasmtime/pull/2052)\n\n* Support for atomics in Cranelift has been added. Support is not fully\n  implemented in Wasmtime at this time, however.\n  [#2077](https://github.com/bytecodealliance/wasmtime/pull/2077)\n\n* The `Caller::get_export` function is now implemented for `Func` references as\n  well.\n  [#2108](https://github.com/bytecodealliance/wasmtime/pull/2108)\n\n### Fixed\n\n* Leaks in the C API have been fixed.\n  [#2040](https://github.com/bytecodealliance/wasmtime/pull/2040)\n\n* The `wasm_val_copy` C API has been fixed for reference types.\n  [#2041](https://github.com/bytecodealliance/wasmtime/pull/2041)\n\n* Fix a panic with `Func::new` and reference types when the store doesn't have\n  reference types enabled.\n  [#2039](https://github.com/bytecodealliance/wasmtime/pull/2039)\n\n--------------------------------------------------------------------------------\n\n## 0.19.0\n\nReleased 2020-07-14.\n\n### Added\n\n* The [WebAssembly reference-types proposal][reftypes] is now supported in\n  Wasmtime and the C API.\n  [#1832](https://github.com/bytecodealliance/wasmtime/pull/1832),\n  [#1882](https://github.com/bytecodealliance/wasmtime/pull/1882),\n  [#1894](https://github.com/bytecodealliance/wasmtime/pull/1894),\n  [#1901](https://github.com/bytecodealliance/wasmtime/pull/1901),\n  [#1923](https://github.com/bytecodealliance/wasmtime/pull/1923),\n  [#1969](https://github.com/bytecodealliance/wasmtime/pull/1969),\n  [#1973](https://github.com/bytecodealliance/wasmtime/pull/1973),\n  [#1982](https://github.com/bytecodealliance/wasmtime/pull/1982),\n  [#1984](https://github.com/bytecodealliance/wasmtime/pull/1984),\n  [#1991](https://github.com/bytecodealliance/wasmtime/pull/1991),\n  [#1996](https://github.com/bytecodealliance/wasmtime/pull/1996)\n\n* The [WebAssembly simd proposal's][simd] spec tests now pass in Wasmtime.\n  [#1765](https://github.com/bytecodealliance/wasmtime/pull/1765),\n  [#1876](https://github.com/bytecodealliance/wasmtime/pull/1876),\n  [#1941](https://github.com/bytecodealliance/wasmtime/pull/1941),\n  [#1957](https://github.com/bytecodealliance/wasmtime/pull/1957),\n  [#1990](https://github.com/bytecodealliance/wasmtime/pull/1990),\n  [#1994](https://github.com/bytecodealliance/wasmtime/pull/1994)\n\n* Wasmtime can now be compiled without the usage of threads for parallel\n  compilation, although this is still enabled by default.\n  [#1903](https://github.com/bytecodealliance/wasmtime/pull/1903)\n\n* The C API is [now\n  documented](https://bytecodealliance.github.io/wasmtime/c-api/).\n  [#1928](https://github.com/bytecodealliance/wasmtime/pull/1928),\n  [#1959](https://github.com/bytecodealliance/wasmtime/pull/1959),\n  [#1968](https://github.com/bytecodealliance/wasmtime/pull/1968)\n\n* A `wasmtime_linker_get_one_by_name` function was added to the C API.\n  [#1897](https://github.com/bytecodealliance/wasmtime/pull/1897)\n\n* A `wasmtime_trap_exit_status` function was added to the C API.\n  [#1912](https://github.com/bytecodealliance/wasmtime/pull/1912)\n\n* Compilation for the `aarch64-linux-android` target should now work, although\n  keep in mind this platform is not fully tested still.\n  [#2002](https://github.com/bytecodealliance/wasmtime/pull/2002)\n\n[reftypes]: https://github.com/WebAssembly/reference-types\n\n### Fixed\n\n* Runtime warnings when using Wasmtime on musl have been fixed.\n  [#1914](https://github.com/bytecodealliance/wasmtime/pull/1914)\n\n* A bug affecting Windows unwind information with functions that have spilled\n  floating point registers has been fixed.\n  [#1983](https://github.com/bytecodealliance/wasmtime/pull/1983)\n\n### Changed\n\n* Wasmtime's default branch and development now happens on the `main` branch\n  instead of `master`.\n  [#1924](https://github.com/bytecodealliance/wasmtime/pull/1924)\n\n### Removed\n\n* The \"host info\" support in the C API has been removed since it was never fully\n  or correctly implemented.\n  [#1922](https://github.com/bytecodealliance/wasmtime/pull/1922)\n\n* Support for the `*_same` functions in the C API has been removed in the same\n  vein as the host info APIs.\n  [#1926](https://github.com/bytecodealliance/wasmtime/pull/1926)\n\n--------------------------------------------------------------------------------\n\n## 0.18.0\n\nRelease 2020-06-09.\n\n### Added\n\nThe `WasmTy` trait is now implemented for `u32` and `u64`.\n\n  [#1808](https://github.com/bytecodealliance/wasmtime/pull/1808)\n\n--------------------------------------------------------------------------------\n\n## 0.17.0\n\nReleased 2020-06-01.\n\n### Added\n\n* The [Commands and Reactors ABI] is now supported in the Rust API. `Linker::module`\n  loads a module and automatically handles Commands and Reactors semantics.\n\n  [#1565](https://github.com/bytecodealliance/wasmtime/pull/1565)\n\n[Commands and Reactors ABI]: https://github.com/WebAssembly/WASI/blob/master/design/application-abi.md#current-unstable-abi\n\nThe `Table::grow` function now returns the previous table size, making it consistent\nwith the `table.grow` instruction.\n\n  [#1653](https://github.com/bytecodealliance/wasmtime/pull/1653)\n\nNew Wasmtime-specific C APIs for working with tables were added which provide more\ndetailed error information and which make growing a table more consistent with the\n`table.grow` instruction as well.\n\n  [#1654](https://github.com/bytecodealliance/wasmtime/pull/1654)\n\nThe C API now includes support for enabling logging in Wasmtime.\n\n  [#1737](https://github.com/bytecodealliance/wasmtime/pull/1737)\n\n### Changed\n\nThe WASI `proc_exit` function no longer exits the host process. It now unwinds the\ncallstack back to the wasm entrypoint, and the exit value is available from the\n`Trap::i32_exit_status` method.\n\n  [#1646](https://github.com/bytecodealliance/wasmtime/pull/1646)\n\nThe WebAssembly [multi-value](https://github.com/WebAssembly/multi-value/) proposal\nis now enabled by default.\n\n  [#1667](https://github.com/bytecodealliance/wasmtime/pull/1667)\n\nThe Rust API does not require a store provided during `Module::new` operation. The `Module` can be send accross threads and instantiate for a specific store. The `Instance::new` now requires the store.\n\n  [#1761](https://github.com/bytecodealliance/wasmtime/pull/1761)\n\n--------------------------------------------------------------------------------\n\n## 0.16.0\n\nReleased 2020-04-29.\n\n### Added\n\n* The `Instance` struct has new accessors, `get_func`, `get_table`,\n  `get_memory`, and `get_global` for quickly looking up exported\n  functions, tables, memories, and globals by name.\n  [#1524](https://github.com/bytecodealliance/wasmtime/pull/1524)\n\n* The C API has a number of new `wasmtime_*` functions which return error\n  objects to get detailed error information when an API fails.\n  [#1467](https://github.com/bytecodealliance/wasmtime/pull/1467)\n\n* Users now have fine-grained control over creation of instances of `Memory`\n  with a new `MemoryCreator` trait.\n  [#1400](https://github.com/bytecodealliance/wasmtime/pull/1400)\n\n* Go bindings for Wasmtime are [now available][go-bindings].\n  [#1481](https://github.com/bytecodealliance/wasmtime/pull/1481)\n\n* APIs for looking up values in a `Linker` have been added.\n  [#1480](https://github.com/bytecodealliance/wasmtime/pull/1480)\n\n* Preliminary support for AArch64, also known as ARM64.\n  [#1581](https://github.com/bytecodealliance/wasmtime/pull/1581)\n\n[go-bindings]: https://github.com/bytecodealliance/wasmtime-go\n\n### Changed\n\n* `Instance::exports` now returns `Export` objects which contain\n  the `name`s of the exports in addition to their `Extern` definitions,\n  so it's no longer necessary to use `Module::exports` to obtain the\n  export names.\n  [#1524](https://github.com/bytecodealliance/wasmtime/pull/1524)\n\n* The `Func::call` API has changed its error type from `Trap` to `anyhow::Error`\n  to distinguish between wasm traps and runtime violations (like the wrong\n  number of parameters).\n  [#1467](https://github.com/bytecodealliance/wasmtime/pull/1467)\n\n* A number of `wasmtime_linker_*` and `wasmtime_config_*` C APIs have new type\n  signatures which reflect returning errors.\n  [#1467](https://github.com/bytecodealliance/wasmtime/pull/1467)\n\n* Bindings for .NET have moved to\n  https://github.com/bytecodealliance/wasmtime-dotnet.\n  [#1477](https://github.com/bytecodealliance/wasmtime/pull/1477)\n\n* Passing too many imports to `Instance::new` is now considered an error.\n  [#1478](https://github.com/bytecodealliance/wasmtime/pull/1478)\n\n### Fixed\n\n* Spurious segfaults due to out-of-stack conditions when handling signals have\n  been fixed.\n  [#1315](https://github.com/bytecodealliance/wasmtime/pull/1315)\n\n--------------------------------------------------------------------------------\n\n## 0.15.0\n\nReleased 2020-03-31.\n\n### Fixed\n\nFull release produced for all artifacts to account for hiccups in 0.13.0 and\n0.14.0.\n\n--------------------------------------------------------------------------------\n\n## 0.14.0\n\n*This version ended up not getting a full release*\n\n### Fixed\n\nFix build errors in wasi-common on Windows.\n\n--------------------------------------------------------------------------------\n\n## 0.13.0\n\nReleased 2020-03-24.\n\n### Added\n\n* Lots of documentation of `wasmtime` has been updated. Be sure to check out the\n  [book](https://bytecodealliance.github.io/wasmtime/) and [API\n  documentation](https://bytecodealliance.github.io/wasmtime/api/wasmtime/)!\n\n* All wasmtime example programs are now in a top-level `examples` directory and\n  are available in both C and Rust.\n  [#1286](https://github.com/bytecodealliance/wasmtime/pull/1286)\n\n* A `wasmtime::Linker` type was added to conveniently link link wasm modules\n  together and create instances that reference one another.\n  [#1384](https://github.com/bytecodealliance/wasmtime/pull/1384)\n\n* Wasmtime now has \"jitdump\" support enabled by default which allows [profiling\n  wasm code on linux][jitdump].\n  [#1310](https://github.com/bytecodealliance/wasmtime/pull/1310)\n\n* The `wasmtime::Caller` type now exists as a first-class way to access the\n  caller's exports, namely memory, when implementing host APIs. This can be the\n  first argument of functions defined with `Func::new` or `Func::wrap` which\n  allows easily implementing methods which take a pointer into wasm memory. Note\n  that this only works for accessing the caller's `Memory` for now and it must\n  be exported. This will eventually be replaced with a more general-purpose\n  mechanism like interface types.\n  [#1290](https://github.com/bytecodealliance/wasmtime/pull/1290)\n\n* The bulk memory proposal has been fully implemented.\n  [#1264](https://github.com/bytecodealliance/wasmtime/pull/1264)\n  [#976](https://github.com/bytecodealliance/wasmtime/pull/976)\n\n* Virtual file support has been added to `wasi-common`.\n  [#701](https://github.com/bytecodealliance/wasmtime/pull/701)\n\n* The C API has been enhanced with a Wasmtime-specific `wasmtime_wat2wasm` to\n  parse `*.wat` files via the C API.\n  [#1206](https://github.com/bytecodealliance/wasmtime/pull/1206)\n\n[jitdump]: https://bytecodealliance.github.io/wasmtime/examples-profiling.html\n\n### Changed\n\n* The `wast` and `wasm2obj` standalone binaries have been removed. They're\n  available via the `wasmtime wast` and `wasmtime wasm2obj` subcommands.\n  [#1372](https://github.com/bytecodealliance/wasmtime/pull/1372)\n\n* The `wasi-common` crate now uses the new `wiggle` crate to auto-generate a\n  trait which is implemented for the current wasi snapshot.\n  [#1202](https://github.com/bytecodealliance/wasmtime/pull/1202)\n\n* Wasmtime no longer has a dependency on a C++ compiler.\n  [#1365](https://github.com/bytecodealliance/wasmtime/pull/1365)\n\n* The `Func::wrapN` APIs have been consolidated into one `Func::wrap` API.\n  [#1363](https://github.com/bytecodealliance/wasmtime/pull/1363)\n\n* The `Callable` trait has been removed and now `Func::new` takes a closure\n  directly.\n  [#1363](https://github.com/bytecodealliance/wasmtime/pull/1363)\n\n* The Cranelift repository has been merged into the Wasmtime repository.\n\n* Support for interface types has been temporarily removed.\n  [#1292](https://github.com/bytecodealliance/wasmtime/pull/1292)\n\n* The exit code of the `wasmtime` CLI has changed if the program traps.\n  [#1274](https://github.com/bytecodealliance/wasmtime/pull/1274)\n\n* The `wasmtime` CLI now logs to stderr by default and the `-d` flag has been\n  renamed to `--log-to-file`.\n  [#1266](https://github.com/bytecodealliance/wasmtime/pull/1266)\n\n* Values cannot cross `Store` objects, meaning you can't instantiate a module\n  with values from different stores nor pass values from different stores into\n  methods.\n  [#1016](https://github.com/bytecodealliance/wasmtime/pull/1016)\n\n--------------------------------------------------------------------------------\n\n## 0.12.0\n\nReleased 2020-02-26.\n\n### Added\n\n* Support for the [WebAssembly text annotations proposal][annotations-proposal]\n  has been added.\n  [#998](https://github.com/bytecodealliance/wasmtime/pull/998)\n\n* An initial C API for instantiating WASI modules has been added.\n  [#977](https://github.com/bytecodealliance/wasmtime/pull/977)\n\n* A new suite of `Func::getN` functions have been added to the `wasmtime` API to\n  call statically-known function signatures in a highly optimized fashion.\n  [#955](https://github.com/bytecodealliance/wasmtime/pull/955)\n\n* Initial support for profiling JIT code through perf jitdump has been added.\n  [#360](https://github.com/bytecodealliance/wasmtime/pull/360)\n\n* More CLI flags corresponding to proposed WebAssembly features have been added.\n  [#917](https://github.com/bytecodealliance/wasmtime/pull/917)\n\n[annotations-proposal]: https://github.com/webassembly/annotations\n\n### Changed\n\n* The `wasmtime` CLI as well as embedding API will optimize WebAssembly code by\n  default now.\n  [#973](https://github.com/bytecodealliance/wasmtime/pull/973)\n  [#988](https://github.com/bytecodealliance/wasmtime/pull/988)\n\n* The `verifier` pass in Cranelift is now no longer run by default when using\n  the embedding API.\n  [#882](https://github.com/bytecodealliance/wasmtime/pull/882)\n\n### Fixed\n\n* Code caching now accurately accounts for optimization levels, ensuring that if\n  you ask for optimized code you're not accidentally handed unoptimized code\n  from the cache.\n  [#974](https://github.com/bytecodealliance/wasmtime/pull/974)\n\n* Automated releases for tags should be up and running again, along with\n  automatic publication of the `wasmtime` Python package.\n  [#971](https://github.com/bytecodealliance/wasmtime/pull/971)\n", "//! Data structures for representing decoded wasm modules.\n\nuse crate::{ModuleTranslation, PrimaryMap, Tunables, WASM_PAGE_SIZE};\nuse cranelift_entity::{packed_option::ReservedValue, EntityRef};\nuse indexmap::IndexMap;\nuse serde::{Deserialize, Serialize};\nuse std::collections::BTreeMap;\nuse std::convert::TryFrom;\nuse std::mem;\nuse std::ops::Range;\nuse wasmtime_types::*;\n\n/// Implementation styles for WebAssembly linear memory.\n#[derive(Debug, Clone, Hash, Serialize, Deserialize)]\npub enum MemoryStyle {\n    /// The actual memory can be resized and moved.\n    Dynamic {\n        /// Extra space to reserve when a memory must be moved due to growth.\n        reserve: u64,\n    },\n    /// Address space is allocated up front.\n    Static {\n        /// The number of mapped and unmapped pages.\n        bound: u64,\n    },\n}\n\nimpl MemoryStyle {\n    /// Decide on an implementation style for the given `Memory`.\n    pub fn for_memory(memory: Memory, tunables: &Tunables) -> (Self, u64) {\n        // A heap with a maximum that doesn't exceed the static memory bound specified by the\n        // tunables make it static.\n        //\n        // If the module doesn't declare an explicit maximum treat it as 4GiB when not\n        // requested to use the static memory bound itself as the maximum.\n        let absolute_max_pages = if memory.memory64 {\n            crate::WASM64_MAX_PAGES\n        } else {\n            crate::WASM32_MAX_PAGES\n        };\n        let maximum = std::cmp::min(\n            memory.maximum.unwrap_or(absolute_max_pages),\n            if tunables.static_memory_bound_is_maximum {\n                std::cmp::min(tunables.static_memory_bound, absolute_max_pages)\n            } else {\n                absolute_max_pages\n            },\n        );\n\n        // Ensure the minimum is less than the maximum; the minimum might exceed the maximum\n        // when the memory is artificially bounded via `static_memory_bound_is_maximum` above\n        if memory.minimum <= maximum && maximum <= tunables.static_memory_bound {\n            return (\n                Self::Static {\n                    bound: tunables.static_memory_bound,\n                },\n                tunables.static_memory_offset_guard_size,\n            );\n        }\n\n        // Otherwise, make it dynamic.\n        (\n            Self::Dynamic {\n                reserve: tunables.dynamic_memory_growth_reserve,\n            },\n            tunables.dynamic_memory_offset_guard_size,\n        )\n    }\n}\n\n/// A WebAssembly linear memory description along with our chosen style for\n/// implementing it.\n#[derive(Debug, Clone, Hash, Serialize, Deserialize)]\npub struct MemoryPlan {\n    /// The WebAssembly linear memory description.\n    pub memory: Memory,\n    /// Our chosen implementation style.\n    pub style: MemoryStyle,\n    /// Chosen size of a guard page before the linear memory allocation.\n    pub pre_guard_size: u64,\n    /// Our chosen offset-guard size.\n    pub offset_guard_size: u64,\n}\n\nimpl MemoryPlan {\n    /// Draw up a plan for implementing a `Memory`.\n    pub fn for_memory(memory: Memory, tunables: &Tunables) -> Self {\n        let (style, offset_guard_size) = MemoryStyle::for_memory(memory, tunables);\n        Self {\n            memory,\n            style,\n            offset_guard_size,\n            pre_guard_size: if tunables.guard_before_linear_memory {\n                offset_guard_size\n            } else {\n                0\n            },\n        }\n    }\n}\n\n/// A WebAssembly linear memory initializer.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct MemoryInitializer {\n    /// The index of a linear memory to initialize.\n    pub memory_index: MemoryIndex,\n    /// Optionally, a global variable giving a base index.\n    pub base: Option<GlobalIndex>,\n    /// The offset to add to the base.\n    pub offset: u64,\n    /// The range of the data to write within the linear memory.\n    ///\n    /// This range indexes into a separately stored data section which will be\n    /// provided with the compiled module's code as well.\n    pub data: Range<u32>,\n}\n\n/// Similar to the above `MemoryInitializer` but only used when memory\n/// initializers are statically known to be valid.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct StaticMemoryInitializer {\n    /// The 64-bit offset, in bytes, of where this initializer starts.\n    pub offset: u64,\n\n    /// The range of data to write at `offset`, where these indices are indexes\n    /// into the compiled wasm module's data section.\n    pub data: Range<u32>,\n}\n\n/// The type of WebAssembly linear memory initialization to use for a module.\n#[derive(Debug, Serialize, Deserialize)]\npub enum MemoryInitialization {\n    /// Memory initialization is segmented.\n    ///\n    /// Segmented initialization can be used for any module, but it is required\n    /// if:\n    ///\n    /// * A data segment referenced an imported memory.\n    /// * A data segment uses a global base.\n    ///\n    /// Segmented initialization is performed by processing the complete set of\n    /// data segments when the module is instantiated.\n    ///\n    /// This is the default memory initialization type.\n    Segmented(Vec<MemoryInitializer>),\n\n    /// Memory initialization is statically known and involves a single `memcpy`\n    /// or otherwise simply making the defined data visible.\n    ///\n    /// To be statically initialized everything must reference a defined memory\n    /// and all data segments have a statically known in-bounds base (no\n    /// globals).\n    ///\n    /// This form of memory initialization is a more optimized version of\n    /// `Segmented` where memory can be initialized with one of a few methods:\n    ///\n    /// * First it could be initialized with a single `memcpy` of data from the\n    ///   module to the linear memory.\n    /// * Otherwise techniques like `mmap` are also possible to make this data,\n    ///   which might reside in a compiled module on disk, available immediately\n    ///   in a linear memory's address space.\n    ///\n    /// To facilitate the latter of these techniques the `try_static_init`\n    /// function below, which creates this variant, takes a host page size\n    /// argument which can page-align everything to make mmap-ing possible.\n    Static {\n        /// The initialization contents for each linear memory.\n        ///\n        /// This array has, for each module's own linear memory, the contents\n        /// necessary to initialize it. If the memory has a `None` value then no\n        /// initialization is necessary (it's zero-filled). Otherwise with\n        /// `Some` the first element of the tuple is the offset in memory to\n        /// start the initialization and the `Range` is the range within the\n        /// final data section of the compiled module of bytes to copy into the\n        /// memory.\n        ///\n        /// The offset, range base, and range end are all guaranteed to be page\n        /// aligned to the page size passed in to `try_static_init`.\n        map: PrimaryMap<MemoryIndex, Option<StaticMemoryInitializer>>,\n    },\n}\n\nimpl ModuleTranslation<'_> {\n    /// Attempts to convert segmented memory initialization into static\n    /// initialization for the module that this translation represents.\n    ///\n    /// If this module's memory initialization is not compatible with paged\n    /// initialization then this won't change anything. Otherwise if it is\n    /// compatible then the `memory_initialization` field will be updated.\n    ///\n    /// Takes a `page_size` argument in order to ensure that all\n    /// initialization is page-aligned for mmap-ability, and\n    /// `max_image_size_always_allowed` to control how we decide\n    /// whether to use static init.\n    ///\n    /// We will try to avoid generating very sparse images, which are\n    /// possible if e.g. a module has an initializer at offset 0 and a\n    /// very high offset (say, 1 GiB). To avoid this, we use a dual\n    /// condition: we always allow images less than\n    /// `max_image_size_always_allowed`, and the embedder of Wasmtime\n    /// can set this if desired to ensure that static init should\n    /// always be done if the size of the module or its heaps is\n    /// otherwise bounded by the system. We also allow images with\n    /// static init data bigger than that, but only if it is \"dense\",\n    /// defined as having at least half (50%) of its pages with some\n    /// data.\n    ///\n    /// We could do something slightly better by building a dense part\n    /// and keeping a sparse list of outlier/leftover segments (see\n    /// issue #3820). This would also allow mostly-static init of\n    /// modules that have some dynamically-placed data segments. But,\n    /// for now, this is sufficient to allow a system that \"knows what\n    /// it's doing\" to always get static init.\n    pub fn try_static_init(&mut self, page_size: u64, max_image_size_always_allowed: u64) {\n        // This method only attempts to transform a `Segmented` memory init\n        // into a `Static` one, no other state.\n        if !self.module.memory_initialization.is_segmented() {\n            return;\n        }\n\n        // First a dry run of memory initialization is performed. This\n        // collects information about the extent of memory initialized for each\n        // memory as well as the size of all data segments being copied in.\n        struct Memory {\n            data_size: u64,\n            min_addr: u64,\n            max_addr: u64,\n            // The `usize` here is a pointer into `self.data` which is the list\n            // of data segments corresponding to what was found in the original\n            // wasm module.\n            segments: Vec<(usize, StaticMemoryInitializer)>,\n        }\n        let mut info = PrimaryMap::with_capacity(self.module.memory_plans.len());\n        for _ in 0..self.module.memory_plans.len() {\n            info.push(Memory {\n                data_size: 0,\n                min_addr: u64::MAX,\n                max_addr: 0,\n                segments: Vec::new(),\n            });\n        }\n        let mut idx = 0;\n        let ok = self.module.memory_initialization.init_memory(\n            &mut (),\n            InitMemory::CompileTime(&self.module),\n            |(), memory, init| {\n                // Currently `Static` only applies to locally-defined memories,\n                // so if a data segment references an imported memory then\n                // transitioning to a `Static` memory initializer is not\n                // possible.\n                if self.module.defined_memory_index(memory).is_none() {\n                    return false;\n                };\n                let info = &mut info[memory];\n                let data_len = u64::from(init.data.end - init.data.start);\n                if data_len > 0 {\n                    info.data_size += data_len;\n                    info.min_addr = info.min_addr.min(init.offset);\n                    info.max_addr = info.max_addr.max(init.offset + data_len);\n                    info.segments.push((idx, init.clone()));\n                }\n                idx += 1;\n                true\n            },\n        );\n        if !ok {\n            return;\n        }\n\n        // Validate that the memory information collected is indeed valid for\n        // static memory initialization.\n        for info in info.values().filter(|i| i.data_size > 0) {\n            let image_size = info.max_addr - info.min_addr;\n\n            // If the range of memory being initialized is less than twice the\n            // total size of the data itself then it's assumed that static\n            // initialization is ok. This means we'll at most double memory\n            // consumption during the memory image creation process, which is\n            // currently assumed to \"probably be ok\" but this will likely need\n            // tweaks over time.\n            if image_size < info.data_size.saturating_mul(2) {\n                continue;\n            }\n\n            // If the memory initialization image is larger than the size of all\n            // data, then we still allow memory initialization if the image will\n            // be of a relatively modest size, such as 1MB here.\n            if image_size < max_image_size_always_allowed {\n                continue;\n            }\n\n            // At this point memory initialization is concluded to be too\n            // expensive to do at compile time so it's entirely deferred to\n            // happen at runtime.\n            return;\n        }\n\n        // Here's where we've now committed to changing to static memory. The\n        // memory initialization image is built here from the page data and then\n        // it's converted to a single initializer.\n        let data = mem::replace(&mut self.data, Vec::new());\n        let mut map = PrimaryMap::with_capacity(info.len());\n        let mut module_data_size = 0u32;\n        for (memory, info) in info.iter() {\n            // Create the in-memory `image` which is the initialized contents of\n            // this linear memory.\n            let extent = if info.segments.len() > 0 {\n                (info.max_addr - info.min_addr) as usize\n            } else {\n                0\n            };\n            let mut image = Vec::with_capacity(extent);\n            for (idx, init) in info.segments.iter() {\n                let data = &data[*idx];\n                assert_eq!(data.len(), init.data.len());\n                let offset = usize::try_from(init.offset - info.min_addr).unwrap();\n                if image.len() < offset {\n                    image.resize(offset, 0u8);\n                    image.extend_from_slice(data);\n                } else {\n                    image.splice(\n                        offset..(offset + data.len()).min(image.len()),\n                        data.iter().copied(),\n                    );\n                }\n            }\n            assert_eq!(image.len(), extent);\n            assert_eq!(image.capacity(), extent);\n            let mut offset = if info.segments.len() > 0 {\n                info.min_addr\n            } else {\n                0\n            };\n\n            // Chop off trailing zeros from the image as memory is already\n            // zero-initialized. Note that `i` is the position of a nonzero\n            // entry here, so to not lose it we truncate to `i + 1`.\n            if let Some(i) = image.iter().rposition(|i| *i != 0) {\n                image.truncate(i + 1);\n            }\n\n            // Also chop off leading zeros, if any.\n            if let Some(i) = image.iter().position(|i| *i != 0) {\n                offset += i as u64;\n                image.drain(..i);\n            }\n            let mut len = u64::try_from(image.len()).unwrap();\n\n            // The goal is to enable mapping this image directly into memory, so\n            // the offset into linear memory must be a multiple of the page\n            // size. If that's not already the case then the image is padded at\n            // the front and back with extra zeros as necessary\n            if offset % page_size != 0 {\n                let zero_padding = offset % page_size;\n                self.data.push(vec![0; zero_padding as usize].into());\n                offset -= zero_padding;\n                len += zero_padding;\n            }\n            self.data.push(image.into());\n            if len % page_size != 0 {\n                let zero_padding = page_size - (len % page_size);\n                self.data.push(vec![0; zero_padding as usize].into());\n                len += zero_padding;\n            }\n\n            // Offset/length should now always be page-aligned.\n            assert!(offset % page_size == 0);\n            assert!(len % page_size == 0);\n\n            // Create the `StaticMemoryInitializer` which describes this image,\n            // only needed if the image is actually present and has a nonzero\n            // length. The `offset` has been calculates above, originally\n            // sourced from `info.min_addr`. The `data` field is the extent\n            // within the final data segment we'll emit to an ELF image, which\n            // is the concatenation of `self.data`, so here it's the size of\n            // the section-so-far plus the current segment we're appending.\n            let len = u32::try_from(len).unwrap();\n            let init = if len > 0 {\n                Some(StaticMemoryInitializer {\n                    offset,\n                    data: module_data_size..module_data_size + len,\n                })\n            } else {\n                None\n            };\n            let idx = map.push(init);\n            assert_eq!(idx, memory);\n            module_data_size += len;\n        }\n        self.data_align = Some(page_size);\n        self.module.memory_initialization = MemoryInitialization::Static { map };\n    }\n\n    /// Attempts to convert the module's table initializers to\n    /// FuncTable form where possible. This enables lazy table\n    /// initialization later by providing a one-to-one map of initial\n    /// table values, without having to parse all segments.\n    pub fn try_func_table_init(&mut self) {\n        // This should be large enough to support very large Wasm\n        // modules with huge funcref tables, but small enough to avoid\n        // OOMs or DoS on truly sparse tables.\n        const MAX_FUNC_TABLE_SIZE: u32 = 1024 * 1024;\n\n        let segments = match &self.module.table_initialization {\n            TableInitialization::Segments { segments } => segments,\n            TableInitialization::FuncTable { .. } => {\n                // Already done!\n                return;\n            }\n        };\n\n        // Build the table arrays per-table.\n        let mut tables = PrimaryMap::with_capacity(self.module.table_plans.len());\n        // Keep the \"leftovers\" for eager init.\n        let mut leftovers = vec![];\n\n        for segment in segments {\n            // Skip imported tables: we can't provide a preconstructed\n            // table for them, because their values depend on the\n            // imported table overlaid with whatever segments we have.\n            if self\n                .module\n                .defined_table_index(segment.table_index)\n                .is_none()\n            {\n                leftovers.push(segment.clone());\n                continue;\n            }\n\n            // If this is not a funcref table, then we can't support a\n            // pre-computed table of function indices.\n            if self.module.table_plans[segment.table_index].table.wasm_ty != WasmType::FuncRef {\n                leftovers.push(segment.clone());\n                continue;\n            }\n\n            // If the base of this segment is dynamic, then we can't\n            // include it in the statically-built array of initial\n            // contents.\n            if segment.base.is_some() {\n                leftovers.push(segment.clone());\n                continue;\n            }\n\n            // Get the end of this segment. If out-of-bounds, or too\n            // large for our dense table representation, then skip the\n            // segment.\n            let top = match segment.offset.checked_add(segment.elements.len() as u32) {\n                Some(top) => top,\n                None => {\n                    leftovers.push(segment.clone());\n                    continue;\n                }\n            };\n            let table_size = self.module.table_plans[segment.table_index].table.minimum;\n            if top > table_size || top > MAX_FUNC_TABLE_SIZE {\n                leftovers.push(segment.clone());\n                continue;\n            }\n\n            // We can now incorporate this segment into the initializers array.\n            while tables.len() <= segment.table_index.index() {\n                tables.push(vec![]);\n            }\n            let elements = &mut tables[segment.table_index];\n            if elements.is_empty() {\n                elements.resize(table_size as usize, FuncIndex::reserved_value());\n            }\n\n            let dst = &mut elements[(segment.offset as usize)..(top as usize)];\n            dst.copy_from_slice(&segment.elements[..]);\n        }\n\n        self.module.table_initialization = TableInitialization::FuncTable {\n            tables,\n            segments: leftovers,\n        };\n    }\n}\n\nimpl Default for MemoryInitialization {\n    fn default() -> Self {\n        Self::Segmented(Vec::new())\n    }\n}\n\nimpl MemoryInitialization {\n    /// Returns whether this initialization is of the form\n    /// `MemoryInitialization::Segmented`.\n    pub fn is_segmented(&self) -> bool {\n        match self {\n            MemoryInitialization::Segmented(_) => true,\n            _ => false,\n        }\n    }\n\n    /// Performs the memory initialization steps for this set of initializers.\n    ///\n    /// This will perform wasm initialization in compliance with the wasm spec\n    /// and how data segments are processed. This doesn't need to necessarily\n    /// only be called as part of initialization, however, as it's structured to\n    /// allow learning about memory ahead-of-time at compile time possibly.\n    ///\n    /// The various callbacks provided here are used to drive the smaller bits\n    /// of initialization, such as:\n    ///\n    /// * `get_cur_size_in_pages` - gets the current size, in wasm pages, of the\n    ///   memory specified. For compile-time purposes this would be the memory\n    ///   type's minimum size.\n    ///\n    /// * `get_global` - gets the value of the global specified. This is\n    ///   statically, via validation, a pointer to the global of the correct\n    ///   type (either u32 or u64 depending on the memory), but the value\n    ///   returned here is `u64`. A `None` value can be returned to indicate\n    ///   that the global's value isn't known yet.\n    ///\n    /// * `write` - a callback used to actually write data. This indicates that\n    ///   the specified memory must receive the specified range of data at the\n    ///   specified offset. This can internally return an false error if it\n    ///   wants to fail.\n    ///\n    /// This function will return true if all memory initializers are processed\n    /// successfully. If any initializer hits an error or, for example, a\n    /// global value is needed but `None` is returned, then false will be\n    /// returned. At compile-time this typically means that the \"error\" in\n    /// question needs to be deferred to runtime, and at runtime this means\n    /// that an invalid initializer has been found and a trap should be\n    /// generated.\n    pub fn init_memory<T>(\n        &self,\n        state: &mut T,\n        init: InitMemory<'_, T>,\n        mut write: impl FnMut(&mut T, MemoryIndex, &StaticMemoryInitializer) -> bool,\n    ) -> bool {\n        let initializers = match self {\n            // Fall through below to the segmented memory one-by-one\n            // initialization.\n            MemoryInitialization::Segmented(list) => list,\n\n            // If previously switched to static initialization then pass through\n            // all those parameters here to the `write` callback.\n            //\n            // Note that existence of `Static` already guarantees that all\n            // indices are in-bounds.\n            MemoryInitialization::Static { map } => {\n                for (index, init) in map {\n                    if let Some(init) = init {\n                        let result = write(state, index, init);\n                        if !result {\n                            return result;\n                        }\n                    }\n                }\n                return true;\n            }\n        };\n\n        for initializer in initializers {\n            let MemoryInitializer {\n                memory_index,\n                base,\n                offset,\n                ref data,\n            } = *initializer;\n\n            // First up determine the start/end range and verify that they're\n            // in-bounds for the initial size of the memory at `memory_index`.\n            // Note that this can bail if we don't have access to globals yet\n            // (e.g. this is a task happening before instantiation at\n            // compile-time).\n            let base = match base {\n                Some(index) => match &init {\n                    InitMemory::Runtime {\n                        get_global_as_u64, ..\n                    } => get_global_as_u64(state, index),\n                    InitMemory::CompileTime(_) => return false,\n                },\n                None => 0,\n            };\n            let start = match base.checked_add(offset) {\n                Some(start) => start,\n                None => return false,\n            };\n            let len = u64::try_from(data.len()).unwrap();\n            let end = match start.checked_add(len) {\n                Some(end) => end,\n                None => return false,\n            };\n\n            let cur_size_in_pages = match &init {\n                InitMemory::CompileTime(module) => module.memory_plans[memory_index].memory.minimum,\n                InitMemory::Runtime {\n                    memory_size_in_pages,\n                    ..\n                } => memory_size_in_pages(state, memory_index),\n            };\n\n            // Note that this `minimum` can overflow if `minimum` is\n            // `1 << 48`, the maximum number of minimum pages for 64-bit\n            // memories. If this overflow happens, though, then there's no need\n            // to check the `end` value since `end` fits in a `u64` and it is\n            // naturally less than the overflowed value.\n            //\n            // This is a bit esoteric though because it's impossible to actually\n            // create a memory of `u64::MAX + 1` bytes, so this is largely just\n            // here to avoid having the multiplication here overflow in debug\n            // mode.\n            if let Some(max) = cur_size_in_pages.checked_mul(u64::from(WASM_PAGE_SIZE)) {\n                if end > max {\n                    return false;\n                }\n            }\n\n            // The limits of the data segment have been validated at this point\n            // so the `write` callback is called with the range of data being\n            // written. Any erroneous result is propagated upwards.\n            let init = StaticMemoryInitializer {\n                offset: start,\n                data: data.clone(),\n            };\n            let result = write(state, memory_index, &init);\n            if !result {\n                return result;\n            }\n        }\n\n        return true;\n    }\n}\n\n/// Argument to [`MemoryInitialization::init_memory`] indicating the current\n/// status of the instance.\npub enum InitMemory<'a, T> {\n    /// This evaluation of memory initializers is happening at compile time.\n    /// This means that the current state of memories is whatever their initial\n    /// state is, and additionally globals are not available if data segments\n    /// have global offsets.\n    CompileTime(&'a Module),\n\n    /// Evaluation of memory initializers is happening at runtime when the\n    /// instance is available, and callbacks are provided to learn about the\n    /// instance's state.\n    Runtime {\n        /// Returns the size, in wasm pages, of the the memory specified.\n        memory_size_in_pages: &'a dyn Fn(&mut T, MemoryIndex) -> u64,\n        /// Returns the value of the global, as a `u64`. Note that this may\n        /// involve zero-extending a 32-bit global to a 64-bit number.\n        get_global_as_u64: &'a dyn Fn(&mut T, GlobalIndex) -> u64,\n    },\n}\n\n/// Implementation styles for WebAssembly tables.\n#[derive(Debug, Clone, Hash, Serialize, Deserialize)]\npub enum TableStyle {\n    /// Signatures are stored in the table and checked in the caller.\n    CallerChecksSignature,\n}\n\nimpl TableStyle {\n    /// Decide on an implementation style for the given `Table`.\n    pub fn for_table(_table: Table, _tunables: &Tunables) -> Self {\n        Self::CallerChecksSignature\n    }\n}\n\n/// A WebAssembly table description along with our chosen style for\n/// implementing it.\n#[derive(Debug, Clone, Hash, Serialize, Deserialize)]\npub struct TablePlan {\n    /// The WebAssembly table description.\n    pub table: Table,\n    /// Our chosen implementation style.\n    pub style: TableStyle,\n}\n\nimpl TablePlan {\n    /// Draw up a plan for implementing a `Table`.\n    pub fn for_table(table: Table, tunables: &Tunables) -> Self {\n        let style = TableStyle::for_table(table, tunables);\n        Self { table, style }\n    }\n}\n\n/// A WebAssembly table initializer segment.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct TableInitializer {\n    /// The index of a table to initialize.\n    pub table_index: TableIndex,\n    /// Optionally, a global variable giving a base index.\n    pub base: Option<GlobalIndex>,\n    /// The offset to add to the base.\n    pub offset: u32,\n    /// The values to write into the table elements.\n    pub elements: Box<[FuncIndex]>,\n}\n\n/// Table initialization data for all tables in the module.\n#[derive(Debug, Serialize, Deserialize)]\npub enum TableInitialization {\n    /// \"Segment\" mode: table initializer segments, possibly with\n    /// dynamic bases, possibly applying to an imported memory.\n    ///\n    /// Every kind of table initialization is supported by the\n    /// Segments mode.\n    Segments {\n        /// The segment initializers. All apply to the table for which\n        /// this TableInitialization is specified.\n        segments: Vec<TableInitializer>,\n    },\n\n    /// \"FuncTable\" mode: a single array per table, with a function\n    /// index or null per slot. This is only possible to provide for a\n    /// given table when it is defined by the module itself, and can\n    /// only include data from initializer segments that have\n    /// statically-knowable bases (i.e., not dependent on global\n    /// values).\n    ///\n    /// Any segments that are not compatible with this mode are held\n    /// in the `segments` array of \"leftover segments\", which are\n    /// still processed eagerly.\n    ///\n    /// This mode facilitates lazy initialization of the tables. It is\n    /// thus \"nice to have\", but not necessary for correctness.\n    FuncTable {\n        /// For each table, an array of function indices (or\n        /// FuncIndex::reserved_value(), meaning no initialized value,\n        /// hence null by default). Array elements correspond\n        /// one-to-one to table elements; i.e., `elements[i]` is the\n        /// initial value for `table[i]`.\n        tables: PrimaryMap<TableIndex, Vec<FuncIndex>>,\n\n        /// Leftover segments that need to be processed eagerly on\n        /// instantiation. These either apply to an imported table (so\n        /// we can't pre-build a full image of the table from this\n        /// overlay) or have dynamically (at instantiation time)\n        /// determined bases.\n        segments: Vec<TableInitializer>,\n    },\n}\n\nimpl Default for TableInitialization {\n    fn default() -> Self {\n        TableInitialization::Segments { segments: vec![] }\n    }\n}\n\n/// Different types that can appear in a module.\n///\n/// Note that each of these variants are intended to index further into a\n/// separate table.\n#[derive(Debug, Copy, Clone, Serialize, Deserialize)]\n#[allow(missing_docs)]\npub enum ModuleType {\n    Function(SignatureIndex),\n}\n\nimpl ModuleType {\n    /// Asserts this is a `ModuleType::Function`, returning the underlying\n    /// `SignatureIndex`.\n    pub fn unwrap_function(&self) -> SignatureIndex {\n        match self {\n            ModuleType::Function(f) => *f,\n        }\n    }\n}\n\n/// A translated WebAssembly module, excluding the function bodies and\n/// memory initializers.\n#[derive(Default, Debug, Serialize, Deserialize)]\npub struct Module {\n    /// The name of this wasm module, often found in the wasm file.\n    pub name: Option<String>,\n\n    /// All import records, in the order they are declared in the module.\n    pub initializers: Vec<Initializer>,\n\n    /// Exported entities.\n    pub exports: IndexMap<String, EntityIndex>,\n\n    /// The module \"start\" function, if present.\n    pub start_func: Option<FuncIndex>,\n\n    /// WebAssembly table initialization data, per table.\n    pub table_initialization: TableInitialization,\n\n    /// WebAssembly linear memory initializer.\n    pub memory_initialization: MemoryInitialization,\n\n    /// WebAssembly passive elements.\n    pub passive_elements: Vec<Box<[FuncIndex]>>,\n\n    /// The map from passive element index (element segment index space) to index in `passive_elements`.\n    pub passive_elements_map: BTreeMap<ElemIndex, usize>,\n\n    /// The map from passive data index (data segment index space) to index in `passive_data`.\n    pub passive_data_map: BTreeMap<DataIndex, Range<u32>>,\n\n    /// Types declared in the wasm module.\n    pub types: PrimaryMap<TypeIndex, ModuleType>,\n\n    /// Number of imported or aliased functions in the module.\n    pub num_imported_funcs: usize,\n\n    /// Number of imported or aliased tables in the module.\n    pub num_imported_tables: usize,\n\n    /// Number of imported or aliased memories in the module.\n    pub num_imported_memories: usize,\n\n    /// Number of imported or aliased globals in the module.\n    pub num_imported_globals: usize,\n\n    /// Number of functions that \"escape\" from this module may need to have a\n    /// `VMCallerCheckedFuncRef` constructed for them.\n    ///\n    /// This is also the number of functions in the `functions` array below with\n    /// an `anyfunc` index (and is the maximum anyfunc index).\n    pub num_escaped_funcs: usize,\n\n    /// Types of functions, imported and local.\n    pub functions: PrimaryMap<FuncIndex, FunctionType>,\n\n    /// WebAssembly tables.\n    pub table_plans: PrimaryMap<TableIndex, TablePlan>,\n\n    /// WebAssembly linear memory plans.\n    pub memory_plans: PrimaryMap<MemoryIndex, MemoryPlan>,\n\n    /// WebAssembly global variables.\n    pub globals: PrimaryMap<GlobalIndex, Global>,\n}\n\n/// Initialization routines for creating an instance, encompassing imports,\n/// modules, instances, aliases, etc.\n#[derive(Debug, Serialize, Deserialize)]\npub enum Initializer {\n    /// An imported item is required to be provided.\n    Import {\n        /// Name of this import\n        name: String,\n        /// The field name projection of this import\n        field: String,\n        /// Where this import will be placed, which also has type information\n        /// about the import.\n        index: EntityIndex,\n    },\n}\n\nimpl Module {\n    /// Allocates the module data structures.\n    pub fn new() -> Self {\n        Module::default()\n    }\n\n    /// Convert a `DefinedFuncIndex` into a `FuncIndex`.\n    #[inline]\n    pub fn func_index(&self, defined_func: DefinedFuncIndex) -> FuncIndex {\n        FuncIndex::new(self.num_imported_funcs + defined_func.index())\n    }\n\n    /// Convert a `FuncIndex` into a `DefinedFuncIndex`. Returns None if the\n    /// index is an imported function.\n    #[inline]\n    pub fn defined_func_index(&self, func: FuncIndex) -> Option<DefinedFuncIndex> {\n        if func.index() < self.num_imported_funcs {\n            None\n        } else {\n            Some(DefinedFuncIndex::new(\n                func.index() - self.num_imported_funcs,\n            ))\n        }\n    }\n\n    /// Test whether the given function index is for an imported function.\n    #[inline]\n    pub fn is_imported_function(&self, index: FuncIndex) -> bool {\n        index.index() < self.num_imported_funcs\n    }\n\n    /// Convert a `DefinedTableIndex` into a `TableIndex`.\n    #[inline]\n    pub fn table_index(&self, defined_table: DefinedTableIndex) -> TableIndex {\n        TableIndex::new(self.num_imported_tables + defined_table.index())\n    }\n\n    /// Convert a `TableIndex` into a `DefinedTableIndex`. Returns None if the\n    /// index is an imported table.\n    #[inline]\n    pub fn defined_table_index(&self, table: TableIndex) -> Option<DefinedTableIndex> {\n        if table.index() < self.num_imported_tables {\n            None\n        } else {\n            Some(DefinedTableIndex::new(\n                table.index() - self.num_imported_tables,\n            ))\n        }\n    }\n\n    /// Test whether the given table index is for an imported table.\n    #[inline]\n    pub fn is_imported_table(&self, index: TableIndex) -> bool {\n        index.index() < self.num_imported_tables\n    }\n\n    /// Convert a `DefinedMemoryIndex` into a `MemoryIndex`.\n    #[inline]\n    pub fn memory_index(&self, defined_memory: DefinedMemoryIndex) -> MemoryIndex {\n        MemoryIndex::new(self.num_imported_memories + defined_memory.index())\n    }\n\n    /// Convert a `MemoryIndex` into a `DefinedMemoryIndex`. Returns None if the\n    /// index is an imported memory.\n    #[inline]\n    pub fn defined_memory_index(&self, memory: MemoryIndex) -> Option<DefinedMemoryIndex> {\n        if memory.index() < self.num_imported_memories {\n            None\n        } else {\n            Some(DefinedMemoryIndex::new(\n                memory.index() - self.num_imported_memories,\n            ))\n        }\n    }\n\n    /// Convert a `DefinedMemoryIndex` into an `OwnedMemoryIndex`. Returns None\n    /// if the index is an imported memory.\n    #[inline]\n    pub fn owned_memory_index(&self, memory: DefinedMemoryIndex) -> OwnedMemoryIndex {\n        assert!(\n            memory.index() < self.memory_plans.len(),\n            \"non-shared memory must have an owned index\"\n        );\n\n        // Once we know that the memory index is not greater than the number of\n        // plans, we can iterate through the plans up to the memory index and\n        // count how many are not shared (i.e., owned).\n        let owned_memory_index = self\n            .memory_plans\n            .iter()\n            .skip(self.num_imported_memories)\n            .take(memory.index())\n            .filter(|(_, mp)| !mp.memory.shared)\n            .count();\n        OwnedMemoryIndex::new(owned_memory_index)\n    }\n\n    /// Test whether the given memory index is for an imported memory.\n    #[inline]\n    pub fn is_imported_memory(&self, index: MemoryIndex) -> bool {\n        index.index() < self.num_imported_memories\n    }\n\n    /// Convert a `DefinedGlobalIndex` into a `GlobalIndex`.\n    #[inline]\n    pub fn global_index(&self, defined_global: DefinedGlobalIndex) -> GlobalIndex {\n        GlobalIndex::new(self.num_imported_globals + defined_global.index())\n    }\n\n    /// Convert a `GlobalIndex` into a `DefinedGlobalIndex`. Returns None if the\n    /// index is an imported global.\n    #[inline]\n    pub fn defined_global_index(&self, global: GlobalIndex) -> Option<DefinedGlobalIndex> {\n        if global.index() < self.num_imported_globals {\n            None\n        } else {\n            Some(DefinedGlobalIndex::new(\n                global.index() - self.num_imported_globals,\n            ))\n        }\n    }\n\n    /// Test whether the given global index is for an imported global.\n    #[inline]\n    pub fn is_imported_global(&self, index: GlobalIndex) -> bool {\n        index.index() < self.num_imported_globals\n    }\n\n    /// Returns an iterator of all the imports in this module, along with their\n    /// module name, field name, and type that's being imported.\n    pub fn imports(&self) -> impl ExactSizeIterator<Item = (&str, &str, EntityType)> {\n        self.initializers.iter().map(move |i| match i {\n            Initializer::Import { name, field, index } => {\n                (name.as_str(), field.as_str(), self.type_of(*index))\n            }\n        })\n    }\n\n    /// Returns the type of an item based on its index\n    pub fn type_of(&self, index: EntityIndex) -> EntityType {\n        match index {\n            EntityIndex::Global(i) => EntityType::Global(self.globals[i]),\n            EntityIndex::Table(i) => EntityType::Table(self.table_plans[i].table),\n            EntityIndex::Memory(i) => EntityType::Memory(self.memory_plans[i].memory),\n            EntityIndex::Function(i) => EntityType::Function(self.functions[i].signature),\n        }\n    }\n\n    /// Appends a new function to this module with the given type information,\n    /// used for functions that either don't escape or aren't certain whether\n    /// they escape yet.\n    pub fn push_function(&mut self, signature: SignatureIndex) -> FuncIndex {\n        self.functions.push(FunctionType {\n            signature,\n            anyfunc: AnyfuncIndex::reserved_value(),\n        })\n    }\n\n    /// Appends a new function to this module with the given type information.\n    pub fn push_escaped_function(\n        &mut self,\n        signature: SignatureIndex,\n        anyfunc: AnyfuncIndex,\n    ) -> FuncIndex {\n        self.functions.push(FunctionType { signature, anyfunc })\n    }\n}\n\n/// Type information about functions in a wasm module.\n#[derive(Debug, Serialize, Deserialize)]\npub struct FunctionType {\n    /// The type of this function, indexed into the module-wide type tables for\n    /// a module compilation.\n    pub signature: SignatureIndex,\n    /// The index into the anyfunc table, if present. Note that this is\n    /// `reserved_value()` if the function does not escape from a module.\n    pub anyfunc: AnyfuncIndex,\n}\n\nimpl FunctionType {\n    /// Returns whether this function's type is one that \"escapes\" the current\n    /// module, meaning that the function is exported, used in `ref.func`, used\n    /// in a table, etc.\n    pub fn is_escaping(&self) -> bool {\n        !self.anyfunc.is_reserved_value()\n    }\n}\n\n/// Index into the anyfunc table within a VMContext for a function.\n#[derive(Copy, Clone, PartialEq, Eq, Hash, PartialOrd, Ord, Debug, Serialize, Deserialize)]\npub struct AnyfuncIndex(u32);\ncranelift_entity::entity_impl!(AnyfuncIndex);\n", "//! An `Instance` contains all the runtime state used by execution of a\n//! wasm module (except its callstack and register state). An\n//! `InstanceHandle` is a reference-counting handle for an `Instance`.\n\nuse crate::export::Export;\nuse crate::externref::VMExternRefActivationsTable;\nuse crate::memory::{Memory, RuntimeMemoryCreator};\nuse crate::table::{Table, TableElement, TableElementType};\nuse crate::vmcontext::{\n    VMBuiltinFunctionsArray, VMCallerCheckedFuncRef, VMContext, VMFunctionImport,\n    VMGlobalDefinition, VMGlobalImport, VMMemoryDefinition, VMMemoryImport, VMOpaqueContext,\n    VMRuntimeLimits, VMTableDefinition, VMTableImport, VMCONTEXT_MAGIC,\n};\nuse crate::{\n    ExportFunction, ExportGlobal, ExportMemory, ExportTable, Imports, ModuleRuntimeInfo, Store,\n    VMFunctionBody, VMSharedSignatureIndex, WasmFault,\n};\nuse anyhow::Error;\nuse anyhow::Result;\nuse memoffset::offset_of;\nuse std::alloc::{self, Layout};\nuse std::any::Any;\nuse std::convert::TryFrom;\nuse std::hash::Hash;\nuse std::ops::Range;\nuse std::ptr::NonNull;\nuse std::sync::atomic::AtomicU64;\nuse std::sync::Arc;\nuse std::{mem, ptr};\nuse wasmtime_environ::{\n    packed_option::ReservedValue, DataIndex, DefinedGlobalIndex, DefinedMemoryIndex,\n    DefinedTableIndex, ElemIndex, EntityIndex, EntityRef, EntitySet, FuncIndex, GlobalIndex,\n    GlobalInit, HostPtr, MemoryIndex, Module, PrimaryMap, SignatureIndex, TableIndex,\n    TableInitialization, Trap, VMOffsets, WasmType,\n};\n\nmod allocator;\n\npub use allocator::*;\n\n/// A type that roughly corresponds to a WebAssembly instance, but is also used\n/// for host-defined objects.\n///\n/// This structure is is never allocated directly but is instead managed through\n/// an `InstanceHandle`. This structure ends with a `VMContext` which has a\n/// dynamic size corresponding to the `module` configured within. Memory\n/// management of this structure is always externalized.\n///\n/// Instances here can correspond to actual instantiated modules, but it's also\n/// used ubiquitously for host-defined objects. For example creating a\n/// host-defined memory will have a `module` that looks like it exports a single\n/// memory (and similar for other constructs).\n///\n/// This `Instance` type is used as a ubiquitous representation for WebAssembly\n/// values, whether or not they were created on the host or through a module.\n#[repr(C)] // ensure that the vmctx field is last.\npub(crate) struct Instance {\n    /// The runtime info (corresponding to the \"compiled module\"\n    /// abstraction in higher layers) that is retained and needed for\n    /// lazy initialization. This provides access to the underlying\n    /// Wasm module entities, the compiled JIT code, metadata about\n    /// functions, lazy initialization state, etc.\n    runtime_info: Arc<dyn ModuleRuntimeInfo>,\n\n    /// WebAssembly linear memory data.\n    ///\n    /// This is where all runtime information about defined linear memories in\n    /// this module lives.\n    memories: PrimaryMap<DefinedMemoryIndex, Memory>,\n\n    /// WebAssembly table data.\n    ///\n    /// Like memories, this is only for defined tables in the module and\n    /// contains all of their runtime state.\n    tables: PrimaryMap<DefinedTableIndex, Table>,\n\n    /// Stores the dropped passive element segments in this instantiation by index.\n    /// If the index is present in the set, the segment has been dropped.\n    dropped_elements: EntitySet<ElemIndex>,\n\n    /// Stores the dropped passive data segments in this instantiation by index.\n    /// If the index is present in the set, the segment has been dropped.\n    dropped_data: EntitySet<DataIndex>,\n\n    /// Hosts can store arbitrary per-instance information here.\n    ///\n    /// Most of the time from Wasmtime this is `Box::new(())`, a noop\n    /// allocation, but some host-defined objects will store their state here.\n    host_state: Box<dyn Any + Send + Sync>,\n\n    /// Instance of this instance within its `InstanceAllocator` trait\n    /// implementation.\n    ///\n    /// This is always 0 for the on-demand instance allocator and it's the\n    /// index of the slot in the pooling allocator.\n    index: usize,\n\n    /// Additional context used by compiled wasm code. This field is last, and\n    /// represents a dynamically-sized array that extends beyond the nominal\n    /// end of the struct (similar to a flexible array member).\n    vmctx: VMContext,\n}\n\n#[allow(clippy::cast_ptr_alignment)]\nimpl Instance {\n    /// Create an instance at the given memory address.\n    ///\n    /// It is assumed the memory was properly aligned and the\n    /// allocation was `alloc_size` in bytes.\n    unsafe fn new(\n        req: InstanceAllocationRequest,\n        index: usize,\n        memories: PrimaryMap<DefinedMemoryIndex, Memory>,\n        tables: PrimaryMap<DefinedTableIndex, Table>,\n    ) -> InstanceHandle {\n        // The allocation must be *at least* the size required of `Instance`.\n        let layout = Self::alloc_layout(req.runtime_info.offsets());\n        let ptr = alloc::alloc(layout);\n        if ptr.is_null() {\n            alloc::handle_alloc_error(layout);\n        }\n        let ptr = ptr.cast::<Instance>();\n\n        let module = req.runtime_info.module();\n        let dropped_elements = EntitySet::with_capacity(module.passive_elements.len());\n        let dropped_data = EntitySet::with_capacity(module.passive_data_map.len());\n\n        ptr::write(\n            ptr,\n            Instance {\n                runtime_info: req.runtime_info.clone(),\n                index,\n                memories,\n                tables,\n                dropped_elements,\n                dropped_data,\n                host_state: req.host_state,\n                vmctx: VMContext {\n                    _marker: std::marker::PhantomPinned,\n                },\n            },\n        );\n\n        (*ptr).initialize_vmctx(module, req.runtime_info.offsets(), req.store, req.imports);\n        InstanceHandle { instance: ptr }\n    }\n\n    /// Helper function to access various locations offset from our `*mut\n    /// VMContext` object.\n    unsafe fn vmctx_plus_offset<T>(&self, offset: u32) -> *const T {\n        (std::ptr::addr_of!(self.vmctx).cast::<u8>())\n            .add(usize::try_from(offset).unwrap())\n            .cast()\n    }\n\n    unsafe fn vmctx_plus_offset_mut<T>(&mut self, offset: u32) -> *mut T {\n        (std::ptr::addr_of_mut!(self.vmctx).cast::<u8>())\n            .add(usize::try_from(offset).unwrap())\n            .cast()\n    }\n\n    pub(crate) fn module(&self) -> &Arc<Module> {\n        self.runtime_info.module()\n    }\n\n    fn offsets(&self) -> &VMOffsets<HostPtr> {\n        self.runtime_info.offsets()\n    }\n\n    /// Return the indexed `VMFunctionImport`.\n    fn imported_function(&self, index: FuncIndex) -> &VMFunctionImport {\n        unsafe { &*self.vmctx_plus_offset(self.offsets().vmctx_vmfunction_import(index)) }\n    }\n\n    /// Return the index `VMTableImport`.\n    fn imported_table(&self, index: TableIndex) -> &VMTableImport {\n        unsafe { &*self.vmctx_plus_offset(self.offsets().vmctx_vmtable_import(index)) }\n    }\n\n    /// Return the indexed `VMMemoryImport`.\n    fn imported_memory(&self, index: MemoryIndex) -> &VMMemoryImport {\n        unsafe { &*self.vmctx_plus_offset(self.offsets().vmctx_vmmemory_import(index)) }\n    }\n\n    /// Return the indexed `VMGlobalImport`.\n    fn imported_global(&self, index: GlobalIndex) -> &VMGlobalImport {\n        unsafe { &*self.vmctx_plus_offset(self.offsets().vmctx_vmglobal_import(index)) }\n    }\n\n    /// Return the indexed `VMTableDefinition`.\n    #[allow(dead_code)]\n    fn table(&mut self, index: DefinedTableIndex) -> VMTableDefinition {\n        unsafe { *self.table_ptr(index) }\n    }\n\n    /// Updates the value for a defined table to `VMTableDefinition`.\n    fn set_table(&mut self, index: DefinedTableIndex, table: VMTableDefinition) {\n        unsafe {\n            *self.table_ptr(index) = table;\n        }\n    }\n\n    /// Return the indexed `VMTableDefinition`.\n    fn table_ptr(&mut self, index: DefinedTableIndex) -> *mut VMTableDefinition {\n        unsafe { self.vmctx_plus_offset_mut(self.offsets().vmctx_vmtable_definition(index)) }\n    }\n\n    /// Get a locally defined or imported memory.\n    pub(crate) fn get_memory(&self, index: MemoryIndex) -> VMMemoryDefinition {\n        if let Some(defined_index) = self.module().defined_memory_index(index) {\n            self.memory(defined_index)\n        } else {\n            let import = self.imported_memory(index);\n            unsafe { VMMemoryDefinition::load(import.from) }\n        }\n    }\n\n    /// Get a locally defined or imported memory.\n    pub(crate) fn get_runtime_memory(&mut self, index: MemoryIndex) -> &mut Memory {\n        if let Some(defined_index) = self.module().defined_memory_index(index) {\n            unsafe { &mut *self.get_defined_memory(defined_index) }\n        } else {\n            let import = self.imported_memory(index);\n            let ctx = unsafe { &mut *import.vmctx };\n            unsafe { &mut *ctx.instance_mut().get_defined_memory(import.index) }\n        }\n    }\n\n    /// Return the indexed `VMMemoryDefinition`.\n    fn memory(&self, index: DefinedMemoryIndex) -> VMMemoryDefinition {\n        unsafe { VMMemoryDefinition::load(self.memory_ptr(index)) }\n    }\n\n    /// Set the indexed memory to `VMMemoryDefinition`.\n    fn set_memory(&self, index: DefinedMemoryIndex, mem: VMMemoryDefinition) {\n        unsafe {\n            *self.memory_ptr(index) = mem;\n        }\n    }\n\n    /// Return the indexed `VMMemoryDefinition`.\n    fn memory_ptr(&self, index: DefinedMemoryIndex) -> *mut VMMemoryDefinition {\n        unsafe { *self.vmctx_plus_offset(self.offsets().vmctx_vmmemory_pointer(index)) }\n    }\n\n    /// Return the indexed `VMGlobalDefinition`.\n    fn global(&mut self, index: DefinedGlobalIndex) -> &VMGlobalDefinition {\n        unsafe { &*self.global_ptr(index) }\n    }\n\n    /// Return the indexed `VMGlobalDefinition`.\n    fn global_ptr(&mut self, index: DefinedGlobalIndex) -> *mut VMGlobalDefinition {\n        unsafe { self.vmctx_plus_offset_mut(self.offsets().vmctx_vmglobal_definition(index)) }\n    }\n\n    /// Get a raw pointer to the global at the given index regardless whether it\n    /// is defined locally or imported from another module.\n    ///\n    /// Panics if the index is out of bound or is the reserved value.\n    pub(crate) fn defined_or_imported_global_ptr(\n        &mut self,\n        index: GlobalIndex,\n    ) -> *mut VMGlobalDefinition {\n        if let Some(index) = self.module().defined_global_index(index) {\n            self.global_ptr(index)\n        } else {\n            self.imported_global(index).from\n        }\n    }\n\n    /// Return a pointer to the interrupts structure\n    pub fn runtime_limits(&mut self) -> *mut *const VMRuntimeLimits {\n        unsafe { self.vmctx_plus_offset_mut(self.offsets().vmctx_runtime_limits()) }\n    }\n\n    /// Return a pointer to the global epoch counter used by this instance.\n    pub fn epoch_ptr(&mut self) -> *mut *const AtomicU64 {\n        unsafe { self.vmctx_plus_offset_mut(self.offsets().vmctx_epoch_ptr()) }\n    }\n\n    /// Return a pointer to the `VMExternRefActivationsTable`.\n    pub fn externref_activations_table(&mut self) -> *mut *mut VMExternRefActivationsTable {\n        unsafe { self.vmctx_plus_offset_mut(self.offsets().vmctx_externref_activations_table()) }\n    }\n\n    /// Gets a pointer to this instance's `Store` which was originally\n    /// configured on creation.\n    ///\n    /// # Panics\n    ///\n    /// This will panic if the originally configured store was `None`. That can\n    /// happen for host functions so host functions can't be queried what their\n    /// original `Store` was since it's just retained as null (since host\n    /// functions are shared amongst threads and don't all share the same\n    /// store).\n    #[inline]\n    pub fn store(&self) -> *mut dyn Store {\n        let ptr =\n            unsafe { *self.vmctx_plus_offset::<*mut dyn Store>(self.offsets().vmctx_store()) };\n        assert!(!ptr.is_null());\n        ptr\n    }\n\n    pub unsafe fn set_store(&mut self, store: Option<*mut dyn Store>) {\n        if let Some(store) = store {\n            *self.vmctx_plus_offset_mut(self.offsets().vmctx_store()) = store;\n            *self.runtime_limits() = (*store).vmruntime_limits();\n            *self.epoch_ptr() = (*store).epoch_ptr();\n            *self.externref_activations_table() = (*store).externref_activations_table().0;\n        } else {\n            assert_eq!(\n                mem::size_of::<*mut dyn Store>(),\n                mem::size_of::<[*mut (); 2]>()\n            );\n            *self.vmctx_plus_offset_mut::<[*mut (); 2]>(self.offsets().vmctx_store()) =\n                [ptr::null_mut(), ptr::null_mut()];\n\n            *self.runtime_limits() = ptr::null_mut();\n            *self.epoch_ptr() = ptr::null_mut();\n            *self.externref_activations_table() = ptr::null_mut();\n        }\n    }\n\n    pub(crate) unsafe fn set_callee(&mut self, callee: Option<NonNull<VMFunctionBody>>) {\n        *self.vmctx_plus_offset_mut(self.offsets().vmctx_callee()) =\n            callee.map_or(ptr::null_mut(), |c| c.as_ptr());\n    }\n\n    /// Return a reference to the vmctx used by compiled wasm code.\n    #[inline]\n    pub fn vmctx(&self) -> &VMContext {\n        &self.vmctx\n    }\n\n    /// Return a raw pointer to the vmctx used by compiled wasm code.\n    #[inline]\n    pub fn vmctx_ptr(&self) -> *mut VMContext {\n        self.vmctx() as *const VMContext as *mut VMContext\n    }\n\n    fn get_exported_func(&mut self, index: FuncIndex) -> ExportFunction {\n        let anyfunc = self.get_caller_checked_anyfunc(index).unwrap();\n        let anyfunc = NonNull::new(anyfunc as *const VMCallerCheckedFuncRef as *mut _).unwrap();\n        ExportFunction { anyfunc }\n    }\n\n    fn get_exported_table(&mut self, index: TableIndex) -> ExportTable {\n        let (definition, vmctx) = if let Some(def_index) = self.module().defined_table_index(index)\n        {\n            (self.table_ptr(def_index), self.vmctx_ptr())\n        } else {\n            let import = self.imported_table(index);\n            (import.from, import.vmctx)\n        };\n        ExportTable {\n            definition,\n            vmctx,\n            table: self.module().table_plans[index].clone(),\n        }\n    }\n\n    fn get_exported_memory(&mut self, index: MemoryIndex) -> ExportMemory {\n        let (definition, vmctx, def_index) =\n            if let Some(def_index) = self.module().defined_memory_index(index) {\n                (self.memory_ptr(def_index), self.vmctx_ptr(), def_index)\n            } else {\n                let import = self.imported_memory(index);\n                (import.from, import.vmctx, import.index)\n            };\n        ExportMemory {\n            definition,\n            vmctx,\n            memory: self.module().memory_plans[index].clone(),\n            index: def_index,\n        }\n    }\n\n    fn get_exported_global(&mut self, index: GlobalIndex) -> ExportGlobal {\n        ExportGlobal {\n            definition: if let Some(def_index) = self.module().defined_global_index(index) {\n                self.global_ptr(def_index)\n            } else {\n                self.imported_global(index).from\n            },\n            global: self.module().globals[index],\n        }\n    }\n\n    /// Return an iterator over the exports of this instance.\n    ///\n    /// Specifically, it provides access to the key-value pairs, where the keys\n    /// are export names, and the values are export declarations which can be\n    /// resolved `lookup_by_declaration`.\n    pub fn exports(&self) -> indexmap::map::Iter<String, EntityIndex> {\n        self.module().exports.iter()\n    }\n\n    /// Return a reference to the custom state attached to this instance.\n    #[inline]\n    pub fn host_state(&self) -> &dyn Any {\n        &*self.host_state\n    }\n\n    /// Return the offset from the vmctx pointer to its containing Instance.\n    #[inline]\n    pub(crate) fn vmctx_offset() -> isize {\n        offset_of!(Self, vmctx) as isize\n    }\n\n    /// Return the table index for the given `VMTableDefinition`.\n    unsafe fn table_index(&mut self, table: &VMTableDefinition) -> DefinedTableIndex {\n        let index = DefinedTableIndex::new(\n            usize::try_from(\n                (table as *const VMTableDefinition)\n                    .offset_from(self.table_ptr(DefinedTableIndex::new(0))),\n            )\n            .unwrap(),\n        );\n        assert!(index.index() < self.tables.len());\n        index\n    }\n\n    /// Grow memory by the specified amount of pages.\n    ///\n    /// Returns `None` if memory can't be grown by the specified amount\n    /// of pages. Returns `Some` with the old size in bytes if growth was\n    /// successful.\n    pub(crate) fn memory_grow(\n        &mut self,\n        index: MemoryIndex,\n        delta: u64,\n    ) -> Result<Option<usize>, Error> {\n        let (idx, instance) = if let Some(idx) = self.module().defined_memory_index(index) {\n            (idx, self)\n        } else {\n            let import = self.imported_memory(index);\n            unsafe {\n                let foreign_instance = (*import.vmctx).instance_mut();\n                (import.index, foreign_instance)\n            }\n        };\n        let store = unsafe { &mut *instance.store() };\n        let memory = &mut instance.memories[idx];\n\n        let result = unsafe { memory.grow(delta, Some(store)) };\n\n        // Update the state used by a non-shared Wasm memory in case the base\n        // pointer and/or the length changed.\n        if memory.as_shared_memory().is_none() {\n            let vmmemory = memory.vmmemory();\n            instance.set_memory(idx, vmmemory);\n        }\n\n        result\n    }\n\n    pub(crate) fn table_element_type(&mut self, table_index: TableIndex) -> TableElementType {\n        unsafe { (*self.get_table(table_index)).element_type() }\n    }\n\n    /// Grow table by the specified amount of elements, filling them with\n    /// `init_value`.\n    ///\n    /// Returns `None` if table can't be grown by the specified amount of\n    /// elements, or if `init_value` is the wrong type of table element.\n    pub(crate) fn table_grow(\n        &mut self,\n        table_index: TableIndex,\n        delta: u32,\n        init_value: TableElement,\n    ) -> Result<Option<u32>, Error> {\n        let (defined_table_index, instance) =\n            self.get_defined_table_index_and_instance(table_index);\n        instance.defined_table_grow(defined_table_index, delta, init_value)\n    }\n\n    fn defined_table_grow(\n        &mut self,\n        table_index: DefinedTableIndex,\n        delta: u32,\n        init_value: TableElement,\n    ) -> Result<Option<u32>, Error> {\n        let store = unsafe { &mut *self.store() };\n        let table = self\n            .tables\n            .get_mut(table_index)\n            .unwrap_or_else(|| panic!(\"no table for index {}\", table_index.index()));\n\n        let result = unsafe { table.grow(delta, init_value, store) };\n\n        // Keep the `VMContext` pointers used by compiled Wasm code up to\n        // date.\n        let element = self.tables[table_index].vmtable();\n        self.set_table(table_index, element);\n\n        result\n    }\n\n    fn alloc_layout(offsets: &VMOffsets<HostPtr>) -> Layout {\n        let size = mem::size_of::<Self>()\n            .checked_add(usize::try_from(offsets.size_of_vmctx()).unwrap())\n            .unwrap();\n        let align = mem::align_of::<Self>();\n        Layout::from_size_align(size, align).unwrap()\n    }\n\n    /// Construct a new VMCallerCheckedFuncRef for the given function\n    /// (imported or defined in this module) and store into the given\n    /// location. Used during lazy initialization.\n    ///\n    /// Note that our current lazy-init scheme actually calls this every\n    /// time the anyfunc pointer is fetched; this turns out to be better\n    /// than tracking state related to whether it's been initialized\n    /// before, because resetting that state on (re)instantiation is\n    /// very expensive if there are many anyfuncs.\n    fn construct_anyfunc(\n        &mut self,\n        index: FuncIndex,\n        sig: SignatureIndex,\n        into: *mut VMCallerCheckedFuncRef,\n    ) {\n        let type_index = unsafe {\n            let base: *const VMSharedSignatureIndex =\n                *self.vmctx_plus_offset_mut(self.offsets().vmctx_signature_ids_array());\n            *base.add(sig.index())\n        };\n\n        let (func_ptr, vmctx) = if let Some(def_index) = self.module().defined_func_index(index) {\n            (\n                self.runtime_info.function(def_index),\n                VMOpaqueContext::from_vmcontext(self.vmctx_ptr()),\n            )\n        } else {\n            let import = self.imported_function(index);\n            (import.body.as_ptr(), import.vmctx)\n        };\n\n        // Safety: we have a `&mut self`, so we have exclusive access\n        // to this Instance.\n        unsafe {\n            *into = VMCallerCheckedFuncRef {\n                vmctx,\n                type_index,\n                func_ptr: NonNull::new(func_ptr).expect(\"Non-null function pointer\"),\n            };\n        }\n    }\n\n    /// Get a `&VMCallerCheckedFuncRef` for the given `FuncIndex`.\n    ///\n    /// Returns `None` if the index is the reserved index value.\n    ///\n    /// The returned reference is a stable reference that won't be moved and can\n    /// be passed into JIT code.\n    pub(crate) fn get_caller_checked_anyfunc(\n        &mut self,\n        index: FuncIndex,\n    ) -> Option<*mut VMCallerCheckedFuncRef> {\n        if index == FuncIndex::reserved_value() {\n            return None;\n        }\n\n        // Safety: we have a `&mut self`, so we have exclusive access\n        // to this Instance.\n        unsafe {\n            // For now, we eagerly initialize an anyfunc struct in-place\n            // whenever asked for a reference to it. This is mostly\n            // fine, because in practice each anyfunc is unlikely to be\n            // requested more than a few times: once-ish for funcref\n            // tables used for call_indirect (the usual compilation\n            // strategy places each function in the table at most once),\n            // and once or a few times when fetching exports via API.\n            // Note that for any case driven by table accesses, the lazy\n            // table init behaves like a higher-level cache layer that\n            // protects this initialization from happening multiple\n            // times, via that particular table at least.\n            //\n            // When `ref.func` becomes more commonly used or if we\n            // otherwise see a use-case where this becomes a hotpath,\n            // we can reconsider by using some state to track\n            // \"uninitialized\" explicitly, for example by zeroing the\n            // anyfuncs (perhaps together with other\n            // zeroed-at-instantiate-time state) or using a separate\n            // is-initialized bitmap.\n            //\n            // We arrived at this design because zeroing memory is\n            // expensive, so it's better for instantiation performance\n            // if we don't have to track \"is-initialized\" state at\n            // all!\n            let func = &self.module().functions[index];\n            let sig = func.signature;\n            let anyfunc: *mut VMCallerCheckedFuncRef = self\n                .vmctx_plus_offset_mut::<VMCallerCheckedFuncRef>(\n                    self.offsets().vmctx_anyfunc(func.anyfunc),\n                );\n            self.construct_anyfunc(index, sig, anyfunc);\n\n            Some(anyfunc)\n        }\n    }\n\n    /// The `table.init` operation: initializes a portion of a table with a\n    /// passive element.\n    ///\n    /// # Errors\n    ///\n    /// Returns a `Trap` error when the range within the table is out of bounds\n    /// or the range within the passive element is out of bounds.\n    pub(crate) fn table_init(\n        &mut self,\n        table_index: TableIndex,\n        elem_index: ElemIndex,\n        dst: u32,\n        src: u32,\n        len: u32,\n    ) -> Result<(), Trap> {\n        // TODO: this `clone()` shouldn't be necessary but is used for now to\n        // inform `rustc` that the lifetime of the elements here are\n        // disconnected from the lifetime of `self`.\n        let module = self.module().clone();\n\n        let elements = match module.passive_elements_map.get(&elem_index) {\n            Some(index) if !self.dropped_elements.contains(elem_index) => {\n                module.passive_elements[*index].as_ref()\n            }\n            _ => &[],\n        };\n        self.table_init_segment(table_index, elements, dst, src, len)\n    }\n\n    pub(crate) fn table_init_segment(\n        &mut self,\n        table_index: TableIndex,\n        elements: &[FuncIndex],\n        dst: u32,\n        src: u32,\n        len: u32,\n    ) -> Result<(), Trap> {\n        // https://webassembly.github.io/bulk-memory-operations/core/exec/instructions.html#exec-table-init\n\n        let table = unsafe { &mut *self.get_table(table_index) };\n\n        let elements = match elements\n            .get(usize::try_from(src).unwrap()..)\n            .and_then(|s| s.get(..usize::try_from(len).unwrap()))\n        {\n            Some(elements) => elements,\n            None => return Err(Trap::TableOutOfBounds),\n        };\n\n        match table.element_type() {\n            TableElementType::Func => {\n                table.init_funcs(\n                    dst,\n                    elements.iter().map(|idx| {\n                        self.get_caller_checked_anyfunc(*idx)\n                            .unwrap_or(std::ptr::null_mut())\n                    }),\n                )?;\n            }\n\n            TableElementType::Extern => {\n                debug_assert!(elements.iter().all(|e| *e == FuncIndex::reserved_value()));\n                table.fill(dst, TableElement::ExternRef(None), len)?;\n            }\n        }\n        Ok(())\n    }\n\n    /// Drop an element.\n    pub(crate) fn elem_drop(&mut self, elem_index: ElemIndex) {\n        // https://webassembly.github.io/reference-types/core/exec/instructions.html#exec-elem-drop\n\n        self.dropped_elements.insert(elem_index);\n\n        // Note that we don't check that we actually removed a segment because\n        // dropping a non-passive segment is a no-op (not a trap).\n    }\n\n    /// Get a locally-defined memory.\n    pub(crate) fn get_defined_memory(&mut self, index: DefinedMemoryIndex) -> *mut Memory {\n        ptr::addr_of_mut!(self.memories[index])\n    }\n\n    /// Do a `memory.copy`\n    ///\n    /// # Errors\n    ///\n    /// Returns a `Trap` error when the source or destination ranges are out of\n    /// bounds.\n    pub(crate) fn memory_copy(\n        &mut self,\n        dst_index: MemoryIndex,\n        dst: u64,\n        src_index: MemoryIndex,\n        src: u64,\n        len: u64,\n    ) -> Result<(), Trap> {\n        // https://webassembly.github.io/reference-types/core/exec/instructions.html#exec-memory-copy\n\n        let src_mem = self.get_memory(src_index);\n        let dst_mem = self.get_memory(dst_index);\n\n        let src = self.validate_inbounds(src_mem.current_length(), src, len)?;\n        let dst = self.validate_inbounds(dst_mem.current_length(), dst, len)?;\n\n        // Bounds and casts are checked above, by this point we know that\n        // everything is safe.\n        unsafe {\n            let dst = dst_mem.base.add(dst);\n            let src = src_mem.base.add(src);\n            // FIXME audit whether this is safe in the presence of shared memory\n            // (https://github.com/bytecodealliance/wasmtime/issues/4203).\n            ptr::copy(src, dst, len as usize);\n        }\n\n        Ok(())\n    }\n\n    fn validate_inbounds(&self, max: usize, ptr: u64, len: u64) -> Result<usize, Trap> {\n        let oob = || Trap::MemoryOutOfBounds;\n        let end = ptr\n            .checked_add(len)\n            .and_then(|i| usize::try_from(i).ok())\n            .ok_or_else(oob)?;\n        if end > max {\n            Err(oob())\n        } else {\n            Ok(ptr as usize)\n        }\n    }\n\n    /// Perform the `memory.fill` operation on a locally defined memory.\n    ///\n    /// # Errors\n    ///\n    /// Returns a `Trap` error if the memory range is out of bounds.\n    pub(crate) fn memory_fill(\n        &mut self,\n        memory_index: MemoryIndex,\n        dst: u64,\n        val: u8,\n        len: u64,\n    ) -> Result<(), Trap> {\n        let memory = self.get_memory(memory_index);\n        let dst = self.validate_inbounds(memory.current_length(), dst, len)?;\n\n        // Bounds and casts are checked above, by this point we know that\n        // everything is safe.\n        unsafe {\n            let dst = memory.base.add(dst);\n            // FIXME audit whether this is safe in the presence of shared memory\n            // (https://github.com/bytecodealliance/wasmtime/issues/4203).\n            ptr::write_bytes(dst, val, len as usize);\n        }\n\n        Ok(())\n    }\n\n    /// Performs the `memory.init` operation.\n    ///\n    /// # Errors\n    ///\n    /// Returns a `Trap` error if the destination range is out of this module's\n    /// memory's bounds or if the source range is outside the data segment's\n    /// bounds.\n    pub(crate) fn memory_init(\n        &mut self,\n        memory_index: MemoryIndex,\n        data_index: DataIndex,\n        dst: u64,\n        src: u32,\n        len: u32,\n    ) -> Result<(), Trap> {\n        let range = match self.module().passive_data_map.get(&data_index).cloned() {\n            Some(range) if !self.dropped_data.contains(data_index) => range,\n            _ => 0..0,\n        };\n        self.memory_init_segment(memory_index, range, dst, src, len)\n    }\n\n    pub(crate) fn wasm_data(&self, range: Range<u32>) -> &[u8] {\n        &self.runtime_info.wasm_data()[range.start as usize..range.end as usize]\n    }\n\n    pub(crate) fn memory_init_segment(\n        &mut self,\n        memory_index: MemoryIndex,\n        range: Range<u32>,\n        dst: u64,\n        src: u32,\n        len: u32,\n    ) -> Result<(), Trap> {\n        // https://webassembly.github.io/bulk-memory-operations/core/exec/instructions.html#exec-memory-init\n\n        let memory = self.get_memory(memory_index);\n        let data = self.wasm_data(range);\n        let dst = self.validate_inbounds(memory.current_length(), dst, len.into())?;\n        let src = self.validate_inbounds(data.len(), src.into(), len.into())?;\n        let len = len as usize;\n\n        unsafe {\n            let src_start = data.as_ptr().add(src);\n            let dst_start = memory.base.add(dst);\n            // FIXME audit whether this is safe in the presence of shared memory\n            // (https://github.com/bytecodealliance/wasmtime/issues/4203).\n            ptr::copy_nonoverlapping(src_start, dst_start, len);\n        }\n\n        Ok(())\n    }\n\n    /// Drop the given data segment, truncating its length to zero.\n    pub(crate) fn data_drop(&mut self, data_index: DataIndex) {\n        self.dropped_data.insert(data_index);\n\n        // Note that we don't check that we actually removed a segment because\n        // dropping a non-passive segment is a no-op (not a trap).\n    }\n\n    /// Get a table by index regardless of whether it is locally-defined\n    /// or an imported, foreign table. Ensure that the given range of\n    /// elements in the table is lazily initialized.  We define this\n    /// operation all-in-one for safety, to ensure the lazy-init\n    /// happens.\n    ///\n    /// Takes an `Iterator` for the index-range to lazy-initialize,\n    /// for flexibility. This can be a range, single item, or empty\n    /// sequence, for example. The iterator should return indices in\n    /// increasing order, so that the break-at-out-of-bounds behavior\n    /// works correctly.\n    pub(crate) fn get_table_with_lazy_init(\n        &mut self,\n        table_index: TableIndex,\n        range: impl Iterator<Item = u32>,\n    ) -> *mut Table {\n        let (idx, instance) = self.get_defined_table_index_and_instance(table_index);\n        let elt_ty = instance.tables[idx].element_type();\n\n        if elt_ty == TableElementType::Func {\n            for i in range {\n                let value = match instance.tables[idx].get(i) {\n                    Some(value) => value,\n                    None => {\n                        // Out-of-bounds; caller will handle by likely\n                        // throwing a trap. No work to do to lazy-init\n                        // beyond the end.\n                        break;\n                    }\n                };\n                if value.is_uninit() {\n                    let table_init = match &instance.module().table_initialization {\n                        // We unfortunately can't borrow `tables`\n                        // outside the loop because we need to call\n                        // `get_caller_checked_anyfunc` (a `&mut`\n                        // method) below; so unwrap it dynamically\n                        // here.\n                        TableInitialization::FuncTable { tables, .. } => tables,\n                        _ => break,\n                    }\n                    .get(table_index);\n\n                    // The TableInitialization::FuncTable elements table may\n                    // be smaller than the current size of the table: it\n                    // always matches the initial table size, if present. We\n                    // want to iterate up through the end of the accessed\n                    // index range so that we set an \"initialized null\" even\n                    // if there is no initializer. We do a checked `get()` on\n                    // the initializer table below and unwrap to a null if\n                    // we're past its end.\n                    let func_index =\n                        table_init.and_then(|indices| indices.get(i as usize).cloned());\n                    let anyfunc = func_index\n                        .and_then(|func_index| instance.get_caller_checked_anyfunc(func_index))\n                        .unwrap_or(std::ptr::null_mut());\n\n                    let value = TableElement::FuncRef(anyfunc);\n\n                    instance.tables[idx]\n                        .set(i, value)\n                        .expect(\"Table type should match and index should be in-bounds\");\n                }\n            }\n        }\n\n        ptr::addr_of_mut!(instance.tables[idx])\n    }\n\n    /// Get a table by index regardless of whether it is locally-defined or an\n    /// imported, foreign table.\n    pub(crate) fn get_table(&mut self, table_index: TableIndex) -> *mut Table {\n        let (idx, instance) = self.get_defined_table_index_and_instance(table_index);\n        ptr::addr_of_mut!(instance.tables[idx])\n    }\n\n    /// Get a locally-defined table.\n    pub(crate) fn get_defined_table(&mut self, index: DefinedTableIndex) -> *mut Table {\n        ptr::addr_of_mut!(self.tables[index])\n    }\n\n    pub(crate) fn get_defined_table_index_and_instance(\n        &mut self,\n        index: TableIndex,\n    ) -> (DefinedTableIndex, &mut Instance) {\n        if let Some(defined_table_index) = self.module().defined_table_index(index) {\n            (defined_table_index, self)\n        } else {\n            let import = self.imported_table(index);\n            unsafe {\n                let foreign_instance = (*import.vmctx).instance_mut();\n                let foreign_table_def = &*import.from;\n                let foreign_table_index = foreign_instance.table_index(foreign_table_def);\n                (foreign_table_index, foreign_instance)\n            }\n        }\n    }\n\n    /// Initialize the VMContext data associated with this Instance.\n    ///\n    /// The `VMContext` memory is assumed to be uninitialized; any field\n    /// that we need in a certain state will be explicitly written by this\n    /// function.\n    unsafe fn initialize_vmctx(\n        &mut self,\n        module: &Module,\n        offsets: &VMOffsets<HostPtr>,\n        store: StorePtr,\n        imports: Imports,\n    ) {\n        assert!(std::ptr::eq(module, self.module().as_ref()));\n\n        *self.vmctx_plus_offset_mut(offsets.vmctx_magic()) = VMCONTEXT_MAGIC;\n        self.set_callee(None);\n        self.set_store(store.as_raw());\n\n        // Initialize shared signatures\n        let signatures = self.runtime_info.signature_ids();\n        *self.vmctx_plus_offset_mut(offsets.vmctx_signature_ids_array()) = signatures.as_ptr();\n\n        // Initialize the built-in functions\n        *self.vmctx_plus_offset_mut(offsets.vmctx_builtin_functions()) =\n            &VMBuiltinFunctionsArray::INIT;\n\n        // Initialize the imports\n        debug_assert_eq!(imports.functions.len(), module.num_imported_funcs);\n        ptr::copy_nonoverlapping(\n            imports.functions.as_ptr(),\n            self.vmctx_plus_offset_mut(offsets.vmctx_imported_functions_begin()),\n            imports.functions.len(),\n        );\n        debug_assert_eq!(imports.tables.len(), module.num_imported_tables);\n        ptr::copy_nonoverlapping(\n            imports.tables.as_ptr(),\n            self.vmctx_plus_offset_mut(offsets.vmctx_imported_tables_begin()),\n            imports.tables.len(),\n        );\n        debug_assert_eq!(imports.memories.len(), module.num_imported_memories);\n        ptr::copy_nonoverlapping(\n            imports.memories.as_ptr(),\n            self.vmctx_plus_offset_mut(offsets.vmctx_imported_memories_begin()),\n            imports.memories.len(),\n        );\n        debug_assert_eq!(imports.globals.len(), module.num_imported_globals);\n        ptr::copy_nonoverlapping(\n            imports.globals.as_ptr(),\n            self.vmctx_plus_offset_mut(offsets.vmctx_imported_globals_begin()),\n            imports.globals.len(),\n        );\n\n        // N.B.: there is no need to initialize the anyfuncs array because\n        // we eagerly construct each element in it whenever asked for a\n        // reference to that element. In other words, there is no state\n        // needed to track the lazy-init, so we don't need to initialize\n        // any state now.\n\n        // Initialize the defined tables\n        let mut ptr = self.vmctx_plus_offset_mut(offsets.vmctx_tables_begin());\n        for i in 0..module.table_plans.len() - module.num_imported_tables {\n            ptr::write(ptr, self.tables[DefinedTableIndex::new(i)].vmtable());\n            ptr = ptr.add(1);\n        }\n\n        // Initialize the defined memories. This fills in both the\n        // `defined_memories` table and the `owned_memories` table at the same\n        // time. Entries in `defined_memories` hold a pointer to a definition\n        // (all memories) whereas the `owned_memories` hold the actual\n        // definitions of memories owned (not shared) in the module.\n        let mut ptr = self.vmctx_plus_offset_mut(offsets.vmctx_memories_begin());\n        let mut owned_ptr = self.vmctx_plus_offset_mut(offsets.vmctx_owned_memories_begin());\n        for i in 0..module.memory_plans.len() - module.num_imported_memories {\n            let defined_memory_index = DefinedMemoryIndex::new(i);\n            let memory_index = module.memory_index(defined_memory_index);\n            if module.memory_plans[memory_index].memory.shared {\n                let def_ptr = self.memories[defined_memory_index]\n                    .as_shared_memory()\n                    .unwrap()\n                    .vmmemory_ptr();\n                ptr::write(ptr, def_ptr.cast_mut());\n            } else {\n                ptr::write(owned_ptr, self.memories[defined_memory_index].vmmemory());\n                ptr::write(ptr, owned_ptr);\n                owned_ptr = owned_ptr.add(1);\n            }\n            ptr = ptr.add(1);\n        }\n\n        // Initialize the defined globals\n        self.initialize_vmctx_globals(module);\n    }\n\n    unsafe fn initialize_vmctx_globals(&mut self, module: &Module) {\n        let num_imports = module.num_imported_globals;\n        for (index, global) in module.globals.iter().skip(num_imports) {\n            let def_index = module.defined_global_index(index).unwrap();\n            let to = self.global_ptr(def_index);\n\n            // Initialize the global before writing to it\n            ptr::write(to, VMGlobalDefinition::new());\n\n            match global.initializer {\n                GlobalInit::I32Const(x) => *(*to).as_i32_mut() = x,\n                GlobalInit::I64Const(x) => *(*to).as_i64_mut() = x,\n                GlobalInit::F32Const(x) => *(*to).as_f32_bits_mut() = x,\n                GlobalInit::F64Const(x) => *(*to).as_f64_bits_mut() = x,\n                GlobalInit::V128Const(x) => *(*to).as_u128_mut() = x,\n                GlobalInit::GetGlobal(x) => {\n                    let from = if let Some(def_x) = module.defined_global_index(x) {\n                        self.global(def_x)\n                    } else {\n                        &*self.imported_global(x).from\n                    };\n                    // Globals of type `externref` need to manage the reference\n                    // count as values move between globals, everything else is just\n                    // copy-able bits.\n                    match global.wasm_ty {\n                        WasmType::ExternRef => {\n                            *(*to).as_externref_mut() = from.as_externref().clone()\n                        }\n                        _ => ptr::copy_nonoverlapping(from, to, 1),\n                    }\n                }\n                GlobalInit::RefFunc(f) => {\n                    *(*to).as_anyfunc_mut() = self.get_caller_checked_anyfunc(f).unwrap()\n                        as *const VMCallerCheckedFuncRef;\n                }\n                GlobalInit::RefNullConst => match global.wasm_ty {\n                    // `VMGlobalDefinition::new()` already zeroed out the bits\n                    WasmType::FuncRef => {}\n                    WasmType::ExternRef => {}\n                    ty => panic!(\"unsupported reference type for global: {:?}\", ty),\n                },\n                GlobalInit::Import => panic!(\"locally-defined global initialized as import\"),\n            }\n        }\n    }\n\n    fn wasm_fault(&self, addr: usize) -> Option<WasmFault> {\n        let mut fault = None;\n        for (_, memory) in self.memories.iter() {\n            let accessible = memory.wasm_accessible();\n            if accessible.start <= addr && addr < accessible.end {\n                // All linear memories should be disjoint so assert that no\n                // prior fault has been found.\n                assert!(fault.is_none());\n                fault = Some(WasmFault {\n                    memory_size: memory.byte_size(),\n                    wasm_address: u64::try_from(addr - accessible.start).unwrap(),\n                });\n            }\n        }\n        fault\n    }\n}\n\nimpl Drop for Instance {\n    fn drop(&mut self) {\n        // Drop any defined globals\n        let module = self.module().clone();\n        for (idx, global) in module.globals.iter() {\n            let idx = match module.defined_global_index(idx) {\n                Some(idx) => idx,\n                None => continue,\n            };\n            match global.wasm_ty {\n                // For now only externref globals need to get destroyed\n                WasmType::ExternRef => {}\n                _ => continue,\n            }\n            unsafe {\n                drop((*self.global_ptr(idx)).as_externref_mut().take());\n            }\n        }\n    }\n}\n\n/// A handle holding an `Instance` of a WebAssembly module.\n#[derive(Hash, PartialEq, Eq)]\npub struct InstanceHandle {\n    instance: *mut Instance,\n}\n\n// These are only valid if the `Instance` type is send/sync, hence the\n// assertion below.\nunsafe impl Send for InstanceHandle {}\nunsafe impl Sync for InstanceHandle {}\n\nfn _assert_send_sync() {\n    fn _assert<T: Send + Sync>() {}\n    _assert::<Instance>();\n}\n\nimpl InstanceHandle {\n    /// Create a new `InstanceHandle` pointing at the instance\n    /// pointed to by the given `VMContext` pointer.\n    ///\n    /// # Safety\n    /// This is unsafe because it doesn't work on just any `VMContext`, it must\n    /// be a `VMContext` allocated as part of an `Instance`.\n    #[inline]\n    pub unsafe fn from_vmctx(vmctx: *mut VMContext) -> Self {\n        let instance = (&mut *vmctx).instance();\n        Self {\n            instance: instance as *const Instance as *mut Instance,\n        }\n    }\n\n    /// Return a reference to the vmctx used by compiled wasm code.\n    pub fn vmctx(&self) -> &VMContext {\n        self.instance().vmctx()\n    }\n\n    /// Return a raw pointer to the vmctx used by compiled wasm code.\n    #[inline]\n    pub fn vmctx_ptr(&self) -> *mut VMContext {\n        self.instance().vmctx_ptr()\n    }\n\n    /// Return a reference to a module.\n    pub fn module(&self) -> &Arc<Module> {\n        self.instance().module()\n    }\n\n    /// Lookup a function by index.\n    pub fn get_exported_func(&mut self, export: FuncIndex) -> ExportFunction {\n        self.instance_mut().get_exported_func(export)\n    }\n\n    /// Lookup a global by index.\n    pub fn get_exported_global(&mut self, export: GlobalIndex) -> ExportGlobal {\n        self.instance_mut().get_exported_global(export)\n    }\n\n    /// Lookup a memory by index.\n    pub fn get_exported_memory(&mut self, export: MemoryIndex) -> ExportMemory {\n        self.instance_mut().get_exported_memory(export)\n    }\n\n    /// Lookup a table by index.\n    pub fn get_exported_table(&mut self, export: TableIndex) -> ExportTable {\n        self.instance_mut().get_exported_table(export)\n    }\n\n    /// Lookup an item with the given index.\n    pub fn get_export_by_index(&mut self, export: EntityIndex) -> Export {\n        match export {\n            EntityIndex::Function(i) => Export::Function(self.get_exported_func(i)),\n            EntityIndex::Global(i) => Export::Global(self.get_exported_global(i)),\n            EntityIndex::Table(i) => Export::Table(self.get_exported_table(i)),\n            EntityIndex::Memory(i) => Export::Memory(self.get_exported_memory(i)),\n        }\n    }\n\n    /// Return an iterator over the exports of this instance.\n    ///\n    /// Specifically, it provides access to the key-value pairs, where the keys\n    /// are export names, and the values are export declarations which can be\n    /// resolved `lookup_by_declaration`.\n    pub fn exports(&self) -> indexmap::map::Iter<String, EntityIndex> {\n        self.instance().exports()\n    }\n\n    /// Return a reference to the custom state attached to this instance.\n    pub fn host_state(&self) -> &dyn Any {\n        self.instance().host_state()\n    }\n\n    /// Get a memory defined locally within this module.\n    pub fn get_defined_memory(&mut self, index: DefinedMemoryIndex) -> *mut Memory {\n        self.instance_mut().get_defined_memory(index)\n    }\n\n    /// Return the table index for the given `VMTableDefinition` in this instance.\n    pub unsafe fn table_index(&mut self, table: &VMTableDefinition) -> DefinedTableIndex {\n        self.instance_mut().table_index(table)\n    }\n\n    /// Get a table defined locally within this module.\n    pub fn get_defined_table(&mut self, index: DefinedTableIndex) -> *mut Table {\n        self.instance_mut().get_defined_table(index)\n    }\n\n    /// Get a table defined locally within this module, lazily\n    /// initializing the given range first.\n    pub fn get_defined_table_with_lazy_init(\n        &mut self,\n        index: DefinedTableIndex,\n        range: impl Iterator<Item = u32>,\n    ) -> *mut Table {\n        let index = self.instance().module().table_index(index);\n        self.instance_mut().get_table_with_lazy_init(index, range)\n    }\n\n    /// Return a reference to the contained `Instance`.\n    #[inline]\n    pub(crate) fn instance(&self) -> &Instance {\n        unsafe { &*(self.instance as *const Instance) }\n    }\n\n    pub(crate) fn instance_mut(&mut self) -> &mut Instance {\n        unsafe { &mut *self.instance }\n    }\n\n    /// Returns the `Store` pointer that was stored on creation\n    #[inline]\n    pub fn store(&self) -> *mut dyn Store {\n        self.instance().store()\n    }\n\n    /// Configure the `*mut dyn Store` internal pointer after-the-fact.\n    ///\n    /// This is provided for the original `Store` itself to configure the first\n    /// self-pointer after the original `Box` has been initialized.\n    pub unsafe fn set_store(&mut self, store: *mut dyn Store) {\n        self.instance_mut().set_store(Some(store));\n    }\n\n    /// Returns a clone of this instance.\n    ///\n    /// This is unsafe because the returned handle here is just a cheap clone\n    /// of the internals, there's no lifetime tracking around its validity.\n    /// You'll need to ensure that the returned handles all go out of scope at\n    /// the same time.\n    #[inline]\n    pub unsafe fn clone(&self) -> InstanceHandle {\n        InstanceHandle {\n            instance: self.instance,\n        }\n    }\n\n    /// Performs post-initialization of an instance after its handle has been\n    /// creqtaed and registered with a store.\n    ///\n    /// Failure of this function means that the instance still must persist\n    /// within the store since failure may indicate partial failure, or some\n    /// state could be referenced by other instances.\n    pub fn initialize(&mut self, module: &Module, is_bulk_memory: bool) -> Result<()> {\n        allocator::initialize_instance(self.instance_mut(), module, is_bulk_memory)\n    }\n\n    /// Attempts to convert from the host `addr` specified to a WebAssembly\n    /// based address recorded in `WasmFault`.\n    ///\n    /// This method will check all linear memories that this instance contains\n    /// to see if any of them contain `addr`. If one does then `Some` is\n    /// returned with metadata about the wasm fault. Otherwise `None` is\n    /// returned and `addr` doesn't belong to this instance.\n    pub fn wasm_fault(&self, addr: usize) -> Option<WasmFault> {\n        self.instance().wasm_fault(addr)\n    }\n}\n", "use crate::imports::Imports;\nuse crate::instance::{Instance, InstanceHandle, RuntimeMemoryCreator};\nuse crate::memory::{DefaultMemoryCreator, Memory};\nuse crate::table::Table;\nuse crate::{CompiledModuleId, ModuleRuntimeInfo, Store};\nuse anyhow::{anyhow, bail, Result};\nuse std::alloc;\nuse std::any::Any;\nuse std::convert::TryFrom;\nuse std::ptr;\nuse std::sync::Arc;\nuse wasmtime_environ::{\n    DefinedMemoryIndex, DefinedTableIndex, HostPtr, InitMemory, MemoryInitialization,\n    MemoryInitializer, Module, PrimaryMap, TableInitialization, TableInitializer, Trap, VMOffsets,\n    WasmType, WASM_PAGE_SIZE,\n};\n\n#[cfg(feature = \"pooling-allocator\")]\nmod pooling;\n\n#[cfg(feature = \"pooling-allocator\")]\npub use self::pooling::{InstanceLimits, PoolingInstanceAllocator, PoolingInstanceAllocatorConfig};\n\n/// Represents a request for a new runtime instance.\npub struct InstanceAllocationRequest<'a> {\n    /// The info related to the compiled version of this module,\n    /// needed for instantiation: function metadata, JIT code\n    /// addresses, precomputed images for lazy memory and table\n    /// initialization, and the like. This Arc is cloned and held for\n    /// the lifetime of the instance.\n    pub runtime_info: &'a Arc<dyn ModuleRuntimeInfo>,\n\n    /// The imports to use for the instantiation.\n    pub imports: Imports<'a>,\n\n    /// The host state to associate with the instance.\n    pub host_state: Box<dyn Any + Send + Sync>,\n\n    /// A pointer to the \"store\" for this instance to be allocated. The store\n    /// correlates with the `Store` in wasmtime itself, and lots of contextual\n    /// information about the execution of wasm can be learned through the store.\n    ///\n    /// Note that this is a raw pointer and has a static lifetime, both of which\n    /// are a bit of a lie. This is done purely so a store can learn about\n    /// itself when it gets called as a host function, and additionally so this\n    /// runtime can access internals as necessary (such as the\n    /// VMExternRefActivationsTable or the resource limiter methods).\n    ///\n    /// Note that this ends up being a self-pointer to the instance when stored.\n    /// The reason is that the instance itself is then stored within the store.\n    /// We use a number of `PhantomPinned` declarations to indicate this to the\n    /// compiler. More info on this in `wasmtime/src/store.rs`\n    pub store: StorePtr,\n}\n\n/// A pointer to a Store. This Option<*mut dyn Store> is wrapped in a struct\n/// so that the function to create a &mut dyn Store is a method on a member of\n/// InstanceAllocationRequest, rather than on a &mut InstanceAllocationRequest\n/// itself, because several use-sites require a split mut borrow on the\n/// InstanceAllocationRequest.\npub struct StorePtr(Option<*mut dyn Store>);\nimpl StorePtr {\n    /// A pointer to no Store.\n    pub fn empty() -> Self {\n        Self(None)\n    }\n    /// A pointer to a Store.\n    pub fn new(ptr: *mut dyn Store) -> Self {\n        Self(Some(ptr))\n    }\n    /// The raw contents of this struct\n    pub fn as_raw(&self) -> Option<*mut dyn Store> {\n        self.0.clone()\n    }\n    /// Use the StorePtr as a mut ref to the Store.\n    /// Safety: must not be used outside the original lifetime of the borrow.\n    pub(crate) unsafe fn get(&mut self) -> Option<&mut dyn Store> {\n        match self.0 {\n            Some(ptr) => Some(&mut *ptr),\n            None => None,\n        }\n    }\n}\n\n/// Represents a runtime instance allocator.\n///\n/// # Safety\n///\n/// This trait is unsafe as it requires knowledge of Wasmtime's runtime internals to implement correctly.\npub unsafe trait InstanceAllocator {\n    /// Validates that a module is supported by the allocator.\n    fn validate(&self, module: &Module, offsets: &VMOffsets<HostPtr>) -> Result<()> {\n        drop((module, offsets));\n        Ok(())\n    }\n\n    /// Allocates a fresh `InstanceHandle` for the `req` given.\n    ///\n    /// This will allocate memories and tables internally from this allocator\n    /// and weave that altogether into a final and complete `InstanceHandle`\n    /// ready to be registered with a store.\n    ///\n    /// Note that the returned instance must still have `.initialize(..)` called\n    /// on it to complete the instantiation process.\n    fn allocate(&self, mut req: InstanceAllocationRequest) -> Result<InstanceHandle> {\n        let index = self.allocate_index(&req)?;\n        let module = req.runtime_info.module();\n        let mut memories =\n            PrimaryMap::with_capacity(module.memory_plans.len() - module.num_imported_memories);\n        let mut tables =\n            PrimaryMap::with_capacity(module.table_plans.len() - module.num_imported_tables);\n\n        let result = self\n            .allocate_memories(index, &mut req, &mut memories)\n            .and_then(|()| self.allocate_tables(index, &mut req, &mut tables));\n        if let Err(e) = result {\n            self.deallocate_memories(index, &mut memories);\n            self.deallocate_tables(index, &mut tables);\n            self.deallocate_index(index);\n            return Err(e);\n        }\n\n        unsafe { Ok(Instance::new(req, index, memories, tables)) }\n    }\n\n    /// Deallocates the provided instance.\n    ///\n    /// This will null-out the pointer within `handle` and otherwise reclaim\n    /// resources such as tables, memories, and the instance memory itself.\n    fn deallocate(&self, handle: &mut InstanceHandle) {\n        let index = handle.instance().index;\n        self.deallocate_memories(index, &mut handle.instance_mut().memories);\n        self.deallocate_tables(index, &mut handle.instance_mut().tables);\n        unsafe {\n            let layout = Instance::alloc_layout(handle.instance().offsets());\n            ptr::drop_in_place(handle.instance);\n            alloc::dealloc(handle.instance.cast(), layout);\n            handle.instance = std::ptr::null_mut();\n        }\n        self.deallocate_index(index);\n    }\n\n    /// Optionally allocates an allocator-defined index for the `req` provided.\n    ///\n    /// The return value here, if successful, is passed to the various methods\n    /// below for memory/table allocation/deallocation.\n    fn allocate_index(&self, req: &InstanceAllocationRequest) -> Result<usize>;\n\n    /// Deallocates indices allocated by `allocate_index`.\n    fn deallocate_index(&self, index: usize);\n\n    /// Attempts to allocate all defined linear memories for a module.\n    ///\n    /// Pushes all memories for `req` onto the `mems` storage provided which is\n    /// already appropriately allocated to contain all memories.\n    ///\n    /// Note that this is allowed to fail. Failure can additionally happen after\n    /// some memories have already been successfully allocated. All memories\n    /// pushed onto `mem` are guaranteed to one day make their way to\n    /// `deallocate_memories`.\n    fn allocate_memories(\n        &self,\n        index: usize,\n        req: &mut InstanceAllocationRequest,\n        mems: &mut PrimaryMap<DefinedMemoryIndex, Memory>,\n    ) -> Result<()>;\n\n    /// Deallocates all memories provided, optionally reclaiming resources for\n    /// the pooling allocator for example.\n    fn deallocate_memories(&self, index: usize, mems: &mut PrimaryMap<DefinedMemoryIndex, Memory>);\n\n    /// Same as `allocate_memories`, but for tables.\n    fn allocate_tables(\n        &self,\n        index: usize,\n        req: &mut InstanceAllocationRequest,\n        tables: &mut PrimaryMap<DefinedTableIndex, Table>,\n    ) -> Result<()>;\n\n    /// Same as `deallocate_memories`, but for tables.\n    fn deallocate_tables(&self, index: usize, tables: &mut PrimaryMap<DefinedTableIndex, Table>);\n\n    /// Allocates a fiber stack for calling async functions on.\n    #[cfg(feature = \"async\")]\n    fn allocate_fiber_stack(&self) -> Result<wasmtime_fiber::FiberStack>;\n\n    /// Deallocates a fiber stack that was previously allocated with `allocate_fiber_stack`.\n    ///\n    /// # Safety\n    ///\n    /// The provided stack is required to have been allocated with `allocate_fiber_stack`.\n    #[cfg(feature = \"async\")]\n    unsafe fn deallocate_fiber_stack(&self, stack: &wasmtime_fiber::FiberStack);\n\n    /// Purges all lingering resources related to `module` from within this\n    /// allocator.\n    ///\n    /// Primarily present for the pooling allocator to remove mappings of\n    /// this module from slots in linear memory.\n    fn purge_module(&self, module: CompiledModuleId);\n}\n\nfn get_table_init_start(init: &TableInitializer, instance: &mut Instance) -> Result<u32> {\n    match init.base {\n        Some(base) => {\n            let val = unsafe { *(*instance.defined_or_imported_global_ptr(base)).as_u32() };\n\n            init.offset\n                .checked_add(val)\n                .ok_or_else(|| anyhow!(\"element segment global base overflows\"))\n        }\n        None => Ok(init.offset),\n    }\n}\n\nfn check_table_init_bounds(instance: &mut Instance, module: &Module) -> Result<()> {\n    match &module.table_initialization {\n        TableInitialization::FuncTable { segments, .. }\n        | TableInitialization::Segments { segments } => {\n            for segment in segments {\n                let table = unsafe { &*instance.get_table(segment.table_index) };\n                let start = get_table_init_start(segment, instance)?;\n                let start = usize::try_from(start).unwrap();\n                let end = start.checked_add(segment.elements.len());\n\n                match end {\n                    Some(end) if end <= table.size() as usize => {\n                        // Initializer is in bounds\n                    }\n                    _ => {\n                        bail!(\"table out of bounds: elements segment does not fit\")\n                    }\n                }\n            }\n        }\n    }\n\n    Ok(())\n}\n\nfn initialize_tables(instance: &mut Instance, module: &Module) -> Result<()> {\n    // Note: if the module's table initializer state is in\n    // FuncTable mode, we will lazily initialize tables based on\n    // any statically-precomputed image of FuncIndexes, but there\n    // may still be \"leftover segments\" that could not be\n    // incorporated. So we have a unified handler here that\n    // iterates over all segments (Segments mode) or leftover\n    // segments (FuncTable mode) to initialize.\n    match &module.table_initialization {\n        TableInitialization::FuncTable { segments, .. }\n        | TableInitialization::Segments { segments } => {\n            for segment in segments {\n                let start = get_table_init_start(segment, instance)?;\n                instance.table_init_segment(\n                    segment.table_index,\n                    &segment.elements,\n                    start,\n                    0,\n                    segment.elements.len() as u32,\n                )?;\n            }\n        }\n    }\n\n    Ok(())\n}\n\nfn get_memory_init_start(init: &MemoryInitializer, instance: &mut Instance) -> Result<u64> {\n    match init.base {\n        Some(base) => {\n            let mem64 = instance.module().memory_plans[init.memory_index]\n                .memory\n                .memory64;\n            let val = unsafe {\n                let global = instance.defined_or_imported_global_ptr(base);\n                if mem64 {\n                    *(*global).as_u64()\n                } else {\n                    u64::from(*(*global).as_u32())\n                }\n            };\n\n            init.offset\n                .checked_add(val)\n                .ok_or_else(|| anyhow!(\"data segment global base overflows\"))\n        }\n        None => Ok(init.offset),\n    }\n}\n\nfn check_memory_init_bounds(\n    instance: &mut Instance,\n    initializers: &[MemoryInitializer],\n) -> Result<()> {\n    for init in initializers {\n        let memory = instance.get_memory(init.memory_index);\n        let start = get_memory_init_start(init, instance)?;\n        let end = usize::try_from(start)\n            .ok()\n            .and_then(|start| start.checked_add(init.data.len()));\n\n        match end {\n            Some(end) if end <= memory.current_length() => {\n                // Initializer is in bounds\n            }\n            _ => {\n                bail!(\"memory out of bounds: data segment does not fit\")\n            }\n        }\n    }\n\n    Ok(())\n}\n\nfn initialize_memories(instance: &mut Instance, module: &Module) -> Result<()> {\n    let memory_size_in_pages = &|instance: &mut Instance, memory| {\n        (instance.get_memory(memory).current_length() as u64) / u64::from(WASM_PAGE_SIZE)\n    };\n\n    // Loads the `global` value and returns it as a `u64`, but sign-extends\n    // 32-bit globals which can be used as the base for 32-bit memories.\n    let get_global_as_u64 = &mut |instance: &mut Instance, global| unsafe {\n        let def = instance.defined_or_imported_global_ptr(global);\n        if module.globals[global].wasm_ty == WasmType::I64 {\n            *(*def).as_u64()\n        } else {\n            u64::from(*(*def).as_u32())\n        }\n    };\n\n    // Delegates to the `init_memory` method which is sort of a duplicate of\n    // `instance.memory_init_segment` but is used at compile-time in other\n    // contexts so is shared here to have only one method of memory\n    // initialization.\n    //\n    // This call to `init_memory` notably implements all the bells and whistles\n    // so errors only happen if an out-of-bounds segment is found, in which case\n    // a trap is returned.\n    let ok = module.memory_initialization.init_memory(\n        instance,\n        InitMemory::Runtime {\n            memory_size_in_pages,\n            get_global_as_u64,\n        },\n        |instance, memory_index, init| {\n            // If this initializer applies to a defined memory but that memory\n            // doesn't need initialization, due to something like copy-on-write\n            // pre-initializing it via mmap magic, then this initializer can be\n            // skipped entirely.\n            if let Some(memory_index) = module.defined_memory_index(memory_index) {\n                if !instance.memories[memory_index].needs_init() {\n                    return true;\n                }\n            }\n            let memory = instance.get_memory(memory_index);\n\n            unsafe {\n                let src = instance.wasm_data(init.data.clone());\n                let dst = memory.base.add(usize::try_from(init.offset).unwrap());\n                // FIXME audit whether this is safe in the presence of shared\n                // memory\n                // (https://github.com/bytecodealliance/wasmtime/issues/4203).\n                ptr::copy_nonoverlapping(src.as_ptr(), dst, src.len())\n            }\n            true\n        },\n    );\n    if !ok {\n        return Err(Trap::MemoryOutOfBounds.into());\n    }\n\n    Ok(())\n}\n\nfn check_init_bounds(instance: &mut Instance, module: &Module) -> Result<()> {\n    check_table_init_bounds(instance, module)?;\n\n    match &module.memory_initialization {\n        MemoryInitialization::Segmented(initializers) => {\n            check_memory_init_bounds(instance, initializers)?;\n        }\n        // Statically validated already to have everything in-bounds.\n        MemoryInitialization::Static { .. } => {}\n    }\n\n    Ok(())\n}\n\npub(super) fn initialize_instance(\n    instance: &mut Instance,\n    module: &Module,\n    is_bulk_memory: bool,\n) -> Result<()> {\n    // If bulk memory is not enabled, bounds check the data and element segments before\n    // making any changes. With bulk memory enabled, initializers are processed\n    // in-order and side effects are observed up to the point of an out-of-bounds\n    // initializer, so the early checking is not desired.\n    if !is_bulk_memory {\n        check_init_bounds(instance, module)?;\n    }\n\n    // Initialize the tables\n    initialize_tables(instance, module)?;\n\n    // Initialize the memories\n    initialize_memories(instance, &module)?;\n\n    Ok(())\n}\n\n/// Represents the on-demand instance allocator.\n#[derive(Clone)]\npub struct OnDemandInstanceAllocator {\n    mem_creator: Option<Arc<dyn RuntimeMemoryCreator>>,\n    #[cfg(feature = \"async\")]\n    stack_size: usize,\n}\n\nimpl OnDemandInstanceAllocator {\n    /// Creates a new on-demand instance allocator.\n    pub fn new(mem_creator: Option<Arc<dyn RuntimeMemoryCreator>>, stack_size: usize) -> Self {\n        drop(stack_size); // suppress unused warnings w/o async feature\n        Self {\n            mem_creator,\n            #[cfg(feature = \"async\")]\n            stack_size,\n        }\n    }\n}\n\nimpl Default for OnDemandInstanceAllocator {\n    fn default() -> Self {\n        Self {\n            mem_creator: None,\n            #[cfg(feature = \"async\")]\n            stack_size: 0,\n        }\n    }\n}\n\nunsafe impl InstanceAllocator for OnDemandInstanceAllocator {\n    fn allocate_index(&self, _req: &InstanceAllocationRequest) -> Result<usize> {\n        Ok(0)\n    }\n\n    fn deallocate_index(&self, index: usize) {\n        assert_eq!(index, 0);\n    }\n\n    fn allocate_memories(\n        &self,\n        _index: usize,\n        req: &mut InstanceAllocationRequest,\n        memories: &mut PrimaryMap<DefinedMemoryIndex, Memory>,\n    ) -> Result<()> {\n        let module = req.runtime_info.module();\n        let creator = self\n            .mem_creator\n            .as_deref()\n            .unwrap_or_else(|| &DefaultMemoryCreator);\n        let num_imports = module.num_imported_memories;\n        for (memory_idx, plan) in module.memory_plans.iter().skip(num_imports) {\n            let defined_memory_idx = module\n                .defined_memory_index(memory_idx)\n                .expect(\"Skipped imports, should never be None\");\n            let image = req.runtime_info.memory_image(defined_memory_idx)?;\n\n            memories.push(Memory::new_dynamic(\n                plan,\n                creator,\n                unsafe {\n                    req.store\n                        .get()\n                        .expect(\"if module has memory plans, store is not empty\")\n                },\n                image,\n            )?);\n        }\n        Ok(())\n    }\n\n    fn deallocate_memories(\n        &self,\n        _index: usize,\n        _mems: &mut PrimaryMap<DefinedMemoryIndex, Memory>,\n    ) {\n        // normal destructors do cleanup here\n    }\n\n    fn allocate_tables(\n        &self,\n        _index: usize,\n        req: &mut InstanceAllocationRequest,\n        tables: &mut PrimaryMap<DefinedTableIndex, Table>,\n    ) -> Result<()> {\n        let module = req.runtime_info.module();\n        let num_imports = module.num_imported_tables;\n        for (_, table) in module.table_plans.iter().skip(num_imports) {\n            tables.push(Table::new_dynamic(table, unsafe {\n                req.store\n                    .get()\n                    .expect(\"if module has table plans, store is not empty\")\n            })?);\n        }\n        Ok(())\n    }\n\n    fn deallocate_tables(&self, _index: usize, _tables: &mut PrimaryMap<DefinedTableIndex, Table>) {\n        // normal destructors do cleanup here\n    }\n\n    #[cfg(feature = \"async\")]\n    fn allocate_fiber_stack(&self) -> Result<wasmtime_fiber::FiberStack> {\n        if self.stack_size == 0 {\n            bail!(\"fiber stacks are not supported by the allocator\")\n        }\n\n        let stack = wasmtime_fiber::FiberStack::new(self.stack_size)?;\n        Ok(stack)\n    }\n\n    #[cfg(feature = \"async\")]\n    unsafe fn deallocate_fiber_stack(&self, _stack: &wasmtime_fiber::FiberStack) {\n        // The on-demand allocator has no further bookkeeping for fiber stacks\n    }\n\n    fn purge_module(&self, _: CompiledModuleId) {}\n}\n", "//! Runtime library calls.\n//!\n//! Note that Wasm compilers may sometimes perform these inline rather than\n//! calling them, particularly when CPUs have special instructions which compute\n//! them directly.\n//!\n//! These functions are called by compiled Wasm code, and therefore must take\n//! certain care about some things:\n//!\n//! * They must only contain basic, raw i32/i64/f32/f64/pointer parameters that\n//!   are safe to pass across the system ABI.\n//!\n//! * If any nested function propagates an `Err(trap)` out to the library\n//!   function frame, we need to raise it. This involves some nasty and quite\n//!   unsafe code under the covers! Notably, after raising the trap, drops\n//!   **will not** be run for local variables! This can lead to things like\n//!   leaking `InstanceHandle`s which leads to never deallocating JIT code,\n//!   instances, and modules if we are not careful!\n//!\n//! * The libcall must be entered via a Wasm-to-libcall trampoline that saves\n//!   the last Wasm FP and PC for stack walking purposes. (For more details, see\n//!   `crates/runtime/src/backtrace.rs`.)\n//!\n//! To make it easier to correctly handle all these things, **all** libcalls\n//! must be defined via the `libcall!` helper macro! See its doc comments below\n//! for an example, or just look at the rest of the file.\n//!\n//! ## Dealing with `externref`s\n//!\n//! When receiving a raw `*mut u8` that is actually a `VMExternRef` reference,\n//! convert it into a proper `VMExternRef` with `VMExternRef::clone_from_raw` as\n//! soon as apossible. Any GC before raw pointer is converted into a reference\n//! can potentially collect the referenced object, which could lead to use after\n//! free.\n//!\n//! Avoid this by eagerly converting into a proper `VMExternRef`! (Unfortunately\n//! there is no macro to help us automatically get this correct, so stay\n//! vigilant!)\n//!\n//! ```ignore\n//! pub unsafe extern \"C\" my_libcall_takes_ref(raw_extern_ref: *mut u8) {\n//!     // Before `clone_from_raw`, `raw_extern_ref` is potentially unrooted,\n//!     // and doing GC here could lead to use after free!\n//!\n//!     let my_extern_ref = if raw_extern_ref.is_null() {\n//!         None\n//!     } else {\n//!         Some(VMExternRef::clone_from_raw(raw_extern_ref))\n//!     };\n//!\n//!     // Now that we did `clone_from_raw`, it is safe to do a GC (or do\n//!     // anything else that might transitively GC, like call back into\n//!     // Wasm!)\n//! }\n//! ```\n\nuse crate::externref::VMExternRef;\nuse crate::table::{Table, TableElementType};\nuse crate::vmcontext::{VMCallerCheckedFuncRef, VMContext};\nuse crate::TrapReason;\nuse anyhow::Result;\nuse std::mem;\nuse std::ptr::{self, NonNull};\nuse std::time::{Duration, Instant};\nuse wasmtime_environ::{\n    DataIndex, ElemIndex, FuncIndex, GlobalIndex, MemoryIndex, TableIndex, Trap,\n};\n\n/// Actually public trampolines which are used by the runtime as the entrypoint\n/// for libcalls.\n///\n/// Note that the trampolines here are actually defined in inline assembly right\n/// now to ensure that the fp/sp on exit are recorded for backtraces to work\n/// properly.\npub mod trampolines {\n    use crate::{TrapReason, VMContext};\n\n    macro_rules! libcall {\n        (\n            $(\n                $( #[$attr:meta] )*\n                $name:ident( vmctx: vmctx $(, $pname:ident: $param:ident )* ) $( -> $result:ident )?;\n            )*\n        ) => {paste::paste! {\n            $(\n                // The actual libcall itself, which has the `pub` name here, is\n                // defined via the `wasm_to_libcall_trampoline!` macro on\n                // supported platforms or otherwise in inline assembly for\n                // platforms like s390x which don't have stable `global_asm!`\n                // yet.\n                extern \"C\" {\n                    #[allow(missing_docs)]\n                    #[allow(improper_ctypes)]\n                    pub fn $name(\n                        vmctx: *mut VMContext,\n                        $( $pname: libcall!(@ty $param), )*\n                    ) $(-> libcall!(@ty $result))?;\n                }\n\n                wasm_to_libcall_trampoline!($name ; [<impl_ $name>]);\n\n                // This is the direct entrypoint from the inline assembly which\n                // still has the same raw signature as the trampoline itself.\n                // This will delegate to the outer module to the actual\n                // implementation and automatically perform `catch_unwind` along\n                // with conversion of the return value in the face of traps.\n                //\n                // Note that rust targets which support `global_asm!` can use\n                // the `sym` operator to get the symbol here, but other targets\n                // like s390x need to use outlined assembly files which requires\n                // `no_mangle`.\n                #[cfg_attr(target_arch = \"s390x\", no_mangle)]\n                unsafe extern \"C\" fn [<impl_ $name>](\n                    vmctx : *mut VMContext,\n                    $( $pname : libcall!(@ty $param), )*\n                ) $( -> libcall!(@ty $result))? {\n                    let result = std::panic::catch_unwind(|| {\n                        super::$name(vmctx, $($pname),*)\n                    });\n                    match result {\n                        Ok(ret) => LibcallResult::convert(ret),\n                        Err(panic) => crate::traphandlers::resume_panic(panic),\n                    }\n                }\n\n                // This works around a `rustc` bug where compiling with LTO\n                // will sometimes strip out some of these symbols resulting\n                // in a linking failure.\n                #[allow(non_upper_case_globals)]\n                #[used]\n                static [<impl_ $name _ref>]: unsafe extern \"C\" fn(\n                    *mut VMContext,\n                    $( $pname : libcall!(@ty $param), )*\n                ) $( -> libcall!(@ty $result))? = [<impl_ $name>];\n\n            )*\n        }};\n\n        (@ty i32) => (u32);\n        (@ty i64) => (u64);\n        (@ty reference) => (*mut u8);\n        (@ty pointer) => (*mut u8);\n        (@ty vmctx) => (*mut VMContext);\n    }\n\n    wasmtime_environ::foreach_builtin_function!(libcall);\n\n    // Helper trait to convert results of libcalls below into the ABI of what\n    // the libcall expects.\n    //\n    // This basically entirely exists for the `Result` implementation which\n    // \"unwraps\" via a throwing of a trap.\n    trait LibcallResult {\n        type Abi;\n        unsafe fn convert(self) -> Self::Abi;\n    }\n\n    impl LibcallResult for () {\n        type Abi = ();\n        unsafe fn convert(self) {}\n    }\n\n    impl<T, E> LibcallResult for Result<T, E>\n    where\n        E: Into<TrapReason>,\n    {\n        type Abi = T;\n        unsafe fn convert(self) -> T {\n            match self {\n                Ok(t) => t,\n                Err(e) => crate::traphandlers::raise_trap(e.into()),\n            }\n        }\n    }\n\n    impl LibcallResult for *mut u8 {\n        type Abi = *mut u8;\n        unsafe fn convert(self) -> *mut u8 {\n            self\n        }\n    }\n}\n\nunsafe fn memory32_grow(\n    vmctx: *mut VMContext,\n    delta: u64,\n    memory_index: u32,\n) -> Result<*mut u8, TrapReason> {\n    let instance = (*vmctx).instance_mut();\n    let memory_index = MemoryIndex::from_u32(memory_index);\n    let result =\n        match instance\n            .memory_grow(memory_index, delta)\n            .map_err(|error| TrapReason::User {\n                error,\n                needs_backtrace: true,\n            })? {\n            Some(size_in_bytes) => size_in_bytes / (wasmtime_environ::WASM_PAGE_SIZE as usize),\n            None => usize::max_value(),\n        };\n    Ok(result as *mut _)\n}\n\n// Implementation of `table.grow`.\n//\n// Table grow can invoke user code provided in a ResourceLimiter{,Async}, so we\n// need to catch a possible panic.\nunsafe fn table_grow(\n    vmctx: *mut VMContext,\n    table_index: u32,\n    delta: u32,\n    // NB: we don't know whether this is a pointer to a `VMCallerCheckedFuncRef`\n    // or is a `VMExternRef` until we look at the table type.\n    init_value: *mut u8,\n) -> Result<u32> {\n    let instance = (*vmctx).instance_mut();\n    let table_index = TableIndex::from_u32(table_index);\n    let element = match instance.table_element_type(table_index) {\n        TableElementType::Func => (init_value as *mut VMCallerCheckedFuncRef).into(),\n        TableElementType::Extern => {\n            let init_value = if init_value.is_null() {\n                None\n            } else {\n                Some(VMExternRef::clone_from_raw(init_value))\n            };\n            init_value.into()\n        }\n    };\n    Ok(match instance.table_grow(table_index, delta, element)? {\n        Some(r) => r,\n        None => -1_i32 as u32,\n    })\n}\n\nuse table_grow as table_grow_funcref;\nuse table_grow as table_grow_externref;\n\n// Implementation of `table.fill`.\nunsafe fn table_fill(\n    vmctx: *mut VMContext,\n    table_index: u32,\n    dst: u32,\n    // NB: we don't know whether this is a `VMExternRef` or a pointer to a\n    // `VMCallerCheckedFuncRef` until we look at the table's element type.\n    val: *mut u8,\n    len: u32,\n) -> Result<(), Trap> {\n    let instance = (*vmctx).instance_mut();\n    let table_index = TableIndex::from_u32(table_index);\n    let table = &mut *instance.get_table(table_index);\n    match table.element_type() {\n        TableElementType::Func => {\n            let val = val as *mut VMCallerCheckedFuncRef;\n            table.fill(dst, val.into(), len)\n        }\n        TableElementType::Extern => {\n            let val = if val.is_null() {\n                None\n            } else {\n                Some(VMExternRef::clone_from_raw(val))\n            };\n            table.fill(dst, val.into(), len)\n        }\n    }\n}\n\nuse table_fill as table_fill_funcref;\nuse table_fill as table_fill_externref;\n\n// Implementation of `table.copy`.\nunsafe fn table_copy(\n    vmctx: *mut VMContext,\n    dst_table_index: u32,\n    src_table_index: u32,\n    dst: u32,\n    src: u32,\n    len: u32,\n) -> Result<(), Trap> {\n    let dst_table_index = TableIndex::from_u32(dst_table_index);\n    let src_table_index = TableIndex::from_u32(src_table_index);\n    let instance = (*vmctx).instance_mut();\n    let dst_table = instance.get_table(dst_table_index);\n    // Lazy-initialize the whole range in the source table first.\n    let src_range = src..(src.checked_add(len).unwrap_or(u32::MAX));\n    let src_table = instance.get_table_with_lazy_init(src_table_index, src_range);\n    Table::copy(dst_table, src_table, dst, src, len)\n}\n\n// Implementation of `table.init`.\nunsafe fn table_init(\n    vmctx: *mut VMContext,\n    table_index: u32,\n    elem_index: u32,\n    dst: u32,\n    src: u32,\n    len: u32,\n) -> Result<(), Trap> {\n    let table_index = TableIndex::from_u32(table_index);\n    let elem_index = ElemIndex::from_u32(elem_index);\n    let instance = (*vmctx).instance_mut();\n    instance.table_init(table_index, elem_index, dst, src, len)\n}\n\n// Implementation of `elem.drop`.\nunsafe fn elem_drop(vmctx: *mut VMContext, elem_index: u32) {\n    let elem_index = ElemIndex::from_u32(elem_index);\n    let instance = (*vmctx).instance_mut();\n    instance.elem_drop(elem_index);\n}\n\n// Implementation of `memory.copy` for locally defined memories.\nunsafe fn memory_copy(\n    vmctx: *mut VMContext,\n    dst_index: u32,\n    dst: u64,\n    src_index: u32,\n    src: u64,\n    len: u64,\n) -> Result<(), Trap> {\n    let src_index = MemoryIndex::from_u32(src_index);\n    let dst_index = MemoryIndex::from_u32(dst_index);\n    let instance = (*vmctx).instance_mut();\n    instance.memory_copy(dst_index, dst, src_index, src, len)\n}\n\n// Implementation of `memory.fill` for locally defined memories.\nunsafe fn memory_fill(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    dst: u64,\n    val: u32,\n    len: u64,\n) -> Result<(), Trap> {\n    let memory_index = MemoryIndex::from_u32(memory_index);\n    let instance = (*vmctx).instance_mut();\n    instance.memory_fill(memory_index, dst, val as u8, len)\n}\n\n// Implementation of `memory.init`.\nunsafe fn memory_init(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    data_index: u32,\n    dst: u64,\n    src: u32,\n    len: u32,\n) -> Result<(), Trap> {\n    let memory_index = MemoryIndex::from_u32(memory_index);\n    let data_index = DataIndex::from_u32(data_index);\n    let instance = (*vmctx).instance_mut();\n    instance.memory_init(memory_index, data_index, dst, src, len)\n}\n\n// Implementation of `ref.func`.\nunsafe fn ref_func(vmctx: *mut VMContext, func_index: u32) -> *mut u8 {\n    let instance = (*vmctx).instance_mut();\n    let anyfunc = instance\n        .get_caller_checked_anyfunc(FuncIndex::from_u32(func_index))\n        .expect(\"ref_func: caller_checked_anyfunc should always be available for given func index\");\n    anyfunc as *mut _\n}\n\n// Implementation of `data.drop`.\nunsafe fn data_drop(vmctx: *mut VMContext, data_index: u32) {\n    let data_index = DataIndex::from_u32(data_index);\n    let instance = (*vmctx).instance_mut();\n    instance.data_drop(data_index)\n}\n\n// Returns a table entry after lazily initializing it.\nunsafe fn table_get_lazy_init_funcref(\n    vmctx: *mut VMContext,\n    table_index: u32,\n    index: u32,\n) -> *mut u8 {\n    let instance = (*vmctx).instance_mut();\n    let table_index = TableIndex::from_u32(table_index);\n    let table = instance.get_table_with_lazy_init(table_index, std::iter::once(index));\n    let elem = (*table)\n        .get(index)\n        .expect(\"table access already bounds-checked\");\n\n    elem.into_ref_asserting_initialized() as *mut _\n}\n\n// Drop a `VMExternRef`.\nunsafe fn drop_externref(_vmctx: *mut VMContext, externref: *mut u8) {\n    let externref = externref as *mut crate::externref::VMExternData;\n    let externref = NonNull::new(externref).unwrap();\n    crate::externref::VMExternData::drop_and_dealloc(externref);\n}\n\n// Do a GC and insert the given `externref` into the\n// `VMExternRefActivationsTable`.\nunsafe fn activations_table_insert_with_gc(vmctx: *mut VMContext, externref: *mut u8) {\n    let externref = VMExternRef::clone_from_raw(externref);\n    let instance = (*vmctx).instance();\n    let (activations_table, module_info_lookup) = (*instance.store()).externref_activations_table();\n\n    // Invariant: all `externref`s on the stack have an entry in the activations\n    // table. So we need to ensure that this `externref` is in the table\n    // *before* we GC, even though `insert_with_gc` will ensure that it is in\n    // the table *after* the GC. This technically results in one more hash table\n    // look up than is strictly necessary -- which we could avoid by having an\n    // additional GC method that is aware of these GC-triggering references --\n    // but it isn't really a concern because this is already a slow path.\n    activations_table.insert_without_gc(externref.clone());\n\n    activations_table.insert_with_gc(externref, module_info_lookup);\n}\n\n// Perform a Wasm `global.get` for `externref` globals.\nunsafe fn externref_global_get(vmctx: *mut VMContext, index: u32) -> *mut u8 {\n    let index = GlobalIndex::from_u32(index);\n    let instance = (*vmctx).instance_mut();\n    let global = instance.defined_or_imported_global_ptr(index);\n    match (*global).as_externref().clone() {\n        None => ptr::null_mut(),\n        Some(externref) => {\n            let raw = externref.as_raw();\n            let (activations_table, module_info_lookup) =\n                (*instance.store()).externref_activations_table();\n            activations_table.insert_with_gc(externref, module_info_lookup);\n            raw\n        }\n    }\n}\n\n// Perform a Wasm `global.set` for `externref` globals.\nunsafe fn externref_global_set(vmctx: *mut VMContext, index: u32, externref: *mut u8) {\n    let externref = if externref.is_null() {\n        None\n    } else {\n        Some(VMExternRef::clone_from_raw(externref))\n    };\n\n    let index = GlobalIndex::from_u32(index);\n    let instance = (*vmctx).instance_mut();\n    let global = instance.defined_or_imported_global_ptr(index);\n\n    // Swap the new `externref` value into the global before we drop the old\n    // value. This protects against an `externref` with a `Drop` implementation\n    // that calls back into Wasm and touches this global again (we want to avoid\n    // it observing a halfway-deinitialized value).\n    let old = mem::replace((*global).as_externref_mut(), externref);\n    drop(old);\n}\n\n// Implementation of `memory.atomic.notify` for locally defined memories.\nunsafe fn memory_atomic_notify(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    addr_index: u64,\n    count: u32,\n) -> Result<u32, Trap> {\n    let memory = MemoryIndex::from_u32(memory_index);\n    let instance = (*vmctx).instance_mut();\n    instance\n        .get_runtime_memory(memory)\n        .atomic_notify(addr_index, count)\n}\n\n// Implementation of `memory.atomic.wait32` for locally defined memories.\nunsafe fn memory_atomic_wait32(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    addr_index: u64,\n    expected: u32,\n    timeout: u64,\n) -> Result<u32, Trap> {\n    // convert timeout to Instant, before any wait happens on locking\n    let timeout = (timeout as i64 >= 0).then(|| Instant::now() + Duration::from_nanos(timeout));\n    let memory = MemoryIndex::from_u32(memory_index);\n    let instance = (*vmctx).instance_mut();\n    Ok(instance\n        .get_runtime_memory(memory)\n        .atomic_wait32(addr_index, expected, timeout)? as u32)\n}\n\n// Implementation of `memory.atomic.wait64` for locally defined memories.\nunsafe fn memory_atomic_wait64(\n    vmctx: *mut VMContext,\n    memory_index: u32,\n    addr_index: u64,\n    expected: u64,\n    timeout: u64,\n) -> Result<u32, Trap> {\n    // convert timeout to Instant, before any wait happens on locking\n    let timeout = (timeout as i64 >= 0).then(|| Instant::now() + Duration::from_nanos(timeout));\n    let memory = MemoryIndex::from_u32(memory_index);\n    let instance = (*vmctx).instance_mut();\n    Ok(instance\n        .get_runtime_memory(memory)\n        .atomic_wait64(addr_index, expected, timeout)? as u32)\n}\n\n// Hook for when an instance runs out of fuel.\nunsafe fn out_of_gas(vmctx: *mut VMContext) -> Result<()> {\n    (*(*vmctx).instance().store()).out_of_gas()\n}\n\n// Hook for when an instance observes that the epoch has changed.\nunsafe fn new_epoch(vmctx: *mut VMContext) -> Result<u64> {\n    (*(*vmctx).instance().store()).new_epoch()\n}\n\n/// This module contains functions which are used for resolving relocations at\n/// runtime if necessary.\n///\n/// These functions are not used by default and currently the only platform\n/// they're used for is on x86_64 when SIMD is disabled and then SSE features\n/// are further disabled. In these configurations Cranelift isn't allowed to use\n/// native CPU instructions so it falls back to libcalls and we rely on the Rust\n/// standard library generally for implementing these.\n#[allow(missing_docs)]\npub mod relocs {\n    pub extern \"C\" fn floorf32(f: f32) -> f32 {\n        f.floor()\n    }\n\n    pub extern \"C\" fn floorf64(f: f64) -> f64 {\n        f.floor()\n    }\n\n    pub extern \"C\" fn ceilf32(f: f32) -> f32 {\n        f.ceil()\n    }\n\n    pub extern \"C\" fn ceilf64(f: f64) -> f64 {\n        f.ceil()\n    }\n\n    pub extern \"C\" fn truncf32(f: f32) -> f32 {\n        f.trunc()\n    }\n\n    pub extern \"C\" fn truncf64(f: f64) -> f64 {\n        f.trunc()\n    }\n\n    const TOINT_32: f32 = 1.0 / f32::EPSILON;\n    const TOINT_64: f64 = 1.0 / f64::EPSILON;\n\n    // NB: replace with `round_ties_even` from libstd when it's stable as\n    // tracked by rust-lang/rust#96710\n    pub extern \"C\" fn nearestf32(x: f32) -> f32 {\n        // Rust doesn't have a nearest function; there's nearbyint, but it's not\n        // stabilized, so do it manually.\n        // Nearest is either ceil or floor depending on which is nearest or even.\n        // This approach exploited round half to even default mode.\n        let i = x.to_bits();\n        let e = i >> 23 & 0xff;\n        if e >= 0x7f_u32 + 23 {\n            // Check for NaNs.\n            if e == 0xff {\n                // Read the 23-bits significand.\n                if i & 0x7fffff != 0 {\n                    // Ensure it's arithmetic by setting the significand's most\n                    // significant bit to 1; it also works for canonical NaNs.\n                    return f32::from_bits(i | (1 << 22));\n                }\n            }\n            x\n        } else {\n            (x.abs() + TOINT_32 - TOINT_32).copysign(x)\n        }\n    }\n\n    pub extern \"C\" fn nearestf64(x: f64) -> f64 {\n        let i = x.to_bits();\n        let e = i >> 52 & 0x7ff;\n        if e >= 0x3ff_u64 + 52 {\n            // Check for NaNs.\n            if e == 0x7ff {\n                // Read the 52-bits significand.\n                if i & 0xfffffffffffff != 0 {\n                    // Ensure it's arithmetic by setting the significand's most\n                    // significant bit to 1; it also works for canonical NaNs.\n                    return f64::from_bits(i | (1 << 51));\n                }\n            }\n            x\n        } else {\n            (x.abs() + TOINT_64 - TOINT_64).copysign(x)\n        }\n    }\n\n    pub extern \"C\" fn fmaf32(a: f32, b: f32, c: f32) -> f32 {\n        a.mul_add(b, c)\n    }\n\n    pub extern \"C\" fn fmaf64(a: f64, b: f64, c: f64) -> f64 {\n        a.mul_add(b, c)\n    }\n}\n", "//! WebAssembly trap handling, which is built on top of the lower-level\n//! signalhandling mechanisms.\n\nmod backtrace;\n\nuse crate::{VMContext, VMRuntimeLimits};\nuse anyhow::Error;\nuse std::any::Any;\nuse std::cell::{Cell, UnsafeCell};\nuse std::mem::MaybeUninit;\nuse std::ptr;\nuse std::sync::Once;\n\npub use self::backtrace::{Backtrace, Frame};\npub use self::tls::{tls_eager_initialize, TlsRestore};\n\n#[link(name = \"wasmtime-helpers\")]\nextern \"C\" {\n    #[allow(improper_ctypes)]\n    fn wasmtime_setjmp(\n        jmp_buf: *mut *const u8,\n        callback: extern \"C\" fn(*mut u8, *mut VMContext),\n        payload: *mut u8,\n        callee: *mut VMContext,\n    ) -> i32;\n    fn wasmtime_longjmp(jmp_buf: *const u8) -> !;\n}\n\ncfg_if::cfg_if! {\n    if #[cfg(all(target_os = \"macos\", not(feature = \"posix-signals-on-macos\")))] {\n        mod macos;\n        use macos as sys;\n    } else if #[cfg(unix)] {\n        mod unix;\n        use unix as sys;\n    } else if #[cfg(target_os = \"windows\")] {\n        mod windows;\n        use windows as sys;\n    }\n}\n\npub use sys::SignalHandler;\n\n/// Globally-set callback to determine whether a program counter is actually a\n/// wasm trap.\n///\n/// This is initialized during `init_traps` below. The definition lives within\n/// `wasmtime` currently.\nstatic mut IS_WASM_PC: fn(usize) -> bool = |_| false;\n\n/// This function is required to be called before any WebAssembly is entered.\n/// This will configure global state such as signal handlers to prepare the\n/// process to receive wasm traps.\n///\n/// This function must not only be called globally once before entering\n/// WebAssembly but it must also be called once-per-thread that enters\n/// WebAssembly. Currently in wasmtime's integration this function is called on\n/// creation of a `Engine`.\n///\n/// The `is_wasm_pc` argument is used when a trap happens to determine if a\n/// program counter is the pc of an actual wasm trap or not. This is then used\n/// to disambiguate faults that happen due to wasm and faults that happen due to\n/// bugs in Rust or elsewhere.\npub fn init_traps(is_wasm_pc: fn(usize) -> bool) {\n    static INIT: Once = Once::new();\n    INIT.call_once(|| unsafe {\n        IS_WASM_PC = is_wasm_pc;\n        sys::platform_init();\n    });\n}\n\n/// Raises a trap immediately.\n///\n/// This function performs as-if a wasm trap was just executed. This trap\n/// payload is then returned from `catch_traps` below.\n///\n/// # Safety\n///\n/// Only safe to call when wasm code is on the stack, aka `catch_traps` must\n/// have been previously called. Additionally no Rust destructors can be on the\n/// stack. They will be skipped and not executed.\npub unsafe fn raise_trap(reason: TrapReason) -> ! {\n    tls::with(|info| info.unwrap().unwind_with(UnwindReason::Trap(reason)))\n}\n\n/// Raises a user-defined trap immediately.\n///\n/// This function performs as-if a wasm trap was just executed, only the trap\n/// has a dynamic payload associated with it which is user-provided. This trap\n/// payload is then returned from `catch_traps` below.\n///\n/// # Safety\n///\n/// Only safe to call when wasm code is on the stack, aka `catch_traps` must\n/// have been previously called. Additionally no Rust destructors can be on the\n/// stack. They will be skipped and not executed.\npub unsafe fn raise_user_trap(error: Error, needs_backtrace: bool) -> ! {\n    raise_trap(TrapReason::User {\n        error,\n        needs_backtrace,\n    })\n}\n\n/// Raises a trap from inside library code immediately.\n///\n/// This function performs as-if a wasm trap was just executed. This trap\n/// payload is then returned from `catch_traps` below.\n///\n/// # Safety\n///\n/// Only safe to call when wasm code is on the stack, aka `catch_traps` must\n/// have been previously called. Additionally no Rust destructors can be on the\n/// stack. They will be skipped and not executed.\npub unsafe fn raise_lib_trap(trap: wasmtime_environ::Trap) -> ! {\n    raise_trap(TrapReason::Wasm(trap))\n}\n\n/// Carries a Rust panic across wasm code and resumes the panic on the other\n/// side.\n///\n/// # Safety\n///\n/// Only safe to call when wasm code is on the stack, aka `catch_traps` must\n/// have been previously called. Additionally no Rust destructors can be on the\n/// stack. They will be skipped and not executed.\npub unsafe fn resume_panic(payload: Box<dyn Any + Send>) -> ! {\n    tls::with(|info| info.unwrap().unwind_with(UnwindReason::Panic(payload)))\n}\n\n/// Stores trace message with backtrace.\n#[derive(Debug)]\npub struct Trap {\n    /// Original reason from where this trap originated.\n    pub reason: TrapReason,\n    /// Wasm backtrace of the trap, if any.\n    pub backtrace: Option<Backtrace>,\n}\n\n/// Enumeration of different methods of raising a trap.\n#[derive(Debug)]\npub enum TrapReason {\n    /// A user-raised trap through `raise_user_trap`.\n    User {\n        /// The actual user trap error.\n        error: Error,\n        /// Whether we need to capture a backtrace for this error or not.\n        needs_backtrace: bool,\n    },\n\n    /// A trap raised from Cranelift-generated code.\n    Jit {\n        /// The program counter where this trap originated.\n        ///\n        /// This is later used with side tables from compilation to translate\n        /// the trapping address to a trap code.\n        pc: usize,\n\n        /// If the trap was a memory-related trap such as SIGSEGV then this\n        /// field will contain the address of the inaccessible data.\n        ///\n        /// Note that wasm loads/stores are not guaranteed to fill in this\n        /// information. Dynamically-bounds-checked memories, for example, will\n        /// not access an invalid address but may instead load from NULL or may\n        /// explicitly jump to a `ud2` instruction. This is only available for\n        /// fault-based traps which are one of the main ways, but not the only\n        /// way, to run wasm.\n        faulting_addr: Option<usize>,\n    },\n\n    /// A trap raised from a wasm libcall\n    Wasm(wasmtime_environ::Trap),\n}\n\nimpl TrapReason {\n    /// Create a new `TrapReason::User` that does not have a backtrace yet.\n    pub fn user_without_backtrace(error: Error) -> Self {\n        TrapReason::User {\n            error,\n            needs_backtrace: true,\n        }\n    }\n\n    /// Create a new `TrapReason::User` that already has a backtrace.\n    pub fn user_with_backtrace(error: Error) -> Self {\n        TrapReason::User {\n            error,\n            needs_backtrace: false,\n        }\n    }\n\n    /// Is this a JIT trap?\n    pub fn is_jit(&self) -> bool {\n        matches!(self, TrapReason::Jit { .. })\n    }\n}\n\nimpl From<Error> for TrapReason {\n    fn from(err: Error) -> Self {\n        TrapReason::user_without_backtrace(err)\n    }\n}\n\nimpl From<wasmtime_environ::Trap> for TrapReason {\n    fn from(code: wasmtime_environ::Trap) -> Self {\n        TrapReason::Wasm(code)\n    }\n}\n\n/// Catches any wasm traps that happen within the execution of `closure`,\n/// returning them as a `Result`.\n///\n/// Highly unsafe since `closure` won't have any dtors run.\npub unsafe fn catch_traps<'a, F>(\n    signal_handler: Option<*const SignalHandler<'static>>,\n    capture_backtrace: bool,\n    caller: *mut VMContext,\n    mut closure: F,\n) -> Result<(), Box<Trap>>\nwhere\n    F: FnMut(*mut VMContext),\n{\n    let limits = (*caller).instance_mut().runtime_limits();\n\n    let result = CallThreadState::new(signal_handler, capture_backtrace, *limits).with(|cx| {\n        wasmtime_setjmp(\n            cx.jmp_buf.as_ptr(),\n            call_closure::<F>,\n            &mut closure as *mut F as *mut u8,\n            caller,\n        )\n    });\n\n    return match result {\n        Ok(x) => Ok(x),\n        Err((UnwindReason::Trap(reason), backtrace)) => Err(Box::new(Trap { reason, backtrace })),\n        Err((UnwindReason::Panic(panic), _)) => std::panic::resume_unwind(panic),\n    };\n\n    extern \"C\" fn call_closure<F>(payload: *mut u8, caller: *mut VMContext)\n    where\n        F: FnMut(*mut VMContext),\n    {\n        unsafe { (*(payload as *mut F))(caller) }\n    }\n}\n\n// Module to hide visibility of the `CallThreadState::prev` field and force\n// usage of its accessor methods.\nmod call_thread_state {\n    use super::*;\n    use std::mem;\n\n    /// Temporary state stored on the stack which is registered in the `tls` module\n    /// below for calls into wasm.\n    pub struct CallThreadState {\n        pub(super) unwind: UnsafeCell<MaybeUninit<(UnwindReason, Option<Backtrace>)>>,\n        pub(super) jmp_buf: Cell<*const u8>,\n        pub(super) signal_handler: Option<*const SignalHandler<'static>>,\n        pub(super) capture_backtrace: bool,\n\n        pub(crate) limits: *const VMRuntimeLimits,\n\n        prev: Cell<tls::Ptr>,\n\n        // The values of `VMRuntimeLimits::last_wasm_{exit_{pc,fp},entry_sp}` for\n        // the *previous* `CallThreadState`. Our *current* last wasm PC/FP/SP are\n        // saved in `self.limits`. We save a copy of the old registers here because\n        // the `VMRuntimeLimits` typically doesn't change across nested calls into\n        // Wasm (i.e. they are typically calls back into the same store and\n        // `self.limits == self.prev.limits`) and we must to maintain the list of\n        // contiguous-Wasm-frames stack regions for backtracing purposes.\n        old_last_wasm_exit_fp: Cell<usize>,\n        old_last_wasm_exit_pc: Cell<usize>,\n        old_last_wasm_entry_sp: Cell<usize>,\n    }\n\n    impl CallThreadState {\n        #[inline]\n        pub(super) fn new(\n            signal_handler: Option<*const SignalHandler<'static>>,\n            capture_backtrace: bool,\n            limits: *const VMRuntimeLimits,\n        ) -> CallThreadState {\n            CallThreadState {\n                unwind: UnsafeCell::new(MaybeUninit::uninit()),\n                jmp_buf: Cell::new(ptr::null()),\n                signal_handler,\n                capture_backtrace,\n                limits,\n                prev: Cell::new(ptr::null()),\n                old_last_wasm_exit_fp: Cell::new(0),\n                old_last_wasm_exit_pc: Cell::new(0),\n                old_last_wasm_entry_sp: Cell::new(0),\n            }\n        }\n\n        /// Get the saved FP upon exit from Wasm for the previous `CallThreadState`.\n        pub fn old_last_wasm_exit_fp(&self) -> usize {\n            self.old_last_wasm_exit_fp.get()\n        }\n\n        /// Get the saved PC upon exit from Wasm for the previous `CallThreadState`.\n        pub fn old_last_wasm_exit_pc(&self) -> usize {\n            self.old_last_wasm_exit_pc.get()\n        }\n\n        /// Get the saved SP upon entry into Wasm for the previous `CallThreadState`.\n        pub fn old_last_wasm_entry_sp(&self) -> usize {\n            self.old_last_wasm_entry_sp.get()\n        }\n\n        /// Get the previous `CallThreadState`.\n        pub fn prev(&self) -> tls::Ptr {\n            self.prev.get()\n        }\n\n        /// Connect the link to the previous `CallThreadState`.\n        ///\n        /// Synchronizes the last wasm FP, PC, and SP on `self` and the old\n        /// `self.prev` for the given new `prev`, and returns the old\n        /// `self.prev`.\n        pub unsafe fn set_prev(&self, prev: tls::Ptr) -> tls::Ptr {\n            let old_prev = self.prev.get();\n\n            // Restore the old `prev`'s saved registers in its\n            // `VMRuntimeLimits`. This is necessary for when we are async\n            // suspending the top `CallThreadState` and doing `set_prev(null)`\n            // on it, and so any stack walking we do subsequently will start at\n            // the old `prev` and look at its `VMRuntimeLimits` to get the\n            // initial saved registers.\n            if let Some(old_prev) = old_prev.as_ref() {\n                *(*old_prev.limits).last_wasm_exit_fp.get() = self.old_last_wasm_exit_fp();\n                *(*old_prev.limits).last_wasm_exit_pc.get() = self.old_last_wasm_exit_pc();\n                *(*old_prev.limits).last_wasm_entry_sp.get() = self.old_last_wasm_entry_sp();\n            }\n\n            self.prev.set(prev);\n\n            let mut old_last_wasm_exit_fp = 0;\n            let mut old_last_wasm_exit_pc = 0;\n            let mut old_last_wasm_entry_sp = 0;\n            if let Some(prev) = prev.as_ref() {\n                // We are entering a new `CallThreadState` or resuming a\n                // previously suspended one. This means we will push new Wasm\n                // frames that save the new Wasm FP/SP/PC registers into\n                // `VMRuntimeLimits`, we need to first save the old Wasm\n                // FP/SP/PC registers into this new `CallThreadState` to\n                // maintain our list of contiguous Wasm frame regions that we\n                // use when capturing stack traces.\n                //\n                // NB: the Wasm<--->host trampolines saved the Wasm FP/SP/PC\n                // registers in the active-at-that-time store's\n                // `VMRuntimeLimits`. For the most recent FP/PC/SP that is the\n                // `state.prev.limits` (since we haven't entered this\n                // `CallThreadState` yet). And that can be a different\n                // `VMRuntimeLimits` instance from the currently active\n                // `state.limits`, which will be used by the upcoming call into\n                // Wasm! Consider the case where we have multiple, nested calls\n                // across stores (with host code in between, by necessity, since\n                // only things in the same store can be linked directly\n                // together):\n                //\n                //     | ...             |\n                //     | Host            |  |\n                //     +-----------------+  | stack\n                //     | Wasm in store A |  | grows\n                //     +-----------------+  | down\n                //     | Host            |  |\n                //     +-----------------+  |\n                //     | Wasm in store B |  V\n                //     +-----------------+\n                //\n                // In this scenario `state.limits != state.prev.limits`,\n                // i.e. `B.limits != A.limits`! Therefore we must take care to\n                // read the old FP/SP/PC from `state.prev.limits`, rather than\n                // `state.limits`, and store those saved registers into the\n                // current `state`.\n                //\n                // See also the comment above the\n                // `CallThreadState::old_last_wasm_*` fields.\n                old_last_wasm_exit_fp =\n                    mem::replace(&mut *(*prev.limits).last_wasm_exit_fp.get(), 0);\n                old_last_wasm_exit_pc =\n                    mem::replace(&mut *(*prev.limits).last_wasm_exit_pc.get(), 0);\n                old_last_wasm_entry_sp =\n                    mem::replace(&mut *(*prev.limits).last_wasm_entry_sp.get(), 0);\n            }\n\n            self.old_last_wasm_exit_fp.set(old_last_wasm_exit_fp);\n            self.old_last_wasm_exit_pc.set(old_last_wasm_exit_pc);\n            self.old_last_wasm_entry_sp.set(old_last_wasm_entry_sp);\n\n            old_prev\n        }\n    }\n}\npub use call_thread_state::*;\n\nenum UnwindReason {\n    Panic(Box<dyn Any + Send>),\n    Trap(TrapReason),\n}\n\nimpl CallThreadState {\n    fn with(\n        mut self,\n        closure: impl FnOnce(&CallThreadState) -> i32,\n    ) -> Result<(), (UnwindReason, Option<Backtrace>)> {\n        let ret = tls::set(&mut self, |me| closure(me));\n        if ret != 0 {\n            Ok(())\n        } else {\n            Err(unsafe { self.read_unwind() })\n        }\n    }\n\n    #[cold]\n    unsafe fn read_unwind(&self) -> (UnwindReason, Option<Backtrace>) {\n        (*self.unwind.get()).as_ptr().read()\n    }\n\n    fn unwind_with(&self, reason: UnwindReason) -> ! {\n        let backtrace = match reason {\n            // Panics don't need backtraces. There is nowhere to attach the\n            // hypothetical backtrace to and it doesn't really make sense to try\n            // in the first place since this is a Rust problem rather than a\n            // Wasm problem.\n            UnwindReason::Panic(_)\n            // And if we are just propagating an existing trap that already has\n            // a backtrace attached to it, then there is no need to capture a\n            // new backtrace either.\n            | UnwindReason::Trap(TrapReason::User {\n                needs_backtrace: false,\n                ..\n            }) => None,\n            UnwindReason::Trap(_) => self.capture_backtrace(None),\n        };\n        unsafe {\n            (*self.unwind.get()).as_mut_ptr().write((reason, backtrace));\n            wasmtime_longjmp(self.jmp_buf.get());\n        }\n    }\n\n    /// Trap handler using our thread-local state.\n    ///\n    /// * `pc` - the program counter the trap happened at\n    /// * `call_handler` - a closure used to invoke the platform-specific\n    ///   signal handler for each instance, if available.\n    ///\n    /// Attempts to handle the trap if it's a wasm trap. Returns a few\n    /// different things:\n    ///\n    /// * null - the trap didn't look like a wasm trap and should continue as a\n    ///   trap\n    /// * 1 as a pointer - the trap was handled by a custom trap handler on an\n    ///   instance, and the trap handler should quickly return.\n    /// * a different pointer - a jmp_buf buffer to longjmp to, meaning that\n    ///   the wasm trap was succesfully handled.\n    #[cfg_attr(target_os = \"macos\", allow(dead_code))] // macOS is more raw and doesn't use this\n    fn take_jmp_buf_if_trap(\n        &self,\n        pc: *const u8,\n        call_handler: impl Fn(&SignalHandler) -> bool,\n    ) -> *const u8 {\n        // If we haven't even started to handle traps yet, bail out.\n        if self.jmp_buf.get().is_null() {\n            return ptr::null();\n        }\n\n        // First up see if any instance registered has a custom trap handler,\n        // in which case run them all. If anything handles the trap then we\n        // return that the trap was handled.\n        if let Some(handler) = self.signal_handler {\n            if unsafe { call_handler(&*handler) } {\n                return 1 as *const _;\n            }\n        }\n\n        // If this fault wasn't in wasm code, then it's not our problem\n        if unsafe { !IS_WASM_PC(pc as usize) } {\n            return ptr::null();\n        }\n\n        // If all that passed then this is indeed a wasm trap, so return the\n        // `jmp_buf` passed to `wasmtime_longjmp` to resume.\n        self.jmp_buf.replace(ptr::null())\n    }\n\n    fn set_jit_trap(&self, pc: *const u8, fp: usize, faulting_addr: Option<usize>) {\n        let backtrace = self.capture_backtrace(Some((pc as usize, fp)));\n        unsafe {\n            (*self.unwind.get()).as_mut_ptr().write((\n                UnwindReason::Trap(TrapReason::Jit {\n                    pc: pc as usize,\n                    faulting_addr,\n                }),\n                backtrace,\n            ));\n        }\n    }\n\n    fn capture_backtrace(&self, pc_and_fp: Option<(usize, usize)>) -> Option<Backtrace> {\n        if !self.capture_backtrace {\n            return None;\n        }\n\n        Some(unsafe { Backtrace::new_with_trap_state(self, pc_and_fp) })\n    }\n\n    pub(crate) fn iter<'a>(&'a self) -> impl Iterator<Item = &Self> + 'a {\n        let mut state = Some(self);\n        std::iter::from_fn(move || {\n            let this = state?;\n            state = unsafe { this.prev().as_ref() };\n            Some(this)\n        })\n    }\n}\n\nstruct ResetCell<'a, T: Copy>(&'a Cell<T>, T);\n\nimpl<T: Copy> Drop for ResetCell<'_, T> {\n    #[inline]\n    fn drop(&mut self) {\n        self.0.set(self.1);\n    }\n}\n\n// A private inner module for managing the TLS state that we require across\n// calls in wasm. The WebAssembly code is called from C++ and then a trap may\n// happen which requires us to read some contextual state to figure out what to\n// do with the trap. This `tls` module is used to persist that information from\n// the caller to the trap site.\nmod tls {\n    use super::CallThreadState;\n    use std::ptr;\n\n    pub use raw::Ptr;\n\n    // An even *more* inner module for dealing with TLS. This actually has the\n    // thread local variable and has functions to access the variable.\n    //\n    // Note that this is specially done to fully encapsulate that the accessors\n    // for tls may or may not be inlined. Wasmtime's async support employs stack\n    // switching which can resume execution on different OS threads. This means\n    // that borrows of our TLS pointer must never live across accesses because\n    // otherwise the access may be split across two threads and cause unsafety.\n    //\n    // This also means that extra care is taken by the runtime to save/restore\n    // these TLS values when the runtime may have crossed threads.\n    //\n    // Note, though, that if async support is disabled at compile time then\n    // these functions are free to be inlined.\n    mod raw {\n        use super::CallThreadState;\n        use std::cell::Cell;\n        use std::ptr;\n\n        pub type Ptr = *const CallThreadState;\n\n        // The first entry here is the `Ptr` which is what's used as part of the\n        // public interface of this module. The second entry is a boolean which\n        // allows the runtime to perform per-thread initialization if necessary\n        // for handling traps (e.g. setting up ports on macOS and sigaltstack on\n        // Unix).\n        thread_local!(static PTR: Cell<(Ptr, bool)> = const { Cell::new((ptr::null(), false)) });\n\n        #[cfg_attr(feature = \"async\", inline(never))] // see module docs\n        #[cfg_attr(not(feature = \"async\"), inline)]\n        pub fn replace(val: Ptr) -> Ptr {\n            PTR.with(|p| {\n                // When a new value is configured that means that we may be\n                // entering WebAssembly so check to see if this thread has\n                // performed per-thread initialization for traps.\n                let (prev, initialized) = p.get();\n                if !initialized {\n                    super::super::sys::lazy_per_thread_init();\n                }\n                p.set((val, true));\n                prev\n            })\n        }\n\n        /// Eagerly initialize thread-local runtime functionality. This will be performed\n        /// lazily by the runtime if users do not perform it eagerly.\n        #[cfg_attr(feature = \"async\", inline(never))] // see module docs\n        #[cfg_attr(not(feature = \"async\"), inline)]\n        pub fn initialize() {\n            PTR.with(|p| {\n                let (state, initialized) = p.get();\n                if initialized {\n                    return;\n                }\n                super::super::sys::lazy_per_thread_init();\n                p.set((state, true));\n            })\n        }\n\n        #[cfg_attr(feature = \"async\", inline(never))] // see module docs\n        #[cfg_attr(not(feature = \"async\"), inline)]\n        pub fn get() -> Ptr {\n            PTR.with(|p| p.get().0)\n        }\n    }\n\n    pub use raw::initialize as tls_eager_initialize;\n\n    /// Opaque state used to help control TLS state across stack switches for\n    /// async support.\n    pub struct TlsRestore {\n        state: raw::Ptr,\n    }\n\n    impl TlsRestore {\n        /// Takes the TLS state that is currently configured and returns a\n        /// token that is used to replace it later.\n        ///\n        /// This is not a safe operation since it's intended to only be used\n        /// with stack switching found with fibers and async wasmtime.\n        pub unsafe fn take() -> TlsRestore {\n            // Our tls pointer must be set at this time, and it must not be\n            // null. We need to restore the previous pointer since we're\n            // removing ourselves from the call-stack, and in the process we\n            // null out our own previous field for safety in case it's\n            // accidentally used later.\n            let state = raw::get();\n            if let Some(state) = state.as_ref() {\n                let prev_state = state.set_prev(ptr::null());\n                raw::replace(prev_state);\n            } else {\n                // Null case: we aren't in a wasm context, so theres no tls to\n                // save for restoration.\n            }\n\n            TlsRestore { state }\n        }\n\n        /// Restores a previous tls state back into this thread's TLS.\n        ///\n        /// This is unsafe because it's intended to only be used within the\n        /// context of stack switching within wasmtime.\n        pub unsafe fn replace(self) {\n            // Null case: we aren't in a wasm context, so theres no tls\n            // to restore.\n            if self.state.is_null() {\n                return;\n            }\n\n            // We need to configure our previous TLS pointer to whatever is in\n            // TLS at this time, and then we set the current state to ourselves.\n            let prev = raw::get();\n            assert!((*self.state).prev().is_null());\n            (*self.state).set_prev(prev);\n            raw::replace(self.state);\n        }\n    }\n\n    /// Configures thread local state such that for the duration of the\n    /// execution of `closure` any call to `with` will yield `state`, unless\n    /// this is recursively called again.\n    #[inline]\n    pub fn set<R>(state: &mut CallThreadState, closure: impl FnOnce(&CallThreadState) -> R) -> R {\n        struct Reset<'a> {\n            state: &'a CallThreadState,\n        }\n\n        impl Drop for Reset<'_> {\n            #[inline]\n            fn drop(&mut self) {\n                unsafe {\n                    let prev = self.state.set_prev(ptr::null());\n                    let old_state = raw::replace(prev);\n                    debug_assert!(std::ptr::eq(old_state, self.state));\n                }\n            }\n        }\n\n        let prev = raw::replace(state);\n\n        unsafe {\n            state.set_prev(prev);\n\n            let reset = Reset { state };\n            closure(reset.state)\n        }\n    }\n\n    /// Returns the last pointer configured with `set` above, if any.\n    pub fn with<R>(closure: impl FnOnce(Option<&CallThreadState>) -> R) -> R {\n        let p = raw::get();\n        unsafe { closure(if p.is_null() { None } else { Some(&*p) }) }\n    }\n}\n"], "filenames": ["RELEASES.md", "crates/environ/src/module.rs", "crates/runtime/src/instance.rs", "crates/runtime/src/instance/allocator.rs", "crates/runtime/src/libcalls.rs", "crates/runtime/src/traphandlers.rs"], "buggy_code_start_loc": [9, 243, 150, 203, 415, 222], "buggy_code_end_loc": [173, 647, 1187, 387, 439, 223], "fixing_code_start_loc": [10, 244, 150, 203, 415, 222], "fixing_code_end_loc": [217, 649, 1195, 379, 439, 223], "type": "CWE-758", "message": "Wasmtime is a standalone runtime for WebAssembly. Prior to versions 6.0.2, 7.0.1, and 8.0.1, Wasmtime's implementation of managing per-instance state, such as tables and memories, contains LLVM-level undefined behavior. This undefined behavior was found to cause runtime-level issues when compiled with LLVM 16 which causes some writes, which are critical for correctness, to be optimized away. Vulnerable versions of Wasmtime compiled with Rust 1.70, which is currently in beta, or later are known to have incorrectly compiled functions. Versions of Wasmtime compiled with the current Rust stable release, 1.69, and prior are not known at this time to have any issues, but can theoretically exhibit potential issues.\n\nThe underlying problem is that Wasmtime's runtime state for an instance involves a Rust-defined structure called `Instance` which has a trailing `VMContext` structure after it. This `VMContext` structure has a runtime-defined layout that is unique per-module. This representation cannot be expressed with safe code in Rust so `unsafe` code is required to maintain this state. The code doing this, however, has methods which take `&self` as an argument but modify data in the `VMContext` part of the allocation. This means that pointers derived from `&self` are mutated. This is typically not allowed, except in the presence of `UnsafeCell`, in Rust. When compiled to LLVM these functions have `noalias readonly` parameters which means it's UB to write through the pointers.\n\nWasmtime's internal representation and management of `VMContext` has been updated to use `&mut self` methods where appropriate. Additionally verification tools for `unsafe` code in Rust, such as `cargo miri`, are planned to be executed on the `main` branch soon to fix any Rust-level issues that may be exploited in future compiler versions.\n\nPrecomplied binaries available for Wasmtime from GitHub releases have been compiled with at most LLVM 15 so are not known to be vulnerable. As mentioned above, however, it's still recommended to update.\n\nWasmtime version 6.0.2, 7.0.1, and 8.0.1 have been issued which contain the patch necessary to work correctly on LLVM 16 and have no known UB on LLVM 15 and earlier. If Wasmtime is compiled with Rust 1.69 and prior, which use LLVM 15, then there are no known issues. There is a theoretical possibility for undefined behavior to exploited, however, so it's recommended that users upgrade to a patched version of Wasmtime. Users using beta Rust (1.70 at this time) or nightly Rust (1.71 at this time) must update to a patched version to work correctly.", "other": {"cve": {"id": "CVE-2023-30624", "sourceIdentifier": "security-advisories@github.com", "published": "2023-04-27T17:15:08.737", "lastModified": "2023-05-11T15:01:12.130", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Wasmtime is a standalone runtime for WebAssembly. Prior to versions 6.0.2, 7.0.1, and 8.0.1, Wasmtime's implementation of managing per-instance state, such as tables and memories, contains LLVM-level undefined behavior. This undefined behavior was found to cause runtime-level issues when compiled with LLVM 16 which causes some writes, which are critical for correctness, to be optimized away. Vulnerable versions of Wasmtime compiled with Rust 1.70, which is currently in beta, or later are known to have incorrectly compiled functions. Versions of Wasmtime compiled with the current Rust stable release, 1.69, and prior are not known at this time to have any issues, but can theoretically exhibit potential issues.\n\nThe underlying problem is that Wasmtime's runtime state for an instance involves a Rust-defined structure called `Instance` which has a trailing `VMContext` structure after it. This `VMContext` structure has a runtime-defined layout that is unique per-module. This representation cannot be expressed with safe code in Rust so `unsafe` code is required to maintain this state. The code doing this, however, has methods which take `&self` as an argument but modify data in the `VMContext` part of the allocation. This means that pointers derived from `&self` are mutated. This is typically not allowed, except in the presence of `UnsafeCell`, in Rust. When compiled to LLVM these functions have `noalias readonly` parameters which means it's UB to write through the pointers.\n\nWasmtime's internal representation and management of `VMContext` has been updated to use `&mut self` methods where appropriate. Additionally verification tools for `unsafe` code in Rust, such as `cargo miri`, are planned to be executed on the `main` branch soon to fix any Rust-level issues that may be exploited in future compiler versions.\n\nPrecomplied binaries available for Wasmtime from GitHub releases have been compiled with at most LLVM 15 so are not known to be vulnerable. As mentioned above, however, it's still recommended to update.\n\nWasmtime version 6.0.2, 7.0.1, and 8.0.1 have been issued which contain the patch necessary to work correctly on LLVM 16 and have no known UB on LLVM 15 and earlier. If Wasmtime is compiled with Rust 1.69 and prior, which use LLVM 15, then there are no known issues. There is a theoretical possibility for undefined behavior to exploited, however, so it's recommended that users upgrade to a patched version of Wasmtime. Users using beta Rust (1.70 at this time) or nightly Rust (1.71 at this time) must update to a patched version to work correctly."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:H/UI:R/S:U/C:L/I:L/A:L", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "HIGH", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 3.9, "baseSeverity": "LOW"}, "exploitabilityScore": 0.5, "impactScore": 3.4}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-758"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:*:*:*:*:*:rust:*:*", "versionEndExcluding": "6.0.2", "matchCriteriaId": "EB4A034B-F7EC-4002-8C5F-0E4176D4CC10"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:7.0.0:*:*:*:*:rust:*:*", "matchCriteriaId": "80F1E946-639F-4878-A2EF-0D0A52B30623"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:8.0.0:*:*:*:*:rust:*:*", "matchCriteriaId": "1AA99C22-6A08-4CDB-AAA2-7498C0DC2EC0"}]}]}], "references": [{"url": "https://github.com/bytecodealliance/wasmtime/commit/0977952dcd9d482bff7c288868ccb52769b3a92e", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ch89-5g45-qwc7", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/bytecodealliance/wasmtime/commit/0977952dcd9d482bff7c288868ccb52769b3a92e"}}